Neural Manifold Operators for Learning the Evolution of Physical
Dynamics
Hao Wu∗
Tencent TEG
Beijing, China
wuhao2022@mail.ustc.edu.cnKangyu Weng
Xingjian College, Tsinghua University
Beijing, China
wengky20@mails.tsinghua.edu.cnShuyi Zhou†
Department of Earth System Science,
Tsinghua University
Beijing, China
zhousy22@mails.tsinghua.edu.cn
Xiaomeng Huang†‡
Department of Earth System Science,
Tsinghua University
Beijing, China
hxm@tsinghua.edu.cnWei Xiong†‡
Department of Earth System Science,
Tsinghua University
Beijing, China
xiongw21@mails.tsinghua.edu.cn
Abstract
Modeling the evolution of physical dynamics is a foundational
problem in science and engineering, and it is regarded as the mod-
eling of an operator mapping between infinite-dimensional func-
tional spaces. Operator learning methods, learning the underlying
infinite-dimensional operator in a high-dimensional latent space,
have shown significant potential in modeling physical dynamics.
However, there remains insufficient research on how to approxi-
mate an infinite-dimensional operator using a finite-dimensional
parameter space. Inappropriate dimensionality representation of
the underlying operator leads to convergence difficulties, decreas-
ing generalization capability, and violating the physical consistency.
To address the problem, we present Neural Manifold Operator
(NMO) to learn the invariant subspace with the intrinsic dimension
to parameterize infinite-dimensional underlying operators. NMO
achieves state-of-the-art performance in statistical and physical
metrics and gains 23.35% average improvement on three real-world
scenarios and four equation-governed scenarios across a wide range
of multi-disciplinary fields. Our paradigm has demonstrated univer-
sal effectiveness across various model structure implementations,
including Multi-Layer Perceptron, Convolutional Neural Networks,
and Transformers. Experimentally, we prove that the intrinsic di-
mension calculated by our paradigm is the optimal dimensional
representation of the underlying operators. We release our code at
https://github.com/AI4EarthLab/Neural-Manifold-Operators.
∗This work was completed during a visit to the AI4EarthLab at Tsinghua University,
supervised by Prof. Huang.
†Department of Earth System Science, Ministry of Education Key Laboratory for Earth
System Modeling, Institute for Global Change Studies, Tsinghua University, Beijing
100084, China.
‡Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671779CCS Concepts
•Computing methodologies →Artificial intelligence; •Oper-
ator learning→Physical Dynamics.
1 Introduction
Modeling the evolution of physical dynamics is the foundation for
studying and predicting physical systems, a common challenge in
science and engineering [ 3]. Throughout the history of science,
analytical models of physical dynamics (e.g. Newton’s laws of mo-
tion) derived from the first principle are used to study the evolution
of physical systems and make physical dynamics predictable [ 23],
which breeds a lot of real-world applications, such as numerical
weather prediction systems [ 2]. However, when facing real-world
scenarios, such physical systems with high degrees of freedom and
complexity make solving the model and quantifying its evolution
harder, which generally means higher computational costs and
more approximate assumptions to compromise.
With the rapid development of deep learning, a new paradigm
for modeling and predicting physical dynamics is widely discussed.
Deep learning models can learn underlying physical relationships
from data and predict the future state at a lower cost, which leads
to many achievements in the study, modeling and prediction of
physical dynamics [ 11]. Different from other areas in deep learning,
learning the evolution of physical dynamics is generally equivalent
to learning nonlinear infinite-dimensional operator mappings be-
tween Banach space [ 49], which requires deep learning models to
be sufficiently generalized to learn the physics system’s intrinsic
dynamics, instead of local fitting for training data.
Learning the intrinsic dynamics of physics systems is critical
for deep learning models. Although several studies regard physics
variables as computer vision tasks and get good performance in
statistical metrics, these methods usually get poor performance
in physics consistency and are hard to generalize into similar sce-
narios in the same physics system (e.g. different initial conditions
or configure parameters). Recently, operator learning, a class of
deep learning methods designed for learning infinite-dimension
operators [ 26,36], has been widely employed for modeling physi-
cal systems. Such methods generally project the original physics
space into higher-dimensional latent space and parameterize the
3356
KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Wu, Kangyu Weng, Shuyi Zhou, Xiaomeng Huang, and Wei Xiong
Figure 1: The prediction visualization of NMO in several scenarios.
underlying operators describing the evolution of physical dynam-
ics by neural network structures. However, the determination of
the latent space dimension in these methods is generally subjec-
tive and empirical, and even it may change in the same physical
system with different configured parameters. However, the redun-
dant dimension representation of the underlying operators leads
to several problems including the convergence difficulty, reducing
the generalization capability, and even destroying the physics con-
sistency of physical systems. Therefore, how to approximate the
infinite-dimension operator using an appropriate finite-dimension
parameter space is the key problem for further developing operator
learning methods.
To address the problem, we propose Neural Manifold Operator
(NMO), an operator learning paradigm for learning the intrinsic
dimension representation of the underlying operator. By calculat-
ing the minimum dimensional submanifold representation of the
variables in the latent space, NMO can adaptively determine the
intrinsic dimension of the physical system. By projecting into a com-
pact invariant-subspace with the intrinsic dimension, NMO enables
efficient and accurate learning of the underlying operators and pre-
serves the physics consistency of the system. We introduce several
benchmarks, including real-world scenarios and equation-governed
scenarios which encompass complex weather and ocean systems, as
well as chaotic and interacting physical dynamics, aiming to evalu-
ate the capacity of our model for approximation, generalization, and
preserving physical properties. Compared to several baseline mod-
els, NMO achieves state-of-the-art performance in statistical and
physical metrics. We experimentally demonstrate that the intrinsic
dimension calculated by our paradigm is the optimal dimension of
the latent space in efficiency and accuracy. Our paradigm applies
to various physical systems and different neural network structure
implementations. In summary, our contributions are as follows:
•Intrinsic dimension representation: NMO learns the intrin-
sic dynamics of the physics system by learning the invariant
subspace of the underlying infinite-dimensional operators with
intrinsic dimension, which is calculated using the Maximum
Likelihood Estimation method.
•Generic operator learning paradigm: NMO is a generic oper-
ator learning paradigm for various network structure implemen-
tations including Multi-Layer Perceptron, Convolutional Neural
Network, and Transformer.
•Benefits in multi-disciplinary areas: NMO achieves state-of-
the-art performance in several real-world and equation-governedscenarios, ranging from mathematics, physics, chemistry and
earth science, as shown in Figure 1.
•Efficiency and Accuracy: By intrinsic dimension projection,
NMO significantly reduces the training parameters and effec-
tively improves the capability of generalization and physical
consistency.
2 Preliminaries
2.1 Deep Learning for Physical Dynamics
In recent years, it has been produced a lot of elaborative deep learn-
ing methods for learning physical dynamics. Due to the similar
tensor shape, modeling physical systems is viewed as computer
vision problems. Several state-of-the-art models designed for com-
puter vision tasks (e.g. image super-resolution or video prediction)
are used to model physics dynamics [ 61,62]. However, physics
inconsistency, unexplainability, and poor generalization limit fur-
ther development. More deep-learning methods guided by physics
theory have been designed, which can be roughly categorized into
equation-constraint, interpretable-structure, and operator learning
methods.
Equation-constraint methods. Incorporating physical laws into
the loss function, physics-informed machine learning methods
[21] ensure that the prediction result satisfies specific physical
properties, and even achieve unsupervised prediction for equation-
governed dynamics [ 13,45,54,55,57,66]. However, for complicated
scenarios such as real-world dynamics, incomplete physics laws
and imbalance of loss function terms makes optimization for the
neural networks significantly hard limit the performance of the
models [40, 58].
Interpretable-structure methods. Interpretable-structure meth-
ods for learning physical dynamics use the mathematical equivari-
ance between deep learning structure and physical equations to
design architectures, which incorporate more physical inductive
bias. PDE-Net [ 35] proves the similar mathematical properties of the
convolution operation as the difference operator and leverages the
theory to develop a framework for learning time-dependent partial
differential equations. Neural ODE [ 8] demonstrates that contin-
uous Residual Networks[ 18] can be mathematically expressed as
ordinary differential equations, and is utilized for predicting the
dynamics systems [ 19,22,38]. Based on Noether’s theorem, equi-
variant deep learning methods incorporate geometric symmetry
3357Neural Manifold Operators for Learning the Evolution of Physical Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain.
into neural networks by equivariant group transformation for con-
strain conservation of physical systems [ 5,12,17,52,53]. However,
such models with strong physics inductive bias in structure may
not be generic applicable in different physics scenarios, and even
degrade the performance and generalization capability of the model
in real-world dynamics with noisy or incomplete data [56].
Operator-learning methods. Operator learning methods are de-
signed for learning mappings between infinite-dimensional func-
tion spaces. Based on the universal approximation theorem [ 10,20],
DeepONet [ 36] learns the target operator by sampling the function
space. Koopman theory [ 24] inspires several methods designed to
approximate the infinite-dimension Koopman operator in the obser-
vation space [ 37,63–65]. Besides, Green’s function-based models
convert infinite-dimensional operator mappings into kernel integral
parameterization [ 25,30–33,50]. However, these methods learn the
underlying operators in a high-dimensional latent space, but fur-
ther discussion about the dimension of latent space is still lacking.
Accurately representing infinite-dimensional operators in finite-
dimensional parameter space remains a challenge.
2.2 Dimension Representation of Operators
The intrinsic dimension can be conceptualized as the minimum
number of variables or parameters required for a minimal represen-
tation, which is often regarded as the minimal number of hidden
neurons for the deep learning model to represent the target. Es-
timating the intrinsic dimension of physical systems is good for
learning the underlying intrinsic dynamics behind the data [ 7,14],
finding parameterized surrogate models and building reduced order
models [ 1,15,28]. For operator-learning methods, finite dimension
representation for infinite operators is indispensable. Low-rank
Neural Operator [ 25] reconstructs r-rank operator by SVD. Dy-
namic Mode Decomposition [ 42,43] and several Koopman-based
deep learning methods [ 63–65] have been developed to identify the
invariant subspace of the Koopman operator, allowing for finite-
dimensional linear representations of complex dynamic systems.
NOMAD learns a low-dimensional representation of solution with
a nonlinear manifold decoder [ 44]. With a new universal approx-
imation theorem under minimal assumptions for the underlying
operator, PCA-Net partially overcomes the general curse of dimen-
sionality for operator learning [ 27]. However, it still lacks a unified
paradigm designed for learning the intrinsic dimension represen-
tation of operators that applies to various physical systems and
model structures.
3 Overview
3.1 Problem Definition
Learning the evolution of a dynamics system can be regarded as an
operator learning issue. A general form of a dynamics system can
be expressed as
𝑑
𝑑𝑡x(𝑡) =f(x,𝛾(𝑡)),x(0) = x0 (1)
where x∈R𝑛is state variables and flow mapping f:R𝑛×𝑇→
R𝑛. The Eq. (1) can be either expressed autonomous system when
𝛾(𝑡)is a known time-invariant input, or a non-autonomous system
when𝛾(𝑡)is a known time-dependent input. Whether the form ofthe equation is known and solvable, the analytical solution of the
system satisfies
x(𝑡) =x0+Z𝑡
0f(x(𝑠),𝛾(𝑠))𝑑𝑠. (2)
According to the operator theory[ 6], we define an infinite Hilbert
space as an observable function 𝑔and vector space 𝑉, expressed as
𝑔=∞∑︁
𝑘=1𝛼𝑘𝑣𝑘, (3)
where𝑣𝑘∈𝑉is coordinate basis for the Hilbert space and 𝛼𝑘
denotes coefficient. Given an appropriate observation space, the
evolution of dynamics can be defined by infinite-dimensional linear
operatorL𝑡, which acts on the observation function 𝑔. Therefore,
the system can be reformulated as
𝑔(x𝑡+𝜀) =L𝑡𝑔(x𝑡), (4)
which is a linearized system. The infinite-dimensional operator
denotes the evolution of the system, which can get the next timestep
of the observation from 𝑔(𝑥𝑡).
Given a time series of samples for the dynamics u={𝑥0,...,𝑥𝑡+𝑡′},
denoting the initial state sequence x𝑡={𝑥0,𝑥1,....,𝑥𝑡}, we can get
its future states by the operator and denoted by y𝑡={𝑦0,𝑦1,...𝑦𝑡′},
where the subscript of xandyis the time length of the sequence
with the same time interval and u=x∪y. Define Ωbe a bounded
open set in R𝑑, and letXandYbe separable Banach spaces defined
onΩwith dimensions 𝑑𝑥and𝑑𝑦, respectively. Suppose that we
have sampling pair combined with two continuous time series of the
dynamics{x𝑖,y𝑖}𝑛
𝑖=1fromXandY, we can build an approximation
ofLby constructing a parametric map 𝐺𝜃,
𝐺𝜃:X→Y,𝜃∈Θ, (5)
where𝜃is the parameter of the parametric map in finite-dimensional
parameter space Θ. Define a cost function 𝐶to seek a minimizer of
the problem
min
𝜃∈ΘE[𝐶(𝐺(𝑥,𝜃),L(𝑥))], (6)
where𝑥is the observation of the initial state 𝐹from the Banach
spaceX, to calculate the optimal parameter 𝜃†∈Θand converge
to
𝐺
·,𝜃†
=𝐺𝜃†≈L. (7)
Therefore, our goal is to construct a parametric map 𝐺𝜃by neural
network to learn the evolution of the system. However, considering
observation space is infinite-dimensional but the parameter space
of the neural network is finite-dimensional, we need to learn a
finite-dimensional invariant-subspace of the infinite-dimensional
𝑉, in which the observation function 𝑔is defined, to get the approx-
imation of the operator L. The operatorLacts on the observation
in the subspace spanned by {𝑣1,𝑣2,···,𝑣𝑚}∈R𝑚, which means
the operator can be parameterized by finite-dimensional parameter
space, and it can be expressed as
L𝑔=𝑚∑︁
𝑘=1𝛽𝑘𝑣𝑘 (8)
3358KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Wu, Kangyu Weng, Shuyi Zhou, Xiaomeng Huang, and Wei Xiong
LoadLoadPredRec
FrozenFrozen
𝑷𝑷𝑸𝑸𝑾𝑾!
𝑻
Figure 2: An overview of NMO, which consists of encoder 𝑃, decoder𝑄and time evolution operator 𝑇. For the reconstruction process, the input
variable of the physical dynamics V1:𝑡is projected into a latent space by encoder 𝑃and reconstructed by decoder 𝑄. For the prediction process,
the latent variable ˆV1:𝑡encoded by 𝑃is projected as the variable ˆZin the compact invariant subspace R𝑚with intrinsic dimension 𝑚by linear
projection𝑊and it is time marched by the time evolution operator 𝑇and evolved into the future state of variable V𝑡+1,𝑡+𝑡′.
where𝛽𝑘denotes coefficient. Therefore, accurately construct-
ing the invariant subspaces in the optimal dimension to learn the
evolution is the main problem we address.
3.2 Overall Paradigm
To address the infinite-dimension approximate challenge, we in-
troduce Neural Manifold Operator, consisting of an encoder 𝑃, a
decoder𝑄, the time evolution operator 𝑇and manifold learning
algorithm with linear projection 𝑊and its pseudo-inverse operator
𝑊+. the parametric map 𝐺𝜃can be defined as
𝐺𝜃:=𝑄◦𝑊+◦𝑇◦𝑊◦𝑃, (9)
where◦denotes the operator composition. Our paradigm can
be divided into three parts as follows.
•Construct the observation space of the physical system. In
operator theory, the observation function is infinite-dimensional
and often non-linear. The encoder 𝑃and the decoder 𝑄are used
to learn the observation function 𝑔and its inverse 𝑔−1by recon-
struction constraint
𝑃𝑄=𝐼. (10)
The well-trained encoder can be expressed P:X→ ˆX, where ˆX
is the latent space on Ωwith a higher-dimension 𝑑𝑙. The latent
space ˆXcan be regarded as a high dimensional approximation
of the observational space. Therefore, the input initial variable
𝑥∈Xof the physical system can be transformed from physical
spaceXinto a finite-dimensional approximation of observation
space ˆXby acting𝑃on𝑥.
•Calculate the intrinsic dimension of observation space. The
intrinsic dimension means the minimal representation dimension.
In this part, the key is calculating the intrinsic dimension of the
latent space ˆXto find the compact and optimal invariant subspace
of the observation function. The latent space ˆXis a𝑙-dimensionalvector space and any ˆ𝑥∈ˆXsatisfy the linear combination
ˆ𝑥=𝑙∑︁
𝑘=1𝜂𝑘ˆ𝑣𝑘, (11)
where𝜂𝑘denotes coefficient and ˆ𝑣𝑘is the basis vector. Deployed
the Maximum Likelihood Estimate algorithm, intrinsic dimension
𝑚of the latent space can be calculated, which is described in
detail in Sec. 4.1. Having defined a linear map 𝑊:ˆX→T , we get
a spaceT∈R𝑚whose dimension is 𝑚, the intrinsic dimension
of the latent space. Due to T⊆ ˆXand𝑊(ˆ𝑥)∈T,∀ˆ𝑥, the space
Tis invariant-subspace of the latent space ˆ𝑋. We will learn the
finite-dimensional approximation of the operator described the
evolution of physical dynamics in the space T.
•Parameterize the underlying operator. According to ergodic
theory, it’s reasonable that assuming the physical system is er-
godic, which means the state variables x(𝑡)can visit all states
inR𝑑with the time evolution. Therefore, given a specific time
difference𝛿, the operator has the relationship as
L= lim𝑡→∞1
𝑡Z
[0,𝑡)g(x𝑠)−1g(x𝑠+𝛿)d𝑠 (12)
and it fits in both autonomous and non-autonomous systems. By
defining an operator 𝑇:T→T in invariant-subspace, we can
use the operator to learn the intrinsic-dimensional approximation
of the operator
L≃𝑎𝑟𝑔𝑚𝑖𝑛
𝑇∈𝑅𝑚𝑛∑︁
𝑘=1y𝑘
𝑡−𝑇x𝑘
𝑡𝐹(13)
where𝑛is the number of the sequence of initial state sequence of
the system xand its future state sequence yand the observation
function𝑔has been equivalently parameterized by the other
operator described above.
3359Neural Manifold Operators for Learning the Evolution of Physical Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain.
4 Method
In this section, we mainly introduce the structure of the Neural
Manifold Operator, the intrinsic dimension calculation algorithm,
the training strategy, and other details.
4.1 Model Architecture
Figure 2 shows the overall paradigm of NMO, comprising an en-
coder, a time evolution module, and a decoder. To enhance the clar-
ity of the description of the computational details of the NMO, we
focus on the tensor calculation in the whole process of our model in
this section. In the perspective of model, the initial state of sequence
xare encoded as a discrete tensor, denoted by V1:𝑡={V1,···,V𝑡}
and consisting of 𝑡time space and represented by ℎ×𝑤discrete
spatial grid. Similarly, the tensor described future state of sequence
yis denoted by V𝑡+1:𝑡+𝑡′=
V𝑡+1,···,V′
𝑡+𝑡	withℎ×𝑤discrete
spatial grid, and 𝑡′denotes the prediction length. Considering that
some systems are multivariate coupled, these variables are encoded
into the channel dimension 𝑐of the tensor. Therefore, each time
step𝑖of the tensor is expressed as V𝑖∈R𝑐×ℎ×𝑤and the tensor is
denoted by V∈R𝑡×𝑐×ℎ×𝑤with𝑡time length.
Encoder. The encoder is comprised of 𝑁𝑒ConvNormReLU blocks,
designed to capture spatial signals effectively. Starting with Z(0)=
V, and employing an activation function denoted by 𝜎, the process
unfolds as follows:
Z(𝑖)=𝜎
GroupNorm
ReLU
Conv2d
Z(𝑖−1)
,1≤𝑖≤𝑁𝑒,
(14)
whereZ(𝑖−1)andZ(𝑖)denote the input and output of the 𝑖-th block
with the shapes (𝑡,𝑐,ℎ,𝑤 )and(𝑡,ˆ𝑐,ˆℎ,ˆ𝑤), respectively.
Time Evolution Operator. The time evolution module consists
of down-sampling, up-sampling, and spectral domain processing
in the middle. Importantly, the middle module adapts to various
architectures, as confirmed in 5.5. The formulas are shown below.
Z(𝑖)=𝜎
Inception
Z(𝑖−1),params
,1≤𝑖≤𝑁𝑡 (15)
Z(𝑖)=𝜎
Fourier
Z(𝑁𝑡),params
(16)
Z(𝑖)=𝜎
Inception
Z(𝑖−1)⊕skip(𝑖),params
, 𝑁𝑡≥𝑖>1(17)
The dimensions before and after remain unchanged.
Decoder. Finally, our decoder contains 𝑁𝑑unConvNormReLU
blocks to output the final predictions V𝑝𝑟𝑒𝑑 =Z(𝑁𝑒+𝑁𝑡+𝑁𝑑). In
formulation, we can obtain:
Z(𝑖)=𝜎
GroupNorm
ReLU
unConv2d
Z(𝑖−1)
,
𝑁𝑒+𝑁𝑡+ 1≤𝑖≤𝑁𝑒+𝑁𝑡+𝑁𝑑,(18)
whereV𝑝𝑟𝑒𝑑 =V𝑡+1:𝑡+𝑡′=
V𝑡+1,···,V′
𝑡+𝑡	, and the shape is
𝑡×𝑐×ℎ×𝑤. The pseudo-algorithm of the model is shown algorithm 1.
4.2 Intrinsic Dimension Calculation
In this section, we discuss the calculation of the intrinsic dimension
𝑚of the latent space ˆX∈R𝑙by the Maximum Likelihood Estima-
tion (MLE) method [ 29]. In our calculation, we assume that we have
a series of data points ˆ𝑋𝑖in the latent space ˆXwhich are sampledAlgorithm 1 Neural Manifold Operator (NMO)
Require: Sequence of tensors X={V1,...,V𝑡}representing ini-
tial states
Ensure: Predicted future state sequence Y={V𝑡+1,...,V𝑡+𝑡′}
1:Initialize Encoder (E) with 𝑁𝑒ConvNormReLU blocks
2:Initialize Time Evolution Operator (T) with 𝑁𝑡modules includ-
ing Inception and Fourier layers
3:Initialize Decoder (D) with 𝑁𝑑unConvNormReLU blocks
4:Z(0)←V
5:for𝑖= 1to𝑁𝑒do
6:Z(𝑖)←𝜎(GroupNorm(ReLU(Conv2d(Z(𝑖−1)))))
7:end for
8:for𝑖= 1to𝑁𝑡do
9:Z(𝑖)←𝜎(Inception(Z(𝑖−1),params))
10: if𝑖=𝑁𝑡then
11:Z(𝑖)←𝜎(Fourier(Z(𝑁𝑡),params))
12: else
13:Z(𝑖)←𝜎(Inception(Z(𝑖−1)⊕skip(𝑖),params))
14: end if
15:end for
16:for𝑖=𝑁𝑒+𝑁𝑡+ 1to𝑁𝑒+𝑁𝑡+𝑁𝑑do
17:Z(𝑖)←𝜎(GroupNorm(ReLU(unConv2d(Z(𝑖−1)))))
18:end for
19:Y←Z(𝑁𝑒+𝑁𝑡+𝑁𝑑)
independently. And we use 𝑇𝑖∈ˆTto denote corresponding points
in a space ˆTwith intrinsic dimension 𝑚, satisfying ˆ𝑋𝑖=𝑀(𝑇𝑖).
The space ˆTshould satisfy that the unknown probability density
function𝑓(·)which generates our dataset, i.e. the probability den-
sity of𝑇𝑖and also the density by which ˆ𝑋𝑖=𝑀(𝑇𝑖)are generated
consequently, is continuous and single-valued on it. 𝑀(·)should
be a injective mapping and in this case 𝑇𝑖are identically and inde-
pendently distributed. Noticing that the space Tand the mapping
𝑊+(·)is in fact an example of ˆTand𝑀but they are not necessary
the same. Although we use linear mapping 𝑊(·)in our neural net-
works, in our theory of calculating the intrinsic dimension 𝑚,𝑀(·)
could be non-linear.
We first calculate the intrinsic dimension based on different ˆ𝑋𝑖=
𝑀(𝑇𝑖)and its k-nearest neighbors denoted by ˆ𝑋(𝑗)
𝑖=𝑀(𝑇(𝑗)
𝑖),1≤
𝑗≤𝑘. The number 𝑘needs to be large enough for statistical esti-
mation while small enough to make the difference between 𝑓(𝑇𝑖)
and𝑓(𝑇(𝑗)
𝑖)insignificant. Assuming 𝑓(𝑇𝑖) =𝑓(𝑇(𝑗)
𝑖),∀1≤𝑗≤𝑘, we
can regard the number of data points inside a sphere centered at 𝑇𝑖
with radius𝑟, which we denote as 𝑁(𝑟,𝑇𝑖)or𝑁(𝑟)for simplicity, as
an inhomogeneous Poisson random process of which the sphere
radius𝑟is the variable. And the rate 𝜆(𝑟)is
𝜆(𝑟) =𝑓(𝑇𝑖)𝑉(𝑚)𝑚𝑟𝑚−1, (19)
where𝑉(𝑚)is the volume of the unit sphere in m-dimensional
Euclidean space and 𝑉(𝑚)𝑚𝑟𝑚−1is the superficial area of the sphere
with radius 𝑟. Letting𝜃=log𝑓(𝑇𝑖)and𝑟(𝑗)
𝑖be the distance from 𝑇𝑖
to𝑇(𝑗)
𝑖, then the log-likelihood [ 46] of the Poisson random process
3360KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Wu, Kangyu Weng, Shuyi Zhou, Xiaomeng Huang, and Wei Xiong
should be
𝐿(𝑚,𝜃) =Z𝑟(𝑘)
𝑖
0log𝜆(𝑟)𝑑𝑁(𝑟)−Z𝑟(𝑘)
𝑖
0𝜆(𝑟)𝑑𝑟. (20)
For the maximal log-likelihood, there should be𝜕𝐿
𝜕𝜃= 0,𝜕𝐿
𝜕𝑚= 0.
Then it can be derived that
1
𝑚(𝑖,𝑘)=1
𝑘 
−Z𝑟(𝑘)
𝑖
0log𝑟d𝑁(𝑟) +𝑘log𝑟(𝑘)
𝑖!
, (21)
where𝑚(𝑖,𝑘)is calculated based on ˆ𝑋𝑖=𝑀(𝑇𝑖)and its k-nearest
neighbors ˆ𝑋(𝑗)
𝑖=𝑀(𝑇(𝑗)
𝑖),1≤𝑗≤𝑘. Noticing𝑁(𝑟) =P𝑘
𝑗=1𝛿(𝑟−𝑟(𝑗)
𝑖)
we have
1
𝑚(𝑖,𝑘)=1
𝑘 
−𝑘∑︁
𝑗=1log𝑟(𝑗)
𝑖+𝑘log𝑟(𝑘)
𝑖!
=1
𝑘𝑘∑︁
𝑗=1log𝑟(𝑘)
𝑖
𝑟(𝑗)
𝑖.(22)
According to Elizaveta Levina and Peter Bickel’s research [ 29],
dividing by𝑘−2makes the estimator asymptotically unbiased. We
take the average 𝑚(𝑖,𝑘)as the estimation of intrinsic dimension,
then the final equation should be
𝑚=1
𝑁𝑁∑︁
𝑖=1 
1
𝑘−2𝑘∑︁
𝑗=1log𝑟(𝑘)
𝑖
𝑟(𝑗)
𝑖!−1
. (23)
4.3 Training Strategy
The training of the NMO is divided into two stages. In the first
stage, the encoder 𝑃and decoder 𝑄are trained by a self-supervised
reconstruction strategy. The initial states of the physical variables
of training samples V1:𝑡={V1,···,V𝑡}are used to calculate the
reconstruction and minimize the reconstruction loss
𝐿1=||𝑄(𝑃(V1:𝑡))−V1:𝑡||2. (24)
When the reconstruction loss function converges to a minimum
value, the high-dimensional latent space constructed by the encoder
𝑃and decoder 𝑄will converge to the target observation space 𝑔.
Having finished the first stage of training, the parameters of 𝑃and
𝑄will be frozen. In the second stage, time evolution operator 𝑇
is trained by supervised learning and we use the future state of
physical variables yis denoted by V𝑡+1:𝑡+𝑡′=
V𝑡+1,···,V′
𝑡+𝑡	
as
label to calculate the prediction loss
𝐿2=||𝐺𝜃(V1:𝑡)−V𝑡+1:𝑡+𝑡′||2, (25)
where the time evolution operator 𝑇is the only trainable structure
of the𝐺𝜃in this stage.
5 Experiments
5.1 Benchmarks and Baselines
5.1.1 Benchmarks. As shown in Table 1, we use real-world scenar-
ios and equation-governed scenarios to evaluate our model which
includes 7 datasets. Here are the descriptions of these datasets.
•SEVIR [51] includes satellite and radar weather data, which
we use to evaluate our model’s accuracy in forecasting weather
events like thunderstorms and intense precipitation.
•Kurushio is a strong western boundary current, which is a chal-
lenge for Earth system modeling and prediction. The Kuroshio
stream dataset is the vector data of sea surface stream velocity
from the Copernicus Marine Environment Monitoring Service.•Typhoon [4] is a three-layer water vapor channel dataset cover-
ing the East and Southeast Asian Pacific coastal regions. We use
it to test the model’s ability to predict water vapor distribution
in the next 36 hours, thereby achieving typhoon forecasting.
•Navier-Stokes equation [30] describes the dynamics and mass
transport of the general fluid. We select the two-dimensional
equations for an incompressible viscous fluid with a viscosity
coefficient of 10−5to test our model for learning complicated
fluid dynamics with high Reynolds numbers.
•Shallow-Water equations [47] describes the fluid in the shal-
low water approximation and barotropic system, which is often
used for large-scale geophysical flows and tsunami simulations.
The dataset is well-suited for testing the performance in mass
conservation and long-term prediction.
•Rayleigh-Bénard convection [9,55] describes the turbulent
flow that arises from convection induced by bottom heating,
which is the main mechanics of the El Niño and Southern Oscilla-
tion. The dataset is simulated by the Lattice Boltzmann Method,
which is appropriate for testing the ability of our model to learn
turbulence and energy conservation.
•Diffusion-Reaction equation [47] models the interplay be-
tween the diffusion of substances and their chemical reactions,
often used to describe processes in materials, biology, and the
environment. The dataset, calculated by the standard finite vol-
ume solver, is a challenging benchmark due to there are two
non-linearly coupled variables, the activator and the inhibitor.
In this section, we present empirical results to demonstrate the
effectiveness of NMO. The experiments aim to investigate the fol-
lowing research questions:
•RQ 1. Does NMO consistently lead in performance across all
architectures?
•RQ 2. Can NMO effectively learn physical consistency?
•RQ 3. Where does the gain in intrinsic dimensions lie?
5.1.2 Baseline. Several advanced and representative models in
computer vision, time series prediction, neural operator and partial
differential equations solving, are used for evaluating our model. U-
Net [ 41], Residual Networks (ResNet) [ 18] and Swin-Transformer
(Swin) [ 34] are representative and mainstream computer vision
backbone models, which is often used for various tasks. SimVP-
v2 [48], PredRNN-V2 [ 59] are representative general models for time
series prediction. EarthFormer [ 16] is designed for the time series of
the Earth system. Fourier Neural Operator (FNO) [ 30] is one of the
most representative neural operator models designed for learning
mapping between Banach space. Turbulence-Flow Net (TF-Net) [ 55]
and Latent Spectral Models (LSM) [ 60] are advanced physics-guided
models incorporating physics knowledge into inductive bias.
5.2 Metrics
Statistical metrics allow us to measure model performance at the
pixel level. However, to assess if the model truly captures physical
properties instead of merely achieving local fitting, introducing
physical metrics is essential. Therefore, we use the following metrics
for evaluation: apart from the Root Mean Square Error (RMSE) as a
statistical metric, all others are physical metrics.
3361Neural Manifold Operators for Learning the Evolution of Physical Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 1: Performance comparison with 9 baseline models in all scenarios. RMSE is used for the evaluation of these models, with a smaller
RMSE value indicating greater accuracy. Since FNO is designed for single variables prediction, we only evaluate these models in single variables
scenarios to ensure optimal performance of the baseline models. The underline indicates the most accurate result in baseline models. The bold
font indicates the most accurate of all models. The asterisk (*) denotes GPU memory overflow (exceeding 40GB). The forward slash (/) indicates
that the original model is only designed for single variable prediction.
ModelReal-
world scenarios Eqation-governed scenarios
SEVIR
kuroshio TyphoonNa
vier
StokesShallow
WaterRayleigh-BénardconvectionDiffusionReaction
U-Net 2.0280
0.0591 0.0546 0.4451
0.0890 0.3977 0.0612
ResNet 2.0787
0.0709 0.1246 0.5246
0.0730 0.5746 0.0820
PredRNN-V2 1.9741
0.0651 0.0234 0.5196
0.0970 2.2965 0.1201
Swin-Transformer 2.0067
0.1682 0.0273 0.4741
0.0434 1.6852 *
SimVP-V2 0.7943
0.0658 0.0193 0.3872
0.0098 2.3804 0.0043
Earthformer 0.2877 0.1612
0.0671 0.4472
* 1.5746 *
TF-Net 2.1946
0.1033 0.0172 0.4243
0.0860 0.2076 0.0037
FNO 1.0099
/ / 0.2547 0.0045 /
0.0008
LSM 1.2569
/ / 0.2863
0.0087 / 0.0009
NMO 0.1698
0.0404 0.0161 0.2487
0.0028 0.1418 0.0007
Promotion 41.01%
31.64% 6.40% 2.35%
37.78% 31.74% 12.5%
NMONMOEarthFormer
SimVP-v2FNOLSMPredRNN-v2SwinTU-NetResNetTF-NetParams (MB)
Params (MB)00
Figur
e 3: The training time and RMSE performance rankings of various models on SEVIR and Navier-Stokes equation scenario.
Table 2: Study on time evolution operator’s dimensions, evaluating three implementations: Multi-Layer Perceptron (MLP), Convolutional
Neural Network (CNN), and Transformer structure. Budget data reveals optimal model parameters (PARAM), floating point operations (FLOPs),
and training time (TIME) for each. Underlines highlight top accuracy per implementation, while bold text signifies each scenario’s ID.
DimensionK
UROSHIO Budget Navier Stokes Budget
MLP
CNN Former Param
FLOPs Time MLP
CNN Former Param
FLOPs Time
2 0.0430
0.0466 0.0443 1.5414
4.9180 49.3971 0.2557
0.2631 0.2631 5.2712
2.4427 8.0935
4 0.0429
0.0469 0.0461 1.6922
5.0717 55.4782 0.2547 0.2487 0.2498 5.5496
2.5147 8.4504
6 0.0421 0.0404 0.0427 1.8568
5.2375 64.8712 0.2605
0.2593 0.2528 5.8418
2.5898 9.6932
8 0.0436
0.0471 0.0482 2.0353
5.4154 68.3761 0.2607
0.2637 0.2566 6.1478
2.6678 9.8732
16 0.0431
0.0484 0.0495 2.8870
6.2474 78.6860 0.2638
0.2677 0.2607 7.5098
3.0102 9.9510
32 0.0442
0.0511 0.0524 3.9423
7.4721 98.3267 0.3021
0.2655 0.2799 8.3212
3.6217 10.2132
64 0.0477
0.0490 0.0497 5.7864
8.4464 111.8921 0.2972
0.2802 0.3021 9.1213
4.2173 13.3530
•Root Mean Square Error(RMSE) is a widely accepted metric
for quantifying the statistical performance of the deep learning
model, which can reflect the average error of the prediction.•Mass Conservation. For incompressible shallow water wave
equation with free surface and closed boundary, the prediction
3362KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Wu, Kangyu Weng, Shuyi Zhou, Xiaomeng Huang, and Wei Xiong
8.78.88.748.768.788.72x 103Prediction StepRelative Mass ErrorPrediction Step
Wave NumberWave NumberEnergy SpectrumEnergy Spectrum
Mean Absolute Divergence601
Prediction StepRMSE1TargetNMOFNOLSMSimVP-V2SwinTResNetTF-NetU-NetPredRNN-V2
0
Figure 4: Left: Relative mass error at each time step and visualization of prediction results of each model on the Shallow-Water equations
scenario. Mid: Turbulence energy spectrum on the Rayleigh-Bénard convection scenario. Right: The average of absolute divergence convection
at each time step and RMSE associated with the prediction step of each model on the Rayleigh-Bénard convection scenario.
11111
1
Figure 5: The prediction performance of various dimensions of the time evolution operator. The dotted lines represent the ID in each scenario.
variableℎnot only describes the depth of water but also is pro-
portional to the mass of the water column. Therefore, the total
mass of the system can be calculated by the variable ℎto evaluate
whether the models preserve first-order conserved quantities.
The mass conservation formula of the 2-dimensional shallow
water equations can be expressed as
𝑑
𝑑𝑡∬
𝐷ℎ𝑑𝑥𝑑𝑦 = 0, (26)
where𝐷is the computational domain of the equations in 2-
dimensional Euclidean space R2.
•Energy Conservation. For the Rayleigh-Bénard convection
scenario, the turbulence energy spectrum indicates the kinetic
energy contained in eddies with wavenumber 𝑘. The turbulence
energy spectrum is calculated by mean turbulence kinetic energyafter Fourier transformation. The metric is appropriate for an-
alyzing energy consistency in different ranges of wavenumber.
The energy spectral 𝐸(𝑘)is calculated by
Z∞
0𝐸(𝑘)𝑑𝑘=1
2𝑇𝑇∑︁
𝑡=0[(𝑢𝑥(𝑡)−¯𝑢𝑥)2+ (𝑢𝑦(𝑡)−¯𝑢𝑦)2], (27)
where𝑢𝑥and𝑢𝑦are the components of velocity for the x-axis
and y-axis, the bar symbol means time average, 𝑡is the time step
and𝑇denotes the prediction length.
•Divergence. Derived by the continuity equation, the divergence
of velocity∇·ushould be zero for the incompressible fluid parcel,
which is the closure condition and fundamental constraint of
mass conservation in fluid dynamics. Calculating the average of
absolute divergence in the whole fluid field as a physical metric
at each time step indicates whether the model learns the intrinsic
3363Neural Manifold Operators for Learning the Evolution of Physical Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain.
Figure 6: Visualization on the 3D GS reaction-diffusion system.
dynamics of fluid transportation. The divergence formula can be
expressed as
∇·u=𝜕𝑢𝑥
𝜕𝑥+𝜕𝑢𝑦
𝜕𝑦= 0, (28)
where u={𝑢𝑥,𝑢𝑦}is a 2-dimensional velocity vector.
5.3 Overall Performance (RQ1)
We summarize the results in Table 1, Figure 3 and Figure 6, from
the experimental results, We make the following Observations:
Obs 1. NMO consistently leads in all baseline fields. NMO
shows significant advantages over baseline models in multiple fields,
with clear proof of performance improvement across various ap-
plication scenarios. In computer vision, NMO reduces the RMSE
value by 31.64% in the Kuroshio scenario compared to U-Net, high-
lighting its significant advantage in processing complex data and
accurate image analysis. In time series forecasting, NMO improves
performance by 41.01% in the SEVIR scenario compared to Earth-
former, showing its excellent capability in handling dynamic time
series data. In addressing partial differential equations, NMO is
2.35% better than FNO in processing the Navier-Stokes equations,
proving its powerful in solving complex physical problems.
Obs 2. NMO demonstrates its effectiveness in real-world sce-
narios and controlled equation scenarios. It significantly out-
performs the Earthformer baseline in the SEVIR dataset with a
41.01% performance increase and leads in accuracy for Kuroshio
and Typhoon forecasts. In equation-controlled environments, NMO
excels in solving the Navier-Stokes and Shallow Water equations,
surpassing the FNO model by 2.35% in the former and achieving a
37.78% improvement in the latter.
Obs 3. NMO excels in both performance and efficiency NMO
stands out for its leading performance in SEVIR and Navier-Stokes
equation scenarios, with significantly shorter training times than
competitors. In the SEVIR scenario, NMO is the top performer, re-
quiring only about 20 seconds per epoch for training, much less than
EarthFormer’s 60+ seconds. Additionally, NMO is more efficient,
using under 50MB in parameters.
Obs 4. NMO handles high-dimensional problems We use the
3D GS reaction-diffusion system dataset [ 39], inputting 10 frames
to predict 10 frames. We conduct experiments with 2400 trainingsamples, 300 validation samples, and 300 test samples. For the U
and V components, we train for 500 epochs each, achieving MSE of
0.000044 and 0.000130, respectively. Visualization results in Figure 6
show that NMO effectively handles higher-dimensional data.
5.4 Analysis of Physical Consistency (RQ2)
We explore and analyze the significant advantages of NMO in en-
suring physical consistency during the learning process. The results
are shown in the figure 4 and the observations are as follows:
Obs 5. NMO shows clear advantages in learning physical
consistency. NMO shows significant advantages in three physical
metrics. First, in the Shallow-Water equation scenario, the NMO
model learns the physical property of mass conservation. Its relative
mass error at each prediction step is much better than other models,
explaining its excellent performance in long-term predictions. Sec-
ond, in the Rayleigh-Bénard convection scenario, NMO’s energy
spectrum closely matches the true target, showing its outstand-
ing ability to predict turbulent energy distribution. Notably, NMO
outperforms TF-Net in maintaining zero absolute divergence, even
though TF-Net has explicit hard constraints. Additionally, NMO’s
predictions visually align closely with the targets, further proving
its physical consistency and predictive accuracy.
5.5 Analysis of Intrinsic Dimension(RQ3)
Obs 6. Intrinsic dimension calculated by our paradigm is op-
timal. The results, as shown in Table 2 and Figure 5, highlight that
selecting the intrinsic dimension significantly enhances prediction
accuracy and computational efficiency. Observations show that the
intrinsic dimension calculated by NMO consistently yields the best
performance across different model implementations. For instance,
in the Kuroshio scenario, an intrinsic dimension of 6 provides the
optimal performance for all three types of time evolution operators.
This indicates that the intrinsic dimension effectively reduces the
number of model parameters and computational resources while
maintaining or improving accuracy. Furthermore, models with in-
trinsic dimensions calculated by NMO exhibit better adherence to
physical laws, such as mass conservation and energy conservation,
compared to those with arbitrary dimensions. In summary, intrinsic
dimension optimizes computational efficiency and ensures faithful
representation of physical dynamics.
6 Conclusion
In this paper, we design a new operator learning paradigm with
three implementations for learning the evolution of physical dy-
namics in intrinsic dimension. Incorporating the manifold learning
algorithm, our paper mathematically and experimentally answers
how to parameterize infinite-dimensional operators by a finite-
dimensional parameter space. In the future, we will further explore
the generalization and physics-preserving capability of our para-
digm and its multi-disciplinary applications.
7 Acknowledge
This work is supported by the National Natural Science Founda-
tion of China (42125503) and the National Key Research and De-
velopment Program of China (2022YFE0195900, 2021YFC3101600,
2020YFA0608000, 2020YFA0607900)
3364KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Wu, Kangyu Weng, Shuyi Zhou, Xiaomeng Huang, and Wei Xiong
References
[1]Zhaojun Bai, Patrick M Dewilde, and Roland W Freund. 2005. Reduced-order
modeling. Handbook of numerical analysis 13 (2005), 825–895.
[2]Peter Bauer, Alan Thorpe, and Gilbert Brunet. 2015. The quiet revolution of
numerical weather prediction. Nature 525, 7567 (2015), 47–55.
[3]Edward A Bender. 2000. An introduction to mathematical modeling. Courier
Corporation.
[4]Kotaro Bessho, Kenji Date, Masahiro Hayashi, Akio Ikeda, Takahito Imai,
Hidekazu Inoue, Yukihiro Kumagai, Takuya Miyakawa, Hidehiko Murata, Tomoo
Ohno, et al .2016. An introduction to Himawari-8/9—Japan’s new-generation
geostationary meteorological satellites. Journal of the Meteorological Society of
Japan. Ser. II 94, 2 (2016), 151–183.
[5]Johannes Brandstetter, Max Welling, and Daniel E Worrall. 2022. Lie point
symmetry data augmentation for neural pde solvers. In International Conference
on Machine Learning. PMLR, 2241–2256.
[6]Steven L Brunton, Bingni W Brunton, Joshua L Proctor, and J Nathan Kutz.
2016. Koopman invariant subspaces and finite linear representations of nonlinear
dynamical systems for control. PloS one 11, 2 (2016), e0150171.
[7]Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. 2019.
Data-driven discovery of coordinates and governing equations. Proceedings of
the National Academy of Sciences 116, 45 (2019), 22445–22451.
[8]Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. 2018.
Neural ordinary differential equations. Advances in neural information processing
systems 31 (2018).
[9]Dragos Bogdan Chirila. 2018. Towards lattice Boltzmann models for climate
sciences: The GeLB programming language with applications. Ph. D. Dissertation.
Universität Bremen.
[10] George Cybenko. 1989. Approximation by superpositions of a sigmoidal function.
Mathematics of control, signals and systems 2, 4 (1989), 303–314.
[11] Emmanuel De Bézenac, Arthur Pajot, and Patrick Gallinari. 2019. Deep learning
for physical processes: Incorporating prior scientific knowledge. Journal of
Statistical Mechanics: Theory and Experiment 2019, 12 (2019), 124009.
[12] Nima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, and Rose Yu. 2021.
Automatic symmetry discovery with lie algebra convolutional network. Advances
in Neural Information Processing Systems 34 (2021), 2503–2515.
[13] N Benjamin Erichson, Michael Muehlebach, and Michael W Mahoney. 2019.
Physics-informed autoencoders for Lyapunov-stable fluid flow prediction. arXiv
preprint arXiv:1905.10866 (2019).
[14] Daniel Floryan and Michael D Graham. 2022. Data-driven discovery of intrinsic
dynamics. Nature Machine Intelligence (2022), 1–8.
[15] Stefania Fresca, Luca Dede’, and Andrea Manzoni. 2021. A comprehensive deep
learning-based approach to reduced order modeling of nonlinear time-dependent
parametrized PDEs. Journal of Scientific Computing 87 (2021), 1–36.
[16] Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu, Yuyang Bernie Wang, Mu Li, and
Dit-Yan Yeung. 2022. Earthformer: Exploring space-time transformers for earth
system forecasting. Advances in Neural Information Processing Systems 35 (2022),
25390–25403.
[17] Jan E Gerken, Jimmy Aronsson, Oscar Carlsson, Hampus Linander, Fredrik Ohls-
son, Christoffer Petersson, and Daniel Persson. 2021. Geometric deep learning
and equivariant neural networks. arXiv preprint arXiv:2105.13926 (2021).
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[19] Marvin Höge, Andreas Scheidegger, Marco Baity-Jesi, Carlo Albert, and Fabrizio
Fenicia. 2022. Improving hydrologic models for predictions and process under-
standing using neural ODEs. Hydrology and Earth System Sciences 26, 19 (2022),
5085–5102.
[20] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feed-
forward networks are universal approximators. Neural networks 2, 5 (1989),
359–366.
[21] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan
Wang, and Liu Yang. 2021. Physics-informed machine learning. Nature Reviews
Physics 3, 6 (2021), 422–440.
[22] Mostafa Kiani Shahvandi, Matthias Schartner, and Benedikt Soja. 2022. Neural
ODE Differential Learning and Its Application in Polar Motion Prediction. Journal
of Geophysical Research: Solid Earth 127, 11 (2022), e2022JB024775.
[23] Tom Kibble and Frank H Berkshire. 2004. Classical mechanics. world scientific
publishing company.
[24] Bernard O Koopman. 1931. Hamiltonian systems and transformation in Hilbert
space. Proceedings of the National Academy of Sciences 17, 5 (1931), 315–318.
[25] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik
Bhattacharya, Andrew Stuart, and Anima Anandkumar. 2021. Neural operator:
Learning maps between function spaces. arXiv preprint arXiv:2108.08481 (2021).
[26] Nikola B Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik
Bhattacharya, Andrew M Stuart, and Anima Anandkumar. 2023. Neural Operator:
Learning Maps Between Function Spaces With Applications to PDEs. J. Mach.
Learn. Res. 24, 89 (2023), 1–97.[27] Samuel Lanthaler. 2023. Operator learning with PCA-Net: upper and lower
complexity bounds. arXiv preprint arXiv:2303.16317 (2023).
[28] Kookjin Lee and Kevin T Carlberg. 2020. Model reduction of dynamical systems
on nonlinear manifolds using deep convolutional autoencoders. J. Comput. Phys.
404 (2020), 108973.
[29] Elizaveta Levina and Peter Bickel. 2004. Maximum likelihood estimation of
intrinsic dimension. Advances in neural information processing systems 17 (2004).
[30] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik
Bhattacharya, Andrew Stuart, and Anima Anandkumar. 2020. Fourier neural oper-
ator for parametric partial differential equations. arXiv preprint arXiv:2010.08895
(2020).
[31] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhat-
tacharya, Andrew Stuart, and Anima Anandkumar. 2020. Neural operator: Graph
kernel network for partial differential equations. arXiv preprint arXiv:2003.03485
(2020).
[32] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew
Stuart, Kaushik Bhattacharya, and Anima Anandkumar. 2020. Multipole graph
neural operator for parametric partial differential equations. Advances in Neural
Information Processing Systems 33 (2020), 6755–6766.
[33] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede
Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. 2021. Physics-informed
neural operator for learning partial differential equations. arXiv preprint
arXiv:2111.03794 (2021).
[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer us-
ing shifted windows. In Proceedings of the IEEE/CVF international conference on
computer vision. 10012–10022.
[35] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. 2018. Pde-net: Learning
pdes from data. In International conference on machine learning. PMLR, 3208–
3216.
[36] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karni-
adakis. 2021. Learning nonlinear operators via DeepONet based on the universal
approximation theorem of operators. Nature machine intelligence 3, 3 (2021),
218–229.
[37] Bethany Lusch, J Nathan Kutz, and Steven L Brunton. 2018. Deep learning for
universal linear embeddings of nonlinear dynamics. Nature communications 9, 1
(2018), 4950.
[38] Viraj Mehta, Ian Char, Willie Neiswanger, Youngseog Chung, Andrew Nelson,
Mark Boyer, Egemen Kolemen, and Jeff Schneider. 2021. Neural dynamical
systems: Balancing structure and flexibility in physical prediction. In 2021 60th
IEEE Conference on Decision and Control (CDC). IEEE, 3735–3742.
[39] Chengping Rao, Pu Ren, Qi Wang, Oral Buyukozturk, Hao Sun, and Yang Liu.
2023. Encoding physics to learn reaction–diffusion processes. Nature Machine
Intelligence 5, 7 (2023), 765–779.
[40] Franz M Rohrhofer, Stefan Posch, Clemens Gößnitzer, and Bernhard C Geiger.
2022. Understanding the difficulty of training physics-informed neural networks
on dynamical systems. arXiv preprint arXiv:2203.13648 (2022).
[41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolu-
tional networks for biomedical image segmentation. In Medical Image Computing
and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234–241.
[42] Peter J Schmid. 2010. Dynamic mode decomposition of numerical and experi-
mental data. Journal of fluid mechanics 656 (2010), 5–28.
[43] Peter J Schmid. 2022. Dynamic mode decomposition and its variants. Annual
Review of Fluid Mechanics 54 (2022), 225–254.
[44] Jacob Seidman, Georgios Kissas, Paris Perdikaris, and George J Pappas. 2022.
NOMAD: Nonlinear manifold decoders for operator learning. Advances in Neural
Information Processing Systems 35 (2022), 5601–5613.
[45] Parisa Shokouhi, Vikas Kumar, Sumedha Prathipati, Seyyed A Hosseini, Clyde Lee
Giles, and Daniel Kifer. 2021. Physics-informed deep learning for prediction of
CO2 storage site response. Journal of Contaminant Hydrology 241 (2021), 103835.
[46] Donald Lee Snyder. 1975. Random point processes. John Wiley & Sons (1975).
[47] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay,
Francesco Alesiani, Dirk Pflüger, and Mathias Niepert. 2022. PDEBench: An
extensive benchmark for scientific machine learning. Advances in Neural Infor-
mation Processing Systems 35 (2022), 1596–1611.
[48] Cheng Tan, Zhangyang Gao, Siyuan Li, and Stan Z Li. 2022. Simvp: To-
wards simple yet powerful spatiotemporal predictive learning. arXiv preprint
arXiv:2211.12509 (2022).
[49] Roger Temam. 2012. Infinite-dimensional dynamical systems in mechanics and
physics. Vol. 68. Springer Science & Business Media.
[50] Tapas Tripura and Souvik Chakraborty. 2022. Wavelet neural operator: a
neural operator for parametric partial differential equations. arXiv preprint
arXiv:2205.02191 (2022).
[51] Mark Veillette, Siddharth Samsi, and Chris Mattioli. 2020. Sevir: A storm event
imagery dataset for deep learning applications in radar and satellite meteorology.
Advances in Neural Information Processing Systems 33 (2020), 22009–22019.
[52] Soledad Villar, David W Hogg, Kate Storey-Fisher, Weichi Yao, and Ben Blum-
Smith. 2021. Scalars are universal: Equivariant machine learning, structured like
3365Neural Manifold Operators for Learning the Evolution of Physical Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain.
classical physics. Advances in Neural Information Processing Systems 34 (2021),
28848–28863.
[53] Robin Walters, Jinxi Li, and Rose Yu. 2020. Trajectory prediction using equivariant
continuous convolution. arXiv preprint arXiv:2010.11344 (2020).
[54] Jian-Xun Wang, Junji Huang, Lian Duan, and Heng Xiao. 2019. Prediction of
Reynolds stresses in high-Mach-number turbulent boundary layers using physics-
informed machine learning. Theoretical and Computational Fluid Dynamics 33
(2019), 1–19.
[55] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu.
2020. Towards physics-informed deep learning for turbulent flow prediction.
InProceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 1457–1466.
[56] Rui Wang, Robin Walters, and Rose Yu. 2022. Approximately equivariant net-
works for imperfectly symmetric dynamics. In International Conference on Ma-
chine Learning. PMLR, 23078–23091.
[57] Sifan Wang and Paris Perdikaris. 2023. Long-time integration of parametric
evolution equations with physics-informed deeponets. J. Comput. Phys. 475
(2023), 111855.
[58] Sifan Wang, Xinling Yu, and Paris Perdikaris. 2022. When and why PINNs fail to
train: A neural tangent kernel perspective. J. Comput. Phys. 449 (2022), 110768.
[59] Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, S Yu Philip,
and Mingsheng Long. 2022. Predrnn: A recurrent neural network for spatiotem-
poral predictive learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence 45, 2 (2022), 2208–2225.
[60] Haixu Wu, Tengge Hu, Huakun Luo, Jianmin Wang, and Mingsheng Long. 2023.
Solving High-Dimensional PDEs with Latent Spectral Models. In Proceedings ofthe 40th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho,
Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 37417–
37438.
[61] Hao Wu, Yuxuan Liang, Wei Xiong, Zhengyang Zhou, Wei Huang, Shilong
Wang, and Kun Wang. 2024. Earthfarsser: Versatile spatio-temporal dynamical
systems modeling in one model. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 38. 15906–15914.
[62] Hao Wu, Wei Xiong, Fan Xu, Xiao Luo, Chong Chen, Xian-Sheng Hua, and Haixin
Wang. 2023. Pastnet: Introducing physical inductive biases for spatio-temporal
video prediction. arXiv preprint arXiv:2305.11421 (2023).
[63] Wei Xiong, Xiaomeng Huang, Ziyang Zhang, Ruixuan Deng, Pei Sun, and Yang
Tian. 2023. Koopman neural operator as a mesh-free solver of non-linear partial
differential equations. arXiv preprint arXiv:2301.10022 (2023).
[64] Wei Xiong, Muyuan Ma, Xiaomeng Huang, Ziyang Zhang, Pei Sun, and Yang
Tian. 2023. Koopmanlab: machine learning for solving complex physics equations.
APL Machine Learning 1, 3 (2023).
[65] Enoch Yeung, Soumya Kundu, and Nathan Hodas. 2019. Learning deep neural
network representations for Koopman operators of nonlinear dynamical systems.
In2019 American Control Conference (ACC). IEEE, 4832–4839.
[66] Qiming Zhu, Zeliang Liu, and Jinhui Yan. 2021. Machine learning for metal
additive manufacturing: predicting temperature and melt pool fluid dynamics
using physics-informed neural networks. Computational Mechanics 67 (2021),
619–635.
3366