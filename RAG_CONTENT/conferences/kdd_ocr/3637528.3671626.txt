SentHYMNent: An Interpretable and Sentiment-Driven Model for
Algorithmic Melody Harmonization
Stephen Hahn
stephen.hahn@duke.edu
Duke University
Durham, North Carolina, USAJerry Yin
zirui.yin@duke.edu
Duke University
Durham, North Carolina, USARico Zhu
rico.zhu@duke.edu
Duke University
Durham, North Carolina, USA
Weihan Xu
weihan.xu@duke.edu
Duke University
Durham, North Carolina, USAYue Jiang
yue.jiang@duke.edu
Duke University
Durham, North Carolina, USASimon Mak
sm769@duke.edu
Duke University
Durham, North Carolina, USA
Cynthia Rudin
cynthia@cs.duke.edu
Duke University
Durham, North Carolina, USA
Figure 1: Representation of a computer taking sentiment and melody information as input and outputting a novel, sentiment-
driven harmonization. Image made with assistance from Stable Diffusion[31].
Abstract
Music composition and analysis is an inherently creative task, in-
volving a combination of heart and mind. However, the vast ma-
jority of algorithmic music models completely ignore the “heart”
component of music, resulting in output that often lacks the rich
emotional direction found in human-composed music. Models that
try to incorporate musical sentiment rely on a “valence-arousal”
model, which insufficiently characterizes emotion in two dimen-
sions. Furthermore, existing methods typically adopt a black-box,
This work is licensed under a Creative Commons
Attribution-NonCommercial International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671626music agnostic approach, treating music-theoretical and sentimen-
tal understanding as a by-product that can be inferred given suffi-
cient data. In this study, we introduce two major novel elements:
a nuanced mixture-based representation for musical sentiment,
including a web tool to gather data, as well as a sentiment- and
theory-driven harmonization model, SentHYMNent. SentHYMNent
employs a novel Hidden Markov Model based on both key and
chord transitions, as well as sentiment mixtures, to provide a prob-
abilistic framework for learning key modulations and chordal pro-
gressions from a given melodic line and sentiment. Furthermore,
our approach leverages compositional principles, resulting in a sim-
pler model that significantly reduces computational burden and
enhances interpretability compared to current state-of-the-art al-
gorithmic harmonization methods. Importantly, as shown in our
experiments, these improvements do not come at the expense of
harmonization quality. We also provide a web app where users can
upload their own melodies for SentHYMNent to harmonize.
5050
KDD ’24, August 25–29, 2024, Barcelona, Spain. Stephen Hahn et al.
CCS Concepts
•Applied computing →Sound and music computing.
Keywords
Music Harmonization, Interpretable AI, Sentiment, Hidden Markov
Model, Music Theory
ACM Reference Format:
Stephen Hahn, Jerry Yin, Rico Zhu, Weihan Xu, Yue Jiang, Simon Mak,
and Cynthia Rudin. 2024. SentHYMNent: An Interpretable and Sentiment-
Driven Model for Algorithmic Melody Harmonization. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671626
1 Introduction
Algorithmic music generation (AMG) is a quickly growing field
of research with a broad and diverse audience. Among its many
applications, AMG is used to augment video games [ 30], personal
videos [ 5], and live performances [ 37] to avoid using repetitive or
copyrighted background music; it is used to empower those with
little-to-no musical knowledge to be creative, building new music
from simple parameters [ 12] or natural language [ 7]; AMG is also
used as a tool for established composers to inspire and collaborate
on new compositions [33].
This paper is particularly concerned with algorithmic melody
harmonization, a vital and ubiquitous sub-task of AMG in which
the model generates a musically appropriate supporting harmony
to a given melody. As with any other music-composition task, ef-
fective harmonization requires 1) a sense of emotional direction
and cohesion, and 2) a deep understanding of music theory, which
describes the patterns found in the target musical genre.
Despite the fact that music is inherently tied to emotion [ 4,17,
36], the majority of existing methods fail to consider sentiment in
their models, detracting from the structure and drive of the gen-
erated music. We illustrate the importance of emotional direction
in the harmonization task through two harmonizations of a single
melody, shown in Figure 2. The melody is originally accompanied
by the following words in German, “Soll’s ja so sein, Daß Straf’
und Pein...,” which translates to, “If it must be so, that punishment
and pain...”. What makes this such a notable example of harmo-
nization is that, while the words describe suffering and pain, the
melody outlines part of a simple major scale that falls, then rises. In
accordance with the affect of the lyrics, Johann Sebastian Bach har-
monizes the rising major scale with several jarring and unexpected
dissonances (Figure 2a). In particular, the downbeat of m. 2 hosts a
dark modal mixture, the minor “i” harmony, turning the melody’s
G into a dissonant passing tone. Furthermore, where we would
expect a simple return to the major “I” on the final beat, Bach leads
to the melody’s B ♭with a highly dissonant and deceptive “viio7/vi
→vi.” This ending on “vi” creates a more anxious and incomplete
feeling at the end of the phrase. In juxtaposition, we provide an
alternative harmonization in Figure 2b that is much more straight-
forward and simple, expressing a more gentle and relaxed affect.
Note that the harmonization is completely diatonic (i.e., within the
current musical key) and composed mostly of simple triadic har-
monies. Additionally, ending on a major, root position “I” provides asense of closure that the Bach harmonization avoided. Both Bach’s
and our harmonizations are entirely viable, yet they each convey a
completely different sentiment. Therefore, it is vital that sentiment
information be encoded in the harmonization process in order to
produce the desired emotional effect.
(a) Original Bach harmonization
with highly dissonant harmonies
inred.
(b) Simple reharmonization by
the authors with pleasant har-
monies in green.
Figure 2: Comparison of original Bach harmonization (Subfig-
ure 2a, from Cantata BWV 48) and simple reharmonization
(Subfigure 2b) to show the importance of emotional context
when completing the harmonization task.
Existing harmonization algorithms, including those that incorpo-
rate sentiment information, have several key limitations. Because
such methods use complex models for learning chorale features,
they typically do notembed the guiding music-theoretical princi-
ples that underlie composition. The expectation is that such models
will learn these rules from data, but this learning is by no means
perfect, and results often lack the musical coherence present in
human-written harmonizations. By ignoring structure described by
music theory, such harmonization models require a large amount
of training data to learn this structure, not to mention idiosyncratic
characteristics of given composers. For certain use cases, the train-
ing sample size needed for satisfactory model training may not
even be available, resulting in unsatisfactory performance. Even
when such data are available, training the harmonization model
with large datasets can be computationally expensive, error-prone,
and difficult to troubleshoot or tune. On the other hand, by includ-
ing the depth of music-theoretical knowledge, models can be much
faster to train, use fewer parameters leading to interpretable models
less prone to overfitting, produce reliably high quality results, and
avoid copyright concerns.
To address this, we present a new sentiment- and theory-driven
learning model that embeds the process an expert musician may use
for harmonization [ 2] within a probabilistic learning framework.
Generally, this harmonization process typically involves (i) gener-
ating the tonal and harmonic progressions from the given melodic
line and (ii) using the progressions to generate voice-leading for
the remaining voices (known as harmony realization). To mimic
Step (i), our proposed model learns the relationship between the
given melody and local tonalities (or keys) of a melody as well as
the relationship between the melody and the underlying harmonic
progression. Vitally, we further augment the harmonization pro-
cess (step (i)) with a novel encoding for musical sentiment based
on a mixture of emotions. By incorporating musical sentiment, our
model can generate a more directionally driven harmonization in a
way that suits the user’s needs. The tonal and harmonic progres-
sions can then be efficiently inferred via Viterbi decoding [ 39]. Once
5051SentHYMNent: An Interpretable and Sentiment-Driven Model for Algorithmic Melody Harmonization KDD ’24, August 25–29, 2024, Barcelona, Spain.
the “backbone” of the composition – its tonal and harmonic progres-
sions – is generated, we mimic Step (ii) by building a probabilistic
model for harmony realization under the inferred progression.
Our model, which we call SentHYMNent , provides an efficient,
sentiment-infused, theory-guided, interpretable, flexible, and easy-
to-tune approach to the melody harmonization problem. Human
experiments suggest preference for SentHYMNent compared to
the best black-box harmonization methods. Furthermore, Turing
tests suggest a surprisingly good ability to generate convincing
chorales. Finally, as we can interpret the model directly, we are able
to gain insights into music composition that cannot be obtained
using existing methods.
Given that hierarchical harmonic patterns appear in many other
genres of music, our harmonization model naturally extends to
such genres as well. In particular, we show in experiments that
the proposed SentHYMNent approach extends naturally to the
harmonization of modern rock music. We also note that, compared
to many existing methods, because our transition matrices are
interpretable, humans can manually adjust them to their own taste.
This would not be possible with a complex black-box approach.
Put concretely, our contribution to algorithmic, affective harmo-
nization consists of 1) a novel representation of musical sentiment
data, 2) a web tool to collect data in the novel representation, 3) a
dataset of sentiment analysis, 4) a sentiment- and theory-driven
model for determining large- and small-scale harmonic features
from a melody, 5) a novel probabilistic model for harmony real-
ization, and 6) a web application for easy, controllable melody
harmonization. The following subsection (1.1) discusses related
work. Section 2 first describes the necessary musical background
to understand our motivations and outcomes (Section 2.1), then
describes our novel sentiment data representation and collection
(Contributions 1-3, Section 2.2). Section 3 describes our model, start-
ing with the sentiment-informed key-chord hidden Markov model
(Contribution 4, Section 3.1). Details about our probabilistic har-
mony realization model (Contribution 5) may be found in Appendix
A. Our experiments follow in Section 4. Finally we describe our
web application and preliminary user feedback in Section 5.
1.1 Related Work
Existing algorithmic harmonization methods can be categorized
into two groups: those conditioned on sentiment and those that are
not. Furthermore, these groups can be broadly classified into those
based on Markovian models and those based on deep learning. First
we discuss methods that do not include sentiment information. We
then discuss existing work on sentiment-based composition.
Algorithmic Harmonization. Yi and Goldsmith [46] use a factored
Markov decision processes planner to generate chordal progres-
sions based on input melody. However, their resulting harmoniza-
tion (see Figure 3) has serious flaws from a musical perspective.
There are parallel octaves, unusual note doublings, dissonant leaps
in the bass, awkward voice spacing and chord inversions (e.g., end-
ing on𝐼6
4), and retrogressive harmonic progressions, which result in
musically displeasing harmonies. These appear to have been caused
by oversimplification of musical theoretic concepts in their model.
Kaliakatsos-Papakostas and Cambouropoulos [22]introduce a hi-
erarchical modeling approach using a hidden Markov model (HMM),with user-specified fixed-chords as “anchors,” and generation of
chords connecting them. Using a second HMM, they produce the
bass voice given the chordal progression produced by the first model.
Although introducing chord constraints is musically interesting,
the reliance on human experts to manually insert fixed chords can
make the harmonization process less flexible, and relies on skills
the user may not have.
Figure 3: Violations of compositional principles for the
chorale harmonization in Yi and Goldsmith [46].
Allan and Williams [1]propose a more musically agnostic ap-
proach that uses HMMs to generate chordal progressions. A first
HMM is used to generate chord notes and a second HMM adds
non-chord notes decorating the harmonization. One drawback is
its overly large search space, requiring a massive number of hidden
states (over 2,800) for chorale representation. One reason is that, for
this model, hidden states represent the unique sequence of intervals
from the bass note. As such, there are many musically redundant
hidden states that represent the same chord with different transpo-
sitions. In contrast, we adopt a more musically-informed (and much
smaller) state space which captures tonality and chordal structure.
Some notable studies in chorale harmonization use deep learning
methods. HARMONET [ 14] is a hierarchical architecture containing
five neural nets determining chordal progression, chord inversions,
the bass note, and non-chord notes. Two recent approaches using
neural networks are DeepBach and BachBot. DeepBach [ 11] is a
graphical model that generates four-part chorales using recurrent
neural networks, while BachBot [ 25] is an automatic composition
system that uses a deep long short-term memory model. Both ap-
proaches are supposedly agnostic as they “rely on little musical
knowledge. ” Likely because of this, generated harmonizations from
such approaches often violate essential composition guidelines. Fur-
thermore, these deep neural networks require a large training set,
are not interpretable, and are harder to tune than HMMs.
Affective Music Composition and Harmonization. The above ap-
proaches are all non-affective, in that they do not factor emotion
as an explicit parameter. Regarding Affective Algorithmic Music
Composition, many methods take a rule-based approach such as
Williams et al . [43] , Williams et al . [44] , and Davis and Moham-
mad [9], to generate music in real time. Scirea et al . [34] uses a
framework that involves melody generation via genetic algorithm.
Similar to our approach, Martinez et al. [28] uses HMMs to model
each categorical emotion. More recently, deep learning models have
5052KDD ’24, August 25–29, 2024, Barcelona, Spain. Stephen Hahn et al.
been applied to the task (see [ 10], [27], [26], [3], [38], and [ 19]). In
these models, emotion is categorized discretely (e.g. positive or
negative, or happy, sad, angry etc.); placed on a two-dimensional
valence-arousal plane, then discretized into sections for process-
ing; or determined based on concurrent lyrics using a pretrained
language model. Such classifications of emotion cannot adequately
describe the range of emotion in music. Therefore, we propose the
use of a mixture of emotions, which allows for complex combina-
tions of emotions while remaining highly interpretable.
As for Affective Harmonization, which is our focus here, recent
works have mostly focused on deep learning approaches. Takahashi
and Barthet [35]uses an encoder-decoder based architecture, apply-
ing both a bi-directional LSTM and transformer as an encoder, with
a LSTM-based decoder. Wang et al . [40] implements a VAE based
on CNNs and LSTMs to generate piano accompaniments. EmoMu-
sicTV, proposed by Ji and Yang [18], uses a modified transformer
in a Conditional VAE, where emotion is included as additional la-
tent variable inputs to the decoder; however, emotion is simply
determined based on harmonic modes such as major or minor. This
oversimplification does not accurately capture the music’s emo-
tional content. Wu and Chen [45] introduce a simple probabilistic
method for accompaniment generation. However, such methods all
rely on a valence-arousal plane embedding to represent emotions,
which can be challenging to interpret and limited in the emotions
that it can portray. Our work addresses this limitation by providing
a simple, limitless, and interpretable alternative to the valence-
arousal emotion representation: a continuous-valued mixture of
emotions. Crucially, and in contrast to deep learning methods, our
model is a highly interpretable variation of a Hidden Markov Model
(HMM), conditioned on sentiment information.
2 Preliminaries
2.1 Musical Background
This section can be skipped for readers familiar with music theory.
The SentHYMNent model was conceptualized through the lens
of Western tonal music, which includes genres such as Baroque,
Classical, Jazz, and Rock. We first provide background on the un-
derlying musical structure for SentHYMNent using the four-part
Baroque chorales of J.S. Bach, then show later how similar music
theories can be used for harmonizing melodies of a broad range of
musical genres. It is necessary to understand the following concepts
regarding Baroque chorales in order to understand the motivations
behind our model and to measure the quality of model results. Fig-
ure 4 shows an excerpt of one such Bach chorale. Here, the soprano
(top) voice line in a chorale is the melody of the chorale, often taken
from pre-existing hymn tunes. This melody can be interpreted as
the “horizontal” aspect of a piece of music. On the other hand, har-
mony involves the relationship between the notes of the chorale
sung simultaneously in different voices. In this sense, harmony is
often viewed as the “vertical” aspect of music. Music theorists [see,
e.g.,32] typically classify vertical harmony using Roman numeral
notation (e.g., I, IV, V), with different numerals describing unique
structures. For instance, “I” represents triadic harmony built on the
tonic scale degree (the first note of the scale), while “V7” represents
a “seventh” chord built on the dominant scale degree (the fifth note
of the scale). An example is provided in Figure 4.
Figure 4: Annotated excerpt of J.S. Bach’s chorale harmoniza-
tion, Jesu, deine tiefen Wunden, BWV 194/6
Music of the Baroque era usually follows a harmonic framework
known as the “phrase model” [ 23,42, see Figure 5]. This framework
dictates that harmonies should progress from those functioning
astonic (I, VI, III) to those functioning as predominant (II, IV, VI)
ordominant (V, VII). Predominants lead to dominants, and domi-
nants resolve back to tonics. Within each functional category (tonic,
predominant, or dominant), harmonies tend to progress by root
relations of a descending third or fifth. Harmonic progressions that
go against the phrase model are known as retrogressive and rarely
occur in Baroque music. Chorales that satisfy the phrase model
yield musically cohesive harmonic progressions; those that do not
may sound improperly resolved, and are musically displeasing.
Figure 5: Visualizing the phrase model harmonic framework
[23]. Here, chords (denoted by numerals) may be major, mi-
nor, or diminished. Numerals are also ordered in terms of
relative “strength.” For instance, 𝐼𝐼>𝐼𝑉>𝑉𝐼, where stronger
harmonies generally follow weaker ones within a particular
functional category.
Another important aspect of chorale composition is the relation-
ship between local tonalities. Tonality refers to a musical passage’s
centricity around a single tone, where other tones in the musical en-
vironment are hierarchically related to the central pitch. Temporary
transitions from one tonality (or key) to another, or modulations,
are widely used to provide a more engaging and complex harmonic
structure. Like surface-level harmonic progressions, deeper tonal
shifts in Baroque music also usually follow the phrase model. The
sequence of tonal shifts in a chorale, or its key progression, is inte-
gral to a chorale’s musical signature. Figure 4 shows the modulation
from the tonic key of B-flat major to F major, then back to B-flat
major (I-V-I on a deeper level of musical structure).
Additional guidelines should be followed for pleasing and con-
vincing chorale harmonizations. For instance, there are guidelines
5053SentHYMNent: An Interpretable and Sentiment-Driven Model for Algorithmic Melody Harmonization KDD ’24, August 25–29, 2024, Barcelona, Spain.
regarding chord inversions in the bass voice, and principles prescrib-
ing which chord notes should be doubled or omitted. To ensure a
full sound, the soprano and alto voices and the alto and tenor voices
should stay within an octave of each other. Notes in an upper voice
should be relatively stable with sparing use of melodic leaps. Paral-
lel fifths and octaves are strongly discouraged as they erode voice
independence and are distracting to the listener. Non-harmonic
tones (such as suspensions, passing, and neighboring tones) may
be added to smooth voice leading and add rhythmic diversity (see
Figure 4). As exemplified in Figure 2, sentiment information is also
vital to the transitions of harmonies.
2.2 Novel Sentiment Data Representation and
Collection
As discussed in Section 1.1, current representations for musical
sentiment are lacking in nuance and diversity. As an alternative to
previous models, we propose a simple yet versatile representation
for sentiment data. Furthermore, we present easy-to-use tools for
collection and visualization of the new data representation.
Modeling Musical Sentiment as a Continuous-Valued Mixture of Emo-
tions. Rather than categorical descriptors for emotion, we represent
emotion as a continuous-valued mixture (see example in Figure 6b).
Using a combination of basic emotions (such as joy, love, sadness,
fear and anger), infinite complex emotions may be represented. For
instance, to represent “nostalgia,” previous models would have to
add a new category. On the other hand, we are able to represent
nostalgia as some mixture of joy, love, and sadness. The choice of
basic emotions used for our harmonization model is based on the
primary emotions of Parrott [29]. Though, it is possible to use any
emotion (and any number of emotions) with this representation.
Sentiment Data Collection. To gather music sentiment data, we
created a web tool (Figure 6a) that records user input in real time
(we sample every ∼0.1 seconds in practice). As the user listens
to a piece of music, they choose the emotion that best suits the
music they hear. The emotion samples from multiple listenings
and/or from multiple listeners are combined to create a mixture
of emotions for every section of music. We have also developed
a method to visualize the continuous-valued mixture of emotions
over a custom range of time (Figure 6b). We gather sentiment data
for music that has Roman numeral annotations so we are able to
determine the probability of each harmony given the concurrent
sentiment mixture. To be precise, we determine the exact times that
describe a harmony’s duration, then extract the sentiment mixture
associated with that time frame.
For our implementation, we used the Schubert Winterreise Dataset
(SWD) [ 41], which hosts multiple harmonic analyses, live record-
ings with annotated timing, and score information for each song in
the Schubert song cycle. Note that Schubert’s Winterreise, despite
its differences with Bach’s compositional practice, tends to follow
the phrase model (Figure 5) like Bach’s music and many other styles.
Harmonies in the SWD are encoded as a combination of root and
quality (major, minor, or diminished), which are easily translated
into Roman numeral notation. Using our data collection tool, we
added sentiment analysis to the publicly available recordings within
(a) Tool for real time music sentiment analy-
sis.
(b) Tool for visualiz-
ing collected data.
Figure 6: Screenshots of online web tool for continuous-
valued sentiment data collection and visualization.
the SWD. Chord timings are already provided, so we are able to
determine the corresponding sentiment mixture for each harmony.
Although we are using our tool for the task of symbolic melody
harmonization, we emphasize that it may be used for a variety of
tasks including audio analysis, audio generation, and other sym-
bolic music tasks. We have additionally used our new sentiment
representation for melody generation [13].
3 Methodology
What distinguishes our approach from existing work is the empha-
sis on sentiment-based generation and our unique incorporation of
music-theoretic elements. Our model makes novel use of three vital
elements: key modulation, Roman numeral harmonic notation, and
continuous-valued mixture-based affective data (see Section 2.2).
Key Modulation: Existing methods generally focus only on
chord transitions, with chords assumed to belong to only one key
throughout the entire piece. However, chord transitions also de-
pend on their location with respect to the whole piece and presence
of modulation, and thus existing methods do not capture these
musically important nuances, overlooking the prevalence and im-
portance of key modulation. To produce an authentic-sounding
harmonization, it is crucial to take into account the key transitions
in addition to the chord transitions.
Chord-Equivalence Representation: Many existing methods
treat any concurrent vertical combination of notes as a unique chord.
However, functionally identical chords might occur in different
forms (e.g., inversions, transpositions). To reduce dimensionality of
the emission and transition matrices, we treat these different forms
as one, as is typical in music analysis. We consider only 35 unique
chords in our training set and transpose all chorales to C major
or A minor, greatly reducing search space and training time. We
emphasize the underlying structure (chordal progression and key
modulation) rather than absolute pitches of notes.
3.1 Sentiment-Informed Key-Chord Hidden
Markov Model
The first step in our harmonization framework is to infer plausi-
ble key and chordal progressions given an input melody. This is
achieved by a novel Key-Chord HMM model which integrates tonal,
5054KDD ’24, August 25–29, 2024, Barcelona, Spain. Stephen Hahn et al.
chordal, and sentiment structure to achieve efficient, scalable and
interpretable harmonization with emotional direction.
LetM=(𝑚1,···,𝑚𝑛)be the given melody line, with 𝑚𝑡the
melody note at time 𝑡(i.e., on the 𝑡thbeat). This can be seen as
a sequence of visible states for the HMM. Let K=(𝑘1,···,𝑘𝑛)
be the hidden key progression capturing the modulation of the
chorale, with 𝑘𝑡∈K the key at time 𝑡, whereKis the state space
of 24 keys (12 major, 12 minor). Let C=(𝑐1,···,𝑐𝑛)be its hidden
chordal progression, with 𝑐𝑡∈C the particular chord at time 𝑡,
whereCis the state space of 35 chords. Finally, let S=(s1,···,sn)
be the given sequence of sentiment mixtures. We aim to recover
(ordecode ) the sequences of keys Kand chords Cfrom the melody
Mand corresponding sentiment mixture sequence S.
We build the Key-Chord HMM in two stages, first for the key pro-
gression, then for the chord progression. For the key sequence K, we
impose the following Markovian model on transition probabilities:
P(𝑘𝑡+1|𝑘1,...,𝑘𝑡)=P(𝑘𝑡+1|𝑘𝑡)=:𝑇𝐾
𝑘𝑡,𝑘𝑡+1, (1)
for𝑡=1,···,𝑛. Here,𝑇𝐾
𝑘𝑡,𝑘𝑡+1:=P(𝑘𝑡+1|𝑘𝑡)denotes the transition
probability from key 𝑘𝑡to𝑘𝑡+1, which we estimate using chorale
data. This first-order Markovian assumption is standard for HMMs,
and can be justified by the earlier phrase model chordal structure.
Given key𝑘𝑡, we assume that 𝑚𝑡, the melody note at time 𝑡, depends
only on𝑘𝑡, i.e.:
P(𝑚𝑡|𝑘1,...,𝑘𝑡,𝑚1,...,𝑚𝑡−1)=P(𝑚𝑡|𝑘𝑡)=:𝐸𝐾
𝑘𝑡,𝑚𝑡, (2)
for𝑡=1,···,𝑛. Here,𝐸𝐾
𝑘𝑡,𝑚𝑡denotes the emission probability
of melody note 𝑚𝑡from key𝑘𝑡. This is again a standard HMM
assumption, justifiable by the earlier discussion that the melody line
can be well-characterized by its underlying tonality and harmony.
Next, for the chord sequence C, we presume that the key se-
quence has already been decoded from data (call this inferred se-
quence K∗, more on this in the next subsection). We again adopt a
first-order Markovian model for transition probabilities, with an
added dependence on the sentiment mixture sat time𝑡:
P(𝑐𝑡+1|𝑐1,...,𝑐𝑡,s1,...,st,st+1)=P(𝑐𝑡+1|𝑐𝑡,st+1)
=𝐸∑︁
𝑒=1P(𝑐𝑡+1|𝑐𝑡)𝑒·𝑠𝑒,𝑡+1=:𝑇𝐶
𝑐𝑡,𝑐𝑡+1,st+1,(3)
for𝑡=1,···,𝑛, where𝑒indexes emotions. P(𝑐𝑡+1|𝑐𝑡)𝑒is the
transition probability from 𝑐𝑡to𝑐𝑡+1based on the transition matrix
associated with emotion 𝑒. Weight𝑠𝑒,𝑡+1represents the propor-
tion of emotion 𝑒at time𝑡+1from the emotion mixture st+1.
Here,𝑇𝐶𝑐𝑡,𝑐𝑡+1,st+1:=P(𝑐𝑡+1|𝑐𝑡,st+1)denotes the transition probabil-
ity from chord 𝑐𝑡to𝑐𝑡+1with sentiment mixture st+1, which we
estimate from data. Specifically, we gather each emotion transition
matrix, which describes all possible P(𝑐𝑡+1|𝑐𝑡)𝑒, by parsing all
transitions in the data where two consecutive chords have greater
than a certain threshold proportion 𝜁of the particular emotion.
The key and chord transitions can again be reasoned by the earlier
phrase model described in Section 2.1. Given the inferred key 𝑘∗
𝑡
and chord𝑐𝑡, we then assume that the transposed melody note
𝛿𝑡=𝑚𝑡−𝑘∗
𝑡(i.e., modulo key change) follows the model:
P(𝛿𝑡|𝑐1,...,𝑐𝑡,𝛿1,...,𝛿𝑡−1)=P(𝛿𝑡|𝑐𝑡)=:𝐸𝐶
𝑐𝑡,𝛿𝑡, (4)for𝑡=1,···,𝑛. This leverages the observation that similar har-
monic structures are used over different tonalities. Figure 7 visual-
izes the Key-Chord HMM model: the observed states are the input
soprano melody line and sequence of corresponding sentiment
mixtures, and the hidden states are the underlying keys and chords.
Figure 7: Key-Chord HMM Visualization. Note that the senti-
ments are given, so there is no need for transition informa-
tion between consecutive sentiments.
In practice, both the emission probabilities 𝐸𝐾and𝐸𝐶, as well
as key and chord transition probabilities 𝑇𝐾and𝑇𝐶, must be esti-
mated from training data. We adopt the following hybrid estima-
tion approach. First, to ensure the harmonization does not violate
progressions from the phrase model, we set the probabilities of ret-
rogressive chord transitions (i.e., those violating the phrase model)
to be near zero. The remaining parameters are then estimated from
the training data using maximum likelihood estimation [ 6]. This
ensures our model not only generates musically coherent chordal
progressions in line with compositional principles, but also per-
mits us to learn a composer’s creative style under such constraints.
Our proposed model requires substantially fewer parameters than
existing HMM harmonization models. Specifically [ 1] requires esti-
mation of over 2,8002transition probabilities while ours requires
352·𝑛𝑒, where𝑛𝑒is the number of emotions considered (5 in our
experiments). As shown later, this yields a computationally effi-
cient and interpretable harmonization model, competitive with
state-of-the-art models in terms of harmonization quality.
The Viterbi decoding algorithm [ 39] is a popular dynamic pro-
gramming method for inferring hidden states in HMMs and is
widely used in signal processing, natural language processing [ 21],
and other fields. Here, a two-step implementation of the Viterbi
algorithm allows for efficient inference of the underlying key and
chord sequences.
Given melody M, the key inference problem is formulated as
K∗∈argmax
KP(K|M). (5)
Here, P(K|M)is the posterior probability of a certain key sequence
Kgiven melody line Munder the Key-Chord HMM. This optimiza-
tion, however, involves |K|𝑛variables where 𝑛is the length of the
melody line, which can be high-dimensional. The Viterbi algorithm
provides an efficient way to solve this optimization problem via
dynamic programming. In our implementation, we used the Viterbi
decoding function in the Python package hmmlearn [24]. Similarly,
given melody line Mand and inferred key sequence K∗, the chord
5055SentHYMNent: An Interpretable and Sentiment-Driven Model for Algorithmic Melody Harmonization KDD ’24, August 25–29, 2024, Barcelona, Spain.
Algorithm 1 Key-Chord Viterbi decoding
Viterbi decoding for keys:
•Set𝑉K
0(0)← 1,𝑉K
𝑘(0)← 0for all𝑘∈K.
•For𝑡=0,···,𝑛−1, update for all 𝑘∈K
𝑉K
𝑘(𝑡+1)← max
𝑖∈Kn
𝑉K
𝑖(𝑡)𝐸𝐾
𝑘,𝑚 𝑡+1𝑇𝐾
𝑖,𝑘o
.
•SetK∗as the key sequence achieving max𝑖∈K𝑉K
𝑖(𝑛).
Viterbi decoding for sentiment-infused chords:
•Set𝑉C
0(0)← 1,𝑉C𝑐(0)← 0for all𝑐∈C.
•For𝑡=0,···,𝑛−1, update for all 𝑐∈C
𝑉C
𝑐(𝑡+1)← max
𝑖∈Cn
𝑉C
𝑖(𝑡)𝐸𝐶
𝑐,𝑚 𝑡+1−𝑘∗
𝑡+1𝑇𝐶
𝑖,𝑐,so
.
•SetC∗as the chord sequence achieving max𝑖∈C𝑉C
𝑖(𝑛).
inference problem can be formulated as
C∗∈argmax
CP(C|M−K∗). (6)
This can again be efficiently solved via the Viterbi algorithm, with
the observed states now taken to be the transposed melody M−K∗.
Algorithm 1 outlines this two-stage Viterbi algorithm for infer-
ring the underlying key-chord sequence (K∗,C∗). Here,𝑉𝑋𝑥(𝑡)rep-
resents the most probable state sequence, P(𝑥1,...,𝑥𝑡,𝑚1,...,𝑚𝑡),
where𝑥represents the last particular chord or key in the set of 𝑋
in the sequence.
4 Experiments
The following section describes the data used for our experiments
(Section 4.1), our survey design (Section 4.2), and the experimental
results (Section 4.3). For detailed descriptions of harmony realiza-
tion models used in our experiments, see Appendix A.
4.1 Data
For classical melody harmonization, we use the Bach chorale corpus
and chorale analyses provided in the Music21 toolkit [ 8]. When pro-
cessing chorale data, we experiment with various discretizations
for one time step; specifically, we experimented with sixteenth,
eighth, and quarter note time steps. For our survey experiment, we
use discretizations of a 16th note. At each time step, melody notes
are represented by their corresponding MIDI pitches, chords are
represented using Roman numeral notation, and keys are repre-
sented using letters. We note that the data used to fit SentHYMNent
is different from the data used in past studies. Instead of using
raw note data corresponding directly to pitches, we use the anno-
tated chorales (containing chordal and modulation information, see
Figure 4) which are proofread by music theorists [20].
For drum rhythm generation of popular genres, we used snippets
of midi files from Hirschberg [15]. The data consists of classical rock
transcriptions from The Beach Boys to Led Zeppelin. For simplicity,
we selected a subset of the files with quadruple time (such as 44) that
regularly used instruments in the drum set. The files were processed
to include only the melody and drum parts. The drum model is built
on a Hidden Markov Model, where the hidden states are defined
as the drum rhythms in each measure, and the observed states arewhether there exists a note or rest for more than two beats in the
melody measure for the corresponding melody part. This observed
state representation is based on the assumption that a long rest or
note likely implies the beginning or ending of a phrase. The drum
rhythm is generated probabilistically, weighted on the observed
states in the given melody.
Playlist for experiment audio samples accessed here .
4.2 Melody Harmonization: Survey Design
To assess performance of SentHYMNent’s melody harmonization,
we conducted audience-preference experiments comparing our har-
monizations to those generated by the existing state-of-the-art
algorithmic harmonization models: (1) Huang et al . [16] , (2) Ji and
Yang [18], and (3) Makris et al . [27] . Models that had no available
reproducible code were not considered. We also determined prefer-
ence between SentHYMNent and Bach’s original harmonizations.
Furthermore, we performed a Turing test with each melody harmo-
nization. Lastly, we compared the perceived emotional content of
our harmonizations against the emotional mixture from which they
were generated. Each survey participant evaluated harmonizations
generated for the purpose of each comparison, and also one set of
“sanity-check” questions which compare the original Bach chorale
to one deliberately composed to be unpleasant and dissonant.
When comparing with Huang et al . [16] , Ji and Yang [18], and
Bach, we simply selected an existing chorale melody and used the
soprano voice as the input melody. We also aimed to compared our
method vs. that of Makris et al . [27] regarding ability to success-
fully create harmonizations for a given melody that corresponded
to various sentiments. In order to avoid conflating harmonization
effects from melody effects, the same melody was used for both
approaches. As the Makris et al . [27] method does not allow for
harmonization using user-input melodies, we used melodies gener-
ated from their model in SentHYMNent. An independent sample
of𝑛=27raters selected five melodies, one from each sentiment
tested, that passed criteria for “acceptability,” whereby no more
than 68% identified “weird/unusual components” and more than
68% had a positive reaction to the generated melody. This was nec-
essary because the generated results of Makris’ model were highly
inconsistent in quality and we needed to compare our model with
theirs in such a way that the melodies were not overly distracting.
For all comparisons, we displayed two audio players, each play-
ing the harmonization generated by one of the methods. To account
for order effects, the harmonization generated using SentHYMNent
had a 50% chance of being first or second. Respondents could pause
and replay audio an unlimited number of times. For each pair of
harmonizations we asked the following: 1)“On a scale of 0 (not en-
joyable) to 10 (very enjoyable), how would you rate harmonization
𝑋?”2)“On a scale of 0 (certain it’s by a computer) to 10 (certain
it’s by a human), what is your degree of belief that a human har-
monized melody 𝑋?”3)“Which harmonization do you prefer?” (a)
strongly prefer 1, (b) prefer 1, (c) no clear preference, (d) prefer 2,
(e) strongly prefer 2. 4)“Were there any parts of harmonization 𝑋
that stood out as sounding weird or bad to you?” (yes=1, no=0)
Finally, to test the accuracy of our emotional input, we produced
a range of five harmonizations based on the five main emotions
5056KDD ’24, August 25–29, 2024, Barcelona, Spain. Stephen Hahn et al.
used in our model. We generated each harmonization with a senti-
ment input of 100% sad, angry, neutral, love, and joyful emotions
respectively. “Neutral” is considered 25% of each emotion listed in
this experiment. To compare with the scale used by Makris et al .
[27], we asked each participant the following: “On a scale of 1 (sad
or angry) to 5 (loving or joyful), how do you perceive the emotional
content of harmonization 𝑋?” The slider associated with the ques-
tion was labeled “extremely sad/angry” (1), “somewhat sad/angry”
(2), “neutral” (3), “somewhat loving/joyful” (4), and “extremely lov-
ing/joyful” (5). At the same time, we asked participants a multiple
choice question, “What emotion best describes harmonization 𝑋?”
The choices are based on our major sentiment categories: anger,
fear, sadness, love, and joy. The full survey instrument, reproducible
code, and excerpts are available in the Supplemental Materials.1
We compared mean enjoyability for each competing excerpt
vs.SentHYMNent using a paired t-test. For the Turing test, we
evaluated mean confidence that each excerpt was composed by a
human compared to the actual human-composed excerpt using a
paired t-test. We evaluated whether there was a difference in the
proportion of respondents that identified a “weird or bad” sounding
excerpt for each competing excerpt vs .SentHYMNent using a chi-
square test. When judging Makris et al . [27] ’s model for its ability
to generate music according to its input valence, we assume nor-
mality and find the 95% confidence interval surrounding a melody’s
mean score on the Likert scale. To judge SentHYMNent, we used a
scoring system where matching emotions award 2 points, similar
emotions award 0 points, and opposing emotions subtract 2 points.
Positive scores indicate that SentHYMNent is producing music that
precisely matches its input more often than not. Similar categories
are considered {joy, love} and {anger, fear, sadness}.
We surveyed an independent sample of (n=48) participants. We
rejected any responses that failed the “sanity-check” (i.e., not indi-
cating a strong preference for Bach’s original harmonization over
our deliberately dissonant composition). Our final analysis dataset
consisted of 30 participants. Among our participants, 27 (90%) re-
ported status as music students or professionals and 6 (20%) listened
to music more than 15 hours a week.
4.3 Results
4.3.1 Weirdness. Figure 8 and Table 6 (Appendix C) suggest that
SentHYMNent has significantly lower incidence of “weird or
bad” segments compared to the current state-of-the-art. This
is vital for reliability and consistency. Users can expect our model
to produce high-quality results at a significantly higher rate than
competing models.
Figure 8: Proportion of participants who heard “weird or bad”
segments in each model (lower is better).
1https://github.com/stephenHahn88/SentHYMNent_Supplement/4.3.2 Preference. Figure 9 suggests a general preference for our
method compared to the current state-of-the-art, and demonstrates
non-inferiority to Coconet. Note that our model is far more inter-
pretable and offers the ability to condition on sentiment, allowing
for greater controllability over other models such as Coconet.
Figure 9: Pairwise preferences in model comparisons. Sen-
tHYMNent’s melodies (1) are in blue on the left. Other models
(2) are on the right in purple.
4.3.3 Enjoyability. Table 1 demonstrates sufficient statistical evi-
dence suggesting greater or similar enjoyability scores for our
excerpts compared to the current state-of-the-art automated
harmonization generators.
Table 1: Enjoyability (higher is better).
Method Mean 95% CI p-value
SentHYMNent 5.90 (5.47, 6.32) ref.
Bach 8.00 (7.08, 8.92) <0.001
Coconet 5.18 (3.54, 6.82) 0.221
Makris 4.95 (4.23, 5.66) 0.016
EmoMusicTV 4.00 (2.70, 5.30) <0.001
4.3.4 Turing Test. While we find that no AI model passes the Tur-
ing test, Table 2 shows that SentHYMNent scores higher than others
andscores closer to “human” than “computer” where other
models fail.
Table 2: Confidence of being composed by human (higher is
better).
Method Mean 95% CI p-value
Bach 7.47 (6.59, 8.36) ref.
SentHYMNent 5.14 (4.51, 5.76) <0.001
Coconet 4.64 (2.21, 7.06) 0.008
Makris 3.89 (3.10, 4.69) <0.001
EmoMusicTV 3.42 (2.19, 4.65) <0.001
4.3.5 Emotion Perception. Finally, Table 3 shows how Makris et al .
[27] and SentHYMNent perform given our respective scoring sys-
tems. Makris struggled to generate pieces with the desired emo-
tional content, particularly pieces with positive valence. On the
other hand, SentHYMNent was able to replicate the target emotion
with relative frequency despite the unusual melodies.
2The neutral harmonization was generated as an even mixture of the other 4 emotions.
On the Likert scale, the neutral mixture received a mean score of 0 as desired.
5057SentHYMNent: An Interpretable and Sentiment-Driven Model for Algorithmic Melody Harmonization KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 3: Average emotion scores for SentHYMNent and
Makris’ model. The first column shows ground truth (GT)
valence scores from which pieces were generated from low
(-2) to high (2) valence. For our harmonizations (based on
the same Makris melodies), matches are 2 points, similar
emotions are 0 points, and opposites are -2 points. Blue rep-
resents good results and purple represents poor results.
Makris SentHYMNent (higher is better)
GT Mean 95% CI GT Mean Score
-2 -1.00 (-1.0, -1.0) Sadness 0.33
-1 -0.56 (-1.2, 0.1) Anger -0.67
0 0.00 (-0.9, 0.9) Neutral ___2
1 -0.25 (-0.8, 0.3) Love 0.25
2 0.10 (-0.6, 0.8) Joy 0.40
5 Deployment
We deployed SentHYMNent in the form of a web application acces-
sible to the public.3Through this website, users are able to interact
with our model to harmonize a melody of their choice with their
preferred sentiments. In addition, our website includes the tool for
users to analyze and create sentiment annotations for pieces of
their own selection (see Section 2.2).
The deployed website follows a typical Client-Server model,
with a user interactive front-end interfacing with our model in
the backend via client requests. Our model is accessible through
the backend as an API endpoint of a Python Flask server, which
takes in melodic pitch and rhythm, harmonic rhythm, and desired
sentiment, and returns a fully harmonized piece consisting of Alto,
Tenor, and Bass parts.
The frontend server is principally built via Vue.js, using the
Open Sheet Music Display engine for rendering music notation,
and Tone.js for playback. Upon entering our website, users can
choose from a range of preset pieces or upload their own melodies
in the form of a MusicXML file, to be then passed to our harmoniza-
tion model; they are then prompted to input their desired sentiment
mixture using an interactive bar plot. See Appendix B for screen-
shots and further details.
6 Conclusion
In this study, we described a probabilistic framework capable of gen-
erating musically convincing harmonizations with affect. The main
strength of the model is that it is musically informed, emulating
the harmonization process of a human composer by incorporating
musical guidelines and emotional context. By using professional
analyses instead of raw musical pitches as input data to our Key-
Chord HMM, our method requires considerably fewer hidden states
compared to other HMM approaches (2800 vs.35). The use of HMMs
themselves leads to the generation process being more interpretable
compared to recent deep learning approaches. All together, we have
demonstrated the benefits of considering domain-specific and af-
fective information when designing generative models.
3https://melody.cs.duke.edu:8000/harmonizeReferences
[1]Moray Allan and Christopher K. I. Williams. 2004. Harmonising Chorales by
Probabilistic Inference. In Proceedings of Neural Information Processing Systems.
[2]William G. Andrews and Molly Sclater. 1993. Materials of Western Music: Part 1 .
Gordon V. Thompson Music.
[3]Chunhui Bao and Qianru Sun. 2022. Generating music with emotions. IEEE
Transactions on Multimedia (2022).
[4]Frederick S Barrett, Kevin J Grimm, Richard W Robins, Tim Wildschut, Constan-
tine Sedikides, and Petr Janata. 2010. Music-evoked nostalgia: affect, memory,
and personality. Emotion 10, 3 (2010), 390.
[5]Beatoven.ai. 2023. Beatoven.ai. Available at: https://www.beatoven.ai/. Accessed:
August 31, 2023.
[6]George Casella and Roger L Berger. 2021. Statistical Inference. Cengage Learning.
[7]Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi
Adi, and Alexandre Défossez. 2023. Simple and Controllable Music Generation.
arXiv:2306.05284 [cs.SD]
[8]Michael Scott Cuthbert and Christopher Ariza. 2021. Music21: A Toolkit for
Computer-Aided Musicology and Symbolic Music Data. https://github.com/
cuthbertLab/music21
[9]Hannah Davis and Saif M Mohammad. 2014. Generating music from literature.
arXiv preprint arXiv:1403.2124 (2014).
[10] Lucas N Ferreira and Jim Whitehead. 2021. Learning to generate music with
sentiment. arXiv preprint arXiv:2103.06125 (2021).
[11] Gaëtan Hadjeres, F. Pachet, and F. Nielsen. 2017. DeepBach: a Steerable Model
for Bach Chorales Generation. In Proceedings of the 34th International Conference
on Machine Learning.
[12] Stephen Hahn, Rico Zhu, Simon Mak, Cynthia Rudin, and Yue Jiang. 2023. An
Interpretable, Flexible, and Interactive Probabilistic Framework for Melody Gener-
ation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (Long Beach, CA, USA) (KDD ’23). Association for Computing
Machinery, New York, NY, USA, 4089–4099. https://doi.org/10.1145/3580305.
3599772
[13] Stephen Hahn, Rico Zhu, Jerry Yin, Simon Mak, Yue Jiang, and Cynthia Rudin.
2023. New Orleans: An Adventure in Music. https://youtu.be/AUqps10--U8?si=
8alrt0aIiaEMHdLH.
[14] H. Hild, J. Feulner, and W. Menzel. 1991. HARMONET: A Neural Net for Harmo-
nizing Chorales in the Style of J. S. Bach. In Proceedings of Neural Information
Processing Systems.
[15] Dan Hirschberg. 2009. Best Classical Rock Midi. https://ics.uci.edu/~dan/midi/
rock/index.html.
[16] Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, and
Douglas Eck. 2017. Counterpoint by Convolution. In International Society for
Music Information Retrieval (ISMIR).
[17] Ray Jackendoff and Fred Lerdahl. 2006. The capacity for music: What is it, and
what’s special about it? Cognition 100, 1 (2006), 33–72.
[18] Shulei Ji and Xinyu Yang. 2021. EmoMusicTV: Emotion-conditioned Symbolic
Music Generation with Hierarchical Transformer VAE. IEEE Transactions on
Multimedia (2021).
[19] Shulei Ji and Xinyu Yang. 2023. Emotion-Conditioned Melody Harmonization
with Hierarchical Variational Autoencoder. (2023).
[20] Andrew Jones, Dmitri Tymoczko, and Hamish Robb. 2021. Music21 Corpus:
Bach Chorale Analyses. https://github.com/cuthbertLab/music21/tree/master/
music21/corpus/bach/choraleAnalyses
[21] Dan Jurafsky. 2000. Speech & Language Processing. Pearson Education India.
[22] Maximos A. Kaliakatsos-Papakostas and E. Cambouropoulos. 2014. Probabilistic
harmonization with fixed intermediate chord constraints. In Proceeding of the
Joint 11th Sound and Music Computing Conference (SMC) and 40th International
Computer Music Conference (ICMC).
[23] Steven Laitz. 2016. The Complete Musician: An Integrated Approach to Tonal
Theory, Analysis, and Listening. Oxford University Press.
[24] Sergei Lebedev, Anthony Lee, Gael Varoquaux, and Chris Farrow. 2021. hmmlearn:
Unsupervised learning and inference of Hidden Markov Models. https://github.
com/hmmlearn/hmmlearn
[25] Feynman T. Liang, Mark Gotham, Matthew Johnson, and J. Shotton. 2017. Auto-
matic Stylistic Composition of Bach Chorales with Deep LSTM. In International
Society for Music Information Retrieval.
[26] Lin Ma, Wei Zhong, Xin Ma, Long Ye, and Qin Zhang. 2022. Learning to generate
emotional music correlated with music structure features. Cognitive Computation
and Systems 4, 2 (2022), 100–107.
[27] Dimos Makris, Kat R. Agres, and Dorien Herremans. 2021. Generating Lead Sheets
with Affect: A Novel Conditional seq2seq Framework. In 2021 International Joint
Conference on Neural Networks (IJCNN).
[28] Tony R Martinez, Kristine Monteith, and Dan A Ventura. 2010. Automatic
generation of music for inducing emotive response. (2010).
[29] W Gerrod Parrott. 2001. Emotions in social psychology: Essential readings. psy-
chology press.
5058KDD ’24, August 25–29, 2024, Barcelona, Spain. Stephen Hahn et al.
[30] Cale Plut and Philippe Pasquier. 2020. Generative music in video games: State of
the art, challenges, and prospects. Entertainment Computing 33 (2020), 100337.
https://doi.org/10.1016/j.entcom.2019.100337
[31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn,
Jonas Müller, Joe Penna, and Robin Rombach. 2023. SDXL: Improving Latent
Diffusion Models for High-Resolution Image Synthesis. arXiv:2307.01952
[32] Jean-Philippe Rameau. 1722. Treatise on Harmony. Dover Publications.
[33] Adam Roberts, Claire Kayacik, Curtis Hawthorne, Douglas Eck, Jesse Engel, Mon-
ica Dinculescu, and Signe Nørly. 2019. Magenta Studio: Augmenting Creativity
with Deep Learning in Ableton Live. In Proceedings of the International Workshop
on Musical Metacreation (MUME).
[34] Marco Scirea, Julian Togelius, Peter Eklund, and Sebastian Risi. 2017. Affective
evolutionary music composition with MetaCompose. Genetic Programming and
Evolvable Machines 18 (2017), 433–465.
[35] T. Takahashi and M. Barthet. 2022. Emotion-driven harmonisation and tempo
arrangement of melodies using transfer learning. In Proc. of the 23rd Int. Society
for Music Information Retrieval Conf.
[36] Marie Thompson and Ian Biddle. 2013. Sound, music, affect: Theorizing sonic
experience. A&C Black.
[37] Olga Vechtomova and Gaurav Sahu. 2022. LyricJam Sonic: A Generative System
for Real-Time Composition and Musical Improvisation. arXiv:2210.15638 [cs.SD]
[38] P Vishesh, A Pavan, Samarth G Vasist, Sindhu Rao, and KS Srinivas. 2022.
DeepTunes-Music Generation based on Facial Emotions using Deep Learning. In
2022 IEEE 7th International conference for Convergence in Technology (I2CT). IEEE,
1–6.
[39] Andrew Viterbi. 1967. Error bounds for convolutional codes and an asymptotically
optimum decoding algorithm. IEEE Transactions on Information Theory 13, 2
(1967), 260–269.
[40] Qi Wang, Shubing Zhang, and Li Zhou. 2023. Emotion-Guided Music Accompa-
niment Generation Based on Variational Autoencoder. In 2023 International Joint
Conference on Neural Networks (IJCNN). IEEE, 1–8.
[41] Christof Weiß, Frank Zalkow, Vlora Arifi-Müller, Meinard Müller, Hendrik Vin-
cent Koops, Anja Volk, and Harald G. Grohganz. 2021. Schubert Winterreise
Dataset. , 25:1–18 pages. https://doi.org/10.5281/zenodo.5139893
[42] Christopher WM White and Ian Quinn. 2018. Chord Context and Harmonic
Function in Tonal Music. Music Theory Spectrum 40, 2 (11 2018), 314–335.
https://doi.org/10.1093/mts/mty021 arXiv:https://academic.oup.com/mts/article-
pdf/40/2/314/26446308/mty021.pdf
[43] Duncan Williams, Alexis Kirke, Joel Eaton, Eduardo Miranda, Ian Daly, James Hal-
lowell, Etienne Roesch, Faustina Hwang, and Slawomir J Nasuto. 2015. Dynamic
game soundtrack generation in response to a continuously varying emotional
trajectory. In Audio engineering society conference: 56th international conference:
Audio for games. Audio Engineering Society.
[44] Duncan Williams, Alexis Kirke, Eduardo R Miranda, Etienne Roesch, Ian Daly,
and Slawomir Nasuto. 2015. Investigating affect in algorithmic composition
systems. Psychology of Music 43, 6 (2015), 831–854.
[45] Yi-Chan Wu and Homer H Chen. 2016. Generation of affective accompaniment
in accordance with emotion flow. IEEE/ACM Transactions on Audio, Speech, and
Language Processing 24, 12 (2016), 2277–2287.
[46] Liangrong Yi and J. Goldsmith. 2007. Automatic Generation of Four-part Harmony.
InProceedings of the Conference on Uncertainty in Artificial Intelligence.
A Contrapuntal Harmony Realization Model
In order to generalize the harmonic realization, we trained a
Markov model (see Figure 10) to find P(A,T,B|M,C), where
A=(𝑎𝑙𝑡𝑜 1,···,𝑎𝑙𝑡𝑜𝑛),T=(𝑡𝑒𝑛𝑜𝑟 1,···,𝑡𝑒𝑛𝑜𝑟𝑛), and B=
(𝑏𝑎𝑠𝑠 1,···,𝑏𝑎𝑠𝑠𝑛)are the sequences of alto, tenor, and bass lines
that harmonize the melody M=(𝑚1,𝑚2,...,𝑚𝑛)respectively.
Figure 10: Reharmonization Model Plate Diagram.
Our model builds the harmony from the bass up, since the bass
is the most essential voice to fill the harmonic texture. Depend-
ing on the number of voices desired, the tenor and alto may be
left out. With the full four-voice texture, we assume the bass,
tenor, and alto are given by P(𝑏𝑎𝑠𝑠𝑖|𝑐𝑖,𝑚𝑖,𝑏𝑎𝑠𝑠𝑖−1),P(𝑡𝑒𝑛𝑜𝑟𝑖|
𝑐𝑖,𝑚𝑖,𝑏𝑎𝑠𝑠𝑖,𝑡𝑒𝑛𝑜𝑟𝑖−1), and P(𝑎𝑙𝑡𝑜𝑖|𝑐𝑖,𝑚𝑖,𝑡𝑒𝑛𝑜𝑟𝑖,𝑏𝑎𝑠𝑠𝑖,𝑎𝑙𝑡𝑜𝑖−1)
respectively. To limit the search space, we assume that the proba-
bility of a note given a chord is independent from the probability of
one note given another. We also assume that the transition proba-
bilities within a particular voice are independent from the emission
probabilities of other voices onto the voice of interest. For example,
this independence assumption allows:
P(𝑡𝑒𝑛𝑜𝑟𝑖|𝑐𝑖,𝑚𝑖,𝑏𝑎𝑠𝑠𝑖,𝑡𝑒𝑛𝑜𝑟𝑖−1)=
P(𝑡𝑒𝑛𝑜𝑟𝑖|𝑐𝑖)P(𝑡𝑒𝑛𝑜𝑟𝑖|𝑡𝑒𝑛𝑜𝑟𝑖−1)P(𝑡𝑒𝑛𝑜𝑟𝑖|𝑚𝑖,𝑏𝑎𝑠𝑠𝑖).
These probabilities can be easily extracted from any musical dataset
with harmonic annotation.
To generate a harmonic realization from the model shown in
Figure 10 requires extra steps, for it does not account for precise
rhythms. Rhythms are collected at one of various discretizations
for one time step such as the sixteenth or eighth note. Therefore,
sampling directly from the model would not make it clear when
notes are held vs. when they are repeated, a difference which may
be vital to the music’s character. To overcome this issue, we simply
sample from the rhythms in the dataset. Concretely, we first ob-
serve the total consecutive duration of each chord. Then, given the
duration of the chord and the rhythm of the melody during that
chord, we sample rhythms for the other voices based on how fre-
quently they appear in the data. If multiple notes in one voice play
against one note in another voice, the probabilities for individual
combinations of notes are averaged. Figure 11 shows an example
of the adjusted inference for P(𝑡𝑒𝑛𝑜𝑟𝑖|M,B). The first tenor note
is simply calculated as P(𝑡𝑒𝑛𝑜𝑟 1|𝑚1,𝑏𝑎𝑠𝑠 1). The second tenor
note occurs under two notes of the soprano, G and B. Therefore,
P(𝑡𝑒𝑛𝑜𝑟 2|M,B)=P(𝑡𝑒𝑛𝑜𝑟 2|𝑚1,𝑏1)P(𝑡𝑒𝑛𝑜𝑟 2|𝑚2,𝑏1)
2.
5059SentHYMNent: An Interpretable and Sentiment-Driven Model for Algorithmic Melody Harmonization KDD ’24, August 25–29, 2024, Barcelona, Spain.
Figure 11: Visualization of the inference for an inner voice
using the sampled rhythms.
B Deployment Details
Figure 12: Image of our deployed web app for interaction
with SentHYMNent.
Similar to Hahn et al . [12] , the website follows a typical Client-
Server model, where the client interacts with a responsive user
interface, and the server safely processes client requests by com-
municating with the database and machine learning model.
MongoDB was used to host our sentiment analyses. Experienced
users can hook up their own accounts using our code. Alternatively,
analyzed data can be downloaded as a .json file.
The front-end is implemented using the Vite, Vue.js, and Boot-
strapVue frameworks along with the Vexflow and Tone.js libraries.
Vexflow is used to generate music notation in the browser. Tone.js
performs the audio playback.
The back-end server, written using the Python Flask framework,
handles saving and loading information to the MongoDB database
and interaction with the SentHYMNent model.C Additional Experimental Results
Table 4: Proportion of excerpts identified as containing
“weird or bad” segments (lower is better).
Method Proportion 95% CI p-value
SentHYMNent 0.33 (0.20, 0.43) ref.
Bach 0.00 (0.0, 0.0) 0.004
Coconet 0.63 (0.30, 0.98) 0.054
Makris 0.82 (0.69, 0.94) <0.001
EmoMusicTV 0.68 (0.45, 0.91) 0.006
Table 5: Background question response counts. For listen-
ing time, low means less than 1 hour per week, medium 1-
15h/week, high >15h/week. For studying time, low is 0 years
studied with private teacher or in school, medium is <5y,
high is >5y.
Experience Listen Time Study Time
Low 0 3
Medium 24 8
High 6 19
Table 6: Screening question response counts. Choices were
either correct, incorrect, or nonsensical trick answers.
Response Time Signature Melodic Interval
Nonsense 0 1
Incorrect 0 8
Correct 30 21
Figure 13: Results of the Turing test for each model.
Figure 14: Results of the enjoyability test for each model.
5060