BoKA: Bayesian Optimization based Knowledge Amalgamation
for Multi-unknown-domain Text Classification
Linzhu Yu
The State Key Laboratory of Blockchain and Data Security,
Zhejiang University
Hangzhou, China
linzhu@zju.edu.cnHuan Li∗
The State Key Laboratory of Blockchain and Data Security,
Zhejiang University
Hangzhou, China
lihuan.cs@zju.edu.cn
Ke Chen
The State Key Laboratory of Blockchain and Data Security,
Zhejiang University
Hangzhou, China
chenk@zju.edu.cnLidan Shou∗
The State Key Laboratory of Blockchain and Data Security,
Zhejiang University
Hangzhou, China
should@zju.edu.cn
ABSTRACT
With breakthroughs in pretrained language models, a large num-
ber of finetuned models specialized in distinct domains have sur-
faced online. Yet, when faced with a fresh dataset covering multi-
ple (sub)domains, their performance might degrade. Reusing these
available finetuned models to train a new model is a more feasi-
ble solution than the finetuning method that demands extensive
manual labeling. Knowledge Amalgamation (KA) is such a model
reusing technique, which derives a new model (termed student
model) by amalgamating those trained models (termed teacher mod-
els) tailored for distinct domains, bypassing the need for manual
labeling. However, when the domains of text samples are unknown,
selecting a number of appropriate teacher models (simply called a
combination) for reuse becomes complicated. To learn an accurate
student model, the classical KA method resorts to manual selec-
tions, a process both tedious and inefficient. Our study pioneers
the automation of this combination selection process for KA in the
fundamental text classification task, an area previously unexplored.
In this paper, we introduce BoKA: an automatic knowledge amal-
gamation framework for identifying a combination that can learn
a superior student model without human labor. Through the lens
of Bayesian optimization, BoKA iteratively samples a subset of pos-
sible combinations for amalgamation instead of manual selections.
Furthermore, we introduce a novel KA method tailored for text
classification, which guides the student model using both soft and
pseudo-hard labels from the teacher models when their predictions
are closely aligned; in cases of significant disagreement, it uses ran-
domly generated labels. Experiments on two public multi-domain
datasets show that BoKA achieves remarkable efficiency by sam-
pling only up to 5.5% of all potential combinations. Moreover, BoKA
∗Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671963is capable of matching or even surpassing leading zero-shot large
language models, despite having dozens of times fewer parameters.
CCS CONCEPTS
•Computing methodologies →Knowledge representation
and reasoning; Natural language processing.
KEYWORDS
knowledge amalgamation, Bayesian optimization, text classification
ACM Reference Format:
Linzhu Yu, Huan Li, Ke Chen, and Lidan Shou. 2024. BoKA: Bayesian Opti-
mization based Knowledge Amalgamation for Multi-unknown-domain Text
Classification. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671963
1 INTRODUCTION
Finetuning pre-trained language models like Bert [ 6], T5 [ 39] and
GPT [ 37,38] on domain-specific datasets has become a popular para-
digm for enhancing text classification performance. This pretraining-
finetuning paradigm has led to the availability of numerous trained
models, proficient in designated domains [ 1,32,47]. On Hugging
Face, there exist over 1000 finetuned models from medical domains
for question answering (QA) referring to https://bit.ly/3utM0a4.
However, when dealing with fresh datasets covering various
(sub)domains, using finetuned models tailored to specific domains
directly can lead to suboptimal results. For example, building a
biomedical QA system, as depicted in Figure 1, using only one of
the pre-existing models fintuned for distinct domains like Skin
and Surgery, proves ineffective. A potential remedy is to manually
label the fresh dataset for pretraining-finetuning, but this method
is highly labor-intensive and inefficient.
Many recent studies [ 22,42,53,61] have explored reusing fine-
tuned models on new, unlabeled multi-domain datasets without
manual labeling. Knowledge Amalgamation (KA) [ 41] stands out
by integrating multiple domain-specific “teacher models” to train
a unified “student model” capable of comprehensive inference on
4035
KDD ’24, August 25–29, 2024, Barcelona, Spain Linzhu Yu, Huan Li, Ke Chen, and Lidan Shou
Learn a new 
model for 
multi-domain 
QAFinetuned
models
…ENT
Skin
Surgery
a) Domains are pre-known
Question: Non cicatrical alopecia is present in-
Option:A) Scleroderma  B) …  C) …  D) … Skin
ENTQuestion: Position of vocal cord in cadaver is:
Option: A) Median  B) …  C) …  D) …Domain Text
… …
b) Domains are unknown
?
?Domain Text
… …Select which 
models to 
reuse?Reuse some 
domain 
related
models
Question:Non cicatrical alopecia is present in-
Option:A) Scleroderma  B) …  C) …  D) …
Question:Position of vocal cord in cadaver is:
Option:A) Median B) … C) … D) …Knowledge
amalgamation
?
?
Figure 1: Reusing existing finetuned models without manual label-
ing: a) Domains are known: to select domain-related models (e.g.,
‘Skin’ and ‘ENT’) for amalgamation; b) Domains are unknown (com-
monly encountered in practice): selecting an optimal subset of fine-
tuned models becomes challenging.
the new multi-domain dataset1. However, the effectiveness of this
process largely depends on selecting the appropriate teacher mod-
els. As depicted in Figure 1, we identify two distinct scenarios: (1)
Domains of samples are pre-known: simply, models tailored for the
corresponding domains (e.g., ‘Skin’ and ‘ENT’) are selected for KA,
as shown in Figure 1 (a). (2) Domains of samples are unknown: the
questions’ domains are not predefined, making the model selection
rather complicated for the entire dataset, as shown in Figure 1 (b).
This paper focuses on addressing this second scenario, the complex
task of Multi-unknown-domain Text Classification.
The task described herein is frequently encountered in practical
applications. Consider that a developer, Eva, collects a new set of
medical questions that lack labels and also annotations for medical
subdomains, primarily due to the high labor costs of professionals.
To complete the task, Eva initially experimented with casually se-
lecting models (e.g., ‘Skin’ + ‘Surgery’) to train a student model via
KA. However, the answers generated by the resultant model were
found to be unsatisfactory. Brutally trying more combinations of
teacher models for KA proved to be impractical. This realization
prompted Eva to explore the possibility of a methodology capable
of autonomously identifying and integrating the most appropriate
models to train a superior student model tailored to the new ques-
tions at hand. This underscores the necessity for the development of
anAutomatic Knowledge Amalgamation (AutoKA) technique.
AutoKA is however non-trivial with two major issues: I1(Lim-
ited Exploration in Text Classification KA). Amalgamating various
text classifiers from different domains is complex due to the absence
of true text labels and possible teacher conflicts that may degrade
the accuracy of the student. Existing KA methods [ 15,29,30,42],
mainly from the computer vision field, and knowledge distillation
techniques [ 25,56,60] requiring labels, are inapplicable. A detailed
discussion on this is in Section 5. I2(Complex Selection of Teacher
Models ). Determining the optimal (teacher model) combinations
from a vast pool is challenging and time-consuming due to the
1In contrast to the established notation of multi-teacher knowledge distillation (KD)
that emphasizes model compression via larger teacher models and known labels, KA
aims to employ specific teachers in lieu of labels. Please see our discussions in Section 5
and empirical analysis comparing several multi-teacher KD methods in Section 4.4.exponential number of possible combinations to sample. An effi-
cient strategy is needed to streamline this selection process without
resorting to exhaustive searches.
We present BoKA, a framework utilizing Bayesian optimization
for AutoKA in multi-unknown-domain text classification. BoKA
features three key modules illustrated in Figure 2. The profiler
initially samples all singular combinations of one teacher model,
and a text classification KA method is designed to evaluate these
early students models’ performance as initial observations (see
Section 3.2). The selector then maps all possible combinations, em-
ploying a Gaussian process and utility computations from current
observations to choose the next combination for sampling (see Sec-
tion 3.3). The sampler executes a plural KA process on this chosen
combination, generating data to aid the selector’s future choices
(see Section 3.4). The proposed plural KA, tailored for text classifi-
cation by using both soft and hard labels from multiple teachers,
iterates until a combination is chosen or a maximum iteration cap
is met, ultimately identifying and outputting the most effective stu-
dent model along with its combination. To sum up, BoKA addresses
the issue I1by incorporating carefully designed KA approaches in
the profiler and sampler, and the issue I2by strategically picking
up the most promising combination in the selector.
Through comprehensive experiments on two real-world datasets,
we validate the significant efficiency and effectiveness of our BoKA
framework in multi-unknown-domain text classification. It learns
a high-performing student model by sampling less than 5.5% of
possible combinations. Moreover, BoKA outperforms conventional
manual KA techniques and is competitive with or superior to both
larger zero-shot language models and models finetuned with labels.
The contributions of this study are summarized as follows.
•We illustrate the multi-unknown-domain text classification com-
monly encountered in practice and formalize it as the automatic
knowledge amalgamation task (Section 2).
•We represent the first effort to propose a novel framework BoKA
through the lens of Bayesian optimization with three carefully
designed modules (Section 3).
•We verify the effectiveness and efficiency of BoKA by extensive
experiments on public multi-domain datasets and provide a visual
analysis to scrutinize the rationale of BoKA (Section 4).
Besides, Section 5 reviews related work, and Section 6 concludes
the paper with future research directions.
2 PRELIMINARIES
2.1 Notation of Knowledge Amalgamation
Consider a text classification scenario with a set 𝑇={𝑡𝑖}𝑛
𝑖=1of
teacher models . Each of these models is finetuned within a specific
(sub)domain denoted as 𝑑𝑖(1≤𝑖≤𝑛). The training data comes
in pairs(x,y), where xis an input text and yrefers to the label
(e.g., y∈{− 1,+1}for binary classification). Each teacher model 𝑡𝑖
consists of a semantic feature encoder, E𝑡
𝑖, and a label classifier H𝑡
𝑖.
In classical pipelines [ 21,26], the label classifier H𝑡
𝑖outcomes the
classifications based on the last token output from the encoder E𝑡
𝑖.
In this setting, the Knowledge Amalgamation (KA) process
aims to train an integrated student model 𝑠, capable of compre-
hensive classification across the collective domain of all teacher
models,Ð
𝑡𝑖∈𝑇𝑑𝑖, by utilizing the outcomes of these teachers. KA
4036BoKA: Bayesian Optimization based Knowledge Amalgamation for Multi-unknown-domain Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain
1. Profiler
Sample singular
combinations
Student models…
…
Initial observationst1 t𝑛 t2
s1s2 𝑠2𝑛−1
t𝑛Multi-domain 
DatasetInput
Teacher 
modelst1
t4
…t3t2
3. Sampler
New student model Selected combinationt1t2t3
t4t5Output
The corresponding
combinationSuperior 
student model
t1t2t3
t4t5Step-1: Update the Gaussian process
Step-2: Enable 𝑢𝑡𝑖𝑙𝑖𝑡𝑦computation and 
select the combination with 
the highest 𝑢𝑡𝑖𝑙𝑖𝑡𝑦 Unsampled combinations…
…2. Selector
t1
t2t1t2t2
t4t5
Sampled combinationst1t2t3
t4t5
Iterative loopThe selected combination 
is sampled or 
max samples is reached.
Augment
(𝐮31, 0.736)Add a new 
observation
Combination
𝐮1
𝐮2
𝐮4
… …
𝐮2n−1Accuracy of student
0.664
0.708
0.670
… …
0.682Soft and hard labels
Random labelsKA
Disagreement 
extent ?(0.1,0.6,0.3) (0.2,0.3,0.5)
(0.2,0.6,0.2) (0.3,0.3,0.4)(0.2,0.4,0.4)
HighLowOutput logitsKA
Evaluate
Figure 2: The BoKA framework initiates with a profiler that gathers initial observations, followed by iterative loops where the selector chooses
combinations and the sampler acquires new observations. This loop continues until the selected combination has already been sampled or the
maximum number of iterations is reached.
is particularly useful in situations where legacy models exist but
lack access to their original training datasets [ 41]. The resultant
student model synthesizes the expertise of the established teacher
models, eliminating the need for additional human labeling efforts.
In terms of structure, the student model 𝑠closely resembles its
teacher counterparts, with minor adjustments made to its classifier.
Specifically, the dimensionality of the output from its classifier H𝑠is
defined by the product of the number of distinct labels and the total
number of teacher models incorporated within the amalgamation.
2.2 Problem Formulation
In practical applications, it is often needed to build an integrated
model capable of classifying texts in a new dataset Dthat spans
multiple domains. This leads to a task known as multi-domain
text classification. When each domain in Dis pre-known, se-
lecting a relevant set of teachers from the gallery, each expert in a
respective domain, for KA becomes straightforward (see Fig. 1 (a)).
However, more often than not, the domains of Dremain un-
known (see Figure 1 (b)). Additionally, there is a risk that some
domains inDmay not correspond to any existing teacher model
in our gallery (Section 4.5 analyzes this case). This introduces a
more general task, which we term multi-unknown-domain text
classification. Addressing the task necessitates the automatic iden-
tification of a set of teachers that align with D, thereby enabling
the amalgamation of a student model that excels in performance
onD. We define the teacher model combination as follows.
Definition 1 (Teacher Model Combination). Given a set𝑇=
{𝑡𝑖}𝑛
𝑖=1of teacher models, we define U𝑇={u𝑗}2𝑛−1
𝑗=1as the set of
all possible non-empty combinations of these teacher models. Each
a(teacher model) combination, as a binary vector u𝑗∈ {0,1}𝑛,
indicates the absence or presence of each of the 𝑛teacher models.
Finally, our research problem is formulated as follows.
Problem (Automatic Knowledge Amalgamation for Multi-
-unknown-domain Text Classification). Given a multi-domain
text datasetDwithout domain specifications and the set 𝑇of existingteacher models, the goal of our research problem is to find a combi-
nation ˆu∈U𝑇for KA such that a new student model trained with ˆu
achieves the best possible classification performance on D.
The exponential nature of the search space U𝑇renders the brute-
force enumeration infeasible. Fortunately, Bayesian optimization
has proven a promising technique in machine learning, particu-
larly for tasks like deep learning hyperparameter tuning and neural
architecture search [ 8,17,45,63]. Despite this, its application in
selecting combinations for KA remains unexplored. Given this con-
text, we propose the BoKA framework. BoKA resolves the research
problem through the lens of Bayesian optimization, providing an
efficient-yet-effective search of the optimal combination in KA.
3 KEY TECHNIQUES IN BOKA
We present the framework overview of BoKA in Section 3.1 and
elaborate on its three key modules in Sections 3.2, 3.3, and 3.4.
3.1 Framework Overview
Bayesian Optimization Notation. Bayesian optimization is a
methodology designed for optimizing black-box functions, which
are notably expensive to evaluate [ 9]. It principally involves cre-
ating an updatable and predictive model to drive optimization de-
cisions [ 40]. Specifically, given a black-box function 𝑓:U→ R,
whereUis some design space of interest (often restricted to a
compact subset of R𝑛) [40], the goal is to find an input ˆu∈U
that globally maximizes (or minimizes) the function 𝑓. It involves
two key concepts: A surrogate model Sfor approximating the
black-box function 𝑓, and an acquisition function A:U→ R
for evaluating the utility for each candidate u∈U. To put it sim-
ply, the utility measures a candidate’s potential contribution to the
optimization. The following steps are gone through to outcome ˆu:
S1Initial observations, denoted as O, are collected. Each item in
Ois a pair of uand its function value 𝑓(u).
S2 The surrogate model Sis updated using current O.
S3 The next input, u′←arg max uA(u)is identified withS.
S4The function 𝑓(u′)is calculated, and the observations are aug-
mented with the new pair (u′,𝑓(u′)).
4037KDD ’24, August 25–29, 2024, Barcelona, Spain Linzhu Yu, Huan Li, Ke Chen, and Lidan Shou
S5Resume step S2 unless a pre-set termination criterion is met,
such as a maximum iteration number.
Key Modules in BoKA. Figure 2 illustrates the framework of
BoKA, which transforms problem-solving into a Bayesian optimiza-
tion process via three integrated modules:
(1) Profiler begins by assessing the effectiveness of knowledge
within individual teacher models. This is achieved by examining
singular combinations, each involving only one teacher model. The
accuracy of student models generated from these singular com-
binations serves as the initial observation (cf. above step S1) for
subsequent steps in Bayesian optimization.
(2) Selector utilizes the Bayesian optimization algorithm to iden-
tify the most promising combination. This involves two crucial
sub-steps. First, a surrogate model is updated using the current
observations (cf. step S2). The updated surrogate model captures
the posterior probability distribution of 𝑓(u). We employ the Gauss-
ian process to build the surrogate model due to its flexibility and
tractability, as revealed in previous studies [ 2,18,45]. Subsequently,
the posterior probabilities outcome by the surrogate model are uti-
lized to design an acquisition function A(u)to estimate the utility
of each combination u. The combination with the highest utility
is predicted as the next one for sampling (cf. step S3), i.e., the KA
process for text classifiers. The process either proceeds to sam-
pling if the predicted combination is unsampled and the maximum
iterations are not reached, or it terminates (cf. step S5).
(3) Sampler implements a novel KA approach for training the stu-
dent model using the selected combination. This process enables
the student to assimilate knowledge from multiple teachers, incor-
porating both soft logits and jointly determined hard labels, without
relying on any human labeling. After each sampling, the accuracy
of the new student model augments the observation for the selector
(cf. step S4), facilitating an update of the surrogate model.
3.2 Profiler
The launch of Bayesian optimization necessitates initial observa-
tions. Intuitively, assessing the effectiveness of the knowledge con-
veyed by each teacher model helps assess combinations of their
knowledge. In this light, we generate a singular combination for
each teacher model in KA and derive a total number of 𝑛students.
We collect each singular combination and its corresponding student
model’s accuracy on a validation dataset as our initial observations.
However, the original KA method [ 41], designed for convolutional
networks in image classification, is not directly applicable to pre-
trained language model modules. Hence, we propose a modified
KA method for text classifiers, as explained below.
Singular KA Procedure . To enhance the student model 𝑠with
more knowledge, we utilize the outputs from the teacher model 𝑡𝑖in
different views: (1) The teacher’s predictions ˜y𝑡
𝑖, called pseudo-hard
labels, which specify the exact class for texts like a human annotator.
(2) The teacher’s classification logits y𝑡
𝑖, called soft labels, which
contain both the probability of the predicted class and the hidden
class correlations. To align the student’s predictions y𝑠with the
teacher’s, we follow previous studies [ 12] and use a combined loss
function that considers the losses concerning both the pseudo-hard
and soft labels:
LSKA=𝛼·CE ˜y𝑡
𝑖,y𝑠+(1−𝛼)·KL y𝑡
𝑖,y𝑠, (1)Table 1: Vector representations and indexes of partial teacher model
combinations in the selector.
Index Combination Vector Representation
1{𝑡1} u1=[1,0,0,0,..., 0]
2{𝑡2} u2=[0,1,0,0,..., 0]
4{𝑡3} u4=[0,0,1,0,..., 0]
5{𝑡1,𝑡3} u5=[0.5,0,0.5,0,..., 0]
7{𝑡1,𝑡2,𝑡3} u7=[0.33,0.33,0.33,0,..., 0]
where the hyperparameter 𝛼balances the importance of the two
types of teacher labels. The cross entropy function CE(·), suitable
for hard labels ˜y𝑡
𝑖, measures its difference to y𝑠, pushing the student
to mimic the teacher’s decision. The Kullback-Leibler (KL) diver-
gence KL(·), on the other hand, is designed for soft labels. It assesses
the information loss when approximating y𝑡
𝑖from y𝑠, aiding the
student in understanding class correlations like the teacher.
3.3 Selector
As the central component in Bayesian optimization, the selector’s
essential designs, including the input space, surrogate model, ac-
quisition function, and termination criterion, are explained below.
Input Space. Given teacher set 𝑇, the input spaceU𝑇={u𝑗}2𝑛−1
𝑗=1
covers all possible combinations of teacher models in 𝑇for KA,
with each u𝑗being an𝑛-dimensional vector corresponding to the 𝑛
teacher models. We adapt the original Definition 1 to fit Bayesian
optimization by replacing binary indicators (0 or 1) with real values
inu𝑗. These real values denote the proportional contribution of
labels of each teacher model 𝑡𝑖to KA, where any non-zero value
signifies the inclusion of the associated teacher in the sampling.
To illustrate, consider a specific combination {𝑡𝑖−1,𝑡𝑖,𝑡𝑖+1}, where
each teacher contributes a certain number of labels, denoted as
𝑙𝑖−1,𝑙𝑖, and𝑙𝑖+1respectively. The corresponding vector u𝑗is then
computed by normalizing these label contributions, resulting in"
0,...,𝑙𝑖−1Í𝑖+1
𝑘=𝑖−1𝑙𝑘,𝑙𝑖Í𝑖+1
𝑘=𝑖−1𝑙𝑘,𝑙𝑖+1Í𝑖+1
𝑘=𝑖−1𝑙𝑘,0,...#
. In this vector, dimen-
sions associated with other non-contributing models are filled
with zeros. To ease tracking and organization, the combination
{𝑡𝑖−1,𝑡𝑖,𝑡𝑖+1}is assigned a unique index 𝑗=2𝑖−2+2𝑖−1+2𝑖.
Example 1. Table 1 exemplifies the vector representation and in-
dexing method of the teacher model combinations in the input space.
Take the combination {𝑡1,𝑡3}as an example. Its index, calculated as
21−1+23−1, equals 5. Suppose that both 𝑡1and𝑡3contribute the same
number of labels, say 𝑙. Therefore, in the vector u5, the dimensions
corresponding to 𝑡1and𝑡3are each assigned a value of 𝑙/(𝑙+𝑙)=0.5,
while all other dimensions are set to zero.
Surrogate Model. We employ the Gaussian process as the surro-
gate model due to its computational convenience and the provided
prior distribution on functions [ 45]. The Gaussian process can ap-
proximate the objective function 𝑓(u)using a prior distribution,
assumed to be a multivariate Normal distribution, especially when
the observational data is absent [ 9]. To be specific, it posits that
for any set of inputs, the output of 𝑓(u)are normally distributed
random variables. This leads to the prior distribution modeled as:
𝑓(u)∼ GP 𝜇(u),𝑘(u,u′). (2)
4038BoKA: Bayesian Optimization based Knowledge Amalgamation for Multi-unknown-domain Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain
In this prior distribution: Partly due to the lack of prior knowledge
about the function’s behavior, the mean function 𝜇(u)is typically
set to zero for computational simplicity; The covariance function
𝑘(u,u′)employs the RBF kernel to measure the similarity between
the two given combinations:
𝑘(u,u′)=𝜎2exp
−∥u−u′∥2
2ℓ2
. (3)
where𝜎andℓrefer to the variance and length hyperparameter
within the RBF kernel [10], respectively.
Notably, the observations on all the singular combinations have
been obtained by the profiler (see Section 3.2). We reorganize the
pairs of a combination and a function value as O=(U𝑜;F𝑜), where
U𝑜=[u1,u2,..., u2𝑛−1]TandF𝑜=[𝑓(u1),𝑓(u2),...,𝑓(u2𝑛−1)]T.
In the Gaussian process, the joint distribution of Oand𝑓(u)at any
uremains a multivariate Normal distribution N(·) . Utilizing the
conditional distribution formula, we obtain the posterior distribu-
tion of𝑓(u)givenOas:
𝑓(u)|O∼N( 𝜇+,𝑘+), (4)
𝜇+(u)=𝑘(U𝑜,u)T·𝑘(U𝑜,U𝑜)−1·F𝑜, (5)
𝑘+(u)=𝑘(u,u)−𝑘(U𝑜,u)T·𝑘(U𝑜,U𝑜)−1·𝑘(U𝑜,u), (6)
where𝜇+(u)and𝑘+(u)are the updated mean and covariance (un-
certainty), respectively. This posterior distribution guides the selec-
tion of new combinations for sampling, as to be detailed soon.
Acquisition Function. To determine the next combination for
sampling, we adopt the Expected Improvement (EI) function as the
acquisition function due to its efficiency [ 16] and minimal need for
parameterization. This function estimates the utility (potential of
each combination) based on the posterior distribution of the ob-
jective function 𝑓(u), considering two critical aspects: exploration
and exploitation. Exploration involves probing combinations with
higher uncertainty (higher posterior standard deviation) to better
understand the overall shape and characteristics of 𝑓(u). Exploita-
tion, on the other hand, focuses on combinations that the current
surrogate model predicts as most likely to be optimal, particularly
those with the highest posterior mean. This helps in quickly finding
local optima and shortening the selection-sampling iteration. The
EI function is given as follows.
EI(u)= 
(𝜇+(u)− F△𝑜−𝜉)·Φ(𝑍)
+√︁
𝑘+(u)·𝜙(𝑍), if√︁
𝑘+(u)>0
0, if√︁
𝑘+(u)=0,
𝑍= 𝜇+(u)− F△
𝑜−𝜉/√︁
𝑘+(u),(7)
where𝜉, set to 0.01 as suggested by Lizotte [ 27], adjusts the balance
between exploration and exploitation. We investigate its impact
on the efficiency of BoKA in Section 4.3. Besides, 𝜙andΦare the
probability density and the cumulative distribution functions of
the standard Normal distribution, respectively. The term F△𝑜refers
to the maximum value in F𝑜. We typically select the combination
with the highest utility value for the next evaluation, as it likely
provides the most beneficial information for optimization.
Termination Criterion. Each time a new combination is selected
and sampled, we add both the combination and its observed func-
tion value toO. This addition leads to an update in the posteriordistribution. The iteration process stops either when a previously
sampled combination is re-selected or when the maximum number
of sampling is reached. In the end, the student model exhibiting the
best performance is identified and reported as the superior model
(see the right side of Figure 2).
3.4 Sampler
𝒚1𝑡Pseudo hard labels𝒚𝑗𝒖
Unlabeled 
text data
Teacher
combination𝑢 𝑗
Student model𝒚4𝑡𝒚2𝑡Output
Overall loss𝓛
Output𝒚𝑗𝑠Concatenate
KL divergence
lossℒ𝑠 Soft labels𝒚𝑗𝒖
(𝑐×𝑚, )Average &
assign class0 1 0 0
(𝑐×𝑚, )
Cross-entropy
lossℒℎPredicted logits
(𝑐, ) (𝑐, ) (𝑐, ) (𝑐, )
Predicted logits
Figure 3: The illustration of the plural KA procedure. A combination
of𝑚=3teachers, each containing 𝑐=4classes, are used to train a
student model 𝑠𝑗, as one round of the sampling.
To address scenarios involving multiple teachers, we extend the
proposed singular KA method (see Section 3.2) to the plural setting.
Plural KA Procedure. As depicted in Figure 3, the sampler module
trains a new student model 𝑠𝑗using a combination u𝑗that includes
𝑚(𝑚>1) teacher models. Recall that in Section 2.1, the student
model’s classification head is shaped by multiplying the distinct
class count𝑐by the number of teacher models, 𝑚. This results in 𝑚
sub-classification heads within 𝑠𝑗, each containing 𝑐classes. Thus,
𝑠𝑗’s classification logits, denoted as y𝑠
𝑗, consists of 𝑚components,
with y𝑠
𝑗[𝑚′]representing the 𝑚′-th (0≤𝑚′<𝑚) component.
Similar to the singular KA method (see Section 3.2), we uti-
lize both pseudo-hard labels and soft labels. To compensate for
the absence of true labels, the teacher models in combination u𝑗
collectively determine the pseudo-hard label by averaging their
predicted logits and assigning the class with the highest score as
the pseudo-hard label ˜yu
𝑗. Then, we take the mean of all y𝑠
𝑗[𝑚′]as
the classification output of the student model 𝑠𝑗and calculate the
cross-entropy between the mean and the pseudo-hard label:
Lℎ=CE
˜yu
𝑗,1
𝑚∑︁
𝑚′=1,...,𝑚y𝑠
𝑗[𝑚′]
. (8)
Simultaneously, to grasp the predicted logits by 𝑚teacher models
inu𝑗, the student model learns the concatenated logits of the 𝑚
teachers via the KL divergence as follows:
L𝑠=KL
y𝑡
1,..., y𝑡
𝑚
,y𝑠
𝑗
. (9)
The final loss function for learning the student model, similar to
Equation 1, is combined as:
LPKA=𝛼Lℎ+(1−𝛼)L𝑠. (10)
However, for an unlabeled text sample, the labels provided by
different teacher models might be inconsistent. We thus propose a
disagreement mitigation protocol as follows.
Disagreement Mitigation. To measure the disagreement within
the combination u𝑗, we calculate the average KL divergence of the
predicted labels from all teacher model pairs in u𝑗as follows:
𝜅𝑗=1
𝑚×(𝑚−1)∑︁
𝑡𝑖,𝑡𝑖′∈u𝑗,𝑖≠𝑖′KL(y𝑡
𝑖,y𝑡
𝑖′). (11)
4039KDD ’24, August 25–29, 2024, Barcelona, Spain Linzhu Yu, Huan Li, Ke Chen, and Lidan Shou
A lower disagreement score 𝜅𝑗below a certain threshold signifies
consensus among teachers in u𝑗, enabling student model learning
as per Equation 10. The threshold is set such that random labels
constitute approximately 5% of all labels. Conversely, a high 𝜅𝑗
indicates that certain teacher models might provide inaccurate
information. Unfortunately, without labels, it is impossible to assess
the accuracy of the teacher models’ outputs. To prevent potential
misguidance of the student model due to training on this uncertain
information, we instead use random pseudo-hard and soft labels to
improve the student model’s ability to generalize. Experiments in
Section 4.4 verify the efficacy of this strategy.
4 EXPERIMENTS
Section 4.1 covers experimental settings and Section 4.2 provides
an extensive comparison of BoKA with three groups of methods.
The framework efficiency, the effect of essential modules, and a
visual analysis are detailed in Sections 4.3, 4.4, and 4.5, respectively.
4.1 Experimental Settings
Datasets and Metrics. We select two public biomedical question-
answering (QA) datasets, each covering multiple distinct subdo-
mains. (1) PubMedQA [14] is a dataset collected from PubMed
articles for answering research-related questions. Each instance
comprises a question, a context, and an extended answer. Pub-
MedQA is divided into three segments: PQA-L, featuring manual
labels; PQA-U, featuring no labels; and PQA-A, featuring artificial
labels. According to the community standard [ 14], PQA-L is di-
vided equally for validation (PQA-L-val) and testing (PQA-L-test).
We train and validate teacher models with PQA-A and PQA-L-val,
train student models with PQA-U, and evaluate both on the PQA-L-
test. It is important to note that the testing in PubMedQA requires
reasoning [ 14], thus disallowing the use of extended answers. (2)
MedMCQA [36] is a multiple-choice dataset collected from real-
world medical entrance exams. We partition the training set into
two segments: one with complete questions for finetuning the teach-
ers, and another includes only examples without labels for student
training. Besides, we use the development set for testing. Table 2
summarizes the key statistics of both datasets, where the number of
answer options and words per question indicate a higher difficulty
ofMedMCQA . To evaluate the classification performance of the
derived student models, we adopt accuracy as the metric, aligned
with previous studies [14, 36].
Table 2: Statistics of the biomedical QA datasets.
Dataset PubMedQA MedMCQA
Answer options [yes,no,maybe] [A,B,C,D]
Question splits
(teacher /student /test)211.3k /61.2k /0.5k 150k /32.8k /4.2k
Words per question 253.3 12 .7
Teacher Models. Our study involves domain-specific teacher mod-
els, trained on different datasets of similar sizes. In PubMedQA,
we divide the PQA-A subset into 10 sections based on the MeSH
vocabulary [ 33], with each section denoting a different biomedical
domain (e.g., “C” for “Infections”). These sections are used to fine-
tune 10 teacher models, labeled as 𝑡𝐴∼𝑡𝐺,𝑡𝑀,𝑡𝑁, and𝑡𝑍, each for
a specific domain. For MedMCQA , its training set is split into 21
medical subject domains. Due to the varying sizes of these subsets(ranging from 14,670 to 1,458), some closely related were combined,
resulting in 14 distinct domains. We trained a set of teachers, de-
noted𝑡𝐴∼𝑡𝑁, each tailored to a specific subject. Please be aware
that the model indexes used in MedMCQA overlap with those in
PubMedQA (e.g.,𝑡𝐴,𝑡𝐵, etc.), yet they refer to different models.
Implementation Details. BoKA is implemented using PyTorch
and Transformers toolkits. The related code is available at https://
github.com/linzhu1967/BoKA. It ran on a server with Intel Xeon 2.30
GHz CPU and NVIDIA RTX A6000 GPU (48 GB VRAM). We used
theBioGPT [28] language model with 347M parameters as the base
for both teacher and student models, with a maximum input length
of 1,024 tokens as per an analysis of the datasets. Training settings
include a batch size of 7, AdamW optimizer with a 2 𝑒-5 learning
rate, 20 epochs for teacher models on PubMedQA, 50 epochs on
MedMCQA, and 1 epochs for students for reduced computational
cost. In KA procedures, the loss function hyperparameter 𝛼is set
to 0.5, and the hyperparameters 𝜎andℓfor the Gaussian process
were set to 0.2 and 0.5, respectively.
4.2 Overall Effectiveness Comparison
BoKA is comprehensively compared against three groups of compet-
itive approaches (see their detailed descriptions in the Appendix),
with accuracy results and parameter volumes reported in Figure 4.
Detailed analyses for each group are as follows.
BoKA vs Manual KA. The first group is the manual KA method
which leans on manual selections for the combination that can
produce a student model exhibiting better performance. Given the
inherent difficulties and labor intensity of manual selection, the
manual KA method is typically confined to three distinct settings:
(1) Selecting the top-performing teacher model: the singular KA
procedure (see Section 3.2) is applied to each singular combina-
tion. (2) Selecting all teacher models: the plural KA method (see
Section 3.4) is used for the combination that includes all teacher
models. (3) Delving into an exhaustive exploration of all possible
combinations, which is often deemed infeasible due to the exten-
sive computational resources and time needed. In our experiments,
we turn to the (1) and (2) settings to implement the manual KA
method. All resulting student models match our BoKA in size at
347M. As shown in Figure 4, BoKA (the orange bar) outperforms
these manual KA methods (blue bars) on both datasets, highlighting
the superiority of our selection methodology. Besides, we noticed
a significant variation in the accuracy of student models trained
with different single teacher models. This illustrates the consider-
able impact of teacher model selection on the performance of the
student model. In Figure 4 (a), the combination that includes all
teacher models (the bar marked with ‘All’) does not surpass the
singular combination from domains 𝐸,𝐺, and𝑀. This suggests that
simply increasing the number of teacher models does not guarantee
improved performance in student models, likely due to significant
variances and inadequate integration among the teachers. Hence,
it is necessary to carefully select suitable teacher models for the
multi-unknown-domain text classification task.
BoKA vs Zero-shot Large Language Models (LLMs). The sec-
ond group consists of LLMs, including Chat-Doctor [62],MedAl-
paca [11],LLaMA-2 [51],ChatGPT [34],ClinicalCamel [49],In-
structGPT [35] and Codex [4], none specifically finetuned on our
4040BoKA: Bayesian Optimization based Knowledge Amalgamation for Multi-unknown-domain Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain
BoKA
Manual KA
Chat-Doctor
MedAlpaca
LLaMA-2-13B
LLaMA-2-70B
ChatGPT
ClinicalCamel-13B
ClinicalCamel-70B
InstructGPT
Codex
BioGPT-FT
LLaMA-FT
PMC-LLaMA
BioMedGPT0.50.60.70.8Accuracy ABCDEFGMNZAll0.736
0.6640.670.6760.6720.708
0.6820.7140.717
0.668
0.6420.694
0.5430.5320.680.743
0.6390.7290.7430.7220.732
0.6980.734
0.6950.761(a) PubMedQA
BoKA
Manual KA
Chat-Doctor
MedAlpaca
LLaMA-2-13B
LLaMA-2-70B
ChatGPT
ClinicalCamel-13B
ClinicalCamel-70B
InstructGPT
Codex
BioGPT-FT
Llama-FT
PMC-LLaMA
BioMedGPT0.300.350.400.450.500.550.60Accuracy ABCDEFGHIJKGMNAll0.533
0.4990.4970.4930.4850.4810.4570.4780.4820.4670.4620.4860.4780.4560.4790.518
0.3110.3110.3740.350.44
0.3910.47
0.440.5090.4920.4820.5050.514(b) MedMCQA
Figure 4: Accuracy results of BoKA (347M), Manual KA Models (347M), Chat-Doctor (7B), Med-Alpaca (13B), LLaMA-2 (13B, 70B), ChatGPT
(175B), ClinicalCamel (13B, 70B), InstructGPT (175B), Codex (175B), BioGPT-FT(347M), LLaMA-FT (7B), PMC-LLaMA (7B) and BioMedGPT
(10B). For PubMedQA andMedMCQA, BoKA selects the combination {𝑡𝐷,𝑡𝐸,𝑡𝐹,𝑡𝐺,𝑡𝑀}and{𝑡𝐴,𝑡𝐵,𝑡𝐷,𝑡𝐸,𝑡𝐼,𝑡𝐾,𝑡𝑁}, respectively. BioGPT-FT
was finetuned with a classification head using all labeled data. Optimal results are reported for ClinicalCamel [49],PMC-LLaMA [58] and
BioMedGPT [31] from their original papers. Results for other models were based on empirical studies [23, 55, 58]. Models highlighted in red
diverge from the standard community benchmark by reporting their performance on PQA-L instead of PQA-L-test [14].
evaluation datasets and each with under 175B parameters. They
were evaluated using zero-shot prompting for fairness. Our BoKA
(347M) is not compared to significantly larger models like PaLM [ 43],
Flan-PaLM [ 43], and Med-PaLMs [ 44], which have 540B parameters,
due to their considerable size and training data differences.
In Figure 4 (a), five LLMs with purple bars and red names were
assessed on the PQA-L set, including the validation dataset, serving
mainly for reference. Despite this, BoKA significantly outperforms
these LLMs, especially in the more challenging MedMCQA, with
the minor exception of LLaMA-2-70B’s slight edge in PubMedQA.
Given their significantly higher parameter sizes, these five models
are less practical. In comparison, BoKA matches the performance of
ClinicalCamel (13B and 70B), Codex, and ChatGPT inPubMedQA
and exceeds them in MedMCQA, making BoKA a more resource-
efficient choice against these LLMs.
BoKA vs Finetuned LLMs. The third group (green bars) includes
BioGPT-FT, LLaMA-FT [50],PMC-LLaMA [58] and BioMedGPT [31].
BioGPT-FT was finetuned for 50 epochs on the labeled PQA-A set
from PubMedQA and the same set used by students in MedM-
CQA, due to the PQA-U set lacking manual labels. As per stud-
ies [31,58],LLaMA-FT ,PMC-LLaMA andBioMedGPT were fine-
tuned for 3 epochs on a combined dataset from PubMedQA’s PQA-A
andMedMCQA’s training set. LLaMA-FT andPMC-LLaMA are
based on LLaMA-7B while BioMedGPT utilizes LLaMa2-Chat-7B
with two encoders and two modality adaptors. Larger models like
MEDITRON (70B) and Galactica (120B), due to their greater param-
eters and training costs, were not considered suitable competitors.
In Figure 4, BioGPT-FT, despite full training and manual label-
ing, underperforms compared to BoKA and incurs higher training
costs, needing about 50 epochs to converge, versus BoKA’s single
epoch per student model. LLaMA-FT andPMC-LLaMA, despite
having over ten times the parameters and being finetuned, are out-
performed by BoKA on both datasets. Meanwhile, the finetuned
BioMedGPT (10B), the largest in its group, slightly exceeds BoKA’s
performance on PubMedQA but falls short on MedMCQA.
All in all, BoKA effectively handles multi-unknown-domain text
classification without extensive finetuning or using very large LLMs,
as shown through comparisons with all three groups of models.4.3 Efficiency Studies
This section evaluates BoKA’s efficiency based on iteration num-
bers, focusing on the role of the hyperparameter 𝜉in balancing
exploration and exploitation, as per the EI function (see Equation 7).
A smaller𝜉promotes exploitation, enhancing faster convergence
by sampling in areas of higher performance certainty. Conversely,
a larger𝜉leans towards exploration, expanding the search in less
certain regions. BoKA sets𝜉to 0.01, as per a study [ 27]. As𝜉signif-
icantly impacts BoKA’s efficiency, we explore its effects by experi-
menting with values of {0.001,0.01,0.011}, focusing on how these
variations influence the number of iterations in BoKA selector.
1001011021030.650.70 Accuracy(12, 0.733)
Iterations:32(a) PubMedQA
1001011021030.650.70 Accuracy(13, 0.736)
Iterations:57
100101102103
Number of iterations0.650.70 Accuracy(61, 0.734)
Iterations:611001011021031040.4750.5000.525(57, 0.53)
Iterations:209(b) MedMCQA
1001011021031040.4750.5000.525(79, 0.533)
Iterations:366
100101102103104
Number of iterations0.4750.5000.525(112, 0.531)
Iterations:425Maximal =0.001
=0.01
=0.011
Figure 5: Sampling results vs iteration rounds (exponentially scaled)
in different 𝜉values of BoKA, with the abscissa’s end indicating
total combination possibilities. Black stars denote peak accuracy of
student models from sampled combinations, and black dashed lines
show the stopping iteration round.
Results in Figure 5 reveal that with increasing 𝜉, iteration num-
bers grow but remain within 6% and 2.6% of all possible combina-
tions on the two datasets, demonstrating BoKA’s ability to reduce
sampling overhead by at least 94%. When tuned optimal (see the
second row in Figure 5), BoKA samples only 57 of 1,023 combina-
tions (5.6%) on PubMedQA and 366 of 16,383 (2.2%) combinations
4041KDD ’24, August 25–29, 2024, Barcelona, Spain Linzhu Yu, Huan Li, Ke Chen, and Lidan Shou
onMedMCQA, showing greater efficiency gain with larger sam-
pling spaces (e.g., MedMCQA). Despite varying 𝜉values, BoKA
consistently achieves high accuracy in generated student models,
indicating its effectiveness is not heavily relied on tuning 𝜉.
4.4 Plural KA vs Multi-teacher KD
To assess the plural KA method’s effectiveness in training stu-
dents (see Section 3.4), we replace it in BoKA with three distinct
multi-teacher KD strategies, each addressing teacher disagreement
differently: (1)MultiKDavg averages soft logits from all teachers
and utilizes them as soft labels for student training, equalizing each
teacher’s label contribution. (2)MultiKDmax selects the highest-
scoring class from the teachers’ predictions as the hard label for
training, making each teacher’s contribution dependent on their
prediction accuracy. (3)MultiKDwei, as described in a study [ 57],
averages logits for soft labels and weights the distillation loss for
each text sample based on teacher disagreement levels, thus also
equalizing each teacher’s label contribution.
Table 3: Plural KA vs Multi-teacher KD on PubMedQA.
Metho
d #(Student Heads)Label ShapeAccuracy Iterations
Soft
Hard
MultiKDavg 𝑐(𝑐
,) ∖ 72.4 90
MultiKDmax 𝑐 ∖(𝑐,) 72.6 37
MultiKDwei 𝑐(𝑐,) ∖ 71.8 59
Plural KA (ours) 𝑐×𝑚(𝑐×𝑚,) (𝑐,) 73.6 57
Table 4: Plural KA vs Multi-teacher KD on MedMCQA.
Metho
d #(Student Heads)Label ShapeAccuracy Iterations
Soft
Hard
MultiKDavg 𝑐(𝑐
,) ∖ 53.3 352
MultiKDmax 𝑐 ∖(𝑐,) 53.1 130
MultiKDwei 𝑐(𝑐,) ∖ 52.8 228
Plural KA (ours) 𝑐×𝑚(𝑐×𝑚,) (𝑐,) 53.3 366
For fairness, these KD-based variants do not use manual labels.
Referring to Table 3, the results on PubmedQA reveal that BoKA
outperforms all variants with relatively few iteration rounds. BoKA
selects a combination with five teachers while the other KD-based
variants involve up to four, indicating these methods prefer fewer
teachers due to their limited capability in managing disagreements
among many teachers. This highlights BoKA’s superior ability with
𝑐×𝑚heads to leverage knowledge from multiple teachers simul-
taneously, enhancing student model learning. In contrast, Mul-
tiKDavg andMultiKDwei ignore individual teacher predictions,
while MultiKDmax uses predictions from only one teacher per text.
Furthermore, despite MultiKDwei’s effectiveness with labels else-
where [ 57], it underperformed on our task, hinting that its weighted
loss approach might hinder training without labels.
Besides, we also report the evaluation results of BoKA and three
multi-teacher KD methods on MedMCQA. Referring to Table 4, the
results on MedMCQA reveal that BoKA performs as well or better
than these three KD methods with relatively more iteration rounds
but remains within 2.2% of all possible combinations (366 of 16,383).
4.5 Visual Analysis
We proceed to examine the rationality of the combination finally
selected by BoKA from a visual perspective on PubMedQA and
Yest!t"t#t$t%t&t!t"t$(c) Selected teachers (Accuracy: 0.736; Iterations:57)Not't(t&t)t*t'(d) Selected teachers when some domains lack teachers (Accuracy: 0.722; Iterations: 8)
(a) Distribution of data volume in different domains(b) Accuracy of students generated by singular combinations from different domains
Figure 6: Visual analysis on PubMedQA, includes: (a) question pro-
portions and (b) corresponding student model accuracy for domains
𝐴∼𝐺,𝑀,𝑁, and𝑍; (c) teacher models chosen by BoKA; (d) model
selections when certain domains lack corresponding teacher models.
t!t"t#t$t%t&t't!t"t$t(t)t'(c) Selected teachers (Accuracy: 0.533; Iterations:366)t*t+t(t,t-t.t)t%No(d) Selected teachers when some domains lack teachers (Accuracy: 0.523; Iterations: 25)
(a) Distribution of data volume in different domains(b) Accuracy of students generated by singular combinations from different domains
Yes
Figure 7: Visual analysis on MedMCQA, includes: (a) question pro-
portions and (b) corresponding student model accuracy for domains
𝐴∼𝑁; (c) teacher models chosen by BoKA; (d) model selections
when certain domains lack corresponding teacher models.
MedMCQA. Referring to Figure 6 (c), the teacher models selected
byBoKA are highlighted in purple, while unselected ones are in
light grey. Notably, we derive that BoKA considers both the data
volumes (cf. Figure 6 (a)) and the teacher models’ capabilities (cf.
Figure 6 (b)) of different domains in its selection. Specifically, it
selects teachers 𝑡𝐸,𝑡𝐺, and𝑡𝑀for their significant data proportions
and strong teaching performance in their domains. Besides, 𝑡𝐷
and𝑡𝐹are chosen for their ability to produce high-quality student
models, despite having less data in their domains. However, despite
larger question volumes in domains 𝐵and𝐶, their teacher models 𝑡𝐵
and𝑡𝐶were not chosen. Our further analysis reveals that model 𝑡𝑀
outperformed others in these domains, indicating that the selection
strategy effectively covers the educational needs in 𝐵and𝐶, hence
the non-selection of 𝑡𝐵and𝑡𝐶. Detailed accuracy evaluations of
teachers across different domains can be found in Appendix B.1.
We also examine scenarios with missing teacher models for
certain domains by randomly removing half of the teachers and
forcing BoKA to select from the rest. As per Figure 6 (d), after 8
iterations, BoKA chose 4 out of 5 teachers, resulting in a student
with a performance of 0.722, surpassing the 0.708 measure from
using all 5 remaining teachers. This indicates BoKA’s effectiveness
even with limited teacher model availability. Notably, BoKA ex-
cluded𝑡𝐴, which has low data proportions, aligning with the model
selection criteria we observed. However, the student’s performance
in this scenario was slightly lower than when more teachers were
available, highlighting the benefit of having a broader range of
teachers for improving student performance.
Similarly, in MedMCQA, we again confirm that BoKA considers
both data volumes (cf. Figure 7 (a)) and the teacher models’ capabil-
ities (cf. Figure 7 (b)) when selecting the combination of teachers.
4042BoKA: Bayesian Optimization based Knowledge Amalgamation for Multi-unknown-domain Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain
To be more specific, it selects teachers 𝑡𝐴,𝑡𝐵,𝑡𝐷,𝑡𝐸,𝑡𝐼,𝑡𝐾, and𝑡𝑁
for their large portions of questions and strong capability of the
teacher in their respective domains. Besides, 𝑡𝑁is chosen for its
ability to produce high-quality student models, despite having less
data in its domains. However, despite larger question volumes in do-
main𝐶, its teacher model 𝑡𝐶was not selected. The further analysis
in Appendix B.1 reveals that teacher 𝑡𝐷,𝑡𝑁, and𝑡𝐾outperformed
𝑡𝐶in domain𝐶, indicating that the selection strategy effectively
covers the educational needs in 𝐶, hence the non-selection of 𝑡𝐶.
In Figure 7 (d), we also examine scenarios with randomly missing
teachers for certain domains. After 25 iterations, BoKA chose 6 out
of 7 teachers, resulting in a student with a performance of 0.523,
surpassing the 0.511 measure from using all 7 remaining teach-
ers. Notably, BoKA excluded𝑡𝑁, which has low data proportions,
aligning with the model selection criteria we found.
5 RELATED WORK
Knowledge Amalgamation . Knowledge Amalgamation (KA) is
one of the model-reusing techniques pioneered by Shen et al. [ 41]
for unsupervised image classification. Given that multiple teacher
models specialize in distinct domains, KA aims to learn a stu-
dent model capable of comprehensive prediction across teacher-
specialized domains by leveraging predictions from these teacher
models in place of unavailable human annotations. Based on this
original KA conception, plenty of variant works are proposed to
fully harness the knowledge concealed in the trained teachers
[15,29,30,42,59,61,64]. Typically, the original KA method follows
two steps: (1) It amalgamates the hidden features from each layer
of teacher models incrementally. (2) The student model is trained
in a layer-wise manner on the amalgamated features and then all
layers are jointly finetuned by concatenating the teacher outputs.
However, the original KA method cannot be directly adapted for
text classification. The reason is that the feature amalgamation part,
designed for convolutional networks like AlexNet [ 20], is ill-suited
for pretrained language models such as Bert and GPT-2. Moreover,
the rigorous layer-by-layer parameter learning process becomes
prohibitively expensive when applied to large pretrained language
models. To address the real-world challenge of multi-unknown-
domain text classification, our BoKA advances two steps beyond
the existing KA techniques. First, BoKA extends the KA method
from the visual domain to the text classification task; Second, BoKA
introduces a Bayesian optimization based strategy for efficiently
and automatically estimating the optimal teacher combination.
Multi-teacher Knowledge Distillation (KD). KD was proposed
in early studies like [ 12] and serves as a technique for model com-
pression in a single-teacher-single-student manner. Recent advance-
ments [ 3,5,7,24,25,46,52] have expanded the original KD to a
multi-teacher architecture, where the student model concurrently
assimilates knowledge from two or more teacher models [ 13]. In
the realm of natural language processing, Yang et al. [ 60], Wu et al.
[56], and Wu et al. [ 57] all focus on training a student model to em-
ulate multiple teachers on an identical task. The referenced teacher
models are all trained on the same dataset only with varied distri-
butions. Wu et al. [ 57] further explore label-free distillation, and
others remain reliant on annotated data. Both Clark et al. [ 5] and
Liu et al. [ 25] delve into the multi-task setting, where the studentmodel learns from diverse teacher models, each trained on unique
textual tasks, and human labels simultaneously. The common aspi-
ration is to develop a model competent across multiple different text
tasks, ranging from question paraphrasing and textual similarity
to linguistic acceptability. However, multi-teacher KD methods, as
listed above, cannot directly address the multi-unknown-domain
text classification problem as they focus more on combining than
orchestrating teacher models. Moreover, our experiments in Sec-
tion 4.4 confirm that employing a multi-KD approach within our
framework leads to less than optimal results.
6 CONCLUSION
This paper proposes BoKA, a novel framework for automatic knowl-
edge amalgamation in multi-unknown-domain text classification,
leveraging Bayesian optimization to select optimal teacher mod-
els for training superior student models. Experiments show BoKA
matches or exceeds moderate zero-shot and finetuned LLMs in ac-
curacy, while being highly efficient, sampling only 5.6% and 2.2%
of all combinations on the two real datasets.
Future directions include refining teacher selection to the level of
text samples for enhanced performance, devising advanced pruning
strategies to reduce the search space early on, and adapting BoKA
for scenarios with composite domains (e.g., sports + medical).
ACKNOWLEDGMENTS
The work was supported by the Pioneer R&D Program of Zhejiang
(No.2024C01021), the Zhejiang Province "Leading Talent of Tech-
nological Innovation Program" (No. 2023R5214), the Fundamental
Research Funds for the Central Universities, and the Major Re-
search Program of Zhejiang Provincial Natural Science Foundation
(No. LD24F020015). The authors are supported by Hangzhou High-
Tech Zone (Binjiang) Institute of Blockchain and Data Security.
REFERENCES
[1]Yusuf Arslan, Kevin Allix, Lisa Veiber, Cedric Lothritz, Tegawendé F Bissyandé,
Jacques Klein, and Anne Goujon. 2021. A comparison of pre-trained language
models for multi-class text classification in the financial domain. In WWW Com-
panion. 260–268.
[2]James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms
for hyper-parameter optimization. NeurIPS 24 (2011), 2546–2554.
[3]Yevgen Chebotar and Austin Waters. 2016. Distilling knowledge from ensembles
of neural networks for speech recognition. In Interspeech. 3439–3443.
[4]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
et al.2021. Evaluating large language models trained on code. arXiv preprint
arXiv:2107.03374 (2021).
[5]Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning,
and Quoc Le. 2019. BAM! Born-Again Multi-Task Networks for Natural Language
Understanding. In ACL. 5931–5937.
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[7]Shangchen Du, Shan You, Xiaojie Li, Jianlong Wu, Fei Wang, Chen Qian, and
Changshui Zhang. 2020. Agree to Disagree: Adaptive Ensemble Knowledge
Distillation in Gradient Space. In NeurIPS, Vol. 33. 12345–12355.
[8]Stefan Falkner, Aaron Klein, and Frank Hutter. 2018. BOHB: Robust and efficient
hyperparameter optimization at scale. In ICML. 1437–1446.
[9]Peter I Frazier. 2018. A Tutorial on Bayesian Optimization. STAT 1050 (2018), 8.
[10] Jochen Görtler, Rebecca Kehlbeck, and Oliver Deussen. 2019. A Visual Exploration
of Gaussian Processes. Distill (2019).
[11] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann,
Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno K Bressem. 2023.
MedAlpaca–An Open-Source Collection of Medical Conversational AI Models
and Training Data. arXiv preprint arXiv:2304.08247 (2023).
4043KDD ’24, August 25–29, 2024, Barcelona, Spain Linzhu Yu, Huan Li, Ke Chen, and Lidan Shou
[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015).
[13] Chengming Hu, Xuan Li, Dan Liu, Xi Chen, Ju Wang, and Xue Liu. 2022.
Teacher-Student Architecture for Knowledge Learning: A Survey. arXiv preprint
arXiv:2210.17332 (2022).
[14] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.
2019. PubMedQA: A Dataset for Biomedical Research Question Answering. In
EMNLP-IJCNLP. 2567–2577.
[15] Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao.
2021. Amalgamating Knowledge from Heterogeneous Graph Neural Networks.
CVPR (2021), 15704–15713.
[16] Donald R Jones, Matthias Schonlau, and William J Welch. 1998. Efficient global
optimization of expensive black-box functions. Journal of Global optimization 13
(1998), 455–492.
[17] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos,
and Eric P Xing. 2018. Neural architecture search with bayesian optimisation
and optimal transport. NeurIPS 31 (2018).
[18] Marc C Kennedy and Anthony O’Hagan. 2001. Bayesian calibration of computer
models. Journal of the Royal Statistical Society: Series B (Statistical Methodology)
63, 3 (2001), 425–464.
[19] Kamran Kowsari, Donald E Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi,
Matthew S Gerber, and Laura E Barnes. 2017. Hdltex: Hierarchical deep learning
for text classification. In ICMLA. IEEE, 364–371.
[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. NeurIPS 25 (2012).
[21] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning
of language representations. arXiv preprint arXiv:1909.11942 (2019).
[22] Zhuoran Li, Chunming Hu, Xiaohui Guo, Junfan Chen, Wenyi Qin, and Richong
Zhang. 2022. An unsupervised multiple-task and multiple-teacher model for
cross-lingual named entity recognition. In ACL. 170–179.
[23] Valentin Liévin, Christoffer Egeberg Hother, and Ole Winther. 2022. Can large
language models reason about medical questions? arXiv preprint arXiv:2207.08143
(2022).
[24] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020. Ensemble
Distillation for Robust Model Fusion in Federated Learning. In NeurIPS, Vol. 33.
2351–2363.
[25] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Improving
multi-task deep neural networks via knowledge distillation for natural language
understanding. arXiv preprint arXiv:1904.09482 (2019).
[26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[27] Daniel James Lizotte. 2008. Practical bayesian optimization. (2008).
[28] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and
Tie-Yan Liu. 2022. BioGPT: generative pre-trained transformer for biomedical
text generation and mining. Briefings in Bioinformatics 23, 6 (2022), bbac409.
[29] Sihui Luo, Wenwen Pan, Xinchao Wang, Dazhou Wang, Haihong Tang, and
Mingli Song. 2020. Collaboration by competition: Self-coordinated knowledge
amalgamation for multi-talent student learning. In ECCV. 631–646.
[30] Sihui Luo, Xinchao Wang, Gongfan Fang, Yao Hu, Dapeng Tao, and Mingli Song.
2019. Knowledge amalgamation from heterogeneous networks by common
feature learning. In AAAI. 3087–3093.
[31] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing
Nie. 2023. Biomedgpt: Open multimodal generative pre-trained transformer for
biomedicine. arXiv preprint arXiv:2308.09442 (2023).
[32] Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David K Evans,
and Taylor Berg-Kirkpatrick. 2022. An empirical analysis of memorization in
fine-tuned autoregressive language models. In EMNLP. 1816–1826.
[33] The National Library of Medicine. 2024. MeSH: Medical Subject Headings. https:
//www.nlm.nih.gov/mesh/meshhome.html
[34] OpenAI. 2022. ChatGPT. https://openai.com/blog/chatgpt
[35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022.
Training language models to follow instructions with human feedback. NeurIPS
35 (2022), 27730–27744.
[36] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.
Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain
question answering. In CHIL. 248–260.
[37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al .2018.
Improving language understanding by generative pre-training. (2018).
[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al. 2019. Language models are unsupervised multitask learners. OpenAI Blog
1, 8 (2019), 9.
[39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits oftransfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research 21, 1 (2020), 5485–5551.
[40] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Fre-
itas. 2015. Taking the human out of the loop: A review of Bayesian optimization.
Proc. IEEE 104, 1 (2015), 148–175.
[41] Chengchao Shen, Xinchao Wang, Jie Song, Li Sun, and Mingli Song. 2019. Amalga-
mating Knowledge towards Comprehensive Classification. In AAAI. 3068–3075.
[42] Chengchao Shen, Mengqi Xue, Xinchao Wang, Jie Song, Li Sun, and Mingli Song.
2019. Customizing Student Networks From Heterogeneous Teachers via Adaptive
Knowledge Amalgamation. In ICCV. 3503–3512.
[43] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won
Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.
2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023),
172–180.
[44] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou,
Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al .2023. To-
wards expert-level medical question answering with large language models. arXiv
preprint arXiv:2305.09617 (2023).
[45] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian
Optimization of Machine Learning Algorithms. In NeurIPS. 2960–2968.
[46] Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, and Weijian Sun.
2020. Feded: Federated learning via ensemble distillation for medical relation
extraction. In EMNLP. 2118–2128.
[47] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to fine-tune bert
for text classification?. In CCL. Springer, 194–206.
[48] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong,
replicable instruction-following model. , 7 pages.
[49] Augustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G Krishnan, Barry B Rubin,
and Bo Wang. 2023. Clinical Camel: An Open-Source Expert-Level Medical
Language Model with Dialogue-Based Knowledge Encoding. arXiv preprint
arXiv:2305.12031 (2023).
[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 (2023).
[52] Devesh Walawalkar, Zhiqiang Shen, and Marios Savvides. 2020. Online Ensemble
Model Compression Using Knowledge Distillation. In ECCV. 18–35.
[53] Kai Wang, Yu Liu, Qian Ma, and Quan Z Sheng. 2021. Mulde: Multi-teacher
knowledge distillation for low-dimensional knowledge graph embeddings. In
WWW. 1716–1726.
[54] Zihan Wang, Peiyi Wang, Lianzhe Huang, Xin Sun, and Houfeng Wang. 2022.
Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach
for Hierarchical Text Classification. In ACL. 7109–7119.
[55] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi
Xie. 2023. PMC-LLaMA: Towards Building Open-source Language Models for
Medicine. arXiv preprint arXiv:2305.10415 6 (2023).
[56] Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. 2021. One Teacher is Enough?
Pre-trained Language Model Distillation from Multiple Teachers. In Findings of
the Association for Computational Linguistics: ACL-IJCNLP 2021. 4408–4413.
[57] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2022. Unified and
effective ensemble knowledge distillation. arXiv preprint arXiv:2204.00548 (2022).
[58] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023. Pmc-
llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454
(2023).
[59] Donglin Xie, Ruonan Yu, Gongfan Fang, Jie Song, Zunlei Feng, Xinchao Wang,
Li Sun, and Mingli Song. 2022. Federated Selective Aggregation for Knowledge
Amalgamation. arXiv preprint arXiv:2207.13309 (2022).
[60] Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. 2020. Model com-
pression with two-stage multi-teacher knowledge distillation for web question
answering system. In WSDM. 690–698.
[61] Jingwen Ye, Yixin Ji, Xinchao Wang, Kairi Ou, Dapeng Tao, and Mingli Song.
2019. Student Becoming the Master: Knowledge Amalgamation for Joint Scene
Parsing, Depth Estimation, and More. In CVPR. 2829–2838.
[62] Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. 2023. Chat-
doctor: A medical chat model fine-tuned on llama model using medical domain
knowledge. arXiv preprint arXiv:2303.14070 (2023).
[63] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. 2018. Towards au-
tomated deep learning: Efficient joint neural architecture and hyperparameter
search. arXiv preprint arXiv:1807.06906 (2018).
[64] Haofei Zhang, Feng Mao, Mengqi Xue, Gongfan Fang, Zunlei Feng, Jie Song,
and Mingli Song. 2023. Knowledge Amalgamation for Object Detection With
Transformers. IEEE Transactions on Image Processing 32 (2023), 2093–2106.
4044BoKA: Bayesian Optimization based Knowledge Amalgamation for Multi-unknown-domain Text Classification KDD ’24, August 25–29, 2024, Barcelona, Spain
A B C D E F G M N Z
T eacher modelsZ N M G F E D C B ASubdomains0.654 0.641 0.654 0.615 0.686 0.641 0.699 0.705 0.635 0.615
0.652 0.627 0.62 0.595 0.639 0.601 0.677 0.665 0.646 0.589
0.688 0.667 0.683 0.669 0.681 0.635 0.691 0.71 0.647 0.619
0.697 0.636 0.661 0.667 0.691 0.648 0.721 0.697 0.654 0.606
0.678 0.643 0.678 0.617 0.687 0.635 0.713 0.696 0.687 0.556
0.694 0.671 0.69 0.667 0.688 0.649 0.711 0.713 0.661 0.618
0.715 0.715 0.726 0.749 0.726 0.648 0.743 0.765 0.676 0.648
0.687 0.669 0.698 0.677 0.69 0.648 0.695 0.721 0.656 0.618
0.69 0.669 0.688 0.663 0.685 0.647 0.708 0.714 0.659 0.615
0.707 0.68 0.713 0.673 0.707 0.687 0.72 0.713 0.653 0.6530.5750.6000.6250.6500.6750.7000.7250.750
Figure 8: Accuracy of teacher models ( 𝑡𝐴∼𝑡𝐺,𝑡𝑀,𝑡𝑁, and𝑡𝑍) on
test sets across different domains on PubMedQA.
A B C D E F G H I J K L M N
T eacher modelsN M L K J I H G F E D C B ASubdomains0.725 0.758 0.725 0.708 0.75 0.742 0.725 0.733 0.708 0.675 0.7 0.75 0.733 0.717
0.865 0.743 0.77 0.797 0.77 0.743 0.784 0.743 0.743 0.784 0.797 0.784 0.797 0.77
0.469 0.502 0.462 0.413 0.393 0.419 0.459 0.419 0.389 0.439 0.396 0.469 0.485 0.449
0.595 0.624 0.61 0.61 0.595 0.59 0.58 0.624 0.522 0.615 0.644 0.62 0.59 0.58
0.566 0.555 0.486 0.532 0.503 0.48 0.486 0.543 0.468 0.572 0.526 0.538 0.468 0.543
0.306 0.318 0.306 0.288 0.256 0.256 0.26 0.278 0.3 0.256 0.275 0.279 0.23 0.303
0.651 0.61 0.589 0.631 0.606 0.585 0.606 0.622 0.564 0.635 0.598 0.635 0.643 0.614
0.492 0.557 0.516 0.508 0.5 0.459 0.541 0.516 0.5 0.516 0.549 0.492 0.475 0.508
0.752 0.76 0.76 0.713 0.767 0.76 0.721 0.783 0.674 0.752 0.736 0.744 0.783 0.729
0.605 0.617 0.613 0.613 0.663 0.621 0.609 0.621 0.535 0.601 0.617 0.588 0.617 0.556
0.419 0.466 0.496 0.47 0.444 0.436 0.44 0.415 0.41 0.427 0.509 0.457 0.453 0.436
0.412 0.377 0.415 0.424 0.371 0.38 0.395 0.386 0.412 0.427 0.418 0.398 0.389 0.415
0.528 0.52 0.531 0.509 0.504 0.509 0.469 0.518 0.52 0.515 0.534 0.501 0.491 0.534
0.651 0.6 0.624 0.597 0.607 0.559 0.597 0.631 0.566 0.627 0.634 0.614 0.634 0.603
0.30.40.50.60.70.8
Figure 9: Accuracy of teacher models ( 𝑡𝐴∼𝑡𝑁) on test sets across
different domains on MedMCQA.
A THE DETAILS OF COMPETITOR MODELS
We describe the competitor models evaluated in our effectiveness
comparison in Section 4.2 (on Page 7). Besides the manual KA meth-
ods (Group 1), the other methods are organized in the following
two groups.
Group 2 (Zero-shot LLMs):
•Chat-Doctor [62] is a specialized language model refined using
100k patient-doctor dialogues and equipped with a self-directed
information retrieval mechanism.
•MedAlpaca [11] is a model finetuned upon the LLaMA variant
with 13B parameters using the MedAlpaca dataset following
Taori et al. [48].
•LLaMA-2 [51] is a set of large language models, specifically opti-
mized for dialogue use cases, ranging from 7B to 70B parameters.
•ChatGPT [34] is a language model developed by OpenAI, de-
signed for generating human-like text based on given prompts.•ClinicalCamel [49] is an open large language model tailored for
clinical research, aiming to foster transparent research and safe
integration of LLMs in healthcare.
•InstructGPT [35] is an advanced model developed by finetuning
GPT-3 using human feedback to better align with user intent.
•Codex [4] is a GPT language model by OpenAI, finetuned on
code from GitHub that significantly specializes in generating and
understanding code.
Group 3 (LLMs Fintuned with Specific Datasets):
•BioGPT [28], using the GPT-2medium as the backbone network, is
generated by pretraining on large-scale biomedical literature.
•LLaMA-FT is obtained by finetuning LLaMA-7B [ 50] on the com-
bination of the training set from PubMedQA andMedMCQA
according to a study [58].
•PMC-LLaMA [58], based on LLaMA-7B, is acquired by first fine-
tuning on 4.8 million biomedical academic papers, and then con-
tinuing to finetune on specific downstream training data.
•BioMedGPT [31] is a large language model composed of a mole-
cule and a protein encoder, a Llama2-7B-Chat trained on biomed-
ical literature, and two modality adaptors, which unifies the
feature spaces of molecules, proteins, and natural language.
B ADDITIONAL EXPERIMENTAL RESULTS
B.1 Accuracy of Teacher Models on Test Sets
across Different Domains
The experiments in this section are a complement to those in Sec-
tion 4.5 (on Page 8) in the main paper. We evaluate each teacher
model on test sets across different domains on PubMedQA and
MedMCQA. In the two heatmaps shown in Figure 8 and Figure 9,
the abscissa represents the teacher model in each domain, and the
ordinate represents the domains.
We observe that teacher models may not always perform best in
their respective domains. Some stronger teacher models, such as 𝑡𝐺
and𝑡𝑀in Figure 8, outperform other teacher models across most
domains. On the highly difficult dataset of MedMCQA, almost all
teacher models perform poorly in certain domains, for example
domains𝐶,𝐷, and𝐼. Besides, we find that no single teacher model
excels in all domains on MedMCQA. This observation suggests that
the differences among the domains within MedMCQA are quite
significant, potentially making it difficult for a single teacher model
to perform well across all of them. Consequently, it is of interest to
explore designing a model with comprehensive domain knowledge.
B.2 BoKA Applied to Other Areas and
Extra-large-scale Classification Scenarios
To explain the applicability of BoKA in other areas, we have added
a new dataset from the scientific area, Web of Science (WOS) [ 19],
a multi-domain text classification dataset comprising 46985 docu-
ments with 141 labels and 7 different domains. Following Wang, et
al. [54], we preprocessed and divided WOS into training, validation,
and test sets, using Micro-F1 as the metric. The training set is split
into 7 domain-specific sections, each used to train a separate teacher
model denoted 𝑡𝐴∼𝑡𝐺. We use the validation set without labels for
student training. Both teacher and student models are built on the
same architecture: BERT followed by a fully connected layer. The
final results of BoKA on the WOS dataset are shown in Table 5. In
4045KDD ’24, August 25–29, 2024, Barcelona, Spain Linzhu Yu, Huan Li, Ke Chen, and Lidan Shou
Table 5: BoKA on the additional WOS dataset.
Iteration Teacher combination Micro-F1 of Student Model
1{𝑡𝐴,𝑡𝐵,𝑡𝐶,𝑡𝐷,𝑡𝐸,𝑡𝐹,𝑡𝐺} 76.89
2{𝑡𝐴,𝑡𝐵,𝑡𝐶,𝑡𝐷,𝑡𝐹,𝑡𝐺} 78.33
3{𝑡𝐴,𝑡𝐶,𝑡𝐷,𝑡𝐹,𝑡𝐺} 77.32
4{𝑡𝐴,𝑡𝐵,𝑡𝐶,𝑡𝐷,𝑡𝐺} 78.04
5{𝑡𝐴,𝑡𝐶,𝑡𝐷,𝑡𝐺} 76.65
Table 6: Compare BoKA with manual KA on the WOS dataset.
Method Teacher combination Micro-F1 of Student Model
Manual KA{𝑡𝐴} 68.08
{𝑡𝐵} 59.92
{𝑡𝐶} 68.65
{𝑡𝐷} 70.25
{𝑡𝐸} 45.30
{𝑡𝐹} 61.49
{𝑡𝐺} 63.55
{𝑡𝐴,𝑡𝐵,𝑡𝐶,𝑡𝐷,𝑡𝐸,𝑡𝐹,𝑡𝐺} 76.89
BoKA{𝑡𝐴,𝑡𝐵,𝑡𝐶,𝑡𝐷,𝑡𝐹,𝑡𝐺} 78.33addition to evaluating all singular combinations in the profiler mod-
ule,BoKA terminated after 5 rounds of selection-sampling iterative
processes. It finally selected combination {𝑡𝐴,𝑡𝐵,𝑡𝐶,𝑡𝐷,𝑡𝐹,𝑡𝐺}and
the produced student model with a Micro-F1 of 78.33 as the output.
Besides, we compare BoKA with the manual KA method mentioned
in Section 4.2. As shown in Table 6, the student model produced
byBoKA performs better than any singular combination and the
combination including all teacher models.
C LIMITATIONS
It is important to note that BoKA is designed for text classification
scenarios with a fixed number of classification classes. This means
that the number of classification classes handled by teacher models
from different (sub)domains is the same, and these teacher models
share the same number of classification classes as the new datasets
covering multiple unknown (sub)domains.
4046