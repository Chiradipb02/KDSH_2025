Learning to Rank for Maps at Airbnb
Malay Haldar
Airbnb, Inc.
San Francisco, CA, USA
malay.haldar@airbnb.comHongwei Zhang
Airbnb, Inc.
San Francisco, CA, USA
hongwei.zhang@airbnb.comKedar Bellare
Airbnb, Inc.
San Francisco, CA, USA
kedar.bellare@airbnb.com
Sherry Chen
Airbnb, Inc.
San Francisco, CA, USA
sherry.chen@airbnb.comSoumyadip Banerjee
Airbnb, Inc.
San Francisco, CA, USA
soumyadip.banerjee@airbnb.comXiaotang Wang
Airbnb, Inc.
San Francisco, CA, USA
xiaotang.wang@airbnb.com
Mustafa Abdool
Airbnb, Inc.
San Francisco, CA, USA
moose.abdool@airbnb.comHuiji Gao
Airbnb, Inc.
San Francisco, CA, USA
huiji.gao@airbnb.comPavan Tapadia
Airbnb, Inc.
San Francisco, CA, USA
pavan.tapadia@airbnb.com
Liwei He
Airbnb, Inc.
San Francisco, CA, USA
liwei.he@airbnb.comSanjeev Katariya
Airbnb, Inc.
San Francisco, CA, USA
sanjeev.katariya@airbnb.com
Abstract
As a two-sided marketplace, Airbnb brings together hosts who own
listings for rent with prospective guests from around the globe.
Results from a guest’s search for listings are displayed primarily
through two interfaces: (1) as a list of rectangular cards that contain
on them the listing image, price, rating, and other details, referred
to as list-results (2) as oval pins on a map showing the listing price,
called map-results. Both these interfaces, since their inception, have
used the same ranking algorithm that orders listings by their book-
ing probabilities and selects the top listings for display. But some of
the basic assumptions underlying ranking, built for a world where
search results are presented as lists, simply break down for maps.
This paper describes how we rebuilt ranking for maps by revising
the mathematical foundations of how users interact with search re-
sults. Our iterative and experiment-driven approach led us through
a path full of twists and turns, ending in a unified theory for the two
interfaces. Our journey shows how assumptions taken for granted
when designing machine learning algorithms may not apply equally
across all user interfaces, and how they can be adapted. The net
impact was one of the largest improvements in user experience for
Airbnb which we discuss as a series of experimental validations.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671648CCS Concepts
•Retrieval models and ranking →Learning to rank; •Human-
centered computing →Graphical user interfaces ;•Electronic
commerce→Online shopping .
Keywords
Search ranking, Map search, e-commerce
ACM Reference Format:
Malay Haldar, Hongwei Zhang, Kedar Bellare, Sherry Chen, Soumyadip
Banerjee, Xiaotang Wang, Mustafa Abdool, Huiji Gao, Pavan Tapadia, Liwei
He, and Sanjeev Katariya. 2024. Learning to Rank for Maps at Airbnb. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 9 pages. https://doi.org/10.1145/3637528.3671648
1 Introduction
As of June 01, 2024, there are more than 7.7million active Airbnb
listings in over 100thousand cities and towns worldwide. The
total number of guest arrivals, contributed by guests all across the
world, now exceeds 1.5billion. The vast majority of these guest
arrivals started with a search, the core mechanism that connects
guests to hosts. What makes Airbnb search a particularly strong
case for machine learning application is its global scale, coupled
with the individuality of each listing. Every listing has its unique
location, along with its own look and feel. Furthermore, even the
most popular listing can only be booked a maximum of 365days in
a year, so memorizing top results and replaying them back is not
an option. Airbnb search demands a truly generalized learning of
what each listing is offering.
And true to expectations, artificial intelligence shines on this
occasion, far surpassing what human intelligence can achieve. As
part of an evaluation exercise, ranking engineers were shown pairs
of listings and asked to identify which listing out of the pair was
5061
KDD ’24, August 25–29, 2024, Barcelona, Spain Malay Haldar et al.
Figure 1: Search box with destination, check-
in/checkout dates and guest count as inputs.
Figure 2: Search results as map pins.
Figure 3: Search results as list of
cards.
booked by a searcher. The ranking model correctly identified the
booked listing 88%of the time, while ranking engineers could man-
age only 70%.
This level of performance wasn’t achieved overnight. The launch
of our first neural network model is described in [ 10], and its evo-
lution is captured by [11], [1], [16], and [9].
The ranking tech stack described in these publications powers
searches from two sources. First is the familiar search box, where
a destination location, check-in/checkout dates, and guest counts
are entered explicitly by the searcher (Figure 1). Results from the
search can be accessed as list-results (Figure 3), and map-results
(Figure 2). The user can then point the map to an area, and the
latitude and longitude boundaries of the map serve as a second
source of searches with implicit query parameters. Overall the
search box generates 20%of searches, the rest coming from maps.
The main goal of these interfaces is to maximize the number
of bookings—the key measure of success both from the guest and
the host perspective. To achieve this goal, listings are sorted by
their booking probabilities, and the top listings are selected for
display as cards in a list and as map pins. The algorithm works by
ensuring that the higher the booking probability of a listing, the
more attention it receives from users. A formal analysis is presented
in Section 3 of [ 9]. This algorithm remained the status quo for many
years. But as it happens, disruptions come unannounced.
2 Map ≠List
Normalized discounted cumulative gain (NDCG) is our de facto tool
for evaluating ranking. To understand ranking performance in vari-
ous settings, we look at NDCG for that query segment. Comparing
the NDCG for queries from the search box against the NDCG for
map-generated queries uncovers a puzzle—a stubborn gap of 2%.
Figure 4: X-axis: Search rank for list-results. Y-axis: Normalized click-through rates.
For many years, engineers suspected the tech stack powering
search through maps ought to include something unique to the
interface itself. But this mostly drove feature engineering focused
on maps, such as distance of the listing from the map center. The
general wisdom was that as long as all the necessary features were
supplied to the model, it would “do the right thing.” There wasn’t
much to explore beyond that.
Although map search interfaces are used by billions every day,
there is little mention of how to customize search for maps in the
machine learning literature. To the best of our knowledge, there
are some references from the early days of the internet, like [ 19],
but there seems to be no research trail for the last twenty years.
Discussion of ranking for products similar to Airbnb ([ 14], [4], [17])
give no special consideration to maps. This lack of prior publications
further supports the thinking that map search is nothing special.
And yet, we have hints that searching using maps leads to a different
user behavior.
To shift tactics, we move away from trying to equate NDCG
across the two interfaces, and instead ask whether we should even
fixate on the NDCG for map-results? For list-results, our observa-
tions are in line with those reported in the past. A plot of click-
through rate per ranking position in Figure 4 visualizes the decay
of user attention for list-results, in agreement with established re-
search findings ([ 6], [15]). Section 3 in [ 9] describes how this mono-
tonic decay of user attention is key to the workings of NDCG.
But what about map-results, where the decay of attention is
no longer applicable? Since user attention decay is at the heart of
NDCG, the metric is no longer meaningful in the context of maps.
In fact, ranking itself is irrelevant for map-results, as there is no
sequential list involved.
To be clear, ranking still plays the crucial role of selecting the
top listings for display from all the listings available in the map
area. But once the top listings are selected, their relative ordering
is irrelevant. In the next section, we experimentally validate the
hypothesis that ranking doesn’t matter for map-results.
2.1 Experimental Results
The validation is a simple online A/B experiment where the treat-
ment randomly shuffles the top results for searches originating
from the map. The experiment is restricted to mobile platforms like
iOS™ and Android™ where users cannot interact with both list-
results and map-results at the same time. The case of desktop web
browsers, where concurrent access to list-results and map-results is
possible, will be discussed later in Section 6. From past experiments,
5062Learning to Rank for Maps at Airbnb KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 5: X-axis: Search rank for list-results and map-results. Y-axis: Normalized click-
through rates.
we know such a randomization applied to queries from the search
box can produce a jaw-dropping booking loss of 8% and degrade
NDCG by as much as 5%. The hypothesis is that for map searches,
randomization will produce no difference.
And this experiment indeed confirms the hypothesis. Metrics
in treatment are almost identical to control, and more specifically,
there is no difference in bookings across treatment and control.
This firmly establishes that for searches through the map, ranking
the top listings has no effect at all.
To show that ranking is irrelevant for map pins, an alternative
to the randomization experiment is to plot a rank vs. CTR plot
for map-results, along the lines of Figure 4. For map-results, we
expect the listing ranks to have no connection to user attention,
and hence the CTR for each position to be the same, making the
plot a straight horizontal line. Instead we get Figure 5! The CTR by
ranking position for map-results continues to slope downwards.
We resolve the mystery of this seeming contradiction in Sec-
tion 7. For now, let’s take the hypothesis we proved through the
randomization experiment and put it into action.
3 Less Is More
Consider a very simplified view of the probability of booking a list-
ing via a given query 𝑄. All listings eligible for the query are sorted
by their booking probabilities and the top 𝑁selected for display.
We use the following notation:
{𝑙1,𝑙2,...,𝑙 𝑁}: Listings ranked 1through𝑁.
𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖): The relative attention received at rank 𝑖.
𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖): The probability of booking for 𝑙𝑖.
We write the probability of booking a listing via 𝑄using an
approach similar to Section 5.1 of [12] and Section 3 of [9]:
𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄)=𝑁∑︁
𝑖=1𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)∗𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖) (1)
For list-results, 𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)>𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑗)∀𝑖<𝑗([8], [13]).
Ordering the listings by 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖)maximizes𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄), as
it iteratively matches the largest probability of booking with the
largest user attention. Disrupting this ordering lowers 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄),
and hence lowers the observed bookings when tested online.
If𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)>𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑗)∀𝑖<𝑗was true for map-results
as well, matching of 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖)with𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)would be sub-
optimal in the randomization experiment, causing 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄)to
Figure 6: X-axis: Number of distinct map pins clicks. Y-axis: Percent of map clickers
covered. 95% of users click≤12map pins.
drop, resulting in a bookings loss for the treatment. But since book-
ings didn’t drop in the randomization experiment, the property
𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)>𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑗)∀𝑖<𝑗must be false for map-results.
Instead, the following must hold 𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)=𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑗)∀𝑖,𝑗∈
{1,...,𝑁}. To represent that user attention is distributed equally
across the top listings displayed as map pins, we assign 𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)=
1/𝑁, which transforms Equation 1 for maps to:
𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄)=1
𝑁∗𝑁∑︁
𝑖=1𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖) (2)
Property 1. The probability of booking a listing from a map-
result is given by the average booking probability of the map pins.
Given that listings with the highest booking probabilities are
selected as map pins, and because of Property 1, the lower the
number of pins, the higher the average booking probability. This
leads to the Less Is More principle of map-results: the lesser the
number of pins shown, the more the bookings.
Applied by itself, the Less Is More principle drives down the
number of pins towards the degenerate case of a single pin on the
map, as that maximizes the average booking probability. So our
simplistic view needs a counterbalance which recognizes that users
often click through multiple pins. Figure 6 shows the distribution
of the number of distinct pins clicked by searchers. A 95%coverage
suggests around 12pins. This indicates the optimal user experience
requires much fewer than 18pins, the fixed number of pins in use
for years.
While the arguments above suggest that lowering the number
of pins below 18might improve user experience, they leave the
important question unanswered—that is, how to determine the op-
timal number of pins for a particular map-result? Intuitively, to
construct a map-result we start with the listing that has the highest
booking probability, and hence must always be shown on the map.
If subsequent listings have comparable booking probabilities, then
adding them as map pins provides the user with choice, without
degrading the average booking probability too much. However, if
booking probabilities of the listings that follow drop significantly,
then adding such pins only increase the chance of the user exhaust-
ing their attention on such pins, and leaving without booking. We
put this intuition into action in Algorithm 1, which we refer to as
theBookability Filter.
Given two listings 𝑙𝑥and𝑙𝑦, their corresponding ranking model
outputs𝑙𝑜𝑔𝑖𝑡(𝑙𝑥)and𝑙𝑜𝑔𝑖𝑡(𝑙𝑦)are related to the booking probabil-
ities by Equation 3. An in-depth discussion of Equation 3 can be
5063KDD ’24, August 25–29, 2024, Barcelona, Spain Malay Haldar et al.
found in Section 2 of [9].
𝑙𝑜𝑔𝑖𝑡(𝑙𝑥)−𝑙𝑜𝑔𝑖𝑡(𝑙𝑦)=𝑙𝑜𝑔(𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑥)/𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑦)) (3)
It is convenient to specify the filtering condition in terms of the
logits since they are directly available as outputs of the ranking
model.
Algorithm 1: Bookability Filter
Input : A set of𝑇listings𝐿𝑖𝑛𝑝𝑢𝑡 ={𝑙1,𝑙2,...,𝑙 𝑇}
1 Filter parameter 𝛼
Output: A set of𝑁≤𝑇listings𝐿𝑜𝑢𝑡𝑝𝑢𝑡 ={𝑙1,𝑙2,...,𝑙 𝑁}
2𝑙𝑚𝑎𝑥←argmax(𝑙𝑜𝑔𝑖𝑡(𝑙𝑖):𝑖=1..𝑇)
3for𝑘←1until𝑇do
4 if𝑙𝑜𝑔𝑖𝑡(𝑙𝑚𝑎𝑥)−𝑙𝑜𝑔𝑖𝑡(𝑙𝑘)<𝛼then
5𝐿𝑜𝑢𝑡𝑝𝑢𝑡←𝐿𝑜𝑢𝑡𝑝𝑢𝑡∪𝑙𝑘
6 end if
7end for
For an intuitive understanding of the condition on line 4 of
Algorithm 1, we rewrite it using Equation 3 as:
𝑙𝑜𝑔𝑖𝑡(𝑙𝑚𝑎𝑥)−𝑙𝑜𝑔𝑖𝑡(𝑙𝑘)<𝛼
𝑙𝑜𝑔(𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑚𝑎𝑥)/𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑘))<𝛼
𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑚𝑎𝑥)/𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑘)<𝑒𝛼
𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑚𝑎𝑥)/𝑒𝛼<𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑘)(4)
The Bookability Filter admits a listing as a map pin only if its
booking probability is greater than 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑚𝑎𝑥)/𝑒𝛼. The smaller
the value of 𝛼, the fewer the number of pins on the map; and hence
higher their average booking probability, but lower the number
of choices available. The 𝛼parameter makes the number of map
pins dynamic and fine tuned for each map-result, taking into ac-
count the booking probabilities of the listings at hand. There is no
simple analytical way to infer the optimal value of 𝛼that balances
both average booking probability and choice. Therefore we shift to
determining 𝛼empirically using online experiments.
3.1 Experimental Results
In the first phase, we study the effects of the 𝛼parameter through
offline simulation of the search system. Table 1 summarizes the ag-
gregate statistics of map-results corresponding to different values
of𝛼. The lower bound of the exploration is set by product experi-
ence considerations, and the upper bound the result of diminishing
effects. The baseline for all these comparisons is map-results with
no filtering and a fixed limit of 18pins.
𝛼 1.0 2.0 4.0 8.0
Number of map pins −39%−25%−9%−1%
Average booking probability 47% 26% 8% 0 .7%
Average total price −19%−16%−11%−3%
Average number of reviews 14% 10% 4% 0 .7%
Average review rating 0.05% 0.09% 0.13% 0.03%
Table 1: Offline exploration of 𝛼compared against a baseline with no filtering, which
is conceptually equivalent to 𝛼=∞.Table 1 validates that the Bookability Filter is having the intended
effect on map-results. The second phase of our testing investigates
how users react to various values of 𝛼. We run multiple A/B experi-
ments online, where control applies no filtering on map-results, and
treatments apply the Bookability Filter with different values of 𝛼.
Table 2 summarizes the effect of 𝛼on searchers. A brief explanation
of the key metrics evaluated in the online experiments:
•Uncanceled bookings: This is the top line metric, the number
of bookings made by searchers that were not cancelled.
•5-star trips: Trips booked by searchers that resulted in 5-star
rating after checkout, evaluated 120days after end of experi-
ment.
•Average impressions to discovery: The average number of dis-
tinct search results that a booker saw before clicking the listing
that was booked. This measures the cognitive load of making a
booking.
•Average clicks to discovery: The number of distinct search
results clicked by a booker before clicking the listing that was
booked. This is an alternative measure of the cognitive load of
making a booking.
𝛼 1.0 2.0 4.0
Uncanceled bookings 1.7% 1.1% 0.35%
5-star trips 1.6% 1.0% 0.2%
Avg impressions to discovery −14%−9.6%−4.0%
Avg clicks to discovery −5.7%−2.5%−0.25%
Table 2: Exploring 𝛼through online A/B experiments.
In the final phase, we fix the value of 𝛼to1.0corresponding to
maximum user benefit, and repeat the online A/B experiment at a
larger scale, allocating 34million searchers worldwide to each of
control and treatment. This grinds the p-value of the key metrics
below 10−5. Uncanceled bookings increase by 1.9%, measured as
a percentage of overall global bookings at Airbnb, making it one
of the largest improvements launched over the last several years.
5-star trips increase by 2%indicating not only growth in bookings,
but a growth in quality bookings. Average number of results seen
by the searcher before clicking on the booked listing reduces by
−16%, while search results clicked drop by −6.8%. The reduction
in effort to locate the booked listing, due to removal of inferior
choices, is the key mechanism driving the gain in bookings.
4 Making It Robust
The Bookability Filter puts a lot of faith in the booking probability
of the topmost listing, denoted as 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑚𝑎𝑥)in Inequality 4.
We refer to it as the anchor booking probability. At times, the anchor
booking probability may be an outlier. An abnormally high anchor
booking probability can make it difficult for the rest of the listings
to pass through the Bookability Filter, limiting the choice of listings
and making searchers quit prematurely.
Gathering insights by debugging a few such cases, we test a
more robust way to compute the anchor booking probability—by
considering the average of top 3 listings instead of relying on the
topmost listing alone. Since median is more resilient to outliers
than averages, we further fortify the anchor booking probability to
5064Learning to Rank for Maps at Airbnb KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 7: X-axis: Number of total listings ranked. Y-axis: Rank of the listing supplying
the anchor booking probability, tuned through empirical evaluations.
be the median of the top 3 listings. This shifts the anchor booking
probability from the topmost listing to the second-highest listing.
4.1 Experimental Results
The online A/B test for improving robustness applies filtering with
𝛼fixed at 1.0 for both control and treatment map searches. The
difference is that in control, the topmost listing supplies the anchor
booking probability, while in treatment it is the second listing from
the top. This increases the number of map pins in treatment by
6%compared to control, indicating a milder filtering. Searchers to
listing viewers conversion improves by 0.15%with a p-value less
than 10−4, with no negative effects on any of the other metrics. The
increase in searchers continuing their journey shows that correcting
for outliers moderates the cases where filtering was restricting
choice for searchers.
Inspired by the success of this experiment we do one final it-
eration: instead of fixing the anchor booking probability to the
second-highest listing, we make it dependent on the total number
of listings in the map area. The reasoning is that the larger the num-
ber of listings ranked, more the chances of getting hit by outliers.
A plot of the rank of anchor booking probability as a function of
the total listings ranked is shown in Figure 7. This further increases
the number of map pins by 2.6%without degrading any metrics,
thus balancing between optimizing average booking probability
and providing as many choices as possible.
5 Focus vs. Urgency?
The blockbuster results of Section 3.1 calls for celebration. But they
also raise some eyebrows in the room questioning the cause behind
such a humongous booking gain. While we can see users clicking
less to discover the booked listing, and overall bookings increasing,
the metrics don’t communicate what the users are experiencing.
The optimistic theory is that restricting the map pins is directing
the user’s attention to the most viable choices, reducing distraction.
So users are experiencing focus. This would mean we are on track
for the celebrations.
The alternative theory is that restricting the number of pins is
making the users think that not many choices are left for them to
book, and what the users are experiencing is urgency. This would
be enough to cancel the party.
The trouble is—how to tell apart the two possibilities?5.1 Experimental Results
We answer the riddle through an online experiment. Let’s walk
through an example map-result to understand the experiment de-
sign. Suppose there are 1000 listings available in a map area. After
ranking these 1000 listings by their booking probability, the base-
line algorithm picks the top 18to show as map pins. Consider the
case where the Bookability Filter restricts the number of pins to 14,
leading to a booking gain. The urgency hypothesis is that reducing
the number of pins from 18to14makes the user panic, which is
the reason behind the booking increase. The focus hypothesis is
that the user has a much higher chance of discovering a bookable
listing among the 14when compared to sifting through the 18. The
test to differentiate between the two hypotheses is as follows:
•𝐶𝑜𝑛𝑡𝑟𝑜𝑙 : Show the 18 map pins.
•𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 1: Restrict the map pins to 14, select based on the
highest booking probabilities from the baseline 18.
•𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2: Restrict the map pins to 14, matching the number
of pins from 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 1, but select randomly from the baseline
18.
𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 1and𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2reduce the number of pins by the
same amount, but their average booking probability of map pins is
different. There are three possible outcomes of this test, depending
on the causes underlying the booking gain:
•Urgency fully responsible for booking gain: If the booking
gain in𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 1is completely due to the user experiencing
urgency, then the user will experience the same urgency in
𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2. Both𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 1and𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2will show the
same booking gain over 𝐶𝑜𝑛𝑡𝑟𝑜𝑙 in this scenario.
•Urgency partially responsible for booking gain: If urgency pro-
vides partial explanation for the booking gain in 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 1,
then a similar gain is expected in 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2as well. We will
see a positive booking gain in 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2over𝐶𝑜𝑛𝑡𝑟𝑜𝑙 , but
less than the gain in 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 1.
•Urgency not at all responsible for booking gain: If the user is
not experiencing any urgency in 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 1, then bookings
in𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2will also be devoid of any urgency-based lift,
and will be determined by the quality of listings alone. The
bookings in 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2will therefore be equal to or less than
the bookings in 𝐶𝑜𝑛𝑡𝑟𝑜𝑙 .
The verdict from the online A/B test is very clear—that urgency is
not at all responsible for the booking gain. Bookings for 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 2
drop by 1.5%compared to 𝐶𝑜𝑛𝑡𝑟𝑜𝑙 , with p-value below 10−6.
6 Tiered User Attention
When using Airbnb search on web browsers, the results are laid
out as a grid of listing cards on the left. On the right the results are
displayed on a map (Figure 8). Consistency of product experience
demands that all the listings shown in the grid, in total 18, must
have a corresponding pin on the map. Lowering the number of map
pins is not applicable in this scenario.
We adapt the insights from Section 3 to the constraints imposed
by desktop browsers by creating two tiers of pins: in addition to
the ovals with price, we create a smaller oval pin without the price
display. We refer to them as mini-pins. Figure 9 depicts a map-
result with regular pins and mini-pins. The mini-pins draw less
user attention by design; click-through rates for mini-pins is 8times
5065KDD ’24, August 25–29, 2024, Barcelona, Spain Malay Haldar et al.
Figure 8: Simultaneous presentation of list-result and map-result on desktop browsers.
Figure 9: Map result with mix of regular pins and mini-pins.
less than regular map pins. Differentiating the pins into two tiers
allow us to write 𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)in Equation 1 as:
𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)= 
1 if𝑙𝑖a regular pin
1/8 if𝑙𝑖a mini-pin
0 otherwise(5)
To optimize 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄)in Equation 1, listings with the highest
booking probabilities are assigned regular pins on desktop browsers,
while listings with relatively lower booking probabilities are as-
signed mini-pins. This prioritizes user attention towards the listings
with higher chances of getting booked. The 𝛼parameter controls
the regular pin vs. mini-pin assignment, similar to the Bookability
Filter.
6.1 Experimental Results
We test the idea through an online A/B experiment where control
displays all 18listings from the list-result as regular pins on the
desktop map. Treatment applies the Bookability Filter to assign
listings satisfying Inequality 4 as regular map pins. The remaining
listings from the list-result are assigned mini-pins. The results in
Table 3 show the effectiveness of tiered user attention.
7 How Attention Flows On Maps
By distributing the user’s attention over all the map pins in equal
measure, Equation 2 led to the Less Is More principle and generated
strong improvements in user experience. Next, we refine our view𝛼 1.0
Uncanceled bookings 0.7%
5-star trips 0.5%
Avg impressions to discovery −21%
Avg clicks to discovery −1.7%
Table 3: Results from online A/B experiment applying
Bookability Filter to differentiate between regular pins
vs. mini-pins on desktop.
of how user attention flows across a map by taking inspiration from
the rank vs. CTR plots we constructed in Figure 4.
In Figure 4, the CTR by position on the list shows the decay of
user attention, going from the top towards the bottom. Explicit eye-
tracking based studies of search result pages by others ([ 15], [13])
report similar findings. What is the equivalent concept for a map? It
is the CTR by coordinates on the map. This generalizes the concept
of attention decay to two dimensions. Figure 10 shows these 2-D
plots of CTR on maps.
(a)iOS™ app
 (b)Android™ app
(c)Desktop web browser
 (d)Mobile web browser
Figure 10: The(𝑥, 𝑦)coordinates in each plot corresponds to coordinate on the map,
and the value at each coordinate represents the ratio of map pins clicked divided by
the number of map pins displayed.
The conclusion from Figure 10 leaps out immediately: user atten-
tion is maximum towards the center of the map, and decays radially
outwards. Figure 10c corresponding to the desktop platform has an
additional twist. There is a grid of listings displayed to the left of the
map (Figure 8), which exerts a leftward pull on the user’s attention.
Figure 10d for mobile web browsers exposes an issue: the topmost
listing card covers the bottom part of the map, making pins in this
region unreachable. The issue is fixed following this discovery.
By aggregating the click behavior of millions of searchers, these
plots give the relation between user attention and map coordinates
that is agnostic to the map’s contents. For any particular individual,
landmarks on the map influence the flow of attention as discussed
in [18].
These plots also solve the mystery posed in Section 2.1. Click-
through rates of map pins drop off with increasing distance from the
5066Learning to Rank for Maps at Airbnb KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 11: X-axis: Normalized distance from the map center. Y-axis: For a given
distance on the x-axis, we compute the average rank 𝑅𝑎𝑣𝑔of listings displayed at that
distance, then plot 𝑙𝑜𝑔(2)/𝑙𝑜𝑔(2+𝑅𝑎𝑣𝑔)on the y-axis.
Figure 12: X-axis: For list-results, the search rank of listings. For map-results, the
rank of the map pin by distance from the map center. Y-axis: For list-results the click-
through rates of listing cards in search results. For map-results, click-through rates of
map pins.
map center. Listing booking probabilities also go down in a similar
manner, as distance of a listing is an important feature contribut-
ing to its booking probability (see Figure 11). As a consequence,
listing rank is correlated with distance from the map center, with
higher ranked listings likely to appear closer to the center. This
makes the CTR of map pins decay with listing rank, creating the
similarity between the map-result and list-result curves in Figure 5.
But this similarity is superficial because searchers using the map
are completely oblivious to the listing’s rank, as proven by the ran-
domization experiment in Section 2.1.
Given the two modes of user attention decay, in lists going from
top to bottom, and in maps going from center to periphery, the
question that arises naturally—how do they compare? But since
we plot the decay of user attention in lists as a curve in Figure 4,
and as a 2-D surface for maps in Figure 10, a direct comparison
is problematic. To make the comparison, we take map-results and
order the map pins by their distance from the map center. This
ordering by distance assigns a rank to each map pin, which we use
to construct a rank vs. CTR plot for maps. Figure 12 puts the search
rank vs. CTR plot for lists next to the distance rank vs. CTR plot
for maps.
Search results presented as a list of cards look nothing like pins
scattered across a map. Yet, the manner in which user attention
decays across the two interfaces in Figure 12 look the same. This
suggests that the physical aspect of attention decay may be tied to
the particular interface, for instance 1-D vs. 2-D decay. But the rateof the attention decay may strongly depend on how much cognitive
load users are willing to take to process successive items, a factor
independent of the interface.
Figure 10 also suggests opportunity to optimize map-results tak-
ing into account the decay in user attention. We generalize Equa-
tion 1 for map-results as:
{𝑥(𝑙𝑖),𝑦(𝑙𝑖)}: Map coordinates for listing 𝑙𝑖.
{𝑥0,𝑦0}: Map center coordinates.
𝑐𝑡𝑟(𝑖,𝑗): Click-through rate at offset {𝑖,𝑗}w.r.t{𝑥0,𝑦0}.
𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑙𝑖): Relative attention for 𝑙𝑖.
𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑙𝑖)=𝑐𝑡𝑟(𝑥(𝑙𝑖)−𝑥0,𝑦(𝑙𝑖)−𝑦0)
𝑐𝑡𝑟(0,0)
𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄)=𝑁∑︁
𝑖=1𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑙𝑖)∗𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖)(6)
Looking back at Equation 2, it is a simplified approximation of
Equation 6, although a very effective one in practice. The ques-
tion now is—how to generate additional improvements based on
the insight from Figure 10? But optimizing map-results based on
Equation 6 turns out to be significantly harder than optimizing list-
results based on Equation 1. In Equation 1, relative attention only
depends on rank 𝑖and is independent of the listing. When deter-
mining the optimal listing for rank 𝑖, the ranking algorithm need
not worry about altering the user attention associated with rank 𝑖.
In Equation 6, selecting listing 𝑙𝑖influences both the booking prob-
ability𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖), as well as user attention 𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑙𝑖), because
the user attention is tied to the coordinates of 𝑙𝑖. If we have in to-
tal𝑡listings eligible for the query 𝑄, and have to find the optimal
choice of𝑘listings that maximizes 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄)in Equation 6, then
the brute force way would be to evaluate Equation 6 for all𝑡𝐶𝑘
subsets. Given the stringent latency budgets for search, crunching
this combinatorial complexity is simply impractical.
We make the problem tractable by first considering the queries
originating from the search box, where we have the freedom to con-
struct the map boundaries. Algorithm 2 presents a greedy heuristic
as a solution. The heuristic first fixes the choice of listings by se-
lecting them based on booking probabilities alone. It then explores
different map centers that optimize for user attention. Figure 13
gives a pictorial view of Algorithm 2. The heuristic simplifies the
problem by optimizing 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖)and𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑙𝑖)in a disjoint
manner. Overall complexity of the heuristic is 𝑂(𝑘2𝑛), where𝑘is
the map width divided by the 𝜖parameter, and 𝑛is the number of
output map pins.
Implicit search queries generated by the user while pointing the
map to an area are simpler to optimize, as they have fewer degrees
of freedom. The map area and the position of the candidate map
pins are fixed in such cases. Including a ranking model feature for
the distance of a pin from the map center is able to factor in the
decay of attention in Figure 10, since the booking label already
contains the information. To study this aspect, we compare ranking
models with and without the feature in the next section.
7.1 Experimental Results
For queries generated by the user’s map movements, the control of
the online A/B experiment is a ranking model that does not have
5067KDD ’24, August 25–29, 2024, Barcelona, Spain Malay Haldar et al.
Algorithm 2: Greedy heuristic for map-result optimization
Input : A set of𝑡listings𝐿𝑖𝑛𝑝𝑢𝑡 ={𝑙1,𝑙2,...,𝑙 𝑡}
1 Iteration step size 𝜖
Output: A set of𝑛≤𝑡listings𝐿𝑚𝑎𝑝={𝑙1,𝑙2,...,𝑙 𝑛}
2 Map center{𝑥0,𝑦0}
3𝐿𝑠𝑜𝑟𝑡𝑒𝑑←𝑆𝑜𝑟𝑡𝐵𝑦(𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖),𝑙𝑖∈𝐿𝑖𝑛𝑝𝑢𝑡)
4𝐿𝑚𝑎𝑝←{𝑙1,𝑙2,...,𝑙 𝑛,𝑙𝑖∈𝐿𝑠𝑜𝑟𝑡𝑒𝑑}
5𝑥𝑚𝑖𝑛,𝑥𝑚𝑎𝑥←𝑚𝑖𝑛(𝑥(𝑙𝑖)),𝑚𝑎𝑥(𝑥(𝑙𝑖)),𝑙𝑖∈𝐿𝑚𝑎𝑝
6𝑦𝑚𝑖𝑛,𝑦𝑚𝑎𝑥←𝑚𝑖𝑛(𝑦(𝑙𝑖)),𝑚𝑎𝑥(𝑦(𝑙𝑖)),𝑙𝑖∈𝐿𝑚𝑎𝑝
7𝑚𝑎𝑥𝐵𝑜𝑜𝑘𝑖𝑛𝑔,𝑥 0,𝑦0←−∞
8for𝑖←𝑥𝑚𝑖𝑛;𝑖<𝑥𝑚𝑎𝑥 ;𝑖←𝑖+𝜖do
9 for𝑗←𝑦𝑚𝑖𝑛;𝑗<𝑦𝑚𝑎𝑥 ;𝑗←𝑗+𝜖do
10𝑃𝑎𝑡𝑡𝑛(𝑙𝑘)←𝑐𝑡𝑟(𝑥(𝑙𝑘)−𝑖,𝑦(𝑙𝑘)−𝑗)
𝑐𝑡𝑟(0,0),𝑙𝑘∈𝐿𝑚𝑎𝑝
11𝑏𝑘𝑁𝑒𝑤←Í𝑃𝑎𝑡𝑡𝑛(𝑙𝑘)∗𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑘),𝑙𝑘∈𝐿𝑚𝑎𝑝
12 if𝑚𝑎𝑥𝐵𝑜𝑜𝑘𝑖𝑛𝑔 <𝑏𝑘𝑁𝑒𝑤 then
13 𝑥0←𝑖
14 𝑦0←𝑗
15 𝑚𝑎𝑥𝐵𝑜𝑜𝑘𝑖𝑛𝑔←𝑏𝑘𝑁𝑒𝑤
16 end if
17 end for
18end for
(a)Initial selection.
 (b)Evaluate potential centers.
(c)Locate optimal center.
 (d)Update center.
Figure 13: An example to illustrate the working of Algorithm 2. (a) Start with an initial
bounding box covering the map pins selected based on highest booking probabilities.
(b) Iterate evaluate candidates centers over a grid. (c) Locate the center that maximizes
𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑄)in Equation 6. (d) Update the bounding box and the map center.
the distance of a pin from the map center as a feature. Treatment
is a ranking model including that feature. Uncanceled bookings
increase by 0.27% in treatment, along with an increase of 2%in
click-through rates of map pins.For queries originating from the search box, we do an online
A/B experiment with Algorithm 1 as the control and the treatment
applies Algorithm 2 to further optimize the map. We observe an
increase of 0.39%in uncanceled bookings for new guests with a p-
value of 0.006. Users new to Airbnb find the placement of the most
bookable listings at the map center particularly helpful as their map
moves decrease by 1.5%, and their chance of encountering areas
with very few listings decrease by 0.8%.
The results indicate that the techniques discussed in Section 3
and Section 6 provide the biggest opportunities for optimization,
with the techniques discussed in Section 7 providing further incre-
mental gains.
8 Insights And Future Work
Models of user attention developed for lists do not apply for map
interfaces. This opens up a new research area where user attention
on maps can be modeled as:
•Distributed uniformly: Summarized as Equation 2, it leads to
Property 1 and the Bookability Filter in Section 3.
•Distributed discretely: Applicable for differentiated map pins
that are useful when outright filtering is not desirable. Captured
by Equation 5 in Section 6.
•Distributed continuously: Based on Figure 10 in Section 7, Equa-
tion 6 forms the basis for Algorithm 2.
Equation 1 is the foundation tying all these insights together. The
literature on improving ranking mostly focuses on the 𝑃𝑏𝑜𝑜𝑘𝑖𝑛𝑔(𝑙𝑖)
term in Equation 1, whereas this paper refines the 𝑃𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑖)
term for maps. This multi-year track of research generated some
of the most impactful launches in the history of search ranking at
Airbnb.
And yet, this is merely a start that points to several exciting re-
search potentials for the future. To name a few, significant work
has gone into making learning to rank unbiased for list-results
([12], [2], [3]). A similar need exists for map-results. Section 6
proved the effectiveness of separating the map pins into two tiers.
This suggests scope to further demarcate the map pins to direct
the user attention towards more relevant ones. For map-results,
the relative ordering of the top results is not as consequential. It
is possible that loss functions that take advantage of this aspect
and optimize for top-k precision ([ 5], [7]) outperform the straight-
forward pairwise cross-entropy loss. In Section 7 we described a
heuristic that was able to improve upon the baseline, raising hopes
for other heuristics that might generate further improvements.
Given this is a completely new area, the lack of established public
benchmarks is a challenge. Creating a dataset to help research in
this area is on our roadmap.
9 Acknowledgements
The work discussed is this paper was a large collaborative effort
bringing together multiple teams at Airbnb. We are grateful for the
contributions of Clarence Quah, Devansh Gupta, Evan Su, Ang Li,
Edward Lin, Linda Dreyer, Lily Zhao, Zhenyu Lai, Vivek Bhardwaj,
and Shanni Weilert. Their support made all the launches possible.
References
[1]Mustafa Abdool, Malay Haldar, Prashant Ramanathan, Tyler Sax, Lanbo Zhang,
Aamir Manaswala, Lynn Yang, Bradley Turnbull, Qing Zhang, and Thomas
5068Learning to Rank for Maps at Airbnb KDD ’24, August 25–29, 2024, Barcelona, Spain
Legrand. 2020. Managing Diversity in Airbnb Search. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining
(Virtual Event, CA, USA) (KDD ’20). 2952–2960.
[2]Qingyao Ai, Keping Bi, Cheng Luo, Jiafeng Guo, and W. Bruce Croft. 2018. Unbi-
ased Learning to Rank with Unbiased Propensity Estimation. In The 41st Interna-
tional ACM SIGIR Conference on Research & Development in Information Retrieval
(Ann Arbor, MI, USA) (SIGIR ’18). Association for Computing Machinery, New
York, NY, USA, 385–394. https://doi.org/10.1145/3209978.3209986
[3]Qingyao Ai, Tao Yang, Huazheng Wang, and Jiaxin Mao. 2021. Unbiased Learning
to Rank: Online or Offline? ACM Trans. Inf. Syst. 39, 2, Article 21 (feb 2021),
29 pages. https://doi.org/10.1145/3439861
[4]Lucas Bernardi, Themistoklis Mavridis, and Pablo Estevez. 2019. 150 successful
machine learning models: 6 lessons learned at booking. com. In Proceedings of
the 25th ACM SIGKDD international conference on knowledge discovery & data
mining. 1743–1751.
[5]L Berrada, A Zisserman, and P Mudigonda. 2018. Smooth loss functions for
deep top-k classification. In International Conference on Learning Representations
(ICLR), 2018.
[6]Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. 2008. An Exper-
imental Comparison of Click Position-Bias Models. In Proceedings of the 2008
International Conference on Web Search and Data Mining (Palo Alto, California,
USA) (WSDM ’08). 87–94.
[7]Camille Garcin, Maximilien Servajean, Alexis Joly, and Joseph Salmon. 2022.
Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced
classification. In International Conference on Machine Learning. PMLR, 7208–7222.
[8]Fan Guo, Chao Liu, Anitha Kannan, Tom Minka, Michael Taylor, Yi-Min Wang,
and Christos Faloutsos. 2009. Click Chain Model in Web Search. In Proceedings
of the 18th International Conference on World Wide Web (Madrid, Spain) (WWW
’09). Association for Computing Machinery, New York, NY, USA, 11–20.
[9]Malay Haldar, Mustafa Abdool, Liwei He, Dillon Davis, Huiji Gao, and Sanjeev
Katariya. 2023. Learning To Rank Diversely At Airbnb. In Proceedings of the 32nd
ACM International Conference on Information and Knowledge Management (Birm-
ingham, United Kingdom,) (CIKM ’23). Association for Computing Machinery,
New York, NY, USA, 4609–4615. https://doi.org/10.1145/3583780.3614692
[10] Malay Haldar, Mustafa Abdool, Prashant Ramanathan, Tao Xu, Shulin Yang,
Huizhong Duan, Qing Zhang, Nick Barrow-Williams, Bradley C. Turnbull, Bren-
dan M. Collins, and Thomas Legrand. 2019. Applying Deep Learning to AirbnbSearch. In Proceedings of the 25th ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining (Anchorage, AK, USA) (KDD ’19). 1927–1935.
[11] Malay Haldar, Prashant Ramanathan, Tyler Sax, Mustafa Abdool, Lanbo Zhang,
Aamir Mansawala, Shulin Yang, Bradley Turnbull, and Junshuo Liao. 2020. Im-
proving Deep Learning for Airbnb Search. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining (Virtual Event,
CA, USA) (KDD ’20). 2822–2830.
[12] Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. 2017. Unbiased
Learning-to-Rank with Biased Feedback. In Proceedings of the Tenth ACM Interna-
tional Conference on Web Search and Data Mining (Cambridge, United Kingdom)
(WSDM ’17). Association for Computing Machinery, New York, NY, USA, 781–789.
https://doi.org/10.1145/3018661.3018699
[13] Lori Lorigo, Maya Haridasan, Hrönn Brynjarsdóttir, Ling Xia, Thorsten Joachims,
Geri Gay, Laura Granka, Fabio Pellacini, and Bing Pan. 2008. Eye tracking and
online search: Lessons learned and challenges ahead. Journal of the American
Society for Information Science and Technology 59, 7 (2008), 1041–1052.
[14] Themis Mavridis, Soraya Hausl, Andrew Mende, and Roberto Pagano. 2020.
Beyond algorithms: Ranking at scale at Booking. com.. In ComplexRec-ImpactRS@
RecSys. Virtual.
[15] Alexandra Papoutsaki, James Laskey, and Jeff Huang. 2017. Searchgazer: Webcam
eye tracking for remote studies of web search. In Proceedings of the 2017 conference
on conference human information interaction and retrieval. 17–26.
[16] Chun How Tan, Austin Chan, Malay Haldar, Jie Tang, Xin Liu, Mustafa Abdool,
Huiji Gao, Liwei He, and Sanjeev Katariya. 2023. Optimizing Airbnb Search
Journey with Multi-Task Learning. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (Long Beach, CA, USA,)
(KDD ’23). Association for Computing Machinery, New York, NY, USA, 4872–4881.
https://doi.org/10.1145/3580305.3599881
[17] Raluca M Ursu. 2018. The power of rankings: Quantifying the effect of rankings
on online consumer search and purchase decisions. Marketing Science 37, 4 (2018),
530–552.
[18] Laura Wenclik and Guillaume Touya. 2023. Where do people look at during
multi-scale map tasks? AGILE: GIScience Series 4 (2023), 51.
[19] Joseph D Yates and Xiaofang Zhou. 2000. Searching the web using a map. In
Proceedings of the First International Conference on Web Information Systems
Engineering, Vol. 1. IEEE, 222–229.
5069