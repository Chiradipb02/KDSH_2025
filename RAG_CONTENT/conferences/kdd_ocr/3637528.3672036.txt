DipDNN: Preserving Inverse Consistency and Approximation
Efficiency for Invertible Learning
Jingyi Yuan
jyuan46@asu.edu
Arizona State University
Tempe, Arizona, USAYang Weng
Yang.Weng@asu.edu
Arizona State University
Tempe, Arizona, USAErik Blasch
erik.blasch@gmail.com
Air Force Research Lab
Arlington, Virginia, USA
Figure 1: Proposed DipDNN to address intricacies and limitations in data-driven inverse modeling.
ABSTRACT
Consistent bi-directional inferences are the key for many machine
learning applications. Without consistency, inverse learning-based
inferences can cause fuzzy images, erroneous control signals, and
cascading failure in SCADA systems. Since standard deep neural
networks (DNNs) are not inherently invertible to offer consistency,
some past methods reconstruct DNN architecture analytically for
one-to-one correspondence but compromise key features such as
universal approximation. Other work maintains the capability of
universal approximation in DNNs via iterative numerical approx-
imation. However, these methods limit their applications signif-
icantly due to Lipschitz conditions and issues of numerical con-
vergence. The dilemma of the analytical and numerical methods
is the incompatibility between nonlinear layer compositions and
bijective function construction for inverse modeling. Based on the
observation, we propose decomposed-invertible-pathway DNNs
(DipDNN). It relaxes the redundant reconstruction of nested DNN
in the former methods and eases the Lipschitz constraint. As a result,
we strictly guarantee the consistency of global inverse modeling
without harming DNN’s capability for universal approximation.
As numerical stability and generalizability are keys for controlling
critical infrastructures, we integrate contractive property with a
parallel structure for inductive biases, leading to stable performance.
Numerical results show that DipDNN performs significantly better
than past methods, thanks to its enforcement of inverse consistency,
numerical stability, and physical regularization.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672036CCS CONCEPTS
•Applied computing →Computer-aided design; •General
and reference→Estimation ;•Computing methodologies →
Neural networks; Supervised learning by regression.
KEYWORDS
Data-driven inverse modeling, inverse consistency, analytical in-
vertibility, inverse computation stability, universal approximation
capability
ACM Reference Format:
Jingyi Yuan, Yang Weng, and Erik Blasch. 2024. DipDNN: Preserving In-
verse Consistency and Approximation Efficiency for Invertible Learning .
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3672036
1 INTRODUCTION
Inferences can be classified into single-directional and bi-directional.
Single-directional inference in regression and classification has
witnessed significant advancements recently, primarily attributed
to the universal approximation capability inherent in deep neu-
ral networks [ 37,54]. However, bi-directional inferences can not
be treated as a superposition of two DNNs in opposite directions
due to the need for a consistent input-output-input relationship
[9,50,57]. Without the consistency properties, image generation
will be distorted, and control in cyber-physical systems can be infea-
sible, causing cascading failures [ 41,60]. For example, recovering
audio/image signals from data is the inverse process of regular
transformations, which will propagate distortions in output sig-
nals without highly accurate approximation [3]. For example, any
physical and engineering systems require estimating the hidden
states that recreate measurements matching the output of forward
systems [32].
4071
KDD ’24, August 25–29, 2024, Barcelona, Spain Jingyi Yuan, Yang Weng, & Erik Blasch
The key to bidirectional consistency is the one-to-one corre-
spondence, which is unavailable in single-directional DNNs due to
complex interconnections in the layered DNN structures. To main-
tain one-to-one correspondence, one way previous works adopted
is to tighten the flexible architecture and universal approximation
capability. For example, [ 19] divides the input and output equally
and restricts how the two output variable sets are coupled with
the input variable sets. Due to such restrictions in the analytical
approach, such methods lead to distorted image construction and
biased states in predictive simulation. On the other hand, some
approaches use the numerical inverse. The goal is to free the re-
striction so the DNNs can still have all the structures to conduct
universal approximation. However, such iterative approaches are
based on certain conditions, such as the Lipschitz constraint, in
order to let the iteration converge. The conditions limit the infer-
ence that the approach can handle significantly and take effects on
local inverse solutions [ 5]. Together with the convergence issue, the
computation requirements compromise performance guarantees in
the analytical methods.
Therefore, can the issues from the analytical and numerical meth-
ods be resolved while preserving their benefits, as we demonstrate
on the left of Fig. ??? The answer is yes. For the analytical methods,
they provide a closed-formed solution based on the bonding proof of
bijective mapping and inverse computation. The strict global invert-
ibility results from fixed splitting of input/output dimensions and
restrictive block separations, which retrograde the flexible architec-
ture and representation power of DNNs. While numerical inverse
approximation relaxes these restrictions, it maintains model flexi-
bility at the cost of accuracy and inverse consistency. Specifically,
the fixed-point algorithm converges to a local inverse solution and
relies on iterative approximation without guaranteeing an explicit
inverse solution.
Thus, we propose combining the advantages of both analytical
and iterative methods to enforce consistency guarantees and pre-
serve DNN’s approximation capability. However, directly merging
them is problematic because the bijective triangular mapping of
addictive coupling layers [ 19] conflicts with the flexible residual
neural network model [ 5]. To resolve such an issue, we decompose
the triangular mapping into basic units and construct wider and
deeper layers in analogy to regular DNN. While each unit remains a
linear pathway that hinders approximation capability, we adopt the
LeakyReLU activation function. It provides bijective and Lipschitz
continuous nonlinearity to boost the representation power without
breaking the bijection.
While the analytical one-to-one correspondence can be distorted
by numerical inverse computation, we not only embed contrac-
tive properties to mitigate the error propagating through layers
but also integrate strong inductive bias to reduce the extrapola-
tion errors originating from the source via an inverse-consistent
parallel structure. Together, the regularizations ensure stable com-
putation of the global inverse solution. Fig. ??presents the design
of how we retain the pros from the previous method and address
the cons, and we call it the decomposed invertible pathway deep
neural network (DipDNN), for which “Dip” vividly describes the
model passing on information among layers without loss. Our the-
oretical analysis demonstrates that DipDNN effectively reducesmodel redundancy/sparsity without compromising its approxima-
tion ability, simultaneously facilitating a computationally efficient
inverse solution.
We conduct experiments on various systems to assess data-
driven inverse modeling, focusing on inverse consistency, computa-
tional efficiency, and generalizability. When implementing previous
methods, they show numerical instability in inverse computation
with wider and deeper nested DNNs, varying convergence rely-
ing on the preset contraction condition, and poor extrapolation
performance facing unseen data samples. On the contrary, the com-
petitive performance of DipDNN validates the complementary ben-
efits based on previous analytical invertible learning and numerical
inverse approximation methods. The flexibility to integrate with
physical prior regularization can dramatically widen the application
fields for DNNs with bi-directional information flow.
2 INVERSE LEARNING MODELING
A deterministic forward mapping is defined as 𝑓:R𝑛→R𝑛with
𝒚=𝑓(𝒙). However, many real-life problems in physical systems
are interested in the opposite mapping direction, identifying hidden
states or original variables. So, we want to find an inverse mapping
𝑔𝜃that has consistency with the forward analysis 𝑓via
𝒙=𝑔𝜃(𝒚)=𝑓−1(𝒚),∀𝒚∈Y. (1)
Unlike forward mapping, which usually has a well-defined govern-
ing function, the inverse counterpart is much more complicated
to recover and match perfectly with the forward [ 4]. As inverse
consistency is difficult to achieve, inverse learning methods aim to
find an approximation of the forward mapping, 𝒚=𝑔−1
𝜃(𝒙), which
has a small empirical loss ℓ1
𝒚𝑖,𝑔−1
𝜃(𝒙𝑖)
based on historical data
{𝒙𝑖,𝒚𝑖}𝑁
𝑖=1. After training, the inverse solution is derived from in-
verting𝑔−1
𝜃(·). However, the problem with previous methods lies
in no guarantee of reaching zero reconstruction loss,
𝑔∗=argmin
𝜃𝑁∑︁
𝑖=1ℓ2
𝒙𝑖,𝑔−1
𝜃(𝑔𝜃(𝒙𝑖))
, (2)
while ensuring a generalizable function approximation of the for-
ward mapping.
2.1 Tackling the Drawbacks of Existing Methods
There are two main approaches in the literature. One provides an
analytical inverse solution; the other uses an iterative numerical
approximation to find the inverse outputs. In the following, we
will show when both methods can not fully address challenges in
inverse learning and the potential to redesign.
2.1.1 Losing Universal Approximation in Analytical Approaches.
For fitting two-way mapping, the autoencoder is a straightforward
method. It enforces inverse consistency by minimizing reconstruc-
tion error (2). However, they rarely reach zero error during training,
not to mention the testing phase, where generalizability is critical
in real-life systems. On the contrary, an analytical inverse ensures
zero reconstruction error for strict inverse consistency. Therefore,
our design shall preserve the one-to-one correspondence. We also
observe that neurons are forced to be split equally on the inputs
𝒙=[𝒙𝐼1,𝒙𝐼2]and the outputs 𝒚=[𝒚𝐼1,𝒚𝐼2]. This is because such
4072DipDNN: Preserving Inverse Consistency and Approximation Efficiency for Invertible Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
an algebraic trick creates a triangular map to ensure bijective func-
tion for inverse modeling via 𝒚𝐼1=𝑎𝒙𝐼1,𝒚𝐼2=𝑏𝒙𝐼2+𝑡(𝒙𝐼1);𝒙𝐼1=
𝒚𝐼1
𝑎,𝒙𝐼2=(𝒚𝐼2−𝑡(𝒙𝐼1))
𝑏. For example, the visualization in the first
layer of Fig. 2(a) explains how the coupling between 𝒙𝐼2and𝒚𝐼1is
eliminated to easily derive the closed-form inverse solution. But,
this structural requirement harms the universal approximation ca-
pability. Therefore, our design needs to avoid variable splitting in
nested architecture.
2.1.2 Losing Performance Guarantee in Numerical Approach. As
the analytical inverse form pushes restrictions on DNN architecture,
the other way is the numerical approximation method. Invertible
residual networks leverage the analogous form of residual layers
and backward Euler discretization. With the trained forward map-
ping, fixed-point iterations are used to compute the local inverse
solution. While it trades global and explicit inverse solutions for
flexible DNN representation, the forward ResNet is enforced with
Lipschitz constraints to obtain a contractive mapping. Therefore,
the result of fixed-point inverse computation depends on the Lips-
chitz condition and the contraction level of the forward mapping,
limiting the applicability to a wide variety of applications. For ex-
ample, computing the inverse of addictive coupling layers could
aggravate error propagation with more stacking layers, which will
be elaborated more in Section 4.2.
2.1.3 Lacking Regularization for Numerical Stability and Generaliz-
ability. Although there are drawbacks of the fixed-point method, we
observe that i-ResNet’s contractive property can be used to avoid
numerical invertibility in analytical methods, as analytical methods
still need to compute the inverse in consecutive layers within a
DNN. This creates scenarios of "divide by zero." In addition to the
need of numerical stability, we also need generalizability for data
beyond training ranges [ 24,45]. For example, we may need to ex-
trapolate new but unseen operation points of critical infrastructure
such as renewable energy systems. Such a generalizability prob-
lem causes problems for existing analytical methods and numerical
methods reviewed in the previous subsections.
3 PROPOSED METHODS
3.1 Decomposed Invertible Pathway Deep
Neural Networks
3.1.1 Constructing Invertible DipDNN. The challenge with earlier
approaches stems from the incompatibility between constructing
bijective function and compositing nonlinear interconnected layers
for inverse modeling. So, we aim to construct one-to-one correspon-
dence with minimum adjustment in DNN to maintain the dense
function representation for universal approximation.
As we illustrate in Sec. 2.1.1, the typical splitting design enables
bijection using a sparse structure, thus harming representation
power. To keep the dense representation in a regular neural net-
work layer: 𝒛=𝑔(𝑊𝒙+𝒃), it’s straightforward that only two
design choices are available to enforce invertibility for DNN layers:
1) activation function and 2) weights. For the nonlinearity 𝑔(·), the
activation is element-wise such that strict monotonicity is a nec-
essary and sufficient condition for a one-dimensional function to
be invertible. We propose using Leaky Rectified Linear Unit (Leaky
Figure 2: (a): Addictive coupling layers use nested DNNs for
invertible transformation and require alternative stackings
for full coupling of input/output dimensions. (b): Retain
triangular map for bijection and enable nonlinearity using
Leaky ReLU activation. (c) The proposed invertible DipDNN.
ReLU) activation, which is a strictly monotone function customized
from ReLU. As Fig. 2(b) presents, it could relax the nested DNN
while embedding nonlinearity. Moreover, Leaky ReLU is a contrac-
tive mapping function to avoid unstable computation of inverse
solutions.
To make the affine function 𝑊𝒙+𝒃bijective, the weight matrix
𝑊needs to be invertible. It indicates independent correlation over
all dimensions, where the one-to-one (injective) mapping means
full column rank of the matrix and onto (surjective) means full row
rank of the matrix. A non-singular square matrix always satisfies
such one-to-one correspondence, but singularity issues may exist
and cause difficulty in inverse computation. For this matter, we
are motivated by addictive coupling that creates a triangular map
in Jacobian for bijection. While the coarse architecture restricts
flexibility, we instead propose decomposing the triangular mapping
into basic units (from Fig. 2(a) to (b)) and constructing dense ar-
chitecture in analogy to regular NN (from Fig. 2(b) to (c)). A basic
invertible unit is shown in the top right corner of Fig. 2(c), which
can eliminate the singularity issue (details in Appendix 8.2).
As an extension of the basic invertible unit in depth and width,
we propose lower and upper triangular weight matrices to keep
layers invertible while maintaining coupling with all-dimension
variables. It can be seen as an equivalent adjustment of fully con-
nected layers using LU decomposition. Let 𝑔1be linear, and we
have𝑊=𝑊𝑡𝑟𝑖𝑙𝑊𝑡𝑟𝑖𝑢 and the easily computed matrix inverse
4073KDD ’24, August 25–29, 2024, Barcelona, Spain Jingyi Yuan, Yang Weng, & Erik Blasch
𝑊−1=𝑊−1
𝑡𝑟𝑖𝑢𝑊−1
𝑡𝑟𝑖𝑙layer-by-layer. While any triangular matrix
is invertible, if and only if all the entries on its main diagonal are
non-zero, we alternately enforce the lower and upper triangular
weight matrices in each block (3)to ensure the complete coupling
over all the dimensions.
Therefore, Fig. 2(c) presents our compact invertible DNN struc-
ture. Mathematically, the proposed model has 𝐾blocks, which
indicate 2𝐾layers. The representation for the 𝑘𝑡ℎblock is:
𝒛(2𝑘−1)=𝑔1(𝑊𝑘
𝑡𝑟𝑖𝑙𝒛 
2(𝑘−1)
+𝒃𝑘
1),
𝒛(2𝑘)=𝑔2(𝑊𝑘
𝑡𝑟𝑖𝑢𝒛(2𝑘−1)+𝒃𝑘
2).(3)
Each blockℎ(𝑘)consists of two layers 𝒛(2𝑘−1)and𝒛(2𝑘). The op-
timizable model parameters include weight matrices 𝑊𝑘
𝑡𝑟𝑖𝑙,𝑊𝑘
𝑡𝑟𝑖𝑢
and bias 𝒃𝑘
1,𝒃𝑘
2.𝑔1,𝑔2are element-wise nonlinear activation func-
tions, and we use Leaky ReLU activation with a fixed parameter
𝛼∈R+\{1},𝑔(𝑥)=𝜎𝛼(𝑥)=(
𝑥, if𝑥>0,
𝛼𝑥, if𝑥≤0,.
We summarize that such a redesign of DNN is invertible and
retains the inverse consistency of addictive coupling.
Proposition 1.The function of the neural network model ℎ:R𝑛→
R𝑛withℎ=ℎ(1)◦···◦ℎ(𝐾)is invertible if the weight matrices
𝑊𝑘
𝑡𝑟𝑖𝑙,𝑊𝑘
𝑡𝑟𝑖𝑢,𝑘∈ [1,𝐾]are lower and upper triangular matrices
with non-zero diagonal components, and all the activation functions
𝑔1
𝑘,𝑔2
𝑘are strictly monotonic.
As the proposed model is a deep neural network structure with
Decomposed Invertible Pathways layer-by-layer, we call it DipDNN,
where “dip” also stands for the lower and raised information flow
vividly.
3.1.2 Preserving Representation Power. With sufficient conditions
for invertibility, we aim to show that DipDNN escapes the dilemma
of inverse consistency and approximation efficiency. Compared
with Fig. 2(a), DipDNN relaxes the fixed input/output dimension
splitting, thus no need to stack multiple separate DNNs alterna-
tively (i.e., at least three layers and normally four or more) for full
couplings among groups. Meanwhile, instead of arbitrary nested
DNN, DipDNN enforces the number of neurons in all the layers to
be the same for strict one-to-one correspondence. Will this weaken
the representation power?
The universal approximation property of shallow wide networks
(fixed depth such as one hidden layer and arbitrary width) has been
well-studied, but it is still enduring work for deep narrow networks
(bounded width and arbitrary depth). Especially, our DipDNN is
a deep narrow network with weight matrices being alternatively
lower and upper triangular. Next, we present the preserved univer-
sal approximation property in the following Theorem 3.1.
Theorem 3.1. LetK⊂R𝑑𝑥be a compact set, then for any con-
tinuous function 𝑓∈𝐶(K,R𝑑𝑦), there is a positive constant 𝜖>0,
∥ℎ(𝑥)−𝑓(𝑥)∥<𝜖,for neural network ℎ:R𝑑𝑥→R𝑑𝑦, where
ℎ=ℎ(1)◦···◦ℎ(𝐾).ℎ(𝑘)is defined in (3)with Leaky ReLU activa-
tion function and alternative lower and upper triangular matrices,
𝑊𝑡𝑟𝑖𝑙for odd layers and 𝑊𝑡𝑟𝑖𝑢for even layers.
Proof. To describe the universal approximation of DNNs, we
say the DNNs ℎare dense in 𝐶(X,Y), if for any continuous function𝑓∈𝐶(X,Y), there is𝜖>0, such that∥ℎ(𝑥)−𝑓(𝑥)∥<𝜖. To prove
the universal approximation of DipDNN, we first refer to the latest
results on the deep narrow Networks with Leaky ReLU activations
as follows [22].
Theorem 3.2. LetK ⊂R𝑑𝑥be compact. Then the set of Leaky
ReLU-activated neural networks with fixed width 𝑑+1(𝑑𝑥=𝑑𝑦=𝑑)
and arbitrary depth is dense in 𝐶(K,R𝑑𝑦)𝐶(Ω,R𝑚).
Theorem 3.2 indicates that there exists a neural network ℎ𝜙of
lower bounded width 𝑑+1such that∥ℎ𝜙(𝑥)−𝑓(𝑥)∥<𝜖/2. To
convertℎ𝜙to networks with triangular weight matrices, we denote
the layer as ℎ𝜙(𝑥)𝑘=𝜎(𝑊𝑘ℎ𝜙(𝑥)(𝑘−1)+𝑏𝑘). Since the dimen-
sions in all layers are equal, each square matrix 𝑊𝑘,𝑘=1,·,𝐾
can be decomposed into a product of lower and upper triangular
matrices,𝑊𝑘=𝑊𝑘
𝑡𝑟𝑖𝑙𝑊𝑘
𝑡𝑟𝑖𝑢. The layer function turns to ℎ𝜙(𝑥)𝑘=
𝜎(𝑊𝑘
𝑡𝑟𝑖𝑙𝑊𝑘
𝑡𝑟𝑖𝑢ℎ𝜙(𝑥)(𝑘−1)+𝑏𝑘). Subsequently, we transfer ℎ𝜙(𝑥)𝑘
into two layers by first inserting an identity map 𝐼:R𝑑→R𝑑
and obtainℎ𝜙(𝑥)𝑘=𝜎(𝑊𝑘
𝑡𝑟𝑖𝑙𝐼𝑊𝑘
𝑡𝑟𝑖𝑢ℎ𝜙(𝑥)(𝑘−1)+𝑏𝑘). Then we ap-
ply some function 𝜌𝑘:R𝑑→R𝑑to approximate 𝐼withℎ𝜓(𝑥)𝑘=
𝜎(𝑊𝑘
𝑡𝑟𝑖𝑙𝜌𝑘𝑊𝑘
𝑡𝑟𝑖𝑢ℎ𝜙(𝑥)(𝑘−1)+𝑏𝑘). From the theorem on the identity
mapping approximation [ 42], we construct 𝜌𝑘to obtainℎ𝜓(𝑥)𝑘=
𝜎(𝑊𝑘
𝑡𝑟𝑖𝑙′𝜎(𝑊𝑘
𝑡𝑟𝑖𝑢′ℎ𝜙(𝑥)(𝑘−1)+𝑏𝑘′)+𝑏𝑘), where𝑊𝑘
𝑡𝑟𝑖𝑙′,𝑊𝑘
𝑡𝑟𝑖𝑢′are
scaled by𝜌𝑘, with structures remaining to be lower/upper trian-
gular matrices. The approximation of identity mapping can reach
arbitrary accuracy, and thus we have ∥ℎ𝜙(𝑥)−ℎ𝜓(𝑥)∥≤𝜖/2. Given
that∥ℎ𝜙(𝑥)−𝑓(𝑥)∥<𝜖/2, we obtain∥ℎ𝜓(𝑥)−𝑓(𝑥)∥<𝜖. Details
of Theorem 3.2 and the theorem on the identity mapping approxi-
mation are included in Appendix 8.2. □
The result shows that any continuous function 𝑓:R𝑑𝑥→
R𝑑𝑦can be approximated. To fit perfectly, DipDNN needs a slight
construction to only expand the input and output dimensions from
𝑑𝑥,𝑑𝑦to𝑑+1by filling in zeros without changing the property
[64].
4 REGULARIZATION FOR BOOSTING
PERFORMANCE IN DIPDNN
While the conditions mentioned earlier guarantee analytical invert-
ibility for a consistent inverse, the computation aspects of deep
learning may raise a lack of generalizability and numerical stability
issues. As supported by empirical observations, small numerical
errors from the forward mapping recovery are propagated through
layers of inverse computation. Such errors can be enlarged origi-
nating from the source due to poor extrapolation performance of
forward mapping on data beyond the training range. In the fol-
lowing, we demonstrate the regularization scheme to address the
source of error and error propagation, improving DipDNN training
for a stable computation of the global inverse solution.
4.1 Inductive Bias Embedding for
Generalizability of Inverse Modeling
As we focus on learning an invertible forward model, The objective
of forward learning is to minimize empirical errors, which hinge
4074DipDNN: Preserving Inverse Consistency and Approximation Efficiency for Invertible Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 3: A parallel structure for DipDNN regularization with
inductive bias is illustrated using a physical system example.
on the model’s approximation capability. However, universal ap-
proximators can have excellent performance on training data but
significant deterioration on out-of-distribution data. Such deteriora-
tion will be passed on to the inverse counterpart. The unpredictable
generalization is especially critical for physical systems that need
predictive modeling on changing operation points.
For many cases, the forward model 𝑓has specific inductive biases
(e.g., governed by physics priors) or exhibits specific properties (e.g.,
obeying logic constraints or unique patterns). Recent works on
physics-informed learning embed these properties into the DNNs
to improve the generalizability. However, when it comes to the
inverse problem, directly adding the symbolic embedding or extra
constraints can break the invertibility of the forward mapping.
Moreover, the exact underlying function may not naturally satisfy
one-to-one mapping, and inverse learning is only to approximate
partially based on the observed data, which may cause conflict.
Therefore, we propose a twin structure in Fig. 3. A physics em-
bedding is added in parallel with the DipDNN instead of embedding
it into the forward model. It is a sparse symbolic regression to re-
cover exact physical expression. For physical systems with known
priors, we could use predefined function forms as candidates. Oth-
erwise, we adopt a state-of-the-art model such as equation learner
to recover the complex expression [ 52]. Specifically, we define split
parameters to represent the hybrid representation of physics em-
bedding and DipDNN: 𝑓(𝒙)=𝜆Phy𝑓1(𝒙)+𝜆DipDNN𝑓2(𝒙), where
𝜆Phy+𝜆DipDNN =1,𝜆Phy,𝜆DipDNN >0.The hybrid models are
trained simultaneously to minimize empirical errors and recover
the underlying function. Since DipDNN is invertible, we obtain
ˆ𝒙=𝑓−1
1(𝒚)from the inverse counterpart and plug into the recov-
ered physical function 𝑓2(ˆ𝒙). It can be used to verify the physical
consistency of the forward approximation in DipDNN.
4.2 Numerical Regularization against Error
Explosion in Inverse Computation
Even though the forward model is analytically invertible, numerical
errors may be aggravated when computing the inverse solution.
Figure 4: Numerical regularization avoids exploding error
propagation of inverse computation in (a) and ensures nu-
merical invertibility of DipDNN in (b).
Here we show the inverse computation sensitivity on well-trained
DipDNNs ( MAPE <0.01%) of different depths using various syn-
thetic datasets (details in 8.3). We show in Fig. 4(a) the error prop-
agation through layers compared with the ablation error of each
invertible block via testing on synthetic datasets with different
dimensions and nonlinearity.
We observe an exponential increase in the propagated error while
the ablation error is nearly zero ( <10−10). The numerical errors
include round-off, forward approximation mismatches, data noises,
etc. If the singular values of the forward mapping approach zero
(without actually being zero, thus maintaining analytical invertibil-
ity), the singular values of the corresponding inverse mapping can
become exceedingly large and amplify numerical errors, which is
termed as an exploding inverse [ 6]. Fig. 4(left) empirically shows
that such errors will be aggravated and propagated as the problem
size and network depth increase.
We quantify the correlation between errors and inverse stability
using bi-Lipschitz continuity with full details in Appendix 8.3. Based
on that, we enforce moderate regularization in the inverse mapping.
Leaky ReLU is a typical 1-Lipschitz activation function for each
layer, and we adopt the 𝐿2norm of the inverse weights to smoothly
clip large entries. While it is a moderate bound to regularize bi-
Lipschitz continuity, the example in Fig. 4 (right) shows a much
smaller error (( <10−10)) propagated through layers.
5 RELATED WORK
5.1 DNN to Approximate Inverse Modeling
Considering the approximation strategies, DNN-based inverse learn-
ing includes direct mapping recovery and two-way mapping recov-
ery that unifies the pair of forward and inverse mappings. The in-
verse mapping is usually more complex than the forward [ 33]. Thus,
direct mapping easily leads to overfitting and a mismatch in be-
tween. For example, unlike the physical priors of the forward system
model, the inverse does not have a pre-defined physical form as a
reference for interoperability [ 50]. Therefore, some studies combine
forward and inverse learning together to match the bi-directional
information flow [ 3]. There are various ways to realize such unified
bi-directional learning: 1) minimizing the reconstruction errors to
4075KDD ’24, August 25–29, 2024, Barcelona, Spain Jingyi Yuan, Yang Weng, & Erik Blasch
approximate a pair of forward and inverse mappings [ 26,46] and
2) enforcing invertibility in the forward model [2, 19, 20].
For 1), the reconstruction error is unavoidable to ensure a matched
inverse. As DNNs are not one-to-one mappings naturally, 2) in-
cludes invertible designs that either nest the DNNs in a triangular
map or normalize the parameters for the Lipschitz constraint. The
former can obtain an analytical inverse at the cost of stacking mul-
tiple layers with nested DNNs for full representation power, which
aggravates error propagation [ 19]. The latter relaxes the restric-
tions on DNN architecture but relies on a fixed-point algorithm to
approximate the inverse after forward training [ 5]. The comparison
of different invertible models shows there is a trade-off between the
representation efficiency and inverse computation stability, which
is also supported by theoretical analysis [ 27]. In this paper, we make
an attempt to minimize the adjustment on standard DNNs with
respect to preserving the analytical inverse solution.
5.2 Build Inverse Consistency for State
Estimation
There are various inverse problems regarding the recovery of latent
variables from physical measurements, e.g., image generation and
recovery tasks in vision-related applications, extracting true states
from observation of physical/engineering systems for monitoring,
and building the relationship between variables and decisions for
planning and control [ 7,13,23,28,65]. Traditional works solve
such problems by iterative simulations, nearest search in a sub-
space, or optimization-based algorithms [ 35,48,58]. Typically, the
identification-based state estimation differs from the traditional
setting of state estimation, which has a completely accurate system
model. Instead, it is a blind scenario where only measurements are
available without knowing the full model [ 29,39,40]. Therefore,
previous work starts with model-free methods to approximate a
direct mapping for state estimation [12].
More works try to build in physical function in the forward map-
ping and conduct state estimation in the inverse simultaneously
using a variational autoencoder [ 21,26,31,46]. However, they do
not enforce strict one-to-one correspondence for inverse consis-
tency. Even though some generative models build bijectivity, the
learning mechanism does not fit most of the discriminative learning
tasks in physical/engineering systems, which have a more critical
requirement on accurate point estimates for both in-distribution
state restoration and extrapolation scenarios. Therefore, this paper
aims to show that strict one-to-one mapping is possible with proper
regularization.
5.3 Regularization for Inverse Learning
The performance of inverse learning is challenged in both directions
based on the accuracy-stability trade-off [ 27]. Therefore, many
regularization strategies are used to minimize extrapolation errors
and ensure stable inverse reconstruction. Typically, for systems
with prior knowledge, model-based regularizations include physics-
informed deep learning via physics loss embedding [ 9,34,50,55],
sparse symbolic regression yields law of parsimony (Occam’s razor)
[8,53], restrict relationships and dependencies between variables
[17, 25, 59, 66].While they solve specific problems effectively with strong priors,
the predefined physics bias and portion may limit DNN’s flexibility
to choose the optimal representation. Besides, the regularization
over the forward system identification may break the invertibility
for inverse computation. Recent works attempt meta-algorithms to
switch between a trusted physics agent and an untrusted black-box
expert for robustness-accuracy balance in safety-critical control
tasks [ 15,38]. Such emerging research inspired us to design a twin
structure to find the optimal integration of physics embedding and
DipDNN approximation without hurting invertibility for general
inverse problems.
6 NUMERICAL EVALUATION
In this section, we evaluate the capability of inverse consistency
and representation power from DipDNN on representative tasks
with diverse benchmark methods. These tasks include synthetic
examples, system identification-based state estimation, privacy-
preserving learning, and image restoration.
6.1 Experiment Settings
6.1.1 Datasets and Inverse Modeling Applications. We use both CPS
and non-CPS datasets.
•Synthetic Examples: We use synthetic datasets of symbolic
functions and elementary physical functions from [ 59]. The
problem size is from 2 variables to 9 variables. Such a dataset
is good for testing the generalizability of inverse modeling
due to explicit functions and data scarcity.
•Image Construction and Face Completion: These two tasks
have much higher dimensions, making the underlying map-
ping difficult to interpret. Specifically, we use MNIST and
CIFAR10 [ 18,36] for density estimation by NICE, transform-
ing from a simple distribution (logistic prior) to complex
images. For comparison, we adopt a discriminative setting to
sample input data using logistic distribution, where i-ResNet
computes the inverse mapping between inputs and the last
layer of feature extraction for classification. The face com-
pletion task constructs the left face from the right [47, 51].
•System Identification-based State Estimation: State Estima-
tion in Power System (PS) estimates inversely estimate volt-
age phasor states from measurements, such as power injec-
tions, branch power flows, and current magnitudes [ 56,62].
Also, false data injection attacks on the defender side con-
duct inverse learning by generating tampered measurements
[16]. Finally, we test inverse learning in a sonar system to de-
termine input parameters [ 32]. To validate the above, we use
real systems for simulations. Some of the data preparation
details are in Appendix 8.4.
6.1.2 Evaluation Metrics.
•Inverse consistency: Mean absolute percentage error (MAPE)
and mean square error (MSE) for bi-directional estimation.
•Representation Power: recovery rate (%) of functional param-
eters and extrapolation error on unseen operation points.
•Computational Efficiency: Running time (sec/epoch) under
the same architecture and data.
6.1.3 Benchmark Methods.
4076DipDNN: Preserving Inverse Consistency and Approximation Efficiency for Invertible Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 5: The results of face image completion, showing DipDNN avoids blurring and distorted features.
•Autoencoder: Autoencoders enforce invertibility in two DNNs
with a reconstruction loss, which many discriminative learn-
ing tasks use for Autoencoders’ flexible construction. In each
case, we build the DNNs with the same architecture (depth,
width, and activation) as DipDNN.
•Addictive Coupling Layers: The NICE model [ 19] is designed
for density estimation and trained with MLE using simple
distribution sampling as inputs. In our case, we only build
the invertible model and train it with MSE for fairness [2].
•Invertible Residual Neural Network (i-ResNet): While i-ResNet
is similar to other generative models built on the probabilis-
tic setup, we use ResNet + Lipschitz constraint for iterative
discriminative learning [5].
6.1.4 Training Details. The Pytorch platform is used to build and
train models by Adam optimizer for >200epochs for each exper-
iment, and early stopping is used. Hyperparameters such as the
number of layers (blocks), negative slope coefficient of Leaky ReLU
({0.1,0.2,0.05}), learning rate ({0.001,0.0002,0.00005}), weighting
factor for regularization ( {0,0.1,0.5,1}) and dropout rate, are ad-
justed by grid search for each dataset on a validation set. All the
experiments are implemented on a computer equipped with Inter(R)
Core(TM) i7-9700k CPU and Nvidia GeForce RTX 3090 GPU. Each
experiment is run at least 3-5 times to record the average results
and statistics for a fair comparison.
As for DipDNN, we enforce non-zero entries on the diagonal
of lower/upper triangular matrices to ensure no violation of in-
vertibility. Previous work on restrictive DNN of input convexity
uses a post-check on each training iteration, e.g., replacing negative
weights with the opposite to enforce positive weights [ 1,14]. Empir-
ically, we find this way takes effect but slows down the training and
may lead to non-convergence in large systems, with the parameters
getting trapped at zero sometimes. Instead, we add a mask (lower
or upper triangular) over the weight matrix, which alleviates the
parameter fluctuations in experiments.6.2 Evaluation of Data-Driven Inverse Modeling
Inverse Consistency with the Forward. To show the inverse con-
sistency, we test on two different image construction tasks. The test
results of the standard MNIST dataset are shown in Fig. 6, where
the forward learning is a classification task, and the inverse is to re-
construct the input images given the last layer of extracted features.
For a fair comparison, both convolutional layer-based i-ResNet and
MLP-based i-ResNet are used as baselines. The visualization ex-
amples of reconstructed images show that DipDNN has a better
consistency with the original inputs, while i-ResNet sometimes has
color fading on reconstruction. Numerical comparison is clearer.
For example, the inverse error of our approach is 1000 times smaller
than the inverse error of i-ResNet, while the forward accuracy of the
two approaches is comparable. This illustrates that our approach
has good consistency, leading to significantly better performance.
For the second comparison for face completion, we display Fig. 5
for the results of bi-directional learning. The goal is to create the left
half face based on the right half faces, while preserving overall facial
properties. These properties do include nearly symmetric patterns,
etc. We can see that the autoencoder in blue has blurring results to
Figure 6: Compare the forward classification and inverse
reconstruction with the baseline method on MNIST Dataset.
4077KDD ’24, August 25–29, 2024, Barcelona, Spain Jingyi Yuan, Yang Weng, & Erik Blasch
Figure 7: Validating state estimation for both normal and unseen system operation points.
Figure 8: Estimation errors ( MSE×10−3/p.u. ) of the inverse
state estimation and forward power flow mapping.
average over pixels without guaranteeing zero reconstruction error.
DipDNN in red reconstructs more details with analytical inverse,
so as the NICE model in training results. The differences are more
evident in unseen data during testing due to DipDNN’s capability
of maintaining inverse consistency. Later, we will illustrate that
this property is not only in the facial examples but also in the cyber-
physical examples in Fig. 8. In summary, DipDNN has much lower
errors for variable prediction of the inverse modeling.
Approximation Capability and Computation Efficiency. For the
MNIST dataset, the approximation capability is interpreted from the
forward classification accuracy. DipDNN has comparable accuracy
as well as training time with i-Resnet, as shown in the upper half
of Fig. 6. In the face completion task, the representation power
can be shown as the recovered details of the left half of the face.
Specifically, although addictive coupling layers have an analytical
inverse, the forward approximation capability is limited compared
to DipDNN. Details on the edge of the face are distorted. Moreover,
addictive coupling layers take more time to build the same nonlinearcouplings, and DipDNN tends to spend more time checking the
satisfaction of invertibility at each iteration with increasing depth.
The conclusion is similarly drawn from the physical system
tests. The bottom half of Fig. 8 presents the prediction errors of
the forward modeling. Under the same architecture (depth and
width), DipDNN performs better than the NICE model, which has
as few errors as the dense and flexible i-ResNet. The representative
point estimates in Fig. 7 present evidently the precise fitting of state
variables using DipDNN.
Figure 9: Correlate forward physical recovery rate (left) with
inverse prediction error (right) to show the generalizability
of inverse modeling using DipDNN.
Generalizability of Inverse Modeling. After discussing DipDNN’s
two benefits above, we further evaluate the generalizability of
DipDNN. It is critical for physical and engineering systems that
frequently change operation points. We first consider synthetic
datasets, whose explicit functions are intuitive for demonstration.
In such a test, we try scenarios with different data scarcity and
4078DipDNN: Preserving Inverse Consistency and Approximation Efficiency for Invertible Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
data variation to match the practical needs. The error results with
respect to different data scenarios are shown in Fig. 9. The proposed
physics embedding not only improves the extrapolation of DipDNN
but also maintains stable performances under scarce and varying
data.
In addition to elementary physical systems, real-life power sys-
tems with larger scales better validate the generalization capability
of DipDNN. As depicted in Fig. 7 (from left to right), while other
methods have deviated point estimates on unseen operational con-
ditions, DipDNN maintains close to the ground truth. This shows
that our proposed approach can provide an accurate estimation of
new operation points never seen in the past.
7 CONCLUSION
This paper discusses how to design learning structures for consis-
tent bi-directional inferences. The proposed model enforces strict
one-to-one correspondence via relatively simple reconstructions of
standard neural networks due to inverse-consistent design. Such
a design resolves the problem in traditional analytical approaches
based on nested structure and numerical approaches based on the
Lipschitz condition. By using the proposed decomposed-invertible-
pathway DNNs (DipDNN), we maintain the universal approxima-
tion capability of i-ResNet while providing analytically invertible
guarantees. As numerical stability and generalizability are keys to
monitoring and controlling critical physical systems, we integrate
contractive property with a parallel structure for inductive biases,
leading to stable performance. Numerical results support our claims
by showing significantly better results in image-related tasks and
tasks in cyber-physical systems. Numerical stability and physical
generalizability broaden the applicability of DipDNN.
ACKNOWLEDGEMENT
This work was supported by the Air Force Office of Scientific Re-
search (AFOSR) under Grant No. FA9550-22-1-0294.
4079KDD ’24, August 25–29, 2024, Barcelona, Spain Jingyi Yuan, Yang Weng, & Erik Blasch
REFERENCES
[1]B. Amos, L. Xu, and J. Z. Kolter. Input Convex Neural Networks. In International
Conference on Machine Learning, pages 146–155, 2017.
[2]L. Ardizzone, J. Kruse, S. Wirkert, D. Rahner, E. W. Pellegrini, R. S. Klessen,
L. Maier-Hein, C. Rother, and U. Köthe. Analyzing inverse problems with invert-
ible neural networks. arXiv preprint arXiv:1808.04730, 2018.
[3]S. Arridge, P. Maass, O. Öktem, and C.-B. Schönlieb. Solving inverse problems
using data-driven models. Acta Numerica, 28:1–174, 2019.
[4]G. Bal. Introduction to inverse problems. Lecture Notes-Department of Applied
Physics and Applied Mathematics, Columbia University, New York, 2012.
[5]J. Behrmann, W. Grathwohl, R. T. Chen, D. Duvenaud, and J.-H. Jacobsen. Invert-
ible residual networks. In International Conference on Machine Learning, pages
573–582, 2019.
[6]J. Behrmann, P. Vicol, K.-C. Wang, R. Grosse, and J.-H. Jacobsen. Understanding
and mitigating exploding inverses in invertible neural networks. In International
Conference on Artificial Intelligence and Statistics, pages 1792–1800. PMLR, 2021.
[7]M. Benning and M. Burger. Modern regularization methods for inverse problems.
arXiv preprint arXiv:1801.09922, 2018.
[8]S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from
data by sparse identification of nonlinear dynamical systems. Proceedings of the
national academy of sciences, 113(15):3932–3937, 2016.
[9]J. Bu and A. Karpatne. Quadratic residual networks: A new class of neural
networks for solving forward and inverse problems in physics involving pdes.
InProceedings of the 2021 SIAM International Conference on Data Mining (SDM),
pages 675–683. SIAM, 2021.
[10] G. Cavraro and V. Kekatos. Graph Algorithms for Topology Identification Using
Power Grid Probing. IEEE Control Systems Letters, 2(4):689–694, 2018.
[11] G. Cavraro and V. Kekatos. Inverter Probing for Power Distribution Network
Topology Processing. IEEE Transactions on Control of Network Systems, 2019.
[12] G. Chen, T. Li, Q. Chen, S. Ren, C. Wang, and S. Li. Application of deep learning
neural network to identify collision load conditions based on permanent plastic
deformation of shell structures. Computational Mechanics, 64:435–449, 2019.
[13] M. Chen, S. Ma, Z. Soltani, R. Ayyanar, V. Vittal, and M. Khorsand. Optimal
placement of pv smart inverters with volt-var control in electric distribution
systems. IEEE Systems Journal, 17(3):3436–3446, 2023.
[14] Y. Chen, Y. Shi, and B. Zhang. Optimal Control via Neural Networks: A Convex
Approach. arXiv preprint arXiv:1805.11835, 2018.
[15] N. Christianson, J. Shen, and A. Wierman. Optimal robustness-consistency trade-
offs for learning-augmented metrical task systems. In International Conference
on Artificial Intelligence and Statistics, pages 9377–9399. PMLR, 2023.
[16] N. Costilla-Enriquez and Y. Weng. Attack power system state estimation by
implicitly learning the underlying models. IEEE Transactions on Smart Grid,
14(1):649–662, 2023.
[17] A. Cotter, M. Gupta, H. Jiang, E. Louidor, J. Muller, T. Narayan, S. Wang, and
T. Zhu. Shape constraints for set functions. In International Conference on Machine
Learning, pages 1388–1396. PMLR, 2019.
[18] L. Deng. The mnist database of handwritten digit images for machine learning
research [best of the web]. IEEE Signal Processing Magazine, 29(6):141–142, 2012.
[19] L. Dinh, D. Krueger, and Y. Bengio. NICE: Non-linear Independent Components
Estimation. arXiv preprint arXiv:1410.8516, 2014.
[20] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using Real NVP.
arXiv preprint arXiv:1605.08803, 2016.
[21] S. Dittmer, T. Kluth, P. Maass, and D. Otero Baguer. Regularization by architecture:
A deep prior approach for inverse problems. Journal of Mathematical Imaging
and Vision, 62:456–470, 2020.
[22] Y. Duan, G. Ji, Y. Cai, et al. Minimum width of leaky-relu neural networks for
uniform universal approximation. In International conference on machine learning.
PMLR, 2023.
[23] H. W. Engl, M. Hanke, and A. Neubauer. Regularization of inverse problems,
volume 375. Springer Science & Business Media, 1996.
[24] E. Fetaya, J.-H. Jacobsen, W. Grathwohl, and R. Zemel. Understanding the limita-
tions of conditional generative models. In International Conference on Learning
Representations, 2020.
[25] F. Fioretto, T. W. Mak, and P. Van Hentenryck. Predicting AC optimal power
flows: combining deep learning and lagrangian dual methods. In Proceedings of
the AAAI Conference on Artificial Intelligence, 2020.
[26] H. Goh, S. Sheriffdeen, J. Wittmer, and T. Bui-Thanh. Solving bayesian inverse
problems via variational autoencoders. arXiv preprint arXiv:1912.04212, 2019.
[27] N. M. Gottschling, V. Antun, A. C. Hansen, and B. Adcock. The troublesome
kernel–on hallucinations, no free lunches and the accuracy-stability trade-off in
inverse problems. arXiv e-prints, pages arXiv–2001, 2020.
[28] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In
Proceedings of the 27th international conference on international conference on
machine learning, pages 399–406, 2010.
[29] M. E. Haque, M. F. Zain, M. A. Hannan, and M. H. Rahman. Building structural
health monitoring using dense and sparse topology wireless sensor network.
Smart Structures and Systems, 16(4):607–621, 2015.[30] M. He and M. Khorsand. Residential appliance-level consumption modeling and
forecasting via conditional hidden semi-markov model. In 2022 North American
Power Symposium (NAPS), pages 1–6, 2022.
[31] X. Hu, H. Hu, S. Verma, and Z.-L. Zhang. Physics-guided deep neural networks
for powerflow analysis. arXiv preprint arXiv:2002.00097, 2020.
[32] C. A. Jensen, R. D. Reed, R. J. Marks, M. A. El-Sharkawi, J.-B. Jung, R. T. Miyamoto,
G. M. Anderson, and C. J. Eggen. Inversion of feedforward neural networks:
algorithms and applications. Proceedings of the IEEE, 87(9):1536–1549, 1999.
[33] S. Kamyab, Z. Azimifar, R. Sabzi, and P. Fieguth. Deep learning methods for
inverse problems. PeerJ Computer Science, 8:e951, 2022.
[34] A. A. Kaptanoglu, J. L. Callaham, A. Aravkin, C. J. Hansen, and S. L. Brunton.
Promoting global stability in data-driven models of quadratic nonlinear dynamics.
Physical Review Fluids, 6(9):094401, 2021.
[35] S. Kucuk and Z. Bingul. Robot kinematics: Forward and inverse kinematics. INTECH
Open Access Publisher, 2006.
[36] Y. LeCun. The mnist database of handwritten digits. http://yann. lecun.
com/exdb/mnist/, 1998.
[37] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature, 521(7553):436–444,
2015.
[38] T. Li, R. Yang, G. Qu, G. Shi, C. Yu, A. Wierman, and S. Low. Robustness and
consistency in linear quadratic control with untrusted predictions. 6(1), feb 2022.
[39] L. Liao, D. Fox, J. Hightower, H. Kautz, and D. Schulz. Voronoi tracking: Location
estimation using sparse and noisy sensor data. In Proceedings 2003 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No.
03CH37453), volume 1, pages 723–728. IEEE, 2003.
[40] M. Z. Liu, L. N. Ochoa, S. Riaz, P. Mancarella, T. Ting, J. San, and J. Theunissen.
Grid and market services from the edge: Using operating envelopes to unlock
network-aware bottom-up flexibility. IEEE Power and Energy Magazine, 19(4):52–
62, 2021.
[41] Q. Liu, J. Xu, R. Jiang, and W. H. Wong. Density estimation using deep gen-
erative neural networks. Proceedings of the National Academy of Sciences,
118(15):e2101344118, 2021.
[42] Y. Liu, S. Jiao, and L.-H. Lim. Lu decomposition and toeplitz decomposition of a
neural network. arXiv preprint arXiv:2211.13935, 2022.
[43] MATPOWER. MATPOWER. 2020. https://matpower.org/.
[44] K. Moffat, M. Bariya, and A. Von Meier. Unsupervised Impedance and Topology
Estimation of Distribution Networks—Limitations and Tools. IEEE Transactions
on Smart Grid, 11(1):846–856, 2020.
[45] E. Nalisnick, A. Matsukawa, Y. Teh, D. Gorur, and B. Lakshminarayanan. Do
deep generative models know what they don’t know? In International Conference
on Learning Representations, 2019.
[46] S. Pakravan, P. A. Mistani, M. A. Aragon-Calvo, and F. Gibou. Solving inverse-pde
problems with physics-aware neural networks. Journal of Computational Physics,
440:110414, 2021.
[47] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, and J. Vanderplas. Scikit-Learn: Ma-
chine Learning in Python. Journal of machine learning research, 12:2825–2830,
2011.
[48] Y. Pei, S. Biswas, D. S. Fussell, and K. Pingali. An elementary introduction to
kalman filtering. Communications of the ACM, 62(11):122–133, 2019.
[49] PJM Interconnection LLC. Metered load data. 2018. https://dataminer2.pjm.com/
feed/hrl_load_metered/definition.
[50] M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving
nonlinear partial differential equations. Journal of Computational physics, 378:686–
707, 2019.
[51] S. Roweis. MS Windows NT kernel description.
[52] S. Sahoo, C. Lampert, and G. Martius. Learning equations for extrapolation and
control. In J. Dy and A. Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pages 4442–4450. PMLR, 10–15 Jul 2018.
[53] S. Sahoo, C. Lampert, and G. Martius. Learning equations for extrapolation and
control. In International Conference on Machine Learning, pages 4442–4450. PMLR,
2018.
[54] J. Schmidhuber. Deep learning in neural networks: An overview. Neural networks,
61:85–117, 2015.
[55] R. Stewart and S. Ermon. Label-free supervision of neural networks with physics
and domain knowledge. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 31, 2017.
[56] P. Sundaray and Y. Weng. Alternative auto-encoder for state estimation in
distribution systems with unobservability. IEEE Transactions on Smart Grid,
14(3):2262–2274, 2023.
[57] A. Tarantola. Inverse problem theory and methods for model parameter estimation.
SIAM, 2005.
[58] W. F. Tinney and C. E. Hart. Power flow solution by newton’s method. IEEE
Transactions on Power Apparatus and Systems, (11):1449–1460, 1967.
[59] S.-M. Udrescu and M. Tegmark. Ai feynman: A physics-inspired method for
symbolic regression. Science Advances, 6(16):eaay2631, 2020.
4080DipDNN: Preserving Inverse Consistency and Approximation Efficiency for Invertible Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
[60] K. Xu and E. Darve. Physics constrained learning for data-driven inverse modeling
from sparse observations. Journal of Computational Physics, 453:110938, 2022.
[61] J. Yuan and Y. Weng. Physics interpretable shallow-deep neural networks for
physical system identification with unobservability. IEEE International Conference
on Data Mining (ICDM), 2021.
[62] J. Yuan and Y. Weng. Invertible neural network for consistent state estimation
in distribution grid with unobservability. In 2023 IEEE Power & Energy Society
General Meeting (PESGM), pages 1–5, 2023.
[63] Y. Yuan, S. Low, O. Ardakanian, and C. Tomlin. On the Inverse Power Flow
Problem. arXiv preprint arXiv:1610.06631, 2016.
[64] H. Zhang, X. Gao, J. Unterman, and T. Arodz. Approximation capabilities of
neural odes and invertible residual networks. In International Conference on
Machine Learning, pages 11086–11095. PMLR, 2020.
[65] Z. Zhang and M. Wu. Real-time locational marginal price forecast: A decision
transformer-based approach. In 2023 IEEE Power & Energy Society General Meeting
(PESGM), pages 1–5, 2023.
[66] W. L. Zhao, P. Gentine, M. Reichstein, Y. Zhang, S. Zhou, Y. Wen, C. Lin, X. Li,
and G. Y. Qiu. Physics-constrained machine learning of evapotranspiration.
Geophysical Research Letters, 2019.
8 APPENDICES
8.1 Derivation for Addictive Coupling Layers
Jacobian Derivation. For the𝑘𝑡ℎunit of addictive coupling layers,
the Jacobian is 𝐽𝑘=𝜕𝒚𝐼1(𝑘)
𝜕𝒙𝐼1(𝑘)𝜕𝒚𝐼1(𝑘)
𝜕𝒙𝐼2(𝑘)
𝜕𝒙𝐼2(𝑘)
𝜕𝒙𝐼1(𝑘)𝜕𝒚𝐼2(𝑘)
𝜕𝒙𝐼2(𝑘)=𝑎(𝑘)𝐼1 0
𝜕𝒕(𝑘)(𝒙𝐼1(𝑘))
𝜕𝒙𝐼1(𝑘)𝑏(𝑘)𝐼2.
For every other layer, the columns exchange due to the in-turn
composition in Fig. 10(b). Using the chain rule, the Jacobian of the
composited function is Π𝑘𝐽𝑘. Only when 𝑘≥3, the 0’s can be
eliminated from the Jacobian matrix, and all the elements can be
non-constant, thus indicating a full transformation of all dimen-
sions.
Π3𝐽3="𝑎3𝐼1 0
𝜕𝒕3
𝜕𝒛2
𝐼1𝑏3𝐼2#
·"
𝑎2𝐼1𝜕𝒕2
𝜕𝒛1
𝐼2
0𝑏2𝐼2#
·"
𝑎1𝐼1 0
𝜕𝒕1
𝜕𝒙𝐼1𝑏1𝐼2#
(4)
=𝑎3𝑎2𝑎1𝐼1+𝑎3𝜕𝒕2
𝜕𝒛1
𝐼2𝜕𝒕1
𝜕𝒙𝐼1𝑎3𝑏1𝜕𝒕2
𝜕𝒛1
𝐼2
𝑎2𝑎1𝜕𝒕3
𝜕𝒛2
𝐼1+𝑏3𝑏2𝜕𝒕1
𝜕𝒙𝐼1+𝜕𝒕3
𝜕𝒛2
𝐼1𝜕𝒕2
𝜕𝒛1
𝐼2𝜕𝒕1
𝜕𝒙𝐼1𝑏3𝑏2𝑏1𝐼2+𝑏1𝜕𝒕3
𝜕𝒛2
𝐼1𝜕𝒕2
𝜕𝒛1
𝐼2.
(5)
Fig. 10(b) shows intuitively that the second layer completes the
nonlinear coupling between 𝒙𝐼1and all 𝒚. However, as we highlight
the paths in gray, 𝒙𝐼2has the first linear path towards 𝒚𝐼2and still
needs the third layer to nonlinearly map to 𝒚𝐼2. Assuming we use
the same architecture (i.e., 𝐾-layer ReLU-activated DNNs) for the
nonlinear function in each layer, Fig. 10(b) needs three such DNNs
to complete the coupling among all dimensions. The separate DNNs
shall weaken the approximation capability so that four or more
layers are usually concatenated in implementation.
Representation Efficiency and Computation Complexity. In Sec.
2.1, we show the typical reconstruction to enforce strict invert-
ibility. As we aim to minimize the adjustment on DNNs subject
to invertibility, we compare its representation power with regular
DNNs. To compare with the dense fully connected layers, Fig. 10(c)
needs to be compressed further. To achieve this, we will have to
superpose the two layers as shown in Fig. 10(d). It can be seen as
an equivalent architecture in representation, e.g., the same nonlin-
ear correlations between each input/output group. The unfolded
model in Fig. 10(d) can be seen as a sparse connection of regular
DNN. Many interconnections are eliminated due to the separation
Figure 10: (a) - (b): Composition of addictive invertible trans-
formation for full coupling of input/output dimensions. (c):
A reduction of (b) that retains full dimension coupling. (d) A
further reduction of (c) for equivalent representation.
of input/output groups, i.e., the full connectivity is isolated within
each group coupling. Thus, the invertibility is enforced at the cost
of∼1.5×computation complexity and sparse interconnections
among different groups of variables.
8.2 Proposed DipDNN
Basic Invertible Unit. Motivated by the triangular map, the neural
representation of one invertible unit is 𝒛(1)=LeakyReLU(𝑊𝑡𝑟𝑖𝑙𝒙+
𝒃1),and𝑏𝑚𝑧(2)=LeakyReLU(𝑊𝑡𝑟𝑖𝑢𝒛(1)+𝒃2),where𝑊𝑡𝑟𝑖𝑙="
𝑤(1)
110
𝑤(1)
21𝑤(1)
22#
,𝑊𝑡𝑟𝑖𝑢="
𝑤(2)
11𝑤(2)
12
0𝑤(2)
22#
.Subsequently, the corre-
sponding inverse function is 𝒛(1)=𝑊−1
𝑡𝑟𝑖𝑢(LeakyReLU−1(𝒛(2))−
𝒃(2)),and𝒙=𝑊−1
𝑡𝑟𝑖𝑙(LeakyReLU−1(𝒛(1))−𝒃(1))The unit 𝒛(1)and
𝑏𝑚𝑧(2)is strictly invertible if 𝑤(1)
11,𝑤(1)
22,𝑤(2)
11,𝑤(2)
22≠0.
Supplementary Theorems. Details of Theorem 3.2 are as follows.
Theorem 8.1. LetK⊂R𝑑𝑥be a compact set; then, for the contin-
uous function class 𝐶(K,R𝑑𝑦), the minimum width 𝑤minof Leaky-
ReLU neural networks having 𝐶−UAP is exactly𝑤min=max(𝑑𝑥+
1,𝑑𝑦)+1=𝑑𝑥+1+𝑑𝑦.Thus, NN(𝜎)is dense in𝐶(𝐾,R𝑑𝑦)if and
only if𝑁≥𝑤min.1𝑑𝑦=𝑑𝑥+1=(
1,if𝑑𝑦=𝑑𝑥+1,
0,if𝑑𝑦≠𝑑𝑥+1.
Lemma 8.2. For any continuous function 𝑓∗∈𝐶(K,R𝑑)on a
compact domainK ⊂R𝑑, and𝜖>0, there exists a Leaky-ReLU
network𝑓𝐿(𝑥)with depth𝐿and width𝑑+1such that∥𝑓𝐿(𝑥)−
𝑓∗(𝑥)∥≤𝜖for all𝑥∈K.
As a follow-up of Theorem 8.1, Lemma 8.2 specifies the condition
for the case where the input and output dimensions are equal,
𝑑𝑥=𝑑𝑦=𝑑. It provides the result that the Leaky ReLU-activated
neural network with width 𝑑+1has enough expressive power to
approximate the continuous function 𝑓∗.
The theorem on the identity mapping approximation is used to
prove the decomposition of weight matrices.
Theorem 8.3. Let𝐼:R𝑛→R𝑛be the identity map. Then for any
compact Ω⊆R𝑛and any𝜖>0, there exists a 𝛿>0such that
whenever 0<|ℎ|<𝛿, the function 𝜌ℎ:R𝑛→R𝑛, satisfies𝜌ℎ(𝑥):=
1
ℎ𝜎′(𝑎)[𝜎(ℎ𝑥+𝑎1𝑛)−𝜎(𝑎)1𝑛],andsup𝑥∈Ω∥𝜌ℎ(𝑥)−𝐼(𝑥)∥≤𝜖.
4081KDD ’24, August 25–29, 2024, Barcelona, Spain Jingyi Yuan, Yang Weng, & Erik Blasch
8.3 Inverse Stability
Toy Examples. Here, we test the performance of the inverse map-
ping based on a well-trained invertible neural network model (for-
ward error≈0). For a comprehensive understanding, we test on
cases with different setups: 1) 𝒙,𝒚∈R𝑚,𝑚∈{2,4,6,8}, 2)𝑓is built
based on the mix of different basis {(·),(·)2,cos(·),sin(·),exp(·)},
3) different depth of network 𝐾={2,4,6,8,10}. We select represen-
tative examples and show in Fig. 4 the error propagation through
layers compared with the ablation error of each invertible block.
Lipschitz and Bi-Lipschitz Continuity related to Inverse Stability.
The numerical invertibility is related to Lipschitz continuous prop-
erty for mapping contraction. As for the Lipschitz continuity of
both the forward and inverse mapping, we recall the definition of
bi-Lipschitz continuity of invertible function.
Definition 1.(Lipschitz and bi-Lipschitz continuity of invertible
function) A function 𝑓:R𝑛→R𝑛is called Lipschitz continuous
if there exists a constant 𝐿:=Lip(𝑓)such that:∥𝑓(𝑥1)−𝑓(𝑥2)∥≤
𝐿∥𝑥1−𝑥2∥,∀𝑥1,𝑥2∈R𝑛.If an inverse 𝑓−1:R𝑛→R𝑛and a
constant𝐿∗:=Lip(𝑓−1)exists, then, for all 𝑦1,𝑦2∈R𝑛,∥𝑓−1(𝑦1)−
𝑓−1(𝑦2)∥≤𝐿∗∥𝑥1−𝑥2∥,∀𝑥1,𝑥2∈R𝑛.Thus, the function 𝑓is called
bi-Lipschitz continuous.
From the mathematical definition, the Lipschitz constant reflects
the amplification of errors in the worst scenario during the recon-
struction. For example, i-ResNet adopts a strict bound Lip(𝑓)<1
as a sufficient condition of invertibility and enforces stable inverse
computation via fixed-point iterations. But, recent theoretical re-
sults of no free lunches in [ 27] show an accuracy-stability trade-off
in inverse problems: high stability may be reached at the expense of
significantly poor approximation performance. In our case, DipDNN
aims for the analytical inverse without requiring a rigid Lipschitz
bound. Meanwhile, we have proposed physics embedding to avoid
overperformance (only minimizing errors) on the training set. In
order to further balance performance and stability, we quantify the
correlation between errors and the Lipschitz bound.
Correlation between Numerical Errors and Lipschitz Bounded Re-
construction Errors. Let𝜎denote the numerical errors (round-off,
forward approximation errors, noises, etc.), then a forward map-
ping𝑧=ℎ(𝑥)becomesℎ𝜎(𝑥)=𝑧+𝜎=𝑧𝜎. Subsequently, the
inverse for input reconstruction becomes 𝑥𝜎1=ℎ−1(𝑧𝜎). Accord-
ing to Definition 1, with Lipschitz constant Lip(ℎ−1), we derive the
bound of reconstruction error ∥𝑥−𝑥𝜎1∥2≤Lip(ℎ−1)∥𝑧−𝑧𝜎∥2=
Lip(ℎ−1)∥𝜎∥2.This is for errors propagated and aggravated from
the forward mapping. As for the inverse mapping, we use 𝜎2to
denote errors for round-off and extrapolation (distribution shift),
and getℎ−1𝜎(𝑧𝜎)=𝑥𝜎1+𝜎2=𝑥𝜎2. Similarly,
∥𝑥−(𝑥𝜎1+𝜎2))∥2≤∥𝑥−𝑥𝜎1∥2+∥𝜎2∥2
≤Lip(ℎ−1)∥𝑧−𝑧𝜎∥2+∥𝜎2∥2
=Lip(ℎ−1)∥𝜎∥2+∥𝜎2∥2,(6)
which shows how much the Lipschitz constant amplifies the errors.
As we observe empirically, 𝜎,𝜎2will be increased with the problem
becoming more complex. As long as we moderately regularize the
Lip(ℎ−1), the reconstruction error can be bounded.Proof of Bi-Lipschitz Continuity of Inverse Mapping. If we can
ensure the bi-Lipschitz continuity of an invertible function, the cor-
responding inverse mapping is also bi-Lipschitz continuous. The
proof is simple in that, with Definition 1, 𝑓has one-to-one corre-
spondence𝑓(𝑥1)=𝑓(𝑥2)⇔𝑥1=𝑥2. For all𝑦1,𝑦2∈𝑓(X), there is
a unique𝑥1,𝑥2∈X such that𝑓−1(𝑦1)=𝑥1and𝑓−1(𝑦2)=𝑥2. Sub-
sequently, we obtain ∥𝑦1−𝑦2∥≤𝐿∥𝑓−1(𝑦1)−𝑓−1(𝑦2)∥, meaning
bi-Lipschitz of 𝑓−1. Considering (6), with bounded Lipschitz con-
stant throughout the network, we can guarantee stable performance
in two-way mapping numerically.
Therefore, we enforce moderate Lipschitz continuity in the in-
verse mapping. The exact computation of DNN’s Lipschitz bound
is NP-hard, but we can decompose it into layers with Lip(ℎ(𝑘−1)◦
ℎ(𝑘))=Lip(ℎ𝑘−1)·Lip(ℎ𝑘). For each layer, Leaky ReLU is a typical
1-Lipschitz activation function, so we need to constrain the param-
eters of the inverse form to avoid arbitrarily large values. Both 𝐿1
and𝐿2norms can mitigate the weights explosion, but the 𝐿1norm
simultaneously encourages the sparse connection of the network.
In our case, the inverses of triangular weight matrices retain their
triangular form and inherently exhibit sparsity. Applying 𝐿1norm
regularization could potentially hinder training efficiency and vi-
olate the conditions for invertibility. Therefore, we adopt the 𝐿2
norm of the inverse weights to smoothly clip large entries. While it
is a moderate bound to regularize bi-Lipschitz continuity, the effect
on the synthetic examples shows a much smaller error (( <10−10))
propagated through layers in Fig. 4 (right).
8.4 Experiments Details of Data Preparation
Compared to the elementary physical systems, the power system
has a much larger problem size and more complex variable cou-
plings due to the grid connection. Moreover, to fulfill the need for
downstream control and operations, the learning model needs to
have robust performance in extrapolation scenarios, such as increas-
ing load and renewable injection conditions [30]. For the forward,
the governing physical law is the grid power flow equations ex-
pressed by system-wide power injections based on voltage phasors
and system topology/parameters [ 61]. Traditional methods assume
an available model and linearize the model to solve for the states.
However, system parameters are unknown or incorrect in many
secondary power distribution systems [10, 11, 44, 63].
IEEE provides standard power system models (IEEE 8- and 123-
bus test feeders), including the grid topology, parameters, genera-
tion models, etc., for simulations. The model files and the simulation
platform, MATPOWER [ 43], are based on MATLAB. For simula-
tions, the load files are needed as the inputs to the systems. Thus,
we introduce power consumption data from utilities such as PJM
Interconnection LLC [ 49]. Such load files contain hourly power con-
sumption in 2017 for the PJM RTO regions. For the Utility feeder,
the collected data also includes distributed photovoltaics generation
profiles. With the above data, MATPOWER produces the system
states of voltages and nodal power injections. Furthermore, the
loading conditions and renewable penetration keep changing in the
current power systems. We validate the extrapolation capability of
the proposed invertible model using out-of-distribution data (3 ×
PV generation and loads [30]).
4082