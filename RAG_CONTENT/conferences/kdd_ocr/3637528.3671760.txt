Mining of Switching Sparse Networks for Missing Value
Imputation in Multivariate Time Series
Kohei Obata
SANKEN, Osaka University
Osaka, Japan
obata88@sanken.osaka-u.ac.jpKoki Kawabata
SANKEN, Osaka University
Osaka, Japan
koki@sanken.osaka-u.ac.jp
Yasuko Matsubara
SANKEN, Osaka University
Osaka, Japan
yasuko@sanken.osaka-u.ac.jpYasushi Sakurai
SANKEN, Osaka University
Osaka, Japan
yasushi@sanken.osaka-u.ac.jp
Abstract
Multivariate time series data suffer from the problem of missing
values, which hinders the application of many analytical methods.
To achieve the accurate imputation of these missing values, ex-
ploiting inter-correlation by employing the relationships between
sequences (i.e., a network) is as important as the use of temporal
dependency, since a sequence normally correlates with other se-
quences. Moreover, exploiting an adequate network depending on
time is also necessary since the network varies over time. However,
in real-world scenarios, we normally know neither the network
structure nor when the network changes beforehand. Here, we pro-
pose a missing value imputation method for multivariate time series,
namely MissNet, that is designed to exploit temporal dependency
with a state-space model and inter-correlation by switching sparse
networks. The network encodes conditional independence between
features, which helps us understand the important relationships
for imputation visually. Our algorithm, which scales linearly with
reference to the length of the data, alternatively infers networks
and fills in missing values using the networks while discovering
the switching of the networks. Extensive experiments demonstrate
thatMissNet outperforms the state-of-the-art algorithms for mul-
tivariate time series imputation and provides interpretable results.
CCS Concepts
•Information systems →Data mining; •Mathematics of
computing→Time series analysis.
Keywords
Multivariate time series, Missing value imputation, Network infer-
ence, State-space model, Graphical lasso
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671760ACM Reference Format:
Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai. 2024.
Mining of Switching Sparse Networks for Missing Value Imputation in
Multivariate Time Series . In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671760
1 Introduction
With the development of the Internet of Things (IoT), multivariate
time series are generated in many real-world applications, such as
motion capture [ 37], and health monitoring [ 8]. However, there
are inevitably many values missing from these data, and this has
many possible causes (e.g., sensor malfunction). As most algorithms
assume an intact input when building models, missing value impu-
tation is indispensable for real-world applications [3, 40].
In time series data, missing values often occur consecutively,
leading to a missing block in a sequence, and can happen simultane-
ously to multiple sequences [ 27]. To effectively reconstruct missing
values from such partially observed data, we must exploit both tem-
poral dependency, by taking account of past and future values in the
sequence, and inter-correlation, by using the relationship between
different sequences (i.e., a network) [ 10,52]. Here, a network does
not necessarily mean the spatial proximity of sensors but rather
underlying connectivity (e.g., Pearson correlation or partial correla-
tion). Moreover, as time series data are normally non-stationary, so
is the network; an adequate network must be exploited depending
on time [ 45,46]. We collectively refer to a group of time points
with the same network as a “regime”. Fig. 1 shows an illustrative
example where missing blocks randomly exist in a multivariate
time series consisting of three features (i.e., A,B, and C). Each time
point belongs to either of two regimes with different networks (i.e.,
#1and#2), where the thickness of the edge indicates the strength
of the interplay between features. It is appropriate to use the values
of feature Cto impute the block missing from feature Bin regime
#1since the network has an edge between BandC. On the other
hand, in regime #2, it is preferable to use feature A, as the network
suggests. Nevertheless, in real-world scenarios, we often have no
information about the data; that is, we do not know the structure
of the network, let alone when the network changes. Thus, given
a partially observed multivariate time series, how can we infer
networks and allocate each time point to the correct regime? How
 
2296
KDD ’24, August 25–29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
Figure 1: An illustrative example of a multivariate time series
including missing blocks, where each time point of the data
is allocated to two regimes, each with a distinct network,
where the edges show relationships between features.
can we achieve an accurate imputation exploiting both temporal
dependency and inter-correlation?
There have been many studies on time series missing value
imputation with machine learning and deep learning models [ 23].
Many of them employ time-variant latent/hidden states, as used in
a State-Space Model (SSM) or a Recurrent Neural Network (RNN),
to learn the dynamic patterns of time series [ 7,9,25,54]. However,
they do not make full use of inter-correlation, while the sequences of
a multivariate time series usually interact. Although they implicitly
capture inter-correlation in latent space, they potentially capture
spurious correlations under the presence of missing values, leading
to inaccurate imputation. To tackle this problem, several studies
explicitly utilize the network [ 5,6,10,17,21,50]. However, they
require a predefined network, even though we rarely know it in
advance.
In this work, we propose MissNet1,Mining of switching sparse
Networks for multivariate time series imputation, which repeat-
edly infers sparse networks from imputed data and fills in miss-
ing values using the networks while allocating each time point to
regimes. Specifically, our model has three components: (1) a regime-
switching model based on a discrete Markov process for detecting
the change point of the network. (2) An imputation model based on
an SSM for exploiting temporal dependency and inter-correlation
by latent space and the networks. (3) A network inference model for
inferring sparse networks via graphical lasso, where each network
encodes pairwise conditional independencies among features, and
the lasso penalty helps avoid capturing spurious correlations. Our
proposed algorithm maximizes the joint probability distribution
over the above components.
Our method has the following desired properties:
•Effective :MissNet, which exploits both temporal depen-
dency and inter-correlation by inferring switching sparse
networks, outperforms the state-of-the-art algorithms for
missing value imputation for multivariate time series.
•Scalable : Our proposed algorithm scales linear with regard
to the length of the time series and is thus applicable to a
long-range time series.
•Interpretable :MissNet discovers regimes with distinct sparse
networks, which help us interpret data, e.g., what is the key
relationship in the data, and why does a particular regime
distinguish itself from others?
1Our source code and datasets are publicly available:
https://github.com/KoheiObata/MissNet.Table 1: Capabilities of MissNet, Matrix Factorization (MF),
State-Space Model (SSM), Deep Learning (DL), and Graphical
Lasso (GL)-based approaches.
MF SSM DL GLSoftImpute
[30]
SoRe
c [28]
TRMF
[55]
SLDS
[36]
D
ynaMMo [25]
DCMF
[5]
BRI
TS [7]
POGEV
ON [50]
TICC
[19]
MMGL
[48]
MissNet
T
emporal dependency - -✓✓✓✓✓✓ - -✓
Inter-corr
elation -✓ - -
-✓ -✓ -✓✓
Time-var
ying inter-correlation - - - - - - -✓ -✓✓
Missing
value imputation ✓
✓ ✓ -✓
✓✓
✓ -✓✓
Segmentation - - -✓ - - - -✓ -✓
Sparse
network inference -
- - -
- - -
-✓
✓✓
2 Related work
We review previous studies closely related to our work. Table 1
shows the relative advantages of MissNet. Current approaches fall
short with respect to at least one of these desired characteristics.
Time series missing value imputation. Missing value imputa-
tion for time series is a very rich topic [ 23]. We roughly classify
missing value imputation methods as Matrix Factorization (MF)-
based, SSM-based, and Deep Learning (DL)-based approaches.
MF-based methods, such as SoftImpute [ 30] based on Singu-
lar Value Decomposition (SVD), recover missing values from low-
dimensional embedding matrices of partially observed data [ 22,
35,52,56]. For example, SoRec [ 28], proposed as a recommenda-
tion system, constrains MF with a predefined network to exploit
inter-correlation. Since MF is limited in capturing temporal depen-
dency, TRMF [ 55] uses an Auto-Regressive (AR) model and imposes
temporal smoothness on MF.
SSMs, such as Linear Dynamical Systems (LDS) [ 16], use latent
space to capture temporal dependency, where the data point de-
pends on all past data points [ 9,11,17,39]. To fit more complex time
series, Switching LDS (SLDS) [ 14,36] switches multiple LDS models.
SSM-based methods, such as DynaMMo [ 25], focus on capturing
the dynamic patterns in time series rather than inter-correlation
implicitly captured through the latent space. To use the underlying
connectivity in multivariate time series, DCMF [ 5], and its tensor
extension Facets [ 6] use SSM constrained with a predefined net-
work, which is effective, especially when the missing rate is high.
However, they assume that the network is accurately known and
fixed, while it is usually unknown and may change over time in
real-world scenarios.
Recently, extensive research has focused on DL-based methods,
employing techniques including graph neural networks [ 10,13,
21,38], self-attention [ 12,41], and, most recently, diffusion mod-
els [1,44,51], to harness their high model capacity [ 2,53]. For
example, BRITS [ 7] and M-RNN [ 54] impute missing values accord-
ing to hidden states from bidirectional RNN. To utilize dynamic
inter-correlation in time series, POGEVON [ 50] requires a sequence
of networks and imputes missing values in time series and missing
edges in the networks, assuming that the network varies over time.
Although DL-based methods can handle complex data, the impu-
tation quality depends heavily on the size and the selection of the
training dataset.
 
2297Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD ’24, August 25–29, 2024, Barcelona, Spain
Sparse network inference. From another perspective, our method
infers sparse networks from time series containing missing values
and discovers regimes (i.e., clusters) based on networks. Inferring a
sparse inverse covariance matrix (i.e., network) from data helps us
to understand feature dependency in a statistical way. Graphical
lasso [ 15], which maximizes the Gaussian log-likelihood impos-
ing anℓ1-norm penalty, is one of the most commonly used tech-
niques for estimating a sparse network from static data. Städler and
Bühlmann [ 42] have tackled inferring a sparse network from par-
tially observed data according to conditional probability. However,
the network varies over time [ 29,32,33]; thus, TVGL [ 18] infers
time-varying networks by considering the time similarity with a
network belonging to neighboring segments. To infer time-varying
networks in the presence of missing values, MMGL [ 48], which
employs TVGL, uses the expectation-maximization (EM) algorithm
to repeat the inference of time-varying networks and missing value
imputation based on conditional probability under the condition
that each segment has the same observed features. Discovering
clusters based on networks [ 34], such as TICC [ 19] and TAGM [ 47],
provides interpretable results that other traditional clustering meth-
ods cannot find. However, they cannot handle missing values.
As a consequence, none of the previous studies have addressed
missing value imputation for multivariate time series by employing
sparse network inference and segmentation based on the network.
3 Preliminaries
3.1 Problem definition
In this paper, we focus on the task of multivariate time series miss-
ing value imputation. We use a multivariate time series with 𝑁
features and 𝑇timesteps 𝑿={𝒙1,𝒙2,...,𝒙𝑇}∈R𝑁×𝑇. To repre-
sent the missing values in 𝑿, we introduce the indicator matrix
𝑾∈R𝑁×𝑇, where 𝑾𝑖,𝑡indicates the availability of feature 𝑖at
timestep𝑡:𝑾𝑖,𝑡being 0or1indicates whether 𝑿𝑖,𝑡is missing or
observed. Thus, the observed entries can be described as ˜𝑿=𝑾◦𝑿,
where◦is a Hadamard product. Our problem is formally written
as follows:
Problem 1 (Multivariate Time Series Missing Value Impu-
tation). Given: a partially observed multivariate time series ˜𝑿;
Recover: its missing parts indicated by 𝑾.
3.2 Graphical lasso
We use graphical lasso [ 15] to infer the network for each regime.
Given 𝑿, graphical lasso estimates the sparse Gaussian inverse
covariance matrix (i.e., network) 𝚯∈R𝑁×𝑁, also known as the
precision matrix. The network encodes pairwise conditional inde-
pendencies among 𝑁features, e.g., if 𝚯𝑖,𝑗=0, then features 𝑖and
𝑗are conditionally independent given the values of all the other
features. The optimization problem is expressed as follows:
minimize𝚯∈𝑆𝑝
++𝜆||𝚯||𝑜𝑑,1−𝑇∑︁
𝑡=1𝑙𝑙(𝒙𝑡,,𝚯), (1)
𝑙𝑙(𝒙𝑡,𝚯)=−1
2(𝒙𝑡−𝝆)′𝚯(𝒙𝑡−𝝆)
+1
2logdet(𝚯)−𝑁
2log(2𝜋), (2)where 𝚯must be symmetric positive definite ( 𝑆𝑝
++).𝑙𝑙(𝒙𝑡,𝚯)is
the log-likelihood and 𝝆∈R𝑁is the empirical mean of 𝑿.𝜆≥
0is a hyperparameter for determining the sparsity level of the
network, and∥·∥𝑜𝑑,1indicates the off-diagonal ℓ1-norm. This is a
convex optimization problem that can be solved via the alternating
direction method of multipliers (ADMM) [4].
4 Proposed MissNet
In this section, we present our proposed MissNet for missing value
imputation. We give the formal formulation of the model and then
provide the detailed algorithm to learn the model.
4.1 Optimization model
MissNet infers sparse networks and fills in missing values using
the networks while discovering regimes. We first introduce three
interacting components of our model: a regime-switching model,
an imputation model, and a network inference model. Then, we
define the optimization formulation.
Regime-switching model. MissNet describes the change of net-
works by regime-switching. Let 𝐾be the number of regimes, 𝑭=
{𝒇1,...𝒇𝑇}∈R𝐾×𝑇be regime assignments, and 𝒇𝑡be a one-hot
vector2that indicates ˜𝒙𝑡∈R𝑁belongs to 𝒇𝑡ℎ
𝑡-regime. We assume
regime-switching to be a discrete first-order Markov process:
𝑝(𝒇1)=𝝅0, 𝑝(𝒇𝑡+1|𝒇𝑡)=𝚷𝒇𝑡+1,𝒇𝑡, (3)
where 𝚷∈R𝐾×𝐾is the Markov transition matrix and 𝝅0∈R𝐾is
the initial state distribution.
Imputation model. MissNet imputes missing values exploiting
temporal dependency and inter-correlation indicated by the net-
works. We assume that the temporal dependency is consistent
throughout all regimes and captured in the latent space of an SSM,
which allows us to consider long-term dependency. We define the la-
tent states 𝒁={𝒛1,𝒛2,..., 𝒛𝑇}∈R𝐿×𝑇corresponding to ˜𝑿, where
𝐿is the number of latent dimensions, and 𝒛𝑡+1∈R𝐿is linear to 𝒛𝑡
through the transition matrix 𝑩∈R𝐿×𝐿with the covariance 𝚺𝒁,
shown in Eq. (5). As defined in Eq. (4), the first timestep of latent
state 𝒛1is defined by the initial state 𝒛0and the covariance 𝚿0.
𝒛1∼N(𝒛0,𝚿0), (4)
𝒛𝑡+1|𝒛𝑡∼N(𝑩𝒛𝑡,𝚺𝒁). (5)
We define the observation ˜𝒙𝑡assigned to 𝒇𝑡ℎ
𝑡-regime as being
linear to the latent state 𝒛𝑡through the observation matrix of 𝒇𝑡ℎ
𝑡-
regime 𝑼(𝒇𝑡)∈R𝑁×𝐿with the covariance 𝚺𝑿(𝒇𝑡):
˜𝒙𝑡|𝒛𝑡,𝑼(𝒇𝑡),𝒇𝑡∼N( 𝑼(𝒇𝑡)𝒛𝑡,𝚺𝑿(𝒇𝑡)). (6)
MissNet captures inter-correlation by adding a constraint on
𝑼(𝑘). Let it be assumed that the contextual matrix of the 𝑘𝑡ℎ-regime
𝑺(𝑘)∈R𝑁×𝑁encodes the inter-correlation of 𝑿belonging to the
𝑘𝑡ℎ-regime ( 1≤𝑘≤𝐾). We define the contextual latent factor of
the𝑘𝑡ℎ-regime 𝑽(𝑘)∈R𝐿×𝑁, and the𝑗-th column (1≤𝑗≤𝑁) of
the contextual matrix 𝒔(𝑘)
𝑗as linear to 𝒗(𝑘)
𝑗through the observation
matrix 𝑼(𝑘)with the covariance 𝚺𝑺(𝑘), formulated in Eq. (7). In
2We use it interchangeably with the index of the regime (i.e., 𝒇𝑡ℎ
𝑡-regime).
 
2298KDD ’24, August 25–29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
Eq. (8), we place zero-mean spherical Gaussian priors on 𝒗(𝑘)
𝑗under
the assumption that −1≤𝑺(𝑘)
𝑖,𝑗≤1.
𝒔(𝑘)
𝑗|𝒗(𝑘)
𝑗,𝑼(𝑘)∼N( 𝑼(𝑘)𝒗(𝑘)
𝑗,𝚺𝑺(𝑘)), (7)
𝒗(𝑘)
𝑗∼N( 0,𝚺𝑽(𝑘)). (8)
To avoid our model overfitting the observed entries since many
entries are missing, we simplify the covariances by assuming that all
noises are independent and identically distributed (i.i.d.). Thus, the
parameters 𝚺𝒁,𝚺𝑿(𝑘),𝚺𝑺(𝑘),𝚺𝑽(𝑘)are reduced to 𝜎2
𝒁,𝜎2
𝑿(𝑘),𝜎2
𝑺(𝑘),
𝜎2
𝑽(𝑘), respectively (i.e., 𝚺𝒁=𝜎2
𝒁𝑰,𝚺𝑿(𝑘)=𝜎2
𝑿(𝑘)𝑰,𝚺𝑺(𝑘)=𝜎2
𝑺(𝑘)𝑰,
and𝚺𝑽(𝑘)=𝜎2
𝑽(𝑘)𝑰).
With the above imputation model, the imputed time series ˆ𝒙𝑡at
timestep𝑡is recovered as follows:
ˆ𝒙𝑡=𝑾:,𝑡◦˜𝒙𝑡+(1−𝑾:,𝑡)◦𝑼(𝒇𝑡)𝒛𝑡. (9)
Network inference model. MissNet infers the network for each
regime to exploit inter-correlation. We define a Gaussian distribu-
tion andℓ1-norm for the imputed data belonging to each regime to
allow us to estimate networks accurately:
ˆ𝒙𝑡|𝒇𝑡∼N( 𝝆(𝒇𝑡),𝚯(𝒇𝑡)−1), 𝑠.𝑡.,||𝚯(𝒇𝑡)||𝑜𝑑,1≤𝑒
𝜆, (10)
where𝑒is any constant value for convenience, and 𝜆is a hyper-
parameter that controls the sparsity of the network (i.e., inverse
covariance matrix) 𝚯(𝒇𝑡)withℓ1-norm, which helps avoid capturing
spurious correlations.
Optimazation formulation. Our goal is to estimate the model
parameters𝜃={𝑩,𝒛0,𝚿0,𝜎𝒁,¤𝜎𝑿,¤𝜎𝑺,¤𝜎𝑽,¤𝝆,¤𝚯,𝝅0,𝚷}and find the
latent factors 𝒁,¤𝑽,¤𝑼,¤𝑺,𝑭, where the letters with a dot indicate a set
of𝐾vectors/matrices/scalers (e.g., ¤𝑺={𝑺(𝑘)}𝐾
𝑘=1), that maximizes
the following joint probability distribution:
arg max𝑝(˜𝑿,𝒁,¤𝑽,¤𝑼,¤𝑺,𝑭)=𝑝(𝒛1)𝑇Ö
𝑡=2𝑝(𝒛𝑡|𝒛𝑡−1)
|                    {z                    }
temporal dependency
𝑇Ö
𝑡=1𝑝(˜𝒙𝑡|𝒛𝑡−1,𝑼(𝒇𝑡),𝒇𝑡)
|                         {z                         }
time series𝐾Ö
𝑘=1(𝑁Ö
𝑗=1𝑝(𝒗(𝑘)
𝑗)𝑁Ö
𝑗=1𝑝(𝒔(𝑘)
𝑗|𝑼(𝑘),𝒗(𝑘)
𝑗))
|                                                 {z                                                 }
network constraint
𝑝(𝒇1)𝑇Ö
𝑡=2𝑝(𝒇𝑡|𝒇𝑡−1)
|                   {z                   }
regime-switching𝑇Ö
𝑡=1𝑝(ˆ𝒙𝑡|𝝆(𝒇𝑡),𝚯(𝒇𝑡),𝒇𝑡)
|                         {z                         }
network inference,(𝑠.𝑡.||𝚯(𝑘)||𝑜𝑑,1≤𝑒
𝜆)
|                      {z                      }
network sparsity.
(11)
4.2 Algorithm
It is difficult to find the global optimal solution of Eq. (11), for the
following reasons: (i)As a constraint for ¤𝑼,¤𝑺has to be fixed and
encode inter-correlation; (ii)¤𝑼,𝒁and𝑭jointly determine ˜𝑿, and¤𝑼
and¤𝑽jointly determine¤𝑺;(iii)Calculating the correct 𝑭is NP-hard.
Hence, we aim to find its local optimum instead, following the EM
algorithm, where the graphical model for each iteration is shown
in Fig. 2. Specifically, to address the aforementioned difficulties, we
Figure 2: Graphical model of MissNet at each iteration.
employ the following steps: (i)we consider¤𝑺is observed in each
iteration, and we update it at the end of the iteration using ¤𝚯;(ii)we
regard¤𝑼as a model parameter, thus {˜𝑿,𝒁,𝑭}are independent with
{¤𝑺,¤𝑽}. We alternate the inference of {𝒁,𝑭}and¤𝑽and the update
of the model parameters; (iii)we employ a Viterbi approximation
and infer the most likely 𝑭.
4.2.1 E-step. Given¤𝑼, we can infer{𝒁,𝑭}and¤𝑽independently.
Let𝒐𝑡denote the indices of the observed entries of ˜𝒙𝑡. The
observed-only data ¯𝒙𝑡and the corresponding observed-only obser-
vation matrix ¯𝑼(𝑘)
𝑡are defined as follows:
𝒐𝑡={𝑖|𝑾𝑖,𝑡>0,𝑖=1,...,𝑁},
¯𝒙𝑡=˜𝒙𝑡(𝒐𝑡),¯𝑼(𝑘)
𝑡=𝑼(𝑘)(𝒐𝑡,:). (12)
Inferring 𝑭and𝒁.𝑭and𝒁are coupled, and so must be jointly
determined. We first use a Viterbi approximation to find the most
likely regime assignments 𝑭that maximize the log-likelihood Eq. (11).
The likelihood term obtained during the calculation of 𝑭also acts
as a Kalman Filter (forward algorithm). Then, we infer 𝒁with a
Rauch-Tung-Streibel (RTS) smoother (backward algorithm).
In a Viterbi approximation, finding 𝑭requires the partial cost
𝐽𝑡,𝑡−1,𝑘,𝑙when the switch is to regime 𝑘at time𝑡from regime 𝑙at
time𝑡−1. To calculate the partial cost, we define the following LDS
state and variance terms:
𝝁𝑡|𝑡−1,𝑘,𝑙=𝑩𝝁𝑡−1|𝑡−1,𝑘,𝑙,
𝝁𝑡|𝑡,𝑘,𝑙=𝝁𝑡|𝑡−1,𝑘,𝑙+𝑫𝑡,𝑘,𝑙(¯𝒙𝑡−¯𝑼(𝑘)
𝑡𝑩𝝁𝑡|𝑡−1,𝑘,𝑙),
𝚿𝑡|𝑡−1,𝑘,𝑙=𝑩𝚿𝑡−1|𝑡−1,𝑘,𝑙𝑩′+𝜎2
𝒁𝑰,
𝚿𝑡|𝑡,𝑘,𝑙=(𝑰−𝑫𝑡,𝑘,𝑙¯𝑼(𝑘)
𝑡)𝚿𝑡−1|𝑡−1,𝑘,𝑙,
𝑫𝑡,𝑘,𝑙=𝚿𝑡|𝑡−1,𝑘,𝑙¯𝑼(𝑘)′
𝑡(¯𝑼(𝑘)
𝑡𝚿𝑡|𝑡−1,𝑘,𝑙¯𝑼(𝑘)′
𝑡+𝜎2
𝑿(𝑘)𝑰)−1,
(13)
with the initial state:
𝝁1|1,𝑘=𝒛0+𝑫1,𝑘(¯𝒙1−¯𝑼(𝑘)
1𝒛0),
𝚿1|1,𝑘=(𝑰−𝑫1,𝑘¯𝑼(𝑘)
1)𝚿0,
𝑫1,𝑘=𝚿0¯𝑼(𝑘)′
1(¯𝑼(𝑘)
1𝚿0¯𝑼(𝑘)′
1+𝜎2
𝑿(𝑘)𝑰)−1, (14)
where 𝝁𝑡|𝑡−1,𝑘,𝑙and𝝁𝑡|𝑡,𝑘,𝑙are the one-step predicted LDS state
and the best-filtered state estimates at 𝑡, respectively, given the
switch is in regime 𝑘at time𝑡and in regime 𝑙at time𝑡−1and only
the𝑡−1measurements are known. Similar definitions are used for
𝚿𝑡|𝑡−1,𝑘,𝑙and𝚿𝑡|𝑡,𝑘,𝑙.
 
2299Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD ’24, August 25–29, 2024, Barcelona, Spain
The partial cost is obtained by calculating the logarithm of
Eq. (11) related to 𝒇𝑡:
𝐽𝑡,𝑡−1,𝑘,𝑙=1
2(¯𝒙𝑡−¯𝑼(𝑘)
𝑡𝝁𝑡|𝑡−1,𝑘,𝑙)′𝑹(¯𝒙𝑡−¯𝑼(𝑘)
𝑡𝝁𝑡|𝑡−1,𝑘,𝑙)
−1
2logdet(𝑹)+𝐿
2𝑙𝑜𝑔(2𝜋)
+1
2(ˆ𝒙𝑡−𝝆(𝑘))′𝚯(𝑘)(ˆ𝒙𝑡−𝝆(𝑘))
−1
2logdet(𝚯(𝑘))+𝑁
2log(2𝜋)−log(𝚷𝑘,𝑙),(15)
𝑹=(¯𝑼(𝑘)
𝑡𝚿𝑡|𝑡−1,𝑘,𝑙¯𝑼(𝑘)′
𝑡+𝜎2
𝑿(𝑘)𝑰)−1.
Once all partial costs are obtained, it is well-known how to apply
a Viterbi inference to a discrete Markov process to obtain the most
likely regime assignments 𝑭[49].
Then, we infer 𝒁. Let the posteriors of 𝒛𝑡be as follows:
𝑝(𝒛𝑡|˜𝒙1,..., ˜𝒙𝑡)=N(𝒛𝑡|𝝁𝑡,𝚿𝑡),
𝑝(𝒛𝑡|˜𝒙1,..., ˜𝒙𝑇)=N(𝒛𝑡|ˆ𝝁𝑡,ˆ𝚿𝑡). (16)
We now obtain 𝝁𝑡,𝚿𝑡,𝚿𝑡|𝑡−1using 𝑭; thus, in practice, we have
conducted a Kalman Filter. We apply the RTS smoother to infer 𝒁.
ˆ𝝁𝑡=𝝁𝑡+𝑪𝑡(ˆ𝝁𝑡+1−𝑩𝝁𝑡),
ˆ𝚿𝑡=𝚿𝑡+𝑪𝑡(ˆ𝚿𝑡+1−𝚿𝑡|𝑡−1)𝑪′
𝑡,
𝑪𝑡=𝚿𝑡𝑩′(𝚿𝑡|𝑡−1)−1, (17)
E[𝒛𝑡]=ˆ𝝁𝑡,
E[𝒛𝑡𝒛′
𝑡−1]=ˆ𝚿𝑡𝑪′
𝑡+ˆ𝝁𝑡ˆ𝝁′
𝑡−1,
E[𝒛𝑡𝒛′
𝑡]=ˆ𝚿𝑡+ˆ𝝁𝑡ˆ𝝁′
𝑡−1. (18)
Inferring¤𝑽.We apply Bayes’ theorem to Eq. (7) and (8) to obtain
the posteriors 𝑝(𝒗(𝑘)
𝑗|𝒔(𝑘)
𝑗)=N(𝒗(𝑘)
𝑗|𝝂(𝑘)
𝑗,𝚼(𝑘)):
𝝂(𝑘)
𝑗=𝑴(𝑘)−1𝑼(𝑘)′𝒔(𝑘)
𝑗,
𝚼(𝑘)=𝜎2
𝑺(𝑘)𝑴(𝑘)−1,
𝑴(𝑘)=𝑼(𝑘)′𝑼(𝑘)+𝜎−2
𝑽(𝑘)𝜎2
𝑺(𝑘)𝑰,
E[𝒗(𝑘)
𝑗]=𝝂(𝑘)
𝑗,
E[𝒗(𝑘)
𝑗𝒗(𝑘)′
𝑗]=𝚼(𝑘)+𝝂(𝑘)
𝑗𝝂(𝑘)′
𝑗. (19)
4.2.2 M-step. After obtaining{𝒁,𝑭}and¤𝑽, we update the model
parameters to maximize the expectation of the log-likelihood:
𝜃𝑛𝑒𝑤=arg max
𝜃𝑄(𝜃,𝜃𝑜𝑙𝑑), (20)
𝑄(𝜃,𝜃𝑜𝑙𝑑)=E𝒁,𝑭,¤𝑽|𝜃𝑜𝑙𝑑[log𝑝(˜𝑿,𝒁,¤𝑽,¤𝑼,¤𝑺,𝑭|𝜃)]−𝐾∑︁
𝑘=1𝜆||𝚯(𝑘)||𝑜𝑑,1,
where we incorporate the ℓ1-norm constraint.
Parameters for the imputation model, ¤𝑼and{𝑩,𝒛0,𝚿0,𝜎𝒁,¤𝜎𝑿,¤𝜎𝑺,¤𝜎𝑽},
can be obtained by taking the derivative of 𝑄(𝜃,𝜃𝑜𝑙𝑑). For 𝑼(𝑘),we update each row 𝑼(𝑘)
𝑖,:individually:
(𝑼(𝑘)
𝑖,:)𝑛𝑒𝑤=𝑨(𝑘)
1𝑨(𝑘)−1
2, (21)
𝑨(𝑘)
1=𝛼𝜎−2
𝑺(𝑘)𝑁∑︁
𝑗=1𝑺(𝑘)
𝑖,𝑗E[𝒗(𝑘)′
𝑗]+(1−𝛼)𝜎−2
𝑿(𝑘)𝑇∑︁
𝑡=1,𝒇𝑡∈𝑘𝑾𝑖,𝑡˜𝑿𝑖,𝑡E[𝒛′
𝑡],
𝑨(𝑘)
2=𝛼𝜎−2
𝑺(𝑘)𝑁∑︁
𝑗=1E[𝒗(𝑘)
𝑗𝒗(𝑘)′
𝑗]+(1−𝛼)𝜎−2
𝑿(𝑘)𝑇∑︁
𝑡=1,𝒇𝑡∈𝑘𝑾𝑖,𝑡E[𝒛𝑡𝒛′
𝑡],
where 0≤𝛼≤1is a hyperparameter employed as a trade-off for
the contributions of inter-correlation and temporal dependency.
The details of updating {𝑩,𝒛0,𝚿0,𝜎𝒁,¤𝜎𝑿,¤𝜎𝑺,¤𝜎𝑽}are presented in
Appendix A.3.
For the network inference model, we calculate ˆ𝑿with Eq. (22)
and then update 𝝆(𝑘)by calculating the empirical mean of ˆ𝑿be-
longing to the 𝑘𝑡ℎ-regime and 𝚯(𝑘)by solving the graphical lasso
problem shown in Eq. (23) via ADMM.
ˆ𝒙𝑡=𝑾:,𝑡◦˜𝒙𝑡+(1−𝑾:,𝑡)◦(𝑼(𝒇𝑡))𝑛𝑒𝑤ˆ𝝁𝑡, (22)
minimize𝚯(𝑘)∈𝑆𝑝
++𝜆||𝚯(𝑘)||𝑜𝑑,1−𝑇∑︁
𝑡=1,𝒇𝑡∈𝑘𝑙𝑙(ˆ𝒙𝑡,𝚯(𝑘)). (23)
For the regime-switching model, the initial state distribution and
the Markov transition matrix are updated as follows:
𝝅0=𝒇1,𝚷=𝑇∑︁
𝑡=2𝒇𝑡𝒇′
𝑡−1
diag𝑇∑︁
𝑡=2𝒇𝑡−1
. (24)
4.2.3 Update¤𝑺.We update 𝑺(𝑘)at the end of each iteration. As
shown in Eq. (25), we define the off-diagonal elements of 𝑺(𝑘)as
partial correlations calculated from the network 𝚯(𝑘)to encode
the inter-correlation.
𝑺(𝑘)
𝑖,𝑗= 
1(𝑖=𝑗)
−(𝚯(𝑘)
𝑖,𝑗√︃
𝚯(𝑘)
𝑖,𝑖√︃
𝚯(𝑘)
𝑗,𝑗) (𝑖 ≠𝑗). (25)
4.2.4 Overall algorithm. We have the overall algorithm shown as
Alg. 1 to obtain a local optimal solution of Eq. (11). Given a partially
observed multivariate time series ˜𝑿, an indicator matrix 𝑾, the
dimension of latent state 𝐿, the number of regimes 𝐾, the network
parameter𝛼, and the sparse parameter 𝜆, our algorithm aims to
find the latent factors 𝒁,¤𝑽,¤𝑼,¤𝑺,𝑭, other model parameters in 𝜃,
and imputed time series ˆ𝑿.
TheMissNet algorithm starts by initializing ˆ𝑿with a linear in-
terpolation, and by randomly initializing 𝒁,¤𝑽,¤𝑼,¤𝑺,𝑭, and𝜃(Line 3).
Then, it alternately updates the latent factors and parameters until
they converge. In each iteration, we consider ¤𝑺to be given and¤𝑼
to be a model parameter. In an iteration, we first conduct a Viterbi
approximation to calculate the most likely regime assignments 𝑭
(Line 5-12). Then, we infer the expectations of 𝒁and¤𝑽(Line 13-15
and Line 16-18), and we update ¤𝑼and model parameters 𝜃(Line 20),
and at the end of the iteration, we update ¤𝑺(Line 21).
4.3 Complexity analysis
Lemma 1. The time complexity of MissNet is
𝑂(#𝑖𝑡𝑒𝑟·(𝐾2Í𝑇
𝑡=1(𝐿3+𝐿2𝑁𝑡+𝐿𝑁2
𝑡+𝑁3
𝑡)+𝐾𝐿2𝑁2+𝐾𝑇𝐿2𝑁+𝐾𝑁3)).
 
2300KDD ’24, August 25–29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
Algorithm 1 MissNet ( ˜𝑿,𝑾,𝐿,𝐾,𝛼,𝜆 )
1:Input: (a) partially observed multivariate time series ˜𝑿,
(b) indicator matrix 𝑾,
(c) and hyperparameters 𝐿,𝐾,𝛼,𝜆
2:Output: latent factors 𝒁,¤𝑽,¤𝑼,¤𝑺,𝑭model parameters 𝜃, and ˆ𝑿
3:Initialize ˆ𝑿with linear interpolation, 𝒁,¤𝑽,¤𝑼,¤𝑺,𝑭, and𝜃;
4:repeat
5:for𝑡=1 :𝑇do
6: for𝑘=1 :𝐾do
7: for𝑙=1 :𝐾do
8: Calculate partial cost 𝐽𝑡,𝑡−1,𝑘,𝑙using Eq. (15)
9: end for
10: end for
11: end for
12: Infer 𝑭and obtain 𝝁𝑡,𝚿𝑡based on Eq. (13), (14)
13: for𝑡=𝑇: 1do
14: Infer ˆ𝝁𝑡,ˆ𝚿𝑡,E[𝒛𝑡𝒛′
𝑡−1],E[𝒛𝑡𝒛′
𝑡]by Eq. (17), (18)
15: end for
16: for𝑗=1 :𝑁do
17: InferE[𝒗(𝑘)
𝑗],E[𝒗(𝑘)
𝑗𝒗(𝑘)′
𝑗]using Eq. (19)
18: end for
19: Set𝒁={ˆ𝝁1,..., ˆ𝝁𝑇},{𝑽(𝑘)={𝝂(𝑘)
1,...,𝝂(𝑘)
𝑁}}𝐾
𝑘=1
20: Update¤𝑼,𝜃and ˆ𝑿by Eq. (21) - (24)
21: Update¤𝑺based on Eq. (25)
22:until convergence;
23:return{𝒁,¤𝑽,¤𝑼,¤𝑺,𝑭,𝜃,ˆ𝑿};
Proof. See Appendix A.2. □
𝑁𝑡represents the number of observed features of ¯𝒙𝑡. Note that
𝐾2Í𝑇
𝑡=1(𝐿3+𝐿2𝑁𝑡+𝐿𝑁2
𝑡+𝑁3
𝑡)is upper bounded by 𝐾2𝑇𝑁3. In
practice, the length of the time series ( 𝑇) is often orders of mag-
nitude greater than the number of features ( 𝑁). Hence, the actual
running time of MissNet is dominated by the term related to 𝑇,
which is linear in 𝑇.
Lemma 2. The space complexity of MissNet is
𝑂(𝑇𝑁+𝐾2𝑇𝐿2+𝐾𝐿2𝑁+𝐾𝑁2).
Proof. See Appendix A.2. □
5 Experiments
In this section, we empirically evaluate our approach against state-
of-the-art baselines on 12 datasets. We present experimental results
for the following questions:
Q1. Effectiveness: How accurate is the proposed MissNet for
recovering missing values?
Q2. Scalability: How does the proposed algorithm scale?
Q3. Interpretability: How can MissNet help us understand the
data?
5.1 Experimental setup
5.1.1 Datasets. We use the following datasets.
Synthetic. We generate two types of synthetic data, PatternA and
PatternB, five times each, by defining 𝒁and¤𝑼. We set𝑇=1000,𝑁=50and𝐿=10(Appendix B.1). PatternA has one regime ( 𝐾=1),
and in PatternB ( 𝐾=2), two regimes switch at every 200timesteps.
MotionCapture. This dataset contains nine types of full body
motions of MotionCapture database3. Each motion measures the
positions of 41bones in the human body, resulting in a total of 123
features (X, Y, and Z coordinates).
Motes. This dataset consists of temperature measurements from
the54sensors deployed in the Intel Berkeley Research Lab4. We
use hourly data for the first two weeks (03-01 ∼03-14). Originally,
9.6%of the data is missing, including a blackout from 03-10 to 03-11
where all the values are missing.
5.1.2 Data preprocessing. We generate a synthetic missing block at
a length of 0∼5%of the data length and place it randomly until the
total missing rate reaches {10,20,...80%}. Thus, a missing block
can be longer than 0.05𝑇when it overlaps. An additional 10%of
missing values are added for hyperparameter tuning. Each dataset
feature is normalized independently using a z-score so that each
dataset has a zero mean and a unit variance.
5.1.3 Comparison methods. We compare our method with state-of-
the-art imputation methods ranging from classical baselines (Linear
and Quadratic), MF-based methods (SoftImpute, CDRec and TRMF),
SSM-based methods (DynaMMo and DCMF), to DL-based methods
(BRITS, SAITS and TIDER).
•Linear/Quadratic5use linear/quadratic equations to inter-
polate missing values.
•SoftImpute [ 30] first fills in missing values with zero, then
alternates between recovering the missing values and updat-
ing the SVD using the recovered matrix.
•CDRec [ 22] is a memory-efficient algorithm that iterates
centroid decomposition (CD) and missing value imputation
until they converge.
•TRMF [ 55] is based on MF that imposes temporal dependency
among the data points with the AR model.
•DynaMMo [ 25] first fills in missing values using linear in-
terpolation and then uses the EM algorithm to iteratively
recover missing values and update the LDS model.
•DCMF [ 5] adds a contextual constraint to SSM and captures
inter-correlation by a predefined network. As suggested in
the original paper, we give the cosine similarity between
each pair of time series calculated after linear interpolation
as a predefined network. This method is similar to MissNet
if we set𝐾=1, employ a predefined network that is fixed
throughout the algorithm, and eliminate the effect of regime-
switching and network inference models from MissNet.
•BRITS [ 7] imputes missing values according to hidden states
from bidirectional RNN.
•SAITS [12] is a self-attention-based model that jointly opti-
mizes imputation and reconstruction to perform the missing
value imputation of multivariate time series.
•TIDER [ 26] learns disentangled seasonal and trend repre-
sentations by employing a neural network combined with
MF.
3http://mocap.cs.cmu.edu
4https://db.csail.mit.edu/labdata/labdata.html
5https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html
 
2301Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) PatternA
(𝑇=1000)
(b) PatternB
(𝑇=1000)
(c) Run
(𝑇=145)
(d) Bouncy walk
(𝑇=644)
(e) Swing shoulder
(𝑇=804)
(f) Walk slow
(𝑇=1223)
(g) Mawashigeri
(𝑇=1472)
(h) Jump distance
(𝑇=547)
(i) Wave hello
(𝑇=299)
(j) Football throw
(𝑇=1091)
(k) Boxing
(𝑇=773)
(l) Motes
(𝑇=336)
Figure 3: RMSE of (a), (b) Synthetic ( 𝑁=50), (c)∼(k) MotionCapture ( 𝑁=123) and (l) Motes ( 𝑁=54) datasets.
Figure 4: Critical difference diagram of real-world datasets.
5.1.4 Hyperparameter setting. ForMissNet, we use the latent di-
mensions of 10,30and15for Synthetic, MotionCapture and Motes,
respectively, and we set 𝜆=1.0,𝛼=0.5for all datasets. We set
the correct number of regimes on Synthetic datasets; we vary
𝐾={1,2,3}for other datasets. We list the detailed hyperparameter
settings for the baselines in Appendix B.2.
5.1.5 Evaluation metric. To evaluate the effectiveness, we use Root
Mean Square Error (RMSE) of the observed time series [5].
5.2 Results
5.2.1 Q1. Effectiveness. We show the effectiveness of MissNet
over baselines in missing value imputation.
Synthetic. Fig. 3 (a) and (b) show the results obtained with Syn-
thetic datasets. SSM and MF-based methods perform worse with
PatternB than with PatternA due to the increased complexity of
data. DL-based methods, especially BRITS, are less affected thanks
to their high modeling power. MissNet significantly outperforms
DCMF for PatternB although it produces similar results for Pat-
ternA. This is because DCMF fails to capture inter-correlation with
PatternB since it can only use one predefined network and cannot
afford a change of network. Meanwhile, MissNet can capture the
inter-correlation for two different regimes thanks to our regime-
switching model. However, MissNet fails to discover the correct
transition when the missing rate exceeds 70%, and RMSE becomes
similar to DCMF.
MotionCapture and Motes. The results for MotionCapture and
Motes datasets are shown in Fig. 3 (c) ∼(l). We can see that MissNetand DCMF constantly outperform other baselines thanks to their
ability to exploit inter-correlation explicitly.
Fig. 4 shows the corresponding critical difference diagram for
all missing rates based on the Wilcoxon-Holm method [ 20], where
methods that are not connected by a bold line are significantly
different in average rank. This confirms that MissNet significantly
outperforms other methods, including DCMF, in average rank. Our
algorithm for repeatedly inferring networks and the use of ℓ1-norm
enables the inference of adequate networks for imputation, con-
tributing to better results than DCMF, which uses cosine similarity
as a predefined network that may contain spurious correlations
in the presence of missing values. Note that MissNet and DCMF
exhibit only minor differences when the missing rate is low (10%)
because a plausible predefined network can be calculated from
observed data.
Classical Linear and Quadratic baselines are unsuitable for im-
puting missing blocks since they impute missing values mostly
from neighboring observed points and cannot capture temporal
patterns when there are large gaps. DL-based methods lack suffi-
cient training data and are not suitable for the data we use here,
making them perform particularly poorly at a high missing rate, as
also noted in [ 23]. MF-based methods, SoftImpute and CDRec, have
a higher RMSE than SSM-based methods since they do not model
the temporal dynamics of the data. TRMF utilizes temporal depen-
dency with the AR model, however, it can only capture certain
lags specified on the hyperparameter of the AR model. SSM-based
methods are superior in imputation to other groups owing to their
ability to capture temporal dependency in latent space. DynaMMo
implicitly captures inter-correlation in latent space. Therefore, it is
no match for MissNet or DCMF.
Fig. 5 demonstrates the results for the MotionCapture Run dataset
(missing rate =60%). We compare the imputation result for the sen-
sor at RKNE provided by the top five methods in terms of average
rank, including MissNet, in Fig. 5 (a). BRITS and SoftImpute fail
to capture the dynamics of time series while providing a good fit
to observed values. The imputation of DynaMMo is smooth, but
 
2302KDD ’24, August 25–29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
(a) Imputation results at RKNE
 (b) Y-network of MissNet
Figure 5: Case study on MotionCapture Run dataset.
(a) Impact of 𝐿
 (b) Impact of 𝛼
 (c) Impact of 𝜆
Figure 6: Hyperparameter sensitivity results.
some parts are imprecise since it cannot explicitly exploit inter-
correlation. MissNet and DCMF can effectively exploit other ob-
served features associated with RKNE, thereby accurately imputing
missing values where other methods fail (e.g., 𝑇=20∼40,60∼80).
Fig. 5 (b) shows the sensor network of Y-coordinate values obtained
byMissNet plotted on the human body, where a green/yellow dot
(node) indicates a sensor placed on the front/back of the body and
the thickness and color (blue/red) of the edges are the value and
sign (positive/negative) of partial correlations, respectively. We can
see that the sensors located close together have edges, meaning they
are conditionally dependent given all other sensor values. For ex-
ample, the sensor at RKNE has edges between RTHI, RSHN, LKNE,
and LTHI. They are located to RKNE nearby and show similar dy-
namics, thus it is reasonable to consider that they are connected.
Since MissNet can infer such a meaningful network from partially
observed data, the imputation of MissNet is more accurate than
that of DCMF.
5.2.2 Hyperparameter sensitivity. We take the Motes dataset and
show the impact of hyperparameters: the latent dimension 𝐿, the
network parameter 𝛼, and the sparse parameter 𝜆. We show the
mean RMSE of all missing rates.
Latent dimension. Fig. 6 (a) shows the impact of 𝐿. As𝐿becomes
larger, the model’s fitting against the observed data increases. As
we can see, the RMSE is constantly decreasing as 𝐿increases and
stabilizes after 15. This shows that MissNet does not overfit the
observed data even for a large 𝐿.
Network parameter. 𝛼determines the contributions of inter-
correlation and temporal dependency to learning ¤𝑼. If𝛼=0, the
Figure 7: Scalability of MissNet.
(a) Results of regime assignments
(b) Regime #1,#of edges: 116
 (c) Regime #2,#of edges: 134
Figure 8: Case study on Motes dataset.
contextual matrix ¤𝑺is ignored. If 𝛼=1, only¤𝑺is considered for
learning¤𝑼. Fig. 6 (b) shows the results of varying 𝛼and they are
robust except when 𝛼=1(RMSE =0.76). We can see that 𝛼=0.4
shows the best result, indicating that both temporal dependency
and inter-correlation are important for precise imputation.
Sparse parameter. 𝜆controls the sparsity of the networks ¤𝚯
throughℓ1-norm. The bigger 𝜆becomes, the more sparse the net-
works become, resulting in MissNet considering only strong inter-
play. By contrast, when 𝜆is small, MissNet considers insignificant
interplays. Fig. 6 (c) shows the impact of 𝜆. We can see that the
sparsity of the networks affects the accuracy, and the best 𝜆exists
between 0.1and10. Thus, the ℓ1-norm constraint helps MissNet
to exploit important relationships.
5.2.3 Q2. Scalability. We test the scalability of the MissNet algo-
rithm by changing the number of the data length ( 𝑇) in PatternA.
Fig. 7 shows the computation time for one iteration plotted with the
data length. As it shows, our proposed MissNet algorithm scales
linearly with regard to the data length 𝑇.
5.2.4 Q3. Interpretability. We demonstrate how MissNet helps us
understand data. We have shown an example with the MotionCap-
ture Run dataset in Fig. 5 (b) where MissNet provides an inter-
pretable network. Here, we demonstrate the results on the Motes
dataset (missing rate =30%) of MissNet (𝐾=2). Fig. 8 (a) shows
the regime assignments 𝑭, and MissNet mostly assigns night hours
to regime #1and working hours (about 9am. to 10pm.) to regime
#2, suggesting that they have different networks. Fig. 8 (b) and (c)
show the networks for regimes #1and#2obtained by MissNet
plotted on the building layout. The sensor numbers in the figure
are plotted on the actual sensor deployments. As we can see, the
two regimes have different networks, and a common feature is that
the neighboring sensors tend to form edges, which aligns with our
expectations, considering that the sensors measure temperature
and, thus, neighboring sensors correlate. The network of regime #2
has more edges than that of #1, and the edges are 1.2times longer
 
2303Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD ’24, August 25–29, 2024, Barcelona, Spain
on average, which might be caused by air convection due to the
movement of people during working hours. Consequently, MissNet
provides regime assignments and sparse networks, which help us
understand how data can be separated and important relationships
for imputation.
6 Conclusion
In this paper, we proposed an effective missing value imputation
method for multivariate time series, namely MissNet, which cap-
tures temporal dependency based on latent space and inter-correlation
by the inferred networks while discovering regimes. Our proposed
method has the following properties: (a) Effective : it outperforms
the state-of-the-art algorithms for multivariate time series imputa-
tion. (b) Scalable : the computation time of MissNet scales linearly
with regard to the length of the data. (c) Interpretable : it provides
sparse networks and regime assignments, which help us understand
the important relationships for imputation visually. Our extensive
experiments demonstrated the above properties of MissNet.
Acknowledgments
The authors would like to thank the anonymous referees for their
valuable comments and helpful suggestions. This work was sup-
ported by JSPS KAKENHI Grant-in-Aid for Scientific Research Num-
ber JP21H03446, JP22K17896, NICT JPJ012368C03501, JST-AIP JP-
MJCR21U4, JST CREST JPMJCR23M3.
References
[1]Juan Miguel Lopez Alcaraz and Nils Strodthoff. 2022. Diffusion-based time series
imputation and forecasting with structured state space models. arXiv preprint
arXiv:2208.09399 (2022).
[2]Parikshit Bansal, Prathamesh Deshpande, and Sunita Sarawagi. 2021. Missing
Value Imputation on Multidimensional Time Series. Proc. VLDB Endow. 14, 11
(jul 2021), 2533–2545.
[3]Jeroen Berrevoets, Fergus Imrie, Trent Kyono, James Jordon, and Mihaela van der
Schaar. 2023. To impute or not to impute? missing data in treatment effect
estimation. In International Conference on Artificial Intelligence and Statistics.
PMLR, 3568–3590.
[4]Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
2011. Distributed Optimization and Statistical Learning via the Alternating
Direction Method of Multipliers. Found. Trends Mach. Learn. 3, 1 (2011), 1–122.
[5]Yongjie Cai, Hanghang Tong, Wei Fan, and Ping Ji. 2015. Fast mining of a network
of coevolving time series. In Proceedings of the 2015 SIAM International Conference
on Data Mining. SIAM, 298–306.
[6]Yongjie Cai, Hanghang Tong, Wei Fan, Ping Ji, and Qing He. 2015. Facets: Fast
comprehensive mining of coevolving high-order time series. In KDD. 79–88.
[7]Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. Brits:
Bidirectional recurrent imputation for time series. Advances in neural information
processing systems 31 (2018).
[8]Stanislas Chambon, Mathieu N Galtier, Pierrick J Arnal, Gilles Wainrib, and
Alexandre Gramfort. 2018. A deep learning architecture for temporal sleep stage
classification using multivariate and multimodal time series. IEEE Transactions
on Neural Systems and Rehabilitation Engineering 26, 4 (2018), 758–769.
[9]Xinyu Chen and Lijun Sun. 2021. Bayesian temporal factorization for multi-
dimensional time series prediction. IEEE Transactions on Pattern Analysis and
Machine Intelligence 44, 9 (2021), 4659–4673.
[10] Andrea Cini, Ivan Marisca, and Cesare Alippi. 2021. Filling the g_ap_s: Mul-
tivariate time series imputation by graph neural networks. arXiv preprint
arXiv:2108.00298 (2021).
[11] Joel Janek Dabrowski, Ashfaqur Rahman, Andrew George, Stuart Arnold, and
John McCulloch. 2018. State Space Models for Forecasting Water Quality Vari-
ables: An Application in Aquaculture Prawn Farming (KDD ’18). Association for
Computing Machinery, New York, NY, USA, 177–185.
[12] Wenjie Du, David Côté, and Yan Liu. 2023. Saits: Self-attention-based imputation
for time series. Expert Systems with Applications 219 (2023), 119619.
[13] Yangxin Fan, Xuanji Yu, Raymond Wieser, David Meakin, Avishai Shaton, Jean-
Nicolas Jaubert, Robert Flottemesch, Michael Howell, Jennifer Braid, Laura Bruck-
man, Roger French, and Yinghui Wu. 2023. Spatio-Temporal Denoising Graph Au-
toencoders with Data Augmentation for Photovoltaic Data Imputation. Proc. ACMManag. Data 1, 1, Article 50 (may 2023), 19 pages. https://doi.org/10.1145/3588730
[14] Emily Fox, Erik Sudderth, Michael Jordan, and Alan Willsky. 2008. Nonparametric
Bayesian learning of switching linear dynamical systems. Advances in neural
information processing systems 21 (2008).
[15] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2008. Sparse inverse
covariance estimation with the graphical lasso. Biostatistics 9, 3 (2008), 432–441.
[16] Zoubin Ghahramani. 1998. Learning dynamic Bayesian networks. Springer Berlin
Heidelberg, Berlin, Heidelberg, 168–197.
[17] N Hairi, Hanghang Tong, and Lei Ying. 2019. NetDyna: mining networked
coevolving time series with missing values. In 2019 IEEE International Conference
on Big Data (Big Data). IEEE, 503–512.
[18] David Hallac, Youngsuk Park, Stephen P. Boyd, and Jure Leskovec. 2017. Network
Inference via the Time-Varying Graphical Lasso. In KDD. 205–213.
[19] David Hallac, Sagar Vare, Stephen P. Boyd, and Jure Leskovec. 2017. Toeplitz
Inverse Covariance-Based Clustering of Multivariate Time Series Data. In KDD.
215–223.
[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,
and Pierre-Alain Muller. 2019. Deep learning for time series classification: a
review. Data mining and knowledge discovery 33, 4 (2019), 917–963.
[21] Baoyu Jing, Hanghang Tong, and Yada Zhu. 2021. Network of tensor time series.
InProceedings of the Web Conference 2021. 2425–2437.
[22] Mourad Khayati, Michael Böhlen, and Johann Gamper. 2014. Memory-efficient
centroid decomposition for long time series. In 2014 IEEE 30th International
Conference on Data Engineering. IEEE, 100–111.
[23] Mourad Khayati, Alberto Lerner, Zakhar Tymchenko, and Philippe Cudré-
Mauroux. 2020. Mind the gap: an experimental evaluation of imputation of
missing values techniques in time series. In Proceedings of the VLDB Endowment,
Vol. 13. 768–782.
[24] Mladen Kolar and Eric P Xing. 2012. Estimating sparse precision matrices from
data with missing values. In International Conference on Machine Learning. 635–
642.
[25] Lei Li, James McCann, Nancy S Pollard, and Christos Faloutsos. 2009. Dynammo:
Mining and summarization of coevolving sequences with missing values. In KDD.
507–516.
[26] SHUAI LIU, Xiucheng Li, Gao Cong, Yile Chen, and YUE JIANG. 2022. Multivari-
ate Time-series Imputation with Disentangled Temporal Representations. In The
Eleventh International Conference on Learning Representations.
[27] Yuehua Liu, Tharam Dillon, Wenjin Yu, Wenny Rahayu, and Fahed Mostafa. 2020.
Missing value imputation for industrial IoT sensor data with large gaps. IEEE
Internet of Things Journal 7, 8 (2020), 6855–6867.
[28] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social
recommendation using probabilistic matrix factorization. In Proceedings of the
17th ACM conference on Information and knowledge management. 931–940.
[29] Solt Kovács Malte Londschien and Peter Bühlmann. 2021. Change-Point Detection
for Graphical Models in the Presence of Missing Values. Journal of Computational
and Graphical Statistics 30, 3 (2021), 768–779.
[30] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. 2010. Spectral regulariza-
tion algorithms for learning large incomplete matrices. The Journal of Machine
Learning Research 11 (2010), 2287–2322.
[31] Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, and Su-In Lee.
2014. Node-Based Learning of Multiple Gaussian Graphical Models. J. Mach.
Learn. Res. 15, 1 (jan 2014), 445–488.
[32] Ricardo Pio Monti, Peter Hellyer, David Sharp, Robert Leech, Christoforos Anag-
nostopoulos, and Giovanni Montana. 2014. Estimating time-varying brain con-
nectivity networks from functional MRI time series. NeuroImage 103 (2014),
427–443.
[33] A. Namaki, A.H. Shirazi, R. Raei, and G.R. Jafari. 2011. Network analysis of a
financial market based on genuine correlation and threshold method. Physica A:
Statistical Mechanics and its Applications 390, 21 (2011), 3835–3841.
[34] Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai. 2024.
Dynamic Multi-Network Mining of Tensor Time Series. In Proceedings of the
ACM on Web Conference 2024 (WWW ’24). 4117–4127.
[35] Spiros Papadimitriou, Jimeng Sun, and Christos Faloutsos. 2005. Streaming
pattern discovery in multiple time-series. (2005).
[36] Vladimir Pavlovic, James M Rehg, Tat-Jen Cham, and Kevin P Murphy. 1999. A
dynamic Bayesian network approach to figure tracking using learned dynamic
models. In Proceedings of the seventh IEEE international conference on computer
vision, Vol. 1. IEEE, 94–101.
[37] Zhen Qin, Yibo Zhang, Shuyu Meng, Zhiguang Qin, and Kim-Kwang Raymond
Choo. 2020. Imaging and fusing time series for wearable sensor-based human
activity recognition. Information Fusion 53 (2020), 80–87.
[38] Xiaobin Ren, Kaiqi Zhao, Patricia Riddle, Katerina Taškova, Lianyan Li, and
Qingyi Pan. 2023. DAMR: Dynamic Adjacency Matrix Representation Learning
for Multivariate Time Series Imputation. SIGMOD (2023). https://doi.org/10.
1145/3589333
[39] Mark Rogers, Lei Li, and Stuart J Russell. 2013. Multilinear dynamical systems for
tensor time series. Advances in Neural Information Processing Systems 26 (2013).
 
2304KDD ’24, August 25–29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
[40] Tolou Shadbahr, Michael Roberts, Jan Stanczuk, Julian Gilbey, Philip Teare, Sören
Dittmer, Matthew Thorpe, Ramon Viñas Torné, Evis Sala, Pietro Lió, Mishal Patel,
Jacobus Preller, Ian Selby, Anna Breger, Jonathan R. Weir-McCall, Effrossyni
Gkrania-Klotsas, Anna Korhonen, Emily Jefferson, Georg Langs, Guang Yang,
Helmut Prosch, Judith Babar, Lorena Escudero Sánchez, Marcel Wassin, Markus
Holzer, Nicholas Walton, Pietro Lió, James H. F. Rudd, Tuomas Mirtti, Antti Sakari
Rannikko, John A. D. Aston, Jing Tang, and Carola-Bibiane Schönlieb. 2023. The
impact of imputation quality on machine learning classifiers for datasets with
missing values. Communications Medicine 3, 1 (2023).
[41] Satya Narayan Shukla and Benjamin M Marlin. 2021. Multi-time attention
networks for irregularly sampled time series. arXiv preprint arXiv:2101.10318
(2021).
[42] Nicolas Städler and Peter Bühlmann. 2012. Missing values: sparse inverse covari-
ance estimation and an extension to sparse regression. Statistics and Computing
22 (2012), 219–235.
[43] Nicolas Städler and Peter Bühlmann. 2012. Missing values: sparse inverse covari-
ance estimation and an extension to sparse regression. Statistics and Computing
22 (2012), 219–235.
[44] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. Csdi: Con-
ditional score-based diffusion models for probabilistic time series imputation.
Advances in Neural Information Processing Systems 34 (2021), 24804–24816.
[45] Federico Tomasi, Veronica Tozzo, and Annalisa Barla. 2021. Temporal Pattern
Detection in Time-Varying Graphical Models. In ICPR. 4481–4488.
[46] Federico Tomasi, Veronica Tozzo, Saverio Salzo, and Alessandro Verri. 2018.
Latent Variable Time-varying Network Inference. In KDD. 2338–2346. https:
//doi.org/10.1145/3219819.3220121
[47] Veronica Tozzo, Federico Ciech, Davide Garbarino, and Alessandro Verri. 2021.
Statistical Models Coupling Allows for Complex Local Multivariate Time Series
Analysis. In KDD. 1593–1603.
[48] Veronica Tozzo, Davide Garbarino, and Annalisa Barla. 2020. Missing Values
in Multiple Joint Inference of Gaussian Graphical Models. In Proceedings of the
10th International Conference on Probabilistic Graphical Models (Proceedings of
Machine Learning Research, Vol. 138), Manfred Jaeger and Thomas Dyhre Nielsen
(Eds.). PMLR, 497–508.
[49] Andrew Viterbi. 1967. Error bounds for convolutional codes and an asymptotically
optimum decoding algorithm. IEEE transactions on Information Theory 13, 2 (1967),
260–269.
[50] Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew
Margenot, and Hanghang Tong. 2023. Networked time series imputation via
position-aware graph enhanced variational autoencoders. In KDD. 2256–2268.
[51] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang,
Zhengyang Zhou, and Yang Wang. 2023. An Observed Value Consistent Diffu-
sion Model for Imputing Missing Values in Multivariate Time Series. In KDD.
2409–2418.
[52] Xiuwen Yi, Yu Zheng, Junbo Zhang, and Tianrui Li. 2016. ST-MVL: filling missing
values in geo-sensory time series data. In Proceedings of the 25th International
Joint Conference on Artificial Intelligence.
[53] Jinsung Yoon, James Jordon, and Mihaela Schaar. 2018. Gain: Missing data impu-
tation using generative adversarial nets. In International conference on machine
learning. PMLR, 5689–5698.
[54] Jinsung Yoon, William R Zame, and Mihaela van der Schaar. 2018. Estimating
missing data in temporal data streams using multi-directional recurrent neural
networks. IEEE Transactions on Biomedical Engineering 66, 5 (2018), 1477–1490.
[55] Hsiang-Fu Yu, Nikhil Rao, and Inderjit S. Dhillon. 2016. Temporal Regularized
Matrix Factoriztion for High-dimensional Time Series Prediction. In Advances in
Neural Information Processing Systems 28.
[56] Dejiao Zhang and Laura Balzano. 2016. Global convergence of a grassmannian
gradient descent algorithm for subspace estimation. In Artificial Intelligence and
Statistics. PMLR, 1460–1468.
A Proposed model
A.1 Symbols
Table 2 lists the main symbols we use throughout this paper.
A.2 Complexity analysis
Proof of Lemma 1.Proof. The overall time complexity is composed of four parts
by taking the most time-consuming part of equations for each
iteration considering 𝑁>𝑁𝑡,𝐿: the complexity for the inference
of𝒁and𝑭is𝑂(𝐾2Í𝑇
𝑡=1(𝐿3+𝐿2𝑁𝑡+𝐿𝑁2
𝑡+𝑁3
𝑡))related to Eq. (13)
and Eq. (15); the inference of ¤𝑽is𝑂(𝐾𝐿2𝑁2)(Eq. (19)); M step is
𝑂(𝐾𝑇𝐿2𝑁)related to the calculation of ¤𝑼(Eq. (21)); and the update
of¤𝚯is𝑂(𝐾𝑁3)(Eq. (23)). Thus, the overall time complexity is
𝑂(#𝑖𝑡𝑒𝑟·(𝐾2Í𝑇
𝑡=1(𝐿3+𝐿2𝑁𝑡+𝐿𝑁2
𝑡+𝑁3
𝑡)+𝐾𝐿2𝑁2+𝐾𝑇𝐿2𝑁+
𝐾𝑁3)). □
Proof of Lemma 2.
Proof. The space complexity is composed of three parts: stor-
ing input dataset 𝑿is𝑂(𝑇𝑁); intermediate values in E step are
𝑂(𝐾2𝑇𝐿2+𝐾𝐿2𝑁); and storing parameter set is 𝑂(𝐾𝑁2). Thus, the
overall space complexity is 𝑂(𝑇𝑁+𝐾2𝑇𝐿2+𝐾𝐿2𝑁+𝐾𝑁2).□
Table 2: Symbols and definitions.
Symbol Definition
𝑿 Multivariate time series 𝑿={𝒙1,𝒙2,...,𝒙𝑇} ∈
R𝑁×𝑇
𝑾 Indicator matrix 𝑾∈R𝑁×𝑇
˜𝑿 Partially observed multivariate time series ˜𝑿=𝑾◦𝑿
ˆ𝑿 Inputed multivariate time series
𝒁 Time series latent states 𝒁∈R𝐿×𝑇
𝑽(𝑘)Contextual latent factor of 𝑘𝑡ℎ-regime 𝑽(𝑘)∈R𝐿×𝑁
𝑺(𝑘)Contextual matrix of 𝑘𝑡ℎ-regime 𝑺(𝑘)∈R𝑁×𝑁
𝑩 Transition matrix 𝑩∈R𝐿×𝐿
𝑼(𝑘)Observation matrix of 𝑘𝑡ℎ-regime 𝑼(𝑘)∈R𝑁×𝐿
𝑭 Regime assignments 𝑭∈R𝐾×𝑇
𝝆(𝑘)Mean vector of 𝑘𝑡ℎ-regime 𝝆(𝑘)∈R𝑁
𝚯(𝑘)Inverse covariance matrix (i.e., network) of 𝑘𝑡ℎ-
regime 𝚯(𝑘)∈R𝑁×𝑁
𝑁 Number of features
𝑇 Number of timesteps
𝐿 Number of latent dimensions
𝐾 Number of regimes
𝛼 Trade-off between temporal dependency and inter-
correlation
𝜆 Parameter for ℓ1-norm that regulates network sparsity
A.3 Updating parameters
The parameters are updated as follows:
𝑩𝑛𝑒𝑤=𝑇∑︁
𝑡=2E[𝒛𝑡𝒛′
𝑡−1]𝑇∑︁
𝑡=2E[𝒛𝑡−1𝒛′
𝑡−1]−1
,
𝒛𝑛𝑒𝑤
0=E[𝒛1],
𝚿𝑛𝑒𝑤
0=E[𝒛1𝒛′
1]−E[𝒛1]E[𝒛′
1],
 
2305Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD ’24, August 25–29, 2024, Barcelona, Spain
(𝜎2
𝒁)𝑛𝑒𝑤=1
(𝑇−1)𝐿tr𝑇∑︁
𝑡=2E[𝒛𝑡𝒛′
𝑡]−𝑩𝑇∑︁
𝑡=2E[𝒛𝑡𝒛′
𝑡]
,
(𝜎2
𝑿(𝑘))𝑛𝑒𝑤=1
Í𝑇
𝑡=1(𝒇𝑡=𝑘)Í𝑁
𝑖=1𝑾𝑖,𝑡
𝑇∑︁
𝑡=1(𝒇𝑡=𝑘)𝑡𝑟
(¯𝑼(𝑘)
𝑡)𝑛𝑒𝑤E[𝒛𝑡𝒛′
𝑡](¯𝑼(𝑘)
𝑡)𝑛𝑒𝑤′
+𝑇∑︁
𝑡=1(𝒇𝑡=𝑘)
(¯𝒙𝑡)′¯𝒙𝑡−2(¯𝒙𝑡)′((¯𝑼(𝑘)
𝑡)𝑛𝑒𝑤E[𝒛𝑡])
,
(𝜎2
𝑺(𝑘))𝑛𝑒𝑤=1
𝑁2𝑁∑︁
𝑗=1
𝒔(𝑘)′
𝑗𝒔(𝑘)
𝑗−2𝒔(𝑘)′
𝑗((𝑼(𝑘))𝑛𝑒𝑤E[𝒗(𝑘)
𝑗])
+𝑡𝑟
(𝑼(𝑘))𝑛𝑒𝑤(𝑁∑︁
𝑗=1E[𝒗(𝑘)
𝑗𝒗(𝑘)′
𝑗])(𝑼(𝑘)𝑛𝑒𝑤)′
,
(𝜎2
𝑽(𝑘))𝑛𝑒𝑤=1
𝑁𝐿𝑁∑︁
𝑗=1tr(E[𝒗(𝑘)
𝑗𝒗(𝑘)′
𝑗]). (26)
B Experiments
B.1 Synthetic data generation
We first generate a latent factor containing a linear trend, a sinu-
soidal seasonal pattern, and a noise, 𝒁𝑖,𝑡=sin(2𝜋𝛽
𝑇𝑡)+𝛾𝑡+𝜂,
s.t.1<𝛽<20,0.3<|𝛾|<1,𝜂∼N( 0,0.3), where 𝒁∈R𝐿×𝑇.
We then project the latent factor with object matrix 𝑿=𝑼(𝑘)𝒁,
where 𝑿∈R𝑁×𝑇, and 𝑼(𝑘)∈R𝑁×𝐿is a random graph created as
follows [31]:
(1)Set𝑼(𝑘)equal to the adjacency matrix of an Erdős-Rényi
directed random graph, where every edge has a 20%chance
of being selected.
(2)Set selected edge 𝑼(𝑘)
𝑖,𝑗∼Uniform([−0.6,−0.3]∪[ 0.3,0.6]),
where 𝑼(𝑘)
𝑖,𝑗denotes the weight between variables 𝑖and𝑗.
We set𝑇=1000,𝑁=50,𝐿=10. We generate two types of syn-
thetic data, PatternA and PatternB, five times each, where 𝐾=1,2,
respectively. In PatternB, the regime switches every 200timesteps.B.2 Hyperparameters
We describe the hyperparameters of the baselines. For Synthetic
datasets, we give a latent dimension of 10for all baselines. For
a fair comparison, we set the latent dimension of the SSM-based
methods at the same value as MissNet. For the MF-based methods,
we vary the latent dimension {3,5,10,15,20,30,40}. We vary the AR
parameter for TRMF {[1,2,3,4,5],[1,24]}. To learn the DL-based
methods, we add 10%of the data as missing values for training the
model. We vary the window size {16,32,𝑇}. Other hyperparameters
are the same as the original codes.
C Discussion
While MissNet achieved superior performance against state-of-
the-art baselines in missing value imputation, here, we mention
two limitations of MissNet in terms of sparse network inference
and data size.
As mentioned in Section 5.2.1, MissNet fails to discover the
correct transition when the missing rate exceeds 70%. However,
we claim that MissNet failing to discover the correct transition
when the missing rate exceeds 70%is reasonable; rather, correctly
discovering transition up to 60% is valuable. Several studies [ 24,
43,48] tackled the sparse network inference under the existence
of missing values. They aim to infer the correct network and, thus,
only utilize the observed value for the network inference. Since
observing a complete pair at a high missing rate is rare, it is difficult
to infer the correct network. Therefore, the maximum missing rate
in their experiments is 30%. Although the experimental settings
are different from ours, we can say that the task of sparse network
inference in the presence of missing values itself is challenging.
As shown in the experiments, MissNet performs well even when
a relatively small number of samples ( 𝑇) and a large number of
features (𝑁) since MissNet is a parametric model and we assume
the sparse networks to capture inter-correlation. This cannot be
achieved by DL models, which contain a massive number of param-
eters that require a large amount of 𝑇, especially when 𝑁is large
since all the relationships between features need to be learned. How-
ever, unlike DL models, the increased number of samples may not
greatly improve MissNet’s performance as it has a much smaller
number of parameters than DL models, even though switching
sparse networks increases the model’s flexibility.
 
2306