Achieving A Better Tradeoff in Multi-stage Recommender
Systems Through Personalization
Ariel Evnine
Meta
Menlo Park, CA, USA
ariel@meta.comStratis Ioannidis
Northeastern University
Boston, MA, USA
ioannidis@ece.neu.eduDimitris Kalimeris
Meta
Menlo Park, CA, USA
kalimeris@meta.com
Shankar Kalyanaraman∗
Meta
Menlo Park, CA, USA
kshankar@meta.comWeiwei Li
Meta
Menlo Park, CA, USA
weiweili90@meta.comIsrael Nir
Meta
Menlo Park, CA, USA
rouli@meta.com
Wei Sun
Meta
Menlo Park, CA, USA
weisunnju@meta.comUdi Weinsberg
Meta
Menlo Park, CA, USA
udi@meta.com
ABSTRACT
Recommender systems in social media websites provide value to
their communities by recommending engaging content and mean-
ingful connections. Scaling high-quality recommendations to bil-
lions of users in real-time requires sophisticated ranking models
operating on a vast number of potential items to recommend, be-
coming prohibitively expensive computationally. A common tech-
nique “funnels” these items through progressively complex models
(“multi-stage”), each ranking fewer items but at higher compu-
tational cost for greater accuracy. This architecture introduces a
trade-off between the cost of ranking items and providing users
with the best recommendations. A key observation we make in this
paper is that, all else equal, ranking more items indeed improves the
overall objective but has diminishing returns. Following this obser-
vation, we provide a rigorous formulation through the framework
of DR-submodularity, and argue that for a certain class of objectives
(reward functions), it is possible to improve the trade-off between
performance and computational cost in multi-stage ranking sys-
tems with strong theoretical guarantees. We show that this class of
reward functions that provide this guarantee is large and robust to
various noise models. Finally, we describe extensive experimenta-
tion of our method on three real-world recommender systems in
Facebook, achieving 8.8% reduction in overall compute resources
with no significant impact on recommendation quality, compared
to a 0.8% quality loss in a non-personalized budget allocation.
∗Corresponding author
This work is licensed under a Creative Commons Attribution-
NonCommercial 4.0 International License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671593CCS CONCEPTS
•Information systems →Recommender systems; Document
filtering; Personalization.
KEYWORDS
Recommender Systems; Personalization; Multi-stage ranking; Sub-
modularity
ACM Reference Format:
Ariel Evnine, Stratis Ioannidis, Dimitris Kalimeris, Shankar Kalyanaraman,
Weiwei Li, Israel Nir, Wei Sun, and Udi Weinsberg. 2024. Achieving A Better
Tradeoff in Multi-stage Recommender Systems Through Personalization. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671593
1 INTRODUCTION
Modern recommender systems serve vast amount of content to
billions of users. At this scale, these systems are necessarily so-
phisticated, driven by the need to deliver high quality personalized
recommendations that cater to the diverse interests of users, with
high efficiency. A commonly used architecture that enables this is
depicted in Figure 2, often referred to as a two-stage recommender.
After retrieving many candidate items from various sources, the
system has a light-weight early-stage ranker ( ESR), that is less com-
plex but can efficiently rank hundreds of thousands of items into
a few hundred items. These are passed to a complex (and costly)
late-stage ranker ( LSR), which produces the final recommendations
to the user. This architecture creates a trade-off between the quality
of recommendations and the cost of ranking the items, and this
trade-off is the focus on our work.
ESRmodels are usually trained through distillation or directly
on user labels, to ensure that the highest ranked items out of LSR
are also most likely to be well-scored by the ESR, increasing the
“consistency” between the stages. This is commonly measured by
recall, i.e., how many items that would be ranked highly by the
later stage are correctly identified and passed on by the early stage.
4939
KDD ’24, August 25–29, 2024, Barcelona, Spain Ariel Evnine et al.
(a) Online retention vs. ESRrecall
(b) Computation vs. ESRbatch size
Figure 1: (a) Offline recall correlates well with online reten-
tion as measured in experiments performed in recommender
system C1(see Section 6). Each point represents the change
in number of items output by ESRas a percentage of the pro-
duction batch size, with the corresponding % change in an
online metric (user retention), (b) Increasing the number of
items output by ESRhas a linear relationship to overall com-
putational cost on C1.
Previous work has focused on increasing recall by training more
sophisticated models (see, e.g., [17, 19, 27, 36, 48]).
In this paper we show an intuitive way to control this cost-
performance trade-off, independent of model complexity – by chang-
ing the number of items the ESRpasses to LSR, i.e., ESR’sbudget
orbatch size, we can improve recall, or alternatively, reduce the
overall recommender system’s computation cost.
As a motivating example, consider Figure 1a that shows the
relation between the change in recall and the change in an an online
metric of a real-world recommender system, for different budgets.
The figure shows that increasing the ESRbudget not only improves
the recall, but also empirically improves the recommendations as
measured by online metrics. A key question is exactly how many
items the ESRshould pass to the LSR. Sending too few will deprive
theLSRmodel of items the user would ultimately engage with, and
sending too many would increase its computational overhead. This
can be seen in Figure 1b, where the batch size directly impacts the
computation cost, betraying the very purpose of having a lighter
intermediate ESRmodel.1
Most recommender systems make a global, fixed choice on the
number of items the ESRshould pass to the LSR. However, we
claim that the optimal number of items should vary per request
1Increasing ESR’s budget might also result in higher system latency, which might
result in lower user engagement.
Figure 2: A two-stage recommender system
and exploit this to improve the overall trade-off between recall and
computational cost. We make the following contributions:
•We propose adopting a personalized policy , taking into ac-
count request level features, in determining the ESRbudget.
Our approach is based on off-policy evaluations of logged
data to estimate the marginal improvement in quality of
the slate of items available to LSR(taking into account user
features), measured on a per-cohort basis.
•We provide a formal model in which a wide class of qual-
ity functions, including the oft-used recall metric, exhibit a
diminishing returns property (namely, DR-submodularity).
Our analysis links this property to the extent in which the
ESRranking agrees with the LSRranking. Crucially, this
property guarantees that, despite the NP-hardness of the
constituent optimization problem, a greedy algorithm comes
with approximation guarantees.
•We experimentally validate this property holds on three
large-scale production recommender systems. We evaluate
the performance of the greedy algorithm through both offline
and online experiments in a real-world large scale deploy-
ment. Our experiments show savings of 8.8% in overall com-
pute resources consumed, without any measurable impact
on engagement metrics.
The remainder of the paper is organized as follows. We review
related work in Section 2. We provide an overview of the architec-
ture of multistage recommender systems in Section 3. We formally
state the problem we study in Section 4, and describe the theory
and resulting methodology we used to approach it in Section 5. Our
experiments are presented in Sections 6. Finally, we present our
conclusions and point to next steps in Section 7.
2 RELATED WORK
The early stage/late stage ranking model we discuss here is an in-
dustry standard, employed in recommendation systems as diverse
as video recommendation systems [ 15], content feed recommen-
dations [ 24], ad recommendation systems [ 3] and media discovery
systems [ 33]. Due to the prevalence of this recommender pipeline,
exploring its inherent accuracy/efficiency tradeoffs [ 4,14], but also
additional desiderata such as fairness [ 7,35,46], reproducibility [ 3],
calibration [ 35,47] and credit attribution [ 3], are extensively studied
topics. Liu et al . [26] provide an overview of multi-stage recom-
mender systems that reviews various neural architectures in use.
In the design of ESRmodels, a key consideration is to ensure that
the model is “consistent” with the LSRmodel. There is however no
4940Achieving A Better Tradeoff in Multi-stage Recommender Systems Through Personalization KDD ’24, August 25–29, 2024, Barcelona, Spain
clear consensus on how to define and measure this consistency. Gu
et al. [17] propose a ranking consistency score and claim that this
measure is well-aligned with online metrics, while Zhang et al . [51]
argue for a metric that estimates online recall between ESRandLSR
by using counterfactual examples through other queries. Li et al .
[25]show that simple score-based thresholding to prune items from
ESRcan certifiably achieve good consistency under some conditions.
The other challenge with regard to ESRmodels concerns the issue
of sample selection bias [ 27] referring to the fact that ESRmodels
are trained on labels that are drawn from a distribution of items
scored by the LSRmodel, but are required to make inference on a
different, larger distribution of items obtained from the retrieval
stage. This highlights the need to take into account interactions
between different stages of the ranking system [ 18] and one com-
mon approach to address this is through joint optimization of the
ESRandLSRmodels [ 19,36,43]. Our work builds on this line of
work by proposing a simple yet effective way of establishing a
tradeoff between general recall metrics and efficiency, that comes
with approximation guarantees.
There is a rich body of work on submodular function maximiza-
tion [ 23,44]. In the context of recommender systems, submodularity
has been explored when looking at subset selection [ 10,16,29] and
ranking under budget constraints [ 50], to name a few applications.
The so-called ProbeMax problem [ 12,39], which also can be used
to model utilities in recommender systems [ 30], also involves a sub-
modular objective [ 22,30]. However, submodularity in this setting
arises under very different modeling assumptions than the ones we
consider here. DR-submodularity, an extension of submodularity
to the integer lattice (see Appendix A), has recently attracted some
attention [ 5,9,11,37,41,42]. These concepts have found applica-
tion in causal structure discovery [ 2], budget allocation [ 28] and
revenue maximization with discrete assignments [ 37], to name a
few.
Our Thm. 5.1 establishes that DR-submodularity arises in the
context of a broad class of recall functions under “mild” assumptions
on the noise model differentiating ESRfrom LSR, which our experi-
ments confirm as holding in practice. To the best of our knowledge,
we are the first to make this observation; as such, our work adds
personalization of the ESRbudget to the menagerie of problems
amenable to an analysis via DR-submodularity techniques. Simi-
larly, reliance on sampling to compute submodular objectives is
very common in submodular optimization. Problems such as influ-
ence maximization [ 6,13,21] and data summarization [ 34] involve
objectives that are estimated through sampling. In fact, research
on so-called stochastic submodular optimization [ 21] has emerged
precisely with this motivation in mind. Our work adds ESR/LSR
budget optimization to this body of work as well.
3 SYSTEM OVERVIEW
Our study involves three production recommender systems, each
serving different types of items exposed to users. However, their ar-
chitecture is similar, and generally common in many large-scale (in
terms of items and users) recommender systems [ 1,31,32]. Figure 2
depicts a high-level overview of this architecture. The entire recom-
mendation stack begins with a user “request", which is implicitlygenerated by the system when a user visits a recommendation sur-
face. The request includes a wide range of user features, enabling
the recommender system to personalize the results to the user. The
end-to-end system comprises three stages: retrieval, early-stage
ranking, and late-stage ranking; we describe each below.
Retrieval. Given a user request, the first stage of a large-scale
recommender system generates an exhaustive inventory of rele-
vant candidate items to be passed to the ESRmodel. For “edge”
recommender systems that recommend possible connections, these
items are sourced based on, e.g., mutual connections, demographic
attributes, shared affiliations and interests, membership in simi-
lar online communities, etc. For content recommender systems,
items are sourced based on, e.g., similarity to content the user (or
their network) previously engaged with, content topics, popularity,
etc. After applying filters for integrity and business considerations,
these are then processed by the ESRmodel.
Early-Stage Ranker. By design, the ESRmodels are meant to be
“lightweight”, since they are tasked with sieving through the large
pool of items from the retrieval stage (these can number in the tens
of thousands). Ideally, items selected by the ESRought to be highly
ranked by the LSRmodel and are not generally required to be precise
with respect to the final ranking order of items. A natural objective
is therefore having high recall w.r.t. highly ranked items of the LSR
(see also Sec. 4.2). A common architectural design for ESRmodels
are the so-called “two-tower” networks [ 15,49], leveraging separate
embedding layers for users and items that are used to produce the
ESRranking. Crucially, “two-tower” networks do not support user-
item interaction features. ESRmodels are trained directly on user
labels (e.g., clicks, reshares, watch-time), on the LSRpredictions,
that is using the LSRpredictions as soft labels, on “p(select)” labels
that are based on how highly these items would be ranked (and
selected for recommendation) by the LSRmodel, or a combination
of all three.
Late-Stage Ranker. At this stage, after the ESRmodel has success-
fully thinned-down the candidate list of items, these are passsed
to the LSRmodel. The latter is more computationally intensive,
utilizing more features (including user-item interaction features),
deeper architectures and possibly outputting predictions of mul-
tiple events. Examples of events for edge recommender systems
are whether the person initiates a connection with the candidate,
whether the candidate will “accept” such a request, whether the
edge will improve the user’s or candidate’s long-term retention, etc.
For content recommender systems, in addition to online engage-
ment, these events can pertain to negative signals such as hiding
or asking to see less, predicted survey outcomes, and so on. Models
trained with such targets are applied on extended user, item, and
other session features, and a combination of the predicted metrics
is then used to rank items; the top items in this ranking are finally
presented to the user as recommendations.
4 PROBLEM STATEMENT
We provide a formal description of an abstract system, aiming to
capture key elements of the multi-stage recommender systems
presented in Sec. 3. Then, we define the optimization problem our
approach solves, aiming to improve the performance of the ESR
stage with a low-cost intervention in its design.
4941KDD ’24, August 25–29, 2024, Barcelona, Spain Ariel Evnine et al.
4.1 System Model
We assume that the ESR/LSRpipeline receives as input (a) a set of
candidate itemsV, with|V|=𝑁∈N, from the retrieval stage,
where each 𝑗∈V is endowed with a feature vector x𝑗∈R𝑑, for
some𝑑∈N, and (b) a context vector x𝑐∈R𝑑′, where𝑑′∈N:
this could correspond to, e.g., features of the user receiving the
recommendation or other contextual information (such as, e.g., the
time of day, current events, etc.), or some combination thereof.
The two stages operate as follows: first, a subset S𝑛⊂V , where
|S𝑛|=𝑛is selected by the early-stage ranker and passed on to the
late-stage ranker. The late-stage ranker thins this down to a smaller
setS′𝑚⊂S, with|S′𝑚|=𝑚≪𝑛, which is then returned as a final
recommendation. Both sets S𝑛,S′𝑚are constructed via ranking, i.e.
by producing scores per item and then selecting the top-scoring
items. That is, the early-stage ranker computes 𝑠𝑗=ESR(x𝑗,x𝑐), for
all𝑗∈V, and passes on the top 𝑛items asS𝑛. Similarly, the late-
stage ranker computes 𝑠′
𝑗=LSR(x𝑗,x𝑐),for𝑗∈S only, and finally
returns the top 𝑚items asS′𝑚. We refer to 𝑛,𝑚∈Nas the budgets
of the ESR/LSRstages, respectively. We stress that, as described in
Sec. 3, the early-stage ranker is faster than the late-stage ranker
but less accurate at capturing user preferences. As such, compared
toLSR(·), the map ESR(·)could be using a subset of all available
features of items (x 𝑗), and context (x 𝑐), and not access the rest.
4.2 Optimizing the ESRBudget via
Personalization
We assume that the two score generating functions (namely, ESR(·)
andLSR(·)) that define the two stages are given, and so is 𝑚∈N,
the number of final recommendations by the late-stage ranker. We
wish to optimize the early-stage selection budget 𝑛, i.e., the number
of items ESR(·)outputs. One possible objective guiding how to set
up this parameter (and, more broadly, the early-stage ranker in
general), is to have high recall. Specifically, let
S∗
𝑚=arg max
S′⊂V:|S′|=𝑚∑︁
𝑗∈S′𝑠′
𝑗,for𝑚∈N, (1)
be the optimal set of size 𝑚that the late-stage recommender could
produce, if it were to rank allitems inV. Then, we would like the
recall of the ESR, defined as
|S∗𝑚∩S𝑛|
|S∗𝑚|(2)
to be as high as possible. We build on this idea to select the ESR
budget by (a) generalizing the objective of the ESRstage to functions
more complex than recall and (b) personalizing the budget selection.
In the extreme, the budget 𝑛of the early stage ranker could be
determined on a per-context basis (e.g., per user): such a flexibility
could allow for longer budgets for certain contexts when this can
have a significant impact on recall. By accounting for the prevalence
of different contexts in a production system, one could still place
upper bounds on, e.g., the average number of items passed to the late
stage ranker, while potentially improving recall over the “one-size-
fits-all” approach of a fixed 𝑛. On the other hand, personalizing on
a per context level poses several challenges. One is w.r.t. estimating
the impact of the personalized budget on recall. Another is that
the policy mapping contexts to budgets should be simple: this isboth to maintain the “lightweight”, highly-efficient nature of the
ESRstage, but also to maintain the tractability of the problem of
identifying optimal budgets per context, which may blow up if the
number of contexts is very large.
For these reasons, we strike a balance between a fixed budget
and full-personalization by personalizing budgets on a per-segment
basis. In particular, we assume that possible contexts x𝑐∈R𝑑′are
partitioned into 𝑀distinct classes 𝐶ℓ⊂R𝑑, termed segments, for
ℓ=1,...,𝑀 . These could correspond to cohorts of users grouped
based on age, or on other contextual features such as day of the
week, number of previous sessions etc. We allow the ESRbudget to
besegment-dependent : we denote by 𝑛ℓ∈Nthe budget for segment
ℓ, and by n=[𝑛ℓ]𝑀
ℓ=1∈N𝑀the vector of ESRbudgets. We wish to
select nas the solution to an optimization problem of the following
form:
Maximize: 𝐹(n)=Í𝑀
ℓ=1𝑝ℓ¯𝑅ℓ(𝑛ℓ) (3a)
subj. to:Í𝑀
ℓ=1𝑝ℓ𝑛ℓ≤𝐾, (3b)
n≤u (3c)
n∈N𝑀(3d)
where𝑝ℓ=P(x𝑐∈𝐶ℓ)are the segment prior (i.e., prevalence)
probabilities, 𝐾is an upper-bound on the expected number of
recommendations passed to the LSRstage, u∈N𝑀a vector of
(optional)2upper bounds on the budget for each segment, and
¯𝑅ℓ(𝑛ℓ):=E[𝑅ℓ(x𝑐,𝑛ℓ) |x𝑐∈𝐶ℓ]for an appropriate reward
function𝑅ℓ(x𝑐,𝑛ℓ), capturing the benefit of passing on 𝑛ℓitems
to the LSRstage. We propose several possible reward functions in
our next section, including ones for which Prob. (3)comes with
approximation guarantees. Note that, even though Eq. (3b)is a so-
called knapsack constraint, the objective Eq. (3a)is not necessarily
a modular function, so this is not a standard knapsack problem.
We stress here that we treat segments as input to Problem (3)
(i.e., the partition to segments is presumed to be given). Moreover,
the approximation guarantees we establish in the next section hold
irrespective of the partition used: in other words, our algorithms
find a constant approximation of the optimal solution achievable
under the given partition, irrespective of what that is. That said,
partitions should in general be defined in a simple fashion, so that
determining segment membership does not place a significant com-
putational burden on the ESR/LSR pipeline at deployment time.
In Section 6 and Appendix C, we provide a principled way for
identifying partitions.
5 METHODOLOGY
5.1 Reward Functions
Ideally, the expected rewards accrued by the ESRstage per seg-
ment should capture the performance of the generated set Swhen
passed on to the LSRstage. Moreover, it should be possible to mea-
sure the corresponding expectations from offline data, on a per
segment basis. We propose several natural functions that meet
these desiderata. For notational convenience, when defining these
functions, we drop the dependence on ℓ, focusing on a specific
segment. Given a vector x𝑐sampled from this segment, we further
2Constraint (3c) is trivially always satisfied by setting 𝑢ℓ=⌈𝐾/𝑝ℓ⌉, for allℓ.
4942Achieving A Better Tradeoff in Multi-stage Recommender Systems Through Personalization KDD ’24, August 25–29, 2024, Barcelona, Spain
assume w.l.o.g. that the set Vis indexed using the late-stage ranker
applied to all items under this context: that is, V={1,2,3,...}s.t.
the scores𝑠′
𝑗=LSR(x𝑗,x𝑐)ofLSRsatisfy
𝑠′
1≥𝑠′
2≥𝑠′
3≥... (4)
Observe that, under this indexing, the optimal set defined by Eq. (1)
is given byS∗𝑚={1,2,...,𝑚}. SetS𝑛can be then seen as a random
subset ofVof size𝑛: the randomness is due to (a) x𝑐, which is
sampled from the segment, as well as (b) any inherent randomness
the early stage ranker may have if it is noisy, for instance. Under
these notational conventions, we propose the following reward
functions:
•Recall: The recall is the reward function in Eq. (2); thus,
under our notational conventions:
¯𝑅(𝑛)=E|S𝑛∩S∗𝑚|
|S∗𝑚|
=Í𝑚
𝑗=1E[1𝑗∈S𝑛]
𝑚=Í𝑚
𝑗=1P(𝑗∈S𝑛)
𝑚.
(5)
•Rank-Weighted Recall: Similar to NDCG [ 20], we can
weigh an item inS∗𝑚higher if it appears earlier in the ideal
(LSR) ranking. This is captured by, e.g.:
¯𝑅(𝑛)=E"Í𝑚
𝑗=1𝑔(𝑗)1𝑗∈𝑆𝑛Í𝑚
𝑗=1𝑔(𝑗)#
=Í𝑚
𝑗=1𝑔(𝑗)P(𝑗∈𝑆𝑛)
Í𝑚
𝑗=1𝑔(𝑗), (6)
for some non-increasing function 𝑔:N→R+, such as𝑔(𝑗)=
1/𝑗, yielding the so-called mean reciprocal rank metric [ 8],
𝑔(𝑗)=1/log(𝑗), etc. Observe that plain recall (Eq. (5)) is
a special case, with 𝑔being the non-increasing function
𝑔(𝑗)=1𝑗≤𝑚.
•Score-Weighted Recall. Finally, rather than using a func-
tion of the rank, Eq. (4)suggests that the score of LSRstage
can be used instead. This yields
¯𝑅(𝑛)=E"Í𝑚
𝑗=1𝑠′
𝑗1𝑗∈S𝑛Í𝑚
𝑗=1𝑠′
𝑗#
. (7)
We stress that, here, both the numerator andthe denominator
are random, as late-stage scores 𝑠′
𝑗depend on the (random)
context x𝑐. This makes this reward harder to compute in a
closed form.
5.2 Theoretical Guarantees
For arbitrary reward functions, Prob. (3)can, in general, be NP-
hard. In this section, we provide a set of conditions on the early-
stage ranker under which there exists a polynomial-time algorithm
solving Prob. (3)with approximation guarantees. In particular we
assume that ranker ESR(·)is randomized, and acts as a “noisy”
version of the late-stage ranker LSR(·). In particular, consider the
following possible models:
•No-Noise/Perfect Ranking. Under the no-noise/perfect
ranking setting, the early stage ranker ranks items exactly
as the late-stage ranker, i.e., it selects the top 𝑘items:
S∗
𝑛≡{1,2,...,𝑛}. (8)•Uniform Selection. In the uniform selection case, the early
ranking ignores the late-stage ranking altogether, and re-
turns a subsetS𝑛selected uniformly at random from all
subsets ofVof size𝑛.
•Monotone DR Inclusions. In this probabilistic model, the
early stage ranker includes item 𝑗∈V when having budget
𝑛with inclusion probability 𝑝𝑗,𝑛=P(𝑗∈S𝑛). We assume
that these probabilities satisfy the following two properties.
First, increasing the budget makes anyitem more likely to
be included: i.e., for all 𝑗∈V,
𝑝𝑗,𝑛′≥𝑝𝑗,𝑛,for all𝑛′≥𝑛. (9)
Second, increasing the budget improves this probability more
when the budget is small, i.e., there is a diminishing returns
property: for all 𝑗∈V
𝑝𝑗,𝑛′+1−𝑝𝑗,𝑛′≤𝑝𝑗,𝑛+1−𝑝𝑗,𝑛,for all𝑛′≥𝑛. (10)
Theorem 5.1. Suppose that (a) the early stage ranker selects set
S𝑛through either the perfect ranking, uniform selection, or monotone
DR inclusions model, and (b) the reward function is rank-weighted
recall, under an arbitrary non-increasing function 𝑔:N→R+. Then,
function𝐹:N𝑀→R+is a monotone DR-submodular function.
We give a definition of monotone DR-submodular functions
and provide a technical backround regarding their properties in
Appendix A. Intuitively, DR-submodular functions exhibit a concav-
ity/diminishing returns property over the integer lattice.3This the-
oretical guarantee implies that, if ESRbehaves as a “noisy” ranker
according to any of the above models, we can construct a poly-
time approximation algorithm solving Prob. (3). We describe this
algorithm below. It is important to note that, the theorem holds
more generally. In particular, Thm. 5.1 has the following immediate
corollary:
Corollary 5.2. Suppose that, given an x𝑐sampled from segment
𝑐, the early stage ranker uses (a) a selection algorithm among the ones
listed in Thm. 5.1 according to some probability distribution over al-
gorithms and, (b) constructs set S𝑛using this algorithm. Suppose also
that the reward function is rank weighted recall under an arbitrary
non-increasing function 𝑔:N→R+. Then, function 𝐹:N𝑀→R+
is a monotone DR-submodular function.
This follows immediately from the fact that the set of monotone
DR-submodular functions is closed under summation and multi-
plication with a positive scalar. Cor. 5.2 allows us to generalize
Thm. 5.1 to much broader segments of early stage rankers than the
ones listed above. For example, if ESRreturns a uniform selection
with probability 0.6 and a perfect ranking with probability 0.4, the
conclusions of Thm. 5.1 still hold.
We note that, despite the generality of Thm. 5.1 and Cor. 5.2,
there are circumstances that ESRmay yield subsets Ssuch that
𝐹isnotDR-submodular. An obvious example is if recommenda-
tions between ESRandLSRare anti-correlated: if the ESRmakes
recommendations in an order fully reversed compared to to the LSR
ranking, the rank-weighted recall would notbe DR-submodular.
Thm. 5.1 and Cor. 5.2 suggest that, DR-submodularity is maintained
under “sufficienty mild” forms of error in the ESRranking. Our
3As opposed to standard submodular functions, that are set functions.
4943KDD ’24, August 25–29, 2024, Barcelona, Spain Ariel Evnine et al.
Algorithm 1 GreedyESR
1:Given:
(1) Segment priors {𝑝ℓ}𝑀
ℓ=1and reward functions {¯𝑅ℓ}𝑀
ℓ=1.
(2) Expected budget 𝐾∈N, upper bounds u∈N𝑀.
2:𝑛ℓ←0, for allℓ=1,...,𝑀
3:𝑄←{ℓ:𝑛ℓ≤𝑢ℓ}
4:whileÍ
ℓ𝑝ℓ·𝑛ℓ≤𝐾and𝑄≠∅do
5: forℓ∈𝑄do
6:𝛿¯𝑅ℓ(𝑛ℓ)← ¯𝑅ℓ(𝑛ℓ+1)− ¯𝑅ℓ(𝑛ℓ)
7: end for
8: Findℓ∗=arg maxℓ∈𝑄𝛿¯𝑅ℓ(𝑛ℓ)
9: ifÍ
ℓ𝑝ℓ·𝑛ℓ+𝑝ℓ∗≤𝐾and𝑛ℓ∗+1≤𝑢ℓthen
10:𝑛ℓ∗←𝑛ℓ∗+1
11: else
12:𝑄←𝑄\{ℓ∗}
13: end if
14:end while
15:return𝑛1,...,𝑛𝑀
experiments (See Fig. 5 and Table 1) provide strong evidence that
our production early-stage rankers indeed exhibit this property,
demonstrating that the errors induced by ESRare mild-enough for
DR-submodularity to naturally emerge.
5.3 A Greedy Approximation Algorithm
When the objective 𝐹is an arbitrary monotone DR-submodular
function, Prob. (3)is NP-hard; nevertheless, greedy approxima-
tion algorithms can be used to find a solution within a 1−1/𝑒
approximation guarantee (see, e.g., [ 40,42]). Applied to our set-
ting, the algorithm by Soma et al. [ 40] amounts to Algorithm 1. We
state its theoretical guarantees, and describe how the algorithm
of Soma et al. reduces to Alg. 1, in Appendix A. In short, the al-
gorithm proceeds greedily, finding at each iteration the segment
whose budget increase yields the largest marginal gain, defined as
𝛿¯𝑅ℓ(𝑛ℓ)=¯𝑅ℓ(𝑛ℓ+1)−¯𝑅ℓ(𝑛ℓ). If this increase can happen without
violating the constraints, the budget of this segment is indeed in-
cremented by one; otherwise, the algorithm “backtracks”, removes
this segment from consideration (thereby fixing its budget from
this point on) and proceeds with the remaining segments, again
greedily. The process halts when no increase can happen without
violating a constraint.
We illustrate this through a toy example in Figure 3, contrasting
the allocation determined by Greedy ESRagainst a uniform reduc-
tion when given a 50% reduction budget. Intuitively, segment1
will receive items at the beginning of the allocation but because of
the DR-submodularity property, marginal gain in recall with every
subsequent item will start to decrease until such time as segment0
demonstrates higher value.
6 EVALUATION
We evaluate our policy on three large-scale recommender systems
at a social media platform: two edge recommender systems (denoted
E1andE2) and one content-based recommender system (denoted
asC1). Our experiments involve data collection, segmentation, and
the estimation of recall functions per segment. We evaluate the
Figure 3: Synthetic example comparing GreedyESR vs. uni-
form policies when 𝐾=50for two user segments: segment0
and segment1 with priors 0.7 and 0.3 respectively. Greedy ESR
allocates more items to segment0 compared to segment1
given that marginal gain in recall is smaller for segment1.
Figure 4: Setup for collecting the necessary data for evaluat-
ingESR’s performance. We added a “bypass flow”, which, for
a random sample of requests, sends all candidate items to
theLSR, allowing us to obtain both ESRand LSRscores, thus
obtaining an unbiased estimate of ESR’s recall.
policy offline in Section 6.1 by comparing ESR recall against the
“uniform policy”, that gives every segment the same budget. Finally,
in Section 6.2, we discuss findings from online experiments con-
ducted on these real-world recommender systems, validating that
our policy reduces the number of candidates while not hurting user
engagement, outperforming the naive uniform policy.
Data Collection. To apply our greedy policy, we need to estimate
¯𝑅ℓ(𝑛), the impact of the ESRbudget on recall metrics for different
segments over the three recommender systems (E1, E2, and C1).
To obtain an accurate estimate of these functions, we need to
know both ESRandLSRscores (or the rank order) of allcandidate
items per request. However, by design, our production systems do
not provide LSRscores for all items generated by the retrieval stage,
as most generated items never reach the LSR. To address this, we
extend each system with the ability to bypass the ESRand obtain
LSRscores for the complete set of items of our logged requests,
as depicted in Figure 4. For a random sample of the requests, we
send the items generated by the retrieval stage both through the
regular system, and in parallel, directly to the LSR. This enables
us to obtain a full log of rankings from both stages. Overall, for
each request, our log contains (a) the complete set of items from
the retrieval stage, (b) user features (e.g., tenure, platform) and non-
item features (e.g., time of day) that serve as context, (c) the full
scores (and corresponding ranks) for each item by both LSRand
4944Achieving A Better Tradeoff in Multi-stage Recommender Systems Through Personalization KDD ’24, August 25–29, 2024, Barcelona, Spain
RS𝑓1fit𝑓2fit 𝑓1params 𝑓2params
(𝑅2) (𝑅2)𝑎
𝑏 𝑐 𝑑 𝑎
𝑏 𝑐
E1 0.9993 0.9948 0.18
0.26 0.1 0.34 0.69
0.15−0.6
E2 0.9999 0.9995 0.97
3.8 2.8𝑒3−7.75 0.005
0.74−0.02
C1 0.9974 0.9922 0.33
0.42 6.19−0.62 0.43
0.21−0.52
Table 1: Fit w.r.t. 𝑓1(𝑥)=𝑎·log(𝑏𝑥+𝑐)+𝑑and𝑓2(𝑥)=𝑎·𝑥𝑏+𝑐on
overall (function 𝐹) curves for each of the three recommender
systems, as shown in Fig. 5. Goodness of fit, in 𝑅2, is very high
for parameter values that correspond to concave functions.
Per-segment fits are reported in Table 3.
ESRstages. To account for daily variations, wherever feasible, we
collected these logs on 7 days of user traffic.
Note that our sample has more items than 𝑛, the number of
items typically passed by the ESRto the LSR. This enables us to
compute the recall for a segment if we were to increase the number
of items passed by the ESRfor this segment. Thus, the policy we
will find might actually increase the number of items for some
segments, while decreasing it for others. At this stage, we also log
the corresponding parameters 𝑚per request–how many items the
LSRpresents to the user. We use this below when estimating the ESR
recall. Finally, we also determine budget 𝐾to be the typical target
of items to be ingested by LSRfor this system; this is a function of
computational cost and latency considerations.
Segment Identification. Since our ESRandLSRmodels use many
thousands of features, it can be computationally challenging to
identify an optimal partition 𝐶={𝐶1,...,𝐶𝑀}across a high-
dimensional representation of contexts. Crucially, our segmentation
needs to be simple, as mapping a request to the right context hap-
pens in real-time and at the scale of our ESR. We describe how
we choose segments in more detail in App. C. At a high level, we
“bucketize” individual features into quantiles and categories, and
use an online experiment to identify and keep only features for
which a reduction of the ESRoutput size has a statistically signif-
icant impact on some online performance metrics (as discussed
later in Section 6.2). Through this process, we identified 6,5, and 3
segments for E1,E2, and C1, respectively. We define the prevalence
𝑝ℓfor each segment to be the percentage of samples in our collected
trace that belong to this segment.
Estimating Recall Functions. We estimate three different recall
functions for each recommender system. For E1, we use a score-
weighted recall (Eq. (7)); for E2, we use a rank-weighted recall
(Eq.(6)) with𝑔(𝑗)=𝑛−𝑗, and for C1we rank-weighted recall with
𝑔(𝑗)=1/𝑗. Notably, to compute ¯𝑅ℓ(𝑘)for𝐶ℓand position 𝑘, we
average𝑅(x𝑐,𝑘)over all user contexts x𝑐∈𝐶ℓfor which we have
logged data, corresponding to position 𝑘present in our sample,
with the user contexts being identified. Note that if we have data
logged at the request-level for 𝑛items that were output by the ESR,
we are able to estimate 𝑅(x𝑐,𝑘)for all positions 𝑘≤𝑛.
6.1 Offline Experiments
We implement the GreedyESR policy in Alg. 1 and compare it to the
“uniform” policy, in which all segments receive the same budget.4To
4Our implementation is publicly available: https://github.com/facebookresearch/
recsys_multistage_personalizationSystem Engagement (%) Computational cost (%)
E1G 0.11 (±0.71) -7.8*
G -0.19 (±0.5) -8.85*
U -0.80 (±0.44) -8.85*
G -0.57 (±0.5) -12.0*
E2G -0.15 (±0.43) -0.36 (±0.28)
G -0.44 (±0.43) -5.39 (±0.28)
G -0.84 (±0.43) -6.9 (±0.28)
C1 G -0.03 (±0.05) -2.81 (±0.13)
G -0.6 (±0.11) -11.32 (±0.16)
Table 2: Online experiment results for E1,E2and C1, for
GreedyESR (G) and uniform (U) policies. Higher values of
Engagement are better and lower values of computational
cost are better; we want the most cost reduction with the
least impact on engagement. Computational cost measure-
ments for E1, annotated with an asterisk ( ∗) were performed
manually offline and did not include error-bars. Values in
bold are statistically significant, G marks GreedyESR and U
marks Uniform.
understand the combination of computational savings and impact in
performance, in offline experiments, we gradually reduce the budget
from the current value 𝐾and study the impact on the expected
reward/recall function (i.e., the objective 𝐹). This mirrors the test
we do in online experiments, described below.
In Figure 5, we plot the estimated recall/reward function ¯𝑅ℓ
for each segment ℓas a function of the fractional ESRbudget𝑛
(expressed in % of maximum value), across each of the three recom-
mender systems. The approximate concavity/diminishing returns
property of all curves is evident and we further verify this numeri-
cally by fitting a logarithmic and a power-law function to each of
these curves. The goodness of fit, measured in 𝑅2values, is reported
in Tables 1 and 3, while Figure 6 illustrates a fit of a logarithmic
and a power-law fit. In all cases, we observe an excellent fit, with
all approximating function coefficients yielding concave functions.
In light of our analysis in Sec. 5, these experimental results suggest
that any errors in ESRin its agreement with LSRrankings are mild
enough and the DR-submodularity property is still maintained.
Next, we study the impact of the reduction in the ESRbudget,
ranging from 10% to 50% over the current 𝐾, on the solution pro-
duced by GreedyESR , as well as the respective reward/overall recall.
In Figure 7a, we see the allocation of the budget across different
segments as we change this budget in E1. As expected, we observe
that for segments for which recall curves have plateaued more,
i.e., have higher consistency between ESRandLSR(e.g., segment0 ),
GreedyESR is more aggressive in reducing the number of candidates,
whereas it is more conservative for those segments (e.g., segment2 )
that are less consistent. We also compared GreedyESR against a
benchmark policy that uniformly reduces the number of items
across all user segments, i.e., if we aim to achieve an 𝛼%reduction
in the number of items passed then all the user requests will incur
the same𝛼%reduction. The results of this comparison can be found
in Figure 8a, which describes the overall recall of the system as a
function of the session-size reduction (x-axis) for E1.GreedyESR
4945KDD ’24, August 25–29, 2024, Barcelona, Spain Ariel Evnine et al.
(a) Score-weighted recall in E1
 (b) Rank-weighted recall ( 𝑔(𝑗)=𝑛−𝑗) inE2
 (c) Rank-weighted recall ( 𝑔(𝑗)=1/𝑗) inC1
Figure 5: Estimated recall functions ¯𝑅ℓacross segments for each recommender system, as a function of 𝑛, measured in percentage
over current production system values 𝐾.Overall indicates average under segment priors. We observe that estimated functions
clearly exhibit a concavity/diminishing returns property. We also confirm this numerically in Tables 1, 3 and in Figure 6.
0 20 40 60 80 100
% items passed to LSR0.00.20.40.6
E1 recall
alog(bx+c)+d
(a)𝑓1(𝑥)=𝑎·log(𝑏𝑥+𝑐)+𝑑
0 20 40 60 80 100
% of items passed to LSR0.2
0.00.20.40.6
E1 recall
axb+c
 (b)𝑓2(𝑥)=𝑎·𝑥𝑏+𝑐
Figure 6: Fitting different DR-submodular functions on recall
curve for E1.
dominates uniform in terms of recall. The difference is less no-
ticeable for E2andC1, reported in Figure. 8b and 8c, respectively,
although GreedyESR still dominates. Nevertheless, GreedyESR does
yield a differentiated allocation of budgets across segments in these
recommender systems too (discussed in Appendix E); this has an
impact on online metrics, as we discuss next.
6.2 Online Experiments
We ran online experiments on the three deployed systems, using
live production traffic. In the experiments, we reduced the total
number of items 𝐾passed from ESRtoLSRas a way to reduce
overall compute cost, and observed the impact on engagement
metrics. We also explored different segmentations, and in real-time,
we affix𝑛ℓto a requesting user, as determined by GreedyESR.
Note that we cannot directly control the compute cost, but in-
stead we measure it after reducing the number of items. Engage-
ment metrics are measured differently for the two types of rec-
ommender systems. For edge recommender systems E1and E2,
we measure engagement using a metric that estimates long-termecosystem value via the retention of both the viewer and the sug-
gested connection. For C1, we measure engagement directly using
a metric for daily user retention.
In all our experiments, control and treatment groups were uni-
formly split and received at least 0.5%of overall traffic (millions
of users), where the control group corresponded to baseline pro-
duction with no reduction in the number of items from ESRtoLSR
and the treatment being policies determined through GreedyESR
or Uniform. Table 2 summarizes the results of a few experiments,
showing the change in percentage of compute and engagement
between the treatment group and the control group.
ForE1, we show a total of four experiments, three with GreedyESR
and one with Uniform policy, which gives the same number of items
in each cohort. When the compute cost is reduced by up to 8.85%,
theGreedyESR policy does not exhibit a significant engagement
impact. However, when the Uniform policy is used with the same
compute reduction of 8.85%, there is a statistically significant impact
of -0.8% on engagement. When we further reduce the compute cost
by -12%, the engagement using GreedyESR was negatively impacted
by -0.57%, which is still lower than the negative impact caused by
the Uniform policy. Similar results can be seen for both E2andC1;
a reduction of compute cost that doesn’t hurt engagement up to a
point beyond which further reduction hurts engagement.
Overall, our experiments validate that 1) we can find an operating
point that saves compute cost while not impacting engagement,
and 2) GreedyESR enables a significantly better trade-off between
compute and engagement than the Uniform policy.
7 CONCLUSION AND DISCUSSION
We presented the novel problem of optimizing a ranking system
performance given a computation budget by considering the num-
ber of items that are passed between the different stages of the
system. Focusing on early stage ranker recall, we used theoreti-
cal derivation to demonstrate that this problem is amenable to a
greedy optimization approach, under reasonable assumptions on
4946Achieving A Better Tradeoff in Multi-stage Recommender Systems Through Personalization KDD ’24, August 25–29, 2024, Barcelona, Spain
(a)E1
 (b)E2
 (c)C1
Figure 7: Budget allocations under GreedyESR as a function of the reduction of average budget 𝐾in % over production system
value in E1,E2, and C1. Allocations are highly differentiated across segments. We observe that for C1, even when given a 0%
budget reduction target, Greedy ESRallocates fewer than the current number of items for segment0 while redistributing the
surplus to segment1 and segment2, reflecting a better tradeoff on overall recall while keeping cost fixed.
50 60 70 80 90 100
%items output to LSR82838485868788RecallGreedy
Uniform
(a)E1: 50% budget reduction
86 88 90 92 94 96 98 100
%items output to LSR64666870RecallGreedy
Uniform (b)E2: 14% budget reduction
50 60 70 80 90 100
%items output to LSR70.072.575.077.580.082.585.087.5RecallGreedy
Uniform (c)C1: 50% budget reduction
Figure 8: Comparing recall for Greedy ESRvs. uniform. The x-axis denotes the percentage reduction of the average session size
while the y-axis is the overall recall of the ESR. In all cases Greedy ESRis dominating uniform but the effect is more pronounced
in the case of E1 due to the use of score-weighted recall as described in Section 6.1. For C1, uniform runs closely to Greedy ESR
because of recall curves being similar across segments as seen in Figure 5c.
the similarity between the ranking of the different stages. We then
employed this algorithm in three different production systems, re-
sulting in sizable reduction in costs without incurring negative
effects on online metrics such as user engagement or retention.
Two key assumptions we make in the work are that ESR’s recall
is a good proxy for the overall performance of the recommender
system and that LSRperforms well across segments. While these
assumptions hold in the recommender systems we studied, an ex-
tension of this work is to study cases where these assumptions do
not hold. For example, when the LSRis not performing well for
some segments, it would make sense to include this as a constraint
in the optimization problem, thus sending fewer items to the LSRfor
these segments. Similarly, we can identify and optimize the cases
where the recall is not a good proxy of end-to-end performance
using online experiments and heterogeneous treatment models [ 45].
Our procedure of segmenting requests (described in Appendix C)
works well in practice, however, we plan to further improve it infuture work, by considering both offline and online signals to find
an optimal segmentation.
From a theoretical standpoint, the structural property we iden-
tified (DR-submodularity) emerges for a broad array of noisy be-
haviors by the early stage ranker, but is not universal: as we dis-
cussed, it would collapse in a regime where the ESRranking is a
full reversal of the one produced the LSR. Nevertheless, the fact DR-
submodularity persists under u.a.r. selection, and that it is validated
by our experiments, suggests that this property is quite robust.
Characterizing the noise models which are necessary and sufficient
for its emergence is an interesting open question.
ACKNOWLEDGMENTS
We are grateful to the anonymous reviewers for their helpful com-
ments. We also would like to thank Nitzan Razin for extremely
helpful discussions, and Jun Xiao for support in facilitating our
real-world experiments.
4947KDD ’24, August 25–29, 2024, Barcelona, Spain Ariel Evnine et al.
REFERENCES
[1]Parag Agrawal. 2024. Building a Large-Scale Recommendation System: People
You May Know . https://www.linkedin.com/blog/engineering/recommendations/
building-a-large-scale-recommendation-system-people-you-may-know
[2]Raj Agrawal, Chandler Squires, Karren Yang, Karthikeyan Shanmugam, and
Caroline Uhler. 2019. Abcd-strategy: Budgeted experimental design for targeted
causal structure discovery. In The 22nd International Conference on Artificial
Intelligence and Statistics. PMLR, 3400–3409.
[3]Rohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin,
Todd Phillips, Cristina Pop, Kevin Regan, Gil I Shamir, et al .2022. On the factory
floor: ML engineering for industrial-scale ads recommendation models. arXiv
preprint arXiv:2209.05310 (2022).
[4]Nima Asadi and Jimmy Lin. 2013. Effectiveness/Efficiency Tradeoffs for Candidate
Generation in Multi-Stage Retrieval Architectures. In Proceedings of the 36th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (Dublin, Ireland) (SIGIR ’13). Association for Computing Machinery,
New York, NY, USA, 997–1000. https://doi.org/10.1145/2484028.2484132
[5]An Bian, Kfir Levy, Andreas Krause, and Joachim M Buhmann. 2017. Continuous
DR-submodular maximization: Structure and algorithms. Advances in Neural
Information Processing Systems 30 (2017).
[6]Christian Borgs, Michael Brautbar, Jennifer Chayes, and Brendan Lucier. 2014.
Maximizing social influence in nearly optimal time. In Proceedings of the twenty-
fifth annual ACM-SIAM symposium on Discrete algorithms. SIAM, 946–957.
[7]Amanda Bower, Kristian Lum, Tomo Lazovich, Kyra Yee, and Luca Belli. 2022.
Random Isn’t Always Fair: Candidate Set Imbalance and Exposure Inequality in
Recommender Systems. arXiv preprint arXiv:2209.05000 (2022).
[8]Andrei Z Broder, David Carmel, Michael Herscovici, Aya Soffer, and Jason Zien.
2003. Efficient query evaluation using a two-level retrieval process. In Proceedings
of the twelfth international conference on Information and knowledge management .
426–434.
[9]Niv Buchbinder and Moran Feldman. 2023. Constrained Submodular Max-
imization via New Bounds for DR-Submodular Functions. arXiv preprint
arXiv:2311.01129 (2023).
[10] Emanuele Bugliarello. 2022. Mostra: A Flexible Balancing Framework to Trade-off
User, Artist and Platform Objectives for Music Sequencing. In Proceedings of the
ACM Web Conference 2022. 1484–1493.
[11] Lin Chen, Hamed Hassani, and Amin Karbasi. 2018. Online continuous sub-
modular maximization. In International Conference on Artificial Intelligence and
Statistics. PMLR, 1896–1905.
[12] Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. 2016. Combinatorial multi-
armed bandit with general reward functions. Advances in Neural Information
Processing Systems 29 (2016).
[13] Wei Chen, Yajun Wang, and Siyu Yang. 2009. Efficient influence maximization in
social networks. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. 199–208.
[14] Charles LA Clarke, J Shane Culpepper, and Alistair Moffat. 2016. Assessing
efficiency–effectiveness tradeoffs in multi-stage retrieval systems without using
relevance judgments. Information Retrieval Journal 19 (2016), 351–377.
[15] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM conference on
recommender systems. 191–198.
[16] Khalid El-Arini, Gaurav Veda, Dafna Shahaf, and Carlos Guestrin. 2009. Turning
down the noise in the blogosphere. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, Paris, France,
June 28 - July 1, 2009, John F. Elder IV, Françoise Fogelman-Soulié, Peter A. Flach,
and Mohammed Javeed Zaki (Eds.). ACM, 289–298. https://doi.org/10.1145/
1557019.1557056
[17] Siyu Gu, Xiang-Rong Sheng, Biye Jiang, Siyuan Lou, Shuguang Han, Hongbo
Deng, and Bo Zheng. 2022. On Ranking Consistency of Pre-ranking Stage Con-
sistency of Pre-ranking Stage. arXiv preprint arXiv:2205.01289 (2022).
[18] Jiri Hron, Karl Krauth, Michael Jordan, and Niki Kilbertus. 2021. On component
interactions in two-stage recommender systems. Advances in neural information
processing systems 34 (2021), 2744–2757.
[19] Xu Huang, Defu Lian, Jin Chen, Liu Zheng, Xing Xie, and Enhong Chen. 2023.
Cooperative Retriever and Ranker in Deep Recommenders. In Proceedings of the
ACM Web Conference 2023. 1150–1161.
[20] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation
of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002),
422–446.
[21] Mohammad Karimi, Mario Lucic, Hamed Hassani, and Andreas Krause. 2017.
Stochastic submodular maximization: The case of coverage functions. Advances
in Neural Information Processing Systems 30 (2017).
[22] Jon Kleinberg and Maithra Raghu. 2018. Team performance with test scores.
ACM Transactions on Economics and Computation (TEAC) 6, 3-4 (2018), 1–26.
[23] Andreas Krause and Daniel Golovin. 2014. Submodular function maximization.
Tractability 3, 71-104 (2014), 3.[24] Lada, Akos and Wang, Meihong and Yan, Tak. 2021. How does News Feed predict
what you want to see? https://tech.facebook.com/engineering/2021/01/news-
feed-ranking/
[25] Minghan Li, Xinyu Zhang, Ji Xin, Hongyang Zhang, and Jimmy Lin. 2022. Certi-
fied Error Control of Candidate Set Pruning for Two-Stage Relevance Ranking.
InProceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Associa-
tion for Computational Linguistics, Abu Dhabi, United Arab Emirates, 333–345.
https://doi.org/10.18653/v1/2022.emnlp-main.23
[26] Weiwen Liu, Yunjia Xi, Jiarui Qin, Fei Sun, Bo Chen, Weinan Zhang, Rui Zhang,
and Ruiming Tang. 2022. Neural Re-ranking in Multi-stage Recommender Sys-
tems: A Review. In Proceedings of the Thirty-First International Joint Conference
on Artificial Intelligence (IJCAI-22).
[27] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Hong,
and Ed H Chi. 2020. Off-policy learning in two-stage recommender systems. In
Proceedings of The Web Conference 2020. 463–473.
[28] Takanori Maehara, Akihiro Yabe, and Ken-ichi Kawarabayashi. 2015. Budget allo-
cation problem with multiple advertisers: A game theoretic view. In International
Conference on Machine Learning. PMLR, 428–437.
[29] Anay Mehrotra and Nisheeth K Vishnoi. 2023. Maximizing Submodular Functions
for Recommendation in the Presence of Biases. In Proceedings of the ACM Web
Conference 2023. 3625–3636.
[30] Aranyak Mehta, Uri Nadav, Alexandros Psomas, and Aviad Rubinstein. 2020.
Hitting the high notes: Subset selection for maximizing expected order statistics.
Advances in Neural Information Processing Systems 33 (2020), 15800–15810.
[31] Meta Inc. 2023. Facebook People You May Know AI system. https://transparency.
fb.com/features/explaining-ranking/fb-people-you-may-know/
[32] Meta Inc. 2023. Instagram Suggested Accounts AI system. https://transparency.fb.
com/features/explaining-ranking/ig-suggested-accounts/
[33] Meta Inc. 2023. Scaling the Instagram Explore recommendations sys-
tem. https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-
explore-recommendations-system/
[34] Marko Mitrovic, Ehsan Kazemi, Morteza Zadimoghaddam, and Amin Karbasi.
2018. Data summarization at scale: A two-stage submodular approach. In Inter-
national Conference on Machine Learning. PMLR, 3596–3605.
[35] Nastaran Okati, Stratis Tsirtsis, and Manuel Gomez Rodriguez. 2023. On the
within-group fairness of screening classifiers. In International Conference on
Machine Learning. PMLR, 26495–26516.
[36] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui
Zhang, Yong Yu, and Weinan Zhang. 2022. RankFlow: Joint Optimization of Multi-
Stage Cascade Ranking Systems as Flows. In Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
814–824.
[37] Aytunc Sahin, Joachim Buhmann, and Andreas Krause. 2020. Constrained maxi-
mization of lattice submodular functions. In IN: ICML 2020 workshop on Negative
Dependence and Submodularity for ML, Vienna, Austria, PMLR, Vol. 119.
[38] Alberto Schiabel, Vyacheslav Kungurtsev, and Jakub Marecek. 2021. Randomized
Algorithms for Monotone Submodular Function Maximization on the Integer
Lattice. arXiv preprint arXiv:2111.10175 (2021).
[39] Danny Segev and Sahil Singla. 2021. Efficient approximation schemes for sto-
chastic probing and prophet problems. In Proceedings of the 22nd ACM Conference
on Economics and Computation. 793–794.
[40] Tasuku Soma, Naonori Kakimura, Kazuhiro Inaba, and Ken-ichi Kawarabayashi.
2014. Optimal budget allocation: Theoretical guarantee and efficient algorithm.
InInternational Conference on Machine Learning. PMLR, 351–359.
[41] Tasuku Soma and Yuichi Yoshida. 2017. Non-monotone dr-submodular function
maximization. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 31.
[42] Tasuku Soma and Yuichi Yoshida. 2018. Maximizing monotone submodular
functions over the integer lattice. Mathematical Programming 172 (2018), 539–
563.
[43] Jinbo Song, Ruoran Huang, Xinyang Wang, Wei Huang, Qian Yu, Mingming
Chen, Yafei Yao, Chaosheng Fan, Changping Peng, Zhangang Lin, Jinghe Hu,
and Jingping Shao. 2022. Rethinking Large-Scale Pre-Ranking System: Entire-
Chain Cross-Domain Models. In Proceedings of the 31st ACM International Con-
ference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM
’22). Association for Computing Machinery, New York, NY, USA, 4495–4499.
https://doi.org/10.1145/3511808.3557683
[44] Jan Vondrák. 2007. Submodularity in combinatorial optimization. Charles Uni-
versity, Prague (2007).
[45] Stefan Wager and Susan Athey. 2018. Estimation and Inference of Hetero-
geneous Treatment Effects using Random Forests. J. Amer. Statist. Assoc.
113, 523 (2018), 1228–1242. https://doi.org/10.1080/01621459.2017.1319839
arXiv:https://doi.org/10.1080/01621459.2017.1319839
[46] Lequn Wang and Thorsten Joachims. 2023. Uncertainty Quantification for Fair-
ness in Two-Stage Recommender Systems. In Proceedings of the Sixteenth ACM
International Conference on Web Search and Data Mining. 940–948.
4948Achieving A Better Tradeoff in Multi-stage Recommender Systems Through Personalization KDD ’24, August 25–29, 2024, Barcelona, Spain
[47] Lequn Wang, Thorsten Joachims, and Manuel Gomez Rodriguez. 2022. Improving
screening processes via calibrated subset selection. In International Conference
on Machine Learning. PMLR, 22702–22726.
[48] Xuewei Wang, Qiang Jin, Shengyu Huang, Min Zhang, Xi Liu, Zhengli Zhao,
Yukun Chen, Zhengyu Zhang, Jiyan Yang, Ellie Wen, Sagar Chordia, Wenlin Chen,
and Qin Huang. 2023. Towards the Better Ranking Consistency: A Multi-task
Learning Framework for Early Stage Ads Ranking. arXiv:2307.11096 [cs.IR]
[49] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee
Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected neural
modeling for large corpus item recommendations. In Proceedings of the 13th
ACM Conference on Recommender Systems (Copenhagen, Denmark) (RecSys ’19).
Association for Computing Machinery, New York, NY, USA, 269–277. https:
//doi.org/10.1145/3298689.3346996
[50] Guangyi Zhang, Nikolaj Tatti, and Aristides Gionis. 2022. Ranking with submod-
ular functions on a budget. Data mining and knowledge discovery 36, 3 (2022),
1197–1218.
[51] Zhixuan Zhang, Yuheng Huang, Dan Ou, Sen Li, Longbin Li, Qingwen Liu, and
Xiaoyi Zeng. 2023. Rethinking the Role of Pre-ranking in Large-scale E-Commerce
Searching System. arXiv preprint arXiv:2305.13647 (2023).
A MAXIMIZING MONOTONE
DR-SUBMODULAR FUNCTIONS OVER THE
INTEGER LATTICE
Definitions. A function𝑓:N𝑀→Ris called monotone if𝑓(k)≤
𝑓(k′)for all k,k′∈N𝑀s.t.k≤k′.5Function𝑓:N𝑀→Ris called
“diminishing-returns” (DR) submodular over the integer lattice if
for all k,k′∈N𝑀s.t.k≤k′and allℓ∈{1,...,𝑀}:
𝑓(k+eℓ)−𝑓(k)≥𝑓(k′+eℓ)−𝑓(k′), (11)
where eℓis theℓ-th standard-basis vector.
Algorithms. There is a rich literature on maximizing DR-submodular
functions subject to several constraints over the integer-lattice, in-
cluding cardinality [ 42], polymatroid [ 38,42], and knapsack con-
straints [ 40,42]. We focus here on algorithms for the latter, as they
can be used to solve Prob. (3).
Formally, given a monotone DR-submodular function 𝑓:N𝑀→
R, consider the following optimization problem.
Maximize: 𝑓(n) (12a)
subj. to: w⊤n≤𝐾, (12b)
n≤u, (12c)
n∈N𝑀(12d)
where w∈(0,1]𝑀+is a positive weight vector, and u∈N𝑀is an
(optional) vector of coordinate-wise upper-bounds.
Soma et al. [ 40] provide a polynomial-time algorithm for solving
problem (12)within a 1−1/𝑒approximation guarantee (see Alg. 1
in [40]). The algorithm by Soma et al. is in fact somewhat more
complex but, applied to our setting, it reduces to Alg. 1, and comes
with the following approximation guarantee:
Theorem A.1 (Thm 2.1 in [ 40]).Alg. 1 finds a solution to Prob-
lem(12)1−1/𝑒and has time complexity 𝑂(𝐾𝑀4𝑇𝑓), where𝑇𝑓is the
time required to make an oracle-call (i.e., evaluate) function 𝑓.
The key difference between the algorithm by Soma et al. and its
application to our setting is the marginal gain they consider, which
takes the form
Δ(ℓ,𝑘,n)≡𝑓(n+𝑘·eℓ)−𝑓(n)
𝑤ℓ𝑘.
5Forx,x′∈R𝑀two𝑀-dimensional vectors, we use x≤x′to indicate coordinate-
wise domination (i.e., 𝑥ℓ≤𝑥′
ℓfor allℓ∈{1,...,𝑀}).where eℓis theℓ-th standard basis vector (i.e., a vector with 1at
theℓ-th coordinate and zero everywhere else. This differs from
our greedy step in that (a) the increment need not be by 1, but an
arbitrary feasible 𝑘≥1and, (b) the marginal gain is weighed by
𝑤ℓ𝑘. However, in our case
𝑓(n+𝑘·eℓ)−𝑓(n)
𝑤ℓ𝑘=𝑝ℓ ¯𝑅ℓ(𝑛ℓ+𝑘))− ¯𝑅ℓ(𝑛ℓ)
𝑝ℓ𝑘
=¯𝑅ℓ(𝑛ℓ+𝑘))− ¯𝑅ℓ(𝑛ℓ)
𝑘
and, by the DR-submodularity of ¯𝑅ℓ, this ratio is maximized for
𝑘=1.
In the context of our setting, the complexity of Alg. 1 depends
on𝑀(the number of segments) as well as the budget 𝐾, when
having access to a function oracle, e.g., an oracle that returns the
objective function in polynomial time; this statement of complexity
is common in submodular optimization. Parameter 𝑁, the number
of items inV, plays a role in estimating the objective function:
the number of samples required to estimate the function within a
certain tolerance depends on 𝑁. As all quantities estimated here are
bounded, Chernoff bounds can be applied on the reward function
to bound the number of samples required to reach the desired
tolerance.
B PROOF OF THEOREM 5.1
As the positively-weighted sum of monotone DR-submodular func-
tions is also monotone DR-submodular, it suffices to show that
¯𝑅ℓ(𝑛)is monotone DR-submodular for every ℓ. Moreover, since
these functions only depend on the ℓ-th argument, it is sufficient
to prove the two properties in the one-dimensional setting. For
simplicity, we omit the dependence on ℓin the notation below, as
the proofs below hold for an arbitrary class.
Perfect Ranking. Assuming perfect ranking, we have that, condi-
tioned on x𝑐,
𝑅(𝑘)=ℎ(𝑚)·(Í𝑛
𝑗=1𝑔(𝑗)if𝑛≤𝑚
1, o.w.
whereℎ(𝑚)=1/Í𝑚
𝑗=1𝑔(𝑗)is the normalizing constant, that does
not depend on 𝑛. Since𝑔(𝑗) ≥ 0for all𝑗∈ V , this is clearly
monotone. Moreover, as 𝑔is non-decreasing, for all 𝑛<𝑚and all
𝑛′≥𝑛we have that
𝑅(𝑛′+1)−𝑅(𝑛′)≤ℎ(𝑚)𝑔(𝑛′+1)
≤ℎ(𝑚)𝑔(𝑛+1)=𝑅(𝑛+1)−𝑅(𝑛),
while the above inequality holds as an equality if 𝑛≥𝑚, as both
differences are 0. The statement therefore follows for ¯𝑅by taking
the expectation over x𝑐in the class.
Monotone DR Inclusions. The monotone DR-inclusions case
follows immediately from the fact that (a) by definition, for every
𝑗∈V,𝑝𝑗,𝑛is a monotone DR-submodular function of 𝑛, and (b)
¯𝑅(𝑛)=ℎ(𝑚)𝑚∑︁
𝑗=1𝑔(𝑗)𝑝𝑗,𝑛
and𝑔(𝑗)≥0for all𝑗∈V. Hence, ¯𝑅is monotone DR-submodular
as a positively-weighted combination of monotone DR-submodular
functions.
4949KDD ’24, August 25–29, 2024, Barcelona, Spain Ariel Evnine et al.
(a)E2
 (b)C1
Figure 9: Offline allocation using GreedyESR policies for fixed budget reduction target.
Uniform. The uniform case follows from the fact that, for all 𝑗∈
V𝑝𝑗,𝑛are monotone DR-submodular in 𝑛when the selection is
uniform. Indeed, for 𝑁≡|V| ,
𝑝𝑗,𝑛=P(𝑗∈S𝑛)= 𝑁−1
𝑛−1
 𝑁
𝑛=𝑛
𝑁,
which is both monotone and DR-submodular in 𝑛.
C IDENTIFICATION OF SEGMENTS
Recall that our segmentation should be simple, i.e., interpretable
and efficient policy that determines membership in each class. We
need this because mapping a context to its class should incur low
latency when done in real-time, consistent with the “light-weight”
nature of the ESRmodel.
We adopted the following approach to cluster representations.
First, we identify a subset of significant features by running a
“boundary A/B test” that partitions users into a control and treat-
ment group. For users in the treatment group, we reduce the size
of the output set of ESRfor all requests in the amount needed to
achieve our budget constraint, while we keep the corresponding
value for users in the control group the same. We assume that
overall, the users in the treated group will experience a reduction
in a performance metric of interest (explained in more detail in
Section 6.2) due to the worse recommendations. Otherwise, we are
not at an optimal point in terms of batch size, and we can globaly
reduce the amount of items passed from ESRtoLSR, before trying
to reduce it further using our personalization method.
Next, we segment each available feature (continuous features
using quantiles, categorical features by categories) and estimate the
online treatment effect in the online metric of choice, across these
segments defined. We then identify the features F1for which at
least some segments of users don’t experience a regression in that
metric (again, we assume that at least some segment will see a re-
gression). This indicates there’s room for batch size personalization,
by creating a policy using our offline algorithm.
Next, we filter this initial set of features based on product or
business constraints, e.g., we want to be absolutely sure our policy
does not hurt the ranking of specific user cohorts. This yields a
set of featuresF′
1. We then create cross-features F′
2⊆F′
1×F′
1
and use a unionF′
1∪F′
2. This last step allows us to consider both
segments defined by a single feature or an interaction between twoRS Segment 𝑓1fit𝑓2fit 𝑓1params 𝑓2params
(𝑅2) (𝑅2)𝑎
𝑏 𝑐 𝑑 𝑎
𝑏 𝑐
E1 segment0 0.9929 0.9870 0.18
0.26 0.1 0.34 2.89
0.05−2.74
segment1 0.9999 0.9966 0.20
0.33 0.84 0.05 0.43
0.21−0.35
segment2 0.9929 0.9980 0.20
4.56 20.56−0.59 0.25
0.27−0.19
segment3 0.9929 0.9894 0.18
0.20 0.1 0.36 2.04
0.07−1.9
segment4 0.9929 0.9963 0.18
0.15 0.29 0.29 0.51
0.18−0.42
segment5 0.9929 0.9975 0.19
11.11 40.99−0.69 0.29
0.25−0.23
E2 segment0 0.9999 0.9978 0.9
0.06 33.01−3.17 0.009
0.68−0.05
segment1 1.0 0.9995 1.01
0.03 24.04−3.22 0.005
0.74−0.03
segment2 0.9998 0.9999 1.08
0.02 20.93−3.28 0.004
0.77−0.01
segment3 0.9997 1.0 1.03
7.7 7.16𝑒3−9.12 0.004
0.76−0.01
segment4 0.9999 0.9999 1.02
0.02 21.29−3.11 0.004
0.77−0.02
C1 segment0 0.9987 0.9942 0.25
22.07 178.51−1.34 0.436
0.21−0.52
segment1 0.9967 0.9911 0.35
1.19 19.14−1.05 0.261
0.29−0.35
segment2 0.9988 0.9947 0.32
22.75 336.89−1.85 0.256
0.29−0.33
Table 3: Fit w.r.t. 𝑓1(𝑥)=𝑎·log(𝑏𝑥+𝑐)+𝑑and𝑓2(𝑥)=𝑎·𝑥𝑏+𝑐
on recall curves for each segment of the three recommender
systems.
features. Naturally, we can extend our method to interactions of
higher degrees. However, as we strive for simplicity and efficiency,
we limit our study to interactions of no more than two features.
We then use the segmentations defined by F′
1∪F′
2to generate
policies using our algorithm for offline and online evaluations and
picking the top handful of policies, least expected to reduce overall
recall, and run an online A/B test to validate their performance.
D PER-SEGMENT RECALL CURVE FITS
In Table 3 we show closeness-of-fit for recall curves against 𝑓1
and𝑓2corresponding to each of the recommender systems at a
per-segment level.
E HETEROGENEITY IN ALLOCATION
Figure 9 characterizes the heterogeneity in allocation of items based
onGreedyESR policies for E2andC1. Note that this corresponds to
the recall curves plotted in Figure 5. Notably, for E2, we see a higher
allocation of items to segments 0, 1, 2 (Figure 9a) in accordance
with the relatively steeper loss in recall when reducing allocation
(Figure 5b).
4950