Dynamic Pricing for Multi-Retailer Delivery Platforms with
Additive Deep Learning and Evolutionary Optimization
Ahmed Abdulaal∗
ahmed.abdulaal@walmart.com
Walmart Global Tech
Sunnyvale, California, USAAli Polat†
Hari Narayan†
alipolat@meta.com
hari.narayan@shipt.com
Shipt
San Francisco, California, USAWenrong Zeng‡
Yimin Yi‡
rzeng@coursera.org
yimin.yi@shipt.com
Shipt
San Francisco, California, USA
Abstract
Dynamic Pricing for online retail has been discussed extensively
in literature. However, past solutions fell short of addressing the
unique challenges of independent multi-retailer platforms for gro-
cery delivery. From limited visibility of retailers’ inventories to
diverse demand-side dynamics across retail brands and locations,
the highly decentralized nature of multi-retailer platforms devi-
ates from the classical framework of modeling price elasticity and
cross-elasticity of demand. In this paper, we present a novel scheme
to scalable and practical price adjustment in the highly dynamic
multi-retailer context. First, we present a deep learning framework
to distinctly model complex cross-elasticity relationships via ad-
ditive neural networks augmented with adversarial data. Second,
we present evolutionary optimization agents for adjusting itemized
prices in a location-decentralized manner, while adhering to cus-
tom business constraints and objectives. The optimization utilizes
the genetic algorithm structure, where we introduce a potential
mechanism, inspired by bandit algorithms, in order to improve con-
vergence speed by managing exploitation and exploration trade-offs.
Our solution is deployed at Shipt and is extendable to other types
of multi-retailer platforms, such as restaurant delivery. Finally, we
empirically demonstrate performance using public and industry
datasets of hundreds and thousands of diverse products across tens
of stores, offering an optimization targets coverage scale in the tens
of thousands, far larger than experimental setups in past research.
CCS Concepts
•Applied computing →Online shopping; Multi-criterion op-
timization and decision-making; •Computing methodologies
→Search methodologies; Neural networks.
∗Author was affiliated with Shipt while conducting the work in this paper.
†Authors contributed equally to this research.
‡Authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671634Keywords
dynamic pricing, deep learning, neural networks, evolutionary op-
timization, high-dimensional time series
ACM Reference Format:
Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, and Yimin Yi.
2024. Dynamic Pricing for Multi-Retailer Delivery Platforms with Additive
Deep Learning and Evolutionary Optimization. In Proceedings of the 30th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671634
1 Introduction
Shipt is an online multi-retailer marketplace that provides on-
demand, shop-to-door, and same-day delivery services for groceries
and household essentials. It is one of several businesses that have
emerged in recent years to offer convenience and competitive pric-
ing across retailer brands and products from retail categories in-
cluding restaurant, office-supplies, and pet-supplies. To compete in
this shared economy market, highly dynamic and adaptive product
pricing decisions become critical.
Although the dynamic pricing problem has enjoyed popularity
in research in the context of e-commerce applications, the pro-
posed solutions have focused on integrated retail platforms. These
platforms enjoy higher levels of centralization and comprehen-
sive information availability throughout the supply chain. They
typically operate fulfilment centers and depend on aggregating
products’ supply and demand data at a higher level. This allows
the provider to utilize inventory information, experimentation, and
aggregations to optimize the products’ pricing at the regional scale.
In contrast, the aforementioned multi-retailer platforms operate
in micro-market environments, where many variables such as the
retailer variety, product variety, sourcing costs, driver availability,
and demand patterns vary by the customer’s relatively narrow lo-
cation. Even two customers from the same city can face different
supply and demand dynamics, if they are located in different neigh-
borhoods, or if they place their orders on different hours or days.
Furthermore, these platforms operate, to some extent, in isolation
from products’ suppliers or retailers. Consequently, they cannot
utilize inventory constraints due to incomplete inventory observ-
ability. They cannot centralize product costs or aggregate consumer
demand patterns to inform pricing decisions as effectively as in the
e-commerce applications of prior research work.
In addition, the impact of products’ inter-dependence on price
and demand is higher for grocery than other types of retail because
4741
KDD ’24, August 25–29, 2024, Barcelona, Spain Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, Yimin Yi
grocery cart sizes tend to number in the tens of products in com-
parison to most other e-commerce carts which consist of a single
product or very few products.
Therefore, a suitable pricing solution must address the above
challenges of multi-retailer grocery marketplaces in the shared
economy space as follows: It must include a broad, multi-dimension-
al model of cross-elasticity of demand. It must support frequent
and granular pricing decisions. Finally, it must be flexible enough
to support multiple retailers and locations, since it is very ineffi-
cient, both from computational and maintainability perspectives,
to replicate machine learning pipelines for each retailer or location.
Accordingly, we focus on solving the dynamic pricing problem
holistically for a group of homogeneous retailers and products us-
ing a single deployment. Furthermore, we forgo explicit modeling
of a priori assumptions regarding product relationships, and we
target solutions flexible to dynamic business constraints and objec-
tives. For this purpose, we introduce a flexible and scalable two-step
dynamic pricing approach, which consists of a demand estimation
framework and a constrained price optimization meta-heuristic.
In the first step, we present a Deep Neural Network (DNN) train-
ing framework for learning complex cross-elasticities in extremely
high-dimensional spaces. The framework utilizes an additive model
of feature encoders and an interrupted-training strategy, where
encoders are sequentially pre-trained to model the incremental
impact of independent sets of factors, such as temporal, macroeco-
nomic, retailer location-based, and price-based factors. The retailer
location-based embeddings serve to extract location-specific elastic-
ities, while the pre-training of the prices encoder is enriched with
adversarial data to introduce rationality into the complex elasticities
learned. In the second step, the demand model acts as a simulation
tool, while we utilize decentralized agents, based on Genetic Algo-
rithms (GAs), to optimize products’ pricing policy across the full
product set simultaneously. This enables addressing the elasticities,
business objectives, and constraints separately by retailer location.
We improve convergence by reducing the search space to discrete
price adjustments, known as markdowns or markups, in addition to
introducing a potential-based selection concept, which is similar to
bandit-algorithms and improves the trade off between exploration
and exploitation throughout the generations. The entire system can
be considered an example of offline reinforcement learning, where
the demand simulation model acts as the environment providing
feedback, while each GA agent utilizes the feedback to adjust its
potential for exploring and exploiting different price settings. Our
solution is highly scalable and has been widely implemented in
production at Shipt, beating baseline economic models of demand
elasticity and traditional optimization methods. To the best of the
authors’ knowledge, the contributions of this work are as follows:
•We distinctly utilize deep learning methods to achieve effect
modeling and control via an additive ensembling method
and unique training strategy. The solution scales efficiently
to millions of products across hundreds of store locations for
multiple retailers with a single DNN implementation, which
makes it particularly practical for industry-implementation.
•We distinctly utilize evolutionary optimization theory and
introduce techniques to improve convergence, in order toachieve desirable pricing policies with high flexibility to
custom constraints and business objectives.
•We provide comprehensive empirical evidence supporting
our method’s superior performance from online experiments,
offline evaluation, and ablation studies, utilizing internal and
external datasets of high-dimensional settings. Furthermore,
we produced an anonymized real-industry dataset, which
can be utilized in various research efforts including dynamic
pricing and multivariate time-series forecasting. The tests
covered hundreds and thousands of products across tens of
retailer locations, making the scale of our experimental setup
significantly larger than other analyses in relevant literature.
The remainder of this paper is structured as follows: Section 2
summarizes findings from dynamic pricing literature and relevant
research. Section 3 describes the components of the implemented
methodology in higher depth and with mathematical notations. Sec-
tion 4 details the setup for empirical benchmark studies of the pro-
posed methods, including production-run experiments. Section 5
summarizes the experimental results and discusses considerations
related to production implementation, usability, and future steps.
Section 6 draws the conclusion from this work.
2 Background
Dynamic pricing is an optimization problem under uncertainty,
with its core aim being the maximization of a sales-related objec-
tive given an unknown set of demand functions. It can be dissected
into two sub-problems: estimating demand and choosing prices
to maximize the objective. Demand estimation literature focuses
primarily on classical and machine learning based estimation mod-
els. The former estimates a joint distribution of demand and prices
using parametric or semi-parametric statistical models, whereas
the latter treats demand as a pure prediction objective, without
creating a data generating process for all relevant variables. Price
optimization literature explores traditional Operations Research
(OR) techniques and multi-armed bandit models, which involve
simultaneous exploration and exploitation of different policies. For
a more exhaustive review of dynamic pricing literature, including
a broader range of studies and methodologies, refer to [11].
Parametric Demand Estimation Models: Parametric demand
estimation entails an explicit specification of the demand function.
This specification can take various forms including linear, expo-
nential and power functions. [ 17,20]. More sophisticated paramet-
ric approaches employ multinomial logit and nested logit models
[16,18]. Bayesian approaches involve modeling beliefs about de-
mand parameters, which evolve given a specific prior distribution
[9,14,25]. These models offer straightforward causal interpretation
and allow researchers to analytically assess model implications,
providing insights into the behavior of different pricing strategies
without the need for extensive numerical experiments. However
these approaches rely on restrictive assumptions about demand
functions and fail to generalize or scale to multi-product setups [ 7].
Deep Learning Approaches to Demand Estimation: With
the reformulation of demand estimation as a high-dimensional time
series forecasting problem, Deep Neural Networks (DNNs) [ 28],
and its variants such as Recurrent Neural Networks (RNNs) [ 29],
Long Short-Term Memory Networks (LSTMs) [ 30] and transformer
4742Dynamic Pricing for Multi-Retailer Platforms KDD ’24, August 25–29, 2024, Barcelona, Spain
based approaches [ 12,21] emerge as powerful tools. These methods
excel in capturing complex, non-linear relationships inherent in
high-dimensional data, making them highly suitable for forecasting
demand in dynamic environments. More recently, machine learning
approaches gained popularity in dynamic pricing setups such as
last mile logistics [ 36]. This paper’s approach extends on the deep
learning literature by employing an additive deep neural network
with distinct strategy for inserting adversarial training data. The
additive nature of our neural network enhances explainability and
facilitates additional network regularization [ 2]. Addressing the
challenge of learning cross-elasticities, particularly in data with
limited price variation for inelastic products, our model incorpo-
rates adversarial data. This addition significantly boosts the model’s
generalizability to new domains [35].
Price Optimization: For the optimization component, most
approaches in recent literature were limited to addressing dynamic
pricing as a price discounting problem, which adopts constraints
based on shelf life, inventory, supplier, or competitor features [ 13,
16,18,22]. Such solutions are inapplicable for the shared econ-
omy multi-retailer platforms, where the service provider has no
access to the supplying retailers’ inventory levels, shelf life, or other
nonpublic cost-related information.
The authors in [ 16,37] utilized Makrov decision processes to
optimize a sequential markdown policy. Other methods, such as
[13,22,37], solve the pricing problem independently for each prod-
uct at the itemized or clustered level, through linear or quadratic
programming methods, with the exception of [ 22], where the au-
thors relied on heuristics to define a feasible region from the demand
estimation model outputs. The work in [ 18] formulated the prob-
lem as an instance of binary-quadratic programming (BQP), then
introduced relaxations methods to address the NP-hardness of BQP.
Such approaches are limited to achieving local optimization and do
not consider the complicated spatial relationships among multiple
products across various categories. These formulations are limited
to first-order interactions of prices at most.
Contextual Multi-Armed Bandits: Contextual multi-armed
bandits emerge as a natural solution to the dynamic pricing prob-
lem as their semi-parametric approach to modeling the objective
function offers great flexibility [ 8,24,26,27,31]. Notably, [ 26] tack-
les dynamic pricing as an online optimization problem, focusing
on cross-elasticities in high-dimensional product spaces. However,
the model’s reliance on restrictive assumptions for cross-elasticity
calculations may not always capture the complex realities in certain
domains. The implementation of these online bandit algorithms
in scenarios with high-dimensional products and sparse demand,
common in shared economy retail, poses challenges due to the need
for extensive feedback and feature sets. Additionally, the intrinsic
nature of multi-armed bandits to balance learning and interven-
tion can lead to sub-optimal pricing decisions, especially when
unobserved confounders are present. [6].
3 Methods
3.1 Cross-Elastic Demand Estimation
Framework
3.1.1 Neural Networks Model. Traditional economic models of de-
mand oversimplify cross-elasticities to interpretable pairwise linearor log-linear relationships. In contrast, we rely on DNNs to cap-
ture complex patterns and increase demand predictability. DNNs
approximate nonlinear and spatial relationships across the feature
vector, without explicit assumptions of the correlations among the
independent and dependent variables. Accordingly, we formulate
the demand model as follows:
𝑄′=Θ′(𝑊′ˆ𝑍+𝐵′) (1)
ˆ𝑍=Ø
𝑋∈ˆ𝑅𝑍𝑋 (2)
𝑍𝑋=Θ𝑋(𝑊𝑋𝑋+𝐵𝑋) (3)
where𝑄′is an𝑁×𝑀vector of the predicted multivariate demand
of𝑀products across 𝑁supply sources (i.e. retailers and locations),
Θ′is a decoder DNN given by the parameters 𝑊′and𝐵′,ˆ𝑍is the
aggregation of latent spaces, 𝑍𝑋, from multiple Θ𝑋encoders of
parameters 𝑊and𝐵, where each encoder represents a regressor
𝑋∈ˆ𝑅. The objective of this formulation is to learn latent represen-
tations independent to each regressor. For example, 𝑍𝑃captures the
cross-elasticity of demand by learning spatial correlations among
various products in the high-dimensional space. The choice of the
type of DNN, Θ𝑋, is dependent on the type of the regressor 𝑋. The
aggregation,Ð, can either be averaging, summation, or concatena-
tion, of latent spaces.
3.1.2 Additive Training for Approximating Causal Cross-Elasticity
Relationships. In the traditional linear model, the ability to con-
trol for extraneous variables complements the intepretability of
the elasticity coefficients. In contrast, DNNs trade interpretability
in exchange for accuracy, hence they are commonly referred to
as black-box models. Often, their superior short-term accuracy is
an attribute of erroneously fabricated relationships, rather than
learning the true variable effects and cross-elasticities. In order to
prevent fitting spurious cross-elasticities at the cost of minimizing
the loss function , we employ an additive training strategy compa-
rable with transfer learning approaches. The aim of this strategy
is to iteratively fine-tune on top of pre-trained models by incre-
mentally introducing each regressor, 𝑋∈ˆ𝑅and its encoder, Θ𝑋.
We rely on intuition and domain knowledge for selecting the or-
der for introducing regressors. For the dynamic pricing objective,
we introduce the pricing, 𝑃, regressor as the last addition before a
final fine-tuning step. We hypothesize that this additive training
strategy prevents the network from making erroneous nonlinear
assumptions and insures that 𝑍𝑃learns realistic cross-elasticities in
the high-dimensional space. In Subsection 5.1 we provide evidence
supporting our hypothesis. Typically, the training objective is to
minimize a loss function, such as the squared errors, given by:
𝔏(𝑄,𝑄′)=𝑄−𝑄′2=𝑄−Θ′(𝑊′ˆ𝑍+𝐵′)2(4)
For example, if we assume 4 regressor types, ˆ𝑅=[𝑅1,𝑅2,𝑆,𝑃],
where𝑅1and𝑅2are exogenous variables for the temporal and
economic features respectively, then the additive training steps,
which are schematically demonstrated in Fig. 1, are summarized as:
Step 1 : Minimize𝑄−Θ′(𝑊′𝑍𝑅1+𝐵′)2, freeze𝑊𝑅1and𝐵𝑅1.
Step 2 : Minimize𝑄−Θ′(𝑊′𝑍𝑅2+𝐵′)2, freeze𝑊𝑅2and𝐵𝑅2.
Step 3 : Minimize∥𝑄−Θ′(𝑊′𝑍𝑆+𝐵′)∥2, freeze𝑊𝑆and𝐵𝑆.
Step 4 : Minimize∥𝑄−Θ′(𝑊′𝑍𝑃+𝐵′)∥2.
4743KDD ’24, August 25–29, 2024, Barcelona, Spain Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, Yimin Yi
Step 5 : Fine-tune decoder Θ′
Figure 1: Additive DNNs training framework with 4 sets of
regressors.
3.2 Insuring Rationality with Adversarial Data
There are some cases where the historical sales data alone are in-
sufficient for learning rational demand curves. This is particularly
a risk for products characterized by highly sparse sales, insufficient
historical price fluctuations, or price-inelastic products, where his-
torical sales spikes corresponded to price increases. In such cases, a
model may learn reversed elasticity curves beyond rational reason-
ing. To address this challenge, we introduce adversarial training
data, which our proposed additive training framework is uniquely
suited to support. The advantage of the proposed architecture for ad-
versarial training is that we can isolate the introduction of artificial
data to the pre-training stage of the pricing encoder, which is the
fourth step in the provided example. This prevents the artificially ir-
rational data from contaminating the learning of realistic temporal,
location, or other factors’ contributions to demand, whilst insuring
the modeled impact of prices adheres to rational bounds. To create
adversarial training data in the contexts of disintegrated retail and
high-dimensional cross-elasticities, we introduce the following hy-
pothesis: If a product is irrationally marked up, its demand falls to
zero and also customers seeking that product would abandon the re-
tailers’ cart in favor of competing retailers, platforms, or a different
method of shopping. Therefore, the demand for all other products
that would have been in the same cart are impacted. Accordingly,
we utilize products’ cart-dependency graphs from historical sales
data, then randomly select products, supply source, and dates from
a copy of the training data, where for each selection we apply a
random irrational markup and adjust the selection’s demand to 0as
well as reduce the demand of the cart-dependent products from the
connected nodes with a percentage derived from the edges’ value.3.3 Constrained Optimization with
Evolutionary Algorithms
3.3.1 Optimization Problem Formulation. Given the trained global
DNN model presented by Equation (1), we utilize decentralized
agents for identifying optimal sets of pricing inputs, where each
agent corresponds to a supply source. This allows flexibility for
adopting location-specific or retailer-specific optimization objec-
tives and constraints. We program the agents using an implementa-
tion of Evolutionary Algorithms (EAs) since we deviated from using
differentiable functions of price-elasticities. EAs are population-
based metaheuristics for generic objectives optimization [ 34]. They
are independent of derivative evaluations, meaning they can ad-
dress non-differentiable objectives and solve both convex and non-
convex optimization problems as in our case. For example, assuming
that the business objective for the 𝑠th supply source, 𝑗𝑠, is revenue
maximization, we formulate the agent’s function as follows:
𝑗𝑠=arg max
˘𝑝𝑠,𝑖𝑀∑︁
𝑖=1𝑞′
𝑠,𝑖×𝑝𝑠,𝑖∀𝑠∈{1,···,𝑁} (5)
s.t:
𝑝𝑠,𝑖=𝑝𝐶
𝑠,𝑖×˘𝑝𝑠,𝑖∀𝑠∈{1,···,𝑁},𝑖∈{1,···,𝑀} (6)
˘𝑝𝐿𝐵
𝑠,𝑖≤˘𝑝𝑠,𝑖≤˘𝑝𝑈𝐵
𝑠,𝑖∀𝑠∈{1,···,𝑁},𝑖∈{1,···,𝑀}(7)
where𝑞′
𝑠,𝑖∈𝑄′and𝑝𝑠,𝑖∈𝑃denote the itemized predicted demands
and prices, ˘𝑝𝑠,𝑖∈˘𝑃are the decision variables and represent the
markups or markdowns applied to the products’ base costs, 𝑝𝐶
𝑠,𝑖∈
𝑃𝐶, and ˘𝑝𝐿𝐵
𝑠,𝑖∈˘𝑃𝐿𝐵,˘𝑝𝑈𝐵
𝑠,𝑖∈˘𝑃𝑈𝐵are the lower and upper bounds for
a product’s markup/markdown as recommended by the business.
In practice, the objective in Equation (5)can be modified from
maximizing revenue to maximizing quantity of sales, profit, or a
combination of objectives.
3.3.2 Discretization and Constraints for Search Space Reduction.
The main limitation of EAs is their computational time costs, par-
ticularly with infinite or extremely large solution spaces. The con-
vergence to a desirable sub-optimal may require large number of
iterations, albeit the algorithmic design of detecting and explor-
ing shortcuts in the solution space. However, we can practically
utilize EAs in dynamic pricing by incorporating business-driven
constraints and discretizing the search space, hence significantly
reducing it. Accordingly, we discretize the optimization problem,
(5), by replacing the decision variable, ˘𝑝𝑠,𝑖, with𝑣𝑠,𝑖, such that:
˘𝑝𝑠,𝑖=1+(𝜂×𝑣𝑠,𝑖) ∀𝑠∈{1,···,𝑁},𝑖∈{1,···,𝑀}(8)
𝑣𝑠,𝑖∈Z∀𝑠∈{1,···,𝑁},𝑖∈{1,···,𝑀} (9)
˘𝑝𝐿𝐵
𝑠,𝑖−1
𝜂≤𝑣𝑠,𝑖≤˘𝑝𝑈𝐵
𝑠,𝑖−1
𝜂∀𝑠∈{1,···,𝑁},𝑖∈{1,···,𝑀}
(10)
where𝜂is a constant defining the discretization step size and Zis
the symbol for the set of integer numbers. This also enables the
business to generate practical markups or markdowns by limit-
ing adjustments to interpretable increments. For instance, setting
𝜂=0.01or𝜂=0.05will explore adjustements in the 1%or5%
increments only. 𝜂adjusts the degree of search space discretization,
thus acting as a trade off between convergence speed and proximity
to global optima.
4744Dynamic Pricing for Multi-Retailer Platforms KDD ’24, August 25–29, 2024, Barcelona, Spain
3.3.3 Markup/Markdown Selection with GA. Each agent’s function
follows the GA framework, which includes the following four main
operations:
•Permutation: Generates an individual, 𝑙𝑠=[˘𝑝𝑠,0,···,˘𝑝𝑠,𝑀].
A group of individuals makes the population, 𝐿𝑠,𝑘, of the𝑘th
generation. ˘𝑝𝑠,𝑖values of individual 𝑙𝑠are randomly sampled
fromZaccording to (9) and (10), then substituted in (8).
•Fitness: Evaluation of population’s individuals, using Equa-
tion (5), to identify the 𝑙𝑒𝑙𝑖𝑡𝑒𝑠 individual with the highest
𝑗𝑠.
•Selection: Fitness-based sampling for surviving individuals,
according to the stochastic universal sampling method [5].
•Crossover: Creates offsprings by merging two sampled in-
dividuals. The ratio of individuals who undergo crossover is
known as the crossover ratio. We apply uniform crossover as
depicted in Fig. 2. After crossover, we check for any violation
of (7) and apply the nearest bound.
•Mutation: Randomly alters traits a proportion of the off-
spring, defined by the mutation ratio. The amount of a mutat-
ing offspring’s altered traits is determined by the mutation
rate. We adopt a weighted-random mutation where altered
traits are replaced by those of a new permutation as depicted
in Fig. 3.
We define the maximum number of generations as 𝐾and a patience
parameter𝑜=0, which limits the amount of consecutive genera-
tions without fitness improvement to 𝑜=𝑂, after which the process
terminates. we illustrate the steps of our GA implementation in
Algorithm 1.
Figure 2: Crossover operation for offsprings creation.
Figure 3: Mutation operation of an offspring.
3.3.4 Potential Vector for Better Convergence. To further increase
the convergence efficiency of our price adjustment metaheuristic,
we introduce a parameter, 𝜌𝑠,𝑖, for each source 𝑠and product 𝑖,
which defines the potential of ˘𝑝𝑠,𝑖to be replaced during mutation.
In contrast, ˘𝑝𝑠,𝑖in a mutation offspring would have had an equal
chance of replacement across all values of 𝑖in the vanilla GA. In
concept, utilizing potentials becomes similar to applying banditAlgorithm 1 GA framework for price adjustment
Input: ˘𝑃𝐿𝐵,˘𝑃𝑈𝐵,𝜂,𝐾,𝑂, crossover ratio, mutation ratio, mutation
rate, and[𝜌𝑠,𝑖]
Output: ˘𝑃=𝑐𝑜𝑛𝑐𝑎𝑡([𝑙𝑒𝑙𝑖𝑡𝑒
1,···,𝑙𝑒𝑙𝑖𝑡𝑒
𝑁])
1:for𝑠=1,···,𝑁do
2: initialize𝐿𝑠,𝑘=1 ⊲Permutations
3: for𝑙𝑠in𝐿𝑠,𝑘=1do
4: score𝑙𝑠 ⊲Fitness
5: end for
6: record𝑙𝑒𝑙𝑖𝑡𝑒|𝑘
𝑠 having the highest 𝑗𝑠|𝑙𝑒𝑙𝑖𝑡𝑒|𝑘
𝑠
7: while𝑜<𝑂or𝑘<𝐾do ⊲Termination criteria
8: select survivors ⊲Selection
9: breed offsprings via combining a proportion of sur-
vivors and cloning the remainder ⊲Crossover
10: Mutate a proportion of the offsprings ⊲Mutation
11: generate𝐿𝑠,𝑘=𝑘+1of𝑙𝑒𝑙𝑖𝑡𝑒|𝑘
𝑠 and offsprings
12: repeat lines 3 to 6
13: if𝑗𝑠|𝑙𝑒𝑙𝑖𝑡𝑒|𝑘=𝑗𝑠|𝑙𝑒𝑙𝑖𝑡𝑒|𝑘+1
𝑠then
14: update[𝜌𝑠,𝑖]: exploration ⊲Equation (12)
15: 𝑜=𝑜+1
16: else
17: update[𝜌𝑠,𝑖]: exploitation ⊲Equation (12)
18: end if
19:𝑘=𝑘+1
20: end while
21:end for
strategies, where the probability of mutating the 𝑖th price in the 𝑘th
generation is influenced by the rewards observed when mutating
it in prior generations, {𝑘−1,···,0}. Manipulating the potentials
thus enables a user-desired trade-off between exploration and ex-
ploitation. Another advantage of 𝜌𝑠,𝑖is that we can preset it to be
zero for certain products where price adjustments are not permitted,
but including them in the demand estimation model and maximiza-
tion objective is necessary, due to the cross-elasticity impact of
other products’ prices. The conditions for potential parameters are
formulated as follows:
𝑀∑︁
𝑖=1𝜌𝑠,𝑖=1∀𝑠∈{1,···,𝑁} (11)
𝜌𝑠,𝑖|𝑘=𝜌𝑠,𝑖|𝑘−1×(1+𝑘
𝐾)(
if˘𝑝𝑠,𝑖|𝑘≠˘𝑝𝑠,𝑖|𝑘−1(exploitation)
if˘𝑝𝑠,𝑖|𝑘=˘𝑝𝑠,𝑖|𝑘−1(exploration)
∀𝑠∈{1,···,𝑁},𝑖∈{1,···,𝑚},𝑘>1
(12)
where Equation (12)defines the options for adjusting 𝜌𝑠,𝑖through-
out the generations. After each generation, the values for 𝜌𝑠,𝑖are
scaled to satisfy the condition in (11). For example, a greedy ap-
proach would be utilizing the exploitation strategy in generations
with higher rewards, 𝑗𝑠|𝑘>𝑗𝑠|𝑘−1. The monotonically increasing
𝑘
𝐾parameter in (12)insures that the products, whose prices impose
higher elasticity or cross-elasticity impacts on revenue, will have in-
creased chances of undergoing mutations in newer generations. In
our implementation we utilize exploitation strategy in generations
where the fitness improves, and exploration strategy otherwise.
4745KDD ’24, August 25–29, 2024, Barcelona, Spain Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, Yimin Yi
3.4 Complete Solution Framework
Finally, we combine the parts in this section to present the complete
2-step dynamic pricing process as illustrated in Fig. 3. The solution
mimics an offline reinforcement learning process: The trained de-
mand estimation model from (1)simulates the environment, which
generates the reward as the improvement in the objective (5), given
an action,𝑙𝑒𝑙𝑖𝑡𝑒𝑠 from the GA agent’s Algorithm 1. The dynamic
environmental states impacting the optimization decisions in every
generation are the iteration and potential states. The figure shows
only a single agent for simplicity, while the actual implementation
distributes agents across the different supply sources.
Figure 4: Dynamic pricing as an offline reinforcement learn-
ing process
4 Experimental Setups
4.1 Data
To assess our solution’s performance, with respect to accuracy
and scalability, we utilize eight datasets with product cardinalities
ranging from 100 to 3,000 products. Six of the datasets consist of
historical sale data from 10 different retailers, stores, or geographical
locations. Thus, the total number of forecast target variables ranges
from 1,000 to 30,000. Five of the datasets are internal to Shipt,
which are composed of actual itemized historical sales from a large
retailer across its various store locations in a select metropolitan
area. The sixth dataset, named the Gig Economy Retail Delivery
(GERD) dataset, was mined from internal Shipt data, anonymized,
and published as part this work, as a contribution to future research
in dynamic pricing or time series applications. The last two datasets
are extracted from the public Online Retail II (ORII) dataset in [ 10].
We summarize the datasets in Table 1 and describe them below.
Shipt’s datasets are weekly or daily in frequency. They represent
online sales from a retail chain. The corresponding feature sets,
ˆ𝑅, include price features, temporal and holiday features, store fea-
tures, and external factors to proxy macroeconomic market changes.
We refer to further details on public datasets in section A of the
appendix.
4.2 Offline Demand Model Evaluation Setup
We benchmark our demand estimation framework against popu-
lar regression models, such as Lasso regression [ 33] and GradientTable 1: Summary of Experimental Validation Datasets
Source𝑀 𝑁 ˆ𝑅size Sparsity Time span
Shipt 100 10 189 2.10% 156 weeks
Shipt 1000 10 1089 8.80% 156 weeks
Shipt 3000 10 3089 23.02% 156 weeks
Shipt 100 10 195 9.35% 1092 days
Shipt 1000 10 1095 46.45% 1092 days
GERD 100 10 163 11.87% 156 weeks
ORII 100 1 261 55.62% 158 weeks
ORII 100 1 261 55.62% 1092 days
Boosting of decision Trees (GBTs) [ 15], as well as state-of-the-art
(SOTA) deep learning architectures, such as Long Short Term Mem-
ory Auto-Regressive networks (LSTM-AR) [ 30], RNNs combined
with a Gaussian copula process output model with a low-rank co-
variance structure (LSTM-VAR) [ 29], Long- and Short-term Time
series Network (LSTNet) which utilizes convolutional networks
and RNNs to learn the local dependency of input variables and
capture long-term dependencies [ 19], and the Temporal Fusion
Transformer (TFT) which is an attention-based DNN with LSTM
structures [ 21]. We used the python implementations for those mod-
els and refer to the relevant details in section A of the appendix.
Finally, we perform an ablation study to demonstrate the contri-
bution of the additive training strategy to distinctly capture the
impact of price factors on demand, specifically in the context of
high cross-elasticity dimensional space. we inspect and contrast
the demand curves in the following 3 training scenarios for the
same DNNs: classical training, proposed additive training, and the
proposed additive training augmented with adversarial data in the
pricing encoder pre-training step. To insure parity, we omit adver-
sarial data from the benchmark setup, since our framework has
an unfair advantage for supporting adversarial data in an isolated
component without impacting the weights learned for other factors.
For models’ configurations and parameters, we apply the re-
spective authors’ recommended settings in addition to performing
hyper-parameter tuning using the framework in [ 3]. We ran all
experiments on a cloud instance with 945 GB of memory and Intel
Xeon of 40 logical processors. Parallelization was utilized where
applicable. We refer to the details of the holdout, parameter, and
configurations applied in section A of the appendix.
For the evaluation metric, we use the Weighted Mean-Absolute-
Percentage-Error (WMAPE), which is preferred for evaluating fore-
casts when the observed sales are intermittent or contain values
close to zero. It is computed as:
𝑊𝑀𝐴𝑃𝐸 =Í𝑁
𝑠=1Í𝑀
𝑖=1|𝑞′𝑠,𝑖−𝑞𝑠,𝑖|
Í𝑁
𝑠=1Í𝑀
𝑖=1𝑞𝑠,𝑖(13)
4.3 Offline Optimization Evaluation Setup
To fairly select optimization benchmarks, we are limited to solutions
that are applicable to our counterfactual demand model. Therefore,
we cannot apply some of the commonly used methods, such as
linear or quadratic-programming methods, given our DNN-based
objective function. In lieu, we evaluate our GA implementation,
4746Dynamic Pricing for Multi-Retailer Platforms KDD ’24, August 25–29, 2024, Barcelona, Spain
which includes our proposed potential methodology for weighted
random mutation, against vanilla GA andsimple EA implemen-
tations [ 4] as baselines, which are available via the PyGAD and
DEAP python libraries.
4.4 Online Evaluation Setup
The presented methods have been implemented in a real world
setting to dynamically price the products’ catalog for hundreds of
grocery retailer store locations across the nation. We continuously
run online experiments by evaluating the performance against the
prevailing pricing strategy set by business managers. Below, we
briefly present an overview of the setup for one of such experiments.
Due to ethical and legal considerations around fair pricing, we
eschewed a user-level A/B testing procedure in favor of quasi-
experimental studies. We stratified the selected stores, based on
order frequency and size, into groups and randomly assign stores
within each group as control or treatment. We refer to the sets
of control and treatment stores as 𝑆𝑐and𝑆𝑡respectively. For the
𝑆𝑡stores, we utilized our method to dynamically price several
thousands of products on a weekly cadence.
We generated our counterfactual values based on the synthetic
control methodology in [ 1], with the exception of utilizing Weighted
Least Squares [ 32] with Elastic Net regularization [ 38] in lieu of
sequential least squares programming, due to observing superior
out-of-sample fitting qualities. The model was fit to a pre-treatment
target variable with 100 time periods, formulated as:
Υ=𝑁𝑡∑︁
𝑠𝑡=1𝑀∑︁
𝑖=1𝜑(𝑞𝑠𝑡,𝑖,𝑝𝑠𝑡,𝑖) (14)
where𝜑is a business metric such as revenue or basket size for
product𝑖at store𝑠𝑡∈𝑆𝑡. Therefore, with 𝑁𝑐contributing donors
for the synthetic control, we can fit the function Υ𝑡=𝑓(Υ𝑐)using
the pre-treatment data and use the fitted function to estimate the
post-treatment Υ𝑡counterfactual values.
5 Results and Learnings
5.1 Demand Model Evaluation Results
5.1.1 Performance Benchmark Results. Table 2 summarizes the re-
sults of the experiments described in Subsection 4.2. Our DNN,
which is trained according to the presented additive deep learn-
ing framework, almost consistently outperformed both classical
approaches and SOTA deep learning architectures. Furthermore,
we conclude superior scalability as evident by the training times.
Our approach trains significantly faster than benchmark deep learn-
ing approaches and is second only to the linear regression model.
However, for the extremely large dataset with 10,000 and 30,000
target variables, it significantly outperforms the linear model. RNN
architectures, particularly LSTM-VAR and TFT, are incapable of
scaling efficiently with higher dimensionalities, at least not with-
out considerable investments in computing resources. "N/A" in
Table 2 indicates that the training process was interrupted due to
running longer than 8 hours without being close to completion,
which contradicts with our scope for efficient, scalable, and fre-
quently revised solutions. We omit the reporting of inference time,which was concluded as insignificant with variances of few to sev-
eral seconds and our method being amongst the faster inferences.
Only training time sensitivity is critical since pricing updates in
grocery retail can practically occur at weekly, daily, or sub-daily
frequencies at most, while real-time updates would be impractical
and would negatively impact customer experience. The benchmark
results constitute evidence of our demand estimation approach’s
superiority as an industry-practical solution in highly dynamic and
large-scale settings. It must also be noted that we do not claim a
superior type of DNN layer. We contribute our method’s perfor-
mance to the additive combination of simple-DNNs and the training
framework, which boosts the simple-DNNs’ performance against
SOTA DNNs, without the additional computational costs. The same
strategy can be applied to any of the referenced types of DNNs.
5.1.2 Contribution of Additive Learning. We utilized the model ex-
plainability tool in [ 23] to extract learned dependencies from the
ablation study. We concluded evidence of the proposed additive
training strategy for capturing realistic demand elasticity relation-
ships in the high-dimensional space, whereas the classically trained
DNNs learned unexplainable relationships, albeit the comparable
fitting loss values. Supporting evidence in the form of dependency
plots are provided in Subsection B.1 of the appendix, which cover a
range of products’ elasticies and various cross-elasticity dependen-
cies including complimenting, competing, and substitute products.
5.1.3 Contribution of Adversarial Data. Insufficient price fluctua-
tions in historical data may limit the models’ ability to learn realistic
demand elasticity functions. We studied the effect of adversarial
data augmentation in pre-training the pricing encoder and con-
cluded that without augmenting adversarial data, the DNN tends
to learn more flattened demand curves, where it assumes that de-
mand elasticity is more bounded to the pricing ranges observed
in historical data set, while it stabilizes beyond these ranges. We
provide an example with Fig. 12 in Subsection B.1 of the appendix.
The example compares the demand of 3 complimenting products,
which tend to co-exist in a shopping cart, in response to simulated
changes in the price of one product.
5.2 Optimization Performance Analysis
5.2.1 Performance Benchmark Results. We compared the conver-
gence performance of our GA implementation against open-sourced
EA benchmarks using the GERD dataset. For comparability, we plot
convergence in terms of percentage improvement over the initial
population’s elite. For the simple EA benchmark, we experimented
with both discrete and continuous decision variables. The compari-
son concluded our custom implementation of GA superior conver-
gence performance, which proves our method’s increased utility in
terms of latency, where we can confidently reduce the maximum
number of generations, while focusing on the industry-applicability
for frequent pricing revisions across multiple-retailers. The com-
parison plots are provided in Subsection B.2 of the appendix.
5.2.2 Contribution of Discretization and Potential-based Selection.
In the ablation study, we we disabled updating the potentials in
one iteration and relaxed the discretization parameter in another.
we observed significantly worse convergence as a result, thus con-
cludes the practical importance of our modifications for achieving
4747KDD ’24, August 25–29, 2024, Barcelona, Spain Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, Yimin Yi
Table 2: Cross-Elastic Demand Models’ Accuracy and Training Time Against Weekly (W) and Daily (D) Real-World Datasets
WMAPE
D-Shipt D-Shipt W-Shipt W-Shipt W-Shipt W-GERD D-ORII W-ORII
𝑁𝑀=1000𝑁𝑀=10000𝑁𝑀=1000𝑁𝑀=10000𝑁𝑀=30000𝑁𝑀=1000𝑁𝑀=100𝑁𝑀=100
Lasso 0.73 1.04 0.36 0.50 0.61 1.07 1.24 1.03
GBTs 0.62 0.94 0.25 0.42 0.55 0.64 0.78 1.02
LSTNet 0.63 1.05 0.33 0.56 0.76 0.61 0.88 0.68
LSTM-AR 0.72 0.92 0.42 0.52 0.58 0.60 0.79 0.81
LSTM-VAR N/A N/A 0.54 N/A N/A 0.79 N/A 0.82
TFT N/A N/A 0.25 N/A N/A 0.69 1.15 1.81
This Paper 0.44 0.72 0.18 0.29 0.38 0.67 0.98 0.62
Training Time (in minutes)
Lasso 0.44 16.66 0.17 7.86 40.37 0.08 0.13 0.01
GBTs 1.19 56.60 0.87 40.03 252.61 0.70 2.63 0.04
LSTNet 1.95 167.42 0.87 21.31 184.83 1.07 0.36 1.63
LSTM-AR 13.02 12.21 32.79 30.10 34.13 38.74 47.24 65.11
LSTM-VAR N/A N/A 203.62 N/A N/A 197.26 N/A 19.85
TFT N/A N/A 366.90 N/A N/A 376.29 433.21 53.84
This Paper 5.20 13.24 0.99 2.27 20.99 0.94 3.03 0.46
Table 3: Percentage Uplift Against Baseline from Online Test
Metric Revenue Profit Basket size Orders
Treatment Effect 5.7% 5.3% 4.5% 1.0%
the desired trade off between exploration and exploitation, which
satisfies our objective in the industry, since we favor fewer compu-
tations and hence reduced execution time. The charts in Subsection
B.2 of the appendix support these findings.
5.3 Online Experiment Results
From the online experiments described in 4.4, we observed clear
uplift in business metrics against the the baseline pricing strategy
from business at the 95% confidence level. In Table 3, we share the
results from one of such experiments, which studied optimizing
a custom objective function that emphasizes revenue maximiza-
tion under strict business constraints. We share additional online
evaluation results in Subsection B.3 of the appendix.
5.4 Current State and Future Direction
The proposed dynamic pricing methods were successfully deployed
at Shipt after an extended period of careful experimentation, val-
idation, and phased rollout. As of the time of writing this paper,
the coverage had already extended to hundreds of retailer locations
and tens of thousands of itemized products, which amounts to tens
of millions of weekly and subweekly pricing decisions. It must
be noted that our production implementation involves additional
practical and business-specific considerations, including product
feature engineering, pricing constraints customization, customized
loss functions, and custom objectives. Specifics of such considera-
tions are omitted from this paper due to business sensitivity.For current and future work, we are exploring product and re-
tailer grouping methods to address edge cases, such as retailers
or products lacking historical sales data or having high degrees of
sparsity. We are also interested in adapting the offline optimization
metaheuristic into an online reinforcement learning approach.
6 Conclusion
This paper presented a solution to the dynamic pricing problem in
the context of multi-retailer delivery platforms, which operate in
the shared economy space. We proposed an additive deep learning
framework for guiding neural networks into capturing the incre-
mental contribution of price variations, while controlling for other
effects, in extremely high-dimensional space. For optimization, we
proposed evolutionary algorithm agents to optimize the solution
uniquely for each retailer or location. We modified the mutation
process and discretized the search space to enhance convergence
speed. Distinct from literature, our optimization method is very
flexible and adaptable to fluctuating objectives, constraints, demand
models, or other business requirements, while the demand mod-
eling solution efficiently scales to encompass multiple retailers or
locations and learns the cross-elasticity without utilizing explicit
assumptions of product correlations or nonpublic product features,
such as shelf-life, inventory levels, or competitor pricing. Results,
from both online and offline experiments with real data, prove sig-
nificantly superior performance in terms of accuracy and speed,
particularly in very large scale settings. Therefore, the industry-
practical solution has been deployed at Shipt and can be extended
to various types of multi-retailer delivery services.
References
[1]Alberto Abadie, Alexis Diamond, and Jens Hainmueller. 2010. Synthetic control
methods for comparative case studies: Estimating the effect of California’s tobacco
control program. Journal of the American statistical Association 105, 490 (2010),
493–505.
4748Dynamic Pricing for Multi-Retailer Platforms KDD ’24, August 25–29, 2024, Barcelona, Spain
[2]Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich,
Rich Caruana, and Geoffrey E Hinton. 2021. Neural additive models: Interpretable
machine learning with neural nets. Advances in neural information processing
systems 34 (2021), 4699–4711.
[3]Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori
Koyama. 2019. Optuna: A next-generation hyperparameter optimization frame-
work. In Proceedings of the 25th ACM SIGKDD international conference on knowl-
edge discovery & data mining. 2623–2631.
[4]Thomas Bäck, David B Fogel, and Zbigniew Michalewicz. 2018. Evolutionary
computation 1: Basic algorithms and operators. CRC press.
[5]James E Baker et al .1987. Reducing bias and inefficiency in the selection algorithm.
InProceedings of the second international conference on genetic algorithms , Vol. 206.
14–21.
[6]Elias Bareinboim, Andrew Forney, and Judea Pearl. 2015. Bandits with unobserved
confounders: A causal approach. Advances in Neural Information Processing
Systems 28 (2015).
[7]Omar Besbes and Assaf Zeevi. 2015. On the (surprising) sufficiency of linear
models for dynamic pricing with demand learning. Management Science 61, 4
(2015), 723–739.
[8]Jinzhi Bu, David Simchi-Levi, and Chonghuan Wang. 2022. Context-Based Dy-
namic Pricing with Partially Linear Demand Model. In Advances in Neural Infor-
mation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho (Eds.). https://openreview.net/forum?id=cLx3kbl2AI
[9]Ping Cao, Nenggui Zhao, and Jie Wu. 2019. Dynamic pricing with Bayesian
demand learning and reference price effect. European Journal of Operational
Research 279, 2 (2019), 540–556.
[10] Daqing Chen, Kun Guo, and George Ubakanma. 2015. Predicting customer
profitability over time based on RFM time series. International Journal of Business
Forecasting and Marketing Intelligence 2, 1 (2015), 1–18.
[11] Arnoud V Den Boer. 2015. Dynamic pricing and learning: historical origins, cur-
rent research, and new directions. Surveys in operations research and management
science 20, 1 (2015), 1–18.
[12] Chenyou Fan, Yuze Zhang, Yi Pan, Xiaoyue Li, Chi Zhang, Rong Yuan, Di Wu,
Wensheng Wang, Jian Pei, and Heng Huang. 2019. Multi-horizon time series
forecasting with temporal attention learning. In Proceedings of the 25th ACM
SIGKDD International conference on knowledge discovery & data mining. 2527–
2535.
[13] Ketki Gupte, Linsey Pang, Avinash Thangali, Prakhar Mehrotra, and Harshada
Vuyyuri. 2021. Robust Price Optimization in Retail. In Time Series Workshop
[Poster Presentation] at the 38th International Conference on Machine Learning.
[14] J Michael Harrison, N Bora Keskin, and Assaf Zeevi. 2012. Bayesian dynamic pric-
ing policies: Learning and earning under a binary prior distribution. Management
Science 58, 3 (2012), 570–586.
[15] Trevor Hastie, Robert Tibshirani, Jerome Friedman, Trevor Hastie, Robert Tibshi-
rani, and Jerome Friedman. 2009. Boosting and additive trees. The elements of
statistical learning: data mining, inference, and prediction (2009), 337–387.
[16] Junhao Hua, Ling Yan, Huan Xu, and Cheng Yang. 2021. Markdowns in E-
Commerce Fresh Retail: A Counterfactual Prediction and Multi-Period Optimiza-
tion Approach. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 3022–3031.
[17] Jian Huang, Mingming Leng, and Mahmut Parlar. 2013. Demand functions in
decision modeling: A comprehensive survey and research directions. Decision
Sciences 44, 3 (2013), 557–609.
[18] Shinji Ito and Ryohei Fujimaki. 2017. Optimization beyond prediction: Prescrip-
tive price optimization. In Proceedings of the 23rd ACM SIGKDD international
conference on knowledge discovery and data mining. 1833–1841.
[19] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling
long-and short-term temporal patterns with deep neural networks. In The 41st
international ACM SIGIR conference on research & development in information
retrieval. 95–104.
[20] Hongmin Li and Woonghee Tim Huh. 2011. Pricing multiple products with the
multinomial logit and nested logit models: Concavity and implications. Manu-
facturing & Service Operations Management 13, 4 (2011), 549–563.
[21] Bryan Lim, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021. Temporal fusion
transformers for interpretable multi-horizon time series forecasting. International
Journal of Forecasting 37, 4 (2021), 1748–1764.
[22] Eleanor Loh, Jalaj Khandelwal, Brian Regan, and Duncan A Little. 2022. Pro-
motheus: An End-to-End Machine Learning Framework for Optimizing Mark-
down in Online Fashion E-commerce. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3447–3457.
[23] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. Advances in neural information processing systems 30 (2017).
[24] Yiyun Luo, Will Wei Sun, and Yufeng Liu. 2023. Distribution-free contextual
dynamic pricing. Mathematics of Operations Research (2023).
[25] Prakhar Mehrotra, Linsey Pang, Karthick Gopalswamy, Avinash Thangali, Timo-
thy Winters, Ketki Gupte, Dnyanesh Kulkarni, Sunil Potnuru, Supreeth Shastry,
and Harshada Vuyyuri. 2020. Price Investment using Prescriptive Analytics and
Optimization in Retail. In Proceedings of the 26th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. 3136–3144.
[26] Jonas W Mueller, Vasilis Syrgkanis, and Matt Taddy. 2019. Low-rank bandit
methods for high-dimensional dynamic pricing. Advances in Neural Information
Processing Systems 32 (2019).
[27] Noemie Perivier and Vineet Goyal. 2022. Dynamic pricing and assortment under
a contextual MNL demand. In Advances in Neural Information Processing Systems,
Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.).
https://openreview.net/forum?id=OptX3Db1P4
[28] Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella,
Yuyang Wang, and Tim Januschowski. 2018. Deep state space models for time
series forecasting. Advances in neural information processing systems 31 (2018).
[29] David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and
Jan Gasthaus. 2019. High-dimensional multivariate forecasting with low-rank
gaussian copula processes. Advances in neural information processing systems 32
(2019).
[30] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.
DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-
national Journal of Forecasting 36, 3 (2020), 1181–1191.
[31] Virag Shah, Ramesh Johari, and Jose Blanchet. 2019. Semi-parametric dynamic
contextual pricing. Advances in Neural Information Processing Systems 32 (2019).
[32] Tilo Strutz. 2011. Data fitting and uncertainty: A practical introduction to weighted
least squares and beyond. Springer.
[33] Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal
of the Royal Statistical Society: Series B (Methodological) 58, 1 (1996), 267–288.
[34] Pradnya A Vikhar. 2016. Evolutionary algorithms: A critical review and its future
prospects. In 2016 International conference on global trends in signal processing,
information computing and communication (ICGTSPICC). IEEE, 261–265.
[35] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino,
and Silvio Savarese. 2018. Generalizing to unseen domains via adversarial data
augmentation. Advances in neural information processing systems 31 (2018).
[36] Kaiwen Xia, Li Lin, Shuai Wang, Haotian Wang, Desheng Zhang, and Tian He.
2023. A Predict-Then-Optimize Couriers Allocation Framework for Emergency
Last-mile Logistics. In Proceedings of the 29th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining. 5237–5248.
[37] Akihiro Yabe, Shinji Ito, and Ryohei Fujimaki. 2017. Robust Quadratic Program-
ming for Price Optimization.. In IJCAI. 4648–4654.
[38] Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the
elastic net. Journal of the royal statistical society: series B (statistical methodology)
67, 2 (2005), 301–320.
A Reproducibility information
The published dataset and detailed reproducibility information are
available publicly on GitHub via https://github.com/aabdulaal/gerd.
The GitHub repository contains the dataset in a ‘.txt‘ format, the
licensing information, and a ‘.pdf‘ document detailing the following:
•Published Dataset
•Other Public Datasets
•Demand Estimation Benchmark Hyperparameters and Set-
tings
•Evolutionary Optimization Metaheurisitc Hyperparameters
•Software Packages
•Generalized Hyperparameters and Settings for the Additive
Deep Learning
B Additional Visualizations
B.1 Demand Elasticity and Cross-Elasticity
Visuals
The figures below contrast the learned elasticity and cross-elasticity
dependencies with and without the additive training framework.
The axes values are resulting from applying the model explainability
tool in [ 23] on models trained with normalized data. Axes values
for plots obtained from internal datasets have been hidden due to
business-sensitivity.
4749KDD ’24, August 25–29, 2024, Barcelona, Spain Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, Yimin Yi
Figure 5: Elastic demand curves for product B10T11C33V1
from the GERD dataset, learned from classically trained
DNNs (left) and the additively trained DNNs (right). The ad-
ditively trained model learned realistic dependencies, with
decreasing demand in response to increasing price.
Figure 6: Cross-elasticity of product B10T11C33V1’s demand
to complimenting product B13T11C50V1’s price, from the
GERD dataset, as explained by classically trained DNNs (left)
and after the additive training application (right).
Figure 7: Elastic demand curves for product B10T10M100bV1
from the GERD dataset, as learned from classically trained
DNNs (left) and after applying the additive training strategy
(right).
Figure 8: Cross-elasticity of B10T10M100bv1’s demand to a
substitute product B10T10M50bv1’s price, which is the same
product but in smaller package size, as explained by classi-
cally trained DNNs (left) and additively trained DNNs (right).
Figure 9: Elasticity curves for cleaning supplies product from
classical DNNs (left) and additive DNNs (right). Data used
from an internal dataset model with 𝑀=1000. Additive train-
ing impact is more critical in higher-dimensional datasets.
Figure 10: Cross-elasticity of a cleaning supplies product’s
demand to a substitute product’s price, as learned by classical
DNNS (left) and additive DNNs (right). Data used from inter-
nal dataset model with 𝑀=1000. Additive training impact is
more critical in higher-dimensional datasets.
4750Dynamic Pricing for Multi-Retailer Platforms KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 11: Elasticity curves for a beverege product from clas-
sical DNNs (left) and additive DNNs (right). Data used from
internal dataset model with 𝑀=1000. Additive training im-
pact is more critical in higher-dimensional datasets.
Figure 12: Impact of adversarial data augmentation on the
demand curves for 3 cart-dependent products in response to
the change of one of their prices.
B.2 Optimization Analysis Visuals
Figure 13: Convergence comparisons against open-source
methods and various settings, such as including discretiza-
tion and the potential-based selection mechanism in our GA
implementation.
Figure 14: Impact of the proposed potential-based mutation
and discretization granularity on GA convergence.
B.3 Online Evaluation Visuals
Figure 15: Raw basket size increase for the sample online
study corresponding to the results in Table 3.
Figure 16: Percentage uplift in key business metrics at select
stores nation-wide, where the proposed dynamic pricing is
activated, against holdout stores in corresponding locations.
Table 4: Percentage uplift in business metrics at activated
stores against holdout stores in select locations across nation.
Metric Revenue Profit Basket size Orders
Treatment Effect 4.9% 3.9% 2.5% 3.3%
4751