Causal Subgraph Learning for Generalizable Inductive Relation
Prediction
Mei Li
limei-666@mail.nankai.edu.cn
Global Institute of Future Technology, Shanghai Jiao Tong
University
Shanghai, China
College of Computer Science, Nankai University
Tianjin, ChinaXiaoguang Liu∗
liuxg@nbjl.nankai.edu.cn
College of Computer Science, TMCC, SysNet, DISSec,
GTIISC, Nankai University
Tianjin, China
Hua Ji
hji@cauc.edu.cn
Department of Computer Science, Civil Aviation
University of China
Tianjin, ChinaShuangjia Zheng∗
shuangjia.zheng@sjtu.edu.cn
Global Institute of Future Technology, Shanghai Jiao Tong
University
Shanghai, China
ABSTRACT
Inductive relation reasoning in knowledge graphs aims at predicting
missing triplets involving unseen entities and/or unseen relations.
While subgraph-based methods that reason about the local struc-
ture surrounding a candidate triplet have shown promise, they often
fall short in accurately modeling the causal dependence between a
triplet’s subgraph and its ground-truth label. This limitation typi-
cally results in a susceptibility to spurious correlations caused by
confounders, adversely affecting generalization capabilities. Herein,
we introduce a novel front-door adjustment-based approach de-
signed to learn the causal relationship between subgraphs and their
ground-truth labels, specifically for inductive relation prediction.
We conceptualize the semantic information of subgraphs as a medi-
ator and employ a graph data augmentation mechanism to create
augmented subgraphs. Furthermore, we integrate a fusion mod-
ule and a decoder within the front-door adjustment framework,
enabling the estimation of the mediator’s combination with aug-
mented subgraphs. We also introduce the reparameterization trick
in the fusion model to enhance model robustness. Extensive ex-
periments on widely recognized benchmark datasets demonstrate
the proposed method’s superiority in inductive relation prediction,
particularly for tasks involving unseen entities and unseen rela-
tions. Additionally, the subgraphs reconstructed by our decoder
offer valuable insights into the model’s decision-making process,
enhancing transparency and interpretability.
∗Corresponding Authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671972CCS CONCEPTS
•Computing methodologies →Causal reasoning and diag-
nostics; Semantic networks.
KEYWORDS
Inductive relation prediction, causal inference, front-door adjust-
ment, knowledge graph.
ACM Reference Format:
Mei Li, Xiaoguang Liu, Hua Ji, and Shuangjia Zheng. 2024. Causal Subgraph
Learning for Generalizable Inductive Relation Prediction . In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671972
1 INTRODUCTION
Knowledge graphs (KGs), which map factual knowledge through
triplets connecting head entities to tail entities, notoriously face
incompleteness due to human knowledge limits and data extraction
shortcomings. This issue highlights the importance of predicting
missing facts, a critical task for applications in social networks [ 52],
recommendation systems [ 9], drug discovery [ 34] and others [ 21].
While embedding-based methods [ 2,29,41,47] have shown promise
by transforming entities and relations into low-dimensional vectors
for edge prediction, they predominantly rely on a transductive
learning approach. This assumes all entities and relations during
test are previously seen during training, neglecting the dynamic
nature of KGs, such as the introduction of new users and products.
As a result, these methods struggle to predict relations for unseen
entities, underscoring the need for models capable of adapting to
new entities and relations without costly re-training.
Recently, inductive relation reasoning, involving unseen en-
tities and/or unseen relations in the test stage, posits a set of
new challenges to tackle. This kind of task aims to learn trans-
ferable structure patterns from a training graph, and then general-
ize to a disjoint test graph to make predictions. Rule-based meth-
ods [ 6,27,28,36,48] and subgraph-based methods [ 3,10,26,39]
are two popular research tracks. Rule-based methods are inherently
inductive via mining entity-independent relational rules based on
 
1610
KDD ’24, August 25–29, 2024, Barcelona, Spain Mei Li, Xiaoguang Liu, Hua Ji, and Shuangjia Zheng
observed frequent co-occurrence patterns in KGs. However, they
suffer from limited expressive power and scalability issues due to
their rule-based nature. For instance, for a KG with 𝑚relations,
there are𝑂(𝑚𝐿)candidate rules within length 𝐿, whereas, only a
few rules are valid during logical reasoning.
GraIL [ 39] and its subsequent methods [ 3,10,22,26] offer in-
novative approaches for inductively predicting links by reasoning
over subgraph structures, leveraging the expressive power of graph
neural networks (GNNs) for inductive relation reasoning. These
methods, which involve extracting subgraphs around target triplets,
annotating entities with relative positions, and using a GNN model
to score the subgraphs, demonstrate a strong ability to learn re-
lational semantics independent of specific entities. This inductive
approach allows for generalization to unseen entities and graphs
by representing specific logical rules through subgraph patterns
and determining initial entity embeddings based on their subgraph
positions. However, these methods also face challenges with gener-
alization due to the potential for capturing spurious dependencies
from confounders within the subgraphs. Redundant and irrelevant
connections may co-occur with meaningful relational patterns, in-
creasing the learning difficulty and affecting the model’s ability to
learn transferable structures in an inductive setting. Identifying
the causal relationship between semantic relational patterns and
relation prediction becomes crucial to overcoming these limitations.
The recent development of causal inference motivates us to
discover the true causal dependence between the subgraph of a
triplet and its ground-truth label. Given the input 𝐺and its label
𝑌, the causal dependence is modeled as 𝑃(𝑌|𝑑𝑜(𝐺)), where the
𝑑𝑜(𝐺)operator denotes an intervention on 𝐺[32,33], which en-
ables the removal of all spurious associations between 𝐺and𝑌.
𝑃(𝑌|𝑑𝑜(𝐺))can be estimated by either the back-door adjustment
or the front-door adjustment (FA) [ 32,33]. The former requires ob-
servable confounders, which is not realistic as confounders are hard
to be explicitly modeled, especially for KGs. The latter supports
causal inference with hidden confounders via finding a mediator,
appearing to be a feasible way of capturing causal structure patterns
for inductive relation reasoning.
Herein, we propose FAGA, a novel method leveraging a front-
door adjustment approach to model the causal distribution in the
context of inductive relation prediction. Our method utilizes seman-
tic information from subgraphs as a mediator to uncover causal
dependencies, positing that hidden features mined from subgraphs
provide essential semantic details for relation reasoning. FAGA com-
bines the front-door criterion with a graph augmentation strategy
to enhance learning. Specifically, for each target triplet, we extract
its surrounding subgraph from the KG and generate a corrupted
counterpart via graph augmentation mechanism. These subgraphs
undergo encoding to learn entity representations, which are then
integrated using a fusion module that incorporates two feature
fusion strategies–Mixup [ 12,49] and an Adaptive Instance Nor-
malization (AdaIN) module [ 15] known from image style transfer,
along with a reparameterization technique to integrate semantic
information and improve model robustness. A decoder is employed
to reconstruct the subgraph by producing edge masks, focusing
on retaining causally significant relation patterns and discarding
spurious connections. This allows for making predictions based on
the refined subgraphs. The proposed method effectively capturessemantic relations of subgraphs and is demonstrated to hold the
promise for fulfilling the front-door requirements. Additionally,
subgraphs reconstructed by the decoder present valuable insights
about model decisions. During training, the proposed model is
optimized in an end-to-end fashion to generate subgraph repre-
sentations. At the inference time, it generates representations of
subgraphs with new entities and relations based on model weights
learned during training.
We summarize our contributions as follows:
•We introduce a competitive front-door adjustment-based ap-
proach, FAGA, to discern causal dependencies between subgraphs
and their ground-truth labels for inductive relation reasoning.
•We integrate the graph augmentation strategy with feature
fusion techniques (Mixup and AdaIN) and a reparameterization
trick to enhance the model’s awareness of causal dependencies.
•Our empirical evaluations on inductive link prediction bench-
mark datasets with both unseen relations and unseen nodes demon-
strate FAGA’s superior generalization capabilities and its favorable
interpretability.
2 RELATED WORK
In this section, we introduce works closely related to this paper,
including inductive relation prediction, causal inference, and graph
augmentation.
2.1 Inductive Relation Prediction
Most real-world KGs are ever-evolving with new entities emerging
continuously. Hence, it is paramount to introduce methods that
are entity-agnostic and can make predictions about new entities
and new relations without expensive re-training. Rule-based meth-
ods and GNN-based methods are two typical inductive relation
prediction methods. Rule-based methods mainly focus on mining
Horn rules in KGs. Classical rule learners, such as AMIE [ 6], Any-
BURL [ 27], and RuleN [ 28], first enumerate possible rules and then
learn a confidence score for each rule. Neural LP [ 48] and its vari-
ants, e.g., DRUM [ 36], propose differentiable rule mining methods
based on TensorLog [ 4] to simultaneously learn logical rules and
their confidence scores. On the other hand, witnessing the promis-
ing performance of GNNs in various domains [ 8,13,20], increasing
methods are designed for inductive relation prediction. For example,
GraIL [ 39] and CoMPILE [ 26] treat relation prediction as a subgraph
reasoning problem. Very recently, TACT [ 3] and RMPI [ 10] model
semantic correlations between relations in an entity-independent
manner and INGRAM [ 18] extend them in an relation-independent
manner. Also, many works combine the advantages of rule-based
and subgraph-based methods for accurate inductive relation predic-
tion [ 23,31]. However, the above-mentioned methods are limited in
exploring the causal dependence between local neighborhood struc-
tures and relation ground-truth labels, and hence are vulnerable to
spurious relations.
2.2 Causal Inference
Causal inference is an effective tool for learning causal relation-
ships between various variables. It has been a hot research topic
in various domains, including computer vision [ 30], recommenda-
tion systems [ 43], and graph-related tasks [ 19,30,38]. The most
 
1611Causal Subgraph Learning for Generalizable Inductive Relation Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
common application of causal inference is to remove spurious cor-
relations that mislead model decisions and worsen model gener-
alization capability. Existing methods often employ the back-door
adjustment to derive the causal dependence, for example, disentan-
gling input graphs into causal substructures and shortcut substruc-
tures [ 19,30,38]. Whereas, practically, confounders are not always
observed [ 17]. In this case, the FA criterion provides an alternative
by using a mediator variable to capture the intermediate causal
effect. The effectiveness of the FA criterion has been verified in
multiple tasks, such as domain generalization [ 30] and credit risk
analysis [ 37]. However, it has not been implemented for relation
reasoning in KGs. Our work can be seen as the first attempt to
effectively apply FA for inductive relation reasoning in KGs.
2.3 Graph Augmentation
Data augmentation techniques enhance model performance and
generalization capability by increasing the amount of training
data and task difficulty. Existing graph data augmentation (GDA)
techniques are mainly inspired by image and sequence augmenta-
tion techniques, including rule-based (non-learnable) and learn-
able GDA methods [ 51]. Particularly, due to simplicity and ef-
ficiency, rule-based GDA methods enjoy the most extensive ap-
plication in graph machine learning. They are categorized into
attribute-based, topology-based, and hybrid approaches [ 25,44,51].
Attribute-based approaches operate on node attributes, such as
node feature masking and node feature shuffle. Topology-based
approaches work on graph adjacency matrix, including edge per-
turbation (e.g., edge adding, edge dropping) and graph diffusion
(creating new connections between nodes based on random walks).
Hybrid approaches involve both attribute-based and topology-based
approaches, typically, subgraph sampling. In this paper, we pay at-
tention to topology-based GDAs as we mainly focus on learning
transferable structure patterns and the initial embeddings of entities
are encoded according to their relative positions in subgraphs.
3 PRELIMINARY
3.1 Problem Formulation
A KGG={E,R,T}is a heterogeneous graph, consisting of a set
of entitiesE, relationsR, and tripletsT={(ℎ,𝑟,𝑡)⊆E×R×E} . A
triplet(ℎ,𝑟,𝑡)specifies the relation 𝑟∈Rbetween the head entity
ℎ∈Eand the tail entity 𝑡∈E. For each relation 𝑟, we add a reverse
relation𝑟−1toR. Meanwhile, for each triplet (ℎ,𝑟,𝑡), we add its
reverse(ℎ,𝑟−1,𝑡)toT. In inductive settings, Gis partitioned into
two entity-disjoint KGs, that is, G𝑡𝑟={E𝑡𝑟,R𝑡𝑟,T𝑡𝑟}for training
andG𝑡𝑒={E𝑡𝑒,R𝑡𝑒,T𝑡𝑒}for test, whereE𝑡𝑟∩E𝑡𝑒=∅, and
R𝑡𝑒⊆R𝑡𝑟, representing that relations in the test set are a part of
these in the training set (or R𝑡𝑒⊄R𝑡𝑟, indicating that there exist
new relations in the test set). The relation prediction task is to score
the target triplet(ℎ,𝑟,𝑡), i.e., to predict the likelihood 𝑃(𝑌|(ℎ,𝑟,𝑡))
the relation 𝑟holds between ℎand𝑡, where𝑌∈{0,1}is the label
of the triplet.
In this work, we mainly center on the scenario of E𝑡𝑟∩E𝑡𝑒=∅
andR𝑡𝑒⊄R𝑡𝑟. We represent a target triplet as an attribute-free
subgraph surrounding target nodes 𝐺=(𝑉,𝐸,𝑅), where𝑉⊆E,
𝐸∈𝑉×𝑅×𝑉, and𝑅⊆R denote the set of nodes, observed edges,
and relation types, respectively, and the values in the adjacencymatrix𝐸are either 0 or 1. Our key idea is to complete the relation
prediction task from 𝐺by inferring causal structures and meanwhile
removing spurious relationships from 𝐺, i.e.,𝑃(𝑌|𝑑𝑜(𝐺)).
3.2 A Causal View on Subgraph-Based Methods
In order to clearly understand the limitation of existing subgraph-
based relation prediction methods, we take a causality perspective
at𝑃(𝑌|𝐺)and construct a structural causal model (SCM) [ 32] in
Figure 1(a). It describes the causalities among three variables, i.e.,
subgraph𝐺, spurious connections 𝑆, and label𝑌, where the link
from one variable to another indicates the cause-effect relationship,
i.e., cause→effect. We present the following explanations for SCM.
•𝐺→𝑌. This describes the true causal relationship between
𝐺and𝑌, indicating that the structural semantics in 𝐺entail the
prediction of label 𝑌.
•𝐺←𝑆→𝑌. This shows the confounding dependence between
𝐺and𝑌induced by noise connections and trivial patterns. Models
relying on spurious relationships 𝑆to make predictions during
training would generalize poorly during inference.
According to the SCM, the statistical distribution of 𝑃(𝑌|𝐺)has
the following mathematical expression,
𝑃(𝑌|𝐺)=∑︁
𝑠∈𝑆𝑃(𝑠|𝐺)𝑃(𝑌|𝐺,𝑠)
=E𝑃(𝑠|𝐺)𝑃(𝑌|𝐺,𝑠),(1)
where𝑠denotes the specific value of 𝑆. We observe that 𝑃(𝑌|𝐺)
comprises the causal and spurious relationships. Since 𝑃(𝑠|𝐺)varies
for different 𝐺, a model is likely to be biased towards some frequent
patterns, enabling wrong decisions in the test phase.
Based on the above analyses, we find that the key to shield models
from the confounder 𝑆is to capture only the true causal relationship
between𝐺and𝑌. From the perspective of causal inference [ 32,33],
we can achieve this via modeling 𝑃(𝑌|𝑑𝑜(𝐺)), where𝑑𝑜(𝐺)denotes
the do-calculus operation, which removes links from 𝑆to𝐺and
blocks associative paths from 𝐺to𝑌apart from the direct causal
one𝐺→𝑌. Two common approaches can be employed to estimate
𝑃(𝑌|𝑑𝑜(𝐺)), that is, the back-door adjustment and the front-door
adjustment. The former is formulated as
𝑃(𝑌|𝑑𝑜(𝐺))=∑︁
𝑠∈𝑆𝑃(𝑠)𝑃(𝑌|𝐺,𝑠), (2)
The back-door adjustment alleviates the confounding effect of 𝑆
by making a model predict with intervened 𝐺, which is created by
stratifying the confounder 𝑆={𝑠}. However, there exist challenges
implementing Equation (2)since𝑆is not observed in 𝐺and there
is no way to estimate 𝑃(𝑠). This work aims to model 𝑃(𝑌|𝑑𝑜(𝐺))
through the latter, and we will elaborate on it in the next section.
3.3 Front-Door Adjustment
Assume variable 𝑍acts as a mediator between 𝐺and𝑌. It satisfies
the following three conditions [ 32,33], namely, (C1) all directed
paths from𝐺to𝑌flow through 𝑍, (C2)𝐺blocks all back-door paths
from𝑍to𝑌, and (C3) no unblocked back-door path exists from
𝐺to𝑍. The first two conditions require 𝑍to capture all semantic
information in 𝐺that provides necessary and sufficient informa-
tion for the prediction of 𝑌, and the last condition suggests that
only the observed training data can be used to estimate the causal
 
1612KDD ’24, August 25–29, 2024, Barcelona, Spain Mei Li, Xiaoguang Liu, Hua Ji, and Shuangjia Zheng
relationship 𝐺→𝑍. Based on the above conditions, we express the
FA formula as
𝑃(𝑌|𝑑𝑜(𝐺))=∑︁
𝑧∈𝑍𝑃(𝑧|𝑑𝑜(𝐺))𝑃(𝑌|𝑧,𝑑𝑜(𝐺)) (𝐵𝑎𝑦𝑒𝑠′𝑟𝑢𝑙𝑒)(3)
=∑︁
𝑧∈𝑍𝑃(𝑧|𝐺)𝑃(𝑌|𝑑𝑜(𝑧),𝑑𝑜(𝐺)) (𝐶2, 𝐶3) (4)
=∑︁
𝑧∈𝑍𝑃(𝑧|𝐺)𝑃(𝑌|𝑑𝑜(𝑧)) ( 𝐶1) (5)
=∑︁
𝑧∈𝑍𝑃(𝑧|𝐺)(∑︁
˜𝐺∈Φ(𝐺)𝑃(˜𝐺)𝑃(𝑌|𝑧,˜𝐺)) (6)
=E𝑃(𝑧|𝐺)E𝑃(˜𝐺)𝑃(𝑌|𝑧,˜𝐺), (7)
where𝑧denotes the specific value of 𝑍,Φ(·)denotes GDA tech-
niques,Í
˜𝐺∈Φ(𝐺)𝑃(˜𝐺)𝑃(𝑌|𝑧,˜𝐺)in Equation (6)is the back-door
adjustment of 𝑃(𝑌|𝑑𝑜(𝑧))under condition C1. In this work, we
treat ˜𝐺as the augmented counterpart of 𝐺, which increases data
quantity and diversity, improving model performance.
Our final objective is to approximate 𝑃(𝑌|𝑑𝑜(𝐺))by modeling
Equation (7). We see that the prediction probability 𝑃(𝑌|𝑧,˜𝐺)no
longer depends on spurious connections 𝑆, which is distinct from
the prediction probability 𝑃(𝑌|𝐺,𝑠)presented in the back-door
adjustment formula in Equation (2). It implies that through FA crite-
rion we can learn a robust model that captures semantically mean-
ingful information from subgraphs and generalizes well during the
inference stage. Additionally, in Equation (7), the two expectations
can be estimated via Monte Carlo sampling, and 𝑃(𝑧|𝐺)can be mod-
eled by a GNN-based encoder that maps a subgraph 𝐺to𝑧. Thus,
our remaining problem is to model 𝑃(𝑌|𝑧,˜𝐺). In the next section,
we propose a novel way to model 𝑃(𝑌|𝑧,˜𝐺)in FA by leveraging
GDA, feature fusion, and the reparameterization strategies.
4 METHOD
4.1 Overall
The overall framework of our proposed model for modeling 𝑃(𝑌|𝑧,˜𝐺)
is illustrated in Figure 1(c). It mainly consists of an encoder, a fu-
sion module, a decoder, and a predictor. The encoder encodes a
subgraph𝐺and its augmented counterpart ˜𝐺into their correspond-
ing node representations, i.e., Z=𝐸𝑛𝑐(𝐺)and˜Z=𝐸𝑛𝑐(˜𝐺). Then,
Zand ˜Zserve as the inputs to the fusion model to generate the
hidden representations ˇZ. Particularly, in this module, we adopt the
Mixup [ 12,49] or the AdaIN module [ 15] used in image style trans-
fer to fuse semantic information Zand˜Z. Additionally, we further
employ a reparameterization trick to produce ˇZ, which enhances
model robustness. Afterwards, we feed ˇZinto the decoder to gener-
ate edge masks and obtain the edge-masked subgraph 𝐺∗of𝐺. Once
the proposed model has been well trained, 𝐺∗captures only seman-
tically meaningful information of 𝐺for prediction. Therefore, we
can condition 𝑌on𝐺∗instead of Zand ˜𝐺and regard E𝑃(˜𝐺)𝑃(𝑌|𝐺∗)
as an approximation of E𝑃(𝑧|𝐺)E𝑃(˜𝐺)𝑃(𝑌|𝑧,˜𝐺). Finally, we achieve
the prediction of 𝐺∗by feeding it into the predictor. In the subse-
quent sections, we will introduce each component of the proposed
model in detail. Before that, we first present the introduction of
subgraph extraction.4.2 Subgraph Extraction
Following GraIL [ 39], we represent a target triplet (ℎ,𝑟,𝑡)as a
subgraph surrounding it. The basic idea is that the local neighbor-
hood of a candidate triplet in the KG contains the logical evidence
that supports the target relation prediction. Particularly, the paths
connecting the two target nodes encompass rich semantic infor-
mation that implies the target relation. We extract the subgraph
𝐺of the target triplet (ℎ,𝑟,𝑡)by taking the intersection of the
𝑘-hop neighborhood sets of nodes ℎand𝑡(i.e.,N𝑘(ℎ)∩N𝑘(𝑡))
and meanwhile pruning nodes that are isolated or at a distance
greater than 𝑘from either ℎor𝑡. Since the extracted subgraph 𝐺
is attribute-free, we initialize nodes using the double radius ver-
tex labeling scheme. Concretely, for each node 𝑣∈𝐺, we label
it with the tuple(𝑑(𝑣,ℎ),𝑑(𝑣,𝑡)), where𝑑(𝑣,ℎ)denotes the short-
est distance between 𝑣andℎ, and so do 𝑑(𝑣,𝑡). Such a labeling
scheme captures the relative position of each node to the target
nodes and reflects the structural role of each node in the subgraph
as well. For target nodes ℎand𝑡, we uniquely label them with (0,
1) and (1, 0), respectively. Then, we convert the feature of node 𝑣
tox𝑣=[one-hot(𝑑(𝑣,ℎ))⊕one-hot(𝑑(𝑣,𝑡))], where⊕denotes the
concatenation operation, and the dimension of x𝑣is bounded by
the number of hops used for subgraph extraction.
4.3 Entity Representation Learning
We introduce the encoder to learn entity representations of sub-
graphs. We first learn relation representations from the training
KG. Concretely, we build a relation graph 𝐺𝑟𝑒𝑙=(R,Ψ,Ω)by con-
verting relations to nodes and affinities between relations to edge
weights, whereR,Ψ, andΩare the node set, the edge set and the
edge weights, respectively, and the affinity between two relations is
measured by the number of common entities they share [ 18]. The
goal of constructing the relation graph is to produce the embed-
ding for a target relation from its neighborhood representations,
so that we can generate embeddings for new relations at inference
time. We divide edge weights into 𝐵bins and map each weight
𝜔∈Ωto a bin. We adopt the attention mechanism to update the
representation of a relation from its neighborhood relations.
r(𝑙+1)
𝑖=𝛿(∑︁
𝑟𝑗∈¯N𝑖𝛽𝑖𝑗W(𝑙)
𝑟𝑒𝑙r(𝑙)
𝑗),
𝛽(𝑙)
𝑖𝑗=exp(a(𝑙)𝛿(Q(𝑙)[r(𝑙)
𝑖||r(𝑙)
𝑗||b𝑖𝑗]))
Í
𝑟𝑜∈¯N𝑖exp(a(𝑙)𝛿(Q(𝑙)[r(𝑙)
𝑖||r(𝑙)
𝑜||b𝑖𝑜])),(8)
where𝛿(·)is the ReLU activation function, ¯N𝑖denotes the neigh-
borhood of 𝑟𝑖,W(𝑙)
𝑟𝑒𝑙∈R𝑑𝑟×𝑑𝑟,a(𝑙)∈R1×𝑑𝑟, and Q(𝑙)∈R𝑑𝑟×3𝑑𝑟
are learnable parameters, r(0)
𝑖andb𝑖𝑜are initialized with trainable
parameters, and||denotes concatenating vectors. At each layer, we
also employ the residual connection and the multi-head attention
mechanism. After updating relation representations with 𝐿𝑟layers,
we extract relation representations R(𝐿𝑟)from the last layer and
project them into the dimension of 𝑑using a linear layer.
E=𝛿(Linear1(R(𝐿𝑟))), (9)
 
1613Causal Subgraph Learning for Generalizable Inductive Relation Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Encoder
 Decoderෘ𝐙Predictor
Knowledge GraphSubgraph
Extraction
Augmented  Subgraph ෨𝐺Reconstructed 
Subgraph 𝐺∗Fusion 𝑟2𝑟3
𝑟2
𝑟3𝑟3𝑟4𝑟4𝑟4
𝑟4𝑟1
𝑟1𝑟5𝑟5
𝑟5(0,1)(1,0)
(1,1)
(1,2)
(2,1)(1,1)
𝑟2𝑟3
𝑟2
𝑟3𝑟3𝑟4
𝑟4𝑟1
𝑟5𝑟2𝑟3
𝑟2
𝑟3𝑟3𝑟4
𝑟4𝑟1
𝑟5
GS
Y
GS
Z
 Y(0,1)(1,0)
(1,1)
(1,2)
(2,1)(1,1)
𝑟2𝑟3
𝑟2
𝑟3𝑟3𝑟4
𝑟4𝑟1
𝑟5
𝐙
෨𝐙
(a) Structural Causal Model
(b) Front -door AdjustmentAdaIN /
Mixup𝐙
෨𝐙
𝐖𝜇ത𝐙
𝐖𝜎
𝜇(ത𝐙)
𝜎(ത𝐙)
𝜖 ~ Normal(0, 𝐈)
ෘ𝐙

(C) Illustration of the Proposed Method𝐺𝔼𝑃෨𝐺𝑃𝑌𝐺∗
≈𝔼𝑃𝑧|𝐺𝔼𝑃෨𝐺𝑃(𝑌|𝑧,෨𝐺)Score
Figure 1: (a) A structural causal model (SCM) describing the causalities among three variables, i.e., subgraph 𝐺, spurious connections 𝑆, and
label𝑌. (b) The front-door adjustment of 𝑃(𝑌|𝑑𝑜(𝐺))with a mediator 𝑍between𝐺and𝑌. (c) Illustration of the proposed method that leverages
graph data augmentation, style transfer/mixup, and reparameterization techniques to perform the front-door adjustment.
where R(𝐿𝑟)[𝑖]=r(𝐿𝑟)
𝑖, and𝛿(·)is the ReLU activation function.
E∈R|R|×𝑑is then involved in the calculation processes of entity
representation learning, edge mask generation, and link prediction.
Given a subgraph 𝐺=(𝑉,𝐸,𝑅)with node embeddings X∈
R𝑁×𝑑𝑛, where𝑁=|𝑉|denotes the number of nodes in 𝐺, and𝑑𝑛
represents the dimension of node embeddings, we first project Xto
˜X∈R𝑁×𝑑using a linear layer followed by an activation function.
˜X=𝛿(Linear2(X)), (10)
where𝛿(·)is the ReLU activation function.
We employ 𝐿message passing and aggregation layers with the
attention mechanism as INGRAM [ 18] to encode entity hidden
representations. At the 𝑙-th layer, we update the representations of
𝑣by aggregating the representations of itself, its neighbors N𝑣=
{𝑢|(𝑣,𝑚,𝑢)∈𝐸,𝑢∈𝑉,𝑚∈𝑅}, and the relations adjacent to 𝑣.
h(𝑙+1)
𝑣=𝛿(𝛼𝑣𝑣W(𝑙)[h(𝑙)
𝑣||¯e𝑣]+∑︁
𝑢∈N𝑣∑︁
𝑚∈𝑅𝛼(𝑙)
𝑣𝑚𝑢W(𝑙)[h(𝑙)
𝑢||e𝑚]),
(11)
where h(0)
𝑣=˜X[𝑣],e𝑚=E[𝑚],𝛿(·)is the ReLU activation func-
tion,||denotes the concatenation operation, W(𝑙)∈R𝑑×2𝑑is the
weight matrix in the 𝑙-th layer, ¯e𝑣denotes the self-loop relation
representation obtained from the mean representations of relations
adjacent to𝑣, and𝛼(𝑙)
𝑣𝑣and𝛼(𝑙)
𝑣𝑚𝑢are the attention coefficients. ¯e𝑚,
𝛼(𝑙)
𝑣𝑣, and𝛼(𝑙)
𝑣𝑚𝑢are defined as
¯e𝑣=∑︁
𝑢∈N𝑣∑︁
𝑚∈𝑅𝑢𝑣e𝑚Í
𝑖∈N𝑣|𝑅𝑖𝑣|,
𝛼𝑣𝑣=exp(q(𝑙)𝛿(P(𝑙)c(𝑙)
𝑣𝑣))
exp(q(𝑙)𝛿(P(𝑙)c(𝑙)
𝑣𝑣))+Í
𝑖∈N𝑣Í
𝑗∈𝑅𝑖𝑣exp(q(𝑙)𝛿(P(𝑙)c(𝑙)
𝑣𝑗𝑖)),
𝛼𝑣𝑚𝑢=exp(q(𝑙)𝛿(P(𝑙)c(𝑙)
𝑣𝑚𝑢))
exp(q(𝑙)𝛿(P(𝑙)c(𝑙)
𝑣𝑣))+Í
𝑖∈N𝑣Í
𝑗∈𝑅𝑖𝑣exp(q(𝑙)𝛿(P(𝑙)c(𝑙)
𝑣𝑗𝑖)),
c(𝑙)
𝑣𝑣=[h(𝑙)
𝑣||h(𝑙)
𝑣||¯e𝑣],c(𝑙)
𝑣𝑚𝑢=[h(𝑙)
𝑣||h(𝑙)
𝑢||e𝑚],
(12)where q(𝑙)∈R1×𝑑is learnable attention parameters, P(𝑙)∈R𝑑×3𝑑
is a linear transformation matrix, 𝛿(·)is the ReLU activation func-
tion, and||denotes the concatenation operation. We also implement
the residual connection and the multi-head attention mechanism
to enhance model expressiveness. By updating h(𝑙+1)
𝑣 using Equa-
tion (11)for𝐿layers (𝑙=0,···,𝐿−1), we obtain the final entity
representations of 𝐺, i.e., Z=H(𝐿), which is regarded as an input
of the fusion module, where H(𝐿)[𝑣]=h(𝐿)
𝑣,𝑣∈𝑉.
4.4 Graph Augmentation
Since the proposed method relies on structural information of ex-
tracted subgraphs to make predictions and each subgraph has no
attribute, we focus on implementing topology-based augmentation
techniques to generate topology-perturbed subgraphs. Given the
subgraph𝐺=(𝑉,𝐸,𝑅)of a target triplet (ℎ,𝑟,𝑡), we define its
perturbed subgraph ˜𝐺=(𝑉,˜𝐸,𝑅)as
˜𝐺=(𝑉,Φ(𝐸),𝑅), (13)
where Φ(·)is the topological augmentation operation, which is
placed on the adjacency of 𝐺. DropEdge [ 35] is an efficient and
widely used graph augmentation strategy, which randomly drops a
fixed fraction of edges at each training step. In this work, we utilize
it as an augmentation strategy. Besides, matapaths connecting two
nodes in a KG embody rich semantic information. We introduce an
alternative of DropEdge by randomly dropping a fixed proportion of
paths that connect two target nodes and we denote this method as
DropMP. In ablation studies, we compare methods using DropEdge
and DropMP, finding that they achieve comparable results. We
obtain node representations of ˜𝐺, i.e., ˜Z, using Equation (10)∼(12)
and treat them as the other input to the fusion module.
4.5 Subgraph Reconstruction
The fusion module takes Zand ˜Zas inputs to produce ˇZ, which is
then used by the decoder to reconstruct subgraphs through generat-
ing edge masks. In the fusion module, we first adopt the Mixup [ 12]
to fuse semantic information Zof the subgraph 𝐺and ˜Zof the
 
1614KDD ’24, August 25–29, 2024, Barcelona, Spain Mei Li, Xiaoguang Liu, Hua Ji, and Shuangjia Zheng
augmented subgraph ˜𝐺as follows.
¯Z=𝜆Z+(1−𝜆)˜Z, (14)
An alternative of the Mixup is the AdaIN module [ 15] used in
image style transfer, which is formulated as
¯Z=AdaIN(Z,˜Z)=𝜇(˜Z)+𝜎(˜Z)Z−𝜇(Z)
𝜎(Z), (15)
where𝜇(Z)and𝜎(Z)denote the mean and the standard deviation
ofZ, which are calculated along its feature dimension, and so do
𝜇(˜Z)and𝜎(˜Z).
In order to increase model robustness, we further learn a latent
Gaussian distribution of ¯Z. Afterward, we use the reparameteriza-
tion trick to sample ˇZfrom the latent Gaussian distribution.
𝜇(¯Z)=W𝜇¯Z, 𝜎(¯Z)=W𝜎¯Z,
𝑞(ˇZ|¯Z)=Normal(𝜇(¯Z),𝜎(¯Z)I),
ˇZ=𝜇(¯Z)+𝜎(¯Z)◦𝜖,(16)
where W𝜇andW𝜎are learnable weights of the mean and the vari-
ance of the latent Guassian distribution, respectively, 𝜖∼Normal(0,I),
and◦denotes element-wise multiplication.
We design a decoder to capture semantic structures that are
the causation for decision making. The decoder takes the latent
representations of nodes ˇZand relations Eas inputs to generate edge
masks. Specifically, to generate the mask for an edge (𝑣,𝑚,𝑢), we
operate the relation translation proposed in TransE [ 2] onˇz𝑣=ˇZ[𝑣],
e𝑚=E[𝑚], and ˇz𝑢=ˇZ[𝑢], and achieve the mask score 𝑎𝑣𝑚𝑢by
summing up a vector into a scalar followed by a activation function.
ˇe𝑣𝑚𝑢=ˇz𝑣+e𝑚−ˇz𝑢,
𝑎𝑣𝑚𝑢=𝛿(∑︁
ˇe𝑣𝑚𝑢),(17)
where𝛿(·)is the sigmoid function. In our implementation, we set
the mask value of(𝑣,𝑚,𝑢)to 1 if𝑎𝑣𝑚𝑢⩾0.5. Once we have com-
puted all edge masks of a subgraph 𝐺, we construct a new subgraph
𝐺∗=(𝑉,𝐸∗,𝑅)and use it for target triplet relation prediction.
4.6 Link Prediction
Our link prediction predictor consists of 𝐿message passing and
aggregation layers formulated in Equation (11)and a multi-layer
perceptron (MLP). Here, the MLP is composed of two linear layers,
where the first layer is followed by the ReLU activation and the
Dropout [ 14].ˇZsampled from Equation (16)is regarded as initial
node representations of 𝐺∗. Let Fbe node representations of 𝐺∗
extracted from the 𝐿-th layer. For the subgraph of a target triplet
(ℎ,𝑟,𝑡), we gain its graph representation by average pooling its
node representations. We represent a target triplet by concatenating
four representation vectors, that is, the subgraph representation
(f𝐺∗), the representations of two target nodes (f ℎandf𝑡), and the
target relation representation (e 𝑟)). Finally, we score the target
triplet by passing its representation through the MLP.
f𝐺∗=1
|𝑉|∑︁
𝑣∈𝑉f𝑣,
score(ℎ,𝑟,𝑡)=MLP(f𝐺∗||fℎ||f𝑡||e𝑟),(18)
where||signifies the concatenation operation.We employ the noise-contrastive hinge loss [ 2] to supervise
model training by scoring positive triplets higher than negative
triplets. For each positive triplet, we sample its negative counterpart
by replacing its head/tail entity with a uniformly sampled entity.
The loss is formulated as
𝐿𝑜𝑠𝑠=|T𝑡𝑟|∑︁
𝑖=1max(0,score(𝑝𝑜𝑠𝑖)−score(𝑛𝑒𝑔𝑖)+𝛾). (19)
whereT𝑡𝑟is the set of triplets in the training set, 𝑝𝑜𝑠𝑖and𝑛𝑒𝑔𝑖
denote the𝑖-th positive triplet and its negative counterpart, respec-
tively, and𝛾is the margin parameter.
5 EXPERIMENTS
To verify the effectiveness of the proposed FAGA, we perform ex-
tensive experiments to answer the following research questions.
(RQ1) How effectively does FAGA generalize to datasets with both
unseen entities and unseen relations? (RQ2) How well does FAGA
perform on established inductive benchmark datasets? (RQ3) What
impacts do the various components of FAGA have on its perfro-
mance? (RQ4) Can FAGA identify causal structural patterns and
provide meaningful interpretations?
5.1 Datasets
We perform experiments on a series of benchmarks raised in IN-
GRAM [ 18]. These benchmarks are derived from three well-known
KGC datasets, namely, Wikidata68K (WK) [ 11], FB15k-237 (FB) [ 40],
and NELL-995 (NL) [ 45]. For each dataset, four versions of induc-
tive benchmarks are extracted by varying the percentage of triplets
with new relations, i.e., 100%, 75%, 50%, and 25%. For examples, in
NL-75, about 75% triplets have unknown relations, and 25% have
known relations. In NL-100, all triplets have unseen relations. Also,
each benchmark consists of a pair of entity-disjoint training and
test KGs. Details about these datasets are summarized in Table 5 in
Appendix A.
5.2 Evaluation and Experimental Setup
We adopt the metrics of the area under the precision-recall curve
(AUC-PR), the mean reciprocal rank (MRR), Hits@1, and Hits@10
to measure prediction performance. There are 514,498 trainable
parameters in the proposed FAGA. The Adam optimizer is em-
ployed with a learning rate 5×104and a weight decay rate 5×104.
We sample a 3-hop subgraph for each triplet in NELL-995, and a
2-hop subgraph for each triplet in Wikidata68K and FB15k-237.
We use three message passing and aggregation layers in both the
encoder and the predictor, and one message passing and aggre-
gation layer for relation representation learning. The number of
bins𝐵is set to 10 as [ 18], and the embeddings of relations and
bins are initialized with the dimension of 32. The dimension of
hidden features is set to 128. The 𝜆used in Equation (14)is set
to 0.5. The margin in the loss is tuned to ∈{1,3,5,7,9,10}. We
generate corrupted subgraphs with edge dropout rate or metapath
dropout rate∈{0.1,0.2,0.3,0.4,0.5,0.6}. The dropout rate in the
MLP is tuned to∈{0.1,0.2,0.3,0.4,0.5}. The model is run for 10
epochs and evaluated every 100 training iterations with the best
performing checkpoint used for the test.
 
1615Causal Subgraph Learning for Generalizable Inductive Relation Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Comparison results on 12 benchmarks with various percentages of triplets involving new relations.
MethodsNL-100 NL-75 NL-50 NL-25
MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10
GraIL [39] 13.50 11.40 17.30 9.60 3.60 20.50 16.20 10.40 28.80 21.60 16.00 36.60
CoMPILE [26] 12.30 7.10 20.90 17.80 9.30 36.10 19.40 12.50 33.00 18.90 11.50 32.40
SNRI [46] 4.20 2.90 6.40 8.80 4.00 17.70 13.00 9.50 18.70 19.00 14.00 27.00
INDIGO [24] 16.00 10.90 24.70 12.10 9.80 15.60 16.70 13.40 21.70 16.60 13.40 20.60
RMPI [10] 22.00 13.60 37.60 13.80 6.10 27.50 18.50 10.90 30.70 21.30 13.00 32.90
CompGCN [42] 0.80 0.10 1.40 1.40 0.30 2.50 0.30 0.00 0.50 0.60 0.00 1.00
NodePiece [7] 1.20 0.40 1.80 4.20 2.00 8.10 3.70 1.30 7.90 9.80 5.70 16.60
NeuralLP [48] 8.40 3.50 18.10 11.70 4.80 27.30 10.10 6.40 19.00 14.80 10.10 27.10
DRUM [36] 7.60 4.40 13.80 15.20 7.20 31.30 10.70 7.00 19.30 16.10 11.90 26.40
BLP [5] 1.90 0.60 3.70 5.10 1.20 12.00 4.10 1.10 9.30 4.90 2.40 9.50
QBLP [1] 0.40 0.00 0.30 4.00 0.70 9.50 4.80 2.00 9.70 7.30 2.70 15.10
NBFNet [53] 9.60 3.20 19.90 13.70 7.70 25.50 22.50 16.10 34.60 28.30 22.40 41.70
RED-GNN [50] 21.20 11.40 38.50 20.30 12.90 35.30 17.90 11.50 28.00 21.40 16.60 26.60
INGRAM [18] 30.90 21.20 50.60 26.10 16.70 46.40 28.10 19.30 45.30 33.40 24.10 50.10
FAGA 58.77 43.38 89.22 49.20 35.42 75.62 37.87 26.95 60.01 55.02 40.42 81.58
MethodsWK-100 WK-75 WK-50 WK-25
MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10
CompGCN [42] 0.30 0.00 0.90 1.50 0.30 2.80 0.30 0.10 0.20 0.90 0.00 2.00
NodePiece [7] 0.70 0.20 1.80 2.10 0.30 5.20 0.80 0.20 1.30 5.30 1.90 12.20
NeuralLP [48] 0.90 0.50 1.60 2.00 0.40 5.40 2.50 0.70 5.40 6.80 4.60 10.40
DRUM [36] 1.00 0.40 1.90 2.00 0.70 4.30 1.70 0.20 4.60 6.40 3.50 11.60
BLP [5] 1.20 0.30 2.50 4.30 1.60 8.90 4.10 1.30 9.20 12.50 5.50 28.30
QBLP [1] 1.20 0.30 2.50 4.40 1.60 9.10 3.50 1.10 8.00 11.60 4.20 29.40
NBFNet [53] 1.40 0.50 2.60 7.20 2.80 17.20 6.20 3.60 10.50 15.40 9.20 30.10
RED-GNN [50] 9.60 7.00 13.60 17.20 11.00 29.00 5.80 3.30 9.30 17.00 11.10 26.30
INGRAM [18] 10.70 7.20 16.90 24.70 17.90 36.20 6.80 3.40 13.50 18.60 12.40 30.90
FAGA 29.56 16.66 54.96 26.01 13.16 54.37 22.03 12.50 39.50 23.73 12.60 46.42
MethodsFB-100 FB-75 FB-50 FB-25
MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10
CompGCN [42] 1.50 0.80 2.50 1.30 0.00 2.60 0.40 0.20 0.60 0.30 0.00 0.40
NodePiece [7] 0.60 0.10 0.90 1.60 0.70 2.90 2.10 0.60 4.80 4.40 1.10 11.40
NeuralLP [48] 2.60 0.70 5.70 5.60 3.00 9.90 8.80 4.30 18.40 16.40 9.80 30.90
DRUM [36] 3.40 1.10 7.70 6.50 3.40 12.10 10.10 6.10 19.10 17.50 10.90 32.00
BLP [5] 1.70 0.40 3.50 4.70 2.40 8.50 7.80 3.70 15.60 10.70 5.30 21.20
QBLP [1] 1.30 0.30 2.60 4.10 1.70 8.40 7.10 3.00 14.70 10.40 4.30 22.60
NBFNet [53] 7.20 2.60 15.40 8.90 4.80 16.60 13.00 7.10 25.90 22.40 13.70 41.00
RED-GNN [50] 12.10 5.30 26.30 10.70 5.70 20.10 12.90 7.20 25.10 14.50 7.70 28.40
INGRAM [18] 22.30 14.60 37.10 18.90 11.90 32.50 11.70 6.70 21.80 13.30 6.70 27.10
FAGA 31.41 18.25 58.48 34.12 23.10 56.29 47.88 36.09 70.60 37.73 25.68 64.38
•The results of baselines are from INGRAM.
5.3 Generalization Capability Analysis (RQ1)
To explore model generalization capability to both unknown entities
and unknown relations, we conduct experiments on 12 benchmarks
extracted in INGRAM [ 18]. We report the experimental results in
Table 1. We observe that FAGA consistently outperforms other
methods across all datasets, with average improvements about 16%,
11%, and 27% in MRR, Hits@1, and Hits@10, respectively. This
improvement is most notable in the most challenging datasets, such
as NL-100 and WK-100, which aligns with expectations since all
relations and nodes in these datasets are unseen during training.
This highlights a significant boost in our method’s generalization
capabilities. In datasets where some relations have been previouslyseen, other methods like INGRAM and RED-GNN also perform
well, suggesting they may rely on prior knowledge of relations for
prediction.
5.4 Performance on Inductive Benchmarks
(RQ2)
Another prevalent inductive setting involves scenarios where only
nodes are unseen, while edges have been previously observed. To
demonstrate our model’s continued robust reasoning capabilities
under these conditions, we compare the proposed method against
existing inductive baselines on a well-established benchmark NELL-
995. Three rule-based methods, i.e., Neural LP [ 48], DRUM [ 36], and
 
1616KDD ’24, August 25–29, 2024, Barcelona, Spain Mei Li, Xiaoguang Liu, Hua Ji, and Shuangjia Zheng
Table 2: Comparison results on AUC-PR (AP) and Hits@10 (H10)
over four benchmarks of the dataset NELL-995.
Metho
dsv1 v2
v3 v4
AP
H10 AP
H10 AP
H10 AP
H10
NeuralLP 64.66
40.78 83.61
78.73 87.58
82.71 85.69
80.58
DRUM 59.86
19.42 83.99
78.55 87.71
82.71 85.94
80.58
RuleN 84.99
53.5 88.4
81.75 87.2
77.26 80.52
61.35
GraIL 86.05 59.5 92.62
93.25 93.34
91.41 87.50 73.19
CoMPILE 80.16
58.38 95.88 93.87 96.08 92.77 85.48
75.19
TACT 77.54
51.5 93.3
91.49 92.53
92.46 85.25
72.98
RMPI 77.89
60.50 94.31
93.49 95.89
95.30 72.34
66.42
F
AGA 87.35
65.00 96.47
96.00 97.15
96.54 95.61
92.54
•The
results of baselines are from GraIL and RMPI.
RuleN [ 28], and four GNN-based methods, i.e., GraIL [ 39], CoM-
PILE [ 26], TACT [ 3], and RMPI [ 10] are included. We report the
comparison results on AUC-PR and Hits@10 in Table 2. We find
that our proposed method significantly outperforms all compari-
son methods on dataset NELL-995 in both metrics of AUC-PR and
Hits@10. In particular, compared to the best results of its base-
lines, it improves by 8.11% on AUP-PR and 11.96% on Hits@10 in
dataset NELL-995-v4. The promising results demonstrate the effec-
tiveness of our method in exploiting complex structural patterns
for inductive relation prediction.
In fact, v2 (88 relations, 2,564 nodes 10,109 links, 1.747e-5) and
v3 (142 relations, 4,647 nodes, 20,117 links, 6.56e-6) of the NELL-
995 are much sparser compared to v4 (77 relations, 2,092 nodes,
9,289 links, 2.756e-5), making the subgraphs extracted from v4 are
rather complex for subgraph-based learning methods. Notably, the
rule-based methods, such as NeuralLP and DRUM, are rather robust
and with strong generalization capability, and show consistent
performance on v4. FAGA demonstrates the ability to identify causal
semantic relationships that are pivotal for decision making, thereby
enhancing its capacity for generalization. Hence, the remarkable
and consistent results of FAGA on v4 are reasonable.
Table 3: Ablation studies on NL-25 and NL-100.
Metho
dsNL-25 NL-100
MRR
Hits@1 Hits@10 MRR
Hits@1 Hits@10
F
AGA 55.02 40.42 81.58 58.77
43.38 89.22
FAGA-v1 54.26 41.33 80.17 55.82
38.97 91.42
FAGA-v2 54.06
38.84 81.18 57.83
40.54 93.13
FAGA w/o RP 53.60
38.08 75.83 50.93
36.76 81.53
FAGA w KL 51.00
37.61 76.08 57.95
42.37 90.16
AE 48.85
36.16 73.76 53.20
36.95 88.65
DAE 52.52
37.84 80.24 55.93
39.34 89.16
FAGA-base 54.65
35.77 71.11 51.01
38.15 74.46
5.5 Ablation Study (RQ3)
In this section, we investigate the impact of each component of
FAGA for inductive relation reasoning, including graph augmen-
tation strategies (i.e., DropEdge (DE) and DropMP (DP)), feature
fusion metrics (i.e., AdaIN (AD) and Mixup (MP)), and the reparam-
eterization trick (RP). We denote FAGA as the model that uses DE,
MP, and RP, FAGA-v1 that uses DE, AD and RP, and FAGA-v2 thatuses DP, MP, and RP. Additionally, FAGA-base is a non-causality
model that extracts a subgraph of a triplet and then feeds the sub-
graph into the encoder followed by an MLP for prediction. Specifi-
cally, it includes the subgraph extraction, the encoder and the MLP
in the predictor shown in Figure 1. We summarize the experimental
results on MRR, Hits@1, and Hits@10 in Table 3.
Impact of graph augmentation strategies. We observe that
the methods (FAGA and FAGA-v1) using DropEdge to generate
augmented subgraphs achieve competitive results to FAGA-v2 that
uses DropMP, suggesting both these two augmentation techniques
are effective and contribute to model performance improvement.
Impact of feature fusion strategies. Comparing FAGA to
FAGA-v1, we find that methods using these two feature fusion
strategies achieve comparable results, indicating both of these two
strategies are feasible.
Impact of the reparameterization trick. Comparing FAGA to
FAGA w/o RP, we observe that the employment of the reparameter-
ization trick contributes to the improvement of model performance,
where the generated variations augment the hidden feature space.
Comparison to AE/VAE. We further explore the connections
of FAGA to (denoising) autoencoder ((D)AE) [ 16] and variational
autoencoder (VAE) models. FAGA w KL introduces the Kullback-
Leibler (KL) divergence term to the final loss. It does not outperform
our current design, suggesting that the KL divergence term does
not offer much help, and it is acceptable not to use it. AE model
retains the clean subgraph and excludes the fusion module, whereas
the DAE model maintains the noisy subgraph and also omits the
fusion module. The comparative results indicate that both models
yield suboptimal outcomes relative to FAGA, indicating that these
parts work synergistically to achieve the causal inference.
Impact of the edge dropout rate. We also investigate the model
sensitivity to different edge dropout rates by varying the value to
∈{0.1,0.2,0.3,0.4,0.5,0.6}. The results on MRR and Hits@10 over
four benchmarks of NELL-995 are illustrated in Figure 2. We observe
that the results on MRR and Hits@10 are rather stable with the
increase of the edge dropout rate over all benchmarks. The results
show that the proposed method are robust to the parameter of edge
dropout rate. This is contributed to that the final prediction is made
on the refined subgraphs which are assumed to be the causality for
decision making.
0.1 0.2 0.3 0.4 0.5 0.65060708090100Hits@10 (%)
0.1 0.2 0.3 0.4 0.5 0.6406080100MRR (%)
v1 v2 v3 v4
Figure 2: Model sensitivity with different edge dropout rates on the
dataset NELL-995.
Time comparison. In Table 4, we compare FAGA to CoMPILE
and GraIL in terms of time consumption on average in dealing
with a single triplet at per training epoch, where the comparison
 
1617Causal Subgraph Learning for Generalizable Inductive Relation Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
methods are subgraph-based relation reasoning methods. We see
that FAGA takes more time in comparison to GraIL and CoMPILE
due to the introduction of the graph reconstruction module.
Table 4: Time comparison
Methods GraIL CoMPILE FAGA
Time (Seconds) 0.0489 0.0618 0.0711
3
05
1
2
4Agent participate 
in event
Raw Reconstructed3
0
1
2
4
Figure 3: Visualization of a subgraph and its corresponding recon-
structed counterpart, where nodes 0 and 1 are target nodes, edges
(marked in yellow) are removed after reconstruction.
5.6 Visualization and Analysis (RQ4)
To offer valuable insights into the model’s decision-making process,
in Figure 3, we present the visualization of a subgraph extracted
within 2 hops in the KG of NELL-995 and its corresponding recon-
structed counterpart. Here, for clarity, nodes are labeled by their
indexes in the subgraph and nodes 0 and 1 are target nodes. Notice
that, the reconstructed subgraph shares the same node indexes
as its raw subgraph. In Figure 3, the model predicts whether the
relation ‘organization also known as ’ holds between target nodes 0
(company: new_york ) and 1 (sportsteam: state_university ). We ob-
serve that some edges (marked in yellow) in the raw subgraph are
removed after reconstruction, for example, the model suggests a
removal of edges between nodes 3 and 5, and between nodes 0 and
5, which is reasonable as the ‘Agent participate in event ’ relation
acts as a redundant information during inference. These visualiza-
tions again prove that our proposed method is capable of capturing
the causal structure of a triplet and meanwhile removing spurious
correlations.
6 CONCLUSION
In this work, we inductively predict missing links in KGs from the
causality perspective. In particular, we address this problem via
the front-door adjustment where semantics features of subgraphs
essential to relation prediction are treated as the mediator vari-
able. To estimate the front-door formula, we leverage the graph
data augmentation mechanism to generate augmented subgraphs
and further propose a fusion module and a decoder to enable the
combination of the mediator and the generated subgraphs. Weevaluate the proposed method on a series of widely used bench-
marks, and experimental results demonstrate the superiority of our
method in inductive relation prediction. Despite good performance,
FAGA faces two limitations. Firstly, the integration of the graph
reconstruction module results in longer training time for the model.
Secondly, FAGA struggles to provide explanations for triplets asso-
ciated with empty subgraphs. Future research could enhance the
capabilities of models like FAGA by incorporating a broader range
of causal inference methods, including counterfactual approaches,
to address these limitations.
ACKNOWLEDGMENTS
The authors acknowledge support from the National Natural Sci-
ence Foundation of China (62272253, 62272252), and the Funda-
mental Research Funds for Central Universities. S. Z. acknowledges
funding from the Baidu scholarship and RayWu Angel Fund.
REFERENCES
[1]Mehdi Ali, Max Berrendorf, Mikhail Galkin, Veronika Thost, Tengfei Ma, Volker
Tresp, and Jens Lehmann. 2021. Improving inductive link prediction using hyper-
relational facts. In ISWC. 74–92.
[2]Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. In NeurIPS, Vol. 26.
[3]Jiajun Chen, Huarui He, Feng Wu, and Jie Wang. 2021. Topology-aware corre-
lations between relations for inductive link prediction in knowledge graphs. In
AAAI, Vol. 35. 6271–6278.
[4]William W Cohen. 2016. Tensorlog: A differentiable deductive database. arXiv
preprint arXiv:1605.06523 (2016).
[5]Daniel Daza, Michael Cochez, and Paul Groth. 2021. Inductive entity representa-
tions from text via link prediction. In WWW. 798–808.
[6]Luis Antonio Galárraga, Christina Teflioudi, Katja Hose, and Fabian Suchanek.
2013. AMIE: association rule mining under incomplete evidence in ontological
knowledge bases. In WWW. 413–422.
[7]Mikhail Galkin, Etienne Denis, Jiapeng Wu, and William L Hamilton. 2021. Node-
piece: Compositional and parameter-efficient representations of large knowledge
graphs. In ICLR.
[8]Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan
Quan, Jianxin Chang, Depeng Jin, Xiangnan He, et al .2023. A survey of graph
neural networks for recommender systems: Challenges, methods, and directions.
ACM Transactions on Recommender Systems 1, 1 (2023), 1–51.
[9]Min Gao, Jian-Yu Li, Chun-Hua Chen, Yun Li, Jun Zhang, and Zhi-Hui Zhan.
2023. Enhanced multi-task learning and knowledge graph-based recommender
system. IEEE Transactions on Knowledge and Data Engineering (2023).
[10] Yuxia Geng, Jiaoyan Chen, Jeff Z Pan, Mingyang Chen, Song Jiang, Wen Zhang,
and Huajun Chen. 2023. Relational message passing for fully inductive knowledge
graph completion. In ICDE. 1221–1233.
[11] Genet Asefa Gesese, Harald Sack, and Mehwish Alam. 2022. RAILD: Towards
leveraging relation features for inductive link prediction In knowledge graphs.
InIJCKG. 82–90.
[12] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. 2022. G-mixup: Graph
data augmentation for graph classification. In ICML. 8230–8248.
[13] Yan Han, Peihao Wang, Souvik Kundu, Ying Ding, and Zhangyang Wang. 2023.
Vision HGNN: An image is more than a graph of nodes. In ICCV. 19878–19888.
[14] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012).
[15] Xun Huang and Serge Belongie. 2017. Arbitrary style transfer in real-time with
adaptive instance normalization. In ICCV. 1501–1510.
[16] Daniel Im Im, Sungjin Ahn, Roland Memisevic, and Yoshua Bengio. 2017. De-
noising Criterion for Variational Auto-Encoding Framework. In AAAI, Vol. 31.
[17] Hyunchai Jeong, Jin Tian, and Elias Bareinboim. 2022. Finding and listing front-
door adjustment sets. NeurIPS 35 (2022), 33173–33185.
[18] Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang. 2023. InGram: Induc-
tive knowledge graph embedding via relation graphs. In ICML. 18796–18809.
[19] Namkyeong Lee, Kanghoon Yoon, Gyoung S. Na, Sein Kim, and Chanyoung Park.
2023. Shift-robust molecular relational learning with causal substructure. In KDD.
1200––1212.
[20] Mei Li, Xiangrui Cai, Sihan Xu, and Hua Ji. 2023. Metapath-aggregated heteroge-
neous graph neural network for drug–target interaction prediction. Briefings in
Bioinformatics 24, 1 (2023), bbac578.
 
1618KDD ’24, August 25–29, 2024, Barcelona, Spain Mei Li, Xiaoguang Liu, Hua Ji, and Shuangjia Zheng
[21] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang
Zhou, Xinwang Liu, and Fuchun Sun. 2022. A survey of knowledge graph
reasoning on graph types: Static, dynamic, and multimodal. arXiv preprint
arXiv:2212.05767 (2022).
[22] Ke Liang, Lingyuan Meng, Sihang Zhou, Wenxuan Tu, Siwei Wang, Yue Liu,
Meng Liu, Long Zhao, Xiangjun Dong, and Xinwang Liu. 2024. MINES: Message
Intercommunication for Inductive Relation Reasoning over Neighbor-Enhanced
Subgraphs. In AAAI, Vol. 38. 10645–10653.
[23] Qika Lin, Jun Liu, Fangzhi Xu, Yudai Pan, Yifan Zhu, Lingling Zhang, and Tianzhe
Zhao. 2022. Incorporating context graph with logical reasoning for inductive
relation prediction. In SIGIR. 893–903.
[24] Shuwen Liu, Bernardo Grau, Ian Horrocks, and Egor Kostylev. 2021. Indigo:
Gnn-based inductive knowledge graph completion using pair-wise encoding. In
NeurIPS, Vol. 34. 2034–2045.
[25] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip.
2022. Graph self-supervised learning: A survey. IEEE Transactions on Knowledge
and Data Engineering 35, 6 (2022), 5879–5900.
[26] Sijie Mai, Shuangjia Zheng, Yuedong Yang, and Haifeng Hu. 2021. Communicative
message passing for inductive relation reasoning. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 35. 4294–4302.
[27] Christian Meilicke, Melisachew Wudage Chekol, Daniel Ruffinelli, and Heiner
Stuckenschmidt. 2019. Anytime bottom-up rule learning for knowledge graph
completion.. In IJCAI. 3137–3143.
[28] Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Ruffinelli, Rainer Gemulla,
and Heiner Stuckenschmidt. 2018. Fine-grained evaluation of rule-and
embedding-based systems for knowledge graph completion. In ISWC. 3–20.
[29] Deepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. 2020. Learn-
ing attention-based embeddings for relation prediction in knowledge graphs.
(2020), 4710—-4723.
[30] Toan Nguyen, Kien Do, Duc Thanh Nguyen, Bao Duong, and Thin Nguyen. 2023.
Causal inference via style transfer for out-of-distribution generalisation. In KDD.
1746–1757.
[31] Yudai Pan, Jun Liu, Lingling Zhang, Tianzhe Zhao, Qika Lin, Xin Hu, and Qianying
Wang. 2022. Inductive relation prediction with logical reasoning using contrastive
representations. In EMNLP. 4261–4274.
[32] Judea Pearl. 2009. Causal inference in statistics: An overview. Statistics Surveys 3
(2009), 96 – 146.
[33] Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal inference in
statistics: A primer. 2016. Internet Resource (2016).
[34] Jiahua Rao, Shuangjia Zheng, Sijie Mai, and Yuedong Yang. 2022. Communica-
tive subgraph representation learning for multi-relational inductive drug-gene
interaction prediction. In IJCAI. 3919–3925.
[35] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. Dropedge:
Towards deep graph convolutional networks on node classification. In ICLR.
[36] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang.
2019. Drum: End-to-end differentiable rule mining on knowledge graphs. In
NeurIPS, Vol. 32.
[37] Abhin Shah, Karthikeyan Shanmugam, and Murat Kocaoglu. 2023. Front-door ad-
justment beyond markov equivalence with limited graph knowledge. In NeurIPS.14–26.
[38] Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng Chua.
2022. Causal attention for interpretable and generalizable graph classification. In
KDD. 1696–1705.
[39] Komal Teru, Etienne Denis, and Will Hamilton. 2020. Inductive relation prediction
by subgraph reasoning. In ICML. 9448–9457.
[40] Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choud-
hury, and Michael Gamon. 2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP. 1499–1509.
[41] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume
Bouchard. 2016. Complex embeddings for simple link prediction. In ICML. 2071–
2080.
[42] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. 2019.
Composition-based multi-relational graph convolutional networks. In ICLR.
[43] Peng Wu, Haoxuan Li, Yuhao Deng, Wenjie Hu, Quanyu Dai, Zhenhua Dong, Jie
Sun, Rui Zhang, and Xiao-Hua Zhou. 2022. On the opportunity of causal learning
in recommendation systems: Foundation, estimation, prediction and challenges.
InIJCAI-22. 5646–5653.
[44] Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. 2022.
Self-supervised learning of graph neural networks: A unified review. IEEE trans-
actions on pattern analysis and machine intelligence 45, 2 (2022), 2412–2429.
[45] Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. Deeppath: A
reinforcement learning method for knowledge graph reasoning. In EMNLP.
[46] Xiaohan Xu, Peng Zhang, Yongquan He, Chengpeng Chao, and Chaoyang Yan.
2022. Subgraph neighboring relations infomax for inductive link prediction on
knowledge graphs. In IJCAI. 2341—-2347.
[47] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Em-
bedding entities and relations for learning and inference in knowledge bases. In
ICLR. 1–13.
[48] Fan Yang, Zhilin Yang, and William W Cohen. 2017. Differentiable learning of
logical rules for knowledge base reasoning. In NeurIPS, Vol. 30.
[49] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2018.
mixup: Beyond Empirical Risk Minimization. In ICLR.
[50] Yongqi Zhang and Quanming Yao. 2022. Knowledge graph reasoning with
relational digraph. In WWW. 912–924.
[51] Tong Zhao, Wei Jin, Yozen Liu, Yingheng Wang, Gang Liu, Stephan Günnemann,
Neil Shah, and Meng Jiang. 2022. Graph data augmentation for graph machine
learning: A survey. arXiv preprint arXiv:2202.08871 (2022).
[52] Zhilun Zhou, Yu Liu, Jingtao Ding, Depeng Jin, and Yong Li. 2023. Hierarchi-
cal knowledge graph learning enabled socioeconomic indicator prediction in
location-based social network. In WWW. 122–132.
[53] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. 2021.
Neural bellman-ford networks: A general graph neural network framework for
link prediction. NeurIPS 34 (2021), 29476–29490.
A DATASETS
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
 
1619Causal Subgraph Learning for Generalizable Inductive Relation Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 5: Datasets for inductive relation prediction
NL-100 NL-50 NL-100 NL-50
|E| |R| |T| |E| |R| |T| |E| |R| |T| |E| |R| |T|
G𝑡𝑟1,258 55 7,832 2,607 96 11,058 4,396 106 17,578 4,396 106 17,578
G𝑡𝑒1,709 53 3,964 1,578 116 3,031 2,335 119 4,294 2,146 120 3,717
WK-100 WK-75 WK-50 WK-25
|E| |R| |T| |E| |R| |T| |E| |R| |T| |E| |R| |T|
G𝑡𝑟9,784 67 49,875 6,853 52 28,741 12,022 72 82,481 12,659 47 41,873
G𝑡𝑒12,136 37 22,479 2,722 65 5,717 9,328 93 16,121 3,228 74 5,652
FB-100 FB-75 FB-50 FB-25
|E| |R| |T| |E| |R| |T| |E| |R| |T| |E| |R| |T|
G𝑡𝑟4,659 134 62,809 4,659 134 62,809 5,190 153 85,375 5,190 163 91,571
G𝑡𝑒2,624 77 11,645 2,792 186 15,528 4,445 205 19,394 4,097 216 28,579
 
1620