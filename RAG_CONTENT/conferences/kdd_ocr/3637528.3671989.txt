Scalable Rule Lists Learning with Sampling
Leonardo Pellegrina
Dept. of Information Engineering, University of Padova
Padova, Italy
leonardo.pellegrina@unipd.itFabio Vandin
Dept. of Information Engineering, University of Padova
Padova, Italy
fabio.vandin@unipd.it
Abstract
Learning interpretable models has become a major focus of ma-
chine learning research, given the increasing prominence of ma-
chine learning in socially important decision-making. Among in-
terpretable models, rule lists are among the best-known and easily
interpretable ones. However, finding optimal rule lists is compu-
tationally challenging, and current approaches are impractical for
large datasets.
We present a novel and scalable approach to learn nearly opti-
mal rule lists from large datasets. Our algorithm uses sampling to
efficiently obtain an approximation of the optimal rule list with rig-
orous guarantees on the quality of the approximation. In particular,
our algorithm guarantees to find a rule list with accuracy very close
to the optimal rule list when a rule list with high accuracy exists.
Our algorithm builds on the VC-dimension of rule lists, for which
we prove novel upper and lower bounds. Our experimental evalua-
tion on large datasets shows that our algorithm identifies nearly
optimal rule lists with a speed-up up to two orders of magnitude
over state-of-the-art exact approaches. Moreover, our algorithm is
as fast as, and sometimes faster than, recent heuristic approaches,
while reporting higher quality rule lists. In addition, the rules re-
ported by our algorithm are more similar to the rules in the optimal
rule list than the rules from heuristic approaches.
CCS Concepts
•Mathematics of computing →Probabilistic algorithms ;•
Information systems →Data mining; •Theory of compu-
tation→Sketching and sampling; Sample complexity and
generalization bounds.
Keywords
Rule list learning, Interpretable Machine Learning, VC-dimension,
Sample complexity bounds
ACM Reference Format:
Leonardo Pellegrina and Fabio Vandin. 2024. Scalable Rule Lists Learning
with Sampling. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.
3671989
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36719891 Introduction
Interpretability is one of the characteristics of machine learning
models that has become a major topic of research due to the ever
increasing impact of machine learning models in socially important
decision-making [ 22]. The goal is to build models that are easily
understood by humans, while achieving high predictive power, in
contrast to black-box models (e.g., large deep-learning models) that
are highly predictive but are not transparent nor interpretable.
Rule lists [ 21], and more in general rule-based models such as
decision trees, are among the best-known and easily interpretable
models. A rule list is a sequence of rules, and the prediction from
such a model is obtained by applying the first rule in the list whose
condition is satisfied for the given input.
Rule-based models often display predictive power comparable
to black-box models in several critical applications [ 22]. While ad-
vanced techniques have been recently developed to speed-up the
discovery of optimal rule lists [ 1,14,23,32], it remains a computa-
tionally challenging task, in particular for large datasets where dif-
ferentiable models (such as neural networks) can be easily learned
with gradient descent and its variants.
Fast heuristic methods for rule lists and other rule-based models
have been developed [ 7,30], but they do not provide guarantees in
terms of the accuracy of the rule list they report. In several appli-
cations identifying an optimal, or close to optimal rule, is crucial,
since the goal is to find models that are not only interpretable,
but also have high predictive power and can therefore be used as
interpretable alternatives to black-box models.
A natural solution to speed up the training on a large dataset,
and to obtain an approximation of the best rule list, is to reduce the
size of the training set, e.g., by only considering a small random
sample of the dataset. On the other hand, it is clear that there is
an intrinsic trade-off between the accuracy of this approximation
and the size of the random sample, i.e., the computational cost
incurred by the learning algorithm. This accuracy depends directly
on the fluctuations of the losses estimated on the sample w.r.t. the
losses measured on the entire dataset: a sample too small can be
analyzed quickly but may provide inaccurate estimates. Moreover, it
is possible that the best rule list, learned from a random sample, may
be qualitatively different, thus not representative, of the optimal
solution computed on the whole dataset.
Contributions. To address these challenges, we precisely quantify
the maximum deviation between the accuracies of rule lists that
are estimated on the random sample w.r.t. the respective exact
counterparts. As a consequence, we obtain sufficient sample sizes
that guarantee that an accurate rule list can be found from a small
random sample. We then propose SamRuLe (Sam pling for Rules
listLearning), a scalable approach to approximate optimal rule lists
from large datasets with rigorous guarantees on the quality of the
approximation.
 
2352
KDD ’24, August 25–29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
•SamRuLe is the first scalable approach to approximate optimal
rule lists from large datasets while providing rigorous guar-
antees on the accuracy of the reported rule list. In particular,
SamRuLe guarantees to find a rule list with accuracy very close
to the optimal one when a rule list with high accuracy exists.
As a result, SamRuLe makes it possible to assess the existence
of high accuracy rule lists for large datasets.
•SamRuLe uses sampling to efficiently obtain an approximation
of the optimal rule list with guarantees. We derive novel bounds
relating the accuracy of a rule lists in the sample with its accu-
racy in the whole dataset andwith the accuracy of the optimal
rule list. Our bounds build on the VC-dimension of rule lists,
for which we prove novel upper and lower bounds, and that
allow us to define sample sizes leading to the desired quality
guarantees.
•Our experimental evaluation shows that SamRuLe enables the
identification of nearly optimal rule lists from large datasets,
with a speed-up of up to two orders of magnitude over state-of-
the-art exact approaches. Moreover, SamRuLe is as fast as, or
faster than, recent heuristic approaches, while reporting higher
quality rule lists, for which it can also certify their quality with
respect to the optimal rule list. The rules reported by SamRuLe
are also more similar to the rules in the optimal rule list than
the rules from heuristic approaches.
2 Related Works
We now discuss the works most related to ours. We focus on meth-
ods to learn rule lists, in particular sparse rule lists. Sparsity is
a crucial requirement for the interpretability of models. For an
overview on the topic of interpretability in machine learning, we
point the interested reader to the survey from Rudin et al. [22].
Finding optimal sparse rule lists is NP-hard, and several methods
have been proposed to learn sparse rule lists. Such methods fall
under two categories: exact methods, that guarantee to find the
optimal rule list (i.e., with highest accuracy), and heuristic methods,
that are faster but provide no guarantees on the accuracy of the
reported rule list w.r.t. the optimal.
In the category of exact methods, Angelino et al. [ 1] propose
CORELS, a branch-and-bound algorithm that achieves several or-
ders of magnitude speedup in time and a significant reduction of
memory consumption, by leveraging algorithmic bounds, efficient
data structures, and computational reuse. Rudin and Ertekin [ 23]
develop a mathematical programming approach to building rule
lists. Okajima and Sadamasa [ 14] define a continuous relaxed ver-
sion of a rule list, and propose an algorithm that optimizes rule lists
based on such continuous relaxed versions. Yu et al. [ 32] propose
a SAT-based approach to find optimal rule sets and lists. While
highly optimized, exact methods such as the aforementioned ones
only apply to problems of moderate size, and do not scale to large
datasets.
In the category of heuristic methods, Rivest [ 21] proposed greedy
splitting techniques, also used in subsequent heuristic algorithms
for learning decision trees [ 5,19]. RIPPER [ 7] builds rule sets in a
greedy fashion, and a similar greedy strategy has been used for find-
ing sets of robust rules in terms of minimum description length [ 9].Other works considered different problems and classification mod-
els, such as probabilistic rules for multiclass classification [ 18], and,
more recently, rule lists with preferred variables [ 15], sets of locally
optimal rules [ 11], and multi-label rule sets [ 6]. Yang et al. [ 30] uses,
instead, a Bayesian approach with Monte-Carlo search to explore
the rule list solution space. All such approaches focus on improving
efficiency with respect to exact approaches, but do not provide
guarantees on the quality of the reported rule lists with respect to
the optimal one. In several challenging instances, the gap with the
optimal solution may be substantial, resulting in a predictive model
with suboptimal performance.
A different line of research, related to the identification of almost-
optimal models, is provided by the exploration of the Rashomon
set, that is the set of almost-optimal models for a machine learning
problem [ 4,10,26]. Works in this area have focused on the explo-
ration or full enumeration of the Rashomon set, including the case
of rule-based models [ 29], for example to study variable impor-
tance in well-performing models [ 8]. While some of our theoretical
results may be useful for such applications as well, our focus is
not the exploration or enumeration of allalmost optimal models,
but rather on efficiently finding one nearly optimal model while
providing guarantees on its quality.
Our algorithm builds on novel upper bounds to the VC-
dimension [ 28] of rule lists, a fundamental combinatorial measure of
the complexity of classification models. We combine these bounds
with sharp concentration inequalities to bound the maximum devi-
ation between the prediction accuracy of rule lists from a random
sample w.r.t. the entire dataset. Our VC-dimension upper bounds
generalize previous results that can be derived from rule-based
classification models [ 24], as they provide a more granular depen-
dence on the rule list parameters compared to known results (see
Section 4.1 for a detailed discussion). We establish the tightness of
our analysis by proving almost matching lower bounds, that signif-
icantly improves over the best known results from Yildiz [ 31]. The
VC-dimension has been used to develop generalization bounds and
sampling algorithms for other data mining tasks, including associa-
tion rules mining [ 20] and sequential pattern mining [ 25]. To the
best of our knowledge, our work is the first to use VC-dimension
in designing efficient sampling approaches for learning rule lists.
3 Preliminaries
We define a datasetDas a collectionD={(𝑠1,𝑡1),...,(𝑠𝑛,𝑡𝑛)}of𝑛
training instances, where each training instance is a pair (𝑠,𝑡)∈D
composed by a set 𝑠of𝑑features, and a categorical label 𝑡. We
denote with 𝑥𝑖the𝑖-th feature, for 1≤𝑖≤𝑑. For simplicity, in this
work we will focus on datasets with binary features, 𝑥𝑖∈{0,1}
for1≤𝑖≤𝑑, and binary labels 𝑡∈{0,1}, but most of our results
extend to the general case. We use D𝑠to denote the collection of
the𝑑features of each training instance, ignoring the labels, that is
D𝑠={𝑠1,𝑠2,...,𝑠𝑛}.
We define a rule list 𝑅=[𝑟1,𝑟2,...,𝑟𝑘,𝑟⊺]of length|𝑅|=𝑘as a
sequence of 𝑘≥0rules𝑟𝑖,1≤𝑖≤𝑘, plus a default rule 𝑟⊺. Each
rule𝑟∈𝑅is defined as a pair 𝑟=(𝑐,𝑝)where𝑐is a condition on a
subset of the 𝑑features of the dataset, and 𝑝∈{0,1}is a prediction
of the label. Similarly to previous rule-based models [ 1,23,30], we
only consider conditions composed by conjunctions of monotone
 
2353Scalable Rule Lists Learning with Sampling KDD ’24, August 25–29, 2024, Barcelona, Spain
variables of the type 𝑐=Ó
𝑖“𝑥𝑖=1”, i.e., we do not consider the
negation of binary features (e.g., “𝑥𝑖=0”). Note that negations
can be included easily by considering additional binary features.
The default rule 𝑟⊺=(𝑐⊺,𝑝)is composed by the condition 𝑐⊺that
is always true. Given an instance (𝑠,𝑡)∈D , the predicted label
𝑃(𝑅,𝑠)by the rule list 𝑅on𝑠is computed as the predicted label 𝑝
of the first rule 𝑟whose condition 𝑐is satisfied by 𝑠; Figure 1 shows
an example of a dataset with 𝑛=5training instances over 𝑑=4
features, and a rule list 𝑅of length𝑘=3with conjunctions with at
most𝑧=2terms.
D𝑥1𝑥2𝑥3𝑥4𝑡
𝑠1 0 1 0 0 1
𝑠2 1 1 0 0 1
𝑠3 0 0 1 1 1
𝑠4 0 0 0 0 0
𝑠5 1 0 1 1 0
(a)if𝑥2=1→1
else
if𝑥1=1∧𝑥3=1→0
else
if𝑥4=1→1
else→0
(b)
Figure 1: (a): example of a dataset Dwith𝑛=5instances and
𝑑=4features. (b): example of a rule list 𝑅with length 𝑘=3
with conjunctions with at most 𝑧=2terms. The rule list 𝑅
perfectly classifies the instances of D.
The regularized binary loss ℓ(𝑅,D)of a rule list 𝑅is defined, for
some constant 𝛼≥0, as
ℓ(𝑅,D)=1
𝑛∑︁
(𝑠,𝑡)∈D1[𝑃(𝑅,𝑠)≠𝑡]+𝛼|𝑅|.
The use of the regularized loss is common practice in rule list
learning [ 1], allowing to prefer shorter (i.e., simpler) rule lists over
longer ones when their accuracy is similar, as quantified by the
parameter𝛼.
DefineR𝑧
𝑘as the set of rule lists with length ≤𝑘, where each
rule in𝑅is given by the conjunction of at most 𝑧terms. The rule
list learning problem is to identify a rule 𝑅★such that
𝑅★=arg min
𝑅∈R𝑧
𝑘{ℓ(𝑅,D)}.
The values of 𝑘and𝑧are usually fairly small, since large values
would compromise the interpretability of the rule list [ 22]. Finding
𝑅★is NP-hard, and to develop efficient algorithms one has to resort
to approximations.
Our goal is to compute, for given accuracy parameters 𝜀,𝜃∈
(0,1], a rule list ˜𝑅that provides an(𝜀,𝜃)-approximation of the
optimal rule list 𝑅★, defined as follows.
Definition 3.1. A rule list ˜𝑅provides an(𝜀,𝜃)-approximation of
the optimal rule list 𝑅★if it holds
ℓ(˜𝑅,D)≤ℓ(𝑅★,D)+𝜀max{ℓ(𝑅★,D),𝜃}.
Our definition of(𝜀,𝜃)-approximation is motivated by the fact
that the optimal loss ℓ(𝑅★,D)is unknown a priori, and often ex-
tremely expensive to compute or even estimate from large datasets.
Therefore, we need to design approximation guarantees that are
sharp in all situations, i.e., for the all possible values of the optimalloss. The(𝜀,𝜃)-approximation of Definition 3.1 allows to interpo-
late between an additive approximation, with small absolute error
𝜀𝜃, when the optimal loss ℓ(𝑅★,D)is small (i.e.,≤𝜃), and a relative
approximation (with parameter 𝜀) when the optimal loss ℓ(𝑅★,D)
is large (i.e., >𝜃). This flexible design avoids statistical bottlenecks
that are incurred when the loss of the best model is large, a well
known issue in statistical learning theory [ 3]. Taking this effect
into account allows SamRuLe to be extremely efficient in terms of
sample sizes.
As we will formalize in Section 4, the main goal of SamRuLe is
to compute an(𝜀,𝜃)-approximation of the optimal rule list from a
randomly drawn samples with high probability.
4SamRuLe Algorithm
As introduced in previous sections, the main idea of our approach
is to compute an approximation of the best rule list performing
the analysis of a small random sample, instead of processing a
large dataset, which may be extremely expensive and unfeasible in
practice. In this section we formally define our algorithm.
Define a sampleSas a collection of 𝑚instances of the dataset
Dtaken uniformly and independently at random. The regularized
loss of a rule list 𝑅computed on the sample Sis defined as
ℓ(𝑅,S)=1
𝑚∑︁
(𝑠,𝑡)∈S1[𝑃(𝑅,𝑠)≠𝑡]+𝛼|𝑅|.
Intuitively,ℓ(𝑅,S)provides an estimate ofℓ(𝑅,D). More precisely,
the loss of any rule 𝑅computed onSis an unbiased estimator of
the loss computed on D; in fact, it holds
E
S[ℓ(𝑅,S)]=ℓ(𝑅,D).
We remark, however, that the convergence of ℓ(𝑅,S)towards
ℓ(𝑅,D)in expectation is not sufficient to derive rigorous approxi-
mations for the best rule list; instead, it is necessary to take into
account the variance of the deviations, that indirectly depends the
complexity of the class of rule list models. Therefore, our approach is
based on properly bounding the deviations|ℓ(𝑅,S)−ℓ(𝑅,D)| with
high probability, in order to guarantee that the best rule identified
from the sampleSis a high-quality approximation of the optimal
solution fromD. To obtain a scalable method, we wish to identify
the smallest possible sample size that yields the above-mentioned
guarantees. We address this challenging problem combining ad-
vanced VC-dimension bounds and sharp concentration inequalities.
We now describe our algorithm SamRuLe. The input to SamRuLe
is: a datasetDof𝑚training instances over 𝑑features; the values
𝑘and𝑧, representing the maximum number of rules in a rule list
and the maximum number of conditions for each rule; constants
𝜀,𝜃∈(0,1], that are the values defining the quality of the (𝜀,𝜃)-
approximation (see Section 3); and a constant 𝛿∈(0,1], that defines
the required confidence, that is the probability that the output by
SamRuLe is a(𝜀,𝜃)-approximation. (Note that the set R𝑧
𝑘of rules
lists that can be produced in output by SamRuLe is defined byD,𝑘,
and𝑧.) Given the input, SamRuLe computes the quantity ˆ𝑚(R𝑧
𝑘,D),
defined as the minimum value of 𝑚≥1such that it holds
√︄
3𝜃ln(2
𝛿)
𝑚+vuut
2
𝜃+√︃
3𝜃ln(2
𝛿)
𝑚 𝜔+ln(2
𝛿)
𝑚+2 𝜔+ln(2
𝛿)
𝑚≤𝜀𝜃,
 
2354KDD ’24, August 25–29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
where𝜔=𝑘𝑧ln(2𝑒𝑑/𝑧)+2. Note that ˆ𝑚(R𝑧
𝑘,D)can be easily com-
puted (e.g., with a binary search over 𝑚). We also show analytical
bounds to ˆ𝑚(R𝑧
𝑘,D)in our analysis (Section 4.2).
SamRuLe than draws a sample Sofˆ𝑚(R𝑧
𝑘,D)instances taken
uniformly at random from D, and outputs ˜𝑅=arg min𝑅∈R𝑧
𝑘ℓ(𝑅,S)
by finding the optimal rule list on the sample S. Note that SamRuLe
can leverage any exact algorithm, such as CORELS [ 1], to find ˜𝑅
fromS.
In the following sections, we prove that SamRuLe pro-
vides an(𝜀,𝜃)-approximation of the optimal rule list 𝑅★=
arg min𝑅∈R𝑧
𝑘ℓ(𝑅,S)with probability≥1−𝛿. Our proof lever-
age novel bounds on the VC-dimension of rule lists, that we present
in Section 4.1, while the approximation guarantees from SamRuLe
are proved in Section 4.2.
4.1 Complexity of Rule Lists
In this section we provide analytical results on the complexity of
rule lists, by studying their VC-dimension [ 28]. Intuitively, the VC-
dimension measures the capacity of a class of predicion models of
predicting arbitrary labels assignments with perfect accuracy. As
discussed in previous sections, these bounds will be instrumental to
derive accurate approximation bounds and prove that the reported
rule by SamRuLe is an(𝜀,𝜃)-approximation.
We start by providing the main concepts related to VC-dimension
(see [12, 13, 27] for a more in depth introduction).
Given a datasetD={(𝑠1,𝑡1),...,(𝑠𝑛,𝑡𝑛)}, remember thatD𝑠=
{𝑠1,...,𝑠𝑛}. Given the setR𝑧
𝑘of rule lists with length ≤𝑘, where
each condition contains conjunctions with at most 𝑧terms, for
each rule list 𝑅∈R𝑧
𝑘we define the projection of𝑅on the dataset
Das𝑋(𝑅,D)={𝑠∈ D𝑠:𝑃(𝑅,𝑠)=1}. We define the range
space associated to the dataset Dand toR𝑧
𝑘as(D,˜R𝑧
𝑘), where
˜R𝑧
𝑘={𝑋(𝑅,D):𝑅∈R𝑧
𝑘}is the range set. Note that in general
|˜R𝑧
𝑘|can be smaller than |R𝑧
𝑘|, since two rule lists 𝑅1and𝑅2may
provide the same predictions for all instances in a dataset D, which
implies𝑋(𝑅1,D)=𝑋(𝑅2,D). The projection of the range set ˜R𝑧
𝑘
on a subsetD′⊆D of the datasetDis defined asP(˜R𝑧
𝑘,D′)=
{𝑋(𝑅,D′):𝑅∈R𝑧
𝑘}.
IfP(˜R𝑧
𝑘,D′)=2D′, i.e., the projection of ˜R𝑧
𝑘onD′produces all
possible dichotomies of the elements of D′, thenD′isshattered by
˜R𝑧
𝑘. The VC-dimension of the range space (D,˜R𝑧
𝑘)is the cardinality
of the largest subset of Dthat is shattered by ˜R𝑧
𝑘. In what follows
we will use𝑉𝐶(R𝑧
𝑘)to denote the VC-dimension of the range space
(D,˜R𝑧
𝑘)associated to the set of rule lists R𝑧
𝑘.
4.1.1 Upper bounds to the VC-dimension. We now prove refined
upper bounds to the VC-dimension of rule lists. We start by provid-
ing upper bounds to the growth function (the maximum cardinality
of distinct projections of a range set, see below) of rule lists R1
𝑘,
for which𝑧=1(i.e., each rule consists of a condition on exactly
1 feature). From such bound we derive an upper bound to the
VC-dimension of the range space associated to R1
𝑘, that we will
generalize to derive an upper bound to the VC-dimension of R𝑧
𝑘for
general values of 𝑧. Due to space constraints, some of the proofs
are in Appendix.Define the growth function Λ(R𝑧
𝑘,𝑚)of the rule listsR𝑧
𝑘as the
maximum number of distinct projections of rule lists in R𝑧
𝑘over
anydatasetDwith𝑚instances, that is:
Λ(R𝑧
𝑘,𝑚)= max
D′⊆D:|D′|=𝑚|P(˜R𝑧
𝑘,D′)|.
Theorem 4.1. It holds
Λ(R1
𝑘,𝑚)≤2+𝑘∑︁
𝑗=1"
2𝑗𝑗−1Ö
𝑖=0(𝑑−𝑖)#
.
Proof. For0≤𝑗≤𝑘, we define the set G𝑗of rule lists with
length exactly 𝑗asG𝑗={𝑅∈R1
𝑘:|𝑅|=𝑗}, and define ˜G𝑗=
{𝑋(𝑅,D):𝑅∈G𝑗}the range set forG𝑗, with ˜R1
𝑘=∪𝑗˜G𝑗. From
an union bound and Jensen’s inequality, it holds
Λ(R1
𝑘,𝑚)= max
D′⊆D:|D′|=𝑚|P(∪𝑗˜G𝑗,D′)|
≤ max
D′⊆D:|D′|=𝑚𝑘∑︁
𝑗=0|P( ˜G𝑗,D′)|≤𝑘∑︁
𝑗=0Λ(G𝑗,𝑚).
We proceed to bound each Λ(G𝑗,𝑚)separately, while excluding
rule lists𝑅∈ G𝑗that have the same projection, on any D′, of
other rules 𝑅′∈G𝑗′, for some𝑗′≠𝑗. First, considerG0; it holds
Λ(G0,𝑚)=2sinceG0contains𝑅1=[(𝑐⊺,1)]and𝑅2=[(𝑐⊺,0)].
Then, considerG𝑗for any 1≤𝑗≤𝑘, and let any rule list
𝑅∈ G𝑗, with𝑅=[(𝑐1,𝑝1),...,(𝑐𝑗,𝑝𝑗),(𝑐⊺,𝑝)]. There are two
possibilities: 𝑝𝑗=𝑝or𝑝𝑗≠𝑝. In the first case, denote the rule
𝑅′=[(𝑐1,𝑝1),...,(𝑐𝑗−1,𝑝𝑗−1),(𝑐⊺,𝑝)] ∈G𝑗−1. It is easy to ob-
serve that the projections 𝑋(𝑅,D′)and𝑋(𝑅′,D′)are equal, for
anyD′. Therefore, any rule list ∈ G𝑗with𝑝𝑗=𝑝can be ig-
nored from the upper bound to Λ(G𝑗,𝑚)as it is already covered
by at least one element of the set G𝑗−1. Then, consider a rule list
𝑅=[(𝑐1,𝑝1),...,(𝑐𝑗,𝑝𝑗),(𝑐⊺,𝑝)]∈G𝑗such that there exist two
indices 1≤𝑦<𝑖≤𝑗with𝑐𝑦=𝑐𝑖. We observe that the rule
𝑅′=[(𝑐1,𝑝1),...,(𝑐𝑖−1,𝑝𝑖−1),(𝑐𝑖+1,𝑝𝑖+1)...,(𝑐𝑗,𝑝𝑗),(𝑐⊺,𝑝)] ∈
G𝑗−1is equivalent to 𝑅, since𝑋(𝑅,D′)=𝑋(𝑅′,D′)for anyD′.
From these observations, the number of rules lists 𝑅∈G𝑗with
distinct projections on a dataset with 𝑚elements cannot exceed
2𝑗Î𝑗−1
𝑖=0(𝑑−𝑖), as we consider all possible ordered choices of 𝑗
distinct elements from the set of 𝑑features{𝑥1,...,𝑥𝑑}, and all 2𝑗
possible assignments of the predicted values {𝑝1,...,𝑝𝑗}∈{ 0,1}𝑗,
while𝑝is set to the unique value ≠𝑝𝑗. The sum of these bounds
yields the statement. □
A consequence of Theorem 4.1 is the following upper bound to
the VC-dimension 𝑉𝐶(R1
𝑘)ofR1
𝑘.
Corollary 4.2. The VC-dimension 𝑉𝐶(R1
𝑘)ofR1
𝑘is
𝑉𝐶(R1
𝑘)≤
𝑘log2(2𝑑)+2
.
Proof. We first note that the VC-dimension of a range set 𝑄is
at most𝑚if it holds log2(Λ(𝑄,𝑚))<𝑚. Using the upper bound to
 
2355Scalable Rule Lists Learning with Sampling KDD ’24, August 25–29, 2024, Barcelona, Spain
Λ(R1
𝑘,𝑚)from Theorem 4.1, we have the following:
log2(Λ(R1
𝑘,𝑚))≤ log2©­
«2+𝑘∑︁
𝑗=1"
2𝑗𝑗−1Ö
𝑖=0(𝑑−𝑖)#
ª®
¬
≤log2©­
«2+𝑘∑︁
𝑗=1(2𝑑)𝑗ª®
¬≤log2 
1+(2𝑑)𝑘+1
2𝑑−1!
≤log2 
(2𝑑)𝑘+1
2𝑑−1!
+log2(𝑒)2𝑑−1
(2𝑑)𝑘+1,
where in the last inequality we have used the fact that log2(1+𝑥)≤
log2(𝑥)+log2(𝑒)/𝑥,∀𝑥≥0. Then, we have
log2 
(2𝑑)𝑘+1
2𝑑−1!
+log2(𝑒)2𝑑−1
(2𝑑)𝑘+1
≤(𝑘+1)log2(2𝑑)−log2(2𝑑−1)+log2(𝑒)(2𝑑)−𝑘
=𝑘log2(2𝑑)+log22𝑑
2𝑑−1
+log2(𝑒)(2𝑑)−𝑘
≤𝑘log2(2𝑑)+1+log2(𝑒)(2𝑑)−𝑘<𝑘log2(2𝑑)+2,
proving the statement. □
Using the above upper bound to 𝑉𝐶(R1
𝑘), we prove the following
upper bound to the VC-dimension 𝑉𝐶(R𝑧
𝑘)ofR𝑧
𝑘.
Corollary 4.3. The VC-dimension 𝑉𝐶(R𝑧
𝑘)ofR𝑧
𝑘is
𝑉𝐶(R𝑧
𝑘)≤
𝑘𝑧log22𝑒𝑑
𝑧
+2
.
The bounds we derived in Corollary 4.2 and Corollary 4.3 scale
linearly with 𝑘and𝑧, and logarithmically with 𝑑. This implies that,
since we are interested in sparse models with small values of 𝑘and
𝑧, the resulting complexity will not be large. This guarantees that
accurate models will generalize well, and that the random sample
Sshould be representative of the dataset D. Moreover, the weak
dependence of the bounds on 𝑑allows SamRuLe to be extremely
effective even on datasets with large sets of features.
Rudin et al. [ 24] study generalization bounds for classes of binary
classification models built from sequences of association rules; this
general model includes rule lists. Moreover, they show that the
VC-dimension of the set of rule lists created using pre-mined rules
is exactly the size of the set of pre-mined rules, which is O(𝑑𝑧)(see
Theorem 3 in [ 24]). However, we remark that this result only holds
for unconstrained rule lists (e.g., without any bound 𝑘on their
length), and cannot be adapted to our setting. Our O(𝑘𝑧 log(𝑑/𝑧))
upper bound on the VC-dimension is more accurate, since it offers a
more granular dependence on the parameters defining the rule list
search space. More precisely, it explicitly depends on the maximum
number𝑘of rules in the list, the number of features 𝑑, and the
maximum number 𝑧of conditions in each rule, while the bound
O(𝑑𝑧)from [ 24] is not sensible to such constraints (as it assumes
𝑘=𝑑𝑧). Since in most cases 𝑘≪𝑑𝑧, our upper bounds are orders
of magnitude smaller.4.1.2 Lower bounds to the VC-dimension. In this section we prove
almost matching lower bounds to the VC-dimension of rule lists,
confirming the tightness of our analysis. Our first result involves a
lower bound to 𝑉𝐶(R1
𝑘).
Theorem 4.4. The VC-dimension 𝑉𝐶(R1
𝑘)ofR1
𝑘is
𝑉𝐶(R1
𝑘)≥
𝑘log2𝑑+𝑘
𝑘
.
We may observe that the upper bound O(𝑘 log(𝑑))given by
Corollary 4.2 is almost tight, since Theorem 4.4 implies that
𝑉𝐶(R1
𝑘)∈Ω(𝑘log(𝑑+𝑘
𝑘)). Reducing this gap is an interesting open
problem. We remark that the lower bound from Theorem 4.4 im-
proves the best known result, which provides a (weaker) lower
bound of rate Ω(𝑘+log(𝑑−𝑘))(see Theorem 3 of [31]).
We now prove a lower bound for the general case 𝑧≥1.
Theorem 4.5. The VC-dimension 𝑉𝐶(R𝑧
𝑘)ofR𝑧
𝑘is
𝑉𝐶(R𝑧
𝑘)≥
𝑘𝑧log2𝑑
𝑧𝑧√
𝑘
.
From Theorem 4.5 and Corollary 4.3 we conclude that 𝑉𝐶(R𝑧
𝑘)∈
O(𝑘𝑧 log(𝑑/𝑧))and𝑉𝐶(R𝑧
𝑘) ∈Ω(𝑘𝑧log(𝑑/𝑧𝑧√
𝑘)), which are al-
most matching. A further refinement of either the upper or lower
bound is an interesting direction for future research.
4.2 Approximation guarantees
In this section we prove sharp approximation bounds for the es-
timated losses ℓ(𝑅,S)of rules𝑅on a random sample Sw.r.t. the
lossesℓ(𝑅,D)on the datasetD, and derive sufficient sample sizes
to compute(𝜀,𝜃)-approximations for the optimal rule list 𝑅★. Due
to space constraints, we provide most of the proofs in the Appendix.
We define𝜔as𝜔=𝑘𝑧ln(2𝑒𝑑/𝑧)+2, a complexity parameter of our
bounds derived from the results of Section 4.1 (Corollary 4.3).
Theorem 4.6. LetSbe an i.i.d. random sample of size 𝑚≥1of
the datasetD. Then, for𝛿∈(0,1), the following inequalities hold
simultaneously with probability ≥1−𝛿:
ℓ(𝑅,D)≤ℓ(𝑅,S)+vt
2ℓ(𝑅,S)
𝜔+ln 2
𝛿
𝑚+2
𝜔+ln 2
𝛿
𝑚,∀𝑅∈R𝑧
𝑘,
ℓ(𝑅★,S)≤ℓ(𝑅★,D)+√︄
3ℓ(𝑅★,D)ln 2
𝛿
𝑚.
From the concentration bounds of Theorem 4.6 we conclude
that, with high probability, all estimates ℓ(𝑅,S)from the sampleS
provide accurate upper bounds to the losses ℓ(𝑅,D)in the dataset,
while the loss ℓ(𝑅★,S)of the optimal rule list strongly concentrates
to its expectation ℓ(𝑅★,D). We now combine these inequalities to
obtain an easy-to-check condition; this condition yields a sufficient
number of samples to obtain a high-quality approximation of the
optimal rule list 𝑅★from a random sample. Define ˆ𝑚(R𝑧
𝑘,D)as the
minimum value of 𝑚≥1such that it holds
√︄
3𝜃ln(2
𝛿)
𝑚+vuut
2
𝜃+√︃
3𝜃ln(2
𝛿)
𝑚 𝜔+ln(2
𝛿)
𝑚+2 𝜔+ln(2
𝛿)
𝑚≤𝜀𝜃.
(1)
 
2356KDD ’24, August 25–29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
We prove that a sample Sof size at least ˆ𝑚(R𝑧
𝑘,D)provides an
(𝜀,𝜃)-approximation of 𝑅★with high probability.
Theorem 4.7. LetSbe an i.i.d. random sample of size ≥
ˆ𝑚(R𝑧
𝑘,D)of the datasetD. The output ˜𝑅=arg min𝑅∈R𝑧
𝑘ℓ(𝑅,S)of
SamRuLe provides an(𝜀,𝜃)-approximation of 𝑅★with probability
≥1−𝛿.
Proof. Our proof is based on obtaining an upper bound to
ℓ(˜𝑅,D), where ˜𝑅is the rule list reported by SamRuLe, that is a
function of the optimal loss ℓ(𝑅★,D), and then showing that it is
sufficiently small to guarantee an (𝜀,𝜃)-approximation. First, we
observe that ℓ(𝑅★,D) ≤ℓ(˜𝑅,D), as𝑅★is one of the rule lists
with minimum loss. Therefore, from the first set of inequalities of
Theorem 4.6 setting 𝑅=˜𝑅, we have the upper bound to ℓ(𝑅★,D)
ℓ(𝑅★,D)≤ℓ(˜𝑅,S)+vt
2ℓ(˜𝑅,S)
𝜔+ln 2
𝛿
𝑚+2
𝜔+ln 2
𝛿
𝑚.(2)
We now prove that, since ˜𝑅is chosen as the rule list with minimum
loss on the sample, its estimated loss ℓ(˜𝑅,S)should be concentrated
towardsℓ(𝑅★,D). We haveℓ(˜𝑅,S) ≤ℓ(𝑅★,S), as ˜𝑅is one of
the rule list with minimum loss in S. Using the last inequality of
Theorem 4.6 it holds
ℓ(˜𝑅,S)≤ℓ(𝑅★,S)≤ℓ(𝑅★,D)+√︄
3ℓ(𝑅★,D)ln 2
𝛿
𝑚.(3)
Replacing all occurences of ℓ(˜𝑅,S)in(2)with its upper bound given
by the r.h.s. of (3), we obtain
ℓ(˜𝑅,D)≤ℓ(𝑅★,D)+𝑢(ℓ(𝑅★,D),𝑚), (4)
where the function 𝑢(𝑣,𝑚)is
𝑢(𝑣,𝑚)=√︄
3𝑣ln 2
𝛿
𝑚+vuuut
2
𝑣+√︂
3𝑣ln 2
𝛿
𝑚
𝜔+ln 2
𝛿
𝑚+2
𝜔+ln 2
𝛿
𝑚.
We now seek to identify the minimum 𝑚such thatℓ(˜𝑅,D)is guar-
anteed to be≤ℓ(𝑅★,D)+𝜀max{𝜃,ℓ(𝑅★,D)}. Note that, from the
derivations above, it is sufficient to verify that 𝑢(ℓ(𝑅★,D),𝑚)≤
𝜀max{𝜃,ℓ(𝑅★,D)}, for anyℓ(𝑅★,D)∈[ 0,1]. First, consider the
caseℓ(𝑅★,D)≤𝜃. We have that
𝑢(ℓ(𝑅★,D),𝑚)≤𝑢(𝜃,𝑚)≤𝜀𝜃
is true when 𝑚≥ˆ𝑚(R𝑧
𝑘,D)by definition of ˆ𝑚(R𝑧
𝑘,D)given in
the statement, since 𝑢(𝜃,𝑚)≤𝜀𝜃is guaranteed by (1). We now
consider the case ℓ(𝑅★,D)≥𝜃. We need to verify that
𝑢(𝑣,𝑚)≤𝜀𝑣,∀𝜃≤𝑣≤1. (5)
From the definition of ˆ𝑚(R𝑧
𝑘,D), we know that 𝑢(𝜃,𝑚)≤𝜀𝜃holds
for all𝑚≥ˆ𝑚(R𝑧
𝑘,D), therefore (5)is verified for 𝑣=𝜃. By taking
the derivative w.r.t. 𝑣of both sides of (5), it is simple to check that
the derivative of the r.h.s. is constant, while the derivative of the
l.h.s. is monotonically decreasing with 𝑣. Therefore, (5)also holds
for all𝜃<𝑣≤1, obtaining the statement. □
We note that, while ˆ𝑚(R𝑧
𝑘,D)can be easily computed (e.g., with
a binary search over 𝑚), from (1)it may not be simple to interpretits dependence on the several parameters. To do so, we prove the
following upper bound to ˆ𝑚(R𝑧
𝑘,D).
Theorem 4.8. It holds ˆ𝑚(R𝑧
𝑘,D)∈O 𝜔+ln(1
𝛿)
𝜀2𝜃.
Proof. First, we prove that ˆ𝑚(R𝑧
𝑘,D)≥ 3ln(2/𝛿)/𝜃. This fol-
lows easily from√︃
3𝜃ln(2/𝛿)
𝑚≤𝜀𝜃, i.e., only considering the left-
most term of (1). Then, it holds
√︄
3𝜃ln(2
𝛿)
𝑚+vuuut2
𝜃+√︃
3𝜃ln(2
𝛿)
𝑚 𝜔+ln(2
𝛿)
𝑚+2 𝜔+ln(2
𝛿)
𝑚
≤√︄
3𝜃ln(2
𝛿)
𝑚+√︄
4𝜃 𝜔+ln(2
𝛿)
𝑚+2 𝜔+ln(2
𝛿)
𝑚
≤√︄
14𝜃 𝜔+ln(2
𝛿)
𝑚+2 𝜔+ln(2
𝛿)
𝑚,
where the second-last inequality holds for all 𝑚≥3ln(2/𝛿)/𝜃. By
solving the quadratic inequality
√︁
14𝑚𝜃(𝜔+ln(2/𝛿))+2(𝜔+ln(2/𝛿))≤𝑚𝜀𝜃,
we observe that (1) holds for all 𝑚with
𝑚≥ 𝜔+ln(2
𝛿)√
14√
8𝜀+14
2𝜀2𝜃+2 𝜔+ln(2
𝛿)
𝜀𝜃+7 𝜔+ln(2
𝛿)
𝜀2𝜃.(6)
The r.h.s. of (6)is∈ O(𝜔+ln(1
𝛿)
𝜀2𝜃); moreover, it upper bounds
ˆ𝑚(R𝑧
𝑘,D), as(6)provides a value of 𝑚that make (1)true (not
necessarily the minimum), obtaining the statement. □
Interestingly, the sample size ˆ𝑚(R𝑧
𝑘,D)is completely indepen-
dent of the size 𝑛of the datasetD: it only depends on the parameters
𝜔=𝑘𝑧ln(2𝑒𝑑/𝑧)+2of the rule lists search space, and the desired
approximation accuracy 𝜀,𝜃and confidence 𝛿. This characteristic
allows SamRuLe to be applied to massive datasets, of arbitrarily
large size𝑛.
5 Experiments
This section presents the results of our experiments. The main
goal of our experimental evaluation is to test the scalability of
SamRuLe in analyzing large datasets compared to exact approaches
(Section 5.1). To do so, we measure the number of samples used by
SamRuLe to obtain an accurate approximation of the best rule, its
running time, and the accuracy of the reported rule list compared
to the optimal solution. Then, we also compare SamRuLe with
state-of-the-art heuristics for rule list training (Section 5.2), as such
methods, while not offering theoretical guarantees, may still provide
accurate rule lists in practice. Finally, we quantify the performance
ofSamRuLe under several settings of the rule list parameters 𝑧and
𝑘(Section 5.3).
Datasets. We tested SamRuLe on8benchmark datasets from
UCI1. We binarized the datasets containing countinous features
by considering 4thresholds at equally spaced quantiles: for each
countinous feature 𝑓and each threshold 𝑡, we created two binary
features “𝑓≥𝑡”and“𝑓<𝑡”. The statistics of the resulting binary
1https://archive.ics.uci.edu/
 
2357Scalable Rule Lists Learning with Sampling KDD ’24, August 25–29, 2024, Barcelona, Spain
datasets are described in Table 1. Since some of these datasets are
quite small, we replicate them 𝑟times, i.e., each training sample of
Dis copied𝑟times (see the column 𝑟of Table 1). Note that this pre-
processing allows to obtain larger datasets, all with approximately
106training instances, while preserving the rule list distribution
and search space structure, as the losses and covered instances of
all rule lists are the same on the original and replicated datasets.
Compared methods. Our main goal is to compare SamRuLe with
CORELS [1], the state-of-the-art exact method to identify the rule
list with minimum loss from a dataset D.SamRuLe, as described in
Section 4, generates the random sample Sand then runs an exact
algorithm to search for the rule list with minimum loss on S(using
the same settings and parameters). Note that by using CORELS as
exact algorithm within SamRuLe, all differences between SamRuLe
and CORELS are due to the use of sampling, i.e., there are no other
confounding factors in the comparison between the two methods.
We also compare SamRuLe with SBRL [ 30] and RIPPER [ 7], two
state-of-the-art heuristic approaches for rule list learning. SBRL
uses a scalable Monte-Carlo approach to approximately search for
accurate rule lists, while RIPPER leverages a greedy selection of
conditions. Our comparison is motivated by the fact that, even if
these methods do not provide guarantees in terms of solution qual-
ity, they are often the methods of choice for practitioners since they
obtain good solutions and are designed to scale to large datasets.
Rule Lists Parameters. To compare SamRuLe with exact and
heuristic methods (Sections 5.1 and 5.2), for each dataset we set the
parameter𝑘as shown in Table 1 and 𝑧=1. In Section 5.3 we evalu-
ate the impact of different choices of 𝑘and𝑧. Regarding SamRuLe,
we compute(𝜀,𝜃)-approximations for several combinations of the
parameters 𝜀and𝜃. We consider 𝜀∈{1,0.5,0.25}, and vary𝜃in
the interval[0.005,0.05]. For all experiments we fix 𝛿=0.05, as we
did not observe significant differences for other values (given the
exponential dependence of the bounds w.r.t. 𝛿).
Experimental setup. We implemented SamRuLe in Python. The
code and the scripts to reproduce all experiments are available
online2. For CORELS, we have used the implementation available
online3. We made minor modifications to the original implemen-
tation to limit its exploration to rule lists of length at most 𝑘(i.e.,
pruning all rules of length >𝑘instead of performing an unbounded
search). We made similar minor changes to the implementation
of SBLR4. We evaluated RIPPER with a recent efficient implemen-
tation5. All the code was compiled and executed on a machine
equipped with 2.30 GHz Intel Xeon CPU, 1TB of RAM, on Ubuntu
20.04. We repeated all experiments 10times, and report averages ±
stds over the 10repetitions.
5.1 Comparison to exact method
In this section we describe the experimental comparison between
SamRuLe and CORELS, the state-of-the-art method to identify the
optimal rule list 𝑅★with minimum loss. Our main goal is to eval-
uate the scalability of SamRuLe in terms of number of samples
and running time required to obtain an accurate approximation of
the best rule. Furthermore, we evaluate, both quantitatively and
2https://github.com/VandinLab/SamRuLe
3https://github.com/corels/corels
4https://github.com/Hongyuy/sbrlmod/
5https://github.com/imoscovitz/wittgensteinTable 1: Statistics of the datasets considered in our experi-
ments.𝑛is the number of transactions, 𝑑is the number of
binary features, 𝑟is the replication factor, 𝑘is the maximum
rule list length.
D 𝑛 𝑑 𝑟 𝑘
a9a 32561 124 100 4
adult 32561 175 100 4
bank 41188 152 100 4
higgs 11000000 531 1 3
ijcnn1 91701 35 100 8
mushroom 8124 118 200 5
phishing 11050 69 100 8
susy 5000000 179 1 5
qualitatively, the accuracy of the rule list found by SamRuLe w.r.t.
the optimal solution returned by CORELS. We ran both methods on
all datasets, setting 𝑧=1and𝑘as in Table 1. For SamRuLe, we vary
the parameters 𝜃and𝜀as described at the beginning of Section 5.
Figure 2 shows the results for these experiments. In Figure 2.(a)
and (b) we compare the running time of SamRuLe with the time
needed by CORELS on the adult and bank datasets. The results
for other datasets are very similar, and shown in Figure 6 (in the
Appendix). From these results, we can immediately conclude that
SamRuLe requires a small fraction of the time needed by CORELS,
with an improvement of up to 2orders of magnitude. The reason
for this significant speedup is that SamRuLe searches for the rule
with minimum loss on a small sample S, which is all cases orders
of magnitude smaller than the size of original dataset D. We show
the number of samples ˆ𝑚(R𝑧
𝑘,D)used by SamRuLe for all datasets
and for all parameters in Figure 5 (in the Appendix).
Then, we evaluated the quality of the solutions returned by Sam-
RuLe with the optimal rule list computed by CORELS. Figures 2.(c)
and (d) show the average deviations |ℓ(˜𝑅,D)−ℓ(𝑅★,D)|of the
loss of the rule list ˜𝑅found by SamRuLe w.r.t. the optimal rule list
𝑅★found by CORELS on the dataset D. To verify the validity of
SamRuLe’s theoretical guarantees, the plots also show the loss of
the optimal solution ℓ(𝑅★,D)found by CORELS (purple horizontal
line), and upper error bars ( +std) for the average deviations (see
Figures 6 and 7 in the Appendix for the plots for all datasets with
both bars). From these results, we observe that the rule lists re-
ported by SamRuLe are extremely accurate in terms of prediction
accuracy, since the deviations |ℓ(˜𝑅,D)−ℓ(𝑅★,D)|areorders of
magnitude smaller than ℓ(𝑅★,D), and smaller than guaranteed by
our theoretical analysis. This confirms that SamRuLe outputs ex-
tremely accurate rule lists, even when trained on random samples
that are orders of magnitude smaller than the entire dataset. Further-
more, it is likely that the guarantees of the (𝜀,𝜃)-approximations
hold for samples smaller than what guaranteed by our analysis;
this leaves significant opportunities for further improvements of
our algorithm. Then, we quantify the deviations between the loss
ℓ(˜𝑅,S)of˜𝑅estimated on the sample w.r.t. to the loss ℓ(˜𝑅,D)on
the dataset, i.e., the approximation error incurred by SamRuLe due
to analyzingSinstead of the entire dataset D. In Figure 8 we show
the average loss approximation error |ℓ(˜𝑅,D)−ℓ(˜𝑅,S)|, which is
 
2358KDD ’24, August 25–29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
102
105
104
Average deviations
Average deviations (susy)
SamRuLe (=0.25)
SamRuLe (=0.5)
SamRuLe (=1.0)
 CORELS
102
103104Running time (s)
Running time (adult)
(a)
102
102103104Running time (s)
Running time (bank) (b)
102
104
103
102
101
Average deviations
Average deviations (adult) (c)
102
104
103
102
101
Average deviations
Average deviations (bank) (d)
Figure 2: Performance and accuracy comparison between SamRuLe and CORELS on adult and bank datasets, for different
values of𝜀and𝜃. (a)-(b): running times of CORELS and SamRuLe. (c)-(d): average deviations |ℓ(˜𝑅,D)−ℓ(𝑅★,D)|of the loss of
the rule list ˜𝑅found by SamRuLe with the optimal rule list 𝑅★found by CORELS (purple horizontal line drawn at 𝑦=ℓ(𝑅★,D)).
The deviation plots only show upper errors bars at +std to improve readability. See Figures 6 and 7 in the Appendix for the
plots for all datasets and with ±std bars.
ifcapital-gain≥2·104→1
else if capital-loss≥1742→1
else
ifeducation-num <13→0
else if marital-status=married →1
else→0
(a)ifcapital-gain≥2·104→1
else if capital-loss≥1742→1
else
ifeducation-num <13→0
else if marital-status=married →1
else→0ifcapital-gain≥2·104→1
else
ifage<26→0
else
ifeducation-num <13→0
else
ifmarital-status=married →1
else→0ifcapital-gain≥2·104→1
else
ifhours-per-week <35→0
else
ifeducation-num <11→0
else
ifmarital-status=married →1
else→0
(b)
Figure 3: (a): optimal rule 𝑅★computed by CORELS on the adult dataset ( ℓ(𝑅★,D)=0.176) to predict high income (the label 1
denotes “≥50𝐾”). (b): set of rule lists computed by SamRuLe over 10runs. SamRuLe identified the optimal rule and slight
variations ˜𝑅1and ˜𝑅2that differ in the second rule of the list: they predict a lower outcome using the age ( ˜𝑅1) and the per-week
work hours features ( ˜𝑅2) with respective loss ℓ(˜𝑅1,D)=0.1763 andℓ(˜𝑅2,D)=0.1775.
also extremely small (i.e., 1to2orders of magnitude smaller than
ℓ(˜𝑅,D)), confirming that the conclusions that can be drawn from
the sample using SamRuLe, e.g., from the estimated loss ℓ(˜𝑅,S), are
very close to the corresponding exact ones. Finally, we compared
the logical conditions in the rule lists returned by SamRuLe with
the ones in the best rule list computed by CORELS. Our goal is to
verify that the insights gained from the approximated prediction
models from SamRuLe were similar to the optimal ones, i.e., that
SamRuLe allows a qualitative interpretation of the reported rule
list that was stable over the different experimental runs and similar
to what obtainable from the exact analysis. Figure 3 reports the
optimal rule list computed by CORELS on the dataset adult (a),
and the set of rule lists computed by SamRuLe (b) over all runs.
Interestingly, we observe that the approximations from SamRuLe
either match the optimal solution, or are very similar to it. In fact,
all rules reported by SamRuLe either share the same conditions
found in the optimal rule list, or replace one of the feature with
alternative reasonable insights (e.g., predicting a lower income for
young individuals and limited weekly working hours). In general,
we found the solutions reported by SamRuLe to be extremely stable
and similar to the respective optimal solutions also for all other
dataset.
From these observations we conclude that SamRuLe computes
extremely accurate rule lists using a fraction of the resources needed
by exact approaches, therefore scaling effectively to large datasets.5.2 Comparison to heuristic methods
In this set of experiments, we compare SamRuLe with two state-of-
the-art heuristic methods SBRL and RIPPER. For SamRuLe we fix
𝜃=0.025and𝜀=0.5. We ran SBRL for 104iterations using default
parameters (as suggested by [ 1,30]). Also for these experiments,
for each dataset we fix 𝑧=1and𝑘to the values in Table 1.
We show the results of these experiments in Figure 4.
Figure 4.(a) shows the running times for the three methods. We
observe that in all but one case, RIPPER is the slowest method.
On two datasets (ijcnn1 and higgs) we stopped it as it could not
complete after more than 12hours, while requiring a very large
memory footprint (e.g., more than 400GB of memory for higgs).
Regarding SBRL and SamRuLe, both methods are fast, e.g., requiring
always less than 18minutes. More precisely, SamRuLe is faster on 3
datasets, up to 3orders of magnitude for the susy dataset. In other
2datasets, the running times of the two methods are comparable,
while for phishing and adult SBRL is faster by a factor at most 4. This
experiment confirms that SamRuLe is very practical, requiring a
lower or comparable amount of resources of state-of-the-art scalable
heuristics.
Figure 4.(b) compares all methods in terms of accuracy. The plots
show the values of the loss ℓ(˜𝑅,D)for the rule list ˜𝑅reported
by all runs of the methods. We may observe that RIPPER is the
worst approach, as it always reports rule lists with the highest loss
 
2359Scalable Rule Lists Learning with Sampling KDD ’24, August 25–29, 2024, Barcelona, Spain
susy
ijcnn1
higgs
mushroom
bank
a9a
phishing
adult101
100101102103104Running time (s)Running time
SamRuLe SBRL RIPPER
susy
ijcnn1
higgs
mushroom
bank
a9a
phishing
adult101
100101102103104Running time (s)Running time
(
a)
susy
ijcnn1
higgs
mushroom
bank
a9a
phishing
adult103
102
101
Rule list lossRule list loss (b)ifage≥26→0
else if education-num <9→0
else if marital-status=married →1
else if education-num <13→0
else→0
(c)
Figure 4: Comparison in terms of running time (a) and accuracy (b) between SamRuLe, SBRL, and RIPPER. (c): rule 𝑅computed
by SBRL on adult with loss ℓ(𝑅,D)=0.238over all 10runs.
(results for ijcnn1 and higgs are not shown as RIPPER could not
complete in reasonable time, as discussed before), while SamRuLe
always provides a rule list with the smallest loss. Regarding SBRL,
we observe that it reports a rule list with the same, or almost the
same, loss of SamRuLe on4datasets, while it provides suboptimal
solutions for other cases, with losses up to 30%higher than Sam-
RuLe. This suggests that, while SBRL scales to large datasets, it
often provides solutions that are sensibly less accurate than the
optimal one. Instead, as discussed previously, SamRuLe outputs a
rule list with guaranteed gap with the optimal solution, and always
very close to it in practice. We remark that providing a suboptimal
solution may also impact the interpretation for the predictions, e.g.,
missing relevant factors for the model. In fact, we report the rule
list computed by SBRL on adult (see Figure 4.(c)) for all the 10runs:
such rule list does not involve the capital-gain feature, a key condi-
tion for the optimal solution to predict high income (Figure 3.(a)),
thus obtaining a higher loss (0 .238). In contrast, this feature was
always found in the rule lists reported by SamRuLe (Figure 3.(b)),
which have loss always very close to the optimal (0 .176).
Overall, compared to RIPPER and SBRL, SamRuLe outputs an
equally accurate solution in less time, or a sensibly better rule list
using comparable resources. We conclude that SamRuLe provides
an excelled combination of scalability and high accuracy for rule
list learning from large datasets, achieving a better trade-off than
state-of-the-art heuristic methods with no theoretical guarantees.
5.3 Impact of rule list parameters
In this final set of experiments, we evaluate the impact to the per-
formance of SamRuLe of the rule list search space parameters 𝑧and
𝑘. We focus on the datasets mushroom and phishing, as the results
for other datasets were similar. We test all values of 1≤𝑧≤3and
1≤𝑘≤5, fixing𝜃=0.025and𝜀=0.5, measuring the number of
samples required by SamRuLe, its running time, and the accuracy
of the rule lists provided in output.
We show these results in Figure 9. When increasing 𝑘and𝑧,
the number of samples ˆ𝑚(R𝑧
𝑘,D)considered by SamRuLe grows
following the expect trend ˆ𝑚(R𝑧
𝑘,D)∈O((𝜔+log(1/𝛿))/(𝜀2𝜃))
proved in our analysis (Theorem 4.8), resulting in sample sizes thatare always a small fraction of the size of the dataset. Consequently,
the running time of SamRuLe increases roughly linearly with the
sample size, remaining practical for all settings. Regarding the
accuracy of the rule lists, we observed the parameter 𝑘to have the
largest impact on the loss, that remains fairly stable w.r.t. 𝑧.
These results demonstrate that SamRuLe is applicable to complex
analysis involving larger values of 𝑘and𝑧while scaling to large
datasets, that are in most cases out of reach of exact approaches.
6 Conclusions
We introduced SamRuLe, a novel and scalable algorithm to find
nearly optimal rule lists. SamRuLe uses sampling to scale to large
datasets, and provides rigorous guarantees on the quality of the
rule lists it reports. Our approach builds on the VC-dimension of
rule lists, for which we proved novel upper and lower bounds. Our
experimental evaluation shows that SamRuLe enables learning
highly accurate rule lists on large datasets, is up to two orders of
magnitude faster than state-of-the-art exact approaches, and is as
fast as, and sometimes faster than, recent heuristic approaches,
while reporting higher quality rule lists.
Our work opens several interesting directions for future research,
including the use of sampling to scale approaches for learning other
rule-based models while providing rigorous guarantees on the qual-
ity of the learned model. Moreover, the efficient computation of ad-
vanced data-dependent complexity measures, such as Rademacher
averages [ 2,16,17], may be useful to obtain even sharper approxi-
mation guarantees for our problem.
Acknowledgments
This work was supported by the “National Center for HPC, Big
Data, and Quantum Computing”, project CN00000013, and by the
PRIN Project n. 2022TS4Y3N - EXPAND: scalable algorithms for
EXPloratory Analyses of heterogeneous and dynamic Networked
Data, funded by the Italian Ministry of University and Research
(MUR), and by the project BRAINTEASER (Bringing Artificial In-
telligence home for a better care of amyotrophic lateral sclerosis
and multiple sclerosis), funded by European Union’s Horizon 2020
(grant agreement No. GA101017598).
 
2360KDD ’24, August 25–29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
References
[1]Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia
Rudin. 2018. Learning certifiably optimal rule lists for categorical data. Journal
of Machine Learning Research 18, 234 (2018), 1–78.
[2]Peter L. Bartlett and Shahar Mendelson. 2002. Rademacher and Gaussian complex-
ities: Risk bounds and structural results. Journal of Machine Learning Research 3,
Nov (2002), 463–482.
[3]Stéphane Boucheron, Olivier Bousquet, and Gábor Lugosi. 2005. Theory of
classification: A survey of some recent advances. ESAIM: probability and statistics
9 (2005), 323–375.
[4]Leo Breiman. 2001. Statistical modeling: The two cultures (with comments and a
rejoinder by the author). Statistical science 16, 3 (2001), 199–231.
[5]L Breiman, J Friedman, R A Olshen, and C J Stone. 1984. Classification and
Regression Trees. Routledge.
[6]Martino Ciaperoni, Han Xiao, and Aristides Gionis. 2023. Concise and inter-
pretable multi-label rule sets. Knowledge and Information Systems 65, 12 (2023),
5657–5694.
[7]William W Cohen. 1995. Fast effective rule induction. In Machine learning
proceedings 1995. Elsevier, 115–123.
[8]Jiayun Dong and Cynthia Rudin. 2020. Exploring the cloud of variable importance
for the set of all good models. Nature Machine Intelligence 2, 12 (2020), 810–824.
[9]Jonas Fischer and Jilles Vreeken. 2019. Sets of robust rules, and how to find them.
InJoint European Conference on Machine Learning and Knowledge Discovery in
Databases. Springer, 38–54.
[10] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All Models are
Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an
Entire Class of Prediction Models Simultaneously. J. Mach. Learn. Res. 20, 177
(2019), 1–81.
[11] Van Quoc Phuong Huynh, Johannes Fürnkranz, and Florian Beck. 2023. Efficient
learning of large sets of locally optimal classification rules. Machine Learning
112, 2 (2023), 571–610.
[12] Michael Mitzenmacher and Eli Upfal. 2017. Probability and computing: Random-
ization and probabilistic techniques in algorithms and data analysis. Cambridge
university press.
[13] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations
of machine learning. MIT press.
[14] Yuzuru Okajima and Kunihiko Sadamasa. 2019. Decision list optimization based
on continuous relaxation. In Proceedings of the 2019 SIAM International Conference
on Data Mining. SIAM, 315–323.
[15] Ioanna Papagianni and Matthijs van Leeuwen. 2023. Discovering Rule Lists
with Preferred Variables. In International Symposium on Intelligent Data Analysis.
Springer, 340–352.
[16] Leonardo Pellegrina. 2023. Efficient Centrality Maximization with Rademacher
Averages. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’23). Association for Computing Machinery,
New York, NY, USA, 1872–1884. https://doi.org/10.1145/3580305.3599325
[17] Leonardo Pellegrina, Cyrus Cousins, Fabio Vandin, and Matteo Riondato. 2022.
MCRapper: Monte-Carlo Rademacher averages for poset families and approx-
imate pattern mining. ACM Transactions on Knowledge Discovery from Data
(TKDD) 16, 6 (2022), 1–29.
[18] Hugo M Proença and Matthijs van Leeuwen. 2020. Interpretable multiclass
classification by MDL-based rule lists. Information Sciences 512 (2020), 1372–
1393.
[19] J. Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA. http://portal.acm.org/citation.cfm?id=
152181
[20] Matteo Riondato and Eli Upfal. 2014. Efficient Discovery of Association Rules and
Frequent Itemsets through Sampling with Tight Performance Guarantees. ACM
Trans. Knowl. Disc. from Data 8, 4 (2014), 20. https://doi.org/10.1145/2629586
[21] Ronald L Rivest. 1987. Learning decision lists. Machine learning 2 (1987), 229–246.
[22] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and
Chudi Zhong. 2022. Interpretable machine learning: Fundamental principles and
10 grand challenges. Statistics Surveys 16, none (2022), 1 – 85.
[23] Cynthia Rudin and Şeyda Ertekin. 2018. Learning customized and optimized
lists of rules with mathematical programming. Mathematical Programming
Computation 10 (2018), 659–702.
[24] Cynthia Rudin, Benjamin Letham, and David B Madigan. 2013. Learning theory
analysis for association rules and sequential event prediction. (2013).
[25] Diego Santoro, Andrea Tonon, and Fabio Vandin. 2020. Mining Sequential Pat-
terns with VC-Dimension and Rademacher Complexity. Algorithms 13, 5 (2020),
123.
[26] Lesia Semenova, Cynthia Rudin, and Ronald Parr. 2022. On the existence of
simpler machine learning models. In Proceedings of the 2022 ACM Conference on
Fairness, Accountability, and Transparency. 1827–1858.
[27] Shai Shalev-Shwartz and Shai Ben-David. 2014. Understanding Machine Learning:
From Theory to Algorithms. Cambridge University Press.[28] V. N. Vapnik and A. Ya. Chervonenkis. 1971. On the Uniform Convergence of
Relative Frequencies of Events to Their Probabilities. Theory of Probability & Its
Applications 16, 2 (1971), 264. https://doi.org/10.1137/1116025
[29] Rui Xin, Chudi Zhong, Zhi Chen, Takuya Takagi, Margo Seltzer, and Cynthia
Rudin. 2022. Exploring the whole rashomon set of sparse decision trees. Advances
in Neural Information Processing Systems 35 (2022), 14071–14084.
[30] Hongyu Yang, Cynthia Rudin, and Margo Seltzer. 2017. Scalable Bayesian rule
lists. In International conference on machine learning. PMLR, 3921–3930.
[31] Olcay Taner Yildiz. 2014. VC-Dimension of Rule Sets. In 2014 22nd International
Conference on Pattern Recognition. IEEE, 3576–3581.
[32] Jinqiang Yu, Alexey Ignatiev, Peter J Stuckey, and Pierre Le Bodic. 2021. Learning
optimal decision sets and lists with sat. Journal of Artificial Intelligence Research
72 (2021), 1251–1279.
A Appendix
In this Appendix we provide proofs and additional experimental
results that could not fit in the main paper due to space constraints.
Some figures, and some of the proofs for the results of Section 4.1
are deferred to the online extended version, that is available at
https://arxiv.org/abs/2406.12803.
A.1 Proofs of Section 4.1
Proof of Corollary 4.3. Given a datasetDwith𝑑features, we
create a new dataset D𝑧built as follows. Let 𝐶={“𝑥1=1”,“𝑥2=
1”,..., “𝑥𝑑=1”}be the set of all 𝑑possible conditions on the
𝑑binary features of D𝑠; for any non-empty subset 𝐴⊆𝐶with
|𝐴|≤𝑧, we add toD𝑧the binary feature 𝑥′
𝐴that is equal to 1for
all training instances of Dsuch thatÓ
𝑐∈𝐴𝑐is true. Equivalently,
the feature values of 𝑥′
𝐴are obtained from the logical AND of the
evaluations of the conditions in 𝐴. It follows that the total number
of features ofD𝑧is𝑑′=Í𝑧
𝑖=1 𝑑
𝑖.
We now observe that the set P(˜R𝑧
𝑘,D)of projections of rule lists
inR𝑧
𝑘onDis contained in the set P(˜R1
𝑘,D𝑧)of projections of rules
inR1
𝑘onD𝑧, since we can replace each rule with a conjunctions
with𝑡terms, with 1≤𝑡≤𝑧, in any rule list∈R𝑧
𝑘onDby a rule
with a single condition on one of the features of D𝑧, obtaining an
equivalent rule list from R1
𝑘. Therefore, from Corollary 4.2 applied
toR1
𝑘over a dataset with 𝑑′features, and the fact 𝑑′=Í𝑧
𝑖=1 𝑑
𝑖≤
(𝑒𝑑
𝑧)𝑧, we obtain
𝑉𝐶(R𝑧
𝑘)≤
𝑘log2 2𝑑′+2
=$
𝑘log2 
2𝑧∑︁
𝑖=1𝑑
𝑖!
+2%
≤
𝑘log2
2𝑒𝑑
𝑧𝑧
+2
≤
𝑘𝑧log22𝑒𝑑
𝑧
+2
,
and the statement follows. □
A.2 Proofs of Section 4.2
To prove our results we use the following Chernoff bounds (see
Theorem 4.4 and 4.5 of [12]).
Theorem A.1. Let𝑋1,...𝑋𝑚be independent Poisson trials such
that Pr(𝑋𝑖=1)=𝑝𝑖. Let𝑋=Í𝑛
𝑖=1𝑋𝑖and𝜇=E[𝑋]. Then the
following Chernoff bounds hold for any 0<𝛾<1:
Pr(𝑋≥(1+𝛾)𝜇)≤exp(−𝜇𝛾2/3),
Pr(𝑋≤(1−𝛾)𝜇)≤exp(−𝜇𝛾2/2).
 
2361Scalable Rule Lists Learning with Sampling KDD ’24, August 25–29, 2024, Barcelona, Spain
Proof of Theorem 4.6. We prove the first set of inequalities.
Let𝑅be an arbitrary rule list from R𝑧
𝑘, and define the functions
𝑔(𝑅,S)=ℓ(𝑅,S)−𝛼|𝑅|, (7)
𝑔(𝑅,D)=ℓ(𝑅,D)−𝛼|𝑅|. (8)
Note that𝑔(𝑅,D)and𝑔(𝑅,S)are the non-regularized variants
of the loss functions ℓ(𝑅,D)andℓ(𝑅,S). It easy to show that
ES[𝑔(𝑅,S)]=𝑔(𝑅,D) ≤ℓ(𝑅,D), and that𝑔(𝑅,S)is an aver-
age of𝑚binary random variables with expectation 𝑔(𝑅,D). From
an application of the Chernoff bound (Theorem A.1) to the random
variable𝑍=𝑚𝑔(𝑅,S)withES[𝑍]=𝑚𝑔(𝑅,D), we have that, for
any0<𝛾<1,
Pr
𝑍≤(1−𝛾)E
S[𝑍]
≤exp
−E
S[𝑍]𝛾2/2
. (9)
Fixing 0<𝛿′<1, imposing the r.h.s. of (9)to be≤𝛿′, and solving
for𝛾, gives
Pr 
𝑔(𝑅,S)+√︂
2𝑔(𝑅,D)ln(1/𝛿′)
𝑚≤𝑔(𝑅,D)!
≤𝛿′.
Define the events
𝐸𝑅=“𝑔(𝑅,S)+√︂
2𝑔(𝑅,D)ln(1/𝛿′)
𝑚≤𝑔(𝑅,D)”,∀𝑅∈R𝑧
𝑘,
and the event 𝐸=“∃𝑅:𝐸𝑅is true” . Note that Pr(𝐸𝑅)≤𝛿′. We
want to prove that Pr(𝐸)≤𝛿/2. First, we observe that
Pr(𝐸)=PrØ
𝑅∈R𝑧
𝑘𝐸𝑅
.
Denote with 𝑅1and𝑅2two rule lists∈R𝑧
𝑘such that the projec-
tions𝑋(𝑅𝑖,D)of𝑅𝑖onDare equal:𝑋(𝑅1,D)=𝑋(𝑅2,D). Con-
sequently, it holds 𝑔(𝑅1,D)=𝑔(𝑅2,D). Moreover, for all possible
samplesS, it holds𝑋(𝑅1,S)=𝑋(𝑅2,S)and𝑔(𝑅1,S)=𝑔(𝑅2,S).
The observations above imply that the events 𝐸𝑅1and𝐸𝑅2are iden-
tical.
We now define the set C(R𝑧
𝑘,D)⊆R𝑧
𝑘as a cover of the rangeset
˜R𝑧
𝑘as follows: fix an arbitrary, deterministic total order over all
elements ofR𝑧
𝑘; then, for every distinct projection 𝐻∈˜R𝑧
𝑘, let
{𝑅:𝑋(𝑅,D)=𝐻,𝑅∈R𝑧
𝑘}be the set of rule lists with projection
equal to𝐻. From this set, we pick the minimum element in the total
order overR𝑧
𝑘and we include it in C(R𝑧
𝑘,D). Therefore,C(R𝑧
𝑘,D)
contains a unique rule list for each distinct projection of the rangeset
˜R𝑧
𝑘over the datasetD. This implies that, for any 𝑅∈R𝑧
𝑘, there is
an unique rule list 𝑅′∈C(R𝑧
𝑘,D)such that𝑋(𝑅,D)=𝑋(𝑅′,D)and that the events 𝐸𝑅and𝐸𝑅′are identical. It follows
PrØ
𝑅∈R𝑧
𝑘𝐸𝑅
=PrØ
𝑅∈C(R𝑧
𝑘,D)𝐸𝑅
,
as we can replace the union over all 𝑅∈R𝑧
𝑘with the union over
the coverC(R𝑧
𝑘,D)containing rule lists with unique projections
onD. From the definitions of Λ(R𝑧
𝑘,𝑛)andC(R𝑧
𝑘,D), it holds
|C(R𝑧
𝑘,D)|=|˜R𝑧
𝑘(D)|≤ Λ(R𝑧
𝑘,𝑛).
From an union bound, we have
Pr(𝐸)=PrØ
𝑅∈C(R𝑧
𝑘,D)𝐸𝑅
≤∑︁
𝑅∈C(R𝑧
𝑘,D)Pr(𝐸𝑅)≤Λ(R𝑧
𝑘,𝑛)𝛿′.
Choosing𝛿′=𝛿/(2Λ(R𝑧
𝑘,𝑛)), we obtain that
𝑔(𝑅,D)≤𝑔(𝑅,S)+vt
2𝑔(𝑅,D)
𝜔+ln 2
𝛿
𝑚(10)
holds for all 𝑅∈R𝑧
𝑘with probability≥1−𝛿/2, since ln(𝛿′)≤
𝜔+ln 2
𝛿(following analogous derivations for the proof of Corol-
lary 4.2). The first set of inequalities is obtained from (10)after
adding𝛼|𝑅|on both sides, from the fact that 𝑔(𝑅,D)≤ℓ(𝑅,D),
after solving the quadratic inequality 𝑦≤𝑢+√𝑣𝑦w.r.t.𝑦, and after
straightforward computations.
We now prove the last inequality and the statement. Let 𝑅★
be an arbitrary rule list with ℓ(𝑅★,D)=min𝑅∈R𝑧
𝑘ℓ(𝑅,D). Note
that𝑅★is fixed and independent of the choice of S. We apply
the Chernoff bound to the random variable 𝑍=𝑚𝑔(𝑅★,S)with
ES[𝑍]=𝑚𝑔(𝑅★,D), obtaining for any 0<𝛾<1
Pr
𝑍≥(1+𝛾)E
S[𝑍]
≤exp
−E
S[𝑍]𝛾2/3
. (11)
Setting the r.h.s. of (11)≤𝛿/2, and solving for 𝛾using𝛼≥0, gives
Pr 
𝑔(𝑅★,S)≥𝑔(𝑅★,D)+√︂
3𝑔(𝑅★,D)ln(2/𝛿)
𝑚!
≤𝛿/2,
and, equivalently,
𝑔(𝑅★,S)≤𝑔(𝑅★,D)+√︂
3𝑔(𝑅★,D)ln(2/𝛿)
𝑚
with probability≥1−𝛿/2. We obtain the last inequality of the
statement after adding 𝛼|𝑅★|to both sides, and from the fact
𝑔(𝑅★,D) ≤ℓ(𝑅★,D). From an union bound, all inequalities of
the theorem are simultaneously valid with probability ≥1−𝛿,
obtaining the statement. □
 
2362KDD ’24, August 25–29, 2024, Barcelona, Spain Leonardo Pellegrina and Fabio Vandin
102
 6×103
2×102
104105Number of samples
Number of samples (=1.0)
a9a adult bank higgs ijcnn1 mushroom phishing susy
102
 6×103
2×102
104105Number of samples
Number of samples (=1.0)
(
a)
102
 2×102
3×102
4×102
104105Number of samples
Number of samples (=0.5)
 (b)
102
 2×102
3×102
4×102
104105Number of samples
Number of samples (=0.25)
 (c)
Figure 5: Number of samples ˆ𝑚(R𝑧
𝑘,D)used by SamRuLe varying𝜀and𝜃for all datasets. 𝑘is set as in Table 1 and 𝑧=1.
102
105
104
Average deviations
Average deviations (susy)
SamRuLe (=0.25)
SamRuLe (=0.5)
SamRuLe (=1.0)
 CORELS
102
102103104Running time (s)
Running time (a9a)
102
103104Running time (s)
Running time (adult)
102
102103104Running time (s)
Running time (bank)
102
102103104Running time (s)
Running time (higgs)
102
102103104Running time (s)
Running time (ijcnn1)
102
102103Running time (s)
Running time (mushroom)
102
103104Running time (s)
Running time (phishing)
102
102
101
100101Running time (s)
Running time (susy)
Figure 6: Running times of SamRuLe and CORELS for different values of 𝜀and𝜃on all datasets.
102
105
104
103
102
101
Average deviations
Average deviations (a9a)
102
104
103
102
101
Average deviations
Average deviations (adult)
102
105
104
103
102
101
Average deviations
Average deviations (bank)
102
104
103
102
101
Average deviations
Average deviations (higgs)
102
104
103
102
101
Average deviations
Average deviations (ijcnn1)
102
105
104
103
Average deviations
Average deviations (mushroom)
102
105
104
103
102
101
Average deviations
Average deviations (phishing)
102
105
104
Average deviations
Average deviations (susy)
Figure 7: Average deviations |ℓ(˜𝑅,D)−ℓ(𝑅★,D)|over all runs for different values of 𝜀and𝜃and for all datasets. The purple
horizontal line is drawn at 𝑦=ℓ(𝑅★,D)(the loss of the optimal rule list 𝑅★found by CORELS). (same legend of Figure 6.)
 
2363