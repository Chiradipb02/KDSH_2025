Algorithmic Fairness Generalization under Covariate and
Dependence Shifts Simultaneously
Chen Zhao‚àó
Baylor University
Waco, Texas, USA
chen_zhao@baylor.eduKai Jiang‚àó
The University of Texas at Dallas
Richardson, Texas, USA
kai.jiang@utdallas.eduXintao Wu
University of Arkansas
Fayetteville, Arkansas, USA
xintaowu@uark.edu
Haoliang Wang
The University of Texas at Dallas
Richardson, Texas, USA
haoliang.wang@utdallas.eduLatifur Khan
The University of Texas at Dallas
Richardson, Texas, USA
lkhan@utdallas.eduChristan Grant
University of Florida
Gainesville, Florida, USA
christan@ufl.edu
Feng Chen
The University of Texas at Dallas
Richardson, Texas, USA
feng.chen@utdallas.edu
ABSTRACT
The endeavor to preserve the generalization of a fair and invariant
classifier across domains, especially in the presence of distribution
shifts, becomes a significant and intricate challenge in machine
learning. In response to this challenge, numerous effective algo-
rithms have been developed with a focus on addressing the prob-
lem of fairness-aware domain generalization. These algorithms are
designed to navigate various types of distribution shifts, with a
particular emphasis on covariate and dependence shifts. In this con-
text, covariate shift pertains to changes in the marginal distribution
of input features, while dependence shift involves alterations in
the joint distribution of the label variable and sensitive attributes.
In this paper, we introduce a simple but effective approach that
aims to learn a fair and invariant classifier by simultaneously ad-
dressing both covariate and dependence shifts across domains. We
assert the existence of an underlying transformation model can
transform data from one domain to another, while preserving the
semantics related to non-sensitive attributes and classes. By aug-
menting various synthetic data domains through the model, we
learn a fair and invariant classifier in source domains. This classifier
can then be generalized to unknown target domains, maintaining
both model prediction and fairness concerns. Extensive empirical
studies on four benchmark datasets demonstrate that our approach
surpasses state-of-the-art methods. The code repository is available
at https://github.com/jk-kaijiang/FDDG.
‚àóBoth authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671909CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíArtificial intelligence; Ma-
chine learning; ‚Ä¢Applied computing ‚ÜíLaw, social and behav-
ioral sciences ;‚Ä¢Social and professional topics ‚ÜíUser character-
istics.
KEYWORDS
Fairness, Generalization, Distribution Shifts
ACM Reference Format:
Chen Zhao, Kai Jiang, Xintao Wu, Haoliang Wang, Latifur Khan, Christan
Grant, and Feng Chen. 2024. Algorithmic Fairness Generalization under
Covariate and Dependence Shifts Simultaneously. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671909
1 INTRODUCTION
While modern fairness-aware machine learning techniques have
demonstrated significant success in various applications [ 31,54,56,
58‚Äì64], their primary objective is to facilitate equitable decision-
making, ensuring algorithmic fairness across all demographic groups
characterized by sensitive attributes, such as race and gender. Nev-
ertheless, the generalization of a fair classifier learned in the source
domain to a target domain during inference often demonstrates
severe limitations in many state-of-the-art methods. The poor gen-
eralization can be attributed to the data distribution shifts from
source to target domains, resulting in catastrophic failures.
There are two main lines of data distribution shifts [ 41]: general
and fairness-specific shifts. The former focuses on shifts involving
input features and labels. Specifically, covariate shift [ 45] and label
shift [ 52] refer to variations due to different marginal distributions
over feature and class variables, respectively. Concept shift [ 53] in-
dicates "functional relation change" due to the change amongst the
instance-conditional distributions [ 40]. Moreover, fairness-specific
shifts consider additional sensitive attributes and hence place a
greater emphasis on ensuring algorithmic fairness. Demographic
4419
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Chen Zhao et al.
NC, Male 
 NC, Male NC, Male 
C, Female 
 C, Female C, Female 
NC, Female 
 NC, Female 
 NC, Male 
C, Male 
 C, Male 
 C, Female Source Domain ( Photos ) Target Domain ( Arts)
Labels: Cooking (C) / No-Cooking (NC); Sensitive Attributes: Male / Female 
Source Domain Target Domain Covariate & Dependence Shift Covariate Shift Dependence Shift 
Instance Class Instance Features Sensitive Attributes 
Figure 1: Illustration of the problem in generalizing fair clas-
sifiers across different data domains under covariate and
dependence shifts simultaneously. (Upper) Images in source
and target domains have different styles (Photos and Arts).
Each data domain is linked to a distinct correlation between
class labels (NC and C) and sensitive attributes (Male and Fe-
male). (Lower) We consider x=[ùë•1,ùë•2]ùëáa simple example of
a two-dimensional feature vector. A fair classifier ùëìlearned
using source data is applied to data sampled from various
types of shifted target domains, resulting in misclassification
and unfairness. ùëì‚àórepresents the true classifier in the target
domain.
shift1[15] refers to certain sensitive population subgroups becom-
ing more or less probable during inference. Dependence shift [ 41]
captures the correlation change between the class variable and
sensitive attributes. Within these distribution shifts, a trained fair
classifier from source domains is directly influenced and may de-
grade when adapted to target domains.
To simplify, we narrow the scope of distribution shifts to two
prominent ones: covariate shift, which has been extensively inves-
tigated in the context of out-of-distribution (OOD) generalization
[40,57], and dependence shift, a topic that has gained attention
in recent research. In the illustrative example shown in Fig. 1, the
source and target domains exhibit variations stemming from differ-
ent image styles (Photos and Arts) and correlations between labels
(No-cooking and Cooking) and sensitive attributes (Male and Fe-
male). Specifically, in the source domain, most males in the kitchen
are not cooking, whereas in the target domain, a distinct correlation
is observed with most males engaging in cooking. To learn a classi-
fier that is both fair and accurate under such hybrid shifts, a variety
of domain generalization approaches have been explored. Predomi-
nantly, these methods often exhibit two specific limitations: they (1)
address either covariate shift [ 26,40,57] or dependence shift [ 8,36],
or (2) solely focus on covariate shift but not explicitly indicate the
existence of dependence shift [ 37]. Therefore, there is a need for
research that explores the problem of fairness-aware domain gener-
alization (FDG), considering both covariate and dependence shifts
simultaneously across source and target domains.
1Dependence shift is named as correlation shift in [15].In this paper, we introduce a novel framework, namely Fair dis-
Entangled DOmain geneRAlization (FEDORA). The key idea in our
framework revolves around learning a fair and accurate classifier
that can generalize from given source domains to target domains,
which remain unknown and inaccessible during training. The vari-
ations in these domains result from the concurrent presence of
covariate and dependence shifts. Notice that, unlike the settings in
some works involving covariate shift [ 32,38,48], we assert each
domain possesses a distinct data style (Photos and Arts), result-
ing in an alternation in feature spaces. Technically, we assert the
existence of a transformation model that can disentangle input
data to a semantic factor that remains invariant across domains, a
style factor that characterizes covariate-related information, and
a sensitive factor that captures attributes of a sensitive nature. To
enhance the generalization of the training classifier and adapt it
to unknown target domains, we augment the data by generating
them through the transformation model. It utilizes semantic factors
associated with various style and sensitive factors sampled from
their respective prior distributions. Furthermore, we leverage this
framework to systematically define the FDG problem as a semi-
infinite constrained optimization problem. Theoretically, we apply
this re-formulation to demonstrate that a tight approximation of the
problem can be achieved by solving the empirical, parameterized
dual for this problem. Moreover, we develop a novel interpretable
bound focusing on fairness within a target domain, considering the
domain generalization arising from both covariate and dependence
shifts. Finally, extensive experimental results on the proposed new
algorithm show that our algorithm significantly outperforms state-
of-the-art baselines on several benchmarks. Our main contributions
are summarized.
‚Ä¢We introduce a fairness-aware domain generalization problem
within a framework that accommodates inter-domain variations
arising from covariate and dependence shifts simultaneously. We
also give a brief survey by comparing the setting of related works.
‚Ä¢We reformulate the problem to a novel constrained learning prob-
lem. We further establish duality gap bounds for the empirically
parameterized dual of this problem and develop a novel upper
bound that specifically addresses fairness within a target domain
while accounting for the domain generalization stemming from
both covariate and dependence shifts.
‚Ä¢We present a novel algorithm, FEDORA, that enforces invariance
across unseen target domains by utilizing generative models
derived from the observed source domains.
‚Ä¢Comprehensive experiments are conducted to verify the effec-
tiveness of FEDORA. We empirically show that it significantly
outperforms state-of-the-art baselines on four benchmarks.
2 RELATED WORKS
Domain generalization. Addressing the challenge of domain shift
and the absence of OOD data has led to the introduction of sev-
eral state-of-the-art methods in the domain generalization field
[3,40,50,57]. These methods are designed to enable deep learning
models to possess intrinsic generalizability, allowing them to adapt
effectively from one or multiple source domains to target domains
characterized by unknown distributions [ 51]. They encompass var-
ious techniques, such as aligning source domain distributions to
4420Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 1: Different Types of Distribution Shifts.
Type of Shifts Notations, ‚àÄùë†‚ààEùë†
Covariate Shift (Cov.) [45] Pùë†
ùëã‚â†Pùë°
ùëã
Label Shift (Lab.) [52] Pùë†
ùëå‚â†Pùë°
ùëå
Concept Shift (Con.) [53] Pùë†
ùëå|ùëã‚â†Pùë°
ùëå|ùëã
Demographic Shift (Dem.) [15] Pùë†
ùëç‚â†Pùë°
ùëç
Dependence Shift (Dep.) [41] Pùë†
ùëå|ùëç‚â†Pùë°
ùëå|ùëçandPùë†
ùëç=Pùë°
ùëç; or,Pùë†
ùëç|ùëå‚â†Pùë°
ùëç|ùëåandPùë†
ùëå=Pùë°
ùëå
Hybrid Shift Any combination of the shifts above.
Table 2: An overview of different settings of existing ap-
proaches in mitigating unfairness under distribution shifts.
Refs.Distribution Shifts Spaces Change*,‚àÄùë†‚ààEùë†
|Eùë†|Access
to Target
Cov. Lab. Con. Dem. Dep. Xùë†‚â†Xùë°Yùë†‚â†Yùë°Zùë†‚â†Zùë°
[32, 38, 48]‚Ä¢ 1 No
[9]‚Ä¢ M No
[10, 39]‚Ä¢ 1 Yes
[37]‚Ä¢ ‚Ä¢ M No
[4] ‚Ä¢ 1 Yes
[20, 21] ‚Ä¢ 1 Yes
[15, 44] ‚Ä¢ ‚Ä¢ 1 Yes
[8] ‚Ä¢ M No
[36] ‚Ä¢ 1 No
[41] ‚Ä¢ 1 Yes
[22]‚Ä¢ ‚Ä¢ 1 Yes
[46]‚Ä¢ ‚Ä¢ 1 Yes
[43]‚Ä¢ ‚Ä¢ ‚Ä¢ 1 Yes
[18]‚Ä¢ ‚Ä¢ ‚Ä¢ 1 No
[7]‚Ä¢ ‚Ä¢ ‚Ä¢ 1 Yes
FEDORA‚Ä¢ ‚Ä¢ ‚Ä¢ M No
*Yùë†‚â†Yùë°andZùë†‚â†Zùë°indicate the introduction of new labels and new sensitive attributes. A
change inXdenotes a shift in feature variation, such as transitioning from photo images to arts.
facilitate domain-invariant representation learning [ 29], subjecting
the model to domain shift during training through meta-learning
[28], and augmenting data with domain analysis, among others [ 65],
and so on. In the context of the number of source domains, a signif-
icant portion of research [ 5,40,57] has focused on the multi-source
setting. This setting assumes the availability of multiple distinct
but relevant domains for the generalization task. As mentioned in
[5], the primary motivation for studying domain generalization
is to harness data from multiple sources in order to unveil stable
patterns. This entails learning representations invariant to the mar-
ginal distributions of data features, all while lacking access to the
target data. Nevertheless, existing domain generalization methods
tend to overlook the aspect of learning with fairness, where group
fairness dependence patterns may not change domains.
Fairness learning for changing environments. Two primary
research directions aim to tackle fairness-aware machine learning
in dynamic or changing environments. The first approach involves
equality-aware monitoring methods [ 1,7,15,24,37,39,46], which
strive to identify and mitigate unfairness in a model‚Äôs behavior by
continuously monitoring its predictions. These methods adapt the
model‚Äôs parameters or structure when unfairness is detected. How-
ever, a significant limitation of such approaches is their assumption
of invariant fairness levels across domains, which may not hold
in real-world applications. The second approach [ 8,36] focuses on
assessing a model‚Äôs fairness in a dynamic environment exclusively
under dependence shifts. However, it does not consider other types
of distribution shifts.In response to these limitations, this paper adopts a novel ap-
proach by attributing the distribution shift from source to target
domains to both covariate shift and fairness dependence shift si-
multaneously. The objective is to train a fairness-aware invariant
classifier capable of effective generalization across domains, ensur-
ing robust performance in terms of both model accuracy and the
preservation of fair dependence between predicted outcomes and
sensitive attributes under both shifts.
3 PRELIMINARIES
Notations. LetX‚äÜRùëëdenote a feature space, Z={‚àí1,1}is a
sensitive space, and Y={0,1}is a label space for classification. Let
C‚äÜRùëê,A‚äÜRùëé, andS‚äÜRùë†be the semantic, sensitive and style
latent spaces, respectively, induced from XandAby an underlying
transformation model ùëá:X√óZ√óE‚ÜíX√óZ . We useùëã,ùëç,ùëå,ùê∂,ùê¥,ùëÜ
to denote random variables that take values in X,Z,Y,C,A,S
andx,ùëß,ùë¶, c,a,sthe realizations. A domain ùëí‚ààE is defined as a
joint distribution Pùëí
ùëãùëçùëå=P(ùëãùëí,ùëçùëí,ùëåùëí):X√óZ√óY‚Üí[ 0,1]. A
classifierùëìin a class spaceFdenotesùëì‚ààF :X‚ÜíY . We denote
EandEùë†‚äÇ E as the set of domain labels for all domains and
source domains, respectively. Superscripts in the samples denote
their domain labels, while subscripts specify the indices of encoders.
For example, ùê∏ùë†(xùë†)denotes a sample xdrawn from the ùë†domain
and encoded by a style encoder ùê∏ùë†.
Fairness notions. When learning a fair classifier ùëì‚ààF that
focuses on statistical parity across different sensitive subgroups,
the fairness criteria require the independence between the sensitive
random variables ùëçand the predicted model outcome ùëì(ùëã)[11].
Addressing the issue of preventing group unfairness can be framed
as the formulation of a constraint. This constraint mitigates bias
by ensuring that ùëì(ùëã)aligns with the ground truth ùëå, fostering
equitable outcomes.
Definition 1 (Group Fairness Notion [ 35,54]).Given a dataset
D={(xùëñ,ùëßùëñ,ùë¶ùëñ)}|D|
ùëñ=1sampled i.i.d. from Pùëãùëçùëå , a classifier ùëì‚ààF :
X‚ÜíY is fair when the prediction ÀÜùëå=ùëì(ùëã)is independent of the
sensitive random variable ùëç. To get rid of the indicator function and
relax the exact values, a linear approximated form of the difference
between sensitive subgroups is defined as
ùúå(ÀÜùëå,ùëç)=EPùëãùëçùëåùëî(ÀÜùëå,ùëç), ùëî(ÀÜùëå,ùëç)=1
ùëù1(1‚àíùëù1)ùëç+1
2‚àíùëù1
ÀÜùëå(1)
ùëù1and1‚àíùëù1are the proportion of samples in the subgroup ùëç=1
andùëç=‚àí1, respectively.
Specifically, when ùëù1=P(ùëç=1)andùëù1=P(ùëç=1,ùëå=1), the
fairness notion ùúå(ÀÜùëå,ùëç)is defined as the difference of demographic
parity and the difference of equalized opportunity, respectively [ 35].
In this paper, we will present the results under demographic parity
(and then the expectation in Eq. (1) is over ùëãùëç), while the framework
can be generalized to multi-class, multi-sensitive attributes and
other fairness notions. Strictly speaking, a classifier ùëìis fair over
subgroups if it satisfies ùúå(ÀÜùëå,ùëç)=0.
Problem setting. Given a datasetD={Dùëí}|E|
ùëí=1, where each
Dùëí={(xùëí
ùëñ,ùëßùëí
ùëñ,ùë¶ùëí
ùëñ)}|Dùëí|
ùëñ=1isi.i.d. sampled from a domain Pùëí
ùëãùëçùëåand
ùëí‚ààE, we consider multiple source domains {Pùë†
ùëãùëçùëå}|Eùë†|
ùë†=1and a
distinct target domain Pùë°
ùëãùëçùëå,ùë°‚â†ùë†,‚àÄùë†‚ààEùë†‚äÇE andùë°‚ààE\Eùë†,
which is unknown and inaccessible during training. Given samples
4421KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Chen Zhao et al.
Data Reconstruction Sensitiveness Loss Factor Reconstruction 
Learning the Transformation Model Augmentation in Synthetic Domains 
Figure 2: (Left) A transformation model ùëáis trained using a bi-directional reconstruction loss (data reconstruction and factor
reconstruction) and a sensitiveness loss. (Right) To enhance the generalization of the classifier ùëìto unseen target domains,
the transformation model ùëáis used for augmentation in synthetic domains by generating data based on invariant semantic
factors and randomly sampled sensitive and style factors that encode synthetic domains. We demonstrate the concept using
theccMNIST dataset, where the domains are distinguished by different digit colors and fair dependencies between class labels
and sensitive attributes. Here, sensitive attributes are defined by image background colors.
{Dùë†}|Eùë†|
ùë†=1from finite source domains, the goal of fairness-aware
domain generalization problems is to learn a classifier ùëì‚ààF that
is generalizable across all possible domains.
Problem 1 (Fairness-aware Domain Generalization). Let
{Pùë†
ùëãùëçùëå}|Eùë†|
ùë†=1be a finite subset of source domains and assume that,
for eachùë†‚ààEùë†, we have access to its corresponding dataset Dùë†=
{(xùë†
ùëñ,ùëßùë†
ùëñ,ùë¶ùë†
ùëñ)}|Dùë†|
ùëñ=1sampled i.i.d from Pùë†
ùëãùëçùëå. Given a classifier set F
and a loss function ‚Ñì:Y√óY‚Üí R, the goal is to learn a fair classifier
ùëì‚ààF for anyDùë†that minimizes the worst-case risk over all domains
in{Pùëí
ùëãùëçùëå}|E|
ùëí=1satisfying a group fairness constraint:
min
ùëì‚ààFmax
ùëí‚ààEEPùë†
ùëãùëçùëå‚Ñì(ùëì(ùëãùë†),ùëåùë†),s.t.ùúå(ùëì(ùëãùë†),ùëçùë†)=0 (2)
The goal of Prob. 1 is to seek a fair classifier ùëìthat generalizes
from the given finite set of source domains to give a good general-
ization performance on all domains. Since we do not assume data
from a target domain is accessible, it makes Prob. 1 challenging to
solve.
Another challenge is how closely the data distributions in un-
known target domains match those in the observed source domains.
As discussed in Sec. 1 and Tab. 1, there are five different types of
distribution shifts. In this paper, we narrow the scope and claim the
shift between source and target domains is solely due to covariate
and dependence shifts.
Definition 2 (Covariate Shift [ 40] and Dependence Shift[ 41]).
In Prob. 1, covariate shift occurs when domain variation is attrib-
uted to disparities in the marginal distributions over input features
Pùë†
ùëã‚â†Pùë°
ùëã,‚àÄùë†. On the other hand, Prob. 1 exhibits a dependence shift
when domain variation arises from alterations in the joint distribution
betweenùëåandùëç, denoted Pùë†
ùëåùëç‚â†Pùë°
ùëåùëç,‚àÄùë†where Pùë†
ùëå|ùëç‚â†Pùë°
ùëå|ùëçand
Pùë†
ùëç=Pùë°
ùëç; orPùë†
ùëç|ùëå‚â†Pùë°
ùëç|ùëåandPùë†
ùëå=Pùë°
ùëå.
Underlying transformation models. Inspired by existing do-
main generalization endeavors [ 19,40,57], distribution shifts cancharacterize generalization tasks across domains through an un-
derlying transformation model ùëá. The motivation behind using ùëá
lies in bolstering the robustness and adaptability of the classifier ùëì
across diverse domains. By learning a transformation model, the ob-
jective is twofold: (1) to enable the model to adapt domain-invariant
data representations (factors) from the input data by disentangling
domain-specific variations and (2) to generate augmented data in
new domains by perturbing existing samples with various varia-
tions. This augmentation enhances the diversity of the source data
and thereby improves the ability to generalize to unseen target
domains.
4 METHODOLOGY
4.1 Learning the Transformation Model
One goal of the transformation model ùëá={ùê∏,ùê∫}is to disentangle
an input sample from source domains into three factors in latent
spaces by learning a set of encoder ùê∏={ùê∏ùëê,ùê∏ùëé,ùê∏ùë†}and a decoder
ùê∫:C√óA√óS‚ÜíX , whereùê∏ùëê:X‚ÜíC ,ùê∏ùëé:X‚ÜíA , and
ùê∏ùë†:X ‚Üí S represent semantic, sensitive and style encoders,
respectively.
Assumption 1 (Multiple Latent Factors). Given datasetDùëí=
{(xùëí
ùëñ,ùëßùëí
ùëñ,ùë¶ùëí
ùëñ)}|Dùëí|
ùëñ=1sampled i.i.d. from Pùëí
ùëãùëçùëådomainùëí‚ààE, we as-
sume that each instance xùëí
ùëñis generated from (1) a latent semantic
factor c‚ààC, whereC={cùë¶=0,cùë¶=1}; (2) a latent sensitive factor
a‚ààA , whereA={aùëß=1,aùëß=‚àí1}; and (3) a latent style factor sùëí,
where sùëíis specific to the individual domain ùëí. We assume that the
semantic and sensitive factors in CandAdo not change across do-
mains. Each domain Pùëí
ùëãùëçùëåis represented by a style factor sùëíand the
dependence score ùúåùëí=ùúå(ùëåùëí,ùëçùëí)2, denotedùëí:=(sùëí,ùúåùëí), where sùëí
andùúåùëíare unique to the domain Pùëí
ùëãùëçùëå.
Note that Assump. 1 is similarly related to the one made in
[19,34,40,57]. In our paper, with a focus on group fairness, we
2Here,ùúåfunctions equivalently as it does in Eq. (1), by substituting ÀÜùëåtoùëå.
4422Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
expand upon the assumptions of existing works by introducing
three latent factors. Under Assump. 1, if two instances (xùëíùëñ,ùëßùëíùëñ,ùë¶)
and(xùëíùëó,ùëßùëíùëó,ùë¶)whereùëíùëñ,ùëíùëó‚ààE,ùëñ‚â†ùëóshare the same class label,
then the latter instance can be reconstructed by decoder ùê∫from the
former using c=ùê∏ùëê(xùëíùëñ),s=ùê∏ùë†(xùëíùëó), and a=ùê∏ùëé(xùëíùëó)throughùëá,
denoted(xùëíùëó,ùëßùëíùëó)=ùëá(xùëíùëñ,ùëßùëíùëñ,ùëíùëó).
To enhance the effectiveness of the transformation model ùëá, our
overall learning loss for these encoders and decoders consists of
two main components: a bidirectional reconstruction loss and a
sensitiveness loss.
Data reconstruction loss encourages learning reconstruction
in the direction of data ‚Üílatent‚Üídata. As for it, a data sample xùë†
fromPùë†
ùëã,‚àÄùë†‚ààEùë†is required to be reconstructed by its encoded
factors.
Lùëëùëéùë°ùëé
ùëüùëíùëêùëúùëõ =Exùë†‚àºPùë†
ùëã[‚à•ùê∫(ùê∏ùëê(xùë†),ùê∏ùëé(xùë†),ùê∏ùë†(xùë†))‚àí xùë†‚à•1]
Factor reconstruction loss. Given latent factors c,a, and sùë†en-
coded from a sample xùë†, they are encouraged to be reconstructed
through some latent factors randomly sampled from the prior Gauss-
ian distributions.
Lùëìùëéùëêùë°ùëúùëü
ùëüùëíùëêùëúùëõ =Ec‚àºPùê∂,a‚àºPùê¥,sùë†‚àºPùëÜ[‚à•ùê∏ùëê(ùê∫(c,a,sùë†))‚àí c‚à•1]
+Ec‚àºPùê∂,a‚àºN( 0,Iùëé),sùë†‚àºPùëÜ[‚à•ùê∏ùëé(ùê∫(c,a,sùë†))‚àí a‚à•1]
+Ec‚àºPùê∂,a‚àºPùê¥,sùë†‚àºN( 0,Iùë†)[‚à•ùê∏ùë†(ùê∫(c,a,sùë†))‚àí s‚à•1]
where Pùê∂,Pùê¥,PùëÜare given by ùê∏ùëê(xùë†),ùê∏ùëé(xùë†), andùê∏ùë†(xùë†), respec-
tively.
Sensitiveness loss. Since a sensitive factor is causally dependent
on the sensitive attribute of data (xùë†,ùëßùë†,ùë¶ùë†), a simple classifier ‚Ñé:
A‚ÜíZ is learned, and further it is used to label the sensitive
attribute in augmented data when learning ùëì.
Lùë†ùëíùëõùë†=ùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶(ùëßùë†,‚Ñé(ùê∏ùëé(xùë†)))
Total loss. We jointly train the encoders and the decoder to opti-
mize the transformation model ùëáwith a weighted sum loss.
min
ùê∏ùëê,ùê∏ùëé,ùê∏ùë†,ùê∫ùõΩ1Lùëëùëéùë°ùëé
ùëüùëíùëêùëúùëõ+ùõΩ2Lùëìùëéùëêùë°ùëúùëü
ùëüùëíùëêùëúùëõ+ùõΩ3Lùë†ùëíùëõùë† (3)
whereùõΩ1,ùõΩ2,ùõΩ3>0are hyperparameters that control the impor-
tance of each loss term.
4.2 Fair Disentangled Domain Generalization
Furthermore, with a trained transformation model ùëá, to learn the
fairness-aware invariant classifier ùëìacross domains, we make the
following assumption.
Assumption 2 (Fairness-aware Domain Shift). We assume
that inter-domain variation is characterized by covariate and de-
pendence shifts. As a consequence, we assume that the conditional
distribution Pùëí
ùëå|ùëãùëçis stable across domains, ‚àÄùëí‚ààE. Given a trans-
formation model ùëá, it holds that Pùëíùëñ
ùëå|ùëãùëç=Pùëíùëó
ùëå|ùëãùëç,‚àÄùëíùëñ,ùëíùëó‚ààE,ùëñ‚â†ùëó,
where(ùëãùëíùëó,ùëçùëíùëó)=ùëá(Xùëíùëñ,ùëçùëíùëñ,ùëíùëó).
In Assump. 2, the domain shift captured by ùëáwould characterize
the mapping from the marginal distributions Pùëíùëñ
ùëãandùúå(ùëåùëíùëñ,ùëçùëíùëñ)
overDùëíùëñto the distribution Pùëíùëó
ùëãandùúå(ùëåùëíùëó,ùëçùëíùëó)overDùëíùëósampled
from a different data domain Pùëíùëó
ùëãùëçùëå, respectively. With this in mind
and under Assump. 2, we introduce a new definition of fairness-
aware invariance with respect to the variation captured by ùëáand
satisfying the group fair constraint introduced in Defn. 1.Definition 3 (Fairness-aware ùëá-Invariance). Given a trans-
formation model ùëá, a fairness-aware classifier ùëì‚àà F is domain
invariant if it holds for all ùëíùëñ,ùëíùëó‚ààE.
ùëì(xùëíùëñ)=ùëì(xùëíùëó),andùúå(ùëì(ùëãùëíùëñ),ùëçùëíùëñ)=ùúå(ùëì(ùëãùëíùëó),ùëçùëíùëó)=0(4)
almost surely when (xùëíùëó,ùëßùëíùëó)=ùëá(xùëíùëñ,ùëßùëíùëñ,ùëíùëó),xùëíùëñ‚àºPùëíùëñ
ùëã,xùëíùëó‚àºPùëíùëó
ùëã.
Defn. 3 is crafted to enforce invariance on the predictions gen-
erated byùëìdirectly. We expect a prediction to remain consistent
across various data realizations ùëáwhile considering group fairness.
Problem 2 (Fair Disentanglement Domain Generalization).
Under Defn. 3 and Assump. 2, if we restrict Fof Prob. 1 to the set of
invariant fairness-aware classifiers, the Prob. 1 is equivalent to the
following problem
ùëÉ‚òÖ‚âúmin
ùëì‚ààFùëÖ(ùëì)‚âúEPùë†ùëñ
ùëãùëçùëå‚Ñì(ùëì(ùëãùë†ùëñ),ùëåùë†ùëñ) (5)
s.t.ùëì(ùëãùë†ùëñ)=ùëì(ùëãùë†ùëó), ùúå(ùëì(ùëãùë†ùëñ),ùëçùë†ùëñ)=ùúå(ùëì(ùëãùë†ùëó),ùëçùë†ùëó)=0
where(ùëãùë†ùëó,ùëçùë†ùëó)=ùëá(ùëãùë†ùëñ,ùëç,ùë†ùëó),‚àÄùë†ùëñ,ùë†ùëó‚ààEùë†,ùëñ‚â†ùëó.
Similar to [ 40], Prob. 2 is not a composite optimization prob-
lem. Moreover, acquiring domain labels is often expensive or even
unattainable, primarily due to privacy concerns. Consequently,
under the assumptions of disentanglement-based invariance and
domain shift, Prob. 1 can be approximated to Prob. 2 by removing
the max operator over E.
In addition, Prob. 2 offers a new and theoretically-principled
perspective on Prob. 1, when data varies from domain to domain
with respect to ùëá. To optimize Prob. 2 is challenging because (1)
The strict equality constraints in Prob. 2 are difficult to enforce in
practice; (2) Enforcing constraints on deep networks is known to be
a challenging problem due to non-convexity. Simply transforming
them to regularization cannot guarantee satisfaction for constrained
problems; and (3) As we have incomplete access to all domains, it
limits the ability to enforce fairness-aware ùëá-invariance and further
makes it hard to estimate ùëÖ(ùëì).
Due to such challenges, we develop a tractable method for ap-
proximately solving Prob. 2 with optimality guarantees. To address
the first challenge, we relax constraints in Prob. 2
ùëÉ‚òÖ(ùõæ1,ùõæ2)‚âúmin
ùëì‚ààFùëÖ(ùëì) (6)
s.t.ùõøùë†ùëñ,ùë†ùëó(ùëì)‚â§ùõæ1, ùúåùë†ùëñ(ùëì)‚â§ùõæ2
2,andùúåùë†ùëó(ùëì)‚â§ùõæ2
2
where
ùõøùë†ùëñ,ùë†ùëó(ùëì)‚âúEPùë†ùëñ
ùëãùëçùëë
ùëì(ùëãùë†ùëñ),ùëì(ùëãùë†ùëó=ùëá(ùëãùë†ùëñ,ùëçùë†ùëñ,ùë†ùëó))
, (7)
ùúåùë†ùëñ(ùëì)‚âúùúå(ùëì(ùëãùë†ùëñ),ùëçùë†ùëñ), ùúåùë†ùëó(ùëì)‚âúùúå(ùëì(ùëãùë†ùëó),ùëçùë†ùëó) (8)
and‚àÄùë†ùëñ,ùë†ùëó‚ààEùë†,ùëñ‚â†ùëó. Here,ùõæ1,ùõæ2>0are constants controlling the
extent of relaxation and ùëë[¬∑]is a distance metric, e.g.,KL-divergence.
Whenùõæ1=ùõæ2=0, Eqs. (5) and (6) are equivalent.
Since it is unrealistic to have access to the full distribution and
we only have access to source domains, given data sampled from
Eùë†, we consider the empirical dual problem.
ùê∑‚òÖ
ùúâ,ùëÅ,Eùë†(ùõæ1,ùõæ2)‚âú max
ùúÜ1(ùë†ùëñ,ùë†ùëó),ùúÜ2(ùë†ùëñ,ùë†ùëó)min
ùúΩ‚ààŒòÀÜùëÖ(ùúΩ)
+1
|Eùë†|‚àëÔ∏Å
ùë†ùëñ,ùë†ùëó‚ààEùë†h
ùúÜ1(ùë†ùëñ,ùë†ùëó) ÀÜùõøùë†ùëñ,ùë†ùëó(ùúΩ)‚àíùõæ1(9)
+ùúÜ2(ùë†ùëñ,ùë†ùëó) ÀÜùúåùë†ùëñ(ùúΩ)+ÀÜùúåùë†ùëó(ùúΩ)‚àíùõæ2i
4423KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Chen Zhao et al.
Algorithm 1 FEDORA: Fair Disentangled Domain Generalization.
Require: Encoders ùê∏={ùê∏ùëê,ùê∏ùëé,ùê∏ùë†}, decoderùê∫and sensitive classifier ‚Ñé.
Initialize: primal and dual learning rate ùúÇùëù,ùúÇùëë, empirical constant ùõæ1,ùõæ2.
1:repeat
2: forminibatchB={(xùëñ,ùëßùëñ,ùë¶ùëñ)}ùëö
ùëñ=1‚äÇDùë†do
3:Lùëêùëôùë†(ùúΩ)=1
ùëö√çùëö
ùëñ=1‚Ñì(ùë¶ùëñ,ÀÜùëì(xùëñ,ùúΩ))
4: InitializeLùëñùëõùë£(ùúΩ)=0andB‚Ä≤=[]
5: foreach(xùëñ,ùëßùëñ,ùë¶ùëñ)in the minibatch do
6: Generate(xùëó,ùëßùëó,ùë¶ùëó)=ùëá(xùëñ,ùëßùëñ,ùë¶ùëñ)and add toB‚Ä≤
7:Lùëñùëõùë£(ùúΩ)+=1
ùëöùëë[ÀÜùëì(xùëñ,ùúΩ),ÀÜùëì(xùëó,ùúΩ)]
8: end for
9:Lùëìùëéùëñùëü(ùúΩ) =1
ùëö√ç
(xùëñ,ùëßùëñ)‚ààBùëî(ÀÜùëì(xùëñ,ùúΩ),ùëßùëñ)+1
ùëö√ç
(xùëó,ùëßùëó)‚ààB‚Ä≤ùëî(ÀÜùëì(xùëó,ùúΩ),ùëßùëó)
10:L(ùúΩ)=Lùëêùëôùë†(ùúΩ)+ùúÜ1¬∑Lùëñùëõùë£(ùúΩ)+ùúÜ2¬∑Lùëìùëéùëñùëü(ùúΩ)
11: ùúΩ‚ÜêAdam(L(ùúΩ),ùúΩ,ùúÇùëù)
12:ùúÜ1‚Üêmax{[ùúÜ1+ùúÇùëë¬∑(Lùëñùëõùë£(ùúΩ)‚àíùõæ1)],0},ùúÜ2‚Üêmax{[ùúÜ2+
ùúÇùëë¬∑(Lùëìùëéùëñùëü(ùúΩ)‚àíùõæ2)],0}
13: end for
14:until convergence
15:procedureùëá(x,ùëß,ùë¶ )
16: c,a,s=ùê∏(x)
17: Sample a‚Ä≤‚àºN( 0,ùêºùëé),s‚Ä≤‚àºN( 0,ùêºùë†)
18: x‚Ä≤=ùê∫(c,a‚Ä≤,s‚Ä≤),ùëß‚Ä≤=‚Ñé(a‚Ä≤)
19: return(x‚Ä≤,ùëß‚Ä≤,ùë¶)
20:end procedure
whereùúâ=EPùëã||ùëì(x)‚àí ÀÜùëì(x,ùúΩ)||‚àû>0is a constant bound-
ing the difference between ùëìand its parameterized counterpart
ÀÜùëì:X√óŒò‚ÜíRdefined in the Defn. 5.1 of [ 40].ùëÅis the num-
ber of samples drawn from Pùëãùëçùëå and it can be empirically re-
placed by√ç
ùë†‚ààEùë†|Dùë†|.ùúÜ1(ùë†ùëñ,ùë†ùëó),ùúÜ2(ùë†ùëñ,ùë†ùëó)>0are dual variables.
ÀÜùëÖ(ùúΩ),ÀÜùõøùë†ùëñ,ùë†ùëó(ùúΩ),ÀÜùúåùë†ùëñ(ùúΩ)andÀÜùúåùë†ùëó(ùúΩ)are the empirical counterparts of
ùëÖ(ÀÜùëì(¬∑,ùúΩ)),ùõøùë†ùëñ,ùë†ùëó(ÀÜùëì(¬∑,ùúΩ)),ùúåùë†ùëñ(ÀÜùëì(¬∑,ùúΩ))andùúåùë†ùëó(ÀÜùëì(¬∑,ùúΩ)), respectively.
4.3 The FEDORA Algorithm
In practice, we propose a simple but effective algorithm, given in
Algorithm 1, which is co-trained with the transformation model ùëá.
The detailed training process of ùëáis provided in Algorithm 2 of the
Appendix. In Algorithm 1, we harness the power of ùëáto address
the unconstrained dual optimization problem outlined in Eq. (9)
through a series of primal-dual iterations.
Given a finite number of observed source domains, to enhance
the generalization performance for unseen target domains, the
invariant classifier ÀÜùëìis trained by expanding the dataset with syn-
thetic domains generated by ùëá. These synthetic domains are created
by introducing random sample style and random sensitive factors,
hence a random sensitive attribute, resulting in an arbitrary fair de-
pendence within such domains. As described in Fig. 2, the sensitive
factor aùë†‚Ä≤and the style factor sùë†‚Ä≤are randomly sampled from their
prior distributions N(0,Iùëé)andN(0,Iùë†), respectively. A sensitive
attributeùëßùë†‚Ä≤is further predicted from aùë†‚Ä≤through‚Ñé. Along with the
unchanged semantic factor cencoded by(xùë†,ùëßùë†,ùë¶), they are fur-
ther passed through ùê∫to generate(xùë†‚Ä≤,ùëßùë†‚Ä≤,ùë¶)with the unchanged
class labels in an augmented synthetic domain. Under Assump. 2
and Defn. 3, according to Eqs. (7) and (8), data augmented in syn-
thetic domains are required to maintain invariance in terms ofaccuracy and fairness with the data in the corresponding original
domains.
Specifically, in lines 15-20 of Algorithm 1, we describe the trans-
formation procedure that takes an example (x,ùëß,ùë¶)as INPUT and
returns an augmented example (x‚Ä≤,ùëß‚Ä≤,ùë¶)from a new synthetic do-
main as OUTPUT. The augmented example has the same semantic
factor as the input example but has different sensitive and style
factors sampled from their associated prior distributions that en-
code a new synthetic domain. Lines 1-14 show the main training
loop for FEDORA. In line 6, for each example in the minibatch B,
we apply the procedure ùëáto generate an augmented example from
a new synthetic domain described above. In line 7, we consider
KL-divergence as the distance metric for ùëë[¬∑]. All the augmented
examples are stored in the set B‚Ä≤. The Lagrangian dual loss function
is defined based on BandB‚Ä≤in line 10. The primal parameters ùúΩ
and the dual parameters ùúÜ1andùúÜ2are updated in lines 11-12.
5 ANALYSIS
With the approximation on the dual problem in Eq. (9), the duality
gap between ùëÉ‚òÖin Eq. (6) and ùê∑‚òÖ
ùúâ,ùëÅ,Eùë†(ùõæ1,ùõæ2)in Eq. (9) can be
explicitly bounded.
Theorem 1 (Fairness-aware Data-dependent Duality Gap).
Givenùúâ>0, assuming{ÀÜùëì(¬∑,ùúΩ):ùúΩ‚ààŒò} ‚äÜ F has finite VC-
dimension, with ùëÄdatapoints sampled from Pùëãùëçùëå we have
|ùëÉ‚òÖ‚àíùê∑‚òÖ
ùúâ,ùëÅ,Eùë†(ùú∏)|‚â§ùêø||ùú∏||1+ùúâùëò(1+||ùùÄ‚òÖ
ùëù||1)+ùëÇ(‚àöÔ∏Å
log(ùëÄ)/ùëÄ)
where ùú∏=[ùõæ1,ùõæ2]ùëá;ùêøis the Lipschitz constant of ùëÉ‚òÖ(ùõæ1,ùõæ2);ùëòis a
small universal constant defined in Proposition 3 of Appendix B; and
ùùÄ‚òÖùëùis the optimal dual variable for a perturbed version of Eq. (6).
The duality gap that arises when solving the empirical prob-
lem presented in Eq. (9) is minimal when the fairness-aware ùëá-
invariance in Defn. 3 margin ùú∏is narrow, and the parametric space
closely approximates F.
Furthermore, we present the following theorem to establish an
upper bound on fairness within an unseen target domain.
Theorem 2 (Fairness Upper Bound of the Unseen Target
Domain). In accordance with Defn. 1 and Eq. (8), for any domain
ùëí‚ààE, the fairness dependence under instance distribution Pùëí
ùëãùëçùëåwith
respect to the classifier ùëì‚ààF is defined as:
ùúåùëí(ùëì)=EPùëí
ùëãùëçùëî(ùëì(ùëãùëí),ùëçùëí)
With observed source domains Eùë†, the dependence at any unseen
target domain ùë°‚àà E\Eùë†is upper bounded. ùëëùëñùë†ùë°[¬∑]is the Jensen-
Shannon distance metric [12].
ùúåùë°(ùëì)‚â§1
|Eùë†|‚àëÔ∏Å
ùë†ùëñ‚ààEùë†ùúåùë†ùëñ(ùëì)+‚àö
2 min
ùë†ùëñ‚ààEùë†ùëëùëñùë†ùë°
Pùë°
ùëãùëçùëå,Pùë†ùëñ
ùëãùëçùëå
+‚àö
2 max
ùë†ùëñ,ùë†ùëó‚ààEùë†ùëëùëñùë†ùë°
Pùë†ùëñ
ùëãùëçùëå,Pùë†ùëó
ùëãùëçùëå
whereùëëùëñùë†ùë°[P1,P2]=‚àöÔ∏É
1
2ùêæùêø(P1||P1+P2
2)+1
2ùêæùêø(P2||P1+P2
2)is JS
divergence defined based on KL divergence.
Notice that the second term in Theorem 2 becomes uncontrol-
lable during training as it relies on the unseen target domain. There-
fore, to preserve fairness across target domains, we aim to learn
semantic factors that map the transformation mode ùëá, ensuring
4424Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 3: Statistics summary of all datasets.
Datasets Domains Sensitive Attr. Labels (ùë†,ùúåùë†),‚àÄùë†‚ààEùë†
ccMNIST digit color background color digit label (R, 0.11), (G, 0.43), (B, 0.87)
FairFace race gender age(B, 0.91), (E, 0.87), (I, 0.58),
(L, 0.48), (M, 0.87), (S, 0.39), (W, 0.49)
YFCC100M-FDG year location in-,outdoor ( ùëë0, 0.73), (ùëë1, 0.84), (ùëë2, 0.72)
NYSF city race stop record(R, 0.93), (B, 0.85), (M, 0.81),
(Q, 0.98), (S, 0.88)
thatPùë†ùëñ
ùê∂|ùëãùëçùëå,‚àÄùë†ùëñ‚ààEùë†remains invariant across source domains.
Simultaneously, we strive for the classifier ùëìto achieve high fair-
ness within the source domains. Proofs of Theorems 1 and 2 are
provided in Appendices B and C.
6 EXPERIMENTS
Due to space limitations, we defer a detailed description of the ex-
perimental settings and comprehensive results to the arXiv version
of this paper, which can be accessed at https://arxiv.org/pdf/2311.
13816.
6.1 Settings
Datasets. We evaluate the performance of our FEDORA on four
benchmarks. To highlight each source data and its fair dependence
scoreùúåùë†defined in Assump. 1, we summarize the statistics in Tab. 3.
(1)ccMNIST is a domain generalization benchmark created by
colorizing digits and the backgrounds of the MNIST dataset [ 27].
ccMNIST consists of images of handwritten digits from 0 to 9. Simi-
lar to ColoredMNIST [3], for binary classification, digits are labeled
with 0 and 1 for digits from 0-4 and 5-9, respectively. ccMNIST
contains 70,000 images divided into three data domains, each char-
acterized by a different digit color (i.e., red, green, blue) and followed
by a different correlation between the class label and sensitive at-
tribute (digit background colors). (2)FairFace [23] is a dataset
that contains a balanced representation of different racial groups.
It includes 108,501 images from seven racial categories: Black (B),
East Asian (E), Indian (I), Latino (L), Middle Eastern (M), South-
east Asian (S), and White (W). In our experiments, we set each
racial group as a domain, gender as the sensitive attributes, and
age (‚â•or<50) as the class label. (3)YFCC100M-FDG is an image
dataset created by Yahoo Labs and released to the public in 2014. It
is randomly selected from the YFCC100M [49] dataset with a total
of 90,000 images. For domain variations, YFCC100M-FDG is divided
into three domains. Each contains 30,000 images from different
year ranges, before 1999 ( ùëë0), 2000 to 2009 ( ùëë1), and 2010 to 2014
(ùëë2). The outdoor or indoor tag is used as the binary class label for
each image. Latitude and longitude coordinates, representing where
images were taken, are translated into different continents. The
North American or non-North American continent is the sensitive
attribute (related to spatial disparity). (4)NYSF [25] is a real-world
dataset on policing in New York City in 2011. It documents whether
a pedestrian who was stopped on suspicion of weapon possession
would, in fact, possess a weapon. NYSF consists of records collected
in five different regions: Manhattan (M), Brooklyn (B), Queens (Q),
Bronx (R), and Staten (S). We use regions as different domains. This
data had a pronounced racial bias against African Americans, so
we consider race (black or non-black) as the sensitive attribute.Baselines. We compare the performance of FEDORA with 19
baseline methods that fall into two main categories: (1)12 state-of-
the-art domain generalizations methods, specifically designed to
address covariate shifts: ColorJitter, ERM [ 50], IRM [ 3], GDRO
[42], Mixup [ 55], MLDG [ 28], CORAL [ 47], MMD [ 29], DANN
[14], CDANN [ 30], DDG [ 57], and MBDG [ 40], where ColorJitter
is a naive function in PyTorch that randomly changes the bright-
ness, contrast, saturation and hue of images; and (2)7 state-of-the-
art fairness-aware domain generalizations methods, specifically
designed to address either covariate or dependence shifts: DDG-
FC, MBDG-FC, EIIL [ 8], FarconVAE [ 36], FCR [ 2], FTCS [ 41], and
FATDM [ 37], where DDG-FC and MBDG-FC are two baselines that
built upon DDG [ 57] and MBDG [ 40], respectively by straightfor-
wardly adding fairness constraints defined in Defn. 1 to the loss
functions of the original models.
Evaluation metrics. Three metrics are used for evaluation.
Two of them are for fairness quantification, Demographic Parity
(ùê∑ùëÉ) [11] and the Area Under the ROC Curve ( ùê¥ùëàùê∂ùëìùëéùëñùëü) between
predictions of sensitive subgroups [33].
‚Ä¢Demographic Parity (ùê∑ùëÉ) [11] is formalized as
DP=ùëò,if DP‚â§1;DP=1/ùëò,otherwise
whereùëò=P(ÀÜùëå=1|ùëç=‚àí1)/P(ÀÜùëå=1|ùëç=1)This is also known
as a lack of disparate impact [ 13]. A value closer to 1 indicates
fairness.
‚Ä¢The Area Under the ROC Curve (ùê¥ùëàùê∂ùëìùëéùëñùëü) [6] varies from zero to
one, and it is symmetric around 0.5, which represents random
predictability or zero bias effect on predictions.
ùê¥ùëàùê∂ùëìùëéùëñùëü=√ç
(xùëñ,ùëß=‚àí1,ùë¶ùëñ)‚ààD‚àí1√ç
(xùëó,ùëß=1,ùë¶ùëó)‚ààD 1ùêº P(ÀÜùë¶ùëñ=1)>P(ÀÜùë¶ùëó=1)
|D‚àí1|√ó|D 1|
where|D‚àí1|and|D1|represent sample size of subgroups ùëß=‚àí1
andùëß=1, respectively. ùêº(¬∑)is the indicator function that returns
1 when its argument is true and 0 otherwise.
Notice that the ùê¥ùëàùê∂ùëìùëéùëñùëü is not the same as the one commonly
used in classification based on TPR and FPR. The intuition behind
thisùê¥ùëàùê∂ùëìùëéùëñùëüis based on the nonparametric Mann-Whitney U test,
in which a fair condition is defined as the classifier‚Äôs prediction
probability of a randomly selected sample x‚àí1from one sensitive
subgroup being greater than a randomly selected sample x1from
the other sensitive subgroup is equal to the probability of x1being
greater than x‚àí1[6,58]. A value of ùê∑ùëÉcloser to 1indicates fairness,
and0.5ofùê¥ùëàùê∂ùëìùëéùëñùëürepresents zero bias effect on predictions.
Model selection. The model selection in domain generaliza-
tion is intrinsically a learning problem, followed by [ 40], we use
leave-one-domain-out validation criteria, which is one of the three
selection methods stated in [ 17]. Specifically, we evaluate FEDORA
on the held-out source domain and average the performance of
|Eùë†|‚àí1domains over the held-out one.
Hyperparameter Search We follow the same set of the MUNIT
[19] for the hyperparameters. More specifically, the learning rate is
0.0001, the number of iterations is 600000, and the batch size is 1.
The loss weights in learning ùëáare chosen from{1,5,10}. The se-
lected best ones are ùõΩ1=10,ùõΩ2=1,ùõΩ3=1,ùõΩ4=1. We monitor the
loss of the validation set and choose the ùõΩwith the lowest validation
loss. For the hyperparameters in learning the classifier ùëì, the learn-
ing rate is chosen from {0.000005,0.00001,0.00005,0.0001,0.0005}.
4425KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Chen Zhao et al.
Original Reconstruction Generated with random style and 
sensitive factors 
 Original Reconstruction Generated with random style and 
sensitive factors 
ccMNIST 
YFCC100M-FDG FairFace 
Figure 3: Visualizations for images under reconstruction and the transformation
modelùëáwith random style and sensitive factors.
Semantic factors 
(digits) Sens. factors 
(background colors) 
Style factors 
(digit colors) 
Outputs 
Figure 4: Example results of gener-
ating images using latent factors en-
coded from three images.
ùúÇis chosen from{0.01,0.05,0.1}.ùõæis chosen from{0.01,0.025,0.05}.
ùúÜis chosen from{0.1,1,10,20}. The batch size is chosen from
{22,64,80,128,512,1024,2048}. The numbers of iterations is chosen
from{500,1000,...,8000}on the ccMNIST andNYSF datasets. The
number of iterations are chosen from {300,600,...,7800,8000}on
theFairFace andYFCC100M-FDG datasets. The selected best ones
are: the learning rate is 0.00005,ùúÇ1=ùúÇ2=0.05,ùõæ1=ùõæ2=0.025,
ùúÜ1=ùúÜ2=1. The batch size on the ccMNIST andYFCC100M-FDG
datasets is 64, and it is 22on the FairFace dataset and 1024 on the
NYSF dataset. The number of iterations on the ccMNIST dataset is
3000,500,7000 for domains R, G, B, respectively. The number of iter-
ations on the FairFace dataset is 7200,7200,7800,8000,6600,7200,
6900 for domains B, E, I, L, M, S, W, respectively. The number
of iterations on the YFCC100M-FDG dataset is 7200,6000,6900 for
ùëë0,ùëë1,ùëë2, respectively. The number of iterations on the NYSF dataset
is500,3500, 4000,1500,8000 for domains R, B, M, Q, and S, respec-
tively. We monitor the accuracy and the value of fairness metrics
from the validation set and select the best ones. The grid space
of the grid search on all the baselines is the same as that of our
method.
6.2 Results
Data augmentation in synthetic domains via ùëá.We visualize
the augmented samples with random variations in Fig. 3. The first
column (Original) shows the images sampled from the datasets. In
the second column (Reconstruction), we display images generated
from latent factors encoded from the images in the first column.
The images in the second column closely resemble those in the
first column. Images in the last three columns are generated using
the semantic factors encoded from images in the first column, as-
sociated with style and sensitive factors randomly sampled from
their respective Gaussian distributions. The images in the last three
columns preserve the fundamental semantic information of the
corresponding samples in the first column. However, their style
and sensitive attributes undergo significant changes at random. The
generated images within synthetic domains enhance the classifier‚Äôs
generalization ( ùëì) to unseen source domains. This demonstrates
that the transformation model ùëáeffectively extracts latent factors
and produces diverse transformations of the provided data domains.
Effectiveness of ùëá.To further validate the effectiveness of ùëá,
drawing inspiration from [ 19], we train a separate transformationmodel for each domain. Subsequently, we generate an output image
by utilizing distinct latent factors from each domain. Using ccMNIST
as an example, we individually train three transformation models
{ùëáùëñ}3
ùëñ=1within each domain. Each ùëáùëñincludes unique encoders ùê∏ùëñùëê,
ùê∏ùëñùëé, andùê∏ùëñùë†. As shown in Fig. 4, an output image is generated through
ùê∫using a semantic factor (digit class, ùê∏1ùëê(x1)), a sensitive factor
(background color, ùê∏2ùëé(x2)), and a style factor (digit color, ùê∏3ùë†(x3))
from images in different domains. As a result, the output image is
constructed from the digit of x1, the background color of x2, and
the digit color of x3, with given variations. This suggests that the
augmented data with random variations in Fig. 3 for the synthetic
domain are not merely altering colors; instead, they are precisely
generated with unchanged semantics and random sensitive and
style factors.
The effectiveness of FEDORA across domains in terms of
predicted fairness and accuracy. Comprehensive experiments
showcase that FEDORA consistently outperforms baselines by a
considerable margin. For all tables in the main paper and Appen-
dix, results shown in each column represent performance on the
target domain, using the rest as source domains. Due to space limit,
selected results for three domains of FairFace are shown in Tab. 4,
but the average results are based on all domains. As shown in Tab. 4,
for the FairFace dataset, our method has the best accuracy and
fairness level for the average DG performance over all the domains.
More specifically, our method has better fairness metrics (3% for ùê∑ùëÉ,
2% forùê¥ùëàùê∂ùëìùëéùëñùëü) and comparable accuracy (0.19% better) than the
best of the baselines for individual metrics. As shown in Tab. 5, for
YFCC100M-FDG , our method excels in fairness metrics (8% for ùê∑ùëÉ,
4% forùê¥ùëàùê∂ùëìùëéùëñùëü) and comparable accuracy (0.35% better) compared
to the best baselines.
Ablation studies. We conduct three ablation studies to study
the robustness of FEDORA on FairFace . In-depth descriptions and
the pseudocodes for these studies and more results can be found in
the arXiv version of our paper at https://arxiv.org/pdf/2311.13816.
(1) In FEDORA w/o ùê∏ùëé, we modify the encoder within ùëáby restrict-
ing its output to only latent semantic and style factors. (2) FEDORA
w/oùëáskips data augmentation in synthetic domains via ùëáand
results are conducted only based ùëìconstrained by fair notions out-
lined in Defn. 1. (3) In FEDORA w/o Lùëìùëéùëñùëü, the fair constraint on
ùëìis not included, and we eliminate the Lùëìùëéùëñùëü in line 9 of Algo-
rithm 1. We include the performance of such ablation studies in
4426Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 4: Performance on FairFace. (bold is the best; underline is the second best).
ùê∑ùëÉ‚Üë/ùê¥ùëàùê∂ùëìùëéùëñùëü‚Üì/Accuracy‚Üë
Methods(B, 0.91) (W, 0.49) (L, 0.48) Avg
ColorJitter 0.64¬±0.26 / 0.64¬±0.15 / 93.47¬±1.56 0.34¬±0.09 / 0.64¬±0.02 / 92.07¬±0.55 0.39¬±0.10 / 0.70¬±0.02 / 91.77¬±0.61 0.42 / 0.66 / 92.94
ERM 0.67¬±0.17 / 0.58¬±0.02 / 91.89¬±1.10 0.39¬±0.09 / 0.61¬±0.01 / 92.82¬±0.38 0.57¬±0.15 / 0.62¬±0.01 / 91.96¬±0.51 0.51 / 0.61 / 93.08
IRM 0.63¬±0.12 / 0.58¬±0.01 / 93.39¬±1.03 0.32¬±0.19 / 0.66¬±0.01 / 90.54¬±1.56 0.41¬±.021 / 0.63¬±0.05 / 92.06¬±1.89 0.43 / 0.62 / 92.48
GDRO 0.71¬±0.16 / 0.57¬±0.02 / 89.81¬±1.10 0.48¬±0.09 / 0.60¬±0.01 / 92.50¬±0.38 0.54¬±0.15 / 0.62¬±0.01 / 91.59¬±0.51 0.55 / 0.60 / 92.55
Mixup 0.58¬±0.19 / 0.59¬±0.02 / 92.46¬±0.69 0.43¬±0.19 / 0.61¬±0.01 / 92.98 ¬±0.03 0.55¬±0.22 / 0.61¬±0.02 / 93.43¬±2.02 0.51 / 0.60 / 93.19
MLDG 0.63¬±0.25 / 0.58¬±0.02 / 92.71¬±2.36 0.47¬±0.20 / 0.59¬±0.01 / 92.82¬±1.65 0.53¬±0.18 / 0.62¬±0.03 / 92.99¬±0.86 0.51 / 0.60 / 93.39
CORAL 0.69¬±0.19 / 0.58¬±0.01 / 92.09¬±2.03 0.50¬±0.14 / 0.60¬±0.02 / 92.47¬±2.04 0.56¬±0.23 / 0.59¬±0.03 / 92.62¬±1.11 0.54 / 0.60 / 93.21
MMD 0.69¬±0.25 / 0.56¬±0.01 / 93.87 ¬±0.14 0.39¬±0.20 / 0.68¬±0.02 / 91.75¬±1.37 0.55¬±0.16 / 0.61¬±0.02 / 92.53¬±1.41 0.50 / 0.60 / 92.34
DANN 0.46¬±0.07 / 0.61¬±0.02 / 91.80¬±0.64 0.11¬±0.09 / 0.66¬±0.01 / 86.80¬±1.18 0.39¬±0.21 / 0.67¬±0.01 / 90.82¬±2.44 0.47 / 0.70 / 90.10
CDANN 0.62¬±0.24 / 0.59¬±0.03 / 91.22¬±0.33 0.35¬±0.17 / 0.67¬±0.02 / 90.19¬±0.60 0.42¬±0.23 / 0.61¬±0.03 / 92.42¬±2.19 0.43 / 0.66 / 91.48
DDG 0.60¬±0.20 / 0.59¬±0.02 / 91.76¬±1.03 0.51¬±0.07 / 0.60¬±0.01 / 91.34¬±0.80 0.44¬±0.17 / 0.62¬±0.02 / 93.46¬±0.32 0.49 / 0.61 / 92.74
MBDG 0.60¬±0.15 / 0.58¬±0.01 / 91.29¬±1.41 0.30¬±0.04 / 0.62¬±0.01 / 91.05¬±0.53 0.56¬±0.09 / 0.61¬±0.01 / 93.49 ¬±0.97 0.50 / 0.60 / 92.71
DDG-FC 0.61¬±0.06 / 0.58¬±0.03 / 92.27¬±1.65 0.48¬±0.15 / 0.62¬±0.02 / 92.45¬±1.55 0.50¬±0.25 / 0.62¬±0.03 / 92.42¬±0.30 0.52 / 0.61 / 93.23
MBDG-FC 0.70¬±0.15 / 0.56¬±0.03 / 92.12¬±0.43 0.32¬±0.07 / 0.60¬±0.03 / 91.50¬±0.57 0.57¬±0.23 / 0.62¬±0.02 / 91.89¬±0.81 0.53 / 0.60 / 92.48
EIIL 0.88¬±0.07 / 0.59¬±0.05 / 84.75¬±2.16 0.46¬±0.05 / 0.65¬±0.03 / 86.53¬±1.02 0.49¬±0.07 / 0.59¬±0.01 / 88.39¬±1.25 0.64 / 0.61 / 87.78
FarconVAE 0.93¬±0.03 / 0.54¬±0.01 / 89.61¬±0.64 0.51¬±0.07 / 0.60¬±0.01 / 86.40¬±0.42 0.58¬±0.05 / 0.60¬±0.05 / 88.70¬±0.71 0.66 / 0.58 / 88.46
FCR 0.81¬±0.05 / 0.59¬±0.02 / 79.66¬±0.25 0.39¬±0.06 / 0.63¬±0.02 / 82.33¬±0.89 0.38¬±0.12 / 0.66¬±0.02 / 85.22¬±2.33 0.54 / 0.63 / 83.68
FTCS 0.75¬±0.10 / 0.60¬±0.02 / 80.00¬±0.20 0.40¬±0.06 / 0.60¬±0.02 / 79.66¬±1.05 0.42¬±0.23 / 0.65¬±0.03 / 79.64¬±1.00 0.57 / 0.64 / 80.91
FATDM 0.93¬±0.03 / 0.57¬±0.02 / 92.20¬±0.36 0.46¬±0.05 / 0.63¬±0.01 / 92.56¬±0.31 0.51¬±0.16 / 0.63¬±0.02 / 93.33¬±0.20 0.67/ 0.61 / 92.54
FEDORA 0.94¬±0.05 / 0.55¬±0.02 / 93.91¬±0.33 0.52¬±0.17 / 0.58¬±0.03 / 93.02¬±0.50 0.58¬±0.15 / 0.59¬±0.01 / 93.73¬±0.26 0.70 /0.58 /93.42
0.400.450.500.550.600.650.700.75DP
FEDORA
w/o E_a
w/o T
w/o L_fair
0.560.580.600.620.640.66AUC_fair
91.5091.7592.0092.2592.5092.7593.0093.2593.50AccuracyFigure 5: Ablation study on FairFace .
Averaged results are plotted across all
domains.
Table 5: Performance on YFCC100M-FDG. (Bold is the best; underline is the second best.)
ùê∑ùëÉ‚Üë/ùê¥ùëàùê∂ùëìùëéùëñùëü‚Üì/Accuracy‚Üë
Methods(ùëë0, 0.73) (ùëë1, 0.84) (ùëë2, 0.72) Avg
ColorJitter 0.67¬±0.06 / 0.57¬±0.02 / 57.47¬±1.20 0.67¬±0.34 / 0.61¬±0.01 / 82.43¬±1.25 0.65¬±0.21 / 0.64¬±0.02 / 87.88¬±0.35 0.66 / 0.61 / 75.93
ERM 0.81¬±0.09 / 0.58¬±0.01 / 40.51¬±0.23 0.71¬±0.18 / 0.66¬±0.03 / 83.91¬±0.33 0.89¬±0.08 / 0.59¬±0.01 / 82.06¬±0.33 0.80 / 0.61 / 68.83
IRM 0.76¬±0.10 / 0.58¬±0.02 / 50.51¬±2.44 0.87¬±0.08 / 0.60¬±0.02 / 73.26¬±0.03 0.70¬±0.24 / 0.57¬±0.02 / 82.78¬±2.19 0.78 / 0.58 / 68.85
GDRO 0.80¬±0.05 / 0.59¬±0.01 / 53.43¬±2.29 0.73¬±0.22 / 0.60¬±0.01 / 87.56¬±2.20 0.79¬±0.13 / 0.65¬±0.02 / 83.10¬±0.64 0.78 / 0.62 / 74.70
Mixup 0.82¬±0.07 / 0.57¬±0.03 / 61.15¬±0.28 0.79¬±0.14 / 0.63¬±0.03 / 78.63¬±0.97 0.89¬±0.05 / 0.60¬±0.01 / 85.18¬±0.80 0.84/ 0.60 / 74.99
MLDG 0.75¬±0.13 / 0.67¬±0.01 / 49.56¬±0.69 0.71¬±0.19 / 0.57¬±0.02 / 89.45¬±0.44 0.71¬±0.14 / 0.57¬±0.03 / 87.51¬±0.18 0.72 / 0.60 / 75.51
CORAL 0.80¬±0.11 / 0.58¬±0.02 / 58.96¬±2.34 0.72¬±0.11 / 0.64¬±0.03 / 91.66¬±0.85 0.70¬±0.07 / 0.64¬±0.03 / 89.28¬±1.77 0.74 / 0.62 / 79.97
MMD 0.79¬±0.11 / 0.59¬±0.02 / 61.51¬±1.79 0.71¬±0.15 / 0.64¬±0.03 / 91.15¬±2.33 0.79¬±0.17 / 0.60¬±0.01 / 86.69¬±0.19 0.76 / 0.61 / 79.87
DANN 0.70¬±0.13 / 0.78¬±0.02 / 47.71¬±1.56 0.79¬±0.12 / 0.53¬±0.01 / 84.80¬±1.14 0.77¬±0.17 / 0.59¬±0.02 / 58.50¬±1.74 0.75 / 0.64 / 63.67
CDANN 0.74¬±0.13 / 0.58¬±0.02 / 55.87¬±2.09 0.70¬±0.22 / 0.65¬±0.02 / 87.06¬±2.43 0.72¬±0.13 / 0.63¬±0.02 / 85.76¬±2.43 0.72 / 0.62 / 76.23
DDG 0.81¬±0.14 / 0.57¬±0.03 / 60.08¬±1.08 0.74¬±0.12 / 0.66¬±0.03 / 92.53¬±0.91 0.71¬±0.21 / 0.59¬±0.03 / 95.02¬±1.92 0.75 / 0.61 / 82.54
MBDG 0.79¬±0.15 / 0.58¬±0.01 / 60.46¬±1.90 0.73¬±0.07 / 0.67¬±0.01 / 94.36¬±0.23 0.71¬±0.11 / 0.59¬±0.03 / 93.48 ¬±0.65 0.74 / 0.61 / 82.77
DDG-FC 0.76¬±0.06 / 0.58¬±0.03 / 59.96¬±2.36 0.83¬±0.06 / 0.58¬±0.01 / 96.80¬±1.28 0.82¬±0.09 / 0.59¬±0.01 / 86.38¬±2.45 0.80 / 0.58 / 81.04
MBDG-FC 0.80¬±0.13 / 0.58¬±0.01 / 62.31 ¬±0.13 0.72¬±0.09 / 0.63¬±0.01 / 94.73 ¬±2.09 0.80¬±0.07 / 0.53¬±0.01 / 87.78¬±2.11 0.77 / 0.58 / 81.61
EIIL 0.87¬±0.11 / 0.55¬±0.02 / 56.74¬±0.60 0.76¬±0.05 / 0.54¬±0.03 / 68.99¬±0.91 0.87¬±0.06 / 0.78¬±0.03 / 72.19¬±0.75 0.83 / 0.62 / 65.98
FarconVAE 0.67¬±0.06 / 0.61¬±0.03 / 51.21¬±0.61 0.90¬±0.06 / 0.59¬±0.01 / 72.40¬±2.13 0.85¬±0.12 / 0.55¬±0.01 / 74.20¬±2.46 0.81 / 0.58 / 65.93
FCR 0.62¬±0.21 / 0.70¬±0.01 / 55.32¬±0.04 0.63¬±0.14 / 0.66¬±0.10 / 70.89¬±0.22 0.66¬±0.30 / 0.78¬±0.02 / 70.58¬±0.23 0.64 / 0.71 / 65.60
FTCS 0.72¬±0.03 / 0.60¬±0.01 / 60.21¬±0.10 0.79¬±0.02 / 0.59¬±0.01 / 79.96¬±0.05 0.69¬±0.10 / 0.60¬±0.06 / 72.99¬±0.50 0.73 / 0.60 / 71.05
FATDM 0.80¬±0.10 / 0.55¬±0.01 / 61.56¬±0.89 0.88¬±0.08 / 0.56¬±0.01 / 90.00¬±0.66 0.86¬±0.10 / 0.60¬±0.02 / 89.12¬±1.30 0.84/ 0.57 / 80.22
FEDORA 0.87¬±0.09 / 0.53¬±0.01 / 62.56¬±2.25 0.94¬±0.05 / 0.52¬±0.01 / 93.36¬±1.70 0.93¬±0.03 / 0.53¬±0.02 / 93.43¬±0.73 0.92 /0.53 /83.12
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Fairness (DP)86889092949698Accuracy (Avg)Accuracy-Fairness Tradeoff (FairFace)
FEDORA
MBDG-FC
DDG-FC
0.5 0.6 0.7 0.8 0.9 1.0
Fairness (DP)65707580859095Accuracy (Avg)Accuracy-Fairness Tradeoff (YFCC100M-FDG)
FEDORA
MBDG-FC
DDG-FCFigure 6: Results of accuracy-fairness
tradeoff on Fairface (left) and
YFCC100M-FDG (right) sweeping over a
range ofùúÜ2.
Fig. 5. The results illustrate that when data is disentangled into
three factors, and the model is designed accordingly, it can enhance
generalization performance due to covariate and dependence shifts.
Generating data in synthetic domains with random fairness de-
pendence patterns proves to be an effective approach for ensuring
fairness invariance across domains.
Fairness-accuracy tradeoff. In our Algorithm 1, because ùúÜ2
(lines 10 and 12) is the parameter that regularizes the fair loss, we
conduct additional experiments to show the change of tradeoffs
between accuracy and fairness sweeping over a range of ùúÜ2‚àà
[0.01,0.05,0.1,1,10]. Our results show that the larger (small) ùúÜ2,
the better(worse) model fairness for each domain as well as in
average, but it gives worse (better) model accuracy. Evaluation on
FairFace andYFCC100M-FDG is given in Fig. 6. Results in the top-
right of the figure indicate good performance. This result is plotted
on the average performance over all target domains.
7 CONCLUSION
In this paper, we introduce a novel approach designed to tackle
the challenges of domain generalization when confronted with co-
variate shift and dependence shift simultaneously. In our pursuit
of learning a fairness-aware invariant classifier, we assert the exis-
tence of an underlying transformation model that can transforminstances across domains. This model plays a crucial role in achiev-
ing fairness-aware domain generalization by generating samples
in synthetic domains characterized by novel data styles and fair de-
pendence patterns. We present a tractable algorithm and showcase
its effectiveness through comprehensive analyses and exhaustive
empirical studies.
ACKNOWLEDGMENTS
This research was supported by the National Science Foundation
under grant numbers 2147375, IIS-2107449, and IIS-1954376, and
in part by the National Center for Transportation Cybersecurity
and Resiliency (TraCR), a U.S. Department of Transportation Na-
tional University Transportation Center headquartered at Clemson
University, Clemson, South Carolina, USA. Any opinions, findings,
conclusions, and recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of
TraCR, and the U.S. Government assumes no liability for the con-
tents or use thereof.
4427KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Chen Zhao et al.
REFERENCES
[1]Sergio Alonso, Rosana Montes, Daniel Molina, Iv√°n Palomares, Eugenio Mart√≠nez-
C√°mara, Manuel Chiachio, Juan Chiachio, Francisco J Melero, Pablo Garc√≠a-
Moral, B√°rbara Fern√°ndez, et al .2021. Ordering artificial intelligence based
recommendations to tackle the sdgs with a decision-making model based on
surveys. Sustainability 13, 11 (2021), 6038.
[2]Bang An, Zora Che, Mucong Ding, and Furong Huang. 2022. Transferring fairness
under distribution shifts via fair consistency regularization. Advances in Neural
Information Processing Systems 35 (2022), 32582‚Äì32597.
[3]Martin Arjovsky, L√©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[4]Arpita Biswas and Suvam Mukherjee. 2021. Ensuring fairness under prior proba-
bility shifts. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and
Society. 414‚Äì424.
[5]Gilles Blanchard, Gyemin Lee, and Clayton Scott. 2011. Generalizing from sev-
eral related classification tasks to a new unlabeled sample. Advances in neural
information processing systems 24 (2011).
[6]Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and Xiangliang Zhang.
2013. Controlling Attribute Effect in Linear Regression. ICDM (2013).
[7]Yatong Chen, Reilly Raab, Jialu Wang, and Yang Liu. 2022. Fairness transferability
subject to bounded distribution shift. Advances in Neural Information Processing
Systems 35 (2022), 11266‚Äì11278.
[8]Elliot Creager, J√∂rn-Henrik Jacobsen, and Richard Zemel. 2021. Environment
inference for invariant learning. In International Conference on Machine Learning.
PMLR, 2189‚Äì2200.
[9]Elliot Creager, David Madras, Toniann Pitassi, and Richard Zemel. 2020. Causal
modeling for fairness in dynamical systems. In International conference on machine
learning. PMLR, 2185‚Äì2195.
[10] Wei Du and Xintao Wu. 2021. Fair and robust classification under sample selection
bias. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management. 2999‚Äì3003.
[11] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel.
2011. Fairness Through Awareness. CoRR (2011).
[12] Dominik Maria Endres and Johannes E Schindelin. 2003. A new metric for
probability distributions. IEEE Transactions on Information theory 49, 7 (2003),
1858‚Äì1860.
[13] Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, and Suresh
Venkatasubramanian. 2015. Certifying and Removing Disparate Impact. KDD
(2015).
[14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, Fran√ßois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The journal of machine learning
research 17, 1 (2016), 2096‚Äì2030.
[15] Stephen Giguere, Blossom Metevier, Yuriy Brun, Bruno Castro da Silva, Philip S
Thomas, and Scott Niekum. 2022. Fairness guarantees under demographic shift.
InProceedings of the 10th International Conference on Learning Representations
(ICLR).
[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial
networks. Commun. ACM 63, 11 (2020), 139‚Äì144.
[17] Ishaan Gulrajani and David Lopez-Paz. 2020. In search of lost domain generaliza-
tion. arXiv preprint arXiv:2007.01434 (2020).
[18] Xiao Han, Lu Zhang, Yongkai Wu, and Shuhan Yuan. 2023. Achieving Counter-
factual Fairness for Anomaly Detection. In Pacific-Asia Conference on Knowledge
Discovery and Data Mining. Springer, 55‚Äì66.
[19] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. 2018. Multimodal un-
supervised image-to-image translation. In Proceedings of the European conference
on computer vision (ECCV). 172‚Äì189.
[20] Vasileios Iosifidis and Eirini Ntoutsi. 2020. FABBOO-Online Fairness-Aware
Learning Under Class Imbalance. In International Conference on Discovery Science.
Springer, 159‚Äì174.
[21] Vasileios Iosifidis, Thi Ngoc Han Tran, and Eirini Ntoutsi. 2019. Fairness-
enhancing interventions in stream classification. In Database and Expert Systems
Applications: 30th International Conference, DEXA 2019, Linz, Austria, August
26‚Äì29, 2019, Proceedings, Part I 30. Springer, 261‚Äì276.
[22] Nathan Kallus and Angela Zhou. 2018. Residual unfairness in fair machine
learning from prejudiced data. In International Conference on Machine Learning.
PMLR, 2439‚Äì2448.
[23] Kimmo Karkkainen and Jungseock Joo. 2021. FairFace: Face Attribute Dataset
for Balanced Race, Gender, and Age for Bias Measurement and Mitigation. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
(WACV). 1548‚Äì1558.
[24] Gill Kirton. 2019. Unions and equality: 50 years on from the fight for fair pay at
Dagenham. Employee Relations: The International Journal 41, 2 (2019), 344‚Äì356.
[25] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang,
Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips,
Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw,Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson,
Sergey Levine, Chelsea Finn, and Percy Liang. 2021. WILDS: A Benchmark of
in-the-Wild Distribution Shifts. In ICML.
[26] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. 2021. Out-of-
distribution generalization via risk extrapolation (rex). In International Conference
on Machine Learning. PMLR, 5815‚Äì5826.
[27] Yann LeCun, L√©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278‚Äì
2324.
[28] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. 2018. Learning to
generalize: Meta-learning for domain generalization. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[29] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. 2018. Domain gener-
alization with adversarial feature learning. In Proceedings of the IEEE conference
on computer vision and pattern recognition. 5400‚Äì5409.
[30] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang,
and Dacheng Tao. 2018. Deep domain generalization via conditional invariant
adversarial networks. In Proceedings of the European Conference on Computer
Vision (ECCV). 624‚Äì639.
[31] Yujie Lin, Dong Li, Chen Zhao, Xintao Wu, Qin Tian, and Minglai Shao. 2024.
Supervised Algorithmic Fairness in Distribution Shifts: A Survey. arXiv preprint
arXiv:2402.01327 (2024).
[32] Yujie Lin, Chen Zhao, Minglai Shao, Baoluo Meng, Xujiang Zhao, and Haifeng
Chen. 2023. Pursuing Counterfactual Fairness via Sequential Autoencoder Across
Domains. ArXiv:2309.13005 (2023).
[33] Charles X Ling, Jin Huang, Harry Zhang, et al .2003. AUC: a statistically consistent
and more discriminating measure than accuracy. In Ijcai, Vol. 3. 519‚Äì524.
[34] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. 2017. Unsupervised image-to-image
translation networks. Advances in neural information processing systems 30 (2017).
[35] Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg. 2020. Too Relaxed to
Be Fair. In ICML.
[36] Changdae Oh, Heeji Won, Junhyuk So, Taero Kim, Yewon Kim, Hosik Choi,
and Kyungwoo Song. 2022. Learning Fair Representation via Distributional
Contrastive Disentanglement. In Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 1295‚Äì1305.
[37] Thai-Hoang Pham, Xueru Zhang, and Ping Zhang. 2023. Fairness and accuracy
under domain generalization. Proceedings of the International Conference on
Learning Representations (2023).
[38] Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian Ziebart. 2020. Fairness
for robust log loss classification. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 34. 5511‚Äì5518.
[39] Ashkan Rezaei, Anqi Liu, Omid Memarrast, and Brian D Ziebart. 2021. Robust
fairness under covariate shift. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 9419‚Äì9427.
[40] Alexander Robey, George J Pappas, and Hamed Hassani. 2021. Model-based
domain generalization. Advances in Neural Information Processing Systems 34
(2021), 20210‚Äì20229.
[41] Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. 2023. Im-
proving fair training under correlation shifts. ICML (2023).
[42] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. 2020.
Distributionally Robust Neural Networks. International Conference on Learning
Representations (2020).
[43] Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva
Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu,
Christina Chen, et al .2022. Diagnosing failures of fairness transfer across dis-
tribution shift in real-world medical settings. Advances in Neural Information
Processing Systems 35 (2022), 19304‚Äì19318.
[44] Candice Schumann, Xuezhi Wang, Alex Beutel, Jilin Chen, Hai Qian, and Ed H
Chi. 2019. Transfer of machine learning fairness across domains. arXiv preprint
arXiv:1906.09688 (2019).
[45] Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate
shift by weighting the log-likelihood function. Journal of statistical planning and
inference 90, 2 (2000), 227‚Äì244.
[46] Harvineet Singh, Rina Singh, Vishwali Mhasawade, and Rumi Chunara. 2021.
Fairness violations and mitigation under covariate shift. In Proceedings of the
2021 ACM Conference on Fairness, Accountability, and Transparency. 3‚Äì13.
[47] Baochen Sun and Kate Saenko. 2016. Deep coral: Correlation alignment for deep
domain adaptation. In European conference on computer vision. Springer, 443‚Äì450.
[48] Bahar Taskesen, Viet Anh Nguyen, Daniel Kuhn, and Jose Blanchet. 2020. A distri-
butionally robust approach to fair classification. arXiv preprint arXiv:2007.09530
(2020).
[49] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,
Douglas Poland, Damian Borth, and Li-Jia Li. 2016. YFCC100M: The new data in
multimedia research. Commun. ACM 59, 2 (2016), 64‚Äì73.
[50] Vladimir Vapnik. 1999. The nature of statistical learning theory. Springer science
& business media.
4428Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
[51] Riccardo Volpi, Diane Larlus, and Gr√©gory Rogez. 2021. Continual adaptation
of visual representations via domain randomization and meta-learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
4443‚Äì4453.
[52] Ke Wang, Senqiang Zhou, Chee Ada Fu, and Jeffrey Xu Yu. 2003. Mining changes
of classification by correspondence tracing. In Proceedings of the 2003 SIAM
International Conference on Data Mining. SIAM, 95‚Äì106.
[53] Gerhard Widmer and Miroslav Kubat. 1996. Learning in the presence of concept
drift and hidden contexts. Machine learning 23 (1996), 69‚Äì101.
[54] Yongkai Wu, Lu Zhang, and Xintao Wu. 2019. On Convexity and Bounds of
Fairness-aware Classification. WWW.
[55] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. 2020. Improve unsu-
pervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677
(2020).
[56] Richard Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork.
2013. Learning Fair Representations. ICML (2013).
[57] Hanlin Zhang, Yi-Fan Zhang, Weiyang Liu, Adrian Weller, Bernhard Sch√∂lkopf,
and Eric P Xing. 2022. Towards principled disentanglement for domain general-
ization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 8024‚Äì8034.
[58] Chen Zhao and Feng Chen. 2019. Rank-Based Multi-task Learning For Fair
Regression. IEEE International Conference on Data Mining (ICDM) (2019).
[59] Chen Zhao and Feng Chen. 2020. Unfairness Discovery and Prevention For
Few-Shot Regression. ICKG (2020).
[60] Chen Zhao, Feng Chen, and Bhavani Thuraisingham. 2021. Fairness-Aware
Online Meta-learning. ACM SIGKDD (2021).
[61] Chen Zhao, Changbin Li, Jincheng Li, and Feng Chen. 2020. Fair Meta-Learning
For Few-Shot Classification. ICKG (2020).
[62] Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, and Feng Chen. 2022.
Adaptive Fairness-Aware Online Meta-Learning for Changing Environments. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 2565‚Äì2575. https://doi.org/10.1145/3534678.3539420
[63] Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, and Feng Chen. 2024.
Dynamic Environment Responsive Online Meta-Learning with Fairness Aware-
ness. ACM Transactions on Knowledge Discovery from Data 18, 6, Article 153
(2024), 23 pages. https://doi.org/10.1145/3648684
[64] Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Christan Grant, and
Feng Chen. 2023. Towards Fair Disentangled Online Learning for Changing
Environments. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 3480‚Äì3491.
[65] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. 2020. Learn-
ing to generate novel domains for domain generalization. In Computer Vision‚Äì
ECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceed-
ings, Part XVI 16. Springer, 561‚Äì578.
A DETAILS OF LEARNING THE
TRANSFORMATION MODEL
For simplicity, we denote the transformation model ùëáconsisting
of three encoders ùê∏ùëê,ùê∏ùëé,ùê∏ùë†, and a decoder ùê∫. However, in prac-
tice, we consider a bi-level auto-encoder (see Fig. 7), wherein an
additional content encoder ùê∏ùëö:X‚ÜíM takes data as input and
outputs a content factor. Furthermore, the decoder ùê∫used in the
main paper is renamed ùê∫ùëú. Specifically, the inner level decoder is
denoted asùê∫ùëñ:C√óA‚ÜíM . As a consequence, the transforma-
tion modelùëáconsists of encoders ùê∏={ùê∏ùëö,ùê∏ùë†,ùê∏ùëê,ùê∏ùëé}and decoders
ùê∫={ùê∫ùëñ,ùê∫ùëú}.
Specifically, in the outer level, an instance is first encoded to
a content factor m‚ààM and a style factor s‚ààS through the
corresponding encoders ùê∏ùëöandùê∏ùë†, respectively. In the inner level,
the content factor mis further encoded to a content factor c‚ààCand
a sensitive factor a‚ààA, through encoders ùê∏ùëêandùê∏ùëé. Therefore, the
bidirectional reconstruction loss and the sensitiveness loss stated
in Sec. 4 are reformulated.
Lùëëùëéùë°ùëé
ùëüùëíùëêùëúùëõ =Exùë†‚àºPùë†
ùëãùê∫ùëú ÀÜm,ùê∏ùë†(xùë†)‚àíxùë†
1
+Em‚àºPùëÄùê∫ùëñ ùê∏ùëê(m),ùê∏ùëé(m)‚àím
1Algorithm 2 Learning the Transformation Model ùëá.
Require: learning rate ùõº1,ùõº2,ùõº3, initial coefficients ùõΩ1,ùõΩ2,ùõΩ3,ùõΩ4.
Initialize: Parameter of encoders {ùúΩùëö,ùúΩùë†,ùúΩùëê,ùúΩùëé}, decoders{ùùìùëñ,ùùìùëú}, sensitive
classifier ùúΩùëß, and discriminators {ùùçùëñ,ùùçùëú}.
1:repeat
2: forminibatch{(xùëñ,ùë¶ùëñ,ùëßùëñ)}ùëû
ùëñ=1‚ààDùë†do
3: Compute Lùë°ùëúùë°ùëéùëô for Stage 1 using Eq. (10).
4: ùùçùëú,ùùçùëñ‚ÜêAdam(ùõΩ4Lùëéùëëùë£,ùùçùëú,ùùçùëñ,ùõº1)
5: ùúΩùëö,ùúΩùëê,ùúΩùë†,ùúΩùëé,ùùìùëú,ùùìùëñ‚Üê Adam ùõΩ1Lùëëùëéùë°ùëé
ùëüùëíùëêùëúùëõ+
ùõΩ2Lùëìùëéùëêùë°ùëúùëü
ùëüùëíùëêùëúùëõ,ùúΩùëö,ùúΩùëê,ùúΩùë†,ùúΩùëé,ùùìùëú,ùùìùëñ,ùõº2
6: ùúΩùëß‚ÜêAdam(ùõΩ3Lùë†ùëíùëõùë†,ùúΩùëß,ùõº3)
7: end for
8:until convergence
9:Return{ùúΩùëö,ùúΩùë†,ùúΩùëê,ùúΩùëé,ùúΩùëß,ùùìùëñ,ùùìùëú}
where ÀÜm=ùê∫ùëñ(c,a)=ùê∫ùëñ ùê∏ùëê(ùê∏ùëö(xùë†)),ùê∏ùëé(ùê∏ùëö(xùë†));PùëÄis given
bym=ùê∏ùëö(xùë†).
Lùëìùëéùëêùë°ùëúùëü
ùëüùëíùëêùëúùëõ =Ec‚àºPùê∂,a‚àºN( 0,Iùëé)ùê∏ùëê ùê∫ùëñ(c,a)‚àíc
1
+Ec‚àºPùê∂,a‚àºN( 0,Iùëé)ùê∏ùëé ùê∫ùëñ(c,a)‚àía
1
+Em‚àºPùëÄ,sùë†‚àºN( 0,Iùë†)ùê∏ùë†(ùê∫ùëú(m,s))‚àí s
1
+Ec‚àºPùê∂,sùë†‚àºN( 0,Iùë†),a‚àºN( 0,Iùëé)ùê∏ùë† ùê∫ùëú(ùê∫ùëñ(c,a),s)‚àís
1
+Em‚àºPùëÄ,sùë†‚àºN( 0,Iùë†)ùê∏ùëö ùê∫ùëú(m,s)‚àím
1
where Pùê∂andPùëÄare given by c=ùê∏ùëê(ùê∏ùëö(xùë†))andm=ùê∏ùëö(xùë†).
a=ùê∏ùëé(ùê∏ùëö(xùë†)), and s=ùê∏ùë†(xùë†).
Lùë†ùëíùëõùë†=ùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶(ùëßùë†,‚Ñé(ùê∏ùëé(ùê∏ùëö(xùë†))))
Additionally, motivated by the observation that GANs [ 16] can
improve data quality for evaluating the disentanglement effect in
the latent spaces, we use GANs to match the distribution of recon-
structed data to the same distribution. Followed by [ 19], data and
semantic factors generated through encoders and decoders should
be indistinguishable from the given ones in the same domain.
Lùëéùëëùë£=Ec‚àºPùê∂,sùë†‚àºN( 0,Iùë†),a‚àºN( 0,Iùëé)
log 1‚àíùê∑ùëú(ùê∫ùëú(ÀÜm,sùë†))
+Exùë†‚àºPùë†
ùëã
logùê∑ùëú(xùë†)
+Ec‚àºPùê∂,a‚àºN( 0,Iùëé)
log 1‚àíùê∑ùëñ(ùê∫ùëñ(c,a))
+Em‚àºPùëÄ
logùê∑ùëñ(m)
whereùê∑ùëú:X‚Üí Randùê∑ùëñ:M‚Üí Rare the discriminators for
the outer and inner levels, respectively.
Total Loss. We jointly train the encoders, decoders, and discrim-
inators to optimize the final objective, a weighted sum of the three
loss terms.
min
ùê∏ùëö,ùê∏ùë†,ùê∏ùëê,ùê∏ùëé,ùê∫ùëñ,ùê∫ùëúmax
ùê∑ùëñ,ùê∑ùëúùõΩ1Lùëëùëéùë°ùëé
ùëüùëíùëêùëúùëõ+ùõΩ2Lùëìùëéùëêùë°ùëúùëü
ùëüùëíùëêùëúùëõ+ùõΩ3Lùë†ùëíùëõùë†+ùõΩ4Lùëéùëëùë£
(10)
whereùõΩ1,ùõΩ2,ùõΩ3,ùõΩ4>0are hyperparameters that control the im-
portance of each loss term. To optimize, the learning algorithm is
given in Algorithm 2.
B SKETCH PROOF OF THEOREM 1
Before we prove Theorem 1, we first make the following proposi-
tions and assumptions.
Proposition 1. Letùëë‚Ä≤be a distance metric between probability
measures for which it holds that ùëë‚Ä≤[P,T]=0for two distributions P
andTif and only if P=Talmost surely. Then ùëÉ‚òÖ(0,0)=ùëÉ‚òÖ
4429KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Chen Zhao et al.
Inner Level 
Outer Level 
Figure 7: A two-level approach for learning the transforma-
tion model ùëá.
Proposition 2. Assuming the perturbation function ùëÉ‚òÖ(ùõæ1,ùõæ2)is
ùêø-lipschitz continuous in ùõæ1,ùõæ2. Then given Proposition 1, it follows
that|ùëÉ‚òÖ‚àíùëÉ‚òÖ(ùõæ1,ùõæ2)|‚â§ùêø||ùú∏||1, where ùú∏=[ùõæ1,ùõæ2]ùëá.
Definition 4. LetŒò‚´ÖRùëùbe a finite-dimensional parameter
space. Forùúâ>0, a function ÀÜùëì:X√óŒò‚ÜíY is said to be an ùúâ-
parameterization of Fif it holds that for each ùëì‚ààF, there exists
a parameter ùúΩ‚ààŒòsuch that EPùëã‚à•ÀÜùëì(x,ùúΩ)‚àíùëì(x)‚à•‚àû‚â§ùúâ. Given
anùúâ-parameterization ÀÜùëìofF, consider the following saddle-point
problem:
ùê∑‚òÖ
ùúâ(ùõæ1,ùõæ2)
‚âú max
ùúÜ1(ùë†ùëñ,ùë†ùëó),ùúÜ2(ùë†ùëñ,ùë†ùëó)min
ùúΩ‚ààŒòùëÖ(ùúΩ)+‚à´
ùë†ùëñ,ùë†ùëó‚ààEùë†[ùõøùë†ùëñ,ùë†ùëó(ùúΩ)‚àíùõæ1]dùúÜ1(ùë†ùëñ,ùë†ùëó)
+‚à´
ùë†ùëñ,ùë†ùëó‚ààEùë†[ùúåùë†ùëñ(ùúΩ)+ùúåùë†ùëó(ùúΩ)‚àíùõæ2]dùúÜ2(ùë†ùëñ,ùë†ùëó)
whereùëÖ(ùúΩ)=ùëÖ(ÀÜùëì(¬∑,ùúΩ))andLùë†ùëñ,ùë†ùëó(ùúΩ)=Lùë†ùëñ,ùë†ùëó(ÀÜùëì(¬∑,ùúΩ)).
Assumption 3. The loss function ‚Ñìis non-negative, convex, and
ùêø‚Ñì-Lipschitz continuous in its first argument,
|‚Ñì(ùëì1(x),ùë¶)‚àí‚Ñì(ùëì2(x),ùë¶)|‚â§‚à•ùëì1(x)‚àíùëì2(x)‚à•‚àû
Assumption 4. The distance metric ùëëis non-negative, convex,
and satisfies the following uniform Lipschitz-like inequality for some
constantùêøùëë>0:
|ùëë[ùëì1(x),ùëì1(x‚Ä≤=ùëá(x,ùëß,ùë†))]‚àíùëë[ùëì2(x),ùëì2(x‚Ä≤=ùëá(x,ùëß,ùë†))]|
‚â§ùêøùëë‚à•ùëì1(x)‚àíùëì2(x)‚à•‚àû,‚àÄùë†‚ààEùë†
Assumption 5. The fairness metric ùëîis non-negative, convex,
and satisfies the following uniform Lipschitz-like inequality for some
constantùêøùëî>0:
|(ùëî‚ó¶ùëì1)(x,ùëß)‚àí(ùëî‚ó¶ùëì2)(x,ùëß)|‚â§ùêøùëî‚à•ùëì1(x)‚àíùëì2(x)‚à•‚àû
Assumption 6. There exists a parameter ùúΩ‚ààŒòsuch thatùõøùë†ùëñ,ùë†ùëó(ùúΩ)<
ùõæ1‚àíùúâ¬∑max{ùêø‚Ñì,ùêøùëë}andùúåùë†ùëñ(ùúΩ)+ùúåùë†ùëó(ùúΩ)<ùõæ2‚àíùúâ¬∑max{ùêø‚Ñì,ùêøùëî},‚àÄùë†ùëñ,ùë†ùëó‚àà
Eùë†
Proposition 3. Letùõæ1,ùõæ2>0be given. With the assumptions
above, it holds that
ùëÉ‚òÖ(ùõæ1,ùõæ2)‚â§ùê∑‚òÖ
ùúâ(ùõæ1,ùõæ2)‚â§ùëÉ‚òÖ(ùõæ1,ùõæ2)+ùúâ(1+‚à•ùúÜ‚òÖ
ùëù‚à•1)¬∑ùëò
whereùúÜ‚òÖùëùis the optimal dual variable for a perturbed version of
Eq.(6)in which the constraints are tightened to hold with margin
ùõæ‚àíùúâ¬∑ùëò,ùëò=max{ùêø‚Ñì,ùêøùëë,ùêøùëî}. In particular, this result implies that
|ùëÉ‚òÖ(ùõæ1,ùõæ2)‚àíùê∑‚òÖ
ùúâ(ùõæ1,ùõæ2)|‚â§ùúâùëò(1+‚à•ùúÜ‚òÖ
ùëù‚à•ùêø1)Proposition 4 (Empirical gap). Assume‚Ñìandùëëare non-negative
and bounded in[‚àíùêµ,ùêµ]and letùëëVCdenote the VC-dimension of the
hypothesis classAùúâ={ÀÜùëì(¬∑,ùúΩ):ùúΩ‚ààŒò}‚äÜF . Then it holds with
probability 1‚àíùúîover theùëÅsamples from each domain that
|ùê∑‚òÖ
ùúâ(ùõæ1,ùõæ2)‚àíùê∑‚òÖ
ùúâ,ùëÅ,Es(ùõæ1,ùõæ2)|‚â§ 2ùêµ‚àöÔ∏Ñ
1
ùëÅ[1+log(4(2ùëÅ)ùëëVC
ùúî)]
Letùúâ>0be given, and let ÀÜùëìbe anùúâ-parameterization of F. Let
the assumptions hold, and further assume that ‚Ñì,ùëë, andùëîare[0,ùêµ]-
bounded and that ùëë[P,T]=0if and only if P=Talmost surely, and
thatùëÉ‚òÖ(ùõæ1,ùõæ2)isùêø-Lipschitz. Then assuming that Aùúâ={ÀÜùëì(¬∑,ùúÉ):
ùúÉ‚ààŒò}‚äÜF has finite VC-dimension, it holds with probability
1‚àíùúîover theùëÅsamples that
|ùëÉ‚òÖ‚àíùê∑‚òÖ
ùúâ,ùëÅ,Eùë†(ùú∏)|‚â§ùêø||ùú∏||1+ùúâùëò(1+||ùùÄ‚òÖ
ùëù||1)+ùëÇ(‚àöÔ∏Å
log(ùëÄ)/ùëÄ)
Now we prove Theorem 1.
Proof. The proof of this theorem is a simple consequence of
the triangle inequality. Indeed, by combining Propositions 2 to 4,
we find that
|ùëÉ‚òÖ‚àíùê∑‚òÖ
ùúâ,ùëÅ,Eùë†(ùõæ1,ùõæ2)|
=|ùëÉ‚òÖ+ùëÉ‚òÖ(ùõæ1,ùõæ2)‚àíùëÉ‚òÖ(ùõæ1,ùõæ2)+ùê∑‚òÖ
ùúâ(ùõæ1,ùõæ2)‚àíùê∑‚òÖ
ùúâ(ùõæ1,ùõæ2)‚àíùê∑‚òÖ
ùúâ,ùëÅ,Eùë†(ùõæ1,ùõæ2)|
‚â§|ùëÉ‚òÖ‚àíùëÉ‚òÖ(ùõæ1,ùõæ2)|+|ùëÉ‚òÖ(ùõæ1,ùõæ2)‚àíùê∑‚òÖ
ùúâ(ùõæ1,ùõæ2)|+|ùê∑‚òÖ
ùúâ(ùõæ1,ùõæ2)‚àíùê∑‚òÖ
ùúâ,ùëÅ,Eùë†(ùõæ1,ùõæ2)|
‚â§ùêø‚à•ùõæ‚à•1+ùúâùëò(1+‚à•ùúÜ‚òÖ
ùëù‚à•1)+2ùêµ‚àöÔ∏Ñ
1
ùëÅ[1+log(4(2ùëÅ)ùëëVC
ùúî)]
‚ñ°
C SKETCH PROOF OF THEOREM 2
Lemma 1. Given two domains ùëíùëñ,ùëíùëó‚ààE,EPùëíùëó
ùëãùëçùëî(ùëì(ùëãùëíùëó),ùëçùëíùëó)can
be bounded by EPùëíùëñ
ùëãùëçùëî(ùëì(ùëãùëíùëñ),ùëçùëíùëñ)as follows:
EPùëíùëó
ùëãùëçùëî(ùëì(ùëãùëíùëó),ùëçùëíùëó)‚â§EPùëíùëñ
ùëãùëçùëî(ùëì(ùëãùëíùëñ),ùëçùëíùëñ)+‚àö
2ùëëùëñùë†ùë°[Pùëíùëó
ùëãùëçùëå,Pùëíùëñ
ùëãùëçùëå]
Lemma 2. Given two domains ùëíùëñ,ùëíùëó‚ààE, under Lemma 1, ùúåùëíùëó(ùëì)
can be bounded by ùúåùëíùëñ(ùëì)as follows:
ùúåùëíùëó(ùëì)‚â§ùúåùëíùëñ(ùëì)+‚àö
2ùëëùëñùë†ùë°[Pùëíùëó
ùëãùëçùëå,Pùëíùëñ
ùëãùëçùëå]
Under Lemmas 1 and 2, we now prove Theorem 2
Proof. Letùë†‚òÖ‚ààEùë†be the source domain nearest to the target
domainùë°‚ààE\Eùë†. Under Lemma 2, we have
ùúåùë°(ùëì)‚â§ùúåùë†ùëñ(ùëì)+‚àö
2ùëëùëñùë†ùë°[Pùë°
ùëãùëçùëå,Pùëíùëñ
ùëãùëçùëå]
whereùë†ùëñ‚ààEùë†. Taking the average of upper bounds based on all
source domains, we have:
ùúåùë°(ùëì)‚â§1
|Eùë†|‚àëÔ∏Å
ùë†ùëñ‚ààEùë†ùúåùë†ùëñ(ùëì)+‚àö
2
|Eùë†|‚àëÔ∏Å
ùë†ùëñ‚ààEùë†ùëëùëñùë†ùë°[Pùë°
ùëãùëçùëå,Pùëíùëñ
ùëãùëçùëå]
‚â§1
|Eùë†|‚àëÔ∏Å
ùë†ùëñ‚ààEùë†ùúåùë†ùëñ(ùëì)+‚àö
2
|Eùë†||Eùë†|ùëëùëñùë†ùë°[Pùë°
ùëãùëçùëå,Pùë†‚òÖ
ùëãùëçùëå]
+‚àö
2
|Eùë†|‚àëÔ∏Å
ùë†ùëñ‚ààEùë†ùëëùëñùë†ùë°[Pùë†‚òÖ
ùëãùëçùëå,Pùë†ùëñ
ùëãùëçùëå]
‚â§1
|Eùë†|‚àëÔ∏Å
ùë†ùëñ‚ààEùë†ùúåùë†ùëñ(ùëì)+‚àö
2 min
ùë†ùëñ‚ààEùë†ùëëùëñùë†ùë°[Pùë°
ùëãùëçùëå,Pùëíùëñ
ùëãùëçùëå]
+‚àö
2 max
ùë†ùëñ,ùë†ùëó‚ààEùë†ùëëùëñùë†ùë°[Pùëíùëñ
ùëãùëçùëå,Pùëíùëó
ùëãùëçùëå]
‚ñ°
4430