Multi-task Conditional Attention Network for Conversion
Prediction in Logistics Advertising
Baoshen Guo∗
Southeast University
Nanjing, China
JD Logistics
Beijing, China
guobaoshen@seu.edu.cnXining Song∗
JD Logistics
Beijing, China
songxining1@jd.comShuai Wang†
Southeast University
Nanjing, China
shuaiwang@seu.edu.cn
Wei Gong
University of Science and Technology
of China
Hefei, China
weigong@ustc.edu.cnTian He
JD Logistics
Beijing, China
tim.he@jd.comXue Liu
McGill University
Montréal, Canada
xueliu@cs.mcgill.ca
Abstract
Logistics advertising is an emerging task in online-to-offline lo-
gistics systems, where logistics companies expand parcel shipping
services to new users through advertisements on shopping websites.
Compared to existing online e-commerce advertising, logistics ad-
vertising has two significant new characteristics: (i) the complex
factors in logistics advertising considering both users’ offline logis-
tics preference and online purchasing profiles; and (ii) data sparsity
and mutual relations among multiple steps due to longer advertis-
ing conversion processes. To address these challenges, we design
MCAC, a Multi-task Conditional Attention network-based logistics
advertising Conversion prediction framework, which consists of (i)
an offline shipping preference extraction model to extract the user’s
offline logistics preference from historical shipping records, and (ii)
a multi-task conditional attention-based conversion rate prediction
module to model mutual relations among multiple steps in logistics
advertising conversion processes. We evaluate and deploy MCAC
on one of the largest e-commerce platforms in China for logistics
advertising. Extensive offline experiments show that our method
outperforms state-of-the-art baselines in various metrics. Moreover,
the conversion rate prediction results of large-scale online A/B test-
ing show that MCAC achieves a 15.22 % improvement compared to
existing industrial practices, which demonstrates the effectiveness
of the proposed framework.
CCS Concepts
•Applied computing →Electronic commerce.
∗Equal contribution
†Shuai Wang is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671549Keywords
Logistics Advertising, Conversion Prediction, Electronic Commerce
ACM Reference Format:
Baoshen Guo, Xining Song, Shuai Wang, Wei Gong, Tian He, and Xue Liu.
2024. Multi-task Conditional Attention Network for Conversion Prediction
in Logistics Advertising. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3637528.3671549
1 Introduction
With the rapid development of e-commerce, the logistics services
(e.g., FedEx [ 7], UPS [ 25], SF Express [ 6], and JD Logistics [ 15])
have significantly altered people’s consumption and lifestyle habits,
increasing the frequency of sending and receiving packages. In
logistics systems, one emerging issue is logistics advertising, which
involves the use of text or image advertisements on online shopping
platforms to attract more new users to use the logistics services.
As is illustrated in Fig. 1, the logistics company acquires new users
from the advertising displayed in a banner for the user in an online
shopping system. Given the cost of advertising placement and the
vast number of e-commerce users, accurately predicting the interest
and conversion probability of target users for logistics services is a
crucial issue in logistics advertising scenarios.
UserPurchasingImpression
Online Shopping PlatformUsing Shipping ServicesConversionSuccessfulWithout Using Shipping Conversion FailureOffline Logistics Platform
Logistics Ad Banner
Figure 1: An example of Logistics advertising
The conventional advertisement conversion prediction [ 2,17,
20,24,32,33] problem has been thoroughly studied in online e-
commerce platforms. Some studies [ 2,32,33] model the conversion
5028
KDD ’24, August 25–29, 2024, Barcelona, Spain Baoshen Guo et al.
prediction of online advertisements as a single task to directly esti-
mate the conversion possibility. There are also works [ 16,17,19,20,
22,24,26,35] formulating conversion prediction as multiple objec-
tives and multiple task learning problems to simultaneously predict
conversion rate of multiple actions (e.g., click and conversion).
These works [ 18,28,29] pay attention to modeling the sequential
pattern in conversion steps by introducing one or two auxiliary
tasks. However, these existing studies are incapable of applying
to logistics advertising directly because of two limitations: (i) Ex-
isting studies mainly focus on users’ online purchasing behaviors
while the logistics advertising needs to consider both users’ offline
shipping and online purchasing behaviors; (ii) Logistics advertis-
ing exhibits longer sequential patterns compared to e-commerce
advertising. The presence of additional auxiliary tasks and interme-
diate steps with intricate relationships to the ultimate conversion
prediction task increases the prediction complexity.
In this paper, we leverage both the historical offline shipping
behavior records and online purchasing-related data to achieve
practical logistics advertising conversion prediction in industrial
scenarios. However, there are still some challenges when perform-
ing conversion prediction of logistics advertising leveraging the
opportunities: (i) the complex feature fusion process in logistics
advertising considering both users’ offline logistics behaviors and
online purchasing preference, and (ii) the longer sequential patterns
result in sparser positive samples for the final conversion prediction.
How to address data sparsity in each step and model the mutual
relations among different tasks is another challenge.
To address these challenges, we propose MCAC, a multi-task
conditional attention-based conversion prediction framework to
model the long sequential pattern of advertising conversion steps.
Specifically, our proposed approach adopts the progressive learning
architecture to refine the task-specific representation. For each task,
the proposed framework generates early predictions in the lower
layers based on task-specific contexts, which provides supplemen-
tary information for extracting more comprehensive task-specific
representations. Due to the sequential relationships of multiple
tasks, the former task can provide valuable information for latter
tasks. So we develop a conditional attention mechanism to fuse the
contextual representation and positive feedback from the former
task to enhance the final conversion prediction performance. In
summary, the key contributions of our work are as follows:
•To the best of our knowledge, we are the first to study the logistics
advertising conversion prediction problem in online-to-offline lo-
gistics e-commerce. Leveraging historical conversion sequential
records, online purchasing data, and offline shipping behavior
data, we build a data-driven conversion prediction framework
considering both offline behavior patterns and online purchasing
preferences.
•We propose a multi-task conditional attention-based conversion
prediction (MCAC) framework, which models the sequential pat-
tern of advertising conversion steps and leverages interactions
of early predictions to improve the task-specific representation.
Our model employs conditional attention to extract more com-
prehensive task-specific representations and ensure the adaptive
information transferring between different tasks to improve the
final conversion prediction performance.•MCAC achieves better performance than representative state-of-
the-art methods on both the real-world industrial logistics adver-
tising dataset and a public advertising dataset. We also evaluate
and deploy MCAC on JD.com, one of the largest online-to-offline
e-commerce and logistics platforms in China. The online A/B
testing and long-term deployment results prove the effectiveness
of MCAC and confirm its economic value in real-world logistics
advertising scenarios.
2 Preliminaries and Formulation
2.1 Background
Fig. 2 shows an example of acquiring new customers in online
logistics advertising and e-commerce advertising. In e-commerce
advertising scenarios, after seeing the advertisements, users’ ac-
tions follow two steps, i.e., clicking the advertisements and pur-
chasing goods (successful conversion). Logistics advertising has
longer sequential patterns than e-commerce advertising. In logis-
tics advertising, it takes four steps for a user to go from seeing an
advertisement to successfully converting into a logistics customer:
(i)Click: Users click on the logistics advertisements when they per-
form online shopping; (ii) Information filling: After clicking, users
subsequently provide the addresses of the sender and the receiver,
and the type of goods. The system will present the user with the
service fee and the estimated delivery time; (iii) Submission: If the
users are satisfied with the shipping fees and delivery time, they
will submit their order information. (iv) Conversion: In this step, a
courier will pick up the shipping orders from the customers. After
pickup, the customer will pay the shipping fee, which confirms the
successful conversion of the logistics advertisement.
ConversionAd Conversion stepsE-commerce AdvertisingLogistics AdvertisingClickInformationfillingSubmissionScenariosClickConversion
Figure 2: Conversion processes comparison between logistics
advertising and e-commerce advertising.
2.2 Preliminaries
Definition 1 (Logistics advertising): Logistics advertising
refers to the dissemination of textual and visual content related to
logistics services on e-commerce platforms, with the aim of promot-
ing and recommending shipping and logistics services to targeted
e-commerce users. If a user clicks on the advertisement, provides
the shipping addresses, submits the orders, and successfully sends
out the packages, it represents a successful conversion from the
advertisement.
Definition 2 (Online purchasing behavior data): The
online purchasing behavior data consists of two parts: (i) Profile of
users with sensitive information removed, such as anonymized IDs
and statistical data on purchase frequency; (ii) Purchasing statis-
tics information, such as the purchasing volume and the product
categories ordered over a period of time.
5029Multi-task Conditional Attention Network for Conversion Prediction in Logistics Advertising KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) # of order Heatmap
2023-03-02 2023-03-09 2023-03-16 2023-03-23 2023-03-3050100# of orders (103)
Industrial Oﬃce Commercial (b) Order quantity in different regions
0.00 0.05 0.10 0.15
# of address ( 103)0.00.51.0# of orders ( 103)
 (c) Relations between # of order and # of address
Figure 3: Analysis of offline logistics behavior patterns. (a) Heatmap of shipping orders amount in Beijing. The brighter the
color, the higher the order amount; (b) The dynamic shipping numbers in three different types of zones (i.e., industrial zones,
office buildings, and commercial zones); (c) The correlations between the shipping amount and number of addresses.
Definition 3 (Offline shipping behavior seqence): The
offline behavior sequence 𝑆={𝑎1,𝑎2,···,𝑎𝑠}of users refers to the
actions of receiving goods in different regions after purchasing. The
user action 𝑎=<𝑢,𝑟,𝑑 >represents user 𝑢received packages in
the region𝑟.𝑑is the distance between 𝑟and the region 𝑟𝑚𝑎𝑥𝑢 with
the highest frequency of historical receipts by the user.
2.3 Problem Formulation
Logistics Advertising Conversion Prediction Problem Formulation: In
logistics advertising scenarios, let the historical advertising conver-
sion dataset be 𝛀={(𝒙𝒊→𝑦𝑖
1,···,𝑦𝑖
𝑡,···,𝑦𝑖
𝑇)}|𝑁
𝑖=1.𝒙𝒊represents
the feature vector of 𝑖-th observed impression. 𝑁is the total number
of impressions. y𝑖
𝑡is a binary step success label in each conversion
step. In the sequential pattern, the user needs 𝑇step of tasks to
complete conversion (as shown in Fig. 2, the number of conversion
steps𝑇in our paper is 4). If the user completes step 𝑡,y𝑖
𝑡=1, oth-
erwise y𝑖
𝑡=0. The sequential dependence of y𝑡−1→y𝑡indicates
that there is always a preceding step 𝑡−1when the𝑡-th event
occurs. Formally, the logistics advertising conversion prediction
problem is to estimate the final conversion probability ˆ𝑦𝑇with
multiple conversion steps. Given impression x𝑖, the probabilities of
each task𝑡is defined as:
ˆ𝑦𝑡=𝑓𝑀𝐶𝐴𝐶(𝑦𝑡|x,ˆ𝑦1,..., ˆ𝑦𝑡−1),1≤𝑡≤𝑇. (1)
where ˆ𝑦𝑡is the conversion probability in step 𝑡. The final conversion
probability ˆ𝑦𝑇of logistics advertising is defined as:
ˆ𝑦𝑇=𝑓𝑀𝐶𝐴𝐶(𝑦𝑇|x,ˆ𝑦1,..., ˆ𝑦𝑇−1), (2)
where𝑓𝑀𝐶𝐴𝐶 is the proposed logistics advertising conversion pre-
diction function.
2.4 Characteristics of Offline Shipping Behavior
Fig. 3 elaborates on the data-driven analysis of offline shipping
behaviors. From Fig. 3a, we can see that the demand for logistics
shipping orders in different regions of Beijing is uneven. Combining
this with Fig. 3b, we find that the type of region has a significant
impact on the number of shipping orders. Fig. 3c reflects the rela-
tionship between the number of shipping orders and the historical
number of offline shipping addresses. From Fig. 3c, it is evident that
users with a higher number of historical shipping addresses also
have a greater demand for logistics services. So it is essential topay attention to the user’s offline behaviors when recommending
logistics advertising to users. For instance, if the recipient area
in the offline behavior data corresponds to a residential location,
there is a high probability that the user will use the service to send
everyday household items. Conversely, if the recipient area in the
offline behavior data corresponds to an office or industrial region,
the user may be more inclined to use logistics services for sending
bulk goods or office supplies.
3 Model Design
3.1 Overview of MCAC
Figure 4 shows the framework of MCAC, which mainly consists of
the following three parts:
Offline Logistics DataUser Profile{𝑎!→𝑎"→⋯→𝑎#}⨁
Order DataOffline behaviors tokenizingSpatialfeatures{	𝑟$!,𝑟$",…,𝑟$%	}EmbeddingEmbeddingUser Preference Representation
OnlineBehaviors DataEmbeddingEmbeddingOnline feature FusionSubmission PredictionConversion PredictionOutput𝑇−1Conditional Attention𝑦%!"#Tower 𝑇−1𝐸!"#$𝐻!"#Output𝑇Conditional AttentionTower 𝑇𝐸!$𝐻!𝑦%!𝐻!"#𝐻!"%𝐸
Figure 4: Framework of MCAC
•Online and offline data processing: In this part, we first pro-
cess the user’s offline behavioral data to obtain offline behavioral
sequences. Then we extract attribute features from online user
profiles and online order purchasing data.
•Offline shipping preference extraction: This module takes
the user’s offline shipping behavioral sequences as inputs. After
5030KDD ’24, August 25–29, 2024, Barcelona, Spain Baoshen Guo et al.
going through interest extraction and fully connected layer map-
ping, we ultimately obtain the embedded representation of the
user’s logistics service preferences.
•Multi-task conversion rate prediction module: In this mod-
ule, we model multiple steps of user conversion as multiple tasks
and take the extracted online and offline preference representa-
tions as input. Knowledge transfer between these multiple tasks
is employed to alleviate data sparsity issues and to enhance the
final conversion prediction performance.
3.2 Offline Shipping Preference Extraction
Motivation: Users’ offline shipping behaviors comprise interac-
tions between users and multiple regions. To capture the dynamic
interactions with different spatial regions, the preference extrac-
tion network uses the user’s historical interaction behavior se-
quences as input and employs multiple stacked Gated Recurrent
Units (GRUs) [ 4,5] to model offline shipping sequential behaviors.
As shown in Fig. 5, the input of the offline behavior prefer-
ence extraction module consists of the offline behavior sequence
{𝑎1,𝑎2,...,𝑎𝑠}and the features of spatial regions 𝑟𝑢={𝑟1𝑢,𝑟2𝑢,...,𝑟𝑛𝑢}.
The user’s offline shipping behaviors contain implicit user inter-
est preferences, and the region features complement the user’s
interest preferences. Through the embedding layer, we obtain the
dense vector of offline behavior sequences with 𝑎′
𝑖=𝑊𝑎𝑖𝑎𝑖and
𝑟′𝑢=𝑊𝑟𝑢𝑟𝑢, where𝑎′
𝑖and𝑟′𝑢are dense embeddings of offline behav-
ior sequence and spatial regions, respectively. Then, we use stacked
GRUs to model users’ offline behavior sequences in order to bet-
ter extract temporal information embedded within the behavioral
sequences. Each GRU represents the user’s offline behavior at a
𝒂𝟏𝒂𝟐𝒂𝒔…𝒓𝒖𝟏𝒓𝒖𝟐𝒓𝒖|𝒓|…GRU𝒂𝒔'𝟏𝒂𝒔(GRU𝒂𝒔'𝟏(GRUGRU𝒂𝟏(𝒂𝟐(…GRUGRUGRUGRU…FC⨁Spatial Region EmbeddingOffline Shipping Sequence Embedding…Preference Vectorℎ!𝑟"#𝐼"
Figure 5: User shipping preference extraction module, taking
historical shipping records and spatial features as input.
specific moment, encoding the user’s current offline behavior 𝑎𝑠
into the corresponding hidden state ℎ𝑠for that moment, which is
calculated through:
ℎ𝑠=(1−𝑧𝑠)⊙ℎ𝑠−1+𝑧𝑠⊙eℎ𝑠 (3)
where𝑧𝑠=𝜎(𝑊𝑧·[ℎ𝑠−1,𝑎𝑠]+𝑏𝑧)is the update gate to control the
extent to which the previous unit’s state information is introduced
into the current unit. eℎ𝑠is the candidate activation computed based
on the input and the reset gate in GRU [ 5], which represents the
new candidate information that could be added to the memory
cell at the current time step. The hidden state ℎ𝑠of the last unit
implicitly captures the patterns of offline behavior sequences. After
obtaining the hidden representation ℎ𝑠of offline shipping behavior
sequences, to fuse spatial region features and obtain final offline
CCAM𝒀"𝑻"𝟏
Encoder𝑬𝑬𝑻"𝟏$			𝑯𝑻"𝟏𝑸𝑲𝑽scalemasksoftmax
Task representation 𝑯𝒕Conditional  vector𝑬𝒕"CCAM𝑬𝑯𝑻𝑬𝑻$			Input Embeddings[𝑬;𝑯𝒕"𝟏]Attention graphConditional graph𝒀"𝑻
EncoderUser ProfileOrder Data..𝑒!𝑒"𝑒#..	𝑰𝒖Figure 6: The left is the progressive modeling on multiple
tasks. The right is the conditional attention mechanism.
preference vectors, we concat the vector 𝑟′𝑢with shipping sequence
representation vector ℎ𝑠(encoded by the GRUs) and fed them into
a feed-forward layer to obtain the final preference vector:
𝐼𝑢=tanh(𝑊𝑢[ℎ𝑠,𝑟′
𝑢]+𝑏𝑢) (4)
where𝑊𝑢is the weight matrix. 𝑏𝑢is the bias value. ℎ𝑠is the em-
bedding of sequential shipping behaviors. 𝑟′𝑢is the embedding of
regional spatial features. 𝐼𝑢is the offline shipping preference repre-
sentation considering both historical shipping records and spatial
features.
3.3 MCAC Design
3.3.1 Progressive modeling on single task. As shown in Fig. 6,
for every single task in advertising conversion prediction, we design
a task-specific tower based on progressive structure [ 23] to perform
single-task classification. We first employ a feature encoder taking
feature embedding 𝐸as input to produce the task-specific dense
embedding 𝐸′
𝑡, which is interpreted as an early prediction for the
task𝑡and is used as additional information to 𝐸. A conditional
attention mechanism is used to progressively improve the final task-
specific representation 𝐻𝑡taking both 𝐸and the early prediction
𝐸′
𝑡as input. Then, we obtain the conversion classification result
ˆ𝑌𝑡for task𝑡with a classifier taking 𝐻𝑡as input. After single-task
progressive modeling, we introduce our multi-task learning design
based on a progressive structure for logistics advertising conversion
prediction.
3.3.2 Multi-task modeling for conversion prediction. In lo-
gistics advertising conversion scenarios, to transfer information
between different conversion stages, we adopt the progressive ar-
chitecture into the multi-task framework to model the sequential
dependence and apply conditional attention to adaptive fuse feature
representation and the feedback from previous tasks.
Shared online-to-offline feature representation: We first introduce
the online-to-offline feature representation part, which generates
the input of multi-task prediction module of MCAC. Based on
the offline shipping preference extraction module, we obtain the
offline preference representation 𝐼𝑢. For the online features, we
collect two types of online data from the e-commerce platform:
user profile data and order purchasing statistical features. (i) For
user profile data, we first remove sensitive information such as
names and text addresses for privacy protection. The remaining
features are then categorized into continuous and discrete types.
Continuous features are normalized (e.g., user monthly purchase
5031Multi-task Conditional Attention Network for Conversion Prediction in Logistics Advertising KDD ’24, August 25–29, 2024, Barcelona, Spain
frequency, average price.) or processed with coarse-grained binning
(such as age range divisions). (ii) For order statistical features, we
use the one-hot encoding of the product categories that a user
has purchased in recent periods to form the initial vector of the
purchasing behaviors.
As shown in Fig. 6 (left part), we feed the extracted online fea-
tures into the embedding layer and obtain the dense embedding
of online features [𝑒1,𝑒2,···], which represents the concatenate
embeddings of each type of features.
𝐸=[𝑒1;𝑒2;···𝑒𝑖;𝑒|X|;𝐼𝑢] (5)
Then, we further concat the online feature vector and the offline
preference vector 𝐼𝑢to form the feature embedding 𝐸through
Equation 5 as the input of the multi-task prediction module.
Task-specific representation learning: After obtaining the input fea-
ture representation 𝐸, for each task tower 𝑡, we employ an encoding
function module to perform early embedding projection and obtain
the dense embedding 𝐸′
𝑡considering different contexts in different
conversion steps. The encoding function of tower 𝑡can be any type
of embedding layer. In this part, we use a two-layer fully connected
network [ 9] to obtain task-specific dense embedding 𝐸′
𝑡, which is
further used as the conditional information of each task to improve
the representation in the top layer.
3.3.3 Conditional attention mechanism (CAM). To fuse the
contextual representation and positive feedback from former tasks,
we design a conditional attention mechanism and extract compre-
hensive task-specific representations based on early predictions
of each task. The conditional attention mechanism first calculates
the attention weights of the positive feedback from former task
𝐻𝑡−1and the contextual representation 𝐸using feature vectors
𝑨𝒕=[𝐸;𝐻𝑡−1]. The attention vector 𝑭𝒕is calculated through the
self-attention mechanism [27] and denoted as follows:
𝑭𝒕=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑄(𝑨𝒕)⊙𝐾(𝑨𝒕)√︁
𝑑𝑘) (6)
where 𝑨𝒕=[𝐸;𝐻𝑡−1]. Then the conditional attention vector 𝐹𝑐𝑛𝑑
𝑡is
calculated by the dot product of conditional vector 𝐸′
𝑡and attention
vector𝐹𝑡:
𝐹cnd
𝑡=𝐹𝑡⊙𝐸′
𝑡 (7)
where⊙represents the dot product. The output of conditional
attention vector 𝐻𝑡is calculate by Eq. 8:
𝐻𝑡=∑︁
𝐴𝑡𝐹cnd
𝑡𝑉(𝐴𝑡). (8)
From the above equations, we integrate the feedback 𝐻𝑡−1from the
former task, the shared online-to-offline feature embedding 𝐸, and
the task-specific feature embedding 𝐸′
𝑡into conditional attention
task vector 𝐻𝑡for𝑡-th task. This kind of conditional attention
mechanism is more effective in extracting more comprehensive task-
specific representations by considering the conditional information
in self-attention mechanisms.
3.3.4 Multi-task prediction and training. For tasks𝑡, the out-
put of the Conditional Attention Mechanism (CAM) that transfers
to the next step is computed as:
𝐻𝑡=CAM(𝐻𝑡−1,𝐸,𝐸′
𝑡) (9)where𝐻𝑡−1is the output of the CAM model of task 𝑡−1. For the
first task without the former task, the output of the CAM module
is initialized as 𝐻1=CAM(𝐸,𝐸′
1). And the conversion prediction
probability of each task 𝑡is:
𝑝𝑡=sigmoid(MLP(𝐻𝑡)) (10)
where MLP(·)is defined as the classification function to project
𝐻𝑡to the output space. The loss function of MCAC is defined as
Equation 11. It consists of total loss terms from all tasks, which are
calculated over samples of all impressions, not only using the loss
of the conversion prediction task:
L(𝜃 1,...,𝜃𝑇)=𝑇∑︁
𝑡=1𝑁∑︁
(𝑥𝑖,𝑦𝑖
𝑡)∈Ω𝑙(𝑦𝑖
𝑡,𝑓(𝑥𝑖,𝜃𝑡)) (11)
where𝜃1,...,𝜃𝑇are the parameters of tasks 1,...,𝑇 .𝑇denotes
the number of tasks. 𝑁is the total number of impressions in the
training dataset Ω.𝑦𝑖
𝑡is the label of impression 𝑖, and𝑙(·)is the
cross-entropy loss.
4 Evaluation
4.1 Dataset
We conduct extensive experiments in two large-scale advertising
datasets, i.e., one logistics advertising dataset and a public online
advertising dataset. The logistics advertising industrial dataset is
collected from one of the largest logistics companies. The public
dataset is Ali-CCP1(Alibaba Click and Conversion Prediction).
Table 1 shows the statistics of two datasets.
Table 1: Statistics of instances in two datasets.
Dataset Conv
ersion steps #
of Positive instances
Industrial
datasetImpr
ession 51.8𝑀
Click 10.5𝑀
Information
filling 563𝐾
Submission 441𝐾
Conv
ersion 363𝐾
Public
datasetImpr
ession 85.2𝑀
Click 3.3𝑀
Conv
ersion 170𝐾
4.1.1 Public dataset: The public dataset is gathered from real-world
traffic logs of the recommender system in Taobao [ 18]. We utilize
all categorical features with single values and exclude any feature
whose frequency is less than 10. Two tasks (i.e., click and conver-
sion) are contained in the dataset.
4.1.2 Logistics industrial dataset: For offline experiments, we col-
lect logistics advertising impression data from JD.com, one of the
largest online e-commerce platforms in China. We randomly sam-
ple a small percentage of data from the overall logistics advertise-
ment impression records covering the entire national user base
of JD.com [ 10] over two months, obtaining a dataset of 63.7GB
in size for offline experimental verification. Four tasks (e.g., click,
information filling, submission, conversion) during the conversion
process are included in the dataset. The key fields and examples of
the logistics industrial dataset are described in Table. 2.
1Ali-CCP: https://tianchi.aliyun.com/dataset/408
5032KDD ’24, August 25–29, 2024, Barcelona, Spain Baoshen Guo et al.
Table 2: An example of key fields of the datasets.
Online
Pur
chasing dataUser
ID Categor
y ID Brand
ID Pr
oduct ID
21203 723 421 182
Off
line
Logistics DataRe
ceive Region Region
Type Ar
ea of region Timestamps
x
x01x Office 0.11 2020/9/1
12:14
Lab
el
DataClick Info
. Filling Submission Conv
ersion
1 1 1 0
Logistics advertising data description: The logistics advertising
dataset involves the analysis of traffic logs from one large logistics
advertising system, focusing on three primary data types: online
purchasing data, offline logistics data, and label data. The details of
online and offline data are as follows:
•Online purchasing data: The online purchasing data records the
details of online orders and consists of user id, product id, order
category, and order brand.
•Offline logistics data: The offline logistics data consists of the
encrypted order shipping address, timestamps, and complaint
labels (1 indicates complaints and 0 indicates not).
It’s important to note that it’s not valid for the label of a subsequent
task to be 1 if the label of the preceding task is 0.
4.2 Experiment Setup
4.2.1 Dataset split: We divide the training, validation, and test sets
according to the ratio of 6:1:3. We focus on two main tasks over the
industrial dataset: (i) submission prediction task and (ii) conversion
prediction task. The click rate prediction and information-filling
behavior prediction tasks are set as auxiliary tasks.
4.2.2 Experimental settings: We implement the MCAC with Python
3.8.0 and PyTorch 1.6.0 on Ubuntu 16.04, with NVIDIA Tesla P40,
180GB RAM. The model is trained through stochastic gradient
descent over shuffled mini-batches with the Adam optimizer with
a 1e-3 learning rate. To mitigate the overfitting problem, dropout
rates are set to 0.1 and 0.3. The embedding size is set to 6. The batch
size is 2000. The tower model is designed with 3 layers, and the
output vector dimensions of each layer are 128, 64, and 32.
4.3 Metrics and Baselines
4.3.1 Metrics. To evaluate the performance, we choose Area Under
ROC (AUC), Precision@K, and Recall@K as the metrics. The AUC
is a measure of the area under the curve (ROC curve), which is
widely employed to evaluate the ranking capability of recommen-
dation systems. Here, Precision@K measures the accuracy of the
recommendation or retrieval system by calculating the proportion
of positive samples among the top K samples. The Recall@K cal-
culates the proportion of positive samples that were successfully
retrieved among the top K samples. We calculate these metrics for
both the submission prediction and the conversion prediction tasks.
4.3.2 Baseline selection. We choose six state-of-the-art baselines to
assess the effectiveness of MCAC. The XGBoost and LightGBM are
state-of-practice methods that have been deployed in our logistics
advertising system previously. We also choose four state-of-the-
art multi-task learning-based recommendation works (i.e., MMoE,ESMM, PLE, and AITM), which are used to test the superiors of our
framework in processing multi-task relationships.
•XGBoost [ 1]: XGBoost is a scalable gradient boosting decision
tree-based framework. We use it to perform submission predic-
tion and conversion prediction separately.
•LightGBM [ 12]: a decision-tree-based gradient boosting frame-
work with innovative techniques, achieving remarkable success
in various machine learning tasks.
•MMoE [ 17]: a powerful multi-gate mixture-of-experts model
that can explicitly learn the relationships between tasks and
effectively share parameters among them.
•PLE [ 24]: a pre-training framework that leverages both the item
content and user behavior sequence data to learn feature repre-
sentations for recommendation.
•ESMM [ 18,28]: state-of-the-art multi-task learning frameworks
that combine CTR and CVR tasks to improve the overall perfor-
mance of recommender systems.
•AITM [ 29]: an adaptive information transfer multi-task (AITM)
conversion prediction framework, which models the sequential
dependence among audience multi-step conversions.
Each baseline is presented with two settings considering the differ-
ent settings in the online-to-offline logistics advertising scenarios.
•Original: In this setup, all methods do not incorporate user
offline behavioral characteristics and only utilize features gen-
erated from user profile data, waybill statistics data, and online
behavioral statistics data as inputs.
•Adaption: In this setup, the offline behavior data are converted
to offline preference vectors with the same offline shipping pref-
erence extraction module. Then we concat them with online
features as the input of these baseline models.
4.4 Main Performance
We first introduce the main performance comparison results and
analysis of offline experiments in the industrial logistics dataset.
The comparison results between the proposed method and baselines
in various metrics of two tasks are shown in Table 3. Overall, the
proposed MCAC achieves the best performance compared to other
baselines. In the submission rate prediction task, MCAC achieves
an AUC of 0.8641, precision@K of 0.1861, and recall@k of 0.1761.
In the conversion rate prediction task, MCAC achieves an AUC of
0.9002, precision@K of 0.1632, and recall@k of 0.2217.
Particularly, we have the following analysis findings: (i) All meth-
ods achieve performance improvements after incorporating offline
behavioral preference features. Therefore, for conversion rate and
click-through rate prediction tasks, user offline behavior should
not be disregarded. (ii) Compared to the single-task model Light-
GBM, all multi-task models (ESMM, MMoE, PLE, AITM, and MCAC)
achieved greater gains by considering the sequential dependence
among multiple tasks during conversion processes.
4.5 Performance Comparison under Different 𝐾
For metric precision@k and recall@k, we conduct experiments for
submission prediction and conversion prediction tasks with differ-
ent𝐾(i.e., 0.5%, 1.0%, 1.5%, and 2.0%) to evaluate the robustness of
the proposed MCAC. As is shown in Figure. 7 and Figure. 8, in both
submission rate and conversion rate prediction tasks, our model
5033Multi-task Conditional Attention Network for Conversion Prediction in Logistics Advertising KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Performance comparisons on the industrial logistics advertising dataset.
Metho
d SettingSubmission
rate prediction Conversion rate prediction
AUC
Precision@1% Recall@1% AUC
Precision@1% Recall@1%
X
GBoost [1]Original 0.7354
0.0981 0.0839 0.7461
0.0926 0.1258
A
daption 0.7998
0.1296 0.1174 0.8268
0.1213 0.1680
LightGBM
[12]Original 0.7393
0.0995 0.0852 0.7457
0.0934 0.1277
A
daption 0.8081
0.1307 0.1218 0.8302
0.1296 0.1719
MMoE
[17]Original 0.7606
0.1097 0.0914 0.7730
0.1034 0.1362
A
daption 0.8268
0.1693 0.1562 0.8513
0.1466 0.1872
PLE
[24]Original 0.7597
0.1018 0.0891 0.7654
0.0997 0.1319
A
daption 0.8317
0.1691 0.1593 0.8576
0.1473 0.1877
ESMM
[18, 28]Original 0.7555
0.1033 0.0933 0.7697
0.1010 0.1337
A
daption 0.8102
0.1625 0.1587 0.8446
0.1449 0.1831
AI
TM [29]Original 0.7708
0.1136 0.0965 0.7811
0.1077 0.1403
A
daption 0.8281
0.1745 0.1626 0.8649
0.1524 0.1906
MCA
C A
daption 0.8641
0.1861 0.1761 0.9002
0.1632 0.2217
achieved the best precision@k and recall@k performance across
different values of 𝐾. This demonstrates the effectiveness of our
model. Additionally, we observed that for the precision@k metric,
as the value of 𝐾increases, the overall performance of all models
tends to decrease. On the other hand, for the recall@k metric, as 𝐾
increases, the overall performance of all models gradually improves.
0.5% 1.0% 1.5% 2.0%
K0.00.10.20.3Precision@KXGBoost
LightGBM
MMoE
PLEESMM
AITM
MCAC
(a) Precision@K on submission
0.5% 1.0% 1.5% 2.0%
K0.00.10.20.3Recall@KXGBoost
LightGBM
MMoE
PLEESMM
AITM
MCAC (b) Recall@K on submission
Figure 7: Performance comparisons on submission task
0.5% 1.0% 1.5% 2.0%
K0.000.050.100.150.200.25Precision@KXGBoost
LightGBM
MMoE
PLEESMM
AITM
MCAC
(a) Precision@K on conversion
0.5% 1.0% 1.5% 2.0%
K0.00.10.20.30.4Recall@KXGBoost
LightGBM
MMoE
PLEESMM
AITM
MCAC (b) Recall@K on conversion
Figure 8: Performance comparisons on conversion task
4.6 Ablation Studies
4.6.1 Effectiveness of different sub-modules: As shown in Table 4,
we conduct ablation analysis by comparing MCAC to two vari-
ations, i.e., MCAC removes conditional attention ( 𝑤/𝑜CA) and
MCAC removes feedback transferring between tasks ( 𝑤/𝑜FE), to
demonstrate the importance of each module in the proposed MCACTable 4: Ablation studies of submodules.
Metho
dSubmission
rate prediction Conversion rate prediction
AUC
Precision@1% Recall@1% AUC
Precision@1% Recall@1%
𝑤/𝑜Fe 0.8221
0.1685 0.1608 0.8786
0.1528 0.1969
𝑤/𝑜CA 0.8301
0.1785 0.1643 0.8675
0.1519 0.1913
MCAC 0.8641
0.1861 0.1761 0.9002
0.1632 0.2217
framework. From the results shown in Table 4, we observe that (i)
MCAC has the best performance compared to variations, which ver-
ifies the effectiveness of two designed modules; and (ii) removing
the CA module results in a significant drop in the model’s perfor-
mance across various metrics, highlighting the importance of the
conditional attention module design.
Table 5: Ablation studies of data sources.
Metho
dSubmission
rate prediction Conversion rate prediction
AUC
Precision@1% Recall@1% AUC
Precision@1% Recall@1%
𝑤/𝑜offline 0.7816
0.1210 0.1028 0.7922
0.1145 0.1467
𝑤/𝑜profile 0.7914
0.1192 0.1069 0.8241
0.1192 0.1638
𝑤/𝑜orders 0.7957
0.1235 0.1071 0.8249
0.1227 0.1654
MCAC 0.8641
0.1861 0.1761 0.9002
0.1632 0.2217
4.6.2 Effectiveness of different data sources: To show how the model
would perform when eliminating each of the target data sources
from the model, we add new experiments to compare MCAC with
some variations including (i) MCAC w/o offline features ( 𝑤/𝑜of-
fline), (ii) MCAC w/o user profile ( 𝑤/𝑜profile), and (iii) MCAC w/o
order feature ( 𝑤/𝑜orders). As shown in Table 5, we find that the
performance will decrease compared to MCAC when eliminating
each of the target data sources from the model, which demonstrates
the effectiveness of different sources of features.
4.7 Parameter Sensitivity Analysis
We further conduct experiments on the industrial logistics adver-
tising dataset to evaluate the impact of some hyperparameters, for
example, batch size and train set size.
Effects of different batch sizes. We conduct experiments under
different batch sizes (i.e., 1000, 2000, 3000, 4000, and 5000). As shown
5034KDD ’24, August 25–29, 2024, Barcelona, Spain Baoshen Guo et al.
in Fig. 9, at the beginning, as the batch size increased, the model’s
performance improved. Subsequently, the model’s performance
steadily declined. This is because, with larger batch sizes, model
performance deteriorates due to test errors. In this paper, we set
the batch size as 2000.
1000 2000 3000 4000 5000
Batch size0.70.80.91.0AUC
Submission
Conversion
Figure 9: Different batch size
0.2 0.4 0.6 0.8
Data size0.70.80.91.0AUC
Submission
Conversion Figure 10: Different data size
Effects of different train set sizes. We conduct thorough com-
parisons on large-scale datasets with various sampling rates (i.e.,
0.2, 0.4, 0.6, and 0.8). As shown in Fig. 10, MCAC has the best per-
formance when the sample rate is 0.8 and exhibits improvements
with an increase in the volume of training samples.
4.8 Performance Comparison on Public Dataset
Then we introduce the performance comparison between MCAC
w/o PE (w/o offline preference extraction) and baselines on the
public dataset (Ali-CCP).
Table 6: Results on public dataset.
Metho
d Click AUC Conversion AUC
X
gboost 0.6043 0.6122
LightGBM 0.5993 0.6070
MMoE 0.6174 0.6459
PLE 0.6188 0.6425
ESMM 0.6112 0.6221
AITM 0.6142 0.6468
MCAC𝑤/𝑜PE 0.6156 0.6561
Table 6 shows the results on the public dataset. From these Ta-
ble 6, we have the following findings: (i) Despite the fact that the
extreme class imbalance led to a certain degree of deterioration in
the performance of all models, the multi-task models still outper-
formed the single-task models in lightGBM. (ii) Although MCAC
is not performing the best in the click prediction tasks, it can still
utilize the previous positive feedback to improve the performance
of subsequent sequential tasks.
4.9 Real-world Deployment and Discussion
4.9.1 Online A/B Testing: We also implement and deploy the
proposed MCAC in the real-world logistics advertising platform
of JD.com. In online A/B testing, we deployed MCAC in the ex-
perimental group and deployed two baselines (i.e., LightGBM and
Mix-expert LightGBM, which are recommended practices in our
logistics advertising system) in the control group. During the one-
month online A/B testing, the daily average page view (PV) for thelogistics advertisement project was 5 million, with peak values reach-
ing 20 million. The traffic allocation between the experimental group
and the control group transitioned from 2:8 to 5:5, and eventually, the
proposed MCAC was fully deployed online.
Online deployment results: Table 7 shows the conversion prediction
results of online A/B testing. We choose the online conversion rate
as the performance indicator to evaluate different online models.
The online conversion rate is calculated by:number of conversions
number of impressions×
100%. In online A/B testing, we use different logistics advertisement
conversion prediction algorithms within different groups to predict
the conversion probability of users and rank them accordingly.
Then the advertisement system of the logistics platform will expose
ads for logistics services to a certain proportion of users based on
the ranking of the predicted conversion probability. Finally, we
collect the number of successful conversion users and calculate the
conversion rate based on the above equation.
Table 7: Results on online A/B testing.
Metho
d Submission task Conversion task
LightGBM
— —
Mix-Expert-LightGBM +9.78% +13.56%
MCAC +15.22% +23.12%
In terms of online results, we find that the MCAC model out-
performs two baselines deployed in control groups. The MCAC
improves by 15.22 % in the information submission task and by
23.12 % in the conversion task compared to LightGBM.
4.9.2 Cold Start Analysis: In logistics advertising, the cold start
users refer to impression users lacking offline shipping behaviors,
which accounts for about 4.4% of the total impression users. The
historical submission and conversion rates of cold start users are
0.4% and 0.2%, which are lower than the overall conversion results.
Through data-driven analysis, we find users in the same spatial
region (e.g., residential, industrial parks, or office buildings) share
similar shipping interests and preferences. Therefore, we take the
following two steps to optimize the cold start problem: (i) Using
online user profiles to calculate the similarity between users in the
same region to generate the top 𝐿most similar users of each cold
start user; (ii) Using the mean pooling of the preference vectors
of top𝐿most similar users as the offline preference vector of cold
start users.
By testing the conversion prediction performance of cold start
users, we find MCAC achieves a 0.74 submission AUC and a 0.80
conversion AUC, which is lower than the overall performance in
the full dataset. But the metrics still indicate the effectiveness of
the proposed method because the precision@1% of submission and
conversion rate of our model in cold-start scenarios are 3.58% and
2.44%, while the historical submission rate and conversion rate of
cold-start users in our dataset are only 0.4% and 0.2%.
4.9.3 Privacy Protection: In logistics advertising, we take the
following steps to protect the privacy of users: (i) We replace iden-
tifiable IDs, such as user IDs, waybill IDs, and order IDs by serial
identifiers (e.g., SHA256); (ii) Sensitive data (e.g., the plaintext ad-
dress) from online and offline behaviors have undergone encryption.
5035Multi-task Conditional Attention Network for Conversion Prediction in Logistics Advertising KDD ’24, August 25–29, 2024, Barcelona, Spain
We use coarse-grained region-level spatial information and do not
employ specific location information.
5 Related works
5.1 Conversion Prediction Methods
Extensive studies have been conducted to enhance the conver-
sion task in online recommendation systems. Rendle et al. [ 21]
propose a factorization machine-based algorithm to address the
high-dimensional sparse data by mapping it into a low-dimensional
dense vector. Juan et al. [ 11] introduce the Field-aware Factorization
Machines (FFM) model, which takes into account the possibility
of features from different domains. In recent years, learning-based
methods [3,8,31,34] have been used for conversion prediction con-
sidering the capabilities in exploring high-order implicit informa-
tion between features. Zhang et al. [ 31] proposed the Factorization
Machine Supported Neural Network algorithm to integrate the FM
algorithm with deep learning. Furthermore, some studies [ 34] [8]
enhance the accuracy of prediction tasks from a user-centric per-
spective by capturing users’ interests and preferences
5.2 Multi-task Learning-based Methods
To address the data sparsity problem in conversion prediction prob-
lems, multi-task learning-based methods have been used widely
by leveraging inter-task correlations. Ma et al. [ 17] proposed a
multi-gate mixture-of-experts (MMoE) to explicitly model the task
relationships with a gating network. Tang et al. [ 24] propose a
progressive layered extraction (PLE) algorithm to gradually extract
and separate deeper knowledge for balancing multiple tasks. Ma et
al. [18] propose an entire space multi-task model, which introduces
two auxiliary tasks (click-through rate prediction and post-click
conversion prediction) to decompose the sequential dependencies
along the user conversion path. Wen et al. [ 28] extend ESMM to fur-
ther address the data sparsity challenge by modeling several parallel
post-click actions in the conversion process. Xi et al. [ 29] explicitly
model multiple steps in the conversion sequence within the struc-
ture of multi-task learning models leveraging the positive feedback
information from the previous conversion step. Some works also ap-
ply self-attention [ 14,30] or conditional attention [ 13] mechanisms
into multi-task conversion prediction frameworks. These works
focus on recommendations in online advertising scenarios, where
the attention mechanism is used to learn relationships between
different tasks taking online behavioral features as inputs. Differ-
ent from existing multi-task learning-based conversion prediction
works, our paper aims to design a practical logistics advertising
framework considering both online purchasing and offline shipping
behaviors while existing works mainly focus on online perspectives.
6 Conclusion
In this study, we introduce MCAC, a multi-task conditional atten-
tion Network-based conversion prediction framework for logistics
advertising considering both online and offline behaviors. To en-
hance conversion prediction performance, we design an offline
shipping preference extraction module to model offline behaviors
and a multi-task conditional attention module to learn relationships
among multiple conversion steps. Both the offline experiments con-
ducted on real-world industrial datasets and public datasets andthe online A/B testing results demonstrate substantial advantages
of MCAC compared to baselines.
Acknowledgments
This work was supported in part by National Science and Tech-
nology Major Project 2021ZD0114200, National Natural Science
Foundation of China under Grant No. 62272098.
References
[1]Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting Sys-
tem. Proceedings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining (2016).
[2]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan
Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.
2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st
Workshop on Deep Learning for Recommender Systems, DLRS@RecSys 2016, Boston,
MA, USA, September 15, 2016, Alexandros Karatzoglou, Balázs Hidasi, Domonkos
Tikk, Oren Sar Shalom, Haggai Roitman, Bracha Shapira, and Lior Rokach (Eds.).
ACM, 7–10. https://doi.org/10.1145/2988450.2988454
[3]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan
Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.
2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st
Workshop on Deep Learning for Recommender Systems, DLRS@RecSys 2016, Boston,
MA, USA, September 15, 2016, Alexandros Karatzoglou, Balázs Hidasi, Domonkos
Tikk, Oren Sar Shalom, Haggai Roitman, Bracha Shapira, and Lior Rokach (Eds.).
ACM, 7–10. https://doi.org/10.1145/2988450.2988454
[4]Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014).
[5]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).
[6] SF Express. 2023. SF Express Official Website. https://www.sf-express.com/
[7] FedEx. 2023. FedEx Official Website. https://www.fedex.com/
[8]Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and
Keping Yang. 2019. Deep Session Interest Network for Click-Through Rate
Prediction. In Proceedings of the Twenty-Eighth International Joint Conference on
Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus
(Ed.). ijcai.org, 2301–2307. https://doi.org/10.24963/ijcai.2019/319
[9]Simon Haykin. 1999. Neural Networks: A Comprehensive Foundation. Prentice
Hall.
[10] JD.com. 2023. JD.com. https://global.jd.com/
[11] Yu-Chin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-
aware Factorization Machines for CTR Prediction. In Proceedings of the 10th ACM
Conference on Recommender Systems, Boston, MA, USA, September 15-19, 2016,
Shilad Sen, Werner Geyer, Jill Freyne, and Pablo Castells (Eds.). ACM, 43–50.
https://doi.org/10.1145/2959100.2959134
[12] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boost-
ing Decision Tree. In Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, December
4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy
Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (Eds.). 3146–3154. https://proceedings.neurips.cc/paper/2017/hash/
6449f44a102fde848669bdd9eb6b76fa-Abstract.html
[13] Shunsuke Kitada, Hitoshi Iyatomi, and Yoshifumi Seki. 2019. Conversion pre-
diction using multi-task conditional attention networks to support the creation
of effective ad creatives. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 2069–2077.
[14] Qi Liu, Zhilong Zhou, Gangwei Jiang, Tiezheng Ge, and Defu Lian. 2023. Deep
Task-specific Bottom Representation Network for Multi-Task Recommendation.
InProceedings of the 32nd ACM International Conference on Information and
Knowledge Management. 1637–1646.
[15] JD Logistics. 2023. JD Logistics Official Website. https://www.jdl.com/
[16] Jiaqi Ma, Zhe Zhao, Jilin Chen, Ang Li, Lichan Hong, and Ed H. Chi. 2019. SNR:
Sub-Network Routing for Flexible Parameter Sharing in Multi-Task Learning.
InThe Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The
Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019,
The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019. AAAI Press,
216–223. https://doi.org/10.1609/aaai.v33i01.3301216
5036KDD ’24, August 25–29, 2024, Barcelona, Spain Baoshen Guo et al.
[17] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018.
Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-
of-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018,
Yike Guo and Faisal Farooq (Eds.). ACM, 1930–1939. https://doi.org/10.1145/
3219819.3220007
[18] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun
Gai. 2018. Entire Space Multi-Task Model: An Effective Approach for Estimating
Post-Click Conversion Rate. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA,
July 08-12, 2018, Kevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun
Liu, and Emine Yilmaz (Eds.). ACM, 1137–1140. https://doi.org/10.1145/3209978.
3210104
[19] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016.
Cross-Stitch Networks for Multi-task Learning. In 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,
2016. IEEE Computer Society, 3994–4003. https://doi.org/10.1109/CVPR.2016.433
[20] Zhen Qin, Yicheng Cheng, Zhe Zhao, Zhe Chen, Donald Metzler, and Jingzheng
Qin. 2020. Multitask Mixture of Sequential Experts for User Activity Streams. In
KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, Virtual Event, CA, USA, August 23-27, 2020, Rajesh Gupta, Yan Liu, Jiliang
Tang, and B. Aditya Prakash (Eds.). ACM, 3083–3091.
[21] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme.
2011. Fast context-aware recommendations with factorization machines. In
Proceeding of the 34th International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011,
Wei-Ying Ma, Jian-Yun Nie, Ricardo Baeza-Yates, Tat-Seng Chua, and W. Bruce
Croft (Eds.). ACM, 635–644. https://doi.org/10.1145/2009916.2010002
[22] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders Søgaard. 2017.
Sluice networks: Learning what to share between loosely related tasks. CoRR
abs/1705.08142 (2017). arXiv:1705.08142 http://arxiv.org/abs/1705.08142
[23] Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2021.
Progressive multi-task learning with controlled information flow for joint en-
tity and relation extraction. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 13851–13859.
[24] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progres-
sive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for
Personalized Recommendations. In RecSys 2020: Fourteenth ACM Conference on
Recommender Systems, Virtual Event, Brazil, September 22-26, 2020, Rodrygo L. T.
Santos, Leandro Balby Marinho, Elizabeth M. Daly, Li Chen, Kim Falk, Noam
Koenigstein, and Edleno Silva de Moura (Eds.). ACM, 269–278.
[25] United Parcel Service (UPS). 2023. UPS Official Website. https://www.ups.com/
[26] Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proes-
mans, Dengxin Dai, and Luc Van Gool. 2022. Multi-Task Learning for Dense
Prediction Tasks: A Survey. IEEE Trans. Pattern Anal. Mach. Intell. 44, 7 (2022),
3614–3633. https://doi.org/10.1109/TPAMI.2021.3054719
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is
All you Need. In Advances in Neural Information Processing Systems 30: An-
nual Conference on Neural Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, SamyBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (Eds.). 5998–6008. https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
[28] Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and Keping
Yang. 2020. Entire Space Multi-Task Modeling via Post-Click Behavior Decom-
position for Conversion Rate Prediction. In Proceedings of the 43rd International
ACM SIGIR conference on research and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang, Yi Chang, Xueqi
Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM,
2377–2386. https://doi.org/10.1145/3397271.3401443
[29] Dongbo Xi, Zhen Chen, Peng Yan, Yinger Zhang, Yongchun Zhu, Fuzhen Zhuang,
and Yu Chen. 2021. Modeling the Sequential Dependence among Audience Multi-
step Conversions with Multi-task Learning in Targeted Display Advertising. In
KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, Virtual Event, Singapore, August 14-18, 2021, Feida Zhu, Beng Chin Ooi,
and Chunyan Miao (Eds.). ACM, 3745–3755.
[30] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022. Leaving no
one behind: A multi-scenario multi-task meta learning approach for advertiser
modeling. In Proceedings of the Fifteenth ACM International Conference on Web
Search and Data Mining. 1368–1376.
[31] Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep Learning over Multi-
field Categorical Data - - A Case Study on User Response Prediction. In Advances
in Information Retrieval - 38th European Conference on IR Research, ECIR 2016,
Padua, Italy, March 20-23, 2016. Proceedings (Lecture Notes in Computer Science,
Vol. 9626) , Nicola Ferro, Fabio Crestani, Marie-Francine Moens, Josiane Mothe,
Fabrizio Silvestri, Giorgio Maria Di Nunzio, Claudia Hauff, and Gianmaria Silvello
(Eds.). Springer, 45–57. https://doi.org/10.1007/978-3-319-30671-1_4
[32] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,
and Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate
Prediction. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI
2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference,
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019.
AAAI Press, 5941–5948. https://doi.org/10.1609/aaai.v33i01.33015941
[33] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,
and Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate
Prediction. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI
2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference,
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019.
AAAI Press, 5941–5948. https://doi.org/10.1609/aaai.v33i01.33015941
[34] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma,
Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for
Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK,
August 19-23, 2018, Yike Guo and Faisal Farooq (Eds.). ACM, 1059–1068.
[35] Yongchun Zhu, Yudan Liu, Ruobing Xie, Fuzhen Zhuang, Xiaobo Hao, Kaikai Ge,
Xu Zhang, Leyu Lin, and Juan Cao. 2021. Learning to Expand Audience via Meta
Hybrid Experts and Critics for Recommendation and Advertising. In KDD ’21: The
27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual
Event, Singapore, August 14-18, 2021, Feida Zhu, Beng Chin Ooi, and Chunyan
Miao (Eds.). ACM, 4005–4013. https://doi.org/10.1145/3447548.3467093
5037