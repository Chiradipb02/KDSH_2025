CASH via Optimal Diversity for Ensemble Learning
Pranav Poduval
MasterCard AI Garage
Gurgaon, India
pranav.poduval@mastercard.comSanjay Kumar Patnala
MasterCard AI Garage
Gurgaon, India
sanjaykumar.patnala@mastercard.comGaurav Oberoi
MasterCard AI Garage
Gurgaon, India
gaurav.oberoi@mastercard.com
Nitish Srivasatava
MasterCard AI Garage
Gurgaon, India
nitish.srivasatava@mastercard.comSiddhartha Asthana
MasterCard AI Garage
Gurgaon, India
siddhartha.asthana@mastercard.com
ABSTRACT
The Combined Algorithm Selection and Hyperparameter Optimiza-
tion (CASH) problem is pivotal in Automatic Machine Learning
(AutoML). Most leading approaches combine Bayesian optimiza-
tion with post-hoc ensemble building to create advanced AutoML
systems. Bayesian optimization (BO) typically focuses on identify-
ing a singular algorithm and its hyperparameters that outperform
all other configurations. Recent developments have highlighted
an oversight in prior CASH methods: the lack of consideration
for diversity among the base learners of the ensemble. This over-
sight was overcome by explicitly injecting the search for diversity
into the traditional CASH problem. However, despite recent devel-
opments, BO’s limitation lies in its inability to directly optimize
ensemble generalization error, offering no theoretical assurance
that increased diversity correlates with enhanced ensemble perfor-
mance. Our research addresses this gap by establishing a theoretical
foundation that integrates diversity into the core of BO for direct
ensemble learning. We explore a theoretically sound framework
that describes the relationship between pair-wise diversity and
ensemble performance, which allows our Bayesian optimization
framework Optimal Diversity Bayesian Optimization (OptDivBO)
to directly and efficiently minimize ensemble generalization error.
OptDivBO guarantees an optimal balance between pairwise diver-
sity and individual model performance, setting a new precedent
in ensemble learning within CASH. Empirical results on 20 public
datasets show that OptDivBO achieves the best average test ranks
of 1.57 and 1.4 in classification and regression tasks.
CCS CONCEPTS
•Computing methodologies →Learning settings; Continu-
ous space search; Discrete space search; Ensemble methods.
KEYWORDS
CASH, AutoML, Bayesian Optimization, Ensemble Learning
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 24–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671894ACM Reference Format:
Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava,
and Siddhartha Asthana. 2024. CASH via Optimal Diversity for Ensemble
Learning. In Proceedings of Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24). ACM, New York, NY,
USA, 9 pages. https://doi.org/10.1145/3637528.3671894
1 INTRODUCTION
The past decade has witnessed remarkable advancements in Ma-
chine Learning (ML), introducing novel architectures and scaling
techniques across various domains such as computer vision, natural
language processing, and recommendation systems [ 9] [24] [28].
However, designing bespoke solutions for individual problems is
knowledge-intensive and laborious, presenting a significant barrier
to entry. This challenge is further compounded by the No Free
Lunch theorem [ 10], asserting that no single ML algorithm can con-
sistently outperform others across all applications. This relevance
is underscored by the intricate processes of algorithm selection, hy-
perparameter tuning, and neural architecture search. Moreover, the
increasing sophistication of state-of-the-art ML techniques poses a
significant challenge for experts attempting to integrate the latest
best practices into their models. In response to these challenges,
the AutoML community introduced the Combined Algorithm Selec-
tion and Hyperparameter Optimization with tools like AutoWEKA
[25], marking a pivotal advancement in lowering barriers and de-
mocratizing the expertise required for deploying high-performance
ML models. Notably, systems such as Auto-sklearn [ 7] and Auto-
PyTorch [ 31] have recognized that ensembles often underlie top-
performing solutions in multiple Kaggle competitions [ 11] [9]. As a
result, these systems integrate ensembling techniques, constructing
ensembles post-hoc from the pool of hyperparameters explored
during Bayesian Optimization.
DivBO [ 22] made a noteworthy observation: the true objective
of CASH is not fully aligned with that of ensemble learning, despite
the success of post-hoc ensembling techniques. In previous CASH
approaches, the goal of Bayesian Optimization is to identify the
optimal hyperparameter set ℎ∗that minimizes the expected vali-
dation errorL(𝑌,𝑓ℎ∗(𝑋)). However, the true objective of CASH is
to identify a set of hyperparameters [ℎ∗
1,···,ℎ∗
𝑁]that minimizes
the ensemble generalization error LEnsemble(𝑌,1
𝑁Í𝑁
𝑖=0𝑓ℎ∗
𝑖(𝑋)). Di-
vBO emphasized the importance of diversity in ensemble learning,
addressing a gap overlooked by earlier CASH methods. It is well-
known that ensembles composed of individually strong and di-
verse models yield superior performance [ 30]. DivBO sought to
 
2411
KDD ’24, August 24–29, 2024, Barcelona, Spain Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava, & Siddhartha Asthana
address this by incorporating a diversity-seeking component into
the Bayesian optimization objective. Specifically, it introduced a
’diversity surrogate,’ a novel mechanism for predicting the pairwise
diversity between two configurations not previously encountered.
This strategy encourages the exploration of hyperparameters dis-
tinct from those in the current ensemble pool, thus enriching the
solution’s diversity.
The primary limitation of DivBO is its inability to identify the
specific type of diversity optimal for minimizing the true target:
the ensemble generalization error. As a result, DivBO does not fully
bridge the gap in current CASH approaches, underscoring the need
for further development of a Bayesian optimization framework that
directly optimizes for ensemble risk. Our research addresses this
shortfall by pioneering an approach to the CASH problem, deriving
diversity from first principles. This allows us to decompose stan-
dard loss functions into components reflecting average individual
model performance and pairwise diversity. This methodology is
theoretically robust and practically feasible, effectively minimiz-
ing the ensemble generalization error—a goal not fully realized by
previous CASH approaches.
In this paper, we introduce OptDivBO, a Bayesian optimiza-
tion approach explicitly designed to identify hyperparameters that
minimize ensemble risk by optimally balancing individual model
performance with model diversity.
•OptDivBO marks the first application of Bayesian optimiza-
tion within the CASH framework that explicitly aims to
minimize the ensemble’s generalization error, setting a new
precedent in the field of CASH.
•We elucidate how the traditional risk associated with en-
semble models in both regression and classification tasks
(including mean square, mean absolute, cross-entropy, and
Brier score) can be upper-bounded by components of in-
dividual model performance and pairwise diversity. This
revelation enables us to conceptualize "optimal diversity", a
critical factor overlooked by prior approaches.
•Empirical evaluations on publicly available datasets demon-
strate that OptDivBO outperforms all previous post-hoc en-
sembling CASH approaches in general black-box optimiza-
tion tasks, validating its superiority.
2 PRELIMINARIES AND RELATED WORKS
Combined algorithm selection and hyperparameters (CASH)
was introduced by Auto-WEKA [ 25]. The objective of a CASH algo-
rithm, given a dataset D={Dtrain,Dval}and a predefined set of
algorithmsA=𝐴1,...,𝐴𝐾, is to identify the optimal algorithm 𝐴∗
and its corresponding hyperparameters 𝜆∗that optimize a specified
metric. For instance, in regression tasks, this often involves min-
imizing the mean square error E(𝑥,𝑦)∼D val[||𝑦−𝑓𝐴,𝜆(𝑥)||2]. The
resolution of CASH typically employs Bayesian optimization [ 2]
[23], wherein a surrogate function (commonly a Gaussian Process)
is fitted to all observed pairs of hyperparameters and algorithms
(with the specified metric as the target).An acquisition function,
such as the Upper Confidence Bound (UCB), is then utilized to guide
the exploration of future configurations. This approach strategi-
cally balances exploration and exploitation to optimize the objective
function effectively.The CASH problem has significantly evolved over time, with
various methodologies enhancing its efficiency and broadening its
application scope. For instance, approaches like Rising Bandits [ 14]
have improved the efficiency of CASH by iteratively eliminating
less promising algorithms and concentrating resources on the most
promising ones. On the other hand, TPOT [ 20] diverges from tradi-
tional Bayesian Optimization, employing genetic programming to
navigate the algorithm selection and hyperparameter tuning land-
scape. Furthermore, an ADMM-based method [ 17] deconstructs
the CASH problem into subproblems, which are then individu-
ally tackled using the Alternating Direction Method of Multipliers
(ADMM).
Building on the foundations laid by Auto-WEKA, subsequent
approaches have addressed its weakness in incorporating Ensem-
ble Learning. Notably, Auto-SKLEARN [ 7] introduced a post-hoc
method for creating ensembles from all configurations explored
during Bayesian Optimization. This method has been empirically
demonstrated to be more robust against overfitting compared to
traditional techniques such as boosting [ 5], bagging [ 5], and stack-
ing [ 4]. Consequently, this post-hoc approach to ensemble creation
has been adopted by future AutoML systems, including VolcanoML
[16] and Auto-Pytorch [ 31]. The process involves starting with
an empty ensemble and iteratively adding models (with replace-
ment) that are orthogonal to the current ensemble set and that
enhance validation performance. It becomes evident that the true
objective of ensemble-oriented CASH is to minimize the ensem-
ble generalization error, denoted as LEnsemble(𝑌,1
𝑁Í𝑁
𝑖=0𝑓ℎ𝑖(𝑋)),
ℎ𝑖=(𝐴𝑖,𝜆𝑖). However, a misalignment exists with this objective in
the standard BO approach utilized by Auto-SKLEARN, VolcanoML,
and others, as BO traditionally proposes hyperparameters expected
to yield promising individual performance, without considering
their collective performance in an ensemble. While Ensemble Op-
timization [ 13] aims to rectify this by considering the interaction
of hyperparameters with the existing ensemble pool, this method
has empirically underperformed in comparison to simple post-hoc
ensembles. This underperformance is attributed to its unstable op-
timization process, which is significantly affected by the addition
of any sub-optimal configuration to the ensemble pool.
In response to the misalignment between the actual objectives
of CASH and the BO framework utilized by prior approaches like
Auto-SKLEARN and VolcanoML, Diversity-aware Bayesian Opti-
mization (DivBO) introduced an explicit search for diversity within
the BO objective function. It accomplished this by establishing
an additional surrogate function, designed to predict the diver-
sity between two unseen configurations, formulated as 𝐷(ℎ𝑖,ℎ𝑗)=
1
|Dval|E(𝑥,𝑦)∼D val||𝑓ℎ𝑖(𝑥)−𝑓ℎ𝑗(𝑥)||2for classification tasks. The
acquisition function guiding the hyperparameter search became a
linear combination of the traditional "performance" surrogate and
this new diversity surrogate. This approach to diversity incentivizes
the exploration of hyperparameters that yield predictions distinct
from those currently in the ensemble pool. While DivBO repre-
sented an innovative step towards authentic ensemble learning, it
was not devoid of limitations. It’s evident that overemphasizing
DivBO’s notion of diversity could potentially degenerate the pool of
learners, resulting in models that predict all classes incorrectly but
 
2412CASH via Optimal Diversity for Ensemble Learning KDD ’24, August 24–29, 2024, Barcelona, Spain
remain distinct from others in the ensemble. DivBO did not thor-
oughly examine the functional form of diversity and its effect on
ensemble generalization error. The intricate relationship between
diversity and ensemble performance constitutes a significant body
of ensemble learning literature [ 1] [12], one that has been largely
overlooked in the CASH methods until now.
3 DIVERSITY-AWARE BAYESIAN
OPTIMIZATION
Before delving into OptDivBO, it is essential to revisit Diversity-
aware Bayesian optimization (DivBO), the diversity-aware method
for addressing the CASH problem within the realm of ensemble
learning, employing BO. DivBO introduced diversity into the BO
process through a two-fold approach: 1) establishing a diversity
metric that quantified the similarity or dissimilarity between two
distinct configurations, and 2) formulating a modified acquisition
function. This acquisition function aimed to propose configurations
that not only demonstrated promising performance but also main-
tained a level of diversity from existing members of the ensemble,
thereby enriching the ensemble’s overall predictive power.
3.0.1 DiBO Diversity Metric and Diversity Surrogate . DivBO in-
corporates a pairwise diversity metric Div(ℎ𝑖,ℎ𝑗)which has been
empirically demonstrated to enhance the diversity of neural net-
works [ 30]. Here,ℎ𝑖represents the joint configuration of algorithm
𝑎𝑖and its corresponding hyperparameters 𝜆𝑖. The diversity metric
is defined as follow:
Div(ℎ𝑖,ℎ𝑗)=1√
2|Dval|∑︁
(𝑥,𝑦)∼D val||𝑓ℎ𝑖(𝑥)−𝑓ℎ𝑗(𝑥)||2 (1)
This specific metric, particularly in classification tasks, encour-
ages the Bayesian Optimization process to consider configurations
whose predictions significantly diverge from those previously se-
lected in the temporary ensemble pool 3.0.2. In this context, Dval
denotes the validation set, and 𝑓ℎ𝑖(𝑥)represents the learner associ-
ated with configuration ℎ𝑖, which was fitted on the train set Dtrain.
To model the diversity between two unseen configurations (ℎ𝑖,ℎ𝑗),
DivBO employs a second surrogate function. This diversity surro-
gate, akin to traditional BO, maps unseen configurations (ℎ𝑖,ℎ𝑗)to
the predictive mean and variance of pair-wise diversity.
The diversity surrogate Mdivconsists of an ensemble of light-
GBM models, selected over the traditional Gaussian Process for
its substantial computational efficiency — O(|𝐷|2log|𝐷|)as com-
pared toO(|𝐷|3). Owing to the symmetry of the diversity metric,
Div(ℎ𝑖,ℎ𝑗)=Div(ℎ𝑗,ℎ𝑖)the number of observations |𝐷|leads to a
quadratic increase in training data points, amounting to |𝐷|2.
3.0.2 Ensemble Pool and Acquisition Function . DivBO maintains a
temporary pool of ensembles, denoted as P, which comprises all
base learners that could potentially form part of the final ensemble.
This ensemble pool is constructed using the traditional ad-hoc
method employed by auto-sklearn [ 7], applied across the entire
history of observations. The acquisition function is designed to
propose configurations that are distinct from those in the current
observation pool. The diversity acquisition function is defined as -
𝛼div(ℎ)=1
𝑁𝑁∑︁
𝑖=0min
𝜃∈PM𝑖
div(ℎ,𝜃) (2)Here,M𝑖
div(ℎ,𝜃)represents the 𝑖𝑡ℎsampled value from the output
distribution of our diversity surrogate Mgiven a pair of config-
urations(𝜃,ℎ). The final acquisition function is the average of N
minimums via sampling. DivBO’s ultimate acquisition function
is a weighted linear combination of the traditional performance-
based acquisition function and the diversity acquisition function,
expressed as -
𝛼DivBO(ℎ)=𝛼perf(ℎ)+𝑤𝛼div(ℎ),𝑤=𝛽(sigmoid(𝛾𝑡)−0.5) (3)
In the above equation, 𝑤signifies the weight of the diversity acqui-
sition function, with 𝑡representing the number of BO iterations.
The parameters 𝛽and𝛾are hyperparameters that control the be-
havior of the acquisition function, ensuring a balanced integration
of performance and diversity in the model selection process.
4 OPTIMAL DIVERSITY FOR CASH
Upon closely examining DivBO, its shortcomings become pro-
nounced. A significant limitation is the absence of a theoretical
framework guaranteeing the minimization of the ensemble general-
ization error. Relying solely on increasing DivBO’s diversity metric
can inadvertently result in degenerate solutions that, while diverse,
are consistently inaccurate. Moreover, the optimality of DivBO’s
diversity metric’s functional form is questionable, particularly in
extreme case scenarios where a learner can precisely predict the
label or target. This casts doubt on the necessity of optimizing
DivBO’s diversity metric, which could unnecessarily push correct
predictions away from the true target.
While extensive research has been conducted to elucidate the
relationship between diversity and ensemble generalization error
[1] [19] [21] [8] [3], this intricate interplay has been largely over-
looked in the CASH literature, including DivBO. Our work aims to
bridge this gap, integrating insights from both the CASH domain
and diversity-based ensemble learning literature.
Building upon the existing literature on diversity-based ensem-
bles, we propose a theoretical framework that upper-bounds the
loss of an ensemble in terms of the average performance of indi-
vidual models and a pairwise diversity metric. Mathematically, this
relationship is delineated as follows
LEnsemble(𝑌,1
𝑁𝑁∑︁
𝑖=0𝑓ℎ𝑖(𝑋))≤1
𝑁𝑁∑︁
𝑖=0L𝑖(𝑌,𝑓ℎ𝑖(𝑋))
+𝑁∑︁
𝑖=0∑︁
𝑗≠𝑖Div(𝑌,𝑓ℎ𝑖(𝑋),𝑓ℎ𝑗(𝑋))
(4)
The optimal diversity, then, is defined as OptDiv(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)=
Div(𝑌,𝑓ℎ𝑖(𝑋),𝑓ℎ𝑗(𝑋))+Div(𝑌,𝑓ℎ𝑗(𝑋),𝑓ℎ𝑖(𝑋)), serving as the tar-
get for our diversity surrogate under the standard DivBO frame-
work. Distinct from prior diversity-based ensemble learning ap-
proaches [ 1,19,21], our methodology opts to upper-bound the en-
semble generalization error with a pairwise diversity metric. This
choice facilitates integration into the DivBO framework within the
CASH domain. By decomposing the ensemble generalization error
in the above manner, our approach ensures that minimizing the
individual model performance and the Optimal Diversity metric
(OptDiv(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)) within the Bayesian Optimization framework
guarantees an improvement in the ensemble’s overall performance.
 
2413KDD ’24, August 24–29, 2024, Barcelona, Spain Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava, & Siddhartha Asthana
Our Diversity metric is distinct from DivBO’s because it is not
only a function of the learners 𝑓ℎ𝑖/𝑓ℎ𝑗, but also depends on the
labels or target (depending on classification or regression task).
Intuitively it makes sense that a Diversity metric should not be
independent of the task at hand. The exact form of the diversity
metric varies according to the task and the loss function targeted
for optimization. In this discussion, we will explore standard loss
functions such as mean square error (MSE) and mean absolute error
(MAE) for regression tasks, along with cross-entropy (CE) and Brier
score (BS) for classification tasks.
4.0.1 Regression 1. : In regression problems, the mean square error
is commonly chosen as the metric to be minimized, with the en-
semble generalization error as E(𝑥,𝑦)∼D𝑣𝑎𝑙[|𝑦−1
𝑁Í𝑁
𝑖=0𝑓ℎ𝑖(𝑥)|2].
Previous research has noted that the discrepancy between ensemble
generalization error and the average performance of individual mod-
els is proportional to the sample variance Var(𝑓ℎ)[1] [21], which
can be considered a form of diversity metric. While this is not a
pairwise diversity metric and thus not directly applicable within
the DivBO framework, the revelation that the difference between
ensemble error and average individual model performance gives
rise to a diversity metric offers valuable insights for our approach
Mathematically, to decompose the ensemble generalization error
into individual model error and pairwise diversity, we examine the
following expression:
E(𝑥,𝑦)∼D𝑣𝑎𝑙𝑦−1
𝑁𝑁∑︁
𝑖=0𝑓ℎ𝑖(𝑥)2=
1
𝑁2E(𝑥,𝑦)∼D𝑣𝑎𝑙𝑁∑︁
𝑖=0(𝑦−𝑓ℎ𝑖(𝑥))2
=1
𝑁2E(𝑥,𝑦)∼D𝑣𝑎𝑙𝑁∑︁
𝑖=0|𝑦−𝑓ℎ𝑖(𝑥)|2+2𝑁∑︁
𝑖=0∑︁
𝑗<𝑖(𝑦−𝑓ℎ𝑖(𝑥))(𝑦−𝑓ℎ𝑗(𝑥))
=1
𝑁2E(𝑥,𝑦)∼D𝑣𝑎𝑙"𝑁∑︁
𝑖=0|𝑦−𝑓ℎ𝑖(𝑥)|2#
+2
𝑁2𝑁∑︁
𝑖=0∑︁
𝑗<𝑖E(𝑥,𝑦)∼D𝑣𝑎𝑙h
(𝑦−𝑓ℎ𝑖(𝑥))(𝑦−𝑓ℎ𝑗(𝑥))i
(5)
In the case of mean square error, we arrive at an exact equal-
ity rather than an upper bound as detailed in Equation 4, where
OptDiv𝑚𝑠𝑒(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)=2E(𝑥,𝑦)∼D𝑣𝑎𝑙[𝜖𝑖𝜖𝑗]. Where𝜖𝑖=𝑦−𝑓ℎ𝑖(𝑥).
Contrary to DivBO’s diversity metric, which aims to increase the
divergence between the predictions of different configurations,
this metric of optimal diversity is focused on not merely push-
ing the predictions apart. Instead, it emphasizes the nuanced ap-
proach of minimizing the covariance between errors (assuming
E(𝑥,𝑦)∼D𝑣𝑎𝑙[𝜖]=0), indicating a sophisticated strategy for enhanc-
ing ensemble performance.
4.0.2 Regression 2. Another prevalent metric in regression-based
problems is the mean absolute error (mae), with the ensemble gen-
eralization error expressed as E(𝑥,𝑦)∼D𝑣𝑎𝑙[|𝑦−1
𝑁Í𝑁
𝑖=0𝑓ℎ𝑖(𝑥)|]. To
decompose MAE into individual model components and pairwisediversity, we proceed as follows:
E(𝑥,𝑦)∼D𝑣𝑎𝑙[|𝑦−1
𝑁𝑁∑︁
𝑖=0𝑓ℎ𝑖(𝑥)|]=E(𝑥,𝑦)∼D𝑣𝑎𝑙[vut
|𝑦−1
𝑁𝑁∑︁
𝑖=0𝑓ℎ𝑖(𝑥)|2]
=1
𝑁E(𝑥,𝑦)∼D𝑣𝑎𝑙vuut𝑁∑︁
𝑖=0|𝑦−𝑓ℎ𝑖(𝑥)|2+2𝑁∑︁
𝑖=0∑︁
𝑗<𝑖𝜖𝑖𝜖𝑗
≤1
𝑁E(𝑥,𝑦)∼D𝑣𝑎𝑙vut𝑁∑︁
𝑖=0|𝑦−𝑓ℎ𝑖(𝑥)|2+vuut
2𝑁∑︁
𝑖=0∑︁
𝑗<𝑖|𝜖𝑖𝜖𝑗|
≤1
𝑁𝑁∑︁
𝑖=0E(𝑥,𝑦)∼D𝑣𝑎𝑙
|𝑦−𝑓ℎ𝑖(𝑥)|
+𝑁∑︁
𝑖=0∑︁
𝑗<𝑖√
2
𝑁E(𝑥,𝑦)∼D𝑣𝑎𝑙√︃
|𝜖𝑖𝜖𝑗|
(6)
In the context of MAE, the Optimal Diversity, OptDiv𝑚𝑎𝑒(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)
=√
2E(𝑥,𝑦)∼D𝑣𝑎𝑙[√︁
|𝜖𝑖𝜖𝑗|]. markedly differs from DivBO’s approach
and is closely tied to the target values 𝑌. Notably, minimizing this
form of optimal diversity ensures a reduction in the ensemble gen-
eralization error, a guarantee that prior CASH approaches have not
provided.
4.0.3 Classification 1. In classification tasks, cross-entropy (ce) is
a commonly optimized metric. The ensemble generalization error
is represented as E(𝑥,𝑦)∼D𝑣𝑎𝑙[−log(1
𝑁Í𝑁
𝑖=0𝑓(𝑦)
ℎ𝑖(𝑥))]. Here,𝑓(𝑦)
ℎ𝑖
denotes the probability that the learner associated with configura-
tionℎ𝑖assigns to the correct class 𝑦. To derive the optimal diversity,
we analyze the gap between the ensemble error and the average
individual performance:
E(𝑥,𝑦)∼D𝑣𝑎𝑙[−log(1
𝑁𝑁∑︁
𝑖=0𝑓(𝑦)
ℎ𝑖(𝑥))+1
𝑁𝑁∑︁
𝑗=0log(𝑓(𝑦)
ℎ𝑗(𝑥))]
=E(𝑥,𝑦)∼D𝑣𝑎𝑙[1
𝑁𝑁∑︁
𝑗=0log(𝑓(𝑦)
ℎ𝑗(𝑥)
Í𝑁
𝑖=0𝑓(𝑦)
ℎ𝑖(𝑥))−log(1
𝑁)](7)
This term can be interpreted as an information-theoretic quantifi-
cation of ensemble diversity [ 1]. However, since it is not a pair-
wise diversity metric, it cannot be directly utilized in the DivBO
framework. To circumvent this, we can upper-bound this metric by
observing that log(𝑓(𝑦)
ℎ𝑗(𝑥)
Í𝑁
𝑖=0𝑓(𝑦)
ℎ𝑖(𝑥)) ≤ log(𝑓(𝑦)
ℎ𝑗(𝑥)
𝑓(𝑦)
ℎ𝑖(𝑥)+𝑓(𝑦)
ℎ𝑗(𝑥))∀𝑖,𝑗. Ap-
plying this to the previous equation 7 yields our optimal pairwise
diversity metric:
E(𝑥,𝑦)∼D𝑣𝑎𝑙[1
𝑁𝑁∑︁
𝑗=0log(𝑓(𝑦)
ℎ𝑗(𝑥)
Í𝑁
𝑖=0𝑓(𝑦)
ℎ𝑖(𝑥))−log(1
𝑁)]
≤E(𝑥,𝑦)∼D𝑣𝑎𝑙[1
𝑁2𝑁∑︁
𝑗=0𝑁∑︁
𝑖=0log(𝑓(𝑦)
ℎ𝑗(𝑥)
𝑓(𝑦)
ℎ𝑗(𝑥)+𝑓(𝑦)
ℎ𝑖(𝑥))−log(1
𝑁)]
(8)
 
2414CASH via Optimal Diversity for Ensemble Learning KDD ’24, August 24–29, 2024, Barcelona, Spain
Hence, our optimal diversity metric OptDiv𝐶𝐸(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)
=1
𝑁E(𝑥,𝑦)∼D𝑣𝑎𝑙[log(𝑓(𝑦)
ℎ𝑗(𝑥)
𝑓(𝑦)
ℎ𝑗(𝑥)+𝑓(𝑦)
ℎ𝑖(𝑥))]
+1
𝑁E(𝑥,𝑦)∼D𝑣𝑎𝑙[log(𝑓(𝑦)
ℎ𝑖(𝑥)
𝑓(𝑦)
ℎ𝑖(𝑥)+𝑓(𝑦)
ℎ𝑗(𝑥))]
OptDiv𝐶𝐸(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)=1
𝑁E(𝑥,𝑦)∼D𝑣𝑎𝑙[log(𝑓(𝑦)
ℎ𝑗(𝑥)𝑓(𝑦)
ℎ𝑖(𝑥)
(𝑓(𝑦)
ℎ𝑗(𝑥)+𝑓(𝑦)
ℎ𝑖(𝑥))2)]
(9)
Similar to mse and mae optimal diversity metrics, this term too
depends on the labels, unlike DivBO.
4.0.4 Classification 2. The Brier Score (BS) is often regarded
as the classification counterpart to the mean square error (MSE)
used in regression problems. The ensemble generalization error for
the Brier Score is expressed as E(𝑥,𝑦)∼D𝑣𝑎𝑙[|𝑦−1
𝑁Í𝑁
𝑖=0𝑓ℎ𝑖(𝑥)|2
2],
where𝑦represents the one-hot encoded label, and 𝑓ℎ𝑖(𝑥)denotes
the probability distribution output by the learner for configura-
tionℎ𝑖. Given its conceptual resemblance to MSE, the approach to
deriving optimal diversity for the Brier Score follows a similar path.
Accordingly, we define the Optimal Diversity for the Brier Score
asOptDiv𝐵𝑆(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)=2E(𝑥,𝑦)∼D𝑣𝑎𝑙[(𝑦−𝑓ℎ𝑖(𝑥))·(𝑦−𝑓ℎ𝑗(𝑥))].
This derivation and all our diversity metrics are fundamentally
rooted in first principles, inherently relying on the labels or tar-
gets—a critical aspect overlooked by DivBO.
Our formulation of optimal diversity ensures that the dual opti-
mization of individual model performance and the diversity surro-
gate as implemented in the DivBO framework, ensures the mini-
mization of ensemble risk across both regression and classification
scenarios.
4.0.5 Acquisition Function . We utilize an ensemble of Gradient
Boosting Decision Trees as the Diversity Surrogate ( Mdiv(𝑓ℎ𝑖,𝑓ℎ𝑗)),
leveraging their well-established capability to provide well-calibrated
uncertainty estimates [ 18,26] and its relatively lower training time
complexity compared to traditional Gaussian Process. This feature
is pivotal for achieving an optimal balance between exploration and
exploitation of new hyperparameters. In contrast to DivBO, which
focuses on the minimum diversity P(see Eq. 2) , our derivation of
optimal diversity necessitates aggregating over all configurations
in the pool, as shown below:
𝜇div(ℎ)=1
𝑁𝑁∑︁
𝑖=0∑︁
𝜃∈PM𝑖
div(ℎ,𝜃) (10)
Drawing inspiration from the Gaussian Process Lower Confi-
dence Bound (GP-LCB) method, we define our acquisition function
𝛼𝑑𝑖𝑣(ℎ)=𝜇div(ℎ)−𝜅𝜎div(ℎ)as the acquisition function. 𝜎div(ℎ)
represents the standard deviation of the ensemble’s predictions.
The final acquisition function combines the performance surrogate
and the diversity surrogate:
𝛼(ℎ)=𝛼𝑃𝑒𝑟𝑓(ℎ)+𝑤 𝛼𝑑𝑖𝑣(ℎ),𝑤=2(𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝜏𝑡)−0.5) (11)Here,𝑤represents the weight assigned to the diversity acquisition
function, varying within the range [0,1),𝑡represents the num-
ber of BO iterations and 𝜏dictates the rate at which saturation is
approached. This weighting strategy ensures that, in the initial itera-
tions, configurations demonstrating strong individual performance
are selected to join the ensemble pool P.
Algorithm 1 outlines the OptDivBO procedure. In each itera-
tion, following the initial setup, OptDivBO performs the following
steps: 1) Fits the performance and diversity surrogates based on the
accumulated observations; 2) Constructs a temporary configura-
tion pool by implementing ensemble selection on the observation
history; 3) Samples candidate configurations and calculates their
ranking values based on the performance, diversity surrogate and
the pool; 4) Identifies and suggests a configuration that minimizes
the combined ranking value, as defined in Equation 11; 5) Evaluates
the suggested configuration using the validation set, subsequently
updating the observation dataset.
Algorithm 1 OptDivBO.
1:Input:Given search budget: 𝐵, architecture search space: X,
ensemble size: 𝐸, training and validation set: 𝐷train,𝐷val.
2:Initialize observations as 𝐷=∅;
3:while𝐵does not exhaust do
4: if|𝐷|<5then
5: Suggest a random configuration ˆ𝑥∈X;
6: else
7: Fit performance surrogate 𝑀perfand diversity surro-
gate𝑀divbased on observations 𝐷and task specific optimal
diversity metric as seen in section 4;
8: Build a temporary pool of configurations as 𝑃=
{Θ1,...,Θ𝐸}=EnsembleSelection (𝐷,𝐷 val,𝐸);
9: Compute the ranks of sampled configurations 𝑅perf
and𝑅divbased on the performance and diversity surrogates
𝑀perf,𝑀divand the temporary pool 𝑃;
10: Suggest a configuration ˆ𝑥=arg min𝑥∈X𝛼(𝑥)based on
Equation 11;
11: end if
12: Build and train the learner 𝑓ˆ𝑥on𝐷train and evaluate its
performance on 𝐷valasˆ𝑦;
13: Update the observations 𝐷=𝐷∪{(ˆ𝑥,ˆ𝑦)};
14: Generate a pool P ={Θ1,...,Θ𝐸} =
EnsembleSelection (𝐷,𝐷 val,𝐸);
15:end while
16:return the final ensemble Ensemble (Θ1,...,Θ𝐸);
5 EXPERIMENTS
In this section, we evaluate our proposed method, OptDivBO, across
20 real-world CASH problems using publicly available datasets. Sur-
prisingly, despite their capability to support regression tasks, most
prior CASH approaches [ 7,16,22] have not been assessed on re-
gression tasks. This oversight may stem from an overemphasis on
classification tasks in earlier works, inadvertently steering subse-
quent research towards classification to ensure fair comparisons.
Our experiments underscore two principal findings: 1) The Opt-
DivBO framework surpasses all previous ensemble learning-based
 
2415KDD ’24, August 24–29, 2024, Barcelona, Spain Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava, & Siddhartha Asthana
Table 1: Classification Test error (%) with standard deviations and the average rank across different datasets.
Method amazon_employee bank32nh cpu_act cpu_small eeg elevators house_8L pol
CASH Methods
RS 5.33±0.08 18.43±0.78 6.18±0.45 7.56±0.41 6.58±0.53 10.22±0.69 11.33±0.26 1.65±0.12
BO 5.27±0.06 18.46±0.50 5.68±0.38 7.77±0.92 5.48±1.36 10.24±0.86 11.19±0.19 1.59±0.50
RB 5.27±0.08 18.32±0.32 5.70±0.31 7.54±0.35 4.70±1.06 9.77±0.20 11.13±0.15 1.39±0.03
Methods for Ensemble Learning
EO 5.19±0.27 18.47±0.53 5.86±0.62 7.42±0.45 3.55±0.75 10.34±0.61 11.15±0.21 1.44±0.13
NES 5.33±0.38 18.22±0.32 5.58±0.20 7.27±0.85 2.69±0.74 9.75±0.30 11.28±0.48 1.74±0.35
Post-hoc Designs
RS-ES 5.29±0.15 18.31±0.66 5.71±0.26 720.00±0.30 4.46±0.22 9.51±0.28 11.21±0.38 1.40±0.15
BO-ES 5.24±0.15 18.41±0.39 5.50±0.47 7.15±0.28 3.55±0.77 9.61±0.36 11.06±0.33 1.35±0.18
RB-ES 5.20±0.11 18.06±0.57 5.58±0.20 7.09±0.22 2.87±0.92 10.01±0.15 10.81±0.27 1.36±0.16
DivBO 5.15±0.09 18.34±0.32 5.36±0.23 7.04±0.29 3.26±0.84 9.40±0.28 10.80±0.22 1.34±0.17
OptDivBO 5.11±0.10 18.36±0.44 5.29±0.38 6.99±0.49 2.99±0.88 9.28±0.40 10.77±0.25 1.31±0.16
Method Pollen Puma32H Quake Satimage Spambase Wind 2dplanes Average Rank
CASH Methods
RS 51.55±0.52 10.61±1.27 48.19±1.87 10.02±0.71 7.16±1.22 14.67±0.57 7.22±0.09 9.37
BO 49.66±3.38 10.43±0.82 46.90±2.15 9.23±0.98 6.46±0.83 14.53±0.61 7.16±0.07 7.47
RB 49.81±1.14 11.23±0.38 48.00±1.55 9.52±0.93 6.73±1.04 14.12±0.24 7.21±0.04 7.30
Methods for Ensemble Learning
EO 49.03±2.09 9.76±1.59 46.90±1.47 9.44±1.10 6.43±0.67 14.63±0.47 7.14±0.06 6.77
NES 51.58±1.52 10.65±0.55 46.44±1.14 8.68±0.94 6.25±0.67 14.26±0.49 7.12±0.07 5.87
Post-hoc Designs
RS-ES 49.71±1.62 10.60±0.74 46.81±1.56 9.37±0.72 6.47±0.22 14.35±0.46 7.12±0.11 6.20
BO-ES 48.93±1.74 9.29±1.12 46.12±2.51 9.12±0.86 6.39±0.63 14.05±0.52 7.08±0.09 4.13
RB-ES 49.60±1.33 7.87±0.42 46.72±1.33 8.57±1.35 6.14±0.35 13.99±0.44 7.21±0.07 3.67
DivBO 49.27±1.34 8.09±0.98 45.57±1.36 8.73±1.24 5.93±0.44 13.94±0.41 7.01±0.07 2.67
OptDivBO 48.90±1.44 7.88±0.51 45.05±1.37 8.55±0.70 5.85±0.51 13.92±0.49 6.9±0.1 1.57
Table 2: OptDivBO performs statistically better (B), the same (S), and worse (W).
(a) vs. RS-ES
B S W
OptDivBO 13 2 0(b) vs. DivBO
B S W
OptDivBO 12 3 0(c) vs. RB-ES
B S W
OptDivBO 11 2 2
CASH approaches, including its predecessor, DivBO, and 2) The
choice of diversity metric exerts a statistically significant influence
on the overall performance of the ensemble.
5.1 Experiment Setup
5.1.1 Baselines. We compare the proposed OptDivBO with the
following nine baselines. - Three CASH methods : 1) Random search
(RS); 2) Bayesian optimization; 3) Rising Bandit (RB) [ 14];Two
AutoML methods proposed for ensemble learning : 4) Ensemble opti-
mization (EO) [ 13]; 5) Neural ensemble search (NES) [ 29]; — Four
post-hoc designs : 6) Random search with post-hoc ensemble (RS-ES);
7) Bayesian optimization with post-hoc ensemble (BO-ES), which is
the default strategy in Auto-sklearn; 8) Rising bandit with post-hocensemble (RB-ES), which is the default strategy in VolcanoML [ 16];
9) Diversity Aware BO DivBO [22].
5.1.2 Datasets and search space. The search space plays a pivotal
role in the optimization of CASH problems. To ensure consistency
and facilitate a fair comparison of algorithms, we have conducted
our experiments within a unified search space. Specifically, we
adopt the same search space utilized by DivBO, which encompasses
approximately 100 configurations. Detailed descriptions of the al-
gorithms and hyperparameters are available in [ 22]. Our study
includes 15 public classification datasets and 5 regression datasets,
all sourced from OpenML [ 27]. These datasets vary in size, with
the number of samples ranging from 2k to 20k.
 
2416CASH via Optimal Diversity for Ensemble Learning KDD ’24, August 24–29, 2024, Barcelona, Spain
5.1.3 Implementation Details. The Bayesian optimization surro-
gate is implemented using OpenBox [ 15],an open-source toolkit
designed for black-box optimization tasks. We adhered to the open-
source versions or the methodologies outlined in the original papers
for all other baseline implementations. For NES, the population size
is set to 30; for EO, the ensemble size is set to 12; for RB, 𝛼and trial
per action are set to 3 and 5, respectively; In the case of DivBO,
the parameters 𝛽and𝜏are set to 0.05 and 0.2; OptDivBO 𝜏too is
fixed at 0.2 ; for all post-hoc ensemble designs, the ensemble size
for ensemble selection is fixed at 25.
Each dataset undergoes a split into three distinct sets: train-
ing (60%), validation (20%), and test (20%). This division ensures a
comprehensive evaluation framework. For comparisons with other
baseline approaches on CASH problems, we report both the best-
observed validation error during the optimization process and the
final test error, providing a holistic view of each method’s perfor-
mance. Each baseline evaluates approximately 250 configurations.
The evaluation of each method on each dataset is repeated 10 times,
and we report the mean±std result.
5.2 Classification Evaluation
The DivBO evaluation metric is inversely proportional to accu-
racy, defined as, i.e. L=100−Acc[(]𝑌,𝑓ℎ(𝑋)]. The accuracy,
or equivalently the 0/1 loss function, is denoted by L0/1(𝑋,𝑌)=
E(𝑥,𝑦)∼D eval1(𝑓ℎ(𝑥)≠𝑦). While prior works have explored upper-
bounding the ensemble generalization error of the 0/1 loss [ 19]
[21], our work does not delve into these analyses due to the imprac-
ticality of deriving the exact form of optimal diversity for every
possible classification metric. Instead, we aim to generalize our
understanding of optimal diversity, creating a broadly applicable,
diversity-aware black-box BO framework. Therefore, for all met-
rics beyond those described in Section 4, we assume the optimal
diversity to be a linear combination of OptDivCE(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)and
OptDivBS(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)i.e.
OptDivblack-box-metric(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)=𝛽𝐶𝐸OptDivCE(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)
+𝛽𝐵𝑆OptDivBS(𝑌,𝑓ℎ𝑖,𝑓ℎ𝑗)(12)
The hyperparameters 𝛽𝐶𝐸and𝛽𝐵𝑆are dependent on the black-
box metric being optimized. For our 0/1 loss L0/1(𝑋,𝑌)setting
𝛽𝐶𝐸=0.2and𝛽𝐵𝑆=0.1yields excellent performance.
The hyperparameters were tuned using a grid search across the
range 0, 0.1, 0.2, 0.3, 0.4, 0.5 for both 𝛽𝐶𝐸and𝛽𝐵𝐸. The objective
of the grid search was to identify 𝛽𝐶𝐸and𝛽𝐵𝐸values that enable
OptDivBO have minimum average error on validation sets of the
amazon _employee, bank32nh, and cpu _act datasets. Once tuned to
these 3 datasets, we keep 𝛽𝐶𝐸and𝛽𝐵𝐸fixed for all other datasets.
Ideally, we could utilize a third surrogate Gaussian Process function
to tune these parameters individually for each dataset, mapping
the𝛽𝐶𝐸and𝛽𝐵𝐸values to OptDivBO’s final validation error. How-
ever, this approach would considerably increase the computational
complexity.
Table 1 is recreated from DivBO’s open source code and Open-
MLs datasets. It demonstrates the test errors and average rank
across 15 datasets. OptDivBO achieves the best test error on 12 out
of 15 datasets, boasting an average rank of 1.53. The second-best
baseline, as anticipated, is DivBO, with an average rank of 2.67.To evaluate the statistical significance of OptDivBO’s improve-
ments, we conducted the Wilcoxon signed-rank test for each dataset,
comparing two methods. A difference was deemed significant at
𝑝≤0.05. The datasets were classified into three categories: 1) in-
stances where OptDivBO’s mean error is lower and the difference
is statistically significant, labeled as (B); 2) instances with no signif-
icant difference, labeled as (S); and 3) instances where OptDivBO’s
mean error is higher and the difference is statistically significant,
labeled as (W). The findings, detailed in Table 2, demonstrate that
although DivBO establishes a solid baseline, OptDivBO surpasses
DivBO on 12 of the 15 datasets and performs at least as well on
all. However, it is noted that OptDivBO underperforms relative
to RB-ES on two datasets. This observation suggests a potential
avenue for future work, such as integrating a rising bandit-like
algorithm selection mechanism into the OptDivBO framework.
The primary distinction between DivBO and OptDivBO lies
in their respective diversity metrics, suggesting that despite the
theoretical guarantees compromised when addressing the 0/1 loss,
the diversity metrics employed by OptDivBO are significantly better
suited for ensemble learning.
5.3 Regression Evaluation
In our regression analysis, we evaluate the mean square error (MSE)
on five open-source OpenML regression datasets (see Table 3).
Similar to the classification experiments, all regression tests were
conducted ten times to obtain reliable estimates of the mean and
standard deviation. For DivBO, we employ the diversity metric
𝐷𝑖𝑣(ℎ𝑖,ℎ𝑗)=E(𝑥,𝑦)∼D𝑣𝑎𝑙[(𝑓ℎ𝑖(𝑥)−𝑓ℎ𝑗(𝑥))2], maintaining consis-
tency with DivBO’s approach in classification tasks. A 𝛽value of
0.01 was found to yield the best performance for DivBO.
Table 3 demonstrates that OptDivBO outperforms both RB-ES
and DivBO, achieving the lowest test error in four out of five
datasets and securing an average rank of 1.2. The performance
gap between DivBO and RB-ES is significantly narrowed, with Di-
vBO achieving an average rank of 2.7 and RB-ES an average rank
of 2.8. This observation highlights the profound impact of diversity
selection on the performance of Bayesian Optimization (BO) frame-
works. Furthermore, Table 4 indicates that RB-ES is the only method
to surpass OptDivBO on one dataset, reinforcing the notion that in-
corporating intelligent algorithm selection through a multi-armed
bandit approach could potentially enhance the OptDivBO/DivBO
frameworks further.
Table 5 conducts an ablation study on the impact of the parameter
𝛽on DivBO’s performance, on DivBO’s performance, using the
mean absolute error (MAE) metric for this analysis. BO-ES can be
considered as DivBO with 𝛽=0. Moderately increasing 𝛽to 0.01
appears to enhance the average rank from 2.63 to 2.38. However,
further increasing 𝛽to 0.05 deteriorates its performance. OptDivBO
achieves the lowest average test MAE, underscoring the criticality of
optimally balancing diversity with individual model performance.
The superiority of OptDivBO over previous methodologies can
be attributed to its inherent capability to identify configurations that
directly optimize the ensemble generalization error. This strategic
focus results in a significant enhancement in performance.
The findings from both our classification and regression exper-
iments establish OptDivBO as a robust black-box BO framework
 
2417KDD ’24, August 24–29, 2024, Barcelona, Spain Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava, & Siddhartha Asthana
Table 3: Regression Test MSE with standard deviations and the average rank across different datasets
Method space_ga delta_elevators wine_quality liver_disorders bodyfat Rank
CASH Methods
RS 0.0150±0.0005 2.2340±0.0221 0.425±0.090 11.56±0.50 0.150±0.050 9.40
BO 0.0130±0.0003 1.9350±0.0115 0.395±0.030 11.57±0.30 0.147±0.030 7.90
RB 0.0140±0.0006 1.9100±0.0342 0.396±0.050 11.50±0.80 0.139±0.030 6.10
Methods for Ensemble Learning
EO 0.0130±0.0006 1.9240±0.0215 0.396±0.050 11.50±0.80 0.139±0.030 6.90
NES 0.0150±0.0006 1.9150±0.0342 0.425±0.090 11.56±0.50 0.150±0.050 8.60
Post-hoc Designs
RS-ES 0.0120±0.0004 1.9180±0.0215 0.386±0.080 11.5±0.3 0.137±0.060 5.60
BO-ES 0.0100±0.0009 1.9110±0.0347 0.373±0.100 11.22±0.80 0.130±0.050 3.60
RB-ES 0.0090±0.0009 1.9120±0.0221 0.368±0.07 11.21±0.50 0.131±0.040 2.80
DivBO (beta = 0.01) 0.0100±0.0005 1.9100±0.0228 0.369±0.050 11.22±0.40 0.129±0.070 2.70
OptDivBO 0.008±0.0004 1.908±0.0228 0.370±0.040 11.18±0.6 0.122±0.06 1.40
Table 4: OptDivBO performs statistically better (B), the same (S), and worse (W).
(a) vs. RS-ES
B S W
OptDivBO 4 1 0(b) vs. RB-ES
B S W
OptDivBO 3 1 1(c) vs. DivBO
B S W
OptDivBO 3 2 0
Table 5: Ablation Study on 𝛽, Test MAE Error
Method space_ga wine_quality liver_disorders bodyfat rank
BO-ES 0.0790±0.0015 0.449±0.015 2.600±0.147 0.2740±0.0243 2.63
DivBO (beta = 0.05) 0.0810±0.0012 0.500±0.025 2.680±0.131 0.2700±0.0427 3.50
DivBO (beta = 0.01) 0.0770±0.0010 0.455±0.012 2.52±0.166 0.2740±0.0243 2.38
OptDivBO 0.070±0.0035 0.449±0.015 2.600±0.147 0.269±0.0349 1.50
Table 6: Classification Test error (%) compared with AutoGluon
Method elevators house_8L pol quake wind
AutoGluon-Tabular 9.08 9.95 1.20 44.70 14.32
OptDivBO (AutoGluon space) 9.00 10.00 1.15 44.72 14 .22
capable of optimizing black box classification/regression metrics.
For standard metrics, it efficiently suggests configurations that
directly minimize the ensemble generalization error.
5.4 Comparison with AutoGluon
AutoGluon [ 6] represents a state-of-the-art AutoML system renowned
for its sophisticated ensembling and multi-layer stacking of mod-
els. Unlike DivBO, which is a Bayesian optimization framework,
OptDivBO extends this framework rather than embodying a com-
prehensive system like AutoGluon. Consequently, AutoGluon was
not initially considered a primary baseline in our study. However,
comparing OptDivBO to AutoGluon is essential to evaluate whether
complex ensembling techniques can substitute search for diverseconfiguration. AutoGluon employs a more compact search space
compared to auto-sklearn, rendering a direct comparison between
OptDivBO (on the auto-sklearn search space) and AutoGluon as
potentially inequitable. To facilitate a more equitable comparison,
we replicated a search space akin to that of AutoGluon.
The outcomes across five datasets are presented in Table 6. It
is evident that the choice of search space significantly influences
the results. For instance, AutoGluon’s performance on the wind
dataset is inferior to that of RS-ES within the auto-sklearn search
space. Nonetheless, on the remaining four datasets, AutoGluon sur-
passes most results obtained using the auto-sklearn space, aligning
 
2418CASH via Optimal Diversity for Ensemble Learning KDD ’24, August 24–29, 2024, Barcelona, Spain
with observations that AutoGluon frequently outperforms auto-
sklearn. This superiority likely stems from AutoGluon’s meticu-
lously curated search space, which effectively excludes less effective
algorithms for contemporary datasets while incorporating more
robust ones. Notably, implementing DivBO within this search space
yielded a decrease in error rates. Specifically, the enhancements
were statistically significant on three datasets, not significant on
one (quake), and marginally inferior on another.
6 CONCLUSION
In this paper, we have introduced OptDivBO, a Bayesian optimization-
based framework designed to address CASH problems in ensemble
learning. This framework adeptly balances the diversity and individ-
ual performance of base learners, thereby optimally minimizing the
ensemble generalization error across a broad spectrum of standard
regression and classification metrics—a capability that eludes prior
CASH methods. Our empirical studies demonstrate OptDivBO’s
effectiveness as a versatile Black Box optimization framework, pro-
ficient in optimizing a wide array of classification and regression
metrics beyond those explicitly addressed herein. Rigorous experi-
mentation across 20 publicly available datasets unequivocally estab-
lishes OptDivBO’s superiority over prior state-of-the-art baselines
for ensemble learning in CASH literature, surpassing even its pre-
decessor, DivBO.
REFERENCES
[1]Taiga Abe, E Kelly Buchanan, Geoff Pleiss, and John Patrick Cunningham. 2022.
The best deep ensembles sacrifice predictive diversity. In I Can’t Believe It’s Not
Better Workshop: Understanding Deep Learning Through Empirical Falsification.
[2]James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms
for hyper-parameter optimization. Advances in neural information processing
systems 24 (2011).
[3]Yijun Bian and Huanhuan Chen. 2021. When does diversity help generalization
in classification ensembles? IEEE Transactions on Cybernetics 52, 9 (2021), 9059–
9075.
[4] Leo Breiman. 1996. Stacked regressions. Machine learning 24 (1996), 49–64.
[5] Thomas G Dietterich. 2000. Ensemble methods in machine learning. In Interna-
tional workshop on multiple classifier systems. Springer, 1–15.
[6]Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu
Li, and Alexander Smola. 2020. Autogluon-tabular: Robust and accurate automl
for structured data. arXiv preprint arXiv:2003.06505 (2020).
[7]Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel
Blum, and Frank Hutter. 2015. Efficient and robust automated machine learning.
Advances in neural information processing systems 28 (2015).
[8]Lars Kai Hansen and Peter Salamon. 1990. Neural network ensembles. IEEE
transactions on pattern analysis and machine intelligence 12, 10 (1990), 993–1001.
[9]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[10] Yu-Chi Ho and David L Pepyne. 2001. Simple explanation of the no free lunch
theorem of optimization. In Proceedings of the 40th IEEE conference on decision
and control (Cat. No. 01CH37228), Vol. 5. IEEE, 4409–4414.
[11] Thomas Hoch. 2015. An Ensemble Learning Approach for the Kaggle Taxi Travel
Time Prediction Challenge.. In DC@ PKDD/ECML.[12] Ludmila I Kuncheva and Christopher J Whitaker. 2003. Measures of diversity in
classifier ensembles and their relationship with the ensemble accuracy. Machine
learning 51 (2003), 181–207.
[13] Julien-Charles Lévesque, Christian Gagné, and Robert Sabourin. 2016.
Bayesian hyperparameter optimization for ensemble learning. arXiv preprint
arXiv:1605.06394 (2016).
[14] Yang Li, Jiawei Jiang, Jinyang Gao, Yingxia Shao, Ce Zhang, and Bin Cui. 2020.
Efficient automatic CASH via rising bandits. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 34. 4763–4771.
[15] Yang Li, Yu Shen, Wentao Zhang, Yuanwei Chen, Huaijun Jiang, Mingchao
Liu, Jiawei Jiang, Jinyang Gao, Wentao Wu, Zhi Yang, et al .2021. Openbox:
A generalized black-box optimization service. In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining. 3209–3219.
[16] Yang Li, Yu Shen, Wentao Zhang, Jiawei Jiang, Bolin Ding, Yaliang Li, Jingren
Zhou, Zhi Yang, Wentao Wu, Ce Zhang, et al .2021. VolcanoML: speeding up
end-to-end AutoML via scalable search space decomposition. arXiv preprint
arXiv:2107.08861 (2021).
[17] Sijia Liu, Parikshit Ram, Deepak Vijaykeerthy, Djallel Bouneffouf, Gregory Bram-
ble, Horst Samulowitz, Dakuo Wang, Andrew Conn, and Alexander Gray. 2020.
An ADMM based framework for automl pipeline configuration. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 34. 4892–4899.
[18] A Malinin, L Prokhorenkova, and A Ustimenko. [n. d.]. Uncertainty in gradient
boosting via ensembles. arXiv 2020. arXiv preprint arXiv:2006.10562 ([n. d.]).
[19] Andrés Masegosa, Stephan Lorenzen, Christian Igel, and Yevgeny Seldin. 2020.
Second order PAC-Bayesian bounds for the weighted majority vote. Advances in
Neural Information Processing Systems 33 (2020), 5263–5273.
[20] Randal S Olson and Jason H Moore. 2016. TPOT: A tree-based pipeline optimiza-
tion tool for automating machine learning. In Workshop on automatic machine
learning. PMLR, 66–74.
[21] Luis A Ortega, Rafael Cabañas, and Andres Masegosa. 2022. Diversity and gener-
alization in neural network ensembles. In International Conference on Artificial
Intelligence and Statistics. PMLR, 11720–11743.
[22] Yu Shen, Yupeng Lu, Yang Li, Yaofeng Tu, Wentao Zhang, and Bin Cui. 2022.
DivBO: Diversity-aware CASH for Ensemble Learning. Advances in Neural
Information Processing Systems 35 (2022), 2958–2971.
[23] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian
optimization of machine learning algorithms. Advances in neural information
processing systems 25 (2012).
[24] Xiaoyuan Su and Taghi M Khoshgoftaar. 2009. A survey of collaborative filtering
techniques. Advances in artificial intelligence 2009 (2009).
[25] Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2013.
Auto-WEKA: Combined selection and hyperparameter optimization of classifica-
tion algorithms. In Proceedings of the 19th ACM SIGKDD international conference
on Knowledge discovery and data mining. 847–855.
[26] Aleksei Ustimenko, Artem Beliakov, and Liudmila Prokhorenkova. 2022. Gradient
Boosting Performs Gaussian Process Inference. In The Eleventh International
Conference on Learning Representations.
[27] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. 2014. OpenML:
networked science in machine learning. ACM SIGKDD Explorations Newsletter
15, 2 (2014), 49–60.
[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[29] Sheheryar Zaidi, Arber Zela, Thomas Elsken, Chris C Holmes, Frank Hutter, and
Yee Teh. 2021. Neural ensemble search for uncertainty estimation and dataset
shift. Advances in Neural Information Processing Systems 34 (2021), 7898–7911.
[30] Wentao Zhang, Jiawei Jiang, Yingxia Shao, and Bin Cui. 2020. Efficient diversity-
driven ensemble for deep neural networks. In 2020 IEEE 36th International Con-
ference on Data Engineering (ICDE). IEEE, 73–84.
[31] Lucas Zimmer, Marius Lindauer, and Frank Hutter. 2021. Auto-pytorch: Multi-
fidelity metalearning for efficient and robust autodl. IEEE Transactions on Pattern
Analysis and Machine Intelligence 43, 9 (2021), 3079–3090.
 
2419