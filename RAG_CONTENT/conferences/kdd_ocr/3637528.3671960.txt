Your Neighbor Matters: Towards Fair Decisions Under Networked
Interference
Wenjing Yang∗
Department of Intelligent Data
Science, College of Computer,
National University of Defense
Technology
Changsha, China
wenjing.yang@nudt.edu.cnHaotian Wang†
Department of Intelligent Data
Science, College of Computer,
National University of Defense
Technology
Changsha, China
accwht@hotmail.comHaoxuan Li
Peking University
Beijing, China
hxli@stu.pku.edu.cn
Hao Zou
ZGC laboratory
Beijing, China
ahio@163.comRuochun Jin
Department of Intelligent Data
Science, College of Computer,
National University of Defense
Technology
Changsha, China
jinrc@nudt.edu.cnKun Kuang‡
Institute of Artificial Intelligence,
Zhejiang University
Hangzhou, China
kunkuang@zju.edu.cn
Peng Cui
Tsinghua University
Beijing, China
cuip@tsinghua.edu.cn
Abstract
In the era of big data, decision-making in social networks may
introduce bias due to interconnected individuals. For instance, in
peer-to-peer loan platforms on the Web, considering an individ-
ual’s attributes along with those of their interconnected neighbors,
including sensitive attributes, is vital for loan approval or rejec-
tion downstream. Unfortunately, conventional fairness approaches
often assume independent individuals, overlooking the impact of
one person’s sensitive attribute on others’ decisions. To fill this
gap, we introduce "Interference-aware Fairness" (IAF) by defining
two forms of discrimination as Self-Fairness (SF) and Peer-Fairness
(PF), leveraging advances in interference analysis within causal
inference. Specifically, SF and PF causally capture and distinguish
discrimination stemming from an individual’s sensitive attributes
(with fixed neighbors’ sensitive attributes) and from neighbors’ sen-
sitive attributes (with fixed self’s sensitive attributes), separately.
Hence, a network-informed decision model is fair only when SF
and PF are satisfied simultaneously, as interventions in individuals’
∗Wenjing Yang, Haotian Wang and Haoxuan Li contributed equally to this research.
†Haotian Wang is the corresponding author.
‡Kun Kuang is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671960sensitive attributes or those of their peers both yield equivalent out-
comes. To achieve IAF, we develop a deep doubly robust framework
to estimate and regularize SF and PF metrics for decision models.
Extensive experiments on synthetic and real-world datasets validate
our proposed concepts and methods.
CCS Concepts
•Applied computing →Law, social and behavioral sciences;
•Computing methodologies →Machine learning.
Keywords
Algorithmic fairness, machine learning, social network
ACM Reference Format:
Wenjing Yang, Haotian Wang, Haoxuan Li, Hao Zou, Ruochun Jin, Kun
Kuang, and Peng Cui. 2024. Your Neighbor Matters: Towards Fair Decisions
Under Networked Interference. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671960
1 Introduction
The widespread use of machine learning (ML) models raises con-
cerns about ethical and legal implications due to potential biases [ 2,
37]. For example, ML-based credit scoring in loan systems may
yield discriminatory results for individuals with similar financial
profiles but different races [ 19]. To ensure fairness, research has
developed various fairness metrics [ 4,9,11,12,14,19,36,38,40,42].
Earlier work focused on statistical independence between ML deci-
sions and sensitive attributes [ 8,11]. There has been much recent
interest in answering the fairness questions from the perspective
3829
KDD ’24, August 25–29, 2024, Barcelona, Spain Wenjing Yang et al.
of causality [ 19,26,30], aiming to achieve equity by examining
interventions on sensitive attributes, e.g., “How would the decision
changes if we intervened with racial attributes?”.
Previous fairness criteria, particularly those rooted in causality-
based fairness metrics, have often assumed individual independence,
i.e., one person’s sensitive attributes do not impact others’ decisions.
This assumption implies that discrimination against individuals,
such as the borrower in our P2P loan example, is solely influenced
by their features, such as credit status and the sensitive attribute of
living location1. However, in the era of big data, our living world
is interconnected and social relationships play a significant role in
decision-making models [ 7,20,31]. For instance, in contemporary
online lending platforms, such as peer-to-peer (P2P) loan systems
on the Web, decision-making increasingly relies on both an indi-
vidual’s attributes and those of their social network neighbors [ 20].
In essence, decisions made by many ML systems now incorporate
sensitive attributes not only from the individual but also from their
neighbors in the social network.
Consequently, when the effects of self-sensitive attributes (self-
effect) and neighbors’ sensitive attributes (peer-effect) intertwine,
a decision model that appears fair under conventional criteria may
exhibit unfairness. Keeping the P2P loan example in mind, a loan
decision model satisfying conventional fairness criteria [ 8,10,11,
14,19,39] entails that the overall correlation/causal effect from
sensitive attribute, i.e., location or race, to the loan decision vanishes.
However, a loan model unfair for minor-group individuals might
seem to be fair with a vanished effect from race to loan decision. The
core reason is that scores assigned by the decision model for minor-
group individuals will be enhanced by considering their social
connections with the major-group neighbors. We term such kinds of
unfairness as interference-specific unfairness throughout our paper.
In general, interference-specific unfairness exists commonly across
many scenarios beyond loan systems. For instance, prestigious
colleges may admit applicants from the minor group (seemingly
fair) with lots of neighbors from the major group, while rejecting
applicants with similar academic qualifications from the minor
group with few neighbors from the major group (indeed unfair) [ 3].
In response to such discrimination specific to interference across
individuals, we advocate for the establishment of more robust
fairness metrics that account for the influence of social relation-
ships on decision-making. Building upon recent advancements in
interference-based causality [ 13,22,27], we introduce the concepts
of "self-fairness (SF) " and "peer-fairness (PF)" to causally evalu-
ate equity among individuals with similar self-sensitive attributes
and distinct neighbor-sensitive attributes, as well as equity among
individuals with dissimilar self-sensitive attributes and analogous
neighbor-sensitive attributes, respectively. Subsequently, the simul-
taneous satisfaction of SF and PF through model regularization can
mitigate interference-specific unfairness.
To the best of our knowledge, we are pioneering the formal
differentiation and mitigation of discrimination stemming from an
individual’s sensitive attributes versus those arising from peers’
sensitive attributes. We summarize our contributions in below:
1.We contribute the Interference-aware Fairness (IAF) metric to
capture such unfair decisions.
1As discussed in the real-world case study by [20].2.To characterize IAF, we introduce causal definitions for Self Fair-
ness (SF) and Peer Fairness (PF) aimed at capturing unfair deci-
sions induced by peer effects.
3.Inspired by networked causal inference [ 27], we devise a deep
doubly robust (DR) framework to regularize unfair decision mod-
els in the presence of interference.
4.Our experiments, conducted on one synthetic data and two real-
world datasets, yield the following key results (a) SF and PF
effectively capture unfair decisions stemming from peer effects,
and (b) our designed DR framework successfully eliminates this
interference-specific unfairness.
2 Preliminaries
2.1 Notations
We formalize definitions of fairness in the essence of interference
across decision subjects. Throughout this paper, uppercase let-
ters denote the Random variables, e.g., 𝑋, and lowercase letters
denote their realizations, e.g., 𝑥. Let{𝐴𝑖,𝑋𝑖,𝑌𝑖}𝑛
𝑖=1∼𝑃be the
logged dataset with 𝑛individuals sampled from the joint distri-
bution𝑃(𝐴,𝑋,𝑌), where𝐴𝑖,𝑋𝑖and𝑌𝑖are sensitive attributes, con-
textual features and outcome to be predicted for individual 𝑖, respec-
tively. A decision model 𝑀𝜃(𝜃is the parameter of 𝑀𝜃) is learned on
{𝐴𝑖,𝑋𝑖,𝑌𝑖}𝑛
𝑖=1with predictions ˆ𝑌𝑖for individual 𝑖. We assume 𝐴to
be binary throughout this paper, while our discussion framework
can be easily generalized to categorical sensitive attributes. We
present all causal notions using the language of potential outcome
framework [ 30], i.e., ˆ𝑌(𝑎)represents the potential decision if the
sensitive attribute 𝐴were set to value 𝑎. Notably,(𝐴,𝑋, ˆ𝑌)is the
observational data and cannot be intervented arbitrarily, as 𝑀𝜃at-
tempts to fit 𝑃(𝐴,𝑋,𝑌). Besides,[𝑛]represents the set{1,2,···,𝑛}
and−𝑖represents all elements in [𝑛]except for𝑖.
2.2 Correlation-based Fairness
By accounting for the correlations between sensitive attribute 𝐴and
outcome𝑌, several popular metrics have been proposed to achieve
fairness in the correlation sense [ 8,10,11]. For instance, the Fairness
Through Unawareness (FTU) [ 10] principle proposes to overlook
the sensitive attribute 𝐴, while the Demographic Parity (DP) [ 8]
criteria enforces 𝑀𝜃to decisions ˆ𝑌independent from 𝐴:𝐴⊥⊥ˆ𝑌. Be-
sides, two important notions, i.e., Equality of Opportunity (EO) [ 11]
and individualized fairness (IF) [ 9], have generalized the DP metric
on some sub-populations/individuals. To be specific, EO requires
DP on individuals receiving positive decisions: 𝐴⊥⊥ˆ𝑌|ˆ𝑌=1,
while IF enforces individuals with similar contextual features to re-
ceive similar decisions: 𝐷(ˆ𝑌𝑖,ˆ𝑌𝑗)≤𝜖𝐷𝑋(𝑋𝑖,𝑋𝑗)(𝐷is some metric
and𝜖is some pre-defined threshold).
2.3 Causality-based fairness
While correlation-based fairness notions promote numerous ap-
proaches with compelling simplicity [ 11,23], they may suffer from
bias caused by confounders (explicit ( 𝑋) or latent factors). We re-
fer to extensive analysis on such phenomena to previous stud-
ies [19,23]. With the aim of quantifying and migrating the causal
effect of𝐴onˆ𝑌via controlling third factors, the causality-based fair-
ness approaches have emerged in recent years [ 14,19,25,26,39,43].
3830Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD ’24, August 25–29, 2024, Barcelona, Spain
X𝟏𝟏
A𝟏𝟏�𝒀𝒀𝟏𝟏
X𝟐𝟐A𝟐𝟐�𝒀𝒀𝟐𝟐A𝟏𝟏�𝒀𝒀𝟏𝟏
A𝟐𝟐�𝒀𝒀𝟐𝟐X𝟏𝟏
X𝟐𝟐A𝟏𝟏�𝒀𝒀𝟏𝟏
A𝟐𝟐�𝒀𝒀𝟐𝟐Direct Self
(a) Example: DP (c) Causal Graph of PSF (d) Causal Graph of IAF𝒂𝒂′
𝒂𝒂𝒂𝒂′
�𝒀𝒀=𝟏𝟏�𝒀𝒀=𝟏𝟏�𝒀𝒀=𝟏𝟏
𝒂𝒂′
𝒂𝒂𝒂𝒂′
�𝒀𝒀=𝟏𝟏�𝒀𝒀=𝟏𝟏 �𝒀𝒀=𝟏𝟏
(b) Example: IF
Figure 1: Counterexamples and causal graphs we illustrated in Section 4.2. In (b), we merge the node A and Y for the clarity.
In (c), direct (unfair) and indirect (fair) causal paths from 𝐴𝑖toˆ𝑌𝑖are colored in green, while the red dashed lines represent that
PSF do not consider causal effects across individuals. In (d), self and peer causal effects from 𝐴𝑖toˆ𝑌𝑖,ˆ𝑌𝑗are colored in blue.
The counterfactual parity and conditional counterfactual parity [ 25]
has described population-level causality-based fairness by elimi-
nating average and conditional treatment effect: 𝐸[ˆ𝑌(𝐴=1)]=
𝐸[ˆ𝑌(𝐴=0)]and𝐸[ˆ𝑌(𝐴=1) |𝑋]=𝐸[ˆ𝑌(𝐴=0) |𝑋]. The
principle fairness [ 14] and counterfactual equalized odds [ 24] have
developed causality-based fairness specific to some important sub-
populations. The unit-level causality-based fairness, i.e., the coun-
terfactual fairness [ 19], has been proposed based on the Structural
Causal Model (SCM) [ 29]:ˆ𝑌𝑖(𝐴𝑖=1)=ˆ𝑌𝑖(𝐴𝑖=0). The path-
specific causality-based fairness [ 4,26,39] has been developed to
distinguish fair and unfair causal paths from 𝐴to the ˆ𝑌with dif-
ferent mediators. For instance, PSF requires that 𝐸[ˆ𝑌(𝐴=1,𝐾(𝐴=
0))]=𝐸[ˆ𝑌(𝐴=0,𝐾(𝐴=0))], where𝐾is the mediator lying in
the causal path from 𝐴toˆ𝑌.
2.4 Fairness on Graphs
Recent research has extensively examined fairness in graph data
across various tasks, such as node classification, link prediction,
and community detection, through graph representation learning
with Graph Neural Networks (GNNs) [ 1,6,7,17,28,31]. Fairness
concepts originally designed for tabular data, including DP, EO, IF,
and sample perturbations, have been integrated into the context of
fair node embedding and classification [ 7]. Notably, prior work in
graph fairness has made significant progress, but none has explicitly
addressed or formalized discrimination arising from peer effects.
3A Real-world Case Study: Unfair Judgment in
Peer-to-Peer Loan
Prior research has highlighted the potential for discrimination stem-
ming from peer effects when incorporating social relationships into
decision-making processes. As depicted in Fig.2, [ 20] constructed a
real-world social network using the Prosper Loans Network Dataset,
encompassing over 1,048,575 Peer-to-peer (P2P) loan data records.
Leveraging this data, our analysis addresses two key objectives (a)
the necessity of considering social relationships when designing
decision-making models, and (b) the identification of fairness con-
cerns arising from the specific modeling of social relationships. Inthis case study, we designate the Location variable2as the sensi-
tive attribute. Therefore, the fairness inquiry centers on whether
the loan decision model exhibits bias against borrowers from less
favorable areas (see Appendix A for details on implementation.)
The first task is to design decision models to judge the individ-
ual’s social score based on their social connections, rating, and
status of corresponding loan records, etc. Meanwhile, the second
task is to design decision models to judge the risk of each loan
record based on features including the credit status, and loan rating
records of the lender and borrower. For the first task, we design and
compare two decision models, i.e., a Graph Neural Network (GNN)
model and a multi-layer MLP model. For the second task, we fol-
low [ 20] and compare the XGBoost classifiers trained with and
without social features [ 20]. This metric quantifies the proportion of
negative judgments on creditworthy individuals with connections
in unfavorable locations and positive judgments on individuals who
lack creditworthiness but have connections in favorable locations3.
In Fig. 2, we observe a significant degradation in model prediction
performance for both tasks in the absence of interference modeling.
However, this improved performance comes at the cost of a con-
siderable increase in Unfair Proportion. This suggests that the bias
introduced by neighboring relationships is concurrently incorpo-
rated into the learning models. Hence, our case study demonstrates
that interference across individuals is a double-edged sword, en-
hancing prediction performance while introducing discrimination
stemming from peer effects.
4 Interference-aware Fairness
4.1 Problem Setting
In the context of interference-aware fairness, we introduce a for-
mal problem. Consider a social network comprising 𝑛individuals,
denoted as decision subjects, with an adjacency matrix 𝐺∈R𝑛∗𝑛.
2A binary variable representing the location of each borrower.
3We explicitly encode the feature "creditworthy" for each individual by its social score
𝑠𝑖, i.e., the true label for the first task. 𝑠𝑖=2refers to creditworthy individuals and
𝑠𝑖=0refers to individuals who are not creditworthy. In similar, we explicitly encode
the feature "favorable" by the value of the variable "Location". When Location= 1, i.e.,
more low-risk neighbors than high-risk borrowers, the location is "favorable".
3831KDD ’24, August 25–29, 2024, Barcelona, Spain Wenjing Yang et al.
Location
Loan Status
Credit Status
…Location
Loan Status
Credit Status
…
Location
Loan Status
Credit Status
…B
L
BL
L
B
Figure 2: In the case study of P2P loans on the Web, we present our findings and outcomes. Specifically, on the left-hand side,
’L’ and ’B’ denote the lender and borrower in a loan record, while on the right-hand side, we refer to ’GNN’ as a three-layer
Graph Neural Network, ’MLP’ as a three-layer fully connected network, ’Xgb:S’ as the XGB boost classifier incorporating social
features, and ’Xgb:nonS’ as the XGB boost classifier excluding social features.
Here,𝐺𝑖𝑗=1signifies a causal relationship between individual
𝐴𝑖and ˆ𝑌𝑗, as well as between 𝐴𝑗and ˆ𝑌𝑖. Additionally, connections
between individuals 𝑖and𝑗imply causal effects from 𝑋𝑖to𝐴𝑗and
ˆ𝑌𝑗, as well as from 𝑋𝑗to𝐴𝑖and ˆ𝑌𝑖. In summary, when individuals 𝑖
and𝑗are neighbors, their own covariates influence the sensitive
attribute and decision of the other party, and their sensitive at-
tribute also impacts the decision of the other party. See Fig. 1 (c) for
an illustrative example of this setting. Consequently, the potential
decision of individual 𝑖can be expressed as ˆ𝑌(𝐴𝑖,𝑋𝑖,𝐴−𝑖,𝑋−𝑖)with
𝐴𝑖(𝑋𝑖,𝑋−𝑖). To address the data inefficiency challenge arising from
the exponential realizations of the high-dimensional treatment vec-
tor (𝐴𝑖,𝐴−𝑖), we commonly adopt the following assumptions in the
field of networked interference in prior work [22, 27].
Assumption 1 (Neighborhood interference). The potential
decision for 𝑖is only decided by its neighbor’s sensitive attributes
with covariates: ˆ𝑌(𝐴𝑖,𝑋𝑖,𝐴−𝑖,𝑋−𝑖)=ˆ𝑌(𝐴𝑖,𝑋𝑖,𝐴N𝑖,𝑋N𝑖), where
N𝑖={𝑗|𝐺𝑖𝑗=1,𝑗≠𝑖,𝑗∈ [𝑛]}represents the neighbors of
individual𝑖on the network. In similar, 𝐴𝑖(𝑋𝑖,𝑋−𝑖)=𝐴𝑖(𝑋𝑖,𝑋N𝑖).
The exposure mapping assumption is further applied to mitigate
the exponential combination of N𝑖[22]:
Assumption 2 (Exposure Mapping). There exists Φwhich maps
AN𝑖to a dense vector Φ(AN𝑖)for any𝑖∈ [𝑛]such that for any
neighboring treatment vectors 𝐴1
N𝑖and𝐴2
N𝑖, we have ˆ𝑌(𝑎,𝐴1
N𝑖)=
ˆ𝑌(𝑎,𝐴2
N𝑖)ifΦ(𝐴1
N𝑖)=Φ(𝐴2
N𝑖), where the notations 𝐴1
N𝑖and𝐴2
N𝑖
are realization values of 𝐴N𝑖.
Notably, the above assumption states that we can use a function Φ
for summarizing the neighbor treatments from a high dimensional
vector𝐴N𝑖to a dense vector Φ(𝐴N𝑖). We denote ˜0in the exposure
set˜𝐴as the no-treatment regime of N𝑖, serving as the control group
for treatment effect without interference. Following prior work [ 21,
22,27], we define 𝑔𝑋as the summary function capturing the in-
fluence of neighboring covariates, i.e., 𝐴𝑖=𝑓𝐴(𝑋𝑖,𝑔𝑋(𝑋N𝑖),𝑈𝐴
𝑖),where𝑓𝐴is an unspecified function, and 𝑈𝐴
𝑖represents individ-
ual characteristics. Similarly, 𝑔𝐴denotes the summary functions
for sensitive attributes, i.e., ˆ𝑌𝑖=𝑓𝑌(𝑋𝑖,𝑔𝑋(𝑋N𝑖),𝐴𝑖,𝑔𝐴(𝐴N𝑖),𝑈𝑌
𝑖),
where𝑓𝑌and𝑈𝑌
𝑖are defined analogously. To ensure fairness in the
presence of interference, we introduce the concept of Interference-
Aware Fairness (IAF) to differentiate and mitigate discrimination
arising from sensitive attributes and network neighbors:
Definition 1. A decision model 𝑀𝜃satisfies Self-Fairness (SF),
i.e., eliminates the discrimination from self’s sensitive attribute, if
the population-level direct (self) effect of𝐴𝑖onˆ𝑌𝑖vanishes:
1
𝑛∑︁
𝑖∈[𝑛]𝐸[ˆ𝑌𝑖(𝐴𝑖=1,˜𝑎)]−𝐸[ˆ𝑌𝑖(𝐴𝑖=0,˜𝑎)]=0,∀˜𝑎∈˜𝐴. (1)
Meanwhile, 𝑀𝜃satisfies Peer-Fairness (PF), i.e., eliminates the
discrimination from neighbor’s sensitive attribute, if the population-
level peer effect of𝐴N𝑖onˆ𝑌𝑖vanishes:
1
𝑛∑︁
𝑖∈[𝑛]𝐸[ˆ𝑌𝑖(𝐴𝑖=𝑎,˜𝑎)]−𝐸[ˆ𝑌𝑖(𝐴𝑖=𝑎,˜0)]=0,∀𝑎∈{0,1}.(2)
Remark. ˜𝑎comes from Φ. By simultaneously satisfying SF and
PF through the enforcement of decision model 𝑀𝜃, we say𝑀𝜃
satisfies IAF. It is worth noting that conceptualizing SF and PF at the
individual lev (counterfactual) will produce much sharper fairness.
However, the computational challenges of deriving individual-level
IAF from observational data, stemming from the absence of an SCM
model, render this approach practically infeasible. Meanwhile, our
experiments empirically show that regularizing SF and PF already
leads to near-optimal performance. We defer the exploration of
individual-level IAF to future research (refer to the Conclusion for
details).
3832Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD ’24, August 25–29, 2024, Barcelona, Spain
Acronym Full Name Definition
DP Demographic Parity 𝐴⊥ˆ𝑌
FTU Fairness Through Unawareness Do not include 𝐴to the prediction model
IF Individualized Fairness 𝐷
ˆ𝑌𝑖,ˆ𝑌𝑗
≤𝜖𝐷𝑋 𝑋𝑖,𝑋𝑗
EO Equality of Opportunity 𝐴⊥ˆ𝑌|𝑌=1
CP Counterfactual Parity 𝐸[ˆ𝑌(𝐴=1)]=𝐸[ˆ𝑌(𝐴=0)]
CCP Conditional Counterfactual Parity 𝐸[ˆ𝑌(𝐴=1)]=𝐸[ˆ𝑌(𝐴=0)|𝑋=𝑥],∀𝑥∈X
CF Counterfactual Fairness ˆ𝑌𝑖(𝐴𝑖=1)=ˆ𝑌𝑖(𝐴𝑖=0),∀𝑖∈[𝑛]
PSF Path-specific Fairness 𝐸[ˆ𝑌(𝐴,𝐾(𝐴))]=𝐸ˆ𝑌(𝐴,𝐾′(𝐴))
,𝐸[ˆ𝑌(𝐴,𝐾(𝐴))]=𝐸ˆ𝑌(𝐴′,𝐾(𝐴))
IACP Interference-aware Counterfactual Parity1
𝑛Í
𝑖∈[𝑛]𝐸ˆ𝑌𝑖(𝐴𝑖=1,˜𝑎)−ˆ𝑌(𝐴𝑖=0,0)
=0,∀˜𝑎∈˜𝐴
IACCP Interference-aware Conditional Counterfactual Parity1
𝑛Í
𝑖∈[𝑛]𝐸ˆ𝑌𝑖(𝐴𝑖=1,˜𝑎)−ˆ𝑌(𝐴𝑖=0,0)|𝑋=𝑥
=0,∀𝑥∈X,˜𝑎∈˜𝐴
IACF Interference-aware Counterefactual Fairness ˆ𝑌𝑖(𝐴𝑖=1,˜𝑎)=ˆ𝑌(𝐴𝑖=0,0),∀𝑖∈[𝑛],˜𝑎∈˜𝐴
SF, PF Self-fairness, Peer-fairness Def. 1
Table 1: Summary of various fairness notions with their capabilities, including whether well-defined, whether can identify
discrimination stemmed from self-effect and peer effects, in our IAF setting.
4.2 Comparison to Other Fairness Notions
In our IAF framework, we assert that prior fairness concepts, such
as correlation-based, causality-based, and graph fairness, fall short
in attaining SF and PF.
Correlation-based Fairness We investigate the limitations of demo-
graphic parity (DP) [ 8], equal opportunity (EO) [ 11], and individual
fairness (IF) [ 9] in achieving statistical fairness (SF) and personal-
ized fairness (PF) through counterexamples. In the context of the
P2P loan case, we modify it by introducing a sensitive attribute,
the borrower’s race ( 𝐴𝑖), while keeping other factors constant. We
consider empty contextual features, i.e., 𝑋=∅, with the ability to
generalize to non-empty 𝑋. In our scenario, two borrowers apply
for a loan, and the decision ˆ𝑌1,ˆ𝑌2depends on both 𝐴1,𝐴2. We dis-
tinguish between minor and major race groups denoted as 𝑎and𝑎′.
We set𝑃(𝐴2=𝑎′)=𝑃(𝐴2=𝑎)=0.5,𝑃(𝐴1=𝑎|𝐴2=𝑎′)=0.99,
and𝑃(𝐴1=𝑎|𝐴2=𝑎)=0.01. We construct an unfair binary
decision model 𝑀𝜃with𝑃(ˆ𝑌=1|𝐴1=𝑎,𝐴 2=𝑎)=0.1,
𝑃(ˆ𝑌=1|𝐴1=𝑎′,𝐴2=𝑎′)=0.9,𝑃(ˆ𝑌=1|𝐴1=𝑎′,𝐴2=𝑎)=
𝑃(ˆ𝑌=1|𝐴1=𝑎,𝐴 2=𝑎′)=0.5. Consequently, we calculate that
𝑃(ˆ𝑌1=1|𝐴1=𝑎′)=0.5≈0.496=𝑃(ˆ𝑌1=1|𝐴1=𝑎)and𝑃(ˆ𝑌2=
1|𝐴2=𝑎′)=0.504≈0.505=𝑃(ˆ𝑌2=1|𝐴2=𝑎)(see Appendix B
for details). Therefore, both population-level and individual-level
DP fail to identify unfairness in this setting. Similarly, EO, which is
a finer-grained version of DP, also cannot detect unfairness in the
presence of interference. In the context of the IF metric, consider
the loan system example in Fig.1(b) with 6individuals: 4in the
major group ( 𝑎′) and 2in the minor group ( 𝑎). The loan decision
model initially exhibits unfairness, rejecting the minor group and
approving the major group with one neighbor. However, when an
applicant from the minor group has two neighbors from the major
group, decisions become positive. Upon utilizing IF for decision
analysis, all subjects (regardless of the same or different 𝐴) receive
identical decisions. This remains true even when incorporating
network structure as features through encoding techniques such
as social score in [20] or motifs in [41].
Causality-based Fairness Previous causality-based fairness
approaches, including CP, CCP, CF, and PSF, rely on the Stable
Unit Treatment Value Assumption (SUTVA) [ 15], which assumes
no interference between an individual’s sensitive attribute andneighbors’ decisions. To facilitate a fair comparison, we extend CP,
CCP, and CF to address self-discrimination and peer discrimination
within our proposed IAF problem, demonstrating their limitations:
Definition 2. A decision model 𝑀𝜃satisfies Interference-aware CP
(IACP), Interference-aware CCP (IACCP) and Interference-aware
CF (IACF), if the following criteria are satisfied:
 
IACP :1
𝑛Í
𝑖∈[𝑛]𝐸[ˆ𝑌𝑖(𝐴𝑖=1,˜𝑎)−ˆ𝑌(𝐴𝑖=0,˜0)]=0∀˜𝑎∈˜𝐴.
IACCP :1
𝑛Í
𝑖∈[𝑛]𝐸[ˆ𝑌𝑖(𝐴𝑖=1,˜𝑎)−ˆ𝑌(𝐴𝑖=0,˜0)|𝑋]=0∀˜𝑎∈˜𝐴.
IACF :ˆ𝑌𝑖(𝐴𝑖=1,˜𝑎)=ˆ𝑌(𝐴𝑖=0,˜0) ∀𝑖∈[𝑛],˜𝑎∈˜𝐴.
In light of the extended concepts mentioned earlier, it becomes
evident that IACP, IACCP, and IACF fail to distinguish between SF
and PF as defined in Def.1. When ˜𝑎is held fixed in SF and 𝑎=0
in PF, we observe that 𝐼𝐴𝐶𝑃 =𝑃𝐹+𝑆𝐹, implying that a vanished
IACP, representing the complete absence of the combined effects of
𝐴𝑖and𝐴N𝑖, can be decomposed into a negative PF (direct effect of
𝐴𝑖) and a positive SF (peer effect of 𝐴N𝑖). Consequently, a decision
model deemed fair under IACP may exhibit unfairness when viewed
through the lenses of SF and PF. Similarly, decisions considered
fair under IACCP and IACF may appear unfair from the SF and PF
perspectives. In contrast, a fair decision model adhering to SF and
PF criteria is guaranteed to satisfy IACP, as indicated by the decom-
position mentioned above. Finally, we compare PSF and our IAF in
Fig.1 (c) and (d). Specifically, PSF quantifies both direct and indirect
causal effects for the same individual via different causal pathways
from𝐴𝑖toˆ𝑌𝑖, such as𝐴𝑖→ˆ𝑌𝑖and𝐴𝑖→𝑀𝑖→ˆ𝑌𝑖, where some
causal pathways are deemed fair while others are deemed unfair.
For example, a loan rejection directly caused by an individual’s
race is considered unfair, while a rejection indirectly caused by
inadequate education is considered fair. In contrast, our IAF focuses
on distinguishing and identifying unfair decisions resulting from
self-effect and peer-effects of sensitive attributes.
Graph Fairness. In our analysis, we find that graph fairness
methods, including DP, EO, IF, and graph representation pertur-
bations [ 7,17,28,31], lack the capability to achieve IAF. These
methods (a) typically apply fairness regularization inherited from
tabular data directly to graph models and (b) fail to distinguish
between discrimination originating from an individual and their
3833KDD ’24, August 25–29, 2024, Barcelona, Spain Wenjing Yang et al.
neighbors, leading to the same limitations we have discussed in the
context of correlation-based fairness comparisons.
Remark. We leave a summary of the boundary capabilities
of various fairness metric in Tab. 1. We observe the potential to
extend PSF’s scope to encompass a broader definition. This involves
analyzing the impact of variable 𝐴through ˆ𝑌across various paths,
both at the feature and individual levels.
Remark. We also provide a summary of different fairness no-
tions and their boundary capabilities in our IAF problem in Tab. 1.
5 A Doubly Robust Debiasing Framework
To achieve IAF, we begin by estimating direct and peer effects
from historical data, following Def.1. We then regularize the ML
model𝑀𝜃to mitigate bias, drawing inspiration from the DragonNet
paradigm [ 32]. We formulate a doubly robust (DR) framework,
named “IAF+DR”, aligning with the optimization objective and
influence curve of the average causal effect under interference [ 27].
Formally, we aim to constrain the causal effect of the joint variables
𝐴𝑖and𝐴N𝑖onˆ𝑌across the population:
𝜙(𝑎∗,˜𝑎∗)=1
𝑛∑︁
𝑖∈[𝑛]𝐸[ˆ𝑌𝑖(𝐴𝑖=𝑎∗,𝐴N𝑖=˜𝑎∗)], (3)
where we use 𝑎∗,˜𝑎∗to highlight the interventional values of 𝐴𝑖
and𝐴N𝑖and distinguish from observational values. We adapt the
well-known overlap assumption and unconfounded assumption
into our problem:
Assumption 3 (Overlap). For all𝑥in the support of 𝑋, for all
𝑖∈[𝑛]and for all ˜𝑎∈˜𝐴,𝑎in the support of 𝐴, we have:
𝑃 𝐴𝑖=𝑎,𝐴N𝑖=˜𝑎|𝑔𝑋(𝑥)>0
Assumption 4 (Unconfoundness). All the variables affecting
both the treatments and the potential outcome are observed:
ˆ𝑌𝑖(𝐴𝑖,𝐴N𝑖)⊥⊥𝐴𝑖,𝐴N𝑖|𝑔𝑋(𝑋)
We then have the following identification result:
Theorem 1. The target estimand, i.e., 𝜙(𝑎∗,˜𝑎∗)for intervened
values𝑎∗,˜𝑎∗, can be identified as follows:
𝜙(𝑎∗,˜𝑎∗)=1
𝑛𝑛∑︁
𝑖=1∫
x𝐸[ˆ𝑌𝑖|𝑎∗,˜𝑎∗,𝑔𝑋(𝑥)]𝑝𝑥(x)𝑑𝑥.
Proof. We have the following derivation:
𝐸[𝜙(𝑎,˜𝑎)]=1
𝑛𝑛∑︁
𝑖=1𝐸ˆ𝑌𝑖(𝑎,˜𝑎)
=1
𝑛𝑛∑︁
𝑖=1∫
x𝐸ˆ𝑌𝑖(𝑎,˜𝑎)|𝑋=𝑥
𝑝𝑋(𝑥)
=1
𝑛𝑛∑︁
𝑖=1∫
x𝐸ˆ𝑌𝑖(𝑎,˜𝑎)|𝑎,˜𝑎,𝑋=𝑥
𝑝𝑋(𝑥)
=1
𝑛𝑛∑︁
𝑖=1∫
x𝐸ˆ𝑌𝑖|𝑎,˜𝑎,𝑋=𝑥
𝑝𝑋(𝑥)
=1
𝑛𝑛∑︁
𝑖=1∫
x𝐸ˆ𝑌𝑖|𝑎,˜𝑎,𝑔𝑋(𝑋)
𝑝𝑋(x).
□We begin by parameterizing the model for regression estimation,
denoted as𝐸[ˆ𝑌𝑖|𝑎,˜𝑎,𝑔𝑋(𝑥)], using a deep model, ˆ𝑌𝑛𝑛, which also
serves as the decision model. To model the aggregation of 𝑔𝑋(𝑥)
from neighboring elements, we opt for a GNN model within ˆ𝑌𝑛𝑛.
Prior research [ 32] has demonstrated the benefit of targeted reg-
ularization for causal inference without interference [ 27,34]. To
improve the robustness and finite-sample efficiency of estimating
𝜙(𝑎,˜𝑎), we incorporate targeted regularization with interference.
Specifically, we introduce a scoring head ˆ𝐴𝑛𝑛to the GNN repre-
sentation output for capturing the propensity score, denoted as
𝑃 𝐴𝑖=𝑎,𝐴N𝑖=˜𝑎|𝑔𝑋(𝑥). We then apply targeted regularization
to ensure that the estimated 𝜙and(ˆ𝐴𝑛𝑛,ˆ𝑌𝑛𝑛)satisfy the estimation
equation, i.e., 𝜑(𝑌,𝐴, ˜𝐴,𝑋;ˆ𝑌𝑛𝑛,ˆ𝐴𝑛𝑛,𝜙), defined as follows:
ˆ𝑌𝑛𝑛(𝑎∗,˜𝑎∗,𝑔𝑋(𝑥))−𝜙+ˆ𝐴𝑛𝑛(𝑎∗,˜𝑎∗|𝑔𝑋(𝑥))
ˆ𝐴𝑛𝑛(𝑎,˜𝑎|𝑔𝑋(𝑥))[]𝑦𝑖−ˆ𝑌𝑛𝑛(𝑎,˜𝑎,𝑔𝑋(𝑥))],
(4)
By regulating that1
𝑛Í𝑛
𝑖=1𝜑(𝑌𝑖,𝐴𝑖,˜𝐴𝑖,𝑋𝑖;ˆ𝑌𝑛𝑛
𝑖,ˆ𝐴𝑛𝑛
𝑖,𝜙𝑖)=0, we de-
signed targeted-regularized outcome estimation as follows:
ˆ𝑌𝑟𝑒𝑔(𝑎∗,˜𝑎∗,𝑔𝑋(𝑥))=ˆ𝑌𝑛𝑛(𝑎∗,˜𝑎∗,𝑔𝑋(𝑥))+𝜖∗ˆ𝐴𝑛𝑛(𝑎∗,˜𝑎∗|𝑔𝑋(𝑥))
ˆ𝐴𝑛𝑛(𝑎,˜𝑎|𝑔𝑋(𝑥))
(5)
Hence, the regularization objective is formularized as
L𝑟𝑒𝑔=1
𝑛𝑛∑︁
𝑖=1∑︁
𝑎∗∈𝐴,˜𝑎∗∈˜𝐴(𝑌𝑖−ˆ𝑌𝑟𝑒𝑔
𝑖(𝑎∗
𝑖,˜𝑎∗
𝑖,𝑔𝑋(𝑥𝑖)))2
and the estimated 𝜙as𝜙𝑟𝑒𝑔(𝑎∗,˜𝑎∗)=1
𝑛Í𝑛
𝑖=1ˆ𝑌𝑟𝑒𝑔
𝑖(𝑎∗,˜𝑎∗,𝑔𝑋(𝑥)).
Based on the above identification of 𝜙(𝑎∗,˜𝑎∗), we are now capable
of designing fairness penalty loss, i.e., L𝑓, to remove bias conveyed
in the decision model ˆ𝑌𝑛𝑛:
L𝑓=∑︁
𝑎∗∈𝐴Var˜𝑎∗∈˜𝐴(𝜙𝑟𝑒𝑔(𝑎∗,˜𝑎∗))+∑︁
˜𝑎∗∈˜𝐴Var𝑎∗∈𝐴(𝜙𝑟𝑒𝑔(𝑎∗,˜𝑎∗)),
(6)
where Var˜𝑎∗∈˜𝐴refers to the variance of 𝜙𝑟𝑒𝑔(𝑎∗,˜𝑎∗)with differ-
ent˜𝑎. This objective can lead to the disappearance of SF and PF,
and attainment of IAF under optimal optimization conditions. Ad-
ditionally, two standard objectives are employed for optimizing
ˆ𝑌𝑛𝑛and ˆ𝐴𝑛𝑛:L𝑌=1
𝑛Í𝑛
𝑖=1L(𝑌𝑖,ˆ𝑌𝑛𝑛
𝑖(𝑎∗
𝑖,˜𝑎∗
𝑖,𝑔𝑋(𝑥𝑖)))2andL𝐴=
1
𝑛Í𝑛
𝑖=1L(𝐴𝑖,ˆ𝐴𝑛𝑛(𝑎,˜𝑎|𝑔𝑋(𝑥𝑖)))(Lrefers to the classification loss,
i.e., cross-entropy loss). To summarize, the overall objective of our
debiasing framework can be formulated as follows:
L𝑠𝑢𝑚=L𝑌+L𝐴+𝛼L𝑓+𝜖L𝑟𝑒𝑔,
where𝛼is the parameter to control the fairness penalty. One obser-
vation hints that minimizing targeted regularization term, e.g., L𝑟𝑒𝑔,
force(𝜙𝑟𝑒𝑔,ˆ𝐴𝑛𝑛,ˆ𝑌𝑟𝑒𝑔)to satisfy the estimating equation in (4):
0=𝜕𝜖(L𝑠𝑢𝑚)|𝜖=𝜖1
𝑛𝑛∑︁
𝑖=1𝜑(𝑌𝑖,𝐴𝑖,˜𝐴𝑖,𝑋𝑖;ˆ𝑌𝑟𝑒𝑔
𝑖,ˆ𝐴𝑛𝑛
𝑖,𝜙𝑟𝑒𝑔
𝑖).
Remark. As our method is formulated using the semi-parametric
analysis, it naturally satisfies the doubly-robustness property. For
ease to understand, we summarize the training &inference proce-
dure as follows:
Step 1. Model training. Learn ˆ𝑌𝑛𝑛as the regression model,
𝑔𝑋as the GNN model to capture the aggregation of neighbors’
3834Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD ’24, August 25–29, 2024, Barcelona, Spain
covariates, and ˆ𝐴𝑛𝑛as the propensity model by minimizing the
training loss:
𝐿reg=1
𝑛𝑛∑︁
𝑖=1∑︁
𝑎∗∈𝐴,˜𝑎∗∈˜𝐴
𝑌𝑖−ˆ𝑌reg
𝑖 𝑎∗
𝑖,˜𝑎∗
𝑖,𝑔𝑋(𝑥𝑖)2
,
where
ˆ𝑌𝑟𝑒𝑔 𝑎∗,˜𝑎∗,𝑔𝑋(𝑥)|𝑎,˜𝑎=ˆ𝑌𝑛𝑛 𝑎∗,˜𝑎∗,𝑔𝑋(𝑥)+𝜖𝐴𝑛𝑛(𝑎∗,˜𝑎∗|𝑔𝑋(𝑥))
ˆ𝐴𝑛𝑛(𝑎,˜𝑎|𝑔𝑋(𝑥));
Step 2. Counterfactual outcomes inference. For testing sam-
plesn
𝑥𝑡𝑒
𝑖,𝑎𝑡𝑒
𝑖,˜𝑎𝑡𝑒
𝑖
|𝑖=𝑛+1,...,𝑛+𝑚o
, the potential outcome
ˆ𝑌𝑡𝑒 𝑎∗,˜𝑎∗,𝑥𝑡𝑒is estimated by:
ˆ𝑌𝑡𝑒 𝑎∗,˜𝑎∗,𝑥𝑡𝑒=ˆ𝑌𝑛𝑛 𝑎∗,˜𝑎∗,𝑔𝑋 𝑥𝑡𝑒+𝜖ˆ𝐴𝑛𝑛 𝑎∗,˜𝑎∗|𝑔𝑋 𝑥𝑡𝑒
ˆ𝐴𝑛𝑛(𝑎𝑡𝑒,˜𝑎𝑡𝑒|𝑔𝑋(𝑥𝑡𝑒))
Step 3. Calculate SF and PF. Then the SF for ˜𝐴=˜𝑎can be
calculated as:
SF=1
𝑚𝑛+𝑚∑︁
𝑖=𝑛+1ˆ𝑌𝑡𝑒
𝑖(𝐴𝑖=1,˜𝑎)−1
𝑚𝑛+𝑚∑︁
𝑖=𝑛+1ˆ𝑌𝑡𝑒
𝑖(𝐴𝑖=0,˜𝑎).
We note that the PF can be calculated from a similar argument.
6 Experiments
In this section, we conduct extensive experiments to answer the
following questions:
Q1.Does our PF and SF models effectively identify peer-induced
unfair decisions in reality?
Q2.Does our IAF+DR framework mitigate bias while preserving
high prediction accuracy?
Q3.Does debiasing with IAF impact other fairness metrics?
Q4.What is the impact of the fairness penalty on the trade-off
between prediction accuracy and fairness?
Q5.Can our IAF+DR framework accurately estimate self-effects
and peer-effects?
6.1 Dataset, Baselines, and Metric
Dataset. In this study, we perform experiments on three datasets:
one synthetic and two real-world datasets. The synthetic dataset,
created based on simulations from [ 5], models hiring decisions
for physically demanding jobs. It comprises one binary sensitive
attribute with three covariates and a binary outcome generated
using a predefined SCM. Additionally, we evaluate our methods on
two real-world datasets, namely, the NBA dataset and the Credit
Default Dataset, which include constructed social networkss [ 1,7].
Baselines. We compare our IAF+DR framework with the follow-
ing baselines including (a) two vanilla decision models on graphs: (1)
Graph Convolution Network (GCN) model [ 18] and (2) the Graph
Attention Network (GAT) model [ 35]; (b) Correlation-based fair
GNN methods : (3)CrossWalk [17] achieves fairness by biasing
random walks to cross group boundaries, (4) FairGNN [7] achieves
fairness by incorporating fairness regularization to ensure equi-
table treatment of different groups, (5) NIFTY [1] aims to improve
counterfactual fairness and stability of node representations by
sample perturbations; (6) InFo_GNN [16] adapts the individual
fairness by considering the interconnectedness of nodes on the
network, (7) GEAR [21] proposes graph augmentation by sampleperturbations. (c) Causality-based Fairness Methods : In our study, we
employ the GNN with CP regularization, denoted as IACP (adapted
from Def. 2). We do not consider path-specific fairness or counter-
factual fairness methods, as they rely on prior causal knowledge
and are not well-suited for interference scenarios. Adapting these
methods to our IAF setting would require independent research.
Metric. In prediction, we assess testing accuracy (ACC) using
AUC for each method. Fairness is quantified following established
protocols [ 1,7] through reporting Demographic Parity (DP) and
Equal Opportunity (EO) as: DP=|𝑃(ˆ𝑌=1|𝐴=1)−𝑃(ˆ𝑌=1|𝐴=
0)|andEO=|𝑃(ˆ𝑌=1|𝑌=1,𝐴=1)−𝑃(ˆ𝑌=1|𝑌=1,𝐴=0)|.
Additionally, we estimate the proposed SF and PF metrics using
IAF+DR on each dataset by reporting the direct effect and peer effect
defined in Assumption. 1. However, recognizing that SF and PF lack
ground truth in real-world data, we introduce a complementary
metric for other baselines to ensure a fair comparison. We extend the
concept of "unfair proportion" used in our case study by employing
matching to mitigate confounding bias from covariates. We term
this metric "Interference-aware Unfairness from Neighbors" (IUFN)
as the quantity measuring the extent of SF and PF. To be spcific,
we first define the variable to reflect the neighboring effect, i.e., 𝑁.
We let𝑁𝑖=1if more than half of the individual’s neighbors have
positive sensitive attributes, and 𝑁𝑖=0otherwise. Supposing that
the test samples are {(𝑋𝑖,𝑌𝑖,𝑁𝑖,𝐴𝑖)}𝑛
𝑖=1, we then define the overall
metric as the quantity measuring the extent of the unfairness arising
from interference in the network as follows:
IUFN =1
𝑛∑︁
𝑖∈𝐵000∑︁
𝑗∈𝐵1011 𝐷 𝑋𝑖,𝑋𝑗≤𝜖+1
𝑛∑︁
𝑖∈𝐵000∑︁
𝑗∈𝐵0111 𝐷 𝑋𝑖,𝑋𝑗≤𝜖
+1
𝑛∑︁
𝑖∈𝐵010∑︁
𝑗∈𝐵1111 𝐷 𝑋𝑖,𝑋𝑗≤𝜖+1
𝑛∑︁
𝑖∈𝐵100∑︁
𝑗∈𝐵1111 𝐷 𝑋𝑖,𝑋𝑗≤𝜖
−1
𝑛∑︁
𝑖∈𝐵001∑︁
𝑗∈𝐵1001 𝐷 𝑋𝑖,𝑋𝑗≤𝜖−1
𝑛∑︁
𝑖∈𝐵001∑︁
𝑗∈𝐵0101 𝐷 𝑋𝑖,𝑋𝑗≤𝜖
−1
𝑛∑︁
𝑖∈𝐵011∑︁
𝑗∈𝐵1101 𝐷 𝑋𝑖,𝑋𝑗≤𝜖−1
𝑛∑︁
𝑖∈𝐵101∑︁
𝑗∈𝐵1101 𝐷 𝑋𝑖,𝑋𝑗≤𝜖,
where𝐵𝑙𝑚𝑘={𝑖∈[𝑛]|𝐴𝑖=𝑙,𝑁𝑖=𝑚,𝑌𝑖=𝑘},1is the indicator
function, and 𝐷(𝑋𝑖,𝑋𝑗)is the L2 distance between 𝑋𝑖and𝑋𝑗, and
the first term is the proportion of units whose outcome would
change from 0 to 1, if we keep 𝑁𝑖=0the same but change 𝐴𝑖from
0 to 1. The rest terms follow a similar argument. We refer to such
metric as “Interference-aware Unfairness from Neighbors” ( IUFN),
as IUFN is similar to the matching method [ 33] to account for causal
peer effects.
Implementations. Our approach, IAF+DR, utilizes a 3-layer
GCN as its core. Specifically, we employ GCN as the embedding
model to capture interference across individuals with respect to
covariates𝑋. We assume a prior knowledge of the neighborhood
exposure mapping, denoted as 𝐴N𝑖∈R|N𝑖|, which transforms into
˜𝐴∈R. Notably, while this assumption may seem restrictive for
general tasks such as estimating causal effects with interference,
we argue that it is justifiable for fairness-related tasks. In fairness
tasks, historical data typically consists of past decision records or
decisions made by previous decision-makers. It is reasonable for the
current decision maker, who is training the model, to possess some
prior knowledge of past decision-making rules. Therefore, following
established protocols in causal inference [ 27,34], we define the
3835KDD ’24, August 25–29, 2024, Barcelona, Spain Wenjing Yang et al.
Table 2: Comparisons of our proposed IAF+DR with the baselines on ACC, AUC, DP, EO, and IUFN. All experiments are repeated
and averaged with 5 independent random seeds. The best performance is marked in bold.
Dataset Metrics GCN GAT CrossWalk FairGNN NIFTY InFoRM_GNN Gear IACP IAF+DR
SyntheticACC (%) 70.1±0.2 71.3±0.2 65.5±0.6 69.6±0.3 65.3±0.3 69.1 ±0.2 74.5±0.1 71.1±0.2 73.2±0.1
AUC (%) 62.7±0.2 65.8±0.1 61.7±0.3 60.6±0.2 50.0±0.4 69.1 ±0.1 72.5±0.1 68.0±0.1 73.1±0.2
DP 0.06±0.03 0.06±0.01 0.04±0.01 0.05±0.01 0.03±0.01 3.68±0.4 0.94 ±0.1 0.16±0.03 0.08±0.02
EO 0.05±0.02 0.05±0.01 0.06±0.01 0.03±0.01 0.03±0.02 1.11 ±0.03 0.95±0.08 0.13±0.03 0.09±0.02
IUFN (%) 17.5±0.3 19.6±0.4 16.3±0.6 18.3±0.6 18.5±0.2 19.5 ±0.3 18.5±0.4 18.0±0.2 0.5±0.2
NBAACC (%) 65.9±0.2 66.8±0.1 46.8±0.4 72.3±0.3 56.3±0.6 68.1 ±0.5 68.2±0.4 70.1±0.2 72.3±0.1
AUC (%) 65.9±0.2 67.2±0.1 46.5±0.6 72.3±0.4 56.3±0.6 68.8 ±0.4 65.9±0.3 70.1±0.3 71.4±0.2
DP 0.31±0.06 0.41±0.10 10.71±1.25 0.14±0.06 0.04±0.01 2.97±0.41 0.19 ±0.08 0.06±0.03 0.05±0.02
EO 0.26±0.05 0.31±0.14 28.69±1.68 0.22±0.07 0.13±0.03 1.20 ±0.32 0.21±0.11 0.26±0.03 0.03±0.01
IUFN (%) 16.60±0.2 22.20±0.4 21.1±0.3 11.1±0.3 17.1±0.4 12.2 ±0.2 18.8±0.2 10.0±0.1 3.3±0.1
CreditACC (%) 69.6±0.2 70.4±0.1 72.7±0.6 66.8±0.4 67.6±0.3 69.3 ±0.1 66.2±0.2 68.7±0.2 70.9±0.2
AUC (%) 64.7±0.2 66.2±0.1 54.3±0.5 63.1±0.3 64.2±0.4 68.1±0.1 65.4±0.3 67.5±0.2 67.9±0.2
DP 0.13±0.01 0.11±0.02 0.03±0.01 0.32±0.10 0.19±0.03 1.49 ±0.69 0.11±0.01 0.06±0.03 0.05±0.01
EO 0.12±0.02 0.12±0.04 0.03±0.02 0.30±0.11 0.19±0.02 6.35 ±1.05 0.11±0.06 0.03±0.02 0.03±0.01
IUFN (%) 5.8±0.2 6.7±0.2 3.6±0.7 4.0±0.6 4.0±0.7 3.8 ±0.1 4.6±0.2 5.2±0.2 0.8±0.2
neighborhood exposure mapping from 𝐴N𝑖to˜𝐴as binary-valued:
˜𝐴=1when half of 𝐴N𝑖is positive, and ˜𝐴=0otherwise (see
Appendix D.2 for details on baselines).
6.2 Performance Comparison
SF and PF captures unfair decisions raised from peer ef-
fects (Q1). In Fig. 3a, 3b, and 3c, we present the IFUN metric on
synthetic data and two real-world datasets prior to applying our
IAF+DR debiasing method. As previously indicated, the IFUN metric
estimates SF and PF by cross-referencing diverse groups of individ-
uals. Statistical findings regarding IFUN, particularly with NBA and
credit data, reveal that real-world data often exhibits bias attribut-
able to self-effect or peer effects of sensitive attributes. For instance,
over 20% of NBA data records exhibit nationality-based discrimina-
tion originating from either an individual’s own nationality or that
of their neighbors. This reaffirms our stance: decision-making in an
interconnected world is susceptible to bias stemming from self or
neighbor-related sensitive attributes, emphasizing the importance
of identifying and mitigating these forms of discrimination.
Our IAF+DR migrates unfair decisions while maintains pre-
diction performance (Q2). In Table 2, we present the mean and
standard deviations of metrics across baseline models for three
datasets. Our observations are as follows:
•Previous debiasing approaches, including CrossWalk, FairGNN,
NIFTY, InFoRM_GNN, and Gear, achieve satisfactory prediction
performance and reduce bias on standard fairness metrics like DP
and EO. However, their performance on the interference-fairness
metric (IUFN) indicates that they still retain discrimination stem-
ming from neighboring relationships. Notably, the IUFN of Cross-
Walk and Gear exceeds that of GCN by nearly 20% on the NBA
dataset.
•Our compared baseline, IACP, falls short in mitigating interference-
aware discrimination across all datasets. These results confirm
our theoretical analysis in Section 4.2, which asserts that the van-
ished total effect cannot be assumed to eliminate both self-effect
and peer effects.•In contrast, our proposed IAF+DR method effectively reduces the
IUFN metric across all datasets when compared to other baselines,
while maintaining competitive prediction accuracy.
Debias on SF and PF will not conflict with conventional
fairness metric (Q3). As depicted in Table 2, our IAF+DR approach
effectively mitigates bias in IUFN while maintaining low bias in DP
and DP across various datasets. These results support our decompo-
sition, TF=SF+PF, where DP and EO represent correlated versions
of TF for the overall population and specific sub-populations. We
contend that addressing bias in SF and PF aligns with established
fairness criteria.
6.3 In-depth Analysis
Impact of fairness penalty to prediction and fairness (Q4). In
Fig.4, we observe that as the penalty parameter ( 𝛼) increases, the
test accuracy of our IAF+DR stabilizes in the range [0.0,1.5]while
the IUFN metric sharply declines for 𝛼>1.25. As𝛼approaches 2.0,
both ACC and IUFN reach and maintain lower values. This phenom-
enon highlights the successful debiasing capability of our proposed
IAF+DR while maintaining high prediction accuracy within the
fairness penalty range of 𝛼∈[1,1.5]. It’s worth noting that the re-
duction in prediction performance with increasing fairness penalty
is a common occurrence due to historical decision data bias, as
discussed in Q1. The fairness task seeks to strike a balance between
acceptable prediction accuracy and minimizing decision bias.
Targeted regularization is crucial for estimations of SF
and PF (Q5). We evaluated SF and PF estimation with and without
our targeted regularization term L𝑟𝑒𝑔(see Fig. 3d). Specifically,
"withoutL𝑟𝑒𝑔" denotes the optimization of only L𝑌andL𝐴, with
decision outcomes derived from ˆ𝑌𝑛𝑛rather than ˆ𝑌𝑟𝑒𝑔Since only
synthetic data possesses ground truth for SF and PF, measuring
estimation error on NBA and Credit is infeasible, necessitating the
introduction of the IUFN metric. Notably, the absence of L𝑟𝑒𝑔(left
bars) results in a significant increase in estimation error compared
to its presence (right bars).
3836Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) Debiasing on Synthetic
 (b) Debiasing on NBA
 (c) Debiasing on Credit
 (d) Estimation on Synthetic
Figure 3: Results on: (a,b,c): Debiasing performance of SF and PF across datasets; (d) Estimation performance on SF and PF for
synthetic data. All experiments are repeated and averaged with 5 independent random seeds.
(a) Trade-off on NBA Data
 (b) Trade-off on Credit Data
Figure 4: Trade-off between fairness and prediction accuracy
on NBA and Credit data by tuning the fairness penalty 𝛼
ranging in[0.0,0.01,0.02,..., 2.00].
(a) AUC
 (b) IUFN
Figure 5: Impact of substitution on the backbone across three
datasets. All experiments are repeated and averaged with 5
independent random seeds.
Impact of different backbones. Questions may arise regarding
the criticality of this choice and the adaptability of our proposed
IAF+DR framework to various backbones. To address these con-
cerns, we present a comparative analysis of AUC and IUFN results
for our IAF+DR model across three datasets using both GCN and
GAT as backbones (see Fig. 5). Our results demonstrate that the
choice of backbone, whether GCN or GAT, does not significantly
impact the prediction performance or debiasing capabilities of our
IAF+DR framework.
Impact of parameter 𝜖.In Fig.6, we observe that as 𝜖increases,
the debias capability of our IAF+DR, i.e., IUFN, and the estimation
error decreases sharply. When 𝜖≥1, all metrics are stabilized. Such
phenomenon shows that the DR regularization effectively estimates
SF and PF, and efficiently migrates bias.
7 Conclusion
In this paper, we introduce Interference-Aware Fairness (IAF), a
novel concept addressing discrimination within interconnected
Figure 6: Impact of the parameter for our DR framework. i.e.,
𝜖, on estimation and fairness. Results are reported by tuning
𝜖ranging in[0.0,0.01,0.02,..., 2.00].
individuals on social networks. We establish that achieving IAF
is tantamount to achieving two proposed fairness metrics: Self-
Fairness (SF) and Peer-Fairness (PF). Consequently, we present a
doubly robust framework for end-to-end estimation and mitigation
of SF and PF in model decisions.
We propose several avenues for future research. Firstly, an exten-
sion of IAF to individual-level fairness, specifically addressing coun-
terfactual causal inference, warrants attention. While individual-
level IAF offers sharper fairness compared to population-level IAF,
its identification in the context of SF and PF (individual-level) re-
mains an open challenge. An alternative approach gaining popular-
ity involves establishing a computationally feasible upper bound
for SF and PF based on observational data.
Acknowledgement
This work was supported in part by the National Natural Science
Foundation of China (623B2002,62141607,62376243,62441605,62302
503,KY0402052402), Starry Night Science Fund of Zhejiang Univer-
sity Shanghai Institute for Advanced Study (SN-ZJU-SIAS-0010),
and NUDT Youth Independent Innovation Science Fund Project
(Grant No. ZK23-15).
3837KDD ’24, August 25–29, 2024, Barcelona, Spain Wenjing Yang et al.
References
[1]Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. 2021. Towards a uni-
fied framework for fair and stable graph representation learning. In Uncertainty
in Artificial Intelligence. PMLR, 2114–2124.
[2]Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias.
(2016).
[3]Hal Berghel. 2020. A Critical Look at the 2019 College Admissions Scandal?
Computer 53, 1 (2020), 72–77.
[4]Silvia Chiappa. 2019. Path-specific counterfactual fairness. In Proceedings of the
AAAI conference on artificial intelligence, Vol. 33. 7801–7808.
[5]Yoichi Chikahara, Shinsaku Sakaue, Akinori Fujino, and Hisashi Kashima. 2021.
Learning individually fair classifier with path-specific causal-effect constraint. In
International conference on artificial intelligence and statistics. PMLR, 145–153.
[6]Manvi Choudhary, Charlotte Laclau, and Christine Largeron. 2022. A survey on
fairness for machine learning on graphs. arXiv preprint arXiv:2205.05396 (2022).
[7] Enyan Dai and Suhang Wang. 2021. Say no to the discrimination: Learning fair
graph neural networks with limited sensitive attribute information. In Proceedings
of the 14th ACM International Conference on Web Search and Data Mining . 680–
688.
[8]Richard B Darlington. 1971. Another look at “cultural fairness” 1. Journal of
educational measurement 8, 2 (1971), 71–82.
[9]Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in
theoretical computer science conference. 214–226.
[10] Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian
Weller. 2016. The case for process fairness in learning: Feature selection for fair
decision making. In NIPS symposium on machine learning and the law, Vol. 1.
Barcelona, Spain, 11.
[11] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. Advances in neural information processing systems 29 (2016).
[12] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. 2020. Fair multiple decision
making through soft interventions. Advances in Neural Information Processing
Systems 33 (2020), 17965–17975.
[13] Michael G Hudgens and M Elizabeth Halloran. 2008. Toward causal inference
with interference. J. Amer. Statist. Assoc. 103, 482 (2008), 832–842.
[14] Kosuke Imai and Zhichao Jiang. 2023. Principal fairness for human and algorith-
mic decision-making. Statist. Sci. 38, 2 (2023), 317–328.
[15] Guido W Imbens and Donald B Rubin. 2010. Rubin causal model. In Microecono-
metrics. Springer, 229–241.
[16] Jian Kang, Jingrui He, Ross Maciejewski, and Hanghang Tong. 2020. Inform:
Individual fairness on graph mining. In Proceedings of the 26th ACM SIGKDD
international conference on knowledge discovery & data mining. 379–389.
[17] Ahmad Khajehnejad, Moein Khajehnejad, Mahmoudreza Babaei, Krishna P Gum-
madi, Adrian Weller, and Baharan Mirzasoleiman. 2022. Crosswalk: Fairness-
enhanced node representation learning. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 36. 11963–11970.
[18] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[19] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfac-
tual fairness. Advances in neural information processing systems 30 (2017).
[20] Yanying Li, Yue Ning, Rong Liu, Ying Wu, and Wendy Hui Wang. 2020. Fairness
of classification using users’ social relationships in online peer-to-peer lending.
InCompanion Proceedings of the Web Conference 2020. 733–742.
[21] Jing Ma, Ruocheng Guo, Mengting Wan, Longqi Yang, Aidong Zhang, and Jun-
dong Li. 2022. Learning fair node representations with graph counterfactual
fairness. In Proceedings of the Fifteenth ACM International Conference on Web
Search and Data Mining. 695–703.
[22] Yunpu Ma and Volker Tresp. 2021. Causal inference under networked interference
and intervention policy enhancement. In International Conference on ArtificialIntelligence and Statistics. PMLR, 3700–3708.
[23] Karima Makhlouf, Sami Zhioua, and Catuscia Palamidessi. 2020. Survey on
causal-based machine learning fairness notions. arXiv preprint arXiv:2010.09553
(2020).
[24] Alan Mishler, Edward H Kennedy, and Alexandra Chouldechova. 2021. Fairness in
risk assessment instruments: Post-processing to achieve counterfactual equalized
odds. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and
Transparency. 386–400.
[25] Shira Mitchell, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian
Lum. 2021. Algorithmic fairness: Choices, assumptions, and definitions. Annual
Review of Statistics and Its Application 8 (2021), 141–163.
[26] Razieh Nabi and Ilya Shpitser. 2018. Fair inference on outcomes. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 32.
[27] Elizabeth L Ogburn, Oleg Sofrygin, Ivan Diaz, and Mark J Van der Laan. 2022.
Causal inference for social network data. J. Amer. Statist. Assoc. (2022), 1–15.
[28] John Palowitch and Bryan Perozzi. 2019. Monet: Debiasing graph embeddings via
the metadata-orthogonal training unit. arXiv preprint arXiv:1909.11793 (2019).
[29] Judea Pearl. 2009. Causality. Cambridge university press.
[30] Donald B Rubin. 2005. Causal inference using potential outcomes: Design, mod-
eling, decisions. J. Amer. Statist. Assoc. 100, 469 (2005), 322–331.
[31] Akrati Saxena, George Fletcher, and Mykola Pechenizkiy. 2022. Fairsna: Al-
gorithmic fairness in social network analysis. arXiv preprint arXiv:2209.01678
(2022).
[32] Claudia Shi, David Blei, and Victor Veitch. 2019. Adapting neural networks for
the estimation of treatment effects. Advances in neural information processing
systems 32 (2019).
[33] Elizabeth A Stuart. 2010. Matching methods for causal inference: A review and a
look forward. Statistical science: a review journal of the Institute of Mathematical
Statistics 25, 1 (2010), 1.
[34] Mark J Van der Laan. 2014. Causal inference for a population of causally connected
units. Journal of Causal Inference 2, 1 (2014), 13–74.
[35] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[36] Yongkai Wu, Lu Zhang, and Xintao Wu. 2018. On discrimination discovery
and removal in ranked data using causal graph. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. 2536–
2544.
[37] Yongkai Wu, Lu Zhang, and Xintao Wu. 2019. Counterfactual fairness: Unidenti-
fication, bound and algorithm. In Proceedings of the twenty-eighth international
joint conference on Artificial Intelligence.
[38] Yongkai Wu, Lu Zhang, and Xintao Wu. 2019. On convexity and bounds of
fairness-aware classification. In The World Wide Web Conference. 3356–3362.
[39] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. 2019. Pc-fairness: A
unified framework for measuring causality-based fairness. Advances in neural
information processing systems 32 (2019).
[40] Depeng Xu, Yongkai Wu, Shuhan Yuan, Lu Zhang, and Xintao Wu. 2019. Achiev-
ing causal fairness through generative adversarial networks. In Proceedings of
the Twenty-Eighth International Joint Conference on Artificial Intelligence.
[41] Yuan Yuan, Kristen Altenburger, and Farshad Kooti. 2021. Causal network motifs:
identifying heterogeneous spillover effects in A/B tests. In Proceedings of the Web
Conference 2021. 3359–3370.
[42] Lu Zhang, Yongkai Wu, and Xintao Wu. 2016. A causal framework for discovering
and removing direct and indirect discrimination. arXiv preprint arXiv:1611.07509
(2016).
[43] Aoqi Zuo, Susan Wei, Tongliang Liu, Bo Han, Kun Zhang, and Mingming Gong.
2022. Counterfactual fairness with partially known causal graph. Advances in
Neural Information Processing Systems 35 (2022), 1238–1252.
3838Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD ’24, August 25–29, 2024, Barcelona, Spain
A Details on Case Study
The Prosper dataset contains 1,048,575 loan records that occurred
from November 2005 to September 2011. Each record contains
nine features, including the Lender ID, Borrower ID, Timestamp,
Amount, Status (a binary variable representing the risk of each bor-
rower), Lender rate, Borrower rate, and the Rating. We refer readers
to [20] for a more detailed introduction to each feature. The social
network is naturally built for each individual by adding undirected
edges among each borrower and lender [ 20]. However, we only
concerned with the borrower network for individual judgment task,
i.e., the network consists of individuals who has become a borrower
more than once.
The first task, i.e. the individual social score judgment task, is
performed as a prediction task on the borrower network, where
the decision outcome is the social score for each borrower. In our
case study, such variable is interpreted as the “Location” for each
borrower. Of course, a loan decision should be discriminative on
the locations of borrowers. Considering the social network built
by loan relationships, this can be regarded as a node classification
task. Especially, the original social score is set as follows:
𝑠𝑖=1
1+
𝜆
1−𝜆𝑔𝑖𝜆𝑝+(1−𝜆)
𝜆+(1−𝜆)𝑝𝐿𝑖𝜆+(1−𝜆)𝑝
𝜆𝑝+(1−𝜆)𝐻𝑖, (7)
where𝐻𝑖and𝐿𝑖account for the number of high-risk and low-
risk neighboring borrowers, and 𝑔𝑖=I(𝐻𝑖>𝐿𝑖). We set hyper-
parameter𝜆=0.46and𝑝=0.5following original implementations
in [20]. Based on the value of 𝑠𝑖∈[0,1], we create discrete labels in
[0,1,2]by setting𝑠𝑖falling in[0.0,0.33),[0.33,0.66), and[0.66,1.0],
respectively. We then construct two models, i.e., one GNN and one
MLP, for prediction4. The GNN model contains three Graph Con-
volutional Layers (GCL) with reluactivation functions. The MLP
model contains three fully connected layers with reluactivation
functions. All the latent representations are set to 64 neurons. We
optimize both GNN and MLP using the Adam optimizer with an
initial learning rate 0.01and weight-decay as 5𝑒−4. Both the accu-
racy and unfair proportions metric are reported on the testing data,
where we split the whole data in the ratio 8 : 2.
The second task, i.e. the loan risk judgment task, is performed as
a prediction task on the overall loan records, where the decision out-
come is the loan status, i.e., the risk of each loan. Following [ 20], we
use the XGBoost classifier in Sklearn https://scikit-learn.org/stable/
to perform prediction. We here set the 𝑠𝑖into binary variables,
where𝑠𝑖<0.5corresponds to label 0and𝑠𝑖>=0.5corresponds
to label 1. To test whether the accuracy and unfair proportion will
change with and without modeling the network structure, we con-
struct two XGBoost predictors with and without the social score
as features, i.e., the XGB:S and XGB:non-S. Notably, we interpret
the social feature here as a signal of the living/working location
of each borrower, as the calculation of 𝑠𝑖relies on𝑠𝑖’s neighbors.
Both the accuracy and unfair proportions metric are reported on
the testing data, where we split the whole data in the ratio 8 : 2.
The unfair proportion we reported in the case study roughly
characterizes interference-specific unfairness, which reports the
4We note that all GNN-related models are constructed by the Pytorch-geometric
package in https://pytorch-geometric.readthedocs.io/en/latest/proportion of individuals who receive unfair decisions caused by
their neighboring relationships. For the first task, individuals re-
ported by unfair proportion can be divided into two types: (1)
individuals with low observed signals (0) and powerful connec-
tions (ground truth of social score 𝑠==2) receive negative credit
judgment (predicted 𝑠==0); (2) individuals with high observed sig-
nals (1) and weak connections (ground truth of social score 𝑠==0)
receives negative credit judgment (predicted 𝑠==2). Similarly,
for the second task, the unfair proportion accounts for two types
of samples: (1) loan record with low loan status (ground truth of
low-risk borrower) and powerful connections (social score of the
borrower𝑠>0.9) receive negative evaluation (high risk); (2) loan
record with high loan status (ground truth of high-risk borrower)
and weak connections (social score of the borrower 𝑠<0.1) receive
positive evaluation (low risk).
B Details on Comparison between our IAF and
Conventional Fairness Notions
We first detail the constructed counterexample to distinguish our
IAF from DP. To be more intuitive, we present the unfair decision
model, i.e.,𝑌, as follows:
𝐴1𝐴2𝑃(ˆ𝑌=1|𝐴1,𝐴2)
𝑎𝑎 0.1
𝑎′𝑎′0.9
𝑎𝑎′0.5
𝑎′𝑎 0.5(8)
, together with 𝑃(𝐴1=𝑎|𝐴2=𝑎′)=0.99,𝑃(𝐴1=𝑎|𝐴2=𝑎)=
0.01and𝑃(𝐴2=𝑎′)=0.5=𝑃(𝐴2=𝑎), then we first derive the
DP for individual 2 as follows:
𝑃
ˆ𝑌2=1|𝐴2=𝑎′
=∑︁
𝑎1∈{𝑎,𝑎}𝑃
ˆ𝑌2=1|𝐴1=𝑎1,𝐴2=𝑎′
𝑃 𝐴1=𝑎1|𝐴2=𝑎′=0.5
𝑃
ˆ𝑌2=1|𝐴2=𝑎
=∑︁
𝑎1∈{𝑎,𝑎}𝑃
ˆ𝑌2=1|𝐴1=𝑎1,𝐴2=𝑎
𝑃(𝐴1=𝑎1|𝐴2=𝑎)=0.496.
We then derive the posterior probabilities as follows:
𝑃 𝐴1=𝑎′
=∑︁
𝑎2∈{𝑎,𝑎}𝑃 𝐴1=𝑎′|𝐴2=𝑎2𝑃(𝐴2=𝑎2)=0.5
𝑃(𝐴1=𝑎)
=∑︁
𝑎2∈{𝑎,𝑎}𝑃(𝐴1=𝑎|𝐴2=𝑎2)𝑃(𝐴2=𝑎2)=0.5,
and we have:
𝑃 𝐴2=𝑎|𝐴1=𝑎′=𝑃(𝐴1=𝑎′|𝐴2=𝑎)𝑃(𝐴2=𝑎)
𝑃(𝐴1=𝑎′)=0.99
𝑃(𝐴2=𝑎|𝐴1=𝑎)=𝑃(𝐴1=𝑎|𝐴2=𝑎)𝑃(𝐴2=𝑎)
𝑃(𝐴1=𝑎)=0.01.
Hence, we derive similar results for individual 2 as follows:
𝑃
ˆ𝑌1=1|𝐴1=𝑎′
=0.504
𝑃
ˆ𝑌1=1|𝐴1=𝑎
=0.505
3839KDD ’24, August 25–29, 2024, Barcelona, Spain Wenjing Yang et al.
Algorithm 1 Training procedure of Our DR Framework
1:Input The datasetD={𝑋𝑖,𝐴𝑖,𝑌𝑖}𝑛
𝑖=1with the adjacent ma-
trix𝐺, the GNN-based feature encoder 𝑉, the neighborhood
exposure mapping Φ, the hyper-parameter 𝜖.
2:foritr = 1 toIdo
3: Embedding covariates 𝑋byEmb X=𝑉(𝑋,𝐺);
4: Computing the neighbor exposure mapping for each node
as˜𝐴=Φ(𝐴,𝐺);
5: Compute (observed) hashed value 𝐴𝑠𝑢𝑚=Φ(𝐴,˜𝐴);
6: Compute the prediction head ˆ𝑌𝑛𝑛(𝐸𝑚𝑏𝑋,𝐴𝑠𝑢𝑚);
7: Compute the score head ˆ𝐴𝑛𝑛(𝐴𝑠𝑢𝑚|𝐸𝑚𝑏𝑋);
8:L𝑟𝑒𝑔=0;
9: forAll possible values (𝑎∗,˜𝑎∗)in𝐴×˜𝐴:do
10: Compute hashed value 𝐴∗,𝑠𝑢𝑚=Φ(𝑎∗,˜𝑎∗);
11: Compute ˆ𝑌𝑟𝑒𝑔(𝐴∗,𝑠𝑢𝑚,𝐸𝑚𝑏𝑋);
12: Compute ˆ𝐴𝑛𝑛(𝐴∗,𝑠𝑢𝑚|𝐸𝑚𝑏𝑋);
13: Compute ˆ𝑌𝑟𝑒𝑔(𝐴∗,𝑠𝑢𝑚,𝐸𝑚𝑏𝑋)based on above terms;
14: AccumulateL𝑟𝑒𝑔;
15: end for
16: forAll possible values (𝑎∗,˜𝑎∗)in𝐴×˜𝐴:do
17: Compute𝜙𝑟𝑒𝑔(𝑎∗,˜𝑎∗);
18: end for
19: ComputeL𝑓based on{𝜙𝑟𝑒𝑔(𝑎∗,˜𝑎∗)}𝑎∗∈𝐴,˜𝑎∗∈˜𝐴;
20: ComputeL𝑌=1
𝑛Í𝑛
𝑖=1L(𝑌𝑖,ˆ𝑌𝑛𝑛
𝑖(𝐴𝑠𝑢𝑚,𝑔𝑋(𝑥𝑖)))2;
21: ComputeL𝐴=1
𝑛Í𝑛
𝑖=1L(𝐴𝑖,ˆ𝐴𝑛𝑛(𝐴𝑠𝑢𝑚|𝑔𝑋(𝑥𝑖)));
22: Update overall objective L𝑠𝑢𝑚;
23:end for
24:Output Using ˆ𝑌𝑛𝑛as the decision model.
C Algorithm Procedure
We detail the procedure of our proposed IAF+DR in Alg. 1.
D Experimental Details
D.1 Dataset Details
Synthetic Dataset Description Following [ 5], we construct the
synthetic data containing the gender 𝐴∈{0,1}, qualification 𝑄,
number of children 𝐷, physical strength 𝑀𝜃, and hiring decision
outcome𝑌∈{0,1}following a pre-defined SCM. We first randomly
sample the social network by generating the adjacent matrix 𝐺:
𝐺𝑖𝑗∼Bernoulli(0.5). We then generate each feature attribute as
follows:
𝑄=
𝑈𝑄
, 𝑄∈R𝑛𝑈𝑄∼N
2,52
,
𝐷=𝐴+⌊0.5𝑄𝑈𝐷⌋, 𝐷∈R𝑛,𝑈𝐷∼TrN
2,12,0.1,3.0
,
𝑀=3𝐴+0.4𝑄𝑈𝑀, 𝑀∈R𝑛, 𝑈𝑀∼TrN
3,22,0.1,3.0
,
𝐴∼Bernoulli
expit(𝐺𝑇𝑄−40)
, 𝐴∈R𝑛,(9)
where expit refers to the sigmoid function. We then perform ex-
posure mapping for 𝐴N𝑖for each𝑖𝑛[𝑛]and thus the mapping of
𝐴:
˜𝐴𝑖=I(𝐺𝑇𝐴>0.5∗mean(𝐺𝑇𝐴)),where mean refers to the mean-value of the vector 𝐺𝑇𝐴, andI
refers to the indicator function. Intuitively, our exposure mapping
here implies that ˜𝐴𝑖=1if the number of positive neighbors of 𝑖is
larger than the mean level of the overall population. Furthermore,
we generated hashed 𝐴from ˜𝐴𝑖and𝐴𝑖:
𝐴ℎ𝑎𝑠ℎ= 
0,if˜𝐴𝑖=0&𝐴=1
1,if˜𝐴𝑖=1&𝐴=0or˜𝐴𝑖=0&𝐴=1
2,if˜𝐴𝑖=1&𝐴=1(10)
Hence,𝑌are generated as follows:
𝑌∼Bernoulli(expit(−20+10𝐴ℎ𝑎𝑠ℎ+0.1∗(𝐺𝑇𝑄+𝐺𝑇𝐷+𝐺𝑇𝑀)))
NBA dataset This is extended from a Kaggle dataset 1 containing
around 400 NBA basketball players. The performance statistics of
players in the 2016-2017 season and other various information e.g.,
nationality, age, and salary are provided. The social network is
constructed by collecting the relationships of the NBA basketball
players on Twitter with its official crawling API 2. The sensitive, i.e.,
the nationality, is binarized into two categories, i.e., U.S. players and
overseas players. The classification, in task is to predict whether
the salary of the player is over the median [7].
Credit Default Dataset Credit Default Dataset is comprised
of 30,000 nodes, where each node represents individuals who are
utilizing some form of credit. Each node contains 13 attributes.
Individuals are connected by their spending and payment behavior.
The classification task is to determine whether an individual will
default on the credit card payment. Age is used as the sensitive
attribute [1].
D.2 Implementation Details
To unify backbone embedding model across all baselines, we
set the GNN model with three GCN layers throughout our exper-
iments (except for the GAT baseline). The activation function is
therelufunction, and the optimizer is set as the Adam optimizer
with initial learning rate 0.001and weight_decay as 5𝑒−4. The
training epoch for all baselines is set to 300. We run experiments
on all baselines using the open-source project, i.e., the pygdebias
package in https://github.com/yushundong/PyGDebias. For the
NIFTY [ 1], we set the penalty parameter as 0.6following original
implementations. For fairGNN method [ 7], we set𝛼=100and
𝛽=1following their optimal setting. For CrossWalk in [ 17], the
random walk length is set to 𝑑=5, and the number of walks is
set to𝑟=500, and their 𝛼=0.5and𝑝=2. For InfoRM_GNN
in [16], we set𝛼=10and𝜂across all datasets by combining
their original suggestions and our cross-fold validation results. For
GEAR [ 21], we follow their original parameter setting, which states
that𝜆=0.6,𝐶=2,𝜆𝑠=0.4,𝛽=10,𝜇=1𝑒−5,𝑘=20,𝐵=4. For
our IAF+DR method, we map ˜𝐴and𝐴to a scalar value by simply
operations as 𝐴ℎ𝑎𝑠ℎ=˜𝐴+2∗𝐴, as we hold the belief that individuals
with low𝐴and low ˜𝐴will receives the worst decisions, individuals
with high𝐴and high ˜𝐴will receive best decisions, and individuals
with low𝐴and high ˜𝐴or high𝐴but low ˜𝐴will lie between. Besides,
we set𝛼=1with𝜖=0.1throughout our experiments.
3840