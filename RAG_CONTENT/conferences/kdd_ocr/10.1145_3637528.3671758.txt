Where Have You Been? A Study of Privacy Risk for
Point-of-Interest Recommendation
Kunlin Cai
kunlin96@g.ucla.edu
University of California, Los Angeles
Los Angeles, USAJinghuai Zhang
jinghuai1998@g.ucla.edu
University of California, Los Angeles
Los Angeles, USAZhiqing Hong
zh252@cs.rutgers.edu
Rutgers University
New Brunswick, USA
William Shand
wss2ec@g.ucla.edu
University of California, Los Angeles
Los Angeles, USAGuang Wang
guang@cs.fsu.edu
Florida State University
Tallahassee, USADesheng Zhang
desheng@cs.rutgers.edu
Rutgers University
New Brunswick, USA
Jianfeng Chi
jianfengchi@meta.com
Meta
New York, USAYuan Tian
yuant@ucla.edu
University of California, Los Angeles
Los Angeles, USA
ABSTRACT
As location-based services (LBS) have grown in popularity, more
human mobility data has been collected. The collected data can be
used to build machine learning (ML) models for LBS to enhance
their performance and improve overall experience for users. How-
ever, the convenience comes with the risk of privacy leakage since
this type of data might contain sensitive information related to
user identities, such as home/work locations. Prior work focuses
on protecting mobility data privacy during transmission or prior to
release, lacking the privacy risk evaluation of mobility data-based
ML models. To better understand and quantify the privacy leak-
age in mobility data-based ML models, we design a privacy attack
suite containing data extraction and membership inference attacks
tailored for point-of-interest (POI) recommendation models, one
of the most widely used mobility data-based ML models. These
attacks in our attack suite assume different adversary knowledge
and aim to extract different types of sensitive information from
mobility data, providing a holistic privacy risk assessment for POI
recommendation models. Our experimental evaluation using two
real-world mobility datasets demonstrates that current POI recom-
mendation models are vulnerable to our attacks. We also present
unique findings to understand what types of mobility data are more
susceptible to privacy attacks. Finally, we evaluate defenses against
these attacks and highlight future directions and challenges.
CCS CONCEPTS
•Security and privacy →Privacy-preserving protocols ;•Infor-
mation systems→Location based services.
Correspondence to Kunlin Cai, Yuan Tian, and Jianfeng Chi. Work unrelated to Meta.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671758KEYWORDS
POI recommendation; privacy-preserving machine learning; data
extraction; membership inference
ACM Reference Format:
Kunlin Cai, Jinghuai Zhang, Zhiqing Hong, William Shand, Guang Wang,
Desheng Zhang, Jianfeng Chi, and Yuan Tian. 2024. Where Have You Been?
A Study of Privacy Risk for Point-of-Interest Recommendation. In Proceed-
ings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671758
1 INTRODUCTION
With the development and wide usage of mobile and wearable de-
vices, large volumes of human mobility data are collected to support
location-based services (LBS), such as traffic management [ 3,34],
store location selection [ 38], and point-of-interest (POI) recom-
mendation [ 58,69]. In particular, POI recommendation involves
relevant POI suggestions to users for future visits based on per-
sonal preferences using ML techniques [ 27], which has recently
gained much research attention1. POI recommendation models
have also been integrated into popular services such as Yelp and
Google Maps to assist users in making informed decisions about
the next destination to visit. However, mobility data collected to
train POI recommendation models are highly sensitive as they can
leak users’ sensitive information such as their social relationships,
trip purposes, and identities [4].
Although there are a significant number of studies [ 2,21,32,54]
on mobility data privacy, the existing research primarily focuses
on analyzing attacks and evaluations within the context of mobil-
ity data transmission and release processes. For example, previous
studies have demonstrated the linkages of mobility data from vari-
ous side channels, including social networks [ 24,26], open-source
datasets [ 20,44], and network packets [ 29,61]. The linkages be-
tween these side channels can lead to the identification of individ-
uals. As a result, efforts to protect mobility data have primarily
1From 2017 to 2023, there are more than 111 papers on POI recommendation built
upon mobility data collected by location service providers [64].
 
175
KDD ’24, August 25–29, 2024, Barcelona, Spain Kunlin Cai et al.
Figure 1: Our attack suite highlights the privacy concerns in
POI recommendation models. In particular, we demonstrate
that an adversary can extract or infer membership informa-
tion of locations or trajectories in the training dataset.
concentrated on data aggregations and releases [ 5,21,41]. These
studies neglect the risk of adversaries extracting sensitive attributes
or properties from the ML models (e.g., POI recommendation mod-
els) that use mobility data for training, which are inherently sus-
ceptible to privacy attacks [6, 52].
Evaluating privacy risks in POI recommendation models remains
challenging because existing attack and defense mechanisms are
ineffective due to the unique features of mobility data. Previous
privacy attacks have mainly focused on ML models trained with
image and text data [ 8,18,52], where each data point can uniquely
identify itself. However, mobility data, such as locations, are less
semantically unique without the context. Moreover, mobility data
is special in that it contains multimodal spatial and temporal infor-
mation, which describes each individual’s movements and behavior
patterns over time. All existing attacks fail to construct meaning-
ful context and leverage spatial-temporal information, resulting in
their failures when applied to POI recommendations. Furthermore,
existing defense mechanisms [ 1,50,51] have mainly been tested
on classification models trained with image or text data. Given the
task and data are significantly different, the effectiveness of defense
mechanisms is unknown when applied to POI recommendation.
In this paper, we design a comprehensive privacy attack suite to
study the privacy leakage in POI recommendation models trained
with mobility data. Specifically, our privacy attack suite contains
the two most popular kinds of privacy attacks on machine learning
models, data extraction and membership inference attacks, to assess
the privacy vulnerabilities of POI recommendation models at both
location andtrajectory levels. In contrast to privacy attacks for
image and text data, the attacks in our attack suite are tailored
for mobility data and aim to extract different types of sensitive
information based on practical adversary knowledge.
We perform experiments on three representative POI recommen-
dation models trained on two mobility datasets. We demonstrate
that POI recommendation models are vulnerable to our designed
data extraction and membership inference attacks. We further pro-
vide an in-depth analysis to understand what factors affect the
attack performance and contribute to the effectiveness of the at-
tacks. Based on our analysis, we discover that the effect of data
outliers exists in privacy attacks against POI recommendations,
making training examples with certain types of users, locations,
and trajectories particularly vulnerable to the attacks in the attack
suite. Finally, We test several existing defenses and find that theydo not effectively thwart our attacks with negligible utility loss,
which calls for better methods to defend against our attacks.
Contributions:
•We introduce a novel privacy attack suite2that incorporates
unique characteristics of mobility data (e.g., spatial-temporal
information) into the attack design. In particular, we target a
previously under-defended attack surface: neural-network-based
POI recommendation. To the best of our knowledge, our work is
the first to comprehensively evaluate the privacy risks in POI rec-
ommendation models using inference attacks from both location
and trajectory levels.
•We conduct extensive experiments on state-of-the-art POI rec-
ommendation models and datasets to demonstrate that POI rec-
ommendation models are vulnerable to data extraction and mem-
bership inference attacks in our attack suite.
•We provide an in-depth analysis to understand what unique
factors in mobility data make them vulnerable to privacy attacks.
We also explore the reason regarding how our attack design
works and test existing defenses against our attacks. Our analysis
identifies the challenges and future directions for developing
privacy-preserving POI recommendation models.
2 BACKGROUND
2.1 Point-of-Interest Recommendation
POI recommendation has recently gained much attention due to
its importance in many business applications [ 27], such as user
experience personalization and resource optimization. Initially, re-
searchers focused on feature engineering and algorithms such as
Markov chain [ 10,71], matrix factorization algorithms [ 11,36], and
Bayesian personalized ranking [ 25,72] for POI recommendation.
However, more recent studies have shifted their attention towards
employing neural networks like RNN [ 37,67], LSTM [ 31,58], and
self-attention models [ 35,39]. Neural networks can better learn
from spatial-temporal correlation in mobility data (e.g., check-ins)
to predict users’ future locations and thus outperform other POI rec-
ommendation algorithms by a large margin. Meanwhile, this could
introduce potential privacy leakage. Thus, we aim to design an
attack suite to measure the privacy risks of neural-network-based
POI recommendations systematically.
We first provide the basics of POI recommendations and no-
tations used throughout this paper. Let Ube the user space, L
be the location space, and Tbe the timestamp space. A POI rec-
ommendation model takes the observed trajectory of a user as
input and predicts the next POI that will be visited, which is for-
mulated as 𝑓𝜃:U×L𝑛×T𝑛→R|L|. Here, the length of the
input trajectory is 𝑛. We denote a user by its user ID 𝑢∈U for
simplicity. For an input trajectory with 𝑛check-ins, we denote its
trajectory sequence as𝑥0:𝑛−1
𝑇={(𝑙0,𝑡0),...,(𝑙𝑛−1,𝑡𝑛−1)}, where
𝑙𝑖∈L and𝑡𝑖∈T indicate the POI location and corresponding
time interval of 𝑖-th check-in. Also, the location sequence of this
trajectory is denoted as 𝑥0:𝑛−1
𝐿={𝑙0,...,𝑙𝑛−1}. The POI recommen-
dation model predicts the next location 𝑙𝑛(also denoted as 𝑦by
convention) by outputting the logits of all the POIs. Then, the user
can select the POI with the highest logit as its prediction ˆ𝑦, where
ˆ𝑦=arg max𝑓𝜃(𝑢,𝑥0:𝑛−1
𝑇). Given the training set 𝐷trsampled from
2Our code is publicly available at: https://github.com/KunlinChoi/POIPrivacy
 
176Where Have You Been? A Study of Privacy Risk for Point-of-Interest Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: A summary of the threat model.
Attack Adversary Objective Adversary Knowledge
LocExtractExtract the most frequently visited location 𝑙of
a target user𝑢–
TrajExtractExtract the location sequence of a target user 𝑢
with length𝑛:𝑥𝐿={𝑙0,...,𝑙 𝑛−1}Starting location 𝑙0
LocMIAInfer the membership of a user-location
pair (𝑢,𝑙)Shadow dataset 𝐷s
TrajMIAInfer the membership of a trajectory sequence
𝑥𝑇={(𝑙0,𝑡0),...,(𝑙𝑛,𝑡𝑛)}Shadow dataset 𝐷s
an underlying distribution D, the model weights are optimized
to minimize the prediction loss on the overall training data, i.e.,
min𝜃1
|𝐷tr|Í
(𝑢,𝑥0:𝑛−1
𝑇,𝑦)∈𝐷trℓ(𝑓𝜃(𝑢,𝑥0:𝑛−1
𝑇),𝑦),whereℓis the cross-
entropy loss, i.e., ℓ(𝑓𝜃(𝑢,𝑥0:𝑛−1
𝑇),𝑦)=−log(𝑓𝜃(𝑢,𝑥0:𝑛−1
𝑇))𝑦. The
goal of the training process is to maximize the performance of the
model on the unseen test dataset 𝐷𝑡𝑒∈D, which is drawn from
the same distribution as the training data. During inference, this
prediction ˆ𝑦is then compared to the next real location label 𝑙𝑛to
compute the prediction accuracy. The performance evaluation of
POI recommendation models typically employs metrics such as
top-𝑘accuracy (e.g., 𝑘=1,5,10).
2.2 Threat Models
Adversary Objectives. To understand the potential privacy leak-
age of training data in POI recommendation models, we design
the following four attacks from the two most common privacy
attack families: membership inference attack [ 53] and data ex-
traction attacks [ 9], based on the characteristics of the mobility
data for POI recommendation, namely common location extrac-
tion(LocExtract), training trajectory extraction (TrajExtract),
location-level membership inference attack (LocMIA), and trajectory-
level membership inference attack (TrajMIA). These four attacks
aim to extract or infer different sensitive information about a user
in the POI recommendation model training data.
LocExtract focuses on extracting a user’s most frequently vis-
ited location; TrajExtract extracts a user’s location sequence with
a certain length given a starting location; LocMIA infers whether
a user has been to a location and used for training; TrajMIA in-
fers where a trajectory sequence has been used for training. The
summary of the threat model is outlined in Table 1.
Adversary Knowledge. For all attacks, we assume the attacker
has access to the query interface of the victim model. Specifically,
the attacker can query the victim model with the target user and
obtain the corresponding output logits. This assumption is realis-
tic in two scenarios: (1) A malicious third-party entity is granted
access to the POI model query API hosted by the model owner
(e.g., location service providers like Foursquare or Yelp) for spe-
cific businesses such as personalized advertisement. This scenario
is well-recognized by [ 42,55,66]. (2) The retention period of the
training data expires. Still, the model owner keeps the model and
an adversary (e.g., a malicious insider of location service providers)
can extract or infer the sensitive information using our attack suite,
even if the training data have been deleted. In this scenario, the
model owner may violate privacy regulations such as GDPR [15].
Depending on different attack objectives, the adversary also
possesses different auxiliary knowledge. In particular, for TrajEx-
tract, we assume the attacker can query the victim model witha starting location 𝑙0that the target user visited. This assump-
tion is reasonable because an attacker can use real-world obser-
vation [56, 60], LocExtract, and LocMIA as cornerstones. As for
LocMIA andTrajMIA, we assume the attacker has access to a
shadow dataset following the standard settings of membership
inference attacks [6, 52].
3 ATTACK SUITE
Our attack suite is used to evaluate privacy vulnerabilities of POI
recommendation models at both location and trajectory levels. The
subsequent sections detail the technical approaches and design of
attacks, taking into account the unique aspects of mobility data.
3.1 Data Extraction Attacks
Our data extraction attacks are rooted in the idea that victim mod-
els display varying levels of memorization in different subsets of
training data. By manipulating the spatial-temporal information in
the queries, the attacker can extract users’ locations or trajectories
that these victim models predominantly memorize.
LocExtract. Common location extraction attack ( LocExtract )
aims to extract a user’s most frequently visited location in the victim
model training, i.e.,
LocExtract(𝑓𝜃,𝑢)→ ˆ𝑙𝑡𝑜𝑝1,..., ˆ𝑙𝑡𝑜𝑝𝑘.
The attack takes the victim model 𝑓𝜃and the target user 𝑢as the
inputs and generates 𝑘predictions ˆ𝑙𝑡𝑜𝑝1,..., ˆ𝑙𝑡𝑜𝑝𝑘 to extract the
most frequently visited location of user 𝑢. The attack is motivated
by our key observation: querying POI recommendation models
with a random location reveals that these models tend to “over-
learn” a user’s most frequently visited locations, making these
locations more likely to appear in the model output. For example,
we randomly choose 10 users and query the victim model using
100 randomly selected locations. Of these queries, 32.5% yield the
most frequent location for the target user. Yet, these most common
locations are present in only 18.7% of these users’ datasets.
InLocExtract, we first generate a set of different random inputs
for a specific user and use them to make iterative queries to the
victim model. Each query returns the prediction logits with a length
of|L|outputted by the victim model. The larger the logit value,
the more confident the model is in predicting the corresponding
location as the next POI. Therefore, by iterating queries to the model
given a target user and aggregating the logit values of all queries,
the most visited location is more likely to have a large logit value
after aggregation. In particular, we use a soft voting mechanism,
i.e., averaging the logits of all the queries. With the resulting mean
logits, we output the top- 𝑘locations with 𝑘largest logit values as
the attack results. Algorithm 1 outlines LocExtract. Though the
attack is straightforward, it is effective and can be a stepping stone
forTrajExtract in our attack suite.
TrajExtract. Our training trajectory extraction attack
(TrajExtract ) aims to extract the location sequence 𝑥0:𝑛−1
𝐿=
{𝑙0,...,𝑙𝑛−1}in a training trajectory of user 𝑢with a length of
𝑛from the victim model 𝑓𝜃. Formally,
TrajExtract(𝑓𝜃,𝑢,𝑙 0,𝑛)→ ˆ𝑥0:𝑛−1
𝐿0,..., ˆ𝑥0:𝑛−1
𝐿𝛽,
 
177KDD ’24, August 25–29, 2024, Barcelona, Spain Kunlin Cai et al.
Algorithm 1 Common Location Extraction Attack
Input: Victim model: 𝑓𝜃, target user: 𝑢, query budget: 𝑞, query
timestamp:𝑡, output size: 𝑘
Output: Top-𝑘predictions:[ˆ𝑙𝑡𝑜𝑝1,. . . ,ˆ𝑙𝑡𝑜𝑝𝑘]
1:logits←{}
2:for𝑞times do
3:𝑙←RandomSample (L)⊲Randomly generate a location
from the location space
4: logits∪𝑓𝜃 𝑢,{(𝑙,𝑡)}
5:end for
6:logits agg=Aggregate(logits) ⊲Aggregate confidence for all
locations
7:return ˆ𝑙𝑡𝑜𝑝1,. . . ,ˆ𝑙𝑡𝑜𝑝𝑘←Argmax𝑘(logits agg)
where ˆ𝑥0:𝑛−1
𝐿0,..., ˆ𝑥0:𝑛−1
𝐿𝛽indicate the top- 𝛽extracted location se-
quences by the attack.
The key idea of the training trajectory extraction attack is to
identify the location sequence with the lowest log perplexity, as
models tend to demonstrate lower log perplexity when they see
trained data. We denote log perplexity as:
PPL𝑓𝜃(𝑢,𝑥0:𝑛−1
𝑇)=−log Pr𝑓𝜃(𝑢,𝑥0:𝑛−1
𝑇)=−𝑛−1∑︁
𝑖=0log Pr𝑓𝜃(𝑢,𝑥0:𝑖−1
𝑇),
where Pr𝑓𝜃(·)is the likelihood of observing 𝑥0:𝑛−1
𝑇with user𝑢un-
der the victim model 𝑓𝜃. In order to get the lowest log perplexity
of location sequences with a length of 𝑛, we have to enumerate all
possible location sequences. However, in the context of POI recom-
mendation, there are O(|L|𝑛−1)possible location sequences for a
given user.|L|equals the number of unique POIs within the mobil-
ity dataset and can include thousands of options. Thus, the cost of
calculating the log perplexity of all location sequences can be very
high. To this end, we use beam search to extract the location se-
quences with both time and space complexity O(|L|×𝑛×𝛽), where
𝛽is the beam size. In particular, to extract a trajectory of length
𝑛, we iteratively query the victim model using a set of candidate
trajectories with a size of 𝛽and update the candidate trajectories
until the extraction finishes. As highlighted in the prior work [ 16],
when using beam search to determine the final outcome of a se-
quential neural network, there is a risk of generating non-diverse
outputs and resembling the training data sequence. However, in
our scenario, this property can be leveraged as an advantage in
TrajExtract, as our primary objective revolves around extracting
the training location sequence with higher confidence. As a final
remark, both LocExtract andTrajExtract need a query times-
tamp to query the victim model, and we will show the effects of
the timestamp in our experiments. Algorithm 2 gives the detailed
steps of TrajExtract.
3.2 Membership Inference Attacks
Membership inference attack ( MIA) aims to determine whether a
target data sample is used in the model training. We extend the
notion to infer whether certain sensitive information (e.g., user-
location pair(𝑢,𝑙)and trajectory sequence (𝑢,𝑥𝑇)) of the user’s
data is involved in the training of the victim model 𝑓𝜃. Since POIAlgorithm 2 Training Trajectory Extraction Attack
Input: Victim model: 𝑓𝜃, target user: 𝑢, starting location: 𝑙0, target
extraction length: 𝑛, query timestamp: 𝑡, beam width: 𝛽
Output: Top-𝛽possible extraction results: ˆ𝑥0:𝑛
𝐿0,..., ˆ𝑥0:𝑛
𝐿𝛽
1:for𝑏←0to𝛽−1do
2: ˆ𝑥0:0
𝑇𝑏←(𝑢,(𝑙0,𝑡))⊲Initialize the beam with 𝑙0and𝑡
3:end for
4:for𝑖←1to𝑛−1do
5: forˆ𝑥0:𝑖−1
𝑇in{ˆ𝑥0:𝑖−1
𝑇0,..., ˆ𝑥0:𝑖−1
𝑇𝛽}do
6:{ˆ𝑥0:𝑖
𝑇0,..., ˆ𝑥0:𝑖
𝑇𝛽}← UpdateBeam 𝛽(𝑓𝜃(𝑢,ˆ𝑥0:𝑖−1
𝑇))⊲Up-
date the beam by keeping 𝛽trajectory with the smallest PPL from
the query output and current beam
7: end for
8:end for
9:ˆ𝑥0:𝑛−1
𝐿0,..., ˆ𝑥0:𝑛−1
𝐿𝛽←Getloc(ˆ𝑥0:𝑛−1
𝑇0,..., ˆ𝑥0:𝑛−1
𝑇𝛽)⊲Take the
location sequence from ˆ𝑥0:𝑛−1
𝑇as result ˆ𝑥0:𝑛−1
𝐿
10:return ˆ𝑥0:𝑛−1
𝐿0,..., ˆ𝑥0:𝑛−1
𝐿𝛽
recommendation models use multi-modal sequential data as inputs
and adversaries lack sufficient information to construct a complete
input, we propose attack designs to manipulate the spatial-temporal
information in queries to enhance effectiveness of attacks. The
membership inference attack can be formulated as follow:
MIA(𝑓𝜃,𝑋𝑡𝑎𝑟𝑔𝑒𝑡,𝐷𝑠)→{ member,nonmember},
where𝑋𝑡𝑎𝑟𝑔𝑒𝑡 represents the target sensitive information ( 𝑋𝑡𝑎𝑟𝑔𝑒𝑡 =
(𝑢,𝑙)inLocMIA and𝑋𝑡𝑎𝑟𝑔𝑒𝑡 =(𝑢,𝑥𝑇)inTrajMIA), and 𝐷𝑠is the
shadow dataset owned by the adversary.
To effectively infer the membership of a given 𝑋𝑡𝑎𝑟𝑔𝑒𝑡 , we adapt
the state-of-the-art membership inference attack – likelihood ratio
attack (LiRA) [ 6] to the context of POI recommendation. The key
insight of LiRA is that the model parameters trained with 𝑋𝑡𝑎𝑟𝑔𝑒𝑡
differ from those trained without it, and the effect of the model
parameter on a data sample can be well approximated using a loss
value. By conducting a hypothesis test on the distributions of the
loss values, we can identify if the victim model is trained with
the𝑋𝑡𝑎𝑟𝑔𝑒𝑡 or not. LiRA consists of four steps: (1) train multiple
shadow models, (2) query the shadow models trained with 𝑋𝑡𝑎𝑟𝑔𝑒𝑡
and without 𝑋𝑡𝑎𝑟𝑔𝑒𝑡 to obtain two distributions, (3) query the vic-
tim model𝑋𝑡𝑎𝑟𝑔𝑒𝑡 to obtain the output logits, and (4) conduct a Λ
hypothesis test to infer the membership of the 𝑋𝑡𝑎𝑟𝑔𝑒𝑡 based on the
two distributions and the query results.
LocMIA. In this attack, the adversary aims to determine whether
a given user 𝑢has visited a location 𝑙in the training data. However,
it is not feasible to directly apply LiRA to LocMIA as the victim
model takes the trajectory sequences as inputs, but the adversary
only has a target location without the needed sequential context.
In particular, LocMIA needs the auxiliary inputs to calculate the
membership confidence score since this process cannot be com-
pleted only using 𝑋𝑡𝑎𝑟𝑔𝑒𝑡 =(𝑢,𝑙). This attack is a stark contrast to
MIA for image/text classification tasks where the 𝑋𝑡𝑎𝑟𝑔𝑒𝑡 itself is
sufficient to compute the membership confidence score.
To this end, we design a spatial-temporal model query algorithm
(Algorithm 3) to tailor LiRA to LocMIA and optimize membership
 
178Where Have You Been? A Study of Privacy Risk for Point-of-Interest Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 3 SpaTemQuery: Spatial-Temporal Model Query Algo-
rithm for LocMIA
Input: Target model: 𝑓𝑡𝑎𝑟𝑔𝑒𝑡 , number of query timestamps: 𝑛𝑡,
number of query locations: 𝑛𝑙, target example: 𝑋𝑡𝑎𝑟𝑔𝑒𝑡
Output: Membership confidence score: 𝑐𝑜𝑛𝑓
1:𝑢,𝑙←𝑋𝑡𝑎𝑟𝑔𝑒𝑡
2:𝑐𝑜𝑛𝑓𝑎𝑙𝑙←{}
3:for𝑖←0to𝑛𝑡−1do
4:𝑐𝑜𝑛𝑓𝑡←{}
5: for𝑗←0to𝑛𝑙−1do
6:𝑡𝑖←𝑖/𝑛𝑡
7:𝑙𝑗←RandomSample(L)
8:𝑐𝑜𝑛𝑓𝑡←𝑐𝑜𝑛𝑓𝑡∪𝑓𝑡𝑎𝑟𝑔𝑒𝑡(𝑢,(𝑙𝑗,𝑡𝑖))⊲Query the model
with random location and a synthetic timestamp
9: end for
10:𝑐𝑜𝑛𝑓𝑎𝑙𝑙←𝑐𝑜𝑛𝑓𝑎𝑙𝑙∪mean(𝑐𝑜𝑛𝑓𝑡)⊲Calculate average confi-
dence from all queries for this timestamp
11:end for
12:return𝑐𝑜𝑛𝑓←max(𝑐𝑜𝑛𝑓𝑎𝑙𝑙)⊲Take the confidence scores with
largest confidence at position 𝑙as output
confidence score calculation. The idea behind the algorithm is that
if a particular user has been to a certain POI location, the model
might “unintentionally” memorize its neighboring POI locations
and the corresponding timestamp in the training data. Motivated
by this, each time we query the models (e.g., the victim and shadow
models), we generate 𝑛𝑙random locations and 𝑛𝑡fixed-interval
timestamps. To obtain stable and precise membership confidence
scores, we first average the corresponding confidence scores at the
target location by querying with 𝑛𝑙locations at the same timestamp.
While the adversary does not possess the ground truth timestamp
linked with the target POI for queries, the adversary aims to mimic
a query close to the real training data. To achieve this, we repeat the
same procedure of querying different locations for 𝑛𝑡timestamps
and take the maximum confidence scores among the 𝑛𝑡averaged
confidence scores as the final membership inference score for the
target example. Algorithm 4 gives the outline of LiRA in terms of
LocMIA, and the lines marked with red are specific to LocMIA.
TrajMIA. The attack aims to determine whether a trajectory is
used in the training data of the victim model. Unlike LocMIA,
𝑋𝑡𝑎𝑟𝑔𝑒𝑡 =(𝑢,𝑥𝑇)suffices to calculate the membership confidence
score in LiRA, and we do not need any auxiliary inputs. To fully
leverage information of the target example querying the victim
model and improve the attack performance, we also utilize the 𝑛−2
intermediate outputs and the final output from the sequence 𝑥𝑇
with a length of 𝑛to compute the membership confidence score, i.e.,
we take the average of all 𝑛−1outputs. This change improves the
attack performance as the intermediate outputs provide additional
membership information for each point in the target trajectory. The
purple lines in Algorithm 4 highlight steps specific to TrajMIA.
3.3 Practical Implications of the Attack Suite
Our attack suite is designed as an integrated framework focusing
on the basic units of mobility data – locations and trajectories. It
contains two prevalent types of privacy attacks: data extraction
and membership inference attacks. Each attack in our attack suiteAlgorithm 4 Membership Inference Attack
Below, we demonstrate our location-level MIA and trajectory-level
MIA algorithms. The lines marked in redare specific to LocMIA,
while the lines marked in purple are specific to TrajMIA. Both
attacks share the remaining lines.
Input: Victim model: 𝑓𝜃, shadow data: 𝐷𝑠, number of shadow mod-
els:𝑁, inference target: 𝑋𝑡𝑎𝑟𝑔𝑒𝑡 ,number of query timestamps:
𝑛𝑡, number of query locations: 𝑛𝑙
Output: The likelihood ratio to determine if we should reject the
hypothesis that 𝑋𝑡𝑎𝑟𝑔𝑒𝑡 is a member of 𝑓𝜃:Λ
1:𝑐𝑜𝑛𝑓𝑖𝑛,𝑐𝑜𝑛𝑓𝑜𝑢𝑡←{},{}
2:𝑋𝑆←RandomSample({𝑋𝑆:𝑋𝑡𝑎𝑟𝑔𝑒𝑡∈𝑋𝑆})⊲Sample a loca-
tion sequence and includes 𝑋𝑡𝑎𝑟𝑔𝑒𝑡
3:𝑋𝑆←𝑋𝑡𝑎𝑟𝑔𝑒𝑡
4:for𝑖←0to𝑁do
5:𝐷𝑖𝑛←RandomSample(𝐷𝑠)∪𝑋𝑆
6:𝐷𝑜𝑢𝑡←RandomSample(𝐷𝑠)\𝑋𝑆
7:𝑓𝑖𝑛,𝑓𝑜𝑢𝑡←Train(𝐷𝑖𝑛),Train(𝐷𝑜𝑢𝑡)⊲Train𝑓𝑖𝑛and𝑓𝑜𝑢𝑡
8:𝑐𝑜𝑛𝑓𝑖𝑛←𝑐𝑜𝑛𝑓𝑖𝑛∪𝜙(SpaTemQuery(𝑓𝑖𝑛,𝑛𝑡,𝑛𝑙,𝑋𝑡𝑎𝑟𝑔𝑒𝑡))
9:𝑐𝑜𝑛𝑓𝑜𝑢𝑡←𝑐𝑜𝑛𝑓𝑜𝑢𝑡∪
10: 𝜙(SpaTemQuery(𝑓𝑜𝑢𝑡,𝑛𝑡,𝑛𝑙,𝑋𝑡𝑎𝑟𝑔𝑒𝑡))
11:𝑐𝑜𝑛𝑓𝑖𝑛←𝑐𝑜𝑛𝑓𝑖𝑛∪
12: 𝜙 mean({𝑓𝑖𝑛(𝑋𝑆)0:0,...,𝑓𝑖𝑛(𝑋𝑆)0:𝑛−1})
13:𝑐𝑜𝑛𝑓𝑜𝑢𝑡←𝑐𝑜𝑛𝑓𝑜𝑢𝑡∪
14: 𝜙 mean({𝑓𝑜𝑢𝑡(𝑋𝑆)0:0,...,𝑓𝑜𝑢𝑡(𝑋𝑆)0:𝑛−1})
15:end for
16:𝜇in,𝜇out←mean(𝑐𝑜𝑛𝑓 in), mean(𝑐𝑜𝑛𝑓 out)
17:𝜎2
in,𝜎2
out←var(𝑐𝑜𝑛𝑓 in),var(𝑐𝑜𝑛𝑓 out)
18:𝑐𝑜𝑛𝑓 obs←𝜙(SpaTemQuery(𝑓𝜃,𝑛𝑡,𝑛𝑙,𝑋𝑡𝑎𝑟𝑔𝑒𝑡))
19:𝑐𝑜𝑛𝑓 obs←𝜙 mean({𝑓𝜃(𝑋𝑆)0:0,...,𝑓𝜃(𝑋𝑆)0:𝑛−1})
20:return Λ=𝑝(𝑐𝑜𝑛𝑓 obs|N(𝜇in,𝜎2
in))
𝑝(𝑐𝑜𝑛𝑓 obs|N(𝜇out,𝜎2
out))⊲Hypothesis test
targets a specific type of mobility data and could serve as a privacy
auditing tool [ 28]. They can also be used to infer additional sensitive
information in mobility data:
•LocExtract extracts a user’s most common location. Combined
with the semantics of the POI, we may infer the user’s address
such as work address, which is closely related to user identity;
•TrajExtract can be further used to infer user trajectories and
identify trip purposes by analyzing the POIs visited during a
journey [40];
•LocMIA can determine the membership of multiple POIs, thereby
facilitating the inference of a user’s activity range and social
connections in Cho et al. [12], Ren et al. [48];
•TrajMIA infers if a user’s trajectory is in the training dataset,
which can serve as an auditing tool to examine the privacy leak-
age by assuming a worst-case adversary.
4 EXPERIMENTS
We empirically evaluate the proposed attack suite to answer the
following research questions: (1) What’s the performance of the
proposed attacks in extracting or inferring the sensitive information
from POI recommendation models (Sec. 4.2.1)? (2) What unique
factors (e.g., user, location, trajectory) in mobility data correlate
 
179KDD ’24, August 25–29, 2024, Barcelona, Spain Kunlin Cai et al.
GETNext LSTPM RNN020406080100Topk-ASR (%)33.053.1
26.357.377.9
42.864.980.9
48.6k= 1
k= 3
k= 5
(a)LocExtract (4sq)
GETNext LSTPM RNN020406080100Topk-ASR (%)26.942.9
3.241.648.2
6.148.154.5
8.9k= 1
k= 3
k= 5 (b)LocExtract (Gowalla)
GETNext LSTPM RNN0510152025Topk-ASR (%)7.7
2.55.915.7
5.911.420.1
8.314.8k= 1
k= 3
k= 5 (c)TrajExtract (4sq)
GETNext LSTPM RNN012345Topk-ASR (%)
0.90.9
0.22.33.0
0.53.33.5
0.6k= 1
k= 3
k= 5 (d)TrajExtract (Gowalla)
Figure 2: Main results of data extraction attacks (LocExtract andTrajExtract) on three victim models and two datasets.
(a)LocMIA (4sq)
 (b)LocMIA (Gowalla)
 (c)TrajMIA (4sq)
 (d)TrajMIA (Gowalla)
Figure 3: Main results of membership inference attacks (LocMIA andTrajMIA) on three victim models and two mobility
datasets. The diagonal line indicates the random guess baseline.
Table 2: The performance of victim models.
Dataset Model Top-1 ACC Top-10 ACC
4sqGETNext 0.34 0.71
LSTPM 0.25 0.67
RNN 0.24 0.68
GowallaGETNext 0.16 0.48
LSTPM 0.15 0.39
RNN 0.10 0.26
with the attack performance (Sec. 4.2.2)? (3) How do different attack
designs (e.g., spatial-temporal querying for membership inference
attacks) improve the attack performance (Sec. 4.2.3)?
4.1 Experimental Setup
We briefly describe the datasets, models, and evaluation metrics
used in our experiments. Due to the space limit, we defer the details
of datasets (e.g., statistics of each dataset), data pre-processing
pipeline, default training and attack parameters to Appendix A.
Datasets. Following the literature [31, 69], we comprehensively
evaluate four privacy attacks on two POI recommendation bench-
marks: FourSquare (4sq) [ 68] and Gowalla [12]. We use the check-
ins collected in NYC for both sources.
Models. We experiment with three representative POI recom-
mendation models, including GETNext3[69],LSTPM4[58], and
RNN [63]. Note that GETNext andLSTPM are the state-of-the-
art POI recommendation methods based on the transformer and
hierarchical LSTM, respectively.
Evaluation Metrics. We use the top- 𝑘extraction attack suc-
cess rate (ASR) to evaluate the effectiveness of data extraction at-
tacks. For LocExtract, the top- 𝑘ASR is defined as|𝑈extracted|/|U| ,
where𝑈extracted is the set of users whose most visited locations are
3https://github.com/songyangme/GETNext
4https://github.com/NLPWM-WHU/LSTPMin the top-𝑘predictions outputted by our attack; For TrajExtract
the top-𝑘ASR is|correct extractions |/|all(𝑢,𝑙0)pairs|, where cor-
rect extractions are (𝑢,𝑙0)pairs with top- 𝑘extracted results match-
ing an exact location sequence in the training data.
ForLocMIA andTrajMIA, we utilize the commonly employed
metrics for evaluating membership inference attacks, namely the
area under the curve (AUC), average-case “accuracy” (ACC), and
true positive rate (TPR) versus false positive rate (FPR) in the low-
false positive rate regime. Our primary focus is the TPR versus
FPR metric in the low-false positive rate regime because evaluat-
ing membership inference attacks should prioritize the worst-case
privacy setting rather than average-case, as emphasized in [6].
4.2 Experimental Results and Analysis
4.2.1 Attack performance (RQ1). Figures 2 and 3 visualize the at-
tack performance of data extraction and membership inference
attacks, respectively. In Figure 2, we observe that LocExtract and
TrajExtract can effectively extract users’ most common locations
and trajectories across various model architectures and datasets as
the attack performance is significantly better than the random guess
baseline, i.e., 1/|L| (0.04%forLocExtract) and 1/|L|𝑛−1(10−8%
forTrajExtract). Likewise, as shown in Figure 3, LocMIA and
TrajMIA successfully determine the membership of a specific user-
location pair or trajectory, significantly outperforming the random
guess baseline (represented by the diagonal line in both figures).
The attack performance also demonstrates that trajectory-level
attacks are significantly more challenging than location-level at-
tacks, evident from the better performance of LocExtract and
LocMIA compared to TrajExtract andTrajMIA for data extrac-
tion and membership inference. We suspect this is because POI
recommendation models are primarily designed to predict a single
location. In contrast, our trajectory-level attacks aim to extract or
infer a trajectory encompassing multiple consecutive locations.
 
180Where Have You Been? A Study of Privacy Risk for Point-of-Interest Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
The attack performance also differs across different model ar-
chitectures and datasets. We see a general trend of privacy-utility
trade-off in POI recommendation models based on the model per-
formance of the victim model in Table 2: with better victim model
performance comes better attack performance. While this is a com-
mon trend, it might not hold in some cases. For example, the MIA
performance against RNN is sometimes better than GETNext and
LSTPM performances. This might be because GETNext andLSTPM
improve upon RNN by better leveraging spatial-temporal informa-
tion in the mobility datasets. However, the adversary cannot use
the exact spatial-temporal information in shadow model training
since the adversary cannot access that information. This result can
be inspiring in that even though spatial-temporal information can
effectively improve attack performance, victim models that better
utilize spatial-temporal information are still more resilient to MIAs.
Future studies should also consider this characteristic when de-
signing attacks or privacy-preserving POI recommendation models
with better privacy-utility trade-offs.
0 25 50 75 100
Percentile0.00.10.20.3ASR
(a)# Unique POIs (4sq)
0 25 50 75 100
Percentile0.000.020.04ASR (b)# Unique POIs (Gowalla)
Figure 4: How user-level aggregate statistics are related to
TrajExtract. Users who have fewer unique POIs are more
vulnerable to TrajExtract.
0 25 50 75 100
Percentile0100200300400 Λ
(a)# of Users Visited
0 25 50 75 100
Percentile0100200 Λ (b)# of Nearby Check-ins
Figure 5: How location-level aggregate statistics are related
toLocMIA. Locations visited by fewer different users or have
fewer surrounding check-ins are more vulnerable to LocMIA.
4.2.2 Factors in mobility data that make it vulnerable to the attacks
(RQ2). Prior research demonstrates that data outliers are the most
vulnerable examples to privacy attacks [ 6,59] in image and text
datasets. However, it is unclear whether the same conclusion holds
in mobility data and what makes mobility data as data outliers.
To this end, we investigate which factors of the mobility datasets
influence the attack’s efficacy. In particular, we collect aggregate
statistics of mobility data from three perspectives: user, location,
and trajectory. We analyze which factors in these three categories
make mobility data vulnerable to our attacks. We defer the details
of selecting the aggregate statistics and the list of selected aggregate
statistics in our study in Appendix A. Our findings are as follows:•ForLocExtract, we do not identify any meaningful pattern
correlated with its attack performance. We speculate that a user’s
most common location is not directly related to the aggregate
statistics we study.
•ForTrajExtract, our findings indicate that users who have vis-
ited fewer unique POIs are more vulnerable to this attack, as
referenced in Figure 4. This can be explained by the fact that
when users have fewer POIs, the model is less uncertain in pre-
dicting the next location due to the reduced number of possible
choices that the model memorizes.
•ForLocMIA, as shown in Figures 5(a) and 5(b), we find that
locations visited by fewer users or have fewer surrounding check-
insare more susceptible to LocMIA. We believe this is because
those locations shared with fewer users or surrounding check-ins
make them training data outliers.
•ForTrajMIA, users with fewer total check-ins (Figure 6(a)), unique
POIs (Figure 6(b)), and fewer or shorter trajectories (Figures 6(c)
and 6(d)) are more susceptible. In Figures 7(a) and 7(b), we also see
that trajectories intercepting less with others or with more check-ins
are more vulnerable to TrajMIA. We believe these user-level and
trajectory-level aggregate statistics make the target examples
data outliers.
In summary, we conclude that the effect of data outliers also exists in
privacy attacks against POI recommendations. In the context of POI
recommendation, the mobility data outliers could be characterized
from the perspectives of user, location, and trajectory. Different
attacks in our attack suite might be vulnerable to particular types
of data outliers, which are more unique and are vulnerable against
our attacks compared to other data.
4.2.3 The impacts of various attack designs (RQ3). We explore dif-
ferent attack designs that may affect the performance of the attack
suite on the 4sq dataset. We summarize the key findings as follows:
The optimal query timestamp leads to better attack perfor-
mance. The significance of timestamps to our attack aligns with
the fact that POI recommendation relies on temporal information
for making accurate predictions. From Figure 8, we find that the
extraction attacks (i.e.,LocExtract andTrajExtract) ASRs first
increase and then decrease as the query timestamp increases, which
peaks at 0.5 (the middle of the day). The reason is that most check-
ins occur during the daytime rather than at night.
ForLocExtract, from Figure 9(b), we observed that utilizing
more queries with different timestamps in our spatial-temporal model
query algorithm improves the results of inferring the membership
of a target user-location pair (𝑢,𝑙). Since the adversary lacks in-
formation about real input sequences before the target location 𝑙,
utilizing more queries helps to traverse the search space to approx-
imate the correct timestamp and promotes the attack performance.
A small number of queries is sufficient for our attacks. Fig-
ure 10 in the Appendix demonstrates that our data extraction at-
tacks (LocExtract andTrajExtract) remain effective even with
a limited number of queries. Specifically, a few queries (i.e., 𝑞=50)
and a small beam width (i.e., 𝛽=10) allow the attacker to achieve
high ASRs for LocExtract andTrajExtract, respectively. In other
words, our attacks are practical in real-world scenarios.
Additionally, Figure 9(a) indicates that LocMIA remains effective
even with a limited number of queries for different location choices
 
181KDD ’24, August 25–29, 2024, Barcelona, Spain Kunlin Cai et al.
0 25 50 75 100
Percentile0102030 Λ
(a)# of User Check-ins
0 25 50 75 100
Percentile01020 Λ (b)# of User POIs
0 25 50 75 100
Percentile0102030 Λ (c)# of User Trajectories
0 25 50 75 100
Percentile0102030 Λ (d)Avg User Traj Length
Figure 6: How user-level aggregate statistics are related to TrajMIA. x-axis: Percentile categorizes users/locations/trajectories
into different groups according to their feature values. y-axis: Λindicates the (averaged) likelihood ratio of training trajecto-
ries/locations being the member over non-member from the hypothesis test for each group, with a higher value indicating
the larger vulnerability. The users with fewer total check-ins, fewer unique POIs, and fewer or shorter trajectories are more
vulnerable to TrajMIA.
0 25 50 75 100
Percentile0510 Λ
(a)# of Intercepting Trajs
0 25 50 75 100
Percentile02468 Λ (b)# of POIs in Traj
Figure 7: How trajectory-level aggregate statistics are related
toTrajMIA. The trajectories with fewer intercepting trajec-
tories or fewer POIs are more vulnerable to TrajMIA.
0 0.25 0.5 0.75 1.0
Normalized Day Time020406080100Topk-ASR (%)
k= 1
k= 3
k= 5
(a)LocExtract
0 0.25 0.5 0.75 1.0
Normalized Day Time01020304050Topk-ASR (%)
k= 1
k= 3
k= 5 (b)TrajExtract
Figure 8: The optimal query timestamp can improve the per-
formance of LocExtract andTrajExtract.
even with a small 𝑛𝑙(i.e., number of query locations) per timestamp,
which yields better practicability of our attack.
A larger number of shadow models leads to better perfor-
mance of membership inference attacks. Figure 11 in the Ap-
pendix shows that a larger number of shadow models greatly
improves the performance of LocMIA andTrajMIA since more
shadow models provide more samples to approximate the real loss
distributions of in-samples and out-samples.
5 DEFENSE
We evaluate existing defenses against privacy attacks on machine
learning models. Due to the limited space, we illustrate the defense
mechanisms and the key findings in the main paper and defer
experimental details to Appendix B.
Defense Techniques. In particular, we evaluate two streams of
defense mechanisms on proposed attacks, including (1) standard
1 3 5 10
Number of Queries per Timestamp5060708090100Percent (%)
 AUC
ACC
TPR @ 10% FPR(a)Impact of𝑛𝑙onLocMIA
1 3 5 10 20
Number of Query Timestamps5060708090100Percent (%)
 AUC
ACC
TPR @ 10% FPR (b)Impact of𝑛𝑡onLocMIA
Figure 9: Both the number of query timestamps 𝑛𝑡and the
number of query locations 𝑛𝑙affect the performance of
LocMIA. We use 𝑛𝑡=10and𝑛𝑙=10in (a) and (b), respec-
tively. LocMIA is effective with a limited number of queries.
techniques to reduce overfitting (e.g., early stopping, 𝑙2regulariza-
tion) and (2) differential privacy-based defenses (e.g., DP-SGD [ 1])
for provable risk mitigation. The standard techniques reduce the vic-
tim model’s memorization to some degree, but they are insufficient
due to the lack of statistical guarantees. To fill this gap, differential
privacy [ 14] is also used to defend against our attacks, which can
theoretically limit the impact of a single data point on the model’s
performance.
Specifically, we first experiment with DP-SGD [ 1], the most rep-
resentative DP-based defense, to train differentially-private POI
recommendation models. The key idea of DP-SGD is to add Gauss-
ian noisesN(0,𝜎2𝐶2𝐼)to the clipped gradients 𝑔of the model
during its training process. 𝐶is a clipping threshold that bounds
the sensitivity of 𝑔by ensuring∥𝑔∥≤𝐶. To achieve(𝜖,𝛿)-DP, we
have𝜎=√︃
2 ln1.25
𝛿/𝜖. Despite that DP-SGD provides promising
defense performance on language tasks [ 17], we find that it can
substantially sacrifice the model’s utility on the POI recommenda-
tion task. Specifically, the top-10 accuracy is only 4.97% when the
mechanism satisfies (5,0.001)-DP, while the original top-10 accu-
racy without DP is 71%. The reason for this performance decrease
is that POI recommendation aims to make accurate user-level pre-
dictions within a large output space (i.e., >4,000possible POIs).
For different users, even the same location sequence may lead to
a different result, which means that the model needs to capture
user-specific behavior patterns from a relatively small user dataset.
As a result, the training is quite sensitive to the noises introduced
by DP-SGD, making it not applicable to POI recommendations.
 
182Where Have You Been? A Study of Privacy Risk for Point-of-Interest Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: The exposure of sensitive information in each attack.
Attack Sensitive Information
LocExtract Most common location of each user
TrajExtract Each location sequence/sub-sequence (𝑥𝐿)
LocMIA Each user-location pair (𝑢,𝑙)
TrajMIA Each trajectory sequence/sub-sequence (𝑥𝑇)
However, we argue that DP-SGD provides undifferentiated pro-
tection for all the mobility data, while for POI recommendation,
protecting more tailored sensitive information is more important.
For example, a defender may only care about whether a list of
check-ins about home addresses is protected or not. To this end, we
introduce the notion of selective DP [ 51] to relax DP and improve
the model’s utility-privacy trade-offs. Specifically, we apply the
state-of-the-art selective DP method JFT [ 50] to protect different
levels of sensitive information for each attack. The key idea of JFT
is to adopt a two-phase training process: in the phase-I training, JFT
redacts the sensitive information in the training dataset and opti-
mizes the model with a standard optimizer; in the phase-II training,
JFT applies DP-SGD to finetune the model on the original dataset
in a privacy-preserving manner. Due to the phase-I training, we
observe that the model’s utility is significantly promoted. In addi-
tion to JFT, we also apply Geo-Indistinguishability (Geo-Ind) [ 2]
to protect common locations in LocExtract. We note that Geo-
Ind is only applicable to LocExtract (but not LocMIA) because it
requires modifying the training data and is incompatible with the
notion of membership inference.
Takeaway Messages from the Defense. We evaluate different
defense mechanisms in terms of their performance in preventing
each attack from stealing the corresponding sensitive information.
Table 3 summarizes the exposure of sensitive information in each
attack. Besides, Appendix B illustrates our evaluation metrics, ex-
perimental setup, and the results. Recall that the check-ins within
a mobility dataset are not equally important. Therefore, in our ex-
periments, we comprehensively evaluate the defense mechanisms
from two perspectives. Specifically, we measure their performance
in(1)protecting all the sensitive information and (2)protecting a
targeted subset of sensitive information from being attacked. Fig-
ures 12 and 13 in the Appendix show that existing defenses provide
a certain degree of guarantee in mitigating privacy risks of ML-
based POI recommendations, especially for the targeted subset of
sensitive information. However, there is no such unified defense
that can successfully defend against all the proposed attacks within
a small utility drop. In other words, our exploration highlights the
need for more advanced defenses.
6 RELATED WORK
Mobility Data Privacy. Mobility data contain rich information
that can reveal individual privacy such as user identity. Previous
work utilizes side-channel attacks to extract sensitive information
about mobility data from LBS, including social relationships [ 44,
57], aggregated trajectories [ 45,46,70], trajectory history [ 22,33],
network packets [ 29,61] and location embeddings [ 13]. Despite
the focus of previous work, deep neural networks (DNN) built
on large volumes of mobility data have recently become state-
of-the-art backbones for LBS, opening a new surface for privacy
attacks. To the best of our knowledge, our work is the first of itskind to investigate the vulnerabilities of DNN models in leaking
sensitive information about mobility data using inference attacks.
Moreover, previous defenses [ 30,43,47,62,65] primarily focus on
data collection, aggregation, and publishing, which can not protect
DNN models built on POI data.
Privacy Attacks. Various types of privacy attacks, such as mem-
bership inference attacks [ 6,23,49,52], training data extraction
attacks [ 7,8], and model inversion attacks [ 19] have been proposed
to infer sensitive information from model training data. Our attack
suite contains membership inference and data extraction attacks.
Existing data extraction and membership inference attacks [ 6,8]
are insufficient for POI recommendation models due to the spatio-
temporal nature of the data. Our work takes the first step to ex-
tracting sensitive location and trajectory patterns from POI rec-
ommendation models and solving unique challenges to infer the
membership of both user-location pairs and user trajectories. As
a final remark, our attacks differ from previous MIAs in mobility
data [ 45,70], which focus on the privacy risks of data aggregation.
7 CONCLUSION
In this work, we take the first step to evaluate the privacy risks
of the POI recommendation models. In particular, we introduce
an attack suite containing data extraction attacks and member-
ship inference attacks to extract and infer sensitive information
about location and trajectory in mobility data. We conduct exten-
sive experiments to demonstrate the effectiveness of our attacks.
Additionally, we analyze what types of mobility data are vulnerable
to the proposed attacks. To mitigate our attacks, we further adapt
two mainstream defense mechanisms to the task of POI recom-
mendation. Our results show that there is no single solid defense
that can simultaneously defend against all proposed attacks. Our
findings underscore the urgent need for better privacy-preserving
approaches for POI recommendation models. Interesting future di-
rections include: (1) Generalize the attack suite to measure privacy
risks of real-world location-based services (e.g., Yelp/Google Maps)
in a more challenging setting (e.g., label-only setting); (2) Develop
more advanced defense mechanisms against our attacks.
Ethics Statement. This work introduces a novel attack suite on
POI recommendation models trained on public anonymized mobil-
ity datasets with no personally identifiable information, aimed at
bringing potential vulnerabilities in POI models to public attention.
The success of our attacks offers insights into future privacy leakage
measurement in learning-based models involving spatio-temporal
data. Moreover, it underscores the need for improved defense so-
lutions for POI models with better utility-privacy trade-offs. We
hope our study fosters further research in protecting the privacy of
mobility data.
ACKNOWLEDGMENTS
We sincerely thank the reviewers for their valuable feedback on the
paper. This work is supported in part by the National Science Foun-
dation (NSF) Awards 1951890, 1952096, 2003874, 2047822, 2317184,
2325369, 2411151, 2411152, 2411153, UCLA ITLP and Okawa foun-
dation. Any opinions, findings, conclusions or recommendations
expressed in this publication are those of the authors and do not
necessarily reflect the views of sponsors.
 
183KDD ’24, August 25–29, 2024, Barcelona, Spain Kunlin Cai et al.
REFERENCES
[1]Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. ACM
SIGSAC Conference on Computer and Communications Security.
[2]Miguel E Andrés, Nicolás E Bordenabe, Konstantinos Chatzikokolakis, and Catus-
cia Palamidessi. 2013. Geo-indistinguishability: Differential privacy for location-
based systems. ACM SIGSAC Conference on Computer and Communications
Security.
[3]Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive graph
convolutional recurrent network for traffic forecasting. International Conference
on Neural Information Processing Systems.
[4]Andrew J Blumberg and Peter Eckersley. 2009. On locational privacy, and how
to avoid losing it forever. Electronic frontier foundation 10, 11 (2009), 1–7.
[5]Nicolás E Bordenabe, Konstantinos Chatzikokolakis, and Catuscia Palamidessi.
2014. Optimal geo-indistinguishable mechanisms for location privacy. ACM
SIGSAC Conference on Computer and Communications Security.
[6]Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and
Florian Tramer. 2022. Membership inference attacks from first principles. IEEE
Symposium on Security and Privacy.
[7]Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,
Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. 2023. Extracting
training data from diffusion models. 32nd USENIX Security Symposium (USENIX
Security 23), 5253–5270.
[8]Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019.
The Secret Sharer: Evaluating and testing unintended memorization in neural
networks. USENIX Security Symposium.
[9]Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-
Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,
et al.2021. Extracting training data from large language models. 30th USENIX
Security Symposium (USENIX Security 21), 2633–2650.
[10] Meng Chen, Yang Liu, and Xiaohui Yu. 2014. Nlpmm: A next location predictor
with markov modeling. Advances in Knowledge Discovery and Data Mining:
Pacific-Asia Conference.
[11] Chen Cheng, Haiqin Yang, Michael R Lyu, and Irwin King. 2013. Where you
like to go next: Successive point-of-interest recommendation. International Joint
Conference on Artificial Intelligence.
[12] Eunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility:
user movement in location-based social networks. ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining.
[13] Jiaxin Ding, Shichuan Xi, Kailong Wu, Pan Liu, Xinbing Wang, and Chenghu Zhou.
2022. Analyzing sensitive information leakage in trajectory embedding models.
International Conference on Advances in Geographic Information Systems.
[14] Cynthia Dwork, Aaron Roth, et al .2014. The algorithmic foundations of differ-
ential privacy. (2014).
[15] EU. 2018. General data protection regulation. https://en.wikipedia.org/wiki/
General_Data_Protection_Regulation.
[16] Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story
generation. (2018).
[17] Natasha Fernandes, Mark Dras, and Annabelle McIver. 2019. Generalised differ-
ential privacy for text document processing. Springer International Publishing,
Principles of Security and Trust: 8th International Conference, POST 2019, Held
as Part of the European Joint Conferences on Theory and Practice of Software,
ETAPS 2019, Prague, Czech Republic, April 6–11, 2019, Proceedings 8, 123–148.
[18] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. ACM
SIGSAC Conference on Computer and Communications Security.
[19] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion At-
tacks That Exploit Confidence Information and Basic Countermeasures. Proceed-
ings of the 22nd ACM SIGSAC Conference on Computer and Communications Se-
curity, New York, NY, USA, 1322–1333. https://doi.org/10.1145/2810103.2813677
[20] Sébastien Gambs, Marc-Olivier Killijian, and Miguel Núñez del Prado Cortez.
2014. De-anonymization attack on geolocated data. (2014).
[21] Bugra Gedik and Ling Liu. 2005. Location privacy in mobile systems: A per-
sonalized anonymization model. IEEE International Conference on Distributed
Computing Systems.
[22] Philippe Golle and Kurt Partridge. 2009. On the anonymity of home/work location
pairs. Springer, Pervasive Computing: 7th International Conference, Pervasive
2009, Nara, Japan, May 11-14, 2009. Proceedings 7, 390–397.
[23] Maziar Gomrokchi, Susan Amin, Hossein Aboutalebi, Alexander Wong, and
Doina Precup. 2023. Membership Inference Attacks Against Temporally Corre-
lated Data in Deep Reinforcement Learning. IEEE Access (2023).
[24] Wajih Ul Hassan, Saad Hussain, and Adam Bates. 2018. Analysis of privacy
protections in fitness tracking social networks-or-you can run, but can you hide?
USENIX Security Symposium.
[25] Jing He, Xin Li, and Lejian Liao. 2017. Category-aware next point-of-interest
recommendation via listwise bayesian personalized ranking. International Joint
Conference on Artificial Intelligence.[26] Benjamin Henne, Christian Szongott, and Matthew Smith. 2013. SnapMe if you
can: Privacy threats of other peoples’ geo-tagged media and what we can do about
it. ACM Conference on Security and Privacy in Wireless and Mobile Networks.
[27] Md. Ashraful Islam, Mir Mahathir Mohammad, Sarkar Snigdha Sarathi Das, and
Mohammed Eunus Ali. 2020. A survey on deep learning based Point-Of-Interest
(POI) recommendations. arXiv:cs.IR/2011.10187
[28] Matthew Jagielski, Jonathan Ullman, and Alina Oprea. 2020. Auditing differen-
tially private machine learning: How private is private SGD? (2020).
[29] Tao Jiang, Helen J Wang, and Yih-Chun Hu. 2007. Preserving location privacy in
wireless LANs. International Conference on Mobile Systems, Applications and
Services.
[30] Yuzhou Jiang, Emre Yilmaz, and Erman Ayday. 2023. Robust Fingerprint of
Privacy-Preserving Location Trajectories. Proceedings on Privacy Enhancing
Technologies (2023).
[31] Dejiang Kong and Fei Wu. 2018. HST-LSTM: A hierarchical spatial-temporal
long-short term memory network for location prediction. International Joint
Conference on Artificial Intelligence.
[32] John Krumm. 2007. Inference attacks on location tracks. Springer, Pervasive
Computing: 5th International Conference, PERVASIVE 2007, Toronto, Canada,
May 13-16, 2007. Proceedings 5, 127–143.
[33] John Krumm. 2007. Inference attacks on location tracks. Springer, Pervasive
Computing: 5th International Conference, PERVASIVE 2007, Toronto, Canada,
May 13-16, 2007. Proceedings 5, 127–143.
[34] Shiyong Lan, Yitong Ma, Weikang Huang, Wenwu Wang, Hongyu Yang, and
Pyang Li. 2022. Dstagnn: Dynamic spatial-temporal aware graph neural network
for traffic flow forecasting. International Conference on Machine Learning.
[35] Defu Lian, Yongji Wu, Yong Ge, Xing Xie, and Enhong Chen. 2020. Geography-
aware sequential location recommendation. ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining.
[36] Defu Lian, Cong Zhao, Xing Xie, Guangzhong Sun, Enhong Chen, and Yong
Rui. 2014. GeoMF: Joint geographical modeling and matrix factorization for
point-of-interest recommendation. ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.
[37] Q. Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. Predicting the next location:
A recurrent model with spatial and temporal contexts. AAAI Conference on
Artificial Intelligence.
[38] Yanchi Liu, Chuanren Liu, Xinjiang Lu, Mingfei Teng, Hengshu Zhu, and Hui
Xiong. 2017. Point-of-Interest demand modeling with human mobility patterns.
ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining.
[39] Yingtao Luo, Qiang Liu, and Zhaocheng Liu. 2021. Stan: Spatio-temporal attention
network for next location recommendation. Web Conference.
[40] Chuishi Meng, Yu Cui, Qing He, Lu Su, and Jing Gao. 2017. Travel purpose
inference with GPS trajectories, POIs, and geo-tagged social media data. IEEE
International Conference on Big Data.
[41] Joseph Meyerowitz and Romit Roy Choudhury. 2009. Hiding stars with fireworks:
location privacy through camouflage. Annual International Conference on Mobile
Computing and Networking.
[42] Mike Boland. 2021. Foursquare’s power play continues with relaunched places
and new API. https://www.localogy.com/2021/03/foursquares-power-play-
continues-with-relaunched-places-and-new-api/
[43] Àlex Miranda-Pascual, Patricia Guerra-Balboa, Javier Parra-Arnau, Jordi Forné,
and Thorsten Strufe. 2023. SoK: Differentially private publication of trajectory
data. Proceedings on Privacy Enhancing Technologies (2023).
[44] Jovan Powar and Alastair R Beresford. 2023. SoK: Managing risks of linkage
attacks on data privacy. (2023).
[45] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. 2017. Knock
knock, who’s there? Membership inference on aggregate location data. (2017).
[46] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. 2020. Mea-
suring membership privacy on aggregate location time-series. Proceedings of the
ACM on Measurement and Analysis of Computing Systems 4, 2 (2020), 1–28.
[47] Jinmeng Rao, Song Gao, Yuhao Kang, and Qunying Huang. 2020. LSTM-
TrajGAN: A deep learning approach to trajectory privacy protection. arXiv
preprint arXiv:2006.10521.
[48] Lingfei Ren, Ruimin Hu, Dengshi Li, Zheng Wang, Junhang Wu, Xixi Li, and
Wenyi Hu. 2023. Who is your friend: inferring cross-regional friendship from
mobility profiles. (2023).
[49] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. 2018. Ml-leaks: Model and data independent membership infer-
ence attacks and defenses on machine learning models. (2018).
[50] Weiyan Shi, Si Chen, Chiyuan Zhang, Ruoxi Jia, and Zhou Yu. 2022. Just fine-tune
twice: Selective differential privacy for large language models. (2022).
[51] Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou Yu. 2022. Selective Differential
Privacy for Language Modeling. Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies.
[52] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. IEEE Symposium on
 
184Where Have You Been? A Study of Privacy Risk for Point-of-Interest Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Security and Privacy.
[53] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. 2017. Membership inference
attacks against machine learning models. IEEE Symposium on Security and
Privacy.
[54] Reza Shokri, George Theodorakopoulos, Panos Papadimitratos, Ehsan Kazemi,
and Jean-Pierre Hubaux. 2013. Hiding in the mobile crowd: Location privacy
through collaboration. (2013).
[55] Shubham Sharma. 2022. How Foursquare helps enterprises drive positive results
with geospatial technology. https://venturebeat.com/data-infrastructure/how-
foursquare-helps-enterprises/
[56] Mudhakar Srivatsa and Mike Hicks. 2012. Deanonymizing mobility traces: Using
social network as a side-channel. ACM Conference on Computer and Communi-
cations Security.
[57] Mudhakar Srivatsa and Mike Hicks. 2012. Deanonymizing mobility traces: Using
social network as a side-channel. ACM Conference on Computer and Communi-
cations Security.
[58] Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung Nguyen, and
Hongzhi Yin. 2020. Where to go next: Modeling long-and short-term user pref-
erences for point-of-interest recommendation. AAAI Conference on Artificial
Intelligence.
[59] Florian Tramèr, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagielski,
Sanghyun Hong, and Nicholas Carlini. 2022. Truth serum: Poisoning machine
learning models to reveal their secrets. ACM SIGSAC Conference on Computer
and Communications Security.
[60] Carmen Ruiz Vicente, Dario Freni, Claudio Bettini, and Christian S Jensen. 2011.
Location-related privacy in geo-social networks. (2011).
[61] Nevena Vratonjic, Kévin Huguenin, Vincent Bindschaedler, and Jean-Pierre
Hubaux. 2014. A location-privacy threat stemming from the use of shared
public IP addresses. (2014).
[62] Huandong Wang, Changzheng Gao, Yuchen Wu, Depeng Jin, Lina Yao, and
Yong Li. 2023. PateGail: a privacy-preserving mobility trajectory generator with
imitation learning. In Proceedings of the AAAI Conference on Artificial Intelligence.
[63] Jingyuan Wang, Jiawei Jiang, Wenjun Jiang, Chao Li, and Wayne Xin Zhao.
2021. LibCity: An open library for traffic prediction. International Conference
on Advances in Geographic Information Systems.
[64] Zehui Wang, Wolfram Höpken, and Dietmar Jannach. 2023. A survey on Point-of-
Interest recommendations leveraging heterogeneous data. arXiv:cs.IR/2308.07426
[65] Zhibo Wang, Wenxin Liu, Xiaoyi Pang, Ju Ren, Zhe Liu, and Yongle Chen. 2020.
Towards pattern-aware privacy-preserving real-time data collection. In IEEE
INFOCOM 2020-IEEE Conference on Computer Communications. IEEE.
[66] Haoran Xin, Xinjiang Lu, Tong Xu, Hao Liu, Jingjing Gu, Dejing Dou, and Hui
Xiong. 2021. Out-of-town recommendation with travel intention modeling.
arXiv:cs.IR/2101.12555
[67] Dingqi Yang, Benjamin Fankhauser, Paolo Rosso, and Philippe Cudre-Mauroux.
2020. Location prediction over sparse user mobility traces using rnns. Interna-
tional Joint Conference on Artificial Intelligence.
[68] Dingqi Yang, Daqing Zhang, Vincent W Zheng, and Zhiyong Yu. 2014. Modeling
user activity preference by leveraging user spatial temporal characteristics in
LBSNs. (2014).
[69] Song Yang, Jiamou Liu, and Kaiqi Zhao. 2022. GETNext: Trajectory flow map
enhanced transformer for next POI recommendation. ACM SIGIR Conference on
Research and Development in Information Retrieval.
[70] Guanglin Zhang, Anqi Zhang, and Ping Zhao. 2020. Locmia: Membership infer-
ence attacks against aggregated location data. (2020).
[71] Jia-Dong Zhang, Chi-Yin Chow, and Yanhua Li. 2014. Lore: Exploiting sequen-
tial influence for location recommendations. ACM SIGSPATIAL International
Conference on Advances in Geographic Information Systems.
[72] Shenglin Zhao, Tong Zhao, Haiqin Yang, Michael Lyu, and Irwin King. 2016.
STELLAR: Spatial-temporal latent ranking for successive point-of-interest rec-
ommendation. AAAI Conference on Artificial Intelligence.
Table 4: Statistics of POI Recommendation Datasets.
#POIs #Check-ins #Users #Trajectories Avg. Len.
4sq 4,556 63,648 1,070 17,700 3.63
Gowalla 2,559 32,633 1,419 7,256 4.46
A EXPERIMENT DETAILS
Data Preprocessing. We preprocess each dataset following the
literature [ 69]: (1) Filter out unpopular POIs and users that appear
less than ten times.(2) Construct trajectories of different users in a
daily manner (24-hours) (3) Normalize the timestamp (from 0:00
AM to 11:59 PM) in each check-in record into [0,1]. After the
aforementioned steps, the key statistics of the 4sqandGowalla
1 3 5 10 50 100
Number of Queries020406080100Topk-ASR (%)
k= 1
k= 3
k= 5(a)LocExtract (4sq)
1 3 5 10 50 100
Beam Size01020304050Topk-ASR (%)
k= 1
k= 3
k= 5 (b)TrajExtract (4sq)
Figure 10: Our LocExtract is effective with a small number
of queries and TrajExtract is effective with a small beam
size (i.e., both attacks are effective in a small query budget).
2 8 16 32 64 128 256
Number of Shadow Model020406080100Percent (%)
 AUC
ACC
TPR @ 10% FPR
(a)LocMIA (4sq)
2 8 16 32 64 128 256
Number of Shadow Model020406080100Percent (%)
 AUC
ACC
TPR @ 10% FPR (b)TrajMIA (4sq)
Figure 11: The attack performance of LocMIA andTrajMIA
significantly improves with more shadow models.
datasets are shown in Table 4. (4) Lastly, we split the datasets into the
training, validation, and test sets using the ratio of 8:1:1. For victim
model training, we use the official implementation of GETNext
andLSTPM to train victim models. By default, we train each model
with a batch size 32 for 200 epochs and use five random seeds in all
experiments to report the average results.
Attack Settings. (1)LocExtract Given a target user 𝑢, we extract
the most visited location 𝑙𝑡𝑜𝑝1from the victim model 𝑓𝜃with a
query number 𝑞=50. We set the query timestamp 𝑡=0.5(i.e., the
middle of the day) by default, and discuss the effect of different
query timestamps on the attack performance in the ablation study.
(2)TrajExtract In this attack, we experiment with 𝑛=4by default,
though the attacker can potentially extract location (sub-)sequences
with arbitrary length. We set the beam size 𝛽=50and also have the
default query timestamp 𝑡=0.5. (3) LocMIA In our experiments,
we randomly sample 80% of trajectories as the training dataset
𝐷𝑡𝑟to build a victim model and treat the remaining 20% data as
non-members. For each target user 𝑢and the POI location 𝑙pair, we
generate𝑁=64synthesis trajectories using TrajSynthesis with
the query timestamp 𝑡𝑠=0.5. With the synthesis trajectories, we
can also have 64 in-models ( 𝑓𝑖𝑛) and 64 out-models ( 𝑓𝑜𝑢𝑡). We also
set𝑛𝑡=10and𝑛𝑙=10. (4) TrajMIA We extract the membership
information of some trajectory sequences with arbitrary lengths
from the victim model. We also build 𝑁=64in-models (𝑓𝑖𝑛) and
𝑁=64out-models ( 𝑓𝑜𝑢𝑡) for a target trajectory sequence. For
evaluation, we conduct a hypothesis test on a balanced number of
members and non-members for both LocMIA andTrajMIA.
How to Select Aggregate Statistics. This section outlines the
basic principles and details for selecting representative aggregate
statistics for analysis. For user-level aggregate statistics, we target
the basic statistical information quantifying properties of locations
and trajectories of a user: (u1) Total number of check-ins; (u2)
 
185KDD ’24, August 25–29, 2024, Barcelona, Spain Kunlin Cai et al.
40 50 60 70 80
Top10-ACC (%)01020304050Top1-ASR (%)Undefended
Early stop
/bardblw/bardbl= 1e−2
/bardblw/bardbl= 3e−2
/epsilon1G= 0.01 (T)
/epsilon1G= 0.05 (T)
/epsilon1G= 0.05 (A)
/epsilon1J= 1 (T)
/epsilon1J= 5 (T)
/epsilon1J= 1 (A)
/epsilon1J= 5 (A)
(a)LocExtract
40 50 60 70 80
Top10-ACC (%)0200400600 Number of Successful AttackUndefended
Early stop
/bardblw/bardbl= 1e−2
/bardblw/bardbl= 3e−2
/epsilon1J= 1 (T)
/epsilon1J= 5 (T)
/epsilon1J= 1 (A)
/epsilon1J= 5 (A) (b)TrajExtract
0 20 40 60 80
Top10-ACC (%)020406080100 TPR @ 10 % FPRUndefended
Early stop
/bardblw/bardbl= 1e−2
/bardblw/bardbl= 3e−2
/epsilon1J= 1 (T)
/epsilon1J= 5 (T)
/epsilon1J= 1 (A)
/epsilon1J= 5 (A) (c)LocMIA
0 20 40 60 80
Top10-ACC (%)020406080100 TPR @ 10 % FPRUndefended
Early stop
/bardblw/bardbl= 1e−2
/bardblw/bardbl= 3e−2
/epsilon1J= 1 (T)
/epsilon1J= 5 (T)
/epsilon1J= 1 (A)
/epsilon1J= 5 (A) (d)TrajMIA
Figure 12: Defense performance on protecting all corresponding sensitive information for each attack.
40 50 60 70 80
Top10-ACC (%)01020304050Top1-ASR (%)Undefended
Early stop
/bardblw/bardbl= 1e−2
/bardblw/bardbl= 3e−2
/epsilon1G= 0.01 (T)
/epsilon1G= 0.05 (T)
/epsilon1G= 0.05 (A)
/epsilon1J= 1 (T)
/epsilon1J= 5 (T)
/epsilon1J= 1 (A)
/epsilon1J= 5 (A)
(a)LocExtract
40 50 60 70 80
Top10-ACC (%)050100150200 Number of Successful AttackUndefended
Early stop
/bardblw/bardbl= 1e−2
/bardblw/bardbl= 3e−2
/epsilon1J= 1 (T)
/epsilon1J= 5 (T)
/epsilon1J= 1 (A)
/epsilon1J= 5 (A) (b)TrajExtract
0 20 40 60 80
Top10-ACC (%)020406080100 TPR @ 10 % FPRUndefended
Early stop
/bardblw/bardbl= 1e−2
/bardblw/bardbl= 3e−2
/epsilon1J= 1 (T)
/epsilon1J= 5 (T)
/epsilon1J= 1 (A)
/epsilon1J= 5 (A) (c)LocMIA
0 20 40 60 80
Top10-ACC (%)020406080100 TPR @ 10 % FPRUndefended
Early stop
/bardblw/bardbl= 1e−2
/bardblw/bardbl= 3e−2
/epsilon1J= 1 (T)
/epsilon1J= 5 (T)
/epsilon1J= 1 (A)
/epsilon1J= 5 (A) (d)TrajMIA
Figure 13: Defense performance on protecting the targeted subset of sensitive information for each attack.
Number of unique visited POIs; (u3) Number of trajectories; (u4)
Average trajectory length. For location-level statistics, we study
their users, “neighboring” check-ins and trajectories, and the check-
in time information. (l1) Number of users who have visited this POI;
(l2) Number of check-ins surrounding ( ≤1km) this POI; (l3) Number
of trajectories sharing this POI; (l4) Average time in a day for the
visits to the POI. Similar to location-level statistics, for trajectory-
level aggregate statistics, we select: (t1) Number of users who have
the same trajectories; (t2) Number of check-ins surrounding ( ≤1km)
all POI in the trajectory; (t3) Number of intercepting trajectories;
(t4) Average check-in time of the trajectory.
B DEFENSE
Defense Metrics. As summarized in Table 3, our attacks target
different sensitive information on the training dataset. To this end,
we evaluate defense mechanisms in terms of their performance in
preventing each attack from stealing the corresponding sensitive
information. Specifically, we measure their defense performance
on protecting all the sensitive information anda targeted subset of
sensitive information for each attack. We define the targeted subset
of sensitive information as what defenders want to protect in prac-
tice (e.g., some selected user-location pairs in LocMIA). We include
this metric because not all the mobility data are sensitive or equally
important. In fact, we should evaluate how defense mechanisms
perform on the sensitive information that needs to be protected
(e.g., user-location pairs that may leak personal identity).
To this end, we jointly measure the defense performance in
protecting all the sensitive information and the targeted subset
of sensitive information for each attack. Based on different attack
objectives, we construct a different targeted subset of sensitive in-
formation for measurement by randomly sampling a portion of
(e.g., 30%) the most common locations in LocExtract, location
sequences in TrajExtract, user-location pairs in LocMIA andtrajectory sequences in TrajMIA. It is noted that we randomly
sample 30% of sensitive information in each attack to construct
the targeted subset for the ease of experiments. In practice, the de-
fender may have more personalized choices based on user-specific
requirements, which we leave as future work.
Defense Setup. The GETNext model and 4sq dataset are used for
experiments. For 𝐿2regularization, we use weight decay ∥𝑤∥=
1𝑒−2and3𝑒−2. For early stopping, we stop training after 5 epochs.
For JFT, we mask sensitive information that needs to be protected
in phase-I. Then in phase-II, we use DP-SGD [ 1] with different 𝜖𝐽
(1 and 5) to finetune the model. The 𝐶and𝛿are set to 10 and 1𝑒−3.
For Geo-Ind against LocExtract, we apply different 𝜖𝐺(0.01 and
0.05) to replace each sensitive POI with its nearby location such
that the original POI is indistinguishable from any location within
𝑟=400meters. Since both JFT and Geo-Ind can be used to protect
different amounts of sensitive information, we either protect nearly
all the sensitive information or only the targeted subset of sensitive
information for each attack, denoted by suffixes (A) and (T).
Results. Figure 12 and 13 showcase the results of existing defenses
in mitigating the privacy risks of all the sensitive information and
the targeted subset of sensitive information regarding each attack.
The results show that existing defenses mitigate our privacy attacks
to some extent. However, it is challenging to remove all the vulnera-
bilities within a reasonable utility drop. This is because existing POI
recommendation models heavily rely on memorizing user-specific
trajectory patterns to make predictions, which lack semantic in-
formation as guidance. As a result, defense mechanisms such as
DP-SGD can easily compromise the utility of the protected model
due to the noises added to the gradients. Moreover, defenses such
as JFT are not general for all inference attacks since each attack
steals different sensitive information. To this end, our evaluation
calls for more advanced mechanisms to defend against our attacks.
 
186