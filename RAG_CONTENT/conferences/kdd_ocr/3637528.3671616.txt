Addressing Shortcomings in Fair Graph Learning Datasets:
Towards a New Benchmark
Xiaowei Qian∗
Rensselaer Polytechnic Institute
Troy, NY, USA
xiaoweiqian0311@gmail.comZhimeng Guo∗
The Pennsylvania State University
University Park, PA, USA
zhimeng@psu.eduJialiang Li
New Jersey Institute of Technology
Newark, NJ, USA
jl2356@njit.edu
Haitao Mao
Michigan State University
East Lansing, MI, USA
haitaoma@msu.eduBingheng Li
Michigan State University
East Lansing, MI, USA
libinghe@msu.eduSuhang Wang
The Pennsylvania State University
University Park, PA, USA
szw494@psu.edu
Yao Ma†
Rensselaer Polytechnic Institute
Troy, NY, USA
may13@rpi.edu
ABSTRACT
Fair graph learning plays a pivotal role in numerous practical ap-
plications. Recently, many fair graph learning methods have been
proposed; however, their evaluation often relies on poorly con-
structed semi-synthetic datasets or substandard real-world datasets.
In such cases, even a basic Multilayer Perceptron (MLP) can outper-
form Graph Neural Networks (GNNs) in both utility and fairness.
In this work, we illustrate that many datasets fail to provide mean-
ingful information in the edges, which may challenge the neces-
sity of using graph structures in these problems. To address these
issues, we develop and introduce a collection of synthetic, semi-
synthetic, and real-world datasets that fulfill a broad spectrum of
requirements. These datasets are thoughtfully designed to include
relevant graph structures and bias information crucial for the fair
evaluation of models. The proposed synthetic and semi-synthetic
datasets offer the flexibility to create data with controllable bias
parameters, thereby enabling the generation of desired datasets
with user-defined bias values with ease. Moreover, we conduct
systematic evaluations of these proposed datasets and establish a
unified evaluation approach for fair graph learning models. Our
extensive experimental results with fair graph learning methods
across our datasets demonstrate their effectiveness in benchmark-
ing the performance of these methods. Our datasets and the code
for reproducing our experiments are available1.
∗These authors contributed equally to this work.
†Corresponding author.
1https://github.com/XweiQ/Benchmark-GraphFairness
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671616CCS CONCEPTS
•Information systems →Data mining.
KEYWORDS
Graph Neural Network, Fairness, Node Classification
ACM Reference Format:
Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li, Suhang
Wang, and Yao Ma. 2024. Addressing Shortcomings in Fair Graph Learn-
ing Datasets: Towards a New Benchmark. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671616
1 INTRODUCTION
Graph structure is ubiquitous language to model complicated re-
lationships. As much information is organized as graph structure,
graph neural networks are becoming increasingly important in
various fields, including knowledge graphs [ 2,33], drug discov-
ery [ 15,27] and social media mining [ 9]. GNNs are versatile in
handling tasks related to graphs, enhancing performance in activi-
ties from node classification [ 13,25,32] to link prediction [ 20,29]
and graph classification [ 24]. However, accompanying the wide
deployment in many critical systems [ 10], concerns about the po-
tential risks associated with GNNs are growing. Research shows
that GNNs can either inherit or exacerbate bias in the data, lead-
ing to unfair and biased predictions, which potentially reinforce
existing prejudices and discrimination [ 4,5]. This issue has raised
ethical and societal concerns, significantly hindering GNNs’ ap-
plication in sensitive decision-making areas, such as ranking job
applicants [18] and predicting criminal behavior [22].
To tackle the fairness challenge, a series of fair graph learning
models have been developed, e.g., FairGNN [ 4], NIFTY [ 1], and ED-
ITS [ 7]. These methods aim to improve fairness while maintaining
the model’s accuracy. Building on the foundational concerns re-
garding the development of fair graph learning models, it is crucial
to scrutinize the existing evaluation frameworks that assess these
5602
KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaowei Qian et al.
models. Upon examination, we find that existing evaluation proto-
cols suffer from several pitfalls that impede our ability to properly
evaluate these methods, which are summarized as follows:
•The evaluation of fair graph learning models is often limited to
a few poorly constructed semi-synthetic datasets [ 1] converted
from tabular datasets and an array of real-world datasets [ 4].
Specifically, the graph connections in the semi-synthetic datasets
are constructed based on feature similarity, which often struggles
to provide additional information for prediction. Our exploration
in Section 4.1 (see results in Table 3) demonstrates that, on the
semi-synthetic datasets, Multilayer Perceptrons (MLPs), which do
not leverage the graph structure outperforms GCN and fairness-
focused GNN methods including NIFTY [ 1] and FairGNN [ 4] by
a decent margin. In addition, not only do MLPs achieve higher
performance, but they also maintain superior fairness metrics on
these semi-synthetic datasets. This phenomenon is not limited to
semi-synthetic datasets. Certain real-world datasets, which we
discuss further in Table 4, exhibit similar patterns. Furthermore,
on some certain real-world datasets, the incorporation of graph
structure does not introduce additional biases, as evidenced by
GCNs and MLPs achieving comparable levels of fairness. These
findings underline a critical issue with the current datasets used
for evaluating fair graph learning models, i.e., the graph structure
in these datasets does not effectively bring additional information.
They do not provide the necessary challenges or contexts where
the sophisticated mechanisms of fair GNNs can demonstrate their
full potential and value over more basic analytical models.
•Unlike traditional node classification tasks where models cease
training upon plateauing performance on a validation set, fair
graph learning models must also consider fairness. Therefore, dif-
ferent model selection strategies have been developed to achieve
this goal, which introduces additional complexities for evalua-
tion. Specifically, our investigation in Section 3 highlights that
the choice of model selection strategy significantly affects both
model performance and fairness. The diversity in model selection
strategies makes it challenging to determine if observed differ-
ences in outcomes are the result of the models’ inherent algorithmic
qualities or the particular model selection mechanisms employed,
complicating the process of accurately evaluating these models.
To address these concerns, we first propose to unify the model
selection mechanisms used across various models. Following this,
we aim to develop a collection of new datasets specifically designed
for evaluating fair graph learning. In creating these datasets, we
focus on two crucial aspects: (1) the Utility of Graph Structure,
and (2) the Potential for Bias Amplification via Graph Structure.
Our approach ensures that only models adept at leveraging graph
structure while neutralizing embedded biases will stand out. These
rigorously designed datasets pose a formidable challenge, pushing
the boundaries of any single approach. Our proposed datasets are
challenging that test the limits of any single method across diverse
datasets, thereby creating significant opportunities for developing
fair graph learning methods and setting new benchmarks in the
field. Our main contributions are:
•We conduct a thorough examination of existing model selection
strategies and widely used datasets within the realm of fair graphlearning, identifying key shortcomings that hinder accurate as-
sessment and progress in the field, and then offer our solutions.
•We unveil a comprehensive collection of synthetic, semi-synthetic,
and real-world datasets specifically crafted for fair graph learning,
aiming to cater to diverse needs of research in this domain.
•By benchmarking leading fair graph learning approaches on
our datasets, we offer valuable insights into their effectiveness,
shedding light on the intricacies of achieving fairness in graph-
based models.
2 RELATED WORKS
In this section, we review the recent advances on graph neural
networks and fair graph learning models.
2.1 Graph Neural Networks
Graph neural networks (GNNs) have revolutionized the analysis of
graph-structured data across various tasks, including node classifi-
cation [ 13,23], graph classification [ 21], and link prediction [ 31,34].
These networks fall into two primary categories: spatial-based
GNNs, which utilize direct graph structures to focus on node and
neighbor interactions for feature learning, and spectral-based GNNs,
which analyze graphs through the spectral domain using the graph
Laplacian and its eigenvectors to grasp global graph properties.
The exceptional capabilities of GNNs have broadened their ap-
plication [ 11], ranging from financial institutions using them to
detect fraudulent activities in transaction networks [ 5] to their in-
tegration into critical decision-making systems where fairness and
interpretability become paramount [ 28]. Despite their widespread
success, recent research highlights a significant concern: GNNs
can exhibit implicit biases towards different groups, potentially
leading to unfair outcomes [ 4]. This issue is of particular concern in
sensitive applications, underscoring the urgency of incorporating
fairness into the GNN modeling process. Bias in GNNs typically
arises from two sources: the inherent prejudices present in the input
data, and the algorithmic tendencies of GNNs that may favor certain
patterns or connections [ 3]. Consequently, there’s a growing move-
ment within the research community towards developing fairer
GNN models, aiming to address these biases for more equitable
graph-based applications [16].
2.2 Fair Graph Learning
Fairness has become a pivotal issue in machine learning, promi-
nently within the Graph Neural Networks (GNNs) domain [ 17–19].
The evaluation of model fairness encompasses various perspectives,
such as group fairness [ 12], individual fairness [ 8], and counterfac-
tual fairness [ 14], all of which are pertinent to GNN assessment [ 1].
In the realm of GNN fairness, concepts like statistical parity [ 30]
and equal opportunity [ 12] are particularly prominent. Efforts such
as FairGNN [ 4] employ adversarial training to improve fairness,
aiming to prevent the model from leveraging sensitive attributes for
predictions. However, the traditional reliance on correlation-based
methods for ensuring fairness is challenged by their inability to
navigate complexities such as Simpson’s paradox. This limitation
has spurred a shift towards counterfactual fairness, rooted in causal
theory, that promises a deeper, more nuanced approach by focusing
5603Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark KDD ’24, August 25–29, 2024, Barcelona, Spain.
on causal relationships to circumvent biases induced by correla-
tions. This shift is exemplified by recent innovations like NIFTY [ 1],
showcasing a keen interest in counterfactual approaches to GNNs.
FairVGNN [ 26] stands out by generating fair views via the auto-
matic identification and masking of sensitive-correlated features,
adjusting to correlation changes after feature propagation, thereby
achieving state-of-the-art performance on a variety of standard fair
graph learning datasets.
3 ISSUES OF EVALUATION SETTINGS
When evaluating fair graph learning methods, we often care about
both the model classification performance and fairness. Specifically,
the following metrics are often adopted for evaluation. For evalu-
ating the classification performance, we often utilize metrics such
as accuracy (ACC), ROC AUC, and F1-score. To quantify group
fairness, we use statistical parity (SP) [ 8] and equal opportunity
(EO) [12]. The exact metrics are defined as:
Δ𝑆𝑃=|𝑃(ˆ𝑦=1|𝑠=0)−𝑃(ˆ𝑦=1|𝑠=1)|,
Δ𝐸𝑂=|𝑃(ˆ𝑦=1|𝑦=1,𝑠=0)−𝑃(ˆ𝑦=1|𝑦=1,𝑠=1)|,(1)
where𝑦∈{0,1}denotes the binary label, 𝑠∈{0,1}denotes the
sensitive attribute, and ˆ𝑦∈{0,1}denotes prediction of the classifier.
Table 1: Comparison of different model selection strategies.
Strategy ACC AUC F1 ΔSPΔEO
Model selection 1 69.20 62.00 81.36 2.58 0.63
Model selection 2 69.20 64.75 80.31 8.16 4.62
Model selection 3 61.60 64.25 68.63 5.55 1.02
Unlike standard node classification models that stop training
when their performance on the validation set stops improving
significantly, fair graph learning methods need to think about how
well they balance accuracy and fairness before deciding when to
stop training. To achieve this goal various model selection strategies
have been developed:
•Model selection 1. It first eliminates the training epochs whose
ACC and AUC are below the preset thresholds. Then, from the
remaining epochs, it picks the ones with the best fairness mea-
sures, specifically Statistical Parity (SP) and Equal Opportunity
(EO). This strategy is used in the implementation of FairGNN [ 4].
•Model selection 2. It uses the validation loss to choose the best
model. The model with the lowest validation loss would be tested.
This strategy is employed in the NIFTY[1] implementation.
•Model selection 3. It uses the validation AUC to select the best
model. The model with the highest validation AUC would be
tested. NIFTY [ 1] utilized this strategy when replicating FairGNN
in its experiment.
The inconsistency of model selection strategies in the same pa-
per caught our attention. Specifically, in NIFTY [ 1], NIFTY methods
adopt Model selection 2, while FairGNN methods adopt Model selec-
tion 3. Table 1 shows the result of running NIFTY-GCN with these
three model selection strategies on the German dataset. Similar
phenomena are observed on other datasets and with other models,Algorithm 1 The Proposed Model Selection Strategy
1:Initialize𝑏𝑒𝑠𝑡_𝑓𝑎𝑖𝑟𝑛𝑒𝑠𝑠 to∞;
2:Initialize𝑏𝑒𝑠𝑡_𝑒𝑝𝑜𝑐ℎ to0;
3:Initialize𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 _𝑟𝑎𝑡𝑖𝑜𝑠 to[0.95,0.94,0.93,0.92,0.91,0.9];
4:for𝑟𝑎𝑡𝑖𝑜 in𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 _𝑟𝑎𝑡𝑖𝑜𝑠 do
5:𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 _𝑎𝑐𝑐←𝑚𝑎𝑥_𝑣𝑎𝑙_𝑎𝑐𝑐×𝑟𝑎𝑡𝑖𝑜 ;
6:𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 _𝑟𝑜𝑐←𝑚𝑎𝑥_𝑣𝑎𝑙_𝑟𝑜𝑐×𝑟𝑎𝑡𝑖𝑜 ;
7:𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 _𝑓1←𝑚𝑎𝑥_𝑣𝑎𝑙_𝑓1×𝑟𝑎𝑡𝑖𝑜 ;
8: foreach𝑒𝑝𝑜𝑐ℎ do
9: if𝑣𝑎𝑙_𝑎𝑐𝑐≥𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 _𝑎𝑐𝑐 and𝑣𝑎𝑙_𝑟𝑜𝑐≥
𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 _𝑟𝑜𝑐and𝑣𝑎𝑙_𝑓1≥𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 _𝑓1and(𝑣𝑎𝑙_𝑝𝑎𝑟𝑖𝑡𝑦+
𝑣𝑎𝑙_𝑒𝑞𝑢𝑎𝑙𝑖𝑡𝑦)<𝑏𝑒𝑠𝑡_𝑓𝑎𝑖𝑟𝑛𝑒𝑠𝑠 then
10: 𝑏𝑒𝑠𝑡_𝑓𝑎𝑖𝑟𝑛𝑒𝑠𝑠←𝑣𝑎𝑙_𝑝𝑎𝑟𝑖𝑡𝑦+𝑣𝑎𝑙_𝑒𝑞𝑢𝑎𝑙𝑖𝑡𝑦 ;
11: 𝑏𝑒𝑠𝑡_𝑒𝑝𝑜𝑐ℎ←𝑒𝑝𝑜𝑐ℎ ;
12: end if
13: end for
14:end for
detailed in Appendix A.2. These results emphasize an inadequate ba-
sis for evaluating these methods fairly caused by the inconsistency
of strategies. For evaluating equitable, a unified model selection
strategy should be adopted for all methods. However, there are
some issues with existing strategies. Model selection 2 and 3 do
not consider the trade-off between accuracy and fairness. The ef-
fectiveness of Model selection 3 is highly dependent on manually
preset thresholds, which is certainly cumbersome for evaluating
various methods on different datasets. Therefore, implementing a
consistent and equitable model selection strategy is imperative for
the benchmark of fair graph learning methods.
Our model selection strategy is described in Algorithm 1. Com-
pared to the existing strategies, the proposed model selection strat-
egy balances utility and fairness and employs the adaptive thresh-
olds. Since these graph fairness learning methods aim to sacrifice a
small portion of utility for higher fairness, the threshold interval
is set as 90%−95%to trade-off. Additionally, using three classifi-
cation performance metrics ensures a fair comparison of baseline
performance. We anticipate that this standardized model selection
approach will assist researchers in achieving a more equitable as-
sessment of fair graph learning models.
4 ISSUES OF POPULAR GRAPH FAIRNESS
DATASETS
Good datasets are essential for advancing the field of study. How-
ever, our thorough review reveals that datasets commonly used for
fair graph learning suffer from significant issues that could slow
progress in this area. To verify these issues, we not only run GNN
method and fairness-focused models, but we also include MLP as
baseline, which is not always included in existing literature. More-
over, Our experiments set a fine-grained parameter search space for
each baseline and uniformly employ our proposed model selection
strategy to obtain feasible comparisons. Our empirical findings,
detailed in Tables 3 and 4, illustrate these problems. We examine
both semi-synthetic and real-world datasets widely used in the
community, identifying specific concerns to be addressed.
5604KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaowei Qian et al.
Table 2: Statistics of commonly-used Semi-synthetic and Real-world datasets in fair graph learning works.
Dataset Type Semi-synthetic Real-world
Dataset German Bail Credit Pokec-z Pokec-n NBA
#Nodes 1,000 18,876 30,000 67,797 66,569 403
#Edges 21,742 311,870 1,421,858 617,958 517,047 10,621
#Features 27 18 13 69 69 39
Average Degree 44.48 34.04 95.79 19.23 16.53 53.71
Sens. Gender Race Age Region Region Nationality
Label Good/bad Credit Bail/no bail Payment default/no default Working Field Working Field Salary
Table 3: Results of models on Semi-synthetic datasets. ↑rep-
resents the larger, the better, while ↓represents the opposite.
MLP
GCN FairGNN NIFTY
Metric
German
A
CC (↑) 71.36±1.35 71.52±1.17 69.84±0.60 70.40±1.36
AUC (↑) 72.45±0.75 73.79±2.09 62.47±1.56 69.3±1.39
F1 (↑) 82.29±0.25 80.01±1.24 82.11±0.26 81.12±1.91
ΔSP(↓) 7.25±6.35 36.67±11.62 1.78±3.15 4.79±1.68
ΔEO(↓) 3.28±3.16 28.78±9.54 2.00±3.08 3.42±2.45
Metric
Bail
A
CC (↑) 88.13±0.62 84.49±1.26 84.50±1.07 77.68±7.14
AUC (↑) 90.69±0.74 88.76±1.30 89.08±1.47 81.27±0.86
F1 (↑) 82.43±1.06 79.38±1.29 79.50±1.19 69.23±2.53
ΔSP(↓) 0.76±0.54 7.43±0.91 7.32±0.94 5.04±0.33
ΔEO(↓) 4.42±0.33 4.31±0.96 4.25±0.91 4.47±1.11
Metric
Credit
A
CC (↑) 76.91±1.94 73.58±0.84 73.41±1.21 73.54±1.84
AUC (↑) 71.36±0.50 67.61±0.27 68.99±0.19 68.93±0.09
F1 (↑) 86.32±2.48 82.87±0.74 82.59±1.06 82.67±1.65
ΔSP(↓) 2.26±4.51 11.47±1.56 4.41±2.71 8.56±0.49
ΔEO(↓) 1.78±3.56 9.61±1.63 2.97±1.99 6.44±0.32Table 4: Results of models on real-world datasets. ↑represents
the larger, the better, while ↓represents the opposite.
MLP
GCN FairGNN NIFTY
Metric
Pokec-z
A
CC (↑) 65.18±1.06 69.36±0.21 65.97±2.09 64.47±1.12
AUC (↑) 70.84±0.79 74.86±0.93 70.74±1.11 70.45±0.66
F1 (↑) 65.89±1.46 67.20±0.51 67.13±0.68 65.56±1.65
ΔSP(↓) 2.76±0.72 4.76±1.00 2.41±1.50 3.51±1.88
ΔEO(↓) 1.90±0.96 5.05±1.11 2.15±1.15 2.46±2.31
Metric
Pokec-n
A
CC (↑) 67.42±0.36 70.15±0.46 65.71±2.52 65.57±1.31
AUC (↑) 72.10±0.47 74.89±0.19 70.40±2.23 68.75±0.38
F1 (↑) 62.23±1.76 65.23±0.53 63.22±1.60 60.21±1.44
ΔSP(↓) 6.57±1.14 7.84±0.76 5.78±3.14 5.66±0.92
ΔEO(↓) 8.67±0.97 11.64±1.12 7.56±3.27 7.28±1.75
Metric
NBA
A
CC (↑) 67.32±0.56 72.02±0.70 70.33±0.46 62.44±4.28
AUC (↑) 72.48±0.74 76.95±0.19 76.33±0.45 69.27±1.22
F1 (↑) 71.14±2.31 74.41±1.19 74.50±0.70 66.87±3.51
ΔSP(↓) 4.00±0.92 2.03±0.86 1.85±1.29 6.21±1.88
ΔEO(↓) 0.78±0.44 3.32±1.49 1.61±2.09 3.91±1.89
4.1 Semi-synthetic Datasets
We examined semi-synthetic datasets, specifically focusing on the
German, Bail, and Credit datasets. The statistical details of these
datasets are provided in Table 2. More descriptions of these datasets
can be found in Appendix A.1.1. Our experiments led to several
concerns detailed below:
Obs 1: Considering predictive capabilities assessed through ACC,
AUC and F1, alongside fair metrics such as difference in ΔSPand
ΔEO, graph-based approaches like GCN do not demonstrate superior
performance compared to MLP across various datasets, which may
challenge the necessity to use graph-based methods in these datasets.
As shown in Table 2, MLPs achieve comparable predictive accuracy
without compromising fairness metrics on three widely used semi-
synthetic datasets. Specifically, performance metrics for ACC, AUC,
and F1 scores for MLP and GCN are quite close across these datasets.
What’s worse, MLPs show a significant advantage in terms of ΔSP
andΔEO, indicating a clear lead in fairness. The result is a strong
signal that the graph structures of these semi-synthetic datasetsdo not contribute meaningful information for enhancing predic-
tions. The rationale behind this is straightforward. According to the
dataset generation process described in [ 1], these datasets originate
from tabular data, with graph structures generated based on feature
similarity. Thus, reiterating this feature similarity through graph
structures does not enrich graph-based methods with novel insights.
Moreover, the emphasis on feature similarity in constructing graph
structures might inadvertently introduce noise to graph-based mod-
els, such as GNNs, potentially deteriorating fairness metrics. These
results raise concerns about the necessity of using graph structures
for these tasks and suggest that these datasets may not be suitable
for fair graph learning problem.
Obs 2: In the analysis of fairness-focused models (FairGNN and
NIFTY) versus MLP, we observe no consistent superiority in utility and
fairness. As detailed in Table 3, a simple MLP model outperforms
these fairness-focused models across all evaluated metrics, includ-
ing ACC, AUC, F1, ΔSP, andΔEO, by a significant margin. This dis-
crepancy deepens concerns regarding the applicability of datasets
for fostering development of fair graph learning algorithms.
5605Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark KDD ’24, August 25–29, 2024, Barcelona, Spain.
4.2 Real-World Datasets
Our investigation extends to real-world datasets, with a particular
focus on the Pokec-z, Pokec-n, and NBA datasets. The specifics of
these datasets are detailed in Table 2, providing a comprehensive
statistical overview. More descriptions of these datasets can be seen
in Appendix A.1.1. From our experiments, these real-world datasets
present several issues:
Obs 1: As shown in Table 4, the bias in topology is not distinctly
apparent among the Pokec-z and NBA datasets. The unfairness
demonstrated by the GCN model is close to the MLP model. These
small values result in the bias-alleviating effect of these fairness
methods being limited. In other words, these datasets do not provide
sufficient room for comparison of fairness methods.
Obs 2: In Pokec-z and Pokec-n datasets, MLP superior fairness
methods in classification performance while the unfairness of MLP
is similar to, or even less than the fairness methods. This makes
one wonder if fair learning methods are needed on these datasets
rather than a simple MLP.
Obs 3: We found that there is a problem with merging multiple la-
bels into binary labels during the processing of real-world datasets.
This conversion oversimplifies the inherent complexity of the data,
potentially leading to a skewed representation of the original infor-
mation. Such a approach may result in less robust models, which
does not allow for a fair evaluation of the various models.
4.3 Summary
Based on the observations, we can conclude that the primary issue
across six existing datasets is the lack of meaningful information
provided by their graph structures. Consequently, graph-based
methods tend to underperform compared to MLP. If graph struc-
ture were sufficiently informative, fairness-focused methods would
still attain higher accuracy than MLP, albeit model utility is slightly
reduced compared to conventional GNN architectures. It is note-
worthy that fairness-focused methods also exhibit shortcomings
in fairness compared to MLP in certain datasets, highlighting the
inadequacy of these datasets for evaluating fair graph learning
methods. Hence, future research should consider these limitations
when selecting or creating datasets for assessing graph unfairness.
5 NEW FAIR GRAPH LEARNING DATASETS
In light of the issues identified with existing semi-synthetic and
real-world datasets, there is a pressing need for new datasets that
better benchmark fair graph learning methods. To address these
challenges and push the boundaries of fair graph learning research
forward, we propose the introduction of new datasets specifically
designed to overcome the limitations of current datasets. We seek
to offer a more robust and challenging benchmark for developing
and evaluating fair graph learning algorithms. This section outlines
the development process, characteristics, and potential impact of
these new datasets on the field of fair graph learning. Our goal is to
facilitate the development of more accurate, fair, and generalizable
graph learning models that can navigate the intricacies of real-world
social structures and biases. In the construction of new datasets,
we prioritize the following critical considerations:
•Graph Structure Utility. Graph structure must demonstrably
enhance predictive performance, i.e., helpful for prediction task.•Bias Amplification through Graph Structure. Graph struc-
ture should amplify the bias information. Thus, it can render
the performance discrepancy for different fair graph learning
methods involving graph structure information.
These principles ensure that only models adept at leveraging graph
structure for enhanced information processing, while simultane-
ously mitigating bias inherent within, will excel. Consequently,
models relying solely on feature-based methodologies will find
themselves at a disadvantage due to their inability to harness the
graph structure. Similarly, methods that overlook the bias present
in graph structures will face challenges, pushing fairness-oriented
models to innovate beyond merely identifying and correcting for
bias. This approach aims to foster the development of models that
not only capitalize on informational wealth of graph structures
but also navigate and neutralize biases effectively, setting a new
standard for fairness in graph learning research. Starting from syn-
thetic datasets allows researchers to control utility and bias, and
then transition to new semi-synthetic datasets, and finally evaluate
models on real-world datasets to provide realistic test scenarios.
This progressive benchmarking approach enables a thorough assess-
ment of model capabilities across different stages of dataset realism,
ensuring robustness and effectiveness in real-world applications.
5.1 Synthetic Datasets
This section explores the relationship between graph structures
and fairness performance, outlines the data generation process, and
introduces two datasets to demonstrate our analysis framework.
5.1.1 Interplay Between Edge Generation Probability and Fairness
Metrics. We propose a comprehensive framework, illustrated in
Table 5, to explore this interplay. Our focus is on scenarios with
binary sensitive attributes and binary labels, where the probability
of edge creation directly influences the accuracy of different groups,
subsequently impacting fairness metrics. This approach aids in the
design and enhancement of synthetic and semi-synthetic datasets.
The process unfolds in two pivotal steps:
•From Edge Generation Probability to Group Accuracy: The
correlation between the probability of generating edges and the
accuracy of specific groups is outlined in Table 5. For instance, if
we fix the edge generation probabilities for other connections and
increase the probability for the "S0Y0-S0Y0" edge, we anticipate
an improvement in the accuracy for the "S0Y0" group. This step
provides a methodical way to predict group accuracy based on
edge generation dynamics.
•From Group Accuracy to Fairness Metrics: The fairness met-
rics, such as Statistical Parity (SP) and Equal Opportunity (EO),
are crucial for assessing fairness. SP gauges the variance in predic-
tive probabilities, whereas EO evaluates the accuracy discrepancy
between groups. By examining the accuracy of various groups,
we gain insights into potential changes in these fairness metrics,
offering a straightforward strategy to assess and enhance fairness
in model predictions.
5.1.2 Dataset Construction Process. This refined approach under-
scores the significance of understanding the underlying graph struc-
ture to inform dataset design and improve fairness in algorithmic
decisions. It can also be an effective tool for us to generate synthetic
5606KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaowei Qian et al.
Table 5: Investigating the Correlation Between Edge Generation Probability and GCN Prediction Accuracy for Different Groups.
Symbols "+" and "−" represent positive and negative correlations, highlighting the variation in accuracy across groups as
influenced by edge generation probability. Nodes are classified into four groups based on sensitive attributes and labels ("S0Y0",
"S0Y1", "S1Y0", "S1Y1"). Edges are further categorized into ten types according to the characteristics of their connecting nodes
(e.g., "S0Y0-S0Y0"), allowing for a detailed analysis of the network’s structure and its impact on model performance.
S0Y0-S0Y0
(𝐸1) S0Y1-S0Y1 ( 𝐸2) S1Y0-S1Y0 ( 𝐸3) S1Y1-S1Y1 ( 𝐸4)S0Y0-S1Y0
(𝐸5) S0Y1-S1Y1 ( 𝐸6)S0Y0-S0Y1
(𝐸7) S1Y0-S0Y1 ( 𝐸8)S0Y1-S1Y0
(𝐸8) S0Y0-S1Y1 ( 𝐸10)
S0Y0 + + - -
S0Y1 + + - -
S1Y0 + + - -
S1Y1 + + - -
Table 6: Results of models for Syn-1 and Syn-2 datasets.
Dataset Metho
d A
CC (↑) AUC (↑) F1 (↑) ΔSP(↓) ΔEO(↓)Gr
oup ACC
S0Y0
S0Y1 S1Y0 S1Y1
Syn-1MLP 78.84 ±0.34
87.25 ±0.28 80.19 ±0.40 1.76 ±1.10 4.35 ±1.89 81.86 ±0.74
80.63 ±1.26 76.51 ±1.01 76.28 ±1.24
GCN 86.96 ±0.66
94.63 ±0.05 87.90 ±0.58 10.97 ±0.85 10.37 ±1.32 95.40 ±1.67
81.95 ±1.17 78.86 ±1.28 92.33 ±0.75
Syn-2MLP 71.04 ±0.79
78.51 ±0.66 72.59 ±1.05 10.52 ±1.04 7.57 ±1.31 71.00 ±3.30
73.16 ±2.18 73.55 ±1.12 65.59 ±2.39
GCN 78.98 ±0.58
86.69 ±0.20 80.32 ±0.57 22.04 ±1.44 24.09 ±2.75 77.80 ±2.91
88.50 ±1.13 81.75 ±1.93 64.41 ±3.04
datasets with biased graph structures for fairness problems. We
present the generating process of synthetic datasets as follows:
(1)Generate𝑦and𝑠for𝑛samples from a categorical distribution,
(𝑠𝑖,𝑦𝑖)∼Categorical(𝑝00,𝑝01,𝑝10,𝑝11)for𝑖=1,...,𝑛,
where𝑝00,𝑝01,𝑝10,𝑝11denote the probabilities of generating
the four possible outcomes for the pairs (𝑠,𝑦), with each pair
representing a unique combination of 𝑠and𝑦, ensuring that
𝑝00+𝑝01+𝑝10+𝑝11=1.
(2)Generate embeddings 𝑒𝑦and𝑒𝑠of dimension 𝑑1for𝑛samples
from separate multivariate Gaussian distributions,
𝑒𝑦𝑖=0∼N −𝜇𝑦,Σ𝑦𝑒𝑦𝑖=1∼N 𝜇𝑦,Σ𝑦
𝑒𝑠𝑖=0∼N(−𝜇𝑠,Σ𝑠)𝑒𝑠𝑖=1∼N(𝜇𝑠,Σ𝑠),
where Σ𝑦=𝑐1·𝐼𝑑1andΣ𝑠=𝑐2·𝐼𝑑1represent the covariance
matrices for 𝑒𝑦and𝑒𝑠embeddings, respectively, with 𝑐1and𝑐2
being scalars and 𝐼𝑑1the identity matrix of dimension 𝑑1. The
variance ( Σ𝑦,Σ𝑠) and mean ( 𝜇𝑦,𝜇𝑠) parameters are adjustable
to modulate the separability between the groups.
(3)To construct the node attribute 𝑥𝑖for each sample, concatenate
the embeddings 𝑒𝑦𝑖and𝑒𝑠𝑖as follows:
𝑥𝑖=
𝑒𝑦𝑖|𝑒𝑠𝑖
,
where[·|·]denotes the concatenation of the 𝑒𝑦𝑖and𝑒𝑠𝑖em-
beddings, resulting in a single, unified node attribute vector 𝑥𝑖
for each node.
(4)In constructing the graph, we initiate the creation of edges by
employing independent Bernoulli distributions for each poten-
tial edge type. Specifically, the existence of each edge type 𝐸𝑖is
determined as follows:
𝐸𝑖∼Bernoulli(𝑝𝑖)for each𝑖=1,2,...,10,
where𝑝𝑖represents the probability associated with the forma-
tion of edge type 𝐸𝑖. These probabilities correspond to the 10 dis-
tinct types of edges enumerated in Table 5, such as "S0Y0-S0Y0",allowing for controlled variability in the graph’s connectivity
based on predefined probabilities.
Table 7: The probability of generating different edges in Syn-
1 and Syn-2.
Syn-1𝐸1𝐸2𝐸3𝐸4𝐸5
0.008 0.004 0.004 0.006 0.002
𝐸6𝐸7𝐸8𝐸9𝐸10
0.002 0.002 0.002 0.001 0.002
Syn-2𝐸1𝐸2𝐸3𝐸4𝐸5
0.006 0.008 0.007 0.005 0.002
𝐸6𝐸7𝐸8𝐸9𝐸10
0.002 0.003 0.004 0.002 0.002
5.1.3 Synthetic Dataset Examples. We introduce two synthetic
datasets, crafted using our framework to illustrate how adjusting
parameters such as group ratios, the means and variances of multi-
variate Gaussian distributions, and edge generation probabilities
can influence dataset fairness and performance. These adjustments
directly affect the fairness by altering the likelihood of different
edge types, as shown in Figure 5, thereby creating disparities in
group performance.
Both datasets comprise 5,000 samples, with node attributes di-
mensioned at 48. Table 7 displays their graph structure distributions.
Analysis of these datasets, detailed in Table 6, highlights the out-
comes of our parameter manipulations.
Syn-1 features a balanced group ratio ( 𝑝00=𝑝01=𝑝10=𝑝11). This
balance results in minimal performance variance across the four
groups when using MLP, enhancing fairness, particularly in terms
of Statistical Parity (SP). The introduction of more intra-label group
edges not only improves the utility of the graph structure but also
enhances GNN performance. Notably, variance in edge types leads
5607Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark KDD ’24, August 25–29, 2024, Barcelona, Spain.
to superior GNN outcomes for groups 𝑠=0,𝑦=0, and𝑠=1,𝑦=1,
albeit introducing some degree of unfairness.
Syn-2 demonstrates an unbalanced group ratio ( 𝑝00=0.22,𝑝01=
0.28,𝑝10=0.28,𝑝11=0.22), which engenders significant unfairness
in MLP predictions. Similar to Syn-1, the graph structure elevates
GNN performance. Here, denser connections yield better results
for groups𝑠=0,𝑦=1, and𝑠=1,𝑦=0, further contributing to
unfairness. By adjusting the Gaussian distribution’s variance, we
lower MLP’s baseline performance, thereby amplifying the graph
structure’s beneficial impact on performance.
5.2 Semi-synthetic Datasets
Table 8: The proportion of different edges in existing semi-
synthetic datasets.
German𝐸1𝐸2𝐸3𝐸4𝐸5
0.059 0.308 0.030 0.076 0.026
𝐸6𝐸7𝐸8𝐸9𝐸10
0.087 0.246 0.085 0.036 0.047
Bail𝐸1𝐸2𝐸3𝐸4𝐸5
0.116 0.069 0.172 0.049 0.259
𝐸6𝐸7𝐸8𝐸9𝐸10
0.110 0.058 0.058 0.048 0.061
Credit𝐸1𝐸2𝐸3𝐸4𝐸5
0.045 0.647 0.003 0.016 0.004
𝐸6𝐸7𝐸8𝐸9𝐸10
0.023 0.238 0.009 0.006 0.007
In this section, we statistics the proportion of edges in existing
semi-synthetic datasets in Table 8, and then specifically analyze
why the graph structure is not sufficiently useful. Then we obtained
three new semi-synthetic datasets by adjusting the number of edges
following the framework stated in Section 5.1.1, and the proportion
of edges in new datasets can be found in Appendix 11.
New German Dataset: As shown in Table 9, the number of intra-
label group edges in the German dataset is generally less than the
inter-label group edges, except for 𝐸2, and the share of 𝐸7is too large.
As claimed in Table 5, these problems prevent GNNs from learning
effective information through the message-passing mechanism,
which leads to lower accuracy. To adjust the proportion of intra- and
inter-label group edges and to maintain the difference in accuracy
between groups, we randomly reduced 4,000 𝐸7and randomly
added 500𝐸1, 1,000𝐸3, and 1,000𝐸4to obtain a new German dataset.
This new German dataset improves the usefulness of the graph
structure while preserving the bias in the graph structure.
New Bail Dataset: Compared with other datasets, the difference
in the proportion of various edges in bail is small, which leads to
another problem the features aggregated by GNNs through such an
average graph structure would be not discriminative. So it appears
that the classification performance of GCN is much worse than
that of MLP in Table 3. For GNNs to better aggregate features, we
randomly reduced 40,000 𝐸5and randomly added 15,000 𝐸2, 20,000
𝐸3, and 4,000𝐸4. We expect GNNs would perform better on the
new bail dataset.New Credit Dataset: Similar to the case of the German dataset, a
large proportion of 𝐸7in the Credit dataset decreases the perfor-
mance of GNNs. We randomly reduced 30,000 𝐸7to construct a
graph structure with a more reasonable proportion of intra- and
inter-label group edges. The following GNN classification results
demonstrate the effectiveness of these simple adjustments.
5.3 Real-world Datasets from Twitter
We have constructed two novel datasets by leveraging the Twitter
API, offering insights into real-world social dynamics and biases.
These datasets, detailed below, serve as the foundation for our
studies on bias mitigation and the robustness of predictive models.
Sport Dataset: Derived from Twitter, this dataset focuses on ath-
letes in the NBA and MLB. We mapped players to their Twitter
accounts, using these accounts as nodes. Edges represent following
relationships between players. The sensitive attribute under consid-
eration is the players’ race, categorized as either black or white. The
objective is to predict the sport of a player (NBA or MLB) without
bias influenced by racial attributes. For node features, we aggre-
gated the first five tweets from each player’s account and utilized
average of their BERT embeddings [6] as feature representations.
Occupation Dataset: This dataset also originates from Twitter,
with nodes representing users and edges indicating follow relation-
ships. The focus is on users identified within the fields of computer
science or psychology. User selection was stratified across multiple
layers: starting from a randomly chosen set of users (1st layer), we
expanded the dataset by including their followers (2nd layer) and
repeated this process up to six layers to ensure diversity. The sensi-
tive attribute here is gender, with the aim to predict a user’s field
of work without gender-based bias. Node features were derived
similarly to the Sport dataset, using the mean of BERT embeddings
from the users’ tweets.
6 BENCHMARKING ON NEW DATASETS
This section outlines our empirical investigation designed to assess
the utility and integrity of the newly developed datasets. The sta-
tistics details of these new datasets are shown in Table 9. Our goal
is to scrutinize the datasets through a series of experiments aimed
at addressing the following pivotal questions:
•(RQ 1) Are the proposed datasets capable of yielding signifi-
cant insights and enhancing predictive performance within their
graph structures?
•(RQ 2) Does the graph structure exhibit biased information,
necessitating a proficient model that can adeptly harness the
graph’s structure while also mitigating any inherent biases?
•(RQ 3) Can we gain insights into the commonly used methods
with our datasets?
This section delineates our experimental evaluation, conducted
to ascertain the efficacy of our newly introduced datasets in facili-
tating fair graph benchmarking. Our experiments are designed to
benchmark existing models, thereby providing insights into their
performance when applied to diverse and challenging scenarios.
6.1 Experimental Setup
In our benchmarking, we selected key fair graph learning methods,
including FairGNN [ 4], which uses an adversarial method with a
5608KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaowei Qian et al.
Table 9: Statistics of new datasets.
Dataset
Syn-1 Syn-2 New German New Bail New Credit Sport Occupation
#
of nodes 5,000 5,000 1,000 18,876 30,000 3,508 6,951
# of edges 34,363 44,949 20,242 31,5870 1,121,858 136,427 44,166
# of features 48 48 27 18 13 768 768
Sensitive attribute 0/1 0/1 Gender (Male/Female) Race (Black/White) Age ( <25/>25) Race (White/Black) Gender (Male/Female)
Label 0/1 0/1 Good/bad Credit Bail/no bail Payment default/no default NBA/MLB Psy/CS
Average degree 13.75 17.98 41.48 34.47 75.79 78.78 13.71
Group Ratio1,218/1,244
1,239/1,2991,078/1,384
1,408/1,130191/499
109/2015,457/3,860
6,315/3,2445,906/21,409
730/1,955136/1,627
1,627/1181,751/1,699
2,951/550
# of𝐸1∪𝐸2∪𝐸3∪𝐸4 17,225 21,590 12,806 170,611 1,013,100 111,736 26,138
# of𝐸5∪𝐸6 6,319 6,238 2,456 75,137 38,592 18,146 15,902
# of𝐸7∪𝐸8 6,198 10,750 3,192 36,210 51,222 1,462 1,154
# of𝐸9∪𝐸10 4,621 6,371 1,788 33,912 18,944 5,083 972
Table 10: Results of models for new datasets.
Dataset Method ACC(↑) AUC(↑) F1(↑) ΔSP(↓) ΔEO(↓)
Syn-1MLP 78.84 ±0.34 87.25 ±0.28 80.19 ±0.40 1.76 ±1.10 4.35 ±1.89
GCN 86.96 ±0.66 94.63 ±0.05 87.90 ±0.58 10.97 ±0.85 10.37 ±1.32
FairGNN 85.06 ±0.42 93.07 ±0.22 85.87 ±0.45 1.77 ±1.03 2.92 ±1.98
NIFTY 80.22 ±2.23 88.52 ±2.23 81.45 ±2.23 15.77 ±6.1 15.65 ±7.24
Syn-2MLP 71.04 ±0.79 78.51 ±0.66 72.59 ±1.05 10.52 ±1.04 7.57 ±1.31
GCN 78.98 ±0.58 86.69 ±0.20 80.32 ±0.57 22.04 ±1.44 24.09 ±2.75
FairGNN 74.74 ±1.04 82.82 ±0.64 77.19 ±1.02 1.17 ±0.43 1.79 ±1.25
NIFTY 73.06 ±0.72 80.10 ±0.33 74.46 ±0.93 31.87 ±3.84 31.47 ±3.76
New GermanMLP 71.36 ±1.35 72.45 ±0.75 82.29 ±0.25 7.25 ±6.35 3.28 ±3.16
GCN 82.08 ±1.55 87.19 ±1.04 88.23 ±1.01 24.52 ±3.50 6.20 ±4.20
FairGNN 77.52 ±3.98 83.81 ±3.54 85.34 ±2.40 18.85 ±7.69 6.68 ±3.63
NIFTY 74.4±4.40 80.13 ±3.36 83.50 ±1.95 4.33 ±3.01 2.21 ±1.74
New BailMLP 88.13 ±0.62 90.69 ±0.74 82.43 ±1.06 0.76 ±0.54 4.42 ±0.33
GCN 92.21 ±1.33 96.37 ±2.47 89.84 ±2.14 9.99 ±1.48 4.45 ±2.11
FairGNN 91.61 ±1.52 95.78 ±1.16 89.06 ±1.86 9.25 ±0.65 4.99 ±1.35
NIFTY 80.51 ±5.85 87.02 ±1.43 75.45 ±3.41 5.4 ±2.22 3.67 ±0.96
New CreditMLP 76.91 ±1.94 71.36 ±0.50 86.32 ±2.48 2.26 ±4.51 1.78 ±3.56
GCN 82.61 ±1.06 91.52 ±1.67 87.80 ±0.74 16.86 ±1.38 24.27 ±1.54
FairGNN 79.02 ±0.94 84.27 ±2.69 86.98 ±0.56 6.76 ±6.93 8.28 ±8.58
NIFTY 75.73 ±2.37 76.41 ±1.18 84.11 ±2.58 6.85 ±4.09 5.37 ±3.89
SportMLP 66.92 ±1.64 73.46 ±1.80 66.87 ±1.69 30.44 ±3.43 9.00 ±3.17
GCN 95.16 ±0.73 98.67 ±0.35 95.22 ±0.70 81.13 ±1.10 3.46 ±1.17
FairGNN 94.53 ±0.73 98.71 ±0.41 94.41 ±0.83 78.49 ±1.80 2.21 ±1.91
NIFTY 88.95 ±4.69 96.84 ±0.45 89.59 ±3.87 70.21 ±7.14 4.12 ±1.60
OccupationMLP 78.59 ±0.55 85.18 ±0.43 61.90 ±1.97 21.43 ±1.61 13.08 ±2.64
GCN 81.70 ±0.56 87.89 ±0.47 69.96 ±1.72 25.24 ±0.95 16.04 ±1.76
FairGNN 80.92 ±0.42 86.47 ±0.19 67.21 ±0.50 22.75 ±0.65 14.88 ±2.04
NIFTY 78.09 ±1.11 83.27 ±0.59 60.25 ±4.88 20.89 ±2.36 17.52 ±1.53
sensitive feature estimator for fairness, and NIFTY [ 1], employing a
novel augmentation for counterfactual fairness through contrastive
learning. Both methods are based on GCN [ 13] to leverage graph
structure. We also compared these with a standard GCN to under-
stand the role of graph topology and an MLP to gauge the benefit
of incorporating graph structure.To ensure a fair and comprehensive comparison, we meticulously
fine-tuned the hyperparameters for each model, tailored to their
optimal performance on our datasets. The specifics regarding these
configurations are provided in Appendix B.1. Aligning with the
setup in NIFTY [ 1], our experiments utilize a one-layer GCN for
encoding, complemented by a linear layer that functions as both
classifier and discriminator. The data partition strategy is consistent
5609Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark KDD ’24, August 25–29, 2024, Barcelona, Spain.
with established protocols [ 1,4], ensuring comparability. To account
for variability in initialization, we report the average results over
five runs, each with a unique random seed.
6.2 Evaluation and Results
Table 10 presents the performance outcomes of various methods
across the datasets we introduce. These datasets are specifically
designed to test the capabilities of fair graph learning approaches,
revealing several key attributes conducive to their evaluation. For
RQ 1, a comparison between MLP and GCN demonstrates signif-
icant enhancements in predictive accuracy, as evidenced by im-
provements in ACC, AUC, and F1 score across all seven datasets.
This distinction is particularly notable when juxtaposed with the
outcomes from semi-synthetic and real-world datasets discussed in
Section 4, underscoring the value of incorporating graph structures
into the analysis for augmenting model performance.
ForRQ 2, an analysis of MLP and GCN’s performance based
on fairness metrics ΔSPandΔEOreveals that incorporating graph
structures often leads to a significant reduction in fairness across all
seven datasets. This observation underscores the inherent trade-off
in using graph data: while it can enhance model performance, it also
risks compromising fairness. Nonetheless, a comparative assess-
ment of fairness-focused algorithms demonstrates notable improve-
ments in fairness metrics for most datasets. Specifically, FairGNN
shows comprehensive enhancements across all indicators—ACC,
AUC, F1 score, ΔSP, andΔEO—on Syn-1 and Syn-2. Similarly, NIFTY
exhibits parallel improvements on the New German dataset. This
trend suggests the feasibility of leveraging graph structures to boost
predictive accuracy while simultaneously mitigating bias. Such ca-
pability is crucial for fairness benchmarking datasets, serving as
a critical measure to evaluate a model’s ability to exploit graph
data beneficially without sacrificing fairness. These findings indi-
cate that with carefully designed fair graph learning approaches,
it is possible to balance both predictive performance and fairness
objectives effectively.
In the context of RQ 3, it is apparent that current fairness
methodologies struggle to consistently excel across all datasets.
This challenge sets a new benchmark, urging further innovation in
model development. For example, in the New Bail dataset, FairGNN
leverages graph structures to surpass MLP in terms of ACC, AUC,
and F1 scores, yet it must navigate the inherent biases within the
graph data, leading to a decrease in ΔSPandΔEO. While FairGNN
manages to mitigate bias more effectively than the baseline GCN,
this adjustment comes at the cost of reduced predictive accuracy.
Conversely, NIFTY faces difficulties in optimally exploiting graph
information, resulting in performance deficits across all metrics,
even trailing behind MLP. Our evaluation presents a set of chal-
lenging tasks, making it difficult for any single method to excel
across all datasets. This situation offers substantial opportunities
for the development of strong fair graph learning methods, paving
the way for new milestones in the field.
7 CONCLUSION
In conclusion, our exploration into fair graph learning underscores
the critical importance of representative datasets for evaluating the
performance of fair graph learning methods. Through this work, wehave identified a significant gap in the quality and applicability of
existing semi-synthetic and real-world datasets. Our findings reveal
that, in many cases, simple models such as MLPs can surpass more
complex GNNs when the datasets lack meaningful graph structures.
To address these shortcomings, we have developed a comprehensive
suite of synthetic, semi-synthetic, and real-world datasets designed
with the explicit aim of facilitating a fair and rigorous evaluation
of fair graph learning methods. These datasets are carefully crafted
to encompass critical graph structures and bias information, chal-
lenging models to not only leverage graph structures for enhanced
prediction accuracy but also to effectively address and mitigate
bias inherent in the data. We introduce a unified framework for
analyzing the edge generation probability to fairness metrics. Based
on this, we provide controllable bias parameters in synthetic and
semi-synthetic datasets, allowing researchers to tailor the datasets
to specific research needs and bias considerations. Our systematic
evaluation of these newly proposed datasets has yielded extensive
experimental insights. This work lays the foundation for future
progress in fair graph learning, promoting the creation of models
that effectively harness graph structures while prioritizing fairness.
We introduce challenging tasks that test the limits of any single
method across diverse datasets, thereby creating significant oppor-
tunities for developing fair graph learning methods and setting new
benchmarks in the field.
8 ACKNOWLEDGEMENTS
This material is based upon work supported by the National Science
Foundation (NSF) Grant #2406648 and #2406647, Army Research
Office (ARO) under grant number W911NF-21-1- 0198, Department
of Homeland Security (DHS) CINA under grant number E205949D,
and Cisco Faculty Research Award.
REFERENCES
[1]Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. 2021. Towards
a Unified Framework for Fair and Stable Graph Representation Learning.
arXiv:2102.13186 [cs.LG]
[2]Siddhant Arora. 2020. A survey on graph neural networks for knowledge graph
completion. arXiv preprint arXiv:2007.12374 (2020).
[3]Enyan Dai, Limeng Cui, Zhengyang Wang, Xianfeng Tang, Yinghan Wang, Mon-
ica Cheng, Bing Yin, and Suhang Wang. 2023. A unified framework of graph
information bottleneck for robustness and membership privacy. In KDD.
[4] Enyan Dai and Suhang Wang. 2021. Say no to the discrimination: Learning fair
graph neural networks with limited sensitive attribute information. In Proceedings
of the 14th ACM International Conference on Web Search and Data Mining. 680–
688.
[5]Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu,
Jiliang Tang, and Suhang Wang. [n. d.]. A Comprehensive Survey on Trustworthy
Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability. 1, 1
([n. d.]).
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[7]Yushun Dong, Ninghao Liu, Brian Jalaian, and Jundong Li. 2022. EDITS: Modeling
and Mitigating Data Bias for Graph Neural Networks.. In The Web Conference
(WWW). ACM, 1259–1269.
[8]Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in
theoretical computer science conference. 214–226.
[9]Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In The world wide web
conference. 417–426.
[10] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh
Venkatasubramanian. 2015. Certifying and removing disparate impact. In pro-
ceedings of the 21th ACM SIGKDD international conference on knowledge discovery
and data mining. 259–268.
5610KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaowei Qian et al.
[11] William L. Hamilton. 2020. Graph Representation Learning. Vol. 14. Morgan
&amp; Claypool Publishers. 1–159 pages.
[12] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. Advances in neural information processing systems 29 (2016).
[13] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[14] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfac-
tual fairness. Advances in neural information processing systems 30 (2017).
[15] Xuan Lin, Zhe Quan, Zhi-Jie Wang, Tengfei Ma, and Xiangxiang Zeng. 2020.
KGNN: Knowledge Graph Neural Network for Drug-Drug Interaction Prediction..
InIJCAI, Vol. 380. 2739–2745.
[16] Jing Ma, Ruocheng Guo, Mengting Wan, Longqi Yang, Aidong Zhang, and Jun-
dong Li. 2022. Learning fair node representations with graph counterfactual
fairness. In Proceedings of the Fifteenth ACM International Conference on Web
Search and Data Mining. 695–703.
[17] David Madras, Elliot Creager, Toniann Pitassi, and Richard S. Zemel. 2018. Learn-
ing Adversarially Fair and Transferable Representations.. In ICML. 3381–3390.
[18] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan. 2021. A survey on bias and fairness in machine learning. ACM com-
puting surveys (CSUR) 54, 6 (2021), 1–35.
[19] Harjit Singh Sekhon, Sanjit Kumar Roy, and James Devlin. 2016. Perceptions of
fairness in financial services: an analysis of distribution channels. International
Journal of Bank Marketing 34, 2 (2016), 171–190.
[20] William Shiao, Zhichun Guo, Tong Zhao, Evangelos E. Papalexakis, Yozen
Liu, and Neil Shah. 2023. Link Prediction with Non-Contrastive Learning.
arXiv:2211.14394 [cs]
[21] Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng Chua.
2022. Causal Attention for Interpretable and Generalizable Graph Classification.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. ACM, Washington DC USA, 1696–1705. https://doi.org/10.1145/
3534678.3539366
[22] Harini Suresh and John V Guttag. 2019. A framework for understanding unin-
tended consequences of machine learning. arXiv preprint arXiv:1901.10002 2, 8
(2019).
[23] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks.. In International
Conference on Learning Representations (ICLR).
[24] Song Wang, Kaize Ding, Chuxu Zhang, Chen Chen, and Jundong Li. 2022. Task-
Adaptive Few-Shot Node Classification. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. ACM, Washington DC USA,
1910–1919. https://doi.org/10.1145/3534678.3539265
[25] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2021. Mixup
for Node and Graph Classification. In Proceedings of the Web Conference 2021.
ACM, Ljubljana Slovenia, 3663–3674. https://doi.org/10.1145/3442381.3449796
[26] Yu Wang, Yuying Zhao, Yushun Dong, Huiyuan Chen, Jundong Li, and Tyler
Derr. 2022. Improving fairness in graph neural networks via mitigating sensitive
attribute leakage. In Proceedings of the 28th ACM SIGKDD conference on knowledge
discovery and data mining. 1938–1948.
[27] Jiacheng Xiong, Zhaoping Xiong, Kaixian Chen, Hualiang Jiang, and Mingyue
Zheng. 2021. Graph neural networks for automated de novo drug design. Drug
Discovery Today 26, 6 (2021), 1382–1393.
[28] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. 2022. Explainability in Graph
Neural Networks: A Taxonomic Survey. IEEE Transactions on Pattern Analysis and
Machine Intelligence (2022), 1–19. https://doi.org/10.1109/TPAMI.2022.3204236
[29] Seongjun Yun, Seoyoon Kim, Junhyun Lee, Jaewoo Kang, and Hyunwoo J Kim.
[n. d.]. Neo-GNNs: Neighborhood Overlap-Aware Graph Neural Networks for
Link Prediction. ([n. d.]).
[30] Richard S. Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork.
2013. Learning Fair Representations.. In International Conference on Machine
Learning (ICML). 325–333.
[31] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural
networks. NeurIPS 31 (2018).
[32] Xiaojuan Zhang, Jun Fu, and Shuang Li. 2023. Contrastive Disentangled Learning
on Graph for Node Classification. arXiv:2306.11344 [cs]
[33] Zhao Zhang, Fuzhen Zhuang, Hengshu Zhu, Zhiping Shi, Hui Xiong, and Qing He.
2020. Relational graph neural network with hierarchical attention for knowledge
graph completion. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 34. 9612–9619.
[34] Tong Zhao, Gang Liu, Daheng Wang, Wenhao Yu, and Meng Jiang. 2022. Learning
from Counterfactual Links for Link Prediction.. In International Conference on
Machine Learning (ICML). 26911–26926.
A DETAILED ISSUES
A.1 Dataset Details
A.1.1 Detailed Existing Datasets. Here we present a detailed de-
scription of six wildly-used datasets we used to validate our pro-
posed issues as follows:•German Credit (German): This dataset models clients as
nodes, where edges reflect a high similarity in credit account
activities. The objective is to classify individuals into high
or low-credit risk categories, considering gender as the sen-
sitive attribute.
•Recidivism (Bail): It comprises nodes representing defen-
dants who were released on bail between 1990 and 2009.
Edges are drawn between nodes with similar criminal records
and demographic characteristics. The classification challenge
involves predicting bail status based on the sensitive attribute
of race.
•Credit Defaulter (Credit): In this dataset, nodes symbolize
credit card users, connected by edges that indicate similarity
in purchasing and payment behaviors. The classification goal
is to identify users likely to default on payments, with age
serving as the sensitive attribute.
•Pokec: A widely recognized dataset from the Slovak social
network, anonymized in 2012, segmented into two subsets:
Pokec-z and Pokec-n. These subsets represent user profiles
from two significant regions within Slovakia, designated by
their respective provinces. The datasets use the geographical
region of the users as the sensitive attribute, aiming to predict
the employment sector of the users.
•NBA: Comprising data on roughly 400 NBA players, this
dataset uses a player’s nationality (categorized into U.S. or
non-U.S.) as the sensitive attribute. The dataset constructs a
social graph of NBA players through their interactions on
Twitter, with the predictive task focusing on determining if
a player’s salary is above or below the league median.
A.1.2 Detailed New datasets. The proportion of different edges in
new semi-synthetic dataset is shown in Table 11.
Table 11: The proportion of different edges in new semi-
synthetic datasets.
New German𝐸1𝐸2𝐸3𝐸4𝐸5
0.088 0.331 0.082 0.131 0.028
𝐸6𝐸7𝐸8𝐸9𝐸10
0.093 0.067 0.091 0.038 0.050
New Bail𝐸1𝐸2𝐸3𝐸4𝐸5
0.115 0.115 0.233 0.077 0.129
𝐸6𝐸7𝐸8𝐸9𝐸10
0.108 0.057 0.058 0.047 0.060
New Credit𝐸1𝐸2𝐸3𝐸4𝐸5
0.057 0.822 0.004 0.021 0.006
𝐸6𝐸7𝐸8𝐸9𝐸10
0.029 0.034 0.012 0.008 0.009
A.2 Different Model Selection Strategy
We compare the different model selection strategies under con-
sistent conditions among several datasets. As shown in Table 12
5611Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 13: Compare the performance of NIFTY using different
strategies on semi-synthetic datasets.
Dataset Strategy A
CC AUC F1 ΔSP ΔEO
GermanStrategy
170.00
65.49 81.84 1.67 0.21
Strategy
271.20
67.86 82.44 1.09 2.42
Strategy
365.60
68.50 73.46 6.86 0.21
Ours 72.00
70.32 83.09 1.47 0.11
BailStrategy
181.10
81.34 68.46 4.85 4.41
Strategy
280.12
79.73 67.32 4.77 4.02
Strategy
370.31
83.37 67.79 2.06 2.52
Ours 81.65
81.81 71.00 4.88 4.05
Cr
editStrategy
168.51
69.15 78.01 10.08 9.30
Strategy
274.20
69.19 83.24 8.76 6.68
Strategy
360.35
69.28 69.43 13.31 13.47
Ours 74.67
69.29 83.64 8.72 6.17
and Table 13, the performance gap caused by the different strate-
gies illustrates the importance of standardizing the model selection
strategy.
B EXPERIMENTAL SETTINGS
B.1 Hyperparameter Selection
Since different methods have different model architectures, their
hyperparameters are various and are described respectively as fol-
lows:
MLP: the number of layers {2,3,4,5}, the number of hidden unit
16, learning rate{1𝑒−2,1𝑒−3,1𝑒−4}, weight decay{1𝑒−4,1𝑒−5},
dropout{0,0.5,0.8}.GCN: the number of layers {1,2,3}, the number of hidden unit 16,
learning rate{1𝑒−2,1𝑒−3,1𝑒−4}, weight decay{1𝑒−4,1𝑒−5},
dropout{0,0.5,0.8}.
FairGNN: the number of hidden unit 32, learning rate {1𝑒−2,1𝑒−
3,1𝑒−4}, weight decay{1𝑒−4,1𝑒−5}, dropout{0,0.5,0.8}, regu-
larization coefficients 𝛼{4,5,50,100}and𝛽{0.01,1,5,20}.
NIFTY: the number of hidden unit 16, project hidden unit 16, drop
edge rate 0.001, drop feature rate 0.1, learning rate {1𝑒−2,1𝑒−3,1𝑒−
4}, weight decay{1𝑒−4,1𝑒−5}, dropout{0,0.5,0.8}, regularization
coefficient{0.2,0.4,0.6,0.8}.
Table 12: Compare the performance of FairGNN using differ-
ent strategies on real-world datasets.
Dataset Strategy A
CC AUC F1 ΔSPΔEO
Poke
c_zStrategy
166.76
73.11 62.44 1.15 1.51
Strategy
268.59
73.85 67.97 1.02 2.54
Strategy
368.71
73.91 68.36 1.34 3.29
Ours 68.24
73.47 68.15 0.84 0.91
Poke
c_nStrategy
168.91
73.25 65.56 8.08 9.99
Strategy
268.35
73.24 65.70 8.11 10.42
Strategy
368.32
73.37 66.51 8.98 9.87
Ours 68.91
73.25 65.56 8.08 9.99
NBAStrategy
169.01
76.73 73.60 1.78 0.99
Strategy
269.01
78.16 68.87 6.98 16.31
Strategy
347.89
79.80 0.00 0.00 0.00
Ours 70.89
76.93 74.80 1.42 0.99
5612