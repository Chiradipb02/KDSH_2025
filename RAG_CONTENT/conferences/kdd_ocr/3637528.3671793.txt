Binder: Hierarchical Concept Representation through Order
Embedding of Binary Vectors
Croix Gyurek∗
University of Waterloo
Ontario, CA
cgyurek@uwaterloo.caNiloy Talukder∗
Indiana University at Indianapolis
Indianapolis, Indiana, USA
ntalukde@iu.eduMohammad Al Hasan
Indiana University at Indianapolis
Indianapolis, IN, USA
alhasan@iu.edu
ABSTRACT
For natural language understanding and generation, embedding
concepts using an order-based representation is an essential task.
Unlike traditional point vector based representation, an order-based
representation imposes geometric constraints on the representa-
tion vectors for explicitly capturing various semantic relationships
that may exist between a pair of concepts. In existing literature,
several approaches on order-based embedding have been proposed,
mostly focusing on capturing hierarchical relationships; examples
include vectors in Euclidean space, complex, Hyperbolic, order, and
Box Embedding. Box embedding creates region-based rich repre-
sentation of concepts, but along the process it sacrifices simplicity,
requiring a custom-made optimization scheme for learning the rep-
resentation. Hyperbolic embedding improves embedding quality
by exploiting the ever-expanding property of Hyperbolic space, but
it also suffers from the same fate as box embedding as gradient
descent like optimization is not simple in the Hyperbolic space. In
this work, we propose Binder, a novel approach for order-based
representation. Binder uses binary vectors for embedding, so the
embedding vectors are compact with an order of magnitude smaller
footprint than other methods. Binder uses a simple and efficient
optimization scheme for learning representation vectors with a
linear time complexity. Our comprehensive experimental results
show that Binder is very accurate, yielding competitive results on
the representation task. But Binder stands out from its competi-
tors on the transitive closure link prediction task as it can learn
concept embeddings just from the direct edges, whereas all existing
order-based approaches rely on the indirect edges. In particular,
Binder achieves a whopping 70% higher F1-score than the second
best method (98.6% vs 29%) in our largest dataset, WordNet Nouns
(743,241 edges), when using only direct edges during training.
CCS CONCEPTS
•Mathematics of computing →Combinatorial optimization ;
•Information systems →Ontologies ;•Computing methodolo-
gies→Discrete space search ;Randomized search ;Semantic
∗Both authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671793networks ;Ontology engineering ;Lexical semantics ;Informa-
tion extraction .
KEYWORDS
Concept Graph; Hierarchical Embedding; Order Embedding; Binary
Vector Embedding
ACM Reference Format:
Croix Gyurek, Niloy Talukder, and Mohammad Al Hasan. 2024. Binder:
Hierarchical Concept Representation through Order Embedding of Binary
Vectors. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3637528.3671793
1 INTRODUCTION
Ontologies transcribe the knowledge of a domain through formal
listing of concepts along with the knowledge of various semantic
relations that may exist among those concepts. The most important
among these relations is hypernym-hyponym, which captures the
is-arelation between a specialized and a general concept of a do-
main. Such knowledge are essential for achieving optimal results in
various natural language generation tasks, such as image caption
generation [ 20], question-answering [ 43], and taxonomy genera-
tion [ 26]. For example, using an appropriate ontology, an image
caption generator can opt for a generic text “a person is walking
a dog” instead of a more informative text, “a woman is walking
her dog”, if the model is not very confident about the gender of the
person. However, building a large conceptual ontology for a domain
is a difficult task requiring gigantic human effort, so sophisticated
machine learning models, which can predict is-alinks between
pairs of concepts in an ontology, is of high demand.
Predicting is-a relationship between two concepts in an ontology
can be viewed as a link prediction task in a concept graph or an
ontology chart. Although link prediction [ 17] is a well-studied task,
predicting links in a concept graph did not receive much attention
by the NLP or Machine Learning researchers. The closest works
that exist are on predicting links in a knowledge graph [ 12,22], but
such works are customized for the Resource Description Framework
(RDF) type of data. Lately, node embedding using shallow [ 15,31,
37] or deep [ 16,27,33,43] neural networks have shown improved
performance for solving link prediction tasks. Most of these works
consider undirected graphs, so they are not suitable for asymmetric
concept graphs. Some node embedding works [ 5,19,36,38] also
embed knowledge graphs, where embedding of head, tail, and rela-
tion nodes are learned, but such works, again, are only suitable for
RDF based data representation.
In recent years, some works have been proposed for embedding
concepts in such a way that the embedding vectors would capture
 
980
KDD ’24, August 25–29, 2024, Barcelona, Spain Croix Gyurek, Niloy Talukder and Mohammad Al Hasan
semantic relationships among the concepts. One of the earliest ef-
forts in this direction is order embedding (OE) [ 21,39]. The main
idea of OE is to embed the concepts in the positive cone, Rd+. In this
embedding, if ais-ab, then their corresponding embedding vectors
satisfy f(b)≤f(a)element-wise. In this way, a generic concept
hovers closer to the origin with a smaller norm than the associated
specialized concept. In another line of works [ 13,23,28,29], which
focus on embedding trees, DAGs or tree-like graphs, hyperbolic
space is used instead of Euclidean space. In Hyperbolic space two
non-intersecting straight lines diverge, henceforth allowing for the
embedding of more objects along the periphery, which is benefi-
cial for embedding a tree structure which has exponentially more
nodes at a higher depth. The third line of work, known as box em-
bedding [ 4,8,9,40], deviates from vector (point) embedding, and
instead uses a rectangular region for embedding a concept. This
representation is richer as it both helps embedding order relation
and part-of relations between concepts, or overlapping concepts,
thereby overcomes some of the limitations of the earlier approaches.
Other existing approaches for embedding generic graph-based re-
lational data use simple [ 3], bilinear [ 30], or complex [ 38] vectors
and make use of inner product and distance metric in Euclidean
space to capture relations.
In this work, we propose an order embedding model that is
simple, elegant and compact. Our idea is to use binary vectors for
order embedding, i.e. for each entity a,f(a)=a∈{0,1}d. In other
words, we embed each object at a vertex of a d-dimensional non-
negative unit hypercube, where dis a user-defined parameter. The
overall approach is simple, as for denoting ais-ab, we require that
f(b)j=1=⇒f(a)j=1,∀j∈[1 :d], i.e., along any embedding
dimension j, ifbjis 1, then ajmust be 1. The idea is fairly intu-
itive; if we consider each dimension denoting some latent property
which make something b, given that ais-ab,aalso have those
properties. Since it uses bits for embedding, the embedding vectors
are compact with an order-of-magnitude smaller memory footprint
(see Section 3.8) compared to other methods. Our embedding idea
is elegant as it captures the is-a relation through intent-extent phi-
losophy of formal concept analysis (FCA) [ 14], which is a principal
way of deriving a concept hierarchy.
The major challenge for our proposed embedding idea is finding
an effective optimization algorithm for learning the embedding, as
we deviated away from continuous Euclidean space and embraced
a combinatorial space. In this sense, given the training data (a col-
lection of hyponym-hypernym pairs), learning the embedding of
the objects in the training data becomes a classical combinatorial
feasibility task, a known NP-complete problem. We use a random-
ized local search algorithm inspired by stochastic gradient descent
for solving this problem. In each epoch, our optimization method
probabilistically flips the bits of the embedding vectors, where the
flipping probability is computed from the “gradient” value for each
bit position. The proposed optimization method is innovative and
novel as it consists of computing a proxy of gradient for a binary
space (Section 2.4), and then use that for computing a flip prob-
ability (Section 2.5). Our optimization algorithm is very fast; the
overall computational complexity is O(ndT(|P|+|N|)), where nis
the number of entities, dis the embedding dimensions, Tis the
number of epochs, Pis the set of positive is-a constraints, and Nisthe set of negative is-a constraints. The complexity is linear with
respect to each variable. We claim the following contributions:
(1)We propose Binder1, a novel order embedding approach which
embeds entities at vertices of a d-dimensional hypercube. We show
thatBinder is ideal for finding representation of entities or concepts
with hyponym-hypernym relationships. Binder is simple, compact,
efficient, and has better generalization capacity over transitive edges
than existing methods in a transductive setting.
(2)Binder uses a novel local search based optimization algorithm
for solving the embedding learning task. The proposed algorithm
is simple, efficient and effective, and a proxy of gradient descent
for the combinatorial space.
(3)Experiments on 6 benchmark datasets show that Binder exhibits
superior performance over the existing state-of-the-art algorithms
on transitive closure link prediction tasks.
2 BINARY ORDER EMBEDDING (BINDER)
2.1 Motivation
We solve the following task: From a collection of is-a relations
between a pair of entities, obtain embedding of the entities such
that embedding vectors geometrically capture the order imposed
through the is-a relation. For representing xis-ay, one earlier
work [ 39] has imposed order by requiring yi≤xi,∀i∈[1 :d]for
embedding vectors xandyin real space. BINDER uses a similar
idea, but instead it uses binary space, in which yi≤xibecomes
yi=⇒xi. Implication obeys the transitive property, so BINDER’s
binary representation works well for is-a relationship, which is
transitive. Binder has the following benefits:
(1) Binary representations are compact, often taking order of mag-
nitude less memory than embeddings in real space, and is compu-
tationally efficient (demonstrated in Section 3.8).
(2) Binary representation can immediately provide representation
of concepts that can be obtained by logical operation over the
given concept vectors. For instance, given vectors for the concepts
“vehicle” and “flying”, we can find a vector for instances of “flying
vehicle” by taking the union of the vehicle andflying vectors. Or,
if we have representation vectors for “men’s shoe” and “women’s
shoe”, we can obtain a representation vector for shoe by taking
the intersection of the above two vectors. Such operation can be
extended to other complex Boolean operations.
(3)Binder provides explainable representation vectors in the sense
that we can treat the bit indices as a set of latent properties: a “1”
value at a dimension means that the entity possesses that property,
and a “0” value means that it does not possess that property. A
vector in real space does not provide such intuitive interpretation.
We give a small demonstration of this in Figure 1 in Section 3.3,
where we trained our model on a small lattice. In particular, the
embedding, being a binary matrix, can be thought of as a machine-
learned object-attribute table. The number of 1’s in a concept’s
representation provides an indication of its specificness. Using
domain knowledge, and by observing the distribution of 1’s in a
column, one can further deduce which dimension may represent
which of the properties (intent). In fact, we can think of Binder’s
1The name Binder is an abbreviation of Binary Order Embedding.
 
981Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors KDD ’24, August 25–29, 2024, Barcelona, Spain
embedding as capturing the intent-extent philosophy of Formal
Concept Analysis (FCA) [ 14], which provides a principled approach
for deriving a concept hierarchy from a collection of objects and
their properties.
Above all of these, our experiments show that Binder performs
competitively on the representation task and is prodigiously supe-
rior on the transitive closure link prediction tasks.
2.2 Problem Formulation
Binder embeds each concept athrough a d-dimensional binary vec-
tora, so every concept is embedded at the vertex of a d-dimensional
non-negative unit hypercube, where dis a user-defined parameter.
Ifais-ab, and aandbare their representation, then Binder satisfies
bk=1=⇒ak=1,∀k∈{1, . . . , d}. This idea comes from the
observation that when ais-ab,amust inherit all the properties that
bpossesses. In Binder’s embedding, a ‘1’ in some representation
dimension denotes “having a latent property”: if the j’th bit of bhas
a value of 1, i.e., bpossesses property j, then amust also possess
property j, which is captured by the above requirement.
To learn embedding vectors for a collection of concepts in a
domain, Binder uses a supervised learning approach. Given a set
of concepts Wand partial order concept-pairs P={(a,b):ais-ab},
Binder’s task is to find an embedding function B:W→{0,1}d
such that for all a,b∈W,
(a∩b)=biff(a,b)∈Panda,b (1)
holds; here a=B(a)andb=B(b)are the embedding vectors for
concepts aandb, and∩denotes the bitwise AND operation.
The above learning task is a binary constraint satisfaction prob-
lem (CSP), which is a known NP-Complete task [ 6]. To solve it,
Binder uses a randomized local search algorithm, which is fast and
effective. Note that given a training dataset, P, ifBinder’s embed-
ding solution satisfies all the constraints, then the embedding is
perfect and all the partial order pairs in Pcan be reconstructed from
the embedding with a 100% accuracy. But the goal of our embedding
is not necessarily yielding a 100% reconstruction accuracy on the
training data, rather to perform ”is-a” prediction task on an unseen
test dataset, so we do not strive to solve the CSP task exactly. In
the next section, we discuss Binder’s learning algorithm.
Notations: Italic letters such as a,bdenote entities, while boldface
a,bdenote their embedding vectors. ajdenotes the value at the j’th
position in a. In the algorithms, we use Bto denote the embedding
function and also the complete binary embedding matrix, B[a,:]for
a, and B[a,j]for bit jof said vector. We use ∗to denote element-
wise multiplication; all arithmetic operations are done in ZorR.
Finally, we write pairs in hyponym-hypernym order: (a,b)refers
to the statement “ ais-ab”.
2.3 Training Algorithm
The learning task of Binder is a CSP task, which assigns |W|distinct
binary d-bit vectors to each of the variables in W, such that each
of the constraints in the partial order Pis satisfied. Various search
strategies have been proposed for solving CSP problems, among
which local search and simulated annealing are used widely [ 2].
For guiding the local search, we model this search problem as an
optimization problem, by designing a loss function that measuresthe fitness of an embedding. A simple measurement is the number
of pairs in Pthat violates the constraint in Equation 1. Note that
the constraint is “if and only if”, which means for any pair (a′,b′)
that is not in P, we want a′andb′tonotsatisfy this constraint. If
|W|=n, we have exactly|P|constraints for the positive pairs (we
call these positive constraints ), and n2−n−|P|negative constraints
for the negative pairs. Using these constraints, we compute a simple
loss function—a linear function of the number of violated positive
and negative constraints, Loss=Loss P+Loss N, as shown below:
Loss P=αÕ
(a,b)∈PÕ
j1(aj,bj)=(0,1)(a,b) (2)
Loss N=βÕ
(a′,b′)∈N1∀j(a′
j,b′
j)∈{(0 ,0),(1,0),(1,1)}(a′,b′) (3)
whereαandβare user-defined parameters and 1is the indica-
tor function. Due to the above loss function, Binder’s learning
algorithm relies on the existence of negative pairs N⊆{(a′,b′):
a′is-not-a b′}. If these negative pairs are not provided, we generate
them by randomly corrupting the positive pairs Pas in [ 13,28,39],
by replacing(a,b)∈Pwith(r,b)or(a,r)where ris sampled ran-
domly from the entity set W.
For local search in continuous machine learning, the search space
is explored using some variant of gradient descent. This gradient is
defined as the derivative of the loss function (or an approximation
thereof) with respect to each parameter. With binary vectors, the
parameters are discrete, but we can get a proxy of the “gradient”
by taking the finite difference between the current loss value and
the new loss value after a move is made. In a continuous space,
parameter vectors can be updated by adding or subtracting a delta,
but in discrete space, the new vector is one belonging to the cur-
rent vector’s neighborhood. If the neighborhood is defined by unit
Hamming distance, we can make a neighbor by flipping one bit of
current vector, but for large datasets, this approach will converge
very slowly. In Binder, we randomly select bits to be flipped by
computing a probability from the gradient of each bit position.
Binder’s algorithm solves a combinatorial satisfiability task, a
known NP-complete problem. By computing a gradient and then
utilizing it to decide bit flipping probability, it works as a gradi-
ent descent scheme in the discrete space to minimize the objective
function defined in Equations 2 and 3. Note that the bit flipping prob-
ability decreases gradually as the number of violated constraints
decreases with each subsequent epoch—this shrinks the neigh-
borhood size as the epochs progresses, giving Binder’s learning
algorithm a flavor of local search with simulated annealing.
2.4 Gradient Derivation
Binder’s gradient descent scheme is based on correcting order of
positive pairs by flipping bits, which are chosen randomly with a
probability computed from gradient value. Below we discuss how
gradient is computed.
A sub-concept will share all the attributes (bits set to 1) of the
concept, and possibly contain more attributes. For each positive pair
(a,b)∈Pand each bit index j, we aim to avoid having (aj,bj)=
(0,1), since that would imply adid not inherit attribute jfrom b. On
the other hand, for negative pairs (a′,b′), we aim to create at least
one bit index with(0,1)bit pair. Suggested bit flipping or protecting
 
982KDD ’24, August 25–29, 2024, Barcelona, Spain Croix Gyurek, Niloy Talukder and Mohammad Al Hasan
Table 1: Logic truth table to flip bits in positive (first three
columns) and negative (last three columns) pairs
ajbjais-ab a′
jb′
ja′is-not-a b′
0 0 Protect bj 0 0 Flipb′
j
0 1 Flip either bit 0 1 Protect both bits
1 0 Don’t care 1 0 Flip both a′
jandb′
j
1 1 Protect aj 1 1 Flipa′
j
Table 2: Logic truth table to calculate positive loss gradient
ajbj∆ajLoss P∆bjLoss PComments
0 0 0−1 Protect bj
0 1 1 1 Flip either (or both) bit
1 0 0 0 Don’t care
1 1−1 0 Protect aj
operations for these requirements are shown in the third and sixth
column of Table 1; if ais-ab, we do not want(0,1)configuration,
hence we protect bjin first row, ajin the fourth row, and flip either
bit in the second row. On the other hand, for anot-is-a b, we want
a(0,1)configuration. If the model currently suggests a′is-ab′(i.e.
there is no jwhere a′
j=0,b′
j=1), we correct this by flipping either
thea′side of a(1,1)pair (fourth row) or the b′side of a(0,0)pair
(first row), as shown in the sixth columns of the same table. Note
that negative samples are needed to avoid trivial embeddings, such
as all words being assigned the zero vector.2
We first derive the gradient of Loss Pwith respect to ajandbj.
We define the дradient ∆ajto be positive if flipping bit ajimproves
the solution according to our loss function, regardless of whether
bitajis currently 0 or 1. As shown in Column 3 of Table 2, flip-
ping ajmakes no change in loss for the first and third rows; but
for the second row, one violation is removed, and for the fourth
row, one new violation is added. For the four bit configurations,
{(0,0),(0,1),(1,0),(1,1)}, the improvement to Loss Pis 0, 1, 0, and
-1, respectively, as shown in Column 3 of Table 2. Column 4 of the
same table shows the value of ∆bjLoss Pcalculated similarly. It is
easy to see that these two sets of gradients values can be written as
bj(1−2aj)and(1−aj)(2bj−1), respectively. Summing this over
all positive pairs,(a,b)∈P, we get the positive gradient vectors
∆aLoss P=αÕ
b:(a,b)∈Pb∗(1−2a) (4)
∆bLoss P=αÕ
a:(a,b)∈P(1−a)∗(2b−1) (5)
For negative loss, we count the number of (a′
j,b′
j)=(0,1)“good”
bit pairs in a negative pair. If Ga′,b′:={j:(a′
j,b′
j)=(0,1)}=0,
then(a′,b′)is a false positive, so we need to flip a bit to make
(a′
j,b′
j)=(0,1). In this case, based on the bit values in Columns 3
and 4 of Table 3, we derived the following algebraic expressions:
2Simply requiring all positive pairs to be distinct is not sufficient, because the model
could converge on some “depth counting” embedding where different subtrees reuse
the same bit patterns. Instead, we rely on negative samples to separate positive pairs,
since if(a,b)is positive then(b,a)will be negative.∆a′Loss N=βÕ
b′:(a′,b′)∈N,
G=0a′∗b′(6)
∆b′Loss N=βÕ
a′:(a′,b′)∈N,
G=0(1−a′)∗(1−b′) (7)
Table 3: Logic truth table to calculate negative loss gradient
a′
jb′
j∆a′
jLoss N∆b′
jLoss N Comments
0 0 0 1 Flipb′
j
0 1 0 0 Don’t care
1 0 0 0 Don’t care
1 1 1 0 Flipa′
j
On the other hand, if Ga′,b′=1, there is no violation, but the
not-is-a relation is enforced by only one bit, so that bit must be
protected. That is, if (a′
j,b′
j)=(0,1)for exactly one j, then we want
the gradient to be−1for that index j(recall that negative gradient
means flipping is bad), and zero gradient for all other indices. This
is true for exactly the bit jsatisfying b′
j(1−a′
j)=1, so we have
∆a′Loss N=−βÕ
b′:(a′,b′)∈N,
G=1b′∗(1−a′) (8)
∆b′Loss N=−βÕ
a′:(a′,b′)∈N,
G=1b′∗(1−a′) (9)
IfGa′,b′>1, no bits need protection, so we set those gradients to 0.
Finally, we add the gradients Loss PandLoss Nto get the final
gradient matrix, ∆=∆++∆−. The gradient computation process is
summarized in Algorithm 1. In this Algorithm, ∆+is the gradient
ofLoss Pand∆−is the gradient of Loss N.∆,∆+and∆−all are
integer-valued matrices of size n×d, where nis the vocabulary
size and dis the embedding dimension. The overall training algo-
rithm is summarized in Algorithm 2. We initialize the embedding
matrix with 0vector. The optimization scheme loops for at most
Tepochs (for loop in Line 4-17), possibly updating bit vectors of
each vocabulary word in each iteration by flipping bits with prob-
ability in proportion to gradient computed through Algorithm 1.
The F1-score metric, defined as2∗T P
2∗T P+F P+F Nis computed at the
end of each epochs and best F1-score is recorded. We exit early if
no improvement is achieved over a sequence of epochs. Its com-
putational complexity is O(ndT|P|∪|N|), which is linear in each
variable.
2.5 Flip probability
In binary embedding, each bit position takes only two values, 0 and
1. Traditional gradient descent, which updates a variable by moving
towards the opposite of gradient, does not apply here. Instead, we
utilize randomness in the update step: bit jof word wis flipped
with a probability based on its gradient, ∆[w,j]. To calculate the
flip probability we used tanh function. For each word wand each
bitjwe compute the gradient ∆[w,j]as in Algorithm 1 and output
FlipProb[ w,j]=max
0,1
2tanh(2(rℓ∆[w,j]+bℓ))	
(10)
 
983Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 1 Gradient Computation
Require: Zero-one Embedding Matrix Bof size n×dinitialized with
all 0; positive Is-A relation set P={(ai,bi)}m
i=1; negative set N=
{(a′i,b′i)}m′
i=1; positive and negative sample weights α,β
1:∆+←zero matrix, same size as B
2:∆−←zero matrix, same size as B
3:for(a,b)∈Pdo ▷∗is element-wise product
4: ∆+[a,:]←∆+[a,:]+B[b,:]∗(1−2B[a,:])
5: ∆+[b,:]←∆+[b,:]+(1−B[a,:])∗(2 B[b,:]−1)
6:end for
7:for(a′,b′)∈Ndo
8: G←B[b′,:]∗(1−B[a′,:]) ▷“good” bit pairs (a vector)
9: ifÍ
jGj=0then ▷false positive, flip something
10: ∆−[a′,:]←∆−[a′,:]+B[a′,:]∗B[b′,:]
11: ∆−[b′,:]←∆−[b′,:]+(1−B[a′,:])∗(1−B[b′,:])
12: else ifÍ
jGj=1then ▷close to being wrong, so protect
13: ∆−[a′,:]←∆−[a′,:]−G▷note only one element of Gis 1
14: ∆−[b′,:]←∆−[b′,:]−G
15: end if
16:end for
17:return ∆:=α∆++β∆−
Algorithm 2 Training Algorithm
Require: Word list W=(w1, . . . , wn); Dimension d; Positive training
setP={(ai,bi)}m
i=1; validation sets V P,V N; gradient weights α,β,
learning params rℓ,bℓ, negative sample multiplier n−(must be even);
maximum epochs T, early stop width ω
1:B←zero matrix of size |W|×d
2:f1←empty list
3:(BestEmbeddin д,Best F 1)←( B,0)
4:fort=1toTdo
5: N←negative samples (Section 2.3)
6: ∆←gradient from Algorithm 1
7: X←max
0,1
2tanh(2( rℓ∆+bℓ))	
▷flip probabilities
8: Flip each bit B[w,j]with (independent) probability X[w,j]
9: f1←f1-score(Evaluate( B,V P,V N))
10: iff1>Best F 1then
11:(BestEmbeddin д,Best F 1)←( B,f1)
12: end if
13: Append f1to list F1
14: ifmean( F1[last 2ωitems])≥ mean( F1[lastωitems]) then
15: Exit Loop ▷Early Exit Criterion if no improvement
16: end if
17:end for
18:return BestEmbeddin д
where the learning rate rℓcontrols the frequency of bit flips, and the
learning bias bℓallows the algorithm to flip bits with zero gradient
(to avoid local optima). The division by 2 prevents the model from
flipping (on average) more than half of the bits in any iteration;
without it, the model would sometimes flip nearly every bit of a
vector, causing it to oscillate. Therefore, we (softly) bound proba-
bilities by1
2to maximize the covariance of the flips of each pair of
vectors. The inside multiplication by 2 is because1
2tanh( 2p)≈p
for small p, so hyperparameter selection is more intuitive: bℓis
(almost exactly) the probability of flipping a neutral bit, and αrℓ
approximates the increase in probability for each positive sample
that would be improved by flipping a given bit (and likewise for βrℓand negative samples). We note that the three hyper parameters
rℓ,α, andβare somewhat redundant, since we can remove rℓand
replaceαandβwith rℓαandrℓβrespectively without changing the
probability. The only reason for using three parameters is to keep
theαandβcomputations in integer arithmetic for faster speed.
2.6 Local Optimality of Binder
The following theorems and lemmas are provided to establish the
claim of Binder’s local optimality.
Lemma 2.1. When bias bℓ=0, for any word a, ifaj, thej’th bit in
the binary representation vector ais updated by Binder’s probabilistic
flipping (keeping the remaining bits the same), the loss function value
decreases in the successive iteration.
Lemma 2.2. When bℓ=0, for any word b, if the bit bjalone is
updated by Binder’s probabilistic flipping, the loss decreases.
Lemma 2.3. When bℓ=0, given a collection of negative data
instances(a′,b′), if the j’th bit in the vector a′orb′(and only that
bit) is updated by Binder’s probabilistic flipping, the loss function
value decreases or remains the same.
Theorem 2.4. Ifbℓ=0and Line 8 of Algorithm 2 is executed
sequentially for each index jfor each of the entities, Binder reaches
a local optimal solution for a 1-Hamming distance neighborhood.
Proofs of these statements are provided in Appendix A. Experi-
mental evidences of convergence to a local optimal value is available
from the loss vs iteration curves shown in Figure 2.
3 EXPERIMENTS AND RESULTS
To demonstrate the effectiveness of Binder, we evaluate its perfor-
mance on two standard tasks for hierarchical representation learn-
ing: Representation and Transitive Closure (TC) Link Prediction.
Our representation experimental setup is identical to [ 4]’s full tran-
sitive closure (TC) representation experiment. Our dataset creation
and experimental setup for TC link prediction task are identical
to [13]. Note that some of the competing works [ 3,21,30,38,39]
did not report results for the representation and the TC link predic-
tion tasks. On the other hand, some competitors [ 13,23] only gave
results on the TC link prediction; in particular [ 23] only reported
results on 80%TC link prediction. The latest work on box embed-
ding [ 4] only reported results on the representation task. Another
box embedding work [ 8] reported results on the ranking (similar
to the reconstruction task) and the 0%TC link prediction task.
Representation is an inverse mapping from embedding vectors
to the list of positive and negative pairs in the full adjacency matrix.
If|W|=nis the vocabulary size, there are n2−nedges in the full
adjacency matrix. A high accuracy in representation testifies for
the capacity of the learning algorithm: it confirms that the learning
algorithm obtains embedding vectors to satisfy the representation
constraints of both the positive and negative pairs. To evaluate
representation, we train models over the complete data set. We then
create a test dataset that includes the entire positive and negative
edge set in the full adjacency matrix (this setup is identical to [ 4]).
We then validate whether for each positive and negative pair, each
model’s embedding vectors satisfy respective constraints or not.
Link Prediction is predicting edges that the learning algorithm
has not seen during training. Since “is-a” relation is transitive,
 
984KDD ’24, August 25–29, 2024, Barcelona, Spain Croix Gyurek, Niloy Talukder and Mohammad Al Hasan
transitive-pair edges are redundant: if (a,b)and(b,c)are positive
pairs, then(a,c)should also be a positive pair. A basic hierarchical
embedding model should be able to deduce this fact automatically.
In link prediction task, we validate each of the models’ ability to
make this deduction. We include the basic (direct) edges in the
train data. These are edges (a,b)with no other word cbetween
them. The remaining non-basic (indirect or transitive) edges are
split into validation and test set. For validation and test set, we use
10 times more negative pairs than positive pairs. We add 0%, 10%,
25% and 50% of the non-basic edges to generate four training sets.
Our experimental setup is identical to [ 13]. We then validate for
each unseen positive and negative pair in the test set, whether each
model’s embedding vectors satisfy respective constraints or not.
The remainder of this section is as follows. In §3.1, we give de-
tails of the datasets. In §3.2, we discuss competing methods and
our reason for why they were chosen. In §3.3, we show a visual
representation of Binder’s embedding in a small dataset. In §3.4, we
discuss our training setup. In §3.5 and §3.6, we compare Binder’s
performance with that of the competing methods. In §3.7, we show
how Binder loss and F1-score converges with the number of it-
erations for both tasks. In §3.8, we show the space advantage of
Binder embeddings over the competitors. We report additional
details of our experimental setup and further results in Appendix B.
In Appendix B.1, we provide low-level details of our training setup
and hyperparameter tuning. In Appendix B.2 we perform an abla-
tion study to show how different components of the loss function
affect Binder’s performance. In Appendix B.3, we demonstrate the
robustness of Binder by showing how changing hyperparameter
values affects its performance. Our code and datasets are publicly
available from https://github.com/ntalukde/BINDER/.
3.1 Datasets
For both representation and prediction task, we evaluate Binder on
6datasets. We downloaded Music and Medical domain dataset from
SemEval-2018 Task 9: Hypernym Discovery. We collected Lex and
random dataset from [ 34], which were constructed by extracting hy-
pernymy relations from WordNet [ 11], DBPedia [ 1], Wiki-data [ 41]
and Yago [ 35]. The largest dataset we consider is the WordNet noun
hierarchy dataset. Its full transitive closure contains 82,115 Nouns
and 743,241 hypernymy relations. We remove the root of WordNet
Nouns as its edges are trivial to predict. The remaining transitive
closure of the WordNet Nouns hierarchy consists of 82,114 Nouns
and 661,127 hypernymy relations. We also train on the subtree of
all hyponyms of animal.n.01 (including itself), which has 4,017
entities, 4,051 direct edges, and 29,795 total edges. We generate
negative samples following a similar method to [ 13,28,39]—corrupt
one of the words in a positive pair randomly while discarding it if
it happens to be a positive pair.
3.2 Competing methods and Metrics Used
We exhaustively compare our model to 9existing embedding meth-
ods for directed graphs: vectors in Euclidean space (Similarity Vec-
tors [ 3] & Bilinear Vectors [ 30]), vectors in Complex space [ 38],
Hyperbolic embedding using Lorentzian distance learning [ 23], the
continuous Order Embeddings [ 39], the Probabilistic Order Embed-
dings [ 21], Hyperbolic Entailment Cones [ 13], and the probabilistic
box embedding methods, Gumbel Box [ 8] and T-Box [ 4]. All thesemethods are intended to produce embedding for entities having
hierarchical organization. Among the above methods, our model
is most similar to Order Embedding [ 39], as our model is simply
the restriction of theirs from (R+)dto{0,1}d. So, this model is a
natural competitor. Binder is also similar to Probabilistic Order
Embedding, Box Embeddings and Hyperbolic Entailment Cones
[13], as they all are transitive order embeddings. We also compare
to Hyperbolic Embeddings, which uses a distance ranking func-
tion to embed the undirected version of the hierarchy graph. We
consider Lorentzian distance learning over Poncaré Embeddings
[23] as it is the latest among hyperbolic methods. In subsequent
discussion and result tables, we will denote Similarity Vector model
as Sim, Bilinear vector model as Bilinear, Complex vector model
as Complex, Lorentzian Distance Learning Hyperbolic model as
Lorentzian Distance, Order Embeddings model as OE, Probabilistic
Order Embeddings as POE, and Hyperbolic Entailment Cones as
HEC. We report F1-score for our experimental results, as it is a
preferred metric for imbalanced data.
3.3 Case Study
We present a case-study experiment of Binder’s embedding results.
We ran Binder on a toy dataset, from [ 39], to generate 8-bit em-
bedding vectors for 15 entities. Binder achieved perfect score in
this task satisfying all constraints. Figure 1 shows the embedding
vectors in a tree structure. By observing the embedding vectors
ofboy,person , and city , one can determine that boyis-aperson
but is-not-a city . We can grasp Binder’s embedding by think-
ing that each bit corresponds to some “attribute” of the object,
where all attributes are passed down to hyponyms. This can help to
build interpretability of embeddings for the concepts in a dataset.
For instance, the second bit can be an attribute which could be
named as has-life, since it is the only 1 bit in the livingThing
embedding. Sometimes, however, bits can be used with different
meanings in different words: the right-most bit is 1 on man,girl ,
andSanJuan embeddings. This is partly because the toy dataset is
very sparse, with only two sub-components of city compared to
ten sub-components of livingThing , and adult andchild were
not included in the dataset, as they are in WordNet. If we consider
the attributes as intent, and objects as extent, Binder’s binary em-
bedding is a manifestation of intent-extent relationship in formal
concept analysis.
3.4 Training Setup
ForBinder, we learn a d-bit array for each concept in the hierarchy.
For all tasks, we train Binder for 10000 epochs, where each epoch
considers the full batch for training. We tune our hyper-parameters:
Figure 1: Visual representation of toy dataset results. White
circles represent 1 and black circles 0.
 
985Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: Representation Experiment results F1-score(%)
Medical Music Animals Shwartz Lex Shwartz Random WordNet Nouns
Model entities = 1.4k entities = 1k entities = 4k entities = 5.8k entities = 13.2k entities = 82k
edges = 4.3k edges = 6.5k edges = 29.8k edges = 13.5k edges = 56.2k edges = 743k
Sim [3] 100 100 100 97.67 99.2 83.74
Bilinear [30] 100 100 100 99.91 98.23 26.02
Complex [38] 100 100 100 100 99.5 94.13
OE [39] 99.88 99.94 99.76 99.9 98.82 87.27
POE [21] 100 100 100 100 100 99.89
Lorentzian distance [23] 58.37 56.77 66.36 69.91 56.54 7.78
HEC [13] 97.01 96.35 95.81 94.63 94.45 57.19
Gumbel Box [8] 100 100 100 100 100 100
T Box (/n/d) [4] 100 100 100 100 100 99.96
BINDER (our model) 98.26±0.18 99.16±0.11 97.98±1.63 99.12±0.1 99.3±0.07 93.34±0.72
dimension d, positive and negative sample weights α,β, negative
sample multiplier n−, and learning rate rℓand bias bℓmanually by
running separate experiments for each dataset. We find that the
optimal learning rate rℓand learning bias bℓare 0.008 and 0.01
respectively for all datasets and tasks. Although ablation study
results in Section B.2 show that the learning bias bℓhas no effect
on the results reported, we still keep it at bℓ=0.01, so bits whose
gradient was exactly neutral have a 1% chance of flipping. We fix
β=10and tuneα; we always find that α≤βgives far too many
false negatives. We report best hyperparameter configurations for
all tasks in Table 7 in Appendix B.1. For the representation task
we need more bits to increase the capacity of our model to better
reconstruct training edges. We also extensively hyperparameter
tuned our competitors, and we provide details of hyperparameter
ranges that we considered in Table 6 in Appendix B.1.
3.5 Representation Task Results
This task is easier; all the competing methods perform better on
this task than on link prediction. This is because representation
is similar to a fitting task (like training accuracy in ML) and with
higher dimensions, models have more freedom (higher capacity)
to satisfy the order constraints in the training data. The results are
shown in Table 4, in which the datasets are arranged in increasing
size from left to right. For each dataset and each model, we report
theF1-scores. Since Binder is a randomized algorithm, we show
mean and standard deviation of the results obtained in 5 runs.
Binder, and all the competing methods except hyperbolic embed-
ding (HEC, Lorentzian), perform very well on the smaller datasets
(entity count around 5K)—some of the competing methods reach
100% F1-score, while Binder achieves 98% to 99.5% F1-score. How-
ever, on the largest dataset, WordNet (82K entities), their F1score
drops drastically (for Bilinear the drop is from 100% to 26%), ex-
cept for T-Box and Gumbel Box, which retain their performance.
Binder’s performance on WordNet is about 93.3%. Given that there
are a finite number of bit vectors, Binder has a limited fitting capac-
ity; considering this, Binder’s results are excellent. Binder reaches
very good embeddings in a relatively small number of iterations
(see bottom left graph in Figure 2 in Appendix 3.7), but near a local
minima, the flipping probability becomes small (due to low gra-
dient), which causes the method not to achieve perfect fit (100%F1-score) like some of the other methods. Since for the representa-
tion task we include all positive pairs in training, a perfect F1-score
indicates that a model is too good at fitting training data, which may
work against itself to achieve generalization on unseen data. On
the other hand, the apparent low capacity of Binder is a blessing
in disguise, which we show in the next section where we compare
Binder’s performance with the competitors on link prediction of
unseen transitive edges.
3.6 Link Prediction Results
Link prediction task is similar to test accuracy in a typical machine
learning task. It is more challenging and practically useful than
representation, as prediction is performed on unseen edges. The re-
sults of this task for all datasets and all methods are shown in Table
5. As we can see from the results, Binder handsomely wins over
all competitors in all transitive closure (TC) link prediction tasks
(except the Music dataset in TC 50% task). In fact in all datasets,
Binder’s F1-score is around 95% or more, whereas the competing
methods struggle to reach even 50% F1-score. Note that, in a tree-
like dataset, there are generally much more transitive edges than
direct edges. As we add more transitive edges to the training dataset,
the competitors’ results improve, which shows that their methods
rely on these edges for link prediction, whereas Binder does not
(Binder’s F1-scores are between 98% to 99.5% for 0%, 10%, 25%, and
50% TC edges). Other models’ performances suffer significantly as
the dataset size increases, whereas Binder maintains its perfor-
mance, and thus is more scalable. For 0% TC link prediction on the
largest dataset, WordNet (with 743,241 edges), Binder’s F1-score
(98.5%) surpassed that of the best competing models (29%) by about
70%. From our investigation, Binder’s superiority comes from its
ability to predict negative pairs ( ais-not-a b) much better than the
competitors (Binder has significantly higher precision, whereas all
the competing models suffer from a very high false-positive count).
Credit for this may go to the design of Binder’s constraints, which
enables it to predict negative pairs ( ais-not-a b) simply by creating
a(0,1)bit pair at only one index.
3.7 Binder Model Convergence Results
We run our model with a large number of iterations to minimize
loss and thus maximize the F1-score. We show train loss curves
 
986KDD ’24, August 25–29, 2024, Barcelona, Spain Croix Gyurek, Niloy Talukder and Mohammad Al Hasan
Table 5: Transitive Closure Link Prediction Experiment results F1-score(%)
Model Medical Music Animals Shwartz Lex Shwartz Random WordNet Nouns
Transitive Closure 0%
Sim 21.23 25.69 18.64 18.69 22.96 23.54
Bilinear 21.92 26.24 26.05 28.05 16.68 25.71
Complex 23.04 29.27 23.35 17.46 16.77 24.13
OE 55.9 46.94 33.99 32.29 30.57 28.52
POE 81.18 48.3 31.81 30.66 28.61 27.26
Lorentzian distance 62.86 33.33 33.53 40.41 34.01 28.26
HEC 53.4 35.88 32.33 36.38 32.92 29.03
Gumbel Box 88.51 64.44 50.65 46.56 29.74 28.3
T Box (/n/d) 71.05 46.77 33.49 30.45 27.53 27.84
BINDER 98.61±1.32 92.13±2.27 98.99±0.24 99.87±0.09 99.52±0.21 98.64±0.09
Transitive Closure 10%
Sim 55.42 40.31 38.15 43.88 31.68 27.89
Bilinear 40.48 37.4 31.74 32.36 16.69 16.67
Complex 38.42 42.32 19.36 17.95 17.06 16.93
OE 52.63 50 33.37 33.7 35.75 28.22
POE 93.83 77.19 48.16 32.15 34.37 33.35
Lorentzian distance 68.02 47.67 42 69.53 62.39 54.75
HEC 66.67 45.97 54.37 78.12 70.07 65.11
Gumbel Box 93.67 83.27 65.26 58.58 52.54 32.33
T Box (/n/d) 76.32 60.35 47.24 37.37 36.02 30.33
BINDER 99.41±0.42 93.9±1.36 98±0.9 99.95±0.07 99.33±0.85 98.49±0.21
Transitive Closure 25%
Sim 64 55.26 50.23 38.31 38.19 28.62
Bilinear 51.81 50 44.33 18.4 16.68 16.69
Complex 58.29 57 31.27 18.02 17.6 16.78
OE 52.34 48.11 33.17 35.85 39.16 28.18
POE 82.02 53.03 31.87 38.57 39.18 28.05
Lorentzian distance 69.31 46.2 61.61 55.1 77.27 59.51
HEC 72.63 56.58 53.6 81 68.77 56.68
Gumbel Box 98.18 89.05 80.98 67.31 67.38 31.42
T Box (/n/d) 81.88 58.12 52.95 34.6 45.28 29.87
BINDER 99.53±0.76 92.78±1.58 99.05±0.8 99.97±0.06 98.78±0.51 99.04±0.14
Transitive Closure 50%
Sim 82.35 72.22 67.04 49.44 38.53 28.91
Bilinear 69.01 66.67 17.90 17.14 16.68 16.67
Complex 50 36.05 31.47 18.24 17.35 16.73
OE 55.51 50.23 32.37 37.48 39.97 28.16
POE 61.22 60.52 31.96 36.2 43.27 28.09
Lorentzian distance 75.9 46.49 65.52 45.82 85.03 61.16
HEC 79.76 59.11 51.96 79.15 66.44 57.79
Gumbel Box 99.41 97.28 90.72 62.99 48.84 28.81
T Box (/n/d) 81.76 72.22 64.26 38.45 49.14 34.81
BINDER 99.42±1.0 93.8±0.87 99.9±0.1 99.92±0.07 99.85±0.14 99.56±0.03
for both the representation and the 0%TC link prediction tasks
respectively in the first (top-left) and second (top-right) graphs in
Figure 2. For representation, we see a smooth loss curve from the
beginning to convergence. However, we see turbulence in 0% TCprediction near the convergence; this is likely due to randomization,
which often unfix more bits than fixing causing loss on (unseen)
test data to grow marginally higher in subsequence epochs. This
does not happen in representation task because for this task the loss
 
987Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors KDD ’24, August 25–29, 2024, Barcelona, Spain
is computed on the (seen) train dataset. For representation task our
model achieves 99% validation F1-score in less than 50 iterations on
theWordNet Nouns dataset, as shown in the third graph in Figure 2.
For TC link prediction task the model attains high F1-score very
quickly, and it continues to improve steadily, as shown in the fourth
graph in Figure 2.
Figure 2: Top: Train loss of the first 100 iterations for the rep-
resentation task (left) and first 5000 iterations of 0% TC link
prediction (right). Bottom: Respective validation F1-scores
over time. Experiments were on our largest dataset, Word-
Net Nouns.
3.8 Space advantage of Binder
One of the main advantages of using binary vectors is their space
efficiency compared to floating-point vectors. The final represen-
tation of the concepts using Binder are bit vectors, so the storage
required for nwords and ddimensions isdn
8bytes. This is much
smaller than OE and HEC’s 4dnbytes, Gumbel Box’s 8dnbytes,
and TBox’s 16dnbytes if 32-bit floating point values are used. If
we consider the WordNet Nouns dataset and d=100, the vector,
Order Embedding and Hyperbolic methods takes at least 34.2 MB
for their embedding (including the 1.33 MB required for the Word-
Net dataset itself), while box embedding methods take up to 67
MB in storage space. In contrast, Binder takes only 2.36 MB for its
embedding, which is significantly smaller than its floating-point
vector competitors.
4 RELATED WORKS
We have discussed the works that perform hierarchical embedding
in the Introduction section and compared our results with those
in the Results section. In this section, we will discuss other related
works which may have used binary bits, but not necessarily for
solving the ordered embedding task. Binarized embedding was
previously explored in Knowledge Graph space. Authors in [ 42]
proposed learning to hash using compact binary codes for KG com-
pletion and query tasks. They show that for large-scale similarity
computation, binary codes are computationally efficient over con-
tinuous dense vectors. To further address the challenge of large
storage and computational cost of continuous graph embedding
methods, [ 24] proposed a discrete knowledge graph embedding
(DKGE) method. The closest to our work is [ 18] which proposed adiscrete optimization method for training a binarized knowledge
graph (KG) embedding model. But unlike Binder, all of these ap-
proaches either use continuous real-valued vectors [ 24,42] or a
combination of binary and continuous real-valued vectors [ 18] for
optimization process. Other than KG embedding models, Locality
Sensitive Hashing (LSH) [ 7] and semantic hashing techniques such
as Deep Semantic Hashing (DSH) [ 25] learn binary hash codes by
performing nearest neighbor (NN) search over a set of entities. LSH
and DSH are embedding techniques where entity relationships are
symmetric (if xis near toy, thenyis near to x) compared to Binder
which learns binary embedding for asymmetric (is-a) relationships.
Binary code is also used with Box Embedding method to model
directed graphs that contain cycles in [ 44]. For data compression
while preserving the Euclidean distance between a pair of entities,
[45] used k-dimensional binary vector space [−1,+1]k.
To the best of our knowledge, we are the first to propose hierar-
chical concept representation in Binary space. Our use of intent-
extent philosophy of formal concept analysis (FCA) to derive hier-
archical concept representation is also novel. Previously, [ 32] used
FCA to encode a formal context’s closure operators into NN, and
[10] introduced fca2vec similar to node2vec [ 15], which embeds ex-
isting FCA systems into real-valued vector spaces using NN method.
None of these works are our competitor, as their embedding objec-
tive is to embed in 2-dimensional real space for visualization.
5 FUTURE WORKS AND CONCLUSION
Binder is the first work to use binary vectors for embedding hier-
archical concepts, so there are numerous scopes for building on top
of this work. First, we wish to explore more efficient combinatorial
optimization algorithms to replace Binder’s learning algorithm,
by using various well-known CSP (constraint satisfaction problem)
heuristics. Binder’s Loss function can also be extended with a node
similarity expression based on Hamming distance between embed-
ding vectors, which we plan to do next. In terms of limitations,
Binder is a transductive model, i.e. a concept must appear in the
training data for its embedding to be learnt, but this limitation is
also shared by existing hierarchical embedding models. However,
Binder can generate embeddings for derived concepts by using
logical functions over existing concepts more readily than other
methods. Another future work can be to make Binder inductive
over unseen concepts by including knowledge about the broadness
or narrowness of a concept from a large distributed language model,
such as BERT, RoBERTa, or GLoVe.
To conclude, in this work, we propose Binder, a novel approach
for order embedding using binary vectors. Binder is ideal for find-
ing representation of concepts exhibiting hypernym-hyponym rela-
tionship. Also, Binder’s binary vector based embedding is extensive
as it allows deriving embeddings of other concepts that are logical
derivatives of existing ones. Experiments on 6 benchmark datasets
show that Binder is superior to state-of-the-art order embedding
methodologies in the transitive closure link prediction task.
6 ACKNOWLEDGEMENTS
This work is supported by National Science Foundation (NSF), USA
through Dr. Hasan’s research grant, No. IIS-1909916.
 
988KDD ’24, August 25–29, 2024, Barcelona, Spain Croix Gyurek, Niloy Talukder and Mohammad Al Hasan
REFERENCES
[1]Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
and Zachary Ives. 2007. DBpedia: A Nucleus for a Web of Open Data. In The
Semantic Web (Lecture Notes in Computer Science), Karl Aberer, Key-Sun Choi,
Natasha Noy, Dean Allemang, Kyung-Il Lee, Lyndon Nixon, Jennifer Golbeck,
Peter Mika, Diana Maynard, Riichiro Mizoguchi, Guus Schreiber, and Philippe
Cudré-Mauroux (Eds.). Springer, Berlin, Heidelberg, 722–735. https://doi.org/10.
1007/978-3-540-76298-0_52
[2]J. Christopher Beck, T. K. Feng, and Jean-Paul Watson. 2011. Combining Con-
straint Programming and Local Search for Job-Shop Scheduling. INFORMS Journal
on Computing 23, 1 (Feb. 2011), 1–14. https://doi.org/10.1287/ijoc.1100.0388
[3]Robi Bhattacharjee and Sanjoy Dasgupta. 2023. What relations are reliably
embeddable in Euclidean space? https://doi.org/10.48550/arXiv.1903.05347
arXiv:1903.05347 [cs, stat].
[4]Michael Boratko, Dongxu Zhang, Nicholas Monath, Luke Vilnis, Kenneth L Clark-
son, and Andrew McCallum. 2021. Capacity and Bias of Learned Geometric Em-
beddings for Directed Graphs. In Advances in Neural Information Processing Sys-
tems, Vol. 34. Curran Associates, Inc., 16423–16436. https://proceedings.neurips.
cc/paper/2021/hash/88d25099b103efd638163ecb40a55589-Abstract.html
[5]Antoine Bordes, Nicolas Usunier, Alberto Garcia-Durán, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. In Proceedings of the 26th International Conference on Neural Information
Processing Systems - Volume 2 (NIPS’13). Curran Associates Inc., Red Hook, NY,
USA, 2787–2795.
[6]Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
2022. Introduction to Algorithms, fourth edition. MIT Press. Google-Books-ID:
RSMuEAAAQBAJ.
[7]Anirban Dasgupta, Ravi Kumar, and Tamas Sarlos. 2011. Fast locality-sensitive
hashing. In Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ’11). Association for Computing
Machinery, New York, NY, USA, 1073–1081. https://doi.org/10.1145/2020408.
2020578
[8]Shib Sankar Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vilnis, Xiang Lor-
raine Li, and Andrew McCallum. 2020. Improving Local Identifiability in
Probabilistic Box Embeddings. NeurIPS 2020 (Virtual) (Oct. 2020). https:
//doi.org/10.48550/arXiv.2010.04831
[9]Shib Sankar Dasgupta, Xiang Lorraine Li, Michael Boratko, Dongxu Zhang,
and Andrew McCallum. 2021. Box-To-Box Transformations for Modeling Joint
Hierarchies. In Proceedings of the 6th Workshop on Representation Learning for NLP
(RepL4NLP-2021). Association for Computational Linguistics, Online, 277–288.
https://doi.org/10.18653/v1/2021.repl4nlp-1.28
[10] Dominik Dürrschnabel, Tom Hanika, and Maximilian Stubbemann. 2019.
FCA2VEC: Embedding Techniques for Formal Concept Analysis. https:
//doi.org/10.48550/arXiv.1911.11496 arXiv:1911.11496 [cs, stat].
[11] Christiane Fellbaum. 2012. WordNet. In The Encyclopedia of Applied Linguistics,
Carol Chapelle (Ed.). John Wiley & Sons, Inc., Hoboken, NJ, USA, wbeal1285.
https://doi.org/10.1002/9781405198431.wbeal1285
[12] Luis Antonio Galárraga, Christina Teflioudi, Katja Hose, and Fabian Suchanek.
2013. AMIE: association rule mining under incomplete evidence in ontological
knowledge bases. In Proceedings of the 22nd international conference on World
Wide Web (WWW ’13). Association for Computing Machinery, New York, NY,
USA, 413–422. https://doi.org/10.1145/2488388.2488425
[13] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. 2018. Hy-
perbolic Entailment Cones for Learning Hierarchical Embeddings. Interna-
tional Conference on Machine Learning (ICML) Stockholm, Sweden. (April 2018).
https://doi.org/10.48550/arXiv.1804.01882
[14] Bernhard Ganter and Rudolf Wille. 1999. Formal Concept Analysis. Springer,
Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-59830-2
[15] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for
Networks. (July 2016). https://doi.org/10.48550/arXiv.1607.00653
[16] Lingbing Guo, Zequn Sun, and Wei Hu. 2019. Learning to Exploit Long-term
Relational Dependencies in Knowledge Graphs. https://doi.org/10.48550/arXiv.
1905.04914 arXiv:1905.04914 [cs].
[17] Mohammad Al Hasan and Mohammed J. Zaki. 2011. A Survey of Link Prediction
in Social Networks. In Social Network Data Analytics, Charu C. Aggarwal (Ed.).
Springer US, Boston, MA, 243–275. https://doi.org/10.1007/978-1-4419-8462-3_9
[18] Katsuhiko Hayashi, Koki Kishimoto, and Masashi Shimbo. 2020. A Greedy Bit-flip
Training Algorithm for Binarized Knowledge Graph Embeddings. In Findings of
the Association for Computational Linguistics: EMNLP 2020, Trevor Cohn, Yulan He,
and Yang Liu (Eds.). Association for Computational Linguistics, Online, 109–114.
https://doi.org/10.18653/v1/2020.findings-emnlp.10
[19] Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015. Learning to Represent
Knowledge Graphs with Gaussian Embedding. In Proceedings of the 24th ACM
International on Conference on Information and Knowledge Management (CIKM
’15). Association for Computing Machinery, New York, NY, USA, 623–632. https:
//doi.org/10.1145/2806416.2806502[20] Andrej Karpathy and Li Fei-Fei. 2015. Deep Visual-Semantic Alignments for
Generating Image Descriptions. https://doi.org/10.48550/arXiv.1412.2306
arXiv:1412.2306 [cs].
[21] Alice Lai and Julia Hockenmaier. 2017. Learning to Predict Denotational Probabili-
ties For Modeling Entailment. In Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Linguistics: Volume 1, Long Papers,
Mirella Lapata, Phil Blunsom, and Alexander Koller (Eds.). Association for Compu-
tational Linguistics, Valencia, Spain, 721–730. https://aclanthology.org/E17-1068
[22] Ni Lao, Tom Mitchell, and William W. Cohen. 2011. Random Walk Infer-
ence and Learning in A Large Scale Knowledge Base. In Proceedings of the
2011 Conference on Empirical Methods in Natural Language Processing. As-
sociation for Computational Linguistics, Edinburgh, Scotland, UK., 529–539.
https://aclanthology.org/D11-1049
[23] Marc Law, Renjie Liao, Jake Snell, and Richard Zemel. 2019. Lorentzian Distance
Learning for Hyperbolic Representations. In Proceedings of the 36th International
Conference on Machine Learning . PMLR, 3672–3681. https://proceedings.mlr.
press/v97/law19a.html
[24] Yunqi Li, Shuyuan Xu, Bo Liu, Zuohui Fu, Shuchang Liu, Xu Chen, and Yongfeng
Zhang. 2021. Discrete Knowledge Graph Embedding based on Discrete Optimiza-
tion. https://doi.org/10.48550/arXiv.2101.04817 arXiv:2101.04817 [cs].
[25] Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, and Jie Zhou. 2015.
Deep hashing for compact binary codes learning. In 2015 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). 2475–2483. https://doi.org/10.
1109/CVPR.2015.7298862 ISSN: 1063-6919.
[26] Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. PATTY:
a taxonomy of relational patterns with semantic types. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-CoNLL ’12). Association for
Computational Linguistics, USA, 1135–1145.
[27] Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Phung. 2018.
A Novel Embedding Model for Knowledge Base Completion Based on Convolu-
tional Neural Network. In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers). 327–333. https://doi.org/10.18653/v1/N18-2053
arXiv:1712.02121 [cs].
[28] Maximillian Nickel and Douwe Kiela. 2017. Poincaré Embeddings for Learning
Hierarchical Representations. In Advances in Neural Information Processing Sys-
tems, Vol. 30. Curran Associates, Inc. https://papers.nips.cc/paper/2017/hash/
59dfa2df42d9e3d41f5b02bfc32229dd-Abstract.html
[29] Maximilian Nickel and Douwe Kiela. 2018. Learning Continuous Hierarchies in
the Lorentz Model of Hyperbolic Geometry. https://doi.org/10.48550/arXiv.1806.
03417 arXiv:1806.03417 [cs, stat].
[30] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way
model for collective learning on multi-relational data. In Proceedings of the 28th In-
ternational Conference on International Conference on Machine Learning (ICML’11) .
Omnipress, Madison, WI, USA, 809–816.
[31] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learn-
ing of Social Representations. (March 2014). https://doi.org/10.1145/2623330.
2623732
[32] Sebastian Rudolph. 2007. Using FCA for Encoding Closure Operators into Neural
Networks. In Proceedings of the 15th international conference on Conceptual Struc-
tures: Knowledge Architectures for Smart Applications (ICCS ’07). Springer-Verlag,
Berlin, Heidelberg, 321–332. https://doi.org/10.1007/978-3-540-73681-3_24
[33] Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, and Bowen Zhou.
2018. End-to-end Structure-Aware Convolutional Networks for Knowledge Base
Completion. https://doi.org/10.48550/arXiv.1811.04441 arXiv:1811.04441 [cs].
[34] Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016. Improving Hypernymy
Detection with an Integrated Path-based and Distributional Method. https:
//doi.org/10.48550/arXiv.1603.06076 arXiv:1603.06076 [cs].
[35] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of
semantic knowledge. In Proceedings of the 16th international conference on World
Wide Web (WWW ’07). Association for Computing Machinery, New York, NY,
USA, 697–706. https://doi.org/10.1145/1242572.1242667
[36] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl-
edge Graph Embedding by Relational Rotation in Complex Space. https:
//doi.org/10.48550/arXiv.1902.10197 arXiv:1902.10197 [cs, stat].
[37] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. LINE: Large-scale Information Network Embedding. (March 2015). https:
//doi.org/10.1145/2736277.2741093
[38] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume
Bouchard. 2016. Complex Embeddings for Simple Link Prediction. https:
//doi.org/10.48550/arXiv.1606.06357 arXiv:1606.06357 [cs, stat].
[39] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2015. Order-
Embeddings of Images and Language. (Nov. 2015). https://doi.org/10.48550/
arXiv.1511.06361
[40] Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. 2018. Probabilistic
Embedding of Knowledge Graphs with Box Lattice Measures. http://arxiv.org/
 
989Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors KDD ’24, August 25–29, 2024, Barcelona, Spain
abs/1805.06627 arXiv:1805.06627 [cs, stat].
[41] Denny Vrandecic. 2012. Wikidata: a new platform for collaborative data col-
lection. In Proceedings of the 21st World Wide Web Conference, WWW 2012,
Lyon, France, April 16-20, 2012 (Companion Volume), Alain Mille, Fabien Gandon,
Jacques Misselis, Michael Rabinovich, and Steffen Staab (Eds.). ACM, 1063–1064.
https://doi.org/10.1145/2187980.2188242
[42] Meng Wang, Haomin Shen, Sen Wang, Lina Yao, Yinlin Jiang, Guilin Qi, and Yang
Chen. 2019. Learning to Hash for Efficient Search Over Incomplete Knowledge
Graphs. In 2019 IEEE International Conference on Data Mining (ICDM). IEEE,
Beijing, China, 1360–1365. https://doi.org/10.1109/ICDM.2019.00174
[43] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. KG-BERT: BERT for Knowledge
Graph Completion. https://doi.org/10.48550/arXiv.1909.03193 arXiv:1909.03193
[cs].
[44] Dongxu Zhang, Michael Boratko, Cameron Musco, and Andrew McCallum.
2022. Modeling Transitivity and Cyclicity in Directed Graphs via Binary
Code Box Embeddings. Advances in Neural Information Processing Systems 35
(Dec. 2022), 10587–10599. https://papers.nips.cc/paper_files/paper/2022/hash/
44a1f18afd6d5cc34d7e5c3d8a80f63b-Abstract-Conference.html
[45] Jinjie Zhang and Rayan Saab. 2021. Faster Binary Embeddings for Preserving
Euclidean Distances. https://doi.org/10.48550/arXiv.2010.00712 arXiv:2010.00712
[cs, math, stat].
A PROOF OF BINDER’S CONVERGENCE TO
LOCAL OPTIMAL SOLUTION
Lemma A.1. When bias bℓ=0, for any word a, if the j’th bit in
the binary representation vector ais updated by Binder’s random
flipping (keeping all other bits the same), the sub-component of the
positive loss corresponding to pairs (a,·)decreases.
Proof. LetP(a,·)be the set of positive data instances (a,b)where
the first entity is the given a, and let a∈{0,1}dbe the embedding
vector of a. Define L(a)as the component of loss function associated
with a. Suppose in an iteration, we randomly flip bit jofa. To
compute this probability, Binder computes the ∆ajLoss P, which is
L(a)− L(ˆa), where ˆais the same as aexcept that the bit value in
thej’th position is different. (Recall that we define our gradient to
bepositive when flipping bit jimproves our model, thus decreasing
the loss function.) Based on Eq. 4, this gradient value is +1only
for the case when a constraint aj→bjis violated (where bis the
other element in a training pair), i.e. aj=0butbj=1(see the
3rd column of Table 2). For the other three choices of ajandbj,
(0,0),(1,0),(1,1), the contribution to gradient value is 0, 0, and −1
respectively. Thus, the gradient will only be positive when aj=0
(andbj=1for some b). Using Line 7 of Algorithm 2 for bℓ=0, the
flip probability can only be positive in the aj=0case, and with the
flip the loss function value decreases by kα(through Eq. 2), where
kis the number of pairs (a,·)∈Pthat violate the constraint. In all
scenarios, the loss value decreases in the successive iteration. □
Lemma A.2. When bias bℓ=0, for any word b, if the j’th bit in the
binary representation vector of bis updated by Binder’s probabilistic
flipping (keeping the remaining bits the same), the sub-component of
the loss function corresponding to positive pairs (·,b)decreases.
Proof. The proof is identical to the proof of Lemma A.1, except
that we use gradient value in Eq. 5 instead of Eq. 4. In this case
also when only one position of b’s embedding vector is flipped
probabilistically, the loss function value decreases. □
Lemma A.3. When bias bℓ=0, given a collection of negative data
instances, say,(a′,b′), if the j’th bit in the vectors of a′orb′inde-
pendently (not simultaneously) is updated by Binder’s probabilistic
flipping (keeping the remaining bits same), the loss function value
decreases or remains the same in the successive iteration.Proof. The proof is identical to proof of Lemma A.1, except
that we use gradient value in Eq. 6 (or Eq. 8) for the case of a′, and
gradient value of Eq. 7 (or Eq. 9) for b′, and the loss function value
decreases through Eq. 3. □
These proofs also apply if rℓα≥bℓ>0andrℓβ≥bℓ>0. In
that case, we can flip a bit with zero gradient. Such flips do not
immediately increase or decrease the loss function; however, they
can allow Binder to improve from a weak local optimum. In our
experiments, rℓαandrℓβare much larger than bℓ, so our algorithm
prioritizes actual improvements over zero-gradient steps.
Theorem A.4. When bias bl=0, if Line 8 of Algorithm 2 is
executed sequentially for each index value j for each of the entities,
Binder reaches a local optimal solution considering a 1-hamming
distance neighborhood.
Proof. Using earlier Lemmas, each bit flipping in any embed-
ding vector of any of the entities, either decreases the loss function
or keeps it the same. Each sub-component of the gradient is exactly
the negative of the change in that sub-component of the loss when
the corresponding bit is flipped; hence, if ∆[a,j]>0, then Loss will
decrease by exactly ∆[a,j]when ajis flipped, and if ∆[a,j]≤0,
then FlipProb( a,j)will be 0 and ajwill not be flipped.
When Line 8 of Algorithm 2 is executed sequentially for each
index j(only one change in one iteration) for each of the entities,
the loss function value monotonically decreases in each successive
iteration. Because the space of embeddings is finite, eventually
Binder will reach a point where no single bit flip improves the
value of Loss. Now, if the bias bl=0, for each entity, the probability
of bit-flipping for each index is computed to be 0 (by Line 7 in
Algorithm 2), so none of the embeddings change any further and
Binder reaches a local optimal solution considering a 1-hamming
distance neighborhood. In other words, for every entity a, if we
change any single bit of a’s embedding, the original embedding of a
is guaranteed to be at least as good as the changed embedding. □
When we change only one bit at a time, keeping everything else
the same (as in the proof), our optimization algorithm becomes a
greedy hill climbing algorithm. However, this would make Binder
extremely slow to converge. Thus, we allow several bits to change at
once, so it acts like gradient descent: Let θbe an embedding vector
of an entity and L(θ)the associated component of loss function.
To minimize L(θ), a hill climbing method would adjust a single
element in θto decrease L(θ), while gradient descent will adjust all
values inθin each iteration, θnew=θold−α∆θL(θold). During
early iterations, Binder works like gradient descent, but as time
progresses, it behaves more like hill climbing as more gradient
values become negative and fewer bits flip.
A.1 Computational Complexity Analysis
During Gradient Computation we possibly update d-dimensional
bit vectors of each vocabulary word along each dimension for a pair
(positive or negative). So, for a vocabulary size of n, and list of pos-
itive pairs P, we randomly generate n−|P|negative samples (where
n−is a hyper-parameter), and so Gradient Computation complexity
isO(nd(1+n−)|P|). We repeat this process for each epoch, so for
Tepochs, the overall algorithm computational complexity is linear
in each variable: O(ndT(|P|+|N|))=O(ndTn−|P|).
 
990KDD ’24, August 25–29, 2024, Barcelona, Spain Croix Gyurek, Niloy Talukder and Mohammad Al Hasan
Table 6: Hyper parameter range used for tuning each model.
Model Hyper parameter range
Lorentzian distance lorentzianα: [0.01, 0.1]
Hyperbolic Entailment Cones relation cone aperture scale: [0.01, 0.1, 1], eps bound: [0.01, 0.1]
Box Embeddings intersection temp: [0.01, 0.1, 1] and volume temp: [0.1, 1]
BINDER α: {15, 20, 25, 30, 40, 100, 500, 5000, 25000, 50000} and n−: {12, 32, 64, 128, 256, 512}
B EXPERIMENTAL SETUP
B.1 Hyperparameter Tuning
For BINDER and the competing models, except Gumbel and T-Box,
we exhaustively tuned dimensions d=8,32,64,128while keeping
patience at 500 (model stops training if loss does not decrease in 500
iterations). Since Gumbel Box requires 2ddimensions and T-Box re-
quires 4dfor their representations, to be fair with other models, we
tuned Gumbel Box for d∈4,16,32,64and T-Box for d∈2,8,16,32.
We report other hyperparameter ranges that we considered for tun-
ing each model in table 6 and the best hyperparameters for Binder
in table 7. All models were run on a Tesla V100 GPU.
B.2 Ablation Study
For an ablation study, we observed the effect on our model’s F1-
score for the test data by removing βandbℓseparately while
keeping dimension fixed at 128, and α= 25 for representation and
α= 25000 for 0%TC link prediction. The default values consid-
ered for bℓandβare 0.01 and 32 respectively. For the representa-
tion task on Animals dataset, setting β=0gave an F1-score of
44.07±1.77%, demonstrating a significant drop from our experimen-
tal results, whereas setting bℓ=0we did not see any significant
effect on the F1-score ( 96.46±1.34%). For the 0%TC prediction
task on Animals dataset, when we set β=0, we got an F1-score
of60.49±0.55%which was lower than our experimental results.
Whereas setting bℓ=0we did not see any significant effect on the
F1-score (98 .73±0.45).
B.3 Robustness of Binder
Binder has a few hyper-parameters, which we tuned to maximize
model F1-score. In this experiment, we validate how Binder’s per-
formance is affected by the choice of hyper-parameter values. We
show plots for hyper parameter tuning study on the Animals dataset
for representation and link prediction (0% transitive) task in Figure
3. We fixβat 10, learning rate rℓat 0.008 and learning bias at 0.01
and tune d, positive weight αand negative sample multiplier n−.
We ran each experiment for 2500 iterations.
For representation task, to see the effect of dimension, we fix α
at 30 and n−at 32 and change bits in the range d∈{32,64,96,128}.Representation Experiment F1-score
0% Transitive Closure Prediction Experiment F1-score
Figure 3: Binder robustness experiment for Representation
and 0% Transitive Closure on Animals dataset.
We see an increasing trend in F1-score with optimum value at
bits 128. This is intuitive as representation task benefits from large
number of bits. To see the effect of αwe fix d=128,n−=32and
changeα∈{15,20,25,30}. We see F1-score reaches maximum at
α=20and decreases after 25. For n−, we fix bits at 128 and αat
25 and change n−∈{64,128,256,512}. As we increase n−we see
increase in F1-score and also sharp decrease in error margin. Based
on this experiment we decided on the optimal hyperparameter
configuration to be (128, 25, 256).
For 0% transitive closure task, we run similar experiments. For
α=25000, n−=32, and d∈{32,64,96,128}, we see a slightly
increasing trend in F1-score with optimum value at 128 bits. So,
transitive closure prediction, which depends on the generalization
of the model, is not heavily influenced by increasing number of
bits like the representation task. For d=128, n−=32, andα∈
{100,500,5000 ,25000}, we see F1-score reaches maximum at α=
25000. For d=128,α=25000, and n−∈{32,64,96,128}, we see
a very slight increase in F1-score as we increase n−, with almost
100% F1-score at n−=128. Based on this experiment we decided
on the optimal hyperparameters to be (128, 25000, 128). Similarly,
we tuned Binder for all datasets and tasks and reported the best
hyper parameters in Table 7 in Appendix B.1. The best dimension
dwas always 128.
Table 7: Best hyper parameter configurations (α,n−)forBinder (d=128is always optimal)
Dataset
Task Medical Music Animals Shwartz Lex Shwartz Random WordNet Nouns
TC 0% (25000, 128) (25000, 128) (25000, 128) (25000, 128) (25000, 128) (25000, 128)
TC 10% (25000, 128) (10000, 128) (25000, 128) (25000, 128) (25000, 128) (25000, 128)
TC 25% (50000, 128) (25000, 128) (50000, 128) (50000, 128) (50000, 128) (50000, 12)
TC 50% (50000, 128) (50000, 128) (50000, 128) (50000, 128) (50000, 128) (50000, 12)
Representation (40, 512) (25, 128) (25, 256) (40, 512) (25, 512) (100, 256)
 
991