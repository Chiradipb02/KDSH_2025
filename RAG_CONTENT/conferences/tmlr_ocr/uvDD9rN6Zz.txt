Published in Transactions on Machine Learning Research (11/2022)
Stochastic Douglas-Rachford Splitting
for Regularized Empirical Risk Minimization:
Convergence, Mini-batch, and Implementation
Aysegul Bumin aysegul.bumin@ufl.edu
Department of Computer and Information Science and Engineering
University of Florida
Kejun Huang kejun.huang@ufl.edu
Department of Computer and Information Science and Engineering
University of Florida
Reviewed on OpenReview: https: // openreview. net/ forum? id= uvDD9rN6Zz
Abstract
Inthispaper, westudythestochasticDouglas-Rachfordsplitting(SDRS)forgeneralempirical
risk minimization (ERM) problems with regularization. Our first contribution is to prove
its convergence for both convex and strongly convex problems; the convergence rates are
O(1/√
t)andO(1/t), respectively. Since SDRS reduces to the stochastic proximal point
algorithm (SPPA) when there is no regularization, it is pleasing to see the result matches that
of SPPA, under the same mild conditions. We also propose the mini-batch version of SDRS
that handles multiple samples simultaneously while maintaining the same efficiency as that
of a single one, which is not a straight-forward extension in the context of stochastic proximal
algorithms. We show that the mini-batch SDRS again enjoys the same convergence rate.
Furthermore, we demonstrate that, for some of the canonical regularized ERM problems,
each iteration of SDRS can be efficiently calculated either in closed form or in close to
closed form via bisection—the resulting complexity is identical to, for example, the stochastic
(sub)gradient method. Experiments on real data demonstrate its effectiveness in terms of
convergence compared to SGD and its variants.
1 Introduction
Regularized empirical risk minimization (ERM) is the foundation for supervised machine learning (Vapnik,
1991). It formulates the learning problem as the following optimization problem
minimize
w∈Rd1
nn/summationdisplay
i=1fi(w) +g(w), (1)
where each fiis the loss function defined on a data sample over a training set, and gis a regularization term
to help improve the generalization ability of the model.
Large-scale machine learning models are ideally trained in a sample-by-sample manner in order to reduce
computational and memory overhead. The majority of existing stochastic algorithms use the stochastic
gradient descent (SGD) framework (Robbins & Monro, 1951; Bottou et al., 2018), possibly with an additional
proximal step to handle the (often nonsmooth) regularization function g. Convergence is often O(1/√
t)for
convex functions and O(1/t)for strongly convex functions. Some follow-up work focus on improving the
theoretical convergence rate, such as SVRG (Johnson & Zhang, 2013), SDCA (Shalev-Shwartz & Zhang,
2013), SAGA (Defazio et al., 2014), and many more (Defazio, 2016; Lei et al., 2017; Schmidt et al., 2017;
1Published in Transactions on Machine Learning Research (11/2022)
Allen-Zhu, 2017; 2018a;b), at the expense of additional computational or memory overhead at the order of
n. On the other hand, some practical variations have been proposed, such as AdaGrad (Duchi et al., 2011),
Adam (Kingma & Ba, 2014), AdaBelief (Zhuang et al., 2020), and SGD-momentum (Liu et al., 2020), even
though their theoretical convergence have not been shown to be better.
More recently, there emerges a family of stochastic algorithms that does not rely on just first-order derivatives
but the full information of the stochastic functions (in some literature they are called model-based stochastic
algorithms). This is most representative by the stochastic proximal point algorithm (SPPA) that tries to solve
(1)without the regularization term g(w)by the update wt+1=Proxλtfit(wt), whereitis randomly sampled
from the index set {1,...,n}(Bertsekas, 2011; Ryu & Boyd, 2014; Bianchi, 2016; Pătraşcu, 2020; Toulis et al.,
2021; Bumin & Huang, 2021). Perhaps due to the fact that the algorithm relies on implementing the proximal
operator of somewhat arbitrary functions, which could be as hard as solving the problem itself, follow-up
works focus on variations of SPPA that typically involve a (sub)gradient update (Wang & Bertsekas, 2013;
Duchi & Ruan, 2018; Asi & Duchi, 2019; Davis & Drusvyatskiy, 2019). As explained by Toulis et al. (2021)
and later in this paper as well, in the context of stochastic optimization the pertinent proximal operators
could be calculated efficiently, despite that they seem lack of structures to be exploited. There remains two
challenges for applying SPPA in practice:
1.How to handle the regularization term in a more systematic manner? A naive approach would simply
treat it as part of the stochastic function and cope with it stochastically, which is certainly not ideal.
On the other hand, since most existing regularization terms have efficient proximal operators, it
does not seem appropriate to use gradient-based update to handle it as suggested by some of the
SPPA variants such as (Wang & Bertsekas, 2013; Duchi & Ruan, 2018; Asi & Duchi, 2019; Davis &
Drusvyatskiy, 2019).
2.How to allow the algorithm to do mini-batch, i.e., using more than a single data sample to apply
the updates? This is trivial for SGD-based methods since any (sub-)gradient is additive, so their
stochastic gradients can be calculated in parallel and summed together. For SPPA, however, mini-
batch seems not only complicated but also hard to parallelize. One attempt was made in (Chadha
et al., 2022) but is parallelizable only if the approximation of the proximal update boils down to a
stochastic gradient step.
In this paper, we address the two issues with similar convergence guarantees. First we revisit the stochastic
Douglas-Rachford splitting (SDRS) to solve (1)with the regularization term. Then we propose SDRS
mini-batch, which is inspired by consensus Douglas-Rachford. The mini-batch version of SDRS also provides
a principled way of doing SPPA mini-batch that can be fully parallelized by essentially dropping the
regularization term. We show that for both cases the convergence guarantee of SDRS is exactly the same as
that of SPPA, i.e., with rate O(1/√
t)for convex problems and O(1/t)for strongly convex problems.
1.1 Stochastic Douglas-Rachford splitting (SDRS)
The stochastic Douglas-Rachford splitting (SDRS) is described in Algorithm 1. As we can see, the algorithm
involves two proximal operators in each iteration. The proximal operator of a function fat point ˜wis
defined as Proxf(˜w) =arg min wf(w) + (1/2)∥w−˜w∥2. Comparing SDRS with some well-known stochastic
algorithm, we notice that:
•if line 4 is replaced by a simple stochastic gradient update, then it becomes the proximal SGD
algorithm;
•if there is no regularization g, line 2 becomes wt+1=˜wt, and the algorithm becomes the stochastic
proximal point algorithm (SPPA) (Rockafellar, 1976);
•if the stochastic step in line 3 is deterministic, then it is the (deterministic) Douglas-Rachford splitting
(Lions & Mercier, 1979; Eckstein & Bertsekas, 1992), which is essentially equivalent to the celebrated
alternating direction method of multipliers (Gabay & Mercier, 1976; Boyd et al., 2011).
There have been attempts to analyze the convergence of SDRS (Shi & Liu, 2016; Salim et al., 2018), but they
both make the assumption that fiare differentiable, which is not ideal since differentiability is typically not
required by any proximal algorithm. We will provide convergence analysis for SDRS that does not require any
2Published in Transactions on Machine Learning Research (11/2022)
Algorithm 1 Stochastic Douglas-Rachford splitting Algorithm (SDRS)
1:initialize ˜w0
2:fort= 0,1,...,T do
3:wt+1←Proxλtg(˜wt)
4:randomly draw ituniformly from{1,...,n}
5: ˜wt+1←˜wt+ Proxλtfit(2wt+1−˜wt)−wt+1
6:end for
7:return ˆw= (/summationtextT
t=1λtwt)//summationtextT
t=1λt
function to be differentiable, as well as several additional results such as convergence under strong convexity
and a mini-batch extension.
1.2 SDRS with mini-batch
One of the important reasons why SGD-type algorithms are so widely used is that it effortlessly include
multiple samples in one stochastic update, called mini-batch. Due to linearity of differentiation, the mini-batch
stochastic gradient is simply the average of the fi’s that are sampled. Furthermore, each stochastic gradient
can be computed in parallel, and a central node only need to collect their average, making it extremely easy
to parallelize.
For SDRS, if each stochastic update involve multiple data samples, evaluating the proximal operator becomes
significantly more computationally demanding. Suppose we consider psamples in each stochastic step, not
only is the complexity be in general O(p2d+p3), but it is also hard to exploit parallelization. To address this
issue, we propose Algorithm 2 that is inspired by consensus optimization using Douglas-Rachford splitting,
which generates p+1sequences{˜w(1)
t},...,{˜w(p)
t}, and{wt}. At iteration t,i(k)
tis the sample index obtained
by processor k; in other words, each processor is in charge of computing only Proxλtf
i(k)
t, which is assumed
to be easy to compute as will be discussed in §3. Then the central node collects their average and computes
Proxλtg. The deterministic counterpart of Algorithm 2 is when p=nand eachi(k)
t=k, which would be
equivalent to consensus ADMM (Boyd et al., 2011) after reorganization of the algorithm.
Algorithm 2 Stochastic Douglas-Rachford splitting Algorithm (SDRS) with mini-batch
1:initialize ˜w(1)
0=···=˜w(p)
0=˜w0
2:fort= 0,1,...,T do
3:wt+1←Proxλtg/parenleftigg
1
pp/summationdisplay
k=1˜w(k)
t/parenrightigg
4:fork= 1,...,p do
5:randomly draw i(k)
tuniformly from{1,...,n}
6: ˜w(k)
t+1←˜w(k)
t+ Proxλtf
i(k)
t/parenleftig
2wt+1−˜w(k)
t/parenrightig
−wt+1
7:end for
8:end for
9:return ˆw= (/summationtextT
t=1λtwt)//summationtextT
t=1λt
An interesting observation is that if Problem (1)does not involve a regularization term g, then line 2 of
the algorithm reduces to a simple averaging of the individual proximal outputs. This also suggests that
one way of doing mini-batch for SPPA should be Algorithm 2 with line 2 being the average, which is not
the same as any of the proposed mini-batch methods in (Chadha et al., 2022). The mini-batch approach
proposed by (Chadha et al., 2022) allows approximate proximal operators, but is fully parallelizable only if
the approximation reduces to a stochastic gradient update. Our proposed mini-batch is the first variant of
SDRS and SPPA that is fully proximal and parallelizable.
3Published in Transactions on Machine Learning Research (11/2022)
2 Convergence Analysis
In this section, we provide convergence analysis of SDRS for general convex loss functions with a regularizer
(1)to a global minimum in expectation. In recent years there have been some work tackling the convergence
analysis of stochastic proximal point algorithms, e.g., (Bertsekas, 2011; Pătraşcu, 2020; Toulis et al., 2021)
and stochastic Douglas-Rachford splitting (Shi & Liu, 2016; Salim et al., 2018). The latter two are the most
relevant work, but they both make additional assumptions that fiare differentiable, which is uncommon
for proximal algorithms. Our result resembles that of SPPA by Bertsekas (2011), which is satisfying since
Douglas-Rachford splitting can be considered a natural extension to the proximal point algorithm to combine
two or more proximal operators.
In what follows, we make the assumption that each fi(·)is Lipschitz continuous with constant at least L, i.e.:
Assumption 2.1. There is a constant Lsuch that for all, w,˜w, andfi(·),|fi(w)−fi(˜w)|≤L∥w−˜w∥.
We note that this assumption is equivalent to the common assumption in analyses of stochastic (sub)gradient
methods that all stochastic (sub)gradients are upperbounded (Vandenberghe, 2020, pp. 3.3). This is also
the only assumption (other than convexity) used in establishing the convergence of SPPA (and some of its
variants) (Bertsekas, 2011). Since the gfunction is not handled in a stochastic fashion, it is pleasing to see
that it is not required to satisfy the Lipschitz continuous assumption.
For simplicity, in the main paper we provide convergence analysis in expectation . In the appendix we utilize
the supermartingale convergence theorem to establish convergence in probability . However, the key inequalities
are established in this section, and the rest of the steps are somewhat standardized, which we relegate to the
appendix.
2.1 Generic convex case without strong convexity
In this subsection we show that for general convex functions that are not necessarily strongly convex, SDRS
converges in expectation with rate O(1/√
t). This is the same rate as SPPA under the same assumption
(Bertsekas, 2011), as well as SGD under the same assumption (Bottou et al., 2018).
Theorem 2.2. Suppose all f1,...,fnandgare convex, and Assumption 2.1 holds. If a solution w⋆exists,
then with initialization ˜w0, the the sequence w1,...,wTgenerated by SDRS (Algorithm 1) satisfies
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
−/parenleftigg
1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆)/parenrightigg
≤∥˜w0−w⋆∥2+/summationtextT
t=1λ2
tL2
2/summationtextT
t=1λt(2)
Proof.As per the update rules defined in Algorithm 1, we have that
1
λt(˜wt−wt+1)∈∂g(wt+1)and1
λt(wt+1−˜wt+1)∈∂fit(˜wt+1−˜wt+wt+1).
Sincegandfitare convex, their first-order conditions imply
g(w⋆)≥g(wt+1) +1
λt(˜wt−wt+1)⊤(w⋆−wt+1) (3)
fit(w⋆)≥fit(˜wt+1−˜wt+wt+1) +1
λt(wt+1−˜wt+1)⊤(w⋆−˜wt+1+˜wt−wt+1). (4)
4Published in Transactions on Machine Learning Research (11/2022)
Adding (3) and (4) together and rearrange, we have
fit(˜wt+1−˜wt+wt+1) +g(wt+1)−fit(w⋆)−g(w⋆)
≤1
λt(wt+1−˜wt)⊤(w⋆−wt+1) +1
λt(˜wt+1−wt+1)⊤(w⋆−˜wt+1+˜wt−wt+1)
=1
λt(˜wt+1−˜wt)⊤(w⋆−wt+1) +1
λt(˜wt+1−wt+1)⊤(˜wt−˜wt+1)
=1
λt(˜wt+1−˜wt)⊤(w⋆−˜wt+1)
=1
2λt∥˜wt−w⋆∥2−1
2λt∥˜wt+1−w⋆∥2−1
2λt∥˜wt−˜wt+1∥2. (5)
We now invoke Assumption 2.1 to have
fit(wt+1)−L∥˜wt−˜wt+1∥≤fit(˜wt+1−˜wt+wt+1). (6)
Combining (5) and (6), we have
2λt(fit(wt+1) +g(wt+1)−fit(w⋆)−g(w⋆))≤
∥˜wt−w⋆∥2−∥˜wt+1−w⋆∥2−∥˜wt−˜wt+1∥2+ 2λtL∥˜wt−˜wt+1∥.
Furthermore, we notice that
−∥˜wt−˜wt+1∥2+ 2λtL∥˜wt−˜wt+1∥=−/parenleftbig
∥˜wt−˜wt+1∥2−λtL/parenrightbig2+λ2
tL2≤λ2
tL2.
As a result,
2λt(fit(wt+1) +g(wt+1)−fit(w⋆)−g(w⋆))≤∥˜wt−w⋆∥2−∥˜wt+1−w⋆∥2+λ2
tL2. (7)
Now we take conditional expectation of the random variable it, conditioned on wt+1and ˜wt; according to
the update rule in Algorithm 1, only the value of ˜wt+1—but not wt+1or˜wt—depends on it, we obtain the
inequality
2λt/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
≤∥˜wt−w⋆∥2−Eit∥˜wt+1−w⋆∥2+λ2
tL2.(8)
Taking total expectation of (8) and summing over the inequalities with t= 0,1,...,T, we have
T/summationdisplay
t=02λt/parenleftigg
E/bracketleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)/bracketrightigg
−/parenleftigg
1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆)/parenrightigg/parenrightigg
≤∥˜w0−w⋆∥2−E∥˜wT+1−w⋆∥2+T/summationdisplay
t=0λ2
tL2≤∥˜w0−w⋆∥2+T/summationdisplay
t=0λ2
tL2.
Dividing both sides by 2/summationtext
tλtand lowerbouding the left-hand-side by Jensen’s inequality (since the objective
function (1) is convex),
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)≤1/summationtextT
t=1λtT/summationdisplay
t=1λt/parenleftigg
1
nn/summationdisplay
i=1fi(wt) +g(wt)/parenrightigg
,
we obtain (2).
5Published in Transactions on Machine Learning Research (11/2022)
Remarks. The proof is somewhat reminiscent to that of SPPA by Bertsekas (2011). Indeed, if function g
is absent, SDRS reduces to SPPA since wt+1=˜wt, and it is pleasing to see that the same result of SPPA
holds for the more general SDRS. The fundamental observation in (Bertsekas, 2011) is that inequality (4)
gives an upperbound on fit(˜wt+1)−fit(w⋆); however, when taking expectation over the random variable
it, the variable ˜wt+1also depends on it, which would not lead to a meaningful expression. This is why we
need to invoke Assumption 2.1 to bound fit(˜wt)−fit(w⋆)instead. In the case of SDRS in (5), not only is
fitevaluated at a point that is dependent on it, but alsofitandgare evaluated at different points; when
changing the argument of fitto be the same as g, we also managed to make the argument independent of it,
thus similar steps can be made in the sequel.
Theorem 2.2 establishes inequality (2)for any choice of λt. We now detail some specific choices of the step
sizes. The basic arguments are essentially the same as those that have been well-studied for SGD.
Corollary 2.3 (Diminishing step sizes) .Suppose all f1,...,fnandgare convex, and Assumption 2.1 holds.
If a solution w⋆exists, then with initialization ˜w0, the sequence w1,...,wTgenerated by SDRS (Algorithm 1)
with diminishing step sizes such that λt→0and/summationtext∞
t=1λt=∞satisfies
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
→1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆). (9)
Proof.Suppose the step sizes are square summable, i.e.,/summationtext
tγ(t)2<∞, then it is obvious that on the
right-hand-side of (2), the numerator is finite while the denominator goes to infinity, which proves the result.
Even if the step sizes are not square summable either, it can be shown that the quotient/summationtext
tλ2
t//summationtext
tλt→0.
A proof can be found in the supplementary, even though it is not new.
Corollary 2.3 suggests that some well-known diminishing rules such as λt=τ/torλt=τ/√
tall guarantee
expected convergence of SDRS.
Corollary 2.4 (Constant step sizes) .Suppose all f1,...,fnandgare convex, and Assumption 2.1 holds. If
a solution w⋆exists, then with initialization ˜w0, the sequence w1,...,wTgenerated by SDRS (Algorithm 1)
with constant step size λt=λsatisfies
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
−/parenleftigg
1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆)/parenrightigg
≤∥˜w0−w⋆∥2
2Tλ+L2
2λ. (10)
This means:
•LettingT→∞, the expected optimality gap is upperbounded by a constant times λ.
•For a given T, the right-hand-side is minimized by letting λ=∥˜w0−w⋆∥/L√
T, and substituting it
back in(10)shows
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
−/parenleftigg
1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆)/parenrightigg
≤∥˜w0−w⋆∥L√
T.
The proof is straight-forward and thus omitted. This shows that the expected convergence rate is O(1/√
t),
which is the same as SPPA (Bertsekas, 2011) and SGD.
2.2 Strongly convex case
We now provide improved convergence analysis when gis strongly convex. This is particularly useful when
the regularization function is the Tikhonov regularization (Euclidean norm squared).
6Published in Transactions on Machine Learning Research (11/2022)
Proposition 2.5. If allf1,...,fnare convex, gis strongly convex with parameter µ, and Assumption 2.1
holds, then with initialization ˜w0, the sequence w1,...,wTgenerated by SDRS (Algorithm 1) with a constant
step sizeλsatisfies
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
−/parenleftigg
1
nn/summationdisplay
i=1fi(w⋆)+g(w⋆)/parenrightigg
≤µ∥˜w0−w⋆∥2
(1 +λµ/2)T−1+λL2+λ2µL2
2.(11)
This means SDRS converges, in expectation, linearly to a suboptimal point with gap proportional to λ+µλ2.
Ifλis small, which is usually the case in practice, then the gap is dominated by a constant times λ.
Proof.Similar to the first few steps of the proof of Theorem 2.2, but replacing (3)with the strongly convex
form
g(w⋆)≥g(wt+1) +1
λt(˜wt−wt+1)⊤(w⋆−wt+1) +µ
2∥w⋆−wt+1∥2,
we get
2λt(fit(wt+1) +g(wt+1)−fit(w⋆)−g(w⋆))≤∥˜wt−w⋆∥2−∥˜wt+1−w⋆∥2+λ2
tL2−λtµ∥w⋆−wt+1∥2.
Using the inequality ∥a+b∥2≤2∥a∥2+ 2∥b∥2, we can further upperbound the right-hand-side by
2λt(fit(wt+1) +g(wt+1)−fit(w⋆)−g(w⋆))
≤∥˜wt−w⋆∥2−∥˜wt+1−w⋆∥2+λ2
tL2−λtµ
2∥w⋆−˜wt+1∥2+λtµ∥˜wt+1−wt+1∥2
≤∥˜wt−w⋆∥2−/parenleftbigg
1 +λtµ
2/parenrightbigg
∥˜wt+1−w⋆∥2+λ2
tL2+λ3
tµL2,
where we upperbound ∥˜wt+1−wt+1∥2≤λ2
tL2because (1/λt)(˜wt+1−wt+1)is a subgradient at fit(˜wt+1−
˜wt+wt+1), of which the norm is no more than Ldue to Assumption 2.1.
Now we take conditional expectation of the random variable it, conditioned on wt+1and ˜wt; according to
the update rule in Algorithm 1, only the value of ˜wt+1—but not wt+1or˜wt—depends on it, we obtain
2λt/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
≤∥˜wt−w⋆∥2−/parenleftbigg
1 +λtµ
2/parenrightbigg
Eit∥˜wt+1−w⋆∥2+λ2
tL2+λ3
tµL2.(12)
Letting the step sizes be constant λt=λ, we multiply both sides by (1 +λµ/2)t, take total expectation, and
sum overt= 0,...,Tto get
2λT/summationdisplay
t=0/parenleftbigg
1 +λµ
2/parenrightbiggt/parenleftigg
E/bracketleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)/bracketrightigg
−/parenleftigg
1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆)/parenrightigg/parenrightigg
≤∥˜w0−w⋆∥2+ (λ2L2+λ3µL2)T/summationdisplay
t=1/parenleftbigg
1 +λµ
2/parenrightbiggt
.
Applying the geometric series/summationtextT
t=0(1 +λµ/2)t= ((1 +λµ/2)T−1)/(λµ/2), and lowerbound all the expected
function values on left-hand-side by Jensen’s inequality, we get (11).
We now show that with diminishing step sizes, the expected optimality gap converges to zero at O(1/t)rate.
Theorem 2.6. If allf1,...,fnare convex, gis strongly convex with parameter µ, and Assumption 2.1 holds.
If a solution w⋆exists, then with initialization ˜w0, the sequence w1,...,wTgenerated by SDRS (Algorithm 1)
with diminishing step sizes λt=β/twhereβ >4/µ, then
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
≤2∥˜w0−w⋆∥2
βT(T+ 1)+2L2β(1 +µβ)
T+ 1. (13)
7Published in Transactions on Machine Learning Research (11/2022)
Whentgoes relatively large, the right-hand-side is dominated by the second term, which yields the O(1/t)
convergence rate.
Proof.Taking total expectation of (12) and plug in the step sizes λt=β/t, we have
E/bracketleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/bracketrightigg
≤t
2βE∥˜wt−w⋆∥2−2t+µβ
4βE∥˜wt+1−w⋆∥2+L2β
2t+β2µL2
2t2
≤t
2βE∥˜wt−w⋆∥2−t+ 2
2βE∥˜wt+1−w⋆∥2+L2β(1 +βµ)
2t,
where the last inequality stems from µβ > 4and1/t2<1/tfort≥1. Multiplying both sides by t+ 1yields
(t+ 1) E/bracketleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/bracketrightigg
≤t(t+ 1)
2βE∥˜wt−w⋆∥2−(t+ 1)(t+ 2)
2βE∥˜wt+1−w⋆∥2+L2β(1 +βµ)(t+ 1)
2t
≤t(t+ 1)
2βE∥˜wt−w⋆∥2−(t+ 1)(t+ 2)
2βE∥˜wt+1−w⋆∥2+L2β(1 +βµ),
where, again, the last inequality is due to (t+ 1)/2t<1fort≥1. Now lett= 1,2,...,Tand sum over all
inequalities, we get
T/summationdisplay
t=1tE/bracketleftigg
1
nn/summationdisplay
i=1fi(wt) +g(wt)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/bracketrightigg
≤1
βE∥˜w1−w⋆∥2−T(T+ 1)
2βE∥˜wT−w⋆∥2+TL2β(1 +βµ)
≤1
βE∥˜w1−w⋆∥2+TL2β(1 +βµ).
Lowerbounding the left-hand-side by Jensen’s inequality and dividing both sides by 1+2+···+T=T(T+1)/2
yields (13).
2.3 Convergence of mini-batch SDRS
Theorem 2.7. The convergence of mini-batch SDRS as in Algorithm 2 is exactly the same as SDRS, i.e.,
•for generic convex functions, we have
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
−/parenleftigg
1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆)/parenrightigg
≤∥˜w0−w⋆∥2+/summationtextT
t=1λ2
tL2
2/summationtextT
t=1λt,(14)
which leads to a convergence rate of O(1/√
t)with appropriately chosen step sizes;
•ifgis strongly convex with parameter µ, then with diminishing step sizes λt=β/twhereβ >4/µ,
then
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
≤2∥˜w0−w⋆∥2
βT(T+ 1)+2L2β(1 +µβ)
T+ 1. (15)
WhenTgoes relatively large, the right-hand-side is dominated by the second term, which yields the
O(1/t)convergence rate.
8Published in Transactions on Machine Learning Research (11/2022)
Proof.As per the update rules defined in Algorithm 2, we have that
1
λt/parenleftigg
1
pp/summationdisplay
k=1˜w(k)
t−wt+1/parenrightigg
∈∂g(wt+1)
and1
λt(wt+1−˜w(k)
t+1)∈∂fi(k)
t(˜w(k)
t+1−˜w(k)
t+wt+1),k= 1,...,p.
Due to (strong) convexity, their first-order conditions imply
g(w⋆)≥g(wt+1) +1
λt/parenleftigg
1
pp/summationdisplay
k=1˜w(k)
t−wt+1/parenrightigg⊤
(w⋆−wt+1) +µ
2∥w⋆−wt+1∥2,
fi(k)
t(w⋆)≥fi(k)
t(˜w(k)
t+1−˜w(k)
t+wt+1) +1
λt(wt+1−˜w(k)
t+1)⊤(w⋆−˜w(k)
t+1+˜w(k)
t−wt+1),k= 1,...,p.
Summing them up gives
2λt/parenleftigg
1
pp/summationdisplay
k=1fi(k)
t(˜w(k)
t+1−˜w(k)
t+wt+1) +g(wt+1)−1
pp/summationdisplay
k=1fi(k)
t(w⋆)−g(w⋆)/parenrightigg
≤2/parenleftigg
wt+1−1
pp/summationdisplay
k=1˜w(k)
t/parenrightigg⊤
(w⋆−wt+1) +2
pp/summationdisplay
k=1(˜w(k)
t+1−wt+1)⊤(w⋆−˜w(k)
t+1+˜w(k)
t−wt+1)
−µλt∥w⋆−wt+1∥2
=2
pp/summationdisplay
k=1/parenleftig
˜w(k)
t+1−˜w(k)
t/parenrightig⊤
(w⋆−wt+1)−2
pp/summationdisplay
k=1(˜w(k)
t+1−wt+1)⊤(˜w(k)
t+1−˜w(k)
t)
−µλt∥w⋆−wt+1∥2
=2
pp/summationdisplay
k=1/parenleftig
˜w(k)
t+1−˜w(k)
t/parenrightig⊤
(w⋆−˜w(k)
t+1)−µλt∥w⋆−wt+1∥2
=1
pp/summationdisplay
k=1/parenleftig
∥w⋆−˜w(k)
t∥2−∥w⋆−˜w(k)
t+1∥2−∥˜w(k)
t+1−˜w(k)
t∥2/parenrightig
−µλt∥w⋆−wt+1∥2(16)
The rest of the steps follows almost identical to those in the proof of Theorem 2.2 after (5). Invoking
Assumption 2.1 to each fi(k)
t, we have
fi(k)
t(wt+1)−L∥˜w(k)
t+1−˜w(k)
t∥≤fi(k)
t(˜w(k)
t+1−˜w(k)
t+wt+1), k = 1,...,p.
Plugging it into (16) gives
2λt/parenleftigg
1
pp/summationdisplay
k=1fi(k)
t(wt+1) +g(wt+1)−1
pp/summationdisplay
k=1fi(k)
t(w⋆)−g(w⋆)/parenrightigg
≤1
pp/summationdisplay
k=1/parenleftig
∥w⋆−˜w(k)
t∥2−∥w⋆−˜w(k)
t+1∥2−∥˜w(k)
t+1−˜w(k)
t∥2+ 2λt∥˜w(k)
t+1−˜w(k)
t∥/parenrightig
−µλt∥w⋆−wt+1∥2.
Also notice that
−∥˜w(k)
t+1−˜w(k)
t∥2+ 2λt∥˜w(k)
t+1−˜w(k)
t∥=−/parenleftig
∥˜w(k)
t+1−˜w(k)
t∥−λtL/parenrightig2
+λ2
tL2≤λ2
tL2,
This implies
2λt/parenleftigg
1
pp/summationdisplay
k=1fi(k)
t(wt+1) +g(wt+1)−1
pp/summationdisplay
k=1fi(k)
t(w⋆)−g(w⋆)/parenrightigg
≤1
pp/summationdisplay
k=1/parenleftig
∥w⋆−˜w(k)
t∥2−∥w⋆−˜w(k)
t+1∥2/parenrightig
+λ2
tL2−µλt∥w⋆−wt+1∥2.
9Published in Transactions on Machine Learning Research (11/2022)
Using the inequality ∥a+b∥2≤2∥a∥2+ 2∥b∥2, we can further upperbound the right-hand-side by
2λt/parenleftigg
1
pp/summationdisplay
k=1fi(k)
t(wt+1) +g(wt+1)−1
pp/summationdisplay
k=1fi(k)
t(w⋆)−g(w⋆)/parenrightigg
≤1
pp/summationdisplay
k=1/parenleftig
∥w⋆−˜w(k)
t∥2−∥w⋆−˜w(k)
t+1∥2/parenrightig
+λ2
tL2−µλt
pp/summationdisplay
k=1/parenleftbigg1
2∥w⋆−˜w(k)
t+1∥2−∥˜w(k)
t+1−wt+1∥2/parenrightbigg
≤1
pp/summationdisplay
k=1/parenleftbigg
∥w⋆−˜w(k)
t∥2−/parenleftbigg
1 +λtµ
2/parenrightbigg
∥w⋆−˜w(k)
t+1∥2/parenrightbigg
+λ2
tL2+λ3
tµL2. (17)
The next step is to take conditional expectation of the random variables i(k)
tin(17), conditioned on wt+1
and each of ˜w(k)
t; according to the update rule in Algorithm 2, only the values of ˜w(k)
t+1—but not wt+1or any
of˜w(k)
t—depends on i(k)
t. Specifically, on the left-hand-side, we have that for each i(k)
t:
Ei(k)
tfi(k)
t(wt+1) =1
nn/summationdisplay
i=1fi(wt+1)and Ei(k)
tfi(k)
t(w⋆) =1
nn/summationdisplay
i=1fi(w⋆),
so
2λt/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
≤1
pp/summationdisplay
k=1/parenleftbigg
∥w⋆−˜w(k)
t∥2−/parenleftbigg
1 +λtµ
2/parenrightbigg
Ei(k)
t∥w⋆−˜w(k)
t+1∥2/parenrightbigg
+λ2
tL2+λ3
tµL2. (18)
Taking total expectation of (18) gives
2λtE/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
≤1
pp/summationdisplay
k=1/parenleftbigg
E∥w⋆−˜w(k)
t∥2−/parenleftbigg
1 +λtµ
2/parenrightbigg
E∥w⋆−˜w(k)
t+1∥2/parenrightbigg
+λ2
tL2+λ3
tµL2. (19)
Now we separate the cases of µ= 0andµ>0:
•Ifµ= 0, then taking summation of (19) with t= 0,...,Tgives
2T/summationdisplay
t=0λtE/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
≤1
pp/summationdisplay
k=1E∥w⋆−˜w(k)
0∥2+T/summationdisplay
t=0λ2
tL2.
Since each ˜w(k)
0is initialized at the same point ˜w0, then we have (14)in Theorem 2.7. For a given
T, if we use constant step sizes λt=∥˜w0−w⋆∥/L√
T, then
E/bracketleftigg
1
nn/summationdisplay
i=1fi(ˆw) +g(ˆw)/bracketrightigg
−/parenleftigg
1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆)/parenrightigg
≤∥˜w0−w⋆∥L√
T,
which shows the O(1/√
t)convergence rate in expectation.
10Published in Transactions on Machine Learning Research (11/2022)
•Ifµ>0, then plugging in the step size λt=β/tinto (19) gives
E/bracketleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/bracketrightigg
≤t
2βpp/summationdisplay
k=1E∥˜w(k)
t−w⋆∥2−2t+µβ
4βpp/summationdisplay
k=1E∥˜w(k)
t+1−−w⋆∥2+L2β
2t+β2µL2
2t2
≤t
2βpp/summationdisplay
k=1E∥˜w(k)
t−w⋆∥2−t+ 2
2βpp/summationdisplay
k=1E∥˜w(k)
t+1−w⋆∥2+L2β(1 +βµ)
2t,
where the last inequality stems from µβ > 4and1/t2<1/tfort≥1. Multiplying both sides by
t+ 1yields
(t+ 1) E/bracketleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/bracketrightigg
≤t(t+ 1)
2βpp/summationdisplay
k=1E∥˜w(k)
t−w⋆∥2−(t+ 1)(t+ 2)
2βpp/summationdisplay
k=1E∥˜w(k)
t+1−w⋆∥2+L2β(1 +βµ)(t+ 1)
2t
≤t(t+ 1)
2βpp/summationdisplay
k=1E∥˜w(k)
t−w⋆∥2−(t+ 1)(t+ 2)
2βpp/summationdisplay
k=1E∥˜w(k)
t+1−w⋆∥2+L2β(1 +βµ),
where, again, the last inequality is due to (t+ 1)/2t<1fort≥1. Now lett= 1,2,...,Tand sum
over all inequalities, we get
T/summationdisplay
t=1tE/bracketleftigg
1
nn/summationdisplay
i=1fi(wt) +g(wt)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/bracketrightigg
≤1
βpp/summationdisplay
k=1E∥˜w(k)
0−w⋆∥2−T(T+ 1)
2βpp/summationdisplay
k=1E∥˜w(k)
T−w⋆∥2+TL2β(1 +βµ)
≤1
βpp/summationdisplay
k=1E∥˜w(k)
0−w⋆∥2+TL2β(1 +βµ).
Since each ˜w(k)
0is initialized at the same point ˜w0, then lowerbounding the left-hand-side by Jensen’s
inequality and dividing both sides by 1 + 2 +···+T=T(T+ 1)/2yields (15).
3 Efficient Computation of the Proximal Operators
In this section, we consider how to efficiently implement SDRS for regularized ERM problems (1). In each
iteration of Algorithm 1, it involves computing two proximal operators, one for the regularization function g
and one for the loss of one sample fi. It is well-known that most regularization functions admit a proximal
operator that is efficient to compute, such as the well-known soft-thresholding for L1norm regularization to
promote sparse solutions, or block soft-thresholding for sum of Euclidean norms to promote group sparsity.
More comprehensive surveys of similar proximal operators have been studied in many works, for example
by Parikh & Boyd (2014). As for the proximal operator for fiin line 4 of Algorithm 1, it may seem just as
challenging as solving the batch problem. However, we will see that there are some interesting properties
whenfiinvolves the loss evaluation of just one or a few data points in a data fitting scenario.
In an ERM formulation for supervised learning, we can often write the one-sample loss function as fi(w) =
ℓ(x⊤
iw−yi)for regression or fi(w) =ℓ(yix⊤
iw)for binary classification, which means it is a scalar function
composedwith x⊤
iw. Supposeitisdifferentiable, thenbyapplyingthechainrule, wehave ∇fi(w) =xiℓ′(x⊤
iw),
11Published in Transactions on Machine Learning Research (11/2022)
whereℓ′is the derivative of the scalar function ℓ(·). This is also truewhen fiis nonsmooth that any subgradient
is some scaled version of xi. Plugging it into the optimality condition, we get that
Proxλfi(w) =w−αxi, (20)
for some scalar α. The proximal update further reduces to finding the scalar αthat solves the following
single-variate convex problem,
minimize
αℓ(x⊤
iw−α∥xi∥2) +α2
2λt∥xi∥2. (21)
This can be done by simply setting the derivative equal to zero. Due to convexity, its derivative is a monotonic
function, so one naive way to find the root is via bisection. In some cases the choice of αeven has a closed-form
expression. In the supplementary we include the specific proximal operator for the least squares loss, logistic
loss, hinge loss, and absolute error loss, which are some of the most widely used losses in ERM.
The special form of the proximal update (20)when the one-sample loss is the composition of a scalar convex
function and linear function also gives an interesting explicit form of SDRS. If line 4 of Algorithm 1 is in the
form of(20), then line 4 simplifies to ˜wt+1←wt+1−αxi; plugging it into line 2 further simplifies SDRS to
wt+1←Proxλtg(wt−αxi),
which looks surprisingly similar to the proximal stochastic gradient descent, except that the step size αis
chosen more delicately via solving (21). As we will see very soon in the next section, this delicate choice often
makes a big difference in terms of convergence in practice. Similar arguments can be made for the mini-batch
version, making the iterates having the form
wt+1←Proxλtg/parenleftigg
wt−1
pp/summationdisplay
k=1α(k)x(k)
i(k)
t/parenrightigg
,
where each α(k)is calculated individually according to its own (21). This makes mini-batch SDRS more
sophisticated than mini-batch SGD, as the search direction takes a weighted sum of each samples to properly
reflect their contribution in the minimization.
All of the aforementioned discussion on the specific implementation of SDRS only applies when the loss
function is in the form fi(w) =ℓ(x⊤
iw−yi)orℓ(yix⊤
iw). However, even for generic nonlinear programming
problems, it is still possible to efficiently evaluate the proximal update. If acquiring first-order information
offiis not computationally expensive, one can use methods such as L-BFGS (Nocedal & Wright, 2006) or
accelerated gradient descent (Nesterov, 1983) to compute Proxλfi(·)with approximately O(d)complexity.
4 Experiments
In this section, we show the performance of SDRS with proposed efficient implementation on both classification
and regression tasks, and compare it with some widely used alternatives such as SGD-momentum (Liu et al.,
2020), Adam (Kingma & Ba, 2014), and AdaBelief (Zhuang et al., 2020). All formulations include an L1
norm regularization, and each SGD-type algorithm is followed by soft-thresholding to handle it. More real
data experiments can be found in the appendix.
4.1 Classification
We perform binary classification by using SVM and logistic regression with L1norm regularization on the bank
note authentication data set(Dua & Graff, 2017) This includes images of genuine and counterfeit banknotes.
There are 1372 images in total. There are five attributes in each image, out of which four are features and
one is the target attribute. The target attribute contains 0 and 1, where 0 indicates genuine notes and 1
indicates fake notes. The ratio between the two classes is 55/45 (genuine/counterfeit). We present Figures 1
and 2, illustrating SDRS’s performance in loss-time and accuracy-time respectively. Figure 1 illustrates how
12Published in Transactions on Machine Learning Research (11/2022)
Figure 1: SVM with L1regularization on Bank Note
Authentication: loss per seconds, batch size 1
Figure 2: SVM with L1regularization on Bank Note
Authentication: classification accuracy per seconds,
batch size 1
Figure 3: Logistic regression with L1regularization
on Bank Note Authentication: loss per seconds
Figure 4: Logistic regression with L1regularization
on Bank Note Authentication: classification accuracy
per seconds
the regularized hinge loss value decreases with time (seconds) by comparing different algorithms. Figures 1
and 2 demonstrate that using our efficient implementation, SDRS takes less time to achieve a smaller loss
and greater accuracy despite its complexity per iteration.
In Figures 3 and 4 we present results on a logistic regression model with L1regularization on Bank Note
Authentication dataset, representing loss/time and accuracy/time respectively. As seen in Figure 3, SDRS
achieves a lower loss value in less time than all other algorithms; the closest performance is achieved by
Stochastic Gradient with Momentum. With reference to Figure 4, SDRS achieves the highest accuracy rate
in the shortest amount of time, and it has the lowest variance of all of the algorithms.
4.2 Classification using mini batch
We also run the classification experiments using the Bank Note Authentication data set. Figures 5 and 6
show the loss and accuracy values of SDRS on Bank Note Authentication dataset for SVM approach. Our
modification allows SDRS to handle larger batches. We observe that the SDRS still outperforms all the other
methods. We also use Logistic Regression on the same Bank Note Authentication dataset. The Figure 7 and
8 represent the loss and accuracy values respectively. We observe that SDRS reaches a smaller value in the
first few iterations.
13Published in Transactions on Machine Learning Research (11/2022)
Figure 5: SVM on Bank Note Authentication data
set: loss per seconds, batch size 4
Figure 6: SVM on Bank Note Authentication data
set: accuracy per seconds, batch size 4
Figure 7: Logistic Regression on Bank Note Authen-
tication data set: loss per seconds, batch size 16
Figure 8: Logistic Regression on Bank Note Authen-
tication data set: accuracy per seconds, batch size
16
4.3 Neural network training using mini-batch SPPA
As suggested in §1.2, the proposed mini-batch SDRS in Algorithm 2 also suggests a fully parallelizable
mini-batch SPPA when the regularization term gdoes not exist, as opposed to the one that cannot be fully
parallelized by Chadha et al. (2022). In this section we apply mini-batch SPPA to the nonconvex problem of
neural network training. We apply it on two famous image classification data sets, CIFAR10 and MNIST,
with two different network architectures, CNN and ResNet.
CIFAR10 (Krizhevsky et al., 2010) consists of 50,000 training and 10,000 test images of size 32×32in 10
classes. The performance of CNN training on CIFAR10 is shown in Figures 9 and 10. The network consists of
5 layers, each of the first 2 being a combination of 5×5convolutional filters and 2×2max pooling, and the
last three being fully connected. The loss function is cross-entropy loss, and the activation function is ReLU.
The batch size is 4. To show the behavior of the algorithms more in detail, we recorded the loss values at
every 500 stochastic updates, the experiment is run for five epochs, and in total, 125 updates are presented
(25×500×4giving the number of training samples). The accuracy is calculated on the test data at every
500 stochastic update overall for five epochs, using the accuracy calculation in the PyTorch tutorial.
5 Conclusion
We studied stochastic Douglas-Rachford splitting (SDRS) in this paper. Our first contribution is to provide
convergence analysis of SDRS, thus closing the theoretical gap. We also discussed various implementation
14Published in Transactions on Machine Learning Research (11/2022)
Figure 9: CNN on CIFAR10: cross entropy loss on
test data
Figure 10: CNN on CIFAR10: prediction accuracy
on test data
issues to make it more applicable for practical use of training regularized ERM problems, including a
mini-batch strategy that keeps the simplicity of pertinent proximal operators. Our experiments showed that
SDRS is able to perform better than many well-known stochastic algorithms for various data sets.
References
Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In STOC, 2017.
Zeyuan Allen-Zhu. Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization.
InProceedings of the 35th International Conference on Machine Learning , ICML ’18, 2018a.
Zeyuan Allen-Zhu. How To Make the Gradients Small Stochastically. In Proceedings of the 32nd Conference
on Neural Information Processing Systems , NeurIPS ’18, 2018b.
Nuno Antonio, Ana de Almeida, and Luis Nunes. Hotel booking demand datasets. Data in brief , 22:41–49,
2019.
Hilal Asi and John C Duchi. Stochastic (approximate) proximal point methods: Convergence, optimality,
and adaptivity. SIAM Journal on Optimization , 29(3):2257–2290, 2019.
Dimitri P Bertsekas. Incremental proximal methods for large scale convex optimization. Mathematical
Programming , 129(2):163, 2011.
Pascal Bianchi. Ergodic convergence of a stochastic proximal point algorithm. SIAM Journal on Optimization ,
26(4):2235–2260, 2016.
Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
SIAM Review , 60(2):223–311, 2018.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and Trends ®in
Machine learning , 3(1):1–122, 2011.
Aysegul Bumin and Kejun Huang. Efficient implementation of stochastic proximal point algorithm for matrix
and tensor completion. In 2021 29th European Signal Processing Conference (EUSIPCO) , pp. 1050–1054.
IEEE, 2021.
Karan Chadha, Gary Cheng, and John Duchi. Accelerated, optimal and parallel: Some results on model-based
stochastic optimization. In International Conference on Machine Learning , pp. 2811–2827. PMLR, 2022.
15Published in Transactions on Machine Learning Research (11/2022)
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions.
SIAM Journal on Optimization , 29(1):207–239, 2019.
Aaron Defazio. A simple practical accelerated method for finite sums. Advances in Neural Information
Processing Systems , 29:676–684, 2016.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in Neural Information Processing
Systems, pp. 1646–1654, 2014.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/
ml.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research , 12(Jul):2121–2159, 2011.
John C Duchi and Feng Ruan. Stochastic methods for composite and weakly convex optimization problems.
SIAM Journal on Optimization , 28(4):3229–3259, 2018.
Jonathan Eckstein and Dimitri P Bertsekas. On the douglas—rachford splitting method and the proximal
point algorithm for maximal monotone operators. Mathematical Programming , 55(1):293–318, 1992.
Hadi Fanaee-T and Joao Gama. Event labeling combining ensemble detectors and background knowledge.
Progress in Artificial Intelligence , pp. 1–15, 2013. ISSN 2192-6352. doi: 10.1007/s13748-013-0040-3.
Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational problems via
finite element approximation. Computers & Mathematics with Applications , 2(1):17–40, 1976.
Simon Haykin. Adaptive Filtering Theory . Prentice Hall, 2002.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 770–778, 2016.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
InAdvances in Neural Information Processing Systems , pp. 315–323, 2013.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd
International Conference on Learning Representations , 2014.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL
http://www. cs. toronto. edu/kriz/cifar. html , 5, 2010.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. 2010. URL http://yann.
lecun. com/exdb/mnist , 7:23, 2010.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg
methods. In Advances in Neural Information Processing Systems , pp. 2348–2358, 2017.
Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM
Journal on Numerical Analysis , 16(6):964–979, 1979.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with momentum.
Advances in Neural Information Processing Systems , 33:18261–18271, 2020.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies , pp. 142–150, Portland, Oregon, USA, June
2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P11-1015 .
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o(1/k2). In
Dokl. akad. nauk Sssr , volume 269, pp. 543–547, 1983.
16Published in Transactions on Machine Learning Research (11/2022)
Jorge Nocedal and Stephen Wright. Numerical optimization . Springer Science & Business Media, 2006.
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends ®in Optimization , 1(3):
127–239, 2014.
Andrei Pătraşcu. New nonasymptotic convergence rates of stochastic proximal point algorithm for stochastic
convex optimization. Optimization , pp. 1–29, 2020.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical
Statistics , pp. 400–407, 1951.
R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and
Optimization , 14(5):877–898, 1976.
Ernest K Ryu and Stephen Boyd. Stochastic proximal iteration: a non-asymptotic improvement upon
stochastic gradient descent. Preprint, 2014.
Adil Salim, Pascal Bianchi, and Walid Hachem. A constant step stochastic douglas-rachford algorithm with
application to non separable regularizations. In 2018 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pp. 2886–2890. IEEE, 2018.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average
gradient. Mathematical Programming , 162(1-2):83–112, 2017.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss
minimization. Journal of Machine Learning Research , 14(2), 2013.
Ziqiang Shi and Rujie Liu. Online and stochastic douglas-rachford splitting method for large scale machine
learning. In ACML workshop on Learning on Big Data , 2016.
Panos Toulis, Thibaut Horel, and Edoardo M Airoldi. The proximal Robbins–Monro method. Journal of the
Royal Statistical Society: Series B (Statistical Methodology) , 83(1):188–212, 2021.
Lieven Vandenberghe. Lecture notes of ECE236C Optimization Methods for Large-Scale Systems, 2020.
Vladimir Vapnik. Principles of risk minimization for learning theory. Advances in Neural Information
Processing Systems , 4, 1991.
Mengdi Wang and Dimitri P Bertsekas. Incremental constraint projection-proximal methods for nonsmooth
convex optimization. SIAM J. Optim.(to appear) , 2013.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris,
and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances
in Neural Information Processing Systems , 33:18795–18806, 2020.
A Efficient Implementation
In this section, we introduce several efficient methods to calculate the proximal operator update in Algorithm 1
line 5. At first glance, it may seem as hard as solving the batch problem itself, but we will see that there are
some interesting properties when fiinvolves the loss evaluation of only one or a few data points in a data
fitting scenario. On the other hand, it is not clear how to parallelize SPPA with mini-batches, which leaves
room for future work.
17Published in Transactions on Machine Learning Research (11/2022)
A.1 Least squares loss
For the least squares loss fi= (yi−x⊤
iw)2, the proximal update is a linear least squares problem with
closed-form solution Proxfi(w) = (I+λtxix⊤
i)−1(w+xiyi). It can be efficiently calculated by invoking the
Sherman-Morrison formula and avoid directly inverting a d×dmatrix for w∈Rdas
Proxfi(w) =w−x⊤
iw−yi
∥xi∥2+ 1/λtxi.
The update rule looks like, but is not exactly the same as, the normalized least mean squares (NLMS)
algorithm (Haykin, 2002). It has appeared in (Pătraşcu, 2020), but we include it here for completeness.
A.2 Logistic (cross-entropy) loss
Consider the logistic loss fi(w) =log(1 + exp(−yixT
iw))withyi=±1. According to the arguments made in
§3, we need to solve the nonlinear equation
α=yiλtexp(x⊤
iw−α∥xi∥2)
1 + exp( x⊤
iw−α∥xi∥2).
Notice that the right-hand-side is a number between 0and±λt, which gives the initial upper and lowerbound
onα. Furthermore, we see that x⊤
iwand∥xi∥2only need to be calculated once, with O(d)flops for w∈Rd;
each bisection step takes constant time and it needs no more than 20∼30scalar computations to render an
accurate-enough solution.
A.3 Hinge loss
Regarding nonsmooth optimization problems, it turns out many of the widely used loss functions admit
closed form updates. The main idea is to consider the subgradient calculus, and find the point where 0 is in
the subdifferential. Take support vector machine (SVM) as an example, in which the loss function is the
hinge lossfi(w) = [1−yix⊤
iw]+. Its subdifferential is
∂fi(w) =

{0} 1−yix⊤
iw<0,
{−yixi} 1−yix⊤
iw>0,
{−αyixi|0≤α≤1}1−yix⊤
iw= 0.
Added with the gradient of the proximal term and letting 0to be an element of the subdifferential set, we get
the update rule
Proxfi(w) =

w yix⊤
iw>1,
w+λtyixiyix⊤
iwt<1−λt∥xi∥2,
w+λtyixi1−yix⊤
iw
yi∥xi∥2otherwise.
An interesting observation here is that it looks like the perceptron algorithm with an adaptively chosen
step-size.
A.4 Absolute error loss
Robust regression using absolute error loss fi(w) =|yi−x⊤
iw|, is another example to nonsmooth convex loss
function. The derivation is similar to that of SVM: the subdifferential for the one-sample loss is
∂fi(w) =

{xi} yi−x⊤
iw<0,
{−xi} yi−x⊤
iw>0,
{αxi|−1≤α≤1}yi−x⊤
iw= 0.
18Published in Transactions on Machine Learning Research (11/2022)
After adding the proximal term, the update rule is
Proxλfi(w) =

w−λxiyi−x⊤
iw<−λ∥xi∥2,
w+λxiyi−x⊤
iw>λ∥xi∥2,
w+yi−x⊤
iw
∥xi∥2xiotherwise.
A.5 Generic losses
Even for generic nonlinear programming problems, it is still possible to efficiently evaluate the proximal
update beyond merely a simple gradient step. The idea is to apply the limited-memory BFGS algorithm
(Nocedal & Wright, 2006), or L-BFGS for short. L-BFGS is a memory-efficient implementation of the famous
quasi-Newton algorithm BFGS. In a nut shell, L-BFGS is an iterative algorithm that makes use of the
second-order information from the optimization loss function, but does not require solving matrix inverses
and only requires explicitly evaluating first-order gradients. For a prescribed number of iterations, it requires
one matrix-vector multiplication and multiple vector multiplications. As a result, the overall complexity is
again O(d)if the initial guess of the Hessian matrix is diagonal.
While there are certain limitations for applying L-BFGS to general nonlinear programming problems, we
reckon that it fits perfectly in the context of SPPA implementations.
•L-BFGS has to specify a good initial guess of the Hessian approximation matrix. While in many cases
people simply use the identity matrix to start, it may result in very poor approximation. Fortunately,
thanks to the proximal term, the identity matrix is in fact a very good initial guess for the Hessian
matrix for SPPA updates.
•In order to save memory consumption, L-BFGS has to prescribe the number of iterations before
running the algorithm. Obviously, if the prescribed number of iteration is too large, we incur
unnecessary computations, while if it is too small we need to invoke another round with a new
estimated Hessian matrix. However, again in the context of SPPA, the proximal term naturally
provides a good initialization wt. Our experience show that prescribing 10 iterations of L-BFGS
updates is more than enough to obtain accurate solutions.
On the other hand, it perhaps makes more sense to simply use some more advanced first-order methods
such as Nesterov’s accelerated gradient descent (1983) to calculate the proximal update. Both L-BFGS and
accelerated gradient descent evaluates the gradient of the loss function with O(d)complexity, and the question
is how to leverage convergence rate versus sophistication.
B Miscellaneous proofs
B.1 Any diminishing step size rule guarantees expected convergence
It is our observation that a lot of the convergence analysis using diminishing step sizes requires that the step
sizes are square summable. Take equation (2)as an example, on the right-hand-side, the second term in the
numerator is typically assumed to be finite, while the denominator goes to infinity, therefore the entire right
hand side goes to zero. This requirement would rule out step size rules such as λt=β/√
tsince it is not
square summable. The purpose of this subsection is to show that the common square summable requirement
is not in fact necessary as the quotient still goes to zero. We are not claiming to be the first to notice this, so
it is only included for completeness.
Proposition B.1. For a nonnegative sequence λ1,λ2,..., that satisfies λt→0and/summationtext∞
t=1λt=∞, we have
that/summationtext∞
t=1λ2
t/summationtext∞
t=1λt= 0.
19Published in Transactions on Machine Learning Research (11/2022)
Proof.Letϵ>0, sinceλt→0, there exists an integer T1such thatλt<ϵfor allt>T 1. There also exists an
integerT2such that
T2/summationdisplay
t=1λt≥1
ϵT1/summationdisplay
t=1λ2
t,
since the right-hand-side is a fixed number and/summationtext∞
t=1λt=∞. Then for any T > max(T1,T2), we have
/summationtextT
t=1λ2
t/summationtextT
t=1λt=/summationtextT1
t=1λ2
t/summationtextT2
t=1λt+/summationtextT
t=T2+1λt+/summationtextT
t=T1+1λ2
t/summationtextT1
t=1λt+/summationtextT
t=T1+1λt
≤/summationtextT1
t=1λ2
t/summationtextT2
t=1λt+/summationtextT
t=T1+1λ2
t/summationtextT
t=T1+1λt≤ϵ+ϵ/summationtextT
t=T1+1λt/summationtextT
t=T1+1λt≤2ϵ.
Since the inequality holds for any ϵ>0, we can let ϵbe arbitrarily close to zero, and there will be Tlarge
enough to make the inequality hold. This proves the proposition.
B.2 Convergence in probability
In this convergence analysis, we will use the following well-known theorem (Bertsekas, 2011).
Theorem B.2 (Supermartingale Convergence Theorem) .LetXt,Yt, andZt,t= 0,1,...,be three sequences
of random variables and let Ft,t= 0,1,...,be sets of random variables such that Ft⊂Ft+1for allt. Suppose
that:
1.The random variables Xt,Yt, andZtare nonnegative, and are functions of the random variables in
Ft.
2. For each t, we have
E[Xt+1|Ft]≤Xt−Yt+Zt.
3. There holds, with probability 1,/summationtext∞
t=0Zt≤∞.
Then we have/summationtext∞
t=0Yt≤∞, and the sequence Xtconverges to a nonnegative random variable X, with
probability 1.
Corollary B.3 (Convergence in probability with constant step sizes) .Suppose all f1,...,fnandgare
convex, and Assumption 2.1 holds. If a solution w⋆exists, then with initialization ˜w0, with probability 1, the
sequence w1,...,wTgenerated by SDRS (Algorithm 1) or mini-batch SDRS (Algorithm 2) with a constant
step sizeλt=λsatisfies
inf
t/bracketleftigg
1
nn/summationdisplay
i=1fi(wt) +g(wt)/bracketrightigg
<1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆) +λL2
2.
Proof.Rewriting (8) as
Eit∥˜wt+1−w⋆∥2≤∥˜wt−w⋆∥2−2λ/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
+λ2L2.
LetXt=∥˜wt−w⋆∥2,Zt= 0, and
Yt= 2λ/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
−λ2L2,
as well as filtration Ft={wt+1,...,w0,˜wt,..., ˜w0}, which would imply Ft⊂Ft+1, then according to the
Supermartingale Convergence Theorem B.2,
∞/summationdisplay
t=12λ/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)−λ2L2/parenrightigg
<∞,
20Published in Transactions on Machine Learning Research (11/2022)
with probability 1. Divide both sides by 2λgives,
∞/summationdisplay
t=1/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)−λL2
2/parenrightigg
<∞.
Since this infinite sum is finite, we must have
inf
t/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)−λL2
2/parenrightigg
≤0.
This is because if the infimum is strictly positive, say ϵ, then the infinite sum would be bigger than ϵtimes
infinite, which contradicts the summable conclusion from the supermartingale convergence theorem.
Corollary B.4 (Convergence in probability with diminishing step sizes) .Suppose all f1,...,fnandgare
convex, and Assumption 2.1 holds. If a solution w⋆exists, then with initialization ˜w0, with probability 1, the
sequence w1,...,wTgenerated by SDRS (Algorithm 1) or mini-batch SDRS (Algorithm 2) with diminishing
step sizes such that λt→0,/summationtext∞
t=1λt=∞, and/summationtext∞
t=1λ2
t<∞satisfies
inf
t/bracketleftigg
1
nn/summationdisplay
i=1fi(wt) +g(wt)/bracketrightigg
→1
nn/summationdisplay
i=1fi(w⋆) +g(w⋆).
Proof.Following the first few steps in the proof of Corollary B.3, but replacing λwithλt, we have
Eit∥˜wt+1−w⋆∥2≤∥˜wt−w⋆∥2−2λt/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
+λ2
tL2.
This time we let Xt=∥˜wt−w⋆∥2,Zt=λ2
tL2, and
Yt= 2λt/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
,
as well as filtration Ft={wt+1,...,w0,˜wt,..., ˜w0}. Since we assume λtis square summable, we can invoke
the Supermartingale Convergence Theorem B.2 to conclude that
∞/summationdisplay
t=12λt/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
<∞
with probability 1. This means
inf
t/parenleftigg
1
nn/summationdisplay
i=1fi(wt+1) +g(wt+1)−1
nn/summationdisplay
i=1fi(w⋆)−g(w⋆)/parenrightigg
= 0;
otherwise, if the infimum is ϵ>0, then the infinite sum is bigger than ϵ/summationtext∞
t=1λt=∞, since we assume λtis
not summable, which contradicts the conclusion from the supermartingale convergence theorem.
C Additional Experiments
C.1 Classification on IMDB
IMDB is a large movie review data set used to analyze binary sentiment (Maas et al., 2011). We have 25,000
movie reviews for training and 25,000 movie reviews for testing. To conduct our experiment, we used a bag
of words format. Positive to negative sentiment ratio is 50/50.
21Published in Transactions on Machine Learning Research (11/2022)
Figure 11: SVM with L1regularization on IMDB:
loss per seconds
Figure 12: SVM with L1regularization on IMDB:
classification accuracy per seconds
Figure 13: Logistic regression with L1regularization
on IMDB: loss per seconds
Figure 14: Logistic regression with L1regularization
on IMDB: classification accuracy per seconds
We use SVM and logistic regression for IMDB classification, as we did for Bank Note Authentication. SDRS
outperforms the other algorithms when we use logistic regression to classify. Surprisingly, SGD Momentum
diverges with this data set in the logistic regression problem. The experiment shows that SGD Momentum is
less stable than SDRS, despite performing similarly in most cases. All algorithms are tested using different
parameters, and the results presented here are the best results for each algorithm. The logistic loss is shown
in Figure 13, and the accuracy per second is shown in Figure 14. Figure 13 shows that logistic loss decreases
with SDRS, whereas it does not decrease quite as much with other algorithms. Furthermore, SDRS achieves a
higher test accuracy than other algorithms in Figure 14. The performance of SDRS is similar but not notably
better than the state-of-the-art methods when we use SVM.
C.2 Regression
We perform linear regression with L1regularization and show the results of different algorithms including
our efficient implementation on both Bike Sharing and Hotel Average Daily Rates (ADR) datasets (Antonio
et al., 2019).
Bike Sharing. Bike Sharing is a publicly available data set containing hourly or daily count of rental
bikes as well as environmental and seasonal settings. The data set we are using is from UC Irvine Machine
Learning Repository.1The core data set consists of two years of historical logs for years 2011 and 2012
from the Capital Bike share system, Washington, DC, USA (Fanaee-T & Gama, 2013). There are 17379
samples in this dataset (12512 training samples, 3476 test samples, 1391 validation samples) with 16 features
to predict the hourly rental bike count.
1https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
22Published in Transactions on Machine Learning Research (11/2022)
Figure 15: Linear regression with L1regularization
onBikeSharing: meansquarederrorlossperseconds.
Figure 16: Linear regression with L1regularization
on Hotel ADR: mean squared error loss per seconds.
Figure 17: Linear regression on Bike Sharing: loss
per second, batch size 4
Figure 18: Linear regression on Hotel ADR data set:
loss per second, batch size 10
On the bike sharing data set, we perform linear regression with L1regularization and compare the performance
of different optimization algorithms. Figure 15 illustrates the decrease in the loss values for different algorithms
over time. The SDRS algorithm outperforms the other algorithms by reaching a smaller value in a shorter
amount of time. We tried different settings for the algorithms, and the results presented in Figure 15 show
the SDRS learning rate to be 1/√
t, wheretis the number of iterations. Similarly, SDRS also offers the
benefit of being stable, as demonstrated in Figure 15. Since the dimension is very low for this data set, SDRS
is not significantly faster than SGD or SGD momemtum. With mini-batch, as shown in Figure 17, we can see
that SDRS takes fewer number of iterations to reach a smaller loss value compared to other methods.
Hotel Average Daily Rates (ADR). Hotel ADR is a publicly available data set consisting of hotel
demand data for two different types of hotels; resort hotels and city hotels. Each hotel data set consists
of 40,060 samples with 31 features each to predict average daily rate values (Antonio et al., 2019). In our
experiments, we used resort hotels with 40,060 samples (30045 training, 10015 test) with 8 features.
We perform linear regression with L1regularization on the Hotel ADR dataset and compare the performances
of different optimization algorithms. Figure 16 shows how loss values have decreased over time for multiple
algorithms. It takes SDRS less than a second to converge to a very small loss value. We tried different
settings for the algorithms, and the results are represented in Figure 16 where the learning rate for SDRS is
1/t, wheretis the number of iterations. With mini-batch, as in Figure 18, we can see the decreasing loss
value over iterations. SDRS outperforms all the methods and provides an almost negligible error bar. The
performance can be further showcased using parallelization over different batches. Nevertheless, the current
results are good enough to celebrate.
23Published in Transactions on Machine Learning Research (11/2022)
Figure 19: ResNet on CIFAR10: cross entropy loss,
batch size 32
Figure 20: ResNet on CIFAR10: prediction accuracy,
batch size 32
Figure 21: CNN on MNIST: cross entropy loss, batch
size 64
Figure 22: CNN on MNIST: prediction accuracy,
batch size 64
C.3 Additional neural network training
To show that mini-batch SPPA works not only on the CNN architecture, we also tried training a residual
neural network (ResNet) on CIFAR10. For the details about the network, one can refer to the 20 layer ResNet
architecture on CIFAR10 (He et al., 2016). In our settings, the batch size is 32. The results are shown in
Figures 19 and 20. As we can see, the performance is indeed consistent as in the CNN case.
Finally, we show the training performance on the MNIST handwritten digits data set (LeCun et al., 2010). It
consists of 60,000 training, 10,000 test images of size 28×28in 10 classes. The network we used for this
experiment is based on a convolutional deep neural network. It consists of 5 layers, each of the first 3 is a
combination of 3×3convolutional filters and 2×2max pooling with a stride of 2 and the last two fully
connected. The loss function is cross-entropy loss, and the activation function is RELU. The batch size is 64.
To show the behavior of the algorithms more in detail, we recorded the loss values at every 25 stochastic
update, the experiment is run for one epoch, and in total, 38 updates are presented ( 25×38×64giving the
number of training samples). The accuracy is calculated on the test data at every 500 stochastic update,
using the accuracy calculation in PyTorch tutorial.2As observed in Figures 21 and 22, SPPA-based methods
outperform all SGD-based algorithms in terms of both the cross-entropy loss and prediction accuracy on the
test set.
2https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py
24