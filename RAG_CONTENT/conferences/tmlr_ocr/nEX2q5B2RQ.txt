Published in Transactions on Machine Learning Research (05/2023)
Analyzing Deep PAC-Bayesian Learning with Neural Tan-
gent Kernel: Convergence, Analytic Generalization Bound,
and Efficient Hyperparameter Selection
Wei Huang∗wei.huang.vr@riken.jp
RIKEN Center for Advanced Intelligence Project (AIP)
Chunrui Liu∗chunrui.liu@student.uts.edu.au
University of Technology Sydney
Yilan Chen yilan@ucsd.edu
University of California San Diego
Richard Yi Da Xu xuyida@hkbu.edu.hk
Hong Kong Baptist University
Miao Zhang miaozhang1991@gmail.com
Harbin Institute of Technology (Shenzhen)
Tsui-Wei Weng lweng@ucsd.edu
University of California San Diego
Reviewed on OpenReview: https: // openreview. net/ forum? id= nEX2q5B2RQ
Abstract
PAC-Bayes is a well-established framework for analyzing generalization performance in ma-
chine learning models. This framework provides a bound on the expected population error
by considering the sum of training error and the divergence between posterior and prior
distributions. In addition to being a successful generalization bound analysis tool, the
PAC-Bayesian bound can also be incorporated into an objective function for training prob-
abilistic neural networks, which we refer to simply as Deep PAC-Bayesian Learning . Deep
PAC-Bayesian learning has been shown to achieve competitive expected test set error and
provide a tight generalization bound in practice at the same time through gradient descent
training. Despite its empirical success, theoretical analysis of deep PAC-Bayesian learning
for neural networks is rarely explored. To this end, this paper proposes a theoretical con-
vergence and generalization analysis for Deep PAC-Bayesian learning. For a deep and wide
probabilistic neural network, our analysis shows that PAC-Bayesian learning corresponds
to solving a kernel ridge regression when the probabilistic neural tangent kernel (PNTK) is
used as the kernel. We utilize this outcome in conjunction with the PAC-Bayes C-bound,
enabling us to derive an analytical and guaranteed PAC-Bayesian generalization bound for
the first time. Finally, drawing insight from our theoretical results, we propose a proxy
measure for efficient hyperparameter selection, which is proven to be time-saving on various
benchmarks. Our work not only provides a better understanding of the theoretical under-
pinnings of Deep PAC-Bayesian learning, but also offers practical tools for improving the
training and generalization performance of these models.
∗Equal Contribution.
1Published in Transactions on Machine Learning Research (05/2023)
1 Introduction
Deep learning has demonstrated powerful learning capability due to its over-parameterization structure, in
which various network architectures have been responsible for its significant leap in performance (LeCun
et al., 2015). However, deep neural networks often suffer from over-fitting and complex hyperparameters,
making the design of generalization guarantees a crucial research goal (Zhang et al., 2021). One recent
breakthrough in this area is the development of a learning framework that connects geometry of the loss
landscape with the flatness of minima such as Sharpness-Aware Minimization Foret et al. (2021) and Deep
PAC-Bayesian learning, which trains a probabilistic neural network using an objective function based on
PAC-Bayesian bounds (Bégin et al., 2016; Dziugaite & Roy, 2017; Neyshabur et al., 2018; Raginsky et al.,
2017; Neyshabur et al., 2017; London, 2017; Smith & Le, 2018; Pérez-Ortiz et al., 2021; Guan & Lu, 2022).
Deep PAC-Bayesian learning provides a tight generalization bound while achieving competitive expected
test set error (Ding et al., 2022). Furthermore, this generalization bound can be computed from the training
data, obviating the need to split data into training, testing, and validation sets, which is highly applicable
when working with limited data (Pérez-Ortiz et al., 2021; Grünwald & Mehta, 2020).
ThesuccessofPAC-Bayesianlearninghasledtotheirwidespreadapplicationindifferentdeepneuralnetwork
architectures, including convolutional neural networks (Zhou et al., 2019; Pérez-Ortiz et al., 2021), binary
activated multilayer networks (Letarte et al., 2019), partially aggregated neural networks (Biggs & Guedj,
2021), and graph neural networks (Liao et al., 2021; Ju et al., 2023); different learning frameworks such
as meta-learning (Amit & Meir, 2018; Rothfuss et al., 2021; Flynn et al., 2022), adversarial training and
robustnessWangetal.(2022b), contrastiveunsupervisedlearning(Nozawaetal.,2020). Theseadvancements
have enabled significant improvements in the generalization performance of deep neural networks, making
PAC-Bayesian learning a valuable tool for training robust and reliable models.
The remarkable empirical success of PAC-Bayesian learning has generated growing interest in understanding
its theoretical properties. However, theoretical investigations have been largely limited to specific variants of
the technique, such as Entropy-SGD, which minimizes an objective indirectly by approximating stochastic
gradient ascent on the so-called local entropy (Dziugaite & Roy, 2018a), and differential privacy (Dziugaite
& Roy, 2018b). Other investigations have relied heavily on empirical exploration (Neyshabur et al., 2017).
To the best of our knowledge, there has been no systematic investigation into why PAC-Bayesian learning is
successful and why the PAC-Bayesian bound is tight on unseen data after training. For example, it is still
unclear when applying gradient descent to PAC-Bayesian learning:
Q1:How effective is gradient descent training on a training set?
Q2:How tight is the generalization bound after convergence on the unseen dataset?
These questions are particularly important because understanding the strengths and weaknesses of PAC-
Bayesian learning can help improve its theoretical foundations and practical applications. For instance,
answering the first question can shed light on the training dynamics and optimization challenges of PAC-
Bayesian learning, while answering the second question can provide insights into the generalization perfor-
mance of this learning framework.
Answering the questions posed earlier can be challenging due to the inherent non-convexity of optimization
in neural networks (Jain et al., 2017), the additional randomness introduced by probabilistic neural networks
(Specht, 1990), and the challenges brought by the Kullback-Leibler divergence between posterior/prior dis-
tribution pairs. However, recent advances in deep learning theory with over-parameterized settings offer
a promising approach to tackling these challenges. It has been shown that wide networks optimized with
gradient descent can achieve near-zero training error, and the neural tangent kernel (NTK) plays a critical
role in the training process. The NTK remains unchanged during gradient descent training (Jacot et al.,
2018), providing a guarantee for achieving a global minimum (Du et al., 2019; Allen-Zhu et al., 2019). In
the PAC-Bayesian framework, the NTK is calculated based on the gradient of the distribution parameters
of the weights, rather than the derivative of the weights themselves. We refer to this as the Probabilistic
NTK (PNTK), which we use to develop a convergence analysis for characterizing the optimization process
of PAC-Bayesian learning. The PAC-Bayes bounds employed in Deep PAC-Bayesian learning typically stem
2Published in Transactions on Machine Learning Research (05/2023)
from the PAC-Bayes-kl theorem, as indicated by (Seeger, 2002). These bounds study the expected risk,
often referred to as Gibbs risk. Conversely, an alternative line of PAC-Bayes bounds, known as the C-bound
Lacasse et al. (2006), presents an upper bound for the risk associated with the Majority Vote classifier, the
risk of expected output. Examining the risk of expected output within the PAC-Bayes bound presents the
benefit of providing an explicit solution for optimization analysis in the infinite-width limit. Moreover, we
formulate the generalization bound of PAC-Bayesian learning upon convergence for the first time.
We summarize our contributions as follows:
•We provide a detailed characterization of the gradient descent training process of the PAC-Bayesian
objective function and show that the final solution corresponds to kernel ridge regression with its
kernel being the PNTK.
•Building upon the optimization solution, we derive an analytical and guaranteed PAC-Bayesian
bound for deep networks after training for the first time. In contrast to other PAC-Bayesian bounds
that require the distribution of the posterior, our bound, based on the C-bound under a Gaussian
posterior, where only the mean parameter is being trained, is entirely independent of computing the
distribution of the posterior.
•We design a training-free proxy based on our theoretical bound to select hyperparameters efficiently,
which is effective and time-saving. Our training-free proxy can help alleviate the computational
burden of hyperparameter selection, which is critical for practical applications of PAC-Bayesian
learning.
•Our technique of analyzing optimization and generalization of probabilistic neural networks through
over-parameterization has a wide range of applications, such as the Variational Auto-encoder
(Kingma & Welling, 2013; Rezende et al., 2014) and deep Bayesian networks (MacKay, 1992; Neal,
2012). We believe that our technique can provide the basis for the analysis of over-parameterized
probabilistic neural networks.
2 Related Work
PAC-Bayes Bound The Probably Approximately Correct (PAC) Bayes framework (McAllester, 1999a;b)
is a useful tool for providing a test performance (generalization) guarantee by incorporating knowledge
about the learning algorithm and probability distribution over a set of hypotheses. This framework has
been extended to the analysis of generalization bounds for probabilistic neural networks using the PAC-
Bayesian method (Langford & Caruana, 2002). While the original PAC-Bayes theory only works with
bounded loss functions, Germain et al. (2016) proposed a PAC-Bayes bound for sub-gamma loss family
and Haddouche et al. (2021) expanded the PAC-Bayesian theory to learning problems with unbounded loss
functions. Furthermore, several improved PAC-Bayesian bounds suitable for different scenarios have been
introduced by Bégin et al. (2014; 2016). The flexibility and generalization properties of the PAC-Bayes
framework make it a popular tool for analyzing complex, non-convex, and overparameterized optimization
problems, particularly for over-parameterized neural networks (Guedj, 2019). In the context of feedforward
neural networks with ReLU activations, Neyshabur et al. (2018) presented a generalization bound based on
the product of the spectral norm of the layers and the Frobenius norm of the weights. This bound has been
used as a benchmark for evaluating the generalization performance of various neural network architectures
and training methods.
PAC-Bayesian Learning. Obtaining a numerical bound on generalization is just as important as achiev-
ing a theoretical analysis for the generalization properties of deep learning. One method for computing an
error upper bound is by training a Bayesian neural network and using a refined PAC-Bayesian bound, as
introduced by Langford & Caruana (2002). Building on this work, Neyshabur et al. (2017) developed a
training objective function derived from a relaxed PAC-Bayesian bound to train deep neural networks. In
standard PAC-Bayes, the prior is often chosen to be a spherical Gaussian centered at the origin. However,
this approach might not be effective in cases where the KL divergence is unreasonably large due to a lack of
3Published in Transactions on Machine Learning Research (05/2023)
information about the data. To address this issue, a growing body of literature proposes to obtain localized
PAC-Bayes bounds via distribution-dependent priors informed by the data (Ambroladze et al., 2007; Negrea
et al., 2019; Dziugaite et al., 2021; Perez-Ortiz et al., 2021). Additionally, Dziugaite & Roy (2018b); Tinsi &
Dalalyan (2022) showed how a differentially private data-dependent prior can yield a valid PAC-Bayes bound
in situations where the data distribution is unknown. Recently, researchers have been focused on providing
PAC-Bayesian bounds for more complex and realistic neural network architectures, such as convolutional
neural networks (Zhou et al., 2019), binary activated multilayer networks (Letarte et al., 2019), partially
aggregated neural networks (Biggs & Guedj, 2021), attention-based neural networks (Wang et al., 2022b),
and graph neural networks (Liao et al., 2021; Ju et al., 2023). In this work, our goal is to demystify the
success of deep PAC-Bayesian Learning by exploring the role of the Probabilistic Neural Tangent Kernel
(PNTK) in achieving a tight generalization bound.
3 Preliminary
Notation. In this paper, we use bold-faced letters for vectors and matrices (e.g., xandW), and non-
bold-faced letters for scalars (e.g., nandt). We use∥·∥ 2to denote the Euclidean norm of a vector and
the spectral norm of a matrix, while ∥·∥Fdenotes the Frobenius norm of a matrix. For a neural network,
we useσ(x)to denote the activation function, which applies an element-wise nonlinearity to its input. We
denote the set 1,2,...,nas[n], and we use λ0(A) =λmin(A)to denote the least eigenvalue of matrix A.
Throughout the paper, we will also use other common notation, such as O(·)for asymptotic notation and
E[·]for the expected value of a random variable. We will define any additional notation as needed.
3.1 Deep probabilistic neural network
In PAC-Bayesian learning, we use probabilistic neural networks (PNNs) instead of deterministic networks.
In PNNs, the weights follow a certain distribution, and in this work, we use the Gaussian distribution. We
define anL-layer PNN fgoverned by the following recursive expression:
x(l)=1√mσ/parenleftbig
W(l)x(l−1)/parenrightbig
,1≤l≤L−1;f=1√mW(L)x(L−1), (1)
wheremiswidthofhiddenlayers, x(0)=x∈Rdistheinputwith dbeingtheinputdimension, W(1)∈Rm×d
is the weight matrix at the first layer, W(l)∈Rm×mis the weight at the l-th layer for 2≤l≤L−1, and
W(L)∈Rm×1is the weight at the output layer.
We employ the reparameterization trick (Kingma & Welling, 2013; Kingma et al., 2015) to model the weights
as Gaussian during gradient descent training, with trainable mean and variance parameters:
W(l)=W(l)
µ+W(l)
σ⊙ξ(l),ξ(l)∼N(0,I),1≤l≤L, (2)
where⊙denotes the element-wide product operation. In this expression, for all 1≤l≤L,ξ(l)share the
same size as their corresponding weight matrix or vector. The re-parameterization trick involves sampling
ξ(l)for1≤l≤Lfrom a normal distribution N(0,I).
We randomly initialize the mean weights as follows: W(l)
µ∼N(0,c2
µ·I)forl∈[1,L]. Here, we omit the size
of the mean 0and variance I, which is determined by the corresponding weight matrix. For the variance
weights, we use an absolute constant to initialize them, namely, W(l)
σ=c2
σ·1, where 1is a matrix with all
elements equal to 1. Our assumption of an isotropic initialization for the covariance matrix is based on the
empirical setting presented in Rivasplata et al. (2019).
3.2 PAC-Bayes bound
Suppose we have a set of ni.i.d. samplesS={(xi,yi)}n
i=1drawn from a non-degenerate distribution D. Let
Hdenote the hypothesis space and h(x)the prediction of hypothesis h∈Hfor input x. The generalization
error of classifier hwith respect toDis denoted by RD(h) =E(x,y)∼D[ℓ(y,h(x))], whereℓ(·)is a given loss
function. The empirical error of classifier hwith respect toSis denoted by RS(h) =1
n/summationtextn
i=1ℓ(yi,h(xi)).
4Published in Transactions on Machine Learning Research (05/2023)
In PAC-Bayes, the prior distribution Q(0)∈ Hrepresents the distribution over the hypothesis space
Hbefore training or at initialization, and the posterior distribution Q∈ Hrepresents the distribu-
tion of parameters after training. The expected population risk and empirical error are defined as
RD(Q) =E(x,y)∼D,h∼Q[ℓ(y,h(x))] =Eh∼Q[RD(h)]andRS(Q) =Eh∼Q[RS(h)], respectively.
The PAC-Bayes theory (Seeger, 2002; Maurer, 2004) provides the following theorem:
Theorem 3.1. For anyδ∈(0,1], any loss function ℓ:R×R→[0,1], the following inequality holds
uniformly for all posteriors distributions Q∈Hwith a probability of at least 1−δ,
kl/parenleftbig
RS(Q)∥RD(Q)/parenrightbig
≤KL(Q∥Q(0)) + log2√n
δ
n, (3)
where KL(Q∥Q(0)) = EQ/bracketleftig
lnQ
Q(0)/bracketrightig
is the Kullback-Leibler (KL) divergence and kl(q∥q′) =qlog(q
q′) + (1−
q) log(1−q
1−q′)is the binary KL divergence.
Furthermore, by combining Pinsker’s inequality for binary KL divergence, kl(ˆp∥p)≥(p−ˆp)2/(2p), where
ˆp<p, we can obtain the following bound:
RD(Q)−RS(Q)≤/radicaligg
2RD(Q)KL(Q∥Q(0)) + log2√n
δ
n. (4)
Equation (4) is a classical result. This result can be further combined with the inequality√
ab≤1
2(¯λa+b
¯λ),
for all ¯λ>0, which leads to a PAC-Bayes- λbound in Theorem 3.2, as proposed by Thiemann et al. (2017):
Theorem 3.2. LetQ(0)∈Hbe some prior distribution over H.Then for any δ∈(0,1], the following
inequality holds uniformly for all posteriors distributions Q∈Hwith a probability of at least 1−δ
RD(Q)≤RS(Q)
1−¯λ/2+KL(Q∥Q(0)) + log2√n
δ
n¯λ(1−¯λ/2). (5)
Another line of PAC-Bayes C-bound research (Laviolette & Marchand, 2007; Laviolette et al., 2011; Germain
et al., 2015) focuses on the upper bound of the Q-weighted majority vote classifier BQ= sng( Eh∼Q[h(xi)]),
also known as the Bayes classifier. In this context, we define the Q-margin realized on an example (x,y),
which is an important notion related to majority votes: MQ(x,y) =y·Eh∼Qh(x). We also consider the
first momentMD
Q=E(x,y)∼DMQ(x,y)and the second moment MD
Q2=E(x,y)∼DE(h,h′)∼Q2h(x)h′(x)of the
Q-margin. The generalization bound of RD(BQ)is given as follows:
Theorem 3.3 (PAC BayesC-Bound (Laviolette et al., 2011)) .LetQ∈Hbe any distribution over H.Then
for anyδ∈(0,1], for anyn≥8, for any auto-complemented family HofB-bounded real value functions,
the following inequality holds with a probability of at least 1−δ:
RD(BQ)≤1−(MD
Q)2
MD
Q2≤1−(MS
Q−2B√
ln(2n
δ)√
2n)2
MS
Q2+ 2B2√
ln(2n
δ)√
2n, (6)
whereMS
Q=1
n/summationtextn
i=1MQ(xi,yi)andMS
Q2=1
n/summationtextn
i=1E(h,h′)∼Q2h(xi)h′(xi).
TheC-bound is an upper bound of the risk of Majority vote classifier, that depends on the first two moments
of the margin of the Q-convex combination realized on the data. While the PAC-Bayes bounds (Theorems
3.1 and 3.2) deal with classifiers that use a fixed subset of the training examples. In contrast, the C-bound
that applies to a stochastic average (and a majority vote) of classifiers using different subsets (of different
sizes) of the training examples. In particular, the C-bound is proposed for the restricted case where the
voters are all classifiers, which is consistent with the kernel ridge regression derived in this work.
5Published in Transactions on Machine Learning Research (05/2023)
3.3 Deep PAC-Bayesian learning
In this work, we aim to use PAC-Bayes theory to design the training objective for PNNs, inspired by previous
work such as Catoni (2007); Rivasplata et al. (2019). Specifically, we choose Equation (5) as the training
objective, which is a PAC-Bayes bound that guarantees generalization error. It is worth noting that the
original interest of Theorem 3.2 in (Thiemann et al., 2017) was to allow the optimization of a quasiconvex
objective with respect to both ¯λandQ. However, since our main goal is to study the optimization and
generalization properties of PNNs, we set ¯λ= 1and omit the factor of two. Moreover, drawing inspiration
from the Bayes classifier, which takes the expectation over Qinside the loss function, we define the objective
function as follows:
L(Q) =RS(Q) +λKL(θ(t)∥θ(0))
n=1
nn/summationdisplay
i=1ℓ/parenleftig
yi,ˆf(xi)/parenrightig
+λKL(θ(t)∥θ(0))
n, (7)
where ˆf(t)≜Ef∼Q[f(x;t)]is an expected output function, λis a hyperparameter introduced in a heuristic
manner to make the method more flexible, and θis the collection of all weights. Contrasting the general-
ization bound presented by Dziugaite & Roy (2017), which computes the average over the training set using
random samples, our approach utilizes the empirical loss of the expected output function inspired by the
PAC-BayesC-bound. In particular, the empirical loss RS(Q)≜1
n/summationtextn
i=1ℓ/parenleftig
yi,ˆf(xi)/parenrightig
used in Equation (7) is
a lower bound of the expected empirical loss (also known as Gibbs empirical risk) RS(Q), as per Jensen’s in-
equality. This approach offers the advantage of yielding an explicit solution for the expected output function
in the infinite-width limit. To train the PNN, we use the reparameterization trick to update the mean and
variance of the weight matrices and vectors. Specifically, the update rule for W(l)
µandW(l)
σwith 1≤l≤L
at iteration tis given by:
W(l)
µ(t+ 1) = W(l)
µ(t)−η∂L(Q)
∂W(l)
µ(t);W(l)
σ(t+ 1) = W(l)
σ(t)−η∂L(Q)
∂W(l)
σ(t), (8)
whereηis the learning rate. In our theoretical analysis, we consider the gradient flow instead of gradient
descent, but the same results can be extended to the gradient descent case with a careful analysis.
4 Main Theoretical Results
In this section, Theorem 4.2 gives a precise characterization of how the objective function without KL
divergence decreases to zero. We then extend the convergence characterization to the full objective, and find
the final solution is a kernel ridge regression, as demonstrated by Theorem 4.3. As a consequence, we are
able to establish an analytic generalization bound through Theorem 4.4.
4.1 Optimization analysis
We begin by simplifying the analysis to focus on optimizing probabilistic neural networks of the form (1)
with the objective function RS(Q), neglecting the KL divergence term for now. We show that the results
obtained for this simplified setting can be extended to the target function with KL divergence in the next
section. In particular, the objective function can be expressed as follows:
RS(Q;t) =1
2n∥y−Ef∼Qf(X;t)∥2
2=1
2n/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;t)/vextenddouble/vextenddouble/vextenddouble2
2, (9)
where X={xi}n
i=1∈Rn×drepresents inputs and the corresponding label is y={yi}n
i=1∈Rn. Given this
premise, we start by establishing that, for a L-layer probabilistic neural network of the form (1), the gradient
flow of expected output function admits the following dynamics:
dˆf(X;t)
dt=∂ˆf(X;t)
∂θµ∂θµ
∂t+∂ˆf(X;t)
∂θσ∂θσ
∂t=1
n/parenleftig
y−ˆf(X;t)/parenrightig
(Θµ(X,X;t) +Θσ(X,X;t)),(10)
6Published in Transactions on Machine Learning Research (05/2023)
where θµ≜({W(l)
µ}L
l=1), and θσ≜({W(l)
σ}L
l=1)arecollectionofmeanweightsandvarianceweights. Without
loss of generality, we set the learning rate η= 1. Furthermore, Θµ(X,X;t)∈Rn×nandΘσ(X,X;t)∈Rn×n
areprobabilistic neural tangent kernels (PNTKs), which are defined as follows:
Definition 4.1 (Probabilistic Neural Tangent Kernel) .The tangent kernels associated with the expected
output function ˆf(X;t)at parameters θµandθσare defined as,
Θµ(X,X;t) =∂ˆf(X;t)
∂θµ/parenleftbigg∂ˆf(X;t)
∂θµ/parenrightbigg⊤
=L/summationdisplay
l=1∇W(l)
µˆf(X;t)∇W(l)
µˆf(X;t)⊤,
Θσ(X,X;t) =∂ˆf(X;t)
∂θσ/parenleftbigg∂ˆf(X;t)
∂θσ/parenrightbigg⊤
=L/summationdisplay
l=1∇W(l)
σˆf(X;t)∇W(l)
σˆf(X;t)⊤.(11)
A few remarks on Definition 4.1 are in order. Unlike standard (deterministic) neural networks, a probabilistic
network has two sets of parameters, namely, θµandθσ, which lead to two corresponding tangent kernels.
We introduce the expected output function ˆf(t)in the definition to correspond to the expected empirical
lossRS(Q)(9), which helps us to address the additional randomization problem caused by PNNs. Explicitly
the analytic forms of PNKT are given as follows:
Θ(l)
µ,ij(0) =m/summationdisplay
r=1Eξ/bracketleftigg
1√m∂ˆf(xi)
∂x(l)
i,rσ′((w(l)
r)⊤x(l−1)
i)/bracketrightigg
Eξ/bracketleftigg
1√m∂ˆf(xj)
∂x(l)
j,rσ′((w(l)
r)⊤x(l−1)
j)/bracketrightigg
, (12)
Θ(l)
σ,ij(0) =m/summationdisplay
r=1Eξ/bracketleftigg
1√m∂ˆf(xi)
∂x(l)
i,rσ′((w(l)
r)⊤ˆx(l−1)
i)⊙ξ(l)
r/bracketrightigg
Eξ/bracketleftigg
1√m∂ˆf(xi)
∂x(l)
j,rσ′((w(l)
r)⊤x(l−1)
j)⊙ξ(l)
r/bracketrightigg
.(13)
One of the key contributions of this work is the observation that the PNTKs Θµ(X,X)andΘσ(X,X)
both converge to limiting deterministic kernels, at initialization and during training when mis sufficiently
large. Specifically, we have limm→∞Θµ(X,X) =Θ∞
µ(X,X)andlimm→∞Θσ(X,X) =Θ∞
σ(X,X). This
result is important because it implies that the training dynamics of PNNs can be analyzed in terms of the
corresponding deterministic kernels.
Thanks to the convergence of the PNTKs to deterministic kernels as the network width goes to infinity, the
gradient flow dynamics of the output function become linear in the infinite-width limit. Specifically, the
dynamics take the form:
dˆf(X;t)
dt=1
n/parenleftig
y−ˆf(X;t)/parenrightig/parenleftbig
Θ∞
µ(X,X) +Θ∞
σ(X,X)/parenrightbig
. (14)
This key finding allows us to establish our main convergence theory for deep probabilistic neural networks,
which we state formally below.
Theorem 4.2 (Convergence of probabilistic networks with large width) .Supposeσ(·)isH-Lipschitz, β-
Smooth. Assume σ(·)and its partial derivative∂σ(x)
∂xare continuous in x,λ0(K(L)
∞)>0, and the network’s
width is ofm= Ω/parenleftbigg
2O(L)max/braceleftbigg
n2log(Ln/δ )
λ2
0(K(L)
∞),n
δ,n4
λ4
0(K(L)
∞)/bracerightbigg/parenrightbigg
. Then, with a probability of at least 1−δover the
random initialization, we have,
RS(Q(t))≤exp/parenleftbig
−λ0(K(L)
∞)t/parenrightbig
RS(Q(0)), (15)
where we define NNGP kernel and its derivative as K(l)
ij≜(ˆx(l)
i)⊤ˆx(l)
jwith ˆx(l)=Eξ(l)1√mσ/parenleftbig
W(l)ˆx(l−1)/parenrightbig
,
and in the infinite-width limit K(l)
ij,∞≜limm→∞K(l)
ijfor1≤l≤L.
The proof sketch of Theorem 4.2 can be found in Section 5.1 and all the detailed proof are shown in Appendix
A. Our main convergence theory for deep probabilistic neural networks is based on the insight that in the
infinite-width limit, the dynamics of the output function with gradient flow is linear. Specifically, if mis large
7Published in Transactions on Machine Learning Research (05/2023)
enough, our theorem establishes that the expected training error converges to zero at a linear rate, with the
least eigenvalue of the NNGP kernel governing the convergence rate. Furthermore, we find that the change
of weight is bounded during training, as shown in Lemma A.6, which is consistent with the requirement of
the PAC-Bayes theory that the loss function is bounded.
4.2 Training with KL divergence
We expand the KL-divergence in the objective function (7) with θ(t)∼N(θµ(t),θσ(t)):
KL(θ(t)∥θ(0)) =1
2nP/summationdisplay
i=1/parenleftbigg
logθσ,i(0)
θσ,i(t)+(θµ,i(t)−θµ,i(0))2
θσ,i(0)2+θσ,i(t)
θσ,i(0)−1/parenrightbigg
, (16)
wherePis the number of parameters. Furthermore, we assume that the variance weights are fixed during
training, which is empirically verified in Appendix C.3. With this assumption, the objective function (7)
reduces to:
L(Q) =1
2n/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;t)/vextenddouble/vextenddouble/vextenddouble2
2+λ
2nc2σ∥θµ(t)−θµ(0)∥2
2, (17)
where θµ(t) = ({W(l)
µ(t)}L
l=1,vµ(t))is the collection of mean weights at time t, andcσis a constant that
controls the scale of variance weights. By analyzing the objective function (17), we arrive at the conclusion
that in the infinite-width limit, a probabilistic neural network performs kernel ridge regression, as stated in
the following theorem:
Theorem 4.3. Supposem≥poly(n,1/λ0,1/δ,1/E)and the objective function follows the form (17), for
any test input xte∈Rdwith probability at least 1−δover the random initialization, we have
ˆf/parenleftbig
xte;t/parenrightbig
|t=∞=Θ∞
µ(x,X)/parenleftbig
Θ∞
µ(X,X) +λ/c2
σI/parenrightbig−1y±E, (18)
whereEis a small error term between output function of finite-width PNN and infinite-width PNN.
The detailed proof is provided in Section 5.2 and Appendix B. Theorem 4.3 sheds light on the regularization
effect of the KL term in PAC-Bayesian learning and provides an explicit expression for the convergence of
the output function.
4.3 Generalization analysis
We adopt a general and suitable loss function ℓ∈[0,1]to evaluate the PNN’s generalization, while using
squared loss for training. Theorem 3.2 provides the PAC-Bayesian bound concerning the distribution at
initialization and after optimization. By combining this result with Theorem 4.3, we can provide a general-
ization bound for PAC-Bayesian learning under ultra-wide conditions.
Theorem 4.4 (PAC-BayeC-bound with NTK) .Suppose thatS={(xi,yi)}n
i=1are i.i.d. samples from a
non-degenerate distribution D, andm≥poly(n,λ−1
0,δ−1). Then with a probability of at least 1−δover the
random initialization and the training samples, the probabilistic neural network (PNN) trained by gradient
descent for T≥Ω(1
ηλ0logn
δ)iterations has RD(BQ)that is bounded as follows:
RD(BQ)≤1−/parenleftbigg
1
n/summationtextn
i=1yiˆf∞(xi)−2B√
ln(2n
δ)√
2n/parenrightbigg2
1
n/summationtextn
i=1/summationtextn
j=1ˆf∞(xi)ˆf∞(xj) + 2B2√
ln(2n
δ)√
2n.(19)
The proof can be found in Section B.1. Theorem 4.4 provides a generalization bound for the PAC-Bayesian
learning framework under the ultra-wide condition, combining results from Theorem 3.3 and Theorem 4.3.
Note that the generalization ability of kernel ridge regression has been studied in standard neural networks
Hu et al. (2020); Nitanda & Suzuki (2021); Nonnenmacher et al. (2021). In this work, the bound is analytic
and computable, in contrast to the PAC-Bayes bound (5), thus providing a theoretical guarantee for the
PAC-Bayesian learning approach.
8Published in Transactions on Machine Learning Research (05/2023)
5 Proof Sketch
5.1 Proof of Theorem 4.2
To prove Theorem 4.2, we observe that if ΘµandΘσconverge to a deterministic kernel, then the dynamics
of output function admit a linear system, which is tractable during evolution. In this paper we focus on
Θ(L)=Θ(L)
µ+Θ(L)
σ, the gram matrix induced by the weights from L-th layer for simplicity at the cost of
a minor degradation in convergence rate. Before demonstrating the main steps, we introduce the expected
neural network model, which is recursively expressed as follows:
ˆx(l)=Eξ(l)1√mσ/parenleftbig
W(l)ˆx(l−1)/parenrightbig
,1≤l≤L−1; ˆf=Eξ(L)1√mW(L)ˆx(L−1).
Note that this definition is in contrast to the definition as expected empirical loss (9) in PAC-Bayes. The
Neural Network Gaussian Process (NNGP) for PNN (Lee et al., 2018) is defined as follows:
K(l)
ij= (ˆx(l)
i)⊤ˆx(l)
j, K(l)
ij,∞= lim
m→∞K(l)
ij,
where subscript i,jdenote the index of input samples. We aim to show that Θ(L)is close to K(L)
∞in large
width. In particular, to prove Theorem 4.2, three core steps are:
Step 1 Show at initialization λ0/parenleftbig
Θ(L)(0)/parenrightbig
≥λ0(K(L)
∞)
2and the required condition on m.
Step 2 Show during training λ0/parenleftbig
Θ(L)(t)/parenrightbig
≥λ0(K(L)
∞)
2and the required condition on m.
Step 3 Show during training the expected empirical loss RS(Q)has a linear convergence rate.
In our proof, we mainly focus on deriving the condition on mby analyzing λ0/parenleftbig
Θ(L)(0)/parenrightbig
at initialization
through Lemma A.2 and Lemma A.3. For step 2, we construct Lemma A.4 Lemma A.5 and Lemma A.6 to
demonstrate that λ0/parenleftbig
Θ(L)(t)/parenrightbig
≥λ0(K(L)
∞)
2. This leads to the required condition on mduring train. Finally,
we summarize all the previous lemmas and conclude that the expected training error converges at a linear
rate through Lemma A.7:
d
dtRS(t) =1
2d
dt/vextenddouble/vextenddouble/vextenddoubleˆf(X;t)−y/vextenddouble/vextenddouble/vextenddouble2
2=−/parenleftig
y−ˆf(X,t)/parenrightig⊤
(Θµ(t) +Θσ(t))/parenleftig
y−ˆf(X,t)/parenrightig
≤−λ0(Θµ(t) +Θσ(t))/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;t)/vextenddouble/vextenddouble/vextenddouble2
2≤−λ0/parenleftig
Θ(L)
µ(t) +Θ(L)
σ(t)/parenrightig/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;t)/vextenddouble/vextenddouble/vextenddouble2
2
≤−λ0/parenleftig
K(L)
∞/parenrightig/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;t)/vextenddouble/vextenddouble/vextenddouble2
2.
Note our proof framework follows Du et al. (2019)’s, especially, we share the same three core steps. However,
themaindifferenceisthatournetworkarchitectureismuchmorecomplex. Becausetheprobabilisticnetwork
contains two sets of parameters, we have two NTKs. As a result, the proof requires bounding many terms
moreelaborately. Forexample, inLemmaA.3andLemmaA.5weboundthePNTKassociatedwithvariance.
The detailed proof can be found in Appendix A.
5.2 Proof of Theorem 4.3
The proof of Theorem 4.3 utilizes an argument of linearization of the network model in the infinite-width
limit. This allows us to obtain an ordinary differential equation for output function.
According the linearization rules for infinitely-wide networks (Lee et al., 2019), the output function can be
expressed as,
ˆf∞(x,t) =ϕµ(x)⊤(θµ(t)−θµ(0)) + ϕσ(x)⊤(θσ(t)−θσ(0)),
9Published in Transactions on Machine Learning Research (05/2023)
where ϕµ(x) =∇θµˆf(x; 0), and ϕσ(x) =∇θσˆf(x; 0). The next step is to show the difference between
finite-width neural network and infinitely-wide network:
/vextendsingle/vextendsingle/vextendsingleˆf(xte)−ˆf∞(xte)/vextendsingle/vextendsingle/vextendsingle≤O(E),
whereE=Einit+√nEΘ
λ0+βwith/vextenddouble/vextenddouble/vextenddoubleˆf(xte; 0)/vextenddouble/vextenddouble/vextenddouble
2≤Einitand∥Θ∞
µ−Θµ(t)∥2≤EΘ. The details to complete the
proof are given in Appendix B.
6 Experiments
As an extension of our finding of the PAC-Bayesian bound in Theorem 4.4, we present a training-free metric
toapproximatethePAC-BayesianboundusingPNTK,whichcanbeusedtoselectthebesthyper-parameters
without the need for additional training and eliminate excessive computation time. Besides, we provide an
empirical verification of our theory in Appendix C.2 to further demonstrate the correctness of theoretical
analysis.
6.1 Experimental setup
In all experiments, we initialize the PNN parameters using the NTK parameterization as given in Equation
(1). The initial mean weights θµare sampled from a truncated Gaussian distribution with mean 0and
standard deviation 1, truncated at two standard deviations. To ensure positivity, the initial variance weights
θσare set by transforming the specified value of cσthrough the formula cσ= log(1 + exp( ρ0)).
In Section 6.2, we conduct experiments on the MNIST and CIFAR-10 datasets to demonstrate the effective-
ness of our training-free PAC-Bayesian network bound for hyperparameter search under different network
architectures. We consider both fully connected and convolutional neural networks, with a 3-layer fully
connected network with 600 neurons on each layer and a 13-layer convolutional architecture with around 10
million learnable parameters. We adopt a data-dependent prior, which is a practical and popular method
(Dziugaite et al., 2021; Perez-Ortiz et al., 2021; Fortuin, 2022), pre-trained on a subset of the total training
data with empirical risk minimization, and the networks for posterior training are initialized by the weights
learned from the prior. To compute the generalization bound, we use Equation (5), with the relevant settings
from the work by Pérez-Ortiz et al. (2021), such as confidence δfor the risk certificate (5) and 150,000 Monte
Carlo samples to estimate the risk certificate.
6.2 Selecting hyperparameters via training-free metric
The PAC-Bayesian learning framework provides competitive performance with non-vacuous generalization
bounds. However, the tightness of this generalization bounds heavily depends on the hyperparameters used,
such as the proportionality of data used for the prior, the initialization of ρ0, and the KL penalty weight
(λ). While it is worth noting that there is a “penalty” associated with δ(see Equation 5), this penalty is
typically relatively small compared to the potential performance improvements that can be gained through
optimal hyperparameter selection. As these values remain fixed during the training process, selecting optimal
hyperparameters is a critical and computationally challenging task. Grid search is one possible approach,
but it can be prohibitively expensive due to the significant computational resources needed to compute the
generalization bounds for each combination of hyperparameters.
To this end, we propose an approach using “training-free” metric to approximate the generalization bound
without performing a time-consuming training process based on a generalization bound developed in theorem
4.4 via NTK. As PNTK is constant during training, we use an upper bound of Equation 7 with PNTK by
as a proxy metric, which can be formulated as follows:
PA= Tr
(/hatwideΘ+λ/c2
σI)−1·yy⊤
c2σ·n+λ
c2σ/radicaligg
(/hatwideΘ+λ/c2σI)−2·yy⊤
n
, (20)
10Published in Transactions on Machine Learning Research (05/2023)
Figure 1: The first row shows correlation results between the generalization bound and the proportion of
prior data, the coefficient of the KL penalty, and ρ0for the FCN structure on the MNIST dataset. To better
visualize the trend, we have implemented a trend line, which represents the overall trend of a set of data
points, and a 1-to-1 line, which represents a perfect relationship between two variables where the values on
the x-axis are equal to the values on the y-axis. We find high Kendall-tau correlations of 0.89, 0.89, and
0.93. Similar results are obtained in the second row for the CNN structure on the CIFAR10 dataset, where
the Kendall-tau correlations are 0.89, 0.83, and 0.57, respectively.
where/hatwideΘis an empirical NTK measured on a finite-width neural network at initialization, and yy⊤is a
n×nlabel similarity matrix, where a joint entry is one if two data points have the same label, and zero
otherwise. We leave the derivation process in Appendix C.1 and note that the proposed proxy metric
in Equation (20) is similar in spirit to kernel alignment, a label similarity metric, that is widely used in
deep active learning (Wang et al., 2022a), model selection for fine-tuning (Deshpande et al., 2021), and
neural architecture search (NAS) (Mok et al., 2022). We should also mention that training-free methods for
searching neural architectures are not new, and can be found in NAS (Chen et al., 2021; Deshpande et al.,
2021), MAE Random Sampling (Camero et al., 2021), pruning at initialization (Abdelfattah et al., 2021).
Further more both Yang et al. (2021) and Immer et al. (2021) focus on hyperparameter (HP) tuning and
model selection. In contrast to both methods, which involve training, our approach is training-free. We have
included a comparison in the updated manuscript. To the best of our knowledge, this is the first training-free
method for selecting hyperparameters in the PAC-Bayesian framework, which we consider to be one of the
novelties of this paper.
To demonstrate the computational practicality of this training-free metric, we compute PAusing a subset
of the data for each class (325 per class for FCN and 75 per class for CNN). Figure 1 demonstrates a
strong correlation between PAand the actual generalization bound. Importantly, we show that using PAto
search for hyperparameters can yield a result that is comparable to the best generalization bound, but with
significantly reduced computational time. We compare the performance of three hyperparameter search
methods (exhaustive search, Bayesian search, and PA) on two architectures (FCN and CNN) and two
datasets (MNIST and CIFAR10) in Table 1. Exhaustive search evaluates 648 hyperparameter combinations
(9 data-dependent prior with different subsets data for prior training, 9 different values of KL penalty, and
8 different values of ρ0), while Bayesian search takes only 36 iterations to find the lowest bound, but still
requires significant computation time when training a large and complex model. For instance, under the
CIFAR10 dataset, it takes 45 hours to train a CNN with the bound. In contrast, using PAsaves 83.33
11Published in Transactions on Machine Learning Research (05/2023)
Setup Risk cert. Test err. Computation time (hours)
Data Method Network ℓx-eℓ01x-e acc. Single Total
MNISTExhaustive
SearchFCN .0010 .0212 .0001 .0189 .50 324.00
CNN .0006 .0110 .0004 .0093 16.92 10964.16
Bayesian
SearchFCN .0010 .0212 .0001 .0189 .50 18.00
CNN .0006 .0110 .0004 .0093 16.92 609.12
PAFCN .0011 .0264 .0002 .0208 .03 19.44
CNN .0009 .0160 .0008 .0108 .03 19.44
CIFAR10Exhaustive
SearchFCN .1740 .5377 .0051 .4866 1.09 706.32
CNN .0142 .1969 .0023 .1510 45.00 29,160.00
Bayesian
SearchFCN .1740 .5377 .0051 .4866 1.09 39.24
CNN .0142 .1969 .0023 .1510 45.00 1,620.00
PAFCN .1780 .5490 .0048 .4920 .03 19.44
CNN .0142 .1970 .0024 .1511 .03 19.44
Table 1: The performance of our training-free method, PA, is compared with two other hyperparameter
search methods, exhaustive search and Bayesian search, on two datasets (MNIST and CIFAR10) and two
neural network structures (FCN and CNN). The evaluation is based on the risk certificates (cross-entropy
ℓx-eand accuracy ℓ01, test error (cross-entropy x-e and accuracy acc.), and computation time. The best
and second-best values of risk certificates and computation time are highlighted in boldface and underlining,
respectively.
times the computational time to find a bound that is close to the lowest risk certificate in accuracy. The
results show that the PAmethod achieves comparable performance to the other methods while requiring
significantly less computation time.
7 Conclusion and Discussion
In this work, we have made several important contributions to the theoretical analysis and practical imple-
mentation of deep probabilistic neural networks trained using objectives derived from PAC-Bayes bounds.
Specifically, we have shown that the learning dynamics of these networks in an over-parameterized setting
can be exactly described by the PNTK, and we have confirmed this through empirical investigation. We have
also demonstrated that the expected output function trained with a PAC-Bayesian bound converges to the
kernel ridge regression, leading to an explicit generalization bound. Moreover, we have proposed a training-
free method, which can effectively select hyper-parameters and lead to lower generalization bounds without
the excessive computational cost of brute-force grid search. Our work opens up several promising directions
for future research. One such direction is the study of PAC-Bayesian learning with data-dependent priors
using PNTK. We hope that our contributions will help advance the understanding of Deep PAC-Bayesian
Learning and implementation of deep probabilistic neural networks and their potential applications.
Acknowledgments
We express our gratitude to the anonymous reviewers and Action Editors, whose insightful suggestions
significantly enhanced the quality of our paper. Our special thanks go to Tianyu Liu, whose constructive
conversations during the preliminary stages of this project were invaluable.
12Published in Transactions on Machine Learning Research (05/2023)
References
Mohamed S Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, and Nicholas D Lane. Zero-cost proxies for
lightweight NAS. International Conference on Learning Representations , 2021.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pp. 242–252. PMLR, 2019.
Amiran Ambroladze, Emilio Parrado-Hernández, and John Shawe-Taylor. Tighter PAC-Bayes bounds. Ad-
vances in neural information processing systems , 19:9, 2007.
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory. In
International Conference on Machine Learning , pp. 205–214. PMLR, 2018.
LucBégin,PascalGermain,FrançoisLaviolette,andJean-FrancisRoy. PAC-Bayesiantheoryfortransductive
learning. In Artificial Intelligence and Statistics , pp. 105–113. PMLR, 2014.
Luc Bégin, Pascal Germain, François Laviolette, and Jean-Francis Roy. PAC-Bayesian bounds based on the
Rényi divergence. In Artificial Intelligence and Statistics , pp. 435–444. PMLR, 2016.
Felix Biggs and Benjamin Guedj. Differentiable PAC-Bayes objectives with partially aggregated neural
networks. Entropy, 23(10):1280, 2021.
Andrés Camero, Hao Wang, Enrique Alba, and Thomas Bäck. Bayesian neural architecture search using a
training-free performance metric. Applied Soft Computing , 106:107356, 2021.
Olivier Catoni. PAC-Bayesian supervised classification: the thermodynamics of statistical learning. IMS,
Beachwood, OH. MR2483528 5544465 , 2007.
Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four GPU
hours: A theoretically inspired perspective. International Conference on Learning Representations , 2021.
Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li, Luca Zancato, Charless Fowlkes,
Rahul Bhotika, Stefano Soatto, and Pietro Perona. A linearized framework and a new benchmark for
model selection for fine-tuning. arXiv preprint arXiv:2102.00084 , 2021.
Nan Ding, Xi Chen, Tomer Levinboim, Soravit Changpinyo, and Radu Soricut. PACTran: PAC-Bayesian
metrics for estimating the transferability of pretrained models to classification tasks. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIV ,
pp. 252–268. Springer, 2022.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In International Conference on Machine Learning , pp. 1675–1685. PMLR, 2019.
Gintare Karolina Dziugaite and Daniel Roy. Entropy-SGD optimizes the prior of a PAC-Bayes bound:
Generalization properties of Entropy-SGD and data-dependent priors. In International Conference on
Machine Learning , pp. 1377–1386. PMLR, 2018a.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep
(stochastic) neural networks with many more parameters than training data. Proceedings of the Thirty-
Third Conference on Uncertainty in Artificial Intelligence, UAI 2016, August 11–15, 2017, Sydney, NSW,
Australia , 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent PAC-Bayes priors via differential privacy.
Advances in Neural Information Processing Systems , 31, 2018b.
Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy. On the role
of data in PAC-Bayes bounds. In International Conference on Artificial Intelligence and Statistics , pp.
604–612. PMLR, 2021.
13Published in Transactions on Machine Learning Research (05/2023)
Hamish Flynn, David Reeb, Melih Kandemir, and Jan Peters. PAC-Bayesian lifelong learning for multi-
armed bandits. Data Mining and Knowledge Discovery , 36(2):841–876, 2022.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for
efficiently improving generalization. International Conference on Learning Representations , 2021.
Vincent Fortuin. Priors in Bayesian deep learning: A review. International Statistical Review , 2022.
Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario March, and Jean-Francis Roy. Risk bounds
for themajority vote: From aPAC-Bayesiananalysis to a learningalgorithm. Journal of Machine Learning
Research , 16(26):787–860, 2015. URL http://jmlr.org/papers/v16/germain15a.html .
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. PAC-Bayesian theory meets
Bayesian inference. Advances in Neural Information Processing Systems , 29, 2016.
Peter D Grünwald and Nishant A Mehta. Fast rates for general unbounded loss functions: From ERM to
generalized Bayes. J. Mach. Learn. Res. , 21:56–1, 2020.
Jiechao Guan and Zhiwu Lu. Fast-rate PAC-Bayesian generalization bounds for Meta-learning. In Interna-
tional Conference on Machine Learning , pp. 7930–7948. PMLR, 2022.
Benjamin Guedj. A primer on PAC-Bayesian learning. Proceedings of the 2nd congress of the Société
Mathématique de France, 2019, pp. 391–414 , 2019.
Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, and John Shawe-Taylor. PAC-Bayes unleashed:
generalisation bounds with unbounded losses. Entropy, 23(10):1330, 2021.
Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and effective regularization methods for training on noisily
labeled data with generalization guarantee. International Conference on Learning Representations , 2020.
Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, and Khan Mohammad Emtiyaz. Scal-
able marginal likelihood estimation for model selection in deep learning. In International Conference on
Machine Learning , pp. 4563–4573. PMLR, 2021.
ArthurJacot, FranckGabriel, andClémentHongler. NeuralTangentKernel: Convergenceandgeneralization
in neural networks. Advances in Neural Information Processing Systems , 31, 2018.
Prateek Jain, Purushottam Kar, et al. Non-convex optimization for machine learning. Foundations and
Trends ®in Machine Learning , 10(3-4):142–363, 2017.
Haotian Ju, Dongyue Li, Aneesh Sharma, and Hongyang R Zhang. Generalization in graph neural networks:
Improved PAC-Bayesian bounds on graph diffusion. In International Conference on Artificial Intelligence
and Statistics , pp. 6314–6341. PMLR, 2023.
Diederik P Kingma and Max Welling. Auto-encoding Variational Bayes. arXiv preprint arXiv:1312.6114 ,
2013.
Durk P Kingma, Tim Salimans, and Max Welling. Variational Dropout and the local reparameterization
trick.Advances in Neural Information Processing Systems , 28:2575–2583, 2015.
Alexandre Lacasse, François Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier. PAC-Bayes
bounds for the risk of the majority vote and the variance of the Gibbs classifier. Advances in Neural
Information Processing Systems , 19, 2006.
John Langfordand Rich Caruana. (Not) bounding thetrue error. Advances in Neural Information Processing
Systems, 2:809–816, 2002.
François Laviolette and Mario Marchand. PAC-Bayes risk bounds for stochastic averages and majority votes
of sample-compressed classifiers. Journal of Machine Learning Research , 8(7), 2007.
14Published in Transactions on Machine Learning Research (05/2023)
FrançoisLaviolette, MarioMarchand, andJean-FrancisRoy. FromPAC-Bayesboundstoquadraticprograms
for majority votes. In Proceedings of International Conference on Machine Learning , pp. 5–59. Citeseer,
2011.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Deep neural networks as Gaussian processes. International Conference on Learning Represen-
tations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
Advances in Neural Information Processing Systems , 32, 2019.
Gaël Letarte, Pascal Germain, Benjamin Guedj, and François Laviolette. Dichotomize and generalize: PAC-
Bayesian binary activated deep neural networks. Advances in Neural Information Processing Systems , 32,
2019.
Renjie Liao, Raquel Urtasun, and Richard Zemel. A PAC-Bayesian approach to generalization bounds for
graph neural networks. International Conference on Learning Representations , 2021.
Ben London. A PAC-Bayesian analysis of randomized learning with application to stochastic gradient
descent. Advances in Neural Information Processing Systems , 30, 2017.
David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural Computation , 4
(3):448–472, 1992.
Andreas Maurer. A note on the PAC Bayesian theorem. arXiv preprint cs/0411099 , 2004.
David A McAllester. PAC-Bayesian model averaging. In Proceedings of the Twelfth Annual Conference on
Computational Learning Theory , pp. 164–170, 1999a.
David A McAllester. Some PAC-Bayesian theorems. Machine Learning , 37(3):355–363, 1999b.
Jisoo Mok, Byunggook Na, Ji-Hoon Kim, Dongyoon Han, and Sungroh Yoon. Demystifying the neural
tangent kernel from a practical perspective: Can it be trusted for neural architecture search without
training? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
11861–11870, 2022.
Radford M Neal. Bayesian learning for neural networks , volume 118. Springer Science & Business Media,
2012.
JeffreyNegrea, MahdiHaghifam, GintareKarolinaDziugaite, AshishKhisti, andDanielMRoy. Information-
theoretic generalization bounds for SGLD via data-dependent estimates. Advances in Neural Information
Processing Systems , 32, 2019.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in
deep learning. Advances in Neural Information Processing Systems , 30, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to spectrally-
normalized margin bounds for neural networks. International Conference on Learning Representations ,
2018.
Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural
tangent kernel regime. International Conference on Learning Representations , 2021.
Manuel Nonnenmacher, David Reeb, and Ingo Steinwart. Which minimizer does my neural network converge
to? InMachine Learning and Knowledge Discovery in Databases. Research Track: European Conference,
ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part III 21 , pp. 87–102. Springer,
2021.
15Published in Transactions on Machine Learning Research (05/2023)
Kento Nozawa, Pascal Germain, and Benjamin Guedj. PAC-Bayesian contrastive unsupervised representa-
tion learning. In Conference on Uncertainty in Artificial Intelligence , pp. 21–30. PMLR, 2020.
Maria Perez-Ortiz, Omar Rivasplata, Benjamin Guedj, Matthew Gleeson, Jingyu Zhang, John Shawe-Taylor,
Miroslaw Bober, and Josef Kittler. Learning PAC-Bayes priors for probabilistic neural networks. arXiv
preprint arXiv:2109.10304 , 2021.
María Pérez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvári. Tighter risk certificates for
neural networks. The Journal of Machine Learning Research , 22(1):10326–10365, 2021.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via Stochastic Gradient
Langevin Dynamics: a nonasymptotic analysis. In Conference on Learning Theory , pp. 1674–1703. PMLR,
2017.
DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagationandapproximate
inference in deep generative models. In International conference on machine learning , pp. 1278–1286.
PMLR, 2014.
Omar Rivasplata, Vikram M Tankasali, and Csaba Szepesvari. PAC-Bayes with Backprop. arXiv preprint
arXiv:1908.07380 , 2019.
Jonas Rothfuss, Vincent Fortuin, Martin Josifoski, and Andreas Krause. PACOH: Bayes-optimal meta-
learning with PAC-guarantees. In International Conference on Machine Learning , pp. 9116–9126. PMLR,
2021.
Matthias Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification. Journal of
machine learning research , 3(Oct):233–269, 2002.
Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient descent.
International Conference on Learning Representations , 2018.
Donald F Specht. Probabilistic neural networks. Neural Networks , 3(1):109–118, 1990.
Niklas Thiemann, Christian Igel, Olivier Wintenberger, and Yevgeny Seldin. A strongly quasiconvex PAC-
Bayesian bound. In International Conference on Algorithmic Learning Theory , pp. 466–492. PMLR, 2017.
Laura Tinsi and Arnak Dalalyan. Risk bounds for aggregated shallow neural networks using Gaussian priors.
InConference on Learning Theory , pp. 227–253. PMLR, 2022.
Haonan Wang, Wei Huang, Ziwei Wu, Hanghang Tong, Andrew J Margenot, and Jingrui He. Deep active
learning by leveraging training dynamics. Advances in Neural Information Processing Systems , 35:25171–
25184, 2022a.
Zifan Wang, Nan Ding, Tomer Levinboim, Xi Chen, and Radu Soricut. Improving robust generalization by
direct PAC-Bayesian bound minimization. arXiv preprint arXiv:2211.12624 , 2022b.
Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub
Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot
hyperparameter transfer. Advances in Neural Information Processing Systems , 2021.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM , 64(3):107–115, 2021.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Non-vacuous generaliza-
tion bounds at the imagenet scale: a PAC-Bayesian compression approach. International Conference on
Learning Representations , 2019.
16Published in Transactions on Machine Learning Research (05/2023)
A Proof of Theorem 4.2
Theorem A.1 (Restatement of Theorem 4.2) .Supposeσ(·)isH-Lipschitz, β-Smooth. Assume σ(·)
and its partial derivative∂σ(x)
∂xare continuous in x,λ0(K(L)
∞)>0, and the network’s width is of
m= Ω/parenleftbigg
2O(L)max/braceleftbigg
n2log(Ln/δ )
λ2
0(K(L)
∞),n
δ,n5log(2/δ)10
λ2
0(K(L)
∞)/bracerightbigg/parenrightbigg
. Then, with a probability of at least 1−δover the random
initialization, we have,
RS(Q(t))≤exp/parenleftbig
−λ0(K(L)
∞)t/parenrightbig
RS(Q(0)),
Proof Sketch of Theorem A.1. To study the behavior of output function under gradient flow, we first write
down its dynamics
dˆf(X;t)
dt=1
n(y−ˆf(X;t))(Θµ(X,X;t) +Θσ(X,X;t)),
where ΘµandΘσare the PNTKs of the PNN. We observe that if ΘµandΘσconverge to a deterministic
kernel, then the dynamics of output function admit a linear system, which is tractable during evolution. In
this paper we focus on Θ(L)=Θ(L)
µ+Θ(L)
σ, the gram matrix induced by the weights from L-th layer for
simplicity at the cost of a minor degradation in convergence rate. Before demonstrating the main steps, we
introduce the expected neural network model, which is recursively expressed as follows:
ˆx(l)=Eξ(l)1√mσ/parenleftbig
W(l)ˆx(l−1)/parenrightbig
,1≤l≤L−1; ˆf=Eξ(L)1√mW(L)ˆx(L−1).
Note that this definition is in inordinate to the the definition as expected empirical loss (9) in PAC-Bayes.
a Neural Network Gaussian Process (NNGP) for PNN (Lee et al., 2018), which is defined as follows:
K(l)
ij= (ˆx(l)
i)⊤ˆx(l)
j, K(l)
ij,∞= lim
m→∞K(l)
ij,
where subscript i,jdenote the index of input samples. We aim to show that Θ(L)is close to K(L)
∞in large
width. In particular, to prove Theorem A.1, three core steps are:
Step 1 Show at initialization λ0/parenleftbig
Θ(L)(0)/parenrightbig
≥λ0(K(L)
∞)
2and the required condition on m.
Step 2 Show during training λ0/parenleftbig
Θ(L)(t)/parenrightbig
≥λ0(K(L)
∞)
2and the required condition on m.
Step 3 Show during training the expected empirical loss RS(Q)has a linear convergence rate.
In our proof, we mainly focus on deriving the condition on mby analyzing λ0/parenleftbig
Θ(L)(0)/parenrightbig
at initialization
through Lemma A.2 and Lemma A.3. For step 2, we construct Lemma A.4 Lemma A.5 and Lemma A.6 to
demonstrate that λ0/parenleftbig
Θ(L)(t)/parenrightbig
≥λ0(K(L)
∞)
2. This leads to the required condition on mduring train. Finally,
we summarize all the previous lemmas and conclude that the expected training error converges at a linear
rate through Lemma A.7.
A.1 Step 1. Bounding least eigenvalue of PNTK at initialization
We first study the behavior of tangent kernels with an ultra-wide condition, namely m= poly(n,1/λ0,1/δ)
at initialization. Lemmas A.2 and A.3 demonstrate that if mis large, then the feature of each layer is approx-
imately normalized, Θµ(0) + Θσ(0)have a lower bound on the smallest eigenvalue with a high probability.
17Published in Transactions on Machine Learning Research (05/2023)
Lemma A.2 (Initial norm at initialization) .Supposeσ(·)isH-Lipschitz. If m= Ω/parenleftbigg
nLgC(L)2
δ/parenrightbigg
, where
C≜(c2
µ+c2
σ)H(2|σ(0)|/radicalig
2
π+2H), and the geometric series function gC(l) =/summationtextl−1
i=0Ci. Then with probability
at least 1−δover random initialization, for each l∈[L]andi∈[n], we have:
1
2≤∥ˆx(l)
i(0)∥2≤2.
Lemma A.3 (PNTK at initialization) .Supposeσ(·)isH-Lipschitz. If m= Ω/parenleftbigg
n2log(Ln/δ )2O(L)
λ2
0(K(L)
∞)/parenrightbigg
, then
with probability at least 1−δ, we have:
λ0(Θ(L)(0))≥3
4λ0(K(L)
∞).
Proof of Lemma A.2. The proof is by induction method. The induction hypothesis is that with probability
at least 1−(l−1)δ
nLoverW(1)
µ(0),...,W(l−1)
µ(0), for every 1≤l′≤l−1, we have
1
2≤1−gC(l′)
2gC(L)≤∥ˆx(l′)
i(0)∥2≤1 +gC(l′)
2gC(L)≤2,
where we define the geometric series function as gC(l) =/summationtextl−1
i=0Ci.
According to the feed-forward expression, we know that
∥ˆx(l)
i(0)∥2
2=1
mm/summationdisplay
r=1Eξ(l)σ((w(l)
µ,r(0) +c2
σξ(l))⊤ˆxl−1
i(0))2.
Then we have the expectation:
E/bracketleftbigg
∥ˆx(l)
i(0)∥2
2/bracketrightbigg
=Ew,ξ(l)/bracketleftbigg
σ((w(l)+c2
σξ(l))⊤ˆx(l−1)
i(0))2/bracketrightbigg
= (c2
µ+c2
σ)EZ∼N(0,1)σ(∥ˆx(l−1)∥2Z)2.
Here we have used the fact that w(l)(0) +c2
σξ(l)∼(c2
µ+c2
σ)N(0,I)where w(l)(0)∼c2
µN(0,I).
Becauseσ(·)isH-Lipschitz, for1
2≤α≤2, we have
/vextendsingle/vextendsingleEZ∼N(0,1)/bracketleftbig
σ(αZ)2/bracketrightbig
−EZ∼N(0,1)/bracketleftbig
σ(Z)2/bracketrightbig/vextendsingle/vextendsingle
≤EZ∼N(0,1)/bracketleftbig
|σ(αZ)2−σ(Z)2|/bracketrightbig
≤H|α−1|·EZ∼N(0,1)/bracketleftbig
|Z(σ(αZ) +σ(Z))|/bracketrightbig
≤H|α−1|·EZ∼N(0,1)/bracketleftbig
|Z|(|2σ(0)|+H|(α+ 1)Z|)/bracketrightbig
≤H|α−1|·(2|σ(0)|EZ∼N(0,1)[|Z|] +H|α+ 1|·EZ∼N(0,1)[Z2])
=H|α−1|·(2|σ(0)|/radicalbigg
2
π+H|α+ 1|)
≤C
c2µ+c2σ|α−1|,
where we define C≜(c2
µ+c2
σ)H(2σ(0)/radicalig
2
π+ 2H). For the variance we have:
Var/bracketleftig
∥ˆx(l)
i(0)∥2
2/bracketrightig
=Var/bracketleftig
σ(w(l)
r(0)⊤ˆx(l)
i(0))2/bracketrightig
≤1
mEw(l),ξ(l)/bracketleftig
σ((w(l)+c2
σξ(l))⊤ˆx(l)
i(0))4/bracketrightig
≤(c2
σ+c2
µ)2
mEw∼N(0,I)/bracketleftbigg/parenleftig
|σ(0)|+H|w⊤ˆx(l)
i(0)|/parenrightig4/bracketrightbigg
≤C2
m,
18Published in Transactions on Machine Learning Research (05/2023)
whereC2≜(c2
σ+c2
µ)2(σ(0)4+ 8|σ(0)|3H/radicalbig
2/π+ 24σ(0)2H2+ 64σ(0)H3/radicalbig
2/π+ 512H4), and the last
inequality we used the formula for the first four absolute moments of Gaussian.
Applying Chebyshev’s inequality and plugging in our assumption on m, we have with probability 1−δ
nL
overW(l)
µ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddoubleˆx(l)
i(0)/vextenddouble/vextenddouble2
2−E/vextenddouble/vextenddoubleˆx(l)
i(0)/vextenddouble/vextenddouble2
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
2gC(L).
Thus with probability 1−dδ
nLoverW(1)
µ,...,W(l)
µ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddoubleˆx(l)
i(0)/vextenddouble/vextenddouble
2−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddoubleˆx(l)
i(0)/vextenddouble/vextenddouble2
2−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤CgC(l−1)
2gC(L)+1
2g(L)
=gC(l)
2gC(L).
Using union bounds over i∈[n], we complete the proof of lemma.
Proof of Lemma A.3. For a weight matrix, we decompose it into mweight vectors, namely W(l)=
[w(l)
1,w(l)
2,···,w(l)
m]. Then the derivative of the expected output over the parameters w(l)
µ,randw(l)
σ,rcan be
expressed as
∂ˆf(xi)
∂w(l)
µ,r=Eξ/bracketleftigg
1√m∂ˆf(xi)
∂ˆx(l)
i,rσ′((w(l)
r)⊤ˆx(l−1))ˆx(l−1)/bracketrightigg
,
∂ˆf(xi)
∂w(l)
σ,r=Eξ/bracketleftigg
1√m∂ˆf(xi)
∂ˆx(l)
i,rσ′((w(l)
r)⊤ˆx(l−1))ˆx(l−1)⊙ξ(l)
r/bracketrightigg
,(21)
where we have interchanged integration and differentiation over activation σ(·). According to the definition
4.1 of PNTK for each layer can be expressed as:
Θ(l)
µ=m/summationdisplay
r=1∇w(l)
µ,rˆf(X;t)⊤∇w(l)
µ,rˆf(X;t),Θ(l)
σ=m/summationdisplay
r=1∇w(l)
σ,rˆf(X;t)⊤∇w(l)
σ,rˆf(X;t).
Through a standard calculation we show that:
Θ(L)
µ,ij(0) =m/summationdisplay
r=1Eξ(L−1)/bracketleftbigg1√mσ((w(L−1)
r )⊤ˆx(L−2)
i )/bracketrightbigg
Eξ(L−1)/bracketleftig
σ((w(L−1)
r )⊤ˆx(L−2)
j )/bracketrightig
, (22)
Θ(L)
σ,ij(0) =m/summationdisplay
r=1Eξ(L−1),ξ(L)
r/bracketleftbigg1√mσ((w(L−1)
r )⊤ˆx(L−2)
i )ξ(L)
r/bracketrightbigg
Eξ(L−1),ξ(L)
r/bracketleftbigg1√mσ((w(L−1)
r )⊤ˆx(L−2)
j )ξ(L)
r/bracketrightbigg
.(23)
Bounding Θ(L)
µ.The proof is by induction. By analyzing Equation 22, we find that for all pairs of i,j,
Θ(L)
µ,ij(0)is the average of mi.i.d. random variables, with the expectation:
K(L)
ij,∞=c2
µ·Ew∼N(0,I)/bracketleftbigg
Eξ/bracketleftbigg1√mσ(w+c2
σξ)⊤ˆx(L−1))/bracketrightbigg
Eξ/bracketleftbigg1√mσ((w+c2
σξ)⊤ˆx(L−1))/bracketrightbigg/bracketrightbigg
.
Then we find the difference between Θ(L)
µ,ij(0)andK(L)
ij,∞can be recursively calculated through an induction
method. The induction hypothesis is that with probability 1−δover the/braceleftig
W(l)
µ/bracerightigL−1
l=1, for any 1≤l≤
L−1,1≤i,j≤n,/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
r=1(ˆx(l)
i,r)⊤ˆx(l)
j,r−K(l)
ij/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤CL/radicalbigg
log(Ln/δ )
m, (24)
19Published in Transactions on Machine Learning Research (05/2023)
whereCis a constant.
We start from the first layer:
E/bracketleftigg
1
mm/summationdisplay
r=1ˆx(1)
i,rˆx(1)⊤
j,r/bracketrightigg
=K(1)
ij,∞.
We can apply standar Hoeffding bound and obtain the following concentration inequalities. With probability
at least 1−δ
L, we have
max
i,j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1ˆx(1)
iˆx(1)⊤
j−K(1)
ij,∞/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤/radicalbigg
2 log(4Ln2)2/δ)
m.
Now we prove the induction step. In the following, by E(l)we mean taking expectation over w(l)∼N(0,c2
µI).
Suppose that Equation (24) holds for 1≤l′≤lwith probability at least 1−l
Lδ, then we want to show
the equations holds for l+ 1with probability at least 1−δ/Lconditioned on previous layers satisfying
Equation 24. Same as the base case, applying concentration inequalities, we have with probability at least
1−δ/L,
max
ij/vextenddouble/vextenddouble/vextenddoubleE(l)K(l)
ij−K(l)
ij/vextenddouble/vextenddouble/vextenddouble
∞≤/radicalbigg
2 log(4Ln2)2/δ)
m.
Now it remains to bound the differences:
max
ij/vextenddouble/vextenddouble/vextenddoubleE(l)K(l)
ij−K(l)
ij,∞/vextenddouble/vextenddouble/vextenddouble
∞≤∥E(u(l),v(l))∼AEZσ(u(l)+c2
σZ)EZσ(v(l)+c2
σZ)
−E(u(l),v(l))∼A∞EZσ(u(l)+c2
σZ)EZσ(v(l)+c2
σZ)
(a)
≤C∥A−A∞∥F
≤2C∥A−A∞∥∞
≤C1max
ij∥K(l−1)
ij−K(l−1)
ij,∞∥∞,
where A=/bracketleftigg
K(l−1)
iiK(l−1)
ij
K(l−1)
jiK(l−1)
jj/bracketrightigg
andA∞=/bracketleftigg
K(l−1)
ii,∞K(l−1)
ij,∞
K(l−1)
ji,∞K(l−1)
jj,∞/bracketrightigg
, (a) is by property of activation function and
Taylor’s Theorem, and C1andC2are positive constants.
Thus by matrix perturbation theory we have,
/vextenddouble/vextenddouble/vextenddoubleΘ(L)
µ−K(l)
∞/vextenddouble/vextenddouble/vextenddouble
2≤/summationdisplay
ij/vextendsingle/vextendsingle/vextendsingleΘ(L)
µ,ij−K(l)
ij,∞/vextendsingle/vextendsingle/vextendsingle≤CLn/radicalbigg
log(Ln/δ )
m.
Bounding Θ(L)
σ(0).By analyzing Equation (23), the independent random variable ξ(L)
ryields:
Θ(L)
σ,ij(0) =m/summationdisplay
r=1Eξ,ξ(L)
r/bracketleftbigg1√mσ((w(L−1)
r )⊤x(L−2)
i )ξ(L)
r/bracketrightbigg
Eξ,ξ(L)
r/bracketleftbigg1√mσ((w(L−1)
r )⊤x(L−2)
j )ξ(L)
r/bracketrightbigg
= 0.
Therefore, at initialization and during, the Θ(L)
σ(0)keeps zero.
Finally, ifm= Ω/parenleftbigg
n2log(Ln/δ )2O(L)
λ2
0(K(L)
∞)/parenrightbigg
, then with probability at least 1−δ,
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleΘ(L)(0)−K(L)
∞/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleΘ(L)
µ(0)−K(L)
∞/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤λ0(K(L))
4,
which completes the proof.
20Published in Transactions on Machine Learning Research (05/2023)
A.2 Step 2. Bounding least eigenvalue of PNTK during training.
The next problem is that PNTKs are time-dependent matrices, thus varying during training. To account
for this problem, we establish following lemmas stating that if the weights during training are close to their
initialization, then the corresponding PNTK is close to their initialization.
Lemma A.4. Suppose for every l∈[L],/vextenddouble/vextenddoubleW(l)
µ(0)/vextenddouble/vextenddouble
2≤cµ,0√m,/vextenddouble/vextenddoubleˆx(l)(0)/vextenddouble/vextenddouble
2≤cx,0and/vextenddouble/vextenddoubleW(l)
µ(t)−
W(l)
µ(0)/vextenddouble/vextenddouble
F≤√mRfor some constant cµ,0,cx,0>0andR≤cµ,0. Ifσ(·)isH-Lipschitz, then with
probability at least 1−δ, we have
/vextenddouble/vextenddoubleˆx(l)(t)−ˆx(l)(0)/vextenddouble/vextenddouble
2≤cx,0HRgcx(l),
wherecx= 2Hcµ,0.
Lemma A.5. Supposeσ(·)isH−Lipschitz and β−smooth. Suppose for l∈[L],/vextenddouble/vextenddoubleW(l)
µ(0)/vextenddouble/vextenddouble
2≤cµ,0√m,
1
cx,0≤/vextenddouble/vextenddoubleˆx(l)(0)/vextenddouble/vextenddouble
2≤cx,0. If/vextenddouble/vextenddoubleW(l)
µ(t)−W(l)
µ(0)/vextenddouble/vextenddouble
F≤√mRwhereR≤cgcx(L)−1λ0(K(L)
∞)n−1for some small
constantcandcx= 2√cσHcµ,0then with probability at least 1−δ, we have:
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleΘ(L)(t)−Θ(L)(0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤λ0(K(L)
∞)
4.
Lemma A.6. IfRS(t′)≤exp(−λ0(K(L)
∞)t′)RS(0)holds for 0≤t′≤t, we have for any 0≤t′≤t,
/vextenddouble/vextenddouble/vextenddoubleW(l)
µ(t′)−W(l)
µ(0)/vextenddouble/vextenddouble/vextenddouble
F≤R′√m,
whereR′=16cx,0(cx)L√n∥y−ˆf(X,0)∥2
λ0(K(L)
∞)√m, for some small constant cwithcx= max{2√cσLcµ,0,1}.
Proof of Lemma A.4. The proof sketch is by induction method.
Forl= 0, where the target is input which is fixed, thus satisfying the hypothesis. Now suppose the induction
hypothesis holds for l′= 0,...,l−1, we consider l′=l.
/vextenddouble/vextenddouble/vextenddoubleˆx(l)(t)−ˆx(l)(0)/vextenddouble/vextenddouble/vextenddouble
2
=1√m/vextenddouble/vextenddouble/vextenddoubleEξ(l)σ(W(l)(t)ˆx(l−1)(t))−Eξ(l)σ(W(l)(0)ˆx(l−1)(0))/vextenddouble/vextenddouble/vextenddouble
2
≤1√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubleEξ(l)σ(W(l)(t)ˆx(l−1)(t))−Eξ(l)σ(W(l)(t)ˆx(l−1)(0))/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
+1√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubleEξ(l)σ(W(l)(t)ˆx(l−1)(0))−Eξ(l)σ(W(l)(0)ˆx(l−1)(0))/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤1√mH/parenleftig/vextenddouble/vextenddouble/vextenddoubleEξ(l)W(l)(0)/vextenddouble/vextenddouble/vextenddouble
2+/vextenddouble/vextenddouble/vextenddoubleEξ(l)W(l)(t)−Eξ(l)W(l)(0)/vextenddouble/vextenddouble/vextenddouble
F/parenrightig
·/vextenddouble/vextenddouble/vextenddouble/vextenddoubleˆx(l−1)(t)−ˆx(l−1)(0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
+1√mH/parenleftbigg/vextenddouble/vextenddouble/vextenddoubleEξ(l)W(l)(t)−Eξ(l)W(l)(0)/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg/vextenddouble/vextenddoubleˆxh−1(0)/vextenddouble/vextenddouble
2
≤1√mH/parenleftbig
cµ,0√m+R√m/parenrightbig
HRcx,0gcx(l−1) +1√mH√mRcx,0
≤HRcx,0(cxgcx(l−1) + 1)
≤HRcx,0gcx(l).
Proof of Lemma A.5. For simplicity, we define zi,r(t) =w(L−1)
r (t)⊤ˆx(L−2)
i (t).
21Published in Transactions on Machine Learning Research (05/2023)
Now we bound the distance between Θ(L)
µ,ij(t)andΘ(L)
µ,ij(0)through the following inequality:
/vextendsingle/vextendsingle/vextendsingle/vextendsingleΘ(L)
µ,ij(t)−Θ(L)
µ,ij(0)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
mm/summationdisplay
r=1Eξ(l−1)σ(zi,r(t))Eξ(l−1)σ(zj,r(t))−1
mm/summationdisplay
r=1Eξ(l−1)σ(zi,r(0))Eξ(l−1)σ(zj,r(0))/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
mm/summationdisplay
r=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleEξ(l−1)σ(zi,r(t))Eξ(l−1)σ(zj,r(t))−Eξ(l−1)σ(zi,r(t))Eξ(l−1)σ(zj,r(0))/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+1
mm/summationdisplay
r=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleEξ(l−1)σ(zi,r(t))Eξ(l−1)σ(zj,r(0))−Eξ(l−1)σ(zi,r(0))Eξ(l−1)σ(zj,r(0))/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βH
m/parenleftiggm/summationdisplay
r=1Eξ(l−1)|zi,r(t)−zi,r(0)|+Eξ(l−1)|zj,r(t)−zj,r(0)|/parenrightigg
≤βH√m/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
r=1/vextendsingle/vextendsingleEξ(l−1)zi,r(t)−Eξ(l−1)zi,r(0)/vextendsingle/vextendsingle2
≤nβHcx,0gcx(L)R.
where we have used the result in Lemma A.4. Therefore we can bound the perturbation:
/vextenddouble/vextenddouble/vextenddoubleΘ(L)
µ(t)−Θ(L)
µ(0)/vextenddouble/vextenddouble/vextenddouble
F=/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i,j=1/vextendsingle/vextendsingle/vextendsingleΘ(L)
µ,ij(t)−Θ(L)
µ,ij(0)/vextendsingle/vextendsingle/vextendsingle2
≤βHcx,0gcx(L)R.
Recall the bound on R, which isR≤cgcx(L)−1λ0(K(L)
∞)n−1, we have the desired result:
/vextenddouble/vextenddouble/vextenddoubleΘ(L)(t)−Θ(L)(0)/vextenddouble/vextenddouble/vextenddouble
2≤λ0(K(L)
∞)
4.
Proof of Lemma A.6. We first consider the derivative of W(l)
µand have:
/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dsW(l)
µ(s)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
=η/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg1
m/parenrightbiggL−l+1
2n/summationdisplay
i=1(yi−ˆf(xi;s))ˆx(l−1)
i(s)Eξ(k)/parenleftiggL/productdisplay
k=l+1J(k)
i(s)W(k)(s)/parenrightigg
J(l)
i(s)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F
≤η/parenleftbigg1
m/parenrightbiggL−l+1
2n/summationdisplay
i=1/vextendsingle/vextendsingleyi−ˆf(xi;s)/vextendsingle/vextendsingle/vextenddouble/vextenddoubleˆx(l−1)
i(s)/vextenddouble/vextenddouble
2L/productdisplay
k=l+1/vextenddouble/vextenddoubleEξ(k)W(k)(s)/vextenddouble/vextenddouble
2HL−l+1,
where
J(l′)≜diag/parenleftig
σ′/parenleftig
(w(l′)
1)⊤x(l′−1)/parenrightig
,...,σ′/parenleftig
(w(l′)
m)⊤x(l′−1)/parenrightig/parenrightig
∈Rm×m
are the derivative matrices induced by the activation function and we have used/vextenddouble/vextenddoubleJ(k)(s)/vextenddouble/vextenddouble
2≤H.
To bound/vextenddouble/vextenddoubleˆx(l−1)
i(s)/vextenddouble/vextenddouble
2, by Lemma A.4 we get:
/vextenddouble/vextenddoubleˆx(l−1)
i(s)/vextenddouble/vextenddouble
2≤Hcx,0gcx(l)R+cx,0≤2cx,0.
22Published in Transactions on Machine Learning Research (05/2023)
To bound/vextenddouble/vextenddoubleEξ(k)W(k)(s)/vextenddouble/vextenddouble
2=/vextenddouble/vextenddoubleW(k)
µ(s)/vextenddouble/vextenddouble
2, we have:
L/productdisplay
k=l+1/vextenddouble/vextenddoubleW(k)
µ(s)/vextenddouble/vextenddouble
2≤L/productdisplay
k=l+1/parenleftig/vextenddouble/vextenddoubleW(k)
µ(0)/vextenddouble/vextenddouble
2+/vextenddouble/vextenddoubleW(k)
µ(s)−W(k)
µ(0)/vextenddouble/vextenddouble
2/parenrightig
≤L/productdisplay
k=l+1(cµ,0√m+R′√m)
= (cµ,0+R′)L−lmL−l
2
≤(2cµ,0)L−lmL−l
2.
Plugging in these two bounds back, we obtain
/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dsW(l)
µ(s)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤4ηcx,0cL
xn/summationdisplay
i=1|yi−ˆf(xi;s)|
≤4ηcx,0cL
x√n/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;s)/vextenddouble/vextenddouble/vextenddouble
2
≤e−λ0sηλ0(K(L)
∞)R′√m.
Integrating the derivative of weights, we obtain:
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleW(l)
µ(s)−W(l)
µ(0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤/integraldisplays
s′=0/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
ds′W(l)
µ(s′)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤R′√m.
A.3 Setp 3. Towards linear convergence rate of expected empirical loss
Now we process to analyze the convergence rate of expected empirical error. Combined with fact that least
eigenvalue of PNTKs and change of weights are bounded during training, the behavior of the loss is traceable.
To finalize the proof for Theorem 4.2, we show:
Lemma A.7. IfR′<R, we haveRS(t)≤exp(−λ0(K(L)
∞)t)RS(0).
Proof of Lemma A.7. According to the gradient flow of output function, we have:
dˆf(xi,t)
dt=L/summationdisplay
l=1/parenleftigg/angbracketleftigg
∂ˆf(xi;t)
∂W(l)
µ,dW(l)
µ(t)
dt/angbracketrightigg
+/angbracketleftigg
∂ˆf(x;t)
∂W(l)
σ,dW(l)
σ(t)
dt/angbracketrightigg/parenrightigg
=n/summationdisplay
j=1(yi−ˆf(xj))L/summationdisplay
l=1/parenleftigg/angbracketleftigg
∂ˆf(xi)
∂W(l)
µ,ˆf(xj)
∂W(l)
µ/angbracketrightigg
+/angbracketleftigg
∂ˆf(xi)
∂W(l)
σ,∂ˆf(xj)
∂W(l)
σ/angbracketrightigg/parenrightigg
=n/summationdisplay
j=1(yj−ˆf(xj;t))(Θµ,ij+Θσ,ij).
Then the dynamics of loss can be calculated:
d
dtRS(t) =1
2d
dt/vextenddouble/vextenddouble/vextenddoubleˆf(X;t)−y/vextenddouble/vextenddouble/vextenddouble2
2
=−/parenleftig
y−ˆf(X,t)/parenrightig⊤
(Θµ(t) +Θσ(t))/parenleftig
y−ˆf(X,t)/parenrightig
≤−λ0(Θµ(t) +Θσ(t))/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;t)/vextenddouble/vextenddouble/vextenddouble2
2
≤−λ0/parenleftig
Θ(L)
µ(t) +Θ(L)
σ(t)/parenrightig/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;t)/vextenddouble/vextenddouble/vextenddouble2
2
≤−λ0/parenleftig
K(L)
∞/parenrightig/vextenddouble/vextenddouble/vextenddoubley−ˆf(X;t)/vextenddouble/vextenddouble/vextenddouble2
2,
23Published in Transactions on Machine Learning Research (05/2023)
where we have used the condition R′< Randλ0(Θ(L)
µ(t) +Θ(L)
σ(t))≤λ0(Θµ(t) +Θσ(t)). Therefore, we
have the desired result:
RS(t)≤exp(−λ0(K(L)
∞)t)RS(0).
Finally, we provide a bound for /hatwideRS(Q(0)):
/vextenddouble/vextenddouble/vextenddoubley−ˆf(X,0)/vextenddouble/vextenddouble/vextenddouble2
2=n/summationdisplay
i=1/parenleftig
y2
i+yiˆf(xi,0) +f(xi,0)2/parenrightig
=n/summationdisplay
i=1(1 +O(1)) =O(n).
Recall that in Lemma A.5 and Lemma A.6:
R≤cgcx(L)−1λ0(K(L)
∞)n−1, R′=16cx,0(cx)L√n∥y−ˆf(X,0)∥2
λ0(K(L)
∞)√m.
ThusR′<Ryieldsm= Ω/parenleftbigg
n42O(L)
λ4
0(K(L)
∞)/parenrightbigg
.
B Proof of Theorem 4.3
Theorem B.1 (Restatement of Theorem 4.3) .Supposem≥poly(n,1/λ0,1/δ,1/E)and the objective func-
tion follows the form (17), for any test input xte∈Rdwith probability at least 1−δover the random
initialization, we have
ˆf/parenleftbig
xte;t/parenrightbig
|t=∞=Θ∞
µ(x,X)/parenleftbig
Θ∞
µ(X,X) +λ/c2
σI/parenrightbig−1y±E, (25)
whereEis a small error term between output function of finite-width PNN and infinite-width PNN.
Proof of Theorem B.1. To proceed the proof, we first establish the result of kernel ridge regression in the
infinite-width limit, and then bound the perturbation on the predict. According the linearization rules for
infinitely-wide networks (Lee et al., 2019), the output function can be expressed as,
ˆf∞(x,t) =ϕµ(x)⊤(θµ(t)−θµ(0)) + ϕσ(x)⊤(θσ(t)−θσ(0)),
where ϕµ(x) =∇θµˆf(x; 0), and ϕσ(x) =∇θσˆf(x; 0). Recall that we assume that θσdoes not change during
training, then the KL divergence reduces to
KL=1
2nc2σ∥θµ(t)−θµ(0)∥2
2.
Then the gradient flow equation for θµbecomes,
dθµ(t)
dt=∂L(t)
∂θµ(t)=/parenleftbig
Θ∞
µ+λ/c2
σI/parenrightbig/parenleftig
θ(µ)(t)−θ(µ)(0)/parenrightig
+ϕµ(X)/parenleftig
ˆf∞(X; 0)−y/parenrightig
,
which is an ordinary differential equation, and the solution is,
θµ(t) =ϕµ(X)⊤/parenleftbig
Θ∞
µ(X,X) +λ/c2
σI/parenrightbig−1/parenleftbig
I−e−(Θ∞
µ(X,X)+λ/c2
σI)t/parenrightbig
y.
Plug this result into the linearization of expected output function, we have,
ˆf∞(x;t) =Θ∞
µ(x,X)(Θ∞
µ(X,X) +λ/c2
σI)−1(I−e−(Θ∞
µ(X,X)+λ/c2
σI)t)y.
Then we take the time to be infinity and have:
ˆf∞(x)|t=∞=Θ∞
µ(x,X)(Θ∞
µ(X,X) +λ/c2
σI)−1y.
24Published in Transactions on Machine Learning Research (05/2023)
The next step is to show the difference between finite-width neural network and infinitely-wide network:
/vextendsingle/vextendsingle/vextendsingleˆf(xte)−ˆf∞(xte)/vextendsingle/vextendsingle/vextendsingle≤O(E),
whereE=Einit+√nEΘ
λ0+βwith/vextenddouble/vextenddouble/vextenddoubleˆf(xte; 0)/vextenddouble/vextenddouble/vextenddouble
2≤Einitand∥Θ∞
µ−Θµ(t)∥2≤EΘ. Note the expression of output
function in the infinite-width limit can be rewritten as ˆf∞(xte) =ϕ(xte)⊤βand the solution to this equation
can be further written as the result of applying gradient flow on the following kernel ridge regression problem
min
βn/summationdisplay
i=11
2n/vextenddouble/vextenddoubleϕ(xi)⊤β−xi/vextenddouble/vextenddouble2
2+λ/c2
σ∥β∥2
2,
with initialization β(0) = 0. We use β(t)to denote this parameter at time ttrained by gradient flow and
ˆf∞(xte,β(t))be the predictor for xteat timet. With these notations, we rewrite
ˆf∞(xte) =/integraldisplay∞
t=0dˆf(β(t),xte)
dtdt,
where we have used the fact that the initial prediction is 0.
We thus can analyze the difference between the PNN predictor and infinite-width PNN predictor via this
integral form as follows:
/vextenddouble/vextenddouble/vextenddoubleˆf∞(xte)−ˆf(xte)/vextenddouble/vextenddouble/vextenddouble
2
≤/vextenddouble/vextenddouble/vextenddoubleˆf(θµ(0),xte)/vextenddouble/vextenddouble/vextenddouble
2+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay∞
t=0/parenleftigg
dˆf(θµ(t),xte)
dt−dˆf∞(β(t),xte)
dt/parenrightigg
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
=/vextenddouble/vextenddouble/vextenddoubleˆf(θ(0),xte)/vextenddouble/vextenddouble/vextenddouble
2+/vextenddouble/vextenddouble/vextenddouble/vextenddouble−1
n/integraldisplay∞
t=0(Θ∞
µ(xte,X)⊤(ˆf∞(t)−y)−Θµ(xte,X;t)⊤(ˆf(t)−X))dt
−λ/c2
σ/integraldisplay∞
t=0(ϕ∞(xte)⊤β(t)−ϕµ(xte;t)⊤θ(t)dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤Einit+/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/integraldisplay∞
t=0(Θµ(xte,X;t)−Θ∞(xte,X))⊤(ˆf(t)−X)dt+β/integraldisplay∞
t=0(ϕµ(xte,t)−ϕ∞(xte))⊤β(t)dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
+/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/integraldisplay∞
t=0Θ∞
µ(xte,X)⊤(ˆf∞(t)−ˆf(t))dt+λ/c2
σ/integraldisplay∞
t=0/parenleftbig
ϕ∞
µ(xte)/parenrightbig⊤(β(t)−θµ(t))dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤Einit+/parenleftbigg
max
0≤t≤∞/vextenddouble/vextenddoubleΘµ(xte,X;t)−Θ∞
µ(xte,X)/vextenddouble/vextenddouble
2/integraldisplay∞
t=0/vextenddouble/vextenddouble/vextenddoubleˆf(t)−y/vextenddouble/vextenddouble/vextenddouble
2dt
+λ/c2
σmax
0≤t≤∞/vextenddouble/vextenddoubleϕµ(xte;t)−ϕ∞
µ(xte)/vextenddouble/vextenddouble
2/integraldisplay∞
t=0∥β∥2dt/parenrightbigg
+/parenleftbigg
max
0≤t≤∞∥Θ∞
µ(xte,X)∥2/integraldisplay∞
t=0∥ˆf(t)−ˆf∞(t)∥2dt
+λ/c2
σmax
0≤t≤∞/vextenddouble/vextenddoubleϕ∞
µ(xte)/vextenddouble/vextenddouble
2/integraldisplay∞
t=0∥β(t)−θµ(t)∥2dt/parenrightbigg
≜Einit+I2+I3,
where θµ(t) =θµ(t)−θµ(0). For the second term I2, we know that∥ˆf(t)−y∥2
2+β∥θµ∥2
2≤exp(−(λ0+
λ/c2
σ)t)∥ˆf(0)−y∥2
2.
Therefore, we can bound:
/integraldisplay∞
0∥ˆf(t)−y∥2+β∥θµ(t)∥2dt
≤/integraldisplay∞
t=0exp(−(λ0+λ/c2
σ)t)(∥ˆf(0)−y∥2)dt
=O/parenleftbigg√n
λ0+λ/c2σ/parenrightbigg
.
25Published in Transactions on Machine Learning Research (05/2023)
As a result, we have I2=O/parenleftig√nEΘ
λ0+λ/c2σ/parenrightig
. To bound I3, we have
/integraldisplay∞
0∥ˆf(t)−ˆf∞(t)∥2+λ/c2
σ∥β−θµ∥2dt
≤/integraldisplay∞
0∥ˆf(t)−y∥2+λ/c2
σ∥θµ∥2dt+/integraldisplay∞
0∥ˆf∞(t)−X∥2+λ/c2
σ∥β∥2dt
≤O/parenleftbigg√n
λ0+λ/c2σ/parenrightbigg
.
As a result, we have I3=O/parenleftig√nEΘ
λ0+λ/c2σ/parenrightig
. Lastly, we put things together and get
|ˆf(t)−ˆf∞(t)|≤O/parenleftbigg
Einit+EΘ√n
λ0+β/parenrightbigg
.
B.1 Proof of Theorem 4.4
Proof of Theorem 4.4. Our proof is based on a characterization of the margin MS
QandMS
Q2via the explicit
solution found in Theorem 4.3:
MS
Q=1
nn/summationdisplay
i=1yi·Eh∼Qh(xi) =1
nn/summationdisplay
i=1yiˆf∞(xi),
MS
Q2=1
nn/summationdisplay
i=1Eh,h′∼Q2h(xi)h′(xi) =1
nn/summationdisplay
i=1n/summationdisplay
j=1ˆf∞(xi)ˆf∞(xj).
Then by Theorem 3.3, we have:
RD(BQ)≤1−/parenleftbigg
1
n/summationtextn
i=1yiˆf∞(xi)−2B√
ln(2n
δ)√
2n/parenrightbigg2
1
n/summationtextn
i=1/summationtextn
j=1ˆf∞(xi)ˆf∞(xj) + 2B2√
ln(2n
δ)√
2n.
C Appendix for Experiments
This section contains additional experimental results and derivation process for proxy. Training is performed
with a server with a CPU with 5,120 cores, and a 32 GB Nvidia Quadro V100.
C.1 Derivation of proxy
Our derivation is based on a characterization of the empirical error and KL divergence term via the explicit
solution found in Theorem 4.3. The objective function consists of two terms, one is the empirical error,
and another is KL divergence. (i) We first bound the empirical error/radicalig/summationtextn
i=1(ˆf∞(xi,t=∞)−yi)2with
following inequality,
/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i=1(ˆf∞(xi,t=∞)−yi)2=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleΘ∞
µ(X,X)(Θ∞
µ(X,X) +λ/c2
σI)−1y−y/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleλ/c2
σ/parenleftbig
Θ∞
µ(X,X) +λ/c2
σI/parenrightbig−1y/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
=λ/c2
σ/radicalig
y⊤/parenleftbig
Θ∞µ(X,X/parenrightbig
+λ/σ2
0I)−2y.
26Published in Transactions on Machine Learning Research (05/2023)
Then we bound the error term as follows:
1
nn/summationdisplay
i=1ℓ/parenleftbigˆf∞(xi/parenrightbig
,yi) =1
nn/summationdisplay
i=1/bracketleftbigg
ℓ(ˆf∞(xi),yi)−ℓ(yi,yi)/bracketrightbigg
≤1
nn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingleˆf∞(xi)−yi/vextendsingle/vextendsingle/vextendsingle
≤1√n/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingleˆf∞(xi)−yi/vextendsingle/vextendsingle/vextendsingle2
≤λ
c2σ/radicaligg
y⊤(Θ∞µ(X,X) +λ/c2σI)−2y
n.
(ii) The next step is to calculate the KL divergence. According to the solution of differential equation in
Theorem B.1, we have:
θµ(t)|t=∞−θµ(0) = ϕµ(x)⊤/parenleftbig
Θ∞
µ(X,X) +λ/c2
σI/parenrightbig−1y
Therefore, the KL divergence is,
KL= 1/c2
σ·y⊤/parenleftbig
Θ∞
µ(X,X) +λ/c2
σI/parenrightbig−1Θ∞
µ(X,X)/parenleftbig
Θ∞
µ(X,X) +λ/c2
σI/parenrightbig−1y
≤1
c2σy⊤/parenleftbig
Θ∞
µ(X,X) +λ/c2
σI/parenrightbig−1y.
Finally, by Equation 5, we have,
RS(Q) +λKL(θ(t)∥θ(0))
n≤y⊤/parenleftbig
Θ∞
µ(X,X) +λ
c2σI/parenrightbig−1y
nc2σ+λ
c2σ/radicaligg
y⊤/parenleftbig
Θ∞µ(X,X) +λ
c2σI/parenrightbig−2y
n.
C.2 Validation of theoretical results
We first provide empirical support showing that the training dynamics of wide probabilistic neural networks
usingthetrainingobjectivederivedfromaPAC-BayesboundarecapturedbyPNTK,whichvalidatesLemma
A.6.
Consider a three hidden layer ReLU fully-connected network of the training objective derived from the PAC-
Bayesian lambda bound in Equation (5), using an ordinary MSE function as loss. The neural network is
trained with a full-batch gradient descent using learning rates equal to one on a fixed subset of MNIST
(|D|= 128) of ten classifications. A random initialized prior with no connection to data is used since it is
in line with our theoretical setting and we only intend to observe the change in parameters rather than the
performance of the actual bound.
AfterT= 217steps of gradient descent updates from different random initialization, we plot the changes
ofW(l)
µandW(l)
σof input/output/hidden layer with respect to width mfor each layer on Figure 2. We
observe that the relative Frobenius norm change in the input/output layer’s weights scales as 1/√mwhile
the hidden layers’ weight scales is 1/mduring the training, which verifies Lemma A.6.
C.3 Comparison of gradient norm with respect to mean weight and variance weight
We then conduct an experiment to compare the gradient of norm with respect to θµandθσ. The result is
shown in Figure 3. We can see that the gradient norm of ∇θµf(x)is much larger than that of ∇θσf(x),
which implies that θσis effectively fixed during gradient descent training.
27Published in Transactions on Machine Learning Research (05/2023)
Figure 2: Relative Frobenius norm change in µandσrespectively during training with MSE loss which is
derived from the classic PAC-Bayesian bound, where mis the width of the network.
Figure 3: Comparison between the norm of derivative of output to mean and variance weights.
Figure 4: Correlation between aggregated proxy PAand generalization bound.
C.4 Correlation between generalization bound proxy metric and generalization bound
In Figure 1, we observe a positive and significant correlation between PAand generalization bound held
among different values of a selected hyperparameter while fixing other hyperparameters. Furthermore, we
provide a Figure 4 presenting the correlation for aggregated values of ρ0andλ, under the circumstance where
50% data is used for prior training. We can clearly see that lower PAcorresponds to the lower bound, with
a strong positive Kendall-tau correlation of 0.7.
28Published in Transactions on Machine Learning Research (05/2023)
C.5 Grid search
For selecting hyperparameters, we conduct a grid search over ρ0, percent of prior data, and KL penalty
λ. Notably, we do grid sweep over the data for prior training with different proportion in [0.2, 0.3, 0.4,
0.5, 0.6, 0.7, 0.8, 0.9] since 0.2 is the minimum proportion required for obtaining a reasonably lower value
generalization bound (Dziugaite et al., 2021). For the rest, we run over ρ0at value [0.03, 0.05, 0.07, 0.09,
0.1, 0.3, 0.5, 0.7] for FCN ([0.05, 0.07, 0.09, 0.1, 0.3, 0.5, 0.7, 0.9] for CNN) and KL penalty at [0.0001,
0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1] for both structures.
C.6 Scability of Proxy
In our proposed method for hyperparameter selection, we choose a subset of data for computing NTK to
balance computational efficiency and accuracy. To investigate the scalability of PA, we carry out additional
experiments to explore the relationship between the error bound and PAconcerning the number of samples
perclass. Theexperimentalresults, asdepictedinFigure5, demonstratethatwhenthesizeofthedatasubset
surpasses a certain threshold (125 samples per class), the performance of PAwith different hyperparameters
stabilizes.
29Published in Transactions on Machine Learning Research (05/2023)
Figure 5: The correlation between PAand error bound concerning the data size.
30