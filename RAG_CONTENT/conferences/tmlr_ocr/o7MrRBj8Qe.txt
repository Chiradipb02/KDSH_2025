Under review as submission to TMLR
Particle-based Online Bayesian Sampling
Anonymous authors
Paper under double-blind review
Abstract
Online learning has gained increasing interest due to its capability of tracking real-world
streaming data. Although it has been widely studied in the setting of frequentist statistics,
few works have considered online learning with the Bayesian sampling problem. In this paper,
we study an Online Particle-based Variational Inference (OPVI) algorithm that updates a set
of particles to gradually approximate the Bayesian posterior. To reduce the gradient error
caused by the use of stochastic approximation, we include a sublinear increasing batch-size
method to reduce the variance. To track the performance of the OPVI algorithm concerning
a sequence of dynamically changing target posterior, we provide the first theoretical analysis
for the dynamic regret from the perspective of Wasserstein gradient flow. Experimental
results on the Bayesian Neural Network show that the proposed algorithm achieves up to
20% improvement than naively applying existing Bayesian sampling methods in the online
setting.
1 Introduction
Online learning is an indispensable paradigm for problems in the real world, as a machine learning system is
often expected to adapt to newly arrived data and respond in real-time. The key challenge in this setting is
that the model cannot be updated with all data in history each time, which grows linearly and would make
the system unsustainable. There are quite a few online optimization methods developed over the decades that
address the challenge by only taking the last arrived batch of data for each update and by using a shrinking
step size to control the increase of error. They have been successfully applied to a wide range of tasks like
online ranking, network scheduling and portfolio selection (Yu et al., 2017; Pang et al., 2022).
Online optimization methods can directly be applied to update models that are fully specified by a certain
value of its parameters. Beyond such models, there is another class of models known as Bayesian models
that treat the parameters as random variables, thus giving an output also as a random variable (often the
expectation is taken as the final output on par with the conventional case). The stochasticity enables Bayesian
models to provide diverse outputs, characterize prediction uncertainty, and be more robust to adversarial
attacks (Hernández-Lobato & Adams, 2015; Li & Gal, 2017; Zhang et al., 2019). However, traditional
Bayesian models, particularly previous Bayesian sampling methods, typically focus on the offline case, where
revisiting past data is required.
In contrast, online statistics can be computed using only the recently arrived streaming data points. The
online Bayesian sampling issue is significant, driven by numerous application scenarios where data is collected
sequentially, necessitating the real-time update of the uncertainty-aware learning model. For instance, certain
types of data, such as Magnetic Resonance Imaging (MRI), are challenging to acquire, making a continuous
update scheme essential for problems like Bayesian matrix/tensor completion Zhang & Hawkins (2018).
Other scenarios include Bayesian on-device learning in uncertain environments with streaming sensor data
Servia-Rodriguez et al. (2021), where it is impractical to obtain the entire dataset at once. Nevertheless,
the learning procedure of Bayesian models is different from conventional models, which poses a challenge
in directly applying online optimization methods in an online setting. This is because a Bayesian model
is characterized by the distribution of its parameters but not a single value, and the learning task, a.k.a.
Bayesian inference, is to approximate the posterior distribution of the parameters given received data. A
tractable solution is Variational Inference (VI) (Jordan et al., 1999; Blundell et al., 2015), which approaches
1Under review as submission to TMLR
the posterior using a parameterized approximating distribution, which enables optimization methods again
(Hoffman et al., 2010a; Broderick et al., 2013a; Foti et al., 2014). However, the accuracy is restricted by the
expressiveness of the approximating distribution which is not systematically improvable.
A more accurate method is Monte Carlo which aims to draw samples from the posterior. As the posterior is
only known with an unnormalized density function, direct sampling is intractable, and Markov chain Monte
Carlo (MCMC) is employed. While it makes sampling tractable, it comes with the issue of sample efficiency
due to the correlation among the samples. Recently, a new class of Bayesian inference methods is developed,
known as particle-based variational inference (ParVI) (Liu & Wang, 2016; Chen et al., 2018; Liu et al., 2019;
Zhu et al., 2020; Zhang et al., 2020; Korba et al., 2021; Liu & Zhu, 2022). They try to approximate the
posterior using a set of particles (i.e., samples) of a given size, which are iteratively updated to minimize
the difference between the particle distribution from the posterior. The accuracy of the method can be
systematically improved with more particles, and due to the limited number of particles, sample efficiency is
enforced so as to minimize the difference. The theoretical nature, under the offline case, has been widely
studied over the Wasserstein space, where the Wasserstein gradient flow has emerged as a promising approach
to solve optimization problems over the space of probability distributions. While ParVI methods have been
successfully applied to the full-batch and mini-batch settings, to our knowledge there is no online version of
ParVI.
In this work, we develop an Online Particle-based Variational Inference (OPVI) method to meet this
desideratum and also provide an analysis on its regret bound which can achieve a sublinear order in the
number of iterations. The method and analysis are inspired by the distribution optimization view of ParVI
on the Wasserstein space, under which we could leverage techniques and theory of conventional online
optimization methods. To do this, we first extend existing Maximum a Posterior (MAP) methods to handle
the prior term better and then extend the results to the Wasserstein space as an online sampling method.
Generally, our contribution can be summarized as follows:
•We propose the new online sampling methods by employing an increasing batch-size scheme and
better prior-term settings that improve upon naively applying existing ParVI methods in an online
setting.
•We give the theoretical analysis for the OPVI method, which is the first theoretical analysis for ParVI
from the perspective of dynamic regret by leveraging the Riemannian structure of the Wasserstein
space.
•We give a detailed analysis of the gradient error led by stochastic gradient descent methods, which is
important for the type of ParVI methods with injected diffusion noise.
•We conduct empirical tests for the OPVI method. The results suggest much better posterior
approximation and classification accuracy than naive online ParVI methods and other MCMC
methods.
2 Related Work
Since (Cesa-Bianchi & Lugosi, 2006) study the online properties of VI, there are a couple of works showing
online VI gives good performance in practice cases (Hoffman et al., 2010b; 2013; Broderick et al., 2013b).
Furthermore, researchers in (Chérief-Abdellatif et al., 2019) derive the theoretical results for the generalization
properties of the Online VI algorithm. Even though online VI is well studied, few papers pay attention to
the problem of online MCMC. A potential alternative to online MCMC could be Sequential Monte Carlo
(SMC) methods (Chopin, 2002; Kantas et al., 2009; Christensen et al., 2012), which combine importance
sampling with Monte Carlo schemes to track the changing distribution. However, SMC does not have the
Markov chain properties of MCMC, such as convergence to a stationary distribution, thus, impossible to
obtain theoretical guarantees of asymptotic convergence to the true posterior distribution. In general, no
previous work considers an online MCMC method from the perspective of optimization methods, which
provides a possible way to find the theory behind the convergence.
2Under review as submission to TMLR
Our method employs a gradient descent-based optimization strategy to update particles toward the target
posterior. However, the target posterior is dynamically changing with streaming data arriving in the system,
which makes the optimal solutions change. To solve this problem, we consider a performance metric called
dynamic regret in our analysis. To achieve a sublinear dynamic regret, researchers propose different constraints
on the sequence of loss functions, like the gradient variation (Rakhlin & Sridharan, 2013), and path variation
(Yang et al., 2016). However, even though this dynamic problem is essential to be considered in the analysis of
Bayesian inference algorithms, no previous papers considered this. As a result, existing theoretical guarantees
regarding the online VI (e.g. (Chérief-Abdellatif et al., 2019)) may be insufficient under the dynamic changing
online environment.
The stochastic gradient descent algorithm is widely used as an incremental gradient algorithm that offers
inexpensive iterations by approximating the gradient with a mini-batch of observations. Through the past
decade, it has been used in a wide variety of problems with different variations, like network optimization
(Pang et al., 2022; Zhou et al., 2022) reinforcement learning (Liu et al., 2021b;a), federated learning (Sun &
Wei, 2022) and recommendation system (Yang et al., 2020). However, this method, at the same time, incurs
gradient error when approximating the gradient. In most of the novel sampling methods, we normally obtain
diverse solutions by injecting diffusion noises (e.g. Langevin Dynamic (LD) (Neal et al., 2011), Stochastic
Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011), which makes this type of algorithm sensitive to
the noise. This instability makes reducing the stochastic gradient error important.
To reduce the gradient error, researchers studied multiple variance reduction methods, like using adaptive
learning rates and increasing batch size. In the previous work, an adaptive learning rate was used to adapt the
optimization to the most informative features with Adagrad (Ward et al., 2019) and estimate the momentum
for Adam(Kingma & Ba, 2014). Compared with the adaptive methods, the increasing batch size methods
have greater parallelism and shorter training times (Smith et al., 2017) and are also studied in offline and
online cases (Friedlander & Schmidt, 2012; Zhou et al., 2018), which shows great importance in achieving
applicable convergence rate and sublinear regret bound.
3 The online Maximum a Posterior on Euclidean Space W
In this section, we first introduce an online MAP algorithm on Euclidean decision space Wwith gradient
descent method, which helps the reader to understand our OPVI sampling method on Wasserstein space.
Here, we give some prior knowledge about the online MAP problem and the dynamic regret metric. Then, we
give a detailed policy using an online stochastic gradient descent algorithm to solve the online MAP problem
and a detailed theoretical analysis based on the dynamic regret metric.
3.1 Preliminaries
For an online MAP algorithm run with time slots t∈[1,T], letW∈Rddenote a convex set, set wt∈W
be some parameter of interest and NT={B1,···,Bt,···,BT}be the set of i.i.d. observations, where
Bt={dk,dNt}is a batch of Ntdata samples arrived at time t. In a typical problem of MAP, we aim to
maximize a target posterior p(w) :=p0(w)/producttextNT
k=1p(dk|w), where we usually take logarithm on both sides to
simplify the computation as logp(w) = logp0(w) +/summationtext
klogp(dk|w).
Different from the offline MAP, we set a ηt=6
π2t2adaptive weight for the prior in our online setting, which
divides the whole prior for each update with/summationtextT
t=1ηt= 1whenT→∞. Then, the goal of the online MAP
problem onWis to find parameter wtthat maximizes the cumulative of a linear combination of minus
likelihood and partial prior, which can be given as:
max
wt∈WT/summationdisplay
t=1(/summationdisplay
dk∈Btlogp(dk|wt) +ηtlogp0(wt)), (1)
To simplify the notation, we use ck
t(wt) :=−logp(dk|w)to denote the log-likelihood with data dk
andc0(wt) :=−logp0(wt)to denote the log-prior, where cis called the cost function in the literature of
optimization and we take minus logarithm since we want to make sure the cost function to be positive all the
3Under review as submission to TMLR
time. We denote ct(wt) =/summationtextNT
k=1ck
t(wt)as the true likelihood considering all data in the dataset. Then, we
can formulate the goal eq. (1) to be an optimization problem with ct+ηtc0as the objective function and
follow the goal of:
min
wt∈WT/summationdisplay
t=1ct(wt) +ηtc0(wt)
As we have mentioned in Section 2, the target posterior is dynamically changing with the new observations,
we are interested in using dynamic regret as the performance metric for our problem, which is defined as
the difference between the total cost incurred at each time slot and a sequence of optimal solutions {w∗
t}in
hindsight, i.e.,
R(T) =T/summationdisplay
t=1ct(wt)+ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t). (2)
3.2 Dynamic Algorithm for Online Maximum a Posterior
It is well known that the online gradient descent algorithm can be used to solve online optimization problems
(Zinkevich, 2003; Besbes et al., 2015; Yang et al., 2022). Here, we give an online stochastic gradient descent
algorithm for the online MAP problem in the following updating policy:
wt=/braceleftigg
w1∈W t= 1
ΠW[wt−1−αvt(wt−1)]t>1, (3)
wherevt(wt−1) =∇ct−1(wt−1) +ηt∇c0(wt−1)
where ΠWis the projection back to the convex set W. projection, need an additional inequality in proof
Next, we first introduce some widely used assumptions required for the theoretical analysis.
Assumption 1. (Bounded Convex Set) For a convex decision set Wand any two decisions w1,w2∈W, we
haved(w1,w2)≤R.
Assumption 2. (Convexity and Lipschitz smooth) The function ct+ηtc0is convex and Lipschitz smooth,
so its derivatives are Lipschitz continuous with constant Lwith a constant L, i.e., for two real w1,w2∈W,
we have:
∥∇ct(w1) +ηt∇c0(w1)−∇ct(w2)−ηt∇c0(w2)∥≤L∥w1−w2∥t∈[1,T].
Assumption 3. (Vanishing gradient) We assume the optimal solutions w∗
tlie in the interior of the convex
setW, where we assume there exists w∗
tsuch that∇ct(w∗
t) +ηt∇c0(w∗
t) = 0
We give a sublinear regret upper bound in the next subsection, which means ∥wt−w∗
t∥is decreasing and the
parameter of interest wtcan converge to the dynamic changing optimal solutions w∗
twhenTis large enough.
That indicates we can obtain a promising MAP result with the policy in eq. (3).
3.3 Theoretical Analysis for Online MAP
In this section, we begin with the proof of the online MAP algorithm following the policy in eq. (3) over
the Euclidean space W. As we mentioned in Section 2, it is impossible to achieve a sublinear regret bound
for any sequence of cost functions. To solve this problem, we consider a path variation budget VTfor the
sequence of optimal solutions {w∗
t}, which bound the cumulative path length of the optimal solutions as
VT:=/summationtextT
t=1∥w∗
t−w∗
t−1∥. The result is summarized in Theorem 4, which gives the sublinear bound for the
dynamic regretR(T)when we set a sublinear path variation VT.
Theorem 4. Under the Assumption 1 - 3, given a sequence of optimal solutions {w∗
t}, variational budget
VT, following the updating policy in eq. (3) on Euclidean Space W∈Rn, we have the dynamic regret:
E[R(T)]≤R2
ξ+RVT
ξ,
4Under review as submission to TMLR
whereξ= 2α−2Lα2
Proof.Detail of the proof can be found in Appendix A.
The theorem presents the regret bound for the online MAP problem within the Euclidean space, where the
bound primarily depends on the variational budget VT. SettingVTas a sublinearly increasing term allows the
regret bound to increase sublinearly with respect to T. Next, we will introduce the proposed OPVI method
and extend the proof of online MAP over the Wasserstein space.
4 Online Particle-based Variational Inference on Wasserstein Space P2(W)
In this section, we propose the OPVI algorithm on P2(W), which formulate the online MAP problem
in Section 3 as an online sampling method from the perspective of Wasserstein gradient flow. To begin
with, we first introduce some preliminary knowledge about the 2-Wasserstein Space P2(W), as well as its
Riemannian structure and the gradient flow on it. Then, we give a brief introduction to a well-known ParVI
method, called SVGD (Liu & Wang, 2016) and take it as an example to illustrate how to simulate a ParVI
problem as a gradient flow on P2(W). Based on this idea, we give the theoretical analysis for OPVI as
a distribution optimization flow on P2(W)to show a sublinear dynamic regret. For convenience, we only
consider Wasserstein Space supported on the Euclidean space Win our analysis.
Here, we first clarify the notation used in this section. We use C∞
cas a set of compactly supported RD−valued
functions onWand useC∞
cto denote the scalar-valued functions in C∞
c. Except for the Euclidean space
Wand Wasserstein space P2(W)we just mentioned, we consider two other types of space in this paper,
the Hilbert space L2
qand the vector-valued Reproducing Kernel Hilbert Space (RKHS) HDof a kernel K.
The Hilbert space L2
q, is a space of RD-valued functions/braceleftbig
u:RD→RD|/integraltext
∥u(w)∥2
2dq<∞/bracerightbig
with inner
product⟨u,v⟩L2q:=/integraltext
u(w)·v(w)dq. The RKHSHis a kernel version of the Hilbert space L2
q, which is the
closure of linear span {f:f(w) =/summationtextm
i=1aiK(w,wi),ai∈R,m∈N,wi∈W}equipped with inner products
⟨f,g⟩L2q=/summationtext
ijaibjK(wi,wj)forg(w) =/summationtext
ibiK(w,wi).
4.1 The Wasserstein Space P2(W), its Riemannian Structure and the Gradient Flow
Generally, the Wasserstein space is a metric space equipped with Wasserstein distance d(·,·). SetP(W)as
the space of probability measures on the Euclidean support space W. The 2-Wasserstein space on Wcan
be defined asP2(W) :={µ∈P(W) :/integraltext
W∥w∥2dµ(w)<∞}. Since the Riemannian structure of Wasserstein
space is discovered (Otto, 2001; Benamou & Brenier, 2000), several interesting quantities have been defined,
like the gradient and the inner product on it.
To define the gradient of a smooth curve (qt)tonP2(W), we can set a time-dependent vector field vt(w)
onW, such that for a.e. t∈R,∂tqt+∇·(vtqt)= 0andvt∈{∇φ:φ∈C∞c}L2
qt, where the overline means
closure (Villani, 2009). Note that the vector field vthere is the so-called tangent vector of the curve (qt)tatqt
and the closure is denoted as tangent space TqtP2atqt, whose elements are the tangent vectors for the curves
passing through the point qt. The relation between TqtP2,vtandP2(W)can be found in Fig. 1. The inner
product in the tangent space TqtP2is defined onL2
q, which defines the Riemannian structure on P2(W)and
is consistent with the Wasserstein distance due to the Benamou-Brenier formula (Benamou & Brenier, 2000).
An important role of the vector field representation is that we can approximate the change of distribution qt
within a distribution curve (qt)t. For a single update in each time slot, we can set (id+εvt)#qtas a first-order
approximation of the updated distribution qt+1in the next time slot (Ambrosio et al., 2005). Therefore, for a
set of particles{x(i)
t}ithat obey distribution qtat timet, we can update these particles with a stepsize of εas
{x(i)
t+εvt(x(i)
t)}i, to approximate distribution qt+1in timet+1, whenεis small. We show this approximation
as a red arrow in Fig. 1. In the task of Bayesian inference, our goal is to minimize the KL-divergence between
a current estimated distribution qtand the target posterior pasKLp(qt) :=/integraltext
Wlog(qt|p)dqt, which has the
tangent vector for its gradient flow (qt)tas a vector field of:
vt=−∇qtKLp(qt) =∇logp−∇logqt,
5Under review as submission to TMLR
approx.
Figure 1: Illustration for the updating of qtover the gradient flow (qt)tonP2(W), and the relationship
between the update of particles {x(i)
t}overHand the update of distribution qtoverP2(W).
4.2 Particle-based Variational Inference Methods
In this section, we first use SVGD as an example to illustrate the ParVI methods. Then we show how to
simulate SVGD as the gradient flow on Wasserstein space P2(W), which can help the analysis of OPVI in
the following subsection. For SVGD, let {x(i)
t}n
i=1be a set of particles that obey an empirical measure of
distribution qt. We initialize qtas some simple distribution q0, then use a vector field vto update these
particles toward the target posterior p:x(i)
t+1=x(i)
t+εv(x(i)
t), wherevshould be chosen to maximize the
decreasing of the KL-divergence −d
dεKLp((id +εv)#q)/vextendsingle/vextendsingle
ε=0. In SVGD, the vector field is chosen to be
optimized over RKHS Hwith a closed-form solution:
vSVGD
H (·) :=∇logp(x)k(x,·) +∇k(x,·) (4)
Note that the updating of SVGD particles is actually an approximation of the P2(W)gradient flow by taking
Has its tangent space instead of L2
qt, since the function in His roughly a kernel smoothed function in L2
qt
(Liu & Zhang, 2019). Thus, the vector field vSVGD
Hin eq. (4) can be used to approximate the vector field
vSVGD
L2qtinL2
qtonP2(W)(Liu et al., 2019, Theorem 2), where the solution gives:
vSVGD
H = max arg max
v∈H,∥v∥H=1⟨vSVGD
L2qt,v⟩L2qt(5)
That enables us to use vSVGD
L2
qtto approximate the vector field vSVGD
HonP2(W)in the following analysis,
which like doing a projection from HtoL2
qt.
4.3 Online Particle-based Variational Inference on P2(W)
In this section, we aim to develop an online sampling method on P2(W)and proposed the OPVI algorithm.
We first illustrate the policy of OPVI over RKHS H. Then, we interpret the OPVI as the gradient flow
onP2(W)and conduct the theoretical analysis by transferring the proof in 3 from Euclidean space Wto
Wasserstein space P2(W). Note that we use vOPVI-H
tas the vector field on RKHS HandvOPVI-L2
tas the
vector field onL2
qt.
We begin with reviewing the minus KL-divergence in an offline setting, which is given as:
−KLp(qt) =Eqt[logp]−Eqt[logqt] =NT/summationdisplay
k=1Eqt[logp(dk|·)] +Eqt[logp0]−Eqt[logqt],
whereNTis the number of data samples in the dataset. Following a similar idea as the online MAP algorithm
we introduced in section 3, we set a ηt=6
π2t2adaptive weight for the prior in our online setting and using
mini-batch with batch size Btto approximate the likelihood.
6Under review as submission to TMLR
However, the approximation leads to a gradient error between the true gradient ∇logpand the estimated
gradient∇ˆlogp, which gives et:=∇ˆlogp−∇logp, which can be calculated by:
et=1
Bt/summationdisplay
k∈Bt∇logp(dk|·)−∇logp, (6)
whereBt=|Bt|is the batch size. Also, we define the error leads by the wrongly added prior term as ht,
which can be defined as ht:=ˆ∇logp−∇logp. For convenience in proof, we define the ϵtto represent the
combination noise of etandht, which is given as ϵt:=et+ηtht. Note that the gradient error etcan be
deterministic or stochastic, depending on the way we set up the mini-batch. In this paper, we choose to select
samples for mini-batch Btarbitrarily fromNT, which makes the gradient error stochastic. We introduce an
error bound ETto measure the cumulative gradient error lead by the stochastic gradient approximation over
t∈[1,T], which is given by:
ET:=T/summationdisplay
t=1ϵt (7)
We will show a sublinear increasing batch size is enough to keep ETgrowing sublinear, which enables the
online MAP algorithm to enjoy a sublinear dynamic regret.
Thus, we give an online stochastic version of minus KL-divergence between qtand the dynamic changing
posteriorptas:
−O-KLpt(qt) =Bt/summationdisplay
k=1Eqt[logp(dk|·)] +ηtEqt[logp0]−Eqt[logqt] (8)
Similar to SVGD, we first draw a set of particles {x(i)
0}n
i=1that obey some simple initial distribution q0.
Then, we update these particles with a gradient descent updating scheme with step size α:
x(i)
t+1=x(i)
t+αvOPVI-H
t,
wherevOPVI-H
tis the vector field on Hthat maximizes the decrease of online stochastic KL-divergence
−d
dαO-KLpt((id+αvt)#q)|α=0to give a closed-form solution:
vOPVI-H
t (·) =Eq(x)[K(x,·)∇Bt/summationdisplay
k=1logp(dk|x) +ηtK(x,·)p0(x) +∇K(x,·)],
whereK(x,x′)is satisfied by commonly used kernels like the exponential kernel K(x,x′) =exp(−1
h∥x−
x′∥2
2)and the general workflow of the OPVI algorithm is summarized in Alg. 1.
4.4 Proof of Dynamic Regret Bound under P2(W)
To begin with, we first formulate the updating rule in Alg. 1 as a Wasserstein gradient flow. Here, we
ignore the kernel smooth used in the implementation of the algorithm by approximating the vector field
vOPVI-H
ton RKHSHwith the vector field vOPVI-L2
ton Hilbert space L2. To simplify the proof, we denote
ck
t(qt) =−Eqt[logp(dk|·)]andc0
t(qt) =−ηtEqt[logp0] +Eqt[logqt]in eq. (8) and follow eq. (6) to represent
the stochastic approximation as the sum of the true gradient and a gradient error et, which gives:
vOPVI-L2
t (qt) =−(ct(qt) +et+c0
t(qt))
Then, the updating of the particles can be formulated as an optimal transport for distribution qtoverP2(W)
as:
qt+1= Expqt(−α(ct(qt) +et+c0
t(qt))) (9)
Before we give the proof for the regret bound, we first re-assume some assumption under the P2(W).
7Under review as submission to TMLR
Algorithm 1 Online Particle-based Variational Inference
Initialize particles {x(i)
0}N
i=1
fort= 1,···,Tdo
x(i)
t+1=x(i)
t+αvOPVI-H
t (x(i)
t)
where:
vOPVI-H
t (x(i)
t) =Eq(x)[K(x,x(i)
t)∇Bt/summationdisplay
k=1logp(dk|x) +ηtK(x,x(i)
t)∇logp0(x) +∇K(x,x(i)
t)]
end for
Assumption 5. (Bounded geodesically-convex set) Assume Kto be a g-convex set on some Wasserstein
spaceP2(W)supported onW. From Theorem 2 of (Gibbs & Su, 2002), we can establish a bound for the
maximum Wasserstein distance in a bounded support space with dim(W)<R. Then∀q1,q2∈P 2(W), we
have:
dK(q1,q2)≤1 +R
which bound the geodescially convex set K.
Assumption 6. (Geodesically-L-Lipschitz (g-L-Lipschitz)). Similar to the definition over W, we assume
ct(q1) +c0
t(q1)to be a g-convex function and has a geodesically L-Lipschitz continuous gradient on P2Wif
there exists a constant L>0that:
|∇ct(q1) +ηt∇c0
t(q1)−∇ct(q2)−ηt∇c0
t(q1)|≤L·d(q1,q2),∀q1,q2∈P 2(W),
whered(a,b)should be some Wasserstein distance.
Compared with the proof on W, the key difference is the way to obtain Lemma 9. Instead of updating a set
of parameters of interest over W, we update the distribution qtby optimal transport over P2(W).
Lemma 7. Suppose thatP2(W)is a Wasserstein space supported on Euclidean space Wwith the sectional
curvature lower bounded by −κ(κ>0). Under Assumption 3, 5, 6, for any qt∈K, following the updating
rule in eq. (9), we have:
E[d(qt+1,q∗
t)]≤E[d(qt,q∗
t)]−Φ
RE[(ct(qt) +ηtc0
t(qt)−ct(q∗
t)−ηtc0
t(q∗
t))] +/radicalig
2αϵ2
tζ(κ,R) + 2αϵtR,
where Φ = 2α−3Lα2ζ(κ,R).
Proof.We start from a fact proved in Lemma 6 of (Zhang & Sra, 2016), which gives an inequality for a
geodesic triangle with curvature bounded by κ, where the length of sides for the triangle is a,b,candAis
the angle between sides bandc, then:
a2≤/radicalbig
|κ|c
tanh(/radicalbig
|κ|c)b2+c2−2bccos(A),
In our work, we map our problem on a triangle, where the vertices of this triangle is set to be three status
of the decisions in our problem, the current step decision qt, the next step decision qt+1and the optimal
solution in current step q∗
t. Denoted(a,b)to be the Wasserstein distance between two distribution aand
boverP2(W). As a result, the three sides of the triangle should be a=d(qt+1,q∗
t),b=d(qt,qt+1)and
c=d(qt,q∗
t). Base on the updating rule, we have d(qt,qt+1) =α∥∇ct(qt) +et+∇c0
t(qt)∥whenαis small
enough and d(qt,qt+1)d(qt,q∗
t) cos(∠qt+1qtq∗
t) =⟨−α(∇ct(qt) +et+∇c0
t(qt)),Exp−1
qt(q∗
t)⟩.
The detailed proof can be found in Appendix B
8Under review as submission to TMLR
Using Lemma 7 and the definition of dynamic regret in eq. (2), we give the dynamic regret bound on P2(W)
in the following Theorem.
Theorem 8. (Regret Bound over P2(W)) Under the Assumption 3, 5, 6, given a sequence of optimal solutions
{q∗
t}, define the variational budget VT:=/summationtextT
t=1d(q∗
t,q∗
t+1)and the error bound ET. Following the updating
rule in eq. (9), we have the dynamic regret bound:
RP2(W)≤O(max(1,ET,VT)) (10)
Proof.The proof start from Lemma 7 with using the triangle inequality, which gives:
E[d(qt+1,q∗
t)]≤E[d(qt+1,q∗
t)] +E[d(q∗
t+1,q∗
t)]
≤E[d(qt,q∗
t)]−Φ
RE[(ct(qt) +ηtc0
t(qt)−ct(q∗
t)−ηtc0
t(q∗
t))] +/radicalig
2αϵ2
tζ(κ,R) + 2αϵtR+E[d(q∗
t+1,q∗
t)]
Rearrange the inequity, taking summation from t∈[1,T], evolving the definition of the regret, we have:
E[RP(W)(T)]≤R
ΦT/summationdisplay
t=1E[d(qt,q∗
t)−d(qt+1,q∗
t)]+
+R
ΦT/summationdisplay
t=1/radicalig
2αϵ2
tζ(κ,R) + 2αϵtR+R
ΦtT/summationdisplay
t=1∥q∗
t+1−q∗
t∥
≤R
Φ(d(q1,q∗
1)−d(qT+1,q∗
t)) +R
ΦT/summationdisplay
t=1/radicalig
2αϵ2
tζ(κ,R) + 2αϵtR+R
ΦtVT
where the second term can be simplified as:
T/summationdisplay
t=1/radicalig
2α2ϵ2
t+ 2αϵtR
≤T/summationdisplay
t=1/radicalig
2α2ϵ2
t+/radicalbig
2αϵtR
≤√
2αT/summationdisplay
t=1ϵt+√
2αRT/summationdisplay
t=1√ϵt
≤√
2αET+√
2αR/radicaltp/radicalvertex/radicalvertex/radicalbtTT/summationdisplay
t=1ϵt
≤√
2αζ(κ,R)ET+/radicalbig
2αRTET
Then, we finally prove the regret bound as:
E[RP(W)(T)]≤R2
ξ+R
ξ(√
2αζ(κ,R)ET+/radicalbig
2αRTET) +R
ΦtVT
≤O(max(1,ET,VT))
Different from the proof of the inexact gradient descent on Euclidean space, we include the trigonometric
distance inequality introduced in (Zhang et al., 2016) and give the first dynamic regret bound for the inexact
infinitesimal gradient descent methods over P2(W). Note the regret bound here is related to a curvature
boundκ, where we set κas a constant since it is not the key point of this paper.
9Under review as submission to TMLR
Since the gradient error is denied in RD, we can follow the same analysis as Section 3.3 to bound the gradient
error bound ET, which gives a sublinear error bound. As a result, by setting a sublinear increasing constraint
for the variational budget VT, we can make sure RP2(W)(T)is increasing sublinear. That means the OPVI
methods can converge to the dynamic changing target posterior ptwhenTis large enough.
In SVGD, the author didn’t consider this gradient error in their algorithm. However, since the gradient error
can be viewed as a part of noise added into the updating process, we should not use the whole diffusion noise
∇K(x,·)in eq. (4). In the experiment, we set the diffusion term as 0.1·∇K(x,·)for OPVI. We observe
that this trick gives tremendous improvements in performance, especially in some high-dimensional tasks like
image classification.
To further find the relationship between ETandBtto boundET, we give some analysis for the gradient
error led by the stochastic batch sampling with the sublinear increasing batch size following Theorem 4. Base
on section 2.8 in (Lohr, 2021), we have:
E[∥et∥2] =NT−Bt
NTBtΛ2, (11)
whereNTis the total number of data samples we have and Λis a bound on the sample variance of the
gradients, which is defined by:
1
NT−1NT/summationdisplay
i=1/vextenddouble/vextenddouble∇ci
t(w)−∇ct(w)/vextenddouble/vextenddouble2≤Λ2w∈W
To fulfill the requirement of E[∥et∥2]in eq. (11), we assume ϵt=/radicalig
1
Bt−1
NTand the sublinear increasing
batch-size as Bt=NTtρ
NT+tρρ>0. Then, we can bound ETas:
ET=T/summationdisplay
t=1ϵt≤T/summationdisplay
t=1(et+ηtht)≤T/summationdisplay
t=1/radicalbigg
1
tq+T/summationdisplay
t=1ηtht (12)
≤T/summationdisplay
t=1/radicalbigg
1
tq+T/summationdisplay
t=1ηt(wt−w∗)≤2
2−ρT1−ρ
2+RT/summationdisplay
t=1∥6
π2t2∥ (13)
Here, the noise contains two parts. The first part of the noise comes from the mini-batch sampling. We
can see when the batch size Btis growing sublinear, the gradient error bound ETis sublinear. Thus if the
variational budget VTis constrained to be sublinear, the regret bound is proved to be sublinear. Note that in
the regret analysis, we set a static stepsize αfor convenience. The algorithm can also achieve a sublinear
regret bound when the stepsize is set to be digressive like αt=t0.55. The second part of the noise comes
from the wrongly added prior term. We can observe that the shrinking factor for the prior term is necessary
for the convergence of the noise leads by the gradually accumulated prior term error. Next, we illustrate why
a static batch size fills to achieve a sublinear regret.
Remark: Set the batch size to be static as B. The agent update wtfor overTrounds and use a total of NT
data samples, where NTcan be calculated by NT=/summationtextT
t=1B=BT. Following a similar setting in eq. (12),
we bound the gradient error over t∈[1,T]as:
ET=T/summationdisplay
t=1ϵt=T/summationdisplay
t=1/radicalbigg
1
B−1
NT≤T/summationdisplay
t=1/radicalbigg
1
B(1−1
T)≤O(T)
which gives a linear increasing gradient error bound ET≤O(T). That makes it impossible to give a sublinear
regret bound, which is necessary to ensure the algorithm can finally converge to the optimal solutions.
5 Experiments
In this section, we test the performance of the proposed OPVI algorithm, and compare it with two famous
Bayesian sampling methods, the LD (Welling & Teh, 2011) and SVGD (Liu & Wang, 2016). We run these
10Under review as submission to TMLR
T ext T extm = (0.49, 0.01)
var = 1.96m = (0.35, 0.02)
var = 2.03m = (0.44, 0.12)
var = 2.43
m = (0.57, 0.03)
var = 1.83m = (0.49, 0)
var = 1.85m = (0.36, -0.05)
var = 1.86m = (0.40, 0.05)
var = 2.34
m = (0.53, 0.10)
var = 2.54
Figure 2: Synthetic experiments for different methods. All methods run 500 rounds. Except for the full batch
methods (which use much more data samples), other methods use the same number of data samples. The
quantitative mean and variance with respect to the distribution parameters in eq. (14) are shown on the
figures.
Table 1: Results on a BNN classification task on the Kin8nm dataset, averaged over 20 tries.
Methods Avg. RMSE Avg. LL Time
OPVIBt=t0.55.127±.008.653±.0602.4
OPVIB= 20.145±.003.516±.0212.4
SVGDB= 20.144±.003.525±.0192.4
SVGDB= 10k.112±.002.783±.0175.8
LDB= 20 .159±.004.425±.0241.7
LDB= 10k.143±.002.527±.0155.2
methods with three types of batch settings, mini-batch with increasing batch size, mini-batch with static
batch size, and full batch. To make the comparison fair, we set a Fixed Iterations and Total Data Samples
(FITDS) policy for experiments under the mini-batch setting, which means we set the total number of data
samplesNTand the total number of time slots Tto be same for each experiment.
Except for the full-batch methods, all algorithms follow the FITDS policy. For a dataset of nearly 10k data
samples, we run all methods for 500 rounds and set B= 20for the static batch size methods and Bt=t0.55for
the increasing batch size methods to keep NTsame. We choose Bt=t0.55as the schedule for the increasing
batch-size method since it fulfills the requirement of sub-linear regret in Theorem 8 and test to be the best
setting for our method. For full batch methods, we use all 10k data samples in each round to show the best
possible results. All experiments are run under the same setting (unless otherwise stated), codes for these
experiments are available at https://github.com/AnonymousSubmission100/OPVI .
5.1 Synthetic Experiments
The synthetic experiments follow the setting in (Welling & Teh, 2011) that conduct a simple example with
two parameters, based on the mixture Gaussian distribution:
(θ1,θ2)∼N/parenleftbig
(0,0),diag/parenleftbig
σ2
1,σ2
2/parenrightbig/parenrightbig
(14)
xi∼0.5·N/parenleftbig
θ1,σ2
x/parenrightbig
+ 0.5·N/parenleftbig
θ1+θ2,σ2
x/parenrightbig
,
whereσ2
1= 10,σ2
2= 1andσx= 2. Here, we draw approximately 10,000 data samples from the above
distribution with θ1= 0andθ2= 1. Except for the full-batch methods, all algorithms follow the FITDS
11Under review as submission to TMLR
Figure 3: Learning curve of the image classification task. We can observe that the proposed OPVI method
outperforms most mini-batch Bayesian sampling methods at the end of training steps
policy. Fig. 2 shows the results for the OPVI, SVGD, and LD with 100 particles, where the true posteriors
are shown as contour and the inference results are represented by the particles.
As we can observe from the result, the proposed increasing batch size OPVI gives a better result than the
static batch size OPVI, which is caused by the use of increasing batch size as a variance reduction method.
Compared with previous SVGD and LD, the OPVI method shows much better performance for tracking the
posterior. That should be led by the influence of the gradient noise on the noise injection process of the LD
method since we use a smaller diffusion term to offset the gradient error. In the last two figures, we can see
the performance of OPVI is approaching or even better than the full batch methods. Also, we can observe
from panels (d) and (h) in Fig. 2 that our proposed OVPI method outperforms both the SVGD and LD
methods as the batch size increases. This indicates that our innovative design of the prior and the kernel
shrinking schedule enhances the performance of the ParVI method in an online setup.
5.2 Bayesian Neural Network (BNN) Experiment
In this subsection, we further compare our work with SVGD and LD on some Bayesian Neural Networks
(BNN) tasks. We follow the experiment setting in (Liu & Tao, 2015), which uses a single hidden layer BNN
with 50 hidden units. We use a Gamma(1, 0.1) function in the prior distribution, Kin8nm as the dataset and
divide the dataset randomly 90% for training and 10% for testing. For all methods, we set the number of
particles to 20.
All ParVI methods use the same stepsize, except for LD, which uses a smaller but best possible stepsize. We
test the Root Mean Squared Error (RMSE) and the test Log-Likelihood (LL). The experiment results are
shown in Table. 1. The OPVI algorithm can achieve an 11.8% and 20.1% improvement compared with
SVGD and LD with the same total number of data NTand the same total time slots Trespectively. This
result is even comparable to the full batch SVGD algorithm. Note that the running time for OPVI is the
same as the SVGD algorithm, which is less than half of the full batch methods.
5.3 Image classification Task
Finally, we conduct experiments to test the performance of the proposed algorithm on a high-dimensional
image classification problem. The dataset we used is the MNIST dataset, which contains 60,000 training
cases and 10,000 test cases. We consider a two-layer BNN model with 100 hidden variables, with a sigmoid
12Under review as submission to TMLR
input layer and a softmax output layer. All experiments are using 20 particles. The comparison result is
shown in Fig. 3. As we can see from the figure, except for the full batch LD algorithm, the OPVI algorithm
with an increasing batch size achieves the best result. However, the full batch LD method uses much more
time (30 times) and data samples (500 times), and the result is similar. We can observe that the noise of the
increasing batch size OPVI is decreasing with tincrease, which verifies our analysis for the gradient error.
An interesting thing is that SVGD shows poor performance in this high-dimensional task, which may lead by
an incorrect approximation for the diffusion term with limited particle numbers. Instead, we improve the
diffusion term in OPVI, which solves this problem.
6 Conclusion
In this paper, we consider the OPVI algorithm as a possible sampling method for the intractable posterior
under the online setting. The proposed algorithm is the first algorithm to think about the online optimization
algorithm from perspective of bayesian sampling and give the theoretical proof to understand the dynamics
from the perspective of Wasserstein gradient flow. To reduce the variance, we include an increasing batch size
scheme and analyze the influence of the choice of batch size on the performance of the algorithm. Furthermore,
we develop a detailed analysis by understanding the algorithm as a Wasserstein gradient flow. Experiments
show the proposed algorithm outperforms other naive online particle-based VI and online MCMC methods.
References
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces and in the space of
probability measures . Springer Science & Business Media, 2005.
Amrit Singh Bedi, Paban Sarma, and Ketan Rajawat. Tracking Moving Agents via Inexact Online Gradient
Descent Algorithm. IEEE Journal of Selected Topics in Signal Processing , 12(1):202–217, February 2018.
Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the monge-kantorovich
mass transfer problem. Numerische Mathematik , 84(3):375–393, 2000.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary Stochastic Optimization. pp. 52, 2015.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
network. In International conference on machine learning , pp. 1613–1622. PMLR, 2015.
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan. Streaming
variational Bayes. Advances in neural information processing systems , 26, 2013a.
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan. Streaming
Variational Bayes. In Advances in Neural Information Processing Systems , volume 26. Curran Associates,
Inc., 2013b.
Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games . Cambridge university press, 2006.
Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A Unified Particle-Optimization
Framework for Scalable Bayesian Sampling. arXiv:1805.11659 [cs, stat] , July 2018. arXiv: 1805.11659.
Badr-Eddine Chérief-Abdellatif, Pierre Alquier, and Mohammad Emtiyaz Khan. A generalization bound for
online variational inference. In Asian conference on machine learning , pp. 662–677. PMLR, 2019.
Nicolas Chopin. A sequential particle filter method for static models. Biometrika , 89(3):539–552, 2002.
Hugh L. Christensen, James Murphy, and Simon J. Godsill. Forecasting high-frequency futures returns using
online langevin dynamics. IEEE Journal of Selected Topics in Signal Processing , 6(4):366–380, 2012.
Nick Foti, Jason Xu, Dillon Laird, and Emily Fox. Stochastic variational inference for hidden Markov models.
Advances in neural information processing systems , 27, 2014.
13Under review as submission to TMLR
Michael P. Friedlander and Mark Schmidt. Hybrid Deterministic-Stochastic Methods for Data Fitting. SIAM
Journal on Scientific Computing , 34(3):A1380–A1405, January 2012. ISSN 1064-8275, 1095-7197.
Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International
statistical review , 70(3):419–435, 2002.
José Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of
Bayesian neural networks. In International conference on machine learning , pp. 1861–1869. PMLR, 2015.
Matthew Hoffman, Francis Bach, and David Blei. Online learning for latent dirichlet allocation. advances in
neural information processing systems , 23, 2010a.
Matthew Hoffman, Francis Bach, and David Blei. Online Learning for Latent Dirichlet Allocation. In
Advances in Neural Information Processing Systems , volume 23. Curran Associates, Inc., 2010b.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal
of Machine Learning Research , 2013.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine learning , 37(2):183–233, 1999.
Nicholas Kantas, Arnaud Doucet, Sumeetpal Sindhu Singh, and Jan Marian Maciejowski. An overview of
sequential monte carlo methods for parameter estimation in general state-space models. IFAC Proceedings
Volumes, 42(10):774–785, 2009.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A Non-Asymptotic Analysis for
Stein Variational Gradient Descent. arXiv:2006.09797 [cs, stat] , January 2021. arXiv: 2006.09797.
Yingzhen Li and Yarin Gal. Dropout inference in Bayesian neural networks with alpha-divergences. In
International conference on machine learning , pp. 2052–2061. PMLR, 2017.
Bo Liu, Xidong Feng, Jie Ren, Luo Mai, Rui Zhu, Haifeng Zhang, Jun Wang, and Yaodong Yang. A theoretical
understanding of gradient bias in meta-reinforcement learning. arXiv preprint arXiv:2112.15400 , 2021a.
Bo Liu, Xidong Feng, Haifeng Zhang, Jun Wang, and Yaodong Yang. Settling the bias and variance of
meta-gradient estimation for meta-reinforcement learning. arXiv e-prints , pp. arXiv–2112, 2021b.
Chang Liu and Jun Zhu. Geometry in sampling methods: A review on manifold MCMC and particle-based
variational inference methods. Handbook of Statistics. Elsevier, 2022. doi: https://doi.org/10.1016/bs.host.
2022.07.004.
Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, Jun Zhu, and Lawrence Carin. Understanding and
accelerating particle-based variational inference. In Proceedings of the 36th International Conference on
Machine Learning , pp. 4082–4092, 2019.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm.
Advances in neural information processing systems , 29, 2016.
TongliangLiuandDachengTao. Classificationwithnoisylabelsbyimportancereweighting. IEEE Transactions
on pattern analysis and machine intelligence , 38(3):447–461, 2015.
Ziming Liu and Zheng Zhang. Quantum-Inspired Hamiltonian Monte Carlo for Bayesian Sampling. pp. 38,
2019.
Sharon L Lohr. Sampling: design and analysis . Chapman and Hall/CRC, 2021.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo , 2(11):2,
2011.
14Under review as submission to TMLR
Felix Otto. The geometry of dissipative evolution equations: the porous medium equation. 2001.
Jinlong Pang, Ziyi Han, Ruiting Zhou, Haisheng Tan, and Yue Cao. Online scheduling algorithms for unbiased
distributed learning over wireless edge networks. Journal of Systems Architecture , 131:102673, 2022.
Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences.
Advances in Neural Information Processing Systems , 26, 2013.
Sandra Servia-Rodriguez, Cecilia Mascolo, and Young D Kwon. Knowing when we do not know: Bayesian
continual learning for sensing-based analysis tasks. arXiv preprint arXiv:2106.05872 , 2021.
Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning rate, increase
the batch size. arXiv preprint arXiv:1711.00489 , 2017.
Zhenyu Sun and Ermin Wei. A communication-efficient algorithm with linear convergence for federated
minimax learning. arXiv preprint arXiv:2206.01132 , 2022.
Cédric Villani. Optimal transport: old and new , volume 338. Springer, 2009.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex
landscapes. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proc. International Conference on
Machine Learning , volume 97, pp. 6677–6686, 09–15 Jun 2019.
Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics. pp. 8,
2011.
Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking Slowly Moving Clairvoyant: Optimal
Dynamic Regret of Online Learning with True and Noisy Gradient. pp. 9, 2016.
Yifan Yang, Jie Xu, Zichuan Xu, Pan Zhou, and Tie Qiu. Quantile context-aware social iot service big data
recommendation with d2d communication. IEEE Internet of Things Journal , 7(6):5533–5548, 2020.
Yifan Yang, Lin Chen, Pan Zhou, and Xiaofeng Ding. Vflh: A following-the-leader-history based algorithm
for adaptive online convex optimization with stochastic constraints. Available at SSRN 4040704 , 2022.
Hao Yu, Michael Neely, and Xiaohan Wei. Online convex optimization with stochastic constraints. Advances
in Neural Information Processing Systems , 30, 2017.
Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Conference on
Learning Theory , pp. 1617–1638. PMLR, 2016.
Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian SVRG: Fast Stochastic Optimization on
Riemannian Manifolds. pp. 9, 2016.
Jianyi Zhang, Yang Zhao, and Changyou Chen. Variance reduction in stochastic particle-optimization
sampling. In International Conference on Machine Learning , pp. 11307–11316. PMLR, 2020.
Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz Ustebay. Bayesian graph convolutional neural
networks for semi-supervised classification. In Proceedings of the AAAI conference on artificial intelligence ,
volume 33, pp. 5829–5836, 2019.
Zheng Zhang and Cole Hawkins. Variational bayesian inference for robust streaming tensor factorization and
completion. In 2018 IEEE International Conference on Data Mining (ICDM) , pp. 1446–1451. IEEE, 2018.
Pan Zhou, Xiaotong Yuan, and Jiashi Feng. New insight into hybrid stochastic gradient descent: Beyond
with-replacement sampling and convexity. Advances in Neural Information Processing Systems , 31, 2018.
Ruiting Zhou, Jinlong Pang, Qin Zhang, Chuan Wu, Lei Jiao, Yi Zhong, and Zongpeng Li. Online scheduling
algorithm for heterogeneous distributed machine learning jobs. IEEE Transactions on Cloud Computing ,
2022.
15Under review as submission to TMLR
Xingyu Zhou. On the fenchel duality between strong convexity and lipschitz continuous gradient. arXiv
preprint arXiv:1803.06573 , 2018.
Michael Zhu, Chang Liu, and Jun Zhu. Variance reduction and quasi-Newton for particle-based variational
inference. In Proceedings of the 37th International Conference on Machine Learning , pp. 11576–11587,
2020.
Martin Zinkevich. Online Convex Programming and Generalized Infinitesimal Gradient Ascent. pp. 28, 2003.
16Under review as submission to TMLR
A Proof of Theorem 4
The main idea of this proof follows (Bedi et al., 2018, Theorem 2). We start by providing a Lemma that gives
the relationship between the distance d(wt+1,w∗
t)and the quantity ct(wt) +ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t).
Then, we use this Lemma to prove the Theorem 4.
Lemma 9. Under Assumptions 1 - 3, given a sequence of optimal solutions {w∗
t}, gradient error E[et]≤ϵt
and the updating policy eq. (3), the online MAP algorithm adheres to the following inequality:
E[∥wt+1−w∗
t∥](a)
≤E[∥wt−w∗
t∥]
−ξ
RE[(ct(wt) +ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t))]
+/radicalig
2α2ϵ2
t+ 2αϵtR, (15)
whereξ:= 2α−4Lα2.
Proof.We start by proving a fact with Assumption 2. For any w∈W, by the smoothness and convexity of
ct(w) +ηtc0(w), in (Zhou, 2018, Lemma 4) we have:
ct(w) +ηtc0(w)−ct(wt)−ηtc0(wt)≤⟨∇ct(wt) +ηt∇c0(wt),w−wt⟩+L
2∥w−wt∥2
Here, we set a specific value for wasw=w′
t:=wt−1
L(∇ct(wt) +ηt∇c0(wt))in the above inequality and get:
ct(w′
t) +ηtc0(w′
t)−ct(wt)−ηtc0(wt)≤−1
2L∥∇ct(wt) +ηt∇c0(wt)∥2.
On the other hand, by the convexity of ct(w)andc0(w)and the vanishing gradient assumption ∇ct(w∗
t) +
ηt∇c0(w∗
t) = 0in Assumption 3, we have:
ct(w′
t) +ηtc0(w′
t)≥ct(w∗
t) +ηtc0(w∗
t) + (∇ct(w∗
t) +ηt∇c0(w∗
t))⊤(w′
t−w∗
t) =ct(w∗
t) +ηtc0(w∗
t),
which leads to the following inequality of interest:
ct(w∗
t) +ηtc0(w∗
t)−ct(wt)−ηtc0(wt)≤ct(w′
t) +ηtc0(w′
t)−ct(wt)−ηtc0(wt)≤−1
2L∥∇ct(wt) +ηt∇c0(wt)∥2,
which is equivalent to:
∥∇ct(wt) +ηt∇c0(wt)∥2≤2L(ct(wt) +ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t)). (16)
Then, we bound the left-hand side of Lemma 9 by evolving the updating policy eq. (6) in ∥wt+1−w∗
t∥2,
then:
∥wt+1−w∗
t∥2⩽∥P(wt−α(∇ct(wt) +ηt∇c0(wt)))−w∗
t∥2(17)
=∥wt−α(∇ct(wt) +ηt∇c0(wt))−w∗
t∥2(18)
(a)=∥wt−w∗
t∥2−2α(∇ct(wt) +ηt∇c0(wt))⊤(wt−w∗
t) +α2∥∇ct(wt) +ηt∇c0(wt)∥2(19)
(b)
⩽∥wt−w∗
t∥2−2α(ct(wt) +ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t)) (20)
+ 2α2L(ct(wt) +ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t)) (21)
(22)
where (a) can be obtained by expanding the squared term, (b) is following the convexity property that
f(b)−f(a)≥∇c(a)⊤(b−a)and eq. (16).
17Under review as submission to TMLR
Finally, we take expectation for {w∗
t}asEand take a root on both sides of the above equation and give:
E[∥wt+1−w∗
t∥]≤E[∥wt−w∗
t∥]−ξ
RE[(ct(wt) +ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t))],
whereξ:= 2α−4Lα2, the inequality follows ∥wt−w∗
t∥≤Rand the fact√
a2−b+c2≤a−b
2a+cproved
as following:
/radicalbig
a2−b≤/radicalbigg
a2(1−b
2a2)2
≤a(1−b
2a2)
=a−b
2a,
wherea,bare all positive and a2>b.
Note that if we take a summation over t∈[1,T]on the second term on the right side of eq. (15), we can get
the regretR(T). However, we can divide the left side of eq. (15) into two parts with triangle inequality for a
tighter bound, which give the proof for Theorem 4 as follows.
First, we start with using triangle inequality on a quantity E[∥wt+1−w∗
t+1∥], which gives:
E[∥wt+1−w∗
t∥]≤E[∥wt+1−w∗
t∥] +E[∥w∗
t+1−w∗
t∥]
(a)
≤E[∥wt−w∗
t∥]−ξ
RE[(ct(wt) +ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t))] +∥w∗
t+1−w∗
t∥
Rearranging the above inequality and take summation for t∈[1,T], by the definition of the dynamic regret
in eq. (2) we have:
E[RW(T)] =T/summationdisplay
t=1E[(ct(wt) +ηtc0(wt)−ct(w∗
t)−ηtc0(w∗
t))]
(a)
≤R
ξ(T/summationdisplay
t=1(E[∥wt−w∗
t∥]−E[∥wt+1−w∗
t+1∥]) +T/summationdisplay
t=1∥w∗
t+1−w∗
t∥)
(b)
≤R
ξ(∥w1−w∗
1∥−∥wT+1−w∗
T+1∥+T/summationdisplay
t=1∥w∗
t+1−w∗
t∥)
≤R2
ξ+RVT
ξ
where (a) is obtained by using Lemma 9 and (b) follows Assumption 1 and the definition of variational budget
VT.
B Proof of Lemma 7
Proof.We start from a fact proved in Lemma 6 of (Zhang & Sra, 2016), which gives an inequality for a
geodesic triangle with curvature bounded by κ, where the length of sides for the triangle is a,b,candAis
the angle between sides bandc, then:
a2≤/radicalbig
|κ|c
tanh(/radicalbig
|κ|c)b2+c2−2bccos(A),
In our work, we map our problem on a triangle, where the vertices of this triangle is set to be three status
of the decisions in our problem, the current step decision qt, the next step decision qt+1and the optimal
18Under review as submission to TMLR
solution in current step q∗
t. Denoted(a,b)to be the Wasserstein distance between two distribution aand
boverP2(W). As a result, the three sides of the triangle should be a=d(qt+1,q∗
t),b=d(qt,qt+1)and
c=d(qt,q∗
t). Base on the updating rule, we have d(qt,qt+1) =α∥∇ct(qt) +et+∇c0
t(qt)∥whenαis small
enough and d(qt,qt+1)d(qt,q∗
t)cos(∠qt+1qtq∗
t) =⟨−α(∇ct(qt) +et+∇c0
t(qt)),Exp−1
qt(q∗
t)⟩. Taking all sides
into the triangle inequality, we have:
d(qt+1,q∗
t)2(a)=ζ(κ,d(qt,q∗
t))d(qt,qt+1)2+d(qt,q∗
t)2−2⟨α(∇ct(qt) +et+ηt∇c0
t(qt) +ηtht,Expqt(q∗
t)⟩
≤ζ(κ,d(qt,q∗
t))(α(∇ct(qt) +et+ηtht+ηt∇c0
t(qt))2+d(qt,q∗
t)2−2⟨α(∇ct(qt) +et+ηtht+ηt∇c0
t(qt),Expqt(q∗
t)⟩
≤d(qt,q∗
t)2+ζ(κ,d(qt,q∗
t))(α2(∇ct(qt) +ηt∇c0
t(qt))2+α2(et+ηtht)2+ 2α2(et+ηtht)(∇ct(qt) +ηt∇c0
t(qt)))
−2α⟨(∇ct(qt) +ηt∇c0
t(qt)),Expqt(q∗
t)⟩−2α⟨(et+ηtht),Expqt(q∗
t)⟩
(b)
≤d(qt,q∗
t)2+ζ(κ,R)(α2(∇ct(qt) +ηt∇c0
t(qt))2+α2(et+ηtht)2+ 2α2(et+ηtht)(∇ct(qt) +ηt∇c0
t(qt)))
−2α⟨(∇ct(qt) +ηt∇c0
t(qt)),Expqt(q∗
t)⟩−2α⟨(et+ηtht),Expqt(q∗
t)⟩
(c)
≤d(qt,q∗
t)2+ζ(κ,R)(2Lα2(ct(qt) +ηtc0
t(qt)−ct(q∗
t)−ηtc0
t(q∗
t)) +α2(et+ηtht)2+ 2α2(et+ηtht)(∇ct(qt) +ηt∇c0
t(qt)))
−2α(ct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t))−2α⟨(et+ηtht),Expqt(q∗
t)⟩
=d(qt,q∗
t)2+ 2Lα2ζ(κ,R)(ct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t)) +ζ(κ,R)α2(et+ηtht)2
−2α(ct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t)) + 2α2(et+ηtht)ζ(κ,R)(∇ct(qt) +∇c0
t(qt))−2α⟨(et+ηtht),Expqt(q∗
t)⟩
where (a) follows (Zhang & Sra, 2016, Lemma 6), ζ(κ,d(qt,q∗
t)) =√
|κ|d(qt,q∗
t)
tanh(√
|κ|d(qt,q∗
t), (b) follows the assumption
5, (c) follows the fact proved in eq. (16) and the convexity of ct(qt) +c0
t(qt)Then, we bound the last two
terms in the above inequality with the expectation on the sequence of {et}, which is denoted by Eetand get::
E[2α2(et+ηtht)ζ(κ,R)(∇ct(qt) +∇c0
t(qt))−2⟨(et+ηtht),Expqt(q∗
t)⟩]
≤2α2ζ(κ,R)E[et+ηtht](∇ct(qt) +∇c0
t(qt)) + 2αE[et+ηtht]R
(a)
≤α2ζ(κ,R)(ϵ2
t+E[(∇ct(qt) +∇c0
t(qt))]2) + 2αϵtR
≤α2ζ(κ,R)(ϵ2
t+ct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t)) + 2αϵtR,
where (a) is obtained by the fact 2ab≤a2+b2. Taking the above inequality back gives:
Eet[d(qt+1,q∗
t)]2≤d(qt,q∗
t)2+ 2Lα2ζ(κ,R)(ct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t)) +ζ(κ,R)α2ϵ2
t
−2α(ct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t))
+α2ζ(κ,R)(ϵ2
t+ 2Lct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t)) + 2αϵtR
=d(qt,q∗
t)2−Φ(ct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t)) + 2αϵ2
tζ(κ,R) + 2αϵtR,
where Φ = 2α−3Lα2ζ(κ,R).
Finally, using the fact√
a2−b+c2≤a−b
2a+cand full expectation E, we finish the proof:
E[d(qt+1,q∗
t)]≤E[[Eet[d(qt+1,q∗
t)2]]1/2]
≤E[d(qt,q∗
t)]−Φ
RE[(ct(qt) +c0
t(qt)−ct(q∗
t)−c0
t(q∗
t))] +/radicalig
2αϵ2
tζ(κ,R) + 2αϵtR
19