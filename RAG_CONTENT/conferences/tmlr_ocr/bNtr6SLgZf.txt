Published in Transactions on Machine Learning Research (04/2024)A Survey of Temporal Credit Assignmentin Deep Reinforcement LearningEduardo Pignatellie.pignatelli@ucl.ac.ukUniversity College LondonJohan Ferretjferret@google.comGoogle DeepMindHado van Hasselthado@google.comGoogle DeepMindMatthieu Geistmfgeist@google.comGoogle DeepMindThomas Mesnardmesnard@google.comGoogle DeepMindOlivier Pietquinpietquin@google.comGoogle DeepMindLaura Tonil.toni@ucl.ac.ukUniversity College LondonReviewed on OpenReview:https: // openreview. net/ forum? id= bNtr6SLgZfAbstractThe Credit Assignment Problem (CAP) refers to the longstanding challenge of Reinforce-ment Learning (RL) agents to associate actions with their long-term consequences. Solvingthe CAP is a crucial step towards the successful deployment of RL in the real world sincemost decision problems provide feedback that is noisy, delayed, and with little or no informa-tion about the causes. These conditions make it hard to distinguish serendipitous outcomesfrom those caused by informed decision-making. However, the mathematical nature of creditand the CAP remains poorly understood and deﬁned. In this survey, we review the state ofthe art of Temporal Credit Assignment (CA) in deep RL. We propose a unifying formalismfor credit that enables equitable comparisons of state of the art algorithms and improvesour understanding of the trade-oﬀs between the various methods. We cast the CAP as theproblem of learning the inﬂuence of an action over an outcome from a ﬁnite amount ofexperience. We discuss the challenges posed bydelayed eﬀects,transpositions, and alack ofaction inﬂuence, and analyse how existing methods aim to address them. Finally, we sur-vey the protocols to evaluate a credit assignment method and suggest ways to diagnose thesources of struggle for diﬀerent methods. Overall, this survey provides an overview of theﬁeld for new-entry practitioners and researchers, it oﬀers a coherent perspective for scholarslooking to expedite the starting stages of a new study on the CAP, and it suggests potentialdirections for future research.1Published in Transactions on Machine Learning Research (04/2024)1 IntroductionRL is poised to impact many real world problems that require sequential decision making, such as strategy(Silver et al.,2016;2018;Schrittwieser et al.,2020;Anthony et al.,2020;Vinyals et al.,2019;Perolatet al.,2022) and arcade video games (Mnih et al.,2013;2015;Badia et al.,2020;Wurman et al.,2022),climate control (Wang & Hong,2020), energy management (Gao,2014), car driving (Filos et al.,2020)and stratospheric balloon navigation (Bellemare et al.,2020), designing circuits (Mirhoseini et al.,2020),cybersecurity (Nguyen & Reddi,2021), robotics (Kormushev et al.,2013), or physics (Degrave et al.,2022).One fundamental mechanism allowing RL agents to succeed in these scenarios is their ability to evaluatetheinﬂuenceof their actions over outcomes – e.g., a win, a loss, a particular event, a payoﬀ. Often, theseoutcomes are consequences of isolated decisions taken in a very remote past: actions can have long-termeﬀects. The problem of learning to associate actions with distant, future outcomes is known as the temporalCredit Assignment Problem (CAP):to distribute the credit of success among the multitude of decisionsinvolved(Minsky,1961). Overall, theinﬂuencethat an action has on an outcome representsknowledgeinthe form ofassociationsbetween actions and outcomes (Sutton et al.,2011;Zhang et al.,2020). Theseassociations constitute the scaﬀolding that agencies can use to deduce, reason, improve and act to addressdecision-making problems and ultimately improve their data eﬃciency.Solving the CAP is paramount since most decision problems have two important characteristics: they take along time to complete, and they seldom provide immediate feedback, but oftenwith delayand little insight asto which actions caused it. These conditions produce environments where the feedback signal is weak, noisy,or deceiving, and the ability to separate serendipitous outcomes from those caused by informed decision-making becomes a hard challenge. Furthermore, as these environments grow in complexity with the aimto scale to real-world tasks (Rahmandad et al.,2009;Luoma et al.,2017), the actions taken by an agentaﬀect an increasingly vanishing part of the outcome. In these conditions, it becomes challenging to learnvalue functions that accurately represent theinﬂuenceof an action and to be able to distinguish and orderthe relative long-term values of diﬀerent actions. In fact, canonical Deep Reinforcement Learning (DeepRL) solutions tocontrolare often brittle to the hyperparameter choice (Henderson et al.,2018), inelasticto generalise zero-shot to diﬀerent tasks (Kirk et al.,2023), prone to overﬁtting (Behzadan & Hsu,2019;Wang et al.,2022), and sample-ineﬃcient (Ye et al.,2021;Kapturowski et al.,2023). Overall, building asolid foundation of knowledge that can unlock solutions to complex problems beyond those already solvedcalls for better CA techniques (Mesnard et al.,2021).In the current state of RL,action valuesare a key proxy foraction inﬂuence. Valuesactualisear e t u r nby synthesising statistics of thefutureinto properties of thepresent: they transform a signal dependent onthe future into one dependent only on the present. Recently, the advent of Deep RL (Arulkumaran et al.,2017) granted access to new avenues to express credit through values, either by using memory (Goyal et al.,2019;Hung et al.,2019), associative memory (Hung et al.,2019;Ferret et al.,2021a;Raposo et al.,2021),counterfactuals (Mesnard et al.,2021), planning (Edwards et al.,2018;Goyal et al.,2019;van Hasselt et al.,2021) or by meta-learning (Xu et al.,2018;Houthooft et al.,2018;Oh et al.,2020;Xu et al.,2020;Zahavyet al.,2020). The research on CAP is now fervent, and with a rapidly growing corpus of works.Motivation.Despite its central role, there is little discussion on the precise mathematical nature of credit.While these proxies are suﬃcient to unlock solutions to complex tasks, it remains unclear where to drawthe line between a generic measure of action inﬂuence andcredit. Existing works focus on partial aspectsor sub-problems (Hung et al.,2019;Arjona-Medina et al.,2019;Arumugam et al.,2021) and not all worksrefer to the CAP explicitly in their text (Andrychowicz et al.,2017;Nota et al.,2021;Goyal et al.,2019),despite their ﬁndings providing relevant contributions to address the problem. The resulting literature isfragmented and lacks a space to connect recent works and put their eﬀorts in perspective for the future. Theﬁeld still holds open questions:Q1.Whatis thecreditof an action? How is it diﬀerent from anaction value? And what is the CAP?What in words, and what in mathematics?Q2.Howdo agents learn toassigncredit? What are the main methods in the literature and how canthey be organised?2Published in Transactions on Machine Learning Research (04/2024)Q3.How can weevaluatewhether a method is improving on a challenge? How can we monitor advance-ments?Goals.Here, we propose potential answers to these questions and set out to realign the fundamental issueraised byMinsky(1961) to the Deep RL framework. Our main goal is to provide an overview of the ﬁeldto new-entry practitioners and researchers, and, for scholars looking to develop the ﬁeld further, to put theheterogeneous set of works into a comprehensive, coherent perspective. Lastly, we aim to reconnect workswhose ﬁndings are relevant for CAP, but that do not refer to it directly. To the best of our knowledge,the work byFerret(2022, Chapter 4) is the only eﬀort in this direction, and the literature oﬀers no explicitsurveys on the temporal CA problem in Deep RL.Scope.The survey focuses on temporal CA in single-agent Deep RL, and the problems of(i)quantifyingthe inﬂuence of an action mathematically and formalising a mathematical objective for the CA problem(ii)deﬁning its challenges, and categorising the existing methods to learn the quantities above,(iii)deﬁninga suitable evaluation protocol to monitor the advancement of the ﬁeld. We do not discussstructuralCA inDeep Neural Networks (DNNs), that is, the problem of assigning credit or blame to individual parameters ofa DNN (Schmidhuber,2015;Balduzzi et al.,2015). We also do not discuss CA in multi-agent RL, that is, toascertain which agents are responsible for creating good reinforcement signals (Chang et al.,2003;Foersteret al.,2018). When credit (assignment) is used without any preceding adjective, we always refer totemporalcredit (assignment). In particular, with the adjectivetemporalwe refer to the fact that“each ultimate successis associated with a vast number of internal decisions”(Minsky,1961) and that these decisions, togetherwith states and rewards, are arranged to form atemporalsequence.The survey focuses on Deep RL. In surveying existing formalisms and methods, we only look at the Deep RLliterature, and when proposing new ones, we tailor them to Deep RL theories and applications. We excludefrom the review methods speciﬁcally designed to solve decision problems with linear or tabular RL, as theydo not bode well for scaling to complex problems.Outline.We addressQ1.,Q2.andQ3.in the three major sections of the manuscript. Respectively:•Section4addressesQ1., proposing a deﬁnition of credit and the CAP and providing a survey ofaction inﬂuence measures.•Section5andSection6addressQ2., discussing the key challenges to solving the CAP and theexisting methods to assign credit, respectively.•Section7answersQ3., reviewing the problem setup, the metrics, and the evaluation protocols tomonitor advancements in the ﬁeld.•Section8summarises the main points of the manuscript and provides a critical discussion tohighlight the open challenges.For each question, we contribute by:(a)systematisingexisting worksinto a simpler, coherent space;(b)dis-cussing it, and(c)synthesising our perspective into a unifying formalism. Table1outlines the suggestedreading ﬂow according to the type of reader.Reader typeSuggested FlowSpecialised CA scholar1→8→4→5→6→7→2RL researcher1→8→4→5→6→7Deep Learning researcher1→3→4→5→6→7→8Practitioner (applied researcher)6→4.4→3Proposing a new CA method8→7→6→2→4Table 1: Suggested ﬂow of reading by type of reader to support the outline in Section1. Numbers representsection numbers.3Published in Transactions on Machine Learning Research (04/2024)2 Related WorkThree existing works stand out for proposing a better understanding of the CAP explicitly.Ferret(2022,Chapter 4) designs a conceptual framework to unify and study credit assignment methods. The chapterproposes a general formalism for a range of credit assignment functions and discusses their characteristicsand general desiderata. UnlikeFerret(2022, Chapter 4), we survey potential formalisms for a mathematicaldeﬁnition of credit (Section4); given the new formalism, we propose an alternative view of the methods toassign credit (Section6), and an evaluation protocol to measure future advancements in the ﬁeld.Arumugamet al.(2021) analyses the CAP from an information theoretic perspective. The work focuses on the notion ofinformation sparsityto clarify the role of credit in solving sparse reward problems in RL. Despite the workquestioning what credit is mathematically, it does not survey existing material, and it does not provide aframework that can unify existing approaches to represent credit under a single formalism.Harutyunyanet al.(2019) propose a principled method to measure the credit of an action. However, the study does notaim to survey existing methods tomeasurecredit, the methods toassigncredit, and the methods to evaluatea credit assignment method, and does not aim to organise them into a cohesive synthesis.The literature also oﬀers surveys on related topics. We discuss them in AppendixAto preserve the ﬂuidityof the manuscript.As a result, none of these works position CAP in a single space that enables thorough discussion, assessmentand critique. Instead, we propose a formalism that uniﬁes the existingquantitiesthat represent the inﬂuenceof an action (Section4). Based on this, we can analyse the advantages and limitations of existing measuresof action inﬂuence. The resulting framework provides a way to gather the variety of existingmethodsthatlearn these quantities from experience (Section6), and to monitor the advancements in solving the CAP.3 Notation and BackgroundHere we introduce the notation that we will follow in the rest of the paper and the required background.Notations.We use calligraphic characters to denote sets and the corresponding lowercases to denote theirelements, for example,x∈X. For a measurable space(X,Σ), we denote the set of probability measuresoverXwith∆(X). We use an uppercase letterXto indicate a random variable, and the notationPXtodenote its distribution over the sample setX, for example,PX:X→∆(X). When we mention arandomeventX(for example, arandom action) we refer to a random draw of a speciﬁc valuex∈Xfrom itsdistributionPXand we write,X∼PX. When a distribution is clear from the context, we omit it from thesubscript and writeP(X)instead ofPX(X).W eu s e
Y(x)for the indicator function that maps an elementx∈Xto1ifx∈Y⊂Xand0otherwise. We useRto denote the set of real numbers andB={0,1}todenote the Boolean domain. We useℓ∞(x)=∥x∥∞=s u pi|xi|to denote theℓ-inﬁnity norm of a vectorxof componentsxi. We write the Kullback-Leibler divergence between two discrete probability distributionsPP(X)andPQ(X)with sample spaceXas:DKL(PP(X)||PQ(X)) =/summationtextx∈X[PP(x) log(PP(x)/PQ(x))].Reinforcement Learning.We consider the problem of learning by interacting with an environment. Aprogram (theagent) interacts with anenvironmentby making decisions (actions). The action is the agent’sinterface with the environment. Before each action, the agent mayobservepart of the environment andtake suitable actions. The action changes the state of the environment. After each action, the agent mayperceive a feedback signal (thereward). The goal of the agent is to learn a rule of behaviour (thepolicy)that maximises the expected sum of rewards.Markov Decision Processes (MDPs).MDPs formalise decision-making problems. This survey focuseson the most common MDP settings for Deep RL. Formally, a discounted MDP (Howard,1960;Puterman,2014)i sd e ﬁ n e db yat u p l eM=(S,A,R ,µ ,γ).Sis a ﬁnite set of states (thestate space) andAis a ﬁniteset of actions (the action space).R:S×A→[rmin,rmax]is a deterministic, bounded reward function thatmaps a state-action pair to a scalar rewardr.γ∈[0,1]is a discount factor andµ:S×A→∆(S)is atransition kernel, which maps a state-action pair to probabilities over states. We refer to an arbitrary states∈Swiths, an actiona∈Awithaand a rewardr∈[rmin,rmax]withr. Given a state-action tuple4Published in Transactions on Machine Learning Research (04/2024)(s, a), the probability of the next random stateSt+1beings′depends on astate-transitiondistribution:Pµ(St+1=s′|St=s, At=a)=µ(s′|s, a),∀s, s′∈S.W e r e f e r t oStas therandom stateat timet.T h eprobability of the actionadepends on the agent’s policy, which is a stationary mappingπ:S→∆(A), froma state to a probability distribution over actions.These settings give rise to a discrete-time, stateless (Markovian), Random Process (RP) with the additionalnotions ofactionsto represent decisions andrewardsfor a feedback signal. Given an initial state distributionPµ0(S0), the process begins with a random states0∼Pµ0. Starting froms0, at each timetthe agent interactswith the environment by choosing an actionAt∼Pπ(·|st), observing the rewardrt∼Rt(St,At)and thenext statest+1∼Pµ. If a statestis also anabsorbingstate (s∈S⊂S), the MDP transitions to the samestatestwith probability1and reward0, and we say that the episode terminates. We refer to the union ofeach temporaltransition(st,at,rt,st+1)as atrajectoryorepisoded={st,at,rt,:0≤t≤T},w h e r eTisthehorizonof the episode.We mostly consider episodic settings where the probability of ending in an absorbing state in ﬁnite time is1, resulting in the random horizonT. We consider discrete action spacesA={ai:1≤i≤n}only.A trajectory is also a random variable in the space of all trajectoriesD=(S×A×R)T, and its distribution isthe joint of all of its componentsPD(D)=PA,S,R(s0,a1,r1,...,sT). Given an MDPM=(S,A,R ,µ ,γ)andﬁxing a policyπproduces a Markov Process (MP)Mπand induces a distribution over trajectoryPµ,π(D).We refer to thereturnrandom variableZtas the sum of discounted rewards from timetto the end of theepisode,Zt=/summationtextTk=tγk−tR(Sk,Ak).T h econtrolobjective of an RL problem is to ﬁnd a policyπ∗thatmaximises the expected return,π∗∈argmaxπEµ,π/bracketleftBiggT/summationdisplayt=0γtR(St,At)/bracketrightBigg=E[Z0].(1)Partially-Observable MDPs (POMDPs).POMDPs are MDPs in which agents do not get to observea true state of the environment, but only a transformation of it, and are speciﬁed with an additionaltuple⟨O,µO⟩,w h e r eOis an observation space, andµO:S→∆(O)is an observation kernel, that mapsthe true environment state to observation probabilities. Because transitioning between observations isnot Markovian, policies are a mapping from partialtrajectories, which we denote ashistories, to actions.Histories are sequences of transitionsht={O0}∪{Ak,Rk,Ok+1:0<k<t−1}∈(O×A×R)t=H.Generalised Policy Iteration (GPI).We now introduce the concept of value functions. Thestate valuefunctionof a policyπis the expected return of the policy from statest,vπ(s)=Eπ,µ[Zt|St=s].T h eaction-value function (or Q-function) of a policyπis the expected return of the policy from statestif theagent takesat,qπ(s, a)=Eπ,µ[Zt|St=s, At=a]. Policy Evaluation (PE) is then the process that mapsa policyπto its value function. A canonical PE procedure starts from an arbitrary value functionV0anditeratively applies the Bellman operator,T, such that:ˆvπk+1(St)=Tπ[ˆvπk(St)] :=Eπ,µ[R(St,At)+γˆvk(St+1)],(2)whereˆvkdenotes the value approximation at iterationk,At∼Pπ(·|St), andSt+1∼Pµ,π(·|St,At).T h eBellman operator is aγ-contraction in theℓ∞and theℓ2norms, and its ﬁxed point is the value of thepolicyπ. Hence, successive applications of the Bellman operator improve the prediction accuracy becausethe current value gets closer to the true value of the policy. We refer to the PE as thepredictionobjective(Sutton & Barto,2018). Policy improvement maps a policyπto an improved policy:πk+1(a|S)=G[πk,S]=
{a}(argmaxu∈A[R(S, u)+γvk(S′)]) =
{a}(argmaxu∈A[qk(S, u)]).(3)We refer to GPI as a general method to solve thecontrolproblem (Sutton & Barto,2018) deriving fromthe composition of PE and Policy Improvement (PI). In particular, we refer to the algorithm that alternatesan arbitrary numberkof PE steps and one PI step as Modiﬁed Policy Iteration (MPI) (Puterman & Shin,5Published in Transactions on Machine Learning Research (04/2024)1978;Scherrer et al.,2015). Fork=1, MPI recovers Value Iteration, while fork→+∞, it recovers PolicyIteration. For any value ofk∈[1,+∞), and under mild assumptions, MPI converges to an optimal policy(Puterman,2014).In Deep RL we parameterise a policy using a neural network with parameters setθand denote the distributionover action asπ(a|s, θ). We apply the same reasoning for value functions, with parameters setφ, which leadstov(s, φ)andq(s, a, φ)for the state and action value functions respectively.4 Quantifying action inﬂuencesWe start by answeringQ1., which aims to address the problem ofwhatto measure, when referring to credit.SinceMinsky(1961) raised the Credit Assignment Problem (CAP), a multitude of works paraphrased hiswords:-“The problem of how to incorporate knowledge” and “given an outcome, how relevant were pastdecisions?”(Harutyunyan et al.,2019),-“Is concerned with identifying the contribution of past actions on observed future outcomes”(Aru-mugam et al.,2021),-“The problem of measuring an actions inﬂuence on future rewards”(Mesnard et al.,2021),-“An agent must assign credit or blame for the rewards it obtains to past states and actions”(Cheluet al.,2022),-“The challenge of matching observed outcomes in the future to decisions made in the past”(Venutoet al.,2022),-“Given an observed outcome, how much did previous actions contribute to its realization?”(Ferret,2022, Chapter 4.1).These descriptions converge to Minsky’s original question and show agreement in the literature on an informalnotion of credit. In this introduction, we propose to reﬂect on the diﬀerent metrics that exist in the literatureto quantify it. We generalise the idea ofaction value, which often only refers toq-values, to that ofactioninﬂuence, which describes a broader range of metrics used to quantify the credit of an action. While we donot provide a deﬁnitive answer on what creditshouldbe, we review how diﬀerent works in the existing RLliterature have characterised it. We now start by developing an intuition of the notion of credit.Consider Figure1, inspired to both Figure 1 ofHarutyunyan et al.(2019) and to theumbrellaproblem inOsband et al.(2020). The action taken atx0determines the return of the episode by itself. From the pointof view ofcontrol, any policy that always takesa′inx0(i.e.,π∗∈Π∗:π∗(a′|x0)=1), and then any otheraction afterwards, is an optimal policy. From the CAP point of view, some optimal actions, namely thoseafter the ﬁrst one, do notactuallycontribute to optimal returns. Indeed, alternative actions still produceoptimal returns and contribute equally to each other to achieve the goal, so their credit is equal. We can seethat, in addition to optimality, credit not only identiﬁes optimal actions but informs them of hownecessarythey are to achieve an outcome of interest.From the example, we can deduce that credit evaluates actions for their potential to inﬂuence an outcome.The resulting CAP is the problem ofestimating the inﬂuenceof an action over an outcome from exper-imental data and describes a pure association between them.Why solving the CAP?Action evaluation is a cornerstone of RL. In fact, solving a control problem ofteninvolves running a GPI scheme. Here, the inﬂuence of an action drives learning, for it suggests a possibledirection to improve the policy. For example, the action-value plays that role in Equation (3). It followsthat the quality of the measure of inﬂuence fundamentally impacts the quality of the policy improvement.Low quality evaluations can lead the policy to diverge from the optimal one, hinder learning, and slow downprogress (Sutton & Barto,2018;van Hasselt et al.,2018). On the contrary, high quality evaluations provideaccurate, robust and reliable signals that foster convergence, sample-eﬃciency and low variance. While6Published in Transactions on Machine Learning Research (04/2024)
Figure 1: A simpliﬁed MDP to develop an intuition of credit. The agent starts inx0, and can choosebetween two actions,a′anda′′in each state; the reward is1when reaching the upper, solid red square, and0otherwise. The ﬁrst action determines the outcome alone.simple evaluations are enough for specialised experiments, the real world is a complex blend of multiple,sometimes hierarchical tasks. In these cases, the optimal value changes from one task to another, and thesesimple evaluations do not bode well to adapt to general problem solving. Yet, the causal structure thatunderlies the real word is shared among all tasks, and the modularity of its causal mechanisms is often avaluable property to incorporate. In these conditions, learning to assign credit in one environment becomesa lever to assign credit in another (Ferret et al.,2021a), and ultimately makes learning faster, more accurateand more eﬃcient. For these reasons, and because an optimal policy only requires discovering one singleoptimal trajectory, credit stores knowledge beyond that expressed by optimal behaviours alone, and solvingthe control problem is not suﬃcient to solve the CAP, with the former being an underspeciﬁcation of thelatter.4.1 Are allaction values,credit?As we stated earlier, most Deep RL algorithms use some form ofaction inﬂuenceto evaluate the impactsof an action on an outcome. This is a fundamental requirement to rank actions and select the optimal oneto solve complex tasks. For example, many model-free methods use thestate-action valuefunctionqπ(s, a)to evaluate actions (Mnih et al.,2015;van Hasselt et al.,2016), where actions contribute as much as theexpected return they achieve at termination of the episode. Advantage Learning (AL) (Baird,1999;Mnihet al.,2016;Wang et al.,2016b, Chapter 5) uses theadvantagefunctionAπ(st,at)=qπ(st,at)−vπ(st)1to measure credit, while other works study the eﬀects of theaction-gap(Farahmand,2011;Bellemare et al.,2016;Vieillard et al.,2020b) on it, that is, the relative diﬀerence between the expected return of the bestaction and that of another action, usually the second best. Action inﬂuence is also a key ingredient ofactor-critic and policy gradient methods (Lillicrap et al.,2015;Mnih et al.,2016;Wang et al.,2016a), wherethe policy gradient is proportional toEµ,π[Aπ(s, a)∇logπ(A|s)],w i t hAπ(s, a)estimating the inﬂuence ofthe actionA.These proxies are suﬃcient to select optimal actions and unlock solutions to complex tasks (Silver et al.,2018;Wang et al.,2016b;Kapturowski et al.,2019;Badia et al.,2020;Ferret et al.,2021b). However, whilemany works explicitly refer to the action inﬂuence as a measure of credit, the term is not formally deﬁnedand, it remains unclear where to draw the line betweencreditand other quantities. Key questions arise:What is the diﬀerence between these quantities and credit? Do they actually represent credit as originallyformulated byMinsky(1961)? If so, under what conditions do they do?Without a clear deﬁnition ofwhatto measure, we do not have an appropriate quantity to target when designing an algorithm to solve theCAP. More importantly, we do not have an appropriate quantity to use as a single source of truth and termof reference to measure the accuracy of other metrics of action inﬂuence, and how well they approximatecredit. To ﬁll this gap, we proceed as follows:•Section4.2formalises what is agoalor anoutcome: what we evaluate the action for;•Section4.3uniﬁes existing functions under a common formalism;1To be consistent with the RL literature we abuse notation and denote the advantage with a capital letterAπdespite notbeing random and being the same symbol of the actionAt.7Published in Transactions on Machine Learning Research (04/2024)•Section4.4formalises the CAP following this deﬁnition;•Section4.5analyses how diﬀerent works interpreted and quantiﬁedaction inﬂuencesand reviewsthem;•Section4.6distils the properties that existing measures of action inﬂuence exhibit.We suggest the reader only interested in the ﬁnal formalism to directly skip to Section4.4, and to come backto the next sections to understand the motivation behind it.4.2 What is agoal?Because credit measures the inﬂuence of an action upon achieving a certain goal, to deﬁne credit formallywe must be able to describegoalsformally, and without a clear understanding of what constitutes one, anagent cannot construct a learning signal to evaluate its actions.Goalis a synonym forpurpose, which we caninformally describe as a performance to meet or a prescription to follow. Deﬁning a goal rigorously allowsmaking the relationship between the action and the goal explicit (Ferret,2022, Chapter 4) and enables theagent to decompose complex behaviour into elementary ones in a compositional (Sutton et al.,1999;Baconet al.,2017), and possibly hierarchical way (Flet-Berliac,2019;Pateria et al.,2021;Hafner et al.,2022). Thisidea is at the foundation of many CA methods (Sutton et al.,1999;2011;Schaul et al.,2015a;Andrychowiczet al.,2017;Harutyunyan et al.,2019;Bacon et al.,2017;Smith et al.,2018;Riemer et al.,2018;Bagaria &Konidaris,2019;Harutyunyan et al.,2018;Klissarov & Precup,2021). We proceed with a formal deﬁnitionofgoalsin the next paragraph, and review how these goals arerepresentedin seminal works on CA in theone after. This will lay the foundation for a unifying notion of credit later in Sections4.3.Deﬁning goals.To deﬁne goals formally, we adopt thereward hypothesis, which posits:That all of what we mean by goals and purposes can be well thought of as maximizationof the expected value of the cumulative sum of a received scalar signal (reward).(Sutton,2004).Here, the goal is deﬁned as thebehaviourthat results from the process of maximising the return. Thereward hypothesis has been further advanced by later studies (Abel et al.,2021b;Pitis,2019;Shakerinava& Ravanbakhsh,2022;Bowling et al.,2023). In the following text, we employ the goal deﬁnition inBowlinget al.(2023), which we report hereafter:Deﬁnition 1(Goal).Given a distribution of ﬁnite historiesP(H),∀H∈H, we deﬁne agoalas a partialordering overP(H),a n df o ra l lh, h′∈Hwe writeh/followsorequalh′to indicate thathis preferred toh′or that thetwo are indiﬀerently preferred.Here,His a random history in the set of all historiesHas described in Section3, andP(H)is an unknowndistribution over histories, diﬀerent from that induced by the policy and the environment. An agent behaviourand an environment then induce a new distribution over histories, and we obtainPµ,π(H)as described inSection3. This in turn allows deﬁning a partial ordering over policies, rather than histories, and we writeanalogouslyπ/followsorequalπ′to indicate the preference. For theMarkov Reward Theorem(Bowling et al.,2023,Theorem 4.1) and under mild conditions (Bowling et al.,2023), there exists a deterministic, Markov rewardfunction2R:O×A→[0,1]such that the maximisation of the expected sum of rewards is consistent withthe preference relation over policies.Subjective and objective goals.TheMarkov Reward Theoremholds both if the preferences are deﬁnedinternallyby the agent itself – this is the case ofintrinsic motivation(Piaget et al.,1952;Chentanez et al.,2004;Barto et al.,2004;Singh et al.,2009;Barto,2013;Colas et al.,2022) – and in case they originatefrom anexternalentity, such as an agent-designer. In the ﬁrst case, the agent doing the maximisation is the2We omit the transition dependent discounting for the sake of conciseness and because not relevant to our problem. Thereader can consultPitis(2019);White(2017)f o rd e t a i l s .8Published in Transactions on Machine Learning Research (04/2024)same as the one holding the ordering over policies, and we refer to the corresponding goal as asubjectivegoal. In the second case, anagent designeror an unknown, non-observable entity holds the ordering and aseparatelearning agentis the one pursuing the optimisation process. We refer to a goal as anobjective goalin this latter case. These settings usually correspond to the distinction between goals and sub-goals in theliterature (Liu et al.,2022).Outcomes.A particularly interesting use of goals for CA is in hindsight (Andrychowicz et al.,2017). Herethe agent acts with a goal in mind, but it evaluates a trajectory as ifareward function – one diﬀerent fromthe original one – was maximised in the current trajectory. We discuss the beneﬁts of these methods inSection6.4. When this is the case, we use the termoutcometo indicate a realised goal in hindsight. Inparticular, given a historyH∼Pµ,π(H), there exists a deterministic, Markov reward functionRthat ismaximal inH. We refer to the correspondingHas an outcome. For example, consider a trajectoryhthatends in a certain states. There exist a Markov reward function that outputs always0and1only when thesis the ﬁnal state ofh.W er e f e rt ohas anoutcome.In other words, this way of deﬁning goals or outcomes corresponds to deﬁning a task to solve, which inturn can be expressed through a reward function with the characteristics described above. Vice-versa, thereward function canencodea task. When credit is assigned with respect to a particular goal or outcome, itthen evaluates the inﬂuence of an action to solving that particular task. As discussed above, this is key todecomposing and recomposing complex behaviours and the deﬁnition aligns with that of other disciplines,such as psychology wherea goal . . . is a cognitive representation of something that is possible in the future(Elliot & Fryer,2008) or philosophy, where representations do not merely read the world as it is, but theyexpresspreferencesover something that is possible in the future (Hoﬀman,2016;Prakash et al.,2021;Le Lanet al.,2022).Representing goals and outcomes.However, expressing the relation between actions and goals explic-itly, that is, when the function that returns the credit of an action has a goal as an input, raises the problemof how torepresenta goal for computational purposes. This is important because among the CA methodsthat deﬁne goals explicitly (Sutton et al.,2011;Schaul et al.,2015a;Andrychowicz et al.,2017;Rauber et al.,2019;Harutyunyan et al.,2019;Tang & Kucukelbir,2021;Arulkumaran et al.,2022;Chen et al.,2021), notmany of them use the rigour of a general-purpose deﬁnition of goal such as that inBowling et al.(2023).In these works, thegoal-representation space, which we denote asg∈G, is arbitrarily chosen to representspeciﬁc features of a trajectory. It denotes anobject, rather than a performance or a prescription to meet.For example, agoal-representationgcan be a state (Sutton et al.,2011;Andrychowicz et al.,2017) andg∈G=S. It can be a speciﬁc observation (Nair et al.,2018)w i t hg∈G=O. Alternatively, it can bean abstract features vector (Mesnard et al.,2021) that reports on some characteristics of a history, and wehaveg∈G=Rd,w h e r edis the dimensionality of the vector. Even, a goal can be represented by a naturallanguage instruction (Luketina et al.,2019) andg∈G=Rdis the embedding of that piece of text. A goalcan be represented by a scalarg∈G=R(Chen et al.,2021) that indicates a speciﬁc return to achieve, oreven a full command (Schmidhuber,2019), that is a return to achieve is a speciﬁc window of time.While these representations are all useful heuristics, they lack formal rigour and leave space for ambiguities.For example, saying that the goal is a state might mean thatvisiting the state at the end of the trajectoryis the goal or that visiting it in themiddleof it is the goal. This is often not formally deﬁned, and whatis the reward function that corresponds to that speciﬁc representation of a goal is not always clear. Inthe following text, when surveying a method or a metric that speciﬁes a goal, we refer to the speciﬁc goalrepresentation used in the work and make an eﬀort to detail what is the reward function that underpins thatgoal representation.4.3 What is anassignment?Having established a formalism for goals and outcomes, we are now ready to describecreditformally and weproceed with a formalism that uniﬁes the existing measures of action inﬂuence. We ﬁrst describe a genericfunction that generalises most CAs, and then proceed to formalise the CAP. Overall, this formulationprovides a term of reference for the quantities described in Section4.5. We now formalise anassignment:9Published in Transactions on Machine Learning Research (04/2024)Deﬁnition 2(Assignment).Consider an actiona∈A, a goalg∈G, and a contextc∈C. We use the termassignment functionor simplyassignmentto denote a functionKthat maps a context, an action, and anoutcome to a quantityy∈Y, which we refer to as theinﬂuenceof the action:K:C×A×G→Y.(4)Here, a contextc∈Crepresents some input data and can be arbitrarily chosen depending on the assignmentin question. For example,ccan be a states. A context must hold information about the present, for example,the current state or the current observation; it may contain information about the past, for example, thesequence of past decisions that occurred until now for a POMDP; to evaluate the current action, it cancontain information about what future actions will be takenin-potentia, for example by specifying a policyto follow whena∈Ais not taken, or a ﬁxed trajectory, in which case the current action is evaluated inhindsight (Andrychowicz et al.,2017).In the general case, the action inﬂuence is a random variableY∈Y⊂Rd. This is the case, for example, ofthe action-value distribution (Bellemare et al.,2017) as described in Equation10, where the action inﬂuenceis deﬁned over the full distribution of returns. However, most methods extract some scalar measures of thefull inﬂuence distribution, such as expectations (Watkins,1989), and the action inﬂuence becomes a scalary∈R. In the following text, we mostly consider scalar forms of the inﬂuenceY=Ras these represent themajority of the existing formulations.In practice, anassignmentprovides a single mathematical form to talk about the multitude of ways toquantify action inﬂuence that are used in the literature. It takes an actiona∈A, some contextual datac∈Cand a goalg∈Gand maps it to some measure ofaction inﬂuence. While maintaining the same mathematicalform, diﬀerent assignments can return diﬀerent values of action inﬂuence and steer the improvement indiﬀerent directions.Equation (4) also resembles the General Value Function (GVF) (Sutton et al.,2011), where the inﬂuencey=qπ(s, a, g)is the expected return of the policyπwhen taking actionain states, with respect a goalg.However, in GVFs:(i)yis anaction valueand does not generalise other forms of action inﬂuence; the goal isan MDP stateg∈Sand does not generalise to our notion of goals in Section4.2; the function only considersforward predictions and does not generalise to evaluating an action in hindsight (Andrychowicz et al.,2017).Table2at page11contains further details on the comparison and further speciﬁes the relationship betweenthe most common functions and their corresponding assignment.4.4 The credit assignment problemThe generality of the assignment formalism reﬂects the great heterogeneity of action inﬂuence metrics, whichwe review later in Section4.5. This heterogeneity shows that, even if most studies agree on an intuitivenotion of credit, they diverge in practice on how to quantify credit mathematically. Having uniﬁed theexisting assignments in the previous section, we now proceed to formalise the CAP analogously. This allowsus to put the existing methods into a coherent perspective as a guarantee for a fair comparison, and tomaintain the heterogeneity of the existing measures of action inﬂuence.We cast the CAP as the problem of approximating a measure of action inﬂuence from experience. Weassume standard model-free, Deep RL settings and consider an assignment represented as a neural networkk:C×A×G×Φ→Rwith parametersφ∈Φ=Rnthat can be used to approximate the credit of theactions. This usually represents the critic or the value function of an RL algorithm. In addition, we admita stochastic function to represent the policy, also in the form of a neural networkf:S×Θ→∆(A),w i t hparameters setθ∈Θ=Rm. We assume thatn≪| S |×| A |andm≪| S |×| A |and note that often subsetsof parameters are shared among the two functions.We further assume that the agent has access to a set of experiencesDand that it can sample from itaccording to a distributionD∼PD. This can be a pre-compiled set of external demonstrations, wherePC(D)=U(D), or an MDP, wherePC=Pµ,π(D), or even a ﬁctitious model of an MDPPC=P˜µ,π(D),where/tildewideµis a function internal to the agent, of the same form ofµ. These are also mild assumptions as they10Published in Transactions on Machine Learning Research (04/2024)AssignmentAction inﬂuenceContextActionGoalState-action-valueqπ(s, a)s∈Sa∈Ag∈RAdvantageqπ(s, a)−v(s)s∈Sa∈Ag∈RGeneralq-value functionqπ,R(s, a)s∈Sa∈Ag∈SDistributional action-valueQπ(s, a)s∈Sa∈Ag∈{0,...,n}Distributional advantageDKL(Qπ(s, a)||Vπ(s, a))s∈Sa∈Ag∈{0,...,n}Hindsight advantage1−π(At|s)PD(At|st,Zt)Zts∈S,hT∈Ha∈hg∈RCounterfactual advantagePD(At=a|St=s, Ft=f)q(s, a, f)s∈Sa∈hg∈RPosterior value/summationtextTt=0Pµ,π(Ut=u|ht)vπ(ot,xt)o∈O,u∈Rd,πA∼πg∈RPolicy-conditioned valueq(s, a, π)s∈S,π∈Πa∈Ag∈RTable 2: A list of the most commonaction inﬂuencesand their assignment functions in the Deep RL literatureanalysed in this survey. For each function, the table speciﬁes the inﬂuence, the context representation, theaction, and the goal representation of the corresponding assignment functionK∈K.correspond to, respectively, oﬄine settings, online settings, and model-based settings where the model islearned. We detail these settings in AppendixB. We now deﬁne the CAP formally.Deﬁnition 3(The credit assignment problem).Consider an MDPM, a goalg∈G, and a set of experienceD. Consider an arbitrary assignmentK∈Kas described in Equation(4). Given a parameterised function/tildewideK:C×A×G×Φ→Rwith parameters setφ∈Φ⊂Rn, we refer to theCredit Assignment Problemas theproblem of ﬁnding the set of parametersφ∈Φsuch that:/tildewideK(c, a, g, φ)=K(c, a, g),∀c∈C,a∈A,g∈G.(5)Diﬀerent choices of action inﬂuence have a great impact on the hardness of the problem. In particular, thereis a trade-oﬀ between:(a)how eﬀective the chosen measure of inﬂuence is to inform the direction of the policy improvement,(b)how easy it is to learn that function from experience.For example, usingcausal inﬂuence(Janzing et al.,2013) as a measure of action inﬂuence makes the CAPhard to solve in practice. In fact, discovering causal mechanisms from associations alone is notoriouslychallenging (Pearl,2009;Bareinboim et al.,2022), and pure causal relationships are rarely observed in nature(Pearl et al.,2000) but in speciﬁc experimental conditions. However, causal knowledge is reliable, robustto changes in the experience collected and eﬀective, and causal mechanisms can be invariant to changes inthe goal. On the contrary,q-values are easier to learn as they represent a measure of statistical correlationbetween state-actions and outcomes, but their knowledge is limited to the bare minimum necessary to solvea control problem. This makes them more brittle to sudden changes to the environment, for example, inopen-ended settings (Abel et al.,2023). Which quantity to use in each speciﬁc instance or each speciﬁcproblem is still the subject of investigation in the literature, as we show in the next sections. Ideally, weseek to use the most general measure of inﬂuence that can be learned with the least amount of experience.4.5 Existing assignment functionsWe now survey the most important assignment functions from the literature and their corresponding measureof action inﬂuence. The following list is not exhaustive, but rather it is representative of the limitations ofexisting credit formalisms. For brevity, and without loss of generality, we omit functions that do not explicitlyevaluate actions (for example, state-values), but we notice that it is still possible to reinterpret an assignmentto a state as an assignment to a set of actions for it aﬀects all the actions that led to that state.11Published in Transactions on Machine Learning Research (04/2024)State-action values(Shannon,1950;Schultz,1967;Michie,1963;Watkins,1989) are a hallmark of RL,and are described by the following expression:qπ(s, a)=Eµ,π[Zt|St=s, At=a].(6)Here, the contextcis a states∈Sin the case of MDPs or a historyh∈Hfor a POMDP. Theq-functionquantiﬁes the credit of an action by the expected return of the action in the context.q-values are among the simplest ways to quantify credit and oﬀer a basic mechanism to solve control prob-lems. However, whileq-functions oﬀer solid theoretical guarantees in tabular RL, they can be unstable inDeep RL. When paired with bootstrapping and oﬀ-policy learning, q-values are well known to diverge fromthe optimal solution (Sutton & Barto,2018).van Hasselt et al.(2018) provide empirical evidence of thephenomenon, investigating the relationship between divergence and performance, and how diﬀerent variablesaﬀect divergence. In particular, the work shows that the Deep Q-Network (DQN) (Mnih et al.,2015) is notguaranteed to converge to the optimalq-function. The divergence rate on both evaluation and control prob-lems increases depending on speciﬁc mechanisms, such as the amount of bootstrapping, or the amount ofprioritisation of updates (Schaul et al.,2015b).An additional problem arises when employing GPI schemes to solve control problems. While during evalua-tion the policy is ﬁxed, here the policy continuously changes. It becomes more challenging to track the targetof the update while converging to it, as the change of policy makes the problem appear non-stationary fromthe point of view of the value estimation. In fact, even if the policy changes, there is no signal that informsthe policy evaluation about the change. To mitigate the issue, many methods either use a ﬁxed network asan evaluation target (Mnih et al.,2015), perform Polyak averaging of the target network (Haarnoja et al.,2018), or clip the gradient update to a maximum cap (Schulman et al.,2017). To further support the idea,theoretical and empirical evidence (Bellemare et al.,2016) shows that theq-function isinconsistent: for anysuboptimal actiona, the optimal value functionq∗(s, a)describes the value of anon-stationarypolicy, whichselects a diﬀerent actionπ∗(s)(rather thana) at each visit ofs.The non-stationarity ofq-values for suboptimal actions has also been shown empirically.Schaul et al.(2022)measure the per-statepolicy changeW(π,π′|s)=/summationtexta∈A|π(a|s)−π′(a|s)|for several Atari 2600 gamesArcade Learning Environment (ALE) (Bellemare et al.,2013), and show that the action-gap undergoesbrutal changes despite the agent maintaining a constant value of expected returns.In practice, Deep RL algorithms often useq-targets to approximate theq-value, for example,n-step targets(Sutton & Barto,2018, Chapter 7), orλ-returns (Watkins,1989;Jaakkola et al.,1993;Sutton & Barto,2018, Chapter 12). However, we consider them asmethods, rather than quantities to measure credit, sincethey all ultimately aim to converge to theq-value. For this reason, we discuss them in Section6.1.Advantage(Baird,1999) measures, in a given state, the diﬀerence between the q-value of an action andthe value of its stateAπ(s, a)=qπ(s, a)−vπ(s).(7)Here, the contextcis the same as in Equation (6). Becausevπ(s)=/summationtexta∈Aq(s, a)π(a|s)andAπ(s, a)=qπ(s, a)−Eπ[qπ(s, a)], the advantage quantiﬁes how much an action is better than average.As also shown inBellemare et al.(2016), using the advantage to quantify credit can increase theaction-gap.Empirical evidence has shown the consistent beneﬁts of advantage over q-values (Baird,1999;Wang et al.,2016b;Bellemare et al.,2016;Schulman et al.,2016), and the most likely hypothesis is its regularisationeﬀects (Vieillard et al.,2020b;a;Ferret et al.,2021a). On the other hand, when estimated directly and notby composing state and state-action values, for example inPan et al.(2022), the advantage does not permitbootstrapping. This is because advantage lacks an absolute measure of action inﬂuence, and only maintainsone that is relative to the other possible actions.Overall, in canonical benchmarks for both evaluation (Wang et al.,2016b) and control (Bellemare et al.,2013), advantage has been shown to improve overq-values (Wang et al.,2016b). In particular, policyevaluation experiences faster convergence in large action spaces because the state-valuevπ(s)can hold12Published in Transactions on Machine Learning Research (04/2024)information that is shared between multiple actions. For control, it improves the score over several Atari2600 games compared to both doubleq-learning (van Hasselt et al.,2016) and Prioritised Experience Replay(PER) (Schaul et al.,2015b).General Value Functions (GVFs)(Sutton et al.,2011;Schaul et al.,2015a) are a set of q-value functionsthat predict returns for multiple reward functions:qπ,R(s, a)={Eµ,π/bracketleftBiggT/summationdisplaytR(St,At)|St=s, At=a/bracketrightBigg:∀R∈R },(8)whereRis a pseudo-reward function andRis an arbitrary, pre-deﬁned set of reward functions. Noticethat we omit the pseudo-termination and pseudo-discounting terms that appear in their original formulation(Sutton et al.,2011) to maintain the focus on credit assignment. The contextcis the same ofq-values andadvantage, and the goal that the pseudo-reward represents is to reach a speciﬁc stateg=s∈S.When ﬁrst introduced (Sutton et al.,2011), the idea of GVFs stemmed from the observation that canonicalvalue functions are limited to address only a single task at a time. Solving a new task would require learninga value functionex-novo. By maintaining multiple assignment functions at the same time, one for each goal,GVFs can instantly quantify the inﬂuence of an action for multiple goals simultaneously. However, whileGVFs maintain multiple assignments, the goal is still not an explicit input of the value function. Instead, itis left implicit, and each assignment serves the ultimate goal to maximise a diﬀerent pseudo-reward function(Sutton et al.,2011).Universal Value Functions Approximators (UVFAs) (Schaul et al.,2015a) scale GVFs to Deep RL andadvance their idea further by conﬂating these multiple assignment functions into a single one, representedas a deep neural network. Here, unlike for state-action values and GVFs, the goal is an explicit input of theassignment:qπ(s, a, g)=Eµ,π[Zt|St=s, At=a, Gt=g].(9)The action inﬂuence here is measured for a goal explicitly. This allows to leverage the generalisation capacityof deep neural networks and to generalise not only over the space of states but also over that of goals.Distributional values(Jaquette,1973;Sobel,1982;White,1988;Bellemare et al.,2017) consider the fullreturn distributionZtinstead of its expected value:Qπ(s, a)=Pµ,π(Zt|St=s, At=a),(10)wherePµ,π(Zt)is the distribution over returns. Notice that we use uppercaseQto denote the value distri-bution and the lowercaseqfor its expectation (Equation (6)).To translate the idea into a practical algorithm,Bellemare et al.(2017) proposes a discretised version ofthe value distribution by projectingPµ,π(Zt)on a ﬁnite supportC={0≤i≤C}. The discretised valuedistribution then becomesQπ(s, a)=PC(Zt|St=s, At=a),w h e r ePCis a categorical Bernoulli thatdescribes the probability that a returnc∈Cis achieved. Here, the context is the current MDP state andthe goal is the expected return. Notice that while the optimal expected value functionq∗(s, a)is unique, ingeneral, there are many optimal value distributions since diﬀerent optimal policies can induce diﬀerent valuedistributions.Experimental evidence (Bellemare et al.,2017) suggests that distributional values provide a better quantiﬁ-cation of the action inﬂuence, leading to superior results in well known benchmarks for control (Bellemareet al.,2013). However, it is yet not clear why distributional values improve over their expected counterparts.One hypothesis is that predicting for multiple goals works as an auxiliary task (Jaderberg et al.,2017),which often leads to better performance. Another hypothesis is that the distributional Bellman optimalityoperator proposed inBellemare et al.(2017) produces a smoother optimisation problem, but the evidenceremains weak or inconclusive (Sun et al.,2022).13Published in Transactions on Machine Learning Research (04/2024)Distributional advantage(Arumugam et al.,2021) proposes a distributional equivalent of the advantage:Aπ(s, a)=DKL(Qπ(s, a)||Vπ(s)),(11)and borrows the properties of both distributional values and the expected advantage. Intuitively, Equa-tion (11) shows how much knowing the action changes the value distribution. To do so, it measures thechange of the value distribution, for a given state-action pair, relative to the distribution for the particularstate only. The KL divergence between the two distributions can then be interpreted as the distributionalanalogue of Equation (7), where the two quantities appear in their expectation instead. The biggest draw-back of this measure of action inﬂuence is that it is only treated in theory, and there is no empirical evidencethat supports distributional advantage as a useful proxy for credit in practice. Future works should considerproviding empirical evidence on how this measure of action inﬂuence behaves compared toq-values anddistributional values.Hindsight advantage(Harutyunyan et al.,2019) stems from conditioning the action inﬂuence on futurestates or returns. The return-conditional hindsight advantage function can be written as follows:Aπ(s, a, z)=/parenleftbigg1−Pπ(At=a|St=s)Pµ,π(At=a|St=s, Zt=z)/parenrightbiggz.(12)HereAπ(s, a, z)denotes the return-conditional advantage andPµ,π(at|St=s, Zt=z)is the return-conditionalhindsight distributionand describes the probability that an actionahas been taken ins, giventhat we observed the returnzat the end of the episode, after followingπ. The context is a state, and thegoal is the expected return, which, in this case, corresponds also to the value of the return collected in thecurrent trajectory.The idea ofhindsight– initially presented inAndrychowicz et al.(2017) – is that even if the trajectorydoes not provide useful information for the main goal, it can be revisited as if the goal was the outcomejust achieved. Hindsight advantage brings this idea to the extreme and rather than evaluating only for apre-deﬁned set of goals such as inAndrychowicz et al.(2017), it evaluates for every experienced state orreturn. Here, the action inﬂuence is quantiﬁed by that proportion of return determined by the ratio inEquation (12). To develop an intuition of it, if the actionaleads to the returnzwith probability>0suchthatPµ,π(At=a|St=s, Zt=z)>0, but the behaviour policyπtakesawith probability0, the credit ofthe actionais0. There exists also a state-conditional formulation rather than a return-conditional one, andwe refer toHarutyunyan et al.(2019) for details on it to keep the description concise.Future-conditional advantage(Mesnard et al.,2021) generalises hindsight advantage to use an arbitraryproperty of the future:Aπ(s, a, f)=Pµ,π(At=a|St=s, Ft=f)qπ(s, a, f).(13)Here,F:DT→Rnis ann-dimensional feature of a trajectoryd, andFtis that feature for a trajectory thatstarts at timetand ends at the random horizonT.qπ(s, a, f)=Eµ,π[Zt|St=s, Ft=f,At=a]denotesthe future-conditioned state-action value function. The context is a tuple of state and feature(s, f);t h egoal is the expected return observed at the end of the trajectory. Notice that you can derive the hindsightadvantage by settingF=Z.To develop an intuition,Fcan represent, for example, whether a day is rainy, and the future-conditionaladvantage expresses the probability of an actiona, given that the day will be rainy.Counterfactual advantage(Mesnard et al.,2021) proposes a speciﬁc choice ofFsuch thatFis inde-pendent of the current action. This produces a future-conditional advantage that factorises the inﬂuenceof an action in two components: the contribution deriving from the intervention itself (the action) and theluck represented by all the components not under the control of the agent at the timet, such as fortuitousoutcomes of the state-transition dynamics, exogenous reward noise, or future actions. The form is the sameas that in Equation13, with the additional condition that the featureFtis independent of the actionAtandwe haveEF[DKL(P(At|St=s)||P(At|St=s, Ft=f)] = 0.14Published in Transactions on Machine Learning Research (04/2024)The main intuition behindcounterfactual advantageis the following. While to compute counterfactualswe need access to a model of the environment, in model-free settings we can still compute all the relevantinformationFtthat does not depend on this model. Once learned, a model ofFcan then represent a validbaseline to compute counterfactuals in a model-free way. To stay in the scope of this section, we detail howto learn this quantity in Section6.4.Posterior value functions(Nota et al.,2021) reﬂect on partial-observability and propose a character-isation of the hindsight advantage bespoke to POMDPs. The intuition behind Posterior Value Functions(PVFs) is that the evaluated action only accounts for a small portion of the variance of returns. The majorityof the variance is often due to the part of the trajectory that still has to happen. For this reason, incorpo-rating in the baseline information of the future could have a greater impact in reducing the variance of thepolicy gradient estimator. PVFs focus on the variance of a future-conditional baseline (Mesnard et al.,2021)caused by the partial observability.Nota et al.(2021) factorises a statesinto an observable componentoand an non-observable oneu, and formalises the PVF as follows:vπt(ht)=/summationdisplayu∈UPµ,π(Ut=u|ht)vπ(ot,ut),(14)whereu∈Uis the non-observable component ofstsuch thats={u, o}. Notice that this method isnot taking into account actions. However, it is trivial to derive the corresponding Posterior Action-ValueFunction (PAVF) asqπ(ht,a)=R(st,at)+γvπ(ht+1).Policy-conditioned values(Harb et al.,2020;Faccio et al.,2021) are value functions that include thepolicy as an input. For example, a policy-conditioned state-action value has the form:q(s, π, a)=qπ(s, a),(15)but a representation of the policyπis used as an explicit input of the inﬂuence function. Here, the contextis the union of the current MDP statesand the policyπ, and the goal is the expected return at termination.The main diﬀerence with state-action values is that, all else being equal,q(s, π, a, g)produces diﬀerent valuesinstantlywhenπvaries, sinceπis now an explicit input. For this reason,q(s, π, a)can generalise over thespace of policies, whileqπ(s, a)cannot. Using the policy as an input raises the problem ofrepresentingapolicy in a way that can be fed to a neural network.Harb et al.(2020) andFaccio et al.(2021) propose twomethods to represent a policy. To keep our attention on the CAP, we refer to their works for further detailson possible ways to represent a policy (Harb et al.,2020;Faccio et al.,2021). Here we limit to convey thatthe problem of representing a policy has been already raised in the literature.4.6 DiscussionThe sheer variety of assignment functions described above leads to an equally broad range of metrics toquantify action inﬂuence and what is the best assignment function for a speciﬁc problem remains an openquestion. While we do not provide a deﬁnitive answer to the question of which properties are necessary orsuﬃcient for an assignment function to output a satisfactory measure of credit, we set out to draw attentionto the problem by abstracting out some of the properties that the metrics above share or lack. We identifythe following properties of an assignment function and summarise our analysis in Table3.Explicitness.We use the termexplicitnesswhen the goal appears as an explicit input of the assignmentand it is not left implicit or inferred from experience. Using the goal as an input allows generalising CAover the space of goals. The decision problem can then more easily be broken down into subroutines thatare both independent of each other and independently useful to achieve some superior goalg.Overall, explicitness allows incorporating more knowledge because the assignment spans each goal withoutlosing information about others, only limited by the capacity of the function approximator. For example,UVFAs, hindsight advantages, and future conditional advantages are explicit assignments. As discussed inthe previous section,distributional valuescan also be interpreted as explicitly assigning credit for each atom15Published in Transactions on Machine Learning Research (04/2024)NameExplicitnessRecursivityFuture-dependentCausalityState-action value◦•◦◦Advantage◦•◦◦◦GVFs/UVFAs••◦◦Distributional action-value•◦•◦◦Distributional advantage•◦◦◦•Hindsight advantage•◦◦•◦◦Counterfactual advantage•◦◦•◦•Posterior value◦◦•◦Observation-action value◦◦◦◦Policy-conditioned value◦••◦Table 3: A list of the most commonaction inﬂuencesand their assignment functions in the Deep RLliterature analysed in this survey, and the properties they respect. Respectively, empty circles, half circlesand bullets indicate that the property is not respected, that it is only partially respected, and it is fullyrespected. See Sections4.5and4.6for details.of the quantised return distribution, which is why we only partially consider them having this property inTable3. Likewise, hindsight and future-conditional advantage, while not conditioning on a goal explicitly,can be interpreted as conditioning the inﬂuence on sub-goals that are states or returns, and future statistics,respectively. For this reason, we consider them as partially explicit assignments.Recursivity.We use the termrecursivityto characterise the ability of an assignment function to supportbootstrapping(Sutton & Barto,2018). When an assignment is recursive, it respects a relationship of thetype:K(ct+1,at+1,g)=f(K(ct,at,g)),w h e r efprojects the inﬂuence from the timettot+1. For example,goal-conditionedq-values can be written as:qπ(st+1,at+1,g)=R(st,at,g)+γqπ(st,at,g),w h e r eR(st,at,g)is the reward function for the goalg.Recursivity provides key advantages whenlearningcredit, which we discuss more in detail in Section6.In theory, it reduces the variance of the estimation at the cost of a bias (Sutton & Barto,2018): sincethe agent does not complete the trajectory, the return it observes is imprecise but varies less. In practice,bootstrapping is often necessary in Deep RL when the length of the episode for certain environments makesfull Monte-Carlo estimations intractable due to computational and memory constraints.When the inﬂuence function does not support bootstrapping, the agent must obtain complete episodesto have unbiased samples of the return. For example, Direct Advantage Estimation (DAE) (Pan et al.,2022) uses the advantage function as a measure of credit, but it does not decompose the advantage into itsrecursive components that support bootstrapping (q(s, a)andv(s)), and requires full Monte-Carlo returnsto approximate it. This is often ill-advised as it increases the variance of the estimate of the return. For thisreason, we consider the advantage to only partially satisfy recursivity.Future-dependent.We use the termfuture-dependentfor assignments that take as input informationabout what actions will be or have been takenafterthe timetat which the actionAtis evaluated. This iskey because the inﬂuence of the current action depends also on what happensafterthe action. For example,picking up a key is not meaningful if the policy does not lead to opening the door afterwards.Future actions can be speciﬁedin-potentia, for example, by specifying a policy to follow after the action.This is the case of policy-conditioned value function, whose beneﬁt is to explicitly condition on the policysuch that, if the policy changes, but the action remains the same, the inﬂuence of the action changesinstantly. They can also be speciﬁedin realisation. This is the case, for example, of hindsight evaluations(Andrychowicz et al.,2017) such as the hindsight advantage, the counterfactual advantage, and the PVFwhere the inﬂuence is conditioned on some features of the future trajectory.However, these functions only considerfeaturesof the future: the hindsight advantage considers only theﬁnal state or the ﬁnal return of a trajectory; the counterfactual advantage considers some action-independent16Published in Transactions on Machine Learning Research (04/2024)features of the future; the posterior value function considers only the non-observable components. Becausefutures are not considered fully, we consider these functions as only partially specifying the future.Furthermore, while state-action value functions, the advantage and their distributional counterparts specifya policy in principle, that information is not an explicit input of the assignment, but only left implicit. Inpractice, in Deep RL, if the policy changes, the output of these assignments does not change unless retraining.Causality.We refer to acausalassignment when the inﬂuence that it produces is also a measure of causalinﬂuence (Janzing et al.,2013). For example, the counterfactual advantage proposes an interpretation of theaction inﬂuence closer to causality, by factorising the inﬂuence of an action in two. The ﬁrst factor includesonly the non-controllable components of the trajectory (e.g., exogenous reward noise, stochasticity of thestate-transition dynamics, stochasticity in the observation kernel), or those not under direct control of theagent at timet, such as future actions. The second factor includes only the eﬀects of the action alone. Theinterpretation is that, while the latter is due to causation, the former is only due to fortuitous correlations.This vicinity to causality theory exists despite the counterfactual advantage not being a satisfactory measureof causal inﬂuence as described inJanzing et al.(2013). Distributional advantage in Equation11can alsobe interpreted as containing elements of causality. In fact, we have that the expectation of the advantageover states and actions is the Conditional Mutual Information (CMI) between the policy and the return,conditioned on the state-transition dynamics:Eµ,π[DKL(Qπ(s, a)||Vπ(s))] =I(Pπ(A|S=s);Z|Pµ(S)).T h eCMI (with its limitations (Janzing et al.,2013)) is a known measure of causal inﬂuence.Overall, these properties deﬁne some characteristics of an assignment, each one bringing positive and negativeaspects. Explicitness allows maintaining the inﬂuence of an action for multiple goals at the same time,promoting the reuse of information and a compositional onset of behaviour. Recursivity ensures that theinﬂuence can be learned via bootstrapping. Future-dependency separates assignments by whether theyinclude information about future actions. Finally, causality ﬁlters out the spurious correlations evaluatingthe eﬀects of the action alone.4.7 SummaryIn this section, we addressedQ1.and discussed the problem of how to quantify action inﬂuences. InSection4.1we formalised our question: “How do diﬀerent works quantify action inﬂuences?” and “Arethese quantities satisfactory measures of credit?” . We proceeded to answer the questions. In Section4.2weformalised the concept ofoutcomeas some arbitrary function of a given history. In Section4.3we deﬁnedthe assignment function as a function that returns a measure of action inﬂuence. In Section4.4we used thisdeﬁnition to formalise the CAP as the problem of learning a measure of action inﬂuence from experience. Werefer to the set of protocols of this learning process as a credit assignmentmethod. In Section4.5we surveyedexisting measures of action inﬂuence from literature, detailed the intuition behind them, their advantagesand drawbacks. Finally, in Section4.6we discussed how these measures of action inﬂuence relate to eachother, the properties that they share and those that are rarer in literature, but still promising for futureadvancements. In the next sections, we proceed to addressQ2.. Section5describes the obstacles to solvingthe CAP and Section6surveys the methods to solve the CAP.5 The challenges to assign credit in Deep RLHaving clariﬁed what measures of action inﬂuence are available in the literature, we now look at the obstaclesthat arise to learn them and, together with Section6, answerQ2.. We ﬁrst survey the literature to identifyknown issuesto assign credit and then systematise the relevant issues into CA challenges. These challengesprovide a perspective to understand the principal directions of development of CA methods and are largelyindependent of the choice of action inﬂuence. However, using a measure of inﬂuence over another can stillimpact the prominence of each challenge.We identify the following issues to assign credit:(a)delayed rewards(Raposo et al.,2021;Hung et al.,2019;Arjona-Medina et al.,2019;Chelu et al.,2022): reward collection happens long after the action thatdetermined it, causing its inﬂuence to be perceived as faint;(b)sparse rewards(Arjona-Medina et al.,2019;17Published in Transactions on Machine Learning Research (04/2024)
(a) Depth of the MDP.
(b) Density of the MDP.
(c) Breadth of the MDP.Figure 2: Visual intuition of the three challenges to temporal CA and their respective set of solutions, usingthe graph analogy. Nodes and arrows represent, respectively, MDP states and actions. Blue nodes andarrows denote the current episode. Black ones show states that could have potentially been visited, buthave not. Square nodes denote goals. Forward arrows (pointing right) represent environment interactions,whereas backward arrows (pointing left) denote credit propagation via state-action back-ups. From top left:(a)the temporal distance between the accountable action and the target state requires propagating creditdeep back in time;(b)considering any state as a target increases the density of possible associations andreduces information sparsity; and ﬁnally,(c)the breadth of possible pathways leading to the target state.Seo et al.,2019;Chen & Lin,2020;Chelu et al.,2022): the reward function is zero everywhere, and rarelyspikes, causing uninformative Temporal Diﬀerence (TD) errors;(c)partial observability(Harutyunyanet al.,2019): where the agent does not hold perfect information about the current state;(d)high variance(Harutyunyan et al.,2019;Mesnard et al.,2021;van Hasselt et al.,2021) of the optimisation process;(e)theresort totime as a heuristicto determine the credit of an action (Harutyunyan et al.,2019;Raposo et al.,2021):(f)the lack ofcounterfactualCA (Harutyunyan et al.,2019;Foerster et al.,2018;Mesnard et al.,2021;Buesing et al.,2019;van Hasselt et al.,2021);(g)slow convergence(Arjona-Medina et al.,2019).While these issues are all very relevant to the CAP, their classiﬁcation is also tailored to control problems.Some of these are described by the use of a particular solution, such as(e), or the lack thereof, like(f),rather than by a characteristic of the decision or of the optimisation problem. Here, we systematise theseissues and transfer them to the CAP. We identify three principal characteristics of MDPs, which we refer toasdimensionsof the MDP:depth,densityandbreadth(see Figure2). Challenges to CA emerge whenpathological conditions on depth, density, and breadth produce speciﬁc phenomena that mask the learningsignal to be unreliable, inaccurate, or insuﬃcient to correctly reinforce an action. We now detail these threedimensions and the corresponding challenges that arise.5.1 Delayed eﬀects due to high MDP depthWe refer to thedepthof an MDP as the number of temporal steps that intervene between a highly inﬂuentialaction and an outcome (Ni et al.,2023). When this happens, we refer to the action as aremoteaction,and to the outcome as adelayedoutcome. When outcomes are delayed, the increase of temporal distanceoften corresponds to a combinatorial increase of possible alternative futures and the paths to get to them.In these conditions, recognising which action was responsible for the outcome is harder, since the space of18Published in Transactions on Machine Learning Research (04/2024)possible associations is very large. We identify two main reasons for an outcome to be delayed, dependingon whether the decision after the remote action inﬂuences the outcome or not.The ﬁrst reason for delayed eﬀects is that the success of the action is not immediate but requires a sequenceof actions to be performedafterwards, which causes the causal chain leading to success to be long. Thisissue originates from the typical hierarchical structure of many MDPs, where the agent must ﬁrst performa sequence of actions to reach a subjective sub-goal, and then perform another sequence to reach another.The key-to-door task (Hung et al.,2019) is a good example of this phenomenon, where the agent must ﬁrstcollect a key, to be able to open a door later.The second reason isdelayed reinforcements: outcomes are onlyobservedafter a long time horizon, and anydecision takenafterthe remote action does not inﬂuence the outcome signiﬁcantly. The phenomenon wasﬁrst noted in behavioural psychology and is known as thedelayed reinforcementproblem (Lattal,2010),Reinforcement is delayed whenever there is a period of time between the response producingthe reinforcer and its subsequent delivery.(Lattal,2010)The main challenge withdelayed reinforcementsis in being able to ignore the series of irrelevant decisionsthat are encountered between the remote action and the delayed outcome, focus on the actions that areresponsible for the outcome, and assign credit accordingly. This is a key requirement because most CAmethods rely on temporal recency as a heuristic to assign credit (Klopf,1972;Sutton,1988;Mahmood et al.,2015;Sutton et al.,2016;Jiang et al.,2021a). When this is the case, the actions in the proximity of achievingthe goal are reinforced, even if not actually being responsible for the outcome (only the remote action is),just because they are temporally close to the outcome.While recent works advance proposals on how to measure the MDP depth, for example, CA length (Ni et al.,2023), there is currently no formal agreement in the literature on how to diagnose the presence of delayedeﬀects.5.2 Low action inﬂuence due to low MDP densityIf delayed eﬀects are characterised by a large temporal distance between an action and the outcome, MDPsparsity derives from alack of inﬂuencebetween them. Even if the literature often confoundssparseanddelayedrewards, there is a substantial diﬀerence between them. With delayed eﬀects, actions can causeoutcomes very frequently, except with delay. Here, actions have little or no impact on the outcome, andoutcomes do not vary regardless of the actions taken, but in a few, rare instances. We identify two mainreasons.The ﬁrst one is highly stochastic state-transition dynamics, which can be diagnosed by measuring the entropyof the state-transition distributionH(Pµ)and/or of the reward functionH(P(R)). In highly stochastic MDPs,actions hardly aﬀect the future states of the trajectory, the agent is unable to make predictions with highconﬁdence, and therefore cannot select actions that are likely to lead to the goal.The second reason is the low goal density. This is the canonical case of reward sparsity in RL, where thegoal is only achievable in a small subset of the state space, or for a speciﬁc sequence of actions. Formally,we can measure the sparsity of an MDP using the notion of information sparsity (Arumugam et al.,2021).Deﬁnition 4(MDP sparsity).An MDP isε-information sparse if:maxπ∈ΠEµ,π[DKL(Pµ,π(Z|s, a)||Pµ,π(Z|s))]≤ε,(16)whereEµ,πdenotes the expectation over the stationary distribution induced by the policy and the state-transition dynamics. The information sparsity of an MDP is the maximum information gain that can beobtained by an agent. When this is low everywhere, and only concentrated in a small subset of decisions,CA methods often struggle to assign credit, because the probability of behaving optimally is lower (Abelet al.,2021a), and there is rarely a signal to propagate.19Published in Transactions on Machine Learning Research (04/2024)5.3 Low action inﬂuence due to high MDP breadthWe use the termbreadthof an MDP to denote the number of alternative historieshthat produce the sameoutcomeg.W e t h e n u s e t h e t e r mdilutionof credit, when many optimal pathways exist, and there is nobottleneckdecision that the agent has to necessarily make to achieve the goal. We formalise the conceptusing the notion of thenull spaceof a policy (Schaul et al.,2022):Null(π): ={Ω|vπ(s)=vπ′(s)}∀π,π′∈Ω⊆Π,∀s∈S.(17)Null(π)is thenull spaceof a policyπ, deﬁned to be the subset of the space of all policiesΩ⊂Πsuch thattwo policiesπ,π′∈Ωhave the same expected state-valuevπ(s)=vπ′(s)in all the states of the MDPs∈S.Credit dilution is often not a challenge for control because optimal behaviours are more probable. However,it can be problematic for CA. Most of the common baselines, such as Advantage Actor Critic (A2C) (Mnihet al.,2016) or Proximal Policy Optimisation (PPO) (Schulman et al.,2017), stop exploring after a smallsubset of optimal histories is found (or after a certain amount of time). Indeed, when diam(Null(π∗))islarge, there are many optimal histories. Yet, most of them are not included in the experience setCsinceexploration stopped prematurely, and credit will not be improved for those. This is particularly relevantfor assignments that measure the inﬂuence of an action relative to another. For example, the advantageAπ(s, a)=qπ(s, a)−Ea′∼π[qπ(s, a′)]is inaccurate ifE′a[qπ(s, a)]is inaccurate, which requires taccuratelyevaluatingq,∀a′∈A. This often results in a low diversity of behaviours (Parker-Holder et al.,2020), and apoor robustness to changes in the environment (Eysenbach & Levine,2022).5.4 Relationship with the exploration problemOne additional challenge in practical experiments is that it is often hard to disentangle the impacts of CAfrom those of exploration. In fact, discerning the eﬀects of the two is often only done qualitatively. Here,we discuss the connection between the two problems, if they can be studied independently, and whether itis possible to ﬁnd a way to diagnose and separate the eﬀect of one from the other.We use the interpretation ofexplorationasthe problem of acting in an unknown environment to discovertemporal sequences of states, actions and rewards with the purpose of acquiring new information(Amin et al.,2021;Jiang et al.,2023). The acquired experiences then become part of the experience setC,w h i c hi su s e dto solve the CAP as described in Equation (5).To visualise the diﬀerence between the exploration problem and the CAP, consider the usual key-to-doorenvironment, where the agent needs to pick up a key, which opens a door, behind which lies a reward. Whilehighly improbable (Abel et al.,2021a), this successful event is the result of chance and random behaviour3.Nevertheless, it is the responsibility ofexplorationtodiscoverfor theﬁrst timean optimal history, andto keep feeding the setCwith useful discoveries. Then, once the successful experienceC∗is in the setC,i tbecomes the responsibility of the CA method to consume that experience and extract a measure of inﬂuencefrom the relationship context-action-outcome (Equation (4)) that supports eﬀective improvements.This is a key diﬀerence because the very same behaviour has a diﬀerent cause whether it comes fromexploration or from CA. If due to exploration, it happens by chance, making it unlikely to occur again.If due to accurate CA, it is the result of informed decision-making, and funded on the ability to forecast(Sutton et al.,2011) the eﬀects of an action. Then, when assignments start to be accurate enough, policyimprovement further increases the probability of visiting optimal trajectories in a virtuous cycle that alsoimproves CA. Many studies show how common RL baselines often struggle to extract a reliable signal froma small set of isolated successes. This is the case, for example, of A2C (Oh et al.,2018), DQN (Schaulet al.,2015b) or PPO (Arjona-Medina et al.,2019). To further support the claim, increasing the samplingprobability of a success, for example through PER (Schaul et al.,2015b) or Self-Imitation Learning (SIL)(Oh et al.,2018), shows great improvements in CA.We can draw two conclusions from the arguments above. On one hand, if there is aminimumnumber ofoptimal trajectoriesC∗⊂CinC, exploration has done its job and failures can be attributed to poor CA. On3Or, rather, by the laws dictated by the exploration algorithm.20Published in Transactions on Machine Learning Research (04/2024)the other hand, a natural question arises: “What is the minimum rate of successesGmin=|C∗|/|C|that aCA method requires to start converging to an optimal policy?” . This is a fundamental open question in thecurrent literature, and an answer to it can produce a valid tool to evaluate a CA method. All else beingequal, the lowest the ratioC∗/C, the better the method, because it requires exploration to randomly collectoptimal histories at a lower rate, and can solve harder MDPs (Abel et al.,2021a).5.5 SummaryIn this section, we surveyed the literature and discussed both the obstacles and the current limitations tosolving the CAP. These include delayed rewards, sparse rewards, partial observability, high variance, thelack of counterfactual CA, and sample eﬃciency. Then, we systematised these issues into challenges thatemerge from speciﬁc properties of the decision problem, which we refer to as dimensions of the MDP: depth,density, and breadth. Challenges emerge when pathological conditions on these dimensions produce speciﬁcphenomena that mask the learning signal to be unreliable, inaccurate, or insuﬃcient to correctly reinforcean action: delayed eﬀects, sparsity, and credit dilution. We have provided an intuition of this classiﬁcationwith the aid of graphs and proceeded to detail each challenge. Finally, we discussed the connection betweenthe CAP and the exploration problem, suggesting a way to diagnose when a failure is caused by one or theother, and disentangling exploration from CA.With these challenges in mind, we now proceed to review the state of the art in CA, and discuss the methodsthat have been proposed to address them.6 Methods to assign credit in Deep RLFollowing the deﬁnition of CAP in Section4.4,acredit assignment methodis then an algorithm that takesan initial guess/tildewideKφ∈Kand a ﬁnite set of experienceD=(S×A×R)T, and, by sampling and learningfrom transitionsD∼PD4, it recursively produces a better approximation of the true assignmentK.In this section, we present a list of the credit assignment methods focused on Deep RL. Our classiﬁcationaims to identify the principal directions of development and to minimise the intersection between each classof methods. We aim to understand the density around each set of approaches, to locate the branchessuggesting the most promising results, and to draw a trend of the latest ﬁndings. This can be helpful toboth the researchers on the CAP who want to have a bigger picture of the current state of the art, to generalRL practitioners and research engineers to identify the most suitable methods to use in their applications,and to the part of the scientiﬁc community that focuses on diﬀerent problems, but that can beneﬁt from theinsights on CA. We deﬁne a CA method according to how it speciﬁes three elements:(a)The measure of action inﬂuence via the assignment functionK.(b)The protocol that the method uses to approximateKfrom the experienceD.(c)The mechanismPD(d)to collect and sample fromd∈D.This provides consistency with the framework just proposed and allows categorising each method by themechanisms that it uses to assign credit. Therefore, for each method, we report the three elements describedabove. We identify the following categories:1.Methods usingtime contiguityas a heuristic (Section6.1).2.Thosedecomposing returnsinto per-timestep utilities (Section6.2).3.Those conditioning onpredeﬁned goalsexplicitly (Section6.3).4.Methods conditioning the present onfuture outcomes in hindsight(Section6.4).4To enhance the ﬂow of the manuscript, we formalisecontextual distributionsin AppendixB,a n ds i n c et h e ya r ei n t u i t i v econcepts, we describe them in words when surveying the methods.21Published in Transactions on Machine Learning Research (04/2024)5.Modelling trajectories assequences(Section6.5).6.Thoseplanning or learning backwardsfrom an outcome (Section6.6).7.Meta-learningdiﬀerent proxies for credit (Section6.7).Note that, we do not claim that this list of methods is exhaustive. Rather, as in Section4.5, this taxonomy isrepresentative of the main approaches and a tool to understand the current state of the art in the ﬁeld. Weare keen to receive feedback on missing methods from the list to improve further revisions of the manuscript.We now proceed to describe the methods, which we also summarise in Table4.PublicationMethodClassDepthDensityBreadthBaird(1999)ALTime◦◦•Wang et al.(2016b)DDQNTime◦◦•Pan et al.(2022)DAETime◦◦•Klopf(1972)ETTime•◦◦Sutton et al.(2016)ETDTime•◦◦Bacon et al.(2017)Option-criticTime•◦◦Hung et al.(2019)TVTReturn decomposition•◦◦Arjona-Medina et al.(2019)RUDDERReturn decomposition•◦◦Ferret et al.(2021a)SECRETReturn decomposition•◦◦Ren et al.(2022)RRDReturn decomposition•◦◦Raposo et al.(2021)SRReturn decomposition•◦◦Sutton et al.(2011)GVFAuxiliary goals◦•◦Schaul et al.(2015a)UVFAAuxiliary goals◦•◦Andrychowicz et al.(2017)HERFuture-conditioning◦•◦Rauber et al.(2019)HPGFuture-conditioning◦•◦Harutyunyan et al.(2019)HCAFuture-conditioning◦•◦Schmidhuber(2019)UDRLFuture-conditioning◦•◦Mesnard et al.(2021)CCAFuture-conditioning◦••Nota et al.(2021)PPGFuture-conditioning◦•◦Janner et al.(2021)TTSequence modelling◦•◦Chen et al.(2021)DTSequence modelling◦•◦Goyal et al.(2019)Recall tracesBackward planning◦••Edwards et al.(2018)FBRLBackward planning◦••Nair et al.(2020)TRASSBackward planning◦••van Hasselt et al.(2021)ET(λ)Learning predecessors•◦•Xu et al.(2018)MGMeta-Learning•◦◦Yin et al.(2023)Distr. MGMeta-Learning•◦◦Table 4: List of the most representative algorithms for CA classiﬁed by the CA challenge they aim to address.For each method, we report the publication that proposed it, the class we assigned to it, and whether it isdesigned to address each challenge described in Section5. Hollow circles mean that the method does notaddress the challenge, and the full circle represents the opposite.6.1 Time as a heuristicOne common way to assign credit is to use time contiguity as a proxy for causality: an action is as inﬂuentialas it is temporally close to the outcome. This means that, regardless of the action being an actual causeof the outcome, if the action and the outcome appear temporally close in the same trajectory, the action isassigned high credit. At their foundation, there is TD learning (Sutton,1988), which we describe below.TD learning(Sutton,1984;1988;Sutton & Barto,2018) iteratively updates an initial guess of the valuefunction according to the diﬀerence between expected and observed outcomes. More speciﬁcally, the agent22Published in Transactions on Machine Learning Research (04/2024)starts with an initial guess of values, acts in the environment, observes returns, and aligns the current guessto the observed return. The diﬀerence between the expected return and the observed one is the TD errorδt:δt=R(st,at)+γqπ(st+1,at+1)−qπ(st,at)(18)withat+1∼πandst+1∼µ.When the temporal distance between the goal and the action is high – a premise at the base of the CAP– it is often improbable to observe very far rewards. As time grows, so does the variance of the observedoutcome, due to the intrinsic stochasticity of the environment dynamics, and the policy. To mitigate theissue, TD methods often replace the theoretical measure of inﬂuence with an approximation: theTD target.In TD learning, the value function is updated to approximate thetarget, and not the theoretical measureof action inﬂuence underneath it. Since policy improvement uses the current approximation of the valueto update the policy, future behaviours are shaped according to it, and theTD targetdrives the learningprocess.We separate the methods in this category in three subgroups: those speciﬁcally designed around the ad-vantage function, those re-weighing updates to stabilise learning, and those assigning credit to subsets oftemporally extended courses of actions.6.1.1 Advantage-based approachesThe ﬁrst subset of methods uses theadvantage(see Section4.5) as a measure of action inﬂuence, but stilluses time as a heuristic to learn it.Actor-Critic (AC)methods with a baseline function (Sutton & Barto,2018, Chapter 13) approximate theaction inﬂuence using some estimator of theadvantagefunction (Equation7). In fact, the policy gradientis proportional toEµ,π[(Qπ(s, a)−b(s))∇logπ(a|s)]and if we choosevπ(s)as our baselineb(s), we getEµ,π[(Aπ(s, a))∇logπ(a|s)]becauseqπ(s, a)−vπ(s, a)=Aπ(s, a). The use of an action-independent baselinefunction usually helps to reduce the variance of the evaluation, and thus of the policy gradients, whilemaintaining an unbiased estimate of it (Sutton & Barto,2018). What function to use as a baseline is thesubject of major studies, and diﬀerent choices of baselines often yield methods that go beyond using time asah e u r i s t i c(Harutyunyan et al.,2019;Mesnard et al.,2021;Nota et al.,2021;Mesnard et al.,2023).Advantage Learning (AL)Baird(1999) also uses time as a proxy for causality. There are many instancesof AL in the Deep RL literature. The Dueling Deep Q-Network (DDQN) (Wang et al.,2016b)i m p r o v e son DQN by calculating the q-value as the sum between the state-value function and a normalised versionof the advantage. Even if this results in using the q-value as a measure of action inﬂuence andK(s, a)=vπ(s)+(Aπ(s, a)−/summationtextaAπ(s, a′)/|A|), approximating the advantage is a necessary step of it.DAE (Pan et al.,2022) followsWang et al.(2016b) with the same speciﬁcation of the advantage but providesbetter connections between the advantage and causality theory. In particular, for fully observable MDPs, thecausal eﬀect of an actionaupon a scalar outcomeGis deﬁned asE[G|s, a]−E[G|s]. If we choose the returnZas outcome, this actually corresponds to the advantageE[Z|s, a]−E[Z|s]=qπ(s, a)−vπ(s), which becomesan approximate expression for the causal inﬂuence of an action upon the random return, as discussed alsoinArumugam et al.(2021). Here, the context is an MDP state, the action is the greedy action with respectto the current advantage estimation, and the goal is the expected return at termination.As explained in Section5.3, advantage can be decomposed in two termsAπ(s, a)=qπ(s, a)−vπ(s, a).S i n c evπ(s)=Eπ[qπ(s, a)], it is clear that the accuracy of the advantage depends on the accuracy of theq-valuesof all actions. It has been shown that, because of this, estimating and incorporating the advantage in theq-value has a regularisation eﬀect (Vieillard et al.,2020a). Another eﬀect is increasing the action-gap (i.e.the diﬀerence in value between the best and second-best action), which facilitates value learning. Becauseevaluations are more accurate for a greater portion of the state-action space, AL-based methods contributeto address MDP breadth, as shown in Table4.23Published in Transactions on Machine Learning Research (04/2024)6.1.2 Re-weighing updates and compound targetsThe second subset of methods in this category re-weighs temporal updates according to some heuristics,which we detail below. Re-weighing updates can be useful to emphasise or de-emphasise important statesor actions to stabilise learning in Deep RL (van Hasselt et al.,2018).Eligibility Traces (ET)(Klopf,1972;Singh & Sutton,1996;Precup,2000a;Geist et al.,2014;Mousaviet al.,2017) credit the long-term impact of actions on future rewards by keeping track of the inﬂuence of pastactions on the agent’s future reward. Speciﬁcally, an eligibility trace (Sutton & Barto,2018, Chapter 12)is a function that assigns a weight to each state-action pair, based on the recency of the last visit to it.Atraceet(s)spikes every time a state (or state-action) is visited and decays exponentially over time untilthe next visit or until it extinguishes. At each update, the TD error, which determines the magnitude ofthe update, is scaled by the value of the trace at that state, andδETt=δtet(s). There are several types ofeligibility traces, depending on the law of decay of the trace. For example, with accumulating traces (Klopf,1972), every visit causes an increment of the trace. Replacing traces (Singh & Sutton,1996) are capped toa speciﬁc value, instead.Deep Q(λ)-Networks (DQ(λ)Ns) (Mousavi et al.,2017) implement eligibility traces on top of DQN (Mnihet al.,2015). Here, the eligibility trace is a vectore∈Rdwith the same number of componentsdas theparameters of the DNN, and the action inﬂuence is measured by theq-value with parameters setθ∈Rd.The context is an MDP state, the action is an oﬀ-policy action in a transition arbitrarily chosen from thebuﬀer; the goal is the expected return. The ET information is embedded in the parametersθsince they areupdated according toθ←θ+δe. Hereeis the eligibility trace, incremented at each update by the valuegradient (Sutton & Barto,2018, Chapter 12):e←γλe+∇θqπ(s, a).Finally, successive works advanced on the idea of ETs, and proposed diﬀerent updates for the eligibilityvector (Singh & Sutton,1996;van Hasselt & Sutton,2015;Precup,2000a).Emphatic Temporal Diﬀerences (ETDs)(Sutton et al.,2016;Mahmood et al.,2015;Jiang et al.,2021b) continue on the idea of ETs to weigh TD updates with a trace. They aim to address the issue thatcanonical ETs may suﬀer from early divergence when combined with non-linear function approximation andoﬀ-policy learning. The re-weighing in ETD is based on theemphatic trace, which encodes the degree ofbootstrapping of a state.Originating from tabular and linear RL, the intuition behind ETDs is that states with high uncertainty – thestates encountered long after the state-action pair of evaluation – are more reliable, and vice versa. The mainadaptation of the algorithm to Deep RL is byJiang et al.(2021b), who propose the Windowed EmphaticTD(λ) (WETD) algorithm. In this approach, ETD is adapted to incorporate update windows of lengthn,introducing a mixed update scheme where each state in the window is updated with a variable bootstrappinglength, all bootstrapping on the last state in the window. The inﬂuence of an action in WETD is the sameas for any other ET, but the trace itself is diﬀerent and measures the amount of bootstrapping of the currentestimate.ETDs provide an additional mechanism to re-weigh updates, the interest functioni:S→[0,∞).B yemphasising or de-emphasising the interest of a state, the interest function can be a helpful tool to encodethe inﬂuence of the actions that had led to that state. Because hand-crafting an interest function requireshuman interventions, allowing suboptimal and biased results,Klissarov et al.(2022) proposes a method tolearn and adapt the interest function at each update using meta-gradients. Improvements on both discretecontrol, such as ALE, and on continuous control problems, such as MuJoCo (Todorov et al.,2012), suggestthat the interest function can be helpful to assign credit faster and more accurately.Re-weighing updates includes a set of techniques to adjust the inﬂuence of past actions based on theirtemporal proximity to the current state. Such methods aim to mitigate the limitations of TD methodsby dynamically adjusting the weight assigned to past actions, thereby emphasizing or de-emphasizing theircontribution to future rewards. For this reason, these methods can be seen as potential solutions to mitigatethe impacts of delayed eﬀects and improve credit assignment in settings with high MDP depth, as shown inTable4.24Published in Transactions on Machine Learning Research (04/2024)6.1.3 Assigning credit to temporally extended actionsThe third and last subset of methods in this category assigns credit to temporally extended actions ratherthan a single, atomic action. This is formalised in theoptions framework(Sutton et al.,1999;Precup,2000b).For the purpose of CA,options, also known asskills(Haarnoja et al.,2017;Eysenbach et al.,2018), can bedescribed as the problem of achievingsub-goals, such that an optimal policy can be seen as the compositionof elementary behaviours. For example, in a key-to-door environment, such as MiniGrid (Chevalier-Boisvertet al.,2018) or MiniHack (Samvelyan et al.,2021) the agent might select the optionpick up the key, followedbyopen the door. Each of this macro-action requires a lower level policy to be executed. For example,pick upthe keyrequires selecting the actions that lead to reach the key before grabbing it. In the option framework,credit is assigned for each speciﬁc subgoal (the macro-action), with the beneﬁts already described for theexplicitnessproperty, from Section4.6. The idea stems from the intuition that it is easier to assign credit tomacro-actions since a sequence of options is usually shorter than a sequence of atomic actions, reducing theoverall temporal distance to the time of achieving the goal.However, since the option literature often does not explicitly condition on goals, but uses other devices todecompose the CA problem, we review works about learning options next, and dedicate a separate sectionto auxiliary goal-conditioning in Section6.3.The option-critic architecture(Bacon et al.,2017) scales options to Deep RL and mirrors the actor-critic architecture but considering options rather than actions. The option-critic architecture allows learningboth how to execute a speciﬁc option, and which option to execute at each time simultaneously and online.The option executes using thecall-and-returnmodel. Starting from a states, the agent picks an optionωaccording to its policy over optionsπΩ. This option then determines the primitive action selection processthrough the intra-policyπωuntil the option termination functionβsignals to stop. Learning options, andassigning credit to its actions, is then possible using theintra-option policy gradientand theterminationgradienttheorems (Bacon et al.,2017), which deﬁne the gradient (thus the corresponding update) for allthree elements of the learning process: the optionω∈Ω, their termination functionβ(s)and the policyover optionsπΩ. Here, the context is a states∈S, the actions to assign credit to are both the intra-optionactiona∈Aand the optionω∈Ω, and the goal is to maximise the return.On the same lines,Riemer et al.(2018) proposehierarchical option-critics, which allows learning options atmultiple hierarchical levels of resolution – nested options – but still only on a ﬁxed number of pre-selectedoptions.Klissarov & Precup(2021) further improve on this method by updating all options with a singlebatch of experience.In the context of the option-critic architecture, CA occurs at multiple levels of the hierarchy. At thelower, intra-option level, where individual actions are taken, credit assignment involves determining thecontribution of each action to the achievement of sub-goals. This is essential for learning eﬀective policies forexecuting primitive actions within each option. At the higher level of the hierarchy, credit assignment involvesattributing credit to options for achieving higher-level goals and involves identifying the contribution of eachoption to achieving the overall task objective. The hierarchical structure of the option-critic architecturefacilitates credit assignment by decomposing the learning problem into multiple levels of abstraction. Fortheir ability to decompose a bigger task into smaller sub-problems, these methods naturally improve creditassignment when eﬀects are delayed and in settings with high MDP depth (see Table4).6.1.4 Summary and discussionThe methods we covered in this section use the temporal distance between the context-action pair and areward to measure the action inﬂuence. The closer is the action, the higher is its inﬂuence and vice versa.While this could maybe be a reasonable assumption when the policy is optimal, it is not the case for the earlyexploratory stages of learning. In fact, as described in Section5.4, highly inﬂuential actions are often takenlong before their rewards are collected while exploring. For example, in our usual key-to-door example, theagent would pick up the key, perform hundreds of random, unnecessary actions, and the goal-tile only reachedafter those. In these cases, the two events are separated by a “multitude of[random and non-inﬂuential]25Published in Transactions on Machine Learning Research (04/2024)decisions”(Minsky,1961). Because these non-inﬂuential actions are temporally closer to reaching the goal-tile than that of picking up the key, these methods mistakenly assign them high inﬂuence and, in particular,a higher inﬂuence than to pick up the key.Today, methods that assign credit only by looking at the temporal distance between the action and theoutcome usually underperform on tasks with delayed eﬀects (Arjona-Medina et al.,2019). Nevertheless,some of the branches in this category improve assignments in condition of high MDP depth by re-weighingupdates, using advantage or breaking down the task into multiple, composable subtasks.6.2 Decomposing return contributionsTo improve CA in settings with high MDP depth, the line of research we describe next focuses on decomposingreturns into per-timestep contributions. These works interpret the CAP as aredistributionproblem: thereturn observed at termination is re-distributed to each time-step with an auxiliary mechanism that dependson each method and complements TD learning.Temporal Value Transport (TVT)(Hung et al.,2019) uses an external long-term memory system toimprove on delayed tasks. The memory mechanism is based on the Diﬀerentiable Neural Computer (DNC)(Grefenstette et al.,2015;Graves et al.,2016), a neural network then reads events from an external memorymatrix, represented as the hidden state of a Long-Short-Term-Memory (LSTM). The agent decides to readfrom and write into it. To write, state-action-reward triples are projected to a lower dimensional space,and processed by the DNC. During training, this works as a trigger: when a past state-action pair is readfrom memory, it gets associated with the current one, transporting the state-action value – credit – from thepresent to the remote state. To read, the state-action-reward is reconstructed from the latent code. Duringinference, this acts as a proxy for credit. If a past state-action-reward triple is retrieved from the memory, itmeans that it is correlated with the current return. This allows to use the retrieval score of a past transitionas a measure of the inﬂuence of its action.Return Decomposition for Delayed Rewards (RUDDER)(Arjona-Medina et al.,2019) stems fromthe intuition that, if we can construct a reward function thatredistributesthe rewards collected in a trajectorysuch that the expected future reward is zero, we obtain an instantaneous signal that immediately informsthe agent about future rewards. The method proposes to learn a functionf:(S×A)T→Rthat mapsa sequence of state-action pairs to the sum of discounted rewards, including the past, present and futurerewards. In practice,fis implemented as an LSTM, which is trained to ﬁt a subset of the whole experiencesetDr⊂D.Dris constructed to contain only trajectories containing delayed rewards, and experience issampled proportionally to the current prediction error. The underlying hypothesis is that, by ﬁtting thereturn, the LSTM’s hidden state holds useful information to redistribute the return to the most relevanttransitions in the sequence.Oncefrepresents a faithful model of the return, at each iteration of the RL algorithm, RUDDER uses theLSTM to infer the return for eachst,atind∼Pµ,π.I tt h e nu s e st h ed i ﬀ e r e n c eb e t w e e nt h ei n f e r r e dr e t u r n s(i.e., the redistributed returns) at two consecutive time steps as a reward to perform canonical TD learning.This quantity, represents the credit of a state-action pair:K(st,at)=f(st+1,at+1)−f(st,at)(19)Here,f(st+1,at+1)−f(st,at)=R∗(st,at)is the reward function ofM∗=(S,A,R∗,µ ,γ), an MDP return-equivalent toM=(S,A,R ,µ ,γ):MandM∗have the same set of optimal policies, but the reward functionR∗ofM∗is such that the sum of expected future rewards is zero for all states (all the future rewards arepaid in the current state). The context is a historyh={ot,at,rt:0≤t≤T}from the assigned MDP, theaction is an action from the trajectorya∈h, and the goal is the achieved return.Self-Attentional Credit Assignment for Transfer (SECRET)(Ferret et al.,2021a) uses a causalTransformer-like architecture (Vaswani et al.,2017) with a self-attention mechanism (Lin et al.,2017)i nt h estandalone supervised task of reconstructing the sequence of rewards from observations and actions. It thenviews attention weights over past state-action pairs as credit for the generated rewards. This was shown to26Published in Transactions on Machine Learning Research (04/2024)help in settings of high MDP depth in a way that transfers to novel tasks when trained over a distributionof tasks. We can write its measure of action inﬂuence as follows:K(st,at)=T/summationdisplayt=1
{St=s, At=a}T/summationdisplayi=tαt←iR(si,ai).(20)Here,αt←iis the attention weight on(oi,ai)when predicting the rewardrj. Also, here the context is ahistoryh, the action is an action from the trajectorya∈h, and the goal is the achieved return.Synthetic returns (SR)(Raposo et al.,2021) assume only one state-action to be responsible for theterminal reward. They propose a form of state pairs association where the earlier state (theoperant)i saleading indicator of the reward obtained in the later one (thereinforcer). The association model is learnedwith a form of episodic memory. Each entry in the memory buﬀer, which holds the states visited in thecurrent episode, is associated with a reward – thesyntheticreward – via supervised learning. At trainingtime, this allows propagating creditdirectlyfrom the reinforcer to the operant, bypassing the local temporaldiﬀerence. When this reward model is accurately learned, each time the operant is observed, the syntheticreward model spikes, indicating a creditable state-action pair. Here the synthetic reward acts as a measureof causal inﬂuence, and we write:K(s, a)=qπ(s, a)+f(s).(21)Heref(s)is the synthetic reward function, and it is trained with value regression on the loss||rt−u(st)/summationtextt−1k=0f(st)−b(st)||2,w h e r eh(st)andb(st)are auxiliary neural networks optimised together withf. As forArjona-Medina et al.(2019), the contextcis a historyhfrom the assigned MDP, the action is anaction from the trajectorya∈h, and the goal is the achieved return. This method is, however, stable onlywithin a narrow range of hyperparameters and assumes that only one single action is to be credited.6.2.1 Summary and discussionThe methods in this section assign credit by decomposing returns into per time-step contributions and thenlearning values from this new, clearer reward signal. For the purposes of this survey, they mainly diﬀer bythe method used to redistribute the contributions to each context-action pair. TVT uses an external memorysystem, RUDDER uses contribution analysis, SECRET exploits the Transformer’s self-attention weights, SRuse a gating function. Their motivation stems from improving on delayed eﬀects, which they often stateas an explicit goal, and for this reason, we report them as improving CA in settings of high MDP depth(Table4).Indeed, the empirical evidence they provide suggests that improvements are consistent, andredistributionmethods provide beneﬁts over their TD learning baselines. On the other hand, these methods do not provideformal guarantees that the assignments improve over TD learning, and there is currently a gap to ﬁll tojustify these improvements also theoretically. This is the case, for example, of other methods (Harutyunyanet al.,2019;Wang et al.,2016b;Mesnard et al.,2021;van Hasselt et al.,2021) that prove to reduce thevariance of the evaluation, some of which we describe in later sections.6.3 Conditioning on a predeﬁned set of auxiliary goalsThe methods in this category evaluate actions for their ability to achieve multiple goals explicitly. Theydo so by conditioning the value function on a goal and then using the resulting value function to evaluateactions. The intuition behind them is that the agent’s knowledge about the future can be decomposed intomore elementary associations between states and goals. We now describe the two most inﬂuential methodsin this category.General Value Functions (GVFs)(Sutton et al.,2011), described in Section4.5, stem from the ideathat knowledge about the world can be expressed in the form of predictions. These predictions can thenbe organised hierarchically to solve more complex problems. While GVFs carry several modiﬁcations to the27Published in Transactions on Machine Learning Research (04/2024)canonical value, we focus on its goal-conditioning for the purpose of this review, which is also its foundationalidea. GVFs conditions the action value on a goal to express the expected return with respect to the rewardfunction that the goal induces. In their original formulation (Sutton et al.,2011), GVFs are a set of valuefunctions, one for each goal. The goal is any object in a predeﬁned goal set of MDP statesg∈S, and theresulting measure of action inﬂuence is the following:K(s, a, g)=qπ(s, a, g),(22)that is theq-function with respect to the goal-conditioned reward functionR(s, a, g),w h i c hi s0everywhere,and1when a certain state is reached. Because GVFs evaluate an action for what it is going to happen in thefuture, GVFs are forward methods, and interpret the CAP as a prediction problem: “What is the expectedreturn of this action, given thatgis the goal?” .Universal Value Functions Approximators (UVFAs)(Schaul et al.,2015a) scale the idea of GVFsto a large set of goals, by using a single value function to learn the whole space of goals. One major beneﬁtof UVFAs over GVFs is that they are readily applicable to Deep RL by simply adding the goal as an input tothe value function approximator. This allows the agent to learn end-to-end with bootstrapping and allowsfor exploiting a shared prediction structure across diﬀerent states and goals. Since they derive from GVFs,UVFA share most of their characteristics. The context is an MDP states∈S; the goal is still any objectin a predeﬁned goal set of states,g∈S, and the credit of an action is the expected return of the rewardfunction induced by the goal (see Equation (22)).6.3.1 Summary and discussionThe methods in this category stand out for using anexplicitgoal to assign credit, as described in Section4.6.What distinguishes these methods from those that follow in the next section (which also use goals explicitly)is their ﬂexibility. While in hindsight methods choose the goal after completing a trajectory, or based oninformation acquired during training, these methods do not. Instead, the set of goals of a GVF is predeﬁnedinSutton et al.(2011). UVFAs, even if they can generalise to new goals in theory, they are designed withthat purpose in mind, and their application is limited. This represents both a strong limitation of thesemethods, and a gap to ﬁll in the literature, since it limits both their ﬂexibility and their autonomy to adaptto diﬀerent tasks, requiring the human designer to specify the set of goalsex-anteand to provide the set ofgoals as an input at the start of training.Furthermore, their interpretation of credit is still linked to the idea of temporal contiguity described inSection6.1. For this reason, they share many drawbacks and limitations with those methods and performpoorly when the MDP is deep, especially if not accompanied by more advanced techniques. To the bestof our knowledge, there are no examples in the literature that pair these methods with more advanced CAtechniques (e.g.,options), which represents a gap to ﬁll.On the other hand, by specifying a goal explicitly (GVFs) and by generalising over the goal space (UVFA),conditioning on a predeﬁned set of goals provides a way to extract signals from the environment even whenthe signal is sparse and action inﬂuence is low. In fact, even when the main task is complex, the set ofauxiliary goals is designed to provide a useful signal for learning. This is the reason why we consider thesemethods improving CA when the MDP is sparse (see Table4).6.4 Conditioning in hindsightThe methods in this category are characterised by the idea of re-evaluating the action inﬂuence according towhat the agent achieved, rather than what it was supposed to achieve. This means that, given a trajectoryh, we can choose some goalg∈G(aftercollectingh) and evaluate the inﬂuence of all the actions inhuponachievingg.We separate the methods in this category into three subgroups.(i)Those that re-label the past experience under a diﬀerent perspective, such as achieving a diﬀerentgoal than the one the agent started.28Published in Transactions on Machine Learning Research (04/2024)(ii)Those that condition the action evaluation on some properties of the future during training, whichbecomes an explicit performance request at inference time.(iii)Those that condition on future factors that are independent on the evaluated action, but that stillinﬂuence future returns.6.4.1 Relabelling experienceHindsight Experience Replay (HER)(Andrychowicz et al.,2017) stems from the problem of learningin sparse rewards environments, which is an example of low action inﬂuence in our framework (see Section5.2.The method exploits the fact that even if a trajectory is suboptimal for the overall implicit goal to maximiseMDP returns, it can be viewed as optimal if the goal is to achieve its ﬁnal state.In practice, HER brings together UVFAs and experience replay (Lin,1992) to re-examine trajectories. Aftercollecting a set of trajectories from the environment, the agent stores each transition in a replay buﬀer,together with both the state it sought to reach and the one that it actually did reach. This allows optimising/tildewideKφ(s, a, g)for both goals. We refer to this process of re-examining a trajectory collected with a prior goalin mind and evaluating it according to the actually realised outcome ashindsight conditioning, which is alsothe main innovation that HER brings to the CAP. Notice that the original goal is important because thetrajectory is collected with a policy that aims to maximise the return for that speciﬁc goal.However, in HER, the goal set is still predeﬁned, which requires additional speciﬁcations from the agent-designer and can limit the autonomy of the overall agent, which increases the autonomy of the agent. HERuses the goal-conditionedq-values described in Section6.3to measure action inﬂuence:K(st,at,sT)=qπ(st,at,sT).(23)Here the context is a history from the MDP, the action is an action from the trajectorya∈h, and the goalgis to visit a statesTat the end of a trajectory.Since HER is limited to oﬀ-policy learning with experience replay,Hindsight Policy Gradients(HPGs)(Rauber et al.,2019) transfers the ﬁndings of HER to Policy Gradient (PG) methods, and ex-tend it to online settings. Instead of updating the policy based on the actual reward received, HindsightPolicy Gradient (HPG) updates the policy based on the hindsight reward, which is calculated based on thenew goals that were deﬁned using HER. The main diﬀerence with HER is that in HPGs, both the critic andthe actor are conditioned on the additional goal. This results in a goal-conditioned policyπ(·|S=s, G=g),describing the probability of taking an action, given the current state and a realised outcome. The actioninﬂuence used in HPG is the advantage formulation of the hindsight policy gradients:K(s, a, g)=qπ(s, a, g)−vπ(s, g),(24)whereqπ(s, a, g)andvπ(s, g)are the goal-conditioned value functions. Here the contextcis a historyh={ot,at,rt:0≤t≤T}, the goal is arbitrarily sampled from a goal set,g∈G. Like HER, HPGis tailored to tasks with low action inﬂuence due to low MDP density, and it is shown to be eﬀective insparse reward settings. Overall, HER and HPG are the ﬁrst completed work to talk abouthindsightas there-examination of outcomes for CA. Their solution is not particularly interesting for the CAP as they do notcast their problem as a CAP and they do not connect the ﬁnding to the CAP explicitly. However, they arekey precursors of the methods that we review next, which instead provide novel and reusable developmentsfor CAP speciﬁcally.6.4.2 Conditioning on the futureHindsight Credit Assignment (HCA)Traditional reinforcement learning algorithms often strugglewith credit assignment as they rely solely on foresight: they evaluate actions against a predetermined goal,selectedbeforeacting. These methods operate under the assumption that we lack knowledge of what occursbeyond a given time step, making accurate credit assignment challenging, especially in tasks with delayedeﬀects. (Harutyunyan et al.,2019), on the other hand, centres on utilising hindsight information, acknowledg-ing that credit assignment and learning typically take place after the agent completes its current trajectory.29Published in Transactions on Machine Learning Research (04/2024)This approach enables us to leverage this additional data to reﬁne the learning of critical variables necessaryfor credit assignment.(Harutyunyan et al.,2019) introduces a new family of algorithms known as Hindsight Credit Assignment(HCA). Hindsight Credit Assignment (HCA) algorithms explicitly assign credit to past actions based onthe likelihood of those actions having been taken, given that a certain outcome has been observed. This isachieved by comparing a learnedhindsight distributionover actions, conditioned by a future state or return,with the policy that generated the trajectory.More precisely, thehindsight distribution,h(a|st,π,g)is the likelihood of an actiona, given the outcomegexperienced in the trajectoryd∼Pµ,π(D|S0=s, at∼π). In practice,Harutyunyan et al.(2019) consider twoclasses of outcomes: states and returns. We refer to the algorithms that derive from these two classes of goalsasstate-HCAandreturn-HCA. For state-HCA, the contextcis the current statestat timet; the outcomeis a future state in the trajectoryst′∈dwheret′>t; the credit is the ratio between the state-conditionalhindsight distribution and the policyht(a|st,s′t)π(a|st). For return-HCA, the contextcis identical; the outcomeis the observed returnZt; the credit is the ratio between the return-conditional hindsight distribution andthe policy1−π(a|st)ht(a|st,Zt). The resulting ratios provide a measure of how crucial a particular action was inachieving the outcome. A ratio deviating further from 1 indicates a greater impact (positive or negative) ofthat action on the outcome. For example, return-HCA measures the inﬂuence of an action with thehindsightadvantagedescribed in Section4:K(st,at,zt)=/parenleftbigg1−π(at|St=st)Pµ,π(at|St=st,Zt=zt)/parenrightbiggzt.(25)To compute thehindsight distribution, HCA algorithms employ a technique related to importance sampling.Importance sampling estimates the expected value of a function under one distribution (thehindsight distri-bution) using samples from another distribution (the policy distribution). In the context of HCA, importancesampling weights are determined based on the likelihood of the agent taking each action in the trajectory,given the hindsight state compared to the likelihood of the policy for that same action. Once the hindsightdistribution is computed, HCA algorithms can be used to update the agent’s policy and value function. Oneapproach involves using the hindsight distribution to reweight the agent’s experience. This means the agentwill learn more from actions that were more likely to have contributed to the observed outcome.Besides advancing the idea of hindsight, (Harutyunyan et al.,2019) carries one novelty: the possibility to dropthe typical policy evaluation settings, where the goal is to learn a value function by the repeated applicationof the Bellman expectation backup. Instead, action values are deﬁned as a measure of the likelihood thatthe action and the outcome appear together in the trajectory, and are a precursor of the sequence modellingtechniques described in the next section (Section6.5).Upside-Down RL (UDRL)(Schmidhuber,2019;Srivastava et al.,2019;Ashley et al.,2022;Štruplet al.,2022) is another implementation of the idea to condition on the future. The intuition behind Upside-Down RL (UDRL) is that rather than conditioning returns on actions, which is the case of the methods inSection6.1, we can invert the dependency and condition actions on returns instead. This allows using returnsas an input and inferring the action distribution that would achieve that return. The action distribution isapproximated using a neural network, thebehaviour policy, that is trained via maximum likelihood estimationusing trajectories collected online from the environment. In UDRL the context is a completed trajectoryd; the outcome is a command that achieves the returnZkinH=T−ktime-steps, which we denote asg=(Zk,H); the credit of an actionais its probability according to the behaviour function,π(a|s, g).I naddition to HCA, UDRL also conditions the return to be achieved in a speciﬁc timespan.Posterior Policy Gradients (PPGs)(Nota et al.,2021) further the idea of hindsight to provide lower-variance, future-conditioned baselines for policy gradient methods. At the base of PPG there is a novelvalue estimator, the PVF. The intuition behind PVFs is that in POMDPs the state value is not a validbaseline because the true state is hidden from the agent, and the observation cannot provide as a suﬃcientstatistic for the return. However, after a full episode, the agent has more information to calculate a better,a30Published in Transactions on Machine Learning Research (04/2024)posterioriguess of the state value at earlier states in the trajectory.Nota et al.(2021) refers to the family ofpossiblea posterioriestimations of the state value as the PVF. Formally, a PVF decomposes a state into itscurrent observationot, and some hidden state that is not observable and typically unknownbt. The value of astate can then be written as the expected observation-action value function over the possible non-observablecomponentsuT∈U=Rd. The action inﬂuence of a PPG is quantiﬁed by the expression:K(ot)=Eu∈U[P(ut=u|ht)v(ot,ut)].(26)Notice that, as explained in Section4.5, the PVF does not depend on an action. However, we can derivethe corresponding action-value formulation withqπ(ht,a)=R(st,at)+γvπ(ht+1). Here, the context is anobservation, the action is the current action and the goal is the observed return. In practice, PVF advancesHCA by learning which statistics of the trajectoryψ(d)are useful to assign credit, rather than specifying itobjectively as a state or a return.6.4.3 Exposing irrelevant factorsCounterfactual Credit Assignment (CCA)For being data eﬃcient, credit assignment methods needto disentangle the eﬀects of a given action of the agent from the eﬀects of external factors and subsequentactions. External factors in reinforcement learning are any factors that aﬀect the state of the environment orthe agent’s reward but are outside the agent’s control. This can include things like the actions of other agentsin the environment, changes in the environment state due to natural processes or events. These factors canmake credit assignment diﬃcult because they can obscure the relationship between the agent’s actions andits rewards.Mesnard et al.(2021) proposes to get inspiration from the counterfactuals from causality theory to improvecredit assignment in model-free reinforcement learning. The key idea is to condition value functions on futureevents, and learn to extract relevant information from a trajectory. Relevant information here correspondsto all information that is predictive of the return while being independent of the agent’s action at timet.This allows the agent to separate the eﬀect of its own actions,the skills, from the eﬀect of external factorsand subsequent actions, theluck, which will enable reﬁned credit assignment and therefore faster and morestable learning. It shows that these algorithms have provably lower variance than vanilla policy gradient, anddevelops valid, practical variants that avoid the potential bias from conditioning on future information. Onevariant explicitly tries to remove information from the hindsight conditioning that depends on the currentaction while the second variant avoids the potential bias from conditioning on future information thanks toa technique related to important sampling. The empirical evidence inMesnard et al.(2021) suggests thatCCA oﬀers great improvements in tasks with delayed eﬀects.6.4.4 Summary and discussionThe methods in this section bring many independent novelties to CA. The most relevant for our scope isthe idea of hindsight conditioning, which can be summarised as evaluating past actions using additionalinformation about the future, usually not available at the time the action was taken. They diﬀer from thosein Section6.3, as they do not act on a pre-deﬁned objective set of goals, but these are chosenin hindsight.One drawback of these methods is that they must be able to generalise to a large goal space to be eﬀective,which is not a mild requirement because the ability to generalise often correlates with the size of the network.This can limit the applicability of the method, especially in cases of low computation and memory budgets.One of the greatest beneﬁts of these methods is to always have a signal to learn from because, by construction,there is always a goal that has been achieved in the current trajectory, for example, the ﬁnal state, or theterminal return. This, in turn, produces a higher number of context-action-outcome associations, translatesinto additional training data that is often beneﬁcial in supervised problems, and results in an overall densersignal. These improvements in MDPs with low density, which we report in Table4, are supported by bothempirical evidence and theoretical guarantees to reduce the variance of the evaluations (Harutyunyan et al.,2019;Wang et al.,2016b;Mesnard et al.,2021;van Hasselt et al.,2021). Incorporating information aboutthe future (for example, future returns or states), is most likely one major reason why these algorithms31Published in Transactions on Machine Learning Research (04/2024)overperform the others. In fact, when this information is designed to express particular features, such asaction-independence or independence to irrelevant factors, such as inMesnard et al.(2021), the gap increaseseven further.Finally, some of these methods (Mesnard et al.,2021) also incentivise the discovery of multiple pathwaysto the same goal, by identifying decisions that are irrelevant to the outcome, resulting in the fact that anyof them can be taken without aﬀecting the outcome. The only requirement is to employ an actor-criticalgorithm, which we consider a mild assumption, since transitioning from actor-critic to value-based settingsis usually trivially achievable.6.5 Modelling trajectories as sequencesThe methods in this category are based on the observation that RL can be seen as a sequence modellingproblem. Their main idea is to transfer the successes of sequence modelling in Natural Language Processing(NLP) to improve RL.On a high level, they all share the same assumption: a sequence in RL is a sequence of transitions(s, a, r),and they diﬀer in either how to model the sequence, the problem they solve, or the speciﬁc method theytransfer from NLP.Trajectory Transformers (TTs)(Janner et al.,2021) implements a decoder-only (Radford et al.,2018;2019) Transformer (Vaswani et al.,2017) to model the sequence of transitions. TTs learn from an observa-tional stream of data, composed of expert demonstrations resulting in an oﬄine RL training protocol. Themain idea of TTs is to model the next token in the sequence, which is composed by the next state, thenext action, and the resulting reward. This enables planning, which TTs exploit to plan via beam search.Notice that, for any of these paradigms, if the sequence model is autoregressive – the next prediction dependsonly on the past history, but since a full episode is available, the future-conditioned probabilities are stillwell-deﬁned, and also TTs can condition on the future. In TTs the action inﬂuence is the product betweenthe action probability according to the demonstration dataset and itsq-value:K(st,at,zt)=Pθ(At=at|Zt=zt)qπ(st,at).(27)Here, the contextcis an MDP statec=s∈S, the action is arbitrarily selected, and the goal is the returndistributionP(Z).Decision Transformers (DTs)(Chen et al.,2021) proceed on the same lines as TTs but ground theproblem in learning, rather than planning. DTs interpret a sequence as a list of(st,at,Zt)triples, whereZtis the return-to-go. They then use a Transformer to learn a model of the actor that takes the currentstate and the return as input and outputs a distribution over actions. In addition, they optionally learn amodel of the critic as well, which takes the current state and each action in the distribution to output thevalue of each action. The sequences are sampled from expert or semi-expert demonstrations, and the modelis trained to maximise the likelihood of the actions taken by the expert. From the perspective of CA, TTsand DTs are equivalent, and they share the same limitation in that they struggle to assign credit accuratelyto experience beyond that of the oﬄine dataset. Furthermore, like HCA (Harutyunyan et al.,2019), DTsbring more than one novelty to RL. Besides modelling the likelihood of the next token, they also use returnsas input to the model, resulting in a form of future conditioning. However, for CA and this section, weare only interested in their idea of sequence modelling and we will not discuss the other novelties. Thereexist further extensions to DT both to online settings (Zheng et al.,2022) and to model quantities beyondthe return (Furuta et al.,2022). The former allows assigning credit by modelling transition sequences inonline settings. The latter, instead, generalises sequence modelling to transitions with additional arbitraryinformation attached – the same way, Future-Conditional Policy Gradient (FC-PG) generalise HCA.6.5.1 Summary and discussionSequence modelling in RL transfers the advances in sequence modelling for NLP to Deep RL setting. Themain idea is to measure credit by estimating the probability of the next action (or the next token), conditioned32Published in Transactions on Machine Learning Research (04/2024)on the context and the goal deﬁned in hindsight, according to an oﬄine dataset of expert trajectories (Chenet al.,2021;Janner et al.,2021).While some works propose adaptation to online ﬁne-tuning (Lee et al.,2022), these methods mostly learnfrom oﬄine datasets and the idea to apply sequence modelling online is underexplored. This representsa strong limitation as it limits the generalisation ability of these methods. For example, DT often fail togeneralise to returns outside the training distribution.The distribution that measures this likelihoodP(a|c, g)can be interpreted as the hindsight distribution(Harutyunyan et al.,2018) described in Section6.4. Their development has a similar pattern to that ofhindsight methods and progressively generalises to more complex settings, such as online learning (Zhenget al.,2022) and more general outcomes (Furuta et al.,2022). In practice, these two trends converge togetherto model the likelihood of action, states and rewards, which hindsight methods call thehindsight distribution.Yet, this set of methods would beneﬁt from a better connection to the RL theory. This has been the casefor hindsight methods, which leverage notions from causality and the policy gradient theorem (Sutton &Barto,2018) to achieve better experimental results (Mesnard et al.,2021). For the same reasons explainedfor hindsight methods in Section6.4, these methods improve CA when the MDP has low density and theaction inﬂuence is low (see Table4).Nevertheless, sequence modelling remains a promising direction for CA, especially for their ability to scaleto large datasets (Reed et al.,2022). It is not clear how these methods position with respect to the CAchallenges described in Section5, for the lack of experimentation on tasks that explicitly stress the agent’sability to assign credit. However, in their vicinity to future-conditioned methods, they bear some of the sameadvantages and also share some limitations. In particular, for their ability to deﬁne outcomes in hindsight,regardless of an objective learning signal, they bode well in tasks with low action inﬂuence.6.6 Planning and learning backwardsThe methods in this category extend CA to potential predecessor decisions that have not been taken, butcould have led to the same outcome (Chelu et al.,2020). The main intuition is that, in environments withlow action inﬂuence, highly inﬂuential actions are rare, and when a goal is achieved the agent should usethat event to extract as much information as possible to assign credit to relevant decisions.We divide the section into two major sub-categories, depending on whether the agent identiﬁes predecessorstates by planning with an inverse model, or by learning relevant statistics without it.6.6.1 Planning backwardsRecall traces(Goyal et al.,2019) combine model-free updates from Section6.1with learning a backwardmodel of the environment. A backward modelµ−1(st−1|St=s, At−1=a)describes the probability of astateSt−1being the predecessor of another states, given that the actionawas taken. This backward actionis sampled from abackward policy,πb(at−1|st), which predicts the previous action, and abackward dynamics.By autoregressively sampling from the backward policy and dynamics, the agent can cross the MDP back-wards, starting from a ﬁnal state,sT, up until a starting state,s0to produce a new trajectory, calledrecalltrace. This allows the agent to collect experience that always leads to a certain state,sT, but that does sofrom diﬀerent starting points, discovering multiple pathways to the same goal.Formally, the agent alternates between steps of GPI via model-free updates and steps of behaviour cloningon trajectories collected via the backward model. Trajectories are reversed to match the forward arrow oftime before cloning. This is a key step towards solving the CAP as it allows propagating credit to decisionsthat have not been taken but could have led to the same outcome without interacting with the environmentdirectly. Recall-traces measure the inﬂuence of an action by itsq-value, but diﬀer from any other methodusing the same action inﬂuence because the contextual data is produced via backward crossing. The goal isto maximise the expected returns.The same paradigm has been presented in a concurrent work (Edwards et al.,2018) as Forward-Backward RL(FBRL). The beneﬁts of a backward model have also been further investigated in other studies.Wang et al.33Published in Transactions on Machine Learning Research (04/2024)(2021) investigate the problem in oﬄine settings, and show that backward models enable better generalisationthan forward ones.van Hasselt et al.(2019) provide empirical evidence suggesting that assigning credit fromhypothetical transitions, that is, via planning, improves the overall eﬃciency in control problems.Chelu et al.(2020) andvan Hasselt et al.(2019) further show that backward planning provides even greater beneﬁts thanforward planning when the state-transition dynamics are stochastic.6.6.2 Learning predecessorsExpected Eligibility Trace (ET(λ))(van Hasselt et al.,2021) provide a model-free alternative tobackward planning that assigns credit to potential predecessors decisions of the outcome: decisions thathave been taken in the past but have not in the last episode. The main idea is to weight the action value byits expected eligibility trace, that is, the instantaneous trace (see Section6.1), but in expectation over therandom trajectory, deﬁned by the policy and the state-transition dynamics.The Deep RL implementation of ET(λ) considers the expected trace upon the action value representation –usually the last layer of a neural network value approximator. Like for other ETs algorithms, ET(λ) measuresaction inﬂuence using theq-value of the decision and encodes the information of the trace in the parametersof the function approximator. In this case, the authors interpret the value network as a composition of anon-linear representation functionφ(s)and a linear value functionv(st)=w⊤φ(s). The expected tracee(s)=Eφ(s)is then the result of applying a second linear operatorEon the representation.e(s)is thentrained to minimise the expectedℓ2norm between the current estimation ofe(s)and the instantaneoustrace.6.6.3 Summary and discussionThe methods in this section assign credit by considering the eﬀects of decisions that have not been taken, butcould have led to the same outcome. The intuition behind them is that, in tasks where the action inﬂuenceis low due to low MDP density, creditable actions are rare ﬁndings. When this happens the agent can usethat occurrence to extract as much information as possible from them.One set of methods does so by learning inverse models of the state-transition dynamics and walking backwardsfrom the outcome.Chelu et al.(2020);van Hasselt et al.(2019) further analyse the conditions in whichbackward planning is beneﬁcial. Another set of methods exploits the idea of eligibility traces and keeps ameasure of the marginal state-action probability to assign credit to actions that could have led to the sameoutcome. Overall, these methods are designed to thrive in tasks where the action inﬂuence is low. Also,for their ability to start from a high-value state, backward planning methods can ﬁnd a higher number ofoptimal transpositions, and therefore provide a less biased estimate of the credit of a state-action pair.6.7 Meta-learning proxies for creditThe methods in this category aim to meta-learn key hyperparameters of canonical TD methods. In fact, RLmethods are often brittle to the choice of hyperparameters, for example, the number of steps to look-aheadin bootstrapping, what discount factor to use, or meta-parameters speciﬁc to the method at hand. Howto select these meta-parameters is an accurate balance that depends on the task, the algorithm, and theobjective of the agent.For this reason, it is sometimes diﬃcult to analyse them using the usual framework, and we present themdiﬀerently, by describing their main idea, and the way they are implemented in Deep RL.Meta Gradient (MG) RL(Xu et al.,2018) remarks how diﬀerent CA measures of action inﬂuenceimpact the performance on control problems, and proposes to answer the question: “Among the most commonTD targets, which one results in the best performance?” . The method interprets the target as a parametric,diﬀerentiable function that can be used and modiﬁed by the agent to guide its behaviour to achieve thehighest returns.In particular,Meta-Gradientsconsider theλ-return (Sutton,1988) target, for it can generalise the choice ofmany targets (Schulman et al.,2016). It then learns its meta-parameters: the bootstrapping parameterλ34Published in Transactions on Machine Learning Research (04/2024)and the discount factorγ. The connection between MG and CA is that, diﬀerent pairs of meta-parametersevaluate actions diﬀerently. For example, changing the discount factor can move the focus of the assignmentfrom early to late actions with eﬀects on policy improvements (Xu et al.,2018). In fact, adapting andlearning the meta-parameters online eﬀectively corresponds to meta-learning a measure of action inﬂuence,and profoundly aﬀects credit.Meta-learning credit assignment strategies has been further extended to distributional (Yin et al.,2023)and continual (Zheng et al.,2020) settings.Badia et al.(2020) investigated the eﬀects of meta-learning thediscount factor and the exploration rate to balance out short and long-term rewards.6.7.1 Summary and discussionOverall, these methods assign credit to actions by applying canonical TD learning algorithms with ameta-learnt measure of action inﬂuence. The goal can come in the form of an update target (Xu et al.,2018;Zheng et al.,2018;Xu et al.,2020), a full return distribution (Yin et al.,2023), or a reward function (Zhenget al.,2020). This allows agents to adapt their inﬂuence function online, especially improving in conditionsof high MDP depth.7 Evaluating creditLike accurate evaluation is fundamental to RL agents to improve their policy, an accurate evaluation of aCA method is fundamental to CA research to monitor if and how a method is advancing the ﬁeld. Theaim of this section is to survey the state of the art of the metrics, the tasks, and the evaluation protocolsto evaluate a CA method. We discuss the main components of the evaluation procedure, the performancemetrics, the tasks, and the evaluation protocols.7.1 MetricsWe categorise existing metrics to evaluate a CA method in two main classes:(a)The metrics that are already used for control problems. These mostly aim to assess the agent’sability to make optimal decisions, but they do not explicitly measure the accuracy of the actioninﬂuence.(b)The metrics that target the quality of an assignment directly, which usually aggregate metricsthroughout the RL training procedure.We now proceed to describe the two classes of metrics.7.1.1 Metrics borrowed from controlBias, variance and contraction rate.The ﬁrst, intuitive, obvious proxy to assess the quality of acredit assignment method is its theoretical performance in suitablecontrolproblems: the bias, variance, andcontraction rate of the policy improvement operator described inRowland et al.(2020). Notice that thesemetrics are not formally deﬁned for all the methods, either because some variables cannot be accessed orbecause the operators they act on are not formally deﬁned for the method in question. For the evaluationoperator described in Equation (2), we can specify these quantities as follows.Γ=s u ps∈S||TVπ(s)−TV′π(s)||∞||Vπ(s)−V′π(s)||∞(28)is the contraction rate and describes how fast the assignment converges to its ﬁxed point, if it does so, andthus how eﬃcient it is. HereVπ(s)andV′π(s)are two estimates of the state-value, which highlights thatthese set of metrics are not suitable to evaluate methods using any measure of action inﬂuence.35Published in Transactions on Machine Learning Research (04/2024)IfTis contractive, thenΓ<1∀VπandV′π,and there exist a ﬁxed-point bias ofTgiven by:ξ=||Vπ(s)−ˆVπ(s)||2,(29)whereˆVπ(s)is the true, unique ﬁxed point ofT, whose existence is guaranteed byΓ<1. For everyevaluation operatorT, there is an update ruleΛ:R|S|×H→Rthat takes as input the current estimationof the state-value function, and a trajectory and outputs the updated function.Λhas a variance:ν=Eµ,π[||Λ[V(s),D]−TV(s)||22].(30)These three quantities are usually in a trade-oﬀ (Rowland et al.,2020). Indeed, many (if not all) studies oncredit assignment (Hung et al.,2019;Mesnard et al.,2021;Ren et al.,2022;Raposo et al.,2021) report theempirical return and its variance. Because the contraction rate is often harder to calculate, an alternativemetric is the time-to-performance, which evaluates the number of interactions necessary to reach a givenperformance. These mostly aim at showing improvement in sample eﬃciency and/or asymptotic performance.While useful, this is often not enough to assess the quality of credit assignment, as superior returns can be theresult of better exploration, better optimisation, better representation learning, luck (as per the environmentdynamics’ stochasticity) or of a combination of such factors. Using empirical returns makes the evaluationmethod empirically viable for any measure of action inﬂuence described in Section4, even if these metrics arenot formally deﬁned for them. Nonetheless, when the only diﬀerence between two RL algorithms lies in howcredit is assigned, and this is not confounded by the aforementioned factors, it is generally safe to attributeimprovements to superior credit, given that the improvements are statistically signiﬁcant (Henderson et al.,2018;Agarwal et al.,2021).Task completion rate.A related, but more precise, metric is the success rate. Given a budget of trials,the success rate measures the frequency of task completion, that is, the number of times the task wassolved over the total number of episodes:G=|C∗|/|C|. Here,C∗is a set of optimal histories experiencedby the agent, andCis the full set of histories used to train it. Considering success rates instead of bias,variance, and trade-oﬀ is useful as it alleviates another issue of these performance metrics: there is nodistinction between easy-to-optimise rewards and hard-to-optimise rewards. This is evident in the key-to-door task with distractors (Hung et al.,2019), which we describe in detail later in Section7.2.D u e t o t h estochasticity from the apple phase (the distractors), it is generally impossible to distinguish performance onapple picking (easy-to-optimise rewards) and door opening (hard-to-optimise rewards that superior creditassignment methods usually obtain). Furthermore, the minimum success rateGmincould also be an eﬀectivemetric to disentangle the eﬀects of exploration from those of CA as discussed in Section5.4,d e s p i t en e v e rbeing employed for that purpose. However, notice that this clarity in reporting credit comes at a cost. Infact, even if these kinds of metrics are more precise than performance metrics, they require expert knowledgeof the task. They often suﬀer from the same confounders as bias, variance, and contraction rate.Value error.As the value function is at the heart of many credit assignment methods, another proxy forthe quality of the credit is the quality of value estimation, which can be estimated from the distributionof TD errors (Andrychowicz et al.,2017;Rauber et al.,2019;Arjona-Medina et al.,2019). We can thengeneralise the value error to one ofinﬂuence error:E[||/tildewideK(s, a, g)−K(s, a, g)||i],w h e r e|| · ||idenotes theithnorm of a vector,/tildewideK(s, a, g)is the current approximation of inﬂuence andK(s, a, g)is the true inﬂuence.A drawback of the inﬂuence error (and the value error) is that it can be misleading. When an algorithmdoes not fully converge, for example, because of high MDP sparsity (see Section(b), it can happen that thevalue error is very low. This is because the current policy never visits a state with a return diﬀerent fromzero, and the value function collapses to always return zero. Nevertheless, this metric is a viable optionto evaluate RL methods that use some form of action inﬂuence. It is not applicable, for example, to PGmethods using Monte-Carlo returns to improve a parametric policy via gradient ascent (Sutton & Barto,2018), or to sequence modelling methods (see Section6.5that only approximate the action probabilities ofa predeﬁned set of demonstrations.36Published in Transactions on Machine Learning Research (04/2024)7.1.2 Bespoke metrics for credit assignmentsWe now review metrics that measure the quality of individual credit assignments, that is, how well actionsare mapped to corresponding outcomes, or how well outcomes are redistributed to past actions. Usually,these metrics are calculated in hindsight, after outcomes have been observed.Using knowledge about the causal structure.Suppose we have expert knowledge about the causalstructure of the task at hand, i.e. which actions cause which outcomes. This is often the case since ashumans we often have an instinctive understanding of the tasks agents tackle. In such a case, given anobserved outcome from an agent’s trajectory, one can compare credit assignments, which approximate suchcause and eﬀect relationships, to the ground truth represented by our causal model of the task. We giveseveral examples from the literature. In Delayed Catch,Raposo et al.(2021) assess whether credit is assignedto the actions that lead to catches or to the end-of-episode reward since they know that these actions arecausing the experienced rewards. They do the same on the Atari game Skiing, which is a more complextask but that shares the fact that only a subset of the actions of the agent yield rewards. For example, inSkiing, going between ski poles is the only thing that grants rewards (with delay) at the end of an episode.Ferret et al.(2021a) adopt a similar approach and look at the inﬂuence attributed to actions responsible fortrigger switches in the Triggers environment, which contribute alone to the end-of-episode reward. Similarly,Arjona-Medina et al.(2019) look at redistributions of RUDDER on several tasks, including the Atari 2600game Bowling.Counterfactual simulation.A natural approach, which is nonetheless seldom explored in the literature,is counterfactual simulation. On a high level, it consists in asking what would have happened if actions thatare credited for particular outcomes had been replaced by another action. This is close to the notion ofhindsight advantage.Comparing to actual values of the estimated quantity.This only applies to methods whose creditassignments are mathematically grounded, in the sense that they are the empirical approximations of well-deﬁned quantities. In general, one can leverage extra compute and the ability to reset a simulator toarbitrary states to obtain accurate estimations of the underlying quantity, and compare it to the actual,resource-constrained quantity estimated from experience.7.2 TasksIn what follows, we present environments that we think are most relevant to evaluate credit assignmentmethods and individual credit assignments. The most signiﬁcant tasks are those that present all three chal-lenges to assign credit: delayed rewards, transpositions, and sparsity of the inﬂuence. This often correspondsto experiments that have reward delay, high marginal entropy of the reward, and partial observability. Tobenchmark explicit credit assignment methods, we additionally need to be able to recover the ground truthinﬂuence of actions w.r.t. given outcomes, or we can use our knowledge of the environment and developmore subjective measures.7.2.1 Diagnostic tasksDiagnostic tasks are useful as sanity checks for RL agents and present the advantage of running ratherquickly, compared to complex environments with visual input that may imply several millions of samplesbefore agents manage to solve the task at hand. Notice that these tasks may not be representative of theperformance of the method at scale, but provide a useful signal to diagnose the behaviour of the algorithmin the challenges described in Section5. Sometimes, the same environment can represent both a diagnostictask and an experiment at scale, simply by changing the space of the observations or the action space.We ﬁrst present chain-like environments, that can be represented graphically by a chain (environmentsatoc), and then a grid-like environment (environmentd), that has more natural grid representations for boththe environment and the state.37Published in Transactions on Machine Learning Research (04/2024)a) Aliasing chain.The aliasing chain (introduced inHarutyunyan et al.(2019) as Delayed Eﬀect) is anenvironment whose outcome depends only on the ﬁrst action. A series of perceptually aliased and zero-rewardstates follow this ﬁrst action, and an outcome is observed at the end of the chain (+1or−1depending onthe binary ﬁrst action).b) Discounting chain.The discounting chain (Osband et al.,2020) is an environment in which a ﬁrstaction leads to a series of states with inconsequential decisions with a ﬁnal reward that is either1or1+ϵ,and a variable length. It highlights issues with the discounting horizon.c) Ambiguous bandit.The ambiguous bandit (Harutyunyan et al.,2019) is a variant of a two-armedbandit problem. The agent is given two actions: one that transitions to a state with a slightly moreadvantageous Gaussian distribution over rewards with probability1−ϵ, and another that does so withprobabilityϵ.d) Triggers.Triggers (Ferret et al.,2021a) is a family of environments and corresponding discrete controltasks that are suited for the quantitative analysis of the credit assignment abilities of RL algorithms. Eachenvironment is a bounded square-shaped 2D gridworld where the agent collects rewards that are conditionedon the previous activation of all the triggers of the map. Collecting all triggers turns the negative value ofrewards into positive and this knowledge can be exploited to assess proper credit assignment: the actionsof collecting triggers appear natural to be credited. The environments are procedurally generated: whenrequesting a new environment, a random layout is drawn according to the input speciﬁcations.7.2.2 Tasks at scaleIn the following, we present higher-dimension benchmarks for agents equipped with credit assignment capa-bilities.Atari.The Arcade Learning Environment (Bellemare et al.,2013) (ALE) is an emulator in which RLagents compete to reach the highest scores on56classic Atari games. We list the ones we deem interestingfor temporal credit assignment assessment due to delayed rewards, which were ﬁrst highlighted byArjona-Medina et al.(2019).Bowling: like in real-life bowling, the agent must throw a bowling ball at pins, whileideally curving the ball so that it can clear all pins in one throw. The agent experiences rewards with a highdelay, at the end of all rolls (between2and4depending on the number of strikes achieved).Venture:t h eagent must enter a room, collect a treasure and shoot monsters. Shooting monsters only give rewards afterthe treasure was collected, and there is no in-game reward for collecting it.Seaquest: the agent controlsa submarine and must sink enemy submarines. To reach higher scores, the agent has to additionally rescuedivers that only provide reward once the submarine lacks oxygen and surfaces to replenish it.Solaris:the agent controls a spaceship that earns points by hunting enemy spaceships. These shooting phases arefollowed by the choice of the next zone to explore on a high-level map, which conditions future rewards.Skiing: the agent controls a skier who has to go between poles while going down the slope. The agent getsno reward until reaching the bottom of the slope, at which time it receives a reward proportional to the pairsof poles it went through, which makes for long-term credit assignment.VizDoom.VizDoom (Kempka et al.,2016) is a suite of partially observable 3D tasks based on the classicalDoom video game, a ﬁrst-person shooter. As mentioned before, it is an interesting sandbox for creditassignment because it optionally provides high-level information such as labelled game objects, depth as wellas a top-view minimap representation; all of which can be used for approximate optimally eﬃcient creditassignment algorithms.BoxWorld.BoxWorld (Zambaldi et al.,2018) is a family of environments that shares similarities withTriggers, while being more challenging. Environments are also procedurally-generated square-shaped 2Dgridworlds with discrete controls. The goal is to reach a gem, which requires going through a series of boxesprotected by locks that can only be opened with keys of the same colour while avoiding distractor boxes.38Published in Transactions on Machine Learning Research (04/2024)The relations between keys and locks can be utilised to assess assigned credit since the completion of thetask (as well as intermediate rewards for opening locks) depends on the collection of the right keys.Sokoban.Sokoban (Racanière et al.,2017) is a family of environments that is similar to the two previousones. The agent must push boxes to intended positions on the grid while avoiding dead-end situations (forinstance, if a block is stuck against walls on two sides, it cannot be moved anymore). While there is nodeﬁnite criterion to identify decisive actions, actions that lead to dead-ends are known and can be exploitedto assess the quality of credit assignment.DeepMind Lab.DeepMind Lab (Beattie et al.,2016) (DMLab) is a suite of partially observable 3Dtasks with rich visual input. We identify several tasks that might be of interest to assess credit assignmentcapabilities, some of which were used in recent work.Keys-Doors: the agent navigates to keys that opendoors (identiﬁed by their shared colour) so that it can get to an absorbing state represented by a cake.Ferretet al.(2021a) consider a harder variant of the task where collecting keys is not directly rewarded anymoreand feedback is delayed until opening doors.Keys-Apples-Doors:Hung et al.(2019) consider an extendedversion of the previous task. The agent still has to collect a key, but after a ﬁxed duration a distractor phasebegins in which it can only collect small rewards from apples, and ﬁnally, the agent must ﬁnd and open adoor with the key it got in the initial phase. To solve the task, the agent has to learn the correlation orcausation link between the key and the door, which is made hard because of the extended temporal distancebetween the two events and of the distractor phase.Deferred Eﬀects: the agent navigates between tworooms, the ﬁrst one of which contains apples that give low rewards, while the other contains cakes that givehigh rewards but it is entirely in the dark. The agent can turn the light on by reaching the switch in theﬁrst room, but it gets an immediate negative reward for it. In the end, the most successful policy is toactivate the switch regardless of the immediate cost so that a maximum number of cakes can be collected inthe second room before the time limit.7.3 ProtocolOnline evaluation.The most standard approach is to evaluate the quality of credit assignment methodsand individual credit assignments along the RL training procedure. As the policy changes, the creditassignments change since the eﬀect of actions depends on subsequent actions (which are dictated by thepolicy). One can dynamically track the quality of credit assignments and that of the credit assignmentmethod using the metrics developed in the previous section. For the credit assignment method, since itrequires a dataset of interaction, one can consider using the most trajectories produced by the agent. Anadvantage of this approach is that it allows evaluating the evolution of the credit assignment quality alongthe RL training, with an evolving policy and resulting dynamics. Also, since the goal of credit assignment isto help turn feedback into improvements, it makes sense to evaluate it in the context of said improvements.While natural, online evaluation means one has little control over the data distribution of the evaluation. Thisis problematic because it is generally hard to disentangle credit quality from the nature of the trajectories itis evaluated on. A corollary is that outcomes that necessitate precise exploration (which can be the outcomesfor which agents would beneﬁt most from accurate credit assignment) might not be explored.Oﬄine evaluation.An alternative is to consider oﬄine evaluation. It requires a dataset of interactions,either collected before or during the RL training. Credit assignments and the credit assignment method thenuse the parameters learned during the RL training while being evaluated on the oﬄine data. As the policy inthe oﬄine data is generally not the latest policy from the online training, oﬄine evaluation is better suited forpolicy-conditioned credit assignment or (to some extent) trajectory-conditioned credit assignment. Indeed,other forms of credit assignment are speciﬁc to a single policy, and evaluating these on data generated fromanother policy would not be accurate. An important advantage of oﬄine evaluation is that it alleviates theimpact of exploration, as one controls the data distribution credit is evaluated on.39Published in Transactions on Machine Learning Research (04/2024)8 Closing, discussion and open challengesThe CAP is the problem to approximate the inﬂuence of an action from a ﬁnite amount of experience,and it is of critical importance to deploy RL agents into the real world that are eﬀective, general, safe andinterpretable. However, there is a misalignment in the current literature on what credit means in words andhow it is formalised. In this survey, we put the basis to reconcile this gap by reviewing the state of the artof the temporal CAP in Deep RL, focusing on three major questions.8.1 SummaryOverall, we observed three major fronts of development around the CAP.The ﬁrst concern is the problem ofhow to quantify action inﬂuence(Q1.). We addressedQ1.inSection4,and analysed the quantities that existing works use to represent the inﬂuence of an action. InSection4.1we uniﬁed these measures of action inﬂuence with theassignmentdeﬁnition. In Sections4.3and4.5weshowed that the existing literature agrees on an intuition of credit as a measure of the inﬂuence of an actionover an outcome, but that it does not translate that well into mathematics and none of the current quantitiesalign with the purpose. As a consequence, we proposed a set of principles that we suggest a measure ofaction inﬂuence should respect to represent credit.The second front aims to address the question ofhow to learn action inﬂuence from experienceand todescribe the existingmethodsto assign credit. InSection5we looked at the challenges that arise fromlearning these measures of action inﬂuence and, together withSection6, answeredQ2..W eﬁ r s tr e v i e w e dthe most common obstacles to learning already identiﬁed in the literature and realigned them to our newlydeveloped formalism. We identiﬁed three dimensions of an MDP, depth, breadth, and density and describedpathological conditions on each of them that hinder the CA. In Section6we deﬁned a CA method as analgorithm whose aim is to approximate a measure of action inﬂuence from a ﬁnite amount of experience.We categorised methods into those that:(i)use temporal contiguity as a proxy for causal inﬂuence;(ii)de-compose the total return into smaller per-timestep contributions;(iii)condition the present on informationabout the future using the idea of hindsight;(iv)use sequence modelling and represent action inﬂuence asthe likelihood of action to follow a state and predict an outcome;(v)learn to imagine backward transitionsthat always start at a key state and propagate back to the state that could generate them;(vi)meta-learnaction inﬂuence measures.Finally, the third research front deals withhow to evaluate quantities and methodsto assign credit andaims to provide an unbiased estimation of the progress in the ﬁeld. InSection7we addressedQ3.andanalysed how current methods evaluate their performance and how we can monitor future advancements.We discussed the resources that each benchmark has to oﬀer and their limitations. For example, diagnosticbenchmarks do not isolate the speciﬁc CAP challenges identiﬁed in Section5: delayed eﬀects, transpositions,and sparsity. Benchmarks at scale often cannot disentangle the CAP from the exploration problem, and itbecomes hard to understand whether a method is advancing one problem or another.8.2 Discussion and open challengesAs this survey suggests, the work in the ﬁeld is now fervent and the number of studies in a bullish trend, withmany works showing substantial gains in control problems only by – to the best of our current knowledge –advancing on the CAP alone (Bellemare et al.,2017;van Hasselt et al.,2021;Edwards et al.,2018;Mesnardet al.,2021;2023).We observed that the take-oﬀ of CA research in the broader area of RL research is only recent. The mostprobable reason for this is to be found in the fact that the tasks considered in earlier Deep RL research wereexplicitly designed to be simple from the CA point of view. Using tasks where assigning credit is hard wouldhave – and probably still does, e.g.,Küttler et al.(2020) – obfuscate other problems that it was necessaryto solve before solving the CAP. For example, adding the CAP on the top of scaling RL to high-dimensionalobservations (Arulkumaran et al.,2017) or dealing with large action spaces (Dulac-Arnold et al.,2015;vanHasselt & Wiering,2009) would have, most likely, concealed any evidence of progress for the underlying40Published in Transactions on Machine Learning Research (04/2024)challenges. This is also why CA methods do not usually shine in classical benchmarks (Bellemare et al.,2013), and peer reviews are often hard on these works. Today, thanks to the advancements in other areas ofRL, the ﬁeld is in a state where improving on the CAP is a compelling challenge.Yet, the CAP still holds open questions and there is still much discussion required to consider the problemsolved. In particular, the following observations describe our positions with respect to this survey.Aligning future works to a common problem deﬁnition.The lack of a review since its conception(Minsky,1961) and the rapid advancements produced a fragmented landscape of deﬁnitions for action inﬂu-ence, an ambiguity in the meaning ofcredit assignment, a misalignment between the general intuition and itspractical quantiﬁcation, and a general lack of coherence in the principal directions of the works. While thisdiversity is beneﬁcial for the diversiﬁcation of the research, it is also detrimental to comparing the methods.Future works aiming to propose a new CA method should clarify these preliminary concepts. Answers to“What is the choice of the measure of action inﬂuence? Why the choice? What is the method of learning itfrom experience? How is it evaluated?” would be good a starting point.Characterising credit.“What is theminimumset of properties that a measure of action inﬂuence shouldrespect to inform control? What the more desirable ones?” . This question remains unanswered, with someideas inFerret(2022, Chapter 4), and we still need to understand what characterises a proper measure ofcredit.Causality.The relationship between CA and causality is underexplored, but in a small subset of works(Mesnard et al.,2021;Pitis et al.,2020;Buesing et al.,2019). The literature lacks a clear and completeformalism that casts the CAP as a problem of causal discovery. Investigating this connection and formalisinga measure of action inﬂuence that is also a satisfactory measure of causal inﬂuence would help betterunderstand the eﬀects of choosing a measure of action inﬂuence over another. Overall, we need to betterunderstand the connections between CA and causality: what happens when credit is a strict measure ofcausal inﬂuence? How do current algorithms perform with respect to this measure? Can we devise analgorithm that exploits a causal measure of inﬂuence?Optimal credit.Many works refer tooptimal creditor toassigning credit optimally, but it is unclearwhat that formally means. “When is credit optimal?” remains unanswered.Combining beneﬁts from diﬀerent methods.Methods conditioning on the future currently showsuperior results compared to methods in other categories. These promising methods include hindsight(Section6.4), sequence modelling (Section6.5) and backward learning and planning methods (Section6.6).However, while hindsight methods are advancing fast, sequence modelling and backward planning methodsare underinvestigated. We need a better understanding of the connection between these two worlds, whichcould potentially lead to even better ways of assigning credit. Could there be a connection between thesemethods? What are the eﬀects of combining backward planning methods with more satisfactory measuresof inﬂuence, for example, with CCA?Benchmarking.The benchmarks currently used to review a CA method (Chevalier-Boisvert et al.,2018;Bellemare et al.,2013;Samvelyan et al.,2021) (see Section7.2) are often borrowed fromcontrolproblems,leading to the issues discussed in Section7and recalled in the summary above. On a complementary note,CA methods are often evaluated in actor-critic settings (Harutyunyan et al.,2019;Mesnard et al.,2021),which adds layers of complexity that are not necessary. This, together with the inclusion of other unnecessaryaccessories, can obfuscate the contributions of CA to the overall RL success. As a consequence, the literaturelacks a fair comparison among all the methods, and it is not clear how all the methods in Section6behavewith respect to each other against the same set of benchmarks. This lack of understanding of the state ofthe art leads to a poor signal to direct future research. We call for a new, community-driven single set ofbenchmarks that disentangles the CAP from the exploration problem and isolate the challenges describedin Section5. How to disentangle the CAP and the exploration problem? How to isolate each challenge?Shall we evaluate in value-based settings, and would the ranking between the methods be consistent with an41Published in Transactions on Machine Learning Research (04/2024)evaluation in actor-critic settings? While we introduced some ideas in Section5.4, these questions are stillunanswered.Reproducibility.Many works propose open-source code, but experiments are often not reproducible, theircode is hard to read, hard to run and hard to understand. Making code public is not enough, and cannot beconsidered open-source if it is not easily usable. Other than public, open-source code should be accessible,documented, easy to run, and accompanied by continuous support for questions and issues that may arisefrom its later usage. We need future research to acquire more rigour in the way to publish, present, andsupport the code that accompanies scientiﬁc publications. In particular, we need(i)a formalised, sharedand broadly agreed standard that is not necessarily anewstandard;(ii)for new studies to adhere to thisstandard, and(iii)for publishers to review the accompanying code at least as thoroughly as when reviewingscientiﬁc manuscripts.Monitoring advancements.The community lacks a database containing comprehensive, curated resultsof each baseline. Currently, baselines are often re-run when a new method is proposed. This can po-tentially lead to comparisons that are unfair both because the baselines could be suboptimal (e.g., in thehyperparameters choice, training regime) and their reproduction could be not faithful (e.g., in translatingthe mathematics into code). When these conditions are not met, it is not clear whether a new method isadvancing the ﬁeld because it assigns credit better or because of misaligned baselines. We call for a new,community-driven database holding the latest evaluations of each baseline. The evaluation should be drivenby the authors and the authors be responsible for its results. When such a database will be available, newpublications should be tested against the same benchmarks and not re-run previous baselines, but ratherrefer to the curated results stored in the database.Peer reviewing CA works.As a consequence of the issues identiﬁed above, and because CA methods donot usually shine in classical benchmarks (Bellemare et al.,2013), peer reviews often do not have the tools tocapture the novelties of a method and its improvements. On one hand, we need a clear evaluation protocol,including a shared benchmark and leaderboard to facilitate peer reviews. On the other hand, peer reviewsmust steer away from using tools and metrics that would be used for control, and use those appropriate forthe CAP instead.Lack of priors and foundation models.Most of the CA methods start to learn credit from scratch,without any prior knowledge but the one held by the initialisation pattern of its underlying network. Thisrepresents a main obstacle to making CA eﬃcient because, at each new learning phase, even elementaryassociations must be learned from scratch. In contrast, when facing a new task, humans often rely on theirprior knowledge to determine the inﬂuence of an action. In the current state of the art, the use of priorsto assign credit more eﬃciently is overlooked. Vice versa, the relevance of the CAP and the use of moreadvanced methods for CA (Mesnard et al.,2021;2023;Edwards et al.,2018;van Hasselt et al.,2021) is oftenunderestimated for the development of foundation models in RL.8.3 ConclusionsTo conclude, in this survey, we have set out to formally settle the CAP in Deep RL. The resulting materialdoes not aim to solve the CAP, but rather proposes a unifying framework that enables a fair comparisonamong the methods that assign credit and organises existing material to expedite the starting stages of newstudies. Where the literature lacks answers, we identify the gaps and organise them in a list of challenges. Wekindly encourage the research community to join in solving these challenges in a shared eﬀort, and we hopethat the material collected in this manuscript can be a helpful resource to both inform future advancementsin the ﬁeld and inspire new applications in the real world.ReferencesDavid Abel, Cameron Allen, Dilip Arumugam, D Ellis Hershkowitz, Michael L Littman, and Law-son LS Wong. Bad-policy density: A measure of reinforcement learning hardness.arXiv preprint42Published in Transactions on Machine Learning Research (04/2024)arXiv:2110.03424, 2021a.David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, and SatinderSingh. On the expressivity of markov reward. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. WortmanVaughan (eds.),Advances in Neural Information Processing Systems, 2021b.David Abel, Andre Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, and Satinder Singh.A deﬁnition of continual reinforcement learning. InThirty-seventh Conference on Neural InformationProcessing Systems, 2023. URLhttps://openreview.net/forum?id=ZZS9WEWYbD.Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deepreinforcement learning at the edge of the statistical precipice.Advances in neural information processingsystems, 34:29304–29320, 2021.Mostafa Al-Emran. Hierarchical reinforcement learning: a survey.International journal of computing anddigital systems, 4(02), 2015.Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A survey of explorationmethods in reinforcement learning.arXiv preprint arXiv:2109.00157, 2021.Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew,Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. InAdvancesin neural information processing systems, volume 30, 2017. URLhttps://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf.Thomas Anthony, Tom Eccles, Andrea Tacchetti, János Kramár, Ian Gemp, Thomas Hudson, Nicolas Porcel,Marc Lanctot, Julien Pérolat, Richard Everett, et al. Learning to play no-press diplomacy with bestresponse policy iteration.Advances in Neural Information Processing Systems, 33:17987–18003, 2020.Jose A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter,and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards.Advances in Neural InformationProcessing Systems, 32, 2019.Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep reinforcementlearning: A brief survey.IEEE Signal Processing Magazine, 34(6):26–38, 2017.Kai Arulkumaran, Dylan R Ashley, Jürgen Schmidhuber, and Rupesh K Srivastava. All you need is super-vised learning: From imitation learning to meta-rl with upside down rl.arXiv preprint arXiv:2202.11960,2022.Dilip Arumugam, Peter Henderson, and Pierre-Luc Bacon. An information-theoretic perspective on creditassignment in reinforcement learning.CoRR, abs/2103.06224, 2021. URLhttps://arxiv.org/abs/2103.06224.Dylan R Ashley, Kai Arulkumaran, Jürgen Schmidhuber, and Rupesh Kumar Srivastava. Learning relativereturn policies with upside-down reinforcement learning.arXiv preprint arXiv:2202.12742, 2022.Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. InProceedings of the AAAIconference on artiﬁcial intelligence, volume 31, 2017.Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhao-han Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. InInter-national Conference on Machine Learning, pp. 507–517. PMLR, 2020.Akhil Bagaria and George Konidaris. Option discovery using deep skill chaining. InInternational Conferenceon Learning Representations, 2019.Leemon C III Baird.Reinforcement Learning Through Gradient Descent. PhD thesis, US Air Force Academy,1999.43Published in Transactions on Machine Learning Research (04/2024)David Balduzzi, Hastagiri Vanchinathan, and Joachim Buhmann. Kickback cuts backprop’s red-tape: Bi-ologically plausible credit assignment in neural networks. InProceedings of the AAAI Conference onArtiﬁcial Intelligence, volume 29, 2015.Elias Bareinboim, Juan D. Correa, Duligur Ibeling, and Thomas Icard.On Pearls Hierarchy and the Foun-dations of Causal Inference, pp. 507556. Association for Computing Machinery, New York, NY, USA, 1edition, 2022. ISBN 9781450395861. URLhttps://doi.org/10.1145/3501714.3501743.Andrew G Barto. Intrinsic motivation and reinforcement learning.Intrinsically motivated learning in naturaland artiﬁcial systems, pp. 17–47, 2013.Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning.Discreteevent dynamic systems, 13(1):41–77, 2003.Andrew G Barto, Satinder Singh, Nuttapong Chentanez, et al. Intrinsically motivated learning of hierarchicalcollections of skills. InProceedings of the 3rd International Conference on Development and Learning,volume 112, pp. 19. Citeseer, 2004.Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, AndrewLefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab.arXiv preprint arXiv:1612.03801,2016.Vahid Behzadan and William Hsu. Adversarial exploitation of policy imitation.arXiv preprintarXiv:1906.01121, 2019.Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: Anevaluation platform for general agents.Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013.Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip Thomas, and Rémi Munos. Increasing the actiongap: New operators for reinforcement learning. InProceedings of the AAAI Conference on ArtiﬁcialIntelligence, volume 30, 2016.Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. InInternational Conference on Machine Learning, pp. 449–458. Proceedings of Machine Learning Research,2017.Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, SubhodeepMoitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric balloons using rein-forcement learning.Nature, 588(7836):77–82, 2020.Michael Bowling, John D Martin, David Abel, and Will Dabney. Settling the reward hypothesis. In AndreasKrause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett(eds.),Proceedings of the 40th International Conference on Machine Learning, volume 202 ofProceedingsof Machine Learning Research, pp. 3003–3020. PMLR, 23–29 Jul 2023. URLhttps://proceedings.mlr.press/v202/bowling23a.html.Lars Buesing, Theophane Weber, Yori Zwols, Nicolas Heess, Sebastien Racaniere, Arthur Guez, and Jean-Baptiste Lespiau. Woulda, coulda, shoulda: Counterfactually-guided policy search. InInternationalConference on Learning Representations, 2019. URLhttps://openreview.net/forum?id=BJG0voC9YQ.Yu-Han Chang, Tracey Ho, and Leslie Kaelbling. All learning is local: Multi-agent learning in global rewardgames.Advances in neural information processing systems, 16, 2003.Veronica Chelu, Doina Precup, and Hado P van Hasselt. Forethought and hindsight in credit assign-ment. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.),Advances in NeuralInformation Processing Systems, volume 33, pp. 2270–2281. Curran Associates, Inc., 2020. URLhttps://proceedings.neurips.cc/paper/2020/file/18064d61b6f93dab8681a460779b8429-Paper.pdf.Veronica Chelu, Diana Borsa, Doina Precup, and Hado van Hasselt. Selective credit assignment.arXivpreprint arXiv:2202.09699, 2022.44Published in Transactions on Machine Learning Research (04/2024)Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, AravindSrinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. InAdvances in Neural Information Processing Systems, volume 34, pp. 15084–15097, 2021. URLhttps://proceedings.neurips.cc/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf.Zhixin Chen and Mengxiang Lin. Self-imitation learning in sparse reward settings.arXiv preprintarXiv:2010.06962, 2020.Nuttapong Chentanez, Andrew Barto, and Satinder Singh. Intrinsically motivated reinforcement learning.Advances in neural information processing systems, 17, 2004.Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openaigym.https://github.com/maximecb/gym-minigrid, 2018.Cédric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Autotelic agents with intrinsicallymotivated goal-conditioned reinforcement learning: a short survey.Journal of Artiﬁcial Intelligence Re-search, 74:1159–1199, 2022.Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, TimoEwalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamakplasmas through deep reinforcement learning.Nature, 602(7897):414–419, 2022.Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt,Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep reinforcement learning in largediscrete action spaces.arXiv preprint arXiv:1512.07679, 2015.Ashley D Edwards, Laura Downs, and James C Davidson. Forward-backward reinforcement learning.arXivpreprint arXiv:1803.10227, 2018.Andrew J Elliot and James W Fryer. The goal construct in psychology. InHandbook of motivation science,volume 18, pp. 235–250, 2008.Benjamin Eysenbach and Sergey Levine. Maximum entropy RL (provably) solves some robust RL problems.InInternational Conference on Learning Representations, 2022.Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learningskills without a reward function. InInternational Conference on Learning Representations, 2018.Francesco Faccio, Louis Kirsch, and Jürgen Schmidhuber. Parameter-based value functions. InInternationalConference on Learning Representations, 2021. URLhttps://openreview.net/forum?id=tV6oBfuyLTQ.Amir-massoud Farahmand. Action-gap phenomenon in reinforcement learning.Advances in Neural Infor-mation Processing Systems, 24, 2011.Johan Ferret.On Actions that Matter: Credit Assignment and Interpretability in Reinforcement Learning.PhD thesis, Université de Lille, 2022.Johan Ferret, Raphaël Marinier, Matthieu Geist, and Olivier Pietquin. Self-attentional credit assignmentfor transfer in reinforcement learning. InProceedings of the Twenty-Ninth International Joint Conferenceon Artiﬁcial Intelligence, IJCAI’20, 2021a. ISBN 9780999241165.Johan Ferret, Olivier Pietquin, and Matthieu Geist. Self-imitation advantage learning. InAAMAS 2021-20thInternational Conference on Autonomous Agents and Multiagent Systems, 2021b.Angelos Filos, Panagiotis Tigkas, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, and Yarin Gal. Canautonomous vehicles identify, recover from, and adapt to distribution shifts? InInternational Conferenceon Machine Learning, pp. 3145–3153. PMLR, 2020.Yannis Flet-Berliac. The promise of hierarchical reinforcement learning.The Gradient, 9, 2019.45Published in Transactions on Machine Learning Research (04/2024)Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Coun-terfactual multi-agent policy gradients. InProceedings of the AAAI conference on artiﬁcial intelligence,volume 32, 2018.Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for oﬄine hindsightinformation matching. InInternational Conference on Learning Representations, 2022. URLhttps://openreview.net/forum?id=CAjxVodl_v.Jim Gao. Machine learning applications for data center optimization, 2014.Javier García, Fern, and o Fernández. A comprehensive survey on safe reinforcement learning.Journal ofMachine Learning Research, 16(42):1437–1480, 2015. URLhttp://jmlr.org/papers/v16/garcia15a.html.Matthieu Geist, Bruno Scherrer, et al. Oﬀ-policy learning with eligibility traces: a survey.J. Mach. Learn.Res., 15(1):289–333, 2014.Anirudh Goyal, Philemon Brakel, William Fedus, Soumye Singhal, Timothy Lillicrap, Sergey Levine, HugoLarochelle, and Yoshua Bengio. Recall traces: Backtracking models for eﬃcient reinforcement learning.InInternational Conference on Learning Representations, 2019. URLhttps://openreview.net/forum?id=HygsfnR9Ym.Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska,Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computingusing a neural network with dynamic external memory.Nature, 538(7626):471–476, 2016.Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transducewith unbounded memory.Advances in neural information processing systems, 28, 2015.Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Matthieu Geist, et al. There is no turning back: Aself-supervised approach for reversibility-aware reinforcement learning.Advances in Neural InformationProcessing Systems, 34:1898–1911, 2021.Arthur Guez, Fabio Viola, Theophane Weber, Lars Buesing, Steven Kapturowski, Doina Precup, DavidSilver, and Nicolas Heess. Value-driven hindsight modelling.Advances in Neural Information ProcessingSystems, 33:12499–12509, 2020.Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deepenergy-based policies. InInternational conference on machine learning, pp. 1352–1361. PMLR, 2017.Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-policy maximumentropy deep reinforcement learning with a stochastic actor. InInternational Conference on MachineLearning, pp. 1861–1870. Proceedings of Machine Learning Research, 2018.Danijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter Abbeel. Deep hierarchical planning from pixels.Advances in Neural Information Processing Systems, 35:26091–26104, 2022.Jean Harb, Tom Schaul, Doina Precup, and Pierre-Luc Bacon. Policy evaluation networks.arXiv preprintarXiv:2002.11833, 2020.Anna Harutyunyan, Peter Vrancx, Pierre-Luc Bacon, Doina Precup, and Ann Nowe. Learning with optionsthat terminate oﬀ-policy. InProceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32,2018.Anna Harutyunyan, Will Dabney, Thomas Mesnard, Mohammad Gheshlaghi Azar, Bilal Piot, Nicolas Heess,Hado P van Hasselt, Gregory Wayne, Satinder Singh, Doina Precup, et al. Hindsight credit assignment.Advances in neural information processing systems, 32, 2019.46Published in Transactions on Machine Learning Research (04/2024)Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deepreinforcement learning that matters. InProceedings of the AAAI conference on artiﬁcial intelligence,volume 32, 2018.Donald D Hoﬀman. The interface theory of perception.Current Directions in Psychological Science, 25(3):157–161, 2016.Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, and PieterAbbeel. Evolved policy gradients.Advances in Neural Information Processing Systems, 31, 2018.Ronald A Howard.Dynamic programming and Markov processes.John Wiley, 1960.Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale, ArunAhuja, and Greg Wayne. Optimizing agent behavior over long time scales by transporting value.NatureCommunications, 10(1):5223, 2019. ISSN 2041-1723. doi: 10.1038/s41467-019-13073-w. URLhttps://doi.org/10.1038/s41467-019-13073-w.Tommi Jaakkola, Michael Jordan, and Satinder Singh. Convergence of stochastic iterative dynamic pro-gramming algorithms.Advances in neural information processing systems, 6, 1993.Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver,and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. InInternationalConference on Learning Representations, 2017. URLhttps://openreview.net/forum?id=SJ6yPD5xg.Michael Janner, Qiyang Li, and Sergey Levine. Oﬄine reinforcement learning as one big sequence modelingproblem. InAdvances in Neural Information Processing Systems, 2021.Dominik Janzing, David Balduzzi, Moritz Grosse-Wentrup, and Bernhard Schölkopf. Quantifying causalinﬂuences.The Annals Of Statistics, pp. 2324–2358, 2013.Stratton C Jaquette. Markov decision processes with a new optimality criterion: Discrete time.The Annalsof Statistics, 1(3):496–505, 1973.Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay. InInternational Conferenceon Machine Learning, pp. 4940–4950. Proceedings of Machine Learning Research, 2021a.Minqi Jiang, Tim Rocktäschel, and Edward Grefenstette. General intelligence requires rethinking explo-ration.Royal Society Open Science, 10(6):230539, 2023.Ray Jiang, Tom Zahavy, Zhongwen Xu, Adam White, Matteo Hessel, Charles Blundell, and Hado vanHasselt. Emphatic algorithms for deep reinforcement learning. In Marina Meila and Tong Zhang (eds.),Proceedings of the 38th International Conference on Machine Learning, volume 139 ofProceedings ofMachine Learning Research, pp. 5023–5033. PMLR, 18–24 Jul 2021b. URLhttps://proceedings.mlr.press/v139/jiang21j.html.Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experiencereplay in distributed reinforcement learning. InInternational conference on learning representations, 2019.Steven Kapturowski, Víctor Campos, Ray Jiang, Nemanja Rakicevic, Hado van Hasselt, Charles Blundell,and Adria Puigdomenech Badia. Human-level atari 200x faster. InThe Eleventh International Conferenceon Learning Representations, 2023. URLhttps://openreview.net/forum?id=JtC6yOHRoJJ.Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaśkowski. Vizdoom: Adoom-based ai research platform for visual reinforcement learning. In2016 IEEE conference on computa-tional intelligence and games (CIG), pp. 1–8. IEEE, 2016.Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A survey of zero-shot generalisationin deep reinforcement learning.Journal of Artiﬁcial Intelligence Research, 76:201–264, 2023.47Published in Transactions on Machine Learning Research (04/2024)Martin Klissarov and Doina Precup. Flexible option learning.Advances in Neural Information ProcessingSystems, 34:4632–4646, 2021.Martin Klissarov, Rasool Fakoor, Jonas Mueller, Kavosh Asadi, Taesup Kim, and Alex Smola. Adaptiveinterest for emphatic reinforcement learning. InDecision Awareness in Reinforcement Learning Workshopat ICML 2022, 2022. URLhttps://openreview.net/forum?id=ZGi3bDRXkx.A Harry Klopf. Brain function and adaptive systems: a heterostatic theory. Technical Report 133, Air ForceCambridge Research Laboratories. Special Reports, Bedford, Massachusets, 1972.Petar Kormushev, Sylvain Calinon, and Darwin G Caldwell. Reinforcement learning in robotics: Applicationsand real-world challenges.Robotics, 2(3):122–148, 2013.Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefen-stette, and Tim Rocktäschel. The nethack learning environment.Advances in Neural Information Pro-cessing Systems, 33:7671–7684, 2020.Kennon A Lattal. Delayed reinforcement of operant behavior.Journal of the Experimental Analysis ofBehavior, 93(1):129–139, 2010.Charline Le Lan, Stephen Tu, Adam Oberman, Rishabh Agarwal, and Marc G Bellemare. On the general-ization of representations in reinforcement learning. InInternational Conference on Artiﬁcial Intelligenceand Statistics, pp. 4132–4157. PMLR, 2022.Kuang-Huei Lee, Oﬁr Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama,Ian Fischer, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers.arXiv preprintarXiv:2205.15241, 2022.Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, DavidSilver, and Daan Wierstra. Continuous control with deep reinforcement learning.arXiv preprintarXiv:1509.02971, 2015.Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.Machinelearning, 8(3):293–321, 1992.Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and YoshuaBengio. A structured self-attentive sentence embedding. InInternational Conference on Learning Repre-sentations, 2017.Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems andsolutions.arXiv preprint arXiv:2201.08299, 2022.J Luketina, N Nardelli, G Farquhar, J Foerster, J Andreas, E Grefenstette, S Whiteson, and T Rocktäschel.A survey of reinforcement learning informed by natural language. InProceedings of the Twenty-EighthInternational Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, August 10-16 2019, Macao, China.,volume 57, pp. 6309–6317. AAAI Press (Association for the Advancement of Artiﬁcial Intelligence), 2019.Jukka Luoma, Sampsa Ruutu, Adelaide Wilcox King, and Henrikki Tikkanen. Time delays, competitiveinterdependence, and ﬁrm performance.Strategic Management Journal, 38(3):506–525, 2017.A Rupam Mahmood, Huizhen Yu, Martha White, and Richard S Sutton. Emphatic temporal-diﬀerencelearning.arXiv preprint arXiv:1507.01569, 2015.Matheus RF Mendonca, Artur Ziviani, and André MS Barreto. Graph-based skill acquisition for reinforce-ment learning.ACM Computing Surveys (CSUR), 52(1):1–26, 2019.Thomas Mesnard, Theophane Weber, Fabio Viola, Shantanu Thakoor, Alaa Saade, Anna Harutyunyan,Will Dabney, Thomas S Stepleton, Nicolas Heess, Arthur Guez, et al. Counterfactual credit assignmentin model-free reinforcement learning. InInternational Conference on Machine Learning, pp. 7654–7664.Proceedings of Machine Learning Research, 2021.48Published in Transactions on Machine Learning Research (04/2024)Thomas Mesnard, Wenqi Chen, Alaa Saade, Yunhao Tang, Mark Rowland, Theophane Weber, Clare Lyle,Audrunas Gruslys, Michal Valko, Will Dabney, Georg Ostrovski, Eric Moulines, and Remi Munos. Quan-tile credit assignment. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, SivanSabato, and Jonathan Scarlett (eds.),Proceedings of the 40th International Conference on Machine Learn-ing, volume 202 ofProceedings of Machine Learning Research, pp. 24517–24531. PMLR, 23–29 Jul 2023.URLhttps://proceedings.mlr.press/v202/mesnard23a.html.Donald Michie. Experiments on the mechanization of game-learning part i. characterization of the modeland its parameters.The Computer Journal, 6(3):232–236, 1963.Marvin Minsky. Steps toward artiﬁcial intelligence.Proceedings of the IRE, 49(1):8–30, 1961. ISSN 00968390.doi: 10.1109/JRPROC.1961.287775.Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-JoonLee, Eric Johnson, Omkar Pathak, Sungmin Bae, et al. Chip placement with deep reinforcement learning.arXiv preprint arXiv:2004.10746, 2020.Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, andMartin Riedmiller. Playing atari with deep reinforcement learning. InAdvances in Neural InformationProcessing Systems, Deep Learning Workshop, 2013.Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, AlexGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control throughdeep reinforcement learning.Nature, 518(7540):529–533, 2015.Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. InInter-national conference on machine learning, pp. 1928–1937. PMLR, 2016.Seyed Sajad Mousavi, Michael Schukat, Enda Howley, and Patrick Mannion. Applying q (λ)-learning indeep reinforcement learning to play atari games. InAAMAS Adaptive Learning Agents (ALA) Workshop,pp. 1–6, 2017.Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual rein-forcement learning with imagined goals.Advances in neural information processing systems, 31, 2018.Suraj Nair, Mohammad Babaeizadeh, Chelsea Finn, Sergey Levine, and Vikash Kumar. Trass: Time reversalas self-supervision. In2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 115–121. IEEE, 2020.Thanh Thi Nguyen and Vijay Janapa Reddi. Deep reinforcement learning for cyber security.IEEE Trans-actions on Neural Networks and Learning Systems, 2021.Tianwei Ni, Michel Ma, Benjamin Eysenbach, and Pierre-Luc Bacon. When do transformers shine in RL? de-coupling memory from credit assignment. InThirty-seventh Conference on Neural Information ProcessingSystems, 2023. URLhttps://openreview.net/forum?id=APGXBNkt6h.Chris Nota, Philip Thomas, and Bruno C. Da Silva. Posterior value functions: Hindsight baselines forpolicy gradient methods. In Marina Meila and Tong Zhang (eds.),Proceedings of the 38th InternationalConference on Machine Learning, volume 139 ofProceedings of Machine Learning Research, pp. 8238–8247.PMLR, 18–24 Jul 2021. URLhttps://proceedings.mlr.press/v139/nota21a.html.Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. InInternational Confer-ence on Machine Learning, pp. 3878–3887. PMLR, 2018.Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, andDavid Silver. Discovering reinforcement learning algorithms. In H. Larochelle, M. Ranzato, R. Hadsell,M.F. Balcan, and H. Lin (eds.),Advances in Neural Information Processing Systems, volume 33, pp.1060–1070. Curran Associates, Inc., 2020. URLhttps://proceedings.neurips.cc/paper/2020/file/0b96d81f0494fde5428c7aea243c9157-Paper.pdf.49Published in Transactions on Machine Learning Research (04/2024)Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney,Tor Lattimore, Csaba Szepesvári, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, andHado van Hasselt. Behaviour suite for reinforcement learning. InInternational Conference on LearningRepresentations, 2020. URLhttps://openreview.net/forum?id=rygf-kSYwH.Hsiao-Ru Pan, Nico Gürtler, Alexander Neitz, and Bernhard Schölkopf. Direct advantage estimation.Ad-vances in Neural Information Processing Systems, 35:11869–11880, 2022.Jack Parker-Holder, Aldo Pacchiano, Krzysztof M Choromanski, and Stephen J Roberts. Eﬀective diversityin population based reinforcement learning.Advances in Neural Information Processing Systems, 33:18050–18062, 2020.Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and Chai Quek. Hierarchical reinforcement learning:A comprehensive survey.ACM Computing Surveys (CSUR), 54(5):1–35, 2021.Judea Pearl. Causal inference in statistics: An overview.Statistics Surveys, 3:96–146, 2009.Judea Pearl et al. Models, reasoning and inference.Cambridge, UK: CambridgeUniversityPress, 19(2):3,2000.Julien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, PaulMuller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego withmodel-free multiagent reinforcement learning.Science, 378(6623):990–996, 2022.Jean Piaget, Margaret Cook, et al.The origins of intelligence in children, volume 8. International UniversitiesPress New York, 1952.Silviu Pitis. Rethinking the discount factor in reinforcement learning: A decision theoretic approach. InProceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 7949–7956, 2019.Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally factoreddynamics.Advances in Neural Information Processing Systems, 33:3976–3990, 2020.Chetan Prakash, Kyle D Stephens, Donald D Hoﬀman, Manish Singh, and Chris Fields. Fitness beats truthin the evolution of perception.Acta Biotheoretica, 69:319–341, 2021.Doina Precup. Eligibility traces for oﬀ-policy policy evaluation.Computer Science Department FacultyPublication Series, pp. 80, 2000a.Doina Precup.Temporal abstraction in reinforcement learning. PhD thesis, University of MassachusettsAmherst, 2000b.Martin L Puterman.Markov decision processes: discrete stochastic dynamic programming. John Wiley &Sons, 2014.Martin L Puterman and Moon Chirl Shin. Modiﬁed policy iteration algorithms for discounted markovdecision problems.Management Science, 24(11):1127–1137, 1978.Sébastien Racanière, Théophane Weber, David Reichert, Lars Buesing, Arthur Guez, DaniloJimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning.Advances in neural information processing systems,30, 2017.Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understandingby generative pre-training.OpenAI blog, 2018.Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.Hazhir Rahmandad, Nelson Repenning, and John Sterman. Eﬀects of feedback delay on learning.SystemDynamics Review, 25(4):309–338, 2009.50Published in Transactions on Machine Learning Research (04/2024)David Raposo, Sam Ritter, Adam Santoro, Greg Wayne, Theophane Weber, Matt Botvinick, Hado vanHasselt, and Francis Song. Synthetic returns for long-term credit assignment.CoRR, 2021.Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, and Jürgen Schmidhuber. Hindsight policy gradients.InInternational Conference on Learning Representations, 2019. URLhttps://openreview.net/forum?id=Bkg2viA5FQ.Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent.arXivpreprint arXiv:2205.06175, 2022.Zhizhou Ren, Ruihan Guo, Yuan Zhou, and Jian Peng. Learning long-term reward redistribution via ran-domized return decomposition. InInternational Conference on Learning Representations, 2022. URLhttps://openreview.net/forum?id=lpkGn3k2YdD.Matthew Riemer, Miao Liu, and Gerald Tesauro. Learning abstract options.Advances in neural informationprocessing systems, 31, 2018.Mark Rowland, Will Dabney, and Rémi Munos. Adaptive trade-oﬀs in oﬀ-policy learning. InInternationalConference on Artiﬁcial Intelligence and Statistics, pp. 34–44. PMLR, 2020.Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro, FabioPetroni, Heinrich Kuttler, Edward Grefenstette, and Tim Rocktäschel. Minihack the planet: A sand-box for open-ended reinforcement learning research. InThirty-ﬁfth Conference on Neural InformationProcessing Systems Datasets and Benchmarks Track (Round 1), 2021. URLhttps://openreview.net/forum?id=skFwlyefkWJ.Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. InFrancis Bach and David Blei (eds.),Proceedings of the 32nd International Conference on Machine Learning,volume 37 ofProceedings of Machine Learning Research, pp. 1312–1320, Lille, France, 07–09 Jul 2015a.PMLR. URLhttps://proceedings.mlr.press/v37/schaul15.html.Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.arXiv preprintarXiv:1511.05952, 2015b.Tom Schaul, André Barreto, John Quan, and Georg Ostrovski. The phenomenon of policy churn.arXivpreprint arXiv:2206.00730, 2022.Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and Matthieu Geist. Approximatemodiﬁed policy iteration and its application to the game of tetris.Journal of Machine Learning Research,16(49):1629–1676, 2015.Juergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map them to actions.arXiv preprint arXiv:1912.02875, 2019.Jürgen Schmidhuber. Deep learning in neural networks: An overview.Neural Networks, 61:85–117, 2015.Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess andshogi by planning with a learned model.Nature, 588(7839):604–609, 2020.John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensionalcontinuous control using generalized advantage estimation. InProceedings of the International Conferenceon Learning Representations (ICLR), 2016.John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-tion algorithms.arXiv preprint arXiv:1707.06347, 2017.Donald G Schultz. State functions and linear control systems.McGraw-Hill Book Company, 1967.51Published in Transactions on Machine Learning Research (04/2024)Minah Seo, Luiz Felipe Vecchietti, Sangkeum Lee, and Dongsoo Har. Rewards prediction-based creditassignment for reinforcement learning with sparse binary rewards.IEEE Access, 7:118776–118791, 2019.Mehran Shakerinava and Siamak Ravanbakhsh. Utility theory for sequential decision making. InInternationalConference on Machine Learning, pp. 19616–19625. PMLR, 2022.Claude E Shannon. Programming a computer for playing chess.The London, Edinburgh, and DublinPhilosophical Magazine and Journal of Science, 41(314):256–275, 1950.David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, JulianSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of gowith deep neural networks and tree search.nature, 529(7587):484–489, 2016.David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, MarcLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learningalgorithm that masters chess, shogi, and go through self-play.Science, 362(6419):1140–1144, 2018.Satinder Singh, Richard L Lewis, and Andrew G Barto. Where do rewards come from. InProceedings of theannual conference of the cognitive science society, pp. 2601–2606. Cognitive Science Society, 2009.Satinder P Singh and Richard S Sutton. Reinforcement learning with replacing eligibility traces.Machinelearning, 22(1):123–158, 1996.Matthew Smith, Herke Hoof, and Joelle Pineau. An inference-based policy gradient method for learningoptions. InInternational Conference on Machine Learning, pp. 4703–4712. PMLR, 2018.Matthew J Sobel. The variance of discounted markov decision processes.Journal of Applied Probability, 19(4):794–802, 1982.Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Jaskowski, and Jürgen Schmidhuber.Training agents using upside-down reinforcement learning.CoRR, abs/1912.02877, 2019. URLhttp://arxiv.org/abs/1912.02877.Miroslav Štrupl, Francesco Faccio, Dylan R Ashley, Jürgen Schmidhuber, and Rupesh Kumar Srivastava.Upside-down reinforcement learning can diverge in stochastic environments with episodic resets.arXivpreprint arXiv:2205.06595, 2022.Ke Sun, Bei Jiang, and Linglong Kong. How does value distribution in distributional reinforcement learninghelp optimization?arXiv preprint arXiv:2209.14513, 2022.Richard S Sutton.Temporal credit assignment in reinforcement learning. PhD thesis, University of Mas-sachusetts, 1984.Richard S Sutton. Learning to predict by the methods of temporal diﬀerences.Machine learning, 3:9–44,1988.Richard S. Sutton. The reward hypothesis.http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html, 2004.Richard S Sutton and Andrew G Barto.Reinforcement Learning: an introduction. MIT Press, 2nd edition,2018. ISBN 9781626239777. URLhttp://incompleteideas.net/book/RLbook2020.pdf.Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework fortemporal abstraction in reinforcement learning.Artiﬁcial intelligence, 112(1-2):181–211, 1999.Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, andDoina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensori-motor interaction. InThe 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 761–768, 2011.52Published in Transactions on Machine Learning Research (04/2024)Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of oﬀ-policy temporal-diﬀerence learning.The Journal of Machine Learning Research, 17(1):2603–2631, 2016.Yunhao Tang and Alp Kucukelbir. Hindsight expectation maximization for goal-conditioned reinforcementlearning. InInternational Conference on Artiﬁcial Intelligence and Statistics, pp. 2863–2871. PMLR, 2021.Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. InInternational conference on intelligent robots and systems, pp. 5026–5033. IEEE, 2012.Hado van Hasselt and Richard S Sutton. Learning to predict independent of span.arXiv preprintarXiv:1508.04582, 2015.Hado van Hasselt and Marco A Wiering. Using continuous action spaces to solve discrete problems. In2009International Joint Conference on Neural Networks, pp. 1149–1156. IEEE, 2009.Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. InProceedings of the AAAI conference on artiﬁcial intelligence, volume 30, 2016.Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deepreinforcement learning and the deadly triad.arXiv preprint arXiv:1812.02648, 2018.Hado van Hasselt, Sephora Madjiheurem, Matteo Hessel, David Silver, André Barreto, and Diana Borsa.Expected eligibility traces. InProceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35,pp. 9997–10005, 2021.Hado P van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcementlearning?Advances in Neural Information Processing Systems, 32, 2019.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30,2017.David Venuto, Elaine Lau, Doina Precup, and Oﬁr Nachum. Policy gradients incorporating the future. InInternational Conference on Learning Representations, 2022. URLhttps://openreview.net/forum?id=EHaUTlm2eHg.Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, Rémi Munos, and Matthieu Geist. Lever-age the average: an analysis of kl regularization in reinforcement learning.Advances in Neural InformationProcessing Systems, 33:12163–12174, 2020a.Nino Vieillard, Olivier Pietquin, and Matthieu Geist. Munchausen reinforcement learning.Advances inNeural Information Processing Systems, 33:4235–4246, 2020b.Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojtek Czarnecki,Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds, Dan Horgan, ManuelKroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre,Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gul-cehre, Ziyu Wang, Tobias Pfaﬀ, Toby Pohlen, Dani Yogatama, Julia Cohen, Katrina McKinney, OliverSmith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, and David Sil-ver. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II.https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019.Jianhao Wang, Wenzhe Li, Haozhe Jiang, Guangxiang Zhu, Siyuan Li, and Chongjie Zhang. Oﬄine re-inforcement learning with reverse model-based imagination.Advances in Neural Information ProcessingSystems, 34:29420–29432, 2021.Tony Tong Wang, Adam Gleave, Nora Belrose, Tom Tseng, Joseph Miller, Michael D Dennis, Yawen Duan,Viktor Pogrebniak, Sergey Levine, and Stuart Russell. Adversarial policies beat professional-level go ais.arXiv preprint arXiv:2211.00241, 2022.53Published in Transactions on Machine Learning Research (04/2024)Zhe Wang and Tianzhen Hong. Reinforcement learning for building controls: The opportunities and chal-lenges.Applied Energy, 269:115036, 2020.Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nandode Freitas. Sample eﬃcient actor-critic with experience replay. InInternational Conference on LearningRepresentations, 2016a.Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling networkarchitectures for deep reinforcement learning. InInternational conference on machine learning, pp. 1995–2003. PMLR, 2016b.Christopher J C H Watkins.Learning from delayed rewards. PhD thesis, King’s College, Cambridge, UnitedKingdom, 1989.Douglas J White. Mean, variance, and probabilistic criteria in ﬁnite markov decision processes: A review.Journal of Optimization Theory and Applications, 56:1–29, 1988.Martha White. Unifying task speciﬁcation in reinforcement learning. InInternational Conference on MachineLearning, pp. 3742–3750. PMLR, 2017.Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas JWalsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing championgran turismo drivers with deep reinforcement learning.Nature, 602(7896):223–228, 2022.Zhongwen Xu, Hado P van Hasselt, and David Silver. Meta-gradient reinforcement learning.Advances inneural information processing systems, 31, 2018.Zhongwen Xu, Hado P van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, and David Silver. Meta-gradient reinforcement learning with an objective discovered online.Advances in Neural InformationProcessing Systems, 33:15254–15264, 2020.Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games withlimited data.Advances in Neural Information Processing Systems, 34:25476–25488, 2021.Haiyan Yin, Shuicheng YAN, and Zhongwen Xu. Distributional meta-gradient reinforcement learning. InThe Eleventh International Conference on Learning Representations, 2023. URLhttps://openreview.net/forum?id=LGkmUauBUL.Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado van Hasselt, David Silver,and Satinder Singh. Self-tuning deep reinforcement learning.arXiv preprint arXiv:2002.12928, 2020.Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls,David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement learning.arXivpreprint arXiv:1806.01830, 2018.Shangtong Zhang, Vivek Veeriah, and Shimon Whiteson. Learning retrospective knowledge with reversereinforcement learning.Advances in Neural Information Processing Systems, 33:19976–19987, 2020.Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In Kamalika Chaudhuri,Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),Proceedings of the 39thInternational Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research,pp. 27042–27059. PMLR, 17–23 Jul 2022. URLhttps://proceedings.mlr.press/v162/zheng22c.html.Zeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient methods.Advances in Neural Information Processing Systems, 31, 2018.Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van Hasselt, David Silver,and Satinder Singh. What can learned intrinsic rewards capture? InInternational Conference on MachineLearning, pp. 11436–11446. PMLR, 2020.54