Under review as submission to TMLR
PAC Privacy Preserving Diffusion Models
Anonymous authors
Paper under double-blind review
Abstract
Data privacy protection is garnering increased attention among researchers. Diffusion mod-
els (DMs), particularly with strict differential privacy, can potentially produce images with
both high privacy and visual quality. However, challenges arise such as in ensuring robust
protection in privatizing specific data attributes, areas where current models often fall short.
To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a
model leverages diffusion principles and ensure Probably Approximately Correct (PAC) pri-
vacy. We enhance privacy protection by integrating a private classifier guidance into the
Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of
models, we have developed a novel metric to gauge privacy levels. Our model, assessed with
this new metric and supported by Gaussian matrix computations for the PAC bound, has
shown superior performance in privacy protection over existing leading private generative
models according to benchmark tests.
1 Introduction
Modern deep learning models, fortified with differential privacy as defined by Dwork et al. (2006), have
been instrumental in significantly preserving the privacy of sensitive data (Dwork et al., 2014). DP-SGD
Abadi et al. (2016), a pioneering method for training deep neural networks within the differential privacy
framework, applies gradient clipping at each step of the SGD (stochastic gradient descent) to enhance privacy
protection effectively.
Diffusion models (DMs) (Song & Ermon, 2019; 2020; Dhariwal & Nichol, 2021) have emerged as state-of-
the-art generative models, setting new benchmarks in various applications, particularly in generating high-
quality images. When trained under strict differential privacy protocols, these DMs can produce images that
safeguard privacy while maintaining high visual fidelity. For instance, DPGEN (Chen et al., 2022) leverages
a randomized response technique to privatize the recovery direction in the Langevin MCMC process for
image generation. The images produced by DPGEN are not only visually appealing but also compliant with
differential privacy standards, although its privacy mechanism has been shown to be data-dependent later
on. Moreover, the Differentially Private Diffusion Models (DPDM) (Dockhorn et al., 2022) adapt DP-SGD
and introduce noise multiplicity in the training process of diffusion models, demonstrating that DPDM can
indeed produce high-utility images while strictly adhering to the principles of differential privacy.
While diffusion models integrated with differential privacy (DP) mark a significant advance in privacy-
preserving generative modeling, several challenges and limitations remain.
•Training modern deep learning models with differential privacy is notoriously difficult, as evidenced
by Tramer & Boneh (2021); He et al. (2023); Ding et al. (2024). Recent progress relies heavily on
the use of large pre-trained models (Li et al., 2022; Golatkar et al., 2022; Yu et al., 2022; Li et al.,
2022) or extensive public training data (De et al., 2022). We believe that such a paradigm is unlikely
to be a holy grail solution, as it does not provide a rigorous privacy guarantee but is more heuristic
in nature. We refer the reader to the recent criticism against this paradigm (Tramèr et al., 2024).
•Most research on diffusion models with DP has concentrated on the privatization of overall image
features. The need to privatize specific attributes, such as facial expressions in human portraits, has
1Under review as submission to TMLR
not been adequately addressed. This oversight suggests a gap in the nuanced application of DP in
generative modeling.
•The absence of a robust privacy measurement for models poses a critical challenge. Without a clear
metric, itbecomesproblematictoassessandcomparethedataprivacyprotectionperformanceacross
different models. This lack of standardized evaluation complicates the advancement and adoption
of privacy-preserving techniques in the field.
These issues highlight the need for continued research and development to overcome the current limitations
of diffusion models with DP and to push the boundaries of privacy protection in generative modeling.
Recently, Xiao & Devadas (2023) introduces a novel definition of privacy known as Probably Approximately
Correct (PAC) Privacy, representing a significant evolution in privacy-preserving methodologies. PAC Pri-
vacy characterizes the information-theoretic hardness to recover sensitive data given arbitrary information
disclosure/leakage during/after any processing. Compared with differential privacy, it has the following
advantages.
•Differential privacy requires bounded sensitivity, which cannot be tightly computed for modern
machine learning models. Artificial modifications are typically required to decompose algorithms
into multiple simpler and analyzable components, e.g., gradient clipping in DP-SGD. The exact
privacy analysis of DP can be, in general, NP-hard. Especially for for deep learning, tight DP
analysis are challenging. However, PAC privacy can be applied to any data processing algorithm (as
a black-box procedure), where security parameters can be produced with arbitrarily high confidence
via Monte-Carlo simulation.
•On the utility side, the magnitude of (necessary) perturbation required in PAC Privacy is not lower
bounded by Θ(√
d)for ad-dimensional release, but could be O(1)(depending on data distribution)
for many practical data processing tasks, which is in contrast to the input-independent worst-case
information-theoretic lower bound like DP. Therefore, PAC Privacy analysis in many applications
can produce much sharpened utility-privacy trade-offs.
To tackle the aforementioned challenges, we have introduced PAC Privacy Preserving Diffusion Models
(P3DM). Drawing from the foundations of DPGEN and harnessing insights from conditional classifier guid-
ance (Dhariwal & Nichol, 2021; Batzolis et al., 2021; Ho & Salimans, 2022), our P3DM incorporates a
conditional private attribute guidance module during the Langevin sampling process. This addition empow-
ers the model to specifically target and privatize certain image attributes with greater precision.
Furthermore, we have crafted a set of privacy evaluation metrics. These metrics operate by measuring
the output class labels of the two nearest neighbor images in the feature space of the Inception V3 model
(Szegedy et al., 2016), using a pretrained classifier. Additionally, we quantify the noise addition Bnecessary
to assure PAC privacy in our model and conduct comparative analyses against the mean L2-norm of Bfrom
various other models.
Through meticulous evaluations that utilize our privacy metrics and benchmarks for noise addition, our
model has proven to offer a superior degree of privacy. It exceeds the capabilities of state-of-the-art (SOTA)
models in this critical aspect, while simultaneously preserving the high quality of synthetic image samples.
These samples remain on par with those produced by the state-of-the-art models, illustrating that our
model does not sacrifice quality for privacy. This achievement underscores our model’s potential to set new
precedents in the domain of privacy-preserving image generation and data protection at large.
Our contributions are summarized as follows:
•We propose the first diffusion model with analysis on its PAC privacy guarantees.
•WeincorporateconditionalprivateclassifierguidanceintotheLangevinSamplingProcess, enhancing
the protection of privacy for specific attributes in images.
2Under review as submission to TMLR
•We introduce a new metric that we developed for assessing the extent of privacy provided by models.
•We compute the noise addition matrix to establish the Probably Approximately Correct (PAC)
upper bound and have conducted a comparative analysis of the norm of this matrix across various
models.
•Through extensive evaluations, we demonstrate that our model sets a new standard in privacy
protection of specific attributes, achieving state-of-the-art (SOTA) results, while maintaining image
quality at a level that is comparable to other SOTA models.
2 Related Work
2.1 Early Works on Differentially Private Image Generation
Image Synthesis with differential privacy has been studied extensively during the research. Recently, there
has been an increased emphasis on utilizing sophisticated generative models to improve the quality of differ-
entially private synthetic data (Hu et al., 2023). Some approaches employ Generative Adversarial Networks
(GANs) (Goodfellow et al., 2014), or GANs that have been trained using the Private Aggregation of Teacher
Ensembles (PATE) framework (Xie et al., 2018; Chen et al., 2020; Harder et al., 2021; Torkzadehmahani
et al., 2019). Other contributions leverage variational autoencoders(VAEs) (Pfitzner & Arnrich, 2022; Jiang
et al., 2022; Takagi et al., 2021), or take advantage of customized architectures (Cao et al., 2021; Harder
et al., 2023). However, there are several limitations for those DP synthesizers: (1) Failure when applying to
high-dimensional data, primarily due to the constraints imposed by discretization. (2) Limited image quality
and lack of expressive generator networks (Cao et al., 2021).
2.2 Differentially Private Diffusion Models
Diffusion models (DMs) (Song & Ermon, 2019; 2020; Dhariwal & Nichol, 2021) , recognized for setting new
standards in image generation, can produce high-quality, privacy-compliant images when trained with dif-
ferential privacy protocols. For example, DPGEN (Chen et al., 2022) employs a data-dependant randomized
response method to privatize the recovery direction in the Langevin MCMC process for image generation.
Furthermore, Differentially Private Diffusion Models (DPDM) (Dockhorn et al., 2022), which adapt DP-
SGD and introduce noise multiplicity, both demonstrate the feasibility of generating visually appealing,
privacy-protective images. Subsequent advancements, including fine-tuning existing models and employing
novel diffusion model architectures, have been made to boost the effectiveness of differentially private image
generation, as detailed in Ghalebikesabi et al. (2023); Lyu et al. (2023). Nevertheless, as previously noted in
the introduction, there remains three key challenges to be addressed. In the following sections, we propose
solutions and methods to tackle these issues.
3 Background
3.1 Differential Privacy
A randomized mechanism Mis said (ε,δ)- differentially private if for any two adjacent datasets DandD′
differing in a single datapoint for any subset of outputs Sas follows (Dwork et al., 2014):
Pr[M(D)∈S]≤eε·Pr[M(D′)∈S] +δ (1)
Here,εis the upper bound on the privacy loss corresponding to M, andδis the probability of violating the
DP constraint.
Differential privacy is a mathematical approach designed to protect individual privacy within datasets. It
offers a robust privacy assurance by enabling data analysis without disclosing sensitive details about any
specific person in the dataset.
3Under review as submission to TMLR
3.2 PAC Privacy
3.2.1 Basic Definition
Definition 3.1. ((δ,ρ,D )PAC Privacy). For a data processing mechanism M, given some data distribution
D, a measure function ρ(.,.), and a finite set X∗, we sayMsatisfies (δ,ρ,D )-PAC Privacy if the following
experiment is impossible: A user generates data Xfrom distribution Dand sendsM(X)to an adversary.
The adversary who knows DandMis asked to return an estimation ˜X∈X∗on X such that with posterior
success probability at least (1−δ),ρ(˜X,X ) = 1.
Definition 3.2. ((∆fδ,ρ,D)PAC Advantage Privacy) Equivalantly, Mcould be defined as (∆fδ,ρ,D)PAC
Advantage Privacy if the posterior advantage measured in f-divergenceDfsatisfies
∆fδ=Df(1δ∥1δρ
o) =δρ
of(δ
δρ
o) + (1−δρ
o)f(1−δ
1−δρ
o), (2)
where (1−δρ
o)represents the optimal prior success rate of recovering X.
1−δρ
o= sup
˜X∈X∗Pr
X∼D/parenleftbig
ρ(˜X,X ) = 1/parenrightbig
, (3)
and1δand1δρ
orepresent two Bernoulli distributions of parameters δandδρ
o, respectively.
The definition above depicts the reconstruction hardness for the attackers to recover the private data dis-
tributionM(X). With a lower bound probability (1−δ), the measure function ρ(.,.)cannot distinguish
the recovery data from the original data. However, the limitation of the naive definition is that the prior
distribution of the public dataset is unknown, resulting in the failure of adversarial inferences.
Definition 3.3. (Mutual Information). For two random variables xandwin some joint distribution, the
mutual information MI(x;w)is defined as
MI(x,w) =H(x)−H(x|w) =DKL(Px,w||Px⊗Pw), (4)
whereDKLdenotes the KL divergence.
Theorem 3.1. For any selected f-divergenceDf, a mechanismM:X∗→Ysatisfies (∆fδ,ρ,D)PAC
Advantage Privacy if
∆fδ=Df/parenleftbig
1δ∥1δρ
o/parenrightbig
≤inf
PWDf/parenleftbig
PX,M(X)∥PX⊗PW/parenrightbig
. (5)
In particular, when we select Dfto be the KL-divergence and PW=PM(X),Msatisfies (∆KLδ,ρ,D)PAC
Advantage Privacy where
∆KLδ=DKL(1δ∥1δρ
o)≤MI/parenleftbig
X;M(X)/parenrightbig
. (6)
Proof.See Appendix A.
In summary,Df/parenleftbig
1δ∥1δρ
o/parenrightbig
quantifies the divergence between optimal a priori and posterior reconstruction,
effectively measuring the difficulty of inference. A higher value of Df/parenleftbig
1δ∥1δρ
o/parenrightbig
signifies greater privacy
leakage. Moreover, since MI/parenleftbig
X;M(X)/parenrightbig
provides an upper bound for Df, a lower value of MI/parenleftbig
X;M(X)/parenrightbig
indicatesstrongerprivacyprotection. Thus, theorem3.1establishesageneralmethodforlinkingthedifficulty
of arbitrary inference to the well-known concept of mutual information. With theorem 3.1, the goal of PAC
privacy is explicit: determining the bound MI/parenleftbig
X;M(X)/parenrightbig
with high confidence.
3.2.2 Noise Determination and Simulated Privacy Guarantee with High Confidence
The natural idea to achieve information leakage control is perturbation: when MI/parenleftbig
X;M(X)/parenrightbig
is not small
enough to produce satisfied PAC Privacy, we may add additional Gaussian noise B, to produce smaller
MI/parenleftbig
X;M(X) +B/parenrightbig
.
4Under review as submission to TMLR
Theorem 3.2 ((Xiao & Devadas, 2023)) .When the mutual information MI(X;M(X))is insufficient to
ensure PAC privacy, additional Gaussian noise B∼N(0,ΣB)can be introduced to yield a reduced mutual
information MI(X;M(X) +B), such that MI(X;M(X) +B)satisfies
MI(X;M(X) +B)≤1
2·logdet/parenleftbig
Id+ ΣM(X)·Σ−1
B/parenrightbig
. (7)
In particular, let the eigenvalues of ΣM(X)be(λ1,...,λd), then there exists some ΣBsuch that E[∥B∥2
2] =
(/summationtextd
j=1/radicalbig
λj)2, and MI (X;M(X) +B)≤1/2.
Proof.See Appendix B.
Therefore, we have a simple upper bound on the mutual information after perturbation which only requires
the knowledge of the covariance of M(X). Another important and appealing property is that the noise
calibrated to ensure the target mutual information bound is notexplicitly dependent on the output dimen-
sionalitydbut instead on the square root sum of eigenvalues of ΣM(X). WhenM(X)is largely distributed
in ap-rank subspace of Rd, the above theorem suggests that a noise of scale O(√p)is needed. Depending on
data distribution, if pis a constant, then the noise can be as low as O(1). This is different from DP where
the expected l2norm of noise is in a scale of Θ(√
d)given constant L2-norm sensitivity.
Based on Theorem 3.2, we can use an automatic protocol Algorithm 1 to determine ΣBand produce an
upper bound such that MI (X;M(X) +B)≤(v+β)with high confidence, where vandβare positive
parameters selected as explained below. After sufficiently many simulations, the following theorem ensures
that we can obtain accurate enough estimation with arbitrarily high probability.
Theorem 3.3 (Xiao & Devadas (2023)) .Assume thatM(X)∈Rdand∥M(X)∥2≤rfor some constant
runiformly for any X, and apply Algorithm 1 to obtain the Gaussian noise covariance ΣBfor a specified
mutual information bound v+β.vandβcan be chosen independently, and cis a security parameter. Then,
there exists a fixed and universal constant κsuch that one can ensure MI (X;M(X) +B)≤v+βwith
confidence at least (1−γ)once the selections of c,mandγsatisfy,
c≥κr/parenleftigg
max/braceleftigg/radicalbigg
d+ log(4/γ)
m,d+ log(4/γ)
m/bracerightigg
+/radicalbigg
dlog(4/γ)
m/parenrightigg
. (6)
Another way to interpret this noise determination is that a smaller E||B||2implies a smaller covariance
matrix ΣBand lesser noise addition on M(X), which indicates XandM(X)have less mutual information.
Therefore, less noise addition signifies a higher privacy protection of the model M.
3.3 Conditional Diffusion Models
Dhariwal & Nichol (2021) proposed a diffusion model that is enhanced by classifier guidance; it has been
shown to outperform existing generative models. By using true labels of datasets, it is possible to train
a classifier on noisy images xtat various timesteps pϕ(y|xt,t), and then use this classifier to guide the
reverse sampling process ∇xtlogpϕ(y|xt,t). What begins as an unconditional reverse noising process is thus
transformed into a conditional one, where the generation is directed to produce specific outcomes based on
the given labels
pθ,ϕ(xt|xt+1,y) =Zpθ(xt|xt+1)pϕ(y|xt) (8)
WhereZis a normalizing constant. According to unconditional reverse process, which predicts timestep xt
fromxt−1leveraging Gaussian distribution, we have
pθ(xt|xt+1)∼N(µ,Σ) (9)
logpθ(xt|xt+1) =−1
2(xt−µ)TΣ−1(xt−µ) +C (10)
5Under review as submission to TMLR
Algorithm 1 (1 -γ)-Confidence Noise Determination of Deterministic Mechanism
Require: Diffusion-Privacy model M, data distribution D, sampling complexity m, security parameter c,
and mutual information quantities νandβ.
1:TrainMmodel with data X.
2:fork= 1,2,...,mdo
3:sample images y(k)=M(X(k)).
4:end for
5:Calculate empirical mean ˆµ=1
m/summationtextm
k=1y(k)and the empirical covariance estimation ˆΣ =1
m/summationtextm
k=1(y(k)−
ˆµ)(y(k)−ˆµ)T.
6:Apply singular value decomposition (SVD) on ˆΣand obtain the decomposition as ˆΣ = ˆUˆΛˆUT, where ˆΛ
is the diagonal matrix of eigenvalues λ1≥λ2≥...≥λd.
7:Determine the maximal index j0= arg max jλjfor thoseλj>c.
8:ifmin 1≤j≤j0,1≤d(λj−ˆλj)>r/radicalbig
d/c+ 2cthenthen
9:forj= 1,2,...,ddo
10:Determine the j-th element of a diagonal matrix ABas
λB,j=2ν
/radicalbig
λj+ 10cν/β·/parenleftig/summationtextd
j=1/radicalbig
λj+ 10cν/β/parenrightig
11:end for
12:Determine the Gaussian noise covariance as ΣB=ˆUABˆUT.
13:else
14:Determine the Gaussian noise covariance as ΣB=/parenleftig/summationtextd
j=1λj+dc/(2ν)/parenrightig
·Id.
15:end if
16:Return Gaussian covariance matrix ΣB.
If we assume that logϕp(y|xt)has low curvature compared to Σ−1, then we can approximate logpϕ(y|xt)
using a Taylor expansion around xt=µas
logpϕ(y|xt)≈logpϕ(y|xt)|xt=µ+ (xt−µ)∇xtlogpϕ(y|xt)|xt=µ
= (xt−µ)g+C1(11)
wheregis the gradient of classifier g=∇xtlogpϕ(y|xt)|x=µandC1is the constant.
Therefore, combing Eqn. 10 and 11 gives us
log(pθ(xt|xt+1)pϕ(y|xt))∼N(µ+ Σg,Σ) (12)
The investigation has led to the conclusion that the conditional transition operator can be closely estimated
using a Gaussian. And the Gaussian resembles the unconditional transition operator, with the distinction
that its mean is adjusted by the product of the covariance matrix, Σ, and the vector g. This methodology
allows for the generation of high-quality, targeted synthetic data.
4 Methods
We introduce the PAC Privacy Preserving Diffusion Model (P3DM), which aims to safeguard privacy for
specificattributes. OurmethodisinspiredbyDPGEN(Chenetal.,2022). WhileDPGENassertscompliance
with stringent ϵ-differential privacy, recent findings from Dockhorn et al. (2022) indicate that DPGEN is,
in fact, data-dependent. The RR mechanism M(d)in DPGEN holds validity only within the perturbed
dataset. Specifically, if an element zbelongs to the output set Obut not to the perturbed dataset d, then
Pr[M(d) =O] = 0, which contravenes the differential privacy definition. Different from DPGEN, which
turns out to have no formal privacy guarantees, our method satisfies PAC privacy.
6Under review as submission to TMLR
Algorithm 2 PAC-Private Adapted Randomized Response Algorithm
Require: a sensitive dataset {xi:i= 1,2,...,m}m
i=1; sample number k
1:Samplingkimage candidates from {xi}m
i=1, with the sampling probability for each image Eqn. 14, to
construct set X←{xj:max(xi−xj)/σj≤β,j∈[m]}
2:Privatize images in Xwith RR from Eqn. 13 and obtain H(xi)
3:M(˜xi) =H(xi) +B;B∼N(0,ΣB)
4:ReturnM(˜xi)
Given a sensitive dataset {xi:i= 1,2,...,m}m
i=1, we first sample an image by ˜xi←N (xi,σ2I). Next, we
utilizes the random response method as follows:
Pr[H(˜xi) =w] =

eϵ
eϵ+k−1, w =xi
1
eϵ+k−1, w =x′
i∈X\xi(13)
In the equation, X ={xj:max(˜xi−xj)/σj≤β,j∈[m]}(where “max” is over the dimensions of ˜xi−xj),
|X|=k≥2, where the hyperparameter kdenotes the number of selected candidates from mtraining images,
and the sampling probability for each image is given by
p(xi) = exp(−d∞(xi,˜xi,σi))/m/summationdisplay
a=1exp(−d∞(xi,˜xi,σa)) (14)
In other words, the mechanism H(·)consists of 2 steps:
1. Sampling kimage candidates from {xi}m
i=1, with the sampling probability for each image from
Eqn. 14, to construct set X.
2. Privatize images in Xwith RR from Eqn. 13.
After applying H(·), we then add Gaussian noise Bto the image to get ˜xi←H(˜xi) +Bfor achieving PAC
privacy. With these methods, ˜xiis designed to point to one of its knearest neighbors with certain probability
Pr[H(˜xi) =xr
i], giving us the privatized the recovery direction dr
i= (˜xi−xr
i)/σ2
i.
Following perturbation and privatization, we learn an energy function qθ(˜x)by optimizing the following loss
function:
l(θ,σ) =1
2Ep(x)E˜x∼N(x,σ2)/bracketleftig
∥d−∇xlogqθ(˜x)∥2/bracketrightig
(15)
After getting the optimal ∇xlogqθ(˜x) = (˜x−xr)/σ2from Eqn. 15, we can then synthesize images using the
Langevin MCMC sampler with the optimal output ∇xlogqθ(˜x)as:
xt←xt−1+αi
2qθ(xt−1) +√αizt,t= 0,1,2,...T. (16)
whereαidenotes the step size.
4.1 Conditional Private Langevin Sampling
We introduce a method of conditional private guidance within the Langevin sampling algorithm, detailed
in Algorithm 3. This method is designed to protect specific attributes within the original datasets against
adversarial attacks.
Prior to commencing the sampling iterations, it is essential to obtain class labels from a balanced attribute of
the dataset. The necessity for a dataset attribute that possesses an approximately equal number of negative
and positive samples is crucial for training classifiers to achieve exceptional performance. Subsequently, we
select a random label yifromyand fix it to initialize x0with a predetermined distribution, such as the
standard normal distribution.
7Under review as submission to TMLR
Algorithm 3 PAC-Private Conditional Guidance Langevin Dynamics Sampling
Require: class labels y={y1,y2,...yn}from one of balanced dataset attributes;
gradient scale k;{δi}L
i=1,ϵ,T;
optimal output qθfrom Eqn. 15;
pretrained classifier model on noisy images cθ
1:Randomly sample ynfromy
2:Initializex0
3:forifrom 1toLdo
4:stepsizeαi←δ2
i/δ2
L
5:fortfrom 1toTdo
6:Sample noise zt∼N(0,I)
7:xt←xt−1+αi
2qθ(xt−1,δi) +√αizt+kΣθ(xt−1)∇xt−1logcθ(yn|xt−1)
8:end for
9:x0←xT
10:end for
11:ReturnxT
Drawing on conditional image generation (Dhariwal & Nichol, 2021; Batzolis et al., 2021; Ho & Salimans,
2022), we adapt the model from the vanilla Langevin dynamic samplings with the selected attributes with
conditional guidance
kΣθ(xt−1)∇xt−1logcθ(yn|xt−1) (17)
wherekis the gradient scale that can be tuned according to the performance of the model, Σθ(xt−1)is
the covariance matrix from reverse process Eqn. 9. In Eqn. 17, the term ∇xt−1logcθ(yn|xt−1)directs the
Langevin sampling process toward a specific class label yn, which is sampled prior to the inference.
This private guidance during the inference phase ensures that the synthesized images are protected from
privacy breaches related to designated attributes. At its core, this method involves intentionally modifying
certain generated trajectories by randomly perturbing the image label yn, which is then used by a pretrained
classifier to guide the image generation process. This strategy is aimed at diverting certain image attributes
that we wish to protect. For instance, consider a scenario where an original dataset image depicting Celebrity
A wearing eyewear, a known trait of the celebrity. If our goal is to privatize the attribute of “wearing
glasses,” private classifier guidance can effectively achieve this. When sampling occurs under the influence
of this guidance, the attributes are likely to vary, creating a chance that the resulting synthesized image
based on Celebrity A might be rendered without glasses. This modification effectively protects the attribute
of ’glasses’ from being a consistent element in the generated depictions of Celebrity A. Consequently, the
images generated with this approach offer a higher degree of privacy compared to those produced by the
original DPGEN method.
4.2 Privacy Metrics
Most privacy-preserving generative models prioritize assessing the utility of images for downstream tasks,
yet often overlook the crucial metric of the models’ own privacy. To bridge this gap in evaluating privacy
extent, we have developed a novel algorithm that computes a privacy score for the models obtained. As
per Algorithm 4, we commence by preparing images xigenerated from Algorithm 3, the original dataset
imagesxG
k, and alongside the classifier model which trained separately with clean images (different from the
classifier from Algorithm 3). Subsequently, we process all synthesized images through InceptionV3 (Szegedy
et al., 2016).
After obtaining the feature vector output from InceptionV3, we locate the feature vector of a ground truth
image that has the smallest L2 distance to that of the synthesized image, effectively finding the nearest
ground truth neighbor. We then test whether a pretrained classifier model can differentiate these two
images. Inability of the classifier to distinguish between the two indicates that the specific attributes, even
in images most similar to the original, are well-protected. Thus, our method successfully preserves privacy for
specific attributes. Finally, we compute the average probability of incorrect classification by the pretrained
8Under review as submission to TMLR
Algorithm 4 Privacy Score
Require: imagesxigenerated by algorithm 3;
ground truth images xG
k;
sample number n;
pretrained InceptionV3 model Iθ;
pretrained classifier model cθ.
1:private score s←0
2:forxifromx0toxndo
3:findargmink||Iθ(xi),Iθ(xG
k)||2
4:ifcθ(xi)̸=cθ(xG
k)then
5:s←s+ 1
6:end if
7:end for
8:Returns/n
model to establish our privacy score. The greater the privacy score a model achieves, the more robust its
privacy protection is deemed to be.
4.3 PAC Privacy Proof of Our Model
Since the randomized response method adds Gaussian noise Zin the random response mechanism, it can be
proven to be PAC private using Theorem 3.1 and Theorem 3.2 in the paper:
•Algorithm 2 can be combined and written as H(X)+B, whereH(X)represents the RR mechanism
andBdenotes Gaussian noise with B∼N(0,ΣB).
•Applying Theorem 3.2 and Theorem 3.3, we can yield a satisfied upper bound (with arbitary high
confidence) for the whole process MI(X;M(X))≤1
2·logdet/parenleftbig
Id+ ΣM(X)·Σ−1
B/parenrightbig
, whereM(X) =
H(X) +B.
•Hence,MI(X;M(X))can be used to produce an upper bound for the attack success probability
via Theorem 3.1.
What’s more, Algorithm 3 is a heuristic method that further enhances privacy. We calculate noise Bto
measure its privacy strength, achieving the best performance as shown in Table 1. Of independent interest
to ensure the PAC privacy of Algorithm 3 itself, we simply need to add noise Bto the final xTin Algorithm 3.
5 Experiments
5.1 Datasets
Our experiments were carried out using the CelebA (Liu et al., 2015) datasets. We specifically targeted
attributes that have a balanced distribution of positive and negative samples, as noted in Rudd et al. (2016),
to facilitate the training of classifiers. Consequently, we selected attributes like gender and smile from the
CelebA dataset (referred to as CelebA-gender and CelebA-smile, respectively) for sampling in Algorithm 3.
All the images used in these experiments were of the resolution 64×64.
5.2 Baselines
In our study, we consider DPGEN (Chen et al., 2022), DPDM (Dockhorn et al., 2022) and DP-MEPF
(Harder et al., 2023) as baseline methods. These approaches, except for DPGEN, which is not a rigorous
DP, excel in synthesizing images under differential privacy (DP) constraints, and they stand out for their
exceptional sample quality in comparison to other DP generative models. These models serve as important
benchmarks against which we evaluate the performance and efficacy of our proposed method.
9Under review as submission to TMLR
(a) DPDM FID = 117, Pri-
vacy Score = 0.47
(b) DPGEN FID = 39.16,
Privacy Score = 0.4
(c) DP-MEPF FID = 57.5,
Privacy Score = 0.4
(d) P3DM (Ours) FID =
37.96, Privacy Score = 0.56
Figure 1: CelebA images generated from DPDM, DPGEN and our model from left to right with image
resolution 64×64.
5.3 Evaluation Metrics
In our evaluation process, we validate the capability of our PAC Privacy Preserving Diffusion Model to
generate high-resolution images using the key metric: the Frechet Inception Distance (Heusel et al., 2017).
This metric is widely recognized and utilized in the field of generative models to assess the visual fidelity of
the images they produce.
Additionally, to demonstrate our model’s effectiveness in preventing privacy leakage, we employ our unique
privacy metrics as outlined in Algorithm 4. We compare the mean norm of the Gaussian noise E||B||2
as detailed in Theorem 3.2. For our experiments, we have chosen the hyperparameters ν=β= 0.5and
γ= 0.01. This approach allows us to comprehensively assess not just the quality of the images generated,
but also the strength of privacy protection our model offers. It is important to note that hyperparameter
tuning does incur an additional privacy cost, albeit modest (Liu & Talwar, 2019; Papernot & Steinke, 2022;
Ding & Wu, 2023; Xiang et al., 2024). For simplicity we ignore the privacy overhead due to hyperparameter
tuning for all methods.
5.4 Empirical Results And Analysis
It is important to note that the ξandτpresented within the tables below is merely a hyperparameter derived
from the Randomized Response (RR) as indicated in Eqn. 13. This ξandτshould not be confused with the
εfromε-Differential Privacy (DP), where ξindicates the model is PAC private, and τindicates the model is
neither PAC private nor DP. We will later illustrate how our model assures privacy through the automatic
control of mutual information for a PAC privacy guarantee in this section.
Fig. 3 details the evaluation results of image visual quality and privacy score on the CelebA dataset with
a resolution of 64×64, where each datapoint in the figure, or each entry in the table, consists of mean
and standard deviation from 3 experimental results with the same epsilon and different random seeds. By
examining the table, we can see that our model achieves image quality comparable to the state-of-the-art
model DPGEN (Chen et al., 2022), a conclusion also supported by Fig.1.
Additionally, from Fig. 3, our model registers the highest performance in privacy score when having similar
FIDs, signaling an enhancement in our model’s ability to preserve privacy without substantially impacting
image utility. The assertion is further supported by Fig. 2, wherein the CelebA dataset is filtered based on
the ’smile’ attribute. Even upon querying the nearest images of P3DM samples, we can distinctly observe
that while all P3DM samples exhibit the absence of a smile, the nearest neighbors predominantly display
smiling faces. This highlights that, in contrast to other models, the closest images between the generated
dataset and the ground truth datasets are notably similar in terms of the ’smile’ expression, while the images
generated by our model demonstrate distinctive features. Hence, the P3DM model effectively conceals the
’smile’ attribute.
10Under review as submission to TMLR
(a) DPGEN for ε=∞
(b) DPDM for ε= 10
(c) DPGEN for ε= 10
(d) P3DM-Smile (Ours) for ξ= 10
Figure 2: Generated images (the second row) and their nearest neighbors measured by the l2distance
between images from CelebA-smile dataset (the first row), with image resolution 64 ×64.
11Under review as submission to TMLR
Table 1: Estimated level of noise Bto approximately ensure PAC privacy
Methods DP PAC Privacy Heuristic E||B||2↓
P3DM-Gender ξ= 5 283.03±2.25
P3DM-Smile ξ= 5 280.60±2.57
DPGEN τ= 5 281.3±2.78
DP-MEPF ε= 5 325.6±3.62
P3DM-Gender ξ= 10 328.80±1.24
P3DM-Smile ξ= 10 327.98±1.45
DPGEN τ= 10 329.48±1.6
DPDM ε= 10 335.83±1.21
DP-MEPF ε= 10 330.75±1.33
P3DM-Gender ξ=∞ 332.87±1.15
P3DM-Smile ξ=∞ 331.67±1.3
DPGEN τ=∞ 332.02±1.32
DPDM ε=∞ 335.83±1.56
DP-MEPF ε=∞ 333.48±1.79
Furthermore, in Table 1, we compute the multivariate Gaussian matrix Bwith a 1−γnoise determination
for the deterministic mechanism Mfollowed the work by Xiao & Devadas (2023). From this, we show
that under the same confidence level of 1−γ= 0.99to ensureMI(X;M(X) +B)≤1, the PAC Privacy
Preserving Diffusion Model exhibits the smallest norm on E||B||2. This demonstrates that data processed by
our model retains the least mutual information with the original dataset, thus affirming our model’s superior
performance in privacy protection.
40 60 80 100 120 140 160 180
FID0.30.40.50.60.7Privacy Score
Privacy Score vs. FID with Error Bars
DP-MEPF
DPDM
DPGEN
P3DM-Gender
P3DM-Smile
Figure 3: Privacy score and FID curve of all datapoints from different models. Each datapoint in
the figure consists of mean and standard deviation from 3 experimental results with the same epsilon and
different random seeds. Top-right corner is preferred. The curve from our method, pushes the frontier to
the upper-right over DPGEN (Chen et al., 2022), DPDM (Dockhorn et al., 2022) and DP-MEPF (Harder
et al., 2023). For more details related to data points in this figure, please refer to Appendix C.
6 Conclusion
We introduce the PAC Privacy Preserving Diffusion Model (P3DM), which incorporates conditional private
classifier guidance into the Langevin Sampling process to selectively privatize image features. In addition,
we have developed and implemented a unique metric for evaluating privacy. This metric involves comparing
a generated image with its nearest counterpart in the dataset to assess whether a pretrained classifier can
differentiate between the two. Furthermore, we calculate the necessary additional noise Bto ensure PAC
privacy and benchmark the noise magnitude against other models. Our thorough empirical and theoretical
testing confirms that our model surpasses current state-of-the-art private generative models in terms of
privacy protection while maintaining comparable image quality.
12Under review as submission to TMLR
References
MartinAbadi, AndyChu, IanGoodfellow, HBrendanMcMahan, IlyaMironov, KunalTalwar, andLiZhang.
Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer
and communications security , pp. 308–318, 2016.
Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, and Christian Etmann. Conditional image
generation with score-based diffusion models. arXiv preprint arXiv:2111.13606 , 2021.
Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, and Karsten Kreis. Don’t generate me: Training differ-
entially private generative models with sinkhorn divergence. Advances in Neural Information Processing
Systems, 34:12480–12492, 2021.
Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradient-sanitized approach for learn-
ing differentially private generators. Advances in Neural Information Processing Systems , 33:12673–12684,
2020.
Jia-Wei Chen, Chia-Mu Yu, Ching-Chia Kao, Tzai-Wei Pang, and Chun-Shien Lu. Dpgen: Differentially
private generative energy-guided network for natural image synthesis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 8387–8396, 2022.
Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlocking high-accuracy
differentially private image classification through scale. arXiv preprint arXiv:2204.13650 , 2022.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural
information processing systems , 34:8780–8794, 2021.
Youlong Ding and Xueyang Wu. Revisiting hyperparameter tuning with differential privacy, 2023. URL
https://arxiv.org/abs/2211.01852 .
Youlong Ding, Xueyang Wu, Yining Meng, Yonggang Luo, Hao Wang, and Weike Pan. Delving into differ-
entially private Transformer. In Proceedings of the 41st International Conference on Machine Learning ,
volume 235, pp. 11049–11071, 2024.
Tim Dockhorn, Tianshi Cao, Arash Vahdat, and Karsten Kreis. Differentially Private Diffusion Models.
arXiv:2210.09929 , 2022.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New
York, NY, USA, March 4-7, 2006. Proceedings 3 , pp. 265–284. Springer, 2006.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and
Trends ®in Theoretical Computer Science , 9(3–4):211–407, 2014.
Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De,
Samuel L Smith, Olivia Wiles, and Borja Balle. Differentially private diffusion models generate useful
synthetic images. arXiv preprint arXiv:2302.13861 , 2023.
Aditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth, Michael Kearns, and Stefano Soatto.
Mixed differential privacy in computer vision. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 8376–8386, 2022.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing
systems, 27, 2014.
Frederik Harder, Kamil Adamczewski, and Mijung Park. Dp-merf: Differentially private mean embeddings
with randomfeatures for practical privacy-preserving data generation. In International conference on
artificial intelligence and statistics , pp. 1819–1827. PMLR, 2021.
13Under review as submission to TMLR
Frederik Harder, Milad Jalali, Danica J Sutherland, and Mijung Park. Pre-trained perceptual features
improve differentially private image generation. Transactions on Machine Learning Research , 2023.
Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai
Yu, and Jiang Bian. Exploring the limits of differentially private deep learning with group-wise clipping.
InInternational Conference on Learning Representations , 2023.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural infor-
mation processing systems , 30, 2017.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 , 2022.
Yuzheng Hu, Fan Wu, Qinbin Li, Yunhui Long, Gonzalo Munilla Garrido, Chang Ge, Bolin Ding, David
Forsyth, Bo Li, and Dawn Song. Sok: Privacy-preserving data synthesis. arXiv preprint arXiv:2307.02106 ,
2023.
Dihong Jiang, Guojun Zhang, Mahdi Karami, Xi Chen, Yunfeng Shao, and Yaoliang Yu Dp2-vae. Differen-
tially private pre-trained variational autoencoders. arXiv preprint arXiv:2208.03409 , 2022.
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong
differentially private learners. In International Conference on Learning Representations , 2022.
Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the 51st ACM
Symposium on Theory of Computing , pp. 298–309, 2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Saiyue Lyu, Margarita Vinaroz, Michael F Liu, and Mijung Park. Differentially private latent diffusion
models.arXiv preprint arXiv:2305.15759 , 2023.
Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In Interna-
tional Conference on Learning Representations , 2022.
Bjarne Pfitzner and Bert Arnrich. Dpd-fvae: Synthetic data generation using federated variational autoen-
coders with differentially-private decoder. arXiv preprint arXiv:2211.11591 , 2022.
Mark S Pinsker, Vyacheslav V Prelov, and Sergio Verdu. Sensitivity of channel capacity. IEEE Transactions
on Information Theory , 41(6):1877–1888, 1995.
Ethan M Rudd, Manuel Günther, and Terrance E Boult. Moon: A mixed objective optimization network
for the recognition of facial attributes. In Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14 , pp. 19–35. Springer, 2016.
Igal Sason and Sergio Verdú. f-divergence inequalities. IEEE Transactions on Information Theory , 62(11):
5973–6006, 2016.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems , 32, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances
in neural information processing systems , 33:12438–12448, 2020.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 2818–2826, 2016.
Shun Takagi, Tsubasa Takahashi, Yang Cao, and Masatoshi Yoshikawa. P3gm: Private high-dimensional
data release via privacy preserving phased generative model. In 2021 IEEE 37th International Conference
on Data Engineering (ICDE) , pp. 169–180. IEEE, 2021.
14Under review as submission to TMLR
Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. Dp-cgan: Differentially private synthetic
data and label generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops , pp. 0–0, 2019.
Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more data).
InInternational Conference on Learning Representations , 2021.
Florian Tramèr, Gautam Kamath, and Nicholas Carlini. Position: Considerations for differentially pri-
vate learning with large-scale public pretraining. In Proceedings of the 41st International Conference on
Machine Learning , volume 235, pp. 48453–48467, 2024.
Zihang Xiang, Tianhao Wang, Chenglong Wang, and Di Wang. Revisiting differentially private hyper-
parameter tuning, 2024. URL https://arxiv.org/abs/2402.13087 .
Hanshen Xiao and Srinivas Devadas. Pac privacy: Automatic privacy measurement and control of data
processing. In Annual International Cryptology Conference , pp. 611–644. Springer, 2023.
Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative adversarial
network. arXiv preprint arXiv:1802.06739 , 2018.
Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan
Kulkarni, YinTatLee, AndreManoel, LukasWutschitz, etal. Differentiallyprivatefine-tuningoflanguage
models. In International Conference on Learning Representations , 2022.
15Under review as submission to TMLR
A Proof of Theorem 3.1
Proof.To start, we need the following lemmas.
Lemma A.1. Given anyf-divergenceDf(·∥·), and three Bernoulli distributions 1a,1band1cof parameters
a,bandc, respectively, where 0≤a≤b≤c≤1. Then,Df(1a∥1b)≤Df(1a∥1c).
Proof.By the definition, g(x) =Df(1a∥1x) =xf(a
x) + (1−x)f(1−a
1−x)and we want to show g(x)is non-
decreasing for x≥a. With some calculation, g′(x) =/parenleftbig
f(a
x)−a
xf′(a
x)/parenrightbig
−/parenleftbig
f(1−a
1−x)−1−a
1−xf′(1−a
1−x)/parenrightbig
.It is noted
thata
x≤1−a
1−xforx≥a. Thus, to show g′(x)≥0forx≥a, it suffices to show t(y) =f(y)−yf′(y)is
non-increasing with respect to y∈[0,1]. On the other hand, t′(y) =f′(y)−f′(y)−yf′′(y)≤0due to the
convex assumption of f. Therefore, the claim holds.
Lemma A.2 (Data Processing Inequality Sason & Verdú (2016)) .Consider a channel that produces Zgiven
Ybased on the law described as a conditional distribution PZ|Y. IfPZis the distribution of ZwhenYis
generated by PY, and QZis the distribution of ZwhenYis generated by QY, then for any f-divergence Df,
Df(PZ∥QZ)≤Df(PY∥QY).
Now, we return to prove Theorem 3.1. First, we have the observation that for a random variable X′∈X∗
in an arbitrary distribution but independent of X,δρ
o≤PrX′⊥X/parenleftbig
ρ(X′,X) = 1/parenrightbig
,sinceδρ
ois the minimum
failure probability achieved by optimal a prioriestimation. Here, a⊥brepresents that ais independent of
b. Let the indicator be a function that for two random variables aandb,1(a,b) = 1ifρ(a,b) = 1, otherwise
0. Apply Lemma A.1 and Lemma A.2, where we view 1(·,·)as a post-processing on (X,˜X)and(X,X′),
respectively, we have that
Df/parenleftbig
1δ∥1δρ
o/parenrightbig
≤Df/parenleftbig
1(X,˜X)∥1(X,X′)/parenrightbig
≤Df/parenleftbig
PX,˜X∥PX,X′/parenrightbig
=Df/parenleftbig
PX,˜X∥PX⊗PX′/parenrightbig
.
On the other hand, we know X→M (X)→˜Xforms a Markov chain, where the adversary’s estimation
˜Xis dependent on observation M(X). Let the adversary’s strategy be some operator gadvwhere ˜X=
gadv(M(X)). Therefore, we can apply the data processing inequality again, where
Df/parenleftbig
PX,˜X∥PX,X′/parenrightbig
≤Df/parenleftbig
PX,M(X)∥PX,W/parenrightbig
=Df/parenleftbig
PX,M(X)∥PX⊗PW/parenrightbig
.
Here,X′=gadv(W)andWis still independent of X. Since the above inequalities hold for arbitrarily
distributed X′once it is independent of X,Wcould also be an arbitrary random variable on the same
support domain as M(X)and independent of X. Therefore,
Df/parenleftbig
1δ∥1δρ
o/parenrightbig
≤inf
PWDf/parenleftbig
PX,M(X)∥PX⊗PW/parenrightbig
= inf
PWDf/parenleftbig
PM(X)|X∥PW|PX).
Here, we use|PXto denote that it is conditional on Xin a distribution PX. In particular, if we select PWto
be the distribution of M(X), and take Dfto be KL-divergence, we have DKL/parenleftbig
1δ∥1δρ
o/parenrightbig
≤MI(X;M(X)).
B Proof of Theorem 3.2
Proof.ForMI(X;M(X) +B), we have
MI(X;M(X) +B)
=/integraldisplay
DKL(PM(X0)+B∥PB)P(X=X0)dX0−DKL(PM(X)+B∥PB)
=/integraldisplay
DKL(PM(X0)+B∥PB)P(X=X0)dX0−/parenleftbig
DKL(PM(X)+B∥PGau(M(X)+B)) +DKL(PGau(M(X)+B)∥PB)/parenrightbig
≤/integraldisplay
DKL(PM(X0)+B∥PB)P(X=X0)dX0−DKL(PGau(M(X)+B)∥PB).
(18)
16Under review as submission to TMLR
Giventhedefinitionofmutualinformation, wefirstapplytheresultsofGaussianapproximationPinskeretal.
(1995), where Gau(A)represents a (multivariate) Gaussian variable with the same mean and (co)variance
as those of A. Then, we drop a negative term to obtain the final inequality of (18). Next, focusing on the
integral (first) term in (18),
DKL(PM(X0)+B∥PB) =1
2·(M(X0))TΣ−1
B(M(X0)),
and therefore
/integraldisplay
DKL(PM(X0)+B∥PB)P(X=X0)dX0=1
2·EX/bracketleftbig
(M(X))TΣ−1
B(M(X))/bracketrightbig
.
As for the second term in the last equation of (18), we have the covariance of M(X)equals ΣM(X)=
EX/bracketleftbig
(M(X)−E[M(X)])(M(X)−E[M(X)])T/bracketrightbig
, while the mean µM(X)=E[M(X)]. The KL divergence
between two multivariate Gaussians has a closed form, where DKL(PGau(M(X)+B)∥PB)equals
DKL(PGau(M(X)+B)∥PB) =1
2·/parenleftbig
Trace (ΣM(X)·Σ−1
B)
+EX[M(X)]TΣ−1
BEX[M(X)]−logdet(Id+ ΣM(X)Σ−1
B)/parenrightbig
.(19)
On the other hand, note that
EX/bracketleftbig
(M(X))TΣ−1
B(M(X))/bracketrightbig
−EX/bracketleftbig
M(X)/bracketrightbigTΣ−1
BEX/bracketleftbig
M(X)/bracketrightbig
−Trace (ΣM(X)·Σ−1
B)
=Trace/parenleftbig
E[M(X)−E[M(X)]]·Σ−1
B·E[M(X)−E[M(X)]]T/parenrightbig
−Trace/parenleftbig
ΣM(X)Σ−1
B/parenrightbig
=Trace/parenleftbig
E[M(X)−E[M(X)]]·E[M(X)−E[M(X)]]T·Σ−1
B−ΣM(X)Σ−1
B/parenrightbig
=Trace/parenleftbig
ΣM(X)·Σ−1
B−ΣM(X)Σ−1
B/parenrightbig
= 0.(20)
In (20), we use the following facts that for two arbitrary vectors v1,v2∈Rd,(v1)Tv2=Trace (v1·(v2)T), and
for two arbitrary matrices A1,A2∈Rd×d, Trace (A1A2) =Trace (A2A1). Therefore, putting it all together,
we have a simplified form of the right hand of (18), where
MI(X;M(X) +B)≤logdet(Id+ ΣM(X)·Σ−1
B)
2. (21)
C Detailed Data in Figure 3
Table 2: Perceptual and privacy score comparisons on CelebA with image resolution 64×64. In our model,
we train with data on a specific label respectively, and Model-Gender means our model is trained with
CelebA-Gender dataset. ξindicates a hyperparameter derived from the Randomized Response (RR).
DPGEN P3DM-Gender P3DM-Smile
ξ FID↓ Privacy Score↑ FID↓ Privacy Score↑ FID↓ Privacy Score↑
1 175 ±2.5 0.6±0.05 170±1.85 0.63±0.08
5 155±1.5 0.46±0.076 125.8±3.6 0.55±0.082 107.85 ±4.7 0.6±0.075
10 39.16±0.68 0.4±0.03 44.82±4.48 0.5±0.05 37.96±1.85 0.56±0.045
15 38.5±0.73 0.4±0.06 40±0.5 0.45±0.05 37.9±0.6 0.5±0.025
∞ 34.4±1.8 0.376±0.03 34.64±2.18 0.4±0.04 36.03±1.32
17Under review as submission to TMLR
Table 3: Perceptual and privacy score comparisons on CelebA with image resolution 64×64. In our model,
we train with data on a specific label respectively, and Model-Gender means our model is trained with
CelebA-Gender dataset. εrepresents the DP parameter of baselines.
DPDM DP-MEPF
ε FID↓ Privacy Score↑ FID↓ Privacy Score↑
1 67.5±1.45 0.48±0.02
5 170±2 0.57±0.06 61.2±1.8 0.45±0.03
10 117±1.5 0.47±0.04 57.5±1.2 0.4±0.04
15 113±1.2 0.35±0.05 55.8±1.4 0.38±0.02
∞ 110±1.2 0.3±0.06 52±2.3 0.34±0.025
18