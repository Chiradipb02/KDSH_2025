Under review as submission to TMLR
BISLERi: Ask Your Neural Network Not To Forget In
Streaming Learning Scenarios
Anonymous authors
Paper under double-blind review
Abstract
This paper introduces a new method for class-incremental streaming learning . In streaming 1
learning, a learner encounters one single training example at a time and is constrained to: 2
(i)utilize each sample only once, i.e., single-pass learning, (ii)adapt the parameters im- 3
mediately, (iii)eﬀectively predict on any new example at any time step without involving 4
any additional computation, i.e., it can be evaluated anytime, and (iv)minimize the storage 5
cost. Moreover, in streaming setting, the input data-stream cannot be assumed i.i.d, that is, 6
there can be a temporal coherence in the input data-stream. Finally, the class-incremental 7
learning implies that the learner does not require any task-id for the inference. A wide 8
variety of the existing lifelong learning approaches are either designed to utilize more than 9
one example once/multiple times or not optimized for the fast update or anytime infer- 10
ence. The premise of their designs, as well as other aspects (e.g., memory buﬀer/replay 11
size, the requirement of ﬁne-tuning), render some of the existing methods sub-optimal, if 12
not ill-suited, for streaming learning setup. We propose a streaming Bayesian framework 13
that enables fast parameter update of the network, given a single example, and allows it 14
to be evaluated anytime. In addition, we also apply an implicit regularizer in the form of 15
snap-shot self-distillation to eﬀectively minimize the information loss further. The proposed 16
method utilizes a tiny-episodic memory buﬀer and replays to conform with the streaming 17
learning constraints. We also propose an eﬃcient online memory replay and buﬀer replace- 18
ment policies that signiﬁcantly boost the model’s performance. Extensive experiments and 19
ablations on multiple datasets in diﬀerent scenarios demonstrate the superior performance 20
of our method over several strong baselines. 21
1 Introduction 22
Inthispaper,weaimtoachievecontinuallearningbysolvinganextremelyrestrictiveformoflifelonglearning, 23
i.e.,‘streaming learning’ (Gama et al., 2013) in the deep neural networks (Hayes et al., 2019a;b). Most of the 24
existing popular and successful methods in continual learning operate in incremental batch learning (IBL) 25
scenarios (Rusu et al., 2016; Shin et al., 2017; Kirkpatrick et al., 2017; Wu et al., 2018; Aljundi et al., 2018a; 26
Nguyen et al., 2017; Mallya & Lazebnik, 2018). In IBL, it is assumed that the current task data is available 27
in batches during training, and the learner visits them sequentially multiple times. However, these methods 28
are ill-suited in a rapidly changing environment, where the learner needs to quickly adapt to an important 29
(since we cannot wait to adapt) and rare (as we may not collect batch) streaming data with no catastrophic 30
forgetting (McCloskey & Cohen, 1989; French, 1999). For example, consider a real-world scenario, where an 31
autonomous agent, such as an autonomous car, might meet with a rare incident/accident, then it could be 32
lifesaving if it can be trained incrementally with that single example without any forgetting. It would be 33
impractical, if not infeasible, to wait and aggregate a batch of samples to train the autonomous agent, as in 34
this case, we may not collect a batch due to its rare nature. 35
The ability to continually learn eﬀectively from streaming data with no catastrophic forgetting (McCloskey & 36
Cohen, 1989; French, 1999) is a challenging problem and this has not received widespread attention (Hayes 37
et al., 2019a;b). However, its utility is apparent, as it enables the practical deployment of autonomous 38
AI agents. Hayes et al. (2019b) argued that streaming learning is more closer to biological learning than 39
1Under review as submission to TMLR
Table 1: Categorization of the baseline approaches depending on the underlying simplifying assumptions
they impose. In ζ(n),nrepresents the number of gradient steps required to train the corresponding model.
ζ(n)/greatermuchζ(k)≥ζ(2)>ζ(1). ‘-’ indicates, we are unable to ﬁnd the exact value.
Note: Although GEM (Lopez-Paz & Ranzato, 2017) performs a single gradient update for online learning, it
does solve a Quadratic Program with inequality constraints, which is a computationally expensive operation;
therefore, we consider it costs higher than a single gradient update. Furthermore, MIR (Aljundi et al., 2019)
requires two gradient update, where it ﬁrst performs a virtual gradient update to select maximally interfered
samples from memory, and then performs another gradient update to ﬁnally update the network parameters.
We provide detailed discussion on each column in the appendix.
‘Class-Incremental Streaming Learning’ (CISL) Crucial Properties
Methods TypeBayesian
FrameworkBatch-Size
(Nt)Fine-tunesSingle Pass
Learning CILSubset
Buﬀer ReplayTraining
TimeInference
TimeViolates Any
CISLConstraintMemory
CapacityRegularization
BasedMemory
Based
EWC
(Kirkpatrick et al., 2017) Batch 7Nt/greatermuch1 7 7 7 n/a ζ(n)ζ(1) 3 n/a 3 7
MAS
(Aljundi et al., 2018a) Batch 7Nt/greatermuch1 7 7 7 n/a ζ(n)ζ(1) 3 n/a 3 7
SI
(Zenke et al., 2017) Batch 7Nt/greatermuch1 7 7 7 n/a ζ(n)ζ(1) 3 n/a 3 7
VCL
(Nguyen et al., 2017) Batch 3Nt/greatermuch1 7 7 7 n/a ζ(n)ζ(1) 3 n/a 3 7
Coreset VCL
(Nguyen et al., 2017) Batch 3Nt/greatermuch1 3 7 7 7 ζ(n)ζ(n) 3 - 3 3
Coreset Only
(Farquhar & Gal, 2018) Batch 3Nt/greatermuch1 3 7 7 7 ζ(n)ζ(n) 3 - 7 3
GDumb
(Prabhu et al., 2020) Online 7Nt/greatermuch1 3 7 3 7 ζ(1) ζ(n) 3 - 7 3
TinyER
(Chaudhry et al., 2019) Online 7Nt/greatermuch1 7 3 3 3 ζ(1) ζ(1) 7 ≤5% 7 3
DER
(Buzzega et al., 2020) Online 7Nt/greatermuch1 7 3 3 3 ζ(1) ζ(1) 7 ≤5% 3 7
DER++
(Buzzega et al., 2020) Online 7Nt/greatermuch1 7 3 3 3 ζ(1) ζ(1) 7 ≤5% 3 3
AGEM
(Chaudhry et al., 2018b) Online 7Nt/greatermuch1 7 3 7 3 ζ(1) ζ(1) 3 - 3 3
GEM
(Lopez-Paz & Ranzato, 2017) Online 7Nt/greatermuch1 7 3 7 7 ζ(k)ζ(1) 3 - 3 3
MIR
(Aljundi et al., 2019) Online 7Nt/greatermuch1 7 7 3 3 ζ(2) ζ(1) 3 - 7 3
ExStream
(Hayes et al., 2019a) Streaming 7Nt= 1 7 3 3 7 ζ(1) ζ(1) 3 ≤5% 7 3
REMIND
(Hayes et al., 2019b) Streaming 7Nt= 1 7 3 3 3 ζ(1) ζ(1) 7 /greatermuch10% 7 3
Ours Streaming 3 Nt= 1 7 3 3 3 ζ(1) ζ(1) 7 ≤5% 3 3
CISL Constraints Nt= 1 7 3 3 3 ζ(1) ζ(1)
other existing incremental learning scenarios. In this paper, we are interested in class-incremental learning 40
in astreaming scenario (CISL), where a learner requires to continually learn in a ‘single-pass’ with no 41
forgetting, given a single training example at every time step. That is, the learner is allowed to utilize the 42
single new example only once (Hayes et al., 2019b;a). Class incremental learning (CIL) setting (Rebuﬃ 43
et al., 2017; Chaudhry et al., 2018a; Belouadah et al., 2020) implies that the label space includes all the 44
classes observed so far and no task id is required during inference, as opposed to task incremental learning 45
methods like VCL/Coreset VCL (Nguyen et al., 2017). In addition, the model being learned should be 46
able to predict eﬃciently at any time step (Gama et al., 2013) without any additional computation to 47
improve the performance. It implies that the ﬁne-tuning as needed in GDumb (Prabhu et al., 2020), Coreset 48
VCL (Nguyen et al., 2017) is forbidden. Finally, for practical applicability, the learning strategy is expected 49
to update the parameters quickly and leverage only a small memory buﬀer. 50
Training a deep neural network in streaming setting continuously with no forgetting (McCloskey & Cohen, 51
1989; French, 1999) is non-trivial due to the aforementioned requirements. Furthermore, adapting an exist- 52
ing online learning method is not straight forward due to various limitations poses by these methods which 53
violates one or multiple conditions of the restrictive class-incremental streaming learning setup. For instance, 54
AGEM (Chaudhry et al., 2018b) requires task id for prediction (i.e., task incremental learning setup). We 55
provide empirical evidence that, when extended to CISL, the gradient computed in AGEM from a single 56
example leads to suboptimal results (perhaps not surprisingly). MIR (Aljundi et al., 2019) infringes the 57
single-pass learning constraint by employing a two-step learning process. It ﬁrst performs a virtual gradient 58
update to select the samples for memory replay and then performs another gradient update to ﬁnally update 59
the model parameters. GDumb (Prabhu et al., 2020) proposes a greedy strategy to maintain past examples 60
in a buﬀer and retrains on all examples during inference. Although it has been shown to work well on various 61
2Under review as submission to TMLR
online learning setttings, GDumb does not update the model until inference (i.e., it does not accumulate any 62
knowledgeinthenetwork)andrequiresﬁne-tuningbeforeeveryinference, whichviolates (i)single-passlearn- 63
ing constraint and (ii)any time inference without further training constraint. Finally, the recently proposed 64
streaming learning approach, REMIND (Hayes et al., 2019b) requires a large amount of cached data for good 65
performance, which restricts its applicability. Table 1 illustrates which critical components/requirements of 66
streaming learning are satisﬁed/infringed by the existing lifelong learning approaches. 67
Contributions. In this paper, we propose a novel method, ‘Bayesian Class Incremental Streaming 68
Learning’(BISLERi) to facilitate lifelong learning in streaming setting by addressing the aforementioned 69
limitations. In particular, we enable streaming learning by leveraging a tiny episodic memory-buﬀer replay 70
with dual regularization. It regularises the model from two sources. One focuses on regularizing the model 71
parameters explicitly in a streaming Bayesian framework (Neal, 2012; Broderick et al., 2013), while the other 72
regularizes, in the form of self-distillation (Mobahi et al., 2020), by enforcing the updated model to produce 73
similar outputs for previous models on the past observed data. Our approach jointly trains the buﬀer replay 74
and current task sample by incorporating the likelihood of the replay and current sample. As a result, unlike 75
VCL (Nguyen et al., 2017), GDumb (Prabhu et al., 2020), we do not need explicit ﬁne-tuning to improve the 76
model’s performance, and the model can be evaluated on the ﬂy. We also propose novel online loss-aware 77
buﬀer replacement and various replay strategies that helps to fraction buﬀer replay or replacement in an 78
optimal way, which signiﬁcantly improves the training and boost the model’s performance. 79
Our experimental results on ﬁve benchmark datasets demonstrate the eﬀectiveness of the proposed method 80
to circumvent catastrophic forgetting in highly challenging streaming settings. The proposed approach 81
signiﬁcantly improves the recent state-of-the-art, and the extensive ablations validate the importance of 82
the proposed components. In Figure 2, we compare the proposed method with the recent strong baselines. 83
Even though designed for streaming learning, it outperforms the baselines by a signiﬁcant margin in all three 84
diﬀerent lifelong learning settings. It implies that a method designed to work in the most restrictive setting 85
can be thought of as a robust and ﬂexible method for the various lifelong learning settings with the widest 86
possible applicability. 87
Our main contributions can be summarised as follows: 88
(i)we propose a novel replay-based dual-regularization framework (BISLERi), comprising a streaming 89
Bayesian framework (Neal, 2012; Broderick et al., 2013) as well as a functional regularizer, to overcome 90
catastrophic forgetting in challenging class-incremental streaming learning scenario, (ii)we propose novel 91
onlineloss-aware buﬀer replacement policies and include various sampling strategies which signiﬁcantly 92
boosts the model’s performance, (iii)we empirically show that selecting a tiny subset of samples from mem- 93
ory and computing the joint likelihood with the current sample is highly eﬃcient in terms of the model’s 94
accuracy, and enough to avoid explicit ﬁnetuning , and (iv)we experimentally show that our method signiﬁ- 95
cantly outperforms the recent strong baselines. 96
2 Problem Formulation 97
In this paper, we study class incremental streaming learning (CISL) in the deep neural networks. 98
Streaming Learning (SL). Streaming learning is the extreme case of online learning, where data arrives 99
sequentially one datum at a time, and the model needs to adapt online in a single pass. That is, it is not 100
allowed to visit any part of the data multiple times, and it can be evaluated at any point of time without 101
waiting for the termination of the training. While in this setting, a learner can use a tiny replay buﬀer, it 102
is strictly forbidden to use any additional computation, such as ﬁne-tuning, to improve the performance at 103
any time. Therefore, approaches like GDumb (Prabhu et al., 2020), Coreset VCL (Nguyen et al., 2017), are 104
not allowed. Furthermore, it cannot be assumed that the input data-stream is independent and identically 105
distributed(i.i.d); thedata-streamcanbetemporallycontiguousand/ortherecanbeaclass-basedcorrelation 106
in the data, for e.g.: (i)class-instance and (ii)instance ordering (for more details, refer to Section 5.1). 107
3Under review as submission to TMLR
Let us consider an example dataset Ddivided into Ttask sequences, i.e., D=/uniontextT
t=1Dt. Each taskDtconsists 108
ofNtlabeled data-points, i.e., Dt=/braceleftBig
d(j)
t/bracerightBigNt
j=1={(xj,yj)}Nt
j=1, and corresponds to diﬀerent labeling task. 109
xjrepresents the input image & yjis the corresponding class-label, and Ntis a variable across tasks. 110
In streaming learning, each task Dtconsist only a single data-point, i.e., |Dt|=Nt= 1,∀t, and the model 111
is required to adapt to this new example with no forgetting in a single pass, i.e., it cannot observe the 112
new example multiple times. This setup is diﬀerent from the widely popular incremental batch learning 113
(IBL) (Kirkpatrick et al., 2017; Nguyen et al., 2017; Aljundi et al., 2017; 2018a;b). In IBL, it is assumesd 114
that the model have access to the whole dataset, i.e., D=/uniontextT
t=1Dt; the data does not come in online manner. 115
It is further assumed that: |Dt|=Nt/greatermuch1,∀t. The model visits each task data Dtsequentially multiple 116
times to mitigate catastrophic forgetting. Online learning methods (Prabhu et al., 2020), on the other hand, 117
while assume that each task Dtcomes sequentially one at a time, assumes that the each data subset size 118
|Dt|=Nt/greatermuch1. Furthermore, in online learning setup, it is allowed to ﬁne-tune the network parameters any 119
time with the samples stored in the memory. Speciﬁcally, it can ﬁne-tune the network as many times as it 120
wants, only constraint is that the samples are needed to be stored in the memory. 121
Class Incremental Learning (CIL). Class-incremental learning (CIL) (Rebuﬃ et al., 2017; Chaudhry 122
et al., 2018a; Belouadah et al., 2020; Rios & Itti, 2018) is a challenging variant of continual learning. During 123
inference, it considers the label space over all the classes that have been observed so far. This is in contrast 124
to the task-incremental learning methods, such as VCL (Nguyen et al., 2017), which requires the task-id to 125
be speciﬁed while doing inference. 126
In Table 1, we categorize various recently proposed strong baseline approaches depending on the underly- 127
ing simplifying assumptions that they impose. It can be observed that TinyER (Chaudhry et al., 2019), 128
REMIND (Hayes et al., 2019b) and BISLERi (Ours) do not violate any constraints of class-incremental 129
streaming learning (CISL) setting. MIR (Aljundi et al., 2019) infringes the single-pass learning constraint 130
by employing a two-step learning process. First, it performs a virtual gradient update to select the sam- 131
ples for memory replay and then performs another gradient update to ﬁnally update the model parameters. 132
While MIR (Aljundi et al., 2019) uses two-pass learning, a modiﬁed version of MIR can be considered as 133
a single-pass method. That is, ﬁrst compute the gradient gnewonDnewand check the interference score 134
after the gradient update (virtual gradient update) to get the dataset Dinterferred . Then, compute the 135
gradient of Dnew∪Dinterferred . Since, we already have the gradient w.r.t gnew, all we need is to compute 136
the gradient over Dinterferred to get the gradient for the ﬁnal update. However, we have to remember that 137
in streaming learning (SL), |Dnew|= 1, i.e., in each incremental step, only a single training example arrives. 138
Therefore, the gradient gnewwill be computed w.r.t a single data-point Dnew, where|Dnew|= 1. SinceSGD 139
with just a single datapoint is extremely noisy and hard to optimize , the gradient update (virtual gradient 140
update) with gnewwill result in poor generalization. Henceforth, the selected maximally interfered dataset 141
(Dinterferred ) will be sub-optimal, such that, it will suﬀer from catastrophic forgetting (McCloskey & Cohen, 142
1989; French, 1999; Goodfellow et al., 2015) in streaming lifelong learning setup. GDumb (Prabhu et al., 143
2020) violates streaming learning constraints by employing ﬁne-tuning the network parameters before each 144
inference. ExStream (Hayes et al., 2019a) despite being a streaming learning method, it replays all buﬀer 145
samples instead of replaying only a few samples while training on the new example. Therefore, we consider 146
that it violates the subset buﬀer replay constraint, which ultimately violates the CISL constraint. It can 147
be observed, among the online learning methods, only TinyER (Chaudhry et al., 2019) can be adapted to 148
the CISL setting without violating any crucial properties that are necessary for the CISL setting. Figure 2 149
compares the performance of the various baselines in diﬀerent lifelong learning settings. We can observe that 150
BISLERi (Ours) performs best compared to the baselines consistently throughout the diﬀerent settings. 151
In Figure 4, we demonstrate the impact of the presence of temporal coherance in the input data-stream. It 152
can be observed that (i)class-instance and (ii)instance ordering are more challenging compared to when 153
the data-stream is organized randomly. It further can observed that while the strong baselines continue to 154
suﬀer from severe forgetting in the presence of temporal coherance, BISLERi (Ours) suﬀer from minimal 155
amount of forgetting. 156
4Under review as submission to TMLR
Figure 1: Schematic representation of the proposed model. θGrepresents the parameters of the feature
extractorG(·), whereasθFrepresents the parameters of the plastic network / Bayesian neural network F(·).
3 Proposed Streaming Learning Framework 157
In the following, we introduce BISLERi, which trains a convolutional neural network (CNN) in streaming 158
learning setup. Formally, it is assumed that we have access to a limited number of labeled training examples, 159
which we use to train the model in a typical oﬄine manner. We term this step as the base initialization step 160
witht= 0. Then in each incremental step, i.e., ∀t >0, the model observes a single new training example , 161
i.e.,Dt={dt}={(xt,yt)}, and adapts to it by doing a single step posterior computation. 162
3.1 Streaming Learning With A Single Example 163
Formally, we separate the CNN into two neural networks (Figure 1): (i)non-plastic feature extractor G(·) 164
consisting the ﬁrst few layers of the CNN, and (ii)plastic neural network F(·)consisting the ﬁnal layers 165
of the CNN. For a given input image x, the predicted class label is computed as: y=F(G(x)). We 166
initialize the parameters of the feature extractor G(·)and keep it frozen throughout streaming learning. 167
We use a Bayesian-neural-network (BNN) (Neal, 2012; Jospin et al., 2020) as the plastic network F(·), and 168
optimize its parameters with sequentially coming data in streaming setting. We discuss how the parameters 169
of the feature extractor G(·)is initialized in Section 3.5. In the below, we describe how the plastic network 170
F(·)is trained with a single step posterior computation in each incremental step with a single data point 171
Dt={dt}={(xt,yt)}in streaming learning setup with no catastrophic forgetting (McCloskey & Cohen, 172
1989; French, 1999). 173
Variational Posterior Estimation. Streaming learning naturally emerges from the Bayes’ rule (Broderick
et al., 2013); given the posterior p(θ|D1:t−1), whenever a new data: Dt={dt}={(xt,yt)}arrives, the new
posteriorp(θ|D1:t)can be computed by combining the previous posterior and the new data likelihood,
i.e.,p(θ|D1:t)∝p(Dt|θ)p(θ|D1:t−1), where the old posterior is treated as the prior. However, for any
complex model, the exact Bayesian inference is not tractable, and an approximation is needed. A Bayesian
neural network (Neal, 2012) commonly approximates the posterior with a variational posterior q(θ)by
minimizing the following KL divergence (Eq. 1), or equivalently by maximizating the evidence lower bound
(ELBO) (Blundell et al., 2015) (Eq. 2):
Lt(θ) = arg min
q∈QKL/parenleftbigg
qt(θ)||1
Ztqt−1(θ)p(Dt|θ)/parenrightbigg
(1)
warg max
q∈QEθ∼qt(θ)[logp(Dt|θ)]−KL(qt(θ)||qt−1(θ))(2)
It is worth mentioning that the KL-divergence in Eq. 2 works as a inherent regularizer, which can prevent 174
forgetting in the network by keeping the prior and the posterior distribution close to each other. However, 175
there are few concerns. Firstly, optimizing the ELBO (in Eq. 2) to approximate the posterior p(θ|D1:t) 176
with only a single training example, i.e., ∀t > 0,Dt={dt}={(xt,yt)}, in each incremental step can 177
fail in streaming setting (Ghosh et al., 2018). Furthermore, likelihood estimation from a single example: 178
5Under review as submission to TMLR
Dt={dt}={(xt,yt)}can bias the model towards the new data disproportionately and can maximize the 179
confusion during inference in class incremental learning setup (Chaudhry et al., 2018a). 180
These concerns cannot be prevented only with the parameters regularization. We, therefore, propose to 181
estimate the likelihood from both the new example: Dtand the previously observed examples in order to 182
compute the new posterior: p(θ|D1:t). For this purpose, a small fraction (≤5%)of past observed examples 183
(for buﬀer capacity refer Table 2) are stored as representatives in a ﬁxed-sized tiny episodic memory buﬀer 184
M. Instead of storing raw pixels (images) x, we store the embedding z=G(x), wherez∈Rd. This allows 185
us to avoid the cost of recomputing the image-embeddings and expedites the learning even further. It also 186
saves a signiﬁcant amount of space and allows us to keep more examples in a small budget. 187
During training on the new example: Dt={dt}={(xt,yt)}, we select a subset of samples: DM,tfrom
the memoryM, where: (i)DM,t⊂M,(ii)|DM,t|=N/prime
1/lessmuch|M|,instead of replaying the whole buﬀer ,
and compute the likelihood jointly with the new example Dtto estimate the new posterior. We, therefore,
compute the new posterior as follows: p(θ|D1:t)∝p(Dt|θ)p(DM,t|θ)p(θ|D1:t−1). Since the exact Bayesian
inference is intractable, we approximate it with a variational posterior qt(θ)as follows:
L1
t(θ) = arg min
q/epsilon1QKL/parenleftbigg
qt(θ)||1
Ztqt−1(θ)p(Dt|θ)p(DM,t|θ)/parenrightbigg
(3)
Note that Eq. 3 is signiﬁcantly diﬀerent from VCL (Nguyen et al., 2017), where they assume task-incremental 188
learning settingwith separate head networks , andincorporatesthecoresetsamplesonlyforexplicitﬁnetuning 189
before inference. For more details on VCL/Coreset VCL (Nguyen et al., 2017) refer to the appendix. 190
The above minimization (in Eq. 3) can be equivalently written as the maximization of the evidence lower
bound (ELBO) as follow:
L1
t(θ) =Eθ∼qt(θ)[logp(yt|θ,G(xt))] +N/prime
1/summationdisplay
n=1Eθ∼qt(θ)/bracketleftBig
logp(y(n)
M,t|θ,z(n)
M,t)/bracketrightBig
−λ1·KL(qt(θ)||qt−1(θ))(4)
where: (i)Dt={dt}={(xt,yt)},(ii)DM,t={d(n)
M,t}N/prime
1
n=1={(z(n)
M,t,y(n)
M,t)}N/prime
1
n=1,(iii)DM,t⊂M, 191
(iv)|DM,t|=N/prime
1/lessmuch|M|, and (v)λ1is a hyper-parameter. 192
193
Snap-Shot Self Distillation. It is worth noting that the KL divergence minimization (in Eq. 4) between 194
the prior and the posterior distribution works as a inherent regularizer, which tries to keep the changes in 195
the network parameters minimal during streaming learning. However, its eﬀect may weaken over the time 196
due to the presence of distribution shift, temporal coherance in the input data-stream. Furthermore, the 197
initialization of the prior with the old posterior at each incremental step can introduce information loss in the 198
network for a longer sequence of streaming learning. On these grounds, we propose a functional regularizer, 199
which encourages the network to mimic the output responses as produced in the past for the previously 200
observed samples (for signiﬁcance of self-distillation, see Section 6). Speciﬁcally, we propose to minimize the 201
KL divergence between the class-probability scores obtained in the past and current time t: 202
t−1/summationdisplay
j=1Eθ∼qt(θ)[KL(softmax (hj)||Fθ(G(xj)))] (5)
where:xjandhjrepresents input examples and the logits obtained while training on Djrespectively. 203
Theaboveobjective(inEq.5)resemblestheteacher-studenttrainingapproach(Hintonetal.,2015); however, 204
since in this case only a single network is used as both teacher & student network, it is called self knowledge 205
distillation or self distillation (Hinton et al., 2015; Mobahi et al., 2020). It (the objective in Eq. 5), however, 206
requires the availability of the embeddings and the corresponding logits for all the past observed data till 207
time instance (t−1). Since storing all the past examples is not feasible, we only store the logits for all samples 208
in memoryM. During training, we uniformly select N/prime
2samples along with their logits and optimize the 209
6Under review as submission to TMLR
following objective: 210
L2
t(θ) =N/prime
2/summationdisplay
j=1Eθ∼qt(θ)[KL(softmax (hj)||Fθ(zj))] (6)
where: (i)zjandhjrepresents the feature-map and the corresponding logit, and (ii)N/prime
2/lessmuch|M|. 211
Under the mild assumptions of knowledge distillation (Hinton et al., 2015), the optimization in Eq. 6 is 212
equivalent to minimization of the Euclidean distance between the corresponding logits. In this work, we, 213
therefore, minimize the following objective instead of the objective above (Eq. 6): 214
L2
t(θ) =λ2·N/prime
2/summationdisplay
j=1Eθ∼qt(θ)/bracketleftBig
||hj−fθ(zj)||2
2/bracketrightBig
(7)
where: (i)f(·)represents the plastic network F(·)without the softmax activation, and (ii)λ2is a hyper- 215
parameter. 216
It is, however, worth mentioning that the stored logits used in Eq.(7) are updated in an online manner. 217
That is, whenever a sample is selected for memory replay, we replace the corresponding old logits with 218
the newly predicted logits. Therefore, it is called snap-shot self-distillation (Yang et al., 2019), where the 219
model is constrained to match the logits obtained during earlier memory replay, i.e., the last snap-shot. It 220
essentially prevents the model from being constrained to match the sub-optimal initial logits and minimizes 221
the information loss in the network. 222
Training. Training the plastic network (BNN) F(·)requires speciﬁcation of q(θ)and, in this work, we 223
modelθby stacking up the parameters (weights & biases) of the network F(·). We use a Gaussian mean- 224
ﬁeld posterior qt(θ)for the network parameters, and choose the prior distribution, i.e., q0(θ) =p(θ), as 225
multivariate Gaussian distribution. We train the network F(·)by maximizing the ELBO in Eq. 4 and 226
minimizing the Euclidean distance in Eq. 7. For memory replay in Eq. 4, we select past informative samples 227
using the strategies mentioned in Section 3.2, and we use uniform sampling to select samples from memory 228
to be used in Eq. 7. Figure 1 shows the schematic diagram of the proposed model as well as the learning 229
process. 230
3.2 Informative Past Sample Selection For Replay 231
We consider the following strategies for selecting past informative samples for memory replay: 232
Uniform Sampling (Uni). In this approach, samples are selected uniformly random from memory. If we 233
have the buﬀer of size Kthen each samples are selected with probability 1/K. 234
Uncertainty-Aware Positive-Negative Sampling (UAPN). UAPN selects N/prime
1/2samples with the 235
highest uncertainty scores (negative samples) and N/prime
1/2samples with the lowest uncertainty scores (positive 236
samples). Empirically, we observe that this sample selection strategy results in the best performance. We 237
measure the predictive uncertainty (Chai, 2018) for an input zwith BNNF(·)as follows: 238
Φ(z)≈−/summationdisplay
c/parenleftbigg/summationtext
kp(ˆy=c|z,θk)
k/parenrightbigg
log/parenleftbigg/summationtext
kp(ˆy=c|z,θk)
k/parenrightbigg
(8)
where:p(ˆy=c|z,θk)is the predicted softmax output for class cusing thek-th sample of weights θkfrom 239
q(θ). We usek= 5samples for uncertainty estimation. 240
Loss-Aware Positive-Negative Sampling (LAPN). LAPN selects N/prime
1/2samples with the highest loss- 241
values (negative-samples), and N/prime
1/2samples with the lowest loss-values (positive-samples). Empirically we 242
observe that the combination of most and least certain samples shows a signiﬁcant performance boost since 243
one ensures quality while the other ensures diversity for the memory replay. 244
7Under review as submission to TMLR
Table 2: Memory buﬀer capacity used for various datasets.
Dataset CIFAR10 CIFAR100 ImageNet100 iCubWorld 1.0 CORe50
Buﬀer Capacity 1000 1000 1000 180 1000
Training-Set Size 50000 50000 127778 6002 119894
3.3 Memory Buﬀer Replacement Policy 245
In a practical lifelong learning scenario (Aljundi et al., 2018b;a), data can come indeﬁnitely throughout the 246
time. It implesthat theepisodicreplaybuﬀerwill requireto have aninﬁnite capacityto storeall theobserved 247
examples. Otherwise, its capacity will be quickly exhausted and the new instances cannot be accommodated 248
in the replay buﬀer; it will then require a buﬀer replacement policy, which will replace a stored example in 249
the memory to accommodate a new one whenever the buﬀer is full. 250
In this work, we use a ‘ﬁxed-sized’ tiny episodic replay buﬀer to store a fraction (≤5%)of all the previously 251
observed data. In the below, we discuss two buﬀer replacement policies, which replaces a previously stored 252
example if the buﬀer is full. Otherwise, the new instance is simply stored. For episodic memory capacity 253
across various dataset, refer to Table 2. 254
Loss-Aware Weighted Class Balancing Replacement (LAWCBR). In this approach, whenever a 255
new sample comes and the buﬀer is full, we remove a sample from the class with maximum number of 256
samples present in the buﬀer, i.e., yr= arg max ClassCount (M). However, instead of removing an example 257
uniformly, we weigh each sample of the majority class inversely w.r.t their loss, i.e., wyr
i∝1
lyr
iand use these 258
weights as the replacement probability; the lesser the loss, the more likely to be removed. 259
Loss-Aware Weighted Random Replacement With A Reservoir (LAWRRR). In this approach, 260
we propose a novel variant of reservoir sampling (Vitter, 1985) to replace an existing sample with the 261
new sample when the buﬀer is full. We weigh each stored sample inversely w.r.t the loss, i.e., wi∝1
li, 262
and proportionally to the total number of examples of that class in which the sample belongs present in 263
the buﬀer, i.e., wi∝ClassCount (M,yi). Whenever a new example satisﬁes the replacement condition of 264
reservoir sampling, we combine these two scores and use that as the replacement probability; the higher the 265
weight, the more likely to be replaced. 266
3.4 Eﬃcient Buﬀer Update 267
Loss-aware and uncertainty-aware sampling strategies require computing these quantities at every time step 268
during streaming learning with all the samples stored in memory. However, this becomes computationally 269
expensive with the larger replay buﬀer size. To overcome such limitation, we store the corresponding loss- 270
values and uncertainty-scores along with the stored examples in the replay buﬀer. Since these quantities 271
are scalar values, the additional storage requirement is negligible but saves the time to compute the loss 272
and uncertainty in each incremental step. Every time a sample is selected for memory replay, its loss and 273
uncertainty are replaced by the new values. We, in addition, replace the stored logits with the new logits. 274
Empirically, we observe that the model’s accuracy degrades if we don’t update the logits with the new logits. 275
3.5 Feature Extractor 276
In this work, we separate the representation learning, i.e., learning the feature extractor G(·), and the 277
classiﬁer learning, i.e., learning the plastic network F(·). Similar to several existing continual learning ap- 278
proaches(Kemker&Kanan,2017;Hayesetal.,2019a;Xiangetal.,2019;Hayesetal.,2019b), weinitializethe 279
featureextractor G(·)withtheweightslearnedthroughsupervisedvisualrepresentationlearning(Krizhevsky 280
et al., 2012) task, and keep them ﬁxed throughout streaming learning. The motivation to use a pre-trained 281
feature extractor is that the features learned by the ﬁrst few layers of the neural networks are highly trans- 282
ferable and not speciﬁc to any particular task or dataset and can be applied to several diﬀerent task(s) or 283
dataset(s) (Yosinski et al., 2014). Furthermore, it is hard, if not infeasible, to learn generalized visual fea- 284
8Under review as submission to TMLR
tures, that can be used across all the classes with having access to only a single example at every time (Zhu 285
et al., 2021). 286
In our experiments, for all the baselines along with BISLERi, we use Mobilenet-V2 (Sandler et al., 2018) 287
pre-trained on ImageNet-1000 (ILSVRC-2012) (Russakovsky et al., 2015) as the base architecture for the 288
visual feature extractor. It consists of a convolutional base and a classiﬁer network. We remove the classiﬁer 289
network and use the convolutional base as the feature extractor G(·)to obtain embedding, which is fed to the 290
plastic network BNN F(·). For details on the plastic network used for other baselines, refer to Section 5.5. 291
4 Related Work 292
Existing continual learning approaches can be broadly classiﬁed into (Parisi et al., 2019; Delange et al., 293
2021): (i)parameter isolation based approaches, (ii)regularization based approaches, and (iii)rehearsal 294
based approaches. 295
Parameter-isolation-based approaches train diﬀerent subsets of model parameters on sequential tasks. 296
PNN (Rusu et al., 2016), DEN (Yoon et al., 2017) expand the network to accommodate the new task. 297
PathNet (Fernando et al., 2017), PackNet (Mallya & Lazebnik, 2018), Piggyback (Mallya et al., 2018), and 298
HAT (Serra et al., 2018) train diﬀerent subsets of network parameters on each task. 299
Regularization-based approaches use an extra regularization term in the loss function to enable continual 300
learning. LWF (Li & Hoiem, 2017) uses knowledge distillation (Hinton et al., 2015) loss to prevent catas- 301
trophic forgetting. EWC (Kirkpatrick et al., 2017), IMM (Lee et al., 2017), SI (Zenke et al., 2017) and 302
MAS (Aljundi et al., 2018a) regularize by penalizing changes to the important weights of the network. 303
FRCL (Titsias et al., 2019) employs a functional regularizer based on Bayesian inference over the function 304
space rather than the parameters of the deep neural networks to enable CL. It avoids forgetting a previ- 305
ous task by constructing and memorizing an approximate posterior belief over the underlying task-speciﬁc 306
function. UCB (Ebrahimi et al., 2019) based on Bayesian neural networks enables continual learning by 307
controlling the learning rate of each parameter as a function of uncertainty. BGD (Zeno et al., 2018) uses 308
closed-form variational Bayes to mitigate catastrophic forgetting in task agnostic scenarios. That is, (i)in 309
contrast to the methods like EWC (Kirkpatrick et al., 2017), MAS (Aljundi et al., 2018a), VCL (Nguyen 310
et al., 2017) which are based on some core action taken on task-switch, BGD (Zeno et al., 2018) does not re- 311
quire any information on task identity, and (ii)BGD updates the posterior over the weights in closed-form, 312
unlike VCL/Coreset VCL (Nguyen et al., 2017), Coreset Only (Farquhar & Gal, 2018), BISLERi (Ours) 313
which relies on BBB (Blundell et al., 2015) to update the posterior. 314
Rehearsal-based approaches replay a subset of past training data during sequential learning. iCaRL (Rebuﬃ 315
et al., 2017), SER (Isele & Cosgun, 2018), and TinyER (Chaudhry et al., 2019) use memory replay when 316
training on a new task. DER/DER++ (Buzzega et al., 2020) uses knowledge distillation and memory replay 317
while learning a new task. DGR (Shin et al., 2017), MeRGAN (Wu et al., 2018), and CloGAN (Rios & Itti, 318
2018) retain the past task(s) distribution with a generative model and replay the synthetic samples during 319
incremental learning. Our approach also leverages memory replay from a tiny episodic memory; however, 320
we store the feature maps instead of raw inputs. 321
Variational Continual Learning (VCL) (Nguyen et al., 2017) leverages Bayesian inference to mitigate catas- 322
trophic forgetting. However, the approach, when naïvely adapted, performs poorly in the streaming learning 323
setting. Additionally, it also needs task-id during inference. Furthermore, the explicit ﬁnetuning with the 324
buﬀer samples (coreset) before inference violates the single-pass learning constraint. Moreover, it still does 325
not outperform our approach even with the ﬁnetuning. More details are given in the appendix. 326
REMIND (Hayes et al., 2019b) is a recently proposed rehearsal-based lifelong learning approach, which 327
combats catastrophic forgetting in streaming setting . While it follows a setting close to the one proposed, the 328
model stores a large number of past examples compared to the other baselines; for example, iCaRL (Rebuﬃ 329
et al., 2017) stores 10K past examples for the ImageNet experiment, whereas REMIND stores 1M past 330
examples. Further, it actually uses a lossy compression to store past samples, which is merely an engineering 331
technique, not an algorithmic improvement, and can be used by any continual learning approach. For more 332
details, please refer to the appendix. 333
9Under review as submission to TMLR
5 Experiments 334
5.1 Datasets And Data Orderings 335
Datasets. To evaluate the eﬃcacy of the proposed model we perform extensive experiments on ﬁve standard 336
datasets: CIFAR10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), ImageNet100, iCubWorld 337
1.0 (Fanello et al., 2013), and CORe50 (Lomonaco & Maltoni, 2017). CIFAR10 and CIFAR100 are standard 338
classiﬁcation datasets with 10 and 100 classes, respectively. ImageNet100 is a subset of ImageNet-1000 339
(ILSVRC-2012)(Russakovskyetal.,2015)containingrandomlychosen100classes, witheachclasscontaining 340
700-1300 training samples and 50 validation samples. Since, the test data for ImageNet-1000 (ILSVRC- 341
2012) (Russakovsky et al., 2015) is not provided with labels, we use the validation data for evaluating the 342
model’s performance, similar to (Hayes et al., 2019b). iCubWorld 1.0 is an object recognition dataset which 343
contains the sequences of video frames, with each containing a single object. There are 10 classes, with 344
each containing 3 diﬀerent object instances with 200-201 images each. Overall, each class contains 600-602 345
samples for training and 200-201 samples for testing. CORe50 is similar to iCubWorld 1.0, containing images 346
from temporally coherent sessions, with the whole dataset divided into 11 distinct sessions characterized by 347
diﬀerent backgrounds and lighting. There are 10 classes, with each containing 5 diﬀerent object instances 348
with 2393-2400 images each. Overall, each class contains 11983-12000 samples for training and 4495-4500 349
samples for testing. Technically, iCubWorld and CORe50 are the ideal dataset for streaming learning, as it 350
requires learning from temporally coherent image sequences, which are naturally non-i.i.d images . 351
Evaluation Over Diﬀerent Data Orderings. The proposed approach is robust to the various streaming 352
learning setting; we evaluate the model’s streaming learning ability with the following four (Hayes et al., 353
2019a;b) challenging data ordering schemes: (i)‘streaming iid’ : where the data-stream is organized by the 354
randomly shuﬄed samples from the dataset, (ii)‘streaming class iid‘ : where the data-stream is organized 355
by the samples from one or more classes, these samples are shuﬄed randomly, (iii)‘streaming instance’ : 356
where the data-stream is organized by temporally ordered samples from diﬀerent object instances, and (iv) 357
‘streaming class instance’ : where the data-stream is organized by the samples from diﬀerent classes, the 358
samples within a class are temporally ordered based on diﬀerent object instances. Only iCubWorld 1.0 359
and CORe50 dataset contains the temporal ordering, therefore ‘streaming instance’ , and‘streaming class 360
instance’ setting are evaluated only on these two datasets. Please refer to the appendix for more details. 361
5.2 Metrics 362
For evaluating the performance of the streaming learner, we use Ωallmetric, similar to (Kemker et al., 2018; 363
Hayes et al., 2019a;b), where Ωallrepresents normalized incremental learning performance with respect to 364
an oﬄine learner: 365
Ωall=1
TT/summationdisplay
t=1αt
αoﬄine,t(9)
whereTis the total number of testing events, αtis the performance of the incremental learner at time t, 366
andαoﬄine,tis the performance of a traditional oﬄine model at time t. 367
5.3 Baselines And Compared Methods 368
The proposed approach follows the ‘streaming learning setup’ ; to the best of our knowledge, recent works 369
ExStream (Hayes et al., 2019a) and REMIND (Hayes et al., 2019b) are the only methods that follow the 370
same setting. We compare our approach against these strong baselines. We also compare our model with (i) 371
a network trained with one sample at a time (Fine-tuning/lower-bound) and (ii)a network trained oﬄine, 372
assuming all the data is available (Oﬄine/upper-bound). Finally, we choose recent popular ‘batch’ (IBL) and 373
‘online’learningmethods, suchasEWC(Kirkpatricketal.,2017), MAS(Aljundietal.,2018a), VCL(Nguyen 374
et al., 2017), Coreset VCL (Nguyen et al., 2017), Coreset Only (Farquhar & Gal, 2018), TinyER (Chaudhry 375
et al., 2019), GDumb (Prabhu et al., 2020), AGEM (Chaudhry et al., 2018b) and DER/DER++ (Buzzega 376
et al., 2020) as baselines and rigorously evaluate our model against these approaches. For a fair comparison, 377
we train all the methods in a streaming learning setup, i.e., one sample at a time. ‘Coreset VCL’ and ‘Coreset 378
10Under review as submission to TMLR
Only’ both are trained in a streaming manner; however, the network is ﬁne-tuned with the stored samples 379
before inference. Furthermore, GDumb stores samples in memory and ﬁne-tunes the network with them 380
before inference, while ﬁne-tuning is prohibited in ‘streaming learning’ . Therefore, Coreset VCL, Coreset 381
Only, and GDumb have an extra advantage compared to the true ‘streaming learning’ approaches. Still, 382
BISLERi outperforms these approaches by a signiﬁcant margin. We provide more details about the baseline 383
methods in the appendix. 384
Table 3: Ωallresults with their associated standard deviations. For each experiment, the method with best
performance in ‘streaming-learning-setup’ is highlighted in Bold. The reported results are average over 10
runs with diﬀerent permutations of the data. Oﬄine model is trained only once. \Oﬄine =1
T/summationtextT
t=1αoﬄine,t,
whereTis the total number of testing events. ‘-’ indicates experiments we are unable to run, because of
compatibility issues.
Note: Methods in Red use ﬁne-tuning, implying that these methods violate streaming learning (SL) con-
straints and have an extra advantage over true streaming learning (SL) methods, such as ‘Ours’.
Methodiid Class-iid
CIFAR10 CIFAR100 ImageNet100 CIFAR10 CIFAR100 ImageNet100
Fine-Tune 0.1175±0.0000 0.0180 ±0.0035 0.0127 ±0.0029 0.3447±0.0003 0.1277 ±0.0022 0.1223 ±0.0052
EWC - - - 0.3446±0.0003 0.1292 ±0.0037 0.1225 ±0.0039
MAS - - - 0.3470±0.0075 0.1280 ±0.0029 0.1234 ±0.0046
VCL - - - 0.3442±0.0006 0.1273 ±0.0041 0.1205 ±0.0015
Coreset VCL - - - 0.3716±0.0501 0.1414 ±0.0224 0.1259 ±0.0122
Coreset Only - - - 0.3684±0.0442 0.1432 ±0.0256 0.1273 ±0.0182
GDumb 0.8686±0.0065 0.6067 ±0.0119 0.8361 ±0.0070 0.9252±0.0057 0.7635±0.0096 0.9197±0.0081
AGEM 0.1175±0.0000 0.0182 ±0.0035 0.0139 ±0.0041 0.3448±0.0002 0.1290 ±0.0037 0.1215 ±0.0025
DER 0.1175±0.0000 0.0165 ±0.0003 0.0126 ±0.0027 0.3449±0.0011 0.1278 ±0.0024 0.1217 ±0.0038
DER++ 0.1175±0.0000 0.0173 ±0.0028 0.0130 ±0.0039 0.3588±0.0423 0.1290 ±0.0054 0.1230 ±0.0068
TinyER 0.9314±0.0114 0.7588 ±0.0128 0.9415 ±0.0085 0.8926±0.0158 0.7402 ±0.0195 0.8995 ±0.0122
ExStream 0.8866±0.0244 0.7845 ±0.0121 0.9293 ±0.0082 0.8123±0.0209 0.7176 ±0.0208 0.8757 ±0.0148
REMIND 0.8910±0.0073 0.6457 ±0.0091 0.9088 ±0.0109 0.8832±0.0201 0.6787 ±0.0215 0.8803 ±0.0157
Ours 0.9579±0.0040 0.8679 ±0.0057 0.9640 ±0.0060 0.8991±0.0089 0.7724 ±0.0188 0.9171 ±0.0073
Oﬄine 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
\Oﬄine 0.8509 0.6083 0.8520 0.8972 0.7154 0.8953
Methodiid Class-iid instance Class-instance
iCubWorld 1.0 CORe50 iCubWorld 1.0 CORe50 iCubWorld 1.0 CORe50 iCubWorld 1.0 CORe50
Fine-Tune 0.1369±0.0184 0.1145 ±0.0000 0.3893±0.0534 0.3485 ±0.0171 0.1307±0.0000 0.1145 ±0.0000 0.3485±0.0022 0.3430 ±0.0003
EWC - - 0.3790±0.0419 0.3508 ±0.0243 - - 0.3487±0.0034 0.3427 ±0.0007
MAS - - 0.3912±0.0613 0.3432 ±0.0004 - - 0.3486±0.0019 0.3429 ±0.0005
VCL - - 0.3806±0.0527 0.3462 ±0.0129 - - 0.3473±0.0025 0.3420 ±0.0009
Coreset VCL - - 0.3948±0.0558 0.3424 ±0.0019 - - 0.4705±0.0165 0.4715 ±0.0054
Coreset Only - - 0.3994±0.0922 0.3688 ±0.0499 - - 0.4669±0.0251 0.4748 ±0.0035
GDumb 0.8993±0.0413 0.9345 ±0.0121 0.9660±0.0201 0.9742 ±0.0081 0.6715±0.0540 0.7433 ±0.0246 0.7908±0.0329 0.6548 ±0.0259
AGEM 0.1311±0.0000 0.1145 ±0.0000 0.4047±0.0632 0.3460 ±0.0101 0.1309±0.0003 0.1145 ±0.0000 0.3489±0.0030 0.3429 ±0.0004
DER 0.1437±0.0393 0.1145 ±0.0000 0.4057±0.1046 0.3432 ±0.0005 0.3759±0.2404 0.1168 ±0.0072 0.4082±0.1662 0.3308 ±0.0385
DER++ 0.1428±0.0364 0.1145 ±0.0000 0.4467±0.1287 0.3431 ±0.0003 0.4518±0.2510 0.1145 ±0.0000 0.4499±0.2311 0.3429 ±0.0006
TinyER 0.9590±0.0378 1.0007 ±0.0121 0.9069±0.0297 0.9573 ±0.0125 0.8726±0.0649 0.8432 ±0.0262 0.8215±0.0341 0.8461 ±0.0247
ExStream 0.9235±0.0584 0.9844 ±0.0156 0.8820±0.0285 0.8760 ±0.0166 0.8954±0.0542 0.8257 ±0.0295 0.8727±0.0229 0.8837 ±0.0211
REMIND 0.9260±0.0311 0.9933 ±0.0115 0.8553±0.0349 0.9448 ±0.0125 0.8157±0.0600 0.8544 ±0.0247 0.7615±0.0319 0.7826 ±0.0377
Ours 0.9716±0.0141 1.0069 ±0.0058 0.9480±0.0215 0.9686 ±0.0122 0.9580±0.0298 0.9824 ±0.0090 0.9585±0.0223 0.9384 ±0.0130
Oﬄine 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
\Oﬄine 0.7626 0.8733 0.8849 0.9070 0.7646 0.8733 0.8840 0.9079
5.4 Results 385
The detailed results of BISLERi over various experimental settings along with the strong baseline methods 386
are shown in Table 3. We can clearly observe that BISLERi consistently outperforms all the baseline 387
by a signiﬁcant margin. The proposed model is also robust to the diﬀerent streaming learning scenarios 388
compared to the baselines. We repeat our experiment ten times, and report the average-accuracy along 389
with the standard-deviations. We observe that ‘batch-learning’ methods severely suﬀer from catastrophic 390
forgetting. Moreover, replay-based ‘online-learning’ method such as AGEM also suﬀer from information loss 391
badly. 392
Although GDumb achieves higher accuracy on several datasets on class-i.i.d ordering, it ﬁne-tunes the 393
network parameters before each inference, ultimately violating the constraints of streaming learning (refer, 394
Section 2). Therefore, we do not consider GDumb as the best-performing method, even when it achieves 395
higher accuracy. 396
11Under review as submission to TMLR
VCL
Coreset
VCL
AGEM
DER
DER++
TinyER
REMIND
Ours0.20.40.60.81.0Ωall Results(a) Class-iid
Batch
Online
Streaming
VCL
Coreset
VCL
AGEM
DER
DER++
TinyER
REMIND
Ours(b) Class-instance
Batch
Online
Streaming
Figure 2: Ωallresults for ‘batch’,‘online’ and‘streaming’ versions of baselines on iCubWorld 1.0 on (a)
Class-iid, and (b)Class-instance ordering. An empty plot in AGEM indicates, we are unable to conduct
experiment due to compatibility issues.
We believe it is important to highlight that iCubWorld 1.0 and CORe50 are two challenging datasets, which 397
evaluate the models in more realistic scenarios or data-orderings. Particularly, class-instance and instance 398
ordering require the learner to learn from temporally ordered video frames one at a time. From Table 3, 399
we observe that BISLERi obtain up to 8.58%&6.26%improvement on iCubWorld 1.0, and 5.47%&12.8% 400
improvementonCORe50, overthestate-of-the-artstreaminglearningapproaches. Figure4showstheimpact 401
of temporal orderings on the streaming learning model’s performance. It is evident that class-instance and 402
instance ordering are more diﬃcult, and the baselines continue to suﬀer from severe forgetting. Figure 3 403
plots the accuracy (αt)of BISLERi (Ours) and other baselines on (i)class-i.i.d and (ii)class-instance 404
data-orderings on iCubWorld 1.0 and CORe50 datasets. It can be observed that BISLERi remembers the 405
previousclassesbetterthantheothercomparedbaselines. Furthermore, itcanalsobeobservedthatBISLERi 406
performs signiﬁcantly better than the baselines in class-instance ordering, i.e., when there exists temporal 407
coherance in the input data-stream. 408
Finally, for completeness, we train BISLERi in ‘batch’ as well as ‘online’ learning setting to determine its 409
eﬀectiveness and compatibility in these settings. In Figure 2, we compare BISLERi with various baselines. It 410
can be observed that BISLERi outperforms the baselines by a signiﬁcant margin on both class-i.i.d and class- 411
instance ordering on iCubWorld. It implies that, even though BISLERi designed to work in the streaming 412
setting, it can be thought of as a robust method for various lifelong learning scenarios with the widest possible 413
applicability . We provide more details in the appendix. 414
5.5 Implementation Details 415
In all the experiments, models are trained with one sample at a time. For a fair comparison, the same 416
network structure is used throughout all the models. For all methods, we use fully connected single-head 417
networks with two hidden layers as the plastic network F(·), where each layer contains 256nodes with ReLU 418
activations; for ‘VCL’, ‘Coreset VCL’, ‘Coreset Only’ and ‘BISLERi’, F(·)is a BNN, whereas for all other 419
methodsF(·)is a deterministic network. For a fair comparison, we store the same number of past examples 420
for all replay-based approaches. For REMIND, we compress and store the feature-maps with Faiss (Johnson 421
et al., 2019) product quantization (PQ) implementation with s= 32sub-vectors and codebook size c= 256. 422
Westorethefeature-mapinmemoryforalltheothermethods, includingourapproachBISLERi. Inaddition, 423
BISLERi also store the corresponding logits, loss-values, and uncertainty-scores. The capacity of our replay 424
buﬀer is mentioned in Table 2. For memory-replay, we use ‘uncertainty-aware positive-negative’ sampling 425
strategy (discussed in Section 3.2) throughout all data-orderings, except for ‘streaming-i.i.d’ ordering, we use 426
‘uniform’ sampling. We use ‘loss-aware weighted random replacement with a reservoir’ sampling strategy as 427
memory replacement policy for all the experiments. We store the same number of past examples in memory 428
12Under review as submission to TMLR
2 4 6 8 10
Number of Classes0.20.40.60.81.0αtClass-i.i.d
2 4 6 8 10
Number of Classes0.20.40.60.81.0αt
2 4 6 8 10
Number of Classes0.20.40.60.81.0αtClass-instance
2 4 6 8 10
Number of Classes0.20.40.60.81.0αtiCubWorld 1.0 CORe50
Offline
OursREMIND
TinyERExStream
GDumbEWC
MASVCL
Coreset VCLCoreset Only
DERDER++
AGEMFine-tune
Figure 3: Performance of various incremental learning models on (i)streaming class-i.i.d (top row) and (ii)
streaming class-instance (bottom row) ordering on iCubWorld 1.0 & CORe50 dataset. The plots suggest
BISLERi (Ours) remembers earlier classes better than most existing algorithms. The performance gain
is even more pronounced in streaming class-instance ordering setting (bottom row) where the baseline in-
cremental learners suﬀer from severe forgetting. Recall that GDumb cannot be considered as a streaming
learning algorithm as it requires ﬁne-tuning.
GDumb TinyERExStream REMIND Ours0.60.70.80.91.01.1Ωall Results(a) iCubWorld 1.0
GDumb TinyERExStream REMIND Ours(b) CORe50
i.i.d instance Class-i.i.d Class-instance
Figure 4: Plots of Ωallas a function of streaming learning model and data-ordering on (a)iCubWorld 1.0,
and(b)CORe50. Only diﬀerence between i.i.d vs instance and class-i.i.d vs class-instance ordering, is the
presence of temporal ordering (ref. Sec. 5.1); however, its eﬀect on the streaming learner’s performance is
signiﬁcant.
13Under review as submission to TMLR
Table 4: ΩallResults with their associated standard deviations. For each experiment, the method with best
performance is highlighted in Bold.
Memory Replacement Sample SelectioniCubWrold ImageNet100
instance Class-instance iid Class-iid
LAWCBRUni 0.8975±0.0454 0.8506±0.0310 0.9582±0.0037 0.9014±0.0073
UAPN 0.9346±0.0395 0.8500±0.0363 0.9327±0.0052 0.9135±0.0081
LAPN 0.9172±0.0373 0.8536±0.0343 0.9253±0.0115 0.9122±0.0091
LAWRRRUni 0.9269±0.0383 0.9346±0.0191 0.9640±0.0060 0.8643±0.0127
UAPN 0.9580±0.0298 0.9585±0.0223 0.9578±0.0035 0.9171±0.0073
LAPN 0.9558±0.0304 0.9497±0.0239 0.9575±0.0047 0.9112±0.0075
across all methods. For memory-replay, we use N/prime
1= 16past samples throughout all experiments across 429
BISLERi, AGEM, DER/DER++, TinyER, ExStream and REMIND. For knowledge-distillation, BISLERi 430
and DER++ use N/prime
2= 16samples at any time step t. We set the hyper-parameter λ1= 1andλ2= 0.3 431
across all experiments; however, for online/batch learning experiments, we use λ2= 0.2and useuniform 432
sampling for memory replay. For EWC, we set hyper-parameter λ= 500, for MAS, we set hyper-parameter 433
λ= 1, and for DER/DER++, we use α=β= 0.5. We repeated each experiments for 10 times with diﬀerent 434
permutations of the data, and reported the results by taking average of 10 runs. More details are given in 435
the appendix. 436
6 Ablation Study 437
We perform extensive ablation to show the importance of the diﬀerent components. The various ablation 438
experiments validate the signiﬁcance of the proposed components. 439
Signiﬁcance Of Diﬀerent Sampling Strategies. In Table 4, we compare the performance of BISLERi 440
while using various sampling strategies and memory replacement policies. We observe that for the buﬀer 441
replacement, LAWRRR performs better compared to LAWCBR. Furthermore, for the sample replay, UAPN, 442
along with LAWRRR memory buﬀer policy, outperforms other sampling strategies, except uniform sampling 443
(Uni) performs better on i.i.d ordering. We provide more details in the appendix. 444
Choice Of Hyperparameter ( λ2).Figure 5 shows the eﬀect of changing the knowledge-distillation loss 445
weightλ2on the ﬁnal Ωallaccuracy for iCubWorld 1.0 on instance and class-instance ordering, while using 446
diﬀerent sampling strategies and buﬀer replacement policies. We observe the best model performance for 447
λ2= 0.3, and use this value for all our experiments. We provide detailed ablation on λ2in the appendix. 448
0.0 0.1 0.2 0.3 0.4 0.5
λ
20.650.700.750.800.850.900.951.00Ω
all Results(i) instance
0.0 0.1 0.2 0.3 0.4 0.5
λ
2(ii) class-instance
Uni + LA WCBR
UAPN + LA WCBRLAPN + LA WCBR
Uni + LA WRRRUAPN + LA WRRR
LAPN + LA WRRR
Figure 5: Plots of Ωallas a function of hyper-parameter λ2and diﬀerent sampling strategies and replacement
policies for (i)instance, (ii)class-instance ordering on iCubWorld 1.0.
Signiﬁcance Of Knowledge-Distillation Loss. Figure 5 with λ2= 0.0represents the model without 449
knowledge distillation. We can observe that the model performance signiﬁcantly degrades without knowledge 450
14Under review as submission to TMLR
distillation. Therefore, knowledge distillation is a key component to the model’s performance. More details 451
are given in the appendix. 452
Choice Of Buﬀer Capacity. We perform an ablation for the diﬀerent buﬀer capacities, i.e., |M|. The 453
results are shown in Figure 6. It is evident that, with the longer sequence of incoming data, the model’s 454
(BISLERi) performance improves with the increase in the buﬀer capacity, as it helps minimize the confusion 455
in the output prediction. 456
CIF AR10 CIF AR1000.600.650.700.750.800.850.900.951.00Ω
all Results0.84
0.670.90
0.770.93
0.810.94
0.820.95
0.83|M| = 500
|M| = 1000
|M| = 1500|M| = 2000
|M| = 2500
Figure 6: Plots of Ωallas a function of buﬀer capacity |M|for class-i.i.d data-ordering on CIFAR10 and
CIFAR100.
7 Conclusion 457
Streaming continual learning (SCL) is the most challenging and realistic framework for continual learning; 458
most of the recent promising models for the CL are unable to handle this above setting. Our work proposes 459
a dual regularization and loss-aware buﬀer replacement to handle the SCL scenario. The proposed model is 460
highly eﬃcient since it learns a joint likelihood from the current and replay samples without leveraging any 461
external ﬁnetuning. Also, to improve the training eﬃciency further, the proposed model selects a few most 462
informative samples from the buﬀer instead of using the entire buﬀer for the replay. We have conducted 463
a rigorous experiment over several challenging datasets and showed that BISLERi outperforms the recent 464
state-of-the-art approaches in this setting by a signiﬁcant margin. To disentangle the importance of the 465
various components, we perform extensive ablation studies and observe that the proposed components are 466
essential to handle the SCL setting. 467
References 468
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network 469
of experts. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 470
3366–3375, 2017. 471
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory 472
aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer 473
Vision (ECCV) , pp. 139–154, 2018a. 474
Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selﬂess sequential learning. arXiv preprint 475
arXiv:1806.05421 , 2018b. 476
Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and 477
Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. arXiv preprint 478
arXiv:1908.04742 , 2019. 479
EdenBelouadah, AdrianPopescu, andIoannisKanellos. Acomprehensivestudyofclassincrementallearning 480
algorithms for visual tasks. Neural Networks , 2020. 481
15Under review as submission to TMLR
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural 482
networks. arXiv preprint arXiv:1505.05424 , 2015. 483
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan. Streaming 484
variational bayes. arXiv preprint arXiv:1307.6769 , 2013. 485
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience 486
for general continual learning: a strong, simple baseline. arXiv preprint arXiv:2004.07211 , 2020. 487
Lucy R Chai. Uncertainty estimation in bayesian neural networks and links to interpretability. Master of 488
Philosophy (University of Cambridge) , 2018. 489
Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk 490
for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European 491
Conference on Computer Vision (ECCV) , pp. 532–547, 2018a. 492
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Eﬃcient lifelong 493
learning with a-gem. arXiv preprint arXiv:1812.00420 , 2018b. 494
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, 495
Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv 496
preprint arXiv:1902.10486 , 2019. 497
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive 498
learning. arXiv preprint arXiv:2003.04297 , 2020. 499
Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, 500
and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation tasks. IEEE 501
Transactions on Pattern Analysis and Machine Intelligence , 2021. 502
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual 503
learning with bayesian neural networks. arXiv preprint arXiv:1906.02425 , 2019. 504
Sean Fanello, Carlo Ciliberto, Matteo Santoro, Lorenzo Natale, Giorgio Metta, Lorenzo Rosasco, and 505
Francesca Odone. icub world: Friendly robots help building good vision data-sets. In Proceedings of 506
the IEEE Conference on Computer Vision and Pattern Recognition Workshops , pp. 700–705, 2013. 507
Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint 508
arXiv:1805.09733 , 2018. 509
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander 510
Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv 511
preprint arXiv:1701.08734 , 2017. 512
Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences , 3(4): 513
128–135, 1999. 514
João Gama, Raquel Sebastião, and Pedro Pereira Rodrigues. On evaluating stream learning algorithms. 515
Mach. Learn. , 90(3):317–346, March 2013. 516
Soumya Ghosh, Jiayu Yao, and Finale Doshi-Velez. Structured variational learning of bayesian neural 517
networks with horseshoe priors. In International Conference on Machine Learning , pp. 1744–1753. PMLR, 518
2018. 519
Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation 520
of catastrophic forgetting in gradient-based neural networks, 2015. 521
Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory eﬃcient experience replay for streaming 522
learning. In 2019 International Conference on Robotics and Automation (ICRA) , pp. 9769–9776. IEEE, 523
2019a. 524
16Under review as submission to TMLR
Tyler L Hayes, Kushal Kaﬂe, Robik Shrestha, Manoj Acharya, and Christopher Kanan. Remind your neural 525
network to prevent catastrophic forgetting. arXiv preprint arXiv:1910.02509 , 2019b. 526
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 527
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016. 528
Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. arXiv preprint 529
arXiv:1503.02531 , 2015. 530
David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. arXiv preprint 531
arXiv:1802.10269 , 2018. 532
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE 533
transactions on pattern analysis and machine intelligence , 33(1):117–128, 2010. 534
Jeﬀ Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions 535
on Big Data , 2019. 536
Laurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, and Mohammed Bennamoun. Hands- 537
on bayesian neural networks–a tutorial for deep learning users. arXiv preprint arXiv:2007.06823 , 2020. 538
Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. arXiv 539
preprint arXiv:1711.10563 , 2017. 540
Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes, and Christopher Kanan. Measuring 541
catastrophic forgetting in neural networks. In Thirty-second AAAI conference on artiﬁcial intelligence , 542
2018. 543
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 544
2013. 545
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, 546
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic 547
forgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526, 2017. 548
Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 549
Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convolutional 550
neural networks. In Advances in neural information processing systems , pp. 1097–1105, 2012. 551
Sang-WooLee, Jin-HwaKim, JaehyunJun, Jung-WooHa, andByoung-TakZhang. Overcomingcatastrophic 552
forgetting by incremental moment matching. In Advances in neural information processing systems , pp. 553
4652–4662, 2017. 554
Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and 555
machine intelligence , 40(12):2935–2947, 2017. 556
Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object 557
recognition. In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg (eds.), Proceedings of the 1st Annual 558
Conference on Robot Learning , volume 78 of Proceedings of Machine Learning Research , pp. 17–26. PMLR, 559
13–15 Nov 2017. URL https://proceedings.mlr.press/v78/lomonaco17a.html . 560
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In Advances 561
in Neural Information Processing Systems , pp. 6467–6476, 2017. 562
ArunMallyaandSvetlanaLazebnik. Packnet: Addingmultipletaskstoasinglenetworkbyiterativepruning. 563
InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 7765–7773, 2018. 564
Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks 565
by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV) , 566
pp. 67–82, 2018. 567
17Under review as submission to TMLR
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential 568
learning problem. In Psychology of learning and motivation , volume 24, pp. 109–165. Elsevier, 1989. 569
Hossein Mobahi, Mehrdad Farajtabar, and Peter L. Bartlett. Self-distillation ampliﬁes regularization in 570
hilbert space. In NIPS, 2020. 571
Radford M Neal. Bayesian learning for neural networks , volume 118. Springer Science & Business Media, 572
2012. 573
Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. arXiv 574
preprint arXiv:1710.10628 , 2017. 575
German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong 576
learning with neural networks: A review. Neural Networks , 113:54–71, 2019. 577
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, 578
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep 579
learning library. In Advances in neural information processing systems , pp. 8026–8037, 2019. 580
Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our 581
progress in continual learning. In European Conference on Computer Vision , pp. 524–540. Springer, 2020. 582
Sylvestre-Alvise Rebuﬃ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental 583
classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and 584
Pattern Recognition , pp. 2001–2010, 2017. 585
Amanda Rios and Laurent Itti. Closed-loop memory gan for continual learning. arXiv preprint 586
arXiv:1811.01146 , 2018. 587
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej 588
Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. 589
International journal of computer vision , 115(3):211–252, 2015. 590
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray 591
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint 592
arXiv:1606.04671 , 2016. 593
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: 594
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and 595
pattern recognition , pp. 4510–4520, 2018. 596
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting 597
with hard attention to the task. arXiv preprint arXiv:1801.01423 , 2018. 598
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. 599
InAdvances in Neural Information Processing Systems , pp. 2990–2999, 2017. 600
Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and Yee Whye Teh. 601
Functional regularisation for continual learning with gaussian processes. arXiv preprint arXiv:1901.11356 , 602
2019. 603
Jeﬀrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS) , 604
11(1):37–57, 1985. 605
Chenshen Wu, Luis Herranz, Xialei Liu, Joost van de Weijer, Bogdan Raducanu, et al. Memory replay gans: 606
Learning to generate new categories without forgetting. In Advances In Neural Information Processing 607
Systems, pp. 5962–5972, 2018. 608
Ye Xiang, Ying Fu, Pan Ji, and Hua Huang. Incremental learning using conditional adversarial networks. 609
InProceedings of the IEEE International Conference on Computer Vision , pp. 6619–6628, 2019. 610
18Under review as submission to TMLR
Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation: Teacher-student optimization in 611
onegeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 612
pp. 2859–2868, 2019. 613
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expand- 614
able networks. arXiv preprint arXiv:1708.01547 , 2017. 615
Jason Yosinski, Jeﬀ Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural 616
networks? In Advances in neural information processing systems , pp. 3320–3328, 2014. 617
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In 618
Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pp. 3987–3995. JMLR. 619
org, 2017. 620
Chen Zeno, Itay Golan, Elad Hoﬀer, and Daniel Soudry. Task agnostic continual learning using online 621
variational bayes. arXiv preprint arXiv:1803.10123 , 2018. 622
Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and self- 623
supervision for incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision 624
and Pattern Recognition , pp. 5871–5880, 2021. 625
19Under review as submission to TMLR
A Preliminaries 626
A.1 ‘Class Incremental Learning’ V/S ‘Task Incremental Learning’ 627
‘Class incremental learning’ (Rebuﬃ et al., 2017; Chaudhry et al., 2018a; Hayes et al., 2019a;b; Rios & 628
Itti, 2018; Belouadah et al., 2020), is a challenging variant of lifelong learning, where the classiﬁer needs 629
to learn to discriminate between diﬀerent class labels from diﬀerent tasks. The key distinction between 630
‘class incremental learning’ and‘task incremental learning’ (Kirkpatrick et al., 2017; Aljundi et al., 2018a;b; 631
Nguyen et al., 2017; Zenke et al., 2017), lies in how the classiﬁer’s accuracy is evaluated at the test time. 632
In‘class incremental learning’ , at the test time, the task identiﬁer tis not speciﬁed, and the accuracy is 633
computed over all the observed classes with1
Cchance, whereCis the total number of classes accumulated 634
so far. However, in ‘task incremental learning’ , the task identiﬁer tis known. 635
Forexample, considerMNISTdividedinto 5tasks:{{0,1},...,{8,9}}, whichareusedforsequentiallearning 636
of a classiﬁer. Then, at the end of 5-th task, in ‘task incremental setting’ , the classiﬁer needs to predict a 637
class out of{8,9}only. However, in ‘class incremental setting’ , a class label is predicted over all the ten 638
classes that is observed so far, i.e., {0,..., 9}with1
10chance for each class. 639
A.2 Variational Continual Learning (VCL) 640
Variational Continual Learning (VCL) (Nguyen et al., 2017) is a recently proposed continual learning ap- 641
proach that mitigates catastrophic forgetting (McCloskey & Cohen, 1989; French, 1999) in neural networks 642
in a Bayesian framework (Neal, 2012; Jospin et al., 2020). It sets the posterior of parameters distribution 643
as the prior before training on the next task, i.e., pt(θ) =qt−1(θ), the new task reuses the previous task’s 644
posterior as the new prior. VCL solves the following KL divergence minimization problem while training on 645
tasktwith the new data Dt: 646
qt(θ) = arg min
q/epsilon1QKL/parenleftbigg
q(θ)||1
Ztqt−1(θ)p(Dt|θ)/parenrightbigg
(10)
While oﬀering a principled way of continual learning, VCL follows task incremental learning setting, and uses 647
‘task speciﬁc head networks’ , for each task t, such that, p(θ|D1:t) =p(θt|Dt)p(θS|D1:t), whereθ={θS,θt}, 648
θsis shared between all the tasks, whereas θtkept ﬁxed after training on task t. This conﬁguration prohibits 649
knowledge transfer across tasks, and results in a poor accuracy in class incremental setting (Farquhar & Gal, 650
2018) for both VCL with or without Coreset. 651
Coreset VCL (Nguyen et al., 2017) withhold some data points from the task data before training and keeps 652
them in a coreset. These data points are not used for the network training and are only used for ﬁnetuning 653
the network before each inference. However, in streaming learning , ﬁnetuning the network at any time 654
is prohibited, as it makes the training process a two-step learning process instead of single-pass learning. 655
Furthermore, the coreset is created by sampling data points from the entire task data, whereas in streaming 656
setting, each instance arrives one at a time. Finally, the performance of Coreset VCL is heavily dependent 657
on the ﬁnetuning with the coreset samples before inference (Farquhar & Gal, 2018), and still not comparable 658
enough to our proposed method (BISLERi). 659
A.3 REMIND 660
REMIND (Hayes et al., 2019b) is a recently proposed rehearsal-based lifelong learning approach which 661
combats catastrophic forgetting (French, 1999) in deep neural network in streaming setting . While following 662
such a challenging setting, it separates the convolutional neural network into two networks: (i)a frozen 663
feature extractor and (ii)a plastic neural network. Learning involves the following steps: (i)compression of 664
each new input using product quantization (PQ) (Jegou et al., 2010), (ii)reconstruction of the previously 665
stored compressed representations using PQ, and (iii)mixing the reconstructed past examples with the new 666
input and updating the parameters of the plastic layers of the network. 667
While it combats catastrophic forgetting and achieves state-of-the-art performance, there are few concerns 668
that can be limiting in the continual learning setup. It stores considerably a large number of past examples 669
20Under review as submission to TMLR
compared to the baselines; for example, iCaRL (Rebuﬃ et al., 2017) stores 10K past examples for Ima- 670
geNet (ILSVRC-2012) (Russakovsky et al., 2015) experiment, whereas REMIND stores 1M past examples. 671
Furthermore, REMIND actually uses a lossy compression method (PQ) to store the past samples, which 672
is merely an engineering technique far from any algorithmic improvement and can be used by any lifelong 673
learning approach. 674
A.4 Bayesian Neural Network 675
Bayesian neural networks (Neal, 2012; Jospin et al., 2020) are discriminative models, which extend the 676
standard deep neural networks with Bayesian inference. The network parameters are assumed to have a prior 677
distribution, p(θ), and it infers the posterior given the observed data D, that is,p(θ|D). However, the exact 678
posterior inference is computationally intractable for any complex models, and an approximation is needed. 679
One such scheme is ‘Bayes-by-Backprop’ (BBB) (Blundell et al., 2015). It uses a mean-ﬁeld variational 680
posteriorq(θ)over the network parameters and uses reparameterization-trick (Kingma & Welling, 2013) 681
to sample from the posterior, which are then used to approximate the evidence lower bound (ELBO) via 682
Monte-Carlo sampling. 683
In our proposed method (BISLERi), we have used a Bayesian neural network (BNN) as the plastic network 684
F(·). We have discussed training the plastic network (BNN) F(·)with a single step posterior update without 685
catastrophic forgetting (French, 1999) in class-incremental streaming learning (CISL) setup in Section 3.1. 686
B Diﬀerences Between VCL/Coreset VCL and BISLERi 687
In this section, we describe the diﬀerences between VCL/Coreset VCL (Nguyen et al., 2017; Farquhar & 688
Gal, 2018) and the proposed method (BISLERi). The diﬀerences are as follows - 689
•While BISLERi and VCL/Coreset VCL both utilizes Bayesian framework to enable continual learn- 690
ing in the deep neural networks, VCL/Coreset VCL is a ‘incremental batch learning’ (IBL) mathod 691
in nature, whereas BISLERi is a streaming/online learning method. That is, in order to approxi- 692
mate the posterior in each incremental step, VCL/Coreset VCL requires visiting the data multiple 693
times, whereas BISLERi approximates the posterior with a single gradient update. In doing so we 694
need to obtain important modiﬁcations to obtain correct estimates of likelihood and updation of the 695
posterior. Naively using VCL can be observed to perform quite inferior to the proposed solution. 696
Our work is a principled adaptation of the formulation to the streaming learning setting and this is 697
quite diﬀerent from the continual learning based on ‘batch-based updates’. 698
•Both VCL and Coreset VCL do not utilize any memory replay, while approximating the new pos- 699
terior, whereas BISLERi replays a subset of the past stored samples along with the newly available 700
sample in order to approximate the new posterior to enable continual learning. Approximating the 701
posterior in this way, i.e., replaying a subset of past samples with the new sample, allows BISLERi to 702
achieve‘any-time-inference’ ability, which is a key-requirement in streaming learning, as ﬁne-tuning 703
the network parameters with the stored samples is forbidden in the streaming learning setup. 704
•While VCL do not use coreset samples during any step of the learning, Coreset VCL withholds 705
a few past samples in memory (coreset), which are then used for ﬁne-tuning the network before 706
inference. However, Coreset VCL stores the samples in the coreset in a task-speciﬁc manner , unlike 707
the methods like GDumb (Prabhu et al., 2020), TinyER (Chaudhry et al., 2019), REMIND (Hayes 708
et al., 2019b), BISLERi (Ours). We explain this with the below example. 709
Consider MNIST divided into 5 tasks: {{0,1},{2,3},...,{8,9}}, which are used for sequential 710
learning of a classiﬁer. Therefore, in each incremental step, the classiﬁer observes sample from only 711
two classes. In this case, Coreset VCL stores samples in memory (coreset) in a task-speciﬁc manner. 712
That is, it divides the coreset into 5 partitions, where each partition is used to store samples from 713
a single task. Before inference, samples corresponding to the current task is utilized to ﬁne-tune 714
the network parameters to improve performance. For example, at the end of 5-th task, since the 715
21Under review as submission to TMLR
classiﬁer only needs to predict a class out of {8,9}, Coreset VCL ﬁne-tunes the network with the 716
withheld samples corresponding to only class {8,9}. While this strategy works nicely in case of 717
‘task-incremental learning’, it suﬀers severely in ‘class-incremental learning’ setup. 718
In contrast methods like GDumb (Prabhu et al., 2020), TinyER (Chaudhry et al., 2019), RE- 719
MIND (Hayes et al., 2019b), BISLERi (Ours), do not store the samples in memory in a ‘task-speciﬁc’ 720
manner, instead it is populated with the samples from all the classes. Therefore, when methods like 721
GDumb, REMIND, BISLERi replays the past samples, it observe samples across all the classes irre- 722
spective of the tasks, whereas Coreset VCL only observes samples corresponding to the speciﬁc task, 723
which causes Coreset VCL to suﬀer from poor generalization in case of ‘class-incremental learning’ 724
setup. 725
•Coreset VCL withhold few data-points from the dataset, and do not utilize them during the incre- 726
mental learning. These withheld samples are only used for ﬁne-tuing the network parameters before 727
inference. In contrast BISLERi maintains a replay buﬀer which is updated in an online manner as 728
mentioned in Section 3.3. During streaming learning, in each incremental step a subset of samples 729
are selected (Section 3.2) and combined with the newly available sample to compute the new poste- 730
rior, which enables the network with the ‘any-time-inference’ ability, a crucial property required in 731
streaming learning. 732
B.1 How VCL/Coreset VCL is adapted in the Streaming Learning? 733
In this subsection, we describe how the actual VCL/Coreset VCL (Nguyen et al., 2017) is adapted, so that it 734
can be trained incrementally with a single training example in each incremental step in streaming learning. 735
ForbothVCLandCoresetVCL,wehaveusedasingle-headedBayesiannetworkastheplasticnetwork (F),as 736
also mentioned in Section 5.5. We follow the same strategy as mentioned by Nguyen et al. (2017); Farquhar & 737
Gal (2018) to approximate the new posterior in each incremental step by combining the previously computed 738
posterior with the new data-likelihood. For Coreset VCL, the samples are stored in memory (coreset) in 739
a task-speciﬁc manner, while arriving one datum at a time in each incremental step. At the end of each 740
task, Coreset VCL selects the samples speciﬁc to the current task and ﬁne-tunes the network parameters. 741
While ﬁne-tuning the network is forbidden in streaming learning, it does not improve the network’s overall 742
performance in ‘class-incremental learning’ setup due to the above mentioned reasons. 743
C Various Columns Of Table 1 In Detail 744
In this section, we describe the various columns that we have used to categorize the existing continual 745
learning approaches on the basis of underlying assumptions as they impose. That is, we categorize each 746
continual learning according to various constraints that they follow/mention in the respective literature. 747
•Type. Each CL approach is classiﬁed into one of three types: (i)incremental batch learning 748
(IBL/Batch), (ii)online learning (Online), and (iii)streaming learning (Streaming). 749
The key diﬀerence between IBL and online/streaming learning approaches is that IBL approaches 750
visit the data multiple times, perform multiple gradient update to adapt to the newly available 751
data. While these approaches works nicely in a static environment, these methods can be applied 752
in a dynamic non-stationary environment. In contrast, online/streaming learning approaches adapt 753
to the newly available data in a single gradient update. 754
The key diﬀerence between an online learning and a streaming learning approaches mainly lies on 755
whether a method is allowed to do ﬁne-tuning or not. While both the approaches learns with a single 756
gradient update, in online learning, it is permitted to ﬁne-tune the network parameters with the 757
stored samples any time. However, this would also imply that an online learning approach involves 758
multiple gradient updates to improve its performance, which is forbidden in streaming learning. For 759
example, GDumb (Prabhu et al., 2020) requires ﬁne-tuning before each inference, therefore, it uses 760
multiple gradient updated, ultimately violating the single-pass learning constraint of the streaming 761
22Under review as submission to TMLR
learning. On the other hand, in streaming learning, no single method is allowed to use any additional 762
computation, such as ﬁne-tuning, to improve its performance. For example, BISLERi, REMIND do 763
not use any ﬁne-tuning at any stage of learning. 764
•Bayesian Framework. Whether a method uses a Bayesian framework/formulation or not. 765
•Batch Size. IBL and online learning method assumes that a batch of samples arrive in each 766
incremental step, where the batch size: Nt/greatermuch1. In constrast, streaming learning approaches assume 767
that incremental step, only a single training example arrives, such that, the batch size: Nt= 1. 768
•Fine-tunes. Whether a method requires ﬁne-tuning or not. 769
•Single Pass Learning. In online/streaming learning, each newly available (training) sample(s) is 770
only allowed to observe only once without storing it in a memory (replay buﬀer), and requires to 771
be adapted in a single gradient update. This is refered as single-pass learning. In each incremental 772
step, however, it is allowed to replay past observed samples stored in memory along with the newly 773
available data. Please also refer to REMIND (Hayes et al., 2019b) (Section 2), where they have 774
deﬁned this single pass learning formulation to emphasize that we have not invented a new problem 775
formulation, it was already existing. 776
In addition, in online learning, it is allowed to ﬁne-tune the network with the stored samples by 777
repeating the ﬁne-tuning for multiple epochs, multiple times. However, this implies that the network 778
would use multiple gradient update instead of a single gradient update to improve its performance, 779
which is essentially forbidden in streaming learning. 780
MIR (Aljundi et al., 2019) violates the single pass learning constraint of streaming learning, by 781
employing a two step/pass learning strategy. Initially, it uses the newly available sample(s) to 782
perform a parameter update to select the maximally interfered past stored samples from memory to 783
be used for experience replay. Finally, it combines the new available sample(s), already used once 784
for a gradient update, with the selected maximally interfered samples to perform another (ﬁnal) 785
gradient update. Therefore, MIR essentially uses a two step/pass learning, instead of a single pass 786
learning as required in streaming learning. For more details on the streaming learning constraints 787
refer to Section 2. 788
GDumb (Prabhu et al., 2020) requires ﬁne-tuning the network parameters for multiple epochs, 789
multiple times with the stored replay buﬀer samples before each inference, as it does not employ any 790
learning when it observes a new sample in each incremental step. It implies that GDumb requires 791
multiple gradient update to improve its performance, ultimately violates the single pass learning 792
constraint. 793
•Class Incremental Learning (CIL). Whether supports class-incremental learning or not. 794
•Subset Buﬀer Replay. Whether replays a subset of samples selected from memory or replays all 795
the samples stored in memory in each incremental step. 796
For example, ExStream (Hayes et al., 2019a) uses memory-replay to enable streaming learning, 797
however, in doing so, it replays all the stored samples along with the newly available sample in each 798
incremental step. While it mitigates catastrophic forgetting in the network, it limits its practical 799
applicability due to obvious reasons. That is, if the buﬀer capacity is considerably large then time 800
required to complete a single gradient update will also be large. On the other hand, the methods 801
like REMIND (Hayes et al., 2019b), DER/DER++ (Buzzega et al., 2020), BISLERi (Ours) uses 802
subset buﬀer replay to enable streaming learning. That is, it select only a few samples from memory, 803
combines them with the newly available sample in order to perform single gradient update to enable 804
continual learning, which is computationally an eﬃcient choice. 805
•Training Time. Training time column denotes the number of gradient updates required by the 806
corresponding method according to the underlying assumption that method impose. Therefore, ζ(n) 807
denotes that the corresponding method would require ngradient update in order to enable continual 808
learning. 809
23Under review as submission to TMLR
For example, EWC (Kirkpatrick et al., 2017), MAS (Aljundi et al., 2018a), VCL/Coreset 810
VCL (Nguyen et al., 2017) visits the data multiple times, performs multiple gradient update, to 811
enable continual learning, therefore, its training time is represented with ζ(n). On the other hand, 812
methods such as DER/DER++ (Buzzega et al., 2020), REMIND (Hayes et al., 2019b), BISLERi 813
(Ours) can adapt to the newly available in a single gradient update, hence, the training time is ζ(1). 814
•Inference Time. Similar to Training time column, it denotes the number of gradient updates 815
required by the corresponding method according to the underlying assumption that method impose. 816
Methods which do not require ﬁne-tuning can be evaluated directly, therefore, inference time 817
is represented as ζ(1). However, methods which require ﬁne-tuning before inference, such as 818
GDumb (Prabhu et al., 2020), uses multiple gradient updates to improve its performance, therefore, 819
inference time is represented as ζ(n), wherendenotes the number of gradient updates used during 820
ﬁne-tuning the network. 821
•Violates Any CISL Constraint. Whether violates any ‘class-incremental streaming learning’ 822
(CISL) constraints or not. 823
•Memory Capacity. It denotes the number of past observed samples are stored in the replay buﬀer 824
(memory). 825
•Regularization Based. Whether a method uses parameter regularization or not. That is, if a 826
method qualiﬁes as a regularization based method, then it uses parameters regularization to enable 827
continual learning. 828
Parisi et al. (2019), Delange et al. (2021) have classiﬁed the existing continual learning approaches 829
on the basis of the mechanisms for mitigating catastrophic forgetting into three main categories, 830
namely: (i)parameter isolation based approaches, (ii)regularization based approaches, and (iii) 831
rehearsal / memory-replay based approaches. In this paper, we have followed this same classiﬁcation 832
to classify the existing CL approaches into one of those three classes. 833
•Memory Based. Whether a method uses memory-replay or not to enable continual learning. 834
D Importance Of Streaming Learning 835
Importance of ‘class-incremental streaming learning’ (CISL) or ‘streaming learning’ (SL) can be described 836
as follows: 837
•It enables practical deployment of the AI agents in real world scenarios, where an AI agent might 838
need to learn from as few as a single (training) example without suﬀering from the catastrophic 839
forgetting. For example, consider an autonomous car might meet with a rare incident/accident, 840
then it could be lifesaving if it can be trained continuously with that single example without any 841
forgetting. It would be impractical, if not infeasible, to wait and aggregate a batch of samples to 842
train the autonomous agent, as we may not collect a batch of such examples due to its rare nature. 843
Hayes et al. (2019b) refered to streaming learning as the closest alternative to the biological learning 844
than the other existing lifelong learning approaches, due to the fact that it enables continual learning 845
from a single example with no forgetting. 846
•IBL methods assume the data available in batches and can visit the data multiple times to enable 847
CL. While it can be applicable in a static environment, it lacks the applicability in a rapidly chang- 848
ing dynamic environment, where a learner needs to adapt quickly in a single pass, such that, it 849
achieves ‘any-time-inference’ ability. Although, the existing online learning approaches aim to en- 850
able continual learning in a dynamic environment from a non-stationary data-stream, these methods 851
have number of limitations, such as: (i)require batch-size, (ii)require ﬁne-tuning before each infer- 852
ence, (iii)require large replay buﬀer, etc., which limits their applicability in a restrictive streaming 853
lifelong learning. Streaming learning approaches addresses the limitations of the existing IBL and 854
online learning approaches and enables lifelong learning following various constraints: (i)single pass 855
24Under review as submission to TMLR
learning, (ii)subset buﬀer replay, (iii)tiny replay buﬀer, etc. It further enables ‘any-time-inference’ 856
ability in a continual learner, which enables practical applicability in real world scenarios. Finally, 857
it also enables lifelong learning from a temporally coherent video sequences (images), which are nat- 858
urally non-i.i.d images. Please refer to Section 2 in REMIND (Hayes et al., 2019b) paper that has 859
deﬁned this problem formulation to emphasize that we have not invented a new problem formulation, 860
it was already existing. 861
E Baselines And Compared Methods In Detail 862
The proposed approach (BISLERi) follows ‘class-incremental streaming learning’ setup, to the best of our 863
knowledge, recent works ExStream (Hayes et al., 2019a), and REMIND (Hayes et al., 2019b) are the only 864
methodthattrainsa deep neural network followingthesamelearningsetting. WecomparedBISLERiagainst 865
these strong baselines. In addition, we have compared various ‘batch’ and ‘online’ learning methods, which 866
we describe below. 867
For a fair comparison, we follow a similar network structure throughout all the methods. We separate a 868
convolutionalneuralnetwork(CNN)intotwonetworks: (i)non-plastic featureextractor G(·), and (ii)plastic 869
neural network F(·). For a given input image x, the predicted class label is computed as: y=F(G(x)). 870
Across all the methods, we use the same initialization step for the feature extractor G(·)(discussed in 871
Section 3.5) and keep it frozen throughout the streaming learning . For all the methods, only the plastic 872
networkF(·)is trained with one sample at a time in streaming manner . For details on the structure of the 873
plastic network F(·)across baselines along with BISLERi, refer to Section 5.5. 874
In the below, we describe the baselines which we have evaluated along with BISLERi in class-incremental 875
streaming setting : 876
1.EWC(Kirkpatrick et al., 2017) :It is a regularization-based incremental learning method, which 877
penalizes any changes to the network parameters by the important weight measure, the diagonal of 878
the Fisher information matrix. 879
2.MAS(Aljundi et al., 2018a) :It is another regularization-based lifelong learning method, where 880
the importance weight of the network parameters are estimated by measuring the magnitude of the 881
gradient of the learned function. 882
3.VCL(Nguyen et al., 2017) :It uses variational inference (VI) with a Bayesian neural network (Neal, 883
2012; Jospin et al., 2020) to mitigate catastrophic forgetting, where it uses the previously learned 884
posterior as the prior while learning incrementally with the sequentially coming data. For more 885
details, please refer to Section A.2. 886
4.Coreset VCL (Nguyen et al., 2017) :This method is the same as the pure VCL as mentioned 887
above, except, at the end of training on each task, the network is ﬁnetuned with the coreset samples. 888
We adapted the coreset selection in streaming setting and stored data points in coreset in an online 889
manner; before inference, the network is ﬁne-tuned with the coreset samples. 890
5.Coreset Only (Farquhar & Gal, 2018) :This method is exactly similar to Coreset VCL (Nguyen 891
et al., 2017), except the prior which is used for variational inference is the initial prior each time, 892
i.e., it is not updated with the previous posterior before training on a new task. 893
6.GDumb (Prabhu et al., 2020) :It is an online learning method. It stores data points with a greedy 894
sampler and retrains the network from scratch each time with stored samples before inference. 895
7.AGEM (Chaudhry et al., 2018b) :It is another online learning approach. It uses past task data 896
stored in memory to build an optimization constraint to be satisﬁed by each new update. If the 897
gradient violates the constraint, then it is projected such that the constraint is satisﬁed. 898
8.DER(Buzzega et al., 2020) :It stores the past logits in memory and matches them while learning 899
on the new data. 900
25Under review as submission to TMLR
9.DER++ (Buzzega et al., 2020) :It stores the past task data points along with the corresponding 901
logits in memory. It uses memory-replay and knowledge-distillation to enable continual learning 902
while learning on the new data. 903
10.TinyER (Chaudhry et al., 2019) :It stores past task data points in a tiny episodic memory and 904
replays them with the current training data to enable continual learning. 905
11.ExStream (Hayes et al., 2019a) :It is a streaming learning method, which uses memory replay to 906
enable continual learning. It maintains buﬀers of prototypes to store the input vectors. Once the 907
buﬀer is full, it combines the two nearest prototypes in the buﬀer and stores the new input vector. 908
12.REMIND (Hayes et al., 2019b) :Similar to ExStream, it is another streaming learning method, 909
which enables lifelong learning with memory replay. For more details on REMIND, please refer to 910
Section A.3. 911
13.Fine-tuning: It is a streaming learning baseline and serves as the lower bound on the network’s 912
performance. In this scenario, the network parameters are ﬁne-tuned with one instance through the 913
whole dataset for a single epoch. 914
14.Oﬄine: It serves as the upper bound on the network’s performance, where the network is trained in 915
the traditional way, that is, the complete dataset is divided into multiple batches, and the network 916
loops over them multiple times. 917
Note:Instreaming learning , ﬁne-tuning the network parameters at any time is prohibited (refer Section 2), 918
and thus, any method which uses ﬁne-tuning has an extra advantage over the true streaming learning 919
methods, and cannot be considered as the best performing method even when they are achieving the best 920
accuracy. Coreset VCL (Nguyen et al., 2017), Coreset Only (Farquhar & Gal, 2018) and GDumb (Prabhu 921
et al., 2020), however, requires ﬁne-tuning before each inference, therefore, we do not consider these methods 922
as the best performing method over the true streaming learning methods. 923
26Under review as submission to TMLR
Bananas
Bottles
Boxes
Bread
Cans
Lemons
Pears
Peppers
Potatoes
Yogurt
Figure 7: The iCubWorld 1.0(Fanello et al., 2013) dataset. 10categories: Bananas, Bottles, Boxes, Bread,
Cans, Lemons, Pears, Peppers, Potatoes, Yogurt. Each category contains 3diﬀerent instances.
27Under review as submission to TMLR
Figure 8: Example images of the 50 object instances in CORe50 (Lomonaco & Maltoni, 2017). Each column
denotes one of the 10 categories.
F Ablation Study Additional Results 924
In this section, we provide additional results for the ablation studies. 925
•CIFAR10/100. Table 5 compares the ﬁnal Ωallaccuracy of the proposed model (BISLERi) for (i) 926
i.i.d and (ii)class-i.i.d ordering on CIFAR10 and CIFAR100 respectively while using diﬀerent values 927
for the knowledge-distillation hyper-parameter λ2, and diﬀerent memory replacement policies and 928
various sample selection strategies. 929
•iCubWorld 1.0. Table 6 and Table 7 compares the ﬁnal Ωallaccuracy of BISLERi for (i)i.i.d, 930
(ii)class-i.i.d, (iii)instance, and (iv)class-instance ordering on iCubWorld 1.0 dataset while using 931
diﬀerent knowledge-distillation hyper-parameter λ2and diﬀerent sampling strategies. For memory 932
replacement policy, Table 6 uses ‘loss-aware weighted class balancing replacement (LAWCBR)’ strat- 933
egy, whereas Table 7 uses ‘loss-aware weighted random replacement with a reservoir (LAWRRR)’ 934
strategy. 935
•CORe50. In Table 8, we compare the ﬁnal Ωallaccuracy of BISLERi for (i)i.i.d, (ii)class-i.i.d, 936
(iii)instance, and (iv)class-instance ordering on CORe50 dataset while using diﬀerent memory 937
replacement policy and past sample selection strategies. 938
28Under review as submission to TMLR
Table 5: ΩallResults as a function of knowledge-distillation hyper-parameter λ2and diﬀerent memory
replacement policies and sample selection strategies for (i)i.i.d ordering, and (ii)class-i.i.d ordering on
CIFAR10 and CIFAR100 datasets.
λ2Memory
ReplacementSample
Selectioniid class-iid
CIFAR10 CIFAR100 CIFAR10 CIFAR100
0.2LAWCBRUni 0.9542 ±0.0053 0.8135 ±0.0054 0.8942 ±0.0062 0.7343 ±0.0131
UAPN 0.9084 ±0.0121 0.4760 ±0.0136 0.8957 ±0.0125 0.6448 ±0.0257
LAPN 0.8462 ±0.0414 0.3834 ±0.0335 0.8797 ±0.0149 0.5332 ±0.0310
LAWRRRUni 0.9584 ±0.0035 0.8617 ±0.0091 0.8792 ±0.0104 0.7221 ±0.0149
UAPN 0.9567 ±0.0031 0.8366 ±0.0107 0.8978 ±0.0107 0.7589 ±0.0185
LAPN 0.9530 ±0.0037 0.8273 ±0.0141 0.8986 ±0.0127 0.7478 ±0.0191
0.3LAWCBRUni 0.9529 ±0.0062 0.8134 ±0.0077 0.8970 ±0.0088 0.7369 ±0.0106
UAPN 0.9145 ±0.0071 0.5096 ±0.0088 0.8944 ±0.0093 0.6836 ±0.0231
LAPN 0.9046 ±0.0152 0.4376 ±0.0220 0.8798 ±0.0230 0.6275 ±0.0291
LAWRRRUni 0.9579 ±0.0040 0.8679 ±0.0057 0.8838 ±0.0088 0.7307 ±0.0122
UAPN 0.9567 ±0.0031 0.8542 ±0.0066 0.8991 ±0.0089 0.7724 ±0.0188
LAPN 0.9538 ±0.0044 0.8453 ±0.0120 0.9024 ±0.0116 0.7573 ±0.0193
Table 6: ΩallResults as a function of knowledge-distillation hyper-parameter λ2, and‘loss-aware weighted
class balancing replacement’ (LAWCBR) and diﬀerent sampling strategies for (i)i.i.d, (ii)class-i.i.d, (iii)
instance, and (iv)class-instance ordering on iCubWorld 1.0 dataset.
λ2Sample
SelectioniCubWorld 1.0
iid Class-iid Instance Class-instance
0.0Uni 0.9431 ±0.0418 0.9105 ±0.0333 0.8414 ±0.0541 0.8259 ±0.0316
UAPN 0.8775 ±0.0753 0.8863 ±0.0529 0.6777 ±0.0764 0.7711 ±0.0574
LAPN 0.8975 ±0.0697 0.8675 ±0.0498 0.7576 ±0.0739 0.7524 ±0.0655
0.1Uni 0.9885 ±0.0245 0.9163 ±0.0237 0.9257 ±0.0299 0.8369 ±0.0329
UAPN 0.9781 ±0.0318 0.9167 ±0.0263 0.9124 ±0.0525 0.8627 ±0.0285
LAPN 0.9779 ±0.0206 0.9224 ±0.0332 0.8988 ±0.0544 0.8543 ±0.0288
0.2Uni 0.9841 ±0.0178 0.9154 ±0.0217 0.9219 ±0.0333 0.8454 ±0.0283
UAPN 0.9868 ±0.0181 0.9293 ±0.0306 0.9152 ±0.0229 0.8712 ±0.0266
LAPN 0.9645 ±0.0189 0.9310 ±0.0227 0.9030 ±0.0503 0.8516 ±0.0400
0.3Uni 0.9777 ±0.0264 0.9257 ±0.0288 0.8975 ±0.0454 0.8506 ±0.0310
UAPN 0.9868 ±0.0125 0.9309 ±0.0355 0.9346 ±0.0395 0.8500 ±0.0363
LAPN 0.9745 ±0.0174 0.9352 ±0.0266 0.9172 ±0.0373 0.8536 ±0.0343
0.4Uni 0.9782 ±0.0200 0.9278 ±0.0295 0.9112 ±0.0327 0.8377 ±0.0292
UAPN 0.9815 ±0.0178 0.9160 ±0.0464 0.8988 ±0.0419 0.8509 ±0.0350
LAPN 0.9718 ±0.0271 0.9325 ±0.0401 0.9243 ±0.0512 0.8499 ±0.0650
0.5Uni 0.9742 ±0.0183 0.8858 ±0.1505 0.9341 ±0.0350 0.7787 ±0.2008
UAPN 0.9692 ±0.0197 0.8587 ±0.2278 0.9082 ±0.0758 0.7914 ±0.2033
LAPN 0.9725 ±0.0184 0.9006 ±0.0635 0.9129 ±0.0467 0.8357 ±0.0334
29Under review as submission to TMLR
Table 7: ΩallResults as a function of knowledge-distillation hyper-parameter λ2, and‘loss-aware weighted
random replacement with a reservoir’ (LAWRRR)anddiﬀerentsamplingstrategiesfor (i)i.i.d, (ii)class-i.i.d,
(iii)instance, and (iv)class-instance ordering on iCubWorld 1.0 dataset.
λ2Sample
SelectioniCubWorld 1.0
iid Class-iid Instance Class-instance
0.0Uni 0.9298 ±0.0329 0.9063 ±0.0396 0.8837 ±0.0544 0.9168 ±0.0312
UAPN 0.9184 ±0.0379 0.8818 ±0.0396 0.7507 ±0.0732 0.8384 ±0.0675
LAPN 0.9285 ±0.0357 0.8912 ±0.0430 0.7735 ±0.0458 0.8657 ±0.0521
0.1Uni 0.9830 ±0.0207 0.9240 ±0.0276 0.9292 ±0.0344 0.9162 ±0.0255
UAPN 0.9644 ±0.0260 0.9368 ±0.0228 0.9439 ±0.0362 0.9411 ±0.0224
LAPN 0.9541 ±0.0280 0.9402 ±0.0368 0.9241 ±0.0401 0.9345 ±0.0235
0.2Uni 0.9600 ±0.0312 0.9351 ±0.0315 0.9155 ±0.0299 0.9229 ±0.0284
UAPN 0.9640 ±0.0236 0.9415 ±0.0307 0.9254 ±0.0331 0.9468 ±0.0273
LAPN 0.9684 ±0.0160 0.9382 ±0.0361 0.9368 ±0.0376 0.9454 ±0.0263
0.3Uni 0.9716 ±0.0141 0.9118 ±0.0344 0.9269 ±0.0383 0.9346 ±0.0191
UAPN 0.9454 ±0.0239 0.9480 ±0.0215 0.9580 ±0.0298 0.9585 ±0.0223
LAPN 0.9667 ±0.0174 0.9538 ±0.0303 0.9558 ±0.0304 0.9497 ±0.0239
0.4Uni 0.9611 ±0.0153 0.9243 ±0.0524 0.9350 ±0.0319 0.9222 ±0.0403
UAPN 0.9647 ±0.0257 0.9387 ±0.0315 0.9476 ±0.0264 0.9005 ±0.1257
LAPN 0.9615 ±0.0194 0.9323 ±0.0421 0.9257 ±0.0212 0.9509 ±0.0323
0.5Uni 0.9615 ±0.0301 0.9391 ±0.0268 0.9001 ±0.0555 0.9145 ±0.0649
UAPN 0.9526 ±0.0179 0.9390 ±0.0267 0.9275 ±0.0322 0.8766 ±0.2320
LAPN 0.9495 ±0.0215 0.9085 ±0.1230 0.9369 ±0.0187 0.9533 ±0.0248
Table 8: ΩallResults as a function of diﬀerent memory replacement policies and sample selection strategies
for(i)i.i.d, (ii)class-i.i.d, (iii)instance, and (iv)class-instance ordering on CORe50.
Memory
ReplacementSample
SelectionCORe50
iid Class-iid instance Class-instance
LAWCBRUni 1.0069 ±0.0058 0.9686 ±0.0122 0.8644 ±0.0237 0.7913 ±0.0434
UAPN 1.0079 ±0.0047 0.8890 ±0.1965 0.8976 ±0.0189 0.7835 ±0.0701
LAPN 1.0065 ±0.0045 0.8871 ±0.2356 0.8888 ±0.0198 0.6979 ±0.2043
LAWRRRUni 0.9974 ±0.0075 0.9146 ±0.0275 0.9711 ±0.0077 0.9093 ±0.0222
UAPN 0.9935 ±0.0050 0.9200 ±0.0408 0.9824 ±0.0090 0.9384 ±0.0130
LAPN 0.9933 ±0.0056 0.9101 ±0.0538 0.9835 ±0.0046 0.8932 ±0.0671
30Under review as submission to TMLR
G ImageNet-100 939
In this paper, we used a subset of ImageNet-1000 (ILSVRC-2012) (Russakovsky et al., 2015) that contains 940
randomly chosen 100 classes. To ease a relevant study, we release the list of these 100 classes that we used 941
to evaluate the streaming learner’s performance in our experiments, as mentioned in Table 9. 942
Table 9: The list of classes from ImageNet-100, which are randomly chosen from the original ImageNet-1000
(ILSVRC-2012) (Russakovsky et al., 2015).
List Of ImageNet-100 Classes
n01632777 n01667114 n01744401 n01753488
n01768244 n01770081 n01798484 n01829413
n01843065 n01871265 n01872401 n01981276
n02006656 n02012849 n02025239 n02085620
n02086079 n02089867 n02091831 n02094258
n02096294 n02100236 n02100877 n02102040
n02105251 n02106550 n02110627 n02120079
n02130308 n02168699 n02169497 n02177972
n02264363 n02417914 n02422699 n02437616
n02483708 n02488291 n02489166 n02494079
n02504013 n02667093 n02687172 n02788148
n02791124 n02794156 n02814860 n02859443
n02895154 n02910353 n03000247 n03208938
n03223299 n03271574 n03291819 n03347037
n03445777 n03529860 n03530642 n03602883
n03627232 n03649909 n03666591 n03761084
n03770439 n03773504 n03788195 n03825788
n03866082 n03877845 n03908618 n03916031
n03929855 n03954731 n04009552 n04019541
n04141327 n04147183 n04235860 n04285008
n04286575 n04328186 n04347754 n04355338
n04423845 n04442312 n04456115 n04485082
n04486054 n04505470 n04525038 n07248320
n07716906 n07730033 n07768694 n07836838
n07860988 n07871810 n11939491 n12267677
H Average Accuracy (µall) 943
In the main paper, we use Ωallmetric to compare the performance of streaming learners across datasets 944
and data-orderings. However, it can hide the raw performance, since it provides a relative performance with 945
respect to an Oﬄine model. Therefore, in this section, we provide average accuracy metric over all testing 946
events, similar to (Rebuﬃ et al., 2017; Hayes et al., 2019b; Kemker et al., 2018): 947
µall=1
TT/summationdisplay
t=1αt (11)
whereTis the total number of testing events, and αtis the accuracy of the streaming learner at time t. 948
In Table 14, we provide µallresults with their associated standard-deviations (for corresponding Ωallresults, 949
refer Table 3) comparing the performance of BISLERi (Ours) and other baselines in diﬀerent streaming 950
learning scenarios on diﬀerent datasets. We do not consider GDumb (Prabhu et al., 2020) as the best 951
performing model even when it achieves higher accuracy on several datasets on class-i.i.d ordering, since it 952
violates the streaming learning constraints (refer Section 2). 953
31Under review as submission to TMLR
Figure 10, 11, 12 plots the accuracy (αt)of BISLERi (Ours) and other baselines on (i)CIFAR10, (ii) 954
CIFAR100, (iii)ImageNet100 dataset. Figure 13 is the (partial) zoom-in version of the plot in Figure 3 955
comparing the accuracy (αt)of BISLERi (Ours) and other baselines on iCubWorld 1.0 & CORe50 datasets. 956
I Ablation Study: µallResults As A Function Of Feature-Extractor 957
In this section, we compare the performance of various baselines (in Table 10) for (i)class-instance and (ii) 958
instance ordering on iCubWorld 1.0 using a feature extractor trained with (i)supervised image-classiﬁcation 959
loss, and (ii)self-supervised loss. For this experiment, we have used ResNet-50 (He et al., 2016) as the 960
base architecture of the feature-extractor. In supervised setup, ResNet-50 is trained with cross-entropy loss, 961
whereas in self-supervised setup, ResNet-50 is trained with momentum contrastive loss (MoCoV2 (Chen 962
et al., 2020)). 963
It can be observed that in both ordering the ﬁnal accuracy drops across streaming learning methods including 964
BISLERi (Ours), if we use a feature-extractor trained with self-supervised loss. However, it is worth men- 965
tioning that BISLERi (Ours) still achieves superior performance compared to the other streaming learning 966
baselines. 967
Table10:µallresultsasafunctionoffeature-extractortrainedwith (i)supervisedlossand (ii)self-supervised
loss on iCubWorld 1.0.
Note: Methods in Red use ﬁne-tuning, implying that these methods violate streaming learning (SL) con-
straints and have an extra advantage over true streaming learning (SL) methods, such as ‘Ours’.
MethodClass-instace instace
Resnet50
SupervisedResnet50
Self-Supervised (MoCoV2)Resnet50
SupervisedResnet50
Self-Supervised (MoCoV2)
Fine tune 0.3282±0.0003 0.3276±0.0008 0.1000±0.0000 0.1000±0.0002
VCL 0.3281±0.0003 0.3269±0.0014 - -
Coreset VCL 0.4331±0.0296 0.3949±0.0309 - -
GDumb 0.7455±0.0219 0.4623±0.0638 0.5442±0.0475 0.1821±0.0274
AGEM 0.3280±0.0006 0.3277±0.0008 0.1000±0.0000 0.1000±0.0000
TinyER 0.7369±0.0282 0.6859±0.0273 0.6503±0.0302 0.6331±0.0363
ExStream 0.7919±0.0200 0.6863±0.0305 0.6470±0.0325 0.6026±0.0319
REMIND 0.6754±0.0293 0.6584±0.0258 0.6114±0.0391 0.5355±0.0750
Ours 0.8303±0.0313 0.7596±0.0419 0.6926±0.0120 0.6872±0.0207
Oﬄine 0.8676 0.8368 0.7311 0.7741
J Ablation Study: µallResults As A Function Of Learning-Rate 968
In this section, we compare the performance ( µallResults) of the baselines including BISLERi (Ours) (in 969
Table 11) as a function of learning rate (lr) across: (i)class-instance and (ii)instance ordering on iCubWorld 970
1.0 dataset. We can observe that BISLERi (Ours) achieves superior performance compared to the baselines 971
while trained with diﬀerent learning rates. 972
Table 11:µallresults as a function of learning rate (lr) on iCubWorld 1.0.
Note: Methods in Red use ﬁne-tuning, implying that these methods violate streaming learning (SL) con-
straints and have an extra advantage over true streaming learning (SL) methods, such as ‘Ours’.
MethodClass-instace instace
lr = 0.01 lr = 0.001 lr = 0.003 lr = 0.01 lr = 0.001 lr = 0.003
Fine tune 0.3258±0.0022 0.3264±0.0039 0.3267±0.0013 0.1000±0.0000 0.1121±0.0265 0.1070±0.0221
VCL 0.3246±0.0024 0.3242±0.0016 0.3248±0.0020 - - -
Coreset VCL 0.4319±0.0148 0.4316±0.0161 0.4288±0.0157 - - -
GDumb 0.7077±0.0293 0.6983±0.0273 0.6989±0.0255 0.5134±0.0413 0.4646±0.0732 0.5332±0.0408
TinyER 0.7346±0.0287 0.7177±0.0157 0.7082±0.0172 0.6672±0.0496 0.6982±0.0392 0.7071±0.0251
ExStream 0.7740±0.0198 0.7842±0.0171 0.7848±0.0149 0.6846±0.0414 0.6964±0.0232 0.7138±0.0160
REMIND 0.6843±0.0270 0.6783±0.0227 0.6597±0.0206 0.6237±0.0459 0.6582±0.0358 0.6518±0.0411
Ours 0.8497±0.0191 0.8416±0.0262 0.8458±0.0186 0.7325±0.0228 0.7141±0.0271 0.7280±0.0268
Oﬄine 0.8840 0.8877 0.8912 0.7646 0.7681 0.7551
32Under review as submission to TMLR
K Ablation Study: GPU Memory And Computation Time Requirements For The 973
Tested Methods 974
In this section, we provide: (i)gpu memory consumption, and (ii)total time required by the corresponding 975
method for streaming learning (in Table 12) on iCubWorld 1.0 for class-instance ordering. 976
Table 12: Comparison of baselines w.r.t (i)gpu memory consumption, and (ii)total time required in
streaming learning on iCubWorld 1.0 for class-instance ordering.
Method GPU Memory Consumption (Mb) Time
Fine-Tune 933 34m 03s
EWC 933 41m 23s
MAS 933 54m 16s
VCL 933 53m 25s
DER/DER++ 933 61m 25s
TinyER 933 54m 19s
REMIND 933 63m 21s
Ours 986 73m 25s
It can be observed that the gpu memory consumption and required time to train in streaming learning, is 977
not signiﬁcantly higher than the other baselines. However, it results in superior performance compared to 978
the baselines. 979
L Ablation Study: µallResults As A Function Of Hyperparameters ( α&β) On 980
DER/DER++ 981
While in the main paper, we use hyperparameters: α=β= 0.5, to evalaute the performance of 982
DER/DER++ (Buzzega et al., 2020), in this section (Figure 9), we compare the performance (µall)of 983
DER/DER++ w.r.t the diﬀerent values of hyperparameters: αandβ. and learning rate (lr) on class-instace 984
ordering on iCubWorld 1.0. 985
0.0 0.1 0.2 0.3 0.4 0.5
β0.0 0.1 0.2 0.3 0.4 0.5α0.3286 0.3759 0.4515 0.5312 0.5576 0.6043
0.4883 0.5244 0.4993 0.5018 0.5172 0.5368
0.4077 0.4783 0.5121 0.5530 0.4949 0.5240
0.4560 0.4637 0.4519 0.4877 0.5096 0.4490
0.4579 0.3940 0.4875 0.4947 0.5312 0.5176
0.3787 0.4913 0.4537 0.4527 0.4786 0.4141lr = 0.01
0.0 0.1 0.2 0.3 0.4 0.5
β0.0 0.1 0.2 0.3 0.4 0.5α0.3246 0.3269 0.3791 0.4406 0.5122 0.5718
0.3468 0.3275 0.3273 0.5502 0.5748 0.3940
0.3393 0.3280 0.3511 0.3832 0.3935 0.3992
0.3283 0.3467 0.3819 0.3861 0.4074 0.4344
0.3276 0.3831 0.3928 0.4149 0.4352 0.4744
0.3261 0.3982 0.4188 0.4262 0.4520 0.4796lr = 0.001
0.350.400.450.500.550.60
0.350.400.450.500.55
Figure 9:µallresults as a function of hyperparameters ( α&β) and learning rate (lr) on DER/DER++.
33Under review as submission to TMLR
Table 13:µallresults as a function of hyperparameters (α&β)and learning rate (lr) on DER/DER++ over
class-instace ordering on iCubWorld 1.0.
DER++ lr = 0.01 lr = 0.001
α= 0.0,β= 0.00.3268±0.0010 0.3246±0.0060
α= 0.0,β= 0.10.3759±0.0148 0.3269±0.0017
α= 0.0,β= 0.20.4515±0.0322 0.3791±0.0154
α= 0.0,β= 0.30.5312±0.0380 0.4406±0.0236
α= 0.0,β= 0.40.5576±0.0516 0.5122±0.0406
α= 0.0,β= 0.50.6043±0.0437 0.5718±0.0354
α= 0.0,β= 0.60.6449±0.0568 0.5745±0.0349
α= 0.0,β= 0.70.6442±0.0704 0.5995±0.0385
α= 0.0,β= 0.80.6817±0.0709 0.6078±0.0396
α= 0.0,β= 0.90.6846±0.0603 0.6127±0.0466
α= 0.0,β= 1.00.7419±0.0523 0.6276±0.0408
Ours 0.8497±0.0191 0.8416±0.0262
Oﬄine 0.8840 0.8877
•Forβ= 0.0andα= 0.0, DER++behavessimilartoFine-Tunemodel(lowerbound). Itsimplyupdatesthe 986
parameters of the network with the gradient computed against the newly available single training example 987
in each incremental step. 988
•Forβ= 0.0andα>0.0, DER++ becomes DER (left most column in both sides in Figure 9), and it uses 989
only knowledge-distillation to mitigate catastrophic forgetting. 990
•Forα= 0.0andβ > 0.0, DER++ behaves similar to a method using only experience replay to mit- 991
igate catastrophic forgetting (top row in both sides in Figure 9). In Table 13, we have compared the 992
performance ( µallResults) of DER++ for α= 0.0,β∈{0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0}, and lr 993
∈{0.01,0.001}. It can be observed that DER++ behaves similar to an experience replay based model with 994
increasing value of β, and achieves the highest ﬁnal accuracy when α= 0.0,β= 1.0, for both learning rates 995
(lr∈{0.01,0.001}). 996
•Forα>0.0andβ >0.0, DER++ (Buzzega et al., 2020) uses both knowledge-distillation and experience 997
replay to circumvent the catastrophic forgetting (French, 1999). 998
M Additional Implementation Details 999
We use Mobilenet-V2 (Sandler et al., 2018) pre-trained on ImageNet (Russakovsky et al., 2015) available in 1000
PyTorch (Paszke et al., 2019) TorchVision package as the base architecture for the feature extractor G(·). 1001
We use the convolutional base of Mobilenet-V2 (Sandler et al., 2018) as the feature extractor G(·)to obtain 1002
embeddings from the raw pixels; we keep it frozen throughout the streaming learning. We use ‘loss-aware 1003
weighted class balancing replacement’ as memory replacement policy and ‘uniform sampling’ strategy to 1004
select informative past samples for training on CORe50 on streaming i.i.d and streaming class-i.i.d ordering. 1005
We provide the parameter settings for the proposed method (BISLERi) and the oﬄine models in Table 15. 1006
34Under review as submission to TMLR
Table 14:µallresults with their associated standard deviations. For each experiment, the method with best
performance in ‘streaming-setting’ is highlighted in Bold. The reported results are average over 10runs
with diﬀerent permutations of the data. Oﬄine model is trained only once. ‘-’ indicates experiments we are
unable to run, because of compatibility issues.
Note: Methods in Red use ﬁne-tuning, implying that these methods violate streaming learning (SL) con-
straints and have an extra advantage over true streaming learning (SL) methods, such as ‘Ours’.
Methodiid Class-iid
CIFAR10 CIFAR100 ImageNet100 CIFAR10 CIFAR100 ImageNet100
Fine-Tune 0.1000±0.0000 0.0109 ±0.0021 0.0108 ±0.0025 0.3250±0.0003 0.1099 ±0.0017 0.1138 ±0.0048
EWC - - - 0.3249±0.0002 0.1111 ±0.0027 0.1139 ±0.0036
MAS - - - 0.3271±0.0070 0.1102 ±0.0022 0.1148 ±0.0043
VCL - - - 0.3245±0.0005 0.1095 ±0.0029 0.1121 ±0.0014
Coreset VCL - - - 0.3488±0.0445 0.1204 ±0.0170 0.1170 ±0.0112
Coreset Only - - - 0.3462±0.0394 0.1216 ±0.0194 0.1182 ±0.0166
GDumb 0.7391±0.0056 0.3690 ±0.0072 0.7124 ±0.0059 0.8324±0.0050 0.5560±0.0069 0.8248±0.0072
AGEM 0.1000±0.0000 0.0111 ±0.0021 0.0118 ±0.0035 0.3251±0.0002 0.1109 ±0.0026 0.1130 ±0.0023
DER 0.1000±0.0000 0.0101 ±0.0002 0.0107 ±0.0023 0.3252±0.0010 0.1101 ±0.0019 0.1132 ±0.0035
DER++ 0.1000±0.0000 0.0105 ±0.0017 0.0111 ±0.0034 0.3374±0.0374 0.1111 ±0.0042 0.1144 ±0.0062
TinyER 0.7925±0.0097 0.4616 ±0.0078 0.8021 ±0.0073 0.8046±0.0138 0.5410 ±0.0137 0.8070 ±0.0108
ExStream 0.7544±0.0208 0.4772 ±0.0074 0.7918 ±0.0070 0.7345±0.0185 0.5239 ±0.0146 0.7854 ±0.0132
REMIND 0.7581±0.0062 0.3928 ±0.0056 0.7743 ±0.0093 0.7962±0.0176 0.4984 ±0.0152 0.7901 ±0.0139
Ours 0.8151±0.0034 0.5279 ±0.0035 0.8213 ±0.0051 0.8099±0.0079 0.5611 ±0.0135 0.8224 ±0.0065
Oﬄine 0.8509 0.6083 0.8520 0.8972 0.7154 0.8953
Methodiid Class-iid instance Class-instance
iCubWorld 1.0 CORe50 iCubWorld 1.0 CORe50 iCubWorld 1.0 CORe50 iCubWorld 1.0 CORe50
Fine-Tune 0.1044±0.0141 0.1000 ±0.0000 0.3625±0.0467 0.3261 ±0.0157 0.1000±0.0000 0.1000 ±0.0000 0.3258±0.0022 0.3212 ±0.0003
EWC - - 0.3539±0.0378 0.3281 ±0.0221 - - 0.3260±0.0033 0.3209 ±0.0007
MAS - - 0.3644±0.0541 0.3212 ±0.0004 - - 0.3259±0.0018 0.3212 ±0.0004
VCL - - 0.3550±0.0466 0.3237 ±0.0114 - - 0.3246±0.0024 0.3203 ±0.0009
Coreset VCL - - 0.3674±0.0487 0.3204 ±0.0018 - - 0.4319±0.0148 0.4366 ±0.0050
Coreset Only - - 0.3711±0.0813 0.3443 ±0.0451 - - 0.4287±0.0226 0.4396 ±0.0032
GDumb 0.6858±0.0315 0.8161 ±0.0106 0.8571±0.0175 0.8842 ±0.0074 0.5134±0.0413 0.6491 ±0.0215 0.7077±0.0293 0.6005 ±0.0233
AGEM 0.1000±0.0000 0.1000 ±0.0000 0.3758±0.0555 0.3238 ±0.0093 0.1001±0.0002 0.1000 ±0.0000 0.3262±0.0029 0.3211 ±0.0004
DER 0.1096±0.0300 0.1000 ±0.0000 0.3779±0.0940 0.3212 ±0.0004 0.2875±0.1838 0.1020 ±0.0063 0.3787±0.1466 0.3102 ±0.0347
DER++ 0.1089±0.0278 0.1000 ±0.0000 0.4143±0.1146 0.3211 ±0.0003 0.3454±0.1919 0.1000 ±0.0000 0.4141±0.2016 0.3211 ±0.0005
TinyER 0.7313±0.0289 0.8739 ±0.0106 0.8062±0.0257 0.8693 ±0.0111 0.6672±0.0496 0.7364 ±0.0229 0.7346±0.0287 0.7715 ±0.0221
ExStream 0.7043±0.0445 0.8597 ±0.0137 0.7839±0.0247 0.7970 ±0.0148 0.6846±0.0414 0.7211 ±0.0258 0.7740±0.0198 0.8044 ±0.0188
REMIND 0.7062±0.0237 0.8675 ±0.0100 0.7623±0.0297 0.8584 ±0.0111 0.6237±0.0459 0.7462 ±0.0215 0.6843±0.0270 0.7152 ±0.0337
Ours 0.7409±0.0107 0.8794 ±0.0050 0.8417±0.0187 0.8793 ±0.0109 0.7325±0.0228 0.8579 ±0.0079 0.8497±0.0191 0.8531 ±0.0117
Oﬄine 0.7626 0.8733 0.8849 0.9070 0.7646 0.8733 0.8840 0.9079
Table 15: Training parameters used for BISLERi and Oﬄine model.
ParametersDatasets
CIFAR10 CIFAR100 ImageNet100 iCubWorld 1.0 CORe50
Optimizer SGD SGD SGD SGD SGD
Learning Rate 0.01 0.01 0.01 0.01 0.01
Momentum 0.9 0.9 0.9 0.9 0.9
Weight Decay 1e-05 1e-05 1e-05 1e-05 1e-05
Hidden Layer [256, 256] [256, 256] [256, 256] [256, 256] [256, 256]
Activation ReLU ReLU ReLU ReLU ReLU
Oﬄine Batch Size 128 128 256 16 256
Oﬄine Epoch 50 50 100 30 50
Buﬀer Capacity 1000 1000 1000 180 100
Train-Set Size 50000 50000 127778 6002 119894
35Under review as submission to TMLR
2 4 6 8 10
Number of Classes0.00.20.40.60.81.0αt
2 4 6 8 10
Number of Classes0.60.70.80.91.0Offline
OursREMIND
TinyERExStream
GDumbEWC
MASVCL
Coreset VCLCoreset Only
DERDER++
AGEMFine-tune
Figure 10: Performance of various incremental learning methods in streaming class iid setting on CIFAR10
dataset. The plot on the right is a (partial) zoom-in version of the left plot. It can be observed that BISLERi
(Ours) remembers earlier classes better than most existing models as examples from new classes arrive in a
streaming fashion. Recall that GDumb cannot be considered as a streaming learning algorithm as it requires
ﬁne-tuning.
102030405060708090100
Number of Classes0.00.20.40.60.81.0αt
102030405060708090100
Number of Classes0.40.60.8Offline
OursREMIND
TinyERExStream
GDumbEWC
MASVCL
Coreset VCLCoreset Only
DERDER++
AGEMFine-tune
Figure 11: Performance of various incremental learning methods in streaming class iid setting on CIFAR100
dataset. The plot on the right is a (partial) zoom-in version of the left plot. It can be observed that
BISLERi (Ours) remembers earlier classes better than existing models as examples from new classes arrive
in a streaming fashion. Recall that GDumb cannot be considered as a streaming learning algorithm as it
requires ﬁne-tuning.
36Under review as submission to TMLR
102030405060708090100
Number of Classes0.00.20.40.60.81.0αt
102030405060708090100
Number of Classes0.70.80.9Offline
OursREMIND
TinyERExStream
GDumbEWC
MASVCL
Coreset VCLCoreset Only
DERDER++
AGEMFine-tune
Figure 12: Performance of various incremental learning methods in streaming class iid setting on Ima-
geNet100 dataset. The plot on the right is a (partial) zoom-in version of the left plot. It can be observed
that BISLERi (Ours) remembers earlier classes better than existing models as examples from new classes
arrive in a streaming fashion. Recall that GDumb cannot be considered as a streaming learning algorithm as
it requires ﬁne-tuning.
2 4 6 8 10
Number of Classes0.50.60.70.80.91.0αtClass-i.i.d
2 4 6 8 10
Number of Classes0.60.70.80.91.0αt
2 4 6 8 10
Number of Classes0.40.60.81.0αtClass-instance
2 4 6 8 10
Number of Classes0.40.60.81.0αtiCubWorld 1.0 CORe50
Offline
OursREMIND
TinyERExStream
GDumbEWC
MASVCL
Coreset VCLCoreset Only
DERDER++
AGEMFine-tune
Figure 13: The patial zoon-in version of the plot in Figure 3. It shows the performance of various incremental
learning models on (i)streaming class-i.i.d (top row) and (ii)streaming class-instance (bottom row) ordering
on iCubWorld 1.0 & CORe50 dataset. The plots suggest BISLERi (Ours) remembers earlier classes better
than most existing algorithms. The performance gain is even more pronounced in streaming class-instance
ordering setting (bottom row) where the baseline incremental learners suﬀer from severe forgetting. Recall
thatGDumb cannot be considered as a streaming learning algorithm as it requires ﬁne-tuning.
37Under review as submission to TMLR
N Evaluation Over Diﬀerent Data Orderings Additional Details 1007
The proposed approach (BISLERi) is robust to various streaming learning scenarios that can induce catas- 1008
trophic forgetting (French, 1999; McCloskey & Cohen, 1989). We evalaute the model’s class-incremental 1009
streaming learning ability with the four challenging data ordering (Hayes et al., 2019b;a) schemes: (i) 1010
‘streaming iid’ ,(ii)‘streaming class iid’ ,(iii)‘streaming instance’ , and (iv)‘streaming class instance’ . We 1011
described this four data ordering schemes in Section 5.1. 1012
Note:Only iCubWorld 1.0 (Fanello et al., 2013), and CORe50 (Lomonaco & Maltoni, 2017) contain the 1013
temporal coherent image sequences, therefore, ‘streaming instance’ ‘streaming class instance’ setting are 1014
evaluated only on this two datasets. 1015
In the below, we describe the following: (i)how the base initialization is performed, and (ii)how the network 1016
is trained in streaming setting according to various data ordering schemes on diﬀerent datasets. 1017
N.1 CIFAR10 1018
CIFAR10 (Krizhevsky et al., 2009) is a standard image classiﬁcation dataset. It contains 10 classes with 1019
each consists of 5000 training images and 1000 testing images. Since, it does not contain any temporally 1020
ordered image sequence, we use CIFAR10 to evaluate the streaming learner’s ability in streaming i.i.d and 1021
streaming class-i.i.d orderings. 1022
•streaming i.i.d: For the base initialization, we randomly select 2%samples from the dataset and 1023
train the model in oﬄine manner. Then we randomly shuﬄe the remaining samples and train the 1024
model incrementally with these samples by feeding one at a time in a streaming manner. 1025
•streaming class-i.i.d: In base initialization, the model is trained in a typical oﬄine mode with 1026
the samples from the ﬁrst two classes. Then, in each incremental step, we select the samples from 1027
the next two classes, which are not included earlier. These samples are randomly shuﬄed and fed 1028
into the model in a streaming manner. 1029
N.2 CIFAR100 1030
CIFAR100 (Krizhevsky et al., 2009) is another standard image classiﬁcation dataset. It contains 100 classes 1031
with each consists of 500 training images and 100 testing images. We use CIFAR100 to evaluate the model’s 1032
ability in streaming i.i.d andstreaming class-i.i.d orderings. 1033
•streaming i.i.d: In this setting, we follow the similar approach as mentioned for the CIFAR10 1034
dataset, with the only exception is that the base initialization is performed with 10%randomly 1035
chosen samples, and the remaining samples are used for streaming learning. 1036
•streaming class-i.i.d: This approach also follows the similar approach as mentioned for the CI- 1037
FAR10 dataset. However, in each incremental step, including the base initialization, we use samples 1038
from 10 classes. For the base initialization, we select samples from the ﬁrst ten classes, and in each 1039
incremental step, we select samples from the succeeding ten classes which are not observed earlier. 1040
N.3 ImageNet100 1041
ImageNet100isasubsetofImageNet-1000(ILSVRC-2012)(Russakovskyetal.,2015)thatcontainsrandomly 1042
chosen 100 classes, with each classes containing 700−1300training samples and 50validation samples. Since, 1043
for test samples, we do not have the ground truth labels, we use the validation data for testing the model’s 1044
accuracy. We provide more details on ImageNet100 in Section G. 1045
We use ImageNet100 dataset to evaluate the model’s ability in streaming i.i.d andstreaming class-i.i.d 1046
orderings. 1047
38Under review as submission to TMLR
•streaming i.i.d: In this case, we follow the similar approach as mentioned for CIFAR100 streaming 1048
i.i.dordering. 1049
•streaming class-i.i.d: We follow the similar approach as has been mentioned for CIFAR100 1050
streaming class-i.i.d ordering. 1051
N.4 iCubWorld 1.0 1052
iCubWorld 1.0 (Fanello et al., 2013) is an object recognition dataset containing the sequence of video frames, 1053
with each frame containing only a single object. It is a more challenging and realistic dataset w.r.t the 1054
other standard datasets such as CIFAR10, CIFAR100, and ImageNet100. Technically, it is an ideal dataset 1055
to evaluate a model’s performance in streaming learning scenarios that are known to induce catastrophic 1056
forgetting (French, 1999; McCloskey & Cohen, 1989), as it requires learning from temporally ordered image 1057
sequences, which are naturally non-i.i.d images. 1058
It contains 10 classes, each with 3 diﬀerent object instances with 200−201images each. Overall, each class 1059
contains 600−602samples for training and 200−201samples for testing. Figure 7 shows example images 1060
of the 30object instances in iCubWorld 1.0, where each row denotes one of the 10categories. 1061
We use iCubWorld 1.0 to evaluate the performance of the streaming learning models in all the four data 1062
orderingschemes, i.e., (i)streaming i.i.d ,(ii)streaming class-i.i.d , (iii)streaming instance , and (iv)streaming 1063
class-instance . 1064
•streaming i.i.d: In this setting, we follow the similar approach as mentioned for the CIFAR10 1065
dataset, with the only exception, that is, 10%randomly selected samples are used for the base 1066
initialization, and the rest are used for streaming learning. 1067
•streaming class-i.i.d: In this case, we follow the same strategy as mentioned for CIFAR10 stream- 1068
ing class-i.i.d ordering. 1069
•streaming class-instance: In base initialization, the model is trained in a typical oﬄine mode 1070
with the samples from the ﬁrst two classes. In each incremental step, the network is trained in a 1071
streaming manner with the samples from the succeeding two classes which were not observed earlier. 1072
However, in this case, (i)samples within a class are temporally ordered based on diﬀerent object 1073
instances, and (ii)all samples from one class are fed into the network before feeding any samples 1074
from the other class. 1075
•streaming instance: For the base initialization, 10%randomly chosen samples are used, and the 1076
remaining samples are used to train the model incrementally with one sample at a time. In streaming 1077
setting, the samples are temporally ordered based on diﬀerent object instances. Speciﬁcally, we 1078
organize the data stream by putting temporally ordered 50frames of an object instance, then we 1079
put temporally ordered 50frames of the second object instance, and so on. In this way, after 1080
putting 50temporally ordered frames from each object instance, we put the next 50temporally 1081
ordered frames of the ﬁrst object instance and follow the earlier approach until all the frames of each 1082
instance have been exhausted. 1083
N.5 CORe50 1084
CORe50 (Lomonaco & Maltoni, 2017), speciﬁcally designed for ContinualObjectRecognition, contains a 1085
collection of 50 domestic object instances belonging to 10 diﬀerent catagories: plug adapters, mobile phones, 1086
scissors, light bulbs, cans, glasses, balls, markers, cups and remote controls. Each object instance contains 1087
2393-2400 sample images for training. Overall, each class contains 11983-12000 samples for training and 1088
4495-4500 samples for testing. Figure 8 shows example images of the 50 object instances in CORe50, where 1089
each column denotes one of the 10 categories. 1090
CORe50 is a challenging dataset, similar to iCubWorld 1.0 (Fanello et al., 2013). It contains temporally 1091
coherent image sequences, which are divided into 11 distinct sessions (8 indoors and 3 outdoors) character- 1092
ized by diﬀerent backgrounds and lighting. Technically, it is also an ideal dataset for streaming learning 1093
39Under review as submission to TMLR
evaluations, aside from iCubWorld 1.0 (Fanello et al., 2013). It can be used to evaluate a model’s robustness 1094
in all four streaming learning scenarios, i.e., i.i.d, class-i.i.d, class-instace and instance ordering. 1095
We use CORe50 to evaluate the performance of the streaming learning models in all four data ordering 1096
schemes, i.e., (i)streaming i.i.d ,(ii)streaming class-i.i.d ,(iii)streaming instance , and (iv)streaming class- 1097
instance ordering. 1098
•streaming i.i.d: In this case, we follow the similar approach as mentioned for iCubWorld 1.0 1099
streaming i.i.d ordering. 1100
•streaming class-i.i.d: We follow the similar approach as has been mentioned for iCubWorld 1.0 1101
streaming class-i.i.d ordering. 1102
•streaming class-instance: In this case, we follow the similar strategy as we use for iCubWorld 1103
1.0streaming class-instance ordering. 1104
•streaming instance: For base initialization, 10%randomly chosen samples are used, and the 1105
remaining samples are used to train the model incrementally with one sample at a time. In streaming 1106
setting, the samples are temporally ordered based on diﬀerent sessions and diﬀerent object instances. 1107
Speciﬁcally, the dataset is divided into 11 sessions depending on diﬀerent backgrounds and lighting 1108
with each session containing temporally coherent image of from various object instances one after 1109
another. We use the data-ordering as provided in paths.pkl ﬁle with CORe50 dataset for training 1110
with remaining samples in the streaming manner. 1111
O Derivation of Joint Posterior 1112
L1
t(θ) = arg min
q/epsilon1QKL/bracketleftbigg
qt(θ)||1
Ztqt−1(θ)p(Dt|θ)p(DM,t|θ)/bracketrightbigg
warg min
q/epsilon1QKL[qt(θ)||qt−1(θ)p(Dt|θ)p(DM,t|θ)]
=/integraldisplay
qt(θ) logqt(θ)
qt−1(θ)p(Dt|θ)p(DM,t|θ)dθ
=−/integraldisplay
qt(θ) logqt−1(θ)p(Dt|θ)p(DM,t|θ)
qt(θ)dθ
= arg max/integraldisplay
qt(θ) logqt−1(θ)p(Dt|θ)p(DM,t|θ)
qt(θ)dθ
=/integraldisplay
qt(θ) logp(Dt|θ)p(DM,t|θ)dθ+/integraldisplay
qt(θ) logqt−1(θ)
qt(θ)dθ
=/integraldisplay
qt(θ) logp(Dt|θ)dθ+/integraldisplay
qt(θ) logp(DM,t|θ)dθ−/integraldisplay
qt(θ) logqt(θ)
qt−1(θ)dθ
=Eθ∼qt(θ)[logp(Dt|θ)] +Eθ∼qt(θ)[logp(DM,t|θ)]−KL[qt(θ)||qt−1(θ)]
=Eθ∼qt(θ)[logp(Dt|θ)] +Eθ∼qt(θ)[logp(DM,t|θ)]−λ1KL[qt(θ)||qt−1(θ)]
= arg max Eθ∼qt(θ)[logp(yt|θ,G(xt))] +N/prime
1/summationdisplay
n=1Eθ∼qt(θ)/bracketleftBig
logp(y(n)
M,t|θ,z(n)
M,t)/bracketrightBig
−λ1·KL(qt(θ)||qt−1(θ))
(12)
where: (i)Dt={dt}={(xt,yt)},(ii)DM,t={d(n)
M,t}N/prime
1
n=1=/braceleftBig
(z(n)
M,t,y(n)
M,t)/bracerightBigN/prime
1
n=1,(iii)|DM,t|=N/prime
1/lessmuch|M|, 1113
(iv)DM,t⊂M, and (v)λ1is a hyper-parameter. 1114
40