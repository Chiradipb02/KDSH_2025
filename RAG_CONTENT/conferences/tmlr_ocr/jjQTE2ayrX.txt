Under review as submission to TMLR
PAC Privacy Preserving Diffusion Models
Anonymous authors
Paper under double-blind review
Abstract
Data privacy protection is garnering increased attention among researchers. Diffusion mod-
els (DMs), particularly with strict differential privacy, can potentially produce images with
both high privacy and visual quality. However, challenges arise such as in ensuring robust
protection in privatizing specific data attributes, areas where current models often fall short.
To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a
model leverages diffusion principles and ensure Probably Approximately Correct (PAC) pri-
vacy. We enhance privacy protection by integrating a private classifier guidance into the
Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of
models, we have developed a novel metric to gauge privacy levels. Our model, assessed with
this new metric and supported by Gaussian matrix computations for the PAC bound, has
shown superior performance in privacy protection over existing leading private generative
models according to benchmark tests.
1 Introduction
Modern deep learning models, fortified with differential privacy as defined by Dwork et al. (2006), have
been instrumental in significantly preserving the privacy of sensitive data (Dwork et al., 2014). DP-SGD
Abadi et al. (2016), a pioneering method for training deep neural networks within the differential privacy
framework, applies gradient clipping at each step of the SGD (stochastic gradient descent) to enhance privacy
protection effectively.
Diffusion models (DMs) (Song & Ermon, 2019; 2020; Dhariwal & Nichol, 2021) have emerged as state-of-
the-art generative models, setting new benchmarks in various applications, particularly in generating high-
quality images. When trained under strict differential privacy protocols, these DMs can produce images that
safeguard privacy while maintaining high visual fidelity. For instance, DPGEN (Chen et al., 2022) leverages
a randomized response technique to privatize the recovery direction in the Langevin MCMC process for
image generation. The images produced by DPGEN are not only visually appealing but also compliant with
differential privacy standards, although its privacy mechanism has been shown to be data-dependent later
on. Moreover, the Differentially Private Diffusion Models (DPDM) (Dockhorn et al., 2022) adapt DP-SGD
and introduce noise multiplicity in the training process of diffusion models, demonstrating that DPDM can
indeed produce high-utility images while strictly adhering to the principles of differential privacy.
While diffusion models integrated with differential privacy (DP) mark a significant advance in privacy-
preserving generative modeling, several challenges and limitations remain.
•Most research on diffusion models with DP has concentrated on the privatization of overall image
features. The need to privatize specific attributes, such as facial expressions in human portraits, has
not been adequately addressed. This oversight suggests a gap in the nuanced application of DP in
generative modeling.
•The core objective of DP is to evaluate the distinguishability between processed and original data.
Yet, a significant hurdle is the difficulty of providing adversarial worst-case proofs, especially when
comparing two distinct datasets. This difficulty presents a substantial barrier to validating the
efficacy of DP methods in practical scenarios.
1Under review as submission to TMLR
•The absence of a robust privacy measurement for models poses a critical challenge. Without a clear
metric, itbecomesproblematictoassessandcomparethedataprivacyprotectionperformanceacross
different models. This lack of standardized evaluation complicates the advancement and adoption
of privacy-preserving techniques in the field.
These issues highlight the need for continued research and development to overcome the current limitations
of diffusion models with DP and to push the boundaries of privacy protection in generative modeling.
Recently, Xiao & Devadas (2023) introduces a novel definition of privacy known as Probably Approximately
Correct (PAC) Privacy, representing a significant evolution in privacy-preserving methodologies. This def-
inition diverges from traditional Differential Privacy (DP) by focusing on the difficulty of reconstructing
data using any measure function, thereby offering broader applicability beyond the adversarial worst-case
scenarios considered by DP. Regarding utility, the necessary perturbation magnitude in PAC Privacy is not
necessarily constrained by DP lower bound of Θ(√
d)for ad-dimensional release. Instead, it could be as low
asO(1)for many practical data processing tasks. This stands in contrast to the input-independent worst-
case information-theoretic lower bound. Furthermore, leveraging PAC privacy comes with a framework that
autonomously determines the minimal noise addition necessary for effective data protection (Xiao & De-
vadas, 2023). This framework simplifies the process of achieving privacy-preserving data handling, making
it more accessible for widespread use in the field of data science and beyond.
To tackle the aforementioned challenges, we have introduced PAC Privacy Preserving Diffusion Models
(P3DM). Drawing from the foundations of DPGEN and harnessing insights from conditional classifier guid-
ance (Dhariwal & Nichol, 2021; Batzolis et al., 2021; Ho & Salimans, 2022), our P3DM incorporates a
conditional private attribute guidance module during the Langevin sampling process. This addition empow-
ers the model to specifically target and privatize certain image attributes with greater precision.
Furthermore, we have crafted a set of privacy evaluation metrics. These metrics operate by measuring
the output class labels of the two nearest neighbor images in the feature space of the Inception V3 model
(Szegedy et al., 2016), using a pretrained classifier. Additionally, we quantify the noise addition Bnecessary
to assure PAC privacy in our model and conduct comparative analyses against the mean L2-norm of Bfrom
various other models.
Through meticulous evaluations that utilize our privacy metrics and benchmarks for noise addition, our
model has proven to offer a superior degree of privacy. It exceeds the capabilities of state-of-the-art (SOTA)
models in this critical aspect, while simultaneously preserving the high quality of synthetic image samples.
These samples remain on par with those produced by the state-of-the-art models, illustrating that our
model does not sacrifice quality for privacy. This achievement underscores our model’s potential to set new
precedents in the domain of privacy-preserving image generation and data protection at large.
Our contributions are summarized as follows:
•We propose the first diffusion model with analysis on its PAC privacy guarantees.
•WeincorporateconditionalprivateclassifierguidanceintotheLangevinSamplingProcess, enhancing
the protection of privacy for specific attributes in images.
•We introduce a new metric that we developed for assessing the extent of privacy provided by models.
•We compute the noise addition matrix to establish the Probably Approximately Correct (PAC)
upper bound and have conducted a comparative analysis of the norm of this matrix across various
models.
•Through extensive evaluations, we demonstrate that our model sets a new standard in privacy
protection of specific attributes, achieving state-of-the-art (SOTA) results, while maintaining image
quality at a level that is comparable to other SOTA models.
2Under review as submission to TMLR
2 Related Work
2.1 Early Works on Differentially Private Image Generation
Image Synthesis with differential privacy has been studied extensively during the research. Recently, there
has been an increased emphasis on utilizing sophisticated generative models to improve the quality of differ-
entially private synthetic data (Hu et al., 2023). Some approaches employ Generative Adversarial Networks
(GANs) (Goodfellow et al., 2014), or GANs that have been trained using the Private Aggregation of Teacher
Ensembles (PATE) framework (Xie et al., 2018; Chen et al., 2020; Harder et al., 2021; Torkzadehmahani
et al., 2019). Other contributions leverage variational autoencoders(VAEs) (Pfitzner & Arnrich, 2022; Jiang
et al., 2022; Takagi et al., 2021), or take advantage of customized architectures (Cao et al., 2021; Harder
et al., 2023). However, there are several limitations for those DP synthesizers: (1) Failure when applying to
high-dimensional data, primarily due to the constraints imposed by discretization. (2) Limited image quality
and lack of expressive generator networks (Cao et al., 2021).
2.2 Differentially Private Diffusion Models
Diffusion models (DMs) (Song & Ermon, 2019; 2020; Dhariwal & Nichol, 2021) , recognized for setting new
standards in image generation, can produce high-quality, privacy-compliant images when trained with dif-
ferential privacy protocols. For example, DPGEN (Chen et al., 2022) employs a data-dependant randomized
response method to privatize the recovery direction in the Langevin MCMC process for image generation.
Furthermore, Differentially Private Diffusion Models (DPDM) (Dockhorn et al., 2022), which adapt DP-
SGD and introduce noise multiplicity, both demonstrate the feasibility of generating visually appealing,
privacy-protective images. Subsequent advancements, including fine-tuning existing models and employing
novel diffusion model architectures, have been made to boost the effectiveness of differentially private image
generation, as detailed in Ghalebikesabi et al. (2023); Lyu et al. (2023). Nevertheless, as previously noted in
the introduction, there remains three key challenges to be addressed. In the following sections, we propose
solutions and methods to tackle these issues.
3 Background
3.1 Differential Privacy
A randomized mechanism Mis said (ε,δ)- differentially private if for any two adjacent datasets DandD′
differing in a single datapoint for any subset of outputs Sas follows (Dwork et al., 2014):
Pr[M(D)∈S]≤eε·Pr[M(D′)∈S] +δ (1)
Here,εis the upper bound on the privacy loss corresponding to M, andδis the probability of violating the
DP constraint.
Differential privacy is a mathematical approach designed to protect individual privacy within datasets. It
offers a robust privacy assurance by enabling data analysis without disclosing sensitive details about any
specific person in the dataset.
3.2 PAC Privacy
Traditional provable privacy methods like Differential Privacy (DP) (Dwork et al., 2014), often require strong
assumptions, which can be precisely computed only in a few simple applications, such as aggregation or linear
queries, and can lead to significant accuracy loss on utility. What’s more, the absence of robust risk quantifi-
cation tools significantly hinders the development and implementation of effective leakage control measures.
Even for perturbation, which is the most popular and straightforward privacy-preserving technique, deter-
mining the minimal noise needed to meet required security parameters remains an unresolved issue for most
practical algorithms (Sridhar et al., 2024). PAC privacy (Xiao & Devadas, 2023) addresses the fundamen-
tal problem of quantifying the relationship among data entropy, disclosure, reconstruction difficulty, and
3Under review as submission to TMLR
overcoming challenges in classic security and privacy regimes for black-box data processing. It allows for
automatic security proof and instance-based worst-case analysis, enabling users to monitor data leakage of
any black-box processing mechanism with confidence through simulation. Furthermore, PAC Privacy also
guarantees simple composition bounds, and the automatic analysis framework can be implemented online
to analyze composite PAC Privacy loss, even with correlated randomness. In terms of utility, different from
DP, the necessary perturbation for PAC Privacy is not lower bounded by Θ(√
d)for ad-dimensional release;
instead the bound can be as low as O(1)for many practical tasks, contrasting with the input-independent
worst-case information-theoretic lower bound.
Definition 3.1. ((δ,ρ,D )PAC Privacy). For a data processing mechanism M, given some data distribution
D, a measure function ρ(.,.), and a finite set X∗, we sayMsatisfies (δ,ρ,D )-PAC Privacy if the following
experiment is impossible: A user generates data Xfrom distribution Dand sendsM(X)to an adversary.
The adversary who knows DandMis asked to return an estimation ˜X∈X∗on X such that with posterior
success probability at least (1−δ),ρ(˜X,X ) = 1.
Definition 3.2. ((∆fδ,ρ,D)PAC Advantage Privacy) Equivalantly, Mcould be defined as (∆fδ,ρ,D)PAC
Advantage Privacy if the posterior advantage measured in f-divergenceDfsatisfies
∆fδ=Df(1δ∥1δρ
o) =δρ
of(δ
δρ
o) + (1−δρ
o)f(1−δ
1−δρ
o), (2)
where (1−δρ
o)represents the optimal prior success rate of recovering X.
1−δρ
o= sup
˜X∈X∗Pr
X∼D/parenleftbig
ρ(˜X,X ) = 1/parenrightbig
, (3)
and1δand1δρ
orepresent two Bernoulli distributions of parameters δandδρ
o, respectively.
The definition above depicts the reconstruction hardness for the attackers to recover the private data dis-
tributionM(X). With a lower bound probability (1−δ), the measure function ρ(.,.)cannot distinguish
the recovery data from the original data. However, the limitation of the naive definition is that the prior
distribution of the public dataset is unknown, resulting in the failure of adversarial inferences.
Definition 3.3. (Mutual Information). For two random variables xandwin some joint distribution, the
mutual information MI(x;w)is defined as
MI(x,w) =H(x)−H(x|w) =DKL(Px,w||Px⊗Pw), (4)
whereDKLdenotes the KL divergence.
Theorem 3.1. For any selected f-divergenceDf, a mechanismM:X∗→Ysatisfies (∆fδ,ρ,D)PAC
Advantage Privacy if
∆fδ=Df/parenleftbig
1δ∥1δρ
o/parenrightbig
≤inf
PWDf/parenleftbig
PX,M(X)∥PX⊗PW/parenrightbig
. (5)
In particular, when we select Dfto be the KL-divergence and PW=PM(X),Msatisfies (∆KLδ,ρ,D)PAC
Advantage Privacy where
∆KLδ=DKL(1δ∥1δρ
o)≤MI/parenleftbig
X;M(X)/parenrightbig
. (6)
Proof.See Appendix A.
In summary,Df/parenleftbig
1δ∥1δρ
o/parenrightbig
quantifies the divergence between optimal a priori and posterior reconstruction,
effectively measuring the difficulty of inference. A higher value of Df/parenleftbig
1δ∥1δρ
o/parenrightbig
signifies greater privacy
leakage. Moreover, since MI/parenleftbig
X;M(X)/parenrightbig
provides an upper bound for Df, a lower value of MI/parenleftbig
X;M(X)/parenrightbig
indicatesstrongerprivacyprotection. Thus, theorem3.1establishesageneralmethodforlinkingthedifficulty
of arbitrary inference to the well-known concept of mutual information. With theorem 3.1, the goal of PAC
privacy is explicit: determining the bound MI/parenleftbig
X;M(X)/parenrightbig
with high confidence.
4Under review as submission to TMLR
3.3 Conditional Diffusion Models
Dhariwal & Nichol (2021) proposed a diffusion model that is enhanced by classifier guidance; it has been
shown to outperform existing generative models. By using true labels of datasets, it is possible to train
a classifier on noisy images xtat various timesteps pϕ(y|xt,t), and then use this classifier to guide the
reverse sampling process ∇xtlogpϕ(y|xt,t). What begins as an unconditional reverse noising process is thus
transformed into a conditional one, where the generation is directed to produce specific outcomes based on
the given labels
pθ,ϕ(xt|xt+1,y) =Zpθ(xt|xt+1)pϕ(y|xt) (7)
WhereZis a normalizing constant. According to unconditional reverse process, which predicts timestep xt
fromxt−1leveraging Gaussian distribution, we have
pθ(xt|xt+1)∼N(µ,Σ) (8)
logpθ(xt|xt+1) =−1
2(xt−µ)TΣ−1(xt−µ) +C (9)
If we assume that logϕp(y|xt)has low curvature compared to Σ−1, then we can approximate logpϕ(y|xt)
using a Taylor expansion around xt=µas
logpϕ(y|xt)≈logpϕ(y|xt)|xt=µ+ (xt−µ)∇xtlogpϕ(y|xt)|xt=µ
= (xt−µ)g+C1(10)
wheregis the gradient of classifier g=∇xtlogpϕ(y|xt)|x=µandC1is the constant.
Therefore, combing Eqn. 9 and 10 gives us
log(pθ(xt|xt+1)pϕ(y|xt))∼N(µ+ Σg,Σ) (11)
The investigation has led to the conclusion that the conditional transition operator can be closely estimated
using a Gaussian. And the Gaussian resembles the unconditional transition operator, with the distinction
that its mean is adjusted by the product of the covariance matrix, Σ, and the vector g. This methodology
allows for the generation of high-quality, targeted synthetic data.
4 Methods
We introduce the PAC Privacy Preserving Diffusion Model (P3DM), which aims to safeguard privacy for
specific attributes, building upon the foundation set by DPGEN (Chen et al., 2022). First, DPGEN injects
Gaussian noise with ˜xi=xi+ziwhereziis sampled from N(0,σ2I)given a sensitive dataset {xi:i=
1,2,...,m}m
i=1, which serves as reconstructing images with noise conditional score networks (Song et al., 2020)
leveraged in DPGEN. Second, after obtaining noisy images, DPGEN utilizes the random response method
as follows, to privatize the training data.:
Pr[H(˜xi) =w] =

eϵ
eϵ+k−1, w =xi
1
eϵ+k−1, w =x′
i∈X\xi(12)
In the equation, X ={xj:max(˜xi−xj)/σj≤β,j∈[m]}(where “max” is over the dimensions of ˜xi−xj),
|X|=k≥2, where the hyperparameter kdenotes the number of selected candidates from mtraining images,
and the sampling probability for each image is given by
p(xi) = exp(−d∞(xi,˜xi,σi))/m/summationdisplay
a=1exp(−d∞(xi,˜xi,σa)) (13)
In other words, the mechanism H(·)consists of 2 steps:
5Under review as submission to TMLR
1. Sampling kimage candidates from {xi}m
i=1, with the sampling probability for each image from
Eqn. 13, to construct set X.
2. Privatize images in Xwith RR from Eqn. 12.
In general, DPGEN privatize images via mechanism MwhereM(xi) = (H◦f). With the method, ˜xi
is designed to point to one of its knearest neighbors with certain probability Pr[H(˜xi) =xr
i], which is
designed to enforce ϵ-differential privacy by privatizing the recovery direction d= (˜xi−xr
i)/σ2
i. Following
perturbation, DPGEN learns an energy score function:
l(θ,σ) =1
2Ep(x)E˜x∼N(x,σ2)/bracketleftig
∥d−∇xlogqθ(˜x)∥2/bracketrightig
(14)
Subsequently, images are synthesized via the Langevin MCMC process.
Since the Gaussian noise is added uniformly to every dataset samples, Pr[H(˜xi) =w]and the set Xin the
RR mechanism do not depend on Gaussian noise Z. Therefore, we can reverse the order of Gaussian noise
addition and the RR mechanism in Algorithm 2.
While DPGEN asserts compliance with stringent ϵ-differential privacy, recent findings Dockhorn et al. (2022)
indicate that DPGEN is, in fact, data-dependent. The RR mechanism M(d)in DPGEN holds validity only
within the perturbed dataset. Specifically, if an element zbelongs to the output set Obut not to the
perturbed dataset d, thenPr[M(d) =O] = 0, which contravenes the differential privacy definition.
4.1 Conditional Private Langevin Sampling
We introduce a method of conditional private guidance within the Langevin sampling algorithm, detailed
in Algorithm 1. This method is designed to protect specific attributes within the original datasets against
adversarial attacks.
Prior to commencing the sampling iterations, it is essential to obtain class labels from a balanced attribute of
the dataset. The necessity for a dataset attribute that possesses an approximately equal number of negative
and positive samples is crucial for training classifiers to achieve exceptional performance. Subsequently, we
select a random label yifromyand fix it to initialize x0with a predetermined distribution, such as the
standard normal distribution.
Drawing on conditional image generation (Dhariwal & Nichol, 2021; Batzolis et al., 2021; Ho & Salimans,
2022), we adapt the model from the vanilla Langevin dynamic samplings with the selected attributes with
conditional guidance
kΣθ(xt−1)∇xt−1logcθ(yn|xt−1) (15)
wherekis the gradient scale that can be tuned according to the performance of the model, Σθ(xt−1)is
the covariance matrix from reverse process Eqn. 8. In Eqn. 15, the term ∇xt−1logcθ(yn|xt−1)directs the
Langevin sampling process toward a specific class label yn, which is sampled prior to the inference.
This private guidance during the inference phase ensures that the synthesized images are protected from
privacy breaches related to designated attributes. At its core, this method involves intentionally modifying
certain generated trajectories by randomly perturbing the image label yn, which is then used by a pretrained
classifier to guide the image generation process. This strategy is aimed at diverting certain image attributes
that we wish to protect. For instance, consider a scenario where an original dataset image depicting Celebrity
A wearing eyewear, a known trait of the celebrity. If our goal is to privatize the attribute of “wearing
glasses,” private classifier guidance can effectively achieve this. When sampling occurs under the influence
of this guidance, the attributes are likely to vary, creating a chance that the resulting synthesized image
based on Celebrity A might be rendered without glasses. This modification effectively protects the attribute
of ’glasses’ from being a consistent element in the generated depictions of Celebrity A. Consequently, the
images generated with this approach offer a higher degree of privacy compared to those produced by the
original DPGEN method.
6Under review as submission to TMLR
Algorithm 1 PAC-Private Conditional Guidance Langevin Dynamics Sampling
Require: class labels y={y1,y2,...yn}from one of balanced dataset attributes;
gradient scale k;{δi}L
i=1,ϵ,T;
pretrained scorenet model sθ;
pretrained classifier model on noisy images cθ
1:Randomly sample ynfromy
2:Initializex0
3:forifrom 1toLdo
4:stepsizeαi←δ2
i/δ2
L
5:fortfrom 1toTdo
6:Sample noise zt∼N(0,I)
7:xt←xt−1+αi
2sθ(xt−1,δi) +√αizt+kΣθ(xt−1)∇xt−1logcθ(yn|xt−1)
8:end for
9:x0←xT
10:end for
11:ReturnxT
Algorithm 2 PAC-Private Adapted Randomized Response Algorithm
Require: a sensitive dataset {xi:i= 1,2,...,m}m
i=1; sample number k
1:Samplingkimage candidates from {xi}m
i=1, with the sampling probability for each image Eqn. 13, to
construct set X←{xj:max(˜xi−xj)/σj≤β,j∈[m]}
2:Privatize images in Xwith RR from Eqn. 12 and obtain H(xi)
3:M(˜xi) =H(xi) +zi;zi∼N(0,σ2I)
4:ReturnM(˜xi)
4.2 Privacy Metrics
Most privacy-preserving generative models prioritize assessing the utility of images for downstream tasks,
yet often overlook the crucial metric of the models’ own privacy. To bridge this gap in evaluating privacy
extent, we have developed a novel algorithm that computes a privacy score for the models obtained. As
per Algorithm 3, we commence by preparing images xigenerated from Algorithm 1, the original dataset
imagesxG
k, and alongside the classifier model which trained separately with clean images (different from the
classifier from Algorithm 1). Subsequently, we process all synthesized images through InceptionV3 (Szegedy
et al., 2016).
After obtaining the feature vector output from InceptionV3, we locate the feature vector of a ground truth
image that has the smallest L2 distance to that of the synthesized image, effectively finding the nearest
ground truth neighbor. We then test whether a pretrained classifier model can differentiate these two
images. Inability of the classifier to distinguish between the two indicates that the specific attributes, even
in images most similar to the original, are well-protected. Thus, our method successfully preserves privacy for
specific attributes. Finally, we compute the average probability of incorrect classification by the pretrained
model to establish our privacy score. The greater the privacy score a model achieves, the more robust its
privacy protection is deemed to be.
4.3 PAC Mutual Information Automatic Control
Theorem 4.1. When the mutual information MI(X;M(X))is insufficient to ensure PAC privacy, addi-
tional Gaussian noise B∼N(0,ΣB)can be introduced to yield a reduced mutual information MI(X;M(X)+
B)(Xiao & Devadas, 2023), such that MI(X;M(X) +B)satisfies
MI(X;M(X) +B)≤1
2·logdet/parenleftbig
Id+ ΣM(X)·Σ−1
B/parenrightbig
. (16)
7Under review as submission to TMLR
Algorithm 3 Privacy Score
Require: imagesxigenerated by algorithm 1;
ground truth images xG
k;
sample number n;
pretrained InceptionV3 model Iθ;
pretrained classifier model cθ.
1:private score s←0
2:forxifromx0toxndo
3:findargmink||Iθ(xi),Iθ(xG
k)||2
4:ifcθ(xi)̸=cθ(xG
k)then
5:s←s+ 1
6:end if
7:end for
8:Returns/n
Proof.See Appendix B.
Based on Theorem 4.1, PAC privacy provides a 1−γconfidence in the noise determination for the deter-
ministic mechanism Mshown in Algorithm 4, to compute the Gaussian noise covariance ΣB, ensuring the
upper bound MI(X;M(X) +B)≤ν+β. In practical terms, this means we can evaluate the privacy extent
of the model by comparing the mean norm E||B||2under the same mutual information upper bound level
ν+β. A smaller E||B||2implies a smaller covariance matrix ΣBand lesser noise addition on M(X), which
indicatesXandM(X)have less mutual information. Therefore, less noise addition signifies a higher privacy
protection of the model M.
4.4 PAC Privacy Proof of Our Model
Since the randomized response method adds Gaussian noise Zin the random response mechanism, it can be
proven to be PAC private using Theorem 3.1 and Theorem 4.1 in the paper:
•Algorithm 2 can be combined and written as H(X)+B, whereH(X)represents the RR mechanism
andBdenotes Gaussian noise with B∼N(0,ΣB).
•Applying Theorem 4.1, we can yield a satisfied upper bound for the whole process MI(X;M(X))≤
1
2·logdet/parenleftbig
Id+ ΣM(X)·Σ−1
B/parenrightbig
, whereM(X) =H(X) +B.
•Hence,MI(X;M(X))can be used to produce a (loose) upper bound of PAC Privacy via Theo-
rem 3.1.
What’s more, Algorithm 1 is a heuristic method that further enhances privacy. We calculate noise Bto
measure its privacy strength, achieving the best performance as shown in Table 2. Of independent interest
to ensure the PAC privacy of Algorithm 1 itself, we simply need to add noise Bto the final xTin Algorithm 1.
5 Experiments
5.1 Datasets
Our experiments were carried out using the CelebA (Liu et al., 2015) datasets. We specifically targeted
attributes that have a balanced distribution of positive and negative samples, as noted in Rudd et al. (2016),
to facilitate the training of classifiers. Consequently, we selected attributes like gender and smile from the
CelebA dataset (referred to as CelebA-gender and CelebA-smile, respectively) for sampling in Algorithm 1.
All the images used in these experiments were of the resolution 64×64.
8Under review as submission to TMLR
Algorithm 4 (1 -γ)-Confidence Noise Determination of Deterministic Mechanism
Require: Diffusion-Privacy model M, data distribution D, sampling complexity m, security parameter c,
and mutual information quantities νandβ.
1:TrainMmodel with data X.
2:fork= 1,2,...,mdo
3:sample images y(k)=M(X(k)).
4:end for
5:Calculate empirical mean ˆµ=1
m/summationtextm
k=1y(k)and the empirical covariance estimation ˆΣ =1
m/summationtextm
k=1(y(k)−
ˆµ)(y(k)−ˆµ)T.
6:Apply singular value decomposition (SVD) on ˆΣand obtain the decomposition as ˆΣ = ˆUˆΛˆUT, where ˆΛ
is the diagonal matrix of eigenvalues λ1≥λ2≥...≥λd.
7:Determine the maximal index j0= arg max jλjfor thoseλj>c.
8:ifmin 1≤j≤j0,1≤d(λj−ˆλj)>r/radicalbig
d/c+ 2cthenthen
9:forj= 1,2,...,ddo
10:Determine the j-th element of a diagonal matrix ABas
λB,j=2ν
/radicalbig
λj+ 10cν/β·/parenleftig/summationtextd
j=1/radicalbig
λj+ 10cν/β/parenrightig
11:end for
12:Determine the Gaussian noise covariance as ΣB=ˆUABˆUT.
13:else
14:Determine the Gaussian noise covariance as ΣB=/parenleftig/summationtextd
j=1λj+dc/(2ν)/parenrightig
·Id.
15:end if
16:Return Gaussian covariance matrix ΣB.
5.2 Baselines
In our study, we consider DPGEN (Chen et al., 2022), DPDM (Dockhorn et al., 2022) and DP-MEPF
(Harder et al., 2023) as baseline methods. Both of these approaches excel in synthesizing images under
differential privacy (DP) constraints, and they stand out for their exceptional sample quality in comparison
to other DP generative models. These models serve as important benchmarks against which we evaluate the
performance and efficacy of our proposed method.
5.3 Evaluation Metrics
In our evaluation process, we validate the capability of our PAC Privacy Preserving Diffusion Model to
generate high-resolution images using two key metrics: (1) the Frechet Inception Distance (Heusel et al.,
2017) and (2) the Inception Score (Salimans et al., 2016). These metrics are widely recognized and utilized
in the field of generative models to assess the visual fidelity of the images they produce.
Additionally, to demonstrate our model’s effectiveness in preventing privacy leakage, we employ our unique
privacy metrics as outlined in Algorithm 3. We compare the mean norm of the Gaussian noise E||B||2as
detailed in Sec. 4.3. For our experiments, we have chosen the hyperparameters ν=β= 0.5andγ= 0.01.
This approach allows us to comprehensively assess not just the quality of the images generated, but also the
strength of privacy protection our model offers.
5.4 Empirical Results And Analysis
It is important to note that the ξpresented in our P3DM model within the tables below is merely a
hyperparameter derived from the Randomized Response (RR) as indicated in Eqn. 12. This ξshould not
be confused with the εfromε-Differential Privacy (DP), as our model uses PAC privacy rather than ε-DP.
9Under review as submission to TMLR
Table 1: Perceptual and privacy score comparisons on CelebA with image resolution 64×64. In our model,
we train with data on a specific label respectively, and Model-Gender means our model is trained with
CelebA-Gender dataset. ϵrepresents the DP parameter of baselines, while ξindicates a hyperparameter
derived from the Randomized Response (RR). The same applies to the following.
P3DM-Gender P3DM-Smile DPGEN DPDM DP-MEPF
εorξFID↓Privacy Score↑FID↓Privacy Score↑FID↓Privacy Score↑FID↓Privacy Score↑FID↓Privacy Score↑
1 175±2.5 0.6±0.05 170±1.85 0.63±0.08 67.5 ±1.45 0.48±0.02
5 125.8±3.6 0.55±0.082 107.85±4.7 0.6±0.075 155±1.5 0.46±0.076 170±2 0.57±0.06 61.2±1.8 0.45±0.03
10 44.82±4.48 0.5±0.05 37.96±1.85 0.56±0.045 39.16±0.68 0.4±0.03 117±1.5 0.47±0.04 57.5±1.2 0.4±0.04
15 40±0.5 0.45±0.05 37.9±0.6 0.5±0.025 38.5±0.73 0.4±0.06 113±1.2 0.35±0.05 55.8±1.4 0.38±0.02
∞ 34.64±2.18 0.4±0.04 36.03±1.32 0.48±0.055 34.4±1.8 0.376±0.03 110±1.2 0.3±0.06 52±2.3 0.34±0.025
(a) DPDM FID = 117, Pri-
vacy Score = 0.47
(b) DPGEN FID = 39.16,
Privacy Score = 0.4
(c) DP-MEPF FID = 57.5,
Privacy Score = 0.4
(d) P3DM (Ours) FID =
37.96, Privacy Score = 0.56
Figure 1: CelebA images generated from DPDM, DPGEN and our model from left to right with image
resolution 64×64.
We will later illustrate how our model assures privacy through the automatic control of mutual information
for a PAC privacy guarantee in this section.
Table 1 and Fig. 3 details the evaluation results of image visual quality and privacy score on the CelebA
dataset with a resolution of 64×64, where each datapoint in the figure, or each entry in the table, consists of
mean and standard deviation from 3 experimental results with the same epsilon and different random seeds.
By examining the table, we can see that our model achieves image quality comparable to the state-of-the-art
model DPGEN (Chen et al., 2022), a conclusion also supported by Fig.1.
Additionally, from Fig. 3, our model registers the highest performance in privacy score when having similar
FIDs, signaling an enhancement in our model’s ability to preserve privacy without substantially impacting
image utility. The assertion is further supported by Fig. 2, wherein the CelebA dataset is filtered based on
the ’smile’ attribute. Even upon querying the nearest images of P3DM samples, we can distinctly observe
that while all P3DM samples exhibit the absence of a smile, the nearest neighbors predominantly display
smiling faces. This highlights that, in contrast to other models, the closest images between the generated
dataset and the ground truth datasets are notably similar in terms of the ’smile’ expression, while the images
generated by our model demonstrate distinctive features. Hence, the P3DM model effectively conceals the
’smile’ attribute.
Furthermore, in Table 2, we compute the multivariate Gaussian matrix Bwith a 1−γnoise determination
for the deterministic mechanism Mfollowed the work by Xiao & Devadas (2023). From this, we show
that under the same confidence level of 1−γ= 0.99to ensureMI(X;M(X) +B)≤1, the PAC Privacy
Preserving Diffusion Model exhibits the smallest norm on E||B||2. This demonstrates that data processed by
our model retains the least mutual information with the original dataset, thus affirming our model’s superior
performance in privacy protection.
10Under review as submission to TMLR
(a) DPGEN for ε=∞
(b) DPDM for ε= 10
(c) DPGEN for ε= 10
(d) P3DM-Smile (Ours) for ξ= 10
Figure 2: Generated images (the second row) and their nearest neighbors measured by the l2distance
between images from CelebA-smile dataset (the first row), with image resolution 64 ×64.
11Under review as submission to TMLR
40 60 80 100 120 140 160 180
FID0.30.40.50.60.7Privacy Score
Privacy Score vs. FID with Error Bars
DP-MEPF
DPDM
DPGEN
P3DM-Gender
P3DM-Smile
Figure 3: Privacy score and FID curve of all datapoints from different models. Each datapoint in
the figure consists of mean and standard deviation from 3 experimental results with the same epsilon and
different random seeds. Top-right corner is preferred. The curve from our method, pushes the frontier to
the upper-right over DPGEN (Chen et al., 2022), DPDM (Dockhorn et al., 2022) and DP-MEPF (Harder
et al., 2023).
6 Limitations
Table 2: E||B||2to ensureMI(X;M(X) +
B)≤1on CelebA dataset
Methods εorξ E||B||2↓
P3DM-Gender 5 283.03±2.25
P3DM-Smile 5 280.60±2.57
DPGEN 5 281.3±2.78
DP-MEPF 5 325.6±3.62
P3DM-Gender 10 328.80±1.24
P3DM-Smile 10 327.98±1.45
DPGEN 10 329.48±1.6
DPDM 10 335.83±1.21
DP-MEPF 10 330.75±1.33
P3DM-Gender ∞ 332.87±1.15
P3DM-Smile ∞ 331.67±1.3
DPGEN ∞ 332.02±1.32
DPDM ∞ 335.83±1.56
DP-MEPF ∞ 333.48±1.79The dataset attributes analyzed in our study predominantly
consist of features with balanced positive and negative sam-
ples. Attributes characterized by imbalanced distributions
remain underexplored. Furthermore, it should be noted that
notalldatasetsincludeannotatedattributes. Therefore, uti-
lizing image-to-text models such as CLIP (Radford et al.,
2021) may prove beneficial for the generation of new at-
tribute labels. Lastly, our model faces challenges in syn-
thesizing higher-resolution privacy-preserving images (e.g.,
256x256 pixels) and handling complex datasets such as the
CUB dataset (Wah et al., 2011)).
7 Conclusion
We introduce the PAC Privacy Preserving Diffusion Model
(P3DM), which incorporates conditional private classifier
guidance into the Langevin Sampling process to selectively privatize image features. In addition, we have
developed and implemented a unique metric for evaluating privacy. This metric involves comparing a gen-
erated image with its nearest counterpart in the dataset to assess whether a pretrained classifier can differ-
entiate between the two. Furthermore, we calculate the necessary additional noise Bto ensure PAC privacy
and benchmark the noise magnitude against other models. Our thorough empirical and theoretical testing
confirms that our model surpasses current state-of-the-art private generative models in terms of privacy
protection while maintaining comparable image quality.
12Under review as submission to TMLR
References
MartinAbadi, AndyChu, IanGoodfellow, HBrendanMcMahan, IlyaMironov, KunalTalwar, andLiZhang.
Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer
and communications security , pp. 308–318, 2016.
Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, and Christian Etmann. Conditional image
generation with score-based diffusion models. arXiv preprint arXiv:2111.13606 , 2021.
Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, and Karsten Kreis. Don’t generate me: Training differ-
entially private generative models with sinkhorn divergence. Advances in Neural Information Processing
Systems, 34:12480–12492, 2021.
Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradient-sanitized approach for learn-
ing differentially private generators. Advances in Neural Information Processing Systems , 33:12673–12684,
2020.
Jia-Wei Chen, Chia-Mu Yu, Ching-Chia Kao, Tzai-Wei Pang, and Chun-Shien Lu. Dpgen: Differentially
private generative energy-guided network for natural image synthesis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 8387–8396, 2022.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural
information processing systems , 34:8780–8794, 2021.
Tim Dockhorn, Tianshi Cao, Arash Vahdat, and Karsten Kreis. Differentially Private Diffusion Models.
arXiv:2210.09929 , 2022.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New
York, NY, USA, March 4-7, 2006. Proceedings 3 , pp. 265–284. Springer, 2006.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and
Trends ®in Theoretical Computer Science , 9(3–4):211–407, 2014.
Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De,
Samuel L Smith, Olivia Wiles, and Borja Balle. Differentially private diffusion models generate useful
synthetic images. arXiv preprint arXiv:2302.13861 , 2023.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing
systems, 27, 2014.
Frederik Harder, Kamil Adamczewski, and Mijung Park. Dp-merf: Differentially private mean embeddings
with randomfeatures for practical privacy-preserving data generation. In International conference on
artificial intelligence and statistics , pp. 1819–1827. PMLR, 2021.
Frederik Harder, Milad Jalali, Danica J Sutherland, and Mijung Park. Pre-trained perceptual features
improve differentially private image generation. Transactions on Machine Learning Research , 2023.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural infor-
mation processing systems , 30, 2017.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 , 2022.
Yuzheng Hu, Fan Wu, Qinbin Li, Yunhui Long, Gonzalo Munilla Garrido, Chang Ge, Bolin Ding, David
Forsyth, Bo Li, and Dawn Song. Sok: Privacy-preserving data synthesis. arXiv preprint arXiv:2307.02106 ,
2023.
Dihong Jiang, Guojun Zhang, Mahdi Karami, Xi Chen, Yunfeng Shao, and Yaoliang Yu Dp2-vae. Differen-
tially private pre-trained variational autoencoders. arXiv preprint arXiv:2208.03409 , 2022.
13Under review as submission to TMLR
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Saiyue Lyu, Margarita Vinaroz, Michael F Liu, and Mijung Park. Differentially private latent diffusion
models.arXiv preprint arXiv:2305.15759 , 2023.
Bjarne Pfitzner and Bert Arnrich. Dpd-fvae: Synthetic data generation using federated variational autoen-
coders with differentially-private decoder. arXiv preprint arXiv:2211.11591 , 2022.
Mark S Pinsker, Vyacheslav V Prelov, and Sergio Verdu. Sensitivity of channel capacity. IEEE Transactions
on Information Theory , 41(6):1877–1888, 1995.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning , pp. 8748–8763. PMLR,
2021.
Ethan M Rudd, Manuel Günther, and Terrance E Boult. Moon: A mixed objective optimization network
for the recognition of facial attributes. In Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14 , pp. 19–35. Springer, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. Advances in neural information processing systems , 29, 2016.
Igal Sason and Sergio Verdú. f-divergence inequalities. IEEE Transactions on Information Theory , 62(11):
5973–6006, 2016.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems , 32, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances
in neural information processing systems , 33:12438–12448, 2020.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456 , 2020.
Mayuri Sridhar, Hanshen Xiao, and Srinivas Devadas. Pac-private algorithms. Cryptology ePrint Archive ,
2024.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 2818–2826, 2016.
Shun Takagi, Tsubasa Takahashi, Yang Cao, and Masatoshi Yoshikawa. P3gm: Private high-dimensional
data release via privacy preserving phased generative model. In 2021 IEEE 37th International Conference
on Data Engineering (ICDE) , pp. 169–180. IEEE, 2021.
Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. Dp-cgan: Differentially private synthetic
data and label generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops , pp. 0–0, 2019.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds-200-2011. Technical Report
CNS-TR-2011-001, California Institute of Technology, 2011.
Hanshen Xiao and Srinivas Devadas. Pac privacy: Automatic privacy measurement and control of data
processing. In Annual International Cryptology Conference , pp. 611–644. Springer, 2023.
Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative adversarial
network. arXiv preprint arXiv:1802.06739 , 2018.
14Under review as submission to TMLR
A Proof of Theorem 3.1
Proof.To start, we need the following lemmas.
Lemma A.1. Given anyf-divergenceDf(·∥·), and three Bernoulli distributions 1a,1band1cof parameters
a,bandc, respectively, where 0≤a≤b≤c≤1. Then,Df(1a∥1b)≤Df(1a∥1c).
Proof.By the definition, g(x) =Df(1a∥1x) =xf(a
x) + (1−x)f(1−a
1−x)and we want to show g(x)is non-
decreasing for x≥a. With some calculation, g′(x) =/parenleftbig
f(a
x)−a
xf′(a
x)/parenrightbig
−/parenleftbig
f(1−a
1−x)−1−a
1−xf′(1−a
1−x)/parenrightbig
.It is noted
thata
x≤1−a
1−xforx≥a. Thus, to show g′(x)≥0forx≥a, it suffices to show t(y) =f(y)−yf′(y)is
non-increasing with respect to y∈[0,1]. On the other hand, t′(y) =f′(y)−f′(y)−yf′′(y)≤0due to the
convex assumption of f. Therefore, the claim holds.
Lemma A.2 (Data Processing Inequality Sason & Verdú (2016)) .Consider a channel that produces Zgiven
Ybased on the law described as a conditional distribution PZ|Y. IfPZis the distribution of ZwhenYis
generated by PY, and QZis the distribution of ZwhenYis generated by QY, then for any f-divergence Df,
Df(PZ∥QZ)≤Df(PY∥QY).
Now, we return to prove Theorem 3.1. First, we have the observation that for a random variable X′∈X∗
in an arbitrary distribution but independent of X,δρ
o≤PrX′⊥X/parenleftbig
ρ(X′,X) = 1/parenrightbig
,sinceδρ
ois the minimum
failure probability achieved by optimal a prioriestimation. Here, a⊥brepresents that ais independent of
b. Let the indicator be a function that for two random variables aandb,1(a,b) = 1ifρ(a,b) = 1, otherwise
0. Apply Lemma A.1 and Lemma A.2, where we view 1(·,·)as a post-processing on (X,˜X)and(X,X′),
respectively, we have that
Df/parenleftbig
1δ∥1δρ
o/parenrightbig
≤Df/parenleftbig
1(X,˜X)∥1(X,X′)/parenrightbig
≤Df/parenleftbig
PX,˜X∥PX,X′/parenrightbig
=Df/parenleftbig
PX,˜X∥PX⊗PX′/parenrightbig
.
On the other hand, we know X→M (X)→˜Xforms a Markov chain, where the adversary’s estimation
˜Xis dependent on observation M(X). Let the adversary’s strategy be some operator gadvwhere ˜X=
gadv(M(X)). Therefore, we can apply the data processing inequality again, where
Df/parenleftbig
PX,˜X∥PX,X′/parenrightbig
≤Df/parenleftbig
PX,M(X)∥PX,W/parenrightbig
=Df/parenleftbig
PX,M(X)∥PX⊗PW/parenrightbig
.
Here,X′=gadv(W)andWis still independent of X. Since the above inequalities hold for arbitrarily
distributed X′once it is independent of X,Wcould also be an arbitrary random variable on the same
support domain as M(X)and independent of X. Therefore,
Df/parenleftbig
1δ∥1δρ
o/parenrightbig
≤inf
PWDf/parenleftbig
PX,M(X)∥PX⊗PW/parenrightbig
= inf
PWDf/parenleftbig
PM(X)|X∥PW|PX).
Here, we use|PXto denote that it is conditional on Xin a distribution PX. In particular, if we select PWto
be the distribution of M(X), and take Dfto be KL-divergence, we have DKL/parenleftbig
1δ∥1δρ
o/parenrightbig
≤MI(X;M(X)).
B Proof of Theorem 4.1
Proof.ForMI(X;M(X) +B), we have
MI(X;M(X) +B)
=/integraldisplay
DKL(PM(X0)+B∥PB)P(X=X0)dX0−DKL(PM(X)+B∥PB)
=/integraldisplay
DKL(PM(X0)+B∥PB)P(X=X0)dX0−/parenleftbig
DKL(PM(X)+B∥PGau(M(X)+B)) +DKL(PGau(M(X)+B)∥PB)/parenrightbig
≤/integraldisplay
DKL(PM(X0)+B∥PB)P(X=X0)dX0−DKL(PGau(M(X)+B)∥PB).
(17)
15Under review as submission to TMLR
Giventhedefinitionofmutualinformation, wefirstapplytheresultsofGaussianapproximationPinskeretal.
(1995), where Gau(A)represents a (multivariate) Gaussian variable with the same mean and (co)variance
as those of A. Then, we drop a negative term to obtain the final inequality of (17). Next, focusing on the
integral (first) term in (17),
DKL(PM(X0)+B∥PB) =1
2·(M(X0))TΣ−1
B(M(X0)),
and therefore
/integraldisplay
DKL(PM(X0)+B∥PB)P(X=X0)dX0=1
2·EX/bracketleftbig
(M(X))TΣ−1
B(M(X))/bracketrightbig
.
As for the second term in the last equation of (17), we have the covariance of M(X)equals ΣM(X)=
EX/bracketleftbig
(M(X)−E[M(X)])(M(X)−E[M(X)])T/bracketrightbig
, while the mean µM(X)=E[M(X)]. The KL divergence
between two multivariate Gaussians has a closed form, where DKL(PGau(M(X)+B)∥PB)equals
DKL(PGau(M(X)+B)∥PB) =1
2·/parenleftbig
Trace (ΣM(X)·Σ−1
B)
+EX[M(X)]TΣ−1
BEX[M(X)]−logdet(Id+ ΣM(X)Σ−1
B)/parenrightbig
.(18)
On the other hand, note that
EX/bracketleftbig
(M(X))TΣ−1
B(M(X))/bracketrightbig
−EX/bracketleftbig
M(X)/bracketrightbigTΣ−1
BEX/bracketleftbig
M(X)/bracketrightbig
−Trace (ΣM(X)·Σ−1
B)
=Trace/parenleftbig
E[M(X)−E[M(X)]]·Σ−1
B·E[M(X)−E[M(X)]]T/parenrightbig
−Trace/parenleftbig
ΣM(X)Σ−1
B/parenrightbig
=Trace/parenleftbig
E[M(X)−E[M(X)]]·E[M(X)−E[M(X)]]T·Σ−1
B−ΣM(X)Σ−1
B/parenrightbig
=Trace/parenleftbig
ΣM(X)·Σ−1
B−ΣM(X)Σ−1
B/parenrightbig
= 0.(19)
In (19), we use the following facts that for two arbitrary vectors v1,v2∈Rd,(v1)Tv2=Trace (v1·(v2)T), and
for two arbitrary matrices A1,A2∈Rd×d, Trace (A1A2) =Trace (A2A1). Therefore, putting it all together,
we have a simplified form of the right hand of (17), where
MI(X;M(X) +B)≤logdet(Id+ ΣM(X)·Σ−1
B)
2. (20)
16