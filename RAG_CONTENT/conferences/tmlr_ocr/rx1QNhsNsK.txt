Published in Transactions on Machine Learning Research (10/2024)
Analyzing the Impact of Learnable Softmax Temperature in
Contrastive Visual-Textual Alignment Systems:
Benefits, Drawbacks, and Alternative Approaches
Zhun Sun zhunsun@gmail.com
Independent Researcher
Chao Li chao.li@riken.jp
RIKEN Center for Advanced Intelligence Project (RIKEN-AIP),
Tokyo, Japan
Reviewed on OpenReview: https: // openreview. net/ forum? id= rx1QNhsNsK
Abstract
This work does NOT read like “fabricate motivation - propose something - obtain sota
results”. Instead, we provide an in-depth analysis of the learnable softmax temperature pa-
rameter in the practical training of contrastive visual-textual alignment models, commonly
known as CLIP models. This parameter is critical for optimal system performance, yet
its mechanism and potential drawbacks have been largely overlooked. Our study addresses
this gap and proposes a novel solution by utilizing the architecture of Vision Transform-
ers (ViTs). We focus on the crucial role of the softmax temperature in managing noisy
training data. We demonstrate that there is a balance in the gradient of the contrastive
loss, with the temperature parameter acting as a distance scaling factor. If not prop-
erly calibrated, the model struggles to align positive pairs due to numerical issues in the
loss term. Conversely, a high temperature can lead to unstable learning dynamics. We
explore alternative approaches to mitigate this problem from a topological perspective of
the contrastive loss. Ultimately, we leverage multiple class tokens embedded within the
transformer architecture to present a concise solution. This configuration significantly en-
hances zero-shot classification performance, improving baseline CLIP models pretrained on
large-scale datasets by an average of 6.1%. The codes and learned weights are provided in
https://github.com/minogame/clip-mtob .
1 Introduction
Learning visual and textual feature representations that are semantically aligned in their embedding space
is an ordinary problem in the vision-language cross-modal tasks (Frome et al., 2013; Karpathy & Fei-Fei,
2015; Romera-Paredes & Torr, 2015; Wang et al., 2016; Faghri et al., 2017; Xian et al., 2016). In early works
that employ feature representations from deep neural networks, e.g.Frome et al. (2013), the alignment is
often achieved by a fundamental metric learning approach with the hinge rank loss. That is, the similarity
between a visual feature vector uand a textual feature vector vis calculated as uTWv, whereWare the
learnable weight parameters. Thanks to significant advances in computational power, we can now use a
more effective and practical approach called contrastive learning. This method aligns positive sample pairs
and simultaneously pushes negative samples apart in large mini-batches using the InfoNCE loss (Radford
et al., 2021; Singh et al., 2022; Jia et al., 2021; Pham et al., 2021; Yuan et al., 2021).
We first review the contrastive visual-textual alignment system. Given a set of semantically related image-
text pairsS={(U,V)1,(U,V)2,..., (U,V)K}, where (U,V)is a pair of an image with a tokenized
text that are considered to be semantic related. The goal is to learn a pair of encoders, simultaneously:
1Published in Transactions on Machine Learning Research (10/2024)
f:U→u,g:V→vto map the image and text into an embedding space, u,vare called embedding
vectors of samples. Following the definition in Oord et al. (2018); Wang & Isola (2020); Chen et al. (2021a);
Radford et al. (2021), the contrastive loss could be formulated as
Lc(f,g;τ,S):=E
U,V∼S
U−
i̸=U
V−
j̸=V/bracketleftigg
−loge−τd(f(U),g(V))
N/bracketrightigg
, (1)
whereτis the temperature term. d(·,·)is the distance function between two embedding vectors, and
N:=/summationdisplay
j∈[M]e−τd(f(U),g(V−
j))+/summationdisplay
i∈[M]e−τd(f(U−
i),g(V)),
is the negative term, with M∈Z+denotes a fixed number of negative samples. The subscript i,jmeans
the image or text chosen from the i,j-th pairs. Intuitively, optimizing this loss term minimizes the distance
between positive image-text pairs and maximizes the distance between negative image-text pairs. It is
worth mentioning that, in recent studies (Radford et al., 2021; Chen et al., 2021b), the contrastive loss is
commonly implemented as the cross-entropy between one-hot labels and the class probability obtained by
softmax within a mini-batch SM. We also employ this implementation in this work as shown in Section 3.
In Equation (1), the standard choice of the distance measure between an image-text pair for the contrastive
learning algorithm is the Cosine Similarity , in both uni-modal (Chen et al., 2020a; Caron et al., 2020;
Chen et al., 2020b) and cross-modal (Radford et al., 2021; Jia et al., 2021; Singh et al., 2022) scenarios.
While this choice provides a solid foundation for feature representation vectors from different sources, it is
widely acknowledged that training such a configuration is challenging in the absence of a learnable softmax
temperature. This learnable temperature is prepended and continuously updated through gradient descent,
along with the training progress in practice (Wu et al., 2018; Radford et al., 2021).
In this work, we conduct a comprehensive analysis of this learnable temperature parameter based on the
following steps. Our primary contribution is highlighted at the beginning of each step.
1. We show that the temperature is learned to achieve the numerical “equilibrium” of the con-
trastiveloss. Inpractice, wecouldnotperfectlyminimize/maximizethedistancebetweenpositive/negative
image-text pairs due to the ambiguity (noise) that exists in S. However, the contrastive alignment system
finds a numerical "equilibrium" where the loss from noisy samples is balanced, minimizing the positive dis-
tance (alignment loss) and maximizing the negative distance (uniformity loss). In Section 2.1, we give a
visualization of the ratio of the two losses under different temperature conditions. We find that both a larger
batch size and more noise data require a large temperature to reach “equilibrium”.
2. We suggest three conditions for contrastive alignment to achieve “equilibrium”, and the
temperature acts as a scaling factor for the numerical value of the distance. In Section 2.2, we
analyzethepropertiesofthecontrastivelossfromatopologicalperspectiveandoutlinethreeprerequisites: 1.
A proper definition of uniformity of the embedding space for samples; 2. A “relaxed” triangular inequality
between sample distances; 3. An expansive numerical value for the distance. We show that the Cosine
Similarity calculates the inner product value between normalized feature representation vectors, resulting
in a similarity ranging from -1 to 1. Hence, the system requires a parameter to expand this range. Next
in Section 2.3, we conduct a toy experiment to demonstrate the necessity of these conditions by varying
topology and distance combinations.
3. We show that a temperature parameter with biased initialization or a large learned value
can degrade training. Subsequently, in Section 2.4, we visualize the temperature learning curve, revealing
that the learned temperature indicates the dataset’s noise level. A biased initialization that doesn’t match
the dataset can impair performance. We also visualize the model’s loss landscape concerning temperature
changes, showing that high temperatures lead to inferior learning dynamics.
4. We develop a practical system to mitigate the effects of the temperature parameter us-
ing the vision transformer’s innate structure. In Section 2.3, we propose using a product spherical
2Published in Transactions on Machine Learning Research (10/2024)
Figure 1: The ratio of alignment loss to uniformity loss under different temperatures. The “equilibrium” is
presented as the ratio equals 1.0, when the log loss in Eq.(1) becomes 0.0. Shuffle = P% standards for P
percent of the labels are shuffled.
embedding space with inner product distance as an alternative. We also explore leveraging the vision trans-
former’s structure, introducing multiple class tokens to create a product spherical embedding space, which
substantially boosts system performance (Section 3). In a comprehensive large-scale experiment, we trained
a ViT-B/16-based CLIP model that surpassed the baseline model in classification and retrieval tasks (Sec-
tion 4).
2 Rethinking the Contrastive Alignment
In this section, we delve into our motivation in detail. First, we provide a visual representation of the
equilibrium. Then, we discuss the conditions (inherent properties) necessary for the contrastive loss to reach
this equilibrium. Next, we design a toy experiment with controlled configurations to demonstrate the effects
of these properties. To generalize the problem, we conceptualize the embedding vectors as points within
specified topologies, using the term “distance between the points” instead of similarity. We assert that
a contrastively “ well-optimized ” visual-textual model should yield shorter distances between semantically
related image-text pairs compared to unrelated pairs, regardless of how the pairs are labeled. Finally, we
discuss the potential drawbacks of the temperature parameter.
2.1 The visualized equilibrium
In Figure 1, we give an intuitionistic presentation of the equilibrium with our learned reference model in
Section4. AsexplainedbyWang&Isola(2020), contrastivelossisacombinationoftwoobjects: a)alignment
of features from positive sample pairs and b) the distribution of the features encouraged to match.
To demonstrate this, we randomly select a mini-batch of samples from our training dataset in varying sizes.
Wethenmanuallyintroducenoisebyshufflingasmallpercentageoftheimages’labels(pairedtexts). Finally,
wecomputetheratioofalignmentlosstouniformityloss(numeratoranddenominatorinEquation (1))under
different temperature conditions, averaging across all positive pairs in the mini-batch. Our findings indicate
that a noisier mini-batch or a larger mini-batch size requires a higher temperature to achieve loss equilibrium.
Additionally, when the system operates at a lower temperature, it tends to push negative pairs away rather
than pull positive pairs closer. Without the temperature adjustment, the alignment loss becomes numerically
too small for the system to effectively align positive pairs. In the next subsection, we discuss the mechanism
behind this observation, focusing on three often overlooked conditions.
2.2 Conditions to achieve the equilibrium in a practical system
In this subsection, we outline three critical conditions necessary for achieving equilibrium in contrastive
learning. These conditions are partly inspired by the properties of measure spaces, but more importantly,
3Published in Transactions on Machine Learning Research (10/2024)
Figure 2: Illustration of the bounded negative distance between true negative pair of samples.
they address the question: if we build a system without temperature, what is essential for ensuring that
the system generalizes well? (See the settings in Section 2.3). The conditions focus on i) Embedding space
(the set of embedding vectors); ii) Properties of the map (the characteristics of the function that maps pairs
of embeddings to similarity values); iii) Range of similarity values (the similarity values produced by the
map should be useful for learning). While missing one or more of these conditions does not prevent the
system from being trained, it may lead to a system that is far from equilibrium, potentially resulting in poor
generalization performance. Below, we discuss these conditions in detail.
2.2.1 Proper definition of uniformity of the embedding space
In this subsection, we explain why we cannot use ℓ2loss in the Euclidean space. Naturally, with
the loss form defined in Equation (1), the distribution object will result in a uniform distribution on the
sphere. Although the distribution of samples doesn’t have to be exactly “uniform” as discovered by Chen
et al. (2021a), it is necessary to define a proper prior distribution for samples to match via optimal transport
algorithms ( e.g.sliced Wasserstein distance), which is undoubtedly a computational burden. Consequently,
the spherical embedding space is deemed the most suitable topology for contrastive alignment, as it exhibits
a proper uniform distribution defined by the surface area measure. In contrast, the commonly adopted
unbounded Euclidean space lacks this property.
2.2.2 “Relaxed” triangular inequality
In this subsection, we explain why we are encouraged to use a non-measure distance. Since
the model is being optimized by the contrastive loss, it is proposed to decrease the distance between labeled
positive pairs and increase that between labeled negative pairs. To examine how the noisy (false) positive
pairs influence the model (even using human-labeled datasets, see Chun et al. (2022)), we assume that we
have a model that is “well-optimized” using a noisy dataset. For this model, we have the following properties:
For a positive pair (U,V)+, their distance d(u∗,v∗)is upper-bounded by a small ϵ+, and the distance for
negative pairs (U,V)−is lower-bounded by a large ϵ−.
Now, let us consider a set of two pairs of its training samples S±={(U1,V1),(U2,V2)}.Accidentally, the
pair(U1,V2)−is also semantically correlated, despite being recognized as a negative sample (It is pervasive
to have negative pairs of image and text that match each other equally well as the positive ones in the noisy
dataset). If the distance function dis ametric, then according to the triangle inequality axiom of metric,
we have the following inequality (see Fig. 2 for an intuitive illustration),
ϵ−≤d(u∗
2,v∗
1)≤d(u∗
1,v∗
2) +d(u∗
2,v∗
2) +d(u∗
1,v∗
1)≤3ϵ+(2)
Fromthissimplederivation, eveninthis“well-optimized”model, stillitwillpredictadistanceupper-bounded
byϵ+for this pair instead of a larger value than ϵ−. Here, we have a contradiction: the system would not
learn numerical separated distance ranges for positive and negative pairs that minimize the contrastive loss,
hence we cannot achieve a “well-optimized” model.
4Published in Transactions on Machine Learning Research (10/2024)
Topology Sphere Sd−1Euclidean RdPS(d/m,m ) PS(d/m,m )
Distance −uTv∥u−v∥2 Geo(u,v)−tr(uTv)
Uniformity surface measure undefined surface measure surface measure
Inequality relaxed restricted restricted relaxed
Distance Range [−1,1] [0,+∞) [0,mπ] [−m,m ]
Table 1: Summary of different topologies endowed with different distances. The total dimension of the
embedding vector is denoted as d. The mini-batch size is denoted as b.Green box stands for the prop-
erties that are favoredfor contrastive learning. Red box stands for the properties that are unfavored for
contrastive learning.
Because the models are not "well-optimized", we have the following observations (See Figure 8 in the sup-
plementary materials). First, the positive pairs of samples (Green bins) usually have a much larger distance
than that in the perfect alignment condition. For instance, the distance between the positive pairs of a
learned Euclidean- ℓ2is around−7, while the perfect alignment condition means the distance should be 0.
Second, although the contrastive loss term does not regularize the distance between negative pairs of samples
in the same modality (Gray/Yellow bins), this distance is implicitly minimized due to the strict triangular
inequality. For instance, the distance negative text pairs distribute closer to the negative image-text pairs.
Finally, in the inner product distance configuration, it only obeys a “relaxed” triangular inequality yielded
byArcCos(the geodesic, see Schubert (2021) for more information.) on the sphere. This makes the distance
between positive/negative pairs slightly more separated, especially in the out-domain scenario (Green/Red
bins in the lower part). From these observations, we find one solution is to have a “relaxed” triangular
inequality to alleviate the ambiguity of the positive/negative pairs. This property could make the distance
between negative pairs not strictly upper-bounded by that of positive pairs.
2.2.3 Broad distance range
In this subsection, we discuss the mechanism behind the learnable temperature trick given the
above two constraints. As discussed in Section 2.1, the contrastive loss is minimized through the gradients
of both alignment and uniformity losses. In a noisy dataset inducing “semantic ambiguity”, the false negative
samples are pushed away from each other (repulsion), while the false positive samples are pulled together
(attraction). Ideally, the system could gradually find a numerical “equilibrium” (the stabilized state) when
the noisy samples’ gradients for attraction and repulsion are equal. For instance, if there is a reasonable
amount of false negative samples, the model would learn a smaller distance between the pairs labeled as
“negative”, such that the uniformity loss won’t be too high when encountering false negative samples in mini-
batches. On the contrary, the model would learn a larger distance between the pairs labeled as “positive”
when the amount of false positive samples is inneglectable. In other words, the model reaches the equilibrium
by learning compromised positive and negative distances under semantic ambiguity.
However, the required numerical value of the compromised distances is out of the range that the cosine
similarity could provide. This is generally because the InfoNCE loss computes the sum of exponential
functions. To reach this equilibrium, we need to expand the distance range to [−τ,τ], and since we don’t
know the noisy level of the dataset, we set the temperature learnable through gradients. For instance, the
officially released CLIP model (Radford et al., 2021) has a glancing similarity of 0.3∼0.5and0.1∼0.3
for positive and negative pairs of samples, respectively. The learned temperature is approaching 100.0,
indicating the value of distances for equilibrium are 30∼50and10∼30for positive and negative pairs of
samples, respectively.
5Published in Transactions on Machine Learning Research (10/2024)
Topology DistanceZero-Shot
I2T R@1Zero-Shot
T2I R@1Zero-Shot
Cls. Acc.Linear PrPSe
Cls. Acc.
Temperature: init= 1.0, gradient=True
Sphere−uTv 49.0 30.33 28.59 59.56
Euc∥u−v∥247.4 30.71 29.85 60.09
PS(64, 8) Geo(u,v)49.9 32.49 30.21 60.61
PS(64, 8)−tr(uTv)50.9 32.71 30.50 60.66
Temperature: init= 1.0, gradient=False (No temperature)
Sphere−uTv 5.1 3.461 4.04 45.37
Euc∥u−v∥247.6 30.43 29.51 59.20
PS(64, 8) Geo(u,v)4.1 2.921 3.10 21.67
PS(64, 8)−tr(uTv)30.3 18.48 21.20 57.93
Table 2: The retrieval and classification performance. “gradient={True/False}” donates if the temperature
is learnable. The configurations are chosen intendedly to support the conclusions in Table 1.
The results shown in this table do not claim some configurations are better than others.
2.3 The toy experiment
Experiment settings: We design a toy experiment to demonstrate how the properties mentioned above
influence the performance of contrastive learning. We employ the 15M subset (Cui et al., 2022) of the
YFCC100M dataset (Thomee et al., 2016) as the training dataset, which contains roughly 15.3 million
internet collected weakly related image-text pairs. We evaluate the learned models with the zero-shot
retrieval performance on Flickr30K, and Zero-Shot/ Linear Probe classification performance on ImageNet
for reference. We employ the original ViT-S/16 architecture for our image encoders (Dosovitskiy et al.,
2020), with an input image resolution of 224, resulting in 196 image tokens.
We evaluate four types of configurations for comparison, each featuring a distinct combination of topology
and distance function. The configurations are summarized in Table 1. Specifically, we consider:
(i).The sphere Sd−1with the inner product as distance, which is the commonly used cosine similarity
(i.e.the standard CLIP configuration).
(ii).The Euclidean space Rdwithℓ2distance, which is the most commonly used loss function in machine
learning algorithms with an unbounded distance range.
(iii).A product spherical embedding space PS(n,m)with the minimizing geodesic as distance, which is
denoted as Geo(u,v) = tr1
2(arccos2(uTv)). We use this configuration to show the importance of properly
defined uniformity. Although its distance range is still bounded, it is sufficient for our system to reach
equilibrium using the proposed dataset (See Figure 3).
(iv).The same product spherical embedding space PS(n,m)with the inner product as distance. We use
this configuration to show the importance of the relaxed tri-angular inequality.
Here, we explain how we implement the product sphere embedding space. The product sphere (PS (n,m))
can be defined as Sn−1×···× Sn−1
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
mcopies, where Sn−1is the sphere embedded in Rn. Intuitively, it is a vector
composed of mchunks ofn−dimensional normalized sub-parts. Our implementation follows this intuition;
we reshape the original feature vector into a matrix of shape m×n, thenℓ2−normalize the columns. It
is worth mentioning that, this study primarily investigates the characteristics of contrastive learning loss,
proposing three distinct properties that the vanilla approach (cosine similarity) fails to satisfy concurrently.
Therefore, we explore an approach that satisfies these three properties simultaneously. While the product
sphere is not the only solution in this scenario, it could facilitate the analysis because I) It has the same
definition of Uniformity; II) We can control the distance range by the number of sub-spheres to design
ablation experiments; III) It is straightforward to implement.
6Published in Transactions on Machine Learning Research (10/2024)
We provide further clarification on the distance functions employed in these configurations. As previously
discussed, the cosine similarity calculates the (negative) inner product for vectors on the unit sphere
as their distance. Accordingly, we adopt this design for the product sphere. Hence, the distance could be
computed as the negative value of the trace of the matrix product, i.e.d(u,v) =−tr(uTv). This distance is
clearly not a restricted metric. Therefore, for reference, we also consider the minimizing geodesic as distance,
which is a restricted metric (obeys the triangular inequality).
Hyperparameters and results: For the product spherical embedding space, we employed a structure of
n= 64,m= 8, denoted as PS(64,8). Concerning the temperature parameter, we initialize it with exp(0.0)
(equivalent to 1.0) and conduct two sets of experiments, one with gradients (learnable) and the other without
gradients(notlearnable). It’simportanttonotethatiftheparameterisinitializedto1.0andisnotlearnable,
it is essentially equivalent to having no such parameter. All the results are presented within Table 2, and
subsequent sections will elucidate the reader on how to interpret this tabular data. Results using other
initialization (Figure 3) are given in the supplementary materials.
Effects of the uniformity: To examine the effects of uniformity, we compare the performance between
the Euclidean with ℓ2distance and the product sphere with geodesic distance. These configurations have
restricted triangular inequality in common. Meanwhile, if the temperature is learnable, the product sphere
also owns a practically unbounded distance range similar to the Euclidean. Then, the only difference is that
uniformity can be defined on the product sphere (surface area measure). We observe that the performance
of the product sphere with geodesic configuration performs better than the Euclidean with ℓ2configuration.
This result indicates the importance of properly defined uniformity.
Effects of the Tri-angular Inequality: Next, we examine the effects of the triangular inequality using
the product sphere topologies endowed with different distance functions, that is, the geodesic distance and
the inner product distance. The results are visually depicted in Table 2, specifically between the 3rd and 4th
lines of each data block presented in the table. Upon observation, it becomes evident that the inner product
distance outperforms the geodesic distance on average in both retrieval and linear probe tasks. Moreover,
whenthetemperatureisunlearnable, theinnerproductdistancestillprovidesthemodeltrainability, showing
the advantage of removing the restriction of tri-angular inequality.
Effects of the Distance Range: Finally, we present the effects of the distance range by comparing the
proposed product sphere topology with inner product distance and the baseline spherical topology. Notably,
when the temperature is learnable, the product sphere demonstrates a reasonable improvement in the top-1
recall and classification accuracy. It is worth mentioning that this implementation does not bring extra
computational complexity, and the only difference is the shape of the embedding space. This suggests that
a larger distance range helps the alignment of the features from different modalities. There is one more
piece of evidence that lies in the last block: the Euclidean with ℓ2distance configuration obtains consistent
performance regardless of the temperature parameter, as it operates with an unrestricted distance range.
2.4 Drawbacks in the learning dynamics
Biased initialization may degrade training: We now examine how the initialization of the temperature
impacts the training progress. Specifically, we additionally run experiments with the temperature initialized
atexp(2.64), which is the default value in the official CLIP implementation, and a notably higher value of
exp(5.3)(∼200). ThechangesinthetemperatureduringthetrainingprocessareillustratedinFigure31. The
figuresconfirmthat: Thetemperatureconvergestoanequilibriumvalueirrespectiveofinitializationwhenthe
uniformity is properly defined. This value reflects the noise level present in the datasets, configurations with
“relaxed” triangular inequality and a broader distance range have lower equilibrium values. These findings
suggest that, if the training is insufficient or the dataset is too noisy, then with a biased initialization of the
temperature parameter, the training progress might be degraded.
Higher temperature hampers the optimization: We next examine the impact of temperature on the
loss landscape and the norm of gradient, using the approach proposed in Li et al. (2018). We employ the
1The detailed retrieval and classification results are provided in the supplementary material. The final performance is not
largely impacted by the initialization though.
7Published in Transactions on Machine Learning Research (10/2024)
Figure 3: The learning curve of the temperature. -<,>, l2 and geodesics denote the negative inner product,
ℓ2, and geodesic distance, respectively. The orange, blue, and green curves denote the initialization of
exp(5.3), exp(2.64), and 1.0, respectively.
(a)
 (b)
 (c)
Figure 4: The impact of temperature on the loss landscape. (a) and (b) depict the contours with two
orthogonal weights ∆in random directions at temperatures of 1.0 and 100.0, respectively. (c) presents the
contour gradient norm with weight ∆in a random direction across various temperatures (dark color stands
for a larger norm).
learned reference model in Section 4 to draw the figures, where the final learned temperature is 100.0 (due to
the “clamp” function). We randomly select a mini-batch of size 1024 from the training dataset. The results
are shown in Figure 4. From figures (a) and (b) we can observe that the model with a lower temperature
has a much smoother loss landscape. Furthermore, in Figure (c) we find that the norm of the gradient might
become greater as the temperature grows. Therefore, in practice, we need to employ techniques such as
gradient norm clipping to stabilize the training.
3 The Multiple Class Tokens Solution
We first review the importance of the class token. In the design of both the textual transformer (BERT, Ken-
ton & Toutanova (2019)) and the visual transformer (ViT, Dosovitskiy et al. (2020)), onelearnable embed-
ding is used to represent global information, termed as class token. Different from the sequence (patch)
tokens, the class token is a key component of the transformer encoder. It is randomly initialized and up-
dated through gradient descent during the optimization. Furthermore, the class token holds a fixed position
embedding, avoiding the influence of the positional information. Therefore, the class token is considered to
participate in the computation of global attention.
Motivated by the properties of the class token, we propose to employ multiple class tokens to build the
product spherical embedding space, with each class token being a sub-sphere of Sn−1. For the visual encoder,
8Published in Transactions on Machine Learning Research (10/2024)
(a) Vanilla CLIP
 (b) CLIP with Multiple Class Tokens
Figure 5: A sketch of the contrastive visual-textual alignment (CLIP) system.
Method
baseline[impl.]IN INV2 IN-A IN-R Flickr30K Zero-shot MSCOCO* Zero-shot
ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1I2T
R@1T2I
R@1Mean
R@1/5/10I2T
R@1T2I
R@1Mean
R@1/5/10
ViT-B/16-224 as visual bone.
CLIP[openAI†] 68.7 61.9 50.1 77.7 81.9 62.1 86.1 55.4 38.4 66.3
CLIP[openCLIP‡] 67.0 59.6 33.2 77.9 83.2 65.5 87.6 52.4 38.4 62.4
CLIP[our-impl.] 69.5 61.4 49.5 70.6 84.2 61.7 86.4 64.1 43.9 72.4
CLIP[Multi(32,16)] 76.4 68.0 55.8 75.285.2 66.3 88.3 63.8 42.9 72.4
ViT-L/14-224 as visual bone for reference.
CLIP[openAI†] 75.5 69.7 70.7 87.9 85.0 65.2 87.7 56.3 36.5 65.2
CLIP[openCLIP‡] 72.7 65.6 46.6 84.8 87.6 70.3 90.1 59.7 43.0 70.0
Table 3: Comparsion of large scale contrastive visual-textual pre-train model on benchmark datasets.†and
‡denote the implementation from Radford et al. (2021) and Ilharco et al. (2021), respectively. The metric
Meanstands for the average value of R@1/5/10 of I2T/T2I retrieval performance. * denotes the Karpathy
test split (Karpathy & Fei-Fei, 2015).
these class tokens are randomly initialized to break symmetry, while for the textual encoder, we use different
absolute positional embeddings for each class token. We present a sketch of the system in Figure 52and
a pseudo-code in Section 3, to facilitate a better understanding. Given the fact that the dimension for the
embedding space could be a critical factor to the performance of the system (Gu et al., 2021), we select
the dimension nwith a conservative strategy. Specifically, we anchor the dimension of Euclidean spaces
to be the same as the reference model, then vary the value of n,msuch thatn×m=l. We denote
this implementation as Multi(n,m). The sub-spheres could benefit from the global attention operation and
provide more representative feature embeddings. On the contrary, the multi-token implementation requires
more computational resources in the backbone since the class tokens are involved in the computation of
global attention.
4 Large-Scale Experimental Results
4.1 Experimental Settings
Please note that our objective is not to produce the best publicly available model; rather, we solely conduct
experimentsundercontrolledandrestrictedconditions. Wecomparetheperformanceoftheproposedmethod
using the configuration, which matches the publicly released ones in teams of dataset samples, model sizes,
and training progress. We also re-implement the naive CLIP model as the reference, which holds a similar
performance as the publicly released ones.
2It is needed to clarify that in the multiple class tokens system, we also employ the learnable temperature practically. Since
the finally learned temperature is significantly smaller than that in the vanilla CLIP system ( ∼4.0versus ∼100 .0), we omit it
to highlight the difference between the systems.
9Published in Transactions on Machine Learning Research (10/2024)
# d - dimensions of the hidden embedding
# U, V - mini-batch of images/texts token, [n, p, d] / [n, l, d]
# PS_m - the dimension of each sub-sphere
# PS_n - the number of [CLS] tokens attached
# cls_U, cls_V - class tokens for images/texts, [n, PS_n, d] / [n, PS_n, d]
# t - learned temperature parameter
# concatenate cls_tokens and extract features
U_, V_ = concatenate([cls_U, U], axis=1), concatenate([cls_V, V], axis=1)
u_bar = visual_transformer(U_) #[n, PS_n + p, d]
v_bar = textual_transformer(V_) #[n, PS_n + l, d]
# map features onto PS(n,m) and calculate distance
u = projection_u(u_bar[:PS_n]).l2_normalize(axis=-1) # [n, PS_n, PS_m]
v = projection_v(v_bar[:PS_n]).l2_normalize(axis=-1) # [n, PS_n, PS_m]
# [n, PS_n, PS_m], [n, PS_n, PS_m] -> [n, n]
neg_distances = einsum( 'inm,jnm->ij ', u, v) * t.exp()
# symmetric loss function
labels = arange(n) # 0, 1, ..., n-1
loss = (CE_loss(neg_distances, labels, axis=0) + CE_loss(neg_distances, labels, axis=1)) / 2
Figure 6: Python-like pseudo-code of the proposed approach.
Datasets: For the experimental analysis in Section 4.2, we collect data from publicly available datasets.
The majority of the image-text comes from the LAION400M(Open Clip) dataset (Schuhmann et al., 2021).
However, since the availability of this dataset decreases through time3, we also add the training splits of some
other well-known large-scale datasets, they are CC3M (Changpinyo et al., 2021), CC12M (Sharma et al.,
2018), MSCOCO (Chen et al., 2015), Visual Genome (Krishna et al., 2017), Flickr30k (Plummer et al.,
2015), ImageNet (Russakovsky et al., 2015), OpenImage (Kuznetsova et al., 2020) and WebVision (Li et al.,
2017). This dataset contains a total of 420 million individual images and roughly 500 million image-text
pairs. This dataset is comparable to the one employed in the official CLIP paper (Radford et al., 2021) and
another open source re-implementation (Ilharco et al., 2021).
Models: Specifically, we employ the ViT-B/16 as our image encoders. For our text encoders, we employ
Ernie-2.0-en-base (Sun et al., 2020), which is a Bert model (Devlin et al., 2018) of 12 layers and 512 hidden
neuron sizes with a customized vocabulary of 30,522 tokens, and the maximum context length is set to be
77. We project the feature representation (class token) from the top layer of transformers to a (sum of)
512-dimensional embedding space. All the parameters except the temperature are optimized from random
initialization. The default initialization of the project matrix employs the Gaussian initializer of zero mean,
and standard deviation equal reversed square root of the input size ( a.k.a.Kaiming initialization). The
details of the hyperparameters are provided in supplementary materials.
Evaluation: We first evaluate the proposed methods with two types of vision tasks: i) Zero-Shot image-
to-text and text-to-image retrieval on Flickr30k (Plummer et al., 2015) and MSCOCO (Lin et al., 2014)
ii)Zero-Shot classification on ImageNet-1K (Russakovsky et al., 2015), ImageNet-V2 (Recht et al., 2019),
ImageNet-R (Hendrycks et al., 2021a) and ImageNet-A (Hendrycks et al., 2021b). For zero-shot retrieval
on Flickr30K and MSCOCO, we employ the logits (distance) computed by the distance function and report
the image-text pairs with the top- kshortest distance as the retrieval results. For zero-shot classification on
ImageNet. We employ multiple prompt templates described in Radford et al. (2021), while we first compute
3The availability of LAION400M is about 90%.
10Published in Transactions on Machine Learning Research (10/2024)
Method
baseline[impl.]COCO 1K COCO 5K CxC ECCV Caption
I2T
R@1T2I
R@1I2T
R@1T2I
R@1I2T
R@1T2I
R@1I2T T2I
mAP@R R-P R@1 mAP@R R-P R@1
ViT-B/16-224 as visual bone.
CLIP[openAI†] 71.7 52.5 52.5 33.1 54.0 34.7 23.7 34.0 68.8 34.8 44.0 73.4
CLIP[openCLIP‡] 74.0 57.6 55.4 38.3 57.3 40.0 26.2 36.6 70.3 36.9 46.4 77.5
CLIP[our-impl.] 80.8 63.0 64.2 43.1 65.3 44.9 30.5 41.0 78.6 40.5 49.9 81.2
CLIP[Multi(32,16)] 81.1 63.1 63.8 42.9 65.344.830.9 41.7 76.341.7 50.5 84.1
ViT-L/14-224 as visual bone for reference.
CLIP[openAI†] 74.3 55.4 56.4 36.6 58.0 38.3 24.0 33.8 71.3 32.0 41.8 73.0
CLIP[openCLIP‡] 77.2 61.4 59.7 43.0 61.1 44.8 28.1 38.3 73.0 38.7 47.9 81.2
Table 4: Comparsion of large scale contrastive visual-textual pre-train model on benchmark datasets.
the distances between image and text embeddings, then average the distances. For linear probe classification
on ImageNet, we remove the learned projection head (no topological structure is preserved), then attach a
random initialized linear projector to map the feature representation to the 1,000 class logits.
Besides the tasks mentioned above, we provide more results using the ECCV dataset (Chun et al., 2022). The
dataset is proposed for eliminating the false negative samples in the validation set of the original MSCOCO
dataset. Instead of the commonly used Recall@K (R@K) metric, the datasets provide a new ranking-based
metric, mAP@R. The authors of the ECCV dataset have shown that the mAP@R metric is more aligned
to humans than Recall@k. Therefore, the performance of a model evaluated by mAP@R would be less
occasional than the R@1. This metric is deemed more precise for evaluating the performance of models in
the presence of noise.
4.2 Experimental results
The evaluations of the learned models on the commonly employed image classification and image-text re-
trieval tasks are reported in Table 3, with the ViT-B/16 as the visual backbone. It can be seen that the
re-implemented CLIP holds a similar performance as the publicly released ones in most cases. On the other
hand, our proposed model with the multi-token implementation of (32,16) significantly outperforms the other
ViT-B/16 models in general, with less than 8% more computational costs. The only exception is the top-1
retrieval performance on the MSCOCO datasets. The reason could be two-fold. Firstly, we observe a mild
“semantic decoupling” between the embedding of tokens through the visualization (see Section 4.3); that
is, some of the individual class tokens focus on specified objects and provide a high alignment confidence.
This may confuse in understanding the given scene as a whole; hence, the recall@top-1 performance is de-
graded. Secondly, the most suitable temperature during training for aligning object-level and scene-level
concepts might differ. In our experiment, we decrease the upper limitation of the temperature to 6.25 (100
/ 16 [tokens]) since the product spherical topology owns the border distance range. The scene-level concept
alignment might require a larger temperature for “ambiguity” to achieve better retrieval performance.
For the ECCV caption dataset, we utilize the officially released evaluation tool and present a summary of
models’ performance in Table 4. Our proposed multi-token product sphere topology outperforms others in
terms of mAP@R, signifying the model’s enhanced robustness in handling “semantic ambiguity” samples.
4.3 Visualization on Tokens Attention Regions
In this section, we provide a commonly adopted neural network explanation method to visualize the influence
of inputs on the final outcome. Specifically, we employ the Grad-CAM (Selvaraju et al., 2017) algorithm
to highlight the interested parts by the model of both images and their corresponding texts. Notably, the
original design of the Grad-CAM algorithm precludes its direct application to textual data. Therefore, we
enhance its capabilities such that it also highlights the contributing parts of texts in a token-wise style.
We employ examples from the evaluation set of the Flickr30K and MSCOCO datasets for visualization.
The results are shown in Figure 7. Through our observations, we have noted that certain pairs of class
11Published in Transactions on Machine Learning Research (10/2024)
Image-Text Pair
 Full Tokens
 Token id=01
 Token id=15
Figure 7: Visualization of the importance map using the Grad-CAM algorithm. The columns from left to
right stand for: the input image-text pair; the importance map computed based on the final matching score;
and the importance maps based on the matching scores of two individual tokens and the involved token IDs.
Additational results are provided in the supplementary material (including the failed cases).
tokens exhibit independent alignment, thereby reflecting a distinct concept or idea embedded within the
image. We attribute the improved classification performance to this phenomenon, as it enables a more
effective representation and understanding of the underlying content by leveraging the distinctive alignment
of the class token pairs. It is noteworthy that the intrinsic decoupling phenomenon observed within the
embeddings of class tokens is NOTuniversally present across all image-text pairs or within every token of an
image-text pair. This is due to the inherent challenges faced by visualization algorithms in achieving precise
correspondence in the importance maps for intricate semantic representations.
5 Related Works
Momentum distillation: In recent works such as Cheng et al. (2021); Li et al. (2021a), the momentum
(self-)distillation is introduced to mitigate the semantic noise in the sample pairs. That is, a momentum
version of the model is updated by the moving average of the model’s historical parameters. Then, the
cross entropy between the softmax logits computed by the model and its momentum version is used as an
additional lossfor supervision. The authorsclaim that the pseudo-targets of the momentum (self-)distillation
will not penalize the model for matching negative samples that are reasonably similar. Here, we consider
that the pseudo-targets do “relax” the triangular inequality restriction implicitly by letting the distance of
alignment be reasonably large. Hence, it could be much easier for the optimizer to find the equilibrium
discussed in Section 2.
Other implemention of non-metric distance: In Yao et al. (2021), the authors proposed a so-called
fine-grained contrastive learning scheme that matches all the visual and textual tokens using a maximum-
average operator. Concretely, for each visual token, it finds the textual token with maximum similarity, then
takes the average over the visual tokens as the similarity of the image to a text and vice versa. Using our
framework, this work can be explained as embedding samples onto the product manifold Sd−1×···× Sd−1
endowed with the maximum-average distance, which is a non-metric distance. At the same time, the authors
employ the sub-manifold Sd−1to represent local information.
The effects of softmax temperature: In Wang & Liu (2021), the authors draw the uniformity of the
embedding distribution and the tolerance to semantically similar samples of learned models under different
temperatures. From the observations, the authors claim that “a good choice of temperature can compromise
these two properties properly to both learn separable features and tolerant to semantically similar samples,
improving the feature qualities and the downstream performances”. Unlike our work, this work is done under
uni-modal contrastive learning, where the semantic correlation of the negative samples is not a property of
the datasets but rather a drawback of the larger mini-batch size.
12Published in Transactions on Machine Learning Research (10/2024)
Uni-modal side tasks: In works such as Mu et al. (2021); Li et al. (2021b); Yang et al. (2022), authors
combine cross-modal contrastive loss with other uni-modal tasks, for instance, visual/textual self-supervised
contrastive learning, masked image/language modeling. These combined methods empirically demonstrate
superior performance in downstream tasks such as zero-shot classification. Although these works do not over-
lap with this one, we find that the uni-modal tasks provide reasonable uniformity within the visual/textual
feature embedding, contrary to the cross-modal contrastive shown in Section 2. Therefore, the model could
obtain a more “numerically relaxed” triangular inequality when dealing with noisy pairs of samples.
6 Conclusion
Summary: This work discusses the essential properties of the feature embedding space for contrastive
alignment. We show that the most commonly adopted cosine similarity has disadvantages in dealing with
noisy data and training stability. Therefore, we propose to combine the product sphere with the negative
innerproductdistancetotackletheseproblems. Weemploymultipleclasstokenstoimplementtheapproach,
which performs better in various zero-shot classification and image-text retrieval tasks practically.
Limitation: First, given significantly constrained computational resources (and time), we acknowledge our
inability to conduct experiments on a larger scale regarding batch size, training data, and neural network
parameters. However, the reported results are robust enough to substantiate our claims. Second, in re-
cent studies, besides the contrastive alignment, more pre-training tasks are appended to the head of the
model using the non-normalized full token embedding. Such as image-text matching (Li et al., 2021a; Yang
et al., 2022), image captioning (Yu et al., 2022), or masked modeling that do not employ the contrastive
alignment (Wang et al., 2022). The performance improvement resulting from a better contrastive alignment
could be marginal in these configurations. Hence leave future work on designing the model of the full token
embedding.
References
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components
with random forests. In Computer vision–ECCV 2014: 13th European conference, zurich, Switzerland,
September 6-12, 2014, proceedings, part VI 13 , pp. 446–461. Springer, 2014.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu-
pervised learning of visual features by contrasting cluster assignments. Advances in Neural Information
Processing Systems , 33:9912–9924, 2020.
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale
image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 3558–3568, 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning , pp. 1597–1607. PMLR,
2020a.
Ting Chen, Calvin Luo, and Lala Li. Intriguing properties of contrastive losses. Advances in Neural Infor-
mation Processing Systems , 34:11834–11845, 2021a.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325 , 2015.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive
learning. arXiv preprint arXiv:2003.04297 , 2020b.
XinleiChen, SainingXie, andKaimingHe. Anempiricalstudyoftrainingself-supervisedvisiontransformers.
InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 9640–9649, 2021b.
13Published in Transactions on Machine Learning Research (10/2024)
Ruizhe Cheng, Bichen Wu, Peizhao Zhang, Peter Vajda, and Joseph E Gonzalez. Data-efficient language-
supervised zero-shot learning with self-distillation. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pp. 3119–3124, 2021.
Sanghyuk Chun. Improved probabilistic image-text representations. arXiv preprint arXiv:2305.18171 , 2023.
Sanghyuk Chun, Wonjae Kim, Song Park, Minsuk Chang Chang, and Seong Joon Oh. Eccv caption:
Correcting false negatives by collecting machine-and-human-verified image-caption associations for ms-
coco. In European Conference on Computer Vision (ECCV) , 2022.
M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In
Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature
learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics ,
pp. 215–223. JMLR Workshop and Conference Proceedings, 2011.
Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, and Jing Shao. Democratizing contrastive language-
image pre-training: A clip benchmark of data, model, and supervision, 2022.
Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data
created by the people, for the people. arXiv preprint arXiv:2111.11431 , 2021.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic
embeddings with hard negatives. arXiv preprint arXiv:1707.05612 , 2017.
Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on
pattern analysis and machine intelligence , 28(4):594–611, 2006.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas
Mikolov. Devise: A deep visual-semantic embedding model. Advances in neural information processing
systems, 26, 2013.
Weiwei Gu, Aditya Tandon, Yong-Yeol Ahn, and Filippo Radicchi. Principled approach to the selection of
the embedding dimension of networks. Nature Communications , 12(1):3772, 2021.
Haochen Han, Kaiyao Miao, Qinghua Zheng, and Minnan Luo. Noisy correspondence learning with meta
similarity correction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 7517–7526, 2023.
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied
Earth Observations and Remote Sensing , 12(7):2217–2226, 2019.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,
Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many
faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021a.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial ex-
amples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
15262–15271, 2021b.
14Published in Transactions on Machine Learning Research (10/2024)
Zhenyu Huang, Guocheng Niu, Xiao Liu, Wenbiao Ding, Xinyan Xiao, Hua Wu, and Xi Peng. Learning with
noisy correspondence for cross-modal matching. Advances in Neural Information Processing Systems , 34:
29406–29419, 2021.
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal
Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig
Schmidt. Openclip. Zenodo, July 2021. doi: 10.5281/zenodo.5143773. URL https://doi.org/10.5281/
zenodo.5143773 .
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,
Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text
supervision. In International Conference on Machine Learning , pp. 4904–4916. PMLR, 2021.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 3128–3137, 2015.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of naacL-HLT , pp. 4171–4186, 2019.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained catego-
rization. In Proceedings of the IEEE international conference on computer vision workshops , pp. 554–561,
2013.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
usingcrowdsourceddenseimageannotations. International journal of computer vision , 123(1):32–73, 2017.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
AlinaKuznetsova, HassanRom, NeilAlldrin, JasperUijlings, IvanKrasin, JordiPont-Tuset, ShahabKamali,
Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. International
Journal of Computer Vision , 128(7):1956–1981, 2020.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of
neural nets. Advances in neural information processing systems , 31, 2018.
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong
Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances
in neural information processing systems , 34:9694–9705, 2021a.
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning
and understanding from web data. arXiv preprint arXiv:1708.02862 , 2017.
Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie
Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.
arXiv preprint arXiv:2110.05208 , 2021b.
Zheng Li, Caili Guo, Zerun Feng, Jenq-Neng Hwang, and Zhongtian Du. Integrating language guidance into
image-text matching for correcting false negatives. IEEE Transactions on Multimedia , 2023.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th
European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pp. 740–755.
Springer, 2014.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 ,
2017.
S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft.
Technical report, 2013.
15Published in Transactions on Machine Learning Research (10/2024)
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-
image pre-training. arXiv preprint arXiv:2112.12750 , 2021.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and
unsupervised feature learning , volume 2011, pp. 4. Granada, 2011.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.
In2008 Sixth Indian conference on computer vision, graphics & image processing , pp.722–729.IEEE,2008.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding.arXiv preprint arXiv:1807.03748 , 2018.
Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned
photographs. Advances in neural information processing systems , 24, 2011.
Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE
conference on computer vision and pattern recognition , pp. 3498–3505. IEEE, 2012.
Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan,
and Quoc V Le. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050 , 2021.
Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence
models. In Proceedings of the IEEE international conference on computer vision , pp. 2641–2649, 2015.
Yang Qin, Dezhong Peng, Xi Peng, Xu Wang, and Peng Hu. Deep evidential learning with noisy correspon-
dence for cross-modal retrieval. In Proceedings of the 30th ACM International Conference on Multimedia ,
pp. 4948–4956, 2022.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR,
2021.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize
to imagenet? In International conference on machine learning , pp. 5389–5400. PMLR, 2019.
Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In
International conference on machine learning , pp. 2152–2161. PMLR, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale
Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015.
doi: 10.1007/s11263-015-0816-y.
Erich Schubert. A triangle inequality for cosine similarity. In International Conference on Similarity Search
and Applications , pp. 32–44. Springer, 2021.
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush
Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered
400 million image-text pairs. arXiv preprint arXiv:2111.02114 , 2021.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In
Proceedings of the IEEE international conference on computer vision , pp. 618–626, 2017.
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hyper-
nymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 2556–2565, 2018.
16Published in Transactions on Machine Learning Research (10/2024)
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus
Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 15638–15650, 2022.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 conference on empirical methods in natural language processing , pp. 1631–1642,
2013.
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The German Traffic Sign Recognition
Benchmark: A multi-class classification competition. In IEEE International Joint Conference on Neural
Networks , pp. 1453–1460, 2011.
Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 2.0: A
continual pre-training framework for language understanding. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 34, pp. 8968–8975, 2020.
Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian
Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM , 59
(2):64–73, 2016.
Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , pp. 2495–2504, 2021.
Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning deep structure-preserving image-text embeddings. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 5005–5013, 2016.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and
uniformity on the hypersphere. In International Conference on Machine Learning , pp. 9929–9939. PMLR,
2020.
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan
Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all
vision and vision-language tasks. arXiv preprint arXiv:2208.10442 , 2022.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric
instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 3733–3742, 2018.
Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, and Bernt Schiele. Latent
embeddings for zero-shot classification. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 69–77, 2016.
Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring
a large collection of scene categories. International Journal of Computer Vision , 119:3–22, 2016.
Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi,
and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In Proceedings of the
IEEE conference on computer vision and pattern recognition , 2022.
Shuo Yang, Zhaopan Xu, Kai Wang, Yang You, Hongxun Yao, Tongliang Liu, and Min Xu. Bicro: Noisy
correspondence rectification for multi-modality data via bi-directional cross-modal similarity consistency.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 19883–
19892, 2023.
Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,
Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint
arXiv:2111.07783 , 2021.
17Published in Transactions on Machine Learning Research (10/2024)
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:
Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917 , 2022.
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong
Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv
preprint arXiv:2111.11432 , 2021.
18Published in Transactions on Machine Learning Research (10/2024)
A Detailed Training Hyper-parameters Used in Experiments in the Main
Manuscripts
HyperparametersValue for
Naive CLIPValue for
CLIP-Multi(32,16)Value for
Ablation Table 7Value for
Ablation Table 6
Batch size 32,768 32,768 2,048 2,048
Vocabulary size 30,522 30,522 30,522 30,522
Training epochs 32 32 15 15
Number [CLS] Tokens 1 16 1/8 2/8/32/128
Projection dims 512 32 512/64 256/64/16/4
Maximum temperature 100.0 3.95 100.0 100.0
Weight decay 0.2 0.2 0.5 0.5
Warm-up iterations 2,000 2,000 5,000 5,000
Peak Learning Rate 0.0005 0.0005 0.0005 0.0005
Adamβ1 0.9 0.9 0.9 0.9
Adamβ2 0.998 0.998 0.98 0.98
Adamϵ 10−810−810−810−8
Gradient global norm 1.0 1.0 1.0 1.0
GPUs 128 ×A100 128×A100 32 ×V100 32 ×V100
Train Time ∼5 days∼5 days ∼1 day ∼1 day
Table 5: Detailed hyper-parameters used for in the experimental analysis.
We provide the hyper-parameters employed in the experiments in Table 5. We follow most of the
hyper-parameters employed in the original CLIP (Radford et al., 2021) paper for both Naive CLIP re-
implementation and our multi-token and single-token product sphere implementation. We provide the details
of the hyper-parameters for large-scale and ablation experiments below.
Large-scale Experiments: We train with a batch size of 32,768 and the AdamW optimizer (Loshchilov
& Hutter, 2017) in all the large-scale experiments. We apply the standard training scheme of the origi-
nal CLIP model, which contains 32 epochs of training. We did not employ mixed precision to reduce the
possible overflow introduced by randomness for a stable reproduction. We set the β1= 0.9,β2= 0.998,
ϵ= 1e-8in AdamW, and weight decay = 0.2to further improve the stability. We use the cosine learning
rate decay scheme of peak learning rate equal to 5e-4, combined with a warmup period of 2,000 iterations.
For data augmentation, we only apply the RandomResizedCrop with a scale range of [0.8,1.0]. Finally, in
our multi-token product sphere implementation, we reduced the maximum temperature to 3.95 due to its
border distance range. This is a value obtained from the ablation study from Appendix E.
Ablation Experiments: We train with a batch size of 2,048 and the AdamW optimizer (Loshchilov &
Hutter, 2017) in all the ablation experiments. We apply a compact training scheme that updates the model
for 108,000 iterations, which is roughly equal to training the model for 15 epochs of the dataset. Since this
is a fast training scheme, we set the β1= 0.9,β2= 0.98,ϵ= 1e-8in AdamW, and weight decay = 0.5,
such that the training could converge faster in a stable approach. We use the cosine learning rate decay
scheme of peak learning rate equal to 5e-4, combined with a warmup period of 5,000 iterations. In the linear
probe evaluation, the hyperparameters follow the setup of MoCo v3 (Chen et al., 2021b). Concretely, we
use SGD without momentum and no weight decay. The learning rate is schemed by cosine decay with a
peak learning rate equal to 1.0, combined with a warmup period of 5 epochs. We train for 100 epochs and
augment the image using the RandomResizedCrop with a scale range of [0.75,1.0]andAutoAugment with the
code rand-m9-mstd0.5-inc1 . In the ablation experiments, we do not change the maximum temperature
clip value, leaving it the same for all topology configurations.
B Choices on structures and multi-token implementation
In Table 6, we modify the structure of the product sphere manifold under fixed total dimensions. It can be
seen that a higher mvalue (i.e.the number of product sub-spheres) is more likely to obtain a better zero-shot
classification accuracy and text-to-image retrieval recall. We, therefore, conjecture that the broader distance
19Published in Transactions on Machine Learning Research (10/2024)
Topology DistanceZero-Shot
I2T R@1Zero-Shot
T2I R@1Zero-Shot
Cls. Acc.Linear PrPSe
Cls. Acc.
Temperature, init= e2.64, gradient=True
Sphere(512) −uTv 48.3 31.45 30.62 60.38
PS(256, 2)−tr(uTv)48.0 32.25 30.33 60.52
PS(64, 8)−tr(uTv)52.3 32.89 30.70 60.32
PS(16, 32)−tr(uTv)50.4 33.01 30.93 60.79
PS(4, 128)−tr(uTv)48.2 32.91 30.57 59.99
Multi(256, 2)−tr(uTv)49.2 32.29 30.04 61.59
Multi(64, 8)−tr(uTv)54.0 34.27 31.93 62.41
Multi(16, 32)−tr(uTv)54.0 33.43 30.88 63.71
Table 6: The retrieval and classification performance of the proposed approach using different PSlique man-
ifold structures and the multi-token implementation. “gradient={True/False}” donates if the temperature
is learnable.
range helps the system reach equilibrium faster. However, an over-complicated structure such as PS(4,128)
could ruin the performance. The possible reason is that each sub-sphere Sd−1that is embedded in Rdhas
one less effective dimension. Therefore, the product sphere structure with large numbers of sub-spheres may
perform worse.
However, an over-complicated structure such as PS(4,128) could ruin the performance. We conjecture
that, since the sphere has one redundant dimension, the larger number of product sub-spheres reduces the
representation capacity of the topology. And because we employ a textual encoder that is gently larger than
the visual one, therefore the moderate reduction of the capacity helps overcome the overfitting on the textual
side.
We also provide the ablation results of different multi-tokens product sphere structure implementations
in the table’s lower half, denoted as Multi (·,·). We concatenate all the representations together for the
linear probe before projecting them to 1,000 class logit. It can be seen that the multi-token product sphere
implementationsoutperformtheirsingle-tokenversionsmostofthetime. Notably,sincetheincreasednumber
of parameters for the class tokens ( n×d) is negligible compared to that of the overall system, we consider the
participants of class tokens in global attention as the primary reason for the performance boost, compared
to their single-token version.
In addition, we discuss the computational complexity of the multi-token implementation. First, we consider
the cost of the logit matrix of the product sphere. The logit matrix is used to record the distance between
all pairs of samples, which is considered to be the major source of computational complexity when the batch
size is large ( e.g.∼231). In our case, given a batch size b, the spherical configure uses b2float numbers
while the Multi(n, m) structure uses b2m. Second, we provide information on the additional cost brought by
additional class tokens. Since we use the transformer structure, the flops (training/inference time) scale with
the number of parameters time numbers of tokens, plus the square of number of tokens we used. Concretely,
given a model with l-layers,d-hidden dimension, and t-tokens, the total flops can be roughly estimated as
t×l×(12×d2+ 2×t×d). The number of activations is roughly t×l×(36×d+ 6×t×d).
Overall, we should use a sufficient amount of sub-sphere to make the distance range able to cover the
"equilibrium" shown in Figure 1, intuitively. However, we need additional computational resources that scale
with the (square of) number of tokens. While there is no closed-form solution to the optimal number, we
consider using 8∼32tokens to be a practical choice.
C Detailed performance of configurations under different temperature initilization
In Table 7, we provide the detailed experimental results of Figure 2 in the main manuscripts. We further
provide the final temperature at the end of training and at what step the temperature converges (changes
20Published in Transactions on Machine Learning Research (10/2024)
Temp.
Init.Temp.
FinalConverge
StepZero-Shot
I2T R@1Zero-Shot
T2I R@1Zero-Shot
Cls. Acc.Linear
Cls. Acc.
Topology: Sphere, Distance: −uTv
2.659 4.033 18k 48.3 31.45 30.62 60.38
5.310 4.021 39k 46.8 31.13 29.60 59.82
1.000 3.976 22k 49.0 30.33 28.59 59.56
1.000 1.000 Detach 5.1 3.461 4.04 45.37
Topology: Euclidean, Distance: ∥u−v∥2
2.659 2.067 21k 47.9 32.29 30.36 59.90
5.310 5.107 1k 48.5 32.69 30.68 59.74
1.000 1.668 25k 47.4 30.71 29.85 60.09
1.000 1.000 Detach 47.6 30.43 29.51 59.20
Topology: PS(64, 8), Distance: Geo(u,v)
2.659 3.135 20k 50.7 32.35 30.79 60.43
5.310 3.168 55k 50.7 31.59 30.34 59.60
1.000 3.024 42k 49.9 32.49 30.21 60.61
1.000 1.000 Detach 4.1 2.921 3.10 21.67
Topology: PS(64, 8), Distance: −tr(uTv)
2.659 2.231 24k 52.3 32.89 30.70 60.32
5.310 2.280 57k 50.3 33.37 30.23 59.76
1.000 2.174 36k 50.9 32.71 30.50 60.66
1.000 1.000 Detach 30.3 18.48 21.20 57.93
Table 7: The retrieval and classification performance of different configurations under different temperature
initialization conditions. “Temp. Init.” denotes the values for initializing temperature; “Temp. Final”
denotes the final temperature at the end of training; “Converge Step” denotes the number of steps for
temperature starts to converge (changes less than 2% for an epoch.)
less than 2% for an epoch). It can be seen that the performance of the Euclidean topology is only slightly
affectedbytheinitializationofthetemperatures, andeventhoughthetemperatureisdetachedfromlearning,
it still performs reasonably well because of the unlimited distance range. At the same time, the spherical
and product sphere topologies are affected by how the temperature is initialized. However, a rough trend
can be seen that the faster the temperature converges, the better performance the model achieves, which
means the learnable temperature delays the learning of the methods. The model needs first to find a proper
temperature and then begin to learn representations well.
D Distribution of Learned Distance
WedepictthedistributionofdistanceforpairsofsamplesinFigure8. WefurtheremploytheRedCaps(Desai
et al., 2021) dataset as the out-domain data for visualizing the distributions of sample distances. As argued
in Section 3.2 of the main manuscripts, since the cross-modal contrastive loss does not handle the uni-modal
data distributions, the distance between negative pairs of images and texts could be much smaller than
that of a positive image-text pair, resulting in a tighter distance bound. Also, we can see this phenomenon
is much more severe in out-domain data, which could reduce the transferability of the feature embeddings
to downstream tasks. It is also notable that, the product sphere with the negative inner product as the
distance function learns similar distributions compared to the sphere reference, while the numerical values
of distances between samples are inherently larger without having multiplied with temperature.
E Additional Ablation on product sphere Structure
We provide more ablation results regarding the structure of the product sphere manifold under fixed total
dimensions in Table 8. We can observe that the PS(32, 32) configuration performs the best in general, while
21Published in Transactions on Machine Learning Research (10/2024)
(a)
(b)
Figure 8: Visualization of the distribution of distances between samples. The logits_pos andlogits_neg
denote the distances between positive and negative image-text pairs, respectively. The img_neg and
text_neg denote the distances between negative image-image and text-text pairs, respectively. The models
are trained using the Yfcc datasets, (a) and (b) depict the distribution of in-domain data (Yfcc) and out-
domain data (RedCaps), respectively.
the sphere with more 1024-dimensional embedding has slightly better linear probe performance. We also
notice that a more complicated structure provides better text-to-image retrieval results.
F Additional Results Using the TCL Framework
We combine our proposed method with the TCL model (Yang et al., 2022), which is one of the state-of-
the-art vision-language retrieval models that employ contrastive visual-textual alignment in its earlier stage.
During the pre-training, the TCL induces a mixture of in-modal and cross-modal contrastive losses, while
conducting the masked language modeling (MLM) and image-text matching tasks simultaneously. During
the testing, the cross-modal contrastive alignment head first lists sample pairs with high similarity scores,
and then these pairs are fed into the matching head to obtain the final matching scores. We alternate the
topologies of all the embedding spaces with PS(128,2). For the experimental analysis in this subsection,
we follow the configurations of the reference models, employ a collection of CC3M (Sharma et al., 2018),
MSCOCO Captions (Chen et al., 2015), Visual genome (Krishna et al., 2017) and SBU (Ordonez et al.,
22Published in Transactions on Machine Learning Research (10/2024)
Topology DistanceZero-Shot
I2T R@1Zero-Shot
T2I R@1Zero-Shot
Cls. Acc.Linear Probe
Cls. Acc.
Temperature, init= e2.64, gradient=True
Sphere(512)−uTv 48.3 31.45 30.62 60.38
Sphere(1024) −uTv 50.7 32.05 29.60 60.53
PS(128, 8)−tr(uTv)49.4 32.85 30.55 60.12
PS(64, 16)−tr(uTv)50.3 33.25 30.34 60.16
PS(32, 32)−tr(uTv)52.3 33.47 30.62 60.32
Table 8: The retrieval and classification performance of the proposed approach using different oblique man-
ifold structures and the multi-token implementation. “gradient={True/False}” donates if the temperature
is learnable.
2011) as the pre-training dataset, which contains roughly 4 million annotated image-text pairs. The models
are then evaluated using Flickr30k (Plummer et al., 2015) and MSCOCO Captions (Chen et al., 2015).
The results are shown in Table 9. Since our method does not affect the matching head, we also report
the performance of the contrastive alignment head. In general, our method improves the average recall
performance, but the improvement is not significant. We consider the reasons as i) The method (or recent
similar methods) employs pre-trained vision and language models, as well as a matching head and an MLM
head; hence it is less sensitive to the gradients from the contrastive alignment; ii) The datasets employed
for training contain less noise, while the training is scheduled with an overlength scheme (the zero-shot
performance does not increase in the last 5 epochs).
Additional Notes on TCL We also provide the comparison results with officially released checkpoints. It
can be seen that our implementation performs 0.5-1.0% worse than the official checkpoints. On the other
hand, our implementation has better alignment head performance. Since we are employing the codes released
in the official repository, the reason might be the following: i) Datasets difference, that we have ∼3000 fewer
images in the SBU dataset while owning 5000 more images in the CC3M dataset; ii) We resize the CC3M
dataset to short edge 500 pixels, while the official repository does not clearly provide the pre-processing
approach; iii) We implicitly have a short training time or smaller matching loss weight than the official
checkpoints due to the difference in the framework.
G Additional Results with more datasets
In Table 10, we evaluate our models with more out-domain datasets, which means, the training splits of
these datasets are not included in our train dataset. We employ the following datasets: Food101 (Bossard
et al., 2014), Cars (Krause et al., 2013), Cifar10/100 (Krizhevsky et al., 2009), Country211 (Thomee et al.,
2016) (a sub-set of the YFCC100M), FGVCAircraft (Maji et al., 2013), GTSRB (Stallkamp et al., 2011),
SUN397 (Xiao et al., 2016), RenderedSST2 (Socher et al., 2013), SVHN (Netzer et al., 2011), STL10 (Coates
et al., 2011), DTD (Cimpoi et al., 2014), OxfordIIIPet (Parkhi et al., 2012), Flowers102 (Nilsback & Zisser-
man, 2008), EuroSAT (Helber et al., 2019) and Caltech101 (Fei-Fei et al., 2006).
H Test of Mixture-of-Expert Hypothesis:
We investigate the mixture-of-expert hypothesis of the proposed method. Since the class token is considered
to encode the global representation of the sample, and all the class tokens are identical, i.e. use the same
positional embedding, the employment of multiple class tokens may function in a mixture-of-expert style.
That is, after training, each sub-sphere (or a subset of sub-spheres) in the product sphere structure is
capable of alignment. Then, the system functions as a mixture of weak alignment models (experts). To test
this hypothesis, we calculate the zero-shot classification performance of the CLIP[Multi(32,16)] model with
randomly selected subsets of sub-spheres. From Table 11, we find that the drop in performance is reasonably
small (∼12%) with half of the alignment tokens. This result reveals a possible mechanism of the product
23Published in Transactions on Machine Learning Research (10/2024)
Method
baseline[impl.]Flickr Coco
I2T
R@1T2I
R@1Recall
meanI2T
R@1T2I
R@1Recall
mean
Zero-shot performance.
TCL[official]93.00 79.60 93.97 71.40 53.50 79.49
(84.20) (67.10) (88.45) (55.40) (40.80) (69.92)
TCL[our-impl.]91.00 78.28 93.25 70.16 53.05 79.07
(83.30) (68.40) (88.73) (57.34) (43.21) (71.31)
TCL[PS(128,2)]91.20 78.14 93.29 70.14 53.35 79.14
(84.80) (67.86) (88.84) (57.10) (43.13) (71.32)
Fine-tuned performance.
TCL[official]94.90 84.00 95.57 75.60 59.00 82.87
(87.90) (71.38) (90.92) (65.34) (48.94) (76.53)
TCL[our-impl.]93.80 83.06 95.17 73.56 57.74 82.06
(88.30) (72.94) (91.27) (66.98) (50.34) (77.43)
TCL[PS(128,2)]93.80 82.90 95.18 74.78 57.72 82.13
(88.60) (73.26) (91.39) (65.60) (49.83) (76.86)
Table 9: Retrieval performance on Flickr30K and MSCOCO of our implemented TCL model and the variant
using our proposed method. The numbers in brackets are the performance obtained using the contrastive
alignment head.
Method
baseline[impl.]Food101 Cars Cifar10 Cifar100 Country211 FGVCAircraft GTSRB SUN397
ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1
ViT-B/16-224 as visual bone.
CLIP[openAI†] 88.7 64.8 90.8 66.9 22.8 24.2 43.4 64.4
CLIP[openCLIP‡] 86.1 83.7 91.7 71.2 18.1 17.7 43.5 69.6
CLIP[our-impl.] 80.8 67.2 94.8 74.0 16.3 10.7 48.5 50.6
CLIP[Multi(32,16)] 82.1 75.3 95.2 76.9 17.2 8.6 39.5 48.9
ViT-L/14-224 as visual bone for reference.
CLIP[openAI†] 93.1 77.9 95.6 75.8 31.9 31.8 50.6 67.6
CLIP[openCLIP‡] 90.1 89.6 94.6 77.4 23.0 24.9 49.9 72.6
Method
baseline[impl.]RenderedSST2 SVHN STL10 DTD OxfordIIIPet Flowers102 EuroSAT Caltech101
ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1ZS cls.
Acc@1
ViT-B/16-224 as visual bone.
CLIP[openAI†] 60.7 51.9 98.3 45.0 88.9 69.1 55.9 89.0
CLIP[openCLIP‡] 54.4 34.1 97.0 51.3 89.2 66.9 50.2 91.3
CLIP[our-impl.] 54.4 31.1 98.5 51.0 85.3 77.4 36.5 85.3
CLIP[Multi(32,16)] 49.4 33.1 99.1 54.0 80.7 73.4 40.3 86.2
ViT-L/14-224 as visual bone for reference.
CLIP[openAI†] 68.8 58.4 99.4 55.4 93.2 79.2 62.6 92.5
CLIP[openCLIP‡] 56.0 49.6 98.1 60.5 91.7 73.1 62.3 92.7
Table 10: Additional zero-shot classification results. None of the datasets listed are used as a part of the
training dataset.
sphere structure during optimization, where a subset of sub-spheres is priorly aligned. Therefore, some
tokens are learned to be more sensitive than others to specific objects (See the GradCAM visualization).
24Published in Transactions on Machine Learning Research (10/2024)
Image-Text Pair
 Full Tokens
 Token id=03
 Token id=10
Image-Text Pair
 Full Tokens
 Token id=04
 Token id=15
Image-Text Pair
 Full Tokens
 Token id=08
 Token id=12
Image-Text Pair
 Full Tokens
 Token id=04
 Token id=10
Figure 9: More visualization of the importance map using the Grad-CAM algorithm.
25Published in Transactions on Machine Learning Research (10/2024)
Image-Text Pair
 Token id=05
 Token id=06
 Token id=07
Image-Text Pair
 Token id=08
 Token id=09
 Token id=13
Image-Text Pair
 Token id=03
 Token id=07
 Token id=10
Image-Text Pair
 Token id=03
 Token id=09
 Token id=13
Figure 10: Failure cases of the importance map using the Grad-CAM algorithm.
26Published in Transactions on Machine Learning Research (10/2024)
#Tokens 1 2 4 8
Top1 Acc. 13.0±9.7 21.5±14.3 27.7±20.4 62.1±4.4
Table 11: ImageNet zero-shot classification performance of CLIP[Multi(32,16)] model using a randomly
selected subset of [CLS] tokens.
I More Visualization using GradCAM
In Figure 9, we provide more visualization results using GradCAM. In Figure 10, we show some failure cases
when the attention in the textual mode is focused on non-object words.
J A brief introduction to the product sphere (oblique) manifold
The oblique manifold is a mathematical structure used in various optimization problems, particularly in the
field of machine learning and signal processing. It is a type of manifold that arises when considering the set
of matrices with columns constrained to have a unit norm.
Definition : The product sphere manifold SP(n,p)is defined as the set of n×pmatricesX, where each
column ofXis a unit vector in Rn. Mathematically, it is expressed as:
SP(n,p) =X∈Rn×p:diag(XTX) =Ip
whereIpis thep×pidentity matrix.
Geometry : The product sphere manifold is a product of pspheresSn−1, each of dimension n−1, Therefore,
it can be thought of as (Sn−1)p.
Tangent Space : The tangent space at a point Xon the product sphere manifold is given by the set of
matricesZof the same size as X, such that each column of Zis orthogonal to the corresponding column of
X. Mathematically, this is expressed as:
TXSP(n,p) =Z∈Rn×p:diag(XTZ) = 0
Exponential Maps : Due to the product structure of spheres, the exponential map at a point Xin the
direction of a tangent vector Zcan be computed column-wise on each sphere.
K A note on the recent advance in noisy image-text matching
Recently, many pieces of research have been made to tackle the noisy image-text matching problem in con-
trast learning. Below, we provide a concise survey of these works, for the readers who want to know more
about this topic. Although these works may not be comparable with our proposed method, they still support
that noisy visual-textual correspondences are an important research topic in this field.
Chun et al. (2022) : This paper argues that existing ITM benchmarks have a significant limitation of
many missing correspondences. Then, it proposes a new dataset, ECCV Caption, to correct the massive
false negatives and a new metric, mAP@R, to evaluate VL models.
Li et al. (2023) : This paper proposes a method to correct false negatives by integrating language guidance
into the ITM framework. This framework corrects the locations of false negatives in the embedding space.
Chun (2023) : This paper also argues that the image-text matching task suffers from ambiguity due to
multiplicity and imperfect annotations. Then, this paper proposes an improved probabilistic ITM approach
that introduces a new probabilistic distance with a closed-form solution.
Huang et al. (2021) : This paper points out that the training data may contain mismatched pairs. To
learn the noisy correspondence, the authors divide the data into clean and noisy partitions and then rectify
the correspondence via an adaptive prediction model.
27Published in Transactions on Machine Learning Research (10/2024)
Qin et al. (2022) : This paper considers the major challenge in cross-modal retrieval is the noisy correspon-
dence in training data. This refers to the fact that some of the training pairs may not be correctly aligned,
i.e., the image and text do not actually correspond to each other. They propose a framework to address
this challenge by integrating two novel techniques: Cross-modal Evidential Learning and Robust Dynamic
Hinge.
Yang et al. (2023) : This paper proposes a general framework for cross-modal matching that can be easily
integrated into existing models and improve their robustness against noisy data. This framework estimates
soft labels for noisy data pairs by exploiting the consistency of cross-modal similarities.
Han et al. (2023) : The paper proposes a Meta Similarity Correction Network to provide reliable similarity
scores for cross-modal retrieval. The method learns to distinguish between positive and negative pairs of
data using meta-data, and can be used to remove noisy samples from the training dataset.
28