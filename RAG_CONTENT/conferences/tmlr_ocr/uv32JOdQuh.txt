Published in Transactions on Machine Learning Research (06/2023)
Invariant Feature Coding using Tensor Product Representa-
tion
Yusuke Mukuta mukuta@mi.t.u-tokyo.ac.jp
The University of Tokyo, RIKEN
Tatsuya Harada harada@mi.t.u-tokyo.ac.jp
The University of Tokyo, RIKEN
Reviewed on OpenReview: https: // openreview. net/ forum? id= uv32JOdQuh
Abstract
In this study, a novel feature coding method that exploits invariance for transformations
represented by a finite group of orthogonal matrices is proposed. We prove that the group-
invariant feature vector contains sufficient discriminative information when learning a linear
classifier using convex loss minimization. Based on this result, a novel feature model that
explicitly considers group action is proposed for principal component analysis and k-means
clustering, which are commonly used in most feature coding methods, and global feature
functions. Although the global feature functions are in general complex nonlinear functions,
the group action on this space can be easily calculated by constructing these functions
as tensor-product representations of basic representations, resulting in an explicit form of
invariant feature functions. The effectiveness of our method is demonstrated on several
image datasets.
1 Introduction
Feature coding is a method of calculating a single global feature by summarizing the statistics of the local
features extracted from a single image. After obtaining the local features {xn}N
n=1∈Rdlocal, a nonlinear
functionFandF=1
N/summationtextN
n=1˜F(xn)∈Rdglobalis used as the global feature. Currently, activations of
convolutional layers of pre-trained convolutional neural networks (CNNs), such as VGG-Net (Simonyan &
Zisserman, 2014), are used as local features, to obtain considerable performance improvement (Sánchez et al.,
2013; Wang et al., 2016). Furthermore, existing studies handle coding methods as differentiable layers and
train them end-to-end to obtain high accuracy (Arandjelovic et al., 2016; Gao et al., 2016; Lin et al., 2015).
Thus, feature coding is a general method for enhancing the performance of CNNs.
The invariance of images under geometric transformations is essential for image recognition because com-
pact and discriminative features can be obtained by focusing on the information that is invariant to the
transformations that preserve image content. For example, some researchers constructed CNNs with more
complex invariances, such as image rotation (Cohen & Welling, 2016; 2017; Worrall et al., 2017), and ob-
tained a model with high accuracy and reduced model parameters. Therefore, we expect to construct a
feature-coding method that contains highly discriminative information per dimension and is robust to the
considered transformations by exploiting the invariance information in the coding methods.
In this study, we propose a novel feature-coding method that exploits invariance. Specifically, we assume
that transformations Tthat preserve image content act as a finite group consisting of orthogonal matrices
on each local feature xn. For example, when concatenating pixel values in the image subregion as a local
feature, image flipping acts as a change in pixel values. Hence, it can be represented by a permutation matrix
that is orthogonal. Also, many existing CNNs, invariant to complex transforms, implement invariance by
restricting convolutional kernels such that transforms act as permutations between filter responses, which
can be represented by the orthogonal matrices. Therefore, the orthogonal assumption is not a strong one.
1Published in Transactions on Machine Learning Research (06/2023)
transformation that 
perserves image contents 
Local Feature Space
local feature 
extraction
orthogonal 
transformation
nonlinear mappingprojection to 
invariant space
Invariant Feature Space
 Encoded Feature Space
orthogonal 
transformation
Input Image
Figure 1: Overview of the proposed feature coding method. In the initially constructed global feature space,
π′(g)˜F(x) = ˜F(π(g)x)holds for some orthogonal π′. Subsequently, the projection is applied to the trivial
representation of P1to obtain the invariant global feature.
Ignoring the change in feature position, because global feature pooling is being applied, we construct a
nonlinear feature coding function Fthat exploitsT. Our first result is that when learning the linear
classifier using L2-regularized convex loss minimization on the vector space, where Tacts as an orthogonal
matrix, the learned weight exists in the subspace invariant under the Taction. From this result, we propose
a guideline that first constructs a vector space in which Tacts orthogonally on F(xn)to calculate the
T-invariant subspace.
On applying our theorem to feature coding, two problems occur when constructing the global feature. The
first problem is that, in general, ˜Fexhibits complex nonlinearity. The action of Ton the CNN features
can be easily calculated because CNNs consist of linear transformations and point-wise activation functions.
This is not the case for feature-coding methods. The second is that ˜Fhas to be learned from the training
data. When we encode the feature, we first apply principal component analysis (PCA) on xnto reduce
the dimension. Clustering is often learned using the k-means or Gaussian mixture model (GMM) and ˜Fis
calculated from the learned model. Therefore, we must consider the effect of Ton the learned model.
To solve these problems, we exploit two concepts of group representation theory: reducible decomposition
of the representation and tensor product of two representations. The former is the decomposition of the
action ofTonxninto a direct sum of irreducible representations. This decomposition is important when
constructing a dimensionality reduction method that is compatible with group actions. Subsequently, the
tensor product of the representations is calculated. The tensor product is a method by which we construct
a vector space where the group acts on the product of the input representations. Therefore, constructing
nonlinear feature functions in which group action can be easily calculated is important.
Based on these concepts, we propose a novel feature coding method and model training method that exploit
the group structure. We conducted experiments on image recognition datasets and observed an improvement
in the performance and robustness to image transformations.
The contributions of this study are as follows:
•We prove that when training the linear classifier on the space where the group acts orthogonally,
the learned weight lies in the invariant subspace.
•We propose a group-invariant extension to feature modeling and feature coding methods for groups
acting orthogonally on local features.
•We evaluated the accuracy and invariance of our methods on image recognition datasets.
2Published in Transactions on Machine Learning Research (06/2023)
2 Related Work
2.1 Feature coding
The covariance-based approach models the distributions of local features based on Gaussian distributions
and uses statistics as the global feature. For example, GLC (Nakayama et al., 2010a) uses the mean and
covariance of the local descriptors as the features. The global Gaussian (Nakayama et al., 2010b) method
appliesaninformation-geometricmetrictothestatisticalmanifoldoftheGaussiandistributionasasimilarity
measure. Bilinear pooling (BP) (Lin et al., 2015) uses the mean of self-products instead of the mean and
covariance, but its performance is similar. The BP is defined as F= vec/parenleftig
1
N/summationtextN
n=1xnx⊤
n/parenrightig
, where vec (A)
denotesthevectorthatstorestheelementsof A. BecauseofthesimplicityofBP,therearevariousextensions,
such as Lin & Maji (2017); Wang et al. (2017); Gou et al. (2018); Yu et al. (2020); Koniusz & Zhang (2021);
Song et al. (2021; 2023) which demonstrate better performance than the original CNNs without a feature
coding module. For example, improved bilinear pooling (iBP) uses the matrix square root as a global
feature. These works mainly focus on the postprocessing of the bilinear matrix given covariance information;
whereas, our motivation is to obtain the covariance information that is important to the classification. We
use invariance as the criterion for feature selection.
The vector of locally aggregated descriptors (VLAD) (Jégou et al., 2010) is a Cdlocal-dimensional vector that
uses k-means clustering with Creferring to the number of clustering components and consists of the sum of
the differences between each local feature and the cluster centroid µcto which it is assigned, expressed as
Fc=/summationtext
xn∈Sc(xn−µc);Scis the set of local descriptors that are assigned to the c-th cluster. The vector
of locally aggregated tensors (VLAT) (Picard & Gosselin, 2013) is an extension of VLAD that exploits
second-order information. VLAT uses the sum of tensor products of the differences between each local
descriptor and cluster centroid µc:Fc= vec/parenleftbig/summationtext
xn∈Sc(xn−µc)(xn−µc)⊤−Tc/parenrightbig
, whereTcis the mean of
(xn−µc)(xn−µc)⊤ofallthelocaldescriptorsassignedtothe c-thcluster. VLATcontainsinformationsimilar
to the full covariance of the GMM. The Fisher vector (FV) (Sánchez et al., 2013) also exploits second-order
information but uses only diagonal covariance. One work has also exploited local second-order information
with lower feature dimensions using local subspace clustering (Dixit & Vasconcelos, 2016).
2.2 Feature extraction that considers invariance
One direction for exploiting the invariance in the model structure is to calculate all transformations and
subsequently apply pooling with respect to the transformation to obtain the invariant feature. TI-pooling
(Laptev et al., 2016) first applies various transformations to input images, subsequently applies the same
CNNs to the transformed images, and finally applies max-pooling to obtain the invariant feature. Anselmi
etal.(2016)alsoproposedalayerthataveragestheactivationswithrespecttoallconsideredtransformations.
RotEqNet (Marcos et al., 2017) calculates the vector field by rotating the convolutional filters, lines them
up with the activations, and subsequently applies pooling to the vector fields to obtain rotation-invariant
features. Group equivariant CNNs (Cohen & Welling, 2016) construct a network based on the average of
activations with respect to the transformed convolutional filters.
Another direction is to exploit the group structure of transformations and construct the group feature using
group representations. Harmonic networks (Worrall et al., 2017) consider continuous image rotation in
constructing a layer with spherical harmonics, which is the basis for the representation of two-dimensional
rotation groups. Steerable CNNs (Cohen & Welling, 2016) construct a filter using the direct sum of the
irreducible representations of the D4 group, to reduce model parameters while preserving accuracy. Weiler
et al. (2018) implemented steerable CNNs as the weighted sum of predefined equivariant filters and extended
it to finer rotation equivariance. Variants of steerable CNNs are summarized in (Weiler & Cesa, 2019).
Jenner & Weiler (2022) constructed the equivariant layer by combining the partial differential operators.
These works mainly focus on equivariant convolutional layers, whereas we mainly focus on invariant feature
coding given equivariant local features. Kondor et al. (2018) also considered tensor product representation
to construct DNNs that are equivariant under 3D rotation for the recognition of molecules and 3D shapes.
While Kondor et al. (2018) directly uses tensor product representation as the nonlinear activation function,
we use tensor product representation as the tool for extending the existing coding methods for equivariance.
3Published in Transactions on Machine Learning Research (06/2023)
Therefore, we introduce several equivariant modules, in addition to tensor product representation. The
difficulty of invariant feature coding is in formulating complex nonlinear feature coding functions that are
consistent with the considered transformations.
Furthermore, there exists the approach to learn invariance from training data instead of deciding the invari-
ance beforehand. Rao & Ruderman (1998); Miao & Rao (2007) learned the group that preserves training
data unsupervisedly by considering reconstruction loss between the original image and the transformed im-
age assuming Gaussian noise. ?used the matrix exponential and applied the matrix backpropagation to
optimize the reconstruction loss. Sohl-Dickstein et al. (2010) proposed an adaptive smoothing method to
obtain good local minima. Benton et al. (2020) tried to learn the neural network to be invariant under data
augmentation by using consistency regularization. Lin et al. (2021) trained the auto-encoder parameterized
by the Lie group. Chau et al. (2022) combined multiple 2D rotation groups with sparse coding to model the
transformation group.
Another approach similar to ours is to calculate the nonlinear invariance with respect to the transformations.
Reisert & Burkhardt (2006) considered the 3D rotation invariance and used the coefficients with respect to
spherical harmonics as the invariant feature. Kobayashi et al. (2011) calculated a rotation-invariant feature
using the norm of the Fourier coefficients. Kakarala (2012) used a bilinear feature with respect to 3D
rotation group. Morère et al. (2017) proposed group-invariant statistics by integrating all the considered
transformations, such as Eq. (1) for image retrieval without assuming that the transformations act linearly.
Kobayashi (2017) exploited the fact that the eigenvalues of the representation of flipping are ±1and used
the absolute values of the coefficients of eigenvectors as the features. Ryu et al. (2018) used the magnitude
of two-dimensional discrete Fourier transformations as the translation-invariant global feature. Compared
to these approaches that manually calculate the invariants, our method can algorithmically calculate the
invariants given the transformations, in fact calculating all the invariants after constructing the vector space
on which the group acts. Therefore, our method exhibits both versatility and high classification performance.
In summary, our work is an extension of existing coding methods, where the effect of transformations and
all the invariants are calculated algorithmically. Compared with existing invariant feature learning methods,
we mainly focus on invariant feature coding from the viewpoint of group representation theory.
3 Overview of Group Representation Theory
In this section, we present an overview of the group representation theory that is necessary for constructing
our method. The contents of this section are not original and can be found in books on group representation
theory (Fulton & Harris, 2014). We present the relationship between the concepts explained in this section
and the proposed method as follows:
•Group representation is the homomorphism from the group element to the matrix, meaning that
the linear action is considered as the effect of transformations.
•Group representation can be decomposed into the direct sum of irreducible representations. The
linear operator that commutes with group action is restricted to the direct sum of linear operators
betweenthesameirreduciblerepresentations. TheseresultsareusedtoformulatePCAthatpreserves
equivariance in Section 4.2. Further, this irreducible decomposition is also used when calculating
the tensor product.
•We obtain a nonlinear feature vector for the group that acts linearly by considering tensor product
representation. Furthermore, the invariant vectors can be determined by exploring the subspace
with respect to the trivial representation 1, which can be obtained by Eq. (1). These results can be
used to prove Theorem 1 and construct the feature coding in Section 4.3.
3.1 Theory
Group representation When setGand operation◦:G×G→Gsatisfy the following properties:
4Published in Transactions on Machine Learning Research (06/2023)
•◦is associative: g1,g2,g3∈Gsatisfies ((g1◦g2)◦g3) = (g1◦(g2◦g3))
•The identity element e∈Gexists and satisfies g◦e=e◦g=gfor allg∈G
•Allg∈Gcontain the inverse g−1that satisfies g◦g−1=g−1◦g=e
the pairG= (G,◦)is called a group. The axioms above are abstractions of the properties of the trans-
formation sets. For example, a set consisting of 2D image rotations associated with the composition of
transformations form a group. When the number of elements in Gwritten as|G|is finite,Gis called a finite
group. As mentioned in Section 1, we consider a finite group.
Now, we consider a complex vector space Cdto simplify the theory; however, in our setting, the proposed
global feature is real. The space of bijective operators on Cdcan be identified with the space of d×dregular
complex matrices written as GL(d,C), which is also a group with the matrix product as the operator. The
homomorphism πfromGtoGL(d,C)is the mapping π:G→GL(d,C), which satisfies
•Forg1,g2∈G,π(g1)◦π(g2) =π(g1◦g2)
•π(e) = 1d×d
and is called the representation of GonCd, where 1d×ddenotes the d-dimensional identity matrix. The
space in which the matrices act is denoted by (π,Cd). The representation that maps all g∈Gto1d×dis
called a trivial representation. The one-dimensional trivial representation is denoted as 1. When all π(g)
are unitary matrices, the representation is called a unitary representation. Furthermore, when the π(g)s
are orthogonal matrices, this is called an orthogonal representation. The orthogonal representation is also a
unitary representation. In this study, transformations are assumed to be orthogonal.
Intertwining operator For two representations (π,Cd)and(π′,Cd′), a linear operator A:Cd→Cd′
is called an intertwining operator if it satisfies π′(g)◦A=A◦π(g). This implies that (π′,ACd)is also
a representation. Thus, when applying linear dimension reduction, the projection matrix must be an in-
tertwining operator. We denote the space of the intertwining operator as HomG(π,π′). When a bijective
A∈HomG(π,π′)exists, we write π≃π′. This implies that the two representations are virtually the same
and that the only difference is the basis of the vector space.
Irreducible representation Given two representations πandσ, the mapping that associates gwith the
matrix to which we concatenate π(g)andσ(g)in block-diagonal form is called the direct sum representation
ofπandσ, written as π⊕σ. When the representation πis equivalent to some direct sum representation,
πis called a completely reducible representation. The direct sum is the composition of the space on which
the group acts independently. Therefore, a completely reducible representation can be decomposed using
independent and simpler representations. When the representation is unitary, the representation that cannot
be decomposed is called an irreducible representation, and all representations are equivalent to the direct sum
oftheirreduciblerepresentations. Theirreduciblerepresentation τtisdeterminedbythegroupstructure, and
πis decomposed into π≃n1τ1⊕n2τ2⊕...nTτT, whereTis the number of different irreducible representations
andntτtisnttimes direct sum of τt. When we denote the characteristic function of gasχπ(g) = Tr(π(g)),
we can calculate these coefficients as nt=1
|G|/summationtext
g∈Gχπ(g)χτt(g). Furthermore, the projection operator Pτ
onntτtis calculated as Pτt=dim(τt)1
|G|/summationtext
g∈Gχτt(g)π(g). Specifically, because χ1(g) = 1, we can calculate
the projection to the trivial representation using
P1=1
|G|/summationdisplay
g∈Gπ(g). (1)
This equation reflects the fact that the average of all π(g)is invariant to group action. Schur’s lemma
indicates that HomG(τt1,τt2) ={0}ift1̸=t2andHomG(τt,τt) =CAfor some matrix A.
5Published in Transactions on Machine Learning Research (06/2023)
Invariant Classifier
 rotationtrain
classiferOriginal Feature Space Decomposed Feature Space Decomposed Feature Space
flip
flip
flipped image
original image
Figure 2: Example of the irreducible decomposition and learned classifier.
Tensor product representation Finally, tensor product representation is important when constructing
nonlinear feature functions. Given πandσ, the mapping that associates gwith the matrix tensor product
ofπ(g),σ(g)is the representation of the space of the tensor product of the input spaces. We denote the
tensor product representation as π⊗σ. The tensor product of the unitary representation is also unitary. The
important properties of the tensor representation are as follows: (i) it is distributive, where (π1⊕π2)⊗(π3⊕
π4) = (π1⊗π3)⊕(π2⊗π3)⊕(π1⊗π4)⊕(π2⊗π4), and (ii)χπ⊗σ(g) =χπ(g)χσ(g). Thus, the irreducible
decomposition of tensor representations can be calculated from the irreducible representations.
3.2 Illustrative example
The group representation theory described above is visualized in a simple setting in this section.
Group consisting of identity mapping and image flipping. We consider group consists of identity
mappingeand horizontal image flipping m. Because the original image was obtained by applying image
flipping twice, it follows that e◦e=e,e◦m=m,m◦e=m,m◦m=e. This definition satisfies the
following three properties of a group by setting m−1=m:
•◦is associative: g1,g2,g3∈Gsatisfy ((g1◦g2)◦g3) = (g1◦(g2◦g3)).
•The identity element e∈Gexists and satisfies g◦e=e◦g=gfor allg∈G.
•Allg∈Gcontain the inverse g−1that satisfy g◦g−1=g−1◦g=e.
The irreducible representations are τ1:τ1(e) = 1,τ1(m) = 1andτ−1:τ−1(e) = 1,τ−1(m) =−1. This is
proven as follows: the τs defined above satisfies the representation conditions
•Forg1,g2∈G,τ(g1)◦τ(g2) =τ(g1◦g2).
•τ(e) = 1d×d.
where 1-dimensional representations are trivially irreducible. When the dimension of the representation
space is greater than 1, π(e) = 1d×dandπ(m) =A∈GL(d,C). We apply eigendecomposition of Ato obtain
A=S−1ΛS, where Λis diagonal, and we denote the i-th diagonal elements as λi. Becauseπ(e) =S−1Id×dS
andπ(m) =S−1ΛS,πis decomposed as a direct sum representation of πi(e) = 1andπi(m) =λifor
eachi-th dimension, this representation is not irreducible. Thus, the irreducible representation must be
1-dimensional. Moreover, π(m)2=π(e) = 1. Therefore, π(m)is 1 or -1 and πcan be deomposed into the
direct sum of τ1andτ−1.
6Published in Transactions on Machine Learning Research (06/2023)
Image feature and its irreducible decomposition. As an example of the image feature, we use con-
catenation of the luminosity of two horizontally adjacent pixels as a local feature and apply average pooling
to get a global feature. Thus, the feature dimension is 2. Since image flipping changes the order of the pixels,
it permutates the first and second elements in feature space. Therefore, the group acts as π(e) =/parenleftbigg1 0
0 1/parenrightbigg
,
π(m) =/parenleftbigg0 1
1 0/parenrightbigg
, as plotted in the left figure of Figure 2. By applying orthogonal matrices calculated by
eigendecomposition, the feature space written in the center of Figure 2 is obtained. In this space, the group
acts asπ(e) =/parenleftbigg1 0
0 1/parenrightbigg
,π(m) =/parenleftbigg1 0
0−1/parenrightbigg
. Thus, this is the irreducible decomposition of πintoτ1⊕τ−1
defined by the Irreducible representation paragraph in the previous subsection. Note that in the gen-
eral case, each representation matrix is block-diagonalized instead of diagonalized, and each
diagonal block becomes more complex, like those in Tables 1 and 7.
Tensor product representation. Subsequently, we consider the “product” of feature spaces to get non-
linear features. Two feature spaces (h1
a,h2
a)and(h1
b,h2
b)withπacting asπ(e) =/parenleftbigg1 0
0 1/parenrightbigg
,π(m) =/parenleftbigg0 1
1 0/parenrightbigg
on both spaces are considered. Any input spaces can be used whenever these spaces and πsatisfy the above
condition. For example, we use the same feature space for (h1
a,h2
a)and(h1
b,h2
b)to obtain bilinear pooling.
The tensor product of these spaces becomes a 4-dimensional vector space consisting of h1
ah1
b,h1
ah2
b,h2
ah1
b
andh2
ah2
b. Sincempermutates h1
aandh2
a,h1
bandh2
bsimultaneously, macts as a permutator of h1
ah1
band
h2
ah2
b,h1
ah2
bandh2
ah1
bsimultaneously. Thus, π(m)is written as π(m) =
0 0 0 1
0 0 1 0
0 1 0 0
1 0 0 0
. Moreover, tensor
product space is also the group representation space. This space can also be decomposed into irreducible
representations, such as π(e) =
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
andπ(m) =
1 0 0 0
0 1 0 0
0 0−1 0
0 0 0−1
.
Intertwining operator Given (π,C4)and (π′,C2)whereπ(e) =
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
andπ(m) =

1 0 0 0
0 1 0 0
0 0−1 0
0 0 0−1
andπ′(e) =/parenleftbigg1 0
0 1/parenrightbigg
,π′(m) =/parenleftbigg1 0
0−1/parenrightbigg
, intertwining operator from πtoπ′maps
τ1elements to τ1elements and τ−1elements to τ−1elements and therefore is represented by the form/parenleftbigga11a12 0 0
0 0a23a24/parenrightbigg
whereaijs are complex.
4 Invariant Tensor Feature Coding
In this section, the proposed method is explained. Our goal was to construct an effective feature function
F=1
N/summationtextN
n=1˜F(xn), where the transformations that preserve the image content act as a finite group of
orthogonal matrices on xn. Hence, we prove a theorem that reveals the condition of the invariant feature with
sufficient discriminative information in Section 4.1. Subsequently, the feature modeling method necessary
for constructing the coding in Section 4.2, is explained. Our proposed invariant feature function is described
in Section 4.3. In Section 4.4, we discuss the limitations of this theorem and extend it under more general
assumptions. In Section 4.5, the effectiveness of the proposed method is discussed in an end-to-end setting.
7Published in Transactions on Machine Learning Research (06/2023)
4.1 Guideline for the invariant features
4.1.1 Theory
First, to determine what an effective feature is, we prove the following theorem:
Theorem 1. We denote byCthe set of image categories, W∈Rd×|C|the linear classification weight whose
c-th column vector corresponds to the weight for c-th category,∥·∥Fthe Frobenius norm of the matrix and
l:R|C|×C→ Rbe the loss function convex with respect to the first argument. Let the finite group Gact as an
orthogonal representation πonRdand preserve the distribution of the training data {(vm,ym)}M
m=1∈Rd×C,
which implies that {(vm,ym)}M
m=1∈Rd×Cexhibits the same distribution as {(π(g)vm,ym)}M
m=1∈Rd×C
for anyg. The solution of the L2-regularized convex loss minimization
arg min
W∈Rd×|C|λ
2∥W∥2
F+1
MM/summationdisplay
m=1l(W⊤vm,ym) (2)
isG-invariant, implying that π(g)W=Wfor anygand therefore P1W=W.
TheG-invariance of the training data corresponds to the fact that gdoes not change the image content. From
another viewpoint, this corresponds to data augmentation that uses the transformed images as additional
training data. For example, given the original dataset {(uo,yo)}O
o=1and a constructed augmented dataset
{(vm,ym)}M
m=1as{(π(g)uo,yo)}O
o=1,g∈G, this augmented dataset is invariant under the action of π(g)for
anyg. We intend to use this theorem by regarding the global feature Fasvm, but it is applicable whenever
the assumption holds. The proof is as follows.
Proof.The non-trivial unitary representation τsatisfies/summationtext
g∈Gτ(g) = 0. This is because if we assume
that/summationtext
g∈Gτ(g) =A̸= 0, there exists vthat satisfies Av̸= 0, and CAvis a one-dimensional G-invariant
subspace. It violates the irreducibility of π. As (π,Rd)is a unitary representation, it is completely reducible.
We denote the ntτtelements of WandvmasW(t)andv(t)
m, respectively, where the decomposition of W
is applied column-wise. It follows that W=/summationtextT
t=1W(t)andvm=/summationtextT
t=1v(t)
m. Furthermore, since W(t)s
andv(t)
ms for different ts lie in a different subspace with respect to the irreducible decomposition, they are
orthogonal among different ts. It follows that,
1
MM/summationdisplay
m=1l(W⊤vm,ym)
=1
MM/summationdisplay
m=1l/parenleftigg
Re/parenleftiggT/summationdisplay
t=1/parenleftig
W(t)/parenrightig⊤
v(t)
m/parenrightigg
,ym/parenrightigg
=1
M|G|M/summationdisplay
m=1/summationdisplay
g∈Gl/parenleftigg
Re/parenleftiggT/summationdisplay
t=1/parenleftig
W(t)/parenrightig⊤
τt(g−1)v(t)
m/parenrightigg
,ym/parenrightigg
=1
M|G|M/summationdisplay
m=1/summationdisplay
g∈Gl/parenleftigg
Re/parenleftiggT/summationdisplay
t=1/parenleftig
τt(g)W(t)/parenrightig⊤
v(t)
m/parenrightigg
,ym/parenrightigg
≥1
MM/summationdisplay
m=1l
T/summationdisplay
t=1Re

1
|G|/summationdisplay
g∈Gτt(g)W(t)
⊤
v(t)
m
,ym

=1
MM/summationdisplay
m=1l/parenleftbigg/parenleftig
W(1)/parenrightig⊤
v(1)
m,ym/parenrightbigg
, (3)
where Reis the real part of a complex number and W(1)andv1
mare the trivial representation elements
under the irreducible decomposition of Wandvmrespectively, which can be obtained by applying the
8Published in Transactions on Machine Learning Research (06/2023)
projection matrix calculated by Eq. (1). The first equation originates from the orthogonality of W(t)s
andv(t)
ms among different ts; the second equation comes from G-invariance of the training data such that/summationtextM
m=1f(vm,ym) =/summationtextM
m=1f(π(g)vm,ym)holds for any f,gfrom the assumption; and the third comes from
the unitarity of τt(g). The inequality comes from the convexity of l, additivity of Re, and inner products.
The final equality comes from the fact that the average of τt(g)is equal to 0for nontrivial τt;W(1)andv1
m
are real. Therefore, this proof exploits the properties of convex functions and group representation theory.
Combined with the fact that ∥W∥2
F≥∥W(1)∥2
F, the loss value of Wis larger than that of W(1). Therefore,
the solution isG-invariant.
We can also prove the same results by focusing on the subgradient, which is described in the Supplementary
Material section. Facts from representation theory are necessary for both cases.
This theorem indicates that the complexity of the problem can be reduced by imposing invariance. Because
the generalization error increases with complexity, this theorem explains one reason that invariance con-
tributes to the good test accuracy. Sokolic et al. (2017) analyzed the generalization error in a similar setting,
using a covering number. This work calculated the upper bound of complexity; whereas, we focus on the
linear classifier, obtain the explicit complexity, and calculate the learned classifier. Furthermore, even when
the obtained classifier is the same as that trained by the augmented samples, the invariant features are more
suitable for training owing to the reduced number of feature dimension and training samples, which may
affect the performance of the learned feature. Moreover, we can apply a larger size of code words for the
clustering-based methods when we match the global feature dimension. Hence, our goal is to construct a
feature that is invariant in the vector space where the group acts orthogonally.
4.1.2 Illustrative example
Theory1isillustratedinthesettingofSection3.2. Inthisdecomposedspace, whentheclassifieristrainedby
considering both original images and flipped images, the classification boundary learned with L2-regularized
convex loss minimization acquires the form of x=b, as plotted in the right figure of Figure 2. Thus, we
can discard y-elements of features and obtain a compact feature vector. This result is validated as follows:
when the feature in the decomposed space that corresponds to the original image is denoted as (v1,v2), the
feature corresponding to the flipped image becomes (v1,−v2)becauseπ(m) =/parenleftbigg1 0
0−1/parenrightbigg
. Expressing the
linear classifier as w1v1+w2v2+b, the loss for these two images can be written as l(w1v1+w2v2+b) +
l(w1v1−w2v2+b). From Jensen’s inequality, this is lower-bounded by 2l(w1v1+b). This means that the
loss is minimized when w2= 0, resulting in the classification boundary w1v1+b= 0.w1is invariant under
the action of π(e)andπ(m).
4.2 Invariant feature modeling
First, a feature-modeling method is constructed for the calculation of invariant feature coding.
Invariant PCA PCA attempts to determine the subspace that maximizes the sum of the variances of
the projected vectors. The original PCA is a solution to maxU⊤U=ITr/parenleftig
U⊤1
N/summationtextN
n=1(xn−µ)(xn−µ)⊤U/parenrightig
,
whereµ=1
N/summationtextN
n=1xn. The solution is a matrix consisting of the eigenvectors of1
N/summationtextN
n=1(xn−µ)(xn−µ)⊤
that correspond to the top eigenvectors.
Our method can only utilize the projected low-dimensional vector if it also lies in the representation space
of the considered group. Therefore, in addition to the original constraint U⊤U=I, we assume that Uis an
intertwining operator in the projected space. From Schur’s lemma described in Section 3, the Uthat satisfies
these conditions is the matrix obtained using the projection operator Pτtwith the dimensionality reduced to
ntτt. Furthermore, when ntτt=/circleplustextnt
o=1τt,oandtheτt,o-thelementof xnasx(t,o)
n, thedimensionalityreduction
withinntτtis in the form of x(t)
n→/summationtextnt
o=1u(t,o)x(t,o)
nforu(t,o)∈Cbecause of Schur’s lemma. Hence, our
basic strategy is to first calculate u(t,o)s that maximize the variance with the orthonormality preserved and
subsequently choose tfor larger variances per dimension. In fact, u(t,o)can be calculated using PCA with
9Published in Transactions on Machine Learning Research (06/2023)
Algorithm 1 Calculation of Invariant PCA
Input:{xn}N
n=1∈Rd,G,dproj
Output:U∈Rd×dprojwhich is intertwining and orthonormal
fort= 1toTdo
foro1,o2= 1tontdo
Σ(t)o1,o2←1
N/summationtextN
n=1/angbracketleftig
x(t,o1)
n−µ(t,o1),x(t,o2)
n−µ(t,o2)/angbracketrightig
Rend for
(λ(t)
p,u(t)
p)←eigendecomposition of Σ(t).
end for
U=empty matrix
whilesize ofUis smaller than d×dprojdo
U←concat ofUand/parenleftig
u(t)
p⊗Idτt/parenrightig
◦Pτtfor non-used p,twith maximum λ(t)
p/dτt
end while
Algorithm 2 Calculation of invariant k-means
Input:{xn}N
n=1∈Rd,G,C
Output:µg,cforg∈G,c∈{1,...,C}
randomly initialize µe,cforc∈{1,...,C}
forit = 1tomaxiterdo
µg,c←π(g)µe,cforg∈G,c∈{1,...,C}
Sg,c←{n|(g,c) = arg min
(g,c)∥xn−µg,c∥}
µe,c←1/summationtext
g∈G|Sg,c|/summationtext
g∈G/summationtext
n∈Sg,cπ(g)−1xnforc∈{1,...,C}.
end for
the sum of covariances between each dimensional element of x(t,o)
n. This algorithm is presented in Algorithm
1. Because the projected vector must be real, additional care is required when certain elements of τtare
complex. The modification for this case are described in the Supplementary Materials section. In the
experiment, we use groups in which all irreducible representations can be real; thus, τtcan be used directly.
As for computational complexity, standard PCA requires square order of the input feature dimen-
sion times the number of input features plus cubic order of the input feature dimension and therefore
O/parenleftbigg
N/parenleftig/summationtextT
t=1ntdim(τt)/parenrightig2
+/parenleftig/summationtextT
t=1ntdim(τt)/parenrightig3/parenrightbigg
. On the other hand, the proposed invariant PCA requires
the computation of PCA for each irreducible representation and therefore O/parenleftig/summationtextT
t=1/parenleftbig
Nn2
tdim(τt) +n3
t/parenrightbig/parenrightig
.
Note that PCA considers the 2D rotation using Fourier transformation and has been applied to cryo-EM
images (Zhao & Singer, 2013; Vonesch et al., 2015; 2013; Zhao et al., 2016). The proposed PCA can
be regarded as an extension of the general noncommutative group. Other studies (Zhao & Singer, 2013;
Vonesch et al., 2015; 2013; Zhao et al., 2016) apply PCA for direct image modeling, whereas we regard our
proposed PCA as a preprocessing method that preserves the equivariance property for successive feature
coding.
Invariant k-means The original k-means algorithm is calculated as minµ/summationtextN
n=1minC
c=1∥xn−µc∥2. To
ensure thatGacts orthogonally on the learned model, we simply retain the cluster centroids by applying all
transformations to the original centroid. This implies that we learn µg,cforg∈G,c∈{1,...,C}such that
µg,csatisfiesπ(g1)−1µg1,c=π(g2)−1µg2,cfor allg1,g2,c. This algorithm is listed in Algorithm 2. As for
computational complexity, the proposed k-means requires |G|times as large computation complexity as the
original k-means since the size of the code words becomes |G|times as large.
10Published in Transactions on Machine Learning Research (06/2023)
4.3 Invariant feature coding
Subsequently, we construct a feature coding function Fas aG-invariant vector in the space where Gacts
orthogonally. First, the spaces of the function xnthatGact orthogonally are calculated as the basis spaces
for constructing more complex representation spaces using the tensor product. The first basis space is that
ofxn. The second is the C|G|-dimensional 1-of-k vector that we assign xnto the nearest µg,c. When we
applyπ(g)toxn, the nearest µisµg,cwhich corresponds to the vector to which we apply π(g)on the nearest
µtoxn. Therefore, gacts as a permutation. We denote this representation by 1µ(g).
Invariant BP Because BP is written as the tensor product of xn, we use
F=vec/parenleftigg
1
NN/summationdisplay
n=1P1(xn⊗xn)/parenrightigg
, (4)
as the invariant global feature. Although P1is a projection onto the trivial representation defined by Eq. (1),
we can calculate this feature from the irreducible decomposition of xnand the irreducible decomposition
of the tensor products. Normalization can also be applied to the invariant covariance with respect to each
P1(τt,o⊗τt,o)-th element to obtain a variant of BP, such as iBP (Lin & Maji, 2017). Because elements that
are not invariant are discarded, the feature dimensions become smaller.
Invariant VLAD Subsequently, we propose an invariant version of VLAD. The first difficulty is that
VLAD is not in the form of a tensor product, and second, the effect of transformation on the nearest code
word is not consistent. We solved the first difficulty by considering VLAD as the tensor product of the
local features and 1-of-k representation, and the second difficulty was solved by using the invariant k-means
proposed in Section 4.2 to learn the code word.
The expression for the invariant VLAD is as follows:
F=vec/parenleftigg
1
NN/summationdisplay
n=1P1(1µ(xn)⊗(xn−µc))/parenrightigg
, (5)
whereµcdenotes the nearest centroid to xn. Because 1µ(xn)is a vector in which only the element corre-
sponding to the nearest µis1, the tensor product becomes the vector in which the elements corresponding
to the nearest µarexn−µc, which is the same as the original VLAD. Because both 1µ(xn)andxn−µcare
orthogonal representation spaces, this space is also an orthogonal representation space. Although the size
of the codebook becomes |G|times larger, the dimensions of the global feature are not as large because we
used the invariant vector.
Illustrative example of invariant k-means and VLAD As shown in Figure 3, when the learned
codebook is µifori= 1,2,3,x1andx2are both assigned to µ1. When these features are flipped, the flipped
x1is assigned to µ3, whereas the flipped x2is assigned to µ2. Thus, image flipping does not act consistently
on the assignment vector 1µ(xn). As plotted in Figure 4, when the codebook is learned such that there
existsµwithy-element flipped for each code word, the group acts consistently on 1µ(xn). This is because
wheneverxnis assigned to µe,i, the flipped xnis assigned to µm,i. In addition, the flipped xnis assigned to
µe,iwhenxnis assigned to µm,i. Therefore, the group acts orthogonally on 1µ(xn).
Invariant VLAT Finally, we propose the invariant VLAT that incorporates local second-order statistics.
This feature can be calculated by combining the two aforementioned features.
F=vec/parenleftigg
1
NN/summationdisplay
n=1P1(1µ(xn)⊗((xn−µc)⊗(xn−µc)−Tc))/parenrightigg
. (6)
ThedimensionoftheinvariantVLATwith C|G|componentsisthesameasthatofVLATwith Ccomponents.
Thus, we can model complex nonlinear statistics and calculate the invariants using the tensor product
representation when using local polynomial statistics as a global feature.
11Published in Transactions on Machine Learning Research (06/2023)
Decomposed Feature Space
flipflippedflipped
Figure 3: VLAD with code words
learned by k-means.
flipflippedflippedDecomposed Feature SpaceFigure 4: Invariant VLAD with code words
learned by proposed k-means.
Table 1: Irreducible representations of the D4 group.
Rep. er r2r3mmrmr2mr3
τ1,1 1 1 1 1 1 1 1 1
τ1,−11 1 1 1 -1 -1 -1 -1
τ−1,11 -1 1 -1 1 -1 1 -1
τ−1,−11 -1 1 -1 -1 1 -1 1
τ2/bracketleftbigg
1 0
0 1/bracketrightbigg/bracketleftbigg
0−1
1 0/bracketrightbigg/bracketleftbigg
−1 0
0−1/bracketrightbigg/bracketleftbigg
0 1
−1 0/bracketrightbigg/bracketleftbigg
−1 0
0 1/bracketrightbigg/bracketleftbigg
0 1
1 0/bracketrightbigg/bracketleftbigg
1 0
0−1/bracketrightbigg/bracketleftbigg
0−1
−1 0/bracketrightbigg
4.4 Limitation and extension
Our theorem and methods depend on two assumptions regarding the transformations: finiteness and or-
thogonality. At first glance, these assumptions may seem strong. However, the outputs of a wide variety of
equivariant CNNs satisfy these requirements. This is because the invariance with respect to infinite trans-
formations can often be approximated by finite invariance. For example, the continuous rotation invariance
is approximated with small discrete rotations (Marcos et al., 2017; Weiler et al., 2018). Moreover, invariance
is often implemented by CNNs with special restrictions on convolutional filters (e.g. filter “A” is filter “B”
rotated by some θ). In this case, the transformations act as permutations between the filter responses, which
are orthogonal. Therefore, these assumptions are not restrictive. Furthermore, we can extend Theorem 1 to
a more general assumption by exploiting a more advanced group representation theory. This case is discussed
in the Supplementary Materials section.
4.5 Effectiveness of the end-to-end setting
Although Theorem 1 assumes that the input features are fixed, the effectiveness of the proposed invariant
feature can also be demonstrated when the local features are learned in an end-to-end manner. We write
the entire classification model as W⊤Fθcfθl(x), whereFθcandfthetaldenote the learnable feature coding
function and feature extractor, respectively. We assume that these functions preserve equivariance, meaning
that the transformations act orthogonally on Fθcfθl(x). This assumption is satisfied when equivariant CNNs
are used as the local feature extractor along with the proposed invariant feature coding functions. Then, the
12Published in Transactions on Machine Learning Research (06/2023)
Table 2: Irreducible representations of tensor products of irreducible representations of the D4 group.
τ1,1τ1,−1τ−1,1τ−1,−1 τ2
τ1,1τ1,1τ1,−1τ−1,1τ−1,−1 τ2
τ1,−1τ1,−1τ1,1τ−1,−1τ−1,1 τ2
τ−1,1τ−1,−1τ−1,1τ1,−1τ1,1 τ2
τ−1,−1τ−1,−1τ−1,1τ1,−1τ1,1 τ2
τ2τ2τ2τ2τ2τ1,1⊕τ1,−1⊕τ−1,1⊕τ−1,−1
Table 3: Comparison of accuracy using fixed features.
Methods Dim. FMD DTD UIUC CUB Cars
BP 525k81.28±1.5475.89±0.7280.83±2.3577.48 86.22
iBP 525k81.38±1.3875.88±0.6281.94±3.1975.90 86.74
VLAD 525k80.38±1.6175.12±0.7780.37±3.2372.94 86.10
VLAT 525k79.98±1.6676.24±0.6680.46±2.6076.62 87.12
FV 525k78.18±1.6675.56±0.8079.35±1.5266.59 81.70
Inv BP (ours) 82k83.34±1.4877.19±0.6081.48±1.7581.79 87.45
Inv iBP (ours) 82k83.46±1.2777.96±0.4683.33±1.8082.1288.80
Inv VLAD (ours) 525k82.42±1.7476.53±0.5681.94±1.4080.97 88.54
Inv VLAT (ours) 525k81.88±1.3477.45±0.5582.13±1.6483.25 88.59
learning problem can be formulated as
arg min
W∈Rd×|C|,θc,θlλ
2∥W∥2
F+1
MM/summationdisplay
m=1l(W⊤Fθcfθl(xm),ym). (7)
For any fixed θ, we can apply Theorem 1 by considering vm=Fθcfθl(xm). Therefore, the invariant feature
is effective, even in an end-to-end setting.
5 Experiment
In this section, the accuracy and invariance of the proposed method are evaluated using the pretrained
features in Section 5.1. In Section 5.2, we evaluate the method on an end-to-end case.
5.1 Experiment with fixed local features
In this subsection, our methods are evaluated on image recognition datasets using pretrained CNN local
features. Note that the local features were fixed to compare only the performance of coding methods; thus,
the overall scores are lower than those of the existing end-to-end methods.
These methods were evaluated using the Flickr Material Dataset (FMD) (Sharan et al., 2013), describable
texture datasets (DTD) (Cimpoi et al., 2014), UIUC material dataset (UIUC) (Liao et al., 2013), Caltech-
UCSD Birds (CUB) (Welinder et al., 2010)) and Stanford Cars (Cars) (Krause et al., 2013). FMD contains
10 material categories with 1,000 images; DTD contains 47 texture categories with 5,640 images; UIUC
contains 18 categories with 216 images; CUB contains 200 bird categories with 11,788 images; and Cars
consists of 196 car categories with 16,185 images. We used given training test splits for DTD, CUB, and
Cars. We randomly split 10 times such that the sizes of the training and testing data would be the same for
each category for FMD and UIUC.
5.1.1 Results on D4 Group
First, we applied our method to the D4 group used in Cohen & Welling (2017), which contains rich informa-
tion and is easy to calculate. The D4 group consists of π/2rotationrand image flipping mwith|G|= 8.
13Published in Transactions on Machine Learning Research (06/2023)
Table 4: Comparison of accuracy using fixed features on the augmented test data.
Methods Dim. FMD DTD UIUC CUB Cars
BP 525k78.17±1.3270.90±0.6469.12±1.5337.35 27.45
iBP 525k78.28±1.1771.00±0.6469.39±1.9138.32 28.45
VLAD 525k77.20±1.1670.62±0.7767.95±2.1836.18 29.78
VLAT 525k77.03±1.1471.45±0.6569.87±1.5341.01 27.27
FV 525k75.28±1.4170.78±0.7665.89±1.0629.36 27.37
Inv BP (ours) 82k83.05±1.4377.09±0.6181.48±1.7581.62 87.50
Inv iBP (ours) 82k83.24±1.1977.83±0.4783.38±1.8181.9888.80
Inv VLAD (ours) 525k82.14±1.6776.38±0.5081.68±1.3280.75 88.54
Inv VLAT (ours) 525k81.77±1.3177.36±0.6082.13±1.6483.01 88.62
Table 5: Comparison of accuracy using invariant feature modeling and existing feature coding methods.
Methods Dim. FMD DTD UIUC CUBCars
BP 525k81.20±1.6075.09±0.5680.19±3.0078.2485.67
iBP 525k81.20±1.6075.05±0.6381.20±3.3576.3686.00
VLAD 525k80.50±1.9575.20±0.8678.80±2.2971.6185.72
VLAT 525k79.98±1.6275.66±0.8281.02±1.8678.2984.77
FV 525k78.50±1.6575.63±0.7279.91±1.8067.4782.47
The irreducible representations and decomposition of the tensor products are summarized in Tables 1 and
2, respectively.
Because D4 is not orthogonal to the output of the standard CNNs, we pretrained the group equivariant
CNNs and used the last convolutional activation as the local feature extractor. The group equivariant
CNN is the model in which we preserve the Gaction using|G|times the number of filters π(g)applied
on the original filters and use the average with respect to gfor the convolution. When the feature is of
dCNN×|G|dimension, it can be regarded as dCNNtimes the direct sum of the eight-dimensional orthogonal
representation space because D4 acts as a permutation. The representation was decomposed as follows:
πCNN=dCNNτ1,1⊕dCNNτ1,−1⊕dCNN.τ−1,1⊕dCNNτ−1,−1⊕2dCNNτ2.
The group equivariant CNNs with the VGG19 architecture were pretrained with convolutional filter sizes
of23,45,91,181,181instead of 64,128,256,512,512as the local feature extractor. Furthermore, we added
batch normalization layers for each convolution layer to accelerate the training. The model was trained
using the ILSVRC2012 dataset (Deng et al., 2009). A standard data augmentation strategy was applied,
and the same learning settings as the original VGG-Net were used. The pretraining code was implemented
using Pytorch (Paszke et al., 2019) with the group equivariant convolution layers implemented using Groupy
(Cohen & Welling, 2016).
We extracted the last convolutional activation of this pretrained equivariant VGG19 group after rescaling
the input images by 2s, wheres=−3,−2.5,...,1.5. For efficiency, the scales that increased the image size
to greater than 1,0242pixels, were discarded. Subsequently, the nonlinear embedding method proposed in
Vedaldi & Zisserman (2012) was applied such that the feature dimensions were three times larger. As this
embedding is a point-wise function, the output can be regarded as three times the direct sum of the original
representations when considering the group action.
The local feature dimension was then reduced using PCA for the existing method and the proposed invariant
PCA for the proposed method. We applied BP and iBP with dimensions of 1,024, VLAD with dimensions of
512 and 1,024 components, FV with 512 dimensions and 512 components, and VLAT with 256 dimensions
and 8 components. We applied the proposed BP and iBP with the same settings, and VLAD and VLAT
with eight times the number of components.
Training code given pretrained VGG19 was implemented using MATLAB. The linear SVM implemented in
LIBLINEAR (Fan et al., 2008) was used to evaluate the average test accuracy. Furthermore, to validate
14Published in Transactions on Machine Learning Research (06/2023)
Table 6: Comparison of accuracy using existing feature coding methods with augmented training data.
Scores in the parentheses correspond to those of the proposed invariant features cited from Table 3.
Methods Dim. FMD UIUC
BP 525k83.00±1.51 (83.34±1.48)81.11±1.46 (81.48±1.75)
iBP 525k83.10±1.38 (83.46±1.27)82.87±2.01 (83.33±1.80)
VLAD 525k82.72±1.66 (82.42±1.74)80.56±1.75 (81.94±1.40)
VLAT 525k82.66±1.27 (81.88±1.34)81.67±1.89 (82.13±1.64)
FV 525k 81.44±1.60 80.74±2.46
Table 7: Irreducible representations of the D6 group.
Rep. r m
τ1,1 1 1
τ1,−1 1 -1
τ−1,1 -1 1
τ−1,−1 -1 -1
τ2a/bracketleftbigg
cos (π/3)−sin (π/3)
sin (π/3) cos (π/3)/bracketrightbigg/bracketleftbigg
0−1
1 0/bracketrightbigg
τ2b/bracketleftbiggcos (2π/3)−sin (2π/3)
sin (2π/3) cos (2π/3)/bracketrightbigg/bracketleftbigg0−1
1 0/bracketrightbigg
that the proposed feature is D4 invariant, we used the same training data and evaluated the accuracy by
augmenting the test data eight-fold using the D4 group.
Tables 3 and 4 show the accuracy for the test as well as the augmented test datasets. Our method demon-
strated better accuracy than non-invariant methods with the dimensions of the invariant BP and iBP ap-
proximately 1/7of the original dimensions. Thus, we obtained features with significantly smaller dimensions
and higher performances. Note that the higher accuracy is not a simple effect of dimensionality reduction
because in general, the accuracy decreases as the dimension is reduced. We observed that iBP with 400 local
feature dimensions (80k global features) scored 81.76 on FMD and 81.30 on UIUC. Furthermore, compact
bilinear pooling with 82k feature dimensions scored 81.54 on FMD and 79.81 on UIUC. Therefore, the result
is significant as Inv iBP shows better accuracy despite the lower feature dimension.
This table also shows that the existing methods exhibit poor performance on the augmented test data, but
the proposed methods demonstrate performance with scores similar to the original. These results suggest
that theexisting methods use information that isnot relatedto image content, reflecting dataset bias instead.
Our method can discard this bias and focus on the image content.
As an ablation study, we conducted experiments in which PCA and k-means were trained using the proposed
invariant feature modeling and existing feature coding methods were then applied.
Table 5 demonstrates that the proposed invariant feature modeling, combined with the standard feature
coding method, does not demonstrate better accuracy than the original methods. This result shows that
only performance is enhanced when the proposed invariant feature modeling is combined with invariant
feature coding.
We also compared our results with those of data augmentation by training the existing non-invariant feature
coding with eight-fold data augmentation. We conducted this by applying all transformations to the input
images and made the number of training samples |G|times as large. Considering the much higher training
cost, experiments were conducted on the FMD and UIUC datasets, which have a smaller number of training
images.
Table 6 shows that the accuracy is comparable or a little lower than the accuracy of the proposed invariant
feature coding methods. This result is consistent with the argument of Theorem 1 that invariant features
learned with the original training data are equivalent to non-invariant features learned with the augmented
15Published in Transactions on Machine Learning Research (06/2023)
Table 8: Irreducible representations of the tensor products of irreducible representations of the D6 group.
τ1,1τ1,−1τ−1,1τ−1,−1 τ2a τ2b
τ1,1τ1,1τ1,−1τ−1,1τ−1,−1 τ2a τ2b
τ1,−1τ1,−1τ1,1τ−1,−1τ−1,1 τ2a τ2b
τ−1,1τ−1,−1τ−1,1τ1,−1τ1,1 τ2b τ2a
τ−1,−1τ−1,−1τ−1,1τ1,−1τ1,1 τ2b τ2a
τ2aτ2aτ2aτ2bτ2bτ1,1⊕τ1,−1⊕τ2bτ−1,1⊕τ−1,−1⊕τ2a
τ2bτ2bτ2bτ2aτ2aτ−1,1⊕τ−1,−1⊕τ2aτ1,1⊕τ1,−1⊕τ2b
Table 9: Comparison of test accuracy using D6-equivariant CNN.
Methods Dim. FMD DTD UIUC CUB Cars
BP 525k77.26±1.6973.46±0.8775.93±2.5872.66 80.15
iBP 525k77.18±1.4273.51±0.6575.56±1.7071.19 81.13
VLAD 525k75.80±1.5571.65±0.9970.19±3.2364.69 78.44
VLAT 525k75.70±1.4072.60±0.8572.31±2.0769.92 78.59
FV 525k75.24±1.4272.37±0.9274.91±3.4863.06 75.55
Inv BP (ours) 55k80.26±1.7075.41±1.0179.26±2.9378.77 80.92
Inv iBP (ours) 55k80.36±1.4875.86±0.5580.09±2.6679.7483.60
Inv VLAD (ours) 525k78.00±1.1772.76±0.7173.98±2.5276.12 82.47
Inv VLAT (ours) 525k78.06±1.4574.15±0.7775.65±2.7377.59 80.15
training data. Therefore, our method exploits the advantages of data augmentation with lower feature
dimensions and lower training costs.
As for computation time, using an Intel Xeon E5-2698v4 x2 20 Core, 2.2 GHz CPU it takes 13 seconds to
extract the training features and 61 seconds to learn SVM to train BP on UIUC, 9.4 seconds to extract the
training features and 2.1 seconds to learn SVM to train Inv BP on UIUC in Table 3, and 81 seconds to
extract the training features and 130 seconds to learn SVM to train BP on the augmented UIUC in Table 6.
The reduced number of feature dimensions by the proposed method contributes to the training efficiency.
5.1.2 Results on D6 Group
Furthermore, we applied our method to the D6 group, which was more complex than the D4 group. The D6
group consists of π/3rotationsrand an image flipping mwith|G|= 12. The irreducible representations
and decomposition of the tensor products for this group are summarized in Tables 7 and 8, respectively.
To obtain the local feature in which D6 acts orthogonally, CNN was pretrained with HexaConv (Hoogeboom
et al., 2018). HexaConv models the input images in the hexagonal axis and applies D6 group-equivariant
convolutional layers to construct the CNN. Because D6 acts as a permutation of the positions along the
hexagonal axis, the convolutional activations are D6-equivariant. Therefore, we used this convolutional
activation as the input for our coding methods.
The group equivariant CNNs with the VGG19 architecture were pretrained with convolutional filter sizes
of18,37,74,148,148instead of 64,128,256,512,512as the local feature extractor. Furthermore, batch
normalization layers were added to each convolution layer to accelerate the training speed. Therefore, the
dimensionality of the local features is 148×12. The model was trained using the ILSVRC2012 dataset (Deng
et al., 2009). Because the input image size increases when the axes are changed from Euclidean to hexagonal,
a160×160image was randomly cropped from the original image rescaled to 192×192for training. The
remaining settings followed those of the original VGG.
We extracted the last convolutional activation of this pretrained model after rescaling the input images by
2s, wheres=−3,−2.5,...,1.5. For efficiency, the scalings that increased the image size to greater than 5122
pixels were discarded. Subsequently, the nonlinear embedding method proposed in Vedaldi & Zisserman
(2012) was applied. The dimensions and number of components used for the coding methods followed
16Published in Transactions on Machine Learning Research (06/2023)
Table 10: Comparison of accuracy on Resnet50. The score with ‘*’ denotes the score reported in Li et al.
(2018).
Method iSQRT-COV* iSQRT-COV Inv iSQRT-COV
(Resnet50) (equivariant Resnet50) (equivariant Resnet50) (ours)
Dim. 32k 265k 25k
Top-1 Err. 22.14 21.65 21.02
Top-5 Err. 6.22 5.92 5.47
Table 11: Comparison of accuracy on Resnet101. The score with ‘*’ denotes the score reported in Li et al.
(2018).
Method iSQRT-COV* iSQRT-COV Inv iSQRT-COV
(Resnet101) (equivariant Resnet101) (equivariant Resnet101) (ours)
Dim. 32k 265k 25k
Top-1 Err. 21.21 20.45 19.98
Top-5 Err. 5.68 5.35 4.96
those used for the D4 experiments. The pretraining was implemented by Pytorch and hexaconv library
(Hoogeboom et al., 2018), and feature coding and classifier training was implemented with MATLAB.
In Table 9, the proposed coding methods consistently demonstrate better performance than non-invariant
methods although the overall accuracy is lower than the D4 case, which may arise from the discriminative
performance of the local feature extractor. Furthermore, the dimensionality of invariant BP and iBP is
smaller than the dimensionality for D4. This is because the D6 group represents more complex transforma-
tions than the D4 group; thus, the number of invariants with respect to the D6 group is smaller than the
number with respect to the D4 group.
Therefore, the proposed framework was also effective for the D6 group.
5.2 Experiment with end-to-end model
The proposed invariant BP was then applied to an end-to-end learning framework.
5.2.1 Results on D4 Group with iSQRT-COV
We constructed a model based on iSQRT-COV (Li et al., 2018), which is a variant of BP that demonstrates
good performance and stable training. Group equivariant CNNs with Resnet50 and 101 (He et al., 2016)
architecture were used for the local feature extractor, where the filter sizes were reduced, as in the case
of VGG19. The iSQRT-COV and the proposed invariant iSQRT-COV were used instead of global average
pooling. We compared iSQRT-COV with Resnet50, iSQRT-COV with D4-equivariant Resnet50, and the
proposed invariant iSQRT-COV with D4-equivariat Resnet50.
All the models were learned, including the feature extractor, using a momentum grad with an initial learning
rate of 0.1, momentum of 0.9, and weight decay rate of 1e-4 for 65 epochs with a batch size of 160. The
learning rate was multiplied by 0.1 at 30, 45, and 60 epochs. The top-1 and top-5 test errors were then
evaluated, using the average score for the original and flipped images for the evaluation. The training code
was implemented with Pytorch, Groupy and fast-MPN-COV libraries (Li et al., 2018).
Tables 10 and 11 show that although iSQRT-COV achieves accuracy using the local features extracted from
equivariant CNN, our Inv iSQRT-COV demonstrates better accuracy with smaller feature dimension.
The above pre-trained models were further fine-tuned and the accuracy was evaluated on fine-grained recog-
nition datasets: CUB, Cars, and Aircraft (Maji et al., 2013). The Aircraft dataset consists of 100 categories
with 6,667 training images and 3,333 test images. Following the settings of existing studies, each input image
was resized to 448 ×448 for fine-tuning and evaluation for CUB and Cars, and the other images were resized
17Published in Transactions on Machine Learning Research (06/2023)
Table 12: Comparison of accuracy using fine-tuning on Resnet50. The score with ‘*’ denotes the score
reported in Li et al. (2018).
Method iSQRT-COV* iSQRT-COV Inv iSQRT-COV
(Resnet50) (equivariant Resnet50) (equivariant Resnet50) (ours)
Dim. 32k 265k 25k
CUB 88.1 87.8±0.19 87.9±0.26
Cars 92.8 93.8±0.11 93.4±0.16
Aircraft 90.0 91.4±0.28 91.7±0.32
Table 13: Comparison of accuracy using fine-tuning on Resnet101. The score with ‘*’ denotes the score
reported in Li et al. (2018).
Method iSQRT-COV* iSQRT-COV Inv iSQRT-COV
(Resnet101) (equivariant Resnet101) (equivariant Resnet101) (ours)
Dim. 32k 265k 25k
CUB 88.7 44.7±2.3 88.5±0.25
Cars 93.3 56.0±7.9 93.9±0.18
Aircraft 91.4 69.1±2.0 92.1±0.26
to 512 ×512, further cropping the 448 ×448 center image for the Aircraft dataset. The training setting
followed the original setting of the iSQRT-COV (Li et al., 2018). For each setting, we evaluated the models
ten times and evaluated the average test accuracy. Tables 12 and 13 show the results. As for ResNet50, the
proposed Inv iSQRT-COV displays comparable or better accuracy than the reported iSQRT-COV and simi-
lar accuracy to iSQRT-COV with equivariant ResNet50 with much smaller feature dimension. Furthermore,
iSQRT-COV with equivariant ResNet101 overfits the training data and displays much lower accuracy than
the proposed Inv iSQRT-COV ResNet101. This result suggests that our invariant feature coding contributes
to the stability of gradient-based training and results in the exploitation of complex local feature extractors
with high recognition accuracy.
As for the computation time, when using 8 A100 GPUs it takes 0.17 seconds/batch to train iSQRT-COV
(Resnet50), 0.72 seconds/batch to train iSQRT-COV (equivariant Resnet50) and 0.45 seconds/batch to train
the proposed Inv iSQRT-COV (equivariant Resnet50) on ImageNet. Though equivariant Resnet50 models
take more time than the original Resnet50, which mainly results from feature extractor module instead of
the feature coding module we target, we can reduce the computation time by the proposed invariant coding
method compared to iSQRT-COV (equivariant Resnet50). The reduced complexity arises from (i) reduction
of the feature dimension that we calculate the bilinear feature, and (ii) reduction of the size of the final
fully-connected layer.
Subsequently, tovisualizeattention, GradCAM++(Chattopadhayetal.,2018)wasusedafterthedimension-
reduction layer (Figure 5). In the existing methods (iSQRT-COV with Resnet50 and equivariant Resnet50),
theshapeoftheheatmapchangeswhenrotationandhorizontalflipareappliedtotheinputimage. Theshape
of the heatmap is relatively preserved with the proposed invariant coding method. This result indicates that
the baseline methods learned in a standard manner do not learn the invariance correctly, whereas our method
was trained to both demonstrate high accuracy and preserve invariance, which is consistent with the result
for the case of fixed local features. Furthermore, the heatmaps are quantatively compared using maximum
mean discrepancy (MMD) by (i) inverting flipped and rotated heatmaps to match the original heatmaps, (ii)
applying l1-normalization to the heatmaps, and (iii) calculating the MMD between the inverted heatmaps
and heatmaps for the original image with the Gaussian kernel function exp(−∥r1−r2∥2
σ2), whereris are two-
dimensional positions of the image pixels normalized within [0,1], andσis set to 0.1. We can see that the
proposed method shows a lower MMD value, which means that the shape of the heatmaps are more similar
to each other. Plots of the results on the other images are available in the Supplementary Materials section.
18Published in Transactions on Machine Learning Research (06/2023)
(a) Original image
 (b) GradCAM++ on the
original image with iSQRT-
COV (Resnet50)
(c) GradCAM++ on the
Original image with iSQRT-
COV (equivariant Resnet50)
(d) GradCAM++ on the
original image with Inv
iSQRT-COV (equivariant
Resnet50) (ours)
(e) Flipped image
 (f) GradCAM++ on the
flipped image with iSQRT-
COV (Resnet50) (MMD:
0.021)
(g) GradCAM++ on the
flipped image with iSQRT-
COV (equivariant Resnet50)
(MMD: 0.068)
(h) GradCAM++ on the
flipped image with Inv
iSQRT-COV (equivariant
Resnet50) (ours) (MMD:
0.017)
(i) Rotated image
 (j) GradCAM++ on the
original image with iSQRT-
COV (Resnet50) (MMD:
0.031)
(k) GradCAM++ on the
original image with iSQRT-
COV (equivariant Resnet50)
(MMD: 0.11)
(l) GradCAM++ on the
Original Image with Inv
iSQRT-COV (equivariant
Resnet50) (ours) (MMD:
0.022)
Figure 5: Comparison of GradCAM++ on the transformed input image. MMD indicates maximum mean
discrepancy to the heatmaps of the original image.
5.2.2 Results on the other groups
The proposed method was then applied to different groups.
First, we used D6 group with HexaConv (Hoogeboom et al., 2018) as in Section 5.1.2 with D6-equivariant
Resnet18 as the base feature extractor. Resnet18 was used to reduce memory consumption. We then
compared the standard Resnet18, D6-equivariant Resnet18, iSQRT-COV with D6-equivariant Resnet18,
and the proposed invariant iSQRT-COV with D6-equivariant Resnet18.
19Published in Transactions on Machine Learning Research (06/2023)
Table 14: Comparison of accuracy for the end-to-end D6-equivariant model.
Method Resnet18 equivariant Resnet18 iSQRT-COV Inv iSQRT-COV
(equivariant Resnet18) (equivariant Resnet18) (ours)
Dim. 0.5k 1.8k 395k 22k
Top-1 Err. 30.24 28.93 27.27 26.42
Top-5 Err. 10.92 9.99 9.46 8.45
Table 15: Comparison of accuracy for the end-to-end C8-equivariant model.
Method Resnet18 equivariant Resnet18 iSQRT-COV Inv iSQRT-COV
(equivariant Resnet18) (equivariant Resnet18) (ours)
Dim. 0.5k 1.5k 131k 10k
Top-1 Err. 30.24 27.32 25.33 25.25
Top-5 Err. 10.92 8.76 7.90 7.86
To learn the iSQRT-COV models, a momentum grad with an initial learning rate of 0.1, momentum of 0.9,
and weight decay rate of 1e-4 were used for 65 epochs with a batch size of 160. The learning rate was further
multiplied by 0.1 at 30, 45, and 60 epochs. Following the original setting, we learned the Resnet models
using a momentum grad with an initial learning rate of 0.1, momentum of 0.9, and weight decay rate of 1e-4
for 100 epochs with a batch size of 256. The learning rate was multiplied by 0.1 at 30, 70, and 90 epochs,
and then the top-1 and top-5 test errors were evaluated. The training code was implemented with Pytorch,
hexaconv and fast-MPN-COV libraries.
Table 14 demonstrates that the proposed invariant features show good accuracy with low feature dimension.
When considering the discrete rotation group, the proposed invariant bilinear pooling was reduced to the
squared norm of the Fourier coefficients. The Fourier coefficients were then applied to the C8 group that
consists ofπ/8rotations. The implementation provided by Weiler & Cesa (2019) was used to implement the
C8-equivariant networks. The standard Resnet18, C8-equivariant Resnet18, C8-equivariant Resnet18 with
iSQRT-COV pooling, and the proposed C8-equivariatn Resnet18 with invariant iSQRT-COV pooling were
then compared. The training setting followed that of the D6 group.
Table 15 demonstrates a similar tendency for the D4 and D6 cases. Therefore, the proposed method is
effective for several invariance settings.
5.2.3 Results on the other coding method
In this section, we further apply our method to another variant of bilinear pooling.
Scaling eigen branch (SEB) (Song et al., 2023) is a method that multiplies improved bilinear pooling with
1 +/radicalig/summationtextdlocal
i=1λie−2λito enhance the importance of features with smaller variance, where λidenotes the
i-th eigenvalue of the bilinear matrix. Although the original method calculates the features using singular
value decomposition, to reduce the computation cost and increase the stability, we calculated the features
using/parenleftig
1 +/radicalbig
trace (Σe−2Σ)/parenrightig
iSQRT(Σ) , where Σis the input covariance feature. The computation does not
require singular value decomposition. We refer to this method as iSQRT-SEB and compare iSQRT-SEB
Resnet50, D4-equivariant Resnet50 with iSQRT-SEB (denoted by “iSQRT-SEB”) and the proposed D4-
equivariant Resnet50 with invariant iSQRT-SEB (denoted by “Inv iSQRT-SEB (ours)”). For iSQRT-SEB
with D4-equivariant Resnet50, the global feature dimension was reduced to 32k by applying invariant PCA,
considering memory limitations. The training strategy was the same as that of iSQRT-COV in the D4 group.
The DifferentialSVD library (Song et al., 2023) was used for implementing SEB. Tables 16 and 17 display
the results. Although the overall accuracy is lower than that of iSQRT, showing the same tendency as in
the original paper, the tendency is the same as that of the proposed method which has better accuracy.
Therefore, invariant feature coding is effective for end-to-end training.
20Published in Transactions on Machine Learning Research (06/2023)
Table 16: Comparison of accuracy for iSQRT-SEB on Resnet50.
Method iSQRT-SEB iSQRT-SEB Inv iSQRT-SEB
(Resnet50) (equivariant Resnet50) (equivariant Resnet50) (ours)
Dim. 32k 32k 25k
Top-1 Err. 25.20 24.62 23.77
Top-5 Err. 7.92 7.71 7.14
Table 17: Comparison of accuracy for iSQRT-SEB on Resnet101.
Method iSQRT-SEB iSQRT-SEB Inv iSQRT-SEB
(Resnet101) (equivariant Resnet101) (equivariant Resnet101) (ours)
Dim. 32k 32k 25k
Top-1 Err. 23.84 23.87 22.96
Top-5 Err. 7.7 7.11 6.62
6 Conclusion
In this study, we proposed a feature-coding method that considers image information preserving transforma-
tions. Based on group representation theory, we propose a guideline used to first construct a feature space
in which groups act orthogonally to calculate the invariant vector. Subsequently, a novel model learning
method and coding methods are introduced to explicitly consider group actions. We applied our method
to image classification datasets and demonstrated that the proposed model can yield high accuracy while
preserving invariance. This study provides a novel framework for constructing an invariant feature.
Acknowledgements
This work was partially supported by JSPS KAKENHI Grant Number JP19176033 and JP19K20290, JST
Moonshot R&D Grant Number JPMJPS2011, CREST Grant Number JPMJCR1403 and JPMJCR2015 and
Basic Research Grant (Super AI) of Institute for AI and Beyond of the University of Tokyo. We would like
to thank Atsushi Kanehira, Takuhiro Kaneko, Toshihiko Matsuura, Takuya Nanri and Masatoshi Hidaka for
the helpful discussion.
References
Fabio Anselmi, Joel Z Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio. Unsu-
pervised learning of invariant representations. Theoretical Computer Science , 633:112–121, 2016.
Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture
for weakly supervised place recognition. In CVPR, 2016.
Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks
from training data. In Neurips, 2020.
Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-cam++:
Generalized gradient-based visual explanations for deep convolutional networks. In WACV, 2018.
Ho Yin Chau, Frank Yuchen Qiu, Yubei Chen, and Bruno Olshausen. Disentangling images with lie group
transformations and sparse coding. In Neurips 2022 Workshop on Symmetry and Geometry in Neural
Representations , 2022.
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing
textures in the wild. In CVPR, 2014.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In ICML, 2016.
Taco S Cohen and Max Welling. Steerable cnns. In ICLR, 2017.
21Published in Transactions on Machine Learning Research (06/2023)
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image
Database. In CVPR, 2009.
Mandar D Dixit and Nuno Vasconcelos. Object based scene representations using fisher scores of local
subspace projections. In Neurips, 2016.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A library
for large linear classification. JMLR, 9:1871–1874, 2008.
William Fulton and Joe Harris. Representation Theory: A First Course . Springer, 2014.
Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell. Compact bilinear pooling. In CVPR, 2016.
Mengran Gou, Fei Xiong, Octavia Camps, and Mario Sznaier. Monet: Moments embedding network. In
CVPR, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, 2016.
Emiel Hoogeboom, Jorn W.T. Peters, Taco S. Cohen, and Max Welling. Hexaconv. In ICLR, 2018.
Hervé Jégou, Matthijs Douze, Cordelia Schmid, and Patrick Pérez. Aggregating local descriptors into a
compact image representation. In CVPR, 2010.
Erik Jenner and Maurice Weiler. Steerable partial differential operators for equivariant neural networks. In
ICLR, 2022.
Ramakrishna Kakarala. The bispectrum as a source of phase-sensitive invariants for fourier descriptors: a
group-theoretic approach. Journal of Mathematical Imaging and Vision , 44:341–353, 2012.
Takumi Kobayashi. Flip-invariant motion representation. In ICCV, 2017.
Takumi Kobayashi, Koiti Hasida, and Nobuyuki Otsu. Rotation invariant feature extraction from 3-d accel-
eration signals. In ICASSP, 2011.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch–gordan nets: a fully fourier space spherical convo-
lutional neural network. In Neurips, 2018.
Piotr Koniusz and Hongguang Zhang. Power normalizations in fine-grained image, few-shot image and graph
classification. TPAMI, 44(2):591–609, 2021.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained cate-
gorization. In ICCV, 2013.
Dmitry Laptev, Nikolay Savinov, Joachim M Buhmann, and Marc Pollefeys. Ti-pooling: transformation-
invariant pooling for feature learning in convolutional neural networks. In CVPR, 2016.
Peihua Li, Jiangtao Xie, Qilong Wang, and Zilin Gao. Towards faster training of global covariance pooling
networks by iterative matrix square root normalization. In CVPR, 2018.
Zicheng Liao, Jason Rock, Yang Wang, and David Forsyth. Non-parametric filtering for geometric detail
extraction and material representation. In CVPR, 2013.
Feng Lin, Haohang Xu, Houqiang Li, Hongkai Xiong, and Guo-Jun Qi. Auto-encoding transformations in
reparameterized lie groups for unsupervised learning. In AAAI, 2021.
Tsung-Yu Lin and Subhransu Maji. Improved bilinear pooling with cnns. In BMVC, 2017.
Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear cnn models for fine-grained visual
recognition. In ICCV, 2015.
22Published in Transactions on Machine Learning Research (06/2023)
S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft.
Technical report, 2013.
Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field networks.
InICCV, 2017.
Xu Miao and Rajesh P. N. Rao. Learning the lie groups of visual invariance. Neural Computation , 19(10):
2665–2693, 2007.
Olivier Morère, Antoine Veillard, Lin Jie, Julie Petta, Vijay Chandrasekhar, and Tomaso Poggio. Group
invariant deep representations for image instance retrieval. In AAAI, 2017.
Hideki Nakayama, Tatsuya Harada, and Yasuo Kuniyoshi. Dense sampling low-level statistics of local
features. IEICE TRANSACTIONS on Information and Systems , 93:1727–1736, 2010a.
Hideki Nakayama, Tatsuya Harada, and Yasuo Kuniyoshi. Global gaussian approach for scene categorization
using information geometry. In CVPR, 2010b.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Neurips,
2019.
David Picard and Philippe-Henri Gosselin. Efficient image signatures and similarities using tensor products
of local descriptors. CVIU, 117:680–687, 2013.
Rajesh Rao and Daniel Ruderman. Learning lie groups for invariant visual perception. In Neurips, 1998.
Marco Reisert and Hans Burkhardt. Using irreducible group representations for invariant 3d shape descrip-
tion. InJoint Pattern Recognition Symposium , 2006.
Jongbin Ryu, Ming-Hsuan Yang, and Jongwoo Lim. Dft-based transformation invariant pooling layer for
visual classification. In ECCV, 2018.
Jorge Sánchez, Florent Perronnin, Thomas Mensink, and Jakob Verbeek. Image classification with the fisher
vector: Theory and practice. IJCV, 105:222–245, 2013.
Lavanya Sharan, Ce Liu, Ruth Rosenholtz, and Edward H Adelson. Recognizing materials using perceptually
inspired features. IJCV, 103(3):348–371, 2013.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
InICLR, 2014.
Jascha Sohl-Dickstein, Ching Ming Wang, and Bruno A Olshausen. An unsupervised algorithm for learning
lie group transformations. arXiv preprint arXiv:1001.1027 , 2010.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Generalization error of invariant
classifiers. In AISTATS , 2017.
Yue Song, Nicu Sebe, and Wei Wang. Why approximate matrix square root outperforms accurate svd in
global covariance pooling? In ICCV, 2021.
Yue Song, Nicu Sebe, and Wei Wang. On the eigenvalues of global covariance pooling for fine-grained visual
recognition. TPAMI, 45(3):3554–3566, 2023.
Andrea Vedaldi and Andrew Zisserman. Efficient additive kernels via explicit feature maps. TPAMI, 34(3):
480–492, 2012.
Cédric Vonesch, Frédéric Stauber, and Michael Unser. Design of steerable filters for the detection of micro-
particles. In International Symposium on Biomedical Imaging , pp. 934–937. IEEE, 2013.
23Published in Transactions on Machine Learning Research (06/2023)
Cédric Vonesch, Frédéric Stauber, and Michael Unser. Steerable pca for rotation-invariant image recognition.
SIAM Journal on Imaging Sciences , 8(3):1857–1873, 2015.
Qilong Wang, Peihua Li, Wangmeng Zuo, and Lei Zhang. Raid-g: Robust estimation of approximate infinite
dimensional gaussian with application to material recognition. In CVPR, 2016.
Qilong Wang, Peihua Li, and Lei Zhang. G2denet: Global gaussian distribution embedding network and its
application to visual recognition. In CVPR, 2017.
Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. In Neurips, 2019.
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation equivariant
cnns. In CVPR, 2018.
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro
Perona. Caltech-ucsd birds 200. Technical Report CNS-TR-201, Caltech, 2010.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks:
Deep translation and rotation equivariance. In CVPR, 2017.
Tan Yu, Yunfeng Cai, and Ping Li. Toward faster and simpler matrix normalization via rank-1 update. In
ECCV, 2020.
Zhizhen Zhao and Amit Singer. Fourier–bessel rotational invariant eigenimages. JOSA A , 30(5):871–877,
2013.
Zhizhen Zhao, Yoel Shkolnisky, and Amit Singer. Fast steerable principal component analysis. IEEE trans-
actions on computational imaging , 2(1):1–12, 2016.
A Another Proof of Theorem 1
In this section, the proof of Theorem 1is provided.
Proof.Denoting the loss function as L(W), the subgradient ∂LatW0is defined as the set,
∂L(W0) ={c|f(W)≧f(W0) +⟨c,W−W0⟩for∀W}. (8)
Fromthis definition, W0minimizesLif and onlyif 0∈∂L(W0). From the assumptionin Theorem 1, L(W) =
L(π(g)W)for anyW,g. Therefore, ∂L(π(g)W0) =π(g)⊤∂L(W0)and0∈∂L(W0)⇐⇒ 0∈∂L(π(g)W0).
From the definition of subgradient, when 0∈∂L(W0), it follows that
∂L
1
|G|/summationdisplay
g∈Gπ(g)W0
⊃/intersectiondisplay
g∂L(π(g)W0)∋0. (9)
The inclusion comes from the fact that if c∈/intersectiontext
g∂L(π(g)W0), thenL(W)≧1
|G|/summationtext
g∈GL(π(g)W0) +⟨c,W−
1
|G|/summationtext
g∈Gπ(g)W0⟩≧L(1
|G|/summationtext
g∈Gπ(g)W0) +⟨c,W−1
|G|/summationtext
g∈Gπ(g)W0⟩Therefore,1
|G|/summationtext
g∈Gπ(g)W0also
minimizesL, which is invariant under π(g).
B Extension of Theorem 1
Non-orthogonal case When we omit the restriction on the orthogonality of π,
1
M|G|M/summationdisplay
m=1/summationdisplay
g∈Gl/parenleftigg
Re/parenleftiggT/summationdisplay
t=1/parenleftig
W(t)/parenrightig⊤
τt(g−1)v(t)
m/parenrightigg
,ym/parenrightigg
=1
M|G|M/summationdisplay
m=1/summationdisplay
g∈Gl/parenleftigg
Re/parenleftiggT/summationdisplay
t=1/parenleftig
τt(g)W(t)/parenrightig⊤
v(t)
m/parenrightigg
,ym/parenrightigg
, (10)
(11)
24Published in Transactions on Machine Learning Research (06/2023)
does not follow. However, matrix Tcan be calculated such that Tπ(g)T−1becomes orthogonal for all g.
Therefore, if we apply an appropriate coordinate transformation beforehand, we can apply Theorem 1.
Infinite case The finite group can be extended to a compact group. The compact group is parameterized
by a closed bounded set, and ◦and−1are continuous with respect to these parameters. Many results
regarding the representation of a finite group also hold for a compact group when1
|G|/summationtext
g∈Gis replaced with/integraltext
dg, wheredgis a special measure called Haar measure. With this measure, the following holds.
Pτt=dim(τt)/integraldisplay
dgχτt(g)π(g), (12)
and
P1=/integraldisplay
dgπ(g). (13)
Theorem 1 can be rewritten as
Theorem 2. When we assume that the compact group Gacts as an orthogonal representation πonRdand
preserves the distribution of the training data P, the solution to the L2-regularized convex loss minimization,
arg min
W∈Rd×|C|λ
2∥W∥2
F+E(x,y)∼P[l(W⊤x,y)] (14)
isG-invariant, implying that π(g)W=Wfor anygandP1W=W.
C Invariant PCA when the irreducible representations are not real.
Whensome τhascomplexelements,Algorithm1cannotbeapplieddirectlybecausetheintertwiningoperator
and covariance matrices become complex. In this case, we couple τtwithτtintoτ′≃τt⊕τt. Because
χτt=χτt, the multiplicities of τtandτtin the decomposition are equal because of Eq.(6), and the projected
vectors are complex conjugates of each other because of Eq.(7). Thus,√
2times the concatenation of the
real and imaginary parts constitute the τ′-th element. In Algorithm 1, τtis replaced with τ′
tto obtain an
invariant PCA for the general case.
D Proof that the dimension of Invariant VLAD and VLAT is the same as that of
original feature
UsingCcomponents, 1µcan be decomposed into Cτ1,1⊕Cτ1,−1⊕Cτ−1,1⊕Cτ−1,−1⊕2Cτ2.
When the first- or second-order statistics are decomposed as n1,1τ1,1⊕n1,−1τ1,−1⊕n−1,1τ−1,1⊕
n−1,−1τ−1,−1⊕n2τ2, the multiplicity of τ1,1of(Cτ1,1⊕Cτ1,−1⊕Cτ−1,1⊕Cτ−1,−1⊕2Cτ2)⊗
(n1,1τ1,1⊕n1,−1τ1,−1⊕n−1,1τ−1,1⊕n−1,−1τ−1,−1⊕n2τ2)isC(n1,1+n1,−1+n−1,1+n−1,−1+ 2n2),
which is the same as the dimension of the original feature.
E GradCAM++ on the other images
GradCAM++ results on the other images are plotted in Figures 6 and 7.
F Results on SIFT features
In this section, we report the results using the SIFT feature in which D4 acts orthogonally. The irreducible
decompositionoftheSIFTfeatureisdescribedinSectionF.1. Theaccuracyoftheimagerecognitiondatasets
was evaluated in Section Section F.2.
25Published in Transactions on Machine Learning Research (06/2023)
(a) Original Image
 (b) GradCAM++ on the
original image with iSQRT-
COV (Resnet50)
(c) GradCAM++ on the
original image with iSQRT-
COV (equivariant Resnet50)
(d) GradCAM++ on the
original image with Inv
iSQRT-COV (equivariant
Resnet50) (ours)
(e) Flipped Image
 (f) GradCAM++ on the
flipped image with iSQRT-
COV (Resnet50) (MMD:
0.027)
(g) GradCAM++ on the
flipped image with iSQRT-
COV (equivariant Resnet50)
(MMD: 0.050)
(h) GradCAM++ on the
flipped image with Inv
iSQRT-COV (equivariant
Resnet50) (ours) (MMD:
0.017)
(i) Rotated Image
 (j) GradCAM++ on the
original image with iSQRT-
COV (Resnet50) (MMD:
0.023)
(k) GradCAM++ on the
original image with iSQRT-
COV (equivariant Resnet50)
(MMD: 0.048)
(l) GradCAM++ontheorig-
inal image with Inv iSQRT-
COV (equivariant Resnet50)
(ours) (MMD: 0.014)
Figure 6: Comparison of GradCAM++ on the transformed input image. MMD indicates maximum mean
discrepancy to the heatmaps of the original image.
F.1 Irreducible decomposition of SIFT
An overview of the SIFT feature is plotted in Figure 8, where a D4 group acts as a permutator on both
{a,b,c,d,e,f,g,h}and1...16. We can further decompose these into permutations on {a,c,e,g}:{b,d,f,h},
{1,4,13,16},{6,7,10,11}, and{2,3,5,8,9,12,13,14}. These permutations can be decomposed as: τ1,1⊕
τ−1,1⊕τ2,τ1,1⊕τ−1,−1⊕τ2,τ1,1⊕τ−1,−1⊕τ2,τ1,1⊕τ−1,−1⊕τ2, andτ1,1⊕τ−1,1⊕τ1,−1⊕τ−1,−1⊕τ2
respectively, which can be calculated using the characteristic functions. Thus, the permutations on SIFT
can be decomposed into (2τ1,1⊕τ−1,1⊕τ−1,−1⊕2τ2)⊗(3τ1,1⊕τ1,−1⊕τ−1,1⊕3τ−1,−1⊕4τ2) = 18τ1,1⊕
14τ1,−1⊕14τ−1,1⊕18τ−1,−1⊕32τ2.
26Published in Transactions on Machine Learning Research (06/2023)
(a) Original Image
 (b) GradCAM++ on the
original image with iSQRT-
COV (Resnet50)
(c) GradCAM++ on the
original image with iSQRT-
COV (equivariant Resnet50)
(d) GradCAM++ on the
original image with Inv
iSQRT-COV (equivariant
Resnet50) (ours)
(e) Flipped Image
 (f) GradCAM++ on the
flipped image with iSQRT-
COV (Resnet50) (MMD:
0.0082)
(g) GradCAM++ on the
flipped image with iSQRT-
COV (equivariant Resnet50)
(MMD: 0.012)
(h) GradCAM++ on the
flipped image with Inv
iSQRT-COV (equivariant
Resnet50) (ours) (MMD:
0.0082)
(i) Rotated Image
 (j) GradCAM++ on the
original image with iSQRT-
COV (Resnet50) (MMD:
0.014)
(k) GradCAM++ on the
original image with iSQRT-
COV (equivariant Resnet50)
(MMD: 0.013)
(l) GradCAM++ontheorig-
inal image with Inv iSQRT-
COV (equivariant Resnet50)
(ours) (MMD: 0.0092)
Figure 7: Comparison of GradCAM++ on the transformed input image. MMD indicates maximum mean
discrepancy to the heatmaps of the original image.
F.2 Experimental Results
The methods were evaluated on (FMD) (Sharan et al., 2013), (DTD) (Cimpoi et al., 2014), (UIUC) (Liao
et al., 2013), and CUB (Welinder et al., 2010)). The evaluation protocol was the same as that for the
VGG-feature.
The dense SIFT feature was extracted from multi-scale images, as in the case of VGG, and then nonlinear
homogeneous mapping (Vedaldi & Zisserman, 2012) was applied to make the feature dimension three times
larger.
27Published in Transactions on Machine Learning Research (06/2023)
1 2 3 4
5 6 7 8
9 10 11 12 
13 14 15 16 ah
g
fedcb
Figure 8: Overview of the SIFT feature.
Table 18: Comparison of accuracy using SIFT features.
Methods Dim. Test Accuracy Augmented Test Accuracy
FMD DTD UIUC FMD DTD UIUC
BP 33k49.84±1.5751.54±0.9356.02±2.4043.36±1.1941.55±0.7342.50±2.23
VLAD 262k60.11±1.4159.37±1.2358.70±3.5054.68±1.1751.54±0.9046.11±2.01
VLAT 525k58.09±1.4058.74±0.8459.72±2.8752.01±1.2550.40±0.8147.85±1.65
FV 262k61.84±1.8761.20±0.9664.26±3.3056.78±0.7853.77±0.7851.89±2.29
Inv BP (ours) 8k55.19±1.6254.78±1.3760.74±2.8455.18±1.7554.79±1.3660.74±3.02
Inv VLAD (ours) 262k67.53±1.0364.50±1.0168.98±2.5567.55±1.1164.53±1.0369.04±2.58
Inv VLAT (ours) 525k66.79±1.4764.60±1.1267.50±4.1566.75±1.4864.61±1.0767.57±4.10
The dimension was reduced to 256 prior to applying BP, VLAD with 1,024 components, FV with 512
components, and VLAT with 16 components. We also applied the proposed invariant BP with the same
setting, and VLAD and VLAT with eight times the number of components.
Table 18 shows a tendency similar to the results of the VGG-feature. Our methods demonstrate better
performance than existing methods for both the original test data and augmented test data.
28