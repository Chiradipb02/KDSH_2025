Under review as submission to TMLR
Stochastic Variance-Reduced Newton:
Accelerating Finite-Sum Minimization with Large Batches
Anonymous authors
Paper under double-blind review
Abstract
Stochastic variance reduction has proven eﬀective at accelerating ﬁrst-order algorithms for
solving convex ﬁnite-sum optimization tasks such as empirical risk minimization. Incorpo-
rating second-order information has proven helpful in further improving the performance of
theseﬁrst-ordermethods. Yet, comparativelylittleisknownaboutthebeneﬁtsofusingvari-
ance reduction to accelerate popular stochastic second-order methods such as Subsampled
Newton. Toaddressthis, weproposeStochasticVariance-ReducedNewton(SVRN),aﬁnite-
sum minimization algorithm that provably accelerates existing stochastic Newton methods
fromO(αlog(1//epsilon1))toO/parenleftbiglog(1//epsilon1)
log(n)/parenrightbig
passes over the data, i.e., by a factor of O(αlog(n)), where
nis the number of sum components and αis the approximation factor in the Hessian esti-
mate. Surprisingly, this acceleration gets more signiﬁcant the larger the data size n, which
is a unique property of SVRN. Our algorithm retains the key advantages of Newton-type
methods, such as easily parallelizable large-batch operations and a simple unit step size. We
use SVRN to accelerate Subsampled Newton and Iterative Hessian Sketch algorithms, and
show that it compares favorably to popular ﬁrst-order methods with variance reduction.
1 Introduction
Consider a convex ﬁnite-sum minimization task:
ﬁnd x∗= argmin
x∈Rdf(x)forf(x) =1
nn/summationdisplay
i=1ψi(x). (1)
This optimization task naturally arises in machine learning through empirical risk minimization, where xis
the model parameter vector and each function ψi(x)corresponds to the loss incurred by the model on the
i-th element in a training data set (e.g., square loss for regression, or logistic loss for classiﬁcation). Many
other optimization tasks, such as solving semi-deﬁnite programs and portfolio optimization, can be cast in
this general form. Our goal is to ﬁnd an /epsilon1-approximate solution, i.e., ˜xsuch thatf(˜x)−f(x∗)≤/epsilon1.
Naturally, one can use classical iterative optimization methods for this task (such as gradient descent and
Newton’s method), which use ﬁrst/second-order information of function fto construct a sequence x0,x1,...
thatconvergesto x∗. However, thisdoesnotleveragetheﬁnite-sumstructureoftheproblem. Thus, extensive
literaturehasbeendedicatedtoeﬃcientlysolvingﬁnite-summinimizationtasksusingstochasticoptimization
methods, which use ﬁrst/second-order information of randomly sampled component functions ψi, that can
often be computed much faster than the entire function f. Among ﬁrst-order methods, variance-reduction
techniques such as SAG (Roux et al., 2012), SDCA (Shalev-Shwartz & Zhang, 2013), SVRG (Johnson &
Zhang, 2013), SAGA (Defazio et al., 2014), Katyusha (Allen-Zhu, 2017) and others (Frostig et al., 2015;
Konecný et al., 2015; Allen-Zhu & Yuan, 2016), have proven particularly eﬀective. One of the most popular
variantsofthisapproachisStochasticVariance-ReducedGradient(SVRG),whichachievesvariancereduction
by combining frequent stochastic gradient queries with occasional full batch gradient queries, to optimize the
overall cost of ﬁnding an /epsilon1-approximate solution, where the cost is measured by the total number of queries
to the components ∇ψi(x).
1Under review as submission to TMLR
Manystochasticsecond-ordermethodshavealsobeenproposedforsolvingﬁnite-summinimization, including
Subsampled Newton (Erdogdu & Montanari, 2015; Roosta-Khorasani & Mahoney, 2019; Bollapragada et al.,
2018; Berahas et al., 2020), Newton Sketch (Pilanci & Wainwright, 2016; 2017; Dereziński et al., 2021), and
others (Kovalev et al., 2019; Moritz et al., 2016; Tripuraneni et al., 2018; Mokhtari et al., 2018). These
approaches are generally less sensitive to hyperparameters such as the step size, and they typically query
larger random batches of component gradients/Hessians at a time, as compared to stochastic ﬁrst-order
methods. The larger queries make these methods less sequential, allowing for more eﬀective vectorization
and parallelization.
A number of works have explored whether second-order information can be used to accelerate stochastic
variance-reduced methods, resulting in several algorithms such as Preconditioned SVRG (Gonen et al.,
2016), SVRG2 (Gower et al., 2018) and others (Gower et al., 2016; Liu et al., 2019). However, these are
still primarily stochastic ﬁrst-order methods, highly sequential and with a problem-dependent step size.
Comparatively little work has been done on using variance reduction to accelerate stochastic Newton-type
methods for convex ﬁnite-sum minimization (see discussion in Section 2.3). To that end, we ask:
Can variance reduction accelerate local convergence of Stochastic Newton
in convex ﬁnite-sum minimization?
We show that the answer to this question is positive. The method that we use to demonstrate this, which
we call Stochastic Variance-Reduced Newton (SVRN), retains the positive characteristics of second-order
methods, including easily parallelizable large-batch gradient queries, as well as minimal hyperparameter
tuning (e.g., accepting a unit step size). We prove that, when the number of components ψiis suﬃciently
large, SVRN achieves a better parallel complexity than SVRG, and a better sequential complexity than the
corresponding Stochastic Newton method (see Table 1).
2 Main result
In this section, we present our main result, which is the parallel and sequential complexity analysis of the
local convergence of SVRN. The algorithm itself is discussed in detail in Section 3.
We now present the assumptions needed for our main result, starting with µ-strong convexity of fand
λ-smoothness of each ψi. These are standard for establishing linear convergence rate of SVRG. Our result
also requires Hessian regularity assumptions (Deﬁnition 1), which are standard for Newton’s method and
only aﬀect the size of the local convergence neighborhood.
Assumption 1 We assume that f(x) =1
n/summationtextn
i=1ψi(x)has continuous ﬁrst and second derivatives, as well
as a bounded condition number κ=λ/µ, whereµandλare deﬁned as follows:
1. Function fisµ-strongly convex, i.e.,
f(x)≥f(x/prime) +∇f(x/prime)/latticetop(x−x/prime) +µ
2/bardblx−x/prime/bardbl2;
2. Each of the ncomponents ψiisλ-smooth, i.e.,
ψi(x)≤ψi(x/prime) +∇ψi(x/prime)/latticetop(x−x/prime) +λ
2/bardblx−x/prime/bardbl2.
TohighlighttheparallelizabilityofSVRNduetolargemini-batches, aswellastheeﬀectofvariancereduction
on its performance, we will consider two standard complexity measures:
1.Parallel complexity: Number of batch gradient queries, i.e., times the algorithm computes a
gradient at an iterate, either over the full batch or a mini-batch. This corresponds to the standard
PRAM model.
2.Sequential complexity: Number of passes over the data. One pass corresponds to nqueries of
component gradients ∇ψi(x), possibly at diﬀerent iterates. This is a standard measure of complexity
for stochastic ﬁrst-order methods.
2Under review as submission to TMLR
Second-order Parallel (batch queries) Sequential (data passes) Speed-up
Gradient Descent x O(κlog(1//epsilon1)) O(κlog(1//epsilon1)) x
Accelerated GD x O(√κlog(1//epsilon1)) O(√κlog(1//epsilon1)) x
Stochastic Newton O(αlog(1//epsilon1)) O(αlog(1//epsilon1)) x
Mini-batch SVRG x O(κlog(1//epsilon1)) O(log(1//epsilon1)) O(κ)
Mini-batch Katyusha x O(√κlog(1//epsilon1)) O(log(1//epsilon1)) O(√κ)
SVRN (this work) O(αlog(1//epsilon1)) O/parenleftbiglog(1//epsilon1)
log(n)/parenrightbig/vextendsingle/vextendsingle/vextendsingleO(αlog(n))
Table 1: Comparison of local convergence behavior for SVRN and related stochastic methods in the big data
regime, i.e., n/greatermuchκ, along with full-batch Gradient Descent (GD), and Accelerated GD. Time complexities
are obtained by ﬁrst optimizing parallel time (batch queries), and then optimizing sequential time (data
passes). For the second-order methods, we assume a Hessian α-approximation (2) where α/lessmuchκ.
Our goal is to design a highly parallelizable algorithm, so in our analysis, we will ﬁrst optimize over parallel
complexity, and then over sequential complexity. Note that for any algorithm using only full gradients
(such as the standard versions of gradient descent or Subsampled Newton), the two notions of complexity
are exactly equivalent. For example, in gradient descent (GD), both parallel and sequential complexity is
O(κlog(1//epsilon1)). By introducing stochastic gradients and variance reduction, as in SVRG, we can improve upon
the sequential complexity of GD, while preserving (but not improving) its parallel complexity. Speciﬁcally,
whenn/greatermuchκ, then SVRG with optimal mini-batch size takes O(log(1//epsilon1))sequential time to ﬁnd an /epsilon1-
approximate solution, however it still needs O(κlog(1//epsilon1))parallel time (Table 1).
We can avoid the dependence of parallel complexity on the condition number κby using second-order
information. In particular, suppose that in each iteration we compute the full gradient ∇f(x)and are given
access to a (typically stochastic) Hessian estimate ˜Hsuch that for some 1≤α/lessmuchκ,
(Hessianα-approximation)1√α∇2f(x)/precedesequal˜H/precedesequal√α∇2f(x), (2)
Then, a standard Stochastic Newton (SN) update, given below in (3), can achieve parallel and sequential
complexity of O(αlog(1//epsilon1))locally in the neighborhood of the optimum. It is thus natural to ask whether
we can use stochastic gradients and variance reduction to accelerate the local sequential complexity of this
method, while preserving its parallel complexity. Our main result shows not only that this is possible, but
also, remarkably, that this acceleration gets more signiﬁcant the larger the data size n. See also Theorem 3
for algorithmic details and convergence analysis.
Theorem 1 (informal Theorem 3) Suppose that Assumption 1 holds and: (a) fhas a Lipschitz Hessian,
or (b)fis self-concordant. Moreover, let n/greatermuchκ/greatermuchα. There is an algorithm (SVRN) and an open
neighborhood Usuch that, given any x∈Uwith a corresponding Hessian α-approximation as in (2), the
cost of returning ˜xsuch thatf(˜x)−f(x∗)≤/epsilon1·(f(x)−f(x∗))is as follows:
Parallel time =O/parenleftbig
αlog(1//epsilon1)/parenrightbig
batch queries and Sequential time =O/parenleftBiglog(1//epsilon1)
log(n)/parenrightBig
data passes.
Remark 1 SVRN improves on the sequential complexity of Stochastic Newton by O(αlog(n)), while retain-
ing the same parallel complexity. Moreover, if α≤2, then the algorithm accepts a unit step size, and still
achievesO(log(n))acceleration. Note that this acceleration improves with the problem size n, which is a
unique property of SVRN.
Remark 2 To ﬁnd an initialization point for SVRN, one can simply run a few iterations of a Subsampled
Newton method with line search. In Section 4, we propose a globally convergent algorithm based on this
approach (SVRN-HA; see Algorithm 1), and in Section 5 we show empirically that it substantially accelerates
Subsampled Newton.
3Under review as submission to TMLR
2.1 Discussion
In this section, we compare the local convergence complexity of SVRN to standard stochastic ﬁrst-order and
second-order algorithms. In this comparison, we focus on what we call the big data regime, i.e., n/greatermuchκ, which
is of primary interest in the literature on Subsampled Newton methods. Then, in Section 2.2, we illustrate
how SVRN can be further improved via sketching and importance sampling, when solving problems with
additional structure, such as least squares.
Comparison to SVRG and Katyusha. As we can see in Table 1, ﬁrst-order algorithms, including
variance-reducedmethodssuchasSVRG(Johnson&Zhang,2013), anditsacceleratedvariantslikeKatyusha
(Allen-Zhu, 2017), suﬀer from a dependence on the condition number κin their parallel complexity. Namely,
they require either O(κlog(1//epsilon1))orO(√κlog(1//epsilon1))batch gradient queries, compared to O(αlog(1//epsilon1))for
SVRN and Stochastic Newton, where αis the Hessian approximation factor, which is often much smaller
than√κ. This is because, unlike SVRN, these methods do not scale well to large mini-batches, making them
less parallelizable.
Another diﬀerence between SVRN and SVRG or Katyusha is that, when the Hessian approximation is
suﬃciently accurate ( α≤2), then SVRN accepts a unit step size, which leads to optimal convergence rate
without any tuning. On the other hand, the optimal step size for SVRG depends on the strong convexity
and smoothness constants µandλ, and thus, requires tuning.
Comparison to Stochastic Newton. We next compare SVRN with Stochastic Newton methods such
as Subsampled Newton and Newton Sketch. Here, the most standard proto-algorithm considered in the
literature is the following update:
˜xs+1=˜xs−˜H−1∇f(˜xs). (3)
As mentioned earlier, this update uses only full gradients, so both its parallel and sequential complexity is
O(αlog(1//epsilon1))(see Stochastic Newton in Table 1). On the other hand, SVRN provides a direct acceleration
of the sequential complexity without sacriﬁcing any parallel complexity.
The Hessian α-approximation ˜H≈∇2f(˜xs)can be produced in a number of ways, but perhaps the most
relevant for this discussion is Hessian subsampling, a.k.a. Subsampled Newton (e.g., see Roosta-Khorasani
& Mahoney, 2019). In this setting, an α-approximation with α≤2can be obtained with high probability
by sampling k=O(κlog(d))component Hessians (see Appendix D.2). To obtain a coarser but still useful
approximation (i.e., with a moderately larger α), we can often use far fewer samples, e.g., as shown by
(Erdogdu & Montanari, 2015). On the other hand, if we wanted to recover SVRN’s sequential complexity
ofO/parenleftbiglog(1//epsilon1)
log(n)/parenrightbig
by improving the Hessian approximation in Subsampled Newton, the required Hessian sample
sizekwould become at least as large as n, meaning that we would essentially have to use the exact Hessian
(i.e., Newton’s method), which is highly undesirable.
We note that some of the literature on Subsampled Newton proposes to subsample both the Hessian and
the gradient (e.g., Bollapragada et al., 2018), which would be akin to ˜xs+1=˜xs−˜H−11
m/summationtextm
i=1∇ψIi(˜xs).
However, as is noted in the literature, to maintain linear convergence of such a method, one has to keep
increasing the gradient sample size at an exponential rate, which means that, for ﬁnite-sum minimization,
we quickly revert back to the full gradient (see experiments in Section 5.2).
2.2 Accelerating SVRN with sketching and importance sampling
When the minimization task possesses additional structure, then we can combine SVRN with Hessian and
gradient estimation techniques other than uniform subsampling. For example, one such family of techniques,
called randomized sketching (Drineas & Mahoney, 2016; Woodruﬀ, 2014; Dereziński & Mahoney, 2021), is
applicable when the Hessian can be represented by a decomposition ∇2f(x) =Af(x)/latticetopAf(x) +C, where
Af(x)is a talln×dmatrix and Cis a ﬁxedd×dmatrix. This setting applies for many empirical risk
minimization tasks, including linear and logistic regression, among others.
4Under review as submission to TMLR
SketchingcanbeusedtoconstructanestimateoftheHessianbyapplyingarandomizedlineartransformation
toAf(x), represented by a k×nrandom matrix S, wherek=˜O(d)is much smaller than n. Using standard
sketching techniques, such as Subsampled Randomized Hadamard Transforms (SRHT, Ailon & Chazelle,
2009), Sparse Johnson-Lindenstrauss Transforms (SJLT, Clarkson & Woodruﬀ, 2017; Meng & Mahoney,
2013; Nelson & Nguyên, 2013) and Leverage Score Sparsiﬁed embeddings (LESS, Dereziński et al., 2021),
we can construct a Hessian estimate that satisﬁes the requirements of Theorem 1 at the cost of ˜O(nd+d3),
which corresponds to a nearly-constant number of data passes and d×dmatrix multiplies. In particular,
this eliminates the dependence of Hessian estimation on the condition number.
Another way of making SVRN more eﬃcient is to use importance sampling in the stochastic gradient mini-
batches. Importance sampling can be introduced to any ﬁnite-sum minimization task (1) by specifying an
n-dimensional probability vector p= (p1,...,pn), such that/summationtext
ipi= 1, and sampling the component gradient
ψIi(x)so that the index Iiis drawn according to p. With the right choice of importance sampling, we can
substantially reduce the smoothness parameter λ, and thereby, the condition number κof the problem (see
Appendix A.4).
The above techniques can be used to accelerate SVRN, for instance, in the important task of solving least
squares regression. Here, given an n×dmatrix Awith rows a/latticetop
iand ann-dimensional vector y, the objective
being minimized is the following quadratic function:
f(x) =1
2n/bardblAx−y/bardbl2=1
nn/summationdisplay
i=11
2(a/latticetop
ix−yi)2. (4)
One of the popular methods for solving the least squares task, known as the Iterative Hessian Sketch (IHS,
Pilanci & Wainwright, 2016), is exactly the Stochastic Newton update (3), where the Hessian estimate ˜His
constructed via sketching. In this context, SVRN can be viewed as an accelerated version of IHS. To fully
leverage the structure of the least squares problem, we use a popular importance sampling technique called
leverage score sampling (Drineas et al., 2006; 2012), where the importance probabilities are (approximately)
proportional to pi∝a/latticetop
i(A/latticetopA)−1ai. Through an adaptation of our main result, we show that a version
of SVRN for least squares, with sketched Hessian and leverage score sampled gradients, improves on the
state-of-the-art complexity for reaching a high-precision solution to a preconditioned least squares task from
O(ndlog(1//epsilon1))(Rokhlin & Tygert, 2008; Avron et al., 2010; Meng et al., 2014) to O/parenleftbig
ndlog(1//epsilon1)
log(n/d)/parenrightbig
. See
Appendix A.4 for proof and further discussion.
Theorem 2 (Fast least squares solver) Given A∈Rn×dandy∈Rn, afterO(ndlogn+d3logd)pre-
processing cost to ﬁnd the sketched Hessian estimate ˜Hand an approximate leverage score distribution,
SVRN ﬁnds ˜xso that
f(˜x)≤(1 +/epsilon1)f(x∗)inO/parenleftBig
ndlog(1//epsilon1)
log(n/d)/parenrightBig
time.
Crucially, the SVRN-based least squares solver only requires a preconditioner ˜Hthat is a constant factor
approximation of the Hessian, i.e., α=O(1). Interestingly, our approach of transforming the problem via
leverage score sampling appears to be connected to a weighted and preconditioned SGD algorithm of (Yang
et al., 2017) for solving a more general class of /lscriptp-regression problems. We expect that Theorem 2 can be
similarly extended beyond least squares regression.
2.3 Further related work
As mentioned in Section 1, a number of works have aimed to accelerate ﬁrst-order variance reduction meth-
ods by preconditioning them with second-order information. For example, (Gonen et al., 2016) proposed
Preconditioned SVRG for ridge regression. The eﬀect of this preconditioning, as in related works (Liu
et al., 2019), is a reduced condition number κof the problem. This is diﬀerent from Theorem 1, which uses
preconditioning to make variance reduction eﬀective for large mini-batches and with a unit step size.
Some works have shown that, instead of preconditioning, one can use momentum to accelerate variance
reduction, andalsotoimproveitsconvergenceratewhenusingmini-batches. ThesemethodsincludeCatalyst
5Under review as submission to TMLR
(Lin et al., 2015) and Katyusha (Allen-Zhu, 2017). However, unlike SVRN, these approaches are still limited
to fairly small mini-batches, as demonstrated in Table 1.
A number of works have proposed applying techniques inspired by variance reduction to stochastic Newton-
type methods in settings which are largely incomparable to ours. First, (Rodomanov & Kropotov, 2016; Ko-
valev et al., 2019) consider algorithms where the Hessian and gradient information is incrementally updated
with either individual samples or mini-batches. However, the approximate Hessian information required
by these methods is quite diﬀerent than the one used in SVRN: they require the Hessian estimate to be
initialized with allncomponent Hessians, possibly computed at diﬀerent locations (compared to, e.g., a
subsampled estimate). For example, in the case of least squares, this means computing the exact Hessian,
which costs O(nd2)time and renders the task trivial (compare this to our Theorem 2, where the Hessian
estimate required by SVRN can be approximated eﬃciently). Setting this aside, we can still compare the
local convergence rate of SVRN with the Stochastic Newton method of (Kovalev et al., 2019, Theorem 1) us-
ing the same mini-batch size. Assuming n/greatermuchκand using the setup from Theorem 1, their method achieves
O(log(1//epsilon1))sequential complexity, whereas SVRN obtains the accelerated complexity of O/parenleftbiglog(1//epsilon1)
log(n)/parenrightbig
data
passes.
In the non-convex setting, variance reduction was used by (Zhou et al., 2019; Zhang et al., 2022) to accelerate
Subsampled Newton with cubic regularization. They use variance reduction both for the gradient and the
Hessian estimates. Also, (Wang et al., 2017) incorporate variance reduction into a stochastic quasi-Newton
method. However, due to the non-convex setting, these results are incomparable to ours, as we are focusing
on strongly convex optimization.
3 Local convergence analysis of SVRN
In this section, we present the convergence analysis for SVRN, leading to the proof of Theorem 1.
Notation. Ford×dpositive semideﬁnite matrices AandB, we deﬁne/bardblv/bardblA=√
v/latticetopAv, and we say
thatA≈/epsilon1B, when (1−/epsilon1)B/precedesequalA/precedesequal(1 +/epsilon1)B, where/precedesequaldenotes the positive semideﬁnite ordering (we
deﬁne analogous notation a≈/epsilon1bfor non-negative scalars a,b). We usecandCto denote positive absolute
constants, and let I∼[n]denote a uniformly random sample from {1,..,n}.
We will present the analysis in a slightly more general setting of expected risk minimization, i.e., where
f(x) =Eψ∼D[ψ(x)]. Here,Dis a distribution over convex functions ψ:Rd→R. Clearly, this setting
subsumes (1), since we can let Dbe a uniformly random sample ψi. Thanks to this extension, our results
can apply to importance sampling of component functions, as in Theorem 2.
Deﬁnition 1 We deﬁne the local convergence neighborhood Uf(/epsilon1), parameterized by /epsilon1∈(0,1), as:
1. Iffhas anL-Lipschitz Hessian, then Uf(/epsilon1) ={x:/bardblx−x∗/bardbl∇2f(x∗)</epsilon1µ3/2/L};
2. Iffis self-concordant, then we use Uf(/epsilon1) ={x:/bardblx−x∗/bardbl∇2f(x∗)</epsilon1/4}.
Our local convergence analysis is captured by the following theorem, which provides the rate of convergence
after one outer iteration of SVRN (stated below), for a range of mini-batch sizes m.
Theorem 3 (Convergence rate of SVRN) Suppose that Assumption 1 holds, α≥1, and either: (a) f
has a Lipschitz continuous Hessian, or (b) fis self-concordant. There is an absolute constant c >0such
that if ˜xs∈Uf(1/cα), and we are given the gradient ˜gs=∇f(˜xs)as well as a Hessian α-approximation,
i.e., ˜Hsuch that1√α∇2f(˜xs)/precedesequal˜H/precedesequal√α∇2f(˜xs), then, letting x0=˜xsand:
xt+1=xt−η˜H−1/parenleftbigg1
mm/summationdisplay
i=1∇ψi(xt)−∇ψi(˜xs) +˜gs/parenrightbigg
, ψ 1,...,ψm∼D,
6Under review as submission to TMLR
Complexity of SVRN
Parallel complexity
Sequential complexity
Figure 1: Illustration of the local convergence complexity analysis for SVRN, as a function of the mini-batch
sizem, with the number of inner iterations set to tmax=n/m. As we decrease the mini-batch size from
n(standard Stochastic Newton; SN) downto m≈n
αlog(n/κ)(optimal SVRN), the sequential complexity
(number of passes over the data) improves by O(αlog(n)), while the parallel complexity (number of batch
gradient queries) remains optimal.
aftertiterations with mini-batch size m≥cα2κlog(t/δ)and step size η= min{/radicalbig
2/α,1}, the iterate
˜xs+1=xt(i.e., one outer iteration of SVRN) with probability 1−δsatisﬁes:
f(˜xs+1)−f(x∗)
f(˜xs)−f(x∗)≤/parenleftBig
1−1
2α/parenrightBigt
+cα2log(t/δ)κ
m.
The proof of Theorem 3, which is given in Appendix A, relies on a new high-probability bound for the error
of the variance-reduced gradient estimates in the large mini-batch regime, measured using the vector norm
deﬁned by the inverse Hessian at the optimum (Lemma 3). Unlike results from prior work, which hold in
expectation, this bound crucially relies on the iterate being in the local neighborhood. Also, unlike standard
SVRG analysis, we achieve our convergence guarantee for the last iterate of SVRN’s inner loop (as opposed
a random or averaged iterate), which is again enabled by exploiting local second-order information.
Discussion. For simplicity, let us ﬁx the number of inner iterations tmax=n/m, so that a single outer
iteration of SVRN always takes O(1)passes over the data. Then, we can deﬁne the linear convergence rate
after one outer iteration as a function of mini-batch size m:
ρm:=/parenleftBig
1−1
2α/parenrightBign/m
+˜O(κ/m).
Let us assume the big data regime, i.e., n/greatermuchκ. If we only use full-batch gradients ( m=n), then the ﬁrst
term in the rate dominates, and we have ρm≈1−1
2α, which is similar to what we would get using standard
Stochastic Newton (3). As we decrease m(and change tmaxaccordingly), the ﬁrst term in ρmdecreases,
whereas the second term increases. As a result, the overall rate rapidly improves, reaching its optimal value
ofρm=˜O(κ/n)form≈n
αlog(n/κ).
Complexity analysis. The complexity analysis given in Theorem 1 follows directly from the above dis-
cussion, since the sequential complexity (number of data passes needed to improve by factor /epsilon1) is given by
O/parenleftbiglog(1//epsilon1)
log(1/ρm)/parenrightbig
, whereas the parallel complexity (number of batch gradient queries) is O/parenleftbig
tmax·log(1//epsilon1)
log(1/ρm)/parenrightbig
. In
Figure 1, we illustrate how these quantities change as a function of m. In particular, we observe that the
batch gradients essentially stay ﬂat at O(αlog(1//epsilon1))as we decrease m, until reachingn
αlog(n/κ). On the other
hand, the data pass complexity decreases linearly with m, until it reaches the optimal value of O/parenleftbiglog(1//epsilon1)
log(n/κ)/parenrightbig
,
which, for suﬃciently large n, recovers Theorem 1.
7Under review as submission to TMLR
Input: iterate ˜x0, gradient batch size m, Hessian sample size k, and local iterations tmax;
Initialize step size η−1= 0and Hessian estimate ˜H−1=0;
fors= 0,1,2,...do
Compute the subsampled Hessian: /hatwideHs=1
k/summationtextk
i=1∇2ψi(˜xs), forψ1,...,ψk∼D;
Compute the Hessian average: ˜Hs=s
s+1˜Hs−1+1
s+1/hatwideHs;
Compute the full gradient: ˜gs=∇f(xs);
ifηs−1<1then
Compute the descent direction ˜vsby solving: ˜Hs˜vs=−˜gs;
else
Initialize x0=˜xs;
fort= 0,...,t max−1do
Compute/hatwidegt(xt)and/hatwidegt(˜xs), for/hatwidegt(x) =1
m/summationtextm
i=1∇ψi(x),ψ1,...,ψm∼D;
Compute variance-reduced gradient ¯gt=/hatwidegt(xt)−/hatwidegt(˜xs) +˜gs;
Compute the descent direction vtby solving: ˜Hsvt=−¯gt;
Update xt+1=xt+vt
end
Compute the descent direction: ˜vs=xtmax−˜xs;
end
Computeηsfor iterate ˜xsand direction ˜vsusing the Armijo condition;
Update ˜xs+1=˜xs+ηs˜vs;
end
Algorithm 1: SVRN with Hessian Averaging (SVRN-HA)
4 Globally convergent algorithm
We next present a practical stochastic second-order method (see Algorithm 1, called SVRN-HA) which uses
SVRN to accelerate its local convergence phase.
The key in implementing SVRN is that the algorithm is guaranteed to converge with unit step size only once
we reach a local neighborhood of the optimum, and if we have a suﬃciently accurate Hessian estimate. For
this reason, we introduce an initial phase of the algorithm, in which a standard Stochastic Newton method
is ran, using the Armijo line search to select the step size. Once the method reaches the local convergence
neighborhood, as long as the Hessian estimates are accurate enough, the line search is guaranteed to return
a unit step size. At this point, the algorithm switches to SVRN and achieves acceleration. Finally, to ensure
that we reach a suﬃciently accurate Hessian estimate, our Stochastic Newton method should gradually
increase the accuracy of the Hessian estimates.
Based on these insights, we propose an algorithm called Stochastic Variance-Reduced Newton with Hessian
Averaging (SVRN-HA). In the initial phase, this algorithm is a variant of Subsampled Newton, based on a
method proposed by (Na et al., 2022), where, at each iteration, we construct a subsampled Hessian estimate
based on a ﬁxed sample size k. To increase the accuracy over time, all past Hessian estimates are averaged
together, and the result is used to precondition the full gradient. At each iteration, we check whether
the last line search returned a unit step size. If yes, then we start running SVRN with local iterations
tmax=⌊log2(n/d)⌋and gradient batch size m=⌊n/log2(n/d)⌋, wherenis the number of data points and d
is the dimension. In the following result, we establish global convergence of SVRN-HA, by showing that the
global phase of this method will not only reach any local neighborhood, but also that the Hessian estimate
will get progressively more accurate, eventually reaching the desired approximation accuracy.
Theorem 4 Letfbe as in Theorem 3. For any neighborhood Uaround the optimum, Algorithm 1 will
almost surely reach a point where: (a) ˜xsbelongs to the neighborhood U, and (b) the Hessian estimate ˜Hs
satisﬁes the condition in Theorem 3. At this point, the line search will return ηs= 1.
8Under review as submission to TMLR
(a) Convergence
 (b) Runtime
Figure2: ConvergenceandruntimecomparisonofSVRN-HAontheEMNISTdatasetagainstthreebaselines:
classical Newton, SVRG (after parameter tuning), and Subsampled Newton with Hessian Averaging (SN-
HA), i.e., the global phase of Algorithm 1, ran without switching to SVRN. Further results on the CIFAR-10
dataset are in Appendix B.
5 Experiments
WenextdemonstratenumericallythatSVRNcanbeeﬀectivelyusedtoacceleratestochasticNewtonmethods
in practice. We also show how variance reduction can be incorporated into a globally convergent Subsampled
Newton method in a way that is robust to hyperparameters and preserves its scalability thanks to large-batch
operations.1
5.1 Logistic regression experiment
In this section, we present numerical experiments for solving a regularized logistic loss minimization task.
For ann×ddata matrix Awith rows a/latticetop
i, ann-dimensional target vector y(with±1entriesyi) and a
regularization parameter γ, our task is to minimize:
f(x) =1
nn/summationdisplay
i=1log(1 + e−yia/latticetop
ix) +γ
2/bardblx/bardbl2. (5)
As a dataset, we used the Extended MNIST dataset of handwritten digits (EMNIST, Cohen et al., 2017) with
n= 500k datapoints, transformed using a random features map (with dimension d= 1000). Experimental
details, as well as further results on the CIFAR-10 dataset and several synthetic data matrices, are presented
in Appendix B.
In Figure 2, we compared SVRN-HA to three baselines which are most directly comparable: (1) the classical
Newton’s method; (2) SVRG with the step size and number of inner iterations tuned for best wall-clock
time; and (3) Subsampled Newton with Hessian Averaging (SN-HA), i.e., the method we use in the global
phase of Algorithm 1 (without the SVRN phase). All of the convergence plots are averaged over multiple
runs. For both SVRN-HA and SN-HA we use Hessian sample size k= 4d.
From Figure 2(a), we conclude that as soon as SVRN-HA exits the initial phase of the optimization, it
accelerates dramatically, to the point where it nearly matches the rate of classical Newton. This accelera-
tion corresponds to the improvement in sequential complexity from O(αlog(1//epsilon1))for Stochastic Newton to
O(log(1//epsilon1)
log(n))for SVRN. Finally, the convergence of SVRG is initially quite fast, but over time, it stabilizes at a
slower rate, indicating that the Hessian information plays a signiﬁcant role in the performance of SVRN-HA.
In Figure 2(b), we plot the wall clock time of the algorithms. Here, SVRN-HA also performs better than
all of the baselines, despite some additional per-iteration overhead. We expect that this can be further
1The code is available at https://github.com/svrnewton/svrn .
9Under review as submission to TMLR
(a) Comparison of three variants of SVRN-HA (alongside
SN-HA), depending on how frequently we resample data
points used to compute the gradient estimate. We consider
threevariantsofSVRN-HA:(1)samplingoncefortheentire
optimization, (2) sampling once for each full gradient stage
(per stage), (3) sampling in each small step (per step).
(b) Comparison of SVRN-HA (alongside SN-HA) against
Subsampled Newton with Gradient Subsampling (SNGS-
HA), which is implemented exactly like SVRN-HA except
without the variance-reducing correction.
Figure 3: How diﬀerent types of gradient estimation aﬀect the convergence properties of SVRN.
optimized. Finally, we note that Newton’s method is drastically slower than all other methods due to the
high cost of solving a large linear system, and the per-iteration time of SVRG is substantially slowed by its
sequential nature.
5.2 Further investigations on a least squares task
We next study the setting of least squares regression (4) to analyze the trade-oﬀs in convergence for diﬀerent
implementations of SVRN, as we vary the gradient and Hessian estimation schemes. We evaluated the
algorithms on synthetic data matrices, as deﬁned in Appendix B.
Frequency of gradient resampling. Our theoretical analysis requires that for each small step of SVRN,
a fresh sample of components ψiis used to compute the gradient estimates. However, in Lemma 3 we showed
that, after variance reduction, the gradient estimates are accurate with high probability, which suggests that
we might be able to reuse previously sampled components. While this technically does not improve the
number of required gradient queries, it can substantially reduce the communication cost for some practical
implementations. In Figure 3(a), we investigate how much the convergence rate of SVRN-HA is aﬀected by
the frequency of component resampling for the gradient estimates. Recall that in all our experiments, we
use a gradient sample size of m=⌊n/log2(n/d)⌋. We consider the following variants:
1. Sampling once : an extreme policy of sampling one set of components and reusing them for all
gradient estimates;
2. Sampling per stage : an intermediate policy of resampling the components after every full gradient
computation.
3. Sampling per step : the policy which is used in our theory, i.e., resampling the gradients at every
step of the inner loop of the algorithm.
From Figure 3(a) we conclude that, while all three variants of SVRN-HA converge and are competitive with
SN-HA, the extreme policy of sampling once leads to a substantial degradation in convergence rate, whereas
sampling per stage and sampling per step perform very similarly. Thus, our overall recommendation is
to resample the components at every stage of SVRN-HA, but reuse the sample for the small steps of the
algorithm (this is what we used for the EMNIST and CIFAR-10 experiments).
10Under review as submission to TMLR
(a) Convergence comparison of SVRN and SN using ﬁxed
Hessian estimates (i.e., without Hessian averaging). Here,
hdenotes the number of Hessian samples used to generate
the estimate.
(b) Convergence comparison of SVRN-HA and SN-HA,
with and without preconditioning using a Randomized
Hadamard Transform (RHT), for a high-coherence least
squares dataset.
Figure 4: How Hessian sample size and data coherence aﬀect the convergence properties of SVRN.
Eﬀect of variance reduction. We next investigate the eﬀect of variance reduction on the convergence
rateofSVRN.WhilegradientsubsamplinghasbeenproposedbymanyworksintheliteratureonSubsampled
Newton, e.g., see (Roosta-Khorasani & Mahoney, 2019), these works have shown that the gradient sample
size must be gradually increased to retain fast local convergence (which means that after a few iterations,
we must use the full gradient). On the other hand, in SVRN, instead of increasing the gradient sample size,
we use variance reduction with a ﬁxed sample size, which allows us to retain the accelerated convergence
indeﬁnitely.
To illustrate this point, in Figure 3(b) we plot how the convergence behavior of our algorithm changes if
we take variance reduction out of it. The resulting method is called Subsampled Newton with Gradient
Subsampling (SNGS-HA). For this experiment, we resample the gradient estimate at every small step (for
both SNGS-HA and SVRN-HA). For the sake of direct comparison, all of the other parameters are retained
from SVRN-HA. In particular, one iteration of SNGS-HA corresponds to ⌊log2(n/d)⌋steps using resampled
gradients, and Hessian averaging occurs once every such iteration. As expected, we observe that, while
initially converging at a fast rate, eventually SNGS-HA reaches a point where the subsampled gradient
estimates are not suﬃciently accurate, resulting in a sudden dramatic drop-oﬀ in the convergence rate, to
thepointwherethemethodvirtuallystopsconvergingaltogether. Ontheotherhand, SVRN-HAcontinuesto
converge at the same fast rate throughout the optimization procedure without any reduction in performance.
This indicates that variance reduction does improve the accuracy of gradient estimates, especially when our
goal is to converge to a high-precision solution.
Eﬀect of Hessian accuracy. In our experiments, for both SVRN and SN, we used Hessian averaging (Na
et al., 2022) to construct the Hessian estimates. This approach is desirable in practice, since it gradually
increases the accuracy of the Hessian estimate as we progress in the optimization. As a result, it is more
robust to the Hessian sample size and we are guaranteed to reach suﬃcient accuracy for SVRN to work well.
In the following experiment, we take Hessian averaging out of the algorithms to provide a better sense of how
the performance of SVRN and SN depends on the accuracy of the provided Hessian estimate. For simplicity,
we focus here on least squares, where the Hessian is the same everywhere, so we can simply construct an
initial Hessian estimate and then use it throughout the optimization. However, our insights apply more
broadly to local convergence for general convex objectives. In Figure 4(a), we plot the performance of the
algorithms as we vary the accuracy of the subsampled Hessian estimates, where hdenotes the number of
samples used to construct the estimate. In all the results, we keep the gradient sample size and local steps
in SVRN ﬁxed as before.
11Under review as submission to TMLR
Remarkably, the performance of SVRN is aﬀected by the Hessian accuracy very diﬀerently than SN. We
observe that SVRN requires a certain level of Hessian accuracy to provide any acceleration over SN. As
soon as this level of Hessian accuracy is reached (by increasing the Hessian sample size h), the peformance
of SVRN quickly jumps to the fast convergence we observed in the other experiments. Further increasing
the accuracy no longer appears to aﬀect the convergence rate of SVRN. This is in contrast to SN, whose
convergence slowly improves as we increase the Hessian sample size. This intriguing phenomenon is actually
fully predicted by our theory for SVRN (together with prior convergence analysis for SN). Our convergence
result (Theorem 3) requires a suﬃciently accurate Hessian inverse estimate for SVRN to work with a unit
step size (which is what is used in SVRN-HA), but the actual rate of convergence is independent of the
Hessian accuracy (only the required number of small steps is aﬀected). We conclude that SVRN is more
desirable than SN when we have a small budget for Hessian samples.
Eﬀect of high coherence. We next analyze the performance of SVRN and SN on a slightly modiﬁed least
squares task. For this experiment, we modify the data matrix A, by multiplying the ith row by 1/√gifor
eachi, wheregiisanindependentrandomvariabledistributedaccordingtotheGammadistrutionwithshape
2and scale 1/2. This is a standard transformation designed to produce a matrix with many rows having
a high leverage score. Recall that the leverage score of the ith row of Ais deﬁned as /lscripti=a/latticetop
i(A/latticetopA)−1ai,
see Appendix A.4. This can be viewed as aﬀecting the component-wise smoothness of the objective, which
hinders subsampling-based estimators of the Hessian and the gradient.
In Figure 4(b), we illustrate how the performance of SVRN-HA and SN-HA degrades for the high-coherence
least squares task, and we also show how this can be addressed by relying on the ideas developed in Section
2.2 (and further discussed in Appendix A.4). First, notice that not only is the convergence rate of both
SVRN-HA and SN-HA worse on the high-coherence dataset than on the previous least squares examples
(e.g., compare with Figure 3(b)), but also, the acceleration coming from variance reduction is drastically
reducedtothepointofbeingnegligible. TheformereﬀectisprimarilycausedbythefactthatuniformHessian
subsampling is much less eﬀective at producing accurate approximations for high-coherence matrices, and
this aﬀects both algorithms similarly (we note that one could construct an even more highly coherent matrix,
for which these methods would essentially stop converging altogether). The latter eﬀect is the consequence
of the fact that gradient subsampling is also adversely aﬀected by high coherence, so it becomes nearly
impossible to produce gradient estimates with uniform sampling that would lead to an accelerated rate, even
with variance reduction. This corresponds to the regime of κ≥nin our theory.
Fortunately, for least squares regression, this phenomenon can be addressed easily. As outlined in Appendix
A.4, we can use one of two strategies: (1) use importance sampling proportional to the leverage scores
ofAfor both the Hessian and gradient estimates; or (2) precondition the problem using the Randomized
Hadamard Transform (RHT) to uniformize all the leverage scores, and then use uniform subsampling. Both
of these methods require roughly O(ndlogn)preprocessing cost and eliminate dependence on the condition
number for both SVRN-HA and SN-HA. The latter strategy is somewhat more straightforward since it does
not require modifying the optimization algorithms, and we apply it here for our high coherence least squares
task: we let SVRN-HA-RHT and SN-HA-RHT denote the two optimization algorithms ran after applying
the RHT preconditioning to the problem. Note that this not only improves the convergence rate of both
methods but also brings back the accelerated rate enjoyed by SVRN-HA in the previous experiments. In
fact, our least squares results (Theorem 2 and Lemma 6) can be directly applied to SVRN-HA-RHT, so
this method and its accelerated convergence rate of ˜O(d/n)is provably unaﬀected by any high-coherence
matrices.
6 Conclusions
We propose and analyze Stochastic Variance-Reduced Newton (SVRN), a provably eﬀective strategy of
incorporatingvariancereductionintopopularstochasticNewtonmethodsforsolvingﬁnite-summinimization
tasks. We show that SVRN improves the local convergence complexity of Subsampled Newton (per data
pass) from O(αlog(1//epsilon1))toO/parenleftbiglog(1//epsilon1)
log(n)/parenrightbig
, while retaining all the beneﬁts of second-order optimization, such
as a simple unit step size and easily scalable large-batch operations.
12Under review as submission to TMLR
References
Nir Ailon and Bernard Chazelle. The fast Johnson–Lindenstrauss transform and approximate nearest neigh-
bors.SIAM Journal on computing , 39(1):302–322, 2009.
Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. The Journal of
Machine Learning Research , 18(1):8194–8244, 2017.
Zeyuan Allen-Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex objectives.
InInternational conference on machine learning , pp. 1080–1089. PMLR, 2016.
HaimAvron, PetarMaymounkov, andSivanToledo. Blendenpik: Supercharginglapack’sleast-squaressolver.
SIAM Journal on Scientiﬁc Computing , 32(3):1217–1236, 2010.
Albert S Berahas, Raghu Bollapragada, and Jorge Nocedal. An investigation of newton-sketch and subsam-
pled newton methods. Optimization Methods and Software , 35(4):661–680, 2020.
Raghu Bollapragada, Richard H Byrd, and Jorge Nocedal. Exact and inexact subsampled newton methods
for optimization. IMA Journal of Numerical Analysis , 39(2), 2018.
Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.
Kenneth L. Clarkson and David P. Woodruﬀ. Low-rank approximation and regression in input sparsity time.
J. ACM, 63(6):54:1–54:45, January 2017.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to
handwritten letters. In 2017 international joint conference on neural networks (IJCNN) , pp. 2921–2926.
IEEE, 2017.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. Advances in neural information processing systems ,
27, 2014.
Michał Dereziński and Michael W Mahoney. Determinantal point processes in randomized numerical linear
algebra. Notices of the American Mathematical Society , 68(1):34–45, 2021.
Michał Dereziński, Jonathan Lacotte, Mert Pilanci, and Michael W Mahoney. Newton-LESS: Sparsiﬁcation
without trade-oﬀs for the sketched newton update. Advances in Neural Information Processing Systems ,
34, 2021.
Michał Dereziński, Zhenyu Liao, Edgar Dobriban, and Michael W Mahoney. Sparse sketches with small
inversion bias. In Proceedings of the 34th Conference on Learning Theory , 2021.
PetrosDrineasandMichaelW.Mahoney. RandNLA:Randomizednumericallinearalgebra. Communications
of the ACM , 59:80–90, 2016.
Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Sampling algorithms for /lscript2regression and
applications. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm , pp.
1127–1136, 2006.
Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruﬀ. Fast approximation
of matrix coherence and statistical leverage. J. Mach. Learn. Res. , 13(1):3475–3506, December 2012.
Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled Newton methods. Advances
in Neural Information Processing Systems , 28:3052–3060, 2015.
Gerald Folland. Advanced calculus . Upper Saddle River, NJ : Prentice Hall, 2002.
Roy Frostig, Rong Ge, Sham M Kakade, and Aaron Sidford. Competing with the empirical risk minimizer
in a single pass. In Conference on learning theory , pp. 728–763. PMLR, 2015.
13Under review as submission to TMLR
Alon Gonen, Francesco Orabona, and Shai Shalev-Shwartz. Solving ridge regression using sketched precon-
ditioned svrg. In International conference on machine learning , pp. 1397–1405. PMLR, 2016.
Robert Gower, Donald Goldfarb, and Peter Richtárik. Stochastic block bfgs: Squeezing more curvature out
of data. In International Conference on Machine Learning , pp. 1869–1878. PMLR, 2016.
Robert Gower, Nicolas Le Roux, and Francis Bach. Tracking the gradients using the hessian: A new look at
variance reducing stochastic methods. In International Conference on Artiﬁcial Intelligence and Statistics ,
pp. 707–715. PMLR, 2018.
Robert Jerrard. Multivariable calculus. https://www.math.toronto.edu/courses/mat237y1/20189/
notes/Contents.html , 2018. Course Notes.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
Advances in neural information processing systems , 26, 2013.
Jakub Konecný, Jie Liu, Peter Richtárik, and Martin Takác. Mini-batch semi-stochastic gradient descent in
the proximal setting. IEEE Journal of Selected Topics in Signal Processing , 10(2):242–255, 2015.
Dmitry Kovalev, Konstantin Mishchenko, and Peter Richtárik. Stochastic newton and cubic newton methods
with simple local linear-quadratic rates. arXiv preprint arXiv:1912.01597 , 2019.
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimization. Advances
in neural information processing systems , 28, 2015.
Yanli Liu, Fei Feng, and Wotao Yin. Acceleration of svrg and katyusha x by inexact preconditioning. In
International Conference on Machine Learning , pp. 4003–4012. PMLR, 2019.
Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and
applications to robust linear regression. In Proceedings of the Forty-ﬁfth Annual ACM Symposium on
Theory of Computing , STOC ’13, pp. 91–100, 2013.
Xiangrui Meng, Michael A Saunders, and Michael W Mahoney. LSRN: A parallel iterative solver for strongly
over-or underdetermined systems. SIAM Journal on Scientiﬁc Computing , 36(2):C95–C118, 2014.
Stanislav Minsker. On some extensions of bernstein’s inequality for self-adjoint operators. Statistics &
Probability Letters , 127:111–119, 2017.
Aryan Mokhtari, Mark Eisen, and Alejandro Ribeiro. Iqn: An incremental quasi-newton method with local
superlinear convergence rate. SIAM Journal on Optimization , 28(2):1670–1698, 2018.
Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic l-bfgs algorithm. In
Artiﬁcial Intelligence and Statistics , pp. 249–258. PMLR, 2016.
Sen Na, Michał Dereziński, and Michael W Mahoney. Hessian averaging in stochastic newton methods
achieves superlinear convergence. Mathematical Programming , 2022.
Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra algorithms via sparser subspace
embeddings. In 2013 ieee 54th annual symposium on foundations of computer science , pp. 117–126. IEEE,
2013.
Mert Pilanci and Martin J Wainwright. Iterative hessian sketch: Fast and accurate solution approximation
for constrained least-squares. The Journal of Machine Learning Research , 17(1):1842–1879, 2016.
Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm with
linear-quadratic convergence. SIAM Journal on Optimization , 27(1):205–245, 2017.
Anton Rodomanov and Dmitry Kropotov. A superlinearly-convergent proximal newton-type method for the
optimization of ﬁnite sums. In International Conference on Machine Learning , pp. 2597–2605. PMLR,
2016.
14Under review as submission to TMLR
Vladimir Rokhlin and Mark Tygert. A fast randomized algorithm for overdetermined linear least-squares
regression. Proceedings of the National Academy of Sciences , 105(36):13212–13217, 2008.
Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods. Mathematical Program-
ming, 174(1):293–326, 2019.
Nicolas Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential conver-
gence rate for ﬁnite training sets. Advances in neural information processing systems , 25, 2012.
Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In Proceedings
of the 47th Annual IEEE Symposium on Foundations of Computer Science , FOCS ’06, pp. 143–152,
Washington, DC, USA, 2006. IEEE Computer Society.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss mini-
mization. Journal of Machine Learning Research , 14(2), 2013.
Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeﬀrey Regier, and Michael I Jordan. Stochastic cubic regular-
ization for fast nonconvex optimization. Advances in neural information processing systems , 31, 2018.
Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational
Mathematics , 12(4):389–434, August 2012.
Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-newton methods for nonconvex
stochastic optimization. SIAM Journal on Optimization , 27(2):927–956, 2017.
David P Woodruﬀ. Sketching as a tool for numerical linear algebra. Foundations and Trends R/circlecopyrtin Theoretical
Computer Science , 10(1–2):1–157, 2014.
Jiyan Yang, Yin-Lam Chow, Christopher Ré, and Michael W Mahoney. Weighted sgd for lp regression with
randomized preconditioning. The Journal of Machine Learning Research , 18(1):7811–7853, 2017.
Junyu Zhang, Lin Xiao, and Shuzhong Zhang. Adaptive stochastic variance reduction for subsampled newton
method with cubic regularization. INFORMS Journal on Optimization , 4(1):45–64, 2022.
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic variance-reduced cubic regularization methods. J.
Mach. Learn. Res. , 20(134):1–47, 2019.
A Proofs for local convergence analysis of SVRN
In this section, we provide the proofs of our main technical results, i.e., local convergence analysis for SVRN.
First, we prove the result for the general case (Theorem 3), then we prove the result for least squares
(Theorem 2).
A.1 Preliminaries
First, let us recall the formal deﬁnitions of the standard Hessian regularity assumptions used in Theorem 3.
For all our results, it is suﬃcient that the function fsatisﬁes either one of these assumptions.
Assumption 2 Functionf:Rd→Rhas Lipschitz continuous Hessian with constant L, i.e.,/bardbl∇2f(x)−
∇2f(x/prime)/bardbl≤L/bardblx−x/prime/bardblfor all x,x/prime∈Rd.
Assumption 3 Functionf:Rd→Ris self-concordant, i.e., for all x,x/prime∈Rd, the function φ(t) =f(x+tx/prime)
satisﬁes:|φ/prime/prime/prime(t)|≤2(φ/prime/prime(t))3/2.
In the proof, we use the following version of Bernstein’s concentration inequality for random vectors (Corol-
lary 4.1 in Minsker, 2017).
15Under review as submission to TMLR
Lemma 1 Letv1,...,vm∈Rdbe independent random vectors such that E[vi] =0and/bardblvi/bardbl≤Ralmost
surely. Denote σ2:=/summationtextm
i=1E/bardblvi/bardbl2. Then, for all t2≥σ2+tR/3, we have
Pr/braceleftbigg/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1vi/vextenddouble/vextenddouble/vextenddouble>t/bracerightbigg
≤28 exp/parenleftBig
−t2/2
σ2+tR/3/parenrightBig
.
We also use the following lemma to convert from convergence in the norm, /bardblx−x∗/bardblH, to convergence in
excess loss, f(x)−f(x∗), in the neighborhood around the optimum x∗. The proof of this lemma, given in
Appendix A.3, uses Quadratic Taylor’s Theorem.
Lemma 2 Iffsatisﬁes Assumption 1 and either Assumption 2 or 3, then for any /epsilon1∈[0,1]andx∈Uf(/epsilon1l),
we have:
∇2f(x)≈/epsilon1l∇2f(x∗)andf(x)−f(x∗)≈/epsilon1l1
2/bardblx−x∗/bardbl2
∇2f(x∗).
A.2 Proof of Theorem 3
To simplify the notation, we will drop the subscript s, so that ˜x=˜xsand ˜g=˜gs. Also, let us deﬁne
/hatwideg(x) =1
m/summationtextm
i=1∇ψi(x). We use g(x) =∇f(x),gt=g(xt),Ht=∇2f(xt),H=∇2f(x∗),/hatwidegt=/hatwideg(xt), and
¯gt=/hatwidegt−/hatwideg(˜x) +˜gas shorthands. Also, we will use ∆t=xt−x∗. We start by splitting up the error bound
into two terms: the ﬁrst one is an error term that would arise if we were using the exact gradient gtinstead
of the gradient estimate ¯gt; and the second term addresses the error coming from the noise in the gradient
estimate. Initially, we use the error /bardbl∆t/bardblHto analyze the convergence rate in one step of the procedure,
where recall that /bardblv/bardblM=√
v/latticetopMv. We then convert that to get convergence in function value via Lemma
2.
We ﬁrst address the assumption that ˜His anα-approximation of Ht, as deﬁned by the condition (2). Note
that, via Lemma 2, for any xt∈Uf(/epsilon1l)we have that Ht≈/epsilon1lH, which for a suﬃciently small /epsilon1limplies that
Htis a1.1-approximation of Hin the sense of (2). This, in turn implies that ˜His a1.1α-approximation
ofH, because ˜H/precedesequal√αHt/precedesequal√
1.1αH(the other direction is analogous). For the sake of simplicity, we will
replace 1.1αwithαand say that ˜His anα-approximation of H(this can be easily accounted for by adjusting
the constants at the end).
Now, suppose that after tinner iterations, we get xt∈Uf(/epsilon1l)satisfying/bardbl∆t/bardblH≤/bardbl∆0/bardblH. Ourdecomposition
of the error into two terms proceeds as follows, where we use ˜pt=˜H−1¯gt:
/bardbl∆t+1/bardblH=/bardbl(xt−η˜pt)−x∗/bardblH
=/bardbl∆t−η˜H−1gt+η˜H−1gt−η˜H−1¯gt/bardblH
≤/bardbl∆t−η˜H−1gt/bardblH+η/bardbl˜H−1(gt−¯gt)/bardblH (6)
To bound the second term in (6), we ﬁrst observe that ˜H−1/precedesequal√αH−1, which in turn yields H1/2˜H−1H1/2/precedesequal√αI. Thus, we can write /bardblH1/2˜H−1H1/2/bardbl≤√αand we get:
/bardbl˜H−1(gt−¯gt)/bardblH=/bardblH1/2˜H−1H1/2H−1/2(gt−¯gt)/bardbl
≤/bardblH1/2˜H−1H1/2/bardbl·/bardblH−1/2(gt−¯gt)/bardbl
≤√α·/bardblgt−¯gt/bardblH−1
We now break/bardblgt−¯gt/bardblH−1down into two parts, introducing /hatwideg(x∗)and separating /hatwideg(˜x)from/hatwidegt:
/bardblgt−¯gt/bardblH−1=/bardblgt−(/hatwidegt−/hatwideg(˜x) +˜g)/bardblH−1
≤/bardblgt−(/hatwidegt−/hatwideg(x∗))/bardblH−1+/bardbl˜g−(/hatwideg(˜x)−/hatwideg(x∗))/bardblH−1.
We bound the above two terms using the following lemma, which gives a new high-probability error bound
for the variance reduced gradient estimates in the large mini-batch regime which, unlike results from prior
work that hold in expectation, crucially relies on the iterate being in the local neighborhood Uf(1)around
the optimum x∗.
16Under review as submission to TMLR
Lemma 3 There is an absolute constant C > 0such that for any x∈Uf(1), letting H=∇2f(x∗), the
gradient estimate /hatwideg(x) =1
m/summationtextm
i=1∇ψi(x)usingm≥κlog(1/δ)samples, with probability 1−δsatisﬁes:
/vextenddouble/vextenddouble/hatwideg(x)−/hatwideg(x∗)−∇f(x)/vextenddouble/vextenddouble2
H−1≤Clog(1/δ)κ
m/bardblx−x∗/bardbl2
H.
ProofWe will apply Bernstein’s concentration inequality for random vectors (Lemma 1) to vi=∇ψi(x)−
∇ψi(x∗)−∇f(x). First, observe that E∇ψi(x) =∇f(x)andE∇ψi(x∗) =∇f(x∗) =0, so in particular,
E[vi] =0.
In the next step, we will use the fact that for any λ-smooth function g, we have/bardbl∇g(x)/bardbl2≤2λ·(g(x)−
minx/primeg(x/prime)), which follows because:
min
x/primeg(x/prime)≤g/parenleftbig
x−1
λ∇g(x)/parenrightbig
≤g(x)−1
λ/bardbl∇g(x)/bardbl2+λ
21
λ2/bardbl∇g(x)/bardbl2
=g(x)−1
2λ/bardbl∇g(x)/bardbl2.
We will use this fact once on f, and also a second time, on the function g(x) =ψi(x)−ψi(x∗)−(x−
x∗)/latticetop∇ψ(x∗), which isλ-smooth because ψiisλ-smooth, observing that ∇g(x) =∇ψi(x)−∇ψi(x∗)and
that minx/primeg(x/prime) =g(x∗) = 0. Thus, we have
/bardblvi/bardbl2≤2/bardbl∇ψi(x)−∇ψi(x∗)/bardbl2+ 2/bardbl∇f(x)/bardbl2
≤4λ·/parenleftbig
ψi(x)−ψi(x∗)−(x−x∗)/latticetop∇ψ(x∗)/parenrightbig
+ 4λ·/parenleftbig
f(x)−f(x∗)/parenrightbig
≤2λ2/bardblx−x∗/bardbl2+ 2λ2/bardblx−x∗/bardbl2= 4λ2/bardblx−x∗/bardbl2,
where in the last step we used again that ψiandfareλ-smooth. To bound the expectation E/bardblvi/bardbl2, we use
the intermediate inequality from the above derivation, obtaining:
E/bardblvi/bardbl2=E/bracketleftbig
/bardbl∇ψi(x)−∇ψi(x∗)/bardbl2/bracketrightbig
−2E/bracketleftbig
∇ψi(x)−∇ψi(x∗)/bracketrightbig/latticetop∇f(x) +/bardbl∇f(x)/bardbl2
=E/bracketleftbig
/bardbl∇ψi(x)−∇ψi(x∗)/bardbl2/bracketrightbig
−/bardbl∇f(x)/bardbl2
≤E/bracketleftbig
/bardbl∇ψi(x)−∇ψi(x∗)/bardbl2/bracketrightbig
≤E/bracketleftBig
2λ·/parenleftbig
ψi(x)−ψi(x∗)−(x−x∗)/latticetop∇ψi(x∗)/parenrightbig/bracketrightBig
= 2λ·/parenleftbig
f(x)−f(x∗)−(x−x∗)∇f(x∗)/parenrightbig
= 2λ·/parenleftbig
f(x)−f(x∗)/parenrightbig
.
We now use the assumption that x∈Uf(1), which implies via Lemma 2 that f(x)−f(x∗)≤2·1
2/bardblx−x∗/bardbl2
H=
/bardblx−x∗/bardbl2
H. Thus, we can use Lemma 1 with R= 2λ/bardblx−x∗/bardblandσ2= 2mλ/bardblx−x∗/bardbl2
H, as well as µ-strong
convexity of f, obtaining that, for some absolute constant C, with probability 1−δ, we have:
/bardbl/hatwideg(x)−/hatwideg(x∗)−∇f(x)/bardbl2
H−1≤1
µ/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1vi/vextenddouble/vextenddouble/vextenddouble2
≤C
µ/parenleftBigσ2log(1/δ)
m2+R2log2(1/δ)
m2/parenrightBig
≤C
µ/parenleftbigg2λ/bardblx−x∗/bardbl2
Hlog(1/δ)
m+4λ2/bardblx−x∗/bardbl2log2(1/δ)
m2/parenrightbigg
≤4C/parenleftbiggκlog(1/δ)
m+κ2log2(1/δ)
m2/parenrightbigg
·/bardblx−x∗/bardbl2
H
≤8Clog(1/δ)·κ
m/bardblx−x∗/bardbl2
H,
17Under review as submission to TMLR
where in the last step we used that m≥κlog(1/δ).
Letting/epsilon1g=/radicalbig
2Clog(t/δ)κ/m, Lemma 3 implies that with probability 1−δ/t2,
/bardblgt−(/hatwidegt−/hatwideg(˜x) +˜g)/bardblH−1≤/epsilon1g/parenleftbig
/bardbl∆t/bardblH+/bardbl∆0/bardblH/parenrightbig
≤2/epsilon1g/bardbl∆0/bardblH.
Finally, we return to the ﬁrst term in (6), i.e., /bardbl∆t−η˜H−1gt/bardblH. To control this term we introduce the
following lemma which is potentially of independent interest to the local convergence analysis of Newton-type
methods.
Lemma 4 Suppose that fsatisﬁes Assumption 1 and either one of the Assumptions 2 or 3, and take any
x∈Uf(/epsilon1l)(see Deﬁnition 1) for /epsilon1l≤1/cαfor a suﬃciently large absolute constant c>0. Let H=∇2f(x∗)
and consider a pd matrix ˜Hthat satisﬁes1√αH/precedesequal˜H/precedesequal√αH. Then, for η:= min{/radicalbig
2/α,1}, we have:
/bardblx−η˜H−1∇f(x)−x∗/bardblH≤/parenleftBig
1−1
1.9α/parenrightBig
/bardblx−x∗/bardblH.
ProofLet∆0:=x−x∗and∆1:=x−η˜H−1∇f(x)−x∗. Using that∇f(x∗) =0, we have:
∆1= ∆ 0−η˜H−1∇f(x)
= ∆ 0−η˜H−1(∇f(x)−∇f(x∗))
= ∆ 0−η˜H−1/integraldisplay1
0∇2f(x∗+θ∆0)∆0dθ
= (I−η˜H−1¯H)∆0,
where we deﬁned ¯H:=/integraltext1
0∇2f(x∗+θ∆0)dθ. It follows that we can bound the norm of ∆1using a norm
deﬁned by the matrix ¯H:
/bardbl∆1/bardbl¯H=/bardbl¯H1/2(I−η˜H−1¯H)∆0/bardbl
=/bardbl(I−ηH1/2˜H−1¯H1/2)¯H1/2∆0/bardbl
≤/bardblI−η¯H1/2˜H−1¯H1/2/bardbl·/bardbl∆0/bardbl¯H.
Observe that for any θ∈[0,1], the vector x∗+θ∆0belongs toUf(/epsilon1l), which via Lemma 2 implies that
∇2f(x∗+θ∆0)≈/epsilon1lH∀θ∈[0,1].
where H=∇2f(x∗). In particular, this means that ¯H≈/epsilon1lH, which, combined with the α-approximation
property of ˜H, allows us to write the following:
˜H−1/precedesequal√αH−1/precedesequal√α(1 +/epsilon1l)¯H−1and ˜H−1/followsequal1√αH−1/followsequal1−/epsilon1l√α¯H−1.
Putting these inequalities together, we obtain that:
η1−/epsilon1l√αI/precedesequalη¯H1/2˜H−1¯H1/2/precedesequalη√α(1 +/epsilon1l)I.
Now, using the fact that η= min{/radicalbig
2/α,1}, we conclude that:
/bardblI−η¯H1/2˜H−1¯H1/2/bardbl≤max/braceleftBig
η√α(1 +/epsilon1l)−1,1−η1−/epsilon1l√α/bracerightBig
≤max/braceleftBig√
2(1 +/epsilon1l)−1,1−√
2(1−/epsilon1l)
α,1−1−/epsilon1l√
2/bracerightBig
≤max/braceleftBig
1−1
1.8,1−1
α/bracerightBig
≤1−1
1.8α,
18Under review as submission to TMLR
where we used that /epsilon1l≤1/cfor a suﬃciently large constant c>0such that max{√
2(1 +/epsilon1l)−1,1−1−/epsilon1l√
2}≤
1−1
1.8. Now, we analyze convergence in the norm induced by H, instead of ¯H, by relying again on the fact
that ¯H≈/epsilon1lH, obtaining:
/bardbl∆1/bardblH≤1√1−/epsilon1l/bardbl∆1/bardbl¯H≤1√1−/epsilon1l/parenleftBig
1−1
1.8α/parenrightBig
/bardbl∆0/bardbl¯H
≤/radicalbigg
1 +/epsilon1l
1−/epsilon1l/parenleftBig
1−1
1.8α/parenrightBig
/bardbl∆0/bardblH≤/parenleftBig
1−1
1.9α/parenrightBig
/bardbl∆0/bardblH,
where the last step requires /epsilon1l= 1/cαfor suﬃciently large absolute constant c>0.
Using Lemma 4 to bound the ﬁrst term in (6), we obtain that:
/bardbl∆t−η˜H−1
tgt/bardblH≤/parenleftBig
1−1
1.9α/parenrightBig
/bardbl∆t/bardblH.
Puttingeverythingtogether, weobtainthefollowingboundfortheerroroftheupdatethatusesthestochastic
variance-reduced gradient estimate:
/bardbl∆t+1/bardblH=/bardbl∆t−η˜pt/bardblH
≤/bardbl∆t−η˜H−1gt/bardblH+η/bardbl˜H−1(gt−¯gt)/bardblH
≤/parenleftBig
1−1
1.9α/parenrightBig
/bardbl∆t/bardblH+ 2η√α/epsilon1g/bardbl∆0/bardblH
≤/parenleftBig
1−1
1.9α/parenrightBig
/bardbl∆t/bardblH+ 3/epsilon1g/bardbl∆0/bardblH.
Note that, as long as 3/epsilon1g≤1
2α(which can be ensured by our assumption on m), this implies that /bardbl∆t+1/bardblH≤
/bardbl∆0/bardblHandso xt+1∈Uf(/epsilon1l). Thus, ouranalysiscanbeappliedrecursivelyateachinneriteration. Toexpand
the error recursion, observe that if we apply a union bound over the high-probability events in Lemma 3 for
each inner iteration tusing failure probability δt=δ/t2, then they hold for all twith probability at least
1−/summationtext∞
t=1δ/t2≥1−δπ2/6. We obtain:
/bardbl∆t/bardblH≤/parenleftBig
1−1
1.9α/parenrightBig
/bardbl∆t−1/bardblH+ 3/epsilon1g/bardbl∆0/bardblH
≤/parenleftBig
1−1
1.9α/parenrightBigt
/bardbl∆0/bardblH+/parenleftbiggt−1/summationdisplay
i=0/parenleftBig
1−1
1.9α/parenrightBigi/parenrightbigg
·3/epsilon1g/bardbl∆0/bardblH
≤/parenleftbigg/parenleftBig
1−1
1.9α/parenrightBigt
+ 9α/epsilon1g/parenrightbigg
·/bardbl∆0/bardblH.
Applying Lemma 2, we convert this to convergence in function value:
f(xt)−f(x∗)≤1
1−/epsilon1l1
2/bardbl∆t/bardbl2
H
≤1
1−/epsilon1l1
2/parenleftbigg/parenleftBig
1−1
1.9α/parenrightBigt
+ 9α/epsilon1g/parenrightbigg2
/bardbl∆0/bardbl2
H
≤1 +/epsilon1l
1−/epsilon1l/parenleftbigg/parenleftBig
1−1
1.9α/parenrightBig2t
+ 92α2/epsilon12
g/parenrightbigg
·(f(x0)−f(x∗))
≤/parenleftbigg/parenleftBig
1−1
2α/parenrightBigt
+C/primeα2κlog(t/δ)
m/parenrightbigg
·(f(x0)−f(x∗)),
whereC/primeis an absolute constant, and we again used that /epsilon1l≤1/cαfor a suﬃciently large c, thus concluding
the proof.
19Under review as submission to TMLR
A.3 Proof of Lemma 2
First, we show that the Hessian at x∈Uf(/epsilon1l)is an/epsilon1l-approximation of the Hessian at the optimum x∗.
This is broken down into two cases, depending on which of the two Assumptions 2 and 3 are satisﬁed.
Case 1: Assumption 2 (Lipschitz Hessian). Using the shorthand H=∇2f(x∗)and the fact that strong
convexity (Assumption 1) implies that ∇2f(x)/followsequalµI, we have:
/bardblH−1/2(∇2f(x)−H)H−1/2/bardbl≤1
µ/bardbl∇2f(x)−H/bardbl≤L
µ/bardblx−x∗/bardbl≤L
µ3/2/bardblx−x∗/bardblH≤/epsilon1l,
which implies that ∇2f(x)≈/epsilon1lH.
Case 2: Assumption 3 (Self-concordance). The fact that ∇2f(x)≈/epsilon1lHfollows from the following property
of self-concordant functions (Boyd & Vandenberghe, 2004, Chapter 9.5), which holds when /bardblx−x∗/bardblH<1:
(1−/bardblx−x∗/bardblH)2·H/precedesequal∇2f(x)/precedesequal(1−/bardblx−x∗/bardblH)−2·H,
where we again let H=∇2f(x∗).
We next use a version of Quadratic Taylor’s Theorem, as given below. See Theorem 3 in Chapter 2.6 of
(Jerrard, 2018) and Chapter 2.7 in (Folland, 2002).
Lemma 5 Suppose that f:Rd→Rhas continuous ﬁrst and second derivatives. Then, for any aandv,
there exists θ∈(0,1)such that:
f(a+v) =f(a) +∇f(a)/latticetopv+1
2v/latticetop∇2f(a+θv)v.
Applying Talyor’s theorem with a=x∗andv=x−x∗, there is a z=x∗+θ(x−x∗)such that:
f(x)−f(x∗) =1
2/bardblx−x∗/bardbl2
∇2f(z),
where we use that ∇f(x∗) =0. Since we assumed that x∈Uf(/epsilon1l), and naturally also x∗∈Uf(/epsilon1l), this means
thatz∈Uf(/epsilon1l), given that Uf(/epsilon1l)is convex. Thus, using that ∇2f(z)≈/epsilon1lH, we have/bardblx−x∗/bardbl2
∇2f(z)≈/epsilon1l
/bardblx−x∗/bardbl2
H.
A.4 Proof of Theorem 2
In this section, we discuss how the convergence analysis of SVRN can be adapted to using leverage score
sampling when solving a least squares task (proving Theorem 2).
Consider an expected risk minimization problem f(x) =E[ψ(x)], whereψ=1
npIψIandIis an index from
{1,...,n}, sampled according to some importance sampling distribution p. More speciﬁcally, consider a least
squares task, where the components are given by ψi(x) =1
2(a/latticetop
ix−yi)2. Then, the overall minimization task
becomes:
E[ψ(x)] =E/bracketleftbig1
npIψI(x)/bracketrightbig
=1
2nn/summationdisplay
i=1(a/latticetop
ix−yi)2. (7)
Moreover, we have f(x)−f(x∗) =1
2n/bardblA(x−x∗)/bardbl2=1
2/bardblx−x∗/bardbl2
H, where H=∇2f(x) =1
nA/latticetopA. Also,
∇ψi(x) = (a/latticetop
ix−yi)ai,∇2ψi(x) =aia/latticetop
i.
Naturally, since the Hessian is the same everywhere for this task, the local convergence neighborhood Uf
is simply the entire Euclidean space Rd. Let us ﬁrst recall our deﬁnition of the condition number for this
task. Assumption 1 states that each ψiisλ-smooth, i.e.,/bardbl∇2ψi(x)/bardbl=/bardblai/bardbl2≤λandfisµ-strongly
20Under review as submission to TMLR
convex, i.e., λmin(H) =1
nσ2
min(A)≥µ, and the condition number of the problem is deﬁned as κ=λ/µ≥
maxi{n/bardblai/bardbl2}/σ2
min(A). Can we reduce the condition number of this problem by importance sampling?
Consider the following naive strategy which can be applied directly with our convergence result. Here, we let
the importance sampling probabilities be pi∝/bardblai/bardbl2, so that the smoothness of the new reweighted problem
will be ˜λ=1
n/summationtextn
i=1/bardblai/bardbl2. In other words, it will be the average smoothness of the original problem, instead
of the worst-case smoothness. Such importance sampling strategy can theoretically be applied to a general
ﬁnite-sum minimization task with some potential gain, however we may need diﬀerent sampling probabilities
at each step. For least squares, the resulting condition number is ˜κ=˜λ/µ= (/summationtext
i/bardblai/bardbl2)/σ2
min(A). This is
still worse than what we claimed for least squares, but it is still potentially much better than κ.
Next, we will show that by slightly adapting our convergence analysis, we can use leverage score sampling
to further improve the convergence of SVRN for the least squares task. Recall that the ith leverage score of
Ais deﬁned as /lscripti=/bardblai/bardbl2
(A/latticetopA)−1=1
n/bardblai/bardbl2
H−1, and the laverage scores satisfy/summationtextn
i=1/lscripti=d. This result will
requireshowingaspecializedversionofLemma3, whichboundstheerrorinthevariance-reducedsubsampled
gradient. In this case we show a bound in expectation, instead of with high probability.
Lemma 6 Suppose that fdeﬁnes a least squares task (7)and the sampling probabilities satisfy pi≥
/bardblai/bardbl2
(A/latticetopA)−1/(Cd). Then,/hatwideg(x) =1
m/summationtextm
i=11
npIi∇ψIi(x), whereI1,...,Im∼p, satisﬁes:
E/bardbl/hatwideg(x)−/hatwideg(x∗)−∇f(x)/bardbl2
H−1≤Cd
m·/bardblx−x∗/bardbl2
H.
ProofWe deﬁne vi=1
npIi(∇ψIi(x)−∇ψIi(x∗))−∇f(x). Note that E[vi] =0, so we have:
E/bardbl/hatwideg(x)−/hatwideg(x∗)−∇f(x)/bardbl2
H−1=E/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1vi/vextenddouble/vextenddouble/vextenddouble2
H−1=1
mE/bardblv1/bardbl2
H−1
≤1
mE1
n2p2
I1/bardbl∇ψI1(x)−∇ψI1(x∗)/bardbl2
H−1
=1
mE/bardblaI1/bardbl2
H−1
n2p2
I1/parenleftbig
a/latticetop
I1(x−x∗)/parenrightbig2
≤1
mCd·E(a/latticetop
I1(x−x∗))2
npI1=C·d
m/bardblx−x∗/bardbl2
H,
where we used that /bardblai/bardbl2
H−1=n/bardblai/bardbl2
(A/latticetopA)−1≤Cndpi.
Since the above bound is obtained in expectation, to insert it into our high probability analysis, we apply
Markov’s inequality. Namely, it holds with probability 1−δthat:
/bardbl/hatwideg(x)−/hatwideg(x∗)−∇f(x)/bardbl2
H−1≤Cd
δm/bardblx−x∗/bardbl2
H.
Compared to Lemma 3, the dependence on the condition number κis completely eliminated in this result.
Lettingm=n/log(n/d)and the number of local iterations of SVRN to be t=O(log(n/d)), we can apply
the union bound argument from the proof of Theorem 3 by letting δ= 1/(Ct), so that with probability
1−1/C, one stage of leverage score sampled SVRN satisﬁes:
f(˜xs+1)−f(x∗)≤ρ·/parenleftbig
f(˜xs)−f(x∗)/parenrightbig
forρ=O/parenleftBigdlog2(n/d)
n/parenrightBig
.
Alternatively, ourmainconvergenceanalysiscanbeadapted(forleastsquares)toconvergenceinexpectation,
obtaining that E[f(˜xs+1)−f(x∗)]≤˜ρ·E[f(˜xs)−f(x∗)]for˜ρ=O(dlog(n/d)/n).
The time complexity stated in Theorem 2 comes from the fact that constructing a preconditioning matrix
˜Hthat is anα-approximation of Hwithα=O(1), together with approximating the leverage scores, takes
O(ndlogn+d3logd)(Drineas et al., 2012), whereas one stage of SVRN takes O(nd+d2log(n/d)). Here, the
21Under review as submission to TMLR
preconditioning matrix can be formed by applying a k×nsketching transformation Sto the data matrix
A, and then computing the Hessian estimate1
nA/latticetopS/latticetopSA≈H. For example, if we use the Subsampled
Randomized Hadamard Transform (SRHT, Ailon & Chazelle, 2009), then it suﬃces to use k=O(dlogd).
Finally, the initial iterate ˜x0can be constructed using the same sketching transformation via the so-called
sketch-and-solve technique (Sarlos, 2006):
˜x0= argmin
x/bardblSAx−Sy/bardbl2.
Withk=O(dlogd), this initial iterate will satisfy f(˜x0)≤O(1)·f(x∗), so the number of iterations of SVRN
needed to obtain f(˜xs)≤(1 +/epsilon1)f(x∗)is onlys=O/parenleftbiglog(1//epsilon1)
log(n/d)/parenrightbig
.
We note that another way to implement SVRN with approximate leverage score sampling is to ﬁrst precon-
dition the entire least squares problem with a Randomized Hadamard Transform (i.e., SRHT without the
subsampling):
˜A=HDA and ˜y=HDy, (8)
where His a Hadamard matrix scaled by 1/√nandDis a diagonal matrix with random sign entries. This is
a popular technique in Randomized Numerical Linear Algebra (Woodruﬀ, 2014; Drineas & Mahoney, 2016;
Dereziński&Mahoney,2021). Thecostofthistransformationis O(ndlogn), thankstofastFouriertransform
techniques, and the resulting least squares task is equivalent to the original one, because /bardbl˜Ax−˜y/bardbl2=
/bardblAx−y/bardbl2for all x. Moreover, with high probability, all of the leverage scores of ˜Aare nearly uniform,
so, after this preconditioning, we can simply implement SVRN with uniform gradient subsampling and still
enjoy the condition-number-free convergence rate from Theorem 2. This strategy is as eﬀﬁcient as direct
leverage score sampling when Ais a dense matrix, but it is less eﬀective when we want to exploit data
sparsity.
B Further experimental details
In this section we provide additional details regarding our experimental setup in Section 5, as well as some
further results on logistic regression with several datasets.
As a dataset, in Section 5, we used the Extended MNIST dataset of handwritten digits (EMNIST, Cohen
et al., 2017) with n= 500k datapoints. Here, we also include results on the CIFAR-10 image dataset
withn= 50k datapoints. Both datasets are preprocessed in the same way: Each image is transformed
by a random features map that approximates a Gaussian kernel having width 0.002, and we partitioned
the classes into two labels 1 and -1. We considered two feature dimensions: d= 500andd= 1000, and
we used the regularization parameter γ= 10−8. To measure the error in the convergence plots, we use
/bardblxt−x∗/bardbl2
H//bardblx0−x∗/bardbl2
H, where H=∇2f(x∗).
We next present further results, studying the convergence properties of SVRN on synthetic datasets with
varying properties, for the logistic regression task as in (5). To construct our synthetic data matrices, we
ﬁrst generate an n×dGaussian matrix G, and let G=UDVbe the reduced SVD of that matrix (we used
n= 500k andd= 1000). Then, we replace diagonal matrix Dwith a matrix ˜Dthat has singular values
spread linearly from 1 to κA. We then let A=U˜DVbe our data matrix. To generate the vector yfor
logistic regression, we ﬁrst draw a random vector x∼N(0,1/d·Id), and then we let y= sign( Ax).
For the least squares tasks in Section 5.2, we generated the same synthetic matrices A, but with the vector
ygenerated as follows: y=Ax+ξwhere ξis the Gaussian noise ξ∼N (0,0.1·In). Here, we observed
little diﬀerence in convergence behavior when varying κA(we show the results for κA= 103).
Logistic regression with varying condition number. To supplement the EMNIST logistic regression
experiments in Section 5, we present convergence of SVRN-HA on the CIFAR-10 dataset, as well as for
the synthetic logistic regression task while varying the squared condition number κ2
Aof the data matrix.
Note that, while κ2
Ais not the same as the condition number of the ﬁnite-sum minimization problem, it
is correlated with it, by aﬀecting the convexity and smoothness of f. From Figure 5, we observe that
22Under review as submission to TMLR
(a) Synthetic LR, κ2
A= 1
 (b) Synthetic LR, κ2
A= 10
(c) CIFAR-10, d= 500
 (d) CIFAR-10, d= 1000
Figure5: ConvergencecomparisonofSVRN-HAagainstSN-HAandNewtonforasyntheticlogisticregression
task as we vary the condition number of the data matrix, and for the CIFAR-10 dataset.
SVRN-HA outperforms SN-HA for both values of the data condition number. However, the convergence of
both algorithms gets noticeably slower after increasing κ2
A, while it does not have as much of an eﬀect on
the Newton’s method. Given that the increased condition number aﬀects both methods similarly, we expect
that the degradation in performance is primarily due to worse Hessian approximations, rather than increased
variance in the gradient estimates. This may be because we are primarily aﬀecting the global convexity of f,
as opposed to the smoothness of individual components ψi. See our high-coherence least squares experiments
for a discussion of how the smoothness of component functions aﬀects the performances of SVRN and SN
very diﬀerently.
C Related work on Subsampled Newton
In this section, we discuss several important prior works on Subsampled Newton methods to put our results
in context. Speciﬁcally, we aim to illustrate how the Hessian approximation condition used in Theorems 1
and 3, i.e.,1√α∇2f(x)/precedesequal˜H/precedesequal√α∇2f(x), relates to the Hessian estimates used in this line of works when
showing fast local convergence rates. Also, in Appendix D.2, we show that to recover our condition with
α≤2via uniform Hessian subsampling, one needs O(κlog(d))samples. Throughout this section, we use
notation from the respective references.
First, we consider the Hessian averaging method studied by (Na et al., 2022). It is important to distinguish
between the condition they impose on the stochastic Hessian oracle /hatwideH, and the Hessian approximation
guarantee that they obtain for the actual estimate ˜Htthat they use (and that we use in SVRN-HA).
23Under review as submission to TMLR
The stochastic oracle is only required to have a sub-exponential tail (see their Assumption 2.1, and also
Example 2.3 illustrating this for Subsampled Newton). However, the actual estimate ˜Htis a result of
averaging many samples from that oracle. Their local convergence analysis is only deployed once enough
oracle samples are averaged so that ˜Htachieves the approximation guarantee given in their Lemma 3.5, i.e.,
(1−ψ)Ht/precedesequal˜Ht/precedesequal(1 +ψ)Ht. This approximation guarantee is strictly stronger than ours, but it becomes
equivalent once α≤2.
Next, we consider (Roosta-Khorasani & Mahoney, 2019) which analyzes a broad class of Subsampled Newton
methods. In this paper, the most relevant results are Lemma 2 (Hessian approximation guarantee) and
Theorem 5 (local convergence result). The guarantee reduces to our condition with α≤2, with the only
diﬀerence being that their Hessian approximation is restricted to the “cone of feasible directions”, deﬁned
in (3). This restriction is only present in a constrained optimization setting (we focus on unconstrained
optimization). The lemma also shows that the required Hessian sample size is again larger than the condition
number of the problem (their condition number κ1matches our κfor unconstrained optimization).
Next, we look at (Bollapragada et al., 2018) which presents a convergence analysis of Subsampled Newton
under slightly diﬀerent assumptions. Here, the key statements for local convergence are Lemma 2.4 in the
journal version (Lemma 2.3 in arxiv), and equation (2.17). The lemma gives an approximation guarantee for
the subsampled Hessian. This guarantee is in some sense weaker than our condition, because it only requires
the Hessian approximation to be good in one direction, i.e., wk−w∗in their notation. However, examining
the convergence bound in (2.17), for the local convergence analysis to hold, the Hessian sample size must
still satisfy|Sk|≥σ2/¯µ2whereσ2is eﬀectively the upper bound on the component Hessians (potentially as
large as our λ-smoothness) and ¯µis the lower bound on the component Hessians. The latter is eﬀectively a
component-wise strong convexity constant, which can be much smaller (i.e., worse) than our global strong
convexityµ. In summary, their Hessian approximation condition for local convergence analysis, while slightly
diﬀerent, also requires the Hessian sample size to be larger than a condition number of the problem. Their
condition number can be much larger than our condition number κ, or even inﬁnite (for problems as simple
as least squares), and is less standard in the literature.
Finally, we examine (Erdogdu & Montanari, 2015), where the authors consider Subsampled Newton with a
possibly low-rank approximation of the Hessian. The most relevant result in that work is Lemma 3.1. Here,
the standard version of Subsampled Newton is recovered when we let Qt=H−1
St. Then, the standard Hessian
approximation condition appears implicitly through the fact that ξt
1has to be less than 1 for the bound to
be non-vacuous. To see the condition more clearly, we point to Equation (B.1) in the appendix (Equation
A.1 in the arxiv version), which requires that /bardblQt/bardbl·/bardblHSt−H/bardbl<1. ForQt=H−1
St, this is essentially
equivalent to our condition with α≤2. Also, from the bound in Lemma 3.1, we see that once again the
condition requires Hessian sample size to be larger than a condition number of the problem (which is for
themK/bardblQt/bardbl, and after some eﬀort, this can be seen as comparable to our condition number). In the main
algorithm of the paper, NewSamp, the authors aim to reduce the required sample size by using a diﬀerent
Qtcomputed from a low-rank approximation of HSt. This roughly corresponds to constructing a Hessian
α-approximation with 1/lessmuchα/lessmuchκ.
D Omitted proofs
Here, we include the proofs of the auxiliary results stated in the paper. First, we discuss in detail the global
convergence analysis of SVRN-HA (Theorem 4). Then, we illustrate how the Hessian approximation required
in Theorem 3 can be obtained via subsampling.
D.1 Global convergence of SVRN-HA
Here, we show how the proof of Theorem 4, i.e., global convergence of SVRN-HA (Algorithm 1), follows
from global convergence analysis of Hessian averaging (Na et al., 2022). They show in Lemma 3.5 that if we
were to run the global phase of SVRN-HA exclusively, then for any /epsilon1,δ∈(0,1)there isT:=T(/epsilon1,δ)such
that with probability 1−δfor alls≥Twe have ˜xs∈Uf(/epsilon1),˜Hs≈/epsilon1∇2f(˜xs), andηs= 1. This means that,
for any/epsilon1, the probability that the above event does not happen with any T <∞is less than any δ>0, so it
24Under review as submission to TMLR
must be 0. This implies that SVRN-HA will eventually switch to the local phase (i.e., to SVRN). Note that
it is possible that the switch will occur before the local neighborhood and Hessian approximation conditions
are met. But if this causes SVRN to produce a poor descent direction, it will be caught by the line search
(resulting in ηs<1) and the method will simply revert back to the global phase. Eventually, the global
phase will ensure that both conditions are met, and we can rely on Theorem 3 for the local convergence
analysis.
D.2 Hessian approximation via subsampling
Here, we illustrate how the Hessian α-approximation condition (2), used in Theorems 1 and 3, can be
obtained via uniformly subsampling O(κlogd)component Hessians. This result follows from Bernstein’s
concentration inequality for random matrices, given below (Tropp, 2012).
Lemma 7 (Matrix Bernstein’s inequality) LetZ1,...,Zkbe independent random symmetric d×dma-
trices such that1
k/summationtext
iE[Zi] =¯Z. Suppose that:
/vextenddouble/vextenddouble1
k/summationdisplay
iE[(Zi−E[Zi])2]/vextenddouble/vextenddouble≤¯σ2and/bardblZi−E[Zi]/bardbl≤R.
Then, for any /epsilon1≥0
Pr/parenleftbigg/vextenddouble/vextenddouble/vextenddouble1
kk/summationdisplay
i=1Zi−¯Z/vextenddouble/vextenddouble/vextenddouble≥/epsilon1/parenrightbigg
≤2d·exp/parenleftBig
−/epsilon12k/2
¯σ2+/epsilon1R/3/parenrightBig
.
We are now ready to show the approximation guarantee for a subsampled Hessian estimate.
Lemma 8 Suppose Assumption 1, and let Dbe the sampling distribution for component functions ψ, as in
Theorem 3. Let ψ1,...,ψk∼Dbe i.i.d. samples from this distribution. There is an absolute constant csuch
that ifk≥cκlog(d/δ), then for any x∈Rd, with probability 1−δ, the matrix ˜H=1
k/summationtextk
i=1∇2ψi(x)is an
α-approximation of ∇2f(x)as in(2)withα≤2.
Proof LetH=∇2f(x). We will use Lemma 7 with Zi=H−1/2∇2ψi(x)H−1/2. First, note that
E[∇2ψi(x)] =∇2f(x)so that ¯Z=E[Zi] =I. Next, we compute the boundedness parameter R:
/bardblZi−E[Zi]/bardbl≤/bardblH−1/2∇2ψi(x)H−1/2/bardbl+ 1≤2κ=:R.
Now, we similarly bound the variance parameter ¯σ2:
/vextenddouble/vextenddoubleE[(Zi−E[Zi])2]/vextenddouble/vextenddouble≤/vextenddouble/vextenddoubleE[Z2
i]/vextenddouble/vextenddouble≤/vextenddouble/vextenddoubleE/bracketleftbig
/bardblZi/bardblZi/bracketrightbig/vextenddouble/vextenddouble≤κ=: ¯σ2.
Thus, Lemma 7 implies that if k≥3κlog(2d/δ)//epsilon12then with probability 1−δwe have:
/bardblH−1/2˜HH−1/2−I/bardbl≤/epsilon1.
We can rewrite this as:
(1−/epsilon1)I/precedesequalH−1/2˜HH−1/2/precedesequal(1 +/epsilon1)I,
which is equivalent to:
(1−/epsilon1)H/precedesequal˜H/precedesequal(1 +/epsilon1)H.
Setting/epsilon1= 1/4and adjusting the constants concludes the proof, since 1−/epsilon1≥1√
2and1 +/epsilon1≤√
2.
25