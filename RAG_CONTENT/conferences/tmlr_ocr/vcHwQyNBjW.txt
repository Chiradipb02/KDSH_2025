Published in Transactions on Machine Learning Research (09/2023)
Stochastic Batch Acquisition:
A Simple Baseline for Deep Active Learning
Andreas Kirsch∗andreas.kirsch@cs.ox.ac.uk
OATML, Department of Computer Science, University of Oxford†
Sebastian Farquhar∗sebastian.farquhar@cs.ox.ac.uk
OATML, Department of Computer Science, University of Oxford†
Parmida Atighehchian parmi.atg@gmail.com
ServiceNow†
Andrew Jesson andrew.jesson@cs.ox.ac.uk
OATML, Department of Computer Science, University of Oxford†
Frédéric Branchaud-Charron frederic.branchaud.charron@gmail.com
ServiceNow†
Yarin Gal yarin.gal@cs.ox.ac.uk
OATML, Department of Computer Science, University of Oxford
Reviewed on OpenReview: https: // openreview. net/ forum? id= vcHwQyNBjW
Abstract
We examine a simple stochastic strategy for adapting well-known single-point acquisition
functions to allow batch active learning. Unlike acquiring the top- Kpoints from the pool set,
score- or rank-based sampling takes into account that acquisition scores change as new data
are acquired. This simple strategy for adapting standard single-sample acquisition strategies
can even perform just as well as compute-intensive state-of-the-art batch acquisition functions,
like BatchBALD or BADGE while using orders of magnitude less compute. In addition to
providing a practical option for machine learning practitioners, the surprising success of the
proposed method in a wide range of experimental settings raises a difficult question for the
field: when are these expensive batch acquisition methods pulling their weight?
1 Introduction
Active learning is a widely used strategy for efficient learning in settings where unlabelled data are plentiful,
but labels are expensive (Atlas et al., 1989; Settles, 2010). For example, labels for medical image data may
require highly trained annotators, and when labels are the results of scientific experiments, each one can
require months of work. Active learning uses information about unlabelled data and the current state of the
model to acquire labels for those samples that are most likely to be informative.
While many acquisition schemes are designed to acquire labels one at a time (Houlsby et al., 2011; Gal et al.,
2017), recent work has highlighted the importance of batch acquisition (Kirsch et al., 2019; Ash et al., 2020).
Acquiring in a batch lets us parallelise labelling. For example, we could hire hundreds of annotators to work
in parallel or run more than one experiment at once. Batch acquisition also saves compute as single-point
selection also incurs the cost of retraining the model for every new data point.
∗Joint first authors
†Work done while there.
1Published in Transactions on Machine Learning Research (09/2023)
Table 1: Acquisition runtime (in seconds, 5 trials, ±s.d.). The stochastic acquisition methods are as fast as
top-K, here with BALD scores, and orders of magnitude faster than BADGE or BatchBALD. Synthetic
pool set with M= 10,000pool points with 10 classes. BatchBALD & BALD: 20 parameter samples. In
superscript, mean accuracy over acquisition steps from Figure 2 on Repeated-MNIST with 4 repetitions (5
trials).
KTop-K(BALD)80%Stochastic (PowerBALD)90%BatchBALD90%BADGE86%
10 0.2±0.0 0 .2±0.0 566 .0±17.4 9.2±0.3
100 0.2±0.0 0 .2±0.0 5 ,363.6±95.4 82.1±2.5
500 0.2±0.0 0 .2±0.0 29 ,984.1±598.7 409.3±3.7
Unfortunately, existing performant batch acquisition schemes are computationally expensive (Table 1).
Intuitively, this is because batch acquisition schemes face combinatorial complexity when accounting for the
interactions between possible acquisition points. Recent works (Pinsler et al., 2019; Ash et al., 2020; 2021)
trade off a principled motivation with various approximations to remain tractable. A commonly used, though
extreme, heuristic is to take the top- Khighest scoring points from an acquisition scheme designed to select a
single point (“ top-Kacquisition ”).
This paper examines a simple baseline for batch active learning that is competitive with methods that cost
orders of magnitude more across a wide range of experimental contexts. We notice that single-acquisition
score methods such as BALD (Houlsby et al., 2011) do not take correlations between the samples into account
and thus only act as a noisy proxy for future acquisition scores as we motivate in Figure 1. This leads us to
stochastically acquire points following a distribution determined by the single-acquisition scores instead of
acquiring the top- Ksamples. In sequential decision making, stochastic multi-armed bandits and reinforcement
learning, variants of this are also known a Boltzmann exploration (Cesa-Bianchi et al., 2017). We examine
other related methods in §4. In deep active learning this simple approach remains under-explored, yet it can
match the performance of earlier more complex methods for batch acquisition (e.g., BatchBALD, Kirsch
et al. (2019); see in §5) despite being very simple. Indeed, this acquisition scheme has a time complexity of
onlyO(MlogK)in the pool size Mand acquisition size K, just like top- Kacquisition.
We show empirically that the presented stochastic strategy performs as well as or better than top- Kacquisition
with almost identical computational cost on several commonly used acquisition scores, empirically making it
a strictly-better batch strategy. Strikingly, the empirical comparisons between this stochastic strategy and
the evaluated more complex methods cast doubt on whether they function as well as claimed. Concretely, in
this paper we:
•examine a family of three computationally cheap stochastic batch acquisition strategies (softmax,
power and soft-rank)—the latter two which have not been explored and compared to in detail before;
•demonstrate that these strategies are preferable to the commonly used top- Kacquisition heuristic;
and
•identify the failure of two existing batch acquisition strategies to outperform this vastly cheaper and
more heuristic strategy.
Outline. In §2, we present active learning notation and commonly used acquisition functions. We propose
stochastic extensions in §3, relate them to previous work in §4, and validate them empirically in §5 on
various datasets, showing that these extensions are competitive with some much more complex active learning
approaches despite being orders of magnitude computationally cheaper. Finally, we further validate some of
the underlying theoretical motivation in §6 and discuss limitations in §7.
2Published in Transactions on Machine Learning Research (09/2023)
2 Background & Problem Setting
The stochastic approach we examine applies to batch acquisition for active learning in a pool-based setting
(Settles, 2010) where we have access to a large unlabelled poolset, but we can only label a small subset of
the points. The challenge of active learning is to use what we already know to pick which points to label in
the most efficient way. Generally, we want to avoid labelling points similar to those already labelled. In the
pool-based active learning setting, we are given a large pool of unlabeled data points and can request labels
for only a small subset, the acquisition batch. To formalize this, we first introduce some notation.
Notation. Following Farquhar et al. (2021), we formulate active learning over indicesinstead over data
points. This simplifies the notation. The large, initially fully unlabelled, pool set containing Minput points is
Dpool={xi}i∈Ipool, (1)
whereIpool={1,...,M}is the initial full index set. We initialise a training dataset with N0randomly
selected points from Dpoolby acquiring their labels, yi,
Dtrain={(xi,yi)}i∈Itrain, (2)
whereItrainis the index set of Dtrain,initially containing N0indices between 1andM. A model of the
predictive distribution, p(y|x), can then be trained on Dtrain.
2.1 Active Learning
At each acquisition step, we select additional points for which to acquire labels. Although many methods
acquire one point at a time (Houlsby et al., 2011; Gal et al., 2017), one can alternatively acquire a whole
batch ofKexamples. An acquisition function atakesItrainandIpooland returns Kindices fromIpoolto
be added toItrain. We then label those Kdatapoints and add them to Itrainwhile making them unavailable
from the pool set. That is,
Itrain←Itrain∪a(Itrain,Ipool), (3)
Ipool←Ipool\Itrain. (4)
A common way to construct the acquisition function is to define some scoring function, s, and then select the
point(s) that score the highest.
Probabilistic Model. We assume classification with inputs X, labelsY, and a discriminative classifier
p(y|x). In the case of Bayesian models, we further assume a subjective probability distribution over the
parameters, p(ω), and we have p(y|x) =Ep(ω)[p(y|x,ω)].
Two commonly used acquisition scoring functions are Bayesian Active Learning by Disagreement (BALD)
(Houlsby et al., 2011) and predictive entropy (Gal et al., 2017):
BALD. Bayesian Active Learning by Disagreement (Houlsby et al., 2011) computes the expected information
gain between the predictive distribution and the parameter distribution p(ω|Dtrain)for a Bayesian model.
For each candidate pool index, i, with mutual information, I, and entropy, H, the score is
sBALD (i;Itrain):= I[Y; Ω|X=xi,Dtrain]
= H[Y|X=xi,Dtrain]−Ep(ω|Dtrain)[H[Y|X=xi,ω]]. (5)
Entropy. The(predictive) entropy (Gal et al., 2017) does not require Bayesian models, unlike BALD, and
performs worse for data with high observation noise Mukhoti et al. (2021). It is identical to the first term of
the BALD score
sentropy (i;Itrain):= H[Y|X=xi,Dtrain]. (6)
3Published in Transactions on Machine Learning Research (09/2023)
2.2 Acquisition Functions
These scoring functions were introduced for single-point acquisition:
as(Itrain):= arg max
i∈Ipools(i;Itrain). (7)
For deep learning in particular, single-point acquisition is computationally expensive due to retraining the
model for every acquired sample. Moreover, it also means that labelling can only happen sequentially instead
of in bulk. Thus, single-point acquisition functions were expanded to multi-point acquisition via acquisition
batches in batch active learning. The most naive batch acquisition function selects the highest Kscoring
points
abatch
s(Itrain;K):= arg max
I⊆Ipool,|I|=K/summationdisplay
i∈Is(i;Itrain). (8)
Maximizing this sum is equivalent to taking the top-k scoring points, which cannot account for the interactions
between points in an acquisition batch because individual points are scored independently. For example, if
the most informative point is duplicated in the pool set, all instances will be acquired, which is likely wasteful
when we assume no label noise (see also Figure 1 in Kirsch et al. (2019)).
2.3 Batch Acquisition Functions
Some acquisition functions are explicitly designed for batch acquisition (Kirsch et al., 2019; Pinsler et al., 2019;
Ash et al., 2020). They try to account for the interaction between points, which can improve performance
relative to simply selecting the top- Kscoring points. However, existing methods can be computationally
expensive. For example, BatchBALD rarely scales to acquisition sizes of more than 5–10 points due to its
long runtime (Kirsch et al., 2019), as we evidence in Table 1.
ACS-FW. Pinsler et al. (2019) propose “Active Bayesian CoreSets with Frank-Wolfe optimization” which
successively minimizes the distance between the (expected) loss on the pool set and the loss on a potential
training set using a greedy Frank-Wolfe optimization in a Hilbert space. Either a posterior-weighted Fisher
inner-product or a posterior-weighted loss inner-product between two samples are used. For non-linear models,
random projections of the gradients speed up the Fisher inner-product.
BADGE. Ash et al. (2020) propose “Batch Active learning by Diverse Gradient Embeddings” : it motivates
its batch selection approach using a k-Determinantal Point Process (Kulesza & Taskar, 2011) based on the
(inner product) similarity matrix of the scores (gradients of the log loss) using hard pseudo-labels (the highest
probability class according to the model’s prediction) for each pool sample. See also Kirsch & Gal (2022) for
a more detailed analysis. In practice, they use the intialization step of k-MEANS++ with Euclidian distances
between the scores to select an acquisition batch. BADGE is also computationally expensive as we elaborate
in Section 4.
BatchBALD. Kirsch et al. (2019) extend BALD to batch acquisition using the mutual information between
the parameter distribution and the jointdistribution of the predictions of multiple point in an acquistion
batch: this mutual information is the expected information gain for a full acquistion batch:
sBatchBALD (i1,...,iK;Itrain):= I[Y1,...,YK; Ω|X1=xi1,...,XK=xiK,Dtrain]. (9)
Kirsch et al. (2019) greedily construct an acquisition batch by iteratively selecting the next unlabelled pool
point that maximizes the joint score with the already selected points. This is 1−1/e-optimal as the expected
information gain is submodular (Krause & Golovin, 2014). They note that their approach is computationally
expensive, and they only consider acquisition batches of up to size 10.
3 Method
Selecting the top- Kpoints at acquisition step tamounts to the assumption that the informativeness of these
points is independent of each other. This leads to the pathology that if the most informative pool point is
duplicated in the pool set, each instance would be selected (up to the acquisition size). This is clearly wrong.
4Published in Transactions on Machine Learning Research (09/2023)
Table 2: Summary of stochastic acquisition variants. Perturbing the scores sithemselves with ϵi∼
Gumbel (0;β−1)i.i.d. yields a softmax distribution. Log-scores result in a power distribution, with as-
sumptions that are reasonable for active learning. Using the score-ranking, rifinally is a robustifying
assumption. βis included for completeness; we use β:= 1in our experiments—except for the ablation in §6.1.
Perturbation Distribution Probability mass
si+ϵi Softmax∝expβsi
logsi+ϵiPower∝sβ
i
−logri+ϵiSoft-rank∝r−β
i
0 50 100 150 200
Acquisition Step n0.51.0Rank Correlation s0:sn
Figure 1: Acquisition scores at individual acquisition
steptare only a loose proxy for later scores at t+
n(here:t= 0).Specifically, the Spearman rank-
correlation between acquisition scores on the zerot-
th andn’th time-step falls with n. While top- K
acquisition incorrectly implicitly assumes the rank-
correlation remains 1, stochastic acquisitions do not.
Using Monte-Carlo Dropout BNN trained on MNIST
at initial 20 points and 73% initial accuracy; score
ranks computed over test set.
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALD
BatchBALD 5BADGE
Uniform
BALDFigure 2: Performance on Repeated-MNIST with 4
repetitions (5 trials). Up and to the right is better
(↗).PowerBALD outperforms (top- K) BALD and
BADGE and is on par with BatchBALD. This is
despite being orders of magnitude faster. Acquisition
size: 10, except for BatchBALD (5). See Figure 10
in the appendix for an ablation study of BADGE’s
acquisition size, and Figure 11 for a comparison of all
BALD variants at acquisition size 5.
Top-KAcquisition Pathologies. Another way to see that this is wrong is to think step by step, that is to
split the batch acquisition into multiple steps of size 1: We select the top pool sample by acquisition score and
retrain the model once for each possible class label for this point. We then compute the averaged acquisition
scores on the pool set given each of these models weighted by the original model’s probability of each class
label. We select the top pool sample using this new (averaged) score, and repeat the process, exponentially
branching out as necessary. This is equivalent to the joint acquisition batch selection in BatchBALD (Kirsch
et al., 2019):
I[Y1,...,YK; Ω|X1=xi1,...,XK=xiK,Dtrain] = (10)
=K/summationdisplay
j=1Ep(yi1,...,yij−1|xi1,...,x ij−1,Dtrain)I[Yj; Ω|Xj=xij,X1=xi1,Y1=yi1,...,X 1=xij−1,Y1=yij−1,Dtrain]
̸=K/summationdisplay
j=1I[Yj; Ω|Xj=xij,Dtrain] =K/summationdisplay
j=1sBALD (ij;Itrain) (11)
using the chain rule of the mutual information. We see that the informativeness of the samples will usually
not be independent of each other. Of course, the acquisition scores for models trained with these additional
points will be quite different from the first set of scores. After all, the purpose of active learning is to add the
most informative points—those that will update the model the most. In contrast, selecting a top- Kbatch
using the same scores implicitly assumes that the score ranking will not change due to other points.
5Published in Transactions on Machine Learning Research (09/2023)
Figure 3: BALD scores for 1000 randomly-chosen points from the MNIST dataset (handwritten digits). The
points are colour-coded by digit label and sorted by score. The model used for scoring has been trained to
90% accuracy first. If we were to pick the top scoring points (e.g. scores above 0.6), most of them would be
8s, even though we can assume that after acquiring the first couple of them, the model would consider them
less informative than other available data. Points are slightly jittered on the x-axis by digit label to avoid
overlaps.
We provide empirical confirmation in Figure 1 that, in fact, the ranking of acquisition scores at step tand
t+Kis decreasingly correlated as Kgrows when we retrain the model for each acquired point. Figure 3 also
illustrates this on MNIST. Moreover, as we will see in §6, this effect is the strongest for the most informative
points.
Instead, we investigate the use of stochastic sampling as an alternative to top- Kacquisition, which implicitly
acknowledges the uncertainty within the batch acquisition step using a simple noise process model governing
how scores change. We motivate and investigate the theory behind this in §6, but given how simple the
examined methods are, this theory only obstructs their simplicity. Specifically, we examine three simple
stochastic extensions of single-sample scoring functions s(i;Itrain)that make slightly different assumptions.
These methods are compatible with conventional active learning frameworks that typically take the top- K
highest scoring samples. For example, it is straightforward to adapt entropy, BALD, and other scoring
functions for use with these extensions.
Gumbel Noise. These stochastic acquisition extensions assume that future scores differ from the current
score by a perturbation. We model the noise distribution of this perturbation as the addition of Gumbel-
distributed noise ϵi∼Gumbel (0; 1), which is frequently used to model the distribution of extrema. At the
same time, the choice of a Gumbel distribution for the noise is also one of mathematical convenience, in the
spirit of a straightforward baseline: the maximum of sets of many other standard distributions, such as the
Gaussian distribution, is not analytically tractable. On the other hand, taking the highest-scoring points
from a distribution perturbed with Gumbel noise is equivalent to sampling from a softmax distribution1
without replacement. We investigate the choice of Gumbel noise further in §6.
This follows from the Gumbel-Max trick (Gumbel, 1954; Maddison et al., 2014) and, more specifically, the
Gumbel-Top- Ktrick (Kool et al., 2019). We provide a short proof in appendix §B.2. Expanding on Maddison
et al. (2014):
Proposition 3.1. For scores si,i∈{1,...,n}, andk≤nandβ > 0, if we draw ϵi∼Gumbel (0;β−1)
independently, then arg topk{si+ϵi}iis an (ordered) sample without replacement from the categorical
distribution Categorical( exp(βsi)//summationtext
jexp(βsj),i∈{1,...,n}).
β≥0is a ‘coldness’ parameter. In the spirit of providing a simple and surprisingly effective baseline without
hyperparameters, we fix β:= 1for our main experiments. However, to understand its effect, we examine
1Also known as Boltzmann/Gibbs distribution.
6Published in Transactions on Machine Learning Research (09/2023)
ablations of βin §6.1. Overall, for β→∞, this distribution will converge towards top- Kacquisition; whereas
forβ→0, it will converge towards uniform acquisition.
We apply the perturbation to three quantities in the three sampling schemes: the scores themselves, the
log scores, and the rank of the scores. Perturbing the log scores assumes that scores are non-negative and
uninformative points should be avoided. Perturbing the ranks can be seen as a robustifying assumption that
requires the relative scores to be reliable but allows the absolute scores to be unreliable. Table 2 summarizes
the three stochastic acquisition variants with their associated sampling distributions, which make slightly
different assumptions about the acquisition scores:
Soft-Rank Acquisition. This first variant only relies on the rank order of the scores and makes no
assumptions on whether the acquisition scores are meaningful beyond that. It thus uses the leastamount of
information from the acquisition scores. It only requires the relative score order to be useful and ignores
theabsolute score values . If the absolute scores provide useful information, we would expect this method to
perform worse than the variants below, which make use of the score values. As we will see, this is indeed
sometimes the case.
Ranking the scores s(i;Itrain)with descending ranks {ri}i∈Ipoolsuch thats(ri;Itrain)≥s(rj;Itrain)for
ri≤rjand smallest rank being 1, we sample index iwith probability psoftrank (i)∝r−β
iwith coldness β.
This is invariant to the actual scores. We can draw ϵi∼Gumbel (0;β−1)and create a perturbed ‘rank’
ssoftrank(i;Itrain) :=−logri+ϵi. (12)
Following Proposition 3.1, taking the top- Kpoints from ssoftrankis equivalent to sampling without replacement
from the rank distribution psoftrank (i).
Softmax Acquisition. The next simplest variant uses the actual scores instead of the ranks. Again, it
perturbs the scores by a Gumbel-distributed random variable ϵi∼Gumbel (0;β−1)
ssoftmax(i;Itrain) :=s(i;Itrain) +ϵi. (13)
However, this makes no assumptions about the semantics of the absolute values of the scores: the softmax
function is invariant to constants shifts. Hence, the sampling distribution will only depend on the relative
scoresand not their absolute value.
Power Acquisition. For many scoring functions, the scores are non-negative, and a score close to zero
means that the sample is not informative in the sense that we do not expect it will improve the model—we
do not want to sample it. This is the case with commonly used score functions such as BALD and entropy.
BALD measures the expected information gain. When it is zero for a sample, we do not expect anything to
be gained from acquiring a label for that sample. Similarly, entropy is upper-bounding BALD, and the same
consideration applies. This assumption also holds for other scoring functions such as the standard deviation
and variation ratios; see Appendix B.1. To take this into account, the last variant models the future log
scores as perturbations of the current log score with Gumbel-distributed noise
spower(i;Itrain) := logs(i;Itrain) +ϵi. (14)
By Proposition 3.1, this is equivalent to sampling from a power distribution
ppower (i)∝/parenleftbigg1
s(i;Itrain)/parenrightbigg−β
. (15)
This may be seen by noting that exp(βlogs(i;Itrain)) =s(i;Itrain)β. Importantly, as scores →0, the
(perturbed) log scores →−∞and will have probability mass →0assigned. This variant takes the absolute
scores into account and avoids data points with score 0.
Given the above considerations, when using BALD, entropy, and other appropriate scoring functions, power
acquisition is the most sensible. Thus, we expect it to work best. Indeed, we find this to be the case in the
toy experiment on Repeated-MNIST (Kirsch et al., 2019) depicted in Figure 2. However, even soft-rank
acquisition often works well in practice compared to top- Kacquisition; see also appendix §D for a more
7Published in Transactions on Machine Learning Research (09/2023)
in-depth comparison. In the rest of the main paper, we mostly focus on power acquisition. We include results
for all methods in §C. In Appendix G, we present the simple implementation of the stochastic acquisition
variants we use in our experiments.
4 Related Work
Researchers in active learning (Atlas et al., 1989; Settles, 2010) have identified the importance of batch
acquisition as well as the failures of top- Kacquisition using straightforward extensions of single-sample
methods in a range of settings including support vector machines (Campbell et al., 2000; Schohn & Cohn,
2000; Brinker, 2003; Hoi et al., 2006; Guo & Schuurmans, 2007), GMMs (Azimi et al., 2012), and neural
networks (Sener & Savarese, 2018; Kirsch et al., 2019; Ash et al., 2020; Baykal et al., 2021).
Many of these methods aim to introduce structured diversity to batch acquisition that accounts for the
interaction of the points acquired in the learning process. In most cases, the computational complexity
scales poorly with the acquisition size ( K) or pool size ( M), for example because of the estimation of
joint mutual information (Kirsch et al., 2019); the O(KM)complexity of using a k-means++ initialisation
scheme (Ash et al., 2020), which approximates k-DPP-based batch active learning (Bıyık et al., 2019), or
Frank-Wolfe optimization (Pinsler et al., 2019); or the O(M2logM)complexity of methods based on K-centre
coresets (Sener & Savarese, 2018) (although heuristics and continuous relaxations can improve this somewhat).
In contrast, we examine simple and efficient stochastic strategies for adapting well-known single-sample
acquisition functions to the batch setting. The proposed stochastic strategies are based on observing that
acquisition scores would change as new points are added to the acquisition batch and modelling this difference
for additional batch samples in the most naive way, using Gumbel noise. The presented stochastic extensions
have the same complexity O(MlogK)as naive top- Kbatch acquisition, yet outperform it, and they can
perform on par with above more complex methods.
For multi-armed bandits, Soft Acquisition is also known as Boltzmann exploration (Cesa-Bianchi et al., 2017).
It has also been shown that adding noise to the scores, specifically via Thompson sampling, is effective
for choosing informative batches (Kalkanli & Özgür, 2021). Similarly, in reinforcement learning, stochastic
prioritisation has been employed as prioritized replay (Schaul et al., 2016) which may be effective for reasons
analogous to those motivating the approach examined in this work.
While stochastic sampling has not been extensively explored for acquisition in deep active learning, most
recently it has been used as an auxiliary step in diversity-based active learning methods that rely on clustering
as main mechanism (Ash et al., 2020; Citovsky et al., 2021). Kirsch et al. (2019) empirically find that
additional noise in the acquisition scores seems to benefit batch acquisition but do not investigate further.
Fredlund et al. (2010) suggest modeling single-point acquisition as sampling from a “ query density ” modulated
by the (unknown) sample density p(x)and analyze a binary classification toy problem. Farquhar et al. (2021)
propose stochastic acquisition as part of de-biasing actively learned estimators.
Most relevant to this work, and building on Fredlund et al. (2010) and Farquhar et al. (2021), Zhan et al.
(2022) propose a stochastic acquisition scheme that is asymptotically optimal. They normalize the acquisition
scores via the softmax function to obtain a query density function for unlabeled samples and draw an
acquisition batch from it, similar to SoftmaxEntropy. Their method aims to achieve asymptotic optimality
for active learning processes by mitigating the impact of bias. In contrast, in this work, we examine multiple
stochastic acquisition strategies based on score-based or rank-based distributions and apply these strategies
to several single-sample acquisition functions, such as BALD and entropy (and standard deviation, variation
ratios, see Figure 12); and we focus on active learning in a (Bayesian) deep learning setting. As such our
empirical results and additional proposed strategies can be seen as complementary to their work.
Thus, while stochastic sampling is generally well-known within acquisition functions, to our knowledge, this
work is the first2to investigate simple stochastic sampling methods entirely as alternatives to naive top- K
acquisition in (Bayesian) deep active learning and to compare them to more complex approaches in various
settings.
2A workshop version was presented at ICML 2021, and the first submission of this work was concurrent to Zhan et al. (2022).
8Published in Transactions on Machine Learning Research (09/2023)
5 Experiments
In this section, we empirically verify that the presented stochastic acquisition methods (a) outperform top- K
acquisition and (b) are competitive with specially designed batch acquisition schemes like BADGE (Ash et al.,
2020) and BatchBALD (Kirsch et al., 2019); and are vastly cheaper than these more complicated methods.
To demonstrate the seriousness of the possible weakness of recent batch acquisition methods, we use a range of
datasets. These experiments show that the performance of the stochastic extensions is not dependent on the
specific characteristics of any particular dataset. Our experiments include computer vision, natural language
processing (NLP), and causal inference (in §6.1). We show that stochastic acquisition helps avoid selecting
redundant samples on Repeated-MNIST (Kirsch et al., 2019), examine performance in active learning for
computer vision on EMNIST (Cohen et al., 2017), MIO-TCD (Luo et al., 2018), Synbols (Lacoste et al.,
2020), and CLINC-150 (Larson et al., 2019) for intent classification in NLP. MIO-TCD is especially close to
real-world datasets in size and quality. In appendix §C.5, we further investigate edges cases using the Synbols
dataset under different types of biases and noise, and in Appendix C.7, we also separately examine stochastic
batch acquisition using last-layer MFVI models on CIFAR-10 (Krizhevsky et al., 2009), SVHN (Netzer et al.,
2011), Repeated-MNIST, Fashion-MNIST (Xiao et al., 2017) and compare to ACS-FW (Pinsler et al., 2019).
In the main paper, we consider BALD as scoring function. We examine predictive entropy on many of
these datasets in the appendix. Additionally, other scoring functions are examined on Repeated-MNIST in
appendix §C.2.1. Overall, we observe similar results as for BALD.
For the sake of legible figures, we mainly focus on power acquisition in this section, as it fits BALD and
entropy best: the scores are non-negative, and zero scores imply uninformative samples. We show that all
three methods (power, softmax, softrank) can perform similarly in appendix §D.
We are not always able to compare to BADGE and BatchBALD because of computational limitations of
those methods. BatchBALD is computationally infeasible for large acquisition sizes ( ≥10) because of time
constraints, cf. Table 1. When possible, we use BatchBALD with acquisition size 5 as baseline. Note that
this gives BatchBALD an advantage as it is known to perform better with smaller acquisition sizes (Kirsch
et al., 2019). Similarly, BADGE ran out of memory for large dataset sizes, such as EMNIST ‘ByMerge’ with
814,255 examples, independently of the acquisition size.
Figures interpolate linearly between available points, and we show 95% confidence intervals.
Experimental Setup & Compute. We document the experimental setup and model architectures in detail
in appendix §C.1. Our experiments used about 25,000 compute hours on Titan RTX GPUs.
Runtime Measurements. We emphasize that the stochastic acquisition strategies are much more com-
putationally efficient compared to specialised batch-acquisition approaches like BADGE and BatchBALD.
Runtimes, shown in Table 1, are essentially identical for top- Kand the stochastic versions. Both are orders
of magnitude faster than BADGE and BatchBALD even for small batches. Unlike those methods, stochastic
acquisition scales linearlyin pool size and logarithmically in acquisition size. Runtime numbers do not include
the cost of retraining models (identical in each case). The runtimes for top- Kand stochastic acquisition
appear constant over Kbecause the execution time is dominated by fixed-cost memory operations. The
synthetic dataset used for benchmarking has 4,096 features, 10 classes, and 10,000 pool points.
Repeated-MNIST. Repeated-MNIST (Kirsch et al., 2019) duplicates MNIST a specified number of times
and adds Gaussian noise to prevent perfect duplicates. Redundant data are incredibly common in industrial
applications but are usually removed from standard benchmark datasets. The controlled redundancies in the
dataset allow us to showcase pathologies in batch acquisition methods. We use an acquisition size of 10 and 4
dataset repetitions.
Figure 2 shows that PowerBALD outperforms top- KBALD. While much cheaper computationally, cf. Table 1,
PowerBALD also outperforms BADGE and even performs on par with BatchBALD. We use an acquisition
size of 10 for all methods, except for BatchBALD, for which we use an acquisition size of 5, as explained
above. Note that BatchBALD performs better for smaller acquisition sizes while BADGE (counterintuitively)
can perform better for larger ones; see Figure 10 in the appendix for an ablation. BatchBALD, BALD, and
9Published in Transactions on Machine Learning Research (09/2023)
0.0 0.1 0.2 0.3 0.4 0.5Accuracy
0
50
100
150
200
250
300Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALD
BatchBALD 5
BADGE
BALD
(a) EMNIST ‘Balanced’ (5 trials)
0.0 0.2 0.4 0.6Accuracy
0
50
100
150
200
250
300Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALD
BatchBALD 5
BALD (b) EMNIST ‘ByMerge’ (5 trials)
0.96 0.97 0.98 0.99Accuracy
0
2500
5000
7500
10000
12500
15000Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALD
BADGE
BALD
Uniform
(c) MIO-TCD (3 trials)
0.80 0.85 0.90 0.95Accuracy
0
2500
5000
7500
10000
12500
15000Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALD
BADGE
BALD
Uniform (d) Synbols with minority groups (10 trials)
Figure 4: Performance on various datasets. BatchBALD took infeasibly long on these datasets & acquisition
sizes.(a)EMNIST ‘Balanced’: On 132k samples, PowerBALD (acq. size 10) outperforms BatchBALD (acq.
size 5) and BADGE (acq. size 10). (b)EMNIST ‘ByMerge’: On 814k samples, PowerBALD (acq. size 10)
outperforms BatchBALD (acq. size 5). BADGE (not shown) OOM’ed, and BatchBALD took >12days for
115 acquisitions. (c)MIO-TCD: PowerBALD performs better than BALD and on par with BADGE (all acq.
size100).(d)Synbols with minority groups: PowerBALD performs on par with BADGE (all acq. size 100).
the stochastic variants all become equivalent for acquisition size 1 when points are acquired individually,
which performs best Kirsch et al. (2019).
Computer Vision: EMNIST. EMNIST (Cohen et al., 2017) contains handwritten digits and letters
and comes with several splits: we examine the ‘Balanced‘ split with 131,600 samples in Figure 4a3and the
‘ByMerge‘ split with 814,255 samples in Figure 4b. Both have 47 classes. We use an acquisition size of 5 for
BatchBALD, and 10 otherwise.
We see that the stochastic methods outperform BatchBALD on it and both BADGE and BatchBALD on
‘Balanced’ (Figure 4a). They do not have any issues with the huge pool set in ‘ByMerge‘ (Figure 4b). In the
appendix, Figures 27 and 28 show results for all three stochastic extensions, and Figure 17 shows an ablation
of different acquisition sizes for BADGE. For ‘ByMerge’, BADGE ran out of memory on our machines, and
BatchBALD took more than 12 days for 115 acquisitions when we halted execution.
Computer Vision: MIO-TCD. The Miovision Traffic Camera Dataset (MIO-TCD) (Luo et al., 2018)
is a vehicle classification and localisation dataset with 648,959 images designed to exhibit realistic data
characteristics like class imbalance, duplicate data, compression artefacts, varying resolution (between 100
and 2,000 pixels), and uninformative examples; see Figure 9 in the appendix. As depicted in Figure 4c,
PowerBALD performs better than BALD and essentially matches BADGE despite being much cheaper to
compute. We use an acquisition size of 100 for all methods.
3This result exactly reproduces BatchBALD’s trajectory in Figure 7 from Kirsch et al. (2019).
10Published in Transactions on Machine Learning Research (09/2023)
Computer Vision: Synbols. Synbols (Lacoste et al., 2020) is a character dataset generator which can
demonstrate the behaviour of batch active learning under various edge cases (Lacoste et al., 2020; Branchaud-
Charron et al., 2021). In Figure 4d, we evaluate PowerBALD on a dataset with minority character types and
colours. PowerBALD outperforms BALD and matches BADGE. Further details as well as an examination of
the ‘spurious correlation’ and ‘missing synbols’ edge cases (Lacoste et al., 2020; Branchaud-Charron et al.,
2021) can be found in appendix §C.5.
Natural Language Processing: CLINC-150. We perform intent classification on CLINC-150 (Larson
et al., 2019), which contains 150 intent classes plus an out-of-scope class. This setting captures data seen in
production for chatbots. We fine-tune a pretrained DistilBERT model from HuggingFace (Wolf et al., 2020)
on CLINC-150 for 5 epochs with Adam as optimiser. In appendix §C.6, we see that PowerEntropy shows
strong performance: it performs better than Entropy and almost on par with BADGE. This demonstrates
that our technique is domain independent and can be easily reused for other tasks.
MFVI Last-Layer Comparison with ACS-FW . In Appendix C.7, we provide a comparison of the
proposed stochastic acquisition functions with BALD and ACS-FW (Pinsler et al., 2019) in a Bayesian
last-layer setting using variational inference with a mean-field Gaussian approximation (instead of Monte-Carlo
dropout). We find that for smaller acquisition sizes, the stochastic acquisition functions also outperform
BALD on Fashion-MNIST and Repeated-MNIST. For larger acquisition sizes on CIFAR-10 and SVHN, this
is not the case, however: no single method seems to perform better than the others. However, we find that
ACW-FW overall takes much longer to run than stochastic and top- Kacquisition (Figure 25). We leave a
more thorough investigation of this setting to future work.
In Summary. We have verified that stochastic acquisition functions outperform top- Kbatch acquisition in
several settings and perform on par with more complex methods such as BADGE or BatchBALD. Moreover,
we refer the reader to Jesson et al. (2021), Murray et al. (2021), Tigas et al. (2022), Holmes et al. (2022),
Malik et al. (2023), and Rubashevskii et al. (2023) for additional works that use the proposed stochastic
acquisition functions in this paper and provide further empirical validation.
6 Further Investigations
In this section, we examine and validate assumptions about the underlying score dynamics by examining the
scores across acquisitions. We further hypothesise about when top- Kacquisition is the most detrimental to
active learning.
Why Gumbel Noise? Intuitively, to select the k-th point in the acquisition batch, we want to take into
account how much additional information (increase in acquisition scores) the still-to-be-selected additional
K−kpoints will provide. As such we want to model the maximum over all possible additional candidate
points that are still to be selected to complete the acquisition batch. Empirically, acquisition scores are
similar to a truncated exponential distribution (‘80/20’ rule) as visualized in Figures 3 and 7c. Note that
this is a rough approximation—we do not claim that the distribution of acquisition scores really truncated
exponential. A sum of i.i.d. exponential variables follows the Erlang distribution (which is a special case of
the Gamma distribution) and has an exponential tail. We know that the maximum of a set of i.i.d. random
variables that follow an exponential distribution or a distribution which has an exponential tail is known to
be well approximated by a Gumbel distribution in the sample limit (Gumbel, 1954)4, and thus the maximum
over sums of such random variables also follows a Gumbel distribution. Concretely, we can use the following
result from Garg et al. (2023):
Theorem 6.1 (Extreme Value Theorem (EVT) (Mood, 1950; Fisher & Tippett, 1928)) .For i.i.d. ran-
dom variables X1,...,Xn∼fX, with exponential tails, limn→∞maxi(Xi)follows the Gumbel (GEV-1)
distribution. Furthermore, Gis max-stable, i.e. if Xi∼G, then maxi(Xi)∼Gholds.
However, it is unlikely that the increase in acquisition scores can be modeled well as i.i.d exponential
random variables with the samerate, but the Gumbel approximation also seems to hold empirically for the
hypoexponential distribution which is a sum of exponential distributions with different rates. We do not
have a proof for this, but we present a numerical simulation in appendix §H.
4See also the following Math StackExchange thread.
11Published in Transactions on Machine Learning Research (09/2023)
Overall, this motivates us to use a Gumbel distribution as a simple model for the increase in acquisition
scores the still-to-be-selected additional K−kpoints will provide under the modelling assumption that the
increase in acquisition scores at each step is exponentially distributed with different rates at each step. Zhan
et al. (2022) provide a different analysis for the use of Gumbel noise in the context of active learning as we
relate in §4.
Acquisition Asymptotics of Bayesian Models. For well-specified and well-defined Bayesian parametric
models, the posterior distribution of the model parameters converges to the true parameters as the number
of data points increases (Van der Vaart, 2000).
For such models and assuming that the predictions are independent given the model parameters, the total
correlation between the predictions decreases as the number of training points increases, as the posterior
distribution of the model parameters becomes more concentrated around the true parameters:
TC[Y1,...,YK|x1,...,xK,Dtrain]→0as|Dtrain|→∞. (16)
This can be proved by noting that in the finite data limit, the posterior parameter distribution converges to
the true model parameters, and the marginal distribution then factorizes. This means that the predictions
become more independent as the number of training points increases and fully independent in the infinite
data limit.
The total correlation is defined as:
TC[Y1,...,YK|x1,...,xK,Dtrain]:=/summationdisplay
iH[Yi|xi,Dtrain]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
top-KEntropy−H[Y1,...,YK|x1,...,xK,Dtrain]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
‘Batch Entropy’.,(17)
We can also write the total correlation as difference between top- KBALD and BatchBALD:
TC[Y1,...,YK|x1,...,xK,Dtrain] =/summationdisplay
iI[Yi; Ω|xi,Dtrain]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
top-KBALD−I[Y1,...,YK; Ω|x1,...,xK,Dtrain]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
BatchBALD.(18)
As the total correlation converges to 0, the top-KBALD term (first term) becomes equal to the BatchBALD
term (the second term on the right side), and the same happens for top- Kentropy and ‘BatchEntropy’, which
we could similarly define.
Thus, for well-specified and well-defined Bayesian parametric models, the top- Kacquisition functions will
eventually become equivalent to the BatchBALD and ‘BatchEntropy’ acquisition functions as the number of
training points increases. This tells us that top- Kacquisition is the most detrimental to active learning in the
earlier stages of learning, when the total correlation between the predictions is still high. This is consistent
with our empirical results below (‘Increasing Top- KAnalysis’).
At the same time, as the number of training points increases and the model parameters concentrate, the
expected information gain (BALD) also decreases. The mutual information with a deterministic variable is
always 0, and thus:
I[Y; Ω|x,Dtrain]→0as|Dtrain|→∞. (19)
This asymptotic behavior is a trivial but important result, as it tells us that the expected information gain
(BALD) will eventually become uninformative as the number of training points increases and no better than
random acquisition, and the important question is: when? Given that we only have noisy estimators, this
determines until when active learning is of use compared to random acquisition.
Many different active learning methods that are considered non-Bayesian nevertheless approximate the
expected information gain or the expected predictive information (Kirsch & Gal, 2022; Smith et al., 2023),
which is an expected total correlation. Hence, the considerations apply to those methods, too.
Finally, we observe that estimators such as BatchBALD, which utilize Monte-Carlo samples for parameter
approximations, are inherently limited by the logarithm of the total number Mof these samples, logM.
12Published in Transactions on Machine Learning Research (09/2023)
0 50 100 150 200
Acquisition Delta Step n0.25
0.000.250.500.751.00Rank Correlation st:st+n
T op
Bottom
 1%
10%
100%
(a) t= 0
0 20 40 60 80 100 120
Acquisition Delta Step n0.25
0.000.250.500.751.00Rank Correlation st:st+n (b) t= 100
Figure 5: Rank correlations for BALD scores on MNIST between the
initial scores and later scores of the top- or bottom-scoring 1%, 10% and
100% of test points (smoothed with a size-10 Parzen window). Rank-
orders decorrelate faster for the most informative samples and in the
early stages of training. The top scorers’ ranks anti-correlate after
roughly 40 (100) acquisitions unlike the bottom ones. Later in training,
the acquisition scores stay more strongly correlated. This suggests the
acquisition size could be increased later in training .
0 50 100 150 200
Training Set Size0.750.800.850.900.95AccuracyFigure 6: Top-Kacquisition hurts
less later in training (BALD on
MNIST). Att∈ {20,100}(blue),
we keep acquiring samples using the
BALD scores from those two steps.
Att= 20(orange), the model per-
forms well for≈20acquisitions; at
t= 120(green), for≈50; see §6.
This constraint implies that their informativeness can diminish rapidly. Concretely, consider an empirical
estimator ˆI[·; Ω]built using Monte-Carlo samples ω1,...,ωk. This is analogous to computing the exact mutual
information I[·;ˆΩ]with the ‘empirical’ random variable ˆΩ, which uniformly samples from ω1,...,ωk. Given
that the discrete mutual information is restricted by the entropy of its terms, we have:
ˆI[·; Ω] = I[·;ˆΩ]≤H[ˆΩ] = logM.
For instance, BatchBALD employs a greedy approach to select the t-th acquisition sample in its batch. It
does this by maximizing the empirical ˆI[Y; Ω|x,Yt−1,xt−1,...,Y 1,x1Dtrain]for the subsequent candidate
samples denoted by x. We can represent this relationship as:
logM≥ˆI[Y1,...,YK; Ω|x1,...,xK,Dtrain] =K/summationdisplay
i=1ˆI[Y; Ω|x,Yy,xy,...,Y 1,x1Dtrain].
From the above equation, as Kgrows, the estimated ˆI[YK; Ω|xK,YK−1,xK−1,...,Y 1,x1Dtrain]approaches
zero since it is restricted by logM. For a scenario with M= 100parameter samples (leading to log10M= 2),
BatchBALD might rapidly lose its informativeness after just two acquisitions in a classification scenario
involving 10 categories. This situation arises if the pool set contains a minimum of two highly diverse, that is
uncorrelated, data points with maximum disagreement.
Rank Correlations Across Acquisitions. In Section 3, we made the following assumptions: (1) the
acquisition scores stat steptare a proxy for scores st′at stept′>t; (2) the larger t′−tis, the worse a
proxystis fors′
t; (3) this effect is the largest for the most informative points.
We demonstrate these empirically by examining the Spearman rank correlation between scores during
acquisition. Specifically, we train a model for nsteps using BALD as single-point acquisition function. We
compare the rank order at each step to the starting rank order at step t. To denoise the rankings across n,
we smooth the rank correlations with a Parzen window of size 10 and to reduce the effect of noise to the rank
order, we round all scores to 2 decimal places. This especially removes unimportant rank changes for points
with low scores around 0.
Figure 1 shows that acquisition scores become less correlated as more points are acquired. Figure 5a shows
this in more detail for the top and bottom 1%, 10% or 100% of scorers of the test set across acquisitions
starting at step t= 0for a model initialised with 20 points. The top-10% scoring points (solid green) quickly
become uncorrelated across acquisitions and even become anti-correlated . In contrast, the points overall
(solid blue) correlate well over time (although they have a much weaker training signal on average). This
result supports all three of our hypotheses.
13Published in Transactions on Machine Learning Research (09/2023)
0 50 100 150 200
Acquisition Delta Step n0.40.60.81.0AUROC st:st+n
(a) t= 0
0 20 40 60 80 100 120
Acquisition Delta Step n0.40.60.81.0AUROC st:st+n
T op
1%Bottom
10% 
50% (b) t= 100
Sorted T est Samples0.00.20.40.60.81.01.2BALDStep 20
Step 100 (c) Scores at Step 20 and 100.
Figure 7: AUROCs for BALD scores on MNIST between the initial scores and later scores of the top- or
bottom-scoring 1%, 10% and 50% of test points (smoothed with a size-10 Parzen window). AUROC between
original points as ‘ground truth’ and later scores as predictors. This is equivalent to the probability that
the acquisition score at nfor a point in t= 0’s top or bottom 1%, etc. is larger than points outside. This
tells us how likely other points outside the batch have higher acquisition scores. This ignores the ranking of
points otherwise. (a, b)Points in the top quantiles are superseded by other points in the top quantiles in
the later acquisitions to a large degree. This is much more pronounced early in the training than later. The
bottom quantiles are more stable. (c)The overall score distributions at steps t= 0,100are visualized and
the relevant top and bottom quantiles are marked.
At the same time, we see that as training progresses, and we converge towards the best model, the order of
scores becomes more stable across acquisitions. In Figure 5b the model begins with 120 points ( t= 100),
rather than 20 ( t= 0). Here, the most informative points are less likely to change their rank—even the
top-1% ranks do not become anti-correlated , only decorrelated. Thus, we hypothesise that further in training,
we might be able to choose larger K.
We do not display the correlations for bottom 1% of scorers as their scores are close to 0 throughout training
and thus noisy and uninformative, see also Figure 7c for this.
Overall, this analysis can be confounded by noisy samples and swaps in rank order between samples with
similar scores that are not meaningful in the sense that they would not influence batch acquisition.
Thus, to provide a different analysis, we also consider the more direct question in Figure 7 of how likely
other samples have higher acquisition scores at t+nthan the top samples from tfor different quantiles (1%,
10%, 50%) of the test set. As a sanity check, we also examine the bottom quantiles. This is equivalent to
computing the AUROC between the original points as ‘ground truth’ and later scores as predictors: for
acquisition step n, the AUROC is p(St,top/bottom p%
n ≶St,bottom/top 1−p%
n )withSt,top/bottom p%
n :={st+n,i:
st,i∈top/bottom p%of{st,j}}. Specifically, we set up a binary classification with the top or bottom 1%,
10% or 50% of the test set as positive and the rest as negative. This again helps us quantify how much
the scores meaningfully change across acquisition steps. These results match the previous ones and provide
another validation for the mentioned assumptions.
Increasing Top- KAnalysis. Another way to investigate the effect of top- Kselection is to freeze the
acquisition scores during training and then continue single-point ‘active learning’ as if those were the correct
scores. Comparing this to the performance of regular active learning with updated single-point scores allows
us to examine how well earlier scores perform as proxies for later scores. We perform this toy experiment on
MNIST, showing that freezing scores early on greatly harms performance while doing it later has only a small
effect (Figure 6). For frozen scores at a training set size of 20 (73% accuracy, t= 0), the accuracy matches
single-acquisition BALD up to a training set size of roughly 40 (dashed orange lines) before diverging to a
lower level. But when freezing the scores of a more accurate model, at a training set size of 120 labels (93%
accuracy,t= 100), selecting the next fifty points according to those frozen scores performs indistinguishably
from step-by-step acquisition (dashed green lines). This result shows that top- Kacquisition hurts less later
in training but can negatively affect performance at the beginning of training.
14Published in Transactions on Machine Learning Research (09/2023)
0.80 0.85 0.90 0.95Accuracy
100
200
300Min Training Set SizePowerBALD =8
BatchBALD 5
PowerBALD
(a) Repeated-MNISTx4
0.2 0.3 0.4 0.5 0.6Accuracy
0
50
100
150
200
250
300Min Training Set SizeSoftmaxBALD =4
SoftmaxBALD =8
SoftmaxBALD =1
BatchBALD 5
BALD (b) EMNIST ‘ByMerge’
100 200 300 400
Training Set Size1.01.52.02.5PEHE
=4
Top-K=1
Uniform (c) IHDP
Figure 8: Effect of changing β.(a)Repeated-MNISTx4 (5 trials): PowerBALD outperforms BatchBALD
forβ= 8.(b)EMNIST ‘ByMerge’ (5 trials): SoftmaxBALD for β= 4performs best. (c)IHDP (400
trials):At high temperature ( β= 0.1), CausalBALD with power acquisition is like random acquisition. As
the temperature decreases, the performance improves (lower√ϵPEHE), surpassing top- Kacquisition. Both
experiments use an acquisition size of 10.
These observations lead us to ask whether we could dynamically change the acquisition size: with smaller
acquisition batches at the beginning and larger ones towards the end of active learning. We leave the
exploration of this for future work.
6.1 Ablation: Changing β
So far, we have set β= 1in the spirit of examining a simple baseline without additional hyperparameters.
The results above show that this already works well and matches the performance of much more expensive
methods, raising questions about their value. In addition, however, tuning βmay be able to further improve
performance. In the following, we show that other values of βcan yield even higher performance on Repeated-
MNIST, EMNIST, and when estimating causal treatment effects; we provide many additional results in
Appendix F.
Repeated-MNIST & EMNIST. In Figure 8a, we see that for PowerBALD the best-performing value,
β= 8,even outperforms BatchBALD. Similarly, tuning βcan improve the performance of other strategies as
well as we see for SoftmaxBALD on EMNIST, where β= 4performs better than β= 8andβ= 1, showing
that the best βdepends on the dataset.
Causal Treatment Effects: Infant Health Development Programme. Active learning for Conditional
Average Treatment Effect (CATE) estimation Heckman et al. (1997; 1998); Hahn (1998); Abrevaya et al.
(2015) on data from the Infant Health and Development Program (IHDP) estimates the causal effect of
treatments on an infant’s health from observational data. Statistical estimands of the CATE are obtainable
from observational data under certain assumptions. Jesson et al. (2021) show how to use active learning to
acquire data for label-efficient estimation. Among other subtleties, this prioritises the data for which matched
treated/untreated pairs are available.
We follow the experiments of Jesson et al. (2021) on both synthetic data and the semi-synthetic IHDP dataset
(Hill, 2011), a commonly used benchmark for causal effects estimation. In Figure 8c we show that power
acquisition performs significantly better than both top- Kand uniform acquisition, using an acquisition size of
10 in all cases with further. We provide additional results on semi-synthetic data in appendix §F.3. Note that
methods such as BADGE and BatchBALD are not well-defined for causal-effect estimation, while stochastic
batch acquisition remains applicable and is effective when fine-tuning β.
Performance on these tasks is measured using the expected Precision in Estimation of Heterogeneous Effect
(PEHE) (Hill, 2011) such that√ϵPEHE =/radicalbig
E[(/tildewideτ(X)−τ(X))2](Shalit et al., 2017) where /tildewideτis the estimated
CATE and τis CATE (i.e. a form of RMSE).
Limitations. Although we highlight the possibility for future work to adapt βto specific datasets or score
functions, our aim is not to offer a practical recipe for this to practitioners. Our focus is on showing how even
the simplest form of stochastic acquisition already raises questions for some recent more complex methods.
15Published in Transactions on Machine Learning Research (09/2023)
7 Discussion & Conclusion
Our experiments demonstrate that the stochastic sampling approach we have examined is orders of magnitude
faster than sophisticated batch-acquisition strategies like BADGE and BatchBALD while retaining comparable
performance in settings across computer vision, NLP, and causal inference. Compared to the flawed top- K
batch acquisition heuristic, it is never worse: we see no reason to continue using top- Kacquisition.
Importantly, our work raises serious questions about these current methods. If they fail to outperform
such a simple baseline in a wide range of settings, do they model the interaction between points sufficiently
well? If so, are the scores themselves unreliable? We call on future work in batch active learning to at least
demonstrate that it can outperform simple stochastic batch acquisition strategies.
At the same time, this opens doors for improved methods. Although we only put forward a naive model
due its computational and mathematical simplicity, future work can explore more sophisticated modelling of
the predicted score changes that take the current model and dataset into account. In its simplest form, this
might mean choosing the βhyperparameter of the acquisition distribution based on the dataset and adapting
it online. Our experiments also highlight that the acquisition size could be adapted dynamically, with larger
batch sizes acceptable in later acquisition steps.
Acknowledgements
The authors would like to thank their anonymous TMLR reviewers for their kind, constructive and helpful
feedback during the review process, which has significantly improved this work. We would also like to thank
Freddie Bickford Smith and Tom Rainforth as well as the members of OATML in general for their feedback
at various stages of the project. SF is supported by the EPSRC via the Centre for Doctoral Training in
Cybersecurity at the University of Oxford as well as Christ Church, University of Oxford. AK is supported
by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems (grant reference EP/L015897/1).
References
Jason Abrevaya, Yu-Chin Hsu, and Robert P Lieli. Estimating conditional average treatment effects. Journal
of Business & Economic Statistics , 33(4):485–505, 2015.
Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch
active learning by diverse, uncertain gradient lower bounds. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL
https://openreview.net/forum?id=ryghZJBKPS .
Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham M. Kakade. Gone fishing: Neural active
learning with fisher embeddings. In Advances in Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual ,
2021.
Parmida Atighehchian, Frédéric Branchaud-Charron, and Alexandre Lacoste. Bayesian active learning for
production, a systematic study and a reusable library. arXiv preprint arXiv:2006.09916 , 2020.
Les E. Atlas, David A. Cohn, and Richard E. Ladner. Training connectionist networks with queries and
selective sampling. In Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver,
Colorado, USA, November 27-30, 1989] , 1989.
Javad Azimi, Alan Fern, Xiaoli Zhang Fern, Glencora Borradaile, and Brent Heeringa. Batch active
learning via coordinated matching. In Proceedings of the 29th International Conference on Machine
Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012 . icml.cc / Omnipress, 2012. URL
http://icml.cc/2012/papers/607.pdf .
Cenk Baykal, Lucas Liebenwein, Dan Feldman, and Daniela Rus. Low-regret active learning. arXiv preprint
arXiv:2104.02822 , 2021.
16Published in Transactions on Machine Learning Research (09/2023)
Sarah Bird, Miro Dudík, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki,
Hanna Wallach, and Kathleen Walker. Fairlearn: A toolkit for assessing and improving fairness in AI.
Technical Report MSR-TR-2020-32, Microsoft, 2020.
Erdem Bıyık, Kenneth Wang, Nima Anari, and Dorsa Sadigh. Batch active learning using determinantal
point processes. arXiv preprint arXiv:1906.07975 , 2019.
Frédéric Branchaud-Charron, Parmida Atighehchian, Pau Rodríguez, Grace Abuhamad, and Alexandre
Lacoste. Can active learning preemptively mitigate fairness issues? ICLR Workshop on Responsable AI ,
2021.
Klaus Brinker. Incorporating diversity in active learning with support vector machines. In Tom Fawcett
and Nina Mishra (eds.), Machine Learning, Proceedings of the Twentieth International Conference (ICML
2003), August 21-24, 2003, Washington, DC, USA , pp. 59–66. AAAI Press, 2003. URL http://www.aaai.
org/Library/ICML/2003/icml03-011.php .
Colin Campbell, Nello Cristianini, and Alexander J. Smola. Query learning with large margin classifiers. In
Pat Langley (ed.), Proceedings of the Seventeenth International Conference on Machine Learning (ICML
2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000 , pp. 111–118. Morgan Kaufmann,
2000.
Nicolò Cesa-Bianchi, Claudio Gentile, Gábor Lugosi, and Gergely Neu. Boltzmann exploration done right.
Advances in neural information processing systems , 30, 2017.
Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Rostamizadeh,
and Sanjiv Kumar. Batch active learning at scale. In Advances in Neural Information Processing Systems
34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,
2021, virtual , 2021.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. EMNIST: an extension of MNIST
to handwritten letters. arXiv preprint arXiv:1702.05373 , 2017.
Sebastian Farquhar, Yarin Gal, and Tom Rainforth. On statistical bias in active learning: How and when to
fix it. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021 , 2021.
Ronald Aylmer Fisher and Leonard Henry Caleb Tippett. Limiting forms of the frequency distribution of
the largest or smallest member of a sample. In Mathematical proceedings of the Cambridge philosophical
society. Cambridge University Press, 1928.
Richard Fredlund, Richard M. Everson, and Jonathan E. Fieldsend. A bayesian framework for active learning.
InInternational Joint Conference on Neural Networks, IJCNN 2010, Barcelona, Spain, 18-23 July, 2010 ,
2010.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In Doina
Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70 of Proceedings of Machine Learning
Research , pp. 1183–1192. PMLR, 2017.
Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent rl without
entropy. arXiv preprint arXiv:2301.02328 , 2023.
Emil Julius Gumbel. Statistical theory of extreme values and some practical applications: a series of lectures ,
volume 33. US Government Printing Office, 1954.
Yuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. In John C. Platt, Daphne
Koller, Yoram Singer, and Sam T. Roweis (eds.), Advances in Neural Information Processing Systems 20,
Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, Vancouver,
British Columbia, Canada, December 3-6, 2007 , pp. 593–600. Curran Associates, Inc., 2007. URL https:
//proceedings.neurips.cc/paper/2007/hash/ccc0aa1b81bf81e16c676ddb977c5881-Abstract.html .
17Published in Transactions on Machine Learning Research (09/2023)
Jinyong Hahn. On the role of the propensity score in efficient semiparametric estimation of average treatment
effects.Econometrica , pp. 315–331, 1998.
James J Heckman, Hidehiko Ichimura, and Petra E Todd. Matching as an econometric evaluation estimator:
Evidence from evaluating a job training programme. The review of economic studies , 64(4):605–654, 1997.
James J Heckman, Hidehiko Ichimura, and Petra Todd. Matching as an econometric evaluation estimator.
The review of economic studies , 65(2):261–294, 1998.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational and
Graphical Statistics , 20(1):217–240, 2011.
Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Batch mode active learning and its application to
medical image classification. In Proceedings of the 23rd international conference on Machine learning , pp.
417–424, 2006.
Geoff Holmes, Eibe Frank, Dale Fletcher, and Corey Sterling. Efficiently correcting machine learning:
considering the role of example ordering in human-in-the-loop training of image classification models. In
IUI 2022: 27th International Conference on Intelligent User Interfaces, Helsinki, Finland, March 22 - 25,
2022, 2022.
Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classification
and preference learning. arXiv preprint arXiv:1112.5745 , 2011.
Andrew Jesson, Panagiotis Tigas, Joost van Amersfoort, Andreas Kirsch, Uri Shalit, and Yarin Gal. Causal-
bald: Deep bayesian active learning of outcomes to infer treatment-effects from observational data.
InAdvances in Neural Information Processing Systems 34: Annual Conference on Neural Information
Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , 2021.
Cem Kalkanli and Ayfer Özgür. Batched thompson sampling. In Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual , 2021.
Andreas Kirsch and Yarin Gal. Unifying approaches in active learning and active sampling via fisher
information and information-theoretic quantities. Transactions on Machine Learning Research , 2022. ISSN
2835-8856. URL https://openreview.net/forum?id=UVDAKQANOW .
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition
for deep bayesian active learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada , pp. 7024–7035, 2019.
Wouter Kool, Herke van Hoof, and Max Welling. Stochastic beams and where to find them: The gumbel-top-k
trick for sampling sequences without replacement. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.),Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research , pp. 3499–3508.
PMLR, 2019.
Andreas Krause and Daniel Golovin. Submodular function maximization. Tractability , 3:71–104, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alex Kulesza and Ben Taskar. k-dpps: Fixed-size determinantal point processes. In Proceedings of the 28th
International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2,
2011, 2011.
18Published in Transactions on Machine Learning Research (09/2023)
Alexandre Lacoste, Pau Rodríguez López, Frederic Branchaud-Charron, Parmida Atighehchian, Massimo
Caccia, Issam Hadj Laradji, Alexandre Drouin, Matt Craddock, Laurent Charlin, and David Vázquez.
Synbols: Probing learning algorithms with synthetic datasets. In Hugo Larochelle, Marc’Aurelio Ran-
zato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
0169cf885f882efd795951253db5cdfb-Abstract.html .
Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K.
Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. An evaluation dataset
for intent classification and out-of-scope prediction. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , 2019.
Zhiming Luo, Frederic Branchaud-Charron, Carl Lemaire, Janusz Konrad, Shaozi Li, Akshaya Mishra, Andrew
Achkar, Justin A. Eichel, and Pierre-Marc Jodoin. MIO-TCD: A new benchmark dataset for vehicle
classification and localization. IEEE Trans. Image Process. , 2018.
Chris J. Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In Zoubin Ghahramani, Max Welling,
Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information
Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December
8-13 2014, Montreal, Quebec, Canada , pp. 3086–3094, 2014.
Shreshth A Malik, Salem Lahlou, Andrew Jesson, Moksh Jain, Nikolay Malkin, Tristan Deleu, Yoshua
Bengio, and Yarin Gal. Batchgfn: Generative flow networks for batch active learning. arXiv preprint
arXiv:2306.15058 , 2023.
Alexander McFarlane Mood. Introduction to the theory of statistics. 1950.
Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deterministic
uncertainty: A simple baseline. arXiv e-prints , pp. arXiv–2102, 2021.
Chelsea Murray, James Urquhart Allingham, Javier Antorán, and José Miguel Hernández-Lobato. Depth
uncertainty networks for active learning. arXiv preprint arXiv:2112.06796 , 2021.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.
Jerzy Neyman. edited and translated by dorota m. dabrowska and terrence p. speed (1990). on the application
of probability theory to agricultural experiments. essay on principles. section 9. Statistical Science , 5(4):
465–472, 1923.
Robert Pinsler, Jonathan Gordon, Eric T. Nalisnick, and José Miguel Hernández-Lobato. Bayesian batch
active learning as sparse subset approximation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,
Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada , pp. 6356–6367, 2019. URL https://proceedings.neurips.cc/
paper/2019/hash/84c2d4860a0fc27bcf854c444fb8b400-Abstract.html .
Aleksandr Rubashevskii, Daria Kotova, and Maxim Panov. Scalable batch acquisition for deep bayesian
active learning. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM) , pp.
739–747. SIAM, 2023.
Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal
of educational Psychology , 66(5):688, 1974.
Donald B Rubin. Randomization analysis of experimental data: The fisher randomization test comment.
Journal of the American Statistical Association , 75(371):591–593, 1980.
19Published in Transactions on Machine Learning Research (09/2023)
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In Yoshua
Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016,
San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings , 2016. URL http://arxiv.org/
abs/1511.05952 .
Greg Schohn and David Cohn. Less is more: Active learning with support vector machines. In Pat Langley
(ed.),Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000), Stanford
University, Stanford, CA, USA, June 29 - July 2, 2000 , pp. 839–846. Morgan Kaufmann, 2000.
Jasjeet S Sekhon. The neyman-rubin model of causal inference and estimation via matching methods. The
Oxford handbook of political methodology , 2:1–32, 2008.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In
6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings . OpenReview.net, 2018. URL https://openreview.net/
forum?id=H1aIuk-RW .
Burr Settles. Active Learning Literature Survey. Machine Learning , 2010.
Uri Shalit, Fredrik D. Johansson, and David A. Sontag. Estimating individual treatment effect: generalization
bounds and algorithms. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70 of
Proceedings of Machine Learning Research , pp. 3076–3085. PMLR, 2017. URL http://proceedings.mlr.
press/v70/shalit17a.html .
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL http:
//arxiv.org/abs/1409.1556 .
Freddie Bickford Smith, Andreas Kirsch, Sebastian Farquhar, Yarin Gal, Adam Foster, and Tom Rainforth.
Prediction-oriented bayesian active learning. arXiv preprint arXiv:2304.08151 , 2023.
Panagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Schölkopf, Yarin Gal, and Stefan Bauer. Inter-
ventions, where and how? experimental design for causal models at scale. arXiv preprint arXiv:2203.02016 ,
2022.
Joost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. Improving deterministic
uncertainty estimation in deep learning for classification and regression. arXiv preprint arXiv:2102.11409 ,
2021.
Aad W Van der Vaart. Asymptotic statistics , volume 3. Cambridge university press, 2000.
Sahil Verma and Julia Rubin. Fairness definitions explained. In Proceedings of the International Workshop
on Software Fairness, FairWare@ICSE 2018, Gothenburg, Sweden, May 29, 2018 , 2018.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,
EMNLP 2020 - Demos, Online, November 16-20, 2020 , 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
Xueying Zhan, Yaowei Wang, and Antoni B. Chan. Asymptotic optimality for active learning processes.
InUncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty in
Artificial Intelligence, UAI 2022, 1-5 August 2022, Eindhoven, The Netherlands , 2022.
20Published in Transactions on Machine Learning Research (09/2023)
A Ethical impact
We do not foresee any ethical risks related to this work. Insofar as the sampling methods we have examined
reduce computational costs, applications might benefit from reduced resource consumption. The methods
appear to be as good or better than alternatives on evaluations examining the ability to learn from data with
under-represented groups and on evaluations that measure the difference between performance for the most-
and least-represented groups, which may aid algorithmic fairness (see C.5).
B Method
B.1 Other scoring functions
Following Gal et al. (2017), we also examine using variation ratios (least confidence) and standard deviation
as scoring functions.
Variation Ratio. Also known as least confidence , the variation-ratios is the complement of the least-confident
class prediction:
svariation-ratios (i;Itrain):= 1−max
yp(y|X=xi). (20)
This scoring function is non-negative and a score of 0means that the sample is uninformative: a score of 0
means that the respective prediction is one-hot, which means that the expected information gain is also 0as
can be easily verified. Thus, variation ratios matches the intuitions behind power acquisition.
Standard Deviation. The standard deviation score function measures the sum of the class probability
deviations and is closely related to the BALD scores:
sstd-dev (i;Itrain):=/summationdisplay
y/radicalig
Varp(ω)[p(y|X=xi,ω)]. (21)
This scoring function is also non-negative, and no variance for the predictions implies a zero expected
information gain and thus an uninformative sample. Thus, the standard deviation should also perform well
with power acquisition.
B.2 Proof of Proposition 3.1
First, we remind the reader that a random variable Gis Gumbel distributed G∼Gumbel (µ;β)when its
cumulative distribution function follows p(G≤g) = exp(−exp(−g−µ
β)).
Furthermore, the Gumbel distribution is closed under translation and positive scaling:
Lemma B.1. LetG∼Gumbel (µ;β)be a Gumbel distributed random variable, then:
αG+d∼Gumbel (d+αµ;αβ). (22)
Proof.We have p(αG+d≤x) = p(G≤x−d
α). Thus, we have:
p(αG+d≤x) = exp(−exp(−x−d
α−µ
β)) (23)
= exp(−exp(−x−(d+αµ)
αβ)) (24)
⇔αG+d∼Gumbel (d+αµ;αβ). (25)
We can then easily prove Proposition 3.1 using Theorem 1 from Kool et al. (2019), which we present it here
slightly reformulated to fit our notation:
21Published in Transactions on Machine Learning Research (09/2023)
(a) Good sample.
(b) Duplicates.
 (c) Class confusion.
(d) Compression artefacts.
 (e) Low-resolution sample.
Figure 9: MIO-TCD Dataset is designed to include common artifacts from production data. The size and
quality of the images vary greatly between crops; from high-quality cameras on sunny days to low-quality
cameras at night. (a) shows an example of clean samples that can be clearly assigned to a class. (b)(c)(d)
and (e) show the different categories of noise. (b) shows an example of duplicates that exist in the dataset.
(c) is a good example where the assigned class is subject to interpretation: motorcycle or bicycle? (d) is a
sample with heavy compression artefacts and (e) is an example of samples with low resolution which again is
considered a hard example to learn for the model.
Lemma B.2. Fork≤n, letI∗
1,...,I∗
k=arg topk{si+ϵi}iwithϵi∼Gumbel (0; 1), i.i.d.. Then I∗
1,...,I∗
kis
an (ordered) sample without replacement from the Categorical/parenleftbigg
expsi/summationtext
j∈nexpsj,i∈{1,...,n}/parenrightbigg
distribution, e.g.
for a realization i∗
1,...,i∗
kit holds that
P(I∗
1=i∗
1,...,I∗
k=i∗
k) =k/productdisplay
j=1expsi∗
j/summationtext
ℓ∈N∗
jexpsℓ
whereN∗
j=N\/braceleftbig
i∗
1,...,i∗
j−1/bracerightbig
is the domain (without replacement) for the j-th sampled element.
Now, it is easy to prove the proposition:
Proposition 3.1. For scores si,i∈{1,...,n}, andk≤nandβ > 0, if we draw ϵi∼Gumbel (0;β−1)
independently, then arg topk{si+ϵi}iis an (ordered) sample without replacement from the categorical
distribution Categorical( exp(βsi)//summationtext
jexp(βsj),i∈{1,...,n}).
Proof.Asϵi∼Gumbel (0;β−1), defineϵ′
i:=βϵi∼Gumbel (0; 1). Further, let s′
i:=βsi. Applying Lemma B.2
ons′
iandϵ′
i,arg topk{s′
i+ϵ′
i}iyields (ordered) samples without replacement from the categorical distribution
Categorical (exp(βsi)/summationtext
jexp(βsj),i∈{1,...,n}). However, multiplication by βdoes not change the resulting indices
ofarg topk:
arg topk{s′
i+ϵ′
i}i= arg topk{si+ϵi}i, (26)
concluding the proof.
22Published in Transactions on Machine Learning Research (09/2023)
C Experiments
C.1 Experimental setup & compute
Frameworks. We use PyTorch. Repeated-MNIST and EMNIST experiments use PyTorch Ignite. Synbols
and MIO-TCD experiments use the BaaL library: https://github.com/baal-org/baal (Atighehchian
et al., 2020). Predictive parity is calculated using FairLearn (Bird et al., 2020). The CausalBALD ex-
periments use https://github.com/anndvision/causal-bald (Jesson et al., 2021). The experiments
comparing to ACS-FW (Pinsler et al., 2019) use the original authors’ implementation with added support
for stochastic batch acquisitions: https://github.com/BlackHC/active-bayesian-coresets/releases/
tag/stoch_batch_acq_paper . The Repeated-MNIST experiments were run using https://github.com/
BlackHC/active_learning_redux/releases/tag/stoch_batch_acq , and the results are also available on
WandB ( https://wandb.ai/oatml-andreas-kirsch/oatml-snow-stoch-acq ).
Compute. Results shown in Table 1 were run inside Docker containers with 8 CPUs (2.2Ghz) and 32 Gb of
RAM. Other experiments were run on similar machines with Titan RTX GPUs. The Repeated-MNIST and
EMNIST experiments take about 5000 GPU hours. The MIO, Synbols and CLINC-150 experiments take
about 19000 GPU hours. The CausalBALD experiments take about 1000 GPU hours.
Dataset Licenses. Repeated-MNIST is based on MNIST which is made available under the terms of the
Creative Commons Attribution-Share Alike 3.0 license. The EMNIST dataset is made available as CC0
1.0 Universal Public Domain Dedication. Synbols is a dataset generator. MIO-TCD is made available
under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
CLINC-150 is made available under the terms of Creative Commons Attribution 3.0 Unported License.
C.1.1 Runtime measurements
The synthetic dataset used for benchmarking has 4,096 features, 10 classes, and 10,000 pool points. VGG-16
models (Simonyan & Zisserman, 2015) were used to sample predictions and latent embeddings.
C.1.2 Repeated-MNIST
The Repeated-MNIST dataset is also constructed following Kirsch et al. (2019) with duplicated examples
from MNIST with isotropic Gaussian noise added to the input images (standard deviation 0.1).
We use the same setup as Kirsch et al. (2019): a LeNet-5-like architecture with ReLU activations instead of
tanh and added dropout. The model obtains 99% test accuracy when trained on the full MNIST dataset.
Specifically, the model is made up of two blocks of a convolution, dropout, max-pooling, ReLU with 32 and
64 channels and 5x5 kernel size, respectively. As classifier head, a two-layer MLP with 128 hidden units
(and 10 output units) is used that includes dropout between the layers. We use a dropout probability of
0.5 everywhere. The model is trained with early stopping using the Adam optimiser and a learning rate of
0.001. We sample predictions using 100 MC-Dropout samples for BALD. Weights are reinitialized after each
acquisition step.
C.1.3 EMNIST
We follow the setup from (Kirsch et al., 2019) with 20 MC dropout samples. We use a similar model as for
Repeated-MNIST but with three blocks instead of two. Specifically, we use 32, 64, and 128 channels and 3x3
kernel size. This is followed by a 2x2 max pooling layer before the classifier head. The classifier head is a
two-layer MLP but with 512 hidden units instead of 128. Again, we use dropout probability 0.5 everywhere.
C.1.4 Synbols & MIO-TCD
The full list of hyperparameters for the Synbols and MIO-TCD experiments is presented in Table 3. Our
experiments are built using the BaaL library (Atighehchian et al., 2020). We compute the predictive parity
using FairLearn (Bird et al., 2020). We use VGG-16 model (Simonyan & Zisserman, 2015) trained for 10
epochs using Monte Carlo dropout for acquisition (Gal et al., 2017) with 20 dropout samples.
23Published in Transactions on Machine Learning Research (09/2023)
Table 3: Hyper-parameters used in Section 5 and C.5
Hyperparameter Value
Learning rate 0.001
Optimiser SGD
Weight decay 0
Momentum 0.9
Loss function Crossentropy
Training duration 10
Batch size 32
Dropoutp 0.5
MC iterations 20
Query size 100
Initial set 500
In Figure 9, we show a set of images with common problems that can be found in MIO-TCD.
C.1.5 CLINC-150
We fine-tune a pretrained DistilBERT model from HuggingFace (Wolf et al., 2020) on CLINC-150 for 5
epochs with Adam as optimiser. Estimating epistemic uncertainty in transformer models is an open research
question, and hence, we do not report results using BALD and focus on entropy instead.
C.1.6 CausalBALD
Using the Neyman-Rubin framework (Neyman, 1923; Rubin, 1974; Sekhon, 2008), the CATE is formulated
in terms of the potential outcomes, Yt, of treatment levels t∈{0,1}. Given observable covariates, X, the
CATE is defined as the expected difference between the potential outcomes at the measured value X=x:
τ(x) =E[Y1−Y0|X=x]. This causal quantity is fundamentally unidentifiable from observational data
without further assumptions because it is not possible to observe both Y1andY0for a given unit. However,
under the assumptions of consistency, non-interference, ignoreability, and positivity, the CATE is identifiable
as the statistical quantity /tildewideτ(x) =E[Y|T = 1,X=x]−E[Y|T = 0,X=x](Rubin, 1980).
Jesson et al. (2021) define BALD acquisition functions for active learning CATE functions from observational
data when the cost of acquiring an outcome, y, for a given covariate and treatment pair, (x,t), is high.
Because we do not have labels for Y1andY0for each (x,t)pair in the dataset, their acquisition function
focusses on acquiring data points (x,t)for which it is likely that a matched pair (x,1−t)exists in the pool
data or has already been acquired at a previous step. We follow their experiments on their synthetic dataset
with limited positivity and the semi-synthetic IHDP dataset (Hill, 2011). Details of the experimental setup
are given in (Jesson et al., 2021), we use their provided code, and implement the power acquisition function.
The settings for causal inference experiments are identical to those used in Jesson et al. (2021), using the
IHDP dataset (Hill, 2011). Like them, we use a Deterministic Uncertainty Estimation Model (van Amersfoort
et al., 2021), which is initialised with 100 data points and acquire 10 data points per acquisition batch for 38
steps. The dataset has 471 pool points and a 201 point validation set.
C.1.7 ACS-FW
We compare to the experiments in the paper ‘Bayesian Batch Active Learning as Sparse Subset Approximation’
(Pinsler et al., 2019) which introduces the ACS-FW algorithm for batch acquisition. For its deep learning
experiments, the paper uses a ResNet feature extractor, followed by a Bayesian multi-class classification
model on the final layer. Since exact inference is intractable, variational inference is used with a factorized
Gaussian posterior approximation. As prior a zero-mean Gaussian is used, and Monte-Carlo samples are
drawn to approximate the predictive distribution.
24Published in Transactions on Machine Learning Research (09/2023)
C.2 Repeated-MNIST
0.6 0.7 0.8 0.9Accuracy
0
50
100
150
200
250
300Min Training Set SizeRepetition Factor = 1
0.6 0.7 0.8 0.9Accuracy
0
50
100
150
200
250
300Repetition Factor = 2
0.6 0.7 0.8 0.9Accuracy
0
50
100
150
200
250
300Repetition Factor = 4
Acquisition Function
BADGE 10
BADGE 20
BADGE 40
Figure 10: Repeated-MNIST x4 (5 trials): acquisition size ablation for BADGE. Acquisition size 40 performs
best out of{10,20,40}for repetition factor 1, acquisition 20 performs best for repetition factor 2, and all
three perform similarly for repetition factor 4.
BADGE Ablation. In Figure 10, we see that BADGE performs best with acquisition size 20 on Repeated-
MNISTx4 overall. BADGE 40 and BADGE 20 have the highest final accuracy, cf. BADGE 10 while BADGE
20 performs better than BADGE 40 for small training set sizes.
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALD
BatchBALD
BALD
Figure 11: Repeated-MNIST x4 (5 trials): acquisition size 5. To compare BatchBALD, BALD and the
stochastic acquisition strategies more fairly, we also show the performance of acquisition size 5. This is in
line with Figure 2.
Acquisition Size 5. In Figure 11, we see that the stochastic acquisition strategies perform better than
BALD and as well as BatchBALD for acquisition size 5 as well. Thus, we see no qualitative difference to the
results in Figure 2.
25Published in Transactions on Machine Learning Research (09/2023)
C.2.1 Other scoring functions
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Min Training Set SizePowerBALD
PowerStdDev
PowerVariationRatios
PowerEntropy
UniformBALD
StdDev
VariationRatios
Entropy
Figure 12: Repeated-MNIST x4 (5 trials): Performance for other scoring functions. Entropy, std dev,
variation ratios behave like BALD when applying a stochastic sampling scheme (Power from §3).
In Figure 12 shows the performance of other scoring functions than BALD on RepeatedMNIST x4.
C.2.2 Redundancy ablation
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Min Training Set SizeRepetition Factor = 1
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Repetition Factor = 2
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Repetition Factor = 4
PowerBALD
BatchBALD 5
BADGEUniform
BALD
Figure 13: Repeated-MNIST (5 trials): Performance ablation for different repetition counts.
In Figure 13, we see the same behaviour in an ablation for different repetition sizes of Repeated-MNIST.
26Published in Transactions on Machine Learning Research (09/2023)
C.3 MIO-TCD
0.95 0.96 0.97 0.98 0.99Accuracy
0
2000
4000
6000
8000
10000
12000
14000Min Training Set SizePowerBALD
BADGEBALD
Uniform
(a) BALD
0.95 0.96 0.97 0.98 0.99Accuracy
0
2000
4000
6000
8000
10000
12000
14000Min Training Set SizePowerEntropy
EntropyBADGE
Uniform (b) Entropy
Figure 14: MIO-TCD (5 trials).
In Figure 14, we see that power acquisition performs on par with BADGE with both BALD and entropy as
underlying score functions.
C.4 EMNIST
0.0 0.1 0.2 0.3 0.4 0.5Accuracy
0
50
100
150
200
250
300Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALD
BatchBALD 5
BADGE
BALD
Figure 15: EMNIST (Balanced) (5 trials): Per-
formance with BALD.
0.0 0.2 0.4 0.6Accuracy
0
50
100
150
200
250
300Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALD
BatchBALD 5
BALDFigure 16: EMNIST (ByMerge) (5 trials): Per-
formance with BALD.
In Figure 15 and 16, we see that PowerBALD outperforms BALD, BatchBALD, and BADGE.
27Published in Transactions on Machine Learning Research (09/2023)
0.0 0.1 0.2 0.3 0.4 0.5Accuracy
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function
BADGE 10
BADGE 20
BADGE 40
Figure 17: EMNIST (Balanced) (5 trials): acquisition size ablation for BADGE.
BADGE Ablation. In Figure 17, we see that BADGE performs similarly with all three acquisition sizes.
Acquisition size 10 is the smoothest.
C.5 Edge cases in Synbols
We use Synbols (Lacoste et al., 2020) to demonstrate the behaviour of batch active learning in artificially
constructed edge cases. Synbols is a character dataset generator for classification where a user can specify
the type and proportion of bias and insert artefacts, backgrounds, masking shapes, and so on. We selected
three datasets with strong biases supplied by Lacoste et al. (2020); Branchaud-Charron et al. (2021) to
evaluate stochastic batch acquisition methods. We use an acquisition size of 100. The experimental settings
are described in Appendix C.1.
For these tasks, performance evaluation includes ‘predictive parity’, also known as ‘accuracy difference’,
which is the maximum difference in accuracy between subgroups—which are, in this case, different coloured
characters. This measure is used most widely in domain adaptation and ethics (Verma & Rubin, 2018). We
want to maximise the accuracy while minimising the predictive parity.
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2000
4000
6000
8000
10000
12000
14000Min Training Set SizePowerBALD
BADGEBALD
Uniform
(a) Accuracy
0 2000 4000 6000 8000 10000 12000 14000
Training Set Size0.000.050.100.150.200.25Predictive ParityPowerBALD
BADGEBALD
Uniform (b) Predictive parity ( Down and left is better. )
Figure 18: Performance on Synbols Spurious Correlations (3 trials) with BALD. Stochastic acquisition
matches BADGE and BALD’s predictive parity and performance, which is reassuring as stochastic acquisition
functions might be affected by spurious correlations.
28Published in Transactions on Machine Learning Research (09/2023)
Spurious Correlations. This dataset includes spurious correlations between character colour and class. As
shown in Branchaud-Charron et al. (2021), active learning is especially strong here as characters that do not
follow the correlation will be informative and thus selected.
We compare the predictive parity between methods in Fig. 18b. We do not see any significant difference
between the stochastic batch acquisition method and BADGE or BALD. This is encouraging, as stochastic
approaches might select more examples following the spurious correlation and thus have higher predictive
parity, but this is not the case.
0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2000
4000
6000
8000
10000
12000
14000Min Training Set SizePowerBALD
BADGEBALD
Uniform
(a) Accuracy
0 2000 4000 6000 8000 10000 12000 14000
Training Set Size0.000.010.020.030.040.050.060.07Predictive ParityPowerBALD
BADGEBALD
Uniform (b) Predictive parity
Figure 19: Synbols Minority Groups (3 trials): Performance on BALD . PowerBALD outperforms BALD and
matches BADGE for both accuracy and predictive parity.
Minority Groups. This dataset includes a subgroup of the data that is under-represented; specifically,
most characters are red while few are blue. As Branchaud-Charron et al. (2021) shows, active learning can
improve the accuracy for these groups.
Our stochastic approach lets batch acquisition better capture under-represented subgroups. In Figure 19a,
PowerBALD has an accuracy almost identical to that of BADGE, despite being much cheaper, and outperforms
BALD. At the same time, we see in Figure 19b that PowerBALD has a lower predictive parity than BALD,
demonstrating a fairer predictive distribution given the unbalanced dataset.
0.955 0.960 0.965 0.970 0.975 0.980 0.985Accuracy
0
2000
4000
6000
8000
10000
12000
14000Min Training Set SizePowerBALD
BADGEBALD
Uniform
Figure 20: BALD
0.955 0.960 0.965 0.970 0.975 0.980 0.985Accuracy
0
2000
4000
6000
8000
10000
12000
14000Min Training Set SizePowerEntropy
EntropyBADGE
Uniform Figure 21: Entropy
Figure 22: Performance on Synbols Missing Characters (3 trials). In this dataset with high aleatoric
uncertainty, PowerBALD matches BADGE and BALD performance. PowerEntropy significantly outperforms
Entropy which confounds aleatoric and epistemic uncertainty.
29Published in Transactions on Machine Learning Research (09/2023)
Missing Synbols. This dataset has high aleatoric uncertainty (input noise). Some images are missing
information required to make high-probability predictions—these images have shapes randomly occluding
the character—so even a perfect model would remain uncertain. Lacoste et al. (2020) demonstrated that
entropy is ineffective on this data as it cannot distinguish between aleatoric and epistemic uncertainty (input
noise and model uncertainty), while BALD can do so. As a consequence, entropy will unfortunately prefer
samples with occluded characters, resulting in degraded active learning performance. For predictive entropy,
stochastic acquisition largely corrects the failure of entropy acquisition to account for missing data (Figure 22)
although PowerEntropy still underperforms BADGE here. For BALD, we show in Figure 20 in the appendix
that, as before, the stochastic batch acquisition method performs on par with BADGE and marginally better
than BALD.
C.6 CLINC-150
0.60 0.65 0.70 0.75 0.80 0.85Accuracy
1000
2000
3000
4000
5000
6000
7000
8000Min Training Set SizePowerEntropy
BADGE
Entropy
Uniform
Figure 23: Performance on CLINC-150 (10 trials). PowerEntropy performs much better than entropy, which
only performs marginally better than uniform, and almost on par with BADGE.
In Figure 23, we see that PowerEntropy performs much better than entropy which only performs marginally
better than the uniform baseline. PowerEntropy also performs better than BADGE at low training set
sizes, but BADGE performs better in the second half. Between ≈2300and4000samples, BADGE and
PowerEntropy perform the same. We use an acquisition size of 100, starting from an initial training set of
1510 points (10 points per intent class).
30Published in Transactions on Machine Learning Research (09/2023)
C.7 ACS-FW
6000 8000 10000 12000 14000 16000
Training Set Size0.700.750.80Accuracy
PowerBALD
ACS-FWBALD
Uniform
(a) CIFAR-10
1000 2000 3000 4000 5000 6000 7000
Training Set Size0.700.750.800.850.90Accuracy
PowerBALD
ACS-FWBALD
Uniform (b) SVHN
50 100 150 200
Training Set Size0.40.50.60.70.80.9Accuracy
PowerBALD
ACS-FWBALD
Uniform
(c) Repeated-MNIST
50 100 150 200
Training Set Size0.40.50.60.7Accuracy
PowerBALD
ACS-FWBALD
Uniform (d) Fashion-MNIST
Figure 24: MFVI Last-Layer Performance (3 trials).
6000 8000 10000 12000 14000 16000
Training Set Size05001000150020002500WalltimePowerBALD
ACS-FWBALD
Uniform
(a) CIFAR-10
1000 2000 3000 4000 5000 6000 7000
Training Set Size02004006008001000WalltimePowerBALD
ACS-FWBALD
Uniform (b) SVHN
50 100 150 200
Training Set Size0200400600WalltimePowerBALD
ACS-FWBALD
Uniform
(c) Repeated-MNIST
50 100 150 200
Training Set Size020040060080010001200WalltimePowerBALD
ACS-FWBALD
Uniform (d) Fashion-MNIST
Figure 25: MFVI Last-Layer Walltime (3 trials).
31Published in Transactions on Machine Learning Research (09/2023)
We run comparisons for several datasets using last-layer mean-field variational inference: CIFAR-10
(Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), Repeated-MNIST (Kirsch et al., 2019), and Fashion-
MNIST (Xiao et al., 2017). We use 5000 initial training samples with an acquisition size of 4000 for CIFAR-10,
1000 initial training samples with an acquisition size of 2000 for SVHN, 20 initial training samples with an
acquisition size of 100 for Repeated-MNIST, and 20 initial training samples with an acquisition size of 25 for
Fashion-MNIST. The same hyperparameters as in Pinsler et al. (2019) are used.
In Figure 24, we see that PowerBALD performs best on Fashion-MNIST and Repeated-MNIST with smaller
acquisition sizes, but ACS-FW and BALD perform similarly on CIFAR-10 and SVHN at larger acquisition
sizes. This is in line with Section 6, where we have seen in Figures 5 and 7 that the top 1% of the samples
are more sensitive than the top 10%: larger acquisition sizes can be less sensitive to the issues of top- K. The
performance of ACS-FW contrasts to the results in Pinsler et al. (2019) where ACS-FW performs better than
BALD on CIFAR-10 and SVHN. In Appendix E.3, we provide ablations with different acquisition sizes and
initial training set sizes for these four datasets. Overall, beyond what we have noted above, these results are
not conclusive, however, and the performance curves are very close to each other. On the other hand, the wall
time curves in Figure 25 show that ACS-FW is significantly slower than PowerBALD. This is congruent with
the motivation that stochastic sampling methods are fast yet competitive and don’t underperform compared
to top-Kacquisition, thus being a good substitute for top- Kacquisition.
D Comparing Power, Softmax and Soft-Rank
D.1 Empirical Evidence
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Min Training Set SizeRepetition Factor = 1
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Repetition Factor = 2
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Repetition Factor = 4
PowerBALD
SoftmaxBALDSoftrankBALD
(a) BALD
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Min Training Set SizeRepetition Factor = 1
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Repetition Factor = 2
0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Accuracy
50
100
150
200
250
300Repetition Factor = 4
PowerEntropy
SoftmaxEntropySoftrankEntropy
(b) Entropy
Figure 26: Repeated-MNIST (5 trials): Performance with all three stochastic strategies.
Repeated-MNIST. In Figure 26, power acquisition performs best overall, followed by soft-rank and then
softmax.
32Published in Transactions on Machine Learning Research (09/2023)
0.0 0.1 0.2 0.3 0.4 0.5Accuracy
0
50
100
150
200
250
300Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALDBatchBALD 5
BADGE
BALD
Figure 27: EMNIST (Balanced) (5 trials): Per-
formance with all three stochastic strategies with
BALD.PowerBALD performs best.
0.0 0.1 0.2 0.3 0.4 0.5 0.6Accuracy
0
50
100
150
200
250
300Min Training Set SizePowerBALD
SoftmaxBALD
SoftrankBALDBatchBALD 5
BALDFigure 28: EMNIST (ByMerge) (5 trials): Per-
formance with all three stochastic strategies with
BALD.PowerBALD performs best.
EMNIST. In Figure 27 and 28, we see that PowerBALD performs best, but Softmax- and SoftrankBALD
also outperform other methods. BADGE did not run on EMNIST (ByMerge) due to out-of-memory issues
and BatchBALD took very long as EMNIST (ByMerge) has more than 800,000 samples.
0.95 0.96 0.97 0.98 0.99Accuracy
0
2000
4000
6000
8000
10000
12000
14000Min Training Set SizePowerBALD
SoftmaxBALDSoftrankBALD
BALD
(a) BALD
0.95 0.96 0.97 0.98 0.99Accuracy
0
2000
4000
6000
8000
10000
12000
14000Min Training Set SizePowerEntropy
SoftmaxEntropySoftrankEntropy
Entropy (b) Entropy
Figure 29: MIO-TCD (3 trials): Performance with all three stochastic strategies.
MIO-TCD. In Figure 29, we see that all three stochastic acquisition methods perform about equally well.
33Published in Transactions on Machine Learning Research (09/2023)
0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2500
5000
7500
10000
12500
15000Min Training Set SizeDataset = Under-Represented Groups
0.955 0.960 0.965 0.970 0.975 0.980 0.985Accuracy
0
2500
5000
7500
10000
12500
15000Dataset = Missing Synbols
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2500
5000
7500
10000
12500
15000Dataset = Spurious Correlations
PowerBALD
SoftmaxBALDSoftrankBALD
(a) BALD
0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2500
5000
7500
10000
12500
15000Min Training Set SizeDataset = Under-Represented Groups
0.955 0.960 0.965 0.970 0.975 0.980 0.985Accuracy
0
2500
5000
7500
10000
12500
15000Dataset = Missing Synbols
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2500
5000
7500
10000
12500
15000Dataset = Spurious Correlations
PowerEntropy
SoftmaxEntropySoftrankEntropy
(b) Entropy
Figure 30: Synbols edge cases (3 trials): Performance with all three stochastic strategies.
Synbols. In Figure 30, power acquisition seems to perform better overall—mainly due to the performance in
Synbols Missing Characters.
0.60 0.65 0.70 0.75 0.80 0.85Accuracy
1000
2000
3000
4000
5000
6000
7000
8000Min Training Set SizePowerEntropy
SoftmaxEntropy
SoftrankEntropy
Figure 31: CLINC-150 (10 trials): Performance with
all three stochastic strategies.
CLINC-150. In Figure 31, all three stochastic methods perform similarly.
D.2 Investigation
To further examine the three stochastic acquisition variants, we plot their score distributions, extracted from
the same MNIST toy example, in Figure 32. Power and softmax acquisition distributions are similar for
β= 8(power, softmax) and β= 4(softmax). This might explain why active learning with these βshows
similar accuracy trajectories.
We find that power and softmax acquisition are quite insensitive to βand thus selecting β= 1might generally
work quite well.
34Published in Transactions on Machine Learning Research (09/2023)
0.00000.00020.00040.00060.00080.0010Probability
1035
1029
1023
1017
1011
105
0.25
1.0
2.0
4.0
8.0
Distribution Type
Power
Softmax
Softrank
Figure 32: Score distribution for power and softmax acquisition of BALD scores on MNIST for varying
Coldnessβatt= 0.Linear and log plot over samples sorted by their BALD score. At β= 8both softmax
and power acquisition have essentially the same distribution for high scoring points (closely followed by the
power distribution for β= 4). This might explain why the coldness ablation shows that these βto have very
similar AL trajectories on MNIST. Yet, while softmax and power acquisition seem transfer to RMNIST, this
is not the case for softrank which is much more sensitive to β. At the same time, power acquisition avoids
low-scoring points more than softmax acquisition.
E Effect of changing the aquisition sizes
E.1 Repeated-MNIST
50
100
150
200
250
300Min Training Set SizeRepetition Factor = 1 | Acquisition Size = 5
50
100
150
200
250
300Repetition Factor = 1 | Acquisition Size = 10
50
100
150
200
250
300Repetition Factor = 1 | Acquisition Size = 20
50
100
150
200
250
300Repetition Factor = 1 | Acquisition Size = 40
50
100
150
200
250
300Min Training Set SizeRepetition Factor = 2 | Acquisition Size = 5
50
100
150
200
250
300Repetition Factor = 2 | Acquisition Size = 10
50
100
150
200
250
300Repetition Factor = 2 | Acquisition Size = 20
50
100
150
200
250
300Repetition Factor = 2 | Acquisition Size = 40
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Min Training Set SizeRepetition Factor = 4 | Acquisition Size = 5
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Repetition Factor = 4 | Acquisition Size = 10
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Repetition Factor = 4 | Acquisition Size = 20
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Repetition Factor = 4 | Acquisition Size = 40Acquisition Function
BALD
Uniform
BADGE
BatchBALD 5
SoftrankBALD
SoftmaxBALD
PowerBALD
Figure 33: Repeated-MNIST: Acquisition size ablation grouped by acquisition size. Stochastic acquisition
strategies (except for Softrank at acquisition size 5) always perform better than top- KBALD and also better
than BADGE. Softrank is the most sensitive to acquisition size changes because it is independent of the
scores.
35Published in Transactions on Machine Learning Research (09/2023)
0.6 0.7 0.8 0.9
50
100
150
200
250
300Min Training Set SizeAcquisition Function = PowerBALD | Repetition Factor = 1
0.6 0.7 0.8 0.9
50
100
150
200
250
300Acquisition Function = PowerBALD | Repetition Factor = 2
0.6 0.7 0.8 0.9
50
100
150
200
250
300Acquisition Function = PowerBALD | Repetition Factor = 4
0.6 0.7 0.8 0.9
50
100
150
200
250
300Min Training Set SizeAcquisition Function = SoftmaxBALD | Repetition Factor = 1
0.6 0.7 0.8 0.9
50
100
150
200
250
300Acquisition Function = SoftmaxBALD | Repetition Factor = 2
0.6 0.7 0.8 0.9
50
100
150
200
250
300Acquisition Function = SoftmaxBALD | Repetition Factor = 4
0.6 0.7 0.8 0.9
50
100
150
200
250
300Min Training Set SizeAcquisition Function = SoftrankBALD | Repetition Factor = 1
0.6 0.7 0.8 0.9
50
100
150
200
250
300Acquisition Function = SoftrankBALD | Repetition Factor = 2
0.6 0.7 0.8 0.9
50
100
150
200
250
300Acquisition Function = SoftrankBALD | Repetition Factor = 4
0.6 0.7 0.8 0.9
50
100
150
200
250
300Min Training Set SizeAcquisition Function = BADGE | Repetition Factor = 1
0.6 0.7 0.8 0.9
50
100
150
200
250
300Acquisition Function = BADGE | Repetition Factor = 2
0.6 0.7 0.8 0.9
50
100
150
200
250
300Acquisition Function = BADGE | Repetition Factor = 4
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Min Training Set SizeAcquisition Function = BALD | Repetition Factor = 1
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Acquisition Function = BALD | Repetition Factor = 2
0.6 0.7 0.8 0.9Accuracy
50
100
150
200
250
300Acquisition Function = BALD | Repetition Factor = 4Acquisition Size
5
10
20
40
Figure 34: Repeated-MNIST: Acquisition size ablation grouped by acquisition function. PowerBALD and
SoftmaxBALDarelesssensitivetoacquisitionsizechangeswhileSoftrankBALDismoresensitivetoacquisition
size changes. Overall, all three are much less sensitive to acquisition size changes than top- KBALD.
36Published in Transactions on Machine Learning Research (09/2023)
E.2 EMNIST
0.0 0.1 0.2 0.3 0.4 0.5
0
50
100
150
200
250
300Min Training Set Size = 1.0 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5
0
50
100
150
200
250
300 = 1.0 | Acquisition Function = BALD
0.0 0.1 0.2 0.3 0.4 0.5
0
50
100
150
200
250
300 = 1.0 | Acquisition Function = SoftmaxBALD
0.0 0.1 0.2 0.3 0.4 0.5
0
50
100
150
200
250
300Min Training Set Size = 4.0 | Acquisition Function = PowerBALD
0.0 0.2 0.4 0.6 0.8 1.0
0
50
100
150
200
250
300 = 4.0 | Acquisition Function = BALD
0.0 0.1 0.2 0.3 0.4 0.5
0
50
100
150
200
250
300 = 4.0 | Acquisition Function = SoftmaxBALD
0.0 0.1 0.2 0.3 0.4 0.5Accuracy
0
50
100
150
200
250
300Min Training Set Size = 8.0 | Acquisition Function = PowerBALD
0.0 0.2 0.4 0.6 0.8 1.0Accuracy
0
50
100
150
200
250
300 = 8.0 | Acquisition Function = BALD
0.0 0.1 0.2 0.3 0.4 0.5Accuracy
0
50
100
150
200
250
300 = 8.0 | Acquisition Function = SoftmaxBALD
Acquisition Size
10
20
40
Figure 35: EMNIST ‘Balanced’: Acquisition size ablation. For (top-K) BALD,βmakes no difference, so
β= 1is used as placeholder. The other two subplots are intentionally left out as they are very similar to this
one.
37Published in Transactions on Machine Learning Research (09/2023)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Min Training Set Size = 1.0 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300 = 1.0 | Acquisition Function = BALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300 = 1.0 | Acquisition Function = SoftmaxBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Min Training Set Size = 4.0 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300 = 4.0 | Acquisition Function = BALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300 = 4.0 | Acquisition Function = SoftmaxBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Accuracy
0
50
100
150
200
250
300Min Training Set Size = 8.0 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Accuracy
0
50
100
150
200
250
300 = 8.0 | Acquisition Function = BALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Accuracy
0
50
100
150
200
250
300 = 8.0 | Acquisition Function = SoftmaxBALD
Acquisition Size
10
20
40
Figure 36: EMNIST ‘ByMerge’: Acquisition size ablation. For (top-K) BALD,βmakes no difference, so
β= 1is used as placeholder. The other two subplots are intentionally left out as they are very similar to this
one.
38Published in Transactions on Machine Learning Research (09/2023)
E.3 ACS-FW
2000 4000 6000 8000 10000 12000 14000 160000.50.60.70.8AccuracyAcquisition Size = 75 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000.700.750.800.85Acquisition Size = 75 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 160000.50.60.70.8AccuracyAcquisition Size = 125 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000.700.750.800.85Acquisition Size = 125 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 160000.50.60.70.8AccuracyAcquisition Size = 250 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000.700.750.800.85Acquisition Size = 250 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 160000.50.60.70.8AccuracyAcquisition Size = 500 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000.700.750.800.85Acquisition Size = 500 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 160000.50.60.70.8AccuracyAcquisition Size = 1000 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000.700.750.80Acquisition Size = 1000 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 160000.50.60.70.8AccuracyAcquisition Size = 2000 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000.700.750.80Acquisition Size = 2000 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 16000
Training Set Size0.50.60.70.8AccuracyAcquisition Size = 4000 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 16000
Training Set Size0.700.750.80Acquisition Size = 4000 | Initial Training Set Size = 5000PowerBALD
SoftmaxBALD
SoftrankBALD
ACS-FW
BALD
Uniform
Figure 37: MFVI Last-Layer Classification on CIFAR-10: Acquisition size ablation.
39Published in Transactions on Machine Learning Research (09/2023)
2000 4000 6000 8000 10000 12000 14000 160000100002000030000WalltimeAcquisition Size = 75 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 16000050001000015000200002500030000Acquisition Size = 75 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 1600005000100001500020000WalltimeAcquisition Size = 125 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 1600005000100001500020000Acquisition Size = 125 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 16000020004000600080001000012000WalltimeAcquisition Size = 250 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000200040006000800010000Acquisition Size = 250 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 16000010002000300040005000WalltimeAcquisition Size = 500 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000100020003000400050006000Acquisition Size = 500 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 1600001000200030004000WalltimeAcquisition Size = 1000 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 160000100020003000Acquisition Size = 1000 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 1600005001000150020002500WalltimeAcquisition Size = 2000 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 1600005001000150020002500Acquisition Size = 2000 | Initial Training Set Size = 5000
2000 4000 6000 8000 10000 12000 14000 16000
Training Set Size0500100015002000WalltimeAcquisition Size = 4000 | Initial Training Set Size = 1000
6000 8000 10000 12000 14000 16000
Training Set Size05001000150020002500Acquisition Size = 4000 | Initial Training Set Size = 5000PowerBALD
SoftmaxBALD
SoftrankBALD
ACS-FW
BALD
Uniform
Figure 38: MFVI Last-Layer Classification on CIFAR-10: Acquisition size ablation (Walltime).
40Published in Transactions on Machine Learning Research (09/2023)
1000 2000 3000 4000 5000 6000 70000.750.800.850.90AccuracyAcquisition Size = 75 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 70000.840.860.880.900.92Acquisition Size = 75 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 70000.750.800.850.90AccuracyAcquisition Size = 125 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 70000.840.860.880.900.92Acquisition Size = 125 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 70000.750.800.850.90AccuracyAcquisition Size = 250 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 70000.840.860.880.900.92Acquisition Size = 250 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 70000.750.800.850.90AccuracyAcquisition Size = 500 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 70000.840.860.880.900.92Acquisition Size = 500 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 70000.750.800.850.90AccuracyAcquisition Size = 1000 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 70000.840.860.880.900.92Acquisition Size = 1000 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 7000
Training Set Size0.700.750.800.850.90AccuracyAcquisition Size = 2000 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 7000
Training Set Size0.840.860.880.90Acquisition Size = 2000 | Initial Training Set Size = 3000PowerBALD
SoftmaxBALD
SoftrankBALD
ACS-FW
BALD
Uniform
Figure 39: MFVI Last-Layer Classification on SVHN: Acquisition size ablation.
41Published in Transactions on Machine Learning Research (09/2023)
1000 2000 3000 4000 5000 6000 70000200040006000800010000WalltimeAcquisition Size = 75 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 70000200040006000Acquisition Size = 75 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 70000200040006000WalltimeAcquisition Size = 125 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 700001000200030004000Acquisition Size = 125 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 70000100020003000WalltimeAcquisition Size = 250 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 70000500100015002000Acquisition Size = 250 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 70000500100015002000WalltimeAcquisition Size = 500 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 700002004006008001000Acquisition Size = 500 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 7000025050075010001250WalltimeAcquisition Size = 1000 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 700002004006008001000Acquisition Size = 1000 | Initial Training Set Size = 3000
1000 2000 3000 4000 5000 6000 7000
Training Set Size02004006008001000WalltimeAcquisition Size = 2000 | Initial Training Set Size = 1000
3000 3500 4000 4500 5000 5500 6000 6500 7000
Training Set Size0200400600Acquisition Size = 2000 | Initial Training Set Size = 3000PowerBALD
SoftmaxBALD
SoftrankBALD
ACS-FW
BALD
Uniform
Figure 40: MFVI Last-Layer Classification on SVHN: Acquisition size ablation (Walltime).
42Published in Transactions on Machine Learning Research (09/2023)
25 50 75 100 125 150 175 200 2250.40.50.60.70.80.9AccuracyAcquisition Size = 25 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 3000.800.820.840.860.880.900.92Acquisition Size = 25 | Initial Training Set Size = 100
25 50 75 100 125 150 175 200 2250.40.50.60.70.80.9AccuracyAcquisition Size = 50 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 3000.8000.8250.8500.8750.9000.9250.950Acquisition Size = 50 | Initial Training Set Size = 100
25 50 75 100 125 150 175 200 225
Training Set Size0.40.50.60.70.80.9AccuracyAcquisition Size = 100 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 300
Training Set Size0.8000.8250.8500.8750.9000.925Acquisition Size = 100 | Initial Training Set Size = 100PowerBALD
SoftmaxBALD
SoftrankBALD
ACS-FW
BALD
Uniform
Figure 41: MFVI Last-Layer Classification on Repeated-MNIST: Acquisition size ablation.
43Published in Transactions on Machine Learning Research (09/2023)
25 50 75 100 125 150 175 200 225050010001500WalltimeAcquisition Size = 25 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 300050010001500Acquisition Size = 25 | Initial Training Set Size = 100
25 50 75 100 125 150 175 200 22502004006008001000WalltimeAcquisition Size = 50 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 30002004006008001000Acquisition Size = 50 | Initial Training Set Size = 100
25 50 75 100 125 150 175 200 225
Training Set Size0200400600WalltimeAcquisition Size = 100 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 300
Training Set Size020040060080010001200Acquisition Size = 100 | Initial Training Set Size = 100PowerBALD
SoftmaxBALD
SoftrankBALD
ACS-FW
BALD
Uniform
Figure 42: MFVI Last-Layer Classification on Repeated-MNIST: Acquisition size ablation (Walltime).
44Published in Transactions on Machine Learning Research (09/2023)
25 50 75 100 125 150 175 200 2250.40.50.60.7AccuracyAcquisition Size = 25 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 3000.680.700.720.740.760.78Acquisition Size = 25 | Initial Training Set Size = 100
25 50 75 100 125 150 175 200 2250.40.50.60.7AccuracyAcquisition Size = 50 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 3000.660.680.700.720.740.760.78Acquisition Size = 50 | Initial Training Set Size = 100
25 50 75 100 125 150 175 200 225
Training Set Size0.40.50.60.7AccuracyAcquisition Size = 100 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 300
Training Set Size0.680.700.720.740.76Acquisition Size = 100 | Initial Training Set Size = 100PowerBALD
SoftmaxBALD
SoftrankBALD
ACS-FW
BALD
Uniform
Figure 43: MFVI Last-Layer Classification on Fashion-MNIST: Acquisition size ablation.
45Published in Transactions on Machine Learning Research (09/2023)
25 50 75 100 125 150 175 200 225020040060080010001200WalltimeAcquisition Size = 25 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 300050010001500Acquisition Size = 25 | Initial Training Set Size = 100
25 50 75 100 125 150 175 200 225020040060080010001200WalltimeAcquisition Size = 50 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 30002004006008001000Acquisition Size = 50 | Initial Training Set Size = 100
25 50 75 100 125 150 175 200 225
Training Set Size0200400600WalltimeAcquisition Size = 100 | Initial Training Set Size = 20
100 125 150 175 200 225 250 275 300
Training Set Size0100200300400500600Acquisition Size = 100 | Initial Training Set Size = 100PowerBALD
SoftmaxBALD
SoftrankBALD
ACS-FW
BALD
Uniform
Figure 44: MFVI Last-Layer Classification on Fashion-MNIST: Acquisition size ablation (Walltime).
46Published in Transactions on Machine Learning Research (09/2023)
F Effect of changing β
F.1 Repeated-MNIST
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = PowerBALD 10 | Repetition Factor = 1
0
50
100
150
200
250
300Acquisition Function = PowerBALD 10 | Repetition Factor = 2
0
50
100
150
200
250
300Acquisition Function = PowerBALD 10 | Repetition Factor = 4
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = SoftmaxBALD 10 | Repetition Factor = 1
0
50
100
150
200
250
300Acquisition Function = SoftmaxBALD 10 | Repetition Factor = 2
0
50
100
150
200
250
300Acquisition Function = SoftmaxBALD 10 | Repetition Factor = 4
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = SoftrankBALD 10 | Repetition Factor = 1
0
50
100
150
200
250
300Acquisition Function = SoftrankBALD 10 | Repetition Factor = 2
0
50
100
150
200
250
300Acquisition Function = SoftrankBALD 10 | Repetition Factor = 4
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = PowerBALD 20 | Repetition Factor = 1
0
50
100
150
200
250
300Acquisition Function = PowerBALD 20 | Repetition Factor = 2
0
50
100
150
200
250
300Acquisition Function = PowerBALD 20 | Repetition Factor = 4
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = SoftmaxBALD 20 | Repetition Factor = 1
0
50
100
150
200
250
300Acquisition Function = SoftmaxBALD 20 | Repetition Factor = 2
0
50
100
150
200
250
300Acquisition Function = SoftmaxBALD 20 | Repetition Factor = 4
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = SoftrankBALD 20 | Repetition Factor = 1
0
50
100
150
200
250
300Acquisition Function = SoftrankBALD 20 | Repetition Factor = 2
0
50
100
150
200
250
300Acquisition Function = SoftrankBALD 20 | Repetition Factor = 4
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = PowerBALD 40 | Repetition Factor = 1
0
50
100
150
200
250
300Acquisition Function = PowerBALD 40 | Repetition Factor = 2
0
50
100
150
200
250
300Acquisition Function = PowerBALD 40 | Repetition Factor = 4
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = SoftmaxBALD 40 | Repetition Factor = 1
0
50
100
150
200
250
300Acquisition Function = SoftmaxBALD 40 | Repetition Factor = 2
0
50
100
150
200
250
300Acquisition Function = SoftmaxBALD 40 | Repetition Factor = 4
0.6 0.7 0.8 0.9Accuracy
0
50
100
150
200
250
300Min Training Set SizeAcquisition Function = SoftrankBALD 40 | Repetition Factor = 1
0.6 0.7 0.8 0.9Accuracy
0
50
100
150
200
250
300Acquisition Function = SoftrankBALD 40 | Repetition Factor = 2
0.6 0.7 0.8 0.9Accuracy
0
50
100
150
200
250
300Acquisition Function = SoftrankBALD 40 | Repetition Factor = 4
0.25
1.0
4.0
8.0
16.0
Figure 45: Repeated-MNIST: βablation for *BALD.
47Published in Transactions on Machine Learning Research (09/2023)
F.1.1 MIO-TCD and Synbols
0.95 0.96 0.97 0.98 0.99
0
2500
5000
7500
10000
12500
15000Min Training Set SizePowerBALD on MIO-TCD
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
0
2500
5000
7500
10000
12500
15000PowerBALD on Spurious Correlations
0.75 0.80 0.85 0.90 0.95 1.00
0
2500
5000
7500
10000
12500
15000PowerBALD on Under-Represented Groups
0.955 0.960 0.965 0.970 0.975 0.980 0.985
0
2500
5000
7500
10000
12500
15000PowerBALD on Missing Synbols
0.95 0.96 0.97 0.98 0.99
0
2500
5000
7500
10000
12500
15000Min Training Set SizeSoftmaxBALD on MIO-TCD
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
0
2500
5000
7500
10000
12500
15000SoftmaxBALD on Spurious Correlations
0.75 0.80 0.85 0.90 0.95 1.00
0
2500
5000
7500
10000
12500
15000SoftmaxBALD on Under-Represented Groups
0.955 0.960 0.965 0.970 0.975 0.980 0.985
0
2500
5000
7500
10000
12500
15000SoftmaxBALD on Missing Synbols
0.95 0.96 0.97 0.98 0.99Accuracy
0
2500
5000
7500
10000
12500
15000Min Training Set SizeSoftrankBALD on MIO-TCD
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2500
5000
7500
10000
12500
15000SoftrankBALD on Spurious Correlations
0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2500
5000
7500
10000
12500
15000SoftrankBALD on Under-Represented Groups
0.955 0.960 0.965 0.970 0.975 0.980 0.985Accuracy
0
2500
5000
7500
10000
12500
15000SoftrankBALD on Missing Synbols
0.125
1.0
8.0
Figure 46: MIO-TCD and Synbols: βablation for *BALD.
0.95 0.96 0.97 0.98 0.99
0
2500
5000
7500
10000
12500
15000Min Training Set SizePowerEntropy on MIO-TCD
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
0
2500
5000
7500
10000
12500
15000PowerEntropy on Spurious Correlations
0.75 0.80 0.85 0.90 0.95 1.00
0
2500
5000
7500
10000
12500
15000PowerEntropy on Under-Represented Groups
0.955 0.960 0.965 0.970 0.975 0.980 0.985
0
2500
5000
7500
10000
12500
15000PowerEntropy on Missing Synbols
0.95 0.96 0.97 0.98 0.99
0
2500
5000
7500
10000
12500
15000Min Training Set SizeSoftmaxEntropy on MIO-TCD
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
0
2500
5000
7500
10000
12500
15000SoftmaxEntropy on Spurious Correlations
0.75 0.80 0.85 0.90 0.95 1.00
0
2500
5000
7500
10000
12500
15000SoftmaxEntropy on Under-Represented Groups
0.955 0.960 0.965 0.970 0.975 0.980 0.985
0
2500
5000
7500
10000
12500
15000SoftmaxEntropy on Missing Synbols
0.95 0.96 0.97 0.98 0.99Accuracy
0
2500
5000
7500
10000
12500
15000Min Training Set SizeSoftrankEntropy on MIO-TCD
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2500
5000
7500
10000
12500
15000SoftrankEntropy on Spurious Correlations
0.75 0.80 0.85 0.90 0.95 1.00Accuracy
0
2500
5000
7500
10000
12500
15000SoftrankEntropy on Under-Represented Groups
0.955 0.960 0.965 0.970 0.975 0.980 0.985Accuracy
0
2500
5000
7500
10000
12500
15000SoftrankEntropy on Missing Synbols
0.125
1.0
8.0
Figure 47: MIO-TCD and Synbols: βablation for *Entropy.
48Published in Transactions on Machine Learning Research (09/2023)
F.2 EMNIST
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Min Training Set SizeAcquisition Size = 10 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Acquisition Size = 10 | Acquisition Function = SoftmaxBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Min Training Set SizeAcquisition Size = 20 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Acquisition Size = 20 | Acquisition Function = SoftmaxBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Accuracy
0
50
100
150
200
250
300Min Training Set SizeAcquisition Size = 40 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Accuracy
0
50
100
150
200
250
300Acquisition Size = 40 | Acquisition Function = SoftmaxBALD
1.0
4.0
8.0
Figure 48: EMNIST ‘Balanced’: βablation for Softmax- and PowerBALD.
49Published in Transactions on Machine Learning Research (09/2023)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Min Training Set SizeAcquisition Size = 10 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Acquisition Size = 10 | Acquisition Function = SoftmaxBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Min Training Set SizeAcquisition Size = 20 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0
50
100
150
200
250
300Acquisition Size = 20 | Acquisition Function = SoftmaxBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Accuracy
0
50
100
150
200
250
300Min Training Set SizeAcquisition Size = 40 | Acquisition Function = PowerBALD
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Accuracy
0
50
100
150
200
250
300Acquisition Size = 40 | Acquisition Function = SoftmaxBALD
1.0
4.0
8.0
Figure 49: EMNIST ‘ByMerge’: βablation for Softmax- and PowerBALD.
50Published in Transactions on Machine Learning Research (09/2023)
F.3 CausalBALD: synthetic dataset
0 50 100 150 200 250 300
Training Set Size0.51.01.52.02.5PEHE
Uniform
=0.01
=1
=4
=100
Top-K
(a) Overall Ablation (Subset)
0 50 100 150 200 250 300
Training Set Size0.51.01.52.02.5PEHE
=4
=10
=100
Top-K(b) Low Temperature Only
0 50 100 150 200 250 300
Training Set Size0.51.01.52.02.5PEHE
Uniform
=0.01
=0.2
=1
=2
=4
(c) High Temperature Only
Figure 50: CausalBALD: Synthetic Dataset. (a) At a very high temperature ( β= 0.1), PowerBALD behaves
very much like random acquisition, and as the temperature decreases the performance of the acquisition
function improves (lower√ϵPEHE). (b) Eventually, the performance reaches an inflection point ( β= 4.0) and
any further decrease in temperature results in the acquisition strategy performing more like top- K. We see
that under the optimal temperature, power acquisition significantly outperforms both random acquisition
and top-Kover a wide range of temperature settings.
We provide further βablations for CausalBALD on the entirely synthetic dataset which is used by Jesson
et al. (2021). This demonstrates the ways in which βinterpolates between uniform and top- Kacquisition.
F.4 CLINC-150
0.60 0.65 0.70 0.75 0.80 0.85Accuracy
2000
4000
6000
8000Min Training Set SizePowerEntropy
0.60 0.65 0.70 0.75 0.80 0.85Accuracy
2000
4000
6000
8000SoftmaxEntropy
0.60 0.65 0.70 0.75 0.80 0.85Accuracy
2000
4000
6000
8000SoftrankEntropy
0.125
1.0
8.0
Figure 51: Performance CLINC-150: βablation for *Entropy.
G Simple Implementation of Stochastic Batch Acquisition
51Published in Transactions on Machine Learning Research (09/2023)
def get_random_samples(scores_N: torch.Tensor, *, aquisition_batch_size: int):
N = len(scores_N)
aquisition_batch_size = min(aquisition_batch_size, N)
indices = np.random.choice(N, size=aquisition_batch_size, replace= False )
return indices.tolist()
def get_softmax_samples(scores_N: torch.Tensor, *, beta: float, aquisition_batch_size: int):
# As beta -> 0, we obtain random sampling.
ifbeta == 0.0:
return get_random_samples(scores_N, aquisition_batch_size=aquisition_batch_size)
N = len(scores_N)
noised_scores_N = scores_N + scipy.stats.gumbel_r.rvs(loc=0, scale=1 / beta, size=N, random_state= None )
return torch.topk(noised_scores_N, k=aquisition_batch_size)
def get_power_samples(scores_N: torch.Tensor, *, beta: float, aquisition_batch_size: int):
return get_softmax_samples(torch.log(scores_N), beta=beta, aquisition_batch_size=aquisition_batch_size)
def get_softrank_samples(scores_N: torch.Tensor, *, beta: float, aquisition_batch_size: int):
N = len(scores_N)
sorted_indices_N = torch.argsort(scores_N, descending= True )
ranks_N = torch.argsort(sorted_indices_N) + 1
return get_power_samples(1 / ranks_N, beta=beta, aquisition_batch_size=aquisition_batch_size)
Listing 1: Code for stochastic batch acquisition. Colab here.
52Published in Transactions on Machine Learning Research (09/2023)
H Hypoexponential Distribution Evaluation
import numpy as np
from scipy.stats import expon, gumbel_r
import matplotlib.pyplot as plt
# Set the rate parameter
# Change here to get different scales of exponentials
lam = 1/2
# Generate 10 samples from the exponential distribution
samples = expon.rvs(scale=1/lam, size=5000)
# Now sample from the samples using the previous samples as lambda of exponential distributions
new_samples = expon.rvs(scale=1/samples, size=(4000,5000))
what_are_you = new_samples.mean(axis=1)
# Fit a Gumbel distribution to the data
params = gumbel_r.fit(what_are_you)
x = np.linspace(np.min(what_are_you), np.max(what_are_you), 1000)
gumbel_cdf = gumbel_r.cdf(x, *params)
# Plot both what_are_you and the Gumbel distribution in the same plot
plt.plot(np.sort(what_are_you), np.linspace(0, 1, len(what_are_you), endpoint= False ), label="Simulated Hypoexponential")
plt.plot(x, gumbel_cdf, label="Gumble Fit")
plt.legend()
plt.show()
3 4 5 6 70.00.20.40.60.81.0 Simulated Hypoexponential
Gumble Fit
Listing 2: Code for fitting a Gumbel distribution to the mean of samples from a hypoexponential distribution.
Colab here.
53