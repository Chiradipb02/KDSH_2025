Under review as submission to TMLR
Spectral State Space Models
Anonymous authors
Paper under double-blind review
Abstract
This paper studies sequence modeling for prediction tasks with long range dependencies. We
propose a new formulation for state space models (SSMs) based on learning linear dynamical
systems with the spectral filtering algorithm (Hazan et al., 2017). This gives rise to a novel
sequence prediction architecture we call a spectral state space model.
Spectral state space models have two primary advantages. First, they have provable robust-
ness properties as their performance depends on neither the spectrum of the underlying
dynamics nor the dimensionality of the problem. Second, these models are constructed with
fixed convolutional filters that do not require learning while still outperforming SSMs in
both theory and practice.
The resulting models are evaluated on synthetic dynamical systems and long-range prediction
tasks of various modalities. These evaluations support the theoretical benefits of spectral
filtering for tasks requiring very long range memory.
1 Introduction
Handling long-range dependencies efficiently remains a core problem in sequence prediction/modelling.
Recurrent Neural Networks (RNN) (Hopfield, 1982; Rumelhart et al., 1985; Elman, 1990) are a natural choice,
but are notoriously hard to train; they often suffer from vanishing and exploding gradients (Bengio et al.,
1994; Pascanu et al., 2013) and despite techniques to mitigate the issue (Hochreiter & Schmidhuber, 1997;
Cho et al., 2014; Arjovsky et al., 2016), they are also hard to scale given the inherently sequential nature of
their computation.
In recent years, transformer models (Vaswani et al., 2017) have become the staple of sequence modelling,
achieving remarkable success across multiple domains (Brown et al., 2020; Dosovitskiy et al., 2020; Jumper
et al., 2021). Transformer models are naturally parallelizable and hence scale significantly better than RNNs.
However, attention layers have memory/computation requirements that scale quadratically with context
length. Many approximations have been proposed (see Tay et al. (2022) for a recent survey).
RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise
in modelling long sequences across varied modalities (Gu et al., 2021a; Dao et al., 2022; Gupta et al., 2022;
Orvieto et al., 2023; Poli et al., 2023; Gu & Dao, 2023). SSMs use linear dynamical systems (LDS) to model
the sequence-to sequence transform by evolving the internal state of a dynamical system according to the
dynamics equations
xt=Axt−1+Butyt=Cxt+Dut.
Herext∈Rdis the hidden state of the dynamical system, utis the input to the system, and ytare observations.
The matrices A,B,C,D govern the evolution of the system and are called system matrices. Despite its
simplicity, this linear model can capture a rich set of natural dynamical systems in engineering and the
physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also
attractive as a sequence model because their structure is amenable to both fast inference and fast training
via parallel scans (Blelloch, 1989; Smith et al., 2023) or convolutions (Gu et al., 2021a). A rich literature
stemming from control theory and recent machine learning interest has given rise to efficient techniques
for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent
1Under review as submission to TMLR
literature see (Hazan & Singh, 2022). These techniques make SSMs attractive for sequence tasks which
inherently depend on long contexts that scale poorly for transformers. Examples include large language
models (Dao et al., 2022), modelling time series (Zhang et al., 2023), and audio generation (Goel et al., 2022).
To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now
proceed to delineate how past states and inputs affect the future.
Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and
imply that in a noiseless environment, the t’th output can be written as
yt=Cxt+Dut=C(Axt−1+But) +Dut=...=t−1/summationdisplay
i=0CAiBut−i+Dut
The matrix Ais asymmetric in general, and can have complex eigenvalues. If the amplitude of these
eigenvalues is >1, then the output ytcan grow without bounds. This is called an “explosive" system. In a
well-behaved system, the eigenvalues of Ahave magnitude≤1. If the magnitudes are bounded away from 1,
say|λi(A)|≤1−δ, for someδ>0(referred to as spectral gap), then we can write
yt=l/summationdisplay
i=0CAiBut−i+ωl,∥ωl∥≤ϵ
forl=O(1
δlog1
ϵ). This mathematical fact implies that the effective memory lof the system is on the order
of1
δ, where the input affects the system minimally after ltime steps. In general, the parameter δis unknown
apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to
instability in training linear dynamical systems with a long context. This issue is specifically highlighted in
the work of Orvieto et al. (2023) who observe that on long range tasks learning an LDS directly does not
succeed and requires interventions such as stable exponential parameterizations and specific normalization
which have been repeatedly used either implicitly or explicitly in the SSM literature (Gu et al., 2021a).
Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this
limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be
circumvented via a significant increase in sample complexity (Ghai et al., 2020) or via control over the input
sequence (Simchowitz et al., 2018).
Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of
linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the
technique of spectral filtering (Hazan et al., 2017). The idea is to project the sequence of inputs to a small
subspace that is constructed using special structure of discrete LDS where successive powers of the system
matrix appear in the impulse response function. The basic idea is to represent the output as
yt=k/summationdisplay
j=1Mj/parenleftigg/summationdisplay
iϕj(i)·ut−i/parenrightigg
,
whereϕjarespectral filters which are sequence-length sized vectors that given the target sequence length
can be computed offline, and Mjare matrices parameterizing the model. These spectral-filters are the
eigenvectors of the matrix constructed as the average of outer products of the discrete impulse-response
functions, viz Z=/integraltext1
0[1,α,α2...][1,α,α2...]⊤dα. It is shown that this matrix is inherently low-dimensional
and for allα∈[0,1], vectors of the form [1,α,α2...]are well approximated by the top-eigenspace of Z. Figure
1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2.
Why is spectral filtering important? The main advantages of spectral filtering are representational
efficiency and stability of learning. Consider the naive parameterization of learning a mapping from the
system input to output. If we attempt to learn the dynamics matrices A,B,C,D explicitly, the problem
becomes non-convex due to the terms CAiB. On the other hand, if we use improper learning and learn
the linear map from ut−itoytseparately for each i, then1
δmatrices are required to express this mapping
where1
δis the effective memory of the system. With spectral filtering, for linear dynamical systems with
2Under review as submission to TMLR
symmetric matrices A, the number of filters (and thus the number of matrix parameters) required to represent
an observation is independent of the spectral gap parameter δ!Moreover, these parameters can be
learned stably via regression, as spectral filtering is a convex relaxation of the original non-convex problem.
These guarantees indicate that if we featurize the input into the spectral basis, we can potentially design
models that are capable of efficiently and stably representing systems with extremely long memory even with
δ→0. These striking properties motivate our derivation of the recurrent spectral architecture, and is the
underlying justification for the performance and training stability gains we see in experiments.
Figure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain.
1.1 Our Contributions
We start by proposing state space models with learned components that apply spectral filtering for their
featurization. We consider two types of spectral filters, which augment the original spectral filters proposed
in Hazan et al. (2017) with negative eigenvalues in two different ways. Our main contribution is a neural
architecture that is based on these spectral state space models. This neural architecture can be applied
recursively in layers, resulting in an expressive architecture for modeling sequential data.
Finally we implement this neural architecture and apply it towards synthetically generated data as well as
the Long Range Arena benchmark (Tay et al., 2021). We demonstrate that spectral state space models can
stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need
for exponential parameterizations, particular initializations and normalizations.
Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling,
surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM
aretheoretically-founded and fixed and thus parameter-free. In addition, our models are provably as
expressive as an LDS. In particular, their expressiveness neither depends on the spectral gap nor on the
dimension of the system, which are necessary in all other methods.
1.2 Related work
Due to limited space, we provide a short overview of the most related work to us below and provide a detailed
report on the related work in the appendix (Section A).
State space models. SSMs for learning long range phenomenon have received much attention in the
deep learning community in recent years starting with the works (Gu et al., 2020),(Gu et al., 2021b) which
propose and develop the HiPPO theory. Gu et al. (2021a) develop the S4 parameterization to address the
bottlenecks of training efficiency, performance and numberical stability. The S4 parameterization restricts
3Under review as submission to TMLR
the system matrices Ato be normal plus low-rank, allowing for stable diagonalization. The S4 model was
further streamlined in later works, viz. using diagonal system matrices without a loss in performance (Gupta
et al., 2022) and the S5 model (Smith et al., 2023) which uses a MIMO diagonal system and associative scans
for computational efficiency. Orvieto et al. (2023) investigate whether simpler deep Linear Recurrent Units
(LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat
that specific modifications on linear RNNs, namely the stable exponential parameterization, γ- normalization
and ring initialization, are necessary to learn on certain challenging long-context modeling tasks. We discuss
the details of this ablation in the appendix (Section A.1).
Spectral filtering. The technique of spectral filtering (Hazan et al., 2017) was developed as a convex
improper learning alternative to directly parameterizing an LDS (as in the case of SSMs) leading to an
efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based
methods (eg. SSMs) that aim to identify the system dynamics, spectral filtering’s guarantee does not depend
on the stability of the underlying system, and is the first method to obtain condition number-free regret
guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in Hazan
et al. (2018).
Convolutional Models for Sequence Modeling. Exploiting the connnection between LDS and convolu-
tions (Gu et al., 2021a), various convolutional models have been proposed for sequence modelling. (Fu et al.,
2023) employ direct learning of convolutional kernels but find that they underperform SSMs, identifying
non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations.
(Li et al., 2022) identifies two key characteristics of convolutions to be crucial for long range modelling,
decay in filters and small number of parameters parameterizing the kernel. (Shi et al., 2023) propose a
multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis.
All these methods parameterize the kernels with specific structures and/or add further regularizations to
emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby
parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel.
Furthermore our kernels are provably at least as expressive as linear dynamical systems capable of directly
capturing and improving the performance of SSMs without the need for specific initializations. Naturally our
kernels (see Fig 1) by default satisfy both the smoothness and the decay condition identified (and explicitly
enforced) by Li et al. (2022) and Fu et al. (2023).
2 Preliminaries
Sequence prediction. We treat sequence prediction as a game between a predictor/learner and nature
in which iteratively at every time t∈[L], the learner is presented an input ut∈Rdin. The learner Athen
produces a candidate output ˆyt=ˆyt(A), and nature reveals the tthelement of a target sequence yt∈Rdout.
The learner then suffers an instantaneous loss of ∥yt−ˆyt∥2. The task of the learner is to minimize regret
over a benchmark set of learning algorithms A, defined as follows
Regret =L/summationdisplay
t=1∥yt−ˆyt∥2−min
A∈AL/summationdisplay
t=1∥yt−ˆyt(A)∥2.
Linear Dynamical Systems (LDS): An example benchmark set of methods is that of a linear dynamical
system, which has four matrix parameters, A∈RN×N,B∈RN×din,C∈Rdout×N,D∈Rdout×din. The system
evolves and generates outputs according to the following equations
xt≜Axt−1+But, ˆyt≜Cxt+Dut (1)
Thus, an example class of benchmark algorithms Aare all predictors that generate ˆytaccording to these
rules, for a fixed set of matrices A,B,C,D .
Spectral Filtering: Another important set of predictors is one which is inspired by spectral filtering
(Hazan et al., 2017). The spectral filtering theory builds an efficient representation for all vectors in the range
4Under review as submission to TMLR
of the function µ: [0,1]→RLdefined asµ(α)≜(α−1)[1,α,α2...]. To build this representation, for any L
define the following Hankel matrix Z∈RL×Lwhose entries are given by
Z[i,j]≜2
(i+j)3−(i+j)
It is shown in the appendix (see Lemma C.1) that Z=/integraltext1
0µ(α)µ(α)⊤dα. Thus it can be seen that Zis a real
PSD Hankel matrix. It is known (see Lemma C.4 in the appendix) that real PSD Hankel matrices have an
exponentially decaying spectrum. As a result, the crux of the spectral filtering theory, lies in showing that for
allα∈[0,1]1, the vector µ(α)is approximately contained in the subspace spanned by the top eigenvectors of
Z, making the subspace spanned by top-eigenvectors of Z a very efficient subspace to project the input into.
This fact is formalized as Lemma C.3 in the appendix. We now use this intuition to describe the Spectral
Filtering algorithm.
Since Z is a real PSD matrix, it admits a real spectral decomposition, and the (non-negative) eigenvalues
can be easily ordered naturally by their value. Let {(σj∈R,ϕj∈RT)}L
j=1be the eigenvalue-eigenvector
pairs ofZordered to satisfy σ1≥σ2≥...≥σd. We consider a fixed number Kof the above eigenvectors.
Algorithms in the spectral filtering class generate ˆytas follows. For each k∈K, we first featurize the input
sequence by projecting the input sequence until time tonϕk, leading to a sequence Ut,k∈Rdindefined as
Ut,k=t/summationdisplay
i=1ut−i·ϕk(i).
The spectral filtering class is further parameterized by matrices Mu
1∈Rdout×din,Mu
2∈Rdout×dinand a set of
matricesMϕ
1,...,Mϕ
K∈Rdout×din. The output at time tis then given by
ˆyt= ˆyt−1+Mu
1ut+Mu
2ut−1+K/summationdisplay
k=1Mϕ
kUt,k. (2)
Note that given an input sequence u1:Lfor anyk, thedin×TmatrixU1:L,kcan be efficiently computed via
convolutions along the time dimension Lin total time O(din·Llog(L)). The following theorem (proved in
Hazan et al. (2017)) establishes that the spectral filtering class of predictors approximately contains bounded
linear dynamical systems with positive semi-definite A. Here we denote by ∥W∥colthe maximum column
norm of a matrix W, and byyLDS
tandySF
tthe outputs of the LDS and Spectral Filtering (via equation 1
and equation 2), respectively.
Theorem 2.1. Consider any linear dynamical system with a PSD transition matrix A, such that∥A∥≤1,
and any sequence u1:Lsatisfying∥ut∥≤a. Then there exists a set of matrices Mu
1,Mu
2,Mϕ
1,...,Mϕ
Ks.t.
Spectral Filtering can approximate the output of the LDS with the following guarantee:
∥yLDS
t−ySF
t∥2≤c·∥B∥col·∥C∥col·L3·a·e−/parenleftbig
π2
4·K
log(L)/parenrightbig
wherec≤106is a universal constant.
We do not provide a proof for this theorem which can be found in Hazan et al. (2017)2. Instead, in the next
section we provide a generalization of this theory to cover all symmetric matrices and not just PSD matrices
and prove a more general theorem (Theorem 3.1). We further build upon this generalization to create a
sequence to sequence prediction unit.
3 Spectral Transform Unit (STU)
In this section we use Spectral Filtering to create a sequence to sequence neural network layer, i.e. given an
input sequence{u1...uL}∈Rdin, it produces an output sequence {y1...yL}∈Rdout.
1in particular all αclose to 1, representing marginally stable systems.
2Note that Hazan et al. (2017) consider a simpler setting where in the ground truth ytis available to the learner for all future
time steps. We do not make such an assumption and theorems have been adjusted to suffer an additional Lfactor in the error
as a result.
5Under review as submission to TMLR
Figure 2: Schematic showing the spectral projection of a 1-dimensional input sequence and how these features
are used to produce the spectral component in the STU output equation 4. In the multi-dimensional case the
operation is applied in parallel across every input dimension.
A single layer of STU (depicted in Figure 2) is parameterized by a number K, denoting the number of
eigenvectors and matrices Mϕ+
1...Mϕ+
K,Mϕ−
1...Mϕ−
K∈Rdout×din,andMu
1,Mu
2,Mu
3∈Rdout×din. The
matrices form the paramsof the layer. Further recall the Hankel matrix Z∈RL×Lwhose entries are given by
Z[i,j]≜2
(i+j)3−(i+j). (3)
and let{(σj∈R,ϕj∈RL)}L
j=1be the eigenvalue-eigenvector pairs of Zordered to satisfy σ1≥σ2≥...σd.
Given an input sequence {u1...uL}∈Rdin, we first featurize the input sequence as follows. For any t,k,
we begin by projecting the input sequence till time tonfixedfiltersϕk, leading to two feature vectors
U+
t,k,U−
t,k∈Rdindefined as
U+
t,k=t−1/summationdisplay
i=0ut−i·ϕk(i)U−
t,k=t−1/summationdisplay
i=0ut−i·(−1)i·ϕk(i).
Note that for every k, the sequence of features U1:L,kcan be computed efficiently via convolution. The output
sequence{y1···yL}is then given by
ˆyt= ˆyt−2+3/summationdisplay
i=1Mu
iut+1−i
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Auto−regressive Component+K/summationdisplay
k=1Mϕ+
kσ1/4
kU+
t−2,k+K/summationdisplay
k=1Mϕ−
kσ1/4
kU−
t−2,k
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Spectral Component. (4)
The above output contains a small auto-regressive component that essentially allows for stable learning of the
spectral component as the memory grows. The differences from the original spectral filtering class equation 2
are the introduction of a negative part in the spectral component and the slight change in the auto-regressive
component. Both of these changes are necessitated by the requirement to capture negative eigenvalues of
A. Note that equation 4 corresponds to the specification of the algorithm presented in Hazan et al. (2018),
when the eigenvalues are known to be real numbers. For completeness and ease of discourse we prove the
following representation theorem in the Appendix which shows that the above class approximately contains
any marginally-stable LDS with symmetric A.3As before, let∥M∥coldenote the maximum column norm of
a matrixM.
Theorem 3.1. Consider any linear dynamical system with a symmetric matrix Awhere∥A∥≤1, and any
sequenceu1:Lsatisfying∥ut∥≤a. Then there exists matrices Mu
1,Mu
2,Mu
3,Mϕ+
1...Mϕ+
K,Mϕ−
1...Mϕ−
K∈
3We discovered some small but easily fixable errors in the original proof of Hazan et al. (2017) which we have corrected in our
proof
6Under review as submission to TMLR
(a)
 (b)
Figure 3: Learning dynamics for learning a marginally stable LDS. (a.)(Smoothed) Learning curves for a
single STU layer (red) vs a single LRU layer (black). The learning rate was tuned for both models. See
Appendix for a detailed discussion of the tuning and sensitivity to hyperparameters for both the models.
Curiously at stable LRs we observe that LRUs show a plateauing of learning. (b.) Error (in log-scale)
obtained by the single STU layer as a function of the model parameter ’K’. We observe an exponential drop
in the reconstruction loss as predicted by the analysis.
Rdout×din, such the output of the LDS (via equation 1) can be approximated by Spectral Filtering (via equation 4)
with the following guarantee:
∥yLDS
t−ySF
t∥2≤c·∥B∥col·∥C∥col·L3·a·e−/parenleftbig
π2
4·K
log(L)/parenrightbig
wherec≤2×106is a universal constant.
The above theorem in particular ensures for any sequence length Lthat setting K =
O/parenleftig
log(L) log/parenleftig
∥B∥col·∥C∥col·L·a
ϵ/parenrightig/parenrightig
we get there exists a spectral filtering model with Kfilters that can
approximate any LDS upto an error of ϵ. Note that the requirement on the number of filters grows logarith-
mically inL, highlighting the efficiency of the representation. The proof of the above theorem is provided in
the appendix along with an alternative version of spectral filtering using slightly modified filters which also
provide the same guarantee.
Remark 3.2. Comparing Theorem 3.1 (our contribution) and Theorem 2.1 (Theorem 1 from Hazan et al.
(2017)), we note firstly that our theorem holds for symmetric matrices and not just PSD matrices. Hazan
et al. (2017) allude to a direct extension for the symmetric case which we believe is not fully correct. We use
a similar idea to prove this theorem. Secondly a minor difference is that in the sequential prediction setting
the prediction is auto-regressive, i.e. uses its own yto make the future predictions.
Due to space limitations, we discuss the runtime scaling of our method and compare it with different methods
in the appendix (Section A).
3.1 Experiment: Learning a marginally-stable LDS
We provide a simple synthetic evaluation of the stability and training efficiency afforded by the STU. We
consider a low-dimensional linear system A,B,C,D generated as follows. B∈R4×3,C∈R3×4are matrices
with iid unit Gaussian entries. Dis a diagonal matrix with iid unit Gaussian entries and Ais a diagonal
matrix with Aii∼0.9999∗ZwhereZis a random sign. By design this is a system with a very high stability
constant (∼104). As a training dataset we generated {(ui,yi)}whereuiis a random input sequence and yi
is the output generated by applying the linear dynamical system on ui. We perform mini-batch (batch size 1)
training with the l2 loss. As comparison we perform the same procedure with an LRU (Linear Recurrent
Unit) layer as proposed by Orvieto et al. (2023) which directly parameterizes the linear system. The results
of the training loss as seen by the two systems are presented in Figure 3a.
7Under review as submission to TMLR
(a) Schematic displaying a multi-layer STU model.Model Specification Pathfinder PathX
STU Eqn equation 4 (K=16) 91.8 89.5
LRU Dense A ✗ ✗
ΛExp. Param. 65.4 ✗
ΛStable Exp. 93.5 ✗
+ Ring Init. 94.4 ✗
+γ-Norm. 95.1 94.2
(b) Comparison of the basic stacked STU model against
LRU ablations in Orvieto et al. (2023)
We use all the initialization/normalization techniques as recommended by Orvieto et al. (2023) for LRU
including the stable exponential parameterization ,γ-normalization andring-initialization . Indeed we find
that all these tricks were necessary to learn this system at all. We provide more details about the ablations
and other hyperparameter setups in the appendix. We observe that the STU is significantly more efficient
at learning the LDS as opposed to the LRU. We further find that there is a wide range of LRs where the
STU has a stable optimization trajectory and the loss decreases continuously highlighting the advantages of a
convex parameterization. On the other hand, LRU is able to eventually learn the system at the right learning
rates, it requires almost 8x the number of samples to get to a system with non-trivial accuracy. More details
can be found in the appendix. Curiously we observe that for the LRU training plateaus completely for the
first 50% of training highlighting the difficulty of optimization via a non-convex landscape.
The STU layer in the previous experiment employs K= 25. In Figure 3b we plot the performance of STU at
various levels of K. As predicted by the theory we observe an exponential decay in the error as Kincreases
with the error effectively plateauing after K≥15.
4 Stacked STU
To increase the representation capacity and to maintain the efficiency of prediction through linear dynamical
systems, proposed models in the SSM literature take the form of stacking these sequence to sequence
transforms into multiple layers. Non-linearities in the model can then be introduced by sandwiching them as
layers lying in between these sequence to sequence transforms.
In this paper we closely follow the stacking approach followed by Orvieto et al. (2023), replacing the LRU
layers appropriately by STU layers. A schematic for the resultant multi-layer model is displayed in Figure
4a. In a nutshell, the input sequence is first embedded via a time-invariant embedding function followed
by multiple repetitions of alternating STU layers and non-linearities (in particular we use GLU). Finally
the resulting output is time-pooled followed by a final readout layer according to the task at hand. This
composite model can now be trained in a standard fashion via back-propagation and other commonly used
deep-learning optimization techniques.
4.1 Experiments on Long Range Arena (Tay et al., 2021)
We evaluate the stacked STU model on the Long Range Arena (LRA) benchmark (Tay et al., 2021). This
benchmark aims to assess the performance of sequence prediction models in long-context scenarios and
consists of six tasks of various modalities, including text and images. The context length for the tasks ranges
from 1K to 16K, and the tasks require capabilities such as hierarchical reasoning, matching and retrieval, and
visual-spatial understanding. SSMs (Gu et al., 2021a) have shown significantly superior performance on most
of the tasks compared to Transformer architectures. In particular for the hardest task in the suite, PathX
(image classification with context length of 16K), no transformer model has been able to achieve accuracy
beyond random guessing. We provide the evaluation of the stacked STU model on the two hardest tasks
namely PathFinder and PathX in Table 4b.
8Under review as submission to TMLR
CIFAR ListOps TextRetrieval Pathfinder PathX
S4 (Gu et al., 2021a) 88.65 59.60 86.82 90.90 94.20 96.35
LRU (Orvieto et al., 2023) 89 60.2 89.4 89.9 95.1 94.2
AR-STU 91.34 61.14 90.47 90.52 95.45 93.24
Table 1: Comparison of the STU model against various proposed SSM models on the LRA benchmark. We
report the median over 5 trials for our experiments.
We compare our performance against the ablation carried out by Orvieto et al. (2023) who find that ring
initialization, stable exponential parameterization and γ-normalization are all crucial towards learning these
tasks. In particular as reported by Orvieto et al. (2023) all three of the above interventions were necessary to
learn on PathX to any non-trivial accuracy. This is a result of the much larger context length of 16K employed
by the PathX task. On the other hand we find that the the stacked STU (with the STU component exactly
as represented by equation 4) is sufficient to learn on both these tasks to relatively high accuracies. Notably
we do not require any other normalizations or initialization techniques We initialize all the parameters of
the STU i.e. M matrices to 0. Details about our implementation as well as details about the experiments
including hyperparameters can be found in the appendix (Section E. This result in particular confirms and
highlights the theoretical stability afforded by the STU even under learning tasks involving large sequence
lengths. In the appendix (Table 2 we provide the performance evalaution of the stacked STU on all tasks of
the LRA benchmark.
In the next section we highlight a simple technique towards significantly improving the achieved accuracy for
the stacked STU model.
5 Hybrid Temporal and Spectral Units
A simple extension to the STU model (Equation equation 4) is to parameterize the dependence of ytonyt−2
with a parameter My, leading to the following prediction model
ˆyt=Myˆyt−2+3/summationdisplay
i=1Mu
iut+1−i
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Auto−regressive Component+K/summationdisplay
k=1Mϕ+
kσ1/4
kU+
t−2,k+K/summationdisplay
k=1Mϕ−
kσ1/4
kU−
t−2,k
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Spectral Component. (5)
SettingMy=Iwe recover the guarantees afforded by Theorem 3.1 and thus the above model is strictly
more powerful. We find that the above change leads to significant improvements over the accuracy achieved
by the simple STU model. We can further extend the auto-regression to depend on multiple previous y
as opposed to just yt−2. Indeed as the following theorem shows adding sufficiently long auto-regression is
powerful enough to capture any LDS.
Theorem 5.1. Given an LDS parameterized by A∈Rd×d,B,C,D, there exist coefficients α1:dand matrices
Γ0:dsuch that given any input sequence u1:L, the output sequence y1:Lgenerated by the action of the LDS on
the input satisfies for all t
yt=d/summationdisplay
i=1αiyt−i+d/summationdisplay
i=0Γiut−i
This is a well-known observation and we provide a proof in the appendix (Section F). Motivated by the
above theorem we propose a generalization of STU, which we call AR-STU, to add auto-regression over the
9Under review as submission to TMLR
previously produced outputs. In particular given a parameter kywe define AR-STU as
ˆyt=ky/summationdisplay
i=1My
iˆyt−i+3/summationdisplay
i=1Mu
iut+1−i
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
Auto−regressive Component+K/summationdisplay
k=1Mϕ+
kσ1/4
kU+
t−2,k+K/summationdisplay
k=1Mϕ−
kσ1/4
kU−
t−2,k
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Spectral Component. (6)
In Table 1, we evaluate the performance of AR-STU on Long Range Arena. In our experiments we search
over two values of ky={2,32}. For non-image tasks, ListOps, Text and Retrieval, we find that setting
ky= 2is sufficient to get optimal results. For the image tasks, CIFAR, Pathfinder and PathX, we found that
ky= 32led to significant performance gains. A performance ablation over this parameter can be found in the
appendix (Table 2). Overall we find that the STU model provides improvements over baselines such as S4 and
LRU on 4 out of the 6 tasks and performs comparably to the best baseline on the others. Remarkably, the
STU layers come with provable guarantees and thus performs well out of the box without the need for specific
initializations, discretizations or normalizations. We initialize all parameters My
i,Mu
i,Mϕ+
k,Mϕ−
kwith 0.
We provide details of the experimental setup, including hyperparameter tuning in the appendix (Section E).
6 Conclusion
Insprired by the success of SSMs, we present a new theoretically-founded deep neural network architecture,
Spectral SSM, for sequence modelling based on the Spectral Filtering algorithm for learning Linear Dynamical
Systems. The SSM performs a reparameterization of the LDS and is guaranteed to learn even marginally
stable symmetric LDS stably and efficiently. We demonstrate the core advantages of the Spectal SSM, viz.
robustness to long memory through experiments on a synthetic LDS and the Long Range Arena benchmark.
We find that the Spectral SSM is able to learn even in the presence of large context lengths/memory without
the need for designing specific initializations, discretizations or normalizations which were necessary for
existing SSMs to learn in such settings. While spectral SSMs only model symmetric A, our presented set of
experiments on the LRA benchmark suggest that the gap between symmetric and general A is potentially
small in real world tasks. Indeed more recent SSM models like Gu & Dao (2023); De et al. (2024) work
with real diagonals (i.e. symmetric case) as they do not find evidence that adding complex eigenvalues help.
Spectral filtering has been extended in certain settings to asymmetric A (Hazan et al., 2018) and a similar
extension to our proposal is straightforward but comes with efficiency losses and we leave it to future work.
References
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International conference on machine learning , pp. 1120–1128. PMLR, 2016.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent
is difficult. IEEE transactions on neural networks , 5(2):157–166, 1994.
Guy E Blelloch. Scans as primitive parallel operations. IEEE Transactions on computers , 38(11):1526–1538,
1989.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. arXiv preprint arXiv:1406.1078 , 2014.
Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. Hungry hungry
hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052 , 2022.
10Under review as submission to TMLR
Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu,
Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear
recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427 , 2024.
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Jeffrey L Elman. Finding structure in time. Cognitive science , 14(2):179–211, 1990.
Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra,
and Christopher Ré. Simple hardware-efficient long convolutions for sequence modeling. arXiv preprint
arXiv:2302.06646 , 2023.
Udaya Ghai, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. No-regret prediction in marginally stable
systems. In Conference on Learning Theory , pp. 1714–1757. PMLR, 2020.
Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It’s raw! audio generation with state-space
models. In International Conference on Machine Learning , pp. 7616–7633. PMLR, 2022.
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint
arXiv:2312.00752 , 2023.
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with
optimal polynomial projections. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems , volume 33, pp. 1474–1487. Curran Associates, Inc.,
2020.
Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces.
arXiv preprint arXiv:2111.00396 , 2021a.
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining
recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural
information processing systems , 34:572–585, 2021b.
Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state
spaces. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural
Information Processing Systems , 2022. URL https://openreview.net/forum?id=RjS0j6tsSrf .
Elad Hazan and Karan Singh. Introduction to online nonstochastic control. arXiv preprint arXiv:2211.09619 ,
2022.
Elad Hazan, Karan Singh, and Cyril Zhang. Learning linear dynamical systems via spectral filtering. In
Advances in Neural Information Processing Systems , pp. 6702–6712, 2017.
Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. Spectral filtering for general linear
dynamical systems. In Advances in Neural Information Processing Systems , pp. 4634–4643, 2018.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780,
1997.
John J Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the national academy of sciences , 79(8):2554–2558, 1982.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure
prediction with alphafold. Nature, 596(7873):583–589, 2021.
Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models
great on long sequence modeling? arXiv preprint arXiv:2210.09298 , 2022.
11Under review as submission to TMLR
Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and
Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349 ,
2023.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks.
InInternational conference on machine learning , pp. 1310–1318. Pmlr, 2013.
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano
Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. arXiv
preprint arXiv:2302.10866 , 2023.
David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error
propagation, 1985.
Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional
memory. In International Conference on Machine Learning , pp. 31312–31327. PMLR, 2023.
Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing:
Towards a sharp analysis of linear system identification. In Conference On Learning Theory , pp. 439–473.
PMLR, 2018.
Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence
modeling. In The Eleventh International Conference on Learning Representations , 2023.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In
International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=
qVyeW-grC2k .
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput.
Surv., 55(6), dec 2022. ISSN 0360-0300. doi: 10.1145/3530811. URL https://doi.org/10.1145/3530811 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Ré. Effectively modeling
time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489 , 2023.
A Detailed Related work
State space models. SSMs for learning long range phenomenon have received much attention in the deep
learning community in recent years. Gu et al. (2020) propose the HiPPO framework for continuous-time
memorization, and shows that with a special class of system matrices A(HiPPO matrices), SSMs have the
capacity for long-range memory. Subsequently, Gu et al. (2021b) propose the Linear State-Space Layer (LSSL),
where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a
convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning
of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO
dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable
in practice as well as memory-intensive. As a result, Gu et al. (2021a) develop the S4 parameterization
to address these bottlenecks. The S4 parameterization restricts the system matrices Ato be normal plus
low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design
memory and computationally efficient methods that are also numerically stable.
The S4 model has been further streamlined in later works. Gupta et al. (2022) simplify the S4 parameterization
to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4
on several benchmarks. Smith et al. (2023) propose the S5 architecture, which improves upon S4 in two
12Under review as submission to TMLR
directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS
that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing
custom-designed algorithms for computing the convolutional filters.
To improve the performance of SSMs on language modeling tasks, Dao et al. (2022) develops the H3 layer by
stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer:
remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift
SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift
SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner
product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization,
to close the speed gap between SSMs and Transformers.
Motivated by the similarities between SSMs and RNNs, Orvieto et al. (2023) investigate whether deep
RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN
architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence
specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization
techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their
performance on benchmarks that require long-term memory. However, the paper also shows that without the
specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization
and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks. We provide
further details about this study after this section.
Spectral filtering. The technique of spectral filtering for learning linear dynamical systems was put forth
in Hazan et al. (2017). This work studies online prediction of the sequence of observations yt, and the goal
is to predict as well as the best symmetric LDS using past inputs and observations. Directly learning the
dynamics is a non-convex optimization problem, and spectral filtering is developed as an improper learning
technique with an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from
regression-based methods that aim to identify the system dynamics, spectral filtering’s guarantee does not
depend on the stability of the underlying system, and is the first method to obtain condition number-free
regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in
Hazan et al. (2018).
Convolutional Models for Sequence Modeling Exploiting the connnection between Linear dynamical
systems and convolutions (as highlighted by Gu et al. (2021a)) various convolutional models have been
proposed for sequence modelling. Fu et al. (2023) employ direct learning of convolutional kernels directly
to sequence modelling but find that they underperform SSMs. They find the non-smoothness of kernels to
be the culprit and propose applying explicit smoothing and squashing operations to the kernels to match
performance on the Long Range Arena benchmark. The proposed model still contains significantly large
number of parameters growing with the sequence length. Li et al. (2022) identifies two key characteristics
of convolutions to be crucial for long range modelling, decay in filters and small number of parameters
parameterizing the kernel. They achieve this via a specific form of the kernel derived by repeating and scaling
the kernel in a dyadic fashion. Shi et al. (2023) propose a multiresolution kernel structure inspired from the
wavelet transform and multiresolution analysis.
All these methods parameterize the kernels with specific structures and/or add further regularizations to
emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby
parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel.
Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly
capturing and improving the performance of SSMs without the need for specific initializations. Naturally our
kernels (see Fig 1) by default satisfy both the smoothness and the decay condition identified (and explicitly
enforced) by Li et al. (2022) and Fu et al. (2023).
A.1 Ablations performed by Orvieto et al. (2023)
Motivated by the success of SSMs, Orvieto et al. (2023) revisit the RNN model (under the same deep
stacked structure as SSMs) to investigate their efficiency. They begin from a simple linear RNN (a directly
13Under review as submission to TMLR
parameterized LDS) and add multiple components inspired from the SSM literature to ensure numerical
stability and trainability of the model especially as the sequences grow larger. Overall they demonstrate that
carefully designed parameterizations and initializations of LDS parameters as well as specifically designed
normalizations are all necessary for model to learn consistently over the LRA dataset and in particularly
over the 16K context length task PathX. These interventions are driven by specific intuitions such as an
inductive bias towards larger memory or controlling the loss blowup at initialization under long contexts
but as such come with no theoretical guarantees towards alleviating the problem. We provide some quick
details towards what these interventions are and refer the reader to Orvieto et al. (2023) to understand the
motivations behind them and comparisons with similar ideas existing in previous SSM literature. The LRU
model considered by Orvieto et al. (2023) is given by
yk=diag(λ)yk−1+γ⊙Buk.
In the above the learned parameters are λand B and note that diag (λ)corresponds to a diagonal A. γis a
specific normalization technique they develop to control the loss blowup under long-context detailed below.
They perform the following interventions towards stable training
•Stable Exponential Parameterization : They parameterize λas
λj= exp(−exp(νlog
j)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
magnitude+iexp(θlog
j)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
phase)
The above is done to ensure a bound on the magnitude of eigenvalues of the effective A matrix as
well as to ensure more resolution in the parameter space closer to the value of 1.
•Ring Initialization : They initialize the λjin the complex annulus [min_rad,max_rad ]. This
ensures that at initialization the magnitude of λjchosen randomly lies in ∈[min_rad,max_rad ]
and the phase is chosen randomly. When not applying this intervention min_rad and max_rad are
chosen to be 0,1 respectively. When applying this intervention these values are chosen to be closer to
1, e.g. 0.9,0.999respectively.
•γ-Normalization : They setγj=/radicalbig
1−|λj|2
•Restricting Phase at initialization : Instead of drawing a random phase at initialization the
authors recommend selecting the initial phase from [0,π/10]. The authors claim that uniform phase
inherently biases the network towards learning spurious features in the input sequence.
Orvieto et al. (2023) provide the following ablation in the paper. In particular we see that all the above
interventions are necessary to make the model get to non-trivial accuracy on PathX. On the contrary,
as we show the STU model achieves comparable accuracy without requiring any specific initialization or
normalization.
Model Specification sCIFAR ListOps Pathfinder PathX
LRU Dense A 72.2 50.4 ✗ ✗
ΛExp. Param. 85.4 60.5 65.4 ✗
ΛStable Exp. Param. 87.2 59.4 93.5 ✗
+ Ring Init. 88.1 59.4 94.4 ✗
+γ-Norm. + Phase Init. 89.0 60.2 95.1 94.2
B Computational complexity and comparison to other methods.
Using the STU method to make a sequence of Lpredictions, the features U+,U−∈RL×din×Kcan be
computed in time O(K·L·dinlog(L))using the Discrete Fast Fourier Transform, where Kis the number of
14Under review as submission to TMLR
filters andLis the context length. The linear prediction part (i.e. spectral component) takes O(K·L·din·dout)
time, and the autoregressive part can be implemented in total time O(L·din·dout). Therefore the overall
runtime isO(K·L·din·(log(L) +dout)).4
For comparison, consider LRU and transformers. The same computation carried out by LRU w. diagonal
system matrices is dominated by the hidden dimension, i.e. O(L·dhidden·(din+dout)). Thus, the number
of filters is replaced by dhidden, which is usually an order of magnitude larger, although STU has another
O(logL)overhead.
A transformer model with full attention runs in time O(L2dindout), which is significantly more costly than
both LRU and STU. This is consistent with the motivation of SSM as more efficient models for sequences.
C Proof of Theorem 3.1
We begin by observing that without loss of generality we can assume that Ais a real-diagonal matrix. This
can be ensured by performing a spectral decomposition of A=UΣU⊤andabsorbing theU,U⊤by redefining
the system. Before continuing with the proof, we will provide some requisite definitions and lemmas. Define
the following vector for any α∈R,µ(α)∈RL, withµ(α)(i) = (α−1)αi−1. Further define the Hankel matrix
Has
Z≜/integraldisplay1
0µ(α)µ(α)⊤dα.
As the following lemma shows the Hankel matrix Zabove is the same Hankel matrix defined in the definition
of STU equation 3.
Lemma C.1. Zis a Hankel matrix with entries given as
Z(i,j) =2
(i+j)3−(i+j)
Lemma C.2. We have that the following statements hold regarding µ(α)for anyα∈[0,1],
•|µ(α)|2≤1
•For anyα∈[0,1]and any unit vector vwe have that
(µ(α)⊤v)2≤12(v⊤Hv)
Lemma C.3. For anyα∈[0,1], let ˜µ(α)be the projection of µ(α)on the subspace spanned by top k
eigenvectors of Z, then we have that
∥µ(α)−˜µ(α)∥2≤12L/summationdisplay
i=k+1σi
Finally the following lemma from Hazan et al. (2017) shows that the spectrum of the matrix Zdecays
exponentially.
Lemma C.4 (Lemma E.3 Hazan et al. (2017)) .Letσjbe the topjtheigenvalue of Z. Then we have that
σj≤Γc−j/log(L)
wherec=eπ2/4∼11.79andΓ = 235200 is an absolute constant.
We now move towards proving Theorem 3.1. Consider the following calculation for the LDS sequence yLDS
t
yLDS
t=T/summationdisplay
i=0CAiBut−i+Dut,
4We shortly note that the Kfilters can be distributed amongst Kmachines and their computations done separately. There
are many other opportunities for distributed computing for all architectures which we will not survey here as it is out of scope.
15Under review as submission to TMLR
and therefore we have that
yLDS
t−yLDS
t−2= (CB+D)ut+CABut−1−Dut−2+T/summationdisplay
i=0C(Ai+2−Ai)But−2−i
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Term of Interest
For anyt1≥t2we define the matrix ¯U{t1:t2}∈Rdout×t1−t2+1whoseithcolumn is the input vector ut1−i+1.
We allowt2to be negative and by convention assume ut= 0for anyt≤0. Denote the diagonal entries of
Aby{αl}dh
l=1, i.e.αl=A(l,l). Further let bl,clbe thel-th column for the matrices B,Crespectively. The
term of interest above can then be written as
L/summationdisplay
i=0C(Ai+2−Ai)But−2−i
=dh/summationdisplay
l=1(cl⊗bl)/parenleftiggL/summationdisplay
i=0(αi+2
l−αi
l)ut−2−i/parenrightigg
=/summationdisplay
l:αl≥0(cl⊗bl)/parenleftiggL/summationdisplay
i=0(α2
l−1)αi
lut−2−i/parenrightigg
+/summationdisplay
l:αl<0(cl⊗bl)/parenleftiggL/summationdisplay
i=0(α2
l−1)αi
lut−2−i/parenrightigg
=/summationdisplay
l:αl≥0(αl+ 1)(cl⊗bl)/parenleftiggL/summationdisplay
i=0(αl−1)αi
lut−2−i/parenrightigg
+/summationdisplay
l:αl<0(1 +|αl|)(cl⊗bl)/parenleftiggL/summationdisplay
i=0(|αl|−1)|αl|i(−1)iut−2−i/parenrightigg
=/summationdisplay
l:αl≥0(αl+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}µ(α)/parenrightbig
+/summationdisplay
l:αl<0(|αl|+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}⊙1±/parenrightbig
µ(|αl|)
where 1±∈Rdout×Lis defined as the matrix whose every row is the alternating sign vector [1,−1,1,−1...])
and⊙is Hadamard product (i.e. entry-wise multiplication).
yLDS
t−yLDS
t−2= (CB+D)ut+CABut−1−Dut−2+/summationdisplay
l:αl≥0(αl+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}µ(α)/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
PositivePart
+/summationdisplay
l:αl<0(|αl|+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}⊙1±/parenrightbig
µ(|αl|)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
NegativePart(7)
Recallthatwedefinedthesequence {σk,ϕk}L
k=1tobetheeigenvalueandeigenvectorpairsfortheHankelmatrix
Z. Foranyαwedefinetheprojectionof µ(α)onthetopkeigenvectorsas ˜µ(α), i.e. ˜µ(α) =/summationtextK
k=1(µ(αl)⊤ϕk)ϕk.
Further define STU parameters as follows
Mu
1=CB+D,Mu
2=CAB,Mu
3=−D
Mϕ+
k=/summationdisplay
l:αl≥0(αl+ 1)(µ(αl)⊤ϕk)σ−1/4
k(cl⊗bl)
Mϕ−
k=/summationdisplay
l:αl<0(|αl|+ 1)(µ(|αl|)⊤ϕk)σ−1/4
k(cl⊗bl) (8)
By the definition of STU prediction equation 4 we have that,
ySTU
t=ySTU
t−2+3/summationdisplay
i=1Mu
iut+1−i+K/summationdisplay
k=1Mϕ+
kσ1/4
k/parenleftiggt−1/summationdisplay
i=0ut−i·ϕk(i)/parenrightigg
+K/summationdisplay
k=1Mϕ−
kσ1/4
k/parenleftiggt−1/summationdisplay
i=0ut−i·(−1)i·ϕk(i)/parenrightigg
=ySTU
t−2+3/summationdisplay
i=1Mu
iut+1−i+K/summationdisplay
k=1Mϕ+
kσ1/4
k/parenleftbig¯U{t−2:t−1−L}ϕk/parenrightbig
+K/summationdisplay
k=1Mϕ−
kσ1/4
k/parenleftbig
(¯U{t−2:t−1−L}⊙1±)ϕk/parenrightbig
.
16Under review as submission to TMLR
Using the parameters specified in equation 8 in the above we have that,
ySTU
t−ySTU
t−2= (CB+D)ut+CABut−1−Dut−2+/summationdisplay
l:αl≥0(αl+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}/parenrightbig
K/summationdisplay
k=1(µ(αl)⊤ϕk)ϕk
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=˜µ(α)

+/summationdisplay
l:αl<0(|αl|+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}⊙1±/parenrightbig
K/summationdisplay
k=1(µ(|αl|)⊤ϕk)ϕk
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=˜µ(|αl|)

Combining the above display with equation 7, we get that
yLDS
t−ySTU
t=yLDS
t−2−ySTU
t−2+/summationdisplay
l:αl≥0(αl+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}/parenrightbig
(µ(α)−˜µ(α))
+/summationdisplay
l:αl<0(|αl|+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}⊙1±/parenrightbig
(µ(|αl|)−˜µ(|αl|))(9)
Let∥B∥col=maxl∥bl∥,∥C∥col=maxl∥cl∥be the maximum column norms of BandCrespectively.
Therefore we have that for all l, the spectral norm of the matrix cl⊗blis bounded as∥B∥col·∥C∥col. Further
note that every column of ¯Uis an input utfor some time t. Further we have assumed that ∥ut∥≤afor allt.
Therefore we have that the frobenius norm (and thus spectral norm) of Ut−2:t−1−Lis bounded as
∥¯Ut−2:t−1−L∥≤∥ ¯Ut−2:t−1−L∥F≤√
L·a.
Putting the above together we get that for all l,
∥(αl+ 1)(cl⊗bl)/parenleftbig¯Ut−2:t−1−L/parenrightbig
∥≤| (αl+ 1)|∥(cl⊗bl)∥∥/parenleftbig¯Ut−2:t−1−L/parenrightbig
∥≤2·∥B∥col·∥C∥col·√
L·a.
Therefore we have (using C.3 that,
∥/summationdisplay
l:αl≥0(αl+ 1)(cl⊗bl)/parenleftbig¯Ut−2:t−1−L/parenrightbig
(µ(α)−˜µ(α))∥
≤/summationdisplay
l:αl≥0∥(αl+ 1)(cl⊗bl)/parenleftbig¯Ut−2:t−1−L/parenrightbig
∥·∥(µ(α)−˜µ(α))∥
≤5·∥B∥col·∥C∥col·L1.5·a·/radicaltp/radicalvertex/radicalvertex/radicalbtL/summationdisplay
i=K+1σi.
Similarly we have that
∥/summationdisplay
l:αl<0(|αl|+ 1)(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}⊙1±/parenrightbig
(µ(|αl|)−˜µ(|αl|))∥≤5·∥B∥col·∥C∥col·L1.5·a·/radicaltp/radicalvertex/radicalvertex/radicalbtL/summationdisplay
i=K+1σi.
Plugging the above into equation 9, we get that
∥yLDS
t−ySTU
t∥≤∥yLDS
t−2−ySTU
t−2∥+ 10·∥B∥col·∥C∥col·L1.5·a·/radicaltp/radicalvertex/radicalvertex/radicalbtL/summationdisplay
i=K+1σi
17Under review as submission to TMLR
Applying the above equation recursively and Lemma C.4 we get that for any K≥log(L),
∥yLDS
t−ySTU
t∥≤5·∥B∥col·∥C∥col·L2.5·a·/radicaltp/radicalvertex/radicalvertex/radicalbtL/summationdisplay
i=K+1σi≤c·∥B∥col·∥C∥col·L3·a·e/parenleftbig
−π2
4·K
log(L)/parenrightbig
.
wherec= 5Γ≤2×106is an absolute constant. This finishes the proof of the theorem.
C.1 Proofs of Lemmas
Proof of Lemma C.1. The lemma follows from the following simple calculations.
Z(i,j) =/integraldisplay1
0(α−1)2αi+j−2dα=/integraldisplay1
0/parenleftbig
αi+j+αi+j−2−2αi+j−1/parenrightbig
dα
=1
(i+j+ 1)+1
(i+j−1)−2
(i+j)
=2
(i+j)3−(i+j)
Lemma C.3 is immediate from the second part of Lemma C.2. We show Lemma C.2 below.
Proof of Lemma C.2. By definition µ(α) = 0forα∈{0,1}. Otherwise we have that for all α∈(0,1),
|µ(α)|2=T/summationdisplay
i=1(α−1)2α2i−2≤(α−1)2
(1−α2)≤1−α
1 +α≤1−α
To prove the second part we consider drawing αfrom the uniform distribution between [0,1]. We get that
E[(µ(α)⊤v)2] =v⊤Zv
We now show that the worst case value is not significantly larger than the expectation. To this end we
consider the function f(α) = (µ(α)⊤v)2and we show that this is a 6-Lipschitz function. To this end consider
the following,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂µ(α)
∂α/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2=T−1/summationdisplay
i=0/braceleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂
∂α(1−α)αi/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracerightigg
=T−1/summationdisplay
i=0/parenleftbig
(1−α)iαi−1−αi/parenrightbig2
≤2(1−α)2T−1/summationdisplay
i=1i2α2(i−1)+ 2T−1/summationdisplay
i=0α2i(a+b)2≤2(a2+b2)
≤2(1−α)2/parenleftbigg1
(1−α2)2+2α2
(1−α2)3/parenrightbigg
+2
1−α2∞/summationdisplay
i=1i2βi−1=1
(1−β)2+2β
(1−β)3
=2
(1 +α)2+4α2
(1−α2)(1 +α)2+2
1−α2.
18Under review as submission to TMLR
Therefore we have that for all α∈[0,1],
∂f(α)
∂α= 2(µ(α)⊤v)/parenleftbigg∂µ(α)⊤
∂αv/parenrightbigg
≤2∥µ(α)∥∥v∥2∥∂µ(α)
∂α∥
≤2/radicaligg
(1−α)∗/parenleftbigg2
(1 +α)2+4α2
(1−α2)(1 +α)2+2
1−α2/parenrightbigg
≤2/radicaligg/parenleftbigg2(1−α)
(1 +α)2+4α2
(1 +α)3+2
1 +α/parenrightbigg
≤6.
Now for the positive function f(α)which is 6-Lipschitz on [0,1]let the maximum value be R. It can be seen
the lowest expected value of f(α)over the uniform distribution over [0,1], one can achieve is R2/2∗6and
therefore we have that
R2/12≤v⊤Zv⇒R≤√
12v⊤Hv,
which finishes the proof.
D Alternative Representation for capturing negative eigenvalues
In this section we setup an alternative version of STU wherein a different Hankel matrix is used but one
can get a similar result. As before a single layer of STU (depicted in figure 2) is parameterized by a number
K, denoting the number of eigenfactors and matrices Mϕ
1...Mϕ
K∈Rdout×din,andMu
1,Mu
2,Mu
3∈Rdout×din.
The matrices form the paramsof the layer. We use a different Hankel matrix ZL∈RL×Lwhose entries are
given by
ZL[i,j]≜((−1)i+j−2+ 1)·8
(i+j+ 3)(i+j−1)(i+j+ 1). (10)
and let{(σj∈R,ϕj∈RT)}T
j=1be the eigenvalue-eigenvector pairs of ZLordered to satisfy σ1≥σ2...σd.
Given an input sequence {u1...uL}∈Rdin, as before we first featurize the input sequence by projecting the
input sequence till time tonfixedfiltersϕk. The main difference is that we do not need to create a negative
featurization now. We define
Ut,k=t−1/summationdisplay
i=0ut−i·ϕk(i).
Note that for every k, the sequence of features X1:T,kcan be computed efficiently via convolution. The output
sequence{y1···yT}is then given by
ˆyt= ˆyt−2+3/summationdisplay
i=1Mu
iut+1−i
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Auto−regressive Component+K/summationdisplay
k=1Mϕ
kσ1/4
kXt−2,k
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Spectral Component. (11)
We prove the following representation theorem which shows that the above class approximately contains any
marginally-stable LDS with symmetric A.
Theorem D.1. Given anyA,B,C,D such thatAis a symmetric matrix with ∥A∥≤1and given any numbers
K∈I+,a∈R+, there exists matrices Mu
1,Mu
2,Mu
3,Mϕ
1...Mϕ
K∈Rdout×dinfor allLand all sequences u1:L
satisfying∥ut∥≤afor allt∈[L]the following holds. Let yLDS
1:Lbe the sequence generated by execution of
the LDS given by A,B,C,D (via equation 1) and ySF
1:Lbe the sequence generated by Spectral Filtering (via
equation 11) using the matrices Mu
1,Mu
2,Mu
3,Mϕ+
1...Mϕ+
K,Mϕ−
1...Mϕ−
K. Then for all t∈[T], we have
that
∥yLDS
t−ySF
t∥2≤c·∥B∥col·∥C∥col·L3·a·e−/parenleftbig
π2
4·K
log(L)/parenrightbig
wherec≤106is a universal constant and ∥B∥col,∥C∥colare the maximum column norm of the matrices B
and C respectively.
19Under review as submission to TMLR
In the following we prove the above theorem.
D.1 Proof of Theorem D.1
Without loss of generality we assume that Ais a real-diagonal matrix. Before continuing with the proof, we
will provide some requisite definitions and lemmas. Define the following vector for any α,µ(α)∈RT, with
µ(α)(i) = (α2−1)αi−1. Further define the Hankel matrix Has
Z≜/integraldisplay1
−1µ(α)µ(α)⊤dα
As the following lemma shows the Hankel matrix Zabove is the same Hankel matrix ZLdefined in the
definition of STU equation 10.
Lemma D.2. Zis a Hankel matrix with entries given as
Z(i,j) = ((−1)i+j−2+ 1)·8
(i+j+ 3)(i+j−1)(i+j+ 1)
Proof.Consider the following simple computations
H(i,j) =/integraldisplay1
−1(α2−1)2αi+j−2dα
=/integraldisplay0
−1(α2−1)2αi+j−2dα+/integraldisplay1
0(α2−1)2αi+j−2dα
=/integraldisplay0
−1(|α|2−1)2(−1)i+j−2|α|i+j−2dα+/integraldisplay1
0(α2−1)2αi+j−2dα
=/integraldisplay1
0(α2−1)2(−1)i+j−2αi+j−2dα+/integraldisplay1
0(α2−1)2αi+j−2dα
= ((−1)i+j−2+ 1)/integraldisplay1
0(α2−1)2αi+j−2dα
= ((−1)i+j−2+ 1)·8
(i+j+ 3)(i+j−1)(i+j+ 1)
Lemma D.3. We have that the following statements hold regarding µ(α)for anyα∈[−1,1],
•|µ(α)|2≤1
•For anyα∈[−1,1]and any unit vector vwe have that
(µ(α)⊤v)2≤6(v⊤Zv)
Proof.By definition µ(α) = 0forα∈{− 1,1}. Otherwise we have that for all α∈(−1,1),
|µ(α)|2=T/summationdisplay
i=1(α2−1)2α2i−2≤(α2−1)2
(1−α2)= 1−α2≤1.
To prove the second part we consider drawing αfrom the uniform distribution between [−1,1]. We get that
E[(µ(α)⊤v)2] =v⊤Zv
2
20Under review as submission to TMLR
We now show that the worst case value is not significantly larger than the expectation. To this end we
consider the function f(α) = (µ(α)⊤v)2and we show that this is a 6-Lipschitz function. To this end consider
the following,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂µ(α)
∂α/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2=T−1/summationdisplay
i=0/braceleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂
∂α(1−α2)αi/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracerightigg
=T−1/summationdisplay
i=0/parenleftbig
(1−α2)iαi−1−2αi+1/parenrightbig2
≤2(1−α2)2T−1/summationdisplay
i=1i2α2(i−1)+ 4T−1/summationdisplay
i=0α2i+2(a+b)2≤2(a2+b2)
≤2(1−α2)2/parenleftbigg1
(1−α2)2+2α2
(1−α2)3/parenrightbigg
+4α2
1−α2∞/summationdisplay
i=1i2βi−1=1
(1−β)2+2β
(1−β)3
= 2 +8α2
(1−α2).
Therefore we have that for all α∈[−1,1],
∂f(α)
∂α= 2(µ(α)⊤v)/parenleftbigg∂µ(α)⊤
∂αv/parenrightbigg
≤2∥µ(α)∥∥v∥2∥∂µ(α)
∂α∥
≤2/radicaligg
(1−α2)∗/parenleftbigg
2 +8α2
(1−α2)/parenrightbigg
≤2/radicalbig
2 + 6α2≤6.
Now for the positive function f(α)which is 6-Lipschitz on [−1,1]let the maximum value be R. It can be
seen the lowest expected value of f(α)over the uniform distribution over [0,1], one can achieve is R2/2∗6
and therefore we have that
R2/12≤v⊤Zv
2⇒R≤√
6v⊤Zv,
which finishes the proof.
A direct consequence of the above lemma is the following.
Lemma D.4. For anyα∈[0,1], let ˜µ(α)be the projection of µ(α)on the subspace spanned by top k
eigenvectors of Z, then we have that
∥µ(α)−˜µ(α)∥2≤6L/summationdisplay
i=k+1σi
Finally the following lemma with a proof similar to C.3 shows that the spectrum of the matrix Zdecays
exponentially.
Lemma D.5. Letσjbe the topjtheigenvalue of Z. Then we have that
σj≤Γc−j/log(L)
wherec=eπ2/4∼11.79andΓ = 235200 is an absolute constant.
We now move towards proving Theorem D.1. Consider the following calculation for the LDS sequence yLDS
t
yLDS
t=T/summationdisplay
i=0CAiBut−i+Dut,
21Under review as submission to TMLR
and therefore we have that
yLDS
t−yLDS
t−2= (CB+D)ut+CABut−1−Dut−2+T/summationdisplay
i=0C(Ai+2−Ai)But−2−i
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Term of Interest
For anyt1≥t2we define the matrix ¯Ut1:t2∈Rdout×t1−t2+1whoseithcolumn is the input vector ut1−i+1. We
allowt2to be negative and by convention assume ut= 0for anyt≤0. Denote the diagonal entries of Aby
{αl}dh
l=1, i.e.αl=A(l,l). The term of interest above can then be written as
L/summationdisplay
i=0C(Ai+2−Ai)But−2−i=dh/summationdisplay
l=1(cl⊗bl)/parenleftiggL/summationdisplay
i=0(αi+2
l−αi
l)ut−2−i/parenrightigg
=dh/summationdisplay
l=1(cl⊗bl)/parenleftiggL/summationdisplay
i=0(α2
l−1)αi
lut−2−i/parenrightigg
=dh/summationdisplay
l=1(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}µ(α)/parenrightbig
.
Therefore we get that
yLDS
t−yLDS
t−2= (CB+D)ut+CABut−1−Dut−2+dh/summationdisplay
l=1(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}µ(α)/parenrightbig
.
Recallthatwedefinedthesequence {σk,ϕk}L
k=1tobetheeigenvalueandeigenvectorpairsfortheHankelmatrix
Z. Foranyαwedefinetheprojectionof µ(α)onthetopkeigenvectorsas ˜µ(α), i.e. ˜µ(α) =/summationtextK
k=1(µ(αl)⊤ϕk)ϕk.
Further define STU parameters as follows
Mu
1=CB+D,Mu
2=CAB,Mu
3=−D
Mϕ
k=/summationdisplay
l(µ(αl)⊤ϕk)σ−1/4
k(cl⊗bl)
The definition of STU prediction (using the above parameters) implies that the predicted sequence satisfies
ySTU
t−ySTU
t−2= (CB+D)ut+CABut−1−Dut−2+/summationdisplay
l(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}/parenrightbig
K/summationdisplay
k=1(µ(αl)⊤ϕk)ϕk
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=˜µ(α)
.
Combining the above displays we get that
yLDS
t−ySTU
t=yLDS
t−2−ySTU
t−2+/summationdisplay
l(cl⊗bl)/parenleftbig¯U{t−2:t−1−L}/parenrightbig
(µ(α)−˜µ(α)).
Using a similar derivation as in the proof of Theorem 3.1 we get that
∥yLDS
t−ySTU
t∥≤∥yLDS
t−2−ySTU
t−2∥+ 10·∥B∥col·∥C∥col·L1.5·a·/radicaltp/radicalvertex/radicalvertex/radicalbtL/summationdisplay
i=K+1σi
Applying the above equation recursively and Lemma D.5 we get that for any K≥log(L)
∥yLDS
t−ySTU
t∥≤5·∥B∥col·∥C∥col·L2.5·a·/radicaltp/radicalvertex/radicalvertex/radicalbtL/summationdisplay
i=K+1σi≤c·∥B∥col·∥C∥col·L3·a·e/parenleftbig
−π2
4·K
log(L)/parenrightbig
.
wherec= 2.5×Γ≤106is an absolute constant. This finishes the proof of the theorem.
22Under review as submission to TMLR
E Experiment Details
E.1 Synthetic Experiments with a marginally-stable LDS
The random system we generated for the experiments displayed in Figure 3a is as follows -
A=
−0.9999 0. 0. 0.
0. 0.9999 0 . 0.
0. 0.−0.9999 0.
0. 0. 0. 0.9999
, B =
0.36858183−0.34219486 0 .1407376
0.18933886−0.1243964 0 .21866894
0.14593862−0.5791096−0.06816235
−0.3095346−0.21441863 0 .08696061

C=
0.5528727−0.51329225 0 .21110639 0 .2840083
−0.18659459 0 .3280034 0 .21890792−0.8686644
−0.10224352−0.46430188−0.32162794 0 .1304409
, D =
1.5905786 0 . 0.
0.−0.45901108 0 .
0. 0. 0.3238576

Hyperparameters for STU: We only tuned the learning rate in the set ( [5e−2,1e−1,5e−1,1,5,10])
for vanilla STU and used K= 25.
Hyperparameters for LRU:
•Model Hyperparameters Orvieto et al. (2023) provide a few recommendations for the LRU model.
We tested exhaustively over the following hyperparameter choices:
–Stable Exp-parameterization: We searched over [True, False]
–Logarithmic Representation of Recurrent Parameters: We searched over [True, False]
–γ-Normalization: We searched over [True, False]
–Ring Initialization: We searched over min_rad∈ { 0.0,0.9,0.99,0.999}and max_rad∈
{0.9,0.99,0.999,1.0}.
–Setting the max_init_phase∈{1.57,3.14,6.28}
We found the Stable Exp-parameterization, Logarithmic Representation of Recurrent Parameters
andγ-normalization to be essential for training in this problem. We did not observe any particular
benefit of Ring Initialization or reducing the phase at initialization and we set them to defaults
eventually. We provide the learning curves over our search space in Figure 6.
•Optimization Hyperparameters Given the comparatively higher sample complexity of the LRU
model we employed standard deep-learning optimization tricks like tuning weight-decay as well as
applying a cosine learning rate schedule with warmup. These optimization tricks did not lead to
gains over standard training with Adam and a fixed learning rate in this problem. We tuned the
learning rate in the set ( [5e−2,1e−1,5e−1,1,5,10]).
23Under review as submission to TMLR
Figure 5: (Smoothed) Learning curves for learning a marginally stable LDS for a single STU layer (dashed)
vs a single LRU layer (solid). Different colors represent different learning rates highlighting that the training
becomes unstable for LRUs quickly as LR increases while the STU trains at much higher learning rates.
Curiously at stable LRs we observe that LRUs show a platea-ing of learning for a large fraction of the training
time.
Figure 6: LRU Hparam search vs STU. All the gray curves represent the hyperparameters for LRU we
tried. The STU curve is the best taken from Figure 5. For LRU we searched over choices of enabling stable
exp-parameterization, gamma-normalization, ring-initialization, phase-initialization, learning rate, weight
decay and constant vs warmup+cosine decay lr schedule.
E.2 Experimental setup for LRA experiments
Our training setup closely follows the experimental setup used by Orvieto et al. (2023). We use the same
batch sizes and training horizons for all the tasks as employed by Orvieto et al. (2023).
Hyperparameter tuning For all of our experiments on the LRA benchmark for both the vanilla STU model
and the auto-regressive AR-STU model we searched the learning rate in the set {1e−4,3e−4,5e−4,1e−
3,2.5e−3,5e−3}and tune the weight decay in the set {1e−3,1e−2,1e−1,5e−1,1.0}. We fix the number
of filtersKto be 24. We use Adam as the training algorithm with other optimization hyperparameters set
to their default values. We use the same learning rate schedule as Orvieto et al. (2023), i.e. 10% warmup
followed by cosine decay to 0. For the AR-STU model we searched over two values of ky∈{2,32}. In Table
2 we present a comparison of vanilla STU with AR-STU with ky= 2and AR-STU with ky= 32. We find
that both vanilla STU and AR-STU ky= 2reach comparable accuracy which is better than the baselines S4
24Under review as submission to TMLR
CIFAR ListOps TextRetrieval Pathfinder PathX
S4 Gu et al. (2021a) 88.65 59.60 86.82 90.90 94.20 96.35
LRU Orvieto et al. (2023) 89 60.2 89.4 89.9 95.1 94.2
STU 83.73 61.0490.48 90.40 91.70 89.71
AR-STU (ky= 2) 86.56 61.14 90.47 90.52 93.85 90.49
AR-STU (ky= 32)91.34 57.66 88.51 87.39 95.45 93.24
Table 2: Comparison of the STU model against various proposed SSM models on the LRA benchmark: Bold
values indicate the best for that task. We find that STU is competitive across all the workloads without the
need for carefully designed initializations, discretizations or normalizations. We report the median over 5
trials for our experiments.
and LRU on non-image datasets. On image datasets we found ky= 32to be helpful in getting better test
accuracies.
Initialization For the STU model we initialized alltheMmatrices at 0.
Finally while training the AR-STU model as employed by the training setup of Orvieto et al. (2023) and
previous SSM implementations, we found that using a smaller value of LR specifically for Mymatrices to be
useful. We decreased the value of LR by a factor 0.1or0.05and searched over this parameter.
F Power of Auto-regression: Dimension-dependent representation for LDS
In this section we give a short proof that any partially-observed LDS can be perfectly predicted via a linear
predictor acting over at most dof its past inputs and outputs where dis the hidden-state dimensionality (i.e.
A∈Rd×d). In particular
Theorem F.1. Given an LDS parameterized by A∈Rd×d,B,C,D, there exist coefficients α1:dand matrices
Γ0:dsuch that given any input sequence u1:L, the output sequence y1:Lgenerated by the action of the LDS on
the input satisfies for all t
yt=d/summationdisplay
i=1αiyt−i+d/summationdisplay
i=0Γiut−i
Proof.By unrolling the LDS we have that yt=/summationtextt
i=0CAiBut−i+Dut.. By the Cayley Hamilton theorem,
the matrix Ahas a characteristic polynomial pof degreed, namely there exists dnumbersc1:dsuch that
p(z) =d/summationdisplay
i=0cizi
satisfiesp(A) = 0. Without loss of generality we can assume the constant term in the polynomial is 1. We
can now consider the series for yt,yt−1,...as
yt−Dut =CBut+CABut−1+... CAtBu1
yt−1−Dut−1= 0 + CBut−1+... CAt−1Bu1
...
yt−d−Dut−d= 0 + 0 + ... CAt−dBu1
Now, if we take the combination of the above rows according to the coefficients of the characteristic polynomial,
we get that
d/summationdisplay
i=0ciyt−i=t/summationdisplay
j=0Rj+d/summationdisplay
i=0Dut−i (12)
25Under review as submission to TMLR
whereRjis the appropriate sum along the j′thcolumn of the matrix above. For all j >d, this amounts to
an expression of the form:
j >d⇒Rj=d/summationdisplay
i=0ciCAi·At−jBut−j=C(d/summationdisplay
i=0ciAi)·At−jBut−j=C·p(A)·At−jBut−j= 0.
Since all but the first dcolumns are zero, rearranging equation 12 and collecting terms, we get that there
exists coefficients α1:dand matrices Γ0:dsuch that
yt=d/summationdisplay
i=1αiyt−i+d/summationdisplay
j=0Γjut−j.
26