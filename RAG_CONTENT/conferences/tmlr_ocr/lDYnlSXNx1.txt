Under review as submission to TMLR
Regret-Optimal Transfer Learning from Pretrained Kernel
Regressors – with Applications to American Option Pricing
Anonymous authors
Paper under double-blind review
Abstract
Severalpre-trainedmachinelearningmodelsoperatebyaggregating, fine-tuning, andadapting
a (set of) models which have been pre-trained on a set of relevant tasks to any given novel
task. This paper exhibits a theoretically regret-optimal iterative scheme for performing this
aggregation for pre-trained finite-rank kernel ridge regression models using a fixed and finite
number of training iterations. We investigate the properties of our algorithm, including
its computational complexity and its adversarial robustness. We show that an adversary
which perturbs qtraining pairs by at most ε>0, across all training sets, cannot reduce the
regret-optimal algorithm’s regret by more than O(εq¯N1/2), where ¯Nis the aggregate number
of training pairs. Additionally, by leveraging symmetries within the regret-optimal algorithm,
we further develop a nearly regret-optimal heuristic that runs with O(Np2)fewer elementary
operations, where pis the dimension of the parameter space and Nis the total number of
training instances. To validate our theoretical findings, we conduct numerical experiments in
the context of American option pricing, utilizing a randomly generated finite-rank kernel.
1 Introduction
A core challenge in machine learning is identifying a set of parameters that optimize the performance of a
learning model on a single dataset D1by empirical risk minimization. Often, especially in transfer learning,
one does not have access to only a single dataset but several relevant datasets D1,...,DNwhose data can
be integrated into the training scheme. Comparable datasets naturally arise in instances of meta-learning
(Finn et al., 2017; Pavasovic et al., 2022)), in multi-task learning (Kumar & Daumé III, 2012; Yu et al.,
2020), in quantitative finance due to natural market cycles (Lowry & Schwert, 2002), or in robust finance
approaches to model uncertainty resulting in several plausible stochastic time-series models wherewith data
can be simulated (Hou & Obłój, 2018; Aksamit et al., 2019; Prashanth & Bhat, 2022).
This paper considers the problem of optimally selecting a deterministic training algorithm which maximizes
the performance of any given finite-rank kernel ridge regressor on a primary dataset D1while incorporating the
information present in Npretrained kernel ridge regressors, each trained on one of the datasets D1,...,DN.
The optimality of a training algorithm is quantified by minimizing a performance metric, the trajectory
generated by the optimization algorithm after a fixed number (T)of iterations.
We focus on the rich class of finite-rank kernel ridge regressor (fKRR) since they subsume all deep neural
networks whose hidden layers have been pre-trained (or randomly generated) and frozen, and only the
final linear layer can be trained on the relevant novel task; see e.g. Herrera et al. (2023); Gonon (2023).
Furthermore, fKRRs are also known to provide analytically tractable approximations to the training dynamics
of deep neural networks under certain conditions Jacot et al. (2018; 2020); Arora et al. (2019); Bordelon et al.
(2021).
1Under review as submission to TMLR
1.1 Problem Formulation
Fix positive integers d,N,p, andT. A finite-rank kernel regressor is a map fθ:Rd→Rwhich is linear in its
trainable parameter1θ∈Rpbut typically non-linear in its inputs x∈Rdthrough a feature map ϕ:Rd→Rp
fθ(x) =ϕ(x)⊤θ; (1)
typically with p≫d. The kernel associated to fis given by κ(x,˜x) =ϕ(x)⊤ϕ(x). One example would be a
pre-trained deep neural network where only the final linear layer θis fine-tuned and the frozen hidden layers
define the feature map ϕ; see Section 3 for details.
We are provided with Nnon-empty datasets D1,...,DN⊆Rd×R, of whichD1is distinguished as describing
the principal (supervised) regression task. Each of the i= 1,...,Npre-trained fKRRs fθnoptimizes their
respective penalized empirical minimization problem defined over their respective dataset Di
inf
θ∈Θ/summationdisplay
(x,y)∈Di(fθ(x)−y)2+κ∥θ∥2; (2)
where the hyperparameter κ > 0controls the regularity of the model by modulating the magnitude of
Euclidean norm∥θ∥2of its weight vector. The minimizer θ⋆
iof(2)for eachiis called a finite-rank kernel
ridge regressor (fKRR) and is available in closed-form, see Appendix C.
Fix a set of weights w∈∆Ndef.={[0,1]N:/summationtextN
i=1wi= 1}, to be optimized. We extend the summed squared
error (SSE) loss function in (2)in the case n= 1to incorporate the influence of other datasets into the
primary regression task
l(Θ,w;D)def.=N/summationdisplay
i=1wi|Di|/summationdisplay
j=1(fθw(xi
j)−yi
j)2, (3)
where Θdef.=(θ⊤
1,···,θ⊤
N)⊤∈RNp×1andθwdef.=/summationtextN
i=1wiθi. The optimal weights ware selected in a PAC-
Bayesian fashion, see Alquier (2021) for an overview of PAC-Bayesian bounds, in such a way that they
prioritize the influence of datasets which are similar to D1while automatically removing sufficiently different
datasets.
Since the optimizer of (3)need not be available in closed-form, we must optimize it iteratively. Upon fixing
a maximal number of iterations Tand an initial parameter choice θ0∈Rp, we follow Casgrain & Kratsios
(2021) and identify every deterministic training algorithm for (3) with a sequence in RNpof length 1 +T
θ0···Tdef.={θ(t)}T
t=0, θ (0) =θ0. (4)
Regret Optimality We seek the algorithms which aim to minimize (3)while biasing their iterates towards
the parameters θ⋆
1,...,θ⋆
Nwhich are optimal for the pre-trained models, weighted by w. To this end, minimize
the following penalized regret functional over the class of deterministic algorithms, i.e. of the form (4),
R(Θ0···T)def.=l(Θ(T),w;D)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Prediction−l⋆+T−1/summationdisplay
t=0/parenleftbig
λ∥Θ(t+ 1)−Θ⋆∥2
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Transfer Learning+β∥∆Θ(t)∥2
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Algo. Stability/parenrightbig
, (5)
where Θ⋆def.=(θ⋆⊤
1,···,θ⋆⊤
N)⊤∈RNp×1records the optimal parameters of each pretrained fKRR, ∆Θ(t)def.=
Θ(t+ 1)−Θ(t)quantifies the magnitude of each training update, and l⋆def.= min Ξ∈RNp×1l(Ξ,w;D)is the
minimal value of the loss function (3). Thus, the formulation of regret in (5)isspecific to the setting of training
a fKRR from multiple datasets . The first term (prediction) in(5)emphasizes that any “good” optimization
algorithm must minimize the regression loss function (3). The second term (transfer learning) encodes the
degree to which an optimization algorithm encodes information from similar datasets into its iterates. Finally,
1We emphasize that all vectors are represented as column vectors, not row vectors.
2Under review as submission to TMLR
the third (algo. stability) term biases towards algorithms whose iterative updates do not change too rapidly,
similar to controlling the learning rate in gradient descent. A game theoretic interpretation of Ris given in
Appendix A.
The primary objective of this paper is to exhibit a regret optimal optimization algorithm , meaning a sequence
Θ0...TminimizingRacross all deterministic algorithms with Titerations; i.e. all elements of R(1+T)Np.
Further, our objective is to explicitly establish the existence of a regret-optimal algorithm, i.e. the algorithm
minimizing (5), and to investigate its theoretical properties such as adversarial robustness and computational
complexity.
1.2 Contributions
We exhibit a regret optimal algorithm, by which we mean an optimizer of (5). Aligned with the recent
trends in optimization Casgrain (2019b); Li et al. (2018), we leverage techniques from optimal control theory
to characterize the regret-optimal algorithm. We show that this is possible due to the observation that
minimizers of (5) coincide with the minimizers of the energy functional
L(Θ0···T)def.=T−1/summationdisplay
t=0/bracketleftig
λ∥Θ(t+ 1)−Θ⋆∥2
2+β∥∆Θ(t)∥2
2/bracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Running-Cost+l(Θ(T),w;D)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Terminal Cost. (6)
From this control-theoretic perspective, every algorithm (4)can be regarded as driven by a “control”, and the
functionalLhas the form of a “cost function” of an optimal control problem, decomposable into the sum of a
“running/operational cost” and a “terminal/objective cost”. This allows us to leverage tools from optimal
control (Touzi & Tourin, 2013) to derive a closed-form expression of the unique regret-optimal algorithm
driven by the unique “optimal control” as defined in Algorithm 2.
We prove that this procedure is adversarially robust, meaning that the value of (6)varies byO(ε√¯Nq)if a
malicious adversary can perturb at most 1/qpercent of all the data in each of the dataset Diby at-most
ε≥0, where ¯N=/summationtextN
i=1|Di|. Furthermore, we show that our regret-optimal algorithm has a computational
complexity ofO(N2p3+T(Np)2.373). Moreover, we characterize the optimal choice of the weights wwhich
the central planner must use to maximize the selected model’s performance on the primary/focal dataset D1.
Finally, we show that if all the datasets D1,...,DNare equally dissimilar (but not necessarily similar)
then the regret-optimal algorithm can be accelerated. This acceleration requires O(Np2)fewer elementary
operations. We also show that if this “accelerated” algorithm is used in situations when the datasets differ by
a little, then it does not deviate far from the regret-optimal algorithm. Thus, it is an accelerated heuristic.
We emphasize that this problem has not previously been studied. Therefore, there are no available benchmarks
for the proposed algorithm. Hence, our numerical experiments consist of ablation studies where the proposed
models are benchmarked against various natural alternatives to the regret-optimal algorithm.
Outline
The paper is organized as follows. In Section 2, we introduce the regret-optimal algorithm and demonstrate
the optimality, computational complexity, and adversarial robustness of the algorithm. We further introduce
an accelerated algorithm of lower computational complexity that can achieve near-regret optimality. In
Section 3, we conduct a convergence analysis of the regret-optimal algorithm and compare it to the standard
gradient descent approach. Then we show the transfer learning capabilities of the algorithm for American
option pricing.
A game theoretic interpretation of Ris given in Appendix A. Appendix B is devoted to proving the optimality,
adversarial robustness, and computational complexity of the regret-optimal algorithm. The near-regret
optimality and computational complexity of the accelerated algorithm are also proven. Appendix C briefly
introduces the background of finite-rank kernel ridge regressors.
3Under review as submission to TMLR
1.3 Related Work
Examples of fθof the form (1)include extreme learning machine (Gonon et al., 2020; 2023; Ghorbani et al.,
2021; Mei & Montanari, 2022) or any finite-rank kernel ridge regressor (fKRR) (Amini, 2021; Amini et al.,
2022). Random feature models, and finite-rank ridge regressors, are typical in contemporary quantitative
finance (Gonon, 2023; Herrera et al., 2023) or reservoir computing (Cuchiero et al., 2021), where it is
favourable to rapidly deploy a deep learning model due to time or computing-power limitations/constraints.
Finite-rank kernel ridge regressors are also typical in the setting of standard transfer (Howard & Ruder, 2018)
and multi-task learning (Ren & Lee, 2018; Jia et al., 2019) pipelines where one often has a pre-trained deep
neural network model and, for any novel task, the user freezes the hidden layers in the deep neural network
model and only fine-tunes/trains the deep neural network’s final linear layer. This is useful, since the hidden
layers of most deep neural network architectures are known to process general, cruder features and the final
layer of any such model captures finer, task-specific features (Zeiler & Fergus, 2014; Yosinski et al., 2014).
The optimization of a model trained from multiple data sources lies at the heart of multi-task learning. In
Sener & Koltun (2018), the authors cast the task of optimizing (3)as a multi-objective optimization problem.
They propose to solve it via a multi-gradient descent algorithm which, under mild conditions, converges to a
Pareto stationary point; i.e. which is essentially a critical point of each of the i-th player’s penalized SSE.
Though iterative schemes for multi-objective optimization can efficiently reach critical points (Fliege et al.,
2019), this problem’s critical point is not necessarily a minimum since the problem (3)is not convex when w
is shared amongst all Nplayers.
This proposed algorithm reflects the ideas of the federated stochastic gradient descent (FedSGD) algorithm of
McMahan et al. (2017) and of its online counterpart FedOGD which is an online federated version of online
gradient descent2and more efficient online versions thereof such as Kwon et al. (2022). From the federated
learning perspective, the problem of training individual fKRR on each dataset, and then centralizing them by
optimizing the mixing parameter win(3)and its origins date back to local gradient descent introduced in
the optimization and control literature in Mangasarian (1995). Though several authors (e.g. Khaled et al.
(2020)) have proposed various progressive improvements of federated gradient-descent-type algorithms for
training a learner from several data sources, the problem of studying themost efficient iterative federated
learning algorithm for regret minimization has not yet been tackled.
We note that (stochastic) optimal control tools have revealed new insights into machine learning. Examples
include an optimal step-size control (Li et al., 2017b) and batch-size control (Zhao et al., 2022), developing
online subgradient methods that adapt to the data’s observed geometry (Duchi et al., 2011), developing
new maximum principle-based training algorithms for deep neural networks (Li et al., 2018), or unifying
(stochastic) optimization frameworks (Casgrain, 2019a).
Notation
We briefly introduce some additional notation. Each dataset has a finite number of samples which we write
asDi={(xi
j,yi
j)|1≤j≤|Di|}. We assume that some feature map ϕis fixed and denote the features of
each sample ofDias
ui
jdef.=ϕ(xi
j).
Moreover, we denote the joint inputs and outputs for each dataset as
Xidef.=/parenleftig
xi
1,···,xi
|Di|/parenrightig⊤
∈R|Di|×dandYidef.= (yi
1,···,yi
|Di|)⊤∈R|Di|,
and the joint features as
Uidef.= Φ(Xi)def.=/parenleftig
ϕ(xi
1),···, ϕ(xi
|Di|)/parenrightig⊤
∈R|Di|×p.
Finally, we use Ipto denote a p×pidentity matrix.
2See (Hazan et al., 2016, Chapter 3) for details on online gradient descent
4Under review as submission to TMLR
2 Main Results
This section contains the description of the uniqueregret-optimal algorithm and an analysis of its theoretical
properties. Throughout our manuscript, we maintain the following two assumptions.
Assumption 1 (D-Boundedness) .There are constants Kx,Ky>0such that all (xi
j,yi
j)∈ Dsatisfy
∥ui
j∥2≤Kxand|yi
j|2≤Ky, whereui
j=ϕ(xi
j).
Assumption 2 ((w⋆,D)-Compatibility) .The dataD=∪N
i=1Diand the weights w⋆satisfy
N/summationdisplay
i=1|Di|/summationdisplay
j=1ui
jui⊤
j≥0, (7)
[w⋆
1Ip,···,w⋆
NIp]⊤
N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1ui
jui⊤
j
[w⋆
1Ip,···,w⋆
NIp]≥0. (8)
With the notation introduced before, the object [w⋆
1Ip,···,w⋆
NIp]above is ap×Npmatrix.
2.1 The Algorithms
Share if Prediction is Good
fKRR LearnerShare if Prediction is Good
Share if Prediction is Good
(a) Step 1: Adaptive “Prior” Sharing: The ithdataset
Diis shared with the main dataset D1if the best
modelfθ⋆
i, optimizing (25)on dataset Di, can outper-
form the optimal model trained on the main dataset,
up to a slack factor η>0. The relative importance
of each dataset is proportional to fθ⋆
i’s performance
relative to the training performance of the best model
trained on the main dataset. When ηis small, sharing
is unlikely; when ηis large, nearly all datasets are
pooled together with approximately the same relative
importance.
fKRR Learner(b) Step 2: Optimized “Posterior” Sharing: We then
optimize the relative importance/weights given to the
shared datasets from the naive adaptive sharing. The
optimization is a closed-form update of the adaptive
“prior” weights which optimize a certain maximum
expected performance criterion subject to a relative
entropy penalization against the weights obtained
from adaptive prior sharing.
Figure 1: Visual Summary of Optimal Information Sharing (Algorithm 1).
We begin by describing our procedure for determining the optimal mixture weights defining the cooperative
objective function (3)used in (5). This procedure is illustrated graphically in Figure 1 and detailed in
Algorithm 1.
Suppose now that we have used Algorithm 1 to compute weights w⋆= (w⋆
1,···,w⋆
N)and the locally optimal
fKRR parameters θ⋆
1, ...,θ⋆
N, andw⋆. The key novelty in the next algorithm follows a similar spirit to Li et al.
(2017a) and Casgrain & Kratsios (2021), by casting the problem of constructing a regret-optimal algorithm
as an optimal control problem.
As before, denote
Θ(t)def.= [θ⊤
1(t),···,θ⊤
N(t)]⊤,Θ⋆def.= [θ⋆⊤
1,···,θ⋆⊤
N]⊤,
5Under review as submission to TMLR
Algorithm 1: Optimal Information Sharing - Initialization to Algorithm 2
Require: DatasetsD1,...,DN, finite-rank kernel ϕ, hyperparameters3κ,η> 0.
// Initialize Locally-Optimal Learners and Record Scores on Main Dataset
Fori: 1,...,N in parallel
θ⋆
i←(Φ⊤(Xi)Φ(Xi) +κIp)−1Φ⊤(Xi)Yi// Optimize each fKRR
si←1
|D1|/summationtext|D1|
j=1/vextenddouble/vextenddoublefθ⋆
i(u1
j)−y1
j/vextenddouble/vextenddouble2
2// Score predictive power of fKRR on main dataset
end
// Get Adaptive Prior
Fori: 1,...,N in parallel
wi←[s1+η−si]+/summationtextN
j=1[s1+η−sj]+
end
// Get Optimized Posterior
Fori: 1,...,N in parallel
w⋆
i←esi/η[s1+η−si]+ /summationtextN
j=1esj/η[s1+η−sj]+
end
returnOptimal Weights w⋆and Locally-Optimal fKRR Parameters θ⋆
1,...,θ⋆
N.
α(t)def.= [α⊤
1(t),···,α⊤
N(t)]⊤,
which are all elements of RNp, and introduce the dynamics that Θfollows
Θ(0) = Θ⋆,Θ(t+ 1) = Θ(t) +α(t),0≤t≤T−1, (9)
which is equivalently written as
Θ(t+ 1) = Θ⋆+t/summationdisplay
u=0α(u),0≤t≤T−1. (10)
Then, since the optimizers of the energy (6)coincide with the optimizers of the systemic regret functional (5),
searching for a regret-optimal algorithm reduces to solving for an optimal control αto minimize the cost (6)
subject to (9). The optimal control problem (6)and(9)is a discrete-time linear quadratic optimal control
problem, and can be solved by dynamic programming methods that are standard in the literature. For
instance, by (Başar & Olsder, 1998, Proposition 5.1), we obtain the solution for the regret-optimal algorithm
in closed-form as solutions of a system of Riccati equations (15) and (16), given in Theorem 2.
Algorithm 2 below implements the optimal control approach in two steps. In Step 1, we solve the equation
system(15)and(16)backwards for t=T,T−1, ...,1. Then in Step 2, we obtain the optimal control α(t)
and update the parameter Θ(t)fort= 0,1, ...T.
In the next subsections, we state our primary results, whose proofs are presented in Appendix B. We first
quantify the optimality of Algorithm 1. Next, we establish the optimality, complexity, and adversarial
robustness of Algorithm 2.
2.2 Properties of Algorithm 1
By decoupling the optimization of the “dataset-relevant weights” win Algorithm 1 from the optimization of
the parameter θin Algorithm 2, we avoid open-loop forward-backward iterations, which iterate between
optimizing wandθ. We now study the optimality of the weight w⋆generated by Algorithm 1. The weights
w⋆represent the optimal probability distributions of randomly selecting amongst the Noptimized regressors
{fθ⋆
i}N
i=1, each optimized locally on their dataset Diby solving (25). Here, optimality is quantified in terms
of the best-expected prediction on the focal dataset D1subject to a penalty for deviating too far from a
6Under review as submission to TMLR
Algorithm 2: Regret-Optimal Optimization Algorithm
Require: DatasetsD1,...,DN,N. Iterations T∈N+, finite-rank kernel ϕand hyperparameters
λ,β,κ,η> 0.
// Get Initialize Weights and Locally-Optimal fKRR Parameters
θ⋆
1,...,θ⋆
N,w⋆←Run: Algorithm 1 with D1,...,DN,ϕ, andκ,η.
// Initialize Updates
P(T) = [w⋆
1Ip,···,w⋆
NIp]⊤/parenleftig/summationtextN
i=1w⋆
i/summationtext|Di|
j=1ui
jui⊤
j/parenrightig
[w⋆
1Ip,···,w⋆
NIp]
S(T) =−[w⋆
1Ip,···,w⋆
NIp]⊤/summationtextN
i=1w⋆
i/summationtext|Di|
j=1ui
jyi
j
Θ(0) = Θ⋆def.= (θ⋆⊤
1,···,θ⋆⊤
N)⊤
// Generate Iterates
fort=T−1,..., 1do
// Update Driving Parameters
P(t) =βINp−β2[(λ+β)INp+P(t+ 1)]−1
S(t) =β[(λ+β)INp+P(t+ 1)]−1(S(t+ 1)−λΘ⋆)
end for
fort= 0,...,T−1do
// Update Control
α(t) =−[(λ+β)INp+P(t+ 1)]−1[(λINp+P(t+ 1))Θ(t)−λΘ⋆+S(t+ 1)]
Θ(t+ 1) = Θ(t) +α(t)
end for
returnReturn the Optimized fKRR fθw⋆
reference distribution. This reference distribution serves to automatically delete models whose datasets are
too dissimilar to D1. Inspired by most meta-learning problems, e.g. Pavasovic et al. (2022), similarity is
quantified by extracting a “similarity score” from each learner.
LetP({θ⋆
i}N
i=1)consist of all probability measures on the set {θ⋆
i}N
i=1. Any probability measure Pwin
P({θ⋆
i}N
i=1)is uniquely determined by a weight win theN-simplex; consisting of all w∈[0,1]Nwhose entries
sum to 1. Letθbe a random variable with law Pw, for some Pw∈P({θ⋆
i}N
i=1). The average performance of
the random learner fθon the datasetD1is
Eθ∼Pw
1
|D1||D1|/summationdisplay
j=1/vextenddouble/vextenddoublefθ(u1
j)−y1
j/vextenddouble/vextenddouble2
2
=N/summationdisplay
i=1wi/bracketleftbigg1
|D1||D1|/summationdisplay
j=1/vextenddouble/vextenddoublefθ⋆
i(u1
j)−y1
j/vextenddouble/vextenddouble2
2/bracketrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
ithlearner’s score (si). (11)
Fix aninformation sharing level η>0. The information sharing level ηadaptively determines which of the
datasets inDare implicitly shared with the principle dataset D1. The information sharing is formalized
through the probability measure
¯Pη(θ⋆
i)def.=[s1+η−si]+/summationtextN
i=1[s1+η−si]+.
on the set of Nlearners, or equivalently on the parameters θ⋆
1,...,θ⋆
N. The measure ¯Pηthus allows us to
minimize the regularized version of the loss in (11),
min
PwEθ∼Pw/bracketleftbigg1
|D1||D1|/summationdisplay
j=1/vextenddouble/vextenddoublefθ(u1
j)−y1
j/vextenddouble/vextenddouble2
2/bracketrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
ithlearner’s score (si)+ηDKL/parenleftbig
Pw∥¯Pη/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Inhomogeneity of
information sharing, (12)
7Under review as submission to TMLR
where the infimum is taken over all PwinP({θ⋆
i}N
i=1)andDKLdenotes the Kulbeck-Leibler divergence with
respect to ¯Pηgiven for any probability measure Pwon{θ⋆
i}N
i=1, namely
DKL/parenleftbig
Pw∥¯Pη)def.=N/summationdisplay
i=1wilog/parenleftbiggwi
[s1+η−si]+Cη/parenrightbigg
,
whereCηdef.=/summationtextN
j=1[s1+η−sj]+, ifPwis absolutely continuous with respect to ¯Pηand otherwise
DKL/parenleftbig
Pw∥¯Pη) =∞. There is a unique probability measure optimizing the penalized functional (12)determined
by the weight w⋆in Algorithm 1.
Theorem 1 (Optimality of Weights w⋆in Algorithm 1) .Fixη>0. The unique weights w⋆
ion theN-simplex
minimizing (12)are given by
w⋆
i=esi/η[s1+η−si]+/summationtextN
j=1esj/η[s1+η−sj]+, i = 1,...,N. (13)
2.3 Guarantees for Algorithm 2
Our next result shows that Algorithm 2, and its iterates, are regret-optimal.
Theorem 2 (Algorithm 2 Computes the Unique Regret-Optimal Algorithm) .Suppose that Assumptions 1
and 2 hold and fix Θ⋆∈RNpas in(28). Then, the systemic regret functional Rdefined in (5)admits a
unique minimizer ˆΘ·def.= (ˆΘ(t))T
t=0of the form (29)with ˆΘ(0) = Θ⋆aand increments
∆ˆΘ(t) =−[(λ+β)INp+P(t+ 1)]−1/bracketleftbig
(λINp+P(t+ 1)) ˆΘ(t)−λΘ⋆+S(t+ 1)/bracketrightbig
, (14)
where, fort= 1,...,T, the matrices P(t)andS(t)in(14)are determined by
/braceleftigg
P(T) = [w⋆
1Ip,···,w⋆
NIp]⊤/parenleftig/summationtextN
i=1w⋆
i/summationtext|Di|
j=1ui
jui⊤
j/parenrightig
[w⋆
1Ip,···,w⋆
NIp]
P(t) =βINp−β2[(λ+β)INp+P(t+ 1)]−1, (15)
/braceleftigg
S(T) =−[w⋆
1Ip,···,w⋆
NIp]⊤/summationtextN
i=1w⋆
i/summationtext|Di|
j=1ui
jyi
j
S(t) =β/bracketleftbig
(λ+β)INp+P(t+ 1)/bracketrightbig−1(S(t+ 1)−λΘ⋆). (16)
In particular, for each t= 0,...,T−1
∆ˆΘ(t) =α(t),
whereαis as in Algorithm 2.
Remark 1. Instead of the sums we can equivalently use our matrix notation to write/summationtext|Di|
j=1ui
jui⊤
j=Ui⊤Ui
and/summationtext|Di|
j=1ui
jyi
j=Ui⊤Yi.
The explicit update rules for the regret-optimal optimization algorithm presented above permit us to compute
its complexity. Since we will be assuming that we work with floating-point arithmetic , all elementary
arithmetic operations (e.g. addition, subtraction, multiplication) are of constant (i.e. O(1)) complexity. We
are most interested in the regime where the number of datasets is much larger than the typical number of
instances/data per dataset; i.e. when
1
NN/summationdisplay
i=1|Di|≤N. (17)
Theorem 3 (Complexity of Regret-Optimal Algorithm) .In the setting of Theorem 2, the computational com-
plexity4of computing the sequence (ˆΘ(t))T
t=0defining the regret-optimal algorithm is O/parenleftbig
N2p3+T(Np)2.373/parenrightbig
.
4We emphasize that we have assumed that computational complexity of all elementary arithmetic operations (e.g. addition,
subtraction, multiplication) is O(1).
8Under review as submission to TMLR
Next, we quantify the sensitivity of Algorithm 2 to adversarial attacks. We study the robustness of Algorithm 2
with respect to a family of perturbations that switch a given percentage 0≤q≤1of all training data by
selecting fake data points at a distance of at-most ε≥0for any perturbed datapoint. Thus, for a fixed set of
datasets{Di}N
i=1, andq,εas above, we define the class Dq,εofadversarially generated datasets of “magnitude”
(q,ε)as all sets of datasets {/tildewideDi}N
i=1,/tildewideDi={{(˜xi
j,˜yi
j)|1≤j≤|Di|}⊆Rd×Rsatisfying Assumptions 1 and 2,
as well as
#{(i,j) :ui
j̸= ˜ui
joryi
j̸= ˜yi
j}
/summationtextN
i=1|Di|≤q
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Attack Persistenceand max
i,j/braceleftbig
max/parenleftbig
∥ui
j−˜ui
j∥2,|yi
j−˜yi
j|2/parenrightbig/bracerightbig
≤ε
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Attack Severity,
where the indices (i,j)run over{(i,j)|1≤i≤N,1≤j≤|Di|}and˜ui
jdef.=ϕ(˜xi
j).
In view of Theorem 2, the updates of the unique regret-optimal algorithm ˆΘ0···Tare determined by the
sequence α·def.=(α(t))T
t=0computed by Algorithm 2. It is convenient to emphasize this connection on α·and
the connection with the dataset Dwhen expressing the following theorem, with the energy (6)determining
the value of the systematic regret functional R. We therefore write L(α;D)def.=L(ˆΘ0···T).
Theorem 4 (Adversarial Robustness) .Fix aDsatisfying Assumptions 1 and 2. For any level of “attack
persistence” 0≤q≤1and any degree of “attack severity” ε≥0we have that
/vextendsingle/vextendsingleL(α;D)−L(/tildewideα;/tildewideD)/vextendsingle/vextendsingle≤O/parenleftig
ε/radicalig
¯Nq/parenrightig
,
where/tildewideαis the output of Algorithm 2 for any given adversarially-generated set of datasets {/tildewideDi}N
i=1∈Dq,εand
¯N=/summationtextN
i=1|Di|. HereOhides a constant depending only on Assumption 1.
2.4 An Accelerated Heuristic
We now discuss a situation in which the computational complexity of the regret-optimal algorithm can be
reduced. This is achieved by exploiting symmetries in the optimal policy computed by Algorithm 2.
Examining the explicit update rules, we notice that ifthe weights wdefining the energy (6)were to be
1/Ndef.=(1/N,..., 1/N)then the matrices defining the optimal policy/control’s updates would become highly
symmetric Toeplitz matrices, depending on exactly three p-dimensional block sub-matrices. This allows
us to parameterize the entire regret-optimal algorithm, for the sub-optimal weights 1/N, using only three
low-dimensional systems whose dimension is independent of the number of datasets N. In line with this,
Algorithm 3 implements the regret-optimal algorithm for the weight specification 1/Nby only keeping track
of these three optimal low-dimensional systems, and then efficiently recombining them by leveraging the
Toeplitz structure defining the optimal policy/control. These low-dimensional systems are defined using
helper functions , listed in equations (18)-(20).
The approximate near-optimality of Algorithm 3 only depends on the stability of the optimal policy/control,
defining the algorithm’s updates as a function of the weights w. If the optimal weights defining the systemic
regret functional (5)are close to 1/N, then Algorithm 3 is nearly regret-optimal. Otherwise, the sub-optimality
it incurs in favor of computational efficiency is explicitly quantifiable and depends only on the difference
between the optimal weights computed in Algorithm 1 and the symmetric weights 1/N.
Note now record, the matrix-valued functions γ1(·)andγ2(·)used to define the updates in Algorithm 3.
The matrix-valued functions γ1(·)andγ2(·)in Algorithm 3 are defined as
γ1(·)def.=/braceleftbig
(λ+β)Ip+π1(·)
−(N−1)π2(·)/bracketleftbig
(λ+β)Ip+π1(·) + (N−2)π2(·)]−1π2(·)/bracerightbig−1, (18)
γ2(·)def.=−γ1(·)π2(·)/bracketleftbig
(λ+β)Ip+π1(·) + (N−2)π2(·)/bracketrightbig−1, (19)
9Under review as submission to TMLR
Algorithm 3: Accelerated (Nearly) Regret-Optimal Algorithm
Require: DatasetsD1,...,DN,N. Iterations T∈N+, finite-rank kernel ϕ, hyperparameters λ,β,κ,η> 0.
// Get Initialize Weights and Locally-Optimal fRKR Parameters
θ⋆
1,...,θ⋆
N,w⋆←Run: Algorithm 1 with D1,...,DN,ϕ, andκ,η.
// Initialize Updates
π1(T) =π2(T) = (1/N3)/summationtextN
i=1/summationtext|Di|
j=1ui
jui⊤
j
π3(T) =−(1/N2)/summationtextN
i=1/summationtext|Di|
j=1ui
jyi
j
θ1(0) =θ⋆
1,θ2(0) =θ⋆
2,...,θN(0) =θ⋆
N
θ⋆
(N)=1
N/summationtextN
i=1θ⋆
i.
// Generate Iterates
fort=T−1,..., 0do
// Update Low-Dimensional Driving Parameters
π1(t) =βIp−β2γ1(t+ 1) //γ1(·)is defined by (18)
π2(t) =−β2γ2(t+ 1) //γ2(·)is defined by (19)
π3(t) =β/bracketleftbig
γ1(t+ 1) + (N−1)γ2(t+ 1)/bracketrightbig
(π3(t+ 1)−λθ⋆
(N))
end for
fort= 0,...,T−1do
// Update Low-Dimensional Control
fori= 1,...,N do
/hatwideαi(t) =/hatwideα/parenleftbig
θi(t),(θj(t))N
j=1,j̸=i,(γi(t+ 1))2
i=1,(πi(t+ 1))3
i=1/parenrightbig
θi(t+ 1) =θi(t) +/hatwideαi(t)
end for
end for
returnSynchronized fKRR fθw⋆
where
π1(T) =π2(T) =1
N3N/summationdisplay
i=1|Di|/summationdisplay
j=1ui
jui⊤
jandπ3(T) =−1
N2N/summationdisplay
i=1|Di|/summationdisplay
j=1ui
jyi
j
and, fort=T−1,..., 0,
π1(t) =βIp−β2γ1(t+ 1)
π2(t) =−β2γ2(t+ 1)
π3(t) =β/bracketleftbig
γ1(t+ 1) + (N−1)γ2(t+ 1)/bracketrightbig/parenleftig
π3(t+ 1)−λθ⋆
(N)/parenrightig
whereθ⋆
(N)=1
N/summationtextN
i=1θ⋆
i.
The updates of the low-dimensional systems which Algorithm 3 exploits are implemented via the helper
function ˆα, defined as
/hatwideα/parenleftbig
θi,((θj)N
j=1,j̸=i),(γi)2
i=1,(πi)3
i=1/parenrightbigdef.=−(γ1−γ2)(λIp+π1−π2)θi
−/braceleftbig
γ1π2+γ2[λIp+π1+ (N−2)π2]/bracerightbigN/summationdisplay
j=1θj
−/bracketleftbig
γ1+ (N−1)γ2/bracketrightbig
(π3−λθ⋆
(N)). (20)
The degree to which Algorithm 3 is nearly regret-optimal depends on how far the weights wdetermined by
Algorithm 1 are from the equal weighting 1/N. This is precisely quantified by the following results.
10Under review as submission to TMLR
Theorem 5 (Near-Regret Optimality) .Suppose thatDsatisfies Assumptions 1 and 2. and let ˆΘ0···Tdenote
the unique regret-optimal algorithm5computed according to Algorithm 2. Then, the output ϑ0···Tof Algorithm 3
satisfies
(i)Near-Optimality: The difference/vextendsingle/vextendsingleL(Θ0···T)−L(ϑ0···T)/vextendsingle/vextendsingleis of the order
O/parenleftig
¯NN3max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle+¯NN3/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle
+¯Nmax
1≤i≤N/vextendsingle/vextendsingle/vextendsingle1
N−w⋆
i/vextendsingle/vextendsingle/vextendsingle+N/vextenddouble/vextenddoubleΘ⋆−Θ⋆
(N)/vextenddouble/vextenddouble
2/parenrightig
,
(ii)Near-Optimal Increments: The difference∥∆Θ0···T−1−∆ϑ0···T−1∥ℓ1is of the order
O/parenleftig
¯NN3/2max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle
+¯NN1/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle+/vextenddouble/vextenddoubleΘ⋆−Θ⋆
(N)/vextenddouble/vextenddouble
2/parenrightig
,
whereOhides a constant depending only on λ,β,Θ⋆,(w⋆
1,...,w⋆
N),T,Kx, and inKy.
The next result shows that Algorithm 3 accelerates the regret-optimal algorithm by a linear factor in Nand
by a quadratic factor in p, whenNis large.
Theorem 6 (O/parenleftbig
Np2/parenrightbig
-Acceleration over Regret-Optimal Algorithm) .With the convention that floating-point
arithmetic operations are O(1), we have that:
(i)Complexity: Algorithm 3 has a complexity of O(TNp max{N,p}))
(ii)Acceleration over Regret-Optimal Algorithm: The regret-optimal algorithm has a complexity of
O/parenleftbig
TN3p3/parenrightbig
.
WhenN≥p, Algorithm 3 requires O(Np2)fewer operations than the regret-optimal algorithm.
Finally, we establish that Algorithm 3 is the regret-optimal algorithm, optimizing the systemic regret
functional (6), in the “symmetric” case where the optimal weights w⋆are all (1/N,..., 1/N).
Theorem 7 (Regret-Optimality in the Symmetric Case) .Suppose that w⋆
i= 1/Nfori= 1,...,N.Then,
ˆΘ0···T=ϑ0···T.
3 Experiments
We now experimentally verify our theoretical results, with a focus on quantitative finance. In particular, we
implemented the regret-optimal algorithm as well as its accelerated version and provide them together with
the code for all experiments described in this section at https://github.com/??? .
First, we conduct a convergence analysis of our regret-optimal algorithm, comparing it to standard gradient
descent. We then demonstrate the transfer learning capabilities of our algorithm in the context of American
option pricing. We use the randomized neural networks approach of Herrera et al. (2023) as our fKRR model,
since it best balances computational efficiency and expressiveness amongst the available non-linear versions
of the Longstaff & Schwartz (2001) algorithm for American option pricing.
5That is, the unique optimizer of (6)
11Under review as submission to TMLR
3.1 Convergence of the Regret-Optimal Algorithm
We first study the evolution of the energy functional (6)and loss (3)achieved in each iteration by our
regret-optimal Algorithm 2 with weights initialized by Algorithm 1. That is to say, we compute L(Θ0···t)as
defined in (6)but for intermediate iterations t, as well as the loss l(Θ(t),w;D). We compare the regret-optimal
algorithm with its accelerated version, given by Algorithm 3 with weights given by Algorithm 1, where we use
Θ(0) = Θ⋆
(N)and replace Θ⋆byΘ⋆
(N)in(6). Moreover, we compare to the results when using gradient descent
to optimize the loss (3)directly. For all algorithms we use a ridge coefficient κ= 0, regret coefficients λ= 0,
β= 1and Algorithm 1 with information sharing level η= 10to get the corresponding optimal weights w⋆
and to initialize Θ(0)with the locally optimal parameters. We note that the comparison of our algorithms to
gradient descent optimizing the loss (3)only makes sense in the λ= 0regime, since otherwise the objectives
and therefore the behaviour are different. On the other hand, using β >0doesn’t change the objective, but
only forces our algorithm to make equally sized steps ∆Θ(t), where the step size depends on βandT.
For our convergence analysis we use N= 5randomly generated datasets. In particular, each dataset Di
corresponds to a neural network gωiwith fixed randomly-generated hidden weights and biases. Here ωi
denotes the vector of all these hidden weights and bias parameters. The networks have d= 5dimensional
input, one hidden layer with 10hidden nodes, ReLUactivation function and map to a 1-dimensional output.
The dataset is generated by sampling Si= 100inputsxi
jfrom ad-dimensional standard normal distribution
and the corresponding target values are given by gωi(xi
j), i.e.,
Di={(xi
j,gωi(xi
j))|1≤i≤Si,xi
j∼N(0,Id)}.
Our feature map ϕin(1)is a randomized neural network6with one hidden layer, 500hidden nodes and ReLU
activation as studied by Mei et al. (2021); Gonon et al. (2020); Gonon (2023). In particular, the weights of
the hidden layers are randomly initialized and fixed and only the weights of the last layer, corresponding
toθin(1), are optimized. We note that by our choice for the number of hidden nodes to equal the total
number of samples, the algorithms can in principle learn to have perfect replication on the training set, i.e.,
lossl(Θ(t),w;D) = 0. For a fair comparison between the algorithms, we use the same randomly generated
datasets and the same randomized neural networks for each of the methods.
Figure 2 shows the averaged results for 10 runs each with a different set of randomly generated datasets and
a different randomized neural network as fKRR. For the regret-optimal algorithm and for its accelerated
version we use T= 103iterations, while we use 105steps for gradient descent with learning rate 7·10−5.
We note that for learning rates ≥8·10−5the training becomes unstable such that the loss explodes. The
gradient descent method gradually but slowly reduces the loss. Since the gradient descent method makes so
small parameter updates (and since λ= 0), the regret is nearly 0and the energyLnearly equal to the loss.
Even though it runs 100times the number of iterations of our algorithms, it would still need much more
training to achieve the same loss. On the other hand, the regret-optimal algorithm and its accelerated version
behave nearly indistinguishably, reducing the loss nearly to 0and piling up a little bit of regret. We note
that by increasing the number of iterations, our algorithms could reduce the loss further by making smaller
parameter updates and therefore piling up less regret, allowing them to focus even more on reducing the loss.
In the regime where λ>0the behaviour of the algorithm changes drastically. In particular, since deviating
from Θ(0) = Θ⋆is penalised, the algorithm remains at the starting point in the initial steps and only starts to
converge when approaching T. The exact behaviour depends on the choice of λ(and is additionally influenced
by the choice of β). In Figure 3 we compare the behaviour for λ= 10−4andλ= 2while otherwise using the
same setup as before. For the smaller value of λ, the algorithm starts the decent earlier and decreases the
loss by more compared to the larger value for λ, where the algorithm deviates from Θ⋆only in the very last
steps.
3.2 American Option Pricing
We test our algorithm in the context of the optimal stopping problem for American option pricing, a
computationally highly challenging problem which has recently become tractable also in higher dimensional
6Also called a random feature model or an extreme learning machine
12Under review as submission to TMLR
0 20000 40000 60000 80000 100000
iterations0102030405060Loss & 
RO loss
RO 
ARO loss
ARO 
GD loss
GD 
0 200 400 600 800 1000
iterations0102030405060Loss & 
RO loss
RO 
ARO loss
ARO 
GD loss
GD 
500 600 700 800 900 1000
iterations2.55.07.510.012.515.017.520.0Loss & 
RO loss
RO 
ARO loss
ARO 
Figure 2: Loss and energy Lfor gradient descent (GD), the regret-optimal algorithm (RO) and the accelerated
regret-optimal algorithm (ARO). Means and standard deviations over 10 runs are shown. Left: all iterations;
middle: first 1000 iterations; right: iterations 500 to 1000 for our algorithms.
0 200 400 600 800 1000
iterations102030405060Loss & 
RO loss
RO 
ARO loss
ARO 
0 200 400 600 800 1000
iterations2030405060Loss & 
RO loss
RO 
ARO loss
ARO 
Figure 3: Loss and energy Lfor the regret-optimal algorithm (RO) and the accelerated regret-optimal
algorithm (ARO). Means and standard deviations over 10 runs are shown. Left: λ= 10−4; right:λ= 2.
settings through deep learning (Becker et al., 2019; Hu, 2020; Becker et al., 2021; Herrera et al., 2023). This
is a good example for our purpose for two reasons. On the one hand, Herrera et al. (2023) did show that
randomized neural networks, which are one class of fKRR, are well suited to (approximately) solve these
optimal stopping problems. Moreover, it was shown that using randomized neural networks can heavily
improve the computation time, which is also of importance to us since we need to solve several optimizations
problems for a potentially large number of datasets. On the other hand, to solve the optimal stopping problem
one needs to generate synthetic datasets. Hence, this problem allows us to generate multiple synthetic
datasets of different sizes and varying similarity on which we can test the transfer-learning capabilities of our
algorithm. The performance can be measured by comparing the achieved prices with those of several baseline
methods.
We first provide a short recap of the optimal stopping problem in American option pricing and then present
our transfer-learning experiments.
3.2.1 Background: American Option Pricing and Optimal Stopping
An American option is a financial derivative that gives its owner the right to execute a certain predetermined
trade at any time between the start date and the maturity of the option. For example, a call (put) option
gives its owner the right to buy (sell) a certain stock from the writer of the option for a fixed price K, called
13Under review as submission to TMLR
the strike. If the current stock price Xtis higher (lower) than Kthen the option owner can make a profit
of(Xt−K)+(or(K−Xt)+respectively) by directly selling (buying) the stock again in the open market
after having exercised their right to buy (sell) from the option writer. While American options allow for
continuous-in-time execution, it is common practice to approximate them via Bermudan options, which can
be executed on a predetermined time grid t0<···tM. We briefly overview the American option pricing
problem, details of which can be found in (Peskir & Shiryaev, 2006, Chapter 25).
In order to capture the computational challenges posed by the American option pricing problem in higher
dimensions, we consider a stochastic process (Xt)M
t=0∈(Rd)(M+1)describing the price of dstocks on the time
grid and let (Zt)M
t=0∈R(M+1)be the corresponding discounted payoff process of an American option written
on a one-dimensional process obtained from Xt(e.g. the largest price among the dstocks). We define the
filtration F= (Ft)M
t=0generated by XviaFt=σ(Xs|s≤t). Then the price of the American option is given
by the starting value U0of theSnell envelope (Snell, 1952), given by
Um= supτ∈TmE[Zτ|Fm], (21)
whereTmis the set of all stopping times τ≥m. The smallest optimal stopping time is
τM:=M, (22)
τm:=/braceleftigg
m,ifZm≥E[Um+1|Fm],
τm+1,otherwise,
hence, to compute the price of the American option it suffices to compute the continuation values E[Um+1|Fm]
for allm. In the following, we will use the randomized least squares Monte Carlo (RLSM) algorithm of
Herrera et al. (2023), which utilises randomized neural networks to approximate the continuation values
via backward induction (note the backward recursive scheme that (21)and(22)imply). In particular, this
leads toMbackward-recursively defined regression problems that need to be solved sequentially. Since (21)
is a maximisation problem and since we only approximate the optimal stopping time, we compute lower
bounds of the price U0. The standard procedure is to fix some law for the process Xand to sample a dataset
ofν=ν1+ν2i.i.d. paths of X, whereν1of them are used for training the randomized neural networks
(approximating the continuation value functions) and ν2of them are used to evaluate the resulting price U0
(using these trained networks). Through the independence of the two parts of the dataset it is clear that any
over-fitting bias the networks might learn will notlead to prices that are too large. This is important, since
the quality of a method can therefore be evaluated by the resulting price, using a higher price indicating a a
better result.
3.2.2 Transfer Learning in American Option Pricing
In order to test the transfer learning capabilities of our algorithm, we do not generate a very large dataset of
the most plausible distribtution of Xas is standard practice. Instead we generate many smaller, more or
less similar datasets D1,...,DNout of whichD1is considered our main dataset on which we evaluate the
performance.
We always use the RLSM algorithm to price the American options and only vary the optimization method
used internally by RLSM to solve the Mrecursive regression problems arising at the exercise dates. Depending
on the chosen optimisation method only one, multiple or all datasets are taken into account to compute the
parameters of the randomized neural network at the given exercise date. We compare our regret-optimal
(RO) optimisation method of Theorem 2 to the following baselines.
•Local optimizer (LO- i).The local optimizers θ⋆
isolves the individual optimisation problem on
the datasetsDiwithout considering the other datasets.
•Mean Local optimizer (MLO). The mean of the local optimizers ¯θ⋆=1
N/summationtextN
i=1θ⋆
iis the bagging
aggregation.
•Joint optimizer (JO). The joint optimizer θ⋆
NwithN={1,...,N}solves the optimisation problem
of the pooled regime D=∪N
i=1Di.
14Under review as submission to TMLR
•Joint Subset optimizer. This is the joint optimizer for any subset I ⊂N of the datasets
DI=∪i∈IDi.
In this application we are only interested in the terminal parameters Θ(T). We know that our algorithm
yields the parameters optimising (6)for anyT≥1since there exists a closed-form solution. Hence, we choose
T= 1which is most specialized on minimizing (3), while the influence of the running costs (26)can be
controlled via the choice of λandβ. Additionally, this minimizes the computation time for our algorithm.
As fKRR we use the same randomized neural networks as in Section 3.1. The random weights of the
randomized neural networks, as well as the randomly sampled paths both introduce some randomness into the
pricing problem. Therefore we follow Herrera et al. (2023) by always running the experiments nruns= 100
times with different seeds and taking means over the prices. For easier comparison, we then consider the
relative performance ( RP) on our main dataset, computed as the mean price of any given method divided
by the mean price of the local optimizer on the main dataset. Additionally, we compute asymptotically
valid (by the central limit theorem) 95%-confidence-intervals as RP±z0.975ˆσ/√nruns, wherez0.975is the
0.975-quantile of the standard normal distribution and ˆσis the standard deviation of RP(computed as the
sample standard deviation of the prices over the runs divided by the mean price of the local optimizer on the
main dataset). To make the results comparable between the optimization methods, we use the same seeds
for all of them, such that run ialways uses the exact same paths and the exact same random weights in
the randomized neural network, independently of the chosen method. Moreover, we always use ν2≥50,000
evaluation paths, such that the noise of the Monte Carlo approximation of the expectations is relatively small
when evaluating the prices, even if we use much smaller training set sizes ν1.
In the first experiment we study the ability of our method to transfer knowledge of those datasets, out a
large number of small equally-sized datasets, benefiting the main task.
Experiment 1. In this experiment we generate 13different datasets, all of a Heston model with ν1= 100
training samples but with different parameters. The Heston model is defined as
dXt= (r−δ)Xtdt+√vtXtdWt,
dvt=−k(vt−v∞)dt+σ√vtdBt,(23)
whereWandBare two Brownian motions with correlation ρ∈[−1,1]. The main dataset D1has the
parameters r= 0.05,δ= 0.1,k= 2,v∞= 0.01,σ= 0.2,ρ=−0.3and starting values X0= 100andv0=v∞.
The other datasets are define as all combinations of r∈{0.05,0.5},σ∈{0.15,0.2,0.25},v∞∈{0.005,0.015}
and otherwise the same parameters as the main dataset. For all datasets we use a max call option with
strikeK= 100ond= 2i.i.d. stocks of the given model with M= 9equidistant exercise dates until the
maturityTmat= 3. The discounting factor is e−rTmatt/Mfor0≤t≤M. Moreover, we use a randomized
neural network with one hidden layer with 300nodes and all methods use the ridge coefficient κ= 2and,
where applicable, regret coefficients λ= 2,β= 1.
We report the results in Table 1. For our regret-optimal method (RO) we use information sharing levels
η∈{10,100,500}and see that η= 100leads to the best result outperforming the local optimizer of the main
dataset (LO-1) by 9%. It also outperforms all other local optimizers (LO- i), the mean local optimizer (MLO)
and the joint optimizer (JO). For η= 10we have similar performance as LO-1 and for η= 500we also have
significant outperformance, though smaller than for η= 100.
The 6 datasets with r= 0.05(LO-2,..., LO-7) are relatively similar to the main dataset (with RP≥0.96),
while the other 6 (LO-8, ..., LO-13) are quite different (with RP < 0.73). Therefore, we claim that our
algorithm should mainly transfer knowledge from those 6 datasets with r= 0.05while avoiding to learn
something wrong from the others. In line with this we provide results of the joint optimizer on the 7 datasets
withr= 0.05, which performs better than the other baselines and outperforms LO-1 by 6.2%. In particular,
this confirms that the datasets with r= 0.05are those to transfer knowledge from, since the joint optimizer
for all datasets performs worse than LO-1. The joint subset optimizer is significantly outperformed by our
regret-optimal algorithm RO ( η= 100) by about 3%, which shows that our method constitutes a better way
for transfer learning than simply selecting the datasets most similar to D1.
15Under review as submission to TMLR
We additionally compare our method to the local optimizer for the main dataset which uses either 700(the
number of training samples in the 7 similar datasets with r= 0.05) or50,000training samples. Importantly,
both are just references that would not be available in a real world setting where the datasets are limited.
The first one only leads to a slight increase in RPof about 1%, which shows that our method extracts nearly
as much knowledge out of the (same amount of) samples of the similar datasets with r= 0.05. The second
one (which is considered a good proxy for the true American option price on the main dataset) has about
10%better performance. This also shows the limitations of our method, in particular, it can only transfer as
much knowledge as available in the other datasets.
Table 1: Relative performance (RP) and 95%-confidence-intervals for different optimization methods in
Experiment 1. We compare the local optimizers on the different datasets (LO- n), with the mean local
optimizer (MLO), the joint optimizer (JO) and our regret-optimal method (RO). The “oracle” local optimizer
on the main dataset with additional training samples (standard: 100samples per dataset) is included.
When the information sharing parameter ηis set to 100, the regret-optimal (RO) algorithm outperforms all
“non-oracle” baselines.
method RP 95%-CI
LO-1 1.000 [0.991; 1.009]
LO-2 0.985 [0.977; 0.994]
LO-3 0.966 [0.958; 0.973]
LO-4 0.975 [0.966; 0.984]
LO-5 0.966 [0.957; 0.975]
LO-6 0.979 [0.971; 0.988]
LO-7 0.978 [0.971; 0.986]
LO-8 0.721 [0.711; 0.730]
LO-9 0.720 [0.711; 0.729]
LO-10 0.706 [0.696; 0.716]
LO-11 0.725 [0.715; 0.736]
LO-12 0.707 [0.697; 0.716]
LO-13 0.718 [0.708; 0.727]
MLO 0.823 [0.813; 0.833]
JO 0.886 [0.878; 0.894]
JO (datasets 1-7) 1.062 [1.057; 1.067]
RO (η= 10) 1.000 [0.990; 1.010]
RO (η= 100 ) 1.090 [1.086; 1.095]
RO (η= 500) 1.050 [1.043; 1.057]
LO-1 (700 tr. samp.) 1.100 [1.097; 1.104]
LO-1 (50K tr. samp.) 1.194 [1.192; 1.195]
In the second experiment we test the ability of our method to transfer knowledge from a small dataset which
is similar to the small main dataset, even if it is confronted with a very different and much larger (dominating
in terms of samples) dataset.
Experiment 2. In this experiment we generate 3 different datasets. The main dataset is generated from a
rough Heston model
dXt= (r−δ)Xtdt+√vtXtdWt,
16Under review as submission to TMLR
vt=v0+/integraldisplayt
0(t−s)H−1/2
Γ(H+ 1/2)κ(v∞−vs)ds+/integraldisplayt
0(t−s)H−1/2
Γ(H+ 1/2)σ√vsdBs,
which is similar to the Heston model except that the volatility is a rough process with Hurst parameter
H∈(0,1/2], where the case H= 1/2coincides with the standard Heston model. We use H= 0.1and
otherwise the same parameters as for the main dataset in Experiment 1. The second and third dataset
generated from standard Heston model (i.e. H= 1/2), with the same parameters, except for the drift
r= 0.5for the second dataset. In particular, dataset 3 is the closest non-rough dataset to the main dataset,
while dataset 2 is quite different. For datasets 1 and 3 we use ν1= 100training samples, while we use
ν1= 50,000for dataset 2, such that it dominates the others in its size. All the other choices are the same as
in Experiment 1.
The results are reported in Table 2. The baselines MLO and JO perform significantly worse than the LO on
the main dataset. As expected, the JO suffers much more from the dominating dataset 2 than the MLO.
Indeed, the MLO would suffer more from a large number of datasets that are quite different from the main
dataset, no matter their sample size. In contrast to this, our regret optimal method with η∈{50,100}
outperforms the LO-1 by 4%, showing that also a quite different dataset dominating in sample size is not a
problem.
Table 2: Relative performance (RP) and 95%-confidence-intervals for different optimization methods in
Experiment 2. We compare the local optimizers on the different datasets (LO- n), the mean local optimizer
(MLO), the joint optimizer (JO) and our regret-optimal method (RO). With η= 100RO outperforms all
baselines.
method RP 95%-CI
LO-1 1.000 [0.993; 1.007]
LO-2 0.773 [0.765; 0.782]
LO-3 1.003 [0.994; 1.012]
MLO 0.932 [0.920; 0.944]
JO 0.763 [0.754; 0.772]
RO (η= 10) 0.993 [0.985; 1.000]
RO (η= 50) 1.040 [1.034; 1.046]
RO (η= 100 ) 1.040 [1.034; 1.047]
Both of these experiments show that our regret optimal method is well suited for automatic (up to the
hyper-parameter η) dataset selection in transfer learning tasks that have potentially large numbers of datasets
and do not allow a (manual) pre-selection. In particular, neither a large number of “bad” datasets (of which
the MLO suffers) nor some “bad” datasets that dominate in sample size (of which the JO suffers) constitute
a problem for our regret-optimal method. Moreover, the sample efficiency when transferring knowledge from
similar datasets to the main task is very high (outperforming the JO on the respective subset of similar
datasets and nearly being on par with the LO on the main dataset using a larger training sample), as we saw
in Experiment 1.
4 Conclusion
In this work we presented a regret-optimal algorithm for federated transfer learning based on finite-rank kernel
ridge-regressors (fKRR). Besides the theoretical properties of this algorithm, we also provided experiments
demonstrating the transfer learning capabilities of our method in the context of American option pricing.
One possible future research direction is to apply our transfer learning method to real world datasets, where
the similarity between the datasets is a priori not easy to quantify. From the theoretical side we would like to
extend our method to regression models that do not necessarily permit a closed-form solution.
17Under review as submission to TMLR
References
Aksamit, A., Deng, S., Obłój, J., and Tan, X. The robust pricing–hedging duality for american options in
discrete time financial markets. Mathematical Finance , 29(3):861–897, 2019.
Alquier, P. User-friendly introduction to pac-bayes bounds. arXiv preprint arXiv:2110.11216 , 2021.
Amini, A. A. Spectrally-truncated kernel ridge regression and its free lunch. Electron. J. Stat. , 15(2):
3743–3761, 2021. doi: 10.1214/21-ejs1873. URL https://doi.org/10.1214/21-ejs1873 .
Amini, A. A., Baumgartner, R., and Feng, D. Target alignment in truncated kernel ridge regression. arXiv
preprint arXiv:2206.14255 , 2022.
Argyriou, A., Evgeniou, T., and Pontil, M. Multi-task feature learning. Advances in neural information
processing systems , 19, 2006.
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R. Fine-grained analysis of optimization and generalization
for overparameterized two-layer neural networks. In International Conference on Machine Learning , pp.
322–332. PMLR, 2019.
Baxter, J. A model of inductive bias learning. J. Artificial Intelligence Res. , 12:149–198, 2000. ISSN 1076-9757.
doi: 10.1613/jair.731. URL https://doi.org/10.1613/jair.731 .
Başar, T. and Olsder, G. J. Dynamic Noncooperative Game Theory, 2nd Edition . Society for Industrial and
Applied Mathematics, 1998. doi: 10.1137/1.9781611971132. URL https://epubs.siam.org/doi/abs/10.
1137/1.9781611971132 .
Becker, S., Cheridito, P., and Jentzen, A. Deep optimal stopping. J. Mach. Learn. Res. , 20:Paper No. 74, 25,
2019. ISSN 1532-4435.
Becker, S., Cheridito, P., Jentzen, A., and Welti, T. Solving high-dimensional optimal stopping problems
using deep learning. European J. Appl. Math. , 32(3):470–514, 2021. ISSN 0956-7925. doi: 10.1017/
S0956792521000073. URL https://doi.org/10.1017/S0956792521000073 .
Bertsekas, D. Convex optimization algorithms . Athena Scientific, 2015.
Bordelon, B., Canatar, A., and Pehlevan, C. Spectrum dependent learning curves in kernel regression and
wide neural networks, 2021.
Briceño Arias, L. M. and Roldán, F. Four-operator splitting via a forward-backward-half-forward algorithm
with line search. J. Optim. Theory Appl. , 195(1):205–225, 2022. ISSN 0022-3239,1573-2878. doi: 10.1007/
s10957-022-02074-3. URL https://doi.org/10.1007/s10957-022-02074-3 .
Carmona, R., Delarue, F., et al. Probabilistic theory of mean field games with applications I-II . Springer,
2018.
Casgrain, P. A latent variational framework for stochastic optimization. In Wallach, H., Larochelle, H.,
Beygelzimer, A., d 'Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information
Processing Systems , volume 32. Curran Associates, Inc., 2019a. URL https://proceedings.neurips.cc/
paper/2019/file/248024541dbda1d3fd75fe49d1a4df4d-Paper.pdf .
Casgrain, P. A latent variational framework for stochastic optimization. Advances in Neural Information
Processing Systems , 32, 2019b.
Casgrain, P. and Kratsios, A. Optimizing optimizers: Regret-optimal gradient descent algorithms. In
Conference on Learning Theory , pp. 883–926. PMLR, 2021.
Cheng, T. S., Lucchi, A., Kratsios, A., Belius, D., and Dokmanić, I. A theoretical analysis of the test error of
finite-rank kernel ridge regression. Neural Information Processing Systems , 2023.
18Under review as submission to TMLR
Combettes, P. L. and Pesquet, J.-C. Stochastic approximations and perturbations in forward-backward
splitting for monotone operators. Pure Appl. Funct. Anal. , 1(1):13–37, 2016. ISSN 2189-3756,2189-3764.
Cuchiero, C., Gonon, L., Grigoryeva, L., Ortega, J.-P., and Teichmann, J. Discrete-time signatures and
randomness in reservoir computing. IEEE Transactions on Neural Networks and Learning Systems , 33(11):
6321–6330, 2021.
Dai Pra, P., Meneghini, L., and Runggaldier, W. J. Connections between stochastic control and dynamic
games.Math. Control Signals Systems , 9(4):303–326, 1996. ISSN 0932-4194. doi: 10.1007/BF01211853.
URL https://doi.org/10.1007/BF01211853 .
Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic
optimization. J. Mach. Learn. Res. , 12:2121–2159, 2011. ISSN 1532-4435.
Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In
International conference on machine learning , pp. 1126–1135. PMLR, 2017.
Fliege, J., Vaz, A. I. F., and Vicente, L. N. Complexity of gradient descent for multiobjective optimization.
Optimization Methods and Software , 34(5):949–959, 2019.
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. Linearized two-layers neural networks in
high dimension. The Annals of Statistics , 49(2):1029 – 1054, 2021. doi: 10.1214/20-AOS1990. URL
https://doi.org/10.1214/20-AOS1990 .
Gonon, L. Random feature neural networks learn black-scholes type pdes without curse of dimensionality.
Journal of Machine Learning Research , 24(189):1–51, 2023.
Gonon, L., Grigoryeva, L., and Ortega, J.-P. Risk bounds for reservoir computing. Journal of Machine
Learning Research (JMLR) , 21, 2020.
Gonon, L., Grigoryeva, L., and Ortega, J.-P. Approximation bounds for random neural networks and reservoir
systems. Ann. Appl. Probab. , 33(1):28–69, 2023. ISSN 1050-5164,2168-8737. doi: 10.1214/22-aap1806.
URL https://doi.org/10.1214/22-aap1806 .
Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends ®in Optimization , 2
(3-4):157–325, 2016.
Herrera, C., Krach, F., Ruyssen, P., and Teichmann, J. Optimal stopping via randomized neural networks,
2023. URL https://www.aimsciences.org/article/id/65797df64aaea5271070548a .
Horn, R. A. and Johnson, C. R. Matrix Analysis . Cambridge University Press, 2 edition, 2012. doi:
10.1017/CBO9781139020411.
Hou, Z. and Obłój, J. Robust pricing–hedging dualities in continuous time. Finance and Stochastics , 22(3):
511–567, 2018.
Howard, J. and Ruder, S. Universal language model fine-tuning for text classification. arXiv preprint
arXiv:1801.06146 , 2018.
Hu, R. Deep learning for ranking response surfaces with applications to optimal stopping problems. Quant.
Finance, 20(9):1567–1581, 2020. ISSN 1469-7688. doi: 10.1080/14697688.2020.1741669. URL https:
//doi.org/10.1080/14697688.2020.1741669 .
Huang, M. and Yang, X. Linear quadratic mean field social optimization: Asymptotic solvability and
decentralized control. Applied Mathematics & Optimization , 84(Suppl 2):1969–2010, 2021.
Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural
networks. Advances in neural information processing systems , 31, 2018.
Jacot, A., Simsek, B., Spadaro, F., Hongler, C., and Gabriel, F. Implicit regularization of random feature
models. In International Conference on Machine Learning , pp. 4631–4640. PMLR, 2020.
19Under review as submission to TMLR
Jia, Y., Johnson, M., Macherey, W., Weiss, R. J., Cao, Y., Chiu, C.-C., Ari, N., Laurenzo, S., and Wu, Y.
Leveraging weakly supervised data to improve end-to-end speech-to-text translation. In ICASSP 2019-2019
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 7180–7184.
IEEE, 2019.
Khaled, A., Mishchenko, K., and Richtarik, P. Tighter theory for local sgd on identical and heterogeneous
data. In Chiappa, S. and Calandra, R. (eds.), Proceedings of the Twenty Third International Conference
on Artificial Intelligence and Statistics , volume 108 of Proceedings of Machine Learning Research , pp.
4519–4529. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/bayoumi20a.html .
Kimeldorf, G. and Wahba, G. Some results on Tchebycheffian spline functions. J. Math. Anal. Appl. , 33:
82–95, 1971. ISSN 0022-247X. doi: 10.1016/0022-247X(71)90184-3. URL https://doi.org/10.1016/
0022-247X(71)90184-3 .
Kimeldorf, G. S. and Wahba, G. A correspondence between Bayesian estimation on stochastic processes
and smoothing by splines. Ann. Math. Statist. , 41:495–502, 1970. ISSN 0003-4851. doi: 10.1214/aoms/
1177697089. URL https://doi.org/10.1214/aoms/1177697089 .
Kumar, A. and Daumé III, H. Learning task grouping and overlap in multi-task learning. International
Conference on Machine Learning (ICML) , 2012.
Kwon, D., Park, J., and Hong, S. Tighter regret analysis and optimization of online federated learning. arXiv
e-prints, pp. arXiv–2205, 2022.
Lax, P. D. Linear algebra and its applications . Pure and Applied Mathematics (Hoboken). Wiley-Interscience,
Hoboken, NJ, second edition, 2007. ISBN 978-0-471-75156-4.
Le Gall, F. Powers of tensors and fast matrix multiplication. In ISSAC 2014—Proceedings of the 39th
International Symposium on Symbolic and Algebraic Computation , pp. 296–303. ACM, New York, 2014.
doi: 10.1145/2608628.2608664. URL https://doi.org/10.1145/2608628.2608664 .
Li, Q., Chen, L., Tai, C., and E, W. Maximum principle based algorithms for deep learning. J. Mach. Learn.
Res., 18:Paper No. 165, 29, 2017a. ISSN 1532-4435.
Li, Q., Tai, C., and Weinan, E. Stochastic modified equations and adaptive stochastic gradient algorithms.
InInternational Conference on Machine Learning , pp. 2101–2110. PMLR, 2017b.
Li, Q., Chen, L., Tai, C., and E, W. Maximum principle based algorithms for deep learning. Journal of
Machine Learning Research , 18(165):1–29, 2018. URL http://jmlr.org/papers/v18/17-653.html .
Longstaff, F. A. and Schwartz, E. S. Valuing american options by simulation: a simple least-squares approach.
The review of financial studies , 14(1):113–147, 2001.
Lowry, M. and Schwert, G. W. Ipo market cycles: Bubbles or sequential learning? The Journal of Finance ,
57(3):1171–1200, 2002.
Mangasarian, O. L. Parallel gradient distribution in unconstrained optimization. SIAM J. Control Optim. ,
33(6):1916–1925, 1995. ISSN 0363-0129. doi: 10.1137/S0363012993250220. URL https://doi.org/10.
1137/S0363012993250220 .
McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. y. Communication-Efficient Learning
of Deep Networks from Decentralized Data. In Singh, A. and Zhu, J. (eds.), Proceedings of the 20th
International Conference on Artificial Intelligence and Statistics , volume 54 of Proceedings of Machine
Learning Research , pp. 1273–1282. PMLR, 20–22 Apr 2017. URL https://proceedings.mlr.press/v54/
mcmahan17a.html .
Mei, S. and Montanari, A. The generalization error of random features regression: precise asymptotics
and the double descent curve. Comm. Pure Appl. Math. , 75(4):667–766, 2022. ISSN 0010-3640. doi:
10.1002/cpa.22008. URL https://doi.org/10.1002/cpa.22008 .
20Under review as submission to TMLR
Mei, S., Misiakiewicz, T., and Montanari, A. Generalization error of random feature and kernel methods:
hypercontractivity and kernel matrix concentration. Applied and Computational Harmonic Analysis , 2021.
Pavasovic, K. L., Rothfuss, J., and Krause, A. Mars: Meta-learning as score matching in the function space.
arXiv preprint arXiv:2210.13319 , 2022.
Peskir, G. and Shiryaev, A. Optimal stopping and free-boundary problems . Lectures in Mathematics ETH
Zürich. Birkhäuser Verlag, Basel, 2006. ISBN 978-3-7643-2419-3; 3-7643-2419-8.
Prashanth, L. and Bhat, S. P. A wasserstein distance approach for concentration of empirical risk estimates.
The Journal of Machine Learning Research , 23(1):10830–10890, 2022.
Ren, Z. and Lee, Y. J. Cross-domain self-supervised multi-task feature learning using synthetic imagery. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 762–771, 2018.
Rothfuss, J., Fortuin, V., Josifoski, M., and Krause, A. Pacoh: Bayes-optimal meta-learning with pac-
guarantees. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine
Learning , volume 139 of Proceedings of Machine Learning Research , pp. 9116–9126. PMLR, 18–24 Jul 2021.
URL https://proceedings.mlr.press/v139/rothfuss21a.html .
Sener, O. and Koltun, V. Multi-task learning as multi-objective optimization. In Bengio, S., Wallach, H.,
Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information
Processing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf .
Snell, J. L. Applications of martingale system theorems. Trans. Amer. Math. Soc. , 73:293–312, 1952. ISSN
0002-9947. doi: 10.2307/1990670. URL https://doi.org/10.2307/1990670 .
Touzi, N. and Tourin, A. Optimal stochastic control, stochastic target problems, and backward SDE , volume 29.
Springer, 2013.
Wang, R., Hyndman, C., and Kratsios, A. The entropic measure transform. Canad. J. Statist. , 48(1):97–129,
2020. ISSN 0319-5724. doi: 10.1002/cjs.11537. URL https://doi.org/10.1002/cjs.11537 .
Xue, Y., Liao, X., Carin, L., and Krishnapuram, B. Multi-task learning for classification with Dirichlet
process priors. J. Mach. Learn. Res. , 8:35–63, 2007. ISSN 1532-4435.
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural networks?
Advances in neural information processing systems , 27, 2014.
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. Gradient surgery for multi-task learning.
In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information
Processing Systems , volume 33, pp. 5824–5836. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper_files/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf .
Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I
13, pp. 818–833. Springer, 2014.
Zhao, J., Lucchi, A., Proske, F. N., Orvieto, A., and Kersting, H. Batch size selection by stochastic optimal
control. In Has it Trained Yet? NeurIPS 2022 Workshop , 2022. URL https://openreview.net/forum?
id=Te-9Ig5ftj .
A Game Theoretic Interpretation of Regret-Optimal Algorithm
We now offer a game-theoretic interpretation of the regret functional, as defined in (6). This interpretation
views the user as a central planner whose objective is to organize a system of individual agents, representing
the pretrained KRR models, to maximize the singular goal of identifying a parameter optimizing (5). We now
interpret the roles and interactions of the central planner and each agent as defined by our regret-optimization
problem.
21Under review as submission to TMLR
The Central Planner: The purpose of the central planner is to avoid the situation whereby the structure
of the different datasets is ignored in the model selection problem, by merging them into a single dataset
Ddef.=∪N
i=1Diand thereby reducing the problem to (2). Though intuitively simple, such merging approach
can be particularly disadvantageous in the presence of heterogeneity among the datasets, since a dataset
that is potentially very different from the focal dataset D1would be equally influential in the regression
problem (2). Instead, the user, acting as a central planner, organizes the model selection problem through a
cooperative objective
l(Θ,w;D)def.=N/summationdisplay
i=1wi|Di|/summationdisplay
j=1(fθw(xi
j)−yi
j)2,
withθwdef.=N/summationdisplay
i=1wiθi,(24)
where Θdef.=(θ⊤
1,···,θ⊤
N)⊤∈RNp×1. Here, the central planner ascribes the influence of each agent on the
model selection problem through the weight vector w= (w1,···,wN)∈[0,1]Nwith/summationtextN
i=1wi= 1. As can
be seen in (24), this influence is two-fold: on the one hand, it impacts the cooperative objective lin the
aggregation of the SSE of each player; on the other hand, it affects the proportion of each player’s preferred
parameter choice entering into the collectively selected parameter θw. The choice of wis approached in
different ways in the literature, e.g. by bagging reminiscent of mean-field games (Carmona et al., 2018), by
data-driven weighting procedures (Baxter, 2000), Bayesian aggregation (Xue et al., 2007; Rothfuss et al.,
2021; Pavasovic et al., 2022), or with dictionaries (Argyriou et al., 2006).
The Agents: When acting in isolation, each agent’s preferred model is determined by optimizing a version
of(2)using only their dataset. Under mild conditions7onf, this corresponds to an individual parameter
selection
θ⋆
i∈argminθ∈Θ/summationdisplay
(x,y)∈Di(fθ(x)−y)2+κ∥θ∥2, (25)
fori= 1,...,N, which we assume to be fixed and known to all agents prior to their collective parameter
selection. Taking into account the other agents, each agent actsby specifying an iterative deterministic
algorithm of the form (4), with the intent of maximizing their influence on the joint model selection
problem (24). Thus, the ithplayer wants the jointly selected model, specified by θw(T) =/summationtextN
n=1wiθi(T),
to be as close to θ⋆
ias possible, thereby maximally encoding the characteristics of the ithdataset into the
selected model fθw(T). The accumulated regretwhich theithplayer incurs by deploying an algorithm θ0···T
i
that deviates from their preferred selection θ∗
iis measured by
T−1/summationdisplay
t=0λ∥θi(t+ 1)−θ⋆
i∥2
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Preference Strength+β∥∆θi(t)∥2
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Algorithm Stability, (26)
whereλ>0is a hyperparameter quantifying the player’s attachment towards their preferred model choice
θ⋆
iand the hyperparameter β >0quantifies the the stability of the algorithm during the iterations, where
∆θi(t)def.=θi(t+ 1)−θi(t).
The central planner then organizes the action Θ0···Tdef.={Θ(t)}T
t=0of the system of Nplayers to reach the
objective (24) while encoding their individual regrets (26), where
Θ(t) = (θ1(t)⊤,...,θN(t)⊤)⊤∈RNp×1.
7E.g. coercivity and lower semi-continuity in θ.
22Under review as submission to TMLR
This is achieved by coupling the individual agent’s regret functionals (26)through the systemic regret
functional
R(Θ0···T)def.=l(Θ(T),w;D)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Cooperative Objective−l⋆+T−1/summationdisplay
t=0/parenleftbig
λ∥Θ(t+ 1)−Θ⋆∥2
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Preference Strength+β∥∆Θ(t)∥2
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Algo. Stability/parenrightbig
, (27)
where
Θ⋆def.= (θ⋆⊤
1,···,θ⋆⊤
N)⊤∈RNp×1(28)
encodes the individual preferences of the players,
∆Θ(t)def.= Θ(t+ 1)−Θ(t) (29)
quantifies the integrative updates of any candidate optimizing sequence Θ0···T, and the ideal terminal loss is
l⋆def.= min
Ξ∈RNp×1l(Ξ,w;D)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Cooperative Sub-optimality.
The term (l(·,·;·)−l⋆)measures the central planner’s regret in failing to optimize (24). The algorithm
stability term in (27)appears in forward-backward proximal splitting algorithms with quadratic objectives
(e.g. Combettes & Pesquet (2016); Briceño Arias & Roldán (2022)) and several standard proximal algorithms
(see (Bertsekas, 2015, Section 5.1)).
The systemic regret functional (27)acts as a performance criterion for any optimization algorithm, played by
Nplayers, initialized at an arbitrary Θ(0), and running for Titerations. In what follows, we take the initial
condition of the algorithm as Θ(0) = Θ⋆for simplicity, but other choices are possible. In other words, the
regret of each player is initialized at zero, as they all start from their individually preferred parameter θ⋆
i,
and starts to accumulate as the system moves away from Θ⋆towards the optimizer of (27).
B Proofs and Technical Results
Remark 2. We keep our notation light and use u⊤vto implement the Euclidean inner-product between any
two vectors u,v∈Rkfor anyk∈N+. We use|·|to denote the Euclidean norm of a vector or Fröbenius
norm of a matrix, whichever is applicable. For two square matrices AandB,A≤B(A<B, resp.) means
thatB−Ais positive semi-definite (positive definite, resp.); see (Lax, 2007, page 146 - Equation (7)) for
details.
Lemmas 1 and 2 give some elementary properties of matrix algebra, which will be used repeatedly in the
subsequent proofs.
Lemma 1. The Euclidean norm |·|of matrices has the following properties:
i). For any matrices A∈Rn1×n2andB∈Rn2×n3, we have that|AB|≤|A|·|B|;
ii). For any matrices A,B∈Rn1×n2, we have|A+B|≤|A|+|B|.
Proof.DenoteA= (Aij)1≤i≤n1,1≤j≤n2andB= (Bjk)1≤j≤n2,1≤k≤n3. By the Cauchy-Schwarz inequality, we
have for each column vector B·kofBthat
|AB·k|2=n1/summationdisplay
i=1/parenleftbign2/summationdisplay
j=1AijBjk/parenrightbig2≤n1/summationdisplay
i=1/parenleftbign2/summationdisplay
j=1A2
ijn2/summationdisplay
j=1B2
jk/parenrightbig
=/parenleftbign1/summationdisplay
i=1n2/summationdisplay
j=1A2
ij/parenrightbign2/summationdisplay
j=1B2
jk=|A|2n2/summationdisplay
j=1B2
jk.
It then follows that
|AB|2=|(AB·1,AB·2,...,AB·n3)|2=|AB·1|2+|AB·2|2+···+|AB·n3|2
23Under review as submission to TMLR
≤|A|2n2/summationdisplay
j=1n3/summationdisplay
k=1B2
jk=|A|2|B|2,
and assertion i) is proved.
ForA,B∈Rn1×n1, we have
|A+B|2=n1/summationdisplay
i=1n2/summationdisplay
j=1(Aij+Bij)2=n1/summationdisplay
i=1n2/summationdisplay
j=1A2
ij+n1/summationdisplay
i=1n2/summationdisplay
j=1B2
ij+ 2n1/summationdisplay
i=1n2/summationdisplay
j=1AijBij
≤|A|2+|B|2+ 2|A|2|B|2= (|A|+|B|)2,
which proves assertion ii).
Lemma 2. Letw1,w2, ...,wnbe positive numbers such that/summationtextn
k=1wk= 1. For anynmatricesA1, ...,
An∈Rn1×n2, it satisfies that/vextendsingle/vextendsingle/summationtextn
k=1wkAk/vextendsingle/vextendsingle2≤/summationtextn
k=1wk|Ak|2; particularly, when w1=w2=···=wn= 1/n,
we have/vextendsingle/vextendsingle/summationtextn
k=1Ak/vextendsingle/vextendsingle2≤n/summationtextn
k=1|Ak|2.
Proof.DenoteAk= (Ak,ij)1≤i≤n1,1≤j≤n2,k= 1, ...,n. By the Jensen’s inequality, we have
(w1A1,ij+···+wnAn,ij)2≤w1A2
1,ij+···+wnA2
n,ij.
It then follows that
|w1A1+···+wnAn|2=n1/summationdisplay
i=1n2/summationdisplay
j=1(w1A1,ij+···+wnAn,ij)2≤n1/summationdisplay
i=1n2/summationdisplay
j=1w1A2
1,ij+···+wnA2
n,ij
=w1n1/summationdisplay
i=1n2/summationdisplay
j=1A2
1,ij+···+wnn1/summationdisplay
i=1n2/summationdisplay
j=1A2
n,ij
=w1|A1|2+···+wn|An|2.
We begin with the relatively short proof of Theorem 1 establishing the optimality of the weights obtained in
Algorithm 1.
Proof of Theorem 1. Fori= 1,...,N, consider the probability measure
¯Pηdef.=N/summationdisplay
i=1[s1+η−si]+/summationtextN
k=1[s1+η−sk]+δθ⋆
i
on the discrete space {θ⋆
i}N
i=1topologized by the discrete metric. Any minimizer of (12)is a minimizer of the
following problem, and vice-versa
min
Pw∈P({θ⋆
i}N
i=1)1
|D1||D1|/summationdisplay
j=1/vextenddouble/vextenddoublefθ(u1
j)−y1
j/vextenddouble/vextenddouble2
2+ηDKL/parenleftbig
Pw∥¯Pη/parenrightbig
= min
Pw∈P({θ⋆
i}N
i=1)Eθ∼Pw/bracketleftigg
1
|D1||D1|/summationdisplay
j=1/vextenddouble/vextenddoublefθ(u1
j)−y1
j/vextenddouble/vextenddouble2
2/bracketrightigg
+ηDKL/parenleftbig
Pw∥¯Pη/parenrightbig
+ι(Pw≪¯Pη)
= min
Pw∈P({θ⋆
i}N
i=1)Eθ∼Pw/bracketleftbiggN/summationdisplay
i=1Iθ=θ⋆
isi/bracketrightbigg
+ηDKL/parenleftbig
Pw∥¯Pη/parenrightbig
+ι(Pw≪¯Pη), (30)
24Under review as submission to TMLR
where the map ι(Pw≪¯Pη)is an “indicator function in the sense of convex analysis”; that is, ι(Pw≪¯Pη) = 0
ifPwis absolutely continuous with respect to ¯Pηand∞otherwise. We may now apply (Wang et al., 2020,
Proposition 1)8to(30)deducing that it admits a minimizer Pw⋆=/summationtextN
i=1w⋆
iδθ⋆
iand its Radon-Nikodym
derivative with respect to the measure ¯Pηis
dPw⋆
¯Pη(θ⋆
i) =e/summationtextN
l=1Iθ⋆
l=θ⋆
isi/η
E¯Pη/bracketleftbigg
exp/parenleftbig/summationtextN
i=1Iθ=θ⋆
isi/parenrightbig/bracketrightbigg (31)
=esi/η
/summationtextN
k=1esk/η[s1+η−sk]+ /summationtextN
m=1[s1+η−sm]+
=/parenleftigN/summationdisplay
m=1[s1+η−sm]+/parenrightigesi/η
/summationtextN
k=1esk/η[s1+η−sk]+(32)
fori= 1,...,N. Since Pw⋆is a minimizer of (30)then we may deduce the expression for the optimal weights
w⋆by first noting that
Pw⋆def.=N/summationdisplay
i=1w⋆
iδθ⋆
i=N/summationdisplay
i=1dPw⋆
d¯Pη(θ⋆
i)/parenleftigg
[s1+η−si]+/summationtextN
j=1[s1+η−sj]+/parenrightigg
δθ⋆
i. (33)
Therefore, (31)- (32) and (33) imply that, for each i= 1,...,N, we have
w⋆
i=dPw⋆
d¯Pη(θ⋆
i)/parenleftigg
[s1+η−si]+/summationtextN
j=1[s1+η−sj]+/parenrightigg
=/parenleftigN/summationdisplay
m=1[s1+η−sm]+/parenrightigesi/η
/summationtextN
k=1esk/η[s1+η−sk]+/parenleftigg
[s1+η−si]+/summationtextN
j=1[s1+η−sj]+/parenrightigg
=esi/η[s1+η−si]+/summationtextN
k=1esk/η[s1+η−sk]+.
Next we now prove our main result, namely Theorem 2. The result is first rewritten, in the optimal
control-theoretic notation introduced in Section 2.1.
Theorem 8 (The Regret-Optimal Algorithm - Control Theoretic Form) .Suppose that Assumptions 1 and 2
hold. Then, the optimal control problem (6)and(9)admits a unique solution for t= 0,1,...,T−1such that
α(t) =−[(λ+β)INp+P(t+ 1)]−1/bracketleftbig
(λINp+P(t+ 1))Θ(t)−λΘ⋆+S(t+ 1)/bracketrightbig
, (34)
and the resulting cost is
L⋆(α;D) = Θ⋆⊤P(0)Θ⋆+ 2S⊤(0)Θ⋆+r(0). (35)
TheP(t),S(t), andr(t)in(34)and(35)are determined by the backward equation system on t= 1,...,T:
/braceleftigg
P(t) =βINp−β2[(λ+β)INp+P(t+ 1)]−1,
P(T) = [w⋆
1Ip,···,w⋆
NIp]⊤/parenleftbig/summationtextN
i=1w⋆
i/summationtext|Di|
j=1ui
jui⊤
j/parenrightbig
[w⋆
1Ip,···,w⋆
NIp],(36)
/braceleftigg
S(t) =β/bracketleftbig
(λ+β)INp+P(t+ 1)/bracketrightbig−1(S(t+ 1)−λΘ⋆),
S(T) =−[w⋆
1Ip,···,w⋆
NIp]⊤/summationtextN
i=1w⋆
i/summationtext|Di|
j=1ui
jyi
j,(37)
8This result is an extension of (Dai Pra et al., 1996, Proposition 2.3) which allows us to vary the influence of the regulative-
entropy term in (30).
25Under review as submission to TMLR


r(t) =−(S(t+ 1)−λΘ⋆)⊤[(λ+β)INp+P(t+ 1)]−1·(S(t+ 1)−λΘ⋆)
+λ∥Θ⋆∥2
2+r(t+ 1),
r(T) =/summationtextN
i=1w⋆
i/summationtext|Di|
j=1|yi
j|2.(38)
Proof of Theorem 8 (and therefore of Theorem 2). LetV(t,Θ)be the value function associated with the
optimal control problem (6)and(9). By the dynamic programming principle, V(t,Θ)satisfies the equation
V(t,Θ) = min
α/bracketleftig
λ|Θ +α−Θ⋆|2+β|α|2+V(t+ 1,Θ +α)/bracketrightig
, t= 0,1,...,T−1, (39)
V(T,Θ) =l(θw⋆;D). (40)
The loss function can be written as a linear quadratic function of Θ = [θ⊤
1,···,θ⊤
N]⊤such that
l(θw⋆;D)
=N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1/bracketleftig
ui⊤
jN/summationdisplay
k=1w⋆
kθk−yi
j/bracketrightig2
=N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1/bracketleftigN/summationdisplay
k=1w⋆2
kθ⊤
kui
jui⊤
jθk−2N/summationdisplay
k=1w⋆
kyi
jui⊤
jθk+|yi
j|2/bracketrightig
=/parenleftigN/summationdisplay
k=1w⋆
kθ⊤
k/parenrightig/parenleftigN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1ui
jui⊤
j/parenrightig/parenleftigN/summationdisplay
k=1w⋆
kθk/parenrightig
−2N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1yi
jui⊤
jN/summationdisplay
k=1w⋆
kθk+N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1|yi
j|2
=Θ⊤[w⋆
1Ip,···,w⋆
NIp]⊤/parenleftigN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1ui
jui⊤
j/parenrightig
[w⋆
1Ip,···,w⋆
NIp]Θ
−2N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1yi
jui⊤
j[w⋆
1Ip,···,w⋆
NIp]Θ +N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1|yi
j|2. (41)
We claim that V(t,Θ)takes the linear quadratic form
V(t,Θ) = Θ⊤P(t)Θ + 2S⊤(t)Θ +r(t). (42)
To see this, note from (40)and(41)thatV(T,Θ)takes the linear quadratic form of (42). By solving the
dynamic programming equation (39)att=T−1for the optimal αand substituting it back into the equation,
we obtainV(T−1,Θ)also taking the linear quadratic form of (42). By backward induction we have that
V(t,Θ)takes the form of (42) for all t= 0,...,T.
Remark 3. Herer(t)is a constant term independent of the “state variable” Θ, which we note exists because
the costL⋆defined by (6)contains terms independent of Θ(t).
Combining (40),(41), and(42)verifies that P(T)andS(T)satisfy the terminal condition as given by (36)
and (37). We substitute (42) into (39) and reorganize the terms to get
Θ⊤P(t)Θ + 2S⊤(t)Θ +r(t)
= min
α/braceleftig
α⊤[(λ+β)INp+P(t+ 1)]α+ 2[Θ⊤P(t+ 1) +S⊤(t+ 1) +λ(Θ⊤−Θ⋆⊤)]α
+ Θ⊤(λINp+P(t+ 1))Θ + 2( S⊤(t+ 1)−λΘ⋆)Θ +λ|Θ⋆|2+r(t+ 1)/bracerightig
. (43)
By the first order condition9for the right-hand side of (43) with respect to α, we have
0 =[(λ+β)INp+P(t+ 1)]α+ [(λINp+P(t+ 1))Θ +S(t+ 1)−λΘ⋆]. (44)
9NB, we can argue this way, since the problem is convexified upon fixing w⋆. In fact, this is the control-theoretic motivation
for using Algorithm 1 to decouple the optimization of wand ofθ·from one another, and not treating them as a single (non-convex
if left coupled) control problem.
26Under review as submission to TMLR
By Assumption 2, we have P(T) + (λ+ 1)INp>0, and therefore from (44)we obtain the optimal αtaking
the form of (34).
We further substitute αof the form (34) into the right side of (43) to obtain
Θ⊤P(t)Θ + 2S⊤(t)Θ +r(t)
=−/bracketleftbig
Θ⊤(λINp+P(t+ 1)) +S⊤(t+ 1)−λΘ⋆⊤/bracketrightbig
·/bracketleftbig
(λ+β)INp+P(t+ 1)/bracketrightbig−1·
/bracketleftbig
(λINp+P(t+ 1))Θ +S(t+ 1)−λΘ⋆/bracketrightbig
+ Θ⊤(λINp+P(t+ 1))Θ
+ 2[S⊤(t+ 1)−λΘ⋆⊤]Θ +λ|Θ⋆|2+r(t+ 1). (45)
Reorganizing the terms on right side of (45), we have
Θ⊤P(t)Θ + 2S⊤(t)Θ +r(t)
=−Θ⊤(λINp+P(t+ 1))[(λ+β)INp+P(t+ 1)]−1(λINp+P(t+ 1))Θ
+ Θ⊤(λINp+P(t+ 1))Θ + 2[ S⊤(t+ 1)−λΘ⋆⊤]Θ
−2[S⊤(t+ 1)−λΘ⋆⊤][(λ+β)INp+P(t+ 1)]−1(λINp+P(t+ 1))Θ
−(S⊤(t+ 1)−λΘ⋆⊤)[(λ+β)INp+P(t+ 1)]−1(S(t+ 1)−λΘ⋆)
+λ|Θ⋆|2+r(t+ 1). (46)
Matching the coefficients of the quadratic terms of Θon both sides of (46), we obtain
P(t) =−(λINp+P(t+ 1))[(λ+β)INp+P(t+ 1)]−1(λINp+P(t+ 1))
+λINp+P(t+ 1). (47)
The right side of (47) further simplifies to
−(λINp+P(t+ 1))[(λ+β)INp+P(t+ 1)]−1(λINp+P(t+ 1)) +λINp+P(t+ 1)
=−[(λ+β)INp+P(t+ 1)−βINp][(λ+β)INp+P(t+ 1)]−1(λINp+P(t+ 1))
+λINp+P(t+ 1)
=−(λINp+P(t+ 1)) +β[(λ+β)INp+P(t+ 1)]−1(λINp+P(t+ 1)) +λINp+P(t+ 1)
=β[(λ+β)INp+P(t+ 1)]−1(λINp+P(t+ 1))
=β[(λ+β)INp+P(t+ 1)]−1[(λ+β)INp+P(t+ 1)−βINp]
=βINp−β2[(λ+ 1)INp+P(t+ 1)]−1. (48)
Combining (47) and (48) gives (36) for t=T−1.
Matching the coefficients of the linear terms of Θon both sides of (46)and taking the transpose, we obtain
S(t) =−(λINp+P(t+ 1))[(λ+β)INp+P(t+ 1)]−1(S(t+ 1)−λΘ⋆)
+S(t+ 1)−λΘ⋆. (49)
The right side of (49) further simplifies to
−(λINp+P(t+ 1))[(λ+β)INp+P(t+ 1)]−1(S(t+ 1)−λΘ⋆) +S(t+ 1)−λΘ⋆
=−[(λ+β)INp+P(t+ 1)−βINp][(λ+β)INp+P(t+ 1)]−1(S(t+ 1)−λΘ⋆)
+S(t+ 1)−λΘ⋆
=−(S(t+ 1)−λΘ⋆) +β[(λ+β)INp+P(t+ 1)]−1(S(t+ 1)−λΘ⋆) +S(t+ 1)−λΘ⋆
=β[(λ+β)INp+P(t+ 1)]−1(S(t+ 1)−λΘ⋆). (50)
Combining (49) and (50) gives (37) for t=T−1.
27Under review as submission to TMLR
Matching the terms independent of Θon both sides of (46) gives
r(t) =−(S(t+ 1)−λΘ⋆)⊤[(λ+β)INp+P(t+ 1)]−1·(S(t+ 1)−λΘ⋆)
+λ|Θ⋆|2+r(t+ 1),
which is (38) for t=T−1.
SinceP(T) + (λ+β)INp>βINp, we have
P(T−1) =βINp−β2[(λ+β)INp+P(T)]−1>0.
By backward induction on t, we can show that αtakes the form of (34)for allt= 0,1,...,T−1, andPand
Ssatisfy the system (36) and (37).
Corollary 1. Under Assumptions 1 and 2, the solution PandSof the system (36)-(37)satisfies that for all
t= 1,2, ...,T,
|[(λ+β)INp+P(t)]−1|≤(λ+β)−1, (51)
|S(t)|≤C·N, (52)
whereC > 0is a constant depending on λ,β,(w⋆
1,...,w⋆
N),Θ⋆,Kx,KyandT.
Proof of Corollary 1. Let∥·∥be the spectral norm of a matrix such that ∥A∥def.= supx̸=0|Ax|/|x|. SinceP(t)
is positive semi-definite, we have (λ+β)INp+P(t)≥(λ+β)INpand
0≤[(λ+β)INp+P(t)]−1≤(λ+β)−1INp,
which, by the Courant-Fisher Theorem (see Horn & Johnson (2012)), implies that ∥[(λ+β)INp+P(t)]−1∥≤
∥(λ+β)−1INp∥= (λ+β)−1. Since all norms of finite dimensional normed spaces are equivalent, and in
particular,|·|≤∥·∥ on the space of Np×Np-matrices, then we find that
/vextendsingle/vextendsingle[(λ+β)INp+P(t)]−1/vextendsingle/vextendsingle≤∥[(λ+β)INp+P(t)]−1∥,
and thus/vextendsingle/vextendsingle[(λ+β)INp+P(t)]−1/vextendsingle/vextendsingle≤(λ+β)−1, which proves (51).
Next, we prove (52). By Lemmas 1 and 2, and (17), we obtain
|S(T)|2=/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightig/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1ui
jyi
j/vextendsingle/vextendsingle/vextendsingle2
≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i/vextendsingle/vextendsingle/vextendsingle|Di|/summationdisplay
j=1ui
jyi
j/vextendsingle/vextendsingle/vextendsingle2
≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
jyi
j|2≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i|Di|2K2
xK2
y
≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
iN2K2
xK2
y, (53)
and therefore the estimate (52) holds for t=T.
Assume by induction that (52) holds for t=u. By (37), Lemma 1, and the triangle inequality, we have
|S(u−1)|≤β/vextendsingle/vextendsingle/bracketleftbig
(λ+β)INp+P(u)/bracketrightbig−1/vextendsingle/vextendsingle·/vextendsingle/vextendsingleS(u)−λΘ⋆/vextendsingle/vextendsingle
≤β/vextendsingle/vextendsingle/bracketleftbig
(λ+β)INp+P(u)/bracketrightbig−1/vextendsingle/vextendsingle·(|S(u)|+λ|Θ⋆|).
The estimate (52)fort=u−1then follows from (51)and the induction hypothesis. Thus, by induction we
have shown that (51) holds for all t= 1,2, ...,T.
28Under review as submission to TMLR
We proceed to prove Theorem 3 about the complexity of the regret-optimal algorithm. To facilitate the proof,
we introduce the following Lemma 3 that summarizes complexities of matrix operations.
Lemma 3. Suppose all elementary arithmetic operations (e.g. addition, subtraction, multiplication) of two
real numbers are of constant (i.e. O(1)) complexity. Then elementary matrix computations have the following
complexities:
(i) Adding two n1×n2matrices has a complexity of O(n1n2).
(ii) Multiplying an n1×n2matrices by a scalar has a complexity of O(n1n2).
(iii)Multiplying an n1×n2matrix with an n2×n3matrix has a complexity of O(n1n2n3). In particular,
multiplying two n×nmatrices has a complexity of O(n2.373).
Proof.Adding two n1×n2matrices involves adding each entry of one matrix with the corresponding entry of
the other matrix, and each entry addition has a complexity of O(1). Hence the total complexity is O(n1n2).
Multiplying an n1×n2matrix by a scalar involves multiplying each entry by the scalar, and the scalar
multiplication of each entry has a complexity of O(1). Hence the total complexity is O(n1n2).
Multiplication of an n1×n2matrix and an n2×n3matrix requires multiplying the n1row vectors of the
first matrix by the n3column vectors of the second matrix, which needs a total of n1n3multiplications.
Multiplication of two n2-dimensional vectors has a complexity of O(n2). Therefore the total complexity is
O(n1n2n3). When multiplying two matrices of n×n-dimension with an optimized CW-like algorithm the
complexity isO(n2.373).
Proof of Theorem 3.
Complexity of Computing P(T)isO/parenleftbig
N2p3/parenrightbig
Sinceui
j∈Rp×1, computing each ui
jui⊤
jhasO(p2)complexity, by Lemma 3-(iii). Multiplying the p×p
matrixui
jui⊤
jby the scalar w⋆
ihas a complexity of O(p2), by Lemma 3-(iii). Then computing the p×p
matrix/summationtextN
i=1w⋆
i/summationtextSi
j=1ui
jui⊤
jby adding the ¯Nmatrices of dimension p×phas complexityO(¯Np2), by
Lemma 3-(i). Since [w⋆
1Ip,···,w⋆
NIp]⊤isNp×p-dimensional, then computing the np×p-dimensional product
[w⋆
1Ip,···,w⋆
NIp]⊤/summationtextN
i=1w⋆
i/summationtextSi
j=1ui
jui⊤
jhasO(Np3)complexity, by Lemma 3-(iii). Likewise, the product
/parenleftig
[w⋆
1Ip,···,w⋆
NIp]⊤N/summationdisplay
i=1w⋆
iSi/summationdisplay
j=1ui
jui⊤
j/parenrightig
[w⋆
1Ip,···,w⋆
NIp]
has a complexity of O(N2p3). Therefore, computing P(T)itself is ofO/parenleftbig¯Np2+N2p3/parenrightbig
complexity. Assump-
tion (17) implies that ¯N≤N2; hence, the complexity of computing P(T)isO/parenleftbig
N2p3/parenrightbig
.
Complexity of Computing S(T)isO/parenleftbig
Npmax{N,p}/parenrightbig
Sinceui
j∈Rp×1andyi
j∈R, computing the product ui
jyi
jhas complexityO(p), by Lemma 3-(iii). And
multiplying the p×1dimensional vector ui
jyi
jby the scalar w⋆
ihas complexityO(p), by Lemma 3-(ii). Then the
complexity of computing/summationtextN
i=1w⋆
i/summationtextSi
j=1ui
jyi
jby adding ¯Nvectors of dimension pisO/parenleftbig¯Np/parenrightbig
, by Lemma 3-(i).
Since−[w⋆
1Ip,···,w⋆
NIp]⊤is anNp×p-dimensional matrix and/summationtextN
i=1w⋆
i/summationtextSi
j=1ui
jyi
jis ap×1-dimensional
vector, then computing their Np×1dimensional product has O(Np2)complexity, according to Lemma 3-(iii).
Therefore, the computing S(T)has a complexity of O(¯Np+NP2). Since(17)implies that ¯N≤N2then,
computing S(T)had a computational cost of O/parenleftbig
Npmax{N,p}/parenrightbig
.
Complexity of Computing P(t)andS(t)fort=T−1,..., 1isO/parenleftbig
T(Np)2.373/parenrightbig
The complexity of computing (λ+β)INp+P(t+ 1)for eachtisO(Np), since we only need to add scalars on
the matrix’s diagonal. By Le Gall (2014), computing the matrix inverse [(λ+β)INp+P(t+ 1)]−1with a
CW-like algorithm has a complexity of O/parenleftbig
(Np)2.373/parenrightbig
. Since the addition βINp−β2[(λ+β)INp+P(t+ 1)]−1
only requires us to multiply all entries of [(λ+β)INp+P(t+ 1)]−1by−β2and add along the diagonal,
then it has a complexity of O/parenleftbig
(Np)2+Np/parenrightbig
, by Lemma 3-(i) and (ii). Performing the matrix addition
29Under review as submission to TMLR
S(t+ 1)−λΘ⋆has complexityO(Np)by Lemma 3-(i). Computing the Np×1-dimensional product
[(λ+ 1)INp+P(t+ 1)]−1(S(t+ 1)−λΘ⋆)by multiplying an Np×Npmatrix by an Np×1matrix has
complexityO(N2p2), according to Lemma 3-(iii). Therefore, computing each P(t)andS(t)fort=T−1,..., 1,
has complexityO/parenleftbig
(Np)2.373/parenrightbig
. Since there are T−1such matrices to compute, then the total cost of computing
everyP(T−1),...,P (1)andS(T−1), ...,S(1)isO/parenleftbig
T(Np)2.373/parenrightbig
.
The Cost of Computing Each Θ(t)isO(TN2p2)
To compute Θ(t)for allt= 1,...,T, starting from Θ(0) = Θ⋆, we need to compute a(t)given by (34)for
t= 0,1,...,T−1. Hence we need to quantify the complexity of computing a(t). By(36), we can obtain
−[(λ+β)INp+P(t+ 1)]−1by computing β−2(P(t)−βINp), which involves adding a scalar to the diagonal
of anNp×Npmatrix, and then multiplying the matrix by a scalar. The cost of computing the product
(λINp+P(t+1))Θ(t)isO(N2p2)andthecostofcomputingallsumsin (λINp+P(t+1))Θ(t)−λΘ⋆+S(t+1)are
oflowerordercomplexity. Finally, thecostofcomputingthematrix-productbetween −[(λ+β)INp+P(t+1)]−1
and/bracketleftbig
(λINp+P(t+ 1))Θ(t)−λΘ⋆+S(t+ 1)/bracketrightbig
isO(N2p2), by Lemma 3-(iii). Therefore, the cost of computing
each a(t)isO(N2p2), and therefore the cost of computing Θ(t)for allt= 1, ...,t=Tfrom Θ(0) = Θ⋆is
O(TN2p2).
Tallying up Complexities: The Complexity of Computing (Θ(t))T
t=0isO(N2p3+T(Np)2.373).
Looking over all computations involved, we deduce that the cost of computing the sequence (Θ(t))T
t=1is (in
order) dominated by the computational complexity of computing the sequence (P(t))T−1
t=1andP(T); whence
it isO(N2p3+T(Np)2.373).
The following Lemma 4, Proposition 1, and Corollary 2 are devoted to proving Theorem 4, the adversarial
robustness of Algorithm 2.
We introduce the linear map M(·) :RNp×Np→RNp×Npsuch thatM(Q) = (λ+β)INp+Qfor any
Q∈RNp×Np.
Lemma 4. Given any two data sets D=∪N
i=1Di=∪N
i=1∪|Di|
j=1{(xi
j,yi
j)}and/tildewideD=∪N
i=1/tildewideDi=∪N
i=1∪|Di|
j=1
{(˜xi
j,˜yi
j)}satisfying Assumption 1, let (P,S)and(/tildewideP,/tildewideS)be the solutions of (36)-(37)corresponding toDand
/tildewideD, respectively. Then, for all t= 1,...,T, we have
/vextendsingle/vextendsingleP(t)−/tildewideP(t)/vextendsingle/vextendsingle≤C·/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2/bracketrightig1/2
, (54)
/vextendsingle/vextendsingle[M(P(t))]−1−[M(/tildewideP(t))]−1/vextendsingle/vextendsingle≤C·/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2/bracketrightig1/2
, (55)
|S(t)−/tildewideS(t)|≤C·/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2+|yi
j−˜yi
j|2/bracketrightig1/2
, (56)
whereC > 0is a constant depending on λ,β,N,¯N,(w⋆
1,...,w⋆
N),Θ⋆,Kx,KyandT.
Proof.We letCbe a constant depending on λ,β,N,¯N,Θ⋆,(w⋆
1,...,w⋆
N),Kx,Ky, andT, and allow Cto
vary from place to place throughout the proof. We employ backward induction starting from t=Tto prove
(55), (54), and (56).
From (36), the terminal condition |P(T)−/tildewideP(T)|2can be written as
/vextendsingle/vextendsingleP(T)−/tildewideP(T)/vextendsingle/vextendsingle2=/vextendsingle/vextendsingle/vextendsingle[w⋆
1Ip,···,w⋆
NIp]⊤/bracketleftigN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1(ui
jui⊤
j−˜ui
j˜ui⊤
j)/bracketrightig
[w⋆
1Ip,···,w⋆
NIp]/vextendsingle/vextendsingle/vextendsingle2
=N/summationdisplay
k,l=1w⋆2
kw⋆2
l/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1(ui
jui⊤
j−˜ui
j˜ui⊤
j)/vextendsingle/vextendsingle/vextendsingle2
.
30Under review as submission to TMLR
By Lemmas 1 and 2, we have
/vextendsingle/vextendsingleP(T)−/tildewideP(T)/vextendsingle/vextendsingle2≤N/summationdisplay
k,l=1w⋆2
kw⋆2
lN/summationdisplay
i=1w⋆
i/vextendsingle/vextendsingle/vextendsingle|Di|/summationdisplay
j=1(ui
jui⊤
j−˜ui
j˜ui⊤
j)/vextendsingle/vextendsingle/vextendsingle2
≤N/summationdisplay
k,l=1w⋆2
kw⋆2
lN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1/vextendsingle/vextendsingleui
jui⊤
j−˜ui
j˜ui⊤
j/vextendsingle/vextendsingle2
=N/summationdisplay
k,l=1w⋆2
kw⋆2
lN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1/vextendsingle/vextendsingle(ui
j−˜ui
j)ui⊤
j+ ˜ui
j(ui⊤
j−˜ui⊤
j)/vextendsingle/vextendsingle2
≤N/summationdisplay
k,l=1w⋆2
kw⋆2
lN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=12/parenleftbig/vextendsingle/vextendsingleui
j−˜ui
j/vextendsingle/vextendsingle2·/vextendsingle/vextendsingleui
j/vextendsingle/vextendsingle2+/vextendsingle/vextendsingle˜ui
j/vextendsingle/vextendsingle2·/vextendsingle/vextendsingleui
j−˜ui
j/vextendsingle/vextendsingle2/parenrightbig
≤N/summationdisplay
k,l=1w⋆2
kw⋆2
lN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=14K2
x/vextendsingle/vextendsingleui
j−˜ui
j/vextendsingle/vextendsingle2. (57)
Therefore (54) holds for t=T.
Assume by induction that (54)holds fort=u. We show that (54)also holds for t=u−1. From(36), we
have
P(u−1)−/tildewideP(u−1) =[M(/tildewideP(u))]−1−[M(P(u))]−1
=[M(P(u))]−1(P(u)−/tildewideP(u))[M(/tildewideP(u))]−1,
and by Lemma 1 we further have
/vextendsingle/vextendsingleP(u−1)−/tildewideP(u−1)/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle[M(P(u))]−1/vextendsingle/vextendsingle·/vextendsingle/vextendsingleP(u)−/tildewideP(u)/vextendsingle/vextendsingle·/vextendsingle/vextendsingle[M(/tildewideP(u))]−1/vextendsingle/vextendsingle. (58)
By Corollary 1 and the induction hypothesis for t=u,(58)implies that (54)holds fort=u−1. We have
shown by induction that (54) holds for all t= 1, ...,T.
Since
[M(P(t))]−1−[M(/tildewideP(t))]−1= [M(P(t))]−1(/tildewideP(t)−P(t))[M(/tildewideP(t))]−1,
by Lemma 1 we have
/vextendsingle/vextendsingle[M(P(t))]−1−[M(/tildewideP(t))]−1/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle[M(P(t))]−1/vextendsingle/vextendsingle·/vextendsingle/vextendsingle/tildewideP(t)−P(t)/vextendsingle/vextendsingle·/vextendsingle/vextendsingle[M(/tildewideP(t))]−1/vextendsingle/vextendsingle. (59)
The estimate (55) then follows from (59), (54), and Corollary 1.
Now we establish the estimate (56). By (37), the terminal condition S(T)−/tildewideS(T)satisfies
|S(T)−/tildewideS(T)|2=/vextendsingle/vextendsingle/vextendsingle[w⋆
1,···,w⋆
N]⊤N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1(ui
jyi
j−˜ui
j˜yi
j)/vextendsingle/vextendsingle/vextendsingle2
=/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightig/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1(ui
jyi
j−˜ui
j˜yi
j)/vextendsingle/vextendsingle/vextendsingle2
.
By Lemmas 1 and 2, we obtain
|S(T)−/tildewideS(T)|2≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i/vextendsingle/vextendsingle/vextendsingle|Di|/summationdisplay
j=1(ui
jyi
j−˜ui
j˜yi
j)/vextendsingle/vextendsingle/vextendsingle2
31Under review as submission to TMLR
≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1/vextendsingle/vextendsingleui
jyi
j−˜ui
j˜yi
j/vextendsingle/vextendsingle2
=/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1/vextendsingle/vextendsingle(ui
j−˜ui
j)yi
j+ ˜ui
j(yi
j−˜yi
j)/vextendsingle/vextendsingle2
≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i|Di|·2|Di|/summationdisplay
j=1/bracketleftbig/vextendsingle/vextendsingle(ui
j−˜ui
j)yi
j/vextendsingle/vextendsingle2+/vextendsingle/vextendsingle˜ui
j(yi
j−˜yi
j)/vextendsingle/vextendsingle2/bracketrightbig
≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i|Di|·2|Di|/summationdisplay
j=1/bracketleftbig
|ui
j−˜ui
j|2·|yi
j|2+|˜ui
j|2·|yi
j−˜yi
j|2/bracketrightbig
≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightigN/summationdisplay
i=1w⋆
i|Di|·2|Di|/summationdisplay
j=1/bracketleftbig
K2
y·|ui
j−˜ui
j|2+K2
x·|yi
j−˜yi
j|2/bracketrightbig
≤/parenleftigN/summationdisplay
i=1w⋆2
i/parenrightig
2(K2
x+K2
y)N/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1/bracketleftbig
|ui
j−˜ui
j|2+|yi
j−˜yi
j|2/bracketrightbig
, (60)
and thus (56)holds fort=T. Assume by induction that (56)holds fort=u, we show that (56)also holds
fort=u−1. From (37), the difference S(t)−/tildewideS(t)can be written as
S(u−1)−/tildewideS(u−1) =[M(/tildewideP(u))]−1[S(u)−/tildewideS(u)]
+/bracketleftbig
(M(P(u)))−1−(M(/tildewideP(u)))−1/bracketrightbig
(S(u)−λΘ⋆),
and satisfies by Lemma 1 and Corollary 1 that
|S(u−1)−/tildewideS(u−1)|≤/vextendsingle/vextendsingle[M(/tildewideP(u))]−1/vextendsingle/vextendsingle·|S(u)−/tildewideS(u)|
+/vextendsingle/vextendsingle(M(P(u)))−1−(M(/tildewideP(u)))−1/vextendsingle/vextendsingle·(|S(u)|+λ|Θ⋆|). (61)
By Corollary 1, (55), and the induction hypothesis, (61)implies that (56)holds fort=u−1. We have shown
by induction that (56) holds for all t= 1, ...,T.
Proposition 1. For two arbitrary data sets D=∪N
i=1Di=∪N
i=1∪|Di|
j=1{(xi
j,yi
j)}and/tildewideD=∪N
i=1/tildewideDi=
∪N
i=1∪|Di|
j=1{(˜xi
j,˜yi
j)}satisfying Assumption 1, let α= (α⊤
1,···,α⊤
N)⊤and/tildewideα= (/tildewideα⊤
1,···,/tildewideα⊤
N)⊤be the optimal
controls for (6)and(9)corresponding to Dand/tildewideD, respectively. Then, we have
T−1/summationdisplay
t=0|α(t)−/tildewideα(t)|≤C/bracketleftigN/summationdisplay
i=1w⋆2
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2+|yi
j−˜yi
j|2/bracketrightig1/2
, (62)
whereC > 0is a constant depending on λ,β,N,¯N,Θ⋆,(w⋆
1,···,w⋆
N),Kx,Ky, andT.
Proof of Proposition 1. LetCbe a constant depending on λ,β,N,¯N,Θ⋆,(w⋆
1,···,w⋆
N),Kx,Ky, andT,
andCis allowed to vary from place to place throughout the proof. By (34)and(10), the optimal control
α(t)along the optimal trajectory can be written as
α(t) =[(M(P(t+ 1)))−1−INp]/bracketleftig
Θ⋆+t−1/summationdisplay
u=0α(u)/bracketrightig
+ [M(P(t+ 1))]−1(λΘ⋆−S(t+ 1)),
and the difference α(t)−/tildewideα(t)can be written as
α(t)−/tildewideα(t) =/bracketleftbig
(M(P(t+ 1)))−1−(M(/tildewideP(t+ 1)))−1/bracketrightbig/bracketleftig
Θ⋆+t−1/summationdisplay
u=0α(u) +λΘ⋆−S(t+ 1)/bracketrightig
32Under review as submission to TMLR
+/bracketleftbig
(M(/tildewideP(t+ 1)))−1−INp/bracketrightbigt−1/summationdisplay
u=0(α(u)−/tildewideα(u))
+ (M(/tildewideP(t+ 1)))−1(S(t+ 1)−/tildewideS(t+ 1)). (63)
Fort= 0, the difference α(0)−/tildewideα(0)can be written as
α(0)−/tildewideα(0) =/bracketleftbig
(M(P(1)))−1−(M(/tildewideP(1)))−1/bracketrightbig/bracketleftbig
(λ+ 1)Θ⋆−S(1)/bracketrightbig
−[M(/tildewideP(1))]−1(S(1)−/tildewideS(1)),
and satisfies by Lemma 1 that
/vextendsingle/vextendsingleα(0)−/tildewideα(0)/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle(M(P(1)))−1−(M(/tildewideP(1)))−1/vextendsingle/vextendsingle·/bracketleftbig
(λ+ 1)|Θ⋆|+|S(1)|/bracketrightbig
+/vextendsingle/vextendsingle[M(/tildewideP(1))]−1/vextendsingle/vextendsingle·/vextendsingle/vextendsingleS(1)−/tildewideS(1)/vextendsingle/vextendsingle.
It then follows from Corollary 1, (55), and (56) that
/vextendsingle/vextendsingleα(0)−/tildewideα(0)/vextendsingle/vextendsingle≤C·/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2+|yi
j−˜yi
j|2/bracketrightig1/2
.
Assume by induction that
t−1/summationdisplay
u=0/vextendsingle/vextendsingleα(u)−/tildewideα(u)/vextendsingle/vextendsingle≤C·/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2+|yi
j−˜yi
j|2/bracketrightig1/2
.
By Lemma 1, (63) implies that
|α(t)−/tildewideα(t)|
≤/vextendsingle/vextendsingle(M(P(t+ 1)))−1−(M(/tildewideP(t+ 1)))−1/vextendsingle/vextendsingle·/bracketleftig
|Θ⋆|+t−1/summationdisplay
u=0|α(u)|+λ|Θ⋆|+|S(t+ 1)|/bracketrightig
+/vextendsingle/vextendsingle(M(/tildewideP(t+ 1)))−1−INp/vextendsingle/vextendsinglet−1/summationdisplay
u=0|α(u)−/tildewideα(u)|+|(M(/tildewideP(t+ 1)))−1|·|S(t+ 1)−/tildewideS(t+ 1)|.
By (55), (56), and the induction hypothesis, the above inequality implies that that
|α(t)−/tildewideα(t)|≤C/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2+|yi
j−˜yi
j|2/bracketrightig1/2
,
and therefore
t/summationdisplay
u=0/vextendsingle/vextendsingleα(u)−/tildewideα(u)/vextendsingle/vextendsingle≤C·/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2+|yi
j−˜yi
j|2/bracketrightig1/2
.
The estimate (62) is then proved by induction.
Corollary 2. Under the hypothesis of Proposition 1, the costs (6)under the optimal controls aand/tildewideasatisfy
the estimate
/vextendsingle/vextendsingleL⋆(α;D)−L⋆(/tildewideα;/tildewideD)/vextendsingle/vextendsingle≤C·/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜xi
j|2+|yi
j−˜yi
j|2/bracketrightig1/2
. (64)
whereC > 0is a constant depending on λ,β,N,¯N,Θ⋆,(w⋆
1,···,w⋆
N),Kx,Ky, andT.
33Under review as submission to TMLR
Proof of Corollary 2. Throughout the proof Cis a constant depending on λ,β,¯N,Θ⋆,(w⋆
1,···,w⋆
N),Kx,
KyandT, and may vary from place to place. By (42) and (9), the difference of the costs can be written as
L⋆(α;D)−L⋆(/tildewideα;/tildewideD) =Θ⊤(0)(P(0)−/tildewideP(0))Θ(0) + 2( S(0)−/tildewideS(0))⊤Θ(0) +r(0)−/tildewider(0),
=Θ⋆⊤(P(0)−/tildewideP(0))Θ⋆+ 2(S(0)−/tildewideS(0))⊤Θ⋆+r(0)−/tildewider(0).
By the triangular inequality, the difference satisfies
/vextendsingle/vextendsingleL⋆(α;D)−L⋆(/tildewideα;/tildewideD)/vextendsingle/vextendsingle≤|P(0)−/tildewideP(0)|·|Θ⋆|2+ 2|S(0)−/tildewideS(0)|·|Θ⋆|+|r(0)−/tildewider(0)|. (65)
We claim that r(0)−/tildewider(0)satisfies the estimate
|r(0)−/tildewider(0)|≤C·/bracketleftigN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−˜ui
j|2+|yi
j−˜yi
j|2/bracketrightig1/2
. (66)
Then the desired estimate (64) is established by (65), (54), (56) and (66).
Now we prove the claim (66). From (38), the difference r−/tildewidersatisfies
r(t)−/tildewider(t) =r(t+ 1)−/tildewider(t+ 1)−(S(t+ 1)−/tildewideS(t+ 1))/braceleftbig
(M(P(t+ 1)))−1/bracketleftbig
S(t+ 1)−2λΘ⋆/bracketrightbig
+ (M(/tildewideP(t+ 1)))−1/tildewideS(t+ 1)/bracerightbig
−/tildewideS⊤(t+ 1)/bracketleftbig
(M(P(t+ 1)))−1−(M(/tildewideP(t+ 1)))−1/bracketrightbig/bracketleftbig
S(t+ 1)−2λΘ⋆/bracketrightbig
−Θ⋆⊤λ/bracketleftbig
(M(P(t+ 1)))−1−(M(/tildewideP(t+ 1)))−1/bracketrightbig
λΘ⋆,
r(T)−/tildewider(T) =N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1(|yi
j|2−|/tildewideyi
j|2). (67)
Inductively, r(0)−/tildewider(0)can be written in terms of r(T)−/tildewider(T)as
r(0)−/tildewider(0)
=r(T)−/tildewider(T)−T/summationdisplay
t=1/braceleftig
(S(t)−/tildewideS(t))/bracketleftbig
(M(P(t)))−1/parenleftbig
S(t)−2λΘ⋆/parenrightbig
+ (M(/tildewideP(t)))−1/tildewideS(t)/bracketrightbig
+/tildewideS⊤(t)/bracketleftbig
(M(P(t)))−1−(M(/tildewideP(t)))−1/bracketrightbig/bracketleftbig
S(t)−2λΘ⋆/bracketrightbig
+ Θ⋆⊤λ/bracketleftbig
(M(P(t)))−1−(M(/tildewideP(t)))−1/bracketrightbig
λΘ⋆/bracerightig
.
By Lemma 1, we have
|r(0)−/tildewider(0)|
≤|r(T)−/tildewider(T)|+T/summationdisplay
t=1/braceleftig/vextendsingle/vextendsingleS(t)−/tildewideS(t)/vextendsingle/vextendsingle·/bracketleftig/vextendsingle/vextendsingle(M(P(t)))−1/vextendsingle/vextendsingle·/parenleftbig
|S(t)|+ 2λ|Θ⋆|/parenrightbig
+/vextendsingle/vextendsingle(M(/tildewideP(t)))−1/vextendsingle/vextendsingle·/vextendsingle/vextendsingle/tildewideS(t)/vextendsingle/vextendsingle/bracketrightig
+/vextendsingle/vextendsingle(M(P(t)))−1−(M(/tildewideP(t)))−1/vextendsingle/vextendsingle·/parenleftbig/vextendsingle/vextendsingle/tildewideS(t)/vextendsingle/vextendsingle·/vextendsingle/vextendsingleS(t)−2λΘ⋆/vextendsingle/vextendsingle+λ|Θ⋆|2/parenrightbig/bracerightig
. (68)
By (67), Lemma 2, and the Cauchy-Schwarz inequality, the terminal condition satisfies
|r(T)−/tildewider(T)|=/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1(yi
j−˜yi
j)(yi
j+ ˜yi
j)/vextendsingle/vextendsingle/vextendsingle≤N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1|(yi
j−˜yi
j)(yi
j+ ˜yi
j)|
≤N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1|yi
j+/tildewideyi
j|·|yi
j−/tildewideyi
j|≤2KyN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1|yi
j−/tildewideyi
j|
≤2Ky/parenleftigN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1|yi
j−/tildewideyi
j|2/parenrightig1/2
. (69)
The estimate (66) then follows from (68), (69), (55), (56), and (52).
34Under review as submission to TMLR
We may now derive the proof of Theorem 4 as a direct consequence of Corollary 2.
Proof of Theorem 4. Fix/tildewideD∈Dp,εand letI⊆{(i,j) :i= 1,...,N, j = 1,...,|Di|}consist of all indices
(i,j)for which{(i,j) :ui
j̸=/tildewideui
jandyi
j̸=/tildewideyi
j}. By Corollary 2, there is a constant C > 0which is independent
ofDsuch that
/vextendsingle/vextendsingleL⋆(α;D)−L⋆(/tildewideα;/tildewideD)/vextendsingle/vextendsingle2≤/tildewideCN/summationdisplay
i=1w⋆
i|Di||Di|/summationdisplay
j=1|ui
j−/tildewideui
j|2+|yi
j−/tildewideyi
j|2
≤/tildewideC·/summationdisplay
(i,j)∈Iw⋆
i|Di|(|ui
j−/tildewideui
j|2+|yi
j−/tildewideyi
j|2)
≤/tildewideC·/summationdisplay
(i,j)∈Iw⋆
i|Di|·max{|ui
j−/tildewideui
j|2,|yi
j−/tildewideyi
j|2}
=/tildewideC·/summationdisplay
(i,j)∈Iw⋆
i|Di|·max{|ui
j−/tildewideui
j|,|yi
j−/tildewideyi
j|}2
≤/tildewideC·/summationdisplay
(i,j)∈Iw⋆
i|Di|·2ε2
≤/tildewideC#Ip2ε2,
where we have set /tildewideCdef.=C1/2.
The computational complexity of the matrix-valued function P(·), as determined by (36), does not primarily
stem from the high dimensionality of the output matrices, but rather from their inherent lack of symmetries.
As we will see later, such symmetry will allow us to efficiently encode P(·)into low-dimensional structures,
therefore completely mitigating the high-dimensionality effect.
We therefore introduce the following control, which is optimal for a “symmetrized version” of the control
problem (34)wherew⋆= (1/N,..., 1/N). This advantages this surrogate problem is that the associated P
function, which we denote by /hatwideP, outputs highly symmetric10Toeplitz matrices which are determined by
exactly three factors. This allows us to encode the entire control problem into a control problem on R˜O(P)
whose dimension is independent of N. Furthermore, the optimal controls of both problems can be related by
stability estimates, depending only on the deviation of (1/N,..., 1/N)fromw⋆, which allows us to infer the
degree of sub-optimality of our /hatwideP. Therefore, we consider the control
/hatwideα(t) =−[(λ+β)INp+/hatwideP(t+ 1)]−1/bracketleftbig
(λINp+/hatwideP(t+ 1))Θ(t)−λΘ⋆
(N)+/hatwideS(t+ 1)/bracketrightbig
. (70)
We will show that the algorithm under control (70)enjoys lower computational complexity in comparison
with the regret-optimal algorithm (34), whose computational complexity is summarized through Theorem 3.
Furthermore, we will then establish the near-regret optimality of (70), stated in Theorem 5.
The associated matrix-valued functions /hatwidePand/hatwideSare uniquely determined by
/braceleftigg/hatwideP(t) =βINp−β2[(λ+β)INp+/hatwideP(t+ 1)]−1,
/hatwideP(T) = [(1/N)Ip,···,(1/N)Ip]⊤/parenleftig/summationtextN
i=1(1/N)/summationtext|Di|
j=1ui
jui⊤
j/parenrightig
[(1/N)Ip,···,(1/N)Ip],(71)
/braceleftigg/hatwideS(t) =β[(λ+β)INp+/hatwideP(t+ 1)]−1/parenleftbig/hatwideS(t+ 1)−λΘ⋆
(N)/parenrightbig
,
/hatwideS(T) =−[(1/N)Ip,···,(1/N)Ip]⊤/summationtextN
i=1(1/N)/summationtext|Di|
j=1ui
jyi
j,(72)
where [Ip,···,Ip]denotes the Np×p-dimensional matrix formed by concatenating Ncopies of the p×p-
dimensional identity matrix1
NIp. The parameter Θ⋆
(N)is defined as
Θ⋆
(N)def.= [θ⋆⊤
(N),···,θ⋆⊤
(N)]⊤, θ⋆
(N)def.=1
NN/summationdisplay
i=1θ⋆
i.
10As formalized in Proposition 4.
35Under review as submission to TMLR
Remark 4. According to (7)in Assumption 2, /hatwideP(T)≥0and therefore (71)admits a unique solution.
Remark 5. Similar to Corollary 1, we have/vextendsingle/vextendsingle[(λ+β)INp+/hatwideP(t)]−1/vextendsingle/vextendsingle≤(λ+β)−1. By replacing (w⋆
1,...,w⋆
N)
in the proof of Corollary 1 with (1/N,..., 1/N), we obtain|/hatwideS(t)|≤C·N1/2for allt= 1, ...,T, whereC > 0
is a constant depending on λ,β,Θ⋆,Kx,KyandT.
Lemma 5. LetPand/hatwidePbe the solutions of the systems (36)and(71), respectively. Then they satisfy that
for allt= 1,2, ...,T,
/vextendsingle/vextendsingleP(t)−/hatwideP(t)/vextendsingle/vextendsingle≤C·¯NN max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle, (73)
/vextendsingle/vextendsingle(M(P(t)))−1−(M(/hatwideP(t)))−1/vextendsingle/vextendsingle≤C·¯NN max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle, (74)
whereC > 0is a constant depending on λ,β,Kx.
Proof of Lemma 5. By (71), (36), and Lemmas 1 and 2, the difference /hatwideP(T)−P(T)satisfies
|/hatwideP(T)−P(T)|2=N/summationdisplay
k,l=1/vextendsingle/vextendsingle/vextendsingle1
N2N/summationdisplay
k=11
N|Di|/summationdisplay
j=1ui
jui⊤
j−w⋆
kw⋆
lN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1ui
jui⊤
j/vextendsingle/vextendsingle/vextendsingle2
≤N2·max
1≤k,l,i≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle2
·/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1|Di|/summationdisplay
j=1ui
jui⊤
j/vextendsingle/vextendsingle/vextendsingle2
≤N2·max
1≤k,l,i≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle2
·¯NN/summationdisplay
i=1|Di|/summationdisplay
j=1|ui
j|4
and
|/hatwideP(T)−P(T)|≤¯NNK2
x·max
1≤k,l,i≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle. (75)
Therefore (73) holds for t=T.
Assume by induction that (73)holds fort=u. We show that (73)also holds for t=u−1. From(71)and
(36), we have
/hatwideP(u−1)−P(u−1) =[M(P(u))]−1−[M(/hatwideP(u))]−1
=[M(P(u))]−1(/hatwideP(u)−P(u))[M(/hatwideP(u))]−1. (76)
By Lemmas 1 and 2, we further have
|/hatwideP(u−1)−P(u−1)|=|[M(P(u))]−1−[M(/hatwideP(u))]−1|
≤|[M(P(u))]−1|·|/hatwideP(u)−P(u)|·|[M(/hatwideP(u))]−1|. (77)
By(77), Corollary 1, Remark 5, and the induction hypothesis, we have that (73)holds fort=u−1. By
induction we have shown that (73) holds for all 1≤t≤T.
From (76), we have
/vextendsingle/vextendsingle(M(P(t)))−1−(M(/hatwideP(t)))−1/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle(M(P(t)))−1/vextendsingle/vextendsingle·/vextendsingle/vextendsingle/hatwideP(t)−P(t)/vextendsingle/vextendsingle·/vextendsingle/vextendsingle(M(/hatwideP(t)))−1/vextendsingle/vextendsingle,
which, together with (73), Corollary 1, and Remark 5, establishes (74).
Lemma 6. LetSand/hatwideSbe the solutions of (37)and(72), respectively. Then they satisfy that for all t= 1,
...,T,
|S(t)−/hatwideS(t)|≤C·/braceleftig
¯NN3/2max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle
36Under review as submission to TMLR
+¯NN1/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle+|Θ⋆−Θ⋆
(N)|/bracerightig
, (78)
whereC > 0is a constant depending on λ,β,Θ⋆,Kx,Ky, andT.
Proof of Lemma 6. By (37), (72), and and Lemmas 1 and 2, the terminal condition S(T)−/hatwideS(T)satisfies
/vextendsingle/vextendsingle/vextendsingleS(T)−/hatwideS(T)/vextendsingle/vextendsingle/vextendsingle2
=N/summationdisplay
k=1/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=11
N|Di|/summationdisplay
j=1ui
jyi
j−w⋆
kN/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1ui
jyi
j/vextendsingle/vextendsingle/vextendsingle2
≤N·max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle2
·/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1|Di|/summationdisplay
j=1ui
jyi
j/vextendsingle/vextendsingle/vextendsingle2
≤N·max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle2
·¯NN/summationdisplay
i=1|Di|/summationdisplay
j=1|ui
j|2·|yi
j|2,
which implies
/vextendsingle/vextendsingleS(T)−/hatwideS(T)/vextendsingle/vextendsingle≤¯NN1/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle·KxKy. (79)
Therefore (78) holds for t=T.
Assume by induction that (78)holds fort=u, we show that (78)holds fort=u−1. From(37)and(72),
the difference S(u−1)−/hatwideS(u−1)can be written as
S(u−1)−/hatwideS(u−1) =[M(P(u))]−1[S(u)−/hatwideS(u)−λ(Θ⋆−Θ⋆
(N))]
−/bracketleftbig
(M(/hatwideP(u)))−1−(M(P(u)))−1/bracketrightbig
(/hatwideS(u)−λΘ⋆
(N)), (80)
and satisfies by Lemma 1 that
|S(u−1)−/hatwideS(u−1)|≤|[M(P(u))]−1|·/bracketleftbig
|S(u)−/hatwideS(u)|+λ|Θ⋆−Θ⋆
(N)|/bracketrightbig
+/vextendsingle/vextendsingle(M(/hatwideP(u)))−1−(M(P(u)))−1/vextendsingle/vextendsingle·/parenleftbig
|/hatwideS(u)|+λ|Θ⋆
(N)|/parenrightbig
. (81)
By(81),(74), Remark 5, and the induction hypothesis, we have that (78)holds fort=u−1. By induction
we have shown that (78) holds for all t= 1, ...,T.
We now obtain Theorem 5; which is a direct consequence of the following Propositions 2 and 3.
Proposition 2. The controls (34)and(70)satisfy|/hatwideα(t)|≤C·N1/2for allt= 0,1, ...,T−1, and
T−1/summationdisplay
t=0|α(t)−/hatwideα(t)|≤C·/braceleftig
¯NN3/2max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle
+¯NN1/2·max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle+|Θ⋆−Θ⋆
(N)|/bracerightig
, (82)
whereC > 0is a constant depending on λ,β,Θ⋆,Kx,Ky, andT.
Proof of Proposition 2. We letCbe a constant depending on λ,β,Θ⋆,Kx,Ky, andT, andCis allowed to
vary from place to place throughout the proof. By (34), (70), and (10), we can write αand/hatwideαas
α(t) =[(M(P(t+ 1)))−1−INp]/bracketleftig
Θ⋆+t−1/summationdisplay
u=0α(u)/bracketrightig
+ [M(P(t+ 1))]−1(λΘ⋆−S(t+ 1)),
37Under review as submission to TMLR
/hatwideα(t) =[(M(/hatwideP(t+ 1)))−1−INp]/bracketleftig
Θ⋆+t−1/summationdisplay
u=0/hatwideα(u)/bracketrightig
+ [M(/hatwideP(t+ 1))]−1(λΘ⋆
(N)−/hatwideS(t+ 1)).
Fort= 0, we have
|/hatwideα(0)|≤|(M(/hatwideP(1)))−1−INp|·|Θ⋆|+/vextendsingle/vextendsingle[M(/hatwideP(1))]−1/vextendsingle/vextendsingle(λ|Θ⋆
(N)|+|/hatwideS(1)|),
which implies|/hatwideα(0)|≤CN1/2. Assume by induction that |/hatwideα(u)|≤CN1/2foru= 0, ...,t−1, then
/hatwideα(t)≤/vextendsingle/vextendsingle(M(/hatwideP(t+ 1)))−1−INp/vextendsingle/vextendsingle/bracketleftig
|Θ⋆|+t−1/summationdisplay
u=0|/hatwideα(u)|/bracketrightig
+/vextendsingle/vextendsingle[M(/hatwideP(t+ 1))]−1/vextendsingle/vextendsingle(λ|Θ⋆
(N)|+|/hatwideS(t+ 1)|)
gives that|/hatwideα(t)|≤CN1/2. By induction we have shown that |/hatwideα(t)|≤CN1/2for allt= 0,1, ...,T−1.
We further write the difference α−/hatwideαas
α(t)−/hatwideα(t)
=/bracketleftbig
(M(P(t+ 1)))−1−(M(/hatwideP(t+ 1)))−1/bracketrightbig/bracketleftig
(λ+ 1)Θ⋆+t−1/summationdisplay
u=0α(u)−S(t+ 1)/bracketrightig
+/bracketleftbig
(M(/hatwideP(t+ 1)))−1−INp/bracketrightbigt−1/summationdisplay
u=0(α(u)−/hatwideα(u)) + [M(/hatwideP(t+ 1))]−1(/hatwideS(t+ 1)−S(t+ 1))
+λ[M(/hatwideP(t+ 1))]−1(Θ⋆−Θ⋆
(N)). (83)
Fort= 0, the difference
α(0)−/hatwideα(0) =/bracketleftbig
(M(P(1)))−1−(M(/hatwideP(1)))−1/bracketrightbig/bracketleftbig
(λ+ 1)Θ⋆
(N)−/hatwideS(1)/bracketrightbig
+ [M(P(1))]−1[(λ+ 1)(Θ⋆−Θ⋆
(N)) +/hatwideS(1)−S(1)]
satisfies
/vextendsingle/vextendsingleα(0)−/hatwideα(0)/vextendsingle/vextendsingle≤C·/braceleftig
¯NN3/2max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle
+¯NN1/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle+|Θ⋆−Θ⋆
(N)|/bracerightig
,
due to(74),(78), Lemma 1, Corollary 1 and Remark 5. Assume by induction for all 0≤u≤t−1,
α(u−1)−/hatwideα(u−1)satisfies
/vextendsingle/vextendsingleα(u−1)−/hatwideα(u−1)/vextendsingle/vextendsingle≤C·/braceleftig
¯NN3/2max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle
+¯NN1/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle+|Θ⋆−Θ⋆
(N)|/bracerightig
.
By (83) we have
|α(t)−/hatwideα(t)|
≤/vextendsingle/vextendsingle(M(P(t+ 1)))−1−(M(/hatwideP(t+ 1)))−1/vextendsingle/vextendsingle·/bracketleftigt−1/summationdisplay
u=0|α(u)|+|(λ+ 1)Θ⋆|+|S(t+ 1)|/bracketrightig
+/vextendsingle/vextendsingle(M(/hatwideP(t+ 1)))−1−INp/vextendsingle/vextendsinglet−1/summationdisplay
u=0|α(u)−/hatwideα(u)|+|(M(/hatwideP(t+ 1)))−1|·|S(t+ 1)−/hatwideS(t+ 1)|,
38Under review as submission to TMLR
+/vextendsingle/vextendsingle[M(/hatwideP(t+ 1))]−1/vextendsingle/vextendsingle·|λ|·/vextendsingle/vextendsingleΘ⋆−Θ⋆
(N)/vextendsingle/vextendsingle.
By the induction hypothesis, (74), (78), and Remark 5, the above inequality implies that
|α(t)−/hatwideα(t)|≤C·/braceleftig
¯NN3/2max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle
+¯NN1/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle+|Θ⋆−Θ⋆
(N)|/bracerightig
. (84)
We have shown by induction that (84) holds for all t= 0,1, ...,T−1, and therefore (82) follows.
Proposition 3. Under the hypothesis of Proposition 2, the costs (6)under the controls (34)and(70),
respectively, satisfy the estimate
/vextendsingle/vextendsingleL⋆(α;D)−L⋆(/hatwideα;D)/vextendsingle/vextendsingle≤C·/braceleftig
¯NN3max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle+¯NN3/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle
+¯Nmax
1≤i≤N/vextendsingle/vextendsingle/vextendsingle1
N−w⋆
i/vextendsingle/vextendsingle/vextendsingle+N|Θ⋆−Θ⋆
(N)|/bracerightig
, (85)
whereC > 0is a constant depending on λ,β,Θ⋆,(w⋆
1,...,w⋆
N),Kx,KyandT.
Proof of Proposition 3 . LetL⋆(/hatwideα;1/N,D),L⋆(/hatwideα;w⋆,D), andL⋆(α;w⋆,D)be as defined in Lemmas 7 and 8.
The difference L⋆(α;D)−L⋆(/hatwideα;D)can be decomposed as
L⋆(α;D)−L⋆(/hatwideα;D) =L⋆(α;w⋆,D)−L⋆(/hatwideα;1/N,D) +L⋆(/hatwideα;1/N,D)−L⋆(/hatwideα;w⋆,D),
and satisfies
|L⋆(α;D)−L⋆(/hatwideα;D)|≤|L⋆(α;w⋆,D)−L⋆(/hatwideα;1/N,D)|+|L⋆(/hatwideα;1/N,D)−L⋆(/hatwideα;w⋆,D)|.
The desired estimate (85) then follows from the above inequality and (86) and (88).
Lemma 7. LetL⋆(/hatwideα;w⋆,D)be(6)with information sharing weights w⋆= (w⋆
1,...,w⋆
N)andΘ(t+ 1)−
Θ(t) =/hatwideα(t), and letL⋆(/hatwideα;1/N,D)be(6)with information sharing weights 1/N= (1/N,..., 1/N)and
Θ(t+ 1)−Θ(t) =/hatwideα(t). Then it satisfies
|L⋆(/hatwideα;1/N,D)−L⋆(/hatwideα;w⋆,D)|≤C·/braceleftig
¯Nmax
1≤i≤N/vextendsingle/vextendsingle/vextendsingle1
N−w⋆
i/vextendsingle/vextendsingle/vextendsingle+N1/2|Θ⋆−Θ⋆
(N)|/bracerightig
, (86)
whereC > 0is a constant depending on λ,β,Θ⋆,(w⋆
1,···,w⋆
N),Kx,KyandT.
Proof.By (6) and (41), we have
L⋆(/hatwideα;1/N,D)−L⋆(/hatwideα;w⋆,D) =T−1/summationdisplay
t=0/parenleftbig
|Θ(t+ 1)−Θ∗|2−|Θ(t+ 1)−Θ∗
(N)|2/parenrightbig
+l(Θ(T),1/N;D)−l(Θ(T),w⋆;D).
We have that/vextendsingle/vextendsingle/vextendsingleT−1/summationdisplay
t=0/parenleftbig
|Θ(t+ 1)−Θ∗|2−|Θ(t+ 1)−Θ∗
(N)|2/parenrightbig/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingleT−1/summationdisplay
t=0(Θ⋆
(N)−Θ⋆)⊤(2Θ(t+ 1)−Θ⋆−Θ⋆
(N))/vextendsingle/vextendsingle/vextendsingle
≤T−1/summationdisplay
t=0|Θ⋆
(N)−Θ⋆|·|2Θ(t+ 1)−Θ⋆−Θ⋆
(N)|≤CN1/2·|Θ⋆
(N)−Θ⋆|,
39Under review as submission to TMLR
and
l(Θ(T),1/N;D)−l(Θ(T),w⋆;D)
=N/summationdisplay
i=1w⋆
i|Di|/summationdisplay
j=1/bracketleftig
ui⊤
jN/summationdisplay
k=1w⋆
kθk−yi
j/bracketrightig2
−N/summationdisplay
i=11
N|Di|/summationdisplay
j=1/bracketleftig
ui⊤
jN/summationdisplay
k=11
Nθk−yi
j/bracketrightig2
=N/summationdisplay
i=1(w⋆
i−1/N)|Di|/summationdisplay
j=1/bracketleftig
ui⊤
jN/summationdisplay
k=1w⋆
kθk−yi
j/bracketrightig2
+N/summationdisplay
i=11
N|Di|/summationdisplay
j=1/bracketleftig
ui⊤
jN/summationdisplay
k=1(w⋆
k−1/N)θk/bracketrightig/bracketleftig
ui⊤
jN/summationdisplay
k=1(w⋆
k+ 1/N)θk−2yi
j/bracketrightig2
,
which implies that
|l(Θ(T),1/N;D)−l(Θ(T),w⋆;D)|≤C¯Nmax
1≤i≤N|1/N−w⋆
i|. (87)
The estimate (86) then follows from (87) and (87).
Lemma 8. LetL⋆(α;w⋆,D)be(6)with information sharing weights w⋆= (w⋆
1,...,w⋆
N)andΘ(t+ 1)−
Θ(t) =α(t), and letL⋆(/hatwideα;1/N,D)be(6)with information sharing weights 1/N= (1/N,..., 1/N)and
Θ(t+ 1)−Θ(t) =/hatwideα(t). Then it satisfies
|L⋆(α;w⋆,D)−L⋆(/hatwideα;1/N,D)|≤C·/braceleftig
¯NN3·max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle
+¯NN3/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle+¯N·max
1≤i≤N/vextendsingle/vextendsingle/vextendsingle1
N−w⋆
i/vextendsingle/vextendsingle/vextendsingle+N|Θ⋆−Θ⋆
(N)|/bracerightig
, (88)
whereCis a constant depending on λ,β,Θ⋆,(w⋆
1,···,w⋆
N),Kx,KyandT.
Proof.By the similar argument for (42), we have
L⋆(/hatwideα;1/N,D) = Θ⋆⊤/hatwideP(0)Θ⋆+ 2/hatwideS(0)⊤Θ⋆+/hatwider(0). (89)
By (42) and (89), the difference L⋆(α;w⋆,D)−L⋆(/hatwideα;1/N,D)can be written as
L⋆(α;w⋆,D)−L⋆(/hatwideα;1/N,D) = Θ⋆⊤(P(0)−/hatwideP(0))Θ⋆+ 2(S(0)−/hatwideS(0))⊤Θ⋆+r(0)−/hatwider(0),
and by Lemma 1, the difference satisfies
/vextendsingle/vextendsingleL⋆(α;w⋆,D)−L⋆(/hatwideα;1/N,D)/vextendsingle/vextendsingle≤|Θ⋆|2·|P(0)−/hatwideP(0)|+ 2|S(0)−/hatwideS(0)|·|Θ⋆|
+|r(0)−/hatwider(0)|. (90)
We claim that r(0)−/hatwider(0)has the estimate
|r(0)−/hatwider(0)|≤C·/braceleftig
¯NN3·max
1≤i,k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N3−w⋆
kw⋆
lw⋆
i/vextendsingle/vextendsingle/vextendsingle+¯NN3/2max
1≤k,l≤N/vextendsingle/vextendsingle/vextendsingle1
N2−w⋆
kw⋆
l/vextendsingle/vextendsingle/vextendsingle
+¯N·max
1≤i≤N/vextendsingle/vextendsingle/vextendsingle1
N−w⋆
i/vextendsingle/vextendsingle/vextendsingle+N·|Θ⋆−Θ⋆
(N)|/bracerightig
, (91)
whereCis a constant depending on λ,β,Θ⋆,(w⋆
1,···,w⋆
N),Kx,KyandT. Then the desired estimate (88)
is established by (90), (73), (78) and (91).
Now we prove the claim (91). From (45) and (41), we obtain the equation that /hatwidersatisfies on 0≤t≤T:


/hatwider(t) =−(/hatwideS(t+ 1)−λΘ⋆
(N))⊤[(λ+β)INp+/hatwideP(t+ 1)]−1·(/hatwideS(t+ 1)−λΘ⋆
(N))
+λ∥Θ⋆
(N)∥2
2+/hatwider(t+ 1),
/hatwider(T) =/summationtextN
i=1(1/N)/summationtext|Di|
j=1|yi
j|2.(92)
40Under review as submission to TMLR
Thenr−/hatwidersatisfies the equation on 0≤t≤T
r(t)−/hatwider(t)
=r(t+ 1)−/hatwider(t+ 1) +λ(Θ⋆−Θ⋆
(N))(Θ⋆+ Θ⋆
(N))
−(S(t+ 1)−λΘ⋆)⊤/bracketleftbig
(M(P(t+ 1)))−1−(M(/hatwideP(t+ 1)))−1/bracketrightbig
(S(t+ 1)−λΘ⋆)
−/bracketleftbig
S(t+ 1)−/hatwideS(t+ 1)−λ(Θ⋆−Θ⋆
(N))/bracketrightbig
(M(/hatwideP(t+ 1)))−1· (93)
/bracketleftbig
S(t+ 1) +/hatwideS(t+ 1)−λ(Θ⋆+ Θ⋆
(N))/bracketrightbig
,
r(T)−/hatwider(T) =N/summationdisplay
i=1(w⋆
i−1/N)|Di|/summationdisplay
j=1|yi
j|2.
Inductively, we have
r(0)−/hatwider(0) =r(T)−/hatwider(T) +Tλ(Θ⋆−Θ⋆
(N))(Θ⋆+ Θ⋆
(N))
−T/summationdisplay
t=1(S(t)−λΘ⋆)⊤/bracketleftbig
(M(P(t)))−1−(M(/hatwideP(t)))−1/bracketrightbig
(S(t)−λΘ⋆)
−T/summationdisplay
t=1/bracketleftbig
S(t)−/hatwideS(t)−λ(Θ⋆−Θ⋆
(N))/bracketrightbig
(M(/hatwideP(t)))−1/bracketleftbig
S(t) +/hatwideS(t)−λ(Θ⋆+ Θ⋆
(N))/bracketrightbig
.
By Lemma 1, we have
|r(0)−/hatwider(0)|≤|r(T)−/hatwider(T)|+Tλ|Θ⋆−Θ⋆
(N)|·|Θ⋆+ Θ⋆
(N)|
+T/summationdisplay
t=1/vextendsingle/vextendsingle(M(P(t)))−1−(M(/hatwideP(t)))−1/vextendsingle/vextendsingle·/vextendsingle/vextendsingleS(t)−λΘ⋆/vextendsingle/vextendsingle2
+T/summationdisplay
t=1/bracketleftbig
|S(t)−/hatwideS(t)|+λ|Θ⋆−Θ⋆
(N)|/bracketrightbig
·/vextendsingle/vextendsingle(M(/hatwideP(t)))−1/vextendsingle/vextendsingle·/vextendsingle/vextendsingleS(t) +/hatwideS(t)−λ(Θ⋆+ Θ⋆
(N))/vextendsingle/vextendsingle.(94)
Then the estimate (91) follows from (94), (74), (78), and Remark 5.
Proof of Theorem 5. Under Assumptions 1 and 2, the existence of the regret-optimal algorithm θ·is implied
by Theorem 8. The first claim follows from Proposition 2 upon noting that the constant “ Cw” therein is
bounded as a function of w; whence we take the constant Chidden byOto beCdef.= supw∈[0,1]N/summationtextN
i=1wi=1Cw.
Likewise, the second statement follows directly from Corollary 2 upon defining the constant hidden by Oas
before.
Further investigation of the “symmetrized system” (71)reveals a symmetric form of /hatwideP, which suggests that
/hatwidePcan be decomposed into homogeneous submatrices of lower dimensions. Specifically, if we decompose the
high dimensional Np×Npmatrix/hatwidePintoN×Nsubmatrices of dimensions p×peach, and either exchange
two submatrices symmetrically positioned with respect to the diagonal of /hatwidePor exchange two submatrices
on the diagonal, the system (71)is invariant. This suggests that all the submatrices on the diagonal are
homogeneous and all the off-diagonal submatrices are homogeneous.
Proposition 4 (Symmetries in /hatwideP).The solution of (71)has the following submatrix decomposition
/hatwideP=
π1π2···π2
π2π1···π2
............
π2π2···π1
∈RNp×Np, π 1∈Rp×p, π 2∈Rp×p, (95)
41Under review as submission to TMLR
and the submatrices π1andπ2are uniquely determined by the system


π1(t) =βIp−β2γ1(t+ 1),
π2(t) =−β2γ2(t+ 1),1≤t≤T−1,
π1(T) =π2(T) = (1/N3)/summationtextN
i=1/summationtext|Di|
j=1ui
jui⊤
j,(96)
whereγ1(·)andγ2(·)are defined as (18)and(19).
We argue as in (Huang & Yang, 2021, Lemmata 1) in deriving the following submatrix decomposition of /hatwideP.
Proof of Proposition 4. We decompose the Np×Npidentity matrix into N×Nsubmatrices I= (Iij)1≤i,j≤N
with eachIij∈Rp×p. For each 1≤i,j≤N, byEijwe denote the Np×Npelementary matrix by
exchanging the ith andjth rows of submatrices of the Np×Npidentity matrix I= (Iij)1≤i,j≤N. Note that
Eij=E−1
ij=ET
ij. Multiplying both sides of (36) from the left by Eijand from the right by Eij, we obtain
/braceleftigg
Eij/hatwideP(t)Eij=βINp−β2[(λ+β)INp+Eij/hatwideP(t+ 1)Eij]−1,
Eij/hatwideP(T)Eij= [(1/N)Ip,···,(1/N)Ip]⊤/parenleftig/summationtextN
i=11
N/summationtext|Di|
j=1ui
jui⊤
j/parenrightig
[(1/N)Ip,···,(1/N)Ip].(97)
Comparing (97) and (71) reveals that /hatwideP(t)andEij/hatwideP(t)Eijsatisfy the same ODE for all 1≤i,j≤N. This
observation implies that
/hatwidePii=/hatwidePjj,/hatwidePij=/hatwidePji,/hatwidePik=/hatwidePjk,/hatwidePki=/hatwidePkj,∀1≤i,j≤N,∀k̸=i,j,
and the first assertion of the proposition is proved.
Since/hatwidePtakes the form (95), it is straight forward to verify that [(λ+β)INp+/hatwideP]−1takes the form
[(λ+β)INp+/hatwideP]−1=
γ1γ2···γ2
γ2γ1···γ2
............
γ2γ2···γ1
, (98)
withγ1andγ2given by (18) and (19).
We substitute (95) and (98) into (71) to obtain the equations (96).
Upon substituting /hatwidePinto(72), we find that the Np×1matrix/hatwideScan be decomposed into homogeneous
submatrices of dimensions p×1each.
Proposition 5. The solution of (72)can be decomposed into submatrices
/hatwideS= (π⊤
3,···,π⊤
3)⊤∈RNp×1, π 3∈Rp×1, (99)
andπ3satisfies
/braceleftigg
π3(t) =β/bracketleftbig
γ1(t+ 1) + (N−1)γ2(t+ 1)/bracketrightbig
(π3(t+ 1)−λθ⋆
(N)),
π3(T) =−(1/N2)/summationtextN
i=1/summationtext|Di|
j=1ui
jyi
j.(100)
Proof of Proposition 5. We multiply both sides of (72)from the left by Eijdefined in the proof of Proposition
4,
Eij/hatwideS(t) =Eijβ[(λ+β)INp+/hatwideP(t+ 1)]−1(/hatwideS(t+ 1)−λΘ⋆
(N))
=Eijβ[(λ+β)INp+/hatwideP(t+ 1)]−1EijEij(/hatwideS(t+ 1)−λΘ⋆
(N))
=β[(λ+β)INp+Eij/hatwideP(t+ 1)Eij]−1(Eij/hatwideS(t+ 1)−λΘ⋆
(N)) (101)
42Under review as submission to TMLR
and
Eij/hatwideS(T) =−Eij[(1/N)Ip,···,(1/N)Ip]⊤N/summationdisplay
i=11
N|Di|/summationdisplay
j=1ui
jyi
j
=−[(1/N)Ip,···,(1/N)Ip]⊤N/summationdisplay
i=11
N|Di|/summationdisplay
j=1ui
jyi
j. (102)
From(101)and(101), wehavethat Eij/hatwideSalsosatisfies (72)forall 1≤i,j≤N. Thisprovesthatallsubmatrices
of/hatwideS= (/hatwideSi)1≤i≤Nare equal on 0≤t≤T. We further substitute (95), (98) and /hatwideS= [π⊤
3,···,π⊤
3]⊤into (72)
to obtain (100).
Corollary 3. Fix a data setDand assume that (17)holds. Then the computational complexity of computing
the sequence/parenleftbig
θ(t)/parenrightbigT
t=0by(70)isO(Tp2max{N,p0.373}).
Proof of Corollary 3.
Complexity of computing π1(T),π2(T), andπ3(T)isO(N2p2)
Computing ui
jui⊤
jhas complexity O(p2)by Lemma 3-(iii) and computing the sum/summationtextN
i=1/summationtext|Di|
j=1ui
jui⊤
jhas a complexity of O(N2p2)by Lemma 3-(i). Moreover, multiplying the p×pmatrix
/summationtextN
i=1/summationtext|Di|
j=1ui
jui⊤
jby the scalar 1/N3has complexity O(p2). Hence the computing π1(T)andπ2(T)has
complexityO(N2p2). Computing the product ui
jyi
jhas a complexity of O(p)by Lemma 3-(iii), and summing/summationtextN
i=1/summationtext|Di|
j=1ui
jyi
jhas complexity O(N2p)by Lemma 3-(i). Multiplying the p×1vector/summationtextN
i=1/summationtext|Di|
j=1ui
jyi
jby
the scalar 1/N2has a complexity of O(p). Therefore computing π3(T)has complexity O(N2p), and the
complexity of computing π1(T),π2(T), andπ3(T)isO(N2p).
Complexity of Computing γ1(T)andγ2(T)isO(p2.373)
Withπ1(T)andπ2(T)obtained, we can compute γ1(T)andγ2(T)by(18)and(19). From(18)and(19),
computing γ1(·)andγ2(·)fromπ1(·)andπ2(·)involves multiplication and addition of p×pmatrices, scalar
multiplication of p×p-dimensional matrices, multiplication of a p×pmatrix and a p×1vector, and inversion
of ap×pmatrix. By Lemma 3, multiplication of two p×pmatrices has the dominant complexity of O(p2.373)
among the above matrix operations. Moreover, inverting a p×pmatrix has complexity O(p2,373). Hence,
computing γ1(T)andγ2(T)has a complexity of O(p2.373).
Complexity of computing π1(t),π2(t),π3(t),γ1(t)andγ2(t)fort=T−1,T−2, ..., 1has complexity
O(Tp2.373).
Similar to computing γ1(T)andγ2(T), computing γ1(t)andγ2(t)fromπ1(t+ 1)andπ2(t+ 1)has complexity
O(p2.373). According to (96), computing π1(t)andπ2(t)fromγ1(t+ 1)andγ2(t+ 1)involves scalar
multiplication and addition along diagonal of p×pmatrices, and therefore has complexity O(p2)by Lemma 3-
(i) and (ii), According to (100), computing π3(t)fromγ1(t+ 1),γ2(t+ 1), andπ3(t+ 1)involves scalar
multiplication and addition of p×pmatrices, scalar multiplication and addition of p×1matrices, and
multiplication of a p×pmatrix with a p×1matrix. By Lemma 3, the dominant complexity of these
operations isO(p2). Summing up the above argument, computing π1(t),π2(t),π3(t),γ1(t)andγ2(t)for each
thas a complexity of O(p2.373), and therefore has a complexity of O(Tp2.373)for allt=T−1,T−2,...,1.
The Cost of Computing Each Θ(t)isO(Tp2max{N,p0.373}).
By(9), the cost of updating from Θ(t)toΘ(t+ 1)is equal to the complexity of computing /hatwideα(t) =
(/hatwideα⊤
1(t),···/hatwideα⊤
N(t))⊤fort= 0,1,...,T−1.
By (20),/hatwideαi(t)for alli= 1, ...,Nshare the common term
−/braceleftbig
γ1(t+ 1)π2(t+ 1) +γ2(t+ 1)[λIp+π1(t+ 1) + (N−2)π2(t+ 1)]/bracerightbigN/summationdisplay
k=1θk(t)
−/bracketleftbig
γ1(t+ 1) + (N−1)γ2(t+ 1)/bracketrightbig
(π3(t+ 1)−λθ⋆
(N)),
43Under review as submission to TMLR
and computing the common term involves matrix multiplication, addition, and scalar multiplication of p×p
matrices, matrix addition and scalar multiplication of p×1matrices, and multiplication of a p×pmatrix
and ap×1matrix. By Lemma 3, these operations have complexity O(Np+p2.373), whereNpis due to
adding the N p-dimensional vectors/summationtextN
k=1θk(t), andp2.373is due to multiplication of two p×pmatrices by
a CW-like algorithm.
Computing the individual terms
−(γ1(t+ 1)−γ2(t+ 1))(λIp+π1(t+ 1)−π2(t+ 1))θi(t)
for/hatwideαi(t),i= 1, ...,Ninvolves addition, scalar multiplication, and matrix multiplication of p×pmatrices,
and multiplication of a p×pmatrix and a p×1matrix. By Lemma 3, these operations have complexity of
O(p2.373+Np), wherep2.373is due to multiplication of two p×pmatrices by a CW-like algorithm and Npis
due to multiplying a p×pmatrix by the p-dimensional vector θi(t)fori= 1, ...,N. Hence the complexity of
computing/hatwideα(t)fort= 0,1,...,T−1isO(Tp2max{N,p0.373}).
The complexity of computing (Θ(t))T
t=0isO(Tp2max{N,p0.373})
Looking over all computations involved, we deduce that the complexity of computing the sequence (Θ(t))T
t=1
starting from Θ(0) = Θ⋆has a complexity of O(Tp2max{N,p0.373}).
We may now deduce Theorem 6 and Theorem 7.
Proof of Theorem 6. The result directly follows from Theorem 3 and Corollary 3.
Proof of Theorem 7. Direct consequence of Propositions 4 and 5.
C Background
In this paper, we consider regret-optimal memoryless training dynamics for finite-rank kernel ridgeregressors
(fKRR) trained from Nfinite datasets drawn from N(possibly) different data-generating distributions. We
are considering a scenario in which these datasets can originate from Ndistinct tasks or be adversarially
generated perturbations of the initial dataset, and we aim for our fKRR to exhibit robustness towards them.
Any continuous feature map ϕ:Rd→Rp, induces a hypothesis of fKRR fθ:Rd→Reach of which is
uniquely determined by a trainable parameter θ∈Rpvia
fθ(x) =ϕ(x)⊤θ; (103)
where all vectors are represented as column-vectors and not row vectors.
The hypothesis class Hpossesses a natural reproducing kernel Hilbert space (RKHS) structure with inner-
product given for fθ,f˜θ∈Hby⟨fθ,f˜θ⟩ϕdef.=/summationtextp
k=1θk˜θk. This RKHS structure suggests an optimal selection
criterion to decide which hypothesis in Hbest describes a singletraining setD1def.={(x1
j,y1
j)}S1
j=1ofS1∈N+,
by solving the penalized least-squares problem
θ⋆
1∈arg min
θ∈Rp/vextenddouble/vextenddoubleΦ(x1)θ−y1/vextenddouble/vextenddouble2
2+κ/vextenddouble/vextenddoubleθ/vextenddouble/vextenddouble2
2, (104)
where
Φ(x1)def.=/parenleftbig
ϕ(x1
1), ϕ(x1
2),···, ϕ(x1
S1)/parenrightbig⊤∈RS1×p, y1def.= (y1
1,y1
2,···,y1
S1)⊤∈RS1,
andκ>0is a fixed hyperparameter. In this classical case, the Representer Theorem Kimeldorf & Wahba
(1970; 1971) states that the optimal choice of fθto the convex optimization problem (104)is parameterized
by
θ⋆
1= (Φ⊤(x1)Φ(x1) +κIp)−1Φ⊤(x1)y1, (105)
44Under review as submission to TMLR
Alternatively, the optimal parameter choice θ⋆
1can be arrived at via gradient descent since(104)has a strictly
convex loss with a Lipschitz gradient. The training of fKRR on a single dataset is very-well understood.
Likewise, other aspects of fKRR are well understood; namely, their statistical properties Cheng et al. (2023),
their in-sample behaviour in Amini et al. (2022), and even the approximation power and risk of randomly
generated fKRR are respectively known Gonon et al. (2023) and Gonon et al. (2020).
Using the fKRR (103), the joint terminal time objective (3) simplifies to
l(θ1,...,θN,w;D)def.=N/summationdisplay
i=1wi/parenleftbiggSi/summationdisplay
j=1/vextendsingle/vextendsingle⟨ui
j,θw⟩−yi
j/vextendsingle/vextendsingle2/parenrightbigg
s.t.θwdef.=N/summationdisplay
i=1θiwi,(106)
where⟨·,·⟩denotes the Euclidean inner-product, ui
jdef.=ϕ(xi
j)and, as in (3),w= (w1,···,wN)∈[0,1]Nwith/summationtextN
i=1wi= 1.
45