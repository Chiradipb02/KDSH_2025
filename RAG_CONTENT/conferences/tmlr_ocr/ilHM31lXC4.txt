Published in Transactions on Machine Learning Research (May/2023)
Personalized Federated Learning: A Unified Framework and
Universal Optimization Techniques
Filip Hanzely∗fhanzely@gmail.com
Toyota Technological Institute at Chicago
Chicago, IL 60637, USA
Boxin Zhao∗boxinz@uchicago.edu
Mladen Kolar mkolar@chicagobooth.edu
The University of Chicago Booth School of Business
Chicago, IL 60637, USA
Reviewed on OpenReview: https: // openreview. net/ forum? id= ilHM31lXC4
Abstract
We investigate the optimization aspects of personalized Federated Learning (FL). We pro-
pose general optimizers that can be applied to numerous existing personalized FL objec-
tives, specifically a tailored variant of Local SGD and variants of accelerated coordinate
descent/accelerated SVRCD. By examining a general personalized objective capable of re-
covering many existing personalized FL objectives as special cases, we develop a compre-
hensive optimization theory applicable to a wide range of strongly convex personalized FL
models in the literature. We showcase the practicality and/or optimality of our methods
in terms of communication and local computation. Remarkably, our general optimization
solvers and theory can recover the best-known communication and computation guarantees
for addressing specific personalized FL objectives. Consequently, our proposed methods can
serve as universal optimizers, rendering the design of task-specific optimizers unnecessary
in many instances.
1 Introduction
Modern personal electronic devices, such as mobile phones, wearable devices, and home assistants, can col-
lectively generate and store vast amounts of user data. This data is essential for training and improving
state-of-the-art machine learning models for tasks ranging from natural language processing to computer
vision. Traditionally, the training process was performed by first collecting all the data into a data cen-
ter (Dean et al., 2012), raising serious concerns about user privacy and placing a considerable burden on
the storage capabilities of server suppliers. To address these issues, a novel paradigm – Federated Learning
(FL) (McMahan et al., 2017; Kairouz et al., 2021) – has been proposed. Informally, the main idea of FL is
to train a model locally on an individual’s device, rather than revealing their data, while communicating the
model updates using private and secure protocols.
While the original goal of FL was to search for a single model to be deployed on each device, this objective
has been recently questioned. As the distribution of user data can vary greatly across devices, a single model
might not serve all devices simultaneously (Hard et al., 2018). Consequently, data heterogeneity has become
the main challenge in the search for efficient federated learning models. Recently, a range of personalized FL
approaches has been proposed to address data heterogeneity (Kulkarni et al., 2020), wherein different local
models are used to fit user-specific data while also capturing the common knowledge distilled from data of
other devices.
∗equal contribution
1Published in Transactions on Machine Learning Research (May/2023)
Since the motivation and goals of each of these personalized approaches vary greatly, examining them sep-
arately can only provide us with an understanding of a specific model. Fortunately, many personalized FL
modelsintheliteraturearetrainedbyminimizingaspeciallystructuredoptimizationprogram. Inthispaper,
we analyze the general properties of such an optimization program, which in turn provides us with high-level
principles for training personalized FL models. We aim to solve the following optimization problem
min
w,β/braceleftigg
F(w,β):=1
MM/summationdisplay
m=1fm(w,βm)/bracerightigg
, (1)
wherew∈Rd0corresponds to the shared parameters, β= (β1,...,βM)withβm∈Rdm,∀m∈[M]
corresponds to the local parameters, Mis the number of devices, and fm:Rd0+dm→Ris the objective that
depends on the local data at the m-th client.
By carefully designing the local loss fm(w,βm), the objective (1) can recover many existing personalized
FL approaches as special cases. The local objective fmdoes not need to correspond to the empirical loss
of a given model on the m-th device’s data. See Section 2 for details. Consequently, (1) serves as a unified
objective that encompasses numerous existing personalized FL approaches as special cases. The primary
goal of our work is to explore the problem (1) from an optimization perspective. By doing so, we develop a
universal convex optimization theory that applies to many personalized FL approaches.
1.1 Contributions
We outline the main contributions of this work.
Single personalized FL objective . We propose a single objective (1) capable of recovering many existing
convexpersonalizedFLapproachesbycarefullyconstructingthelocalloss fm(w,βm). Consequently, training
different personalized FL models is equivalent to solving a particular instance of (1).
Recovering best-known complexity and novel guarantees . We develop algorithms for solving (1)
and prove sharp convergence rates for strongly convex objectives. Specializing our rates from the general
setting to the individual personalized FL objectives, we recover the best-known optimization guarantees from
the literature or advance beyond the state-of-the-art with a single exception: objective (11) with λ > L′.
Therefore, our results often render optimization tailored to solve a specific personalized FL unnecessary in
many cases.
Universal (convex) optimization methods and theory for personalized FL . To develop an optimiza-
tion theory for solving (1), we impose particular assumptions on the objective: µ−strong convexity of Fand
convexity and (Lw,MLβ)-smoothness of fmfor allm∈[M](see Assumptions 1, 2). These assumptions are
naturally satisfied for the vast majority of personalized FL objectives in the literature, with the exception of
personalized FL approaches that are inherently nonconvex, such as MAML (Finn et al., 2017). Under these
assumptions, we propose three algorithms for solving the general personalized FL objective (1): i) Local
Stochastic Gradient Descent for Personalized FL (LSGD-PFL), ii) Accelerated Block Coordinate Descent
for Personalized FL (ACD-PFL), and iii) Accelerated Stochastic Variance Reduced Coordinate Descent for
Personalized FL (ASVRCD-PFL). The convergence rates of these methods are summarized in Table 1. We
emphasize that these optimizers can be used to solve many (convex) personalized FL objectives from the lit-
erature by casting a given objective as a special case of (1), oftentimes matching or outperforming algorithms
originally designed for the particular scenario.
Minimax optimal rates . We provide lower complexity bounds for solving (1). Using the construction
of Hendrikx et al. (2021), we show that to solve (1), one requires at least O/parenleftig/radicalbig
Lw/µlogϵ−1/parenrightig
commu-
nication rounds. Note that communication is often the bottleneck when training distributed and person-
alized FL models. Furthermore, one needs at least O/parenleftig/radicalbig
Lw/µlogϵ−1/parenrightig
evaluations of∇wFand at least
O/parenleftig/radicalbig
Lβ/µlogϵ−1/parenrightig
evaluations of∇βF. Given the n-finite sum structure of fmwith (Lw,MLβ)-smooth
components, we show that one requires at least O/parenleftig
n+/radicalbig
nLw/µlogϵ−1/parenrightig
stochastic gradient evaluations
with respect to w-parameters and at least O/parenleftig
n+/radicalbig
nLβ/µlogϵ−1/parenrightig
stochastic gradient evaluations with
2Published in Transactions on Machine Learning Research (May/2023)
Alg. Communication #∇w #∇β
LSGD-PFLmax(Lβτ−1,Lw)
µ
+σ2
MBτµϵ
+1
µ/radicalig
Lw(ζ2∗+σ2B−1)
ϵmax(Lβ,τLw)
µ
+σ2
MBµϵ
+τ
µ/radicalig
Lw(ζ2∗+σ2B−1)
ϵmax(Lβ,τLw)
µ
+σ2
MBµϵ
+τ
µ/radicalig
Lw(ζ2∗+σ2B−1)
ϵ
ACD-PFL/radicalbig
Lw/µ❀/radicalbig
Lw/µ❀/radicalbig
Lβ/µ❀
ASVRCD-PFL n+/radicalbig
nLw/µ n+/radicalbig
nLw/µ❀n+/radicalbig
nLβ/µ❀
Table 1: Complexity guarantees of the proposed methods when ignoring constant and log factors.
#∇w/#∇β:number of (stochastic) gradient calls with respect to the w/β-parameters. Symbol ❀, indicates
the minimax optimal complexity. Local Stochastic Gradient Descent (LSGD): Local access to B-minibatches
of stochastic gradients, each with σ2-bounded variance. Each device takes (τ−1)local steps in between the
communication rounds. Accelerated Coordinate Descent (ACD): Access to the full local gradient, yielding
both the optimal communication complexity and the optimal computational complexity (both in terms of
∇wand∇β). ASVRCD: Assuming that fiis ann-finite sum, the oracle provides access to a single stochastic
gradient with respect to that sum. The corresponding local computation is either optimal with respect to
∇wor with respect to ∇β. Achieving both optimal rates simultaneously remains an open problem.
respect toβ-parameters. We show that ACD-PFL is always optimal in terms of communication and local
computation when the full gradients are available, while ASVRCD-PFL can be optimal either in terms of
the number of evaluations of the w-stochastic gradient or the β-stochastic gradient. However, note that
ASVRCD-PFL cannot achieve optimal rate for both evaluations of the w-stochastic gradient and the β-
stochastic gradient simultaneously, which we leave for future research.
Personalization and communication complexity . Given that a specific FL objective contains a pa-
rameter that determines the amount of personalization, we observe that the value of/radicalbig
Lw/µis always a
non-increasing function of this parameter. Since the communication complexity of (1) is equal to/radicalbig
Lw/µ
up to constant and log factors, we conclude that personalization has a positive effect on the communication
complexity of training an FL model.
New personalized FL objectives . The universal personalized FL objective (1) enables us to obtain a
range of novel personalized FL formulations as special cases. While we study various (parametric) extensions
of known models, we believe that the objective (1) can lead to easier development of brand new objectives
as well. However, we stress that proposing novel personalized FL models is not the main focus of our work;
the paper’s primary focus is on providing universal optimization guarantees for (convex) personalized FL.
Despite the aforementioned benefits of our proposed unified framework, we acknowledge that this is neither
the only nor the universally best approach for personalized federated learning. However, providing a general
framework that can include many existing methods as special cases can help us gain a clear understanding
and motivate us to propose new personalized methods.
1.2 Assumptions and Notations
Complexity Notations. For two sequences {an}and{bn},an=O(bn)if there exists C > 0such that
|an/bn|≤Cfor allnlarge enough; an= Θ(bn)ifan=O(bn)andbn=O(an)simultaneously. Similarly,
an=˜O(bn)ifan=O(bnlogkbn)for somek≥0;an=˜Θ(bn)ifan=˜O(bn)andbn=˜O(an)simultaneously.
Local Objective. We assume three different ways to access the gradient of the local objective fm. The
first, and the simplest case, corresponds to having access to the full gradient of fmwith respect to either w
orβmfor allm∈[M]simultaneously. The second case corresponds to a situation where ∇fm(w,βm)is the
expectation itself, i.e.,
∇fm(w,βm) =Eξ∈Dm/bracketleftig
∇ˆfm(w,βm;ξ)/bracketrightig
, (2)
3Published in Transactions on Machine Learning Research (May/2023)
while having access to stochastic gradients with respect to either worβmsimultaneously for all m∈M,
where ˆfmrepresents the loss function on a single data point. The third case corresponds to a finite sum fm:
fm(w,βm) =1
nn/summationdisplay
i=1fm,i(w,βm), (3)
having access to ∇wfm,i(w,βm)or to∇βfm,i(w,βm)for allm∈[M]andi∈[n]selected uniformly at
random.
Assumptions. We argue that the objective (1) is capable of recovering virtually any (convex) personalized
FL objective. Since the structure of the individual personalized FL objectives varies greatly, it is important
to impose reasonable assumptions on the problem (1) in order to obtain meaningful rates in the special cases.
Assumption 1. The function F(w,β)is jointlyµ-strongly convex for µ≥0, while for all m∈[M], function
fm(w,βm)is jointly convex, Lw-smooth w.r.t. parameter wand(MLβ)-smooth w.r.t. parameter βm. In the
case whenµ= 0, assume additionally that (1)has a unique solution, denoted as w⋆andβ⋆= (β⋆
1,...,β⋆
M).
Whenfmis a finite sum (3), we require the smoothness of the finite sum components.
Assumption 2. Suppose that for all m∈[M],i∈[n], functionfm,i(w,βm)is jointly convex, Lw-smooth
w.r.t. parameter wand(MLβ)-smooth w.r.t. parameter βm.1
In Section 2 we justify Assumptions 1 and 2 and characterize the constants µ,Lw,Lβ,Lw,Lβfor special
cases of (1). Table 2 summarizes these parameters.
Price of generality. Since Assumption 1 is the only structural assumption we impose on (1), one cannot
hope to recover the minimax optimal rates, that is, the rates that match the lower complexity bounds, for
all individual personalized FL objectives as a special case of our general guarantees. Note that any given
instance of (1) has a structure that is not covered by Assumption 1, but can be exploited by an optimization
algorithm to improve either communication or local computation. Therefore, our convergence guarantees
are optimal in light of Assumption 1 only. Despite this, our general rates specialize surprisingly well as
we show in Section 2: our complexities are state-of-the-art in all scenarios with a single exception: the
communication/computation complexity of (11).
Individual treatment of wandβ.Throughout this work, we allow different smoothness of the objective
with respect to global parameters wand local parameters β. At the same time, our algorithm is allowed
to exploit the separate access to gradients with respect to wandβ, given that these gradients can be
efficiently computed separately. Without such a distinction, one might not hope for the communication
complexity better than Θ/parenleftbig
max{Lw,Lβ}/µlogϵ−1/parenrightbig
, which is suboptimal in the special cases. Similarly, the
computational guarantees would be suboptimal as well. See Section 2 for more details.
Data heterogeneity. While the convergence rate of LSGD-PFL relies on data heterogeneity (See The-
orem 1), we allow for an arbitrary dissimilarity among the individual clients for analyzing ACD-PFL and
ASVRCD-PFL (see Theorem 7 and Theorem 8). Our experimental results also support that ASCD-PFL
(ACD-PFL with stochastic gradient to reduce computation) and ASVRCD-PFL are more robust to data
heterogeneity compared to the widely used Local SGD.
The rest of the paper is organized as follows. In Section 2, we show how (1) can be used to recover various
personalized federated learning objectives in the literature. In Section 3, we propose a local-SGD based
algorithm, LSGD-PFL, for solving (1). We further establish computational upper bounds for LSGD-PFL
in strongly convex, weakly convex, and nonconvex cases. In Section 4, we discuss the minimax optimal
algorithms for solving (1). We first show the minimax lower bounds in terms of the number of communication
rounds, number of evaluations of the gradient of global parameters, and number of evaluations of the gradient
of local parameters, respectively. We subsequently propose two coordinate-descent based algorithms, ACD-
PFL and ASVRCD-PFL, which can match the lower bounds. In Section 5 and Section 6, we use experiments
on synthetic and real data to illustrate the performance of the proposed algorithms and empirically validate
the theorems. Finally, we conclude the paper with Section7. Technical proofs are deferred to the Appendix.
1It is easy to see that Lw≥Lw≥Lw
nandLβ≥Lβ≥Lβ
n.
4Published in Transactions on Machine Learning Research (May/2023)
2 Personalized FL objectives
We recover a range of known personalized FL approaches as special cases of (1). In this section, we detail
the optimization challenges that arise in each one of the special cases. We discuss the relation to our results,
particularly focusing on how Assumptions 1, 2 and our general rates (presented in Sections 3 and 4) behave
in the special cases. Table 2 presents the smoothness and strong convexity constants with respect to (1) for
the special cases, while Table 3 provides the corresponding convergence rates for our methods when applied
to these specific objectives. For the sake of convenience, define
Fi(w,β):=1
MM/summationdisplay
m=1fm,i(w,βm).
in the case when functions fmhave a finite sum structure (3).
2.1 Traditional FL
The traditional, non-personalized FL objective (McMahan et al., 2017) is given as
min
w∈RdF′(w):=1
MM/summationdisplay
m=1f′
m(w), (4)
wheref′
mcorresponds to the loss on the m-th client’s data. Assume that f′
misL′-smooth and µ′−strongly
convex for all m∈[M]. The minimax optimal communication to solve (4) up to ϵ-neighborhood of the
optimum is ˜Θ/parenleftig/radicalbig
L′/µ′logϵ−1/parenrightig
(Scaman et al., 2018). When f′
m=1
n/summationtextn
j=1f′
m,j(w)is ann-finite sum
with convex and L′−smooth components, the minimax optimal local stochastic gradient complexity is
˜Θ/parenleftig/parenleftig
n+/radicalbig
nL′/µ/parenrightig
logϵ−1/parenrightig
(Hendrikx et al., 2021). The FL objective (4) is a special case of (1) with
d1=···=dM= 0and our theory recovers the aforementioned rates.
2.2 Fully Personalized FL
At the other end of the spectrum lies the fully personalized FL where the m-th client trains their own model
without any influence from other clients:
min
β1,...,βM∈RdFfull(β):=1
MM/summationdisplay
m=1f′
m(βm). (5)
The above objective is a special case of (1) with d0= 0. As the objective is separable in β1,...,βM, we
do not require any communication to train it. At the same time, we need ˜Θ/parenleftig/parenleftig
n+/radicalbig
nL′/µ/parenrightig
logϵ−1/parenrightig
local
stochastic oracle calls to solve it (Lan & Zhou, 2018) – which is what our algorithms achieve.
2.3 Multi-Task FL of Li et al. (2020)
The objective is given as
min
β1,...,βM∈RdFMT(β) =1
MM/summationdisplay
i=1/parenleftbigg
f′
m(βm) +λ
2∥βm−(w′)∗∥2/parenrightbigg
, (6)
where (w′)∗is a solution of the traditional FL in (4) and λ≥0. Assuming that (w′)∗is known (which Li
et al. (2020) does), the problem (6) is a particular instance of (5); thus our approach achieves the optimal
complexity.
A more challenging objective (in terms of optimization) is the following relaxed version of (6):
min
w,β1
MM/summationdisplay
m=1/parenleftbig
Λf′
m(w) +f′
m(βm) +λ∥w−βm∥2/parenrightbig
, (7)
5Published in Transactions on Machine Learning Research (May/2023)
F(w,β) µ LwLβLwLβRate?
Traditional FL ((4))
(McMahan et al., 2017)µ′L′0L′0 recovered
Fully Personalized FL
((5))µ′
M0L′
M0L′
Mrecovered
MT2 ((8))
(Li et al., 2020)λ
2MΛL′+λ
2ML′+λ
2MΛL′+λ
2ML′+λ
2Mnew♣
MX2 ((11))
(Smith et al., 2017)µ′
3Mλ
ML′+λ
Mλ
ML′+λ
Mrecovered♠
APFL2 ((14))
(Deng et al., 2020)µ′(1−αmax)2
M(Λ+α2
max)L′
M(1−αmin)2L′
M(Λ+α2
max)L′
M(1−αmin)2L′
Mnew♣
WS2 ((16))
(Liang et al., 2020)µ′L′L′L′L′new
Fed Residual ((18))
(Agarwal et al., 2020)µ Lw
R Lβ
RLw
RLβ
R new
Table 2: Parameters in Assumptions 1 and 2 for personalized FL objectives, with a note about the rate: we either recover the best-known rate for a
given objective, or provide a novel rate that is the best under the given assumptions.♣: Rate for the novel personalized FL objective (extension of a
known one).♠: Best-known communication complexity recovered only for λ=O(L′).
F(w,β) # Comm #∇wF #∇βF #∇wFi #∇βFi
Traditional FL ((4))
(McMahan et al., 2017)/radicalig
L′
µ′/radicalig
L′
µ′ 0 n+/radicalig
nL′
µ′ 0
Fully Personalized FL
((5))0 0/radicalig
L′
µ′ 0 n+/radicalig
nL′
µ′
MT2 ((8))
(Li et al., 2020)/radicalig
ΛL′
λ/radicalig
ΛL′
λ/radicalig
L′
λn+/radicalig
nΛL′
λn+/radicalig
nL′
λ
MX2 ((11))
(Smith et al., 2017)/radicalig
λ
µ′/radicalig
λ
µ′/radicalig
L′+λ
µ′ - n+/radicalig
n(L′+λ)
µ′
APFL2 ((14))
(Deng et al., 2020)/radicalig
(Λ+α2max)L′
(1−αmax)2µ′/radicalig
(Λ+α2max)L′
(1−αmax)2µ′/radicalig
(1−αmin)2L′
(1−αmax)2µ′n+/radicalig
n(Λ+α2max)L′
(1−αmax)2µ′n+/radicalig
n(1−αmin)2L′
(1−αmax)2µ′
WS2 ((16))
(Liang et al., 2020)/radicalig
L′
µ′/radicalig
L′
µ′/radicalig
L′
µ′ n+/radicalig
nL′
µ′ n+/radicalig
nL′
µ′
Fed Residual ((18))
(Agarwal et al., 2020)/radicalig
Lw
R
µ/radicalig
Lw
R
µ/radicalbigg
Lβ
R
µn+/radicalig
nLw
R
µ′ n+/radicalbigg
nLβ
R
µ′
Table 3: Complexity of solving personalized FL objectives by Algorithms 2 (second, third, and fourth column) and 3 (fifth and sixth column).
Constant and log factors are ignored.
6Published in Transactions on Machine Learning Research (May/2023)
where Λ≥0is the relaxation parameter, recovering the original objective for Λ→∞. Note that, since
Λ→∞, finding a minimax optimal method for the optimization of (6) is straightforward. First, one has
to compute a minimizer (w′)∗of the classical FL objective (4), which can be done with a minimax optimal
complexity. Next, one needs to compute the local solution β∗
m= arg minβm∈Rdf′
m(βm)+λ∥w∗−βm∥2, which
only depends on the local data and thus can also be optimized with a minimax optimal algorithm.
A more interesting scenario is obtained when we do not set Λ→∞in (7), but rather consider a finite
Λ>0that is sufficiently large. To obtain the right smoothness/strong convexity parameter (according to
Assumption 1), we scale the global parameter wby a factor of M−1
2and arrive at the following objective:
min
w,β1,...,βM∈RdFMT2(w,β) =1
MM/summationdisplay
m=1fm(w,βm), (8)
where
fm(w,βm) = Λf′
m(M−1
2w) +f′
m(βm) +λ
2∥βm−M−1
2w∥2.
The next lemma determines parameters µ,Lw,Lβ,Lw,Lβin Assumption 1. See the proof in Appendix B.1.
Lemma 1. LetΛ≥3λ/(2µ′). Then, the objective (8)is jointly (λ/(2M))−strongly convex, while the
functionfmis jointly convex, ((ΛL′+λ)/M)-smooth w.r.t. wand(L′+λ)-smooth w.r.t. βm. Similarly, the
functionfm,jis jointly convex, ((ΛL′+λ)/M)-smooth w.r.t. wand(L′+λ)-smooth w.r.t. βm.
Evaluating gradients. Note that evaluating ∇wfm(x,βm)under the objective (8) can be perfectly de-
coupled from evaluating ∇βfm(x,βm). Therefore, we can make full use of our theory and take advantage
of different complexities w.r.t. ∇wand∇β. Resulting communication and computation complexities for
solving (8) are presented in Table 3.
2.4 Multi-Task Personalized FL and Implicit MAML
In its simplest form, the multi-task personalized objective (Smith et al., 2017; Wang et al., 2018) is given as
min
β1,...,βM∈RdFMX(β) =1
MM/summationdisplay
m=1f′
m(βm) +λ
2MM/summationdisplay
m=1∥¯β−βm∥2, (9)
where ¯β:=1
M/summationtextM
m=1βmandλ≥0(Hanzely & Richtárik, 2020). On the other hand, the goal of implicit
MAML (Rajeswaran et al., 2019; Dinh et al., 2020) is to minimize
min
w∈RdFME(w) =1
MM/summationdisplay
i=1/parenleftbigg
min
βm∈Rd/parenleftbigg
f′
m(βm) +λ
2∥w−βm∥2/parenrightbigg/parenrightbigg
. (10)
By reparametrizing (1), we can recover an objective that is simultaneously equivalent to both (9) and (10).
In particular, by setting
fm(w,βm) =f′
m(βm) +λ∥M−1
2w−βm∥2,
the objective (1) becomes
min
w,β1,...,βM∈RdFMX2(w,β):=1
MM/summationdisplay
m=1f′
m(βm) +λ
2MM/summationdisplay
m=1∥M−1
2w−βm∥2. (11)
It is a simple exercise to notice the equivalence of (11) to both (9) and (10).2Indeed, we can always mini-
mize (11) in w, arriving at w∗=M1
2¯β, and thus recovering the solution of (9). Similarly, by minimizing (11)
inβwe arrive at (10).
Next, we establish the parameters in Assumptions 1 and 2.
2To the best of our knowledge, we are the first to notice the equivalence of (9) and (10).
7Published in Transactions on Machine Learning Research (May/2023)
Lemma 2. Letµ′≤λ/2. Then the objective (11)is jointly (µ′/(3M))-strongly convex, while fmis(λ/M )-
smooth w.r.t. wand(L′+λ)-smooth w.r.t. β. The function
fm,i(w,βm) =f′
m,i(βm) + (λ/2)∥M−1
2w−βm∥2
is jointly convex, (λ/M )-smooth w.r.t. wand(L′+λ)-smooth w.r.t. β.
The proof is given in Appendix B.2. Hanzely et al. (2020a) showed that the minimax optimal communication
complexity to solve (9) (and therefore to solve (10) and (11)) is Θ/parenleftig/radicalbig
min(L′,λ)/µ′logϵ−1/parenrightig
. Furthermore,
they showed that the minimax optimal number of gradients w.r.t. f′is˜Θ/parenleftig/parenleftig/radicalbig
L′/µ′/parenrightig
logϵ−1/parenrightig
and proposed
a method that has the complexity Θ/parenleftig/parenleftig
n+/radicalbig
n(L′+λ)/µ′/parenrightig
logϵ−1/parenrightig
w.r.t. the number of f′
m,j-gradients. We
match the aforementioned communication guarantees when λ=O(L′)and computation guarantees when
L′=O(λ). Furthermore, when λ=O(L′), our complexity guarantees are strictly better compared to the
guarantees for solving the implicit MAML objective (10) directly (Rajeswaran et al., 2019; Dinh et al., 2020).
2.5 Adaptive Personalized FL (Deng et al., 2020)
The objective is given as
min
β1,...,βMFAPFL (β) =1
MM/summationdisplay
m=1f′
m((1−αm)βm+αm(w′∗)), (12)
where (w′)∗= arg minw∈RdF′(w)is a solution to (4) and 0<α1,...αM<1. Assuming that (w′)∗is known,
as was done in Deng et al. (2020), the problem (12) is an instance of (5); thus our approach achieves the
optimal complexity.
A more interesting case (in terms of optimization) is when considering a relaxed variant of (12), given as
min
w,β1
MM/summationdisplay
m=1(Λf′
m(w) +f′
m((1−αm)βm+αmw)) (13)
where Λ≥0is the relaxation parameter that allows recovering the original objective when Λ→∞. Such a
choice, alongside with the usual rescaling of the parameter wresults in the following objective:
min
w,β1,...,βM∈RdFAPFL 2(w,β) :=1
MM/summationdisplay
i=1f(w,βm), (14)
where
f(w,βm) = Λf′
m(M−1
2w) +f′
m((1−αm)βm+αmM−1
2w).
Lemma 3. Letαmin:= min 1≤m≤Mαmandαmax:= max 1≤m≤Mαm. If
Λ≥max
1≤m≤M(3α2
m+ (1−αm)2/2),
then the function FAPFL 2is jointly/parenleftbig
µ′(1−αmax)2/M/parenrightbig
-strongly convex,/parenleftbig
(Λ +α2
max)L′/M/parenrightbig
-smooth w.r.t. w
and/parenleftbig
(1−αmin)2L′/M/parenrightbig
-smooth w.r.t. β.
The proof is given in Appendix B.3.
2.6 Personalized FL with Explicit Weight Sharing
The most typical example of the weight sharing setting is when parameters w,βcorrespond to different
layers of the same neural network. For example, β1,...,βMcould be the weights of first few layers of a
neural network, while ware the weights of the remaining layers (Liang et al., 2020). Or, alternatively, each
8Published in Transactions on Machine Learning Research (May/2023)
ofβ1,...,βMcan correspond to the weights of last few layers, while the remaining weights are included in
the global parameter w(Arivazhagan et al., 2019). Overall, we can write the objective as follows:
min
w∈Rdw,
β1,...,βM∈RdβFWS(w,β) =1
MM/summationdisplay
m=1f′
m([w,βm]), (15)
wheredw+dβ=d. Using an equivalent reparameterization of the w−space, we aim to minimize
min
w∈Rdw,
β1,...,βM∈RdβFWS2(w,β) =1
MM/summationdisplay
m=1f′
m([M−1
2w,βm]), (16)
which is an instance of (1) with fm(w,βm) =f′
m([M−1
2w,βm]).
Lemma 4. The function FWS2is jointlyµ′-strongly convex,/parenleftig
L′
M/parenrightig
-smooth w.r.t. wandL′-smooth w.r.t. β.
Similarly, the function fmis jointly convex,/parenleftig
L′
M/parenrightig
-smooth w.r.t. wandL′-smooth w.r.t. β.
The proof is straightforward and, therefore, omitted. A distinctive characteristic of the explicit weight
sharing paradigm is that evaluating a gradient w.r.t. w-parameters automatically grants either free or highly
cost-effective access to the gradient w.r.t. β-parameters (and vice versa).
2.7 Federated Residual Learning (Agarwal et al., 2020)
Agarwal et al. (2020) proposed federated residual learning:
min
w∈Rdw,
β1,...,βM∈RdβFR(w,β) =1
MM/summationdisplay
m=1lm(Aw(w,xw
m),Aβ(βm,xβ
m)), (17)
where (xw
m,xβ
m)is a local feature vector (there may be an overlap between xw
mandxβ
m),Aw(w,xw
m)represents
the model prediction using global parameters/features, Aβ(β,xβ
m)denotes the model prediction using local
parameters/features, and l(·,·)is a loss function. Clearly, we can recover (17) with
fm(w,βm) =l(Aw(M−1
2w,xw
m),Aβ(βm,xβ
m)). (18)
Unlike the other objectives, here we cannot relate constants µ′,L′,L′toFR, since we do not write fmas
a function of f′m. However, it seems natural to assume Lw
R(orLβ
R)-smoothness of l(Aw(w,xwm),aβ
m)(or
l(aw
m,Aβ(βm,xβ
m))) as a function of w(orβ) for anyaβ
m,xβ
m,aw
m,xw
m. Let us defineLw
R,Lβ
Ranalogously,
given thatlhas ann-finite sum structure. Assuming, furthermore, that Fisµ-strongly convex and fmis
convex (for each m∈M), we can apply our theory.
2.8 MAML Based Approaches
Meta-learning has recently been employed for personalization (Chen et al., 2018; Khodak et al., 2019; Jiang
et al., 2019; Fallah et al., 2020; Lin et al., 2020). Notably, the model-agnostic meta-learning (MAML) (Finn
et al., 2017) based personalized FL objective is given as
min
w∈RdFMAML (w) =1
MM/summationdisplay
m=1f′
m(w−α∇f′
m(w)). (19)
Although we can recover (19) as a special case of (1) by setting fm(w,βm) =f′
m(w−α∇f′
m(w)), our (convex)
convergence theory does not apply due to the inherent non-convex structure of (19). Specifically, objective
FMAMLis non-convex even if function f′
mis convex. In this scenario, only our non-convex rates of Local
SGD apply.
9Published in Transactions on Machine Learning Research (May/2023)
Algorithm 1 LSGD-PFL
inputStepsizes{ηk}k≥0∈R, starting point w0∈Rd0,β0
m∈Rdmfor allm∈[M], communication period τ.
fork= 0,1,2,...do
ifkmodτ= 0then
Send allwk
m’s to server, let wk=1
M/summationtextM
m=1wk
m
Sendwkto each device, set wk
m=wk,∀m∈[M]
end if
form= 1,2,...,Min parallel do
Sampleξk
1,m,...ξk
B,m∼Dmindependently
Computegk
m=1
B/summationtextB
j=1∇ˆfm(wk
m,βk
m;ξk
j,m)
Update the iterates (wk+1
m,βk+1
m) = (wk
m,βk
m)−ηk·gk
m
end for
end for
3 Local SGD
The most popular optimizer to train non-personalized FL models is the local SGD/FedAvg (McMahan et al.,
2016; Stich, 2019). We devise a local SGD variant tailored to solve the personalized FL objective (1) – LSGD-
PFL. See the detailed description in Algorithm 1. Specifically, LSGD-PFL can be seen as a combination
of local SGD applied on global parameters wand SGD applied on local parameters β. To mimic the non-
personalized setup of local SGD, we assume access to the local objective fm(w,βm)in the form of an unbiased
stochastic gradient with bounded variance.
Admittedly, LSGD-PFL was already proposed by Arivazhagan et al. (2019) and Liang et al. (2020) to solve
a particular instance of (1). However, no optimization guarantees were provided. In contrast, we provide
convergence guarantees for LSGD-PFL that recover the convergence rate of LSGD when d1=d2=···=
dM= 0and the rate of SGD when d0= 0. Next, we demonstrate that LSGD-PFL works best when applied
to an objective with rescaled w-space, unlike what was proposed in the aforementioned papers.
We will need the following assumption on the stochastic gradients.
Assumption 3. Assume that the stochastic gradients ∇wˆfm(w,βm,ζ)and∇βˆfm(w,βm,ζ)satisfy the fol-
lowing conditions for all m∈[M],w∈Rd0, andβm∈Rdm:
E∇wˆfm(w,βm,ζ) =∇wfm(w,βm),
E∇βˆfm(w,βm,ζ) =∇βfm(w,βm),
E∥∇wˆfm(w,βm,ζ)−∇wfm(w,βm)∥2≤σ2,and
EM/summationdisplay
m=1∥∇βˆfm(w,βm,ζ)−∇βfm(w,βm)∥2≤Mσ2.
Let
(wK,βK):=/parenleftiggK/summationdisplay
k=0(1−ηµ)−k−1/parenrightigg−1K/summationdisplay
k=0(1−ηµ)−k−1(wk,βk).
We are now ready to state the convergence rate of LSGD-PFL.
Theorem 1. Suppose that Assumptions 1 and 3 hold. Let ηk=ηfor allk≥0, whereηsatisfies
0<η≤min/braceleftbigg1
4Lβ,1
8√
3e(τ−1)Lw/bracerightbigg
.
10Published in Transactions on Machine Learning Research (May/2023)
Letζ2
∗:=1
MM/summationtext
m=1∥∇fm(w∗,β∗)∥2be the data heterogeneity parameter at the optimum. The iteration com-
plexity of Algorithm 1 to achieve Ef(wK,βK)−f(w∗,β∗)≤ϵis
˜O/parenleftigg
max/parenleftbig
Lβ,τLw/parenrightbig
µ+σ2
MBµϵ+τ
µ/radicalbigg
Lw(ζ2∗+σ2B−1)
ϵ/parenrightigg
.
TheiterationcomplexityofLSGD-PFLcanbeseenasasumoftwocomplexities: thecomplexityofminibatch
SGD to minimize a problem with a condition number Lβ/µand the complexity of local SGD to minimize a
problem with a condition number Lw/µ. Note that the key reason why we were able to obtain such a rate
for LSGD-PFL is the rescaling of the w-space by a constant M−1
2. Arivazhagan et al. (2019) and Liang
et al. (2020), where LSGD-PFL was introduced without optimization guarantees, did not consider such a
reparametrization.
We also have the following result for weakly convex objectives.
Theorem 2. Suppose that the conditions of Theorem 1 hold. Let µ= 0. The iteration complexity of
Algorithm 1 to achieve Ef(wK,βK)−f(w∗,β∗)≤ϵis
˜O/parenleftigg
max/parenleftbig
Lβ,τLw/parenrightbig
ϵ+σ2
MBϵ2+τ/radicalbig
Lw(ζ2∗+σ2B−1)
ϵ3
2/parenrightigg
.
The proofs of Theorem 1 and Theorem 2 can be found in Section 3.2. The reparametrization of wplays a
key role in proving the iteration complexity bound. Unlike the convergence guarantees for ACD-PFL and
ASVRCD-PFL, which are introduced in Section 4, we do not claim any optimality properties for the rates
obtained in Theorem 1 and Theorem 2. However, given the popularity of Local SGD as an optimizer for
non-personalized FL models, Algorithm 1 is a natural extension of Local SGD to personalized FL models,
and the corresponding convergence rate is an important contribution.
3.1 Nonconvex Theory for LSGD-PFL
To demonstrate that LSGD-PFL works in the nonconvex setting as well, we develop a non-convex theory for
it. Therefore, the algorithm is also applicable, for example, for solving the explicit MAML objective. Note
that we do not claim the optimality of our results.
Before stating the results, we need to make the following assumptions, which are slightly different from the
rest of the paper. First, we need a smoothness assumption on local objective functions.
Assumption 4. The local objective function fm(·)is differentiable and L-smooth, that is, ∥∇fm(u)−
∇fm(v)∥≤L∥u−v∥for allu,v∈Rd0+dmandm∈[M].
This condition is slightly different compared to the smoothness assumption on the objective stated in As-
sumption 1. Next, we need local stochastic gradients to have bounded variance.
Assumption5. The stochastic gradients ∇wˆfm(w,βm,ζ),∇βˆfm(w,βm,ζ)satisfy for all m∈[M],w∈Rd0,
βm∈Rdm:
Eζ/bracketleftig
∥∇wˆfm(w,βm,ζ)−∇wfm(w,βm)∥2/bracketrightig
≤C1∥∇wfm(w,βm)∥2+σ2
1
B,
Eζ/bracketleftig
∥∇βˆfm(w,βm,ζ)−∇βfm(w,βm)∥2/bracketrightig
≤C2∥∇βfm(w,βm)∥2+σ2
2
B,
for allm∈[M], whereC1,C2,σ2
1,σ2
2are all positive constants.
This assumption is common in the literature. See, for example, Assumption 3 in Haddadpour & Mahdavi
(2019). Note that this assumption is weaker than Assumption 3. We also need an assumption on data
heterogeneity.
11Published in Transactions on Machine Learning Research (May/2023)
Assumption 6 (Bounded Dissimilarity) .There is a positive constant λ>0such that for all w∈Rd0and
βm∈Rdm,m∈[M], we have
1
MM/summationdisplay
m=1∥∇fm(w,βm)∥2≤λ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(w,βm)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+σ2
dif.
This way of characterizing data heterogeneity was used in Haddadpour & Mahdavi (2019) – see Definition
1 therein. Given the above assumptions, we can establish the following convergence rate of LSGD-PFL for
general non-convex objectives.
Theorem 3. Suppose that Assumptions 4-6 hold. Let ηk=η, for allk≥0, whereηis small enough to
satisfy
−1 +ηLλ/parenleftbiggC1
M+C2+ 1/parenrightbigg
+λη2L2(τ−1)τ(C1+ 1)≤0. (20)
We have
1
KK−1/summationdisplay
k=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(wk,βk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

≤2E/bracketleftig
1
M/summationtextM
m=1fm(w0,β0
m)−f∗/bracketrightig
ηK+ηLλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbigg
+η2L2σ2
dif(τ−1)2(C1+ 1) +η2L2σ2
1(τ−1)2
B,
wherewk:=1
M/summationtextM
m=1wk
mis a sequence of so-called virtual iterates.
The following assumption is commonly used to characterize non-convex objectives in the literature.
Assumption 7 (µ-Polyak-Łojasiewicz (PL)) .There exists a positive constant µ > 0, such that for all
w∈Rd0andβm∈Rdm,m∈[M], we have
1
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(w,βm)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≥µ/parenleftigg
1
MM/summationdisplay
m=1fm(w,βm)−f∗/parenrightigg
.
When the local objective functions satisfy the PL-condition, we obtain a faster convergence rate stated in
the theorem below.
Theorem 4. Suppose that Assumptions 4-7 hold. Suppose ηk= 1/(µ(k+βτ+ 1)), whereβis a positive
constant satisfying
β >max/braceleftbigg2λL
µ/parenleftbiggC1
M+C2+ 1/parenrightbigg
−2,2L2λ(C1+ 1)
µ2,1/bracerightbigg
andτis large enough such that
τ≥/radicaligg
max/braceleftbig
(2L2λ(C1+ 1)/µ2)e1/β−4,0/bracerightbig
β2−(2L2λ(C1+ 1)/µ2)e1
β.
Then
E/bracketleftigg
1
MM/summationdisplay
m=1fm/parenleftbig
wK,βk
m/parenrightbig
−f∗/bracketrightigg
≤b3
(K+βτ)3E/bracketleftigg
1
MM/summationdisplay
m=1fm/parenleftbig
w0,β0
m/parenrightbig
−f∗/bracketrightigg
+2L2(τ−1)2K
µ3(K+βτ)3/braceleftbigg
σ2
dif(C1+ 1) +σ2
1
B/bracerightbigg
+LK(K+ 2βτ+ 2)
4µ2(K+βτ)3/braceleftbigg
σ2
dif/parenleftbiggC1
M+C2+ 1/parenrightbigg
+σ2
1
MB+σ2
2
B/bracerightbigg
.
The proofs of Theorem 3 and Theorem 4 can be found in Appendix B.7.
12Published in Transactions on Machine Learning Research (May/2023)
3.2 Proof of Theorem 1 and Theorem 2
The main idea consists of invoking the framework for analyzing local SGD methods introduced in Gorbunov
et al. (2021) with several minor modifications. In particular, Algorithm 1 is an intriguing method that runs
a local SGD on w-parameters and SGD on β-parameters. Therefore, we shall treat these parameter sets
differently. Define Vk:=1
M/summationtextM
m=1∥wk−wk
m∥2wherewk:=1
M/summationtextM
m=1wk
mis defined as in Theorem 3.
The first step towards the convergence rate is to figure out the parameters of Assumption 2.3 from Gorbunov
et al. (2021). The following lemma is an analog of Lemma G.1 in Gorbunov et al. (2021).
Lemma 5. Let Assumptions 1 and 3 hold. Let L= max{Lw,Lβ}. Then, we have:
1
MM/summationdisplay
m=1∥∇wfm(wk
m,βk
m)∥2≤6Lw/parenleftbig
f(wk,βk
m)−f(w∗,β∗)/parenrightbig
+ 3(Lw)2Vk+ 3ζ2
∗, (21)
and
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(wk
m,βk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
M2M/summationdisplay
m=1/vextenddouble/vextenddouble∇βfm(wk
m,βk
m)/vextenddouble/vextenddouble2≤4L/parenleftbig
f(wk,βk
m)−f(w∗,β∗)/parenrightbig
+2(Lw)2Vk.(22)
The proof is given in Appendix B.4. Using Lemma 5 we recover a set of crucial parameters of Assumption
2.3 from Gorbunov et al. (2021).
Lemma 6. Letgk
w,m:= (gk
m)1:d0andgk
β,m:= (gk
m)(d0+1):(d0+dm). Then
1
MM/summationdisplay
m=1E∥gk
w,m∥2≤6Lw/parenleftbig
f(wk,βk
m)−f(w∗,β∗)/parenrightbig
+ 3(Lw)2Vk+σ2
B+ 3ζ2
∗, (23)
and
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1gk
w,m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
M2M/summationdisplay
m=1/vextenddouble/vextenddoublegk
β,m/vextenddouble/vextenddouble2≤4L/parenleftbig
f(wk,βk
m)−f(w∗,β∗)/parenrightbig
+ 2(Lw)2Vk+2σ2
BM.(24)
The proof is given in Appendix B.5. Finally, the following lemma is an analog of Lemma E.1 in Gorbunov
et al. (2021) and gives us the remaining parameters of Assumption 2.3 therein.
Lemma 7. Suppose that Assumptions 1 and 3 hold and
η≤1
8√
3e(τ−1)Lw.
Then
2LwK/summationdisplay
k=0(1−ηµ)−k−1EVk≤1
2K/summationdisplay
k=0(1−ηµ)−k−1EF(wk,βk)−F(w∗,β∗) + 2LwDη2K/summationdisplay
k=0(1−ηµ)−k−1,(25)
where
D= 2e(τ−1)τ/parenleftbigg
3ζ2
∗+σ2
B/parenrightbigg
.
The proof is given in Appendix B.6. With these preliminary results, we are ready to state the main conver-
gence result for Algorithm 1.
Theorem 5. Suppose that Assumptions 1 and 3 hold and the stepsize ηsatisfies
0<η≤min/braceleftbigg1
4Lβ,1
8√
3e(τ−1)Lw/bracerightbigg
.
13Published in Transactions on Machine Learning Research (May/2023)
Define
(wK,βK):=/parenleftiggK/summationdisplay
k=0(1−ηµ)−k−1/parenrightigg−1K/summationdisplay
k=0(1−ηµ)−k−1(wk,βk),
Φ0:=2∥w0−w∗∥2+/summationtextM
m=1∥β0
m−β∗
m∥2
η,and
Ψ0:=2σ2
BM+ 8Lwηe(τ−1)τ/parenleftbigg
3ζ2
∗+σ2
B/parenrightbigg
.
Then, ifµ>0, we have
Ef(wK,βK)−f(w∗,β∗)≤(1−ηµ)KΦ0+ηΨ0, (26)
while, in the case when µ= 0, we have
Ef(wK,βK)−f(w∗,β∗)≤Φ0
K+ηΨ0. (27)
The proof follows directly from Theorem 2.1 of Gorbunov et al. (2021), once the conditions are verified, as
has been done in Lemma 5, Lemma 6, and Lemma 7. Theorem 1 and Theorem 2 then follow from Corollary
D.1 and Corollary D.2 of Gorbunov et al. (2021).
4 Minimax Optimal Methods
We discuss the complexity of solving (1) in terms of the number of communication rounds required to reach
anϵ-solution and the amount of local computation, both in terms of the number of (stochastic) gradients
with respect to global w-parameters and local β-parameters.
4.1 Lower Complexity Bounds
We provide lower complexity bounds for solving (1) when fmis a finite sum (3). We show that any algorithm
with access to the communication oracle and local (stochastic) gradient oracle with respect to the worβ
parameters requires at least a certain number of oracle calls to approximately solve (1).
Oracle. The considered oracle allows us at any iteration to compute either:
•∇wfm,i(wm,βm)on each device for a randomly selected i∈[n]and anywm,βm; or
•∇βfm,i(wm,βm)on each device for a randomly selected i∈[n]and anywm,βm; or
•the average of wm’s alongside broadcasting the average back to clients (communication step).
Our lower bound is provided for iterative algorithms whose iterates lie in the span of historical oracle queries
only. Let us denote such a class of algorithms as A. In particular, for each m,kwe must have
βk
m∈Lin/parenleftbig
β0
m,∇βfm(w0
m,β0
m),...,∇βfm(wk−1
m,βk−1
m)/parenrightbig
and
wk
m∈Lin/parenleftbig
w0,∇wfm(w0
m,β0
m),...,∇wfm(wk−1
m,βk−1
m),Ql(k)/parenrightbig
,
where
Qk=M/uniondisplay
m=1/braceleftig
w0,∇wfm(w0
m,β0
m),...,∇wfm(wl(k)
m,βl(k)
m)/bracerightig
,
withl(k)being the index of the last communication round until iteration k. While such a restriction is
widespread in the classical optimization literature (Nesterov, 2018; Scaman et al., 2018; Hendrikx et al.,
2021; Hanzely et al., 2020a), it can be avoided by more complex arguments (Nemirovskij & Yudin, 1983;
Woodworth & Srebro, 2016; Woodworth et al., 2018).
We then have the following theorem regarding the minimal calls of oracles for solving (1).
14Published in Transactions on Machine Learning Research (May/2023)
Algorithm 2 ACD-PFL
input 0<θ< 1,η,ν > 0,w0
y=w0
z∈Rd0,β0
y,m=β0
z,m∈Rdmfor1≤m≤M.
fork= 0,1,2,...do
wk+1
x= (1−θ)wk
y+θwk
z
form= 1,...,Min parallel do
βk+1
x,m= (1−θ)βk
y,m+θβk
z,m
end for
ξ=

1,with probability pw=√
Lw√
Lw+√
Lβ
0,with probability pβ=√
Lβ√
Lw+√
Lβ
ifξ= 0then
wk+1
y=wk+1
x−1
Lw1
M/summationtextM
m=1∇wfm(wk+1
x,βk+1
x,m)
wk+1
z=1
1+ην/parenleftig
wk
z+ηνwk+1
x−η√
Lw(√
Lw+√
Lβ)1
M/summationtextM
m=1∇wfm(wk+1
x,βk+1
x,m)/parenrightig
form= 1,...,Min parallel do
βk+1
z,m=1
1+ην/parenleftbig
βk
z,m+ηνβk+1
x,m/parenrightbig
end for
else
form= 1,...,Min parallel do
βk+1
y,m=βk+1
x,m−1
Lβ∇βfm(wk+1
x,βk+1
x,m)
end for
βk+1
z,m=1
1+ην/parenleftig
βk
z,m+ηνβk+1
x,m−η√
Lβ(√
Lw+√
Lβ)∇βfm(wk+1
x,βk+1
x,m)/parenrightig
wk+1
z=1
1+ην/parenleftbig
wk
z+ηνwk+1
x/parenrightbig
end if
end for
Theorem 6. LetFsatisfy Assumptions 1 and 2. Then, any algorithm from the class Are-
quires at least Ω(/radicalbig
Lw/µlogϵ−1)communication rounds, Ω/parenleftig
n+/radicalbig
nLw/µlogϵ−1/parenrightig
calls to∇w-oracle and
Ω/parenleftig
n+/radicalbig
nLβ/µlogϵ−1/parenrightig
calls to∇β-oracle to reach the ϵ-solution.
The proof is given in Appendix B.8. In the special case where n= 1, Theorem 6 provides a lower complexity
bound for solving (1) with access to the full gradient locally. Specifically, it shows both the communication
complexity and local gradient complexity with respect to w-variables of the order Ω/parenleftig/radicalig
Lw
µlog1
ϵ/parenrightig
, and the
local gradient complexity with respect to β-variables of the order Ω/parenleftig/radicalig
Lβ
µlog1
ϵ/parenrightig
.
4.2 Accelerated Coordinate Descent for PFL
We apply Accelerated Block Coordinate Descent (ACD) (Allen-Zhu et al., 2016; Nesterov & Stich, 2017;
Hanzely & Richtárik, 2019) to solve (1). We separate the domain into two blocks of coordinates to sample
from: the first one corresponding to wparameters and the second one corresponding to β= [β1,β2,...,βM].
Specifically, at every iteration, we toss an unfair coin. With probability pw=√
Lw/(√
Lw+√
Lβ), we com-
pute∇wF(w,β)and update block w. Alternatively, with probability pβ= 1−pw, we compute∇βF(w,β)
and update block β. Plugging the described sampling of coordinate blocks into ACD, we arrive at Algo-
rithm 2. Note that ACD from Allen-Zhu et al. (2016) only allows for subsampling individual coordinates
and does not allow for “blocks.” A variant of ACD that provides the right convergence guarantees for block
sampling was proposed in Nesterov & Stich (2017) and Hanzely & Richtárik (2019).
We provide an optimization guarantee for Algorithm 2 in the following theorem.
Theorem 7. Suppose that Assumption 1 holds. Let
ν=µ
(√
Lw+√
Lβ)2, θ=√
ν2+ 4ν−ν
2,andη=θ−1.
15Published in Transactions on Machine Learning Research (May/2023)
The iteration complexity of ACD-PFL is
O/parenleftbigg/radicalig
(Lw+Lβ)/µlogϵ−1/parenrightbigg
.
The proof follows directly from Theorem 4.2 of Hanzely & Richtárik (2019). Since ∇wF(w,β)is evaluated
on average once every 1/pwiterations, ACD-PFL requires O/parenleftig/radicalbig
Lw/µlogϵ−1/parenrightig
communication rounds and
O/parenleftig/radicalbig
Lw/µlogϵ−1/parenrightig
gradient evaluations with respect to w, thus matching the lower bound. Similarly, as
∇βF(w,β)is evaluated on average once every 1/pβiterations, we require O/parenleftig/radicalbig
Lβ/µlogϵ−1/parenrightig
evaluations
of∇βF(w,β)to reach an ϵ-solution; again matching the lower bound. Consequently, ACD-PFL is minimax
optimal in terms of all three quantities of interest simultaneously.
We are not the first to propose a variant of coordinate descent (Nesterov, 2012) for personalized FL. Wu et al.
(2021) introduced block coordinate descent to solve a variant of (11) formulated over a network. However,
they do not argue about any form of optimality for their approach, which is also less general as it only covers
a single personalized FL objective.
4.3 Accelerated SVRCD for PFL
Despite being minimax optimal, the main drawback of ACD-PFL is the necessity of having access to the
full gradient of local loss fmwith respect to either worβat each iteration. Specifically, computing the full
gradient with respect to fmmight be very expensive when fmis a finite sum (3). Ideally, one would desire
an algorithm that is i) subsampling the global/local variables wandβjust as ACD-PFL, ii) subsampling the
localfinitesum, iii)employingcontrolvariatestoreducethevarianceofthelocalstochasticgradient(Johnson
& Zhang, 2013; Defazio et al., 2014), and iv) accelerated in the sense of Nesterov (1983).
We propose a method – ASVRCD-PFL – that satisfies all four conditions above, by carefully designing
an instance of ASVRCD (Accelerated proximal Stochastic Variance Reduced Coordinate Descent) (Hanzely
et al., 2020b) applied to minimize an objective in a lifted space that is equivalent to (1). We are not aware
of any other algorithm capable of satisfying i)-iv) simultaneously.
The construction of ASVRCD-PFL involves four main ingredients. First, we rewrite the original problem
in a lifted space, which corresponds to the problem form discussed in Hanzely et al. (2020b). Second, we
construct an unbiased stochastic gradient estimator by sampling coordinate blocks. Next, we enrich the
stochastic gradient by control variates as in SVRG. Finally, we incorporate Nesterov’s momentum. We
explain the construction of ASVRCD-PFL in detail below.
Lifting the problem space. ASVRCD-PFL is an instance of ASVRCD applied to an objective (1) in a
lifted space. We have that
min
w∈Rd
0,βm∈Rdm,∀m∈[M]F(w,β) = min
X[1,:,:]∈RM×n×d0
X[2,m,:]∈Rn×dm,∀m∈M{P(X):=F(X) +ψ(X)},
where
F(X):=1
MM/summationdisplay
m=1
1
nn/summationdisplay
j=1fm,j(X[1,m,j ],X[2,m,j ])

and
ψ(X):=

0ifm,m′∈[M],j,j′∈[n] :X[1,m,j ] =X[1,m′,j′], X[2,m,j ] =X[2,m,j′]
∞otherwise.
VariablesX[1,m,j ]correspond to wfor allm∈[M]andj∈[n], while variables X[2,m,j ]correspond to βm
for allj∈[n]. The equivalence between the objective (1) and the objective in the lifted space is ensured with
16Published in Transactions on Machine Learning Research (May/2023)
the indicator function ψ(X), which forces different Xvariables to take the same values. We apply ASVRCD
with a carefully chosen non-uniform sampling of coordinate blocks to minimize P(X).
Sampling of coordinate blocks. The key component of ASVRCD-PFL is the construction of the unbiased
stochastic gradient estimator of ∇F(X), which we describe here. We consider two independent sources of
randomness when sampling the coordinate blocks.
First, we toss an unfair coin ζ. With probability pw, we have ζ= 1. In such a case, we ignore the
local variables and update the global variables only, corresponding to worX[1]in our current notation.
Alternatively, ζ= 2with probability pβ:= 1−pw. In such a case, we ignore the global variables and update
local variables only, corresponding to βorX[2]in our current notation.
Second, we consider local subsampling. At each iteration, the stochastic gradient is constructed using ∇Fj
only, where Fj(w,β):=1
M/summationtextM
m=1fm,j(w,βm)andjis selected uniformly at random from [n]. For the sake of
simplicity, we assume that all clients sample the same index, i.e., the randomness is synchronized. A similar
rate can be obtained without shared randomness.
With these sources of randomness, we arrive at the following construction of G(X), which is an unbiased
stochastic estimator of ∇F(X):
G(X)[1,m,j′] =

1
pw∇wfj′,m(X[1,m,j′],X[2,m,j′])ifζ= 1andj′=j;
0∈Rd0 otherwise ;
G(X)[2,m,j′] =

1
pβ∇βfj′,m(X[1,m,j′],X[2,m,j′])ifζ= 2andj′=j;
0∈Rdm otherwise.
Control variates and acceleration. We enrich the stochastic gradient by incorporating control variates,
resulting in an SVRG-style stochastic gradient estimator. In particular, the resulting stochastic gradient will
take the form of G(X)−G(Y) +∇F(Y), whereYis another point that is updated upon a successful toss
of aρ-coin. The last ingredient of the method is to incorporate Nesterov’s momentum.
Combining the above ingredients, we arrive at the ASVRCD-PFL procedure, which is detailed in Algorithm 3
in the lifted notation. Algorithm 4 details ASVRCD-PFL in the notation consistent with the rest of the
paper.
The following theorem provides convergence guarantees for ASVRCD-PFL.
Theorem 8. Suppose Assumptions 1 and 2 hold. The iteration complexity of ASVRCD-PFL with
η=1
4L, θ 2=1
2, γ =1
max{2µ,4θ1/η},
ν= 1−γµ, θ 1= min/braceleftigg
1
2,/radicaligg
ηµmax/braceleftbigg1
2,θ2
ρ/bracerightbigg/bracerightigg
,andpw=Lw
Lβ+Lw
is
O/parenleftbigg/parenleftbigg
ρ−1+/radicalig
(Lw+Lβ)/(ρµ)/parenrightbigg
logϵ−1/parenrightbigg
,
whereρis the frequency of updating the control variates.
The communication complexity and the local stochastic gradient complexity with respect to w-parameters
of orderO/parenleftig/parenleftig
n+/radicalbig
nLw/µ/parenrightig
logϵ−1/parenrightig
, is obtained by setting ρ=Lw//parenleftbig
(Lw+Lβ)n/parenrightbig
. Analogously, setting
ρ=Lβ/((Lw+Lβ)n)yields the local stochastic gradient complexity with respect to β-parameters of order
O/parenleftig/parenleftig
n+/radicalbig
nLβ/µ/parenrightig
logϵ−1/parenrightig
. In contrast with Theorem 6, this result shows that ASVRCD-PFL can be
optimal in terms of the local computation either with respect to β-variables or in terms of the w-variables.
Unfortunately, these bounds are not achieved simultaneously unless Lw,Lβare of a similar order, which we
leave for future research. The proof is given in Appendix B.9. Additional discussion on how to choose the
tuning parameters is given in Theorem 9.
17Published in Transactions on Machine Learning Research (May/2023)
Algorithm 3 ASVRCD-PFL (lifted notation)
input 0<θ1,θ2<1,η,ν,γ > 0,ρ∈(0,1),Y0=Z0=X0.
fork= 0,1,2,...do
Xk=θ1Zk+θ2Vk+ (1−θ1−θ2)Yk
gk=G(Xk)−G(Vk) +∇F(Vk)
Yk+1= proxηψ(Xk−ηgk)
Zk+1=νZk+ (1−ν)Xk+γ
η(Yk+1−Yk)
Vk+1=

Yk,with probability ρ
Vk,with probability 1−ρ
end for
5 Simulations
We present an extensive numerical evaluation to verify and support the theoretical claims. We perform
experiments on both synthetic and real data, with a range of different objectives and methods (both ours
and the baselines from the literature). The experiments are designed to shed light on various aspects of the
theory. In this section, we present the results on synthetic data, while in the next section, we illustrate the
performance of different methods on real data. The code to reproduce the experiments is publicly available
at
https://github.com/boxinz17/PFL-Unified-Framework .
The experiments on synthetic data were conducted on a personal laptop with a CPU (Intel(R) Core(TM)
i7-9750H CPU@2.60GHz). The results are summarized over 30 independent runs.
5.1 Multi-Task Personalized FL and Implicit MAML Objective
In this section, we focus on the performance of different methods when solving the objective (11). We
implement three proposed algorithms – LSGD-PFL, ASCD-PFL3, and ASVRCD-PFL – and compare them
with two baselines – L2SGD+ (Hanzely & Richtárik, 2020) and pFedMe (Dinh et al., 2020). As both
L2SGD+ and pFedMe were designed specifically to solve (11), the aim of this experiment is to demonstrate
that our universal approach is competitive with these specifically designed methods.
Data and model. We perform this experiment on synthetically generated data which allows us to properly
control the data heterogeneity level. As a model, we choose logistic regression. We generate w∗∈Rdwith
i.i.d. entries from Uniform [0.49,0.51], and setβ∗
m=w∗+ ∆β∗
m∈Rd, where entries of ∆β∗
mare generated
i.i.d. from Uniform [µm−0.01,µm+ 0.01]andµm∼N(0,σ2
h)for allm= 1,2,...,M. Thus,σhcan be
regarded as a measure of heterogeneity level, with a large σhcorresponding to large heterogeneity. Finally,
for each device m= 1,2,...,M, we generate xm,i∈Rdwith entries i.i.d. from Uniform [0.2,0.5]for all
i= 1,2,...,n, andym,i∼Bernoulli (pm,i), wherepm,i= 1/(1 + exp(β∗⊤
mxm,i)). We setd= 15,n= 1000,
M= 20, and letσh∈{0.1,0.3,1.0}to explore different levels of heterogeneity.
Objective function. We use objective (11) with f′
m(βm)being the cross-entropy loss function. We set
λ=σh·10−2, so that larger heterogeneity level will induce a larger penalty, which will further encourage
parameters on each device to be closer to their geometric center. In addition to the training loss, we also
record the estimation error in the training process, defined as ∥ˆw−w∗∥2+/summationtextM
m=1∥ˆβm−β∗
m∥2.
Tuning parameters of proposed algorithms. For LSGD-PFL (Algorithm 1), we set the batch size to
compute the stochastic gradient B= 1, the average period τ= 5, and the learning rate η= 0.01. Forpw
andpβin ASCD-PFL (Algorithm 5) and ASVRCD-PFL (Algorithm 4), we set them as pw=Lw/(Lβ+Lw)
andpβ= 1−pw, whereLw=λ/MandLβ= (L′+λ)/M. We setL′= max 1≤m≤M,1≤i≤n∥xm,i∥2/4. For
η,θ2,γ,νandθ1in ASVRCD-PFL, we set them according to Theorem 9, where L= 2 max{Lw/pw,Lβ/pβ},
ρ=pw/n, andµ=µ′/(3M). We letµ′be the smallest eigenvalue of1
nM/summationtextM
m=1/summationtextn
i=1exp(β∗⊤
mxm,i)/(1 +
3ASCD-PFL is ASVRCD-PFL without control variates. See the detailed description in Appendix A.
18Published in Transactions on Machine Learning Research (May/2023)
Algorithm 4 ASVRCD-PFL
input 0<θ1,θ2<1,η,ν,γ > 0,ρ∈(0,1),pw∈(0,1),pβ= 1−pw,w0
y=w0
z=w0
v∈Rd0,β0
y,m=β0
z,m=
β0
v,m∈Rdmfor1≤m≤M.
fork= 0,1,2,...do
wk
x=θ1wk
z+θ2wk
v+ (1−θ1−θ2)wk
y
form= 1,...,Min parallel do
βk
x,m=θ1βk
z,m+θ2βk
v,m+ (1−θ1−θ2)βk
y,m
end for
Sample random j∈{1,2,...,n}andζ=

1,with probability pw
2,with probability pβ
gk
w=

1
pw/parenleftigg
1
MM/summationdisplay
m=1∇wfm,j(wk
x,βk
x,m)−1
MM/summationdisplay
m=1∇wfm,j(wk
v,βk
v,m)/parenrightigg
+∇wF(wk
v,βk
v)ifζ= 1
∇wF(wk
v,βk
v) ifζ= 2
wk+1
y=wk
x−ηgk
w
wk+1
z=νwk
z+ (1−ν)wk
x+γ
η(wk+1
y−wk
x)
wk+1
v=

wk
y,with probability ρ
wk
v,with probability 1−ρ
form= 1,...,Min parallel do
gk
β,m=

1
M∇βfm(wk
v,βk
v,m) ifζ= 1
1
pβM/parenleftbig
∇βfm,j(wk
x,βk
x,m)−∇βfm,j(wk
v,βk
v,m)/parenrightbig
+1
M∇βfm(wk
v,βk
v,m)ifζ= 2
βk+1
y,m=βk
x,m−ηgk
β,m
βk+1
z,m=νβk
z,m+ (1−ν)βk
x,m+γ
η(βk+1
y,m−βk
x,m)
βk+1
v,m=

βk
y,m,with probability ρ
βk
v,m,with probability 1−ρ
end for
end for
exp(β∗⊤
mxm,i))2xm,ix⊤
m,i. Theη,ν,γ,ρ in ASCD-PFL are the same as in ASVRCD-PFL, and we let θ=
min{0.8,1/η}. In addition, we initialize all iterates at zero for all algorithms.
Tuning Parameters of pFedMe. For pFedMe (Algorithm 1 in Dinh et al. (2020)), we set all parameters
according to the suggestions in Section 5.2 of Dinh et al. (2020). Specifically, we set the local computation
rounds toR= 20, computation complexity to K= 5, Mini-Batch size to |D|= 5, andη= 0.005. We also
setS=M= 20. To solve the objective (7) in Dinh et al. (2020), we use gradient descent, as suggested in
the paper. Additionally, we initialize all iterates at zero.
Tuning Parameters of L2SGD+. For L2SGD+4, we set the stepsize η(the parameter αin Hanzely &
Richtárik (2020)) and the probability of averaging pwto be the same as in ASRVCD-PFL and ASCD-PFL.
We also initialize all iterates at zero.
4Algorithm 2 in Hanzely & Richtárik (2020)
19Published in Transactions on Machine Learning Research (May/2023)
Figure 1: Comparison of various methods for minimizing (11). Each experiment was repeated 30 times, and
the solid line represents the average performance, while the shaded region indicates the mean ±standard
error. We set the communication period of LSGD-PFL at 5, while other methods synchronize based on the
corresponding theory. The first row shows the training loss, and the second row shows the estimation error.
The different columns correspond to varying heterogeneity levels, which are parameterized by σh. Asσh
increases, the heterogeneity level also increases.
Algorithmσh0.1 0.3 1.0
LSGD 260.98 256.16 237.18
ASCD 28.79 51.71 142.59
ASVRCD 47.94 97.61 274.60
pFedMe 399.33 370.42 370.35
L2SGDplus 25.83 47.59 182.29
Table 4: The average wall-clock running time in seconds over 30 independent runs when solving the objec-
tive (11). Each entry in the table reports the average time for 1,000 communication rounds. We ignore any
additional communication costs that might occur in practice.
Results. The results are summarized in Figure 1. We observe that our general-purpose optimizers are
competitive with L2SGD+ and pFedMe. In particular, both ASVRCD-PFL and L2SGD+ consistently
achieve the same training error as the other methods, which is well predicted by our theory. Although
L2SGD+ is slightly faster in terms of convergence due to the specific parameter setting, it is not as general
as the methods we propose. Furthermore, we note that the widely used LSGD-PFL suffers from data
heterogeneity on different devices, whereas ASVRCD-PFL is not affected by this heterogeneity, as predicted
by our theory. The average running time over 30 independent runs is reported in Table 4.
20Published in Transactions on Machine Learning Research (May/2023)
Figure 2: Comparison of the three proposed algorithms – LSGD-PFL, ASCD-PFL, and ASVRCD-PFL –
when minimizing (16). Each experiment is repeated 30 times; the solid line represents the mean performance,
and the shaded region covers the mean ±standard error. We set the communication period of LSGD-PFL
to5, while other methods synchronize according to their respective theories. The first row represents the
training loss, and the second row corresponds to the estimation error. Different columns indicate various
heterogeneity levels, parameterized by σh. The heterogeneity level increases with σh.
5.2 Explicit Weight Sharing Objective
In this section, we present another experiment on synthetic data that aims to optimize the explicit weight
sharing objective (16). Since there is no good baseline algorithm for this objective, the purpose of this
experiment is to compare the three proposed algorithms: LSGD-PFL, ASCD-PFL, and ASVRCD-PFL.
Data and Model. As a model, we choose logistic regression. We generate w∗∈Rdgwith i.i.d. entries
fromN(0,1), andβ∗
m∈Rdlwith i.i.d. entries from Uniform [µm−0.01,µm+ 0.01], whereµm∼N(0,σ2
h)
for allm= 1,2,...,M. Thus,σhcan be regarded as a measure of the heterogeneity level, with a large
σhcorresponding to a high degree of heterogeneity. Finally, for each device m= 1,2,...,M, we generate
xm,i∈Rdg+dlwith entries i.i.d. from Uniform [0.0,0.1]for alli= 1,2,...,n, andym,i∼Bernoulli (pm,i),
wherepm,i= 1/(1 + exp((w∗⊤,β∗⊤
m)xm,i)). We setdg= 10,dl= 5,n= 1000,M= 20, and letσh∈
{5.0,10.0,15.0}to explore different levels of heterogeneity.
Objective function. We use objective (11) with f′
m(βm)representing the cross-entropy loss function. We
setλ=σh·10−2, so smaller heterogeneity levels will induce a larger penalty, encouraging parameters on each
device to be closer to their geometric center. In addition to the training loss, we also record the estimation
error during the training process, defined as ∥ˆw−w∗∥2+/summationtextM
m=1∥ˆβm−β∗
m∥2.
Results. TheresultsaresummarizedinFigure2. Whenexaminingthetrainingloss, weobservethatASCD-
PFL drives the loss down quickly initially, while ASVRCD-PFL ultimately achieves a better optimum. This
suggests that we can apply ASCD-PFL at the beginning of training and add control variates to reduce the
variance at a later stage of training, thus combining the benefits of both algorithms. Both ASCD-PFL
and ASVRCD-PFL perform much better than the widely used LSGD-PFL. When analyzing the estimation
21Published in Transactions on Machine Learning Research (May/2023)
Algorithmsσh5.0 10.0 15.0
LSGD 238.01 237.67 234.33
ASCD 17.00 16.55 16.46
ASVRCD 23.35 22.12 23.51
Table 5: The average wall-clock running time in seconds over 30 independent runs when solving the objec-
tive (16) is presented. Each entry of the table reports the average time for 1,000 communication rounds. We
ignore any additional communication costs that might occur in practice.
error, we observe that when the heterogeneity level is small, there is a tendency for overfitting, especially
for ASCD-PFL; and when the heterogeneity level increases, there is less concern for overfitting. In general,
however, ASCD-PFL and ASVRCD-PFL still achieve better estimation error than LSGD-PFL. The average
running time over 30 independent runs is reported in Table 5.
6 Real Data Experiment Results
In this section, we use real data to illustrate the performance and various properties of the proposed methods.
InSection6.1, wecomparetheperformanceof the threeproposed algorithms. InSection6.2, weillustratethe
effect of communication frequency of global parameters for ASCD-PFL and demonstrate that the theoretical
choice based on Theorem 9 can generate the best communication complexity. Finally, in Section 6.3, we
show the effect of reparametrizing wfor ASCD-PFL.
6.1 Performance of the Proposed Methods on Real Data
We compare the three proposed algorithms – LSGD-PFL, ASCD-PFL (ASVRCD-PFL without control
variates), and ASVRCD-PFL – across four image classification datasets – MNIST (Deng, 2012), KMIN-
IST (Clanuwat et al., 2018), FMINST (Xiao et al., 2017), and CIFAR-10 (Krizhevsky, 2009) with three
objective functions (8), (11), and (14). As a model, we use a multiclass logistic regression, which is a single-
layer fully connected neural network combined with a softmax function and cross-entropy loss. Experiments
were conducted on a personal laptop (Intel(R) Core(TM) i7-9750H CPU@2.60GHz) with a GPU (NVIDIA
GeForce RTX 2070 with Max-Q Design).
Data preparation. We set the number of devices M= 20. We focus on a non-i.i.d. setting of McMahan
et al. (2017) and Liang et al. (2020) by assigning Kclasses out of ten to each device. We let K= 2,4,8
to generate different levels of heterogeneity. A larger Kmeans a more balanced data distribution and thus
smaller data heterogeneity. We then randomly select n= 100samples for each device based on its class
assignment for training and n′= 300samples for testing. We normalize each dataset in two steps: first, we
normalize the columns (features) to have mean zero and unit variance; next, we normalize the rows (samples)
so that every input vector has a unit norm.
Model. Given a grayscale picture with a label y∈{1,2,...,C}, we unroll its pixel matrix into a vector
x∈Rp. Then, given a parameter matrix Θ∈Rp×C, the function f′
m(·)in (8), (11), and (14) is defined as
f′
m(Θ):=lCE(ς(Θx) ;y),
whereς(·) :RK→RKis the softmax function and lCE(·)is the cross-entropy loss function. In this setting,
the function f′
m(·)is convex.
22Published in Transactions on Machine Learning Research (May/2023)
(a) Training Loss
(b) Validation Accuracy
Figure 3: The real data experiment results for K= 2. Different rows correspond to different objective
functions, and columns correspond to different datasets.
23Published in Transactions on Machine Learning Research (May/2023)
(a) Training Loss
(b) Validation Accuracy
Figure 4: The real data experiment results for K= 4. Different rows orrespond to different objective
functions, and columns correspond to different datasets.
24Published in Transactions on Machine Learning Research (May/2023)
(a) Training Loss
(b) Validation Accuracy
Figure 5: The real data experiment results for K= 8. Different rows correspond to different objective
functions, and columns correspond to different datasets.
25Published in Transactions on Machine Learning Research (May/2023)
Personalized FL objectives. We consider three different objectives:
1. the multitask FL objective (8) with Λ = 1.0andλ= 1.0/K;
2. the mixture FL objective (11), with λ= 1.0/K; and
3. the adaptive personalized FL objective (14), with Λ = 1.0andαm= 0.05×Kfor allm∈[M],
whereKis the number of labels for each device. When computing the testing accuracy, we only use the local
parameters. Note that the choices of hyperparameters in the chosen objectives are purely heuristic. Our
purpose is to demonstrate the convergence properties of the proposed algorithms on the training loss. Thus,
it is possible that a smaller training loss does not necessarily imply better testing accuracy (or generalization
ability). How to choose hyperparameters optimally is not the focus of this paper and requires further
research.
Tuning parameters of proposed algorithms. For LSGD-PFL (Algorithm 1), we set the batch size to
computethestochasticgradient B= 1andsettheaverageperiod τ= 5. ForpwinASCD-PFL(Algorithm6)
and ASVRCD-PFL (Algorithm 7), we set it as pw=Lw/(Lβ+Lw). For the objective FMT2in (8), we set
Lw= (ΛL′+λ)/MandLβ=L′+λ; for the objective FMX2in (11), we setLw=λ/MandLβ=L′+λ; for
the objective FAPFL 2in (14), we setLw= (Λ+max 1≤m≤Mα2
m)L′/MandLβ= (1−max 1≤m≤Mαm)2L′/M.
We setL′= 1.0for all objectives. We set ρ=pw/nfor ASCD-PFL and ASVRCD-PFL. For η,θ2,γ,νand
θ1in ASVRCD-PFL, we set them according to Theorem 9, where L= 2 max{Lw/pw,Lβ/pβ},ρ=pw/n,
andµ=µ′/(3M). We letµ′= 0.01. Since the dimension of the iterates is larger than the sample size,
the objective is weakly convex and, thus, µ′= 0. Therefore, our choice of µ′is aimed at improving the
numerical behavior of algorithms. The η,ν,γ,ρ in ASCD-PFL are the same as in ASVRCD-PFL, and we
letθ= min{0.8,1/η}. In addition, we initialize all iterates at zero for all algorithms.
Results. The results are summarized in Figure 3, Figure 4, and Figure 5 for K= 2,4,8respectively.
We observe that ASCD-PFL outperforms the widely-used LSGD-PFL. We also observe that ASVRCD-PFL
converges slowly when minimizing the training loss. As we are working in the over parametrization regimes,
which makes µ′= 0, the assumptions of our theory are violated. As a result, it is more advisable to use
ASCD-PFL during the initial phase of training and use ASVRCD-PFL when the iterates get closer to the
optimum.
6.2 Subsampling of the Global and Local Parameters
We demonstrate that the choice of pwbased on Theorem 9, specifically setting pw=Lw/(Lw+Lβ), results in
the best communication complexity of ASCD-PFL. More precisely, based on Theorem 9, we set the learning
rateη= 1/(4L), whereL:= 2 max/braceleftbig
Lw/pw,Lβ/pβ/bracerightbig
. The expressions of LwandLβforFMT2,FMX2, and
FAPFL 2are stated in Lemma 1, Lemma 2, Lemma 3, and also restated in the previous section, where L′is1
after normalization. We set ρ=pw/n. We compare the performance of ASCD-PFL using the theoretically
suggestedpwwith other choices of pw∈{0.1,0.3,0.5,0.7,0.9}. We fix those parameters that are independent
ofpw. See more details about the choice of tuning parameters in the previous section.
We plot the loss against the number of communication rounds, which illustrates the communication com-
plexity. The number of classes for each device is K= 2. The results are summarized in Figure 6, which
also includes the test accuracy. We observe that choosing pwbased on theoretical considerations leads to
the best communication complexity.
6.3 Effect of Reparametrization in ASCD-PFL.
We demonstrate the importance of reparametrization of the global parameter wby rescaling wby a factor
ofM−1
2. We run reparameterized and non-reparameterized ASCD-PFL across different objectives and
datasets. We set the number of classes for each device K= 2. The results are summarized in Figure 7. For
the training loss, we observe that reparametrization improves the convergence of ASCD-PFL, except for the
APFL2 objective (14), where the non-reparametrized variant performs slightly better in three of the four
datasets. On the other hand, when considering the testing accuracy, reparametrization always helps improve
the results, indicating that reparametrization can help prevent overfitting. Based on the experiment, we
26Published in Transactions on Machine Learning Research (May/2023)
(a) Training Loss
(b) Validation Accuracy
Figure 6: Communication complexity for different choices of pw. The theoretical choice based on Theorem 9
can provide the best communication complexity. Specifically, the theoretical choice of pwresults inpw= 0.5
forFMT2,pw= 0.25forFMX2, andpw= 0.55forFAPFL 2. Different rows correspond to different objective
functions, and columns correspond to different datasets.
27Published in Transactions on Machine Learning Research (May/2023)
(a) Training Loss
(b) Validation Accuracy
Figure 7: Effect of reparametrization of global space in ASCD-PFL. Reparametrization generally helps
achieve faster convergence in minimizing training loss and consistently improves testing accuracy. Different
rows correspond to different objective functions, and columns correspond to different datasets.
28Published in Transactions on Machine Learning Research (May/2023)
Figure 8: Empirical exploration of the effect of Λon the performance of the objective in (7).
suggest always using reparametrization to ensure the scale of the learning rate is appropriate for both global
and local parameters.
6.4 Potential Benefits of Extended Objectives
We empirically justify the potential benefits of our extended objectives. More specifically, we vary the
relaxation parameter Λin (7) to show the change in performance. Since the multitask objective proposed
by Li et al. (2020) is equivalent to setting Λ→∞, our main interest is to explore whether a larger Λalways
implies better performance. We set K= 4,n= 30,n′= 100,M= 20, andλ= 0.1. The remaining settings
are the same as in Section 6.1. We vary Λover the values{1.0,10.0,100.0}, and the resulting performance
is shown in Figure 8. The plot shows that the performance slightly improves as Λincreases from 1.0to
10.0, but then drops when Λ = 100.0. This result suggests that by selecting an appropriate Λ, it is possible
to achieve better empirical performance. Furthermore, although proposing new personalized FL objectives
is not the main focus of this paper, the above empirical result suggests the potential benefits of a general
framework.
7 Conclusions and Directions for Future Research
We proposed a general convex optimization theory for personalized FL. While our work answers a range of
important questions, there are many directions in which our work can be extended in the future, such as
partial participation, minimax optimal rates for specific personalized FL objectives, brand new personalized
FL objectives, and non-convex theory.
Partial participation and client sampling. An essential aspect of FL that is not covered in this work is
the partial participation or client sampling when one has access to only a subset of devices at each iteration.
While we did not cover partial participation and focused on answering orthogonal questions, we believe that
partial participation should be considered when extending our results in the future. Typically, when one
chooses clients uniformly, the theorems in this paper should be extended easily; however, a more interesting
question is how to sample clients with a non-uniform distribution to speed up the convergence. We leave
this problem for future study.
Minimax optimal rates for specific personalized FL objectives. As outlined in Section 1.2, one
cannot hope for the general optimization framework to be minimax optimal in every single special case.
Consequently, there is still a need to keep exploring the optimization aspects of individual personalized FL
29Published in Transactions on Machine Learning Research (May/2023)
objectives as one might come up with a more efficient optimizer that exploits the specific structure not
covered by Assumption 1 or Assumption 2.
Brand new personalized FL objectives. While in this work we propose a couple of novel personalized
FL objectives obtained as an extension of known objectives, we believe that seeing personalized FL as an
instance of (1) might lead to the development of brand new approaches for personalized FL.
Non-convex theory. In this work, we have focused on a general convex optimization theory for person-
alized FL. Our convex rates are meaningful – they are minimax optimal and correspond to the empirical
convergence. However, an inherent drawback of such an approach is the inability to cover non-convex FL
approaches, such as MAML (see Section 2.8), or non-convex FL models. We believe that obtaining minimax
optimal rates in the non-convex world would be very valuable.
30Published in Transactions on Machine Learning Research (May/2023)
References
A. Agarwal, J. Langford, and C.-Y. Wei. Federated residual learning. arXiv preprint arXiv:2003.12880 ,
2020.
Z.Allen-Zhu, Z.Qu, P.Richtárik, andY.Yuan. Evenfasteracceleratedcoordinatedescentusingnon-uniform
sampling. In International Conference on Machine Learning , 2016.
M. G. Arivazhagan, V. Aggarwal, A. K. Singh, and S. Choudhary. Federated learning with personalization
layers.arXiv preprint arXiv:1912.00818 , 2019.
F. Chen, M. Luo, Z. Dong, Z. Li, and X. He. Federated meta-learning with fast convergence and efficient
communication. arXiv preprint arXiv:1802.07876 , 2018.
T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha. Deep learning for classical
japanese literature. arXiv preprint arXiv:1812.01718 , 2018.
J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. W. Senior, P. A.
Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In Advances in Neural Information
Processing Systems , 2012.
A. Defazio, F. R. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems , 2014.
L. Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine , 29(6):141–142, 2012.
Y. Deng, M. M. Kamani, and M. Mahdavi. Adaptive personalized federated learning. arXiv preprint
arXiv:2003.13461 , 2020.
C. T. Dinh, N. H. Tran, and T. D. Nguyen. Personalized federated learning with moreau envelopes. In
Advances in Neural Information Processing Systems , 2020.
A. Fallah, A. Mokhtari, and A. Ozdaglar. Personalized federated learning: A meta-learning approach. arXiv
preprint arXiv:2002.07948 , 2020.
C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In
International Conference on Machine Learning , 2017.
E. Gorbunov, F. Hanzely, and P. Richtárik. Local SGD: unified theory and new efficient methods. In
International Conference on Artificial Intelligence and Statistics , 2021.
F. Haddadpour and M. Mahdavi. On the convergence of local descent methods in federated learning. arXiv
preprint arXiv:1910.14425 , 2019.
F. Hanzely and P. Richtárik. Accelerated coordinate descent with arbitrary sampling and best rates for
minibatches. In International Conference on Artificial Intelligence and Statistics , 2019.
F. Hanzely and P. Richtárik. Federated learning of a mixture of global and local models. arXiv preprint
arXiv:2002.05516 , 2020.
F. Hanzely, S. Hanzely, S. Horváth, and P. Richtárik. Lower bounds and optimal algorithms for personalized
federated learning. In Advances in Neural Information Processing Systems , 2020a.
F. Hanzely, D. Kovalev, and P. Richtárik. Variance reduced coordinate descent with acceleration: New
method with a surprising application to finite-sum problems. In International Conference on Machine
Learning , 2020b.
A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon, and
D. Ramage. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604 , 2018.
31Published in Transactions on Machine Learning Research (May/2023)
H. Hendrikx, F. Bach, and L. Massoulie. An optimal algorithm for decentralized finite-sum optimization.
SIAM Journal on Optimization , 31(4):2753–2783, 2021.
Y. Jiang, J. Konečný, K. Rush, and S. Kannan. Improving federated learning personalization via model
agnostic meta learning. arXiv preprint arXiv:1909.12488 , 2019.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In
Advances in Neural Information Processing Systems , 2013.
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles,
G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. Foundations and
Trends ®in Machine Learning , 14(1–2):1–210, 2021.
M. Khodak, M. Balcan, and A. Talwalkar. Adaptive gradient-based meta-learning methods. In Advances in
Neural Information Processing Systems , 2019.
A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto,
2009.
V. Kulkarni, M. Kulkarni, and A. Pant. Survey of personalization techniques for federated learning. In
2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4) , pp.
794–797, 2020.
G. Lan and Y. Zhou. An optimal randomized incremental gradient method. Mathematical programming ,
171(1-2):167–215, 2018.
T. Li, S. Hu, A. Beirami, and V. Smith. Federated multi-task learning for competing constraints. arXiv
preprint arXiv:2012.04221 , 2020.
P. P. Liang, T. Liu, L. Ziyin, R. Salakhutdinov, and L.-P. Morency. Think locally, act globally: Federated
learning with local and global representations. arXiv preprint arXiv:2001.01523 , 2020.
S. Lin, G. Yang, and J. Zhang. A collaborative learning framework via federated meta-learning. In Interna-
tional Conference on Distributed Computing Systems , 2020.
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of
deepnetworksfromdecentralizeddata. In International Conference on Artificial Intelligence and Statistics ,
2017.
H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas. Federated learning of deep networks using model
averaging. arXiv preprint arXiv:1602.05629 , 2016.
A. S. Nemirovskij and D. B. Yudin. Problem complexity and method efficiency in optimization . Wiley-
Interscience, 1983.
Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal
on Optimization , 22(2):341–362, 2012.
Y. Nesterov. A method for solving the convex programming problem with convergence rate o(1/k2). In
Doklady Akademii Nauk , volume 269, pp. 543–547, 1983.
Y. Nesterov. Lectures on Convex Optimization . Springer, 2018.
Y. Nesterov and S. U. Stich. Efficiency of the accelerated coordinate descent method on structured opti-
mization problems. SIAM Journal on Optimization , 27(1):110–123, 2017.
A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine. Meta-learning with implicit gradients. In Advances
in Neural Information Processing Systems , 2019.
K. Scaman, F. R. Bach, S. Bubeck, L. Massoulié, and Y. T. Lee. Optimal algorithms for non-smooth
distributed optimization in networks. In Advances in Neural Information Processing Systems , 2018.
32Published in Transactions on Machine Learning Research (May/2023)
V. Smith, C. Chiang, M. Sanjabi, and A. Talwalkar. Federated multi-task learning. In Advances in Neural
Information Processing Systems , 2017.
S. U. Stich. Local SGD converges fast and communicates little. In International Conference on Learning
Representations , 2019.
W. Wang, J. Wang, M. Kolar, and N. Srebro. Distributed stochastic multi-task learning with graph regu-
larization. arXiv preprint arXiv:1802.03830 , 2018.
B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives. In Advances
in Neural Information Processing Systems , 2016.
B. E. Woodworth, J. Wang, A. D. Smith, B. McMahan, and N. Srebro. Graph oracle models, lower bounds,
and gaps for parallel stochastic optimization. In Advances in Neural Information Processing Systems ,
2018.
R. Wu, A. Scaglione, H. Wai, N. Karakoç, K. Hreinsson, and W. Ma. Federated block coordinate descent
scheme for learning global and personalized models. In AAAI Conference on Artificial Intelligence , 2021.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning
algorithms. arXiv preprint arXiv:1708.07747 , 2017.
33Published in Transactions on Machine Learning Research (May/2023)
A Additional Algorithms Used in Simulations
In this section, we detail algorithms that are used in Section 5. We detail ASCD-PFL in Algorithm 5.
ASCD-PFL is a simplified version of ASVRCD-PFL that does not incorporate control variates. SCD-PFL is
detailed in Algorithm 6. SCD-PFL is a simplified version of ASCD-PFL that does not incorporate the control
variates or Nesterov’s acceleration. SVRCD-PFL is detailed in Algorithm 7. SVRCD-PFL is a simplified
version of ASVRCD-PFL that does not incorporate Nesterov’s acceleration.
Algorithm 5 ASCD-PFL
input 0< θ < 1,η,ν,γ > 0,ρ∈(0,1),pw∈(0,1),pβ= 1−pw,w0
y=w0
z∈Rd0,β0
y,m=β0
z,m∈Rdmfor
1≤m≤M.
fork= 0,1,2,...do
wk
x=θwk
z+ (1−θ)wk
y
form= 1,...,Min parallel do
βk
x,m=θβk
z,m+ (1−θ)βk
y,m
end for
Sample random j∈{1,2,...,n}andζ=

1with probability pw
2with probability pβ
ifζ= 1then
gk
w=1
pwM/summationtextM
m=1∇wfm,j(wk
x,βk
x,m)
wk+1
y=wk
x−ηgk
w
wk+1
z=νwk
z+ (1−ν)wk
x+γ
η(wk+1
y−wk
x)
wk+1
v=

wk
y,with probability ρ
wk
v,with probability 1−ρ
else
form= 1,...,Min parallel do
gk
β,m=1
pβM∇βfm,j(wk
x,βk
x,m)
βk+1
y,m=βk
x,m−ηgk
β,m
βk+1
z,m=νβk
z,m+ (1−ν)βk
x,m+γ
η(βk+1
y,m−βk
x,m)
βk+1
v,m=

βk
y,m,with probability ρ
βk
v,m,with probability 1−ρ
end for
end if
end for
34Published in Transactions on Machine Learning Research (May/2023)
Algorithm 6 SCD-PFL
inputη>0,pw∈(0,1),pβ= 1−pw,w0∈Rd,β0
m∈Rdfor1≤m≤M.
fork= 0,1,2,...K−1do
Sample random jm∈{1,2,...,nm}for1≤m≤Mandζ=

1with probability pw
2with probability pβ
gk
w=

1
pwM/summationtextM
m=1∇wfm,jm(wk,βk
m)ifζ= 1
0 ifζ= 2
wk+1=wk−ηgk
w
form= 1,...,Min parallel do
gk
β,m=

0 ifζ= 1
1
pβM∇βfm,jm(wk,βk
m)ifζ= 2
βk+1
m=βk
m−ηgk
β,m
end for
end for
outputwK,βK
mfor1≤m≤M.
Algorithm 7 SVRCD-PFL
inputη>0,pw∈(0,1),pβ= 1−pw,ρ∈(0,1),w0
y=w0
v∈Rd,β0
y,m=β0
v,m∈Rdfor1≤m≤M.
fork= 0,1,2,...K−1do
Sample random jm∈{1,2,...,nm}for1≤m≤Mandζ=

1with probability pw
2with probability pβ
gk
w=

1
pwM/summationtextM
m=1∇wfm,jm(wk
y,βk
y,m) +∇wF(wk
v,βk
v)ifζ= 1
∇wF(wk
v,βk
v) ifζ= 2
wk+1
y=wk
y−ηgk
w
wk+1
v=

wk
y,with probability ρ
wk
v,with probability 1−ρ
form= 1,...,Min parallel do
gk
β,m=

1
M∇βfm(wk
v,βk
v,m) ifζ= 1
1
pβM∇βfm,jm(wk
y,βk
y,m) +1
M∇βfm(wk
v,βk
v,m)ifζ= 2
βk+1
y,m=βk
y,m−ηgk
β,m
βk+1
v,m=

βk
y,m,with probability ρ
βk
v,m,with probability 1−ρ
end for
end for
outputwK
y,βK
y,mfor1≤m≤M.
35Published in Transactions on Machine Learning Research (May/2023)
B Technical Proofs
Throughout this section, we use Id′to denote the d′×d′identity matrix, 0d′
1×d′
2to denote the d′
1×d′
2zero
matrix, and 1′
d∈Rd′to denote the vector of ones.
B.1 Proof of Lemma 1
To show the strong convexity, we shall verify the positive definiteness of
∇2FMT2(w,β)−λ
2MId(M+1)
=
Λ
M∇F′(w) +λ
MId −λ
M3
2(1⊤
M⊗Id)
−λ
M3
2(1M⊗Id)λ
M(Im⊗Id) + Diag(∇2f′
1(β1),...,∇2f′
M(βM))
−λ
2MId(M+1)
⪰
/parenleftig
Λµ′
M+λ
2M/parenrightig
Id−λ
M3
2(1⊤
M⊗Id)
−λ
M3
2(1M⊗Id)/parenleftig
λ
2M+µ′
M/parenrightig
(Im⊗Id)

=1
M
Λµ′+λ
2−λ
M1
21⊤
M
−λ
M1
21M/parenleftbigλ
2+ 2µ′/parenrightbig
Im

/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
:=M⊗Id.
Note that Mcan be written as a sum of Mmatrices, each of them having
Mm=
Λµ′+λ
2
M−λ
M1
2
−λ
M1
2/parenleftbigλ
2+ 2µ′/parenrightbig

as a(m+ 1)×(m+ 1)submatrix and zeros everywhere else. To verify positive semidefiniteness of Mm, we
shall prove that the determinant is positive:
det(Mm) =1
M/parenleftbigg/parenleftbigg
Λµ′+λ
2/parenrightbigg/parenleftbiggλ
2+ 2µ′/parenrightbigg
−λ2/parenrightbigg
≥1
M/parenleftbigg
(2λ)/parenleftbiggλ
2+ 2µ′/parenrightbigg
−λ2/parenrightbigg
≥0
as desired. Verifying the smoothness constants is straightforward.
B.2 Proof of Lemma 2
We have
∇2FMFL 2(w,β)−µ′
3MId(M+1)
=
λ
MId −λ
M3
2(1⊤
M⊗Id)
−λ
M3
2(1M⊗Id)λ
M(Im⊗Id) + Diag(∇2f′
1(β1),...,∇2f′
M(βM))
−µ′
3MId(M+1)
⪰
/parenleftig
λ
M−µ′
3M/parenrightig
Id−λ
M3
2(1⊤
M⊗Id)
−λ
M3
2(1M⊗Id)/parenleftig
λ
M+2µ′
3M/parenrightig
(Im⊗Id)

=1
M
λ−µ′
3−λ
M1
21⊤
M
−λ
M1
21M/parenleftig
λ+2µ′
3/parenrightig
Im

/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
:=M⊗Id.
Note that Mcan be written as a sum of Mmatrices, each of them havingλ
M−µ′
3Mat the position (1,1),
−λ
M1
2at positions (1,m),(m,1)and/parenleftig
λ
M+2µ′
3M/parenrightig
at the position (m,m ). Using the assumption µ′≤λ
2,
36Published in Transactions on Machine Learning Research (May/2023)
it is easy to see that each of these matrices is positive semidefinite, and thus so is M. Consequently,
∇FMFL 2(w,β)−µ′
3MId(M+1)ispositivesemidefiniteandthus FMFL 2isjointlyµ′
3M-stronglyconvex. Verifying
the smoothness constants is straightforward.
B.3 Proof of Lemma 3
Letxm= (1−αm)βm+αmM−1
2wfor notational simplicity. We have
∇2fm(w,βm) =
Λ
M∇2f′(M−1
2w) +α2
m
M∇2f′
m(xm)αm(1−αm)
M1
2∇2f′
m(xm)
αm(1−αm)
M1
2∇2f′
m(xm) (1−αm)2∇2f′
m(xm)

=/parenleftigg
Λ
M∇2f′(M−1
2w) 0d×d
0d×d 0d×d/parenrightigg
+1
M
α2
m
Mαm(1−αm)
M1
2
αm(1−αm)
M1
2(1−αm)2
⊗∇2f′
m(xm)
⪰/parenleftigg
Λµ′
MId0d×d
0d×d0d×d/parenrightigg
+
α2
m
Mαm(1−αm)
M1
2
αm(1−αm)
M1
2(1−αm)2
⊗(µ′Id)
=µ′
Λ+α2
m
Mαm(1−αm)
M1
2
αm(1−αm)
M1
2(1−αm)2

/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
:=Mm⊗Id.
Next, we show that
Mm⪰/parenleftigg
(1−αm)2
2M0
0(1−αm)2
2/parenrightigg
. (28)
For that, it suffices to show that
det/parenleftigg
Mm−/parenleftigg
(1−αm)2
2M0
0(1−αm)2
2/parenrightigg/parenrightigg
≥0,
which holds since
det/parenleftigg
Mm−/parenleftigg
(1−αm)2
2M0
0(1−αm)2
2/parenrightigg/parenrightigg
=/parenleftigg
Λ +α2
m−(1−αm)2
2
M−/parenrightigg
(1−αm)2
2−α2
m(1−αm)2
M
≥/parenleftbigg
2α2
m
M/parenrightbigg(1−αm)2
2−α2
m(1−αm)2
M= 0.
Finally, using (28) Mtimes, it is easy to see that
∇2FAPFL 2(w,β)⪰µ′(1−αmax)2
MId(M+1)
as desired. Verifying the smoothness constants is straightforward.
B.4 Proof of Lemma 5
We have
1
MM/summationdisplay
m=1∥∇wfm(wk
m,βk
m)∥2≤3
MM/summationdisplay
m=1∥∇wfm(wk
m,βk
m)−∇wfm(wk,βk
m)∥2
+3
MM/summationdisplay
m=1∥∇wfm(wk,βk
m)−∇wfm(w∗,β∗)∥2
37Published in Transactions on Machine Learning Research (May/2023)
+3
MM/summationdisplay
m=1∥∇wfm(w∗,β∗)∥2.
Then, using Assumption 1, the above display is bounded as
3L2
MM/summationdisplay
m=1∥wk
m−wk∥2+6L
MM/summationdisplay
m=1Dfm((wk,βk
m),(w∗,β∗)) + 3ζ2
∗
= 6Lw/parenleftbig
f(wk,βk
m)−f(w∗,β∗)/parenrightbig
+ 3(Lw)2Vk+ 3ζ2
∗,
which shows (21).
To establish (22), we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(wk
m,βk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
M2M/summationdisplay
m=1/vextenddouble/vextenddouble∇βfm(wk
m,βk
m)/vextenddouble/vextenddouble2
≤2
MM/summationdisplay
i=1∥∇wfm(wk
m,βk
m)−∇wfm(wk,βk
m)∥2
+2
MM/summationdisplay
m=1∥∇βfm(wk
m,βk
m)−∇βfm(w∗,β∗)∥2
+2
M2M/summationdisplay
m=1/vextenddouble/vextenddouble∇βfm(wk
m,βk
m)−∇βfm(w∗,β∗)/vextenddouble/vextenddouble2.
Then, using Assumption 1, the above display is bounded as
2(Lw)2
MM/summationdisplay
m=1∥wk
m−wk∥2+4L
MM/summationdisplay
m=1Dfm((wk,βk
m),(w∗,β∗)) = 4L/parenleftbig
f(wk,βk
m)−f(w∗,β∗)/parenrightbig
+ 2(Lw)2Vk.
This completes the proof.
B.5 Proof of Lemma 6
Let us start with establishing (23). We have
1
MM/summationdisplay
m=1E∥gk
w,m∥2=1
MM/summationdisplay
m=1/parenleftbig
E∥gk
w,m−∇wfm(wk
m,βk
m)∥2+∥∇wfm(wk
m,βk
m)∥2/parenrightbig
≤σ2
B+∥∇wfm(wk
m,βk
m)∥2.
Now (23) follows from an application of (21). Similarly, to show (24), we have
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1gk
w,m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
M2M/summationdisplay
m=1/vextenddouble/vextenddoublegk
β,m/vextenddouble/vextenddouble2
=E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1(gk
w,m−∇wfm(wk
m,βk
m))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(wk
m,βk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
M2M/summationdisplay
m=1/parenleftig
E/vextenddouble/vextenddoublegk
β,m−∇βfm(wk
m,βk
m)/vextenddouble/vextenddouble2+/vextenddouble/vextenddouble∇βfm(wk
m,βk
m)/vextenddouble/vextenddouble2/parenrightig
≤σ2
MB+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(wk
m,βk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
38Published in Transactions on Machine Learning Research (May/2023)
+σ2
MB+1
M2M/summationdisplay
m=1/vextenddouble/vextenddouble∇βfm(wk
m,βk
m)/vextenddouble/vextenddouble2.
Now (24) follows from (22), which completes the proof.
B.6 Proof of Lemma 7
The proof is identical to the proof of Lemma E.1 from (Gorbunov et al., 2021) with a single difference –
using inequality (23) instead of Assumption E.1 from (Gorbunov et al., 2021). We omit the details.
B.7 Proof of Theorem 3 and Theorem 4
We start by introducing additional notation. We set kp=p·τ, whereτ∈N+is the length of the averaging
period. Let kp=pτ+τ−1 =kp+1−1 =vp. Denote the total number of iterations as Kand assume that
K=k¯pfor some ¯p∈N+. The final result is set to be that ˆw=wKandˆβm=βK
mfor allm∈[M]. We assume
that the solution to (1) is w∗,β∗
1,...,β∗
Mand that the optimal value is f∗. Letwk=1
M/summationtextM
m=1wk
mfor allk.
Note that this quantity will not be actually computed in practice unless k=kpfor somep∈N, where we
havewkp=wkpmfor allm∈[M]. In addition, let ξk
m={ξk
1,m,ξk
2,m,...,ξk
B,m}andξk={ξk
1,ξk
2,...,ξk
M}.
Letθm= ((wm)⊤,(βm)⊤)⊤,θk
m= ((wk
m)⊤,(βk
m)⊤)⊤,θ∗
m= ((w∗)⊤,(β∗
m)⊤)⊤and ˆθk
m= ((wk)⊤,(βk
m)⊤)⊤.
Let
gk
m=1
B∇ˆfm(wk
m,βk
m;ξk
m), (29)
where
∇ˆfm(wk
m,βk
m;ξk
m) =B/summationdisplay
j=1∇ˆfm(wk
m,βk
m;ξk
j,m).
We assume that the gradient is unbiased, that is,
E/bracketleftbig
gk
m/bracketrightbig
=∇fm(wk
m,βk
m).
Let
gk
m,1=1
B∇wˆfm(wk
m,βk
m;ξk
m), gk
m,2=1
B∇βmˆfm(wk
m,βk
m;ξk
m), (30)
so thatgk
m= ((gk
m,1)⊤,(gk
m,2)⊤)⊤. We update the parameters by
(wk+1
m,βk+1
m) = (wk
m,βk
m)−ηkgk
m.
In addition, we define
hk=1
MM/summationdisplay
m=1gk
m,1, Vk=1
MM/summationdisplay
m=1∥wk
m−wk∥2.
Thenwk+1=wk−ηkhkfor allk.
We denote the Bregman divergence associated with fmforθmand¯θmas
Dfm(θm,¯θm):=fm(θm)−f(¯θm)−⟨∇fm(¯θm),θm−¯θm⟩.
Finally, we define the sum of residuals as
rk=∥wk−w∗∥2+1
MM/summationdisplay
m=1∥βk
m−β∗
m∥2=1
MM/summationdisplay
m=1∥ˆθk
m−θ∗
m∥2(31)
and letσ2
dif=1
M/summationtextM
m=1∥∇fm(θ∗
m)∥2.
The following proposition states some useful results that will be used in the proof below. The results are are
standard and can be found in, for example, Nesterov (2018).
39Published in Transactions on Machine Learning Research (May/2023)
Proposition 1. If the function fis differentiable and L-smooth, then
f(x)−f(y)−⟨∇f(y),x−y⟩≤L
2∥x−y∥2. (32)
Iffis also convex, then
∥∇f(x)−∇f(y)∥2≤2LDf(x,y) (33)
for allx,y.
For all vectors x,y, we have
2⟨x,y⟩≤ξ∥x∥2+ξ−1∥y∥2,∀ξ>0, (34)
−⟨x,y⟩=−1
2∥x∥2−1
2∥y∥2+1
2∥x−y∥2. (35)
For vectors v1,v2,...,vn, by the Jensen’s inequality and the convexity of the map: x∝⇕⊣√∫⊔≀→∥x∥2, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1vi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤1
nn/summationdisplay
i=1∥vi∥2. (36)
Next, we establish a few technical results.
Lemma 8. Suppose Assumption 4 holds. Given {θk
m}m∈[M], we have
Eξk/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθk+1
m)/bracketrightigg
−1
MM/summationdisplay
m=1fm(ˆθk
m)
≤−ηk/angbracketleftigg
1
MM/summationdisplay
m=1∇wfm(ˆθk
m),1
MM/summationdisplay
m=1∇wfm(θk
m)/angbracketrightigg
−ηk
MM/summationdisplay
m=1⟨∇βmfm(ˆθk
m),∇βmfm(θk
m)⟩
+η2
kL
2Eξk/bracketleftbig
∥hk∥2/bracketrightbig
+η2
kL
2MM/summationdisplay
m=1Eξkm/bracketleftbig
∥gk
m,2∥2/bracketrightbig
,
where the expectation is taken only with respect to the randomness in ξk.
Proof.By theL-smoothness assumption on fm(·)and (32), we have
fm(ˆθk+1
m)−fm(ˆθk
m)−⟨∇fm(ˆθk
m),ˆθk+1
m−ˆθk
m⟩≤L
2∥ˆθk+1
m−ˆθk
m∥2.
Thus, we have
fm(ˆθk+1
m)−fm(ˆθk
m)≤−ηk⟨∇wfm(ˆθk
m),hk⟩−ηk⟨∇βmfm(ˆθk
m),gk
m,2⟩+η2
kL
2∥hk∥2+η2
kL
2∥gk
m,2∥2,
which further implies that
1
MM/summationdisplay
m=1fm(ˆθk+1
m)−1
MM/summationdisplay
m=1fm(ˆθk
m)
≤−ηk/angbracketleftigg
1
MM/summationdisplay
m=1∇wfm(ˆθk
m),hk/angbracketrightigg
−ηk
MM/summationdisplay
m=1⟨∇βmfm(ˆθk
m),gk
m,2⟩+η2
kL
2∥hk∥2
+η2
kL
2MM/summationdisplay
m=1∥gk
m,2∥2.
The result follows by taking the expectation with respect to the randomness in ξk, while keeping the other
quantities fixed.
40Published in Transactions on Machine Learning Research (May/2023)
Lemma 9. Suppose Assumptions 5 and 6 hold. Given {θk
m}m∈[M], we have
Eξk/bracketleftbig
∥hk∥2/bracketrightbig
+1
MM/summationdisplay
m=1Eξkm/bracketleftbig
∥gk
m,2∥2/bracketrightbig
≤/parenleftbiggC1
M+C2+ 1/parenrightbigg1
MM/summationdisplay
m=1∥∇fm(θk
m)∥2+σ2
1
MB+σ2
2
B
≤λ/parenleftbiggC1
M+C2+ 1/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B,
where the expectation is taken only with respect to the randomness in ξk.
Proof.Note that
Eξk/bracketleftbig
∥hk∥2/bracketrightbig
=Eξk
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1gk
m,1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

(i)=Eξk
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1/parenleftbig
gk
m,1−∇wfm(θk
m)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(ii)=1
M2M/summationdisplay
m=1Eξkm/bracketleftig/vextenddouble/vextenddoublegk
m,1−∇wfm(θk
m)/vextenddouble/vextenddouble2/bracketrightig
+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(iii)
≤1
M2M/summationdisplay
m=1/parenleftbigg
C1∥∇fm(θk
m)∥2+σ2
1
B/parenrightbigg
+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(iv)
≤C1
M2M/summationdisplay
m=1∥∇fm(θk
m)∥2+σ2
1
MB+1
MM/summationdisplay
m=1∥∇wfm(θk
m)∥2,
where (i) is due to gk
m,1being unbiased, (ii) is by the fact that ξk
1,ξk
2,...,ξk
Mare independent, (iii) is by
Assumption 5, and (iv) is by (36). Similarly, we have
1
MM/summationdisplay
m=1Eξkm/bracketleftbig
∥gk
m,2∥2/bracketrightbig
=1
MM/summationdisplay
m=1Eξkm/bracketleftbig
∥gk
m,2−∇βmfm(θk
m)∥2/bracketrightbig
+1
MM/summationdisplay
m=1∥∇βmfm(θk
m)∥2
≤C2
MM/summationdisplay
m=1∥∇fm(θk
m)∥2+σ2
2
B+1
MM/summationdisplay
m=1∥∇βmfm(θk
m)∥2.
The lemma follows by combining the two inequalities.
Lemma 10. Under Assumption 4, we have
−ηk/angbracketleftigg
1
MM/summationdisplay
m=1∇wfm(ˆθk
m),1
MM/summationdisplay
m=1∇wfm(θk
m)/angbracketrightigg
−ηk
MM/summationdisplay
m=1⟨∇βmfm(ˆθk),∇βmfm(θk
m)⟩
≤−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηkL2
2Vk.
Proof.By (35), we have
−ηk/angbracketleftigg
1
MM/summationdisplay
m=1∇wfm(ˆθk
m),1
MM/summationdisplay
m=1∇wfm(θk
m)/angbracketrightigg
41Published in Transactions on Machine Learning Research (May/2023)
=−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1/parenleftig
∇wfm(ˆθk
m)−∇wfm(θk
m)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇wfm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηk
2MM/summationdisplay
m=1/vextenddouble/vextenddouble/vextenddouble∇wfm(ˆθk
m)−∇wfm(θk
m)/vextenddouble/vextenddouble/vextenddouble2
,
where the last inequality follows from (36). We also have
−ηk⟨∇βmfm(ˆθk),∇βmfm(θk
m)⟩
=−ηk
2∥∇βmfm(ˆθk
m)∥2−ηk
2∥∇βmfm(θk
m)∥2+ηk
2∥∇βmfm(ˆθk
m)−∇βmfm(θk
m)∥2.
Thus,
−ηk
M⟨∇βmfm(ˆθk),∇βmfm(θk
m)⟩=−ηk
2MM/summationdisplay
m=1∥∇βmfm(ˆθk
m)∥2−ηk
2MM/summationdisplay
m=1∥∇βmfm(θk
m)∥2
+ηk
2MM/summationdisplay
m=1∥∇βmfm(ˆθk
m)−∇βmfm(θk
m)∥2
≤−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇βmfm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇βmfm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηk
2MM/summationdisplay
m=1∥∇βmfm(ˆθk
m)−∇βmfm(θk
m)∥2.
Combining the above equations, we have
−ηk/angbracketleftigg
1
MM/summationdisplay
m=1∇wfm(ˆθk
m),1
MM/summationdisplay
m=1∇wfm(θk
m)/angbracketrightigg
−ηk
MM/summationdisplay
m=1⟨∇βmfm(ˆθk
m),∇βmfm(θk
m)⟩
≤−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηk
2MM/summationdisplay
m=1/vextenddouble/vextenddouble/vextenddouble∇fm(ˆθk
m)−∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble2
(i)
≤−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηkL2
2MM/summationdisplay
m=1/vextenddouble/vextenddoublewk
m−wk/vextenddouble/vextenddouble2
=−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηkL2
2Vk,
where (i)is by Assumption 4.
Lemma 11. Under Assumptions 4 and 7, we have
−ηk/angbracketleftigg
1
MM/summationdisplay
m=1∇wfm(ˆθk
m),1
MM/summationdisplay
m=1∇wfm(θk
m)/angbracketrightigg
−ηk
MM/summationdisplay
m=1⟨∇βmfm(ˆθk),∇βmfm(θk
m)⟩
42Published in Transactions on Machine Learning Research (May/2023)
≤−ηkµ/parenleftigg
1
MM/summationdisplay
m=1∇fm(ˆθk
m)−f∗/parenrightigg
−ηk
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηkL2
2Vk.
Proof.The proof follows directly from Lemma 10 and Assumption 7.
Lemma 12. Suppose Assumptions 5 and 6 hold. For kp+ 1≤k≤vp, we have
E/bracketleftbig
Vk/bracketrightbig
≤λ(τ−1)(C1+ 1)k−1/summationdisplay
t=kpη2
tE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θt
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+σ2
dif(τ−1)(C1+ 1)k−1/summationdisplay
t=kpη2
t+σ2
1(τ−1)
Bk−1/summationdisplay
t=kpη2
t.
Note thatVkp= 0.
Proof.Note thatwkp=wkpmfor allm∈[M]. Thus, for kp+ 1≤k≤vp, we have
∥wk
m−wk∥2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublewkp
m−k−1/summationdisplay
t=kpηtgt
m,1−wkp−k−1/summationdisplay
t=kpηtht/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
t=kpηtgt
m,1−k−1/summationdisplay
t=kpηtht/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
.
Since
1
MM/summationdisplay
m=1k−1/summationdisplay
t=kpηtgt
m,1=k−1/summationdisplay
t=kpηtht,
we have
1
MM/summationdisplay
m=1∥wk
m−wk∥2=1
MM/summationdisplay
m=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
t=kpηtgt
m,1−k−1/summationdisplay
t=kpηtht/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=1
MM/summationdisplay
m=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
t=kpηtgt
m,1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
t=kpηtht/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤1
MM/summationdisplay
m=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
t=kpηtgt
m,1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤k−kp
MM/summationdisplay
m=1k−1/summationdisplay
t=kpη2
t∥gt
m,1∥2≤τ−1
MM/summationdisplay
m=1k−1/summationdisplay
t=kpη2
t∥gt
m,1∥2.(37)
Given{θk
m}m∈[M], we have
Eξk/bracketleftigg
1
MM/summationdisplay
m=1∥gk
m,1∥2/bracketrightigg
=1
MM/summationdisplay
m=1Eξkm/bracketleftbig
∥gk
m,1∥2/bracketrightbig
=1
MM/summationdisplay
m=1Eξkm/bracketleftbig
∥gk
m,1−∇wfm(θk
m)∥2/bracketrightbig
+1
MM/summationdisplay
m=1∥∇wfm(θk
m)∥2
≤1
MM/summationdisplay
m=1/bracketleftbigg
(C1+ 1)∇∥fm(θk
m)∥2+σ2
1
B/bracketrightbigg
+1
MM/summationdisplay
m=1∥∇fm(θk
m)∥2
=C1+ 1
MM/summationdisplay
m=1∥∇fm(θk
m)∥2+σ2
1
B,
43Published in Transactions on Machine Learning Research (May/2023)
where the expectation is taken with respect to the randomness in ξk. Thus, by the independence of
ξ(1),ξ(2),...,ξkand taking an unconditional expectation on both sides of (37), we have
E/bracketleftbig
Vk/bracketrightbig
= (τ−1)k−1/summationdisplay
t=kpη2
tE/bracketleftigg
Eξt/bracketleftigg
1
MM/summationdisplay
m=1∥gt
m,1∥2/bracketrightigg/bracketrightigg
≤(τ−1)(C1+ 1)k−1/summationdisplay
t=kpη2
tE/bracketleftigg
1
MM/summationdisplay
m=1∥∇fm(θt
m)∥2/bracketrightigg
+(τ−1)σ2
1
Bk−1/summationdisplay
t=kpη2
t
≤λ(τ−1)(C1+ 1)k−1/summationdisplay
t=kpη2
tE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θt
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+σ2
dif(τ−1)(C1+ 1)k−1/summationdisplay
t=kpη2
t+(τ−1)σ2
1
Bk−1/summationdisplay
t=kpη2
t,
where the last inequality follows Assumption 6.
With these preliminaries, we are ready to prove Theorem 3 and Theorem 4.
B.7.1 Proof of Theorem 3
Under Assumptions 4-6, given {θk
m}m∈[M], it follows from Lemmas 8-10 that
Eξk/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθk+1
m)/bracketrightigg
−1
MM/summationdisplay
m=1fm(ˆθk
m)
≤−η
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−η
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηL2
2Vk
+1
2η2Lλ/parenleftbiggC1
M+C2+ 1/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
2η2Lλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbigg
,
where the expectation is taken with respect to the randomness in ξk. Thus, taking the unconditional
expectation on both sides of the equation above, we have
E/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθk+1
m)−1
MM/summationdisplay
m=1fm(ˆθk
m)/bracketrightigg
≤−η
2E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
−η
2E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ηL2
2E/bracketleftbig
Vk/bracketrightbig
+1
2η2Lλ/parenleftbiggC1
M+C2+ 1/parenrightbigg
E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+1
2η2Lλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbigg
,
which implies that
44Published in Transactions on Machine Learning Research (May/2023)
E/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθkp+1
m)−1
MM/summationdisplay
m=1fm(ˆθkp
m)/bracketrightigg
=vp/summationdisplay
k=kpE/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθk+1
m)−1
MM/summationdisplay
m=1fm(ˆθk
m)/bracketrightigg
≤−η
2vp/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+η
2/braceleftbigg
−1 +ηLλ/parenleftbiggC1
M+C2+ 1/parenrightbigg/bracerightbiggvp/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+ηL2
2vp/summationdisplay
k=kpE/bracketleftbig
Vk/bracketrightbig
+1
2η2Lλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbiggvp/summationdisplay
k=kp1.(38)
By Lemma 12, for all kp≤k≤vp, we have that
E/bracketleftbig
Vk/bracketrightbig
≤λη2(τ−1)(C1+ 1)k−1/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+η2σ2
dif(τ−1)(C1+ 1)(k−kp) +η2σ2
1(τ−1)
B(k−kp)
≤λη2(τ−1)(C1+ 1)vp/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+η2σ2
dif(τ−1)2(C1+ 1) +η2σ2
1(τ−1)2
B.
Therefore, we have
ηL2
2vp/summationdisplay
k=kpE/bracketleftbig
Vk/bracketrightbig
≤1
2λη3L2(τ−1)τ(C1+ 1)vp/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+1
2η3L2σ2
dif(τ−1)2(C1+ 1)vp/summationdisplay
k=kp1 +η3L2σ2
1(τ−1)2
2Bvp/summationdisplay
k=kp1.
Combined with (38), we have
E/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθkp+1
m)−1
MM/summationdisplay
m=1fm(ˆθkp
m)/bracketrightigg
≤−η
2vp/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+η
2/braceleftbigg
−1 +ηLλ/parenleftbiggC1
M+C2+ 1/parenrightbigg
+λη2L2(τ−1)τ(C1+ 1)/bracerightbiggvp/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+1
2η2Lλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbiggvp/summationdisplay
k=kp1
+1
2η3L2σ2
dif(τ−1)2(C1+ 1)vp/summationdisplay
k=kp1 +η3L2σ2
1(τ−1)2
2Bvp/summationdisplay
k=kp1.
45Published in Transactions on Machine Learning Research (May/2023)
Since we require that
−1 +ηLλ/parenleftbiggC1
M+C2+ 1/parenrightbigg
+η2L2(τ−1)τ(C1+ 1)≤0,
the equation above implies that
E/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθkp+1
m)−1
MM/summationdisplay
m=1fm(ˆθkp
m)/bracketrightigg
≤−η
2vp/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
2η2Lλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbiggvp/summationdisplay
k=kp1
+1
2η3L2σ2
dif(τ−1)2(C1+ 1)vp/summationdisplay
k=kp1 +η3L2σ2
1(τ−1)2
2Bvp/summationdisplay
k=kp1.
Since we have assumed that K=k¯pfor some ¯p∈N+, we further have
1
KE/bracketleftigg/parenleftigg
1
MM/summationdisplay
m=1fm(ˆθK
m)−f∗/parenrightigg
−/parenleftigg
1
MM/summationdisplay
m=1fm(ˆθ0
m)−f∗/parenrightigg/bracketrightigg
=1
KE/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθK
m)−1
MM/summationdisplay
m=1fm(ˆθ0
m)/bracketrightigg
=1
K¯p−1/summationdisplay
p=0E/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθkp+1
m)−1
MM/summationdisplay
m=1fm(ˆθkp
m)/bracketrightigg
≤−η
2K¯p−1/summationdisplay
p=0vp/summationdisplay
k=kpE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+1
2η2Lλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbigg1
K¯p−1/summationdisplay
p=0vp/summationdisplay
k=kp1
+1
2η3L2σ2
dif(τ−1)2(C1+ 1)1
K¯p−1/summationdisplay
p=0vp/summationdisplay
k=kp1 +η3L2σ2
1(τ−1)2
2B1
K¯p−1/summationdisplay
p=0vp/summationdisplay
k=kp1
=−η
2KK−1/summationdisplay
k=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
2η2Lλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbigg
+1
2η3L2σ2
dif(τ−1)2(C1+ 1) +η3L2σ2
1(τ−1)2
2B.
This implies that
1
KK−1/summationdisplay
k=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1fm(ˆθk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

≤2E/bracketleftig
1
M/summationtextM
m=1fm(ˆθ0
m)−f∗/bracketrightig
ηK+ηLλ/braceleftbigg/parenleftbiggC1
M+C2+ 1/parenrightbigg
σ2
dif+σ2
1
MB+σ2
2
B/bracerightbigg
+η2L2σ2
dif(τ−1)2(C1+ 1) +η2L2σ2
1(τ−1)2
B
and the proof is complete.
46Published in Transactions on Machine Learning Research (May/2023)
B.7.2 Proof of Theorem 4
By Lemmas 8, 9, 11 and 12, for kp+ 1≤k≤vp, we have
E/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθk+1
m)−f∗/bracketrightigg
≤∆kE/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθk
m)−f∗/bracketrightigg
+ηk
2/braceleftbigg
−1 +ηkλL/parenleftbiggC1
M+C2+ 1/parenrightbigg/bracerightbigg
E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+Bkk−1/summationdisplay
t=kpη2
tE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θ(t)
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ck,
where
∆k= 1−ηkµ,
Bk=1
2ηkL2λ(τ−1)(C1+ 1),and
ck=ηkL2
2

σ2
dif(τ−1)(C1+ 1)k−1/summationdisplay
t=kpη2
t+σ2
1(τ−1)
Bk−1/summationdisplay
t=kpη2
t


+η2
kL
2/braceleftbigg
σ2
dif/parenleftbiggC1
M+C2+ 1/parenrightbigg
+σ2
1
MB+σ2
2
B/bracerightbigg
.(39)
Let
ak=E/bracketleftigg
1
MM/summationdisplay
m=1fm(ˆθk
m)−f∗/bracketrightigg
,
D=λL/parenleftbiggC1
M+C2+ 1/parenrightbigg
,and
ek=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1∇fm(θk
m)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
,
and denote
kp−1/summationdisplay
k=kpη2
ket= 0,
ckp=α2
kpL
2/braceleftbigg
σ2
dif/parenleftbiggC1
M+C2+ 1/parenrightbigg
+σ2
1
MB+σ2
2
B/bracerightbigg
.
Then
ak+1≤∆kak+ηk
2(−1 +Dηk)ek+Bkk−1/summationdisplay
t=kpη2
ket+ck,
for allkp≤k≤vp. Under the conditions on βandτ, by Lemmas 13 and 14, we have
avp+1≤
vp/productdisplay
k=kp∆k
akp+vp−1/summationdisplay
k=kp/parenleftiggvp/productdisplay
i=k+1∆i/parenrightigg
ck+cvp. (40)
47Published in Transactions on Machine Learning Research (May/2023)
Letzk= (k+b)2, whereb=βτ+ 1. Then
∆kzk
ηk= (1−µηk)µ(k+b)3= (1−1
k+b)µ(k+b)3=µ(k+b−1)(k+b)2≤µ(k+b−1)3=zk−1
ηk−1
and, thus,
zvp
ηvp/parenleftiggvp/productdisplay
i=k+1∆i/parenrightigg
=zvp
ηvp∆vp/parenleftiggvp−1/productdisplay
i=k+1∆i/parenrightigg
≤zvp−1
ηvp−1/parenleftiggvp−1/productdisplay
i=k+1∆i/parenrightigg
...≤zk
ηk.
Note thatvp+ 1 =kp+1. Plugging the above inequality into (40), we then get
zvp
ηvpakp+1≤zkp
ηkpakp+vp/summationdisplay
k=kpzk
ηkck.
Since we have assumed that K=k¯p, we thus have
zK−1
ηK−1aK=zv¯p−1
ηv¯p−1ak¯p≤zk¯p−1
ηk¯p−1ak¯p−1+v¯p−1/summationdisplay
t=k¯p−1zt
ηtct...≤z0
η0a0+K−1/summationdisplay
k=0zk
ηkck. (41)
Since, forkp≤k≤vp, we have
ck=ηkL2
2

σ2
dif(τ−1)(C1+ 1)t−1/summationdisplay
k=kpη2
k+σ2
1(τ−1)
Bt−1/summationdisplay
k=kpη2
k


+η2
kL
2/braceleftbigg
σ2
dif/parenleftbiggC1
M+C2+ 1/parenrightbigg
+σ2
1
MB+σ2
2
B/bracerightbigg
≤ηkη2
⌊k
τ⌋τL2(τ−1)2
2/braceleftbigg
σ2
dif(C1+ 1) +σ2
1
B/bracerightbigg
+η2
kL
2/braceleftbigg
σ2
dif/parenleftbiggC1
M+C2+ 1/parenrightbigg
+σ2
1
MB+σ2
2
B/bracerightbigg
,
we also have
K−1/summationdisplay
k=0zk
ηkck≤L2(τ−1)2
2/braceleftbigg
σ2
dif(C1+ 1) +σ2
1
B/bracerightbiggK−1/summationdisplay
k=0zkη2
⌊k
τ⌋τ
+L
2/braceleftbigg
σ2
dif/parenleftbiggC1
M+C2+ 1/parenrightbigg
+σ2
1
MB+σ2
2
B/bracerightbiggK−1/summationdisplay
k=0zkηk.(42)
Assume that k=pτ+r, where 0≤r≤τ−1. Then
/floorleftbiggt
τ/floorrightbigg
τ+b=pτ+βτ+ 1 = (p+β)τ+ 1≥βτ≥r,
as we have assumed that β >1. Thus
2/parenleftbigg/floorleftbiggk
τ/floorrightbigg
τ+b/parenrightbigg
≥(p+β)τ+ 1 +r=k+b
and
K−1/summationdisplay
k=0zkη2
⌊k
τ⌋τ=1
µ2K−1/summationdisplay
k=0/parenleftigg
k+b/floorleftbigk
τ/floorrightbig
τ+b/parenrightigg2
≤4K
µ2.
48Published in Transactions on Machine Learning Research (May/2023)
Next, note that
K−1/summationdisplay
k=0zkηk=1
µK−1/summationdisplay
k=0(k+b)≤K(K+ 2b)
2µ. (43)
Combining equations (41)-(43), we have
E/bracketleftigg
1
MM/summationdisplay
m=1fm/parenleftig
ˆθK
m/parenrightig
−f∗/bracketrightigg
≤b3
(K+βτ)3E/bracketleftigg
1
MM/summationdisplay
m=1fm/parenleftig
ˆθ0
m/parenrightig
−f∗/bracketrightigg
+2L2(τ−1)2K
µ3(K+βτ)3/braceleftbigg
σ2
dif(C1+ 1) +σ2
1
B/bracerightbigg
+LK(K+ 2βτ+ 2)
4µ2(K+βτ)3/braceleftbigg
σ2
dif/parenleftbiggC1
M+C2+ 1/parenrightbigg
+σ2
1
MB+σ2
2
B/bracerightbigg
,
which completes the proof.
B.7.3 Auxiliary Results
We give two technical lemmas that are used to prove Theorem 4.
Lemma 13. Consider the sequence {ak}kp≤k≤vpin the proof of Theorem 4 that satisfies
ak+1≤∆kak+ηk
2(−1 +Dηk)ek+Bkk−1/summationdisplay
t=kpη2
tet+ck,
where ∆k,Bk, andckare defined in (39). Suppose the sequence of learning rates {ηk}satisfies
ηvp≤D−1, (44)
ηvp−1≤/parenleftbigg
D+2Bvp
∆vp/parenrightbigg−1
, (45)
...
ηkp≤
D+2/parenleftig
Bvp+/summationtextvp−1
j=kp+1Bj/producttextvp
i=j+1∆i/parenrightig
/producttextvp
i=kp+1∆i
−1
. (46)
Then
avp+1≤
vp/productdisplay
k=kp∆k
akp+vp−1/summationdisplay
k=kp/parenleftiggvp/productdisplay
i=k+1∆i/parenrightigg
ck+cvp.
Proof.We start by noting that
avp+1≤∆vpavp+ηvp
2(−1 +Dηvp)evp+Bvpvp−1/summationdisplay
k=kpη2
kek+cvp
≤∆vpavp+Bvpvp−1/summationdisplay
k=kpη2
kek+cvp,
where the last inequality is due to (44). Thus, we have
avp+1≤∆vpavp+Bvpvp−1/summationdisplay
k=kpη2
kek+cvp
49Published in Transactions on Machine Learning Research (May/2023)
= ∆vpavp+Bvp
vp−2/summationdisplay
k=kpη2
kek+η2
vp−1evp−1
+cvp
≤∆vp
∆vp−1avp−1+ηvp−1
2(−1 +Dηvp−1)evp−1+Bvp−1vp−2/summationdisplay
k=kpη2
kek+cvp−1

+Bvp
vp−2/summationdisplay
k=kpη2
kek+η2
vp−1evp−1
+cvp
= ∆vp∆vp−1avp−1+ηvp−1∆vp
2/bracketleftbigg
−1 +Dηvp−1+2Bvpηvp−1
∆vp/bracketrightbigg
evp−1
+/parenleftbig
∆vpBvp−1+Bvp/parenrightbigvp−2/summationdisplay
k=kpη2
kek+/parenleftbig
∆vpcvp−1+cvp/parenrightbig
.
By (45), we have
−1 +Dηvp−1+2Bvpηvp−1
∆vp≤0.
Therefore,
avp+1≤∆vp∆vp−1avp−1+/parenleftbig
∆vpBvp−1+Bvp/parenrightbigvp−2/summationdisplay
k=kpη2
kek+/parenleftbig
∆vpcvp−1+cvp/parenrightbig
.
Under the assumptions on ηk, repeating the process above, we have
avp+1≤
vp/productdisplay
i=kp+1∆i
akp+1
+

vp/productdisplay
i=kp+2∆i
Bkp+1+
vp/productdisplay
i=kp+3∆i
Bkp+2+···+ ∆vpBvp−1+Bvp
η2
kpekp
+vp−1/summationdisplay
k=kp/parenleftiggvp/productdisplay
i=k+1∆i/parenrightigg
ck.
Since
akp+1≤∆kpakp+ηkp
2(−1 +Dηkp)ekp+ckp,
combining with (46), the final result follows.
Lemma 14. Letηk= (µ(k+βτ+ 1))−1where
β >max/braceleftbigg2λL
µ/parenleftbiggC1
M+C2+ 1/parenrightbigg
−2,2L2λ(C1+ 1)
µ2/bracerightbigg
.
and
τ≥/radicaligg
max/braceleftbig
(2L2λ(C1+ 1)/µ2)e1/β−4,0/bracerightbig
β2−(2L2λ(C1+ 1)/µ2)e1
β.
Then the conditions in Lemma 13 are satisfied for ηkfor allk≥0.
Proof.Let∆kandBkbe defined as in (39). Since ∆k<1for allk, afterp-th communication, for the right
hand side of (46), we have

D+2/parenleftig
Bvp+/summationtextvp−1
j=kp+1Bj/producttextvp
i=j+1∆i/parenrightig
/producttextvp
i=kp+1∆i
−1
50Published in Transactions on Machine Learning Research (May/2023)
≤
D+2/parenleftig
Bvp+/summationtextvp−1
j=kp+2Bj/producttextvp
i=j+1∆i/parenrightig
/producttextvp
i=kp+1∆i
−1
≤
D+2/parenleftig
Bvp+/summationtextvp−1
j=kp+2Bj/producttextvp
i=j+1∆i/parenrightig
/producttextvp
i=kp+2∆i
−1
.
Thus, by induction, we have

D+2/parenleftig
Bvp+/summationtextvp−1
j=kp+1Bj/producttextvp
i=j+1∆i/parenrightig
/producttextvp
i=kp+1∆i
−1
≤
D+2/parenleftig
Bj+/summationtextvp−1
j=kp+2Bj/producttextvp
i=j+1∆i/parenrightig
/producttextvp
i=kp+2∆i
−1
≤...
≤/parenleftbigg
D+2Bvp
∆vp/parenrightbigg−1
≤D−1.(47)
Askincreases, we have that ηkdecreases, ∆kincreases, and Bkdecreases. Thus, for 1≤k≤K, we have
ηK≤ηK−1≤···≤η1. On the other hand, we can lower bound the right hand side of (46) as

D+2/parenleftig
Bvp+/summationtextvp−1
j=kp+1Bj/producttextvp
i=j+1∆i/parenrightig
/producttextvp
i=kp+1∆i
−1
≥
D+2/parenleftig
B1+/summationtextvp−1
j=kp+1B1/producttextvp
i=j+1∆K/parenrightig
/producttextvp
i=kp+1∆1
−1
≥
D+2B1/parenleftig
1 +/summationtextvp−1
j=kp+1∆vp−j
K/parenrightig
∆τ−1
1
−1
≥
D+2B1/parenleftig
1 +/summationtextvp−1
j=kp+11/parenrightig
∆τ−1
1
−1
=/parenleftbigg
D+2B1(τ−1)
∆τ−1
1/parenrightbigg−1
.(48)
If
η1≤1
D+2B1(τ−1)
∆τ−1
1, (49)
then the conditions on stepsizes in Lemma 13 are satisfied for all ηkby combining (47)-(49). Thus, we only
need to show that (49) is satisfied to complete the proof.
To that end, we need to have
/parenleftbigg
D+2B1(τ−1)
∆τ−1
1/parenrightbigg
τ1≤1
⇐⇒/parenleftbigg
λL/parenleftbiggC1
M+C2+ 1/parenrightbigg
+η1L2λ(τ−1)2(C1+ 1)
(1−η1µ)τ−1/parenrightbigg
η1≤1
51Published in Transactions on Machine Learning Research (May/2023)
⇐⇒λL/parenleftbiggC1
M+C2+ 1/parenrightbigg
(1−η1µ)τ−1+η1L2λ(τ−1)2(C1+ 1)≤(1−η1µ)τ−1
η1.
To satisfy the above equation, we need


λL/parenleftbigC1
M+C2+ 1/parenrightbig
(1−η1µ)τ−1≤(2η1)−1(1−η1µ)τ−1
η1L2λ(τ−1)2(C1+ 1)≤(2η1)−1(1−η1µ)τ−1.(50)
Note thatη1= 1/(µ(βτ+ 2)). Thus, to satisfy the first inequality in (50), we need
2λL/parenleftbiggC1
M+C2+ 1/parenrightbigg
≤1
η1=µ(βτ+ 2).
Sinceµ(βτ+ 2)≥µ(β+ 2), the condition above follows if
β≥2λL
µ/parenleftbiggC1
M+C2+ 1/parenrightbigg
−2. (51)
Next, to satisfy the second inequality in (50), we need
2η2
1L2λ(τ−1)2(C1+ 1)≤(1−η1µ)τ−1
⇐⇒2L2λ(C1+ 1)
µ2/parenleftbiggτ−1
βτ+ 2/parenrightbigg2/parenleftbiggβτ+ 2
βτ+ 1/parenrightbiggτ−1
≤1.
Since
/parenleftbiggβτ+ 2
βτ+ 1/parenrightbiggτ−1
=/parenleftbigg
1 +1
βτ+ 1/parenrightbiggτ−1
=/parenleftbigg
1 +(τ−1)/(βτ+ 1)
τ−1/parenrightbiggτ−1
≤exp/braceleftbiggτ−1
βτ+ 1/bracerightbigg
≤e1
β,
we need
2L2λ(C1+ 1)
µ2/parenleftbiggτ−1
βτ+ 2/parenrightbigg2
e1
β≤1.
Letν= 2L2λ(C1+ 1)/µ2. Then the above equation is equivalent to
(β2−νe1
β)τ2+ 2(β+νe1
β)τ+ (4−νe1
β)≥0.
First, we let β2−νe1
β>0or equivalently
β2
e1
β>2L2λ(C1+ 1)
µ2. (52)
Then we need τto be large enough such that
τ≥−2(β+νe1
β) +/radicalbigg
4(β+νe1
β)2−max/braceleftig
4(β2−νe1
β)(4−νe1
β),0/bracerightig
2(β2−νe1
β).
Since√
a2+b≤|a|+/radicalbig
|b|for anya,b∈R, the left hand side is smaller or equal to
/radicaligg
max/braceleftbig
νe1/β−4,0/bracerightbig
β2−νe1
β=/radicaligg
max/braceleftbig
(2L2λ(C1+ 1)/µ2)e1/β−4,0/bracerightbig
β2−(2L2λ(C1+ 1)/µ2)e1
β.
Therefore, we need
τ≥/radicaligg
max/braceleftbig
(2L2λ(C1+ 1)/µ2)e1/β−4,0/bracerightbig
β2−(2L2λ(C1+ 1)/µ2)e1
β. (53)
The final result follows from the combination of (51)-(53).
52Published in Transactions on Machine Learning Research (May/2023)
B.8 Proof of Theorem 6
Nesterov’s worst case objective. (Nesterov, 2018) Leth′:R∞→Rbe the Nesterov’s worst case
objective (see), i.e., h′(y) =1
2y⊤Ay−e⊤
1ywith tridiagonal Ahaving diagonal elements equal to 2 +c(for
somec>0) and offdiagonal elements equal to 1.5The proof rationale is to show that a k-th iterate of any
first order method must satisfy ∥yk∥0≤kand consequently
∥yk−y∗∥2≥/parenleftbigg√κ−1√κ+ 1/parenrightbigg2k
∥y∗∥2(54)
wherey∗:= arg miny∈R∞h′(y),κ:=λmax(A)
λmin(A).
Finite sum worst case objective. (Lan & Zhou, 2018) The construction of the worst case finite-
sum objective6h:R∞→R,h(z) =1
n/summationtextn
j=1hj(z)is such that hjcorresponds only on a j-th block of the
coordinates; in particular if z= [z1,z2,...,zn];z1,z2,...,zn∈R∞we sethj(z) =h′(zj). It was shown that
to reach∥zk−z∗∥2≤ϵone requires at least Ω/parenleftig/parenleftig
n+/radicalig
nL
µ/parenrightig
log1
ϵ/parenrightig
iterations forL-smooth functions hjand
µ−strongly convex h.
Distributed worst case objective. (Scaman et al., 2018) Define
g′
1(z):=1
2/parenleftbig
c1∥z∥2+c2/parenleftbig
e⊤
1z+z⊤M1z/parenrightbig/parenrightbig
g′
2(z) =g′
3(z) =···=g′
M(z):=1
2(M−1)/parenleftbig
c1∥z∥2+c2z⊤M2z/parenrightbig
whereM1is an infinite block diagonal matrix with blocks
1 1 0 0
1 2 1 0
0 1 1 0
0 0 0 0
andM2:=
1 0
0 0
M1
and
c1,c2>0are some constants determining the smoothness and strong convexity of the objective. The worst
case objective of Scaman et al. (2018) is now g(z) =1
M/summationtextm
m=1g′
m(z).
Distributed worst case objective with local finite sum. (Hendrikx et al., 2021) The given con-
struction is obtained from the one of Scaman et al. (2018) in the same way as the worst case finite sum
objective (Lan & Zhou, 2018) was obtained from the construction of Nesterov (2018). In particular, one
would setgm,j(z) =g′
m(zj)wherez= [z1,z2,...,zn]. Next, it was shown that such a construction with
properly chosen c1,c2yields a lower bound on the communication complexity of order Ω/parenleftig/radicalig
L
µlog1
ϵ/parenrightig
and
the lower bound on the local computation of order Ω/parenleftig/parenleftig
n+/radicalig
nL
µ/parenrightig
log1
ϵ/parenrightig
whereLis a smoothness constant
ofgm,j,Lis a smoothness constant of gm(z) =1
n/summationtextn
j=1gj(z)andµis the strong convexity constant of
g(z) =1
M/summationtextM
m=1gm(z).
Our construction and sketch of the proof. Now, our construction is straightforward – we set
fm(w,βm) =g(w) +h(βm)withg,hscaled appropriately such that the strong convexity ratio is as per
Assumption 1. Clearly, to minimize the global part g(w), we require at least Ω/parenleftig/radicalig
Lw
µlog1
ϵ/parenrightig
iterations
and at least Ω/parenleftig/parenleftig
n+/radicalig
nLw
µ/parenrightig
log1
ϵ/parenrightig
stochastic gradients of g. Similarly, to minimize h, we require at least
Ω/parenleftig/parenleftig
n+/radicalig
nLβ
µ/parenrightig
log1
ϵ/parenrightig
stochastic gradients of h. Therefore, Theorem 6 is established.
5This is for the strongly convex case; one can do convex similarly.
6We have lifted their construction to the infinite-dimensional space for the sake of simplicity. One can get a similar finite-
dimensional results.
53Published in Transactions on Machine Learning Research (May/2023)
B.9 Proof of Theorem 8
Taking the stochastic gradient step followed by the proximal step with respect to ψ, both with stepsize η, is
equivalent to (Hanzely et al., 2020b):
w.p.pw:

w+=w−η/parenleftigg
1
pw/parenleftigg
1
MM/summationdisplay
m=1∇wfm,j(w,βm)−1
MM/summationdisplay
m=1∇wfm,j(w′,β′
m)/parenrightigg
+∇wF(w′,β′)),
β+
m=βm−η
M∇fm(w′,β′
m)
w.p.pβ:

w+=w−η∇wF(w′,β′),
β+
m=βm−η
M/parenleftig
1
pβ(∇βfm,j(w,βm)−∇βfm,j(w′,β′
m)) +∇βfm(w′,β′
m)/parenrightig
.(55)
Letx= [w,β 1,...,βM],x′= [w′,β′
1,...,β′
M]. The update rule (55) can be rewritten as
x+=x−η(g(x)−g(x′) +∇F(x′)),
whereg(x)corresponds to the described unbiased stochastic gradient obtained by subsampling both the
space and the finite sum simultaneously. To give the rate of the aforementioned method, we shall determine
the expected smoothness constant. To achieve that, we introduce the following two lemmas.
Lemma 15. Suppose that Assumptions 1 and 2 hold. Then
E∥(g(x)−g(x′) +∇F(x′))−∇F(x)∥2≤2LDF(x,y),
whereL:= 2 max/parenleftig
Lw
pw,Lβ
pβ/parenrightig
.
Proof.Letdβ:=/summationtextm
m=1dm. We have:
E∥(g(x)−g(x′) +∇F(x′))−∇F(x)∥2
≤E∥g(x)−g(x′)∥2
=pwE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublep−1
w1
MM/summationdisplay
m=1(∇wfm,j(w,βm)−∇wfm,j(w′,β′
m))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
|ζ= 1
+pβ1
M2M/summationdisplay
m=1E∥p−1
β∇fm,j(w,βm)−p−1
β∇fm,j(w′,β′
m)∥2|ζ= 2
=p−1
wE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
MM/summationdisplay
m=1(∇wfm,j(w,βm)−∇wfm,j(w′,β′
m))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
|ζ= 1
+p−1
β1
M2M/summationdisplay
m=1E∥∇fm,j(w,βm)−∇fm,j(w′,β′
m)∥2|ζ= 2
=E(Fj(x)−∇Fj(x′))⊤/parenleftigg
p−1
wId0×d0 0
0p−1
βIdβ×dβ/parenrightigg
(Fj(x)−∇Fj(x′))
(∗)
≤E4 max/parenleftbiggLw
pw,Lβ
pβ/parenrightbigg
DFj(x,x′)
= 4 max/parenleftbiggLw
pw,Lβ
pβ/parenrightbigg
DF(x,x′),
where (∗)holds due to the (Lw,Lβ)-smoothness of Fj(from Assumption 2) and Lemma 16.
54Published in Transactions on Machine Learning Research (May/2023)
Lemma 16. LetH(x,y) :Rdx+dy→Rbe a (jointly) convex function such that
∇2
xH(x,y)≤LxIand∇2
yH(x,y)≤LyI.
Then
∇2H(x,y)≤2/parenleftigg
LxI 0
0LyI/parenrightigg
(56)
and
DH((x,y),(x′y′))
≥1
2(∇H(x,y)−∇H(x′,y′))⊤/parenleftigg
1
2L−1
xI 0
01
2L−1
yI/parenrightigg
(∇H(x,y)−∇H(x′,y′)).(57)
Proof.To show (56), observe that
2/parenleftigg
LxI 0
0LyI/parenrightigg
−∇2H(x,y) =/parenleftigg
2LxI−∇2
x,xH(x,y)−∇2
x,yH(x,y)
−∇2
y,xH(x,y) 2LyI−∇2
y,yH(x,y)/parenrightigg
⪰/parenleftigg
∇2
x,xH(x,y)−∇2
x,yH(x,y)
−∇2
y,xH(x,y)∇2
y,yH(x,y)/parenrightigg
⪰∇2
x,xH(x,−y)
⪰0.
Finally, we note that (57) is a direct consequence of (56) and joint convexity of H.
We are now ready to state the convergence rate of ASVRCD-PFL.
Theorem 9. Iteration complexity of Algorithm 3 with
η=1
4L, θ 2=1
2, γ =1
max{2µ,4θ1/η},
ν= 1−γµ,andθ1= min/braceleftigg
1
2,/radicaligg
ηµmax/braceleftbigg1
2,θ2
ρ/bracerightbigg/bracerightigg
is
O

1
ρ+/radicaltp/radicalvertex/radicalvertex/radicalbtmax/parenleftig
Lw
pw,Lβ
pβ/parenrightig
ρµ
log1
ϵ
.
Settingpw=Lw
Lβ+Lwyields the complexity
O

1
ρ+/radicaligg
Lw+Lβ
ρµ
log1
ϵ
.
Proof.The proof follows from Lemma 15 and Theorem 4.1 of Hanzely et al. (2020b), thus is omitted.
Overall, the algorithm requires
O

1
ρ+/radicaligg
Lw+Lβ
ρµ
/parenleftbigg
log1
ϵ/parenrightbigg
(ρn+pw)

55Published in Transactions on Machine Learning Research (May/2023)
communication rounds and the same number of gradient calls w.r.t. parameter w. Settingρ=pw
n, we have

1
ρ+/radicaligg
Lw+Lβ
ρµ
/parenleftbigg
log1
ϵ/parenrightbigg
(ρn+pw) = 2
1
ρ+/radicaligg
Lw+Lβ
ρµ
/parenleftbigg
log1
ϵ/parenrightbigg
ρn
= 2
n+/radicaligg
ρn2(Lw+Lβ)
µ
/parenleftbigg
log1
ϵ/parenrightbigg
= 2/parenleftigg
n+/radicaligg
nLw
µ/parenrightigg/parenleftbigg
log1
ϵ/parenrightbigg
,
which shows that Algorithm 3 enjoys both communication complexity and the global gradient complexity of
orderO/parenleftig/parenleftig
n+/radicalig
nLw
µ/parenrightig
log1
ϵ/parenrightig
. Analogously, setting ρ=pβ
nyields personalized/local gradient complexity of
orderO/parenleftig/parenleftig
n+/radicalig
nLβ
µ/parenrightig
log1
ϵ/parenrightig
.
56