Published in Transactions on Machine Learning Research (06/2023)
Provably Convergent Policy Optimization via Metric-aware
Trust Region Methods
Jun Song juns113@uw.edu
Department of Industrial and Systems Engineering
University of Washington
Niao He niao.he@inf.ethz.ch
Department of Computer Science
ETH Zürich
Lijun Ding lding47@wisc.edu
Wisconsin Institute for Discovery
University of Wisconsin - Madison
Chaoyue Zhao cyzhao@uw.edu
Department of Industrial and Systems Engineering
University of Washington
Reviewed on OpenReview: https: // openreview. net/ forum? id= jkTqJJOGMS
Abstract
Trust-region methods based on Kullback-Leibler divergence are pervasively used to stabilize
policy optimization in reinforcement learning. In this paper, we exploit more flexible
metrics and examine two natural extensions of policy optimization with Wasserstein and
Sinkhorn trust regions, namely Wasserstein policy optimization (WPO) andSinkhorn policy
optimization (SPO) . Instead of restricting the policy to a parametric distribution class,
we directly optimize the policy distribution and derive their closed-form policy updates
based on the Lagrangian duality. Theoretically, we show that WPO guarantees a monotonic
performance improvement, and SPO provably converges to WPO as the entropic regularizer
diminishes. Moreover, we prove that with a decaying Lagrangian multiplier to the trust region
constraint, both methods converge to global optimality. Experiments across tabular domains,
robotic locomotion, and continuous control tasks further demonstrate the performance
improvement of both approaches, more robustness of WPO to sample insufficiency, and faster
convergence of SPO, over state-of-art policy gradient methods.
1 Introduction
Policy-based reinforcement learning (RL) approaches have received remarkable success in many domains,
including video games (Wang et al., 2017; Wu et al., 2017; Mnih et al., 2016), robotics (Grudic et al., 2003;
Levine et al., 2016), and continuous control tasks (Duan et al., 2016; Schulman et al., 2016; Heess et al.,
2015). One prominent example is policy gradient method (Grudic et al., 2003; Peters & Schaal, 2006; Lillicrap
et al., 2016; Sutton et al., 1999; Williams, 1992; Mnih et al., 2016; Silver et al., 2014). The core idea is to
represent the policy with a probability distribution πθ(a|s) =P[a|s;θ], such that the action ain statesis
chosen stochastically following the policy πθcontrolled by parameter θ. Determining the right step size to
update the policy is crucial for maintaining the stability of policy gradient methods: too conservative choice
of stepsizes result in slow convergence, while too large stepsizes may lead to catastrophically bad updates.
To control the size of policy updates, Kullback-Leibler (KL) divergence is commonly adopted to measure the
difference between two policies. For example, the seminal work on trust region policy optimization (TRPO)
1Published in Transactions on Machine Learning Research (06/2023)
by Schulman et al. (2015) introduced KL divergence based constraints (trust region constraints) to restrict
the size of the policy update; see also Peng et al. (2019); Abdolmaleki et al. (2018). Kakade (2001) and
Schulman et al. (2017) introduced a KL-based penalty term to the objective to prevent excessive policy shift.
Though KL-based policy optimization has achieved promising results, it remains interesting whether using
other metrics to gauge the similarity between policies could bring additional benefits. Recently, a few
work (Richemond & Maginnis, 2017; Zhang et al., 2018; Moskovitz et al., 2021; Pacchiano et al., 2020) has
explored the Wasserstein metric to restrict the deviation between consecutive policies. Compared with KL
divergence, the Wasserstein metric has several desirable properties. Firstly, it is a true symmetric distance
measure. Secondly, it allows flexible user-defined costs between actions and is less sensitive to ill-posed
likelihood ratios. Thirdly but most importantly, the Wasserstein metric takes into account the geometry of
the metric space (Panaretos & Zemel, 2019) and allows distributions to have different or even non-overlapping
supports.
Motivating Example : Below we provide an example of a grid world (see Figure 1) that illustrates the
advantages of using the Wasserstein metric over the KL divergence to construct trust regions and policy
updates. The grid world consists of 5regular grids and 2goal grids, and there are three possible actions: left,
right, and pickup. The player always starts from the middle grid, and making a left or right move results in a
reward of−1. Picking up yields a reward of −3at regular grids, +5at the blue goal grid, and +10at the red
goal grid. An episode terminates either at the maximum length of 10or immediately after picking up. We
define the geometric distance between left and right actions to be 1, and 4between other actions.
Figure 1: Motivating grid world example
Figure 2 shows the Wasserstein distance and KL divergence for different policy shifts of this grid world
example. We can see that Wasserstein metric utilizes the geometric distance between actions to distinguish
the shift of policy distribution to a close action (policy distribution 1 − →2 in Figure 2a) from the shift to a
far action (policy distribution 1 − →3 in Figure 2b), while KL divergence does not. Figure 3 demonstrates the
constrained policy updates based on Wasserstein distance and KL divergence respectively with a fixed trust
region size 1. We can see that Wasserstein-based policy update finds the optimal policy faster than KL-based
policy update. This is because KL distance is larger than Waserstein when considering policy shifts of close
actions (see Figure 2a). Therefore, Wasserstein policy update is able to shift action (from left to right) in
multiple states, while KL update is only allowed to shift action in a single state. Besides, KL policy update
keeps using a suboptimal short-sighted solution between the 2nd and 4th iteration, which further slows down
the convergence.
(a) Policy shift of close action
 (b) Policy shift of far action
Figure 2: Wasserstein utilizes geometric feature of action space
2Published in Transactions on Machine Learning Research (06/2023)
Figure 3: Demonstration of policy updates under different trust regions
However, the challenge of applying the Wasserstein metric for policy optimization is also evident: evaluating
the Wasserstein distance requires solving an optimal transport problem, which could be computationally
expensive. To avoid this computation hurdle, existing work resorts to different techniques to approximate
the policy update under Wasserstein regularization. For example, Richemond & Maginnis (2017) solved the
resulting RL problem using Fokker-Planck equations; Zhang et al. (2018) introduced particle approximation
method to estimate the Wasserstein gradient flow. Recently, Moskovitz et al. (2021) instead considered
the second-order Taylor expansion of Wasserstein distance based on Wasserstein information matrix to
characterize the local behavioral structure of policies. Pacchiano et al. (2020) tackled behavior-guided policy
optimization with smooth Wasserstein regularization by solving an approximate dual reformulation defined
on reproducing kernel Hilbert spaces. Aside from such approximation, some of these work also limits the
policy representation to a particular parametric distribution class, As indicated in Tessler et al. (2019), since
parametric distributions are not convex in the distribution space, optimizing over such distributions results
in local movements in the action space and thus leads to convergence to a sub-optimal solution. Until now,
the theoretical performance of policy optimization under the Wasserstein metric remains elusive in light of
these approximation errors.
In this paper, we study policy optimization with trust regions based on Wasserstein distance and Sinkhorn
divergence. The latter is a smooth variant of Waserstein distance by imposing an entropic regularization
to the optimal transport problem (Cuturi, 2013). We call them, Wasserstein Policy Optimization (WPO)
andSinkhorn Policy Optimization (SPO) , respectively. Instead of confining the distribution of policy to a
particular distribution class, we work on the space of policy distribution directly, and consider all admissible
policies that are within the trust regions with the goal of avoiding approximation errors. Unlike existing work,
we focus on exact characterization of the policy updates. We would like to emphasize that our methodology
and theoretical analysis in Section 3, 4, and 5 primarily concentrate on a discrete action space. However, we
also present an extension of our method to accommodate a continuous action space, detailed in Section 7.5.
We highlight our contributions as follows:
1.Algorithms: We develop closed-form expressions of the policy updates for both WPO and SPO based
on the corresponding optimal Lagrangian multipliers of the trust region constraints. To the best of our
knowledge, this is the first explicit closed-form updates for policy optimization based on Wasserstein and
Sinkhorn trust regions. In particular, the optimal Lagrangian multiplier of SPO admits a simple form and
can be computed efficiently. A practical on-policy actor-critic algorithm is proposed based on the derived
expressions of policy updates and advantage value function estimation.
2.Theory: We theoretically show that WPO guarantees a monotonic performance improvement through
the iterations, even with non-optimal Lagrangian multipliers . We also prove that SPO converges to WPO
as the entropic regularizer diminishes. Moreover, we prove that with a decaying schedule of the multiplier,
3Published in Transactions on Machine Learning Research (06/2023)
SPO and WPO converge to global optimality , and with a constant multiplier, both methods converge
linearlyup to a neighborhood of the optimal value. To our best knowledge, this appears to be the first
convergence rate analysis of policy optimization based on Wasserstein-type metrics.
3.Experiments: We provide comprehensive evaluation on the efficiency of WPO and SPO under several
types of testing environments including tabular domains, robotic locomotion tasks, and further extend it
to continuous control tasks. Compared to state-of-art policy gradients approaches that use KL divergence
such as TRPO and PPO and those use Wasserstein metric such as Wasserstein Natural Policy Gradient
(WNPG) (Moskovitz et al., 2021) and Behavior Guided Policy Gradients (BGPG) (Pacchiano et al.,
2020), our methods achieve better sample efficiency, faster convergence, and improved final performance.
Numerical study indicates that by properly choosing the weight of entropic regularizer, SPO achieves a
better trade-off between convergence and final performance than WPO.
Related work: Wasserstein-like metrics have been explored in a number of works in the context reinforcement
learning. Ferns et al. (2004) first introduced bisimulation metrics based on Wasserstein distance to quantify
behavioral similarity between states for the purpose of state aggregation. Such bisimulation metrics were
recently utilized for representation learning of RL; see e.g., Castro (2020); Agarwal et al. (2021b). In addition,
a few recent work has also exploited Wasserstein distance for imitation learning (see e.g., Xiao et al. (2019);
Dadashi et al. (2021)) and unsupervised RL (see e.g., He et al. (2022)). Our work is closely related to several
previous studies, including Richemond & Maginnis (2017); Zhang et al. (2018); Moskovitz et al. (2021);
Pacchiano et al. (2020), which also utilize Wasserstein distance to measure the proximity of policies. However,
unlike the aforementioned studies that solely employ Wasserstein distance as an explicit penalty function,
we additionally utilize it as a trust region constraint. Moreover, we consider nonparametric policies and
derive explicit policy update forms, whereas these studies update parametric policies using policy gradients.
Furthermore, we demonstrate monotonic performance improvement and global convergence with our policy
update, which is not provided in these previous works. Regarding the use of Sinkhorn divergence in RL,
Pacchiano et al. (2020)is the only related work to our best knowledge, where the entropy regularization is
used to mitigate the computational burden of computing Wasserstein metric. However, no explicit form of
policy update is provided in this work, while we derive an explicit Sinkhorn policy update and demonstrate
its advantage in convergence speed. Additionally, we use Wasserstein distance to directly measure the
proximity of nonparametric policies in the distribution space, while Pacchiano et al. (2020); Moskovitz et al.
(2021)measure the similarity of parametric policies in the behavioral space.
Wasserstein-like metrics are also pervasively studied in distributionally robust optimization (DRO); see e.g.,
Esfahani & Kuhn (2018); Gao & Kleywegt (2016); Zhao & Guan (2018); Blanchet & Murthy (2019). We also
point out that a recent concurrent work by Wang et al. (2021a) studied DRO using the Sinkhorn distance.
Our duality formulations are largely inspired from existing work in DRO. However, we note that constrained
policy optimization is conceptually different from DRO. Constrained policy optimization focuses on finding
the optimistic policy that falls in a trust region, whereas DRO (e.g., the KL DRO) aims to optimize some
worst-case loss given by the adversarial distribution of unknown parameters within some ambiguity set.
2 Background and Notations
Markov Decision Process (MDP): We consider an infinite-horizon discounted MDP, defined by the
tuple (S,A,P,r,υ,γ ), whereSis the state space, Ais the action space, P:S×A×S− → Ris the transition
probability, r:S×A− → Ris the reward function, υ:S− →Ris the distribution of the initial state s0, and
γ∈(0,1)is the discount factor. We define the return of timestep tas the accumulated discounted reward
fromt,Rt=/summationtext∞
k=0γkr(st+k,at+k), and the value function as Vπ(s) =E[Rt|st=s;π]. The performance of a
stochastic policy πis defined as J(π) =Es0,a0,s1...[/summationtext∞
t=0γtr(st,at)]whereat∼π(at|st),st+1∼P(st+1|st,at).
As shown in Kakade & Langford (2002), the expected return of a new policy π′can be expressed in terms of
the advantage over the old policy π:J(π′) =J(π) +Es∼ρπ′
υ,a∼π′[Aπ(s,a)], whereAπ(s,a) =E[Rt|st=s,at=
a;π]−E[Rt|st=s;π]represents the advantage function and ρπ
υrepresents the unnormalized discounted
visitation frequencies with initial state distribution υ, i.e.,ρπ
υ(s) =Es0∼υ[/summationtext∞
t=0γtP(st=s|s0)].
4Published in Transactions on Machine Learning Research (06/2023)
Trust Region Policy Optimization (TRPO): In TRPO (Schulman et al., 2015), the policy πis
parameterized as πθwith parameter vector θ. For notation brevity, we use θto represent the policy πθ.
Then, the new policy θ′is found in each iteration to maximize the expected improvement J(π′)−J(π), or
equivalently, the expected value of the advantage function:
max
θ′Es∼ρθυ,a∼θ′[Aθ(s,a)]
s.t.Es∼ρθυ[dKL(θ′,θ)]≤δ,(1)
wheredKLrepresents the KL divergence and δis the threshold of the distance between new and old policies.
Wasserstein Distance: Given two probability distributions of policies πandπ′on the discrete action
spaceA={a1,a2,...,aN}, the Wasserstein distance between the policies is defined as:
dW(π′,π) = inf
Q∈Π(π′,π)⟨Q,D⟩, (2)
where⟨·,·⟩denotes the Frobenius inner product. The infimum is taken over all joint distributions Qwith
marginalsπ′andπ, andDis the cost matrix with Dij=d(ai,aj), whered(ai,aj)is defined as the distance
between actions aiandaj. Its largest entry in magnitude is denoted by ∥D∥∞. In implementation, our choice
of distance dis task-dependent and is reported in Table 3 in Appendix A.
Sinkhorn Divergence: Sinkhorn divergence (Cuturi, 2013) provides a smooth approximation of the
Wasserstein distance by adding an entropic regularizer. The Sinkhorn divergence is defined as:
dS(π′,π|λ) = inf
Q∈Π(π′,π)/braceleftbigg
⟨Q,D⟩−1
λh(Q)/bracerightbigg
, (3)
whereh(Q) =−/summationtextN
i=1/summationtextN
j=1QijlogQijrepresents the entropy term, and λ>0is a regularization parameter.
The intuition of adding the entropic regularization is: since most elements of the optimal joint distribution
Qwill be 0with a high probability, by trading the sparsity with entropy, a smoother and denser coupling
between distributions can be achieved (Courty et al., 2014; 2016). Therefore, when the weight of the
entropic regularization decreases (i.e., λincreases), the sparsity of the divergence increases, and the Sinkhorn
divergence converges to the Wasserstein metric, i.e., limλ→∞dS(π′,π|λ) =dW(π′,π). More critically, Sinkhorn
divergence is useful to mitigate the computational burden of computing Wasserstein distance. In fact, the
efficiency improvement that Sinkhorn divergence and the related algorithms brought paves the way to utilize
Wasserstein-like metrics in many machine learning domains, including online learning (Cesa-Bianchi & Lugosi,
2006), model selection (Juditsky et al., 2008; Rigollet & Tsybakov, 2011), generative modeling (Genevay
et al., 2018; Petzka et al., 2017; Patrini et al., 2019), dimensionality reduction (Huang et al., 2021; Lin et al.,
2020; Wang et al., 2021b).
3 Wasserstein Policy Optimization
Motivated by TRPO, here we consider a trust region based on the Wasserstein metric. Moreover, we lift the
restrictive assumption that a policy has to follow a parametric distribution class by allowing all admissible
policies. Then, the new policy π′is found in each iteration to maximize the estimated expected value of the
advantage function. Therefore, the Wasserstein Policy Optimization (WPO) framework is shown as follows:
max
π′∈DEs∼ρπυ,a∼π′(·|s)[Aπ(s,a)]
whereD={π′|Es∼ρπυ[dW(π′(·|s),π(·|s))]≤δ},(4)
where the Wasserstein distance dW(·,·)is defined in (2).
In most practical cases, the reward ris bounded and correspondingly, the accumulated discounted reward Rt
is bounded. So without loss of generality, we make the following assumption:
Assumption 1. AssumeAπ(s,a)is bounded, i.e., supa∈A,s∈S|Aπ(s,a)|≤Amaxfor someAmax>0.
5Published in Transactions on Machine Learning Research (06/2023)
With Wasserstein metric based trust region constraint, we are able to derive the closed-form of the policy
update shown in Theorem 1. The main idea is to form the Lagrangian dual of the constrained optimization
problem presented above, which is inspired by the way to obtain the extremal distribution in Wasserstein
DRO literature, see e.g., Kuhn et al. (2019); Blanchet & Murthy (2019); Zhao & Guan (2018). The detailed
proof can be found in Appendix B.
Theorem 1. (Closed-form policy update) Letκπ
s(β,j) =argmaxk=1...N{Aπ(s,ak)−βDkj}, whereD
denotes the cost matrix. If Assumption 1 holds, then an optimal solution to (4) is:
π∗(ai|s) =/summationdisplayN
j=1π(aj|s)f∗
s(i,j), (5)
wheref∗
s(i,j) = 1ifi=κπ
s(β∗,j)andf∗
s(i,j) = 0otherwise, and β∗is an optimal Lagrangian multiplier
corresponds to the following dual formulation:
min
β≥0F(β) = min
β≥0{βδ+Es∼ρπυ/summationdisplayN
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)}. (6)
Moreover, we have β∗≤¯β, where ¯β:= maxs∈S,k,j=1...N,k̸=j(Dkj)−1(Aπ(s,ak)−Aπ(s,aj)).
Remark 1. For ease of notation and simplicity, we assume the uniqueness of κπ
s(β,j)in order to form the
simple expression of f∗
sin Theorem 1. When it is not unique, a necessary condition for the optimality of π∗in
(5) is/summationtext
i∈Kπs(β,j)f∗
s(i,j) = 1, andf∗
s(i,j) = 0fori /∈Kπ
s(β,j), whereKπ
s(β,j) =argmaxk=1...NAπ(s,ak)−
βDkj. The weight f∗
s(i,j)fori∈Kπ
s(β,j)could be determined through linear programming (see details in
(17) in Appendix B).
The exact policy update for WPO in (5) requires computing the optimal Lagrangian multiplier β∗by solving
the one-dimensional subproblem (6). A closed-form of β∗is not easy to obtain in general, except for special
cases of the distance d(x,y)or cost matrix D. In Appendix C, we provide the closed-form of β∗for the case
whend(x,y) = 0ifx=yand1otherwise.
WPO Policy Update: Based on Theorem 1, we introduce the following WPO policy updating rule:
πk+1(ai|s) =FWPO(πk) :=N/summationdisplay
j=1πk(aj|s)fk
s(i,j), (WPO)
wherefk
s(i,j) = 1ifi=κπks(βk,j)and0otherwise. Note that different from (5), we allow βkto be chosen
arbitrarily and time dependently. We show that such policy update always leads to a monotonic improvement
of the performance even when βkis not the optimal Lagrangian multiplier. In particular, we propose two
strategies to update multiplier βk:
(i)Approximation of optimal βk: To improve the convergence, we can approximately solve the optimal
Lagrangian multiplier based on Sinkhorn divergence. More details in Section 4.
(ii)Time-dependent βk: To improve the computational efficiency, we can simply treat βkas a time-
dependent parameter, e.g., we can set βkas a diminishing sequence. In this setting, (WPO) produces
the solution to the following penalty version of problem (4) (with d=dW):
max
πk+1Es∼ρπkυ,a∼πk+1(·|s)[Aπk(s,a)]−βkEs∼ρπkυ[d(πk+1(·|s),πk(·|s))]. (7)
4 Sinkhorn Policy Optimization
In this section, we introduce Sinkhorn policy optimization (SPO) by constructing trust region with Sinkhorn
divergence. In the following theorem, we derive the optimal policy update in each step when using Sinkhorn
divergence based trust region. Detailed proofs are provided in Appendix D.
6Published in Transactions on Machine Learning Research (06/2023)
Theorem 2. If Assumption 1 holds, then the optimal solution to (4) with Sinkhorn divergence is:
π∗
λ(ai|s) =N/summationdisplay
j=1π(aj|s)f∗
s,λ(i,j), (8)
whereDdenotes the cost matrix, f∗
s,λ(i,j) =exp (λ
β∗
λAπ(s,ai)−λDij)
/summationtextN
k=1exp (λ
β∗
λAπ(s,ak)−λDkj)andβ∗
λis an optimal solution to the
following dual formulation:
min
β≥0Fλ(β) = min
β≥0/braceleftig
βδ−Es∼ρπυN/summationdisplay
j=1π(aj|s)(β
λ+β
λln(π(aj|s))−β
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)])
Es∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1β
λexp (λ
βAπ(s,ai)−λDij)·π(aj|s)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)/bracerightig
.(9)
Moreover, we have β∗
λ≤2Amax
δ.
In contrast to the Wasserstein dual formulation (6), the objective in the Sinkhorn dual formulation (9) is
differentiable in βand admits closed-form gradients (shown in Appendix F). With this gradient information,
we can use gradient-based global optimization algorithms (Wales & Doye, 1998; Zhan et al., 2006; Leary,
2000) to find a global optimal solution β∗
λto (9).
Next, we show that if the entropic regularization parameter λis large enough, then the optimal solution β∗
λis
a close approximation to the β∗of Wasserstein dual formulation. Proof is provided in Appendix G.
Theorem 3. DefineβUB= max{2Amax
δ,¯β}. We have:
1.Fλ(β)converges to F(β)uniformly on [0,βUB]:lim
λ− →∞sup
0≤β≤βUB/vextendsingle/vextendsingle/vextendsingleFλ(β)−F(β)/vextendsingle/vextendsingle/vextendsingle≤lim
λ− →∞βUB
λNlnN= 0.
2.lim
λ− →∞argmin0≤β≤βUBFλ(β)⊆argmin0≤β≤βUBF(β).
Although it is difficult to obtain the exact value of the optimal solution β∗to the Wasserstein dual formulation
(6), the above theorem suggests that we can approximate β∗viaβ∗
λby setting up a relative large λ. In
practice, we can also adopt a smooth homotopy approach by setting an increasing sequence λkfor each
iteration and letting λk→∞.
SPO Policy Update: Based on Theorem 2, we introduce the following SPO policy updating rule:
πk+1(ai|s) =FSPO(πk) =N/summationdisplay
j=1πk(aj|s)fk
s,λk(i,j). (SPO)
Herefk
s,λk(i,j) =exp (λk
βkAπk(s,ai)−λkDij)/summationtextN
l=1exp (λk
βkAπk(s,al)−λkDlj),λk≥0andβk≥0are some control parameters. The
parameter βkcan be either computed via solving the one-dimensional subproblem (9) or simply set as a
diminishing sequence. The proper setup of λkcan effectively adjust the trade-off between convergence speed
and final performance. More details are provided in the ablation study in Section 7.
5 Theoretical Analysis
We first show that SPO policy update converges to WPO policy update as the regularization parameter
increases (i.e., λ− →∞). The detailed proof is provided in Appendix H.
Lemma 1. Asλk− →∞, SPO update converges to WPO update: limλk− →∞FSPO(πk)∈FWPO(πk).
7Published in Transactions on Machine Learning Research (06/2023)
We then provide a theoretical justification that WPO policy update (and SPO with λ− →∞) are always
guaranteed to improve the true performance Jmonotonically if we have access to the true advantage function.
If the advantage function can only be evaluated inexactly with limited samples, then an extra estimation
error (measured by the largest absolute entry ∥·∥∞) will occur. Proof can be found in Appendix I.
Theorem 4. (Performance improvement) For any initial state distribution υand anyβk≥0, if
||ˆAπ−Aπ||∞≤ϵfor someϵ>0, let ˆKπks(βk,j) =argmaxi=1,...,N{ˆAπk(s,ai)−βkDij}, WPO policy update
(and SPO with λ− →∞) guarantee the following performance improvement bound when the inaccurate
advantage function ˆAπis used,
J(πk+1)≥J(πk) +βkEs∼ρπk+1
υN/summationdisplay
j=1πk(aj|s)/summationdisplay
i∈ˆKπks(βk,j)fk
s(i,j)Dij−2ϵ
1−γ. (10)
The value of ϵ, which quantifies the approximation error of the advantage function, is dependent on various
factors such as the advantage estimation algorithm used and the number of samples (Schulman et al., 2016).
It is worth noting that the improvement bound of NPG/TRPO (Cen et al., 2021)includes the same additional
term−2ϵ
1−γ, which indicates that our methods offer comparable theoretical performance guarantees to KL
based updates. In the following, we show that with a decreasing schedule of the multiplier βk, both WPO
and SPO policy updates have their values J(πk)converging to the optimal J⋆=maxπJ(π)on the tabular
domain. To start, for k-th iteration, we consider (WPO) and (SPO) (with arbitrary λ>0) whose updates
πk+1are optimal solutions to (7) with dbeingdWanddSrespectively.
Assumption 2. The state space and the action space are both finite, the reward function ris non-negative,
and the initial distribution covers all state.
Note that once state and action spaces are both finite, the reward can be assumed non-negative without
loss of generality, as we can always add maxs,a|r(s,a)|to the reward function without changing the optimal
policy and the order of the policies. Defining the optimal value function V⋆(s) =maxπE[Rt|st=s], we have
the following theorem, whose proof is in Appendix J and is inspired by Bhandari & Russo (2021).
Theorem 5. (Global convergence) Under Assumption 2, we have for any βk≥0, (WPO) satisfies that
∥V⋆−Vπk+1∥∞≤γ∥V⋆−Vπk∥∞+βk∥D∥∞, (11)
and (SPO) satisfies that
∥V⋆−Vπk+1∥∞≤γ∥V⋆−Vπk∥∞+ 2βk
1−γ/parenleftbigg
∥D∥∞+ 2logN
λ/parenrightbigg
. (12)
Iflimk→∞βk= 0, we further have limk→∞J(πk) =J⋆.
Remark 2. Note the convergence is geometric . If we keep βkas a constant, then 0≤J⋆−J(πT)≤
∥V⋆−VπT∥∞≤γT∥V⋆−Vπ0∥∞+βB
1−γ, whereB=∥D∥∞for (WPO) and B= 2∥D∥∞+2logN
λ
1−γfor (SPO).
To achieve an ϵoptimality gap, we only need to take β=(1−γ)ϵ
2Band letT≥log(ϵ/2)
γ.
Remark 3. The study of global non-asymptotic convergence of nonconvex policy optimization algorithms
has been an active research topic. Recent theoretical work has mostly centered on PG and natural policy
gradient (NPG) Kakade (2001) - a close relative of TRPO; see e.g., Agarwal et al. (2021a); Cen et al. (2021);
Lan (2022). To our best knowledge, a few work has discussed the global convergence of TRPO. Neu et al.
(2017) and Geist et al. (2019) established the connection of TRPO to Mirror Descent, but did not provide
any non-asymptotic rate; Shani et al. (2020) showed that adaptive TRPO with decaying stepsize achieved
O(1/√
T)convergence rate for unregularized MDPs in the tabular setting (finite state and finite action).
Our result seems to be the first non-asymptotic analysis of policy optimization based on Wasserstein and
Sinkhorn divergence. It remains interesting to extend the convergence theory of TRPO/WPO/SPO to function
approximation regime following recent advance Agarwal et al. (2021a). However, this is beyond the scope of
our current work, as we focus on explicit closed-form update of WPO/SPO, which can be a viable alternative
to TRPO in practice.
8Published in Transactions on Machine Learning Research (06/2023)
6 A Practical Algorithm
In practice, the advantage value functions are often estimated from sampled trajectories. In this section, we
provide a practical on-policy actor-critic algorithm, described in Algorithm 1, that combines WPO/SPO with
advantage function estimation.
At each iteration, the first step is to collect trajectories, which can be either complete or partial. If the
trajectory is complete, the total return can be directly expressed as the accumulated discounted rewards
Rt=/summationtextT−t−1
k=0γkrt+k. If the trajectory is partial, it can be estimated by applying the multi-step temporal
difference (TD) methods (De Asis et al., 2017): ˆRt:t+n=/summationtextn−1
k=0γkrt+k+γnV(st+n). Then for the advantage
estimation, we can use Monte Carlo advantage estimation, i.e., ˆAπk
t=Rt−Vψk(st)or Generalized Advantage
Estimation (GAE) (Schulman et al., 2016), which provides a more explicit control over the bias-variance trade-
off. In the value update step, we use a neural net to represent the value function, where ψis the parameter
that specifies the value net s→V(s). Then, we can update ψby using gradient descent, which significantly
reduces the computational burden of computing advantage directly. The computational complexity of the
algorithm is discussed in Appendix K.
Algorithm 1: On-policy WPO/SPO algorithm
Input: number of iterations K, learning rate α
Initialize policy π0and value network Vψ0with random parameter ψ0
fork= 0,1,2...K do
Collect trajectory set Dkon policyπk
For each timestep tin each trajectory, compute total returns Gtand estimate advantages ˆAπk
t
Update value:
ψk+1← −ψk−α∇ψk/summationdisplay
(Gt−Vψk(st))2
Update policy:
πk+1← −F(πk)via WPO/ SPO with ˆAπk
t
end
7 Experiments
In this section, we evaluate the proposed WPO and SPO approaches presented in Algorithm 1. We compare
the performance of our methods with benchmarks including TRPO (Schulman et al., 2015), PPO (Schulman
et al., 2017), A2C (Mnih et al., 2016); and with BGPG (Pacchiano et al., 2020), WNPG (Moskovitz et al.,
2021) for continuous control. The code of our WPO/SPO can be found here1. We adopt the implementations
of TRPO, PPO and A2C from OpenAI Baselines (Dhariwal et al., 2017) for MuJuCo tasks and Stable
Baselines (Hill et al., 2018) for other tasks. For BGPG, we adopt the same implementation2as (Pacchiano
et al., 2020).
Our experiments include (1) ablation study that focuses on sensitivity analysis of WPO and SPO; (2) tabular
domain tasks with discrete state and action including the Taxi, Chain, and Cliff Walking environments; (3)
locomotion tasks with continuous state and discrete action including the CartPole, Acrobot environments;
(4) comparison of KL and Wasserstein trust regions under tabular domain and locomotion tasks; and (5)
extension to continuous control tasks with continuous action including HalfCheetah, Hopper, Walker, and
Ant environments from MuJuCo. See Table 4 in Appendix A for a summary of performance. The setting of
hyperparameters and network sizes of our algorithms and additional results are provided in Appendix A.
7.1 Ablation Study
In this experiment, we first examine the sensitivity of WPO in terms of different strategies of βk. We test
four settings of βvalue for WPO policy update: (1) Setting 1: Computing optimal βvalue for all policy
update; (2) Setting 2: Computing optimal βvalue for first 20%of policy updates and decaying βfor the
1https://github.com/efficientwpo/EfficientWPO
2https://github.com/behaviorguidedRL/BGRL
9Published in Transactions on Machine Learning Research (06/2023)
remaining; (3) Setting 3: Computing optimal βvalue for first 20%of policy updates and fix βas its last
updated value for the remaining; (4) Setting 4: Decaying βfor all policy updates (e.g., βk= Θ(1/logk)). In
particular, Setting 2 is rooted in the observation that β∗decays slowly in the later stage of the experiments
carried out in the paper. Small perturbations are added to the approximate values to avoid any stagnation in
updating. Taxi task (Dietterich, 1998) from tabular domain is selected for this experiment.
Table 1: Runtime for different βsettings, average across 5runs with random initialization
Runtime Taxi (s) CartPole (s)
Setting 1 (optimal β) 1224.3 ±105.7 129.7±15.2
Setting 2 (optimal-then-decay) 648.4 ±55.7 63.2±8.3
Setting 3 (optimal-then-fix) 630.2 ±67.4 67.1±9.7
Setting 4 (decaying β) 522.7 ±49.5 44.3±6.2
The performance comparisons and average run times are shown in Figure 4 and Table 1 respectively. Figure
4a and Table 1 clearly indicate a tradeoff between computation efficiency and accuracy in terms of different
choices ofβvalue. Setting 2 is the most effective way to balance the tradeoff between performance and run
time. For the rest of experiments, we adopt this setting for both WPO and SPO (see Appendix A.2 for how
Setting 2 is tuned for each task). Figure 4b shows that as λincreases, the convergence of SPO becomes
slower but the final performance of SPO improves and becomes closer to that of WPO, which verifies the
convergence property of Sinkhorn to Wasserstein distance shown in Theorem 3. Therefore, the choice of λ
can effectively adjust the trade-off between convergence and final performance. Similar results are observed
when using time-varying λon Taxi, Chain and CartPole tasks, presented in Figure 9 in Appendix A.
(a) Different settings of β
 (b) Different choices of λ
Figure 4: Episode rewards for Taxi with different βandλsettings, averaged across 5runs with random
initialization. The shaded area depicts the mean ±the standard deviation.
7.2 Tabular Domains
We evaluate WPO and SPO on tabular domain tasks and test the exploration ability of the algorithms on
several environments including Taxi, Chain, and Cliff Walking. We use a table of size |S|×|A| to represent
the policyπ(a|s). For the value function, we use a neural net to smoothly update the values. The performance
of WPO and SPO are compared to the performance of TRPO, PPO and A2C under the same neural net
structure. Results on Taxi, Cliff and Chain are reported in Figure 5.
As shown in Figure 5, the performances of WPO, SPO and TRPO are manifestly better than A2C and PPO.
Among the trust region based methods, WPO and SPO outperform TRPO in Taxi and Cliff Walking, whereas
in Chain, the performances of these three methods are comparable. In all of the test cases, SPO converges
faster than WPO but to a lower optimum. As further shown in Table 2, for the Taxi environment, WPO has
a higher successful drop-off rate and a lower task completion time while the original TRPO reaches the time
limit with a drop-off rate 0, suggesting that WPO finds a better policy than the original TRPO. In Figure 7,
10Published in Transactions on Machine Learning Research (06/2023)
we also compare the performance of WPO under Wasserstein and KL divergences given different number of
samplesNAused to estimate the advantage function, and the result suggests that using Wasserstein metric is
more robust than KL divergence under inaccurate advantage values.
Figure 5: Episode rewards during training for tabular domain tasks, averaged across 5runs with random
initialization. The shaded area depicts the mean ±the standard deviation.
Table 2: Trained agents performance on Taxi
Success (+20) Fail (-10) Steps (-1) Return
WPO 0.753 0.232 70.891 -58.151
TRPO 0 0 200 -200
7.3 Robotic Locomotion Tasks
We now integrate deep neural network architecture into WPO and SPO and evaluate their performance on
several locomotion tasks (with continuous state and discrete action), including CartPole (Barto et al., 1983)
and Acrobot (Geramifard et al., 2015). We use two separate neural nets to represent the policy and the
value. The policy neural net receives state sas an input and outputs the categorical distribution of π(a|s). A
random subset of states Sk∈Sis sampled at each iteration to perform policy updates.
Figure 6 shows that WPO and SPO outperform TRPO, PPO and A2C in most tasks in terms of final
performance, except in Acrobot where PPO performs the best. In most cases, SPO converges faster but
WPO has a better final performance. To train 105timesteps in the discrete locomotion tasks, the training
wall-clock time is 63.2±8.2sfor WPO, 64.7±7.8sfor SPO, 59.4±10.2sfor TRPO and 69.9±10.5sfor PPO.
Therefore, WPO has a similar computational efficiency as TRPO and PPO.
Figure 6: Episode rewards during the training process for the locomotion tasks, averaged across 5runs with
random initialization. The shaded area depicts the mean ±the standard deviation.
7.4 Comparison of Wasserstein and KL Trust Regions
We show that compared with the KL divergence, the utilization of Wasserstein metric can cope with the
inaccurate advantage estimations caused by the lack of samples. Let NAdenote the number of samples used
11Published in Transactions on Machine Learning Research (06/2023)
to estimate the advantage function. We evaluate the performance of WPO framework (4) with Wasserstein
and KL constraints (as derived in Peng et al. (2019)). We consider the Chain task and different NA. As
shown in Figure 7, when NAis1000, KL performs slightly better than WPO. However, when NAdecreases to
100or250, WPO outperforms KL. These results indicate that WPO is more robust than KL under inaccurate
advantage values. This finding is consistent with our observations on the policy update formulations of
Wasserstein and KL. For the Wasserstein update in (5), policy will be updated only when the advantage
difference between two actions is significant, i.e., Aπ(s,aj)−βDij≥Aπ(s,ai). However, for the KL update
in Peng et al. (2019), policy will be updated as long as the current advantage function has a single non-zero
value. Therefore, KL update is more sensitive; while Wasserstein update is more robust and more tolerant to
advantage inaccuracies. Similar results are obtained for the locomotion tasks (Figure 10 in Appendix A).
The runtime of Wasserstein and KL updates are reported in Table 5 in Appendix A.
(a)NA= 100
 (b)NA= 250
 (c)NA= 1000
Figure 7: Episode rewards during training for the Chain task, where advantage value function is estimated
under different number of samples, averaged across 5 runs with random initialization. The shaded area
depicts the mean ±the standard deviation.
7.5 Extension to Continuous Control
To extend to environments with continuous action, we use Implicit Quantile Networks (IQN) (Will Dabney &
Munos, 2018) actor that can represent an arbitrary complex non-parametric policy. Let F−1
s(p)represent the
quantile function associated with policy π(·|s). The IQN actor takes state sand probability p∈[0,1]as input,
and outputs the corresponding quantile value a=F−1
s(p). IQN actor can be trained to approach pre-defined
target policy distributions through quantile regression (Will Dabney & Munos, 2018; Tessler et al., 2019).
Define the action support for state sink-th iteration as Iπk(s) ={a′:Aπk(s,a′)>mina∈Iπk−1(s)Aπk(s,a)}.
Then, the WPO/SPO target policy distribution to guide IQN update in the k-th iteration is:
PIπk(s)(a′|s) =/summationdisplay
a∈Iπk−1(s)πk(a|s)fs(a′,a), (13)
where for WPO update fs(a′,a) = 1ifa′=argmaxa′∈Iπk(s){Aπk(s,a′)−βkd(a′,a)}andfs(a′,a) = 0
otherwise; for SPO update, fs(a′,a) =exp(λk
βkAπk(s,a′)−λkd(a′,a))/summationtext
a′∈Iπk(s)exp(λk
βkAπk(s,a′)−λkd(a′,a)). In implementation, we sample a
batch of statesSk∈Sat each iteration to perform policy updates, and for each s∈Sk, we sample|Ak|
actions to approximate the support Iπk(s)and the target policy distribution PIπk(s)(·|s).
We additionally compare WPO and SPO with BGPG (Pacchiano et al., 2020) and WNPG (Moskovitz et al.,
2021) that are specially designed to address the continuous control with Wasserstein metric, for several
MuJuCo tasks including HalfCheetah, Hopper, Walker, and Ant. Figure 8 shows that WPO and SPO have
consistently better performances than other benchmarks. Similar results are obtained for the challenging
Humanoid task, presented in Figure 11 in Appendix A. We also provide the runtime of each algorithm in
Table 6 in Appendix A.
12Published in Transactions on Machine Learning Research (06/2023)
Figure 8: Episode rewards during training for MuJuCo continuous control tasks, averaged across 10runs
with random initialization. The shaded area depicts the mean ±the standard deviation.
8 Conclusion
In this paper, we present two policy optimization frameworks, WPO and SPO, which can exactly characterize
the policy updates instead of confining their distributions to particular distribution class or requiring any
approximation. Our methods outperform TRPO and PPO with better sample efficiency, faster convergence,
and improved final performance. Our numerical results show that the Wasserstein metric is more robust to
the ambiguity of advantage functions, compared with the KL divergence. Our strategy for adjusting βvalue
for WPO can reduce the computational time and boost the convergence without noticeable performance
degradation. SPO improves the convergence speed of WPO by properly choosing the weight of the entropic
regularizer. Performance improvement and global convergence for WPO are discussed. For future work, it
remains interesting to extend the idea to PPO and natural policy gradients, which penalize the policy update
instead of imposing trust region constraint, and extend it to off-policy frameworks.
13Published in Transactions on Machine Learning Research (06/2023)
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin
Riedmiller. Maximum a posteriori policy optimisation. ArXiv Preprint , pp. arXiv:1806.06920, 2018.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient
methods: Optimality, approximation, and distribution shift. J. Mach. Learn. Res. , 22(98):1–76, 2021a.
Rishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive behavioral
similarity embeddings for generalization in reinforcement learning. In Proceedings of the 9th International
Conference on Learning Representations , 2021b.
Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuronlike adaptive elements that can solve
difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics , SMC-13(5):
834–846, 1983.
Jalaj Bhandari and Daniel Russo. On the linear convergence of policy gradient methods for finite mdps. In
International Conference on Artificial Intelligence and Statistics , pp. 2386–2394. PMLR, 2021.
Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. Mathematics
of Operations Research , 44(2):565–600, 2019.
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic Markov decision
processes. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence , volume 34, pp.
10069–10076, 2020.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural
policy gradient methods with entropy regularization. Operations Research , 2021.
Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games . Cambridge University Press, 2006.
Nicolas Courty, Rémi Flamary, and Devis Tuia. Domain adaptation with regularized optimal transport. In
Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pp. 274–289.
Springer, 2014.
Nicolas Courty, Rémi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain
adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(9):1853–1865, 2016.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural
Information Processing Systems , volume 26, pp. 2292–2300, 2013.
Robert Dadashi, Léonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal Wasserstein imitation
learning. In Proceedings of the 9th International Conference on Learning Representations , 2021.
Kristopher De Asis, J. Fernando Hernandez-Garcia, G. Zacharias Holland, and Richard S. Sutton. Multi-step
reinforcement learning: A unifying algorithm. ArXiv Preprint , pp. arXiv:1703.01327, 2017.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John
Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI baselines. https://github.com/
openai/baselines , 2017.
Thomas G. Dietterich. The MAXQ method for hierarchical reinforcement learning. In Proceedings of the
15th International Conference on Machine Learning , pp. 118–126, 1998.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement
learning for continuous control. In Proceedings of the 33rd International Conference on Machine Learning ,
pp. 1329–1338, 2016.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the
Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming , 171
(1):115–166, 2018.
14Published in Transactions on Machine Learning Research (06/2023)
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite Markov decision processes. In
Uncertainty in Artificial Intelligence , volume 4, pp. 162–169, 2004.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with Wasserstein distance.
arXiv preprint arXiv:1604.02199 , 2016.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In
International Conference on Machine Learning , pp. 2160–2169. PMLR, 2019.
Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with Sinkhorn divergences. In
International Conference on Artificial Intelligence and Statistics , pp. 1608–1617, 2018.
Alborz Geramifard, Christoph Dann, Robert H. Klein, William Dabney, and Jonathan P. How. RLPy: A
value-function-based reinforcement learning framework for education and research. Journal of Machine
Learning Research , 16(46):1573–1578, 2015.
Gregory Z. Grudic, Vijay Kumar, and Lyle H. Ungar. Using policy gradient reinforcement learning on
autonomous robot controllers. In Proceedings of the 2003 IEEE/RSJ International Conference on Intelligent
Robots and Systems , pp. 406–411, 2003.
Shuncheng He, Yuhang Jiang, Hongchang Zhang, Jianzhun Shao, and Xiangyang Ji. Wasserstein Unsupervised
Reinforcement Learning. In Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence ,
2022.
Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning
continuous control policies by stochastic value gradients. In Advances in Neural Information Processing
Systems, volume 28, 2015.
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla
Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman,
Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.com/hill-a/stable-baselines , 2018.
Minhui Huang, Shiqian Ma, and Lifeng Lai. A Riemannian block coordinate descent method for computing
the projection robust Wasserstein distance. In Proceedings of the 38th International Conference on Machine
Learning , pp. 4446–4455, 2021.
Anatoli Juditsky, Philippe Rigollet, and Alexandre B Tsybakov. Learning by mirror averaging. The Annals
of Statistics , 36(5):2183–2206, 2008.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings
of the 19th International Conference on Machine Learning , pp. 267–274, 2002.
Sham M Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems , volume 14,
2001.
Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh. Wasserstein
distributionally robust optimization: Theory and applications in machine learning. In Operations Research
& Management Science in the Age of Analytics , pp. 130–166. INFORMS, 2019.
Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity,
and generalized problem classes. Mathematical programming , pp. 1–48, 2022.
Robert Leary. Global optimization on funneling landscapes. Journal of Global Optimization , 18:367–383,
2000.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor
policies. Journal of Machine Learning Research , 17(39):1–40, 2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of the 4th
International Conference on Learning Representations , 2016.
15Published in Transactions on Machine Learning Research (06/2023)
Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, and Michael I Jordan. Projection robust Wasserstein
distance and Riemannian optimization. ArXiv Preprint , pp. arXiv:2006.07458, 2020.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In
Proceedings of the 33rd International Conference on Machine Learning , pp. 1928–1937, 2016.
Ted Moskovitz, Michael Arbel, Ferenc Huszar, and Arthur Gretton. Efficient Wasserstein natural gradients
for reinforcement learning. In Proceedings of the 9th International Conference on Learning Representations ,
2021.
Gergely Neu, Anders Jonsson, and Vicenç Gómez. A unified view of entropy-regularized markov decision
processes. arXiv preprint arXiv:1705.07798 , 2017.
Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Krzysztof Choromanski, Anna Choromanska, and Michael
Jordan. Learning to score behaviors for guided policy optimization. In Proceedings of the 37th International
Conference on Machine Learning , pp. 7445–7454, 2020.
Victor M. Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual Review of Statistics
and Its Application , 6(1):405–431, 2019.
Giorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max Welling, Tim
Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Uncertainty in Artificial Intelligence , pp. 733–743,
2019.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and
scalable off-policy reinforcement learning. ArXiv Preprint , pp. arXiv:1910.00177, 2019.
Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In Proceedings of the 2006 IEEE/RSJ
International Conference on Intelligent Robots and Systems , pp. 2219–2225, 2006.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of Wasserstein GANs. ArXiv
Preprint, pp. arXiv:1709.08894, 2017.
Pierre H. Richemond and Brendan Maginnis. On Wasserstein reinforcement learning and the Fokker-Planck
equation. ArXiv Preprint , pp. arXiv:1712.07185, 2017.
Philippe Rigollet and Alexandre Tsybakov. Exponential screening and optimal rates of sparse estimation.
The Annals of Statistics , 39(2):731–771, 2011.
R. Tyrrell Rockafellar and Roger J. B. Wets. Variational Analysis . Springer, 1998.
Johannes O. Royset. Approximations and solution estimates in optimization. Mathematical Programming ,
170:479–506, 2018.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy
optimization. In Proceedings of the 32nd International Conference on Machine Learning , pp. 1889–1897,
2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional
continuouscontrolusinggeneralizedadvantageestimation. In Proceedings of the 4th International Conference
on Learning Representations , 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. ArXiv Preprint , pp. arXiv:1707.06347, 2017.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global convergence
and faster rates for regularized mdps. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial
Intelligence , volume 34, pp. 5668–5675, 2020.
16Published in Transactions on Machine Learning Research (06/2023)
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic
policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning , pp.
387–395, 2014.
Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics , 8(1):171–176, 1958.
Student. The probable error of a mean. Biometrika , 6(1):1–25, 1908.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. In Advances in Neural Information Processing Systems ,
pp. 1057–1063, 1999.
Chen Tessler, Guy Tennenholtz, and Shie Mannor. Distributional policy optimization: An alternative
approach for continuous control. In Advances in Neural Information Processing Systems , pp. 1350–1360,
2019.
David Wales and Jonathan Doye. Global optimization by basin-hopping and the lowest energy structures
of Lennard-Jones clusters containing up to 110 atoms. The Journal of Physical Chemistry A , 101(28):
5111–5116, 1998.
Jie Wang, Rui Gao, and Yao Xie. Sinkhorn distributionally robust optimization. ArXiv Preprint , pp.
arXiv:2109.11926, 2021a.
Jie Wang, Rui Gao, and Yao Xie. Two-sample test using projected Wasserstein distance. In Proceedings of
IEEE International Symposium on Information Theory , volume 21, 2021b.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando
de Freitas. Sample efficient actor-critic with experience replay. In Proceedings of the 5th International
Conference on Learning Representations , 2017.
David Silver Will Dabney, Georg Ostrovski and Rémi Munos. Implicit quantile networks for distributional
reinforcement learning. In International Conference on Machine Learning , 2018.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine Learning , 8(3–4):229–256, 1992.
Yuhuai Wu, Elman Mansimov, Shun Liao, Roger Grosse, and Jimmy Ba. Scalable trust-region method for
deep reinforcement learning using Kronecker-factored approximation. In Advances in Neural Information
Processing Systems , pp. 5285–5294, 2017.
Huang Xiao, Michael Herman, Joerg Wagner, Sebastian Ziesche, Jalal Etesami, and Thai Hong Linh.
Wasserstein adversarial imitation learning. ArXiv Preprint , pp. arXiv:1906.08113, 2019.
Lixin Zhan, Jeff Chen, and Wing-Ki Liu. Monte Carlo basin paving: An improved global optimization
method. Physical Review E , 73:015701, 2006.
Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as Wasserstein gradient
flows. In Proceedings of the 35th International Conference on Machine Learning , pp. 5737–5746, 2018.
Chaoyue Zhao and Yongpei Guan. Data-driven risk-averse stochastic optimization with Wasserstein metric.
Operations Research Letters , 46(2):262–267, 2018.
17Published in Transactions on Machine Learning Research (06/2023)
A Implementation Details and Additional Results
The implementation of our WPO/SPO can be found in https://github.com/efficientwpo/EfficientWPO. We
use the implementations of TRPO, PPO and A2C from OpenAI Baselines (Dhariwal et al., 2017) for MuJuCo
tasks and Stable Baselines (Hill et al., 2018) for other tasks. For BGPG, we adopt the same implementation
as Pacchinao et al., (2020) based on the released code https://github.com/behaviorguidedRL/BGRL.
A.1 Visitation Frequencies Estimation:
The unnormalized discounted visitation frequencies are needed to compute the global optimal β∗. At the
k-th iteration, the visitation frequencies ρπ
kare estimated using samples of the trajectory set Dk. Specifically,
we first initialize ρπ
k(s) = 0,∀s∈S. Then for each timestep tin each trajectory from Dk, we update ρπ
kas
ρπ
k(st)← −ρπ
k(st) +γt/|Dk|.
A.2 Optimal-then-decay Beta Strategy:
During the training of multiple tasks, including Taxi, Chain and CartPole, we observe a consistent trend
in the behavior of the optimal βvalue during the policy updates: It initially fluctuates, then stabilizes and
decays slowly towards 0. In the Taxi task, the optimal βstabilizes after approximately 18%of the total
training iterations. If we decay βbefore this stabilization point (e.g, using optimal beta for only first 5%
or10%updates), we observe a drop in performance. However, we do not observe any notable performance
difference when we decay βafter this stabilization point (e.g., using optimal βfor first 20%or30%updates).
We also observe that the optimal βdecays at a very slow rate, and Θ(1/log(k))matches this trend best. If
we employ a faster decaying function, such as Θ(1/k)orΘ(1/k2), we observe a drop in performance.
Based on these findings, when implementing the optimal-then-decay βstrategy on other tasks, we compute
the optimal βfor each policy update until we observe that its value stabilizes across updates. At this point,
we stop calculating the optimal βand decay it using Θ(1/log(k))for the remaining policy updates. The
specific iteration at which the optimal βvalue stabilizes varies across tasks, and we denote this point as kβ,
which is reported in Table 3.
A.3 Hyperparameters and Performance Summary
Our main experimental results are reported in section 7. In addition, we provide the setting of hyperparameters
and network sizes of our WPO/SPO algorithms in Table 3, and a summary of performance in Table 4.
Table 3: Hyperparameters and network sizes
Taxi-v3 NChain-v0 CartPole-v1 Acrobot-v1 MuJuCo tasks
CliffWalking-v0
γ 0.9 0.9 0.95 0 .95 0 .99
lrπ \ \ 10−25×10−310−4
lrvalue 10−210−210−25×10−310−3
|Dk| 60(Taxi) 1(Chain) 2 3 partial
3(CliffWalking)
πsize 2D array 2D array [64,64] [64 ,64] [400 ,300]
Q/v size [10,7,5] [10 ,7,5] [64 ,64] [64 ,64] [400 ,300]
|Sk|all states,|S|all states,|S| 128 128 64
|Ak|all actions,|A|all actions,|A|all actions,|A|all actions,|A| 32
18Published in Transactions on Machine Learning Research (06/2023)
d(a,a′)0-1 distance30-1 distance 0-1 distance 0-1 distance L1 distance
kβ 250 100 (Chain) 150 150 1000
50(CliffWalking)
Table 4: Averaged rewards over last 10% episodes during the training process
Environment WPO SPO TRPO PPO A2C BGPG WNPG
Taxi-v3 −45±27−87±11−202±3−381±34−338±30- -
NChain-v0 3549±197 3432±131 3522±258 3506±237 1606±10 - -
CliffWalking-v0 −35±15−25±1−159±94−3290±2106−5587±1942- -
CartPole-v1 388±54 370±30 297±65 193±45 267±61 - -
Acrobot-v1 −162±8−185±15−248±33−103±5−379±39- -
HalfCheetah-v2 2050±108 1750±172 1158±35 1628±136−645±31 1697±195 1832±125
Hopper-v2 3208±259 2834±305 2035±248 2321±233 43±21 1982±218 2361±272
Walker2d-v2 3739±298 3489±257 2535±369 3290±354 28±1 2775±301 3059±209
Ant-v2 1863±271 1780±257 21±10 1487±206−39±8 1622±235 1587±221
Humanoid-v2 965±76 914±93 725±112 632±73 107±15 797±85 820±91
A.4 Additional Results for Ablation Studies
Figure 9: Episode rewards during the training process for different βandλsettings, averaged across 5runs
with a random initialization. The shaded area depicts the mean ±the standard deviation.
3We note that specifying distance based on control relevance leads to higher performance in this test case: i.e., d= 1to
distinct actions from set A={move north, move south, move west, move east },d= 1to distinct actions from set B={pickup,
dropoff}, andd= 4to actions from different sets.
19Published in Transactions on Machine Learning Research (06/2023)
A.5 Additional Comparison of Wasserstein and KL Trust Regions
(a)NA= 100
 (b)NA= 500
 (c)NA= 100
 (d)NA= 500
Figure 10: Episode rewards during the training process for the locomotion tasks, averaged across 5runs with
a random initialization. The shaded area depicts the mean ±the standard deviation.
Table 5: Average runtime (seconds) of WPO, SPO and KL
WPO SPO KL
Taxi-v3 (per 103steps) 71.0±7.3 69.5±8.7 74.3±9.5
NChain-v0 (per 103steps) 58.4±9.1 63.1±7.4 59.9±8.7
CartPole-v1 (per 106steps) 11.4±1.8 10.2±2.3 9.7±1.9
Acrobot-v1 (per 105steps) 10.4±1.9 9.7±2.5 10.9±2.3
Humanoid-v2 (per 105steps) 422.7±65.4 409.1±46.5 438.5±61.2
A.6 Additional Results for Large-scale Continuous Control
Figure 11: Episode rewards during training for MuJuCo Humanoid task, averaged across 10 runs with random
initialization. The shaded area depicts the mean ±the standard deviation.
Table 6: Average runtime (seconds per 105timesteps) for the MuJuCo continuous control tasks
Environment WPO SPO TRPO PPO A2C BGPG WNPG
HalfCheetah-v2 297±31 289±25 290±28 292±36 293±27 306±33 298±22
Hopper-v2 233±38 226±42 242±56 167±36 254±49 201±32 197±31
Walker2d-v2 289±55 312±61 253±39 307±52 259±46 322±62 214±45
Ant-v2 307±51 290±57 296±63 251±47 291±41 286±63 269±54
Humanoid-v2 423±65 401±47 446±52 395±57 230±31 425±58 398±49
20Published in Transactions on Machine Learning Research (06/2023)
B Proof of Theorem 1
Theorem 1. (Closed-form policy update) Letκπ
s(β,j) =argmaxk=1...N{Aπ(s,ak)−βDkj}, whereD
denotes the cost matrix. If Assumption 1 holds, then an optimal solution to (4) is:
π∗(ai|s) =/summationdisplayN
j=1π(aj|s)f∗
s(i,j), (5)
wheref∗
s(i,j) = 1ifi=κπ
s(β∗,j)andf∗
s(i,j) = 0otherwise, and β∗is an optimal Lagrangian multiplier
corresponds to the following dual formulation:
min
β≥0F(β) = min
β≥0{βδ+Es∼ρπυ/summationdisplayN
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)}. (6)
Moreover, we have β∗≤¯β, where ¯β:= maxs∈S,k,j=1...N,k̸=j(Dkj)−1(Aπ(s,ak)−Aπ(s,aj)).
Proof of Theorem 1. First, we denote Qsas the joint distribution of π(·|s)andπ′(·|s)with/summationtextN
i=1Qs
ij=π(aj|s)
and/summationtextN
j=1Qs
ij=π′(ai|s). Also, letfs(i,j)represent the conditional distribution of π′(ai|s)underπ(aj|s).
ThenQs
ij=π(aj|s)fs(i,j),π′(ai|s) =/summationtextN
j=1Qs
ij=/summationtextN
j=1π(aj|s)fs(i,j). In addition:
dW(π′(·|s),π(·|s)) = min
Qs
ijN/summationdisplay
i=1N/summationdisplay
j=1DijQs
ij= min
fs(i,j)N/summationdisplay
i=1N/summationdisplay
j=1Dijπ(aj|s)fs(i,j),and
Ea∼π′(·|s)[Aπ(s,a)] =N/summationdisplay
i=1Aπ(s,ai)π′(ai|s) =N/summationdisplay
i=1N/summationdisplay
j=1Aπ(s,ai)π(aj|s)fs(i,j).
Thus, the WPO problem in (4) can be reformulated as:
max
fs(i,j)≥0Es∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1Aπ(s,ai)π(aj|s)fs(i,j) (14a)
s.t.Es∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1Dijπ(aj|s)fs(i,j)≤δ, (14b)
N/summationdisplay
i=1fs(i,j) = 1,∀s∈S,j= 1...N. (14c)
Note here that (14b) is equivalent to Es∼ρπυminfs(i,j)/summationtextN
i=1/summationtextN
j=1Dijπ(aj|s)fs(i,j)≤δbecause if we have a
feasiblefs(i,j)to make (14b) hold, we must have Es∼ρπυminfs(i,j)/summationtextN
i=1/summationtextN
j=1Dijπ(aj|s)fs(i,j)≤δ.
Since both the objective function and the constraint are linear in fs(i,j), (14) is a convex optimization
problem. Also, Slater’s condition holds for (14) as the feasible region has an interior point, which is fs(i,i) = 1
∀i, andfs(i,j) = 0∀i̸=j. Meanwhile, since Aπ(s,a)is bounded based on Assumption 1, the objective is
bounded above. Therefore, strong duality holds for (14). At this point we can derive the dual problem of (14)
as its equivalent reformulation:
min
β≥0,ζs
jβδ+/integraldisplay
s∈SN/summationdisplay
j=1ζs
jds
s.t. Aπ(s,ai)π(aj|s)−βDijπ(aj|s)−ζs
j
ρπυ(s)≤0,∀s∈S,i,j= 1...N.(15)
We observe that with a fixed β, the optimal ζs
jwill be achieved at:
ζs∗
j(β) = max
i=1...Nρπ
υ(s)π(aj|s)(Aπ(s,ai)−βDij). (16)
21Published in Transactions on Machine Learning Research (06/2023)
Denoteβ∗as an optimal solution to (15) and f∗
s(i,j)as an optimal solution to (14). Due to the complimentary
slackness, the following equations hold:
(Aπ(s,ai)π(aj|s)−β∗Dijπ(aj|s)−ζs∗
j(β∗)
ρπυ(s))f∗
s(i,j) = 0,∀s,i,j.
In this case, f∗
s(i,j)can have non-zero values only when Aπ(s,ai)π(aj|s)−β∗Dijπ(aj|s)−ζs∗
j(β∗)
ρπυ(s)= 0, which
meansζs∗
j(β∗) =ρπ
υ(s)π(aj|s)(Aπ(s,ai)−β∗Dij). Given the expression of the optimal ζs∗
jin (16),f∗
s(i,j)
can have non-zero values only when i∈Kπ
s(β∗,j), whereKπ
s(β,j) =argmaxk=1...NAπ(s,ak)−βDkj.
When there exists a unique optimizer, i.e., |Kπ
s(β∗,j)|= 1, letκπ
s(β∗,j)denote the optimizer. Since/summationtextN
i=1f∗
s(i,j) = 1as indicated in (14c), the only optimal solution is:
f∗
s(i,j) =/braceleftigg
1ifi=κπ
s(β∗,j),
0otherwise.
When there exists multiple optimizers, i.e., |Kπ
s(β∗,j)|>1, the optimal weights f∗
s(i,j)fori∈Kπ
s(β∗,j)
could be determined by solving the following linear programming:
max
f∗s(i,j)≥0,i∈Kπs(β∗,j)Es∼ρπυN/summationdisplay
j=1π(aj|s)/summationdisplay
i∈Kπs(β∗,j)Aπ(s,ai)f∗
s(i,j)
s.t.Es∼ρπυN/summationdisplay
j=1π(aj|s)/summationdisplay
i∈Kπs(β∗,j)Dijf∗
s(i,j)≤δ,
/summationdisplay
i∈Kπs(β∗,j)f∗
s(i,j) = 1,∀s∈S,j= 1...N.(17)
And then the corresponding optimal solution is, π∗(ai|s) =/summationtextN
j=1π(aj|s)f∗
s(i,j).
Last, by substituting ζs∗
j(β) =ρπ
υ(s)π(aj|s)maxi=1...N(Aπ(s,ai)−βDij)into the dual problem (15), we can
reformulate (15) into:
min
β≥0{βδ+/integraldisplay
s∈SN/summationdisplay
j=1ζs∗
j(β)ds}= min
β≥0{βδ+Es∼ρπυN/summationdisplay
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)}.(18)
The optimal βcan then be obtained by solving (18).
We will further show that β∗≤¯β:= maxs∈S,k,j=1...N,k̸=j(Dkj)−1(Aπ(s,ak)−Aπ(s,aj)).
In the general case, i.e., β≥0, (14a) is non-negative because:
Es∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1Aπ(s,ai)π(aj|s)f∗
s(i,j) (19a)
=Es∼ρπυN/summationdisplay
j=1π(aj|s)N/summationdisplay
i=1Aπ(s,ai)f∗
s(i,j) (19b)
=Es∼ρπυN/summationdisplay
j=1π(aj|s)/summationdisplay
i∈Kπs(β∗,j)f∗
s(i,j)Aπ(s,ai) (19c)
≥Es∼ρπυN/summationdisplay
j=1π(aj|s)/summationdisplay
i∈Kπs(β∗,j)f∗
s(i,j)[Aπ(s,aj) +β∗Dij] (19d)
22Published in Transactions on Machine Learning Research (06/2023)
=Es∼ρπυN/summationdisplay
j=1π(aj|s)Aπ(s,aj) +Es∼ρπυN/summationdisplay
j=1π(aj|s)/summationdisplay
i∈Kπs(β∗,j)f∗
s(i,j)β∗Dij (19e)
=Es∼ρπυN/summationdisplay
j=1π(aj|s)β∗/summationdisplay
i∈Kπs(β∗,j)f∗
s(i,j)Dij (19f)
≥0, (19g)
where (19d) holds since for i∈Kπ
s(β∗,j),Aπ(s,ai)−β∗Dij≥Aπ(s,aj)−β∗Djj=Aπ(s,aj). Whenβ∗>
maxs∈S,k,j=1...N,k̸=j{Aπ(s,ak)−Aπ(s,aj)
Dkj}, we have that for all s∈S,κπ
s(β∗,j) =j. Thus,f∗
s(i,i) = 1,∀iand
f∗
s(i,j) = 0,∀i̸=j. The objective value (14a) will be 0because Es∼ρπυ/summationtextN
i=1/summationtextN
j=1Aπ(s,ai)π(aj|s)f∗
s(i,j) =
Es∼ρπυ/summationtextN
i=1Aπ(s,ai)π(ai|s) = 0. The left hand side of (14b) equals to Es∼ρπυ/summationtextN
i=1/summationtextN
j=1Dijπ(aj|s)f∗
s(i,j) =
Es∼ρπυ/summationtextN
i=1Diiπ(ai|s) = 0. Thus, for any δ>0, (14b) is always satisfied.
Since the objective of the primal Wasserstein trust-region constrained problem in (6) constantly
evaluates to 0whenβ∗>maxs∈S,k,j=1...N,k̸=j{Aπ(s,ak)−Aπ(s,aj)
Dkj}, and is non-negative when β∗≤
maxs∈S,k,j=1...N,k̸=j{Aπ(s,ak)−Aπ(s,aj)
Dkj}, wecanuse maxs∈S,k,j=1...N,k̸=j{Aπ(s,ak)−Aπ(s,aj)
Dkj}asanupperbound
for the optimal dual variable β∗.
C Optimal Beta for a Special Distance
Proposition 1. Letks=argmaxi=1,...,NAπ(s,ai), we have:
(1). If the initial point β0is in [maxs,j{Aπ(s,aks)−Aπ(s,aj)},+∞), the local optimal βsolution is
maxs,j{Aπ(s,aks)−Aπ(s,aj)}.
(2). If the initial point β0is in [0,mins,j̸=ks{Aπ(s,aks)−Aπ(s,aj)}]: ifδ−/integraltext
s∈Sρπ(s)(1−π(aks|s))ds< 0,
the local optimal βismins,j̸=ks{Aπ(s,aks)−Aπ(s,aj)}; otherwise, the local optimal βsolution is 0.
(3). If the initial point β0is in (mins,j̸=ks{Aπ(s,aks)−Aπ(s,aj)},maxs,j{Aπ(s,aks)−Aπ(s,aj)}), we
construct sets I1
sandI2
sas:
fors∈S,j∈{1,2...N}:ifβ0≥Aπ(s,aks)−Aπ(s,aj)thenAddjtoI1
selseAddjtoI2
s. Then, if
δ−Es∼ρπ/summationtext
j∈I2sπ(aj|s)<0, the local optimal βismins∈S,j∈I2s{Aπ(s,aks)−Aπ(s,aj)}; otherwise, the local
optimalβismaxs∈S,j∈I1s{Aπ(s,aks)−Aπ(s,aj)}.
Proof of Proposition 1. (1). When β∈[maxs,j{Aπ(s,aks)−Aπ(s,aj)},+∞), we have Aπ(s,aj)≥
Aπ(s,aks)−βfor alls∈S,j= 1...N. SinceAπ(s,aks)−β≥Aπ(s,ak)−βfor allk= 1...N, we have
Aπ(s,aj)≥Aπ(s,ak)−βfor alls∈S,j= 1...N,k= 1...N. Thus,j∈argmaxk=1...N{Aπ(s,ak)−βDkj},
for alls∈S,j= 1...N. Therefore, (6) can be reformulated as:
min
β≥0{βδ+Es∼ρπυN/summationdisplay
j=1π(aj|s)Aπ(s,aj)}.
Sinceδ≥0, we have the local optimal β= maxs,j{Aπ(s,aks)−Aπ(s,aj)}.
(2). When β∈[0,mins,j̸=ks{Aπ(s,aks)−Aπ(s,aj)}], we haveAπ(s,aj)≤Aπ(s,aks)−βfor alls∈S,
j= 1...N,j̸=ks. Thusks∈argmaxk=1...N{Aπ(s,ak)−βDkj}for alls∈S,j= 1...N. The inner part of
23Published in Transactions on Machine Learning Research (06/2023)
(6) then is:
βδ+Es∼ρπυ{N/summationdisplay
j=1,j̸=ksπ(aj|s)(Aπ(s,aks)−β) +π(aks|s)Aπ(s,aks)}
=β(δ−Es∼ρπυN/summationdisplay
j=1,j̸=ksπ(aj|s)) +Es∼ρπυN/summationdisplay
j=1π(aj|s)Aπ(s,aks)
=β(δ−/integraldisplay
s∈Sρπ
υ(s)(1−π(aks|s))ds) +Es∼ρπυN/summationdisplay
j=1π(aj|s)Aπ(s,aks).
Ifδ−/integraltext
s∈Sρπ
υ(s)(1−π(aks|s))ds< 0, we have the local optimal β=mins,j̸=ks{Aπ(s,aks)−Aπ(s,aj)}. If
δ−/integraltext
s∈Sρπ
υ(s)(1−π(aks|s))ds≥0, we have the local optimal β= 0.
(3). For an initial point β0in(mins,j̸=ks{Aπ(s,aks)−Aπ(s,aj)},maxs,j{Aπ(s,aks)−Aπ(s,aj)}), we construct
partitionsI1
sandI2
sof the set{1,2...N}in the way described in Proposition 1 for all s∈S. Considerβin
the neighborhood of β0, i.e.,β≥Aπ(s,aks)−Aπ(s,aj)fors∈S,j∈I1
sandβ≤Aπ(s,aks)−Aπ(s,aj)for
s∈S,j∈I2
s. Then the inner part of (6) can be reformulated as:
βδ+Es∼ρπυ{/summationdisplay
j∈I1sπ(aj|s)Aπ(s,aj) +/summationdisplay
j∈I2sπ(aj|s)(Aπ(s,aks)−β)}
=β(δ−Es∼ρπυ/summationdisplay
j∈I2sπ(aj|s)) +Es∼ρπυ{/summationdisplay
j∈I1sπ(aj|s)Aπ(s,aj) +/summationdisplay
j∈I2sπ(aj|s)Aπ(s,aks)}.
Ifδ−Es∼ρπυ/summationtext
j∈I2sπ(aj|s)<0, we have the local optimal β=mins∈S,j∈I2s{Aπ(s,aks)−Aπ(s,aj)}. If
δ−Es∼ρπυ/summationtext
j∈I2sπ(aj|s)≥0, we have the local optimal β= maxs∈S,j∈I1s{Aπ(s,aks)−Aπ(s,aj)}.
D Proof of Theorem 2
Theorem 2. If Assumption 1 holds, then the optimal solution to (4) with Sinkhorn divergence is:
π∗
λ(ai|s) =N/summationdisplay
j=1π(aj|s)f∗
s,λ(i,j), (8)
whereDdenotes the cost matrix, f∗
s,λ(i,j) =exp (λ
β∗
λAπ(s,ai)−λDij)
/summationtextN
k=1exp (λ
β∗
λAπ(s,ak)−λDkj)andβ∗
λis an optimal solution to the
following dual formulation:
min
β≥0Fλ(β) = min
β≥0/braceleftig
βδ−Es∼ρπυN/summationdisplay
j=1π(aj|s)(β
λ+β
λln(π(aj|s))−β
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)])
Es∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1β
λexp (λ
βAπ(s,ai)−λDij)·π(aj|s)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)/bracerightig
.(9)
Moreover, we have β∗
λ≤2Amax
δ.
Proof of Theorem 2. Invoking the definition of Sinkhorn divergence in (3), the trust region constrained
problem with Sinkhorn divergence can be reformulated as:
max
QEs∼ρπυ[/summationdisplayN
i=1Aπ(s,ai)/summationdisplayN
j=1Qs
ij] (20a)
s.t.Es∼ρπυ[/summationdisplayN
i=1/summationdisplayN
j=1DijQs
ij+1
λQs
ijlogQs
ij]≤δ (20b)
24Published in Transactions on Machine Learning Research (06/2023)
/summationdisplayN
i=1Qs
ij=π(aj|s),∀j= 1,...,N,s∈S. (20c)
Letβandωrepresent the dual variables of constraints (20b) and (20c) respectively, then the Lagrangian
duality of (20) can be derived as:
max
Qmin
β≥0,ωL(Q,β,ω ) = max
Qmin
β≥0,ωEs∼ρπυ[N/summationdisplay
i=1Aπ(s,ai)N/summationdisplay
j=1Qs
ij]
+/integraldisplay
s∈SN/summationdisplay
j=1ωs
j(N/summationdisplay
i=1Qs
ij−π(aj|s))ds+β(δ−Es∼ρπυ[N/summationdisplay
i=1N/summationdisplay
j=1DijQs
ij+1
λQs
ijlogQs
ij])(21a)
= max
Qmin
β≥0,ωEs∼ρπυ[N/summationdisplay
i=1Aπ(s,ai)N/summationdisplay
j=1Qs
ij] +/integraldisplay
s∈SN/summationdisplay
j=1N/summationdisplay
i=1ωs
j
ρπυ(s)Qs
ijρπ
υ(s)ds
−/integraldisplay
s∈SN/summationdisplay
j=1ωs
jπ(aj|s)ds+βδ−βEs∼ρπυ[N/summationdisplay
i=1N/summationdisplay
j=1DijQs
ij+1
λQs
ijlogQs
ij]) (21b)
= max
Qmin
β≥0,ωβδ−/integraldisplay
s∈SN/summationdisplay
j=1ωs
jπ(aj|s)ds
+Es∼ρπυ[N/summationdisplay
i=1N/summationdisplay
j=1(Aπ(s,ai)−βDij+ωs
j
ρπυ(s))Qs
ij−β
λQs
ijlogQs
ij] (21c)
= min
β≥0,ωmax
Qβδ−/integraldisplay
s∈SN/summationdisplay
j=1ωs
jπ(aj|s)ds
+Es∼ρπυ[N/summationdisplay
i=1N/summationdisplay
j=1(Aπ(s,ai)−βDij+ωs
j
ρπυ(s))Qs
ij−β
λQs
ijlogQs
ij], (21d)
where (21d) holds since the Lagrangian function L(Q,β,ω )is concave in Qand linear in βandω, and we
can exchange the maxand the minfollowing the Minimax theorem (Sion, 1958).
Note that the inner max problem of (21d) is an unconstrained concave problem, and we can obtain the
optimalQby taking the derivatives and setting them to 0. That is,
∂L
∂Qs
ij=Aπ(s,ai)−βDij+ωs
j
ρπυ(s)−β
λ(logQs
ij+ 1) = 0,∀i,j= 1,···,N,s∈S. (22)
Therefore, we have the optimal Qs∗
ijas:
Qs∗
ij= exp (λ
βAπ(s,ai)−λDij) exp (λωs
j
βρπυ(s)−1),∀i,j= 1,···,N,s∈S. (23)
In addition, since/summationtextN
i=1Qs∗
ij=π(aj|s), we have the following hold:
exp (λωs
j
βρπυ(s)−1) =π(aj|s)/summationtextN
i=1exp (λ
βAπ(s,ai)−λDij). (24)
By substituting the left hand side of (24) into (23), we can further reformulate the optimal Qs∗
ijas:
Qs∗
ij=exp (λ
βAπ(s,ai)−λDij)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)π(aj|s),∀i,j= 1,···,N,s∈S. (25)
25Published in Transactions on Machine Learning Research (06/2023)
To obtain the optimal dual variables, based on (24), we have the optimal ω∗as:
ωs∗
j=ρπ
υ(s){β
λ+β
λln(π(aj|s))−β
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)]},∀j= 1,···,N,s∈S(26)
By substituting (25) and (26) into (21d), we can obtain the optimal β∗via:
min
β≥0βδ−Es∼ρπυN/summationdisplay
j=1π(aj|s){β
λ+β
λln(π(aj|s))−β
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)]}
+Es∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1β
λexp (λ
βAπ(s,ai)−λDij)·π(aj|s)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj).
The proof for the upper bound of sinkhorn optimal βcan be found in Appendix E.
E Upper bound of Sinkhorn Optimal Beta
In this section, we will derive the upper bound of Sinkhorn optimal β. First, for a given β, the optimal
Qs∗
ij(β)to the Lagrangian dual L(Q,β,ω )can be expressed in (25). With this, we will present the following
two lemmas:
Lemma 1. The objective function (20a) with respect to Qs∗
ij(β)decreases as the dual variable βincreases.
Lemma 2. If Assumption 1 holds, then for every δ>0,Qs∗
ij(2Amax
δ)is feasible to (20b) for any λ.
We provide proofs for Lemma 1 and Lemma 2 in Appendix E.1 and Appendix E.2 respectively. Given the
above two lemmas, we are able to prove the following proposition on the upper bound of Sinkhorn optimal β:
Proposition 2. Ifβ∗
λis the optimal dual solution to the Sinkhorn dual formulation (9), then β∗
λ≤2Amax
δfor
anyλ.
Proof of Proposition 2. We will prove it by contradiction. According to Lemma 2, Qs∗
ij(2Amax
δ)is feasible to
(20b). Since β∗
λis the optimal dual solution, Qs∗
ij(β∗
λ)is optimal to (20). If β∗
λ>2Amax
δ, according to Lemma
1, the objective value in (20a) with respect to2Amax
δis smaller than the objective value in (20a) with respect
toβ∗
λ, which contradicts the fact that Qs∗
ij(β∗
λ)is the optimal solution to (20).
E.1 Proof of Lemma 1
Lemma 1. The objective function (20a) with respect to Qs∗
ij(β)decreases as the dual variable βincreases.
Proof of Lemma 1. LetGλ(β)represent the objective function (20a). By substituting the optimal Qs∗
ijin
(25) into (20a), we have:
Gλ(β) =Es∼ρπυ[N/summationdisplay
i=1Aπ(s,ai)N/summationdisplay
j=1exp (λ
βAπ(s,ai)−λDij)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)π(aj|s)] (27a)
=Es∼ρπυ[N/summationdisplay
j=1π(aj|s)N/summationdisplay
i=1Aπ(s,ai)exp (λ
βAπ(s,ai)−λDij)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)]. (27b)
For anyβ2>β1>0, we have:
Gλ(β1)−Gλ(β2)
=Es∼ρπυN/summationdisplay
j=1π(aj|s)N/summationdisplay
i=1Aπ(s,ai){exp (λ
β1Aπ(s,ai)−λDij)
/summationtextN
k=1exp (λ
β1Aπ(s,ak)−λDkj)
26Published in Transactions on Machine Learning Research (06/2023)
−exp (λ
β2Aπ(s,ai)−λDij)
/summationtextN
k=1exp (λ
β2Aπ(s,ak)−λDkj)} (28a)
=Es∼ρπυN/summationdisplay
j=1π(aj|s)N/summationdisplay
i=1Aπ(s,a[i]){exp (λ
β1Aπ(s,a[i])−λD[i]j)
/summationtextN
k=1exp (λ
β1Aπ(s,a[k])−λD[k]j)
−exp (λ
β2Aπ(s,a[i])−λD[i]j)
/summationtextN
k=1exp (λ
β2Aπ(s,a[k])−λD[k]j)}, (28b)
where [i]denotes sorted indices that satisfy Aπ(s,a[1])≥Aπ(s,a[2])≥···≥Aπ(s,a[N]). Let
fs(i) =exp (λ
β1Aπ(s,a[i])−λD[i]j)
/summationtextN
k=1exp (λ
β1Aπ(s,a[k])−λD[k]j)−exp (λ
β2Aπ(s,a[i])−λD[i]j)
/summationtextN
k=1exp (λ
β2Aπ(s,a[k])−λD[k]j)(29a)
=exp ((λ
β1−λ
β2)Aπ(s,a[i])) exp (λ
β2Aπ(s,a[i])−λD[i]j)
/summationtextN
k=1exp ((λ
β1−λ
β2)Aπ(s,a[k])) exp (λ
β2Aπ(s,a[k])−λD[k]j)
−exp (λ
β2Aπ(s,a[i])−λD[i]j)
/summationtextN
k=1exp (λ
β2Aπ(s,a[k])−λD[k]j). (29b)
For notation brevity, we let ms(i) =exp ((λ
β1−λ
β2)Aπ(s,a[i]))>0,ws(i) =exp (λ
β2Aπ(s,a[i])−λD[i]j)>0
andqs(i) =1/summationtextN
k=1ms(k)ws(k)−1/summationtextN
k=1ms(i)ws(k). Then we have
(29b) =ms(i)ws(i)/summationtextN
k=1ms(k)ws(k)−ws(i)/summationtextN
k=1ws(k)(30a)
=ms(i)ws(i)(1/summationtextN
k=1ms(k)ws(k)−1/summationtextN
k=1ms(i)ws(k)) (30b)
=ms(i)ws(i)qs(i). (30c)
Sinceλ
β1−λ
β2>0,ms(i)decreases as iincreases. Thus, qs(i)decreases as iincreases. Since ms(1)≥ms(k)and
ms(N)≤ms(k)for allk= 1,...,N, we haveqs(1) =1/summationtextN
k=1ms(k)ws(k)−1/summationtextN
k=1ms(1)ws(k)≥1/summationtextN
k=1ms(k)ws(k)−
1/summationtextN
k=1ms(k)ws(k)= 0, andqs(N) =1/summationtextN
k=1ms(k)ws(k)−1/summationtextN
k=1ms(N)ws(k)≤1/summationtextN
k=1ms(k)ws(k)−1/summationtextN
k=1ms(k)ws(k)=
0. Sinceqs(1)≥0,qs(N)≤0andqs(i)decreases as iincreases, there exists an index 1≤ks≤Nsuch
thatqs(i)≥0fori≤ksandqs(i)<0fori > ks. Sincems(i),ws(i)>0, we havefs(i)≥0fori≤ks
andfs(i)<0fori > ks. In addition, we have/summationtextN
i=1fs(i) = 0directly follows from the definition. Thus,/summationtextN
i=1fs(i) =/summationtextks
i=1|fs(i)|−/summationtextN
i=ks+1|fs(i)|= 0. Therefore,
Gλ(β1)−Gλ(β2) =Es∼ρπυN/summationdisplay
j=1π(aj|s)N/summationdisplay
i=1Aπ(s,a[i])fs(i) (31a)
=Es∼ρπυN/summationdisplay
j=1π(aj|s){ks/summationdisplay
i=1Aπ(s,a[i])|fs(i)|−N/summationdisplay
i=ks+1Aπ(s,a[i])|fs(i)|} (31b)
≥Es∼ρπυN/summationdisplay
j=1π(aj|s){ks/summationdisplay
i=1Aπ(s,a[ks])|fs(i)|−N/summationdisplay
i=ks+1Aπ(s,a[ks+1])|fs(i)|} (31c)
=Es∼ρπυN/summationdisplay
j=1π(aj|s){Aπ(s,a[ks])ks/summationdisplay
i=1|fs(i)|−Aπ(s,a[ks+1])N/summationdisplay
i=ks+1|fs(i)|}(31d)
=Es∼ρπυN/summationdisplay
j=1π(aj|s){Aπ(s,a[ks])ks/summationdisplay
i=1|fs(i)|−Aπ(s,a[ks+1])ks/summationdisplay
i=1|fs(i)|} (31e)
27Published in Transactions on Machine Learning Research (06/2023)
=Es∼ρπυN/summationdisplay
j=1π(aj|s)(Aπ(s,a[ks])−Aπ(s,a[ks+1]))ks/summationdisplay
i=1|fs(i)| (31f)
≥0. (31g)
where (31c) and (31g) hold since Aπ(s,a[i])is non-increasing as iincreases. Furthermore, at least one inequality
of (31c) and (31g) will not hold at equality since/summationtextN
i=1π(ai|s)Aπ(s,ai) = 0,∀s∈S, and for non-trivial
cases,Pr{Aπ(s,a) = 0,∀s∈S,∀a∈A}<1, which means Pr{∃s1,s2∈S,a1,a2∈A, s.t. Aπ(s1,a1)̸=
Aπ(s2,a2)}>0. Therefore, we have Gλ(β1)−Gλ(β2)>0.
E.2 Proof of Lemma 2
Lemma 2. If Assumption 1 holds, then for every δ>0,Qs∗
ij(2Amax
δ)is feasible to (20b) for any λ.
Proof of Lemma 2. By substituting the optimal Qs∗
ijin (25) into (20b), we can reformulate the left hand side
of (20b) as follows:
Es∼ρπυ[N/summationdisplay
i=1N/summationdisplay
j=1DijQs∗
ij+1
λQs∗
ijlogQs∗
ij] (32a)
=Es∼ρπυ{N/summationdisplay
i=1N/summationdisplay
j=1DijQs∗
ij+1
λQs∗
ij[λ
βAπ(s,ai)−λDij+ logπ(aj|s)/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)]}(32b)
=Es∼ρπυ{N/summationdisplay
i=1N/summationdisplay
j=11
βQs∗
ijAπ(s,ai) +1
λQs∗
ijlogπ(aj|s)/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)}. (32c)
Now we prove that when β=2Amax
δ,Es∼ρπυ{/summationtextN
i=1/summationtextN
j=11
βQs∗
ij(β)Aπ(s,ai)} ≤δ
2and
Es∼ρπ
υ{1
λQs∗
ij(β) logπ(aj|s)/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)}≤δ
2hold. For the first part, we have:
Es∼ρπυ{N/summationdisplay
i=1N/summationdisplay
j=11
βQs∗
ijAπ(s,ai)} (33a)
=1
βEs∼ρπυ{N/summationdisplay
i=1[N/summationdisplay
j=1Qs∗
ij]Aπ(s,ai)} (33b)
=1
βEs∼ρπ
υ{N/summationdisplay
i=1π′(ai|s)Aπ(s,ai)} (33c)
≤1
βEs∼ρπυ{N/summationdisplay
i=1π′(ai|s)|Aπ(s,ai)|} (33d)
≤Amax
β=δ
2. (33e)
For the second part, the followings hold:
Es∼ρπυ{N/summationdisplay
i=1N/summationdisplay
j=11
λQs∗
ijlogπ(aj|s)/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)} (34a)
=Es∼ρπυ{N/summationdisplay
j=11
λ(N/summationdisplay
i=1Qs∗
ij) logπ(aj|s)/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)} (34b)
=1
λEs∼ρπυ{N/summationdisplay
j=1π(aj|s) logπ(aj|s)/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)} (34c)
28Published in Transactions on Machine Learning Research (06/2023)
≤1
λEs∼ρπυ{N/summationdisplay
j=1π(aj|s) logπ(aj|s)
exp (λ
βAπ(s,aj))} (34d)
≤1
λEs∼ρπυ{N/summationdisplay
j=1π(aj|s) log1
exp (λ
βAπ(s,aj))} (34e)
=1
λEs∼ρπυ{N/summationdisplay
j=1π(aj|s)(−λ
βAπ(s,aj))} (34f)
≤1
βEs∼ρπυ{N/summationdisplay
j=1π(aj|s)|Aπ(s,aj)|} (34g)
≤Amax
β=δ
2. (34h)
Therefore,Qs∗
ij(2Amax
δ)is feasible to (20b) for any λ.
F Gradient of the Objective in the Sinkhorn Dual Formulation
The closed-form gradient of the objective in the Sinkhorn dual formulation (9) is as follows:
δ−Es∼ρπυN/summationdisplay
j=1π(aj|s)/braceleftig1
λ+1
λln(π(aj|s))−1
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)]
−β
λ·1/summationtextN
i=1exp (λ
βAπ(s,ai)−λDij)×N/summationdisplay
i=1[exp (λ
βAπ(s,ai)−λDij)×−λAπ(s,ai)β−2]/bracerightig
+Es∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1/braceleftigπ(aj|s)
λexp (λ
βAπ(s,ai)−λDij)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)
+βπ(aj|s)
λ·exp (λ
βAπ(s,ai)−λDij)×−λAπ(s,ai)β−2×/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)
(/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj))2
−βπ(aj|s)
λ·exp (λ
βAπ(s,ai)−λDij)×/summationtextN
k=1[exp (λ
βAπ(s,ak)−λDkj)×−λAπ(s,ak)β−2]
(/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj))2/bracerightig
.
G Proof of Theorem 3
Given the upper bound of Wassertein optimal βin Theorem 1 and the upper bound of Sinkhorn optimal βin
Proposition 2, we are able to derive the following theorem:
Theorem 3. DefineβUB= max{2Amax
δ,¯β}. We have:
1.Fλ(β)converges to F(β)uniformly on [0,βUB]:lim
λ− →∞sup
0≤β≤βUB/vextendsingle/vextendsingle/vextendsingleFλ(β)−F(β)/vextendsingle/vextendsingle/vextendsingle≤lim
λ− →∞βUB
λNlnN= 0.
2.lim
λ− →∞argmin0≤β≤βUBFλ(β)⊆argmin0≤β≤βUBF(β).
Proof of Theorem 3. To show that Fλ(β)converges to F(β)uniformly on [0,βUB], it is equivalent to show that
limλ− →∞sup0≤β≤βUB/vextendsingle/vextendsingle/vextendsingleFλ(β)−F(β)/vextendsingle/vextendsingle/vextendsingle= 0. Letϵπ
s(β,i,j )=maxk=1...N(Aπ(s,ak)−βDkj)−[Aπ(s,ai)−βDij],
andϵπ
s(β,i,j )≥0. First, we have
/vextendsingle/vextendsingle/vextendsingleFλ(β)−F(β)/vextendsingle/vextendsingle/vextendsingle
29Published in Transactions on Machine Learning Research (06/2023)
=/vextendsingle/vextendsingle/vextendsingleβδ−Es∼ρπυN/summationdisplay
j=1π(aj|s){β
λ+β
λln(π(aj|s))−β
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)]}
+Es∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1β
λexp (λ
βAπ(s,ai)−λDij)·π(aj|s)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)−βδ
−Es∼ρπυN/summationdisplay
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)/vextendsingle/vextendsingle/vextendsingle (35a)
≤/vextendsingle/vextendsingle/vextendsingleβ
λEs∼ρπυN/summationdisplay
j=1π(aj|s)/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleβ
λEs∼ρπυN/summationdisplay
j=1π(aj|s) ln(π(aj|s))/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
i=1N/summationdisplay
j=1β
λexp (λ
βAπ(s,ai)−λDij)·π(aj|s)
/summationtextN
k=1exp (λ
βAπ(s,ak)−λDkj)/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s)β
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)]
−Es∼ρπυN/summationdisplay
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)/vextendsingle/vextendsingle/vextendsingle (35b)
≤2/vextendsingle/vextendsingle/vextendsingleβ
λEs∼ρπυN/summationdisplay
j=1π(aj|s)/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleβ
λEs∼ρπυN/summationdisplay
j=1π(aj|s) ln(π(aj|s))/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s)β
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)]
−Es∼ρπυN/summationdisplay
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)/vextendsingle/vextendsingle/vextendsingle. (35c)
In addition,
/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s)β
λln[N/summationdisplay
i=1exp (λ
βAπ(s,ai)−λDij)]
−Es∼ρπυN/summationdisplay
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)/vextendsingle/vextendsingle/vextendsingle (36a)
=/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s)β
λln[exp (λ
βmax
k=1...N(Aπ(s,ak)−βDkj))N/summationdisplay
i=1exp (−λ
βϵπ
s(β,i,j ))]
−Es∼ρπυN/summationdisplay
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)/vextendsingle/vextendsingle/vextendsingle (36b)
=/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s)β
λ{ln[exp (λ
βmax
k=1...N(Aπ(s,ak)−βDkj))] + ln[N/summationdisplay
i=1exp (−λ
βϵπ
s(β,i,j ))]}
−Es∼ρπυN/summationdisplay
j=1π(aj|s) max
i=1...N(Aπ(s,ai)−βDij)/vextendsingle/vextendsingle/vextendsingle (36c)
=/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s)β
λln[N/summationdisplay
i=1exp (−λ
βϵπ
s(β,i,j ))]/vextendsingle/vextendsingle/vextendsingle. (36d)
30Published in Transactions on Machine Learning Research (06/2023)
Therefore,
lim
λ− →∞sup
0≤β≤βUB/vextendsingle/vextendsingle/vextendsingleFλ(β)−F(β)/vextendsingle/vextendsingle/vextendsingle (37a)
≤lim
λ− →∞2βUB
λ/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s)/vextendsingle/vextendsingle/vextendsingle+ lim
λ− →∞βUB
λ/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s) ln(π(aj|s))/vextendsingle/vextendsingle/vextendsingle
+ lim
λ− →∞sup
0≤β≤βUBβ
λ/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s) ln[N/summationdisplay
i=1exp (−λ
βϵπ
s(β,i,j ))]/vextendsingle/vextendsingle/vextendsingle (37b)
= lim
λ− →∞sup
0≤β≤βUBβ
λ/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s) ln[N/summationdisplay
i=1exp (−λ
βϵπ
s(β,i,j ))]/vextendsingle/vextendsingle/vextendsingle. (37c)
In addition,∀β∈[0,βUB]and∀λ,ϵπ
s(β,i,j )is bounded since
/vextendsingle/vextendsingle/vextendsingleϵπ
s(β,i,j )/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsinglemax
k=1...N(Aπ(s,ak)−βDkj)−[Aπ(s,ai)−βDij]/vextendsingle/vextendsingle/vextendsingle (38)
≤2 max
s,aAπ(s,a) +βUBmax
i,jDij
≤2Amax+βUBmax
i,jDij<∞. (39)
Then,/vextendsingle/vextendsingle/vextendsingleEs∼ρπυ/summationtextN
j=1π(aj|s)ln[/summationtextN
i=1exp (−λ
βϵπ
s(β,i,j ))]/vextendsingle/vextendsingle/vextendsingleis bounded. Therefore in (37c), the optimal βcan be
achieved. Let βλ=argmax0≤β≤βUBβ
λ/vextendsingle/vextendsingle/vextendsingleEs∼ρπυ/summationtextN
j=1π(aj|s) ln[/summationtextN
i=1exp (−λ
βϵπ
s(β,i,j ))]/vextendsingle/vextendsingle/vextendsingle, and then we have:
lim
λ− →∞sup
0≤β≤βUBβ
λ/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s) ln[N/summationdisplay
i=1exp (−λ
βϵπ
s(β,i,j ))]/vextendsingle/vextendsingle/vextendsingle (40a)
= lim
λ− →∞βλ
λ/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s) ln[N/summationdisplay
i=1exp (−λ
βλϵπ
s(βλ,i,j))]/vextendsingle/vextendsingle/vextendsingle. (40b)
LetKπ
s(β,j) =argmaxk=1...NAπ(s,ak)−βDkj. Defineσs(j)=min0≤β≤βUBmini=1...N,i/∈Kπs(β,j)ϵπ
s(β,i,j ).
Then since ϵπ
s(β,i,j )>0fori /∈Kπ
s(β,j)based on its definition, we have σs(j)>0. On one hand, we have
lim
λ− →∞ln[N/summationdisplay
i=1exp (−λ
βλϵπ
s(βλ,i,j))] (41a)
= lim
λ− →∞ln[N/summationdisplay
i=1|i/∈Kπs(βλ,j)exp (−λ
βλϵπ
s(βλ,i,j)) +N/summationdisplay
i=1|i∈Kπs(βλ,j)exp (−λ
βλϵπ
s(βλ,i,j))] (41b)
≤lim
λ− →∞ln[N/summationdisplay
i=1|i/∈Kπs(βλ,j)exp (−λ
βUBσs(j)) +N/summationdisplay
i=1|i∈Kπs(βλ,j)exp (0)] (41c)
= lim
λ− →∞ln[N/summationdisplay
i=1|i/∈Kπs(βλ,j)exp (−λ
βUBσs(j)) +|Kπ
s(βλ,j)|] (41d)
= lim
λ− →∞ln[|Kπ
s(βλ,j)|]. (41e)
On the other hand, we have
lim
λ− →∞ln[N/summationdisplay
i=1exp (−λ
βλϵπ
s(βλ,i,j))] (42a)
31Published in Transactions on Machine Learning Research (06/2023)
= lim
λ− →∞ln[N/summationdisplay
i=1|i/∈Kπs(βλ,j)exp (−λ
βλϵπ
s(βλ,i,j)) +N/summationdisplay
i=1|i∈Kπs(βλ,j)exp (−λ
βλϵπ
s(βλ,i,j))] (42b)
≥lim
λ− →∞ln[N/summationdisplay
i=1|i∈Kπs(βλ,j)exp (−λ
βλϵπ
s(βλ,i,j))] (42c)
= lim
λ− →∞ln[N/summationdisplay
i=1|i∈Kπs(βλ,j)exp (0)] (42d)
= lim
λ− →∞ln[|Kπ
s(βλ,j)|]. (42e)
Therefore, limλ− →∞/vextendsingle/vextendsingle/vextendsingleln[/summationtextN
i=1exp (−λ
βλϵπ
s(βλ,i,j))]/vextendsingle/vextendsingle/vextendsingle= limλ− →∞ln[|Kπ
s(βλ,j)|]. Based on that, we have
lim
λ− →∞βλ
λ/vextendsingle/vextendsingle/vextendsingleEs∼ρπυN/summationdisplay
j=1π(aj|s) ln[N/summationdisplay
i=1exp (−λ
βλϵπ
s(βλ,i,j))]/vextendsingle/vextendsingle/vextendsingle (43a)
≤lim
λ− →∞βλ
λ/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
j=1ln[N/summationdisplay
i=1exp (−λ
βλϵπ
s(βλ,i,j))]/vextendsingle/vextendsingle/vextendsingle (43b)
≤lim
λ− →∞βλ
λN/summationdisplay
j=1/vextendsingle/vextendsingle/vextendsingleln[N/summationdisplay
i=1exp (−λ
βλϵπ
s(βλ,i,j))]/vextendsingle/vextendsingle/vextendsingle (43c)
= lim
λ− →∞βλ
λN/summationdisplay
j=1ln[|Kπ
s(βλ,j)|] (43d)
≤lim
λ− →∞βUB
λNlnN= 0, (43e)
which means limλ− →∞sup0≤β≤βUB/vextendsingle/vextendsingle/vextendsingleFλ(β)−F(β)/vextendsingle/vextendsingle/vextendsingle≤0. Furthermore, since limλ− →∞sup0≤β≤βUB|Fλ(β)−
F(β)| ≥ 0holds naturally, we have limλ− →∞sup0≤β≤βUB|Fλ(β)−F(β)|= 0. Therefore, Fλ(β)
converges to F(β)uniformly on [0,βUB], which also indicates Fλ(β)epi-converges to F(β)on
[0,βUB](Royset, 2018; Rockafellar & Wets, 1998). By properties of epi-convergence, we have that
lim
λ− →∞argmin0≤β≤βUBFλ(β)⊆argmin0≤β≤βUBF(β)(Rockafellar & Wets, 1998).
H Proof of Lemma 1
Lemma 1. Asλk− →∞, SPO update converges to WPO update: limλk− →∞FSPO(πk)∈FWPO(πk).
Proof of Lemma 1. Letξk
s(i,j) =λ
βk{maxl=1,...,N(ˆAπk(s,al)−βkDlj)−[ˆAπk(s,ai)−βkDij]}. The SPO
update with λ− →∞equals to:
πk+1(ai|s) = lim
λ− →∞FSPO(πk) = lim
λ− →∞N/summationdisplay
j=1exp (λ
βkˆAπk(s,ai)−λDij)
/summationtextN
l=1exp (λ
βkˆAπk(s,al)−λDlj)πk(aj|s) (44a)
= lim
λ− →∞N/summationdisplay
j=1exp ( ˆAπk(s,aˆkπks(βk,j))−βkDˆkπks(βk,j)j)·exp (−ξk
s(i,j))
exp ( ˆAπt(s,aˆkπks(βk,j))−βkDˆkπks(βk,j)j)·/summationtextN
l=1exp(−ξks(l,j))πk(aj|s)(44b)
= lim
λ− →∞N/summationdisplay
j=1exp (−ξk
s(i,j))/summationtextN
l=1exp(−ξks(l,j))πk(aj|s) (44c)
= lim
λ− →∞N/summationdisplay
j=1exp (−ξk
s(i,j))·πk(aj|s)/summationtext
l∈ˆKπks(βk,j)exp(−ξks(l,j)) +/summationtext
l/∈ˆKπks(βk,j)exp(−ξks(l,j))(44d)
32Published in Transactions on Machine Learning Research (06/2023)
=N/summationdisplay
j=1limλ− →∞exp (−ξk
s(i,j))·πk(aj|s)/summationtext
l∈ˆKπks(βk,j)limλ− →∞exp(−ξks(l,j)) +/summationtext
l/∈ˆKπks(βk,j)limλ− →∞exp(−ξks(l,j))(44e)
=N/summationdisplay
j=1IˆKπks(βk,j)(i)
|ˆKπks(βk,j)|πk(aj|s), (44f)
whereIdenotes the indicator function; (44f) holds because as λ− →∞,ξk
s(i,j) =∞fori /∈ˆKπks(βk,j)and0
otherwise, thus limλ− →∞exp (−ξk
s(i,j)) = 0fori /∈ˆKπks(βk,j)and1otherwise.
Letfk
s(i,j) =1
|ˆKπks(βk,j)|ifi∈ˆKπks(βk,j), andfk
s(i,j) = 0otherwise. Therefore, SPO update with λ− →∞
equals to the following WPO update, FWPO(πk) =/summationtextN
j=1πk(aj|s)fk
s(i,j).
I Proof of Theorem 4
Theorem 4. (Performance improvement) For any initial state distribution υand anyβk≥0, if
||ˆAπ−Aπ||∞≤ϵfor someϵ>0, let ˆKπks(βk,j) =argmaxi=1,...,N{ˆAπk(s,ai)−βkDij}, WPO policy update
(and SPO with λ− →∞) guarantee the following performance improvement bound when the inaccurate
advantage function ˆAπis used,
J(πk+1)≥J(πk) +βkEs∼ρπk+1
υN/summationdisplay
j=1πk(aj|s)/summationdisplay
i∈ˆKπks(βk,j)fk
s(i,j)Dij−2ϵ
1−γ. (10)
Proof of Theorem 4.
J(πk+1)−J(πk) =Es∼ρπk+1
υEa∼πk+1[Aπk(s,a)] (45a)
=Es∼ρπk+1
υN/summationdisplay
i=1πk+1(ai|s)Aπk(s,ai) (45b)
=Es∼ρπk+1
υN/summationdisplay
i=1N/summationdisplay
j=1πk(aj|s)fk
s(i,j)Aπk(s,ai) (45c)
=Es∼ρπk+1
υN/summationdisplay
j=1πk(aj|s)N/summationdisplay
i=1fk
s(i,j)Aπk(s,ai) (45d)
=Es∼ρπk+1
υN/summationdisplay
j=1πk(aj|s)/summationdisplay
i∈ˆKπks(βk,j)fk
s(i,j)Aπk(s,ai) (45e)
≥Es∼ρπk+1
υN/summationdisplay
j=1πk(aj|s)/summationdisplay
i∈ˆKπks(βk,j)fk
s(i,j)[Aπk(s,aj) +βkDij−2ϵ](45f)
=βkEs∼ρπk+1
υN/summationdisplay
j=1πk(aj|s)/summationdisplay
i∈ˆKπks(βk,j)fk
s(i,j)Dij−2ϵ
1−γ, (45g)
where (45a) holds due to the performance difference lemma in Kakade & Langford (2002); (45f) follows from
the definition of ˆKπks(βk,j)and the fact that ||ˆAπk−Aπk||∞≤ϵ, therefore for i∈ˆKπks(βk,j),[Aπk(s,ai) +
ϵ]−βkDij≥ˆAπk(s,ai)−βkDij≥ˆAπk(s,aj)−βkDjj=ˆAπk(s,aj)≥Aπk(s,aj)−ϵ; (45g) holds since
Ea∼π[Aπ(s,a)] = 0.
33Published in Transactions on Machine Learning Research (06/2023)
J Proof of Theorem 5
Theorem 5. (Global convergence) Under Assumption 2, we have for any βk≥0, (WPO) satisfies that
∥V⋆−Vπk+1∥∞≤γ∥V⋆−Vπk∥∞+βk∥D∥∞, (11)
and (SPO) satisfies that
∥V⋆−Vπk+1∥∞≤γ∥V⋆−Vπk∥∞+ 2βk
1−γ/parenleftbigg
∥D∥∞+ 2logN
λ/parenrightbigg
. (12)
Iflimk→∞βk= 0, we further have limk→∞J(πk) =J⋆.
Proof of Theorem 5. Our proof is inspired by the work Bhandari & Russo (2021).
We use the shorthand πsfor the probability distribution π(·|s)on the actions and denote the probability
distribution on the action space Aas∆. To save notations, we rewrite πk+1,πkandβkasπ+,πandβ
respectively. We use dfor eitherdWordSin the following derivation. Note d≤∥D∥∞= :Dfor both cases4,
anddS≥−2logN
λ.5
Since a policy πis indeed just a member of/producttext|S|
i=1∆, we find that the problem (7) can be split into |S|many
optimization problems. For each s∈S, we need to solve
max
π′s∈∆ρπ(s)Ea∼π′(·|s)[Aπ(s,a)]−βρπ(s)d(π′
s,πs).(46)
Denote the quality function of πasQπ(s,a) =E[Rt|st=s,at=a;π], and the value function of πas
Vπ(s) =E[Rt|st=s;π], we find that Aπ(s,a) =Qπ(s,a)−Vπ(s). Since the second term is only a function
of the current policy πand the state s, we find that Problem (46) is further equivalent to (in the sense of the
same solution set):
max
π′s∈∆Ea∼π′s[Qπ(s,a)]−βd(π′
s,πs).(47)
Here we use ρ0(s)>0for alls. Let ¯πbe a solution of the policy iteration:
¯πs∈arg max
π′sEa∼π′s[Qπ(s,a)]. (48)
Also define the bellman operator T:R|S|→R|S|and the operator Tπ′:R|S|→R|S|: for anyV∈R|S|,
(TV)s= max
a∈Ar(s,a) +γEs′∼P(·|s,a)[V(s′)], (49)
(Tπ′V)s=Ea∼π′s[r(s,a) +γEs′∼P(·|s,a)V(s′)]. (50)
Using the relation between the quality function and the value function, Qπ(s,a) =r(s,a)+Es′∼P(·|s,a)[Vπ(s′)],
we can rewrite the above equations in terms of the quality function for V=Vπ:
(TVπ)s= max
a∈Ar(s,a) +γEs′∼P(·|s,a)[Vπ(s′)] = max
a∈AQ(s,a) =T¯πVπ, (51)
(Tπ′Vπ)s=Ea∼π′s[Qπ(s,a)]. (52)
4For Sinkhorn divergence, note that the entropy function is always nonnegative.
5This lower bound is obtained via dS(π′,π|λ)≥ minQ≥0,/summationtext
i,jQij=1/braceleftbig
⟨Q,D⟩−1
λh(Q)/bracerightbig(a)=⟨Q,D⟩ −
1
λh(Q)|
Qij=exp(−λDij)/summationtext
i,jexp(−λDij)=−1
λlog/parenleftig/summationtext
i,jexp(−λDij)/parenrightig(b)
≥ −2 logN
λ. Here in the step (a), we use the Lagrangian mul-
tiplier method to derive the optimal Qij=exp(−λDij)/summationtext
i,jexp(−λDij). In the step (b), we use the fact that log(/summationtextn
i=1exp(xi))≤
max{x1,...,xn}+ lognfor anyx1,...,xn∈RandDii= 0for anyi.
34Published in Transactions on Machine Learning Research (06/2023)
Let us consider d=dWfirst. Using the optimality of π+for the problem (46), we know that
Ea∈π+
s[Qπ(s,a)]−βd(π+
s,πs)≥Ea∈¯πs[Qπ(s,a)]−βd(¯πs,πs)
=⇒Ea∈π+
s[Qπ(s,a)]≥Ea∈¯πs[Qπ(s,a)]−βD.(53)
and
Ea∈π+
s[Qπ(s,a)]−βd(π+
s,πs)≥Ea∈πs[Qπ(s,a)]−βd(πs,πs)
=⇒Ea∈π+
s[Qπ(s,a)]≥Ea∈πs[Qπ(s,a)] =Vπ(s).(54)
Using the notation in (51) and (52), (53) and (54) become
Tπ+Vπ≥TVπ−βD1|S|, (55)
Tπ+Vπ≥Vπ. (56)
Here 1|S|is a vector of all one entries and the inequality ≥means entrywisely larger than or equal to. By
iteratively applying Tπ+to (56) and use the fact that Tπ+is a monotone and contraction map with Vπ+as
the unique fixed point, we have
Vπ+≥···≥ (Tπ+)2Vπ≥Tπ+Vπ≥Vπ. (57)
Hence we have
0(a)
≤V⋆−Vπ+(b)
≤V⋆−Tπ+Vπ(c)
≤V⋆−TVπ+βD1|S|. (58)
Here the inequality (a)is due to the optimality of V⋆. The inequality (b)is due to (57), and the inequality
(c)is due to (55). Now using the fact V⋆is the unique fixed point of T, andTis a monotone and contraction
map, we have from (58) that
∥V⋆−Vπ+∥∞≤∥TV⋆−TVπ∥∞+βD≤γ∥V⋆−Vπ∥∞+βD. (59)
Next consider d=dS. The optimality of π+reveals that for ˜π= ¯πorπ:
Ea∈π+
s[Qπ(s,a)]−βd(π+
s,πs)≥Ea∈˜πs[Qπ(s,a)]−βd(˜πs,πs)
=⇒Ea∈π+
s[Qπ(s,a)]≥Ea∈˜πs[Qπ(s,a)]−β(D+ 2logN
λ).(60)
Thus we have the following
Tπ+Vπ≥TVπ−β(D+2 logN
λ)1|S|, (61)
Tπ+Vπ≥Vπ−β(D+2 logN
λ)1|S|. (62)
By iteratively applying Tπ+to (62) and use the fact that Tπ+is a monotone and contraction map with Vπ+
as the unique fixed point, we have
Vπ+≥Vπ−β
1−γ(D+ 2logN
λ)1|S|. (63)
Hence we have
0(a)
≤V⋆−Vπ+(b)
≤V⋆−Tπ+Vπ+β
1−γ(D+ 2logN
λ)1|S|
(c)
≤V⋆−TVπ+ 2β
1−γ(D+ 2logN
λ)1|S|.(64)
Here the inequality (a)is due to the optimality of V⋆. The inequality (b)is due to (63), and the inequality
(c)is due to (61). A similar derivation as (59) shows the inequality in the theorem. Hence the theorem is
established.
35Published in Transactions on Machine Learning Research (06/2023)
K Computational Complexity of the Algorithm 1
Our overall algorithm applies a general actor-critic framework: the actor follows the proposed WPO or SPO
update while the critic follows TD methods. The computational complexity depends on (i) the per-iteration
computation cost of the policy and critic update and (ii) the iteration complexity of the actor-critic method.
Here we mainly discuss the per-iteration computation cost of the policy update, as studies on the iteration
complexity of actor-critic framework for constrained policy optimization are limited.
The computation cost of WPO and SPO updates at each iteration depends on the selection of βk. Ifβkis
chosen time dependently, the computation cost of WPO/SPO policy update is O(n2
ans), wherenaandnsare
the number of actions and states to perform policy update. If we set βkas the dual optimizer, there will
be additional cost to run gradient descent to solve the one-dimensional dual formulation. As discussed in
our experiments, we can set βkto be the dual optimizer only in the first a few iterations and use a decaying
afterward. Therefore, the average computational complexity of a policy update step can be O(n2
ans).
L Difference between SPO/WPO and Other Exponential Style Updates
Sinkhorn divergence smooths the original Wasserstein by adding an entropy term, which causes the SPO
update to contain exponential components similar to standard exponential style updates such as NPG (Kakade,
2001; Peng et al., 2019). Thus, SPO can be viewed as a smoother version of WPO update. Nonetheless, it’s
important to note that SPO/WPO updates differ fundamentally from standard exponential style updates
that are based solely on entropy or KL divergence. In both SPO and WPO, the probability mass at action
ais redistributed to neighboring actions with high value (i.e., those a′with highAπ(s,a′)−βd(a′,a)). In
contrast, in these standard exponential style updates, probability mass at action ais reweighted according to
its exponential advantage or Q value.
M Exploration Properties of WPO/SPO
Compared to the Wasserstein metric, the KL divergence between policies is often larger, especially when
considering the policy shifts of closely related actions, as shown in Figure 2. In practice, when employing
the same trust region size δ, Wasserstein metric allows for more admissible policies within the trust region
compared to KL, thereby leading to better exploration. This advantage is demonstrated in our motivating
example in Figure 3.
Furthermore, Sinkhorn divergence has even more exploration advantages than using Wasserstein. As Sinkhorn
smooths the original Wasserstein with an entropy term, it includes additional smoother (more uniform)
policies in the trust region, leading to even faster exploration.
Our numerical results in Section 7 also support that WPO/SPO explores better than KL; and SPO achieves
faster exploration than WPO.
N Policy Parametrization, Prior Work on Nonparametric Policy
As noted in (Tessler et al., 2019), the suboptimality of policy gradient is not due to parametrization (e.g.,
neural network), but is a result of the parametric distribution assumption imposed on policy, which constrains
policies to a predefined set. In our work, we strive to avoid suboptimality by circumventing the parametric
distribution assumption imposed on policy, while still allowing for parametrization of policy in our empirical
studies.
Previous research, such as (Abdolmaleki et al., 2018; Peng et al., 2019), has investigated theoretical policy
update rules based on KL divergence without making explicit parametric assumptions about the policy being
used. However, to our best knowledge, no prior work has explored theoretical policy update rules based on
Wasserstein metric or Sinkhorn divergence.
36Published in Transactions on Machine Learning Research (06/2023)
O T-tests to Compare the Performance of WPO, SPO with BGPG and WNPG
We conduct independent two-sample one-tailed t-tests (Student, 1908) to compare the mean performance of
our proposed methods (WPO and SPO) with two other Wasserstein-based policy optimization approaches:
BGPG (Pacchiano et al., 2020) and WNPG (Moskovitz et al., 2021). Specifically, we formulate four alternative
hypotheses for each task: JWPO>JBGPG,JWPO>JWNPG,JSPO>JBGPG, andJSPO>JWNPG.
MuJuCo continuous control tasks are considered for the t-tests, with a sample size of 10for each algorithm.
All t-tests are conducted at a confidence level of 90%. The results of the t-tests are presented in Table 7,
where a checkmark ( ✓) indicates that the alternative hypothesis is supported with 90%confidence, and a
dash (−) indicates a failure to support the alternative hypothesis.
Based on the results presented in Table 7, we can conclude the following:
•The mean performance of WPO is higher than BGPG with 90%confidence for all tasks.
•The mean performance of WPO is higher than WNPG with 90%confidence for all tasks.
•The mean performance of SPO is higher than BGPG with 90%confidence for all tasks except Ant-v2.
•The mean performance of SPO is higher than WNPG with 90%confidence for all tasks except
HalfCheetah-v2.
We note that though SPO’s performance is not statistically significantly higher than BGPG or WNPG in
Ant-v2 and HalfCheetah-v2 tasks, SPO demonstrates a faster convergence speed than WNPG and BGPG in
these two tasks.
Table 7: T-tests results on the performance of WPO, SPO, BGPG and WNPG
Environment JWPO>JBGPGJWPO>JWNPGJSPO>JBGPGJSPO>JWNPG
HalfCheetah-v2 ✓ ✓ ✓ −
Hopper-v2 ✓ ✓ ✓ ✓
Walker2d-v2 ✓ ✓ ✓ ✓
Ant-v2 ✓ ✓ − ✓
Humanoid-v2 ✓ ✓ ✓ ✓
37