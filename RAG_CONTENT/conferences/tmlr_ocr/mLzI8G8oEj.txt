Under review as submission to TMLR
Iterative Machine Teaching for Black-Box Markov Learners
Anonymous authors
Paper under double-blind review
Abstract
Machine teaching has traditionally been constrained by the assumption of a fixed learner
model, where the learner’s progress follows given rules, such as gradient update with fixed
learning rates and version space update with a given preference function. In this paper, we
consider a generic setting which views the learner as a black box, and the learner’s dynamics
can be learned during the teaching process. We model the learner’s dynamics as a Markov
decision process (MDP) with unknown parameters, encompassing a wide range of learner
types studied in the machine teaching literature. In such a setting, machine teaching reduces
to finding an optimal policy for the underlying MDP. We then introduce an algorithm for
teaching such black-box Markov learners, and provide an analysis of the teaching cost under
both discounted and non-discounted settings. The Markov learners considered in this work
can be naturally linked to epiphany learning as studied in decision psychology. Supported
by numerical study results, this paper delivers a novel perspective for machine teaching
under the black-box setting, introducing a robust, versatile learner model with a rigorous
theoretical foundation.
1 Introduction
Machine teaching seeks effective policies for selecting training examples to help a learner learn a target
concept. Over the past few decades, the field of machine teaching has been pushed forward and shown great
promise in various application domains, including those targeting human learners, such as automated tutoring
systems (Rafferty et al., 2016; Sen et al., 2018; Zhu et al., 2018; Hunziker et al., 2019), citizen science and
crowdsourcing services (Sullivan et al., 2009; Nugent, 2018), or those targeting machine learning systems,
such as model compression (Romero et al., 2014) and understanding the vulnerability of data poisoning
attacks (Mei and Zhu, 2015; Zhu, 2018).
Figure 1: An illustration of the teaching framework.
We focus on steps (b), (c) and (d), and assume the
feature mapping (i.e. learned through step (a)) is
known and given.As illustrated in figure 1, a machine teaching frame-
work assumes a computational model of the learner—
either known or unknown to the teacher—which
typically consists of two components: (a) a model
for representing the learner’s state (e.g., learner’s
current hypotheses, as in figure 1 (a)), and (b) a
model for the learning dynamics (e.g. parameters
capturing learner’s initial knowledge, learning rate,
and learning behavior etc. as in figure 1 (b)). When
both models are known to the teacher, the teaching
problem boils down to an optimal planning problem
as in figure 1 (c). Upon receiving the teaching in-
structions, the learner makes an update according
to its own intrinsic dynamics, and proceeds to the
next knowledge state.
Classical theory of machine teaching often focuses
on specific realizations of such a framework. Depending on the learner type and how much information
of the learner the teacher can access, various teaching models have been proposed. We summarize a few
1Under review as submission to TMLR
representation
model known unknown
knownwhite-box “black-box learner”
(Goldman and Kearns, 1995; Zilles et al., 2011; Chen et al.,
2018; Mansouri et al., 2019; Lessard et al., 2019; Tabibian et al.,
2019; Hunziker et al., 2019)(Dasgupta et al., 2019;
Liu et al., 2018)
unknownblack-box MDP learner–(this work)
Table 1: A summary of different teaching settings and difference between our work and the existing literature
representative works in table 1. Under the “white-box” setting where the teacher has full access to the
learner’s dynamics and state representation, one may derive strong theoretical guarantees on the complexity
of teaching (Goldman and Kearns, 1995; Zilles et al., 2011; Chen et al., 2018; Mansouri et al., 2019; Lessard
et al., 2019; Tabibian et al., 2019; Hunziker et al., 2019). When the learner’s representation is unknown but
the learner’s dynamics (e.g. learning algorithm) are given, it has been shown that the teacher can efficiently
find a set of teaching examples with strong approximation guarantees in finding the optimal set (Dasgupta
et al., 2019) or convergence guarantees (Liu et al., 2018) in teaching the target concept. However, the practical
teaching scenario with unknown learner dynamics has been under-explored so far.
To capture the learner’s dynamics, we propose to model the learning/teaching problem via a Markov decision
process (MDP), where the learner transits among different hypotheses (states) upon receiving teaching
instructions (actions). The goal of teaching is to steer the learner towards the goal state (e.g. the concept
being taught) via the underlying MDP. As later discussed in section 2, we show that the learner models
in table 1 can be viewed as special cases of Markov learners . Note that the corresponding studies in the
literature are often focused on heuristic learner models or representations. In contrast, the Markov learner
entails a versatile framework generic to a broad class of learner models.
Furthermore, most of the existing learner models in algorithmic machine teaching, such as the preference-based
version space model (Mansouri et al., 2019; Gao et al., 2017) or the gradient-based model (Liu et al., 2017),
assume that the learner follows specific incremental hypothesis update rules which do not capture certain
drastic transitions between hypotheses. These learner models naturally align with the concept of non-epiphany
learners (Chen and Krajbich, 2017; Dufwenberg et al., 2010), a class of learner studied in decision psychology
and neuroscience that was shown not always suitable for modeling human behavior. The restrictions on the
hypothesis update rule hinder their applicability to solving practical problems, where the learner model is
often a complicated black box (e.g. inferred from historic student data (Corbett and Anderson, 1994; Yudelson
et al., 2013; Piech et al., 2015; Settles and Meeder, 2016; Sen et al., 2018; Hunziker et al., 2019)).
Our contributions. In this paper, we set forward a generic teaching framework capable of capturing
unknown complex learner dynamics in real-world teaching applications. For better understanding of the
overall teacher/learner process, we study machine teaching under a generic black-box setting, where the
learner’s dynamics are modeled by a Markov decision process (MDP) with unknown parameters. We show
that many different learner models can be interpreted as Markov learners, and teaching such learners amounts
to identifying the optimal policy for the underlying MDP. To provide a theoretical understanding, we derive
the teaching cost under the assumption that the learning dynamics can be approximated by a linear function
of the learner’s state and the teaching instruction. These results are further backed up by a numerical case
study demonstrating the effective of the proposed algorithm.
Our contributions are highlighted below.
•We introduce a generic machine teaching model with parametric Markov learners, which can be used in
place of many existing learner models for characterizing learner’s transition dynamics. This model allows
us to estimate the learner’s dynamics from data, providing a versatile approach to machine teaching.
2Under review as submission to TMLR
•As a side product of our model, we establish a natural connection between Markov learners and the
concepts of epiphany and non-epiphany learning in the behavioral science and educational research.
•Under our teaching framework, we provide rigorous analyses on the teaching costs for various teaching
scenarios. Whenthelearningdynamicsislinear, weshowthattheteachingcostsgrowsatmostpolynomially
in the optimal teaching cost and feature dimension d; when the dynamics is non-linear, we show that
teaching is not always feasible, and provide teachability conditions such that the teaching cost becomes
linear (ignoring log factors) in the optimal cost.
•Complementing our theoretical results, we conduct conducted a case study on a numerical example to
demonstrate the use case of our proposed algorithm, and provide guidelines for setting its hyperparameters.
2 Related Work
Markov learners in machine teaching. Various learner models studied in the machine teaching literature
can be viewed as Markov learners. Among these models, most are investigated under a white-box setting,
where both the states st(learner’s knowledge) and the transition probabilities Pθ(learning dynamics) are
observable and given to the teacher (Liu et al., 2017; Lessard et al., 2019; Tabibian et al., 2019; Hunziker et al.,
2019; Chen et al., 2018; Mansouri et al., 2019). Notably, some recent work consider the black-box setting,
where learner’s states stis unknown but the transition probabilities Pθare given (Dasgupta et al., 2019; Liu
et al., 2018; Yudelson et al., 2013; Rafferty et al., 2016). Additionally, popular models in educational research,
such as Deep Knowledge Tracing (DKT) (Piech et al., 2015), capture the learner’s hypothesis temporal neural
networks, typically using Recurrent Neural Networks (RNNs). This setup can conceptually be aligned with
the Markov framework by interpreting the decision process as a Partially Observable Markov Decision Process
(POMDP), where the states are represented by RNNs. In this model, transition probabilities are derived
from empirical data, and the belief states correspond to the cumulative observation history. In section 3, we
provide a few more concrete examples, along with their instantiation of the states, actions and transition
dynamics under the MDP model. Nevertheless, these machine teaching models all assume the learner’s model
is known, and are designed in an ad-hoc way. In contrast, our work focuses on proposing a generic framework
that can capture these heuristic models and allow learning the learner’s model from data.
Reinforcement learning. Our work is also closely related to the reinforcement learning literature (Jaksch
et al., 2010; Jin et al., 2020; Zhou et al., 2021; Ouyang et al., 2019; Min et al., 2021). In particular, our
algorithm design is built upon the least-squares regression algorithm for estimating the parameter of the
dynamics function, and the extended value iteration (EVI) (Jaksch et al., 2010) for generating the teaching
policy. These two sub-algorithms are commonly used as backbones in algorithm design. In comparison, our
work focuses on the non-episodic setting and takes the initialization into consideration, which better fits in the
machine teaching problem. Specifically, the learners of interest to this work are always resource-constrained
(e.g. by the perceptual capacity of human learning), and such initialization plays a critical role in the final
teaching cost as detailed in section 5. Another related line of works is teaching with reinforcement learning
policy (Wu et al., 2018; Fan et al., 2018; Florensa et al., 2018; Omidshafiei et al., 2019). However, all of these
works focus on improving the training efficiency of neural networks, i.e., whitebox learners. Their major
contributions are developing better state representation and reward shaping functions based on different
heuristics, which can serve as a complement to our work, i.e., the step (a) in figure 1.
Epiphany learning. The concept of Epiphany Learning (EL) has been rigorously studied in behavioral
science (Chen and Krajbich, 2017; Dufwenberg et al., 2010). EL denotes a phenomenon where a learning
agent (for instance, humans) experiences an abrupt enlightenment or comprehension regarding a specific
subject matter. In the context of educational research, EL manifests when students achieve an insightful
moment of comprehension or forge a substantial link between concepts, resulting in profound understanding
of a topic or problem. Conversely, Non-Epiphany Learning implies scenarios in which such transformative
moments do not transpire. Such epiphany/ non-epiphany learners naturally fit into the Markovian framework
considered in this paper (see figure 2). We use the MDP learner model as a computational model to capture
these learners, and subsequently provide an in-depth analysis of the teaching performance.
3Under review as submission to TMLR
Type of the learner States Actions Transition function
Preference-based version space (Mansouri et al., 2019) ht∈2H×Hxt∈Xht+1←σ(ht,xt)
Gradient-based (Liu et al., 2017) ht∈Rdxt∈Xht+1←ht−η·∇wℓ(ht,xt)
Skill-based (Whitehill and Movellan, 2017) ht∈[0,1]dxt∈Xht+1∝ht⊙q(xt)α⊙(1−q(xt))(1−α)
Memory-based (Settles and Meeder, 2016; Hunziker et al., 2019) ht∈R2xt∈{0,1}ht+1←ht·HL(xt)
Table 2: Examples of existing sequential learner models that have the Markov property. Detailed discussion
over these models is provided in section 3.1.
3 Problem Formulation
We deal with the black-box setting, where the teacher initially has no knowledge of the learning dynamics (e.g.,
the parameters) of the learner, i.e., how the learner updates its knowledge state upon receiving the teacher
instruction. We assume that the teacher can observe the learner’s state directly, and also knows the cost
function1. The goal of the teacher is to help the learner reach some target knowledge state with minimal cost.
To assist the learner, the teacher will not only provide informative teaching instructions to the learner but
also needs to learn about the learner’s dynamics.
Notations. Before we proceed, we first introduce some notation. We use Hto represent the set of all possible
knowledge states of learners, h0denotes the initial knowledge state, and htis the learner’s knowledge state
at iteration t. At each iteration t, the teacher can choose one teaching instruction xtfrom the teaching set X.
The learner’s knowledge state will be updated upon receiving the teaching instruction. The teacher’s goal is
to help the learner transit to the target knowledge state h⋆with minimal cost. Throughout the entire paper,
we useC⋆to denote the tightest upper bound on the expected teaching cost of the optimal teaching policy
by starting from any initial state.
3.1 Parametric Markov Learners
We model the learner as a Markov learner, which is able to cover a broad class of learners considered in the
literature (Gao et al., 2017; Whitehill and Movellan, 2017; Liu et al., 2017; Hunziker et al., 2019; Mansouri
et al., 2019). Specifically, for any given learner, it starts from some initial knowledge state h0, which represents
its current knowledge state. For each iteration t, when the learner receives the teaching instruction xt, it
updates its knowledge based on its transition probability,
ht+1∼Pθℓ[ht+1|ht,xt], (1)
whereθℓrefers to the parameters that define the transition probability or learning dynamics of the learner.
Different learners may have different θℓ. The transition probability induces a preference over the next
knowledge states for the learner, which captures the learning dynamics of the learner. Intuitively, for fast
learners, it will assign a higher transition probability to states that are close to the target state h⋆upon
receiving the teaching instructions. In contrast, sometimes, a learner may not be able to understand advanced
teaching instructions before it reaches certain knowledge state. To model such scenarios, the learner may
assign a very high probability to remain at the current knowledge state when receiving obscure teaching
instructions (i.e., no learning progress after receiving the teaching instruction).
In the following, we will illustrate through a set of examples how the parametric Markov model described in
equation 1 captures the learner’s dynamics characterized by several existing sequential learner models, as
summarized in table 2.
Example 1 (Preference-based model for version space learner) For preference-based learners (Chen
et al., 2018; Mansouri et al., 2019), a state ht:= (Ht,ht)is represented as a combination of the learner’s
current version space, denoted by Ht, and their current hypothesis, denoted by ht. This representation captures
both the set of all hypotheses that are consistent with the observed data (the version space) and the learner’s
1In practice, the teacher can probe the learner’s knowledge state by quizzing the learner. The cost could be the price of the
teaching instruction.
4Under review as submission to TMLR
specific hypothesis at any given time. An action xtcorresponds to the provision of a teaching example, which
influences the learner’s hypothesis. The transition from one state to the next is governed by the preference
functionσ, determining how the learner navigates through Htin response to teaching actions:
ht+1←σ(ht,xt)
Mansouri et al. (2019) show that by instantiating the preference-based learner with different preference
functionsσ, it reduces to several classic learner models in algorithmic machine teaching: When σ(ht,xt) =c
for some constant c, it corresponds to the classic “worst-case” version space model (Goldman and Kearns,
1995); when σ(ht,xt) =g(·)for some function gthat does not depend on the learner’s current state (i.e.
current hypothesis htand the version space Ht, it corresponds to the (global) preference-based learner’s
model (Zilles et al., 2011; Gao et al., 2017).
Example 2 (Gradient-based learner)
ht+1←ht−η·∇wℓ(ht,xt)
whereηdenotes the learning rate, ht∈Rddenotes the learner’s state at t, andℓdenotes the loss function.
Liu et al. (2017; 2018) study both the white-box setting and the black-box setting. For the black-box setting,
they assume that the learner’s state htis unknown but the transition function, defined through the learning
rateη, is known. When ηis unknown, the teacher needs to spend an extra budget to estimate it, which does
not affect the teaching complexity overall.
Example 3 (Skill-based learner) Skill-based learners (Bower, 1961; Corbett and Anderson, 1994; White-
hill and Movellan, 2017) conceptualize learning as the acquisition of discrete, independent skills. In the
simplest form of such models, the state space corresponds to dindependent skills; each skill is binary, indicating
whether it is “learned” (1) or “not learned” (0). At time step t, when an exercise xtis presented, the skill
associated with xtcan jump from 0 to 1 state with some probability. This probabilistic transition is captured
by
ht+1∝ht⊙q(xt)α⊙(1−q(xt))(1−α)
Here,q(xt)represents the probability of learning the skill associated with exercise xt, andαis a binary
variable indicating the presence (1) or absence (0) of the skill prior to xt.
Skill-based learners represent a fundamental learner’s model in intelligent tutoring systems (ITS), which is
integral to the knowledge tracing frameworks (Corbett and Anderson, 1994). More advanced models such as
Deep Knowledge Tracing (Piech et al., 2015) extend beyond binary skill states, employing continuous and
correlated variables to capture more intricate representation of learners’ skill sets, thereby enhancing the
model’s capacity to navigate and support the complex landscape of learning trajectories.
Example 4 (Memory-based learner) Classical computational models of human memory, such as the
Half-Life Regression (HLR) model (Settles and Meeder, 2016), have been used in machine teaching to model
the long term learning behavior of a human subject. The HLR model posits an exponential decay of memory
over time, where the probability of correctly recalling an item is influenced by the time elapsed since last
reviewed, and the memory strength, quantified as the half-life ( HLθ(·)). A statehtcorresponds to a retention
level and a forgetting rate, and a transition is specified by the half-life dynamics HL:
ht+1←ht·HLθ(xt).
A concrete HLR model studied by Settles and Meeder (2016) calculates the half-life based on a learner’s
interactions with the teaching example, using the feature representation xtand a parameter vector θ, yielding
the estimated half-life as HLθ(xt) = 2θ·xt. This model extends beyond traditional methods like Leitner
(Leitner and Totter, 1972) and Pimsleur (Pimsleur, 1967) systems by empirically fitting θto actual learning
data, accommodating a wider array of features to more accurately mirror a learner’s memory dynamics.
5Under review as submission to TMLR
3.2 The Teacher’s Objective
The teacher’s goal is to help the learner learn as fast as possible, i.e., minimizing the cost of steering the
learner to reach the target knowledge state h⋆. In order to teach, there are two tasks that the teacher needs
to solve, namely estimating θℓand generating the teaching instruction. The entire problem can be formulated
as follows, where c(·,·)is the cost function.
min
x1:T∈XT,T∈Z+E/bracketleftiggT/summationdisplay
t=1c(ht,xt)/bracketrightigg
s.t.hT+1=h⋆andht+1∼Pθℓ(h|ht,xt). (2)
If the teacher knows the true parameters, then the above problem becomes a (stochastic) planning problem.
In this work, we assume that the teacher only knows the parametric form of the learner’s transition function,
and it doesn’t know the true parameters of the learner. This makes our problem formulation more general,
but also introduces an extra challenge in solving the above problem.
4 Preliminaries and Background
Teaching Markov learners can be captured by an MDP M:={H,X,P,c,h0,h⋆,γ}, wherec:H×X→ R+
is the cost function and h⋆is the target knowledge state. For any (h,x,h′)∈H×X×H ,Pθℓ(h′|h,x)
denotes the probability of transiting to knowledge state h′given the teaching instruction xunderh. To
be noted, when the learner reaches the target knowledge state, the cost will be zero for all the teaching
instructions, i.e., c(h⋆,x) = 0,∀x∈X, andP(h⋆|h⋆,x) = 1, which means the target knowledge state is an
absorbing state. γ∈(0,1]is the cost discounting factor. In the teaching context, 1−γis the probability of
the learner transiting to the target knowledge state from any other state, i.e., the probability of epiphany
learning (Dufwenberg et al., 2010; Chen and Krajbich, 2017).
Definition 1 (Proper Policy) A stationary policy πis proper if, given any initial state, the probability of
reaching the goal state gwithin a finite number of steps, when following π, is strictly positive.
In the remaining of this section, we introduce the key assumptions that the subsequent sections rely on. Let
us denote by Πproper (M)the set of stationary polices of the underlying MDP Msuch that for any policy
π∈Πproper (M), the expected time that it takes to reach the target knowledge state h⋆from any initial
knowledge state his finite. In the teaching context, the existence of proper polices for a learner means that
there is a way to teach him/her the target knowledge state h⋆. In our analysis, we will assume that the
Markov learner is linear and teachable under some known and given feature mapping ϕ:H×X×H→ Rd.
We summarize the essential idea in the following assumption. Similar assumption has also been studied in
Zhou et al. (2021); Min et al. (2021).
Assumption 1 (Teachable Linear Markov Learners) M:={H,X,Pθℓ,c,h0,h⋆,γ}is a teachable lin-
ear Markov learner, if it satisfies
•Linearity : Given a known feature mapping ϕ, there exists an unknown parameter θℓ∈Rd(∥θℓ∥2
2≤d)
such that Pθℓ(h′|h,x) =⟨ϕ(h′|h,x),θℓ⟩,∀(h,x,h′)∈H×X×H .
•Teachable : There exists at least one proper policy, i.e., Πproper (M)̸=∅.
Furthermore, for any bounded value function V:H→ [0,C]withC⋆≤C,∥ϕV(h,x)∥2≤√
dCholds for any
(h,x)∈H×X , whereϕV(h,x) =/summationtext
h′ϕ(h′|h,x)V(h′).
For any value function V:H→R+, we define PV(h,x) =/summationtext
h′P(h′|h,x)V(h′)for any (h,x)∈H×X .
Under the linear MDP assumption, we further have PθℓV(h,x) =⟨ϕV(h,x),θℓ⟩. For convenience of notation,
we further define the cost-to-go function for policy πunderMθℓas
Vπ(h|θℓ):= lim
T→+∞E/bracketleftiggT/summationdisplay
t=0c(ht,xt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleh0=h/bracketrightigg
,whereht+1∼Pθℓ(h|ht,xt)andxt=π(ht).
Consequently, the Q-value function of policy πunderMθℓcan be written as
Qπ(h,x|θℓ):=c(h,x) +PθℓVπ(h,x|θℓ). (3)
6Under review as submission to TMLR
Subsequently, when there is no ambiguity, we use V(h)andQ(h,x)to simplify notation. Next, we introduce
another assumption tailored to the teaching setting.
Assumption 2 ( δ0-Closeness) The true parameter θℓisδ0-close to the teacher’s initial estimation θ0, i.e.,
∥θℓ−θ0∥2≤δ0√
dwith 0≤δ0≤1.
The above assumption is natural in the teaching setting. Without such an assumption, the teacher may need
to interact with the learner for a large number of rounds before it can teach in an effective way, which is
impractical for teaching resource-constrained learners, such as humans. In practice, to fulfil Assumption 2, we
can first fit a transition function on the offline teacher-learner interaction data to serve as the initialization.
For simplicity, we denote the associated MDP of a learner with parameter θℓasMθℓand the teacher’s initial
estimation on the parameter as θ0.
Lastly, we define two categories of Markov learners depending on their behaviors during learning, which
can be modelled by undiscounted MDP and discounted MDP, respectively. We call a Markov learners an
epiphany learner ifγ <1for its associated MDP. When the learner’s associated MDP has γ= 1, we call it a
non-epiphany learner .
<latexit sha1_base64="6wlj5dXMUDK7T8paHZe90b96dxo=">AAAB/nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EEsugjWUE8wHJEfY2e8mS3b1jd08Ix4G/wVZrO7H1r1j6T9wkV5jEBwOP92aYmRfEnGnjut9OYWNza3unuFva2z84PCofn7R1lChCWyTikeoGWFPOJG0ZZjjtxopiEXDaCSZ3M7/zRJVmkXw005j6Ao8kCxnBxkrdfiDScTZgg3LFrbpzoHXi5aQCOZqD8k9/GJFEUGkIx1r3PDc2foqVYYTTrNRPNI0xmeAR7VkqsaDaT+f3ZujCKkMURsqWNGiu/p1IsdB6KgLbKbAZ61VvJv7n9RIT3vgpk3FiqCSLRWHCkYnQ7Hk0ZIoSw6eWYKKYvRWRMVaYGBvR0pZAZDYTbzWBddK+qnq1au3hutK4zdMpwhmcwyV4UIcG3EMTWkCAwwu8wpvz7Lw7H87norXg5DOnsATn6xccM5a+</latexit>hi
<latexit sha1_base64="Ze7v9ouRvvpuLVJRIyBSO9tc17k=">AAAB/nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EomXQxjKC+YDkCHubvWTN7t6xuyeE48DfYKu1ndj6Vyz9J26SK0zig4HHezPMzAtizrRx3W+nsLa+sblV3C7t7O7tH5QPj1o6ShShTRLxSHUCrClnkjYNM5x2YkWxCDhtB+Pbqd9+okqzSD6YSUx9gYeShYxgY6VOLxDpKOs/9ssVt+rOgFaJl5MK5Gj0yz+9QUQSQaUhHGvd9dzY+ClWhhFOs1Iv0TTGZIyHtGupxIJqP53dm6EzqwxQGClb0qCZ+ncixULriQhsp8BmpJe9qfif101MeO2nTMaJoZLMF4UJRyZC0+fRgClKDJ9Ygoli9lZERlhhYmxEC1sCkdlMvOUEVknrourVqrX7y0r9Jk+nCCdwCufgwRXU4Q4a0AQCHF7gFd6cZ+fd+XA+560FJ585hgU4X78dxpa/</latexit>hj<latexit sha1_base64="FqZVKibIX8naCbLoUgKPqtVHL9E=">AAAB/nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EEsugjWUE8wHJEfY2e8mS3b1jd08Ix4G/wVZrO7H1r1j6T9wkV5jEBwOP92aYmRfEnGnjut9OYWNza3unuFva2z84PCofn7R1lChCWyTikeoGWFPOJG0ZZjjtxopiEXDaCSZ3M7/zRJVmkXw005j6Ao8kCxnBxkrdfiDScTaYDMoVt+rOgdaJl5MK5GgOyj/9YUQSQaUhHGvd89zY+ClWhhFOs1I/0TTGZIJHtGepxIJqP53fm6ELqwxRGClb0qC5+ncixULrqQhsp8BmrFe9mfif10tMeOOnTMaJoZIsFoUJRyZCs+fRkClKDJ9agoli9lZExlhhYmxES1sCkdlMvNUE1kn7qurVqrWH60rjNk+nCGdwDpfgQR0acA9NaAEBDi/wCm/Os/PufDifi9aCk8+cwhKcr18fWZbA</latexit>hk<latexit sha1_base64="yuG35BpwLVA339JSFHkURpHdu9c=">AAACInicbVDLSgMxFM34rPU16tJNsAi1SJkRqS6LblxWsA/ojEOSpm1oMjMkGbEM8w9+hN/gVtfuxJXgxj8xfSxs64HAuefcy809OOZMacf5spaWV1bX1nMb+c2t7Z1de2+/oaJEElonEY9kCyNFOQtpXTPNaSuWFAnMaRMPrkd+84FKxaLwTg9j6gvUC1mXEaSNFNglTyDdxzitZUUPi7Sf3ZegJ1gHTqqAnY7ZY3YS2AWn7IwBF4k7JQUwRS2wf7xORBJBQ004UqrtOrH2UyQ1I5xmeS9RNEZkgHq0bWiIBFV+Or4pg8dG6cBuJM0LNRyrfydSJJQaCmw6RxeoeW8k/ue1E9299FMWxommIZks6iYc6giOAoIdJinRfGgIIpKZv0LSRxIRbWKc2YJFZjJx5xNYJI2zslspV27PC9WraTo5cAiOQBG44AJUwQ2ogTog4Am8gFfwZj1b79aH9TlpXbKmMwdgBtb3L/UNpHY=</latexit>P(h⇤|hi,x)<latexit sha1_base64="3kTEwMXO16RivEYgptDhMf54Qs0=">AAAB/nicbVA9SwNBEJ3zM8avqKXNYhDEItyJRMugjWUE8wHJGfY2e8mS3b1jd08Ix4G/wVZrO7H1r1j6T9wkV5jEBwOP92aYmRfEnGnjut/Oyura+sZmYau4vbO7t186OGzqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYxuJ37riSrNIvlgxjH1BR5IFjKCjZXa3UCkw+zxvFcquxV3CrRMvJyUIUe9V/rp9iOSCCoN4VjrjufGxk+xMoxwmhW7iaYxJiM8oB1LJRZU++n03gydWqWPwkjZkgZN1b8TKRZaj0VgOwU2Q73oTcT/vE5iwms/ZTJODJVktihMODIRmjyP+kxRYvjYEkwUs7ciMsQKE2MjmtsSiMxm4i0msEyaFxWvWqneX5ZrN3k6BTiGEzgDD66gBndQhwYQ4PACr/DmPDvvzofzOWtdcfKZI5iD8/ULt2OWfg==</latexit>h⇤
<latexit sha1_base64="+QIjkNpZ7ppNxpr8SDHvyT8lHc8=">AAACA3icbVDLSsNAFJ3UV62vqks3g0VwVRKR6rLoxmUF+4AmlMn0ph06M4kzE6GELP0Gt7p2J279EJf+iUmbhW09cOFwzr2cy/EjzrSx7W+rtLa+sblV3q7s7O7tH1QPjzo6jBWFNg15qHo+0cCZhLZhhkMvUkCEz6HrT25zv/sESrNQPphpBJ4gI8kCRonJJM/1RTJOB4kLnKeDas2u2zPgVeIUpIYKtAbVH3cY0liANJQTrfuOHRkvIcowyiGtuLGGiNAJGUE/o5II0F4yezrFZ5kyxEGospEGz9S/FwkRWk+Fn20KYsZ62cvF/7x+bIJrL2Eyig1IOg8KYo5NiPMG8JApoIZPM0KoYtmvmI6JItRkPS2k+CLvxFluYJV0LupOo964v6w1b4p2yugEnaJz5KAr1ER3qIXaiKJH9IJe0Zv1bL1bH9bnfLVkFTfHaAHW1y9PXJkY</latexit>h`
Figure 2: Modeling epiphany learning
as a discounted MDP. The ‘epiphany’ or
sudden leap is illustrated by the dashed
arrows with a light bulb. The solid ar-
rows represent normal transitions between
states. The probability of epiphany at
statehicanbeinterpretedastheprobabil-
ityofreachingthegoalstate P(h∗|hi,x).Epiphany learning (Dufwenberg et al., 2010; Chen and Krajbich,
2017) is an observed phenomenon in human learners, which refers
to sudden insights or realizations of human learners that lead to a
rapid increase in understanding or problem-solving ability. Such
learners may not show gradual improvement but instead have
significant leaps in learning after periods of stagnation or slow
progress. In the context of machine teaching (see figure 2), the
‘epiphany’ or sudden leap in understanding can be viewed as a
significant reward. More specifically, such epiphany is specifically
modeled as direct transitions to the goal state , highlighting its
significant impact on the learning process. The discount factor 1−
γcould model the decreasing likelihood or value of such epiphanies
over time, or the increasing value of immediate, incremental
learning compared to waiting for less predictable, more significant
breakthroughs. In other words, 1−γcan be intuitively understood
as the lower bound of the probability of epiphany learning (i.e.,
transit to the goal knowledge state) at all the knowledge states.
The skill-based learners in table 2 can naturally be considered as
epiphany learners, while the others (e.g., preference-based learners,
gradient learners and memory-based learners) are more suitable to be modeled as non-epiphany learners.
5 Teaching Black-box Markov Learners: Algorithm and Analysis
In this section, we present an algorithm for teaching black-box Markov learners (including epiphany learners
and non-epiphany learners), which takes the initialization into account. We then conduct a rigorous analysis
for upper bounding the teaching cost under different teaching scenarios, including 1) the Markov learner is
linear and teachable; 2) the Markov learner is nonlinear and teachable.
5.1 Black-box Teaching for Linear Markov Learners: Algorithm
We first consider the case where the Markov learner is linear and teachable (see Assumption 1). We first
present an algorithm for solving the teaching problem, which takes the initialization into consideration. The
entire algorithm is built upon solving a regularized least-squares regression (for computing ˆθ), and extended
value iteration (for generating the teaching policy). These two sub-algorithms are often used as backbones
designing RL algorithms (Jaksch et al., 2010; Jin et al., 2020; Zhou et al., 2021; Ouyang et al., 2019; Min
et al., 2021). In contrast to these works, our algorithm 1) takes the initialization θ0into account, which
7Under review as submission to TMLR
Algorithm 1 Black-box Teaching Algorithm for Non-Epiphany and Epiphany Learners.
Require: Initial estimation ˆθ0=θ0, iterationt= 0, EVI index t0= 0,k= 0,Σ0=λI,µ0=λθ0and
discount factor γ(for epiphany learners).
1:Q0←EVI ({θ∈Rd/vextendsingle/vextendsingle∥θ−ˆθ0∥2≤δ0},1
λ,1
λ)
2:whileht̸=h⋆do
3:Providext= arg minx∈XQk(ht,x)
4:Receivect=c(ht,xt);ht+1∼Pθℓ(·|ht,xt).
5:Σt←Σt−1+ϕVk(ht,xt)ϕVk(ht,xt)⊤
6:µt←µt−1+ϕVk(ht,xt)Vk(ht+1)
7:ifdet(Σt)≥2 det( Σtk)ort≥2tk+λthen
8:k←k+ 1
9:tk←t
10:Qk=EVI (Ct,1
λt,1−1
λt)
11:Qk=EVI (Ct,1
λt,γ)
12:end if
13:t←t+ 1
14:end while
is crucial to teaching effectively; 2) and applies to both epiphany and non-epiphany learners. Intuitively,
Algorithm 1 can be divided into two parts as described below.
Parameter learning. For parameter learning, once the updating criteria is satisfied, the teacher will update
its estimation of the learner’s parameter based on the interactions so far. Updating the estimation reduces to
solving the following initialization-regularized least-squares problem:
ˆθm←arg min
θ∈Rdm−1/summationdisplay
t=0[⟨ϕVk(t)(ht,xt),θ⟩−Vk(t)(ht+1)]2+λ∥θ−θ0∥2
2, (4)
wherek(t)is the index of the value function at iteration t, e.g., fortj−1≤t≤tj−1, the index is k(t) =j−1,
andVj(h)is thejthvalue function returned by the extended value iteration (EVI) algorithm (Jaksch et al.,
2010). The above problem has a closed-form solution ˆθm=Σ−1
mµm, where (see also Lines 4&5 in Algorithm 1),
Σm=λI+m−1/summationdisplay
t=0ϕVk(t)(ht,xt)ϕVk(t)(ht,xt)⊤,µm=λθ0+m−1/summationdisplay
t=0ϕVk(t)(ht,xt)Vk(t)(ht+1).
The value of λindicates our confidence on the optimality of the initialization θ0. When the initialization is
very likely close to the true parameter θℓ, we should set a large λ, otherwise we should set a small λ. In
addition,λalso affects the updating frequency of the parameter, which is triggered by two criteria, namely 1)
the log-determinant of Σt; and 2) the number of iterations (see Line 7 in Algorithm 1). When λis larger, the
parameter will be updated less frequently, since we trust our current estimate more. To be noted, in our
analysis, we always assume λ≥12.
Teaching. During the teaching phase, the teacher’s policy is induced by the Q-value function returned by
EVI (see Algorithm 2). After the teacher’s teaching instruction, the teacher will receive a cost incurred by
the teaching instruction, and also observe the learner’s latest knowledge state,
xt= arg min
x∈XQk(t)(ht,xt),wherect=c(ht,xt),ht+1∼Pθℓ(h|ht,xt). (5)
In detail, the EVI algorithm takes the confidence set Ct(see Lemma 1 for t≥1), the error tolerance of the
value iteration ξand the cost discounting factor νas input. The confidence set Ctis an ellipsoid centered at
the current estimation ˆθt. With high probability, the true parameter θℓlies in the intersection of BandCt,
whereBis defined as
B:={θ∈Rd/vextendsingle/vextendsingle⟨ϕ(·|h,x),θ⟩∈∆d,∀(h,x)∈H×X}.
2This is because we found that λ∝1/δ2
0is a good choice in practice (see section 6), and δ0≤1by Assumption 2.
8Under review as submission to TMLR
Algorithm 2 Extended Value Iteration: EVI( C,ξ,ν)
Require: Confidence setC, error tolerance of valute iteration ξ, iterationi= 0, cost discount factor ν.
1:Q(0)(·,·) = 0
2:Q(·,·) = 0
3:V(0)(·) = 0
4:V(−1)(·) = +∞
5:ifC∩B̸ =∅then
6:while∥V(i)−V(i−1)∥∞≥ϵdo
7:Q(i+1)(·,·)←c(·,·) +ν·minθ∈C∩B⟨θ,ϕV(i)(·,·)⟩
8:V(i+1)(·)←minx∈XQ(i+1)(·,x)
9:i←i+ 1
10:end while
11:Q(·,·)←Q(i+1)(·,·).
12:end if
13:returnQ(·,·)
The error tolerance parameter is chosen to be ξ= 1/(λt). Intuitively, the error tolerance will be smaller when
we 1) collect more data (i.e., tbecomes large); and 2) start from a better initialization (i.e., δ0is smaller).
For the cost discount factor ν, we set it to be 1−1/(λt), when the underlying MDP of the Markov learner
is undiscounted (i.e., non-epiphany learners ). By doing so, the cost discount factor νwill become closer to
1as the teaching continues, which helps us avoid a teaching cost that is linear in T(i.e, the total number
of teaching instructions) and also ensures the convergence of EVI. When the learner’s underlying MDP is
discounted (i.e., epiphany learners ), we will set ν=γto be a constant. Intuitively, the cost discount factor ν
captures the probability of epiphany learning.
Overall, the EVI algorithm adapts the standard value iteration algorithm to incorporate the optimism-in-the-
face-of-uncertainty (OFU) principle (see Line 7 in Algorithm 2) proposed by Abbasi-Yadkori et al. (2011),
which has been demonstrated to be effective in online learning settings.
5.2 Theoretical Analysis for the Linear Case
In this section, we analyze the cost upper bounds of using Algorithm 1 for teaching both non-epiphany and
epiphany learners. The core of the algorithm is to build the confidence set that contains the true parameter
θℓ, which balances exploration (parameter learning) and exploitation (teaching). In general, the smaller
the confidence set that we can construct, the lower the cost. In the following, we present Lemma 1, which
provides a confidence set containing θℓwith high probability.
Lemma 1 Under Assumptions 1 and 2, for any t≥1, with probability at least 1−δ, we have that the true
parameterθℓlies in
Ct=/braceleftig
θ∈Rd/vextendsingle/vextendsingle/vextendsingle∥ˆθt−θ∥Σt≤C/radicalbig
dlog ((4(t2+t3C2/λ))/δ) +√
λδ0/bracerightig
. (6)
The confidence set Ctis centered at the current estimation ˆθt. Its radius is computed based on the iteration
t, feature dimension d, regularization parameter λ, the upper bound of the optimal cost C, and the upper
bound on the distance between θ0andθℓ, i.e.,δ0. As expected, the confidence set will become smaller as δ0
decreases, indicating that a good initialization is desired. Now Theorem 1 provides an upper bound on the
cost of teaching non-epiphany learners using Algorithm 1.
Theorem 1 Under Assumptions 1 and 2, if the confidence set Ctis constructed according to Lemma 1 with
C=O(C⋆),λ= 1/δ2
0, and the cost function is bounded from below by cminfor all non-goal knowledge
states (H\{h⋆}) and teaching instruction ( X) pairs, then with probability at least 1−2δ, the teaching cost
of Algorithm 1 for non-epiphany learners (i.e., γ= 1) is upper bounded by
O/parenleftigg/parenleftigg
1 +d/radicalbigg
log/parenleftig
1 +C⋆dδ0
δcmin/parenrightig/parenrightigg
·log1.5/parenleftigC⋆d
cminδ/parenrightig
·C2
⋆d
cmin/parenrightigg
. (7)
9Under review as submission to TMLR
𝒉∗𝒉𝟐𝒉𝟏1-𝛜/𝟐	|𝒙=	𝒙𝟐𝟏.𝟎	|	𝒙=𝒙𝟏𝛜/𝟐|	𝒙=𝒙𝟐𝟎	|	𝒙=𝒙𝟏𝟏-𝛜/𝟐|𝒙=	𝒙𝟐1|𝒙=𝒙𝟏	𝒐𝒓	𝒙𝟐𝛜/𝟐	|𝒙=	𝒙𝟐1|𝒙=	𝒙𝟏	𝟎|𝒙=	𝒙𝟏𝒉∗𝒉𝟐𝒉𝟏1.𝟎|𝒙=𝒙𝟐𝟏-𝛜/𝟐|𝒙=𝒙𝟏0|𝒙=𝒙𝟐𝛜/𝟐|𝒙=𝒙𝟏1|𝒙=	𝒙𝟐1|𝒙=𝒙𝟏	𝒐𝒓	𝒙𝟐0|𝒙=	𝒙𝟐1-𝛜/𝟐	|𝒙=	𝒙𝟏	𝛜/𝟐	|𝒙=	𝒙𝟏Misspecification
Figure 3: An illustration of the failure case under the misspecified setting. The MDP consists of 2 teaching
actionsX={x1,x2}and 3 statesH={h0,h2,h⋆}, and the misspecification level is ϵ. For the teaching
policy induced by the misspecified MDP (right), the learner can get stuck at the state h2with probability
ϵ/2.
The cost upper bound in Theorem 1 has a polynomial dependency on the expected cost of the optimal policy,
C⋆. It’s worth noting that when δ0→0, the purple term inside the parentheses of Equation 7 will vanish
leaving only the constant term 1. The constant term 1is due to the stochasticity in the transition of the
learner’s knowledge states, which is independent of the teaching algorithm used.
Next, we consider the case where the learner is an epiphany learner . Intuitively, epiphany learning can be
interpreted as adding a shortcut from the current knowledge state to the target knowledge state in the
underlying MDP, which is equivalent to the discounted MDP case. The following theorem provides an upper
bound on the cost of teaching epiphany learners.
Theorem 2 Under Assumptions 1 and 2, if the confidence set Ctis constructed according to Lemma 1 with
C=O(C⋆),λ= 1/δ2
0, then with probability at least 1−3δ, the total cost incurred by running Algorithm 1 for
epiphany learners with γ <1, is upper bounded by
O/parenleftigg
C⋆·/parenleftigg
1 +d/radicaligg
log/parenleftbigg
1 +C2⋆δ2
0logδ
logγ/parenrightbigg/parenrightigg
·/radicaligg
logδ
logγlog/parenleftbigg
C⋆logδ
δlogγ/parenrightbigg/parenrightigg
. (8)
Compared with Theorem 1 for non-epiphany learners, the upper bound of the teaching cost for epiphany
learners is linear (ignoring the log factors) in the expected teaching cost of the optimal policy C⋆and the
feature dimension d. Moreover, the dependency on dwill vanish when δ0→0as well. In addition, Theorem 2
does not require the cost function to be bounded from below.
5.3 Theoretical Analysis for the Non-linear Case
In the previous section, we presented the theoretical analysis for both non-epiphany and epiphany learners
when their learning dynamics are linear. One natural follow-up question is: what would happen if the learner’s
dynamics is non-linear, i.e., the linear model is misspecified? To study this problem, we consider the case
where teaching the learner can be approximately modelled as a linear MDP. This idea is captured in the
following assumption.
Definition 2 ( ϵ-Approximate Teachable Markov Learners) For anyϵ∈(0,1], a MDP M=
(H,X,P,c,h0,h⋆,γ)is anϵ-approximate teachable MDP with a feature map ϕ, if there exists a unknown
teachable linear MDP Mθ⋆such that for any (h,x)∈H×X , we have∥P(·|h,x)−⟨ϕ(·|h,x),θ⋆⟩∥TV≤ϵ,
,whereTVdenotes the total variation distance.
By definition, the learner is an ϵ-approximate teachable Markov learner if the learning dynamics function
of the learner is close to a linear transition function under the given feature mapping ϕ. We measure the
closeness between the dynamics functions by the total variation distance.
In general, the algorithm designed for the linear case will fail when the transition function is non-linear.
Specifically, for non-epiphany learners, the teaching cost can be unbounded even for a small model misspecifi-
cation level ϵ. To illustrate this, we present an informal example (see Figure 3), where the teaching policy
10Under review as submission to TMLR
induced by the closest linear MDP to the learner’s MDP will incur an infinite teaching cost. The intuition
behind such counterexamples is that the teaching policy induced by the misspecified MDP will get trapped
in a circle of the true MDP. Fortunately, for epiphany learners, the teaching cost of Algorithm 1 can still be
bounded well, and it is robust to small misspecification levels. The results are stated in the following theorem.
Theorem 3 Forϵ-approximate teachable epiphany learners as defined in Definition 2, if ∥θ0−θ⋆∥2≤δ0,
the cost function is bounded from above by cmax, the confidence set Ctis constructed according to Lemma 1
withC=O(ϵγcmax/(1−γ)2+C⋆), and ifλ= 1/δ2
0, then with probability at least 1−3δ, the teaching cost
incurred by running Algorithm 1 is upper bounded by
O/parenleftigg
C·/parenleftigg
1 +d/radicaligg
log/parenleftbigg
1 +C2δ2
0logδ
logγ/parenrightbigg/parenrightigg
·/radicaligg
logδ
logγlog/parenleftbigg
Clogδ
δlogγ/parenrightbigg
·ϵlogδ
logγC/parenrightigg
. (9)
In contrast to Theorem 2, the major difference is that there is one extra cost term in Theorem 3 due to the
intrinsic bias of the linear approximation. When ϵis sufficiently small, those terms with coefficient ϵcan be
ignored safely, which gives us the following proposition.
Proposition 1 Under the same assumptions as Theorem 3, if ϵ=O/parenleftbig
C⋆(1−γ)2/(γcmax)/parenrightbig
then with proba-
bility at least 1−3δ, the total cost incurred by running Algorithm 1 is upper bounded by
O/parenleftigg
C⋆·/parenleftigg
1 +d/radicaligg
log/parenleftbigg
1 +C2⋆δ2
0logδ
logγ/parenrightbigg/parenrightigg
·/radicaligg
logδ
logγlog/parenleftbigg
C⋆logδ
δlogγ/parenrightbigg
+ϵlogδ
logγC⋆/parenrightigg
.
Hence, as indicated by Theorem 3 and Proposition 1, our algorithm can still attain good theoretical guarantees
when the misspecification level is low.
6 A Numerical Case Study
In this section, we provide a case study on a synthetic learner to illustrate the algorithm. We also evaluate
how the choice of λaffects the empirical teaching cost, as λplays a critical role in our algorithm design.
6.1 Experimental Setup
Knowledge states and teaching instructions . We sample 100 weights {hi}100
i=1uniformly at random
from [−3,3]dto simulate different knowledge states, each of which corresponds to a linear regressor. We then
pick one of the weights to represent the target knowledge state, denoted as h⋆. To generate the teaching
instructions, we first sample 20 points {zi}20
i=1from a normal distribution N(0,I), and their corresponding
labels are generated by yi=⟨h⋆,zi⟩+ζ, whereζ∼N(0,1)is the observation noise. By {xi}20
i=1we denote
the set of teaching instructions, where xi= (zi,yi).
Feature representation . We consider the feature representation for each triplet (h,x,h′)to be
ϕ(h′|h,x) =/bracketleftig
1//parenleftig
Z(1)
(h,x)·∥h′−h+η∇hℓ(h,x)∥2/parenrightig
,1//parenleftig
Z(2)
(h,x)·∥∇hℓ(h,x)∥2/parenrightig/bracketrightig
(10)
whereηis the learning rate, and Z(i)
(h,x)is the normalizing constant for the ithdimension of the feature
representation ϕ(·|h,x). The normalizing constants are used to ensure that/summationtext
h′∈Hϕ(h′|h,x) = (1,1).
Therefore, all the feasible θthat forms a probabilistic distribution lies in a 1-dsimplex. Intuitively, the first
dimension indicates that the learner is more likely to transit to those knowledge states that align well with
the updated knowledge state, i.e., h−η∇hℓ(h,x), whereas the second dimension implies that the learner’s
knowledge state transition will become more random if the teaching instruction is more difficult, which is
measured by the gradient norm ∥∇hℓ(h,x)∥2.
11Under review as submission to TMLR
Random Blackbox Teaching Optimal
Algorithm050100Cost (#Examples)
1 3 5 6 8 10
λ60708090
Figure 4: Left: A comparison between random teaching policy, black-box teaching policy and the optimal
teaching policy in terms of the mean of the averaged teaching cost for 99 initial states; Right: Effect of
different values of λon the teaching cost (computed with the first 10 states to save computation time).
6.2 Empirical Results
Comparing with baselines. We first evaluate the empirical performance of Algorithm 1 under the above
experimental setup. Specifically, we set the learning rate η= 1, and compare it with the random teaching
policy and the optimal policy. We compute the mean of the averaged teaching cost of starting from each
non-goal knowledge state (99 states). The averaged teaching cost is computed with 50 random seeds. The
results are presented in figure 4 left. As expected, the black-box teaching algorithm outperforms the random
teaching policy but underperforms the optimal teaching policy.
How to set λ?The initialization plays an important role in our algorithm design and theoretical analysis.
Our theoretical analysis has demonstrated the impact of the initialization on the teaching cost. However,
given the initialization, it is still unclear how to set the right regularization parameter λ. We conjecture that
the ‘optimal’ λshould be around 1/δ2
0, which is also adopted in our theoretical analysis. To verify this idea,
we study how the choice of λaffects the teaching cost. Under the same setting as above, we vary the value of
λin{1.0,1.5,...,10.0}. To save computation time, we adopt the first 10states to serve as the initial state
and repeat the previous experiments. The results are reported in the right plot of figure 4. The red dashed
line corresponds to the line of x= 1/δ2
0withδ2
0= 0.18. Based on the empirical results, we can observe that
the best choice of λis6, which is close to 1/δ2
0. In addition, if we set λtoo large or too small, the teaching
cost will increase accordingly.
In summary, our experimental results highlight that modelling the learner’s learning dynamics is crucial to
achieve a low teaching cost. Furthermore, given the initialization, setting λ= 1/δ2
0is a reasonable choice for
obtaining good empirical performance.
7 Conclusion
In this paper, we investigate a generic framework for machine teaching, under which the learner’s dynamics can
be represented as an MDP with unknown, learnable parameters. To solve the teaching problem, we introduce
an algorithm that accommodates both epiphany and non-epiphany learners, thus bridging a significant gap
in the current literature. Moreover, we furnish a rigorous analysis of the teaching costs associated with
these two types of learners under disparate settings. Complementing our theoretical insights, we conduct
empirical research to demonstrate the efficiency of our proposed algorithm and provide a guideline for setting
hyperparameters. It is our aspiration that this work will stimulate future research in proposing more nuanced
assumptions about the structure of the learner’s MDP and more efficient algorithms for machine teaching.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.
Advances in neural information processing systems , 24, 2011.
Gordon H Bower. Application of a model to paired-associate learning. Psychometrika , 26(3):255–280, 1961.
12Under review as submission to TMLR
Wei James Chen and Ian Krajbich. Computational modeling of epiphany learning. Proceedings of the National
Academy of Sciences , 114(18):4637–4642, 2017.
Yuxin Chen, Adish Singla, Oisin Mac Aodha, Pietro Perona, and Yisong Yue. Understanding the role of
adaptivity in machine teaching: The case of version space learners. Advances in Neural Information
Processing Systems 32 , 2018.
Albert T Corbett and John R Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge.
User modeling and user-adapted interaction , 1994.
Balázs Csanád Csáji and László Monostori. Value function based reinforcement learning in changing markovian
environments. Journal of Machine Learning Research , 9(8), 2008.
Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. Teaching a black-box learner. In International
Conference on Machine Learning , pages 1547–1555. PMLR, 2019.
Martin Dufwenberg, Ramya Sundaram, and David Butler. Epiphany in the game of 21. Journal of Economic
Behavior & Organization , 75(2):132–143, 2010.
Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. In International Conference
on Learning Representations , 2018. URL https://openreview.net/forum?id=HJewuJWCZ .
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement
learning agents. In International conference on machine learning , pages 1515–1528. PMLR, 2018.
Ziyuan Gao, Christoph Ries, Hans Ulrich Simon, and Sandra Zilles. Preference-based teaching. Journal of
Machine Learning Research , 18:31:1–31:32, 2017.
Sally A Goldman and Michael J Kearns. On the complexity of teaching. Journal of Computer and System
Sciences, 50(1):20–31, 1995.
Anette Hunziker, Yuxin Chen, Oisin Mac Aodha, Manuel Gomez Rodriguez, Andreas Krause, Pietro Perona,
Yisong Yue, and Adish Singla. Teaching multiple concepts to a forgetful learner. In NeurIPS , 2019.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research , 11(51):1563–1600, 2010. URL http://jmlr.org/papers/v11/
jaksch10a.html .
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with
linear function approximation. In Conference on Learning Theory , pages 2137–2143. PMLR, 2020.
S. Leitner and R. Totter. So lernt man lernen . Angewandte Lernpsychologie ein Weg zum Erfolg. Herder,
1972. ISBN 9783451162657.
Laurent Lessard, Xuezhou Zhang, and Xiaojin Zhu. An optimal control approach to sequential machine
teaching. In The 22nd International Conference on Artificial Intelligence and Statistics , pages 2495–2503.
PMLR, 2019.
Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg, and
Le Song. Iterative machine teaching. In International Conference on Machine Learning , pages 2149–2158.
PMLR, 2017.
Weiyang Liu, Bo Dai, Xingguo Li, Zhen Liu, James Rehg, and Le Song. Towards black-box iterative machine
teaching. In International Conference on Machine Learning , pages 3141–3149. PMLR, 2018.
Farnam Mansouri, Yuxin Chen, Ara Vartanian, Xiaojin Zhu, and Adish Singla. Preference-based batch and
sequential teaching: Towards a unified view of models. Advances in Neural Information Processing Systems
32, 2019.
Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine
learners. In AAAI, pages 2871–2877, 2015.
13Under review as submission to TMLR
Yifei Min, Jiafan He, Tianhao Wang, and Quanquan Gu. Learning stochastic shortest path with linear
function approximation. arXiv preprint arXiv:2110.12727 , 2021.
Jill Nugent. iNaturalist: citizen science for 21st-century naturalists. Science Scope , 41(7):12, 2018.
Shayegan Omidshafiei, Dong-Ki Kim, Miao Liu, Gerald Tesauro, Matthew Riemer, Christopher Amato,
Murray Campbell, and Jonathan P How. Learning to teach in cooperative multiagent reinforcement
learning. In Proceedings of the AAAI conference on artificial intelligence , volume 33, pages 6128–6136,
2019.
Yi Ouyang, Mukul Gagrani, and Rahul Jain. Posterior sampling-based reinforcement learning for control of
unknown linear systems. IEEE Transactions on Automatic Control , 65(8):3600–3607, 2019.
Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, and
Jascha Sohl-Dickstein. Deep knowledge tracing. In NIPS, 2015.
Paul Pimsleur. A memory schedule. The Modern Language Journal , 51(2):73–75, 1967.
Anna N Rafferty, Emma Brunskill, Thomas L Griffiths, and Patrick Shafto. Faster teaching via pomdp
planning. Cognitive science , 40(6):1290–1332, 2016.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 , 2014.
Ayon Sen, Purav Patel, Martina A. Rau, Blake Mason, Robert Nowak, Timothy T. Rogers, and Xiaojin Zhu.
Machine beats human at sequencing visuals for perceptual-fluency practice. In EDM, 2018.
Burr Settles and Brendan Meeder. A trainable spaced repetition model for language learning. In ACL, pages
1848–1858, 2016.
Brian L Sullivan, Christopher L Wood, Marshall J Iliff, Rick E Bonney, Daniel Fink, and Steve Kelling. eBird:
A citizen-based bird observation network in the biological sciences. Biological Conservation , 2009.
Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard Schölkopf, and Manuel Gomez-
Rodriguez. Enhancing human learning via spaced repetition optimization. Proceedings of the National
Academy of Sciences , 116(10):3988–3993, 2019.
Jacob Whitehill and Javier Movellan. Approximately optimal teaching of approximately optimal learners.
IEEE Transactions on Learning Technologies , 11(2):152–164, 2017.
Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, and Tie-Yan Liu. Learning to teach
with dynamic loss functions. Advances in Neural Information Processing Systems , 31, 2018.
Michael V Yudelson, Kenneth R Koedinger, and Geoffrey J Gordon. Individualized bayesian knowledge
tracing models. In International conference on artificial intelligence in education , pages 171–180. Springer,
2013.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted mdps
with feature mapping. In International Conference on Machine Learning , pages 12793–12802. PMLR, 2021.
Xiaojin Zhu. An optimal control view of adversarial machine learning. CoRR, abs/1811.04422, 2018.
Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N. Rafferty. An overview of machine teaching. CoRR,
abs/1801.05927, 2018.
Sandra Zilles, Steffen Lange, Robert Holte, and Martin Zinkevich. Models of cooperative teaching and
learning. JMLR, 12(Feb):349–384, 2011.
14Under review as submission to TMLR
A Appendix
In the appendix, we present the proofs of our theorems. The proofs of Lemma 1, Theorem 1, Theorem 2 and
Theorem 3 can be found in sections D, E, F and G, respectively. In section H, we provide a reference to the
existing lemmas that we rely on.
B Additional Discussions
Limitations. First of all, our algorithm requires the knowledge of the upper bound of C⋆andδ0for
setting the hyperparameters. To address this issue, we can use binary search to find a good choice of these
hyperparameters in practice. Secondly, it assumes a pregiven and fixedfeature mapping during the entire
teaching process, which might be suboptimal in reality and lead to biased teaching policies. Lastly, we assume
that the teaching algorithm can observe the learner’s state at every iteration, which might be expensive for
learners such as humans (i.e., one needs to quiz the learner at every iteration).
Future directions. As future directions, it would be exciting to incorporate representation learning into the
entire framework. Besides, instead of assuming the learner’s state to be observable, a partially observable
setting could be considered, under which probing the learner’s state will incur some cost. Therefore, the
algorithm should smartly determine when probing the learner’s state is necessary. In addition, it would
be interesting to propose more fine-grained and learner-specific structural assumptions on the underlying
MDPs. For example, for version space learners, the knowledge states will form a topological order; for
forgetful learners, there will be a non-zero probability to transit back to the previous knowledge states in
the topological graph. By exploiting the structural assumptions, we will design more efficient algorithms
and achieve stronger theoretical guarantees. Lastly, it would be interesting to incorporate ideas from meta
learning for learning the initialization θ0.
C Extended Backgrounds on Various Learner’s Models
Version-space learner. The version space learner was studied by Goldman and Kearns (1995) for machine
teaching. The hypothesis class of the version space learner is usually a finite set H, which contains a target
hypothesis h⋆∈H. The teacher can pick a teaching example from the ground set Xto teach the learner.
Once an example (x,h⋆(x))) is provided to the learner, the learner will update its version space by removing
those hypotheses that are not consistent with the example, i.e., H←H\{h∈H|h(x)̸=h⋆(x)}. Under this
teacher-learner interaction protocol, the teacher *knows* the aforementioned update rule of the learner. The
entire problem is essentially a set cover problem, which is NP-hard. But a greedy-approximation algorithm
admits a teaching complexity of O(log(H)·C⋆), whereC⋆is the optimal teaching cost. The version space
learner can also be regarded as a tabular case of the machine teaching problem, which falls in the Markov
learner case, i.e., a special case of our teaching framework.
Black-box version-space learner. The black-box version-space learner was studied in Dasgupta et al.
(2019). In this framework, they assume the teacher does not know the hypothesis class Hat the beginning,
but the teacher knows the learner’s dynamics rule (i.e., how does the learner update the knowledge state).
Then the teaching problem is equivalent to the *online set cover* problem. The analysis of the online set
cover applies to the analysis of the teaching cost. This work can be regarded as a complement to our work,
as they assume the learner’s model is known, but the state is unknown. Our work assumes the learner’s state
is observable, but the learner’s model is unknown.
Black-box iterative learner. The black-box iterative learner Liu et al. (2017) is in the same philosophy
as Dasgupta et al. (2019). The main difference is that, for the black-box iterative learner, it deals with
gradient-based learner, i.e., the learner updates it by following the gradient descent rule. Therefore, this work
still assumes the learner’s model is known.
Memory-based learner. The memory-based learner was studied in Settles and Meeder (2016); Hunziker
et al. (2019) for modeling the forgetting behavior of human learning. In these works, they used the half-life
15Under review as submission to TMLR
model as a proxy to model the human learner’s model. Specifically, in Hunziker et al. (2019) the teaching
problem was formulated as a submodular maximization problem (maximizing the memorization utility of the
underlying learner) due to the property of the half-life model.
Bayesian knowledge tracing (BKT) learner. As an instance of skill-based learners (Whitehill and
Movellan, 2017), BKT assumes that student knowledge is represented as a set of binary variables, one per
skill, where the skill is either mastered by the student or not. Observations in BKT are also binary: a
student gets a problem/step either right or wrong. The learner’s state is updated by Bayes rule given the
new observation. Hence, the teacher still knows the learner’s model.
D Proof of Lemma 1
Lemma 1 Under Assumptions 1 and 2, for any t≥1, with probability at least 1−δ, we have that the true
parameterθℓlies in
Ct=/braceleftig
θ∈Rd/vextendsingle/vextendsingle/vextendsingle∥ˆθt−θ∥Σt≤C/radicalbig
dlog ((4(t2+t3C2/λ))/δ) +√
λδ0/bracerightig
. (6)
Proof:We prove this by induction on k, which is the index of the value functions returned by EVI. By
definition, the fitted value function in the interval [tk,tk+1−1]isVk(·). To be noted, since when t= 0, we
must haveθℓ∈C0by Assumption 2. Therefore, we abuse the notation a little bit by reloading t0= 1for the
proof. Therefore, we first prove the base step, where t∈[1,t1−1]. For notation simplicity, we define
ϕm=ϕV0(hm,xm),Φt= (ϕ1,....,ϕt),vt= (V0(h2),...,V 0(ht+1))⊤.
Recall the definition of ˆθt, by rewriting it in the matrix form, we get
ˆθt=Σ−1
tbt=Σ−1
t/parenleftigg
λθ0+t/summationdisplay
m=1ϕmV0(hm+1)/parenrightigg
=/parenleftbig
λI+ΦtΦ⊤
t/parenrightbig−1(λθ0+Φtvt)
=/parenleftbig
λI+ΦtΦ⊤
t/parenrightbig−1Φt(vt−Φ⊤
tθ0) +θ0.
=Σ−1
tΦt(vt−Φ⊤
tθ0) +θ0.
Next, we define the following random variables
ηm=V0(sm+1)−⟨ϕm,θℓ⟩,ηt= (η1,...,ηt)⊤.
SinceC≥C⋆, the sequence{ηt}t1
t=1areC-sub-Gaussian. Now, we can rewrite ˆθtas
ˆθt=Σ−1
tΦt/parenleftbig
ηt+Φ⊤
t(θℓ−θ0)/parenrightbig
+θ0
=Σ−1
tΦtηt+Σ−1
tΦtΦ⊤
t(θℓ−θ0) +θ0.
By subtracting θℓon both sides, we get
ˆθt−θℓ=Σ−1
tΦtηt+/parenleftbig
Σ−1
tΦtΦ⊤
t−I/parenrightbig
(θℓ−θ0)
=Σ−1
tΦtηt+Σ−1
t/parenleftbig
ΦtΦ⊤
t−Σt/parenrightbig
(θℓ−θ0)
=Σ−1
tΦtηt+λΣ−1
t(θ0−θℓ).
Then, we further obtain the following by the Cauchy-Schwarz inequality,
/vextenddouble/vextenddouble/vextenddoubleˆθt−θℓ/vextenddouble/vextenddouble/vextenddouble2
Σt=/angbracketleftig
Σt(ˆθt−θℓ),Φtηt/angbracketrightig
Σ−1
t+λ/angbracketleftig
Σt(ˆθt−θℓ),θ0−θℓ/angbracketrightig
Σ−1
t
≤/vextenddouble/vextenddouble/vextenddoubleΣt(ˆθt−θℓ)/vextenddouble/vextenddouble/vextenddouble
Σ−1
t/parenleftig
∥Φtηt∥Σ−1
t+λ∥θ0−θℓ∥Σ−1
t/parenrightig
=/vextenddouble/vextenddouble/vextenddoubleˆθt−θℓ/vextenddouble/vextenddouble/vextenddouble
Σt/parenleftig
∥Φtηt∥Σ−1
t+λ/vextenddouble/vextenddoubleθ0−θℓ/vextenddouble/vextenddouble
Σ−1
t/parenrightig
.
16Under review as submission to TMLR
By Lemma 6 from Abbasi-Yadkori et al. (2011), for any t∈[1,t1], we have the following hold with probability
at least 1−δ/(t1(t1+ 1)),
∥Φtηt∥Σ−1
t≤C/radicaligg
2 log/parenleftbiggdet(Σt)1/2
λd/2·δ/(t1(t1+ 1))/parenrightbigg
≤C/radicaligg
2 log/parenleftbigg(λ+tC2)d/2
λd/2·δ/(t1(t1+ 1))/parenrightbigg
≤C/radicaligg
dlog/parenleftbigg1 +tC2/λ
δ/(t1(t1+ 1))/parenrightbigg
=C/radicaligg
dlog/parenleftbiggt1(t1+ 1) +t·t1(1 +t1)C2/λ
δ/parenrightbigg
.
In the next, we bound ∥θ0−θℓ∥Σ−1
t,
/vextenddouble/vextenddoubleθ0−θℓ/vextenddouble/vextenddouble2
Σ−1
t≤1
λmin(Σt)∥θ0−θℓ∥2
2=1
λ∥θ0−θℓ∥2
2.
Finally, by plugging in the above bounds, we get the desired result for the base step
/vextenddouble/vextenddouble/vextenddoubleˆθt−θℓ/vextenddouble/vextenddouble/vextenddouble
Σt≤C/radicaligg
dlog/parenleftbiggt1(t1+ 1) +t·t1(1 +t1)C2/λ
δ/parenrightbigg
+√
λ∥θ0−θℓ∥2.
Since 2t≥t1, then we have
/vextenddouble/vextenddouble/vextenddoubleˆθt−θℓ/vextenddouble/vextenddouble/vextenddouble
Σt≤C/radicaligg
dlog/parenleftbigg4(t2+t3C2)/λ
δ/parenrightbigg
+√
λδ0. (11)
Let’s suppose that, for any k∈{0,...,n−1}, equation 11 holds for all t∈[tk,tk+1−1]. For the induction
step, we define the following notations for any k∈{0,...,n−1},
˘Vk(·) = min{C,Vk(·)}.
Consequently, for any k∈{0,...,n}andt∈[tk,tk+1−1], we further define
˘Σt=λI+t/summationdisplay
i=1ϕ˘Vk(i)(hi,xi)ϕ˘Vk(i)(hi,xi)⊤,˘µt=λθ0+t/summationdisplay
i=1ϕ˘Vk(i)(hi,xi)˘Vk(i)(hi+1),
In analogy, we reload the definition for ˆθtandηtby
˘θt=˘Σ−1
t˘µt, ηt=˘Vk(t)(ht+1)−⟨ϕ˘Vk(t)(ht,xt),θℓ⟩
By the above definition, it’s easy to verify that {˘ηt}tn
t=1is almost surely C-sub-Gaussian.3Then, we can
apply the Lemma 6 again, and conclude that θℓ∈˘Ctholds with probability at least 1−δ/(tn(tn+ 1))for
anyt∈[tn,tn+1−1]with
˘Ct=/braceleftigg
θ∈Rd/vextendsingle/vextendsingle/vextendsingle∥˘θt−θ∥˘Σt≤C/radicaligg
dlog/parenleftbigg4(t2+t3C2/λ)
δ/parenrightbigg
+√
λδ0/bracerightigg
.
By the optimism principle in Algorithm 2 and the base step of induction, we will have ˘Vk(·) = ˘Vkfor
k∈{0,...,n−1}, which further gives us that ˘Σt=Σt,˘µt=µt,˘ηt=ηtand ˘θt=ˆθtfor allt∈[1,tn+1−1].
3To be noted, without such construction, if the induction step conditions on the base step, there is no guarantee that the
(conditional) distribution of ηtisC-sub-Gaussian. This may prevent us from applying the Lemma 6.
17Under review as submission to TMLR
Consequently, we further have ˘Ct=Ct. Lastly, by applying the union bound over k≥0, we will get that the
probability of the event in Lemma 1 holds is at least
1−/summationdisplay
k=0δ
tk(tk+ 1)≥1−δ.
□
E Proof of Theorem 1
Theorem 1 Under Assumptions 1 and 2, if the confidence set Ctis constructed according to Lemma 1 with
C=O(C⋆),λ= 1/δ2
0, and the cost function is bounded from below by cminfor all non-goal knowledge
states (H\{h⋆}) and teaching instruction ( X) pairs, then with probability at least 1−2δ, the teaching cost
of Algorithm 1 for non-epiphany learners (i.e., γ= 1) is upper bounded by
O/parenleftigg/parenleftigg
1 +d/radicalbigg
log/parenleftig
1 +C⋆dδ0
δcmin/parenrightig/parenrightigg
·log1.5/parenleftigC⋆d
cminδ/parenrightig
·C2
⋆d
cmin/parenrightigg
. (7)
Proof:To prove Theorem 1, we first bound the teaching cost for running Algorithm 1 for Tsteps. Then, we
can derive a bound for T, and plugging it back to obtain the final result.
For anyT, we can decompose the teaching cost into the following
T/summationdisplay
t=0c(ht,xt)≤T/summationdisplay
t=0c(ht,xt)−V0(h0) +C. (12)
By Lemma 3, we know that
−T/summationdisplay
t=1/parenleftbig
Vk(t)(ht)−Vk(t)(ht+1)/parenrightbig
+ 2dClog/parenleftbigg
1 +TC2
λ/parenrightbigg
+Clog/parenleftbigg
1 +2T
λ/parenrightbigg
+V0(h0)≥0.
By adding it to the r.h.s of equation 12, we get
T/summationdisplay
t=0c(ht,xt)≤T/summationdisplay
t=0c(ht,xt)−V0(h0) +T/summationdisplay
t=1/parenleftbig
Vk(t)(ht+1)−Vk(t)(ht)/parenrightbig
+ 2dClog/parenleftbigg
1 +TC2
λ/parenrightbigg
+Clog/parenleftbigg
1 +2T
λ/parenrightbigg
+V0(h0) +C.
By rearranging the above terms, we can get the following terms
T/summationdisplay
t=0c(ht,xt)≤T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PθℓVk(t)(ht,xt)−Vk(t)(ht)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1+T/summationdisplay
t=0/bracketleftbig
Vk(t)(ht+1)−PθℓVk(t)(ht,xt)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
2
+ 2dClog/parenleftbigg
1 +TC2
λ/parenrightbigg
+Clog/parenleftbigg
1 +2T
λ/parenrightbigg
+C.
In the next, it remains to bound 1and2. By Lemma 4, we can bound 1by
1≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2(C+ 1)·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +T
λ/parenrightbigg
+ 1/parenrightbigg
.
18Under review as submission to TMLR
Then, by Lemma 9, we can bound the martingale difference 2, with probability at least 1−δ, by
2≤2C/radicaligg
2Tlog/parenleftbiggT
δ/parenrightbigg
.
By merging the terms, we simplify the upper bound of the teaching cost to be
T/summationdisplay
t=1c(ht,xt)≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 15Cdlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2C/radicaligg
2Tlog/parenleftbiggT
δ/parenrightbigg
,
whereβT=C/radicalbig
dlog ((4(T2+T3C2/λ))/δ)+√
λδ0. Now, it remains to bound T. Since the cost function is
bounded from below by cmin, then we will have
T·cmin≤T/summationdisplay
t=0c(ht,xt).
By replacing the r.h.s. term with the upper bound derived, we get
T·cmin−C≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 15Cdlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2C/radicaligg
2Tlog/parenleftbiggT
δ/parenrightbigg
.
For the terms on the r.h.s, we can loosely bound the second term by
15Cdlog/parenleftbigg
1 +TC2
λ/parenrightbigg
≤8βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
.
Then, we can bound Tby
T≤1
cmin/parenleftigg
12βT/radicaligg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2C/radicaligg
2 log/parenleftbiggT
δ/parenrightbigg/parenrightigg
·√
T+C
cmin.
Using the fact that c≤a√c+b⇒c≤(a+√
b)2fora,b≥0, we have
T≤/parenleftigg
1
c2
min/parenleftigg
12βT/radicaligg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2C/radicaligg
2 log/parenleftbiggT
δ/parenrightbigg/parenrightigg
+/radicalbigg
C
cmin/parenrightigg2
.
By using the inequality (a+b)2≤2a2+ 2b2twice, we get
T≤32
c2
min/parenleftbigg
36β2
Tdlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+C2log/parenleftbiggT
δ/parenrightbigg/parenrightbigg
+2C
cmin.
Plugging in the following upper bound of β2
T,
β2
T≤2C2dlog/parenleftbigg4(T2+T3C2/λ)
δ/parenrightbigg
+ 2λδ2
0d,
Then, we get
T≤32
c2
min/parenleftbigg
72/parenleftbigg
C2d2log/parenleftbigg4T2+ 4T3C2/λ
δ/parenrightbigg
+λδ2
0d2/parenrightbigg
·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+C2log/parenleftbiggT
δ/parenrightbigg/parenrightbigg
+2C
cmin.
19Under review as submission to TMLR
By rearranging the terms, we get
T≤2304C2d2
c2
min·log/parenleftbigg4T2+ 4T3C2/λ
δ/parenrightbigg
·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+2304λd2C2δ2
0
c2
min·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+32C2
c2
min·log/parenleftbiggT
δ/parenrightbigg
+2C
cmin.
Sinceλ= 1/δ2
0, we can get the following bound
T≤4608C2d2
c2
min·log/parenleftbigg4T2+ 4T3C2δ2
0
δ/parenrightbigg
·log/parenleftbig
1 +TC2δ2
0/parenrightbig
+32C2
c2
min·log/parenleftbiggT
δ/parenrightbigg
+2C
cmin.
We now consider the following cases: when δ0≤1/(TC2), we will have, for some universal constant C0,
T≤C0/parenleftbiggC2d2
c2
minlog2/parenleftbiggT
δ/parenrightbigg/parenrightbigg
.
Whenδ0>1/(TC2), we will have, for some universal constant C1,
T≤C1/parenleftbiggC2d2
c2
minlog2/parenleftbiggTC
δ/parenrightbigg/parenrightbigg
.
According to Lemma 5, we arrive at the desired bound for T
T=O/parenleftbiggC2d2
c2
minlog2/parenleftbiggCd
cminδ/parenrightbigg/parenrightbigg
.
BecauseC=O(C⋆)and plugging in the bound for Tinto the original bound, we can finally get the desired
bound for the teaching cost hold with probability at least 1−2δby further applying union bound on the two
events (i.e., Lemma 1 and bounding 2),
/summationdisplay
tc(ht,xt) =O/parenleftigg/parenleftigg
1 +d/radicaligg
log/parenleftbigg
1 +C⋆dδ0
δcmin/parenrightbigg/parenrightigg
·log1.5/parenleftbiggC⋆d
cminδ/parenrightbigg
·C2
⋆d
cmin/parenrightigg
.
□
Lemma 2 Under the same assumptions as Theorem 1, if Algorithm 1 runs for Tsteps, then the total number
of value function updates (i.e., the number of EVI calls) Kis at most
K≤2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +2T
λ/parenrightbigg
.
Proof:The value function update can be triggered by either the determinant criteria ( K1) or the iteration
criteria (K2). We bound each part separately.
Bounding K1: To bound K1, it suffices to bound the determinant of ΣT. By Lemma 7, the fact that
Σ0=λI, and the Assumption 1, we have
det(ΣT)≤(λ+TC2)d.
20Under review as submission to TMLR
Therefore, we can immediately bound K1by
2K1·det(Σ0) = 2K1·λd≤(λ+TC2)d
⇒K1≤2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
.
Bounding K2: To bound K2, we can look at the criteria triggered by it, which immediately gives us that
(1 +λ)·2K2≤T+λ
⇒K2≤log/parenleftbiggT+λ
1 +λ/parenrightbigg
≤log/parenleftbigg
1 +T
λ/parenrightbigg
.
SinceK=K1+K2, we can conclude that
K≤2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +T
λ/parenrightbigg
.
□
Lemma 3 Under the same assumptions as Theorem 1, for any T, the following holds,
T/summationdisplay
t=0/parenleftbig
Vk(t)(ht)−Vk(t)(ht+1)/parenrightbig
≤2dClog/parenleftbigg
1 +TC2
λ/parenrightbigg
+Clog/parenleftbigg
1 +2T
λ/parenrightbigg
+V0(h0).
Proof:By Lemma 2, we can divide the Tsteps intoK+ 1segments, and within each segment, all the steps
share the same value function. Let’s denote the ending step of kthsegment as tk+1−1, then we will have (by
canceling out the intermediate terms)
T/summationdisplay
t=0/parenleftbig
Vk(t)(ht)−Vk(t)(ht+1)/parenrightbig
=K/summationdisplay
k=0Vk(htk)−Vk(htk+1).
By rearranging terms, we can further get
T/summationdisplay
t=0/parenleftbig
Vk(t)(ht)−Vk(t)(ht+1)/parenrightbig
=K−1/summationdisplay
k=0/parenleftbig
Vk+1(htk+1)−Vk(htk+1)/parenrightbig
+K−1/summationdisplay
k=0/parenleftbig
Vk(htk)−Vk+1(htk+1)/parenrightbig
+VK(htK)−VK(htK+1)
=K−1/summationdisplay
k=0/parenleftbig
Vk+1(htk+1)−Vk(htk+1)/parenrightbig
+V0(ht0)−VK(htK) +VK(htK)−VK(htK+1)
=K−1/summationdisplay
k=0/parenleftbig
Vk+1(htk+1)−Vk(htk+1)/parenrightbig
+V0(ht0)−VK(htK+1).
Since the value function is non-negative, then we have
T/summationdisplay
t=1/parenleftbig
Vk(t)(ht)−Vk(t)(ht+1)/parenrightbig
≤K·max
k∥Vk∥∞+V0(h0).
By plugging in the upper bound of Kfrom Lemma 2 and the upper bound of the value function, C, we
finally arrive at
T/summationdisplay
t=1/parenleftbig
Vk(t)(ht)−Vk(t)(ht+1)/parenrightbig
≤2dClog/parenleftbigg
1 +TC2
λ/parenrightbigg
+Clog/parenleftbigg
1 +2T
λ/parenrightbigg
+V0(h0).
□
21Under review as submission to TMLR
Lemma 4 Under the same assumptions as Theorem 1, for any T, we can bound 1by,
1=T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PθℓVk(t)(ht,xt)−Vk(t)(ht)/bracketrightbig
≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2(C+ 1)·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +T
λ/parenrightbigg
+ 1/parenrightbigg
,
whereβT=C/radicalbig
dlog ((4(T2+T3C2/λ))/δ) +√
λδ0.
Proof:First of all, by the fact that Vk(t)(ht) = minx∈XQk(t)(ht,x) =Qk(t)(ht,xt), we have
1=T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PθℓVk(t)(ht,xt)−Qk(t)(ht,xt)/bracketrightbig
.
Let’s suppose that Qk(t)(·,·)is the value function at the lk(t)th value iteration of Algorithm 2, i.e., the last
iteration of the while loop. Then, based on the EVI algorithm, we have
Qk(t)(ht,xt) =c(ht,xt) +ν·min
θ∈Ct∩B⟨θ,ϕV(lk(t)−1)(ht,xt)⟩
=c(ht,xt) +ν·⟨θt,ϕV(lk(t)−1)(ht,xt)⟩
=c(ht,xt) +ν·⟨θt,ϕV(lk(t))(ht,xt)⟩+ν·⟨θt,[ϕV(lk(t)−1)−ϕV(lk(t))](ht,xt)⟩,
whereθt=arg minθ∈Ct∩B⟨θ,ϕV(lk(t)−1)(ht,xt)⟩. By plugging the above equation into 1to replace
Qk(t)(ht,xt), and then rearrange terms, we get
c(ht,xt) +PθℓVk(t)(ht,xt)−Qk(t)(ht,xt)
=c(ht,xt) +PθℓVk(t)(ht,xt)−c(ht,xt)−ν·⟨θt,ϕV(lk(t))(ht,xt)⟩
−ν·⟨θt,[ϕV(lk(t)−1)−ϕV(lk(t))](ht,xt)⟩
=⟨θℓ,ϕV(lk(t))(ht,xt)⟩−ν·⟨θt,ϕV(lk(t))(ht,xt)⟩
−ν·⟨θt,[ϕV(lk(t)−1)−ϕV(lk(t))](ht,xt)⟩
=⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩+ (1−ν)·⟨θt,ϕV(lk(t))(ht,xt)⟩
−ν·⟨θt,[ϕV(lk(t)−1)−ϕV(lk(t))](ht,xt)⟩.
By the termination condition of the EVI algorithm, we have
c(ht,xt) +PθℓVk(t)(ht,xt)−Qk(t)(ht,xt)
≤⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩+ (1−ν)·⟨θt,ϕV(lk(t))(ht,xt)⟩+ν·1
λ·t′
k(t)
≤⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩+ (1−ν)·C+ν
λ·t′
k(t),
wheret′
k(t)is the time step of k(t)thEVI call, we use t′
k(t)instead oftk(t)to avoid ambiguity. Therefore, we
can bound 1by
1≤T/summationdisplay
t=0⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩+ (1−ν)·C+ν
λ·t′
k(t)
=T/summationdisplay
t=0⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩+T/summationdisplay
t=0/parenleftigg
ν
λ·t′
k(t)+ (1−ν)·C/parenrightigg
.
22Under review as submission to TMLR
By the fact that both θℓandθtare inCtand Lemma 1, we must have
∥θℓ−θt∥Σt≤2βt≤2βT.
Together with the Cauchy-Schwartz inequality, we obtain
⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩≤∥θℓ−θt∥Σt·∥ϕV(lk(t))(ht,xt)∥Σ−1
t
≤2∥θℓ−θt∥Σt·∥ϕV(lk(t))(ht,xt)∥Σ−1
t
≤4βT∥ϕV(lk(t))(ht,xt)∥Σ−1
t
In the meantime, we also have
⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩≤C.
Then, since C≤βT, we get
⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩≤min/braceleftig
C,4βT∥ϕV(lk(t))(ht,xt)∥Σ−1
t/bracerightig
.
≤min/braceleftig
βT,4βT∥ϕV(lk(t))(ht,xt)∥Σ−1
t/bracerightig
.
By Lemma 8, we have
T/summationdisplay
t=0⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩
≤4βTT/summationdisplay
t=0min/braceleftig
1,∥ϕV(lk(t))(ht,xt)∥Σ−1
t/bracerightig
≤4βT/radicaltp/radicalvertex/radicalvertex/radicalbtT·/parenleftiggT/summationdisplay
t=0min/braceleftig
1,∥ϕV(lk(t))(ht,xt)∥Σ−1
t/bracerightig/parenrightigg
≤4βT/radicaligg
T·/bracketleftbigg
2dlog/parenleftbiggtr(λI) +TC2d
d/parenrightbigg
−log det(λI)/bracketrightbigg
≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
.
Next, we will bound the other part. By plugging in 1−ν= 1/(λ·t′
k(t)), we have
T/summationdisplay
t=0/parenleftigg
ν
λ·t′
k(t)+ (1−ν)·C/parenrightigg
≤T/summationdisplay
t=0C+ 1
λ·t′
k(t)= (C+ 1)T/summationdisplay
t=01
λ·t′
k(t).
Considering the iteration triggering criteria, we get
t′
k(t)+1≤2t′
k(t)+λ.
Then, we can conclude that
T/summationdisplay
t=0/parenleftigg
ν
λ·t′
k(t)+ (1−ν)·C/parenrightigg
≤K/summationdisplay
k=0(C+ 1)·/parenleftbigg1
λ+1
t′
k/parenrightbigg
≤2(K+ 1)·(C+ 1)
= 2(C+ 1)·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +T
λ/parenrightbigg
+ 1/parenrightbigg
23Under review as submission to TMLR
By combining the two bounds, we get
1≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2(C+ 1)·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +T
λ/parenrightbigg
+ 1/parenrightbigg
.
□
Lemma 5 Suppose that T≥2,a≥1andT≤klog2(aT)for all large enough k. Then, there exists η=η(a)
such thatT≤η·klog2(ak)for all large enough k, i.e.,T=O(klog2(ak)).
Proof:We prove the above lemma by contrapositive. Suppose that there doesn’t exist such an η. Then, we
will have, for all large enough k,
T≥bk·klog2(ak),
where{bk}∞
k=1is a sequence with limk→+∞bk= +∞. The above inequality also implies that
bk≤T
klog2(ak)≤log2(aT)
log2(ak).
Now, let’s consider the following
log2(aT)≤log2(ak·log2(aT)) = (log(ak) + log log2(aT))2.
By the inequality (a+b)2≤2a2+ 2b2, we get
log2(aT)≤2 log2(ak) + 2 log2(log2(aT)).
SinceaT≥2, we will have
log2(log2(aT))≤1
4log2(aT)⇒ log2(aT)≤2 log2(ak) +1
2log2(aT).
Therefore, we can get
1
2log2(aT)≤2 log2(ak)⇒bk≤log2(aT)
log2(ak)≤4,
which leads to a contradiction with limk→+∞bk= +∞. Hence, we have T=O(klog2(ak)). □
F Proof of Theorem 2
Theorem 2 Under Assumptions 1 and 2, if the confidence set Ctis constructed according to Lemma 1 with
C=O(C⋆),λ= 1/δ2
0, then with probability at least 1−3δ, the total cost incurred by running Algorithm 1 for
epiphany learners with γ <1, is upper bounded by
O/parenleftigg
C⋆·/parenleftigg
1 +d/radicaligg
log/parenleftbigg
1 +C2⋆δ2
0logδ
logγ/parenrightbigg/parenrightigg
·/radicaligg
logδ
logγlog/parenleftbigg
C⋆logδ
δlogγ/parenrightbigg/parenrightigg
. (8)
Proof:The proof for the epiphany learner case mostly follows from the proof of the non-epiphany learner
case, i.e., Theorem 1. In the same way, we can still decompose the cost as in Theorem 1. The only differences
are in the bound of 1in and the upper bound on T.
T/summationdisplay
t=0c(ht,xt)≤T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PθℓVk(t)(ht,xt)−Vk(t)(ht)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1+T/summationdisplay
t=0/bracketleftbig
Vk(t)(ht+1)−PθℓVk(t)(ht,xt)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
2
+ 2dClog/parenleftbigg
1 +TC2
λ/parenrightbigg
+Clog/parenleftbigg
1 +2T
λ/parenrightbigg
+C.
24Under review as submission to TMLR
In analogy to Lemma 4, we can get the following bound for 1,
1=T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PθℓVk(t)(ht)−Vk(t)(ht)/bracketrightbig
≤T/summationdisplay
t=0⟨θℓ−θt,ϕV(lk(t))(ht,xt)⟩+T/summationdisplay
t=0/parenleftigg
γ
λ·t′
k(t)+ (1−γ)·C/parenrightigg
≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+T/summationdisplay
t=0/parenleftigg
γ
λ·t′
k(t)+ (1−γ)·C/parenrightigg
.
In the following, we will bound the r.h.s term in the above equation in a similar way to the proof in Lemma 4,
T/summationdisplay
t=0/parenleftigg
γ
λ·t′
k(t)+ (1−γ)·C/parenrightigg
≤K/summationdisplay
k=0γ·/parenleftbigg1
λ+1
t′
k/parenrightbigg
+ (1−γ)·T·C
≤2γ·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +T
λ/parenrightbigg/parenrightbigg
+ (1−γ)·T·C.
Then, by plugging in the above bounds, we get
1≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2γ·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +T
λ/parenrightbigg/parenrightbigg
+ (1−γ)·T·C.
The bound for 2in Theorem 1 still holds with probability at least 1−δ. Hence, we can merge all the terms
and simply them into
T/summationdisplay
t=0c(ht,xt)≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 9Cdlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2C/radicaligg
2Tlog/parenleftbiggT
δ/parenrightbigg
+ (1−γ)·T·C+C
=O/parenleftbigg
C·/parenleftbigg
1 +d/radicalig
log (1 +TC2δ2
0)/parenrightbigg
·/radicalbig
T·log (TC/δ ) + (1−γ)·T·C/parenrightbigg
In the next, it’s easy to show that, with probability at least 1−δ, the following holds4
T=O(log(δ)/log(γ)).
Lastly, since C=O(C⋆), and plugging in the value of T, we have the following hold with probability at least
1−3δby applying the union bound over the three events (i.e., Lemma 1, bounding 2and bounding T),
/summationdisplay
t=0c(ht,xt) =O/parenleftigg
C⋆·/parenleftigg
1 +d/radicaligg
log/parenleftbigg
1 +C2⋆δ2
0logδ
logγ/parenrightbigg/parenrightigg
·/radicaligg
logδ
logγlog/parenleftbiggC⋆logδ
δlogγ/parenrightbigg/parenrightigg
.
□
G Proof of Theorem 3
Theorem 3 Forϵ-approximate teachable epiphany learners as defined in Definition 2, if ∥θ0−θ⋆∥2≤δ0,
the cost function is bounded from above by cmax, the confidence set Ctis constructed according to Lemma 1
4Without the loss of generality, we assume log(δ)/log(γ)≥1.
25Under review as submission to TMLR
withC=O(ϵγcmax/(1−γ)2+C⋆), and ifλ= 1/δ2
0, then with probability at least 1−3δ, the teaching cost
incurred by running Algorithm 1 is upper bounded by
O/parenleftigg
C·/parenleftigg
1 +d/radicaligg
log/parenleftbigg
1 +C2δ2
0logδ
logγ/parenrightbigg/parenrightigg
·/radicaligg
logδ
logγlog/parenleftbigg
Clogδ
δlogγ/parenrightbigg
·ϵlogδ
logγC/parenrightigg
. (9)
Proof:The proof for ϵ-approximate teachable epiphany learner also follows from the proof of Theorem 1 and
Theorem 2. However, to make the similar proof work, we have to bound the maximum value of the value
function underMθ⋆. To show this, by Lemma 10 and Definition 2, we have
∥V⋆(·|θ⋆)−V⋆(·)∥∞≤γcmaxϵ
(1−γ)2,
where we use V⋆(·|θ⋆)andV⋆(·)to denote the optimal value function under the approximate MDP Mθ⋆
and the true MDP M, respectively. Therefore, we can conclude that
∥V⋆(·|θ⋆)∥∞≤C⋆+γcmaxϵ
(1−γ)2.
Together with the optimism principle in Algorithm 2, recall that
ηt=Vk(t)−⟨ϕVk(t)(ht,xt),θ⋆⟩.
We will have ηtis(C⋆+γcmaxϵ
(1−γ)2)-sub-Gaussian. Therefore, by choosing C=C⋆+γcmaxϵ
(1−γ)2as assumed, we will
have the following holds with probability at least 1−δby following the same proof as in Lemma 1,
θ⋆∈Ct∩B.
Condition on the above event, the same teaching cost decomposition in Theorem 1 still holds,
T/summationdisplay
t=0c(ht,xt)≤T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PθℓVk(t)(ht,xt)−Vk(t)(ht)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1+T/summationdisplay
t=0/bracketleftbig
Vk(t)(ht+1)−PθℓVk(t)(ht,xt)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
2
+ 2dClog/parenleftbigg
1 +TC2
λ/parenrightbigg
+Clog/parenleftbigg
1 +2T
λ/parenrightbigg
+C.
To bound 1, the idea is similar to Lemma 4. Due to the model misspecification, there will be one additional
term in the bound,
T/summationdisplay
t=0c(ht,xt) +PVk(t)(ht,xt)−Qk(t)(ht,xt)
=T/summationdisplay
t=0c(ht,xt) +PVk(t)(ht,xt)−Pθ⋆Vk(t)(ht,xt) +Pθ⋆Vk(t)(ht,xt)−Qk(t)(ht,xt)
=T/summationdisplay
t=0/parenleftbig
c(ht,xt) +Pθ⋆Vk(t)(ht,xt)−Qk(t)(ht,xt)/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
♣+ [P−Pθ⋆]Vk(t)(ht,xt)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
♥.
The bound of the ♣term is still the same as it in Theorem 2, and the bound for the term ♥is
[P−Pθ⋆]Vk(t)(ht,xt)≤C·ϵ.
By putting the two bounds together we get
1≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2γ·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ log/parenleftbigg
1 +T
λ/parenrightbigg/parenrightbigg
+ (1−γ)·T·C+ϵ·T·C.
26Under review as submission to TMLR
The bound for 2in Theorem 1 still holds here with probability at least 1−δ. Hence, we can merge all the
terms and simply them into
T/summationdisplay
t=0c(ht,xt)≤4βT/radicaligg
2Td·log/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 9Cdlog/parenleftbigg
1 +TC2
λ/parenrightbigg
+ 2C/radicaligg
2Tlog/parenleftbiggT
δ/parenrightbigg
+ (1−γ)·T·C+C+ϵ·T·C
Following the proof in Theorem 2, we have the following hold with probability at least 1−δ,
T=O(log(δ)/log(γ)).
By applying the union bound for the three events (i.e., Lemma 1, bounding 2and bounding T), and
plugging in the above TandC=O(C⋆+γcmaxϵ
(1−γ)2), we can get the final bound for the teaching cost, with
probability at least 1−3δ,
T/summationdisplay
t=0c(ht,xt) =O/parenleftigg
C·/parenleftigg
1 +d/radicaligg
log/parenleftbigg
1 +C2δ2
0logδ
logγ/parenrightbigg/parenrightigg
·/radicaligg
logδ
logγlog/parenleftbiggClogδ
δlogγ/parenrightbigg
+ϵlogδ
logγC/parenrightigg
.
□
H Additional Theorems and Lemmas
Lemma 6 (Abbasi-Yadkori et al. (2011)) Let{Ft}∞
t=0be a filtration. Let {ηt}∞
t=1be a real-valued
stochastic process such that ηtisFt-measurable and ηtis conditionally B-sub-Gaussian. Let {ϕt}∞
t=1be an
Rd-valued stochastic process such that ϕtisFt−1-measurable. Assume that Σis ad×dpositive definite
matrix. For any t≥0, define
Σt=Σ+t/summationdisplay
i=1ϕiϕ⊤
i,st=t/summationdisplay
i=1ηiϕi.
Then, for any δ>0, with probability at least 1−δ, for allt≥0,
∥Σ−1/2
tst∥2≤B/radicaligg
2 log/parenleftbiggdet(Σt)1/2
δ·det(Σ)1/2/parenrightbigg
.
Lemma 7 (Abbasi-Yadkori et al. (2011)) Suppose that ϕ1,...,ϕt∈Rdand for any 1≤s≤t, we have
∥ϕs∥≤L. Let Σt=λI+/summationtextt
s=1ϕsϕ⊤
sfor someλ>0. Then,
det(Σt)≤(λ+tL2/d)d.
Lemma 8 (Abbasi-Yadkori et al. (2011)) Let{ϕt}∞
t=1be in Rd, and∥ϕt∥≤Lfor anyt. Then, for
Σt=λI+/summationtextt
s=1ϕsϕ⊤
s, we will have
t/summationdisplay
s=1min/braceleftig
1,∥ϕs∥Σ−1
s−1/bracerightig
≤2/bracketleftbigg
dlog/parenleftbiggtr(λI) +tL2
d/parenrightbigg
−log det(λI)/bracketrightbigg
.
Lemma 9 (Min et al. (2021)) For a transition function P, a sequence of bounded and non-negative value
functions{Vk}K
k=1under P, and a state action sequence {(ht,xt)}T
t=1, where∥Vk∥∞≤Candht+1∼
P[·|ht,xt], we have the following hold with probability at least 1−δ,
T/summationdisplay
t=0/bracketleftbig
Vk(t)(ht)−PVk(t)(ht,xt)/bracketrightbig
≤2C/radicaligg
2Tlog/parenleftbiggT
δ/parenrightbigg
.
27Under review as submission to TMLR
Lemma 10 (Csáji and Monostori (2008)) For two discounted MDPs with discounting factor γ, if they
differ only in the transition functions, denoted by P1andP2. If their corresponding optimal value functions
areV⋆
1andV⋆
2, respectively, and the cost function is bounded from above by cmax, then
∥V⋆
1−V⋆
2∥∞≤γcmax
(1−γ)2∥P1−P2∥∞.
28