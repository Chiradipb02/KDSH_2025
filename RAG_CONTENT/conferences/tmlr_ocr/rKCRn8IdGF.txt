Under review as submission to TMLR
Empowering Clinicians with Medical Decision Transformers:
A Framework for Sepsis Treatment
Anonymous authors
Paper under double-blind review
Abstract
Offline reinforcement learning has shown promise for solving tasks in safety-critical settings,
such as clinical decision support. Its application, however, has been limited by the lack of
interpretability and interactivity for clinicians. To address these challenges, we propose the
medical decision transformer (MeDT) , a novel and versatile framework based on the goal-
conditioned reinforcement learning paradigm for sepsis treatment recommendation. MeDT
uses the decision transformer architecture to learn a policy for drug dosage recommendation.
During offline training, MeDTutilizes collected treatment trajectories to predict adminis-
tered treatments for each time step, incorporating known treatment outcomes, target acuity
scores, past treatment decisions, and current and past medical states. This analysis en-
ablesMeDTto capture complex dependencies among a patient’s medical history, treatment
decisions, outcomes, and short-term effects on stability. Our proposed conditioning uses
acuity scores to address sparse reward issues and to facilitate clinician-model interactions,
enhancing decision-making. Following training, MeDTcan generate tailored treatment rec-
ommendations by conditioning on the desired positive outcome (survival) and user-specified
short-term stability improvements. We carry out rigorous experiments on data from the
MIMIC-III dataset and use off-policy evaluation to demonstrate that MeDTrecommends
interventions that outperform or are competitive with existing offline reinforcement learning
methods while enabling a more interpretable, personalized and clinician-directed approach.
1 Introduction
Sepsis is a fatal medical condition caused by the body’s extreme response to an infection. Due to the
rapid progression of this disease, clinicians often face challenges in choosing optimal medication dosages.
Hence, there is significant interest in developing clinical decision support systems that can help healthcare
professionalsinmakingmoreinformeddecisions(Suttonetal.,2020). Inthemedicalfield, manytasksinvolve
sequential decision-making, such as evaluating a patient’s evolving condition in the intensive care unit (ICU)
to make informed medical interventions. This is where reinforcement learning (RL) comes in as a promising
solution for developing policies that recommend optimal treatment strategies for septic patients (Raghu
et al., 2017; Komorowski et al., 2018; Killian et al., 2020; Saria, 2018; Huang et al., 2022).
These tools are intended to bolster and assist healthcare workers rather than replace them (Gottesman
et al., 2018). Therefore, the reward function employed by these RL algorithms ideally necessitates clinician
input to ensure that the policy generates decisions aligned with the domain expert’s intentions (Gottesman
et al., 2019). However, the majority of existing studies predominantly depend on binary reward functions,
signifying the patient’s mortality (Komorowski et al., 2018; Killian et al., 2020; Tang et al., 2022). In other
words, the reward at each timestep in the patient’s history remains zero until the final interval of the episode.
This design leaves no room for clinician input to modulate the policy toward the achievement of desirable
tasks, such as the stabilization of certain vital signs.
Existing works (Killian et al., 2020; Lu et al., 2020; Li et al., 2019) often rely on modeling the patient’s
medical history using recurrent neural networks (RNNs). These networks struggle with complex and long
medical records due to vanishing or exploding gradients (Pascanu et al., 2013), leading to sub-optimal RL
1Under review as submission to TMLR
policies (Parisotto et al., 2020). Sparse rewards also pose challenges in the learning of optimal policies since
it can be difficult to identify a causal relationship between an action and a distant reward (Sutton & Barto,
2018). The sequential design of RNNs aggravates this problem. The low interpretability of model reasoning
is another problem, given the high-stakes nature of clinical decision making. It is essential to address
these challenges to create reliable decision support systems and improve clinical uptake of machine learning
solutions. Transformers (Vaswani et al., 2017) are shown to effectively model long sequences, which enables
learning of better representations for treatment histories of patients, potentially yielding more informed
predictions.
Inthispaper, weproposethe medical decision transformer (MeDT) ,anofflineRLframeworkwheretreatment
dosage recommendation for sepsis is framed as a sequence modeling problem. MeDT, as shown in Figure 1, is
based on the decision transformer (DT) architecture (Chen et al., 2021). It recommends optimal treatment
dosages by autoregressively modeling a patient’s state while conditioning on hindsight returns. To provide
the policy with more informative and goal-directed input, we also condition MeDTon one-step look-ahead
patient acuity scores (Le Gall et al., 1993) at every time-step. This enhances the potential for more granular
conditioning while facilitating the interaction of domain experts with the model.
Below we summarize the main contributions of this work:
•We propose MeDT, a transformer-based policy network that models the full context of a patient’s
clinical history and recommends medication dosages.
•We develop a framework to enable clinicians to guide the generation of treatment decisions by
specifying short-term target improvements in patient stability, which addresses the sparse reward
issue.
•We demonstrate that MeDToutperforms or is competitive with popularly used offline RL baselines
over multiple methods of off-policy evaluation (OPE) such as fitted Q-evaluation (FQE), weighted
doubly robust (WDR) and weighted importance sampling (WIS). Additionally, we leverage a trans-
former network, the state predictor , to serve as an approximate model to capture the evolution of
a patient’s clinical state in response to treatment. This model enables autoregressive inference of
MeDTand also serves as an interpretable evaluation framework of models used for clinical dosage
recommendation.
2 Related Work
2.1 RL for Sepsis Treatment
The use of RL in sepsis treatment aims to deliver personalized, real-time decision support. It involves
modeling optimal strategies for the administration of treatments, such as vasopressors (VPs) and intravenous
fluids (IVs), based on patient data and expert advice. This problem poses a considerable challenge due to the
potential for long-term effects associated with these treatments, such as the accumulation of interstitial fluid
and subsequent organ dysfunction resulting from excessive fluid administration (Gottesman et al., 2018).
To address this issue, Komorowski et al. (2018) propose a value-iteration algorithm using discretized patient
data from electronic health records (EHRs) for treatment action selection. Subsequent work uses Q-learning
withcontinuousstatesanddiscreteactionsandemploysOPEtoevaluatepolicies(Raghuetal.,2017). Huang
et al. (2022) uses deep deterministic policy gradient (DDPG) with continuous states and actions to provide
precise dosage recommendations. Other works explore model-based RL (Peng et al., 2018) and combined
deep RL with kernel-based RL (Raghu et al., 2018) to further improve treatment recommendations for
septic patients. Yet, several significant issues still need to be resolved, which currently impede the practical
implementation of RL for the treatment of sepsis. Most of these studies assume that agents begin with a
baseline reward of zero until the end of treatment. At the final time-step in a patient’s history, a positive
reward is given for survival and a negative reward otherwise. Since the manifestation of treatment outcomes
(mortality) can occur with a delay of several days after decisions are made, it is challenging to identify
effective treatment strategies. Shorter-term objectives, such as the stabilization of vital signs, are often
2Under review as submission to TMLR
Figure 1: MeDTtraining: At each time-step t, theMeDTpolicy attends to the past treatment trajectory.
This includes the desired treatment outcome r(at inference time fixed to +1indicating survival), desired
next-step acuity scores k1,...,ktwherekt= (kct,krt,knt,klt,kht,kmt,kot), patient states s1,...,st, ad-
ministered drug doses a1,...,at−1, and outputs a dose prediction ˆat.
overlooked. Additionally, given the wealth of data being generated for each ICU patient, identifying the
most relevant aspects in the treatment history may not be immediately apparent (Gottesman et al., 2018).
2.2 Transformer-based Policies
Another challenge in treatment modeling is introduced by the partial observability of the patient’s state
at each time-step. A single reading of vital signs provides incomplete information on the patient’s well-
being. RNNs address this issue by sequentially processing multiple time-steps of data, but face difficulties
in capturing a patient’s complete state history due to unstable gradients (Pascanu et al., 2013). This may
result in incomplete information and, consequently, inaccurate decision-making (Yu et al., 2021). Recent
research in RL (Parisotto et al., 2020; Parisotto & Salakhutdinov, 2021; Janner et al., 2021; Tao et al., 2022)
is shifting towards attention-based networks (Niu et al., 2021) like transformers (Vaswani et al., 2017; Lin
et al., 2022), which process information from past time-steps in parallel.
Transformers better capture long contexts and can be effectively trained in parallel (Lin et al., 2022). This
addresses challenges posed by the sequential processing in RNNs (Wen et al., 2022). The self-attention mech-
anism in transformers is particularly beneficial, addressing issues related to sparse or distracting rewards.
Self-attention, in short, first computes attention weights for information in each time-step by matching their
corresponding keysandqueries, which are learnable projections of input tokens. Afterwards, these weights
are used to compute weighted sums of valuescorresponding to each time-step, potentially discovering depen-
dencies between distant time-steps. DT (Chen et al., 2021) leverages these advantages for offline RL (Furuta
et al., 2021; Xu et al., 2022; Meng et al., 2021), by conditioning a policy on the full history of states, ac-
tions and an observed or desired reward-to-go . Building on the DT architecture, we propose MeDT, which
integrates additional conditioning via short-term goals for improvements in patient vital signs, yielding a
framework for effective sepsis treatment recommendation.
2.3 Off-Policy Evaluation
OPE is a fundamental problem in RL concerned with estimating the expected return of a given decision
policy using historical data obtained by different behavior policies (Uehara et al., 2022). Such an evaluation
strategyisparticularlyusefulinsituationswhereinteractingwiththeenvironment iscostly, risky, or ethically
challenging, like in healthcare (Sutton & Barto, 2018; Precup, 2000; Gottesman et al., 2020). However, OPE
is inherently difficult because it necessitates counterfactual reasoning, i.e. unraveling what would have
3Under review as submission to TMLR
Figure 2: Autoregressive evaluation pipeline: At each time-step t, the pre-trained state predictor attends
to past recommended doses ˆa1,..., ˆat, the initial patient state s1and predicted patient states ˆs2,..., ˆst,
and outputs a prediction ˆst+1of the patient state at time t+ 1. Both dosage recommendations ˆat+1and
predicted states are fed back to MeDTto simulate treatment trajectories with multiple sequential decisions.
occurred if the agent had acted differently based on historical data. Nevertheless, while OPE may not
necessarily help learn the optimal policy, it can help identify policies with lower suboptimality (Tang &
Wiens, 2021).
A large subset of OPE methods are based on the concept of importance sampling (IS). IS uses validation
data to assess the evaluation policy’s value by adjusting the weight of each episode based on its relative
likelihood (Păduraru et al., 2013; Voloshin et al., 2019). The WIS estimator is considered more stable
than IS (Păduraru et al., 2013; Voloshin et al., 2019). On the other hand, we can directly estimate the
evaluationpolicy’sperformanceusingtheQ-functionwithFQE,ratherthanadjustingtheweightsofobserved
experiences like WIS (Le et al., 2019). FQE predicts the expected cumulative reward for taking a specific
action in a given state. WDR is another OPE technique that combines two approaches for estimating policy
value (Thomas & Brunskill, 2016; Jiang & Li, 2016). It leverages importance sampling, which adjusts the
weight of past experiences based on their likelihood. Additionally, WDR incorporates value estimates at each
step to improve accuracy and reduce overall variation in the learning process. Finally, approximate models
(AMs) are another class of OPE that involves directly modeling the dynamics of the environment (Jiang
& Li, 2016; Voloshin et al., 2019). This approximation, while not exact, may be sufficient to evaluate the
policy’s performance. Tang & Wiens (2021) empirically demonstrated that FQE provided the most accurate
and stable estimations over varying data conditions. Given the difficult nature of evaluation in this problem
setting, we utilize each of the four mentioned OPE methods to infer rigorous and robust policy evaluations.
2.4 Interpretability
The need for interpretability is more significant in safety-critical fields such as healthcare (Amann et al.,
2020). Despiteextensiveresearch, thedeploymentofdeeplearninginhealthcarehasbeenmetwithresistance
(Yin et al., 2021). This is primarily due to the black-box nature of these networks, resulting from their
complexity and large number of parameters. Moreover, attaining interpretability in RL has been a major
4Under review as submission to TMLR
Figure3: (a)Dosagerecommendedby MeDTandclinicianpolicyfordifferentSAPS2scores. (b)Distribution
of IV fluids and VPs given by the MeDTand clinician policies.
challenge hindering its deployment (Agarwal et al., 2023). Owing to their complexity, existing RL algorithms
fall short of being fully interpretable (Glanois et al., 2021).
One simple method of attempting to understand the inner workings of transformers, is to visualize the
computed attention weights. However, Serrano & Smith (2019) show that attention weights only produce
noisy predictions of the relevance of each input token. Recent works delve into formulating methods that
more representatively capture the relevance of input tokens. Abnar & Zuidema (2020) propose the rollout
method, which considers paths over the pairwise attention graph while assuming that attention is computed
linearly. However, this work is shown to assign importance to irrelevant tokens (Chefer et al., 2021b).
Chefer et al. (2021b) introduce a method based on layer-wise relevance propagation, which is effective for
encoder transformers. Chefer et al. (2021a) present a generic interpretability method that is compatible with
every type of transformer architecture. The proposed method relies on the concept of information flow. It
involves monitoring the mixing and evolution of attention to generate representative heatmaps, illustrating
the importance assigned to input tokens in the model’s decision. This method produces interpretations that
are similarly or more accurate than prior methods while being simpler to implement. In this work, we utilize
this method to generate interpretations of MeDTand visualize the relevance assigned in the input space to
aid clinicians in understanding the rationale behind the model’s decision-making.
3 Medical Decision Transformer (MeDT)
We frame our problem as a Markov decision process (MDP), comprising a tuple (S,A,P,R,S′), where
Sdenotes the set of possible patient states, Athe set of possible dosage recommendations, Pthe state
transition function, Rthe reward function and S′is the next patient state. While this framework is well-
suited for learning policies via trial-and-error using RL methods, direct interaction with the environment
can be risky in safety-critical applications like clinical decision making. To mitigate this risk, we use offline
RL, a subcategory of RL that learns an optimal policy using a fixed dataset of collected trajectories each
containing the selected actions, observed states and obtained rewards.
DT(Chenetal.,2021)usestransformerstomodelofflineRLviaanupside-downRLapproach, whereapolicy
hastoselectactions, thatarelikelytoyieldaspecifiedfuturerewardforagivenpasttrajectory(Schmidhuber,
2019). Our proposed MeDTarchitecture follows a similar approach for learning policies.
The input tokens for the policy model encode past treatment decisions and patient states, as well as the
desiredreturns-to-go(RTG).Theoutputateachtime-stepisadistributionoverpossibleactions. Specifically,
we condition the model using RTG rt=/summationtextT
t′=tRt′=RT, which represents the singular positive or negative
treatment outcome at the last time-step, similar to DT. In addition, we propose to condition MeDTon
short-term goals, such as future patient acuity scores, or acuity-to-go (ATG). The acuity score provides
an indication of the severity of the illness of the patient in the ICU based on the status of the patient’s
physiologicalsystemsandcanbeinferredfromvitalsignsofthecorrespondingtime-step. Higheracuityscores
indicate a higher severity of illness. In this work, we opt to use the SAPS2 (Le Gall et al., 1993) acuity score
as opposed to popular scores such as SOFA (Jones et al., 2009). This is because SAPS2 considers relatively
5Under review as submission to TMLR
more physiological variables, which we believe will provide more informative conditioning, allowing flexible
user interactions (Morkar et al., 2022). This formulation allows clinicians to input desired acuity scores for
the next state, providing additional context for treatment selection. This leads to more information-dense
conditioning, allowing clinicians to interact with the model and guide the policy’s generation of treatment
dosages.
To enable clinicians to provide more detailed inputs, we break down the SAPS2 score into constituent
scores that correspond to specific organ systems (Le Gall et al., 1993). Following the definitions provided
by Schlapbach et al. (2022), we define split scores k= (kc,kr,kn,kl,kh,km,ko) to represent the status
of the cardiovascular, respiratory, neurological, renal, hepatic, haematologic and other systems, respectively.
A more detailed breakdown of these scores can be found in the Appendix in Table ??.
In addition to the specification of the treatment outcome, our overall framework empowers domain experts
to establish and select a scheme for interpretable short-term goal-conditioning, allowing clinicians to guide
the model using their knowledge of the relation between intermediate goals, such as maintaining patients’
vital signs within a specified range, and favorable long-term outcomes such as reduced mortality. This
enhances the usability of the model for clinicians, enabling efficient interaction with the model for future
dosage recommendations, considering the current state of the patient’s condition. It is important to note
that defining short-term goals presents a challenge, given the ongoing complexity in determining ideal targets
for sepsis resuscitation (Simpson et al., 2017). Using these scores, the treatment progress over Ttime steps
forms a trajectory
τ= ((r1,k1,s1,a1),(r2,k2,s2,a2),..., (rT,kT,sT,aT)), (1)
where, for each time-step t,rt,kt,st,atrespectively correspond to the RTG, the ATG, the patient state,
and the treatment decisions. We train our policy, a causal transformer network, to predict ground truth
dosages that were administered by a clinician at each time-step given the treatment trajectory, ignoring
future information and the prediction target via masking (Figure 1). MeDTaims to learn an optimal policy
distribution
Pπ(at|s≤t,r≤t,k≤t,a<t), (2)
inspired by the model architecture used in Chen et al. (2021). We use an encoder with a linear layer and
a normalization layer for each type of input (i.e. RTG, ATG, state, action) to project raw inputs into
token embeddings. To capture the temporal dynamics of the patient’s trajectory, we use learned position
embeddings for each time-step, which are added to the token embeddings. Finally, the resultant embeddings
are fed into a causal transformer, which autoregressively predicts the next action tokens.
3.1 Evaluation
In online RL, policies are typically assessed by having them interact with the environment. However, health-
care involves patients, and employing this evaluation method is unsafe. In this work, we evaluate the learned
RL policy in an observational setting, where the treatment strategy is assessed based on historical data
(Gottesman et al., 2018). Following the model-based OPE approach, we introduce an additional predictor
network based on the causal transformer (Radford et al., 2018).
The predictor network, shown in Figure 2, acts as a stand-in for the simulator during inference. It is trained
to learn a state-prediction model defined by the distribution
Pθ(st|a<t,s<t), (3)
using a similar architecture as the policy network. This allows us to model how a patient’s state changes
in response to medical interventions. Rather than introducing a termination model, we use a fixed rollout
length ofH. The estimated acuity scores can provide more clinically relevant estimates because they indicate
how the stability of the physiological state of the patient may change given a treatment policy. While not
exact, this approximation can prove adequate for generating reasonable estimates of a patient’s physiological
dynamics. This enables inferring estimates of patient acuity scores (SAPS2) from predicted states, which
can then be used for policy evaluation. Furthermore, during inference, this model allows autoregressive
6Under review as submission to TMLR
generation of a sequence of actions by predicting how the patient’s state evolves as a result of those actions.
Figure 2 and Algorithm 1 depict this rollout procedure.
Algorithm 1 Evaluation Loop
1.Input:Initial patient state s0
2.Output: Acuity score g1,...,gT
3. Set target return rT= 1
4. Initialize state s1=s0, target return r1:T=rTand action sequence a0={}
5.fort= 1,2,...,T
6. Select desired Acuity To Go kt
7. Select action at= MeDT(r1:t,k1:t,s1:t,a0:t−1)
8. Append atto the sequence of actions: a1:t=a0:t−1+ [at]
9. Estimate new state: st+1= state_estimator(s1:t,a1:t)
10. Evaluate acuity score gt+1for statest+1
11. Append stto the sequence of states: s1:t=s1:t−1+ [st]
12.end for
13.returnacuity score g1,...,gT
Additionally, we use WIS (Păduraru et al., 2013; Voloshin et al., 2019), WDR (Jiang & Li, 2016; Voloshin
et al., 2019) and FQE (Le et al., 2019) to rigorously evaluate the performance of policies.
WIS uses a behavior policy πbto evaluate a policy πby re-weighting episodes according to their likelihood of
occurrence (Păduraru et al., 2013; Voloshin et al., 2019). With the per-step importance ratio ρt=π(at|st)
πb(at|st)
and cumulative importance ratio ρ1:t=/producttextt
t′=1ρt′, WIS can be computed as
1
NN/summationdisplay
n=1ρ(n)
1:T(n)
wT(n)
T(n)/summationdisplay
t=1γt−1r(n)
t
, (4)
whereNis the total number of episodes, T(n)is the total number of time-steps for episode n,γis the
discount factor and the average cumulative importance ratio wt=1
N/summationtextM
n=1ρ(n)
1:t.
FQE is a value-based temporal difference algorithm that utilizes the Bellman equation to compute boot-
strapped target transitions from collected trajectories and then uses function approximation to compute the
Qvalue of policy π. This can be formalized as
1
NN/summationdisplay
n=1/summationdisplay
a∈Aπ(a|s(n)
1)/hatwideQπ
FQE(s(n)
1,a), (5)
where/hatwideQFQEis the estimated Qfunction of π.
WDR utilizes both value estimators from FQE as well as importance sampling from WIS in order to reduce
the overall variance of estimations (Jiang & Li, 2016; Thomas & Brunskill, 2016). The WDR estimator is
defined as follows:
1
NN/summationdisplay
n=1T(n)/summationdisplay
t=1/bracketleftigg
ρ(n)
1:t
wtγt−1r(n)
t−/parenleftigg
ρ(n)
1:t
wt/hatwideQπ/parenleftig
s(n)
t,a(n)
t/parenrightig
−ρ(n)
1:t−1
wt−1/hatwideVπ/parenleftig
s(n)
t/parenrightig/parenrightigg/bracketrightigg
, (6)
7Under review as submission to TMLR
Table 1: Estimated final patient acuity scores (averaged over 2,898 patients) for BCQ: batch constrained
Q-learning, NFQI: Neural Fitted Q-Learning, DDQN: Double Deep Q-Learning, CQL: Conservative Q-
Learning, BC: behaviour cloning, DT: decision transformer and MeDT: medical decision transformer.
Models Overall↓ Low↓ Mid↓ High↓
BCQ 42.10±0.0341.78±0.0742.38±0.0341.59±0.15
NFQI 44.21±0.0543.26±0.0844.85±0.0645.13±0.13
DDQN 43.47±0.0443.08±0.0743.64±0.0543.29±0.11
CQL 40.42±0.0340.18±0.0740.59±0.0441.03±0.11
BC 40.50±0.0340.33±0.0740.56±0.0340.29±0.12
DT 40.38±0.0340.16±0.0640.49±0.03 40.06±0.12
MeDT 40.31±0.03 40.05±0.06 40.40±0.0340.35±0.14
where/hatwideQFQEand/hatwideVFQEis the estimated Qand value function of policy πrespectively.
3.2 Interpretability
We utilize the transformer interpretability method introduced by Chefer et al. (2021a) which is based on the
principle of information flow. We adapt this algorithm for the decoder transformer architecture of MeDT
used in this work. This subsection outlines the mechanisms underlying the computation of relevance scores
used to visualize interpretations.
Letirefer to the input tokens of MeDT.Aiirepresents the self-attention interactions between these tokens.
Based on these interactions, we seek to compute the relevancy map Rii. Relevancy maps are constructed
with a forward pass through the self-attention layers, where these layers attribute to aggregated relevance
maps via the following propagation rules.
Given that each token is self-contained prior to attention operations, self-attention interactions are initialized
with identity matrices. Thus, the relevancy maps are also initialized as identity matrices:
Rii=⊮i×i. (7)
The attention matrix Afrom each layer is used to update the relevance maps. The gradients ∇Aare used
to average over the heads hdimension of the attention map, to account for the differing importance assigned
across the heads of the matrix (Voita et al., 2019). ∇A:=∂y
∂A, whereyrefers to the output for which we
wish to visualize relevance. The aggregated attention is then defined as:
¯A=Eh/parenleftbig
(∇A⊙A)+/parenrightbig
, (8)
whereEhis the mean over the hdimension and⊙is the Hadamard product.+denotes that the negative
values are replaced by zero prior to computing the expectation.
At each attention layer, these aggregated attention scores are then used to calculate the aggregated relevancy
scores as follows:
Rss=Rss+¯A·Rss. (9)
These relevancy scores can then be used to visualize the importance assigned across the input token space
in the form of a heatmap. Since in transformer decoders, future tokens are masked, there is more attention
toward initial tokens in the input sequence. Hence, to apply these methods to MeDT, we normalize based
on the receptive field of attention.
4 Experiments
4.1 Experimental Settings
In this work, we train and evaluate the performance of MeDTon a cohort of septic patients. The cohort
data is obtained from the medical information mart for intensive care (MIMIC-III) dataset (Johnson et al.,
8Under review as submission to TMLR
Figure 4: Box-plots of FQE, WIS and WDR off-policy evaluations for MeDT and baselines.
2016), which includes 19,633 patients, with a mortality rate of 9%. These patients were selected on fulfilling
the sepsis-3 definition criteria (Singer et al., 2016). To pre-process the data, we follow the pipeline defined
by Killian et al. (2020). We extract physiological measurements of patients recorded over 4-hour intervals
and impute missing values using the K-nearest neighbor algorithm. Multiple observations within each 4-hour
window are averaged.
The patient state consists of 5 demographic variables and 38 time-varying continuous variables such as lab
measurements and vital signs. This work centers on the timing and optimal dosage of administering VP
and IV fluids. The administration of each drug for patients is sampled at 4-hour intervals. We discretized
the dosages for each drug into 5 bins, resulting in a combinatorial action space of 25 possible treatment
administrations. Limiting our focus to IV fluids and vasopressors implies that these are the only treatments
within our control; other interventions like antibiotics that the patient might receive are outside the scope
of our consideration.
4.2 Baselines
We compare MeDTto batch constrained Q-learning (BCQ), neural fitted Q-learning (NFQI), double deep
Q-learning (DDQN) and conservative Q-learning (CQL) algorithms, which are commonly used baselines in
recent works related to offline reinforcement learning (Killian et al., 2020; Tang et al., 2022; Pace et al.,
2023). Additionally, we train and evaluate DT and a transformer-based behaviour cloning (BC) algorithm.
BC refers to a transformer that takes as input past states and actions, guided by cross-entropy loss on
predicted actions, to directly imitate the behavior of the clinician’s policy. DT builds on BC by conditioning
on returns-to-go. The proposed MeDTdiffers from DT in that it also conditions on acuity-to-go at each
time-step.
4.3 Training
The transformer policy is trained on mini-batches of fixed context length, which are randomly sampled from
a dataset of offline patient trajectories. In our case, we chose a context length of 20, which is the longest
patient trajectory in the dataset following pre-processing. For trajectories shorter than this length, we use
zero padding to adjust them. During training, we use teacher-forcing, where the ground-truth sequence is
providedasinputtothemodel. Ateachtime-step t, theATG( kt)issettotheactualacuityscoresofthestate
at time-step t+ 1in the sequence. The prediction head of the policy model, associated with the input token
st, is trained to predict the corresponding discrete treatment action atusing a cross-entropy loss. The loss
for a complete trajectory is averaged over time-steps. Additionally, the state estimator is separately trained
to predict the patient’s state following the treatment actions. The prediction head of the state predictor
9Under review as submission to TMLR
Figure 5: Visualization of 4 patient trajectories computed by the state predictor following treatment recom-
mendation from DT (red) and MeDT(blue).
model, corresponding to the input token at, is trained to estimate the continuous state st+1using a mean
square error loss. The models are trained on NVIDIA V100 GPUs. We aggregate experimental results for
each model into mean and standard error over five random seeds. Additional details on hyperparameter
selection can be found in the Appendix in Section ??.
4.4 Results and Analysis
We evaluate our proposed MeDTnetwork in the autoregressive inference loop with the state predictor (Table
1). As elaborated in the Appendix in Section ??, we use a naive heuristic to select ATG, and investigate
whether the network conditioned on these prompts results in more stable patient outcomes. We compare our
proposed approach to multiple baselines and run this loop over only 10 time-steps to avoid the accumulation
of state-prediction errors resulting from the autoregressive nature of evaluation. We calculate the average
and standard error of the SAPS2 scores of the states estimated by the predictor network for every patient
in the test cohort. This cohort split comprises 2,945 patients. We also evaluate all policies over additional
methods of OPE such as WIS, FQE and WDR.
4.4.1 Quantitative Analysis
From Table 1, we infer that the MeDTpolicy, which is conditioned on both positive RTGs and our chosen
ATG heuristic, results in the most stable estimated patient states. The DT framework conditioned only
with positive RTGs performs better than BC and other baselines. The learned policies are also evaluated
for patients with different severity of sepsis (denoted as low, mid and high severity) based on the SAPS2
score of the initial state. Comparing the models, we observe that the MeDTpolicy results in more stable
states for low and mid-severity patients, while DT performs best for high-severity patients. We hypothesize
that, given there are far fewer data samples for patients in the higher severity bracket, MeDTwas not able
to learn an accurate mapping of patient states to actions given the additional ATG context. This suggests
thatMeDTrequires more samples relative to DT to reach convergence.
We run an experiment to evaluate the sample efficiency of DT and MeDTin Figure ??in the Appendix.
We evaluate the performance of the policies when trained on 50%, 75% and 100% of the data from the
train split. DT performs better when trained on the smallest 50% split, while MeDTis performant on the
75% and 100% splits. This supports the hypothesis that the additional conditioning used in MeDThas a
negative impact on sample efficiency. It is worth noting that given the small size of the sepsis cohort from the
MIMIC-III dataset, the 50%, 75% and 100% splits are all low training sample settings relative to standard
sizes of training data used in RL. Nevertheless, this is an optimistic observation, given the potential for the
exponential growth of data available from large-scale EHRs.
Figure 4 depicts the results of the FQE, WIS and WDR evaluations. The MeDTpolicy produces the highest
estimated values on FQE and WDR while CQL performs best on WIS. It is worth noting that the MeDT
and DT policies show noticeably less variance than the baselines, suggesting they are more robust models.
These results indicate that the clinical dosage recommendations based on our proposed conditioning method
may have had the intended treatment effects.
10Under review as submission to TMLR
Figure 6: Relevancy maps depicting the importance assigned by the model to input tokens upon predicting
actiona5for a sample patient trajectory. Darker and lighter colors indicate lower and higher relevance
scores, respectively. In (a) the mean of the relevancy assigned to the ATG components is taken to better
visualize the relevance across time. Heatmap (b) depicts relevance across each ATG component to visualize
the importance assigned to each conditioning.
4.4.2 Qualitative Analysis
We qualitatively evaluate the policy of MeDTagainst the clinician’s policy. To ensure accurate analysis, we
use ground-truth trajectories as input sequences instead of relying on autoregressive inference, which may
lead to compounding errors. In Figure 3a, we conduct a comparative analysis of the mean dose of VPs and
IVs recommended by the MeDTpolicy and the clinician policy, for patient states with varying SAPS2 scores.
Figure 3b presents the dosage distribution of IVs and VPs recommended by both the MeDTand clinician
policies.
Our results show that the MeDTpolicy generally aligns with the clinician’s treatment strategy but recom-
mends lower doses of IVs on average. Both policies exhibit a similar trend of increasing medication doses
with worsening patient condition, for both VPs and IVs. Figure 3b reveals that the MeDTpolicy uses more
zero dosage instances for both IVs and VPs, compared to the clinician policy. We hypothesize the signficant
overlap between the MeDTand clinician policy is a byproduct of the imbalanced nature of the dataset, given
that over 91% of patient trajectories in the dataset resulted in positive outcomes (survival). As a result,
MeDTdecides to imitate the clinician policy. Nevertheless, the alignment with the domain expert policy
is ideal, especially in this high-stakes task where the algorithm relies solely on pre-existing static data for
learning, as it is preferred to assess policies that only recommend subtle changes and closely resemble those
of physicians as a precautionary measure (Gottesman et al., 2019).
Furthermore, previous studies have demonstrated a trend wherein lower dosages are recommended for pa-
tients with higher acuity scores (Raghu et al., 2017). This pattern can be linked to the common practice
among clinicians of administering elevated dosages to individuals with high acuity scores, often associated
with more severe medical conditions and, consequently, higher mortality rates. The challenge arises when
algorithms lack data samples featuring high acuity scores coupled with minimal dosages. In such instances,
these algorithms default to advocating lower medication doses. Figure 3 demonstrates that the MeDTpolicy
diverges from prior research by refraining from recommending minimal dosages for patients with elevated
acuity scores. This serves as an indicator of better generalization and sample efficient properties from MeDT
given this negative behavior is not observed.
In Figure 5, we visualize the trajectories of multiple patients computed by the state predictor, following
treatment actions recommended by both the DT and MeDTpolicies. The impact of ATG conditioning
on patient health is evident, as MeDTleads to more stable trajectories, demonstrating the potential of our
framework to generate targeted and improved treatment recommendations by considering both the hindsight
returns and ATG at each time-step. In the Appendix in Figure ??, we provide visualizations of some patient
trajectories, where we observed that the MeDTpolicy produced the same or worse action policies relative
to DT, with no discernible effect of ATG conditioning. We hypothesize that this may be due to limitations
of the dataset, which may not sufficiently cover some regions of the joint space of vital signs, treatment
decisions and outcomes, causing the model to be unable to discover some causal relations.
11Under review as submission to TMLR
4.4.3 Interpretability
Currently, RL algorithms typically function as opaque systems (Gottesman et al., 2019). They take in data
and generate a policy as output, but these policies are often challenging to interpret. This makes it difficult
to pinpoint the specific data features influencing a suggested action. The lack of interpretability raises
concerns, hindering experts from identifying errors and potentially slowing down adoption. Thus, clinicians
may be hesitant to embrace recommendations that lack transparent clinical reasoning.
To improve interpretability and reliability of our MeDTmodel for users, we illustrate the relevance assigned
by the transformer to input tokens for an example patient trajectory in Figure 6. The relevance across the
ATG components are averaged in Figure 6a to better depict the relevance assigned over time, while Figure 6b
visualizes the importance assigned to each ATG component. We observe that MeDTassigns relatively more
importance to time-steps 2 and 3 for this patient sample. Figure Figure 6b shows that the model considers
the conditioning for the Hepatic of higher relevance in its prediction.
This allows clinicians to monitor the specific points in time when the model assigns the highest importance,
facilitating an assessment of its reasonableness. If the model differs from ground-truth clinician actions,
an analysis may reveal which features carry the most weight in the decision-making shift. Additionally,
if the model relies on clinically irrelevant features, it signals to clinicians that the recommendation may
be unsound. This not only enhances understanding of the model’s decision process but also invites future
research into the reliability of deep RL decision-making from a clinical perspective.
5 Conclusion
Inthiswork, weproposethe Medical Decision Transformer , anovelreinforcementlearningapproachbasedon
the transformer architecture. It models the full context of a patient’s medical history to recommend effective
sepsis treatment decisions. During training, our framework conditions the model not only on hindsight
rewards but also on look-ahead patient acuity scores at each time-step. This enables clinicians to later
interact with the model and guide its treatment recommendations by conditioning the model on short-term
goals for patient stability. For autoregressive evaluation of our proposed approach, we present a separately
trainedstatepredictorthatmodelsapatient’sclinicalstateevolutiongivenasequenceoftreatmentdecisions.
Our experimental results demonstrate the potential of MeDTto bolster clinical decision support systems by
providing clinicians with an interpretable and interactive intervention support system.
References
SamiraAbnarandWillemZuidema. Quantifyingattentionflowintransformers. InDanJurafsky, JoyceChai,
Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , pp. 4190–4197, Online, July 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.acl-main.385. URL https://aclanthology.org/2020.acl-main.385 .
Pranav Agarwal, Aamer Abdul Rahman, Pierre-Luc St-Charles, Simon JD Prince, and Samira Ebrahimi
Kahou. Transformers in reinforcement learning: a survey. arXiv preprint arXiv:2307.05979 , 2023.
Julia Amann, Alessandro Blasimme, Effy Vayena, Dietmar Frey, Vince I Madai, and Precise4Q Consor-
tium. Explainability for artificial intelligence in healthcare: a multidisciplinary perspective. BMC medical
informatics and decision making , 20:1–9, 2020.
Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal and
encoder-decoder transformers. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 397–406, 2021a.
Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 782–791, 2021b.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Ar-
avind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.
Advances in neural information processing systems , 34:15084–15097, 2021.
12Under review as submission to TMLR
HirokiFuruta, YutakaMatsuo, andShixiangShaneGu. Generalizeddecisiontransformerforofflinehindsight
information matching. arXiv preprint arXiv:2111.10364 , 2021.
Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, and Wulong Liu. A
survey on interpretable reinforcement learning. arXiv preprint arXiv:2112.13112 , 2021.
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan, Linying
Zhang, Yi Ding, David Wihl, Xuefeng Peng, et al. Evaluating reinforcement learning algorithms in
observational health settings. arXiv preprint arXiv:1805.12298 , 2018.
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale Doshi-Velez,
and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nature medicine , 25(1):16–18,
2019.
Omer Gottesman, Joseph Futoma, Yao Liu, Sonali Parbhoo, Leo Celi, Emma Brunskill, and Finale Doshi-
Velez. Interpretable off-policy evaluation in reinforcement learning by highlighting influential transitions.
InInternational Conference on Machine Learning , pp. 3658–3667. PMLR, 2020.
Yong Huang, Rui Cao, and Amir Rahmani. Reinforcement learning for sepsis treatment: A continuous action
space solution. In Zachary Lipton, Rajesh Ranganath, Mark Sendak, Michael Sjoding, and Serena Yeung
(eds.),Proceedings of the 7th Machine Learning for Healthcare Conference , volume 182 of Proceedings of
Machine Learning Research , pp. 631–647. PMLR, 05–06 Aug 2022.
Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling
problem. Advances in neural information processing systems , 34:1273–1286, 2021.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In Interna-
tional Conference on Machine Learning , pp. 652–661. PMLR, 2016.
Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,
Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible
critical care database. Scientific data , 3(1):1–9, 2016.
Alan E Jones, Stephen Trzeciak, and Jeffrey A Kline. The sequential organ failure assessment score for
predicting outcome in patients with severe sepsis and evidence of hypoperfusion at the time of emergency
department presentation. Critical care medicine , 37(5):1649–1654, 2009.
Taylor W. Killian, Haoran Zhang, Jayakumar Subramanian, Mehdi Fatemi, and Marzyeh Ghassemi. An
empirical study of representation learning for reinforcement learning in healthcare. In ML4H@NeurIPS ,
2020.
Matthieu Komorowski, Leo Anthony Celi, Omar Badawi, Anthony C. Gordon, and Aldo A. Faisal. The
artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature
Medicine , 24:1716–1720, 2018.
Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In International
Conference on Machine Learning , pp. 3703–3712. PMLR, 2019.
Jean-Roger Le Gall, Stanley Lemeshow, and Fabienne Saulnier. A new simplified acute physiology score
(saps ii) based on a european/north american multicenter study. Jama, 270(24):2957–2963, 1993.
Luchen Li, Matthieu Komorowski, and Aldo A. Faisal. Optimizing sequential medical treatments with
auto-encoding heuristic search in pomdps. ArXiv, abs/1905.07465, 2019.
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI Open, 2022.
Mingyu Lu, Zach Shahn, Daby M. Sow, Finale Doshi-Velez, and Li wei H. Lehman. Is deep reinforcement
learningreadyforpracticalapplicationsinhealthcare? asensitivityanalysisofduel-ddqnforhemodynamic
management in sepsis patients. AMIA ... Annual Symposium proceedings. AMIA Symposium , 2020:773–
782, 2020.
13Under review as submission to TMLR
Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan Zhang, Ying Wen, Haifeng
Zhang, Jun Wang, and Bo Xu. Offline pre-trained multi-agent decision transformer: One big sequence
model conquers all starcraftii tasks. arXiv preprint arXiv:2112.02845 , 2021.
Dnyanesh N Morkar, Manisha Dwivedi, and Priyanka Patil. Comparative study of sofa, apache ii, saps ii,
as a predictor of mortality in patients of sepsis admitted in medical icu. The Journal of the Association
of Physicians of India , 70(4):11–12, 2022.
Zhaoyang Niu, Guoqiang Zhong, and Hui Yu. A review on the attention mechanism of deep learning.
Neurocomputing , 452:48–62, 2021.
Alizée Pace, Hugo Yèche, Bernhard Schölkopf, Gunnar Rätsch, and Guy Tennenholtz. Delphic offline
reinforcement learning under nonidentifiable hidden confounding. arXiv preprint arXiv:2306.01157 , 2023.
Cosmin Păduraru, Doina Precup, Joelle Pineau, and Gheorghe Comănici. An empirical analysis of off-policy
learning in discrete mdps. In European Workshop on Reinforcement Learning , pp. 89–102. PMLR, 2013.
Emilio Parisotto and Ruslan Salakhutdinov. Efficient transformers in reinforcement learning using actor-
learner distillation. arXiv preprint arXiv:2104.01655 , 2021.
Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max
Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforce-
ment learning. In International conference on machine learning , pp. 7487–7498. PMLR, 2020.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks.
InInternational conference on machine learning , pp. 1310–1318. Pmlr, 2013.
Xuefeng Peng, Yi Ding, David Wihl, Omer Gottesman, Matthieu Komorowski, Li wei H. Lehman, An-
drew Slavin Ross, A. Aldo Faisal, and Finale Doshi-Velez. Improving sepsis treatment strategies by com-
bining deep and kernel-based reinforcement learning. AMIA ... Annual Symposium proceedings. AMIA
Symposium , 2018:887–896, 2018.
Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty
Publication Series , pp. 80, 2000.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding
by generative pre-training. 2018.
Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh Ghassemi.
Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602 , 2017.
Aniruddh Raghu, Matthieu Komorowski, and Sumeetpal S. Singh. Model-based reinforcement learning for
sepsis treatment. ArXiv, abs/1811.09602, 2018.
Suchi Saria. Individualized sepsis treatment using reinforcement learning. Nature Medicine , 24:1641 – 1642,
2018.
Luregn J. Schlapbach, Scott L. Weiss, Melania M. Bembea, Joseph A. Carcillo, Francis Leclerc, Stephane
Leteurtre, Pierre Tissieres, James L. Wynn, Jerry Zimmerman, Jacques Lacroix, and Marie E Steiner.
Scoring systems for organ dysfunction and multiple organ dysfunction: The podium consensus conference.
January 2022. doi: 10.1542/peds.2021-052888d.
JuergenSchmidhuber. Reinforcementlearningupsidedown: Don’tpredictrewards–justmapthemtoactions.
arXiv preprint arXiv:1912.02875 , 2019.
Sofia Serrano and Noah A. Smith. Is attention interpretable? In Anna Korhonen, David Traum, and Lluís
Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,
pp. 2931–2951, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/
P19-1282. URL https://aclanthology.org/P19-1282 .
14Under review as submission to TMLR
Nicholas Simpson, Francois Lamontagne, and Manu Shankar-Hari. Septic shock resuscitation in the first
hour.Current opinion in critical care , 23(6):561–566, 2017.
Mervyn Singer, Clifford S Deutschman, Christopher Warren Seymour, Manu Shankar-Hari, Djillali Annane,
Michael Bauer, Rinaldo Bellomo, Gordon R Bernard, Jean-Daniel Chiche, Craig M Coopersmith, et al.
The third international consensus definitions for sepsis and septic shock (sepsis-3). Jama, 315(8):801–810,
2016.
Reed T Sutton, David Pincock, Daniel C Baumgart, Daniel C Sadowski, Richard N Fedorak, and Karen I
Kroeker. An overview of clinical decision support systems: benefits, risks, and strategies for success. NPJ
digital medicine , 3(1):17, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical considerations
for healthcare settings. In Machine Learning for Healthcare Conference , pp. 2–35. PMLR, 2021.
Shengpu Tang, Maggie Makar, Michael Sjoding, Finale Doshi-Velez, and Jenna Wiens. Leveraging factored
action spaces for efficient offline reinforcement learning in healthcare. In Alice H. Oh, Alekh Agarwal,
Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022.
URL https://openreview.net/forum?id=Jd70afzIvJ4 .
Tianxin Tao, Daniele Reda, and Michiel van de Panne. Evaluating vision transformer methods for deep
reinforcement learning from pixels. arXiv preprint arXiv:2204.04905 , 2022.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning.
InInternational Conference on Machine Learning , pp. 2139–2148. PMLR, 2016.
Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of off-policy evaluation in reinforcement
learning. arXiv preprint arXiv:2212.06355 , 2022.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-
attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418 ,
2019.
Cameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policy evaluation
for reinforcement learning. arXiv preprint arXiv:1911.06854 , 2019.
Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers
in time series: A survey. arXiv preprint arXiv:2202.07125 , 2022.
Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan.
Promptingdecisiontransformerforfew-shotpolicygeneralization. In International Conference on Machine
Learning , pp. 24631–24645. PMLR, 2022.
Jiamin Yin, Kee Yuan Ngiam, and Hock Hai Teo. Role of artificial intelligence applications in real-life clinical
practice: systematic review. Journal of medical Internet research , 23(4):e25759, 2021.
Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey.
ACM Computing Surveys (CSUR) , 55(1):1–36, 2021.
15