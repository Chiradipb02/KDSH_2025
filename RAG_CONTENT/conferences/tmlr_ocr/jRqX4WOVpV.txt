Under review as submission to TMLR
Probabilistic Rank and Reward: A Scalable Model for Slate
Recommendation
Anonymous authors
Paper under double-blind review
Abstract
We introduce Probabilistic Rank andReward ( PRR), a scalable probabilistic model for
personalized slate recommendation. Our approach allows off-policy estimation of the reward
in the ubiquitous scenario where the user interacts with at most one item from a slate of
Kitems. We show that the probability of a slate being successful can be learned efficiently
by combining the reward , whether the user successfully interacted with the slate, and the
rank, the item that was selected within the slate. PRRoutperforms existing off-policy reward
optimizingmethodsandisfarmorescalabletolargeactionspaces. Moreover, PRRallowsfast
delivery of recommendations powered by maximum inner product search (MIPS), making
it suitable in low latency domains such as computational advertising.
1 INTRODUCTION
Recommender systems (advertising, search, music streaming, etc.) are becoming prevalent in society helping
users navigate enormous catalogs of items to identify those relevant to their interests. In practice, recom-
mender systems must optimize the content of an entire section of the web page that the user is navigating.
This section is viewed as an ordered collection (or slate) of Kitems (Swaminathan et al., 2017; Chen et al.,
2019; Aouali et al., 2021). It often takes the form of a menu and the user can choose to interact with one of
its items. Both in academia and industry, A/B tests (Kohavi & Longbotham, 2017) are seen as the golden
standard to measure the performance of recommender systems. A/B tests enable us to directly measure
utility metrics that rely on interventions, being the slates shown to the user. However, they are costly.
Thus a clear need remains for reliable offline procedures to propose candidate recommender systems and
consequently reduce the cost of A/B tests.
In this work, we propose a probabilistic model called Probabilistic Rank andReward ( PRR) for large-scale
slaterecommendation. PRRaddressesthefollowingpracticallimitationsofexistingmethods. 1)Collaborative
filteringandcontent-basedrecommendersystems(Su&Khoshgoftaar,2009;Lopsetal.,2011)optimize proxy
metricsofthereward. ThismayleadtoastrikinggapbetweentheirofflineevaluationandtheA/Btestresult
(Garcin et al., 2014). 2)Counterfactual estimators (Bottou et al., 2013), which are often based on inverse
propensity scoring (IPS) (Horvitz & Thompson, 1952), suffer high bias (Sachdeva et al., 2020) and variance
(Swaminathan et al., 2017) in large-scale scenarios. Moreover, policy learning objectives for these estimators
(Swaminathan & Joachims, 2015) are mostly not suitable for slates and large action spaces. 3)The decision
rules produced by most existing methods do not fit the engineering constraints for deployment in large-scale,
low-latency systems such as computational advertising; they are either expensive or intractable. To address
these challenges, our paper makes the following contributions.
1. Problem formulation: We formalize the ubiquitous slate recommendation setting where the user is
shown a slate of Kitems and they can choose to interact with at most one of its items. After that, the
information received consists of two signals: did the user successfully interact with one of the items? Then if
an item was interacted with, which one was it? These are referred to by rewardandrank, respectively. Note
that it is very common in practice that the user interacts with at mostone item in the slate. For example,
in ad placement, a click on an item causes the whole slate to disappear. As a result, the user cannot click
on the other items in that slate. Similarly, in video recommendation, a click on a video to watch it reloads
1Under review as submission to TMLR
the homepage and changes it. Most offline reward optimizing methods do not consider this and assume that
the user can interact with multiple items in the slate.
2. Modeling the reward and rank: We propose a probabilistic model ( PRR) that combines both signals,
the reward and rank. This is important as both contain useful information about the user interests, and
discarding one of them may lead to inferior performance. Existing methods either use one signal only or
partially combine the two by assuming that the reward is a function of the rank and only use the latter.
3. Incorporating extra features: PRRdistinguishes between slate and item level features that contribute
to an interaction with the slate and one of its items, respectively. PRRalso incorporates that interactions can
be predicted by engagement features that neither represent the user interests nor the recommended items.
This includes the slate size and the overall level of user activity and engagement. While these features are
not used in decision-making, incorporating them helps learn the user interests more accurately. Precisely,
it allows differentiating between interactions that are caused by the quality of recommendations and those
that happen due to overall user engagement. Existing model-based methods do not include such features.
4. Fast decision making: PRR’s decision rule is reduced to solving a maximum inner product search
(MIPS).Thisallowsfastdeliveryofrecommendationsinlarge-scaletasksusingapproximateMIPSalgorithms
(Shrivastava & Li, 2014; Malkov & Yashunin, 2018) at the cost of restricting the decision rule to a particular
form. Most existing offline reward optimizing methods do not take into account the real-time response
requirement of production systems and as such can propose expensive decision rules.
5. Experiments: We show empirically that PRRoutperforms commonly used baselines and that it is far
more scalable in terms of both statistical and computational efficiency.
The remainder of the paper is organized as follows. In Section 2 we review the state of the art in off-policy, or
offline, reward optimizing recommendation. In Section 3, we describe the setting for slate recommendation
andpresentouralgorithm. InSection4,wedescribethebaselinesandpresentourqualitativeandquantitative
results showing that PRRhas favorable performance both in terms of recommendation quality and scalability.
In Section 5, we make concluding remarks and outline potential directions for future work.
2 Related Work
A significant part of the classic recommender systems literature is inspired by the Netflix prize (Bennett
et al., 2007) which formulated recommendation as predicting item ratings in a matrix (Portugal et al.,
2018). A practitioner will often work with datasets such as MovieLens (Harper & Konstan, 2015) that
includeneitherrecommendationsnor rewards, butaresuitable for collaborativefiltering(Su& Khoshgoftaar,
2009) or content-based recommendation (Lops et al., 2011). While interesting, such datasets and problem
formulations do not reflect the actual interactions between the users and the recommender systems; they are
only imperfect proxies. In particular, the performance of an algorithm on these problems and datasets may
be very different from its actual A/B test performance (Garcin et al., 2014, Section 5.1). Instead, off-policy,
or offline, reward optimizing recommendation approaches aim at directly optimizing the reward using logged
datasummarizing the previous interactions of users with the existing recommender system. These are also
different from on-policy, or online, approaches that we do not cover in this work. Here the reward is learned
offline using logged data. In this section, we review these approaches and compare them to ours.
2.1 Inverse Propensity Scoring (IPS)
Here we assume that the recommender system is represented by a stochastic policy π. That is, given a user
u,π(·|u)is a probability distribution over the set of items. Under this assumption, Dudík et al. (2012) used
inverse propensity scoring (IPS) (Horvitz & Thompson, 1952) to estimate the reward for recommendation
tasks with small action spaces. Unfortunately, IPS can suffer high bias and variance in realistic settings such
as slate recommendation. This is mainly driven by two practical factors; the action space is combinatorially
large and the policy used to collect data primarily exploitcertain recommendations with minimal or no
exploration making its support deficient (Sachdeva et al., 2020). The high variance of IPS is acknowledged
and several fixes have been proposed such as clipping, and self-normalizing importance weights (Gilotte
2Under review as submission to TMLR
et al., 2018). Another solution is doubly robust (DR) (Dudík et al., 2014) which combines a reward model
with IPS to reduce the variance. DR relies on a reward model and PRRcan be integrated into it.
In slate recommendation, recent works made simplifying structural assumptions to reduce the variance.
For instance, Li et al. (2018) restricted the search space by assuming that items contribute to the reward
individually. Similarly, Swaminathan et al. (2017) assumed that reward is additive w.r.t. unobserved and
independent ranks. The independence assumption is restrictive and can be violated in many production
settings. A relaxed assumption was proposed in McInerney et al. (2020) where the interaction between the
user and the ℓ-th item in the slate, sℓ, depends only on sℓ,sℓ−1and its rank rℓ−1. This sequential dependence
scheme is not sufficient to encode the ubiquitous setting where the user views the whole slate at once and
interacts with at mostone of its items. PRRis model-based, does not use inverse propensity scoring and is
specifically designed for this ubiquitous setting.
2.2 Direct Methods (DM)
Directmethodslearnarewardmodelandthenuseittoestimatetheperformanceoftherecommendersystem.
Existing methods (Sakhi et al., 2020; Jeunen & Goethals, 2021) focus on single-item recommendation (slates
of size 1) and neglect that the quality of recommendations might not have a strong effect on the reward.
Instead,engagement features thatarenotrelevanttorecommendationsuchastheoveralllevelofuseractivity
and engagement may have a higher effect on the reward. PRRexplicitly includes nuisance parameters to
capture these effects independently of the quality of recommendations.
Another popular family related to direct methods is called click (or ranking) models (Chuklin et al., 2015).
The simplest is click-through-rate models which defines a single parameter for the probability that an item is
clicked, possibly depending on its position or the user (Joachims et al., 2017; Craswell et al., 2008). Another
type is called cascade models (Dupret & Piwowarski, 2008; Guo et al., 2009; Chapelle & Zhang, 2009), which
is suitable when items are presented in sequential order. Later, these models were extended to accommodate
multiple user sessions (Chuklin et al., 2013; Zhang et al., 2011), granularity levels (Hu et al., 2011; Ashkan
& Clarke, 2012), and additional user signals (Huang et al., 2012; Liu et al., 2014). Click models are often
represented as graphical models and as such define dependencies manually and are not always scalable to
large action spaces. Moreover, they do not incorporate extra features that are available in recommendation
since they were primarily designed for search engine retrieval. Typically they are evaluated using proxy
Information Retrieval metrics such as Recall@K instead of the actual reward.
2.3 Policy Learning
Finding an optimal policy to implement in a recommender system requires both estimating the expected
reward of policies (using either direct or IPS methods above) and then searching the space of policies to
find the one with the highest expected reward (policy learning). The literature mainly focuses on combining
IPS with a softmax policy for single item recommendation (Swaminathan & Joachims, 2015). Extending
this to slates is challenging. In fact, it is tempting to use factored softmax policies but this may cause the
learned policy to recommend slates with repeated items. This was acknowledged in Chen et al. (2019) that
introduced a top-K heuristic to prevent the collapse of the policy on a single item. In our case, policy learning
is not needed as the optimal policy of PRRfalls out neatly to a MIPS task due to its parametrization.
2.4 Decision Making
Animportantchallengeinpracticeisthedesignoftractabledecisionrulesthatsatisfyengineeringconstraints.
Precisely, real-world recommender systems must quickly recover a slate of items sgiven a context vector x.
The optimal decision rule under either the IPS or DM formulation amounts to solving argmaxs∈SL(x,s).
HereL(x,s)might either be the reward estimate in DM or the policy in IPS. However, the space of eligible
slatesSis combinatorially large. Thus exhaustively searching the space of slates is untenable and we must
resort to finding good but implementable decision rules rather than optimal ones. There are three main
strategies for doing this. (A)Reducing the combinatorial search over Sto a sorting operation over a set
ofPitems.(B)Two-stage systems that massively reduce candidate sets with a pre-filtering operation.
3Under review as submission to TMLR
(C)Fast approximate maximum inner product search (MIPS) on which we focus in this work since it allows
end-to-end optimization as opposed to the two-stage scheme. Next, we provide more detail for each solution.
(A) Reducing the combinatorial search to sorting: To accelerate decision making, existing methods
moved the search space from the combinatorially large set of slates Sto the catalog of items {1,...,P}.
This is achieved by first associating a score for items instead of slates and then recommending the slate
composed of the top-K items with the highest scores. This leads to a O(P)delivery time due to finding
the top-K items. Unfortunately, reducing a combinatorial search to a sort is still unsuitable for low-latency
recommender systems with large catalogs. We present next the common solution to improve this.
(B) Two-stage recommendation (Borisyuk et al., 2016): Here we first generate a smallsubset of
potential item candidates Psub⊂{1,...,P}, and then select the top-K items in Psubleading to aO(|Psub|)
delivery time. This has two main shortcomings. First, the scoring model, which selects the highest scoring
items fromPsub, does not directly optimize the reward for the whole slate, and rather optimizes a proxy
offline metric for each item individually. This induces numerous biases related to the layout of the slate
such as position biases where users tend to interact more often with specific positions (Yue et al., 2010).
Second, the candidate generation and the scoring models are not necessarily trained jointly, which may lead
to having candidates in Psubthat are not the highest scoring items.
(C) Maximum inner product search (MIPS): A practical approach to avoid the candidate generation
stepreliesonapproximateMIPSalgorithms. Kochetal.(2021)demonstratedthepowerofsuchalgorithmsin
a recommendation setting. Roughly speaking, they are capable of quickly sorting Pitems inO(logP)as long
as the scores of items a∈{1,...,P}have the form u⊤βa. Hereu∈Rdis ad-dimensional user embedding
andβa∈Rdis thed-dimensional embedding of item a. This allows fast delivery of recommendation in
roughlyO(logP)instead ofO(P)without any additional candidate generation step. PRRuses approximate
MIPS algorithms (Shrivastava & Li, 2014; Malkov & Yashunin, 2018) making it suitable for extremely low-
latency systems. We note that both IPS and DM can lead to a MIPS-compatible recommender system if the
model (or the policy) is appropriately parametrized. However, much of the existing literature neglect this
importantconsideration. Inourcase, decision-makingisreducedtoaMIPStaskandweusefastapproximate
algorithms to solve it. This allows us to avoid the two-stage scheme and its limitations.
2.5 Summary of Limitations
Here we summarize the limitations of existing methods. (a) Poor estimation of the reward: this is due
to the high variance and bias of IPS, the incorrect assumptions of existing DM and their potential modeling
bias (e.g., only a single item is recommended, ignoring engagement features, etc.). (b) Policy learning
for single items: extending existing approaches to slates is complicated. (c) Slow decision making:
the real-time response requirement is not respected by most existing methods. The two-stage system is a
remarkably pragmatic compromise. But it poses some challenges as we explained before. MIPS is a reliable
practical alternative to two-stage systems but existing methods are usually not MIPS compatible.
3 ALGORITHM
PRRis a binary reward model that differentiates between item-level and slate-level features. The former
reflects the quality of the slate as a whole while the latter is associated with the quality of individual items
in the slate. This allows PRRto predict whether the user will interact with the slate (the reward) and which
item will be interacted with (the rank). To see this, we give an example of the output of PRRin Fig. 1. Here
we imagine that a user is interested in technology . We show three slates of size 2. In the left panel, the
slate consists of two good1items:phoneandmicrophone . The model predictions (0.91,0.06,0.03)are the
probabilities for no click, click on phoneand click on microphone , respectively. The probability of a click
on slatephone, microphone is higher than the other slates and is equal to 0.09. For comparison, the panel
in the middle contains a good item ( phone) in the prime first position but the shoein the second position,
which is a poor match with the user interest in technology. As a consequence, the probabilities become
1Here a good item refers to a technology item.
4Under review as submission to TMLR
Figure 1: Example of 3 slates of size 2 on a technology website. From left to right are good, mixed and bad
recommendations. ¯R, r 1, r2denote the probabilities of no-click, click on the 1st and 2nd item, respectively.
(0.94,0.04,0.01)for no click, click on phoneand click on shoe. Finally, in the right panel, we show two poor
itemsshoeandpillowresulting in the highest no-click probability 0.97.
The goal is to establish the level of association of each item (in this case phone,microphone ,shoeand
pillow) with a particular user interest (in this case technology ). At first glance, analyzing logs of successful
and unsuccessful recommendations is the best possible way to learn this association. However, in practice,
therearenumerousfactorsthatinfluencetheprobabilityofaclickotherthanthequalityofrecommendations.
In this example, the non-click probability of the good recommendations ( phone,microphone ) is 0.91 (click
probability of 0.09), while the non-click probability of the bad recommendations ( shoe,pillow) is 0.97 (click
probability of 0.03). The change in the click probability from good to bad recommendations is relatively
modest at only 0.06. Thus the model must capture additional factors that influence clicks.
To account for this, PRRincorporates a real-world observation made by practitioners: typically the most
informative features to predict successful interactions are engagement features . These summarize how likely
the user is to interact with the slate independently of the quality and relevance of its items to the user. This
includes for example the slate size, its visibility and the level of user activity and engagement. While these
features are strong predictors of interactions, they do not provide any information about which items are
responsible for which interactions. In contrast, the recommendation features , which include the user interests
and the items shown in the slate, provide a relatively modest signal for predicting interactions. But they are
very important for the recommendation task. Based on these observations, PRRleverages the engagement
features to accurately learn the parameters associated with the usefulrecommendation features.
PRRalso incorporates the information that different positions in the slate may have different properties.
Some positions may boost a recommendation by making it more visible, and other positions may lessen the
impact of the recommendation. To see this, consider the example in Fig. 1, the probability of clicking on
shoesincreased by 0.01when placed in the prime first position (slate in the middle) compared to placing it
in the second position (slate in the right panel).
It may be relevant to draw an analogy. Optical astronomers who take images of far-away galaxies need
to develop a sophisticated understanding of many local phenomena: the atmosphere, the ambient light,
etc. The understanding of all these large effects allows them to construct precise images of faint objects.
Similarly, PRRis able to capture a weak recommendation signal by carefully incorporating the other factors
that often have high contributions to predicting the reward such as the engagement features and position
biases. After providing the intuition of our approach, we are in a position to formally present it. But before
that, we need to introduce our setting and notation.
3.1 Setting
For any positive integer P, we define [P] ={1,2,...,P}. Vectors and matrices are denoted by bold letters.
Thei-th coordinate of a vector xisxi; unless the vector is already indexed such as xj, in which case we
writexj,i. LetA∈RP×dbe aP×dmatrix. Then for any i∈[P], thed-dimensional vector corresponding
to thei-th row ofAis denoted by Ai∈Rd. Items are referenced by integers so that [P]denotes the catalog
ofPitems. We define a slateof sizeK,s= (sℓ)ℓ∈[K]= (s1,...,sK), as aK-permutation of [P], which is an
ordered collection of Kitems from [P]. The space of all slates of size Kis denoted byS.
5Under review as submission to TMLR
We consider a contextual bandit setting where the agent interacts with users as follows. The agent observes
adx-dimensional contextvectorx∈X⊆ Rdx. After that, the agent recommends a slate s∈S, and then
receives a binary reward R∈{0,1}and a list of Kbinary ranks [r1,...,rK]∈{0,1}Kthat depend on both
the contextxand the slate s. The reward Rindicates whether the user interacted with the slate sand
for anyℓ∈[K]the rankrℓindicates whether the user interacted with the ℓ-th item in the slate, sℓ. The
user can interact with at most one item in the slate, and thus R=/summationtext
ℓ∈[K]rℓ. We let ¯R= 1−Rso that
¯R+/summationtext
ℓ∈[K]rℓ= 1. Then the vector (¯R,r1,...,rK)∈RK+1has one non-zero entry which is equal to one.
We assume that the context xdecomposes into two vectors as x= (y,z)wherey∈Rd′andz∈Rdz. Here
ydenotes the engagement features that are useful for predicting the reward of a slate, independently of its
items and the user interests. On the other hand, z∈Rdzdenotes the remaining features in the context x,
which summarize the user interests. The dimensions of zandxare varying as they can contain the list of
previously viewed items whose length may differ from one user to another. For this reason, these dimensions
are subscripted by zandx, respectively. In contrast, to simplify the notation, the dimension of y,d′, is
fixed (although it can also be varying). We give a summary of our notation in Table 2 in Appendix A.
3.2 Modeling Rank and Reward
As we highlighted before, engagement features can be strong predictors of the reward of a slate independently
of the quality of its items. Thus a model using these features while discarding the user interests might enjoy
a high likelihood. But such a model is useless for personalized recommendation as it does not learn the user
interests. Thisobservationisoftenusedtojustifyabandoninglikelihood-basedapproachesinfavorofranking.
Instead, PRRsolves this issue by carefully incorporating both the engagement features y, the user interests
featureszand the whole slate sto predict interactions accurately. The vector (¯R,r1,...,rK)∈RK+1
has exactly one non-zero entry which is equal to one (Section 3.1). Thus we model it using a categorical
distribution. Precisely, the PRRmodel has the following form
¯R,r1,...,rK|x,s∼cat/parenleftbiggθ0
Z,θ1
Z,...,θK
Z/parenrightbigg
, Z =K/summationdisplay
k=0θℓ, (1)
where ¯R,r1,...,rKandx= (y,z)are defined in Section 3.1, cat()is the categorical distribution, θ0is the
score of no interaction with the slate and θℓ, forℓ∈[K], is the score of interaction with the ℓ-th item in the
slate,sℓ. The engagement features yare used to produce the positive score θ0which is high if the chance of
no interaction with the slate is high, independently of its items. It is defined as
θ0= exp(y⊤ϕ), (2)
whereϕis a vector of learnable parameters of dimension d′>0. Similarly, let ℓ∈[K], the positive score θℓis
associated with the item in position ℓin the slate, sℓ, and is calculated in a way that captures user interests,
position biases, and interactions that occur by accident. Precisely, given a slate s= (sℓ)ℓ∈[K]= (s1,...,sK)
and user interests features z, the scoreθℓhas the following form
θℓ= exp{gΓ(z)⊤Ψsℓ}exp(γℓ) + exp(αℓ). (3)
Again this formulation is motivated by practitioners experience. The quantity exp(αℓ)denotes the additive
bias for position ℓ∈[K]in the slate. It is high if there is a high chance of interaction with the ℓ-th item
in the slate irrespective of how appealing it is to the user. This quantity also explains interactions that
are not associated at all with the recommendation (e.g., clicks by accident). The quantity exp(γℓ)is the
multiplicative bias for position ℓ∈[K]. It is high if a recommendation is boostedby being in position ℓ∈[K].
To see this, consider the example of ad placement and assume that we recommend a large slate of the form
(phone,..., microphone ). Herephoneis placed in the first position while microphone is placed in the last
one. Now suppose that the user clicked on phone. Then from a ranking perspective, we would assume that
the user prefers the phoneover the microphone . However, the user might have clicked on the phoneonly
because it was placed in the top position. PRRcaptures this through the multiplicative terms exp(γℓ).
6Under review as submission to TMLR
Finally, the main quantity of interest is the recommendation score gΓ(z)⊤Ψafora∈[P], which can be
understood as follows. The vector z∈Rdzrepresents the user interests and the parameter vector Ψsℓ∈Rd
represents the embedding of the ℓ-th item in the slate, sℓ. The vector zis first mapped into a fixed size
d-dimensional embedding space using gΓ(·). The resulting inner product gΓ(z)⊤Ψsℓproduces a positive or
negative score that quantifies how good sℓis to the user with interests z. In practice, zcan be the sequence
of previously viewed items, in which case gΓis a sequence model (Hidasi et al., 2015).
3.3 Learning
PRRhas multiple parameters ϕ,Γ,Ψ,γℓ,andαℓforℓ∈[K], which are learned using the maximum likelihood
principle. Precisely, we assume access to logged data Dnof the formDn={xi,si,¯Ri,ri,1,...,ri,K;i∈[n]},
such thatxi= (yi,zi)for anyi∈[n]. LetZi= exp(y⊤
iϕ) +/summationtext
ℓ∈[K]exp{gΓ(zi)⊤Ψsi,ℓ}exp(γℓ) + exp(αℓ)be
the normalizing constant for the i-th data-point in Dn, then log-likelihood reads
L(Dn;ϕ,Γ,Ψ,γ,α) =/summationdisplay
i∈[n]logP(¯Ri,ri,1,...,ri,K|xi,si,ϕ,Γ,Ψ,γ,α), (4)
=/summationdisplay
i∈[n]/parenleftbig
y⊤
iϕ/parenrightbig
I{¯Ri=1}+/summationdisplay
ℓ∈[K]/parenleftbig
log/parenleftbig
exp{gΓ(zi)⊤Ψsi,ℓ}exp(γℓ) + exp(αℓ)/parenrightbig/parenrightbig
I{ri,ℓ=1}−log(Zi).
Finally, the parameters are estimated as ˆϕn,ˆΓn,ˆΨn,ˆγn,ˆαn= argmaxϕ,Γ,Ψ,γ,αL(Dn;ϕ,Γ,Ψ,γ,α). With a
slight abuse of notation, we will refer to the learned parameters by ϕ,Γ,Ψ,γ,αin the sequel.
3.4 Decision Making
From Eq. (1), the probability of an interaction with the slate is
P(R= 1|x,s) = 1−P(¯R= 1|x,s) = 1−θ0
Z= 1−θ0
θ0+/summationtext
ℓ∈[K]θℓ. (5)
Then, from Eqs. (2) and (3), the decision rule follows as
argmaxs∈SP(R= 1|x,s) = argmins∈Sθ0
θ0+/summationtext
ℓ∈[K]θℓ(i)= argmaxs∈S/summationdisplay
ℓ∈[K]θℓ,
= argmaxs∈S/summationdisplay
ℓ∈[K]exp{gΓ(z)⊤Ψsℓ}exp(γℓ) + exp(αℓ)
(ii)= argmaxs∈S/summationdisplay
ℓ∈[K]exp{gΓ(z)⊤Ψsℓ}exp(γℓ), (6)
where (i)and(ii)follow from the fact that both θ0andexp(αℓ)are additive and do not depend on s. Our
goal is to reduce decision-making to a MIPS task. Thus the parametric form u⊤βmust be satisfied, which
means that the sum/summationtext
ℓ∈[K], the exponential in exp{gΓ(z)⊤Ψsℓ}and the term exp(γℓ)in Eq. (6) need to be
removed. This is achieved by first sorting the position biases as
i1,...,iK= argsort(γ). (7)
This is done once since Eq. (7) neither depend on the items nor the user. Then MIPS is performed as
s′
1,...,s′
K= argsort(gΓ(z)⊤Ψ)1:K. (8)
Finally, the recommended slate s= (s1,s2,...,sK)is obtained by rearranging the items s′
1,...,s′
Kas
s1,s2,...,sK=s′
i1,s′
i2,...,s′
iK. (9)
In other terms, we select the top-K items with the highest recommendation scores gΓ(z)⊤Ψafora∈[P].
We then place the highest scoring item into the best position, that is the position ℓ∈[K]with the largest
7Under review as submission to TMLR
value ofγℓ. Then the second-highest scoring item is placed into the second-best position, and so on. The
procedure in Eqs. (7) to (9) is equivalent to the decision rule in Eq. (6). It is also computationally efficient
as Eq. (8) can be performed roughly in O(logP)thanks to fast approximate MIPS algorithms (Malkov &
Yashunin, 2018), while Eq. (6) requires roughly O(PK). The time complexity is also improved compared to
ranking approaches by O(P/logP). This makes PRRscalable to huge action spaces.
Note thatϕ,αarenuisance parameters as they are not needed for decision making; only the recommenda-
tion scores gΓ(z)⊤Ψaand the multiplicative position biases exp(γℓ)are used in the procedure in Eqs. (7)
to (9). While not used in decision-making, learning these parameters is necessary to accurately predict the
recommendation scores. Also, including them in the model provides room for interpretability in some cases.
To summarize, PRRhas the following properties. 1)It models the joint distribution of the reward and ranks
(¯R,r1,...,rK)in the simple formulation given in Eq. (1). 2)It makes use of engagement features yin order
to help learn the recommendation signal more accurately. 3)Its recommendation scores have a parametric
form that is suitable for MIPS, which allows fast delivery of recommendations in O(logP).
When compared to prior works, PRRenjoys the advantages of both worlds, reward and ranking approaches.
Rewardbasedapproaches(Dudíketal.,2012)focusexclusivelyontherewardsignal. Thishasaveryprofound
advantage since what is optimized offline is aligned with the reward observed in A/B tests. However, the
rank signal is ignored, and this loss of information makes learning difficult, especially in large-scale tasks.
On the other hand, ranking approaches (Rendle et al., 2012) are driven by heuristics focused on proxy scores
for individual items, which are not aligned with A/B test results (Garcin et al., 2014). PRRis similar to
reward-based approaches as it directly optimizes the reward as measured in A/B tests. It is also similar
to ranking as it makes use of the rank signal. However, PRRoptimizes the reward for the whole slates and
incorporates extra factors that may influence the reward independently of the quality of recommendations.
4 EXPERIMENTS
Weevaluate PRRbycreatingsyntheticandreal-worldproblemsthatmimicthesequentialinteractionsbetween
users and recommender systems. The other evaluation alternatives consist in either using information-
retrieval metrics or IPS. Unfortunately, the former is not aligned with online A/B test results, while the
latter can suffer high bias and variance in large-scale settings (Aouali et al., 2022). Next we briefly present
our experimental design while we defer further detail to Appendix B.
4.1 Baselines
PRRoptimizestherewardofflineusingloggeddataandthusweonlycompareittooff-policyrewardoptimizing
approaches. This does not include collaborative filtering, content-based (Su & Khoshgoftaar, 2009; Lops
et al., 2011) or on-policy reinforcement learning methods (e.g., (Ie et al., 2019)). Thus we mainly compare
PRRto the methods reviewed in Sections 2.1 and 2.2. This includes IPS and its variants (Section 2.1) and
three DMs (Section 2.2) derived from PRRwhich are used to validate some of our modeling assumptions. We
do not include other direct methods because the variants of PRRshould be preferred as they are specifically
designed for our problem. The others are either designed for single-item recommendation (slates of size 1)
or for information-retrieval problems that are different from ours such as web search.
Variants of PRR:we consider three variants of PRR. First, PRR-reward uses only the reward and ignores the
rank. PRR-reward is trained on both, successful and unsuccessful slates. Second, PRR-rank only uses the
rank and is consequently trained on successful slates only. Finally, PRR-bias ignores the engagement features
yand setsθ0= exp(ϕ)whereϕis a scalar parameters ( ϕreplacesy⊤ϕ). Comparing PRRtoPRR-reward and
PRR-rank is to show the benefits of combining both signals, while comparing it to PRR-bias it to highlight
8Under review as submission to TMLR
the effect and importance of the engagement features y. The three models are summarized below.
PRR-reward: ¯R,r1,...,rK|x,s∼cat/parenleftigθ0
Z,/summationtextK
ℓ=1θℓ
Z/parenrightig
, Z =K/summationdisplay
ℓ=0θℓ,
PRR-rank: r1,...,rK|x,s∼cat/parenleftigθ1
Z,...,θK
Z/parenrightig
, Z =K/summationdisplay
ℓ=1θℓ,
PRR-bias: ¯R,r1,...,rK|x,s∼cat/parenleftigϕ
Z,θ1
Z,...,θK
Z/parenrightig
, Z =ϕ+K/summationdisplay
ℓ=1θℓ.
whereθ0andθℓforℓ∈[K]are defined in Eqs. (2) and (3) while ϕ∈RinPRR-bias is a learnable parameter.
Inverse propensity scoring: We also consider IPS estimators of the expected reward of policies that are
designedbyremovingthepreferencebiasoftheloggingpolicy π0indataDn. Thisisachievedbyre-weighting
samples using the discrepancy between the learning policy πand the logging policy π0such as
ˆVIPS
n(π) =1
nn/summationdisplay
i=1Riπ(si|zi)
π0(si|zi). (10)
This estimator is unbiased when πandπ0have common support. But it can be highly biased when this
assumption is violated (Sachdeva et al., 2020), which is common in practice. It also suffers high variance.
One way to mitigate this is to reduce the action space from slates to items (Li et al., 2018). This is achieved
by assuming that the reward Ris the sum of rank r1,...,rK, and that the ℓ-th rank,rℓ, only depends on
the itemsℓand its position ℓ. This allows estimating the expected reward of the learning policy πas
ˆVIIPS(π) =1
nn/summationdisplay
i=1K/summationdisplay
ℓ=1ri,ℓπ(si,ℓ,ℓ|zi)
π0(si,ℓ,ℓ|zi), (11)
whereπ(a,ℓ|z)andπ0(a,ℓ|z)are the marginal probabilities that the learning policy πand the logging policy
π0place the item ain position ℓ∈[K]given user interests z, respectively. Note that in practice computing
these marginals is often intractable for both πandπ0; in which case approximation must be employed.
The next step is to optimize the estimator ( ˆVIPS
n(π)orˆVIIPS
n(π)) to find the policy that will be used for
decision-making (Swaminathan & Joachims, 2015). To achieve this, we need to parameterize the learning
policyπ. Here we assume that πis parametrized as a factored softmax
π(s|z) =πΞ,β,K(s|z) =K/productdisplay
ℓ=1pΞ,β(sℓ|z), p Ξ,β(a|z) =exp{fΞ(z)⊤βa}/summationtext
a′∈[P]exp{fΞ(z)⊤βa′},(12)
whereβa∈Rdis the embedding of item aandfΞmaps user interests z∈Rdzinto ad-dimensional
embedding. Finally, ˆVIIPS
n(π)also requires the marginal probabilities π(a,ℓ|z)andπ0(a,ℓ|z). In our case,
π(a,ℓ|z) =pΞ,β(a|z)while we may need to approximate π0(a,ℓ|z)depending on the logging policy. While
convenient, factored policies have significant limitations. In particular, IIPS with factored policies might
cause the learned policy to converge to selecting slates with repeated items, which is illegal. Thus to be fair
to IIPS, we use sampling without replacement in decision making. Another alternative to mitigate this is
the top-K heuristic (Chen et al., 2019) which causes the probability mass in πΞ,β,K(s|z)to be spread out
over the top-K high scoring items rather than a single one. We denote the IIPS estimator combined with
the top-K heuristic by top-K IIPS .
4.2 Synthetic Problems
We design a simulated A/B test protocol that takes different recommender systems as input and outputs
their respective reward. We first define the problem instance consisting of the true parameters (oracle)
and the logging policy as {ϕ,Ψ,γ,α,gΓ(·),Py(·),Pz(·),PK(·)}andπ0. HerePy(·),Pz(·), andPK(·)are the
9Under review as submission to TMLR
0.000.010.020.030.040.050.060.07logging: uniform1k items
0.000.020.040.060.080.100.125k items
0.000.020.040.060.080.100.1210k items
2 4 6 80.000.010.020.030.040.050.060.07logging: top-K pop1k items
2 4 6 8
slate size0.000.020.040.060.080.100.125k items
2 4 6 80.000.020.040.060.080.100.1210k items
oracle
PRR
PRR-bias
PRR-rank
PRR-reward
IPS
IIPS
top-K IIPS
top-K pop
uniform
Figure 2: The reward (y-axis) of PRRand baselines in synthetic problems with varying slate sizes (x-axis), number
of items (columns) and logging policies (rows). The shaded areas represent uncertainty and they are small since we
run long A/B tests with ntest= 100k .
distributions of the engagement features, the user interests features and the slate size, respectively. Given
the oracle, we produce offline training logs and propensity scores {D,P}by running the logging policy π0in
our simulated environment and observing its reward and rank. These logs are then used to train PRRand
the baselines. After training, a simulated A/B test is used for testing. We defer a detailed description of our
simulation environment for reproducibility to Appendix B.1. For instance, we summarize in Algorithm 1 the
data generation process while we present in Algorithm 2 the simulated A/B test.
We consider two non-personalized logging policies. (a)uniform: this policy samples uniformly without
replacement Kitems from the catalog [P]. That isπ0(s|z) =1
P(P−1)...(P−K+1)for any slates∈Sand any
user interests z. The marginal distribution can be computed in closed-form as π0(a,ℓ|z) = 1/P.(b)top-K
pop:this policy samples without replacement Kitems where the probability of an item ais proportional to
theL2norm of its embedding, ∥Ψa∥. This is based on the intuition that a large value of ∥Ψa∥means that
itemais recommended more often and thus it is more popular. We stress that this logging policy has access
to the true embeddings Ψof the simulated environment (Algorithm 1). Here the marginal distribution is
intractable and we simply approximate is as π0(a,ℓ|z)≈∥Ψa∥//summationtext
a′∥Ψa′∥for IIPS.
In Fig. 2, we report the average A/B test reward of PRRwith varying slate sizes, number of items and logging
policies using 100k training samples. Overall, we observe that PRRoutperforms the baselines across the
different settings. Next we summarize the general trends of algorithms.
(a) Varying logging policy: models that use the reward only, IPSand PRR-reward , favor uniform
logging policies while those that use only the rank, IIPSand PRR-rank perform better with the
top-K pop logging policy. PRR-bias discards the slate-level features yand uses a single parameter
ϕfor all slates. Thus PRR-bias benefits from uniform logging policies as they allow learning ϕthat
works well across all slates. Indeed, in Fig. 2 the gap between PRRand PRR-bias shrinks for the
uniform logging policy. Finally, the performance of PRRis relatively stable for both logging policies.
10Under review as submission to TMLR
2 4 6 8
slate size0.000.050.100.150.200.250.30reward
100 100000 200000
catalog size0102030405060training time
uniform
top-K pop
IIPS
top-K IIPS
IPS
PRR
PRR-rank
PRR-reward
Figure 3: On the left-hand side , we report the reward (y-axis) of PRRand baselines in session completion
problems with varying slate sizes (x-axis). On the right-hand side , we report the training time (y-axis) of PRR
and baselines in session completion problems with varying catalog sizes (x-axis).
(b) Varying slate size: the performance of models that use the reward only, IPSand PRR-reward ,
deteriorateswhenthemaximumslatesizeincreases. Ontheotherhand, thosethatuseonlytherank,
IIPSand PRR-rank , benefit from larger slates as this leads to displaying more item comparisons.
The addition of the top-Kheuristic improves the performance of IIPSin some cases by spreading
the mass over different items, making it not only focus on retrieving one but several high scoring
items. However, the increase in performance is not always guaranteed which might be due to our
choice of hyperparameters or our approximation of the marginal distributions of policies. Finally,
PRRperforms well across all slate sizes as it uses both the reward and rank.
(c) Varying number of items: the models that use the rank benefit from large slates. Here we
observe that the increase in performance is more significant for large catalogs. In contrast, models
that use only the reward suffer a drop in performance when the number of items increases.
4.3 Session Completion Problems
We use the Twitch dataset (Rappaz et al., 2021) to evaluate PRRon user session completion tasks. Roughly
speaking, we process the dataset such that each user uhas a listIuthat contains the items that the user
interacted with. We randomly split these user-item interactions Iuinto two parts,Iview
uandIhide
u.Iview
uis
observed whileIhide
uis hidden and should be predicted. That is, the baselines are evaluated based on their
ability to predict the complete session of a user Iuby only observing a part of it, Iview
u.
Logged dataDnis collected using the top-K pop logging policy. In each iteration i∈[n], we ran-
domly sample a user ui. Then, we recommend a slate si= (si,ℓ)ℓ∈[K]by sampling without replace-
mentKitems with probabilities proportional to their popularity, i.e., their number of occurrences in the
dataset. After recommending si= (si,ℓ)ℓ∈[K], we construct a binary vector bi= (I{si,ℓ∈Ihideu})ℓ∈[K]=
(I{si,1∈Ihideu},I{si,2∈Ihideu},...,I{si,K∈Ihideu})∈RK. In other words, for any ℓ∈[K],bi,ℓ= 1if theℓ-th item
in the slate, si,ℓ, is in the hidden user-item interaction Ihide
u, andbi,ℓ= 0otherwise. This binary vector bi
is then used to generate the reward and rank signals Ri,ri,1,...,ri,Kfor useruiand slatesi. This allows
constructing logged data Dn. After training the baselines on Dn, they are evaluated following the data
collection process except that they make decisions instead of the logging policy π0. More specific details
about the data generation and testing processes are given in Appendix B.2.
In Fig. 3 (left-hand side), we report the results of PRRand the baselines on user session completion tasks.
Note that there are no engagement features in this problem. Thus PRRis the same as PRR-bias and hence we
only include PRRin Fig. 3. Overall, PRRoutperforms the other baselines in the session completion problem.
We also observe that PRR-reward has good performance in this scenario, while all the other methods have
comparable performance which is lower than that of PRR-reward .
11Under review as submission to TMLR
Table 1: Properties of PRRand the baselines.
Method Computational efficiency Statistical efficiency
PRR O(K) High
PRR-bias O(K) Medium
PRR-rank O(K) Medium
PRR-reward O(K) Low
IPS O(P) Low
IIPS O(P) Medium
Top-K IIPS O(P) Medium
4.4 Computational Efficiency
We assess the training speed of the algorithms with respect to the catalog and slate sizes PandK. First,
PRRand its variants compute K+ 1scoresθ0,...,θKand normalize them using Z=/summationtextK
ℓ=0θℓ. Therefore,
evaluating PRRand its variants in one data-point costs roughly O(K), where we omit the cost of computing
the scores since it is the same for all algorithms. In contrast, IPSand its variants compute a softmax over
the catalog. This requires computing the normalization constant/summationtext
a′∈[P]exp{fΞ(z)⊤βa′}in (12). Thus
the evaluation cost of IPSand its variants is roughly O(P). This is very costly compared to O(K)in
realistic settings where P≫K. An additional consideration to compare the training speed of algorithms is
whether they use successful slates only, which significantly reduces the size of training data. Taking this into
account, the fastest of all algorithms is the PRR-rank since its evaluation speed is O(K)and it is trained on
successful slates only. In our experiments, PRR-rank is≈20×faster to train than IPS. These computational
dependencies on KandPare also highlighted in Fig. 3 (right-hand side). In particular, IPStraining time
scales linearly in Pwhile PRRdo not have such a dependency on P.
4.5 Limitations of PRR
After presenting and evaluating our algorithm, we are in a position to discuss its limitations. (a) Modeling
capacity: PRRis trained to predict the reward of any slate which is a complex task. Thus high-dimensional
embeddings might be needed to make accurate and calibrated predictions. As a result, the MIPS task
produced by PRRmight require high dimensional embeddings that do not conform to engineering constraints.
A possible path to mitigate this is to observe that accurately predicting the reward is sufficient but not
necessary for the recommendation task. When recommending, we only need to find the best slate. This is a
simplertaskandmightbeachievedwithmuchsmallerembeddingsthanthoserequiredby PRR. Therefore,one
can train PRRwith high-dimensional embeddings. Then optimize policies parametrized with low-dimensional
embeddings using the learned reward estimates of PRRinstead of IPS. The learned policy that fitst the
engineering constraints will be then used in decision making. (b) Theoretical analysis: one of the main
appeals of IPS is that it can be analyzed theoretically. This is difficult for PRRalthough a Bayesian analysis
where we assume that the model of the environment is the same as that of PRRmight be possible.
5 CONCLUSION
We present PRR, a scalable probabilistic model for personalized slate recommendation. PRRefficiently esti-
mates the probability of a slate being successful by combining the reward and rank signals. It also optimizes
the reward of the whole slate by distinguishing between slate-level and item-level features. Experiments
attest that PRRoutperforms the baselines and it is far more scalable, in both training and decision making.
The shortcomings of ourapproach are discussed in Section 4.5 andweleaveaddressing themto future works.
12Under review as submission to TMLR
References
Imad Aouali, Sergey Ivanov, Mike Gartrell, David Rohde, Flavian Vasile, VictorZaytsev, and Diego Legrand.
Combiningrewardandranksignalsforslaterecommendation, 2021. URL https://arxiv.org/abs/2107.
12455.
Imad Aouali, Amine Benhalloum, Martin Bompaire, Benjamin Heymann, Olivier Jeunen, David Rohde,
Otmane Sakhi, and Flavian Vasile. Offline evaluation of reward-optimizing recommender systems: The
case of simulation, 2022. URL https://arxiv.org/abs/2209.08642 .
Azin Ashkan and Charles LA Clarke. Modeling browsing behavior for click analysis in sponsored search.
InProceedings of the 21st ACM international conference on Information and knowledge management , pp.
2015–2019, 2012.
James Bennett, Stan Lanning, et al. The netflix prize. In Proceedings of KDD cup and workshop , volume
2007, pp. 35. New York, 2007.
Fedor Borisyuk, Krishnaram Kenthapadi, David Stein, and Bo Zhao. Casmos: A framework for learning
candidate selection models over structured queries and documents. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 441–450, 2016.
LéonBottou,JonasPeters,JoaquinQuiñonero-Candela,DenisXCharles,DMaxChickering,ElonPortugaly,
Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The
example of computational advertising. Journal of Machine Learning Research , 14(11), 2013.
Olivier Chapelle and Ya Zhang. A dynamic bayesian network click model for web search ranking. In
Proceedings of the 18th international conference on World wide web , pp. 1–10, 2009.
Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. Top-k off-
policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM International
Conference on Web Search and Data Mining , pp. 456–464, 2019.
Aleksandr Chuklin, Pavel Serdyukov, and Maarten De Rijke. Modeling clicks beyond the first result page.
InProceedings of the 22nd ACM international conference on Information & Knowledge Management , pp.
1217–1220, 2013.
Aleksandr Chuklin, Ilya Markov, and Maarten de Rijke. Click models for web search. Synthesis lectures on
information concepts, retrieval, and services , 7(3):1–115, 2015.
Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. An experimental comparison of click position-
bias models. In Proceedings of the 2008 international conference on web search and data mining , pp. 87–94,
2008.
Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li. Sample-efficient nonstationary policy eval-
uation for contextual bandits. In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial
Intelligence , UAI’12, pp. 247–254, Arlington, Virginia, USA, 2012. AUAI Press. ISBN 9780974903989.
Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and
optimization. Statistical Science , 29(4):485–511, 2014.
Georges E. Dupret and Benjamin Piwowarski. A user browsing model to predict search engine click data
from past observations. SIGIR ’08, 2008.
F. Garcin, B. Faltings, O. Donatsch, A. Alazzawi, C. Bruttin, and A. Huber. Offline and Online Evaluation
of News Recommender Systems at Swissinfo.Ch. In Proc. of the 8th ACM Conference on Recommender
Systems, RecSys ’14, pp. 169–176, 2014.
Alexandre Gilotte, Clément Calauzènes, Thomas Nedelec, Alexandre Abraham, and Simon Dollé. Offline
a/b testing for recommender systems. In Proceedings of the Eleventh ACM International Conference on
Web Search and Data Mining , pp. 198–206, 2018.
13Under review as submission to TMLR
Fan Guo, Chao Liu, and Yi Min Wang. Efficient multiple-click models in web search. In Proceedings of the
Second ACM International Conference on Web Search and Data Mining , WSDM ’09, 2009.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions
on interactive intelligent systems (tiis) , 5(4):1–19, 2015.
Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommenda-
tions with recurrent neural networks. arXiv preprint arXiv:1511.06939 , 2015.
Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement from a finite
universe. Journal of the American statistical Association , 47(260):663–685, 1952.
Botao Hu, Yuchen Zhang, Weizhu Chen, Gang Wang, and Qiang Yang. Characterizing search intent diversity
into click models. In Proceedings of the 20th international conference on World wide web , pp. 17–26, 2011.
Jeff Huang, Ryen W White, Georg Buscher, and Kuansan Wang. Improving searcher models using mouse
cursor activity. In Proceedings of the 35th international ACM SIGIR conference on Research and devel-
opment in information retrieval , pp. 195–204, 2012.
Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Tushar
Chandra, and Craig Boutilier. Slateq: A tractable decomposition for reinforcement learning with recom-
mendation sets. 2019.
Olivier Jeunen and Bart Goethals. Pessimistic reward models for off-policy learning in recommendation. In
Fifteenth ACM Conference on Recommender Systems , pp. 63–74, 2021.
Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. Accurately interpreting
clickthrough data as implicit feedback. In Acm Sigir Forum , volume 51, pp. 4–11. Acm New York, NY,
USA, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Olivier Koch, Amine Benhalloum, Guillaume Genthial, Denis Kuzin, and Dmitry Parfenchik. Lightweight
representation learning for efficient and scalable recommendation. arXiv e-prints , pp. arXiv–2101, 2021.
RonKohaviandRogerLongbotham. Onlinecontrolledexperimentsanda/btesting. Encyclopedia of machine
learning and data mining , 7(8):922–929, 2017.
Shuai Li, Yasin Abbasi-Yadkori, Branislav Kveton, S Muthukrishnan, Vishwa Vinay, and Zheng Wen. Offline
evaluation of ranking policies with click models. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining , pp. 1685–1694, 2018.
Yiqun Liu, Chao Wang, Ke Zhou, Jianyun Nie, Min Zhang, and Shaoping Ma. From skimming to reading:
A two-stage examination model for web search. In Proceedings of the 23rd ACM International Conference
on Conference on Information and Knowledge Management , pp. 849–858, 2014.
Pasquale Lops, Marco de Gemmis, and Giovanni Semeraro. Content-based recommender systems: State of
the art and trends. Recommender systems handbook , pp. 73–105, 2011.
Yu A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search using
hierarchicalnavigablesmallworldgraphs. IEEE transactions on pattern analysis and machine intelligence ,
42(4):824–836, 2018.
James McInerney, Brian Brost, Praveen Chandar, Rishabh Mehrotra, and Benjamin Carterette. Counterfac-
tual evaluation of slate recommendations with sequential reward interactions. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 1779–1788, 2020.
Ivens Portugal, Paulo Alencar, and Donald Cowan. The use of machine learning algorithms in recommender
systems: A systematic review. Expert Systems with Applications , 97:205–227, 2018.
14Under review as submission to TMLR
Jérémie Rappaz, Julian McAuley, and Karl Aberer. Recommendation on live-streaming platforms: Dynamic
availabilityandrepeatconsumption. In Fifteenth ACM Conference on Recommender Systems , pp.390–399,
2021.
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian person-
alized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 , 2012.
Noveen Sachdeva, Yi Su, and Thorsten Joachims. Off-policy bandits with deficient support. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 965–975,
2020.
Otmane Sakhi, Stephen Bonner, David Rohde, and Flavian Vasile. Blob: A probabilistic model for recom-
mendation that combines organic and bandit signals. In Proceedings of the 26th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data Mining , pp. 783–793, 2020.
Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search
(mips).Advances in neural information processing systems , 27, 2014.
Xiaoyuan Su and Taghi M Khoshgoftaar. A survey of collaborative filtering techniques. Advances in artificial
intelligence , 2009, 2009.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit
feedback. In International Conference on Machine Learning , pp. 814–823. PMLR, 2015.
Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford, Damien Jose, and
Imed Zitouni. Off-policy evaluation for slate recommendation. Advances in Neural Information Processing
Systems, 30, 2017.
Yisong Yue, Rajan Patel, and Hein Roehrig. Beyond position bias: Examining result attractiveness as a
source of presentation bias in clickthrough data. In Proceedings of the 19th International Conference on
World Wide Web , WWW ’10, pp. 1011–1018, 2010.
Yuchen Zhang, Weizhu Chen, Dong Wang, and Qiang Yang. User-click modeling for understanding and pre-
dicting search-behavior. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge
discovery and data mining , pp. 1388–1396, 2011.
15Under review as submission to TMLR
A Summary of Notation
We provide a summary of our notation in Table 2.
Table 2: Notation.
Notation Definition
x= (y,z)∈Rdxcontext.
y∈Rd′engagement features.
z∈Rdz user interests features.
R∈{0,1} reward indicator.
¯R∈{0,1} regret indicator.
rℓ∈{0,1} rank indicator of the item in position ℓ∈[K].
ϕ∈Rd′engagement parameters.
γℓ∈R multiplicative position bias in position ℓ∈[K].
αℓ∈R additive position bias in position ℓ∈[K].
gΓ(z)∈Rduser embedding.
Ψ∈RP×ditems embeddings.
θ0∈R score for no-interaction with the slate.
θℓ∈R score for an interaction with the item in position ℓ∈[K].
s= (s1,...,sK)slate ofKrecommendations sℓ∈[P]for anyℓ∈[K].
B Experimental Design
Herewegivemoreinformationaboutourexperiments. WestartwiththesyntheticproblemsinAppendixB.1
and then present the session completion problems in Appendix B.2.
B.1 Synthetic Problems
We design a simulated A/B test protocol that takes different recommender systems as input and outputs
their respective reward. We first define the problem instance consisting of the true parameters (oracle)
and the logging policy as {ϕ,Ψ,γ,α,gΓ(·),Py(·),Pz(·),PK(·)}andπ0. HerePy(·),Pz(·)andPK(·)are the
distributions of the engagement features, the user interests features and the slate size, respectively. Given
the oracle, we produce offline training logs and propensity scores {D,P}by running the logging policy π0
as described in Algorithm 1. These logs are then used to train PRRand competing baselines. After training,
the simulated A/B test in Algorithm 2 is used for testing.
In all our experiments, the true parameters are sampled randomly as
ϕ∼N(µϕ,Σϕ), Ψ∼N(µψ,Σψ), Γ∼N(µΓ,ΣΓ),γ∼N(µγ,Σγ),α∼N(µα,Σα).
For each user, the engagement features yare sampled randomly following the distribution Py=N(µy,Σy).
For the interest features z, we assume that there are Ltopics andzis consequently generated as follows.
For each user u, we randomly sample the number of topics that interest user uasLu∼1 +Poison (3).
After that, we uniformly sample Lutopics that interest the user from [L]. It follows that z∈RL(dz=L)
is represented as a binary vector such as zℓ= 1if the user is interested in topic ℓ, andzℓ= 0otherwise.
For simplicity, the mapping gΓis linear and defined as gΓ(z) =Γz. Finally, for each user, the slate size is
sampled uniformly from [K]whereKis the maximum slate size. For reproducibility, the SEEDis fixed at 42
which was chosen randomly. We considered realistic settings with relatively large catalogs and slates. The
catalog size varies between 1000and10000and the slate size varies between 2and8.PRRis suitable for
larger catalogs but the baselines become very slow to train. This explains our choice of 10000as a maximum
catalog size. In optimization, we use Adam (Kingma & Ba, 2014) with a learning rate of 0.005 for 100 epochs
using mini-batches of size 516.
16Under review as submission to TMLR
Algorithm 1: Synthetic training logs
Input:oracle parameters {ϕ,Ψ,γ,α,gΓ(·),Py(·),Pz(·),PK(·)}, logging policy π0(s|x), marginal
logging policies π0(s1|x),...,π 0(sK|x), number of training samples ntrain.
Output: logsD, propensity scores P.
D←{},P←{}
fori= 1,...,n traindo
yi∼Py(·),zi∼Pz(·), Ki∼PK(·)
si= (si,1,...,si,Ki)∼π0(·|zi)
θ0←exp(y⊤
iϕ)
forℓ= 1,...,Kido
θℓ←exp(gΓ(zi)⊤Ψsi,ℓ) exp(γℓ) + exp(αℓ)
end
¯Ri,ri,1,...,ri,K∼cat/parenleftbigθ0
Z,θ1
Z,...,θK
Z/parenrightbig
, Z =/summationtextKi
ℓ=0θℓ
D←D∪{xi,si,¯Ri,ri,1,...,ri,K}
P←P∪{π0(si|zi),π0(si,1,1|zi),...,π 0(si,K,K|zi)}
end
Algorithm 2: Synthetic A/B test
Input:oracle parameters {ϕ,Ψ,γ,α,gΓ(·),Py(·),Pz(·),PK(·)}, decision rules daanddb, number of
testing samples ntest.
Output: lists of rewards RaandRb.
Ra←{}, R b←{}
fori= 1,...,n testdo
yi∼Py(·),zi∼Pz(·), Ki∼PK(·)
form∈{a,b}do
si= (si,1,...,si,Ki)←dm(yi,zi)(wheredmis the decision rule of m∈{a,b})
θ0←exp(y⊤
iϕ)
forℓ= 1,...,Kido
θℓ←exp(gΓ(zi)⊤Ψsi,ℓ) exp(γℓ) + exp(αℓ)
end
Rm←Rm∪{1−θ0
Z}, Z =/summationtextKi
ℓ=1θℓ
end
end
B.2 Session Completion Problems
We use the Twitch dataset (Rappaz et al., 2021) to evaluate PRRon user session completion tasks. For each
user, we randomly split the user-item interactions Iuinto two parts, an observed part by the baselines Iview
u
and a hidden one Ihide
uthat should be predicted. The task is to complete the observed user session Iview
uto
retrieve the whole session of a user Iu. Logged dataDnis collected using the top-K pop logging policy as
follows. In each iteration i∈[n], we randomly sample a user ui. Then, we recommend a slate siby sampling
without replacement Kitems with probabilities proportional to their popularity. Precisely, the probability
of selecting an item aisca//summationtext
a′ca′wherecais the number of occurrences of item ain the dataset. After
that, we construct a binary vector bi= (I{si,1∈Ihideui},I{si,2∈Ihideui},...,I{si,K∈Ihideui})∈RK. In other words, for
anyℓ∈[K],bi,ℓ= 1if theℓ-th item in the slate, si,ℓ, is in the hidden user-item interaction Ihide
u, andbi,ℓ= 0
otherwise. This binary vector biis then used to generate the reward and rank signals as
¯Ri,ri,1,...,ri,K∼cat/parenleftigg
β0K
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ,β1bi,1
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ,...,βKbi,K
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ/parenrightigg
,
17Under review as submission to TMLR
whereβ0andβℓforℓ∈[K]are sampled from N(3,9)andUniform([16]) , respectively. This allows us to
generate a dataset in the form
Dn={Iview
ui,si,¯Ri,ri,1,...,ri,K;i∈[n]}.
HereIview
uican be seen as the user interest features. This data generation process is summarized in Algo-
rithm 3.
Algorithm 3: Session completion training logs
Input:set ofUusers, set of Pitems, user-item interactions Iview
uandIhide
ufor any user u∈[U],
number of occurrences cafor any item a∈[P], maximum slate size K, position biases βℓfor
ℓ∈[K], logging policy π0, number of training samples ntrain.
Output: logsD, propensity scores P.
D←{},P←{}
fori= 1,...,n traindo
ui∼Uniform([U])
Ki∼Uniform([K])
si= (si,1,...,si,Ki)∼π0(·|Iview
ui)
bi= (I{si,1∈Ihideui},I{si,2∈Ihideui},...,I{si,K∈Ihideui})
¯Ri,ri,1,...,ri,K∼cat/parenleftig
β0K
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ,β1bi,1
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ,...,βKbi,K
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ/parenrightig
D←D∪{Iview
ui,si,¯Ri,ri,1,...,ri,K}
P←P∪{π0(si|Iview
ui),π0(si,1,1|Iview
ui),...,π 0(si,K,K|Iview
ui)}
end
After training the baselines on Dn, they are evaluated following the data collection process except they are
run instead of the logging policy π0. More specific details about the data generation and testing processes
are given in Appendix B.2.
Algorithm 4: Session completion A/B test
Input:set ofUusers, set of Pitems, user-item interactions Iview
uandIhide
ufor any user u∈[U],
number of occurrences cafor any item a∈[P], maximum slate size K, position biases βℓfor
ℓ∈[K], logging policy π0, number of testing samples ntest.
Output: lists of rewards RaandRb.
Ra←{}, R b←{}
fori= 1,...,n testdo
ui∼Uniform([U])
Ki∼Uniform([K])
form∈{a,b}do
si= (si,1,...,si,Ki)←dm(Iview
ui)(dmis the decision rule of m∈{a,b})
bi= (I{si,1∈Ihideui},I{si,2∈Ihideui},...,I{si,K∈Ihideui})
¯Ri,ri,1,...,ri,K∼cat/parenleftig
β0K
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ,β1bi,1
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ,...,βKbi,K
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ/parenrightig
Rm←Rm∪/braceleftbigg/summationtext
ℓ∈[K]βℓbi,ℓ
β0K+/summationtext
ℓ∈[K]βℓbi,ℓ/bracerightbigg
end
end
18