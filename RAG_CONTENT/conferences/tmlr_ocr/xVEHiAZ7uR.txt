Under review as submission to TMLR
Adam-family Methods with Decoupled Weight Decay in
Deep Learning
Anonymous authors
Paper under double-blind review
Abstract
Inthispaper, weinvestigatetheconvergencepropertiesofawideclassofAdam-familymeth-
ods for minimizing quadratically regularized nonsmooth nonconvex optimization problems,
especially in the context of training nonsmooth neural networks with weight decay. Moti-
vated by AdamW, we propose a novel framework for Adam-family methods with decoupled
weight decay. Within our framework, the estimators for the first-order and second-order
moments of stochastic subgradients are updated independently of the weight decay term.
Under mild assumptions and with non-diminishing stepsizes for updating the primary op-
timization variables, we establish the convergence properties of our proposed framework.
In addition, we show that our proposed framework encompasses a wide variety of well-
known Adam-family methods, hence offering convergence guarantees for these methods in
the training of nonsmooth neural networks. More importantly, compared to the existing
results on the choices of the parameters for the moment terms in Adam, we show that our
proposed framework provides more flexibility for these parameters. As a practical appli-
cation of our proposed framework, we propose a novel Adam-family method named Adam
with Decoupled Weight Decay (AdamD), and establish its convergence properties under
mild conditions. Numerical experiments demonstrate that AdamD outperforms Adam and
is comparable to AdamW, in the aspects of both generalization performance and efficiency.
1 Introduction
We consider the following unconstrained stochastic optimization problem:
min
x∈Rng(x) :=f(x) +σ
2∥x∥2, (UOP)
where the function f:Rn→Ris assumed to be locally Lipschitz continuous and possibly nonsmooth over
Rn. Moreover, the constant σ > 0is the penalty parameter for the quadratic regularization term. Such
a regularization term is also known as the weight decay term, which is widely employed to enhance the
generalization performance in training neural networks (Bos & Chug, 1996; Krogh & Hertz, 1991).
Stochastic Gradient Descent (SGD) is one of the most fundamental methods for solving (UOP). In SGD, all
coordinates of the variable xare updated using the same stepsize (i.e., learning rate). To accelerate SGD,
Kingma and Ba (Kingma & Ba, 2015) developed the widely used Adam method, which adjusts coordinate-
wise stepsizes based on first-order and second-order moments of the stochastic gradients. Due to its high
efficiency in training neural networks, Adam has become one of the most popular choices for various neural
network optimization tasks.
Motivated by Adam, numerous efficient Adam-family methods have been developed, such as AdaBelief
(Zhuang et al., 2020), AMSGrad (Reddi et al., 2018), Yogi (Zaheer et al., 2018), etc. From a theoretical
perspective, the majority of existing works (Barakat & Bianchi, 2021; Guo et al., 2021; Shi et al., 2021;
Wang et al., 2022; Zaheer et al., 2018; Zhang et al., 2022; Zou et al., 2019) establish convergence properties
for these Adam-family methods, based on the assumption that fis continuously differentiable over Rn.
However, as emphasized in (Bolte et al., 2021; Bolte & Pauwels, 2021; Bolte et al., 2022b), nonsmooth
1Under review as submission to TMLR
activation functions, including ReLU and leaky ReLU, are popular choices in building neural networks. For
any neural network built from these nonsmooth activation functions, its loss function is usually nonsmooth
and lacks Clarke regularity (e.g., differentiability, weak convexity, etc.). Consequently, these existing works
are unable to provide convergence guarantees for their analyzed methods in the training of nonsmooth neural
networks.
1.1 Existing works on training nonsmooth neural networks
Innonsmoothoptimization, ithasbeendemonstratedin(Daniilidis&Drusvyatskiy,2020)thatageneralLip-
schitz continuous function fcan exhibit highly pathological properties, leading to the failure of subgradient
descent method to find a critical point of f. Moreover, the chain rule may fail for the Clarke subdifferential
(Clarke, 1990) of the loss function of a nonsmooth neural network. Specifically, when we differentiate the
loss function of a nonsmooth neural network using automatic differentiation (AD) algorithms, the outputs
may not be contained in the Clarke subdifferential of f(Bolte & Pauwels, 2020).
Consequently, most of the existing works restrict their analysis to the class of path-differentiable functions
(Bolte & Pauwels, 2021, Definition 3). For any path-differentiable function f, there exists a graph-closed
set-valued mapping Df, calledconservative field forf, such that for any absolutely continuous mapping
γ: [0,∞)→Rn, it holds that f(γ(t))−f(γ(0)) =/integraltextt
0maxd∈Df(γ(s))⟨˙γ(s),d⟩dsfor anyt≥0. It is worth
mentioning that the most important choice of the conservative field Dfis the Clarke subdifferential of
f. Moreover, as discussed in (Bolte & Pauwels, 2021; Castera et al., 2021; Davis et al., 2020), the class
of path-differentiable functions are general enough to cover a wide range of objective functions in neural
network training tasks, especially when the neural networks employ nonsmooth building blocks, such as
the ReLU activation function. In addition, Bolte & Pauwels (2020; 2021) show that the outputs of AD
algorithms in differentiating nonsmooth neural networks are contained in a conservative field of the loss
function. Therefore, the concept of the conservative field is capable of characterizing the outputs of AD
algorithms, which are implemented in training nonsmooth neural networks in practice.
Based on the stochastic approximation frameworks (Benaïm, 2006; Benaïm et al., 2005; Borkar, 2009; Davis
et al., 2020), several existing works have investigated the convergence properties of stochastic subgradient
methods in training nonsmooth neural networks. In particular, Bolte & Pauwels (2021); Davis et al. (2020)
study the convergence properties of SGD and proximal SGD for minimizing nonsmooth path-differentiable
functions. Moreover, (Castera et al., 2021) proposes the inertial Newton algorithm (INNA), which can be
regarded as a variant of momentum-accelerated SGD method. Additionally, Le (2023); Ruszczyński (2020);
Xiao et al. (2023b) establish the convergence properties of SGD with heavy-ball momentum. Furthermore,
Hu et al. (2022a;b) apply these methods to solve manifold optimization problems based on the constraint
dissolving approach (Xiao et al., 2023c). In addition, Gürbüzbalaban et al. (2022); Ruszczynski (2021) design
stochastic subgradient methods for solving multi-level nested optimization problems.
1.1.1 Challenges from non-diminishing stepsizes in Adam
With the concept of conservative field, Adam utilizes the following framework when applied to solve (UOP):


gk=dk+ξk+1,
mk+1= (1−θk)mk+θk(gk+σxk),
vk+1= (1−βk)vk+βk(gk+σxk)2,
xk+1=xk−ηk(√vk+1+ε)−1⊙mk+1.(1)
Here,gkis a stochastic subgradient of fatxk, in the sense that dkrepresents a possibly inexact evaluation
ofDf(xk)andξk+1is a random vector characterizing the evaluation noise. The operators ⊙and(·)pdenote
element-wise multiplication and element-wise p-th power of a given vector, respectively. The sequences {mk}
and{vk}, referred to as momentum terms and estimators respectively, are updated to track the first-order
and second-order moments of {gk+σxk}. The sequences{ηk},{θk}, and{βk}represent the stepsizes for the
primal variables{xk}, the parameters for the momentum terms {mk}, and the parameters for the estimators
{vk}, respectively.
2Under review as submission to TMLR
In the framework (1), the weight decay term is integrated with the function fthroughout the iterations.
As a result, we can directly apply the existing convergence results on Adam to analyze the convergence
properties of the framework (1). In particular, when fis a nonsmooth path-differentiable function, (Xiao
et al., 2023a) investigates the convergence of a class of Adam-family methods based on the frameworks
proposed by (Benaïm et al., 2005; Bianchi et al., 2022; Davis et al., 2020). However, in the analysis of (Xiao
et al., 2023a), the stepsizes and parameters sequences are assumed to be diminishing and single-timescale,
in the sense that {ηk},{θk}and{βk}converge to 0at the same rate as kgoes to infinity.
Beyond the single-timescale scheme, some existing works (Reddi et al., 2018; Zhang et al., 2022) establish the
convergence of Adam for continuously differentiable fwith{θk}and{βk}fixed as constants. In particular,
(Zhang et al., 2022) proves that for any θ∈(0,1)andηk=O(1/√
k), there exists a sufficiently small β
that forces{xk}to stabilize within a neighborhood of the critical points of g. However, their analyses are
restricted to continuously differentiable objectives. Therefore, these results are not capable of explaining
the convergence of Adam in a wide range of practical settings, where the neural networks are built from
nonsmooth blocks.
Furthermore, in establishing the convergence properties for stochastic subgradient methods, the diminishing
stepsizes is a common assumption, as it leads to the almost sure convergence of the iterates {xk}to critical
points under various assumptions (Benaïm et al., 2005; Bolte et al., 2022a; Bolte & Pauwels, 2021; Castera
etal.,2021;Davisetal.,2020;Le,2023;Ruszczyński,2020;Xiaoetal.,2023a;b). However,fortheconvergence
of Adam, the results in (Reddi et al., 2018; Zhang et al., 2022) illustrate that, even if the sequence {ηk}is
diminishing, the sequence {xk}is only guaranteed to converge to a prefixed neighborhood of critical points.
Furthermore, Bianchi et al. (2022); Josz et al. (2023) show that with nonsmooth path-differentiable objective
functions and a fixed stepsize, the iterates of SGD only converges to a neighborhood of the Df-stationary
points offalmost surely. However, their analysis is restricted to SGD and SGD with heavy-ball momentum,
and cannot be extended to Adam. Given the fact that non-diminishing stepsizes (i.e., lim infk→∞ηk>0) are
widelyemployedinmostcomputationalframeworks, itisthusimportantforustoinvestigatetheconvergence
properties of the Adam-family methods in cases where the sequence of stepsizes {ηk}is non-diminishing.
1.1.2 Challenges from decoupling the weight decay term in Adam
Another challenge in solving (UOP) by Adam is related to the incorporation of the weight decay term.
The conventional approach is to directly minimize gby Adam, as is implemented in various computational
frameworks. That is, the weight decay is coupled with the stochastic subgradients of f, in the sense that f
and the weight decay termσ
2∥x∥2are treated as an integrated function to be minimized. As demonstrated
in (Loshchilov & Hutter, 2017), Adam with coupled weight decay usually exhibits worse generalization
performance than SGD. To address this issue, Loshchilov & Hutter (2017) suggests a novel method named
AdamW, which decouples the weight decay term from the stochastic subgradients of f. The update schemes
of AdamW can be summarized by the following framework:


gk=dk+ξk+1,
mk+1= (1−θk)mk+θkgk,
vk+1= (1−βk)vk+βk(gk)2,
xk+1=xk−ηk(√vk+1+ε)−1⊙mk+1−ηkσxk.(AdamW)
Here, Loshchilov&Hutter(2017)demonstratesthattheweightdecayisdecoupledfromthemomentumterms
{mk}and the estimators {vk}, in the sense that the update schemes for {mk}and{vk}are independent of
the weight decay parameter σ. Moreover, unlike Adam in (1), the weight decay term σxkis not scaled by
the preconditioner (√vk+1+ε)−1in AdamW.
The AdamW, recognized for its superior generalization performance over Adam with coupled weight decay
(i.e., themethodin(1)), hasbecomeapopularchoiceinthetrainingofneuralnetworks(Loshchilov&Hutter,
2017), particularly in tasks such as image classification and language modeling. However, compared with
Adam, the convergence properties of AdamW remain relatively unexplored. As suggested in (Loshchilov &
Hutter, 2017; Zhou et al., 2024), AdamW iterates by taking a descent step towards a dynamically adjusted
surrogate function f(x) +σ
2/angbracketleftbig
x,(√vk+1+ε)⊙x/angbracketrightbig
in thek-th iteration, thereby lacking a clearly defined
3Under review as submission to TMLR
objective function to minimize. As a result, only the paper by (Zhou et al., 2024) has established the
convergence properties of AdamW for continuously differentiable f. In (Zhou et al., 2024), the stationarity
of AdamW is measured by/vextenddouble/vextenddouble∇f(x) +σ(√vk+1+ε)⊙x/vextenddouble/vextenddouble. As the estimators {vk}evolves over iterations and
may not converge, the proposed stationarity measure is at best an approximation of the standard notion of
stationarity. More importantly, the analysis in (Zhou et al., 2024) relies on the differentiability of f, and
cannot be extended to analyze the convergence of AdamW for nonsmooth cases. Consequently, the results
presented in (Zhou et al., 2024) do not sufficiently explain the convergence of AdamW in real-world training
tasks, where the neural networks are typically nonsmooth.
Given that Adam-family methods with coupled weight decay usually perform less effectively than AdamW,
and considering that AdamW lacks convergence guarantees in training nonsmooth neural networks, we are
driven to raise the following question:
Can we design Adam-family methods with decoupled weight decay that have convergence
guarantees with non-diminishing stepsizes, in the context of training nonsmooth neural
networks?
1.2 Contributions
The contributions of our paper are summarized as follows.
•A novel framework with decoupled weight decay
In this paper, motivated by AdamW, we propose a novel framework for Adam-family methods with
decoupled weight decay (AFMDW),


gk=dk+ξk+1,
mk+1= (1−θk)mk+θkgk,
Choose the estimator vk+1,
xk+1=xk−ηkH(vk+1)⊙(mk+1+σxk).(AFMDW)
Here,dkis an approximated evaluation of Df(xk), whileξk+1is the corresponding evaluation noise
ofdk. Therefore, gkrepresents the stochastic subgradients of fatxk. Moreover, the sequences
{ηk}and{θk}are stepsizes for the variables {xk}and parameters for the momentum terms {mk},
respectively. Furthermore, H:Rn→Rnis the mapping that determines how we construct the
preconditioner based on vk+1. As the framework (AFMDW) is designed to minimize (UOP), both
the momentum term mk+1and the weight decay term σxkare scaled by H(vk+1)in (AFMDW),
distinguishing it from AdamW.
•Convergence analysis
We establish the global convergence of the framework (AFMDW) under mild conditions with non-
diminishing stepsizes. When the noises {ξk}correspond to random reshuffling (RR), and the es-
timator{vk}is updated as in (AdamW) with non-diminishing {ηk}and{βk}, we prove that with
sufficiently small but non-diminishing {θk}, the sequence{xk}could stabilize within a neighborhood
of the critical points of (UOP). In addition, when we further assume {θk}→ 0, we prove that the
sequence{xk}converges to the critical points of (UOP) almost surely. Moreover, by employing
single-timescale scheme in (AFMDW), we prove that with sufficiently small {ηk}, the sequence{xk}
stabilizes within a neighborhood of the critical points of (UOP).
Furthermore, we extend the convergence analysis of the framework (AFMDW) with diminishing
stepsizes and with replacement sampling (WRS), and establish the almost sure convergence to
critical points of (UOP). Table 1 presents a brief comparison of our results with existing works
on the convergence of stochastic subgradient methods.
•Advantages in incorporating weight decay into Adam
4Under review as submission to TMLR
Table 1: A brief comparison of our results and existing works on the convergence of stochastic subgradient
methods.
Result Sampling method Update scheme Stepsizes Convergence Guaranteed stability
Theorem 3.10 & 3.22 WRS Adam Diminishing Almost sure Y
Theorem 3.13 RR Adam Constant Almost sure Y
(Josz et al., 2023) RR SGD Constant Almost sure Y
(Bianchi et al., 2022) WRS SGD Constant High probability Y
(Xiao et al., 2023a) WRS Adam Diminishing Almost sure N
We demonstrate that the framework (AFMDW) encompasses (see Table 2 for details) a wide range
of Adam-family methods, including SGD, Adam, AMSGrad, AdaBelief, AdaBound, Yogi. Therefore,
our analysis provides convergence guarantees for these Adam-family methods in training nonsmooth
neural networks.
Moreover, compared with the non-convergence analysis of Adam in (Reddi et al., 2018; Zhang et al.,
2022), our analysis illustrates that the incorporation of a weight decay term grants more flexibility on
the choices of the parameters {θk}and{βk}for the framework (AFMDW). These results illustrate
the great theoretical advantages of a weight decay term in the framework (AFMDW).
•Numerical experiments
Based on our proposed framework (AFMDW), we develop a novel method named Adam with Decou-
pled Weight Decay (AdamD) and establish its convergence guarantees in training nonsmooth neural
networks. We conduct numerical experiments in both image classification and language modeling
taskstoassesstheperformanceofourproposedAdamD.Theresultsshowthatinimageclassification
tasks, AdamD outperforms Adam and performs comparably to AdamW in both generalization and
efficiency. In language modeling tasks, it demonstrates similar effectiveness to Adam and outper-
forms AdamW, highlighting its versatility and effectiveness across different tasks. Additionally, our
numerical experiments illustrate that the sequence {∥yk−xk∥}tends to 0, whereykis an auxiliary
variable that approximates the dynamics of SGD. This validates our theoretical analysis that the
proposed AdamD asymptotically approximates the SGD method. These results further demonstrate
the promising potential of our proposed framework (AFMDW).
1.3 Organization
The rest of this paper is organized as follows. In Section 2, we define the notations used throughout the
paper and present some basic concepts related to nonsmooth analysis and stochastic approximation. Section
3 presents the convergence properties of our proposed framework (AFMDW) with non-diminishing stepsizes
{ηk}. Moreover, we extend these convergence properties to the framework (AFMDW) with single-timescale
stepsizes. As an application of our theoretical analysis, we propose a new Adam-family method named Adam
with Decoupled Weight Decay (AdamD) and establish its convergence properties in Section 4. In Section
5, we present the results of our numerical experiments that investigate the performance of the proposed
AdamD in training nonsmooth neural networks. Some further discussions on AdamD are also presented in
Section 5. Finally, we conclude the paper in the last section.
2 Preliminaries
2.1 Notations
For any vectors xandyinRnandδ∈R, we denote x⊙y,xδ,x/y,|x|,x+δ,√xas the vectors whose i-th
entries are given by xiyi,xδ
i,xi/yi,|xi|,xi+δ, and√xi, respectively. We denote Rn
+:={x∈Rn:xi≥
0for any 1≤i≤n}. Moreover, for any subsets X,Y⊂Rn, we denoteX⊙Y :={x⊙y:x∈X,y∈Y},
|X|:={|x|:x∈X}and∥X∥ = sup{∥w∥:w∈X}. In addition, for any z∈Rn, we denote z+X:={z}+X
andz⊙X :={z}⊙X.
5Under review as submission to TMLR
Furthermore, for any positive sequence {θk}, we defineλ0:= 0,λi:=/summationtexti−1
k=0θkfori≥1, and Λ(t) := sup{k≥
0 :t≥λk}. More explicitly, Λ(t) =pifλp≤t<λp+1for anyp≥0. In particular, Λ(λp) =p.
2.2 Probability theory
In this subsection, we present some essential concepts from probability theory, which are necessary for the
proofs in this paper.
Definition 2.1. Let(Ω,F,P)be a probability space. We say that {Fk}k∈Nis a filtration if {Fk}is a
collection of σ-algebras that satisfies F0⊆F 1⊆···⊆F∞⊆F.
Definition 2.2. We say that a stochastic series {ξk}is a martingale difference sequence if the following
conditions hold,
•The sequence of random vectors {ξk}is adapted to the filtration {Fk},
•For eachk≥0, almost surely, it holds that E[|ξk|]<∞andE[ξk|Fk−1] = 0.
Moreover, we say that a martingale difference sequence {ξk}is uniformly bounded if there exists a constant
Mξ>0such that supk≥0∥ξk∥≤Mξ.
In the following, we present the results in (Benaïm, 2006, Proposition 4.4), which controls the weighted
summation of any uniformly bounded martingale difference sequence, and plays a crucial role in establishing
the convergence properties for our proposed framework (AFMDW).
Proposition 2.3 (Proposition 4.4 in (Benaïm, 2006)) .Suppose{θk}is a diminishing positive sequence of
real numbers that satisfy limk→∞θklog(k) = 0. Then for any T >0, and any uniformly bounded martingale
difference sequence {ξk}, almost surely it holds that
lim
s→∞sup
s≤i≤Λ(λs+T)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublei/summationdisplay
k=sθkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble= 0. (2)
2.3 Nonsmooth analysis
In this subsection, we introduce some basic concepts in nonsmooth optimization, especially those related to
the concept of conservative field (Bolte & Pauwels, 2021). Interested readers could refer to (Bolte & Pauwels,
2021; Davis et al., 2020) for more details.
We begin our introduction on the concept of Clarke subdifferential (Clarke, 1990), which plays an essential
role in characterizing stationarity and the development of algorithms for nonsmooth optimization problems.
Definition 2.4 ((Clarke, 1990)) .For any given locally Lipschitz continuous function f:Rn→Rand any
x∈Rn, the Clarke subdifferential ∂fis defined as
∂f(x) := conv ({d∈Rn:xk→x,∇f(xk)→d}). (3)
Next we present a brief introduction on the concept of conservative field, which can be applied to characterize
how nonsmooth neural networks are differentiated by automatic differentiation (AD) algorithms.
Definition 2.5. A set-valued mapping D:Rn⇒Rsis a mapping from Rnto a collection of subsets of Rs.
Dis said to have a closed graph, or is graph-closed if the graph of D, defined by
graph(D) :={(w,z)∈Rn×Rs:w∈Rn,z∈D(w)},
is a closed subset of Rn×Rs.
Definition 2.6. A set-valued mapping D:Rn⇒Rsis said to be locally bounded if, for any x∈Rn, there
is a neighborhood Vxofxsuch that∪y∈VxD(y)is bounded.
Next, we present the definition of conservative field and its corresponding potential function.
6Under review as submission to TMLR
Definition 2.7. An absolutely continuous curve is a continuous mapping γ:R+→Rnwhose derivative
γ′exists almost everywhere in R+andγ(t)−γ(0)equals the Lebesgue integral of γ′between 0andtfor all
t∈R+, i.e.,
γ(t) =γ(0) +/integraldisplayt
0γ′(u)du,for allt∈R+.
Definition 2.8 (Definition 1 in (Bolte & Pauwels, 2021)) .LetDbe a graph-closed set-valued mapping from
Rnto subsets of Rn. We callDa conservative field whenever it has nonempty compact values, and for any
absolutely continuous curve γ: [0,1]→Rnsatisfyingγ(0) =γ(1), it holds that
/integraldisplay1
0max
v∈D(γ(t))⟨γ′(t),v⟩dt= 0. (4)
Here the integral is understood in the Lebesgue sense.
It is important to note that any conservative field is locally bounded (Bolte & Pauwels, 2021, Remark 3).
We now introduce the definition of potential function corresponding to a conservative field.
Definition 2.9 (Definition 2 in (Bolte & Pauwels, 2021)) .LetDbe a conservative field in Rn. Then with
any givenx0∈Rn, we can define a function f:Rn→Rthrough the path integral
f(x) =f(x0) +/integraldisplay1
0max
d∈D(γ(t))⟨γ′(t),d⟩dt=f(x0) +/integraldisplay1
0min
d∈D(γ(t))⟨γ′(t),d⟩dt (5)
for any absolutely continuous curve γthat satisfies γ(0) =x0andγ(1) =x. The function fis called a
potential function for D. We also say that Dadmitsfas its potential function, or that Dis a conservative
field forf.
The following two lemmas characterize the relationship between conservative field and Clarke subdifferential.
Lemma 2.10 (Theorem 1 in (Bolte & Pauwels, 2021)) .Letf:Rn→Rbe a potential function that admits
Dfas its conservative field. Then Df(x) ={∇f(x)}almost everywhere.
Lemma 2.11 (Corollary 1 in (Bolte & Pauwels, 2021)) .Letf:Rn→Rbe a potential function that admits
Dfas its conservative field. Then ∂fis a conservative field for f, and for all x∈Rn, it holds that
∂f(x)⊆conv (Df(x)). (6)
From the above two lemmas, we can conclude that the concept of conservative field can be regarded as a gen-
eralization of Clarke subdifferential. Therefore, conservative field can be applied to characterize stationarity,
as illustrated in the following definition.
Definition 2.12. Letf:Rn→Rbe a potential function that admits Dfas its conservative field. We say
thatxis aDf-stationary point of fif0∈conv (Df(x)). In particular, we say xis a∂f-stationary point of
fif0∈∂f(x).
As demonstrated in (Bolte & Pauwels, 2021), a conservative field can be regarded as a generalization of
Clarke subdifferential. Therefore, a function is differentiable in the sense of conservative field if it admits a
conservative field for which Definition 2.9 holds true. Such functions are called path-differentiable (Bolte &
Pauwels, 2021, Definition 3), which is given below.
Definition 2.13. Given a locally Lipschitz continuous function f:Rn→R, we say that fis path-
differentiable if fis the potential function of a conservative field on Rn.
It is worth mentioning that the class of path-differentiable functions is general enough to cover the objectives
in a wide range of real-world problems. As shown in (Davis et al., 2020, Section 5.1), any Clarke regular func-
tion is path-differentiable. Beyond Clarke regular functions, another important class of path-differentiable
functions are functions whose graphs are definable in an o-minimal structure (Davis et al., 2020, Definition
5.10). Usually, the o-minimal structure is fixed, and we simply call these functions definable. As demon-
strated in (Van den Dries & Miller, 1996), any definable function admits a Whitney Csstratification (Davis
7Under review as submission to TMLR
et al., 2020, Definition 5.6) for any s≥1, hence is path-differentiable (Bolte & Pauwels, 2021; Davis et al.,
2020). To characterize the class of definable functions, (Davis et al., 2020; Bolte & Pauwels, 2021; Bolte
et al., 2022b) shows that numerous common activation functions and dissimilarity functions are all definable.
Furthermore, since definability is preserved under finite summation and composition (Bolte & Pauwels, 2021;
Davis et al., 2020), for any neural network built from definable blocks, its loss function is definable and thus
belongs to the class of path-differentiable functions.
Moreover, (Bolte et al., 2007) shows that any Clarke subdifferential of definable functions is definable.
Consequently, for any neural network constructed from definable blocks, the conservative field corresponding
to the AD algorithms can be chosen as a definable set-valued mapping formulated by compositing the Clarke
subdifferentials of all its building blocks (Bolte & Pauwels, 2021). The following proposition shows that the
definability of fandDfleads to the nonsmooth Morse–Sard property (Bolte et al., 2007) for (UOP).
Proposition 2.14 (Theorem 5 in (Bolte & Pauwels, 2021)) .Letfbe a potential function that admits Dfas
its conservative field. Suppose both fandDfare definable over Rn, then the set{f(x) : 0∈conv (Df(x))}
is finite.
2.4 Differential inclusion and stochastic subgradient methods
Inthissubsection,weintroducesomefundamentalconceptsrelatedtothestochasticapproximationtechnique
that are essential for the proofs presented in this paper. The concepts discussed in this subsection are mainly
from (Benaïm et al., 2005). Interested readers could refer to (Benaïm, 2006; Benaïm et al., 2005; Borkar,
2009; Davis et al., 2020) for more details on the stochastic approximation technique.
Definition 2.15. For any locally bounded set-valued mapping D:Rn⇒Rnthat is nonempty compact
convex valued and has closed graph, we say that an absolutely continuous path x(t)inRnis a solution for
the differential inclusion
dx
dt∈D(x), (7)
with initial point x0ifx(0) =x0, and ˙x(t)∈D(x(t))holds for almost every t≥0.
Definition 2.16. For any given set-valued mapping D:Rn⇒Rnand any constant δ≥0, the set-valued
mappingDδis defined as
Dδ(x) :={w∈Rn:∃z∈Bδ(x),dist(w,D(z))≤δ}. (8)
Definition 2.17. LetB⊂Rnbe a closed set. A continuous function ϕ:Rn→Ris referred to as a Lyapunov
function for the differential inclusion (7)with the stable set B, if it satisfies the following conditions:
1. For any γthat is a solution for (7)withγ(0)∈B, it holds that ϕ(γ(t))≤ϕ(γ(0))for anyt≥0.
2. For any γthat is a solution for (7)withγ(0)/∈B, it holds that ϕ(γ(t))<ϕ(γ(0))for anyt>0.
The following proposition illustrates that fis a Lyapunov function for the differential inclusiondx
dt∈−Df(x).
The proof of the following proposition directly follows from (Bolte & Pauwels, 2021), hence is omitted for
simplicity.
Proposition 2.18. Supposefis a path-differentiable function fthat admitsDfas its conservative field.
Thenfis a Lyapunov function for the differential inclusiondx
dt∈−Df(x)with the stable set {x∈Rn: 0∈
Df(x)}.
Definition 2.19. We say that an absolutely continuous function γis a perturbed solution to (7)if there
exists a locally integrable function u:R+→Rn, such that
•For anyT >0, it holds that lim
t→∞sup
0≤l≤T/vextenddouble/vextenddouble/vextenddouble/integraltextt+l
tu(s) ds/vextenddouble/vextenddouble/vextenddouble= 0.
•There exists δ:R+→Rsuch that lim
t→∞δ(t) = 0and ˙γ(t)−u(t)∈Dδ(t)(γ(t)).
8Under review as submission to TMLR
Now consider the sequence {xk}generated by the following updating scheme,
xk+1=xk+ηk(dk+ξk), (9)
where{ηk}is a diminishing positive sequence of real numbers. We define the (continuous-time) interpolated
process of{xk}generated by (9) as follows.
Definition 2.20. The (continuous-time) interpolated process of {xk}generated by (9)is the mapping w:
R+→Rnsuch that
w(λi+s) :=xi+s
ηi(xi+1−xi), s∈[0,ηi). (10)
Hereλ0:= 0, andλi:=/summationtexti−1
k=0ηkfori≥1.
The following lemma is an extension of (Benaïm et al., 2005, Proposition 1.3), which allows for inexact
evaluations of the set-valued mapping D. It shows that the interpolated process of {xk}from (9) is a
perturbed solution of the differential inclusion (7).
Lemma 2.21. LetD:Rn⇒Rnbe a locally bounded set-valued mapping that is nonempty compact convex
valued with closed graph. Suppose the following conditions hold in (9):
1. For any T >0, it holds that lim
s→∞sup
s≤i≤Λ(λs+T)/vextenddouble/vextenddouble/vextenddouble/summationtexti
k=sηkξk/vextenddouble/vextenddouble/vextenddouble= 0.
2. There exist a positive sequence {δk}such that limk→∞δk= 0anddk∈Dδk(xk).
3.supk≥0∥xk∥<∞,supk≥0∥dk∥<∞.
Then the interpolated process of {xk}defined in (10)is a perturbed solution for (7).
The following theorem summarizes the results in (Benaïm et al., 2005), which illustrates the convergence of
{xk}generated by (9). It is worth mentioning that Theorem 2.22 is directly derived from putting (Benaïm
et al., 2005, Proposition 3.27) and (Benaïm et al., 2005, Theorem 3.6) together. Therefore, we omit the
proof of Theorem 2.22 for simplicity.
Theorem 2.22. LetD:Rn⇒Rnbe a locally bounded set-valued mapping that is nonempty compact convex
valued with closed graph. For any sequence {xk}, suppose there exist a continuous function ϕ:Rn→Rand
a closed subsetBofRnsuch that
1.ϕis bounded from below, and the set {ϕ(x) :x∈B}has empty interior in R.
2.ϕis a Lyapunov function for the differential inclusion (7)that admitsBas its stable set.
3. The interpolated process of {xk}is a perturbed solution of (7).
Then any cluster point of {xk}lies inB, and the sequence {ϕ(xk)}converges.
Similar results under slightly different conditions can be found in (Borkar, 2009; Davis et al., 2020; Duchi &
Ruan, 2018). Moreover, towards the convergence properties of (9) with potentially non-diminishing stepsizes,
several recentworks (Bianchiet al.,2022; Joszet al.,2023; Xiaoet al.,2023b) provideconvergence guarantees
under more relaxed conditions. Interested readers could refer to those works for details.
3 Global Convergence
In this section, we prove the convergence properties of the framework (AFMDW) even though the sequence
of stepsizes{ηk}is assumed to be non-diminishing. The proofs are provided in the Appendix.
9Under review as submission to TMLR
3.1 Basic assumptions
We first make the following assumptions on the quadratically regularized optimization problem (UOP).
Assumption 3.1. 1.fis a path-differentiable function that admits a convex valued set-valued mapping
Dfas its conservative field.
2. There exists a constant L > 0andν∈[0,1), such that for any x∈Rn, it holds that∥Df(x)∥≤
L(1 +∥x∥ν).
3. The set{g(x) : 0∈Df(x) +σx}has empty interior in R.
As discussed in Section 2.3, the class of path-differentiable functions covers a wide variety of objective
functions in real-world applications. In particular, for a wide range of common neural networks, their loss
functions are definable and thus path-differentiable, as demonstrated in (Bolte & Pauwels, 2021; Castera
et al., 2021; Davis et al., 2020). As a result, Assumption 3.1(1) is mild in practice. Moreover, Assumption
3.1(2) imposes a growth condition on the conservative field. Furthermore, Assumption 3.1(3) is referred to
as the nonsmooth weak Sard’s property, which is commonly observed in various existing works (Bianchi &
Rios-Zertuche, 2021; Bolte et al., 2022a; Bolte & Pauwels, 2021; Castera et al., 2021; Davis et al., 2020; Le,
2023) and is demonstrated to be mild in (Bolte & Pauwels, 2021; Castera et al., 2021; Davis et al., 2020).
Notice that the chain rule holds for conservative fields (Bolte & Pauwels, 2021, Lemma 5), and it is easy to
verify that gis a path-differentiable function that admits Df(x) +σxas its conservative field. Therefore, in
the rest of the paper, we fix the conservative field Dg:Rn⇒Rnfor the objective function gin (UOP) as:
Dg(x) :=Df(x) +σx. (11)
In the following lemma, we present some basic properties of Dg. The proof of Lemma 3.2 follows straight-
forwardly from (Bolte & Pauwels, 2021, Corollary 4), hence it is omitted for simplicity.
Lemma 3.2. Suppose Assumption 3.1 holds. Then gis a path-differentiable function, and Dgis a convex-
valued graph-closed conservative field that admits gas its potential function.
We also need the following assumptions on the framework (AFMDW) to establish its convergence properties.
Assumption 3.3. 1. There exist constants εvandMvwith 0<εv<Mv, such thatεv≤H(vk)≤Mv
holds for any k≥0.
2. There exists a non-negative sequence {δk}such that limk→∞δk= 0anddk∈Dδk
f(xk).
3. The sequence of noises {ξk}is a uniformly bounded martingale difference sequence. That is, there
exists a constant Mξsuch that almost surely, supk≥0∥ξk∥≤Mξ, andE[ξk+1|Fk] = 0for anyk≥0.
Here, we make some comments on the assumptions in Assumption 3.3. Assumption 3.3(1) assumes the
uniformboundednessof {H(vk)}, whichissatisfiedinvariousexistingworksasshowninTable2. Inaddition,
later in Section 3.2, we provide some sufficient conditions that guarantee the uniform boundedness of {xk}.
Assumption 3.3(2) characterizes how dkapproximatesDf(xk). Furthermore, Assumption 3.3(3) assumes
that the evaluation noises {ξk}is a uniformly bounded martingale difference sequence. As demonstrated
in (Bolte & Pauwels, 2021; Castera et al., 2021), Assumption 3.3(3) holds when ffollows a finite-sum
formulation, hence it is mild in practical applications of (UOP).
3.2 Uniform boundedness of {xk}and{vk}
In this subsection, we present some sufficient and easy-to-verify conditions that guarantee the validity of
uniform boundedness of {xk}. The following proposition illustrates that under some mild global growth
conditions for fand the uniform boundedness of {H(vk)}, the sequence{xk}is uniformly bounded.
Proposition 3.4. Suppose Assumption 3.1 and Assumption 3.3 hold, and supk≥0ηk≤1
σεv. Then for any
initial point (x0,m0,v0), there exists a constant Q> 0such that supk≥0∥xk∥≤Q.
10Under review as submission to TMLR
Next, we discuss the uniform boundedness of the sequence {H(vk)}. Apart from Assumption 3.1 and
Assumption 3.3, we make the assumption on the global Lipschitz continuity of f, in the sense that
sup
x∈Rn∥Df(x)∥≤Mf,for some constant Mf>0. (12)
Such an assumption is standard in various existing works. Table 2 lists some Adam-family methods, where
the sequence{H(vk)}remains uniformly bounded under Assumption 3.1, Assumption 3.3, and equation
(12).
Table 2: Different update schemes for {vk}in the framework (AFMDW) under Assumption 3.1, Assumption
3.3, and (12). Here ε,cl,cu>0are hyper-parameters for these Adam-family methods.
Method Update scheme for {vk} Formulation for H(v) Choice of (εv,Mv)
SGDW (Loshchilov & Hutter, 2017) vk+1= (1−β1)vk+β1g2
k1 (1 ,1)
Adam (Kingma & Ba, 2015) vk+1= (1−β1)vk+β1g2
k(√v+ε)−1(1
(Mf+Mξ)+ε,1
ε)
AMSGrad (Reddi et al., 2018) vk+1= max{vk,(1−β1)vk+β1g2
k} (√v+ε)−1(1
(Mf+Mξ)+ε,1
ε)
Adamax (Kingma & Ba, 2015) vk+1= max{β1vk,|gk|+ε} (v)−1(1
(Mf+Mξ)2+ε,1
ε)
RAdam (Liu et al., 2019) vk+1= (1−β1)vk+β1g2
k(√v+ε)−1(1
(Mf+Mξ)+ε,1
ε)
AdaBelief (Zhuang et al., 2020) vk+1= (1−β1)vk+β1(gk−mk+1)2(√v+ε)−1(1
2(Mf+Mξ)+ε,1
ε)
AdaBound (Luo et al., 2019) vk+1= (1−β1)vk+β1g2
kmin{cl,max{cu,v−1
2}} (cl,cu)
Yogi (Zaheer et al., 2018) vk+1=vk−β1sign(vk−g2
k)⊙g2
k(√v+ε)−1(1
(Mf+Mξ)+ε,1
ε)
3.3 Convergence with non-diminishing stepsizes {ηk}
Assumption 3.5. The sequences of stepsizes {ηk}and momentum parameters {θk}satisfy
ηmax:= sup
k≥0ηk<min/braceleftbigg2
σMv,1
σεv/bracerightbigg
, η min:= inf
k≥0ηk>0,and∞/summationdisplay
k=0θk=∞. (13)
We begin our theoretical analysis with Lemma 3.6, which shows that the sequence {mk}and{gk}are
uniformly bounded. Lemma 3.6 directly follows from the uniform boundedness of {xk}in Proposition 3.4
and{ξk}in Assumption 3.3(3) and the fact that Dfis locally bounded, hence we omit its proof for simplicity.
Lemma 3.6. Suppose Assumption 3.1 and Assumption 3.3 hold. Then there exists a constant Md>0such
thatsupk≥0{∥gk∥+∥mk∥}≤Mdholds almost surely.
Lemma 3.7 illustrates that ∥σxk+mk∥→0as the momentum parameter {θk}diminishes.
Lemma 3.7. Suppose Assumption 3.1, Assumption 3.3, and Assumption 3.5 hold. Then for any {θk}
satisfying limk→+∞θk= 0, we have that limk→+∞∥σxk+mk∥= 0holds almost surely.
From the proof of Lemma 3.7, it follows that the asymptotic behavior of ∥σxk+mk∥can be controlled
by{θk}ask→∞. Specifically, from equation (32), we have limk→∞ˆδk+1= 0when limk→∞θk= 0.
Consequently, for any ε>0, there exists a threshold θmax>0such that, if lim supk→∞θk≤θmax, it follows
that lim supk→∞∥σxk+mk∥≤ε. Moreover, the convergence of ∥σxk+mk∥is faster as{θk}decreases more
rapidly.
Based on the Lemma 3.7, let the auxiliary sequence {yk}be defined as
yk:=−1
σmk,for anyk≥0. (14)
Then we can conclude that limk→∞∥yk−xk∥= 0. More importantly, substituting (14) into the update
scheme for{mk}in (AFMDW), we arrive at the following relation
yk+1=yk−θk
σ(dk+σyk+ξk+1). (15)
In the following lemma, we prove that dk+σykcan be regarded as an approximated evaluation for Dg(yk).
11Under review as submission to TMLR
Lemma 3.8. Suppose Assumption 3.1, Assumption 3.3, and Assumption 3.5 hold. Then let δ⋆
k:= (1 +
σ)δk+ˆδk, it holds that
dk+σyk∈Dδ⋆
kg(yk), (16)
where ˆδkis defined in equation (32).
We can conclude from Lemma 3.8 that the auxiliary sequence {yk}follows the differential inclusion,
yk+1∈yk−θk
σ/parenleftig
Dδ⋆
kg(yk) +ξk+1/parenrightig
. (17)
This fact illustrates that the sequence {yk}can be viewed as a sequence generated by the SGD method for
minimizing g. Therefore, in the following proposition, we prove that the interpolated process of the sequence
{yk}is a perturbed solution of the following differential inclusion:
dy
dt∈−Dg(y). (18)
We first present the results for the case where the noise is induced by with-replacement sampling.
Proposition 3.9. Suppose Assumption 3.1, Assumption 3.3 and Assumption 3.5 hold, and
limk→+∞θklog(k) = 0. Then the interpolated process of the sequence {yk}is a perturbed solution for
the differential inclusion (18).
In the following theorem, we prove the convergence properties of the framework (AFMDW).
Theorem 3.10. Suppose Assumption 3.1, Assumption 3.3 and Assumption 3.5 hold, and
limk→+∞θklog(k) = 0. Then almost surely, any cluster point of the sequence {xk}is aDg-stationary point
ofg, and{g(xk)}converges.
In the rest of this subsection, we aim to establish the global stability of the framework (AFMDW), where
the noises{ξk}correspond to random reshuffling. Therefore, we make the following assumptions on the
momentum parameters {θk}and noises{ξk}.
Assumption 3.11. There exists an integer N > 0such that
1. For any nonnegative integers i,j <N, it holds that θkN+i=θkN+jfor anyk∈N+.
2. For any j∈N+, almost surely, it holds that/summationtext(j+1)N−1
i=jNξk+1= 0.
Lemma 3.12. Suppose Assumption 3.3(3) and Assumption 3.11 hold for the sequence of noises {ξk}and
momentum parameters {θk}. Then for any ε >0andT > 0, there exists θε>0such that for any {θk}
satisfying lim supk→+∞θk≤θε, almost surely, it holds that
lim sup
s→+∞sup
s≤i≤Λ(λ(s)+T)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublei/summationdisplay
k=sθkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤ε. (19)
Then we have the following theorem illustrating the global stability of the framework (AFMDW) with
non-diminishing{θk}.
Theorem 3.13. Suppose Assumption 3.1, Assumption 3.3, Assumption 3.5, and Assumption 3.11 hold.
Then for any ε>0, there exists θmax>0such that for any {θk}satisfying lim supk→+∞θk≤θmax, almost
surely, it holds that
lim sup
k→+∞dist (xk,{x∈Rn: 0∈Dg(x)})≤ε. (20)
Theorem 3.13 implies that as long as the momentum parameters θkare sufficiently small, the sequence xk
maintains stability, regardless of how the estimator vkis updated. When the estimator vkis updated in the
manner of Adam, i.e., as a second-moment estimator, AdamD consistently works as long as the momentum
12Under review as submission to TMLR
parameterθkiskeptsmall, irrespectiveofthechoiceofparametersforupdatingthesecond-momentestimator
vk. Therefore, our proposed AdamD offers greater flexibility in selecting momentum parameters and those
associated with updating vk. For a numerical illustration, refer to Figure 7 in Section 5.1.3.
Moreover, from the results in Theorem 3.13, we can prove that with diminishing {θk}, the sequence{xk}
can asymptotically finds the stationary points of (UOP). The result is presented in the following corollary
and is omitted for simplicity.
Corollary 3.14. Suppose Assumption 3.1, Assumption 3.3, Assumption 3.5, and Assumption 3.11 hold, and
limk→+∞θk= 0. Then almost surely, any cluster point of the sequence {xk}is aDg-stationary point of g,
and{g(xk)}converges.
3.4 Convergence with a single-timescale in {ηk}and{θk}
In this subsection, we investigate the convergence of the framework (AFMDW) when the sequences of
stepsizes{ηk}and momentum parameters {θk}are single-timescale in the sense that they diminish at the
same rate.
The convergence properties presented in Section 3 suggest that the sequence {yk}asymptotically approx-
imates the trajectory of the differential inclusion (18). One may conjecture that this phenomenon is at-
tributable to the involvement of non-diminishing stepsizes {ηk}in the framework (AFMDW).
However, in this section, we aim to show that when single-timescale stepsizes and momentum parameters are
employed in the framework (AFMDW), the interpolated process of {yk}is still a perturbed solution of the
differential inclusion (18). These theoretical results suggest that it is the decoupled weight decay that leads
to the asymptotic approximation of the differential inclusion (18) in the framework (AFMDW), regardless
of the timescale of the employed sequences {ηk}and{θk}.
The proof techniques in this section are motivated by the techniques in (Xiao et al., 2023a, Section 3). To
prove the convergence of (AFMDW) with single-timescale sequences {ηk}and{θk}, we make the following
assumptions.
Assumption 3.15. 1. There exists a locally bounded mapping W:Rn×Rn→Rn
+and a prefixed
constantτ2>0such that the sequence of estimators {vk}follows the update scheme vk+1=vk−
τ2ηk(vk−W(gk,mk+1)).
2. The mapping H:Rn
+→Rn
+is fixed asH(v) = (max{v,0}+ε)−1
2for a prefixed constant ε>0.
3. The sequences {ηk}and{θk}are positive and satisfies
∞/summationdisplay
k=0ηk=∞,∞/summationdisplay
k=0θk=∞,lim
k→∞θk
ηk=τ1, (21)
for a prefixed positive constant τ1∈[τ2
4,∞).
4. There exists a non-negative sequence {δk}such that limk→∞δk= 0anddk∈Dδk
f(xk).
5. The sequence of noises {ξk}is a uniformly bounded martingale difference sequence.
Here we make some comments on Assumption 3.15. Assumption 3.15(4)(5) are identical to Assumption
3.3(2)(3), respectively. Assumption 3.15(1) characterizes how the estimators {vk}are updated. As discussed
in (Barakat & Bianchi, 2021; Xiao et al., 2023a), Assumption 3.15(1) is general enough to include the update
schemes for Adam, AdaBelief, AMSGrad, and Yogi. Moreover, Assumption 3.15(2) fixes the formulation of
the mapping H, and Assumption 3.15(3) assumes that the stepsizes {ηk}and momentum parameters {θk}
in the framework (AFMDW) are single-timescale.
We begin our analysis with the following lemma, which shows the uniform boundedness of {mk}and{gk}
directlyfromtheuniformboundednessof {xk}inProposition3.4. Asaresult, weomititsproofforsimplicity.
Lemma 3.16. Suppose Assumption 3.1 and Assumption 3.15 hold. Then there exists a constant Md>0
such that supk≥0∥gk∥+∥mk∥≤Mdholds almost surely.
13Under review as submission to TMLR
Next we present the following auxiliary lemma, which follows directly from the uniform boundedness of {xk},
{mk}and{gk}in Lemma 3.16, together with the local boundedness of the mappings DfandW.
Lemma 3.17. Suppose Assumption 3.1 and Assumption 3.15 hold. Then there exists a constant MW>0
such that supk≥0∥W(gk,mk+1)∥≤MWholds almost surely.
LetP+(v) := max{v,0}, andU(x,m) :={d∈Rn
+:∥d∥ ≤MW}. Consider the set-valued mapping
G:Rn×Rn×Rn⇒Rn×Rn×Rndefined by
G(x,m,v ) :=
(P+(v) +ε)−1
2⊙(m+σx)
τ1m−τ1Df(x)
τ2v−τ2U(x,m)
, (22)
and the following differential inclusion:
/parenleftbiggdx
dt,dm
dt,dv
dt/parenrightbigg
∈−G (x,m,v ). (23)
In the following lemma, we prove that the set-valued mapping Gis capable of characterizing the update
direction of{(xk,mk,vk)}in the framework (AFMDW). The proof straightforwardly follows from Lemma
3.17, hence we omit it for simplicity.
Lemma 3.18. Suppose Assumption 3.1 and Assumption 3.15 hold. Then the inclusion
vk+1∈vk−τ2ηk(vk−U(xk,mk)) (24)
holds for any k≥0. Furthermore, supk≥0∥vk+1∥<∞holds almost surely.
Let∂P+be the generalized Jacobian of the mapping P+, and define the function h:Rn×Rn×Rn→Ras
h(x,m,v ) =f(x) +σ
2∥x∥2+1
2τ1/angbracketleftig
m+σx,(P+(v) +ε)−1
2⊙(m+σx)/angbracketrightig
. (25)
The next Lemma 3.19 presents the formulation of the conservative field of h.
Lemma 3.19. Suppose Assumption 3.1 and Assumption 3.15 hold. Then his a potential function that
admits
Dh(x,m,v ) =
Df(x) +σx+σ
τ1(P+(v) +ε)−1
2⊙(m+σx)
1
τ1(P+(v) +ε)−1
2⊙(m+σx)
−1
4τ1(m+σx)2⊙(P+(v) +ε)−3
2⊙∂P+(v)
 (26)
as its conservative field.
Proposition 3.20. Suppose Assumption 3.1 and Assumption 3.15 hold. Then his a Lyapunov function for
the differential inclusion (23)with the stable set {(x,m,v )∈Rn×Rn×Rn: 0∈Dg(x),m+σx= 0}.
In the next proposition, we show that the interpolated process of the sequence {(xk,mk,vk)}is a perturbed
solution to the differential inclusion (23).
Proposition 3.21. Suppose Assumption 3.1 and Assumption 3.15 hold, and limk→+∞ηklog(k) = 0. Then
almost surely, the interpolated process of {(xk,mk,vk)}is a perturbed solution for the differential inclusion
(23).
In the following theorem, we present the convergence properties of the sequence {(xk,mk,vk)}, and prove
that limk→∞∥mk+σxk∥= 0almost surely.
Theorem 3.22. Suppose Assumption 3.1 and Assumption 3.15 hold, and limk→+∞ηklog(k) = 0. Then for
the sequence{(xk,mk,vk)}generated by the framework (AFMDW) , almost surely, it holds that
1. any cluster point of the sequence {xk}is aDg-stationary point of g;
14Under review as submission to TMLR
2.limk→∞∥mk+σxk∥= 0;
3. the sequence of function values {g(xk)}converges.
Theorem 3.22 illustrates that limk→∞∥xk−yk∥= 0. Therefore, substituting the formulation of {yk}in
(14) into the update scheme of {mk}in the framework (AFMDW), we conclude that {yk}follows the same
scheme as (15). Together with the fact that limk→∞∥xk−yk∥= 0, based on the same proof techniques as
in Lemma 3.8, we can conclude that there exists a sequence of non-negative random variables {τk}such that
limk→∞τk= 0holds almost surely, and
yk+1∈yk−θk
σ(Dτkg(yk) +ξk+1).
Thenwehavethefollowingcorollaryshowingthattheinterpolatedprocessofthesequence {yk}isaperturbed
solution of the differential inclusion (18). The proof of Corollary 3.23 is the same as Proposition 3.9, hence
is omitted for simplicity.
Corollary 3.23. Suppose Assumption 3.1 and Assumption 3.15 hold. Then the interpolated path of the
sequence{yk}is a perturbed solution of the differential inclusion (18).
4 Application: Adam with Decoupled Weight Decay
In this section, we propose a novel variant of Adam, which is named as Adam with decoupled weight decay
(AdamD). As an application of our theoretical analysis in Section 3, we show the convergence properties of
AdamD directly from the results in Theorem 3.10 and Theorem 3.22.
Throughout this section, we focus on the settings where fin (UOP) takes the following finite-sum formula-
tion:
f(x) =1
NN/summationdisplay
i=1fi(x). (27)
Here we make the following assumptions on the functions {fi:i∈[N]}in (27).
Assumption 4.1. 1. For each i∈[N],fiis a definable function that admits a definable set-valued
mappingDfias its conservative field.
2.supi∈[N], x∈Rn∥Dfi(x)∥<∞.
3.fis bounded from below.
As demonstrated in (Bolte & Pauwels, 2021), for any neural network that is built from definable blocks, the
conservative field corresponds the AD algorithms is a definable set-valued mapping. Hence, we can conclude
that Assumption 4.1(1) can be satisfied in a wide range of training tasks. Assumption 4.1(2) assumes the
Lipschitz continuity of the function f, which is common in various existing works (Barakat & Bianchi, 2021;
Guo et al., 2021; Shi et al., 2021; Zhang et al., 2022).
Moreover, (Bolte et al., 2021, Corollary 4) illustrates that fis a path-differentiable function and admits
1
N/summationtextN
i=1Dfias its conservative field. Therefore, in the rest of this section, we choose the conservative field
Dfas
Df(x) = conv/parenleftigg
1
NN/summationdisplay
i=1Dfi(x)/parenrightigg
. (28)
The detailed AdamD method is presented in Algorithm 1. In our proposed AdamD method, the weight
decay term σxkis decoupled from the update schemes for {mk}and{vk}. In particular, the estimators {vk}
are updated as an exponential moving average over {g2
k}with parameter β∈(0,1).
Then based on the convergence properties of the framework (AFMDW) presented in Theorem 3.10, the
following theorem establishes the convergence properties of Algorithm 1 with non-diminishing {ηk}.
15Under review as submission to TMLR
Algorithm 1 Adam with decoupled weight decay (AdamD) for nonsmooth optimization problem (UOP).
Require: Initial point x0∈Rn,m0∈Rnandv0∈Rn
+, weight decay parameter σ>0, safeguard parameter
ε>0, stepsizeη≤ε
σandβ∈(0,1);
1:Setk= 0;
2:whilenot terminated do
3:Independently sample ikfrom [N], and compute gk∈Dfik(xk);
4:Update the momentum term by mk+1= (1−θk)mk+θkgk;
5:Update the estimator vk+1byvk+1= (1−β)vk+βg2
k;
6:Updatexkbyxk+1=xk−η(√vk+1+ε)−1⊙(mk+1+σxk);
7:k=k+ 1;
8:end while
9:Returnxk.
Theorem 4.2. Suppose Assumption 3.5 and Assumption 4.1 hold. Moreover, we assume that the momentum
parameters{θk}is a positive sequence that satisfies limk→∞θklog(k) = 0. Then almost surely, any cluster
point of{xk}in Algorithm 1 is a Dg-stationary point of g, and the sequence {g(xk)}converges.
In the following theorem, we establish the convergence properties for Algorithm 1 when it is equipped with
single-timescale stepsizes. The results in Theorem 4.3 are direct consequences of Theorem 3.22. Hence, we
omit its proof for simplicity.
Theorem 4.3. Suppose Assumption 4.1 holds. Moreover, we assume that
1. The stepsizes ηandβare replaced by ηkandβkrespectively in Algorithm 1;
2. There exists constants τ2≥4τ1>0such thatθk=τ1ηkandβk=τ2ηkhold for any k≥0. Moreover,
the sequence{ηk}satisfies/summationtext∞
k=0ηk=∞andlimk→∞ηklog(k) = 0.
3. In Step 6 of Algorithm 1, the sequence {xk}follows the update scheme
xk+1=xk−ηk(vk+1+ε)−1
2⊙(mk+1+σxk).
Then almost surely, any cluster point of {xk}in Algorithm 1 is a Dg-stationary point of g, and the sequence
{g(xk)}converges.
5 Numerical Experiments
In this section, we conduct numerical experiments to demonstrate the effectiveness of AdamD in the context
of image classification and language modeling tasks. We compare AdamD with the most popular adaptive
algorithms used for training neural networks, i.e. Adam and AdamW. All experiments are conducted using
an NVIDIA RTX 3090 GPU and were implemented in Python 3.9 with PyTorch 1.12.0.
5.1 Implementations of AdamD
Inournumericalexperiments,wefocusontwokeytasks: imageclassificationemployingConvolutionalNeural
Networks (CNNs) and language modeling using Long Short-Term Memory (LSTM) networks (Hochreiter
& Schmidhuber, 1997). Specifically, our image classification experiments include the deployment of well-
established architectures, namely Resnet34 (He et al., 2016) and Densenet121 (Huang et al., 2018), to train
the CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al., 2009). Our language modeling experiments focus
on LSTM networks applied to the Penn Treebank dataset (Marcus et al., 1993). It is worth noting that
AdamW typically demonstrates superior generalization performance when used to train CNNs for image
classification tasks. For training LSTMs, prior studies such as (Ding et al., 2023; Loshchilov & Hutter, 2017;
Zhuang et al., 2020) have observed that Adam exhibits better generalization capacity than AdamW.
16Under review as submission to TMLR
5.1.1 CNNs on image classification
In all our experiments on image classification, we train the models consistently for 200 epochs, employing a
batch size of 128. At the 150th epoch, we reduce the step size by a factor of 0.1. This step size reduction
schedule is a prevalent practice in contemporary deep neural network training. It is helpful to accelerate the
convergence of the optimization algorithm, and to enhance generalization capacity. Similar strategies can be
observed in previous works, such as (He et al., 2016; Zhuang et al., 2020). The weight decay parameter σis
fixed to be 5×10−3. We use the following hyperparameters setting for tested algorithms:
•Adam/AdamW: We search the stepszie ηwithin the range of {5×10−4,10−3,5×10−3,10−2,5×
10−2,10−1,5×10−1,1}. Additionally, we set ε= 10−8,θk= 10−1andβ= 10−3as the default
setting in Pytorch.
•AdamD: We adopt the searching scheme for stepsize as 0.1×{5×10−4,10−3,5×10−3,10−2,5×
10−2,10−1,5×10−1,1}. We setθs=θ0
(log(s+2))3
2, withsrepresenting the epoch number. Within the
s-th epoch,θktakes the constant value θs. Under this setting, we can easily verify that θk=o(1
logk).
Here, we set the initial momentum parameter to θ0= 10−1, the second moment parameter to
β= 10−3and the regularization parameter to ε= 10−8, which are the same as the default settings
in PyTorch for Adam/AdamW.
0 25 50 75 100 125 150 175 200
Training Epoch5060708090100AccuracyTrain accuracy ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch868890929496AccuracyT est accuracy ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch050100150200250300350400LossTrain loss ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch0255075100125150175200LossT est loss ~ Training epoch
AdamD
Adam
AdamW
(a) Train accuracy (b) Test accuracy (c) Train loss (d) Test loss
Figure 1: ResNet34 on CIFAR10 dataset. Stepsize is reduced to 0.1 times of the original value at the 150th
epoch.
0 25 50 75 100 125 150 175 200
Training Epoch5060708090100AccuracyTrain accuracy ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch707580859095AccuracyT est accuracy ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch050100150200250300350400LossTrain loss ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch0255075100125150175200LossT est loss ~ Training epoch
AdamD
Adam
AdamW
(a) Train accuracy (b) Test accuracy (c) Train loss (d) Test loss
Figure 2: DenseNet121 on CIFAR10 dataset. Stepsize is reduced to 0.1 times of the original value at the
150th epoch.
In Step 6 of Algorithm 1, the coefficient associated with xkis expressed as 1−ησ(√vk+1+ε)−1. It is
worth noting that as training progresses, the value of√vk+1+εtends to become small. To ensure that the
coefficient does not become excessively small, in practice, AdamD employs a smaller stepsize compared to
Adam and AdamW. This practice of selecting a smaller scale stepsize also occurs in other optimizers, such
as Lion (Chen et al., 2023). The numerical results, as illustrated in Figure 4, reveal compelling insights.
Both AdamD and AdamW consistently achieve 100% training accuracy, whereas Adam falls short in this
regard. From the training loss plots, we observe that the convergence speed of AdamD falls between that of
17Under review as submission to TMLR
0 25 50 75 100 125 150 175 200
Training Epoch5060708090100AccuracyTrain accuracy ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch304050607080AccuracyT est accuracy ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch02004006008001000LossTrain loss ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch050100150200250300350400LossT est loss ~ Training epoch
AdamD
Adam
AdamW
(a) Train accuracy (b) Test accuracy (c) Train loss (d) Test loss
Figure 3: ResNet34 on CIFAR100 dataset. Stepsize is reduced to 0.1 times of the original value at the 150th
epoch.
0 25 50 75 100 125 150 175 200
Training Epoch5060708090100AccuracyTrain accuracy ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch404550556065707580AccuracyT est accuracy ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch02004006008001000LossTrain loss ~ Training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch050100150200250300350400LossT est loss ~ Training epoch
AdamD
Adam
AdamW
(a) Train accuracy (b) Test accuracy (c) Train loss (d) Test loss
Figure 4: DenseNet121 on CIFAR100 dataset. Stepsize is reduced to 0.1 times of the original value at the
150th epoch.
AdamW and Adam. In most instances, AdamD achieves nearly the same level of generalization as AdamW.
Moreover, the generalization capacity of Adam is noticessly inferior to that of the other two algorithms.
This observation underscores the necessity of weight decoupling when solving the quadratically regularized
problem defined in (UOP).
To verify the results in Lemma 3.7, we also present a plot of ∥xk+σmk∥as shown in Figure 5. When θk
adheres to a decay schedule described by O(k−γ), (32) and basic calculus imply that ∥σxk+mk∥exhibits
an asymptotic behavior of O(k−γ). The results in Figure 5 are consistent with our theoretical analysis
that{∥mk+σxk∥}converges to 0, or equivalently {∥xk−yk∥}converges to 0. Notably, larger values of γ
correspond to a more rapid decline in ∥σxk+mk∥.
5.1.2 LSTMs on language modeling
In all our language modeling experiments, we train our models for 200 epochs while employing a batch
size of 128. Additionally, we adopt a stepsize reduction strategy that decreases the stepsize to 0.1 times
its original value twice during training, specifically at the 75th and 150th epochs. These settings adhere
to the commonly used experimental setup for training LSTMs, as demonstrated in previous works (Chen
et al., 2021; Zhuang et al., 2020). This stepsize reduction strategy serves to accelerate the convergence of the
optimization algorithm, simultaneously enhancing its generalization capacity. The weight decay parameter
σis fixed at 1×10−5throughout these experiments. Other hyperparameter settings are the same as those
in Section 5.1.1. The numerical results are displayed in Figure 6.
From Figure 6, we can observe that both AdamD and Adam exhibit superior generalization capacity com-
pared to AdamW. For 1- and 2-layer LSTM, AdamD exhibits a similar generalization capacity as Adam. In
the case of larger 3-layer LSTM models, AdamD outperforms Adam, achieving a test perplexity that is at
least 2 units lower.
18Under review as submission to TMLR
0 10000 20000 30000 40000 50000 60000 70000 80000
Training Iterations108
106
104
102
100102xk+mk
Train residual ~ Training epoch
1/(log(k))3/2
1/k
1/k
1/k2
0 10000 20000 30000 40000 50000 60000 70000 80000
Training Iterations106
105
104
103
102
101
100101102xk+mk
Train residual ~ Training epoch
1/(log(k))3/2
1/k
1/k
1/k2
(a) ResNet34 on CIFAR10 (b) ResNet34 on CIFAR100
Figure 5:∥mk+σxk∥under different decay rates of {θk}. The stepsizes for updating {xk}are fixed.
0 25 50 75 100 125 150 175 200
Training Epoch406080100120140160180PerplexityTrain set perplexity ~ training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch20406080100120140PerplexityTrain set perplexity ~ training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch20406080100120140PerplexityTrain set perplexity ~ training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch8090100110120130PerplexityT est set perplexity ~ training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch65707580859095100PerplexityT est set perplexity ~ training epoch
AdamD
Adam
AdamW
0 25 50 75 100 125 150 175 200
Training Epoch6065707580859095100PerplexityT est set perplexity ~ training epoch
AdamD
Adam
AdamW
(a) 1-layer LSTM (b) 2-layer LSTM (c) 3-layer LSTM
Figure 6: Training and test perplexity (lower is better) of LSTMs on Penn Treebank dataset with stepsize
reduced to 0.1 times of the original value at the 75th epoch and 150th epoch.
5.1.3 Performance with different choices of {θk}and{βk}
Finally, we investigate the performance of AdamD method with different choices of its stepsizes {θk}and
{βk}for momentum terms {mk}and{vk}, respectively. In our numerical experiments, the sequences of
stepsizes{θk}and{βk}are fixed as constants θ∈[0,1]andβ∈[0,1], respectively. The weight decay
parameter is set to 5×10−4. The step size is η= 10−3, the regularization parameter for the second moment
term isε= 10−8as the default settings in PyTorch for Adam.
From Figure 7, we can observe that the blue region corresponding to AdamD is larger than that of Adam. As
demonstrated by (Zhang et al., 2022), Adam can fail when θis small and βis large, as shown by the red area
(indicating larger training loss) on the bottom right portion of Figure 7(a). In contrast, the corresponding
region for AdamD in Figure 7(b) remains blue (indicating lower training loss). Thus, we can conclude that
the incorporation of weight decay in AdamD enhances its robustness to the choices of θandβcompared
to Adam with coupled weight decay. Moreover, these results verify the theoretical results in Theorem 3.13,
which implies that as long as the momentum parameters θkremain sufficiently small, the sequence xkstays
19Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.0
1
0.00.20.40.60.81.02
0.60.81.01.21.41.61.82.0
train loss
1-¸1-²
0.0 0.2 0.4 0.6 0.8 1.0
1
0.00.20.40.60.81.02
0.60.81.01.21.41.61.82.0
train loss
1-θ¸1-²
1-¸
(a) Adam (b) AdamD
Figure 7: The performance of Adam and AdamD on training three-layer CNN on MNIST dataset, with {θk}
and{βk}fixed as constants θandβ, respectively.
stable. This stability is preserved with significantly relaxed requirements on the updates of the estimator
vk, provided the preconditioner H(vk)remains bounded both below and above.
5.2 Further discussions on the AdamD
5.2.1 Asymptotic approximation to SGD sequence helps generalization
As demonstrated in Lemma 3.7, the term ∥σxk+mk∥converges to 0asktends to infinity. Then as discussed
in Lemma 3.8, the sequence {yk}(defined by yk:=−σ−1mk) approximately follows the update scheme (15),
which asymptotically approximates a SGD method. Together with the fact that limk→∞∥xk−yk∥= 0, we
can conclude that the sequence {xk}in AdamD is controlled by an SGD sequence {yk}askgoes to infinity.
Moreover, the interpolated process of {yk}is a perturbed solution of the differential inclusion (18), i.e.,
dy
dt∈−(Df(y) +σy). (29)
On the other hand, in the early stage of the iterations of AdamD, the term ∥σxk+mk∥is large, and the
ratio ofθkandηkusually remains nearly unchanged. Then as illustrated in the discussion in Section 4, the
sequence{(xk,mk,vk)}jointly tracks the trajectories of the differential inclusion
/parenleftbiggdx
dt,dm
dt,dv
dt/parenrightbigg
∈−
(P+(v) +ε)−1
2⊙(m+σx)
τ1m−τ1Df(x)
τ2v−τ2U(x)
. (30)
HereU(x) :=1
N/summationtextN
i=1{d⊙d:d∈Dfi(x)}. Similar results are also exhibited in (Bianchi et al., 2022; Xiao
et al., 2023a). As the differential inclusion (30) imposes preconditioners to the update directions of {xk}
based on the second-order moments of the stochastic subgradients, the sequence could quickly converge to a
neighborhood of the stationary points.
These theoretical properties explain the fast convergence of AdamD in the early stage of the training and its
lower generalization error than Adam with coupled weight decay. Based on the numerical experiments and
our theoretical analysis, we believe the ability to asymptotically track an SGD sequence in AdamD helps to
explain its superior generalization performance over Adam.
5.2.2 Decoupled weight decay is equivalent to quadratic regularization
It is conjectured in (Loshchilov & Hutter, 2017) that the quadratic regularization term contributes to the
low generalization error in training neural networks. Moreover, the authors in (Loshchilov & Hutter, 2017)
20Under review as submission to TMLR
develop AdamW, showing that weight decay is not equivalent to quadratic regularization. As a result, the
termσxkin AdamW is not scaled by the preconditioner (√vk+1+ε)−1. Therefore, AdamW does not have
a clear objective function and lacks convergence guarantees in training nonsmooth neural networks.
In our AdamD method, the objective function is exactly the g(x)in (UOP). Hence the weight decay
parameter σis exactly the penalty parameter for the quadratic penalty termσ
2∥x∥2in (UOP). More
importantly, we provide theoretical guarantees for AdamD in training nonsmooth neural networks. The
stationarity of the iterates {xk}is characterized by Df(xk)+σxk, hence has clearer meaning when compared
with AdamW.
Furthermore, our numerical experiments demonstrate the superior performance of AdamD, illustrating that
employing the quadratic regularization term in (UOP) does not undermine the generalization error. Based
on these results, we can conclude that within our framework (AFMDW), the weight decay can be interpreted
as the quadratic regularization, which is different from the demonstrations in (Loshchilov & Hutter, 2017)
regarding AdamW.
6 Conclusion
In this paper, motivated by AdamW, we propose a novel framework (AFMDW) for Adam-family methods
with decoupled weight decay. We prove that under mild assumptions with non-diminishing stepsizes {ηk}
and diminishing momentum parameters {θk}, any cluster point of {xk}is aDg-stationary point of (UOP).
When{θk}is also non-diminishing, the sequence {xk}eventually stabilizes around the critical points of
theDg-stationary point of (UOP). Moreover, when employing a single-timescale scheme, any cluster point
of{xk}is aDg-stationary point of (UOP). Compared with AdamW, our proposed framework (AFMDW)
enjoys convergence guarantees in training nonsmooth neural networks and yields solutions that have clearer
meanings. More importantly, we prove that the decoupled weight decay grants more flexibility of the choices
of the parameters {θk}and{βk}in (AFMDW) than Adam. This fact theoretically illustrates the advantages
of the employment of the decoupled weight decay.
As an application of our proposed framework (AFMDW), we develop a novel Adam-family method named
Adam with decoupled weight decay (AdamD), and prove its convergence properties under mild conditions.
Numerical experiments on image classification and language modeling demonstrate the effectiveness of our
proposed method. To conclude, we believe that our work has enriched the theoretical understanding of
weight decay and explained its practical utility in the field of deep learning applications.
References
Anas Barakat and Pascal Bianchi. Convergence and dynamical behavior of the ADAM algorithm for non-
convex stochastic optimization. SIAM Journal on Optimization , 31(1):244–274, 2021.
Michel Benaïm. Dynamics of stochastic approximation algorithms. In Seminaire de probabilites XXXIII , pp.
1–68. Springer, 2006.
Michel Benaïm, Josef Hofbauer, and Sylvain Sorin. Stochastic approximations and differential inclusions.
SIAM Journal on Control and Optimization , 44(1):328–348, 2005.
Pascal Bianchi and Rodolfo Rios-Zertuche. A closed-measure approach to stochastic approximation. arXiv
preprint arXiv:2112.05482 , 2021.
Pascal Bianchi, Walid Hachem, and Sholom Schechtman. Convergence of constant step stochastic gradient
descent for non-smooth non-convex functions. Set-Valued and Variational Analysis , pp. 1–31, 2022.
JérômeBolteandEdouardPauwels. Amathematicalmodelforautomaticdifferentiationinmachinelearning.
Advances in Neural Information Processing Systems , 33:10809–10819, 2020.
Jérôme Bolte and Edouard Pauwels. Conservative set valued fields, automatic differentiation, stochastic
gradient methods and deep learning. Mathematical Programming , 188(1):19–51, 2021.
21Under review as submission to TMLR
Jérôme Bolte, Aris Daniilidis, Adrian Lewis, and Masahiro Shiota. Clarke subgradients of stratifiable func-
tions.SIAM Journal on Optimization , 18(2):556–572, 2007.
Jérôme Bolte, Tam Le, Edouard Pauwels, and Tony Silveti-Falls. Nonsmooth implicit differentiation for
machine-learning and optimization. Advances in Neural Information Processing Systems , 34, 2021.
Jérôme Bolte, Tam Le, and Edouard Pauwels. Subgradient sampling for nonsmooth nonconvex minimization.
arXiv preprint arXiv:2202.13744 , 2022a.
Jérôme Bolte, Edouard Pauwels, and Antonio José Silveti-Falls. Differentiating nonsmooth solutions to
parametric monotone inclusion problems. arXiv preprint arXiv:2212.07844 , 2022b.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint , volume 48. Springer, 2009.
Siegfried Bos and E Chug. Using weight decay to optimize the generalization ability of a perceptron. In
Proceedings of International Conference on Neural Networks (ICNN’96) , volume 1, pp. 241–246. IEEE,
1996.
Camille Castera, Jérôme Bolte, Cédric Févotte, and Edouard Pauwels. An inertial Newton algorithm for
deep learning. The Journal of Machine Learning Research , 22(1):5977–6007, 2021.
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing the gen-
eralization gap of adaptive gradient methods in training deep neural networks. In Proceedings of the
Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence , pp.
3267–3275, 2021.
Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi
Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint
arXiv:2302.06675 , 2023.
Frank H Clarke. Optimization and nonsmooth analysis , volume 5. SIAM, 1990.
Aris Daniilidis and Dmitriy Drusvyatskiy. Pathological subgradient dynamics. SIAM Journal on Optimiza-
tion, 30(2):1327–1338, 2020.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient method
converges on tame functions. Foundations of Computational Mathematics , 20(1):119–154, 2020.
Kuangyu Ding, Jingyang Li, and Kim-Chuan Toh. Nonconvex stochastic Bregman proximal gradient method
with application to deep learning. arXiv preprint arXiv:2306.14522 , 2023.
John C Duchi and Feng Ruan. Stochastic methods for composite and weakly convex optimization problems.
SIAM Journal on Optimization , 28(4):3229–3259, 2018.
Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for algorithms
of the Adam family. NeurIPS OPT Workshop , 2021.
Mert Gürbüzbalaban, Andrzej Ruszczyński, and Landi Zhu. A stochastic subgradient method for distribu-
tionally robust non-convex and non-smooth learning. Journal of Optimization Theory and Applications ,
194(3):1014–1041, 2022.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780,
1997.
Xiaoyin Hu, Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. A constraint dissolving approach for nonsmooth
optimization over the Stiefel manifold. arXiv preprint arXiv:2205.10500 , 2022a.
22Under review as submission to TMLR
Xiaoyin Hu, Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. An improved unconstrained approach for bilevel
optimization. arXiv preprint arXiv:2208.00732 , 2022b.
Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: An efficient
densenet using learned group convolutions. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 2752–2761, 2018.
Cédric Josz, Lexiao Lai, and Xiaopeng Li. Convergence of the momentum method for semialgebraic functions
with locally lipschitz gradients. SIAM Journal on Optimization , 33(4):3012–3037, 2023.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd
International Conference for Learning Representations , 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Anders Krogh and John Hertz. A simple weight decay can improve generalization. Advances in neural
information processing systems , 4, 1991.
Tam Le. Nonsmooth nonconvex stochastic heavy ball. arXiv preprint arXiv:2304.13328 , 2023.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On
the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265 , 2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 ,
2017.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic bound of
learning rate. arXiv preprint arXiv:1902.09843 , 2019.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of
english: The penn treebank. 1993.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In 6th
International Conference on Learning Representations (ICLR) , 2018.
Andrzej Ruszczyński. Convergence of a stochastic subgradient method with averaging for nonsmooth non-
convex constrained optimization. Optimization Letters , 14(7):1615–1625, 2020.
Andrzej Ruszczynski. A stochastic subgradient method for nonsmooth nonconvex multilevel composition
optimization. SIAM Journal on Control and Optimization , 59(3):2301–2320, 2021.
Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. Rmsprop converges with proper hyperparameter. In
International Conference on Learning Representation , 2021.
Lou Van den Dries and Chris Miller. Geometric categories and o-minimal structures. Duke Mathematical
Journal, 84(2):497–540, 1996.
Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Zhi-Ming Ma, Tie-Yan Liu, and Wei Chen. Provable
adaptivity in adam. arXiv preprint arXiv:2208.09900 , 2022.
NachuanXiao,XiaoyinHu,XinLiu,andKim-ChuanToh. Adam-familymethodsfornonsmoothoptimization
with convergence guarantees. arXiv preprint arXiv:2305.03938 , 2023a.
NachuanXiao, XiaoyinHu, andKim-ChuanToh. Convergenceguaranteesforstochasticsubgradientmethods
in nonsmooth nonconvex optimization. arXiv preprint arXiv:2307.10053 , 2023b.
Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for Riemannian optimization. Mathe-
matics of Operations Research , 2023c.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for
nonconvex optimization. Advances in Neural Information Processing Systems , 31, 2018.
23Under review as submission to TMLR
Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without
any modification on update rules. Advances in Neural Information Processing Systems , 35:28386–28399,
2022.
Pan Zhou, Xingyu Xie, Zhouchen Lin, and Shuicheng Yan. Towards understanding convergence and gener-
alization of AdamW. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2024.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris,
and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances
in neural information processing systems , 33:18795–18806, 2020.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences
of Adam and RMSProp. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pp. 11127–11135, 2019.
A Proofs
Proof of Proposition 3.4. As illustrated in Assumption 3.3, dk∈Dδk
f(xk)and{ξk}is uniformly bounded.
Then it is easy to verify that there exists a constant ˆLsuch that∥gk∥=∥dk+ξk+1∥≤ˆL(1 +∥xk∥ν)holds
for anyk≥0.
Let the constant Qbe defined as
Q≥max

/parenleftigg
2MvˆL
εvσ/parenrightigg 1
1−ν
,Mv∥m0∥
εvσ,∥x0∥+ 1

. (31)
In the following, for any sequence {xk}generated from (AFMDW), we aim to prove that the set {k≥0 :
∥xk∥≥Q}is an empty set by contradiction. Therefore, we assume that the set {k≥0 :∥xk∥≥Q}is non-
empty and set τ= inf{k≥0 :∥xk∥≥Q}−1. Then from the definition of τ, we have∥xτ+1∥≥Q>∥xτ∥.
On the other hand, from the update scheme (AFMDW), for any k<τ, we have
∥mk+1∥≤max/braceleftbigg
m0,sup
0≤i≤k+1∥gi∥/bracerightbigg
<max{∥m0∥,ˆL(1 +Qν)}≤σεv
MvQ,
where the last inequality follows from the definition of Qand the fact that
ˆL(1 +Qν)≤2ˆLQν=σεv
Mv·2MvˆL
σεvQν≤σεv
MvQ1−νQν=σεv
MvQ.
Then it holds that
∥xτ+1∥=∥(1−ηkσHτ(vτ+1))⊙xτ−ηkHτ(vτ+1)⊙mτ+1∥
≤(1−ηkσεv)∥xτ∥+ηkMv∥mτ+1∥<(1−ηkσεv)Q+ηkMv·σεv
MvQ=Q.
But∥xτ+1∥<Qcontradicts to the definition of τ. Thus, the set{k≥0 :∥xk∥≥Q}is empty. Therefore,
we have that supk≥0∥xk∥≤Qholds almost surely. This completes the proof.
Proof of Lemma 3.7. From Assumption 3.5, there exists a constant ˜η∈(0,1)such that max{|1−ηkσMv|,|1−
ηkσεv|}≤ 1−˜ηholds for any k≥0. Then from the update scheme of {xk}in the framework (AFMDW),
24Under review as submission to TMLR
almost surely, it holds that
∥σxk+1+mk+1∥
=∥(1−ηkσH(vk+1))⊙(σxk+mk) +θk(1−ηkσH(vk+1))⊙(gk−mk)∥
≤max{|1−ηkσMv|,|1−ηkσεv|}(∥σxk+mk∥+θk∥gk−mk∥)
≤(1−˜η)∥σxk+mk∥+ 2Mdθk≤(1−˜η)k+1∥σx0+m0∥+ 2Mdk/summationdisplay
i=0(1−˜η)k−iθi
≤(1−˜η)k+1(σMx+Md) + 2Mdk/summationdisplay
i=0(1−˜η)k−iθi=:ˆδk+1.(32)
Since limk→∞θk= 0, we have lim
k→∞/summationtextk
i=0(1−˜η)k−iθi= 0. Thus we get limk→∞ˆδk= 0, and∥σxk+mk∥≤ˆδk
holds for any k≥0. This completes the proof.
Proof of Lemma 3.8. As illustrated in Assumption 3.3(2), there exists ˜xk∈Bδk(xk)and ˜dk∈Df(˜xk)
such that/vextenddouble/vextenddoubledk−˜dk/vextenddouble/vextenddouble≤δkandlimk→∞δk= 0. Combining with equation (32), it holds that ∥yk−˜xk∥≤
∥yk−xk∥+∥xk−˜xk∥≤ˆδk
σ+δk. As a result,
dist (dk+σyk,Dg(˜xk))≤/vextenddouble/vextenddoubledk+σyk−(˜dk+σ˜xk)/vextenddouble/vextenddouble
≤/vextenddouble/vextenddoubledk−˜dk/vextenddouble/vextenddouble+σ∥yk−˜xk∥≤δk+σ(ˆδk
σ+δk).
Since ˜xk∈Bδ⋆
k(yk)anddist(dk+σyk,Dg(˜xk))≤δ⋆
k, we get (16).
Proof of Proposition 3.9. BasedonLemma2.21,byverifyingitsconditions,wecanprovethattheinterpolated
process of{yk}is a perturbed solution for the differential inclusion (18).
Condition (1) of Lemma 2.21 directly follows from Assumption 3.3(3) and Proposition 2.3, by choosing the
stepsizes in (9) as {θk
σ}. Moreover, Lemma 3.8 guarantees the validity of the condition (2) in Lemma 2.21 by
noting that limk→∞δ∗
k= 0. Furthermore, condition (3) of Lemma 2.21 follows from Assumption 3.3(2) and
Lemma 3.6. As a result, directly from Lemma 2.21, we can conclude that almost surely, the interpolated
process of{yk}is a perturbed trajectory of the differential inclusion (18).
Proof of Theorem 3.10. From Lemma 3.2 and Proposition 2.18, we can conclude that gis a Lyapunov
function for the differential inclusion (18) with the stable set {x∈Rn: 0∈Dg(x)}. Moreover, Proposition
(3.9) illustrates that almost surely, the interpolated process of the sequence {yk}in (14) is a perturbed
solution of the differential inclusion (18). As a result, it follows from Theorem 2.22 that any cluster point of
{yk}lies in the set{x∈Rn: 0∈Dg(x)}and the sequence {g(yk)}converges.
Since limk→∞θk= 0, Lemma 3.7 implies that limk→∞∥xk−yk∥= 0holds almost surely. Then from the
continuity of gand the convergence properties of {yk}, we can conclude that any cluster point of {xk}lies
in the set{x∈Rn: 0∈Dg(x)}and the sequence {g(xk)}converges. This completes the proof.
Proof of Lemma 3.12. From Assumption 3.11, it holds for all s≥0and anyisatisfyings≤i≤Λ(λ(s) +T)
that/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublei/summationdisplay
k=sθkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN·⌈s
N⌉−1/summationdisplay
k=sθkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN·⌊i
N⌋−1/summationdisplay
k=N·⌈s
N⌉θkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublei/summationdisplay
k=N·⌊i
N⌋θkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN·⌈s
N⌉−1/summationdisplay
k=sθkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublei/summationdisplay
k=N·⌊i
N⌋θkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble.(33)
25Under review as submission to TMLR
Therefore, for any any ε>0, chooseθε=ε
2NMξguarantees that
lim sup
s→+∞sup
s≤i≤Λ(λ(s)+T)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublei/summationdisplay
k=sθkξk+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤2NMξ lim sup
s→+∞, s≤i≤Λ(λ(s)+T)θi≤ε.
This completes the proof.
Proof of Theorem 3.13. For the update scheme (15), Lemma 3.12 and (Xiao et al., 2023b, Theorem 3.5)
illustrate that for any ε > 0, there exists ˆθ1,T > 0such that whenever lim supk≥0θk≤ˆθ1and{ξk}is
(ε,T,{θk})-controlled, we have
lim sup
k→+∞dist (yk,{x∈Rn: 0∈Dg(x)})≤ε
2. (34)
Then by equation (32) in Lemma 3.7, we have that there exists ˆθ2such that whenever lim supk≥0θk≤ˆθ2,
lim supk→∞∥xk−yk∥≤ε
2. Therefore, whenever lim supk≥0θk≤min{ˆθ1,ˆθ2}, we have that
lim sup
k→+∞dist (xk,{x∈Rn: 0∈Dg(x)})≤lim sup
k→+∞dist (yk,{x∈Rn: 0∈Dg(x)}) +ε
2≤ε.(35)
This completes the proof.
Proof of Lemma 3.19. Notice that fis a potential function that admits Dfas its conservative field, and the
function (x,m,v )∝⇕⊣√∫⊔≀→/angbracketleftig
m+σx,(P+(v) +ε)−1
2⊙(m+σx)/angbracketrightig
is semi-algebraic and thus definable. Then by
the chain rule for conservative field (Bolte & Pauwels, 2021), we can conclude that his a potential function
that admitsDhas its conservative field. Moreover, as Dfand∂P+are convex-valued over Rn, it holds that
Dhis convex-valued over Rn×Rn×Rn. This completes the proof.
Proof ofof Proposition 3.20. For any trajectory of the differential inclusion (23), there exists lf:R+→Rn
andlu:R+→Rnsuch thatlf(s)∈Df(x(s))andlu(s)∈U(x(s),m(s))for almost every s≥0, and
( ˙x(s),˙m(s),˙v(s)) =
−(P+(v(s)) +ε)−1
2⊙(m(s) +σx(s))
−τ1m(s) +τ1lf(s)
−τ2P+(v(s)) +τ2lu(s)
. (36)
Then from the formulation of h, we have
⟨Dh(x(s),m(s),v(s)),( ˙x(s),˙m(s),˙v(s))⟩
∋ −/angbracketleftig
lf(s) +σx(s) +σ
τ1(P+(v(s)) +ε)−1
2⊙(m(s) +σx(s)),(P+(v(s)) +ε)−1
2⊙(m(s) +σx(s))/angbracketrightig
+/angbracketleftig
(P+(v(s)) +ε)−1
2⊙(m(s) +σx(s)),−m(s) +lf(s)/angbracketrightig
+τ2
4τ1/angbracketleftig
(m(s) +σx(s))2⊙(P+(v(s)) +ε)−3
2⊙∂P+(v(s)),v(s)−lu(s)/angbracketrightig
≤ −σ
τ1/angbracketleftbig
(P+(v(s)) +ε)−1⊙(m(s) +σx(s)),m(s) +σx(s)/angbracketrightbig
−/angbracketleftig
(P+(v(s)) +ε)−1
2⊙(m(s) +σx(s)),m(s) +σx(s)/angbracketrightig
+τ2
4τ1/angbracketleftig
(m(s) +σx(s))2,P+(v(s))⊙(P+(v(s)) +ε)−3
2/angbracketrightig
≤ −σ
τ1/angbracketleftbig
(P+(v(s)) +ε)−1⊙(m(s) +σx(s)),m(s) +σx(s)/angbracketrightbig
−/parenleftig
1−τ2
4τ1/parenrightig/angbracketleftig
(P+(v(s)) +ε)−1
2⊙(m(s) +σx(s)),m(s) +σx(s)/angbracketrightig
≤ −σ
τ1/angbracketleftbig
(P+(v(s)) +ε)−1⊙(m(s) +σx(s)),m(s) +σx(s)/angbracketrightbig
.
26Under review as submission to TMLR
Here the first inequality follows from the fact that lu(s)≥0and∂P+(v)⊙v=P+(v). The third inequality
follows from the fact that 1−τ2
4τ1≥0in Assumption 3.15(3). Therefore, we can conclude that for any initial
point (x(0),m(0),v(0))∈Rn×Rn×Rn, it holds for any t≥0that
h(x(t),m(t),v(t))−h(x(0),m(0),v(0))
=/integraldisplayt
0min
l∈Dh(x(s),m(s),v(s))⟨l,( ˙x(s),˙m(s),˙v(s))⟩ds
≤ −σ
τ1/integraldisplayt
0/angbracketleftbig
(P+(v(s)) +ε)−1⊙(m(s) +σx(s)),m(s) +σx(s)/angbracketrightbig
ds.(37)
As a result, we can conclude that for any trajectory of the differential inclusion (23), it holds for any t>0
thath(x(t),m(t),v(t))≤h(x(0),m(0),v(0)).
Now consider the case when (x(0),m(0),v(0))/∈{(x,m,v )∈Rn×Rn×Rn: 0∈Dg(x),m+σx= 0}.
Suppose there exists some T >0such that
h(x(T),m(T),v(T)) =h(x(0),m(0),v(0)). (38)
Then (37) implies that m(s) +σx(s) = 0holds for almost every s∈[0,T]. Therefore, ˙m(s) +σ˙x(s) = 0and
(23) implies that ˙x(s) = 0hold for almost every s∈[0,T]. As a result, we have
0 = ˙m(s)∈−τ1m(s) +τ1Df(x(s)) =τ1σx(s) +τ1Df(x(s))
holds for almost every s∈[0,T]. Together with the fact that (x(t),m(t),v(t))is absolutely continuous and
Dfis graph-closed and locally bounded, we have that
m(0) +σx(0) = 0,0∈Df(x(0)) +σx(0) =Dg(x(0)).
But the above contradicts the condition that (x(0),m(0),v(0))/∈{(x,m,v ) : 0∈Dg(x),m+σx= 0}. As a
result, we can conclude that for any T >0, whenever (x(0),m(0),v(0))/∈{(x,m,v ) : 0∈Dg(x),m+σx= 0},
it holds that
h(x(T),m(T),v(T))<h(x(0),m(0),v(0)).
This completes the proof.
Proof of Proposition 3.21. From the uniform boundedness of {mk},{vk}and{gk}in Lemma 3.16 and
Lemma 3.17, and Assumption 3.15(4), we can conclude that limk→∞∥mk+1−mk∥+∥vk+1−vk∥= 0.
Therefore, there exists a sequence of random variables {τk}such that almost surely, limk→∞τk= 0holds
and∥mk+1−mk∥+∥vk+1−vk∥≤τk.
Then from the formulation of the framework (AFMDW), the sequence {(xk,mk,vk)}satisfies the following
inclusion
(xk+1,mk+1,vk+1)∈(xk,mk,vk)−ηkGτk(xk,mk,vk)−ηk(0,−τ1ξk+1,0).
Then it directly follows from Assumption 3.15(4) and Proposition 2.3 that
lim
s→∞sup
s≤i≤Λ(λs+T)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublei/summationdisplay
k=sηk(0,τ1ξk+1,0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble= 0.
Therefore, we can conclude that the conditions (1) and (2) in Lemma 2.21 hold. Moreover, condition (3)
in Lemma 2.21 directly follows from Assumption 3.15(1), Lemma 3.16 and Lemma 3.17. Therefore, from
Lemma 2.21, we can conclude that the interpolated process of {(xk,mk,vk)}is a perturbed solution for the
differential inclusion (23). This completes the proof.
Proof of Theorem 3.22. From Proposition 3.21, we can conclude that the interpolated process of
{(xk,mk,vk)}isaperturbedsolutionforthedifferentialinclusion(23). Moreover, Proposition3.20illustrates
27Under review as submission to TMLR
thathis a Lyapunov function for the differential inclusion (23) with stable set {(x,m,v )∈Rn×Rn×Rn:
0∈Dg(x),m+σx= 0}. Then we can conclude that any cluster point of {(xk,mk,vk)}lies in the set
{(x,m,v )∈Rn×Rn×Rn: 0∈Dg(x),m+σx= 0}, and the sequence {h(xk,mk,vk)}converges.
Thus, we can conclude that any cluster point of {xk}lies in the set{x∈Rn: 0∈Dg(x)}, and any cluster
point of{(xk,mk)}lies in{(x,m)∈Rn×Rn:σx+m= 0}. As a result, noting that {σxk+mk}is bounded
inRn, it holds that limk→∞∥σxk+mk∥= 0. Furthermore, since
lim
k→∞|h(xk,mk,vk)−g(xk)|≤lim
k→∞1
2τ1√ε∥σxk+mk∥2= 0,
we can deduce that the sequence {g(xk)}converges. This completes the proof.
Proof of Theorem 4.2. We first verify the validity of Assumption 3.1. The definability of fiandDfiimplies
the definability of fandDf, hence from (Bolte & Pauwels, 2021, Theorem 5), fis path-differentiable and
the set{f(x) : 0∈Df(x)}is a finite subset of R. Additionally, Assumption 4.1(2) ensures the validity of
Assumption 3.1(2). This verifies the validity of Assumption 3.1.
Moreover, let{Fk}be a sequence of σ-algebras generated by {xj,dj,mj:j≤k},dk=E[gk|Fk]and
ξk+1=gk−dk. Then we can conclude that dk∈Df(xk)andE[ξk+1|Fk] = 0. Moreover, Assumption 4.1(2)
illustrates that there exists a constant Mfsuch that supi∈[N], x∈Rn∥Df(x)∥≤Mf. Thus we can conclude
that supk≥0∥gk∥≤Mfandsupk≥0∥dk∥≤Mfhold almost surely. Then supk≥0∥ξk+1∥≤2Mfholds almost
surely. This verifies the validity of Assumption 3.3(3).
Furthermore, from the update scheme in Step 5 of Algorithm 1, we can conclude that supk≥0∥vk∥ ≤
supk≥0/vextenddouble/vextenddoubleg2
k/vextenddouble/vextenddouble≤M2
f. This illustrates that Assumption 3.3(1) holds with εv=1
Mf+εandMv=1
ε. Therefore,
from Theorem 3.10, we can conclude that any cluster point of the sequence {xk}is aDg-stationary point of
g, and the sequence {g(xk)}converges. This completes the proof.
28