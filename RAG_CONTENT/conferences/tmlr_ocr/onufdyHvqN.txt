Published in Transactions on Machine Learning Research (04/2023)
Private Multi-Task Learning: Formulation and Applications
to Federated Learning
Shengyuan Hu shengyuanhu@cmu.edu
Carnegie Mellon University
Zhiwei Steven Wu zstevenwu@cmu.edu
Carnegie Mellon University
Virginia Smith smithv@cmu.edu
Carnegie Mellon University
Reviewed on OpenReview: https: // openreview. net/ forum? id= onufdyHvqN
Abstract
Many problems in machine learning rely on multi-task learning (MTL) , in which the goal is
to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant
for privacy-sensitive applications in areas such as healthcare, ﬁnance, and IoT computing,
where sensitive data from multiple, varied sources are shared for the purpose of learning. In
this work, we formalize notions of client-level privacy for MTL via billboard privacy (BP), a
relaxation of diﬀerential privacy for mechanism design and distributed optimization. We
then propose an algorithm for mean-regularized MTL, an objective commonly used for
applications in personalized federated learning, subject to BP. We analyze our objective and
solver, providing certiﬁable guarantees on both privacy and utility. Empirically, we ﬁnd that
our method provides improved privacy/utility trade-oﬀs relative to global baselines across
common federated learning benchmarks.
1 Introduction
Multi-task learning (MTL) aims to solve multiple learning tasks simultaneously while exploiting similari-
ties/diﬀerences across tasks (Caruana, 1997). MTL is commonly used in applications that warrant strong
privacy guarantees. For example, MTL has been used in healthcare, as a way to learn over diverse populations
or between multiple institutions (Baytas et al., 2016; Suresh et al., 2018; Harutyunyan et al., 2019); in
ﬁnancial forecasting, to combine knowledge from multiple indicators or across organizations (Ghosn & Bengio,
1997; Cheng et al., 2020); and in IoT computing, as an approach for personalized federated learning (Smith
et al., 2017; Hanzely & Richtárik, 2020; Hanzely et al., 2020; Ghosh et al., 2020; Sattler et al., 2020; Deng
et al., 2020; Mansour et al., 2020). While MTL can signiﬁcantly improve accuracy when learning in these
applications, there is a dearth of work studying the privacy implications of multi-task learning.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
0.00.10.20.30.40.50.60.7T est accuracyBP-MTL (our approach)
Global
DP-MTL
Figure 1: Naively using current client-level
DP formulations with MTL results in models
that are no better than a random guess.In this work, we develop and theoretically analyze methods for
MTL with formal privacy guarantees. Motivated by applications
in federated learning, we aim to provide client-level privacy ,
where each task corresponds to a client/user/data silo, and the
goal isto protectsensitiveinformation ineachtask’s data (McMa-
han et al., 2018). We focus on ensuring diﬀerential privacy (DP)
(Dwork et al., 2006), which (informally) requires an algorithm’s
output to be insensitive to changes in any single entity’s data.
For MTL, where a separate model is generated for each client,
using client-level DP directly would require the entire set of
predictive models across all tasks to be insensitive to changes in the private data of any single task. This
1Published in Transactions on Machine Learning Research (04/2023)
requirement is too stringent for most applications, as it implies that the predictive model for task kmust have
little dependence on the training data for task k, thus preventing the usefulness of the model (see Figures 1).
To address this issue, we leverage a privacy model known as the billboard model (Hsu et al., 2016b). The
billboard model is built using: (1) a global signal from a diﬀerentially private process that is public to all the
clients, and (2) every client i’s private data. Unlike DP, the billboard model ensures that for each task k, the
set of output predictive models for all other tasks exceptkis insensitive to k’s private data.1Therefore, it
allows the predictive model for task kto depend on k’s private data, helping to preserve each task’s utility.
In this work, we develop new learning algorithms for MTL that satisfy the billboard model with rigorous
privacy and utility guarantees. Speciﬁcally, we propose Private Mean-Regularized MTL, a simple framework
for learning multiple tasks while ensuring client-level privacy. We show that our method achieves (/epsilon1,δ)-billboard
privacy (BP) (Deﬁned in Section 3.2). Our scalable solver builds on FedAvg (McMahan et al., 2017), a
common method for communication-eﬃcient federated optimization. We analyze the convergence of our solver
on both nonconvex and convex objectives, demonstrating a tradeoﬀ between privacy and utility, and evaluate
this trade-oﬀ empirically on multiple federated learning benchmarks. We summarize our contributions below:
•We propose Private Mean-Regularized MTL, a simple MTL framework that provides client-level bill-
board privacy (BP) (Section 4). We prove that our method achieves (/epsilon1,δ)-BP.
•We analyze the convergence of our communication-eﬃcient solver on convex and nonconvex objectives.
Our convergence analysis extends to non-private settings with partial participation, which may be of
independent interest for problems in cross-device federated learning.
•Finally, we explore the performance of our approach on common federated learning benchmarks (Section 5).
Our results show that we can retain the accuracy beneﬁts of MTL in these settings relative to global
baselines while still providing meaningful privacy guarantees. Further, even in cases where the MTL and
global objectives achieve similar accuracy, we ﬁnd that privacy/utility beneﬁts exist when employing our
private MTL formulation compared to privately learning a single global model.
2 Background and Related Work
Multi-task learning. Multi-task learning considers jointly solving multiple related ML tasks. Our work
focuses on the general and widely-used formulation of multi-task relationship learning (Zhang & Yeung,
2010), as detailed in Section 3. This form of MTL is particularly useful in privacy-sensitive applications
where datasets are split among multiple heterogeneous entities (Baytas et al., 2016; Smith et al., 2017; Ghosn
& Bengio, 1997). In these cases, it is natural to view each data source (e.g., ﬁnancial institution, hospital,
mobile phone) as a separate ‘task’ that is learned in unison with the other tasks. This allows learning to
be performed jointly, but the models to be personalized to each data silo. For example, in the setting of
cross-device federated learning, MTL is commonly used to train a personalized model for each device in a
distributed network (Smith et al., 2017; Liu et al., 2017).
Federated learning. A motivation for our work is the application of federated learning (FL), in which the
goal is to collaboratively learn from a number of private data silos, such as remote devices or servers (McMahan
et al., 2017; Kairouz et al., 2019; Li et al., 2020a). To ensure client-level DP in FL, a common technique is to
learn one global model across the distributed data and then add noise to the aggregated model to suﬃciently
mask any speciﬁc client’s update (e.g., Kairouz et al., 2019; McMahan et al., 2018; Geyer et al., 2017; Levy
et al., 2021; Lowy & Razaviyayn, 2021; Lowy et al., 2022). However, a deﬁning characteristic of federated
learning is that the distributed data are likely to be heterogeneous, i.e., each client may generate data via
a distinct data distribution (Kairouz et al., 2019; Li et al., 2020a). To model the (possibly) varying data
distributions on each client, it is natural to instead consider learning a separate model for each client’s
local dataset. To this end, a number of recent works have explored multi-task learning as a way to improve
the accuracy of learning in federated networks (Smith et al., 2017; Hanzely & Richtárik, 2020; Hanzely
et al., 2020; Ghosh et al., 2020; Sattler et al., 2020; Deng et al., 2020; Mansour et al., 2020). Despite the
1This privacy guarantee is known as joint diﬀerential privacy (JDP) Kearns et al. (2014), and billboard privacy is a common
way to achieve JDP.
2Published in Transactions on Machine Learning Research (04/2023)
prevalence of multi-task federated learning, we are unaware of any work that has explored client-level privacy
for commonly-used multi-task relationship models (Section 3) in federated settings.
Diﬀerentially private MTL. Prior work in private MTL diﬀers from our own either in terms of the privacy
formulation or MTL objective. For example, Wu et al. (2020) explore a speciﬁc MTL setting where a shared
private feature representation is ﬁrst learned, followed by task-speciﬁc models. We instead study multi-task
relationship learning (Section 3), which is a general and widely-used MTL framework, particularly in federated
learning (Smith et al., 2017). While our work focuses on client-level privacy, there has been work on data-level
privacy for MTL, which aims to protect any single sample of local data rather than protecting the entire
local dataset. For example, Xie et al. (2017) propose a method for data-level privacy by modeling each
task as a sum of a public shared weight and a task-speciﬁc weight that is only updated locally, and Gupta
et al. (2016) study data-level privacy for a mean estimation MTL problem. Li et al. (2019) study multiple
notions of DP for meta-learning. Although similarly motivated by personalization, their framework does not
cover the multi-task setting, where there exists a separate model for each task. Hu et al. (2020) studied
example-level private multi-task learning but only their method is restricted to small scale convex task, which
is diﬀerent from our focus on client-level privacy. More closely related to our work, Jain et al. (2021) study a
personalization method that learns a private shared representation. Although they similarly leverage the
billboard model, their formulation cannot be applied to the general form of multi-task learning in this work.
Their results are also limited to the special case of linear regression, unlike the broad set of convex and
nonconvex objectives considered herein. Finally, Bietti et al. (2022) similarly propose a personalized federated
learning method using the billboard model. Although both methods train a global model shared across tasks,
the concrete algorithms are rather diﬀerent. The main algorithm of Bietti et al. (2022) builds on Federated
Residual Learning (Agarwal et al., 2020) while our main algorithm builds on mean-regularized multi-task
learning. We provide an in-depth discussion and empirical comparison to this method in Appendix A.8.
3 Multi-Task Learning Setup and Privacy Formulation
In this section, we ﬁrst formalize the multi-task learning objectives of interest (Section 3.1), and then discuss
our proposed privacy formulation (Section 3.2).
3.1 Problem Setup
Multi-task learning aims to improve generalization by jointly solving and exploiting relationships between
multiple tasks Caruana (1997); Ando & Zhang (2005). The classical setting of multi-task relationship
learning (Zhang & Yang, 2017; Zhang & Yeung, 2010) considers mdiﬀerent tasks with their own task-speciﬁc
data, learned jointly through the following objective:
min
W,Ω/braceleftBigg
F(W,Ω) =/braceleftBigg
1
mm/summationdisplay
k=1nk/summationdisplay
i=1lk(xi,wk) +R(W,Ω)/bracerightBigg/bracerightBigg
. (1)
Herewkis model for task k,{x1,...,xnk}is the local data for the kthtask,lk(·)is the empirical loss,
W= [w1;···;wm], and Ω∈Rm×mcharacterizes the relationship between every pair of tasks. A common
choice for setting the regularization term R(W,Ω)in prior works (Zhang & Yeung, 2010; Smith et al., 2017)
is:
R(W,Ω) =λ1tr(WΩWT),
where Ωcan be viewed as a covariance matrix, used to learn/encode positive, negative, or unrelated task
relationships. Inthispaper, wefocusprimarilyonthemean-regularizedmulti-tasklearningobjective(Evgeniou
& Pontil, 2004): a special case of (1) where Ω = ( Im×m−1
m1m1T
m)2is ﬁxed. Here Im×mis the identity
matrix of size m×mand1m∈Rmis the vector with all entries equal to 1. By picking λ1=λ
2, we can
rewrite Objective 1 as:
min
W/braceleftBigg
F(W) =/braceleftBigg
1
mm/summationdisplay
k=1/parenleftBigg
λ
2/bardblwk−¯w/bardbl2+nk/summationdisplay
i=1lk(xi,wk)/parenrightBigg/bracerightBigg/bracerightBigg
, (2)
where ¯wis the average of task-speciﬁc models, i.e., ¯w=1
m/summationtextm
i=1wk. Note that ¯wis shared across all tasks,
and eachwkis kept locally for task learner k. During optimization, each task learner ksolves:
3Published in Transactions on Machine Learning Research (04/2023)
min
wk/braceleftbigg
fk(wk; ¯w) =λ
2/bardblwk−¯w/bardbl2+nk/summationdisplay
i=1lk(xi,wk)/bracerightbigg
. (3)
Application to Federated Learning. In federated learning, where the goal is to learn over a set of m
clients in a privacy-preserving manner, initial approaches focused on learning a single global model across the
data McMahan et al. (2017). However, as data distributions may diﬀer from one client to another, MTL has
become a popular alternative that enables every client to collaborate and learn a separate, personalized
model of its own Smith et al. (2017). Speciﬁcally, in the case of mean-regularized MTL, each client solves
Equation 3 and utilizes wkas its ﬁnal personalized model. Unlike ﬁnetuning from a global model, MTL
itself learns a separate model for each client by solving Objective 1 in order to improve the generalization
performance (Zhang & Yang, 2017; Zhang & Yeung, 2010), which is not equivalent to simple ﬁnetuning
from a global model (see Section 5). Despite the prevalence of mean-regularized multi-task learning and its
recent use in applications such as federated learning with strong privacy motivations (e.g., Smith et al., 2017;
Hanzely & Richtárik, 2020; Hanzely et al., 2020; Dinh et al., 2020), we are unaware of prior work that has
formalized client-level diﬀerential privacy in the context of solving Objective 2.
3.2 Privacy Formulation
To consider privacy for MTL, we start by introducing the deﬁnition of diﬀerential privacy (DP) and then
discuss its generalization to joint diﬀerential privacy (JDP) . In the context of multi-task learning, each of the
mtask learners owns a private dataset Di∈Ui⊂U. We deﬁne D={D1,···,Dm}andD/prime={D/prime
1,···,D/prime
m},
and call two sets D,D/primeneighboring sets if they only diﬀer on the index i, i.e.,Dj=D/prime
jfor alljexcepti.
Deﬁnition 1 (Diﬀerential Privacy (DP) for MTL (Dwork et al., 2006)) .A randomized algorithm M:Um→
Rmis(/epsilon1,δ)-diﬀerentially private if for every pair of neighboring sets that only diﬀer in arbitrary index i:
D,D/prime∈Uand for every set of subsets of outputs S⊂R,
Pr(M(D)∈S)≤e/epsilon1Pr(M(D/prime)∈S) +δ. (4)
In the context of MTL, an algorithm outputs one model for every task. In this work we are interested in
studying client-level diﬀerential privacy , where the purpose is to protect one task’s data from leakage to any
other task McMahan et al. (2017). As mentioned previously and illustrated in Figure 2, since the output of
MTL is a collection of models , traditional client-level DP would require that all the models produced by an
MTL algorithm are insensitive to changes that happen in the private dataset of anysingle client/task.
Why can’t we apply traditional client-level DP? With the above deﬁnition in mind, note that DP
has a severe restriction: the model of any task learner must also be insensitive to changes in its own data ,
eﬀectively rendering each model useless. Although it is intuitive that this would result in unacceptable
performance, we verify it empirically in Figure 1. For a common federated learning benchmark (FEMNIST,
discussed in Section 5), we apply DPSGD on the joint model that concatenates all clients’ parameters. We
compare MTL with vanilla client-level DP relative to training a global model with client-level DP and our
proposed MTL approach using BP (below). With the naive DP formulation, MTL is signiﬁcantly worse than
the other approaches—improving only marginally upon random guessing.
Billboard Privacy. To overcome this limitation of traditional DP, motivated by the billboard model (Hsu
et al., 2016b), we propose billboard privacy (BP) , a relaxed notion of DP, to formalize the client-level privacy
guarantees for MTL algorithm. We provide the formal deﬁnition below.
Deﬁnition 2 (Billboard Privacy (BP) (Hsu et al., 2016b)) .Consider any set of functions: fi:Ui×R→R/prime
andg:U→R, a randomized algorithm M:Um→Rmrepresented as [fi(ΠiD,g(D))]mis(/epsilon1,δ)-billboard
private if for every i, for every pair of neighboring datasets that only diﬀer in index i:D,D/prime∈Umand for
every set of subsets of outputs S⊂Rm,
Pr(M(D)−i∈S)≤e/epsilon1Pr(M(D/prime)−i∈S) +δ, (5)
where ΠiDisD’s projection onto the i-th index andM(D)−irepresents the vector M(D)with thei-th entry
removed.
4Published in Transactions on Machine Learning Research (04/2023)
BP allows the predictive model for task kto depend on the private data of k, while still providing a strong
guarantee. BP provides m−1-out-of-mprivacy under Shamir’s scheme of secret sharing (Shamir, 1979):
even if all the other m−1collude and share their information, they still will not be able to learn much about
the private data in the task k. BP has mostly been used in applications related to mechanism design (Hsu
et al., 2016a; Kannan et al., 2015; Hsu et al., 2016b). Although it is a natural choice for achieving client-level
privacy in MTL, we are unaware of any work that studies the general MTL formulations considered herein
subject to BP. We also note that we can naturally connect billboard privacy to standard diﬀerential privacy.
Informally, if gis(/epsilon1,δ)-diﬀerentially private, then [fi(ΠiD,g(D))]mis(/epsilon1,δ)-billboard private for arbitrary
{fi}i∈[1:m]by deﬁnition of billboard privacy. In other words, if we take the output of a diﬀerentially private
process and run some algorithm on top of that locally for each task learner withoutcommunicating to the
global learner or other task learners, this whole process can be shown to be BP.
Remark (Generality of Privacy Formulation). Finally, note that our privacy formulation itself is not
limited to the multi-task relationship learning framework. For any form of multi-task learning where each
task-speciﬁc model is obtained by training a combination of global component and local component(e.g. Li
et al. (2021)), we can provide a BP guarantee for the MTL training process by using a diﬀerentially private
global component.
Connection to Joint Diﬀerential Privacy. Compared to BP, a weaker yet more general privacy formula-
tion is known as joint diﬀerential privacy (JDP) (Kearns et al., 2014) deﬁned formally below. By deﬁnition,
(/epsilon1,δ)-BP implies (/epsilon1,δ)-JDP. Diﬀerent from billboard privacy where a global diﬀerentially private message
g(D)is needed, JDP does not need any global information shared across all the clients. Compared to JDP,
achieving BP is a harder problem since it requires learning a shared message (in the case of MTL, a private
global model) that could be used for all mclients while a JDP mechanism does not necessarily produce such
message.
Deﬁnition3 (JointDiﬀerentialPrivacy(JDP)(Kearnsetal.,2014)) .A randomized algorithm M:Um→Rm
is(/epsilon1,δ)-joint diﬀerentially private if for every i, for every pair of neighboring datasets that only diﬀer in
indexi:D,D/prime∈Umand for every set of subsets of outputs S⊂Rm,
Pr(M(D)−i∈S)≤e/epsilon1Pr(M(D/prime)−i∈S) +δ, (6)
whereM(D)−irepresents the vector M(D)with thei-th entry removed.
4 PMTL: Private Multi-Task Learning
We now present PMTL, a method for joint diﬀerentially-private MTL (Section 4.1). We provide both a
formal privacy guarantee (Section 4.2) and utility guarantee (Section 4.3) for our approach.
4.1 Algorithm
We summarize our solver for private multi-task learning in Algorithm 1. Our method is based oﬀ of
FedAvg (McMahan et al., 2017), a communication-eﬃcient method widely used in federated learning. FedAvg
alternates between two steps: (i) each task learner selected at one communication round solves its own local
objective by running stochastic gradient descent for Eiterations and sending the updated model to the
global learner; (ii) the global learner aggregates the local updates and broadcasts the aggregated mean. By
performing local updating in this manner, FedAvg has been shown to empirically reduce the total number of
communication rounds needed for convergence in federated settings relative to baselines such as mini-batch
FedSGD (McMahan et al., 2017). Our private MTL algorithm diﬀers from FedAvg in that: (i) instead
of learning a single global model, all task learners collaboratively learn separate, personalized models for
each task; (ii) each task learner solves the local objective with the mean-regularization term; (iii) individual
model updates are clipped and random Gaussian noise is added to the aggregated model updates to ensure
client-level privacy.
In this work, we focus on providing global client-level billboard privacy. Therefore, we assume that we have
access to a trusted global learner while aggregating updates from each task, i.e., it is safe for some global
entity to observe/collect the individual model updates from each task. This is a standard assumption in
federated learning, where access to a trusted server is assumed in order to collect client updates (Kairouz
et al., 2019). To add an additional layer on security, our method has a natural extension to support secure
5Published in Transactions on Machine Learning Research (04/2023)
Algorithm 1 PMTL: Private Mean-Regularized MTL
1:Input:m,T,λ,η,{w0
1,···,w0
m},/tildewidew0=1
m/summationtextm
k=1w0
k
2:fort= 0,···,T−1do
3:Global Learner randomly selects a set of tasks Stand broadcasts the mean weight /tildewidewt
4:fork∈Stin parallel do
5:Each client updates its weight wkforEiterations,okis the last iteration task kis selected
6:Each client sends gt+1
k=wt+1
k−wt
kback to the global learner.
7:end for
8:Global Learner computes a noisy aggregator of the weights
/tildewidewt+1=/tildewidewt+1
|St|/summationdisplay
k∈Stgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg
+N(0,σ2Id×d)
9:end for
10:Outputw1,···,wmas diﬀerentially private personalized models
11:ClientUpdate (w)
12:forj= 0,···,E−1do
13:Task learner performs SGD locally
w=w−η(∇wlk(w) +λ(w−/tildewidewt))
14:end for
aggregation, a common cryptographic primitive used in federated learning (Bonawitz et al., 2016; 2019;
Kairouz et al., 2021). While SA/MPC are important to ensure secrecy while communicating model updates,
neither trusted server assumption nor secure aggregation protects a client’s private data from leakage to
other clients by observing the model output. Thus, our algorithm focuses on addressing privacy concern for
model personalization in federated learning.
There are several ways to overcome this privacy risk and thus achieve (/epsilon1,δ)-diﬀerential privacy. In this paper,
we use the Gaussian Mechanism (Dwork & Roth, 2014) during global aggregation as a simple yet eﬀective
method, highlighted in redin line 8 of Algorithm 1. In this case, each client receives a noisy aggregated
global model, making it diﬃcult for any task to leak private information to the others. To apply the Gaussian
mechanism, we need to bound the /lscript2-sensitivity of each local model update that is communicated to lie in
B={∆w|/bardbl∆w/bardbl2≤γ}, as highlighted in bluein line 8 of Algorithm 1. Hence, at each communication round,
the global learner receives the model updates from each clients, and clips the model updates to Bbefore
aggregation. Note that diﬀerent from DPSGD (Abadi et al., 2016), when we solve the local objective for
each selected task at each communication round, our algorithm doesn’t clip and perturb the gradient used
to update the task-speciﬁc model. Instead, since the purpose is to protect task or client-level privacy in
multi-task learning, we perform standard SGD locally for each task and only clip and perturb the model
update that is sent to the global learner. We formalize the privacy guarantee of Algorithm 1 in Section 4.2.
4.2 Privacy Analysis
We now rigorously explore the privacy guarantee provided by Algorithm 1. In our optimization scheme, for
each taskk, at the end of each communication round, a shared global model is received. After that the task
speciﬁc model is updated by optimizing the local objective. We formalize this local task learning process as
hk:Dk×W→W . Here we simply assume W⊂Rdis closed. Deﬁne the mechanism for communication
roundtto be
Mt({Di},{hi(·)},/tildewidewt,σ) =/tildewidewt+1
|St|/summationdisplay
k∈Sthk(Dk,/tildewidewt) +βt, (7)
whereβt∼N (0,σ2Id×d). Note thatMtcharacterizes a Sampled Gaussian Mechanism given /tildewidewtas a
ﬁxed model rather than the output of a composition of Mjforj < t. To analyze the privacy guarantee
6Published in Transactions on Machine Learning Research (04/2023)
of Algorithm 1 over Tcommunication rounds, we deﬁne the composition of M1toMTrecursively as
M1:T=MT({Di},{hi(·)},MT−1,σ).
Theorem 1. Assume|St|=qfor alltand the total number of communication rounds is T. There exists
constantsc1,c2such that for any /epsilon1<c 1q2
m2T, the mechanismM1:Tis(/epsilon1,δ)client-level diﬀerentially private
for anyδ>0if we choose σ≥c2γ√
Tlog(1/δ)
/epsilon1m. Whenq=m,M1:Tis(/epsilon1,δ)client-level diﬀerentially private
if we choose σ=4γ√
Tlog(1/δ)
/epsilon1m.
Theorem 1 provides a provable privacy guarantee on the learned model. When all tasks participate in every
communication round, i.e. q=m, the global aggregation step in Algorithm 1 reduce to applying Gaussian
Mechanism without sampling rather than Sampled Gaussian Mechanism on the average model updates. We
provide a detailed proof of Theorem 1 in Appendix A.1. Note in particular that Theorem 1 doesn’t rely on
how task learners optimize their local objective. Hence, Theorem 1 is not limited to Algorithm 1 and could
be generalized to other local objectives and other global aggregation methods that produce a single model
aggregate.
Now we show that Algorithm 1, which outputs mseparate models, satisﬁes BP. Given /tildewidewtfor anyt≤T, we
formally deﬁne the process that each task learner koptimize its local objective to be h/prime
k:Dk×W→W .
Note thath/prime
kis not restricted to be hkand could represent the optimization process for any local objective.
In our MR-MTL problem, the average model broadcast by the global learner at every communication round
is the output of a diﬀerentially private learning process. Task learners then individually train their models on
the respective private data to obtain personalized models. By deﬁnition of billboard privacy in Section 3.2,
we are able to show that Algorithm 1 satisﬁes BP:
Corollary 2. There exists constants c1,c2, for any 0< /epsilon1 < c 1q2
m2Tandδ > 0, letσ≥c2γ√
Tlog(1/δ)
/epsilon1m.
Algorithm 1 that outputs h/prime
k(Dk,M1:T)for each task is (/epsilon1,δ)-billboard private.
From Theorem 2, for any ﬁxed δ, the more tasks involved in the learning process, the smaller σwe need in
order to keep the privacy parameter /epsilon1the same. In other words, less noise is required to keep the task-speciﬁc
data private. When we have inﬁnitely many tasks ( m→∞), we haveσ→0, in which case only a negligible
amount of noise is needed for the model aggregates to make the global model private to all tasks. We provide
a detailed proof in Appendix A.1.
Remark (Generality of Corollary 2). Note that the privacy guarantee provided by Corollary 2 is not
limited to mean-regularized MTL. For any form of multi-task relationship learning with ﬁxed relationship
matrix Ω, as long as we ﬁx the /lscript2-sensitivity of model updates and the noise scale of the Gaussian mechanism
applied to the statistics broadcast to all task learners, the privacy guarantee induced by this aggregation
step is ﬁxed, regardless of the local objective being optimized. For example, as a natural extension of
mean-regularized MTL, consider the case where task learners are partitioned into ﬁxed clusters and optimize
the mean-regularized MTL objective within each cluster, as in Evgeniou et al. (2005). In this scenario,
Theorem 2 directly applies to the algorithm run on each cluster.
4.3 Convergence Analysis
As discussed in Section 3, we are interested in the following task-speciﬁc objective:
fk(wk;/tildewidew) =lk(wk) +λ
2/bardblwk−/tildewidew/bardbl2
2 (8)
where/tildewidewis an estimate for the average model w;lk(wk) =/summationtextnk
i=1lk(xi,wk)is the empirical loss for task k;
wk∈Rd.
Here, we analyze the convergence behavior in the setting where a set Stofqtasks participate in the
optimization process at every communication round. Further, we assume the total number of communication
roundTis divisible by the number of local optimization steps E:T= 0 modE. We present the following
convergence result:
Theorem 3 (Convergence under nonconvex loss (Informal)) .Letfkbe(L+λ)-smooth. Assume fk
isG-Lipschitz in /lscript2norm such that γ≥G. Further let f∗
k=minw,¯wfk(w;¯w),p=q
m, andB=
7Published in Transactions on Machine Learning Research (04/2023)
0 200 400 600
Communication rounds5.05.56.0Losses
051015
Privacy parameter()
=0.01, =0.5
(a) StackOverﬂow tag prediction
0 200 400 600 800 1000
Communication rounds01234Losses
0.02.55.07.510.012.5
Privacy parameter()
=0.01, =0.2
 (b) FEMNIST
0 200 400 600 800 1000
Communication rounds01234Losses
051015
Privacy parameter()
=0.02, =1
 (c) CelebA
Figure 2: Loss and privacy parameter vs. communication rounds for PMTL. The blue line shows the change of /epsilon1in
terms of number of communication rounds during training. The orange line shows the average training loss.
maxtmaxkfk(wt
k;/tildewidewt). If we use a ﬁxed learning rate ηt=η=1
pL+(p+1
p)λ, Algorithm 1 satisﬁes:
1
mTT−1/summationdisplay
t=0m/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2≤O/parenleftbiggλ
T/parenrightbigg
+O/parenleftbiggλB
E/parenrightbigg
+O/parenleftbiggdλσ2
E/parenrightbigg
. (9)
Letσchosen as we set in Theorem 2. Take T=O/parenleftBig
m
λdγ2/parenrightBig
, the right hand side is bounded by
1
mTT−1/summationdisplay
t=0m/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2≤O/parenleftbiggdγ2
m/parenrightbigg
+O/parenleftbiggλB
E/parenrightbigg
+O/parenleftbigg1
mE/parenrightbigglog(1/δ)
/epsilon12. (10)
We provide formal statement and proof of Theorem 3 in Appendix A.2. The upper bound in Equation 9
consists of two parts: error induced by the gradient descent algorithm and error induced by the Gaussian
Mechanism. When σ= 0, Algorithm 1 recovers a non-private MR-MTL solver.
Corollary 4. Whenσ= 0, Algorithm 1 with (L+λ)-smooth and nonconvex fksatisﬁes
1
mTT−1/summationdisplay
t=0m/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2≤O/parenleftbiggλ
T/parenrightbigg
+O/parenleftbiggλB
E/parenrightbigg
. (11)
By Theorem 2, given ﬁxed /epsilon1,σ2grows linearly with respect to T. Hence, given the same privacy guarantee,
larger noise is required if the algorithm is run for more communication rounds. Note that in Theorem 3, the
upper bound consists of O(1
m/epsilon12), which means when there are more tasks, the upper bound becomes smaller
while the privacy parameter remains the same. On the other hand, Theorem 3 also shows a privacy-utility
tradeoﬀ using our Algorithm 1: the upper bound grows inversely proportional to the privacy parameter /epsilon1. We
also provide a convergence analysis of Algorithm 1 with strongly-convex losses in Theorem 5 below (formal
statement and proof in Appendix A.3).
Theorem 5 (Convergence under strongly-convex loss (Informal)) .Letfkbe(L+λ)-smooth and (µ+λ)-
strongly convex. Assume γ≥maxk,t/bardbl∇wt
kfk(wt
k;/tildewidewt)/bardbl2. Letw∗
k=arg minwfk(w;¯w∗)andp=q
m. If we set
ηt=cp
Lp2+λp2−2λfor some constant csuch that1
2≤ηp(c−2)(µ+λ)≤1, we have:
1
mm/summationdisplay
k=1fk(wt
k;/tildewidewt)−fk(w∗
k;/tildewidew∗)≤O/parenleftbigg1
2Tm/parenrightbigg
+O/parenleftbigg2EB
2E−1/parenrightbigg
+O/parenleftbigg2Edλσ2
2E−1/parenrightbigg
. (12)
Letσbe chosen as in Theorem 2, then there exists T=O/parenleftBig
m
λdγ2/parenrightBig
such that
1
mm/summationdisplay
k=1fk(wt
k;/tildewidewt)−fk(w∗
k;/tildewidew∗)≤O/parenleftbigg1
2Tm/parenrightbigg
+O/parenleftbigg2EB
2E−1/parenrightbigg
+O/parenleftbigg2E
m(2E−1)/parenrightbigglog(1/δ)
/epsilon12(13)
8Published in Transactions on Machine Learning Research (04/2023)
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
0.000.050.100.150.200.250.30Test accuracyMTL
Global
MTL non-private
Global non-private
(a) StackOverﬂow tag prediction
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
0.00.10.20.30.40.50.60.70.8Test accuracy (b) FEMNIST
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
0.50.60.70.80.9Test accuracy (c) CelebA
Figure 3: Comparison of training PMTL vs. a private global model. PMTL is able to retain advantages over global
approaches in private settings. In addition, even in settings where the non-private MTL and global baselines are
similar (e.g., FEMNIST, CelebA), there exist utility beneﬁts at all levels of /epsilon1when using PMTL.
As with Corollary 4, we recover the bound of the non-private mean-regularized MTL solver for σ=0. We
provide convergence under convex loss in non-private scenario in Appendix A.3.
Convergence to a neighborhood of the optimal. It is worth noting that for both convex and non-
convex case, given the nature of the mean-regularized MTL objective, the local objective will only converge
to within a neighborhood of the optimal. Consider the following simple mean-estimation problem as an
example. Assume that we have mdiﬀerent clients/tasks, each with local data xiand local model wi. The
mean-regularized MTL objective for this problem would be1
m/summationtext
i(xi−wi)2+1
m/summationtext
i(wi−¯w)2, which is greater
than1
2m/summationtext
i(xi−wi+wi−¯w)2=1
2m/summationtext
i(xi−¯w)2. Note that this lower bound neither converges to 0 as
¯wchanges over time nor diminishes with increasing m. This result is therefore expected/standard, and is
in line with other previous works that study the same objective but with diﬀerent solvers, where similar
dependencies on ηandBcan be seen (Hanzely & Richtárik, 2020).
5 Experiments
We empirically evaluate our private MTL solver on common federated learning benchmarks Caldas et al.
(2018). We ﬁrst demonstrate the superior privacy-utility trade-oﬀ that exists when training our private MTL
method compared with training a single global model (Section 5.2). We also compare our method with simple
ﬁnetuning—exploring the results of performing local ﬁnetuning after learning an MTL objective vs. a global
objective (Section 5.3). Our code is publicly available at: https://github.com/s-huu/PMTL
5.1 Setup
For all experiments, we evaluate the test accuracy and privacy parameter of our private MTL solver given a
ﬁxed clipping bound γ, variance of Gaussian noise σ2, and communication rounds T. All experiments are
performed on common federated learning benchmarks as a natural application of multi-task learning. We
provide a detailed description of datasets and models in Appendix A.4. Each dataset is naturally partitioned
amongmdiﬀerent clients. Under such a scenario, each client can be viewed as a task and the data that a
client generates is only visible locally.
FEMNIST /epsilon1= 0.1 /epsilon1= 0.8 /epsilon1= 2.0 /epsilon1=∞
MTL Global MTL Global MTL Global MTL Global
Vanilla Finetuning 0.645±0.013 0.606±0.017 0.640±0.016 0.648±0.017 0.677±0.008 0.653±0.010 0.832±0.005 0.812±0.009
Mean-regularization 0.608±0.011 0.581±0.011 0.605±0.008 0.574±0.006 0.656±0.009 0.633±0.003 0.826±0.011 0.839±0.006
Symmetrized KL 0.486±0.012 0.348±0.005 0.584±0.012 0.481±0.016 0.662±0.016 0.565±0.019 0.839±0.006 0.829±0.015
EWC 0.663±0.002 0.556±0.001 0.595±0.004 0.607±0.007 0.681±0.002 0.666±0.001 0.837±0.001 0.823±0.005
Table 1: Comparison of PMTL vs. a private global model with diﬀerent local ﬁnetuning methods. /epsilon1=∞
corresponds to no noise and clipping, i.e., training non-privately. The higher accuracy between MTL and
Global given the same /epsilon1and ﬁnetuning method is bolded.
5.2 Privacy-Utility Trade-oﬀ of PMTL
We ﬁrst explore the training loss ( orange) and privacy parameter /epsilon1(blue) as a function of communication
rounds across three datasets (Figure 2). Speciﬁcally, we evaluate the average loss for all the tasks and /epsilon1given
9Published in Transactions on Machine Learning Research (04/2023)
a ﬁxedδafter each round, where δis set to be1
mfor all experiments. In general, for a ﬁxed clipping bound γ
andσ, we see that the method converges fairly quickly with respect to the resulting privacy, but that privacy
guarantees may be sacriﬁced in order to achieve very small losses.
To put these results in context, we also compare the test performance of our PMTL solver with that of
training a global model. In particular, we use FedAvg (McMahan et al., 2017) to train a global model. At each
communication round, clients solve their local objective individually. While aggregating the model updates,
the global learner applies Gaussian Mechanism and sends the noisy aggregation back to the clients. As a
result, private FedAvg diﬀers from our PMTL solver in the following two places: (i) the MTL objective solved
locally by each task learner has a mean-regularized term; (ii) the MTL method evaluates on one task-speciﬁc
model for every task while the global method evaluates all tasks on one global model. For each dataset, we
select privacy parameter /epsilon1∈[0.05,0.1,0.2,0.4,0.8,1.6,2.0,4.0]. For each /epsilon1, we select the γ,σ, andTthat
result in the best validation accuracy for a given /epsilon1and record the test accuracy. A detailed description of
hyperparameters is listed in Appendix A.5. We plot the test accuracy with respect to the highest validation
accuracy given one /epsilon1for both private MTL model and private global model. Results are shown in Figure 3.
In all three datasets, our private MTL solver achieves higher test accuracy compared with training a private
global model with FedAvg given the same /epsilon1. Moreover, the proposed mean regularized MTL solver is able to
retain an advantage over global model even with noisy aggregation. In particular, for small /epsilon1<1, adding
random Gaussian noise during global aggregation ampliﬁes the test accuracy diﬀerence between our MTL
solver and FedAvg. Under the StackOverﬂow task, both methods obtain test accuracy close to the non private
baseline for large /epsilon1. To demonstrate that applying private MTL has an advantage over private global training
more generally, we also compared our PMTL method with private FedProx (Li et al., 2020b). The results
(which mirror Figure 3) are in Appendix A.6.
5.3 PMTL with local ﬁnetuning
Finally, in federated learning, previous works have shown local ﬁnetuning with diﬀerent objectives is helpful
for improving utility while training a diﬀerentially private global model (Yu et al., 2020). In this section,
after obtaining a private global model, we explore locally ﬁnetuning the task speciﬁc models by optimizing
diﬀerent local objective functions. In particular, we use common objectives which (i) naively optimize the
local empirical risk (Vanilla Finetuning), or (ii) encourage minimizing the distance between local and global
model under diﬀerent distance metrics (Mean-regularization, Symmetrized KL, EWC (Kirkpatrick et al.,
2017; Yu et al., 2020)). The results are listed in Table 1. When /epsilon1=∞(the non-private setting), global with
mean-regularization ﬁnetuning outperforms all MTL+ﬁnetuning methods. However, when we add privacy to
both methods, private MTL+ﬁnetuning has an advantage over global with ﬁnetuning on diﬀerent ﬁnetuning
objectives. In some cases, e.g. using Symmetrized KL, the test accuracy gap between private MTL+ﬁntuning
and private global+ﬁnetuning is ampliﬁed when /epsilon1is small compared to the case where no privacy is added
during training. We also compare our PMTL+ﬁnetuning with training pure local model in Appendix A.7.
6 Conclusion and Future work
In this work, we deﬁned notions of client-level diﬀerential privacy for multi-task learning and proposed
a simple method for private mean-regularized MTL. Theoretically, we provided both privacy and utility
guarantees for our approach. Empirically, we showed that private MTL retains advantages over training a
private global model on common federated learning benchmarks. In future work, we are interested in building
on our results to explore privacy for more general forms of MTL, e.g., the family of objectives in (1) with
arbitrary Ω, as well as studying how client-level privacy relates to issues of fairness in multi-task settings.
7 Acknowledgements
ZSW was supported in part by the NSF FAI Award #1939606, NSF Award #2120667, a Google Faculty
Research Award, a J.P. Morgan Faculty Award, a Meta Research Award, and a Cisco research grant.
10Published in Transactions on Machine Learning Research (04/2023)
References
Tensorﬂow federated: Machine learning on decentralized data. URL https://www.tensorflow.org/
federated .
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with diﬀerential privacy. In ACM SIGSAC Conference on Computer and Communications
Security, 2016.
Alekh Agarwal, John Langford, and Chen-Yu Wei. Federated residual learning. arXiv preprint
arXiv:2003.12880 , 2020.
Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple tasks and
unlabeled data. Journal of Machine Learning Research , 6:1817–1853, 2005.
Inci M Baytas, Ming Yan, Anil K Jain, and Jiayu Zhou. Asynchronous multi-task learning. In International
Conference on Data Mining (ICDM) , 2016.
Alberto Bietti, Chen-Yu Wei, Miroslav Dudik, John Langford, and Steven Wu. Personalization improves
privacy-accuracy tradeoﬀs in federated learning. In Proceedings of the 39th International Conference on
Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp. 1945–1962. PMLR, 17–23
Jul 2022.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated learning on
user-held data. arXiv preprint arXiv:1611.04482 , 2016.
Keith Bonawitz, Fariborz Salehi, Jakub Konečný, Brendan McMahan, and Marco Gruteser. Federated
learning with autotuned communication-eﬃcient secure aggregation. In 2019 53rd Asilomar Conference on
Signals, Systems, and Computers , 2019.
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konečn` y, H Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. Leaf: A benchmark for federated settings, https://leaf.cmu.edu/ .arXiv preprint
arXiv:1812.01097 , 2018.
Rich Caruana. Multitask learning. Machine Learning , 28:41–75, 1997.
Yong Cheng, Yang Liu, Tianjian Chen, and Qiang Yang. Federated learning for privacy-preserving ai.
Communications of the ACM , 63(12), 2020.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to
handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN) , pp. 2921–2926,
2017.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning.
arXiv preprint arXiv:2003.13461 , 2020.
Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau
envelopes. In Advances in Neural Information Processing Systems , 2020.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of diﬀerential privacy. Foundations and Trends
in Theoretical Computer Science , 9(3-4):211–407, 2014.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Conference on Theory of Cryptography (TCC) , 2006.
Theodoros Evgeniou and Massimiliano Pontil. Regularized multi-task learning. In Conference on Knowledge
Discovery and Data Mining , 2004.
Theodoros Evgeniou, Charles A Micchelli, Massimiliano Pontil, and John Shawe-Taylor. Learning multiple
tasks with kernel methods. Journal of Machine Learning Research , 6(4), 2005.
11Published in Transactions on Machine Learning Research (04/2023)
Robin C Geyer, Tassilo Klein, and Moin Nabi. Diﬀerentially private federated learning: A client level
perspective. arXiv preprint arXiv:1712.07557 , 2017.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An eﬃcient framework for clustered
federated learning. In Advances in Neural Information Processing Systems , 2020.
Joumana Ghosn and Yoshua Bengio. Multi-task learning for stock selection. In Advances in Neural Information
Processing Systems , 1997.
Sunil Kumar Gupta, Santu Rana, and Svetha Venkatesh. Diﬀerentially private multi-task learning. In
Paciﬁc-Asia Workshop on Intelligence and Security Informatics , pp. 101–113. Springer, 2016.
Filip Hanzely and Peter Richtárik. Federated learning of a mixture of global and local models. arXiv preprint
arXiv:2002.05516 , 2020.
Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter Richtarik. Lower bounds and optimal algorithms
for personalized federated learning. Advances in Neural Information Processing Systems , 2020.
Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan. Multitask
learning and benchmarking with clinical time series data. Scientiﬁc data , 6(1):1–18, 2019.
Justin Hsu, Zhiyi Huang, Aaron Roth, Tim Roughgarden, and Zhiwei Steven Wu. Private matchings and
allocations. SIAM Journal of Computing , 45(6), 2016a.
Justin Hsu, Zhiyi Huang, Aaron Roth, and Zhiwei Steven Wu. Jointly private convex programming. In
Symposium on Discrete Algorithms, SODA , 2016b.
Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. Personalized federated learning with
diﬀerential privacy. IEEE Internet of Things Journal , 7(10):9530–9539, 2020.
Prateek Jain, John Rush, Adam Smith, Shuang Song, and Abhradeep Guha Thakurta. Diﬀerentially private
model personalization. In Neural Information Processing Systems , 2021.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems
in federated learning. arXiv preprint arXiv:1912.04977 , 2019.
Peter Kairouz, Ziyu Liu, and Thomas Steinke. The distributed discrete gaussian mechanism for federated
learning with secure aggregation. In International Conference on Machine Learning , pp. 5201–5212. PMLR,
2021.
Sampath Kannan, Jamie Morgenstern, Aaron Roth, and Zhiwei Steven Wu. Approximately stable, school
optimal, and student-truthful many-to-one matchings (via diﬀerential privacy). In Symposium on Discrete
Algorithms, SODA , 2015.
Michael J. Kearns, Mallesh M. Pai, Aaron Roth, and Jonathan R. Ullman. Mechanism design in large
games: incentives and privacy. In Moni Naor (ed.), Innovations in Theoretical Computer Science, ITCS’14,
Princeton, NJ, USA, January 12-14, 2014 , pp. 403–410. ACM, 2014.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences , 114(13):3521–3526, 2017.
Yassine Laguel, Krishna Pillutla, Jérôme Malick, and Zaid Harchaoui. A superquantile approach to federated
learning with heterogeneous devices. In 55th Annual Conference on Information Sciences and Systems,
CISS 2021, Baltimore, MD, USA, March 24-26, 2021 , pp. 1–6. IEEE, 2021.
Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, and Ananda Theertha
Suresh. Learning with user-level privacy. Advances in Neural Information Processing Systems , 34, 2021.
12Published in Transactions on Machine Learning Research (04/2023)
Jeﬀrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Diﬀerentially private meta-learning. In
International Conference on Learning Representations , 2019.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods,
and future directions. IEEE Signal Processing Magazine , 37(3):50–60, 2020a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. In Conference on Machine Learning and Systems , 2020b.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In International Conference on Machine Learning , pp. 6357–6368. PMLR, 2021.
Sulin Liu, Sinno Jialin Pan, and Qirong Ho. Distributed multi-task relationship learning. In International
Conference on Knowledge Discovery and Data Mining (ICDM) , pp. 937–946, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
International Conference on Computer Vision , 2015.
Andrew Lowy and Meisam Razaviyayn. Private federated learning without a trusted server: Optimal
algorithms for convex losses. arXiv preprint arXiv:2106.09779 , 2021.
Andrew Lowy, Ali Ghafelebashi, and Meisam Razaviyayn. Private non-convex federated learning without a
trusted server. arXiv preprint arXiv:2203.06735 , 2022.
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization
with applications to federated learning. arXiv preprint arXiv:2002.10619 , 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-
eﬃcient learning of deep networks from decentralized data. In Artiﬁcial Intelligence and Statistics , pp.
1273–1282. PMLR, 2017.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning diﬀerentially private recurrent
language models. In International Conference on Learning Representations , 2018.
Ilya Mironov. Rényi diﬀerential privacy. In 2017 IEEE 30th Computer Security Foundations Symposium
(CSF), pp. 263–275. IEEE, 2017.
Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. Clustered federated learning: Model-agnostic
distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and
Learning Systems , 2020.
Adi Shamir. How to share a secret. Commun. ACM , 22(11):612–613, nov 1979. ISSN 0001-0782.
Virginia Smith, Chaokai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task learning. In
Advances in Neural Information Processing Systems , 2017.
Harini Suresh, Jen J Gong, and John V Guttag. Learning tasks for multitask learning: Heterogenous patient
populations in the icu. In International Conference on Knowledge Discovery & Data Mining , 2018.
Huiwen Wu, Cen Chen, and Li Wang. A Theoretical Perspective on Diﬀerentially Private Federated Multi-task
Learning. arXiv e-prints , art. arXiv:2011.07179, November 2020.
Liyang Xie, Inci M Baytas, Kaixiang Lin, and Jiayu Zhou. Privacy-preserving distributed multi-task learning
with asynchronous updates. In International Conference on Knowledge Discovery and Data Mining , 2017.
Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adaptation. arXiv
preprint arXiv:2002.04758 , 2020.
Yu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114 , 2017.
Yu Zhang and Dit-Yan Yeung. A convex formulation for learning task relationships in multi-task learning. In
Conference on Uncertainty in Artiﬁcial Intelligence , 2010.
13Published in Transactions on Machine Learning Research (04/2023)
A Appendix
A.1 Privacy Analysis: Proof for Theorem 1 and 2
In the proofs of Theorem 1 and 2 we follow the line of reasoning in Abadi et al. (2016), which analyzes the
privacy of DPSGD. We ﬁrst state the following lemma from Abadi et al. (2016).
Lemma 1. (Abadi et al., 2016, Theorem 1) There exists constants c1andc2such that given the sampling
probabilityp=q
mand the number of steps T, for any/epsilon1<c 1p2T, DPSGD is (/epsilon1,δ)-diﬀerentially private for
anyδ>0if we choose σ≥c2p√
Tlog(1/δ)
/epsilon1.
To prove Theorem 1, we also need the following deﬁnitions and lemmas.
Deﬁnition 4 (/lscript2-sensitivity) .Letf:U→Rdbe some arbitrary function, the /lscript2-sensitivity offis deﬁned as
∆2f= max
adjacentD,D/prime∈U/bardblf(D)−f(D/prime)/bardbl2 (14)
Deﬁnition 5 (Rényi Divergence) .(Mironov, 2017, Deﬁnition 3) Let P,Qbe two probability distribution over
the same probability space, and let p,qbe the respective probability density function. The Rényi Divergence
with ﬁnite order α/negationslash= 1is:
Dα(P/bardblQ) =1
α−1ln/integraldisplay
Xq(x)/parenleftbiggp(x)
q(x)/parenrightbiggα
dx (15)
Deﬁnition 6 ((α,/epsilon1)-Rényi Diﬀerential Privacy) .(Mironov, 2017, Deﬁnition 4) A randomized mechanism
f:D→Ris said to have (α,/epsilon1)-Rényi Diﬀerential Privacy if for all adjacent D,D/prime∈Dit holds that:
Dα(f(D)/bardblf(D/prime))≤/epsilon1. (16)
Lemma 2. (Mironov, 2017, Corollary 3) The Gaussian mechanism is (α,α(2(∆ 2f)2/σ2))-Renyi Diﬀeren-
tially Private.
Lemma 3. (Mironov, 2017, Proposition 3) If fis(α,/epsilon1)-RDP, then it is (/epsilon1+log(1/δ)
α−1,δ)-DP for all δ>0.
We begin by proving the ﬁrst part of Theorem 1, where q/negationslash=m.
Proof for Theorem 1: q/negationslash=m.Note that aggregation step in line 8 of Algorithm 1 can be rewritten as
/tildewidewt+1=/tildewidewt+1
|St|/summationdisplay
k∈Stgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg
+N(0,σ2Id×d) (17)
=/tildewidewt+1
q/summationdisplay
k∈Stgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg
+N/parenleftBigg
0,/parenleftbiggσ
γ/parenrightbigg2
γ2Id×d/parenrightBigg
(18)
=/tildewidewt+1
q/parenleftBigg/summationdisplay
k∈Stgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg
+N/parenleftBigg
0,/parenleftbiggqσ
γ/parenrightbigg2
γ2Id×d/parenrightBigg/parenrightBigg
. (19)
From here, we can directly apply Lemma 1 with σset to beqσ
γ. Hence, we conclude that when q/negationslash=m,
there exists constants c1andc2such that given the number of steps T, for any/epsilon1 < c 1q2
m2T,M1:Tis
(/epsilon1,δ)-diﬀerentially private for any δ>0if we choose σ≥c2γ√
Tlog(1/δ)
m/epsilon1.
This proof can extend to the case where q=m. In the remainder of this section, we provide a proof that
gives a more speciﬁc bound on the variance σ2in the case where q=m.
Proof for Theorem 1: q=m.DeﬁneHt:/producttextm
i=1Di×W→W as
Ht({Di},{hi(·)},/tildewidewt) =/tildewidewt+1
mm/summationdisplay
i=1ht
i(Di,/tildewidewt). (20)
14Published in Transactions on Machine Learning Research (04/2023)
As a result, we have Mt({Di},{hi(·)},/tildewidewt,σ) =Ht({Di},{hi(·)},/tildewidewt) +βt.
By Lemma 2,Mtis(α,2α(∆2Ht)2/dσ2)-Renyi Diﬀerentially Private. Note that
(∆2Ht)2= max
jmax
adjacentDj,D/prime
j∈Dj/vextenddouble/vextenddoubleHt({D1,···,Dj,···,Dm})−Ht({D1,···,D/prime
j,···,Dm})/vextenddouble/vextenddouble2(21)
= max
jmax
adjacentDj,D/prime
j∈Dj/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mht
j(Dj,/tildewidewt)−1
mht
j(D/prime
j,/tildewidewt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(22)
=1
m2max
jmax
adjacentDj,D/prime
j∈Dj/vextenddouble/vextenddoubleht
j(Dj,/tildewidewt)−ht
j(D/prime
j,/tildewidewt)/vextenddouble/vextenddouble2(23)
=1
m2max
j(∆2ht
j)2. (24)
Hence, by sequential composition of Rényi Diﬀerential Privacy (Mironov, 2017, Proposition 1), M1:Tis
(α,/summationtextT
i=12αmaxj(∆2ht
j)2/m2σ2)-RDP.
By Lemma 3, we know that M1:Tis(/summationtextT
i=12αmaxj(∆2ht
j)2/m2σ2+log(1/δ)
α−1,δ)-DP.
Plugging in α=4 log(1/δ)
/epsilon1,σ=4γ√
Tlog(1/δ)
/epsilon1m, we have
T/summationdisplay
i=12αmax
j(∆2ht
j)2/m2σ2+log(1/δ)
α−1≤T/summationdisplay
i=12αγ2/m2σ2+log(1/δ)
α−1(25)
=24 log(1/δ)
/epsilon1γ2
m2(4γ√
Tlog(1/δ)
/epsilon1m)2+log(1/δ)
4 log(1/δ)
/epsilon1−1(26)
≤/epsilon1
2+/epsilon1
2(27)
=/epsilon1. (28)
Hence,M1:Tis(/epsilon1,δ)-DP if we choose σ=4γ√
Tlog(1/δ)
/epsilon1m.
By Theorem 1 and Billboard Lemma , it directly follows that Algorithm 1 is (/epsilon1,δ)−JDP.
Proof for Theorem 2. Theorem 1 shows that Algorithm 1 consists of a (/epsilon1,δ)-DP process to produce global
model. After that each task learner trains local model with the DP global model and its private data. By
deﬁnition of BP, it directly follows that Algorithm 1 is (/epsilon1,δ)-BP.
A.2 Convergence Analysis(nonconvex):
We ﬁrst present the formal statement of Theorem 3.
Theorem 6 (Convergence under nonconvex loss) .Letfkbe(L+λ)-smooth. Assume γis suﬃciently large
such thatγ≥maxk,t/bardbl∇wt
kfk(wt
k;/tildewidewt)/bardbl2. Further let f∗
k=minw,¯wfk(w;¯w)andp=q
m. If we use a ﬁxed
learning rate ηt=η=1
pL+(p+1
p)λ, Algorithm 1 satisﬁes:
1
mTT−1/summationdisplay
t=0m/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2≤/parenleftBig
4/parenleftBig
L+λ+1
p2λ/parenrightBig
+2λ
E(Lp2+λp2+2λ)/parenrightBig/summationtextm
k=1(fk(w0
k;/tildewidew0)−f∗
k)
mT
+O/parenleftBig
L+λ+λ
p2/parenrightBig/summationtextT/E
i=1BiE
T+O/parenleftbiggLdλ+dλ2+dλ2/p2
mE/parenrightbigg
σ2.(29)
where
Bt= max
kfk(wt
k;/tildewidewt). (30)
15Published in Transactions on Machine Learning Research (04/2023)
Letσchosen as we set in Theorem 2. Take T=O/parenleftBig
m
λdγ2/parenrightBig
, the right hand side is bounded by
1
mTT−1/summationdisplay
t=0m/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2≤O/parenleftBig
4/parenleftBig
L+λ+1
p2λ/parenrightBig
+2λ
E(Lp2+λp2+2λ)/parenrightBig
m
+O/parenleftBigg
L+λ+λ
p2
E/parenrightBigg
B+O/parenleftBigg
L+λ+λ
p2
mE/parenrightBigg
log(1/δ)
/epsilon12.(31)
Proof for Theorem 6. Letw∗
k=arg minwfk(w;¯w∗). LetIt
kbe the random variable indicating whether task
kis selected in communication round t. Note that the probability task learner kis selected in any arbitrary
communication round pk=/parenleftbiggm−1
q−1/parenrightbigg
/parenleftbiggm
q/parenrightbigg=q
m. Thus E[It
k] =pk=q
m. ByL-smoothness of fk, we have
E[fk(wt+1
k;/tildewidewt)−fk(wt
k;/tildewidewt)]≤E/bracketleftbigg
/angbracketleft∇fk(wt
k;/tildewidewt),wt+1
k−wt
k/angbracketright+L
2/bardblwt+1
k−wt
k/bardbl2/bracketrightbigg
(32)
=E/bracketleftbigg
/angbracketleft∇fk(wt
k;/tildewidewt),ηtIt
k∇fk(wt
k;/tildewidewt)/angbracketright+L
2/bardblηtIt
k∇fk(wt
k;/tildewidewt)/bardbl2/bracketrightbigg
(33)
=/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2. (34)
In the case where t+ 1/negationslash≡0modE, i.e.t+ 1is not a communication round, /tildewidewt+1=/tildewidewt. Therefore, we have
E[fk(wt+1
k;/tildewidewt+1)−fk(wt
k;/tildewidewt)]≤/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2. (35)
In the case where t+ 1≡0 modE, we have
E[fk(wt+1
k;/tildewidewt+1)−fk(wt
k;/tildewidewt)]≤E[fk(wt+1
k;/tildewidewt+1)−fk(wt+1
k;/tildewidewt)]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B+/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2.
(36)
It suﬃces to bound B:
B=E/bracketleftBigg
λ
2/bardblwt+1
k−/tildewidewt+1/bardbl2−λ
2/bardblwt+1
k−/tildewidewt/bardbl2/bracketrightBigg
(37)
=λ
2E[/bardbl/tildewidewt−/tildewidewt+1/bardbl/bardbl2wt+1
k−/tildewidewt−/tildewidewt+1/bardbl] (38)
≤λ
2/radicalBig
E[/bardbl/tildewidewt−/tildewidewt+1/bardbl2]E[/bardbl2wt+1
k−/tildewidewt−/tildewidewt+1/bardbl2] (39)
=λ
2/radicalbig
E[/bardbl/tildewidewt−/tildewidewt+1/bardbl2]/radicalBig
E[/bardbl(/tildewidewt+1−/tildewidewt) + 2(wt+1
k−/tildewidewt+1)/bardbl2] (40)
≤λ
2/radicalbig
E[/bardbl/tildewidewt−/tildewidewt+1/bardbl2]/radicalBig
E[/bardbl/tildewidewt+1−/tildewidewt/bardbl2] + 4/bardblwt+1
k−/tildewidewt+1/bardbl2+ 4E[/bardbl/tildewidewt+1−/tildewidewt/bardbl/bardblwt+1
k−/tildewidewt+1/bardbl](41)
≤λ
2/radicalBigg
E[/bardbl/tildewidewt−/tildewidewt+1/bardbl2]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C1/radicaltp/radicalvertex/radicalvertex/radicalbtE[/bardbl/tildewidewt+1−/tildewidewt/bardbl2] + 4/bardblwt+1
k−/tildewidewt+1/bardbl2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C2+4/radicalBig
E[/bardbl/tildewidewt+1−/tildewidewt/bardbl2]/bardblwt+1
k−/tildewidewt+1/bardbl2
(42)
16Published in Transactions on Machine Learning Research (04/2023)
where the ﬁrst and third inequality follows from Cauchy-Schwartz Inequality: E[XY]≤/radicalbig
E[X2]E[Y2]. We
can then upper bound C 1and C 2.
C1=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
q/summationdisplay
k∈Stgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg
+βt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (43)
≤
/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
qm/summationdisplay
k=1It+1
kgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+/radicalbig
E[/bardblβt/bardbl2]
2
(44)
=
/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
qm/summationdisplay
k=1It+1
kgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+√
dσ
2
(45)
≤
/radicaltp/radicalvertex/radicalvertex/radicalbtm
q2m/summationdisplay
k=1E/bracketleftBigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleIt+1
kgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightBigg
+√
dσ
2
(46)
≤
/radicaltp/radicalvertex/radicalvertex/radicalbtm
q2m/summationdisplay
k=1E/bracketleftBigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleIt+1
kηt∇fk(wt
k) min/parenleftbigg
1,γ
ηt/bardbl∇fk(wt
k)/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightBigg
+√
dσ
2
(47)
≤
/radicaltp/radicalvertex/radicalvertex/radicalbt1
mm/summationdisplay
k=1/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηt∇fk(wt
k) min/parenleftbigg
1,γ
ηt/bardbl∇fk(wt
k)/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+√
dσ
2
. (48)
Denoteh(t) =/radicalBigg
1
m/summationtextm
k=1/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηt∇fk(wt
k) min/parenleftbigg
1,γ
ηt/bardbl∇fk(wt
k)/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
. We have:
C2≤2
λλ
2/bardblwt+1
k−/tildewidewt+1/bardbl2(49)
≤2
λfk(wt+1
k;/tildewidewt+1) (50)
=2
λBt+1 (51)
Plugging the bounds for C 1and C 2into B yields:
B≤λ
2(h(t) +√
dσ)/parenleftBigg
h(t) +√
dσ+ 2/radicalbigg
2
λBt+1/parenrightBigg
. (52)
Denote the right hand side as β(t), we have
E[fk(wt+1
k;/tildewidewt+1)−fk(wt
k;/tildewidewt)]≤β(t) +/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2. (53)
Letδt=E[fk(wt
k;/tildewidewt)−fk(w∗
k; ¯w∗)], we have
δt+1≤δt+β(t) +/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2. (54)
In the nonconvex case, we have
T−1/summationdisplay
t=0/parenleftbigg
ηtpk−L+λ
2η2
tp2
k/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2−β(t)≤fk(w0
k;/tildewidew0)−f∗
k (55)
17Published in Transactions on Machine Learning Research (04/2023)
Summing over kon the left handed side, when γis large enough so that no clipping happens we have
m/summationdisplay
k=1/summationdisplay
t+1≡0 modE/parenleftbigg/parenleftbigg
ηtpk−L+λ
2η2
tp2
k/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2−β(t)/parenrightbigg
+/summationdisplay
t+1/negationslash≡0 modE/parenleftbigg
ηtpk−L+λ
2η2
tp2
k/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2(56)
=/summationdisplay
t+1≡0 modE/parenleftbigg
ηtp−L+λ
2η2
tp2/parenrightbiggm/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2−mβ(t)
+/summationdisplay
t+1/negationslash≡0 modE/parenleftbigg
ηtp−L+λ
2η2
tp2/parenrightbiggm/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2(57)
=/summationdisplay
t+1≡0 modE/parenleftbigg
ηtp−L+λ
2η2
tp2/parenrightbiggm/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2
−λ
2/parenleftBigg
mh2(t) +/parenleftBigg
2√
dσ+ 2/radicalbigg
2
λBt+1/parenrightBigg
mh(t) +m/parenleftBigg
dσ2+ 2σ/radicalbigg
2d
λBt+1/parenrightBigg/parenrightBigg
+/summationdisplay
t+1/negationslash≡0 modE/parenleftbigg
ηtp−L+λ
2η2
tp2/parenrightbiggm/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2(58)
=T−1/summationdisplay
t=0/parenleftbigg
ηtp−L+λ
2η2
tp2/parenrightbigg
G2
t
+/summationdisplay
t+1/negationslash≡0 modE−λ
2η2
tG2
t−λ√m/parenleftBigg
√
dσ+/radicalbigg
2
λBt+1/parenrightBigg
ηtGt−λm
2/parenleftBigg
dσ2+ 2σ/radicalbigg
2d
λBt+1/parenrightBigg(59)
≤m/summationdisplay
k=1fk(w0
k;/tildewidew0)−f∗
k, (60)
whereGt=/radicalbig/summationtextm
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2. Pickingηt=p
p2L+(p2+1)λyields
/summationdisplay
t+1≡0 modEp2
2(p2L+ (p2+ 1)λ)G2
t−λ√m/parenleftBig√
dσ+/radicalBig
2
λBt+1/parenrightBig
p
p2L+ (p2+ 1)λGt−λm
2/parenleftBigg
dσ2+ 2σ/radicalbigg
2d
λBt+1/parenrightBigg
(61)
+/summationdisplay
t+1/negationslash≡0 modEp2(p2L+ (p2+ 2)λ)
2(p2L+ (p2+ 1)λ)2G2
t (62)
≤m/summationdisplay
k=1fk(w0
k;/tildewidew0)−f∗
k. (63)
This is equivalent to
T−1/summationdisplay
t=0G2
t+/summationdisplay
t+1≡0 modE−2λ√m/parenleftBigg
√
dσ+/radicalbigg
2
λBt+1/parenrightBigg
m
qGt−/parenleftbigg
L+λ+λ
p2/parenrightbigg
λm/parenleftBigg
dσ2+ 2σ/radicalbigg
2d
λBt+1/parenrightBigg
(64)
≤
21
E/parenleftbigg
L+λ+1
p2λ/parenrightbigg
+ 2E−1
E/parenleftBig
L+λ+1
p2λ/parenrightBig2
L+λ+2
p2λ
m/summationdisplay
k=1fk(w0
k;/tildewidew0)−f∗
k (65)
=
2/parenleftbigg
L+λ+1
p2λ/parenrightbigg
+1
p2λ
E/parenleftBig
L+λ+2
p2λ/parenrightBig
m/summationdisplay
k=1fk(w0
k;/tildewidew0)−f∗
k. (66)
18Published in Transactions on Machine Learning Research (04/2023)
Hence, we have
/summationdisplay
t+1≡0 modE/parenleftBigg
Gt−λm/parenleftBigg
√
dσ+/radicalbigg
2
λBt+1/parenrightBigg
1
p/parenrightBigg2
+/summationdisplay
t+1/negationslash≡0 modEG2
t (67)
≤
2/parenleftbigg
L+λ+1
p2λ/parenrightbigg
+1
p2λ
E/parenleftBig
L+λ+2
p2λ/parenrightBig
m/summationdisplay
k=1fk(w0
k;/tildewidew0)−f∗
k (68)
+/summationdisplay
t+1≡0 modE(Lλm +mλ2)/parenleftBigg
dσ2+ 2σ/radicalbigg
2d
λBt+1/parenrightBigg
+ 2mλBt+1
p2. (69)
This implies
T−1/summationdisplay
t=0G2
t≤2
2/parenleftbigg
L+λ+1
p2λ/parenrightbigg
+1
p2λ
E/parenleftBig
L+λ+2
p2λ/parenrightBig
m/summationdisplay
k=1fk(w0
k;/tildewidew0)−f∗
k (70)
+ 2/parenleftBigg/summationdisplay
t+1≡0 modE(Lλm +mλ2+2λ2m
p2)/parenleftBigg
dσ2+ 2σ/radicalbigg
2d
λBt+1/parenrightBigg
+ 4mλBt+1
p2/parenrightBigg
.(71)
Hence, we conclude that
1
mTT−1/summationdisplay
t=0m/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2(72)
≤/parenleftBig
4/parenleftBig
L+λ+1
p2λ/parenrightBig
+2λ
E(Lp2+λp2+2λ)/parenrightBig/summationtextm
k=1(fk(w0
k;/tildewidew0)−f∗
k)
mT
+O/parenleftBig
Lλ+λ2+λ2
p2/parenrightBig/summationtextT/E
i=1/parenleftbigg
dσ2+ 2σ/radicalBig
2d
λBiE+2BiE
λ/parenrightbigg
T(73)
≤/parenleftBig
4/parenleftBig
L+λ+1
p2λ/parenrightBig
+2λ
E(Lp2+λp2+2λ)/parenrightBig/summationtextm
k=1(fk(w0
k;/tildewidew0)−f∗
k)
mT
+O/parenleftBig
L+λ+λ
p2/parenrightBig/summationtextT/E
i=1/parenleftBig√
dλσ+√2BiE/parenrightBig2
T(74)
≤/parenleftBig
4/parenleftBig
L+λ+1
p2λ/parenrightBig
+2λ
E(Lp2+λp2+2λ)/parenrightBig/summationtextm
k=1(fk(w0
k;/tildewidew0)−f∗
k)
mT+O/parenleftBig
L+λ+λm2
q2/parenrightBig/summationtextT/E
i=1BiE
T
+O/parenleftbiggLdλ+dλ2+dλ2/p2
E/parenrightbigg
σ2.(75)
Takingσ=c2γ√
Tlog(1/δ)
m/epsilon1andT=O/parenleftBig
m
λdγ2/parenrightBig
, we have
1
mTT−1/summationdisplay
t=0m/summationdisplay
k=1/bardbl∇fk(wt
k;/tildewidewt)/bardbl2≤/parenleftBig
4/parenleftBig
L+λ+1
p2λ/parenrightBig
+2λ
E(Lp2+λp2+2λ)/parenrightBig/summationtextm
k=1(fk(w0
k;/tildewidew0)−f∗
k)
mT(76)
+O/parenleftBig
L+λ+λ
p2/parenrightBig/summationtextT−1
t=0Bt+1
T+1
EO/parenleftbigg
L+λ+λ
p2/parenrightbigglog(1/δ)
/epsilon12.(77)
19Published in Transactions on Machine Learning Research (04/2023)
A.3 Convergence Analysis (Convex):
We ﬁrst present the formal statement of Theorem 5.
Theorem 7. Letfkbe(L+λ)-smooth and (µ+λ)-strongly convex. Assume γis suﬃciently large such that
γ≥maxk,t/bardbl∇wt
kfk(wt
k;/tildewidewt)/bardbl2. Further let w∗
k=arg minwfk(w;¯w∗), where ¯w∗=1
m/summationtextm
k=1w∗
kandp=q
m. If
we use a ﬁxed learning rate ηt=η=cp
Lp2+λp2−2λfor some constant csuch that 0≤1−ηp(c−2)(µ+λ)≤1
2,
Algorithm 1 satisﬁes:
∆T≤1
2T/parenleftBigg
∆0−mλ/parenleftBigg
dσ2+ 2√
dσ/radicalbigg
2
λB+1
λB/parenrightBigg/parenrightBigg
+mλ/parenleftBig
dσ2+ 2√
dσ/radicalBig
2
λB+1
λB/parenrightBig
1−1
2E,(78)
where ∆t=/summationtextm
k=1fk(wt
k;/tildewidewt)−fk(w∗
k;/tildewidew∗)andB= maxtmaxkfk(wt
k;/tildewidewt).
Letσbe chosen as in Theorem 2, then there exists T=O/parenleftBig
m
λdγ2/parenrightBig
such that
1
m∆T≤1
2T/parenleftbigg1
m∆0−log(1/δ)
m/epsilon12−O(B)/parenrightbigg
+log(1/δ)
m/epsilon12+O(B)
1−1
2E. (79)
Proof for Theorem 7. Letw∗
k=arg minwfk(w;¯w∗). LetIt
kbe the random variable indicating whether task
kis selected in communication round t. Thus E[It
k] =pk. ByL+λ-smoothness and µ+λ-strong convexity
offk, we have
E[fk(wt+1
k;/tildewidewt)−fk(wt
k;/tildewidewt)]≤E/bracketleftbigg
/angbracketleft∇fk(wt
k;/tildewidewt),wt+1
k−wt
k/angbracketright+L
2/bardblwt+1
k−wt
k/bardbl2/bracketrightbigg
(80)
=E/bracketleftbigg
/angbracketleft∇fk(wt
k;/tildewidewt),ηtIt
k∇fk(wt
k;/tildewidewt)/angbracketright+L
2/bardblηtIt
k∇fk(wt
k;/tildewidewt)/bardbl2/bracketrightbigg
(81)
=/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
/bardbl∇fk(wt
k;/tildewidewt)/bardbl2(82)
≤/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
2(µ+λ)(f(wt
k;/tildewidewt)−f(w∗
k;/tildewidewt)) (83)
≤/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
2(µ+λ)(f(wt
k;/tildewidewt)−f(w∗
k;/tildewidew∗)). (84)
In the case where t+ 1/negationslash≡0modE, i.e.t+ 1is not a communication round, /tildewidewt+1=/tildewidewt. Therefore, we have
E[fk(wt+1
k;/tildewidewt+1)−fk(wt
k;/tildewidewt)]≤/parenleftbiggL+λ
2η2
tp2
k−ηtpk/parenrightbigg
2(µ+λ)(f(wt
k;/tildewidewt)−f(w∗
k;/tildewidew∗)).(85)
In the case where t+ 1≡0 modE, we have
E[fk(wt+1
k;/tildewidewt+1)−fk(wt
k;/tildewidewt)]≤E[fk(wt+1
k;/tildewidewt+1)−fk(wt+1
k;/tildewidewt)]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B
+/parenleftbig
(L+λ)η2
tp2
k−2ηtpk/parenrightbig
(µ+λ)(f(wt
k;/tildewidewt)−f∗
k).(86)
20Published in Transactions on Machine Learning Research (04/2023)
It suﬃces to bound B:
B=E/bracketleftBigg
λ
2/bardblwt+1
k−/tildewidewt+1/bardbl2−λ
2/bardblwt+1
k−/tildewidewt/bardbl2/bracketrightBigg
(87)
=λ
2E[/bardbl/tildewidewt−/tildewidewt+1/bardbl/bardbl2wt+1
k−/tildewidewt−/tildewidewt+1/bardbl] (88)
≤λ
2/radicalBig
E[/bardbl/tildewidewt−/tildewidewt+1/bardbl2]E[/bardbl2wt+1
k−/tildewidewt−/tildewidewt+1/bardbl2] (89)
=λ
2/radicalbig
E[/bardbl/tildewidewt−/tildewidewt+1/bardbl2]/radicalBig
E[/bardbl(/tildewidewt+1−/tildewidewt) + 2(wt+1
k−/tildewidewt+1)/bardbl2] (90)
≤λ
2/radicalbig
E[/bardbl/tildewidewt−/tildewidewt+1/bardbl2]/radicalBig
E[/bardbl/tildewidewt+1−/tildewidewt/bardbl2] + 4/bardblwt+1
k−/tildewidewt+1/bardbl2+ 4E[/bardbl/tildewidewt+1−/tildewidewt/bardbl/bardblwt+1
k−/tildewidewt+1/bardbl](91)
≤λ
2/radicalBigg
E[/bardbl/tildewidewt−/tildewidewt+1/bardbl2]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C1/radicaltp/radicalvertex/radicalvertex/radicalbtE[/bardbl/tildewidewt+1−/tildewidewt/bardbl2] + 4/bardblwt+1
k−/tildewidewt+1/bardbl2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C2+4/radicalBig
E[/bardbl/tildewidewt+1−/tildewidewt/bardbl2]/bardblwt+1
k−/tildewidewt+1/bardbl2
(92)
where the ﬁrst and third inequality follows from Cauchy-Schwartz Inequality: E[XY]≤/radicalbig
E[X2]E[Y2]. It
suﬃces to ﬁnd the upper bound of C 1and C 2.
C1=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
q/summationdisplay
k∈Stgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg
+βt/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (93)
≤
/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
qm/summationdisplay
k=1It+1
kgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+/radicalbig
E[/bardblβt/bardbl2]
2
(94)
=
/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
qm/summationdisplay
k=1It+1
kgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+√
dσ
2
(95)
≤
/radicaltp/radicalvertex/radicalvertex/radicalbtm
q2m/summationdisplay
k=1E/bracketleftBigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleIt+1
kgt+1
kmin/parenleftbigg
1,γ
/bardblgt+1
k/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightBigg
+√
dσ
2
(96)
≤
/radicaltp/radicalvertex/radicalvertex/radicalbtm
q2m/summationdisplay
k=1E/bracketleftBigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleIt+1
kηt∇fk(wt
k) min/parenleftbigg
1,γ
ηt/bardbl∇fk(wt
k)/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightBigg
+√
dσ
2
(97)
≤
/radicaltp/radicalvertex/radicalvertex/radicalbt1
mm/summationdisplay
k=1/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηt∇fk(wt
k) min/parenleftbigg
1,γ
ηt/bardbl∇fk(wt
k)/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+√
dσ
2
. (98)
Denoteh(t) =/radicalBigg
1
m/summationtextm
k=1/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηt∇fk(wt
k) min/parenleftbigg
1,γ
ηt/bardbl∇fk(wt
k)/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
. On the other hand,
C2≤2
λλ
2/bardblwt+1
k−/tildewidewt+1/bardbl2(99)
≤2
λfk(wt+1
k;/tildewidewt+1) (100)
=2
λBt+1. (101)
21Published in Transactions on Machine Learning Research (04/2023)
Plug the bounds for C 1and C 2into B:
B≤λ
2(h(t) +√
dσ)/parenleftBigg
h(t) +√
dσ+ 2/radicalbigg
2
λBt+1/parenrightBigg
(102)
≤λ/parenleftBigg
h2(t) +dσ2+ 2√
dσ/radicalbigg
2
λBt+1+1
λBt+1/parenrightBigg
(103)
Denoting the right hand side as β(t), we have
E[fk(wt+1
k;/tildewidewt+1)−fk(wt
k;/tildewidewt)]≤β(t) +/parenleftbig
(L+λ)η2
tp2
k−2ηtpk/parenrightbig
(µ+λ)(f(wt
k;/tildewidewt)−f∗
k).(104)
Lettingδt
k=E[fk(wt
k;/tildewidewt)−fk(w∗
k; ¯w∗)], we have
δt+1
k≤/parenleftbig
1−/parenleftbig
(L+λ)η2
tp2
k−2ηtpk/parenrightbig
(µ+λ)/parenrightbig
δt
k+β(t) (105)
Summing over kon the left handed side, when γis large enough so that no clipping happens we have
m/summationdisplay
k=1δt+1
k≤/parenleftbig
1−/parenleftbig
(L+λ)η2
tp2−2ηtp/parenrightbig
(µ+λ)/parenrightbigm/summationdisplay
k=1δt
k+mβ(t) (106)
=/parenleftbig
1−/parenleftbig
(Lp2+λp2−2λ)η2
t−2ηtp/parenrightbig
(µ+λ)/parenrightbigm/summationdisplay
k=1δt
k+mλ/parenleftBigg
dσ2+ 2√
dσ/radicalbigg
2
λBt+1+1
λBt+1/parenrightBigg
.
(107)
Let∆t=/summationtextm
k=1δt
k. Assume maxt≤TBt=B. PickC=mλ/parenleftbig
dσ2+2√
dσ√
2
λB+1
λB/parenrightbig
((Lp2+λp2−2λ)η2
t−2ηtp)(µ+λ), we have
∆t+1−C≤/parenleftbig
1−/parenleftbig
(Lp2+λp2−2λ)η2
t−2ηtp/parenrightbig
(µ+λ)/parenrightbig
(∆t−C). (108)
Note that in the case where t+ 1/negationslash≡0 modE, we have
∆t+1≤/parenleftbig
1−/parenleftbig
(Lp2+λp2)η2
t−2ηtp/parenrightbig
(µ+λ)/parenrightbig
∆t (109)
≤/parenleftbig
1−/parenleftbig
(Lp2+λp2−2λ)η2
t−2ηtp/parenrightbig
(µ+λ)/parenrightbig
∆t. (110)
Chooseηt=η=cp
Lp2+λp2−2λfor some constant csuch that 0</parenleftbig
1−/parenleftbig
(Lp2+λp2−2λ)η2
t−2ηtp/parenrightbig
(µ+λ)/parenrightbig
<
1
2. We have
∆t+1−C≤/parenleftBigg
1−(c2−2c)(µ+λ)
L+λ−2λ
p2/parenrightBigg
(∆t−C) (111)
≤/parenleftBigg
1−(c2−2c)(µ+λ)
L+λ−2λ
p2/parenrightBigg
/parenleftBigg
1−(c2−2c)(µ+λ)
L+λ−2λ
p2/parenrightBiggE−1
∆t−E+1−C
.(112)
This is equivalent to
∆t+1−D≤/parenleftBigg
1−(c2−2c)(µ+λ)
L+λ−2λ
p2/parenrightBiggE
(∆t−E+1−D) (113)
22Published in Transactions on Machine Learning Research (04/2023)
where
D=(c2−2c)(µ+λ)
L+λ−2λ
p2
1−/parenleftbigg
1−(c2−2c)(µ+λ)
L+λ−2λ
p2/parenrightbiggEC (114)
=mλ/parenleftBig
dσ2+ 2√
dσ/radicalBig
2
λB+1
λB/parenrightBig
1−/parenleftbigg
1−(c2−2c)(µ+λ)
L+λ−2λ
p2/parenrightbiggE(115)
∈
mλ/parenleftBigg
dσ2+ 2√
dσ/radicalbigg
2
λB+1
λB/parenrightBigg
,mλ/parenleftBig
dσ2+ 2√
dσ/radicalBig
2
λB+1
λB/parenrightBig
1−1
2E
. (116)
Apply recursively to all t, we obtain
∆T≤/parenleftBigg
1−(c2−2c)(µ+λ)
L+λ−2λ
p2/parenrightBiggT
(∆0−D) +D (117)
≤1
2T/parenleftBigg
∆0−mλ/parenleftBigg
dσ2+ 2√
dσ/radicalbigg
2
λB+1
λB/parenrightBigg/parenrightBigg
+mλ/parenleftBig
dσ2+ 2√
dσ/radicalBig
2
λB+1
λB/parenrightBig
1−1
2E.(118)
Takeσ=c2γ√
Tlog(1/δ)
m/epsilon1and we can ﬁnd T=O/parenleftBig
m
λdγ2/parenrightBig
such that,
∆T≤1
2T/parenleftbigg
∆0−log(1/δ)
/epsilon12−O(mB)/parenrightbigg
+log(1/δ)
/epsilon12+O(mB)
1−1
2E. (119)
Divide both side by m, we have
1
m∆T≤1
2T/parenleftbigg1
m∆0−log(1/δ)
m/epsilon12−O(B)/parenrightbigg
+log(1/δ)
m/epsilon12+O(B)
1−1
2E. (120)
In the non-private case, our Theorem 3 could reduce to the following corollary, which is of independent
interest.
Corollary 8. Whenσ= 0, Algorithm 1 with (L+λ)-smooth and (µ+λ)-strongly convex fksatisﬁes
1
m∆T≤1
2T/parenleftbigg1
m∆0−B/parenrightbigg
+B
1−1
2E. (121)
A.4 Datasets and Models
We summarize the details of the datasets and models we used in our empirical study in Table 2. Our
experiments include both convex (Logistic Regression) and non-convex (CNN) loss objectives on both
text (StackOverﬂow) and image (CelebA and FEMNIST) datasets. We provide anonymized code in the
supplementary material for reproducibility. Our code makes use of the FL implementation from the public
repo of Laguel et al. (2021) and Li et al. (2021).
23Published in Transactions on Machine Learning Research (04/2023)
Table 2
Dataset Number of tasks Model Task Type
FEMNIST (Cohen et al., 2017; Caldas et al., 2018) 205 4-layer CNN 62-class image classiﬁcation
StackOverﬂow (tﬀ) 400 Logistic Regression 500-class tag prediction
CelebA (Liu et al., 2015; Caldas et al., 2018) 515 4-layer CNN Binary image classiﬁcation
A.5 Hyperparameters
Each ﬁxed privacy parameter /epsilon1could be computed by diﬀerent combinations of noise scale σ, clipping norm γ,
number of communication rounds T, and subsampling rate p=q
m. In all our experiments, we subsample 100
diﬀerent tasks for each round, i.e. q= 100, to perform local training as well as involved in global aggregation.
For FEMNIST and CelebA, we choose σ∈{0.02,0.05,0.1}andγ∈{0.2,0.5,1}. For StackOverﬂow, we
chooseσ∈{0.01,0.05,0.1}andγ∈{0.1,0.5,1}. We summarize both utility and privacy performance for
diﬀerent hyperparameters below.
0 200 400 600 800 1000
Communication rounds0.00.20.40.60.8Test AccuracyFEMNIST, =0.2
=0.1
=0.05
=0.02
non-private
0 200 400 600 800 1000
Communication rounds0.00.20.40.60.8Test AccuracyFEMNIST, =0.5
=0.1
=0.05
=0.02
non-private
0 200 400 600 800 1000
Communication rounds0.00.20.40.60.8Test AccuracyFEMNIST, =1
=0.1
=0.05
=0.02
non-private
0 200 400 600 800 1000
Communication rounds012345
FEMNIST, =0.2
=0.1
=0.05
=0.02
0 200 400 600 800 1000
Communication rounds051015
FEMNIST, =0.5
=0.1
=0.05
=0.02
0 200 400 600 800 1000
Communication rounds02040
FEMNIST, =1
=0.1
=0.05
=0.02
Figure 4: FEMNIST results
A.6 Comparison with FedProx
Besides FedAvg, we also compared private mean-regularized MTL with other methods that aims to train a
global model privately. In particular, we studied private FedProx (Li et al., 2020b) as an alternative global
baseline. Note that although the local objective being solved in FedProx is similar to that in mean-regularized
MTL, FedProx is a fundamentally diﬀerent method to handle data heterogeneity in FL from MTL. Speciﬁcally,
FedProx learns a global model where each client solves an inexact minimizer by optimizing local empirical
risk with a regularization term. We instead explore learning a multi-task objective where each client solves a
mean-regularized objective and learns a separate, client-speciﬁc model . The results are shown in Figure 7.
In all three datasets, private FedProx is very similar to private FedAvg under diﬀerent private parameters
/epsilon1and worse than private MTL. In particular, in FEMNIST and Stackoverﬂow, private MTL signiﬁcantly
outperforms training a private global model (FedAvg and FedProx), for all /epsilon1’s.
24Published in Transactions on Machine Learning Research (04/2023)
0 200 400 600 800 1000
Communication rounds0.50.60.70.80.9Test AccuracyCelebA, =0.2
=0.1
=0.05
=0.02
non-private
0 200 400 600 800 1000
Communication rounds0.50.60.70.80.9Test AccuracyCelebA, =0.5
=0.1
=0.05
=0.02
non-private
0 200 400 600 800 1000
Communication rounds0.50.60.70.80.9Test AccuracyCelebA, =1
=0.1
=0.05
=0.02
non-private
0 200 400 600 800 1000
Communication rounds0.00.51.01.5
CelebA, =0.2
=0.1
=0.05
=0.02
0 200 400 600 800 1000
Communication rounds0246
CelebA, =0.5
=0.1
=0.05
=0.02
0 200 400 600 800 1000
Communication rounds051015
CelebA, =1
=0.1
=0.05
=0.02
Figure 5: CelebA results
0 200 400 600
Communication rounds0.00.10.20.3Test AccuracyStackOverflow, =0.5
=0.1
=0.05
=0.01
non-private
0 200 400 600
Communication rounds0.00.10.20.3Test AccuracyStackOverflow, =1
=0.1
=0.05
=0.01
non-private
0 200 400 600
Communication rounds0.00.10.20.3Test AccuracyStackOverflow, =2
=0.1
=0.05
=0.01
non-private
0 200 400 600
Communication rounds051015
StackOverflow, =0.5
=0.1
=0.05
=0.01
0 200 400 600
Communication rounds01020304050
StackOverflow, =1
=0.1
=0.05
=0.01
0 200 400 600
Communication rounds050100150200250
StackOverflow, =2
=0.1
=0.05
=0.01
Figure 6: StackOverﬂow results
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
0.000.050.100.150.200.250.30Test accuracy
MTL
Global
FedProx
(a) StackOverﬂow tag prediction
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
0.10.20.30.40.50.60.7Test accuracy
MTL
Global
FedProx (b) FEMNIST
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
0.5000.5250.5500.5750.6000.6250.6500.675Test accuracy
MTL
Global
FedProx (c) CelebA
Figure 7: Comparison of PMTL and training a private global model(FedAvg/FedProx).
25Published in Transactions on Machine Learning Research (04/2023)
Local PMTL PMTL+best ﬁnetuning
/epsilon1= 0.1/epsilon1= 0.8/epsilon1= 2.0/epsilon1= 0.1/epsilon1= 0.8/epsilon1= 2.0
StackOverﬂow .318 .305 .323 .324 ∗ ∗ ∗
FEMNIST .618 .371 .498 .621 .663 .640 .681
CelebA .694 .633 .641 .667 .801 .817 .818
Table 3: Comparison between PMTL and Local training. For the PMTL+ﬁnetuning results on the non-convex
problems, we pick the ﬁnetuning method that yields the highest test accuracy from all the methods introduced in
Section 5.3.
A.7 Comparison with pure local baseline
While federated learning could yield better utility performance compared to pure local training, this is not
always true when we apply client-level DP during federated learning. When a small /epsilon1is enforced, accuracy
for federated learning could drastically drop (see Section 5). In this section, we compare our PMTL with
training pure local model. In addition, since local ﬁnetuning does not incur additional privacy cost in our
scenario to protect client-level privacy, we also compare PMTL+ﬁnetuning with local training. We present
the results in Table 3. For StackOverﬂow where a convex model is used, ﬁnetuning with suﬃciently many
rounds should be the same as training the local model. For the other two datasets where a neural network is
trained, training a purely local model performs worse than PMTL under large /epsilon1and PMTL with the best
local ﬁnetuning objective under all /epsilon1we evaluated. We note that the goal of our work is not to argue that
MTL is better than global/local baselines in all scenarios, but rather to show that it is possible to provide
eﬀective private training methods for commonly-used MTL objectives.
A.8 Comparison to PP-SGD (Bietti et al., 2022)
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
0.000.050.100.150.200.250.30T est accuracyPMTL
PP-SGD
Global
Figure 8: Comparison with PP-SGD on Stackover-
ﬂow tag predictionIn this section, we compare PP-SGD (Bietti et al., 2022),
a similar form of model personalization in federated learn-
ing, with our proposed PMTL on Stackoverﬂow tag predic-
tion. PP-SGD aims to solve the following localobjective:
minw,θifi(w,θi,(x,y)) :=/lscript(y,(w+θi)/latticetopx). It is worth
noting that when the model is a neural network, there
isn’t a straightforward extension for this method to sup-
port personalized model weights for each layer, in contrast
to our method where the mean-regularization term is
calculated by taking the diﬀerence of the entire model
parameter vector of global and local model. Therefore, for
fair comparison, we run PP-SGD on the logstic regression
Stackoverﬂow tag prediction task. Recall that diﬀerent
from the stackoverﬂow task in the original Bietti et al.
(2022) paper, we look at a slightly diﬀerent setting where feature dimension is 10000 and number of classes
is 500 (instead of 5000 features dimension and 80 classes in Bietti et al. (2022)). The results are shown in
Figure 8. As we see, when we require strong privacy ( /epsilon1<1), PP-SGD gives a worse privacy-utility trade-oﬀ
compared to our method. When privacy is weak, our method and PP-SGD achieves similar utility under
same privacy budget.
26