Published in Transactions on Machine Learning Research (04/2023)
U-NO: U-shaped Neural Operators
Md Ashiqur Rahman rahman79@purdue.edu
Department of Computer Science
Purdue University
Zachary E. Ross zross@caltech.edu
Seismological Laboratory
California Institute of Technology
Kamyar Azizzadenesheli kamyara@nvidia.com
NVIDIA Corporation
Reviewed on OpenReview: https: // openreview. net/ forum? id= j3oQF9coJd
Abstract
Neural operators generalize classical neural networks to maps between infinite-dimensional
spaces, e.g., function spaces. Prior works on neural operators proposed a series of novel
methods to learn such maps and demonstrated unprecedented success in learning solution
operators of partial differential equations. Due to their close proximity to fully connected
architectures, these models mainly suffer from high memory usage and are generally limited to
shallow deep learning models. In this paper, we propose U-shaped Neural Operator ( U-NO),
a U-shaped memory enhanced architecture that allows for deeper neural operators. U-NOs
exploit the problem structures in function predictions and demonstrate fast training, data
efficiency, and robustness with respect to hyperparameters choices. We study the performance
ofU-NOon PDE benchmarks, namely, Darcyâ€™s flow law and the Navier-Stokes equations.
We show that U-NOresults in an average of 26% and 44% prediction improvement on
Darcyâ€™s flow and turbulent Navier-Stokes equations, respectively, over the state of art. On
Navier-Stokes 3D spatiotemporal operator learning task, we show U-NOprovides 37%
improvement over the state of the art methods.
Keywords: Neural Operators, Partial Differential Equations
1 Introduction
Conventional deep learning research is mainly dominated by neural networks that allow for learning maps
between finite dimensional spaces. Such developments have resulted in significant advancements in many
practicalsettings, fromvisionandnaturelanguageprocessing, toroboticsandrecommendationsystems(Levine
et al., 2016; Brown et al., 2020; Simonyan & Zisserman, 2014). Recently, in the field of scientific computing,
deep neural networks have been used to great success, particularly in simulating physical systems following
differential equations (Mishra & Molinaro, 2020; Brigham, 1988; Chandler & Kerswell, 2013; Long et al., 2015;
Chen et al., 2019; Raissi & Karniadakis, 2018). Many real-world problems, however, require learning maps
between infinite dimensional functions spaces (Evans, 2010). Neural operators are generalizations of neural
networks and allow to learn maps between infinite dimensional spaces, including function spaces (Li et al.,
2020b). Neural operators are universal approximators of operators and have shown numerous applications in
tackling problems in partial differential equations (Kovachki et al., 2021b).
Neural operators, in an abstract form, consist of a sequence of linear integral operators, each followed by
a nonlinear point-wise operator. Various neural operator architectures have been proposed, most of which
focus on the approximation of the NystrÃ¶m type of the inner linear integral operator of neural operators (Li
1Published in Transactions on Machine Learning Research (04/2023)

a P
G1
G2 GL-1Q u
 Gl
 Glâ€™G
LNeural Operator Layer
v
W+ ÏƒÊƒ k(x,y)v(y)dÎ¼(y) + b(x) 
Figure 1: U-NOarchitecture. ais an input function, uis the output. Orange circles are point-wise operators,
rectangles denote general operators, and smaller blue circles denote concatenations in function spaces.
et al., 2020a;c; Gupta et al., 2021; Tripura & Chakraborty, 2023). For instance, Li et al. (2020a) suggests to
use convolution kernel integration and proposes Fourier neural operator ( FNO), a model that computes the
linear integral operation in the Fourier domain. The linear integral operation can also be approximated using
discrete wavelet transform (Gupta et al., 2021; Tripura & Chakraborty, 2023). But FNOapproximates the
integration using the fast Fourier transform method (Brigham, 1988) and enjoys desirable approximation
theoretic guarantee for continuous operator (Kovachki et al., 2021a). The efficient use of the built-in fast
Fourier transform makes the FNOapproach among the fastest neural operator architectures. Tran et al.
(2021) proposes slight modification to the integral operator of FNOby factorizing the multidimensional
Fourier transform along each dimension. These developments have shown successes in tackling problems in
seismology and geophysics, modeling the so-called Digital Twin Earth, and modeling fluid flow in carbon
capture and storage experiments (Pathak et al., 2022; Yang et al., 2021; Wen et al., 2021; Shui et al., 2020; Li
et al., 2022). These are crucial components in dealing with climate change and natural hazards. Many of the
initial neural operator architectures are inspired by fully-connected neural networks and result in models with
high memory demand that, despite the successes of deep neural networks, prohibit very deep neural operators.
Though there are considerable efforts in finding more effective integral operators, works investigating suitable
architectures of Neural operators are scarce.
To alleviate this problem, we propose the U-shaped Neural Operator ( U-NO) by devising integral operators
that map between functions over different domains. We provide a rigorous mathematical formulation to
adapt the U-net architecture for neural networks for finite-dimensional spaces to neural operators mapping
between function spaces (Ronneberger et al., 2015). Following the u-shaped architecture, the U-NO- first
progressively maps the input function to functions defined with smaller domains (encoding) and later reverses
this operation to generate an appropriate output function (decoding) with skip connections from the encoder
part (see Fig. 1). This efficient contraction and subsequent expansion of domains allow for designing the
over-parametrized and memory-efficient model, which regular architectures cannot achieve.
Over the first half (encoding) of the U-NOlayers, we map each input function space to vector-valued function
spaces with steadily shrunken domains over layers, all the while increasing the dimension of the co-domains,
i.e., the output space of each function. Throughout the second half (decoding) of U-NOlayers, we gradually
expand the domain of each intermediate function space while reducing the dimension of the output function
co-domains and employ skip connections from the first half to construct vector-valued functions with larger
co-dimension. The gradual contraction of the domain of the input function space in each layer allows for the
encoding of function spaces with smaller domains. And the skip connections allow for the direct passage
of information between domain sizes bypassing the bottleneck of the first half, i.e., the functions with the
smallest domain (Long et al., 2015). The U-NOarchitecture can be adapted to the existing operator
2Published in Transactions on Machine Learning Research (04/2023)
learning techniques (Gupta et al., 2021; Tripura & Chakraborty, 2023; Li et al., 2020a; Tran et al., 2021) to
achieve a more effective model. In this work, for the implementation of the inner integral operator of U-NO,
we adopt the Fourier transform-based integration method developed in FNO. AsU-NOcontracts the domain
of each input function at the encoder layers, we progressively define functions over smaller domains. At a
fixed sampling rate (resolution of discretization of the domain), this requires progressively fewer data points
to represent the functions. This makes the U-NOa memory-efficient architecture, allowing for deeper and
highly parameterized neural operators compared with prior works. It is known that over-parameterization is
one of the main essential components of architecture design in deep learning (He et al., 2016), crucial for
performance (Belkin et al., 2019), and optimization (Du et al., 2019). Also recent works show that the
model overparameterization improves generalization performance (Neyshabur et al., 2017; Zhang et al., 2021;
Dar et al., 2021). The U-NOarchitecture allows efficient training of overparameterized model with smaller
memory footprint and opens neural operator learning to deeper and highly parameterized methods.
We establish our empirical study on Darcyâ€™s flow and the Navier-Stokes equations, two PDEs which have
served as benchmarks for the study of neural operator models. We compare the performance of U-NO
against the state of art FNO. We empirically show that the advanced structure of U-NOallows for much
deeper neural operator models with smaller memory usage. We demonstrate that U-NOachieves average
performance improvements of 26%on high resolution simulations of Darcyâ€™s flow equation, and 44%on
Navier-Stokes equation, with the best improvement of 51%. On Navier-Stokes 3D spatio-temporal operator
learning task, for which the input functions are defined on the 3Dspatio-temporal domain, we show U-NO
provides37%improvement over the state of art FNOmodel. Also, the U-NOoutperforms the baseline FNO
on zero-shot super resolution experiment.
It is important to note that U-NOis the first neural operator trained for mapping from function spaces
with 3D domains. Prior studies on 3D domains either transform the input function space with the 3D
spatio-temporal domain to another auxiliary function space with 2D domain for the training purposes,
resulting in an operator that is time discretization dependent or modifies the input function by extending
the domain and co-domain (Li et al., 2020a). We further show that U-NOallows for much deeper models
(3Ã—) with far more parameters (25 Ã—), while still providing some performance improvement (Appendix A.2).
Such a model can be trained with only a few thousand data points, emphasizing the data efficiency of
problem-specific neural operator architecture design.
Moreover, we empirically study the sensitivity of U-NOagainst hyperparameters (Appendix A.2) where
we demonstrate the superiority of this model against FNO. We observe that U-NOis much faster to train
(Appendix A.6)and also much easier to tune. Furthermore, for U-NOas a model that gradually contracts
(expands) the domain (co-domain) of function spaces, we study its variant which applies these transformations
more aggressively. A detailed empirical analysis of such variant of U-NOshows that the architecture is quite
robust with respect to domain/co-domain transformation hyperparameters.
2 Neural Operator Learning
LetAandUdenote input and output function spaces, such that, for any vector valued function aâˆˆA,
a:DAâ†’RdA, withDAâŠ‚Rdand for any vector valued function uâˆˆU,u:DAâ†’RdU, withDUâŠ‚Rd.
Given a set of Ndata points,{(aj,uj)}N
j=1, we aim to train a neural operator GÎ¸:Aâ†’U, parameterized by Î¸,
to learn the underlying map from atou. Neural operators, in an abstract sense, are mainly constructed using
a sequence of non-linear operators Githat are linear integral operators followed by point-wise non-linearity,
i.e., for an input function viat theith layer,Gi:viâ†’vi+1is computed as follows,
Givi(x) :=Ïƒ/parenleftbig/integraldisplay
Îºi(x,y)vi(y)dÂµi(y) +Wivi(x)/parenrightbig
(1)
Here,Îºiis a kernel function that acts, together with the integral, as a global linear operator with measure Âµi,
Wiis a matrix that acts as a point-wise operator, and Ïƒis the point-wise non-linearity. One can explicitly
add a bias function biseparately. Together, these operations constitute Gi, i.e., a nonlinear operator between
a function space of didimensional vector-valued functions with the domain DiâŠ‚Rdand a function space
ofdi+1dimensional vector-valued functions with the domain Di+1âŠ‚Rd. Neural operators often times are
3Published in Transactions on Machine Learning Research (04/2023)
accompanied with a point-wise operator Pthat constitute the first block of the neural operator and mainly
serves as a lifting operator directly applied to input function, and a point-wise operator Q, for the last block,
that is mainly purposed as a projection operator to the output function space.
Previous deployments of neural operators have studied settings in which the domains and co-domains of
function spaces are preserved throughout the layers, i.e., D=Di=Di+1, anddâ€²=di=di+1, for alli. These
models, e.g. FNO, impose that each layer is a map between functions spaces with identical domain and
co-domain spaces. Therefore, at any layer i, the input function is Î½i:Dâ†’Rdâ€²and the output function is
Î½i+1:Dâ†’Rdâ€², defined on the same domain Dand co-domain Rdâ€². This is one of the main reasons for the
high memory demands of previous implementations of neural operators, which occurs primarily from the
global integration step. In this work, we avoid this problem, since as we move to the innermost layers of
the neural operator, we contract the domain of each layer while increasing the dimension of co-domains. As
a result we sequentially encode the input functions into functions with smaller domain, learning compact
representation as well as resulting in integral operations in much smaller domains.
3 A U-shaped Neural Operator ( U-NO)
In this section, we introduce the U-NOarchitecture, which is composed of several standard elements from
prior works on Neural Operators, along with U-shaped additions tailored to the structure of function spaces
(and maps between them). Given an input function a:DAâ†’RdA, we first apply a point-wise operator Pto
aand compute v0:DAâ†’Rd0. The point-wise operator Pis parameterized with a function PÎ¸:RdAâ†’Rd0
and acts as
v0(x) =PÎ¸(a(x)),âˆ€xâˆˆD0
whereD0=DA. The function PÎ¸can be matrix or more generally a deep neural network. For the purpose of
this paper, we take d0â‰«dA, makingPa lifting operator.
Then a sequence of L1non-linear integral operators is applied to v0,
Gi:{vi:Diâ†’Rdvi}â†’{vi+1:Di+1â†’Rdvi+1}foriâˆˆ{0,1,2...L 1},
where for each i,DiâŠ‚Rdis a measurable set accompanied by a measure Âµi. Application of this sequence of
operators results in a sequence of intermediate functions {vi:Diâ†’Rdvi}L1+1
i=1in the encoding part of U-NO.
In this study, without loss of generality, we choose Lebesgue measure ÂµforÂµiâ€™s. Over this first L1layers, i.e.,
the encoding half of U-NO, we map the input function to a set of vector-valued functions with increasingly
contracted domain and higher dimensional co-domain, i.e. for each i, we have Âµ(Di)â‰¥Âµ(Di+1)and
dvi+1â‰¥dvi.
Then, given vL1+1, another sequence of L2non-linear integral operator layers (i.e. the decoder), is applied.
In this stage, the operators gradually expand the domain size and decrease the co-domain dimension to
ultimately match the domain of the terget function, i.e., for each iin these layers, we have Âµ(Di+1)â‰¥Âµ(Di)
anddviâ‰¥dvi+1.
To include the skip connections from the encoder part from the encoder part to the decoder in (see Fig. 1),
the operator GL1+iin the decoder part, takes a vector-wise concatenation of both vL1+iandvL1âˆ’ias its
input. The vector-wise concatenation of vL1+iandvL1âˆ’iis a function vâ€²
L1+iwhere,
vâ€²
L1+i:DL1+iâ†’Rd(L1âˆ’i)+d(L1+i)andvâ€²
L1+i(x) =/bracketleftbig
vL1âˆ’i(x)âŠ¤,vL1+i(x)âŠ¤/bracketrightbigâŠ¤,âˆ€xâˆˆDL1+i.
vâ€²
L1+iconstitutes the input to the operator GL1+i, and we compute the output function of the next layer as,
v(L1+i)+1=Gi+L1vâ€²
i+L1.
Here, for simplicity, we assumed DL1+i=DL1âˆ’i1.
1More generally, concatenation can be defined with a map between the domains m:DL1+iâ†’DL1âˆ’iasvâ€²
i+L1(x) =/bracketleftbig
vL1âˆ’i/parenleftbig
m(x)/parenrightbig
, vL1+i/parenleftbig
x/parenrightbig/bracketrightbigâŠ¤
4Published in Transactions on Machine Learning Research (04/2023)
Computing vL+1for the layer L=L1+L2, we conclude U-NOarchitecture with another point-wise operator
Qkernelized with the QÎ¸:RdL+1â†’RdUsuch that for u=QvL+1we have
u(x) =QÎ¸(vL+1(x)),âˆ€xâˆˆDL+1
whereDL+1=DU. In this paper we choose dL+1â‰«dUsuch thatQis a projection operator. In the next
section, we instantiate the U-NOarchitecture for two benchmark operator learning tasks.
4 Empirical Studies
In this section, we describe the problem settings, implementations, and empirical results.
4.1 Darcy Flow Equation
Darcyâ€™s law is the simplest model for fluid flow in a porous medium. The 2-d Darcy Flow can be described as
a second-order linear elliptic equation with a Dirichlet boundary condition in the following form,
âˆ’âˆ‡Â· (a(x)âˆ‡u(x)) =f(x) xâˆˆD
u(x) = 0 xâˆˆâˆ‚D
whereaâˆˆ A âŠ† Lâˆž(D;R+)represents the diffusion coefficient ,uâˆˆ U âŠ† H1
0(D;R)
is the velocity function, and fâˆˆF=Hâˆ’1(D;R)is the forcing function. Hrepresents Sobolev space
withp= 2.
We takeD= (0,1)2and aim to learn the solution operator Gâ€ , which maps a diffusion coefficient function a
to the solution u, i.e.,u=Gâ€ (a). Please note that due to the Dirichlet boundary condition, we have the
value of the solution ualong the boundary âˆ‚D.
For dataset preparation, we define Âµas a pushforward of the Gaussian measure by operator Ïˆ#asÂµ=
Ïˆ#N(0,(âˆ’âˆ† + 9I)âˆ’2)as a probability measure with zero Neumann boundary conditions on the Laplacian,
i.e., the Laplacian is 0along the boundary. The N(0,(âˆ’âˆ† + 9I)âˆ’2)describes a Gaussian measure with
covariance operator (âˆ’âˆ† + 9I)âˆ’2. TheÏˆ(x)is defined as
Ïˆ(x) =/braceleftï£¬igg
3ifx<0
12ifxâ‰¥0
The diffusion coefficients a(x)are generated according to aâˆ¼Âµand we fix f(x) = 1. The solutions are
obtained by using a second-order finite difference method (Larsson & ThomÃ©e, 2003) on a uniform 421Ã—421
grid over (0,1)2. And solutions of any other resolution are down-sampled from the high-resolution data. This
setup is the same as the benchmark setup in the prior works.
4.2 Navier-Stokes Equation
The Navier-Stokes equations describe the motion of fluids taking into account the effects of viscosity and
external forces. Among the different formulations, we consider the vorticity-streamfunction formulation of
the 2-d Navier-Stokes equations for a viscous and incompressible fluid on the unit torus ( T2), which can be
described as,
âˆ‚tw(x,t) +âˆ‡âŠ¥ÏˆÂ·âˆ‡w(x,t) =Î½âˆ†w(x,t) +g(x), xâˆˆT2,tâˆˆ(0,âˆž),
âˆ’âˆ†Ïˆ=Ï‰, x âˆˆT2,tâˆˆ(0,âˆž),
w(x,0) =w0(x), x âˆˆT2,
HereuâˆˆR+Ã—T2â†’R2is the velocity field. And wis the out-of-plane component of the vorticity field âˆ‡Ã—u
(curl ofu). Since we are considering 2-D flow, the velocity ucan be written by splitting it into components as
/bracketleftbig
u1(x1,x2),u2(x1,x2),0/bracketrightbig
âˆ€(x1,x2)âˆˆT2
5Published in Transactions on Machine Learning Research (04/2023)
And it follows that âˆ‡Ã—u= (0,0,w).
The stream function Ïˆis related to velocity by u=âˆ‡âŠ¥Ïˆ, whereâˆ‡âŠ¥is the skew gradient operator, which is
defined as
âˆ‡âŠ¥Ïˆ= (âˆ’âˆ‚Ïˆ
âˆ‚y,âˆ‚Ïˆ
âˆ‚x)
The function gis defined as the out-of-plane component of the curl of the forcing function, f, i.e., (0,0,g) :=
(âˆ†Ã—f)andgâˆˆT2â†’R, andÎ½âˆˆR+is the viscosity coefficient.
We generated the initial vorticity w0from Gaussian measure as (w0âˆ¼N(0,71.5(âˆ’âˆ† + 49I)âˆ’2.5)with periodic
boundary condition with the constant mean function 0and covariance 71.5(âˆ’âˆ† + 49I)âˆ’2.5. We fixgas
g(x1,x2) = 0.1/parenleftbig
sin(2Ï€(x1+x2)) + cos(2Ï€(x1+x2))/parenrightbig
,âˆ€(x1,x2)âˆˆT2
Following (Kovachki et al., 2021b), the equation is solved using the pseudo-spectral split-step method. The
forcing terms are advanced using Heunâ€™s method (an improved Euler method). And the viscous terms are
advanced using a Crankâ€“Nicolson update with a time-step of 10âˆ’4. Crankâ€“Nicolson update is a second-order
implicit method in time (Larsson & ThomÃ©e, 2003). We record the solution every t= 1time unit. The data
is generated on a uniform 256Ã—256grid and are downsampled to 64Ã—64for the low-resolution training.
For this time-dependent problem, we aim to learn an operator that maps the vorticity field covering a time
interval spanning [0,Tin], into the vorticity field for a later time interval, (Tin,T],
Gâ€ :C([0,Tin];Hr
per(T2;R))â†’C((Tin,T];Hr
per(T2;R))
whereHr
peris the periodic Sobolev space Hrwith constant râ‰¥0. In terms of vorticity, the operator is
defined byw|T2Ã—[0,Tin]â†’w|T2Ã—(Tin,T].
4.3 Model Implementation
We present the specific implementation of the models used for our experiments discussed in the remainder
of this section. For the internal integral operators, we adopt the approach of Li et al. (2020a), in which,
evoking convolution theorem, the integral kernel (convolution) operators are computed by the multiplication
of Fourier transform of kernel convolution operators and the input function in the Fourier domain. Such
operation is approximately computed using fast Fourier transform, providing computational benefit compared
to the other NystrÃ¶m integral approximation methods developed in neural operator literature, and additional
performance benefits results from computing the kernels directly in the Fourier domain, evoking the spectral
convergence (Canuto et al., 2007). Therefore, for a given function vi, i.e., the input to the Gi, we have,
Givi(x) =Ïƒ/parenleftbigg
Fâˆ’1/parenleftbig
RiÂ·F(vi)/parenrightbig
(x) +Wivi(x)/parenrightbigg
HereFandFâˆ’1are the Fourier and Inverse Fourier transforms, respectively. Ri:Zdâ†’Cdi+1Ã—di, and for
each Fourier mode kâˆˆZd, it is the Fourier transform of the periodic convolution function in the integral
convolution operator. We directly parameterize the matrix valued functions RionZdin the Fourier domain
and learn it from data. For each layer i, we also truncate the Fourier series at a preset number of modes,
ki
max=|Zi
kmax|=|kâˆˆZd:kjâ‰¤ki
max,jforjâˆˆ{1,...d}|.
Thus,Riis implicitly a complex valued (ki
maxÃ—dvi+1Ã—dvi)tensor. Assuming the discretization of the
domain is regular, this non-linear operator is implemented efficiently with the fast Fourier transform. Finally,
we define the point-wise residual operation Wiviwhere
Wivi(x) =Wivi(s(x))
withxâˆˆDi+1ands:Di+1â†’Diis a fixed homeomorphism between Di+1andDi.
6Published in Transactions on Machine Learning Research (04/2023)
In the empirical study, we need two classes of operators. The first one is an operator setting where the input
and output functions are defined on 2D spatial domain. And in the second setting, the input and output are
defined on 3D spatio temporal setting. In the following, we describe the architecture of U-NOfor both of
these settings.
Operator learning on functions defined on 2D spatial domains. For mapping between functions
defined on 2Dspatial domain, the solution operator is of form Gâ€ :{a: (0,1)2â†’RdA}â†’{u: (0,1)2â†’RdU}.
TheU-NOarchitecture consists of a series of seven non-linear integral operators,/braceleftbig
Gi/bracerightbig7
i=0, that are placed
between lifting and projection operators. Each layer performs a 2D integral operation in the spatial domain
and contract (or expand) the spatial domain (see Fig. 1).
The first non-linear operator, G0, contracts the domain by a factor of3
4uniformly along each dimension,
while increasing the dimension of the co-domain by a factor of3
2,
G0:{v0: (0,1)2â†’Rdv0}â†’{v1:/parenleftbig
0,3/4/parenrightbig2â†’RâŒˆ3
2dv0âŒ‰}.
Similarly,G1andG2sequentially contract the domain by a factor of2
3and1
2, respectively, while doubling the
dimension of the co-domain. G3is a regular Fourier operator layer and thus the domain or co-domain of its
input and out function spaces stay the same. The last three Fourier operators,/braceleftbig
Gi/bracerightbig6
i=4, expand the domain
and decrease the co-domain dimension, restoring the domain and co-domain to that of the input to G0.
V0:(0,0.5)2â†’R64V1:(0,0.25)2â†’R128V6:(0,0.5)2â†’R64V5:(0,0.25)2â†’R128Diffusion Coefficients a:(0,1)2â†’RSolution Function u:(0,1)2â†’R
PredictionsGround TruthInitial V orticityt = 0 s
t = 14 st = 12 st = 10 st = 8 s
AB
Figure 2: (A) An illustration of aggressive contraction and re-expansion of the domain (and vice versa for the
co-domains) by the variant of U-NOwith a factor of1
2on an instance of Darcy flow equation. (B) Vorticity
field generated by the U-NOvariant as a solution to an instance of the two-dimensional Navier-Stokes
equation with viscosity 10âˆ’6.
We further propose U-NOâ€ which follows a more aggressive factor of1
2(shown in Fig. 2A) while contracting
(or expanding) the domain in each of the integral operators. One of the reasons for this choice is that such a
selection of scaling factors is more memory efficient than the initial U-NOarchitecture.
7Published in Transactions on Machine Learning Research (04/2023)
The projection operator Qand lifting operator Pare implemented as fully-connected neural networks with
lifting dimension d0= 32. And we set skip connections from the first three integral operator ( G0,G1,G2)
respectively to the last three ( G6,G5,G4).
For time dependent problems, where we need to map between input functions aof an initial time interval
[0,Tin], to the solution functions uof later time interval (Tin,T], i.e., to learn an operator of form,
Gâ€ :C/parenleftbig
[0,Tin],{a: (0,1)2â†’RdA}/parenrightbig
â†’C/parenleftbig
{(Tin,T],u: (0,1)2â†’RdU}/parenrightbig
we use an auto regressive model for U-NOandU-NOâ€ . Here recurrently compose the neural operator in
time to produce solution functions for the time interval (Tin,T].
Operator learning on functions defined on 3D spatio-temporal domains. Separately, we also use a
U-NOmodel with non-linear operators that performs the integral operation in space and time ( 3D), which
directly maps the input functions aof interval [0,Tin]to the later time interval (Tin,T], without any recurrent
composition in time. As this model does not require recurrent composition in time, it is fast during both
training and inference. We redefine both aanduon3Dspatio-temporal domain and learn the the operator
Gâ€ :{a: (0,1)2Ã—[0,Tin]â†’RdA}â†’{u: (0,1)2Ã—(Tin,T]â†’RdU}
The non-linear operators constructing U-NO,{Gi}L
i=0, are defined as,
Gi:{vi: (0,Î±i)2Ã—Tiâ†’Rdv0}â†’{vi+1:/parenleftbig
0,cs
iÎ±i)2Ã—(Tin,Tin+ct
iTin]â†’Rcc
idv0}.
Here, (0,Î±i)2Ã—Tiis the domain of the function vi. Andcs
i,ct
iandcc
iare respectively the expansion (or
contraction) factors for spatial domain, temporal domain, and co-domain for iâ€™th operator. Note that
T0= [0,Tin], (0,Î±0) = (0,Î±L+1) = (0,1), andTL+1= (Tin,T].
Here we also first contract and then expand (i.e., cs
iâ‰¤1in the encoding part and cs
iâ‰¥1in the decoding
part) the spatial domain following the U-NOperforming 2Dintegral operation. And we set ct
iâ‰¥1for
all operators Giif the output time interval is larger than input time interval i.e. Tin<Toutâˆ’Tini.e., we
gradually increase the time domain of the input function to match the output. Here the projection operator
Qand lifting operator Pare also implemented as fully connected neural network with d0= 8for the lifting
operatorP.
For the experiments, we use Adam optimizer (Kingma & Ba, 2014) and the initial learning rate is scaled down
by a factor of 0.5every 100epochs. As the non-linearity, we have used the GELU (Hendrycks & Gimpel,
2016) activation function. We use the architecture and implementation of FNOprovided in the original work
Li et al. (2020a). All the computations are carried on a single Nvidia GPU with 24GB memory. For each
experiment, the data is divided into train, test, and validation sets. And the weight of the best-performing
model on the validation set is used for evaluation. Each experiment is repeated 3 times and the mean is
reported.
In the following, we empirically study the performance of these models on Darcy Flow and Navier-Stokes
Equations.
4.4 Results on Darcy Flow Equation
From the dataset of N= 2000simulations of the Darcy flow equation, we set aside 250simulations for testing
and use the rest for training and validation. We use U-NOâ€ with 9layers including the integral, lifting, and
projection operators . We train for 700epochs and save the best performing model on the validation set for
evaluation. The performances of U-NOâ€ andFNOon several different grid resolutions are shown in Table. 1.
The number of modes used in each integral operator stays unchanged throughout different resolutions. We
demonstrate that U-NOâ€ achieve lower relative error on every resolution with 27%improvement on high
resolution (421Ã—421)simulations. The U-shaped structure exploits the problem structure and gradually
encodes the input function to functions with smaller domains, which enables U-NOâ€ to have efficient deep
architectures, modeling complex non-linear mapping between the input and solution function spaces. We
8Published in Transactions on Machine Learning Research (04/2023)
also notice that U-NOâ€ requires23%less training memory while having 7.5times more parameters. With
vanilla FNO, designing such over paramterized deep operator to learn complex maps is impracticable due to
high memory requirements.
Table 1: Benchmarks on Darcy Flow. The average relative error in percentage is reported with error bars
are indicated in the superscript. The average back-propagation memory requirements for a single training
instance is reported over different resolutions are reported. U-NOâ€ performs better than FNOfor every
resolution, while requiring 23%lower training memory which allows efficient training of 7.5times more
parameters
Mem. Resolution sÃ—s
Model Req.# Parameter s = 421 s = 211 s = 141 s = 85
(MB)Ã—106
U-NOâ€ 166 8.2 0.57Â±1.4eâˆ’20.58Â±0.4eâˆ’20.60Â±0.5eâˆ’20.73Â±0.1eâˆ’2
FNO 214 1.1 0.78Â±4.0eâˆ’20.84Â±5.5eâˆ’20.84Â±1.1eâˆ’20.87Â±3.0eâˆ’2
Table 2: Zero-shot super resolution result on Darcy Flow. Neural operators trained on lower spatial resolution
data is directly tested on higher resolution with no further training. We observe U-NOâ€ archives lower
percentage relative error rate.
TrainTests = 141 s = 211 s = 421
s=85 4.7 6.2 8.3
U-NOâ€ s=141 - 2.6 6.3
s=211 - - 4.5
s=85 7.5 14.1 23.9
FNOs=141 - 4.3 13.1
s=211 - - 9.6
4.5 Results on Navier-Stokes Equation
For the auto-regressive model, we follow the architecture for U-NOandU-NOâ€ described in Sec. 4.3. For
U-NOperforming spatio-temporal ( 3D) integral operation, we use 7stacked non-linear integral operator
following the same spatial contracting and expansion of 2D spatial U-NO. Additionally, as the positional
embedding for the domain (unit torus T2) we used the Clifford torus (Appendix A.1) which embeds the
domain into Euclidean space. It is important to note that, for spatio-temporal FNOsetting, the domain of
the input function is extended to match the solution function as it maps between the function spaces with
same domains. Capable of mapping between function spaces with different domains, no such modification is
required for U-NO.
For each experiment, 10%of the total number of simulations Nare set aside for testing, and the rest is used
for training and validation. We experiment with the viscosity as Î½âˆˆ{1eâˆ’3,1eâˆ’4,1eâˆ’5,1eâˆ’6}, adjusting
the final time Tas the flow becomes more chaotic with smaller viscosities.
Table 3 demonstrates the results of the Navier-Stokes experiment. We observe that U-NOachieves the best
performance, with nearly a 50%reduction in the relative error over FNOin some experimental settings;
it even achieves nearly a 1.76%relative error on a problem with a Reynolds number2of2Ã—104(Fig.
2B). In each case, the U-NOâ€ with aggressive contraction ( and expansion) factor, performs significantly
better than FNOâ€“while requiring less memory. Also for neural operators performing 3Dintegral operation,
U-NOachieved37%lower relative error while requiring 50%less memory on average with 19times more
parameters. It is important to note that designing an FNOarchitecture matching the parameter count of
2Reynolds number is estimated as Re=âˆš
0.1
Î½(2Ï€)(3/2)(Chandler & Kerswell, 2013)
9Published in Transactions on Machine Learning Research (04/2023)
Table 3: Benchmarks on Navier-Stokes (Relative Error (%)). For the models performing a 2Dintegral
operator with recurrent structure in time, back-propagation memory requirement per time step is reported.
U-NOyields superior performance compared with U-NOâ€ andFNO, while U-NOâ€ is the most memory
efficient model. In the 3Dintegration setting, U-NOprovides significant performance improvement with
almost three times less memory requirement.
Avg. Î½= 1eâˆ’3Î½= 1eâˆ’4Î½= 1eâˆ’5Î½= 1eâˆ’6
model Mem. #Parameters T= 50s T = 30s T = 20s T = 15s
Req. Tin= 10s Tin= 10s Tin= 10s Tin= 6s
(MB) (Ã—106)N= 5000N= 11000 N= 11000 N= 11000
2DU-NO 16 15.3 0.28Â±2.1eâˆ’23.44Â±6.3eâˆ’22.94Â±4.9eâˆ’21.76Â±1.4eâˆ’2
U-NOâ€ 11 6.7 0.35Â±2.0eâˆ’23.56Â±4.3eâˆ’23.60Â±1.2eâˆ’22.2Â±1.5eâˆ’2
FNO 13 1.3 0.58Â±1.4eâˆ’25.57Â±7.7eâˆ’25.12Â±0.7eâˆ’23.33Â±0.7eâˆ’2
3DU-NO 108 24.0 0.31Â±3.9eâˆ’25.59Â±2.9eâˆ’17.03Â±4.6eâˆ’25.10Â±1.0eâˆ’1
FNO 216 1.3 0.68Â±0.8eâˆ’29.60Â±5.4eâˆ’28.67Â±1.2eâˆ’17.35Â±6.3eâˆ’1
U-NOis impractical due to high computational requirements (see Fig. 3). FNOarchitecture with similar
parameter counts demands enormous computational resources with multiple GPUs and would require a
long training time due to the smaller batch size. Also unlike FNO,U-NOis able to perform zero-shot
super-resolution in both time and spacial domain (Appendix A.5).
Table 4: Relative Error (%) achieved by 2D U-NOand 3D U-NOtrained and evaluated on high resolution
(256Ã—256) Simulations of Navier-Stokes. Even with highly constrained neural operator architecture and
smaller training data, the U-NOarchitecture can achieve comparable performance.
Models Memory Requirement Î½= 1eâˆ’3Î½= 1eâˆ’4Î½= 1eâˆ’5Î½= 1eâˆ’6
U-NO (MB) N= 2400N= 6000N= 6000N= 6000
2DU-NO 86 0.51 5.9 7.0 4.4
3DU-NO 850 0.83 8.3 11.2 8.2
Due toU-shaped encoding decodingstructureâ€“which allowus totrain U-NOon very highresolution ( 256Ã—256)
data on a single GPU with a 24GBmemory. To accommodate training with high-resolution simulations
of the Navier-Stocks equation, we have adjusted the contraction (or expansion) factors. The Operators in
the encoder part of U-NOfollow a contraction ratio of1
4and1
2for the spatial domain. But the co-domain
dimension is increased only by a factor of 2. And the operators in the decoder part follow the reciprocals
of these ratios. We have also used a smaller lifting dimension for the lifting operator P. Even with such
a compact architecture and lower training data, U-NOachieves lower error rate on high resolution data
(Table 4). These gains are attributed to the deeper architecture, skip connections, and encoding/decoding
components of U-NO.
4.6 Remarks on Memory Usage
Compared to FNO, the proposed U-NOarchitectures consume less memory during training and testing. Due
to the gradual encoding of input function by contracting the domain, the U-NOallow for deeper architectures
with a greater number of stacked non-linear operators and higher resolution training data (Appendix A.4).
ForU-NO, an increase in depth, does not significantly raise the memory requirement. The training memory
requirement for 3D spatio-temporal U-NOandFNOis reported in Fig. 3A.
We can observe a linear increase in memory requirement for FNOwith the increase of depth. This makes FNO
s unsuitable for designing and training very deep architectures as the memory requirement keep increasing
10Published in Transactions on Machine Learning Research (04/2023)
rapidly with the increase of the depth. On the other hand, due to the repeated scaling of the domain, we do
not observe any significant increase in the memory requirement for U-NO. This make the U-NOa suitable
architecture for designing deep neural operators. The improvement of the performance with the increase of
AB
Figure 3: (A) Training memory requirements (in MB) for the 3D spatio-temporal problem of Navier-Stokes
equation (Î½= 1eâˆ’3). For different depth only the number of stacked non-linear operators is varied. For
deeper models, the additive memory requirement of U-NOis negligible compared to FNOmodel. For
addition of 7more integral layer memory requirement only increased by only 80MB(vsâˆ¼400MB forFNO).
(B) Relative error in percentage on 3D spatio-temporal Navier-Stokes equation ( Î½= 1eâˆ’3) for different depth
(average over three repeated experiment is reported). We can observe a gradual decrease in the relative error
with the increase of depth.
depth is shown in Fig. 3B. This is expected as highly parameterized deep architectures allow for effective
approximation of highly complex non-linear operators. U-NOstyle architectures are more favorable for
learning complex maps between function spaces.
5 Conclusion
In this paper, we propose U-NO, a new neural operator architecture with a multitude of advantages as
compared with prior works. Our approach is inspired by Unet, a neural network architecture that is highly
successful at learning maps between finite-dimensional spaces with similar structures, e.g., image to image
translation. U-NOpossesses a similar set of benefits to Unet, including data efficiency, robustness to
hyperparameter tuning (Appendix A.2), flexible training, memory efficiency, convergence (Appendix A.6),
and non-vanishing gradients.This approach, while advancing neural operator research, significantly improves
the performance and learning paradigm, still carries the fundamental and inherent memory usage limitation of
integral operators. Nevertheless, U-NOallows for much deeper models incorporating the problem structures
and provides highly parameterized neural operators for learning maps between function spaces. U-NOs are
very easy to tune, possess faster convergence to desired accuracies, and achieve superior performance with
minimal tuning.
11Published in Transactions on Machine Learning Research (04/2023)
References
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical biasâ€“variance trade-off. Proceedings of the National Academy of Sciences , 116(32):
15849â€“15854, 2019.
E Oran Brigham. The fast Fourier transform and its applications . Prentice-Hall, Inc., 1988.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877â€“1901, 2020.
Claudio Canuto, M Yousuff Hussaini, Alfio Quarteroni, and Thomas A Zang. Spectral methods: fundamentals
in single domains . Springer Science & Business Media, 2007.
Gary J Chandler and Rich R Kerswell. Invariant recurrent solutions embedded in a turbulent two-dimensional
kolmogorov flow. Journal of Fluid Mechanics , 722:554â€“595, 2013.
Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and LÃ©on Bottou. Symplectic recurrent neural networks.
arXiv preprint arXiv:1909.13334 , 2019.
Yehuda Dar, Vidya Muthukumar, and Richard G Baraniuk. A farewell to the bias-variance tradeoff? an
overview of the theory of overparameterized machine learning. arXiv preprint arXiv:2109.02355 , 2021.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In International conference on machine learning , pp. 1675â€“1685. PMLR, 2019.
Lawrence C Evans. Partial differential equations , volume 19. American Mathematical Soc., 2010.
Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential
equations. Advances in Neural Information Processing Systems , 34, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770â€“778, 2016.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 ,
2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds
for fourier neural operators. Journal of Machine Learning Research , 22:Artâ€“No, 2021a.
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart,
and Anima Anandkumar. Neural operator: Learning maps between function spaces. arXiv preprint
arXiv:2108.08481 , 2021b.
Stig Larsson and Vidar ThomÃ©e. Partial differential equations with numerical methods , volume 45. Springer,
2003.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor
policies. The Journal of Machine Learning Research , 17(1):1334â€“1373, 2016.
Bian Li, Hanchen Wang, Xiu Yang, and Youzuo Lin. Solving seismic wave equations on variable velocity
models with fourier neural operator. arXiv preprint arXiv:2209.12340 , 2022.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,
and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv
preprint arXiv:2010.08895 , 2020a.
12Published in Transactions on Machine Learning Research (04/2023)
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,
and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv
preprint arXiv:2003.03485 , 2020b.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya,
and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations.
Advances in Neural Information Processing Systems , 33, 2020c.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation.
InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 3431â€“3440, 2015.
Siddhartha Mishra and Roberto Molinaro. Estimates on the generalization error of physics informed neural
networks (pinns) for approximating a class of inverse problems for pdes. arXiv preprint arXiv:2007.01138 ,
2020.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in
deep learning. Advances in neural information processing systems , 30, 2017.
Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza
Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A
global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint
arXiv:2202.11214 , 2022.
Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear partial
differential equations. Journal of Computational Physics , 357:125â€“141, 2018.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medical image computing and computer-assisted intervention ,
pp. 234â€“241. Springer, 2015.
Changjian Shui, Qi Chen, Jun Wen, Fan Zhou, Christian GagnÃ©, and Boyu Wang. Beyond H-divergence:
Domain adaptation theory with jensen-shannon divergence. arXiv preprint arXiv:2007.15567 , 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators.
arXiv preprint arXiv:2111.13802 , 2021.
Tapas Tripura and Souvik Chakraborty. Wavelet neural operator for solving parametric partial differential
equations in computational mechanics problems. Computer Methods in Applied Mechanics and Engineering ,
404:115783, 2023.
Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-fnoâ€“an enhanced
fourier neural operator based-deep learning model for multiphase flow. arXiv preprint arXiv:2109.03697 ,
2021.
Yan Yang, Angela F Gao, Jorge C Castellanos, Zachary E Ross, Kamyar Azizzadenesheli, and Robert W
Clayton. Seismic wave propagation and inversion with neural operators. The Seismic Record , 1(3):126â€“134,
2021.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM , 64(3):107â€“115, 2021.
13Published in Transactions on Machine Learning Research (04/2023)
A Appendix
A.1 Positional Embedding For Unit Torus
A unit torus T2is homeomorphic to the Cartesian product of two unit circles: S1Ã—S1. We can also consider
the cartesian product of the embedding of the circle. which produces Clifford torus. The Clifford torus can
be defined as
1
2S1Ã—1
2S1=/braceleftbigg1
2(sin(Î¸),cos(Î¸),sin(Ï•)cos(Ï•))|0â‰¤Î¸â‰¤2Ï€,0â‰¤Ï•â‰¤2Ï€/bracerightbigg
A.2 Sensitivity to Hyper-parameter Selection
Learning Rate
Depth
Figure 4: Result of sensitivity of U-NOâ€ to learning rate and number of stacked non-linear operator (depth)
used. All models are trained on the dataset of the Darcy Flow equation with resolution 211Ã—211following the
training protocol described in 4.4. Models for each of the configuration is trained three times and the average
error rate (in %) is reported. We can notice that except for a very high ( â‰¥0.01) or very low (â‰¤0.0001)
learning rate, U-NOâ€ achieves low error rate at every other configurations ( error rate achieved by FNO
is 0.85 ) . We also note that at all the high-performing hyper-parameter configuration setting U-NOhas low
generalization gap.
Table 5: Number of Parameters in U-NOâ€ models at different depth. We observe U-NOâ€ architecture can
efficiently learn large number of parameter ( 25times of the FNO) from very limited training data (only 1500
simulations) and can perform reasonably (See Table. 4)
.Depth 7 9 11 13 15 17
#Parameters (Ã—106)5.3 8.3 9.6 18.5 22.8 27.8
A.3 FNOwith Skip Connection
Table 6: Performance of vanilla 2D FNOequipped with skip connection on Navier-Stokes equations described
in Section 4.5 along with memory requirement during training. The result of 2D FNOis reported again for
comparison. We notice that skip connection alone does not improve the performance of FNO.
Models Memory Requirement Î½= 1eâˆ’3Î½= 1eâˆ’4Î½= 1eâˆ’5Î½= 1eâˆ’6
U-NO (MB) N= 2400N= 6000N= 6000N= 6000
FNOw. skip con. 13.14 MB 0.011 0.074 0.075 0.049
FNO 13.03 MB 0.009 0.072 0.074 0.052
A.4 Spatial Memory
The memory requirements to learn the operator w|(0,1)2Ã—[0,10]â†’w|(0,1)2Ã—(10,11]during back-propagation for
the Navier-Stokes problem are shown in Table 7. On average, for various tested resolutions, U-NOwith a
14Published in Transactions on Machine Learning Research (04/2023)
contraction (and expansion) factor of1
2requires 40% less memory than FNOduring training. This makes
U-NOand its variants more suitable for problems where high-resolution data are crucial, e.g., weather
forecasting model (Pathak et al., 2022).
Table 7: Memory requirements (in MB) for a single training instance of the Navier-Stokes equations for
different grid resolutions. All models have seven non-linear integral operator layers.
Spatial Resolution FNO U-NO U-NOâ€ 
64Ã—64 13.0 16.8 11.3
128Ã—128 76.1 67.3 45.4
256Ã—256 304.5 269.0 181.5
512Ã—512 1218.0 1076.0 726.0
1024Ã—1024 4872.0 4304.0 2990.9
A.5 Zero-Shot super resolution on 3D Spatio-Temporal Data
Table 8: Zero-shot super resolution result on 3D spatio-temporal Navier Stocks equation. The 3D FNOis not
resolution invariant in the time domain and due the specific construction can not process data at different
temporal resolution. But U-NOis resolution invariant both in time and spatial domain.
Spatial Res.Temporal Res.fps = 1 fps = 1.5 fps = 2 fps = 3
s=64 5.10 17.43 19.39 20.32
U-NOs=128 6.34 17.61 19.56 20.62
s=256 8.16 17.86 19.94 20.83
A.6 Superior Convergence Rate
AB
Figure 5: The training and test set error rate (in % on log scale) of U-NOandFNOfor Navier-Stokes
equation with viscosity 10eâˆ’3, (A) models performing 2Dspatio-temporal convolution (B) models performing
3Dspatial covolution. We can notice what U-NOconverges much faster than FNO. The final test set error
rate for FNOafter500epochs is achieved only after around 200epochs by U-NOand continues to improve
on the error rate.
A.7 Domain Contraction and Expansion
In this work, the domain contraction (or expansion) is performed by setting homeomorphism or mapping
between the domains of the input and output function of an integral operator. Following the notations of Sec.
15Published in Transactions on Machine Learning Research (04/2023)
4.3, an integral operator can be defined as
[GiVi](x) =Vi+1(x) =Ïƒ/parenleftbigg
Fâˆ’1/parenleftbig
RiÂ·F(vi)/parenrightbig
(s(x)) +Wivi(s(x))/parenrightbigg
Where with xâˆˆDi+1ands:Di+1â†’Diis a fixed homeomorphism between Di+1andDi. For the discussed
problems in this work (Darcy flow and Navier Stokes equation), the domains of the input and output functions
are bounded and connected. As a result, the mapping can be trivially established by scaling operation.
Lets the domain of the input and the output functions be (a,b)2and(c,d)2respectively, and the function
s: [c,d]â†’[a,b]is a homeomorphism between them. The function shas a linear form s(x) =a+mâ—¦(xâˆ’c)
withm= (bâˆ’a)âŠ˜(dâˆ’c)whereâ—¦andâŠ˜represents element wise multiplication and division. If c=a= 0,s
becomes a scaling operation on the domain. And the resultant output function can be efficiently computed
through interpolation with appropriate factors along each dimension.
A.8 Training Memory Requirement for 2D U-NOat Different Depths
6 7 8 9 10 11 12 13
Depth10.012.515.017.520.022.525.027.5Memory R equir ement (MB)FNO
U-NOâ€ 
U-NO
Figure 6: Memory requirements (in MB) for a single training instance of the Navier-Stokes equation during
back-propagation on input and outputs of resolution 64Ã—64. Number of stacked non-linear operators is varied
and the additive memory requirement of U-NOarchitectures for deeper models are negligible compared
toFNOmodel. For U-NOâ€ the addition of 7stacked non-linear integral operator increases the memory
requirement by only 2.5 MB
A.9 Comparison of Inference Time
As across different problem settings, the number of parameters of U-NOisâˆ¼8âˆ’25times more than the
FNO; it has more floating point operations (FLOPs). For this reason, the inference time of U-NOis more
than FNO(see Table 9). But it is important to note that even if t U-NOhasâˆ¼8âˆ’25times more parameters,
the inference time is only âˆ¼1âˆ’4times more. As U-NOin the encoding part gradually contracts the domain
of the functions, it reduces the time in the forward and inverse discrete Fourier transform.
16Published in Transactions on Machine Learning Research (04/2023)
Table 9: The running time of U-NOandFNOduring inference of a single sample. For the Darcy Flow
problem, we test on the resolution of 421Ã—421, and for Navier Stocks, we use the problem setting with
viscosity 10âˆ’3with the resolution of 64Ã—64. For Navier-Stocks 2D time to infer a single time step and for
Navier-Stocks 3D time to infer the whole trajectory are reported.
ProblemModel U-NO
(sec)FNO
(sec)
Darcy Flow 0.13 0.12
Navier-Stcoks (2D) 0.08 0.02
Navier-Stocks (3D) 0.33 0.09
17