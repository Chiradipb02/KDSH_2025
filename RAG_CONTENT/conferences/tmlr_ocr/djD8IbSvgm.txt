Published in Transactions on Machine Learning Research (10/2023)
Greedier is Better: Selecting Multiple Neighbors per
Iteration for Sparse Subspace Clustering
Jwo-Yuh Wu jywu@nycu.edu.tw
Institute of Communications Engineering
National Yang Ming Chiao Tung University
Liang-Chi Huang lchuang@nycu.edu.tw
Institute of Communications Engineering
National Yang Ming Chiao Tung University
Wen-Hsuan Li vincent@nycu.edu.tw
Institute of Communications Engineering
National Yang Ming Chiao Tung University
Chun-Hung Liu chliu@ece.msstate.edu
Department of Electrical and Computer Engineering
Mississippi State University
Rung-Hung Gau gaurunghung@nycu.edu.tw
Institute of Communications Engineering
National Yang Ming Chiao Tung University
Reviewed on OpenReview: https: // openreview. net/ forum? id= djD8IbSvgm
Abstract
Sparse subspace clustering (SSC) using greedy-based neighbor selection, such as orthogonal
matching pursuit (OMP), has been known as a popular computationally-efficient alternative
to the standard ℓ1-minimization based methods. However, existing stopping rules of OMP
to halt neighbor search needs additional offline work to estimate some ground truths, e.g.,
subspace dimension and/or noise strength. This paper proposes a new SSC scheme using
generalized OMP (GOMP), a soup-up of OMP whereby multiple, say p(≥1), neighbors are
identified per iteration to further speed up neighbor acquisition, along with a new stopping
rule requiring nothing more than a knowledge of the ambient signal dimension and the
numberpof identified neighbors in each iteration. Compared to conventional OMP (i.e., p=
1), theproposedGOMPmethodinvolvesfeweriterations, therebyenjoyingloweralgorithmic
complexity. Under the semi-random model, analytic performance guarantees are provided.
It is shown that, with a high probability, (i) GOMP can retrieve more true neighbors than
OMP, consequently yielding higher data clustering accuracy, and (ii) the proposed stopping
rule terminates neighbor search once the number of recovered neighbors is close to the
subspacedimension. Issuesaboutselecting pforpracticalimplementationarealsodiscussed.
Computer simulations using both synthetic and real data are provided to demonstrate the
effectiveness of the proposed approach and validate our analytic study.
1 Introduction
1.1 Motivation
Subspace clustering (Vidal, 2011; Yang et al., 2008; Goh & Vidal, 2007) is a key enabling technique in modern
unsupervised machine learning and its principles can be recapitulated as follows. Consider a noisy dataset
1Published in Transactions on Machine Learning Research (10/2023)
Algorithm 1 SSC-LASSO algorithm
Input: Observed dataset Y={y1,y2,...,yN}, data matrix Y= [y1...yN], regularization parameter λ>0
fori= 1toNdo
1)c∗
i= arg minλ∥ci∥1+1
2∥yi−Yci∥2
2s.t.ci,i= 0.
2) Normalize c∗
iand let c∗
i= [c∗
i,1...c∗
i,i−10c∗
i,i+1...c∗
i,N]T∈RN.
end for
3) Set C= [ci,j] = [c∗
1···c∗
N], and G= [gi,j], wheregi,j=|ci,j|+|cj,i|.
4) Form an N-node similarity graph in which the edge between nodes iandjhas edge weight gi,j.
5) Apply spectral clustering to the similarity graph.
Output: PartitionY=/hatwideY1∪...∪/hatwideY/hatwideL.
Y={y1,y2,...,yN}⊂Rnwhose ground truth obeys a disjoint union as
Y=Y1∪Y2∪...∪YL, (1)
where clusterYk⊂Rnconsists of|Yk|>0noisy data points coming from a dk-dimensional subspace Sk,
and|Y1|+...+|YL|=N. A partition of Yinto the form (1) is widely known as the union-of-subspaces
model (Vidal, 2011), which underpins a panoply of practical data clusters ranging from human face images,
hand-written digits, to trajectories of moving objects in videos. Given Ywith unknown Landdk,1≤k≤L,
the task of subspace clustering is to uncover the partition (1). Among existing solutions to this problem,
sparse subspace clustering (SSC) (Liu et al., 2013; Li et al., 2017; Lu et al., 2019; Elhamifar & Vidal,
2013), catalyzed by the witnessed success of compressive sensing (CS) and sparse representation (Baraniuk,
2007; Candès & Wakin, 2008; Davenport et al., 2011; Elad, 2010), has gained much attention because of its
compelling experimental performance and provable performance guarantees. A key ingredient of SSC is to
identify for each data point yia neighbor group by using the sparse representation technique. Formally, we
collect all data points in Yto form the matrix Y= [y1y2...yN]∈Rn×Nand try to express yias a linear
combination of columns of Yexcept yi, say, yi=Yci, where ci= [ci,1ci,2...ci,N]T∈RNsubject toci,i= 0.
SSC aims to find an optimal sparsesolution c∗
iso that the columns of Yindexed by the support of c∗
iare
correct neighbors of yi(i.e., from the same cluster). Computing c∗
iis typically done by solving the following
ℓ1-minimization problem (a.k.a. the Lasso regressor (Hastie et al., 2015))
c∗
i= arg minλ∥ci∥1+1
2∥yi−Yci∥2
2s.t.ci,i= 0, (2)
where 0< λ < 1is a regularization factor. Once c∗
iis obtained, SSC accordingly constructs a similarity
graph with the edge connecting yiandyjwith weight gi,j=|ci,j|+|cj,i|(see step 3 of Algorithm 1), followed
by spectral clustering (von Luxburg, 2007) for final data segmentation (see Algorithms 1 and 2 for outlines
of SSC-LASSO and spectral clustering algorithms, respectively). Solving problem (2) is computationally
demanding, especiallyforlarge-scalehigh-dimensionaldatasets. Therefore, low-complexityalternativesusing
greedy-based neighbor selection, e.g., orthogonal matching pursuit (OMP) (Davenport et al., 2011; Elad,
2010), were proposed to perform on par with ℓ1-minimization in many cases (see Algorithm 3 for an outline
of OMP-based SSC algorithm). SSC-OMP conducts neighbor identification by computing a sequence of
orthogonal projections. Specifically, for each yiOMP iteratively identifies one neighbor each time as the
data point yields the peak absolute inner product when paired with the residual vector (the initial residual
vector r(i)
0=yi). As a new neighbor is identified, its identity is added to the "already-detected" neighbor
index subset Λm, and the new residual vector r(i)
mis updated as the orthogonal projection r(i)
m= (I−
YΛm(YT
ΛmYΛm)−1YT
Λm)r(i)
m−1, inwhich YΛmconsistsofthecolumnsof Yindexedby Λm. Oncetheneighbor
identification process ends, the sparse representation vector c∗
iis then obtained by solving a least-squares
problem (see step 4 of Algorithm 3). Notably, existing stopping rules of OMP halt neighbor search if either
the number of iterations reaches a predesignated maximum Mor the residual power is below a threshold
τ(Tschannen & Bölcskei, 2018). While the number Mis closely related to the ground truth subspace
dimension, the threshold τis determined by the background noise strength. Hence, offline estimating the
subspace dimension and noise strength is necessary.
2Published in Transactions on Machine Learning Research (10/2023)
Algorithm 2 Spectral clustering algorithm
Input: Weighted adjacency matrix G= [gi,j]∈RN×N
1) Let L=I−A−1/2GA−1/2, where A=diag{a1,...,aN}andai=/summationtextN
j=1gi,j.
2) Estimate the number of clusters /hatwideL= mink|λk+1−λk|, whereλkis thekth smallest eigenvalue of L.
3) Find v1,v2,...,v/hatwideL, the eigenvectors of Lassociated with the /hatwideLsmallest eigenvalues of L, and set
V=/bracketleftig
v1/∥v1∥2v2/∥v2∥2...v/hatwideL/∥v/hatwideL∥2/bracketrightig
∈RN×/hatwideL.
4) Segment the Nrows of Vinto/hatwideLclusters using the K-means algorithm.
5) Declare yi∈/hatwideYlif theith row of Vis assigned to the lth cluster.
Output: PartitionY=/hatwideY1∪...∪/hatwideY/hatwideL.
Algorithm 3 SSC-OMP algorithm
Input: Observed dataset Y={y1,y2,...,yN}, data matrix Y= [y1...yN], maximum number of iterations
M, residual vector norm threshold τ
fori= 1toNdo
Letm= 0,r(i)
0=yi,Λ0=ϕ.
if(m<M )and(∥r(i)
m∥2>τ)then
1)m←m+ 1.
2)Λm= Λm−1∪j∗, wherej∗= arg max
1≤j̸=i≤N|⟨yj,r(i)
m−1⟩|.
3)r(i)
m= (I−YΛm(YT
ΛmYΛm)−1YT
Λm)r(i)
m−1.
end if
4) When the above procedure terminates with M(i)iterations, compute c∗
i= arg min
c:supp(c)⊂ΛM(i)∥yi−Yc∥2.
5) Normalize c∗
iand let c∗
i= [c∗
i,1...c∗
i,i−10c∗
i,i+1...c∗
i,N]T∈RN.
end for
6) Set C= [ci,j] = [c∗
1···c∗
N], and G= [gi,j], wheregi,j=|ci,j|+|cj,i|.
7) Form an N-point similarity graph in which the edge between nodes iandjhas edge weight gi,j.
8) Apply spectral clustering to the similarity graph.
Output: PartitionY=/hatwideY1∪...∪/hatwideY/hatwideL.
In addition to algorithm development, investigating the mathematical performance guarantees of SSC us-
ing fruitful analytical tools from CS also received considerable attention. The vast majority of related
works focused on investigating sufficient conditions ensuring the so-called subspace detection property (SDP)
(Soltanolkotabi & Candès, 2012), that is, neighbor identification is correct in the way that the coefficient
c∗
i,j̸= 0only if yiandyjare in the same cluster; see (Soltanolkotabi & Candès, 2012; Soltanolkotabi et al.,
2014; Wu et al., 2021; Wang & Xu, 2016; Wang et al., 2019) regarding the ℓ1-minimization solutions, and
(Dyer et al., 2013; Heckel & Bölcskei, 2015; You et al., 2016b; Tschannen & Bölcskei, 2018) pertaining to
greedy search. However, SDP is neither necessary nor sufficient for perfect data segmentation. Indeed, as
reported in many studies (Ng et al., 2001; Vershynin, 2018), a known type of similarity graph effectuating
successful clustering is one configured with many intra-cluster and few inter-cluster edges. Evidently, this
arises when the sparse regression misidentifies few neighbors, hence violating SDP. The downside of few
falsely directed edges from cluster to cluster can be effectively compensated by spectral clustering (see (von
Luxburg, 2007) for more discussions on this issue). Fulfillment of SDP does not guarantee correct cluster-
ing, especially when accompanied by meager neighbors. The reason is that, despite no inter-cluster edges,
the resultant similarity graph is cut into excessively many isolated pieces; poor graph connectivity in this
way tends to cause over-estimation of the number Lof clusters, leading to a large data clustering error1
(Soltanolkotabi et al., 2014; Wu et al., 2021). The above facts altogether shed further light on the study of
sparse regression for SSC. On the aspect of algorithm design, the efforts shall be particularly geared towards
1Once a similarity graph is constructed, e.g., using either ℓ1-minimization or greedy methods, one can further employ pruning
schemes such as Yang et al. (2020); Qin et al. (2023) to obtain an updated graph with improved connectivity.
3Published in Transactions on Machine Learning Research (10/2023)
fast acquisition of plentiful neighbors so that the similarity graph can be fleshed out in a right configuration.
As to neighbor recovery performance guarantees, much remains to be explored in search of new analysis
criteria that are not so stringent as SDP, especially able to reflect neighbor recovery error. In the frame-
work of two-step weighted ℓ1-minimization (Wu et al., 2021), the neighbor recovery rate was analyzed, and
specifically the probability that the sparse regressor produces at least kt(>0)correct and at most kf(≥0)
incorrect neighbors was found. Such a probabilistic characterization is intuitive and quite flexible in that it
directly takes account of the general case with neighbor misidentification. For the special case of kf= 0, i.e.,
error-free neighbor identification, it can reveal how much chance SDP stands with no less than ktrecovered
neighbors.
1.2 Paper Contributions
This paper aims at tackling the aforesaid challenges by revamping the OMP, considering its up-to-par
performance, reduced computational complexity, and, most importantly, the inherent flexibility to boost
neighbor acquisition. Pivoted on the Generalized OMP (GOMP), a prominent variant of OMP that is first
introduced in the literature of CS (Wang et al., 2012) and allowed to identify multiple neighbors per iteration,
we propose a new sparse regression scheme for SSC, together with an in-depth analytic study of its neighbor
recovery performance guarantee. Specific technical contributions of this paper are summarized as follows.
•We propose to employ GOMP as an effective alternative to OMP for fast neighbor identification.
In particular, we first point out that the deviation (caused by noise) of the residual vector from the
desired ground truth subspace, pinned down by the Angle of Deviation (AoD), plays a pivotal role
in neighbor identification. According to this fact, we then argue that GOMP, while digging out more
neighbors per iteration, enjoys a smaller AoD, which makes it more resilient to noise corruption and
able to achieve higher neighbor identification accuracy than OMP.
•Efficient stopping rules are crucial for greedy-based neighbor selection. If the algorithm stops early,
we would end up with scant correct neighbors, yet if late, with overly many false ones; either
case is apt to cause poor graph connectivity and eventual erroneous data clustering. Alongside
the proposed GOMP we devise a new stopping rule geared toward fulfilling the dimension-aware
property, that is, the algorithm is halted once the number of recovered neighbors is fairly close to
the subspace dimension (in general this is the right moment to leave off neighbor search since the
residual thereafter is typically dominated by noise). Mathematically, the proposed stopping rule
judges the ratio of residual norms over consecutive two iterations against a threshold dependent
on the ambient space dimension n, which is known once the dataset is given, and the number pof
neighbors identified per iteration that is at the designer’s disposal. Advantageously, this dispenses
withanextraofflineestimationofthesubspacedimensionornoisestrengthasrequiredintheexisting
solutions (Soltanolkotabi & Candès, 2012; Soltanolkotabi et al., 2014; Wu et al., 2021; Wang & Xu,
2016; Wang et al., 2019; Dyer et al., 2013; Heckel & Bölcskei, 2015; You et al., 2016b; Tschannen &
Bölcskei, 2018).
•Capitalized on the work (Wu et al., 2021) and under the semi-random model (Soltanolkotabi &
Candès, 2012; Soltanolkotabi et al., 2014), we conduct recovery rate analysis to bear out the claimed
meritsoftheproposedGOMPscheme. Supposingthatthegroundtruthsubspacesarewell-separated
from each other, we first derive an analytic probability lower bound for the event that at least km
neighbors ( 0≤km≤p) are correct (i.e., from the ground truth subspace) in the mth iteration. Such
a local iteration-wise recovery rate result is then exploited to obtain the global recovery rate, namely,
the probability lower bound for the event that at least ktcorrect neighbors in total are identified
throughout. The obtained analytic formula shows that, for a large data size Nand small noise
power, GOMP enjoys a higher correct neighbor recovery rate than the conventional OMP, thereby
confirming GOMP can facilitate fast acquisition of many correct neighbors. Finally, we show that,
with a high probability, the proposed stopping rule possesses the dimension-aware property.
•To implement the proposed GOMP method, the number pof neighbors identified per iteration
should be set beforehand. By further analyzing the obtained recovery rate formulae, the impact
4Published in Transactions on Machine Learning Research (10/2023)
ofpon the recovery rate is first discussed. For real-world datasets, oftentimes unable to meet the
assumptions required by the semi-random model, recovery rate analysis is rather difficult to carry
through. We, therefore, conduct numerical simulations to investigate the selection of paimed at
fulfilling the dimension-aware property. Interestingly, both the recovery rate analysis for the semi-
random model and our simulation study for real-world datasets indicate that a large pis preferred
when (i) the ground truth subspaces are well-separated from each other, or (ii) the data size Nis
large. Some guidelines for selecting pare suggested accordingly.
1.3 Connection to Previous Works
Efficient sparse regression for neighbor identification has played a pivotal role in the success of SSC. The
standardℓ1-minimization based method was first introduced in the landmark paper (Elhamifar & Vidal,
2013), and many related solutions have been proposed since then. In (Soltanolkotabi et al., 2014; Wu et al.,
2021), iterative re-weighted ℓ1-minimization was adopted to further improve neighbor identification accuracy
at the expense of higher algorithmic complexity. In (You et al., 2018; Peng et al., 2013; Matsushima & Brbic,
2019), computationally-efficient solutions under the framework of ℓ1-minimization were then proposed; the
basic idea therein was to pre-process a small amount of data points to acquire side information about the
ground truth subspaces, based on which a pruned dataset can then be used to reduce computations. Notably,
(You et al., 2016a; Panagakis & Kotropoulos, 2014) utilized mixed-norm regularization to further enhance
connectivity of the similarity graph. In contrast to ℓ1-minimization, greedy search such as OMP is one widely
considered low-complexity neighbor identification scheme (Dyer et al., 2013; Heckel & Bölcskei, 2015; You
et al., 2016b; Tschannen & Bölcskei, 2018). Recently in (Chen et al., 2018; Zhu et al., 2019), modified OMP
algorithms aiming at improving network connectivity have also been proposed; the methods therein utilized
the already established neighbor connections to narrow down the candidate neighbor list, overall promoting
neighbor recovery and consequently better network connectivity. It is worth noting that all the existing
OMP-based solutions identify one neighbor per iteration, and employ stopping rules calling for a knowledge
of the subspace dimension or noise strength. Boosting neighbor recovery via multi-neighbor identification
per iteration as well as the development of efficient stopping rules free from aforementioned side information
is not yet addressed in the literature of SSC.
Regarding the study of mathematical performance guarantees, sufficient conditions for SDP under the ℓ1-
minimization framework have been investigated in, say, (Elhamifar & Vidal, 2013; Soltanolkotabi & Candès,
2012)forthenoiselesscase,and(Soltanolkotabietal.,2014;Wang&Xu,2016;Wangetal.,2019)forthenoisy
case. Elhamifar & Vidal (2013) utilized convex geometry techniques to derive sufficient conditions tailored
for specialized subspace orientations (e.g., disjoint or independent subspaces), while Soltanolkotabi & Candès
(2012) dealt with the generalization to subspaces with a non-trivial intersection. Under noise corruption,
Soltanolkotabi et al. (2014) leveraged certain approximation of the LASSO functional and the restricted
isometry property of the noisy data matrix to estimate the probability that SDP holds; for LASSO sparse
regression, Wang & Xu (2016) further investigated sufficient conditions that the regularization parameter
must satisfy in order to guarantee SDP. Recently, Wu et al. (2021) extended the study in (Soltanolkotabi
et al., 2014) to provide recovery rate analyses for general neighbor recovery events. For SSC employing
greedy neighbor identification, Dyer et al. (2013) and You et al. (2016b) considered the noiseless scenario
and derived sufficient conditions for SDP using convex geometry analysis; Tschannen & Bölcskei (2018)
then extended the results in (Dyer et al., 2013) and (You et al., 2016b) to the Gaussian-noise setting, and
derived probability lower bounds for the event the SDP holds. As far as we can see, all existing studies of
performance guarantees for OMP-based SSC revolved around the fulfillment of SDP; the general case when
neighbor misidentification occurs is left unaddressed.
To sum up, while GOMP has been investigated in CS (Wang et al., 2012), its application and poten-
tial impacts on SSC remain yet to be explored. This paper is a first step toward this goal. Thanks to
multi-neighbor identification per iteration, the proposed GOMP method boosts neighbor recovery at lower
algorithmic complexity as compared to conventional OMP. In addition, our newly developed stopping rule
enjoys the dimension-aware property, thereby free from off-line subspace dimension estimation. Moreover,
we leverage recovery rate analysis to derive mathematical performance guarantees for general neighbor re-
5Published in Transactions on Machine Learning Research (10/2023)
covery events. In view of the above achievements, our study of SSC with GOMP can contribute to more
well-rounded literature on SSC under the framework of greedy neighbor selection.
Therestofthispaperisorganizedasfollows. Section2firstexplainswhyGOMPcanoutperformconventional
OMP, and then introduces the foundations behind the proposed stopping rule. Afterwards, the algorithmic
complexityofOMPandtheproposedGOMPareanalyzed. Section3analyzestherecoveryrateanddiscusses
the issue of selecting the number pof recovered neighbors per iteration. Section 4 provides numerical
simulations to verify our theoretical findings and discussions in Section 3. Section 5 presents the proofs
of the main mathematical results. Finally, Section 6 concludes this paper. To ease reading, some detailed
technical proofs are relegated to the appendix.
2 Proposed SSC-GOMP
This section introduces the proposed SSC-GOMP scheme. We first brief in Section 2.1 the reason why
multiple neighbor recovery in each iteration is favored, in an attempt to motivate our GOMP proposal. In
Section 2.2, we then encapsulate the foundations behind the proposed stopping rule. Finally, in Section 2.3
we provide algorithmic complexity analysis to justify the computational efficiency of the proposed GOMP
as compared to OMP.
2.1 Why Multiple Neighbor Recovery per Iteration?
Recall that OMP iteratively identifies a neighbor each time as the data point when paired with the residual
vector yields peak absolute inner product (see step 2 of Algorithm 3). Hence, the orientation of the residual
vector in each iteration, in particular, the degree to which it deviates from the ground truth subspace, is
important for accurate neighbor identification. To formalize this notion, assume that we are to build a
neighbor list for the data point yicoming from the cluster Ykwhose ground truth subspace is Sk. Impaired
by noise, the residual vector r(i)
mcomputed in the mth iteration ( m≥1) is perturbed outwards Sk. If
we write r(i)
m=r(i)
m,∥+r(i)
m,⊥, where r(i)
m,∥∈Skandr(i)
m,⊥∈S⊥
k(the orthogonal complement of Sk), such
perturbation can be pinned down by the angle of deviation (AoD)
ϕi
m≜tan−1(∥r(i)
m,⊥∥2/∥r(i)
m,∥∥2) (3)
whereby a large ϕi
mmeans r(i)
mseverely deviates from Sk. As the OMP algorithm iterates, perturbation
of the residual would become increasingly severe. This is mainly because, owing to orthogonal projection
(see step 3 of Algorithm 1), the current residual r(i)
mis obtained by removing from the previous r(i)
m−1the
component lying in the subspace spanned by the already-selected neighbors, most of which are likely correct.
Consequently, the magnitude ∥r(i)
m,∥∥2of the component r(i)
m,∥∈Skdiminishes from iteration to iteration;
instead, the term ∥r(i)
m,⊥∥2, which reflects the strength of projected misidentified neighbors (if any) plus noise
ontoS⊥
k, is typically non-decreasing with m. Put together, the cascade effect is, therefore, an increase in
ϕi
mwithm, rendering the residual r(i)
mmore and more prone to neighbor misidentification as the algorithm
iterates. In this regard, a simple remedy for securing enough correct neighbors in few iterations (a “small
m” is favored) is therefore to identify multiple neighbors per iteration, say, the pdata points ( p > 1)
corresponding to the largest pabsolute inner products. GOMP is therefore a potential solution to meet
this goal. Using synthetic data, Fig. 1-(a) clearly demonstrates GOMP yields smaller average AoD than
OMP thanks to fewer iterations2; this accordingly brings about higher neighbor identification accuracy, as
illustrated in Fig. 1-(b) (this issue will be elaborated in Section 3). The reduction in the number of iterations
can moreover reduce algorithmic complexity, which is potentially appealing in real-time applications. Indeed,
the major computational bottleneck of the OMP algorithm is the orthogonal projection operation (step 3
in Algorithm 3). To recover p(>1)neighbors, conventional OMP requires piterations, hence porthogonal
projections. Instead, GOMP calls for just one iteration, so one orthogonal projection only, and therefore is
more computationally efficient (detailed algorithmic complexity comparison of OMP and GOMP is given at
2An analytic study of average AoD for GOMP/OMP is rather challenging, and is one of our future works.
6Published in Transactions on Machine Learning Research (10/2023)
Figure 1: Comparison of GOMP and OMP in terms of average AoD and empirical recovery rate. We
consider a synthetic data set of 135 vectors drawn from L= 3orthogonal subspaces, each of a dimension
9, in an ambient domain R100; 45 data points per cluster. The data vectors are sampled uniformly from
the intersection of the unit-sphere in R100with the ground truth subspace and are corrupted by zero mean
Gaussian noise with variance 0.04. For GOMP, p= 3neighbors are picked per iteration as those when
matched to the residual yielding the largest three absolute inner products. A total number of 9 neighbors
are recovered using both OMP and GOMP. (a) Left: plot of average AoD upon detection of the kth neighbor,
1≤k≤9. For GOMP, every three neighbors are detected in each iteration based on the same residual,
leading to a staircase AoD curve. Clearly, GOMP results in smaller AoD thanks to fewer iterations. (b)
Right: plot of the true neighbor rate, i.e., the fraction of true neighbors recovered, versus the index kof the
detected neighbor. Benefiting from smaller AoD, GOMP is seen to improve neighbor identification accuracy.
Figure 2: An illustration of AoD of GOMP and OMP in the presence of data coherence. We consider
a synthetic dataset of 273vectors drawn from L= 3orthogonal subspaces, each of a dimension 9, in an
ambient domain R100;91data points per cluster. To generate coherent data, in each cluster the first 46data
points are uniformly drawn from the intersection of the unit-sphere in R100and the ground truth subspace
and are corrupted by zero-mean Gaussian noise with variance 0.04, while the remaining 47th∼91th points
are "small perturbations" of the 2nd∼46th points, by an additive zero mean Gaussian noise with variance
10−4. The inherent case, wherein all 91data vectors per cluster are uniformly generated as above, is included
as the baseline. We set p= 3for GOMP. (a) Plots of AoD when identifying neighbors for the 1st data point.
Since the neighboring points are pair-wise strongly coherent, it is highly likely a coherent pair is picked in
each iteration. For both OMP and GOMP, data coherence even reduces AoD as compared to the inherent
case. (b) The true neighbor recovery rates are higher, thanks to smaller AoD.
the end of this section). Considering all the above facts, we thus propose to adopt GOMP in place of OMP
for neighbor identification.
Remark: Finally, we would like to comment on the the case when coherent data points are present. Assume
that two correct neighbors of yiare coherent (so that they are aligned toward the same direction), and both
are selected by GOMP in the same iteration. As such, the dimension of the subspace identified throughout
this iteration is p−1, rather than pas in the inherent case (i.e., all data points are sufficiently uncorrelated
with each other). To obtain the new residual vector r(i)
m, the current residual r(i)
m−1is projected onto the
orthogonal complement of the span of the "already-selected" neighbors, which is of a higher dimension (one
more) than the inherent case. In this way, the resultant signal component r(i)
m,∥is better retained, leading to
a larger∥r(i)
m,∥∥2and consequently a smaller AoD = tan−1(∥r(i)
m,⊥∥2/∥r(i)
m,∥∥2)(since the strength ∥r(i)
m,⊥∥2of
the projected misidentified neighbors (if any) plus noise onto S⊥
k, is roughly the same in both cases). This
is confirmed by our experimental study as illustrated in Fig. 2. The above results indicate that GOMP can
work well even in the presence of coherent data points.
7Published in Transactions on Machine Learning Research (10/2023)
2.2 Halt When There Are About as Many Neighbors as Subspace Dimension
Since the data point yicomes from the dk-dimensional ground truth subspace Sk, a group of around dktrue
neighbors would reach a “critical mass” to well explain yiand, if so, the residual from then on is highly apt
to be dominated by noise, standing very little chance to uncover more true neighbors (this will be born out
by our mathematical analysis in Section 3). Grounded on this fact, the algorithm is expected to be halted
oncedkneighbors or so are available. We shall recall the stopping rule widely adopted in the literature
(You et al., 2016b; Tschannen & Bölcskei, 2018), which terminates neighbor search when either the number
of iterations reaches a pre-set maximum M, or the residual becomes so small that ∥r(i)
m∥2≤τfor some
thresholdτ > 0. Though implicit, this assumes the availability of prior knowledge about Mandτ; the
former is arguably all about the subspace dimension dkand the latter is closely related to the background
noise strength, both of which can only be acquired through extra off-line estimation process. Considering
that the dedicated overhead of parameter estimation could be costly, below we develop a new stopping rule
which is per se aware of the subspace dimension without the need of knowing Morτ.
To introduce the proposed approach, let us write the data point under consideration as yi=xi+ei, where
xiis the noiseless signal point and eiis the additive noise. The residual vector r(i)
m, which can be obtained
fromyithrough a sequence of morthogonal projections (step 3 of Algorithm 3), can be expressed as
r(i)
m=m/productdisplay
l=1Plyi=m/productdisplay
l=1Pl(xi+ei) =m/productdisplay
l=1Plxi+m/productdisplay
l=1Plei, (4)
where Plis the orthogonal projection onto the orthogonal complement of the subspace spanned by the
already-selected neighbors up to the lth iteration. Assume that most of the recovered neighbors up to the
miterations are from the correct subspace Skso that the projected signal is very small and the residual r(i)
m
is strongly dominated by the projected noise, i.e., r(i)
m≈/producttextm
l=1Plei. This is typically the case once about as
many neighbors as the subspace dimension are recovered. If the noise eiis Gaussian, so is the residual r(i)
m,
which, being nearly isotropic, tends to distribute its power evenly over all the dimensions (about n−dk)
of the orthogonal complement of the subspace spanned by all the already-selected neighbors; that is to say,
each dimension shares a factor 1/(n−dk)of the total power ∥r(i)
m∥2
2. During the (m+ 1)th iteration, r(i)
m+1is
then obtained from r(i)
mby removing from it the components along the pnewly selected neighbors, implying
that∥r(i)
m−r(i)
m+1∥2
2is close top×∥r(i)
m∥2
2/(n−dk)≈p×∥r(i)
m∥2
2/n, in which the approximation makes sense
since the subspace dimension dkis in general very small in comparison with the ambient space dimension n.
Taking the square root and using the triangle inequality, we then obtain the following condition to halt the
neighbor search
∥r(i)
m∥2−∥r(i)
m+1∥2≤(∥r(i)
m∥2√p)/√n, (5)
or equivalently,
∥r(i)
m+1∥2
∥r(i)
m∥2≥1−/radicalbig
p/n. (6)
The proposed halting rule (6) is dimension-aware because, for most cases, it is triggered once the residual
r(i)
mis dominated by noise owing to the recovery of dkneighbors or thereabouts. Notably, the left-hand-side
of (6) is a random variable and, thus, there is no way of ensuring (6) always holds. Instead, under Gaussian
noise assumption it is shown in Section 5.3 that, with m=⌈dL/p⌉, inequality (6) holds with a probability
higher than 1−2pe−√
n/p. As a result, as p/nis close to zero (thus, n/pis very large), the proposed stopping
rule (6) can highly likely be triggered.3The proposed halting rule (6) is dimension-aware because, for most
cases, it is triggered once the residual r(i)
mis dominated by noise owing to the recovery of dkneighbors or
thereabouts. We should moreover note that the left-hand-side of (6) admits the form of a residual norm
ratio, which advantageously rids off the knowledge of noise strength. Indeed, since r(i)
m≈/producttextm
l=1Plei, we have
∥r(i)
m∥2≈∥m/productdisplay
l=1Plei∥2=αm∥ei∥2,for some 0<αm<1. (7)
3Such a probabilistic interpretation of stopping criteria is not uncommon, e.g., the widely considered stopping rule ∥r(i)
m∥2<τ
(Tschannen & Bölcskei, 2018) cannot be guaranteed to meet deterministically, as ∥r(i)
m∥2is a random variable.
8Published in Transactions on Machine Learning Research (10/2023)
Algorithm 4 SSC-GOMP algorithm with the proposed data-dependent stopping criterion
Input: Observed data set Y={y1,y2,...,yN}, data matrix Y= [y1...yN]
fori= 1toNdo
Letm= 0,r(i)
0=yi,r(i)
−1= 2yi,Λ0=ϕ.
if(1−∥r(i)
m∥2/∥r(i)
m−1∥2≥/radicalbig
p/n)then
1)m←m+ 1.
2)Λm= Λm−1∪Tm, whereTmis the set of cardinality psuch that
|⟨yj,r(i)
m−1⟩|≥|⟨ yq,r(i)
m−1⟩|,∀j∈Tm, q /∈Tm.
3)r(i)
m= (I−YΛm(YT
ΛmYΛm)−1YT
Λm)r(i)
m−1.
end if
4) When the above procedure terminates with M(i)iterations, compute c∗
i= arg min
c:supp(c)⊂ΛM(i)−1∥yi−Yc∥2.
5) Normalize the column vector c∗
ito be unit-norm. Let c∗
i= [c∗
i,1...c∗
i,i−10c∗
i,i+1...c∗
i,N]T∈RN.
end for
6) Set C= [ci,j] = [c∗
1···c∗
N], and G= [gi,j], wheregi,j=|ci,j|+|cj,i|.
7) Form an N-point similarity graph in which the edge between nodes iandjhas edge weight gi,j.
8) Apply spectral clustering to the similarity graph.
Output: PartitionY=/hatwideY1∪...∪/hatwideY/hatwideL.
The condition (7) motivates the stopping rule of the form You et al. (2016b); Tschannen & Bölcskei (2018),
whereby the threshold τdepends on the noise strength ∥ei∥2. Using (7), the residual norm ratio in the
proposed stopping rule (6) reads
∥r(i)
m+1∥2
∥r(i)
m∥2≈αm+1
αm,for some 0<αm+1<αm<1. (8)
which is clearly independent of the noise strength ∥ei∥2. The decision threshold in (6) assumes nothing more
than a knowledge of the ambient dimension n, which is always known in advance, and the parameter p, which
is at the designer’s disposal. As such, it is free from the need of an extra offline estimation of the subspace
dimension or noise strength, in marked contrast with the existing solutions You et al. (2016b); Tschannen &
Bölcskei (2018). We summarize the proposed SSC-GOMP algorithm in Algorithm 4. We note from step 4)
of Algorithm 4 that the support of c∗
iisΛM(i)−1, namely, the indexes of "already-detected" neighbors up to
the(M(i)−1)th iteration, thereby precluding those identified in the M(i)th iteration, i.e., j∈TM(i). To see
the reason behind this, recall from step 3) of Algorithm 4 that the residual in the M(i)th iteration reads
r(i)
M(i)=r(i)
M(i)−1−YΛM(i)(YT
ΛM(i)YΛM(i))−1YT
ΛM(i)r(i)
M(i)−1. (9)
If the stopping condition (6) is triggered, we have ∥r(i)
M(i)−1∥2≈∥r(i)
M(i)∥2, which together with (9) implies
YΛM(i)(YT
ΛM(i)YΛM(i))−1YT
ΛM(i)r(i)
M(i)−1≈0. (10)
Since ΛM(i)= ΛM(i)−1∪TM(i), (10) implies that r(i)
M(i)−1is nearly orthogonal to yj,j∈TM(i), which are
therefore unlikely to be neighbors of yi.
Remark: In case that the projected signal component/producttextm
l=1Plxiis non-zero, our arguments of deriving (6)
remains valid. This is because, as long as sufficiently many true neighbors are recovered, the term/producttextm
l=1Plxi
actually acts as a noise; thus, the residual r(i)
m=/producttextm
l=1Plxi+/producttextm
l=1Pleiis indeed “noise-only” irrespective
of whether/producttextm
l=1Plxiis zero or not. To see this, let us stack the “already-selected” neighbors up to the mth
iteration as a matrix /tildewideY(i)
m∈Rn×pm, which admits the form /tildewideY(i)
m=/tildewideX(i)
m+/tildewideE(i)
m, where/tildewideX(i)
mand/tildewideE(i)
mare the
signal point and noise matrices, respectively. In case there are sufficiently many correct neighbors recovered,
we have xi=/tildewideX(i)
mcfor some c. In this way, the projected signal component reads
9Published in Transactions on Machine Learning Research (10/2023)
Table 1: Comparison of GOMP and OMP in terms of running time ( ×10−5second).
number of iteration 1 2 3 4 5 6 7 8 9
OMP (p= 1) 3.2 3.4 3.4 3.5 3.6 3.7 3.8 3.9 4.0
GOMP (p= 3) 3.9 4.1 4.2 × × × × × ×
m/productdisplay
l=1Plxi=m−1/productdisplay
l=1Pl/parenleftig
I−/tildewideY(i)
m(/tildewideY(i)
mT/tildewideY(i)
m)−1/tildewideY(i)
mT/parenrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Pmxi
=m−1/productdisplay
l=1Pl/parenleftig
I−/tildewideY(i)
m(/tildewideY(i)
mT/tildewideY(i)
m)−1/tildewideY(i)
mT/parenrightig
(xi−/tildewideY(i)
mc)
=m−1/productdisplay
l=1Pl/parenleftig
I−/tildewideY(i)
m(/tildewideY(i)
mT/tildewideY(i)
m)−1/tildewideY(i)
mT/parenrightig
(xi−/tildewideX(i)
mc−/tildewideE(i)
mc)
=m−1/productdisplay
l=1Pl/parenleftig
I−/tildewideY(i)
m(/tildewideY(i)
mT/tildewideY(i)
m)−1/tildewideY(i)
mT/parenrightig
(−/tildewideE(i)
mc) =m/productdisplay
l=1Pl(−/tildewideE(i)
mc).(11)
The residual vector accordingly becomes r(i)
m=/producttextm
l=1Pl(−/tildewideE(i)
mc)+/producttextm
l=1Plei=/producttextm
l=1Pl(−/tildewideE(i)
mc+ei), which
is again a projected noise. Hence, the proposed stopping rule (6) makes sense and still works even/producttextm
l=1Plei
is non-zero.
2.3 Algorithmic Complexity
We end this section by analyzing the algorithmic complexity of the proposed GOMP and OMP. To ease
discussion, assume that pMneighbors are to be recovered. Then OMP requires pMiterations, in which the
mth iteration has complexity O(nN+nm+nm2+m3),1≤m≤pM, respectively (Sturm & Christensen,
2012). Instead, GOMP requires just Miterations, the mth iteration with complexity O(nN+npm +
n(pm)2+ (pm)3),1≤m≤M, respectively. Notice that, for 1≤m≤M, thepmth iteration of OMP
has the same complexity as the mth iteration of GOMP. Hence, it is clear that the OMP scheme calls for
additional (p−1)Miterations, among which the complexity is O(nN+nm+nm2+m3)for1≤m≤pM,
m̸=p,2p,...,Mp . To further justify the low-complexity advantage of the proposed GOMP, we consider the
data set used in Fig. 1 and compare the running time of the proposed GOMP ( p= 3) and OMP ( p= 1);
the results are listed in Table 1. From the table, we observe the following.
•Running time increases with m, the number of iteration. This is mainly because, as mincreases, the
number of the identified neighbors, and hence the dimension of the least squares problem, in step
4) increases, leading to longer running time.
•Even though the mth iteration of GOMP and the pmth iteration of OMP have the same order of
complexity, the running time of the former is slightly higher. The reason behind this is that, while
OMP computes the maximal absolute inner product, GOMP seeks the first plargest ones: this
entails additional sorting efforts and therefore higher running time. Despite this, GOMP involves
(p−1)Mless iterations, and overall less running time, than OMP. Based on Table 1, the running
time of GOMP is about 37.5% of that of OMP.
3 Theoretical Results
In this section, we present the recovery rate analysis for the proposed SSC-GOMP. Under the semi-random
model assumption, Section 3.1 first derives analytic recovery rate formulae. Section 3.2 then discusses the
guidelines for selecting the number pof recovered neighbors per iteration.
10Published in Transactions on Machine Learning Research (10/2023)
3.1 Recovery Rate Analysis
To formalize matters, the data vector is assumed to follow the standard additive noise model, that is,
yi=xi+ei,1≤i≤N, (12)
where xi∈Rnis the unit-norm noiseless signal vector and ei∈Rnis the noise. The analyses below are
built on the popular semi-random model (Soltanolkotabi & Candès, 2012; Soltanolkotabi et al., 2014; Wu
et al., 2021; Wang & Xu, 2016), i.e., the ground truth subspaces S1,...,SLin the partition (1) are fixed but
otherwise unknown, whereas the data vectors and noise are random. Such a model is widely used in the
theoretical study of SSC, thanks to its interpretability and amiability to analysis. Similar to the previous
works(Wuetal.,2021;Wangetal.,2019;Heckel&Bölcskei,2015;Tschannen&Bölcskei,2018), thefollowing
assumptions are made in the sequel.
Assumption 1. For each 1≤i≤N, the signal vector xi∈Rnis uniformly sampled from Bk, where
Bk≜{x|x∈Rn,∥x∥2= 1}∩Skis the intersection of the unit sphere with the subspace Sk.
Assumption 2. For each 1≤i≤N, the noise ei∈Rn,1≤i≤N, are i.i.d. Gaussian random vectors
with zero mean and covariance matrix (σ2/n)I, and are independent of the signal vectors xi’s.
To gauge the degree to which two subspaces are separated away from each other, we recall the affinity
between two distinct subspaces SkandSlthat is defined to be (Soltanolkotabi & Candès, 2012)
aff(Sk,Sl)≜∥UT
kUl∥F/radicalbig
min{dk,dl}, (13)
where columns of Uk(Ul, respectively) form an orthonormal basis for Sk(Sl, respectively).
Assumption 3. The subspace affinity satisfies
max
k,k̸=laff(Sk,Sl)+9√
3dL(1 +σ)
(8−12σ)√n−dLlogN≤τ
4 logN, (14)
where 0<τ < 1.
Notably, Assumptions 3 guarantees that different subspaces are well separated from each other; affinity
conditions akin to (14) are also needed in many existing studies of performance guarantees for SSC (Wu
et al., 2021; Wang et al., 2019; Heckel & Bölcskei, 2015; Tschannen & Bölcskei, 2018). Without loss of
generality, we assume in the sequel xN∈SL, therefore yN∈YL, and our goal is to identify a neighbor group
ofyN. Under the above three assumptions it can be shown that GOMP stands a high chance to recover
many true neighbors in each of the first ⌈dL/p⌉iterations. More precisely, we have the following theorem.
Theorem 1. (Iteration-Wise Recovery Rate): Let {k1,k2,...,kM}be a sequence of integers satisfying 0≤
km≤p, for 1≤m≤M≤⌈dL/p⌉. Under Assumptions 1 to 3, the proposed SSC-GOMP obtains at least km
true neighbors at the mth iteration, 1≤m≤M, with a probability exceeding
1−Ne−n/8−6/parenleftbiggσ√π/parenrightbiggdL−p(M−1)
−M/summationdisplay
m=1/bracketleftigg/parenleftigg/parenleftbigg2e(N−|YL|)
(p−km+1)N8 logN/dL/parenrightbiggp−km+1
+/parenleftig/radicalbigg
2
πτ/parenrightig|YL|−dL−km/parenleftige(|YL|−1)
km−1/parenrightigkm−1
+4 + 2c
N2/parenrightigg
1(km>0)/bracketrightigg
,(15)
where 1(•)is the indicator function, c>0is a constant, and dLis the dimension of the subspace SL.
Proof :See Section 5.1.
Further scrutiny reveals the lower bound (15) is high in most practical cases. Indeed, with a large ambient
dimensionnand small noise level σ, the second and third terms in (15) are kept small; regarding the last
summation, the first and third terms scale like N[1−(8 logN/dL)](p−km+1)andN−2, respectively, whereas the
11Published in Transactions on Machine Learning Research (10/2023)
second term decays exponentially fast as |YL|increases, thanks to 0< τ < 1(see Assumption 3). When
specialized to p= 1andkm= 1for all 1≤m≤M, i.e., the case with conventional OMP subject to all
detected neighbors being true, the lower bound (15) then reads
1−Ne−n/8−6/parenleftbiggσ√π/parenrightbiggdL−p(M−1)
−M
2e(N−|YL|)
N8 logN/dL+/parenleftigg/radicalbigg
2
πτ/parenrightigg|YL|−dL−1
+4 + 2c
N2
.(16)
Notably, a probability lower bound akin to (16) was also reported in (Tschannen & Bölcskei, 2018), which
addressed sufficient conditions ensuring correct neighbor recovery. Based on the iteration-wise recovery
rate result given in Theorem 1, the following corollary further establishes the global recovery rate, namely,
the probability of the event that GOMP succeeds in recovering a specified total number of true neighbors
throughout the M≤⌈dL/p⌉iterations.
Corollary 1. (Global Recovery Rate): Let 0≤kt≤pMbe an integer and write kt=Mqt+rt, where
0≤rt≤M−1. Under the same setup as in Theorem 1, GOMP can recover at least kttrue neighbors in
total throughout M(≤⌈dL/p⌉)iterations with a probability higher than
1−Ne−n/8−6/parenleftbiggσ√π/parenrightbiggdL−p(M−1)
−rt
/parenleftbigg2e(N−|YL)|
(p−qt)N8 logN/dL/parenrightbiggp−qt
+/parenleftigg/radicalbigg
2
πτ/parenrightigg|YL|−dL−qt−1/parenleftbigge(|YL|−1)
qt/parenrightbiggqt
+4 + 2c
N2

−(M−rt)
/parenleftbigg2e(N−|YL|)
(p−qt+ 1)N8 logN/dL/parenrightbiggp−qt+1
+/parenleftigg/radicalbigg
2
πτ/parenrightigg|YL|−dL−qt/parenleftbigge(|YL|−1)
qt−1/parenrightbiggqt−1
+4 + 2c
N2
1(qt>0).(17)
Proof :See Section 5.2.
By following our examination of (15), it is easy to check the probability lower bound (17) is high in most
practical cases. Moreover, with the aid of (17), GOMP is seen to yield a higher true neighbor recovery
rate as compared with the conventional OMP. To better illustrate this, we assume without loss of generality
that a group of pMneighbors is to be found, while demanding at least kt=kMtrue neighbors, for some
1< k≤p(that is to say, k/pof the total neighbors are true). Hence, GOMP requires Miterations, and
OMPptimes more. Under this setting, the lower bound (17) for GOMP reads
1−Ne−n/8−6/parenleftbiggσ√π/parenrightbiggdL−pM+p
−M/parenleftbigg2e(N−|YL|)
(p−k+ 1)N8 logN/dL/parenrightbiggp−k+1
−M/parenleftigg/radicalbigg
2
πτ/parenrightigg|YL|−dL−k/parenleftbigge(|YL|−1)
k−1/parenrightbiggk−1
−(4 + 2c)M
N2,(18)
whereas for OMP the bound becomes
1−Ne−n/8−6/parenleftbiggσ√π/parenrightbiggdL−pM+1
−2kMe (N−|YL|)
N8 logN/dL−kM/parenleftigg/radicalbigg
2
πτ/parenrightigg|YL|−dL−1
−(4 + 2c)kM
N2.(19)
Note that (18) differs from (19) in the last four terms, among which it is clear that 6 (σ/√π)dL−pM+p<
6 (σ/√π)dL−pM+1for small noise level σ, and(4+2c)M
N2<(4+2c)kM
N2becausek > 1. Since the 4thterm
M/parenleftig
2e(N−|YL|)
(p−k+1)N8 logN/dL/parenrightigp−k+1
in (18) and the 4thterm2kMe(N−|YL|)
N8 logN/dLin (19) scale like N[1−(8 logN/dL)](p−k+1)
andN[1−(8 logN/dL)], respectively, both vanish whenever the data size Nis very large. Also, since τ < 1,
both the 5thtermM/parenleftig/radicalig
2
πτ/parenrightig|YL|−dL−k/parenleftig
e(|YL|−1)
k−1/parenrightigk−1
in (18) and the 5thtermkM/parenleftig/radicalig
2
πτ/parenrightig|YL|−dL−1
in (19)
decay exponentially fast to zero as the cluster size |YL|grows, therefore negligible. Accordingly, when the
12Published in Transactions on Machine Learning Research (10/2023)
subspaces are well separated from each other and the data/cluster size is large enough, GOMP improves the
true neighbor recovery rate.
Theorem 1 and Corollary 1 specify the neighbor recovery rate up to the ⌈dL/p⌉th iteration, upon which
GOMP is expected to be halted by the proposed stopping rule (6) thanks to the availability of about dL
recovered neighbors. Such a conjecture is provably true with a high probability, as established in the next
theorem.
Theorem 2. Under the same setup as in Theorem 1, GOMP accompanied by the proposed stopping rule (6)
halts neighbor search till the ⌈dL/p⌉th iteration with a probability exceeding
1−Ne−n/8−6/parenleftbiggσ√π/parenrightbiggdL−p(⌈dL/p⌉−1)
−2pe−√
n/p
−/ceilingleftigdL
p/ceilingrightig/bracketleftigg
2e(N−|YL|)
N8 logN/dL+/parenleftig/radicalbigg
2
πτ/parenrightig|YL|−dL−p/parenleftige(|YL|−1)
p−1/parenrightigp−1
+4 + 2c
N2/bracketrightigg
.(20)
Proof :See Section 5.3.
Under Assumptions 1 to 3, Theorem 2 provides an analytic probability lower bound for the event that the
proposed stopping rule (6) is aware of the subspace dimension, i.e., it terminates the GOMP algorithm when
the number of recovered neighbors is close to the subspace dimension dL. We further remark that when
the noise power increases slightly further, the dimension of the span of data points in YLwould exceed dL;
accordingly, the algorithm would stop with more than ⌈dL/p⌉iterations in order to recover more neighbors
commensuratewiththeincreaseddimension. Asthenoisepowergrowshigher, theresidualissoondominated
by noise so as to trigger the stopping condition (6), rendering the algorithm terminated in fewer than ⌈dL/p⌉
iterations with scant neighbors. This phenomenon will be seen in our simulation study.
3.2 On Selection of p
The performance of the proposed GOMP algorithm depends on the parameter p, i.e., the number of identified
neighbors per iteration. With the aid of the recovery rate analyses for the semi-random model, below we first
discuss the selection of paimed at improving the recovery rate. The results will shed light on the selection
ofpfor practical datasets.
Recall the assertion of Theorem 2 that, with a high probability, the proposed stopping rule (6) halts the
algorithm once dLneighbors or so are recovered and, if so, the GOMP algorithm yields a high global
recovery rate throughout all iterations according to Corollary 1. Hence, a natural criterion for selecting pis
to maximize both the probability lower bounds (17) and (20); in this way, the proposed algorithm stands a
high chance of recovering many true neighbors and only few false neighbors. However, we should note that
both probability lower bounds are rather complicated functions in pand the ground truth parameters (such
as the data size N, subspace dimension dL, cluster size|YL|, noise power σ, and the subspace affinity τ).
An explicit and tractable rule for choosing ptoward recovery rate enhancement is therefore very difficult to
obtain. Even if such a solution can be found, it necessarily depends on the ground truth parameters, which
are nonetheless unknown to the designers. Despite this, by analyzing the lower bounds (17) and (20) we can
still summarize certain interesting properties of the recovery rate as the parameter pvaries.
For this, let us examine each individual term in the probability lower bounds (17) and (20) to see how they
change with p. Suppose that a total number Mt=pMof neighbors are recovered, among which at least
kt(>0)are true. To ease discussions, we assume that kt≥Mso thatqt=⌊kt/M⌋>0, where⌊•⌋is the
floor function; hence, on average at least qttrue neighbors are found in each iteration. We can first observe
from (17) the followings:
a) Clearly, the 3rdterm−6(σ/√π)dL−Mt+pin the lower bound (17) increases with pwhen the noise
level is so small that σ<√π.
b) Following the discussions below (19), the 4thand 7thterms of (17) scale like −N[1−(8 logN/dL)](p−qt)
and−N[1−(8 logN/dL)](p−qt+1), respectively. Since
p−qt=p−⌊kt/(Mt/p)⌋=p−⌊p(kt/Mt)⌋, (21)
13Published in Transactions on Machine Learning Research (10/2023)
which increases with p, we conclude that the 4thand 7thterms of (17) also increases with p.
c) To check the 5thand 8thterms of (17), we first note that (i) the subspace affinity bound τ < 1
(see (14)), thereby τ/radicalbig
2/π < 1, and (ii) since the number ktof recovered true neighbors never
exceeds the cluster size |YL|, we must have|YL|>kt>qt=⌊kt/M⌋, leading to e(|YL|−1)/qt>1.
Hence, it can be readily seen that the 5thterm−rt/parenleftig/radicalig
2
πτ/parenrightig|YL|−dL−qt−1/parenleftig
e(|YL|−1)
qt/parenrightigqt
and the 8th
term−(M−rt)/parenleftig/radicalig
2
πτ/parenrightig|YL|−dL−qt/parenleftig
e(|YL|−1)
qt−1/parenrightigqt−1
decrease with qt. Notably, since qt=⌊p(kt/Mt)⌋
grows aspincreases, we therefore conclude that the 5thand 8thterms decrease with p.
d) Finally, the sum of the 6thand 9thterms of (17) equals−M(4+2c)
N2 =−Mt(4+2c)
pN2, which increases with
p.
Combining (a)∼(d), we know that the 3rd, 4th, 6th, 7th, and 9thterms of (17) increase with p, whereas the
5thand 8thterms decrease with p. In particular, if the subspace affinity τis very small, the 5thand 8th
terms in (17) can be neglected so that the lower bound (17) increases with p. Hence, as long as the ground
truth subspaces are well-separated from each other, a large pis preferred. On the other hand, we consider
the scenario that the data size Nand cluster size|YL|are large; therefore the 4th, 5th, 7th, and 8thterms
in the lower bound (17) are vanishingly small (see also the discussions below (19)). The lower bound (17) is
then reduced to
1−Ne−n/8−6/parenleftbiggσ√π/parenrightbiggdL−Mt+p
−Mt(4 + 2c)
pN2. (22)
Clearly, with Mt,N,dL, andσbeing fixed, the lower bound (22) increases with p. Hence, we can also
conclude that, as long as the data size Nand cluster size|YL|are large enough, a large pis preferred. Next,
we consider the lower bound (20). To ease discussion, we assume that pdividesdL. Based on our analyses
on the lower bound (17) (see the discussions (a) to (d) above) we note that:
e) The 3rdterm−6(σ/√π)dL−p(⌈dL/p⌉−1)=−6(σ/√π)pincreases with pwhen the noise level is so
small thatσ<√π.
f) The 4thterm−2pe−√
n/pis negligible since the ambient dimension n≫p.
g) Clearly, the 5thterm−/ceilingleftig
dL
p/ceilingrightig
2e(N−|YL|)
N8 logN/dLincreases with p.
h) Sinceτ/radicalbig
2/π < 1ande(|YL|−1)/p > 1(see discussion (c)),/parenleftig/radicalig
2
πτ/parenrightig|YL|−dL−p/parenleftig
e(|YL|−1)
p−1/parenrightigp−1
increases with p; obviously⌈dL/p⌉decreases with p. Hence, it is hard to tell whether the 6thterm
−⌈dL
p⌉/parenleftig/radicalig
2
πτ/parenrightig|YL|−dL−p/parenleftig
e(|YL|−1)
p−1/parenrightigp−1
of (20) increases with por not.
i) Finally, the 7thterm−⌈dL
p⌉4+2c
N2increases with p.
By (e), (f), (g), and (i), we know that the 3rd, 4th, 5th, 7thterms of (20) increase with p. Still, if the subspace
affinityτis very small, the 6thterm of (20) is negligible, leading to the conclusion that (20) increases with p.
Hence, a large pis favored for well-separated ground truth subspaces. On the other hand, when Nand|YL|
are large, the 5thand 6thterms are vanishingly small (again, see the discussions below (19)) so that (20)
increases with p. As a result, a large pis preferred whenever the data size Nand cluster size|YL|are large.
It is worth noting that the inferred tendency of (20) as pvaries totally agrees with that of (17). Finally, we
should note that the cluster size |YL|is unknown beforehand. For balanced datasets, wherein all clusters are
about equally large, |YL|is typically in direct proportion to the data size N; in this case, we then would like
to increase pfor large-scale datasets. We can therefore conclude that, for well-separated subspaces (small
affinity) or large-scale balanced datasets (large Nand|YL|), a largepis preferred (the same tendency is also
seen in our simulation study using both synthetic and real-world datasets). This is expected since, in the
former case, the data point under consideration is surrounded by many true neighbors, and in the latter case
tends to be strongly correlated with many neighbors. Either way, the residual would quickly diminish in
14Published in Transactions on Machine Learning Research (10/2023)
just few iterations, and one should therefore adopt a large pto boost neighbor recovery before the residual
is depleted of the component from the desired subspace.
We should note however that our recovery rate analyses are developed under the semi-random model, that
is, the signal points are uniform (Assumption 1) and noise is Gaussian (Assumption 2), which are seldom
met in practice. Hence, for real-world datasets, performance guarantees similar to the semi-random model
can hardly be obtained. Without such analytic metrics, a simple and intuitively reasonable way of judging
a goodpwould be the fulfillment of the dimension-aware property; this is because the availability of about
dLneighbors is likely to begat a similarity graph with good connectivity, i.e., sufficiently many intra-cluster
edgesandfewinter-clusteredges. Basedonoursimulationstudy(seeSection4formoredetails)itisobserved
that the dimension-aware property tends to hold for a wide range of pwhen datasets enjoy a small subspace
affinity, such as the Coil-100 (Nene et al., 1996) and MNIST datasets (LeCun et al., 1998), but seems to
fail for datasets with medium-to-large subspace affinities, such as the Devanagari (Acharya et al., 2015)
and Extended Yale B (Lee et al., 2005) datasets. In particular, the Devanagari dataset is subject to large
noise corruption and prefers a large p; contrarily, the Extended Yale B is under small noise corruption and
prefers a small p. We would therefore conclude that the dimension-aware property holds when the subspace
affinity is small; this is because there would then exist ample intra-cluster neighbors for the data point under
consideration, making it highly likely to find about dLcorrect neighbors no matter how many are recovered
in each iteration. The above simulation findings also suggest that the selection of pis highly dependent
on the affinity and noise power, which are nonetheless known unless additional offline estimation is further
conducted. In the absence of such side information, we would then propose to select a large pwhen the
data sizeNis large; this is justified by our simulation results and also agrees with our previous claim in the
semi-random model case. In particular, we would suggest choosing p= 2,3for small-scale datasets such as
Extended Yale B, Coil-100, and NCKU human face (Chen & Lien, 2009) datasets (all with thousands of data
points or less), while p= 5,6for large-scale datasets, e.g. MNIST, Devanagari, and Cifar-10 (Krizhevsky,
2009) datasets (with tens of thousands of data points or more).
Remark: In the context of sparse signal recovery via GOMP, the selection of p, in general, depends on the
sparsity level, but the development of explicit rules for choosing pstill remains a difficult open problem (Fu
et al., 2022).
4 Experimental Results
In this section, numerical simulations based on both synthetic data and real-world data are used to cor-
roborate our theoretical study and illustrate the performance of the proposed GOMP method. Synthetic
data generated are similar to (Elhamifar & Vidal, 2013; Wu et al., 2021). The ground truth is a union of
three subspacesS1,S2andS3ofR350, with an equal subspace dimension d1=d2=d3= 6; separation
between two distinct subspaces is gauged by the subspace affinity defined in (13). Noiseless signal points
are uniformly drawn at random from each subspace and are corrupted by Gaussian noise with zero mean
and standard deviation σ. The sampling density is defined to be ϕk≜|Yk|/dk,1≤k≤3. Regarding the
neighbor identification performance metrics, we consider the true neighbor rate (TNR) (Wu et al., 2021),
that is,
TNR≜/summationtext
(i,j)∈T1(c∗
i,j̸= 0)
/summationtext
1≤i,j≤N1(c∗
i,j̸= 0),T≜{(i,j)|yi,yj∈Yl,for some 1≤l≤L}, (23)
wherec∗
i,jis thejth entry of the computed sparse representation vector c∗
iusing GOMP/OMP (see step 4
of Algorithm 3 and Algorithm 4), and the average number of recovered neighbors (ANRN)
ANRN ≜N/summationdisplay
i=1∥c∗
i∥0/N, (24)
whichassessestheabilitytoboostneighboracquisition. Asin(Soltanolkotabi&Candès,2012;Soltanolkotabi
et al., 2014; Wu et al., 2021; You et al., 2018; Matsushima & Brbic, 2019), the global clustering performance
is evaluated based on the correct clustering rate (CCR), defined as
CCR≜(#of correctly clustered data points )/N. (25)
15Published in Transactions on Machine Learning Research (10/2023)
Figure 3: TNR versus noise standard deviation σfor nine different pairs of subspace affinity ρand sample
densityϕ; assume the subspace dimension is known beforehand and the number of iterations is preset to be
M=⌈6/p⌉(p= 1,2,3).
Figure 4: CCR versus noise standard deviation σfor nine different pairs of subspace affinity ρand sample
densityϕ; assume the subspace dimension is known beforehand and the number of iterations is preset to be
M=⌈6/p⌉(p= 1,2,3).
4.1 Synthetic Data
We first use synthetic data to test the performance of the proposed method. To ease illustration, we consider
the case that aff(Sk,Sl) =ρfor all 1≤k̸=l≤3, i.e., the three subspaces are equally separated from
each other, and that ϕ1=ϕ2=ϕ3=ϕ, so with equal sample density. Associated with 9 different pairs
(ρ,ϕ)of subspace affinity and sample density, Fig. 3 and 4 plot the simulated TNR and CCR, respectively,
versus noise standard deviation σ; sub-figures on the same row (column, respectively) correspond to an
identicalϕ(ρ, respectively), while ρ(ϕ, respectively) is increased when going from left to right (top to
bottom, respectively). In generating Fig. 3 and 4, the total number of iterations is preset to be M= 6for
OMP andM=⌈6/p⌉for GOMP, respectively; this represents the ideal situation that subspace dimension
(equal to 6) is known. It is first seen from Fig. 3 that, as compared to the conventional OMP ( p= 1), the
16Published in Transactions on Machine Learning Research (10/2023)
Figure 5: TNR versus noise standard deviation σfor nine different pairs of subspace affinity ρand sample
densityϕ; the proposed stopping rule (6) is used.
Figure 6: CCR versus noise standard deviation σfor nine different pairs of subspace affinity ρand sample
densityϕ; the proposed stopping rule (6) is used.
proposed GOMP with multiple neighbor selection ( p >1) yields higher TNR. In particular, GOMP with
p= 3achieves the highest TNR because it involves the fewest iterations (only two) and, thus, is subject to
the least perturbation of the residual vector. Then, it is seen from Fig. 4 that improved TNR in turn leads
to higher CCR. The above experiment is conducted again by instead using the proposed stopping rule (6) to
halt the algorithm, and the results are shown in Fig. 5 and Fig. 6. It can be seen that the proposed GOMP
still outperforms OMP. Fig. 7 further plots the ANRN of the two methods, both employing the proposed
stopping rule (6), for different noise standard deviation σ. We first observe from the figure that, when
noise is so small that σ < 0.1, ANRN of all cases is about six, in support of the assertion in Theorem 2
that as many neighbors as the subspace dimension suffice to well explain the data point under consideration.
Whenσgrows up to 0.1, the proposed GOMP (with p= 2,3) tends to retrieve more neighbors. As we have
mentioned in the discussions at the end of Section 3, further noise corruption would enlarge the dimension
of the span of the data cluster; as a result, the algorithm is terminated after more than ⌈dL/p⌉iterations so
as to recover more neighbors (than the ground truth subspace dimension) to explain the data point. When
17Published in Transactions on Machine Learning Research (10/2023)
Figure 7: ANRN versus noise standard deviation σfor nine different pairs of subspace affinity ρand sample
densityϕ; the proposed stopping rule (6) is used.
Figure 8: TNR and CCR versus noise standard deviation σfor three different pairs of subspace affinity ρ
and sample density ϕ; the subspace dimension ( dk= 6) is known. We set p= 6for GOMP so that only one
iteration is conducted, whereas OMP conducts M= 6iterations.
σgets even larger, the residual is then severely dominated by noise, only to halt neighbor search earlier,
say, in less than⌈dL/p⌉iterations; this ends up with fewer recovered neighbors and reduced ANRN. Finally,
we consider the special case that subspace dimension dkis known, and set p=dk(= 6)so that GOMP
executes just one iteration. Specifically, we consider the following three affinity-density pairs ( ρ,ϕ)=(0.5,6),
(0.5,2), (0.7,6), and the total number of iterations for OMP is set to be M= 6. Fig. 8 plots the TNR and
CCR curves. Fig. 8-(a) clearly demonstrates that GOMP outperforms OMP when the subspace affinity ρ
is small and the sample density ϕis large. The results are expected since, with only one single iteration
andp=dk, GOMP can perform well only when each data point has at least pcorrect neighbors nearby,
a situation fulfilled when subspaces are separated far away from each other (small ρ) and there are many
neighbors around (large ϕ). However, OMP performs better as the sample density ϕdecreases to 2 so that
correct neighbors are scant (see Fig. 8-(b)), or the subspace affinity ρincreases to 0.7, resulting in many
incorrect neighbors (see Fig. 8-(c)). Hence, while GOMP conducts just one iteration and enjoys a smaller
AoD, lack of enough good neighbors outweighs the benefit of small AoD.
18Published in Transactions on Machine Learning Research (10/2023)
Figure 9: Samples of four real datasets. (a) The Extended Yale B human face dataset. (b) The Coil-100
object image dataset. (c) The MNIST handwritten digit dataset. (d) The Devanagari handwritten character
dataset.
Table 2: Running time (in minutes) of OMP ( p= 1) and GOMP ( p= 2,3,4,6) when 12 neighbors are
recovered.
Running time (min) p= 1p= 2p= 3p= 4p= 6
Extended Yale B 1.51 0.85 0.61 0.48 0.3
Coil-100 2.83 1.45 0.98 0.75 0.51
MNIST 194 100 68 50 37
Devanagari 218 130 91 72 47
4.2 Real-World Data
We proceed to evaluate the performance of the proposed method by using the following four real-world
datasets (an illustration of their test samples is shown in Fig. 9).
•The Extended Yale B human face dataset (Lee et al., 2005), comprised of photos of 38 people each
with 65 images of 192 ×168 pixels. To reduce the computational cost, we use the dimensionality
reduction technique as in (Elhamifar & Vidal, 2013; Peng et al., 2013; Matsushima & Brbic, 2019)
for reducing the image size to 48 ×42.
•The Coil-100 dataset (Nene et al., 1996), composed of 7200 images (each with size 32 ×32) of 100
objects (72 images for each object).
•The MNIST dataset (LeCun et al., 1998), containing 70000 images of handwritten digits with size
28×28; at least 6000 images for each digit.
•The Devanagari dataset (Acharya et al., 2015), containing images of handwritten Devanagari char-
acters and digits. We use the character image part, which consists of 72000 images of 36 characters
with size 32×32 (2000 images per character).
In the first part of the simulations, we compare the proposed GOMP with OMP using the above four real
datasets. Assuming that a total number of 12 neighbors are to be recovered, Table 2 lists the running time
for OMP (p= 1) and GOMP ( p= 2,3,4,6); the experiments are conducted using MATLAB 2016b on a
desktop with an AMD 3700X CPU of 3.6GHz and 64 GB RAM. It can be seen that GOMP enjoys a shorter
running time, confirming the computational efficiency of GOMP. Table 3 then compares the resultant TNR,
showing that GOMP yields higher TNR when the same number of neighbors are to be found. We note that,
unlike the synthetic data case, the TNR of the two greedy methods is less than 0.5 for Yale-B, Coil-100, and
Devanagari datasets, in disagreement with the high-TNR assertion of our recovery rate analyses in Section
3. This is because real-world datasets seldom meet the assumptions underlying the semi-random model
(Assumptions 1 and 2): neither the signal points are uniform nor the noise is Gaussian (e.g., real human face
data are typically corrupted by sparse outliers (Elhamifar & Vidal, 2013; Soltanolkotabi & Candès, 2012)).
Hence, the developed recovery rate formulae in Section 3 may not fully explain the simulated results when
19Published in Transactions on Machine Learning Research (10/2023)
Table 3: TNR of OMP ( p= 1) and GOMP ( p= 2,3,4,6) when 12 neighbors are recovered.
TNR p= 1p= 2p= 3p= 4p= 6
Extended Yale B 0.37 0.41 0.46 0.49 0.58
Coil-100 0.20 0.21 0.26 0.33 0.45
MNIST 0.58 0.61 0.64 0.66 0.68
Devanagari 0.2 0.23 0.30 0.39 0.51
Table 4: FDR of OMP ( p= 1) and GOMP ( p= 2,3,4,6) when 12 neighbors are recovered.
FDR p= 1p= 2p= 3p= 4p= 6
Extended Yale B 0.85 0.86 0.87 0.87 0.90
Coil-100 0.94 0.93 0.93 0.94 0.95
MNIST 0.80 0.81 0.81 0.83 0.84
Devanagari 0.51 0.56 0.60 0.65 0.72
Table 5: CCR of OMP ( p= 1) and GOMP ( p= 2,3,4,6) when 12 neighbors are recovered.
CCR p= 1p= 2p= 3p= 4p= 6
Extended Yale B 0.47 0.49 0.56 0.63 0.67
Coil-100 0.29 0.32 0.37 0.42 0.48
MNIST 0.43 0.51 0.63 0.64 0.66
Devanagari 0.26 0.31 0.35 0.37 0.38
real-world datasets are considered. Despite TNR may not be high in certain cases, we would like to remark
that the existence of false neighbors does not largely degrade final data clustering accuracy. To see this, we
further consider the feature detection rate (FDR) (Heckel & Bölcskei, 2015), defined to be
FDR≜1
NN/summationdisplay
i=1∥/tildewidec∗
i∥2
∥c∗
i∥2, (26)
where/tildewidec∗
iis the vector containing the entries of c∗
isupported on true neighbors. If FDR is high, the weights
on the false edges in the similarity graph are small, meaning that the existence of false neighbors is somehow
not harmful. Table 4 presents the computed FDR; it can be observed that both OMP and GOMP achieve
high FDR (above 0.7 for most cases). Table 5 then lists the CCR, showing that GOMP achieves higher CCR
thanks to higher TNR.
Finally, we illustrate the ANRN of GOMP when employing the proposed stopping rule (6), and discuss the
selection of the parameter paimed at fulfilling the dimension-aware property. For this purpose, knowledge
aboutgroundtruthsubspacedimensionsandnoiselevelisneeded. Todeterminethesubspacedimensions, we
vectorize all data points associated with each cluster, stack them into a matrix, and calculate the normalized
singular values (with respect to the maximal one). For each dataset, we choose the first 9 clusters for singular
value computation and the results are plotted in Fig. 10, respectively. Observe from the four figures that
the normalized singular values of all datasets exhibit a sudden slump from 1 to 0.1, beyond which they
decrease slowly; the curves are seen to bend to the floors around 7 ∼9 for Yale-B, 5∼7 for Coil-100, 10∼15 for
MNIST, and 15∼20 for Devanagari, respectively. The above singular value index ranges are then used as the
20Published in Transactions on Machine Learning Research (10/2023)
Figure 10: Normalized singular values of data matrices of the first 9 clusters of (a) Extended Yale B dataset,
(b) Coil-100 dataset, (c) MNIST dataset, and (d) Devanagari dataset.
Table 6: Estimated subspace dimensions, noise powers, subspace affinity, data size, and ANRN of GOMP
with proposed stopping rule.
Coil-100 MNIST Devanagari Extended Yale B
Range of subspace dimension [6,10] [10,15] [15,20] [7,9]
Estimated noise powers 0.036 0.194 0.199 0.032
Estimated subspace affinity 0.447 0.553 0.628 0.706
Data size 7200 70000 72000 2470
ANRN (p= 2) 4.7 8.6 6.2 7.5
ANRN (p= 3) 6.3 11.6 7.4 11.2
ANRN (p= 4) 8.4 13.5 9.4 14.3
ANRN (p= 6) 12.1 17.7 16.8 19.6
estimated subspace dimensions. Also, the noise power of each dataset is estimated as the averaged sum of the
rest nondominant singular values. Table 6 then shows the ranges of estimated subspace dimensions, average
subspace affinity, estimated noise powers, and the computed ANRN of GOMP using the proposed stopping
rule (6) with p= 2,3,4,6. We can first observe from the table that, for the Coil-100 and MNIST datasets
(both with relatively small affinity, though the latter subject to large noise), their ANRNs’ are within the
ranges 4.7∼12.1 and 8.6∼17.7, respectively, which are fairly close to the estimated subspace dimensions
[6,10]and[10,15]. This implies that, for these two datasets, the proposed stopping rule (6) is aware of the
subspace dimension. However, for the Devanagari dataset (with a medium-to-large affinity) and Extended
Yale B dataset (with a large affinity, despite small noise corruption), their ANRNs’ deviate largely from
the subspace dimensions, and hence the dimension-aware property seems to fail. We particularly observe
that, for the Devanagari dataset, the algorithm is halted earlier (in less than ⌈d/p⌉iterations) with a small
p(p= 2,3,4), resulting in fewer recovered neighbors and reduced ANRN. This is mainly because, with
large noise corruption, the residual is quickly dominated by noise so as to trigger the stopping rule (6) early
(hence, less than ⌈d/p⌉iterations); in this case, a large p(sayp= 6) to soon recover many neighbors is
21Published in Transactions on Machine Learning Research (10/2023)
desired. Such a tendency is also seen in the synthetic data case (see Fig. 7). Regarding the Extended Yale B
dataset, GOMP with a small p(p= 2,3) is instead preferred. This is because, with a large affinity, different
clusters are close to each other; if pis large, the algorithm is likely to pick neighbors from incorrect clusters
and is then misled to find neighbors from some subspace (other than the ground truth one) with a larger
dimension, thereby ending up with higher ANRN. Based on the above simulation findings we conclude that,
as long as the subspace affinity is small, the proposed stopping rule (6) is aware of the subspace dimension,
allowing for a wide range of p. If the affinity is medium-to-large, a good pis highly dependent on the noise
power: large (small, respectively) pfor large (small, respectively) noise power. Overall, the selection of p
is therefore closely related to the subspace affinity and noise power, which are nonetheless unknown unless
additional off-line estimation is further conducted. Notably, we can also observe from Table 6 that the good
pincreases with the data size N; this has also been inferred based on our recovery rate analysis for the
semi-random model (see the discussions in Section 3.2). Hence a very simple guideline, without the need
of knowing the subspace dimensions and noise strength, is to choose small p(say,p= 2,3) for small-size
datasets (with thousands of data points or less) and large p(p= 5,6) for large-scale datasets (with tens of
thousands of data points or more).
In the second part of this simulation, we compare GOMP using the proposed stopping rule (6) (dubbed by
"proposed") with the following neighbor identification schemes:
•GOMP terminated after ⌈d/p⌉iterations, in which dis the ground-truth subspace dimension de-
termined as the floor of the average of the subspace dimensions within the range in Table 6. This
serves as the ideal implementation of GOMP that employs the knowledge of the subspace dimension
(hereafter coined as "ideal").
•OMP (Tschannen & Bölcskei, 2018) terminated after a pre-determined number Mof iterations.
•Active OMP (AOMP) (Chen et al., 2018) terminated after a pre-determined number Mof iterations;
the neighbor dropping probability is set to be 0.8, as used in (Chen et al., 2018).
•Restricted connection OMP (ROMP) (Zhu et al., 2019); following the same setting as in (Zhu et al.,
2019), the number of iterations is set to be 3, and the number of restricted connections is set to be
2.
•SSC with conventional ℓ1-minimization (LASSO) technique (Elhamifar & Vidal, 2013); the regular-
ization parameter λis obtained via an exhaustive search over the interval (0,0.5], as in (Matsushima
& Brbic, 2019).
•Exemplar-based subspace clustering (You et al., 2018); the number of exemplar data points is set as
160, as used in (You et al., 2018), and the regularization parameter λis obtained via an exhaustive
search over the interval (0,0.5], as in (Matsushima & Brbic, 2019).
•Scalable sparse subspace clustering (S3C) (Peng et al., 2013); following the same setting as in (Peng
et al., 2013), we randomly choose 1000 data points as “in-sample data” to find coarse estimates of
the ground-truth subspaces; the regularization parameter λis obtained via an exhaustive search
over (0,0.5], as in (Matsushima & Brbic, 2019).
•Selective sampling-based scalable sparse subspace clustering (S5C) (Matsushima & Brbic, 2019); to
carry out representation learning to find representative data points, the batch size is set to be 1, as
used in (Matsushima & Brbic, 2019), and the size of representation set is chosen to be 10L. Again,
the regularization parameter λis obtained via an exhaustive search over (0,0.5]as in (Matsushima
& Brbic, 2019).
•Elastic net subspace clustering with oracle guided elastic net (ORN) (You et al., 2016a); the noise
trade-off parameter γis set to be 1, and the regularization parameter λis obtained via an exhaustive
search over (0,0.5], as in (Matsushima & Brbic, 2019).
Note that the last four methods, namely, ESC, S3C, S5C, and ORN, are reduced-complexity variants of the
ℓ1-minimization scheme (Elhamifar & Vidal, 2013). For both OMP and AOMP, we conduct an exhaustive
22Published in Transactions on Machine Learning Research (10/2023)
Table 7: Simulation parameters for GOMP, OMP, and AOMP.
Parameters Extended Yale B Coil-100 MNIST Devanagari
GOMP-ideal ( p) 2 3 4 6
GOMP-proposed ( p) 2 3 4 6
OMP (M) 8 4 4 5
AOMP (M) 12 4 6 6
Table 8: CCR of different clustering algorithms (the blank denotes that the time limit of 7 days is exceeded).
CCR OMP AOMP ROMP SSC ESC S3C S5C ORN proposed ideal
Yale B 0.73 0.81 0.77 0.81 0.44 0.33 0.71 0.83 0.84 0.84
Coil 100 0.46 0.35 0.38 0.79 0.23 0.14 0.53 0.76 0.71 0.79
MNIST 0.61 0.64 0.63 ×0.27 0.22 0.55 0.72 0.64 0.66
Deva. 0.28 0.33 0.28 ×0.20 0.15 0.33 0.40 0.35 0.38
Table 9: Running time (in minutes) of different clustering algorithms (the blank denotes that the time limit
of 7 days is exceeded).
(minute) OMP AOMP ROMP SSC ESC S3C S5C ORN proposed ideal
Yale B 0.96 1.55 0.87 577 4 140 11 13 0.55 0.43
Coil 100 0.95 2.7 0.82 988 6 138 42 31 0.55 0.5
MNIST 77 56 54 ×83 109 46 69 52 37
Deva. 96 74 69 ×96 142 54 80 59 47
search over the integer set {1,2,...,18}to find the best number Mof iterations that achieve the highest CCR;
such an approach has been adopted in the simulation study of (Matsushima & Brbic, 2019); for both GOMP-
proposed and GOMP-ideal, the parameter pfor each dataset is selected according to the suggested guidelines
(see Table 7 for a list of the parameters used for OMP and GOMP algorithms). Table 8 shows the CCR of
all methods. Compared to the competing greedy algorithms OMP, AOMP, and ROMP, GOMP-proposed
(proposed) achieves higher CCR. As against the five ℓ1-minimization based solutions, GOMP-proposed
(proposed) is next to ORN (You et al., 2016a), which is essentially a LASSO regression modified to further
promote connectivity. It is also observed that GOMP-proposed (proposed) performs just slightly inferior to
GOMP-ideal (ideal), which requires knowledge of the subspace dimension. This indicates that the proposed
stopping rule (6) in conjunction with the suggested pis aware of the subspace dimension. Table 9 then lists
the running time of all methods. The results show that GOMP-proposed (proposed) is very computationally
efficient: it enjoys the lowest running time in most cases. For the MNIST and Devanagari datasets, S5C is
faster but is inferior to GOMP-ideal (ideal).
5 Proofs
5.1 Proof of Theorem 1
We set about the proof by defining the following per-iteration neighbor recovery event:
Em≜{at leastkmtrue neighbors are obtained in the mth iteration},1≤m≤M. (27)
23Published in Transactions on Machine Learning Research (10/2023)
It suffices to show the event/intersectiontextM
m=1Emholds with a probability as high as claimed in (15). The following
lemmas are needed for deriving Theorem 1.
Lemma 1. ((Davenport et al., 2011, Ex. 25); (Heckel & Bölcskei, 2015, eq. 59)) Let a∈Rmbe uniformly
distributed over the unit-sphere of Rm, and b∈Rmbe a random vector independent of a. Then, for ϵ≥0
we have
Pr{|aTb|>ϵ∥b∥2|}≤ 2e−mϵ2/2, (28)
Pr{|aTb|<ϵ∥b∥2|}≤/radicalbig
2/πϵ. (29)
Lemma 2. LetUl∈Rn×dlbe a matrix whose columns form an orthonormal basis for subspace Sl,1≤l≤L.
Forxi/∈SL,we have
Pr

/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
xi,r(N)
m,∥
∥r(N)
m,∥∥2/angbracketrightig/vextendsingle/vextendsingle/vextendsingle>max
l̸=L4 log(N)∥UT
lUL∥F√dldL

<2
N(8 logN)/dL(30)
Proof :See Appendix A.
Lemma 3. Forxi/∈SL,we have
Pr/braceleftigg/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
xi,r(N)
m,⊥
∥r(N)
m,⊥∥2/angbracketrightig/vextendsingle/vextendsingle/vextendsingle≤/radicalbigg
6 logN
n−dL/bracerightigg
≥1−2cN−3(31)
Proof :See Appendix B.
Lemma 4. (Tschannen & Bölcskei, 2018, Lemma 10) The event/intersectiontextN
i=1/braceleftbig
∥ei∥2≤3σ/2/bracerightbig
occurs with a prob-
ability at least 1−Ne−n/8.
Ifkm= 0, it follows Pr{Em}= 1because the number of true neighbors is never negative. We then consider
the case that km>0. Since the already chosen data vectors in all the previous m−1iterations are orthogonal
to the residual r(N)
m−1, i.e.,|⟨yj,r(N)
m−1⟩|= 0for allj∈Λm−1, the “yet-to-be-selected” candidate neighbors are
yj’s forj∈{1,2,...,N−1}\Λm−1. Given that Λm−1= Λt
m−1∪Λf
m−1, where the disjoint subsets Λt
m−1
andΛf
m−1consist of, respectively, the indexes of true and false neighbors already selected up to the first
m−1iterations,|YL|−1−|Λt
m−1|andN−|YL|−|Λf
m−1|data vectors remain in the same and different
clusters as yN, respectively. Let the true neighbor candidates ytj, where xtj∈SLandtj/∈Λt
m−1, be sorted
according to|⟨yt1,r(N)
m−1⟩|≥...≥|⟨yt|YL|−1−|Λt
m−1|,r(N)
m−1⟩|,and likewise the false neighbor candidates yfk,
where xfk/∈SLandfk/∈Λf
m−1, in the way that |⟨yf1,r(N)
m−1⟩|≥...≥|⟨yfN−|YL|−|Λf
m−1|,r(N)
m−1⟩|.Also, let the
noiselesssignalvectors xtg∈SL,tg/∈Λt
m−1, beorderedsothat |⟨xt1,r(N)
m−1⟩|≥...≥|⟨xt|YL|−1−|Λt
m−1|,r(N)
m−1⟩|,
while those xfh/∈SL,fh/∈Λf
m−1, in the way|⟨xf1,r(N)
m−1⟩|≥...≥|⟨xfN−|YL|−|Λf
m−1|,r(N)
m−1⟩|.
The event Emdefined in (27) can be expressed in terms of the above ordered statistics. By definition,
Emoccurs when at least kmtrue neighbors, and so at most p−kmfalse neighbors, are recovered in the
mth iteration. That is to say, ytkmis chosen as a neighbor but yfp−km+1is not, justifying the inequality
|⟨yfp−km+1,r(N)
m−1⟩|≤|⟨ ytkm,r(N)
m−1⟩|. The converse is obviously true. Hence we can rewrite Emas
Em={|⟨yfp−km+1,r(N)
m−1⟩|≤|⟨ ytkm,r(N)
m−1⟩|}. (32)
The expression (32) involves ordered statistics of the absolute inner products between the residual and noisy
data points yj’s, whose distribution is however quite complicated. To ease our lower bound derivation,
we shall instead seek sufficient conditions, specified by absolute inner products between the residual and
24Published in Transactions on Machine Learning Research (10/2023)
noise-free signal points xj’s, for|⟨yfp−km+1,r(N)
m−1⟩|≤|⟨ ytkm,r(N)
m−1⟩|; this allows us to employ the assumed
uniform distribution of xj’s (Assumption 1) as well as the ground-truth subspace orientation (Assumption
3) to facilitate analysis. For this we first note from (12) that, for all 1≤j≤N−1, we have
⟨yj,r(N)
m−1⟩=⟨xj,r(N)
m−1,∥⟩+⟨xj,r(N)
m−1,⊥⟩+⟨ej,r(N)
m−1⟩. (33)
Then for all u≤km, it follows
|⟨ytu,r(N)
m−1⟩|=|⟨xtu,r(N)
m−1,∥⟩+⟨xtu,r(N)
m−1,⊥⟩+⟨etu,r(N)
m−1⟩|
(a)=|⟨xtu,r(N)
m−1,∥⟩+⟨etu,r(N)
m−1⟩|
≥|⟨xtu,r(N)
m−1,∥⟩|− max
1≤j≤|YL|−1−|Λt
m−1||⟨etj,r(N)
m−1⟩|
(b)
≥|⟨xtkm,r(N)
m−1,∥⟩|− max
1≤j≤|YL|−1−|Λt
m−1||⟨etj,r(N)
m−1⟩|,(34)
where (a) holds since xtj∈SLforxtj∈SL, and (b) follows from the ordering of xtg’s. Hence, at least km
absolute inner products |⟨ytu,r(N)
m−1⟩|are no smaller than the term on the right-hand-side (RHS) of (34); in
particular, for|⟨ytkm,r(N)
m−1⟩|, thekmth largest|⟨ytj,r(N)
m−1⟩|, we must have
|⟨ytkm,r(N)
m−1⟩|≥|⟨ xtkm,r(N)
m−1,∥⟩|− max
1≤j≤|YL|−1−|Λt
m−1||⟨etj,r(N)
m−1⟩|.(35)
Similarly, by the ordering of |⟨xfk,r(N)
m−1,∥⟩|, for allu≥p−km+ 1, we have
|⟨yfp−km+1,r(N)
m−1⟩|≤|⟨ xfp−km+1,r(N)
m−1,∥⟩|+ max
1≤k≤N−|YL|−|Λf
m−1||⟨xfk,r(N)
m−1,⊥⟩|
+ max
1≤k≤N−|YL|−|Λf
m−1||⟨efk,r(N)
m−1⟩|.(36)
Putting (35) and (36) together, the condition |⟨yfp−km+1,r(N)
m−1⟩|≤|⟨ ytkm,r(N)
m−1⟩|is guaranteed, and hence
the eventEmoccurs, once the following inequality is true
|⟨xfp−km+1,r(N)
m−1,∥⟩|+ max
1≤k≤N−|YL|−|Λf
m−1||⟨xfk,r(N)
m−1,⊥⟩|
+ max
1≤k≤N−|YL|−|Λf
m−1||⟨efk,r(N)
m−1⟩|+ max
1≤j≤|YL|−1−|Λt
m−1||⟨etj,r(N)
m−1⟩|≤|⟨ xtkm,r(N)
m−1,∥⟩|.(37)
Recall that our goal is to show that/intersectiontextM
m=1Emoccurs with a high probability. Under the semi-random model,
we go on to estimate the probability of which the inequality (37) holds for all 1≤m≤M, and in turn
a lower bound for Pr {/intersectiontextM
m=1Em}. The basic idea is to “split and then lump”: find an upper bound for
each individual left-hand-side (LHS) term of (37) and estimate, one by one, the probability about which
the obtained inequality holds, and similarly a lower bound for the RHS term along with an estimated
probability of its validity; putting the results altogether gives a sufficient condition for (37) along with the
desired probability lower bound.
25Published in Transactions on Machine Learning Research (10/2023)
a) An Upper Bound for the 1stLHS Term of (37): For a fixed α,we first note that
Pr{|⟨xfp−km+1,r(N)
m−1,∥⟩|≤α}
= 1−Pr{|⟨xfp−km+1,r(N)
m−1,∥⟩|>α}
(a)
≥1−Pr/braceleftbig
∃I⊂[N−|YL|−|Λf
m−1|]with
|I|=p−km+ 1s.t.|⟨xfk,r(N)
m−1,∥⟩|>α,∀k∈I/bracerightbig
(b)
≥1−/parenleftbiggN−|YL|
p−km+ 1/parenrightbigg/parenleftig
max
I:|I|=p−km+1
I⊂[N−|YL|−|Λf
m−1|]Pr{|⟨xfk,r(N)
m−1,∥⟩|>α,∀k∈I}/parenrightig
(c)
≥1−/parenleftbigge(N−|YL|)
p−km+ 1/parenrightbiggp−km+1/parenleftig
max
I:|I|=p−km+1
I⊂[N−|YL|−|Λf
m−1|]Pr{|⟨xfk,r(N)
m−1,∥⟩|>α,∀k∈I}/parenrightig
,(38)
where in (a) the notation [Q]≜{1,2,...,Q},Q∈N, in (b)/parenleftbigQ
k/parenrightbig
≜Q!
(Q−k)!k!, and (c) holds by (Cormen
etal.,2009, inequality(C.5)). ByLemma1andsincedatapointsareindependent, for |I|=p−km+1
we have
Pr/braceleftbigg
|⟨xfk,r(N)
m,∥⟩|>max
l̸=L4 log(N)∥UT
lUL∥F√dldL∥r(N)
m,∥∥2,∀k∈I/bracerightbigg
<(2
N(8 logN)/dL)p−km+1.(39)
Combining (38) and (39) and setting α= max
l̸=L4 log(N)∥UT
lUL∥F∥r(N)
m,∥∥2/√dldL, it then follows
|⟨xfp−km+1,r(N)
m−1,∥⟩|≤max
l̸=L4 log(N)∥UT
lUL∥F√dldL∥r(N)
m,∥∥2 (40)
holds with a probability at least 1−(2e(N−|YL|)
(p−km+1)N(8 log N)/dL)p−km+1.
b) An Upper Bound for the 2ndLHS Term of (37): We first note that
Pr/braceleftig
max
1≤k≤N−|YL|−|Λf
m−1||⟨xfk,r(N)
m−1,⊥⟩|≤α/bracerightig
= 1−Pr/braceleftig
max
1≤k≤N−|YL|−|Λf
m−1||⟨xfk,r(N)
m−1,⊥⟩|>α/bracerightig
≥1−Pr{∃k∈[N−|YL|−|Λf
m−1|]s.t.|⟨xfk,r(N)
m−1,⊥⟩|>α}
= 1−Pr/braceleftigg/parenleftiggN−|YL|−|Λf
m−1|/intersectiondisplay
k=1{|⟨xfk,r(N)
m−1,⊥⟩|≤α}/parenrightiggc/bracerightigg
= 1−Pr/braceleftiggN−|YL|−|Λf
m−1|/uniondisplay
k=1{|⟨xfk,r(N)
m−1,⊥⟩|>α}/bracerightigg
≥1−N−|YL|−|Λf
m−1|/summationdisplay
k=1Pr{|⟨xfk,r(N)
m−1,⊥⟩|>α}.(41)
By Lemma 3, we have
Pr/braceleftigg
|⟨xfk,r(N)
m,⊥⟩|>/radicalbigg
6 logN
n−dL∥r(N)
m,⊥∥2/bracerightigg
<2cN−3,1≤k≤N−|YL|−|Λf
m−1|.(42)
26Published in Transactions on Machine Learning Research (10/2023)
Combing (41) and (42) and with α=/radicalig
6 logN
n−dL∥r(N)
m,⊥∥2, we obtain
max
1≤k≤N−|YL|−|Λf
m−1||⟨xfk,r(N)
m−1,⊥⟩|≤/radicalbigg
6 logN
n−dL∥r(N)
m,⊥∥2 (43)
holds with a probability at least 1−2cN−2.
c) An Upper Bound for the 3rdLHS Term of (37): Using similar techniques as in deriving (41) we can
first reach
Pr/braceleftig
max
1≤k≤N−|YL|−|Λf
m−1||⟨efk,r(N)
m−1⟩|≤α/bracerightig
≥1−N−|YL|−|Λf
m−1|/summationdisplay
k=1Pr{|⟨efk,r(N)
m−1⟩|>α}.(44)
Using (28) and with a=efk/∥efk∥2andb=r(N)
m−1, it follows
Pr/braceleftigg
|⟨efk,r(N)
m⟩|>/radicalbigg
6 logN
n∥r(N)
m∥2∥efk∥2/bracerightigg
=Pr/braceleftigg/vextendsingle/vextendsingle/vextendsingle/angbracketleftigefk
∥efk∥2,r(N)
m/angbracketrightig/vextendsingle/vextendsingle/vextendsingle>/radicalbigg
6 logN
n∥r(N)
m∥2/bracerightigg
≤2N−3.(45)
Under Assumption 1 we have
∥r(N)
m∥2=∥PR(YΛm−1)yN∥2≤∥yN∥2=∥xN+eN∥2≤1 +∥eN∥2, (46)
in whichR(YΛm−1)is the column space of YΛm−1and the last inequality holds by the triangle
inequality. Combing (45) and (46) yields
Pr/braceleftigg
|⟨efk,r(N)
m⟩|>/radicalbigg
6 logN
n(1 +∥eN∥2)∥efk∥2/bracerightigg
≤2N−3. (47)
Settingα=/radicalig
6 logN
n(1 +∥eN∥2) max
1≤k≤N−|YL|−|Λf
m−1|∥efk∥2, (44) and (47) imply
max
1≤k≤N−|YL|−|Λf
m−1||⟨efk,r(N)
m−1⟩|≤/radicalbigg
6 logN
n(1 +∥eN∥2) max
1≤k≤N−|YL|−|Λf
m−1|∥efk∥2(48)
holds with a probability at least 1−2N−2.
d) An Upper Bound for the 4thLHS Term of (37): By following the same procedures as from (44) to
(48), we can readily show the inequality
max
1≤j≤|YL|−1−|Λt
m−1||⟨etj,r(N)
m−1⟩|≤/radicalbigg
6 logN
n(1 +∥eN∥2) max
1≤j≤|YL|−1−|Λt
m−1||etj∥2.(49)
e) A Lower Bound for the RHS Term of (37): Next, we go on to find a lower bound for the RHS term
of (37). Using (29) and with a=xtjandb=r(N)
m−1,∥, we have
Pr/braceleftigg
|⟨xtj,r(N)
m,∥⟩|<τ√dL∥r(N)
m,∥∥2/bracerightigg
≤/radicalbigg
2
πτ,1≤j≤|YL|−1−|Λt
m−1|, (50)
which together with the assumption that xtj’s are independent yields
Pr/braceleftig
|⟨xtj,r(N)
m,∥⟩|<τ√dL∥r(N)
m,∥∥2,∀j∈I/bracerightig
≤/parenleftig/radicalbigg
2
πτ/parenrightig|YL|−|Λt
m−1|−km
,
I⊂/bracketleftbig
|YL|−1−|Λt
m−1|/bracketrightbig
,|I|=|YL|−|Λt
m−1|−km.(51)
27Published in Transactions on Machine Learning Research (10/2023)
It then follows
Pr/braceleftig
⟨xtkm,r(N)
m−1,∥⟩|≥τ√dL∥r(N)
m,∥∥2/bracerightig
= 1−Pr/braceleftig
⟨xtkm,r(N)
m−1,∥⟩|<τ√dL∥r(N)
m,∥∥2/bracerightig
≥1−Pr/braceleftig
∃I⊂/bracketleftbig
|YL|−1−|Λt
m−1|/bracketrightbig
with|I|=|YL|−1−|Λt
m−1|−km+ 1
s.t.|⟨xtj,r(N)
m,∥⟩|<τ√dL∥r(N)
m,∥∥2,∀j∈I/bracerightig
≥1−/parenleftbigg|YL|−1
km−1/parenrightbigg
max
|I|=|YL|−|Λt
m−1|−km
I⊂[|YL|−1−|Λt
m−1|]Pr/braceleftig
|⟨xtj,r(N)
m,∥⟩|<τ√dL∥r(N)
m,∥∥2,∀j∈I/bracerightig
(a)
≥1−/parenleftbigg|YL|−1
km−1/parenrightbigg
max
|I|=|YL|−|Λt
m−1|−km
I⊂[|YL|−1−|Λt
m−1|]/parenleftig/radicalbigg
2
πτ/parenrightig|YL|−|Λt
m−1|−km
≥1−/parenleftbigge(|YL|−1)
km−1/parenrightbiggkm−1/parenleftig/radicalbigg
2
πτ/parenrightig|YL|−dL−km
,(52)
where (a) holds thanks to (51). Hence, the inequality
|⟨xtkm,r(N)
m−1,∥⟩|≥τ√dL∥r(N)
m−1,∥∥2 (53)
holds with a probability at least 1−/parenleftig
e(|YL|−1)
km−1/parenrightigkm−1/parenleftig/radicalig
2
πτ/parenrightig|YL|−|Λt
m−1|−km
.
With the bounds in (40), (43), (48), (49) and (53), the inequality in (37) is true, and hence the event Em
occurs, when the following inequality holds:
max
l̸=L4 log(N)∥UT
lUL∥F√dldL∥r(N)
m,∥∥2+/radicalbigg
6 logN
n−dL∥r(N)
m,⊥∥2
+ 2/radicalbigg
6 logN
n(1 +∥eN∥2) max
1≤i≤N∥ei∥2≤τ√dL∥r(N)
m−1,∥∥2,(54)
or equivalently,
max
l̸=L∥UT
lUL∥F√dl+√3dL∥r(N)
m,⊥∥2/radicalbig
8(n−dL) logN∥r(N)
m,∥∥2+2√3dL(1 +∥eN∥2)
√8nlogN∥r(N)
m−1,∥∥2max
1≤i≤N∥ei∥2≤τ
4 logN,(55)
which is obtained by multiplying the inequality (54) throughout by the factor√dL×1/(4 log(N)∥r(N)
m−1,∥∥2).
It can then be concluded that occurs once (40), (43), (48), (49), (53) and (55) hold for all 1≤m≤M.
Hence, a lower bound for Pr {/intersectiontextM
m=1Em}can be obtained by finding a lower bound for the probability that
(55) holds for all 1≤m≤M.
Still, our approach seeks an upper bound for the LHS term in (55), hence a sufficient condition guaranteeing
(55), and proves this bound holds with a high probability. Towards this end, we derive an upper bound
for∥r(N)
m−1,⊥∥2,1≤m≤M, an upper bound for ∥e(N)
i∥2,1≤i≤N, and a lower bound for ∥r(N)
m−1,∥∥2,
1≤m≤M; lumping these altogether yields the claimed result. To begin with, we write
∥r(N)
m−1,⊥∥2=∥PS⊥
L(I−YΛm−1(YT
Λm−1YΛm−1)−1YT
Λm−1)yN∥2≤∥PS⊥
LyN∥2≤∥eN∥2.(56)
According to lemma 4, the event/intersectiontextN
i=1{∥ei∥2≤3σ/2}occurs with a probability at least 1−Ne−n/8. Hence,
we conclude that the following sets of inequalities
∥r(N)
m−1,⊥∥2≤3σ/2,1≤m≤M, (57)
28Published in Transactions on Machine Learning Research (10/2023)
∥ei∥2≤3σ/2,1≤i≤N, (58)
hold at once with a probability at least 1−Ne−n/8. Next, we will find a lower bound for ∥r(N)
m−1,∥∥2’s. For
1≤m≤M, we have
r(N)
m−1,∥=PSL(I−YΛm−1(YT
Λm−1YΛm−1)−1YT
Λm−1)yN
=PSL(yN−YΛm−1c∗
m−1) =yN,∥−YΛm−1,∥c∗
m−1,(59)
where yN,∥≜PSLyN,YΛm−1,∥≜PSLYΛm−1andc∗
m−1≜argmin
cm−1∥yN−YΛm−1cm−1∥2. Then a lower
bound for∥r(N)
m−1,∥∥2is obtained as
∥r(N)
m−1,∥∥2=∥yN,∥−YΛm−1,∥c∗
m−1∥2
≥min
c∥yN,∥−YΛm−1,∥c∥2
=∥yN,∥−YΛm−1,∥(YT
Λm−1,∥YΛm−1,∥)−1YT
Λm−1,∥yN,∥∥2
≥∥yN,∥−YΛM−1,∥(YT
ΛM−1,∥YΛM−1,∥)−1YT
ΛM−1,∥yN,∥∥2=∥PByN,∥∥2,(60)
whereB=SL∩R(YΛM−1,∥)⊥is a subspace of dimension dL−p(M−1)>0. Since yN,∥=xN+eN,∥
and both the distributions of xNandeN,∥are rotationally invariant in SL, the normalized yN,∥/∥yN,∥∥2
is uniformly distributed over SL∩Sn−1. Let VB= [v1v2...vdL−p(M−1)]∈Rn×(dL−p(M−1))be a matrix
whose columns form an orthonormal basis for B; augment VBby adding extra p(M−1)columns so that
thedLcolumns of VB= [v1v2...vdL−p(M−1)...vdL]∈Rn×dLform an orthonormal basis for SL. Clearly,
we have yN,∥/∥yN,∥∥2=Va, where a= [a1a2...adL]Tobeys uniform distribution over the unit sphere of
RdLand, in particular, P B(yN,∥/∥yN,∥∥2) =VB/tildewidea, where/tildewidea= [a1a2...adL−p(M−1)]Tobeys the distribution
specified in (Knokhlov, 2006, eq. (7)). Noticing ∥PB(yN,∥/∥yN,∥∥2)∥2=∥VB/tildewidea∥2=∥/tildewidea∥2, the following set
of inequalities hold
Pr/braceleftig/vextenddouble/vextenddouble/vextenddoublePB/parenleftigyN,∥
∥yN,∥∥2/parenrightig/vextenddouble/vextenddouble/vextenddouble
2≤λ/bracerightig
=Pr{∥/tildewidea∥2≤λ}
(a)
≤(Γ(dL/2)
π(dL−p(M−1))/2Γ(p(M−1)/2))/integraldisplay
∥/tildewidea∥2≤λ(1−∥/tildewidea∥2
2)(p(M−1)−2)/2d/tildewidea
(b)
≤(dL
2π)(dL−p(M−1))/2/integraldisplay
∥/tildewidea∥2≤λ(1−∥/tildewidea∥2
2)(p(M−1)−2)/2d/tildewidea
≤(dL
2π)(dL−p(M−1))/2/integraldisplay
∥/tildewidea∥2≤λ1d/tildewidea
= (dL
2π)(dL−p(M−1))/2v(dL−p(M−1))λdL−p(M−1)
(c)
≤(dL
2π)(dL−p(M−1))/26λdL−p(M−1),(61)
where (a) holds by (Knokhlov, 2006, eq. (7)), (b) follows from (Foucart & Rauhut, 2013, eq. (8.1)), in which
v(r)is the volume of unit-ball in Rr, and (c) is true since v(r)≤6for allr∈N(Smith & Vamanamurthy,
1989). Using (60) and (61) with λ=σ/radicalbig
2/dL, we can obtain
∥r(N)
m−1,∥∥2>/radicalbigg
2
dLσ∥yN,∥∥2,1≤m≤M, (62)
holds with a probability at least 1−6(σ/√π)dL−p(M−1). Since yN,∥=xN+eN,∥and∥xN∥2= 1(see
Assumption 1), triangle inequality gives ∥yN,∥∥2≥1−∥eN,∥∥2, which together with (62) implies
∥r(N)
m−1,∥∥2>/radicalbigg
2
dLσ(1−∥eN,∥∥2),1≤m≤M, (63)
29Published in Transactions on Machine Learning Research (10/2023)
holds with a probability as high as (59). With the aid of (57), (58), and (63), we go on to find a sufficient
condition ensuring (55) by finding an upper bound for the LHS of (55). Once (57), (58), and (63) hold, we
can obtain the following set of inequalities
max
l̸=L∥UT
lUL∥F√dl+√3dL∥r(N)
m,⊥∥2/radicalbig
8(n−dL) logN∥r(N)
m,∥∥2+2√3dL(1 +∥eN∥2)
√8nlogN∥r(N)
m−1,∥∥2max
1≤i≤N∥ei∥2
≤max
l̸=L∥UT
lUL∥F√dl+3√
3dL(3 + 3σ)
(8−12σ)√nlogN
(a)
≤max
l̸=Laff(Sl,SL) +3√
3dL(3 + 3σ)
(8−12σ)√nlogN(b)
≤τ
4 logN,(64)
where (a) follows from the definition (13) and (b) is true thanks to Assumption 3 As a result, when (57),
(58) and (63) hold, the inequality in (64) for all 1≤m≤Mis guaranteed. By employing the union bound
technique it can be concluded that the inequality (55) for all 1≤m≤Mholds with a probability at least
1−6(σ/√π)dL−p(M−1)−Ne−n/8. Finally, by using (40), (43), (48), (49), (53) and (55), the proof is thus
completed again by using the union bound.
5.2 Proof of Corollary 1
The lower bound (17) is simply obtained as the probability of the event that the numbers km’s of recovered
true neighbors throughout all Miterations yield the maximal lower bound (15). That is to say, (17) is the
maximum of (15) over all feasible 0≤km≤p,1≤m≤M; more precisely, the minimum objective of the
following optimization problem
min
(k1,...,k m)J0=M/summationdisplay
m=1J(km)
s.t.M/summationdisplay
m=1km−kt= 0
km∈{0,1,2,...,p},∀1≤m≤M,(65)
where
J(km)≜/parenleftigg/parenleftbigg2e(N−|YL|)
(p−km+1)N8 logN/dL/parenrightbiggp−km+1
+/parenleftig/radicalbigg
2
πτ/parenrightig|YL|−dL−km/parenleftige(|YL|−1)
km−1/parenrightigkm−1
+4 + 2c
N2/parenrightigg
1(km>0)(66)
Below, we will show that the two-level sequence
(k1,...,km) =/parenleftig
qt+ 1,qt+ 1,...,qt+ 1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
rt−fold,qt,qt,...,qt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(M−rt)−fold/parenrightig
(67)
solves (65), consequently leading to (17). We recall the following definition and lemma, which are needed in
our proof.
Definition 1. (Pečarić et al., 1992) Let x= [x1x2...xM]T∈RMandy= [y1y2...yM]T∈RMbe two
real vectors whose entries are ordered in the way that x[1]≥x[2]≥...≥x[M]andy[1]≥y[2]≥...≥y[M],
respectively. Then xis said to be majorized by yif
s/summationdisplay
m=1x[m]≤s/summationdisplay
m=1y[m],∀1≤s≤M (68)
ands/summationdisplay
m=1xm=s/summationdisplay
m=1ym. (69)
We sayf:A⊂RMis Schur-convex if f(x)≤f(y)whenever xis majorized by y,x,y∈A.
30Published in Transactions on Machine Learning Research (10/2023)
Lemma 5. (Pečarić et al., 1992, Theorem 12.25) Let f:A⊂RMbe a permutation invariant function, that
is,f(x) =f(Px)for all x∈Aand permutation matrices P∈RM×M, whose first partial derivatives exist
inA. Thenfis Schur-convex in Aif and only if
(xi−xj)/parenleftbigg∂f
∂xi−∂f
∂xj/parenrightbigg
≥0,∀x= [x1x2...xM]T∈A (70)
holds for all 1≤i̸=j≤M.
Using vector-matrix notation, all we have to do is to show
k∗≜/bracketleftig
qt+ 1,qt+ 1,...,qt+ 1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
rt−fold,qt,qt,...,qt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(M−rt)−fold/bracketrightigT
(71)
solves the following optimization problem:
min
k=[k1,...,k m]TJ0=M/summationdisplay
m=1J(kTvm)
s.t.kT1=kt
kTvm∈{0,1,2,...,p},∀1≤m≤M,(72)
in which vm≜[ 0...0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
m−1fold1 0...0 ]Tis themth standard unit vector. It suffices to prove
J0(k∗)≤J0(k) (73)
for any k∈DJ, the feasible set of (72), based on Lemma 5. For this, we first note that the objective J0is
not differentiable, since the function Jin (66) involves the indicator function. To rid of this difficulty, we
consider the differentiable surrogate /tildewideJ:R→RforJ, constructed according to
/tildewideJ(x) =J(x),∀x≥0.9andx= 0, (74)
and
/tildewideJ′(0) =J′(1). (75)
Thanks to (74), the function /tildewideJthus obtained satisfies /tildewideJ(km) =J(km)for allkm∈{0,1,...,p}and/tildewideJ′(km) =
J′(km)for allkm∈{1,2,...,p}. The corresponding differentiable surrogate for J0is accordingly given by
/tildewideJ0(k) =M/summationdisplay
m=1/tildewideJ(kTvm). (76)
Clearly,/tildewideJ0(k) =J0(k)for all k∈DJ, due to condition (74); all the better, /tildewideJ0is Schur-convex in DJ(a
proof is given in Appendix C). Hence, for k,q∈DJsuch that kis majorized by q, we haveJ0(k) =/tildewideJ0(k)≤
/tildewideJ0(q) =J0(q). The inequality (73) is guaranteed once k∗is majorized by any feasible k, which is indeed true
as shown below. For a feasible k= [k1k2...kM]Tsuch thatk1≥k2≥...≥kMwithout loss of generality.
By Definition 1, it suffices to show
s/summationdisplay
m=1k∗
m≤s/summationdisplay
m=1km,∀1≤s≤M. (77)
Assume otherwise that there exists 1≤s≤Msuch that/summationtextq
m=1k∗
m≤/summationtextq
m=1km,1≤q≤s−1, whereas/summationtexts
m=1k∗
m>/summationtexts
m=1km. Then we have k∗
s> ks, which together with k∗
s∈{qt,qt+ 1}impliesqt≥km, for
s+ 1≤m≤M, ending in the following contradiction:
kt=s/summationdisplay
m=1km+M/summationdisplay
m=s+1km≤s/summationdisplay
m=1km+M/summationdisplay
m=s+1qt≤s/summationdisplay
m=1km+M/summationdisplay
m=s+1k∗
m<s/summationdisplay
m=1k∗
m+M/summationdisplay
m=s+1k∗
m=kt.(78)
31Published in Transactions on Machine Learning Research (10/2023)
5.3 Proof of Theorem 2
Belowwefirstderiveanequivalentconditionfortheproposedstoppingrulethatismoreamenabletoanalysis.
Recall the residual r(N)
mobtained in the mth iteration is the orthogonal projection of the previous residual
r(N)
m−1ontoR(YΛm)⊥. Since∥r(N)
m−1∥2=∥r(N)
m∥2+∥r(N)
m−1−r(N)
m∥2, obtained from the Pythagorean theorem,
the proposed stopping rule can be rewritten as
1−∥r(N)
m∥2
∥r(N)
m−1∥2= 1−/radicalig
1−∥/tildewider(N)
m∥2
2≤/radicalbiggp
n, (79)
where
/tildewider(N)
m≜r(N)
m−1−r(N)
m
∥r(N)
m−1∥2∈R(YΛm) (80)
is the normalized difference of the residual vectors. Rearranging the inequality in (79) yields the following
equivalent stopping condition
∥/tildewider(N)
m∥2≤/radicalig
2/radicalbig
p/n−p/n. (81)
Below we show that, with a high chance, (81) does not hold when m≤⌈dL/p⌉and is achieved (hence,
GOMP stops) with m=⌈dL/p⌉+ 1. Formally, we prove the following inequalities
∥/tildewider(N)
m∥2>/radicalig
2/radicalbig
p/n−p/n,∀1≤m≤⌈dL/p⌉ (82)
and
∥/tildewider(N)
⌈dL/p⌉+1∥2≤/radicalig
2/radicalbig
p/n−p/n (83)
hold at once with a probability as high as claimed by Theorem 2.
To proceed, let yjbe a selected data vector in the mth iteration, 1≤m≤⌈dL/p⌉. As long as (49), (53),
(57), (58) and (63) hold, we have
|⟨yj,r(N)
m−1⟩|(a)
≥|⟨ykm,r(N)
m−1⟩|(b)
≥|⟨xtkm,r(N)
m−1,∥⟩|− max
1≤j≤|YL|−1−|Λt
m−1||⟨etj,r(N)
m−1⟩|
(c)
≥/bracketleftig∥r(N)
m−1,∥∥2√dL−/radicalbigg
6 logN
n∥r(N)
m−1,∥∥2max
1≤i≤N∥ei∥2/bracketrightig
=∥r(N)
m−1∥2/bracketleftig∥r(N)
m−1,∥∥2/radicalig
dL(∥r(N)
m−1,⊥∥2
2+∥r(N)
m−1,∥∥2
2)−/radicalbigg
6 logN
nmax
1≤i≤N∥ei∥2/bracketrightig
(d)
≥∥r(N)
m−1∥2/bracketleftig√
2(1−3σ/2)/radicalbig
9d2
L/4 + 4(1−3σ/2)2dL−3√6 logNσ
2√n
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜η/bracketrightig
,(84)
in which (a) is true since yjis selected as a neighbor, (b) follows form (35), (c) holds due to (49)
and (53), and (d) is obtained by combining (57), (58), and (63). By definition (80) we have /tildewider(N)
m=
PR(YΛm)(r(N)
m−1/∥r(N)
m−1∥2), hence
∥/tildewider(N)
m∥2≥∥Pspan{yj}(r(N)
m−1
∥r(N)
m−1)∥2))∥2=|⟨yj,r(N)
m−1⟩|
∥yj∥2∥r(N)
m−1∥2, (85)
which together with (84) implies
∥/tildewider(N)
m∥2≥|⟨yj,r(N)
m−1⟩|
∥yj∥2∥r(N)
m−1∥2≥η∥r(N)
m−1∥2
∥yj∥2∥r(N)
m−1∥2=η
∥yj∥2. (86)
32Published in Transactions on Machine Learning Research (10/2023)
With the aid of (86), inequality (82) is guaranteed once η/∥yj∥2>/radicalig
2/radicalbig
p/n−p/n, which is typically true
since the ambient dimension nis drastically large.
As for (83), again by definition (80) we write
/tildewider(N)
⌈dL/p⌉+1=PR(YΛ⌈dL/p⌉+1)(r(N)
⌈dL/p⌉/∥r(N)
⌈dL/p⌉∥2). (87)
Let{b1,...,b(⌈dL/p⌉+1)p}be an orthonormal basis of R(YΛ⌈dL/p⌉+1), arranged in a way that
{bp+1,...,b(⌈dL/p⌉+1)p}is an orthonormal basis of R(YΛ⌈dL/p⌉). Since r(N)
⌈dL/p⌉is orthogonal toR(YΛ⌈dL/p⌉),
⟨r(N)
⌈dL/p⌉,bj⟩= 0for allp+ 1≤j≤(⌈dL/p⌉+ 1)p. Thus, it follows
Pr/braceleftbig
∥/tildewider(N)
⌈dL/p⌉+1∥2≤/radicalig
2/radicalbig
p/n−p/n/bracerightbig
=Pr/braceleftig
∥PR(YΛ⌈dL/p⌉+1)(r(N)
⌈dL/p⌉/∥r(N)
⌈dL/p⌉∥2)∥2≤/radicalig
2/radicalbig
p/n−p/n/bracerightig
≥Pr/braceleftbiggp/intersectiondisplay
k=1/braceleftig
∥Pspan{bk}(r(N)
⌈dL/p⌉/∥r(N)
⌈dL/p⌉∥2)∥2≤/radicalig
2/radicalbig
p/n−p/n/√p/bracerightig/bracerightbigg
= 1−Pr/braceleftbiggp/uniondisplay
k=1/braceleftig
∥Pspan{bk}(r(N)
⌈dL/p⌉/∥r(N)
⌈dL/p⌉∥2)∥2>/radicalig
2/radicalbig
p/n−p/n/√p/bracerightig/bracerightbigg
≥1−p/summationdisplay
k=1Pr/braceleftig
∥Pspan{bk}(r(N)
⌈dL/p⌉/∥r(N)
⌈dL/p⌉∥2)∥2>/radicalig
2/radicalbig
p/n−p/n/√p/bracerightig
= 1−p/summationdisplay
k=1Pr/braceleftig
|⟨r(N)
⌈dL/p⌉/∥r(N)
⌈dL/p⌉∥2,bk⟩|>/radicalig
2/radicalbig
p/n−p/n/√p/bracerightig
.(88)
According to our proof of Theorem 1, the neighbors selected in all ⌈dL/p⌉iterations (i.e., all columns of
YΛ⌈dL/p⌉) are correct when (40), (43), (48), (49), (53), (57), (58) and (63) hold for all 1≤m≤⌈dL/p⌉. If
so, there then exists /tildewidec∈Rp⌈dL/p⌉such that the “noiseless” signal vector xNand neighbor matrix XΛ⌈dL/p⌉
satisfy
xN=XΛ⌈dL/p⌉/tildewidec. (89)
Noting that YΛ⌈dL/p⌉=XΛ⌈dL/p⌉+EΛ⌈dL/p⌉, where EΛ⌈dL/p⌉is the noise matrix, the residual vector at the
⌈dL/p⌉th iteration can be written as
r(N)
⌈dL/p⌉= (I−YΛ⌈dL/p⌉(YT
Λ⌈dL/p⌉YΛ⌈dL/p⌉)−1YT
Λ⌈dL/p⌉)(xN+eN)
= (I−YΛ⌈dL/p⌉(YT
Λ⌈dL/p⌉YΛ⌈dL/p⌉)−1YT
Λ⌈dL/p⌉)(xN+YΛ⌈dL/p⌉/tildewidec+eN)
= (I−YΛ⌈dL/p⌉(YT
Λ⌈dL/p⌉YΛ⌈dL/p⌉)−1YT
Λ⌈dL/p⌉)(EΛ⌈dL/p⌉/tildewidec+eN),(90)
which is the projection of the Gaussian random vector EΛ⌈dL/p⌉/tildewidec+eNontoR(YΛ⌈dL/p⌉)⊥. Being a linear
combination of rotationally invariant vectors, EΛ⌈dL/p⌉/tildewidec+eNremains so; this implies the projection r(N)
⌈dL/p⌉
is also rotationally invariant on R(YΛ⌈dL/p⌉)⊥. Then using (28) with a=r(N)
⌈dL/p⌉/∥r(N)
⌈dL/p⌉∥2andb=bk,
1≤k≤p, we can obtain
Pr/braceleftig
|⟨r(N)
⌈dL/p⌉/∥r(N)
⌈dL/p⌉∥2,bk⟩|>/radicalig
2/radicalbig
p/n−p/n/√p/bracerightig
≈2e−√
n/p, (91)
which together with (88) implies (83) holds with a probability at least 1−2pe−√
n/p. In summary, once the
inequalities (40), (43), (48), (49), (53), (57), (58), and (63) hold for all 1≤m≤⌈dL/p⌉, then (82) and (83)
hold with a probability at least 1−2pe−√
n/p. The proof is then completed by employing the union bound.
33Published in Transactions on Machine Learning Research (10/2023)
6 Conclusion
Fast acquisition of many true neighbors underlies the success of SSC in real-world applications. Under
the framework of greedy selection, which is pretty suited for low-complexity implementation, we propose a
GOMP sparse regression scheme, together with a novel subspace-dimension-aware stopping rule, to boost
neighbor recovery. Thanks to multiple neighbor identification per iteration, the proposed GOMP involves
fewer iterations, thereby enjoying even lower algorithmic complexity than conventional OMP. In addition,
the residual vector better stands up to noise corruption, consequently bringing about higher neighbor iden-
tification accuracy. Our proposed stopping criterion is appealing in that it depends entirely on a knowledge
of the ambient space dimension, in marked contrast with the existing solution (Tschannen & Bölcskei, 2018)
that requires extra off-line estimation of either subspace dimension or noise power. Besides algorithm de-
velopment, in-depth neighbor recovery rate analyses were conducted to justify the merits of the proposed
GOMP; the obtained analytic results are further validated by computations using both synthetic and real
datasets. Overall, our study presents a new greedy-based SSC scheme, with provable performance guar-
antees, that can pave the way for practical applications. Future work will analyze AoD of both GOMP
and OMP under the considered semi-random model, particularly to show GOMP enjoys a smaller average
AoD; an analytic study of AoD would further offer certain guidelines on determining the optimum number
of neighbors to recover in each iteration, for the purpose of improving the current scheme, which identifies
a constant number (p≥1)of neighbors throughout all iterations, in a more dynamic environment.
Acknowledgments
This work is sponsored by the National Council of Science and Technology of Taiwan under grants NSTC
111-2221-E-A49-067-MY3, NSTC 112-2811-E-A49-545-MY2, and MOST 110-2221-E-A49-042-MY3. The
work of C.-H. Liu was supported in part by the National Science Foundation under Awards CNS-2006453
and ECCS-2210106.
References
S. Acharya, A. K. Pant, and P. K. Gyawali. Deep learning based large scale handwritten devanagari charac-
ter recognition. International Conference on Software, Knowledge, Information Management and Appli-
cations, pp. 1–6, 2015.
R. G. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine , 24(4):118–124, 2007.
E. J. Candès and M. B. Wakin. An introduction to compressive sampling. IEEE Signal Processing Magazine ,
25(2):21–30, 2008.
J.C.ChenandJ.J.J.Lien. Aview-basedstatisticalsystemformulti-viewfacedetectionandposeestimation.
Image and Vision Computing , 27(9):1252–1271, 2009.
Y. Chen, G. Li, and Y. Gu. Active orthogonal matching pursuit for sparse subspace clustering. IEEE Signal
Processing Letters , 25(2):164–168, 2018.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms . MIT Press, 2009.
M. Davenport, M. Duarte, Y. C. Eldar, and G. Kutyniok. Compressed Sensing: Theory and Applications .
Cambridge University Press, 2011.
E. L. Dyer, A. C. Sankaranarayanan, and R. G. Baraniuk. Greedy feature selection for subspace clustering.
Journal of Machine Learning Research , 14(1):2487–2517, 2013.
M. Elad. Sparse and Redundant Representations: From Theory to Applications in Signal and Image Pro-
cessing. Springer, 2010.
E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithms, theory, and applications. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 35:2765–2781, 2013.
34Published in Transactions on Machine Learning Research (10/2023)
S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing . Springer, 2013.
T. Fu, Z. Zong, and X. Yin. Generalized orthogonal matching pursuit with singular value decomposition.
IEEE Geoscience and Remote Sensing Letters , 19:1–5, 2022.
A. Goh and R. Vidal. Segmenting motions of different types by unsupervised manifold clustering. IEEE
Conference on Computer Vision and Pattern Recognition , 2007.
T. Hastie, R. Tibshirani, and M. Wainwright. Statistical Learning with Sparsity: The Lasso and Generaliza-
tions. CRC Press, 2015.
R. Heckel and H. Bölcskei. Robust subspace clustering via thresholding. IEEE Transactions on Information
Theory, 61(11):6320–6342, 2015.
V. I. Knokhlov. The uniform distribution on a sphere in Rs.Theory of Probability and Its Applications , 50
(3):386–399, 2006.
A. Krizhevsky. Learning multiple layers of features from tiny images, technical report, university of toronto.
2009.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE , 86(11):2278–2324, 1998.
K. C. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face recognition under variable lighting.
IEEE Transactions on Pattern Analysis and Machine Intelligence , 27(5):684–698, 2005.
C. Li, C. You, and R. Vidal. Structured sparse subspace clustering: A joint affinity learning and subspace
clustering framework. IEEE Transactions on Image Processing , 26(6):2988–3001, 2017.
G. Liu, S. Yan Z. Lin, J. Sun, Y. Yu, and Y. Ma. Robust recovery of subspace structures by low-rank
representation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 35(1):171–184, 2013.
C. Lu, J. Feng, Z. Lin, T. Mei, and S. Yan. Subspace clustering by block diagonal representation. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 41(2):487–501, 2019.
J. E. Marsden and M. J. Hoffman. Elementary Classical Analysis . W. H. Freeman Co, 1993.
S. Matsushima and M. Brbic. Selective sampling-based scalable sparse subspace clustering. Advances in
Neural Information Processing Systems , 2019.
S. A. Nene, S. K. Nayar, and H. Murase. Columbia object image library (coil-100), technical report cucs-
006-96. 1996.
A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. Advances in
Neural Information Processing Systems , 2001.
Y. Panagakis and C. Kotropoulos. Elastic net subspace clustering applied to pop/rock music structure
analysis. Pattern Recognition Letters , 38:46–53, 2014.
X. Peng, L. Zhang, and Z. Yi. Scalable sparse subspace clustering. IEEE Conference on Computer Vision
and Pattern Recognition , pp. 430–437, 2013.
J.E.Pečarić, F.Proschan, andY.L.Tong. Convex Functions, Partial Orderings, and Statistical Applications ,
volume 187 of Mathematics in Science and Engineering. Academic Press, 1992.
Y. Qin, X. Zhang, L. Shen, and G. Feng. Maximum block energy guided robust subspace clustering. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 45(2):2652–2659, 2023.
D. J. Smith and M. K. Vamanamurthy. How small is a unit ball. Mathematics Magazine , 62(2):101–107,
1989.
35Published in Transactions on Machine Learning Research (10/2023)
M. Soltanolkotabi and E. J. Candès. A geometric analysis of subspace clustering with outliers. The Annals
of Statistics , 40(4):2195–2238, 2012.
M. Soltanolkotabi, E. Elhamifar, and E. J. Candès. Robust subspace clustering. The Annals of Statistics ,
42(2):669–699, 2014.
B. L. Sturm and M. G. Christensen. Comparison of orthogonal matching pursuit implementations. European
Signal Processing Conference , 2012.
M. Tschannen and H. Bölcskei. Noisy subspace clustering via matching pursuits. IEEE Transactions on
Information Theory , 64(6):4081–4104, 2018.
R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science . Cambridge,
2018.
R. Vidal. Subspace clustering. IEEE Signal Processing Magazine , 28(2):52–68, 2011.
U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing , 17:395–416, 2007.
J. Wang, S. Kwon, and B. Shim. Generalized orthogonal matching pursuit. IEEE Transactions on Signal
Processing , 60(12):6202–6216, 2012.
Y. Wang, Y. X. Wang, and A. Singh. A theoretical analysis of noisy sparse subspace clustering on
dimensionally-reduced data. IEEE Transactions on Information Theory , 65(2):685–706, 2019.
Y. X. Wang and H. Xu. Noisy sparse subspace clustering. Journal of Machine Learning Research , 17(1):
1–41, 2016.
J. Y. Wu, L. C. Huang, M. H. Yang, and C. H. Liu. Sparse subspace clustering via two-step reweighted
ℓ1-minimization: Algorithm and provable neighbor recovery rates. IEEE Transactions on Information
Theory, 67(2):1216–1263, 2021.
A. Yang, J. Wright, Y. Ma, and S. Sastry. Unsupervised segmentation of natural images via lossy data
compression. Computer Vision and Image Understanding , 110(2):212–225, 2008.
J. Yang, K. Wang J. Liang, P. L. Rosin, and M. H. Yang. Subspace clustering via good neighbors. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 42(6):1537–1544, 2020.
C. You, D. Robinson C. Li, and R. Vidal. Oracle based active set algorithm for scalable elastic net subspace
clustering. IEEE Conference on Computer Vision and Pattern Recognition , pp. 3928–3937, 2016a.
C. You, D. Robinson, and R. Vidal. Scalable sparse subspace clustering by orthogonal matching pursuit.
IEEE Conference on Computer Vision and Pattern Recognition , pp. 3918–3927, 2016b.
C. You, C. Li, D.P. Robinson, and R. Vidal. A scalable exemplar-based subspace clustering algorithm for
class-imbalanced data. Proceedings of the European Conference on Computer Vision , pp. 67–83, 2018.
W. Zhu, S. Yang, and Y. Zhu. Restricted connection orthogonal matching pursuit for sparse subspace
clustering. IEEE Signal Processing Letters , 26(12):1892–1896, 2019.
A Appendix
The following lemma is needed for deriving Lemma 2.
Lemma 6. (Soltanolkotabi & Candès, 2012, Extracted from the proof of Lemma 7.5) Let a∈Rd1be
distributed uniformly on {x|x∈Rd1,∥x∥2= 1}andB∈Rd2×d1. Then for α>0we have
Pr/braceleftbigg
∥Ba∥2>α∥B∥F√d2/bracerightbigg
≤2e−α2/2. (92)
36Published in Transactions on Machine Learning Research (10/2023)
Assume xi∈Sl,l̸=L. Then we have xi=Ulb, in which bis uniformly distributed over the unit sphere of
Rdlby Assumption 1. Setting C=UT
LUlandα= 4 logN/√dL, Lemma 6 implies
Pr/braceleftbigg
∥UT
Lxi∥2>4 logN∥UT
LUl∥F√dLdl/bracerightbigg
≤2e−8(logN)2/dL= 2/parenleftbig
elogN/parenrightbig−8 logN/dL=2
N(8 logN)/dL.(93)
Since r(N)
m,∥∈SL, we have r(N)
m,∥/∥r(N)
m,∥∥2=ULzwith∥z∥2= 1. The Cauchy-Schwartz inequality implies
/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
r(N)
m,∥/∥r(N)
m,∥∥2,xi/angbracketrightig/vextendsingle/vextendsingle/vextendsingle=|zTUT
Lxi|≤∥z∥2∥UT
Lxi∥2=∥UT
Lxi∥2, (94)
and therefore
/braceleftig/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
xi,r(N)
m,∥/∥r(N)
m,∥∥2/angbracketrightig/vextendsingle/vextendsingle/vextendsingle>4 log(N)∥UT
lUL∥F√dLdl/bracerightig
⊂/braceleftbigg
∥UT
Lxi∥2>4 log(N)∥UT
lUL∥F√dLdl/bracerightbigg
.(95)
With (93) and (95), it follows immediately
Pr/braceleftig/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
xi,r(N)
m,∥/∥r(N)
m,∥∥2/angbracketrightig/vextendsingle/vextendsingle/vextendsingle>4 log(N)∥UT
lUL∥F√dldL/bracerightig
≤2
N(8 logN)/dL. (96)
Appendix B
Let
D(v)≜/braceleftig
(y1,y2,...,yN) :r(N)
m,⊥/∥r(N)
m,⊥∥2=v/bracerightig
, (97)
f:Rn×...×Rn→Rbe the probability density function of (y1,y2,...,yN), andSn−1be the unit-sphere of
Rn. Then for z∈S⊥
L∩Sn−1, we have
Pr/braceleftig
|⟨z,r(N)
m,⊥/∥r(N)
m,⊥∥2⟩|>ϵ/bracerightig
=/integraldisplay
(S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|>ϵ}/bracketleftig/integraldisplay
D(v)f(y1,y2,...,yN)dy1dy2...dyN/bracketrightig
dv
≤/integraldisplay
(S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|≥ϵ}/bracketleftig/integraldisplay
K∩D (v)f(y1,y2,...,yN)dy1dy2...dyN
+/integraldisplay
Kc∩D(v)f(y1,y2,...,yN)dy1dy2...dyN/bracketrightig
dv,(98)
where the last inequality holds by defining
K≜{x+e:x∈Sk1∩Sn−1,∥e∥2≤3σ/2}×...×{x+e:x∈SkN∩Sn−1,∥e∥2≤3σ/2},(99)
in which we assume that yi∈ Yki, hence xi∈ Ski, for 1≤i≤N. Since the maximum function is
continuous and the composition of continuous functions is also continuous, r(N)
m,⊥/∥r(N)
m,⊥∥2is a continuous
function of (y1,y2,...,yN). Consequently, since (S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|≥ϵ}is closed, the inverse image
ofD((S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|≥ϵ})is also closed (Marsden & Hoffman, 1993). Notice that K∩D ((S⊥
L∩
Sn−1)∩{r:|⟨r,z⟩|≥ϵ})is closed and bounded in the Euclidean space, and is therefore compact by the
Heine-Borel theorem (Marsden & Hoffman, 1993). Since f(y1,y2,...,yN)is continuous on the compact set
K∩D ((S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|≥ϵ}), by the extreme value theorem (Marsden & Hoffman, 1993) there
exists a constant δsuch that
f(y1,y2,...,yN)≤δ,∀(y1,y2,...,yN)∈K∩D ((S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|≥ϵ}), (100)
37Published in Transactions on Machine Learning Research (10/2023)
which together with (98) implies
Pr/braceleftig/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
z,r(N)
m,⊥
∥r(N)
m,⊥∥2/angbracketrightig/vextendsingle/vextendsingle/vextendsingle>ϵ/bracerightig
=/integraldisplay
(S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|≥ϵ}/bracketleftig
δ|K|+/integraldisplay
Kc∩D(v)f(y1,y2,...,yN)dy1dy2...dyN/bracketrightig
dv
(a)
≤/integraldisplay
(S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|≥ϵ}/bracketleftig
δ|K|+Ne−n/8/bracketrightig
dv
(b)=/integraldisplay
(S⊥
L∩Sn−1)∩{r:|⟨r,z⟩|≥ϵ}/bracketleftig
c/A(S⊥
L∩Sn−1)/bracketrightig
dv(c)
≤2ce−(n−dL)ϵ2/2,(101)
where (a) holds by lemma 4 with |K|here denoting the Lebesgue measure of K, (b) is true as we define
A(S⊥
L∩Sn−1)to be the surface area of S⊥
L∩Sn−1and the constant cis defined as c≜(δ|K|+Ne−n/8)A(S⊥
L∩
Sn−1), and (c) follows from (28). With (101), we reach
Pr/braceleftig/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
xi,r(N)
m,⊥/∥r(N)
m,⊥∥2/angbracketrightig/vextendsingle/vextendsingle/vextendsingle>ϵ/bracerightig
=/integraldisplay
RnPr/braceleftig/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
xi,r(N)
m,⊥/∥r(N)
m,⊥∥2/angbracketrightig/vextendsingle/vextendsingle/vextendsingle>ϵ/vextendsingle/vextendsingle/vextendsinglexi=v/bracerightig
fxi(v)dv
(a)
≤/integraldisplay
Rn2en−(n−dL)ϵ2/2fxi(v)dv= 2en−(n−dL)ϵ2/2,(102)
where (a) holds by (101), in which fxiis the probability density function of xi. Letϵ=/radicalbig
6 logN/(n−dL).
Then (102) gives
Pr/braceleftig/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
xi,r(N)
m,⊥/∥r(N)
m,⊥∥2/angbracketrightig/vextendsingle/vextendsingle/vextendsingle>/radicalbigg
6 logN
n−dL/bracerightig
≤2cN−3. (103)
Appendix C
In this appendix, we prove the Schur-convexity of /tildewideJ0. Notably, straightforward manipulations show
(km−kq)/parenleftig∂J0
∂km−∂J0
∂kq/parenrightig
= (km−kq)(J′(km)−J′(kq))≥0,∀km,kq∈{1,2,...,p}.(104)
It then follows
(km−kq)/parenleftig∂/tildewideJ0
∂km−∂/tildewideJ0
∂kq/parenrightig
= (km−kq)(/tildewideJ′(km)−/tildewideJ′(kq))(a)= (km−kq)(J′(km)−J′(kq)),
∀km,kq∈{1,2,...,p}.(105)
where (a) holds by (74) and (b) is due to (104). When evaluated around km∈{1,2,...,p}andkq= 0, the
resultant partial derivative reads
(km−kq)/parenleftig∂/tildewideJ0
∂km−∂/tildewideJ0
∂kq/parenrightig
=km(/tildewideJ′(km)−/tildewideJ′(0))(a)=km(J′(km)−/tildewideJ′(0))
(b)=km(J′(km)−J′(1))(c)
≥0,(106)
where (a) holds by (74), (b) follows from (75), and (c) is true due to (104). Clearly, the function /tildewideJ0is
permutation invariant, together with (105) and (106) ensures that /tildewideJ0satisfies the conditions of Lemma 5,
and thus is Schur-convex.
38