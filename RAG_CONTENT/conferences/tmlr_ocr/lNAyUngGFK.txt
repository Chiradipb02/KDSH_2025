Published in Transactions on Machine Learning Research (04/2024)
Beyond Human Data: Scaling Self-Training for Problem-
Solving with Language Models
Avi Singh*, John D Co-Reyes*, Rishabh Agarwal*,
Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon
Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova,
Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle
Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy,
Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L Bileschi, Noah
Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan
Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, Noah Fiedel
*Equal Contribution, All authors are with Google DeepMind.
Correspondence to {singhavi, jcoreyes, rishabhagarwal}@google.com.
Reviewed on OpenReview: https://openreview.net/forum?id=lNAyUngGFK
Abstract
Fine-tuning language models (LMs) on human-generated data remains a prevalent practice.
However, the performance of such models is often limited by the quantity and diversity of
high-quality human data. In this paper, we explore whether we can go beyond human data
on tasks where we have access to scalar feedback, for example, on math problems where
one can verify correctness. To do so, we investigate a simple self-training method based on
expectation-maximization, which we call ReSTEM, where we (1) generate samples from the
model and filter them using binary feedback, (2) fine-tune the model on these samples, and
(3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding
benchmarks using PaLM-2 models, we find that ReSTEMscales favorably with model size
and significantly surpasses fine-tuning only on human data. Overall, our findings suggest
self-training with feedback can reduce dependence on human-generated data.
1 Introduction
Large Language Models (LLMs) are revolutionizing the landscape of deep learning, showcasing remarkable
capabilities in generating human-quality text and tackling diverse language tasks (Google et al., 2023; Ope-
nAI, 2023). While supervised fine-tuning (SFT) on human-collected data further boosts their performance
on tasks of interest, acquiring high-quality human data poses a significant bottleneck. This is particularly
demanding for complex problem-solving tasks, requiring significant resources and expert knowledge. To
address this hurdle, model-generated synthetic data emerges as a promising alternative, offering scalability
and cost-effectiveness, provided its quality can be ensured. While LLMs hold the potential to self-evaluate
generated data, this paper explores a simpler setting where an external, scalar feedback signal serves as a
quality indicator for each generated sample.
To investigate training on model-generated data, we consider a simple yet powerful self-training approach
for language models that requires only two capabilities: 1) generating samples from the model and 2)
evaluating these samples with a scoring mechanism. This approach shares similarities with Reinforced Self-
Training (ReST) proposed by Gulcehre et al. (2023). We make some modifications to ReST (detailed in
Section 3), and call our approach ReSTEM. We show that ReSTEMcan be viewed as applying expectation-
maximization for reinforcement learning (Dayan & Hinton, 1997; Peters & Schaal, 2007), which we present
formally in Section 3. Specifically, ReSTEMalternates between the expectation and maximization steps:
1Published in Transactions on Machine Learning Research (04/2024)
1520253035404-shot T est Accuracy (%)GPT-4
LLaMA-2 70BWizardMath 70BMetaMath 70B
Inflection-1Llemma 34B
Llemma 7B 
Mistral 7B (maj@4)Minerva 62BMinerva 540B
PaLM 2-SPaLM 2-L
Grok-0 (33B)PaLM 2-L (ReSTEM)
PaLM 2-S (ReSTEM)Reasoning: MATH
304050600-shot Accuracy (%)PaLM 2-S*PaLM 2-LGPT-4
GPT-3.5 (ChatGPT)WizardCoder 15B
LLaMA-2 70BCode LLaMA 34BCode Llama Python 34B
Inflection-1
Mistral 7BGrok-0 (33B)PaLM 2-L (ReSTEM)
PaLM 2-S* (ReSTEM)Code Generation: HumanEval
Figure 1: Self-training with ReSTEMsubstantially improves test performance of PaLM 2 models on two
challenging benchmarks: MATH and HumanEval. Results for other models are shown for general progress
on these tasks and are typically not comparable due to difference in model scales. GPT-4 results are taken
from Bubeck et al. (2023). The x-axis approximately denotes release time (not to scale).
1.Generate (E-step): The language model generates multiple output samples for each input context.
Then, we filter these samples using a binary reward to collect the training dataset.
2.Improve (M-step): The original language model is supervised fine-tuned on the training dataset
from the previous Generate step. The fine-tuned model is then used in the next Generate step.
ReSTEM, with its various adaptations (Section 4), has demonstrated success in enhancing language models
across diverse domains, including machine translation (Norouzi et al., 2016; Gulcehre et al., 2023), semantic
parsing (Agarwal et al., 2019), preference alignment (Dong et al., 2023), and elementary reasoning (Zelikman
et al., 2022; Yuan et al., 2023). However, prior works primarily applied training with self-generated data
to relatively small language models (up to 7B parameters), with limited scalability observed for larger
models (Yuan et al., 2023). Complementing these efforts, our work aims to investigate the effectiveness and
scalability of model-generated synthetic data compared to human-generated data in two challenging, less
explored domains: competition-level mathematical problem-solving (MATH) (Hendrycks et al., 2021b) and
code generation (APPS) (Hendrycks et al., 2021a).
Our empirical findings reveal significant advancements in both mathematical reasoning and code generation
capabilities when applying ReSTEMto PaLM 2 models of varying scales (Figure 1). Notably, models fine-
tuned on model-generated synthetic data exhibit remarkably larger performance gains compared to those
trained on human-written data (Figure 2, 3). Interestingly, exceeding a couple of iterations of ReSTEMleads
todiminishingimprovement, indicatingpotentialoverfittingonsmallamountoftrainingproblems(Figure4).
Additionally, models fine-tuned using ReSTEMimprove pass@k as well as majority voting performance. Fur-
thermore, these fine-tuned models demonstrate enhanced performance on related but held-out benchmarks,
including math problems (GSM8K and Hungarian HS finals), coding (HumanEval), and Big-Bench Hard
tasks. We also perform ablation studies to investigate the effect of number of model-generated solutions,
training problems, and iterations for ReSTEMfine-tuning. Overall, our findings suggest self-training with
feedback as a promising approach to reduce dependence on human data.
The key contributions of this work are:
•We introduce ReSTEMthat enables learning from self-generated data for LLMs, employing a prin-
cipled expectation-maximization approach within a reinforcement learning framework.
•We demonstrate that training on self-generated solutions surpasses training on human-generated
solutions in problem-solving domains, such as mathematics and code generation.
•Through comprehensive ablation studies, we pinpoint the crucial elements necessary for attaining
optimal performance.
2Published in Transactions on Machine Learning Research (04/2024)
•LLMs fine-tuned with ReSTEMexhibit robust transfer capabilities across various held-out tasks.
2 Preliminaries
An autoregressive language model produces an output sequence y= (y1,y2,....yT)given a context (or source
input) x= (x1,x2,...xL), where the tokens xl,ytbelong to a fixed vocabulary. Auto-regressive generation
involves predicting tokens one at a time, based on the previously generated tokens. Assuming that the model
is parameterized by θ, the conditional probability distribution of generating a sequence ygiven xis
pθ(y|x) =T/productdisplay
t=1pθ(yt|y<t,x),
with the convention y1:0=∅andy1:t−1= (y1,y2,....yt−1). For ease of notation, we define p(yt|x) :=
p(yt|y<t,x). The probability of predicting tthtokenyt,p(yt|x), is determined using a softmax with temper-
atureγ:p(yt|x) =exp(zt/γ)/summationtextM
i=1exp(zi/γ), whereztis the logit score for the token yt. Higher values of temperature γ
introduces more randomness, while a lower value makes the output more deterministic by favoring the most
probable words.
Given a dataset Dof inputs xand human-generated outputs y, supervised fine-tuning (SFT) trains the
policy by minimizing the negative log likelihood loss:
LSFT(θ) =−E(x,y)∼D/bracketleftiggT/summationdisplay
t=1logpθ(yt|y1:t−1,x)/bracketrightigg
. (1)
Wealsoassumeaccesstoadeterministicsequence-level(orterminal)reward r(x,y). Then, thereinforcement
learning (RL) objective corresponds to:
LRL(θ) =Ex∼D/bracketleftbig
Ey∼pθ(y|x)[r(x,y)]/bracketrightbig
.
OptimizingLRLloss directly using online RL methods, such as policy gradients, requires updating and
sampling from the policy numerous times during training. However, the computational cost of fine-tuning
on a continual flow of new samples becomes a limitation of online methods, especially when the sizes of the
policy network grow to tens or hundreds of billion parameters. We discuss an alternative to such online RL
approaches in the next section.
3 Expectation-Maximization for Reinforced Self-Training
Expectation-Maximization (EM) for RL We first describe the EM-based framework for RL with
language models, building upon the prior work by Dayan & Hinton (1997). Let’s define a binary optimality
variableO,suchthat p(O= 1|x,y)∝f(r(x,y)), forsomenon-decreasingnon-negativefunction f:R→R+.
We want to maximize the log-likelihood of observing O= 1(obtaining high reward):
logp(O= 1|x) := log/summationdisplay
ypθ(y|x)p(O= 1|x,y).
However, thesumoverallpossiblesequences yistypicallyintractable. Insteadofmaximizing logp(O= 1;x),
one can consider maximizing its ELBO L(pθ,q)with respect to parameters θand variational distribution
q(y|x). Specifically,
logp(O= 1|x) = log Eq(y|x)/bracketleftbiggp(O= 1|x,y)pθ(y|x)
q(y|x)/bracketrightbigg
≥Eq(y|x)/bracketleftbigg
logp(O= 1|x,y)pθ(y|x)
q(y|x)/bracketrightbigg
(Jensen’s inequality )
=Eq(y|x)[logp(O= 1|x,y)]−KL[q(y|x)||pθ(y|x)]
=:L(pθ,q) (2)
3Published in Transactions on Machine Learning Research (04/2024)
Algorithm 1: ReST (Expectation-Maximization). Given a initial policy (e.g., pre-trained LM),
ReSTEMiteratively applies Generate andImprove steps to update the policy.
Input:D: Training dataset, Dval: Validation dataset, L(x,y;θ): loss,r(x,y): Non-negative reward
function,I: number of iterations, N: number of samples per context
fori= 1toIdo
// Generate (E-step)
Generate dataset Diby sampling:Di={(xj,yj)|N
j=1s.t.xj∼D,yj∼pθ(y|xj)}AnnotateDi
with the reward r(x,y).
// Improve (M-step)
whilereward improves on Dvaldo
Optimiseθto maximize objective: J(θ) =E(x,y)∼Di[r(x,y) logpθ(y|x)]
end
end
Output: Policypθ
The EM algorithm (Dempster et al., 1977) for Equation 2 alternates between an E-step and M-step: at
iterationt, denote the language model parameter to be θtand the variational distribution to be qt.
•E-step:qt+1= arg max qL(pθt,q). SinceL(pθt,q)can be written as −KL[q(y|x)||q∗(y|x)],qt+1(y|
x)∝q∗(y|x) :=p(O= 1|x,y)pθt(y|x). Thus, this step is equivalent to weighting the output
samples from conditional language model distribution based on their likelihood of obtaining high
rewards.
•M-step: θt+1:= arg max θL(pθ,qt+1) = arg min θKL/bracketleftbig
qt+1(y|x)||pθ(y|x)/bracketrightbig
=
arg minθ/summationtext
y−qt+1(y|x) logpθ(y|x). As such, this step corresponds to maximizing a weighted
negative log-likelihood loss.
Alternating between above steps ensures a monotonic improvement in the ELBO: L(pθt+1,qt+1)≥
L(pθt,qt+1)≥L(pθt,qt).
EM with non-negative rewards . If the rewards are non-negative and fis set to the identity function,
thenp(O= 1|x,y)∝r(x,y)which implies qt+1(y|x)∝r(x,y)pθt(y|x). In this scenario, the updated
policy parameters θt+1resulting from the M-step at iteration tare given by:
θt+1:= arg max
θEx∼D/bracketleftig
Ey∼pt
θ(y|x)[r(x,y) logpθ(y|x)]/bracketrightig
. (3)
Comparing the above equation with the typical RL objective ( LRL) reveals the key distinction between
standard RL and EM-based RL: how output data is sampled. Standard RL continuously updates the policy
and uses this latest policy to collect data. In contrast, EM-based RL employs a fixed sampling policy from
the previous iteration, decoupling data collection from policy optimization. This decoupling in EM-based
approaches enables easier scaling to large policy networks, such as LLMs.
ReSTEMMotivated by the EM framework, we now discuss a simplified version of Reinforced Self-
Training (ReST) approach by Gulcehre et al. (2023). This approach, which we call ReSTEM, decouples
data collection (E-step) and policy optimization (M-step) in a typical RL pipeline. Algorithm 1 outlines the
ReSTEMalgorithm with multiple iterations, where each iteration corresponds to one Generate andImprove
step. We describe these steps in detail below.
•Generate (E-step): In this step, we generate a dataset Diby sampling many output sequences from
the current policy pθ:Di={(xj,yj)|N
j=1s.t.xj∼D,yj∼pθ(y|xj)}. Here, the inputs are
resampled from the original dataset xj∼D. The output sequences in Diare then scored with
a binary reward function r(x,y). In our experiments, we condition the language model using a
few-shot prompt with programs for code generation and step-by-step solutions for math problems.
4Published in Transactions on Machine Learning Research (04/2024)
•Improve (M-step): In the ithiteration, we use the new dataset Difrom Generate step to fine-tune
the policypθ. To mitigate task-specific over-fitting, we minimize drift from the base model by always
fine tuning the base pretrained language model. For fine-tuning, we minimize the reward-weighted
negative log-likelihood loss J(θ) =E(x,y)∼Di[r(x,y) logpθ(y|x)]. Once the policy is improved, a
new dataset of better quality samples can be created once again.
Differences with ReST (Gulcehre et al., 2023). Unlike ReST, we refrain from augmenting DiinGenerate
step with human-generated outputs as such data may not always be optimal for learning or it might not be
easily available. Furthermore, each Improve step fine-tunes the base model instead of the model obtained
from the previous ReST iteration. This results in comparable task-specific performance but much better
transfer performance on held-out tasks (see Figure 7).
Remark. Our experiments focus on problem-solving settings with binary rewards (either 0 or 1), unlike
the bounded real-valued rewards assumed by Gulcehre et al. (2023). Specifically, for each Generate step,
Gulcehre et al. (2023) perform multiple Improve steps, where each Improve step can be viewed as an M-step
with the function f(r(x,y)) =r(x,y)> τ, whereτ∈R+increases in successive M-steps. However, with
binary rewards, any value of τ∈(0,1)corresponds to the identical Improve steps.
4 Related work
Several prior methods can be instantiated using the expectation-maximization framework presented in Sec-
tion 3. We discuss methods and their relation to ReSTEMin this section.
•Expert Iteration (ExiT) (Anthony et al., 2017) alternates between two steps: expert improvement
and policy distillation. During the expert improvement step (E-step), we combine a base policy with
a search procedure to generate samples from a better policy, called the expert policy. Then, in the
policy distillation step (M-step), we use these expert samples to train the base policy in a supervised
way, effectivelyimprovingittomatchtheexpertpolicy. WhileExiTusedmonte-carlotree-search, we
simply use temperature sampling for collecting samples from the expert policy in ReST. That said,
improving the E-step in ReST using the ExIT framework via search and planning procedures with
language models would be interesting for future work. For example, Huang et al. (2022) implement a
single iteration of ReSTEMon simple math reasoning problems. However, unlike our setup, they do
not assume access to a correctness reward and instead employ majority-voting (Wang et al., 2023)
as a search procedure within the E-step.
•Self-Taught Reasoner (STaR) (Zelikman et al., 2022) employed greedy decoding instead of tem-
perature sampling for the E-step in ReSTEM, which is restricted to one model-generated solution
per problem during data collection. Additionally, STaR proposed rationalization as an alternative to
temperature sampling, where the language model is provided with the correct answer as part of the
input to generate correct solutions for difficult problems. However, in our preliminary experiments,
rationalization leads to substantial increase in false positive solutions that result in correct answer
but with incorrect reasoning.
•Rejection Sampling Fine-tuning (RFT) (Yuan et al., 2023) improves reasoning performance on
GSM8K and corresponds to running a single generate (E-step) and improve (M-step) of ReSTEM.
While RFT demonstrated limited performance improvements on GSM8K with increasing language
model capacity, ReSTEMachieves larger gains on more challenging APPS and MATH benchmarks
when scaling PaLM 2 model capacity. Moreover, we observe that using multiple iterations of
ReSTEMresult in larger performance gains.
•Iterative Maximum Likelihood (IML) optimizes a policy using a reward-weighted log-likelihood
objective on self-collected data. IML has been shown to perform well with relatively small-scale
language models for semantic parsing (Liang et al., 2016; Agarwal et al., 2019), machine transla-
tion (Wu et al., 2016) and simple math reasoning (Ni et al., 2022). Each E-step and M-step in IML
5Published in Transactions on Machine Learning Research (04/2024)
ReSTEMReST STaR RFT
Starts from fine-tuned model ✗ ✓ ✗ ✗
Finetunes from base model in each iteration ✓ ✗ ✓ N/A
Uses rationalizations for unsolved questions ✗ ✗ ✓ ✗
Temperature sampling for exploration ✓ ✓ ✗ ✓
Experiments with Large LMs ✓ ✗ ✗ ✓
Multiple iterations ✓ ✓ ✓ ✗
Larger gains on bigger models ✓ N/A N/A ✗
Evaluation on held out tasks ✓ ✗ ✗ ✗
Table 1: Differences between ReSTEMand other closely related approaches utilizing synthetic data for
advancing language model capabilities.
is performed over a mini-batch of training examples instead of the entire training dataset, as done
in ReSTEM. In IML, the learned policy can significantly diverge from the initial pretrained model,
which can manifest as task-specific overfitting, where the model performs well on the target task
but loses its ability to generalize to other tasks or domains. Additionally, the tightly coupled nature
of data collection and policy optimization in IML leads to high computational cost with large LMs,
making it significantly more expensive than ReSTEM.
•Reward weighted regression (RWR) (Peters & Schaal, 2007) corresponds to EM where we set
p(O= 1|x,y)∝exp (r(x,y))in Section 3. RWR has been previously applied to robotic control,
as it can be easily applied to non-binary reward functions. Norouzi et al. (2016) build on RWR to
propose a general variant of IML for machine translation.
•Rewardrankedfine-tuning (RAFT)(Dongetal.,2023)canbeinterpretedasalternatingbetween
E-stepandM-stepovermini-batches,whereE-stepusesthetheoutputsamplewithmaximumreward
for each input context. For binary reward functions, RAFT is analogous to IML and as such, can
be viewed as an instantiation of ReSTEM.
Other related works : TRICE (Phan et al., 2023) proposes an EM-based approach to maximize the
marginal log-likelihood (MML) of generating a correct answer for a reasoning problem, where the chain-of-
thought rationale is treated as a latent variable. While E-step in ReSTEMsimply corresponds to sampling
from the model and filtering with a binary reward, TRICE uses Markov-chain Monte Carlo with a control
variate to approximate the MML gradient. Sordoni et al. (2023) propose a gradient-free EM-based approach,
similar to RAFT, for prompt-optimization for frozen LLMs.
5 Experiments and analysis
The goal of our experiments is to answer the following questions:
1. How effective is ReSTEMcompared to fine-tuning on human-generated data?
2. How many iterations are needed for optimal performance? How quickly does ReSTEMleads to
overfitting on training set?
3. How does ReSTEMaffect pass@k and majority voting performance?
4. If we fine-tune using model-generated data on a specific task, do we see positive transfer to related
tasks? Is there any performance degradation compared to the base model when evaluating our
fine-tuned models on a broad suite of tasks?
5. How much input data do we need to get most of the performance gains from ReSTEM? Is one
iteration of ReSTEMsufficient?
6Published in Transactions on Machine Learning Research (04/2024)
0 1 2 3
Num iterations2025303540Pass@1 T est Accuracy (%)Hendrycks MATH
Palm-2-S Palm-2-L Palm-2-L-SFT Palm-2-S-SFT
0 1 2 3
Num iterations 607080Pass@1 T est Accuracy (%)Transfer to GSM8K
Figure 2: ReSTEMfor math problem-solving . Test performance on MATH and GSM8K (transfer)
for PaLM 2-S* and PaLM 2-L as a function of ReSTEMiterations. We also report performance of models
fine-tuned via SFT on human-generated data as a baseline. Iteration 0 corresponds to pre-trained model
performance. Following Google et al. (2023), we use greedy decoding for evaluation.
Training Datasets . We evaluate ReSTEMprimarily on mathematical problem solving using the
Hendrycks’ MATH dataset (Hendrycks et al., 2021b) and code generation using the APPS (Introductory)
dataset(Hendrycksetal.,2021a). MATHandAPPS(Introductory)contain7500and2342trainingproblems
respectively. Weselectthesetasksbecausethemodeloutputscanbeautomaticallyevaluatedascorrectorin-
correct, perfectly suited for ReSTEM. Both these datasets offer binary rewards: on MATH, model-generated
answers can be easily verified for correctness using the ground-truth answer, while on APPS, test cases
determine whether the generated code is correct.
Models. WeusethePaLM2models(Googleetal.,2023)withpublicAPIsonGoogleCloudforexperiments,
including PaLM 2-S (Bison), PaLM 2-S* (Codey), and PaLM 2-L (Unicorn).
Evaluation . We report generalization performance using the test splits of the MATH and APPS (Introduc-
tory) datasets. For measuring transfer performance, we look at GSM8K (Cobbe et al., 2021), Hungarian HS
finals (Paster, 2023), and HumanEval (Chen et al., 2021) datasets. We also evaluate our models using the
Big-Bench Hard (Suzgun et al., 2022) benchmark to evaluate general capabilities. All evaluations follow the
settings from Google et al. (2023), unless specified otherwise.
Implementation Details . During each iteration of ReSTEM, we generated a fixed number of solutions per
problem for the E-step: 32 for the MATH dataset and 64 for the APPS dataset. For generating solutions,
we sample from the language model using top-K sampling with K=40 and temperature of 0.7. However,
directly using all these model-generated solutions can lead to an imbalanced dataset, as we will have a lot
more correct solutions for the easier problems. To mitigate this, we introduced a cut-off threshold for the
maximum number of solutions per problem, a design choice also used by Zelikman et al. (2022), included in
the fine-tuning dataset: 10 for both MATH and APPS. This approach ensures diversity in the training data
and safeguards against overfitting on easier problems. For fine-tuning, we use the few-shot prompt (and the
question) as input to the model, and use the model-generated solutions as targets. We only apply the next
token prediction loss (Equation 1) on the targets. Due to the cost of our experiments (thousands of TPU
hours for every fine-tuning run), each experiment is performed once.
5.1 ReSTEMon MATH and APPS
Figures 2 and 3 show the performance of ReSTEMwhen trained on the MATH and APPS datasets, re-
spectively. We see that MATH benefits from performing multiple iterations of ReSTEM, both in terms of
performance on the MATH test set, as well as transfer to GSM8K. On the other hand, we see that most of
7Published in Transactions on Machine Learning Research (04/2024)
0 1 2
Num iterations1820222426Pass@1 T est Accuracy (%)APPS (Introductory)
Palm-2-S* Palm-2-L Palm-2-L-SFT Palm-2-S*-SFT
0 1 2
Num iterations 40455055Pass@1 T est Accuracy (%)Transfer to HumanEval
Figure 3: ReSTEMfor code-generation . Test performance on APPS (introductory) and Hu-
manEval (transfer) for PaLM 2-S* and PaLM 2-L as a function of ReSTEMiterations.
0 1 2 3
Num iterations 354045505560Pass@1 Performance (%)Hendrycks MATH
Palm-2-L (Train) Palm-2-L (T est)
0 1 2
Num iterations 2030405060Pass@1 Performance (%)APPS (Introductory)
Palm-2-L (Train) Palm-2-L (T est)
Figure 4: Train-test performance gap on (left) MATH with PaLM-2-L, and (right) APPS with PaLM-
2-S*, as a function of ReSTEMiterations.
the gains for APPS come from the first iteration, and the performing more iterations leads to a regression
in performance on both APPS and HumanEval.
Interestingly, Figures 2 and 3 demonstrate that fine-tuning on model-generated solutions substantially out-
performs using human-written solutions, especially for the PaLM 2-L model. This aligns with findings of
Yuan et al. (2023) and recent work on distilling LLMs using model-generated data (Agarwal et al., 2023; Gu
et al., 2023). However, unlike Yuan et al. (2023), who observed diminishing returns from model-generated
data on GSM8K when scaling model capacity, our results suggest an opposite trend: ReSTEMleads to larger
performance gains as model capacity increases. On the MATH dataset, the test accuracy improvement with
ReSTEMis5.94%for PaLM 2-S compared to 6.34%for the larger PaLM 2-L model. Similarly, on the APPS
dataset, improvements are 5.6%for PaLM 2-S* compared to 6.4% for PaLM 2-L. This is in addition to
the fact that the larger models start with a much stronger initial performance, and improvements on these
benchmarks generally get harder as the baseline performance goes up.
Train-test performance gap . Figure 4 shows that while training performance increases linearly with the
number of ReSTEMiterations, test set performance does not. For MATH, test performance improvements
are small after the first iteration, and for APPS, we observe a regression in performance in the 2nditeration.
Wesuspectthattheregressioninperformanceislikelyduetooverfittingonthesmallsetoftrainingproblems.
Since the APPS dataset is about a third of the size of the MATH dataset, it suffers more from this problem.
8Published in Transactions on Machine Learning Research (04/2024)
0 20 40 60
Num samples (K)40%60%80%Pass @ K T est Accuracy (%)HumanEval
PaLM-2-L
PaLM-2-L (ReST)
2 4 6 8 10
Num samples (K)10%20%30%40%Pass @ K T est Accuracy (%)APPS (Introductory)
PaLM-2-L
PaLM-2-L (ReST)
0 20 40 60
Num samples (K)20%30%40%50%60%70%80%Pass @ K T est Accuracy (%)Hendrycks MATH
Palm-2-L
Palm-2-L (ReST)
Figure 5: Pass@K results for PaLM-2-L pretrained model as well as model fine-tuned with ReSTEM. For
a fixed number of samples K, fine-tuning with ReSTEMsubstantially improves Pass@K performance. We
set temperature to 1.0 and use nucleus sampling with p= 0.95.
5.2 Impact on Pass@K and Majority-Voting Performance
Toinvestigatetheimpactoffine-tuningwithReSTEMonthediversityofthefinalmodel’sgeneratedoutputs,
we evaluate pass@k (Chen et al., 2021) and majority voting (Wang et al., 2023) performance of the fine-tuned
PaLM 2-L model relative to the base model.
Pass@K measures the probability that at least one of the K generated solutions for a problem is correct, that
is, outputs the correct answer for math problems or passes all the unit tests for code generation. Figure 5
shows the performance of Palm-2-L on the pass@K metric. We see that model obtained after ReSTEM
fine-tuning is stronger for all values of K, with the performance gap typically being the highest for K=1.
Majority voting first samples a diverse set of reasoning paths instead of only taking the greedy one, and
then selects the most consistent answer by marginalizing out the sampled reasoning paths. For Hendrycks
MATH, it is possible to use majority voting to maximize Pass@1 performance, and we find that when using
64 samples per question, the PaLM 2-L fine-tuned with ReSTEMobtains a test accuracy of 48.82, while
the base model gets 44.02.
5.3 Ablation Studies
Impact of multiple iterations Our results show that multiple iterations can sometimes lead to over-
fitting on the train set (Figure 4). This raises the question of whether multiple iterations are really necessary.
Is it better to collect a larger dataset and perform just a single iteration of ReSTEM? To investigate this,
we collect a dataset with the base PaLM-2-L model on Hendrycks MATH that is 3×as many solutions
per problem as used in a single iteration of ReSTEMfor the E-step. Fine-tuning with this dataset results
in pass@1 performance of 40.3%, which is lower than the 41%in second and 41.9%in third iteration, as
shown in Figure 2. These results indicate that performing multiple iterations of ReSTEMleads to higher
performance compared a single iteration with 3x the data.
Comparing model-generated data with human data A key strength of ReSTEMis its ability to gen-
erate multiple correct solutions for each problem. This provides valuable additional training data compared
to human-generated data, which typically offers only a single solution per problem. While this makes a com-
parison in Figures 2 and 3 not entirely fair, it also highlights the potential of ReSTEMto boost performance
with diverse and correct solutions.
In order to enable an apples-to-apples comparison, we conduct the following study: we select all Hendrycks
MATH questions for which we have at least one correct model-generated solution, resulting in about 5K
questions. For these 5K questions, we run two fine-tuning experiments: SFT(5K) where we fine-tune on
human-writtensolutions(oneperquestion), andReST∗(5K)wherewefine-tuneonmodel-generatedsolutions
(also one per question, selected at random).
9Published in Transactions on Machine Learning Research (04/2024)
SFT (7K) SFT (5K)ReST* (5K)ReSTEM (5K)
Method (Num questions)3436384042Pass@1 Performance (%)Hendrycks MATH (T est)
SFT (Human) Distill* (2-L) ReSTEM (2-S)Distill (2-L)
Method (Data Source)15.017.520.022.525.027.530.0Pass@1 Performance (%)PaLM 2-S on Hendrycks MATH (T est)
Figure 6: Left. Comparing ReSTEMwith SFT on MATH. SFT refers to fine-tuning on human data, while
ReST* refers to a version of ReSTEMwith one iteration that uses only one correct sample per problem.
Here, ReST denotes ReSTEMwith 3 iterations. For each method, we denote the number of questions in
parenthesis. Right. Impact of Model-Generated Data for Distillation.
The results in Figure 6 (right), show that ReSTEMoutperforms fine-tuning on human data even in this much
more restricted setting. Furthermore, the efficacy of ReST(5K) over ReST∗(5K) highlights the additional
gain in performance that we can obtain by spending more compute on sampling a large number of solutions
and performing multiple iterations of ReSTEM.
Distillation with ReSTEM-generated data The above results indicate that self-generated data can
be better than human data for fine-tuning language models. We hypothesize this may be because model-
generated solutions are more in-distribution compared to human-written solutions. This raises the question
of whether ReSTEM-generated data can benefit different models than the one generating the data.
To answer this question, we consider a distillation setup on MATH where we fine-tune PaLM 2-S using data
generated by PaLM 2-L, resulting in solutions for about 5K questions. Specifically, we ran two distillation
experiments: Distill∗(2-L) where we fine-tune on teacher-generated solutions (one per question), similar
to ReST (5K), and Distill (2-L), which includes multiple solutions per problem, generated during the final
iteration of ReSTEMwith PaLM 2-L.
Our results, shown in Figure 6 (right), reveal that Distill∗surpasses the performance achieved by fine-tuning
on human-written solutions, despite having smaller number of training questions. Additionally, fine-tuning
PaLM 2-S with multiple solutions from PaLM 2-L, namely Distill (2-L), is superior than using self-generated
solutions via ReSTEM. This improvement is likely due to the larger number of training questions with
solutions in PaLM 2-L generated data compared to 2-S. Overall, these results indicate that model-generated
data can be more effective for fine-tuning smaller models than relying on human-generated data.
ReSTEMReST
Method21.021.221.421.621.822.0Pass@1 T est Accuracy (%)APPS (Introductory)
Palm-2-S*-SFTReSTEMReST
Method384042444648Pass@1 T est Accuracy (%)Transfer to HumanEval
Figure 7: ReSTEMvsReST using PaLM 2-S*.ReST vsReSTEMOne of the main differences
between ReST and ReSTEMis that ReSTEMalways
fine-tunes the base model for each iteration while
ReST continues to finetune the the model from the
last iteration. We run an ablation comparing these
options using PaLM 2-S* in Figure 7 and observe
that while ReST and ReSTEMhave similar perfor-
mance on APPS, the transfer performance to Hu-
manEval is substantially better with ReSTEM.
Impact of dataset size Since one of the main
ingredients needed for ReSTEMis a dataset of input
contexts (e.g., questions for MATH), we are interested in evaluating the effect of number of input problems.
10Published in Transactions on Machine Learning Research (04/2024)
0 1000 2000 4000 7000
Number of Questions3436384042Pass@1 Performance (%)Hendrycks MATH (T est)
Very Hard Hard Medium Easy
Problem Difficulty Level020406080100Average Success Rate (%)Average Success Rate by Difficulty Level
Base Model
ReSTEM
Figure 8: Left. Performance for a single iteration of ReSTEMas a function of dataset size (number of
questions) on MATH. Right. Improvement from ReSTEMbased on the difficulty level of the question.
The results from our dataset ablations using the PaLM-2-L model on Hendrycks MATH, Figure 8 (left), show
thatutilizing just1000 MATH questions resultsin significantgains, implying thatthemethod isvery efficient
in the number of prompts needed. However, we noted a slight decrease in performance when using 4,000
questions compared to 2,000, indicating potential variance in the fine-tuning process. Ideally, conducting
this experiment multiple times would help quantify this variance, but this is prohibitively resource-intensive.
Overall, we find that ReSTEMis quite sample efficient and performance gains from ReSTEMimprove as we
increase the dataset size.
WhichQuestionsBenefitMostfromReSTEMWeevaluatetheperformanceenhancementofReSTEM
across different question difficulties in the Hendrycks MATH dataset. Questions are classified based on
success rates from the base model at a temperature setting of T=1.0 into four categories: “easy” (answered
correctly 75%-100% of the time), “medium” (50%-75%), “hard” (25%-50%), and “very hard” (below 25%).
Figure 8 (right) presents the average success rates for these categories, comparing the base model to the
ReSTEM-finetuned model. The results demonstrate that ReSTEMconsistently improves performance across
all difficulties, with the highest gains coming for questions categorized as medium and hard.
5.4 Impact on Reasoning capabilities
General capabilities . BIG-Bench provides a suite of over 200 tasks that can be used to probe LLMs’
performance across a range of fields and capabilities. BIG-Bench Hard (BBH) (Suzgun et al., 2022) is a
subset of 23 BIG-Bench tasks where the previous generation of LLMs, such as Codex and PaLM 540B,
performed below the average human rater. We follow the protocol of Google et al. (2023) and evaluate on
BBH using both few-shot and chain-of-thought prompting. Figure 9 shows the performance of ReSTEM-
finetuned models, and compares them against the base PaLM-2 model. We see no major degradation on
any of the BBH tasks. Furthermore, the model fine-tuned on Hendrycks MATH outperforms the base model
on this suite when using chain-of-thought prompting, and the model fine-tuned on APPS also shows slight
performance gains. When using direct prompting, all three models perform similarly.
Problem-solving . Tostresstestthemathproblem-solvingcapabilitiesonaheld-out“real-world"evaluation
set, we evaluate our model on the 2023 Hungarian high school finals exam in mathematics, akin to Grok.
We follow the evaluation protocol from Paster (2023). Specifically, we evaluate the PaLM 2-L model, fine-
tuned with ReSTEMon Hendrycks MATH, using the 1-shot prompt from Grok, sample solutions using
temperature 0.1, and manually grade the outputs using the rubric provided by the examiners. The results
from evaluation are shown in Figure 10. We find that PaLM-2-L fine-tuned with ReSTEMperforms well on
this exam, surpassing the performance of all existing models except GPT-4.
11Published in Transactions on Machine Learning Research (04/2024)
Boolean ExpressionsCausal JudgementDate UnderstandingDisambiguation QADyck LanguagesFormal FallaciesGeometric ShapesHyperbaton
Movie RecommendationMulti-step Arithmetic [T wo]Navigate
Object CountingPenguins in a T able
Reasoning about Colored ObjectsRuin Names
Salient Translation Error DetectionSnarks
Sports UnderstandingT emporal SequencesWeb of LiesWord Sorting
Logical Deduction (avg)
Tracking Shuffled Objects (avg)
Big-Bench Hard (BBH) T ask30405060708090100Few-shot Performance with CoTPaLM 2-L PaLM 2-L (MATH) PaLM 2-L (APPS)
CoT Direct
Prompt Type6065707580Average BBH PerformancePaLM 2-L
PaLM 2-L (APPS)
PaLM 2-L (MATH)
Figure 9: Comparing the ReSTEMmodels to the base model on the Big-Bench Hard suite of tasks. Evalu-
ations were conducted across multiple checkpoints, and the vertical black lines denote standard deviation.
20 30 40 50 60 70
Hungarian HS Finals Exam Score (%)30405060708090GSM8K Score (%)MetaMath 7BMetaMath Mistral 7B OpenChat 3.5
Code Llama 34BLlemma 34BGPT-3.5 TurboGPT-4
Grok-0 (33B)Grok-1
Qwen 7BClaude 2
Mistral 7BMAmmoTH 7BPaLM 2-L (ReSTEM)Exam Score vs GSM8K Performance of Various Models
Figure 10: Transfer results on Hungarian HS Finals Exam. Results for models other than PaLM-2-L
finetuned with ReSTEMare taken from Paster (2023). Several models specialized for mathematics perform
well on the widely-used GSM8K benchmark but perform poorly on the Hungarian exam. In contrast, PaLM
2-L model fine-tuned with ReSTEMperforms well on both these benchmarks.
6 Discussion
In this paper, we propose training on model-generated data combined with a reward function, via ReSTEM,
for improving the performance of LLMs on problem-solving tasks. Furthermore, we demonstrate that
ReSTEMis theoretically grounded in the application of expectation-maximization to RL. We evaluate
ReSTEMon mathematical problem solving and code generation, and show that ReSTEMoffers signifi-
cant performance gains at a relatively low computational cost, especially when compared to the cost of
pre-training. Our experiments also show that ReSTEMdoes not lead to regression on other tasks. We
conduct a number of ablations to better understand the strengths and weaknesses of this method, and find
that it is data-efficient, but also requires some vigilance to avoid over-fitting.
12Published in Transactions on Machine Learning Research (04/2024)
There are a number of limitations associated with ReSTEM. First, this method requires a moderately-sized
training set of problems or prompts, which would need to be collected (from humans) for any new task of
interest. Second, ReSTEMalso requires access to a manually-designed or learned reward function, ideally one
that can be computed automatically. Finally, while ReSTEMallows significant performance improvements
in pass@1 performance, it may not quite close the gap to pass@K performance for the same task (with a
sufficiently large K). Future research in self-improvement in language models should focus on automating
manual parts of the pipeline (likely through language models as well), and explore algorithmic improvements
that reduce the gap to pass@K performance.
Acknowledgements
WewouldliketothankTomLePaineforprovidingfeedbacktoanearlydraft. WealsoacknowledgeBenjamin
Anderson, Sridhar Thiagarajan, Feryal Behbahani, Aleksandra Faust, Doina Precup, Olivier Bachem, and
Slav Petrov for helpful discussions.
Author Contributions
Avi, Rishabh, and JD jointly led the project. Avi was responsible for training and evaluation infrastructure,
ablations and experiments on MATH, JD led the experiments on APPS, Rishabh was responsible for the
paper writing, evaluations, and distillation ablations.
Ankesh, Piyush, Ethan, andBehnamobservedpreliminaryfindingsaboutefficacyofmodel-generateddataon
MATH for Minerva models and motivated this research. Piyush also helped Avi in setting up infrastructure.
Xavier, Peter, James, Jaeheoon, Kelvin and Yamini took part in project discussions. Jascha and Noah
sponsored and advised the project. All other authors provided feedback on this work.
References
Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. Learning to generalize from
sparse and underspecified rewards. In International conference on machine learning , pp. 130–140. PMLR,
2019.
Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. Gkd:
Generalized knowledge distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649 ,
2023.
Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search.
Advances in neural information processing systems , 30, 2017.
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter
Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and
Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712,
2023. doi: 10.48550/ARXIV.2303.12712. URL https://doi.org/10.48550/arXiv.2303.12712 .
MarkChen, JerryTworek, HeewooJun, QimingYuan, HenriquePondédeOliveiraPinto, JaredKaplan, Har-
rison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
MikhailPavlov, AletheaPower, LukaszKaiser, MohammadBavarian, ClemensWinter, PhilippeTillet, Fe-
lipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-
Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir
Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam,
Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie
Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech
Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.
13Published in Transactions on Machine Learning Research (04/2024)
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training
verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
Peter Dayan and Geoffrey E Hinton. Using expectation-maximization for reinforcement learning. Neural
Computation , 9(2):271–278, 1997.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the
em algorithm. Journal of the royal statistical society: series B (methodological) , 39(1):1–22, 1977.
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong
Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint
arXiv:2304.06767 , 2023.
Google, Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 , 2023.
Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation of large language models. arXiv
preprint arXiv:2306.08543 , 2023.
CaglarGulcehre, TomLePaine, SrivatsanSrinivasan, KseniaKonyushkova, LotteWeerts, AbhishekSharma,
AdityaSiddhant, AlexAhern, MiaosenWang, ChenjieGu, etal. Reinforcedself-training(rest)forlanguage
modeling. arXiv preprint arXiv:2308.08998 , 2023.
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns,
Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv
preprint arXiv:2105.09938 , 2021a.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint
arXiv:2103.03874 , 2021b.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large
language models can self-improve. CoRR, abs/2210.11610, 2022. doi: 10.48550/ARXIV.2210.11610. URL
https://doi.org/10.48550/arXiv.2210.11610 .
ChenLiang, JonathanBerant, QuocLe, KennethDForbus, andNiLao. Neuralsymbolicmachines: Learning
semantic parsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020 , 2016.
Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and
Jianfeng Gao. Learning math reasoning from self-sampled correct and partially-correct solutions. In The
Eleventh International Conference on Learning Representations , 2022.
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, et al.
Reward augmented maximum likelihood for neural structured prediction. Advances In Neural Information
Processing Systems , 29, 2016.
OpenAI. Gpt-4 technical report, 2023.
Keiran Paster. Testing language models on a held-out high school national finals exam.
https://huggingface.co/datasets/keirp/hungarian nationalhsfinalsexam, 2023.
Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space
control. In Proceedings of the 24th international conference on Machine learning , pp. 745–750, 2007.
Du Phan, Matthew D Hoffman, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov,
CharlesSutton,SharadVikram,andRifASaurous. Trainingchain-of-thoughtvialatent-variableinference.
arXiv preprint arXiv:2312.02179 , 2023.
14Published in Transactions on Machine Learning Research (04/2024)
Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Côté, Matheus Pereira, Adam Trischler, Ziang Xiao,
Arian Hosseini, Friederike Niedtner, and Nicolas Le Roux. Joint prompt optimization of stacked llms
using variational inference. In Thirty-seventh Conference on Neural Information Processing Systems ,
2023.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and
whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 , 2022.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The
Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,
2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=1PL1NIMMrw .
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging
the gap between human and machine translation. arXiv preprint arXiv:1609.08144 , 2016.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relation-
ship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825 ,
2023.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning.
Advances in Neural Information Processing Systems , 35:15476–15488, 2022.
15