Published in Transactions on Machine Learning Research (09/2022)
HEAT: Hyperedge Attention Networks
Dobrik Georgiev∗dgg30@cam.ac.uk
Department of Computer Science and Technology, University of Cambridge, UK
Marc Brockschmidt mmjb@google.com
Microsoft Research, Cambridge, UK†
Miltiadis Allamanis mallamanis@google.com
Microsoft Research, Cambridge, UK†
Reviewed on OpenReview: https: // openreview. net/ forum? id= gCmQK6McbR
Abstract
Learning from structured data is a core machine learning task. Commonly, such data is
represented as graphs, which normally only consider (typed) binary relationships between
pairs of nodes. This is a substantial limitation for many domains with highly-structured
data. One important such domain is source code, where hypergraph-based representations
can better capture the semantically rich and structured nature of code.
In this work, we present HEAT, a neural model capable of representing typed and qualiﬁed
hypergraphs, where each hyperedge explicitly qualiﬁes how participating nodes contribute. It
can be viewed as a generalization of both message passing neural networks and Transformers.
We evaluate HEATon knowledge base completion and on bug detection and repair using
a novel hypergraph representation of programs. In both settings, it outperforms strong
baselines, indicating its power and generality.
1 Introduction
Large parts of human knowledge can be formally represented as sets of relations between entities, allowing
for mechanical reasoning over it. Common examples of this view are knowledge graphs representing our
environment,databasesrepresentingbusinessdetails,andﬁrst-orderformulasdescribingmathematicalinsights.
Such structured data hence regularly appears as input to machine learning (ML) systems.
In practice, this very generic framework is not easy to handle in ML models. One issue is that the set of
relations is not necessarily known beforehand, and that the precise structure of a relation is not easily ﬁxed.
As an example, consider studied(person: P, institution: I, major:M), encoding the fact that a person
Pstudied at institution I, majoring in M. However, if the institution is unknown, we may want to just
consider studied(person: P, major:M), or we may need to handle case of people double-majoring, using
studied(person: P, major:M2, major:M1).
Existing ML approaches usually cast such data as hypergraphs, and extend the ﬁeld of graph learning
approaches to this setting, but struggle with its generality. Some solutions require to know the set of relations
beforehand (ﬁxing their arity, and assigning ﬁxed meanings to each parameter), while others abstract the set
of entities to a simple set and forego the use of the name of the relation and its parameters.
∗Work mainly performed while interning at Microsoft Research, Cambridge, UK.
†Now at Google Research.
1Published in Transactions on Machine Learning Research (09/2022)
n1
n2
n3ρieτ1
τ2
τ3
Figure 1: Representation of relation ρ(τ1:n1,τ2:n2,τ3:n3)as typed and qualiﬁed hyperedge e,i.e.a
relation of type ρ, and the qualiﬁed participation of the nodes n1asτ1,n2asτ2, andn3asτ3ine.HEAT
operates on hypergraphs with such hyperedges.
One form of data that can proﬁt from being modeled as a hypergraph is program source code. Existing
approaches model code either as simple sequence of tokens (Hindle et al., 2012) or as a graph (Allamanis
et al., 2018b; Hellendoorn et al., 2020). While the former can easily leverage successful techniques from the
NLP domain, it is not possible to include additional domain-speciﬁc knowledge (such as the ﬂow of data) in
the input. On the other hand, graph models are able to take some of this additional information, but struggle
to represent more complex relationships that require hyperedges.
In this work, we propose a new architecture, HEAT(HyperEdgeATtention), that is able to handle an open
set of relations that may appear in several variants. To this end, we combine the idea of message passing
schemes that follow the graph structure with the ﬂexibility and representational power of Transformer
architectures. Concretely, our model handles arbitrary relations by presenting each one as a separate sequence
in a Transformer, where an idea akin to standard positional encodings is used to represent how each entity
participatesintherelation. TheTransformeroutputisthenusedtoupdatetherepresentationsofparticipating
entities.
We illustrate the success of this technique in two very diﬀerent settings: learning to infer additional relations
in knowledge graphs, and learning to ﬁnd and repair bugs in programs. In both cases, our model shows
improvements over strong state-of-the-art methods. Concretely, we (a) deﬁne HEATas a novel hypergraph
neural network architecture (Sec. 2), (b) deﬁne a novel representation of programs as hypergraphs (Sec. 3),
(c) evaluate HEATon the tasks of detecting and repairing bugs (Sec. 4.1) and link prediction in knowledge
graphs (Sec. 4.2).
Ourimplementation ofthe HEAT modelis available on the heatbranchof https://github.com/microsoft/
neurips21-self-supervised-bug-detection-and-repair/tree/heat . This includes code for the extrac-
tion of hypergraph representations of Python code as discussed in Sec. 3.
2 The HEAT Model
In thiswork,weareinterestedin representingtypedandqualiﬁedrelationsoftheform relName(parName1: n1,
parName2:n2, ...)where relName represents the name of a relation, n1,n2,...are entities participating
in the relation, with parName1 , ...describes (qualiﬁes) their role in the relation.
Formally, such relations can be represented as typedandqualiﬁed hypergraphs: we consider a set of nodes
(entities)N={n1,...}and a set of hyperedges H={e1,...}. Each hyperedge e=(ρ,{(τ1,n1),..., (τk,nk)})
describes a named relationship of type ρamong nodes n1...nkwhere the role of node nkwithineis
qualiﬁed using τk. Fig. 1 illustrates the form of such a hyperedge. Note that this is in contrast to traditional
hypergraphs where nodes participate in a hyperedge without any qualiﬁers. Instead, typed and qualiﬁed
graphs can accurately represent a large range of domains maintaining valuable information.
2Published in Transactions on Machine Learning Research (09/2022)
2.1 Background
Message passing neural networks (MPNN) (Gilmer et al., 2017) operate on sets of nodes Nand sets of
(typed) edgesE, where each edge (ni,τi,nj)of typeτconnects a pair of nodes. In MPNNs, each node n∈N
is associated with representations h(t)
nthat are computed incrementally. The initial h(0)
nstems from input
features, but subsequent representations are computed by exchanging information between nodes. Optionally,
each edgeemay also be associated with a state/representation h(t)
ethat is also computed incrementally.
Concretely, each edge gives rise to a “message” using a learnable function fm:
m(t)
(ni,τ,n j)=fm/parenleftBig
h(t)
ni,τ,h(t)
nj,h(t)
e/parenrightBig
.
To update the representation of a node n, all incoming messages are aggregated with a permutation-invariant
function Agg(·)into a single update, i.e.
u(t)
nj=Agg/parenleftBig/braceleftBig
m(t)
(ni,τ,n j)|(ni,τ,nj)∈E/bracerightBig/parenrightBig
. (1)
Aggis commonly implemented as summation or max-pooling, though attention-based variants ex-
ist (Veličković et al., 2018). Finally, each node’s representation is updated using its original representation
and the aggregated messages. Many diﬀerent update mechanisms have been considered, ranging from just
using the aggregated messages (Kipf & Welling, 2017) to gated update functions (Li et al., 2016).
Transformers (Vaswani et al., 2017) learn representations of sets of elements Nwithout explicit edges.
Instead, they consider all pairwise interactions and use a learned attention mechanism to identify particularly
important pairs. Transformer layers are split into two sublayers. First, multihead attention ( MHA) uses the
representations of all Nto compute an “update” unfor eachn∈N,i.e.
/bracketleftBig
u(t)
n1,..., u(t)
nk/bracketrightBig
=MHA/parenleftBig/bracketleftBig
h(t)
n1,..., h(t)
nk/bracketrightBig/parenrightBig
. (2)
Internally, MHAuses an attention mechanism to determine the relative importance of pairs of entities and
to compute the updated representations. Note that MHAtreats its input as a (multi-)set, and hence is
permutation-invariant. To provide ordering information, a common approach is to use positional encodings,
i.e.to extend each representation with explicit information about the position. In practice, this means that
MHAoperates on/bracketleftBig
h(t)
n1⊕p1,..., h(t)
nk⊕pk/bracketrightBig
, where piprovides information about the position of ni, and⊕
is a combination operation (commonly element-wise addition).
The second Transformer sublayer is used to combine the updated representations with residual information
and add computational depth to the model. This is implemented as
q(t)
ni=LN/parenleftBig
h(t)
ni+u(t)
ni/parenrightBig
(3)
h(t+1)
ni=LN/parenleftBig
q(t)
ni+FFN/parenleftBig
q(t)
ni/parenrightBig/parenrightBig
, (4)
where LNis layer normalisation (Ba et al., 2016) and FFNis a feedforward neural network (commonly with
one large intermediate hidden layer).
2.2 HEAT: An Attention-Based Hypergraph Neural Network
We now present HEAT, a Transformer-based message passing hypergraph neural network that learns to
represent typed and qualiﬁed hypergraphs.
Overall,we follow the core idea ofthe standardmessage passing paradigm: ouraim is to update the representa-
tion of each entity using messages arising from its relations to other entities. Following Battaglia et al. (2018),
we also explicitly consider representations of each hyperedge, based on their type and the representation of
adjacent entities. Intuitively, we want each entity to “receive” one message per hyperedge it participates in,
reﬂecting both its qualiﬁer ( i.e.role) in that hyperedge, as well as the relation to other participating entities.
3Published in Transactions on Machine Learning Research (09/2022)
h(t)
e h(t)
n1r(t)
τ1 h(t)
n2r(t)
τ2 h(t)
n3r(t)
τ3
⊕⊕⊕
Multihead Attention
m(t)
e m(t)
e→n1 m(t)
e→n2 m(t)
e→n3
(a) Message computation for hyperedge efrom Fig. 1. The
previous node states h(t)
niare combined with their respec-
tive qualiﬁerembeddings r(t)
τi. Then,a multiheadattention
mechanism computes the messages m(t)
e→·andm(t)
e.m(t)
e1→n
...
m(t)
ek→nAgg h(t)
n
Add & Norm
Feed Forward
Add & Norm
h(t+1)
n
(b) The state update for each node nin the hypergraph.
Apart from Aggthis matches a standard Transformer
layer of Vaswani et al. (2017).
Figure 2: The HEATArchitecture.
To compute these messages, we borrow ideas from the Transformer architecture. We view each qualiﬁed
hyperedge as a set of entities, with each entity being associated with a position (its qualiﬁer in the hyperedge),
and then use multihead attention — as in Eq. 2 — to compute one message per involved entity:
/bracketleftBig
m(t)
e,m(t)
e→n1,..., m(t)
e→nk/bracketrightBig
=MHA/parenleftBig/bracketleftBig
h(t)
e,r(t)
τ1⊕h(t)
n1,..., r(t)
τk⊕h(t)
nk/bracketrightBig/parenrightBig
. (5)
Here, h(t)
eis the current representation of the hyperedge, h(t)
niis the current representation of node ni,r(t)
τi
is the representation (embedding) of the qualiﬁer τiwhich denotes the role of niine, and⊕combines two
vectors. Fig. 2a illustrates this. In this work, we consider element-wise addition for ⊕but other operators,
such as vector concatenation would be possible.
We can then compute a single update u(t)
nifor nodeniby aggregating the relevant messages computed for
the hyperedges it occurs in, i.e.
u(t)
ni=Agg/parenleftBig/braceleftBig
m(t)
e→ni|e= (ρ,{...,(τi,ni),...})∈H/bracerightBig/parenrightBig
.
This is equivalent to the aggregation of messages in MPNNs (cf. Eq. 1). We then compute an updated entity
representation h(t+1)
nifollowing the Transformer update as described in Eqs. 3, 4. This is illustrated in Fig. 2b.
The core diﬀerence to the Transformer case is the aggregation Agg(·)used to combine all messages (as in a
message passing network). Agg(·)can be any permutation invariant function, such as elementwise-sum and
max, or even an another transformer (see Sec. 4.1 for experimental evaluation of these variants).
Finally, the hyperedge states h(t)
eare also updated as in Eqs. 3, 4 using the messages m(t)
ecomputed as in
Eq. 5, and no aggregation is required because there is only a single message for each hyperedge e. Initial node
states h(0)
niare initialised with node-level information (as in GNNs). Qualiﬁer embeddings rτi(resp. initial
hyperedge states h(0)
e) are obtained by breaking the name of τi(resp.ρe) into subtokens ( e.g.,parName is
split into parandnameorfoo_bar2 intofoo,bar, and 2), embedding these through a (learnable) vocabulary
embedding matrix, and then sum-pooling. Then for each HEATlayer r(t)
τiis computed through a linear
learnable layer of the sum-pooled embedding, i.e.r(t)
τi=W(t)
τrτi+b(t)
τ.
Generalising Transformers Note that if the hypergraph is made up of a single hyperedge of the form
Seq(pos1:n1, pos2:n2, ...)thenHEATdegenerates to the standard transformer of Vaswani et al. (2017)
with positional encodings over the sequence n1,n2,...at each layer. In the case of such sequence positions,
we simply use the sinusoidal positional encodings of Vaswani et al. (2017) as position embedding, rather
than using a learnable embedding. Thus HEATcan be thought to generalise transformers from sets (and
sequences) to richer structures.
4Published in Transactions on Machine Learning Research (09/2022)
Computational Considerations A naïve implementation of a HEATmessage passing step requires one
sample per hyperedge to be fed into a multihead attention layer, as each hyperedge of knodes gives rise to
a sequence of length k+ 1, holding the edge and all node representations. To enable eﬃcient batching, this
would require to pad all such sequences to the longest sequence present in a batch. However, the length of
these sequences may vary wildly. On the other hand, processing each hyperedge separately would not make
use of parallel computation in modern GPUs.
To resolve this, we use two tricks. First, we consider “microbatches” with a pre-deﬁned set of sequence lengths
{16,64,256,768,1024}, minimising wastage from padding. Second, we “pack” several shorter sequences into
a single sequence, and then use appropriate attention masks in the MHAcomputation to avoid interactions.
For example, a hyperedge of size 9 and a hyperedge of size 7 can be joined together to yield a sequence
of length 16. Appx. A details the packing algorithm used. This process is performed in-CPU during the
minibatch preparation.
3 Representing Code as Hypergraphs
Program source code is a highly structured object that can be augmented with rich relations obtained from
program analyses. Traditionally, source code is represented in machine learning either as a sequence of tokens
(Hindle et al., 2012), a tree (Yin & Neubig, 2017), or as a graph (Allamanis et al., 2018b; Hellendoorn et al.,
2020). Graph-based representations (subsuming tree-based ones) commonly consider pairwise relationships
among entities in the code, whereas token-level ones simply consume a token sequence. In this section, we
present a novel hypergraph representation of code that retains the best of both representations.
Our program hypergraph construction is derived from the graph construction presented by Allamanis et al.
(2021), but uses hyperedges to produce a more informative and compact representation. The set of nodes
in the generated hypergraphs include tokens, expressions, abstract syntax tree (AST) nodes, and symbols.
However, in contrast to prior work, we do not only consider pairwise relationships, but instead use a typed
and qualiﬁed hypergraph (Fig. 1). We detail the used hyperedges and their advantages next. Fig. 3 in Appx. B
illustrates some of the considered relations on a small synthetic snippet.
Tokens Forthe token sequence t1,t2,...,tLof a snippet of source code,we create the relation Tokens(p1: t1,
p2:t2, ...). This is the entire information used by token-level Transformer models, considering all-to-all
relations among tokens. Note that in standard graph-based code representations, the token sequence is
usually represented using a chain of NextToken binary edges, meaning that long-distance relationships are
hard to discover for models consuming such graphs. For very long token sequences, which may cause memory
issues, we “chunk” the sequence into overlapping segments of length L, similar to windowed, sparse attention
approaches. We use L= 512in the experiments. Within HEATand speciﬁcally forthe Tokens(·)hyperedges,
to reduce the used parameters and to allow arbitrarily long sequences, the qualiﬁer embeddings rp1,rp2,rp3,...
are computed using the ﬁxed sinusoidal embeddings of Vaswani et al. (2017) instead of learnable embeddings.
ASTWe representthe program’s abstractsyntaxtree using AstNode relations,withqualiﬁers corresponding
to the names of children of each node. For example, an AST node of a binary operation is represented as
AstNode(node: nBinOp, left:nleft, op:nop, right:nright). Similarly, a AstNode(node: nIfStmt , cond:nc,
then:nt, else:ne)represents an ifstatement node nIfStmt. In cases where the children have the same
qualiﬁer and are ordered, ( e.g.the sequential statements within a block) we create numbered relations, i.e.
AstNode(node: nBlock, s1:n1, s2:n2, ...). For Python, we use as qualiﬁer names those used in libCST.
In contrast, most graph-based approaches use Childedges to connect a parent node to all of its children, and
thus lose information about the speciﬁc role of child nodes. As a consequence, this means that left and right
children of non-commutative operations can not easily be distinguished. Allamanis et al. (2021) attempts to
rectify this using a NextSibling edge type, but still is not able to make use of the known role of each child.
Control and Data Flow To indicate that program execution can ﬂow from any of the AST nodes
p1,p2,...to one ofthe AST nodes s1,s2,...,we use the relation CtrlF(prev: np1, prev:np2, ..., succ: ns1,
succ:ns2). For example, if c: p1 else: p2; s1 would yield CtrlF(prev:p1, prev:p2, succ:s1) . Sim-
ilarly, we use relations MayRead andMayWrite to represent dataﬂow, where the previous location at which a
5Published in Transactions on Machine Learning Research (09/2022)
symbol may have been read (written to) are connected to the succeeding locations. Note that these relations
compactly represent data and control ﬂow consolidating N-to-Mrelations ( e.g.Nstates may lead to one of
Mstates) into a single relation, which would otherwise require N·Medges in a standard graph.
Symbols We use the relation Symbol(sym: ns, occ:n1, occ:n2, ..., may_last_use: nu1,
may_last_use: nu2, ... ) to connect all nodes n1,n2,...referring to a symbol ( e.g.variable) to a
fresh nodens, introduced for each occurring symbol. We annotate within this relation the nodes nu1,nu2,...
that are the potential last uses of the symbol within the code snippet.
Functions A challenge in representing source code is how to handle calls to (potentially user-deﬁned)
functions. As it is usually not feasible to include the source code of all called functions in the model input
(for computational and memory reasons), appropriate abstractions need to be used. For a call (invocation) of
a function foo(arg1, ..., argN) deﬁned by def foo(par1, ..., parN): ... , we introduce the relation
foo(rval:n, par1:narg1, ..., par3: nargN)wherenis the invocation expression node and narg1,...,n argN
are the nodes representing each of the arguments. Hence, we generate one relation symbol for each deﬁned
function, and match nodes representing arguments to the formal parameter names as qualiﬁers. This repre-
sentation allows to naturally handle the case of variable numbers of parameters and arbitrary functions.
Syntactic sugar and operators are converted in the same way, using the name of the corresponding built-in
function. For example, in Python, a in bis converted into the relation __contains__(self: nb, item:na)
anda -= bis converted into __isub__(self: na, other:nb)following the reference Python data model.
Finally, we use the relation Returns(fn: nf, from:n1, from:n2, ...)to connect all possible return points
for a function with the AST node nffor the function deﬁnition. Similarly,a Yields(·)is deﬁned for generator
functions.
4 Evaluation
We evaluate HEAT on two tasks from the literature: bug detection and repair (Allamanis
et al., 2021) and knowledge base completion (Galkin et al., 2020). We implemented it as a Py-
Torch (Paszke et al., 2019) Module, available on the heatbranch of https://github.com/microsoft/
neurips21-self-supervised-bug-detection-and-repair/tree/heat .
4.1 HEAT for Bug Detection & Repair
We evaluate HEATon the bug localisation and detection task of Allamanis et al. (2021) in the supervised
setting. This is a hard task that requires combining ambiguous information with reasoning capabilities able
to detect bugs in real-life source code.
For this, we built on the open-source release of PyBugLab of Allamanis et al. (2021), making two changes:
(1) we adapt the graph construction from programs to produce hypergraphs as discussed in Sec. 3, and (2)
useHEATto compute entity representations from the generated graphs, rather than GNNs or GREAT.
Dataset We use the code of Allamanis et al. (2021) to generate a dataset of randomly inserted bugs to train
and evaluate a neural network in a supervised fashion. Consequently, we obtain a new variant of the “Random
Bugs” test dataset, consisting of ∼760kgraphs. We additionally re-extract the PyPIBugs dataset with the
provided script,generating hypergraphs as consumed by HEAT,and graphs generated by the baseline models.
However, since the PyPIBugs dataset is provided in the form of GitHub URLs referring to the buggy commit
SHAs, some of them have been removed from GitHub and thus our PyPIBugs dataset contains 2354 samples,
20 less than the one used by Allamanis et al. (2021).
Model Architecture We modify the architecture of Allamanis et al. (2021) to use 6 HEATlayers with
hidden dimension of 256, 8 heads, feed-forward (FFN in Eq. 4) hidden layer of 2048, and dropout rate of
0.1. As discussed above, our datasets diﬀer slightly from the data used by Allamanis et al. (2021), and we
re-evaluatedtheirreleasedcode on ournew datasets. We foundthis rerun to perform notably betterthan what
6Published in Transactions on Machine Learning Research (09/2022)
Table 1: Evaluation Results on Supervised Bug Detection and Repair on supervised PyBugLab.
Random Bugs PyPIBugs
Joint Loc. Repair Joint Loc. Repair
GNN (Allamanis et al., 2021)†62.4 73.6 81.2 20.0 28.4 61.8
GREAT (Allamanis et al., 2021)†51.0 61.9 76.3 16.8 25.8 58.6
GNN (Allamanis et al., 2021) (rerun) 69.8 79.6 83.4 22.0 28.3 66.7
GREAT (Allamanis et al., 2021) (rerun) 65.6 74.4 81.8 16.8 21.9 67.7
HEAT 76.5 83.1 88.5 24.6 29.6 71.0
HEAT– DeepSet-based messages 74.0 81.1 87.3 23.2 28.6 69.0
HEAT– without qualiﬁer embeddings 69.2 76.7 85.6 19.1 25.7 65.0
HEAT–Agg,CrossAtt 76.583.0 88.2 23.4 27.9 71.5
HEAT– without FFN 73.7 80.7 87.0 20.7 25.4 69.2
HEAT– without hyperedge state 75.7 82.4 88.0 23.0 28.6 71.1
†Reported on a diﬀerent random bugs dataset and a slightly diﬀerent PyPIBugs dataset.
was originally reported in the paper. In private communication, the authors explained that their public code
included a small change to the model architecture compared to the paper: the subnetworks used for selecting
a program repair rule are now shallow MLPs (rather than inner products),which increases performance across
the board. Our HEATextension follows the code release, and hence we use max-pooled subtoken embeddings
to initialise node embeddings, a pointer network-like submodel to select which part of the program to repair,
and a shallow MLPs to select the repair rules. We also re-use the PyBugLab supervised objective, which is
composed of two parts: a PointerNet-style objective requiring to identify the graph node corresponding to a
program bug, and a ranking objective requiring to select a ﬁxing program rewrite at the selected location.
Results We showthe results ofourexperiments in Table 1,where “Loc.” refers to the accuracyin identifying
the buggy location in an input program, “Repair” to the accuracy in determining the correct ﬁx given the
buggy location, and “Joint” to solving both tasks together. The results indicate that HEATimproves
performance on both considered datasets, improving the joint localisation and repair accuracy by ∼10%over
the two well-tuned baselines.
In particular, we observe that HEATsubstantially improves over GREAT (Hellendoorn et al., 2020), which
also adapts the Transformer architecture to include relational information. However, GREAT eschews explicit
modelling of edges, and instead uses (binary) relations between tokens to bias the attention weights in the
MHAcomputation. We observe a less pronounced gain over GNNs, which we believe is due to the simpler
information ﬂow across long distances and the clearer way of encoding structural information in HEAT. (see
Sec. 3) We note that Allamanis et al. (2021) showed that their models also outperform ﬁne-tuned variants
of the cuBERT (Kanade et al., 2020) model, which stems from self-supervised pre-training using masking
objectives. Consequently, we believe that HEAToutperforms such large models as well, though a comparison
to recent very large models, adapted to the task (Chen et al., 2021; Austin et al., 2021; Li et al., 2022) is left
to future work.
Variations and Ablations To understand the importance of diﬀerent components of HEAT, we experi-
ment with ﬁve ablations and variations, shown on the bottom of Table 1.
First, we study the importance of using multi-head attention to compute messages. In particular, we are
interested in determining whether considering interactions between diﬀerent nodes participating in a hyper-
edge is necessary. To this end, we consider an alternative scheme in which we ﬁrst compute a hyperedge
representation using aggregation of all adjacent nodes, using their qualiﬁer information, i.e.
q(t)
e=Agg/prime/parenleftBig/bracketleftBig
h(t)
e,rτ1⊕h(t)
n1,rτ2⊕h(t)
n2,.../bracketrightBig/parenrightBig
.
7Published in Transactions on Machine Learning Research (09/2022)
Table 2: Results on the Random Bugs dataset when applying HEATon a graph dataset representing edges
as (binary) edges.
Joint Localisation & Repair Localisation Repair
GNN (Allamanis et al., 2021) (rerun) 69.8 79.6 83.4
GREAT (Allamanis et al., 2021) (rerun) 65.6 74.4 81.8
HEAT(on binarised hypergraphs) 71.6 79.3 85.5
HEAT(on hypergraphs) 76.5 83.1 88.5
In our experiments, we use a Deep Set (Zaheer et al., 2017) model to implement Agg/prime. We then compute
messages for each node niusing a single linear layer W,i.e.
m(t)
e→ni=ReLU/parenleftBig
W·[rτi⊕h(t)
ni,q(t)
e]/parenrightBig
.
The results indicate that this model variant is still stronger than the GNN and GREAT architectures, but
thatHEATproﬁts from explicitly considering the relationships of nodes participating in a hyperedge.
Next, we analyse the importance of using qualiﬁer information in our model. To this end, we consider a
variant of HEATin which we remove from Eq. 5 the qualiﬁer information rτi,i.e.to
/bracketleftBig
m(t)
e,m(t)
e→n1,m(t)
e→n2,.../bracketrightBig
=MHA/parenleftBig/bracketleftBig
h(t)
e,h(t)
n1,h(t)
n2,.../bracketrightBig/parenrightBig
.
Theresultsclearlyindicatethatthequaliﬁer-as-positionschemeusedin HEATiscrucialforgoodperformance.
In particular, it indicates that the qualiﬁer information contained in the data is very valuable, and emphasises
the importance of considering qualiﬁed hypergraphs.
We also considered using a more expressive aggregation mechanism for messages, replacing the max pooling
we use to implement Agg. Speciﬁcally, we use a multi-head attention between the current node state (as
queries in the attention mechanism) and the messages computed for all adjacent hyperedges (appearing as
keys and values). This is reminiscent of graph attention networks (Veličković et al., 2018), which use an
attention mechanism to determine the respective importance of binary edges when aggregating messages.
In our experiments, this performed slightly worse than the aggregation using a simple max, while being
substantially more memory- and compute-intensive.
Next, we analyse whether the representational capacity is improved by the feedforward network (Eq. 4) in
the node and edge update. Removing these substantially reduces the number of parameters of the model.
The results indicate that these intermediate, per-representation computation steps add valuable capacity to
the model. This is especially apparent on the PyPIBugs dataset.
Another ablation we consider is a model variant in which we do not use evolving edge states, but instead
re-use h(0)
eon all HEATlayers. We observe a similar or slightly reduced performance on the joint localisation
and repair. This suggests that updating the edge state provides valuable representational capacity to the
model. Edge states, can be seen as the [CLS]token in traditional transformers, providing “scratch space” for
storing intermediate information for each hyperedge.
Finally, we assess the importance of the underlying data representation, investigating whether our represen-
tation of programs as hypergraphs (diﬀering from the PyBugLab baseline) alone explains the performance
improvement. For this, we convert the generated hyperedges to (binary) edges and use HEATto learn from
these (binary) graphs. This experiment intents to disentagle the eﬀects of the diﬀerent data representation
from the eﬀects of HEAT’s architecture. The results, shown in Table 2, indicate that even on binarised
hypergraphs, HEAToutperforms the baseline GNN and GREAT models (especially on accuracy of choosing
program repairs), even though large parts of its architecture are not properly utilised by the data. However,
HEATalso takes advantage of the more expressive and compact data representation.
8Published in Transactions on Machine Learning Research (09/2022)
4.2 HEAT for Knowledge Graph Completion
Knowledge graphs (KGs) can be accurately represented as typed and qualiﬁed hypergraphs. We now focus on
link prediction over a hyper-relational KG, which can be viewed as completing a knowledge base by additional
likely facts.
Dataset Following the discoveryoftestleaks anddesign ﬂaws byGalkin et al. (2020) in common benchmark
datasets such as WikiPeople (Guan et al., 2019) and JF17K (Wen et al., 2016) we chose one of the variations
of the new WD50K dataset presented there – WD50K (100). It is derived from Wikidata, containing 31k
statements,all of which use some qualiﬁer. To model a qualiﬁed triple statement of the form (s,r,o,Q )withQ
a set of qualiﬁer/entity pairs (qri,qvi), we create a single hyperedge with special qualiﬁers src(resp.) objfor
the source and object of the relation, i.e.(r,{(src,s),(obj,o)}∪Q). Using the example of Fig. 1 of Galkin
et al. (2020), the fact that Einstein studied mathematics at ETH Zurich in his undergraduate is expressed as
the hyperedge: EducatedAt(src: nEinstein , obj:nETH Zurich , degree:nBachelor , major:nMathematics ).
In the released WD50K dataset,raw Wikidata identiﬁers ( e.g.P69,Q937,etc.) are used to refer to entities and
relation names. Weenrichthedatasetbyadditionallyretrievingnaturallanguageinformation fortheseentities
from Wikidata ( e.g.replacing P69by “educated at”) and allow the models to consume this information, to
encourage similar treatment of similarly named relationships and entities.
Model Architecture We modiﬁed the open-source release of StarEby Galkin et al. (2020), replacing the
GNN-based StarEencoder by HEAT. We used a single layer of HEATwith embedding size of 100 and a
single layerofthe Transformerusedforcalculating the ﬁnalpredictions1. Apartfrom some trainingparameters
(see below), hyperparameters (dropout, etc.) remain as documented in Appendix C of Galkin et al. (2020).
Since our goal is to evaluate the eﬀectiveness of HEAT, the remainder of the model is identical to the original
implementation of Galkin et al. (2020): queries for a relation jare represented as the concatenation of the
entity embeddings, relation embedding, and qualiﬁer embeddings, as shown in Fig. 3 of Galkin et al. (2020).
These are then passed through a Transformer block and a fully-connected layer to obtain the probability over
each entity being a possible object of the relation.
To process the additional natural language information about relations and entities we extracted (see above),
we build a vocabulary using byte pair encoding (BPE), and then embed the individual tokens and use sum
pooling to obtain initial node and relation representations. We evaluated variations of both the original
StarEmodel and our HEAT-based variant using this information, see below.
Training and evaluation Training is performed as in Galkin et al. (2020) using binary cross entropy with
label smoothing. In this link prediction task, a matching score is calculated for all possible relation objects
given source, relation and qualiﬁer-entity pairs (Dettmers et al., 2018, p.3). We trained our model for 1k
epochs with a learning rate of 0.0004 and batch size of 512. Hyperparameters were ﬁne-tuned manually, using
the provided validation set in the StarE implementation. For direct comparison with Table 3 of Galkin et al.
(2020) we report mean reciprocal rank (MRR) and hits at 1 and 10 (H@1, H@10) matching the original
evaluation setting. We train and evaluate all models using 5 random seeds and report standard deviations.
Results Table 3 shows the results of our evaluation on WD50K (100). We reran the original StarE
implementation, both to validate our setup and to obtain standard deviations. First, we observe that using
HEATinstead of the original GNN-based StarEencoder improves results, without any further changes
to the architecture. We expect that recent orthogonal work improving StarEby Yu & Yang (2021) would
similarly improve with our model.
Next, we consider the inﬂuence of using natural language information extracted from Wikidata. We note
that the original StarEencoder does not proﬁt from this information. In contrast, the HEAT-based model
slightly improves results, even though most words in the extracted data are extremely sparse. Finally, we
evaluate the less expressive message aggregation scheme of max pooling (as discussed in Sec. 4.1). Here, we
see that its performance is only marginally worse.
1Figure 3 of Galkin et al. (2020), bottom rectangle.
9Published in Transactions on Machine Learning Research (09/2022)
Table 3: Link prediction results on WD50K (100) of Galkin et al. (2020). Standard deviations are obtained
over 5 diﬀerent seeds.
Model MRR H@1 H@10
StarE(GNN)†(Galkin et al., 2020) 0.654±0.002 0.586±0.002 0.777±0.002
StarE(GNN) – with NL information 0.653±0.003 0.586±0.003 0.774±0.005
StarE(HEAT) –Agg,max, no NL information 0.666±0.003 0.605±0.004 0.779±0.002
StarE(HEAT) –Agg,CrossAtt, no NL information 0.666±0.003 0.599±0.004 0.787±0.003
StarE(HEAT) –Agg,CrossAtt, with NL information 0.667±0.003 0.601±0.003 0.789±0.001
Hy-Transformer⋆(Yu & Yang, 2021) 0.699 0 .637 0 .812
†Rerun of Galkin et al. (2020) implementation with 5 seeds.
∗Current state-of-the-art.
5 Related Work
We review closely related techniques for learning on hypergraphs, and then brieﬂy discuss some particularly
relevant work from the application areas of learning on code and knowledge bases.
Hypergraph learning We broadly classify hypergraph learning into three approaches: hyperedge expan-
sion, spectral methods, and spatial (message passing-based) methods. Compared to HEAT, none of the
existing methods can directly work on typed and qualiﬁed hypergraphs.
In the ﬁrst class, hypergraphs are transformed into (binary) graphs and then a standard GNN is applied
on the resulting graph. This approach is taken by Agarwal et al. (2006), who use clique and star expansion
(representing each hyperedge as a full or a star graph respectively), Yadati et al. (2019), who represent each
hyperedge by a simple weighted edge whose endpoints can be further connected with weighted edges to
mediator nodes and Yang et al. (2020), who create a node for each pair of incident nodes and hyperedges
and connect those stemming from the same node or hyperedge. These approaches can re-use existing GNN
architectures, but at the cost of a signiﬁcantly increased number of edges.
The next class of methods aims at generalising the concepts of graph Laplacians and spectral convolutions to
the domain of hypergraphs. Feng et al. (2019) use the observations that nodes in the same hyperedge should
not diﬀer much in embedding to deﬁne a normalised hypergraph Laplacian. A similar approach has been
employed by Fu et al. (2019), who utilise the hypergraph p-Laplacian. Bai et al. (2021) extend Feng et al.
(2019) where the node-edge incidence matrix is weighted via an attention mechanism. Such methods are
usually applied to transductive learning tasks and either do not fully support typed or qualiﬁed hyperedges
or are limited to a ﬁxed number of hyperedge types or nodes in a hyperedge. This makes them inapplicable
in our setting.
The ﬁnal class of methods generalises the concept of neural message passing to hypergraphs. HyperSAGNN
(Zhang et al., 2020) is a self-attention based neural network, capable of handling variable-sized hyperedges,
but has been developed for the purposes of hyperedge prediction and is not directly applicable for node
classiﬁcation. HyperSAGE (Arya et al., 2020) performs convolution in two steps: nodes to hyperedges and
hyperedges to nodes, where the aggregation during the node and hyperedge message passing step is a power-
mean function, but it suﬀers from poor parallelisation and other practical issues (Huang & Yang, 2021, p. 2).
Chien et al. (2021) propose AllSet, a message passing scheme based on representing hyperedges as sets and
aggregations using permutation-invariant functions. AllSet can provably subsume a substantial number of
previous hypergraph convolution methods. Chien et al. (2021) propose two implementations of AllSet, using
DeepSets (Zaheer et al., 2017) and Transformers (Vaswani et al., 2017). This work is most similar to ours, but
does not consider the setting of qualiﬁed hyperedges. A natural consequence is that the model then computes
a single “message” per hyperedge, whereas HEATcomputes diﬀerent messages for each participating node
(which only is required when nodes play diﬀerent roles in the hyperedge). In Sec. 4.1, we consider two relevant
ablations. These show that the use of qualiﬁer information is crucial for good results in the PyBugLab setting,
and that our message computation based on multihead attention is stronger than a DeepSet-based alternative.
10Published in Transactions on Machine Learning Research (09/2022)
To the best of our knowledge, we are the ﬁrst to focus on processing typed and qualiﬁed hyperedges: two
hyperedges with the same element set can be diﬀerent if their type does not match and elements in a
hyperedge can have diﬀerent qualiﬁers (roles). Furthermore, our architecture is not restricted to a ﬁxed
number of hyperedge types/qualiﬁer and can generalise to types/qualiﬁers unseen during train time.
Knowledge Graph Completion Knowledge graph (KG) completion has emerged due to the incomplete-
ness of KGs (Ji et al., 2021). Embedding-based models (Bordes et al., 2013; Schlichtkrull et al., 2018; Shi
& Weninger, 2017) ﬁrst learn a low-dimensional embedding and then use it to calculate scores based on
these embeddings and rank the top kcandidates. HEATis a variation of an embedding-based model, but we
focus on hyper-relational KGs. Other,non-embedding-based approaches also exist, e.g.reinforcement learning
(Xiong et al., 2017) or rule-based ones (Rocktäschel & Riedel, 2017). Since we do not use such techniques,
we omit discussing them here and refer the reader to Ji et al. (2021, §IV.A).
Similarly to hypergraph learning, other works model hyper-relational KGs by simplifying qualiﬁed relations
to simpler representations: Wen et al. (2016) use clique expansion, which can be costly, (Fatemi et al., 2020)
represent hyper-relational facts as n-ary relations, but do not have explicit source/object of the relation and
instead of considering the qualiﬁer of an entity as in HEAT, their model only considers the position of an
entity within the relation; Guan et al. (2019) break n-ary facts into n+ 2qualiﬁer/entity pairs2, making
qualiﬁer/entity pairs indistinguishable to “standard” (s,rel,o )triples. These expansion-based approaches
cannot leverage semantic information such as the interaction of diﬀerent qualiﬁer/entity pairs (Yu & Yang,
2021).
Closest to our work is StarE(Galkin et al., 2020). StarEuses a GNN-like convolution, consisting of several
steps (cf. equations (5)-(7) of Galkin et al., 2020) on hyper-relational graphs to calculate updated entity
embeddings, which are then fed through a Transformer module. In Sec. 4 we show that HEAToutperforms
their GNN-based encoder.
Learning on Code Over the last decade, machine learning has been applied to code on a variety of
tasks (Allamanis et al., 2018a). A central theme in this research area is the representation of source code.
Traditionally, token-level representations have been used (Hindle et al., 2012), but Allamanis et al. (2018b)
showedthatgraph-levelapproachescanleverageadditionalsemanticinformationforsubstantialimprovements.
Subsequently, Fernandes et al. (2019); Hellendoorn et al. (2020) showed that combining token-level and graph-
level approaches yields best results. By using a relational transformer model over the code tokens Hellendoorn
et al. (2020) overcomes the inability of GNN models to handle long-range interactions well, while allowing to
make use of additional semantic information expressed as graph edges over the tokens. However, token-based
representations do not provide unambiguous locations for annotating semantic information ( i.e.edges) for
non-token units such as expressions ( e.g.a+bora+b+c). Additionally, all these approaches have been limited
to standard (binary) edges, making the resulting graphs large and/or imprecise (see Sec. 3). Our experiments
withHEATshow that a suitable representation as typed and qualiﬁed hypergraph further improves over the
combination of token-level and binary graph-level approaches.
6 Conclusions & Discussion
We introduced HEAT, a neural network architecture that operates on typed and qualiﬁed hypergraphs. To
model such hypergraphs, HEATcombines the idea of message passing with the representational power of
Transformers. Furthermore, we showed how to convert program source code into such hypergraphs. Our
experiments show that HEATis able to learn well from these highly structured hypergraphs, outperforming
strong recent baselines on their own datasets. A core insight in our work is to apply the power of Transformers
to several, overlapping sets of relations at the same time. This allows to concurrently model sequences and
graph-structured data. We believe that this opens up exciting opportunities for future work in the joint
handling of natural language and knowledge bases.
2+2for the source and object qualiﬁers
11Published in Transactions on Machine Learning Research (09/2022)
References
Sameer Agarwal, Kristin Branson, and Serge J. Belongie. Higher order learning with graphs. In International
Conference on Machine Learning (ICML) , 2006.
Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine learning
for big code and naturalness. ACM Computing Surveys (CSUR) , 51(4):1–37, 2018a.
Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with
graphs. In International Conference on Learning Representations (ICLR) , 2018b.
Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. Self-supervised bug detection and repair.
InNeural Information Processing Systems (NeurIPS) , 2021.
Devanshu Arya, Deepak K. Gupta, Stevan Rudinac, and Marcel Worring. Hypersage: Generalizing inductive
representation learning on hypergraphs. arXiv preprint arXiv:2010.04558 , 2020.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language
models.arXiv preprint arXiv:2108.07732 , 2021.
Lei Jimmy Ba, Ryan Kiros, and Geoﬀrey E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016.
Song Bai, Feihu Zhang, and Philip H. S. Torr. Hypergraph convolution and hypergraph attention. Pattern
Recognit. , 110:107637, 2021. doi: 10.1016/j.patcog.2020.107637.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinícius Flores Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Çaglar Gülçehre,
Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles
Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick,
OriolVinyals,Yujia Li,andRazvan Pascanu. Relationalinductive biases,deeplearning,andgraphnetworks.
arXiv preprint arXiv:1806.01261 , 2018.
Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Oksana Yakhnenko. Translating
embeddings for modeling multi-relational data. In Neural Information Processing Systems (NeurIPS) ,
2013.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374 , 2021.
Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are AllSet: A multiset function framework
for hypergraph neural networks. arXiv preprint arXiv:2106.13264 , 2021.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge
graph embeddings. In AAAI Conference on Artiﬁcial Intelligence , 2018.
Bahare Fatemi, Perouz Taslakian, David Vázquez, and David Poole. Knowledge hypergraphs: Prediction
beyond binary relations. In International Joint Conferences on Artiﬁcial Intelligence (IJCAI) , 2020.
Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In AAAI
Conference on Artiﬁcial Intelligence , 2019.
Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured neural summarization. In
International Conference on Learning Representations (ICLR) , 2019.
Sichao Fu, Weifeng Liu, Yicong Zhou, and Liqiang Nie. HpLapGCN: Hypergraph p-laplacian graph convolu-
tional networks. Neurocomputing , 362:166–174, 2019. doi: 10.1016/j.neucom.2019.06.068.
12Published in Transactions on Machine Learning Research (09/2022)
Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens Lehmann. Message passing
for hyper-relational knowledge graphs. In Empirical Methods in Natural Language Processing , 2020.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message
passing for quantum chemistry. In International Conference on Machine Learning (ICML) , 2017.
Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. Link prediction on n-ary relational data.
InWorld Wide Web Conference , 2019.
Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global relational
models of source code. In International Conference on Learning Representations , 2020.
Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness of
software. In 2012 34th International Conference on Software Engineering (ICSE) , 2012.
Jing Huang and Jie Yang. UniGNN: a uniﬁed framework for graph and hypergraph neural networks. In
Internal Joint Conferences on Artiﬁcial Intelligence (IJCAI) , 2021.
Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs:
Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning
Systems, 2021.
Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual
embedding of source code. In International Conference on Machine Learning (ICML) , 2020.
Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In
International Conference on Learning Representations (ICLR) , 2017.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In
International Conference on Learning Representations (ICLR) , 2016.
YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,RémiLeblond,TomEccles,James
Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor
Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,
Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu,
and Oriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814 ,
2022.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Neural
Information Processing Systems (NeurIPS) . 2019.
Tim Rocktäschel and Sebastian Riedel. End-to-end diﬀerentiable proving. In Neural Information Processing
Systems (NeurIPS) , 2017.
Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling.
Modelingrelationaldatawithgraphconvolutionalnetwork. In ExtendedSemantic WebConference (ESWC) ,
2018.
Baoxu Shi and Tim Weninger. Proje: Embedding projection for knowledge graph completion. In AAAI
Conference on Artiﬁcial Intelligence , 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems (NeurIPS) ,
2017.
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph Attention Networks. In International Conference on Learning Representations (ICLR) , 2018.
13Published in Transactions on Machine Learning Research (09/2022)
Jianfeng Wen,Jianxin Li,Yongyi Mao,Shini Chen,andRichong Zhang. On the representation andembedding
of knowledge bases beyond binary relations. In International Joint Conferences on Artiﬁcial Intelligence
(IJCAI), 2016.
Wenhan Xiong, Thien Hoang, and William Yang Wang. DeepPath: A reinforcement learning method for
knowledge graph reasoning. arXiv preprint arXiv:1707.06690 , 2017.
Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha P. Taluk-
dar. HyperGCN: A new method for training graph convolutional networks on hypergraphs. In Neural
Information Processing Systems (NeurIPS) , 2019.
Chaoqi Yang,Ruijie Wang,Shuochao Yao,andTarek F. Abdelzaher. Hypergraphlearning withline expansion.
arXiv preprint arXiv:2005.04843 , 2020.
Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 440–450, 2017.
Donghan Yu and Yiming Yang. Improving hyper-relational knowledge graph completion. arXiv preprint
arXiv:2104.08167 , 2021.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabás Póczos, Ruslan Salakhutdinov, and Alexan-
der J. Smola. Deep sets. In Neural Information Processing Systems (NeurIPS) , 2017.
Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-SAGNN: a self-attention based graph neural network for
hypergraphs. In International Conference on Learning Representations (ICLR) , 2020.
14Published in Transactions on Machine Learning Research (09/2022)
A Packing Hyperedges for HEAT
Hyperedges in HEAThave a variable size, i.e.a variable number of nodes that participate in each hyperedge.
For each hyperedge HEATuses a multihead attention to compute the outgoing messages mei→nj. However,
multihead attention has a quadratic computational and memory cost with respect to the size of the hyperedge.
A naïve solution would require us to pad all hyperedges up to a maximum size. However, this would be
wasteful. On the other hand, we need to batch the computation across multiple hyperedges for eﬃcient GPU
utilisation. To achieve a good trade-oﬀ between large batch sizes (a lot of elements) and minimal padding, we
consider a small set “microbatches”. To pack the hyperedges optimally, we would solve the following integer
linear program (ILP)
min1..k/summationdisplay
j/parenleftbig
L2
j·yj/parenrightbig
−/summationdisplay
il2
i
s.t.
yj∈{0,1},xij∈{0,1},∀i,j
1..k/summationdisplay
jxij= 1,∀i
1..n/summationdisplay
ixijsi≤yjLj,∀j,
whereyjis a binary variable indicating whether “bucket” jof sizeLj(i.e.belonging in the microbatch
of sizeLj) is used ( i.e.contains any elements). Then, ljis the width of the jth hyperedge. xijare binary
variables for i= 1..nandj= 1..kindicating if the element iis in bucket jandsithe width of bucket i.
The objective creates as few buckets as possible to minimise the wasted (quadratic) space/time needed for
multihead attention over variable-sized sequences. The ﬁrst constraint requires that each element is assigned
to exactly one bucket and the second constraint that each used bucket is ﬁlled up to its capacity.
However, the complexity of this ILP is prohibitive. Instead, we resort in using a greedy algorithm to select
the “microbatches” used. This is detailed in Algorithm 1.
Algorithm 1 Greedy Hyperedge Packing into Microbatches
buckets←[ ]
forh in sortedDescendingByWidth(hyperedges) do
wasAdded←False
forbucket in buckets do
ifbucket.remainingSize ≥h.width then
bucket.add(h)
wasAdded←True
break
ifnot wasAdded then
bucketSize←smallestFittingMicrobatchWidth(h.width)
newBucket←createBucket(bucketSize)
newBucket.add(e)
buckets.append(newBucket)
returnGroupBucketsToMicrobatches(buckets)
15Published in Transactions on Machine Learning Research (09/2022)
if1is_foo2(3x4)56:719
[INDENT]8x9=10foo11(12x13)1415161719
[DEDENT]1819
y20.21bar2223(24x25)2627
Sample Hyperedges (Relations)
•Tokens(p1: n1, p2:n2, p3:n3, p4:n4, p5:n5, p6:n7, p7:n8, p8:n9, p9:n10, p10:n11, p11:n12,
p12:n13,···)
•AstNode(node: n19, test:n6, body:n17)
•AstNode(node: n17, value:n16, target:n9)
•foo(rval:n12, fzz:n9)
Assuming that foois deﬁned as def foo(fzz): ...
•__getattribute__(rval: n23, self:n20, name:n22)
•MayRead(prev: n4, prev:n13, succ:n25)
•CtrlF(prev: n6, prev:n16, succ:n23)
•Symbol(sym: nx, occ:n4, occ:n9, occ:n13, may_last_use: n25)
Figure 3: Sample relations for the synthetic snippet shown on the top. The AST nodes and tokens are
wrapped in boxes and numbered appropriately in a preorder fashion. Only a few samples of the relations
(mapped to hyperedges) are shown below.
B Code as Hypergraph Example
Fig. 3 shows a synthetic code snippet with all the token and AST nodes enclosed in boxes. Some sample
relations are also shown. Finally, Fig. 4 shows a full hypergraph for the following code snippet
def foo(a, b):
if a in b:
a += 1
return a * 2
16Published in Transactions on Machine Learning Research (09/2022)
FunctionDef
FunctionDef$rval
Symboloccurrence
Returnsfn
ControlFlowNextprevIndentedBlock
body
IndentedBlock$rvalParameters
paramsParameters$rvalParam
params1
Param$rvalParam
params Param$rval
If
body
If$rvalSimpleStatementLine
body1
SimpleStatementLine$rvalIndentedBlock
body
IndentedBlock$rvalComparison
test
Comparison$rval
___assign$rvalComparisonTarget
comparisons
ComparisonTarget$rval
__contains__$rval source next
ControlFlowNextprevSimpleStatementLine
body
SimpleStatementLine$rvalReturn
body
Return$rvalfrom nextReturn
body
Return$rval from
ControlFlowNextnextBinaryOperation
value
BinaryOperation$rval
__add__$rvalnext prevfoo
symbol
a
Symbolsymbolb
Symbolsymboldef
foo
<DEDENT>(
name)
:
<INDENT>a
if, name
occurrence
MayWriteprevious_usesbcomma
name
occurrence
MayWriteprevious_uses
<INDENT>
return:a
in
left self
occurrence next_uses
MayUselast_may_writesb
comparator item occurrence may_final_use next_usesoperator
a
<DEDENT>value
occurrence may_final_use next_uses uses return
a
+
1 operator
right otherleft selfoccurrence may_final_use next_uses uses
Figure 4: A full hypergraph sample for the snippet disc Hyperedges are denoted as shaded (blue) boxes. Best
viewed on screen. The Tokens(·)hyperedge is omitted for clarity but the sequence of tokens is placed in the
box (right).
17