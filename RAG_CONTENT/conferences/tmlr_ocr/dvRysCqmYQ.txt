Under review as submission to TMLR
Balanced Mixed-Type Tabular Data Synthesis with Diffusion
Models
Anonymous authors
Paper under double-blind review
Abstract
Diffusion models have emerged as a robust framework for various generative tasks, including
tabular data synthesis. However, current tabular diffusion models tend to inherit bias in
the training dataset and generate biased synthetic data, which may influence discriminatory
actions. In this research, we introduce a novel tabular diffusion model that incorporates
sensitive guidance to generate fair synthetic data with balanced joint distributions of the
targetlabelandsensitiveattributes, suchassexandrace. Theempiricalresultsdemonstrate
that our method effectively mitigates bias in training data while maintaining the quality of
the generated samples. Furthermore, we provide evidence that our approach outperforms
existing methods for synthesizing tabular data on fairness metrics such as demographic
parity ratio and equalized odds ratio, achieving improvements of over 10%.
1 Introduction
Tabular data synthesis, particularly when involving mixed-type variables, presents unique challenges due to
the need to simultaneously handle continuous and discrete features. Recent advances in diffusion models
(Sohl-Dickstein et al., 2015; Ho et al., 2020) are now being leveraged to tackle these challenges. Diffusion
models have demonstrated remarkable performance in generative modeling for various tasks, especially text-
to-image synthesis. Researchers use diffusion frameworks to model images paired with text descriptions and
show promising results (Nichol et al., 2021; Saharia et al., 2022). Advancing further from diffusion models in
pixel space, latent diffusion frameworks optimize image representation in a low-dimensional space, thereby
enhancing synthetic image quality and efficiency (Rombach et al., 2022; Ramesh et al., 2022). The impressive
results of diffusion models in image synthesis extended to generating sequential data such as audio and time
series (Kong et al., 2020; Rasul et al., 2021; Li et al., 2022). This advancement has gradually overshadowed
generative adversarial networks, which were previously a significant focus in generative modeling, as diffu-
sion models consistently demonstrate superior performance across diverse tasks (Dhariwal & Nichol, 2021;
Stypułkowski et al., 2023; Mazé & Ahmed, 2023).
Researchers have recently been motivated by the versatility of diffusion models to explore the potential in
synthesizing tabular data (Kotelnikov et al., 2023; He et al., 2023). Tabular data often involves sensitive
user information, which raises significant privacy concerns when used for model training. Moreover, such
datasets suffer from limitations in size and imbalanced class distribution (Jesus et al., 2022), leading to
challenges in training robust and fair machine learning models. To address these issues, there is a growing
demand for effective tabular data synthesis. However, generative modeling of tabular data is challenging due
to the combination of discrete and continuous features and their varying value distributions. Interpolation-
based techniques, such as SMOTE (Chawla et al., 2002), are simple and effective but they pose privacy
concerns as they generate new data points through direct interpolation of existing data. Researchers have
used adversarial generative networks and variational autoencoders to generate tabular data with promising
results (Ma et al., 2020; Xu et al., 2019). Research on diffusion models for tabular data is ongoing, and the
primary challenge is to handle continuous and discrete features simultaneously. In Austin et al. (2021), the
authors suggest using diffusion frameworks for discrete features with Markov transition matrices. Recent
developments such as TabDDPM (Kotelnikov et al., 2023) and CoDi (Lee et al., 2023), which combine
Markov transition functions (called diffusion kernels) for continuous and discrete features, mark significant
1Under review as submission to TMLR
progress in this field and underscore the potential of diffusion models as a comprehensive solution for tabular
data synthesis.
Despite the success of diffusion models in various generative tasks, ethical concerns regarding potential bias
and unsafe content in the synthetic data persist. Generative models are inherently data-driven, thereby
propagating these biases into the synthetic data. Studies such as fair diffusion (Friedrich et al., 2023) and
safe latent diffusion (Schramowski et al., 2023) have shown that latent diffusion models, particularly in text-
to-image synthesis, can create images biased toward certain privileged groups and may contain inappropriate
elements, such as nudity and violence. In response, they propose methods to apply hazardous or sensitive
guidance in latent space and then eliminate bias and offensive elements in the synthetic images. In addition
totextandimages, tabulardatasetsfrequentlyexhibitnotablebias, oftenstemmingfromissuesrelatedtothe
disproportionaterepresentationofsensitivegroups. Forinstance,somedatasetsexhibitsignificantimbalances
in demographic groups regarding sensitive attributes such as sex and race (Le Quy et al., 2022). Training
machine learning models on biased tabular data can result in discriminatory outcomes for underrepresented
groups. However, in our literature review, research on the safety and fairness of diffusion models in areas
beyond text-to-image synthesis is limited. Although diffusion models have shown remarkable performance
in tabular data synthesis (Sattarov et al., 2023; He et al., 2023; Kotelnikov et al., 2023; Lee et al., 2023; Kim
et al., 2022), they are prone to learning and replicating these biases in training data.
Existing tabular diffusion models are typically either unconditional or conditioned on a single target vari-
able. Forexample, CoDi(Leeetal.,2023)leveragestwointerconnecteddiffusionmodelstoseparatelyprocess
continuous and discrete variables while maintaining their inter-variable correlations through co-evolutionary
conditioning and contrastive learning. Although CoDi efficiently captures inter-variable relationships, its
conditioning mechanism remains restricted to single-label guidance. As a result, CoDi does not explicitly
address fairness considerations in the presence of sensitive attributes. In contrast, we introduce a novel tabu-
lar diffusion model that approximates mixed-type tabular data distributions, conditioned on both outcomes
and multiple sensitive attributes. Our method extends tabular diffusion models from label-only conditioning
to multivariate feature-level conditioning for enhancing fairness. To promote fairness, our approach generates
data with balanced joint distributions of the target label and sensitive attributes. Specifically, our approach
integrates sensitive guidance into the tabular diffusion model and extends it to manage multiple sensitive
attributes, such as sex and race. We utilize a U-Net architecture to approximate the data distributions in
the latent space during the reverse diffusion process conditioned on various variables, including the target
label and sensitive attributes. To ensure fairness while preserving the integrity of the original data, we apply
the element-wise comparison technique from safe latent diffusion to regulate the sensitive guidance within
an appropriate range. Additionally, we employ balanced sampling techniques to reduce disparities in group
representation within the synthetic data. Given that our method leverages diffusion frameworks tailored for
tabular data and supports multiple variables for conditional generation of fair tabular data, it also has the
potential for extension to multi-modal data generation with fairness considerations using diffusion models.
In summary, our main contributions are as follows: We employ balanced sampling techniques to reduce
disparities in group representation within the synthetic data. In summary, our main contributions are as
follows:
1. We introduce, to our knowledge, the first diffusion-based framework tailored to learn distributions of
mixed-type tabular data with the extension from label-only conditioning to multivariate feature-level
conditioning for fairness.
2. We generate tabular data that is balanced for a predefined set of sensitive attributes, addressing
inherent biases in the data.
3. Our model demonstrates strong performance in terms of fidelity and diversity of synthetic data,
while surpassing existing models in fairness metrics such as demographic parity ratio and equalized
oddes ratio by more than 10%on average across three experimental datasets.
The remainder of this paper is organized as follows: Section 2 provides an overview of related work on
fairness-aware machine learning and diffusion models. Section 3 discusses the relevant background of mixed-
type modeling with diffusion models. In Section 4, we present our approach to multivariate conditioning and
2Under review as submission to TMLR
balanced sampling. In Section 5, we illustrate the experiments and present the results. Section 6 discusses
the limitations of our method. Finally, in Section 7, we conclude our findings and discuss their implications.
2 Related Work
2.1 Bias in Machine Learning
The concept of bias in machine learning that involves assigning significance to specific features to enhance
overall generalization is essential for the effective performance of models (Mehrabi et al., 2021). However, it is
crucial to acknowledge that bias in machine learning can also have negative implications. Negative bias is an
inaccurate assumption made by machine learning algorithms that reflects systematic or historical prejudices
against certain groups of people (Zanna et al., 2022). Decisions derived from such biased algorithms can lead
to adverse effects, particularly impacting specific social groups defined by factors such as sex, age, disability,
race, and more.
This concept of bias and fairness in machine learning has been widely studied, the aim being to mitigate
algorithmic bias of sensitive attributes from machine learning models (Mehrabi et al., 2021; Pessach &
Shmueli, 2022). Many methods for bias mitigation in classification tasks have been proposed, and they
can be classified into three categories: pre-processing, in-processing, and post-processing methods. Pre-
processing methods (Kamiran & Calders, 2012; Calmon et al., 2017) transform the collected data to remove
discrimination and train machine learning models on discrimination-free datasets. In-processing methods
(Zhang et al., 2018; Agarwal et al., 2018; Kamishima et al., 2011; Zafar et al., 2017; Kamishima et al., 2012)
regulate machine learning algorithms by incorporating fairness constraints or regularization terms into the
objective functions. Lastly, post-processing methods (Hardt et al., 2016; Pleiss et al., 2017), implemented
after training machine learning models on collected datasets, directly override the predicted labels to improve
fairness.
2.2 Fair Data Synthesis
Fairness-aware generative models typically operate as pre-processing methods. One such method proposed
by Xu et al. (2018) introduced fairness-aware adversarial generative networks that employ a fair discrimi-
nator to maintain equality in the joint probability of features and labels conditioned on subgroups within
sensitive attributes. Similarly, DECAF (Van Breugel et al., 2021), a causally-aware GAN-based framework,
incorporates structural causal models into the generator to ensure fairness, enabling inference-time debiasing
and compatibility with multiple fairness definitions. However, these approaches do not effectively tackle the
imbalance in sensitive classes within synthetic data, and diffusion-based tabular data synthesis methods such
as Kotelnikov et al. (2023); Lee et al. (2023); Zhang et al. (2023) excel in producing synthetic data of superior
quality. Fair Class Balancing (Yan et al., 2020), an oversampling algorithm based on SMOTE, demonstrates
impressive results but lacks flexibility in manipulating data distributions. As an interpolation-based method,
it struggles with data diversity and is not well-suited for multi-modal generation. Recently, Fair4Free (Sikder
et al., 2024) introduced a novel approach to fairness-aware data synthesis using data-free distillation, which
trains a smaller student model to generate fair synthetic data while preserving privacy and outperforming
state-of-the-art methods in fairness, utility, and data quality.
2.3 Fair/Safe Diffusion Models
With impressive capabilities in generative modeling, diffusion models are a class of deep generative models
that utilize a Markov process that starts from noise and ends with the target data distribution. Diffusion
models have demonstrated superior performance over large language models in modeling structured data
like images and tables, as they are specifically designed to handle mixed-type features and maintain complex
inter-dependencies (Zhang et al., 2023). Although diffusion models are widely studied, research on the
fairness and safety of diffusion models is limited, and existing works (Schramowski et al., 2023; Friedrich
et al., 2023) mainly focus on text-to-image synthesis tasks. Friedrich et al. (2023) utilize sensitive content,
such as sex and race, within text prompts to guide training diffusion models. Subsequently, during the
sampling phase, a uniform distribution of sensitive attributes is applied to generate images that are fair and
3Under review as submission to TMLR
do not exhibit preference towards privileged groups. Similarly, (Schramowski et al., 2023) uses unsafe text,
such as nudity and violence, to guide diffusion models and remove unsafe content in the sampling phase.
In addition, this method applies some techniques to remove unsafe content from synthetic images while
keeping changes minimal. However, these fairness-aware diffusion models have not been adapted to tabular
data synthesis, even though bias commonly exists in tabular datasets (Le Quy et al., 2022). This paper
studies how to model mixed-type tabular data conditioning on labels and multiple sensitive attributes in
latent space. Our method can generate balanced tabular data considering multiple sensitive attributes and
subsequently achieve improved fairness scores without compromising the quality of the synthetic data.
3 Diffusion Models
Diffusion models, taking inspiration from thermodynamics, are probabilistic generative models that operate
under the Markov assumption. Diffusion models involve two major components: a forward process and a
reverse process. The forward process gradually transforms the given data x0into noise xTpassing through
Tsteps of the diffusion kernel q(xt|xt−1). In the reverse process, xTis restored to the original data x0
byTsteps of posterior estimator pθ(xt−1|xt)with trainable parameters θ. In this section, we explain the
Gaussian diffusion kernel for continuous features and the multinomial diffusion kernel for discrete features.
Furthermore, we show how to optimize the parameters θof the posterior estimator considering mixed-type
data with both continuous and discrete features. Lastly, we explain classifier-free guidance Ho & Salimans
(2022) for conditional data synthesis of diffusion models.
3.1 Gaussian Diffusion Kernel
TheGaussiandiffusionkernelisusedintheforwardprocessforcontinuousfeatures. AfteraseriesofGaussian
diffusion kernels with Ttimesteps, xTfollows the standard Gaussian distribution N(0,I). Specifically, with
the values of βtpredefined according to a schedule such as linear, cosine, etc. (Chen, 2023), the Gaussian
diffusion kernel q(xt|xt−1)can be written as:
q(xt|xt−1) =N(xt|/radicalbig
1−βtxt−1,βtI)
With the starting point x0known, the posterior distribution in the forward process can be calculated
analytically as follows,
q(xt−1|xt,x0) =N(xt−1|˜µt(xt,x0),˜βtI)
˜µt(xt,x0) =1√αt(xt−βt√1−¯αtϵ) (1)
˜βt=1−¯αt−1
1−¯αtβt
whereαt= 1−βt,¯αt=/producttextt
s=1αs. In the reverse process, a posterior estimator with parameters θis used to
approximate pθ(xt−1|xt)at each timestep tgiven xt.
pθ(xt−1|xt) =N(xt−1|ˆµ(xt),ˆΣ(xt))
Ho et al. (2020) set the estimated covariance matrix ˆΣ(xt)as a diagonal matrix with constant values βtor
˜βt, and calculate the estimated mean ˆµ(xt)as follows,
ˆµ(xt) =1√αt(xt−βt√1−¯αtˆϵ(xt)) (2)
where ˆϵ(xt)is the estimated noise at time t.
3.2 Multinomial Diffusion Kernel
The multinomial diffusion kernel is applied in the forward process for discrete features with the same noise
scheduleβtusedbytheGaussiandiffusionkernel. Fordiscretefeatures, inthefinaltimestep T,xTfollowsthe
4Under review as submission to TMLR
discrete uniform distribution Uniform(K), whereKis the number of categories. The multinomial diffusion
kernel can be written as:
q(xt|xt−1) = Cat( xt|(1−βt)xt−1+βt/K)
Similarly to the continuous case, the posterior distribution of the multinomial diffusion kernel has an ana-
lytical solution.
q(xt−1|xt,x0) = Cat( xt−1|˜θ/K/summationdisplay
k=1˜θk)
˜θ= [αtxt+ (1−αt)/K]⊙[¯αt−1x0+ (1−¯αt−1)/K]
The posterior estimator for discrete features is preferably parameterized by the estimated starting point
ˆx0(xt)as suggested by Hoogeboom et al. (2021).
pθ(xt−1|xt) =q(xt−1|xt,ˆx0(xt)) (3)
3.3 Model Fitting
The parameters θof the posterior estimator are learned by minimizing the variational lower bound,
L(x0) =Eq(x0)[D(q(xT|x0)∥p(xT))−logpθ(x0|x1)
+T/summationdisplay
t=2D(q(xt−1|xt,x0)∥pθ(xt−1|xt))](4)
where Kullback–Leibler divergence is denoted by D. The variational lower bound can be simplified for
continuous variables as the mean squared error between true noise ϵin Equation (1) and estimated noise ˆϵ
in Equation (2), denoted by LG.
LG=Eq(x0)[∥ϵ−ˆϵ(xt)∥2]
For mixed-type data containing continuous features and Ccategorical features, the total loss LTcan be
expressed as the summation of the Gaussian diffusion loss term LGand the average of the multinomial
diffusion loss terms in Equation (4) for all categorical features:
LT=LG+/summationtext
i≤CL(i)
C
To train latent diffusion models, let ztbe the latent representation of xt, the estimated noise ˆϵ(xt)for
continuous features in Equation (2) can be reformulated as ˆϵ(zt). The estimated original input ˆx0(xt)in
Equation (3) can be reformulated as ˆx0(zt).
3.4 Classifier-Free Guidance
Classifier-free guidance (Ho & Salimans, 2022) is a mechanism used in diffusion models to condition the
generation process on auxiliary information without requiring an external classifier. Below, we provide a
mathematical overview of this method. Let xtdenote the noisy data at timestep tin the reverse diffusion
process, and let crepresent the conditional information (e.g., a class label). The reverse diffusion process
aims to estimate the posterior p(xt−1|xt,c), which is parameterized by a neural network ϵθ. Classifier-free
guidance interpolates between conditional and unconditional estimates of the noise, as shown below,
ϵ(xt,c) =ϵθ(xt) +wg(ϵθ(xt,c)−ϵθ(xt))
whereϵθ(xt)is the unconditional noise estimate, ϵθ(xt,c)is the conditional noise estimate, and wgis the
guidance weight, controlling the influence of the conditional term. The term ϵθ(xt,c)−ϵθ(xt)represents the
guidance signal derived from the condition c. By scaling this signal with wg, the model adjusts the strength
of conditioning during generation. Larger values of wgenforce stronger adherence to the condition, while
smaller values reduce its influence, yielding outputs closer to the unconditional distribution.
5Under review as submission to TMLR
For tasks involving multiple conditions, the guidance term can be extended to support multivariate condi-
tioning. Suppose we have Nconditionsc1,c2,...,cN, the guided noise estimate can be formulated as,
ϵ(xt,c1,c2,...,cN) =ϵθ(xt) +N/summationdisplay
i=1wgiγ(xt,ci)
whereγ(xt,ci) =ϵθ(xt,ci)−ϵθ(xt)represents the guidance signal for the i-th condition, and wgirepresents
thei-th guidance weight. The overall guidance is computed as the sum of these individual contributions
across all conditions.
4 Methods
As explained in Section 1, generative diffusion models for tabular data are prone to learning the sensitive
group size disparity in training data. In this section, we describe the process of synthesizing balanced
mixed-type tabular data considering sensitive attributes. We first introduce conditional generation given
multivariate guidance while preserving the synthesis quality. Then, we explain the architecture of the back-
bone deep neural network used as the posterior estimator in our method. Lastly, we explain the procedure for
sampling balanced data with consideration for sensitive attributes. The high-level structure of our method
is shown in Figure 1.
MLPGaussian
Diffusion
Multinomial
DiffusionPreprocessing
MLP
Denoising Step Cross Attention Skip ConnectionConditioning
/ Balanced
Sampling
MLP
Figure 1: The diagram of our model architecture. In the forward process, the input data point xis pre-
processed into numerical part xnumand categorical part xcat, and then passing through Tsteps of the
diffusion kernel to get x1,···,xT.z1,···,zTis the latent representation of x1,···,xT. In the reverse
process, the posterior estimator iteratively denoises noisy input zTconditioning on an outcome candN
sensitive attributes s(1),···,s(N). The estimated data point is ˆx. Our model can generate fair synthetic
data by leveraging sensitive guidance to ensure a balanced joint distribution of the target label and sensitive
attributes.
4.1 Multivariate Latent Guidance
To achieve conditional data generation with diffusion models, classifier-free guidance (Ho & Salimans, 2022)
is a simple but effective approach. Intuitively, the classifier-free guidance method tries to let the posterior
estimator “know” the label of the data it models. In latent space, it uses one neural network to receive zt
6Under review as submission to TMLR
and condition cto make estimations. We can denote the estimated noise given corresponding condition c
for continuous features as ¯ϵ(zt,c),
¯ϵ(zt,c) = ˆϵ(zt) +wg(ˆϵ(zt,c)−ˆϵ(zt)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
γ(zt,c)) (5)
whereγ(zt,c) = ˆϵ(zt,c)−ˆϵ(zt)is guidance of cand it is scaled by guidance weight wg.
Equation (5) can be extended to support multivariate guidance. Especially in tabular synthesis, we want to
generate samples conditioned on labels cand sensitive features s. In this case, we can let our neural network
takezt,c, and sas inputs, resulting in:
¯ϵ(zt,c,s) = ˆϵ(zt) +wg(γ(zt,c) +γ(zt,c,s)) (6)
Using the mechanism of multivariate classifier-free guidance introduced in Section 3.4, Equation (6) extends
label-only conditioning in classifier-free guidance with multivariate feature-level conditioning. The term
γ(zt,c)is the guidance term for the target label, representing the adjustment to the noise estimate based
on the label condition. The term γ(zt,c,s)is the sensitive guidance term, incorporating adjustments based
on the sensitive attribute s, and the label condition chere is used as a reference to restrict the influence
of the sensitive guidance, ensuring fairness without distorting the primary label distribution. Since the
primary conditional guidance is the target label, incorporating sensitive guidance from multiple attributes
should influence the synthetic data without significantly altering it compared to using only the target label
as guidance. To achieve this, the sensitive guidance γ(zt,c,s)is controlled element-wisely by a “security
gate”µ(c,s;ws,λ),
γ(zt,c,s) =µ(c,s;ws,λ)(ˆϵ(zt,s)−ˆϵ(zt))
µ(c,s;ws,λ) =/braceleftigg
ϕ,where ˆϵ(zt,c)⊖ˆϵ(zt,s)<λ
0,otherwise
ϕ= max(1,ws|ˆϵ(zt,c)−ˆϵ(zt,s)|)
where⊖is element-wise subtraction, and wsis the sensitive guidance weight. The purpose of µ(c,s;ws,λ)
is to keep the alterations caused by the sensitive guidance steady and minimal. It deactivates the sensitive
guidance when the difference between the estimation conditioned on cand the estimation conditioned on sis
larger than a threshold λ. Furthermore, sensitive guidance is deactivated in the early phase of the diffusion
process when tis smaller than a warm-up timestep δ(a positive integer indicating the timestep at which
sensitive guidance starts):
γ(zt,c,s) :=0ift<δ
Additionally, a momentum term with momentum weight wmis added to the sensitivity guidance to make
guidance direction stable,
γt(zt,c,s) =µ(c,s;ws,λ)(ˆϵ(zt,s)−ˆϵ(zt)) +wmνt
νt+1=βνt+ (1−β)γt
whereν0=0andβ∈[0,1]. To include not only one sensitive feature, but Nsensitive features S=
{s(1),···,s(N)}, we can extend the sensitive guidance as follows:
γ(zt,c,S) =N/summationdisplay
i=1µ(c,s(i);ws,λ)(ˆϵ(zt,s(i))−ˆϵ(zt))
By this approach, we can model continuous features given labels cand a set of sensitive features S. Similarly,
the estimated starting point given candSfor discrete features can be formulated as follows,
¯x0(zt,c,S) =ˆx0(zt) +wg(γ(zt,c) +γ(zt,c,S)) (7)
withγ(zt,c)andγ(zt,c,S)parameterized by ˆx0.
7Under review as submission to TMLR
4.2 Backbone
In this research, we model mixed-type tabular data in latent space following a similar setup in (Rombach
et al., 2022). The tabular data are encoded into latent space and decoded back with multilayer perceptrons.
We choose U-Net (Ronneberger et al., 2015) with transformers as a posterior estimator to predict ¯ϵ(zt,c,S)
and ¯x0(zt,c,S). This choice is based on the hypothesis that the U-Net, augmented by the contextual
understanding capabilities of transformers, can effectively manage the heterogeneous nature of tabular data.
This integration leverages the U-Net’s ability to capture spatial correlation and the transformers’ strengths
in sequence modeling, thereby offering a novel approach to latent space modeling of tabular data.
4.3 Balanced Sampling
After training diffusion models to approximate the distribution of the training data conditioned on labels c
and sensitive features S, we can generate samples from this conditional distribution and align them with any
desired target distribution by adjusting the conditioning variables. Specifically, during sampling, the denois-
ing neural network utilizes guidance from the target label and sensitive attributes. The joint distribution of
the target label and sensitive attributes can be customized. In our bias mitigation framework, we expect the
synthetic data to preserve the same label distribution as the real data, while making the sensitive attributes
nearly independent of the target label. To achieve this, we first compute the empirical label distribution
from the real data and apply uniform categorical distributions for each sensitive feature. This results in a
balanced joint distribution of the target label and sensitive features, which serves as input for the conditional
generation in our diffusion model. Then our diffusion model iteratively denoises the Gaussian noise with
label and sensitive guidance to generate fair tabular data.
5 Experiments
In this section, we first introduce experimental datasets, data processing, baselines, and how we assess
synthetic tabular data. Then we present computational results on experimental datasets. The training
settings are presented in Appendix B.
5.1 Datasets
We evaluate the effectiveness and fairness of our proposed method using three binary classification tabular
datasets: Adult (Kohavi et al., 1996), Bank Marketing (Moro et al., 2014), and COMPAS (Larson et al.,
2016). These datasets contain numerical and categorical features, including some sensitive attributes with
observedclassimbalancesorstrongcorrelationstothetargetvariable. Weprovideasummaryofthedatasets,
includingtheircorrespondingsensitiveattributesinTable1,andofferfurtherdetailsinAppendixA.Common
sensitive attributes such as sex and race can be found in Adult and COMPAS. As mentioned in (Rajabi &
Garibay, 2022), the age group in the Bank Marketing dataset is considered sensitive, with individuals below
25classifiedas“young”andthoseabove25classifiedas“old”. Inourexperiments, eachdatasetisdividedinto
training, validation, and test sets using a 50/25/25 split. Following the training of our diffusion models with
multivariate guidance, we proceed to generate synthetic data that adheres to the original label distribution
while incorporating a uniform distribution for sensitive features.
Table 1: Experimental datasets.
Dataset Train Validation Test Sensitive Target
Adult 22611 11305 11306 Sex & Race Income
Bank 22605 11303 11303 Age Subscription
COMPAS 8322 4161 4161 Sex & Race Recidivism
8Under review as submission to TMLR
5.2 Data Processing and Baselines
5.2.1 Data Processing
Following the data processing steps in Kotelnikov et al. (2023), we split the numerical and categorical
features for each dataset and pre-process them separately. Numerical features are converted through quantile
transformation, a non-parametric technique transforming variables into a standard Gaussian distribution
based on quantiles, thus normalizing the data and enhancing the robustness to outliers. In preparation
for the Markov transition process in the latent diffusion model, we encode categorical features into one-hot
vectors and concatenate the resulting vectors with normalized numerical features.
5.2.2 Baselines
To evaluate the effectiveness of our approach across different datasets, we perform a comparative analysis
against state-of-the-art (SOTA) baseline models that have publicly available code for tabular data synthesis.
Additionally, we compare our method with fair tabular generative models based on GAN and SMOTE. The
specific baseline methods chosen for comparison are detailed below.
•Fair Class Balancing (FairCB) (Yan et al., 2020): a SMOTE-based oversampling method to
adjust class distributions in the training data to address class imbalances.
•TabFairGAN (FairTGAN) (Rajabi & Garibay, 2022): a fairness-aware GAN tailored for tabular
data incorporates a fair discriminator to ensure equality in the joint probability distribution of
sensitive features and labels.
•CoDi(Lee et al., 2023): a recent diffusion-based approach that combines two diffusion models: one
for continuous variables and another for discrete variables. These models are trained separately
to effectively learn the distributions of their respective data types. However, they are also linked
together by conditioning each model’s sample generation on the other model’s samples at each
time step, which enhances the learning of correlations between continuous and discrete variables.
Additionally, to further strengthen the connection between the two models, contrastive learning is
applied to both diffusion models. Negative samples are created by breaking the inter-correlation
between the continuous and discrete variable sets, thus reinforcing the models’ ability to learn these
relationships.
•GReaT (Borisov et al., 2022): utilizes pre-trained transformer-decoder language models (LLMs) to
generate synthetic tabular data through a two-stage process. In the first stage, it transforms each
row of the real tabular dataset into a textual representation by representing each feature with its
name and value in a subject-predicate-object structure. The transformation of these features is then
concatenated in a random order to form the feature vector of the corresponding row. The random
orderingenablestheLLMtobefine-tunedonsampleswithoutdependingonthesequenceoffeatures,
allowing for arbitrary conditioning during tabular data generation. This textual representation of
thetabulardatasetisusedtofine-tunethepre-trainedLLM.Inthesecondstage, thefine-tunedLLM
generates synthetic data by initializing it with any combination of feature names or feature name-
valuepairs, allowingthemodeltocompletetheremainingfeaturevectorinitstextualrepresentation,
which can then be converted back into tabular format.
•SMOTE (Chawla et al., 2002): a robust interpolation-based method that creates synthetic data
points by combining a real data point with its knearest neighbors from the dataset. In our appli-
cation, we leverage SMOTE to generate additional tabular data samples, employing interpolation
among existing samples with the same label. The implementation of SMOTE is available in Lemaître
et al. (2017).
•STaSy(Kim et al., 2022): a Score-based Tabular Data Synthesis (STaSy) method that adopts the
score-based generative modeling approach. It introduces a self-paced learning method that begins
by training on simpler records with a denoising score matching loss below a certain threshold. As
9Under review as submission to TMLR
training progresses and the model becomes more robust, progressively more challenging records are
included in the training. Additionally, STaSy includes a fine-tuning approach that adjusts the model
parameters by iteratively retraining it using the log probabilities of the generated samples.
•TabDDPM (Kotelnikov et al., 2023): a recent diffusion-based model that has demonstrated to
surpass existing methods for synthesizing tabular data. Leveraging multilayer perceptrons as its
backbone, TabDDPM excels in learning unbalanced distributions in the training data.
•TabSyn (Zhang et al., 2023): a SOTA method that utilizes a latent diffusion model to generate
tabular data containing both numerical and categorical features. First, a VAE model, specifically
designed for tabular-structured data, converts raw tabular data with both numerical and categorical
features into embeddings in a regularized latent space that has a distribution close to the standard
normal distribution. Following this, a score-based diffusion model is trained in the embedding space
to capture the distribution of these latent embeddings. The diffusion model adds Gaussian noise
that increases linearly over time in the forward process, which minimizes approximation errors in
the reverse process, thus allowing for synthetic data to be generated in significantly fewer reverse
steps.
5.3 Assessing Synthetic Tabular Data
There are two primary approaches to evaluating synthetic tabular data. One approach is a model-based
method that involves training machine learning models on synthetic tabular data and evaluating the trained
models on validation data extracted from the original dataset. The other approach is a data-based method
that operates independently of machine learning models and directly compares synthetic and real tabular
data.
5.3.1 Data-Based Evaluation
We evaluate synthetic tabular data by directly comparing it with real data, with a focus on column-wise
density estimation and pair-wise column correlations. To address privacy concerns when sharing tabular
data publicly, synthetic data should not replicate the original data. As suggested by (Zhang et al., 2023), we
compute“distance to the closest record” (DCR) scores to ensure this. Furthermore, when assessing
the fairness of synthetic tabular data using data-based methods, we analyze the class imbalances within
sensitive features like sex or race, as well as the joint distribution with the target label.
5.3.2 Model-Based Evaluation
The model-based evaluation process for synthetic tabular data in classification tasks involves the following
steps: first, training a generative model on the original training set; next, generating synthetic data using
the trained generative model; then, training classifiers on the synthetic data; and finally, evaluating these
classifiers on the original test data. Specifically, when assessing model accuracy on the original test data, it
is referred to as machine learning efficiency as used in (Choi et al., 2017).
Beyond accuracy assessments, the evaluation of synthetic data can extend to fairness scores of classifiers,
providing insights into the fairness of the generated data. We evaluate the fairness of machine learning
models trained on the synthetic data, focusing on two key aspects: equal allocation and equal performance
(Agarwal et al., 2018).
•Equal allocation, a fundamental principle in model-based fairness evaluations, entails the idea that
a model should distribute resources or opportunities proportionally among different groups, irre-
spective of their affiliation with any privileged group. Equal allocation is quantified using the
demographic parity ratio (DPR) . This ratio reveals the extent of the imbalance by comparing
the lowest and highest selection rates (the proportion of examples predicted as positive) between
groups.
•Equal performance is grounded in the principle that a fair model should exhibit consistent perfor-
mance across all groups. This entails maintaining the same precision level for each group. The
10Under review as submission to TMLR
equalized odds ratio (EOR) , which represents the smaller of two ratios comparing true and false
positive rates between groups, serves as a metric for assessing performance fairness in this context.
Both the demographic parity ratio and equalized odds ratio are within the range [0,1]. The demographic
parity ratio highlights the importance of distributing resources equally among different groups, whereas the
equalized odds ratio prioritizes impartial decision-making within each specific group. Higher values in either
metric indicate progress toward fairness. In our experiments, when evaluating the fairness scores of classifiers
as shown in Table 4, we evaluate the fairness scores of classifiers by selecting sex as the sensitive attribute
for the Adult and COMPAS datasets, and age group for the Bank dataset.
5.4 Computational Results
This section presents the computational results of our evaluation of synthetic tabular data generated with
three different random seeds. We trained the SOTA machine learning model for tabular data, CatBoost, on
three different sets of synthetic data, each generated with a unique random seed.
5.4.1 Fidelity and Diversity of Synthetic Data
Table 2: The values of error rates of column-wise density estimation and pair-wise column correlations.
Adult Bank COMPAS Mean Adult Bank COMPAS Mean
Density (↓) % Correlation (↓) %
CoDi .145±.000.154±.000.205±.001 16.8%.495±.000.344±.001.550±.001 46.3%
GReaT .077±.000.102±.001.093±.001 9.1%.208±.015.213±.010.165±.016 19.5%
SMOTE .025±.000.020±.000.021±.001 2.2%.054±.004.042±.005.047±.005 4.8%
STaSy .102±.000.182±.000.108±.001 13.1%.163±.000.221±.001.138±.001 17.4%
TabDDPM .037±.001.028±.001.057±.001 4.1%.055±.001.052±.001.090±.001 6.6%
TabSyn .010±.000.009±.000.027±.001 1.5%.035±.001.033±.002.054±.001 4.1%
FairCB .076±.000.066±.000.039±.000 6.0%.125±.000.111±.000.074±.000 10.3%
FairTGAN .034±.000.030±.000.055±.001 4.0%.080±.001.053±.000.087±.001 7.3%
Ours .126±.001.121±.000.109±.001 11.9%.201±.000.174±.005.174±.003 18.3%
Table 3: The values of machine learning efficiency with the best classifier for each dataset and DCR scores.
Adult Bank COMPAS Mean Adult Bank COMPAS Mean
AUC (↑) % DCR Scores (Ideally Around .500) %
Real .928±.000.936±.000.810±.001 89.1% - - - -
CoDi .858±.001.826±.014.678±.002 78.7%.331±.003.348±.001.400±.002 36.0%
Goggle .740±.002.737±.006.650±.009 70.9%.336±.002.354±.001.356±.003 34.9%
GReaT .901±.002.688±.024.717±.008 76.9%.320±.002.348±.003.377±.003 34.8%
SMOTE .914±.000.928±.001.778±.002 87.3%.327±.004.265±.001.273±.003 28.8%
STaSy .885±.003.895±.003.728±.013 83.6%.344±.001.345±.002.362±.001 35.0%
TabDDPM .907±.001.917±.002.745±.001 85.6%.339±.000.350±.002.367±.005 35.2%
TabSyn .911±.000.919±.000.749±.001 86.0%.339±.002.351±.005.367±.006 35.2%
FairCB .915±.001.907±.002.771±.001 86.4%.054±.000.031±.001.012±.001 3.2%
FairTGAN .881±.000.863±.006.705±.002 81.6%.348±.002.348±.004.374±.006 35.7%
Ours .893±.002.914±.002.734±.001 84.7%.344±.001.350±.002.370±.004 35.5%
Table 2 and Table 3 show that our method remains competitive with SOTA models in terms of column-wise
density estimation, pair-wise column correlations, and machine learning efficiency, even after incorporating
a uniformly distributed direction for sensitive features during the sampling phase. Across the three exper-
imental datasets, our model achieved an average AUC of 84.7%, which is only 1.3%lower than TabSyn,
the SOTA deep learning tabular synthesis method. Although altering the original sensitive distribution is
anticipated to reduce AUC, the results demonstrate that our method effectively preserves high fidelity in
the generated synthetic data. While SMOTE and FairCB excel in fidelity, their low DCR scores suggest
that they closely mimic the real data, which is expected given their direct interpolation of the original data
points.
11Under review as submission to TMLR
5.4.2 Fairness Scores in Classification Tasks
Table 4: The values of DPR and EOR with best classifier for each dataset.
Adult Bank COMPAS Mean Adult Bank COMPAS Mean
DPR (↑) % EOR (↑) %
Real .309±.001.402±.015.675±.006 46.2%.193±.005.367±.024.645±.015 40.2%
CoDi .293±.034.189±.054.855±.025 44.6%.247±.042.172±.063.857±.031 42.5%
GReaT .249±.015.572±.289.624±.066 48.2%.155±.028.380±.126.543±.075 35.9%
SMOTE .321±.006.405±.028.648±.021 45.8%.254±.011.381±.026.589±.009 40.8%
STaSy .261±.045.468±.123.436±.092 38.8%.182±.069.451±.113.433±.140 35.5%
TabDDPM .261±.006.337±.020.558±.041 38.5%.156±.007.334±.028.540±.047 34.3%
TabSyn .281±.017.336±.016.697±.040 43.8%.178±.019.317±.023.664±.062 38.6%
FairCB .286±.002.719±.005.675±.006 56.0%.192±.003.801±.016.638±.021 54.4%
FairTGAN .554±.006.338±.093.448±.025 44.7%.697±.017.158±.033.392±.041 41.6%
Ours .543±.026.710±.057.800±.080 68.4%.667±.059.649±.040.825±.113 71.4%
As shown in Table 4, our method significantly outperforms all baseline models, achieving a mean DPR of
68.4%and a mean EOR of 71.4%. In comparison, state-of-the-art tabular generative models and FairTGAN
fall below 50%for both fairness metrics, while FairCB remains below 60%for both. From Table 3, our
method achieves an average AUC of 84.7%, which is higher than FairTGAN and only 1.6% lower than
FairCB. By jointly optimizing performance and fairness, our method presents a more balanced trade-off than
existing approaches. Moreover, in Appendix E.1, we demonstrate that simply overwriting the distribution
of sensitive attributes is insufficient to mitigate bias in downstream classification tasks and can lead to a
decrease in classification performance.
5.4.3 Sensitive Feature Distributions
In addition to evaluating the fairness of machine learning models, we also examine the class distribution of
sensitive attributes in synthetic data. To achieve this, we use stacked bar plots to visualize the contingency
tables for sensitive features and the target label. We further compare the distribution of sensitive attributes
between real and synthetic data using bar plots.
<=50K >50K
Class0.00.20.40.6PercentageSex
Female
Male
<=50K >50K
Class0.00.20.40.6PercentageRace
Amer-Indian-Eskimo
Asian-Pac-Islander
Black
Other
White(Ours) sensitive attributes vs target label
on the Adult dataset
Figure 2: The distribution of sensitive attributes across different target label values on the Adult dataset
using our method.
We present the distribution of sensitive attributes versus the target label on the Adult dataset using our
methodinFigure2. Thesyntheticdatageneratedbyourapproachdemonstratesabalancedjointdistribution
between sensitive features and the target label, effectively reducing the disparities observed in the original
data. Additionally, we provide plots of the real data distribution and synthetic distributions generated by
other methods in Appendix D. Note that we only visualize the synthetic data generated by our method,
FairTGAN, and FairCB, which are three fairness-aware generative models, because we assume that the other
baseline models replicate the real data distribution. In comparison, our method effectively produces a more
12Under review as submission to TMLR
balanced joint distribution of sensitive attributes and target labels than the FairTGAN, FairCB, and the
real distribution.
Sensitive Attribute Distribution in Real and Synthetic Data with Our Method
0.00.20.40.60.8Percentage0.671
0.3290.861
0.093
0.027 0.011 0.008Adult Real
Sex
Race
MaleFemale White Black
Asian-Pac-Islander Amer-Indian-EskimoOther0.00.10.20.30.40.5Percentage0.497 0.503
0.209 0.204 0.199 0.190 0.198Ours0.00.20.40.60.81.0Percentage0.971
0.029Bank Real
Age-Group
OldYoung0.00.10.20.30.40.5Percentage0.501 0.499Ours0.00.20.40.60.8Percentage0.821
0.1790.544
0.327
0.0760.0440.005 0.003COMPAS Real
Sex
Race
MaleFemale
African-AmericanCaucasian HispanicOther Asian
Native American0.00.10.20.30.40.5Percentage0.527
0.473
0.171 0.175 0.174 0.1640.1460.170Ours
Figure 3: Comparison in the real versus synthetic distribution of sensitive attributes across all datasets.
5.4.4 Performance-Fairness Trade-Off
Balancing fairness and predictive accuracy is a key challenge in synthetic data generation, particularly when
fairness improvements might impact important outcomes. This section explores how adjustments in fairness
levels influence both performance and fairness metrics in the generated data. Our method provides a way
to control this trade-off, allows practitioners to find a balance that meets their specific needs, whether they
prioritize fairness, accuracy, or a compromise between the two.
To analyze the trade-off between fairness and accuracy, we define a balancing level iranging from 0to10.
This level quantifies how much we adjust the original joint distribution of the target and sensitive attributes
to make it more uniform.
Suppose we have Nsensitive attributes, each attribute with Ciunique values, where iranges from 1 to
N. This results in Kcombinations of values where K=Ci×···×CN. Under the same targe label, each
combinationhasaprobability ykwherekisoneofthe Kcombinations. Foreachcombination, wecalculatean
offsetdk= ¯y−yk, the difference between the average probability ¯y=/summationtext
kyk/|y|and the current probability
yk. Theoffset dkrepresentshowmuchthecurrentjointprobabilitydeviation ykforacombinationofsensitive
attributes and the target label deviate from the average probability ¯y. By identifying these deviations, we
can measure how far the joint distribution is from a uniform distribution, which is often desired for achieving
fairness. For a balancing level i∈[0,10], the balancing ratio is i/10, and the adjusted values ybalanced
kare
calculated as:
ybalanced
k =yk+dk×i
10(8)
The balanced values ybalanced
krepresent the adjusted probabilities for each combination of sensitive attributes
and the target label after applying a fairness transformation. The balancing process ensures that ybalanced
k
systematically reduces disparities in the joint distribution of sensitive attributes and target labels, moving
it closer to uniformity as the balancing level iincreases. By choosing an appropriate balancing level, the
method allows for a practical balance between performance and fairness.
The results presented in the previous sections are based on a balancing level of 10, which achieves a nearly
uniform joint distribution of the target and sensitive attributes. We conducted experiments to assess how
different balancing levels affect the performance-fairness trade-off. We evaluated metrics such as AUC,
DPR, and EOR, with DPR and EOR averaged across all sensitive attributes. We computed a composite
score as a weighted sum of these metrics, with weights of 0.5for AUC, 0.25for DPR, and 0.25for EOR.
Figure 4 illustrates the trade-off between fairness and performance across different balancing levels. The
13Under review as submission to TMLR
optimal composite scores for the Adult and Bank datasets are achieved at a balancing level of 10, indicating
a uniform distribution. For the COMPAS dataset, the best score is at level 9, representing a nearly uniform
distribution. Practitioners can adjust the metric weights to suit specific application needs or establish
thresholds for individual metrics based on their requirements.
0 1 2 3 4 5 6 7 8 910
Balancing Level0.20.40.60.8Value0.619Adult
AUC
DPR
EOR
SCORE
0 1 2 3 4 5 6 7 8 910
Balancing Level0.30.40.50.60.7Value0.655COMPAS
AUC
DPR
EOR
SCORE
0 1 2 3 4 5 6 7 8 910
Balancing Level0.20.30.40.50.60.70.80.9Value0.784Bank Marketing
AUC
DPR
EOR
SCOREPerformance-Fairness Trade-Off for Experimental Datasets
Figure 4: Trade-off between performance and fairness across balancing levels for experimental datasets.
SCORE was computed as a weighted sum of these metrics, with weights of 0.5 for AUC, 0.25 for DPR, and
0.25 for EOR. The best composite scores for the Adult and Bank datasets are achieved at a balancing level
of 10, while the COMPAS dataset achieves the best score at level 9.
6 Limitations and Discussion
Our proposed method is designed to generate balanced synthetic tabular data while considering sensitive
attributes. However, it requires sensitive features to be specified in advance, which can be challenging in
large-scale enterprise datasets with thousands of features. Furthermore, we could extend our testing to
datasets with more than two sensitive attributes and imbalanced target distributions.
In terms of efficiency, we use a U-Net with attention layers as the backbone neural network for the posterior
estimator in the diffusion framework, but this approach is more time-consuming compared to TabSyn. In
the future, we aim to explore methods to reduce computational costs.
Lastly, our proposed method is designed to condition on specified group labels, which limits its applicability
in scenarios where demographic information is unavailable or restricted due to privacy concerns. Future
iterations of the model could integrate privacy-preserving techniques, such as synthetic attribute imputation,
to enable fairness-aware data generation without requiring explicit sensitive attribute disclosure.
On the other hand, our proposed method generates fair data with a balanced joint distribution of the target
label and multiple sensitive attributes, achieving higher fairness scores than baseline methods. Additionally,
the model shows strong potential for extending to multimodal data synthesis. If we can improve the efficiency
of our model and successfully adapt it to multimodal scenarios, we believe it could be widely applied to
address fairness issues in fields such as healthcare, finance, and beyond.
7 Conclusion
In this work, we propose a novel diffusion model framework for mixed-type tabular data conditioned on
both outcome and sensitive feature variables. Our approach leverages a multivariate guidance mechanism
and performs balanced sampling considering sensitive features while ensuring a fair representation of the
generateddata. Extensiveexperimentsonreal-worlddatasetscontainingsensitivedemographicsdemonstrate
that our model achieves competitive performance and superior fairness compared to existing baselines.
14Under review as submission to TMLR
Impact Statements
The objective of this study is to make progress in the area of fair machine learning. Tabular datasets
sometimes contain inherent bias, such as imbalanced distributions in sensitive attributes. Training machine
learning models on biased datasets may result in decisions that could negatively affect minority groups. By
mitigatingbiasespresentintabulardatasetsthroughthegenerationofequitablesyntheticdata, ourapproach
contributes to fostering equitable decision-making processes across industries such as finance, healthcare,
and employment. Moreover, in instances where sharing datasets becomes necessary, the utilization of fair
synthetic data ensures the preservation of user privacy and avoids causing emotional distress to minority
groups. An adverse possibility is that bad individuals could manipulate distributions in sensitive attributes
to generate biased (even stronger than original) data to harm minority groups. Additionally, the widespread
release of synthetic data can diminish the quality of data sources available on the internet. Repeatedly
training generative models on synthetic data in a self-consuming loop can lead to a decline in the quality of
data synthesis, as discussed in (Alemohammad et al., 2023).
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. A reductions
approach to fair classification. In International conference on machine learning , pp. 60–69. PMLR, 2018.
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-
generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD interna-
tional conference on knowledge discovery & data mining , pp. 2623–2631, 2019.
SinaAlemohammad, JosueCasco-Rodriguez, LorenzoLuzi, AhmedImtiazHumayun, HosseinBabaei, Daniel
LeJeune,AliSiahkoohi,andRichardGBaraniuk. Self-consuminggenerativemodelsgomad. arXiv preprint
arXiv:2307.01850 , 2023.
Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured
denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems ,
34:17981–17993, 2021.
Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models
are realistic tabular data generators. arXiv preprint arXiv:2210.06280 , 2022.
Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varsh-
ney. Optimized pre-processing for discrimination prevention. Advances in neural information processing
systems, 30, 2017.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority
over-sampling technique. Journal of artificial intelligence research , 16:321–357, 2002.
Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972 ,
2023.
Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F Stewart, and Jimeng Sun. Generat-
ing multi-label discrete patient records using generative adversarial networks. In Machine learning for
healthcare conference , pp. 286–305. PMLR, 2017.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural
information processing systems , 34:8780–8794, 2021.
Felix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha Luccioni,
and Kristian Kersting. Fair diffusion: Instructing text-to-image generation models on fairness. arXiv
preprint arXiv:2302.10893 , 2023.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in
neural information processing systems , 29, 2016.
15Under review as submission to TMLR
Huan He, Shifan Zhao, Yuanzhe Xi, and Joyce C Ho. Meddiff: Generating electronic health records using
accelerated denoising diffusion model. arXiv preprint arXiv:2302.04355 , 2023.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 , 2022.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural
information processing systems , 33:6840–6851, 2020.
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and
multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing
Systems, 34:12454–12465, 2021.
Sérgio Jesus, José Pombal, Duarte Alves, André Cruz, Pedro Saleiro, Rita Ribeiro, João Gama, and Pedro
Bizarro. Turning the tables: Biased, imbalanced, dynamic tabular datasets for ml evaluation. Advances
in Neural Information Processing Systems , 35:33563–33575, 2022.
Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination.
Knowledge and information systems , 33(1):1–33, 2012.
Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regularization
approach. In 2011 IEEE 11th International Conference on Data Mining Workshops , pp. 643–650. IEEE,
2011.
ToshihiroKamishima, ShotaroAkaho, HidekiAsoh, andJunSakuma. Fairness-awareclassifierwithprejudice
remover regularizer. In Machine Learning and Knowledge Discovery in Databases: European Conference,
ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23 , pp. 35–50. Springer,
2012.
Jayoung Kim, Chaejeong Lee, and Noseong Park. Stasy: Score-based tabular data synthesis. arXiv preprint
arXiv:2210.04018 , 2022.
Ron Kohavi et al. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Kdd,
volume 96, pp. 202–207, 1996.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion
model for audio synthesis. arXiv preprint arXiv:2009.09761 , 2020.
Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular
data with diffusion models. In International Conference on Machine Learning , pp. 17564–17579. PMLR,
2023.
Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. How we ana-
lyzed the compas recidivism algorithm. https://www.propublica.org/article/
how-we-analyzed-the-compas-recidivism-algorithm , 2016. Accessed: 2024-12-22.
Tai Le Quy, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, and Eirini Ntoutsi. A survey on datasets for
fairness-aware machine learning. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery ,
12(3):e1452, 2022.
Chaejeong Lee, Jayoung Kim, and Noseong Park. Codi: Co-evolving contrastive diffusion models for mixed-
type tabular synthesis. In International Conference on Machine Learning , pp. 18940–18956. PMLR, 2023.
Guillaume Lemaître, Fernando Nogueira, and Christos K Aridas. Imbalanced-learn: A python toolbox to
tackle the curse of imbalanced datasets in machine learning. Journal of machine learning research , 18(17):
1–5, 2017.
Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. Generative time series forecasting with diffusion,
denoise, and disentanglement. Advances in Neural Information Processing Systems , 35:23009–23022, 2022.
16Under review as submission to TMLR
ChaoMa, SebastianTschiatschek, RichardTurner, JoséMiguelHernández-Lobato, andChengZhang. Vaem:
a deep generative model for heterogeneous mixed type data. Advances in Neural Information Processing
Systems, 33:11237–11247, 2020.
François Mazé and Faez Ahmed. Diffusion models beat gans on topology optimization. In Proceedings of
the AAAI Conference on Artificial Intelligence (AAAI), Washington, DC , 2023.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on
bias and fairness in machine learning. ACM computing surveys (CSUR) , 54(6):1–35, 2021.
Sérgio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of bank telemar-
keting.Decision Support Systems , 62:22–31, 2014.
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided
diffusion models. arXiv preprint arXiv:2112.10741 , 2021.
Dana Pessach and Erez Shmueli. A review on fairness in machine learning. ACM Computing Surveys
(CSUR), 55(3):1–44, 2022.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and cali-
bration. Advances in neural information processing systems , 30, 2017.
Amirarsalan Rajabi and Ozlem Ozmen Garibay. Tabfairgan: Fair tabular data generation with generative
adversarial networks. Machine Learning and Knowledge Extraction , 4(2):488–501, 2022.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3, 2022.
Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion
models for multivariate probabilistic time series forecasting. In International Conference on Machine
Learning , pp. 8857–8868. PMLR, 2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pp. 10684–10695, 2022.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th
International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pp. 234–241.
Springer, 2015.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural Information Processing
Systems, 35:36479–36494, 2022.
Timur Sattarov, Marco Schreyer, and Damian Borth. Findiff: Diffusion models for financial tabular data
generation. In Proceedings of the Fourth ACM International Conference on AI in Finance , pp. 64–72,
2023.
Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffusion: Miti-
gating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 22522–22531, 2023.
Md Fahim Sikder, Daniel de Leng, and Fredrik Heintz. Fair4free: Generating high-fidelity fair synthetic
samples using data free distillation. arXiv preprint arXiv:2410.01423 , 2024.
17Under review as submission to TMLR
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In International conference on machine learning , pp. 2256–2265.
PMLR, 2015.
Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zieba, Stavros Petridis, and Maja Pantic.
Diffused heads: Diffusion models beat gans on talking-face generation. arXiv preprint arXiv:2301.03396 ,
2023.
Boris Van Breugel, Trent Kyono, Jeroen Berrevoets, and Mihaela Van der Schaar. Decaf: Generating
fair synthetic data using causally-aware generative networks. Advances in Neural Information Processing
Systems, 34:22221–22233, 2021.
Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. Fairgan: Fairness-aware generative adversarial
networks. In 2018 IEEE International Conference on Big Data (Big Data) , pp. 570–575. IEEE, 2018.
Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data
using conditional gan. Advances in neural information processing systems , 32, 2019.
Shen Yan, Hsien-te Kao, and Emilio Ferrara. Fair class balancing: Enhancing model fairness without
observing sensitive attributes. In Proceedings of the 29th ACM International Conference on Information
& Knowledge Management , pp. 1715–1724, 2020.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness con-
straints: Mechanisms for fair classification. In Artificial intelligence and statistics , pp. 962–970. PMLR,
2017.
Khadija Zanna, Kusha Sridhar, Han Yu, and Akane Sano. Bias reducing multitask learning on mental health
prediction. In 2022 10th International Conference on Affective Computing and Intelligent Interaction
(ACII), pp. 1–8. IEEE, 2022.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial
learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society , pp. 335–340,
2018.
Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen, Xiao Qin, Christos Faloutsos,
Huzefa Rangwala, and George Karypis. Mixed-type tabular data synthesis with score-based diffusion in
latent space. arXiv preprint arXiv:2310.09656 , 2023.
18Under review as submission to TMLR
A Details about Datasets
A.1 Adult
The Adult dataset (Kohavi et al., 1996) is widely used as a benchmark for exploring fairness and bias
in machine learning. It contains 48842 data points. Each data point has 14 attributes and a binary target
variableindicatingwhetheranindividualearnsoverfiftythousanddollarsannually. Thedatasetencompasses
employment, education, and demographic information, and sensitive features are sex and race. Download it
from OpenML.
A.2 Bank Marketing
TheBank Marketingdataset (Moroet al.,2014) iscollectedfrom directmarketing campaignsofa Portuguese
banking institution. Comprising 45211 instances with 16 features, the predicted outcome is a binary variable
indicating whether a client subscribed to a term deposit. Demographic, economic, and past marketing
campaign data are included in features, and marital status is sensitive. Download it from OpenML.
A.3 COMPAS
TheCOMPAS(CorrectionalOffenderManagementProfilingforAlternativeSanctions)dataset(Larsonetal.,
2016) is derived from a risk assessment tool used in the criminal justice system to evaluate the likelihood of
recidivism among criminal defendants. It includes data on individuals arrested in Broward County, Florida,
with 8 features and a binary outcome indicating whether the individual was predicted to reoffend. Features
encompass demographic information, criminal history, and COMPAS risk scores, with sex and race being
sensitive attributes. Download it from OpenML.
B Reproducibility
We performed our experiments on Ubuntu 20.04.2 LTS, utilizing Python version 3.10.14. Our framework of
choice was PyTorch 2.3.0, with CUDA version 12.1, and we ran our computations on an NVIDIA GeForce
RTX 3090. The hyperparameters we used for our method are listed as follows. The variable names can be
found in our attached code. We tune the hyperparameters using Optuna Akiba et al. (2019) to optimize the
validation AUC.
Table 5: The values of hyperparameters for our method.
Notation Variable Meaning Value/Search Space
- batch_size Batch size 256
δ warmup_steps Warm-up timestep 0
ωs cond_guid_weight Sensitive guidance weight 1.0
λ cond_guid_threshold Threshold of the “security gate“ 1.0
ωm cond_momentum_weight Momentum weight 0.5
β cond_momentum_beta Momentum correction factor 0.5
ωg overall_guid_weight Guidance weight 1.0
T n_timesteps Number of timesteps in the diffusion process {100,1000}
- n_epochs Number of training epochs {100,500,1000}
- lr Learning rate [0.00001,0.003]
For deep learning-based methods, we use the original hyperparameter settings provided in their respective
implementations and mainly fine-tune the number of epochs, learning rate, and number of diffusion timesteps
(if applicable) to optimize performance. For SMOTE-based methods, we adjust the number of nearest
neighbors to achieve better results. We have detailed the hyperparameter search spaces for all baseline
methods in Table 6.
19Under review as submission to TMLR
Table 6: The values of hyperparameters for baseline methods.
Method Variable Meaning Value/Search Space
CoDitotal_epochs_both Number of training epochs {100,500,1000,3000}
n_timesteps Number of timesteps in the diffusion process {100,1000}
GReaTbatch_size Batch size {4,8}
n_epochs Number of training epochs {5,10,20}
SMOTE knn Number of nearest neighbors for interpolation {2,21}
STaSylr Learning rate [0.00001,0.003]
n_epochs Number of training epochs {100,500,1000}
TabDDPMlr Learning rate [0.00001,0.003]
n_epochs Number of training epochs {100,500,1000}
num_timesteps Number of timesteps in the diffusion process {100,1000}
TabSynlr Learning rate [0.001,0.002]
n_epochs Number of training epochs {4000}
FairCB knn Number of nearest neighbors for interpolation {2,21}
FairTGANlr Learning rate [0.00001,0.003]
n_epochs Number of training epochs {100,500,1000}
fair_epochs Number of fairness-related training epochs {100,500}
C Efficiency
Table 7 provides a summary of the number of parameters, training time, and sampling time for various
tabular data synthesis methods. Our proposed method is moderately sized, with 482,545 parameters, which
is larger than TabDDPM but significantly smaller than most deep learning-based models, except FairTGAN.
In terms of training efficiency, our method achieves a training time of 11.8 seconds per epoch, which is
comparable to CoDi (10.5 seconds per epoch). However, the sampling time is a significant limitation, as
it takes approximately 1,200 seconds to generate a synthetic dataset of the same size as the training data,
making it substantially slower than all other baseline methods.
Table 7: Comparison of training and sampling efficiency across methods
Method Number of Parameters Training Time (s/epoch) Sampling Time (s)
CoDi 2206711 10.5 9.1
GReaT 81912576 725.5 236.3
SMOTE - 51.1 17.0
STaSy 10653326 1.4 25.1
TabDDPM 109672 6.0 118.9
TabSyn 10575912 0.6 2.7
FairCB - 64.3 5.4
FairTGAN 45368 1.9 0.3
Ours 482545 11.8 1213.9
20Under review as submission to TMLR
D Stacked Bar Plots of Contingency Tables
D.1 Adult
<=50K >50K
Class0.00.20.40.6PercentageSex
Female
Male
<=50K >50K
Class0.00.20.40.6PercentageRace
Amer-Indian-Eskimo
Asian-Pac-Islander
Black
Other
White(Ours) sensitive attributes vs target label
on the Adult dataset
<=50K >50K
Class0.00.20.40.6PercentageSex
Female
Male
<=50K >50K
Class0.00.20.40.6PercentageRace
Amer-Indian-Eskimo
Asian-Pac-Islander
Black
Other
White(Real) sensitive attributes vs target label
on the Adult dataset
<=50K >50K
Class0.00.10.20.30.40.5PercentageSex
Female
Male
<=50K >50K
Class0.00.10.20.30.40.5PercentageRace
Amer-Indian-Eskimo
Asian-Pac-Islander
Black
Other
White(FairCB) sensitive attributes vs target label
on the Adult dataset
<=50K >50K
Class0.00.20.40.6PercentageSex
Female
Male
<=50K >50K
Class0.00.20.40.6PercentageRace
Amer-Indian-Eskimo
Asian-Pac-Islander
Black
Other
White(FairTGAN) sensitive attributes vs target label
on the Adult dataset
Figure 5: Comparison of models on the Adult dataset.
21Under review as submission to TMLR
D.2 Bank Marketing
no yes
Y0.00.20.40.60.8PercentageAge-group
old
young(Ours) sensitive attributes vs target label
on the Bank dataset
no yes
Y0.00.20.40.60.8PercentageAge-group
old
young(Real) sensitive attributes vs target label
on the Bank dataset
no yes
Y0.00.10.20.30.40.5PercentageAge-group
old
young(FairCB) sensitive attributes vs target label
on the Bank dataset
no yes
Y0.00.20.40.60.8PercentageAge-group
old
young(FairTGAN) sensitive attributes vs target label
on the Bank dataset
Figure 6: Comparison of models on the Bank Marketing dataset.
22Under review as submission to TMLR
D.3 COMPAS
No Yes
Is_recid0.00.10.20.30.40.5PercentageSex
Female
Male
No Yes
Is_recid0.00.10.20.30.40.5PercentageRace
African-American
Asian
Caucasian
Hispanic
Native American
Other(Ours) sensitive attributes vs target label
on the COMPAS dataset
No Yes
Is_recid0.00.10.20.30.40.5PercentageSex
Female
Male
No Yes
Is_recid0.00.10.20.30.40.5PercentageRace
African-American
Asian
Caucasian
Hispanic
Native American
Other(Real) sensitive attributes vs target label
on the COMPAS dataset
No Yes
Is_recid0.00.10.20.30.40.5PercentageSex
Female
Male
No Yes
Is_recid0.00.10.20.30.40.5PercentageRace
African-American
Asian
Caucasian
Hispanic
Native American
Other(FairCB) sensitive attributes vs target label
on the COMPAS dataset
No Yes
Is_recid0.00.10.20.30.40.5PercentageSex
Female
Male
No Yes
Is_recid0.00.10.20.30.40.5PercentageRace
African-American
Asian
Caucasian
Hispanic
Native American
Other(FairTGAN) sensitive attributes vs target label
on the COMPAS dataset
Figure 7: Comparison of models on the COMPAS dataset.
23Under review as submission to TMLR
E Additional Numerical Results
E.1 Performance and Fairness Metrics with Sensitive Features Replaced by Uniform Distribution
We present performance and fairness of CatBoost trained on datasets with sensitive attribute replaced by
uniform distribution in Table 8 and Table 9 respectively.
Table 8: The values of machine learning efficiency with the best classifier for each dataset. The sensitive
attributes are replaced by uniform distribution.
Adult Bank COMPAS Mean
AUC (↑) %
Real .928±.000.936±.000.799±.000 88.8%
CoDi .858±.002.827±.015.678±.008 78.8%
Goggle .738±.004.729±.005.653±.010 70.7%
GReaT .900±.003.690±.026.714±.010 76.8%
SMOTE .914±.000.928±.001.772±.002 87.1%
STaSy .883±.000.894±.003.722±.014 83.3%
TabDDPM .906±.001.917±.002.745±.002 85.6%
TabSyn .910±.000.918±.001.745±.001 85.8%
FairCB .914±.000.907±.001.765±.001 86.2%
FairTGAN .885±.002.873±.006.703±.002 82.0%
Ours .895±.001.913±.002.732±.001 84.7%
Table 9: The values of DPR and EOR with best classifier for each dataset. The sensitive attributes are
replaced by uniform distribution.
Adult Bank COMPAS Mean Adult Bank COMPAS Mean
DPR (↑) % EOR (↑) %
Real .307±.004.415±.008.790±.005 50.4%.192±.008.368±.008.857±.007 47.2%
CoDi .345±.030.402±.063.901±.011 54.9%.328±.038.556±.125.923±.010 60.2%
Goggle .838±.027.666±.004.854±.094 78.6%.879±.034.715±.007.878±.093 82.4%
GReaT .256±.010.622±.269.814±.024 56.4%.169±.017.405±.079.802±.055 45.9%
SMOTE .331±.017.413±.025.777±.028 50.7%.276±.045.359±.026.826±.041 48.7%
STaSy .304±.072.532±.060.780±.072 53.9%.240±.106.542±.065.739±.073 50.7%
TabDDPM .281±.004.341±.017.750±.031 45.7%.191±.010.339±.010.816±.040 44.9%
TabSyn .286±.005.319±.018.815±.016 47.3%.182±.011.285±.022.847±.016 43.8%
FairCB .305±.005.623±.011.762±.023 56.3%.227±.006.680±.021.824±.023 57.7%
FairTGAN .481±.021.812±.115.692±.026 66.2%.544±.036.567±.169.732±.024 61.4%
Ours .503±.017.690±.085.781±.006 65.8%.580±.027.631±.087.781±.015 66.4%
24