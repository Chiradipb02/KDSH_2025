Under review as submission to TMLR
Understanding and Controlling a Maze-solving Policy Net-
work
Anonymous authors
Paper under double-blind review
Abstract
To understand the goals and goal representations of AI systems, we carefully study a pre-
trained reinforcement learning policy that solves mazes by navigating to a range of target
squares, similarly to how model organisms are studied in biology. We find this network
pursues multiple context-dependent goals, and we further identify circuits within the net-
work that correspond to one of these goals. In particular, we identified eleven channels
that track the location of the goal. By modifying these channels, either with hand-designed
interventions or by combining forward passes, we can partially control the policy. We show
that this network contains redundant, distributed, and retargetable goal representations,
shedding light on the nature of goal-direction in trained policy networks.
1 Introduction
To safely deploy AI systems, we need to be able to predict their behavior. Traditionally, researchers do so
by evaluating how a model behaves across a range of inputs—for example, with model-written evaluations
(Perez et al., 2022), or on static benchmark datasets (Hendrycks et al., 2020; Lin et al., 2021; Liang et al.,
2022). Moreover, practitioners usually align AI systems by specifying good behavior, such as via expert
demonstrations or preference learning (e.g., Christiano et al., 2017; Hussein et al., 2017; Ouyang et al., 2022;
Glaese et al., 2022; Touvron et al., 2023).
However, behavioral analysis and control methods can be misleading. In particular, models may appearto
be aligned with human goals but competently pursue unintended or even harmful goals when deployed. This
behavior is known as goal misgeneralization and has been demonstrated by Shah et al. (2022) and Langosco
et al. (2023). Moreover, it may be dangerous (Ngo, 2022).
In this work, we therefore investigate the internal objectives (i.e., goals) of trained systems. Intuitively, if
we understand the goals of a system, we can better predict the system’s behavior in novel contexts during
deployment. We focus on goals, while AI interpretability (e.g., Elhage et al., 2021; Fan et al., 2021; Zhang
et al., 2021) often pursues a more general understanding of different models.
In particular, we investigate a maze-solving reinforcement learning policy network trained by Langosco et al.
(2023); our work is thus analogous to how model organisms are studied in biology (Ankeny & Leonelli,
2020), a paradigm which is also emerging within AI research (Hubinger et al., 2023; 2024). This network
exhibits goal misgeneralization—it sometimes ignores a given maze’s cheese square in favor of navigating to
the top-right corner, which is where the cheese was placed during training (fig. 1a). Moreover, because the
policy operates in a human-understandable environment, we can easily interpret its actions and underlying
goals. Altogether, this network thus represents an interesting case study.
First, we demonstrate the trained policy network pursues multiple, context-dependent goals (§2.1). In∼5,000
mazes, we examine the policy’s choices at decision squares —maze locations where the policy must choose
betweenthecheese(theintendedgeneralization)andthehistoricallocationofcheeseduringtraining(misgen-
eralization). By using a few features of each maze, we can predict whether the policy network misgeneralizes.
This predictability suggests the policy pursues different goals depending on certain maze conditions.
1Under review as submission to TMLR
We then find internal representations of these goals. We identify eleven residual channels that track the
location of the cheese (fig. 1b; §2.2). We demonstrate that these channels primarily affect the behavior of
the policy through the location of the cheese, rather than other maze factors. This shows there are circuits
in the trained policy network that track this goal. To our knowledge, we are the first to pinpoint internal
goal representations in a trained policy network.
We corroborate these findings by showing we can steer the policy without additional training (§3). We
modify the activations either through manual hand-designed edits to the eleven channels, or by combining
the activations corresponding to forward passes. By doing so, we change the policy’s behavior in predictable
ways. Instead of updating the network, we steer the network by interacting with its “internal motivational
api."
Overall, our research clarifies the internal goals and mechanisms in pretrained policy networks. We find that
these systems have a nuanced and context-dependent set of goals that can be partially understood and even
controlled through activation engineering approaches.
Figure 1: Understanding and controlling a maze-solving policy. (a) We examine a maze-solving
policy network that navigates within a maze towards a goal location, marked by cheese. During training, the
cheese was placed in the upper right 5×5corner of the maze— the historical goal location . However, during
deployment, the cheese may be placed anywhere. The white dot shows a decision square where the policy
must choose between navigating to the cheese and the top-right corner. (b) We identify residual channels
whose activations track the location of the cheese. (c) We manually set one of these activations to +5.5.
(d) We retarget the policy. Due to the modified activation during the forward pass, the policy goes to the
location implied by the edited activation.
2 Understanding the Maze-solving Policy Network
We study a maze-solving policy network trained by Langosco et al. (2023). The network is deep, with
3.5mparameters and 15 convolutional layers—see appendix A. The network solves mazes to reach a goal:
the cheese. But it exhibits goal misgeneralization . It sometimes capably pursues an unintended goal at
deployment. In this case, the policy often navigates towards the top-right corner (where the cheese was
placed during training) rather than to the actual cheese (fig. 1a).
During training, the cheese is placed within the top right 5 ×5 corner of each randomly generated maze.
During deployment, the cheese may be anywhere. The mazes are procedurally generated using the Procgen
benchmark (Cobbe et al., 2020). We also consider other policy networks which were pretrained with different
historical cheese regions.
We chose this network because it exhibits goal misgeneralization. Furthermore, the network is large enough
to be challenging for humans to understand. Finally, the maze environment is easy to visualise, and policies
in this environment can be easily understood as making spatial tradeoffs.
Section overview. We focus on understanding the goals and goal representations of the maze-solving
policy network. First, we examine whether we can predict the generalization behavior of the network by
2Under review as submission to TMLR
performing a statistical analysis of the factors that affect the policy’s behavior (§2.1). Following this, we
identify several residual channels within the network that track the location of the cheese (§2.2). We find the
network pursues multiple context-dependent goals, and these goals are internally represented in redundant,
distributed ways.
2.1 Understanding The Maze-Solving Policy Through Behavioral Statistics
In this environment, the training algorithm does not produce a policy that consistently navigates to the
cheese (fig. 2). Specifically, in some mazes, the policy navigates to the cheese, but in other mazes, the same
policy navigates to the historical cheese location.1This suggests the network has the capability to pursue
at least two distinct objectives: (i) navigating to the cheese; and (ii) navigating to the top-right corner. We
now examine whether behavior can be predicted based on environmental factors. If environmental factors
are predictive of the goal pursued by the network, this suggests the goal selected by the network to pursue
is context-dependent, rather than chosen at random.
Maze A
 Maze B
 Maze C
 Maze D
Figure 2: The policy network pursues multiple goals. During training, the cheese was always in the
top right corner of the maze. We show trajectories in four mazes not from the training distribution. In mazes
A and B, the policy ignores the cheese and navigates to the historical goal location (the top-right corner).
However, in mazes C and D, the agent navigates to the cheese.
Experiment details. We now examine whether we can predict whether the policy navigates to the cheese
or the historical cheese location based on maze factors. To do so, we considered 5K mazes where the policy
must choose between these goals at a decision square (marked by white dots in fig. 1; see also fig. 14 in
the appendix). We conducted 10 iterations of train/validation splitting with a validation size of 20%. In
each iteration, we performed ℓ1-regularized logistic regression to predict whether a network navigates to the
cheese for a given environment.
We hypothesized several different environmental factors that may affect the policy’s behavior. However,
we run our primary analysis only with the following features, which had robust effects across the different
analyses: (i) the Euclidean distance from the top-right corner to the cheese; (ii) the step distance from the
decision square to the cheese; and (iii) the Euclidean distance from the decision square to the cheese. See
appendix B for further details and illustration of these features.
Results. Logisticregressiononthesefeaturesachievesanaverageaccuracyof82.4%, substantiallyexceeding
the 71.4% accuracy of always predicting “reaches cheese.” Our three maze features provide substantial
information about the goal the policy pursues, which is evidence that the policy pursues context-dependent
goals. As explored more thoroughly in appendix B, the Euclidean distance from the decision square to the
cheese predicts the network’s behavior, even after controlling for the step distance from the decision square
to the cheese.2This indicates that the network’s goal pursuit is perceptually activated by visual proximity
to cheese.
1In certain mazes (such as fig. 1), the policy doesn’t navigate to the cheese orto the top-right corner.
2These findings are mostly consistent across over a dozen different policy networks trained with different historical cheese
locations (see appendix B).
3Under review as submission to TMLR
12
3
Figure 3: Network channels track the goal location. We show the activations for channel 55 after the
first residual block of the second impalablock. The activations of channel 55 are a 16×16grid. We plot
the activation values for the same maze when the cheese is placed in different locations. (b-d) show that
channel 55 tracks the cheese. See appendix E.1 for more examples.
2.2 Finding Goal-Motivation Circuits in the Maze-Solving Policy Network
We have seen that the policy network pursues multiple, context-dependent goals. The network likely contains
circuits that correspond to these goals. We identify circuits for the goal of navigating towards the cheese
location. Specifically, we find eleven channels about halfway through the network that track the location of
the cheese. We consider the network activations after the the first residual block of the second impalablock
(see fig. 11 in the appendix). At this point of the forward pass, there are 128 separate 16×16channels,
meaning there are 32,768 activations.
First, we find that some of these channels track the location of the cheese. fig. 3 shows the activations of
channel 55 for mazes where the goal is placed in different locations (further examples in appendix E.1). The
positive activations (marked in red) correspond to the location of the cheese. By visual inspection, we found
that 11 out of these 128 channels track the cheese, showing that the goal representation is redundant. We
refer to these 11 channels as the “cheese-tracking” channels.
Suppose these “cheese-tracking channels” do, in fact, track the cheese. Then if we resample their activations
(Chan et al., 2022) from another maze with the cheese in the same location,3this resampling should not
affect the behavior of the network. Moreover, if we resample these activations from a maze where the cheese
is placed in a different location, the network should behave as if the cheese were placed in that location. We
now test this hypothesis.
First, we visually investigate the effect of resampling the activations of the cheese tracking channels from
different mazes (fig. 4; more examples in appendix E.2). Indeed, resampling the activations of these “cheese-
tracking” channels modifies the network of the behavior ifthe activations were sampled from another maze
where the cheese is in a different location. In contrast, resampling the activations from a maze where the
cheese is in the samelocationdoes not modify the behavior. Overall, these findings provide further evidence
that these 11 channels affect the network’s final decision mostly based on the cheese location in the maze.
We measure how frequently resampling the cheese tracking channels changes the most likely action at a deci-
sion square. If these channels mostly affect the network’s behavior based on the cheese location, resampling
these channels from mazes where the cheese is in the same location should only rarely affect the behavior at
a decision square. Moreover, resampling from mazes where the cheese is placed in a different location would
be more likely to affect the decision square behavior.
Across 200 mazes, resampling the cheese-tracking channels from mazes with a different cheese location
changes the most probable action at a decision square in 40% of cases, which is much more than when
3Specifically, we compute the network activations for a different maze (maze B) where the cheese is placed in the same
location as in the original maze (maze A). To “resample the activations”, we replace the relevant network activations when
computing the policy for maze A with the activation values computed using a network forward pass on maze B.
4Under review as submission to TMLR
resampling from mazes with the same cheese location (11%). However, because resampling from mazes
with the same cheese location can sometimes affect the network behavior, this suggests the cheese tracking
channels also (weakly) affect the network behavior through factors other than the location of the cheese.
appendix C.1 provides more evidence that these 11 channels primarily affect behavior by tracking the cheese.
(a) Original Maze(d) Resampling from
red cheese location(c) Resampling from
red cheese location(b) Resampling from
same location
Figure 4: Resampling cheese-tracking activations from different mazes. (a) Unmodified network
behavior. (b) Resampling these activations from other mazes with the same cheese location does not affect
the policy’s behavior. (c) In contrast, if we replace the activations from a maze where the cheese is placed at
a different location, the network behaves as if the cheese were at that location. If the cheese-tracking channel
activations are resampled from a maze where the cheese is close to the historical cheese location, the policy
navigates to the cheese. (d) If the cheese-tracking channel activations are resampled from a maze where the
cheese is far from the historical cheese location, the policy ignores the cheese. Please see appendix E.2 for
more examples.
3 Controlling the Maze-Solving Policy Network
In the previous section, we showed the maze-solving policy pursues multiple, context-dependent goals. More-
over, about halfway through the network, multiple residual channels track the location of the goal. We now
corroborate these findings by leveraging this understanding to design interventions that control the network’s
behavior. Our approach does not require collecting additional data or retraining the network, but instead
utilizes existing circuits. We consider two classes of interventions: (i) manually modifying the activations in
the cheese-tracking channels (§3.1); and (ii) combining activations corresponding to different forward passes
(§3.2).
3.1 Controlling the Policy by Modifying the Cheese Channels
Previously, we identified eleven residual channels whose activations track the location of the cheese in the
maze. If these activations determine network behavior by tracking the cheese location, intuitively, by mod-
ifying the activations in those channels, one should be able to modify the behavior of the policy. We now
show that this is indeed the case.
First, we consider a simple, hand-designed intervention where we directly modify the activations of one of
the cheese-tracking channels. Specifically, we set just one activation in channel 55 to a large positive value
(+5.54; c.f. fig. 1c). We then consider the modified policy whose action probabilities are computed by
completing the network’s forward pass with this modification.
In fig. 5, we show this simple intervention retargets the policy. The network often navigates towards the
region of the maze corresponding to the activation edit. We emphasize that changing just oneactivation
(out of 32,768) drastically affects the behavior of the network. However, it can only partially retarget the
4We considered a range of effect sizes, and manually optimized them on the maze at seed 0.
5Under review as submission to TMLR
(a) Success (b) Success (c) Success (d) Failure
Figure 5: Controlling the maze-solving policy by modifying a single activation. By modifying just
a single network activation, we control where the policy navigates. We set a single activation in channel 55,
one of the cheese-tracking channels, to a large positive value (+5.5; see also fig. 1c). The red dots show the
location corresponding to the activation intervention, computed by linearly mapping the 16×16activation
grid to the 25×25 game grid. (a-c) Successful policy retargeting. This intervention successfully makes the
policy navigate to the red dot (the targeted location) and ignore the cheese in the maze. (d) We cannot
make the policy navigate to arbitrary maze-locations. See appendix E.3 for more examples.
policy. Moreover, just as the trained network sometimes ignores the cheese, we find that the retargeted
network sometimes ignores the edited activation location.
Figure 6:Normalized path probability heatmap. The colour of each maze square shows the normalized
path probability for the path from the starting position in the maze to the square for the unmodified policy.
Retargetability heatmaps. To quantify the impact of our retargeting procedure, we compute the nor-
malized path probability for paths from the starting position in a maze to each square of that maze. This
is the joint probability that the policy navigates directlyto a given square in the maze, normalized by the
path distance. Specifically, we compute the geometric mean of the action probabilities leading to a given
square from the start position (see eq. (2) in appendix D). In particular, for a path of nsteps with constant
per-step action probability, the normalized path probability is independent of n.
We visualise normalized path probability heatmaps for the paths from the initial position in the maze to each
square. For example, fig. 6 reveals that the policy tends to navigate towards the historical cheese location.
The normalized path probabilities are higher at maze squares closer to the path between the bottom-left
and the top-right corners of the maze.
Some locations are more easily steered to. Figure 7c shows the effect of intervening on channel 55
to target each square of the maze. That is, for each square, we retarget the policy to that square with an
activation edit. We then compute the normalized path probability for the path to the target square, given
the modified forward pass. For these experiments, to reduce variance, we removed cheese from the maze. In
appendix D, we plot how retargetability decreases as the target location becomes increasingly far from the
path to the top-right corner.
6Under review as submission to TMLR
Intervening on all 11 channels slightly improves retargetability. Similar to the single-channel
intervention, we set one of the activations of each channel to a positive value (+1.05). Comparing the
heatmaps for this intervention (fig. 7a, b) with the single-channel intervention, this edit slightly increases the
normalized path-probabilities. On 13×13mazes,6the averaged path probability over all legal maze squares
is 0.647 from just modifying channel 55, while modifying all hypothesized cheese-tracking channels boosts
the probability to 0.695.
There are more cheese-tracking circuits. We now compute the normalized path probabilities when
targeting each square of the maze by placing the cheese in the location. If the only cheese-tracking circuits
were related to the cheese-tracking channels we identified, then our activation edits would probably achieve
the same retargetability as if the cheese were placed in that location. However, by actually moving the
cheese around the maze, we achieve even stronger retargetability than do our activation edits (fig. 7d). This
suggests that there are additional unidentified cheese-seeking mechanisms, beyond the 11 channels.
Figure 7:Retargetability heatmaps. The color of each maze square shows the normalized path probability
for the path from the starting position in the maze to the square for the modified policy which targets that
square. We modify the activations of the relevant channels so that they contain a positive value near
the relevant square. The heatmap in (a) shows the base probability that each tile can be retargeted to.
Intervening on a single channel (b) increases retargetability less than intervening on all cheese-tracking
channels (c). However, all retargeting methods we investigated were less effective than directly moving the
cheese to a tile (d), indicating that we did not find all relevant cheese-tracking circuits in the network.
3.2 Controlling the Policy By Combining Forward Passes
Beyond simple manual edits, we can modify the behavior of the policy by combining the activations of
different forward passes of the network. These interventions do not require retraining the policy but instead
leverage existing circuits. Specifically, we design different goal-modifying “steering vectors” (Subramani
et al., 2022). By adding or subtracting these vectors to network activations, we modify the behavior of the
network.
Notation. LetActiv(m,x cheese,xagent)∈R128×16×16be the activations after the first residual block of the
second impalablock of the network (see fig. 11 in the appendix). At this point of the network, there are 128
channels, each of which corresponds to a 16×16grid.7Activis a function of the maze layout m, the position
of the cheese xcheese, and the position of the agent xagent.m∈{0,1}25×25represents whether position in
the maze is filled with a wall or not. Further, let xstart
agentbe the starting position of the agent in a maze.
Reducing cheese-seeking behavior. First, we design a “cheese vector” that weakens the policy’s pur-
suit of cheese. The cheese vector is computed as the difference in activations when the cheese is present
and not present in a given maze. Specifically, we calculate the cheese vector as Activ cheese (m,x cheese ) :=
5We optimized the magnitude of the edit to increase retargetability for both the single-channel and 11-channel interventions.
6Appendix D plots how retargetability decreases with maze size.
7The 11 cheese-tracking channels are also present at this layer.
7Under review as submission to TMLR
Activ(m,x cheese,xstart
agent)−Activ(m,,xstart
agent). For intervention coefficient α∈R,
Activ′(m,x cheese,xagent) :=
Activ(m,x cheese,xagent) +α·Activ cheese (m,x cheese ),(1)
and replace the original activations Activwith the modified activations Activ′. This intervention can be con-
sidered to define a custom bias term at the relevant residual-addition block. Figure 8 shows how subtracting
the cheese vector affects the policy in a single maze.
(a) Decisions before
intervention(c) The actions which
changed(b) Subtracted cheese
vector
Figure 8: Subtracting the cheese vector often appears to make the policy ignore the cheese. We
run a forward pass at each valid maze square sto get action probabilities π(a|s). For each square s, we plot
a “net probability vector” with components x:=π(right|s)−π(left|s)andy:=π(up|s)−π(down|s).
The policy always starts in the bottom left corner. By default, the policy goes to the cheese when near
the cheese, and otherwise goes along a path towards the top-right (although it stops short of the top right
corner).
The quantitative effect of subtracting the cheese vector. We consider 100 mazes and analyse how
this subtraction affects the behavior of the policy on decision squares. Recall that decision squares are the
spots of the maze where the policy must choose to navigate to the cheese or the top right corner.
In fig. 9a, subtracting the cheese vector (i.e., α=−1)8substantially reduces the probability of cheese-
seeking actions. Appendix C.2 shows that subtracting the cheese vector is often equivalent to the network
from perceiving cheese at a given maze location, and that the cheese vector from one maze can transfer
to another maze. However, addingthe cheese vector (i.e., α= +1) does not affect cheese-seeking action
probabilities.
Steering the policy towards the top-right corner. We design a “top-right corner” motivational vector
whose addition increases the probability that the policy navigates towards the top-right corner. We compute
Activ top-right (m,x cheese ) := Activ(m,x cheese,xstart
agent)−Activ(m′,,xstart
agent), wherem′is the original maze now
modified so that the reachable top-right point is higher up (see fig. 28 in the appendix). Figure 10 visualizes
the effect of adding the top-right vector.
In fig. 9b, we analyse the effect of different activation engineering approaches that use Activ top-right. We
find that adding Activ top-right(i.e.,α= +1) increases the probability the policy navigates to the top-right
corner, but surprisingly, subtracting the top-right corner vector does not decrease the probability the policy
navigates to the top-right. Lastly, in our experience, we can simultaneously add the top-right vector and
subtract the cheese vector in order to achieve both effects . We were surprised that these activation vectors
did not “destructively interfere” with each other.
8For both the cheese and top-right vectors, we tried optimizing αbut found that it didn’t make an appreciable difference -
straightforward addition and subtraction worked best.
8Under review as submission to TMLR
0.00.20.40.60.81.0P(Cheese | Decision Square)
(a): Cheese Vector
Original
Added
Subtracted
0.00.20.40.60.81.0P(Top Right | Decision Square)
(b): Top Right Vector
Figure 9: Controlling the policy by combining network forward passes . For 100 mazes, we compute
the decision-square probabilities assigned to the actions which lead to the cheese (a) and to the top-right
corner (b). For example, in (a), a value of 0.75 under “original” indicates that at the decision square of
one maze, the unmodified policy assigns 0.75 probability to the first action which heads towards the cheese.
Subtracting the cheese vector and adding the top-right vector each produce strong effects.
(a) Decisions before
intervention(b) Added top-right
vector(c) The actions which
changed
Figure 10: Adding the “top-right vector" often appears to attract the policy to the top-right
corner. Originally, the policy does not fully navigate to the top-right corner, instead settling towards the
bottom-right. After adding the top-right vector, the policy navigates to the extreme top-right.
Overall, our results demonstrate that we can control the behavior of the policy, albeit imperfectly, by
combining different forward passes of the network. We were surprised, since the network was never trained
to behave coherently under the addition of these “bias terms.”
4 Related Works
Interpretability. Understanding AI has been a longstanding goal (e.g., Gilpin et al., 2018; Rudin et al.,
2022;Zhangetal.,2021;Fanetal.,2021;Hookeretal.,2019, inter alia ). Mechanisticapproaches(Olah,2022;
Elhage et al., 2022) look to understand neural network circuits. Recently, mechanistic interpretability has
helped e.g. understand grokking (Nanda et al., 2023). Lieberum et al. (2023) suggest that these approaches
can scale to large models. Far less interpretability work has been done on reinforcement learning policy
networks (Hilton et al., 2020; Bloom & Colognese, 2023; Rudin et al., 2022), which is our setting. To our
knowledge, we are the first to interpret a non-toy policy network, and to pinpoint goal representations
therein.
Steering network behavior. We intervened on a policy network’s activations to steer its behavior,
considering both hand-designed edits (§3.1) and combining forward passes (§3.2). We did not use extra
training data to do so. In contrast, the most popular approaches for steering AI use training data, by e.g.
9Under review as submission to TMLR
specifying preferences over different behaviors (Christiano et al., 2017; Leike et al., 2018; Ouyang et al., 2022;
Bai et al., 2022b; Rafailov et al., 2023; Bai et al., 2022a) or through expert demonstrations (Ng et al., 2000;
Torabi et al., 2018).
Activationengineering. Ourpolicyinterventions(§3)areexamplesof activation engineering approaches.
This newly-emerging class of techniques re-use existing model capabilities. In general, these approaches can
steer network behavior without behavioral data and add negligible computational overhead. For example,
Subramani et al. (2022); Turner et al. (2023); Li et al. (2023) steer the behavior of language models by
adding in activation vectors. In contrast, our work shows these techniques can steer a reinforcement learning
policy.
Maze navigation and goal representation in biological and artificial networks. Spatial naviga-
tion to a goal, such as food or mates, is an ubiquitous and crucial skill for survival and is a well-studied
phenomenon in neuroscience. Broadly, different types of cells play a role in navigation and path finding,
such as place cells (O’Keefe, 1976), head direction cells (Taube et al., 1990a;b), grid cells (Hafting et al.,
2005), and boundary cells (Lever et al., 2009). See Nyberg et al. (2022) for a thorough review of spatial
goal coding in the brain. Computational models can also exhibit goal representations, such as those found
through a dynamical systems lens in Singh et al. (2023).
Maze navigation is also well-studied among artificial neural networks. Some older work uses handcrafted
neural networks that encode specific environmental information like distance to a goal (Bush et al., 2015), or
manuallyreplicatescertaintypesofcellsfoundinmammalianspatialnavigationsystems(Erdem&Hasselmo,
2012). More recent work does not use such manual methods. Banino et al. (2018) use deep reinforcement
learning agents to solve navigation tasks, finding vector-based goal representations through certain ’grid-like’
representations that develop in the network. Wijmans et al. (2023) also test for goal-like representations
while giving maze-solving networks minimal information. Somewhat similarly, we find vectors which seem
to activate goals in pre-trained deep reinforcement learning models. We use these vectors to effectively steer
the network at inference time.
5 Discussion
Activation steering approaches for controlling a model are well-studied9for different problems, as they
propose a method for customization not based on potentially expensive fine-tuning strategies (Turner et al.,
2023). They have been used for reducing toxicity (Liu et al., 2024), sentiment steering for style transfer
(Konen et al., 2024), and for enhanced LLM red teaming (Wang & Shu, 2023). Activation steering can also
be combined with other techniques such as sparse autoencoders Bricken et al. (2023), to steer a model in a
more interpretable way (Nanda et al., 2024).
We lay the foundation for using these techniques to steer policy networks, especially in cases where safe
navigation and control may be essential. For example, in safety-critical systems such as autonomous vehicles,
explainablereinforcementlearningsystemsareimportantfornotjustsafety, butalsosocialacceptanceofthis
new technology by the broader public (Atakishiyev et al., 2024). If explainability methods can isolate safety
relevant concepts to internal representations, our method can then be used for efficient steering and control
without re-training. Similarly, in robotics applications, instead of requiring modifications to an optimization
objective or training scheme with the intent to build a safer system (García & Shafie, 2020; Pham et al.,
2018), we speculate that activation steering could be used at inference to steer a model towards not only its
goal (as we have tested) but also away from dangerous scenarios. More broadly, activation steering often
works with a few dozen supervised datapoints (Li et al., 2023), which is promising for real-world domains
where data collection is expensive.
9These activation steering techniques were published after the first revision of this work.
10Under review as submission to TMLR
6 Conclusion
We studied the goals and goal representations of a pretrained policy network, following a model organisms
paradigm. We found that this network pursues multiple, context-dependent goals (§2.1). We found 11
channels that track the location of the cheese within each maze (§2.2). By modifying just a singleactivation,
or by adding in simple activation vectors, we steered which goals the policy pursued (§3). Our work shows
the goals of this network are redundant, distributed, and retargetable. In general, policy networks may be
well-understood as pursuing multiple context-dependent goals.
7 Impact Statement
This work makes progress toward understanding and controlling the internal goal representations within
reinforcement learning policy networks. We studied a maze navigation policy that exhibits harmful goal
misgeneralization during deployment and how to steer the policy towards its intended goal.
These techniques could potentially be used by malicious actors to misdirect autonomous systems. However,
with appropriate safeguards, the ability to understand and direct policy networks could also have benefits.
For example, this approach might be used for alignment verification, safer exploration in RL, or correcting
unwanted bias quickly rather than retraining models. By developing theories of the goals and motivations
latent in AI systems, we may be able to build more robust and beneficial systems.
References
Rachel A. Ankeny and Sabina Leonelli. Model organisms . Cambridge University Press, 2020. URL https:
//www.cambridge.org/core/elements/model-organisms/F895B26EAC0373BCA5A138835AC73AEA .
Shahin Atakishiyev, Mohammad Salameh, Hengshuai Yao, and Randy Goebel. Explainable Artificial In-
telligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research
Directions. IEEE Access , 12:101603–101625, 2024. ISSN 2169-3536. doi: 10.1109/ACCESS.2024.3431437.
URL https://ieeexplore.ieee.org/abstract/document/10604830 . Conference Name: IEEE Access.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher
Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie
Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt,
Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby,
Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera
Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac
Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared
Kaplan. Constitutional AI: Harmlessness from AI Feedback, December 2022b. URL http://arxiv.org/
abs/2212.08073 . arXiv:2212.08073 [cs].
Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexan-
der Pritzel, Martin J. Chadwick, Thomas Degris, Joseph Modayil, Greg Wayne, Hubert Soyer, Fabio
Viola, Brian Zhang, Ross Goroshin, Neil Rabinowitz, Razvan Pascanu, Charlie Beattie, Stig Petersen,
Amir Sadik, Stephen Gaffney, Helen King, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, and
Dharshan Kumaran. Vector-based navigation using grid-like representations in artificial agents. Na-
ture, 557(7705):429–433, May 2018. ISSN 1476-4687. doi: 10.1038/s41586-018-0102-6. URL https:
//www.nature.com/articles/s41586-018-0102-6 . Publisher: Nature Publishing Group.
Joseph Bloom and Paul Colognese. Decision Transformer Interpretability, February 2023. URL https:
//www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability .
11Under review as submission to TMLR
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,
Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer,
Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,
Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monose-
manticity: Decomposing language models with dictionary learning. Transformer Circuits Thread , 2023.
https://transformer-circuits.pub/2023/monosemantic-features/index.html.
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language
models without supervision. arXiv preprint arXiv:2212.03827 , 2022.
Daniel Bush, Caswell Barry, Daniel Manson, and Neil Burgess. Using Grid Cells for Navigation. Neuron,
87(3):507–520, August 2015. ISSN 0896-6273. doi: 10.1016/j.neuron.2015.07.006. URL https://www.
sciencedirect.com/science/article/pii/S0896627315006285 .
Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya,
Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing: A method for rigorously
testing interpretability hypotheses. In Alignment Forum , 2022.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforce-
ment learning from human preferences. Advances in neural information processing systems , 30, 2017.
Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging Procedural Genera-
tion to Benchmark Reinforcement Learning, July 2020. URL http://arxiv.org/abs/1912.01588 .
arXiv:1912.01588 [cs, stat].
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,
Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Trans-
former Circuits Thread , 1, 2021.
Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer
ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal
Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-
Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacob-
son, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and
Christopher Olah. Softmax linear units. Transformer Circuits Thread , 2022. https://transformer-
circuits.pub/2022/solu/index.html.
Uğur M. Erdem and Michael Hasselmo. A goal-directed spatial navigation model using forward tra-
jectory planning based on grid cells. European Journal of Neuroscience , 35(6):916–931, 2012. ISSN
1460-9568. doi: 10.1111/j.1460-9568.2012.08015.x. URL https://onlinelibrary.wiley.com/doi/abs/
10.1111/j.1460-9568.2012.08015.x . _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-
9568.2012.08015.x.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad
Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable Distributed
Deep-RL with Importance Weighted Actor-Learner Architectures. In Proceedings of the 35th International
Conference on Machine Learning , pp. 1407–1416. PMLR, July 2018. URL https://proceedings.mlr.
press/v80/espeholt18a.html . ISSN: 2640-3498.
Feng-Lei Fan, Jinjun Xiong, Mengzhou Li, and Ge Wang. On interpretability of artificial neural networks:
A survey. IEEE Transactions on Radiation and Plasma Medical Sciences , 5(6):741–760, 2021.
Javier García and Diogo Shafie. Teaching a humanoid robot to walk faster through Safe Reinforcement
Learning. Engineering Applications of Artificial Intelligence , 88:103360, February 2020. ISSN 0952-1976.
doi: 10.1016/j.engappai.2019.103360. URL https://www.sciencedirect.com/science/article/pii/
S0952197619302921 .
12Under review as submission to TMLR
Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. Explaining
Explanations: An Overview of Interpretability of Machine Learning. In 2018 IEEE 5th International
Conference on Data Science and Advanced Analytics (DSAA) , pp. 80–89, October 2018. doi: 10.1109/
DSAA.2018.00018.
Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via
targeted human judgements. arXiv preprint arXiv:2209.14375 , 2022.
Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I. Moser. Microstructure
of a spatial map in the entorhinal cortex. Nature, 436(7052):801–806, August 2005. ISSN 1476-4687.
doi: 10.1038/nature03721. URL https://www.nature.com/articles/nature03721 . Publisher: Nature
Publishing Group.
DanHendrycks, CollinBurns, StevenBasart, AndyZou, MantasMazeika, DawnSong, andJacobSteinhardt.
Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.
Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah. Understanding rl vision. Distill,
2020. doi: 10.23915/distill.00029. https://distill.pub/2020/understanding-rl-vision.
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretability
methods in deep neural networks. Advances in neural information processing systems , 32, 2019.
Evan Hubinger, Nicholas Schiefer, Carson Denison, and Ethan Perez. Model Organ-
isms of Misalignment: The Case for a New Pillar of Alignment Research. Au-
gust 2023. URL https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/
model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1 .
Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham,
Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan,
Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan,
Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary
Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan
Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, and
Ethan Perez. Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training, January
2024. URL http://arxiv.org/abs/2401.05566 . arXiv:2401.05566 [cs].
Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey
of learning methods. ACM Computing Surveys (CSUR) , 50(2):1–35, 2017.
GarethJames,DanielaWitten,TrevorHastie,RobertTibshirani,etal. An introduction to statistical learning ,
volume 112. Springer, 2013.
Kai Konen, Sophie Jentzsch, Diaoulé Diallo, Peer Schütt, Oliver Bensch, Roxanne El Baff, Dominik Opitz,
and Tobias Hecking. Style Vectors for Steering Generative Large Language Model, February 2024. URL
http://arxiv.org/abs/2402.01618 . arXiv:2402.01618.
Lauro Langosco, Jack Koch, Lee Sharkey, Jacob Pfau, Laurent Orseau, and David Krueger. Goal Misgen-
eralization in Deep Reinforcement Learning, January 2023. URL http://arxiv.org/abs/2105.14111 .
arXiv:2105.14111 [cs].
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent
alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871 , 2018.
C. Lever, S. Burton, A. Jeewajee, J. O’Keefe, and N. Burgess. Boundary vector cells in the subiculum of the
hippocampal formation. Journal of Neuroscience , 29(31):9771–9777, 2009. doi: 10.1523/JNEUROSCI.
1319-09.2009.
13Under review as submission to TMLR
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-Time
Intervention: Eliciting Truthful Answers from a Language Model, July 2023. URL http://arxiv.org/
abs/2306.03341 . arXiv:2306.03341 [cs].
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,
Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv
preprint arXiv:2211.09110 , 2022.
Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, and Vladimir
Mikulik. Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in
Chinchilla, July 2023. URL http://arxiv.org/abs/2307.09458 . arXiv:2307.09458 [cs].
StephanieLin, JacobHilton, andOwainEvans. Truthfulqa: Measuringhowmodelsmimichumanfalsehoods.
arXiv preprint arXiv:2109.07958 , 2021.
Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context Vectors: Making In Context Learning More
Effective and Controllable Through Latent Space Steering, February 2024. URL http://arxiv.org/abs/
2311.06668 . arXiv:2311.06668 [cs].
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for
grokking via mechanistic interpretability, January 2023. URL https://arxiv.org/abs/2301.05217v2 .
Neel Nanda, Arthur Conmy, Lewis Smith, Senthooran Rajamanoharan, Tom Lieberum, János
Kramár, and Vikrant Varma. [Full Post] Progress Update #1 from the GDM Mech In-
terp Team. April 2024. URL https://www.alignmentforum.org/posts/C5KAZQib3bzzpeyrg/
full-post-progress-update-1-from-the-gdm-mech-interp-team .
Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, pp.
2, 2000.
Richard Ngo. The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626 ,
2022.
Nils Nyberg, Éléonore Duvelle, Caswell Barry, and Hugo J. Spiers. Spatial goal coding in the hippocampal
formation. Neuron, 110(3):394–422, February 2022. ISSN 0896-6273. doi: 10.1016/j.neuron.2021.12.012.
URL https://www.cell.com/neuron/abstract/S0896-6273(21)01029-1 . Publisher: Elsevier.
John O’Keefe. Place units in the hippocampus of the freely moving rat. Experimental Neurology , 51
(1):78–109, January 1976. ISSN 0014-4886. doi: 10.1016/0014-4886(76)90055-8. URL https://www.
sciencedirect.com/science/article/pii/0014488676900558 .
Chris Olah. Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases, June 2022.
URL https://www.transformer-circuits.pub/2022/mech-interp-essay/index.html .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.
Ethan Perez, Sam Ringer, Kamil˙ e Lukoši¯ ut˙ e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit,
Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with
model-written evaluations. arXiv preprint arXiv:2212.09251 , 2022.
Tu-HoaPham, GiovanniDeMagistris, andRyukiTachibana. OptLayer-PracticalConstrainedOptimization
for Deep Reinforcement Learning in the Real World. In 2018 IEEE International Conference on Robotics
and Automation (ICRA) , pp. 6236–6243, May 2018. doi: 10.1109/ICRA.2018.8460547. URL https:
//ieeexplore.ieee.org/document/8460547/?arnumber=8460547 . ISSN: 2577-087X.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint
arXiv:2305.18290 , 2023.
14Under review as submission to TMLR
Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable
machine learning: Fundamental principles and 10 grand challenges. Statistics Surveys , 16(none):1–85,
January 2022. ISSN 1935-7516. doi: 10.1214/21-SS133. Publisher: Amer. Statist. Assoc., the Bernoulli
Soc., the Inst. Math. Statist., and the Statist. Soc. Canada.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Opti-
mization Algorithms, August 2017. URL http://arxiv.org/abs/1707.06347 . arXiv:1707.06347 [cs].
Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac
Kenton. Goal misgeneralization: Why correct specifications aren’t enough for correct goals. arXiv preprint
arXiv:2210.01790 , 2022.
Satpreet H. Singh, Floris van Breugel, Rajesh P. N. Rao, and Bingni W. Brunton. Emergent behaviour
and neural dynamics in artificial agents tracking odour plumes. Nature Machine Intelligence , 5(1):58–
70, January 2023. ISSN 2522-5839. doi: 10.1038/s42256-022-00599-w. URL https://www.nature.com/
articles/s42256-022-00599-w . Publisher: Nature Publishing Group.
Nishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting Latent Steering Vectors from
Pretrained Language Models, May 2022. URL http://arxiv.org/abs/2205.05124 . arXiv:2205.05124
[cs].
Jeffrey S. Taube, Robert U. Muller, and James B. Ranck. Head-direction cells recorded from the postsubicu-
luminfreelymovingrats.I.Descriptionandquantitativeanalysis. Journal of Neuroscience , 10(2):420–435,
1990a. URL https://www.jneurosci.org/content/10/2/420.short . Publisher: Soc Neuroscience.
Jeffrey S. Taube, Robert U. Muller, and James B. Ranck. Head-direction cells recorded from the post-
subiculum in freely moving rats. II. Effects of environmental manipulations. Journal of Neuroscience ,
10(2):436–447, 1990b. URL https://www.jneurosci.org/content/10/2/436.short . Publisher: Soc
Neuroscience.
Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral Cloning from Observation, May 2018. URL
http://arxiv.org/abs/1805.01954 . arXiv:1805.01954 [cs].
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288 , 2023.
Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation
Addition: Steering Language Models Without Optimization, August 2023. URL http://arxiv.org/abs/
2308.10248 . arXiv:2308.10248 [cs].
Haoran Wang and Kai Shu. Trojan Activation Attack: Red-Teaming Large Language Models using
Activation Steering for Safety-Alignment, August 2023. URL http://arxiv.org/abs/2311.09433 .
arXiv:2311.09433.
Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S. Morcos, and Dhruv Batra. Emergence of Maps
in the Memories of Blind Navigation Agents. AI Matters , 9(2):8–14, October 2023. doi: 10.1145/3609468.
3609471. URL https://dl.acm.org/doi/10.1145/3609468.3609471 .
Yu Zhang, Peter Tiňo, Aleš Leonardis, and Ke Tang. A survey on neural network interpretability. IEEE
Transactions on Emerging Topics in Computational Intelligence , 5(5):726–742, 2021.
A Training Details
We did not train the network which we studied. Langosco et al. (2023) trained 15 maze-solving 3.5 m-
parameter deep convolutional network using Proximal Policy Optimization (Schulman et al., 2017). For
15Under review as submission to TMLR
Figure 11: A high-level visualization of the policy network architecture, using impalablocks from Espeholt
et al. (2018). The red-outlined layer contains the 11 goal-tracking channels and is where the “cheese vector"
and “top-right vector" were applied. For more details, refer to Langosco et al. (2023).
each ofn= 1,..., 15, networknwas trained in mazes where cheese was randomly placed in a free tile in the
top-rightn×nsquares of the maze. We primarily study the n= 5network.
When the policy reached the cheese, the episode terminated and a reward of +10 was recorded. Each model
was trained on 100 kprocedurally generated levels over the course of 200 mtimesteps. Figure 11 diagrams
the high-level architecture.
At each timestep, the policy observes a 64×64rgbimage, as shown by fig. 13. The policy has five actions
available:A:={↑,→,↓,←,ado nothing}.
16Under review as submission to TMLR
(a) No cheese present
 (b) Subtracting the cheese vector
 (c) The actions which changed
Figure 12: In seed 0, subtracting the cheese vector is behaviorally equivalent to hiding the cheese.
(a) What the policy observes
 (b) Human-friendly visualization
Figure 13: The mazes are defined on a 25×25game grid. For some mazes, the accessible maze is smaller,
and the rest is padded. Furthermore, the network observes a 64×64rgbimage (fig. 13(a)). In contrast, we
visualize mazes as in fig. 13(b): without padding, and as a higher-resolution image.
B Behavioral Statistics
We wanted to better understand the generalization behavior of the network. During training, the cheese
was always in the top-right n×ncorner. During testing, the cheese can be anywhere in the maze. In the
test distribution, visual inspection of sampled trajectories suggested that the network has goals related to
at least two historical reward proxies: the cheese, and the top-right corner.
To understand generalization behavior, we wanted to understand which maze features correlate with the
network’s decision to pursue the cheese or the corner. For each of the 15 pretrained networks, we uniformly
randomly sampled (without replacement) 10,000 maze seeds between 0 and 1e6. We sampled a rollout in
each seed. We recorded various statistics of the maze and rollout, such as whether the agent reached the
cheese. We then discarded mazes without decision squares (fig. 14), since in these mazes the policy does not
have to choose between the cheese or the corner. We also discarded mazes with cheese in the top-right 5×5
17Under review as submission to TMLR
corner, because i) we wanted to test generalization behavior, and ii) the cheese is probably just a few steps
from the decision square. This left us with 5,239 rollouts.
(a)
 (b)
Figure 14: A decision square is the square where there is divergence between the paths to the cheese and to
the top-right corner. In the first maze, the decision square is shown by a red dot. The second maze does not
have a decision square.
We considered a range of metrics. We considered two notions of distance and five pairs of maze landmarks,
and then measured their 10 possible combinations. The distances comprised:
1. The Euclidean L2distance in the game grid, d2.
2. Themazepathdistance, dpath. Eachmazeissimplyconnected,withoutloopsor“islands.” Therefore,
there is a unique shortest path between any two maze squares.
The pairs of maze landmarks were:
1. The top-right 5×5region and the cheese.
2. The top-right 5×5region and the decision square.
3. The cheese and the decision square.
4. The cheese and the top-right square.
5. The decision square and the top-right square.
Figure 15 visualizes four of these feature combinations.
We also regressed upon the L2norm of the cheese coordinate within the 25×25game grid (where the
bottom-left corner is the origin (0,0)). All else equal, larger coordinate norm is correlated with the cheese
being closer to the top-right corner (fig. 16).
To discover which of the 11 features are predictive, we trained single-variable regression models using ℓ1-
regularized logistic regression. As a baseline, always predicting that the agent gets the cheese yields an
accuracy 71.4%. Among the 11 variables investigated, 6 variables outperformed this baseline (table 1). The
rest performed worse than the no-regression baseline (table 2).
18Under review as submission to TMLR
2gray!25whiteVariable Prediction accuracy
d2(cheese,top-right 5×5) 0.775
d2(cheese,top-right square ) 0.773
d2(cheese,decision square ) 0.761
dpath(cheese,decision square ) 0.754
dpath(cheese,top-right 5×5) 0.735
dpath(cheese,top-right square ) 0.732
Table 1: Variables that outperform the no-regression baseline of 71.4%. We found that these variables have
negative regression coefficients, which matched our expectation that increased distance generally discourages
cheese-seeking behavior.
2gray!25whiteVariable Prediction accuracy
∥cheese coord∥2 0.713
d2(decision square ,top-right square ) 0.712
dpath(decision square ,top-right square ) 0.709
dpath(decision square ,top-right 5×5) 0.708
d2(decision square ,top-right 5×5) 0.708
Table 2: Variables that underperform the no-regression baseline of 71.4%.
B.1 Handling multicollinearity
Table 1 yielded 6 individually predictive features. However, many of these features are strongly correlated
(fig. 17 and fig. 18). In these situations, we must take extra care when regressing on all 6 variables and then
interpreting the regression coefficients.
We measure the variance inflation factor ( vif) in order to quantify the potential multicollinearity (James
et al., 2013). vifgreater than 4 is considered indicative of multicollinearity.
Features vif
d2(cheese,decision square )5.16
d2(cheese,top-right )107.96
d2(cheese,5×5top-right )107.52
dpath(cheese,decision square )5.43
dpath(cheese,5×5top-right )8.01
dpath(cheese,top-right )7.88
Table 3: Variation inflation factors for the 6 predictive variables. These variables display large multicollinear-
ity.
B.2 Assessing stability of regression coefficients
With the multicollinearity in mind, we perform an ℓ1-regularized multiple logistic regression on the 6 predic-
tive variables to assess their stability and importance. We compute results for 2,000 randomized test/train
splits. The results are shown in table 4.
Over the 2,000 regressions, the three italicized variables in table 4 are the only variables to not sign flip.
To further validate these results, we found that our conclusions held on another dataset of 10krandomly
seeded mazes.
Wealsoregressedon200randomsubsetsofthe6variables. Theaforementioned3variablesneverexperienced
a sign flip, strengthening our confidence that multicollinearity has not distorted our original regressions.
Taken together, this is why section 2.1 presents results for these three features.
19Under review as submission to TMLR
Attribute Coefficient
Steps between cheese and top-right 5×5−0.003
Euclidean distance between cheese and top-right 5×5 0.282
Steps between cheese and top-right square 1.142
Euclidean distance between cheese and top-right square −2.522
Steps between cheese and decision-square −1.200
Euclidean distance between cheese and decision-square 0.523
Intercept 1.418
Table 4: Coefficients from the initial ℓ1-regularized multiple regression. The 3 variables from section 2.1 are
italicized. Regression accuracy is 84.1%.
B.2.1 Regressing on the stable features
A regression using only the three stable variables retains an accuracy of 82.4%, averaged over 10 splits
(table 5). This is a 1.7% accuracy drop from the initial multiple regression on all 6 variables (table 4).
Attribute Coefficient
Euclidean distance between cheese and top-right square −1.405
Steps between cheese and decision-square −0.577
Euclidean distance between cheese and decision-square −0.516
Intercept 1.355
Table5: Regressionaccuracyis82.4%. Coefficientswhenregressingonlyonstablevariables. Wecautionthat
our analysis is not meant to hinge on the coefficient magnitudes , which are often contingent and unreliable
metrics. Instead, we think their sign and stability are better correlational evidence for the impact of these
features on the policy’s decisions.
We found that while adding a fourth variable (from the 6 above) can increase regression accuracy slightly,
the fourth variable has flipped sign. We interpret this as further evidence that the other variables do not
represent interpretable, meaningful influences on the policy’s decision-making.
B.3 Speculation on causality
Figure 19 demonstrates the large impact of increasing path distance to cheese, while holding constant the
other two stable variables.
Table 6 examines how dropping each stable variable impacts the regression accuracy. This provides evidence
on the predictive importance of each feature.
Considering both the qualitative and statistical findings, we have strong confidence that
dstep(cheese, decision-square )influences decision-making. We are more cautious about
d2(cheese, decision-square ), although its removal causes a notable accuracy drop similar to that of
dstep(cheese, decision-square ), a variable we are confident about. Overall, we suspect bothof these variables
affect decision-making, even though optimal policies would generally only depend on step distance (due to
the discounting term).
20Under review as submission to TMLR
Regression variables Accuracy
d2(cheese, top-right square )
dstep(cheese, decision-square )82.4%
d2(cheese, decision-square )
dstep(cheese, decision-square )75.9%
d2(cheese, decision-square )
d2(cheese, top-right square )81.9%
d2(cheese, decision-square )
d2(cheese, top-right square )81.7%
dstep(cheese, decision-square )
Table 6: Regression accuracy after dropping variables. Similar drops in accuracy occur for dropping
dstep(cheese, decision-square )andd2(cheese, decision-square ).
21Under review as submission to TMLR
(a)L2(decision sq. ,cheese )
(b) Steps from the decision
square to the cheese
(c)L2(cheese,top-right sq. )
(d)
L2(decision sq. ,top-right 5x5
Figure 15: Four of the features we regress upon.
22Under review as submission to TMLR
0 5 10 15 20 25 305101520253035
Norm of cheese coordinateEuclidean distance between cheese and top right square
Figure 16: Among mazes with decision squares, there is a Pearson correlation of −.550 between the norm of
the cheese coordinate and the distance. That is, the larger the norm, the closer the cheese is (in L2) to the
top-right square of the 25×25grid.
23Under review as submission to TMLR
0 5 10 15 2001020304050
Euclidean distance between cheese and decision squareSteps between cheese and decision square
Figure 17: Among mazes with decision squares, there is a Pearson correlation of .886 between these two
distances.
24Under review as submission to TMLR
Steps between cheese and top-right 5x5Euclidean distance between cheese and top-right 5x5Steps between decision square and top-right 5x5Euclidean distance between decision square and top-right 5x5Steps between cheese and top right squareEuclidean distance between cheese and top right squareSteps between decision square and top right squareEuclidean distance between decision square and top right squareSteps between cheese and decision squareEuclidean distance between cheese and decision squareNorm of cheese coordinateSteps between cheese and top-right 5x5
Euclidean distance between cheese and top-right 5x5
Steps between decision square and top-right 5x5
Euclidean distance between decision square and top-right 5x5
Steps between cheese and top right square
Euclidean distance between cheese and top right square
Steps between decision square and top right square
Euclidean distance between decision square and top right square
Steps between cheese and decision square
Euclidean distance between cheese and decision square
Norm of cheese coordinate
−1−0.500.51Correlation
Figure 18: The correlations between maze features, considering only mazes with decision squares.
25Under review as submission to TMLR
(a) Lowdpath (decision sq. ,cheese )
(b) Highdpath (decision sq. ,cheese )
Figure 19: The causal effect of increased path distance to cheese. We illustrate policy behavior
using the “vector field" view introduced by fig. 8. The decision square is boxed in red. Holding constant
d2(decision sq. ,cheese )(in green) and d2(decision sq. ,top-right sq. ), an increase in dpath(decision sq. ,cheese )
(in blue) makes the policy far less likely to pursue the cheese.
26Under review as submission to TMLR
B.4 Data from other models
We briefly examined other pretrained models from Langosco et al. (2023). For each of the models trained
on cheese in the top-right n×ncorner forn= 3,..., 15, we run the ℓ1-regularized logistic regression on the
three stable variables. Each setting regresses upon about 550 mazes.
2gray!25white
SizeSteps from decision sq. to cheese L2(decision sq. ,cheese )L2(top-right sq. ,cheese )
3 −0.681 0.000 −1.935
4 −0.276 −0.476 −1.438
5 −0.348 −0.745 −1.278
6 −1.606 −0.324 −1.361
7 −1.087 −0.208 −1.670
8 −0.759 −0.606 −1.833
9 −0.933 −0.112 −1.943
10 −1.051 −0.040 −2.075
11 −1.102 0.000 −1.212
12 −0.860 0.000 −1.732
13 −1.002 −0.045 −2.286
14 −0.743 0.150 −1.394
15 −0.663 −0.402 −1.726
Table 7:Regression coefficient signs are somewhat stable across nsettings. The regression coeffi-
cients found by ℓ1-regularized logistic regression. A sizeofnindicates that the cheese was spawned in the
top-rightn×nregion of each maze during training. Each size value corresponds to a separate pretrained
model from Langosco et al. (2023). Recall that this work mostly examines the n= 5case (and thus that
row is bolded). The n= 1,2cases did not pursue cheese outside of the top-right 5×5region, and so are
omitted.
C Additional Experiments
C.1 Causal scrubbing
In fig. 4, we explored the results of resampling channel activations from other mazes. In this subsection, we
motivate this technique and explore additional quantitative results.
Chan et al. (2022) introduce causal scrubbing . The basic idea is: If the important computa-
tion performed by part of a network only depends on a few input features (like the presence of
cheese at a certain coordinate), then randomizing other input features shouldn’t degrade performance.
Figure 20: The computational graph which we
test.We test the hypothesis that, at the forward pass lo-
cation highlighted by fig. 11, the residual channels
7,8,42,44,55,77,82,88,89,99,113are some function fof
theabsolutepositionofcheeseintheinputimage(fig.20).
We call these the “cheese-tracking” channels. If this hy-
pothesis is true, then we should be able to replace the
cheese-tracking activations with the activations from a
another maze with cheese in the same absolute location,
without disrupting behavior. We will call this the same-
cheesecondition. Alternatively, we could resample ac-
tivations from any other maze (not requiring the cheese
to be in the same location). This is the random-cheese
condition.
27Under review as submission to TMLR
(a) The original actions
 (b) Fixed-cheese resampling
 (c) The actions which changed
Figure 21: We resample activations for the 11 channels which we hypothesized to track cheese: Using the
vector field visualization introduced by fig. 8, behavior is almost invariant to resampling activations from
another maze with cheese in the same location (shown as a red dot). This invariance is demonstrated by the
imperceptible green difference arrows in fig. 21(c).
If the same-cheese condition changes the action probabil-
ities less than the random condition, this is evidence for
our hypothesis (shown in fig. 20). To quantify change in action probabilities, we perform the following
procedure for each of the first 30 maze seeds:
1. Compute the action probabilities at every free square in the maze. These are the baseprobabilities.
2. For each channel:
(a) Generate another maze with cheese in the same location, and a totally random maze seed.
(b) Record the activations for each.
3. Substituting the appropriate channel activations during the forward pass, compute the same-cheese
and random-cheese action probabilities for each free square in the maze.
4. Compute the average absolute difference10between action probabilities between:
(a) The fixed-cheese and base probabilities, and
(b) The random-cheese and base probabilities.
Figure 21 and fig. 22 show the impact of the two resampling conditions.
As a control, we further compare to the effects of resampling activations to a random subset of 11 channels
(excluding those we are already testing). Table 8 shows the results.
Same cheese location Random cheese location
11 “cheese-tracking" channels 0.88% 1.26%
11 randomly selected channels110.60% 0.54%
Table 8: Average change in action probabilities given different resampling procedures. The average is taken
across the first 30 maze seeds.
table 8’s quantitative results seem somewhat weaker than expected if fig. 20’s hypothesis were entirely accu-
rate. However, our channel selection could inherently be biased towards those that have a more significant
10I.e. the total-variation distance.
28Under review as submission to TMLR
(a) The original actions
 (b) Random-cheese resampling
 (c) The actions which changed
Figure 22: Behavior significantly changes when resampling “cheese-tracking” activations from another maze
with cheese in a different location (shown as a red dot).
impactonactionprobabilities. Wefoundsomeadditionalevidence(notincludedinthismanuscript)support-
ing this hypothesis. Furthermore, the total variation distance statistic does not account for the distribution
of changes in action probabilities—whether the changes are distributed across multiple minor adjustments
or concentrated in a few pivotal locations.
C.2 Subtracting the cheese vector probably removes the ability to see cheese at a location
C.2.1 Subtracting the cheese vector often has similar effects to hiding the cheese
Figure 23 and fig. 24 demonstrate our experience that “subtracting the cheese vector” is often behaviorally
equivalent to “hide the cheese from view.” If true, this allows us to interpret the effect of subtracting the
steering vector. This high-level understanding could lead to further insights into the learned computational
structure of the policy network which we studied.
(a) No cheese present
 (b) Subtracting the cheese vector
 (c) The actions which changed
Figure 23: In seed 0, subtracting the cheese vector is behaviorally equivalent to hiding the cheese.
However, in a few mazes (as in fig. 25), the cheese vector is not functionally equivalent to hiding the cheese.
This suggests that “hides the cheese location” is an important approximation to the function of the cheese
vector, but is not the whole story.
29Under review as submission to TMLR
(a) No cheese present
 (b) Subtracting the cheese vector
 (c) The actions which changed
Figure 24: In seed 12, subtracting the cheese vector is behaviorally equivalent to hiding the cheese.
(a) No cheese present
 (b) Subtracting the cheese vector
 (c) The actions which changed
Figure 25: In seed 7, subtracting the cheese vector is notbehaviorally equivalent to hiding the cheese.
C.2.2 Cheese vectors transfer to mazes with similarly-placed cheese
Suppose we compute a cheese vector for maze A. Can we also subtract the vector during navigation of some
other maze B? Our qualitative results indicate “yes, but only if the cheese is within about 2 tiles of its
original position.”
Figure 27 shows that the cheese vector computed on seed 0 also works on seed 795 (which has cheese at the
same location; fig. 26). This suggests that the cheese vector is a function of cheese location, and not of e.g.
the placement of walls in the maze.
C.3 Computation of steering vectors
We discuss the “contrast pair” (Burns et al., 2022) we used to compute the top-right steering vector (as
defined in section 3.2).
Empirically, having a path to the extreme top-right increases the policy’s attraction towards the top-right
corner. We hypothesize that the policy tracks the “priority” of navigating to the top right corner, and adding
in the top-right vector increases that priority.
30Under review as submission to TMLR
Figure 26: Two seeds with cheese at the same position.
(a) No cheese present
 (b) Subtracting cheese vector from
seed 0
(c) The actions which changed
Figure 27: In seed 795, subtracting the cheese vector from seed 0 still makes the policy ignore the cheese.
D Quantitative Analysis of Retargetability
Thetop-right path is the path from the policy’s starting location in the bottom left, to the top right corner.
Figure 29 shows a heatmap of each square’s path distance from the top-right path.
We find that the probability of successfully retargeting the policy decreases as the path distance from the
top-right path increases, as in fig. 30. These results corroborate the data and heatmaps discussed in §3.1.
We analyze the policy’s retargetability on the first 100 maze seeds. Suppose we wish to compute retar-
getability to state stin the maze. We do this as follows: Given initial state s0and target state st(neither
containing a wall), the normalized path probability is
Ppath(st|π) :=t/radicaltp/radicalvertex/radicalvertex/radicalbtt−1/productdisplay
i=0π(ai|si), (2)
31Under review as submission to TMLR
(a) The maze modified to have a reachable top-
right-most square.
(b) The original maze.
Figure28: Werunaforwardpassonbothmazes. Thetop-rightvectorconsistsoftheactivationsfor(a)minus
the activations for (b), at the relevant layer (see fig. 11). This operation can be performed algorithmically
for any maze, although we found that the top-right vector from maze A often transfers to other mazes.
Figure 29: Each square’s path distance from the top-right path (which is not at all reddened). The brighter
the red, the greater the distance.
wheres0,s1,...,s tis the unique12shortest path between s0andst, navigated by actions ai. Ifs0=st, then
Ppath(st|π) :=π(no-op|s0).
For each of the 100 maze seeds, we remove the cheese from the maze. Then fig. 7 computes the following
statistics for each target square st:
Base Probability Computes eq. (2) for the unmodified policy π.
Channel 55 πis modified to incorporate an α= +5.5-strength intervention at the channel-55 activation
corresponding to the location of st.
Effective Channels (not shown in fig. 7) Anα = +2.3intervention on channels
{8,55,77,82,88,89,113}.
12Because the maze is simply connected.
32Under review as submission to TMLR
All Channels Anα= +1.0intervention on channels {7,8,42,44,55,77,82,88,89,99,113}.
Cheese The unmodified policy πis retained, but cheese is placed at st, and eq. (2) is computed according
to the new state observations si.
0 10 20 30 40 50
Distance from Top Right Path0.20.30.40.50.60.70.80.9Probability of Successful Retargeting
Figure 30
33Under review as submission to TMLR
0 10 20 30 40 50
Distance from Top Right Path1.01.52.02.53.03.54.04.55.0Ratio of Successful Retargeting
Figure 31: Targets stwhich are farther off the top-right path, will often have lower retargeting probability
andlower base probability. To control for this, we plot the ratioPpath(st|πretarget )
Ppath(st|π). Ratios greater than
1 indicate that the retargeting increased the normalized path probability. Thus, retargeting increases the
probability of reaching a tile, no matter its distance from the top-right path.
34Under review as submission to TMLR
3 5 7 9 11 13 15 17 19 21 23 25
Maze size0.500.550.600.650.700.750.80Average probability of retargetting success
All Cheese Channels
Effective Channels
Channel 55
Figure 32: Modifying more channels increases the probability of successful retargeting over the whole maze in
comparison to modifying a single channel. Crucially, this is true even with larger magnitudes in the single-
channel interventions . This data shows that intervening on more of the circuits distributed throughout
the network is more effective. Finding such circuits is important for controlling networks through manual
activation engineering.
35Under review as submission to TMLR
E Further Examples of Network Behavior
E.1 Further Examples of fig. 3: Network Channels Track The Goal Location
12
3
Figure 33: Channel 7.
2
31
Figure 34: Channel 8.
12
3
Figure 35: Channel 42.
36Under review as submission to TMLR
12
3
Figure 36: Channel 44.
12
3
Figure 37: Channel 55.
12
3
Figure 38: Channel 77.
12
3
Figure 39: Channel 82.
37Under review as submission to TMLR
12
3
Figure 40: Channel 88.
12
3
Figure 41: Channel 89.
12
3
Figure 42: Channel 99.
12
3
Figure 43: Channel 113.
38Under review as submission to TMLR
E.2 Further Examples of fig. 4: Resampling Cheese-Tracking Activations From Different Mazes
Here we show 3 examples of each size maze from the first 100 seeds. The resampled locations are always in
the top right and bottom right corners, respectively. Resampling locations that were farther from the path to
the top-right corner (appendix D for further details) were more difficult to steer towards. In most instances
of resampling from cheese located in the bottom right, the policy instead steered towards the historical goal
location in the top right.
Figure 44: Maze size 3x3: seed 1.
Figure 45: Maze size 3x3: seed 10.
Figure 46: Maze size 3x3: seed 11.
39Under review as submission to TMLR
Figure 47: Maze size 5x5: seed 3.
Figure 48: Maze size 5x5: seed 7.
Figure 49: Maze size 5x5: seed 19.
Figure 50: Maze size 7x7: seed 26.
40Under review as submission to TMLR
Figure 51: Maze size 7x7: seed 34.
Figure 52: Maze size 7x7: seed 54.
Figure 53: Maze size 7x7: seed 6.
Figure 54: Maze size 7x7: seed 20.
41Under review as submission to TMLR
Figure 55: Maze size 7x7: seed 52.
Figure 56: Maze size 11x11: seed 35.
Figure 57: Maze size 11x11: seed 37.
Figure 58: Maze size 11x11: seed 42.
42Under review as submission to TMLR
Figure 59: Maze size 13x13: seed 51.
Figure 60: Maze size 13x13: seed 74.
Figure 61: Maze size 13x13: seed 84.
Figure 62: Maze size 15x15: seed 9.
43Under review as submission to TMLR
Figure 63: Maze size 15x15: seed 25.
Figure 64: Maze size 15x15: seed 36.
Figure 65: Maze size 17x17: seed 50.
Figure 66: Maze size 17x17: seed 64.
44Under review as submission to TMLR
Figure 67: Maze size 17x17: seed 76.
Figure 68: Maze size 19x19: seed 24.
Figure 69: Maze size 19x19: seed 46.
Figure 70: Maze size 19x19: seed 81.
45Under review as submission to TMLR
Figure 71: Maze size 21x21: seed 8.
Figure 72: Maze size 21x21: seed 32.
Figure 73: Maze size 21x21: seed 53.
(a): Original Maze
(b): Resampling from
same location
(c): Resampling from
 red cheese location
(d): Resampling from
 red cheese location
Figure 74: Maze size 23x23: seed 12.
46Under review as submission to TMLR
Figure 75: Maze size 23x23: seed 13.
Figure 76: Maze size 23x23: seed 67.
Figure 77: Maze size 25x25: seed 40.
Figure 78: Maze size 25x25: seed 55.
47Under review as submission to TMLR
Figure 79: Maze size 25x25: seed 71.
48Under review as submission to TMLR
E.3 Further Examples of fig. 5: Controlling The Maze-Solving Policy By Modifying A Single
Activation
Here we take the same specific activations from fig. 5, with intervention magnitude α= +5.5, and apply
them to other mazes of the same size. Arbitrary retargeting does not always work, especially for activations
farther away from the top-right path. See appendix D for more information and statistics on the top-right
path. The most-probable paths indicate that it’s harder to retarget the mouse farther off of the top-right
path. Instead, the policy navigates to the historical goal location. In fact, some seeds do not see any change
in the most probable path, although quantitative analyses in appendix D detail the changing probabilities
of all paths through different maze sizes and interventions.
Figure 80: Patching specific activations: seed 0.
Figure 81: Patching specific activations: seed 2.
Figure 82: Patching specific activations: seed 16.
49Under review as submission to TMLR
Figure 83: Patching specific activations: seed 51.
Figure 84: Patching specific activations: seed 74.
Figure 85: Patching specific activations: seed 84.
Figure 86: Patching specific activations: seed 85.
50Under review as submission to TMLR
Figure 87: Patching specific activations: seed 99.
Figure 88: Patching specific activations: seed 107.
Figure 89: Patching specific activations: seed 108.
Figure 90: Patching specific activations: seed 132.
51Under review as submission to TMLR
Figure 91: Patching specific activations: seed 169.
Figure 92: Patching specific activations: seed 183.
Figure 93: Patching specific activations: seed 189.
Figure 94: Patching specific activations: seed 192.
52