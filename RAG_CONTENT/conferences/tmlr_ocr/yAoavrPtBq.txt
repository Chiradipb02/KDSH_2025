Under review as submission to TMLR
Zero-Order One-Point Estimate with Distributed Stochastic
Gradient Techniques
Anonymous authors
Paper under double-blind review
Abstract
In this work, we consider a distributed multi-agent stochastic optimization problem, where
each agent holds a local objective function that is smooth and strongly convex and that is
subject to a stochastic process. The goal is for all agents to collaborate to ﬁnd a common
solution that optimizes the sum of these local functions. With the practical assumption that
agents can only obtain noisy numerical function queries at precisely one point at a time, we
extend the distributed stochastic gradient-tracking (DSGT) method to the bandit setting
where we do not have access to the gradient, and we introduce a zero-order (ZO) one-point
estimate (1P-DSGT). We then consider another consensus-based distributed stochastic gra-
dient (DSG) method under the same setting and introduce the same estimate (1P-DSG). We
analyze the convergence of these novel techniques for smooth and strongly convex objectives
using stochastic approximation tools, and we prove that they converge almost surely to the
optimum despite the biasedness of our gradient estimate. We then study the convergence
rate of our methods. With constant step sizes, our methods compete with their ﬁrst-order
(FO) counterparts by achieving a linear rate O(/rho1k)as a function of number of iterations
k. To the best of our knowledge, this is the ﬁrst work that proves this rate in the noisy
estimation setting or with one-point estimators. With vanishing step sizes, we establish a
rate ofO(1√
k)after a suﬃcient number of iterations k>K 0. This is the optimal rate proven
in the literature for centralized techniques utilizing one-point estimators. We then provide
a regret bound of O(√
k)with vanishing step sizes. We further illustrate the usefulness of
the proposed techniques using numerical experiments.
1 Introduction
Gradient-free optimization is an old topic in the research community; however, there has been an increased
interest recently, especially in machine learning applications, where optimization problems are typically
solvedwithgradientdescentalgorithms. Successfulapplicationsofgradient-freemethodsinmachinelearning
include competing with an adversary in bandit problems (Flaxman et al., 2004; Agarwal et al., 2010),
generating adversarial attacks for deep learning models (Chen et al., 2019; Liu et al., 2019) and reinforcement
learning (Vemula et al., 2019). Gradient-free optimization aims to solve optimization problems with only
functional ZO information rather than FO gradient information. These techniques are essential in settings
where explicit gradient computation may be impractical, expensive, or impossible. Instances of such settings
include high data dimensionality, time or resource straining function diﬀerentiation, or the cost function not
having a closed-form. ZO information-based methods include direct search methods (Golovin et al., 2019),
1-point methods (Flaxman et al., 2004; Bach & Perchet, 2016; Vemula et al., 2019; Li & Assaad, 2021) where
a functionf(·,S) :Rd→Ris evaluated at a single point with some randomization to estimate the gradient
as such
g(1)
γ,z(x,S) =d
γf(x+γz,S )z, (1)
withxthe optimization variable, γ > 0a small value, and za random vector following a symmetrical
distribution. ZO also includes 2- or more point methods (Duchi et al., 2015; Nesterov & Spokoiny, 2017;
Gorbunov et al., 2018; Bach & Perchet, 2016; Hajinezhad et al., 2019; Kumar Sahu et al., 2018; Agarwal
1Under review as submission to TMLR
et al., 2010; Chen et al., 2019; Liu et al., 2019; Vemula et al., 2019), where functional diﬀerence at various
points is employed for estimation, generally having the respective structures
g(2)
γ,z(x,S) =df(x+γz,S )−f(x−γz,S )
2γz (2)
andg(2d)
γ(x,S) =d/summationdisplay
j=1f(x+γej,S)−f(x−γej,S)
2γej (3)
where{ej}j=1,...,dis the canonical basis, and other methods such as sign information of gradient estimates
(Liu et al., 2019).
Another area of great interest is distributed multi-agent optimization, where agents try to cooperatively
solve a problem with information exchange only limited to immediate neighbors in the network. Distributed
computing and data storing are particularly essential in ﬁelds such as vehicular communications and coordi-
nation, data processing and distributed control in sensor networks (Shi & Eryilmaz, 2020), big-data analytics
(Daneshmand et al., 2015), and federated learning (McMahan et al., 2017). More speciﬁcally, one direction
of research integrates (sub)gradient-based methods with a consensus/averaging strategy; the local agent
incorporates one or multiple consensus steps alongside evaluating the local gradient during optimization.
Hence, these algorithms can tackle a fundamental challenge: overcoming diﬀerences between agents’ local
data distributions.
1.1 Problem Description
Consider a set of agents N={1,2,...,n}connected by a communication network. Each agent iis associated
with a local objective function fi(·,S) :K→R, whereK⊂Rdis a convex feasible set. The global goal
of the agents is to collaboratively locate the decision variable x∈Kthat solves the stochastic optimization
problem:
min
x∈KF(x) =1
nn/summationdisplay
i=1Fi(x) (4)
where
Fi(x) =ESfi(x,S),
withS∈Sdenoting an i.i.d. ergodic stochastic process describing uncertainties in the communication
system.
We assume that at each time step, agent ican only query the function values of fiat exactly one point, and
can only communicate with its neighbors. Further, we assume that the function queries are noisy ˜fi=fi+ζi
withζisome additive noise. Agent imust then employ this query to estimate the gradient of the form
gi(x,Si).
One eﬃcient algorithm with a straightforward averaging scheme to solve this problem is the gradient-tracking
(GT) technique, which has proven to achieve rates competing with its centralized counterparts. For example,
the acquired error bound under a distributed stochastic variant was found to decrease with the network size
n (Pu & Nedić, 2018). In most work, this technique proved to converge linearly to a neighborhood of the
optimal solution with constant step size (Qu & Li, 2018; Nedić et al., 2017; Pu, 2020), which is also a unique
attribute among other distributed stochastic gradient algorithms. It has been extended to time-varying
(undirected or directed) graphs (Nedić et al., 2017), a gossip-like method which is eﬃcient in communication
(Pu&Nedić,2018), andnonconvexsettings(Tangetal.,2021;Lorenzo&Scutari,2016;Jiangetal.,2022;Lu
et al., 2019). All references mentioned above consider the case where an accurate gradient computation or a
unbiased gradient estimation with bounded variance (BV) is available, the variance being the mean squared
approximation error of the gradient estimate with respect to the true gradient. Alongside this gradient
tracking method, we explore another consensus-based distributed method without gradient tracking.
2Under review as submission to TMLR
1.2 Function Classes and Gradient Estimate Assumptions
Consider the following ﬁve classes of functions:
•The convex class Ccvxcontaining all functions f:Rd→Rthat are convex.
•The strongly convex class Csccontaining all functions f:Rd→Rthat are continuously diﬀerentiable
and admit a constant λfsuch that
/angbracketleft∇f(x)−∇f(y),x−y/angbracketright/bardbl≥λf/bardblx−y/bardbl2,∀x,y∈Rd.
•The Lipschitz continuous class Clipcontaining all functions f:Rd→Rthat admit a constant Lf
such that
|f(x)−f(y)|≤Lf/bardblx−y/bardbl,∀x,y∈Rd.
•The smooth class Csmocontaining all functions f:Rd→Rthat are continuously diﬀerentiable and
admit a constant Gfsuch that
/bardbl∇f(x)−∇f(y)/bardbl≤Gf/bardblx−y/bardbl,∀x,y∈Rd.
•The gradient dominated class Cgdcontaining all functions f:Rd→Rthat are diﬀerentiable, have
a global minimizer x∗, and admit a constant νfsuch that
2νf(f(x)−f(x∗))≤/bardbl∇f(x)/bardbl2,∀x∈Rd.
This gradient domination property can be viewed as a nonconvex analogy of strong convexity.
In addition, consider the following assumptions on the gradient estimate:
•A gradient estimate gis said to be unbiased w.r.t. the true gradient ∇fif for allx∈Rdand
independent S∈S, it satisﬁes the following equality
ES[g(x,S)|x] =∇f(x).
•Otherwise, it is said to be biased and satisﬁes
ES[g(x,S)|x] =∇f(x) +b(x),
withb(x)some bias term.
•A gradient estimate gis said to have bounded variance when for all x∈Rdand independent S∈S,
ES[/bardblg(x,S)−∇f(x)/bardbl2|x]≤σfor someσ>0.
•Otherwise, when this bound is unknown or does not exist, it is said to have unbounded variance.
In general, FO stochastic gradient estimates are unbiased and have bounded variance. ZO estimates, on the
other hand, are biased. While multi-point ZO estimates have bounded or even vanishing variance, one-point
estimates have unbounded variance Liu et al. (2020).
3Under review as submission to TMLR
GR. OPFUNCTION
CLASSCONS-
ENSUSSTEP
SIZEREGRET
BOUNDCONVERGENCE
RATE
ZOOne-
pointCent. Ccvx/intersectiontext
Clip - f. O(k3
4) O(1
4√
k)Flaxman et al. (2004)
Cent. Csc/intersectiontext
Clip/intersectiontext
Csm - v. O(√
k) O(1√
k)Bach & Perchet (2016)
Dist. Csc/intersectiontext
Csmo GT (w/o GT) v. O(√
k) O(1√
k)1P-DSGT (1P-DSG)
Dist. Csc/intersectiontext
Csmo GT (w/o GT) f. - O(/rho1k)1P-DSGT (1P-DSG)
Two-
pointCent. Ccvx/intersectiontext
Clip - v. O(√
k) O(1√
k)Agarwal et al. (2010)
Cent. Csc/intersectiontext
Clip - v. O(logk) O(logk
k)Agarwal et al. (2010)
Dist. Clip/intersectiontext
Csmo w/o GT v. - O(1√
klogk)Tang et al. (2021)
Dist. Csmo/intersectiontext
Cgd w/o GT v. - O(1
k)Tang et al. (2021)
(d+1)-
pointCent. Csc/intersectiontext
Clip/intersectiontext
Csmo - v. O(logk) O(logk
k)Agarwal et al. (2010)
2d-
pointDist. Csmo GT f. - O(1
k)Tang et al. (2021)
Dist. Csmo/intersectiontext
Cgd GT f. - O(/rho1k)Tang et al. (2021)
Kernel
-basedCent. Ccvx/intersectiontext
Clip - v. O(√
k) O(1√
k)Bubeck et al. (2021)
FOUnbiased
/BVDist. Csc/intersectiontext
Csmo GT f. - O(/rho1k)Pu & Nedić (2018) ;
Xin et al. (2019); Pu (2020)
Dist. Csc/intersectiontext
Csmo GT v. - O(1
k)Pu & Nedić (2018)
Dist. Csmo GT v. - O(1√
k)Lu et al. (2019)
Table 1: Convergence rates for various algorithms related to our work, classiﬁed according to the nature
of the gradient estimate (gr.), whether the optimization problem (OP) is centralized or distributed, the
assumptions on the objective function, the consensus strategy, if any, either with gradient tracking (GT) or
without (w/o GT), whether the step size is ﬁxed (f.) or varying (v.), and the achieved regret bound and
convergence rate
1.3 Related Work
The optimal convergence rate for solving problem (4), assuming the objective function Fis strongly convex
with Lipschitz continuous gradients, has been established as O(1
k)under a diminishing step size with full
gradient information Pu & Nedić (2018); Nemirovski et al. (2009). However, when employing a constant
step sizeα > 0that is suﬃciently small, the iterates produced by a stochastic gradient method converge
exponentially fast (in expectation) to an O(α)-neighborhood of the optimal solution (Pu & Nedić, 2018);
this is known as the linear rate O(/rho1k). To solve problem (4), all Qu & Li (2018); Lorenzo & Scutari
(2016); Nedić et al. (2017); Shi et al. (2015); Li et al. (2022); Jiang et al. (2022) present a distributed
gradient-tracking method that employs local auxiliary variables to track the average of all agents’ gradients,
considering the availability of accurate gradient information. In both Li et al. (2022); Jiang et al. (2022),
each local objective function is an average of ﬁnite instantaneous functions. Thus, they incorporate the
gradient-tracking algorithm with stochastic averaging gradient technology (Li et al., 2022) (smooth convex
optimization) or with variance reduction techniques (Jiang et al., 2022) (smooth nonconvex optimization).
At each iteration, they randomly select only one gradient of an instantaneous function to approximate the
4Under review as submission to TMLR
local batch gradient. In Li et al. (2022), this is an unbiased estimate of the local gradient, whereas, in Jiang
et al. (2022), it is biased. Nevertheless, both references assume access to an exact gradient oracle.
All Pu & Nedić (2018); Xin et al. (2019); Pu (2020); Lu et al. (2019) assume access to local stochastic FO
oracles where the gradient is unbiased and with a bounded variance. In the ﬁrst three, they additionally
assume smooth and strongly-convex local objectives, and they all accomplish a linear convergence rate under
a constant step size. Pu & Nedić (2018) propose a distributed stochastic gradient-tracking method (DSGT)
and a gossip-like stochastic gradient-tracking method (GSGT) where at each round, each agent wakes up
with a certain probability. Further, in Pu & Nedić (2018), when the step-size is diminishing, the convergence
rate is that of O(1
k). Xin et al. (2019) employ a gradient-tracking algorithm for agents communicating
over a strongly-connected graph. Pu (2020) introduces a robust gradient-tracking method (R-Push-Pull)
in the context of noisy information exchange between agents and with a directed network topology. Lu
et al. (2019) propose a gradient-tracking based nonconvex stochastic decentralized (GNSD) algorithm for
nonconvex optimization problems in machine learning, and they fulﬁll a convergence rate of O(1√
k)under
constant step size.
On the other hand, ZO methods are known to have worse convergence rates than their FO counterparts
under the same conditions. For example, under a convex centralized setting, Flaxman et al. (2004) prove a
regret bound of O(k3
4)(or equivalently a rate of O(1
4√
k)) with a one-point estimator for Lipschitz continuous
functions. For strongly convex and smooth objective functions, Hazan & Levy (2014) and Ito (2020) improve
upon this result by proving a regret of O(√klogk)and Bach & Perchet (2016) that of O(√
k). In the work
of Agarwal et al. (2010), when the number of points is two, they prove regret bounds of ˜O(√
k)with high
probability and of O(log(k))in expectation for strongly convex loss functions. When the number is d+ 1
point, they prove regret bounds of O(√
k)and ofO(log(k))with strong convexity. The reason why the
performance improves with the addition of number of points in the estimate, is that their variance can be
bounded, unlike one-point estimates whose variance cannot be bounded (Liu et al., 2020). However, when
the function queries are subjected to noise, multi-point estimates start behaving like one-point ones. In noisy
function queries (centralized) scenario, it has been proven that gradient-free methods cannot achieve a better
convergence rate than Ω(1√
k)which is the lower bound derived by Duchi et al. (2015); Jamieson et al. (2012);
Shamir (2013) for strongly convex and smooth objective functions. In the work of Bubeck et al. (2021), a
kernelized loss estimator is proposed where a generalization of Bernoulli convolutions is adopted, and an
annealing schedule for exponential weights is used to control the estimator’s variance in a focus region for
dimensions higher than 1. Their method achieves a regret bound of O(√
k).
In distributed settings, Tang et al. (2021) develop two algorithms for a noise-free nonconvex multi-agent
optimization problem aiming at consensus. One of them is gradient-tracking based on a 2d-point estimator
of the gradient with vanishing variance that achieves a rate of O(1
k)with smoothness assumptions and a
linearrateforanextra ν-gradientdominatedobjectiveassumptionandforﬁxedstepsizes. Theotherisbased
on a 2-point estimator without global gradient tracking and achieves a rate of O(1√
klogk)under Lipschitz
continuity and smoothness conditions and O(1
k)under an extra gradient dominated function structure.
We summarize all the mentioned convergence rates from the literature in Table 1.
1.4 Contributions
While the gradient tracking method has been extended to the ZO case (Tang et al., 2021), the approach
followed by Tang et al. (2021) relies on a multi-point gradient estimator. It also assumes a static objective
function devoid of any stochasticity or noise in the system. However, real-world scenarios often involve
various sources of stochasticity and noise, such as diﬀering data distributions among devices, perturbations
in electronic components, quantization errors, data compression losses, and ﬂuctuations in communication
channels over time. Consequently, static objective functions become inadequate for realistic modeling. More-
over, the multi-point estimation technique assumes the ability to observe multiple instances of the objective
function under identical system conditions, i.e., many function queries are done for the same realization of S
in (2) and (3). However, this assumption needs to be revised in applications such as mobile edge computing
(Mao et al., 2017; Chen et al., 2021; Zhou et al., 2022) where computational tasks from mobile users are
5Under review as submission to TMLR
oﬄoaded to servers within the cellular network. Thus, queries requested from the servers by the users are
subject to the wireless environment and are corrupted by noise not necessarily additive. Other applications
include sensor selection for an accurate parameter estimation (Liu et al., 2018) where the observation of
each sensor is continuously changing. Thus, in such scenarios, one-point estimates oﬀer a vital alternative to
solving online optimization/learning problems. Yet, one-point estimators are not generally used because of
their slow convergence rate. The main reason is due to their unbounded variance. To avoid this unbounded
variance, in this work, we don’t use the estimate given in (1), we extend the one point approach in Li &
Assaad (2021)’s work where the action of the agent is a scalar and diﬀerent agents have diﬀerent variables,
to our consensual problem with vector variables. The diﬀerence is that in our gradient estimate, we don’t
divide byγ. This brings additional challenges in proving that our algorithm converges and a consensus can
be achieved by all agents. And even with bounded variance, there’s still a diﬃculty achieving good con-
vergence rates when combining two-point estimates with the gradient tracking method due to the constant
upper bound of the variance (Tang et al., 2021). Here, despite this constant bound, we were able to go
beyond two-point estimates and achieve a linear rate. Moreover, while it requires 2dpoints to achieve a
linear rate in Tang et al. (2021)’s work, which is twice the dimension of the gradient itself, here we only need
one scalar point or query. This is much more computationally eﬃcient. We were also able to prove that this
same method without the gradient tracking step converges linearly to a neighborhood of the optimum with
ZO information, which has not been done before with that method.
We summarize our contribution in the following points,
•We consider smooth and strongly convex local objectives, and we extend the gradient-tracking
algorithm to the case where we do not have access to the gradient in the noisy setting. We also
extend another consensus-based distributed algorithm to the same setting.
•Under the realistic assumption that the agent only has access to a single noisy function value at
each time without necessarily knowing the form of this function, we propose a one-point estimator
in a stochastic framework.
•Naturally, one-point estimators are biased with respect to the true gradient and suﬀer from high
variance (Liu et al., 2020); hence, they do not match the assumptions for convergence presented by
Tang et al. (2021); Pu & Nedić (2018); Xin et al. (2019); Pu (2020); Lu et al. (2019). However,
in this work, we analyze and indeed prove the algorithm’s convergence almost surely with a biased
estimate. This convergence is stronger than expected convergence analysis usually established for
ZO optimization. We also consider that a stochastic process inﬂuences the objective function from
one iteration to the other, which is not the case in the aforementioned references.
•We then study the convergence rate and we demonstrate that with ﬁxed step sizes, the algorithm
achieves a linear convergence rate O(/rho1k)to a neighborhood of the optimal solution, marking the
ﬁrst instance where this rate is attained in ZO optimization with one-point/two-point estimates and
in a noisy query setting, to the best of our knowledge. This linear rate competes with FO methods
and even centralized algorithms in terms of convergence speed (Pu & Nedić, 2018).
•Moreinterestingly,ourproofcanbeusedtoanalyzetheconvergenceofotherdistributedZOmethods.
For instance, we prove that other consensus-based methods can achieve linear convergence rates with
single-point ZO estimates. This shows that, in general, single-point methods can converge at a linear
rate to a neighborhood of the global optimum. We re-emphasize that this is the ﬁrst time such a
convergence rate has been established.
•Whenthestep-sizesarevanishing, weprovethatarateof O(1√
k)isattainabletoconvergetoanexact
solution after a suﬃcient number of iterations k>K 0. This rate satisﬁes the lower bounds achieved
by its centralized counterparts in the same derivative-free setting (Duchi et al., 2015; Jamieson et al.,
2012; Shamir, 2013).
•We then show that a regret bound of O(√
k)is achieved for this algorithm.
•Finally, we support our theoretical claims by providing numerical evidence and comparing the algo-
rithm’s performance to its FO and centralized counterparts.
6Under review as submission to TMLR
The rest of this paper is divided as follow. In subsection 1.5, we present the mathematical notation followed
in this paper. In subsection 1.6, we present the main assumptions of our optimization problem. We then
describe our gradient estimate followed by the proposed algorithms in subsection 2.1. We then prove the
almost sure convergence of our ﬁrst algorithm in subsection 3.1 and study its rate in subsection 3.2 with
varying step sizes. In subsection 3.3, we ﬁnd its regret bound. And in subsection 3.4, we consider the case
of ﬁxed step sizes, study the convergence of our algorithm and its rate. In section 4, we re-establish all our
results for the second algorithm. Finally, in section 5, we provide numerical evidence and conclude the paper
in section 6.
1.5 Notation
In all that follows, vectors are column-shaped unless deﬁned otherwise and 1denotes the vector of all entries
equal to 1. For two vectors a,bof the same dimension, /angbracketlefta,b/angbracketrightis the inner product. For two matrices A,
B∈Rn×d, we deﬁne
/angbracketleftA,B/angbracketright=n/summationdisplay
i=1/angbracketleftAi,Bi/angbracketright
whereAi(respectively, Bi) represents the i-th row ofA(respectively, B). This matrix product is the Hilbert-
Schmidt inner product which is written as /angbracketleftA,B/angbracketright= tr(ABT)./bardbl./bardbldenotes the 2-norm for vectors and the
Frobenius norm for matrices.
We next let ΠK(·)denote the Euclidean projection of a vector on the set K. We know that this projection
on a closed convex set Kis nonexpansive (Kinderlehrer & Stampacchia (2000) - Corollary 2.4), i.e.,
/bardblΠK(x)−ΠK(x/prime)/bardbl≤/bardblx−x/prime/bardbl,∀x,x/prime∈Rd. (5)
We assume that each agent imaintains a local copy xi∈Kof the decision variable and another auxiliary
variableyi∈Rd(considered only in the gradient tracking method) and each agent’s local function is subject
to the stochastic variable Si∈Rm. At iteration k, the respective values are denoted as xi,k,yi,k, andSi,k.
Bold notations denote the concatenated version of the variables, i.e.,
x:= [x1,x2,...,xn]Tandy:= [y1,y2,...,yn]Tare of dimension n×d,
andS:= [S1,S2,...,Sn]Tof dimension n×m.
We then deﬁne the means of the previous two variables as ¯x:=1
n1Txand¯y:=1
n1Tyof dimension 1×d.
We deﬁne the gradient of Fiat the local variable ∇Fi(xi)∈Rdand its Hessian matrix ∇2Fi(xi)∈Rd×dand
we let
∇F(x) := [∇F1(x1),∇F2(x2),...,∇Fn(xn)]T∈Rn×d
and
g:=g(x,S) := [g1(x1,S1),g2(x2,S2),...,gn(xn,Sn)]T∈Rn×d.
We deﬁne its mean ¯g:=1
n1Tg∈R1×dand we denote each agent’s gradient estimate at time kbygi,k=
gi(xi,k,Si,k).
1.6 Basic Assumptions
In this subsection, we introduce the fundamental assumptions that ensure the performance of the 1P-DSGT
algorithm.
Assumption 1.1. (on the graph) The topology of the network is represented by the graph G= (N,E)where
the edges inE⊆N×N represent communication links. A graph Gis undirected, i.e., (i,j)∈Eiﬀ(j,i)∈E,
and connected (there exists a path of links between any two agents).
W= [wij]∈Rn×ndenotes the agents’ coupling matrix, where agents iandjare connected iﬀ wij=wji>0
(wij=wji= 0otherwise). Wis a nonnegative matrix and doubly stochastic, i.e., W1=1and1TW=1T.
All diagonal elements wiiare strictly positive.
7Under review as submission to TMLR
Assumption 1.2. (on the objective function) We assume the existence and the continuity of both ∇Fi(x)
and∇2Fi(x). Letx∗∈Kdenote the solution of the problem (4), then ∇Fi(x∗) = 0anddet(∇2Fi(x∗))>0,
∀i∈N. To ensure the existence of x∗, we letFi(x)beλi-strongly convex for all i∈N. Then,F(x)is also
strongly convex with λ=1
n/summationtextn
i=1λi>0and
/angbracketleft∇F(x),x−x∗/angbracketright≥λ/bardblx−x∗/bardbl2,∀x∈K.
We further assume the boundedness of the local Hessian where there exists a constant c1∈R+such that
/bardbl∇2Fi(x)/bardbl2≤c1,∀x∈K,∀i∈N,
where here it suﬃces to the spectral norm (keeping in mind for a matrix A,/bardblA/bardbl2≤/bardblA/bardblF).
Assumption 1.3. (on the additive noise) ζi,kis a zero-mean uncorrelated noise with bounded variance,
whereE(ζi,k) = 0andE(ζ2
i,k) =c2<∞,∀i∈N.
Lemma 1.4. (Qu & Li, 2018) Let ρwbe the spectral norm of W−1
n11T. When Assumption 1.1 is satisﬁed,
we have the following inequality
/bardblWω−1¯ω/bardbl≤ρw/bardblω−1¯ω/bardbl,∀ω∈Rn×dand¯ω=1
n1Tω,
andρw<1.
Lemma 1.5. (Pu & Nedić, 2018) Deﬁne h(x) :=1
n1T∇F(x)∈R1×d. Due to the boundedness of the second
derivative in Assumption 1.2, there exists a scalar L>0such that the objective function is L-smooth, and
/bardbl∇F (¯x)−h(x)/bardbl≤L√n/bardblx−1¯x/bardbl.
Proof: See Appendix A.
2 Distributed Stochastic Gradient Methods
We propose to employ a zero-order one-point estimate of the gradient subject to the stochastic process S
and an additive noise ζwhile a stochastic perturbation and a step size are introduced, and we assume that
each agent can perform this estimation at each iteration. To elaborate, let gi,kdenote the aforementioned
gradient estimate for agent iat timek, then we deﬁne it as
gi,k= Φi,k˜fi(xi,k+γkΦi,k,Si,k)
= Φi,k(fi(xi,k+γkΦi,k,Si,k) +ζi,k),(6)
whereγk>0is a vanishing step size and Φi,k∈Rdis a perturbation randomly and independently generated
by each agent i.gi,kis in fact a biased estimation of the gradient ∇Fi(xi,k)and the algorithm can converge
under the condition that all parameters are properly chosen. For clariﬁcation on the form of this bias and
more on the properties of this estimate, refer to Appendix B.
2.1 The Algorithms
The ﬁrst algorithm is a distributed stochastic gradient-tracking method denoted as 1P-DSGT employing the
gradient estimate presented in (6). Every agent iinitializes its variables with an arbitrary valued vector
xi,0∈Kandyi,0=gi,0. Then, at each time k∈N, agentiupdates its variables independently according to
the following steps:
zi,k+1=n/summationdisplay
j=1wij(xj,k−αkyj,k)
xi,k+1= ΠK(zi,k+1)
perform the action: xi,k+1+γk+1Φi,k+1
yi,k+1=n/summationdisplay
j=1wijyj,k+gi,k+1−gi,k(7)
8Under review as submission to TMLR
whereαk>0is a vanishing step size. Algorithm (7) can then be written in the following compact matrix
form for clarity of analysis:
zk+1=W(xk−αkyk)
xk+1= [x1,k+1,x2,k+1,...,xn,k+1]T
perform the action: xk+1+γk+1Φk+1
yk+1=Wyk+gk+1−gk(8)
where Φk∈Rn×dis deﬁned as Φk= [Φ 1,k,Φ2,k,..., Φn,k]T.
As is evident from the update of the variables, the exchange between agents is limited to neighboring nodes,
and it encompasses the decision variable xk+1and the auxiliary variable yk+1.
By construction of Algorithm (8), we note that the mean of the auxiliary variable ykis equal to that of the
gradient estimate gkat every iteration ksince y0=g0, and by recursion, we obtain ¯yk=1
n1Tgk= ¯gk.
We next remark the eﬀect of the gradient estimate variance on the convergence by carefully examining the
steps in (8). Naturally, when the estimates have a large variance, the estimated gradients can vary widely
from one sample to another. This means that the norm of yk+1, which is directly aﬀected by this variance,
may also grow considerably. As xk+1itself is aﬀected by yk, it may then take longer to converge to the
optimal solution because it cannot reliably discern the direction of the steepest descent. In the worst case,
the huge variance causes instability as the optimizer may oscillate around the optimum or even diverge if
the variance is too high, making converging to a satisfactory solution diﬃcult. In this work, we use the
fact that the local functions and the noise variance are bounded to prove that the variance of gradient
estimate presented in (6) is indeed bounded. This boundedness, alongside the properties of the matrix Win
Assumption 1.1, allows us to ﬁnd then an upper bound on the variation of yk+1with respect to its mean at
every iteration. The latter result can be veriﬁed by inductive reasoning: The mean of the auxiliary variable
is already bounded, assume one iteration of ykto be bounded, then yk+1must be bounded. We provide all
the details in Appendices B, D.1, and D.2.
We then consider another zero-order stochastic consensus-based algorithm without gradient tracking we
denote as 1P-DSG employing again the gradient estimate gi,kin (6). Every agent iinitializes its variables
with an arbitrary valued vector xi,0∈Kcomputesgi,0at that variable. Then, at each time k∈N, agenti
updates its variables independently according to the following steps:
zi,k+1=n/summationdisplay
j=1wij(xj,k−αkgj,k)
xi,k+1= ΠK(zi,k+1)
perform the action: xi,k+1+γk+1Φi,k+1(9)
whereαk>0is again a step size. Algorithm (9) can then be written in the following compact matrix form
for clarity of analysis:
zk+1=W(xk−αkgk)
xk+1= [x1,k+1,x2,k+1,...,xn,k+1]T
perform the action: xk+1+γk+1Φk+1(10)
where Φk∈Rn×dis again deﬁned as Φk= [Φ 1,k,Φ2,k,..., Φn,k]T.
The following assumptions apply to both algorithms and the ﬁrst assumption is only taken into account
when we study the algorithms’ behavior with varying step sizes, otherwise it is dropped.
Assumption 2.1. (on the step-sizes) Both αkandγkvanish to 0ask→∞, and satisfy the the following
sums∞/summationdisplay
k=1αkγk=∞,∞/summationdisplay
k=1α2
k<∞,and∞/summationdisplay
k=1αkγ2
k<∞.
9Under review as submission to TMLR
Assumption 2.2. (on the random perturbation) Let Φi,k= (φ1
i,k,φ2
i,k,...,φd
i,k)T.
Each agent ichooses its Φi,kvector independently from other agents j/negationslash=i. In addition, the elements of Φi,k
are assumed i.i.d with E(φd1
i,kφd2
i,k) = 0ford1/negationslash=d2and there exists c3>0such that E(φdj
i,k)2=c3,∀dj,∀i,
almost surely. We further assume that there exists a constant c4>0where/bardblΦi,k/bardbl≤c4,∀i, almost surely.
Example 2.3. One example is to take αk=α0(k+ 1)−υ1andγk=γ0(k+ 1)−υ2with the constants α0,γ0,
υ1,υ2∈R+. As/summationtext∞
k=1αkγkdiverges for υ1+υ2≤1,/summationtext∞
k=1α2
kconverges for υ1>0.5,/summationtext∞
k=1αkγ2
kconverges
forυ1+ 2υ2>1, and/summationtext∞
k=1α4
kγ2
k(k+ 1)2converges for 2υ1+υ2>3
2, we can ﬁnd pairs of υ1andυ2so that
Assumption 2.1 is satisﬁed.
To achieve the conditions in Assumption 2.2, we can choose the probability distribution of φdj
i,kto be the
symmetrical Bernoulli distribution where φdj
i,k∈{−1√
d,1√
d}withP(φdj
i,k=−1√
d) =P(φdj
i,k=1√
d) = 0.5,∀dj,
∀i.
Assumption 2.4. (on the local functions) Kis a compact convex set and all local functions x/mapsto→fi(x,S)
are bounded on the c4γ0-neighborhood ofK, i.e.,
|fi(x,S)|<∞,∀x∈Nc4γ0(K),∀S∈Rm,∀i∈N,
whereNc4γ0(K) ={x∈Rd|infa∈K/bardblx−a/bardbl<c4γ0}is thec4γ0-neighborhood ofK.
3 The 1P-DSGT Algorithm With Gradient Tracking
In this section, we analyze Algorithm 1P-DSGT presented in (7) and (8).
3.1 Convergence Results
ThegoalofthispartistoanalyzetheasymptoticbehaviorofAlgorithm(8). Westarttheanalysisbydeﬁning
Hkas the history sequence {x0,y0,S0,...,xk−1,yk−1,Sk−1,xk}and denoting by E[.|Hk]as the conditional
expectation given Hk.
We deﬁne ˜gkto be the expected value of ¯gkwith respect to all the stochastic terms S,Φ,ζgivenHk, i.e.,
˜gk=ES,Φ,ζ[¯gk|Hk].
In what follows, we use ˜gk=E[¯gk|Hk]for shorthand notation.
We deﬁne the error ekto be the diﬀerence between the value of a single realization of ¯gkand its conditional
expectation ˜gk, i.e.,
ek= ¯gk−˜gk,
whereekcan be seen as a stochastic noise. The following lemma describing the vanishing of the stochastic
noise is essential for our main result.
Lemma 3.1. If all Assumptions 1.2, 1.3, 2.1, 2.2, and 2.4 hold, then for any constant ν >0, we have
P( lim
K→∞sup
K/prime≥K/bardblK/prime/summationdisplay
k=Kαkek/bardbl≥ν) = 0,∀ν >0.
Proof: See Appendix C.
For any integer k≥0, we deﬁne the divergence, or the error between the average action taken by the agents
¯xkand the optimal solution x∗as
dk=/bardbl¯xk−x∗/bardbl2. (11)
The following theorem describes the main convergence result.
Theorem 3.2. If all Assumptions 1.1-1.3, 2.1-2.2, and 2.4 hold, then as k→∞,dk→0,¯xk→x∗, and
xi,k→¯xk, for alli∈N, almost surely by applying the Algorithm.
Proof: See Appendix D.
10Under review as submission to TMLR
3.2 Convergence Rate with Vanishing Step Sizes
This part deals with how fast the expected divergence vanishes to ﬁnd the proposed algorithm’s expected
convergence rate. To do so, we deﬁne the expected divergence as
Dk=E[/bardbl¯xk−x∗/bardbl2].
The goal is to bound this divergence from above by sequences whose convergence rate is known. The analysis
is highly associated with the parameters αkandγkthat play a signiﬁcant role in determining this upper
bound. Hence, in what follows, the analysis starts with a general form of αkandγk, then a particular case
is considered.
3.2.1 General Form of αkandγk
We ﬁrst study the rate of convergence of the consensus error by introducing the following lemma.
Lemma 3.3. Let Assumptions 1.1-1.3, 2.1-2.2, and 2.4 hold. Deﬁne
K1= arg min
α2
k+1
α2
k>1+ρ2w
2k.
Then, fork≥K1, there exist 0<ϑ1,ϑ2<∞, such that
/bardblxk−1¯xk/bardbl2<ϑ2
1α2
kand/bardblzk+1−1¯xk/bardbl2≤ϑ2
2α2
k. (12)
Proof: Refer to Appendix D.3.
Our main result regarding the convergence rate is summarized in the following theorem.
Theorem 3.4. Let Assumptions 1.1-1.3, 2.1-2.2, and 2.4 hold. We then deﬁne the constants A=λc3,
B=2c3L2ϑ2
1
λn,C=c2
1c6
4
2c3λ,E=ϑ2
n,
K2= arg min
Aαkγk<1k,andK0= max{K1,K2}.
We ﬁnally deﬁne the following parameters:
κk=1−(γk+1
γk)2
αkγk, σ 1= max
k≥K0κk, σ 2= max
k≥K0α2
k
γ2
k, σ 3= max
k≥K0αk
γ3
k,
τk=1−αk+1γ−1
k+1
αkγ−1
k
αkγk, σ 4= max
k≥K0τk, σ 5= max
k≥K0αkγk, σ 6= max
k≥K0γ3
k
αk.(13)
Ifκk<Afor anyk≥K0, then
Dk≤ς1γ2
k,∀k≥K0, (14)
with
ς1≥max/braceleftBigg
DK0
γ2
K0,Bσ2+Eσ3+C
A−σ1/bracerightBigg
. (15)
Ifτk<Afor anyk≥K0, then
Dk≤ς2αk
γk,∀k≥K0, (16)
with
ς2≥max/braceleftBigg
DK0γK0
αK0,Bσ5+Cσ6+E
A−σ4/bracerightBigg
. (17)
Proof: See Appendix E.1.
11Under review as submission to TMLR
3.2.2 A Special Case of αkandγk
We now consider the special case mentioned in Example 2.3:
αk=α0(k+ 1)−υ1andγk=γ0(k+ 1)−υ2, (18)
where 0.5<υ1<1,0<υ2≤1−υ1, andυ1+ 2υ2>1.
Theorem 3.5. Letαkandγkhave the forms given in (18) and consider the same assumptions of Theorem
3.4. Ifα0γ0≥max{2υ2,υ1−υ2}/A, then we can say that there exists Υ<∞, where
Dk≤Υ(k+ 1)−min{2υ2,υ1−υ2},∀k≥K0.
Proof: See Appendix E.4.
The parameters clearly aﬀect the upper bound of the convergence rate or rate of expected divergence decay
in Theorem 3.5. As it is evident that
max{2υ2,υ1−υ2}≤0.5,
the best choice is when equality holds for υ1= 0.75andυ2= 0.25. With the suﬃcient condition on the
parameters in Theorem 3.5, we can ﬁnally state that our algorithm converges with a rate of O(1√
k)after a
suﬃcient number of iterations k>K 0when the step sizes are vanishing.
3.3 Regret Bound
To further examine the performance of our algorithm, we present the following theorem on the achieved
regret bound.
Theorem 3.6. Let the assumptions of Theorem 3.4 hold. When αkandγkhave the forms of (18) with
υ1= 0.75andυ2= 0.25, the regret bound is given by
E/bracketleftbigg1
nK/summationdisplay
k=1n/summationdisplay
i=1Fi(xi,k)−Fi(x∗)/bracketrightbigg
≤O(√
K).
Proof: See Appendix F.
3.4 Convergence Rate with Constant Step Sizes
In this subsection, we ﬁx the step sizes to αk=α > 0andγk=γ > 0,∀k≥0, and we assume them to
be two arbitrarily small values. We also deﬁne the following terms, A=λc3,B=2c3L2
λn,C=c2
1c6
4
2c3λ, and
R=/bardblx0−1¯x0/bardbl2. We letMandGdenote the upper bounds on /bardbl¯gk/bardbl2and/bardblyk−1¯yk/bardbl, respectively. We then
letG1=2(ρ2
wG2+nM)(1+ρ2
w)
(1−ρ2
w)2andG2= (ρ2
wG2+nM)/parenleftbigg/parenleftBig
1+ρ2
w
1−ρ2
w/parenrightBig2
+1+ρ2
w
1−ρ2
w/parenrightbigg
. We ﬁnally deﬁne /rho11= 1−Aαγ
and/rho12=1+ρ2
w
2.
Theorem 3.7. Assumeαγ <1
Aandα<γ. Let Assumptions 1.1-1.3, 2.2, and 2.4 hold, then
/bardblxk+1−1¯xk+1/bardbl2≤/rho1k+1
2R+α2G1and/bardblzk+1−1¯xk/bardbl2≤/rho1k+1
2R+α2G2. (19)
Meaning,/bardblxk+1−1¯xk+1/bardbl2converges with the linear rate of O/parenleftbig
/rho1k
2/parenrightbig
for an arbitrary small αalmost surely.
Further,
•When/rho11≤/rho12,
Dk+1≤/rho1k+1
1D0+/rho1k+1
22R/parenleftBig
Bαγ +/rho12
n/parenrightBig
2Aαγ +ρ2w−1+α2BG1
A+α
γG2
nA+γ2C
A.(20)
Then, for arbitrary small step sizes, Dkconverges with the linear rate of O/parenleftbig
/rho1k
2/parenrightbig
.
12Under review as submission to TMLR
•When/rho11>/rho12,
Dk+1≤/rho1k+1
1/parenleftBig
D0+2RBαγ +2R/rho12
n
1−2Aαγ−ρ2w/parenrightBig
+α2BG1
A+α
γG2
nA+γ2C
A. (21)
Then, for arbitrary small step sizes, Dkconverges with the linear rate of O/parenleftbig
/rho1k
1/parenrightbig
.
Proof: See Appendix G.
Taking arbitrarily small values of α,γsatisfyingαγ <1
Aandα < γ, and setting /rho1= max{/rho11,/rho12}, the
convergence rate becomes O(/rho1k), achieving the same rate as the gradient tracking technique with FO infor-
mation.
4 The 1P-DSG Algorithm Without Gradient Tracking
In this section, we analyze Algorithm 1P-DSG presented in (9). We prove that the previous convergence
results and rates are conserved for the same one-point estimator.
Theorem 4.1. Let all Assumptions 1.1-1.3, 2.1 (when the step sizes are vanishing), 2.2, and 2.4 hold.
Then, Algorithm (9) converges almost surely, and it achieves, in expectation, the rates of convergence
•O(1√
k)to the optimum for αk=α0(k+ 1)−0.75andγk=γ0(k+ 1)−0.25withα0γ0≥1
2λc3,
•O(/rho1k)to a neighborhood of the optimum for αk=αandγk=γwithαγ <1
λc3andα<γ.
Proof: See Appendix H.
5 Numerical Results
In this section, we provide numerical examples to illustrate the performance of the algorithms 1P-DSGT and
1P-DSG without GT. We compare them with FO distributed methods aiming to achieve consensus, namely
DSGT (Pu & Nedić, 2018) and EXTRA (Shi et al., 2015), and a ZO centralized algorithm based on gradient
descent (e.g. Flaxman et al. (2004) and Bach & Perchet (2016)) using another one-point estimate which is
presented in (1). We denote the ZO centralized algorithm by 1P-GD. For DSGT and EXTRA, we calculate
the exact gradient and add white noise to it to form an unbiased FO estimator. The network topology is a
connected Erdős-Rényi random graph with a probability of 0.05.
We consider a logistic classiﬁcation problem to classify mimages of the two digits, labeled as yij= +1or
−1from the MNIST data set (LeCun & Cortes, 2005). Each image, Xij, is a 785-dimensional vector and
is compressed using a lossy autoencoder to become 10-dimensional denoted as X/prime
ij, i.e.,d= 10. The total
images are split equally among the agents such that each agent has mi=m
nimages and no access to other
ones for privacy constraints. However, the goal is still to make use of all images and to solve collaboratively
min
θ∈K1
nn/summationdisplay
i=11
mmi/summationdisplay
j=1Eu∼N(1,σu)ln(1 + exp(−uijyij.X/primeT
ijθ)) +c/bardblθ/bardbl2,
while reaching consensus on the decision variable θ∈KwithK= [−10,10]d. We note here that umodels
some perturbation on the local querying of every example to add to the randomization of the communication
process.
We consider classifying the digits 1and2withm= 12700 images. There are n= 100agents in the network
and thus each has a local batch of mi= 127images. We take σu= 0.01and letαk= 0.05(k+ 1)−0.75and
γk= 0.8(k+ 1)−0.25for 1P-DSGT and 1P-DSG without GT with vanishing step sizes, and α= 0.05and
γ= 0.6with constant step sizes. We choose Φk∈{−1√
d,1√
d}dwith equal probability. Also, every function
query is subject to a white noise generated by the standard normal distribution. For the general DSGT
13Under review as submission to TMLR
algorithm, we set the step size to αk= 0.015(k+ 1)−1when it is vanishing and α= 0.015when constant,
and we do not consider the perturbation on the objective function nor the noise on the objective function,
only the noise on the exact gradient. Similarly for EXTRA and we set its step size to α= 0.01. For the
centralized 1P-GD algorithm, we set α= 0.001andγ= 0.5. We letc= 0.1, and the initialization be the
same for both algorithms, with θi,0uniformly chosen from [−0.5,0.5]d,∀i∈N, per instance. We ﬁnally
average the simulations over 30instances.
The expected evolution of the loss objective function is presented in Figure 1 and the graphs are zoomed in on
in Figure 2. Experimental results seem to validate our theoretical results: Our proposed algorithms converge
linearly fast with constant step sizes, however the ﬁnal gap is due to converging to an O(α)-neighborhood
of the optimal solution. 1P-DSGT and 1P-DSG with vanishing step sizes converge with an O(1√
k)while
DSGT with vanishing step size converges at a rate of O(1
k)(Pu & Nedić, 2018). Using constant vs vanishing
step size does not seem to aﬀect the convergence rate of the loss function of DSGT. EXTRA consistently
performs similarly to DSGT. The most interesting point is that 1P-DSGT and 1P-DSG, with vanishing and
constant step sizes, outperform the centralized ZO counterpart 1P-GD to an impressive extent highlighting
the signiﬁcance of our proposed estimate and methods. We also note that 1P-GD ﬂuctuates a lot due to
the division by the small value γin the gradient estimate, which causes instability and diﬃculty in tuning
generally. Finally, the gradient tracking step diﬀerentiating 1P-DSGT and 1P-DSG seems to have a minor
eﬀect on the convergence rate between the two algorithms, as both perform consistently and similarly well.
Figure 1: Expected loss function evolution of the
proposed algorithms vs. DSGT, EXTRA, and
1P-GD considering vanishing vs. constant step
sizes.
Figure 2: Expected loss function evolution of the
proposed algorithms vs. DSGT, EXTRA, and
1P-GD considering vanishing vs. constant step
sizes.
In Figure 3, we measure at every iteration the classiﬁcation accuracy against an independent test set of
2167images using the updated mean vector ¯θk=1
n/summationtextn
i=1θi,kof the local decision variables. The interest
of the constant step sizes appears in the convergence rate of this accuracy, where our algorithms 1P-DSGT
and 1P-DSG are able to compete with DSGT with full FO information, and to outperform DSGT with a
vanishing step size. This is an important result as it shows that the classiﬁcation goal with ZO is well met
despite the limiting upper bounds of convergence rate and that O(α)-neighborhood of the optimal solution
achieved linearly fast can be suﬃcient to achieve the best possible accuracy.
The reason for this better accuracy attainment is generally because the constant step-size version of a gradi-
ent method often achieves better generalization performance due to its ability to ﬁnd a good balance between
exploration and exploitation during the optimization process. In other words, it allows the algorithm to ex-
plore a wide range of parameter values while still exploiting gradients to make consistent progress toward the
optimal solution. This balance enhances the model’s ability to capture the underlying structure of the data
and avoid overﬁtting. While a vanishing step size version also oﬀers this balance at the beginning, as the step
sizes become smaller, the exploration is substituted for exploitation, causing worse/slower generalization.
14Under review as submission to TMLR
Figure3: Expectedtestaccuracyevolutionofthe
proposed algorithms vs. DSGT, EXTRA, and
1P-GD considering vanishing vs. constant step
sizes.This is well conﬁrmed by the centralized 1P-GD
vs 1P-DSGT (and 1P-DSG) with vanishing step
sizes. Despite the latter outperforming the ﬁrst
in convergence speed (of the objective function),
the ﬁrst with constant step sizes seems to gener-
alize better.
In Figures 4, 5, and 6 the curves are those of
the evolution of the expected consensus error, or
E/bracketleftbig/summationtextn
i=1/bardblθi,k−¯θk/bardbl2/bracketrightbig
which is the expected er-
ror between the local decision variables and their
average. For all algorithms, the error again vali-
dates the theoretical bounds and decreases quite
fast. Generally, asevidentinFigure6forallalgo-
rithms, vanishing step sizes allow the consensus
error to completely vanish while constant step
sizes leave an O(α2)-gap.
Figures 7, 8, and 9 show the evolution of the expected gradient tracking error, or E/bracketleftbig
/bardblyk−1¯yk/bardbl2/bracketrightbig
(replaced by
E/bracketleftbig
/bardblgk−1¯gk/bardbl2/bracketrightbig
for 1P-DSG and EXTRA) which is the expected error between the auxiliary local variables
(agents’ gradients/estimates for 1P-DSG and EXTRA) and their average. Despite not knowing the theoret-
ical constants that bound the tracking error of all these algorithms, we see that the ﬁnal error attained by
the ZO gradient is much smaller than that of the FO one no matter the step sizes. This shows in a sense
that the ZO gradient is "easier to track" across network agents.
We add other numerical examples for diﬀerent image labels in Appendix I.
Figure 4: Expected consensus error evolution of
the proposed algorithms vs. DSGT and EXTRA
considering vanishing vs. constant step sizes.
Figure 5: Expected consensus error evolution of
the proposed algorithms vs. DSGT and EXTRA
considering vanishing vs. constant step sizes.
6 Conclusion
In this work, we extended the gradient-tracking algorithm to present a practical solution to a relevant
problem with realistic assumptions. A distributed stochastic gradient-tracking algorithm was studied and
proved to converge with a biased and high variance one-point gradient estimate and a stochastic perturbation
on the objective function. In the context of noisy ZO optimization, we have successfully established a linear
convergence rate of O(/rho1k)using ﬁxed step sizes and O(1√
k)with vanishing step sizes. We then extend
these rates to another distributed ZO method, which suggests that these rates can be extended to other ZO
methods. These rates align with the optimal expectations examined in the existing literature. We also prove
15Under review as submission to TMLR
a regret bound that of O(√
k)with vanishing step sizes. A numerical application conﬁrmed the success and
eﬃciency of the algorithms.
Figure 6: Expected consensus error evolution of
the algorithms.
Figure 7: Expected gradient tracking error evo-
lution of the proposed algorithms vs. DSGT and
EXTRA considering vanishing vs. constant step
sizes.
Figure 8: Expected gradient tracking error evo-
lution of the proposed algorithms vs. DSGT and
EXTRA considering vanishing vs. constant step
sizes.
Figure 9: Expected gradient tracking error evo-
lution of the algorithms.
References
Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with multi-
point bandit feedback. In COLT, 2010.
Francis Bach and Vianney Perchet. Highly-smooth zero-th order online optimization vianney perchet, 2016.
URL https://arxiv.org/abs/1605.08165 .
Sébastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex optimization. J.
ACM,68(4), jun2021. ISSN0004-5411. doi: 10.1145/3453721. URL https://doi.org/10.1145/3453721 .
Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, and David Cox. Zo-adamm: Zeroth-
order adaptive momentum method for black-box optimization. In NeurIPS , 2019.
16Under review as submission to TMLR
YingChen, ZhiyongLiu, YongchaoZhang, YuanWu, XinChen, andLianZhao. Deepreinforcementlearning-
based dynamic resource management for mobile edge computing in industrial internet of things. IEEE
Transactions on Industrial Informatics , 17(7):4925–4934, 2021. doi: 10.1109/TII.2020.3028963.
Amir Daneshmand, Francisco Facchinei, Vyacheslav Kungurtsev, and Gesualdo Scutari. Hybrid ran-
dom/deterministicparallelalgorithmsforconvexandnonconvexbigdataoptimization. IEEE Transactions
on Signal Processing , 63(15):3914–3929, 2015. doi: 10.1109/TSP.2015.2436357.
Joseph L. Doob. Stochastic processes. 1953.
John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for zero-order
convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory ,
61(5):2788–2806, 2015. doi: 10.1109/TIT.2015.2409256.
Abraham Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the
bandit setting: gradient descent without a gradient. CoRR, cs.LG/0408007, 2004. URL http://arxiv.
org/abs/cs.LG/0408007 .
Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi (Richard) Zhang.
Gradientless descent: High-dimensional zeroth-order optimization. CoRR, abs/1911.06317, 2019. URL
http://arxiv.org/abs/1911.06317 .
Eduard Gorbunov, Pavel Dvurechensky, and Alexander Gasnikov. An accelerated method for derivative-free
smooth stochastic convex optimization, 2018. URL https://arxiv.org/abs/1802.09022 .
Davood Hajinezhad, Mingyi Hong, and Alfredo Garcia. Zone: Zeroth-order nonconvex multiagent op-
timization over networks. IEEE Transactions on Automatic Control , 64(10):3995–4010, 2019. doi:
10.1109/TAC.2019.2896025.
Elad Hazan and Kﬁr Levy. Bandit convex optimization: Towards tight bounds. In Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems , volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/
paper_files/paper/2014/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf .
Shinji Ito. An optimal algorithm for bandit convex optimization with strongly-convex and smooth loss. In
Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference
on Artiﬁcial Intelligence and Statistics , volume 108 of Proceedings of Machine Learning Research , pp.
2229–2239. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/ito20a.html .
Kevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query complexity of derivative-free optimiza-
tion. InNIPS, 2012.
Xia Jiang, Xianlin Zeng, Jian Sun, and Jie Chen. Distributed stochastic gradient tracking algorithm with
variance reduction for non-convex optimization. IEEE Transactions on Neural Networks and Learning
Systems, pp. 1–12, 2022. doi: 10.1109/TNNLS.2022.3170944.
DavidKinderlehrerandGuidoStampacchia. Anintroductiontovariationalinequalitiesandtheirapplication.
31, 01 2000. doi: 10.1137/1.9780898719451.
Anit Kumar Sahu, Dusan Jakovetic, Dragana Bajovic, and Soummya Kar. Distributed zeroth order opti-
mization over random networks: A kiefer-wolfowitz stochastic approximation approach. In 2018 IEEE
Conference on Decision and Control (CDC) , pp. 4951–4958, 2018. doi: 10.1109/CDC.2018.8619044.
Yann LeCun and Corinna Cortes. The mnist database of handwritten digits. 2005.
Huaqing Li, Lifeng Zheng, Zheng Wang, Yu Yan, Liping Feng, and Jing Guo. S-diging: A stochastic gradient
tracking algorithm for distributed optimization. IEEE Transactions on Emerging Topics in Computational
Intelligence , 6(1):53–65, 2022. doi: 10.1109/TETCI.2020.3017242.
17Under review as submission to TMLR
Wenjie Li and Mohamad Assaad. Distributed stochastic optimization in networks with low informational
exchange. IEEE Transactions on Information Theory , 67(5):2989–3008, 2021. doi: 10.1109/TIT.2021.
3064925.
Sijia Liu, Jie Chen, Pin-Yu Chen, and Alfred Hero. Zeroth-order online alternating direction method of
multipliers: Convergence analysis and applications. In Amos Storkey and Fernando Perez-Cruz (eds.),
Proceedings of the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics , vol-
ume 84 of Proceedings of Machine Learning Research , pp. 288–297. PMLR, 09–11 Apr 2018. URL
https://proceedings.mlr.press/v84/liu18a.html .
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signsgd via zeroth-order oracle. In ICLR, 2019.
Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O. Hero III, and Pramod K. Varshney. A
primeronzeroth-orderoptimizationinsignalprocessingandmachinelearning: Principals, recentadvances,
and applications. IEEE Signal Processing Magazine , 37(5):43–54, 2020. doi: 10.1109/MSP.2020.3003837.
Paolo Di Lorenzo and Gesualdo Scutari. Next: In-network nonconvex optimization. IEEE Transactions on
Signal and Information Processing over Networks , 2(2):120–136, 2016. doi: 10.1109/TSIPN.2016.2524588.
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong. Gnsd: a gradient-tracking based nonconvex
stochastic algorithm for decentralized optimization. In 2019 IEEE Data Science Workshop (DSW) , pp.
315–321, 2019. doi: 10.1109/DSW.2019.8755807.
Yuyi Mao, Changsheng You, Jun Zhang, Kaibin Huang, and Khaled B. Letaief. A survey on mobile edge
computing: The communication perspective. IEEE Communications Surveys Tutorials , 19(4):2322–2358,
2017. doi: 10.1109/COMST.2017.2745201.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-Eﬃcient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry
Zhu (eds.), Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics , vol-
ume 54 of Proceedings of Machine Learning Research , pp. 1273–1282. PMLR, 20–22 Apr 2017. URL
https://proceedings.mlr.press/v54/mcmahan17a.html .
Angelia Nedić, Alex Olshevsky, and Wei Shi. Achieving geometric convergence for distributed opti-
mization over time-varying graphs. SIAM Journal on Optimization , 27(4):2597–2633, 2017. doi:
10.1137/16M1084316. URL https://doi.org/10.1137/16M1084316 .
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic
programming. SIAM Journal on Optimization , 19(4):1574–1609, 2009. doi: 10.1137/070704277. URL
https://doi.org/10.1137/070704277 .
Yurii Nesterov and Vladimir G. Spokoiny. Random gradient-free minimization of convex functions. Foun-
dations of Computational Mathematics , 17:527–566, 2017.
Shi Pu. A robust gradient tracking method for distributed optimization over directed networks. In 2020
59th IEEE Conference on Decision and Control (CDC) , pp. 2335–2341, 2020. doi: 10.1109/CDC42340.
2020.9303917.
Shi Pu and Angelia Nedić. Distributed stochastic gradient tracking methods, 2018. URL https://arxiv.
org/abs/1805.11454 .
Guannan Qu and Na Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions
on Control of Network Systems , 5(3):1245–1260, 2018. doi: 10.1109/TCNS.2017.2698261.
Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In Shai
Shalev-Shwartz and Ingo Steinwart (eds.), Proceedings of the 26th Annual Conference on Learning Theory ,
volume 30 of Proceedings of Machine Learning Research , pp. 3–24, Princeton, NJ, USA, 12–14 Jun 2013.
PMLR. URL https://proceedings.mlr.press/v30/Shamir13.html .
18Under review as submission to TMLR
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact ﬁrst-order algorithm for decentralized
consensus optimization. SIAM Journal on Optimization , 25(2):944–966, 2015. doi: 10.1137/14096668X.
URL https://doi.org/10.1137/14096668X .
Zai Shi and Atilla Eryilmaz. A zeroth-order admm algorithm for stochastic optimization over distributed
processing networks. In IEEE INFOCOM 2020 - IEEE Conference on Computer Communications , pp.
726–735, 2020. doi: 10.1109/INFOCOM41043.2020.9155520.
Yujie Tang, Junshan Zhang, and Na Li. Distributed zero-order algorithms for nonconvex multiagent opti-
mization. IEEE Transactions on Control of Network Systems , 8(1):269–281, 2021. doi: 10.1109/TCNS.
2020.3024321.
Anirudh Vemula, Wen Sun, and J. Andrew Bagnell. Contrasting exploration in parameter and action space:
A zeroth-order optimization perspective. ArXiv, abs/1901.11503, 2019.
Ran Xin, Anit Kumar Sahu, Usman A. Khan, and Soummya Kar. Distributed stochastic optimization
with gradient tracking over strongly-connected networks. In 2019 IEEE 58th Conference on Decision and
Control (CDC) , pp. 8353–8358, 2019. doi: 10.1109/CDC40024.2019.9029217.
Fanqin Zhou, Lei Feng, Michel Kadoch, Peng Yu, Wenjing Li, and Zhili Wang. Multiagent rl aided task
oﬄoading and resource management in wi-ﬁ 6 and 5g coexisting industrial wireless environment. IEEE
Transactions on Industrial Informatics , 18(5):2923–2933, 2022. doi: 10.1109/TII.2021.3106973.
A L-Smoothness Property
/bardbl∇F (¯x)−h(x)/bardbl=/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1/parenleftBig
∇Fi(¯x)−∇Fi(xi)/parenrightBig/vextenddouble/vextenddouble/vextenddouble
≤1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble∇Fi(¯x)−∇Fi(xi)/vextenddouble/vextenddouble/vextenddouble
≤L
nn/summationdisplay
i=1/bardbl¯x−xi/bardbl
=L
nn/summationdisplay
i=1/bardblxi−¯x/bardbl
(a)
≤L
n/radicaltp/radicalvertex/radicalvertex/radicalbtnn/summationdisplay
i=1/bardblxi−¯x/bardbl2
(b)=L√n/bardblx−1¯x/bardbl,
where (a)is by applying the Cauchy-Schwarz inequality, |/summationtextn
i=1ai·1| ≤ (/summationtextn
i=1a2
i)1
2·(/summationtextn
i=112)1
2=
n1
2(/summationtextn
i=1a2
i)1
2, and (b)is by deﬁnition of the Frobenius norm, /bardblx−1¯x/bardbl2=/summationtextn
i=1/bardblxi−¯x/bardbl2.
B Estimated Gradient
In this section, we derive the bias of the gradient estimate with respect to the real gradient of the local
objective function. Let
˘gi,k=ES,Φ,ζ[gi,k|Hk].
Thus, by Assumption 1.3 and the deﬁnition in (4),
˘gi,k=ES,Φ,ζ[Φi,k(fi(xi,k+γkΦi,k,Si,k) +ζi,k)|Hk]
=ES,Φ[Φi,kfi(xi,k+γkΦi,k,Si,k)|Hk]
=EΦ[Φi,kFi(xi,k+γkΦi,k)|Hk].
19Under review as submission to TMLR
By Taylor’s theorem and the mean-valued theorem, there exists ˜xi,klocated between xi,kandxi,k+γkΦi,k
where
Fi(xi,k+γkΦi,k) =Fi(xi,k) +γk/angbracketleftΦi,k,∇Fi(xi,k)/angbracketright+γ2
k
2/angbracketleftΦi,k,∇2Fi(˜xi,k)Φi,k/angbracketright,
substituting in the previous deﬁnition,
˘gi,k=Fi(xi,k)EΦ[Φi,k] +γkEΦ[Φi,kΦT
i,k]∇Fi(xi,k) +γ2
k
2EΦ[Φi,kΦT
i,k∇2Fi(˜xi,k)Φi,k|Hk]
=c3γk[∇Fi(xi,k) +bi,k].
Thus, the estimation bias has the form
bi,k=˘gi,k
c3γk−∇Fi(xi,k)
=γk
2c3EΦ[Φi,kΦT
i,k∇2Fi(˜xi,k)Φi,k|Hk].
Let Assumptions 1.2 and 2.2 hold. Then, we can bound the bias as
/bardblbi,k/bardbl≤γk
2c3EΦ[/bardblΦi,k/bardbl2/bardblΦT
i,k/bardbl2/bardbl∇2Fi(˜xi,k)/bardbl2/bardblΦi,k/bardbl2|Hk]
≤γkc3
4c1
2c3.
We can see/bardblbi,k/bardbl→0ask→∞sinceγkis vanishing. We remark that
˜gk=E[¯gk|Hk]
=1
nn/summationdisplay
i=1E[gi,k|Hk]
=1
nn/summationdisplay
i=1c3γk[∇Fi(xi,k) +bi,k]
=c3γk[h(xk) +¯bk](22)
is also a biased estimator of h(xk)with
/bardbl¯bk/bardbl=/bardbl1
nn/summationdisplay
i=1bi,k/bardbl
≤1
nn/summationdisplay
i=1/bardblbi,k/bardbl
≤1
nn/summationdisplay
i=1γkc3
4c1
2c3
=γkc3
4c1
2c3.(23)
Lemma B.1. Let all Assumptions 1.3, 2.2, and 2.4 hold, then there exists a bounded constant ¯M > 0, such
thatE[/bardbl¯gk/bardbl2|Hk]<¯M.
Proof.∀i∈N, we have
E[/bardblgi,k/bardbl2|Hk] =E[/bardblΦi,k(fi(xi,k+γkΦi,k,Si,k) +ζi,k)/bardbl2|Hk]
=E[/bardblΦi,k/bardbl2/bardblfi(xi,k+γkΦi,k,Si,k) +ζi,k/bardbl2|Hk]
(a)
≤c2
4E[(fi(xi,k+γkΦi,k,Si,k) +ζi,k)2|Hk]
(b)=c2
4E[f2
i(xi,k+γkΦi,k,Si,k)|Hk] +c2
4c2
(f)
<∞,
20Under review as submission to TMLR
where (a)is due to Assumption 2.2, (b)Assumption 1.3, and (c)Assumption 2.4.
Then, E[/bardblgk/bardbl2|Hk] =E/bracketleftBig/summationtextn
i=1/bardblgi,k/bardbl2/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig
=/summationtextn
i=1E[/bardblgi,k/bardbl2|Hk]<∞and
E[/bardbl¯gk/bardbl2|Hk] =E/bracketleftBig
/bardbl1
nn/summationdisplay
i=1gi,k/bardbl2/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig
=1
n2E/bracketleftBig
/bardbln/summationdisplay
i=1gi,k/bardbl2/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig
≤n
n2E/bracketleftBign/summationdisplay
i=1/bardblgi,k/bardbl2/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig
=1
nn/summationdisplay
i=1E[/bardblgi,k/bardbl2|Hk]
<∞.
C Stochastic Noise
To prove Lemma 3.1, we begin by demonstrating that the sequence {/summationtextK/prime
k=Kαkek}K/prime≥Kis a martingale. To
do so, we have to prove that for all K/prime≥K,XK/prime=/summationtextK/prime
k=Kαkeksatisﬁes the following two conditions:
(i)E[XK/prime+1|XK/prime] =XK/prime
(ii)E[/bardblXK/prime/bardbl2]<∞
We know that
E[ek] =E[¯gk−E[¯gk|Hk]] =EHk/bracketleftBig
E/bracketleftBig
¯gk−E[¯gk|Hk]/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig/bracketrightBig
= 0
by the law of total expectation. Hence,
E[XK/prime+1|XK/prime] =E/bracketleftBig
αK/prime+1eK/prime+1+K/prime/summationdisplay
k=Kαkek/vextendsingle/vextendsingle/vextendsingleK/prime/summationdisplay
k=Kαkek/bracketrightBig
= 0 +K/prime/summationdisplay
k=Kαkek=XK/prime. (24)
In addition, ekandek/primeare uncorrelated for any k/negationslash=k/primesince (assuming k>k/prime)E/bracketleftbig
eT
kek/prime/bracketrightbig
=E/bracketleftbig
E[eT
kek/prime|Hk]/bracketrightbig
=
E/bracketleftbig
ek/primeE[eT
k|Hk]/bracketrightbig
= 0. Thus,
E(/bardblK/prime/summationdisplay
k=Kαkek/bardbl2) =E(K/prime/summationdisplay
k=KK/prime/summationdisplay
k/prime=Kαkαk/prime/angbracketleftek,ek/prime/angbracketright)
(a)=E(K/prime/summationdisplay
k=K/bardblαkek/bardbl2)
≤∞/summationdisplay
k=KE(α2
k/bardbl¯gk−E[¯gk|Hk]/bardbl2)
=∞/summationdisplay
k=Kα2
kE(/bardbl¯gk/bardbl2)−EHk(/bardblE[¯gk|Hk]/bardbl2)
≤∞/summationdisplay
k=Kα2
kE(/bardbl¯gk/bardbl2)
(b)
≤M∞/summationdisplay
k=Kα2
k(c)
<∞,(25)
21Under review as submission to TMLR
where (a)is due to the uncorrelatedness E[/angbracketleftek,ek/prime/angbracketright] = 0,(b)is by Lemma B.1, and (c)is by Assumption
2.1. Therefore, both (i) and (ii) are satisﬁed and we can say that {/summationtextK/prime
k=Kαkek}K/prime≥Kis a martingale. This
permits us to use Doob’s martingale inequality Doob (1953):
For any constant ν >0,
P( sup
K/prime≥K/bardblK/prime/summationdisplay
k=Kαkek/bardbl≥ν)≤1
ν2E(/bardblK/prime/summationdisplay
k=Kαkek/bardbl2)
(a)
≤M
ν2∞/summationdisplay
k=Kα2
k,(26)
where (a)is following the exact same steps as (25).
SinceMis a bounded constant and limK→∞/summationtext∞
k=Kα2
k= 0by Assumption 2.1, we get
limK→∞M
ν2/summationtext∞
k=Kα2
k= 0for any bounded constant ν. Hence, the probability that /bardbl/summationtextK/prime
k=Kαkek/bardbl≥ν
also vanishes as K→∞, which concludes the proof.
D Proof of Convergence
We start by stating the following lemma that will be useful for the proof of convergence.
Lemma D.1. If all Assumptions 1.1-1.3, 2.1-2.2, and 2.4 hold, then limk→∞/bardblxk−1¯xk/bardbl2= 0. In fact, we
have
∞/summationdisplay
k=0/bardblxk−1¯xk/bardbl2<∞,∞/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2<∞,and∞/summationdisplay
k=0γkαk/bardblxk−1¯xk/bardbl<∞,
almost surely.
Proof: See Appendix D.2.
D.1 Proof of Theorem 3.2
We know that ¯yk= ¯gksince
¯yk+1=1
n1T(Wyk+gk+1−gk)(a)=1
n1T(yk+gk+1−gk) = ¯yk+ ¯gk+1−¯gk, (27)
where (a)is due to the doubly stochastic property of Win Assumption 1.1.
Setting y0=g0gives ¯y0= ¯g0. Assuming ¯yk= ¯gkfor anyk≥0, we get ¯yk+1= ¯yk+ ¯gk+1−¯gk=
¯gk+ ¯gk+1−¯gk= ¯gk+1, proved by induction.
This allows us to write
¯zk+1=1
n1TW(xk−αkyk)(a)=1
n1T(xk−αkyk) = ¯xk−αk¯gk, (28)
where (a)is again due to the doubly stochastic property of W.
22Under review as submission to TMLR
We again write the divergence at time k+ 1can be written as
dk+1=/bardbl¯xk+1−x∗/bardbl2
=/bardbl1
nn/summationdisplay
i=1(xi,k+1−x∗)/bardbl2
≤n
n2n/summationdisplay
i=1/bardblxi,k+1−x∗/bardbl2
(a)
≤1
nn/summationdisplay
i=1/bardblzi,k+1−x∗/bardbl2
=1
nn/summationdisplay
i=1/bardblzi,k+1−¯xk+ ¯xk−x∗/bardbl2
=1
nn/summationdisplay
i=1/bardblzi,k+1−¯xk/bardbl2+ 21
nn/summationdisplay
i=1/angbracketleftzi,k+1−¯xk,¯xk−x∗/angbracketright+1
nn/summationdisplay
i=1/bardbl¯xk−x∗/bardbl2
=1
n/bardblzk+1−1¯xk/bardbl2+ 2/angbracketleft¯zk+1−¯xk,¯xk−x∗/angbracketright+/bardbl¯xk−x∗/bardbl2
(b)=1
n/bardblzk+1−1¯xk/bardbl2+ 2/angbracketleft−αk¯gk,¯xk−x∗/angbracketright+dk
=dk−2αk/angbracketleft¯xk−x∗,¯gk−E[¯gk|Hk] +E[¯gk|Hk]/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2
=dk−2αk/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright−2αk/angbracketleft¯xk−x∗,ek/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2
(c)=dk−2c3γkαk/angbracketleft¯xk−x∗,h(xk) +¯bk/angbracketright−2αk/angbracketleft¯xk−x∗,ek/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2
=dk−2c3γkαk/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright+ 2c3γkαk/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright
−2c3γkαk/angbracketleft¯xk−x∗,¯bk/angbracketright−2αk/angbracketleft¯xk−x∗,ek/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2
(d)
≤dk−2c3γkαk/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright+2c3Lγkαk√n/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl
+ 2c3γkαk/bardbl¯xk−x∗/bardbl/bardbl¯bk/bardbl−2αk/angbracketleft¯xk−x∗,ek/angbracketright+1
n/bardblzk+1−1¯xk/bardbl2,(29)
where (a)is by the projection inequality (5) noting that x∗∈K(so projecting it onto Kgives us the same
point), (b)is by (28), (c)is due to (22), and (d)is due to Lemma 1.5.
By recursion of inequality (29), we have
dK+1≤d0−2c3K/summationdisplay
k=0γkαk/angbracketleft¯xk−x∗,∇F(¯xk) +¯bk/angbracketright+2c3L√nK/summationdisplay
k=0γkαk/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl
+ 2c3K/summationdisplay
k=0γkαk/bardbl¯xk−x∗/bardbl/bardbl¯bk/bardbl−2K/summationdisplay
k=0αk/angbracketleft¯xk−x∗,ek/angbracketright+1
nK/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2.(30)
By Lemma 3.1, we have limK→∞/bardbl/summationtextK
k=0αkek/bardbl<∞almost surely. Since /bardbl¯xk−x∗/bardbl<∞by the compactness
ofKin Assumption 2.4, hence
lim
K→∞/bardblK/summationdisplay
k=0αk/angbracketleft¯xk−x∗,ek/angbracketright/bardbl<∞. (31)
From (43) in Lemma D.1, we have
lim
K→∞K/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2<∞. (32)
23Under review as submission to TMLR
As stated in Lemma D.1, we have/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl<∞, adding to/bardbl¯xk−x∗/bardbl<∞by Assumption
2.4, then
lim
K→∞K/summationdisplay
k=0γkαk/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl<∞. (33)
By (23), we know that /bardbl¯bk/bardbl≤c3
4c1
2c3γkand/bardbl¯xk−x∗/bardbl<∞by Assumption 2.4,
lim
K→∞K/summationdisplay
k=0γ2
kαk/bardbl¯xk−x∗/bardbl<∞, (34)
by Assumption 2.1.
From the above inequalities (30)-(34), we see that there exists 0<D/prime<∞such thatdK+1≤D/prime+zK, with
zKdeﬁned as
zK=−2c3K/summationdisplay
k=0γkαk/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright. (35)
By the strong convexity, we have
−/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright≤−λ/bardbl¯xk−x∗/bardbl2≤0. (36)
Thus,zK≤0, conﬁrming dK+1<∞.
Let’s assume that ∀/epsilon1h>0,∃Khsuch that/bardbl¯xk−x∗/bardbl2>/epsilon1hfork≥Kh, meaning
lim
K→∞−K/summationdisplay
k=Khγkαk/bardbl¯xk−x∗/bardbl2<−/epsilon1hlim
K→∞K/summationdisplay
k=Khγkαk<−∞, (37)
since/summationtext
kαkγkdiverges by Assumption 2.1. However, this implies that zK<−∞and as a consequence,
dK+1≤D/prime+zK<−∞which is a contradiction as dK+1≥0. We conclude that limk→∞dk= 0and
limk→∞¯xk=x∗, almost surely.
D.2 Proof of Lemma D.1
The goal is to bound /bardblxk+1−1¯xk+1/bardbl2by/bardblxk−1¯xk/bardbl2and other vanishing terms.
/bardblxk+1−1¯xk+1/bardbl2=/bardblxk+1−1¯xk+1¯xk−1¯xk+1/bardbl2
=/bardblxk+1−1¯xk/bardbl2+ 2/angbracketleftxk+1−1¯xk,1¯xk−1¯xk+1/angbracketright+/bardbl1¯xk−1¯xk+1/bardbl2
(a)=/bardblxk+1−1¯xk/bardbl2−/bardbl1¯xk−1¯xk+1/bardbl2
≤/bardblxk+1−1¯xk/bardbl2
=n/summationdisplay
i=1/bardblxi,k+1−¯xk/bardbl2
(b)
≤n/summationdisplay
i=1/bardblzi,k+1−¯xk/bardbl2
=/bardblzk+1−1¯xk/bardbl2
=/bardblWxk−αkWyk−1¯xk/bardbl2
=/bardblWxk−1¯xk/bardbl2−2αk/angbracketleftWxk−1¯xk,Wyk/angbracketright+α2
k/bardblWyk/bardbl2
(c)
≤/bardblWxk−1¯xk/bardbl2+αk[1−ρ2
w
2ρ2wαk/bardblWxk−1¯xk/bardbl2+2ρ2
wαk
1−ρ2w/bardblWyk/bardbl2] +α2
k/bardblWyk/bardbl2
24Under review as submission to TMLR
(d)
≤ρ2
w/bardblxk−1¯xk/bardbl2+αk[1−ρ2
w
2αk/bardblxk−1¯xk/bardbl2+2ρ2
wαk
1−ρ2w/bardblWyk/bardbl2] +α2
k/bardblWyk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWyk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWyk−1¯yk+1¯yk/bardbl2
(e)=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWyk−1¯yk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2
≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kρ2
w(1 +ρ2
w)
1−ρ2w/bardblyk−1¯yk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2(38)
where (a)is by (39), (b)is the projection inequality (5) noting that ¯xk∈KsinceKis a convex set (so
projecting it onto Kgives us the same point), (c)is by−2/epsilon1×1
/epsilon1/angbracketlefta,b/angbracketright=−2/angbracketleft/epsilon1a,1
/epsilon1b/angbracketright≤/epsilon12/bardbla/bardbl2+1
/epsilon12/bardblb/bardbl2(d)is
by Lemma 1.4, and (e)is by (40).
2/angbracketleftxk+1−1¯xk,1¯xk−1¯xk+1/angbracketright=2n/summationdisplay
i=1/angbracketleftxi,k+1−¯xk,¯xk−¯xk+1/angbracketright
=2/angbracketleftn/summationdisplay
i=1(xi,k+1−¯xk),¯xk−¯xk+1/angbracketright
=2/angbracketleftn(¯xk+1−¯xk),¯xk−¯xk+1/angbracketright
=−2n/angbracketleft¯xk−¯xk+1,¯xk−¯xk+1/angbracketright
=−2n/bardbl¯xk−¯xk+1/bardbl2
=−2/bardbl1¯xk−1¯xk+1/bardbl2.(39)
/angbracketleftWyk−1¯yk,1¯yk/angbracketright=n/summationdisplay
i=1/angbracketleftn/summationdisplay
j=1wijyj,k−¯yk,¯yk/angbracketright
=/angbracketleftn/summationdisplay
i=1n/summationdisplay
j=1wijyj,k−n¯yk,¯yk/angbracketright
=/angbracketleftn/summationdisplay
j=1(n/summationdisplay
i=1wij)yj,k−n¯yk,¯yk/angbracketright
=/angbracketleftn/summationdisplay
j=1yj,k−n¯yk,¯yk/angbracketright
=0.(40)
From Lemma B.1, we know that /bardbl¯gk/bardbl2≤M <∞almost surely. We now bound /bardblyk−1¯yk/bardbl2. By repeatedly
replacing the variables with the algorithm’s iterations, we see that
yk+1=Wyk+gk+1−gk
=W(Wyk−1+gk−gk−1) +gk+1−gk
=W2yk−1−Wgk−1+ (W−I)gk+gk+1
=W2(Wyk−2+gk−1−gk−2)−Wgk−1+ (W−I)gk+gk+1
=W3yk−2−W2gk−2+W(W−I)gk−1+ (W−I)gk+gk+1
=W3(Wyk−3+gk−2−gk−3)−W2gk−2+W(W−I)gk−1+ (W−I)gk+gk+1
25Under review as submission to TMLR
=W4yk−3−W3gk−3+W2(W−I)gk−2+W(W−I)gk−1+ (W−I)gk+gk+1
=...
=Wk+1y0−Wkg0+k−1/summationdisplay
j=0Wj(W−I)gk−j+gk+1
=Wk(W−I)g0+k−1/summationdisplay
j=0Wj(W−I)gk−j+gk+1
=k/summationdisplay
j=0Wj(W−I)gk−j+gk+1,
yk−1¯yk=k−1/summationdisplay
j=0Wj(W−I)gk−1−j+gk−k−1/summationdisplay
j=01
n11TWj(W−I)gk−1−j−1
n11Tgk
=k−1/summationdisplay
j=0Wj(W−I)gk−1−j+gk−k−1/summationdisplay
j=01
n11T(W−I)gk−1−j−1
n11Tgk
=k−1/summationdisplay
j=0(Wj−1
n11T)(W−I)gk−1−j+gk−1
n11Tgk
=k−1/summationdisplay
j=0(W−1
n11T)j(W−I)gk−1−j+gk−1¯gk,
where the last equality can be proven by recursion and the fact that the matrix Wis doubly stochastic by
Assumption 1.1:
(W−1
n11T)j+1= (Wj−1
n11T)(W−1
n11T) =Wj+1−1
nWj11T−1
n11TW+1
n11T=Wj+1−2
n11T+1
n11T=
Wj+1−1
n11T.
Thus,
/bardblyk−1¯yk/bardbl≤k−1/summationdisplay
j=0/bardbl(W−1
n11T)j(W−I)gk−1−j/bardbl+/bardblgk−1¯gk/bardbl
≤k−1/summationdisplay
j=0ρj
w/bardbl(W−I)gk−1−j/bardbl+/bardblgk−1¯gk/bardbl.
From Lemma B.1, we have /bardblgk/bardbl2<∞almost surely.
Then,
/bardblgk−1¯gk/bardbl2=n/summationdisplay
i=1/bardblgi,k−1
nn/summationdisplay
j=1gj,k/bardbl2
=n/summationdisplay
i=1/parenleftbigg
/bardblgi,k/bardbl2−2/angbracketleftgi,k,1
nn/summationdisplay
j=1gj,k/angbracketright+/bardbl¯gk/bardbl2/parenrightbigg
=/bardblgk/bardbl2−2n/bardbl¯gk/bardbl2+n/bardbl¯gk/bardbl2
=/bardblgk/bardbl2−n/bardbl¯gk/bardbl2
≤/bardblgk/bardbl2
≤M/prime2<∞.
26Under review as submission to TMLR
Inserting in the previous inequality, we get
/bardblyk−1¯yk/bardbl≤M/prime
1−ρw/bardbl(W−I)/bardbl+M/prime
=G<∞,(41)
where we have a geometric sum as ρw<1.
1.Proving limK→∞/summationtextK
k=0/bardblxk−1¯xk/bardbl2<∞,limK→∞/summationtextK
k=0/bardblzk+1−1¯xk/bardbl2<∞, and limk→∞/bardblxk−
1¯xk/bardbl2= 0
Reconsider (38),
/bardblxk+1−1¯xk+1/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/parenleftBig
ρ2
w/bardblyk−1¯yk/bardbl2+n/bardbl¯gk/bardbl2/parenrightBig
/bardblxk−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1−1¯xk−1/bardbl2+α2
k−11 +ρ2
w
1−ρ2w/parenleftBig
ρ2
w/bardblyk−1−1¯yk−1/bardbl2+n/bardbl¯gk−1/bardbl2/parenrightBig
...
/bardblx1−1¯x1/bardbl2≤1 +ρ2
w
2/bardblx0−1¯x0/bardbl2+α2
01 +ρ2
w
1−ρ2w/parenleftBig
ρ2
w/bardbly0−1¯y0/bardbl2+n/bardbl¯g0/bardbl2/parenrightBig
.
(42)
Adding all inequalities in (42), we obtain
/bardblxk+1−1¯xk+1/bardbl2≤−1−ρ2
w
2k/summationdisplay
l=1/bardblxl−1¯xl/bardbl2+1 +ρ2
w
2/bardblx0−1¯x0/bardbl2
+1 +ρ2
w
1−ρ2wk/summationdisplay
l=0α2
l/parenleftBig
ρ2
w/bardblyl−1¯yl/bardbl2+n/bardbl¯gl/bardbl2/parenrightBig
(a)
≤−1−ρ2
w
2k/summationdisplay
l=1/bardblxl−1¯xl/bardbl2+1 +ρ2
w
2/bardblx0−1¯x0/bardbl2
+1 +ρ2
w
1−ρ2w(ρ2
wG2+nM)k/summationdisplay
l=0α2
l,
with (a)being due to (41). Let k→∞, then the second and third terms are bounded due to
Assumption 2.1. There are then 2 cases:/summationtext
l/bardblxl−1¯xl/bardbl2either diverges or converges. Assume the
validity of the hypothesis H1)/summationtext
l/bardblxl−1¯xl/bardbl2diverges, i.e.,/summationtext∞
l=1/bardblxl−1¯xl/bardbl2→∞. This leads to
/bardblxk+1−1¯xk+1/bardbl2<−∞,
as−1−ρ2
w
2<0. However,/bardblxk+1−1¯xk+1/bardbl2should be positive. Thus, hypothesis H1cannot be true
and/summationtext
l/bardblxl−1¯xl/bardbl2converges. Hence, limk→∞/bardblxk−1¯xk/bardbl2= 0almost surely.
Thus, reconsider (38),
/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/parenleftBig
ρ2
w/bardblyk−1¯yk/bardbl2+n/bardbl¯gk/bardbl2/parenrightBig
K/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2K/summationdisplay
k=0/bardblxk−1¯xk/bardbl2+1 +ρ2
w
1−ρ2w/parenleftBig
ρ2
wG2+nM/parenrightBigK/summationdisplay
k=0α2
k
<∞.(43)
27Under review as submission to TMLR
2.Proving/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl<∞
By induction from (38), we have
/bardblxk+1−1¯xk+1/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+2(ρ2
wG2+nM)
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1α2
k−j.(44)
Since√
a+b<√a+√
b,
/bardblxk+1−1¯xk+1/bardbl≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1
2/bardblx0−1¯x0/bardbl+/radicalBigg
2(ρ2wG2+nM)
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−j.(45)
Then, substituting into the sum/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl,
∞/summationdisplay
k=1γkαk/parenleftBigg
/parenleftbig1 +ρ2
w
2/parenrightbigk
2/bardblx0−1¯x0/bardbl+/radicalBigg
2(ρ2wG2+nM)
1−ρ2wk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j/parenrightBigg
≤γ0α0/bardblx0−1¯x0/bardbl/radicalbig
1 +ρ2w√
2−/radicalbig
1 +ρ2w+/radicalBigg
2(ρ2wG2+nM)
1−ρ2w∞/summationdisplay
k=1γkαkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j,
where the inequality is due to the fact that γkandαkare both decreasing step-sizes and we have a
geometric sum of ratio/radicalBig
1+ρ2
2<1. We then study the sums in the second term,
∞/summationdisplay
k=1γkαkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j≤∞/summationdisplay
k=1γkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2α2
k−1−j
=∞/summationdisplay
k=1γkk/summationdisplay
j=1/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2α2
j−1
=∞/summationdisplay
j=1α2
j−1∞/summationdisplay
k=jγk/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2
≤γ0∞/summationdisplay
j=1α2
j−1∞/summationdisplay
k=j/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2
=γ0/radicalbig
1 +ρ2w√
2−/radicalbig
1 +ρ2w∞/summationdisplay
j=1α2
j−1
<∞,
as/summationtextα2
kconverges by Assumption 2.1.
Finally,/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl<∞.
D.3 Convergence Rate of the Consensus Error /bardblxk−1¯xk/bardbl2and of/bardblzk+1−1¯xk/bardbl2
As/summationtext
k/bardblxk−1¯xk/bardbl2<∞, let us assume that /bardblxk−1¯xk/bardbl2vanishes with the same rate as α2
k. Then, there
must be a scalar ϑ1>0such that/bardblxk−1¯xk/bardbl2<ϑ2
1α2
k. To test if such ϑ1exists, we employ (38) to check
whether/bardblxk+1−1¯xk+1/bardbl2<ϑ2
1α2
k+1holds,
/bardblxk+1−1¯xk+1/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+(1 +ρ2
w)
1−ρ2w(ρ2
wG2+nM)α2
k
≤1 +ρ2
w
2ϑ2
1α2
k+(1 +ρ2
w)
1−ρ2w(ρ2
wG2+nM)α2
k
=/parenleftBig1 +ρ2
w
2ϑ2
1+(1 +ρ2
w)
1−ρ2w(ρ2
wG2+nM)/parenrightBig
α2
k.(46)
28Under review as submission to TMLR
Then, testing
/parenleftBig1 +ρ2
w
2ϑ2
1+(1 +ρ2
w)
1−ρ2w(ρ2
wG2+nM)/parenrightBig
α2
k≤ϑ2
1α2
k+1
(1 +ρ2
w)
1−ρ2w(ρ2
wG2+nM)≤ϑ2
1/parenleftbiggα2
k+1
α2
k−1 +ρ2
w
2/parenrightbigg
(1+ρ2
w)
1−ρ2w(ρ2
wG2+nM)
α2
k+1
α2
k−1+ρ2w
2≤ϑ2
1.(47)
Thus, 0</rho12<∞wheneverα2
k+1
α2
k−1+ρ2
w
2>0.
Let us consider αkhaving the form in Example 2.3, thenα2
k+1
α2
k=/parenleftBig
k+1
k+2/parenrightBig2υ1
is an increasing function of k
taking values between 0and1, and deﬁne
K1= arg min
α2
k+1
α2
k>1+ρ2w
2k.
To test whether K1grows very large, we ﬁnd the intersectionα2
k+1
α2
k=1+ρ2
w
2,
/parenleftBigk+ 1
k+ 2/parenrightBig2υ1
=1 +ρ2
w
2
k+ 1
k+ 2=/parenleftBig1 +ρ2
w
2/parenrightBig1
2υ1
k+ 1 = (k+ 2)/parenleftBig1 +ρ2
w
2/parenrightBig1
2υ1
k=2/parenleftBig
1+ρ2
w
2/parenrightBig1
2υ1−1
1−/parenleftBig
1+ρ2w
2/parenrightBig1
2υ1.(48)
Deﬁne the function h(x,υ1) =2x1
2υ1−1
1−x1
2υ1for0<x< 1and0.5<υ< 1.
∂h(x,υ1)
∂υ1=−exp(lnx
2υ1) lnx
2υ2
1(1−x1
2υ1)2>0for a ﬁxed 0<x< 1.
∂h(x,υ1)
∂x=x−2υ1+1
2υ1
2υ1(1−x1
2υ1)2>0for a ﬁxed 0.5<υ1<1.
Taking an extreme case of x=υ1= 0.99, we obtain h(0.99,0.99)≈196iterations. For x=υ1= 0.95,
h(0.95,0.95)≈36iterations. It decreases even more drastically for realistic choices of ρwandυ1. Thus, it
is reasonable to study the rate for k≥K1.
We conclude that for k≥K1, there exists 0<ϑ1<∞, such that
/bardblxk−1¯xk/bardbl2<ϑ2
1α2
k. (49)
Thus, from (38), for k≥K1, we also have
/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w(ρ2
wG2+nM)
≤/parenleftBig1 +ρ2
w
2ϑ2
1+1 +ρ2
w
1−ρ2w(ρ2
wG2+nM)/parenrightBig
α2
k
:=ϑ2
2α2
k.(50)
29Under review as submission to TMLR
E Convergence Rate
Our primary result, stated in the following Lemma, is based on ﬁnding a relation between two successive
iterations of the expected divergence.
Lemma E.1. LetA=λc3,B=2c3L2ϑ2
1
λn,C=c2
1c6
4
2c3λ, andE=ϑ2
n. Then, for k>K 1,
Dk+1≤(1−Aαkγk)Dk+Bα3
kγk+Cαkγ3
k+Eα2
k. (51)
Proof: See Appendix E.2.
Next, we let
K2= arg min
Aαkγk<1k
andK0= max{K1,K2}. For the ensuing part, the purpose is to locate a vanishing upper bound of Dk,
making use of the inequality (51). The idea is to propose a decreasing sequence Uk+1≤Ukand suppose that
Dk≤Uk,∀k≥K0, and then verify that Dk+1≤Uk+1by induction. The choice of Ukis the most diﬃcult
component as one has to keep in mind the general forms of αkandγkin (51) and what kind of decisions to
take regarding these forms. An essential property of Ukis presented in the subsequent lemma.
Lemma E.2. If a decreasing sequence Uk+1≤Ukfork≥K0exists such that Dk+1≤Uk+1can be deduced
fromDk≤Ukand (51), then
Uk≥B
Aα2
k+C
Aγ2
k+E
Aαk
γk. (52)
Proof: See Appendix E.3.
An important remark is that the lower bound of Ukin (52) is vanishing as α2
k,γ2
k, andαk
γkare all vanishing.
This lower bound provides an insight on the convergence rate of Dkas it cannot be better than that of
α2
k,γ2
k, orαk
γk.
The previous Lemma allows us to move forward in conﬁrming the existence of the constants ς1andς2that
permitDk≤ς1γ2
kandDk≤ς2αk
γkin Theorem 3.4, respectively.
E.1 Proof of Theorem 3.4
1.Proof of (14)
By deﬁnition of ς1,DK0≤ς1γ2
K0. The next step is to make sure that Dk+1≤Uk+1can be obtained
fromDk≤Uk,∀k≥K0. TakeUk=ς1γ2
k, letDk≤Ukhold, and substitute in (51),
Dk+1≤(1−Aαkγk)ς1γ2
k+Bα3
kγk+Cαkγ3
k+Eα2
k.
We solveDk+1≤Uk+1forς1∈R+
(1−Aαkγk)ς1γ2
k+Bα3
kγk+Cαkγ3
k+Eα2
k≤Uk+1=ς1γ2
k+1.
Then, by considering κk=1−(γk+1
γk)2
αkγk>0as given in (13),
Bα2
kγ−2
k+Eαkγ−3
k+C≤ς1(A−κk),
and assuming A−κk>0, we ﬁnd a constant ¯ς1such that
ς1≥¯ς1=Bα2
kγ−2
k+Eαkγ−3
k+C
A−κk,
keeping in mind that Bα2
kγ−2
k+Eαkγ−3
k+Cis positive by deﬁnition. Examine the parameters σ1,
σ2, andσ3as they are introduced in (13), then
¯ς1≤Bσ2+Eσ3+C
A−σ1,
We conclude that Dk≤ς1γ2
kwhereς1satisﬁes the deﬁnition (15).
30Under review as submission to TMLR
2.Proof of (16)
DK0≤ς2γK0
αK0by deﬁnition of ς2.∀k≥K0, letDk≤ς2αk
γk, then
Dk+1≤(1−Aαkγk)ς2αk
γk+Bα3
kγk+Cαkγ3
k+Eα2
k.
SolvingDk+1≤ς2αk+1
γk+1forς2∈R+,
(1−Aαkγk)ς2αk
γk+Bα3
kγk+Cαkγ3
k+Eα2
k≤ς2αk+1
γk+1.
Takeτk=αk
γk−αk+1
γk+1
α2
k>0as given in (13), then
Bαkγk+Cα−1
kγ3
k+E≤(A−τk)ς2.
Ifαk
γk−αk+1
γk+1<Aα2
k, then∃¯ς2such that
ς2≥¯ς2=Bαkγk+Cα−1
kγ3
k+E
(A−τk).
Examineσ4,σ5, andσ6that are deﬁned in (13), we can say
¯ς2≤Bσ5+Cσ6+E
(A−σ4).
We conclude that Dk≤ς2αk
γkwithς2satisfying (17).
E.2 Proof of Lemma E.1
Starting with the same steps as in (29),
Dk+1=E[/bardbl¯xk+1−x∗/bardbl2]
≤E[1
n/bardblzk+1−1¯xk/bardbl2+ 2/angbracketleft−αk¯gk,¯xk−x∗/angbracketright+dk]
=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2αkE[/angbracketleft¯xk−x∗,¯gk/angbracketright]
(a)=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2c3αkγkE[/angbracketleft¯xk−x∗,h(xk) +¯bk/angbracketright]
=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2c3αkγkE[/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright] + 2c3αkγkE[/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright]
−2c3αkγkE[/angbracketleft¯xk−x∗,¯bk/angbracketright]
(53)
where (a)is due to both E[ek|Hk] = 0and (22):
E[/angbracketleft¯xk−x∗,¯gk/angbracketright] =E[/angbracketleft¯xk−x∗,¯gk−E[¯gk|Hk] +E[¯gk|Hk]/angbracketright]
=E[/angbracketleft¯xk−x∗,ek/angbracketright] +E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright]
=EHk[E[/angbracketleft¯xk−x∗,ek/angbracketright|Hk]] +E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright]
= 0 + E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright].
From Lemma B.1, we have E[/bardbl¯gk/bardbl2]<¯Mwith ¯Ma bounded constant.
By the strong convexity in Assumption 1.2, we have
−2c3αkγkE[/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright]≤−2λc3αkγkE[/bardbl¯xk−x∗/bardbl2]
=−2λc3αkγkDk.(54)
31Under review as submission to TMLR
Next, from Lemma 1.5, we have
2c3αkγk/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright≤2c3αkγkL√n/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl
(a)
≤λc3αkγk
2/bardbl¯xk−x∗/bardbl2+ 2c3αkγkL2
λn/bardblxk−1¯xk/bardbl2,
where (a)is due to 2√/epsilon1×1√/epsilon1/angbracketlefta,b/angbracketright= 2/angbracketleft√/epsilon1a,1√/epsilon1b/angbracketright≤/epsilon1/bardbla/bardbl2+1
/epsilon1/bardblb/bardbl2. From (49), we have for k≥K1,
/bardblxk−1¯xk/bardbl2≤ϑ2
1α2
k.
Hence,
2c3αkγkE[/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright]≤λc3αkγk
2Dk+2c3L2ϑ2
1
λnα3
kγk. (55)
From (23),
−2c3αkγkE[/angbracketleft¯xk−x∗,¯bk/angbracketright]≤λc3αkγk
2Dk+2c3αkγk
λE[/bardbl¯bk/bardbl2]
≤λc3αkγk
2Dk+c2
1c6
4αkγ3
k
2c3λ(56)
From (50), for k≥K1, we have
1
nE[/bardblzk+1−1¯xk/bardbl2]≤ϑ2
nα2
k. (57)
Finally, by combining (53), (54), (55), (56), and (57) we get (51).
E.3 Proof of Lemma E.2
Since 1−Aαkγk>0whenk≥K0, we may substitute Dk≤Ukin (51),
Dk+1≤(1−Aαkγk)Uk+Bα3
kγk+Cαkγ3
k+Eα2
k.
TestingDk+1≤Uk+1in the previous inequality, we get
(1−Aαkγk)Uk+Bα3
kγk+Cαkγ3
k+Eα2
k≤Uk+1≤Uk
B
Aα2
k+C
Aγ2
k+E
Aαk
γk≤Uk. (58)
E.4 Proof of Theorem 3.5
Theorem 3.4 indicates that the convergence rate is a function of υ1andυ2, asγ2
k∝(k+ 1)−2υ2and
αk
γk∝(k+ 1)−(υ1−υ2). Nonetheless, we must still verify the validity of the assumptions presented in the
theorem, meaning:
•Areσ1<Aandσ4<Afulﬁlled?
•Areς1andς2bounded?
We must remark that in what follows, the analysis is done for k≥K0.
Letαkandγkhave the forms given in (18).
1.Verifying that σ1<Aandσ4<A
The idea is to ﬁnd a bound on α0andγ0to guarantee σ1<Aandσ4<A. We start by bounding
σ1andσ4from above, i.e.,
32Under review as submission to TMLR
σ1= max
k≥K01−(γk+1
γk)2
αkγk= max
k≥K01−(1 +1
k+1)−2υ2
α0γ0(k+ 1)−υ1−υ2
and
σ4= max
k≥K01−αk+1γ−1
k+1
αkγ−1
k
αkγk= max
k≥K01−(1 +1
k+1)−(υ1−υ2)
α0γ0(k+ 1)−υ1−υ2.
To do so, we deﬁne a function q(x) =x−a(1−(1 +x)−b)witha,b,x∈(0,1]. Sincex−a≤x−1, we
haveq(x)≤x−1(1−(1 +x)−b) =r(x). To further bound q(x), We study the derivative of r(x)as
it is simpler to do so,
r/prime(x) =x−2/parenleftbigg
((b+ 1)x+ 1)(1 +x)−b−1−1/parenrightbigg
=x−2s(x).
Hence the sign of r/prime(x)is that ofs(x). We again calculate the derivative of s(x)to ﬁnd its sign,
s/prime(x) =−b(b+ 1)x(1 +x)−b−2≤0
sinceb > 0andx > 0. Then,s(x)is a decreasing function of xover (0,1]. We remark that
limx→0s(x) = 0, meanings(x)<0andr/prime(x)<0,∀x∈(0,1]. Finally,
r(x)<lim
x→0r(x) =1−(1 +x)−b
x=b,
andq(x)≤r(x)< b, noting that limx→0q(x) =bfora= 1. We conclude that σ1<2υ2
α0γ0and
σ4<υ1−υ2
α0γ0. Forσ1<Aandσ4<Ato be valid, we must have
α0γ0≥max{2υ2,υ1−υ2}/A. (59)
2.Verifying that ς1andς2are bounded
The goal is to verify that the constant term in the convergence rate is bounded. Thus, we must
check that the lower bounds given in (15) and (17) are indeed ﬁnite. We start by analyzing σ2and
σ5,
σ2=α2
0γ−2
0max
k≥K0(1 +k)−2(υ1−υ2)=α2
0γ−2
0(1 +K0)−2(υ1−υ2),as0<υ2≤υ1,
and
σ5=α0γ0max
k≥K0(1 +k)−(υ1+υ2)=α0γ0(1 +K0)−(υ1+υ2),as0<υ2+υ1.
We end with the analysis of σ3andσ6, i.e.,
σ3=α0γ−3
0max
k≥K0(1 +k)−(υ1−3υ2)=/braceleftBigg
α0γ−3
0(1 +K0)−(υ1−3υ2),ifυ1≥3υ2,
∞, ifυ1<3υ2,
and
σ6=α−1
0γ3
0max
k≥K0(1 +k)υ1−3υ2=/braceleftBigg
α−1
0γ3
0(1 +K0)υ1−3υ2,ifυ1≤3υ2,
∞, ifυ1>3υ2.
There are clearly 3 cases:
•υ1>3υ2
Thus,σ3is bounded.
Sinceσ2andς1(by deﬁnition) are also bounded provided that α0γ0≥2υ2
Ain (59).
However,ς2→∞sinceσ6→∞resulting in a loose upper bound in (16).
To that end, we can write Dk≤Υ1(1 +k)−2υ2with Υ1a bounded constant.
33Under review as submission to TMLR
•υ1<3υ2
Similarly,σ6is bounded while σ3→∞. Then,∃Υ2<∞, whereDk≤Υ2(1 +k)−(υ1−υ2)
provided that α0γ0≥υ1−υ2
A.
•υ1= 3υ2
Bothσ3andσ6are bounded allowing both previous inequalities corresponding to Dkto be
valid.
By this analysis, we conclude the proof of Theorem 3.5.
We present Figure 10 for easier reading of the conditions on
the step sizes’ exponents where we plot υ2vs.υ1.
Figure 10: Plot of υ2vs.υ1where
the yellow shaded area is the feasibil-
ity region determined by Assumption
2.1.
F Regret Analysis
Since, by Lemma 1.5, the local objective function is L-smooth, we can write
Fi(y)≤Fi(x) +/angbracketleft∇Fi(x),y−x/angbracketright+L
2/bardbly−x/bardbl2,∀x,y∈Rd,∀i∈N. (60)
To ﬁnd the regret bound, consider
E/bracketleftbigg1
nK/summationdisplay
k=K0n/summationdisplay
i=1Fi(xi,k)−Fi(x∗)/bracketrightbigg
(a)
≤E/bracketleftbiggL
2nK/summationdisplay
k=K0n/summationdisplay
i=1/bardblxi,k−x∗/bardbl2/bracketrightbigg
=E/bracketleftbiggL
2nK/summationdisplay
k=K0n/summationdisplay
i=1/bardblxi,k−¯xk+ ¯xk−x∗/bardbl2/bracketrightbigg
≤E/bracketleftbiggL
nK/summationdisplay
k=K0n/summationdisplay
i=1/parenleftBig
/bardblxi,k−¯xk/bardbl2+/bardbl¯xk−x∗/bardbl2/parenrightBig/bracketrightbigg
=E/bracketleftbigg
LK/summationdisplay
k=K0/parenleftBig1
n/bardblxk−1¯xk/bardbl2+/bardbl¯xk−x∗/bardbl2/parenrightBig/bracketrightbigg
(b)
≤LK/summationdisplay
k=K0ϑ2
1
nα2
k+Dk
=ϑ2
1α2
0L
nK/summationdisplay
k=K01
(k+ 1)3
2+LΥK/summationdisplay
k=K01√
k+ 1
34Under review as submission to TMLR
(c)
≤ϑ2
1α2
0L
n/integraldisplayK
K0−11
(u+ 1)3
2du+ ΥL/integraldisplayK
K0−11√u+ 1du
=2ϑ2
1α2
0L
n/parenleftBig1√K0−1√
K+ 1/parenrightBig
+ 2ΥL(√
K+ 1−/radicalbig
K0)
where (a)is due to∇Fi(x∗) = 0,∀i∈N, by deﬁnition of x∗, in (60). (b)is by Lemma 3.3 and Theorem
3.5, and in (c), we interpret the sums over K0≤k≤Kas Riemann sums in which the functions1
(u+1)3
2
and1√u+1are evaluated at the right endpoint of the interval [i−1,i]fori=K0,K0+ 1,...,K. Since the
functions1
(u+1)3
2and1√u+1are monotonically decreasing, the sums are in fact lowerRiemann sums and
therefore bounded from above by the integrals/integraltextK
K0−11
(u+1)3
2duand/integraltextK
K0−11√u+1du, respectively.
G Convergence Rate with Constant Step Sizes
We start by going over previous derivations,
˘gi,k=ES,Φ,ζ[Φi,k(fi(xi,k+γΦi,k,Si,k) +ζi,k)|Hk]
=EΦ[Φi,kFi(xi,k+γΦi,k)|Hk]
=Fi(xi,k)EΦ[Φi,k] +γEΦ[Φi,kΦT
i,k|Hk]∇Fi(xi,k) +γ2
2EΦ[Φi,kΦT
i,k∇2Fi(˜xi,k)Φi,k|Hk]
=c3γ[∇Fi(xi,k) +bi,k].
Thus,bi,k=γ
2c3EΦ[Φi,kΦT
i,k∇2Fi(˜xi,k)Φi,k|Hk].
Let Assumptions 1.2 and 2.2 hold. Then, we can bound the bias as
/bardblbi,k/bardbl≤γ
2c3EΦ[/bardblΦi,k/bardbl2/bardblΦT
i,k/bardbl2/bardbl∇2Fi(˜xi,k)/bardbl2/bardblΦi,k/bardbl2|Hk]
≤γc3
4c1
2c3.
We remark that
˜gk=E[¯gk|Hk]
=1
nn/summationdisplay
i=1E[gi,k|Hk]
=1
nn/summationdisplay
i=1c3γ[∇Fi(xi,k) +bi,k]
=c3γ[h(xk) +¯bk](61)
is also a biased estimator of h(xk)with
/bardbl¯bk/bardbl=/bardbl1
nn/summationdisplay
i=1bi,k/bardbl
≤1
nn/summationdisplay
i=1/bardblbi,k/bardbl
≤1
nn/summationdisplay
i=1γc3
4c1
2c3
=γc3
4c1
2c3.(62)
35Under review as submission to TMLR
Lemma G.1. Let all Assumptions 1.3, 2.2, and 2.4 hold, then there exists a bounded constant ¯M > 0, such
thatE[/bardbl¯gk/bardbl2]<¯M.
Proof.∀i∈N, we have
E[/bardblgi,k/bardbl2|Hk] =E[/bardblΦi,k(fi(xi,k+γΦi,k,Si,k) +ζi,k)/bardbl2|Hk]
=E[/bardblΦi,k/bardbl2/bardblfi(xi,k+γΦi,k,Si,k) +ζi,k/bardbl2|Hk]
(a)
≤c2
4E[(fi(xi,k+γΦi,k,Si,k) +ζi,k)2|Hk]
(b)=c2
4E[f2
i(xi,k+γΦi,k,Si,k)|Hk] +c2
4c2
(c)
<∞,
where (a)is due to Assumption 2.2, (b)Assumption 1.3, and (c)Assumption 2.4.
The stochastic noise is still deﬁned as ek= ¯gk−˜gkand retains its property
E[ek] =E[¯gk−E[¯gk|Hk]] =EHk/bracketleftBig
E/bracketleftBig
¯gk−E[¯gk|Hk]/vextendsingle/vextendsingle/vextendsingleHk/bracketrightBig/bracketrightBig
= 0.
1.Proving/bardblxk−1¯xk/bardbl2and/bardblzk+1−1¯xk/bardbl2converge linearly
/bardblxk+1−1¯xk+1/bardbl2=/bardblxk+1−1¯xk+1¯xk−1¯xk+1/bardbl2
=/bardblxk+1−1¯xk/bardbl2+ 2/angbracketleftxk+1−1¯xk,1¯xk−1¯xk+1/angbracketright+/bardbl1¯xk−1¯xk+1/bardbl2
=/bardblxk+1−1¯xk/bardbl2−/bardbl1¯xk−1¯xk+1/bardbl2
≤/bardblxk+1−1¯xk/bardbl2
=n/summationdisplay
i=1/bardblxi,k+1−¯xk/bardbl2
(a)
≤n/summationdisplay
i=1/bardblzi,k+1−¯xk/bardbl2
=/bardblzk+1−1¯xk/bardbl2
=/bardblWxk−αWyk−1¯xk/bardbl2
=/bardblWxk−1¯xk/bardbl2−2α/angbracketleftWxk−1¯xk,Wyk/angbracketright+α2/bardblWyk/bardbl2
(b)
≤/bardblWxk−1¯xk/bardbl2+α[1−ρ2
w
2ρ2wα/bardblWxk−1¯xk/bardbl2+2ρ2
wα
1−ρ2w/bardblWyk/bardbl2] +α2/bardblWyk/bardbl2
(c)
≤ρ2
w/bardblxk−1¯xk/bardbl2+α[1−ρ2
w
2α/bardblxk−1¯xk/bardbl2+2ρ2
wα
1−ρ2w/bardblWyk/bardbl2] +α2/bardblWyk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α21 +ρ2
w
1−ρ2w/bardblWyk−1¯yk+1¯yk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α21 +ρ2
w
1−ρ2w/bardblWyk−1¯yk/bardbl2+α2n(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2
≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α21 +ρ2
w
1−ρ2w/parenleftBig
ρ2
w/bardblyk−1¯yk/bardbl2+n/bardbl¯gk/bardbl2/parenrightBig
(63)
where (a)is the projection inequality (5) noting that ¯xk∈KsinceKis a convex set (so projecting
it ontoKgives us the same point), (b)is by−2/epsilon1×1
/epsilon1/angbracketlefta,b/angbracketright=−2/angbracketleft/epsilon1a,1
/epsilon1b/angbracketright≤/epsilon12/bardbla/bardbl2+1
/epsilon12/bardblb/bardbl2, and (c)
is by Lemma 1.4.
36Under review as submission to TMLR
By induction, we have
/bardblxk+1−1¯xk+1/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2
+α22
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1/parenleftBig
ρ2
w/bardblyk−j−1¯yk−j/bardbl2+n/bardbl¯gk/bardbl2/parenrightBig
.(64)
To bound the term /bardblyk−j−1¯yk−j/bardbl2, we repeatedly replace the auxiliary variables with the algo-
rithm’s iterations,
yk+1=Wyk+gk+1−gk
=W(Wyk−1+gk−gk−1) +gk+1−gk
=W2yk−1−Wgk−1+ (W−I)gk+gk+1
=W3yk−2−W2gk−2+W(W−I)gk−1+ (W−I)gk+gk+1
=...
=Wk+1y0−Wkg0+k−1/summationdisplay
j=0Wj(W−I)gk−j+gk+1
=Wk(W−I)g0+k−1/summationdisplay
j=0Wj(W−I)gk−j+gk+1
=k/summationdisplay
j=0Wj(W−I)gk−j+gk+1,
yk−1¯yk=k−1/summationdisplay
j=0Wj(W−I)gk−1−j+gk−k−1/summationdisplay
j=01
n11TWj(W−I)gk−1−j−1
n11Tgk
=k−1/summationdisplay
j=0Wj(W−I)gk−1−j+gk−k−1/summationdisplay
j=01
n11T(W−I)gk−1−j−1
n11Tgk
=k−1/summationdisplay
j=0(Wj−1
n11T)(W−I)gk−1−j+gk−1
n11Tgk
=k−1/summationdisplay
j=0(W−1
n11T)j(W−I)gk−1−j+gk−1¯gk,
wherethelastequalitycanbeprovenbyrecursionandthefactthatthematrix Wisdoublystochastic
by Assumption 1.1:
(W−1
n11T)j+1= (Wj−1
n11T)(W−1
n11T) =Wj+1−1
nWj11T−1
n11TW+1
n11T=Wj+1−
2
n11T+1
n11T=Wj+1−1
n11T.
Thus,
/bardblyk−1¯yk/bardbl≤k−1/summationdisplay
j=0/bardbl(W−1
n11T)j(W−I)gk−1−j/bardbl+/bardblgk−1¯gk/bardbl
≤k−1/summationdisplay
j=0ρj
w/bardbl(W−I)gk−1−j/bardbl+/bardblgk−1¯gk/bardbl.
37Under review as submission to TMLR
From Lemma G.1, we have /bardblgk/bardbl2<∞almost surely.
/bardblgk−1¯gk/bardbl2=n/summationdisplay
i=1/bardblgi,k−1
nn/summationdisplay
j=1gj,k/bardbl2
=n/summationdisplay
i=1/parenleftbigg
/bardblgi,k/bardbl2−2/angbracketleftgi,k,1
nn/summationdisplay
j=1gj,k/angbracketright+/bardbl¯gk/bardbl2/parenrightbigg
=/bardblgk/bardbl2−2n/bardbl¯gk/bardbl2+n/bardbl¯gk/bardbl2
=/bardblgk/bardbl2−n/bardbl¯gk/bardbl2
≤/bardblgk/bardbl2
≤M/prime2<∞.
Inserting in the previous inequality, we get
/bardblyk−1¯yk/bardbl≤M/prime
1−ρw/bardbl(W−I)/bardbl+M/prime
=G<∞,(65)
where we have a geometric sum as ρw<1.
Substituting the upper bound in (64),
/bardblxk+1−1¯xk+1/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+α22(ρ2
wG2+nM)
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
(a)
≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+α22(ρ2
wG2+nM)(1 +ρ2
w)
(1−ρ2w)2(66)
where (a)is due to the geometric sum with1+ρ2
w
2<1.
We conclude that /bardblxk−1¯xk/bardbl2converges linearly almost surely.
Substituting in (63),
/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α21 +ρ2
w
1−ρ2w(ρ2
wG2+nM)
≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+α2(ρ2
wG2+nM)(1 +ρ2
w)2
(1−ρ2w)2+α21 +ρ2
w
1−ρ2w(ρ2
wG2+nM)
=/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+α2(ρ2
wG2+nM)/parenleftbigg/parenleftBig1 +ρ2
w
1−ρ2w/parenrightBig2
+1 +ρ2
w
1−ρ2w/parenrightbigg
(67)
Finally,/bardblzk+1−1¯xk/bardbl2converges linearly almost surely as well.
2.ProvingDk=E[/bardbl¯xk−x∗/bardbl2]converges linearly
Dk+1=E[/bardbl¯xk+1−x∗/bardbl2]
≤E[1
n/bardblzk+1−1¯xk/bardbl2+ 2/angbracketleft−α¯gk,¯xk−x∗/angbracketright+dk]
=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2αE[/angbracketleft¯xk−x∗,¯gk/angbracketright]
(a)=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2c3αγE[/angbracketleft¯xk−x∗,h(xk) +¯bk/angbracketright]
=Dk+1
nE[/bardblzk+1−1¯xk/bardbl2]−2c3αγE[/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright] + 2c3αγE[/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright]
−2c3αγE[/angbracketleft¯xk−x∗,¯bk/angbracketright]
(68)
38Under review as submission to TMLR
where (a)is due to both E[ek|Hk] = 0and (22):
E[/angbracketleft¯xk−x∗,¯gk/angbracketright] =E[/angbracketleft¯xk−x∗,¯gk−E[¯gk|Hk] +E[¯gk|Hk]/angbracketright]
=E[/angbracketleft¯xk−x∗,ek/angbracketright] +E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright]
=EHk[E[/angbracketleft¯xk−x∗,ek/angbracketright|Hk]] +E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright]
= 0 + E[/angbracketleft¯xk−x∗,E[¯gk|Hk]/angbracketright].
From Lemma G.1, we have E[/bardbl¯gk/bardbl2]<¯Mwith ¯Ma bounded constant.
By the strong convexity in Assumption 1.2, we have
−2c3αγE[/angbracketleft¯xk−x∗,∇F(¯xk)/angbracketright]≤−2λc3αγE[/bardbl¯xk−x∗/bardbl2]
=−2λc3αγDk.(69)
Next, from Lemma 1.5, we have
2c3αγ/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright≤2c3αγL√n/bardbl¯xk−x∗/bardbl/bardblxk−1¯xk/bardbl
(a)
≤λc3αγ
2/bardbl¯xk−x∗/bardbl2+ 2c3αγL2
λn/bardblxk−1¯xk/bardbl2,
where (a)is due to 2√/epsilon1×1√/epsilon1/angbracketlefta,b/angbracketright= 2/angbracketleft√/epsilon1a,1√/epsilon1b/angbracketright≤/epsilon1/bardbla/bardbl2+1
/epsilon1/bardblb/bardbl2.
In (66) and (67), we let R=/bardblx0−1¯x0/bardbl2, andG1=2(ρ2
wG2+nM)(1+ρ2
w)
(1−ρ2w)2,G2= (ρ2
wG2+
nM)/parenleftbigg/parenleftBig
1+ρ2
w
1−ρ2w/parenrightBig2
+1+ρ2
w
1−ρ2w/parenrightbigg
,
/bardblxk−1¯xk/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigkR+α2G1and/bardblzk+1−1¯xk/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1R+α2G2.(70)
Hence,
2c3αγE[/angbracketleft¯xk−x∗,∇F(¯xk)−h(xk)/angbracketright]≤λc3αγ
2Dk+ 2c3αγL2
λn/bracketleftBig/parenleftbig1 +ρ2
w
2/parenrightbigkR+α2G1/bracketrightBig
.(71)
From (62),
−2c3αγE[/angbracketleft¯xk−x∗,¯bk/angbracketright]≤λc3αγ
2Dk+2c3αγ
λE[/bardbl¯bk/bardbl2]
≤λc3αγ
2Dk+αγ3c2
1c6
4
2c3λ(72)
Finally, by combining (68), (69), (70), (71), and (72), and setting now A=λc3,B=2c3L2
λn, and
C=c2
1c6
4
2c3λ, we get
Dk+1≤(1−Aαγ)Dk+Bαγ/bracketleftBig/parenleftbig1 +ρ2
w
2/parenrightbigkR+α2G1/bracketrightBig
+1
n/bracketleftBig/parenleftbig1 +ρ2
w
2/parenrightbigk+1R+α2G2/bracketrightBig
+Cαγ3
=(1−Aαγ)Dk+R/bracketleftBig
Bαγ +1
n/parenleftbig1 +ρ2
w
2/parenrightbig/bracketrightBig/parenleftbig1 +ρ2
w
2/parenrightbigk+α3γBG 1+α21
nG2+αγ3C.(73)
39Under review as submission to TMLR
Let/rho11= 1−Aαγand/rho12=/parenleftbig1+ρ2
w
2/parenrightbig
. Then, assuming αγ <1
Aand taking the telescoping sum
Dk+1≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBigk/summationdisplay
i=0/rho1i
1/rho1k−i
2+ (α3γBG 1+α21
nG2+αγ3C)k/summationdisplay
i=0/rho1i
1
=/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBigk/summationdisplay
i=0/rho1i
1/rho1k−i
2+ (α3γBG 1+α21
nG2+αγ3C)/parenleftBig1−/rho1k+1
1
1−/rho11/parenrightBig
=/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBigk/summationdisplay
i=0/rho1i
1/rho1k−i
2+/parenleftBig
α2BG1
A+α
γG2
nA+γ2C
A/parenrightBig
(1−/rho1k+1
1)
≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBigk/summationdisplay
i=0/rho1i
1/rho1k−i
2+α2BG1
A+α
γG2
nA+γ2C
A(74)
where in the last equality, we further imposed the step sizes to satisfy α<γ.
In what follows, we discuss the summation in the second term of the inequality to avoid setting loose
bounds. We know that this summation can be written as follows,
k/summationdisplay
i=0/rho1i
1/rho1k−i
2=k/summationdisplay
i=0/rho1k−i
1/rho1i
2 (75)
Thus, without imposing further assumptions on the step sizes, we consider the following function
the two cases:
•When/rho11≤/rho12, we use the left hand side of the previous equality
Dk+1≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBig
/rho1k
2k/summationdisplay
i=0/rho1i
1/rho1−i
2+α2BG1
A+α
γG2
nA+γ2C
A
≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBig
/rho1k
21
1−/rho11
/rho12+α2BG1
A+α
γG2
nA+γ2C
A
=/rho1k+1
1D0+/rho1k+1
22R/parenleftBig
Bαγ +/rho12
n/parenrightBig
2Aαγ +ρ2w−1+α2BG1
A+α
γG2
nA+γ2C
A(76)
Then, for arbitrary small step sizes satisfying αγ <1
Aandα<γ,Dkconverges with the linear
rate ofO/parenleftbig
/rho1k
2/parenrightbig
.
•When/rho11>/rho12, we use the right hand side
Dk+1≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBig
/rho1k
1k/summationdisplay
i=0/rho1−i/rho1i
2+α2BG1
A+α
γG2
nA+γ2C
A
≤/rho1k+1
1D0+R/parenleftBig
Bαγ +/rho12
n/parenrightBig
/rho1k
11
1−/rho12
/rho11+α2BG1
A+α
γG2
nA+γ2C
A
=/rho1k+1
1/parenleftBig
D0+2RBαγ +2R/rho12
n
1−2Aαγ−ρ2w/parenrightBig
+α2BG1
A+α
γG2
nA+γ2C
A(77)
Then, for arbitrary small step sizes satisfying αγ <1
Aandα<γ,Dkconverges with the linear
rate ofO/parenleftbig
/rho1k
1/parenrightbig
.
H Distributed Consensus-Based Algorithm Without Gradient Tracking
As the gradient estimate is similar to that used in the algorithm with gradient tracking, all properties
discussed in Appendix B are conserved.
40Under review as submission to TMLR
We also have,
¯zk+1=1
n1TW(xk−αkgk)(a)=1
n1T(xk−αkgk) = ¯xk−αk¯gk, (78)
where (a)is due to the doubly stochastic property of W.
Asdk+1andDk+1in the algorithm with gradient-tracking are analyzed in terms of ¯gkand not yk, thus their
dependence to ykappears only through the consensus errors /bardblxk−1¯xk/bardbl2and/bardblzk+1−1¯xk/bardbl2. This is why we
only analyze these errors in this section. The subsequent analysis concerning the almost sure convergence,
the convergence rate with constant and vanishing step sizes, and the regret bounds follow exactly the same
afterwards as in the case with gradient tracking.
We rewrite the error /bardblxk+1−1¯xk+1/bardbl2as a function of/bardblxk−1¯xk/bardbl2and other vanishing terms,
/bardblxk+1−1¯xk+1/bardbl2=/bardblxk+1−1¯xk+1¯xk−1¯xk+1/bardbl2
=/bardblxk+1−1¯xk/bardbl2+ 2/angbracketleftxk+1−1¯xk,1¯xk−1¯xk+1/angbracketright+/bardbl1¯xk−1¯xk+1/bardbl2
(a)=/bardblxk+1−1¯xk/bardbl2−/bardbl1¯xk−1¯xk+1/bardbl2
≤/bardblxk+1−1¯xk/bardbl2
=n/summationdisplay
i=1/bardblxi,k+1−¯xk/bardbl2
(b)
≤n/summationdisplay
i=1/bardblzi,k+1−¯xk/bardbl2
=/bardblzk+1−1¯xk/bardbl2
=/bardblWxk−αkWgk−1¯xk/bardbl2
=/bardblWxk−1¯xk/bardbl2−2αk/angbracketleftWxk−1¯xk,Wgk/angbracketright+α2
k/bardblWgk/bardbl2
(c)
≤/bardblWxk−1¯xk/bardbl2+αk[1−ρ2
w
2ρ2wαk/bardblWxk−1¯xk/bardbl2+2ρ2
wαk
1−ρ2w/bardblWgk/bardbl2] +α2
k/bardblWgk/bardbl2
(d)
≤ρ2
w/bardblxk−1¯xk/bardbl2+αk[1−ρ2
w
2αk/bardblxk−1¯xk/bardbl2+2ρ2
wαk
1−ρ2w/bardblWgk/bardbl2] +α2
k/bardblWgk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWgk/bardbl2
=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWgk−1¯gk+1¯gk/bardbl2
(e)=1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
k1 +ρ2
w
1−ρ2w/bardblWgk−1¯gk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2
≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kρ2
w(1 +ρ2
w)
1−ρ2w/bardblgk−1¯gk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2w/bardbl¯gk/bardbl2
(f)
≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2wM.(79)
where (a)is by (80), (b)is the projection inequality (5) noting that ¯xk∈KsinceKis a convex set (so
projecting it onto Kgives us the same point), (c)is by−2/epsilon1×1
/epsilon1/angbracketlefta,b/angbracketright=−2/angbracketleft/epsilon1a,1
/epsilon1b/angbracketright≤/epsilon12/bardbla/bardbl2+1
/epsilon12/bardblb/bardbl2(d)is
by Lemma 1.4, (e)is by (81), and (f)is by (82) and (83).
41Under review as submission to TMLR
2/angbracketleftxk+1−1¯xk,1¯xk−1¯xk+1/angbracketright=2n/summationdisplay
i=1/angbracketleftxi,k+1−¯xk,¯xk−¯xk+1/angbracketright
=2/angbracketleftn/summationdisplay
i=1(xi,k+1−¯xk),¯xk−¯xk+1/angbracketright
=2/angbracketleftn(¯xk+1−¯xk),¯xk−¯xk+1/angbracketright
=−2n/angbracketleft¯xk−¯xk+1,¯xk−¯xk+1/angbracketright
=−2n/bardbl¯xk−¯xk+1/bardbl2
=−2/bardbl1¯xk−1¯xk+1/bardbl2.(80)
/angbracketleftWgk−1¯gk,1¯gk/angbracketright=n/summationdisplay
i=1/angbracketleftn/summationdisplay
j=1wijgj,k−¯gk,¯gk/angbracketright
=/angbracketleftn/summationdisplay
i=1n/summationdisplay
j=1wijgj,k−n¯gk,¯gk/angbracketright
=/angbracketleftn/summationdisplay
j=1(n/summationdisplay
i=1wij)gj,k−n¯gk,¯gk/angbracketright
=/angbracketleftn/summationdisplay
j=1gj,k−n¯gk,¯gk/angbracketright
=0.(81)
From Lemma B.1, we know that /bardbl¯gk/bardbl2≤M <∞almost surely,
/bardblgk−1¯gk/bardbl2=n/summationdisplay
i=1/bardblgi,k−1
nn/summationdisplay
j=1gj,k/bardbl2
=n/summationdisplay
i=1/parenleftbigg
/bardblgi,k/bardbl2−2/angbracketleftgi,k,1
nn/summationdisplay
j=1gj,k/angbracketright+/bardbl¯gk/bardbl2/parenrightbigg
=/bardblgk/bardbl2−2n/bardbl¯gk/bardbl2+n/bardbl¯gk/bardbl2
=/bardblgk/bardbl2−n/bardbl¯gk/bardbl2(82)
Then,
ρ2
w/bardblgk−1¯gk/bardbl2+n/bardbl¯gk/bardbl2=ρ2
w/bardblgk/bardbl2+n(1−ρ2
w)/bardbl¯gk/bardbl2
≤ρ2
wnM+n(1−ρ2
w)M
=nM.(83)
1.Proving limK→∞/summationtextK
k=0/bardblxk−1¯xk/bardbl2<∞,limK→∞/summationtextK
k=0/bardblzk+1−1¯xk/bardbl2<∞, and limk→∞/bardblxk−
1¯xk/bardbl2= 0
42Under review as submission to TMLR
Reconsider (79),
/bardblxk+1−1¯xk+1/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2wM
/bardblxk−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1−1¯xk−1/bardbl2+α2
k−1n(1 +ρ2
w)
1−ρ2wM
...
/bardblx1−1¯x1/bardbl2≤1 +ρ2
w
2/bardblx0−1¯x0/bardbl2+α2
0n(1 +ρ2
w)
1−ρ2wM.(84)
Adding all inequalities in (84), we obtain
/bardblxk+1−1¯xk+1/bardbl2≤−1−ρ2
w
2k/summationdisplay
l=1/bardblxl−1¯xl/bardbl2+1 +ρ2
w
2/bardblx0−1¯x0/bardbl2+n(1 +ρ2
w)
1−ρ2wMk/summationdisplay
l=0α2
l
Letk→∞, then the second and third terms are bounded due to Assumption 2.1. There are then
2 cases:/summationtext
l/bardblxl−1¯xl/bardbl2either diverges or converges. Assume the validity of the hypothesis H2)/summationtext
l/bardblxl−1¯xl/bardbl2diverges, i.e.,/summationtext∞
l=1/bardblxl−1¯xl/bardbl2→∞. This leads to
/bardblxk+1−1¯xk+1/bardbl2<−∞,
as−1−ρ2
w
2<0. However,/bardblxk+1−1¯xk+1/bardbl2should be positive. Thus, hypothesis H2cannot be true
and/summationtext
l/bardblxl−1¯xl/bardbl2converges. Hence, limk→∞/bardblxk−1¯xk/bardbl2= 0almost surely.
Thus, reconsider (79),
/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kn(1 +ρ2
w)
1−ρ2wM
K/summationdisplay
k=0/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2K/summationdisplay
k=0/bardblxk−1¯xk/bardbl2+n(1 +ρ2
w)
1−ρ2wMK/summationdisplay
k=0α2
k
<∞.(85)
2.Proving/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl<∞
By induction from (79), we have
/bardblxk+1−1¯xk+1/bardbl2≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1/bardblx0−1¯x0/bardbl2+2nM
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1α2
k−j.(86)
Since√
a+b<√a+√
b,
/bardblxk+1−1¯xk+1/bardbl≤/parenleftbig1 +ρ2
w
2/parenrightbigk+1
2/bardblx0−1¯x0/bardbl+/radicalBigg
2nM
1−ρ2wk/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−j.(87)
Then, substituting into the sum/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl,
∞/summationdisplay
k=1γkαk/parenleftBigg
/parenleftbig1 +ρ2
w
2/parenrightbigk
2/bardblx0−1¯x0/bardbl+/radicalBigg
2nM
1−ρ2wk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j/parenrightBigg
≤γ0α0/bardblx0−1¯x0/bardbl/radicalbig
1 +ρ2w√
2−/radicalbig
1 +ρ2w+/radicalBigg
2nM
1−ρ2w∞/summationdisplay
k=1γkαkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j,
43Under review as submission to TMLR
where the inequality is due to the fact that γkandαkare both decreasing step-sizes and we have a
geometric sum of ratio/radicalBig
1+ρ2
2<1. We then study the sums in the second term,
∞/summationdisplay
k=1γkαkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2αk−1−j≤∞/summationdisplay
k=1γkk−1/summationdisplay
j=0/parenleftbig1 +ρ2
w
2/parenrightbigj+1
2α2
k−1−j
=∞/summationdisplay
k=1γkk/summationdisplay
j=1/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2α2
j−1
=∞/summationdisplay
j=1α2
j−1∞/summationdisplay
k=jγk/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2
≤γ0∞/summationdisplay
j=1α2
j−1∞/summationdisplay
k=j/parenleftbig1 +ρ2
w
2/parenrightbigk−j+1
2
=γ0/radicalbig
1 +ρ2w√
2−/radicalbig
1 +ρ2w∞/summationdisplay
j=1α2
j−1
<∞,
as/summationtextα2
kconverges by Assumption 2.1.
Finally,/summationtext∞
k=0γkαk/bardblxk−1¯xk/bardbl<∞.
H.1 Convergence Rate of the Consensus Error /bardblxk−1¯xk/bardbl2and of/bardblzk+1−1¯xk/bardbl2
As/summationtext
k/bardblxk−1¯xk/bardbl2<∞, let us assume that /bardblxk−1¯xk/bardbl2vanishes with the same rate as α2
k. Then, there
must be a scalar ˘ϑ1>0such that/bardblxk−1¯xk/bardbl2<˘ϑ12α2
k. To test if such ˘ϑ1exists, we employ (79) to check
whether/bardblxk+1−1¯xk+1/bardbl2<˘ϑ12α2
k+1holds,
/bardblxk+1−1¯xk+1/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+n(1 +ρ2
w)M
1−ρ2wα2
k
≤1 +ρ2
w
2˘ϑ12α2
k+n(1 +ρ2
w)M
1−ρ2wα2
k
=/parenleftBig1 +ρ2
w
2˘ϑ12+n(1 +ρ2
w)M
1−ρ2w/parenrightBig
α2
k.(88)
Then, testing
/parenleftBig1 +ρ2
w
2˘ϑ12+n(1 +ρ2
w)M
1−ρ2w/parenrightBig
α2
k≤˘ϑ12α2
k+1
n(1 +ρ2
w)M
1−ρ2w≤˘ϑ12/parenleftbiggα2
k+1
α2
k−1 +ρ2
w
2/parenrightbigg
n(1+ρ2
w)M
1−ρ2w
α2
k+1
α2
k−1+ρ2w
2≤˘ϑ12.(89)
Thus, 0</rho12<∞wheneverα2
k+1
α2
k−1+ρ2
w
2>0.
44Under review as submission to TMLR
Let us consider αkhaving the form in Example 2.3, thenα2
k+1
α2
k=/parenleftBig
k+1
k+2/parenrightBig2υ1
is an increasing function of k
taking values between 0and1, and deﬁne
K1= arg min
α2
k+1
α2
k>1+ρ2w
2k.
To test whether K1grows very large, we ﬁnd the intersectionα2
k+1
α2
k=1+ρ2
w
2,
/parenleftBigk+ 1
k+ 2/parenrightBig2υ1
=1 +ρ2
w
2
k+ 1
k+ 2=/parenleftBig1 +ρ2
w
2/parenrightBig1
2υ1
k+ 1 = (k+ 2)/parenleftBig1 +ρ2
w
2/parenrightBig1
2υ1
k=2/parenleftBig
1+ρ2
w
2/parenrightBig1
2υ1−1
1−/parenleftBig
1+ρ2w
2/parenrightBig1
2υ1.(90)
Deﬁne the function h(x,υ1) =2x1
2υ1−1
1−x1
2υ1for0<x< 1and0.5<υ< 1.
∂h(x,υ1)
∂υ1=−exp(lnx
2υ1) lnx
2υ2
1(1−x1
2υ1)2>0for a ﬁxed 0<x< 1.
∂h(x,υ1)
∂x=x−2υ1+1
2υ1
2υ1(1−x1
2υ1)2>0for a ﬁxed 0.5<υ1<1.
Taking an extreme case of x=υ1= 0.99, we obtain h(0.99,0.99)≈196iterations. For x=υ1= 0.95,
h(0.95,0.95)≈36iterations. It decreases even more drastically for realistic choices of ρwandυ1. Thus, it
is reasonable to study the rate for k≥K1.
We conclude that for k≥K1, there exists 0<˘ϑ1<∞, such that
/bardblxk−1¯xk/bardbl2<˘ϑ12α2
k. (91)
Thus, from (79), for k≥K1, we also have
/bardblzk+1−1¯xk/bardbl2≤1 +ρ2
w
2/bardblxk−1¯xk/bardbl2+α2
kn(1 +ρ2
w)M
1−ρ2w
≤/parenleftBig1 +ρ2
w
2˘ϑ12+n(1 +ρ2
w)M
1−ρ2w/parenrightBig
α2
k
:=˘ϑ22α2
k.(92)
The rest of the analysis concerning the almost sure convergence, the convergence rate with constant and
vanishing step sizes, and the regret bounds follow exactly the same as in the case with gradient tracking.
I Additional Numerical Examples
Figures 11-14 depict the classiﬁcation of images with the labels 2and3and Figures 15-18 depict those with
the labels 3and4.
45Under review as submission to TMLR
Figure 11: Expected loss function evolution of
the proposed algorithms vs. DSGT, EXTRA,
and 1P-GD considering vanishing vs. constant
step sizes classifying images with labels 2and3.
Figure 12: Expected test accuracy evolution of
the proposed algorithms vs. DSGT, EXTRA,
and 1P-GD considering vanishing vs. constant
step sizes classifying images with labels 2and3.
Figure 13: Expected consensus error evolution of
the proposed algorithms vs. DSGT and EXTRA
consideringvanishingvs. constantstepsizesclas-
sifying images with labels 2and3.
Figure 14: Expected gradient tracking error evo-
lution of the proposed algorithms vs. DSGT and
EXTRA considering vanishing vs. constant step
sizes classifying images with labels 2and3.
Figure 15: Expected loss function evolution of
the proposed algorithms vs. DSGT, EXTRA,
and 1P-GD considering vanishing vs. constant
step sizes classifying images with labels 3and4.
Figure 16: Expected test accuracy evolution of
the proposed algorithms vs. DSGT, EXTRA,
and 1P-GD considering vanishing vs. constant
step sizes classifying images with labels 3and4.
46Under review as submission to TMLR
Figure 17: Expected consensus error evolution of
the proposed algorithms vs. DSGT and EXTRA
consideringvanishingvs. constantstepsizesclas-
sifying images with labels 3and4.
Figure 18: Expected gradient tracking error evo-
lution of the proposed algorithms vs. DSGT and
EXTRA considering vanishing vs. constant step
sizes classifying images with labels 3and4.
47