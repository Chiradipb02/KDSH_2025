Under review as submission to TMLR
A Teacher-Student Perspective on the Dynamics of
Learning Near the Optimal Point
Anonymous authors
Paper under double-blind review
Abstract
Near an optimal learning point of a neural network, the learning performance of gradient
descent dynamics is dictated by the Hessian matrix of the loss function with respect to the
network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-
student problems, when the teacher and student networks have matching weights, showing
that the smaller eigenvalues of the Hessian determine long-time learning performance. For
linear networks, we analytically establish that for large networks the spectrum asymptoti-
cally follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur
distribution. We numerically analyse the Hessian spectrum for polynomial and other non-
linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as
an eﬀective number of parameters for networks using polynomial activation functions. For
a generic non-linear activation function, such as the error function, we empirically observe
that the Hessian matrix is always full rank.
1 Introduction
Neural networks have achieved tremendous success in the last decade. Their practical usefulness and tech-
nological potential is now undeniable. However, there is an enormous gap between our current theoretical
understanding and the state-of-the-art techniques used in recent applications.
Among the many unsolved theoretical puzzles, understanding the generalization and robustness of trained
models is particularly important since modern neural networks often work with a number of parameters
vastly larger than the amount of available training data. The unexpected eﬀectiveness of stochastic gradient
descent as a training method, for high-dimensional and non-convex learning tasks, is also one of the many
empirical observations that still lack a consensual explanation.
In this context, understanding the universal properties of the dynamics of learning in high-dimensional
neural networks, although particularly challenging, has the potential of unveiling some of the magic behind
their remarkable practical eﬀectiveness. In this work, we study learning under gradient descent dynamics
in simple, yet nontrivial examples, in an attempt to characterize the eﬀectiveness of the learning process in
terms of the features of the network.
We make two important simplifying assumptions:
1. We focus on a teacher-student setup (Zhang et al., 2019; Goldt et al., 2020; Safran et al., 2021;
Akiyama & Suzuki, 2023). Here, a neural network, called the student , is tasked with learning
another ﬁxed neural network, the teacher , through its outputs. The learning problem is completely
determined by the student and teacher architectures. Thus, empirical claims, which are typically
diﬃcult to validate in less academic setups, can be eﬀectively veriﬁed using numerical tools and
sometimes even analytically.
2. We assume that the network’s initialization is suﬃciently close to an optimal point. In this case,
the Hessian matrix of the loss function with respect to the network’s parameters fully characterizes
the loss landscape in quadratic order. It is worth noting that previous analyses of the Hessian have
1Under review as submission to TMLR
been used to evaluate the ﬂatness and overall curvature of the loss function and its rank used as
an eﬀective measure of the number of the network’s parameters (Maddox et al., 2020; Singh et al.,
2021). However, the link of the properties of the Hessian with the training dynamics has, to our
knowledge, been overlooked.
Under these assumptions, we are seeking answers to the following questions:
•How does the evolution of the loss function for a student initialized near the optimal point depend
on the characteristics (number of parameters, activation functions) of the network?
•Under which conditions can the Hessian rank at the optimal point be interpreted as an eﬀective
number of parameters ?
All the code supporting this research is available in the provided supplementary material.
Related work In 2021, Singh et al. (2021) published an analytical study of the Hessian rank of deep
linear networks, providing tight upper bounds. For non-linear networks, they found that the linear formulas
were still empirically valid in determining the numerical Hessian rank. That study came at a time where
empirical investigations into the eigenspectrum of the Hessian matrix were being performed (Sagun et al.,
2017; 2018). In our work, we determine the Hessian rank at the optimal point, in the teacher-student setup,
and we propose using the Hessian rank as a measure of the eﬀective number of parameters. We also provide
upper bounds for the Hessian rank at the optimal point for polynomial networks.
Still related to the Hessian matrix, Liao & Mahoney (2021) in 2021 studied the Hessian eigenspectra of
nonlinear neural networks, with the objective of understanding the eﬀect of some simplifying assumptions
made in the literature to turn the Hessian spectral analysis tractable. Those authors performed a theoretical
analysis using random matrix theory that did not make such simplifying assumptions. They found that
the Hessian eigenspectra for a broad category of nonlinear models can have diﬀerent behaviors. They can
exhibit either single or multiple bulks of eigenvalues; they can have isolated eigenvalues away from the bulk,
and even distributions with bounded and unbounded support. In our work, we shall see that some of the
eigenspectra we ﬁnd also exhibit these very diﬀerent kinds of spectral behavior.
The teacher-student setup as an investigative tool had a revival in recent work by Goldt et al. (2020), where
they studied the dynamics of online stochastic gradient descent for a two-layer teacher-student setup. They
focused on the eﬀect of overparameterization of the student network with respect to the teacher network
and found that, when training both layers, the generalization error either stayed constant or decreased with
student size, depending on the speciﬁc activation function chosen for both networks. This result provided
a rigorous foundation for a series of earlier papers that studied the teacher-student setup in soft committee
machines (Biehl & Schwarze, 1995; Saad & Solla, 1995a;b). Although their object of study is diﬀerent from
ours, we share some of the underlying assumptions, namely, the input data distribution in the teacher-student
setup.
Paper structure In the next subsection, we formally describe the teacher-student setup as well as the
notation used in the rest of this work. In Section 2, we derive the associated gradient and Hessian equations
as well as the learning dynamics near the optimal point. In Section 3, we answer our two main questions
for linear networks, where we are able to fully characterize the probability distribution for the eigenvalues of
the Hessian matrix. In Sections 4 and 5, the same analysis is performed for networks with polynomial and
another non-linear activation function, the error function. Finally, in Section 6, we summarize our work and
discuss possible future work.
1.1 Teacher-Student Setup
We use a teacher-student setup where a neural network, called the student, has to learn a function represented
by yet another neural network, called the teacher. The teacher network is a ﬁxed neural network which is
randomly initialized and is not trained, serving only to create a learning problem that we can tune to alter
its complexity.
2Under review as submission to TMLR
We can control several parameters in both the student and the teacher networks, like the number of layers,
the activation function or whether there are any biases in the linear transformations at each layer. Figure 1
depicts the neural networks under study. However, to be able to study the behavior of the student network
at the optimal point, we assume that the parameters that deﬁne the architectures of both the teacher and
student are the same. Doing so, enables us to always know one optimal solution where the student is capable
of reproducing the output of the teacher, which is when all the weights and biases of the student coincide
with those of the teacher network.
To further simplify the analysis we make the following architectural choices:
•We work with two-layer neural networks, with one hidden layer with a given activation function,
and one linear output layer (no activation function).
•We do not use biases, so that at each layer the pre-activation vector is given by a linear map.
•The output is always a single real number.
•The weights of the teacher network are taken from a normal distribution centered at zero with
variance1
N, where Nis the size of the previous layer.
Figure 1: Depiction of the architecture of neural networks considered.
Notation used throughout the article We denote by Nithe size of the input layer so that xis a vector
inRNireferred to as input. Similarly, we denote by Nhthe size of the hidden layer, i.e., the number of
neurons in the hidden layer. Matrix W1, which is a Nh×Nimatrix, represents the linear transformation
between the input and the hidden layer. Similarly, matrix W2is the linear transformation between the
hidden layer and the output: it is a 1×Nhmatrix, which can be seen as a row vector due to our choice of
only having a real scalar output.
The output of the student network is represented by y. Under the above architectural choices, the output
of the student network is given by
y=W2g(W1x),
where g:R→Ris the activation function that acts on the entries of the pre-activation vector z=W1x,
yielding the hidden vector h.
We denote the vector of all the network parameters by θ. Any of the previous quantities with an additional
hat above denotes a parameter of the teacher network, e.g. ˆθrepresents the set of parameters of the teacher
andˆythe output of the teacher network.
3Under review as submission to TMLR
We also have some usage conventions for indices. The index iis only used to identify a sample in a given
batch. As such, when talking about inputs x, hidden vectors h, or pre-activation values z, the ﬁrst index
is the batch index if and only if it is an i. Otherwise, we assume that a batch size of one is being used. To
further simplify the notation, we represent the (m, n)entry of the weight matrix W1asW1mnand similarly
forW2.
2 Hessian and Learning Equations
The loss function used in this study is the mean square error (MSE), which, for a batch size of Ninput
vectors, takes the form
L=1
2NN/summationdisplay
i=1(yi−ˆyi)2,
where yiis the output of the student network and ˆyiis the output of the teacher network.
In the simplest case, where the student is a two-layer neural network without any biases, the Hessian matrix
for the derivatives of the loss function with respect to the parameters of the student can be decomposed into
three diﬀerent submatrices, or blocks, as
H=A C
CTB
.
Matrix Acontains the derivatives with respect to weights of matrix W1. Similarly, Bcontains the derivatives
with respect to weights of matrix W2. Finally, the matrix in the antidiagonal, C, contains all the cross-
derivatives with respect to the two diﬀerent layers.
Taking the derivative of the loss function with respect to the parameters leads to
∂L
∂W 1mn=1
Nn/summationdisplay
i=1(yi−ˆyi)W2mg′(zim)xin, (1)
∂L
∂W 2k=1
NN/summationdisplay
i=1(yi−ˆyi)hik. (2)
Note that, as we will work with the above equations at the optimal point, the two terms that depend on
yi−ˆyiare always zero. Under the notation and deﬁnitions of Singh et al. (2021), this corresponds to only
studying what the authors call outer product Hessian , since the functional Hessian , which depends on the
term yi−ˆyi, is zero.
If we again take the derivatives with respect to the parameters of Equations (1) and (2) we obtain the
Hessian. In total there are three expressions, one for each of the three blocks of the Hessian,
∂2L
∂W 2k∂W 2j=1
NN/summationdisplay
i=1hikhij, (3)
∂2L
∂W 1pq∂W 1mn=1
NN/summationdisplay
i=1[W2mW2pg′(zim)g′(zip)xinxiq+ (yi−ˆyi)W2mg′′(zim)xinxiqδpm], (4)
∂2L
∂W 2k∂W 1mn=1
NN/summationdisplay
i=1[δkm(yi−ˆyi)g′(zim)xin+hikW2mg′(zim)xin]. (5)
To obtain analytical results, we need additional assumptions. We assume that the input data are distributed
according to a standard normal distribution centered at the origin. With this in mind, we deﬁne the
generalization error as the expected value of the loss function, given by
Lg=Ex[L] =Ex1
2(y−ˆy)2
=/integraldisplay
Rn1
21
2πd/2
(y(x)−ˆy(x))2e−1
2∥x∥2dx.
4Under review as submission to TMLR
The expressions for learning and Hessian components remain the same as long as we make the substitution
1
N/summationtextN
i=1→/integraltext
Rn/parenleftbig1
2π/parenrightbigd/2e−1
2∥x∥2dx. The advantage of working with the generalization error is that we are
now able to perform analytical computations by taking expected values of Equations (3-5). These result
in expressions that depend on the weights of the student network, which coincide with the weights of the
teacher network at the optimal point. From this point onwards, we will only work with the generalization
error, denoting it by Lto simplify the notation. Similarly, unless stated otherwise, we use the designations
loss function and generalization error interchangeably.
2.1 Learning Dynamics near the Optimal Point
Near the optimal point, θ∗, which is the minimum of the loss function, L(θ), with θbeing the set of all
trainable parameters of the network, the Hessian matrix determines the convergence rate of a network
initialized at a point θ′close to θ∗. To see this, we look at the gradient ﬂow dynamicsdθ
dt=−dL
dθaround θ∗,
by taking the Taylor series of L(θ)around θ∗,
L(θ)≈ L(θ∗) +D/summationdisplay
i=1∂L(θ∗)
∂θiδθi+1
2D/summationdisplay
i,j=1∂2L(θ∗)
∂θi∂θjδθiδθj
=1
2D/summationdisplay
i,j=1δθiHijδθj,
with δθi= (θi−θ∗
i),Hijbeing the (i, j)Hessian matrix component, and where D= dim( θ)is the number
of parameters. The ﬁrst two terms of the Taylor series vanish at the minimizer of the loss function, as it is
a zero as well as an absolute minimum. Now, we have that
dθ
dt=−∇ θL ≈ − Hδθ =⇒δθ(t)≈δθ(0)e−Ht,
so that the exponential of the Hessian controls how the network parameters evolve with time. If we know
the eigensystem of H, i.e., the unit eigenvectors viand respective eigenvalues λisuch that Hvi=λivi, we
have that
∥δθ(t)∥2≈D/summationdisplay
i=1e−2λit/vextendsingle/vextendsinglevT
iδθ(0)/vextendsingle/vextendsingle2. (6)
This shows that, ultimately, the eigenvalues near the optimum point of the loss function drive the parameter
evolution under gradient descent.
We can lose the dependency on the eigenvectors in Equation (6) by averaging over the initial condition,
δθ(0), assuming it follows a multivariate Gaussian distribution with mean zero and variance σ2
0, yielding
/angbracketleftig
∥δθ(t)∥2/angbracketrightig
δθ(0)=D/summationdisplay
i=1e−2λit/angbracketleftig/vextendsingle/vextendsinglevT
i·δθ(0)/vextendsingle/vextendsingle2/angbracketrightig
δθ(0)
=σ2
0T/summationdisplay
i=1e−2λit.
An expression for the time evolution of the loss function can thus be derived
⟨L(t)⟩δθ(0)≈1
2/angbracketleftbig
δθ(0)Te−2HtHδθ(0)/angbracketrightbig
δθ(0)
=σ2
0
2Tre−2HtH
=σ2
0
2/integraldisplay+∞
0e−2λtλρ(λ)dλ , (7)
5Under review as submission to TMLR
where ρ(λ)is the eigenspectrum of the Hessian matrix.
For a single realization of the teacher-student setup, where the spectrum is discrete, we thus expect the
large-time behavior of the loss function near the optimal point to be determined by the smallest non-zero
eigenvalue. This behavior can be clearly seen in the linear network, discussed in the next section.
3 The Linear Network
In a linear network, the activation function is the identity function, i.e., we have g(x) =x, so that g′(x) = 1
andg′′(x) = 0 . Its ﬁrst derivative is always equal to one and the second derivative is always equal to zero.
Taking the second derivatives of the generalization error with respect to the student’s weights leads to the
following Hessian components at the optimal point,
∂2L
∂W 2k∂W 2j=Ex[hkhj] =/summationdisplay
mW1kmW1jm, (8)
∂2L
∂W 1pq∂W 1mn=W2mW2pδnq, (9)
∂2L
∂W 2k∂W 1mn=W1knW2m, (10)
where we used the fact that the loss function is zero at the optimum. We ﬁnd that, for linear networks, each
sub-matrix of the Hessian depends only on weights of a single layer and only the anti-diagonal contains cross
terms.
As these equations are valid at the optimal point, we have that the teacher’s weights are the same as the
student’s weights. Thus, using Equations (9-10), together with the teacher weights, we can directly compute
the Hessian matrix of a student at the optimal point. From the Hessian, we can then also calculate the
eigenvalues. More interestingly, if we know how the weights of the teacher network are distributed, we can
even derive an expression for the eigenvalue distribution of the Hessian.
3.1 Eigenvalues of the Hessian at the Optimal Point
The linear teacher-student setup exhibits a particularly interesting structure for the eigenvalues of the Hessian
at the optimal point: they are given by sums of eigenvalues of each of the diagonal sub-matrices of the Hessian.
For a detailed analysis we refer the reader to Appendix A.
Thus, in the linear network, we are able to calculate the entire eigenspectrum from the eigenspectrum of
each diagonal sub-matrix of the Hessian. The Asub-matrix has just one non-zero eigenvalue given by/summationtextNh
k=1W2
2k. This eigenvalue has a multiplicity equal to Ni, the input dimension. Under the assumption
that the components of ˆW2were taken from a normal distribution with zero mean and variance1
Nh, we
ﬁnd that this eigenvalue follows a scaled chi-squared distribution with Nhdegrees of freedom, λ∼1
Nhχ2
Nh.
On the other hand, the Bblock has min(Ni, Nh)non-zero eigenvalues that asymptotically follow a scaled
Marchenko-Pastur distribution (Götze & Tikhomirov, 2004). The probability density function of the scaled
Marchenko-Pastur distribution is given by Equation (15) in Appendix A.
The eigenvalues of the entire Hessian matrix are given by summing the eigenvalue of the upper left block with
the (possibly zero-padded) eigenvalues of the lower right block, yielding Ninon-zero eigenvalues in total.
Thus, if Ni≤Nh, asymptotically the eigenvalues follow a convolution of the scaled chi-square distribution
with the scaled Marchenko-Pastur distribution, denoted by C. IfNi> N h, then the eigenvalue distribution
is a mixed distribution, asymptotically given by
λ∼Ni−Nh
Ni1
Nhχ2
Nh
+Nh
NiC.
Figure 2 illustrates both cases. On the left, we have Ni< N h, thus the eigenvalue distribution is fully
described by the convolution of the scaled chi-squared distribution and the scaled Marchenko-Pastur distri-
6Under review as submission to TMLR
bution. On the right, we have Ni> N h, thus the distribution is mixed, having contributions from both the
convoluted distribution as well as the scaled chi-squared distribution.
(a)Ni= 10 , Nh= 20 , only the convolution is valid.
 (b)Ni= 30 , Nh= 10 , the distribution is mixed.
Figure 2: Agreement between the predicted eigenvalue distribution for the Hessian and numerical simulations.
A detailed proof for the expression of the Hessian eigenspectrum in the linear teacher-student setup can
be found in Appendix A. This eigenvalue distribution is quite diﬀerent from all the others distributions
considered in the present work. The most notable diﬀerence is that, unlike non-linear networks, here there
is a lack of a bulk of eigenvalues concentrated near zero. As such, the long term behavior of Equation (7)
can be more easily checked in this case, as we can clearly distinguish the correct eigenvalues from numerical
error when we diagonalize the Hessian matrix that is determined by a test dataset.
Figure 3 shows, in blue, the loss function evolution with time of a linear student network initialized near the
optimal point. Time here is to be understood as the product of the number of iterations with the learning
rate. We obtain this by randomly generating a teacher network, copying its weights to the student, adding
random noise to them, and ﬁnally training the student using stochastic gradient descent. In black, we see the
function αexp (−2λmint), where λminis the smallest non-zero eigenvalue at the optimal point, calculated
beforehand. Here, αis such that L(tf) =αexp (−2λmintf)so that both functions meet at the ﬁnal time,
tf. Initially the loss function decays faster due to the contribution from the larger eigenvalues. However, the
long-time behavior of the loss function is well determined by just the exponential of its smallest eigenvalue,
as predicted.
Figure 3: Agreement between the loss function of a student network initialized near the optimum point and
the exponential of the smallest eigenvalue.
7Under review as submission to TMLR
3.2 Number of Eﬀective Parameters
In the linear network, we ﬁnd that for a randomly generated teacher with weights following a normal
distribution, the Hessian at the optimal point always has Nipositive eigenvalues, with all others being zero.
Thus, in terms of the loss landscape, of the (Ni+ 1)Nhpossible directions in which the loss can vary, only
a set of Nidirections are required to fully characterize the behavior of the loss function around the optimal
point. The loss function does not vary in all the other directions orthogonal to these Nidirections.
We can look at this from another perspective, trying to answer the following question: "When is a student
network equivalent to another student network?". Here, "equivalent" is to be understood as, given any input,
both students giving the exact same output. To answer this question, we notice that, due to the activation
function being the identity, the student network takes the form
y=W2W1x=Ax,
where Ais a1×Nimatrix. Therefore, for two students to be equivalent, it is only required that the
components of Aare equal. This shows that, even though the network has (Ni+ 1)Nhparameters, due to
the architecture, it can be encoded using only Niparameters.
Thus, for the linear network, we ﬁnd that the number of positive eigenvalues of the Hessian, or equivalently,
the rank of the Hessian, at the optimal point, gives a measure of an eﬀective number of parameters.
4 Polynomial Networks
4.1 The Quadratic Network
In this subsection, we consider a teacher-student setup, where the underlying activation function is of the
form
g(x) =x+ϵx2.
The associated ﬁrst and second derivatives are g′(x) = 1 + 2 ϵxandg′′(x) = 2 ϵ.
Following the same procedure as for the linear network, we solve the associated Gaussian integrals and arrive
at the following expressions for the Hessian matrix components:
∂2L
∂W 2k∂W 2j=/summationdisplay
mW1kmW1jm+ϵ2F1(k, j), (11)
∂2L
∂W 1pq∂W 1mn=W2mW2pδnq+ 4ϵ2F2(p, q, m, n ), (12)
∂2L
∂W 2k∂W 1mn=W2mW1kn+ 2ϵ2F3(k, m, n ), (13)
with
F1(k, j) =/summationdisplay
mW2
1km/summationdisplay
mW2
1jm+ 2/parenleftigg/summationdisplay
mW1kmW1jm/parenrightigg2
,
F2(p, q, m, n ) =δnq/summationdisplay
rW1mrW1pr+W1mnW1pq+W1mqW1pn,
F3(k, m, n ) =W1mn/summationdisplay
p(W1kp)2+ 2W1kn/summationdisplay
pW1mpW1kp.
As expected, for ϵ= 0, we recover Equations (9-10) for the linear network. However, unlike the linear case,
here we were not able to analytically obtain the eigenvalues. Nevertheless, we brieﬂy make some remarks on
the eigenspectrum of quadratic networks.
The eigenspectrum of a realization of this teacher-student setup where the weights of the teacher are sampled
from a normal distribution exhibits a bulk of eigenvalues near zero, with a single eigenvalue being much
8Under review as submission to TMLR
farther apart from the bulk. In Figure 4, we see that behavior when the input dimension equals the hidden
dimension: as the dimension of the network increases, the bulk of the eigenvalues gets closer to zero. In
opposition, the highest eigenvalue, which is always far away from the bulk near zero, diverges, leaving another
much smaller bulk of eigenvalues for larger and larger values of λ, as the dimensions of the network increase.
Figure 4: Eigenspectrum distribution for a quadratic teacher-student setup with ϵ= 1andNi=Nh=N.
4.1.1 Number of Eﬀective Parameters
In the linear case, we saw that a network with Niinput neurons and Nhhidden neurons has (Ni+ 1)Nh
parameters in total, but its Hessian matrix has at most Ninon-zero eigenvalues. In the quadratic case, if Nh
is the number of hidden neurons, then above a certain threshold, say T, for Nh> Tthe number of zeroes of
the Hessian is given by (Nh−1)Nh/2, which are the triangular numbers ( 0,1,3,6,10,15,21,28, . . .).
We use the same strategy as in the linear network to ﬁnd the number of eﬀective parameters, Neﬀ. As the
activation function is only used once, in the hidden layer, we know that the function being represented by
the student network is a quadratic function of the input vector x. Such a function can be written as
f(⃗ x) =A·x+xTBx,
without a constant term as the network has no biases. Such a quadratic function is represented by Ni
parameters for AandNi(Ni+ 1)/2forB. The latter come from the fact that, in the expression xTBx=/summationtextN
i,j=0xixjBij, for i̸=j, what matters is the sum Bji+Bij, and so we only need to deal with symmetric
matrices.
As such, the number of eﬀective parameters of the quadratic network is Ni+Ni(Ni+1)
2. Note however, that
this assumes that the network has enough free parameters in the Hessian matrix, in particular it is only valid
whenever Nh≥Ni.
The previous result is only an upper bound for the total number of independent parameters. The actual
number depends on both NiandNh, because the network may not be able to fully express any degree-two
polynomial in the weights if Nhis not large enough. Numerically, we found that the threshold for this upper
bound to be saturated happens for Nh=Ni. We were able to prove that the number of eﬀective parameters
in the general case is given by
Neﬀ=Ni+Ni(Ni+ 1)
2−ν2+ 3ν
2
H(ν), (14)
9Under review as submission to TMLR
where ν=Ni−NhandH(x)is the Heaviside step function. We refer to Appendix B for a detailed proof of
Equation (14).
4.2 Upper Bound for the Eﬀective Number of Parameters for Higher-Order Polynomials
We can generalize the analysis above for activation functions that are higher-order polynomials. We can
give an upper bound for the eﬀective number of parameters by counting the number of parameters that
describe an n-th polynomial function of a vector x. Similarly to the quadratic network approach, a degree- n
polynomial can be represented as
f(⃗ x) =Ni/summationdisplay
i=1A(1)
ixi+Ni/summationdisplay
i,j=1A(2)
ijxixj+· · ·+Ni/summationdisplay
i1,...,i n=1A(n)
i1...inxi1· · ·xin,
where each A(k)is ak-dimensional symmetric tensor. Finally, counting the number of parameters amounts
to summing the number of symmetric components of each tensor. For a symmetric tensor of order n, the
number of independent parameters is given by/parenleftbigNi+n−1
n/parenrightbig
. With the help of the hockey-stick identity (Jones,
1996), we ﬁnd that
Neﬀ≤n/summationdisplay
k=1Ni+k−1
k
=Ni+n
n
−1,
which is the upper bound for the number of eﬀective parameters for a polynomial activation function of
degree n.
5 Error Function Network
Finally. we study the case where the activation function is the error function
g(x) = erfx√
2
=2√π/integraldisplayx/√
2
0e−t2dt ,
which has ﬁrst derivative g′(x) =/radicalbig
2/π e−x2/2and second derivative g′′(x) =−x g′(x). This choice of
activation function is common in machine learning as it is a sigmoid function. Moreover, its associated
Gaussian integrals are well deﬁned analytically. The derivation and ﬁnal system of Equations (16-18) is
available in Appendix C.
The results of a numerical analysis of the eigenspectrum of the Hessian matrix obtained from Equations
(16-18) are displayed in Figure 5. We see a similar structure to the quadratic case. There is a bulk of
positive eigenvalues near zero that, as the network size increases, gets closer and closer to zero. There is
another bulk of larger eigenvalues that remains ﬁxed as the network size increases. For a single realization
of this teacher-student setup, we ﬁnd that the number of eigenvalues belonging to the bulk away from zero
is always equal to Nh, the number of parameters in W2. This may be due to the fact that the output layer
does not have an activation function whereas the hidden layer does.
10Under review as submission to TMLR
Figure 5: Eigenspectrum distribution for a error function teacher-student setup with Ni=Nh=N.
5.1 Number of Eﬀective Parameters
For the error function network, we empirically ﬁnd that no matter how large the networks are, the Hessian
matrix is always full rank. If we try to follow our strategy for counting the number of eﬀective parameters,
as the teacher network is some linear combination of error functions, we can no longer compress this network
onto a ﬁnite polynomial. Thus, it appears that no further compression is possible and the entire suite of
parameters of the teacher is required for the student to perfectly replicate the teacher’s outputs.
6 Conclusion and Future Work
In this work, we explored the Hessian rank as a measure of the eﬀective number of parameters. Under the
teacher-student setup, where the teacher and the student networks share their architecture, we found that
this measure is valid for polynomial networks, for a suﬃciently large number of hidden neurons. For a more
complicated activation function, like the error function, this approach breaks down as the Hessian is always
full rank at the optimal point, an indication that no further compression of the number of parameters can
be done for such networks.
For all linear, quadratic, and error function networks, we were able to derive the equations for the Hessian
components, under the assumption of the input distribution being a Gaussian centered at zero. Speciﬁcally
for linear networks, we were able to analytically derive the distribution of the eigenvalues of the Hessian,
when the distribution of weights of the teacher network is known. In the case where this distribution is
Gaussian, we found that, in general, the distribution of eigenvalues of the Hessian asymptotically follows a
convolution of a scaled Marchenko-Pastur distribution with a scaled chi-squared distribution.
Finally, we point out future research directions in this teacher-student perspective. Firstly, we could study
how this notion of an eﬀective number of parameters changes if we add biases to the analysis. Following the
same strategy as in this work, we would expect to obtain the same results incremented by one. This new
degree of freedom comes from the constant term of the polynomial function in the compressed representa-
tion. Following the same train of thought, we could also try to generalize the result for deeper networks.
On the other hand, still working with two-layer neural networks, one can possibly study the eﬀects of over-
parameterization on student networks. This is because, in the case where the student has more parameters
than the teacher, the optimal solution still exists. The optimal solution in this case can be obtained by
zeroing out any extra neurons present in the student network. Comparing the Hessian spectrum of such an
overparameterized student with the ones obtained in this work could provide insight into the unreasonable
11Under review as submission to TMLR
performance of large neural networks. Finally, one other possible path to explore would be to study the
Hessian rank evolution with time.
References
Shunta Akiyama and Taiji Suzuki. Excess risk of two-layer reLU neural networks in teacher-student settings
and its superiority to kernel methods. In The Eleventh International Conference on Learning Representa-
tions , 2023. URL https://openreview.net/forum?id=6doXHqwMayf .
Michael Biehl and Holm Schwarze. Learning by on-line gradient descent. Journal of Physics A, Mathe-
matical and General , 28(3):643–656, February 1995. ISSN 0305-4470. doi: 10.1088/0305-4470/28/3/018.
Relation: http://www.rug.nl/informatica/organisatie/overorganisatie/iwi Rights: University of Gronin-
gen. Research Institute for Mathematics and Computing Science (IWI).
Sebastian Goldt, Madhu S Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborová. Dynamics of
stochastic gradient descent for two-layer neural networks in the teacher-student setup. Journal of Statistical
Mechanics: Theory and Experiment , 2020(12):124010, dec 2020. doi: 10.1088/1742-5468/abc61e.
Friedrich Götze and Alexander Tikhomirov. Rate of convergence in probability to the Marchenko-Pastur
law. Bernoulli , 10(3):503–548, 2004.
Charles H. Jones. Generalized hockey stick identities and n-dimensional block walking. Fibonacci Quarterly ,
34(3):280–288, 1996.
N. E. Korotkov and Alexander N. Korotkov. Integrals related to the error function . CRC Press, Taylor &
Francis Group, Boca Raton, 2020. ISBN 978-0-367-40820-6.
Zhenyu Liao and Michael W. Mahoney. Hessian eigenspectra of more realistic nonlinear models, March 2021.
URL http://arxiv.org/abs/2103.01519 . arXiv:2103.01519 [cs, math, stat].
Wesley J. Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking Parameter Counting in Deep
Models: Eﬀective Dimensionality Revisited, May 2020. URL http://arxiv.org/abs/2003.02139 . Num-
ber: arXiv:2003.02139 arXiv:2003.02139 [cs, stat].
Edward W. Ng and Murray Geller. A table of integrals of the Error functions. Journal of Research of the
National Bureau of Standards, Section B: Mathematical Sciences , 73B(1):1, January 1969. ISSN 0098-8979.
doi: 10.6028/jres.073B.001. URL https://nvlpubs.nist.gov/nistpubs/jres/73B/jresv73Bn1p1_A1b.
pdf.
David Saad and Sara A. Solla. On-line learning in soft committee machines. Phys. Rev. E , 52:4225–4243, Oct
1995a. doi: 10.1103/PhysRevE.52.4225. URL https://link.aps.org/doi/10.1103/PhysRevE.52.4225 .
David Saad and Sara A. Solla. Exact solution for on-line learning in multilayer neural networks. Phys. Rev.
Lett., 74:4337–4340, May 1995b. doi: 10.1103/PhysRevLett.74.4337. URL https://link.aps.org/doi/
10.1103/PhysRevLett.74.4337 .
Itay M. Safran, Gilad Yehudai, and Ohad Shamir. The Eﬀects of Mild Over-parameterization on the Op-
timization Landscape of Shallow ReLU Neural Networks. In Proceedings of Thirty Fourth Conference
on Learning Theory , pp. 3889–3934. PMLR, July 2021. URL https://proceedings.mlr.press/v134/
safran21a.html . ISSN: 2640-3498.
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the Hessian in Deep Learning: Singularity
and Beyond, October 2017. URL http://arxiv.org/abs/1611.07476 . arXiv:1611.07476 [cs].
Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical Analysis of the
Hessian of Over-Parametrized Neural Networks, May 2018. URL http://arxiv.org/abs/1706.04454 .
arXiv:1706.04454 [cs].
12Under review as submission to TMLR
Sidak Pal Singh, Gregor Bachmann, and Thomas Hofmann. Analytic insights into structure and rank of
neural network Hessian maps. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.),
Advances in Neural Information Processing Systems , 2021. URL https://openreview.net/forum?id=
otDgw7LM7Nn .
Joram Soch, The Book of Statistical Proofs, Maja, Pietro Monticone, Thomas J. Faulkenberry, Alex Kipnis,
Kenneth Petrykowski, Carsten Allefeld, Heiner Atze, Adam Knapp, Ciarán D. McInerney, Lo4ding00,
and amvosk. StatProofBook/StatProofBook.github.io: StatProofBook 2023, January 2024. URL https:
//zenodo.org/doi/10.5281/zenodo.4305949 .
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning One-hidden-layer ReLU Networks
via Gradient Descent. In Proceedings of the Twenty-Second International Conference on Artiﬁcial Intel-
ligence and Statistics , pp. 1524–1534. PMLR, April 2019. URL https://proceedings.mlr.press/v89/
zhang19g.html . ISSN: 2640-3498.
A The Linear Teacher-Student Case
To understand where the expression for the distribution of the eigenvalues of the Hessian comes from, it is
helpful to ﬁrst study the eigenvalues of each Hessian sub-matrix separately.
A.1 Eigenvalues of the Ablock of the Hessian
TheAblock of the Hessian matrix has an interesting symmetry to it. Its components are given by Equation
(9),
∂2L
∂W 1pq∂W 1mn=W2mW2pδnq.
This matrix is very sparse. We ﬂatten the matrix W1by following a row-ﬁrst convention, i.e., we ﬂatten
the matrix by concatenating its rows. As such, the δnqterm makes it so that we can only have a non-zero
element every Nhsteps, when kmod Nh= 0, with kbeing the current row. The eﬀect of this term is that
this matrix will be built of identity INiblocks being multiplied by a real number.
For example, for Ni= 3andNh= 4we would have

W22
1 0 0 W2 1W2 2 0 0 W2 1W2 3 0 0 W2 1W2 4 0 0
0 W22
1 0 0 W2 1W2 2 0 0 W2 1W2 3 0 0 W2 1W2 4 0
0 0 W22
1 0 0 W2 1W2 2 0 0 W2 1W2 3 0 0 W2 1W2 4
W2 1W2 2 0 0 W22
2 0 0 W2 2W2 3 0 0 W2 2W2 4 0 0
0 W2 1W2 2 0 0 W22
2 0 0 W2 2W2 3 0 0 W2 2W2 4 0
0 0 W2 1W2 2 0 0 W22
2 0 0 W2 2W2 3 0 0 W2 2W2 4
W2 1W2 3 0 0 W2 2W2 3 0 0 W22
3 0 0 W2 3W2 4 0 0
0 W2 1W2 3 0 0 W2 2W2 3 0 0 W22
3 0 0 W2 3W2 4 0
0 0 W2 1W2 3 0 0 W2 2W2 3 0 0 W22
3 0 0 W2 3W2 4
W2 1W2 4 0 0 W2 2W2 4 0 0 W2 3W2 4 0 0 W22
4 0 0
0 W2 1W2 4 0 0 W2 2W2 4 0 0 W2 3W2 4 0 0 W22
4 0
0 0 W2 1W2 4 0 0 W2 2W2 4 0 0 W2 3W2 4 0 0 W22
4

We can rewrite the above matrix in a more compact notation using the following tensor product

W22
1 W2 1W2 2W2 1W2 3W2 1W2 4
W2 1W2 2 W22
2 W2 2W2 3W2 2W2 4
W2 1W2 3W2 2W2 3 W22
3 W2 3W2 4
W2 1W2 4W2 2W2 4W2 3W2 4 W22
4
⊗I3.
One important consequence of the above is that every eigenvalue of the original matrix will have a multiplicity
ofNi. In general, this block of the Hessian matrix can be written as (W2WT
2)⊗INi, with W2∈RNh. Thus,
W2hasNh−1orthogonal directions with eigenvalue equal to zero. To get the other eigenvalue, note that
by associativity
(W2WT
2)W2=♣♣W2♣♣2W2.
13Under review as submission to TMLR
From this we get that the other eigenvalue is given by/summationtextNh
i=1W2
2i. We thus have the values and multiplicities
of all eigenvalues of the Ablock of the Hessian matrix: we have Ni(Nh−1)eigenvalues equal to zero and
Niequal to/summationtextNh
i=1W2
2i. Being sums of squares of a normal distribution with ﬁxed variance, this eigenvalue
follows a scaled chi-square distribution with Nhdegrees of freedom, λ∼σ2χ2
Nh. In the following ﬁgure we
can see some numerical simulations of this part of the Hessian as well as the predicted distribution on top.
(a)Nh= 3
 (b)Nh= 100
Figure 6: Agreement between the predicted distribution for this eigenvalue of the ﬁrst diagonal block, A, of
the Hessian and numerical simulations.
A.2 Eigenvalues of the Bblock of the Hessian
This block is given by the expression W1WT
1. Asymptotically, the eigenvalues of this block then follow a
scaled Marchenko-Pastur distribution (because the entries are not being normalized) (Götze & Tikhomirov,
2004). This distribution is scaled by a factor of Ni. Furthermore, if Nh> N ithe matrix is singular and has
only Ninon-zero eigenvalues. In the other cases it has Nhnon-zero eigenvalues.
The pdf of the scaled distribution of eigenvalues is given by
pd f(x) =1
2πσ2/radicalbig
(λ+−x) (x−λ−)
λx1x∈[λ−,λ+](1λ≤1+λ1λ>1), (15)
where λ=Nh/Ni,λ±=σ2/parenleftig
1±√
λ2
,σ2=1
Niand1Cis an indicator function valid in the region denoted
by the condition C. In the following ﬁgure we can see the agreement between the above predicted distribution
and numerical experiments.
14Under review as submission to TMLR
(a)Ni= 30 , Nh= 30
 (b)Ni= 100 , Nh= 30
Figure 7: Agreement between the predicted eigenvalue distribution for the second diagonal block, B, of the
Hessian and numerical simulations.
A.3 A description of the positive eigenvalues for the Hessian
The Hessian matrix may be written as
H=W2WT
2⊗INiW2⊗WT
1
WT
2⊗W1 W1WT
1
.
We have that W2is a column vector and, as analysed previously, its eigenvector is v:=WT
2, we shall call the
associated eigenvector λ2. From the rank-nullity theorem for W1, we have that Ni=rank (W1) +null(W1).
From the SVD decomposition of W1, we have that there will be as many non-zero singular values as the
rank of W1. For each one of these non-zero singular values, call it√λ1, letz′be the corresponding singular
vector. Then we have that /braceleftigg
W1z′=√λ1z
WT
1z=√λ1z′,
which implies that zis an eigenvector of W1WT
1associated with the eigenvalue λ1.
Now, note that, with the vector given by y= [(v⊗z′)√λ1z]T, the product of the Hessian matrix with the
above vector gives Hy= (λ1+λ2)y. Thus, for each non-zero singular value of W1we have obtained an
eigenvector of the Hessian where the corresponding eigenvalue is indeed the sum of the eigenvalues of each
diagonal block matrix of the Hessian. The number of such eigenvectors is equal to the rank of W1.
Now, let z′be a vector in the kernel of W1. The above relations remain true if we take into account that
nowλ1= 0. And thus the vector y=/bracketleftbig
(v⊗z′)⃗0/bracketrightbigT, where ⃗0is a vector of Nhzeros, is also an eigenvector
ofH, asHy=λ2y= (0 + λ2)y= (λ1+λ2)y. For the random matrices we considered for the teacher, the
kernel of W1is non-trivial almost surely whenever Ni≥Nh.
Thus, in general, by the rank-nullity theorem for W1, we will have Nilinearly independent eigenvectors of the
Hessian matrix where the associated eigenvalues are the sum of the eigenvalues of each diagonal submatrix
of the Hessian.
15Under review as submission to TMLR
B Counting the Eﬀective Parameters of a Polynomial Network
B.1 Quadratic Network: The Nh< N icase
To make this discussion more clear, we shall consider the case where Nh=Ni−1. Here, two interesting
eﬀects happen: the ﬁrst is that both the vector ⃗Aand the matrix Blose a dependency on the last parameter
W2, which accounts for the loss of one of the degrees of freedom, when compared to the upper bound. If we
keep reducing Nhthe number of degrees of freedom should also decrease linearly, for the same reason.
The second eﬀect is on the rank of the matrix B. In the case where Ni≤Nh, the matrix Bis made up of
the sum of at least Nirandom rank one matrices and so, in general, it will be a rank Nisymmetric matrix.
However, when we have Nh< N i, now Bis made up of only Nhrank one matrices and, as such, it has a
smaller number of independent parameters.
To count this new number, we note that any symmetric real matrix has an eigenvalue decomposition such
that
B=XΛXT,
where Λis a diagonal matrix whose components are the eigenvalues of BandXis an Ni×Nireal orthogonal
matrix. Furthermore, for real symmetric matrices, the number of non-zero eigenvalues is equal to the rank
of the matrix.
If we assume that Bhas full rank, then the degrees of freedom of Bcan be derived from the following
exercise:
•Each non-zero eigenvalue in Λis one additional degree of freedom, giving Nin total.
•The orthogonal matrix XhasNi(Ni−1)
2independent components.
•In total we thus have Ni+Ni(Ni−1)
2=Ni(Ni+1)
2.
Thus, we get the same number that we obtained before for the independent components of B, if it is a full
rank matrix.
Now, if Bis a matrix of rank r < N i, the following happens:
•We now have only rdegrees of freedom from the eigenvalues, as the other Ni−reigenvalues are
zero.
•The orthogonal matrix has additional symmetries that appear due to the degeneracy of the zero
eigenvalues. In particular, we need to remove the number of permutations of columns of Xas-
sociated with the null eigenvalue, as they will yield the same matrix B. Thus, we must remove
(Ni−r)(Ni−r−1)
2=/parenleftbigNi−r
2/parenrightbig
degrees of freedom.
•Another way to state the above is that the space of the zero eigenvectors of Bdoes not contribute
towards the number of independent components and, as such, we need to remove the number of
degrees of freedom that those eigenvectors used to contribute. That happens to be/parenleftbigNi−r
2/parenrightbig
.
•Yet another way to put is to say that the matrix Bis independent of rotations of the space associated
with the null eigenvectors as, as such, we need to removen(n−1)
2degrees of freedom from the
orthogonal matrix.
•Thus, when Bis a rank rmatrix we have
r+Ni(Ni−1)
2−Ni−r
2
degrees of freedom.
16Under review as submission to TMLR
The overall result is that, whenever r=Nh< N i, the number of eﬀective parameters of the network is given
by
Ni+Ni(Ni+ 1)
2−(Ni−r)−(Ni−r+Ni−r
2
) =Ni+Ni(Ni+ 1)
2−(Ni−r)2+ 3(Ni−r)
2
,
which yields the predicted numerical results.
B.2 The number of symmetric components of a tensor of order n
Let us recall here how to obtain the number of symmetric components of a tensor of order n. Let A(n)be a
symmetric tensor of order nwhere the size of each dimension is Ni. Then/parenleftbigNi
n/parenrightbig
gives the number of distinct
groupings of nindices, thus accounting for the fact that the tensor is symmetric.
However, we are missing the terms with repeated indices. One way to account for them is to introduce some
abstract symbol R1to indicate the repetition of some element.
For example, in counting the number of independent components of a symmetric matrix,/parenleftbigNi+1
2/parenrightbig
represents
all the possible pairings of the Nidiﬀerent values for each index as well as the additional R1symbol, which
we must interpret as the instruction "repeat the 1st lowest number in this pairing". Thus, ¶1, R1♢ ↔ x1x1
whereas ¶1,2♢ ↔x1x2↔x2x1.
To generalize this to higher dimensions we just need to add additional repetition symbols that are interpreted
as "Rnmeans that the nth lowest number in this grouping is to be repeated once". Thus, in the cubic case
we interpret ¶1, R1,2♢ ↔ x1x1x2and¶R2,1,2♢ ↔ x1x2x2– giving us the components of the tensor with
two equal indices. Also, we interpret ¶R1,2, R2♢ ↔x2x2x2, which gives the components with three repeated
indices. With the addition of these symbols we can see that the expression/parenleftbigNi+n−1
n/parenrightbig
counts all independent
symmetric components of an n-th order tensor.
17Under review as submission to TMLR
C The Error Function Network Hessian equations
The equations for the case where the activation function is the error function are:
∂L
∂W 1pq∂W 1mn=2W2mW2p
π√
det Σ−1[Σ]nq, (16)
∂2L
∂W 2k∂W 2j=2
πarctan
/summationtextn
i=1WjiWki/radicalig
1 +/summationtextn
i=1W2
ki+/summationtextn
i=1W2
ji+/parenleftbig/summationtextn
i=1W2
ki/parenrightbig/parenleftbig/summationtextn
i=1W2
ji/parenrightbig
−/parenleftbig/summationtextn
i=1WjiWki/parenrightbig2
,(17)
∂2L
∂W 2k∂W 1mn=2
π/radicalbig
1 +W1mWT
1mW2mW1kn−W1kWT
1mW1mn
1+W1mWT
1m/radicalbigg
1 +W1kWT
1k−(W1mWT
1k)2
1+W1mWT
1m, (18)
where we have that
Σ = I−WT
mWm
1 +WmWTm−WT
pWp
1 +WpWTp−
(WmWT
p)2(WT
mWm
1+WmWTm+WT
pWp
1+WpWTp)−(WmWT
p)(WT
mWp+WT
pWm)
(1 + WpWTp)(1 + WmWTm)−(WmWTp)2

.
The Upper Left block Looking at the upper left part of the Hessian, which contains the W1−W1
correlations, we need to calculate the following expected value
∂L
∂W 1pq∂W 1mn=W2mW2pEx[g′(zm)g′(zp)xnxq] +W2mδpmEx[g′′(zm)xnxq]
Focusing on the ﬁrst additive term we have that
g′(zm)g′(zp) =2
πe−1
2/summationtext
k,l(W1mkW1ml+W1pkW1pl)xkxl=2
πe−1
2/summationtext
k,lAmp
klxkxl
i.e., the product of the two derivatives of the activation function result is an unnormalized multivariate
Gaussian distribution with inverse covariance matrix Amp. The components of this matrix are Amp
kl=
W1mkW1ml+W1pkW1pl.
When taking the expected value, what we will eﬀectively do is calculate the second moments of a multivariate
Gaussian distribution, up to a multiplicative constant. This multivariate normal distribution has inverse
covariance matrix equal to Σ−1=Amp+I.
As for the second additive term, noting that g′′(x) =−xg′(x)is an odd function, we can directly see that
Ex[g′′(zm)xnxq] =−/summationdisplay
kW1mkE[g′(zm)xkxnxq] = 0
because third moments of a normal distribution are always zero.
Derivation of the Second Moments We want to calculate Ex[g′(zm)g′(zp)xnxq]. We saw that this could
be reformulated in terms of the second moments when the distribution is the modiﬁed multivariate normal
distribution with inverse covariance matrix Σ−1=I+Amp, where Amp=WT
mWm+WT
pWpandWm, Wp
are row vectors of the matrix W1. Taking into account the missing prefactor of the normal distribution as
well as the factor of 2/πwe have that
Ex[g′(zm)g′(zp)xnxq] =2
π√
det Σ/integraldisplay
dx1√
det 2πΣxnxqe−1
2xTΣ−1x=2
π√
det Σ−1[Σ]nq.
And so all we have to do is ﬁnd the determinant of Σ−1=I+Ampas well as component nqof its inverse.
18Under review as submission to TMLR
To accomplish this we will make use of the following two linear algebra results: the Sherman–Morrison
formula for the inverse of
(A+uvT)−1=A−1−A−1uvTA−1
1 +vTA−1u,
valid if 1 +vTA−1u̸= 0; and the matrix determinant lemma which states that
det(A+uvT) = (1 + vTA−1u) det A .
By applying the above two formulas inductively we ﬁnd that the determinant of Σ−1is
det(I+Amp) = 1 + WmWT
m+WpWT
p+WmWT
mWpWT
p−(WmWT
p)2
and that the inverse matrix can be computed as
Σ = I−WT
mWm
1 +WmWTm−WT
pWp
1 +WpWTp−
(WmWT
p)2(WT
mWm
1+WmWTm+WT
pWp
1+WpWTp)−(WmWT
p)(WT
mWp+WT
pWm)
(1 + WpWTp)(1 + WmWTm)−(WmWTp)2

The Lower Right block Now, for the W2−W2block, we need to calculate:
∂2L
∂W 2k∂W 2j=Ex[hkhj] =Ez[g(zk)g(zj)]
Because, zk=/summationtext
iW1kixiand each xi∼N(0,1), we have that zk∼N(0,/summationtext
iW2
1ki)and similarly for zj,
thus both being marginally normal distributed. However, to take the expected value we need to know their
joint distribution. Multivariate statistics tells us that — see for example, Soch et al. (2024) — if xis a
multivariate normal distributed variable, x∼N(µ,Σ), then z=Ax+bis also normally distributed as
z∼N(Aµ+b, AΣAT).
In our case µ= 0,Σ = I, where Iis the n×nidentity matrix, and A=Wk
Wj
. Thus, (zk, zj) =z∼
N(0, AAT)where the covariance matrix
AAT=/summationtextn
i=1W2
ki/summationtextn
i=1WkiWji/summationtextn
i=1WkiWji/summationtextn
i=1W2
ji
.
So we are now in the position to evaluate
Ez[g(zk)g(zj)] =/integraldisplay∞
−∞dzj/integraldisplay∞
−∞dzk(2π)−1det(Σ)−1/2erfzk√
2
erfzj√
2
e−1
2zTΣ−1z
Solving the integral For simplicity let Σ−1=A F
F B
. We will ﬁrst integrate one of the error functions
using the identity
/integraldisplay∞
−∞erf(ax+b)1√
2πσ2exp
−(x−µ)2
2σ2
dx= erfaµ+b√
1 + 2 a2σ2
.
And so, keeping only the terms in zkwe want to solve
/integraldisplay∞
−∞dzkerfzk√
2
e−1
2(Az2
k+2F zjzk)
Completing the square and using the aforementioned identity, we get
/integraldisplay∞
−∞dzkerfzk√
2
e−1
2(Az2
k+2F zjzk)=eF2z2
j
2A/radicalbigg
2π
Aerf
−Fzj√
2√
A+A2
19Under review as submission to TMLR
And so, using the previous result, all that is left to do is calculate
/integraldisplay∞
−∞dzj(2π)−1
2det(Σ)−1/2erfzj√
2
eF2
2Az2
j−B
2z2
j1√
Aerf
−F√
A+A2zj√
2
.
This expression looks rather intimidating. However, if we can ﬁnd a formula for
I(a) =/integraldisplay∞
−∞dxerf(x) erf( ax)e−b2x2,
then we can proceed. To calculate this we will use the Feynman integral trick, i.e., we will diﬀerentiate under
the integral w.r.t the parameter a. We have that
dI
da=/integraldisplay∞
−∞dxerf(x)2√πxe−a2x2e−b2x2.
This integral is solvable and using the software Mathematica we ﬁnd that
dI
da=1
(a2+b2)3/2/radicalig
1
a2+b2+ 1.
Now, to obtain I(a)we just need to integrate both sides of the above from a′= 0toa′=a. Noting that
I(0) = 0 , we obtain
I(a) =−2/radicalig
a2+b2
a2+b2+1/radicalig
1
a2+b2+ 1 tan−1/parenleftig
−a√
a2+b2+1
b+a2
b+b
√πb+2/radicalig
1
b2+ 1/radicalig
b2
b2+1tan−1(b)
√πb
In our case, if we make the substitution x=zj√
2, we can rewrite the integral as
/integraldisplay∞
−∞dx√
2(2π)−1
2det(Σ)−1/2erf (x)e−AB −F2
Ax21√
Aerf
−F√
A+A2x
.
Using the previous derivation with a=−F√
A+A2andb2=1
Adet(Σ)(which is always positive), we ﬁnally
obtain, after some simpliﬁcation steps
Ez[g(zk)g(zj)] =2
πarctan/parenleftigg
F/radicalbig
(AB−F2) (AB+A+B−F2+ 1)/parenrightigg
which, when written in terms of the components of the covariance matrix Σis
Ez[g(zk)g(zj)] =2
πarctan
/summationtextn
i=1WjiWki/radicalig
1 +/summationtextn
i=1W2
ki+/summationtextn
i=1W2
ji+/parenleftbig/summationtextn
i=1W2
ki/parenrightbig/parenleftbig/summationtextn
i=1W2
ji/parenrightbig
−/parenleftbig/summationtextn
i=1WjiWki/parenrightbig2

The Upper Right block Here, we need to calculate
∂2L
∂W 2k∂W 1mn=W2mEx[g(zk)g′(zm)xn] =W2m/radicalbigg
2
π1√
det Σ−1E˜x[g(zk)xn]
As we saw previously, g′(zm)will modify the underlying multivariate normal distribution. Similarly, to
the previous calculation, the inverse of the covariance matrix will become Σ−1=Am+I, where Am
kl=
W1mkW1mlandIis the identity matrix.
Thus, to perform the calculation we need to be able to calculate E˜x[g(zk)xn], where ˜xis the random variable
that follows the modiﬁed normal distribution. To do the above, we can try to ﬁnd the distribution of
20Under review as submission to TMLR
Y= (zk, xn). Asxis normally distributed then y=Mxis also normally distributed as z∼N(Mµ, MσMT).
In this case µ= 0, σ= (I+Am)−1andM=
Wk
δn
. Thus, (zk, xn)follows a normal distribution N(0, M(I+
Am)−1MT). Using the same techniques as before we ﬁnd that
Σ=
WkWT
kWkn
Wkn 1
−1
1 +WmWTm(WmWT
k)2WkWT
mWmn
WmWT
kWmn W2
mn
Calculating the integral Assuming that we were able to show that (zk, xn)∼N(0,Σ)with
Σ=σzzσzx
σzxσxx
,
we now want to perform the following integral
/integraldisplay∞
−∞/integraldisplay∞
−∞dxndzk1
2π√
det Σexp
−1
2yTΣ−1y
xnerfzk√
2
.
With the help of some tabulated integrals of error functions (check Ng & Geller (1969) and Korotkov &
Korotkov (2020)) it can be shown that the above yields the following result
E[xnerf(zk/√
2)] =/radicalbigg
2
πσzx√1 +σzz.
Using the previous result we ﬁnally have that
∂2L
∂W 2k∂W 1mn=2
π/radicalbig
1 +W1mWT
1mW2mW1kn−W1kWT
1mW1mn
1+W1mWT
1m/radicalbigg
1 +W1kWT
1k−(W1mWT
1k)2
1+W1mWT
1m
21