Published in Transactions on Machine Learning Research (09/2022)
Sparse MoEs meet Eﬃcient Ensembles
James Urquhart Allingham1,∗jua23@cam.ac.uk
Florian Wenzel2,†ﬂn.wenzel@gmail.com
Zelda E Mariet, Basil Mustafa {zmariet,basilm}@google.com
Joan Puigcerver, Neil Houlsby {jpuigcerver,neilhoulsby}@google.com
Ghassen Jerfel3,†ghassen@google.com
Vincent Fortuin1,4,∗vbf21@cam.ac.uk
Balaji Lakshminarayanan, Jasper Snoek {balajiln,jsnoek}@google.com
Dustin Tran, Carlos Riquelme, Rodolphe Jenatton {trandustin,rikel,rjenatton}@google.com
Google Research, Brain Team;1University of Cambridge;2no aﬃliation;3Waymo;4ETH Zürich
Reviewed on OpenReview: https: // openreview. net/ forum? id= i0ZM36d2qU
Abstract
Machine learning models based on the aggregated outputs of submodels, either at the acti-
vation or prediction levels, often exhibit strong performance compared to individual models.
We study the interplay of two popular classes of such models: ensembles of neural networks
and sparse mixture of experts (sparse MoEs). First, we show that the two approaches have
complementary features whose combination is beneﬁcial. This includes a comprehensive
evaluation of sparse MoEs in uncertainty related benchmarks. Then, we present eﬃcient
ensemble of experts (e3), a scalable and simple ensemble of sparse MoEs that takes the best
of both classes of models, while using up to 45% fewer FLOPs than a deep ensemble. Ex-
tensive experiments demonstrate the accuracy, log-likelihood, few-shot learning, robustness,
and uncertainty improvements of e3over several challenging vision Transformer-based base-
lines. e3not only preserves its eﬃciency while scaling to models with up to 2.7B parameters,
but also provides better predictive performance and uncertainty estimates for larger models.
1 Introduction
Neural networks (NNs) typically use all their parameters to process an input. Sustaining the growth of such
models—reaching today up to 100B+ parameters (Brown et al., 2020)—is challenging, e.g., due to their high
computational and environmental costs (Strubell et al., 2019; Patterson et al., 2021). In this context, sparse
mixtures of experts (sparse MoEs) employ conditional computation (Bengio et al., 2013) to combine multiple
submodels and route examples to speciﬁc “expert” submodels (Shazeer et al., 2017; Lepikhin et al., 2021;
Fedus et al., 2021; Riquelme et al., 2021). Conditional computation can decouple the growth of the number
of parameters from the training and inference costs, by only activating a subset of the overall model in an
input-dependent fashion.
Paralleling this trend, the deployment of ML systems in safety-critical ﬁelds, e.g., medical diagnosis (Dusen-
berry et al., 2020b) and self-driving cars (Levinson et al., 2011), has motivated the development of reliable
∗Work done as a Google Research intern.
†Work done while at Google Research.
1Published in Transactions on Machine Learning Research (09/2022)
h2h1
h3h3,1h2,1h1,1
h1,2h1,2
h2,2h2,2
h3,2h3,2TilegateK(W1hi,1)
gateK(W2hi,2)MLP2(hi,1)MLP1(hi,1)
MLP3(hi,1)
MLP5(hi,2)MLP4(hi,2)
MLP6(hi,2)Dispatch
(K=2)ˆh2,1ˆh1,1
ˆh3,1
ˆh2,2ˆh2,2ˆh1,2ˆh1,2
ˆh3,2ˆh3,2Combine¯h1,1
¯h1,2¯h1,2
¯h2,1
¯h2,2¯h2,2
¯h3,1
¯h3,2¯h3,2Softmax
Ensemble
(Mean)Transformer
Block(n−3)×
Norm MSA + Norm p-MoE +p-MoE Block
Transformer
Blockp-MoE
BlockClassifier
1st p-MoE
block only
Figure1: End-to-endoverviewof e3withE=6experts, partitionedinto M=2groups, withsparsityof K=2,
and a “last-2” conﬁguration. Top:e3contains a sequence of transformer blocks, followed by alternating
transformer and p(artitioned)-MoE blocks. As in ViT, images are split into patches whose embeddings are
processed by each block. Here, we show 1 embedding for each of three images ( ,,).Bottom left :
In a p-MoE block, we replace the transformer block’s MLP with parallel partitioned expert MLPs, see (2).
The eﬀect of the routing weights is not depicted. Embeddings are tiled ( ) in the ﬁrst p-MoE block only.
Bottom right : The classiﬁer averages predictions from the ﬁnal tiled representations ( ).
deeplearning, e.g., for calibrated and robust predictions (Ovadiaetal.,2019). Amongtheexistingapproaches,
ensembles of NNs have remarkable performance for calibration and accuracy under dataset shifts (Ovadia
et al., 2019). These methods improve reliability by aggregating the predictions of individual submodels,
referred to as ensemble members. However, this improvement comes at a signiﬁcant computational cost.
Hence, naively ensembling NNs that continue to grow in size becomes less and less feasible. In this work,
we try to overcome this limitation. Our core motivation is to improve the robustness and uncertainty esti-
mates of large-scale ﬁne-tuned models through ensembling, but to do so in a tractable—and thus practically
useful—manner, by carefully developing a hybrid approach using advances in sparse MoEs.
While sharing conceptual similarities, these two classes of models—MoEs and ensembles—have diﬀerent
properties. Sparse MoEs adaptively combine their experts depending on the inputs, and the combination
generally happens at internal activation levels. Ensembles typically combine several models in a static way
and at the prediction level. Moreover, these two classes of models tend to be benchmarked on diﬀerent
tasks: few-shot classiﬁcation for MoEs (Riquelme et al., 2021) and uncertainty-related evaluation for ensem-
bles (Ovadia et al., 2019; Gustafsson et al., 2020). For example, sparse MoEs are seldom, if ever, applied to
the problems of calibration.
Here, we study the interplay between sparse MoEs and ensembles. This results in two sets of contributions:
Contribution 1: Complementarity of sparse MoEs and ensembles. We show that sparse MoEs and
ensembles have complementary features and beneﬁt from each other. Speciﬁcally:
•The adaptive computation in sparse MoEs and the static combination in ensembles are orthogonal,
with additive beneﬁts when associated together. At the intersection of these two model families
is an exciting trade-oﬀ between performance and compute (FLOPs). That is, the frontier can be
mapped out by varying the ensemble size and the sparsity of MoEs.
•Over tasks where either sparse MoEs or ensembles are known to perform well, naive—and computa-
tionally expensive—ensembles of MoEs provide the best predictive performance. Our benchmarking
eﬀort includes the ﬁrst evaluation of sparse MoEs on uncertainty-related vision tasks, which builds
upon the work of Riquelme et al. (2021).
Contribution 2: Eﬃcient ensemble of experts. We propose Eﬃcient Ensemble of Experts ( e3), see
Figure 1, an eﬃcient ensemble approach tailored to sparse MoEs:
2Published in Transactions on Machine Learning Research (09/2022)
•e3improves over sparse MoEs across few-shot error, likelihood and calibration error. e3matches
the performance of deep ensembles while using from 30% to 45% fewer FLOPs.
•e3gracefully scales up to 2.7B parameter models.
•e3is both simple—requiring only minor implementation changes—and convenient— e3models can
be ﬁne-tuned directly from standard sparse-MoE checkpoints. Code can be found at https://
github.com/google-research/vmoe .
2 Preliminaries
We focus on classiﬁcation tasks where we learn classiﬁers of the form f(x;θ)based on some training data
D={(xn,yn)}N
n=1. A pair (xn,yn)corresponds to an input xn∈RPtogether with its label yn∈{1,...,C}
belonging to one of the Cclasses. The model f(·;θ)is parametrized by θand outputs a C-dimensional
probability vector. We use ◦to refer to the matrix element-wise product.
2.1 Vision Transformers and Sparse MoEs
Vision Transformers. Throughout the paper, we choose the model fto be a vision Transformer
(ViT) (Dosovitskiy et al., 2021). ViT is growing in popularity for vision, especially in transfer-learning
settings where it was shown to outperform convolutional networks while requiring fewer pre-training re-
sources. ViT operates at the level of patches. An input image is split into equal-sized patches (e.g., 32×32,
16×16, or 14×14pixels) whose resulting sequence is (linearly) embedded and processed by a Trans-
former (Vaswani et al., 2017). The operations in the Transformer then mostly consist of a succession of
multiheaded self-attention (MSA) and MLP layers. ViT is deﬁned at diﬀerent scales (Dosovitskiy et al.,
2021): S(mall), B(ase), L(arge) and H(uge); see speciﬁcations in Appendix A.1. For example, ViT-L/16
stands for a large ViT with patch size 16×16.
Sparse MoEs and V-MoEs. The main feature of sparsely-gated mixture-of-experts models (sparse
MoEs) lies in the joint use of sparsity and conditional computation (Bengio et al., 2013). In those mod-
els, we only activate a small subset of the network parameters for a given input , which allows the total
number of parameters θto grow while keeping the overall computational cost constant. The expertsare the
subparts of the network activated on a per-input fashion.
Central to our study, Riquelme et al. (2021) recently extended ViT to sparse MoEs. Their extension, referred
to as V-MoE, follows the successful applications of sparse models in NLP (Shazeer et al., 2017). Riquelme
et al. (2021) show that V-MoEs dominate their “dense” ViT counterparts on a variety of tasks for the
same computational cost. In the speciﬁc case of V-MoEs, the experts are placed in the MLP layers of the
Transformer, a design choice reminiscent of Lepikhin et al. (2021) in NLP. Given the input h∈RDof such
a layer, the output of a single MLP(h)is replaced by
MoE(h) =E/summationdisplay
e=1ge(h)·MLPe(h)with{ge(h)}E
e=1=topK(softmax (Wh)), (1)
where the routingweights{ge(h)}E
e=1combine the outputs of the Ediﬀerent experts{MLPe}E
e=1. To sparsely
select the experts, topKsets all but the Klargest weights to zero. The router parameters W∈RE×Dare
trained together with the rest of the network parameters. We call the layer deﬁned by (1) an MoE layer. In
practice, theweights {ge(h)}E
e=1areobtainedbyanoisyversionoftheroutingfunction topK(softmax (Wh+
σε))withε∼N (0,I), which mitigates the non-diﬀerentiability of topKwhen combined with auxiliary
losses (see Appendix A in Shazeer et al. (2017)). Making non-diﬀerentiable operators smooth with some
noise injection is an active area of research (Berthet et al., 2020; Abernethy, 2016; Duchi et al., 2012). We
use the shorthand gateK(z) =topK(softmax (z+σε))and takeσ= 1/Eas in Riquelme et al. (2021).
In this paper, we consider the “last- n” setting of Riquelme et al. (2021) wherein only a few MoE layers are
placed at the end of the Transformer ( n= 2for the {S, B, L} scale and n= 5for H). This setting retains
most of the performance gains of V-MoEs while greatly reducing the training cost.
3Published in Transactions on Machine Learning Research (09/2022)
Table 1: Overview of key properties of sparse MoEs, ensembles, and e3.e3acheives the best of both worlds.
denseis a base model upon which we add the sparse MoE or ensemble logic, e.g., a ViT model in this paper.
Predictions Combinations Conditional Computation Cost
Sparse MoEs Single Activation level Yes, adaptively per-input ≈dense
Ensembles Multiple Prediction level No, static >dense
e3Multiple Activation & prediction level Yes, adaptively per-input ≈dense
2.2 Ensembles of Neural Networks
Ensembles. We build on the idea of ensembles, which is a known scheme to improve the performance of
individual models (Hansen & Salamon, 1990; Geman et al., 1992; Krogh & Vedelsby, 1995; Opitz & Maclin,
1999; Dietterich, 2000; Lakshminarayanan et al., 2017). Formally, we assume a set of Mmodel parameters
Θ ={θm}M
m=1. We refer to Mas theensemble size . Prediction proceeds by computing1
M/summationtext
θ∈Θf(x;θ), i.e.,
the average probability vector over the Mmodels. To assess the diversity of the predictions in the ensemble,
we will use the KL divergence DKL(f(xt;θm)/bardblf(xt;θm/prime))between the predictive distributions f(xt;θm)and
f(xt;θm/prime), averaged over the test input xtand all pairs (m,m/prime)of ensemble members.
Batchensembles. Wenetal.(2019)constructa batch ensemble (BE)asacollectionofsubmodels, withthe
parametersθm∈Θsharing components. This mitigates the computational and memory cost of ensembling,
while still improving performance. We focus on the example of a single dense layer in fwith parameters
U∈RD×L, assuming no bias. BE deﬁnes Mcopies of parameters {Um}M
m=1so thatUm=U◦(rms/latticetop
m),
whereUare parametersshared across ensemble members, and rmandsmare separate D- andL-dimensional
vectors for ensemble member m. Given an input, BE produces Moutputs, which are averaged after applying
all layers. Despite the simple rank-1 parametrization, BE leads to remarkable predictive performance and
robustness (Wen et al., 2019). Notably, the eﬃciency of BE relies on both the parameter sharing and the
tiling of the inputs to predict with the Mensemble members, two insights that we exploit in our paper.
2.3 Pre-training and Fine-tuning
Large-scale Transformers pre-trained on upstream tasks were shown to have strong performance when ﬁne-
tuned on smaller downstream tasks, across a variety of domains (Devlin et al., 2018; Dosovitskiy et al., 2021;
Radford et al., 2021). We follow this paradigm and focus on the ﬁne-tuning of models pre-trained on JFT-
300M (Sun et al., 2017), similar to Riquelme et al. (2021). We will thus assume the availability of already
pre-trained ViT and V-MoE model checkpoints. Our assumption relies on the growing popularity of transfer
learning, e.g. Kolesnikov et al. (2020), and the increasing accessibility of pre-trained models in repositories
such as www.tensorflow.org/hub orwww.pytorch.org/hub . The ﬁne-tuning of all the approaches we study
here, including extensions of ViT and V-MoE, will be either directly compatible with those checkpoints
or require only mild adjustments, e.g., reshaping or introducing new downstream-speciﬁc parameters (see
Appendix B). Also, unless otherwise mentioned, the performance we report will always be downstream, e.g.,
for ImageNet (Deng et al., 2009) or CIFAR10/100 (Krizhevsky, 2009). In all our comparisons, we will use
the downstream training ﬂoating point operations per second (FLOPs), or GFLOPs (i.e., 109×FLOPs), to
quantify the computational cost of the diﬀerent methods.
3 Sparse MoEs meet Ensembles
As illustrated in Table 1, sparse MoEs and ensembles have diﬀerent properties. For instance, ensembles
typically do not use conditional computation and just statically combine members at the prediction level.
This contrasts with sparse MoEs where the diﬀerent experts are combined at internal activation levels while
enjoying per-input adaptivity through the routing logic, see (1). In terms of cost, sparse MoEs are usually
designed to match the inference time of their dense counterparts whereas ensembles, in their simplest forms,
will typically lead to a substantial overhead. In this section, we study the extent to which these properties are
complementary and may beneﬁt from each other. In Section 5, we further evaluate this complementarity on
tasks where either sparse MoEs orensembles areknown to perform well, e.g., few-shotand out-of-distribution
(OOD) evaluations, respectively.
4Published in Transactions on Machine Learning Research (09/2022)
1 2 3 4 5 6 7 8
K1 2 3 4 5 6 7 8M
0.84
0.82
0.80
0.78
0.76
(a) Log-Likelihood (LL)/bracketleftBig
I(
I(/bracketrightBig
1 2 3 4 5 6 7 8
K1 2 3 4 5 6 7 8M
50150250350450
(b) (downstream) GFLOPs/bracketleftBig
I(
I(/bracketrightBig
1 2 3 4 5 6 7 8
K1 2 3 4 5 6 7 8M
8.0
7.5
7.0
6.5
6.0
(c) log/bracketleftBig
LL(K,M)−LL(1,1)
GFLOPs(K,M)−GFLOPs(1,1)/bracketrightBig
Figure 2: Increasing static ( M) and adaptive ( K) ensembling on ImageNet, for V-MoE-S/32. Yel-
low/purpleindicates better/worse performance. Increasing bothstatic and adaptive ensembling is ben-
eﬁcial, the latter being more eﬃcient.
To investigate the interactions of the properties in Table 1, we study the performance of downstream deep
ensembles (i.e., with all ensemble members having the same upstream checkpoint) formed by Mindependent
V-MoEs with Eexperts per MoE layer and a sparsity level K(the larger K, the more selected experts).
Mcontrols the static combination, while KandEimpact the adaptive combination of experts in each
sparse MoE model. We report in Figure 2 the ImageNet performance and compute cost for ensembles with
varying choices of KandM, while keeping E= 32ﬁxed. We focus on Krather than Eto explore adaptive
computation, as we found the performance quickly plateaus with E(see Figure 12 in the Appendix). Also,
by ﬁxingE= 32, we match more closely the setup of Riquelme et al. (2021). The architecture of the V-MoE
is ViT-S/32, see details in Appendix A.7. We make the following observations:
Investigating the cumulative eﬀects of adaptive and static ensembling. In the absence of
ensembles (i.e., when we consider M= 1), and given a ﬁxed number of experts, the authors
of Riquelme et al. (2021) already reported an increase in performance as Kgets larger. Interest-
ingly, we observe that for each value of K, it is also beneﬁcial to increase the ensemble size M.
2 3 40.40.50.60.70.80.9NLL (lower is better)
downstream log(GFLOPs) (lower is better)
Figure 3: ImageNet evalu-
ation for ViT ( ) and V-
MoE (K= 1) () ensembles of
size{1,2,4}(,,). Model
sizes: S/32 ( ), B/32 ( ), L/32 ( ),
L/16 (), and H/14 ( ). ViT and
V-MoE beneﬁt from ensembling
equally, at all scales.In other words, the static combination of ensembles is beneﬁcial when
applied to sparse MoEs. This observation is perhaps surprising since
adaptivecombinationmayalreadyencapsulatetheeﬀectofstaticcom-
bination. Figure 3, and Appendix L.1, show that the combination of
static ensembling and adaptivity is beneﬁcial to NLL for a range of
ViT families. We also see that the beneﬁts of static ensembling are
similar for V-MoE and ViT (which does not have any adaptivity).
Investigating ensembles of sparse MoEs with fewer experts.
In Appendix L.2, we compare the performance of a V-MoE with E=
32expertsandensemblesofV-MoEswith fewerexperts, namely ( M=
2,E= 16) and (M= 4,E= 8). We see that the performance—e.g.,
as measured by NLL—is better for ( M= 4,E= 8) than (M= 2,
E= 16) which is in turn better than ( M= 1,E= 32). Thus, we
conclude that reducing the number of experts only mildly aﬀects the
combination of adaptive and static ensembling.
Investigating the trade-oﬀ between FLOPs and performance.
Without any computational constraints, the previous observation
would favor approaches with the largest values of KandM. How-
ever, diﬀerent values of (K,M )lead to diﬀerent computational costs,
as measured here by FLOPs, with (K,M ) = (1,1)being the cheapest.
Figure 2b shows, as expected, that the number of FLOPs grows more
quickly along the Maxis than along the Kaxis. To capture the various trade-oﬀs at play, in Figure 2c
we report the logarithm of the normalized gains in log likelihoodLL(K,M)−LL(1,1)
GFLOPs (K,M)−GFLOPs (1,1)when going from
(K,M ) = (1,1)to other choices of (K,M ). Interestingly, it appears more advantageous to ﬁrst grow K, i.e.,
the adaptive combination, before growing M.
5Published in Transactions on Machine Learning Research (09/2022)
Summary. While simple ensembling of sparse MoEs results in strong predictive performance, we lose their
computational eﬃciency. We next show how to eﬃciently combine ensembling and sparse MoEs, exploiting
the fact that statically combining sparse MoEs with fewer experts remains eﬀective.
4 Eﬃcient Ensemble of Experts
Equipped with the insights from Section 3, we describe eﬃcient ensemble of experts ( e3), with the goal of
keeping the strengths of both sparse MoEs and ensembles. Conceptually, e3jointly learns an ensemble of
smaller sparse MoEs, where all layers without experts (e.g., attention layers) are shared across the members.
4.1 The Architecture
There are two main components in e3:
Disjoint subsets of experts. We change the structure of (1) by partitioning the set ofEexperts into
Msubsets ofE/Mexperts (assuming that Eis a multiple of M). We denote the subsets by Em. For
example,E1={1,2,3}andE2={4,5,6}forE= 6andM= 2. Intuitively, the ensemble members have
separate parameters for independent predictions, while eﬃciently sharing parameters among all non-expert
layers. Instead of having a single routing function gateK(W·)as in (1), we apply separate routing functions
gateK(Wm·)to each subsetEm. Note that this does not aﬀect the total number of parameters since W
hasErows while each WmhasE/Mrows. A similar partitioning of the experts was done in Yang et al.
(2021) but not exploited to create diﬀerent ensemble members, in particular not in conjunction with tiled
representations, which we show to be required to get performance gains (see Section 4.2.1).
Tiled representation. To jointly handle the predictions of the Mensemble members, we tile the inputs
by a factor M, inspired by Wen et al. (2019). This enables a simple implementation of e3on top of an
existing MoE. In Appendix E, we connect sparse MoEs and BE, illustrating that tiling naturally ﬁts into the
formalism of sparse MoEs. Because of the tiling, a given image patch has Mdiﬀerent representations that,
when entering an MoE layer, are each routed to their respective expert subsets Em. Formally, consider some
tiled inputsH∈RB×M×DwhereBrefers to the batch size (a batch contains image patches) and hi,m∈RD
is the representation of the i-th input for the m-th member. The routing logic in e3can be written as
p-MoE (hi,m) =/summationdisplay
e∈Emge(hi,m)·MLPe(hi,m)with{ge(hi,m)}e∈Em=gateK(Wmhi,m), (2)
where the routing weights are now Wm∈R(E/M )×D; see Figure 1.
To echo the observations from Section 3, we can ﬁrst see that e3brings together the static and adaptive
combination of ensembles and sparse MoEs, which we found to be complementary. However, we have seen
that static ensembling comes at the cost of a large increase in FLOPs, thus we opt for an eﬃcient ensembling
approach. Second, we “split” the MoE layers along the axis of the experts, i.e., from Eexperts toMtimes
E/Mexperts. We do so since we observed that the performance of sparse MoEs tends to plateau quickly for
more experts. We note that e3retains the property of ensembles to output multiple predictions per input.
In a generic implementation, we tile a batch of BinputsX∈RB×Pby a factor Mto obtain the tiled
inputsXtiled= [X;...;X]∈R(M·B)×Pand the model processes f(Xtiled;θ). Since tiling in e3has an eﬀect
only from the ﬁrst MoE layer onwards, we postpone the tiling operation to that stage, thus saving otherwise
redundant prior computations in non-MoE-layers. For example, for L/16 and K=M= 2, we can save about
47%oftheFLOPs. Furtherimplementationdetailsof e3, andadiscussionoftheincreasedmemoryconsump-
tion due to tiling, are in Appendix C. Code can be found at https://github.com/google-research/vmoe .
Finally, we note that although e3and BE share conceptual design similarities—tiled representation and
sharing of parameters—they diﬀer in fundamental structural ways, see Appendix F.
4.2 Ablation Studies: Partitioning and Tiling
Our method introduces two changes to V-MoEs: (a) the partitioning of the experts and (b) the tiling of
the representations. In this section, we assess the separate impact of each of those changes and show that
6Published in Transactions on Machine Learning Research (09/2022)
Table 2: ImageNet performance (means ±standard errors over 8 seeds) of e3-B/32 (K=M= 2), V-MoE
(K= 4), and two ablations: onlytiling and onlypartitioning. The noise in gateKis denoted by σ.
NLL↓ Error↓ ECE↓ KL↑
V-MoE 0.636 ±0.00116.70±0.040.034±0.001 —
e30.612±0.00116.49±0.020.013±0.0000.198±0.003
Tiling 0.637 ±0.00216.74±0.060.028±0.0010.000±0.000
Tiling (σ×2) 0.638±0.00116.72±0.030.033±0.0010.001±0.000
Tiling (σ×4) 0.638±0.00116.74±0.030.033±0.0010.002±0.000
Partitioning 0.640 ±0.00116.72±0.050.034±0.001 —
it is indeed their combination that explains the performance gains. We summarize the results of this study
in Table 2 which shows ImageNet performance—negative log likelihood (NLL), classiﬁcation error, expected
calibration error (ECE) (Guo et al., 2017), and Kullback–Leibler divergence—for diﬀerent ablations of e3.
See Appendix N, for end-to-end overview diagrams in the style of Figure 1, for the these ablations. We use
E= 32experts. We provide FLOP measurements for these ablations in Appendix M.1.
4.2.1 Partitioning without Tiling
We ﬁrst compare e3with a variant of V-MoE where we only partition the set of experts ( Partitioning ). In
this variant, each input hi∈RD(note the dropping of the index mdue to the absence of tiling) can select
Kexperts in subset Em, resulting in a total of K×Mselected experts per input. Formally, (2) becomes
partitioning-only-MoE (hi) =M/summationdisplay
m=1/summationdisplay
e∈Emge(hi)·MLPe(hi)with{ge(hi)}e∈Em=gateK(Wmhi).
Theexpert prototyping of Yang et al. (2021) leads to a similar formulation. As shown in Table 2, across all
metrics, Partitioning is not competitive with e3. We do not report KL since without tiling, Partitioning
does not output multiple predictions per input.
4.2.2 Tiling without Partitioning
We now compare e3with the variant where only the tiling is enabled ( Tiling). In this case, we have tiled
inputsH∈RB×M×Dapplied to the standard formulation of (1). Compared with (2), there is no mechanism
to enforce the Mrepresentations of the i-th input across the ensemble members, i.e., {MoE(hi,m)}M
m=1, to be
diﬀerent. Indeed, without partitioning, each hi,mcould select Kidentical experts. As a result, we expect
Tilingto outputMsimilar predictions across ensemble members. This is conﬁrmed in Table 2 where we
observe that the KL for Tilingis orders of magnitude smaller than for e3. To mitigate this eﬀect, we also
tried to increase the level of noise σingateK(by a factor{2,4}), to cause the expert assignments to diﬀer
across{hi,m}M
m=1. While we do see an increase in KL, Tilingstill performs worse than e3across all metrics.
Interestingly, we can interpret Tilingas an approximation, via Msamples, of the marginalization
Eε1,...,ε/lscript[f(x;θ)]with respect to the noise {εl}/lscript
l=1in the/lscriptMoE layers of f(·;θ)(further assuming the
capacity constraints of the experts, as described in Riquelme et al. (2021), do not bias the Msamples).
4.2.3 Tiling with Increasing Parameter Sharing
The results in Table 2 (as well as Appendix I) suggest that the strong performance of e3is related to its
high-diversity predictions. We hypothesise that this diversity is a result of the large number of non-shared
parameters in each ensemble member, i.e., the partitioning of the experts. To test this hypothesis, we allow
the subsetsEmine3to havesome degree of overlap (i.e., ensemble members share some experts), thus
interpolating between e3andTiling. For example, with total experts E= 32and an ensemble size M= 2,
an overlap of 8 shared experts means that each ensemble member has (32−8)/2 = 12unique experts, and
12 + 8 = 20 in total. Table 3 shows that increasing the number of shared experts directly decreases diversity
and thus NLL, Error, and ECE. We see the same trends for K= 1(rather than K= 2; see Table 18).
7Published in Transactions on Machine Learning Research (09/2022)
Table 3: ImageNet performance (means ±standard errors over 8 seeds) of e3-B/32 (K=M= 2),E= 32
total experts, and varying expert overlap between subsets Em.
Overlap NLL ↓ Error↓ ECE↓ KL↑
0 (=e3)0.612±0.00116.49±0.020.013±0.0000.198±0.003
2 0.617 ±0.00316.55±0.090.016±0.0010.167±0.005
4 0.622 ±0.00116.62±0.020.017±0.0010.148±0.003
8 0.627 ±0.00216.67±0.070.021±0.0010.122±0.010
16 0.639 ±0.00416.82±0.070.030±0.0030.077±0.011
32 (=Tiling) 0.637 ±0.00216.74±0.060.028±0.0010.000±0.000
Table 4: ImageNet performance (means ±standard errors over 8 seeds) of V-MoE-B/32 and a simple
multi-prediction variant ( Multi-pred ) whose last MoE layer is changed as in (3).
K NLL↓ Error↓ ECE↓ KL↑
e3(M= 2) 10.622±0.00116.70±0.030.018±0.0000.217±0.003
Multi-pred 2 0.636 ±0.00117.16±0.020.024±0.0000.032±0.001
V-MoE 2 0.638 ±0.00116.76±0.050.033±0.001 —
e3(M= 2) 20.612±0.00116.49±0.020.013±0.0000.198±0.003
Multi-pred 4 0.645 ±0.00117.39±0.040.021±0.0000.011±0.001
V-MoE 4 0.636 ±0.00116.70±0.040.034±0.001 —
e3(M= 2) 40.611±0.00116.45±0.030.014±0.0000.193±0.003
Multi-pred 8 0.650 ±0.00117.50±0.030.021±0.0000.005±0.000
V-MoE 8 0.635 ±0.00216.72±0.060.028±0.001 —
4.2.4 Multiple Predictions without Tiling or Partitioning
As highlighted in Table 1, an ensemble of size MoutputsMpredictions for a given input (thereafter,
averaged) while sparse MoEs only produce a single prediction. Thus, a natural question is how much the
gains of e3are simply due to its ability to produce multiple predictions, rather than its speciﬁc tiling and
partitioning mechanisms? To answer this, we investigate a simple multi-prediction variant of sparse MoEs
(Multi-pred ) wherein the last MoE layer of the form (1) is replaced by
Multi-pred-MoE (h) ={ge(h)·MLPe(h)}ge(h)>0∈RK×Qwith{ge(h)}E
e=1=gateK(Wh),(3)
where we have assumed MLPe(h)∈RQ. Instead of summing the expert outputs like in (1), we stacktheK
selected expert contributions (as a reminder, gateKzeroes out the E−Ksmallest weights). Keeping track
of thoseKcontributions makes it possible to generate Kdiﬀerent predictions per input as in the classiﬁer
of Figure 1, thus aiming at capturing model uncertainty around the true prediction.
Table 4 compares the ImageNet performance of this simple multiple-prediction method with the standard
V-MoE and e3. In all cases, Multi-pred under performs relative to e3. Indeed, despite improvements in ECE,
it is only for K= 2, thatMulti-pred provides small gains in NLL relative to V-MoE, while its classiﬁcation
error is always worse. In fact, Multi-pred forK= 4performs worsein terms of NLL, classiﬁcation error, and
diversity than for K= 2. The KL diversity metric indicates that the Multi-pred is unable to provide diverse
predictions. This indicates that it is speciﬁcally tiling and partitioning in e3that provide good performance.
4.3 Comparison with Alternative Eﬃcient Ensembling Strategies
In the previous subsection we saw that a simple approach to multiple predictions in a V-MoE model is unable
to achieve good diversity in predictions and thus strong predictive performance. Following Havasi et al.
(2020); Soﬂaei et al. (2020), a possible ﬁx to this problem would be to have a multi-prediction and multi-
inputapproach. Furthermore, other eﬃcient ensembling strategies could provide alternative solutions to this
problem. Unfortunately, as we show in Table 5, while common eﬃcient ensembling strategies such as BE
(Wen et al., 2019), MC Dropout (Gal & Ghahramani, 2016), and MIMO (Havasi et al., 2020), do improve
slightly on ViT/V-MoE, they are unable to match the performance of e3. In Appendix G, we provide
8Published in Transactions on Machine Learning Research (09/2022)
Table 5: ImageNet performance (means ±standard errors over 8 seeds) of diﬀerent eﬃcient ensemble
approaches based on a ViT-B/32 architecture.
K M NLL ↓ Error↓ ECE↓ KL↑ GFLOPs↓
ViT – – 0.688 ±0.00318.65±0.080.022±0.000 – 78.0
BE ViT – 2 0.682 ±0.00318.47±0.050.021±0.0000.040±0.001 97.1
V-MoE 2 – 0.638 ±0.00116.76±0.050.033±0.001 – 94.9
MC Dropout V-MoE 1 2 0.648 ±0.00217.10±0.050.019±0.0010.046±0.000 97.2
MIMO V-MoE2 2 0.636 ±0.00216.97±0.040.028±0.0010.000±0.000 96.3
2 4 0.672 ±0.00117.72±0.040.037±0.0000.001±0.000 99.0
e31 20.622±0.00116.70±0.030.018±0.0000.217±0.003 105.9
2 3 40.40.50.60.70.8NLL (lower is better)ImageNet
3.0 3.2 3.4 3.60.400.420.440.460.48ImageNet
2 3 415202510-Shot Error (lower is better)Mean Across Datasets
3.0 3.2 3.4 3.614151617Mean Across Datasets
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 L/16 H/14 1 Member 2 Members 4 Members
Figure 4: ImageNet NLL (left, center left) and mean 10-shot error across datasets (center right, right), with
zoomed-in plots of highlighted areas. Zoomed-in plots include additional ensemble baselines for comparison.
Dashed lines show Pareto frontiers which tend to be deﬁned by e3.
a detailed description of how we carefully apply these methods to ViT/V-MoE to ensure a fair evaluation
(sometimes even designing extensions of these methods). We also give results for additional KandMvalues.
5 Evaluation
We now benchmark e3against V-MoE. As a baseline we also include results for downstream ensembles of
V-MoE and ViT. These ensembles oﬀer a natural baseline against e3as they also use a single upstream check-
point, are easy to implement, and provide consistent improvements upon V-MoE. In Appendix J, we compare
withupstream ensembles that require multiple upstream checkpoints (Mustafa et al., 2020). All results cor-
respond to the average over 8 (for {S, B, L} single models) or 5 (for H single models and all up/downstream
ensembles) replications. In Appendices L and M we provide results for additional datasets and metrics as well
as standard errors. Following Riquelme et al. (2021), we compare the predictive-performance vs. compute
cost trade-oﬀs for each method across a range of ViT families. In the results below, e3uses (K,M ) = (1,2),
single V-MoE models use K= 2, V-MoE ensembles use K= 1, and all use E= 32. Experimental details,
including those for our upstream training, downstream ﬁne-tuning, hyperparameter sweeps and (linear)
few-shot evaluation can be found in Appendix A. Our main ﬁndings are as follows:
5.1 V-MoE vs. ViT
•Ensembles help V-MoE just as much as ViT. Ensembling was expected to beneﬁt ViT. However,
Figures 3 to 8 suggest that ensembling provides similar gains for V-MoE in terms of few-shot performance,
NLL, ECE, OOD detection, and robustness to distribution shift. We believe this has not been observed
before. Moreover, a downstream ensemble with four H/14 V-MoEs leads to a 88.8% accuracy on ImageNet
(even reaching an impressive 89.3% for an upstream ensemble that further beneﬁts from multiple upstream
checkpoints, see Table 15).
•ViT consistently provides better ECE than V-MoE. Surprisingly, despite V-MoE tending to have
9Published in Transactions on Machine Learning Research (09/2022)
2 3 40.010.020.030.04ECE (lower is better)ImageNet
3.0 3.2 3.4 3.60.01250.01500.01750.02000.02250.0250ImageNet
2 3 40.020.030.040.050.060.07ImageNet-C (average)
2 3 40.100.150.200.250.300.350.40ImageNet-A
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 L/16 H/14 1 Member 2 Members 4 Members
Figure 5: ECE for ImageNet (left), with a zoomed-in plot of the highlighted area (center left), and under
distribution shift (right, center right). This is a metric for which ensembles are known to perform well
whereas, to the best of our knowledge, the performance of V-MoE has not been evaluated. The zoomed-in
plot includes additional ensemble baselines for comparison. Dashed lines show Pareto frontiers.
2 30.000.050.100.150.200.25FPR@95 (lower is better)CIFAR10 vs. CIFAR100
2.50 2.75 3.000.0300.0350.0400.0450.0500.0550.060CIFAR10 vs. CIFAR100
2 30.000.010.020.030.04CIFAR10 vs. SVHN
2 30.020.040.060.080.100.120.14CIFAR10 vs. Places365
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 L/16 1 Member 2 Members 4 Members
Figure 6: OOD detection, measured by false positive rate at 95% precision (Fort et al., 2021), for models
ﬁne-tuned on CIFAR10, with a zoomed-in plot of the highlighted area. This is a metric for which ensembles
are known to perform well whereas, to the best of our knowledge, the performance of V-MoE has not been
evaluated. The zoomed-in plot includes additional ensemble baselines for comparison. Dashed lines show
Pareto frontiers.
better NLL than ViT (Figure 3), their ECE is worse (Figure 5).
•ECE is not consistent for diﬀerent ViT/V-MoE families. We see the ECE, unlike other metrics
presented in this work, tends not to provide consistent trends as we increase the ViT family size (Figure 5).
This observation is consistent with Minderer et al. (2021), who noted similar behaviour for a range of models.
They note that post-hoc temperature scaling can improve consistency.
•V-MoE outperforms ViT in OOD detection. With L/32 being the only exception, V-MoE outper-
forms ViT on a range of OOD detection tasks (Figure 6). While this may seem surprising, given the opposite
trend for ECE, it suggests that ViT makes more accurate predictions about the scale of the uncertainty es-
timates while V-MoE makes better predictions about the relative ordering of the uncertainty estimates.
•For smaller ViT families, V-MoE outperforms ViT in the presence of distribution shift. In
contrast to the OOD detection results, Figure 7 shows that for smaller ViT families V-MoE improves on the
performance of ViT. However, as the ViT family becomes larger, this trend is less consistent.
5.2 Eﬃcient Ensemble of Experts
•e3improves classiﬁcation performance. As shown in Figure 4, e3is either on, or very near to, the
Pareto frontiers for NLL and 10-shot classiﬁcation error, despite the fact that these are metrics for which
ensembles and V-MoE, respectively, are known to perform well. Figure 8 shows that similar conclusions hold
for CIFAR10/100 NLL.
•e3performs best at the largest scale. The diﬀerence in predictive performance between e3and V-
10Published in Transactions on Machine Learning Research (09/2022)
2 3 41.01.52.02.5NLL (lower is better)ImageNet-C (average)
3.0 3.2 3.4 3.60.850.900.951.001.051.10ImageNet-C (average)
2 3 4234567ImageNet-A
2 3 40.81.01.21.41.6ImageNet-V2
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 L/16 H/14 1 Member 2 Members 4 Members
Figure 7: NLL under distribution shift for models trained on ImageNet. For ImageNet-C, we provide
a zoomed-in plot of the highlighted area. The zoomed-in plot includes additional ensemble baselines for
comparison. Dashed lines show Pareto frontiers which tend to be deﬁned by e3.
2 30.020.030.040.050.060.07NLL (lower is better)CIFAR10
2.50 2.75 3.000.0250.0300.0350.040CIFAR10
2 30.20.30.40.5CIFAR10-C
2 30.200.250.300.350.400.45CIFAR100
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 L/16 1 Member 2 Members 4 Members
Figure 8: NLL for CIFAR10 (left), with a zoomed-in plot of the highlighted area (center left), CIFAR10-
C (center right), and CIFAR100 (right). The zoomed-in plot includes additional ensemble baselines for
comparison. Dashed lines show Pareto frontiers.
MoE—or ensembles thereof—increases as the ViT family becomes larger (Figures 4 to 8, and Appendix D
where we propose a scheme to normalize performances across scales).
•e3becomes Pareto eﬃcient for larger ViT families in the presence of distribution shift.
Figure 7 shows that e3is more robust to distribution shift for larger ViT families, despite less consistent
V-MoEperformanceatscale. Whenaveragedovertheshifteddatasets(ImageNet-C,ImageNet-A,ImageNet-
V2),e3improves on V-MoE in terms of NLL for all ViT families other than S/32, with improvements up to
8.33% at the largest scale; see Appendix M.3.
•e3improves ECE over ViT and V-MoE. Despite V-MoE providing poor ECE, e3does not suﬀer
from this limitation (Figure 5). For most ViT families, e3also provides better ECE than V-MoE ensembles.
•e3does not provide consistent OOD detection performance. Firstly, Figure 6 shows that for small
ViT families, e3performs worse than V-MoE and (even ViT in some cases). Nevertheless, as above, the
relative performance improves for larger ViT families such that e3becomes Pareto eﬃcient for two dataset
pairs. Secondly, e3seems to perform better on the more diﬃcult near OOD detection task (CIFAR10
vs. CIFAR100). These results, although sometimes subtle, are consistent across OOD detection metrics and
dataset pairs, as shown in Appendix L.
Summary. While no single model performs best in all of our evaluation settings, we do ﬁnd that e3
performs well and is Pareto eﬃcient in mostcases. This is particularly true for the larger ViT families.
One exception was OOD detection, where e3performance was somewhat inconsistent. On the other hand,
while V-MoE clearly outperforms ViT in terms of accuracy, the uncertainty estimates can be better or worse
depending on the downstream application.
11Published in Transactions on Machine Learning Research (09/2022)
6 Related Work
Mixture of Experts. MoEs (Jacobs et al., 1991; Jordan & Jacobs, 1994; Chen et al., 1999; Yuksel et al.,
2012; Eigen et al., 2014) combine the outputs of diﬀerent submodels, or experts, in an input-dependent
way. Sparse MoEs only select a few experts per input, enabling to greatly scale models while keeping the
prediction time constant. Sparse MoEs have been used to build large language models (Shazeer et al., 2017;
Lepikhin et al., 2021; Fedus et al., 2021). Recently, sparse MoEs have been also successfully applied to vision
problems (Riquelme et al., 2021; Yang et al., 2021; Lou et al., 2021; Xue et al., 2021). Our work builds
on the V-MoE architecture proposed by Riquelme et al. (2021), which is based on the vision Transformer
(ViT) (Dosovitskiy et al., 2021) and showed improved performance for the same computational cost as
ViT. We explore the interplay between sparse MoEs and ensembles and show that V-MoEs beneﬁt from
ensembling, by improving their predictive performance and robustness. While previous work studied ViT’s
calibration and robustness (Minderer et al., 2021; Fort et al., 2021; Paul & Chen, 2021; Mao et al., 2021),
we are the ﬁrst to study the robustness of V-MoE models.
Ensembles. Ensemblemethodscombineseveraldiﬀerentmodelstoimprovegeneralizationanduncertainty
estimation. Ensembles achieve the best performance when they are composed of diverse members that make
complementary errors (Hansen & Salamon, 1990; Fort et al., 2019; Wenzel et al., 2020; D’Angelo & Fortuin,
2021; Lopes et al., 2021). However, standard ensembles are ineﬃcient since they consist of multiple models,
each of which can already be expensive. To reduce test time, Xie et al. (2013) and Hinton et al. (2015);
Tran et al. (2020); Nam et al. (2021) use compression and distillation mechanisms, respectively. To reduce
training time, ensembles can be constructed with cyclical learning-rate schedules to snapshot models along
the training trajectory (Huang et al., 2017; Zhang et al., 2019). Our work builds on batch ensemble (Wen
et al., 2019) where a singlemodel encapsulates an ensemble of networks, a strategy also explored by Lee
et al. (2015); Havasi et al. (2020); Antorán et al. (2020); Dusenberry et al. (2020a); Rame et al. (2021).
Wenzel et al. (2020) extended BE to models with diﬀerent hyperparameters.
Bayesian Neural Networks. BNNs are an alternative approach to deep ensembles for uncertainty quan-
tiﬁcation in neural networks. In a BNN the weights are treated as random variables and the uncertainty in
the weights is translated to uncertainty in predictions via marginalisation. However, because the weight pos-
terior is intractable for NNs, approximation is required. Popular approximations include variational inference
(Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015), the Laplace approximation (MacKay, 1992;
Ritter et al., 2018), and MC Dropout (Gal & Ghahramani, 2016). However, many of these approximations
make restrictive mean-ﬁeld assumptions which hurt performance (Foong et al., 2020; Coker et al., 2022; For-
tuin et al., 2021). Unfortunately, modeling full weight correlations for even small ResNets—with relatively
few parameters compared to the ViT models considered here—is intractable (Daxberger et al., 2021).
7 Conclusions and Future Work
OurstudyoftheinterplaybetweensparseMoEsandensembleshasshownthatthesetwoclassesofmodelsare
symbiotic. Eﬃcient ensemble of experts exempliﬁes those mutual beneﬁts—as illustrated by its accuracy, log-
likelihood, few-shot learning, robustness, and uncertainty calibration improvements over several challenging
baselinesinarangeofbenchmarks. Wehavealsoprovidedtheﬁrst,tothebestofourknowledge,investigation
into the robustness and uncertainty calibration properties of V-MoEs—showing that these models are robust
todatasetshiftandareabletodetectOODexamples. Whileourstudyhasfocusedondownstreamﬁne-tuned
models, we believe that an extension to the upstream case and from-scratch training, would also result in a
fruitful investigation. In fact, in Appendix K we provide some promising, but preliminary, results for from-
scratch training. Similarly, although we have focused on computer vision, our approach should be readily
applicable to the modelling of text, where sparse MoEs have been shown to be remarkably eﬀective. With
the growing prevalence of sparse MoEs in NLP (Patterson et al., 2021), the questions of understanding and
improving the robustness and reliability of such models become increasingly important. Furthermore, the
computational scale at which those models operate make those questions challenging to tackle. We believe
that our study, and approaches such as e3, make steps in those directions.
12Published in Transactions on Machine Learning Research (09/2022)
Acknowledgements
We would like to extend our gratitude to Josip Djolonga for having reviewed an earlier version of the
draft and for helping us with robustness_metrics (Djolonga et al., 2020). We would also like to thank
André Susano Pinto and Cedric Renggli for fruitful comments on the project, as well as Jakob Uszkoreit for
insightful discussions. JUA acknowledges funding from the EPSRC, the Michael E. Fisher Studentship in
Machine Learning, and the Qualcomm Innovation Fellowship. VF acknowledges funding from the Swiss Data
Science Center through a PhD Fellowship, as well as from the Swiss National Science Foundation through a
Postdoc.Mobility Fellowship, and from St. John’s College Cambridge through a Junior Research Fellowship.
References
Jacob D. Abernethy. Perturbation techniques in online learning and optimization. 2016.
Javier Antorán, James Urquhart Allingham, and José Miguel Hernández-Lobato. Depth uncertainty in
neural networks. In Advances in Neural Information Processing Systems , 2020.
MonikaBansal, MunishKumar, MonikaSachdeva, andAjayMittal. Transferlearningforimageclassiﬁcation
using vgg19: Caltech-101 image data set. Journal of Ambient Intelligence and Humanized Computing , pp.
1–12, 2021.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arxiv preprint arxiv:1308.3432 , 2013.
Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, and Francis R. Bach.
Learning with diﬀerentiable perturbed optimizers. CoRR, abs/2002.08676, 2020. URL https://arxiv.
org/abs/2002.08676 .
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
network. In International Conference on Machine Learning , pp. 1613–1622, 2015.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: compos-
able transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax .
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arxiv preprint arxiv:2005.14165 , 2020.
Ke Chen, Lei Xu, and Huisheng Chi. Improved learning algorithms for mixture of experts in multiclass
classiﬁcation. Neural Networks , 1999.
M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In
Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.
Beau Coker, Wessel P. Bruinsma, David R. Burt, Weiwei Pan, and Finale Doshi-Velez. Wide mean-ﬁeld
bayesian neural networks ignore the data. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel
Valera (eds.), Proceedings of The 25th International Conference on Artiﬁcial Intelligence and Statistics ,
volume 151 of Proceedings of Machine Learning Research , pp. 5276–5333. PMLR, 28–30 Mar 2022. URL
https://proceedings.mlr.press/v151/coker22a.html .
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data
augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops , pp. 702–703, 2020.
Francesco D’Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. In Marc’Aurelio Ranzato,
Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in
Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems
2021, NeurIPS 2021, December 6-14, 2021, virtual , pp. 3451–3465, 2021. URL https://proceedings.
neurips.cc/paper/2021/hash/1c63926ebcabda26b5cdb31b5cc91efb-Abstract.html .
13Published in Transactions on Machine Learning Research (09/2022)
Erik A. Daxberger, Eric T. Nalisnick, James Urquhart Allingham, Javier Antorán, and José Miguel
Hernández-Lobato. Bayesian deep learning via subnetwork inference. In Marina Meila and Tong Zhang
(eds.),Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pp. 2510–2521. PMLR,
2021. URL http://proceedings.mlr.press/v139/daxberger21a.html .
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical
image database. In CVPR, 2009.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional
transformers for language understanding. arxiv preprint arxiv:1810.04805 , 2018.
Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple
classiﬁer systems , pp. 1–15. Springer, 2000.
Josip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijnders, Dustin
Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/google-research/
robustness_metrics .
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil
Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.
John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic opti-
mization. SIAM J. Optim. , 22(2):674–701, 2012. doi: 10.1137/110831659. URL https://doi.org/10.
1137/110831659 .
Michael W Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-an Ma, Jasper Snoek, Katherine Heller, Balaji
Lakshminarayanan, and Dustin Tran. Eﬃcient and scalable bayesian neural nets with rank-1 factors. In
International conference on machine learning , 2020a.
Michael W Dusenberry, Dustin Tran, Edward Choi, Jonas Kemp, Jeremy Nixon, Ghassen Jerfel, Katherine
Heller, and Andrew M Dai. Analyzing the role of model uncertainty for electronic health records. In
Proceedings of the ACM Conference on Health, Inference, and Learning , pp. 204–213, 2020b.
David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture
of experts. In ICLR (Workshop Poster) , 2014.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and eﬃcient sparsity. arxiv preprint arxiv:2101.03961 , 2021.
Andrew Y. K. Foong, David R. Burt, Yingzhen Li, and Richard E. Turner. On the expressiveness
of approximate inference in bayesian neural networks. In Hugo Larochelle, Marc’Aurelio Ranzato,
Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Pro-
cessing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
b6dfd41875bc090bd31d0b1740eb5b1b-Abstract.html .
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective.
arxiv preprint arxiv:1912.02757 , 2019.
Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection.
arxiv, 2021.
Vincent Fortuin, Adrià Garriga-Alonso, Florian Wenzel, Gunnar Rätsch, Richard E. Turner, Mark van der
Wilk, and Laurence Aitchison. Bayesian neural network priors revisited. CoRR, abs/2102.06571, 2021.
URL https://arxiv.org/abs/2102.06571 .
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In International conference on machine learning , pp. 1050–1059, 2016.
14Published in Transactions on Machine Learning Research (09/2022)
Stuart Geman, Elie Bienenstock, and René Doursat. Neural networks and the bias/variance dilemma. Neural
computation , 4(1):1–58, 1992.
Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R. S. Zemel,
P. L. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), Advances in Neural Information Process-
ing Systems 24 , pp. 2348–2356. Curran Associates, Inc., 2011. URL http://papers.nips.cc/paper/
4329-practical-variational-inference-for-neural-networks.pdf .
Chuan Guo, Geoﬀ Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
International Conference on Machine Learning , pp. 1321–1330. PMLR, 2017.
Fredrik K Gustafsson, Martin Danelljan, and Thomas B Schon. Evaluating scalable bayesian deep learning
methods for robust computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition Workshops , pp. 318–319, 2020.
Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern analysis and
machine intelligence , 12(10):993–1001, 1990.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining,
inference, and prediction . Springer, 2017.
Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshmi-
narayanan, Andrew Mingbo Dai, and Dustin Tran. Training independent subnetworks for robust pre-
diction. In ICLR, 2020.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. arXiv preprint arXiv:1903.12261 , 2019.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial exam-
ples.arXiv preprint arXiv:1907.07174 , 2019.
Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. NIPS Deep
Learning and Representation Learning Workshop , 2015.
Geoﬀrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings of the sixth annual conference on Computational learning theory , pp.
5–13, 1993.
Gao Huang, Yixuan Li, Geoﬀ Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot
ensembles: Train 1, get m for free. arxiv preprint arxiv:1704.00109 , 2017.
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural
Computation , 1991.
Michael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural
Computation , 1994.
Jakob Nikolas Kather, Cleo-Aron Weis, Francesco Bianconi, Susanne M Melchers, Lothar R Schad, Timo
Gaiser, Alexander Marx, and Frank Gerrit Zöllner. Multi-class texture analysis in colorectal cancer his-
tology.Scientiﬁc reports , 6(1):1–11, 2016.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil
Houlsby. Big transfer (bit): General visual representation learning. In ECCV, pp. 491–507. Springer, 2020.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained catego-
rization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) , Sydney,
Australia, 2013.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of
Toronto, 2009.
15Published in Transactions on Machine Learning Research (09/2022)
Anders Krogh and Jesper Vedelsby. Neural network ensembles, cross validation, and active learning. In
Advances in neural information processing systems , pp. 231–238, 1995.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive un-
certainty estimation using deep ensembles. In Advances in Neural Information Processing Systems , pp.
6402–6413, 2017.
Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why m heads are
better than one: Training a diverse ensemble of deep networks. arxiv preprint arxiv:1511.06314 , 2015.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation
and automatic sharding. In ICLR, 2021.
JesseLevinson, JakeAskeland, JanBecker, JenniferDolson, DavidHeld, SoerenKammel, JZicoKolter, Dirk
Langer, Oliver Pink, Vaughan Pratt, et al. Towards fully autonomous driving: Systems and algorithms.
In2011 IEEE Intelligent Vehicles Symposium (IV) , pp. 163–168. IEEE, 2011.
Raphael Gontijo Lopes, Yann Dauphin, and Ekin D. Cubuk. No one representation to rule them all:
Overlapping features of training methods. CoRR, abs/2110.12899, 2021. URL https://arxiv.org/abs/
2110.12899 .
Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. Sparse-mlp: A fully-mlp architecture with
conditional computation. arxiv, 2021.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computation , 4
(3):448–472, 1992.
Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue.
Towards robust vision transformer, 2021.
Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin
Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. arxiv, 2021.
Basil Mustafa, Carlos Riquelme, Joan Puigcerver, André Susano Pinto, Daniel Keysers, and Neil Houlsby.
Deep ensembles for low-data transfer learning. arXiv preprint arXiv:2010.06866 , 2020.
Giung Nam, Jongmin Yoon, Yoonho Lee, and Juho Lee. Diversity matters when learning from ensembles. In
M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural
Information Processing Systems , volume 34, pp. 8367–8377. Curran Associates, Inc., 2021. URL https:
//proceedings.neurips.cc/paper/2021/file/466473650870501e3600d9a1b4ee5d44-Paper.pdf .
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.
Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes.
In2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing , pp. 722–729. IEEE,
2008.
David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of artiﬁcial
intelligence research , 11:169–198, 1999.
Yaniv Ovadia, Emily Fertig, J. Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji
Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive
uncertainty under dataset shift. In Advances in Neural Information Processing Systems , 2019.
Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE
conference on computer vision and pattern recognition , pp. 3498–3505. IEEE, 2012.
16Published in Transactions on Machine Learning Research (09/2022)
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David
So, Maud Texier, and Jeﬀ Dean. Carbon emissions and large neural network training. arxiv preprint
arxiv:2104.10350 , 2021.
Sayak Paul and Pin-Yu Chen. Vision transformers are robust learners. arxiv, 2021.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. arxiv preprint arxiv:2103.00020 , 2021.
Alexandre Rame, Remy Sun, and Matthieu Cord. Mixmo: Mixing multiple inputs for multiple outputs via
deep subnetworks. arXiv preprint arXiv:2103.06132 , 2021.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize
to imagenet? In International Conference on Machine Learning , pp. 5389–5400, 2019.
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano
Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. arxiv preprint
arxiv:2106.05974 , 2021.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural networks.
In6th International Conference on Learning Representations, ICLR 2018-Conference Track Proceedings ,
volume 6. International Conference on Representation Learning, 2018.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoﬀrey Hinton, and Jeﬀ Dean.
Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017.
Masoumeh Soﬂaei, Hongyu Guo, Ali Al-Bashabsheh, Yongyi Mao, and Richong Zhang. Aggregated learn-
ing: A vector-quantization approach to learning neural network classiﬁers. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence , volume 34, pp. 5810–5817, 2020.
Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overﬁtting. The journal of machine learning research , 15(1):
1929–1958, 2014.
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.
How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint
arXiv:2106.10270 , 2021.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning
in NLP. arxiv preprint arxiv:1906.02243 , 2019.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable eﬀectiveness
of data in deep learning era. In Proceedings of the IEEE international conference on computer vision , pp.
843–852, 2017.
Linh Tran, Bastiaan S. Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V. Dillon, Jasper Snoek, Stephan
Mandt, Tim Salimans, Sebastian Nowozin, and Rodolphe Jenatton. Hydra: Preserving ensemble diversity
for model distillation. CoRR, abs/2001.04694, 2020. URL https://arxiv.org/abs/2001.04694 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In NeurIPS , 2017. URL https://proceedings.neurips.
cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-
200-2011 dataset. 2011.
Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to eﬃcient ensemble
and lifelong learning. In ICLR, 2019.
17Published in Transactions on Machine Learning Research (09/2022)
Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. Hyperparameter ensembles for robust-
ness and uncertainty quantiﬁcation. In Neural Information Processing Systems) , 2020.
Jingjing Xie, Bing Xu, and Chuang Zhang. Horizontal and vertical ensemble with deep representation for
classiﬁcation. arxiv, 2013.
Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. Go wider instead of deeper. arxiv,
2021.
An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang,
Yong Li, et al. Exploring sparse expert models and beyond. arxiv preprint arxiv:2105.15082 , 2021.
Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classiﬁcation. In
Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information
systems, pp. 270–279, 2010.
Seniha Esen Yuksel, Joseph N Wilson, and Paul D Gader. Twenty years of mixture of experts. IEEE
transactions on neural networks and learning systems , 23(8):1177–1193, 2012.
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. arXiv
preprint arXiv:2106.04560 , 2021.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. In International Conference on Learning Representations , 2018.
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cyclical stochastic
gradient mcmc for bayesian deep learning. In ICLR, 2019.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image
database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2017.
18Published in Transactions on Machine Learning Research (09/2022)
Appendix Structure
This appendix is structured as follows:
•We provide experimental details, such as model speciﬁcations, upstream and downstream training
hyperparameters, and few-shot evaluation settings, in Appendix A.
•We discuss the compatibility and adaption of upstream V-MoE and ViT checkpoints to the various
models considered in this paper, in Appendix B.
•We provide implementation details for e3, in Appendix C.
•We show that e3provides larger relative improvements versus V-MoE for larger models, in Ap-
pendix D.
•We draw connections between BE and sparse MoEs, in Appendix E.
•We highlight diﬀerences between e3and BE, in Appendix F.
•We compare empirical results for e3and various other eﬃcient ensemble approaches applied to ViT
and V-MoE, in Appendix G.
•We provide sensitivity analyses for the load balancing loss strength, expert initialisation diversity,
and expert group permutation, in Appendix H.
•We investigate the roles of diversity and individual model performance in overall ensemble perfor-
mance, in Appendix I.
•Wecompareensemblingonlydownstreamwithensemblingthattakesplacebothupanddownstream,
in Appendix J.
•We provide preliminary results for training e3on ImageNet from scratch, without any pre-training,
in Appendix K.
•We extend the results of Sections 3 to 5 to additional datasets, metrics, model sizes, and other
settings, in Appendix L.
•We provide a range of tabular results, including ﬂops numbers, parameter counts, and standard
errors for the experiments in Section 5, in Appendix M.
•We provide additional overview diagrams, in the style of Figure 1, for the various algorithms and
ablations presented in the main text, in Appendix N.
A Experiment Settings
A.1 ViT Model Speciﬁcations
FollowingDosovitskiyetal.(2021), werecallthespeciﬁcationsoftheViTmodelsofdiﬀerentscalesinTable6.
Table 6: Speciﬁcations of ViT-S, ViT-B, ViT-L and ViT-H.
Hidden dimension MLP dimension # layers
Small 512 2048 8
Base 768 3072 12
Large 1024 4096 24
Huge 1280 5144 32
19Published in Transactions on Machine Learning Research (09/2022)
A.2 Upstream Setting
For all our upstream experiments, we scrupulously follow the setting described in Riquelme et al. (2021),
see their Section B.2 in their appendix. For completeness, we just recall that S/32 models are trained for 5
epochs while B/{16, 32} and L/32 models are trained for 7 epochs. For L/16 models, both 7 and 14 epochs
can be considered (Dosovitskiy et al., 2021; Riquelme et al., 2021); we opted for 7 epochs given the breadth
of our experiments. Finally, the H/14 model are trained for 14 epochs.
In particular, the models are all trained on JFT-300M (Sun et al., 2017). This dataset contains about 305M
training and 50000 validation images. The labels have a hierarchical structure, with a total of 18291 classes,
leading on average to 1.89 labels per image.
A.3 Downstream Setting
During ﬁne-tuning, there is a number of common design choices we apply. In particular:
•Image resolution: 384.
•Clipping gradient norm at: 10.0.
•Optimizer: SGD with momentum (using half-precision, β= 0.9).
•Batch size: 512.
•For V-MoE models, we ﬁnetune with capacity ratio C= 1.5and evaluate with C= 8.
We use the following train/validation splits depending on the dataset:
Dataset Train Dataset Fraction Validation Dataset Fraction
ImageNet 99% 1%
CIFAR10 98% 2%
CIFAR100 98% 2%
Oxford-IIIT Pets 90% 10%
Oxford Flowers-102 90% 10%
All those design choices follow from Riquelme et al. (2021) and Dosovitskiy et al. (2021).
A.4 Hyperparameter Sweep for Fine-tuning
In all our ﬁne-tuning experiments, we use the sweep of hyperparameters described in Table 7. We use
the recommendations from Dosovitskiy et al. (2021) and Riquelme et al. (2021), further considering several
factors {0.5, 1.0, 1.5, 2.0} to sweep over diﬀerent numbers of steps. Riquelme et al. (2021) use a half schedule
(with the factor 0.5) while Dosovitskiy et al. (2021) take the factor 1.0.
We show in Table 8 the impact of this enlarged sweep of hyperparameters in the light of the results reported
in Riquelme et al. (2021). We notably tend to improve the performance of ViT and V-MoE (especially for
smaller models), which thus makes the baselines we compare to more competitive.
A.5 Details about the (Linear) Few-shot Evaluation
We follow the evaluation methodology proposed by Dosovitskiy et al. (2021); Riquelme et al. (2021) which
we recall for completeness. Let us rewrite our model fwith parameters θ={Q,θ/prime}as
f(x;θ) =softmax (Qφ(x;θ/prime))
whereQ∈RC×Scorresponds to the parameters of the last layer of fwith theS-dimensional representation
φ(x;θ/prime)∈RS.
20Published in Transactions on Machine Learning Research (09/2022)
Table 7: Hyperparameter values for ﬁne-tuning on diﬀerent datasets. Compared with Dosovitskiy et al.
(2021) and Riquelme et al. (2021), we further consider several factors {0.5, 1.0, 1.5, 2.0} to sweep over
diﬀerent numbers of steps.
Dataset Steps Base LR Expert Dropout
ImageNet 20000 ×{0.5, 1.0, 1.5, 2.0} {0.0024, 0.003, 0.01, 0.03} 0.1
CIFAR10 5000 ×{0.5, 1.0, 1.5, 2.0} {0.001, 0.003, 0.01, 0.03} 0.1
CIFAR100 5000 ×{0.5, 1.0, 1.5, 2.0} {0.001, 0.003, 0.01, 0.03} 0.1
Oxford-IIIT Pets 500 ×{0.5, 1.0, 1.5, 2.0} {0.001, 0.003, 0.01, 0.03} 0.1
Oxford Flowers-102 500 ×{0.5, 1.0, 1.5, 2.0} {0.001, 0.003, 0.01, 0.03} 0.1
Table 8: Impact of using the enlarged sweep of hyperparameters described in Table 7. We typically improve
the results reported in Riquelme et al. (2021), therefore strengthening the baselines we compare to. The
table displays means and standard errors over 8 replications, except for H/14 that has 4 replications. L/16⋆:
For L/16, we consider the setting where the upstream models are trained with 7 epochs, as opposed to 14
epochs in Riquelme et al. (2021), hence the slightly worse accuracy reported in this paper.
Model size Model name Accuracy (this paper) Accuracy (Riquelme et al., 2021)
S/32ViT 76.31 ±0.05 73.73
V-MoE (K=2) 78.91 ±0.08 77.10
B/32ViT 81.35 ±0.08 80.73
V-MoE (K=2) 83.24 ±0.05 82.60
L/32ViT 84.62 ±0.05 84.37
V-MoE (K=2) 84.95 ±0.03 85.04
B/16ViT 84.30 ±0.06 84.15
V-MoE (K=2) 85.40 ±0.04 85.39
L/16⋆ ViT 86.63 ±0.08 87.12
V-MoE (K=2) 87.12 ±0.04 87.54
H/14ViT 88.01 ±0.05 88.08
V-MoE (K=2) 88.11 ±0.13 88.23
In linear few-shot evaluation, we construct a linear classiﬁer to predict the target labels (encoded as one-hot
vectors) from the S-dimensional feature vectors induced by φ(·;θ/prime); see Chapter 5 in Hastie et al. (2017) for
more background about this type of linear classiﬁers. This evaluation protocol makes it possible to evaluate
the quality of the representations φlearned by f.
While Dosovitskiy et al. (2021); Riquelme et al. (2021) essentially focus on the quality of the representations
learned upstream on JFT by computing the (linear) few-shot accuracy on ImageNet, we are interested in the
representations after ﬁne-tuning on ImageNet. As a result, we consider a collection of 8 few-shot datasets
(that does not contain ImageNet):
•Caltech-UCSD Birds 200 (Wah et al., 2011) with 200 classes,
•Caltech 101 (Bansal et al., 2021) with 101 classes,
•Cars196 (Krause et al., 2013) with 196 classes,
•CIFAR100 (Krizhevsky, 2009) with 100 classes,
•Colorectal histology (Kather et al., 2016) with 8 classes,
•Describable Textures Dataset (Cimpoi et al., 2014) with 47 classes,
•Oxford-IIIT pet (Parkhi et al., 2012) with 37 classes and
21Published in Transactions on Machine Learning Research (09/2022)
•UC Merced (Yang & Newsam, 2010) with 21 classes.
In the experiments, we compute the few-shot accuracy for each of the above datasets and we report the
averaged accuracy over the datasets, for various number of shots in {1,5,10,25}. As commonly deﬁned in
few-shot learning, we understand by sshots a setting wherein we have access to straining images per class
label in each of the dataset.
To account for the diﬀerent scales of accuracy across the 8 datasets, we also tested to compute a weighted av-
erage, normalizing by the accuracy of a reference model (ViT-B/32). This is reminiscent of the normalization
carried out in Hendrycks & Dietterich (2019) according to the score of AlexNet. We found the conclusions
with the standard average and weighted average to be similar.
A.5.1 Speciﬁc Considerations in the Ensemble Case
For an ensemble with Mmembers, we have access to Mrepresentations{φ(x;θ/prime
m)}M
m=1for a given input x.
We have explored two ways to use those representations:
•Joint: We concatenate the Mrepresentations{φ(x;θ/prime
m)}M
m=1into a single “joint” feature vector in
RM·S, remembering that each φ(x;θ/prime
m)∈RS. We then train a singlea linear classiﬁer to predict
the target labels from the “joint” feature vectors.
•Disjoint : For each of the Mrepresentations{φ(x;θ/prime
m)}M
m=1, we separately train a linear classiﬁer
to predict the target labels from the feature vectors induced by φ(x;θ/prime
m). We then average the
predictions of the Mlinear classiﬁers trained in this fashion.
In Table 9, we report a comparison of those approaches. We aggregate the results over all ensemble models
(namely, e3and upstream ViT/V-MoE ensembles of size 2 and 4) and over 8 replications, for the ViT families
S/32, B/32 and L/32.
The results indicate that “joint” and “disjoint” perform similarly. Throughout our experiments, we use the
“joint” approach because it eased some implementation considerations.
A.6 List of Datasets
For completeness, in addition to the few-shot datasets listed in Appendix A.5, we list the datasets used for
downstream training and evaluation in this work.
•ImageNet (ILSVRC2012) (Deng et al., 2009) with 1000 classes and 1281167 training examples.
•ImageNet-C (Hendrycks & Dietterich, 2019), an ImageNet test set constructed by applying 15 dif-
ferent corruptions at 5 levels of intensity to the original ImageNet test set. (We report the mean
performance over the diﬀerent corruptions and intensities.)
•ImageNet-A (Hendrycks et al., 2019), an ImageNet test set constructed by collecting new data and
keeping only those images which a ResNet-50 classiﬁed incorrectly.
•ImageNet-V2 (Recht et al., 2019), an ImageNet test set independently collected using the same
methodology as the original ImageNet dataset.
•CIFAR10 (Krizhevsky, 2009) with 10 classes and 50000 training examples.
•CIFAR10-C(Hendrycks&Dietterich,2019), aCIFAR10testsetconstructedbyapplying15diﬀerent
corruptionsat5levelsofintensitytotheoriginalCIFAR10testset. (Wereportthemeanperformance
over the diﬀerent corruptions and intensities.)
•CIFAR100 (Krizhevsky, 2009) with 100 classes and training 50000 examples.
•Oxford Flowers 102 (Nilsback & Zisserman, 2008) with 102 classes and 1020 training examples.
22Published in Transactions on Machine Learning Research (09/2022)
•Oxford-IIIT pet (Parkhi et al., 2012) with 37 classes and 3680 training examples.
•SVHN (Netzer et al., 2011) with 10 classes.
•Places365 (Zhou et al., 2017) with 365 classes.
•Describable Textures Dataset (DTD) (Cimpoi et al., 2014) with 47 classes.
A.7 Sparse MoEs meet Ensembles Experimental Details
The setup for the experiments in Figures 2 and 12 diﬀers slightly the other experiments in this paper.
Speciﬁcally, while for all other experiments we used upstream V-MoE checkpoints with (K,E ) = (2,32), for
these experiments we matched the upstream and downstream checkpoints. We did this to avoid a checkpoint
mismatch as a potential confounder in our results.
A.8 Multiple Predictions without Tiling or Partitioning Details
The naive multi-pred method presented in Section 4.2.4 was trained in almost the same manner as the vanilla
V-MoE, the only diﬀerence being the handling of multiple predictions. This was accomplished by using the
average ensemble member cross entropy as described for e3in Appendix C. In contrast, in order to compute
the evaluation metrics presented in Table 4, we ﬁrst averaged predictions of the model and then used the
average prediction when calculating each metric.
Table 9: Comparison of two approaches, “joint” and “disjoint”, to compute the linear few-shot evaluation in
the case of ensembles. For the ViT families S/32, B/32 and L/32, the mean error across datasets is averaged
over 8 replications and over all the ensemble models of size 2 and 4.
Model size Method Mean error across datasets
1 shot 5 shots 10 shots 25 shots
S/32disjoint 51.01 ±0.4332.80±0.3426.33±0.2620.97±0.18
joint 51.12 ±0.4232.81±0.3026.30±0.2420.77±0.17
B/32disjoint 42.43 ±0.4125.49±0.2120.30±0.1515.98±0.11
joint 42.59 ±0.4025.74±0.1820.54±0.1316.06±0.10
L/32disjoint 36.41 ±0.3121.49±0.1517.13±0.1213.56±0.10
joint 36.48 ±0.3021.66±0.1317.34±0.1013.56±0.08
B Compatibility and Adaptation of the Upstream Checkpoints
Throughout the paper, we make the assumption that we can start from existing checkpoints of ViT and V-
MoE models (trained on JFT-300M; see Appendix A.2). We next describe how we can use those checkpoints
for the ﬁne-tuning of the extensions of ViT and V-MoE that we consider in this paper.
In all our experiments that involve V-MoEs, we consider checkpoints with K= 2andE= 32, which is the
canonical setting advocated by Riquelme et al. (2021).
B.1 Eﬃcient Ensemble of Experts
In the case of e3, the set of parameters is identical to that of a V-MoE model. In particular, neither the
tiled representation nor the partitioning of the experts transforms the set of parameters.
To deal with the fact that the single routing function gateK(W·)of a V-MoE becomes separate routing
functions{gateK(Wm·)}M
m=1, one for expert subset Em, we simply slice row-wise W∈RE×Dinto theM
matricesWm∈R(E/M )×D.
23Published in Transactions on Machine Learning Research (09/2022)
B.2 Batch Ensembles (BE)
We train BE starting from ViT checkpoints, which requires to introduce downstream-speciﬁc parameters.
Following the design of V-MoEs, we place the batch-ensemble layers in the MLP layers of the Transformer.
Let us consider a dense layer in one of those MLPs, with parameters U∈RD×L, in absence of bias term.
In BE, the parametrization of each ensemble member has the following structure Um=U◦(rms/latticetop
m)where
{rm}M
m=1and{sm}M
m=1are respectively D- andL-dimensional vectors.
A standard ViT checkpoint provides pre-trained parameters for U. We then introduce {rm}M
m=1and
{sm}M
m=1at ﬁne-tuning time, following the random initialization schemes proposed in Wen et al. (2019); see
details in the hyperparameter sweep for BE in Appendix G.1.
B.3 MIMO
We train MIMO models from V-MoE checkpoints. The only required modiﬁcations are to the input and
output parameters of the checkpoints. The linear input embedding must be modiﬁed to be compatible with
input images containing Mtimes as more channels, as required by the multiple-input structure of MIMO.
Similarly, the ﬁnal dense layer in the classiﬁcation head must be modiﬁed to have Mtimes more output
units, following the multiple-output structure in MIMO.
Concretely, theembeddingweight Win∈RH×W×3×Disreplicatedinthethird(channel)dimension, resulting
inWMIMO ,in∈RH×W×3·M×D, whereHandWare the height and width of the convolution and Dis the
hidden dimension of the ViT family (speciﬁed in Table 6). The output layer weight Wout∈RD×Cis
replicated in the second (output) dimension, resulting in WMIMO ,out∈RH×C·M, whereCis the number of
classes. The output layer bias bout∈RCis replicated resulting in bMIMO ,out∈RC×M. Finally, in order to
preserve the magnitude of the activation for these layers, WMIMO ,inandWMIMO ,outare scaled by 1/M.
C Implementation Details of Eﬃcient Ensemble of Experts
We provide details about the training loss and the regularizer used by e3. We also discuss the memory
requirements compared to V-MoE.
C.1 Training Loss
Since e3outputsMpredictions{f(x;θm)}M
m=1for a given input x, we need to adapt the choice of the
training lossLaccordingly. Following the literature on eﬃcient ensembles (Wen et al., 2019; Dusenberry
et al., 2020a; Wenzel et al., 2020), we choose the average ensemble-member cross entropy
L(y,x;θ) =1
MM/summationdisplay
m=1cross-entropy (y,f(x;θm))
instead of other alternatives such as the ensemble cross-entropy
cross-entropy/parenleftBig
y,1
MM/summationdisplay
m=1f(x;θm)/parenrightBig
that was observed to generalize worse (Dusenberry et al., 2020a).
C.2 Auxiliary Losses
Inspired by previous applications of sparse MoEs in NLP (Shazeer et al., 2017), Riquelme et al. (2021)
employ regularizers, also referred to as auxiliary losses , to guarantee a balanced usage of the Eexperts. Two
auxiliary losses—the importance and load losses, see Appendix A in Riquelme et al. (2021) for their formal
deﬁnitions—are averaged together to form the ﬁnal regularization term that we denote by Ω.
24Published in Transactions on Machine Learning Research (09/2022)
As a reminder, let us recall the notation of the routing function
h∈RD/mapsto→gateK(Wh) =topK(softmax (Wh +σε))∈RE,
withW∈RE×Dandε∼N(0,I). Consider a batch of Binputs{hi}B
i=1that we represent by H∈RB×D.
Finally, let us deﬁne
A=HW/latticetop+σεB×E∈RB×E,
where we emphasise that εB×Eis a matrix of Gaussian noise entries in RB×E. The regularization term Ω
used by Riquelme et al. (2021) can be seen as a function that depends on AandHW/latticetop.
In the context of eﬃcient ensemble of experts, the set of Eexperts is partitioned into Msubsets of
E/Mexperts, denoted by ∪M
m=1Em; see Section 4.1. With the introduction of the Mrouting func-
tions{gateK(Wm·)}M
m=1with eachWm∈R(E/M )×D, the matrix Abecomes accordingly partitioned into
{Am}M
m=1where eachAm∈RB×(E/M ).
Since we want to enforce a balanced usage of the E/Mexperts in each subset Emof the experts, we thus
redeﬁne the regularization as the average regularization separately applied to each part of the partition
Ωpartition (A,HW/latticetop) =1
MM/summationdisplay
m=1Ω(Am,HW/latticetop
m).
We found this option to work better in practice. To guarantee a fair comparison, we also applied Ωpartition
to the “Only partitioning” model in the ablation study of Section 4.2.1.
Following Riquelme et al. (2021), the regularization parameter controlling the strength of Ωpartitionwas set
to 0.01 throughout the experiments.
C.3 Memory Requirements versus V-MoE
Due to tiling, e3requires more memory than V-MoE. To be concrete, the memory complexity of V-MoE can
be decomposed into two terms
O/parenleftbig
memoryparams +memoryactivations/parenrightbig
,
for the forward and backward passes, respectively. For e3, the complexity becomes
O/parenleftbigg
memoryparams +memoryactivations×Lbefore +Lafter×M
Ltotal/parenrightbigg
,
whereMistheensemblesize, and, Lbefore,Lafter, andLtotalarethenumberoflayersbeforetiling, aftertiling,
and in total, respectively. Importantly, neither memoryparamsnormemoryactivations depend on M. Thanks
to the “last- n” setting employed in the paper, we have Lafter/lessmuchLbefore, and thus the increase in memory
due to tiling remains mild. More concretely, for ViT-L, we have Ltotal= 24,Lbefore = 21, andLafter= 3
(with MoE layers placed at layers 22 and 24). Thus, for an ensemble of size M= 2,e3would only increase
memoryactivations by 12.5%, while leaving memoryparamsunchanged.
D Eﬃcient Ensemble of Experts and V-MoE Relative Improvements per ViT Family
In Section 5 we claim that e3performs best at the largest scale. In this section we motivate that claim
in more detail. Speciﬁcally, we consider two metrics of improvement in performance. Firstly, we consider
the percentage improvement in NLL for both e3and V-MoE versus vanilla ViT. Secondly, we consider a
normalised version of this improvement. We consider this second metric to take into account the “diﬃculty”
in further improving the NLL of larger ViT family models. Intuitively, the larger the ViT family, the better
the corresponding NLL will be, and the more diﬃcult it will be to improve on that NLL.
The normalisation we apply is based on the gradient of the NLL with respect to FLOPs. Indeed, this
gradient captures the typical variation of NLL at a particular amount of FLOPs. The ratio of this gradient
at the FLOPs values (i.e., the instantaneous change in NLL at those FLOPs values) for two ViT families
25Published in Transactions on Machine Learning Research (09/2022)
is a measure of the relative diﬃculty in increasing the NLL. Thus, we can use this ratio to normalise our
results. To be more concrete, let us deﬁne the mapping
NLL =ϕ(FLOPs) and its derivative ϕ/prime(FLOPs) =dϕ(FLOPs)
dFLOPs.
We estimate ϕand its gradient by ﬁtting a linear model to the (NLL,FLOPs) pairs for each
ViT family, using the data of the standard ViT models we trained. We use feature expansion
[1.0,log(FLOPs) ,log(FLOPs)2,log(FLOPs)3]and solve for the parameters of the linear model via ordinary
leastsquares. WedeterminethegradientofthisfunctionateachFLOPsvalueusingautomaticdiﬀerentiation
inJAX(Bradbury et al., 2018). See Figure 9 for the resulting ﬁt and an indication of the gradients.
0 1000 2000 3000
(downstream) GFLOPs0.40.50.60.70.80.91.0NLL
ϕ
Tangent
ViT
Figure 9: Estimated ϕcompared to the ImageNet NLL values for our ViT models. We also include the
tangent at the points corresponding to each ViT model to indicate the gradients at those points.
The normalised values are calculated as:
Normalised improvement( v) = improvement( v)×ϕ/prime(FLOPs H/14)
ϕ/prime(FLOPs v), (4)
wherevis one of the ViT families, i.e., S/32, B/32, L/32, L/16, or H/14. Note that this normalisation leaves
the improvement for H/14 the same. We tried to normalize with respect to other choices of ViT family,
diﬀerent from H/14. Our conclusions are robust, in the sense that both the ordering and the monotonic
behavior with respect to scale are preserved. Using the ratio for normalisation also has the advantage that
the normalisation is less sensitive to the particular parameterisation of ϕ.
Table 10: Percentage improvements in NLL for e3with (K,M ) = (1,2)and V-MoE with K= 1vs. ViT for
families of increasing size. The top two rows show normalised improvements, see (4), which take into con-
sideration the increased diﬃculty of improving NLL for larger ViT families whose performance is beginning
to saturate. The bottom two rows are the original percentage improvements without normalisation.
S/32 B/32 L/32 L/16 H/14
Normalisede3vs. ViT 0.02% 0.09% 0.24% 2.35% 4.27%
V-MoE vs. ViT 0.01% 0.06% -0.04% 0.89% 0.02%
Not normalisede3vs. ViT 9.82% 9.53% 3.76% 5.38% 4.27%
V-MoE vs. ViT 7.98% 6.62% -0.60% 2.05% 0.02%
Table 10 shows both the diﬃculty-normalised and original improvements (without normalisation). Looking
ﬁrst at the original improvements, we can see that while both e3and V-MoE have smaller improvements over
ViT for larger families, e3’s improvements decrease more slowly. Furthermore, by comparing the normalised
improvementsweseethat e3’simprovementsactually growmonotonicallywhentakingdiﬃcultyintoaccount.
This is not the case for V-MoE.
26Published in Transactions on Machine Learning Research (09/2022)
E From Batch Ensembles to Sparse MoEs
Wen et al. (2019) have shown that, given a batch of BinputsX∈RB×P, a single forward pass can eﬃciently
compute the predictions of all the ensemble members {f(X;θm)}M
m=1. By appropriately tiling the inputs of
the networkXtiled∈R(M·B)×Pby a factor M, each internal operation per ensemble member can then be
vectorized.
We take the previous example of a dense layer with parameters U∈RD×Land we assume the layer
receives the tiled inputs {Hm}M
m=1whereHm∈RB×D. We need to compute for each ensemble member
HmUm=Hm[U◦(rms/latticetop
m)]. Denoting by hi,m∈RDthei-th input inHm, we have
h/latticetop
i,mUm=E/summationdisplay
e=1ge(hi,m)·experte(hi,m)withM=E,/braceleftBigg
ge(hi,m) = 1ife=m,
ge(hi,m) = 0otherwise(5)
andexperte(z) =z/latticetop[U◦(res/latticetop
e)]. Although (5) may appear as a convoluted way of writing the operations
in batch ensembles, it unveils a connection with (1). Indeed, operations in batch ensembles can be seen as
a speciﬁc sparse MoE, e.g., with binary routing weights depending only on the position in the tiled inputs.
While Wen et al. (2019) primarily tiled the inputs for the sake of eﬃciency, it also induces some form of
conditional computation, an insight that we exploit in e3.
F Batch Ensembles versus Eﬃcient Ensemble of Experts
Table 11: Summary of the diﬀerences between BE and e3.
Shared Parameters Unique Parameters Tiling Training Epochs
BE Kernels in each linear layer “Fast-weights” and biases Start of the model Typically ≈50% more
e3All parameters outside MoE layer Experts in subset Em Before 1stMoE layer Unchanged
While e3is inspired in several ways by BE, it has also been specialised to sparse MoEs. This leads to a
number of signiﬁcant diﬀerences between the two algorithms. These diﬀerences are summarised in Table 11,
and we elaborate upon them here:
•Shared parameters: In BE, the shared parameters are the kernels in each linear (e.g. dense
or convolution) layer. This makes up the majority of the parameters in BE. In e3, the shared
parameters are all of those not found in the expert layers.
•Ensemble member speciﬁc parameters: BE uses pairs of “fast-weights” {rm,sm}and bias
terms for each linear layer. e3uses the parameters of all the E/Mexperts in the m-th subsetEm
and the router for those expert, in each MoE layer. This results in a much larger set of ensemble
member speciﬁc parameters, and thus a higher level of predictive diversity (see Section 4.2.3).
•Tiling: In BE, tiling is performed at the very beginning of the model, regardless of the model’s
internal structure. Tiling in e3occurs before the ﬁrst MoE layer, thus saving redundant computation
for “Last-n” V-MoE.
•Number of training epochs: BE usually requires 50% more training epochs than a vanilla model.
e3requires the same number of training epochs as V-MoE.
G Eﬃcient Ensemble Comparisons
Inthissection, wecompareeﬃcientensembleofexperts( e3)toseveralpopulareﬃcientensembleapproaches,
namely MIMO (Havasi et al., 2020), batch ensemble (BE) (Wen et al., 2019), and MC Dropout (Gal &
Ghahramani, 2016).
27Published in Transactions on Machine Learning Research (09/2022)
Table 12: ImageNet performance of diﬀerent eﬃcient ensemble approaches. The table reports the means ±
standard errors over 8 replications. All models have a ViT-B/32 architecture. Kstands for the sparsity in
V-MoEs,Mdenotes the ensemble size while “BR” corresponds to the batch repetition in MIMO (Havasi
et al., 2020).
K M NLL ↓ Error↓ ECE↓ KL↑ GFLOPs↓
ViT – – 0.688 ±0.00318.65±0.080.022±0.000 – 78.0
BE ViT– 2 0.682 ±0.00318.47±0.050.021±0.0000.040±0.001 97.1
– 4 0.675 ±0.00318.40±0.090.017±0.0000.035±0.001 135.4
V-MoE1 – 0.642 ±0.00216.90±0.050.029±0.001 – 82.4
2 – 0.638 ±0.00116.76±0.050.033±0.001 – 94.9
4 – 0.636 ±0.00116.70±0.040.034±0.001 – 120.1
8 – 0.635 ±0.00216.72±0.060.028±0.001 – 170.4
MC Dropout V-MoE1 2 0.648 ±0.00217.10±0.050.019±0.0010.046±0.000 97.2
1 4 0.641 ±0.00216.96±0.050.017±0.0010.046±0.001 135.6
2 2 0.642 ±0.00216.94±0.040.021±0.0010.046±0.001 113.7
2 4 0.634 ±0.00116.80±0.030.020±0.0000.046±0.001 168.6
4 2 0.639 ±0.00216.91±0.060.022±0.0010.045±0.001 146.7
MIMO V-MoE (BR=1)2 2 0.636 ±0.00216.97±0.040.028±0.0010.000±0.000 96.3
2 4 0.672 ±0.00117.72±0.040.037±0.0000.001±0.000 99.0
MIMO V-MoE (BR=2)2 2 0.638 ±0.00117.14±0.030.031±0.0000.001±0.000 192.6
2 4 0.665 ±0.00217.38±0.040.038±0.0000.000±0.000 198.1
e31 2 0.622 ±0.00116.70±0.030.018±0.0000.217±0.003 105.9
1 4 0.624 ±0.00116.99±0.030.013±0.0000.164±0.001 153.0
2 2 0.612 ±0.00116.49±0.020.013±0.0000.198±0.003 131.1
2 4 0.620 ±0.00116.86±0.020.015±0.0000.170±0.001 203.3
4 2 0.611 ±0.00116.45±0.030.014±0.0000.193±0.003 181.4
Table 12 reports the ImageNet performance of those diﬀerent techniques, when all models are based on a
ViT-B/32 architecture. We start by highlighting the most salient conclusions of the experiment and defer
to the next subsections the descriptions of the diﬀerent competing techniques.
We make the following observations:
•BE built upon ViT improves the performance of ViT in terms of NLL, classiﬁcation error and ECE.
However, the resulting increase in FLOPs makes BE a less viable option compared to e3.
•MC Dropout V-MoE is on par with standard V-MoE in terms of NLL and classiﬁcation error, while
it improves the ECE. For all values of K, we observe that the performance tends to improve as the
number of samples, i.e., M, increases. However, already for Min{2,4}, the resulting increase in
FLOPs makes MC Dropout V-MoE a less favorable option compared to e3.
•Perhaps surprisingly (see detailed investigations in Appendix G.3), MIMO V-MoE does not lead to
improvements compared with V-MoE. In fact, for higher ensembles sizes, MIMO V-MoE results in
worse performance than standard V-MoE. Moreover, increasing the batch repetition parameter of
MIMO (“BR” in Table 12) further worsens the results. Interestingly, we can see that MIMO does
not manage to produce diverse predictions, as illustrated by the small values of KL.
•e3oﬀers the best performance vs. FLOPs trade-oﬀs, e.g., when looking at (K,M ) = (1,2)and
(K,M ) = (2,2). Wenotablyobservethatthediversityofthepredictionsin e3isordersofmagnitude
larger than that of the other ensemble approaches.
28Published in Transactions on Machine Learning Research (09/2022)
We brieﬂy recall the optimization explained in Section 4.1 to save redundant computations: In the “last-
n” setting of Riquelme et al. (2021), it is suﬃcient to tile the representations only when entering the ﬁrst
MoE layer/dropout layer/batch-ensemble layer for respectively e3/MC Dropout V-MoE/BE. We apply this
optimization to all the eﬃcient ensemble methods.
G.1 Batch Ensembles
Following the design of V-MoEs, we place the batch-ensemble layers in the MLP layers of the Transformer,
following the “last- n” setting of Riquelme et al. (2021); see Section 2.1.
The vectors of the rank-1 parametrization introduced at ﬁne-tuning time (see Appendix B) need to be
initialized and optimized. Following the recommendation from Wen et al. (2019), we consider the following
hyperparameters in addition to the common sweep described in Table 7:
•Initialization: Either a random sign vector with entries in {−1,1}independently drawn with
probability1
2or a random Gaussian vector with entries independently drawn from N(1,0.5).
•Learning-ratescalefactor: Thevectorsoftherank-1parametrizationareupdatedwithalearning
rate scaled by a factor in {0.5,1,2}.
G.2 MC Dropout V-MoEs
For MC Dropout V-MoE, we take the available ﬁne-tuned V-MoEs and enable dropout at prediction time.
Indeed, as described in Table 7, all V-MoE models already have a 0.1 dropout rate in the experts.
G.3 MIMO V-MoEs
Following Havasi et al. (2020) we consider two MIMO-speciﬁc hyperparameters, in addition to the hyperpa-
rameters listed in Table 7:
•Input replication probability: {0.5,0.625,0.75}
•Batch repetitions: {1,2}
Our preliminary investigations also considered lower input repetition probabilities and higher batch repeti-
tions. However, lower input repetition probabilities tended to result in poorer performance. While higher
batch repetitions did help to some extent, the additional computational cost made it impractical.
Given the surprising result that an ensemble size of M= 2provides no performance improvement over
the standard V-MoE and that increasing Mfurther provides worse performance, there seems to be some
incompatibility between MIMO and V-MoE. In fact, our investigations revealed that ViT is the source of the
problems since applying MIMO to vanilla ViT without experts resulted in the same trends as for V-MoE.
Thus we hypothesise that the diﬀerences between ViT and ResNet—the architecture to which MIMO was
originally applied by Havasi et al. (2020)—are responsible for MIMO’s poor performance when applied to
ViT.
Diﬀerence 1: Class token. One of the diﬀerences between ViT and ResNet is that ViT makes use of a
special learnable class token to classify an image (see Dosovitskiy et al. (2021) for details). ResNet on the
other hand makes use of the representation from an entire image for classiﬁcation. We tried two strategies
to mitigate this diﬀerence:
1. We applied the global average pooling (GAP) and multi-head attention pooling (MAP) classiﬁcation
strategies introduced in Dosovitskiy et al. (2021) and Zhai et al. (2021), respectively. In short, both
of these methods make use of all the tokens from an image for classiﬁcation. However, neither of
these strategies made a signiﬁcant diﬀerence to the relative performance of MIMO and ViT. In fact,
the choice of classiﬁcation method was the least impactful hyperparameter in our sweep.
29Published in Transactions on Machine Learning Research (09/2022)
2. Rather than learning a single class token, we learnt Mclass tokens. This strategy resulted in MIMO
withM= 2outperforming ViT. However, for M > 2the improvement was small enough that ViT
still outperformed MIMO.
Diﬀerence 2: Attention. The other major diﬀerence between ViT and ResNet is the building block for
each model. While ResNets are primarily composed of convolution operations, ViT makes heavy used of
attention. We hypothesised that attention is less suited to separating the information for Minput images
stored in the channel dimension of a single image. We tried two strategies to mitigate this potential issue:
1. We applied the hybrid architecture, described in Dosovitskiy et al. (2021), in which the input se-
quence to ViT is formed by CNN feature maps. We used ResNet14 and ResNet50. In both cases,
we found that strategy boosted the performance of ViT and MIMO equally.
2. Rather than concatenating images in the channel dimension, we concatenated them in the width
dimension, resulting in 3 times as many patches for ViT to process. This strategy was successful in
the sense that the MIMO performance for M > 2improved signiﬁcantly. However, the signiﬁcant
additional computational cost made it an infeasible solution.
Our ﬁndings suggest that MIMO and ViT are indeed somewhat incompatible. Unfortunately, none of our
proposed solutions to this problem provided high enough predictive performance increases (or indeed low
enough computational cost increases in some cases) to warrant immediate further investigation.
H Sensitivity Analyses
H.1 Load Balancing
Figure 10 explores the eﬀect of the load balancing loss on diversity and predictive performance. We see
that for both V-MoE and e3the predictive performance, as measured by NLL, Error, and ECE are largely
insensitive to the strength of the load balancing loss. Similarly, the diversity of the predictions of e3—as
measured by the KL—mildly depends on that regularization. Only when the load balancing loss strength
becomes excessively large (4 orders of magnitude larger than standard values) do all the performance metrics
plummet. In other words, this hyperparameter does not allow us to increase the diversity and thereby the
predictive performance of our models.
4
 2
 0 2
log10()
0.60.70.80.91.0NLL (lower is better)V-MoE (K=2)
V-MoE (K=4)
E3 (K=1, M=2)
E3 (K=2, M=2)
4
 2
 0 2
log10()
1618202224Error (lower is better)
4
 2
 0 2
log10()
0.010.020.030.040.050.06ECE (lower is better)
4
 2
 0 2
log10()
0.160.170.180.190.200.210.220.23KL (higher is better)
Figure 10: The eﬀect of λ—the strength of the load balancing, see appendix A of Riquelme et al. (2021)—on
NLL, Error, ECE and diversity, for e3and V-MoE. Results are averaged over three random seeds. All models
have a ViT-B/32 architecture.
H.2 Expert Initialization
Figure 11 explores the inﬂuence of the initialization of the experts. In particular, we add Gaussian noise with
varying standard deviations to the initial weights of the expert MLPs. We notably show that more diverse
initializations (larger standard deviations) do not translate to any clear performance gain. Note that this
experiment takes place upstream , since downstream, the experts are already initialized (we ﬁne-tune them
from the upstream checkpoint).
30Published in Transactions on Machine Learning Research (09/2022)
0108
107
106
104
103
102
101
 1
Initialization standard deviation0.450.500.550.600.65ImageNet 1-Shot Error (lower is better)V-MoE-S/32
0108
107
106
104
103
102
101
 1
Initialization standard deviationImageNet 5-Shot Error (lower is better)
0108
107
106
104
103
102
101
 1
Initialization standard deviationImageNet 10-Shot Error (lower is better)
0108
107
106
104
103
102
101
 1
Initialization standard deviationImageNet 25-Shot Error (lower is better)
Figure 11: The inﬂuence of noise in the expert MLPs’ initial random weights. The models are trained on
JFT-300M and we measure the ImageNet few-shot Error as in Riquelme et al. (2021). Results are for a
single random seed.
H.3 Expert Group Permutation
Table 13 investigates the performance of e3where the ordering of the upstream V-MoE experts has been
randomlypermutedbeforeﬁne-tuning1. Weseethatthepermutationoftheexpertshaslittle-to-nosigniﬁcant
impact on performance (as measured by NLL and error). The small discrepancies between error for K= 1
and NLL for K= 2can be explained by noting that e3aggregates results with diﬀerent upstream checkpoints
while e3(permuted ) only takes one of these checkpoints and varies the permutations; this was done to isolate
the eﬀect of the permutations only. Note also that the standard errors for e3ande3(permuted ) are very
close to each other, which indicates that permutation of experts has a similar impact to random initialization.
Table 13: Comparison of e3-B/32 with E= 32total experts and M= 2partitions, and e3where the
order of the experts has been permuted . For e3results are aggregated over 8 random seeds whereas for e3
(permuted ) results are aggregated over 5 permutations.
K NLL↓ Error↓
e310.622±0.00116.70±0.03
e3(permuted ) 10.620±0.00116.80±0.04
e32 0.612±0.00116.49±0.02
e3(permuted ) 20.608±0.00016.47±0.03
I The Roles of Ensemble Diversity and Individual Model Performance
Table 14 shows the individual member performance for each of our eﬃcient ensemble variants as well as
upstream and downstream deep ensembles, for sizes M= 2andM= 4. For each method and ensemble
size, the ﬁrst row shows the combined ensemble performance, and the following rows show the performance
of the individual ensemble members.
Fore3, the gap between single members and their ensemble is considerable. This is reminiscent of deep
ensembles (both variants) and in stark contrast with what we observe for the other eﬃcient ensembles: BE
ViT and MIMO V-MoE. The diversity of e3is comparable to that of upstream deep ensembles and much
larger than downstream deep ensembles. For example, if we compare MIMO V-MoE (M=2) and e3(M=2),
the single members of e3are all worse in NLL, ACC, and ECE than the single members of MIMO. However,
the performance gap between the individual members and the ensemble is much larger for e3than MIMO
(where there is almost no diﬀerence). The diversity of the individual models in e3is key to its strong
performance. Thus, e3outperforms MIMO.
1We also permute the columns of the routing matrix Wmaccordingly to keep the routing consistent.
31Published in Transactions on Machine Learning Research (09/2022)
In short, this suggests that e3approximates a deep ensemble of smaller V-MoE models. That is, with e3,
we are able to take advantage of the overparameterization of the experts to create a rich set of ensemble
members within a single model. Each ensemble member has a large number of non-shared parameters, thus
high induced diversity. In comparison, BE only has a few vectors speciﬁc to each member.
32Published in Transactions on Machine Learning Research (09/2022)
Table 14: Comparison of the individual ensemble member performance and combined ensemble performance
for BE ViT, MIMO V-MoE ( K= 2, BR=1), e3(K= 1), as well as upstream and downstream V-MoE
(K= 1) ensembles.
NLL↓ Error↓ ECE↓ KL↑
BE ViTM=2ensemble 0.682 ±0.00318.47±0.050.021±0.0000.040±0.001
member 0 0.693 ±0.00318.68±0.050.025±0.000 —
member 1 0.693 ±0.00318.68±0.040.025±0.001 —
M=4ensemble 0.675 ±0.00318.40±0.090.017±0.0000.035±0.001
member 0 0.690 ±0.00318.70±0.090.022±0.000 —
member 1 0.690 ±0.00318.70±0.080.023±0.000 —
member 2 0.690 ±0.00318.70±0.070.023±0.000 —
member 3 0.691 ±0.00318.70±0.070.023±0.000 —
MIMO V-MoEM=2ensemble 0.636 ±0.00216.97±0.040.028±0.0010.001±0.000
member 0 0.636 ±0.00216.97±0.040.028±0.001 —
member 1 0.636 ±0.00216.97±0.040.028±0.000 —
M=4ensemble 0.672 ±0.00117.72±0.040.037±0.0000.001±0.000
member 0 0.672 ±0.00117.74±0.050.037±0.000 —
member 1 0.672 ±0.00117.71±0.050.037±0.000 —
member 2 0.672 ±0.00117.72±0.050.037±0.000 —
member 3 0.673 ±0.00117.73±0.050.037±0.000 —
e3M=2ensemble 0.622 ±0.00116.70±0.030.018±0.0000.217±0.003
member 0 0.671 ±0.00317.74±0.060.038±0.001 —
member 1 0.683 ±0.00217.94±0.0050.038±0.001 —
M=4ensemble 0.624 ±0.00116.99±0.030.013±0.0000.164±0.001
member 0 0.677 ±0.00218.04±0.050.034±0.001 —
member 1 0.685 ±0.00118.23±0.050.034±0.001 —
member 2 0.691 ±0.00218.35±0.070.035±0.001 —
member 3 0.697 ±0.00218.47±0.080.035±0.001 —
Up-DEM=2ensemble 0.588 ±0.00115.74±0.050.017±0.0010.214±0.001
member 0 0.640 ±0.00116.82±0.040.030±0.000 —
member 1 0.645 ±0.00216.97±0.060.029±0.002 —
M=4ensemble 0.561 ±0.00115.10±0.030.020±0.0000.214±0.001
member 0 0.639 ±0.00116.80±0.030.030±0.000 —
member 1 0.641 ±0.00116.84±0.050.030±0.000 —
member 2 0.642 ±0.00116.90±0.030.031±0.000 —
member 3 0.647 ±0.00217.05±0.060.028±0.002 —
Down-DEM=2ensemble 0.620 ±0.00116.44±0.040.023±0.0000.073±0.001
member 0 0.642 ±0.00116.83±0.030.030±0.000 —
member 1 0.643 ±0.00116.84±0.030.030±0.000 —
M=4ensemble 0.607 ±0.00016.17±0.020.021±0.0010.073±0.000
member 0 0.641 ±0.00116.82±0.030.030±0.000 —
member 1 0.642 ±0.00116.85±0.030.030±0.000 —
member 2 0.643 ±0.00116.89±0.040.031±0.000 —
member 3 0.643 ±0.00116.93±0.070.031±0.000 —
33Published in Transactions on Machine Learning Research (09/2022)
J Upstream & Downstream versus Downstream-only Ensembles
In Section 5, and Appendix L include downstream deep ensembles (down-DE) of V-MoE, and in some cases ViT, as a baseline. This choice was
motivated by the fact that like ViT, V-MoE, and e3, down-DE requires a only a single upstream checkpoint, which all of the methods more comparable.
However, it is clear that using diﬀerent upstream checkpoints and then further ﬁne-tuning each of these with diﬀerent random seeds to construct an
upstream deep ensemble (up-DE) would result in more varied ensemble members and as a result, a better performing ensemble. This idea has recently
been explored by Mustafa et al. (2020).
Thus, for completeness, we also investigate the eﬀects of upstream ensembling on V-MoE. Table 15 compares the performance of upstream and
downstream V-MoE ( K= 1) ensembles of sizes M= 2andM= 4. Across the range of metrics, for both ImageNet and ImageNet-C, for all ViT
families, and for both values of M, we see that up-DE outperforms down-DE. In fact, up-DE with M= 2is very often better than or equal to
down-DE with M= 4. This is especially true for the diversity metrics, which indicates that diversity is indeed the driver for improved performance
in up-DE. Not shown in the table is the very large computational cost associated with training upstream ensembles.
Table 15: Comparison of upstream and downstream ensembles of V-MoE with ( K= 1).
ImageNet ImageNet-C
M NLL↓ Error↓ ECE↓ KL↑ Cos. Sim.↓Norm. Dis.↑ NLL↓ Error↓ ECE↓ KL↑ Cos. Sim.↓Norm. Dis.↑
H/14down-DE 2 0.403 ±0.00011.35±0.050.018±0.0010.079±0.0030.974±0.001 0.488±0.0060.871±0.01221.37±0.200.021±0.0010.218±0.0020.925±0.001 0.628±0.003
up-DE 2 0.391 ±0.00011.12±0.100.016±0.0010.126±0.0080.963±0.0020.625±0.0120.839±0.01120.66±0.220.022±0.0000.355±0.0070.892±0.0020.809±0.006
down-DE 4 0.392 ±0.00011.20±0.0000.014±0.0000.083±0.0000.973±0.000 0.509±0.0000.851±0.00020.97±0.0000.021±0.0000.221±0.0000.923±0.000 0.650±0.000
up-DE 4 0.375±0.00010.66±0.0000.013±0.0000.129±0.0000.963±0.0000.652±0.0000.792±0.00019.61±0.0000.032±0.0000.361±0.0000.892±0.0000.850±0.000
L/16down-DE 2 0.450 ±0.00212.62±0.040.016±0.0000.061±0.0010.979±0.000 0.419±0.0021.010±0.00624.43±0.120.021±0.0000.168±0.0020.936±0.001 0.539±0.003
up-DE 2 0.434 ±0.00012.23±0.040.014±0.0000.118±0.0010.964±0.0000.584±0.0010.961±0.00123.46±0.030.023±0.0000.342±0.0010.890±0.0000.766±0.001
down-DE 4 0.440 ±0.00212.39±0.060.015±0.0000.061±0.0010.979±0.000 0.425±0.0020.983±0.00623.95±0.120.020±0.0000.166±0.0010.937±0.000 0.547±0.002
up-DE 4 0.418±0.00011.86±0.010.013±0.0000.118±0.0000.964±0.0000.603±0.0010.916±0.00122.45±0.020.034±0.0000.341±0.0000.890±0.0000.800±0.001
L/32down-DE 2 0.533 ±0.00214.55±0.040.025±0.0010.092±0.0010.969±0.000 0.479±0.0041.184±0.00327.98±0.040.029±0.0000.199±0.0020.925±0.001 0.556±0.002
up-DE 2 0.511 ±0.00114.07±0.020.019±0.0000.191±0.0010.945±0.0000.694±0.0051.133±0.00226.97±0.040.022±0.0000.449±0.0050.861±0.0010.820±0.003
down-DE 4 0.518 ±0.00214.29±0.030.022±0.0000.092±0.0010.969±0.000 0.487±0.0031.154±0.00427.47±0.050.023±0.0000.199±0.0020.925±0.001 0.567±0.002
up-DE 4 0.486±0.00013.52±0.020.016±0.0000.190±0.0010.946±0.0000.722±0.0011.073±0.00125.74±0.020.030±0.0000.446±0.0010.862±0.0000.857±0.000
B/16down-DE 2 0.519 ±0.00214.09±0.020.021±0.0010.048±0.0000.982±0.000 0.351±0.0021.316±0.00830.02±0.180.030±0.0000.132±0.0010.943±0.000 0.448±0.002
up-DE 2 0.489 ±0.00113.40±0.030.015±0.0000.169±0.0020.951±0.0000.668±0.0041.231±0.00428.41±0.090.023±0.0000.481±0.0060.845±0.0010.838±0.003
down-DE 4 0.511 ±0.00213.95±0.010.019±0.0010.048±0.0000.982±0.000 0.354±0.0021.293±0.00829.67±0.180.026±0.0000.132±0.0010.943±0.000 0.453±0.002
up-DE 4 0.468±0.00012.89±0.030.016±0.0000.168±0.0000.951±0.0000.690±0.0011.166±0.00227.08±0.050.037±0.0000.479±0.0010.846±0.0000.879±0.001
B/32down-DE 2 0.620 ±0.00116.44±0.040.023±0.0000.073±0.0010.973±0.000 0.414±0.0021.510±0.00533.79±0.080.032±0.0000.175±0.0010.925±0.000 0.498±0.002
up-DE 2 0.588 ±0.00115.74±0.050.017±0.0010.214±0.0010.937±0.0000.709±0.0011.430±0.00332.37±0.050.022±0.0000.537±0.0020.824±0.0010.844±0.002
down-DE 4 0.607 ±0.00016.17±0.020.021±0.0010.073±0.0000.973±0.000 0.418±0.0051.483±0.00833.36±0.130.027±0.0000.174±0.0010.926±0.001 0.504±0.002
up-DE 4 0.561±0.00115.10±0.030.020±0.0000.214±0.0010.937±0.0000.739±0.0011.357±0.00230.92±0.030.036±0.0000.537±0.0010.824±0.0000.884±0.001
S/32down-DE 2 0.807 ±0.00320.90±0.100.018±0.0010.102±0.0010.962±0.000 0.458±0.0032.106±0.01044.52±0.180.038±0.0010.223±0.0030.900±0.001 0.521±0.002
up-DE 2 0.763 ±0.00119.85±0.040.016±0.0000.305±0.0020.911±0.0000.773±0.0022.004±0.00442.92±0.080.025±0.0000.683±0.0030.767±0.0010.856±0.002
down-DE 4 0.795 ±0.00320.66±0.130.015±0.0010.102±0.0020.962±0.001 0.462±0.0042.076±0.01244.16±0.210.031±0.0000.222±0.0030.900±0.001 0.526±0.003
up-DE 4 0.728±0.00119.06±0.040.025±0.0000.304±0.0010.911±0.0000.808±0.0021.914±0.00341.38±0.050.034±0.0000.682±0.0030.767±0.0010.891±0.002
34Published in Transactions on Machine Learning Research (09/2022)
Table 16: ImageNet performance (means ±standard errors over 3 seeds) of V-MoE and e3for a S/32
backbone, without pre-training.
K NLL↓ Error↓
e3(M= 2) 11.420±0.00723.78±0.22
V-MoE 2 1.478 ±0.00324.45±0.08
K Preliminary ImageNet Results without Pre-training
Training large-scale sparse MoEs on datasets such as ImageNet and in absence of any pre-training is a
diﬃcult task. Indeed, the massive number of parameters causes models to severely overﬁt in that regime. In
practice, we need to combine various regularization techniques to control this overﬁtting.
To obtain preliminary results of e3trained from scratch on ImageNet, we adapt the recipe proposed in the
code released by Riquelme et al. (2021). The recipe is itself based on the regularisation protocol of Steiner
et al. (2021) (“AugReg”), which trained performant dense ViT models from scratch on ImageNet, without
pre-training. Overall, compared to the default training schema used for this paper, we add:
•Mixup (Zhang et al., 2018),
•Weight decay,
•Dropout (Srivastava et al., 2014) on expert MLPs.
The baseline conﬁguration for V-MoE was tuned according to the hyperparameter search space deﬁned
by Steiner et al. (2021), with an extra sweep over expert MLP dropout in {0.1,0.2}. Selecting according to
validationaccuracy, theoptimalV-MoEsettingwas medium2, i.e., mixupratio0.5andRandAugment(Cubuk
et al., 2020) parameters 2 and 15 (2 augmentations applied of magnitude 15), alongside expert dropout 0.2.
Fore3, these regularization-related hyperparameters were kept ﬁxed and were not further tuned. More
precisely, we keep medium2 and tune only the learning rate in {0.001,0.003}; we thus suspect we could
improve our current results for e3by considering a broader sweep of hyperparameters (e.g., re-tuning all the
regularization-related hyperparameters).
In Table 16, we report the performance for both V-MoE and e3with a S/32 backbone. In terms of both
NLL and classiﬁcation error, e3outperforms V-MoE. The model checkpoint, along with our code, can be
found at https://github.com/google-research/vmoe .
Code can be found at https://github.com/google-research/vmoe .
L Additional Experimental Results
In this section we expand on various experiments presented in Sections 3 to 5. In experiments considering
multiple ViT families we also include B/16 which was excluded from the main text for clarity.
L.1 Static versus Adaptive Combination
Here we continue the investigation into static versus adaptive combination from Section 3.
Individual gains with respect to E,KandM.Figure 12 shows the eﬀect of increasing the the various
‘ensemble size’ parameters for a deep ensemble of V-MoEs. In particular, we investigate the static combina-
tion axisM(the number of ensemble members), as well as the two adaptive axes— K(the number of experts
chosen per patch) and E(the total number of experts).
When investigating the eﬀect of K, we ﬁxE= 32and average over M∈{1,..,8}. Similarly, when investi-
gatingM, we ﬁxE= 32and average over K∈{1,..,8}. When investigating the eﬀect of Ewe ﬁxK= 2
35Published in Transactions on Machine Learning Research (09/2022)
and average over M∈{1,..,8}. As a result of this procedure the exact values of the curves are not directly
comparable. However, we can still examine the relative trends.
Speciﬁcally, we note that while the variation in KandMcurves is roughly of the same size, the variation
in theEcurve is smaller. We also note that there is very little variation beyond E= 8(note the diﬀerence
in the scales of the axes for the curves). These observations motivate the design of e3, where we split the
sub-models along the Eaxis, in order to better take advantage of the experts.
1 2 3 4 5 6 7 8
K/M0.760.780.800.820.84NLLNLL vs. 'ensemble size'
K
M
E4 8 16 32E
Figure 12: Comparison for the impact on ImageNet NLL of variations in K,EandM. The underlying
model is ViT-S/32.
Extended Results for the Cumulative Eﬀects of Static and Adaptive Combination. In Figure 13
we extend the ImageNet NLL results, presented in Figure 3, to a range of other datasets and performance
metrics. We see that in most cases, the trends are the same as before. That is, (static) ensembling tends
to improve the performance of ViT and V-MoE equally. The two exceptions are ECE and OOD detection
for ViT-S/32 where we see that larger ensemble sizes can result in decreased performance. These results
indicatethattheforsmallViTmodels, largerensemblescanhaveslightlylowerqualityuncertaintyestimates.
The trend for ImageNet-C performance is also not as consistent with ensembling sometimes helping ViT or
V-MoE less (as indicated by the changes in ordering on the y-axis).
L.2 An Additional Motivating Experiment – Deep Ensembles of V-MoEs with Fewer Experts
As an additional motivation for combining sparse MoEs and ensembles, Table 17 compares the performance
of a V-MoE with E= 32total experts and ensembles of V-MoEs with ( M= 2,E= 16) and (M= 4,
E= 8), for both K= 1andK= 2. We see that, in terms of NLL, ( M= 4,E= 8) is better than ( M= 2,
E= 16) which is in turn better than ( M= 1,E= 32). We see similar results for Error and ECE.
We note that this result is especially remarkable since the individual upstream (and later, down-
stream) models are such that E= 8is worse than E= 16which in turn performs worse than E= 32;
ensembling thus manages to counterbalance the poorer individual model performance. This suggests that
the eﬃcient ensembling—i.e., the combination of multiple models within a single model—of sparse MoEs
could lead to strong performance while reducing computational costs.
L.3 Extended Results for the Tiling with Increasing Parameter Sharing Ablation
In Table 18, we extend the results for our parameter sharing ablation in Section 4.2.3, from K= 2toK= 1.
We see that the results remain the same in this case.
36Published in Transactions on Machine Learning Research (09/2022)
Table 17: Comparison of upstream deep ensembles of V-MoE-B/32 models with fewer experts.
K M E NLL↓ Error↓ECE↓ KL↑
V-MoE11 32 0.642 ±0.00216.90±0.050.029±0.001–
2 16 0.588 15.97 0.015 0.211
4 8 0.577 15.82 0.017 0.228
21 32 0.638 ±0.00116.76±0.050.033±0.001–
2 16 0.583 15.75 0.016 0.211
4 8 0.580 15.94 0.015 0.184
Table 18: Extension of Table 3, showing the impact of parameter sharing in e3, forK= 1.
Overlap NLL ↓ Error↓ ECE↓ KL↑
0 (=e3)0.622±0.00116.70±0.030.018±0.0000.217±0.003
2 0.627 ±0.00316.83±0.070.022±0.0010.194±0.005
4 0.634 ±0.00216.92±0.070.024±0.0010.178±0.004
8 0.642 ±0.00117.04±0.100.028±0.0010.151±0.009
16 0.659 ±0.00417.28±0.120.036±0.0010.103±0.009
L.4 Extended Results for Few-shot Learning
InFigure14,weextendthefew-shotlearningresultsofFigure4toalsoinclude1,5,and25-shot. Additionally,
we show results for the weighted aggregation strategy mentioned in Appendix A.5.
We conﬁrm the result that few-shot performance for e3gets better, relative to the other baselines, with
larger ViT families. Additionally, we see that e3performance seems to get better, again relative to the other
baselines, with more shots. This phenomenon can most easily be noticed by comparing the results for S/32
across the diﬀerent numbers of shots. Finally, we see that the trends with and without the weighted mean
are the same.
L.5 Extended Results for OOD Detection
Here we extended the OOD results of Figure 6. Speciﬁcally, we add CIFAR100 as an in-distribution dataset
and Describable Textures Dataset (DTD) (Cimpoi et al., 2014) as an OOD dataset. We also add area under
the receiver operating characteristic (AUC (ROC)) and area under the precision-recall curve (AUC (PR)) as
metrics. Figures 15 and 16 contain the results with CIFAR10 and CIFAR100 as the in-distribution datasets,
respectively.
As in Figure 6, we see that e3performs better (relative to the other baselines) for larger ViT families.
Furthermore, e3seems to perform better in near OOD detection (i.e. CIFAR10 versus CIFAR100, and vice
versa) than far OOD detection. Finally, we see that these trends are consistent across OOD metrics.
L.6 Extended Results for ImageNet
In this section we extend the results for ImageNet and the corrupted variants presented in Figures 4, 5 and 7.
In addition to NLL, classiﬁcation error, ECE (for standard ImageNet), and Brier score, Figure 17 provides
classiﬁcation error and ECE for all ImageNet variants.
Most of the trends observed in Section 5 remain true:
•e3tends to be Pareto eﬃcient in the presence of distribution shift.
•For smaller ViT families, V-MoE outperforms ViT in the presence of distribution shift.
37Published in Transactions on Machine Learning Research (09/2022)
•e3improves ECE over ViT and V-MoE.
•e3improves classiﬁcation performance.
•ViT consistently provides better ECE than V-MoE.
However, there are some exceptions:
•ImageNet-A classiﬁcation error. All models (including e3) under-perform relative to ViT-S/32
and ViT-H/14.
•ECE for ImageNet-C, ImageNet-A, and ImageNet-V2. Interestingly for the non-standard
ImageNet variants, and in particular for ImageNet-A, there is a strong correlation between lower
ECE and larger ViT families.
We also ﬁnd that the results for classiﬁcation error and Brier score follow those for NLL closely.
L.7 Additional CIFAR10, CIFAR100, Flowers, and Pets Results
Here we extend the results for ImageNet and the corrupted variants presented in Figures 4, 5 and 7 to four
additional datasets. Figures 18, 19, 20 and 21 provide results for CIFAR10, CIFAR100, Oxford Flowers
102, and Oxford IIIT Pet, respectively. As in Appendix L.6, we ﬁnd that the results are similar to those in
Section 5.
Compared to ImageNet, for CIFAR10, CIFAR10-C, and CIFAR100, e3seems to perform even better relative
to the other baselines. Note, for example, that e3is Pareto eﬃcient (even for S/32) in the cases of CIFAR10-
C and CIFAR100 NLL. As in Appendix L.6, we see that the ECE has a stronger downward trend with
respect to increased ViT family size for shifted test data.
For Flowers and Pets, where we only have results for smaller ViT families, e3seems to under perform.
However, the performance for L/32 is better than for S/32 and B/32 which suggests that the results for
these datasets are consistent with the other datasets presented in this work and, therefore, that we should
expect e3’s predictive performance to keep improving with larger models.
L.8 Eﬃcient ensemble of experts and V-MoE with larger values of KandM
Figure 22 and Figure 23 show the eﬀect of varying Kone3and V-MoE, and the eﬀect of varying Mone3,
respectively. We make the following observations:
•In almost all cases, increasing KorMdoes not result in Pareto eﬃcient models.
•For V-MoE, increasing Kseems to help in most cases, except for ECE performance where it usually
hurts.
•Fore3, going from K= 1toK= 2seems to help in most cases but going from K= 2toK= 4
usually hurts. Going from K= 1toK= 4still helps but to a lesser extent than from K= 1to
K= 2.
•Fore3, increasing Meither doesn’t make a consistent and signiﬁcant diﬀerence or hurts (e.g. in
OOD detection).
These conclusions should, however, be considered with caution. Recall that the upstream checkpoints used
for ﬁne-tuning all V-MoE and e3models in this work are V-MoE models with K= 2. Thus, the results in
this experiment are confounded by upstream and downstream checkpoint mismatch for all e3models and all
V-MoE models with K/negationslash= 2. This phenomenon was observed in Riquelme et al. (2021) for V-MoE models.
I.e., performance of downstream V-MoE models with mismatched values of Kwere relatively worse than
those with matched values of K; see their Appendix E.4 (Figures 33-35). We also hypothesise that it is more
38Published in Transactions on Machine Learning Research (09/2022)
diﬃcult to train downstream e3models with larger values of Mfrom upstream V-MoE models because in
each subset of the experts some common expert specialisations will need to be duplicated. Our ensemble
members are ﬁne-tuned from an upstream V-MoE with a predeﬁned total number of experts ( E= 32)
meaning that increasing Mdecreases the number of experts available to each ensemble member (with E/M
experts per member). This could also impact the performance of e3with larger sizes of M.
L.8.1 Upstream vs. Downstream Mismatch
Table 19: ImageNet performance of e3models ﬁne-tuned from V-MoE-B/32 checkpoints with K= 2or
K= 4, andE= 32.Kupstream = 2results are averaged over 8 random seeds, while Kupstream = 4results are
averaged over 3 seeds.
K M K upstream NLL↓ Error↓
e31 220.622±0.00116.70±0.03
40.622±0.00116.81±0.05
1 42 0.624 ±0.00116.99±0.03
40.622±0.00016.93±0.01
Here we investigate whether upstream versus downstream mismatch partially explains the counter-intuitive
result that increasing Mcan result in worse performance for e3. We train downstream e3models with
(K= 1,M= 2) and (K= 1,M= 4) from upstream V-MoE checkpoints with K= 2andK= 4. Table 19
shows the results. We see that ( K= 1,M= 2) does not seem to beneﬁt from increasing Kupstreamfrom
2 to 4, despite the fact that the upstream K= 4model is better than the upstream K= 2model (NLL
upstream is 8.18 for K= 4and 8.27 for K= 2). On the other hand, ( K= 1,M= 4) does beneﬁt from
having K=4 upstream.
This conﬁrms that the mismatch between upstream and downstream models is one of the factors explaining
the results of e3for growing M. However, we also see that the best performing model is ( Kupstream = 2,
K= 1,M= 2), which suggests that there are other confounding factors, such as those mentioned above.
39Published in Transactions on Machine Learning Research (09/2022)
0.40.50.60.70.80.9ImageNetNLL (lower is better)
10.012.515.017.520.022.525.0Error (lower is better)
0.0050.0100.0150.0200.0250.0300.035ECE (lower is better)
1.01.52.0ImageNet-C (average)NLL (lower is better)
20253035404550Error (lower is better)
0.020.030.040.050.06ECE (lower is better)
1520253035Mean Across Datasets5-Shot Error (lower is better)
1520253010-Shot Error (lower is better)
10.012.515.017.520.022.525.025-Shot Error (lower is better)
2 3 40.050.100.150.200.25CIFAR10 vs. CIFAR100FPR@95 (lower is better)
2 3 40.960.970.980.99AUC (ROC) (higher is better)
2 3 40.9600.9650.9700.9750.9800.9850.990AUC (PR) (higher is better)
downstream log(GFLOPs) (lower is better)
S/32
V-MoEB/32
ViTL/32
1 MemberB/16
2 MembersL/16
4 MembersH/14
Figure 13: Extended results for Figure 3 to a selection of other tasks and metrics. We see that in most cases,
ensembles tend to help ViT and V-MoE ( K= 1) equally.
40Published in Transactions on Machine Learning Research (09/2022)
303540455055Mean Across Datasets1-Shot Error (lower is better)
202530355-Shot Error (lower is better)
1520253010-Shot Error (lower is better)
10.012.515.017.520.022.525.025-Shot Error (lower is better)
2 3 47880828486Weighted Mean Across Datasets
2 3 48182838485
2 3 48182838485
2 3 4828384
downstream log(GFLOPs) (lower is better)
S/32
E3B/32
V-MoEL/32
ViTB/16
1 MemberL/16
2 MembersH/14
Figure 14: Extended few-shot results from Figure 4 with an additional aggregation method and numbers of
shots.
41Published in Transactions on Machine Learning Research (09/2022)
0.050.100.150.200.25CIFAR10 vs. CIFAR100FPR@95 (lower is better)
0.960.970.980.99AUC (ROC) (higher is better)
0.9600.9650.9700.9750.9800.9850.990AUC (PR) (higher is better)
0.0000.0050.0100.0150.020CIFAR10 vs. DTD
0.9920.9940.9960.9981.000
0.998500.998750.999000.999250.999500.999751.00000
0.020.040.060.080.100.120.14CIFAR10 vs. Places3650.9750.9800.9850.9900.995
0.650.700.750.800.850.900.95
2 30.000.010.020.030.04CIFAR10 vs. SVHN
2 30.9880.9900.9920.9940.9960.998
2 30.9800.9850.9900.995
downstream log(GFLOPs) (lower is better)
S/32
E3B/32
V-MoEL/32
ViTB/16
1 MemberL/16
2 Members
Figure 15: Extended OOD detection the results from Figure 6 with an additional OOD dataset and more
metrics.
42Published in Transactions on Machine Learning Research (09/2022)
0.30.40.50.6CIFAR100 vs. CIFAR10FPR@95 (lower is better)
0.860.880.900.920.940.96AUC (ROC) (higher is better)
0.860.880.900.920.940.96AUC (PR) (higher is better)
0.050.100.150.200.250.30CIFAR100 vs. DTD
0.930.940.950.960.970.980.99
0.98250.98500.98750.99000.99250.99500.9975
0.20.30.40.50.6CIFAR100 vs. Places365
0.8250.8500.8750.9000.9250.9500.975
0.30.40.50.60.70.8
2 30.300.350.400.450.50CIFAR100 vs. SVHN
2 30.890.900.910.920.930.940.95
2 30.800.820.840.860.880.90
downstream log(GFLOPs) (lower is better)
S/32
E3B/32
V-MoEL/32
ViTB/16
1 MemberL/16
2 Members
Figure 16: Extended OOD detection the results from Figure 6 with CIFAR100 as the in-distribution dataset,
an additional OOD dataset, and more metrics.
43Published in Transactions on Machine Learning Research (09/2022)
0.40.50.60.70.80.9ImageNetNLL (lower is better)
1012141618202224Error (lower is better)
0.0100.0150.0200.0250.0300.0350.040ECE (lower is better)
0.1500.1750.2000.2250.2500.2750.3000.325Brier (lower is better)
0.751.001.251.501.752.002.25ImageNet-C (average)
20253035404550
0.020.030.040.050.060.07
0.300.350.400.450.500.550.60
234567ImageNet-A
5060708090100
0.100.150.200.250.300.350.40
0.60.70.80.91.01.11.2
2 3 40.81.01.21.41.6ImageNet-V2
2 3 420253035
2 3 40.030.040.050.060.070.080.09
2 3 40.300.350.400.450.50
downstream log(GFLOPs) (lower is better)
S/32
E3B/32
V-MoEL/32
ViTB/16
1 MemberL/16
2 MembersH/14
Figure 17: Extended results from Figures 4, 5 and 7 with additional metrics.
44Published in Transactions on Machine Learning Research (09/2022)
0.020.030.040.050.060.07CIFAR10NLL (lower is better)
0.51.01.52.0Error (lower is better)
0.0030.0040.0050.0060.0070.008ECE (lower is better)
0.0100.0150.0200.0250.030Brier (lower is better)
2 30.20.30.40.5CIFAR10-C
2 36810121416
2 30.030.040.050.060.070.08
2 30.100.150.20
downstream log(GFLOPs) (lower is better)
S/32
E3B/32
V-MoEL/32
ViTB/16
1 MemberL/16
2 Members
Figure 18: Results for CIFAR10 and CIFAR10-C.
2 30.200.250.300.350.400.45CIFAR100NLL (lower is better)
2 3681012Error (lower is better)
2 30.010.020.030.04ECE (lower is better)
2 30.080.100.120.140.160.18Brier (lower is better)
downstream log(GFLOPs) (lower is better)
S/32
E3B/32
V-MoEL/32
ViTB/16
1 MemberL/16
2 Members
Figure 19: Results for CIFAR100.
45Published in Transactions on Machine Learning Research (09/2022)
1.5 2.0 2.50.050.100.150.20Oxford FlowersNLL (lower is better)
1.5 2.0 2.5012345Error (lower is better)
1.5 2.0 2.50.000.010.020.030.04ECE (lower is better)
1.5 2.0 2.50.020.040.060.08Brier (lower is better)
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 1 Member 2 Members
Figure 20: Results for Oxford Flowers 102.
1.5 2.0 2.50.150.200.250.30Oxford PetsNLL (lower is better)
1.5 2.0 2.5456789Error (lower is better)
1.5 2.0 2.50.010.020.030.04ECE (lower is better)
1.5 2.0 2.50.060.080.100.120.14Brier (lower is better)
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 1 Member 2 Members
Figure 21: Results for Oxford IIIT Pet.
46Published in Transactions on Machine Learning Research (09/2022)
0.50.60.70.80.9ImageNetNLL (lower is better)
141618202224Error (lower is better)
0.0100.0150.0200.0250.0300.0350.040ECE (lower is better)
1.251.501.752.002.25ImageNet-C (average)NLL (lower is better)
3035404550Error (lower is better)
0.020.030.040.050.060.07ECE (lower is better)
20253035Mean Across Datasets5-Shot Error (lower is better)
17.520.022.525.027.530.010-Shot Error (lower is better)
14161820222425-Shot Error (lower is better)
1.5 2.0 2.50.050.100.150.200.25CIFAR10 vs. CIFAR100FPR@95 (lower is better)
1.5 2.0 2.50.960.970.980.99AUC (ROC) (higher is better)
1.5 2.0 2.50.9600.9650.9700.9750.9800.9850.990AUC (PR) (higher is better)
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 1 Member 2 Members
Figure 22: Results for V-MoE with K∈{1,2,4,8}ande3withK∈{1,2,4}. Models with larger values of
Khave larger FLOPs.
47Published in Transactions on Machine Learning Research (09/2022)
0.50.60.70.80.9ImageNetNLL (lower is better)
141618202224Error (lower is better)
0.0100.0150.0200.0250.0300.035ECE (lower is better)
1.21.41.61.82.02.22.4ImageNet-C (average)NLL (lower is better)
3035404550Error (lower is better)
0.020.030.040.050.06ECE (lower is better)
253035Mean Across Datasets5-Shot Error (lower is better)
17.520.022.525.027.530.010-Shot Error (lower is better)
14161820222425-Shot Error (lower is better)
1.5 2.0 2.50.050.100.150.200.250.30CIFAR10 vs. CIFAR100FPR@95 (lower is better)
1.5 2.0 2.50.950.960.970.980.99AUC (ROC) (higher is better)
1.5 2.0 2.50.960.970.980.99AUC (PR) (higher is better)
downstream log(GFLOPs) (lower is better)
E3V-MoE ViT S/32 B/32 L/32 1 Member 2 Members 4 Members
Figure 23: Results for e3withM= 4andK∈{1,2}.
48Published in Transactions on Machine Learning Research (09/2022)
M Results Tables
M.1 FLOPs Numbers
Table 20 provides the downstream training FLOPs for various e3, V-MoE, and ViT conﬁgurations. These
numbers correspond to the x-values of the points in the ﬁgures presented in Section 5 and Appendix L.
Table 21 provides the percentage diﬀerence in FLOPs between the e3, V-MoE and down-DE models most
commonly used in this work. Note that the percentage diﬀerences for H/14 do not follow the trend of the
other sizes, e.g. that the percentage diﬀerence between e3and V-MoE gets smaller for larger sizes, due to the
fact that for H/15 we use a last-5 conﬁguration rather than the last-2 conﬁguration used for the other ViT
families. Table 22 provides the downstream training FLOPs for the various ablation study models presented
in Section 4.2.2
Table 20: Downstream training GFLOPs for the various e3, V-MoE, and ViT baselines used in this work.
K M S/32 B/32 B/16 L/32 L/16 H/14
e31 2 36.69 105.89 452.62 320.92 1356.46 4183.28
1 4 58.03 152.98 — 403.77 — —
2 2 47.98 131.06 552.65 365.42 1533.74 —
2 4 80.66 203.31 — 492.77 — —
4 2 70.61 181.40 — 454.43 — —
ViT- 1 24.01 77.97 334.48 271.83 1151.71 3033.60
- 2 48.02 155.93 668.95 543.66 2303.43 6067.21
- 4 96.03 311.87 1337.90 1087.33 4606.85 12134.41
V-MoE1 1 26.02 82.35 351.91 279.55 1182.08 3179.90
1 2 52.04 164.70 703.81 559.10 2364.16 6359.81
1 4 104.07 329.41 1407.63 1118.21 4728.32 12719.61
2 1 31.66 94.93 401.98 301.75 1270.78 3617.94
4 1 42.95 120.11 501.99 346.25 1448.06 —
8 1 65.59 170.44 — 435.26 — —
Table 21: Percentage diﬀerence in downstream training FLOPs for e3with (K,M ) = (1,2)compared with
V-MoE with K= 1and an ensemble of two such V-MoE members.
S/32 B/32 B/16 L/32 L/16 H/14
e3vs. V-MoE 41.01 28.58 28.62 14.80 14.75 31.55
e3vs. down-DE -29.49 -35.71 -35.69 -42.60 -42.62 -34.22
M.2 Parameter Counts
Table 23 compares the parameter counts for ViT and V-MoE/ e3models in each ViT family.
M.3 Summary for NLL under Distribution Shift
Table 24 shows the percentage improvement for e3versus V-MoE in NLL (i.e.,NLL V-MoE−NLLe3
NLL V-MoE×100, with
positive values thus indicating that e3improves upon V-MoE) averaged over ImageNet-C, ImageNet-A, and
ImageNet-V2, for a given ViT family and ( K,M) in {(1, 2), (2, 2)} (compared to V-MoE with K= 2and
2Note that the e3and V-MoE results here are diﬀerent to those in Table 20 due to a diﬀerence in implementation. We used
a simpliﬁed, but less computationally eﬃcient expert-routing implementation for all of the ablation studies. As a result the
V-MoE and e3FLOPs in Table 20 are lower and cannot be fairly compared with the ablation models presented here. We thus
re-benchmarked e3and V-MoE to obtain comparable results.
49Published in Transactions on Machine Learning Research (09/2022)
Table 22: Downstream training GFLOPs comparison for the ablation study models in Section 4.2.
K M GFLOPs
V-MoE 2 — 96.895
V-MoE 4 — 123.644
V-MoE 8 — 178.133
e31 2 109.781
e32 2 138.457
e34 2 196.870
Tiling 2 2 138.460
Partitioning 2 — 97.885
Overlap = 2 2 2 138.457
Overlap = 4 2 2 138.458
Overlap = 8 2 2 138.459
Overlap = 6 2 2 138.460
Multi-pred 2 — 96.330
Multi-pred 4 — 122.526
Multi-pred 8 — 175.889
Table 23: Parameter counts for ViT vs V-MoE and e3.
S/32 B/32 B/16 L/32 L16 H/14
ViT 36.5M 102.1M 100.5M 325.3M 323.1M 655.8M
V-MoE/ e3166.7M 395.0M 393.3M 845.8M 843.6M 2688.6M
K= 4, respectively). We see that for all ViT families except S/32, e3outperforms V-MoE. This result is
not unexpected, since ensembles tend to provide improved performance under distribution shift relative to
single models (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Havasi et al., 2020).
Table24: Percentageimprovementfor e3vsV-MoEinNLLunderdistributionshift, averagedoverImageNet-
C, ImageNet-A, and ImageNet-V2.
S/32 B/32 B/16 L/32 L/16 H/14
e3(K= 1,M= 2) vs. V-MoE ( K= 2) -1.15 0.52 0.31 5.34 2.78 8.33
e3(K= 2,M= 2) vs. V-MoE ( K= 4) -0.21 3.91 1.33 7.37 2.27 —
M.4 Auxiliary Tables with Standard Errors.
The tables in this section provide the mean values and corresponding standard errors for many of the results
depicted in ﬁgures throughout Section 5 and Appendix L.
50Published in Transactions on Machine Learning Research (09/2022)
Table 25: ImageNet comparison of V-MoE, downstream ensembles there-of, and e3with 2 experts per input in each case.
ImageNet ImageNet-C (average) ImageNet-A ImageNet-V2 Downstream
K M NLL ↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ ∆FLOPs (%)↓
H/14e31 2 0.408 ±0.00111.63±0.050.012±0.0000.865±0.01021.46±0.160.018±0.0002.276±0.04249.09±0.780.101±0.0030.745±0.00119.50±0.080.035±0.001 15.63
down-DE 1 2 0.403±0.00011.35±0.050.018±0.0010.871±0.01221.37±0.200.021±0.0012.273±0.04947.93±0.930.108±0.0010.758±0.00319.28±0.170.044±0.001 75.79
V-MoE 2 1 0.428 ±0.00311.89±0.130.030±0.0010.934±0.01322.41±0.200.038±0.0012.517±0.06350.34±1.120.167±0.0030.811±0.00520.25±0.130.065±0.001 —
L/16e31 20.448±0.00112.60±0.030.020±0.0001.023±0.00524.89±0.110.024±0.0002.836±0.01557.97±0.210.160±0.0010.838±0.00221.30±0.070.051±0.001 6.74
down-DE 1 2 0.450±0.00212.62±0.040.016±0.0001.010±0.00624.43±0.120.021±0.0002.796±0.01657.06±0.320.136±0.0000.818±0.00321.06±0.110.044±0.001 86.04
V-MoE 2 1 0.464 ±0.00112.88±0.040.025±0.0001.058±0.00425.27±0.080.034±0.0002.945±0.01657.85±0.280.178±0.0010.848±0.00221.33±0.030.057±0.001 —
L/32e31 20.535±0.00114.70±0.030.027±0.0001.193±0.00328.28±0.050.032±0.0004.170±0.01074.73±0.090.266±0.0020.989±0.00224.62±0.080.063±0.001 6.35
down-DE 1 2 0.533±0.00214.55±0.040.025±0.0011.184±0.00327.98±0.040.029±0.0004.139±0.01774.29±0.210.254±0.0030.982±0.00224.42±0.080.061±0.002 85.29
V-MoE 2 1 0.563 ±0.00115.05±0.030.039±0.0001.261±0.00529.12±0.070.052±0.0004.394±0.01475.39±0.170.301±0.0021.046±0.00225.22±0.060.083±0.001 —
B/16e31 20.519±0.00114.26±0.030.020±0.0001.352±0.00531.20±0.090.022±0.0003.835±0.01972.25±0.220.223±0.0020.974±0.00224.10±0.100.051±0.001 12.60
down-DE 1 2 0.519±0.00214.09±0.020.021±0.0011.316±0.00830.02±0.180.030±0.0003.618±0.01366.99±0.280.203±0.0020.945±0.00323.39±0.090.054±0.001 75.09
V-MoE 2 1 0.533 ±0.00114.60±0.040.022±0.0001.372±0.00531.27±0.110.034±0.0003.875±0.01670.79±0.300.235±0.0020.959±0.00324.09±0.110.056±0.001 —
B/32e31 20.622±0.00116.70±0.030.018±0.0001.532±0.00534.73±0.080.022±0.0005.080±0.01384.91±0.090.301±0.0011.143±0.00227.97±0.100.055±0.001 11.54
down-DE 1 2 0.620±0.00116.44±0.040.023±0.0001.510±0.00533.79±0.080.032±0.0004.891±0.01481.98±0.180.284±0.0021.116±0.00227.24±0.080.061±0.000 73.49
V-MoE 2 1 0.638 ±0.00116.76±0.050.033±0.0011.562±0.00434.40±0.060.050±0.0015.032±0.01481.71±0.110.317±0.0021.150±0.00227.50±0.080.076±0.001 —
S/32e31 20.818±0.00221.18±0.040.015±0.0002.169±0.00845.79±0.100.030±0.0006.419±0.01193.98±0.090.345±0.0011.437±0.00234.03±0.050.054±0.001 15.87
down-DE 1 2 0.807±0.00320.90±0.100.018±0.0012.106±0.01044.52±0.180.038±0.0016.063±0.00792.33±0.070.335±0.0011.393±0.00333.29±0.100.062±0.002 64.34
V-MoE 2 1 0.829 ±0.00221.09±0.080.035±0.0012.162±0.00744.95±0.120.066±0.0016.227±0.01592.09±0.120.373±0.0011.437±0.00233.42±0.080.086±0.001 —
Table 26: ImageNet comparison of V-MoE, downstream ensembles there-of, and e3with 4 experts per input in each case.
ImageNet ImageNet-C (average) ImageNet-A ImageNet-V2 Downstream
K M NLL ↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ ∆FLOPs (%)↓
L/16e32 2 0.451 ±0.00112.61±0.040.023±0.0001.028±0.00324.90±0.070.028±0.0002.886±0.01658.26±0.240.171±0.0020.849±0.00221.40±0.110.055±0.001 5.92
down-DE 1 4 0.440±0.00212.39±0.060.015±0.0000.983±0.00623.95±0.120.020±0.0002.712±0.01556.36±0.360.120±0.0020.803±0.00220.83±0.100.039±0.001 226.53
V-MoE 4 1 0.465 ±0.00112.86±0.050.026±0.0001.060±0.00525.28±0.090.035±0.0002.976±0.02258.26±0.290.186±0.0020.856±0.00221.53±0.070.060±0.001 —
L/32e32 2 0.529 ±0.00114.63±0.020.019±0.0001.188±0.00428.23±0.070.025±0.0004.095±0.01574.38±0.140.253±0.0020.962±0.00124.41±0.040.055±0.001 5.54
down-DE 1 4 0.518±0.00214.29±0.030.022±0.0001.154±0.00427.47±0.050.023±0.0004.033±0.01673.79±0.130.237±0.0020.959±0.00224.03±0.040.054±0.001 222.95
V-MoE 4 1 0.566 ±0.00215.08±0.040.041±0.0001.267±0.00329.19±0.050.053±0.0004.428±0.01775.52±0.060.306±0.0011.050±0.00325.21±0.050.084±0.000 —
B/16e32 20.510±0.00114.13±0.020.016±0.0001.328±0.00630.81±0.110.020±0.0003.743±0.01171.12±0.160.211±0.0020.945±0.00223.75±0.030.044±0.001 10.09
down-DE 1 4 0.511±0.00213.95±0.010.019±0.0011.293±0.00829.67±0.180.026±0.0003.544±0.01566.49±0.270.190±0.0020.930±0.00323.17±0.060.050±0.001 180.41
V-MoE 4 1 0.532 ±0.00214.21±0.040.029±0.0011.350±0.00530.44±0.110.046±0.0013.726±0.01466.88±0.150.238±0.0020.973±0.00323.69±0.100.069±0.001 —
B/32e32 2 0.612 ±0.00116.49±0.020.013±0.0001.491±0.00333.85±0.050.019±0.0004.872±0.00782.80±0.080.275±0.0011.099±0.00327.36±0.100.045±0.001 9.12
down-DE 1 4 0.607±0.00016.17±0.020.021±0.0011.483±0.00833.36±0.130.027±0.0004.787±0.01182.11±0.070.276±0.0001.099±0.00326.98±0.010.055±0.002 174.26
V-MoE 4 1 0.636 ±0.00116.70±0.040.034±0.0011.555±0.00334.33±0.050.051±0.0015.031±0.01381.62±0.130.322±0.0021.150±0.00327.49±0.100.079±0.002 —
S/32e32 2 0.805 ±0.00220.97±0.050.013±0.0002.112±0.00745.05±0.090.028±0.0006.283±0.01393.68±0.090.342±0.0011.408±0.00333.71±0.100.051±0.001 11.70
down-DE 1 4 0.795±0.00320.66±0.130.015±0.0012.076±0.01244.16±0.210.031±0.0005.990±0.01192.25±0.030.324±0.0011.372±0.00232.83±0.130.054±0.002 142.29
V-MoE 4 1 0.820 ±0.00220.97±0.070.030±0.0012.133±0.00644.53±0.120.060±0.0016.142±0.01491.62±0.120.365±0.0021.417±0.00333.30±0.120.081±0.001 —
51Published in Transactions on Machine Learning Research (09/2022)
Table 27: ImageNet comparison of V-MoE and ViT.
ImageNet ImageNet-C ImageNet-A ImageNet-V2
K M NLL ↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓
H/14V-MoE 1 1 0.426±0.00211.81±0.080.026±0.0000.932±0.00922.43±0.160.033±0.0002.494±0.05750.22±1.020.153±0.0040.806±0.00520.08±0.130.059±0.001
ViT - 1 0.426±0.00111.99±0.050.023±0.0000.929±0.00422.45±0.060.031±0.0002.332±0.01846.99±0.210.149±0.0020.788±0.00420.08±0.030.058±0.001
L/16V-MoE 1 1 0.464±0.00112.91±0.040.022±0.0001.050±0.00425.19±0.080.030±0.0002.914±0.01658.02±0.330.167±0.0010.844±0.00321.42±0.060.053±0.001
ViT - 1 0.473 ±0.00413.37±0.080.020±0.0001.114±0.00926.68±0.170.032±0.0013.094±0.04260.92±0.560.203±0.0040.864±0.00722.04±0.170.055±0.001
L/32V-MoE 1 1 0.559±0.00115.10±0.030.036±0.0011.246±0.00429.02±0.070.046±0.0004.320±0.01375.25±0.110.288±0.0021.032±0.00325.13±0.090.077±0.001
ViT - 1 0.556±0.00215.38±0.050.025±0.0001.255±0.00929.57±0.160.038±0.0004.286±0.02175.32±0.300.287±0.0021.002±0.00425.32±0.090.067±0.001
B/16V-MoE 1 1 0.533±0.00114.33±0.040.026±0.0001.355±0.00530.59±0.100.040±0.0003.727±0.01467.72±0.160.222±0.0010.965±0.00323.63±0.110.060±0.001
ViT - 1 0.565 ±0.00315.70±0.060.021±0.0011.515±0.00634.52±0.100.042±0.0014.219±0.03275.85±0.290.280±0.0031.020±0.00625.77±0.120.061±0.001
B/32V-MoE 1 1 0.642±0.00216.90±0.050.029±0.0011.568±0.00334.68±0.050.045±0.0015.039±0.00982.49±0.130.307±0.0021.151±0.00327.83±0.100.071±0.001
ViT - 1 0.688 ±0.00318.65±0.080.022±0.0001.689±0.00538.02±0.090.045±0.0005.358±0.01487.00±0.120.342±0.0011.209±0.00529.89±0.100.067±0.001
S/32V-MoE 1 1 0.834±0.00221.43±0.080.029±0.0012.171±0.00545.44±0.090.056±0.0016.199±0.01292.47±0.110.355±0.0011.433±0.00233.84±0.020.078±0.001
ViT - 1 0.907 ±0.00323.69±0.050.016±0.0002.309±0.01048.75±0.130.055±0.0016.639±0.01494.66±0.070.375±0.0011.529±0.00336.42±0.080.066±0.001
52Published in Transactions on Machine Learning Research (09/2022)
Table 28: CIFAR10 comparison of V-MoE, downstream ensembles there-of, and e3with 2 experts per input
in each case.
Cifar10 Cifar10-C Downstream
K M NLL ↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ ∆FLOPs (%)↓
L/16E31 20.022±0.0000.55±0.020.003±0.0000.198±0.0125.65±0.200.027±0.002 6.74
down-DE 1 2 0.026 ±0.0010.59±0.040.004±0.0000.227±0.0116.18±0.210.032±0.003 86.04
V-MoE 2 1 0.028 ±0.0000.61±0.010.004±0.0000.220±0.0126.17±0.230.032±0.003 —
L/32E31 20.026±0.0010.69±0.030.003±0.0000.246±0.0087.01±0.150.033±0.002 6.35
down-DE 1 2 0.034 ±0.0010.81±0.020.005±0.0000.252±0.0137.00±0.260.032±0.004 85.29
V-MoE 2 1 0.042 ±0.0010.90±0.030.006±0.0000.325±0.0118.05±0.170.048±0.002 —
B/16E31 2 0.036 ±0.0010.84±0.020.005±0.0000.295±0.0088.28±0.150.038±0.002 12.60
down-DE 1 2 0.030±0.0010.81±0.040.003±0.0000.247±0.0057.60±0.160.028±0.001 75.09
V-MoE 2 1 0.040 ±0.0010.97±0.020.006±0.0000.303±0.0088.08±0.090.047±0.002 —
B/32E31 2 0.041 ±0.0011.11±0.030.004±0.0000.340±0.01010.01±0.180.040±0.003 11.54
down-DE 1 2 0.036±0.0001.07±0.030.003±0.0000.345±0.00610.87±0.130.042±0.002 73.49
V-MoE 2 1 0.042 ±0.0011.11±0.030.006±0.0000.405±0.01511.30±0.240.063±0.004 —
S/32E31 2 0.060 ±0.0011.92±0.030.003±0.0000.476±0.00715.30±0.170.049±0.002 15.86
down-DE 1 2 0.056±0.0011.75±0.030.005±0.0000.491±0.00415.42±0.150.064±0.001 64.34
V-MoE 2 1 0.058±0.0011.80±0.030.007±0.0000.514±0.00615.48±0.210.076±0.002 —
Table 29: CIFAR10 comparison of V-MoE, downstream ensembles there-of, and e3with 4 experts per input
in each case.
Cifar10 Cifar10-C Downstream
K M NLL ↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓ ∆FLOPs (%)↓
L/16E32 20.026±0.0010.58±0.020.003±0.0000.247±0.0166.56±0.260.036±0.003 5.92
down-DE 1 4 0.025±0.0010.57±0.030.004±0.0000.219±0.0106.12±0.190.030±0.003 226.53
V-MoE 4 1 0.034 ±0.0010.75±0.020.005±0.0000.278±0.0107.71±0.190.043±0.002 —
L/32E32 20.033±0.0010.80±0.020.004±0.0000.242±0.0066.65±0.120.031±0.002 5.54
down-DE 1 4 0.030±0.0010.78±0.020.004±0.0000.232±0.0126.74±0.270.028±0.003 222.94
V-MoE 4 1 0.035 ±0.0010.81±0.020.005±0.0000.295±0.0097.63±0.140.044±0.002 —
B/16E32 2 0.034 ±0.0000.81±0.020.004±0.0000.283±0.0068.06±0.120.037±0.002 10.09
down-DE 1 4 0.029±0.0000.80±0.030.003±0.0000.242±0.0067.54±0.160.026±0.001 180.41
V-MoE 4 1 0.032±0.0010.79±0.020.005±0.0000.266±0.0047.55±0.100.040±0.001 —
B/32E32 20.034±0.0011.00±0.020.003±0.0000.306±0.0099.44±0.230.037±0.002 9.12
down-DE 1 4 0.035±0.0011.06±0.040.003±0.0000.338±0.00510.77±0.120.040±0.002 174.26
V-MoE 4 1 0.042 ±0.0011.12±0.020.006±0.0000.410±0.01711.29±0.260.066±0.004 —
S/32E32 2 0.058 ±0.0011.82±0.010.004±0.0000.472±0.00715.01±0.170.051±0.002 11.69
down-DE 1 4 0.055±0.0011.76±0.040.004±0.0000.486±0.00315.36±0.130.061±0.001 142.28
V-MoE 4 1 0.059 ±0.0011.80±0.050.007±0.0000.536±0.01215.64±0.270.084±0.004 —
Table 30: CIFAR10 comparison of V-MoE and ViT.
Cifar10 Cifar10-C
K M NLL ↓ Error↓ ECE↓ NLL↓ Error↓ ECE↓
L/16V-MoE 1 1 0.029±0.0010.59±0.020.004±0.0000.236±0.0116.25±0.200.034±0.002
ViT - 1 0.034 ±0.0010.69±0.030.005±0.0000.325±0.0207.53±0.400.050±0.004
L/32V-MoE 1 1 0.040 ±0.0010.86±0.020.006±0.0000.290±0.0137.35±0.220.043±0.003
ViT - 1 0.030±0.0010.78±0.020.005±0.0000.281±0.0107.28±0.130.043±0.002
B/16V-MoE 1 1 0.030±0.0010.85±0.020.003±0.0000.249±0.0047.53±0.120.030±0.001
ViT - 1 0.031±0.0010.87±0.030.004±0.0000.293±0.0097.99±0.150.043±0.002
B/32V-MoE 1 1 0.038±0.0001.14±0.030.004±0.0000.359±0.00811.09±0.170.046±0.002
ViT - 1 0.050 ±0.0021.38±0.040.007±0.0000.330±0.0108.97±0.190.051±0.002
S/32V-MoE 1 1 0.059±0.0011.84±0.020.006±0.0000.497±0.00715.53±0.200.066±0.002
ViT - 1 0.065 ±0.0012.08±0.020.006±0.0000.518±0.00815.04±0.240.072±0.002
53Published in Transactions on Machine Learning Research (09/2022)
Table 31: CIFAR10 OOD comparison of V-MoE, downstream ensembles there-of, and e3with 2 experts per input in each case.
CIFAR10 vs. CIFAR100 CIFAR10 vs. DTD CIFAR10 vs. Places365 CIFAR10 vs. SVHN
K M AUC (PR) ↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓
L/16e31 20.9905±0.00030.9902±0.00030.0313±0.00100.9999±0.0000 0.9993±0.0000 0.0005±0.00010.9103±0.0068 0.9936±0.0003 0.0294±0.00130.9963±0.00020.9978±0.00010.0007±0.0002
down-DE 1 2 0.9896±0.00050.9896±0.00050.0357±0.00290.9999±0.00000.9996±0.00000.0001±0.00010.9421±0.00630.9959±0.00030.0162±0.00100.9968±0.00020.9982±0.00020.0007±0.0002
V-MoE 2 1 0.9895±0.00040.9890±0.00030.0389±0.00180.9998±0.0000 0.9988±0.0001 0.0011±0.00010.7891±0.0120 0.9887±0.0004 0.0435±0.00050.9966±0.00010.9980±0.00010.0005±0.0001
L/32e31 20.9875±0.00030.9873±0.00020.0427±0.00110.9998±0.00000.9990±0.00000.0011±0.00020.9196±0.00280.9934±0.00020.0321±0.00130.9950±0.00020.9970±0.00020.0012±0.0001
down-DE 1 2 0.9852 ±0.0005 0.9842±0.0005 0.0571±0.00160.9994±0.0001 0.9966±0.0004 0.0032±0.00040.7388±0.0165 0.9846±0.0007 0.0561±0.00190.9939±0.00040.9960±0.00030.0012±0.0002
V-MoE 2 1 0.9839 ±0.0007 0.9839±0.0005 0.0589±0.00200.9998±0.0000 0.9987±0.00010.0013±0.00030.8967±0.00560.9927±0.00030.0346±0.00120.9944±0.00030.9965±0.00020.0017±0.0003
B/16e31 2 0.9823 ±0.0003 0.9800±0.0003 0.0846±0.00260.9994±0.0000 0.9967±0.00020.0015±0.00020.7661±0.0121 0.9834±0.0004 0.0602±0.00100.9914±0.0002 0.9942±0.00020.0029±0.0005
down-DE 1 2 0.9860±0.00050.9859±0.00040.0540±0.00260.9999±0.00000.9993±0.00010.0009±0.00020.8807±0.01480.9922±0.00050.0388±0.00130.9951±0.00030.9974±0.00020.0027±0.0005
V-MoE 2 1 0.9822 ±0.0005 0.9809±0.0005 0.0800±0.00300.9994±0.0000 0.9964±0.0002 0.0023±0.00030.7399±0.0141 0.9841±0.0006 0.0568±0.00140.9925±0.0004 0.9953±0.00020.0037±0.0009
B/32e31 2 0.9773 ±0.0003 0.9738±0.0004 0.1254±0.00200.9992±0.0000 0.9957±0.0003 0.0038±0.00060.7438±0.0085 0.9794±0.0006 0.0802±0.00230.9881±0.0001 0.9918±0.0002 0.0071±0.0004
down-DE 1 2 0.9820±0.00030.9813±0.00020.0810±0.00330.9998±0.00000.9989±0.00010.0014±0.00040.8567±0.00990.9900±0.00050.0514±0.00170.9924±0.00040.9957±0.00030.0054±0.0003
V-MoE 2 1 0.9798 ±0.0004 0.9790±0.00040.0875±0.00200.9997±0.00000.9986±0.00010.0017±0.00030.8725±0.00740.9904±0.00060.0482±0.00190.9919±0.00020.9952±0.00020.0047±0.0004
S/32e31 2 0.9617 ±0.0003 0.9567±0.0003 0.2629±0.00400.9992±0.0000 0.9956±0.0002 0.0118±0.00110.7431±0.0070 0.9750±0.0006 0.1270±0.00360.9824±0.0004 0.9899±0.0003 0.0388±0.0028
down-DE 1 2 0.9696±0.00030.9669±0.00050.1856±0.00460.9996±0.00000.9981±0.00010.0037±0.00020.8135±0.01460.9845±0.00070.0815±0.00360.9879±0.00040.9933±0.00020.0164±0.0019
V-MoE 2 1 0.9691±0.00080.9667±0.00090.1870±0.00650.9996±0.00000.9979±0.00010.0043±0.00060.8010±0.00870.9839±0.00070.0827±0.00320.9873±0.00050.9927±0.00040.0192±0.0016
Table 32: CIFAR10 OOD comparison of ViT and V-MoE
CIFAR10 vs. CIFAR100 CIFAR10 vs. DTD CIFAR10 vs. Places365 CIFAR10 vs. SVHN
K M AUC (PR) ↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓
L/16V-MoE 1 1 0.9891±0.00040.9895±0.00040.0379±0.00220.9999±0.00000.9997±0.00000.0001±0.00010.9400±0.00570.9960±0.00020.0162±0.00130.9972±0.00010.9984±0.00010.0007±0.0001
ViT - 1 0.9839 ±0.0008 0.9845±0.0007 0.0541±0.00260.9996±0.0000 0.9978±0.0002 0.0018±0.00040.7334±0.0227 0.9857±0.0010 0.0492±0.00240.9947±0.0003 0.9967±0.0002 0.0022±0.0003
L/32V-MoE 1 1 0.9854±0.00030.9850±0.00030.0573±0.00180.9996±0.0000 0.9976±0.0002 0.0030±0.00030.7489±0.0049 0.9862±0.0003 0.0520±0.00110.9946±0.00030.9967±0.00020.0015±0.0003
ViT - 1 0.9842±0.00050.9847±0.00040.0551±0.00180.9998±0.00000.9987±0.00010.0016±0.00030.9164±0.00460.9938±0.00030.0279±0.00150.9942±0.00020.9966±0.00010.0028±0.0003
B/16V-MoE 1 1 0.9856±0.00040.9855±0.00040.0554±0.00250.9998±0.00000.9992±0.00010.0015±0.00050.8598±0.01910.9912±0.00080.0413±0.00280.9949±0.00030.9972±0.00020.0027±0.0003
ViT - 1 0.9801 ±0.0005 0.9798±0.0004 0.0857±0.00230.9996±0.0000 0.9979±0.0001 0.0046±0.00070.8536±0.00570.9895±0.00030.0511±0.00120.9926±0.0002 0.9961±0.0002 0.0048±0.0003
B/32V-MoE 1 1 0.9814±0.00030.9806±0.00030.0853±0.00270.9998±0.00000.9989±0.00010.0017±0.00050.8590±0.00970.9902±0.00050.0506±0.00210.9923±0.00030.9958±0.00030.0061±0.0005
ViT - 1 0.9752 ±0.0005 0.9716±0.0006 0.1485±0.00400.9985±0.0001 0.9915±0.0004 0.0186±0.00100.6507±0.0056 0.9734±0.0006 0.1021±0.00370.9872±0.0004 0.9920±0.0003 0.0148±0.0012
S/32V-MoE 1 1 0.9685±0.00080.9658±0.00100.1922±0.00560.9996±0.00000.9977±0.00020.0045±0.00070.8092±0.01050.9841±0.00080.0821±0.00320.9874±0.00060.9929±0.00040.0185±0.0019
ViT - 1 0.9629 ±0.0004 0.9588±0.0004 0.2422±0.00270.9993±0.0000 0.9963±0.0002 0.0134±0.00130.7177±0.0048 0.9775±0.0003 0.1110±0.00150.9807±0.0003 0.9889±0.0002 0.0426±0.0013
54Published in Transactions on Machine Learning Research (09/2022)
Table 33: CIFAR100 OOD comparison of V-MoE, downstream ensembles there-of, and e3with 2 experts per input in each case.
CIFAR100 vs. CIFAR10 CIFAR100 vs. DTD CIFAR100 vs. Places365 CIFAR100 vs. SVHN
K M AUC (PR) ↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓
L/16e31 20.9507±0.00120.9490±0.00110.2889±0.00580.9969±0.0001 0.9847±0.0002 0.0862±0.00100.7281±0.0023 0.9542±0.0007 0.2969±0.00360.9011±0.00570.9482±0.00240.3071±0.0093
down-DE 1 2 0.9455 ±0.00130.9481±0.00190.2692±0.00930.9975±0.00000.9881±0.00020.0665±0.00350.7619±0.00650.9664±0.00120.2158±0.00710.9071±0.00270.9507±0.00160.2932±0.0144
V-MoE 2 1 0.9391 ±0.00140.9443±0.00150.2901±0.00760.9977±0.00000.9887±0.00020.0674±0.00170.7773±0.00540.9680±0.00090.2150±0.00410.9002±0.00450.9461±0.00240.3223±0.0124
L/32e31 20.9423±0.00030.9379±0.00060.3591±0.00580.9958±0.00010.9792±0.00040.1165±0.00280.6653±0.00370.9394±0.00110.3670±0.00510.8848±0.00610.9398±0.00260.3428±0.0080
down-DE 1 2 0.9380±0.00290.9387±0.00150.3362±0.00420.9958±0.00010.9796±0.00040.1176±0.00150.6710±0.00880.9436±0.00190.3465±0.00930.8877±0.00810.9460±0.00270.3033±0.0077
V-MoE 2 1 0.9275 ±0.00410.9330±0.00200.3604±0.00520.9958±0.00010.9790±0.00050.1306±0.00290.6773±0.00750.9445±0.00170.3489±0.00890.8767±0.00750.9393±0.00200.3404±0.0069
B/16e31 20.9200±0.00080.9121±0.0010 0.4656±0.00710.9938±0.0002 0.9710±0.0011 0.1542±0.00630.5475±0.0076 0.9102±0.0014 0.4653±0.00520.8730±0.00560.9319±0.00250.3699±0.0090
down-DE 1 2 0.9242±0.00160.9275±0.00130.3506±0.00700.9952±0.00030.9777±0.00110.1144±0.00520.6023±0.01010.9343±0.00150.3577±0.00490.8726±0.00490.9340±0.00270.3526±0.0124
V-MoE 2 1 0.9159 ±0.0019 0.9211±0.00150.3729±0.00530.9950±0.00020.9768±0.00080.1193±0.00380.6029±0.00660.9331±0.00080.3682±0.00290.8704±0.00380.9283±0.00200.3915±0.0076
B/32e31 2 0.9075 ±0.0012 0.8945±0.0015 0.5358±0.00630.9906±0.0004 0.9554±0.0019 0.2481±0.01010.4355±0.0071 0.8811±0.0014 0.5465±0.00430.8583±0.00400.9221±0.00240.4051±0.0100
down-DE 1 2 0.9188±0.00210.9158±0.00140.4248±0.00770.9942±0.00030.9719±0.00140.1517±0.00900.5525±0.00960.9176±0.00220.4276±0.00850.8756±0.00830.9292±0.00330.3959±0.0083
V-MoE 2 1 0.9192±0.00140.9151±0.00120.4444±0.00600.9913±0.0002 0.9580±0.0008 0.2481±0.00640.4449±0.0100 0.8969±0.0009 0.5230±0.00270.8641±0.00730.9252±0.00280.4249±0.0080
S/32e31 20.8677±0.00160.8528±0.0017 0.6086±0.00490.9878±0.00070.9430±0.0026 0.2677±0.00840.3628±0.0072 0.8519±0.0026 0.5716±0.00740.8279±0.00370.9004±0.00240.4543±0.0098
down-DE 1 2 0.8697±0.00190.8651±0.00240.5203±0.00950.9892±0.00050.9532±0.00230.1909±0.00860.4185±0.00630.8814±0.00200.4657±0.00780.8162±0.00450.8912±0.00260.4835±0.0066
V-MoE 2 1 0.8687±0.00110.8663±0.00120.5206±0.00610.9894±0.00040.9523±0.00180.2076±0.00750.4129±0.00820.8752±0.00260.5039±0.00720.8133±0.0029 0.8882±0.0020 0.5031±0.0058
Table 34: CIFAR100 OOD comparison of V-MoE and ViT
CIFAR100 vs. CIFAR10 CIFAR100 vs. DTD CIFAR100 vs. Places365 CIFAR100 vs. SVHN
K M AUC (PR) ↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓ AUC (PR)↑AUC (ROC)↑FPR@95↓
L/16V-MoE 1 1 0.9454±0.00130.9481±0.00150.2682±0.00730.9976±0.00000.9882±0.00020.0658±0.00240.7631±0.00430.9667±0.00070.2141±0.00400.9115±0.00350.9533±0.00200.2794±0.0123
ViT - 1 0.9411±0.00190.9449±0.00120.2734±0.00620.9970±0.0001 0.9854±0.0006 0.0853±0.00390.7552±0.01180.9608±0.0017 0.2566±0.00660.8697±0.0033 0.9326±0.0016 0.3690±0.0058
L/32V-MoE 1 1 0.9358±0.00280.9368±0.00180.3431±0.00520.9961±0.0001 0.9806±0.0004 0.1148±0.00220.6757±0.0080 0.9461±0.0016 0.3308±0.00810.8863±0.00930.9459±0.00250.3063±0.0123
ViT - 1 0.9285±0.00200.9323±0.00160.3201±0.00680.9967±0.00010.9838±0.00060.0911±0.00360.7541±0.00660.9634±0.00140.2292±0.00680.8495±0.0068 0.9234±0.0035 0.3949±0.0137
B/16V-MoE 1 1 0.9206±0.00150.9241±0.00140.3572±0.00610.9951±0.00020.9776±0.00070.1094±0.00340.5924±0.00760.9334±0.00120.3532±0.00350.8720±0.00480.9309±0.00230.3705±0.0122
ViT - 1 0.9177±0.00130.9171±0.0014 0.3925±0.00690.9906±0.0003 0.9571±0.0012 0.2199±0.00590.4913±0.0088 0.8980±0.0024 0.4927±0.00790.8525±0.0045 0.9204±0.0022 0.4226±0.0065
B/32V-MoE 1 1 0.9166±0.00170.9145±0.00120.4211±0.00480.9940±0.00020.9714±0.00090.1562±0.00680.5433±0.00890.9178±0.00160.4258±0.00490.8730±0.00710.9276±0.00330.4026±0.0100
ViT - 1 0.9038 ±0.0022 0.9044±0.00180.4180±0.00610.9927±0.0002 0.9663±0.00080.1631±0.00550.5306±0.00460.9112±0.00150.4176±0.00620.8379±0.0029 0.9116±0.00140.4328±0.0062
S/32V-MoE 1 1 0.8678±0.00120.8631±0.00150.5281±0.00500.9893±0.00070.9524±0.00310.1978±0.01290.4024±0.00910.8778±0.00290.4763±0.00850.8224±0.00580.8945±0.00340.4729±0.0098
ViT - 1 0.8644±0.00180.8541±0.0020 0.5716±0.00610.9836±0.0007 0.9287±0.0026 0.2976±0.00820.3149±0.0031 0.8349±0.0024 0.5982±0.00610.8088±0.00510.8894±0.00290.4888±0.0084
55Published in Transactions on Machine Learning Research (09/2022)
Table 35: Few-shot comparison of e3, V-MoE and ensembles thereof.
Mean Across Datasets Weighted Mean Across Datasets Downstream
K M 1-Shot Error ↓5-Shot Error↓10-Shot Error ↓25-Shot Error ↓1-Shot Error↓5-Shot Error↓10-Shot Error ↓25-Shot Error ↓∆FLOPs (%)↓
H/14E31 2 31.47±0.72 17.87±0.32 14.29±0.22 10.64±0.25 78.51±0.24 80.65±0.08 81.28±0.05 81.51±0.06 15.63
down-DE 1 2 30.84±0.42 17.77±0.51 14.43±0.03 11.06±0.40 78.36±0.20 80.63±0.14 81.31±0.02 81.60±0.09 75.79
V-MoE 2 1 32.47±0.55 18.77±0.41 15.19±0.27 12.09±0.38 78.93±0.20 80.89±0.11 81.48±0.07 81.83±0.09 —
L/16E31 2 34.08±0.21 19.57±0.16 15.21±0.14 11.75±0.11 79.50±0.07 81.09±0.04 81.48±0.03 81.76±0.02 6.74
down-DE 1 2 32.98±0.42 19.65±0.16 15.44±0.16 11.92±0.14 79.09±0.13 81.10±0.04 81.54±0.04 81.79±0.03 86.04
V-MoE 2 1 33.98±0.28 20.42±0.12 16.20±0.08 12.73±0.13 79.50±0.10 81.30±0.03 81.72±0.02 81.97±0.03 —
L/32E31 2 36.71±0.20 22.14±0.16 17.79±0.05 13.97±0.09 80.51±0.07 81.77±0.04 82.11±0.01 82.26±0.02 6.35
down-DE 1 2 36.15±0.28 22.18±0.15 17.81±0.14 14.00±0.12 80.45±0.13 81.79±0.04 82.13±0.03 82.27±0.03 85.29
V-MoE 2 1 36.56±0.34 23.00±0.12 18.86±0.07 15.02±0.16 80.56±0.13 82.00±0.03 82.37±0.02 82.50±0.03 —
B/16E31 2 38.60±0.38 21.93±0.17 17.43±0.13 13.62±0.08 81.14±0.11 81.74±0.04 82.05±0.03 82.19±0.02 12.60
down-DE 1 2 38.96 ±0.36 23.36±0.23 18.99±0.12 14.84±0.11 81.39±0.10 82.09±0.06 82.40±0.03 82.46±0.02 75.09
V-MoE 2 1 37.76±0.15 23.16±0.09 18.79±0.14 14.94±0.16 80.97±0.05 82.04±0.02 82.36±0.03 82.48±0.04 —
B/32E31 2 43.39 ±0.33 26.51±0.11 21.18±0.17 16.82±0.09 82.86±0.11 82.91±0.03 82.93±0.04 82.90±0.02 11.54
down-DE 1 2 41.09±0.31 26.48±0.24 21.61±0.22 17.20±0.13 82.24±0.13 82.89±0.06 83.03±0.05 82.99±0.03 73.49
V-MoE 2 1 42.50 ±0.28 27.44±0.19 22.73±0.19 18.60±0.19 82.66±0.08 83.16±0.05 83.29±0.05 83.29±0.04 —
S/32E31 2 52.91 ±0.27 34.28±0.19 27.54±0.18 21.85±0.11 85.89±0.08 84.81±0.04 84.39±0.04 84.01±0.02 15.87
down-DE 1 2 48.27±0.21 32.19±0.22 26.55±0.10 21.95±0.20 84.62±0.06 84.35±0.05 84.19±0.03 84.04±0.05 64.34
V-MoE 2 1 49.37 ±0.19 33.51±0.17 28.00±0.14 23.25±0.15 85.04±0.06 84.69±0.04 84.54±0.03 84.33±0.03 —
Table 36: Few-shot comparison of V-MoE and ViT.
Mean Across Datasets Weighted Mean Across Datasets
K M 1-Shot Error ↓5-Shot Error↓10-Shot Error ↓25-Shot Error ↓1-Shot Error↓5-Shot Error↓10-Shot Error ↓25-Shot Error ↓
H/14V-MoE 1 1 33.04±0.32 19.23±0.35 15.63±0.25 12.06±0.36 79.05±0.12 81.00±0.09 81.59±0.06 81.82±0.08
ViT - 1 35.78 ±0.41 20.61±0.15 16.78±0.12 13.62±0.24 79.97±0.16 81.37±0.04 81.87±0.03 82.16±0.05
L/16V-MoE 1 1 34.15±0.27 20.32±0.12 16.14±0.12 12.71±0.15 79.54±0.08 81.27±0.03 81.71±0.03 81.97±0.03
ViT - 1 36.52 ±0.20 21.79±0.12 17.51±0.08 14.07±0.14 80.38±0.05 81.67±0.03 82.03±0.02 82.27±0.03
L/32V-MoE 1 1 36.83±0.27 23.05±0.08 18.78±0.11 14.95±0.07 80.67±0.10 82.01±0.02 82.35±0.02 82.48±0.02
ViT - 1 38.22 ±0.31 23.97±0.14 19.56±0.13 15.75±0.14 81.10±0.10 82.25±0.04 82.53±0.03 82.65±0.03
B/16V-MoE 1 1 39.22±0.27 24.42±0.13 20.01±0.11 15.94±0.18 81.47±0.08 82.36±0.04 82.64±0.03 82.70±0.04
ViT - 1 41.29 ±0.14 25.03±0.10 20.08±0.10 15.87±0.17 82.16±0.04 82.51±0.03 82.65±0.02 82.68±0.04
B/32V-MoE 1 1 42.37±0.31 27.60±0.20 22.89±0.19 18.46±0.10 82.64±0.11 83.19±0.05 83.33±0.05 83.26±0.02
ViT - 1 44.60 ±0.22 28.20±0.17 22.97±0.12 18.86±0.15 83.35±0.08 83.35±0.04 83.35±0.03 83.35±0.03
S/32V-MoE 1 1 49.60±0.28 33.34±0.13 27.88±0.11 23.30±0.18 85.09±0.08 84.64±0.03 84.50±0.03 84.33±0.04
ViT - 1 54.16 ±0.16 36.25±0.18 29.88±0.17 24.42±0.13 86.32±0.05 85.26±0.04 84.90±0.04 84.54±0.03
56Published in Transactions on Machine Learning Research (09/2022)
N Additional Algorithm Overview Diagrams
h2h1
h3gateK(W1hi)
gateK(W2hi)MLP2(hi)MLP1(hi)
MLP3(hi)
MLP5(hi)MLP4(hi)
MLP6(hi)Dispatch
(K=2)
ˆh2ˆh1
ˆh3Combine¯h1
¯h2
¯h3SoftmaxTransformer
Block(n−3)×
Norm MSA + Norm p-MoE +p-MoE Block
Transformer
Blockp-MoE
BlockClassifier
Figure 24: End-to-end overview of the Partitioning method, from Section 4.2.1, with E= 6experts, parti-
tioned into M= 2groups, with sparsity of K= 2, and a “last-2” conﬁguration. Top:Partitioning contains
a sequence of transformer blocks, followed by alternating transformer and p(artitioned)-MoE blocks. As in
ViT, images are split into patches whose embeddings are processed by each block. Here, we show 1 embed-
ding for each of three images ( ,,).Bottom left : In a MoE block, we replace the transformer block’s
MLP with parallel partitioned expert MLPs. The eﬀect of the routing weights is not depicted. Bottom
right: The classiﬁer makes predictions from the ﬁnal representations ( ). Notice that without a tiling
mechanism, there is only a single prediction per input.
h2h1
h3h3,1h2,1h1,1
h1,2h1,2
h2,2h2,2
h3,2h3,2Tile
gateK(Whi)MLP1(hi,1)
MLP2(hi,1)
MLP3(hi,1)
MLP4(hi,2)
MLP5(hi,2)
MLP6(hi,2)Dispatch
(K=2)ˆh2,1ˆh1,1
ˆh3,1
ˆh2,2ˆh2,2ˆh1,2ˆh1,2
ˆh3,2ˆh3,2Combine¯h1,1
¯h1,2¯h1,2
¯h2,1
¯h2,2¯h2,2
¯h3,1
¯h3,2¯h3,2Softmax
Ensemble
(Mean)Transformer
Block(n−3)×
Norm MSA + Norm MoE +MoE Block
Transformer
BlockMoE
BlockClassifier
1st MoE
block only
Figure 25: End-to-end overview of the Tilingmethod, from Section 4.2.2, with E= 6experts, a sparsity
ofK= 2, and a “last-2” conﬁguration. Top:Tilingcontains a sequence of transformer blocks, followed by
alternating transformer and MoE blocks. As in ViT, images are split into patches whose embeddings are
processed by each block. Here, we show 1 embedding for each of three images ( ,,).Bottom left : In
a MoE block, we replace the transformer block’s MLP with parallel partitioned expert MLPs. The eﬀect of
the routing weights is not depicted. Embeddings are tiled ( ) in the ﬁrst p-MoE block only. Bottom right :
The classiﬁer averages predictions from the ﬁnal tiled representations ( ). Notice that without partitioning,
some patches and their corresponding tiled versions can be routed to the same experts, resulting in a reduced
diversity in predictions.
57Published in Transactions on Machine Learning Research (09/2022)
h2h1
h3gateK(Whi) MLP2(hi)MLP1(hi)
MLP3(hi)Dispatch
(K=2)ˆh2ˆh1
ˆh3Combine¯h1,1
¯h1,2¯h1,2
¯h2,1
¯h2,2¯h2,2
¯h3,1
¯h3,2¯h3,2Softmax
Ensemble
(Mean)Transformer
Block(n−3)×
Norm MSA + Norm MoE +MoE Block
Transformer
BlockMoE
BlockClassifier
Except Last
MoE block
Figure 26: End-to-end overview of the simple Multi-pred MoE, from Section 4.2.4, with E= 3experts,
sparsity ofK=2, and a “last-2” conﬁguration. Top: The multi-pred MoE contains a sequence of transformer
blocks, followed by alternating transformer and MoE blocks. As in ViT, images are split into patches whose
embeddings are processed by each block. Here, we show 1 embedding for each of three images ( ,,).
Bottom left : In all but the last MoE block, we recombine the predictions as usual. Bottom right : The
classiﬁer averages predictions from the ﬁnal representations ( ).
58