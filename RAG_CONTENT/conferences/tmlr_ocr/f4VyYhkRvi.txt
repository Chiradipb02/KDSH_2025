Published in Transactions on Machine Learning Research (05/2023)
Fairness via In-Processing in the Over-parameterized Regime:
A Cautionary Tale with MinDiff Loss
Akshaj Kumar Veldanda akv275@nyu.edu
Electrical and Computer Engineering Department
New York University
Ivan Brugere∗ivan.brugere@jpmchase.com
JP Morgan Chase AI Research
Jiahao Chen∗cjiahao@gmail.com
Parity
Sanghamitra Dutta∗sanghamd@umd.edu
Electrical and Computer Engineering Department
University of Maryland College Park
Alan Mishler∗alan.mishler@jpmchase.com
JP Morgan Chase AI Research
Siddharth Garg sg175@nyu.edu
Electrical and Computer Engineering Department
New York University
Reviewed on OpenReview: https: // openreview. net/ forum? id= f4VyYhkRvi& noteId
Abstract
Prior work has observed that the test error of state-of-the-art deep neural networks often
continues to decrease with increasing over-parameterization, a phenomenon referred to as
doubledescent. Thisallowsdeeplearningengineerstoinstantiatelargemodelswithouthaving
to worry about over-fitting. Despite its benefits, however, prior work has shown that over-
parameterizationcanexacerbatebiasagainstminoritysubgroups. Severalfairness-constrained
DNN training methods have been proposed to address this concern. Here, we critically
examine MinDiff, a fairness-constrained training procedure implemented within TensorFlow’s
Responsible AI Toolkit, that aims to achieve Equality of Opportunity. We show that although
MinDiff improves fairness for under-parameterized models, it is likely to be ineffective in
the over-parameterized regime. This is because an overfit model with zero training loss is
trivially group-wise fair on training data, creating an “illusion of fairness,” thus turning off
the MinDiff optimization (this will apply to any disparity-based measures which care about
errors or accuracy; while it won’t apply to demographic parity). We find that within specified
fairness constraints, under-parameterized MinDiff models can even have lower error compared
to their over-parameterized counterparts (despite baseline over-parameterized models having
lower error compared to their under-parameterized counterparts). We further show that
MinDiff optimization is very sensitive to choice of batch size in the under-parameterized
regime. Thus, fair model training using MinDiff requires time-consuming hyper-parameter
searches. Finally, we suggest using previously proposed regularization techniques, viz. L2,
early stopping and flooding in conjunction with MinDiff to train fair over-parameterized
models. In our results, over-parameterized models trained using MinDiff+regularization with
standard batch sizes are fairer than their under-parameterized counterparts, suggesting that
at the very least, regularizers should be integrated into fair deep learning flows, like MinDiff.
∗Equal Contribution
1Published in Transactions on Machine Learning Research (05/2023)
1 Introduction
Over the past few years, machine learning (ML) solutions have found wide applicability in a wide range
of domains. However, recent work has shown that ML methods can exhibit unintended biases towards
specific population groups, for instance in applications like hiring (Schumann et al., 2020), credit verification
(Khandani et al., 2010), facial recognition (Buolamwini & Gebru, 2018; Grother et al., 2010; Ngan & Grother,
2015), recidivism prediction (Chouldechova, 2017) and recommendation systems (Biega et al., 2018; Singh
& Joachims, 2018), resulting in negative societal consequences. To address this concern, there is a growing
and influential body of work on mitigating algorithmic unfairness of ML models. These solutions are being
integrated within widely used ML frameworks and are beginning to find practical deployment (Responsible-AI;
Akihiko Fukuchi, 2020). As ML fairness methods make the transition from theory to practice, their ability to
achieve stated goals in real-world deployments merits closer examination.
Methodstotrainfairmodelscanbebroadlycategorizedbasedonthestageatwhichtheyareused: pre-training,
in-training, or post-training. Of these, only in-training methods substantively modify the model training
process. This paper examines the performance of MinDiff (Prost et al., 2019), the principal in-training method
integrated within TensorFlow’s Responsible AI Toolkit (Responsible-AI). We are particularly interested in
MinDiff for training fair deep learning models because, given TensorFlow’s widespread adoption, there is good
reason to believe that it will be picked as the default choice by practitioners working within this framework.
We evaluate MinDiff on three datasets, Waterbirds, CelebA, and HAM10000, that are commonly used in
fairness literature; and we observe several notes of caution. Because the success of deep learning can be
attributed at least in part to the surprising ability of over-parameterized deep networks to generalize (Nakkiran
et al., 2020), we begin by evaluating the relationship between model capacity and fairness with MinDiff.
Over-parameterized models have more parameters than required to memorize the training dataset; under-
parameterized models have fewer. The point at which a model is just large enough to memorize the training
data is referred to as the interpolation threshold. We observe that MinDiff does increase fairness for small,
under-parameterized models, but is almost entirely ineffective on larger over-parameterized networks. Thus,
in some cases, under-parameterized MinDiff models can have lower fairness-constrained error compared to
their over-parameterized counterparts even though over-parameterized models are always better on baseline
error (i.e., error on models trained without MinDiff optimization). We caution that when using MinDiff for
fairness, ML practitioners must carefully choose model capacity, something which is generally unnecessary
when fairness is not a concern and the goal is simply to minimize error.
We find the reason MinDiff is ineffective in the over-parameterized regime is because when a model’s training
loss goes to zero, any fairness metric that relies on differences in the model’s accuracy across different
sub-groups will also go to zero, thus creating an “illusion of fairness" and turning off MinDiff optimization.
For completeness, we note that this argument does not apply to demographic parity (Calders et al., 2009)
which cares about equal proportions of positive predictions in each group. Thus, we explore whether strong
regularization used along with MinDiff can alleviate its ineffectiveness. Specifically, we consider two classes
of regularization techniques: implicit such as batch sizing (Smith et al., 2021; Barrett & Dherin, 2021) and
early stopping (Morgan & Bourlard, 1990), and explicit such as weight decay (Krogh & Hertz, 1992) and a
recently proposed “loss flooding” method (Ishida et al., 2020) regularizers. We find that: (1) batch sizing
only helps for medium sized models around the interpolation threshold; (2) the remaining three methods all
improve fairness in the over-parameterized regime; (3) early-stopping and flooding result in the fairest models
for the Waterbirds, CelebA and HAM10000 datasets, respectively; and (4) with effective regularization,
over-parameterized models are fairer than their under-parameterized counterparts.
2 Related Work
There are several techniques in the literature to mitigate algorithmic bias. These techniques can be broadly
categorized as: pre-processing, in-processing and post-processing. Pre-processing techniques aim to de-identify
sensitive information and create more balanced training datasets (Quadrianto et al., 2019; Ryu et al., 2018;
Feldman et al., 2015; Wang & Deng, 2019; Karkkainen & Joo, 2021; Dixon et al., 2018). In-processing
techniques (Prost et al., 2019; Cherepanova et al., 2021; Sagawa et al., 2020a;b; Padala & Gujar, 2021;
2Published in Transactions on Machine Learning Research (05/2023)
Agarwal et al., 2018; Zafar et al., 2019; Donini et al., 2018; Lahoti et al., 2020; Beutel et al., 2019; Martinez
et al., 2020; Wadsworth et al., 2018; Goel et al., 2018; Wang & Deng, 2019; Hashimoto et al., 2018) alter
the training mechanism by imposing fairness constraints to the training objective, or utilize adversarial
training (Beutel et al., 2017; Zhang et al., 2018; Madras et al., 2018) to make predictions independent of
sensitive attributes. Post-processing techniques (Hardt et al., 2016; Wang et al., 2020; Savani et al., 2020;
Chzhen et al., 2019; Jiang et al., 2020; Wei et al., 2020) alter the outputs of an existing model, for instance,
using threshold correction (Zhou & Liu, 2006; Collell et al., 2016; Menon et al., 2021a) that applies different
classification thresholds to each sensitive group (Hardt et al., 2016). In this paper, we focus on MinDiff (Prost
et al., 2019), the primary in-processing procedure implemented within TensorFlow’s Responsible AI toolkit.
While our quantitative conclusions might differ, we believe that similar qualitative conclusions might hold for
other in-processing methods because overfit models are trivially fair.
With the growing adoption of large over-parameterized deep networks, recent efforts have sought to investigate
their fairness properties (Menon et al., 2021b; Sagawa et al., 2020a; Cherepanova et al., 2021; Sagawa et al.,
2020b). Pham et al. (2021); Dehghani et al. (2023) observed that over-parameterized ERM models have better
worst-group generalization compared to their under-parameterized counterparts. However, Maity et al. (2022)
warn that baseline ERM models should not be considered state-of-the-art to train fair over-parameterized
models. Sagawa et al. (2020b) proposed a pre-processing technique by investigating the role of training data
characteristics (such as ratio of majority to minority groups and relative informativeness of spurious versus
core features) on fairness and observed that sub-sampling improves fairness in the over-parameterized regime.
Menon et al. (2021b) found that post-processing techniques including retraining with sub-sampled majority
groups and threshold correction also enhance fairness in over-parameterized models. Alabdulmohsin & Lucic
(2021) proposed a post-processing algorithm that can be effectively used to debias large models. Cherepanova
et al. (2021) report that in-processing convex surrogates of fairness constraints like equal loss, equalized odds
penalty, disparate impact penalty, etc., (Padala & Gujar, 2021) are ineffective on over-parameterized models,
but do not propose any techniques to increase the effectiveness of in-processing methods. Wald et al. (2022)
theoretically show that interpolating models cannot satisfy fairness constraints. However, it is not thoroughly
investigated how fairness constraints can be effectively implemented in over-parameterized models. It is
possible that using methods such as MinDiff may improve the training of fair over-parameterized models.
Our work is the first to systematically compare under- vs. over-parameterized deep learning models trained
using in-processing fairness methods, using MinDiff as a representative technique.
Regularization techniques (Morgan & Bourlard, 1990; Srivastava et al., 2014; Krogh & Hertz, 1992; Ishida
et al., 2020) are popularly used in deep learning frameworks to avoid over-fitting. Lately, researchers have
also exploited the benefits of regularizers to train fair models. For example, Sagawa et al. (2020a) proposed
distributionally robust optimization (DRO) to improve worst-group generalization, but they observed that
their approach fails if training loss converges to zero. Hence, they use L2 weight regularization and early
stopping to improve fairness in the over-parameterized regime. Our paper systematically evaluates different
regularizers, including batch sizing, early stopping, weight decay and the recently proposed flooding loss (Ishida
et al., 2020) for MinDiff training across different model sizes, and makes several new observations about the
role of regularization in enhancing fairness.
3 Methodology
We now describe our evaluation methodology.
3.1 Formal Setup
In this paper, we consider binary classification problem on a training dataset D={xi,ai,yi}N
i=1, wherexi
is an input (an image for instance) ai∈{0,1}is a sensitive attribute of the input, and yi∈{0,1}is the
corresponding ground-truth label. The training data is sampled from a joint distribution DX,A,Yover random
variablesX,A, andY. Deep neural network (DNN) classifiers are represented as a parameterized function
fθ:X→ [0,1], whereθare trainable parameters, obtained in practice by minimizing the empirical binary
3Published in Transactions on Machine Learning Research (05/2023)
cross-entropy primary loss function LP:
LP=−1
NN/summationdisplay
i=1[yi·log(fθ(xi)) + (1−yi)·log(1−fθ(xi))] (1)
via stochastic gradient descent (SGD).
We denote the classification threshold as τwhich can be used to make predictions ˆfθ(x;τ)as shown below
ˆfθ(x;τ) =/braceleftigg
1, fθ(x)≥τ
0,otherwise.(2)
The goal is to attain low error at the population level, i.e., P[ˆfθ(X;τ)̸=Y](typically,τ= 0.5). As the
data-generating distribution is unavailable, standard DNN training seeks to achieve low empirical error on a
test set, which we refer to as test error for short. However, performance conditioned on sensitive attributes
can vary, leading to outcomes that are biased in favor of or against specific sub-groups. Several fairness
metrics have been defined in prior work to account for this bias; in this paper, we will use the widely adopted
equality of opportunity metric (Hardt et al., 2016).
Equality of Opportunity (Hardt et al., 2016) is a widely adopted fairness notion that seeks to equalize
false negative rates (FNR) across sensitive groups to achieve fairness in contexts where a higher FNR for
a certain group can result in unfair outcomes, such as wrongful convictions or denial of opportunities in
comparison to other groups. For binary sensitive attributes the FNR gapat the population level is defined as:
/vextendsingle/vextendsingleP[ˆfθ(X;τ) = 0|Y= 1,A= 0]−P[ˆfθ(X;τ) = 0|Y= 1,A= 1]/vextendsingle/vextendsingle. (3)
In practice, we measure the population-level FNR gapon a test set, which we will refer to as just FNR gap
for short. As we describe next, MinDiff (and several other methods) seek to minimize the FNR gapduring
training.
3.2 MinDiff Training
MinDiff (Prost et al., 2019) is an in-processing optimization framework that seeks to achieve a balance
between two objectives: low test error and low FNR gap. For this, MinDiff proposes a modified loss function
LT=LP+λLM, whereLTis the total loss, that is a weighted sum of the primary cross-entropy loss, defined
in Equation (1), and LM, a differentiable proxy for the FNR gap. In the modified loss function, λ∈R+is a
user-defined parameter that controls the relative importance of the fairness versus the empirical cross-entropy
loss. The fairness term in the modified loss function, LM, uses the maximum mean discrepancy ( MMD)
distance between the neural network’s outputs for the two sensitive groups when Y= 1, i.e.,
LM= MMD(fθ(X)|A= 0,Y= 1,fθ(X)|A= 1,Y= 1). (4)
Intuitively, the MMD loss LMpenalizes any statistical dependence between the predictions for the two
subgroups which are made by the model on positive examples. We refer the reader to Appendix A for a
formal mathematical definition of the MMDdistance (Prost et al., 2019).
3.3 Post-hoc Threshold Correction
Due to fairness requirements of the application, ML practitioners might seek to train models with a population
level FNR gaplower than a specified threshold ∆FNR. This can be done using an additional post-processing
step as proposed by Hardt et al. (2016). The idea is to use different classification thresholds for each sub-group,
τA=0andτA=1, that are selected such that the population level FNR gapunder these thresholds is below the
constraint:
/vextendsingle/vextendsingleP[ˆfθ(X;τA=0) = 0|Y= 1,A= 0]−P[ˆfθ(X;τA=1) = 0|Y= 1,A= 1]/vextendsingle/vextendsingle≤∆FNR (5)
4Published in Transactions on Machine Learning Research (05/2023)
(a) Waterbirds
 (b) CelebA
Figure 1: We show the model-wise double descent behaviour on baseline models trained using (a) Waterbirds
and (b) CelebA datasets respectively. Interpolation threshold (shown in green dotted line) is the point where
the model is large enough to fit the training data. The region beyond the interpolation threshold is called the
over-parameterized regime.
In practice, the two thresholds can be picked using grid search and empirically measuring population level
FNR gapat each grid point on a validation dataset. The resulting thresholds yield a test error which we refer
to as the fairness-constrained test error . Pareto front of fairness-constrained test error and ∆FNRis used
to compare different model sizes and fairness methods. In particular, a model or method is fairer if it has
lower fairness-constrained test error compared to the alternative for a fixed ∆FNR.
3.4 Regularization Techniques
We evaluate four regularization techniques to improve performance of MinDiff on over-parameterized networks.
Reduced Batch Sizes: Due to the stochastic nature of SGD, smaller batch sizes can act as implicit
regularizers during training (Smith et al., 2021; Barrett & Dherin, 2021). This is because smaller batch sizes
provide a noisy estimate of the total loss LT.
Weight Decay: Weight decay (Krogh & Hertz, 1992) explicitly penalizes the parameters θof the DNN
from growing too large. Weight decay adds a penalty, usually the L2 norm of the weights, to the loss function.
Early Stopping: Early stopping (Morgan & Bourlard, 1990) terminates DNN training earlier than the
point at which training loss converges to a local minima, and has been shown to be particularly effective
for over-parameterized deep networks (Li et al., 2019). A common implementation of early stopping is
to terminate training once the validation loss has not improved for a certain number of gradient steps or
epochs (Morgan & Bourlard, 1990). For models trained with MinDiff, we explore two versions of early
stopping in which we use either the primary loss, LP, or total loss,LT, as a stopping criterion.
Flooding Regularizer: Finally, motivated by our goal to prevent the primary training loss from going to
zero (which then turns off MinDiff as well), we apply the flooding regularizer (Ishida et al., 2020) that encodes
this goal explicitly . Flooding operates by performing gradient descent only if LP>b, wherebis the flood level.
Otherwise, ifLP≤b, then gradient ascent takes place as shown in Equation 6. This phenomenon ensures
thatLPfloats around the flood level band never approaches zero. We implement flooding by replacing the
primary loss term in LTwith a new loss term:
L′
P=/vextendsingle/vextendsingleLP−b/vextendsingle/vextendsingle+b, (6)
which, in turn, enables continued minimization of the MinDiff loss term, LM, over the training process.
5Published in Transactions on Machine Learning Research (05/2023)
Figure 2: We show the (a) average test error, and (b) FNR gapversus model width for MinDiff optimization
withλ={0.0,0.5,1.0,1.5}’s on Waterbirds dataset. MinDiff optimization has negligible to no impact on
fairness in the over-parameterized models. However, for under-parameterized models, we find that increasing
λsubstantially reduces the FNR gap.
4 Experimental Setup
We perform our experiments on the Waterbirds (Sagawa et al., 2020a), CelebA (Liu et al., 2015) and
HAM10000 (Tschandl, 2018) datasets which have previously been used in fairness evaluations of deep learning
models. Here, we describe network architectures, training and evaluations for the two datasets.
4.1 Waterbirds Dataset
Waterbirds is a synthetically created dataset (Sagawa et al., 2020a) which contains water- and land-bird
images overlaid on water and land backgrounds. A majority of waterbirds (landbirds) appear in water (land)
backgrounds, but in a minority of cases waterbirds (landbirds) also appear on land (water) backgrounds. As
in past work, we use the background as the sensitive feature. Further, we use waterbirds as the positive class
and landbirds as the negative class. The dataset is split into training, validation and test sets with 4795,
1199 and 5794 images in each dataset respectively.
We follow the training methodology described in (Sagawa et al., 2020b) to train a deep network for this
dataset. First, a fixed pre-trained ResNet-18 model is used to extract a d-dimensional feature vector µ.
This feature vector is then converted into an m-dimensional feature µ′=ReLU (Uµ), whereU∈Rm×dis a
random matrix with each row sampled uniformly from a unit sphere Sd−1. A logistic regression classifier is
trained onµ′. Model width is controlled by varying m, the dimensionality of µ′, from 10to10,000.
4.2 CelebA Dataset
The CelebA dataset consists of 202,599 celebrity face images annotated with 40 binary attributes including
gender, hair colour, hair style, eyeglasses, etc. In our experiments, we set the target label Yto be hair
color, which is either blond ( Y= 1) or non-blond ( Y= 0), and the sensitive attribute to be gender. Blond
individuals constitute only 15%of this dataset, and only 6%of blond individuals are men. Consequently
baseline models make disproportionately large errors on blond men versus blond women. The objective of
MinDiff training is to minimize the FNR gapbetween blond men and blond women1. The dataset is split into
training, validation and test sets with 162770, 19867 and 19962 images, respectively.
We used the ResNet-preact-18 model for this dataset, and vary model capacity by uniformly scaling the
number of channels in all layers. We also train a LeNet-5 architecture on CelebA dataset and compare it
with the ResNet-preact-18 model.
1Note that the MinDiff paper uses False Positive Rate, but this is totally arbitrary here since the class labels are arbitrary.
6Published in Transactions on Machine Learning Research (05/2023)
(a) Under-parameterized (b) Over-parameterized
Figure 3: We show the progress of primary loss and population level FNR gap, evaluated on training dataset and
on the validation dataset, versus SGD steps during MinDiff optimization ( λ= 1.5) for under-parameterized
and over-parameterized models on CelebA dataset. We find that over-parameterized models over-fits to the
training data and achieve zero population level FNR gapon the training dataset, thus turning off MinDiff
optimization. Whereas, population level FNR gapon the training dataset is positive in the under-parameterized
models, allowing for MinDiff optimization to be effective.
4.3 HAM10000 Dataset
HAM10000 dataset consists of 10,015 dermatoscopic images for automatic diagnosis of pigmented skin
lesions. Each image in the dataset contains several annotations including sensitive attributes like age, gender
and the diagnosis labels of the patient. Medical imaging is a problem with direct human impact and this
dataset has been previously studied in the context of bias and fairness (Daneshjou et al., 2022). Following
prior work (Zong et al., 2023; Maron et al., 2019), we split the 7 diagnostic labels into benign: basal cell
carcinoma (bcc), benign keratosis-like lesions (bkl), dermatofibroma (df), melanocytic nevi (nv), and vascular
lesions (vasc), and malignant: actinic keratoses and intraepithelial carcinoma / Bowen’s disease (akiec), and
melanoma (mel). In our experiments, we set the target label Yto be the diagnostic label, which is either
malignant ( Y= 1) or benign ( Y= 0), and the sensitive attribute to be age. We categorize all patients into
two demographic groups based on their age: those under 40 ( <40) and those 40 years old or above ( ≥40).
Malignant individuals constitute only 15%of this dataset, and only 6%of malignant individuals are <40.
Consequently baseline models make disproportionately large errors on malignant ( <40) individuals versus
malignant (≥40) individuals. The objective of MinDiff training is to minimize the FNR gapbetween malignant
(<40) and malignant ( ≥40) individuals. We discard images whose sensitive attributes are not available
and the dataset is split into training, validation and test sets with 7967, 989 and 1002 images, respectively.
We used the pre-trained ResNet-18 as an over-parameterized model and LeNet-5 as an under-parameterized
model for this dataset.
4.4 Hyper-parameters and Training Details
Waterbirds We train for a total of 30,000gradient steps using the Adam optimizer. For our baseline
experiments, we set batch size to 128and use a learning rate schedule with initial learning rate = 0.01
and decay factor of 10 for every 10,000gradient steps. We ran every experiment 10 times with random
initializations and report the average of all the runs. We trained and evaluated all the models using the
Waterbirds dataset on an Intel Xeon Platinum 8268 CPU (24 cores, 2.9 GHz).
CelebA We train for a total of 48,000gradient steps using the Adam optimizer. We set the baseline
batch size to 128and adopt a learning rate scheduler with initial learning rate of 0.0001and decay factor
of 10 for every 16,000gradient steps. In our experiments, we varied the number of channels in the first
ResNet-preact-18 block from 1to64(the number of channels is then scaled up by two in each block). We
trained all the models using the CelebA dataset on NVIDIA 4 x V100 (32 GB) GPU cards.
7Published in Transactions on Machine Learning Research (05/2023)
HAM10000 We train for a total of 12,600and7,000gradient steps using the Adam optimizer for LeNet-5
and ResNet-18 architectures, respectively. We set the baseline batch size to 128and set the learning rate
to0.0001. We adopt a learning rate scheduler with a decay factor of 10 for every 1,260gradient steps for
ResNet-18 models. We trained all the models using the HAM10000 dataset on NVIDIA V100 (32 GB) GPU
cards.
For Waterbirds and CelebA datasets, we performed MinDiff training with values of λ={0.0,0.5,1.0,1.5},
where recall that λcontrols the importance of the fairness objective. λ= 0.0corresponds to training with
the primary loss only, and we refer to the resulting model as the baseline model. To study the effect of batch
sizing, we trained additional models with batch sizes {8,32}. We explored with three different weight decay
strengths ={0.001,0.1,10.0}and two different flood levels, b∈{0.05,0.1}. We report the average ±95%
confidence interval in all figures and tables. For HAM10000 dataset, we performed MinDiff training with
values ofλ={0.0,0.5,1.0,1.5}. We explored with two different weight decay strengths ={0.001,0.1}and
={0.001,0.01}for LeNet-5 and ResNet-18 architectures, respectively. Additionally, we tested the impact of
two different flood levels, which were b∈{0.2,0.3}for LeNet-5 and b∈{0.05,0.1}for ResNet-18.
5 Experimental Results
5.1 Identifying the Interpolation Threshold
To distinguish between under- and over-parameterized models, we begin by identifying the interpolation
threshold; the point where the model is sufficiently large to interpolate the training data (achieve population
level zero error on training data). Figure 1(a) and Figure 1(b) show the population level error on training
data and test error curves for baseline training versus model width for the Waterbirds and CelebA datasets,
respectively, showing interpolation thresholds at model widths of 400 for Waterbirds and 11 for CelebA. On
both the datasets, we also observe the double descent phenomenon (Nakkiran et al., 2020), where the test
errordecreases with increasing model capacity beyond the interpolation threshold. That is, for the original
model (training without MinDiff), the largest models provide high accuracy.
(a) Waterbirds
 (b) CelebA
Figure 4: We plot the fairness constrained test error with a ∆FNR≤10%constraint on the MinDiff trained
models with λ={0.0,0.5,1.0,1.5}on (a) Waterbirds and (b) CelebA datasets respectively. On both these
datasets, we find that MinDiff is only effective for under-parameterized models.
5.2 MinDiff Evaluation
We now re-train our models with MinDiff optimization with λ={0.0,0.5,1.0,1.5}. Figure 2 shows the test
error and FNR gapversus model width for the Waterbirds dataset. In the under-parameterized regime, we
observe that, increasing the MinDiff weights significantly reduces the FNR gapwith only a small drop in test
accuracy. However, in the over-parameterized regime, we find that MinDiff training has no impact on either
test error or the FNR gap.
8Published in Transactions on Machine Learning Research (05/2023)
(a) Waterbirds
 (b) CelebA
Figure 5: We plot the fairness constrained test error with a ∆FNR≤10%constraint on the MinDiff trained
model with λ= 1.5on (a) Waterbirds and (b) CelebA datasets respectively. We find that smaller batch sizes
are only effective around the interpolation threshold.
Table 1: We pick three under-parameterized and four over-parameterized model widths and report the average
test error and FNR gapfor baseline training and several values of λon CelebA dataset. We find that, in the
under-parameterized models, the drop in FNR gapfor MinDiff training vs baseline training is more compared
to that in the over-parameterized models. We report each data point by averaging over 10 runs.
Width λ= 0 λ= 0.5 λ= 1.0 λ= 1.5
Error FNR Gap Error FNR Gap Error FNR Gap Error FNR Gap
5 (Under-) 4.74±0.07 48.77±1.44 5.55±0.39 39.85±3.82 5.46±0.36 40.45±5.58 5.47±0.34 37.06±5.88
7 (Under-) 5.07±0.07 47.35±1.53 5.76±0.5 40.27±2.45 5.64±0.52 42.7±3.12 5.48±0.4 43.05±4.2
9 (Under-) 5.31±0.07 44.78±1.72 5.72±0.4 42.37±1.75 5.77±0.53 40.91±3.28 5.52±0.42 42.43±3.74
13 (Over-) 5.52±0.07 42.79±2.03 5.57±0.1 42.16±2.33 5.66±0.17 42.53±0.98 5.67±0.32 43.07±2.12
19 (Over-) 5.36±0.09 43.98±1.81 5.33±0.05 42.52±1.25 5.42±0.07 42.63±1.01 5.45±0.11 41.34±1.41
55 (Over-) 4.98±0.07 44.14±1.55 5.05±0.07 42.46±1.28 5.24±0.06 42.65±1.85 5.47±0.15 41.93±1.88
64 (Over-) 4.99±0.06 42.54±2.01 5.11±0.06 43.12±2.04 5.28±0.12 41.8±2.32 5.40±0.15 41.56±2.7
Table 1 shows the test error and FNR gapfor selected under- and over-parameterized models on the CelebA
dataset trained with MinDiff. Our conclusions are qualitatively the same as for Waterbirds. We find that,
MinDiff training significantly reduces the FNR gapcompared to the baseline model in the under-parameterized
regime, for example, from a 48%FNR gap without MinDiff to a 37%FNR gap with λ= 1.5for a model
width of 5. On the other hand, MinDiff training compared with the baseline model has a negligible effect on
both test error and FNR gapin the over-parameterized regime, sometimes even resulting in a (small) increase
in both.
Table 2: We tabulate the fairness constrained test error (with a ∆FNR≤10%constraint) of early stopped
MinDiff models (trained with λ={0.5,1.0,1.5}) for different widths. We compare two methods for early
stopping (es) based on the stopping criterion: primary validation loss (es( LP)) and total validation loss
(es(LT)). We find that, for CelebA dataset, MinDiff +es(LP) is better than MinDiff +es(LT).
λ= 0.5 λ= 1.0 λ= 1.5
Width = 5Width = 19Width = 64Width = 5Width = 19Width = 64Width = 5Width = 19Width = 64
No es 6.36±0.51 6.54±0.12 5.88±0.13 6.25±0.43 6.43±0.12 6.06±0.06 6.36±0.35 6.54±0.18 6.07±0.2
es(LP)6.17±0.34 5.91±0.26 5.45±0.22 6.32±0.28 5.82±0.32 4.79±0.17 6.37±0.25 5.63±0.3 4.70±0.07
es(LT)7.76±0.56 6.47±0.31 5.76±0.61 7.79±0.47 7.18±0.58 6.05±0.53 8.65±0.65 7.38±0.77 6.80±0.64
We observe that MinDiff performs poorly for over-parameterized models: Figure 3 shows how the population
level primary loss and FNR gap’s change, evaluated on training and validation datasets, during SGD steps for
under-parameterized and over-parameterized models on the CelebA dataset. We note that at the beginning of
training, the population level FNR gapon training dataset is small because randomly initialized models make
random predictions. These random predictions are fair w.r.t the error rates but not necessarily fair according
9Published in Transactions on Machine Learning Research (05/2023)
to other criteria like Demographic Parity. As training progresses, the over-parameterized model eventually
over-fits the training data at around 20,000 steps, achieving zero primary loss. This model also appears to
be trivially fair from the standpoint of the training data—observe that at the same point, the population
level FNR gapon the training dataset also goes to zero, and no further optimization takes place. On the other
hand, the population level FNR gapduring training remains positive for the under-parameterized model.
Figure 4 plots the fairness constrained test error with a ∆FNR≤10%constraint using post-training threshold
correction on the MinDiff trained models. We can again observe that MinDiff is only effective for under-
parameterized models. We include results for larger λ∈{8.0,16.0,32.0}values in Appendix B and observe
that the findings do not change. For CelebA, the lowest fairness constrained test error is actually achieved by
an under-parameterized model. In other words, achieving fairness via MinDiff optimization requires careful
selection of model width, including exploring the under-parameterized regime .
Table 3: We tabulate the fairness constrained test error (with a ∆FNR≤10%constraint) for different
regularization schemes used in conjunction with MinDiff optimization ( λ= 1.5) on LeNet-5 and ResNet-
preact-18 (with three model widths ∈{5,19,64}) models trained using CelebA dataset. The performance of
best regularizer is highlighted in bold for each model width. Notation: wd is weight decay, es( LP) is early
stopping w.r.t primary loss and fl is flooding.
Method LeNet-5 Width = 5 Width = 19 Width = 64
Under- Under- Over- Over-
λ= 0 9 .73±0.26 5.92±0.19 6.51±0.11 5.76±0.1
λ= 1.5 9 .53±0.17 6.36±0.35 6.54±0.18 6.07±0.2
λ= 1.5+ wd = 0.001 9.87±0.28 5.82±0.26.53±0.16 5.08±0.15
λ= 1.5+ wd = 0.1 12.68±0.03 6.15±0.15 5.82±0.16 6.57±0.1
λ= 1.5+ es(LP) 9.44±0.14 6.37±0.25 5.63±0.3 4.70±0.07
λ= 1.5+ fl = 0.05 9.53±0.17 6.28±0.25 6.30±0.23 5.49±0.17
λ= 1.5+ fl = 0.1 9.52±0.17 6.30±0.35.25±0.25 4.67±0.12
5.3 Impact of Regularization in Over-parameterized Regime
We now examine if additional regularization can help improve the fairness of MinDiff-regularized models in the
over-parameterized regime. Unless otherwise stated, in all subsequent evaluations we perform post-training
threshold correction with a ∆FNR≤10%constraint and compare fairness constrained test error.
Batch sizing only helps around the interpolation threshold. Small batch sizes cause primary training
loss curves to converge more slowly, potentially providing more opportunity for MinDiff optimizations. In
Figure 5(a) and Figure 5(b), we plot fairness constrained test error curves versus model widths for different
batch sizes on the Waterbirds and CelebA datasets, respectively. We find that smaller batch sizes improve
fairness constrained test error only around the interpolation threshold for both the datasets, but do not
noticeably benefit smaller and larger models. On further examination, we note the benefits around the
interpolation threshold are because smaller batch sizes induce stronger regularization effects and push the
interpolation threshold to the right (see Appendix Figure 13). As a result, MinDiff is effective on a slightly
increased range of model widths. However, other than this behaviour, we see no other benefits of using batch
sizing as a regularizer and do not explore it further.
Early stopping criterion. We evaluate two methods for early stopping. The first uses the primary
validation loss (MinDiff+es( LP)) as a stopping criterion, while the second uses total validation loss for
stopping (MinDiff+es( LT)). Figure 6 plots fairness constrained test error versus model width for these two
schemes and different values of λon Waterbirds, and Table 2 shows the same data for CelebA. We find
that both schemes improve fairness for over-parameterized models, but have limited impact in the under-
parameterized regime. For Waterbirds, the differences between the two are small, although using primary
validation loss as the stopping criterion (MinDiff+es( LP)) is marginally better than using total validation
loss (MinDiff+es( LT)). However, for CelebA, we find that primary loss stopping criterion (MinDiff+es( LP))
is substantially better than total loss stopping criterion (MinDiff+es( LT)), especially for large models. Thus,
we use the former for the remainder of our experiments.
10Published in Transactions on Machine Learning Research (05/2023)
Comparing regularization methods on Waterbirds. In Figure 7, we plot the fairness constrained test
error versus model width for different regularization schemes including early stopping ( λ+es), weight decay
with two different values ( λ+wd=0.001,λ+wd=0.1) and flooding ( λ+fl) on Waterbirds. We find that the early
stopping and weight decay regularizes substantially improve fairness for models just below the interpolation
threshold and all over-parameterized models. In contrast, flooding shows only small improvements in fairness.
Forλ= 0.5, we find that early stopping is the best across the board. For λ= 1.5, either early stopping and
weight decay are the best depending on model width, although the differences are small.
(a)λ= 0.5
 (b)λ= 1.0
 (c)λ= 1.5
Figure 6: We plot the fairness constrained test error (with a ∆FNR≤10%constraint) of early stopped
MinDiff models (trained with λ={0.5,1.0,1.5}) for several model widths. We compare two methods for
early stopping (es) based on the stopping criterion: primary validation loss (es( LP)) and total validation
loss (es(LT)). We find that, for Waterbirds dataset, using either stopping criterion will significantly improve
fairness constrained error, especially in the over-parameterized regime.
(a)λ= 0.5
 (b)λ= 1.5
Figure 7: We plot fairness constrained test error (with a ∆FNR≤10%constraint) versus model widths
for different regularization schemes used in conjunction with MinDiff optimization on Waterbirds dataset.
We find that early stopping and weight decay are preferred choice of regularizers for Waterbirds dataset.
Notation: wd is weight decay, es( LP) is early stopping w.r.t primary loss and fl is flooding.
11Published in Transactions on Machine Learning Research (05/2023)
Table 4: We tabulate the fairness constrained test error (with a ∆FNR≤10%constraint) for different
regularization schemes used in conjunction with MinDiff optimization ( λ= 0.5,λ= 1.0andλ= 1.5) on
HAM10000 dataset. The performance of best regularizer is highlighted in bold for each model. Notation: wd
is weight decay, es( LP) is early stopping w.r.t primary loss, es( LT) is early stopping w.r.t total loss and fl is
flooding.
λ= 0.5 λ= 1.0 λ= 1.5
LeNet-5 ResNet-18 LeNet-5 ResNet-18 LeNet-5 ResNet-18
λ= 0 14.64±0.40 10.85±0.22 14.64±0.40 10.85±0.22 14.64±0.40 10.85±0.22
λ 14.60±0.35 11.91±0.12 14.34±0.74 10.86±0.45 14.61±0.53 11.47±0.19
λ+ wd = 0.001 13.97±0.14 12.77±1.02 14.60±0.47 12.34±0.46 15.01±0.64 10.75±0.08
λ+ wd = 0.01 - 10.94±1.29 - 12.74±1.23 - 12.35±0.66
λ+ wd = 0.1 14.27±0.10 - 14.11±0.08 - 14.18±0.24 -
λ+ es(LP) 14.30±0.47 13.41±1.32 15.10±1.14 12.04±1.09 14.73±1.11 13.12±1.31
λ+ es(Lt) 14.20±0.09 12.54±0.77 14.14±0.09 12.51±0.86 14.15±0.16 12.95±1.4
λ+ fl = 0.05 - 12.11±0.37 - 10.68±0.59 - 11.02±0.79
λ+ fl = 0.1 - 11.78±1.07 - 12.64±1.13 - 12.82±0.51
λ+ fl = 0.2 14.47±0.22 - 14.27±0.65 - 14.53±0.56 -
λ+ fl = 0.3 14.77±0.72 - 14.50±0.64 - 15.13±0.89 -
Comparing regularization methods on CelebA. Table 3 compares different regularization schemes
forλ= 1.5on LeNet-5 architecture and three ResNet-preact-18 model widths, each trained on CelebA
dataset. For LeNet-5, we observe that early stopping results in lowest fairness constrained test error. For
ResNet-preact-18 with smallest width, we find that weight decay schemes result in the lowest fairness
constrained test error, while for the large over-parameterized model, flooding works best. Comparing across
all models, we find that, the largest model with flooding provides the overall lowest fairness constrained
test error. Recalling that flooding was ineffective on Waterbirds, we conclude that no one regularizer works
best across datasets and model widths, but additional regularization in general can restore the benefits of
over-parameterization with MinDiff training.
Comparing regularization methods on HAM10000. From Appendix Figure 9, we observe that for
ResNet-18 ( λ= 1.5) the primary loss over-fits to the training dataset and achieves zero training loss thus
turning off MinDiff optimization on the over-parameterized model. In contrast, for the under-parameterized
model, MinDiff optimization is active and aids in improving fairness since training loss is non-zero. Table 4
compares different regularization schemes on the HAM10000 dataset for λ= 0.5,λ= 1.0andλ= 1.5. We can
observe that the fairness constrained test error of MinDiff model is lower for LeNet-5 architecture compared
to baseline model, whereas, for over-parameterized ResNet-18 model, the fairness constrained test error of
baseline model is lower compared to MinDiff model. However, for both the models, regularization improves
the fairness constrained test error for all values of λexcept for ResNet-18, where a smaller value of λ= 0.5
seems to be having no effect on fairness.
6 Conclusion
In this paper, we have critically examined the performance of MinDiff, an in-training fairness regularization
technique implemented within TensorFlow’s Responsible AI toolkit, with respect to DNN model complexity,
with a particular eye towards over-parameterized models. We find that although MinDiff improves the fairness
of under-parameterized models relative to baseline, it is ineffective in improving fairness for over-parameterized
models. As a result, we find that for one of our datasets, under-parameterized MinDiff models have lower
fairness constrained test error than their over-parameterized counterparts, suggesting that time-consuming
searches for best model size might be necessary when MinDiff is used with the goal of training a fair model.
To address these concerns, we explore traditional batch sizing, weight decay and early stopping regularizers
to improve MinDiff training, in addition to flooding, a recently proposed method that is evaluated for
the first time in the context of fair training. We find that batch sizing is ineffective in improving fairness
except for model widths near the interpolation threshold. The other regularizers do improve fairness for
12Published in Transactions on Machine Learning Research (05/2023)
over-parameterized models, but the best regularizer depends on the dataset and model size. In particular,
flooding results in the fairest models on the CelebA dataset, suggesting its utility for MinDiff optimization.
Finally, we show that with appropriate choice of regularizer, over-parameterized MinDiff models regain their
benefits over under-parameterized counterparts even from a fairness lens.
Availability
Code with README.txt file is available at: https://github.com/akshajkumarv/MinDiff
13Published in Transactions on Machine Learning Research (05/2023)
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. A reductions
approach to fair classification. arXiv preprint arXiv:1803.02453, 2018.
Masashi Sode Akihiko Fukuchi, Yoko Yabe. Fairtorch. https://github.com/wbawakate/fairtorch , 2020.
Ibrahim M Alabdulmohsin and Mario Lucic. A near-optimal algorithm for debiasing trained ma-
chine learning models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wort-
man Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34, pp. 8072–
8084. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/
43c656628a4a479e108ed86f7a28a010-Paper.pdf .
David Barrett and Benoit Dherin. Implicit gradient regularization. In International Conference on Learning
Representations , 2021. URL https://openreview.net/forum?id=3q5IqUrkcF .
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H. Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.
Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Allison Woodruff, Christine Luu, Pierre Kreitmann,
Jonathan Bischof, and Ed H. Chi. Putting fairness principles into practice: Challenges, metrics, and
improvements. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society , AIES ’19,
pp. 453–459, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450363242. doi:
10.1145/3306618.3314234. URL https://doi.org/10.1145/3306618.3314234 .
Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. Equity of attention: Amortizing individual
fairness in rankings. The 41st International ACM SIGIR Conference on Research & Development in
Information Retrieval , 2018.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender
classification. In Sorelle A. Friedler and Christo Wilson (eds.), Proceedings of the 1st Conference on
Fairness, Accountability and Transparency , volume 81 of Proceedings of Machine Learning Research , pp.
77–91. PMLR, 23–24 Feb 2018. URL https://proceedings.mlr.press/v81/buolamwini18a.html .
Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency constraints. In
2009 IEEE International Conference on Data Mining Workshops , pp. 13–18, 2009. doi: 10.1109/ICDMW.
2009.83.
Valeriia Cherepanova, Vedant Nanda, Micah Goldblum, John P. Dickerson, and Tom Goldstein. Technical
challenges for training fair neural networks. arXiv preprint arXiv:2102.06764, 2021.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments. Big Data , 5(2):153–163, 2017. doi: 10.1089/big.2016.0047. URL https://doi.org/10.1089/
big.2016.0047 . PMID: 28632438.
Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Leveraging
Labeled and Unlabeled Data for Consistent Fair Binary Classification . Curran Associates Inc., Red Hook,
NY, USA, 2019.
Guillem Collell, Drazen Prelec, and Kaustubh R. Patil. Reviving threshold-moving: a simple plug-in bagging
ensemble for binary and multiclass imbalanced data. ArXiv, abs/1606.08698, 2016.
Roxana Daneshjou, Kailas Vodrahalli, Roberto A. Novoa, Melissa Jenkins, Weixin Liang, Veronica Rotemberg,
Justin Ko, Susan M. Swetter, Elizabeth E. Bailey, Olivier Gevaert, Pritam Mukherjee, Michelle Phung,
Kiana Yekrang, Bradley Fong, Rachna Sahasrabudhe, Johan A. C. Allerup, Utako Okata-Karigane,
James Zou, and Albert S. Chiou. Disparities in dermatology ai performance on a diverse, curated
clinical image set. Science Advances , 8(32):eabq6147, 2022. doi: 10.1126/sciadv.abq6147. URL https:
//www.science.org/doi/abs/10.1126/sciadv.abq6147 .
14Published in Transactions on Machine Learning Research (05/2023)
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas
Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer,
Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver,
Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu,
Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar,
Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas
Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision
transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023.
Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating
unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,
and Society , AIES ’18, pp. 67–73, New York, NY, USA, 2018. Association for Computing Machinery. ISBN
9781450360128. doi: 10.1145/3278721.3278729. URL https://doi.org/10.1145/3278721.3278729 .
Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano Pontil. Empirical risk
minimization under fairness constraints. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems , NIPS’18, pp. 2796–2806, Red Hook, NY, USA, 2018. Curran Associates
Inc.
Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian.
Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining , KDD ’15, pp. 259–268, New York, NY, USA,
2015. Association for Computing Machinery. ISBN 9781450336642. doi: 10.1145/2783258.2783311. URL
https://doi.org/10.1145/2783258.2783311 .
Naman Goel, Mohammad Yaghini, and Boi Faltings. Non-discriminatory machine learning through convex
fairness criteria. Proceedings of the AAAI Conference on Artificial Intelligence , 32(1), Apr. 2018. URL
https://ojs.aaai.org/index.php/AAAI/article/view/11662 .
Patrick Grother, George Quinn, and P Phillips. Report on the evaluation of 2d still-image face recognition
algorithms. NIST Interagency/Internal Report (NISTIR), National Institute of Standards and Technology,
Gaithersburg, MD, 2010-06-17 2010.
Moritz Hardt, Eric Price, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In
D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems , volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/
paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf .
Tatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without
demographics in repeated loss minimization. In ICML, 2018.
Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama. Do we need zero training
loss after achieving zero training error? In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research ,
pp. 4604–4614. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/ishida20a.html .
Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair classification.
In Ryan P. Adams and Vibhav Gogate (eds.), Proceedings of The 35th Uncertainty in Artificial Intelligence
Conference , volume 115 of Proceedings of Machine Learning Research , pp. 862–872. PMLR, 22–25 Jul 2020.
URL https://proceedings.mlr.press/v115/jiang20a.html .
Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age
for bias measurement and mitigation. In Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision , pp. 1548–1558, 2021.
Amir E. Khandani, Adlar J. Kim, and Andrew W. Lo. Consumer credit-risk models via machine-learning
algorithms. Journal of Banking & Finance , 34(11):2767–2787, 2010. ISSN 0378-4266. doi: https://
doi.org/10.1016/j.jbankfin.2010.06.001. URL https://www.sciencedirect.com/science/article/pii/
S0378426610002372 .
15Published in Transactions on Machine Learning Research (05/2023)
Anders Krogh and John Hertz. A simple weight decay can improve generalization. In J. Moody,
S. Hanson, and R. P. Lippmann (eds.), Advances in Neural Information Processing Systems ,
volume 4. Morgan-Kaufmann, 1992. URL https://proceedings.neurips.cc/paper/1991/file/
8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf .
Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed H. Chi.
Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114,
2020.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably
robust to label noise for overparameterized neural networks. arXiv preprint arXiv:1903.11680, 2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable
representations. arXiv preprint arXiv:1802.06309, 2018.
Subha Maity, Saptarshi Roy, Songkai Xue, Mikhail Yurochkin, and Yuekai Sun. How does overparametrization
affect performance on minority groups? arXiv preprint arXiv:2206.03515, 2022. URL https://arxiv.
org/abs/2206.03515 .
Roman C. Maron, Michael Weichenthal, Jochen S. Utikal, Achim Hekler, Carola Berking, Axel Hauschild,
Alexander H. Enk, Sebastian Haferkamp, Joachim Klode, Dirk Schadendorf, Philipp Jansen, Tim Holland-
Letz, Bastian Schilling, Christof von Kalle, Stefan Fröhling, Maria R. Gaiser, Daniela Hartmann, Anja
Gesierich, Katharina C. Kähler, Ulrike Wehkamp, Ante Karoglan, Claudia Bär, Titus J. Brinker, Laurenz
Schmitt, Wiebke K. Peitsch, Friederike Hoffmann, Jürgen C. Becker, Christina Drusio, Philipp Jansen,
Joachim Klode, Georg Lodde, Stefanie Sammet, Dirk Schadendorf, Wiebke Sondermann, Selma Ugurel,
Jeannine Zader, Alexander Enk, Martin Salzmann, Sarah Schäfer, Knut Schäkel, Julia Winkler, Priscilla
Wölbing, Hiba Asper, Ann-Sophie Bohne, Victoria Brown, Bianca Burba, Sophia Deffaa, Cecilia Dietrich,
Matthias Dietrich, Katharina Antonia Drerup, Friederike Egberts, Anna-Sophie Erkens, Salim Greven, Viola
Harde, Marion Jost, Merit Kaeding, Katharina Kosova, Stephan Lischner, Maria Maagk, Anna Laetitia
Messinger, Malte Metzner, Rogina Motamedi, Ann-Christine Rosenthal, Ulrich Seidl, Jana Stemmermann,
Kaspar Torz, Juliana Giraldo Velez, Jennifer Haiduk, Mareike Alter, Claudia Bär, Paul Bergenthal, Anne
Gerlach, Christian Holtorf, Ante Karoglan, Sophie Kindermann, Luise Kraas, Moritz Felcht, Maria R.
Gaiser, Claus-Detlev Klemke, Hjalmar Kurzen, Thomas Leibing, Verena Müller, Raphael R. Reinhard,
Jochen Utikal, Franziska Winter, Carola Berking, Laurie Eicher, Daniela Hartmann, Markus Heppt,
Katharina Kilian, Sebastian Krammer, Diana Lill, Anne-Charlotte Niesert, Eva Oppel, Elke Sattler,
Sonja Senner, Jens Wallmichrath, Hans Wolff, Tina Giner, Valerie Glutsch, Andreas Kerstan, Dagmar
Presser, Philipp Schrüfer, Patrick Schummer, Ina Stolze, Judith Weber, Konstantin Drexler, Sebastian
Haferkamp, Marion Mickler, Camila Toledo Stauner, and Alexander Thiem. Systematic outperformance of
112 dermatologists in multiclass skin cancer image classification by convolutional neural networks. European
Journal of Cancer , 119:57–65, 2019. ISSN 0959-8049. doi: https://doi.org/10.1016/j.ejca.2019.06.013. URL
https://www.sciencedirect.com/science/article/pii/S0959804919303818 .
Natalia Martinez, Martin Bertran, and Guillermo Sapiro. Minimax PARETO Fairness: A multi objective
perspective. arXiv preprint arXiv:2011.01821, 2020.
Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv
Kumar. Long-tail learning via logit adjustment. In International Conference on Learning Representations ,
2021a. URL https://openreview.net/forum?id=37nvvqkCo5 .
Aditya Krishna Menon, Ankit Singh Rawat, and Sanjiv Kumar. Overparameterisation and worst-case
generalisation: friend or foe? In International Conference on Learning Representations , 2021b. URL
https://openreview.net/forum?id=jphnJNOwe36 .
N. Morgan and H. Bourlard. Generalization and Parameter Estimation in Feedforward Nets: Some Ex-
periments , pp. 630–637. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1990. ISBN
1558601007.
16Published in Transactions on Machine Learning Research (05/2023)
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations , 2020. URL https://openreview.net/forum?id=B1g5sA4twr .
Mei Ngan and Patrick Grother. Face recognition vendor test (frvt) - performance of automated gender
classification algorithms. NIST Interagency/Internal Report (NISTIR), National Institute of Standards
and Technology, Gaithersburg, MD, 2015-04-20 2015.
Manisha Padala and Sujit Gujar. Fnnc: Achieving fairness through neural networks. In Proceedings
of the Twenty-Ninth International Joint Conference on Artificial Intelligence , IJCAI’20, 2021. ISBN
9780999241165.
Alan Pham, Eunice Chan, Vikranth Srivatsa, Dhruba Ghosh, Yaoqing Yang, Yaodong Yu, Ruiqi Zhong,
Joseph E. Gonzalez, and Jacob Steinhardt. The effect of model size on worst-group generalization. arXiv
preprint arXiv:2112.04094, 2021. URL https://arxiv.org/abs/2112.04094 .
Flavien Prost, Hai Qian, Qiuwen Chen, Ed H. Chi, Jilin Chen, and Alex Beutel. Toward a better trade-off
between performance and fairness with kernel-based distribution matching. In NeurIPS Workshop on ML
with Guarantees , 2019.
Novi Quadrianto, Viktoriia Sharmanska, and Oliver Thomas. Discovering fair representations in the data
domain. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Responsible-AI. Tensorflow Responsible AI Toolkit, Accessed 16.02.2022. URL https://www.tensorflow.
org/responsible_ai .
Hee Jung Ryu, Hartwig Adam, and Margaret Mitchell. Inclusivefacenet: Improving face attribute detection
with race and gender diversity. arXiv preprint arXiv:1712.00193, 2018.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural
networks. In International Conference on Learning Representations , 2020a. URL https://openreview.
net/forum?id=ryxGuJrFvS .
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why over-
parameterization exacerbates spurious correlations. In ICML, pp. 8346–8356, 2020b. URL http:
//proceedings.mlr.press/v119/sagawa20a.html .
Yash Savani, Colin White, and Naveen Sundar Govindarajulu. Intra-processing methods for debiasing neural
networks. arXiv preprint arXiv:2006.08564, 2020.
Candice Schumann, Jeffrey S. Foster, Nicholas Mattei, and John P. Dickerson. We need fairness and
explainability in algorithmic hiring. In Proceedings of the 19th International Conference on Autonomous
Agents and MultiAgent Systems , AAMAS ’20, pp. 1716–1720, Richland, SC, 2020. International Foundation
for Autonomous Agents and Multiagent Systems. ISBN 9781450375184.
Ashudeep Singh and Thorsten Joachims. Fairness of exposure in rankings. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery &; Data Mining , KDD ’18, pp. 2219–2228,
New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450355520. doi: 10.1145/
3219819.3220088. URL https://doi.org/10.1145/3219819.3220088 .
Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit regularization
in stochastic gradient descent. In International Conference on Learning Representations , 2021. URL
https://openreview.net/forum?id=rq_Qr0c1Hyo .
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A
simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(56):
1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html .
17Published in Transactions on Machine Learning Research (05/2023)
Philipp Tschandl. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common
pigmented skin lesions, 2018. URL https://doi.org/10.7910/DVN/DBW86T .
Christina Wadsworth, Francesca Vera, and Chris Piech. Achieving fairness through adversarial learning: an
application to recidivism prediction. arXiv preprint arXiv:1807.00199, 2018.
Yoav Wald, Gal Yona, Uri Shalit, and Yair Carmon. Malign overfitting: Interpolation and invariance are
fundamentally at odds. In NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and
Applications , 2022. URL https://openreview.net/forum?id=1xadmcm2CC .
Mei Wang and Weihong Deng. Mitigate bias in face recognition using skewness-aware reinforcement learning.
arXiv preprint arXiv:1911.10692, 2019.
Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga
Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. arXiv preprint
arXiv:1911.11834, 2020.
Dennis Wei, Karthikeyan Natesan Ramamurthy, and Flavio Calmon. Optimized score transformation for fair
classification. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International
Conference on Artificial Intelligence and Statistics , volume 108 of Proceedings of Machine Learning Research ,
pp. 1673–1683. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/wei20a.html .
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness
constraints: A flexible approach for fair classification. Journal of Machine Learning Research , 20(75):1–42,
2019. URL http://jmlr.org/papers/v20/18-262.html .
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning.
arXiv preprint arXiv:1801.07593, 2018.
Zhi-Hua Zhou and Xu-Ying Liu. Training cost-sensitive neural networks with methods addressing the class
imbalance problem. IEEE Transactions on Knowledge and Data Engineering , 18(1):63–77, 2006. doi:
10.1109/TKDE.2006.17.
Yongshuo Zong, Yongxin Yang, and Timothy Hospedales. MEDFAIR: Benchmarking fairness for medical
imaging. In The Eleventh International Conference on Learning Representations , 2023. URL https:
//openreview.net/forum?id=6ve2CkeQe5S .
18Published in Transactions on Machine Learning Research (05/2023)
A Maximum Mean Discrepancy
Maximum Mean Discrepancy (MMD) is a statistic to test the statistical dependency between two sample
distributions. It’s mathematical formulation consists of taking the mean between two samples Z0(consisting
ofmelements) and Z1(consisting of nelements) mapped into a Reproducing Kernel Hilbert Subspace, and
is computed as follows (Prost et al., 2019):
MMD(Z0,Z1) =1
m2m/summationdisplay
i,j=1k(z0,i,z0,j)−2
mnm,n/summationdisplay
i,j=1k(z0,i,z1,j) +1
n2n/summationdisplay
i,j=1k(z1,i,z1,j) (7)
wherekis either a Gaussian or a Laplace kernel, (z0,i)i=1,mand(z1,i)i=1,nare elements from Z0andZ1,
respectively.
B MinDiff Evaluation
To study the sensitivity analysis on λ, we plot the fairness constrained test error (with a ∆FNR≤10%
constraint) on Waterbirds dataset for different values of λ={0,1.5,8.0,16.0,32.0}in Figure 8. We observe
that MinDiff is only effective for under-parameterized models even for larger values of λ, but provides no
significant gains compared to smaller values of λ. We also observe that when λis set to high values, like λ=
8.0 or 16.0 or 32.0, the optimization prioritizes minimizing FNR gaprather than accurately classifying the
data, leading to poor classification performance in the over-parameterized regime. Therefore, it is crucial to
choose lambda carefully in order to strike a balance between classification accuracy and fairness for different
model widths.
Figure 8: We plot the fairness constrained test error with a ∆FNR≤10%constraint on the MinDiff trained
models with λ={0.0,1.5,8.0,16.0,32.0}on Waterbirds dataset.
On HAM10000 dataset, we observe that for Resnet-18 the primary loss over-fits to the training dataset
and achieves zero training loss thus turning off MinDiff optimization on the over-parameterized model. In
contrast, for the under-parameterized model, MinDiff optimization is active and aids in improving fairness
since training loss is non-zero.
C Impact of Regularization in Over-parameterized Regime
C.1 Batch Size
C.1.1 Waterbirds
19Published in Transactions on Machine Learning Research (05/2023)
(a) LeNet-5
 (b) ResNet-18
Figure 9: We show the progress of primary loss, evaluated on training dataset and on the validation
dataset, versus epochs during MinDiff optimization ( λ= 1.5) for under-parameterized (LeNet-5) and over-
parameterized (ResNet-18) models on HAM10000 dataset. We find that over-parameterized ResNet-18 model
over-fits to the training data, thus turning off MinDiff optimization.
(a) MinDiff ( λ= 0.5)
 (b) MinDiff ( λ= 1.0)
 (c) MinDiff ( λ= 1.5)
Figure 10: Batch size - THR with fairness constraint <10%
C.2 All Other Regularizers
C.2.1 Waterbirds
20Published in Transactions on Machine Learning Research (05/2023)
(a) MinDiff ( λ= 0.5)
 (b) MinDiff ( λ= 1.0)
 (c) MinDiff ( λ= 1.5)
Figure 11: Batch size - THR with fairness constraint <5%
(a) MinDiff ( λ= 0.5)
 (b) MinDiff ( λ= 1.0)
 (c) MinDiff ( λ= 1.5)
Figure 12: Batch size - THR with fairness constraint <1%
C.2.2 CelebA
Table 5: We tabulate the fairness constrained test error (with a ∆FNR≤10%constraint) for different
regularization schemes used in conjunction with MinDiff optimization ( λ= 0.5) on CelebA dataset. The
performance of best regularizer is highlighted in bold for each model width. Notation: wd is weight decay,
es(LP) is early stopping w.r.t primary loss and fl is flooding
Method Width = 5 Width = 19 Width = 64
Under- Over- Over-
λ= 0 5 .92±0.27 6.51±0.16 5.76±0.14
λ= 0.5 6 .36±0.72 6.54±0.17 5.88±0.19
λ= 0.5+ wd = 0.001 5.89±0.32 6.94±0.18 6.25±0.48
λ= 0.5+ wd = 0.1 5.66±0.18 5.27±0.16 6.40±0.18
λ= 0.5+ es(LP) 6.17±0.48 5.91±0.37 5.45±0.32
λ= 0.5+ fl = 0.05 6.45±0.69 6.53±0.21 6.16±0.09
λ= 0.5+ fl = 0.1 6.34±0.63 5.75±0.35 4.80±0.28
21Published in Transactions on Machine Learning Research (05/2023)
(a) MinDiff ( λ= 0.5)
 (b) MinDiff ( λ= 1.0)
 (c) MinDiff ( λ= 1.5)
Figure 13: Training loss vs model widths for different batch sizes
(a) MinDiff ( λ= 0.5)
 (b) MinDiff ( λ= 1.0)
 (c) MinDiff ( λ= 1.5)
Figure 14: Regularization - THR with fairness constraint <10%
Table 6: We tabulate the fairness constrained test error (with a ∆FNR≤10%constraint) for different
regularization schemes used in conjunction with MinDiff optimization ( λ= 1.0) on CelebA dataset. The
performance of best regularizer is highlighted in bold for each model width. Notation: wd is weight decay,
es(LP) is early stopping w.r.t primary loss and fl is flooding
Method Width = 5 Width = 19 Width = 64
Under- Over- Over-
λ= 0 5 .92±0.27 6.51±0.16 5.76±0.14
λ= 1.0 6 .25±0.61 6.43±0.17 6.06±0.09
λ= 1.0+ wd = 0.001 5.94±0.39 6.75±0.35 5.90±0.24
λ= 1.0+ wd = 0.1 5.80±0.17 5.55±0.19 6.42±0.12
λ= 1.0+ es(LP) 6.32±0.40 5.82±0.46 4.79±0.24
λ= 1.0+ fl = 0.05 6.30±0.68 6.43±0.17 5.87±0.11
λ= 1.0+ fl = 0.1 6.33±0.52 5.4±0.35 4.77±0.13
22Published in Transactions on Machine Learning Research (05/2023)
(a) MinDiff ( λ= 0.5)
 (b) MinDiff ( λ= 1.0)
 (c) MinDiff ( λ= 1.5)
Figure 15: Regularization - THR with fairness constraint <5%
(a) MinDiff ( λ= 0.5)
 (b) MinDiff ( λ= 1.0)
 (c) MinDiff ( λ= 1.5)
Figure 16: Regularization - THR with fairness constraint <1%
Table 7: CelebA (MinDiff: 0.5, FNR Gap ≤5%, Batch Size: 128)
Method Width = 5 Width = 19 Width = 64
Under- Over- Over-
Original Model 6.22±0.31 6 .81±0.21 6 .10±0.14
MinDiff 6.72±0.70 6 .78±0.16 6 .18±0.25
MinDiff + wd = 0.001 6.20±0.35 7 .29±0.21 6 .51±0.55
MinDiff + wd = 0.1 5.84±0.13 5 .51±0.15 6.70±0.14
MinDiff + es 6.48±0.48 6 .14±0.38 5 .61±0.30
MinDiff + fl = 0.05 6.73±0.70 6 .80±0.21 6 .47±0.11
MinDiff + fl = 0.1 6.65±0.67 5 .99±0.37 5.03±0.29
23Published in Transactions on Machine Learning Research (05/2023)
Table 8: CelebA (MinDiff: 1.0, FNR Gap ≤5%, Batch Size: 128)
Method Width = 5 Width = 19 Width = 64
Under- Over- Over-
Original Model 6.22±0.31 6 .81±0.21 6 .10±0.14
MinDiff 6.57±0.56 6 .74±0.20 6 .35±0.07
MinDiff + wd = 0.001 6.26±0.45 7 .11±0.34 6 .21±0.23
MinDiff + wd = 0.1 6.15±0.18 5.82±0.14 6 .70±0.14
MinDiff + es 6.60±0.40 6 .15±0.50 5 .05±0.18
MinDiff + fl = 0.05 6.57±0.70 6 .74±0.14 6 .15±0.15
MinDiff + fl = 0.1 6.58±0.43 5.67±0.38 4 .92±0.06
Table 9: CelebA (MinDiff: 1.5, FNR Gap ≤5%, Batch Size: 128)
Method Width = 5 Width = 19 Width = 64
Under- Over- Over-
Original Model 6.22±0.31 6 .81±0.21 6 .10±0.14
MinDiff 6.65±0.46 6 .90±0.30 6 .40±0.26
MinDiff + wd = 0.001 6.15±0.41 6.85±0.30 5 .33±0.21
MinDiff + wd = 0.1 6.45±0.18 6 .11±0.13 6 .81±0.10
MinDiff + es 6.73±0.43 5 .89±0.45 4 .88±0.05
MinDiff + fl = 0.05 6.58±0.39 6 .64±0.40 5 .77±0.18
MinDiff + fl = 0.1 6.68±0.44 5.56±0.41 4 .86±0.14
Table 10: CelebA (MinDiff: 0.5, FNR Gap ≤1%, Batch Size: 128)
Method Width = 5 Width = 19 Width = 64
Under- Over- Over-
Original Model 6.57±0.32 7 .13±0.22 6 .31±0.12
MinDiff 6.96±0.72 7 .07±0.14 6 .50±0.26
MinDiff + wd = 0.001 6.52±0.35 7 .57±0.24 6 .86±0.55
MinDiff + wd = 0.1 6.12±0.13 5 .82±0.18 6.94±0.20
MinDiff + es 6.71±.048 6 .41±0.42 5 .85±0.29
MinDiff + fl = 0.05 7.04±0.70 7 .09±0.24 6 .78±0.10
MinDiff + fl = 0.1 6.94±0.67 6 .20±0.42 5.17±0.28
Table 11: CelebA (MinDiff: 1.0, FNR Gap ≤1%, Batch Size: 128)
Method Width = 5 Width = 19 Width = 64
Under- Over- Over-
Original Model 6.57±0.32 7 .13±0.22 6 .31±0.12
MinDiff 6.76±0.54 7 .00±0.18 6 .55±0.14
MinDiff + wd = 0.001 6.54±0.46 7 .43±0.41 6 .46±0.20
MinDiff + wd = 0.1 6.43±0.22 6.11±0.12 6 .97±0.17
MinDiff + es 6.86±0.42 6 .40±0.53 5 .24±0.21
MinDiff + fl = 0.05 6.87±0.66 7 .04±0.17 6 .33±0.15
MinDiff + fl = 0.1 6.90±0.41 5.94±0.33 5 .09±0.08
24Published in Transactions on Machine Learning Research (05/2023)
Table 12: CelebA (MinDiff: 1.5, FNR Gap ≤1%, Batch Size: 128)
Method Width = 5 Width = 19 Width = 64
Under- Over- Over-
Original Model 6.57±0.32 7 .13±0.22 6 .31±0.12
MinDiff 6.86±0.49 7 .24±0.31 6 .64±0.26
MinDiff + wd = 0.001 6.42±0.45 7.13±0.30 5 .68±0.25
MinDiff + wd = 0.1 6.75±0.22 6 .29±0.17 7 .05±0.14
MinDiff + es 6.97±0.38 6 .12±0.49 5 .07±0.06
MinDiff + fl = 0.05 6.85±0.39 6 .92±0.42 5 .93±0.24
MinDiff + fl = 0.1 6.89±0.45 5.84±0.42 5 .04±0.16
25