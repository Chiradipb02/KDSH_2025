Published in Transactions on Machine Learning Research (02/2023)
Action Poisoning Attacks on Linear Contextual Bandits
Guanlin Liu glnliu@ucdavis.edu
Department of Electrical and Computer Engineering
University of California, Davis
Lifeng Lai lflai@ucdavis.edu
Department of Electrical and Computer Engineering
University of California, Davis
Reviewed on OpenReview: https: // openreview. net/ forum? id= yhGCKUsKJS
Abstract
Contextual bandit algorithms have many applicants in a variety of scenarios. In order
to develop trustworthy contextual bandit systems, understanding the impacts of various
adversarial attacks on contextual bandit algorithms is essential. In this paper, we propose
a new class of attacks: action poisoning attacks, where an adversary can change the action
signal selected by the agent. We design action poisoning attack schemes against disjoint
linear contextual bandit algorithms in both white-box and black-box settings. We further
analyze the cost of the proposed attack strategies for a very popular and widely used bandit
algorithm: LinUCB. We show that, in both white-box and black-box settings, the proposed
attack schemes can force the LinUCB agent to pull a target arm very frequently by spending
only logarithm cost. We also extend the proposed attack strategies to generalized linear
models and show the effectiveness of the proposed strategies.
1 Introduction
Multiple armed bandits(MABs), a popular framework of sequential decision making model, has been widely
investigated and has many applicants in a variety of scenarios (Chapelle et al., 2014; Lai et al., 2011; Kveton
et al., 2015). The contextual bandits model is an extension of the multi-armed bandits model with contextual
information. At each round, the reward is associated with both the arm (a.k.a, action) and the context.
Contextual bandits algorithms have a broad range of applications, such as recommender systems (Li et al.,
2010), wireless networks (Saxena et al., 2019), etc.
In the modern industry-scale applications of bandit algorithms, action decisions, reward signal collection,
and policy iterations are normally implemented in a distributed network. Action decisions and reward
signals may need to be transmitted over communication links. For example, in recommendation systems, the
transitionsofthedecisionsandtherewardsignalrelyonafeedbackloopbetweentherecommendationsystem
and the user. When data packets containing the reward signals and action decisions etc are transmitted
through the network, the adversary can implement adversarial attacks by intercepting and modifying these
data packets. As the result, poisoning attacks on contextual bandits could possibly happen. In many
applications of contextual bandits, an adversary may have an incentive to force the contextual bandits system
to learn a specific policy. For example, a restaurant may attack the bandit systems to force the systems into
increasing the restaurant’s exposure. Thus, understanding the risks of different kinds of adversarial attacks
on contextual bandits is essential for the safe applications of the contextual bandit model and designing
robust contextual bandit systems.
Depending on the target of the poisoning attacks, the poisoning attacks against contextual linear bandits
can be categorized into four types: reward poisoning attack, action poisoning attack, context poisoning
attack and the mix of them. In this paper, we aim to investigate the impact of action poisoning attacks
on contextual bandit models. To our best knowledge, this paper is the first work to analyze the impact
1Published in Transactions on Machine Learning Research (02/2023)
of action poisoning attacks on contextual bandit models. Detailed comparisons of various types of attacks
against contextual bandits will be provided in Section 3. We note that the goal of this paper is not to
promote any particular type of poisoning attack. Rather, our goal is to understand the potential risks of
action poisoning attacks. We note that for the safe applications and design of robust contextual bandit
algorithms, it is essential to address all possible weaknesses of the models and understanding the risks of
different kinds of adversarial attacks. Our hope is that the understanding of the potential risks of action
poisoning attacks could motivate follow up research to design algorithms that are robust to such attacks.
In this paper, we study the action poisoning attack against disjoint linear contextual bandit (Li et al., 2010;
Kong et al., 2020; Garcelon et al., 2020; Huang et al., 2021) in both white-box and black-box settings. In the
white-box setting, we assume that the attacker knows the coefficient vectors associated with arms. Thus,
at each round, the attacker knows the mean rewards of all arms. While it is often unrealistic to exactly
know the coefficient vectors, the understanding of the white-box attacks could provide valuable insights on
how to design the more practical black-box attacks. In the black-box setting, we assume that the attacker
has no prior information about the arms and does not know the agent’s algorithm. The limited information
that the attacker has are the context information, the action signal chosen by the agent, and the reward
signal generated from the environment. In both white-box and black-box settings, the attacker aims to
manipulate the agent into frequently pulling a target arm chosen by the attacker with a minimum cost. The
cost is measublack by the number of rounds that the attacker changes the actions selected by the agent. The
contributions of this paper are:
(1)We propose a new online action poisoning attack against contextual bandit in which the attacker aims to
force the agent to frequently pull a target arm chosen by the attacker via strategically changing the agent’s
actions.
(2)We introduce a white-box attack strategy that can manipulate any sublinear-regret disjoint linear con-
textual bandit agent into pulling a target arm T−o(T)rounds over a horizon of Trounds, while incurring
a cost that is sublinear dependent on T.
(3)We design a black-box attack strategy whose performance nearly matches that of the white-box attack
strategy. We apply the black-box attack strategy against a very popular and widely used bandit algorithm:
LinUCB (Li et al., 2010). We show that our proposed attack scheme can force the LinUCB agent into pulling
a target arm T−O(log3T)times with attack cost scaling as O(log3T).
(4)We extend the proposed attack strategies to generalized linear contextual bandits. We further analyze
thecostoftheproposedattackstrategiesforageneralizedlinearcontextualbanditalgorithm: UCB-GLM(Li
et al., 2017).
(5)We evaluate our attack strategies using both synthetic and real datasets. We observe empirically that
the total cost of our black-box attack is sublinear for a variety of contextual bandit algorithms.
2 Related Work
In this section, we discuss related works on two parts: attacks that cause standard bandit algorithms to fail
and robust bandit algorithms that can defend attacks.
AttacksModels. Whiletherearemanyexistingworksaddressingadversarialattacksonsupervisedlearning
models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017; Cohen et al., 2019; Dohmatob, 2019; Wang et al.,
2019; Dasgupta et al., 2019; Cicalese et al., 2020), the understanding of adversarial attacks on contextual
bandit models is less complete. Of particular relevance to our work is a line of interesting recent work
on adversarial attacks on MABs (Jun et al., 2018; Liu & Shroff, 2019; Liu & Lai, 2020b) and on linear
contextual bandits (Ma et al., 2018; Garcelon et al., 2020). In recent works in MABs setting, the types of
attacks include both reward poisoning attacks and action poisoning attacks. In the reward poisoning attacks,
there is an adversary who can manipulate the reward signal received by the agent (Jun et al., 2018; Liu &
Shroff, 2019). In the action poisoning attacks, the adversary can manipulate the action signal chosen by the
agent before the environment receives it (Liu & Lai, 2020b). Existing works on adversarial attacks against
linear contextual bandits focus on the reward (Ma et al., 2018; Garcelon et al., 2020) or context poisoning
2Published in Transactions on Machine Learning Research (02/2023)
attacks (Garcelon et al., 2020). In the context poisoning attacks, the adversary can modify the context
observed by the agent without changing the reward associated with the context. (Wang et al., 2022) defines
the concept of attackability of linear stochastic bandits and introduces the sufficient and necessary conditions
on attackability. There are also some recent interesting work on adversarial attacks against reinforcement
learning (RL) algorithms under various setting (Behzadan & Munir, 2017; Huang & Zhu, 2019; Ma et al.,
2019; Zhang et al., 2020; Sun et al., 2021; Rakhsha et al., 2020; 2021; Liu & Lai, 2021; Rangi et al., 2022).
Although there are some recent works on the action poisoning attacks against MABs and RL, the action
poisoning attack on contextual linear bandit is not a simple extension of the case of MAB or RL. Firstly,
in the MAB settings the rewards only depend on the arm (action), while in the contextual bandit setting,
the rewards depend on both the arm and the context (state). Secondly, (Liu & Lai, 2021) discusses the
action poisoning attack in the tabular RL case where the number of states is finite. In the linear contextual
bandit problem, the number of contexts is infinite. These factors make the design of attack strategies and
performance analysis for the contextual linear bandit problems much more challenging.
Robustalgorithms. Lotsofeffortshavebeenmadetodesignrobustbanditalgorithmstodefendadversarial
attacks. In the MABs setting, (Lykouris et al., 2018) introduces a bandit algorithm, called Multilayer Active
Arm Elimination Race algorithm, that is robust to reward poisoning attacks. (Gupta et al., 2019) presents
an algorithm named BARBAR that is robust to reward poisoning attacks and the regret of the proposed
algorithm is nearly optimal. (Guan et al., 2020) proposes algorithms that are robust to a reward poisoning
attack model where an adversary attacks with a certain probability at each round. (Feng et al., 2020)
proves that Thompson Sampling, UCB, and ϵ-greedy can be modified to be robust to self-interested reward
poisoning attacks. (Liu & Lai, 2020a) introduce a bandit algorithm, called MOUCB, that is robust to action
poisoning attacks. The algorithms for the context-free stochastic multi-armed bandit (MAB) settings are
not suited for our settings. In the linear contextual bandit setting, (Bogunovic et al., 2021) proposes two
variants of stochastic linear bandit algorithm that is robust to reward poisoning attacks, which separately
work on known attack budget case and agnostic attack budget case. (Bogunovic et al., 2021) also proves
that a simple greedy algorithm based on linear regression can be robust to linear contextual bandits with
shablack coefficient under a stringent diversity assumption on the contexts. (Ding et al., 2022) provides a
robust linear contextual bandit algorithm that works under both the reward poisoning attacks and context
poisoning attacks. (He et al., 2022) provides nearly optimal algorithms for linear contextual bandits with
adversarial corruptions.
3 Problem Setup
3.1 Review of Linear Contextual Bandit
Consider the standard disjoint linear contextual bandit model in which the environment consists of Karms.
In each round t= 1,2,3,...,T, the agent observes a context xt∈DwhereD⊂Rd, pulls an arm Itand
receives a reward rt,It. Each arm iis associated with an unknown but fixed coefficient vector θi∈Θwhere
Θ⊂Rd. In each round t, the reward satisfies
rt,It=⟨xt,θIt⟩+ηt, (1)
whereηtis a conditionally independent zero-mean R-subgaussian noise and ⟨·,·⟩denotes the inner product.
Hence, the expected reward of arm iunder context xtfollows the linear setting
E[rt,i] =⟨xt,θi⟩ (2)
for alltand all arm i. If we consider the σ-algebraFt=σ(η1,...,ηt),ηtbecomesFtmeasurable which
is a conditionally independent zero-mean random variable. The agent aims to minimize the cumulative
pseudo-regret
¯RT=T/summationdisplay
t=1/parenleftbig
⟨xt,θI∗
t⟩−⟨xt,θIt⟩/parenrightbig
,
whereI∗
t= arg max i⟨xt,θi⟩.
3Published in Transactions on Machine Learning Research (02/2023)
In this paper, we assume that there exist L>0andS >0, such that for all round tand armi,∥xt∥2≤L
and∥θi∥2≤S, where∥·∥2denotes the ℓ2-norm. We assume that there exist D⊂Rdsuch that for all x∈D
and all arm i,⟨x,θi⟩>0. In addition, for all t,xt∈D.
3.2 Action Poisoning Attack Model
In this paper, we introduce a novel adversary setting, in which the attacker can manipulate the action chosen
by the agent. In particular, at each round t, after the agent chooses an arm It, the attacker can manipulate
the agent’s action by changing Itto another I0
t∈{1,...,K}. If the attacker decides not to attack, I0
t=It.
The environment generates a random reward rt,I0
tbased on the post-attack arm I0
tand the context xt.
Then the agent and the attacker receive reward rt,I0
tfrom the environment. Since the agent does not know
the attacker’s manipulations and the presence of the attacker, the agent will still view rt,I0
tas the reward
corresponding to the arm It. The action poisoning attack model is summarized in Algorithm 1 .
Algorithm 1 Action poisoning attacks on contextual linear bandit agent
1:fort= 1,2,...,Tdo
2:Agent chooses arm Itafter observing the context xt.
3:Attacker observes the agent’s action It. If the attacker decides to attack, it manipulates the action to
I0
t. If the attacker does not attack, I0
t=It.
4:The environment generates reward rt,I0
taccording to arm I0
tand context xt.
5:The agent and attacker receive reward rt,I0
t.
6:end for
The goal of the attacker is to design attack strategy to manipulate the agent into pulling a target arm very
frequently but by making attacks as rarely as possible. Without loss of generality, we assume arm Kis
the “attack target" arm or target arm. Define the set of rounds when the attacker decides to attack as
C:={t:t≤T,I0
t̸=It}. The cumulative attack cost is the total number of rounds where the attacker
decides to attack, i.e., |C|. The attacker can monitor the contexts, the actions of the agent and the reward
signals from the environment.
Context(x)Mean Reward(<x, >)arm 1
arm 2
arm 3
arm 4
Figure 1: An example of one dimension linear contextual bandit model.
As the action poisoning attack only changes the actions, it can impact but does not have direct control of
the agent’s observations. Furthermore, when the action space is discrete and finite, the ability of the action
poisoning attacker is severely limited. It is reasonable to limit the choice of the target policy. Here we
introduce an important assumption that the target arm is not the worst arm:
Assumption 1. For allx∈D, the mean reward of the target arm satisfies ⟨x,θK⟩>mini∈[K]⟨x,θi⟩.
If the target arm is the worst arm in most contexts, the attacker should change the target arm to a better
arm or the optimal arm so that the agent learns that the target set is optimal for almost every context. In
this case, the cost of attack may be up to O(T). Assumption 1 does not imply that the target arm is optimal
4Published in Transactions on Machine Learning Research (02/2023)
at some contexts. The target arm could be sub-optimal for all contexts. Fig. 1 shows an example of one
dimension linear contextual bandit model, where the x-axis represents the contexts and the y-axis represents
the mean rewards of arms under different contexts. As shown in Fig. 1, arms 3 and 4 satisfy Assumption 1.
In addition, arm 3 is not optimal at any context.
Under Assumption 1, there exists α∈(0,1
2)such that
max
x∈Dmini⟨x,θi⟩
⟨x,θK⟩≤(1−2α).
Equivalently, Assumption 1 implies that there exists α∈(0,1
2), such that for all t, we have
(1−2α)⟨xt,θK⟩≥min
i∈[K]⟨xt,θi⟩. (3)
Equation 3 describes the relation of the target arm and the worst arm at context xt. The action poisoning
attack can only indirectly impacts the agent’s rewards. The mean rewards at xtafter the action attacks
must be larger or equal to mini∈[K]⟨xt,θi⟩. Under Equation 3, (1−2α)⟨xt,θK⟩≥mini∈[K]⟨xt,θi⟩at any
contextxt. Then, with⟨xt,θK⟩>0,(1−α)⟨xt,θK⟩>mini∈[K]⟨xt,θi⟩at any context xt. Thus, the attacker
can indirectly change the agent’s mean reward of the non-target arm to (1−α)⟨xt,θK⟩. Then, the agent’s
estimate of each non-target arm’s coefficient vector is close to (1−α)θK, which is worse than the target arm
at any context. Equation 3 brings the possibility of successful action attacks.
Assumption 1 is necessary in our analysis to prove a formal bound of the attack cost. In Appendix A, we
show that this assumption is necessary in the sense that, if this assumption does not hold, there exist a
sequence of contexts {xt}t∈[T]such that no efficient action poisoning attack scheme can successfully attack
LinUCB. Certainly, these sequences of contexts are worst case scenarios for attacks. In practice, if these
worst case scenarios do not occur, the proposed algorithms in Sections 4.2 and 4.3 may still work even if the
target arm is the worst in a small portion of the contexts (as illustrated in the numerical example section).
3.3 Comparisons with Different Poisoning Attacks
We now compare the three types of poisoning attacks against contextual linear bandit: reward poisoning
attack, action poisoning attack and context poisoning attack. In the reward poisoning attack (Ma et al.,
2018; Garcelon et al., 2020), after the agent observes context xtand chooses arm It, the environment will
generate reward rt,Itbased on context xtand armIt. Then, the attacker can change the reward rt,Itto/tildewidert
and feed/tildewidertto the agent.
Compablack with the reward poisoning attacks, the action poisoning attack consideblack in this paper is
more difficult to carry out. In particular, as the action poisoning attack only changes the action, it can
impact but does not have direct control of the reward signal. By changing the action IttoI0
t, the reward
received by the agent is changed from rt,Ittort,I0
twhich is a random variable drawn from a distribution
based on the action I0
tand context xt. This is in contrast to reward poisoning attacks where an attacker
has direct control and can change the reward signal to any value /tildewidertof his choice.
In the context poisoning attack (Garcelon et al., 2020), the attacker changes the context shown to the agent.
The reward is generated based on the true context xtand the agent’s action It. Nevertheless, the agent’s
action may be indirectly impacted by the manipulation of the context, and so as the reward. Since the
attacker attacks before the agent pulls an arm, the context poisoning attack is the most difficult to carry
out.
For numerical comparison, as our paper is the first paper that discusses the action poisoning attack against
contextual linear bandits, there is no existing action poisoning attack method to compare. One could run
simulations to compare with reward or context poisoning attacks, but the attack costs are defined differently,
due to the different nature of attacks. For example, for reward poisoning attacks, (Garcelon et al., 2020)
proposed a reward poisoning attack method whose attack cost scales as O(log(T)2). However, the definition
of the attack cost in (Garcelon et al., 2020) is different from that of our paper. The cost of the reward attack
is defined as the cumulative differences between the post-attack rewards and the pre-attack rewards. The
5Published in Transactions on Machine Learning Research (02/2023)
cost of the action attack is defined as the number of rounds that the attacker changes the actions selected by
the agent. Although the definition of the attack cost of these two different kinds of attacks are different, the
attack cost of our proposed white-box attack strategy scales on O(log(T)2), which is same with the results
in (Garcelon et al., 2020).
As mentioned in the introduction, the goal of this paper is not to promote any particular types of poisoning
attacks. Instead, our goal is to understand the potential risks of action poisoning attacks, as the safe
applications and design of robust contextual bandit algorithm relies on the addressing all possible weakness
of the models.
4 Attack Schemes and Cost Analysis
In this section, we introduce the proposed action poisoning attack schemes in the white-box setting and
black-box setting respectively. In order to demonstrate the significant security threat of action poisoning
attacks to linear contextual bandits, we investigate our attack strategies against a widely used algorithm:
LinUCB algorithm. Furthermore, we analyze the attack cost of our action poisoning attack schemes.
4.1 Overview of LinUCB
For reader’s convenience, we first provide a brief overview of the LinUCB algorithm (Li et al., 2010). The
LinUCB algorithm is summarized in Algorithm 2. The main steps of LinUCB are to obtain estimates of the
unknown parameters θiusing past observations and then make decisions based on these estimates. Define
τi(t) :={s:s≤t,Is=i}as the set of rounds up to twhere the agent pulls arm i. LetNi(t) =|τi(t)|. Then,
at roundt, theℓ2-regularized least-squares estimate of θiwith regularization parameter λ >0is obtained
by (Li et al., 2010)
ˆθt,i=V−1
t,i/summationdisplay
k∈τi(t−1)rt,ixk, (4)
where
Vt,i=/summationdisplay
k∈τi(t−1)xkx⊤
k+λI
withIbeing identity matrix.
After ˆθi’s are obtained, at each round, an upper confidence bound of the mean reward has to be calculated
for each arm (step 4 of Algorithm 2). Then, the LinUCB algorithm picks the arm with the largest upper
confidence bound (step 5 of Algorithm 2). By following the setup in "optimism in the face of uncertainty
linear algorithm" (OFUL) (Abbasi-Yadkori et al., 2011), we set
ω(N) =√
λS+R/radicalbig
2 logK/δ+dlog (1 +L2N/(λd)),
and
βt,i=ω(Ni(t)) =√
λS+R/radicalbig
2 logK/δ+dlog (1 +L2Ni(t)/(λd)).
It is easy to verify that ω(N)is a monotonically increasing function over N∈(0,+∞).
4.2 White-box Attack
We first consider the white-box attack scenario, in which the attacker has knowledge of the environment.
In particular, in the white-box attack scenario, the attacker knows the coefficient vectors θi’s for alli. The
understanding of this scenario could provide useful insights for the more practical black-box attack scenario
to be discussed in Section 4.3.
Our proposed attack strategy works as follows. When the agent chooses arm K, the attacker does not attack.
When the agent chooses arm It̸=K, the attacker changes it to arm
I0
t=/braceleftigg
Kwith probability ϵt
I†
twith probability 1−ϵt(5)
6Published in Transactions on Machine Learning Research (02/2023)
Algorithm 2 Contextual LinUCB (Li et al., 2010)
Require:λ,K,T,L,S.
1:Initialize for every arm i,Vi←λI,bi←0,ˆθi←V−1
ibi.
2:fort= 1,2,...,Tdo
3:Observe the context xt.
4:Compute the upper confidence bound: pt,i←x⊤
tˆθi+βt,i/radicalig
x⊤
tV−1
ixtfor all arm i∈[K].
5:Pull armIt= arg max ipt,i.
6:The environment generates reward rtaccording to arm It. The agent receive rt.
7:VIt←VIt+xtx⊤
t,bIt←bIt+rtxt,ˆθIt←V−1
ItbIt.
8:end for
whereI†
t= arg min i⟨xt,θi⟩and
ϵt=(1−α)⟨xt,θK⟩−mini⟨xt,θi⟩
⟨xt,θK⟩−mini⟨xt,θi⟩. (6)
We now explain the main idea behind the attack strategy specified in Equation 5 and Equation 6. Intuitively
speaking, using Equation 5 and Equation 6, the attacker can manipulate the agent into learning some
particular coefficient vectors. In particular, for arm K(the target arm), the agent obtains the true coefficient
vectorθK. For any arm i̸=K, because of the attacks, the agent will obtain an estimate of arm i’s coefficient
vector that is close to (1−α)θK. This can be seen as follows. Under Assumption 1 and the fact that the
mean rewards are positive, we have that 1/2<ϵt<1−α. Under the white-box attack, when the agent pulls
a non-target arm It̸=K, the post-attack action I0
tis a random variable and the mean reward received by
the agent should satisfy
E[rt,I0
t|Ft−1,It] =E[⟨xt,θI0
t⟩|Ft−1,It] =ϵt⟨xt,θK⟩+ (1−ϵt) min
i⟨xt,θi⟩= (1−α)⟨xt,θK⟩.(7)
Because of the positive mean reward assumption and the fact that 0<1−α<1, the target arm Kappears
to be the optimal arm for any context. Hence the proposed white-box attack strategy can easily mislead an
agent who employs a contextual bandit algorithm with sublinear pseudo-regret, as shown in the following
proposition.
Proposition 1. Forδ>0. Suppose the agent employs a linear contextual bandit algorithm whose cumulative
pseudo-regret is upper bounded by o(T)with probability at least 1−δ. When using the proposed white-box
attack scheme, with probability at least 1−δ, the attacker can force the agent to pull the target arm for
T−o(T)times, while the total attack cost |C|is upper bounded by o(T).
Proposition 1 applies to any contextual bandit algorithm whose cumulative pseudo-regret is upper bounded
byo(T). To further illustrate the proposed attack scheme, we now provide a finer analysis the impact of
this attack on LinUCB described in Algorithm 2. We first build a high probability bound for the estimate of
parameter at the agent, when the agent computes the estimate of θiby equation 4 and the attacker employs
the white-box attack. Recall that ω(N) =√
λS+R/radicalbig
2 logK/δ+dlog (1 +L2N/(λd)).
Lemma 1. Under the proposed white-box attack, the estimate of θifor each arm iobtained by LinUCB
agent as described in Algorithm 2 satisfies
|x⊤
tˆθt,i−x⊤
t(1−α)θK|≤/parenleftig
ω(Ni(t)) +LS/radicalbig
0.5 log (2KT/δ )/parenrightig
∥xt∥V−1
t,i, (8)
with probability 1−2(K−1)δ/K, for all arm i̸=Kand allt≥0. Here,∥x∥V=√
x⊤Vxis the weighted
norm of vector xfor a positive definite matrix V.
Lemma 1 shows that, under our white-box attack, the agent’s estimate of the parameter of non-target arm,
i.e.ˆθi, will converge to (1−α)θK. Thus, the agent is misled to believe that arm Kis the optimal arm for
every context in most rounds. The following theorem provides an upper bound of the cumulative cost of the
attack.
7Published in Transactions on Machine Learning Research (02/2023)
Theorem 1. Defineγ= minx∈D⟨x,θK⟩. Under the same assumptions as in Lemma 1, for any δ >0with
probability at least 1−2δ, for allT≥0, the attacker can manipulate the LinUCB agent into pulling the
target arm in at least T−|C|rounds, using an attack cost
|C|≤2d(K−1)
(αγ)2log/parenleftbig
1 +TL2/(dλ)/parenrightbig/parenleftig
2ω(T) +LS/radicalbig
0.5 log (2KT/δ )/parenrightig2
. (9)
Theorem 1 shows that our white-box attack strategy can force LinUCB agent into pulling the target arm
T−O(log2T)times with attack cost scaled only as O(log2T).
Proof.Detailed proof of Theorem 1 can be found in Appendix B.3. Here we provide sketch of the main proof
idea.
For roundtand context xt, if LinUCB pulls arm i̸=K, we have
x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxt≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
tV−1
t,ixt.
Since the attacker does not attack the target arm, the confidence bound of arm Kdoes not change and
x⊤
tθK≤x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxtholds with probability 1−δ
K.
Then, by Lemma 1,
x⊤
tθK≤x⊤
t(1−α)θK+βt,i∥xt∥V−1
t,i+/parenleftigg
LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+ω(Ni(t))/parenrightigg
∥xt∥V−1
t,i. (10)
By multiplying both sides 1{It=i}and summing over rounds, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK
≤T/summationdisplay
k=11{Ik=i}/parenleftigg
βk,i+√
λS+LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+R/radicaligg
2 logK
δ+dlog/parenleftbigg
1 +L2Ni(k)
λd/parenrightbigg/parenrightigg
∥xk∥V−1
k,i.(11)
Here, we use Lemma 11 from (Abbasi-Yadkori et al., 2011) and obtain
T/summationdisplay
k=11{Ik=i}∥xk∥V−1
k,i≤/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (12)
Setγ= minx∈D⟨x,θK⟩. SinceNi(t) =/summationtextT
k=11{Ik=i}, we have
Ni(t)≤2d
(αγ)2log/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenleftigg
2√
λS+LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+ 2R/radicaligg
2 logK
δ+dlog/parenleftbigg
1 +tL2
λd/parenrightbigg/parenrightigg2
.(13)
4.3 Black-box Attack
We now focus on the more practical black-box setting, in which the attacker does not know any of arm’s
coefficient vector. The attacker knows the value of α(or a lower bound) in which the Equation 3 holds for
allt. Although the attacker does not know the coefficient vectors, the attacker can compute an estimate
of the unknown parameters by using past observations. On the other hand, there are multiple challenges
brought by the estimation errors that need to properly addressed.
8Published in Transactions on Machine Learning Research (02/2023)
The proposed black-box attack strategy works as follows. When the agent chooses arm K, the attacker does
not attack. When the agent chooses arm It̸=K, the attacker changes it to arm
I0
t=/braceleftigg
Kwith probability ϵt
I†
twith probability 1−ϵt(14)
where
I†
t= arg min
i̸=K/parenleftig
⟨xt,ˆθ0
t,i⟩−β0
t,i∥xt∥(V0
t,i)−1/parenrightig
, (15)
and
β0
t,i=ϕi/parenleftig
ω(N†
i(t)) +LS/radicalbig
0.5 log (2KT/δ )/parenrightig
,
ϕi= 1/αwheni̸=KandϕK= 2, and
ϵt=clip
1
2,(1−α)⟨xt,ˆθ0
t,K⟩−⟨xt,ˆθ0
t,I†
t⟩
⟨xt,ˆθ0
t,K⟩−⟨xt,ˆθ0
t,I†
t⟩,1−α
, (16)
with clip (a,x,b ) = min(b,max(x,a))wherea≤b.
For notational convenience, we set I†
t=Kandϵt= 1whenIt=K. We define that, if i̸=K,
τ†
i(t) :={s:s≤t,I†
s=i}
andN†
i(t) =|τ†
i(t)|. We also define τ†
K(t) :={s:s≤t}andN†
K(t) =|τ†
K(t)|.
ˆθ0
t,i=/parenleftbig
V0
t,i/parenrightbig−1/summationdisplay
k∈τ†
i(t−1)wk,irk,I0
kxk, (17)
where
V0
t,i=/summationdisplay
k∈τ†
i(t−1)xkx⊤
k+λI
and
wt,i=

1/ϵt ifi=I0
t=K
1/(1−ϵt)ifi=I0
t=I†
t
0 ifi̸=I0
t. (18)
Here, ˆθ0
t,iis the estimation of θiby the attacker, while ˆθt,iin Equation 4 is the estimation of θiat the agent
side. We will show in Lemma 2 and Lemma 4 that ˆθ0
t,iwill be close to the true value of θiwhile ˆθt,iwill
be close to a sub-optimal value chosen by the attacker. This disparity gives the attacker the advantage for
carrying out the attack.
We now highlight the main idea of why our black-box attack strategy works. As discussed in Section 4.2,
if the attacker knows the coefficient vectors of all arms, the proposed white-box attack scheme can mislead
the agent to believe that the coefficient vector of every non-target arm is (1−α)θK, hence the agent will
think the target arm is optimal. In the black-box setting, the attacker does not know the coefficient vector
for any arm. The attacker should estimate an coefficient vector of each arm. Then, the attacker will use
the estimated coefficient vector to replace the true coefficient vector in the white-box attack scheme. As
the attacker does not know the true values of θi’s, we need to design the estimator ˆθ0
t,i, the attack choice
I†
tand the probability ϵtcarefully. In the following, we explain the main ideas behind our design choices.
1): Firstly, we explain why we design estimator ˆθ0
t,iusing the form Equation 17, in which the attacker
employs the importance sampling to obtain an estimate of θi. There are two reasons for this. The first
reason is that, for a successful attack, the number of observations in arm i̸=Kwill be limited. Hence if the
importance sampling is not used, the estimation variance of the mean reward ⟨x,θi⟩at the attacker side for
some contexts xmay be large. The second reason is that the attacker’s action is stochastic when the agent
9Published in Transactions on Machine Learning Research (02/2023)
pulls a non-target arm. Thus, the attacker uses the observations at round twhen the attacker pulls arm i
with certain probability, i.e. when t∈τ†
i, to estimate θi. Since the agent’s action is deterministic, the agent
uses the observations at round twhen the agent pulls arm i, i.e. when t∈τi, to estimate θi. 2): Secondly,
we explain ideas behind the choice of I†
tin Equation 15. Under our black-box attack, when the agent pulls
a non-target arm It̸=K, the mean reward received by the agent satisfies
E[rt,I0
t|Ft−1,It] =E[⟨xt,θI0
t⟩|Ft−1,It] =ϵt⟨xt,θK⟩+ (1−ϵt)⟨xt,θI†
t⟩. (19)
In white-box attack scheme, I†
tis the worst arm at context xt. In the black-box setting, the attacker does not
know a priori which arm is the worst. In the proposed black-box attack scheme, as indicated in Equation 15,
we use the lower confidence bound (LCB) method to explore the worst arm and arm I†
thas the smallest lower
confidence bound. 3): Finally, we provide reasons why we choose ϵtusing Equation 16. In our white-box
attack scheme, we have that 1/2< ϵt<1−α. Thus, in our black-box attack scheme, we limit the choice
ofϵtto[1/2,1−α]. Furthermore, in Equation 6 used for the white-box attack, ϵtis computed by the true
mean reward. Now, in the black-box attack, as the attacker does not know the true coefficient vector, the
attacker uses an estimation of θto compute the second term in the clip function in Equation 16.
In summary, our design of ˆθ0
t,i,I†
tandϵtcan ensure that the attacker’s estimation ˆθ0
t,iis close toθi, while
the agent’s estimation ˆθt,iwill be close to (1−α)θK. In the following, we make these statements precise, and
formally analyze the performance of the proposed black-box attack scheme. First, we analyze the estimation
ˆθ0
t,iat the attacker side. We establish a confidence ellipsoid of ⟨xt,ˆθ0
t,i⟩at the attacker.
Lemma 2. Assume the attacker performs the proposed black-box action poisoning attack. With probability
1−2δ, we have
|x⊤
tˆθ0
t,i−x⊤
tθi|≤β0
t,i∥xt∥(V0
t,i)−1
holds for all arm iand allt≥0.
Lemma 2 shows that ˆθ0
ilies in an ellipsoid with center at θiwith high probability, which implies that the
attacker has good estimate. We then analyze the estimation ˆθt,iat the agent side. The following lemma
provides an upper bound on the difference between E[rt,I0
t|Ft−1,It]and(1−α)⟨xt,θK⟩.
Lemma 3. Under the black-box attack, the estimate obtained by an LinUCB agent satisfies
/vextendsingle/vextendsingle/vextendsingleE[rt,I0
t|Ft−1,It]−(1−α)⟨xt,θK⟩/vextendsingle/vextendsingle/vextendsingle≤(1−α)β0
t,K∥xt∥(V0
t,K)−1+ (1 +α)β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1.(20)
simultaneously for all t≥0whenIt̸=K, with probability 1−2δ.
The bound in Lemma 3 consists of the confidence ellipsoid of the estimate of arm I†
tand that of arm K. As
mentionedabove, forasuccessfulattack, thenumberofpullsonarm I†
twillbelimited. Thus, inourproposed
algorithm, the attacker use the importance sampling to obtain the estimate of θi, which will increases the
number of observations that can be used to estimate the coefficient vector of arm I†
t. Using Lemma 3, we
have the following lemma regarding the estimation ˆθt,iat the agent side.
Lemma 4. Consider the same assumption as in Lemma 2. With a probability at least 1−(3K−1)δ
K, the
estimate ˆθt,iobtained by the LinUCB agent will satisfy
|x⊤
tˆθt,i−x⊤
t(1−α)θK|≤/parenleftig
1 + 4d/α/radicalbig
Klog (1 +tL2/(dλ))/parenrightig/parenleftig
ω(t) +LS/radicalbig
0.5 log (2KT/δ )/parenrightig
∥xt∥V−1
t,i,
(21)
simultaneously for all arm i̸=Kand allt≥0.
Lemma 4 shows that, under the proposed black-box attack scheme, the agent’s estimate of the parameter
of the non-target arm, i.e. ˆθi, will converge to (1−α)θK. Hence the agent will believe that the target arm
Kis the optimal arm for any context in most rounds. Using these supporting lemmas, we can then analyze
the performance of the proposed black-box attack strategy.
10Published in Transactions on Machine Learning Research (02/2023)
Theorem 2. Defineγ= minx∈D⟨x,θK⟩. Under the same assumptions as in Lemma 4, with probability at
least 1−3δ, for allT≥0, the attacker can manipulate a LinUCB agent into pulling the target arm in at
leastT−|C|rounds, using an attack cost
|C|≤2d(K−1)
(αγ)2/parenleftigg
2 +4d
α/radicaligg
Klog/parenleftbigg
1 +TL2
dλ/parenrightbigg/parenrightigg2
log/parenleftbig
1 +TL2/(dλ)/parenrightbig/parenleftig
ω(T) +LS/radicalbig
0.5 log (2KT/δ )/parenrightig2
.
(22)
Proof.Detailed proof of Theorem 2 can be found in Appendix C.4. Here we provide a sketch of the main
proof idea.
For roundtand context xt, if LinUCB pulls arm i̸=K, we have
x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxt≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
tV−1
t,ixt.
Since the attacker does not attack the target arm, the confidence bound of arm Kdoes not change and
x⊤
tθK≤x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxtholds with probability 1−δ
K.
Thus, by Lemma 4,
x⊤
tθK≤x⊤
t(1−α)θK+ω(Ni(t))∥xt∥V−1
t,i
+/parenleftigg
1 +4d
α/radicaligg
Klog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenrightigg/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg
∥xt∥V−1
t,i.(23)
By multiplying both sides by 1{It=i}and summing over rounds, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK
≤T/summationdisplay
k=11{Ik=i}/parenleftigg
2 +4d
α/radicaligg
Klog/parenleftbigg
1 +kL2
dλ/parenrightbigg/parenrightigg/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg
∥xk∥V−1
k,i.(24)
Here, we use Lemma 11 from (Abbasi-Yadkori et al., 2011) and get
T/summationdisplay
k=11{Ik=i}∥xk∥2
V−1
k,i≤/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (25)
Thus, we have
Ni(t) =T/summationdisplay
k=11{Ik=i}
≤2d
(αγ)2log/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenleftigg
2 +4d
α/radicaligg
Klog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenrightigg2/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg2
,(26)
whereγ= minx∈D⟨x,θK⟩.
Theorem 2 shows that our black-box attack strategy can manipulate a LinUCB agent into pulling a target
armT−O(log3T)times with attack cost scaling as O(log3T). Compablack with the result for the white-box
attack, the black-box attack only brings an additional logTfactor.
11Published in Transactions on Machine Learning Research (02/2023)
4.4 Generalized Linear Models
We note that the proposed attack strategies can also be extended to generalized linear models. Detailed
analysis of the cost of the proposed attack strategies for the generalized linear contextual bandit model can
be found in Appendix D.
5 Numerical Experiments
In this section, we provide numerical examples to illustrate the impact of proposed action poisoning attack
schemes.
5.1 Attack Linear Contextual Bandit Algorithms
We first empirically evaluate the performance of the proposed action poisoning attack schemes on three
contextual bandit algorithms: LinUCB (Abbasi-Yadkori et al., 2011), LinTS (Agrawal & Goyal, 2013), and
ϵ-Greedy. We run the experiments on three datasets:
Synthetic data: The dimension of contexts and the coefficient vectors is d= 6. We set the first entry
of every context and coefficient vector to 1. The other entries of every context and coefficient vector are
uniformly drawn from (−1√d−1,1√d−1). Thus,∥x∥2≤√
2,∥θ∥2≤√
2and mean rewards ⟨x,θ⟩>0. The
reward noise ηtis drawn from a Gaussian distribution N(0,0.01).
Jester dataset (Goldberg et al., 2001): Jester contains 4.1 million ratings of jokes in which the rating
values scale from −10.00to+10.00. We normalize the rating to [0,1]. The dataset includes 100 jokes and
the ratings were collected from 73,421 users between April 1999 - May 2003. We consider a subset of 10 jokes
and 38432 users. Every jokes are rated by each user. We perform a low-rank matrix factorization ( d= 6) on
the ratings data and obtain the features for both users and jokes. At each round, the environment randomly
select a user as the context and the reward noise is drawn from a Gaussian distribution N(0,0.01).
MovieLens 25M dataset: (Harper & Konstan, 2015) MovieLens 25M dataset contains 25 million
5-star ratings of 62,000 movies by 162,000 users. The preprocessing of this data is almost the same as the
Jester dataset, except that we consider a subset of 10 movies and 7344 users. At each round, the environment
randomly select a user as the context and the reward noise is drawn from N(0,0.01).
We setδ= 0.1andλ= 2. For all the experiments, we set the total number of rounds T= 106and the
number of arms K= 10. We independently run ten repeated experiments. Results reported are averaged
over the ten experiments. We set αto0.2for the two proposed attack strategies, hence the target arm may
be the worst arm in some rounds. Each of the individual experimental runs costs up to 10 minutes on one
physical CPU core. The type of CPU is Intel Core i7-8700.
0 2 4 6 8 10
Time(t) 1050246810Cost104
0 2 4 6 8 10
Time(t) 105051015Cost104
0 2 4 6 8 10
Time(t) 10500.511.52Cost105
White-box attack on LinTS
Black-box attack on LinTS
White-box attack on LinUCB
Black-box attack on LinUCB
White-box attack on -Greeedy
Black-box attack on -Greeedy
Figure 2: The cumulative cost of the attacks for the synthetic (Left), Jester (Center) and MovieLens (Right)
datasets.
The results are shown in Table 1 and Figure 2. These experiments show that the action poisoning attacks
can force the three agents to pull the target arm very frequently, while the agents rarely pull the target arm
under no attack. Under the attacks, the true regret of the agent becomes linear as the target arm is not
optimal for most context. Table 1 show the number of rounds the agent pulls the target arm among 106
total rounds. In the synthetic dataset, under the proposed white-box attacks, the target arm is pulled more
12Published in Transactions on Machine Learning Research (02/2023)
Synthetic Jester MovieLens
ϵ-Greeedy without attacks 2124.6 5908.7 3273.5
White-box attack on ϵ-Greeedy 982122.5 971650.9 980065.6
Black-box attack on ϵ-Greeedy 973378.5 939090.2 935293.8
LinUCB without attacks 8680.9 16927.2 13303.4
White-box attack on LinUCB 981018.7 911676.9 969118.6
Black-box attack on LinUCB 916140.8 875284.7 887373.1
LinTS without attacks 5046.9 18038.0 9759.0
White-box attack on LinTS 981112.8 908488.3 956821.1
Black-box attack on LinTS 918403.8 862556.8 825034.8
Table 1: Average number of rounds when the agent pulls the target arm over T= 106rounds.
than 98.1%of the times by the three agent (see Table 1). The target arm is pulled more than 91.6%of the
times in the worst case (the black-box attacks on LinUCB). Fig 2 shows the cumulative cost of the attacks
on three agents for the three datasets. The results show that the attack cost |C|of every attack scheme
on every agent for every dataset scales sublinearly, which exposes a significant security threat of the action
poisoning attacks on linear contextual bandits.
5.2 Attack Robust Algorithms
We now discuss existing robust linear bandit algorithms (Ding et al., 2022; Bogunovic et al., 2021) and
evaluate the performance of the proposed attack strategy on these algorithms.
Inparticular, (Bogunovicetal.,2021)focusesonaspecialcaseinwhichthecontextandcoefficientvectorsare
assumed to be fixed over rounds, and developed Robust Phased Elimination (RPE) algorithm. In contrast,
our paper focuses on the general setting where the contexts are different for each round and coefficients are
different for each arm. Hence the RPE algorithm will not work for the contextual bandit setting consideblack
in our paper. (Bogunovic et al., 2021) also proves that a simple greedy algorithm based on linear regression
can be robust to linear contextual bandits with shablack coefficient under a stringent diversity assumption
on the contexts. We empirically evaluate the action attacks on the greedy algorithm in (Bogunovic et al.,
2021). The greedy algorithm is designed in the shablack coefficient setting and may not work in the disjoint
setting.
(Ding et al., 2022) provides a linear contextual bandit algorithm that is robust to rewards attacks and context
attacks. The scheme in (Ding et al., 2022) could be used to defend against the action attacks. However, our
numerical results show that the proposed attack strategy can successfully defeat the scheme in (Ding et al.,
2022). In the following, we empirically evaluate the performance of the proposed action poisoning attack
schemes on RobustBandit algorithm in (Ding et al., 2022).
(Heetal.,2022)providesnearlyoptimalalgorithmsforlinearcontextualbanditswithadversarialcorruptions.
(He et al., 2022) consider two cases: 1) the agent knows the corruption budget; and 2) the agent does not
know the corruption budget. We also empirically evaluate the action attacks on CW-OFUL algorithm in
(He et al., 2022).
We use the same synthetic data setting in Section 5.1. We set δ= 0.1andλ= 2. For all the experiments,
we set the total number of rounds T= 106and the number of arms K= 10. We independently run ten
repeated experiments. Results reported are averaged over the ten experiments. We set αto0.2for the two
proposed attack strategies. For the action attacks on CW-OFUL algorithm, we run the simulation on the
two cases that the agent know the corruption budget and the agent does not know the corruption budget.
For the case that the agent know the corruption budget, we set the attack budget to 3000and the attacker
will stop the attack if the budget is exhaust. For the case that the agent does not know the corruption
budget, we does not limit the attack budget.
The simulation results in Figure 3 show that our proposed white-box action poisoning attack can force
the greedy contextual agent in (Bogunovic et al., 2021) to pull the target arm. Although the black-box
13Published in Transactions on Machine Learning Research (02/2023)
0 2 4 6 8 10
Time(t) 105050100150200250300Total Attack CostsWhite-box attack on Contextual Greedy
0 2 4 6 8 10
Time(t) 10500.511.522.5Total Attack Costs105Black-box attack on Contextual Greedy
0 2 4 6 8 10
Time(t) 10500.511.522.5Total Regret105Black-box attack on Contextual Greedy
Figure 3: The cumulative cost of the attacks against greedy contextual algorithm in (Bogunovic et al., 2021).
action poisoning attack strategy fails, it causes linear regret on the agent. The greedy contextual algorithm
in (Bogunovic et al., 2021) highly relies a stringent diversity assumption on the contexts. In the disjoint
setting, the agent will pull each arm in some correlated contexts. The stringent diversity assumption may
not be satisfied in disjoint setting. The greedy contextual algorithm in (Bogunovic et al., 2021) will induced
to linear regret as shown in Figure 3.
0 2 4 6 8 10
Time(t) 10500.511.52Cost105
White-box attack on RobustBandit
Black-box attack on RobustBandit
Figure 4: The cumulative cost of the attacks against RobustBandit algorithm for the synthetic datasets.
ThesimulationresultsinFigure4showthatourproposedactionpoisoningattackcanforcetheRobustBandit
agent to pull the target arm with T−o(T)times. The attack cost also scales sublinearly on T. The reason
why the RobustBandit agent cannot defend against the action attacks is that its regret scales on O(C√
T).
If the attack cost C=O(√
T), the regret will be linear and the agent will be fooled.
0 2 4 6 8 10
Time(t) 105024681012Total Attack Costs104
White-box attack on CW-OFUL with unknown C
Black-box attack on CW-OFUL with unknown C
0 2 4 6 8 10
Time(t) 1050246810Total Regret104
White-box attack on CW-OFUL with unknown C
Black-box attack on CW-OFUL with unknown C
Figure 5: The cumulative cost of the attacks against CW-OFUL algorithm for the synthetic datasets.
The simulation results in Figure 5 show that our proposed action poisoning attack can force the CW-OFUL
agent to pull the target arm in the case that the agent does not know the corruption budget and the attack
budget is unlimited. The attack cost also scales sublinearly on T. The reason why the CW-OFUL agent
cannot defend against the action attacks is that its regret scales on O(T)once the corruption level is larger
14Published in Transactions on Machine Learning Research (02/2023)
than√
T. CW-OFUL agent can defend the action poisoning attack and achieve sublinear regret when the
agent knows the corruption budget and the attack budget is limited.
5.3 The Attack Performance When the Assumption 1 Violates
0 0.1 0.2 0.3 0.4 0.5
The proportion of contexts when the target arm is the worst012345678Total Attack Costs105
White-box attack on -Greeedy
Black-box attack on -Greeedy
White-box attack on LinUCB
Black-box attack on LinUCB
White-box attack on LinTS
Black-box attack on LinTS
Figure 6: The total costs of the attacks for the synthetic datasets when the Assumption 1 violates.
We now evaluate the sensitivity of the proposed algorithms on Assumption 1. We empirically evaluate
the performance of the proposed action poisoning attack schemes on three contextual bandit algorithms:
LinUCB, LinTS, and ϵ-greedy, when the target arm is the worst for some contexts.
We use the same synthetic data setting in Section 5.1. We set δ= 0.1andλ= 2. For all the experiments,
we set the total number of rounds T= 106and the number of arms K= 10. We independently run ten
repeated experiments. Results reported are averaged over the ten experiments. We set αto0.2for the two
proposed attack strategies. We manipulate the simulation environments so that the proportions of contexts,
at which the target arm is the worst, are separately controlled around: 0.001, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5.
The results in Figure 6 show the total costs under different proportions of contexts when the target arm is
the worst. The x-axis represents the proportions of context when the target arm is the worst. The y-axis
represents the total cost over 106rounds. The results shows that the total attack costs scale linearly on the
proportions of contexts when the target arm is the worst. When the proportions of contexts when the target
arm is the worst is small, our proposed attack strategies can still efficiently attacks.
5.4 The Attack Performance When the Rewards Are Normalized in [-1,1]
Here, we would like to note that we make the positive reward assumption for the formal analysis, our results
actually can be generalized to the case with negative rewards. If the positive reward assumption does not
hold, for the case with negative rewards, the attacker can preprocess the reward by adding a positive constant
to all rewards such that all rewards become positive. As long as Assumption 1 holds, the proposed attack
strategy can mislead the agent to believe that the target arm is optimal regardless of the positive reward
assumption.
To illustrate this, we evaluate the performance of the proposed algorithms when negative rewards exist. We
use the same synthetic data setting in Section 5.1. We set δ= 0.1andλ= 2. For all the experiments, we set
the total number of rounds T= 106and the number of arms K= 10. We independently run ten repeated
experiments. Results reported are averaged over the ten experiments. We set αto0.2for the two proposed
attack strategies. We normalize the rewards in [-1,1]. The attacker preprocesses the rewards by adding a
constantc= 3to the rewards in his algorithms. In the white-box attack setting, the attacker adds c= 3
to every⟨x,θ⟩and use the preprocessed ⟨x,θ⟩to compute ϵt. In the black-box attack setting, the attacker
addsc= 3to every reward rtand use the preprocessed rtto compute ˆθandϵt.
15Published in Transactions on Machine Learning Research (02/2023)
0 2 4 6 8 10
Time(t) 105051015Cost104
White-box attack on LinTS
Black-box attack on LinTS
White-box attack on LinUCB
Black-box attack on LinUCB
White-box attack on -Greeedy
Black-box attack on -Greeedy
Figure 7: The cumulative cost of the attacks when the rewards are normalized in [-1,1].
The results in Figure 7 show that our proposed attack strategies still work for the case with negative rewards.
6 Future Works
In this section, we discuss several future directions.
Linear Bandits with Shablack Coefficients. In this paper, we discuss the disjoint linear contextual
model, similar to those consideblack in (Li et al., 2010; Kong et al., 2020; Garcelon et al., 2020; Huang
et al., 2021), where each arm is associated with its own coefficient vector. At each time, the agent observes
a contextual vector xtthat is shablack with all arms.
Our model is different from the linear contextual bandit model with shablack coefficient. In that model,
the coefficient vector θ∗is shablack with all arms. At each time, each arm iis given a specific contextual
vectorxi,t. These contextual vectors form the decision set. The decision set changes over time and can even
be infinite. In this model, adversarial attacks may not always be successful. For example, (Wang et al.,
2022) shows that some attack goals can never be achieved in linear stochastic bandits due to the shablack
coefficient. This may also occur in the linear contextual bandit model with shablack coefficient. It is of
interest to conduct rigorous analysis of action attacks in the shablack coefficient model in the future work.
Attack Strategy without Assumption 1. In Section 5.3, we empirically evaluate the sensitive of the
proposed algorithms on Assumption 1. The results shows that the total attack costs seem to scale linearly
on the proportions of contexts when the target arm is the worst. It is of interest to further theoretically
analyze the phenomenon shown in the simulation results in Section 5.3. However, using current tools analysis
developed in the paper, it is challenging to formally analyze the relationship between the effectiveness of the
action attack strategies and the portion of contexts when the target arm is the worst. On the other hand,
we can try to find a new algorithm that can force the agent to choose the target arm when the target arm
is not the worst. In the following, we provide our initial ideas for this direction.
In the action attack strategies consideblack in the paper, the attacker misleads the agent to obtain an
estimate of the non-target arms’ coefficient vectors that are close to (1−α)θK. However, this can not be
always achieved without Assumption 1, since the target arm is the worst at some contexts but (1−α)θKis
worse than the target arm. Action attacks can not force the agent to learn (1−α)θK. We need some new
attack strategies. Here, we provide our initial thoughts on theoretical insights to solve this challenge in the
white-box attack scenario.
If the Assumption 1 does not hold, we set
DK:={x∈D:⟨x,θK⟩= min
i∈[K]⟨x,θi⟩}
16Published in Transactions on Machine Learning Research (02/2023)
as the set of context where the target arm is the worst. We consider such sequence of contexts {xt}t∈[T]that
xt∈DKfor allt<TKandTKlinearly depends on T.
We consider a r-neighborhood of DK:
/tildewideDK:=∪x∈DKBr(x),
whereBr(x) ={x′∈D :∥x−x′∥2< r}. In the white-box attack case, we can find a coefficient vector θ†
such thatx⊤θK> x⊤θ†for all context x∈DKandx⊤θK> x⊤θ†for all context x∈D//tildewideDK. Then the
attacker force the agent to obtain an estimate of the non-target arms’ coefficient vectors that are close to θ†.
Then the target arm is the optimal in D//tildewideDK, and the distribution of /tildewideDKis limited.
The proposed attack strategies in this paper may make the target arm to be the optimal in D//tildewideDKwith
somer. Thus, even when the target arm is the worst arm, the attack algorithm designed in this paper can
still work.
It is of interest to conduct rigorous analysis of this strategy in the future work. Furthermore, it is of interest
to carry out the design and analysis for algorithms for the black box attack strategy under this scenario,
which will be more challenging.
7 Conclusion
In this paper, we have proposed a class of action poisoning attacks on linear contextual bandits. We have
shown that our white-box attack strategy is able to force any linear contextual bandit agent, whose regret
scales sublinearly with the total number of rounds, into pulling a target arm chosen by the attacker. We
have also shown that our white-box attack strategy can force LinUCB agent into pulling a target arm
T−O(log2T)times with attack cost scaled as O(log2T). We have further shown that the proposed black-
box attack strategy can force LinUCB agent into pulling a target arm T−O(log3T)times with attack cost
scaled asO(log3T). Our results expose a significant security threat to contextual bandit algorithms. In the
future, we will investigate the defense strategy to mitigate the effects of this attack.
8 Acknowledgement
This work was supported in part by the National Science Foundation under Grants ECCS-1824553, CCF-
1908258 and ECCS-2000415.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.
Advances in neural information processing systems , 24:2312–2320, 2011.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In Pro-
ceedings of the 30th International Conference on Machine Learning , Proceedings of Machine Learning
Research, pp. 127–135, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.
Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction attacks.
InInternational Conference on Machine Learning and Data Mining in Pattern Recognition , pp. 262–275.
Springer, 2017.
Ilija Bogunovic, Arpan Losalka, Andreas Krause, and Jonathan Scarlett. Stochastic linear bandits robust
to adversarial attacks. In International Conference on Artificial Intelligence and Statistics , pp. 991–999.
PMLR, 2021.
Olivier Chapelle, Eren Manavoglu, and Romer Rosales. Simple and scalable response prediction for display
advertising. ACM Transactions on Intelligent Systems and Technology (TIST) , 5(4):61:1–61:34, December
2014. ISSN 2157-6904. doi: 10.1145/2532128. URL http://doi.acm.org/10.1145/2532128 .
Ferdinando Cicalese, Eduardo Laber, Marco Molinaro, et al. Teaching with limited information on the
learner’s behaviour. In International Conference on Machine Learning , pp. 2016–2026, 2020.
17Published in Transactions on Machine Learning Research (02/2023)
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing.
InInternational Conference on Machine Learning , pp. 1310–1320, 2019.
Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. Teaching a black-box learner. In Proceedings
of the 36th International Conference on Machine Learning , volume 97, pp. 1547–1555, 2019.
Qin Ding, Cho-Jui Hsieh, and James Sharpnack. Robust stochastic linear contextual bandits under adver-
sarial attacks. In International Conference on Artificial Intelligence and Statistics , pp. 7111–7123. PMLR,
2022.
Elvis Dohmatob. Generalized no free lunch theorem for adversarial robustness. In Proceedings of the 36th
International Conference on Machine Learning , pp. 1646–1654, 2019.
Zhe Feng, David Parkes, and Haifeng Xu. The intrinsic robustness of stochastic bandits to strategic manip-
ulation. In International Conference on Machine Learning , pp. 3092–3101. PMLR, 2020.
EvrardGarcelon, BaptisteRoziere, LaurentMeunier, JeanTarbouriech, OlivierTeytaud, AlessandroLazaric,
and Matteo Pirotta. Adversarial attacks on linear contextual bandits. In Advances in Neural Information
Processing Systems , pp. 14362–14373, 2020.
Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Eigentaste: A constant time collaborative
filtering algorithm. information retrieval , 4(2):133–151, 2001.
Z. Guan, K. Ji, D. Bucci, T. Hu, J. Palombo, M. Liston, and Y. Liang. Robust stochastic bandit algorithms
under probabilistic unbounded adversarial attack. In Proceedings of the AAAI Conference on Artificial
Intelligence , pp. 4036–4043, New York City, NY, Feb. 2020.
Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with adversarial
corruptions. In Conference on Learning Theory , pp. 1562–1578. PMLR, 2019.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions
on interactive intelligent systems (tiis) , 5(4):1–19, 2015.
Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Nearly optimal algorithms for linear contextual
bandits with adversarial corruptions. arXiv preprint arXiv:2205.06811 , 2022.
Ruiquan Huang, Weiqiang Wu, Jing Yang, and Cong Shen. Federated linear contextual bandits. Advances
in Neural Information Processing Systems , 34:27057–27068, 2021.
Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipulations on cost
signals. In International Conference on Decision and Game Theory for Security , pp. 217–237. Springer,
2019.
Kwang-Sung Jun, Lihong Li, Yuzhe Ma, and Jerry Zhu. Adversarial attacks on stochastic bandits. In
Advances in Neural Information Processing Systems , pp. 3644–3653, Montréal, Canada, Dec. 2018.
WeihaoKong, EmmaBrunskill, andGregoryValiant. Sublinearoptimalpolicyvalueestimationincontextual
bandits. In International Conference on Artificial Intelligence and Statistics , pp. 4377–4387. PMLR, 2020.
Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank
in the cascade model. In International Conference on Machine Learning , pp. 767–776, Lille, France, July
2015. PMLR.
Lifeng Lai, Hesham El Gamal, Hai Jiang, and H Vincent Poor. Cognitive medium access: Exploration,
exploitation and competition. IEEE transactions on mobile computing , 10(2):239–253, Feb. 2011.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World wide web , pp.
661–670, 2010.
18Published in Transactions on Machine Learning Research (02/2023)
Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual
bandits. In International Conference on Machine Learning , pp. 2071–2080. PMLR, 2017.
Fang Liu and Ness Shroff. Data poisoning attacks on stochastic bandits. In International Conference on
Machine Learning , volume 97, pp. 4042–4050, Long Beach, CA, June 2019.
Guanlin Liu and Lifeng Lai. Action-manipulation attacks against stochastic bandits: Attacks and defense.
IEEE Transactions on Signal Processing , 68:5152–5165, 2020a.
Guanlin Liu and Lifeng Lai. Action-manipulation attacks on stochastic bandits. In IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 3112–3116, 2020b.
Guanlin Liu and Lifeng Lai. Provably efficient black-box action poisoning attacks against reinforcement
learning. Advances in Neural Information Processing Systems , 34, 2021.
Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial cor-
ruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing , pp.
114–122, Los Angeles, CA, June 2018. ISBN 978-1-4503-5559-9.
Yuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu. Data poisoning attacks in contextual bandits. In
International Conference on Decision and Game Theory for Security , pp. 186–204. Springer, 2018.
Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement learning and
control. In Advances in Neural Information Processing Systems , volume 32, 2019.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial
perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.
1765–1773, 2017.
Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching via en-
vironment poisoning: Training-time adversarial attacks against reinforcement learning. In International
Conference on Machine Learning , pp. 7974–7984, 2020.
Amin Rakhsha, Xuezhou Zhang, Xiaojin Zhu, and Adish Singla. Reward poisoning in reinforcement learning:
Attacks against unknown learners in unknown environments. arXiv preprint arXiv:2102.08492 , 2021.
Anshuka Rangi, Haifeng Xu, Long Tran-Thanh, and Massimo Franceschetti. Understanding the limits of
poisoning attacks in episodic reinforcement learning. In Proceedings of the Thirty-First International
Joint Conference on Artificial Intelligence, IJCAI-22 , pp. 3394–3400. International Joint Conferences on
Artificial Intelligence Organization, 7 2022.
Vidit Saxena, Joakim Jaldén, Joseph E Gonzalez, Mats Bengtsson, Hugo Tullberg, and Ion Stoica. Contex-
tual multi-armed bandits for link adaptation in cellular networks. In Proceedings of the 2019 Workshop
on Network Meets AI & ML , pp. 44–49, 2019.
Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl with
unknown dynamics. In International Conference on Learning Representations , 2021.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguingpropertiesofneuralnetworks. In International Conference on Learning Representations ,
2014.
Huazheng Wang, Haifeng Xu, and Hongning Wang. When are linear stochastic bandits attackable? In
Proceedings of the 39th International Conference on Machine Learning , pp. 23254–23273, 2022.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the convergence
and robustness of adversarial training. In Proceedings of the 36th International Conference on Machine
Learning , volume 1, pp. 2, 2019.
Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks against
reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning , volume
119, pp. 11225–11234, 2020.
19Published in Transactions on Machine Learning Research (02/2023)
A Necessity of Assumption 1
In the main paper, we show that, if Assumption 1 holds, our proposed attack schemes can successfully attack
LinUCB regardless the context.
Here, we highlight the necessity of Assumption 1 in the sense that, if Assumption 1 does not hold, for some
sequence of context {xt}t∈[T], there is no efficient action poisoning attack scheme that can successfully attack
LinUCB. If the Assumption 1 does not hold, we set
DK:={x∈D|⟨x,θK⟩= min
i∈[K]⟨x,θi⟩}
as the set of context where the target arm is the worst. We consider such sequence of contexts {xt}t∈[T]that
xt∈DKfor allt<TKandTKlinearly depends on T.
If the attacker does not attack the target arm, the target arm is the worst in DK. No matter how we change
the non-target arms to another arm, the cost and loss will linearly depend on TKand therefore linearly
depend onT.
If the attacker attacks the target arm when xt∈DKand changes the target arm to a better arm or the
optimal arm so that the agent learns that the target set is optimal even in the context set DK, the cost of
attack may be up to O(TK)and hence scales as O(T). We define
τK,[K−1](t) ={s:s≤t,Is=KbutI0
s̸=K}
as the set of rounds up to twhere the attacker attacks arm KandCK(t) =|τK,[K−1](t)|as the attack cost
of attacking arm K. We can show that if we attack the target arm, for any attack scheme,
∥ˆθt,K−θK∥Vt,K≤∥V−1
t,K/summationdisplay
s∈τK,[K−1](t)LSxs∥Vt,K
≤/summationdisplay
s∈τK,[K−1](t)LS/radicalig
x⊤sV−1
t,Kxs
≤LS/radicaligg
CK(t)/summationdisplay
s∈τK,[K−1](t)∥xs∥2
V−1
t,K
≤LS/radicaligg
2dCK(t) log/parenleftbigg
1 +NK(t)L2
dλ/parenrightbigg
,(27)
where the last inequality is obtained by the Lemma 11 from (Abbasi-Yadkori et al., 2011). Then, the average
estimation error of the mean reward of arm Kis bounded by
1/TKTK/summationdisplay
t=1|x⊤
t(ˆθt,K−θK)|
≤1/TKTK/summationdisplay
t=1∥ˆθt,K−θK∥Vt,K∥xt∥V−1
t,K
≤2dLSlog/parenleftbigg
1 +NK(TK)L2
dλ/parenrightbigg/radicalbig
CK(TK)/TK.(28)
Note that we consider a sequence of contexts {xt}t∈[T]thatxt∈DKfor allt<TKandτlinearly depends on
T. In the action poisoning attacks, if the attacker tries to blackuce the rewards of non-target arms, he can
change the non-target arm to a worse arm or the worst arm. However, the target arm Kis the worst arm
for allt<TK. The mean rewards of the non-target arm at the contexts {xt}t∈[TK]must be larger or equal
tox⊤
tθK. If the attacker tries to force the agent to pull the target arm in linear time at the first TKrounds,
the attacker needs to let the post-attack arm Kbe∆-optimal than the original arm K. If the attacker tries
20Published in Transactions on Machine Learning Research (02/2023)
to let the post-attack arm Kbe∆-optimal than the original arm K, we haveCK(TK) =/tildewideO(TK), where/tildewideO
ignores the logarithm dependency.
In summary, without the prior information of the context distribution or the context sequence, there is no
action poisoning attack scheme that can always efficiently attack without Assumption 1. However, in some
situations, some action poisoning attack scheme may still work even if the target arm is the worst in a small
portion of the contexts, as shown in the numerical result session.
B Attack Cost Analysis of White-box Setting
B.1 Proof of Proposition 1
When the agent pulls a non-target arm It̸=K, the mean reward received by the agent should satisfy
E[rt,I0
t|Ft−1,It] = (1−α)⟨xt,θK⟩.
In the observation of the agent, the target arm becomes optimal and the non-target arms are associated with
the coefficient vector (1−α)θK. In addition, the cumulative pseudo-regret should satisfy
¯RT=T/summationdisplay
t=11{It̸=K}α⟨xt,θK⟩≥T/summationdisplay
t=11{It̸=K}αγ.
We defineγ= minx∈D⟨x,θK⟩. If ¯RTis upper bounded by o(T),/summationtextT
t=11{It̸=K}is also upper bounded by
o(T).
B.2 Proof of Lemma 1
If the agent computes an estimate of θiby equation 4 and Vt,i=/parenleftig/summationtext
k∈τi(t−1)xkx⊤
k+λI/parenrightig
, we have
x⊤
tˆθt,i−x⊤
t(1−α)θK
=x⊤
tV−1
t,i
/summationdisplay
k∈τi(t−1)rt,I0
kxk
−x⊤
tV−1
t,iVt,i(1−α)θK
=x⊤
tV−1
t,i
/summationdisplay
k∈τi(t−1)xk/parenleftig
rt,I0
k−(1−α)x⊤
kθK/parenrightig
−λx⊤
tV−1
t,i(1−α)θK
=/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k+ηk−(1−α)x⊤
kθK/parenrightig
−λx⊤
tV−1
t,i(1−α)θK,(29)
and by triangle inequality,
|x⊤
tˆθt,i−x⊤
t(1−α)θK|
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k−(1−α)x⊤
kθK/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixkηk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingleλx⊤
tV−1
t,i(1−α)θK/vextendsingle/vextendsingle.(30)
Now we separately bound the three items in the RHS of Equation 30.
(1) In our model, the mean reward is bounded by 0<⟨xt,θi⟩ ≤ ∥xt∥2
2∥θi∥2
2=LS. Since the mean
rewards are bounded and the rewards are generated independently, we have 0≤/vextendsingle/vextendsingle/vextendsinglex⊤
kθI0
k−(1−α)x⊤
kθK/vextendsingle/vextendsingle/vextendsingle≤
LSandE[x⊤
kθI0
k|Fk−1] = (1−α)x⊤
kθK, where the post-attack action I0
kis the random variable. Thus,/braceleftig
x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k−(1−α)x⊤
kθK/parenrightig/bracerightig
k∈τi(t−1)isaboundedmartingaledifferencesequencew.r.tthefiltration
{Fk}k∈τi(t−1).
21Published in Transactions on Machine Learning Research (02/2023)
Then, by Azuma’s inequality,
P(|/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k−(1−α)x⊤
kθK/parenrightig
|≥B)
≤2 exp/parenleftigg
−2B2
/summationtext
k∈τi(t−1)(x⊤
tV−1
t,ixkLS)2/parenrightigg
=Pt,i,(31)
whereBrepresents confidence bound. In order to ensure the confidence bounds hold for all arms and all
roundtsimultaneously, we set Pt,i=δ
KTso
B=LS/radicaltp/radicalvertex/radicalvertex/radicalbt1
2log/parenleftbigg2KT
δ/parenrightbigg/summationdisplay
k∈τi(t−1)(x⊤
tV−1
t,ixk)2≤LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
∥xt∥V−1
t,i, (32)
where the last inequality is obtained from the fact that
∥xt∥2
V−1
t,i=x⊤
tV−1
t,i
/summationdisplay
k∈τi(t−1)xkx⊤
k+λI
V−1
t,ixt
≥x⊤
tV−1
t,i
/summationdisplay
k∈τi(t−1)xkx⊤
k
V−1
t,ixt
=/summationdisplay
k∈τi(t−1)(x⊤
tV−1
t,ixk)2.(33)
In other words, with probability 1−δ, we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k−(1−α)x⊤
kθK/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
∥xt∥V−1
t,i, (34)
for all arms and all t.
(2) Note that Vt,i=/summationtext
k∈τi(t−1)xkx⊤
k+λIis positive definite. We define ⟨x,y⟩V=x⊤Vyas the weighted
inner-product. According to Cauchy-Schwarz inequality, we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixkηk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤∥xt∥V−1
t,i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
k∈τi(t−1)xkηk/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
V−1
t,i. (35)
Assume that λ≥L. From Theorem 1 and Lemma 11 in (Abbasi-Yadkori et al., 2011), we know that for any
δ>0, with probability at least 1−δ
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
k∈τi(t−1)xkηk/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
V−1
t,i≤2R2log/parenleftbiggKdet(Vt,i)1/2det(λI)−1/2
δ/parenrightbigg
≤R2/parenleftbigg
2 logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λd/parenrightbigg/parenrightbigg
,(36)
for all arms and all t>0.
(3) For the third part of the right hand side of Equation 30,
|λx⊤
tV−1
t,i(1−α)θK|≤∥ (1−α)λθK∥V−1
t,i∥xt∥V−1
t,i. (37)
22Published in Transactions on Machine Learning Research (02/2023)
SinceVt,i⪰λI, the maximum eigenvalue of V−1
t,iis smaller or equal to 1/λ. Thus,
∥(1−α)λθK∥2
V−1
t,i≤1
λ∥(1−α)λθK∥2
2≤(1−α)2λS2.
In summary,
|x⊤
tˆθt,i−x⊤
t(1−α)θK|
≤/parenleftigg
(1−α)√
λS+LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+R/radicaligg
2 logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λd/parenrightbigg/parenrightigg
∥xt∥V−1
t,i.(38)
B.3 Proof of Theorem 1
For roundtand context xt, if LinUCB pulls arm i̸=K, we have
x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxt≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
tV−1
t,ixt.
Recallβt,i=√
λS+R/radicalbigg
2 logK
δ+dlog/parenleftig
1 +L2Ni(t)
λd/parenrightig
.
Since the attacker does not attack the target arm, the confidence bound of arm Kdoes not change and
x⊤
tθK≤x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxtholds with probability 1−δ
K.
Then, by Lemma 1,
x⊤
tθK≤x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxt
≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
tV−1
t,ixt
≤x⊤
t(1−α)θK+βt,i∥xt∥V−1
t,i+/parenleftigg
LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+ω(Ni(t))/parenrightigg
∥xt∥V−1
t,i.(39)
By multiplying both sides 1{It=i}and summing over rounds, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK
≤T/summationdisplay
k=11{Ik=i}/parenleftigg
βk,i+√
λS+LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+R/radicaligg
2 logK
δ+dlog/parenleftbigg
1 +L2Ni(k)
λd/parenrightbigg/parenrightigg
∥xk∥V−1
k,i.(40)
Here, we use Lemma 11 from (Abbasi-Yadkori et al., 2011) and obtain
T/summationdisplay
k=11{Ik=i}∥xk∥2
V−1
k,i≤2dlog(1 +Ni(t)L2
dλ)
≤2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
.(41)
According to/summationtextT
k=11{Ik=i}∥xk∥V−1
k,i≤/radicalbigg
Ni(t)/summationtextT
k=11{Ik=i}∥xk∥2
V−1
k,i, we have
T/summationdisplay
k=11{Ik=i}∥xk∥V−1
k,i≤/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (42)
23Published in Transactions on Machine Learning Research (02/2023)
Thus, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK
≤/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenleftigg
LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+ 2√
λS+ 2R/radicaligg
2 logK
δ+dlog/parenleftbigg
1 +tL2
λd/parenrightbigg/parenrightigg
.(43)
and
T/summationdisplay
k=11{Ik=i}
≤1
αγ/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenleftigg
LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+ 2√
λS+ 2R/radicaligg
2 logK
δ+dlog/parenleftbigg
1 +tL2
λd/parenrightbigg/parenrightigg
,(44)
whereγ= minx∈D⟨x,θK⟩. SinceNi(t) =/summationtextT
k=11{Ik=i}, we have
Ni(t)≤2d
(αγ)2log/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenleftigg
2√
λS+LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+ 2R/radicaligg
2 logK
δ+dlog/parenleftbigg
1 +tL2
λd/parenrightbigg/parenrightigg2
.(45)
C Attack Cost Analysis of Black-box Setting
C.1 Proof of Lemma 2
Since the estimate of θiobtained by the agent satisfies
ˆθ0
t,i=/parenleftbig
V0
t,i/parenrightbig−1
/summationdisplay
k∈τ†
i(t−1)wk,irk,I0
kxk
, (46)
we have
x⊤
tˆθ0
t,i−x⊤
tθi
=x⊤
t/parenleftbig
V0
t,i/parenrightbig−1
/summationdisplay
k∈τ†
i(t−1)wk,irk,I0
kxk
−x⊤
t/parenleftbig
V0
t,i/parenrightbig−1V0
t,iθi
=x⊤
t/parenleftbig
V0
t,i/parenrightbig−1
/summationdisplay
k∈τ†
i(t−1)(wk,irk,I0
k−x⊤
kθi)xk
−λx⊤
t/parenleftbig
V0
t,i/parenrightbig−1θi
=x⊤
t/parenleftbig
V0
t,i/parenrightbig−1
/summationdisplay
k∈τ†
i(t−1)(wk,ix⊤
kθI0
k−x⊤
kθi)xk

+x⊤
t/parenleftbig
V0
t,i/parenrightbig−1
/summationdisplay
k∈τ†
i(t−1)wk,iηk
−λx⊤
t/parenleftbig
V0
t,i/parenrightbig−1θi.
Now we separately bound the three items in the RHS of Equation 47.
(1) We have 0≤/vextendsingle/vextendsingle/vextendsinglewk,ix⊤
kθI0
k−x⊤
kθi/vextendsingle/vextendsingle/vextendsingle≤wk,iLSandE[wk,ix⊤
kθI0
k|Fk−1] =x⊤
kθi, where the post-attack action
I0
kis the random variable. In addition, by the definition of wk,i, we have that wk,i≤1/αifi̸=K,
24Published in Transactions on Machine Learning Research (02/2023)
andwk,i≤2ifi=K. Thus,/braceleftig
x⊤
t(V0
t,i)−1/summationtext
k∈τ†
i(t−1)(wk,ix⊤
kθI0
k−x⊤
kθi)xk/bracerightig
k∈τi(t−1)is also a bounded
martingale difference sequence w.r.t the filtration {Fk}k∈τi(t−1). By following the steps in Section B.2, we
have, with probability 1−K−1
Kδ, for any arm i̸=Kand any round t,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglex⊤
t/parenleftbig
V0
t,i/parenrightbig−1
/summationdisplay
k∈τ†
i(t−1)(wk,ix⊤
kθI0
k−x⊤
kθi)xk
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤LS
α/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
∥xt∥(V0
t,i)−1,
and with probability 1−1
Kδ, for armKand any round t,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglex⊤
t/parenleftbig
V0
t,K/parenrightbig−1
/summationdisplay
k∈τ†
K(t−1)(wk,Kx⊤
kθI0
k−x⊤
kθK)xk
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤2LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
∥xt∥(V0
t,K)−1.
(2) The confidence bound of the second item of the right side hand of Equation 47 can be obtained from
Equation 36. With probability, 1−K−1
Kδ, for any arm i̸=Kand any round t,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglex⊤
t/parenleftbig
V0
t,i/parenrightbig−1
/summationdisplay
k∈τ†
i(t−1)wk,iηk
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤R
α/radicaltp/radicalvertex/radicalvertex/radicalbt2 logK
δ+dlog/parenleftigg
1 +L2N†
i(t)
λd/parenrightigg
∥xt∥(V0
t,i)−1. (47)
With probability, 1−1
Kδ, for armKand any round t,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglex⊤
t/parenleftbig
V0
t,K/parenrightbig−1
/summationdisplay
k∈τ†
K(t−1)wk,Kηk
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤2R/radicaltp/radicalvertex/radicalvertex/radicalbt2 logK
δ+dlog/parenleftigg
1 +L2N†
K(t)
λd/parenrightigg
∥xt∥(V0
t,K)−1.(48)
(3) For the third part of the right hand side of Equation 47,
|λx⊤
t/parenleftbig
V0
t,i/parenrightbig−1θi|≤∥λθi∥(V0
t,i)−1∥xt∥(V0
t,i)−1≤√
λS∥xt∥(V0
t,i)−1. (49)
In summary,
|x⊤
tˆθ0
t,i−x⊤
tθi|
≤ϕi
√
λS+LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
+R/radicaltp/radicalvertex/radicalvertex/radicalbt2 logK
δ+dlog/parenleftigg
1 +L2N†
i(t)
λd/parenrightigg
∥xt∥(V0
t,K)−1,(50)
whereϕi= 1/αwheni̸=KandϕK= 2.
C.2 Proof of Lemma 3
Recall the definition of ϵt:
ϵt=clip
1
2,(1−α)⟨xt,ˆθ0
t,K⟩−⟨xt,ˆθ0
t,I†
t⟩
⟨xt,ˆθ0
t,K⟩−⟨xt,ˆθ0
t,I†
t⟩,1−α
, (51)
and the definition of I†
t:
I†
t= arg min
i̸=K/parenleftig
⟨xt,ˆθ0
t,i⟩−β0
t,i∥xt∥(V0
t,i)−1/parenrightig
. (52)
25Published in Transactions on Machine Learning Research (02/2023)
By Lemma 2,⟨xt,ˆθ0
t,I†
t⟩−β0
t,I†
t∥xt∥(V0
t,I†
t)−1≤mini⟨xt,θi⟩with probability 1−2δ.
Becauseϵtis bounded by [1/2,1−α], we can analyze E[rt,I0
t|Ft−1,It]in four cases.
Case 1: when⟨xt,ˆθ0
t,K⟩<⟨xt,ˆθ0
t,I†
t⟩andϵt= 1−α, we have
E[rt,I0
t|Ft−1,It] = (1−α)⟨xt,θK⟩+α⟨xt,θI†
t⟩. (53)
Then, by Lemma 2,
(1−α)x⊤
tθK+αx⊤
tθI†
t−(1−α)x⊤
tθK
≤(1−α)/parenleftbigg
x⊤
tˆθ0
t,K+β0
t,K∥xt∥(V0
t,K)−1/parenrightbigg
+α
x⊤
tˆθ0
t,I†
t+β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1
−(1−α)x⊤
tθK
≤x⊤
tˆθ0
t,I†
t+ (1−α)β0
t,K∥xt∥(V0
t,K)−1+αβ0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1−(1−α)x⊤
tθK
≤(1−α)β0
t,K∥xt∥(V0
t,K)−1+ (1 +α)β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1,(54)
where the second inequality is obtained by the condition of Case 1 and the last inequality is obtained by
x⊤
tˆθ0
t,I†
t−β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1≤mini⟨xt,θi⟩and Assumption 1.
On the other side, we have
(1−α)x⊤
tθK+αx⊤
tθI†
t−(1−α)x⊤
tθK=αx⊤
tθI†
t≥0. (55)
Case 2: when⟨xt,ˆθ0
t,K⟩≥⟨xt,ˆθ0
t,I†
t⟩>(1−2α)⟨xt,ˆθ0
t,K⟩andϵt= 1/2, we have
E[rt,I0
t|Ft−1,It] =1
2⟨xt,θK⟩+1
2⟨xt,θI†
t⟩. (56)
Then, by Lemma 2,
1
2(x⊤
tθK+x⊤
tθI†
t)−(1−α)x⊤
tθK
=1
2/parenleftig
x⊤
tθI†
t−(1−2α)x⊤
tθK/parenrightig
≤1
2
x⊤
tˆθ0
t,I†
t+β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1−(1−2α)x⊤
tθK

≤β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1
where the last inequality is obtained by x⊤
tˆθ0
t,I†
t−β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1≤mini⟨xt,θi⟩and Assumption 1.
26Published in Transactions on Machine Learning Research (02/2023)
On the other side, by Lemma 2,
1
2(x⊤
tθK+x⊤
tθI†
t)−(1−α)x⊤
tθK
≥1
2
x⊤
tˆθ0
t,I†
t−β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1
−1
2(1−2α)/parenleftbigg
x⊤
tˆθ0
t,K+β0
t,K∥xt∥(V0
t,K)−1/parenrightbigg
≥−1
2β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1−1
2(1−2α)β0
t,K∥xt∥(V0
t,K)−1.(57)
where the last inequality is obtained by the conditions of Case 2.
Case 3: when 0≤⟨xt,ˆθ0
t,I†
t⟩≤(1−2α)⟨xt,ˆθ0
t,K⟩and1/2≤ϵt≤1−α, we have
E[rt,I0
t|Ft−1,It] =ϵt⟨xt,θK⟩+ (1−ϵt)⟨xt,θI†
t⟩. (58)
We can find that
ϵt⟨xt,θK⟩+ (1−ϵt)⟨xt,θI†
t⟩−(1−α)⟨xt,θK⟩
=ϵt(⟨xt,θK⟩−⟨xt,θI†
t⟩) +⟨xt,θI†
t⟩−(1−α)⟨xt,θK⟩
=ϵt(⟨xt,ˆθ0
t,K⟩−⟨xt,ˆθ0
t,I†
t⟩) +⟨xt,θI†
t⟩−(1−α)⟨xt,θK⟩
+ϵt(⟨xt,ˆθ0
t,I†
t⟩−⟨xt,θI†
t⟩) +ϵt(⟨xt,θK⟩−⟨xt,ˆθ0
t,K⟩)
=(1−α)⟨xt,ˆθ0
t,K⟩−⟨xt,ˆθ0
t,I†
t⟩+⟨xt,θI†
t⟩−(1−α)⟨xt,θK⟩
+ϵt(⟨xt,ˆθ0
t,I†
t⟩−⟨xt,θI†
t⟩) +ϵt(⟨xt,θK⟩−⟨xt,ˆθ0
t,K⟩)
=(1−α−ϵt)/parenleftig
⟨xt,ˆθ0
t,K⟩−⟨xt,θK⟩/parenrightig
+ (1−ϵt)/parenleftig
⟨xt,ˆθ0
t,I†
t⟩−⟨xt,θI†
t⟩/parenrightig
,(59)
which is equivalent to
/vextendsingle/vextendsingle/vextendsingleE[rt,I0
t|Ft−1,It]−(1−α)⟨xt,θK⟩/vextendsingle/vextendsingle/vextendsingle
≤(1−α−ϵt)β0
t,K∥xt∥(V0
t,K)−1+ (1−ϵt)β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1. (60)
Case 4: when⟨xt,ˆθ0
t,I†
t⟩<0andϵt= 1−α, we have
E[rt,I0
t|Ft−1,It] = (1−α)⟨xt,θK⟩+α⟨xt,θI†
t⟩. (61)
Then, by Lemma 2,
(1−α)x⊤
tθK+αx⊤
tθI†
t−(1−α)x⊤
tθK
=αx⊤
tθI†
t
≤αx⊤
tˆθ0
t,I†
t+αβ0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1
≤αβ0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1,(62)
where the last inequality is obtained by the condition of Case 4. We also have
(1−α)x⊤
tθK+αx⊤
tθI†
t−(1−α)x⊤
tθK=αx⊤
tθI†
t≥0. (63)
27Published in Transactions on Machine Learning Research (02/2023)
Combining these four cases, we have
/vextendsingle/vextendsingle/vextendsingleE[rt,I0
t|Ft−1,It]−(1−α)⟨xt,θK⟩/vextendsingle/vextendsingle/vextendsingle
≤(1−α)β0
t,K∥xt∥(V0
t,K)−1+ (1 +α)β0
t,I†
t∥xt∥/parenleftbigg
V0
t,I†
t/parenrightbigg−1. (64)
C.3 Proof of Lemma 4
From Section B.2, we have, for any arm i̸=K,
|x⊤
tˆθt,i−x⊤
t(1−α)θK|
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k−(1−α)x⊤
kθK/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixkηk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingleλx⊤
tV−1
t,i(1−α)θK/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k−ϵk⟨xk,θK⟩−(1−ϵk)⟨xk,θI†
k⟩/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
ϵk⟨xk,θK⟩+ (1−ϵk)⟨xk,θI†
k⟩−(1−α)x⊤
kθK/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixkηk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingleλx⊤
tV−1
t,i(1−α)θK/vextendsingle/vextendsingle.(65)
Now we separately bound the first item and second item in the RHS of Equation 65. The bounds of the
third item and fourth item in the RHS of Equation 65 are provided in Section B.2.
(1) Since the mean rewards are bounded and the rewards are generated independently, we have 0≤/vextendsingle/vextendsingle/vextendsinglex⊤
kθI0
k−ϵk⟨xk,θK⟩−(1−ϵk)⟨xk,θI†
k⟩/vextendsingle/vextendsingle/vextendsingle≤LSandE[x⊤
kθI0
k|Fk−1] =ϵk⟨xk,θK⟩+ (1−ϵk)⟨xk,θI†
k⟩.
Then/braceleftig
x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k−E[x⊤
kθI0
k|Fk−1]/parenrightig/bracerightig
k∈τi(t−1)is also a bounded martingale difference sequence w.r.t
the filtration{Fk}k∈τi(t−1). By following the steps in Section B.2, we have, with probability 1−δ, for any
armiand any round t,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
x⊤
kθI0
k−E[x⊤
kθI0
k|Fk−1]/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg
∥xt∥V−1
t,i. (66)
(2) From Equation 33 in Section B.2, we have
∥xt∥2
V−1
t,i≥/summationdisplay
k∈τi(t−1)(x⊤
tV−1
t,ixk)2.(67)
28Published in Transactions on Machine Learning Research (02/2023)
Then, the second item of the right hand side of Equation 65 can be upper bounded by
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈τi(t−1)x⊤
tV−1
t,ixk/parenleftig
ϵk⟨xk,θK⟩+ (1−ϵk)⟨xt,θI†
k⟩−(1−α)x⊤
kθK/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/radicaltp/radicalvertex/radicalvertex/radicalbt/summationdisplay
k∈τi(t−1)/parenleftig
E[rk,I0
k|Fk−1,Ik]−(1−α)x⊤
kθK/parenrightig2/radicaligg/summationdisplay
k∈τi(t−1)(x⊤
tV−1
t,ixk)2
≤
/summationdisplay
k∈τi(t−1)
(1−α)β0
k,K∥xk∥(V0
k,K)−1+ (1 +α)β0
k,I†
k∥xk∥/parenleftbigg
V0
k,I†
k/parenrightbigg−1
2
1
2
∥xt∥V−1
t,i,(68)
where the first inequality is obtained from Cauchy-Schwarz inequality, the second inequality is obtained from
Lemma 3 and Equation 33.
In addition, by the fact that (a+b)2≤2a2+ 2b2for any real number, we have
/summationdisplay
k∈τi(t−1)
(1−α)β0
k,K∥xk∥(V0
k,K)−1+ (1 +α)β0
k,I†
k∥xk∥/parenleftbigg
V0
k,I†
k/parenrightbigg−1
2
≤/summationdisplay
k∈τi(t−1)2/parenleftbigg
(1−α)β0
k,K∥xk∥(V0
k,K)−1/parenrightbigg2
+/summationdisplay
k∈τi(t−1)2
(1 +α)β0
k,I†
k∥xk∥/parenleftbigg
V0
k,I†
k/parenrightbigg−1
2
.(69)
Here, we use Lemma 11 from (Abbasi-Yadkori et al., 2011) and get, for any arm i,
/summationdisplay
k∈τ†
i(t−1)∥xk∥2
(V0
k,i)−1≤2dlog/parenleftbigg
1 +Ni(t)L2
dλ/parenrightbigg
≤2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
.(70)
By the fact that/summationtext
iτi(t−1) =τ†
K(t−1), and/summationtext
i̸=Kτi(t−1) =/summationtext
i̸=Kτ†
i(t−1), we have, for any arm i,
τi(t−1)⊆τ†
K(t−1), andτi(t−1)⊆/summationtext
j̸=Kτ†
j(t−1). Thus,
/summationdisplay
k∈τi(t−1)∥xk∥2
(V0
k,K)−1≤/summationdisplay
k∈τ†
K(t−1)∥xk∥2
(V0
k,K)−1≤2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
,(71)
and
/summationdisplay
k∈τi(t−1)∥xk∥2/parenleftbigg
V0
k,I†
k/parenrightbigg−1≤/summationdisplay
i̸=K/summationdisplay
k∈τ†
i(t−1)∥xk∥2
(V0
k,i)−1≤2(K−1)dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
.(72)
29Published in Transactions on Machine Learning Research (02/2023)
By combining the definition of β0
t,i, Equation 69, Equation 71 and Equation 72, we have
/summationdisplay
k∈τi(t−1)
(1−α)β0
k,K∥xk∥(V0
k,K)−1+ (1 +α)β0
k,I†
k∥xk∥/parenleftbigg
V0
k,I†
k/parenrightbigg−1
2
≤/summationdisplay
k∈τi(t−1)2/parenleftbigg
β0
k,K∥xk∥(V0
k,K)−1/parenrightbigg2
+/summationdisplay
k∈τi(t−1)2
2β0
k,I†
k∥xk∥/parenleftbigg
V0
k,I†
k/parenrightbigg−1
2
≤16d2/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg2
log/parenleftbigg
1 +tL2
dλ/parenrightbigg
+16d2(K−1)
α2/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg2
log/parenleftbigg
1 +tL2
dλ/parenrightbigg
≤16d2K
α2/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg2
log/parenleftbigg
1 +tL2
dλ/parenrightbigg
.(73)
In summary, we have
|x⊤
tˆθt,i−x⊤
t(1−α)θK|
≤/parenleftigg
1 +4d
α/radicaligg
Klog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenrightigg/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg
∥xt∥V−1
t,i.(74)
C.4 Proof of Theorem 2
For roundtand context xt, if LinUCB pulls arm i̸=K, we have
x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxt≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
tV−1
t,ixt.
In this case, βt,i=ω(Ni(t)) =√
λS+R/radicalbigg
2 logK
δ+dlog/parenleftig
1 +L2Ni(t)
λd/parenrightig
.
Since the attacker does not attack the target arm, the confidence bound of arm Kdoes not change and
x⊤
tθK≤x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxtholds with probability 1−δ
K.
Thus, by Lemma 4,
x⊤
tθK≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
tV−1
t,ixt
≤x⊤
t(1−α)θK+ω(Ni(t))∥xt∥V−1
t,i
+/parenleftigg
1 +4d
α/radicaligg
Klog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenrightigg/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg
∥xt∥V−1
t,i.(75)
By multiplying both sides by 1{It=i}and summing over rounds, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK
≤T/summationdisplay
k=11{Ik=i}/parenleftigg
2 +4d
α/radicaligg
Klog/parenleftbigg
1 +kL2
dλ/parenrightbigg/parenrightigg/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg
∥xk∥V−1
k,i.(76)
30Published in Transactions on Machine Learning Research (02/2023)
Here, we use Lemma 11 from (Abbasi-Yadkori et al., 2011) and get
T/summationdisplay
k=11{Ik=i}∥xk∥2
V−1
k,i≤2dlog/parenleftbigg
1 +Ni(t)L2
dλ/parenrightbigg
≤2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (77)
According to/summationtextT
k=11{Ik=i}∥xk∥V−1
k,i≤/radicalbigg
Ni(t)/summationtextT
k=11{Ik=i}∥xk∥2
V−1
k,i, we have
T/summationdisplay
k=11{Ik=i}∥xk∥2
V−1
k,i≤/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (78)
Thus, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK
≤/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenleftigg
2 +4d
α/radicaligg
Klog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenrightigg/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg
,(79)
and
Ni(t) =T/summationdisplay
k=11{Ik=i}
≤2d
(αγ)2log/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenleftigg
2 +4d
α/radicaligg
Klog/parenleftbigg
1 +tL2
dλ/parenrightbigg/parenrightigg2/parenleftigg
ω(t) +LS/radicaligg
1
2log/parenleftbigg2KT
δ/parenrightbigg/parenrightigg2
,(80)
whereγ= minx∈D⟨x,θK⟩.
D Attacks on Generalized Linear Contextual Bandits
In the generalized linear model (GLM), there is a fixed, strictly increasing link function µ:R→Rsuch that
the reward satisfies
rt,It=µ(⟨xt,θIt⟩) +ηt,
whereηtis a conditionally independent zero-mean R-subgaussian noise and ⟨·,·⟩denotes the inner product.
If we consider the σ-algebraFt=σ(η1,...,ηt),ηtbecomesFtmeasurable.
Hence, the expected reward of arm iunder context xtfollows the GLM setting: E[rt,i] =µ(⟨xt,θi⟩)for allt
and all arm i. One can verify that µ(x) =xleads to the linear model and µ(x) = exp(x)/(1 + exp(x))leads
to the logistic model.
We assume that the link function µis continuously twice differentiable, Lipschitz with constant kµand such
thatcµ= infθ∈Θ,x∈D˙µ(x⊤θ)>0, where ˙µdenote the first derivatives of µ. It can be verified that the link
function of the linear model is Lipschitz with constant kµ= 1and which of the logistic model is Lipschitz
with constant kµ= 1/4.
The agent is interested in minimizing the cumulative pseudo-regret, and the cumulative pseudo-regret for
the GLM can be formally written as
RT=T/summationdisplay
t=1/parenleftbig
µ(⟨xt,θI∗
t⟩)−µ(⟨xt,θIt⟩)/parenrightbig
, (81)
whereI∗
t= arg max iµ(⟨xt,θi⟩).
For the GLM consideblack here, µis a strictly increasing function and I∗
t= arg max iµ(⟨xt,θi⟩) =
arg maxi⟨xt,θi⟩. As the link function µis strictly increasing, the target arm is not the worst arm under
Assumption 1.
31Published in Transactions on Machine Learning Research (02/2023)
D.1 Overview of UCB-GLM
For reader’s convenience, we first provide a brief overview of the UCB-GLM algorithm (Li et al., 2017). The
UCB-GLM algorithm is summarized in Algorithm 3.
The algorithm is simply initialized by play every arm jtimes to ensure a unique solution of ˆθifor each arm
i. We assume that after playing arm i Jtimes,Viis invertible and the minimal eigenvalue of Viis greater
or equal to λ0for all arm i. We assume that xtis drawn iid from some distribution vwith support in the
unit ball and set Σ := E[xtx⊤
t]. Proposition 1 in (Li et al., 2017) shows that there exist positive, universal
constantsD1andD2such thatλmin(Vi)≥λ0with probability at least 1−δ, as long as
J≥/parenleftigg
D1√
d+D2/radicalbig
log(1/δ)
λmin(Σ)/parenrightigg2
+2λ0
λmin(Σ). (82)
Algorithm 3 UCB-GLM (Li et al., 2017)
Require:
number of arms K, number of rounds T, number of initial rounds j.
1:Initialize for every arm i.
2:Play every arm Jtimes. At each time, update ¯VIt←VIt+xtx⊤
t.
3:fort=KJ+ 1,KJ + 2,...,Tdo
4:observe the context xt.
5:fori= 1,2,...,Kdo
6:Calculate the maximum-likelihood estimator ˆθiby solving the equation
/summationdisplay
n∈τi(t−1)(rn−µ(x⊤
nˆθi))xn= 0.
7:Compute the upper confidence bound: pt,i←x⊤
tˆθi+βt,i/radicalig
x⊤
t¯V−1
ixt.
8:end for
9:Pull armIt= arg max ipt,i.
10:The environment generates reward rtaccording to arm It.
11:The agent receive rt.
12: ¯VIt←¯VIt+xtx⊤
t.
13:end for
By following the setup in (Li et al., 2017), we set βt,i=2R
cµ/radicalbigg
logK
δ+dlog/parenleftig
1 +L2Ni(t)
λ0d/parenrightig
for GLM.
D.2 White-box Attack Strategy
Similar to the white-box attack strategy for linear contextual bandits, the white-box attack strategy for
GLM works as follows. When the agent chooses arm K, the attacker does not attack. When the agent
chooses arm It̸=K, the attacker changes it to arm
I0
t=/braceleftigg
Kwith probability ϵt
I†
twith probability 1−ϵt(83)
whereI†
t= arg min i⟨xt,θi⟩and
ϵt=µ((1−α)⟨xt,θK⟩)−µ(mini⟨xt,θi⟩)
µ(⟨xt,θK⟩)−µ(mini⟨xt,θi⟩). (84)
The attacker can manipulate the agent into learning some particular coefficient vectors. In particular, for
armK(the target arm), the agent obtains the true coefficient vector θK. For any arm i̸=K, because of
32Published in Transactions on Machine Learning Research (02/2023)
the attacks, the agent will obtain an estimate of arm i’s coefficient vector to be close to (1−α)θK. Under
the white-box attack, when the agent pulls a non-target arm It̸=K, the mean reward received by the agent
should satisfy
E[rt,I0
t|Ft−1,It] =µ((1−α)⟨xt,θK⟩).
Proposition 2. Forδ > 0. Suppose the agent employs a generalized linear contextual bandit algorithm
whose cumulative pseudo-regret is upper bounded by o(T)overTwith probability at least 1−δ. When using
the proposed white-box attack scheme, with probability at least 1−δ, the attacker can force the agent to pull
the target arm for T−o(T)times, while the total attack cost |C|is upper bounded by o(T).
The proof of Proposition 2 is provided in Appendix E.1. To further illustrate the proposed attack scheme,
we now provide a finer analysis the impact of this attack on UCB-GLM described in Algorithm 3.
Lemma 5. Under the proposed white-box attack, the estimate of θifor each arm i̸=Kobtained by UCB-
GLM agent as described in Algorithm 3 satisfies
|x⊤
tˆθt,i−x⊤
t(1−α)θK|≤2kµLS+ 2R
cµ/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λ0d/parenrightbigg
∥xt∥¯V−1
t,i. (85)
The proof of Lemma 5 is provided in Appendix E.2.
Theorem 3. Defineγ= minx∈D⟨x,θK⟩. Under the same assumptions as in Lemma 5, for any δ >0with
probability at least 1−2δ, for allT≥0, the attacker can manipulate the UCB-GLM agent into pulling the
target arm in at least T−|C|rounds, using an attack cost
|C|≤4d(K−1)
(αγ)2log/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenleftbigg2kµLS+ 4R
cµ/parenrightbigg2/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2T
λ0d/parenrightbigg/parenrightbigg
. (86)
The proof of Theorem 3 is provided in Appendix E.3.
D.3 Black-box Attack Strategy
The modified black-box attack strategy for GLM works as follows. When the agent chooses arm K, the
attacker does not attack. When the agent chooses arm It̸=K, the attacker changes it to arm
I0
t=/braceleftigg
Kwith probability ϵt
I†
twith probability 1−ϵt(87)
where
I†
t= arg min
i̸=K/parenleftig
⟨xt,ˆθ0
t,i⟩−β0
t,i∥xt∥(¯V0
t,i)−1/parenrightig
, (88)
and
β0
t,i= 2ϕikµLS+R
cµ/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λ0d/parenrightbigg
, (89)
ϕi=kµ
cµαwheni̸=KandϕK= 1 +kµ
cµ, and
ϵt=clip
cµ
cµ+kµ,µ((1−α)x⊤
tˆθ0
t,K)−µ(x⊤
tˆθ0
t,I†
t)
µ(x⊤
tˆθ0
t,K)−µ(x⊤
tˆθ0
t,I†
t),1−αcµ
kµ
, (90)
with clip (a,x,b ) = min(b,max(x,a))wherea≤b.
For notational convenience, we set I†
t=Kandϵt= 1whenIt=K. We define that, if i̸=K,τ†
i(t) :={s:
s≤t,I†
s=i}andN†
i(t) =|τ†
i(t)|;τ†
K(t) :={s:s≤t}andN†
K(t) =|τ†
K(t)|.
33Published in Transactions on Machine Learning Research (02/2023)
Calculate the maximum-likelihood estimator ˆθ0
t,iby solving the equation
/summationdisplay
n∈τi(t−1)†(wt,irn−µ(x⊤
nˆθt,i))xn= 0.
where ¯V0
t,i=/summationtext
k∈τ†
i(t−1)xkx⊤
k
wt,i=

1/ϵt ifi=I0
t=K
1/(1−ϵt)ifi=I0
t=I†
t
0 ifi̸=I0
t. (91)
First, we analyze the estimation ˆθ0
t,iat the attacker side. We establish a confidence ellipsoid of ⟨xt,ˆθ0
t,i⟩at
the attacker.
Lemma 6. Assume the attacker performs the proposed black-box action poisoning attack. With probability
1−2δ, we have
|x⊤
tˆθ0
t,i−x⊤
tθi|≤β0
t,i∥xt∥(¯V0
t,i)−1. (92)
holds for all arm iand allt≥0simultaneously.
The proof of Lemma 6 is provided in Appendix E.4.
Lemma 7. Under the black-box attack, with probability 1−2δ, the estimate obtained by an UCB-GLM agent
satisfies
/vextendsingle/vextendsingle/vextendsingleE[rt,I0
t|Ft−1,It]−µ((1−α)x⊤
tθK)/vextendsingle/vextendsingle/vextendsingle≤2kµβ0
t,K∥xt∥(¯V0
t,K)−1+ 2kµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1
simultaneously for all t≥0whenIt̸=K.
Lemma 8. Assume the attacker performs the proposed black-box action poisoning attack. With a probability
at least 1−3Kδ
K, the estimate ˆθt,iobtained by the UCB-GLM agent will satisfy
|x⊤
tˆθt,i−x⊤
t(1−α)θK|
≤2kµLS+ 2R
cµ/parenleftigg
1 +16k2
µd
cµα/radicaligg
Klog/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenrightigg/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg
∥x∥¯V−1
t,i(93)
simultaneously for all arm i̸=Kand allt≥0.
Theorem 4. Defineγ= minx∈D⟨x,θK⟩. Assume the attacker performs the proposed black-box action
poisoning attack. For any δ >0with probability at least 1−2δ, for allT≥0, the attacker can manipulate
the UCB-GLM agent into pulling the target arm in at least T−|C|rounds, using an attack cost
|C|≤4d(K−1)
(αγ)2/parenleftbigg2kµLS+ 2R
cµ/parenrightbigg2
log/parenleftbigg
1 +TL2
dλ0/parenrightbigg
×/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2T
λ0d/parenrightbigg/parenrightbigg/parenleftigg
1 +16k2
µd
cµα/radicaligg
Klog/parenleftbigg
1 +TL2
dλ0/parenrightbigg/parenrightigg2
.(94)
The proof of Theorem 4 is provided in Appendix E.7. Theorem 4 shows that our black-box attack strategy
can manipulate a UCB-GLM agent into pulling a target arm T−O(log3T)times with attack cost scaling
asO(log3T). Compablack with the result for the white-box attack, the black-box attack only brings an
additional logTfactor.
34Published in Transactions on Machine Learning Research (02/2023)
E Attack Cost Analysis of GLM
E.1 Proof of Proposition 2
Let us consider a contextual bandit problem P1, in which the arm K(the target arm) is associated with
a fixed coefficient vector θKand all other arms are associated with the coefficient vector (1−α)θK. For a
generalized linear contextual bandit algorithm A, we suppose that the cumulative pseudo-regret regret of
algorithm Afor the problem P1is upper bounded with probability at least 1−δby a function fA(T)such
thatfA(T) =o(T).
Under the proposed white-box attack, when the agent pulls a non-target arm It̸=K, the mean reward
received by the agent should satisfy E[rt,I0
t|Ft−1,It] =µ((1−α)⟨xt,θK⟩). In the observation of the agent,
the target arm becomes optimal and the non-target arms are associated with the coefficient vector (1−α)θK.
For the agent, the combination of the attacker and the environment form problem P1. The cumulative
pseudo-regret should satisfy
RT=T/summationdisplay
t=11{It̸=K}(µ(⟨xt,θK⟩)−µ(⟨xt,(1−α)θK⟩))
≥T/summationdisplay
t=11{It̸=K}cµ⟨xt,αθK⟩
≥T/summationdisplay
t=11{It̸=K}cµγ,
which is equivalent to/summationtextT
t=11{It̸=K}≤RT/(cµγ). SinceRTis upper bounded by fA(T) =o(T),|C|=/summationtextT
t=11{It̸=K}is also upper bounded by o(T).
E.2 Proof of Lemma 5
The maximum-likelihood estimation can be written as the solution to the following equation
/summationdisplay
n∈τi(t−1)(rn−µ(x⊤
nˆθt,i))xn= 0. (95)
Definegt,i(θ) =/summationtext
n∈τi(t−1)µ(x⊤
nθ))xn.gt,i(ˆθt,i) =/summationtext
n∈τi(t−1)rnxn. Sinceµis continuously twice differen-
tiable,∇gt,iis continuous, and for any θ∈Θ,∇gt,i(θ) =/summationtext
n∈τi(t−1)xnx⊤
n˙µ(x⊤
nθ)).∇gt,i(θ)denotes the
Jacobian matrix of gt,iatθ. By the Fundamental Theorem of Calculus,
gt,i(ˆθt,i)−gt,i((1−α)θK) =Gt,i(ˆθt,i−(1−α)θK), (96)
where
Gt,i=/integraldisplay1
0∇gt,i/parenleftig
sˆθt,i+ (1−s)(1−α)θK/parenrightig
ds. (97)
Note that∇gt,i(θ) =/summationtext
n∈τi(t−1)xnx⊤
n˙µ(x⊤
nθ)). According to the assumption that cµ= infθ∈Θ,x∈D˙µ(x⊤θ)>
0, we haveGt,i⪰cµVt,i⪰cµVKJ,i⪰λ0I≻0, where in the last two step we used the assumption that the
minimal eigenvalue of Viis greater or equal to λ0after playing arm iJtimes. Thus, Gt,iis positive definite
and non-singular. Therefore,
ˆθi−(1−α)θK=G−1
t,i/parenleftig
gt,i(ˆθi)−gt,i((1−α)θK)/parenrightig
. (98)
For armK,gt,i(ˆθi)−gt,i(θK) =/summationtext
n∈τi(t−1)ηnxn.
35Published in Transactions on Machine Learning Research (02/2023)
For all arm i̸=K, the right hand side of Equation 98 is equivalent to
gt,i(ˆθi)−gt,i((1−α)θK)
=/summationdisplay
n∈τi(t−1)(rn−µ((1−α)x⊤
nθK))xn
=/summationdisplay
n∈τi(t−1)(µ(x⊤
nθI0n)−µ((1−α)x⊤
nθK))xn+/summationdisplay
n∈τi(t−1)ηnxn.(99)
We setZ1=/summationtext
n∈τi(t−1)(µ(x⊤
nθI0
n)−µ((1−α)x⊤
nθK))xnandZ2=/summationtext
n∈τi(t−1)ηnxn.
We havegt,i(ˆθi)−gt,i((1−α)θK) =Z1+Z2and
x⊤
t(ˆθi−(1−α)θK) =x⊤
tG−1
t,i(Z1+Z2). (100)
For any context x∈Dand armi̸=K, we have
|x⊤(ˆθt,i−(1−α)θK)|
=|x⊤G−1
t,i(Z1+Z2)|
≤|x⊤G−1
t,iZ1|+|x⊤G−1
t,iZ2|.(101)
We first bound|x⊤G−1
t,iZ2|. SinceGt,iis positive definite and G−1
t,iis also positive definite, |x⊤G−1
t,iZ2|≤
∥x∥G−1
t,i∥Z2∥G−1
t,i.
SinceGt,i⪰cµVt,iimplies that G−1
t,i⪯c−1
µ¯V−1
t,i, we have∥x∥G−1
t,i≤1√cµ∥x∥¯V−1
t,iholds for any x∈Rd. Thus,
|x⊤G−1
t,iZ2|≤1
cµ∥x∥¯V−1
t,i∥Z2∥¯V−1
t,i. (102)
Note thatVt,i=¯Vt,i+λI. Hence, for all vector x∈ Rd
∥x∥2
¯V−1
t,i=∥x∥2
V−1
t,i+x⊤(¯V−1
t,i−V−1
t,i)x. (103)
Since (A+B)−1=A−1−A−1B(A+B)−1,
V−1
t,i=¯V−1
t,i−λ¯V−1
t,iV−1
t,i. (104)
The above implies that
0≤x⊤(¯V−1
t,i−V−1
t,i)x
=x⊤/parenleftbig
λ¯V−1
t,iV−1
t,i/parenrightbig
x
≤λ
λ0∥x∥2
V−1
t,i.(105)
and∥x∥2
¯V−1
t,i≤(1 +λ
λ0)∥x∥2
V−1
t,i.
FromTheorem1andLemma11in(Abbasi-Yadkorietal.,2011), weknowthatforany δ>0, withprobability
at least 1−δ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
k∈τi(t−1)xkηk/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
V−1
t,i≤2R2log/parenleftbiggKdet(Vt,i)1/2det(λI)−1/2
δ/parenrightbigg
≤2R2/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λd/parenrightbigg/parenrightbigg
,(106)
36Published in Transactions on Machine Learning Research (02/2023)
for all arms and all t>0.
Setλ=λ0, we have
∥Z2∥¯V−1
t,i≤2R/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λ0d/parenrightbigg
. (107)
Now we bound|x⊤G−1
t,iZ1|. Similarly,
|x⊤G−1
t,iZ1|≤1
cµ∥x∥¯V−1
t,i∥Z1∥¯V−1
t,i. (108)
In our model, we have 0<⟨xt,θi⟩≤∥xt∥2
2∥θi∥2
2=LS. Further,
0≤/vextendsingle/vextendsingle/vextendsingleµ(x⊤
kθI0
k)−µ((1−α)x⊤
kθK)/vextendsingle/vextendsingle/vextendsingle
≤kµ/vextendsingle/vextendsingle/vextendsinglex⊤
kθI0
k−(1−α)x⊤
kθK/vextendsingle/vextendsingle/vextendsingle
≤kµLS.(109)
Since we have E[µ(x⊤
kθI0
k)|Fk−1] =µ((1−α)x⊤
kθK),/braceleftig
µ(x⊤
kθI0
k)−µ/parenleftbig
(1−α)x⊤
kθK/parenrightbig/bracerightig
k∈τi(t−1)is a bounded
martingaledifferencesequencew.r.tthefiltration {Fk}k∈τi(t−1)andisalsokµLS-sub-Gaussian-sub-Gaussian.
FromTheorem1andLemma11in(Abbasi-Yadkorietal.,2011), weknowthatforany δ>0, withprobability
at least 1−K−1
Kδ
∥Z1∥¯V−1
t,i≤2kµLS/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λ0d/parenrightbigg
, (110)
for any arm i̸=Kand allt>0.
In summary, for all arm i̸=K,
|x⊤(ˆθi−(1−α)θK)|≤2kµLS+ 2R
cµ/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λ0d/parenrightbigg
∥x∥¯V−1
t,i. (111)
E.3 Proof of Theorem 3
For roundtand context xt, if UCB-GLM pulls arm i̸=K, we have
x⊤
tˆθt,K+βt,K/radicalig
x⊤
t¯V−1
t,Kxt≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
t¯V−1
t,ixt,
Recallβt,i=2R
cµ/radicalbigg
logK
δ+dlog/parenleftig
1 +L2Ni(t)
λ0d/parenrightig
in GLM.
Since the attacker does not attack the target arm, the confidence bound of arm Kdoes not change and
x⊤
tθK≤x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxtholds with probability 1−δ
K.
Thus, by Lemma 5,
x⊤
tθK≤x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxt
≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
tV−1
t,ixt
≤x⊤
tˆθt,i+2kµLS+ 2R
cµcu
2Rβt,i/radicalig
x⊤
tV−1
t,ixt+βt,i/radicalig
x⊤
tV−1
t,ixt
≤x⊤
t(1−α)θK+kµLS+ 2R
Rβt,i∥xt∥¯V−1
t,i.(112)
37Published in Transactions on Machine Learning Research (02/2023)
By multiplying both sides 1{It=i}and summing over rounds, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK≤T/summationdisplay
k=11{Ik=i}kµLS+ 2R
Rβt,i∥xt∥¯V−1
t,i. (113)
Here, we use Lemma 11 from (Abbasi-Yadkori et al., 2011) and obtain
T/summationdisplay
k=11{Ik=i}∥xk∥2
V−1
k,i≤2dlog(1 +Ni(t)L2
dλ)≤2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (114)
According to/summationtextT
k=11{Ik=i}∥xk∥V−1
k,i≤/radicalbigg
Ni(t)/summationtextT
k=11{Ik=i}∥xk∥2
V−1
k,i, we have
T/summationdisplay
k=11{Ik=i}∥xk∥V−1
k,i≤/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (115)
Setλ=λ0, we have∥x∥2
¯V−1
t,i≤(1 +λ
λ0)∥x∥2
V−1
t,i= 2∥x∥2
V−1
t,i. Thus, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK≤kµLS+ 2R
Rβt,i/radicaligg
4Ni(t)dlog/parenleftbigg
1 +tL2
dλ0/parenrightbigg
, (116)
and
Ni(t) =T/summationdisplay
k=11{Ik=i}≤4d
(αγ)2log/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenleftbiggkµLS+ 2R
Rβt,i/parenrightbigg2
, (117)
whereγ= minx∈D⟨x,θK⟩.
E.4 Proof of Lemma 6
The attacker calculate the maximum-likelihood estimator ˆθ0
t,iby solving the equation
/summationdisplay
n∈τi(t−1)†(wn,irn−µ(x⊤
nˆθi))xn= 0. (118)
Note thatg0
t,i(θ) =/summationtext
n∈τ†
i(t−1)µ(x⊤
nθ))xn.g0
t,i(ˆθ0
t,i) =/summationtext
n∈τ†
i(t−1)wn,irnxn.
For all arm i,
g0
t,i(ˆθ0
t,i)−g0
t,i(θi)
=/summationdisplay
n∈τ†
i(t−1)(wn,irn−µ(x⊤
nθi))xn
=/summationdisplay
n∈τ†
i(t−1)(wn,iµ(x⊤
nθI0n)−µ(x⊤
nθi))xn+/summationdisplay
n∈τ†
i(t−1)wn,iηnxn.(119)
Similarly, we set Z3=/summationtext
n∈τ†
i(t−1)wn,iηnxnandZ4=/summationtext
n∈τ†
i(t−1)(wn,iµ(x⊤
nθI0n)−µ((1−α)x⊤
nθK))xn.
We haveg0
t,i(ˆθ0
t,i)−g0
t,i(θi) =Z3+Z4and
x⊤
t(ˆθ0
t,i−θi) =x⊤
t(G0
t,i)−1(Z3+Z4), (120)
38Published in Transactions on Machine Learning Research (02/2023)
where
G0
t,i=/integraldisplay1
0∇g0
t,i/parenleftig
sˆθ0
t,i+ (1−s)θi/parenrightig
ds. (121)
For any context x∈D, we have
|x⊤(ˆθ0
t,i−θi)|
=|x⊤(G0
t,i)−1(Z3+Z4)|
≤|x⊤(G0
t,i)−1Z3|+|x⊤(G0
t,i)−1Z4|.(122)
We first bound|x⊤(G0
t,i)−1Z3|. We have
|x⊤(G0
t,i)−1Z3|≤1
cµ∥x∥(¯V0
t,i)−1∥Z3∥(¯V0
t,i)−1, (123)
where ¯V0
t,i=/summationtext
n∈τi(t−1)†xnx⊤
n.
Note thatV=
t,i¯V0
t,i+λI. Hence,
∥Z3∥2
(¯V0
t,i)−1=∥Z3∥2
(V0
t,i)−1+Z⊤
3((¯V0
t,i)−1−(V0
t,i)−1)Z3
≤(1 +λ
λ0)∥Z3∥2
(V0
t,i)−1.(124)
FromTheorem1andLemma11in(Abbasi-Yadkorietal.,2011), weknowthatforany δ>0, withprobability
at least 1−δ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
k∈τi(t−1)xkηk/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
V−1
t,i≤2R2log/parenleftbiggKdet(Vt,i)1/2det(λI)−1/2
δ/parenrightbigg
≤2R2/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λd/parenrightbigg/parenrightbigg
,(125)
for all arms and all t>0.
Setλ=λ0, we have
∥Z3∥2
(¯V0
t,i)−1≤2ϕiR/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2N0
i(t)
λ0d/parenrightbigg
. (126)
Now we bound|x⊤(G0
t,i)−1Z4|. Similarly,
|x⊤(G0
t,i)−1Z4|≤1
cµ∥x∥(¯V0
t,i)−1∥Z4∥(¯V0
t,i)−1. (127)
In our model, we have 0<⟨xt,θi⟩≤∥xt∥2
2∥θi∥2
2=LS. Further,
0≤/vextendsingle/vextendsingle/vextendsinglewk,iµ(x⊤
kθI0
k)−µ((1−α)x⊤
kθK)/vextendsingle/vextendsingle/vextendsingle≤ϕikµLS. (128)
Sincewehave E[wk,iµ(x⊤
kθI0
k)|Fk−1] =µ(x⊤
kθi),/braceleftig
wk,iµ(x⊤
kθI0
k)−µ/parenleftbig
x⊤
kθi/parenrightbig/bracerightig
k∈τi(t−1)isaboundedmartingale
difference sequence w.r.t the filtration {Fk}k∈τi(t−1)and is also ϕikµLS-sub-Gaussian-sub-Gaussian.
FromTheorem1andLemma11in(Abbasi-Yadkorietal.,2011), weknowthatforany δ>0, withprobability
at least 1−δ
∥Z4∥2
(¯V0
t,i)−1≤2ϕikµLS/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2N0
i(t)
λ0d/parenrightbigg
. (129)
39Published in Transactions on Machine Learning Research (02/2023)
for any arm i̸=Kand allt>0.
In summary, for all arm i̸=K,
|x⊤(ˆθi−(1−α)θK)|≤2ϕikµLS+R
cµ/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λ0d/parenrightbigg
∥x∥(¯V0
t,i)−1. (130)
E.5 Proof of Lemma 7
Recall the definition of ϵt:
ϵt=clip
cµ
cµ+kµ,µ((1−α)x⊤
tˆθ0
t,K)−µ(x⊤
tˆθ0
t,I†
t)
µ(x⊤
tˆθ0
t,K)−µ(x⊤
tˆθ0
t,I†
t),1−αcµ
kµ
, (131)
and the definition of I†
t:
I†
t= arg min
i̸=K/parenleftig
⟨xt,ˆθ0
t,i⟩−β0
t,i∥xt∥(¯V0
t,i)−1/parenrightig
. (132)
By Lemma 6,⟨xt,ˆθ0
t,I†
t⟩−β0
t,I†
t∥xt∥(¯V0
t,I†
t)−1≤mini⟨xt,θi⟩with probability 1−2δ. Thus, with probability
1−2δ,µ(x⊤
tˆθ0
t,I†
t)−miniµ(x⊤
tθi)≤kµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1.
Becauseϵtis bounded by [cµ
cµ+kµ,1−αcµ
kµ], we can analyze E[rt,I0
t|Ft−1,It]in four cases.
Case 1: when⟨xt,ˆθ0
t,K⟩<⟨xt,ˆθ0
t,I†
t⟩, we haveϵt= 1−αcµ
kµandµ(⟨xt,ˆθ0
t,K⟩)<µ(⟨xt,ˆθ0
t,I†
t⟩). Thus,
E[rt,I0
t|Ft−1,It] = (1−αcµ
kµ)µ(x⊤
tθK) +αcµ
kµµ(x⊤
tθI†
t). (133)
Then, by Lemma 6,
(1−αcµ
kµ)µ(x⊤
tθK) +αcµ
kµµ(x⊤
tθI†
t)−µ((1−α)x⊤
tθK)
≤(1−αcµ
kµ)/parenleftbigg
µ(x⊤
tˆθ0
t,K) +kµβ0
t,K∥xt∥(¯V0
t,K)−1/parenrightbigg
+αcµ
kµ
µ(x⊤
tˆθ0
t,I†
t) +kµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1

−µ((1−α)x⊤
tθK)
≤µ(x⊤
tˆθ0
t,I†
t) +αcµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1+ (1−αcµ
kµ)kµβ0
t,K∥xt∥(¯V0
t,K)−1−µ((1−α)x⊤
tθK)
≤(kµ−αcµ)β0
t,K∥xt∥(¯V0
t,K)−1+ (kµ+αcµ)β0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1,(134)
where the second inequality is obtains by µ(⟨xt,ˆθ0
t,K⟩)<µ(⟨xt,ˆθ0
t,I†
t⟩)and the last inequality is obtained by
µ(x⊤
tˆθ0
t,I†
t)−miniµ(x⊤
tθi)≤kµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1and Assumption 1.
40Published in Transactions on Machine Learning Research (02/2023)
On the other side, we have
(1−αcµ
kµ)µ(x⊤
tθK) +αcµ
kµµ(x⊤
tθI†
t)−µ((1−α)x⊤
tθK)
≥(1−αcµ
kµ)/parenleftbigg
µ(x⊤
tˆθ0
t,K)−kµβ0
t,K∥xt∥(¯V0
t,K)−1/parenrightbigg
+αcµ
kµ
µ(x⊤
tˆθ0
t,I†
t)−kµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1

−µ((1−α)x⊤
tθK)
≥µ(x⊤
tˆθ0
t,K)−µ((1−α)x⊤
tθK)−αcµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1−(1−αcµ
kµ)kµβ0
t,K∥xt∥(¯V0
t,K)−1
≥µ(x⊤
tθK)−µ((1−α)x⊤
tθK)−αcµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1−(2−αcµ
kµ)kµβ0
t,K∥xt∥(¯V0
t,K)−1
≥−(2kµ−αcµ)β0
t,K∥xt∥(¯V0
t,K)−1−αcµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1.(135)
Case 2: whenµ(x⊤
tˆθ0
t,K)≥µ(x⊤
tˆθ0
t,I†
t)>(1 +cµ
kµ)µ((1−α)x⊤
tˆθ0
t,K)−cµ
kµµ(x⊤
tˆθ0
t,K)andϵt=cµ
cµ+kµ, we have
E[rt,I0
t|Ft−1,It] =cµ
cµ+kµµ(x⊤
tθK) +kµ
cµ+kµµ(x⊤
tθI†
t). (136)
By Lemma 6, with probability 1−2δ,
µ(x⊤
tˆθ0
t,I†
t)−min
iµ(x⊤
tθi)≤kµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1.
Since minix⊤
tθi≤(1−2α)x⊤
tθK, we have
µ(x⊤
tˆθ0
t,I†
t)≤µ((1−2α)x⊤
tθK) +kµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1
and then
µ(x⊤
tθI†
t)≤µ((1−2α)x⊤
tθK) + 2kµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1.
Thus,
cµµ(x⊤
tθK)
cµ+kµ+kµµ(x⊤
tθI†
t)
cµ+kµ−µ((1−α)x⊤
tθK)
=cµ
cµ+kµµ(x⊤
tθK)−cµ
cµ+kµµ((1−α)x⊤
tθK)
+kµ
cµ+kµµ(x⊤
tθI†
t)−kµ
cµ+kµµ((1−α)x⊤
tθK)
≤cµ
cµ+kµ/parenleftbig
µ(x⊤
tθK)−µ((1−α)x⊤
tθK)/parenrightbig
+kµ
cµ+kµ/parenleftbig
µ((1−2α)x⊤
tθK)−µ((1−α)x⊤
tθK)/parenrightbig
+2k2
µ
cµ+kµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1.(137)
41Published in Transactions on Machine Learning Research (02/2023)
According to the definition of kµandcµand Lemma 6,
cµµ(x⊤
tθK)
cµ+kµ+kµµ(x⊤
tθI†
t)
cµ+kµ−µ((1−α)x⊤
tθK)
≤cµ
cµ+kµkµ/parenleftbig
x⊤
tθK−(1−α)x⊤
tθK/parenrightbig
+kµ
cµ+kµcµ/parenleftbig
(1−2α)x⊤
tθK−(1−α)x⊤
tθK/parenrightbig
+2k2
µ
cµ+kµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1
=2k2
µ
cµ+kµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1.(138)
In addition, by Lemma 6,
cµµ(x⊤
tθK)
cµ+kµ+kµµ(x⊤
tθI†
t)
cµ+kµ−µ((1−α)x⊤
tθK)
=cµµ(x⊤
tθK)
cµ+kµ+kµµ(x⊤
tθI†
t)
cµ+kµ−µ((1−α)x⊤
tˆθ0
t,K)
+µ((1−α)x⊤
tˆθ0
t,K)−µ((1−α)x⊤
tθK)
≥cµ
cµ+kµµ(x⊤
tθK) +kµ
cµ+kµµ(x⊤
tθI†
t)−cµ
cµ+kµµ(x⊤
tˆθ0
t,K)−kµ
cµ+kµµ(x⊤
tˆθ0
t,I†
t)
+µ((1−α)x⊤
tˆθ0
t,K)−µ((1−α)x⊤
tθK)
≥−(1−α+cµ
cµ+kµ)kµβ0
t,K∥xt∥(¯V0
t,K)−1−kµ
cµ+kµkµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1,(139)
where the first inequality is obtained by the condition of case 2:
µ(x⊤
tˆθ0
t,I†
t)>(1 +cµ
kµ)µ((1−α)x⊤
tˆθ0
t,K)−cµ
kµµ(x⊤
tˆθ0
t,K)
which is equivalent to
cµµ(x⊤
tˆθ0
t,K)
cµ+kµ+kµµ(x⊤
tˆθ0
t,I†
t)
cµ+kµ>µ((1−α)x⊤
tˆθ0
t,K).
Case 3: when the attacker’s estimates satisfy
kµ
αcµµ((1−α)x⊤
tˆθ0
t,K)−(kµ
αcµ−1)µ(x⊤
tˆθ0
t,K)
≤µ(x⊤
tˆθ0
t,I†
t)
≤(1 +cµ
kµ)µ((1−α)x⊤
tˆθ0
t,K)−cµ
kµµ(x⊤
tˆθ0
t,K)(140)
and hencecµ
cµ+kµ≤ϵt≤1−αcµ
kµ, we have
E[rt,I0
t|Ft−1,It] =ϵtµ(x⊤
tθK) + (1−ϵt)µ(x⊤
tθI†
t). (141)
42Published in Transactions on Machine Learning Research (02/2023)
We can find that
ϵtµ(x⊤
tθK) + (1−ϵt)µ(x⊤
tθI†
t)−µ((1−α)x⊤
tθK)
=ϵt(µ(x⊤
tθK)−µ(x⊤
tθI†
t)) +µ(x⊤
tθI†
t)−µ((1−α)x⊤
tθK)
=ϵt(µ(x⊤
tˆθ0
t,K)−µ(x⊤
tˆθ0
t,I†
t)) +ϵt(µ(x⊤
tˆθ0
t,I†
t)−µ(x⊤
tθI†
t)) +ϵt(µ(x⊤
tθK)−µ(x⊤
tˆθ0
t,K))
+µ(x⊤
tθI†
t)−µ((1−α)x⊤
tθK)
=µ((1−α)x⊤
tˆθ0
t,K)−µ(x⊤
tˆθ0
t,I†
t) +ϵt(µ(x⊤
tˆθ0
t,I†
t)−µ(x⊤
tθI†
t)) +ϵt(µ(x⊤
tθK)−µ(x⊤
tˆθ0
t,K))
+µ(x⊤
tθI†
t)−µ((1−α)x⊤
tθK)
=µ((1−α)x⊤
tˆθ0
t,K)−µ((1−α)x⊤
tθK) +ϵt(µ(x⊤
tθK)−µ(x⊤
tˆθ0
t,K)) + (1−ϵt)/parenleftig
µ(x⊤
tθI†
t)−µ(x⊤
tˆθ0
t,I†
t)/parenrightig
.
(142)
From Lemma 6,/vextendsingle/vextendsingle/vextendsingleE[rt,I0
t|Ft−1,It]−µ((1−α)x⊤
tθK/vextendsingle/vextendsingle/vextendsingle
≤(1−α+ϵt)kµβ0
t,K∥xt∥(¯V0
t,K)−1
+ (1−ϵt)kµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1.(143)
Case 4: whenµ(x⊤
tˆθ0
t,I†
t)<kµ
αcµµ((1−α)x⊤
tˆθ0
t,K)−(kµ
αcµ−1)µ(x⊤
tˆθ0
t,K)andϵt= 1−αcµ
kµ, we have
E[rt,I0
t|Ft−1,It] = (1−αcµ
kµ)µ(x⊤
tθK) +αcµ
kµµ(x⊤
tθI†
t). (144)
Then, by Lemma 6,
(1−αcµ
kµ)µ(x⊤
tθK) +αcµ
kµµ(x⊤
tθI†
t)−µ((1−α)x⊤
tθK)
≤(1−αcµ
kµ)µ(x⊤
tθK)−µ((1−α)x⊤
tθK) +αcµ
kµµ(x⊤
tˆθ0
t,I†
t) +αcµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1
<(1−αcµ
kµ)µ(x⊤
tθK)−µ((1−α)x⊤
tθK) +µ((1−α)x⊤
tˆθ0
t,K)−(1−αcµ
kµ)µ(x⊤
tˆθ0
t,K) +αcµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1
≤(kµ−cµ)β0
t,K∥xt∥(¯V0
t,K)−1+αcµβ0
t,I†
t∥xt∥(¯V0
t,I†
t)−1.
(145)
Sincex⊤
tθI†
t>0,µ(x⊤
tθK)−µ(x⊤
tθI†
t)≤kµx⊤
tθK. Hence, we also have
(1−αcµ
kµ)µ(x⊤
tθK) +αcµ
kµµ(x⊤
tθI†
t)−µ((1−α)x⊤
tθK)
=µ(x⊤
tθK)−µ((1−α)x⊤
tθK)−αcµ
kµ/parenleftig
µ(x⊤
tθK)−µ(x⊤
tθI†
t)/parenrightig
≥cµαx⊤
tθK−αcµ
kµkµx⊤
tθK= 0.(146)
Combining these four cases, we have
/vextendsingle/vextendsingle/vextendsingleE[rt,I0
t|Ft−1,It]−µ((1−α)x⊤
tθK)/vextendsingle/vextendsingle/vextendsingle
≤2kµβ0
t,K∥xt∥(¯V0
t,K)−1+ 2kµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1. (147)
43Published in Transactions on Machine Learning Research (02/2023)
E.6 Proof of Lemma 8
The agent’s maximum-likelihood estimation can be written as the solution to the following equation
/summationdisplay
n∈τi(t−1)(rn−µ(x⊤
nˆθt,i))xn= 0. (148)
As described in the section E.2, we have gt,i(ˆθi)−gt,i((1−α)θK) =Z1+Z2and
x⊤
t(ˆθi−(1−α)θK) =x⊤
tG−1
t,i(Z1+Z2). (149)
We setZ1=/summationtext
n∈τi(t−1)(µ(x⊤
nθI0n)−µ((1−α)x⊤
nθK))xnandZ2=/summationtext
n∈τi(t−1)ηnxn.
In the white-box attack case, we have E[µ(x⊤
kθI0
k)|Fk−1] =µ((1−α)x⊤
kθK)and hence E[Z1|Fk−1] =0. Under
the proposed black-box attack, E[Z1|Fk−1]̸=0but
/vextendsingle/vextendsingle/vextendsingleE[µ(x⊤
tθI0
t)|Ft−1,It]−(1−α)⟨xt,θK⟩/vextendsingle/vextendsingle/vextendsingle
≤2kµβ0
t,K∥xt∥(¯V0
t,K)−1+ 2kµβ0
t,I†
t∥xt∥/parenleftbigg
¯V0
t,I†
t/parenrightbigg−1. (150)
We setZ1=Z5+Z6, where
Z5=/summationdisplay
n∈τi(t−1)(µ(x⊤
nθI0n)−E[µ(x⊤
tθI0
t)|Ft−1,It])xn
and
Z6=/summationdisplay
n∈τi(t−1)(E[µ(x⊤
tθI0
t)|Ft−1,It]−µ((1−α)x⊤
nθK))xn.
For any context x∈Dand armi̸=K, we have
|x⊤(ˆθt,i−(1−α)θK)|
≤|x⊤G−1
t,iZ2|+|x⊤G−1
t,iZ5|+|x⊤G−1
t,iZ6|.(151)
Since we have/braceleftig
µ(x⊤
kθI0
k)−E[µ(x⊤
kθI0
k)|Fk−1,Ik]/bracerightig
k∈τi(t−1)is a bounded martingale difference sequence w.r.t
the filtration{Fk,Ik}k∈τi(t−1)and is also kµLS-sub-Gaussian-sub-Gaussian.
FromTheorem1andLemma11in(Abbasi-Yadkorietal.,2011), weknowthatforany δ>0, withprobability
at least 1−K−1
Kδ
∥Z5∥¯V−1
t,i≤2kµLS/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2Ni(t)
λ0d/parenrightbigg
, (152)
for any arm i̸=Kand allt>0.
We have the fact that
∥xt∥2
V−1
t,i=x⊤
tV−1
t,i
/summationdisplay
k∈τi(t−1)xkx⊤
k+λI
V−1
t,ixt
≥x⊤
tV−1
t,i
/summationdisplay
k∈τi(t−1)xkx⊤
k
V−1
t,ixt
=/summationdisplay
k∈τi(t−1)(x⊤
tV−1
t,ixk)2.(153)
44Published in Transactions on Machine Learning Research (02/2023)
Hence,
∥xt∥2
G−1
t,i=x⊤
tG−1
t,iGt,iG−1
t,ixt
≥cµx⊤
tV−1
t,i
/summationdisplay
k∈τi(t−1)xkx⊤
k
V−1
t,ixt
=cµ/summationdisplay
k∈τi(t−1)(x⊤
tG−1
t,ixk)2,(154)
and hence/summationtext
k∈τi(t−1)(x⊤
tG−1
t,ixk)2≤1
c2µ∥xt∥2
¯V−1
t,i.
Then,|x⊤G−1
t,iZ6|can be upper bounded by
|x⊤G−1
t,iZ6|
≤/radicaltp/radicalvertex/radicalvertex/radicalbt/summationdisplay
k∈τi(t−1)/parenleftig
E[rk,I0
k|Fk−1,Ik]−µ((1−α)x⊤
kθK)/parenrightig2/radicaligg/summationdisplay
k∈τi(t−1)(x⊤
tG−1
t,ixk)2
≤
/summationdisplay
k∈τi(t−1)
2kµβ0
k,K∥xk∥(¯V0
k,K)−1+ 2kµβ0
k,I†
k∥xk∥/parenleftbigg
¯V0
k,I†
k/parenrightbigg−1
2
1
2
1
cµ∥xt∥¯V−1
t,i,(155)
where the first inequality is obtained from Cauchy-Schwarz inequality, the second inequality is obtained from
Lemma 7 and Equation 153.
In addition, by the fact that (a+b)2≤2a2+ 2b2for any real number, we have
/summationdisplay
k∈τi(t−1)
2kµβ0
k,K∥xk∥(¯V0
k,K)−1+ 2kµβ0
k,I†
k∥xk∥/parenleftbigg
¯V0
k,I†
k/parenrightbigg−1
2
≤/summationdisplay
k∈τi(t−1)2/parenleftbigg
2kµβ0
k,K∥xk∥(¯V0
k,K)−1/parenrightbigg2
+/summationdisplay
k∈τi(t−1)2
2kµβ0
k,I†
k∥xk∥/parenleftbigg
¯V0
k,I†
k/parenrightbigg−1
2
.(156)
Here, we use Lemma 11 from (Abbasi-Yadkori et al., 2011) and get, for any arm i,
/summationdisplay
k∈τ†
i(t−1)∥xk∥2
(V0
k,i)−1≤2dlog/parenleftbigg
1 +Ni(t)L2
dλ/parenrightbigg
≤2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
.(157)
Setλ=λ0, we have∥x∥2
¯V−1
t,i≤(1 +λ
λ0)∥x∥2
V−1
t,i≤2∥x∥2
V−1
t,i.
By the fact that/summationtext
iτi(t−1) =τ†
K(t−1), and/summationtext
i̸=Kτi(t−1) =/summationtext
i̸=Kτ†
i(t−1), we have, for any arm i,
τi(t−1)⊆τ†
K(t−1), andτi(t−1)⊆/summationtext
j̸=Kτ†
j(t−1). Thus,
/summationdisplay
k∈τi(t−1)∥xk∥2
(¯V0
k,K)−1≤/summationdisplay
k∈τ†
K(t−1)∥xk∥2
(¯V0
k,K)−1≤4dlog/parenleftbigg
1 +tL2
dλ0/parenrightbigg
,(158)
and/summationdisplay
k∈τi(t−1)∥xk∥2/parenleftbigg
¯V0
k,I†
k/parenrightbigg−1≤/summationdisplay
i̸=K/summationdisplay
k∈τ†
i(t−1)∥xk∥2
(¯V0
k,i)−1≤4(K−1)dlog/parenleftbigg
1 +tL2
dλ0/parenrightbigg
.(159)
45Published in Transactions on Machine Learning Research (02/2023)
By combining Equation 156, Equation 158 and Equation 159 and when K≥3, we have
/summationdisplay
k∈τi(t−1)
2kµβ0
k,K∥xk∥(V0
k,K)−1+ 2kµβ0
k,I†
k∥xk∥/parenleftbigg
V0
k,I†
k/parenrightbigg−1
2
≤/summationdisplay
k∈τi(t−1)2/parenleftbigg
2kµβ0
k,K∥xk∥(V0
k,K)−1/parenrightbigg2
+/summationdisplay
k∈τi(t−1)2
2kµβ0
k,I†
k∥xk∥/parenleftbigg
V0
k,I†
k/parenrightbigg−1
2
≤/parenleftbigg
2ϕKkµLS+R
cµ/parenrightbigg2/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg/parenrightbigg
×8k2
µ×16d2log/parenleftbigg
1 +tL2
dλ0/parenrightbigg
+/parenleftbigg
2Kµ
cµαkµLS+R
cµ/parenrightbigg2/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg/parenrightbigg
×8k2
µ×16d2(K−1) log/parenleftbigg
1 +tL2
dλ0/parenrightbigg
≤128k2
µd2/parenleftbigg2kµLS+ 2R
cµ/parenrightbigg2/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg/parenrightbigg
log/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenleftigg
(K−1)k2
µ
c2µα2+ (1 +kµ
cµ)2/parenrightigg
≤128k2
µd2/parenleftbigg2kµLS+ 2R
cµ/parenrightbigg2/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg/parenrightbigg
log/parenleftbigg
1 +tL2
dλ0/parenrightbigg
×2Kk2
µ
c2µα2.
(160)
In summary, we have
|x⊤
tˆθt,i−x⊤
t(1−α)θK|
≤2kµLS+ 2R
cµ/parenleftigg
1 +16k2
µd
cµα/radicaligg
Klog/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenrightigg/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg
∥x∥¯V−1
t,i.(161)
E.7 Proof of Theorem 4
For roundtand context xt, if UCB-GLM pulls arm i̸=K, we have
x⊤
tˆθt,K+βt,K/radicalig
x⊤
t¯V−1
t,Kxt≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
t¯V−1
t,ixt.
Recallβt,i=4R
cµ/radicalbigg
logK
δ+dlog/parenleftig
1 +L2Ni(t)
λ0d/parenrightig
.
Since the attacker does not attack the target arm, the confidence bound of arm Kdoes not change and
x⊤
tθK≤x⊤
tˆθt,K+βt,K/radicalig
x⊤
tV−1
t,Kxtholds with probability 1−δ
K.
Thus, by Lemma Equation 8,
x⊤
tθK≤x⊤
tˆθt,i+βt,i/radicalig
x⊤
tV−1
t,ixt
≤x⊤
t(1−α)θK+2kµLS+ 2R
cµ/parenleftigg
1 +16k2
µd
cµα/radicaligg
Klog/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenrightigg/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg
∥x∥¯V−1
t,i.
(162)
By multiplying both sides 1{It=i}and summing over rounds, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK
≤T/summationdisplay
k=11{Ik=i}2kµLS+ 2R
cµ/parenleftigg
1 +16k2
µd
cµα/radicaligg
Klog/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenrightigg/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg
∥x∥¯V−1
t,i.(163)
46Published in Transactions on Machine Learning Research (02/2023)
Here, we use Lemma 11 from (Abbasi-Yadkori et al., 2011) and obtain
T/summationdisplay
k=11{Ik=i}∥xk∥2
V−1
k,i≤2dlog(1 +Ni(t)L2
dλ)≤2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (164)
According to/summationtextT
k=11{Ik=i}∥xk∥V−1
k,i≤/radicalbigg
Ni(t)/summationtextT
k=11{Ik=i}∥xk∥2
V−1
k,i, we have
T/summationdisplay
k=11{Ik=i}∥xk∥V−1
k,i≤/radicaligg
Ni(t)2dlog/parenleftbigg
1 +tL2
dλ/parenrightbigg
. (165)
Setλ=λ0, we have∥x∥2
¯V−1
t,i≤(1 +λ
λ0)∥x∥2
V−1
t,i≤2∥x∥2
V−1
t,i. Thus, we have
T/summationdisplay
k=11{Ik=i}αx⊤
kθK
≤2kµLS+ 2R
cµ/parenleftigg
1 +16k2
µd
cµα/radicaligg
Klog/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenrightigg/radicaligg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg/radicaligg
4Ni(t)dlog/parenleftbigg
1 +tL2
dλ0/parenrightbigg
,
(166)
and
Ni(t) =T/summationdisplay
k=11{Ik=i}
≤4d
(αγ)2/parenleftbigg2kµLS+ 2R
cµ/parenrightbigg2
log/parenleftbigg
1 +tL2
dλ0/parenrightbigg
×/parenleftbigg
logK
δ+dlog/parenleftbigg
1 +L2t
λ0d/parenrightbigg/parenrightbigg/parenleftigg
1 +16k2
µd
cµα/radicaligg
Klog/parenleftbigg
1 +tL2
dλ0/parenrightbigg/parenrightigg2
,(167)
whereγ= minx∈D⟨x,θK⟩.
47