Under review as submission to TMLR
Investigations on Modularity and Invariance for Composi-
tional Robustness
Anonymous authors
Paper under double-blind review
Abstract
By default neural networks are not robust to changes in data distribution. This has been
demonstrated with simple image corruptions, such as blurring or adding noise, degrading
image classification performance. Many methods have been proposed to mitigate these issues
but for the most part models are evaluated on single corruptions. In reality, visual space is
compositionalinnature, thatis, thataswellasrobustnesstoelementalcorruptions, robustness
to compositions of corruptions is also needed. In this work we develop a compositional image
classification task where, given a few elemental corruptions, models are asked to generalize
to compositions of these corruptions. That is, to achieve compositional robustness . We
experimentally compare empirical risk minimization with an invariance building pairwise
contrastive loss and, counter to common intuitions in domain generalization, achieve only
marginal improvements in compositional robustness by encouraging invariance. To move
beyond invariance, following previously proposed inductive biases that model architectures
should reflect data structure, we introduce a modular architecture whose structure replicates
the compositional nature of the task. We then show that this modular approach consistently
achieves better compositional robustness than non-modular approaches. We additionally find
empirical evidence that the degree of invariance between representations of ‘in-distribution’
elemental corruptions fails to correlate with robustness to ‘out-of-distribution’ compositions
of corruptions.
1 Introduction
Biologically intelligent systems show a remarkable ability to generalize beyond their training stimuli, that
is to learn new concepts from no, or few, examples by combining previously learned concepts (Ito et al.,
2022; Lake et al., 2019; Piantadosi & Aslin, 2016; Schulz et al., 2016). In contrast, artificial neural networks
are surprisingly brittle, failing to recognize known categories when presented with images with fairly minor
corruptions (Dodge & Karam, 2017; Geirhos et al., 2021; Hosseini et al., 2017; Hendrycks & Dietterich, 2019;
Jang et al., 2021). Many methods have been proposed for learning more robust representations, including
data augmented training techniques (Hendrycks et al., 2020; 2021; Jang et al., 2021; Yun et al., 2019; Zhang
et al., 2018), and encouraging invariant representations or predictions (Huang et al., 2022; Kim et al., 2019;
Sinha & Dieng, 2021; Von Kügelgen et al., 2021). However, when the robustness of these methods is evaluated
it tends to be on single corruptions of the type seen in ImageNet-C (Hendrycks & Dietterich, 2019).
In reality, the space of possible corruptions is compositional. If we draw a loose correspondence between
corruptions and real world weather conditions, with noise akin to rain on a windshield, blur as fog and
a contrast change as a change in brightness, we see it is in fact possible to have rain, fog and bright
sun simultaneously. In this work we extend the notion of robustness over corruptions to robustness over
compositions of corruptions . We construct a compositional image classification task where a neural network
is trained on single elemental corruptions and evaluated on compositions of these corruptions (Figure 1).
Importantly, this is not an adversarial or no-free-lunch task, as we want the AI systems we develop to be
capable of compositional generalization (Bahdanau et al., 2019b; Chalmers, 1990; Fodor & Pylyshyn, 1988;
Goyal & Bengio, 2022; Lake & Baroni, 2018; Lake et al., 2017; Mendez & Eaton, 2022).
1Under review as submission to TMLR
Gaussian Blur (GB)Identity (ID)Contrast (CO)CO◦GBR90◦IM◦CO
IN◦R90◦CO◦SWIM◦INR90◦SW
SW◦IN◦GBImpulse Noise (IM)
Invert (IN)Swirl (SW)Rotate 90 (R90)GB◦R90◦SW◦COCO◦GB◦IM◦ IN◦R90◦SWElemental CorruptionsCompositions of Corruptions
Figure 1: The compositional robustness task. A model is trained jointly on images corrupted with elemental
corruptions (left) and evaluated on images corrupted with compositions of these corruptions (right). Shown
isall7elemental corruptions and a subsetof the 160compositions of corruptions.
If natural visual data can be decomposed into a set of elemental functions (or mechanisms (Peters et al.,
2017; Parascandolo et al., 2018)), we do not yet know how to find them. The compositional robustness task
we create allows us to experiment with a compositional structure where the underlying elemental functions
are known. By studying the behaviors of neural networks under this structure, we aim to gain insights into
how we might develop methods for better compositional robustness. Such insights could be applied to create
systems that generalize more robustly or allow for lower data collection costs, needing only to collect or
synthesize the elemental corruptions instead of the exponentially large number of compositions. Finally, this
task creates a new domain generalization task on which we can evaluate the generality of proposed methods
for domain generalization. In domain generalization parlance, a system is trained on data from multiple
training domains (the elemental corruptions), and then evaluated on data from a related set of test domains
(the compositions), from which no data samples are seen during training.
To better understand how neural networks behave on out-of-distribution compositional data we evaluate
different methods for domain generalization on this task. Firstly, we explore empirical risk minimization
(ERM), which has been shown to be a strong baseline when correctly tuned (Gulrajani & Lopez-Paz, 2022).
Secondly, we evaluate a setup where invariance between the same image under different corruptions is
explicitly encouraged using the contrastive loss (Chen et al., 2020; Gutmann & Hyvärinen, 2010; Hadsell
et al., 2006), since a central theme in domain generalization has been to encourage the learning of invariant
representations (Ahmed et al., 2021; Albuquerque et al., 2019; Arjovsky et al., 2019; Dou et al., 2019; Li et al.,
2018b; Ghifary et al., 2015; Kim et al., 2021; Li et al., 2018a; Motiian et al., 2017; Sakai et al., 2022; Creager
et al., 2021). Finally, we introduce a modular architecture to better reflect the compositional structure of the
task (Pfeiffer et al., 2023). Here, rather than all parameters jointly modelling all corruptions, each elemental
image corruption is ‘undone’ by a separate module in latent space.
Counter to our initial expectations we find that training to encourage invariant representations with the
contrastive loss offers only minor improvements in terms of out-of-distribution accuracy, whilst the modular
architecture consistently outperforms other methods. Additionally, we find that the degree of invariance
between representations of elemental corruptions fails to correlate with performance on out-of-distribution
compositions of corruptions. At their narrowest interpretation, these results empirically show that for composi-
tional robustness, when training domains consist only of the elemental components, modular approaches tend
to outperform monolithic (non-modular) approaches. At their broadest interpretation our results question
whether encouraging non-trivially1invariant representations is sufficient to achieve compositional domain
generalization. This indicates that there is still work to be done on understanding the additional properties
required for compositional robustness and suggests more modular architectures as a promising candidate for
one such property.
1The trivial case, with constant representations, has maximal invariance but cannot achieve good generalization.
2Under review as submission to TMLR
2 Related Work
We now briefly recap related works from the areas of domain generalization, invariant representations,
modularity, compositional generalization and robustness.
Domain Generalization and Invariant Representations. The creation of models that are robust to
unseen changes in data distribution is the work of domain generalization. Given certain training domains,
the aim of domain generalization is to build models that can generalize to related unseen test domains.
One common approach is to encourage the learning of invariant representations between training domains
whilst achieving high performance (Ahmed et al., 2021; Albuquerque et al., 2019; Arjovsky et al., 2019; Dou
et al., 2019; Ghifary et al., 2015; Kim et al., 2021; Li et al., 2018a;b; Motiian et al., 2017; Sakai et al., 2022;
Creager et al., 2021), with the idea that this will lead to invariant representations between training and test
domains and hence good generalization performance. However, this relies on an implicit assumption that we
have sufficient training domains that are reasonably representative samples from some meta-distribution of
domains (this has been made explicit in some works Krueger et al. (2021); Eastwood et al. (2022c)). It is
not clear that this will be true in general, and arguably replaces the problematic assumption of i.i.ddata
with an equally problematic assumption of i.i.ddomains. What’s more, such generalist approaches may be
unable to take structure amongst training domains into account. It should be noted that there has also been
substantial work on encouraging invariance for the related task of domain adaptation where (unlabelled) data
from test domains is available (Ganin et al., 2016; Tzeng et al., 2017; Sun & Saenko, 2016; Long et al., 2015;
2018; Eastwood et al., 2022b). Despite being motivated by theoretical work (Ben-David et al., 2007; 2010),
the central role of invariance in domain adaptation and generalization has been questioned (Zhao et al., 2019;
Johansson et al., 2019; Rosenfeld et al., 2021; Shen et al., 2022; Akuzawa et al., 2020). In Section 4.3 we
discuss the limitations of encouraging invariance for compositional robustness.
Relational Inductive Biases and Modularity. A closely related approach to learning robust represen-
tations aims to take advantage of explicit structure in data. These relational inductive biases (Battaglia
et al., 2018) aim to include knowledge about entities and the relations between them into neural network
architectures. For example, we can encode that entities should not change under certain transformations by
building invariance to these transformations into our architectures. Work on equivariance beyond translation
explicitly creates such robustness (Cohen & Welling, 2016; Weiler et al., 2018; Worrall et al., 2017) but is
usually formulated in terms of group actions (Cohen et al., 2019) so is limited to invertible transformations.
More general approaches aim to uncover structure by decomposing data into independent (causal) mechanisms
(Peters et al., 2017; Parascandolo et al., 2018; Schölkopf et al., 2021; Goyal et al., 2021; Goyal & Bengio, 2022)
or disentangled factors of variation (Chen et al., 2016; Higgins et al., 2017; Kim & Mnih, 2018; Roth et al.,
2023; Eastwood & Williams, 2018; Locatello et al., 2019; Schott et al., 2022; Montero et al., 2021; 2022).
Ways to explicitly model decomposable structures in data include pre-training on primitive components (Ito
et al., 2022) and using modular architectures to encode structure (Jaderberg et al., 2015; Andreas et al.,
2016a;b; D’Amario et al., 2021; Bahdanau et al., 2019b; Goyal et al., 2021; 2022; Mendez & Eaton, 2021;
Madan et al., 2022; Carvalho et al., 2023; Pfeiffer et al., 2023). In contrast, in this work we know how the
data structure decomposes and explore the performance of modular and non-modular architectures on the
recomposition of known elemental components.
Compositional Generalization. The visual world is compositional (Bahdanau et al., 2019a; Lake et al.,
2015; 2017; Romaszko et al., 2017; Krishna et al., 2017). Whilst much has been made of compositionality in
language (linguistic compositionality) and reasoning (conceptual compositionality) (Andreas et al., 2016a;
Battaglia et al., 2018; Furrer et al., 2020; Hu et al., 2017; Johnson et al., 2017; Lake & Baroni, 2018; Lake
et al., 2017; Liška et al., 2018; Mendez & Eaton, 2022; Qiu et al., 2022; Xie et al., 2022; Schmidhuber,
1990), compositional robustness has received relatively little attention. Recent AI systems still fail on
compositional tasks (Keysers et al., 2020; Lake & Baroni, 2018; Schott et al., 2022; van der Velde et al., 2004)
where the space of generalization grows exponentially with the number of elemental components. Whilst
practically it is not possible to sample all combinations of elemental components, one interpretation of
large models (Geirhos et al., 2021; Kaplan et al., 2020; Radford et al., 2021) is that they aim to sample
densely enough to generalize to unseen combinations. However, for real world data, it is unclear how big the
compositional space is and how densely we need to sample, with this being particularly pertinent if the data
distribution is high-dimensional (Geiger et al., 2020; Wainwright, 2019) or fat tailed (Taleb, 2020). To that
3Under review as submission to TMLR
end, several works have analyzed controlled settings, aiming to understand the best settings for training in
order to achieve the best generalization (Ahmed et al., 2021; Cooper et al., 2021; Schott et al., 2022).
Robustness Over Image Corruptions. Whilst the aforementioned work aims to improve the robustness
of neural networks, many have worked specifically on improving robustness for common image corruptions
and adversarial examples (Hendrycks & Dietterich, 2019; Jang et al., 2021; Hendrycks et al., 2020; 2021; Yun
et al., 2019; Zhang et al., 2018; Huang et al., 2022; Kim et al., 2019; Sinha & Dieng, 2021; Von Kügelgen et al.,
2021; Baidya et al., 2021). However, the majority of previous works are evaluated only on single corruptions,
ignoring the true compositional space formed by the corruptions.
3 Methods
We now describe the methodology we use to investigate robustness to compositions of image corruptions.
We begin by creating a framework for evaluation and then describe the different approaches we investigate
including training with ERM, encouraging invariance and building modular architectures.
3.1 A Framework for Evaluating Compositional Robustness
We design a framework for evaluating compositional robustness on any dataset for image classification. We
first create elemental components by applying six different corruptions separately to all images. These
corruptions along with the original, Identity (ID) , data create 7training domains. We use the corruptions
Contrast (CO) ,Gaussian Blur (GB) ,Impulse Noise (IM) ,Invert (IN) ,Rotate 90◦(R90)andSwirl (SW) ,
seen in Figure 1 (left). We choose these corruptions to include a mixture of long-range and local effects as
well as invertible and non-invertible corruptions. A further exploration of the choice and parameter settings
of corruptions is given in Appendix A.
To test compositional robustness we create images from compositions of the elemental corruptions, see
Figure 1 (right). We consider every possible permutation of compositions of two corruptions (excluding
Identity) giving6P2= 30possible compositions. For compositions of more than two corruptions we sample
the possible permutations to approximately balance the contributions of compositions containing different
numbers of elemental corruptions (the sampling process is described in Appendix A). This creates 40possible
compositions of 3corruptions and 30possible compositions for each of 4,5and6corruptions. Altogether the
compositions form 160test domains. The task we then try to solve is to achieve the highest classification
accuracy on images from the 160compositional test domains whilst training only on the 7elemental training
domains.
3.2 Monolithic Approaches
A domain generalization task consists of data from related domains or environments De={(x(i)
e,y(i)
e)}Ne
i=1,
withe∈Eall, whereEallis the set of all domains we wish to generalize to and Nethe number of datapoints
in domaine. However, during training we only have access to a subset of domains Etr⊂Eall. For our task,
Etris the set of elemental training domains, |Etr|= 7, andEalladditionally includes the compositional test
domains,|Eall|= 167. As we use the same set of base images to create corrupted images, the number of
datapoints, Ne, is the same across all domains.
For a neural network fθparameterized by θ, we aim to find parameters, θ∗, from parameter space Θ, that
optimize loss function L, on training domains Etr. The accuracy of fθ∗is then evaluated on the test domains.
Monolithic approaches share all parameters, θ∗, over all domains where,
θ∗= argmin
θ∈Θ/summationdisplay
e∈EtrNe/summationdisplay
i=1L(fθ(x(i)
e,y(i)
e)). (1)
The first approach we evaluate is Empirical Risk Minimization (ERM), training all parameters jointly to
minimize some risk function over training domains. We set Lto be the mean cross entropy loss.
The second approach we evaluate is contrastive training. A standard domain generalization approach is
to encourage invariance between representations on the training domains (Zhou et al., 2022) and since we
4Under review as submission to TMLR
have paired data between domains we can explicitly encourage invariance using the contrastive loss (Chen
et al., 2020; Gutmann & Hyvärinen, 2010; Hadsell et al., 2006). Note that the availability of paired data
creates a best-case set up for the learning of invariant representations and that learning a representation
that is invariant for paired images from different domains would satisfy the invariance encouraging criteria of
previous works (Arjovsky et al., 2019; Li et al., 2018b; Dou et al., 2019).
We follow the SimCLR contrastive training formulation (Chen et al., 2020), taking Bdatapoints from each
elemental training domain (created from the same base images) to get a minibatch of size B|Etr|. Applying
an additional index to each of the domains in Etrto getEtr={ed}D
d=1, positive pairs come from pairs of
the same image under different corruptions (x(i)
er,x(i)
es),r̸=s, and negative pairs from all other pairs in the
minibatch (x(i)
er,x(j)
es),i̸=j. We apply the contrastive loss on representations from the penultimate layer of
fθ, notating the representation for x(i)
easz(i)
e. Using cosine similarity, sim(u,v) =uTv/∥u∥∥v∥, to measure
similarity between representations we define the loss for a positive pair in the minibatch as
ℓ(x(i)
er,x(i)
es) =−logexp(sim(z(i)
er,z(i)
es)/τ)
/summationtextD
d=1/summationtextB
k=11[k̸=i] exp(sim(z(i)
er,z(k)
ed)/τ), (2)
whereτis a temperature parameter and 1[k̸=i]is an indicator function equal to 1whenk̸=iand0otherwise.
We compute this loss across all positive pairs in the minibatch to encourage invariant representations. To
learn to classify, we additionally include the cross entropy loss to arrive at,
L(fθ(x(i)
er,y(i)
er)) =−C/summationdisplay
c=11[y(i)
er=c] log(σ(fθ(x(i)
er))c+λ/summationdisplay
es∈Etr
s̸=rℓ(x(i)
er,x(i)
es). (3)
Here the first term is the cross entropy loss, with Cthe total number of categories, 1[y=c]an indicator
function that is 1wheny=cand0otherwise, σthe softmax operation, and, in a slight overloading of
notation, subscript crepresents the cthentry of the log-softmax vector. λis a hyper-parameter weighting the
influence of the cross entropy and contrastive terms. Note also, as described above, Equation 3 is calculated
on a minibatch rather than over all datapoints simultaneously.
To evaluate the monolithic approaches on compositions of corruptions we simply calculate classification
accuracy on the domains in Eall.
3.3 A Modular Approach
The final approach we evaluate is a modular architecture, as it has been argued that modularity is a key
feature of robust, intelligent systems (Goyal & Bengio, 2022; Mahowald et al., 2023). For each elemental
corruption we add one module to our network which aims to ‘undo’ the corruption in latent space. In practice
these modules are intermediate layers that operate on hidden representations to map the representation of a
corrupted image to the representation of the same image when uncorrupted. To make this possible modules
are designed to have input and output features with the same shape. When classifying a test image corrupted
with a composition of elemental corruptions we sequentially apply the modules for each corruption present
in the composition. For example, if we are testing on the composition IN◦GBwe apply both the module
trained on the Invertcorruption and the module trained on the Gaussian Blur corruption. Modules that are
located in-between earlier layers of the network are applied first, if modules are in the same layer we apply
the module which appears first in the permutation ordering (Section 3.1).
To formalize this idea, we split network parameters θinto one set of parameters shared over all domains,
θshared, and an additional set of domain specific module parameters for each training domain {θe}e∈Etr,
similar to residual adaptation (Rebuffi et al., 2017; 2018). In practice θsharedparameterizes a neural network
andθethe intermediate layers that can be inserted when working with domain e.
To train this system we first train parameters θsharedonIdentity data using the cross entropy loss. We then
freezeθsharedand train separate modules parameterized by θeon data from each elemental training domain
e∈Etralong with paired Identity data. Since we encourage the modules to ‘undo’ corruptions, we use the
loss function from Equation 3 with minor modifications. Firstly, the set of domains for the contrastive loss is
5Under review as submission to TMLR
limited to only the relevant elemental training domain and the Identity domain. Secondly, for the Identity
data, latent representation zis from the layer at which the module is inserted and for the corrupted data
from the output of the module, spatially flattening the feature map if required (as opposed to from the
penultimate network layer as described when introducing Equation 2). Appendix B contains a graphical
depiction of this process.
An important design choice for any modular approach is how to choose where to locate the modules, with
recent works observing that different domain changes should be dealt with in different neural network layers
(Eastwood et al., 2022a; Royer & Lampert, 2020; Lee et al., 2023). We take a very simple automatic approach,
training separate modules between each layer of the network parameterized by θsharedfor5epochs. We then
select the module with the best in-distribution accuracy on a held-out validation set as the module to train
to completion. This is similar to using adaptation speed (Bengio et al., 2019; Le Priol et al., 2021) as a proxy
to discover modular decompositions, although in practice we find if we use adaptation times substantially
smaller than 5epochs we can erroneously select module locations that do not achieve optimal in-distribution
performance.
3.4 Measuring the Invariance of Learned Representations
Since encouraging invariance is a prominent theme in the domain generalization literature (Zhou et al., 2022)
we also empirically investigate the role of invariant representations in generalizing to unseen compositions of
corruptions. We create two invariance scores following the methods of Madan et al. (2022).
For every neuron in the penultimate layer of a network (after applying modules if applicable) we calculate the
mean activation per domain-category pair over all test data. The activations are normalized by the maximum
firing of the neuron over all domains with any dead neurons (with maximum firing less than 10−6) discarded.
For a specific test domain (a specific composition of corruptions) we select only the domain-category pairs
where the domain is either the test domain itself or one of the elemental corruptions used to create the
composition for the test domain. For each neuron, this creates an activation grid with a column-count equal
to the number of categories in the dataset and row-count equal to the number of elemental corruptions in the
composition plus one row for the composition itself. An example grid is shown in Appendix C, Table 3.
This activation grid is then normalized again so that all values lie between 0and1by subtracting the
minimum value in the grid from every cell and dividing by the difference between the maximum and minimum
values. We notate the activation values by ai,j, withireferencing the domain and jthe category. Additionally
we take the number of elemental corruption domains in the grid to be indexed 1,...,Eand the composition
to have index E+ 1, that is,i∈{1,...,E + 1}. Taking the view that neurons can be interpreted as feature
detectors (Olah et al., 2020; Sarkar et al., 2023), we select the preferred category, j∗, on the training domains
as the category for which the neuron maximally activates, j∗=argmaxj/summationtextE
i=1ai,j. We then calculate the
elemental invariance score ,Ieas the maximum difference in activations amongst the elemental corruptions,
with the idea that this score should be high when all elemental corruptions activate the neuron in a similar
way. We additionally calculate the composition invariance score ,Ic, which measures how similarly the neuron
activates on the composition compared to the closest elemental corruption.
Ie= 1−(max
iai,j∗−min
iai,j∗), I c= 1−min{|ai,j∗−aE+1,j∗|}E
i=1. (4)
These scores always lie between 0and1, with higher numbers representing more invariant representations.
We calculate these scores for every neuron in the penultimate layer of the network and report the median
scores over all (non-dead) neurons in our results. We refer the reader to Madan et al. for further motivation
and details or Appendix C for an illustrative example.
3.5 Datasets, Architectures and Training Procedure
We evaluate each training approach on three different datasets for image classification: emnist(Cohen
et al., 2017), an extended mnistwith 47handwritten character classes; cifar- 10(Krizhevsky et al., 2009), a
simple object recognition dataset with 10classes, and facescrub (Ng & Winkler, 2014), a face-recognition
6Under review as submission to TMLR
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling
(a) EMNIST
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling (b) CIFAR-10
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling
ERM
Contrastive
Modular (c) FACESCRUB
Figure 2: Evaluating compositional robustness on different datasets. Evaluation domains are divided into
groups depending on the number of elemental corruptions making up a composition. Different colored boxes
(left to right in each triple) show the performance of ERM, contrastive training, and the modular approach.
Ceiling accuracy is determined by a model trained and tested on Identity data.
dataset. For facescrub we follow (Vogelsang et al., 2018) removing classes with fewer than 100 images,
resulting in 388classes, with each class representing an individual identity. We train using stochastic
gradient descent with momentum 0.9and weight decay 5×10−4, learning rate is set using a grid search over
{1,10−1,10−2,10−3}and contrastive loss weighting, λ, over{10,1,10−1,10−2}, with the best setting selected
based on the performance on a validation set of the training domains (Gulrajani & Lopez-Paz, 2022). τfrom
Equation 2 is set to 0.15in all experiments. We use a batch size of 256(or the nearest multiple of |Etr|for
the contrastive loss) and train for a maximum of 200epochs, using early stopping on the held out validation
set. Each experiment is run over three randomly seeded initializations. cifar- 10andfacescrub images are
augmented with random cropping and flipping, ensuring positively paired examples receive exactly the same
augmentation. For emnistwe use a simple convolutional network with a LeNet-like (LeCun et al., 1998)
architecture with modules made up from convolutional layers. For cifar- 10we use ResNet18 (He et al., 2016)
without the first max pooling layer, wherever possible using ResNet blocks as modules. For facescrub we
use Inception-v3 (Szegedy et al., 2016) without the auxiliary classifier. As with ResNet we use additional
Inception-v3 layers as modules wherever possible. For full architectural details see Appendix D.
4 Results
In this section we evaluate the compositional robustness of the different training approaches, first by
examining the accuracy of different methods on unseen compositions of corruptions. We additionally explore
the relationship between compositional robustness and invariance amongst representations of elemental
corruptions. We end on the practical limitations of the approaches we consider in this study.
4.1 Monolithic Approaches Show Limited Compositional Robustness
Figure 2 shows the classification accuracies of each of the three approaches for each of the three datasets.
The evaluation domains, Eall, are divided into groups depending on how many elemental corruptions are in
the composition applied to images in a domain. Across all methods and datasets we see domains with 1
corruption achieve very good, near ceiling, performance. This is not surprising as this represents the accuracy
on the elemental training domains. A granular view for each of the 167domains for every method can be
seen in heat maps in Appendix G.
In Figure 2 the blue and orange box plots show the performance of ERM and contrastive training respectively,
for which we can observe some general trends. Firstly, accuracy on compositions drops as the number of
elemental corruptions in a composition increases, with compositions of 5or6corruptions rarely performing
above chance level. Intuitively, as each additional corruption makes the image harder to recognize (see
Figure 1), it makes sense that this pattern emerges. Perhaps more surprisingly, both methods achieve
7Under review as submission to TMLR
Number of Corruptions 1 2 3 4 5 6
ERM 88.2 (0.1) 79.5 (0.1) 17.6 (0.1) 7.4 (0.3) 2.8 (0.2) 2.5 (0.1)
emnist Contrastive 87.7 (0.1) 79.7 (0.2) 19.5 (0.4) 8.6 (0.4) 3.1 (0.2) 2.9(0.2)
Modular 88.8 (0.0) 85.1 (0.5) 54.2 (3.2) 12.9 (1.6) 3.4(0.3) 2.3 (0.1)
ERM 88.5 (0.2) 66.4 (0.6) 26.1 (1.2) 13.5 (0.2) 10.2 (0.1) 9.6 (0.5)
cifar- 10 Contrastive 89.2 (0.2) 67.6 (1.8) 28.6 (2.1) 14.7 (0.8) 9.9 (0.3) 9.2 (0.2)
Modular 89.4 (0.1) 78.5 (0.9) 60.5 (6.1) 23.9 (5.8) 12.3 (1.2) 10.4 (0.4)
ERM 95.1 (0.1) 90.1 (0.3) 35.4 (4) 2.3 (0.3) 0.5 (0.1) 0.3 (0.0)
facescrub Contrastive 96.4 (0.2) 92.1 (0.5) 38.9 (6.8) 1.6 (0.2) 0.4 (0.0) 0.3 (0.0)
Modular 95.8 (0.2) 89.9 (1.0) 48.8 (25.1) 5.3(2.7) 1.1(0.4) 0.4(0.1)
Table 1: Compositional robustness shown for compositions of different numbers of corruptions. The median
accuracy over all compositions for a given number of corruptions is calculated, this table reports the maximum
median (and standard deviation) over three random runs.
accuracy far above chance for compositions of 2corruptions and perform relatively well for compositions of 3
corruptions despite these domains being outside of the training distribution. We also see that the contrastive
training approach makes only minor improvements over ERM, with the most improvement for cifar- 10.
This runs counter to our assumption that encouraging invariance amongst training domains would increase
compositional robustness. Finally, we note that neither method optimally solves the task, some compositions
of2corruptions contain only invertible corruptions, yet neither method reaches ceiling performance for any
composition of 2corruptions.
4.2 The Modular Approach Achieves the Best Compositional Robustness
Comparing all three training approaches, we observe that the modular approach outperforms both ERM and
contrastive training, with higher average performance in almost all cases in Figure 2. The only exception is
on compositions of 2corruptions for facescrub , where the modular approach is marginally outperformed
by contrastive training. We can observe that, in general, explicitly modularizing the modelling of elemental
corruptions outperforms the direct encouragement of invariance in terms of compositional robustness. This
demonstrates that the monolithic approaches are unable to learn to modularize the structure of the task
in the same way as the modular approach. Figure 2 additionally shows the spread of results over different
numbers of corruptions. Across methods there are always some compositions of corruptions that are much
harder to resolve than others. For example, for compositions of two corruptions using cifar- 10, there are
several compositions which achieve accuracy of 70-90%, but several others below 30%. Overall, no method is
yet fully robust to compositions of corruptions.
A tabular view on the performance and variability over seeds is provided in Table 1, where we see the modular
approach outperforming alternative methods in almost all cases as in Figure 2. Table 1, reports the maximum
median performance over seeds along with the standard deviation. This metric is chosen because, within
a fixed number of corruptions, most of the variance for the modular approach comes from the placement
of the modules rather than the effect of modularizing visual processing. In these results we aim to explore
primarily the effect of good modularization when compared with other methods for improving compositional
robustness, the effect of module location on results is investigated further in Section 4.4.
4.3 In-Distribution Invariance Does Not Correlate With Compositional Robustness
To investigate our findings further, we examine the invariance scores for the different approaches splitting
test domains by the number of elemental corruptions they include to plot correlations. In Figure 3, top
row, we plot the elemental invariance score against accuracy on compositions for emnist. We observe no
meaningful correlation between elemental invariance scores and accuracies on compositional test domains,
with high p-values and low r-values. This runs counter to our initial expectations based on the ubiquity of
invariant representation learning in the domain generalization literature. For our task, these results indicate
that encouraging invariance between representations on the training domains may be insufficient to achieve
8Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.10. p = 0.60.
Elemental Invariance Score
 Contrastive. r = 0.07. p = 0.72.
Elemental Invariance Score
 Modular. r = -0.46. p = 0.01.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.30. p = 0.06.
Elemental Invariance Score
 Contrastive. r = 0.13. p = 0.42.
Elemental Invariance Score
 Modular. r = -0.55. p = 0.00.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.35. p = 0.06.
Elemental Invariance Score
 Contrastive. r = 0.14. p = 0.47.
Elemental Invariance Score
 Modular. r = -0.23. p = 0.22.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.38. p = 0.04.
Elemental Invariance Score
 Contrastive. r = -0.23. p = 0.21.
Elemental Invariance Score
 Modular. r = -0.11. p = 0.55.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.10. p = 0.59.
Elemental Invariance Score
 Contrastive. r = 0.21. p = 0.26.
Elemental Invariance Score
 Modular. r = -0.70. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.01. p = 0.94.
Elemental Invariance Score
 Contrastive. r = -0.01. p = 0.96.
Elemental Invariance Score
 Modular. r = -0.83. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.11. p = 0.58.
Elemental Invariance Score
 Contrastive. r = -0.15. p = 0.43.
Elemental Invariance Score
 Modular. r = -0.85. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.50. p = 0.00.
Elemental Invariance Score
 Contrastive. r = -0.87. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.93. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = -0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.45. p = 0.01.
Composition Invariance Score
 Contrastive. r = 0.83. p = 0.00.
Composition Invariance Score
 Modular. r = 0.80. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.56. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.95. p = 0.00.
Composition Invariance Score
 Modular. r = 0.86. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.42. p = 0.02.
Composition Invariance Score
 Contrastive. r = 0.94. p = 0.00.
Composition Invariance Score
 Modular. r = 0.64. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = -0.40. p = 0.03.
Composition Invariance Score
 Contrastive. r = 0.60. p = 0.00.
Composition Invariance Score
 Modular. r = 0.30. p = 0.11.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.86. p = 0.00.
Composition Invariance Score
 Contrastive. r = -0.87. p = 0.00.
Composition Invariance Score
 Modular. r = 0.12. p = 0.52.
Corruptions in Composition = 6
Figure 3: Correlating invariance scores with compositional robustness for emnist. Row one shows the level
of representational invariance amongst elemental corruptions fails to correlate with compositional robustness
(accuracy). Row two shows the lack of dissemination of invariance between elemental corruptions and
compositions. Row three plots composition invariance scores against compositional robustness. Columns show
subsets of evaluation domains depending on the number of elemental corruptions making up a composition
(as in Figure 2).
9Under review as submission to TMLR
robustness. We even see some points for the modular approach (in the upper left of the plots) that achieve
higher accuracy than ERM or contrastive training achieve on any domain yet have lower invariance scores.
We also note that contrastive training only slightly increases the observed invariance between elemental
corruptions, with a small rightward shift of points when compared to ERM. One possible reason for this
smaller than expected increase may be because we set hyper-parameters on the training domains (Gulrajani
& Lopez-Paz, 2022) and high contrastive weights take away from in-distribution performance. Alternatively,
there has been some discussion on whether the contrastive loss improves performance because of increased
invariance or by other mechanisms Shen et al. (2022); Sakai et al. (2022).
Row three of Figure 3 shows strong positive correlations between the composition invariance score and
accuracy on compositions. This is as expected, since a high composition invariance score indicates a similar
representation between compositions and elemental corruptions (which all achieve good accuracy). However,
in row two of Figure 3 we again see limited, or even negative, correlations between elemental and composition
invariance scores. This suggests that invariance built on elemental training domains may not transfer to
invariance on compositional test domains, so we cannot consistently improve the composition invariance score
by encouraging elemental invariance.
By and large these trends are consistent over datasets (Appendix E) and seeds (Appendix F). A notable
exception is the negative correlation for the modular approach in row two of Figure 3 is not seen in other
datasets. We also observe a positive correlation between elemental invariance score and accuracy for ERM on
cifar- 10. Oncifar- 10, the encouraging of invariance with contrastive training builds slightly more invariant
representations but the correlation between elemental invariance and accuracy disappears.
4.4 Practical Limitations
The aim of this work is to provide greater understanding of the factors that influence compositional robustness
in neural networks. In particular, it is not our aim to provide an directly applicable method for improving
compositional robustness. Nevertheless we now show some additional experiments to briefly highlight some
of the practical limitations of the modular approach taken in this study.
Firstly, compared to the monolithic approaches, the modular approach has substantially higher variance
over seeds (particularly for facescrub in Table 1). This is primarily due to variance in the selection of the
module locations. To show this, Table 2 shows results for individual seeds for the modular approach evaluated
withfacescrub using alternative module locations. In Table 2 for Seed 1the automatic module placement
strategy described in Section 3.3 finds a placement that outperforms the monolithic approaches (Table 1).
However, for the remaining seeds, the automatically placed modules show substantially worse performance,
hence why Table 1 shows such high variance for the modular approach. The Manualcolumns of Table 2 show
that by manually setting the locations of the modules to match those found for Seed 1, these alternate seeds
achieve similar accuracy to Seed 1demonstrating that most of the variance is due to the difficulty of placing
modules in the correct location. This result emphasises the strength of the modular approach for handling
compositional structure but highlights the challenge of how to correctly architect modular systems as we
cannot feasibly try every module in every location and often we do not have compositional validation data
for model selection (Gulrajani & Lopez-Paz, 2022).
Additionally, apart from ERM all of the evaluated methods require paired data between domains which is an
unrealistic expectation in practical applications. For modular approaches we must know which corruptions
are applied in a given test domain in order to apply the correct modules. Another interesting angle for future,
more practically minded, solutions is to remove or reduce these assumptions.
5 Discussions
We end with several discussions on different interpretations of this work and links to larger questions that
may motivate future work.
What is the structure of natural data? In our compositional robustness framework we see only the
elemental factors of variation (elemental corruptions) during training. In reality, whilst it is likely not possible
to see every composition, most real-world data will contain an unstructured sampling of the compositional
10Under review as submission to TMLR
Num. Corrs. 1 2 3 4 5 6
Seed 1Automatic 95.5 87.9 48.8 5.3 1.1 0.4
Manual 95.5 87.9 48.8 5.3 1.1 0.4
Seed 2Automatic 95.8 88.7 7.7 0.8 0.3 0.3
Manual 95.7 89.1 47.1 5.5 0.9 0.4
Seed 3Automatic 95.6 89.9 3.3 0.4 0.3 0.3
Manual 95.4 89.4 47.9 4.6 0.6 0.4
MeanAutomatic 95.6 (0.2) 88.8 (1.0) 19.9 (25.1) 2.2 (2.7) 0.6 (0.5) 0.3 (0.1)
Manual 95.5 (0.2) 88.8 (0.8) 47.9 (0.9) 5.1(0.5) 0.9(0.3) 0.4(0.0)
Table 2: Comparing compositional robustness for facescrub using automatic module placement vs. manually
placing modules at the locations found when using automatic placement for Seed 1. The median accuracy
over all compositions for a given number of corruptions is calculated. The final row reports the mean (and
standard deviation) of these median values over the three random runs.
space. This assumes however, that it is possible to decompose data from the environment into elemental factors
of variation (Chen et al., 2016; Higgins et al., 2017; Kim & Mnih, 2018; Roth et al., 2023) or independent
(causal) mechanisms (Peters et al., 2017; Schölkopf et al., 2021; Parascandolo et al., 2018). At present it
remains unknown if there exists a practically sized set of elemental transformations from which all visual
stimuli can be composed, but if such a set exists, the ideas presented in this work suggest that modular
architectures may be able to model this space more efficiently than large monolithic models.
Learning to decompose from data. If there exists a set of elemental transformations from which all
visual stimuli can be composed, and we are to make use of modularity as an inductive bias to model them, we
mustlearnhow to decompose datasets into their constituent factors and how to modularize knowledge in the
appropriate semantic spaces (Goyal & Bengio, 2022). In this work we have shown that modular approaches
have the potential to surpass previous approaches if the decomposition is available and progress has been
made on finding appropriate semantic spaces (Royer & Lampert, 2020; Eastwood et al., 2022a). The learning
of decompositions remains an open problem (Parascandolo et al., 2018; Bengio et al., 2019; Locatello et al.,
2019; Goyal et al., 2021).
How modular should neural networks be? The modular approach taken in this study uses neural network
layers as modules which are manually assigned to handle specific corruptions, yet we have also experimented
with monolithic networks and with using entirely separate networks for each corruption (Section 4.4). Even if
we are able to decompose data into constituent factors, there remains a question of what degree of modularity
should be used to model these factors. There have been recent exciting empirical studies in this direction
(D’Amario et al., 2021; Madan et al., 2022; Yamada et al., 2023) but no consensus has yet been reached.
What is a module and why do they work? We consider modular methods to be exactly those methods
that explicitly parameterize domain specific information. For example, rather than only encouraging invariance
across elemental corruptions we could additionally have provided each sample with a label for each corruption.
Adding a label can be seen as a special, less general, case of modularity. In the case of a one hot label, this is
equivalent to adding a learnable bias vector per-corruption in the first layer of the network. This means a set
of corruption specific parameters is added per-corruption, i.e. that we can consider this a module.
In this work, we use this understanding of modularity to broadly categorize approaches for improved robustness
and domain generalization into three main strands. Firstly, ERM, where we train jointly with no task-specific
inductive biases. Secondly, invariance , where we try to build invariant representations without including any
domain specific information. Thirdly, modularity , which is any method that includes separate processing of
domain information, explicitly (as we do) or implicitly by encouraging modules to form or by adding labels.
In this work we find modular networks often outperform monolithic networks that do not contain any domain
specific information. It may even be possible to get the benefits of different approaches, with recent works
beginning to investigate hybrid solutions that make use of both invariance and domain specific information
(Eastwood et al., 2023).
11Under review as submission to TMLR
Our results, and the results of others (Zhao et al., 2019; Johansson et al., 2019; Rosenfeld et al., 2021; Shen
et al., 2022), raise questions about whether encouraging invariance alone is sufficient to achieve domain
generalization in general. We know that invariance is a key factor for robust generalization but we do not
yet know how far invariance will be able to take us. Perhaps we simply need to better understand and
implement the neural mechanisms that allow invariances to build (Anselmi et al., 2016; Poggio & Anselmi,
2016; Schölkopf et al., 2021; Goyal & Bengio, 2022), or we may need to further explore learning representations
that are only partially invariant (Kong et al., 2022; Shen et al., 2022; Sun et al., 2023).
6 Conclusion
Since the visual space containing all corruptions is compositional in nature, we have introduced a new
framework to evaluate the compositional robustness of different models. We have observed that modular
approaches outperform monolithic approaches on this task, even when invariant representations are encouraged.
For domain generalization tasks with compositional structure our results raise questions about the efficacy of
encouraging invariance without further inductive biases. This work representsonly a first step in understanding
howneuralnetworksbehaveundercompositionalstructures, furtherresearchisneededintodevelopingmethods
that make fewer assumptions about the information available at test time and that can work with large
unstructured datasets where factors of variation are unknown.
References
Faruk Ahmed, Yoshua Bengio, Harm Van Seijen, and Aaron Courville. Systematic generalisation with group
invariant predictions. In International Conference on Learning Representations , 2021.
Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning with accuracy
constraint for domain generalization. In Machine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2019, Würzburg, Germany, September 16–20, 2019, Proceedings, Part II , pp.
315–331. Springer, 2020.
Isabela Albuquerque, João Monteiro, Mohammad Darvishi, Tiago H Falk, and Ioannis Mitliagkas. Generalizing
to unseen domains via distribution matching. arXiv preprint arXiv:1911.00804 , 2019.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks
for question answering. In Proceedings of the 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , 2016a.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pp. 39–48, 2016b.
Fabio Anselmi, Joel Z Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio. Unsuper-
vised learning of invariant representations. Theoretical Computer Science , 633:112–121, 2016.
Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv
preprint arXiv:1907.02893 , 2019.
Dzmitry Bahdanau, Harm de Vries, Timothy J O’Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua
Bengio, and Aaron Courville. Closure: Assessing systematic generalization of clevr models. arXiv preprint
arXiv:1912.05783 , 2019a.
Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron
Courville. Systematic generalization: What is required and can it be learned? In International Conference
on Learning Representations , 2019b.
Avinash Baidya, Joel Dapello, James J DiCarlo, and Tiago Marques. Combining different v1 brain model
variants to improve robustness to image corruptions in cnns. In SVRHM at NeurIPS 2021 Workshops ,
2021.
12Under review as submission to TMLR
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz
Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive
biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261 , 2018.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain
adaptation. In Advances in Neural Information Processing Systems , pp. 137–144, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine Learning , 79(1):151–175, 2010.
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh
Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms.
arXiv preprint arXiv:1901.10912 , 2019.
Wilka Carvalho, Angelos Filos, Richard L Lewis, Satinder Singh, et al. Composing task knowledge with
modular successor feature approximators. In International Conference on Learning Representations , 2023.
David Chalmers. Why Fodor and Pylyshyn were wrong: The simplest refutation. In Proceedings of the twelfth
Annual Conference of the Cognitive Science Society , pp. 340–347, 1990.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International Conference on Machine Learning , pp. 1597–1607. PMLR,
2020.
XiChen, YanDuan, ReinHouthooft, JohnSchulman, IlyaSutskever, andPieterAbbeel. Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. Advances in neural
information processing systems , 29, 2016.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending MNIST to
handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN) , pp. 2921–2926.
IEEE, 2017.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on
machine learning , pp. 2990–2999. PMLR, 2016.
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on homogeneous
spaces.Advances in neural information processing systems , 32, 2019.
Avi Cooper, Xavier Boix, Daniel Harari, Spandan Madan, Hanspeter Pfister, Tomotake Sasaki, and Pawan
Sinha. To which out-of-distribution object orientations are dnns capable of generalizing? arXiv preprint
arXiv:2109.13445 , 2021.
Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In
International Conference on Machine Learning , pp. 2189–2200. PMLR, 2021.
Vanessa D’Amario, Tomotake Sasaki, and Xavier Boix. How modular should neural module networks be for
systematic generalization? Advances in Neural Information Processing Systems , 34:23374–23385, 2021.
Samuel Dodge and Lina Karam. A study and comparison of human and deep learning recognition performance
under visual distortions. In 2017 26th International Conference on Computer Communication and Networks
(ICCCN) , pp. 1–7. IEEE, 2017.
Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via
model-agnostic learning of semantic features. Advances in Neural Information Processing Systems , 32,
2019.
Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disentangled
representations. In International Conference on Learning Representations , 2018.
13Under review as submission to TMLR
Cian Eastwood, Ian Mason, and Christopher K. I. Williams. Unit-level surprise in neural networks. In
Proceedings of “I (Still) Can’t Believe It’s Not Better” at NeurIPS 2021 Workshops , volume 163 of
Proceedings of Machine Learning Research . PMLR, 2022a.
Cian Eastwood, Ian Mason, Christopher K. I. Williams, and Bernhard Schölkopf. Source-free adapta-
tion to measurement shift via bottom-up feature restoration. In International Conference on Learning
Representations , 2022b.
Cian Eastwood, Alexander Robey, Shashank Singh, Julius von Kügelgen, Hamed Hassani, George J. Pappas,
and Bernhard Schölkopf. Probable domain generalization via quantile risk minimization. In Advances in
Neural Information Processing Systems , volume 35, pp. 17340–17358, 2022c.
Cian Eastwood, Shashank Singh, Andrei Liviu Nicolicioiu, Marin Vlastelica, Julius von Kügelgen, and
Bernhard Schölkopf. Spuriosity didn’t kill the classifier: Using invariant predictions to harness spurious
features. Advances in Neural Information Processing Systems , 2023.
Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis.
Cognition , 28(1-2):3–71, 1988.
Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Schärli. Compositional generalization in semantic
parsing: Pre-training vs. specialized architectures. arXiv preprint arXiv:2007.08970 , 2020.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette,
Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of
Machine Learning Research , 17(1):2096–2030, 2016.
Mario Geiger, Leonardo Petrini, and Matthieu Wyart. Perspective: A phase diagram for deep learning
unifying jamming, feature learning and lazy training. arXiv preprint arXiv:2012.15110 , 2020.
Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A
Wichmann, and Wieland Brendel. Partial success in closing the gap between human and machine vision.
Advances in Neural Information Processing Systems , 34:23885–23899, 2021.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for
object recognition with multi-task autoencoders. In Proceedings of the IEEE International Conference on
Computer Vision , pp. 2551–2559, 2015.
Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition. Proceedings
of the Royal Society A , 478(2266):20210068, 2022.
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard
Schölkopf. Recurrent independent mechanisms. In International Conference on Learning Representations ,
2021.
Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman,
Jonathan Binas, Charles Blundell, Michael Mozer, and Yoshua Bengio. Coordination among neural modules
through a shared global workspace. In International Conference on Learning Representations , 2022.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference
on Learning Representations , 2022.
Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for
unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial
Intelligence and Statistics , volume 9, pp. 297–304. PMLR, 2010.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.
In2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) ,
volume 2, pp. 1735–1742. IEEE, 2006.
14Under review as submission to TMLR
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 770–778, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. In International Conference on Learning Representations , 2019.
Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan.
Augmix: A simple data processing method to improve robustness and uncertainty. In International
Conference on Learning Representations , 2020.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,
Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces
of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pp. 8340–8349, October 2021.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational
framework. In International conference on learning representations , 2017.
Hossein Hosseini, Baicen Xiao, and Radha Poovendran. Google’s cloud vision api is not robust to noise. In
2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA) , pp. 101–105.
IEEE, 2017.
Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason:
End-to-end module networks for visual question answering. In Proceedings of the IEEE International
Conference on Computer Vision , pp. 804–813, 2017.
Tianjian Huang, Shaunak Ashish Halbe, Chinnadhurai Sankar, Pooyan Amini, Satwik Kottur, Al-
borz Geramifard, Meisam Razaviyayn, and Ahmad Beirami. Robustness through data augmenta-
tion loss consistency. Transactions on Machine Learning Research , 2022. ISSN 2835-8856. URL
https://openreview.net/forum?id=a1meaRy1bN .
Takuya Ito, Tim Klinger, Douglas H Schultz, John D Murray, Michael W Cole, and Mattia Rigotti.
Compositional generalization through abstract representations in human and artificial neural networks. In
Advances in Neural Information Processing Systems , 2022.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in neural
information processing systems , 28, 2015.
Hojin Jang, Devin McCormack, and Frank Tong. Noise-trained deep neural networks effectively predict
human vision and its neural responses to challenging images. PLOS Biology , 19(12):e3001418, 2021.
Fredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-invariant
representations. In The 22nd International Conference on Artificial Intelligence and Statistics , pp. 527–536.
PMLR, 2019.
Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 2901–2910, 2017.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020.
Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola
Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generaliza-
tion: A comprehensive method on realistic data. In International Conference on Learning Representations ,
2020.
15Under review as submission to TMLR
Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn: Training
deep neural networks with biased data. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 9012–9020, 2019.
Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised
contrastive regularization for domain generalization. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pp. 9619–9628, 2021.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on Machine
Learning , pp. 2649–2658. PMLR, 2018.
Wolfgang Köhler. Gestalt psychology: An introduction to new concepts in modern psychology , volume 18.
WW Norton & Company, 1970.
Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, Guangyi Chen, Petar Stojanov, Victor Akinwande,
and Kun Zhang. Partial disentanglement for domain adaptation. In International Conference on Machine
Learning , pp. 11455–11472. PMLR, 2022.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. International journal of computer vision , 123:32–73, 2017.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang,
Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In
International Conference on Machine Learning , pp. 5815–5826. PMLR, 2021.
Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of
sequence-to-sequence recurrent networks. In International Conference on Machine Learning , pp. 2873–2882.
PMLR, 2018.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through
probabilistic program induction. Science, 350(6266):1332–1338, 2015.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that
learn and think like people. Behavioral and Brain Sciences , 40, 2017.
Brenden M Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instructions. In
Proceedings of the 41st Annual Conference of the Cognitive Science Society , 2019.
Rémi Le Priol, Reza Babanezhad, Yoshua Bengio, and Simon Lacoste-Julien. An analysis of the adaptation
speed of causal models. In International Conference on Artificial Intelligence and Statistics , pp. 775–783.
PMLR, 2021.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn.
Surgical fine-tuning improves adaptation to distribution shifts. In International Conference on Learning
Representations , 2023.
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial
feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.
5400–5409, 2018a.
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain
generalization via conditional invariant adversarial networks. In Proceedings of the European Conference
on Computer Vision (ECCV) , pp. 624–639, 2018b.
16Under review as submission to TMLR
Adam Liška, Germán Kruszewski, and Marco Baroni. Memorize or generalize? searching for a compositional
rnn in a haystack. In AEGAP Workshop, ICML , 2018.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled
representations. In International Conference on Machine Learning , pp. 4114–4124. PMLR, 2019.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep
adaptation networks. In International Conference on Machine Learning , pp. 97–105, 2015.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain
adaptation. In Advances in Neural Information Processing Systems , 2018.
Spandan Madan, Timothy Henry, Jamell Dozier, Helen Ho, Nishchal Bhandari, Tomotake Sasaki, Frédo
Durand, Hanspeter Pfister, and Xavier Boix. When and how convolutional neural networks generalize to
out-of-distribution category–viewpoint combinations. Nature Machine Intelligence , 4(2):146–153, 2022.
Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina
Fedorenko. Dissociating language and thought in large language models: a cognitive perspective. arXiv
preprint arXiv:2301.06627 , 2023.
Jorge A Mendez and Eric Eaton. Lifelong learning of compositional structures. In International Conference
on Learning Representations , 2021.
Jorge A Mendez and Eric Eaton. How to reuse and compose knowledge for a lifetime of tasks: A survey on
continual learning and functional composition. arXiv preprint arXiv:2207.07730 , 2022.
Milton Montero, Jeffrey Bowers, Rui Ponte Costa, Casimir Ludwig, and Gaurav Malhotra. Lost in latent
space: Examining failures of disentangled models at combinatorial generalisation. In Advances in Neural
Information Processing Systems , volume 35, pp. 10136–10149, 2022.
Milton Llera Montero, Casimir JH Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey Bowers. The role
of disentanglement in generalisation. In International Conference on Learning Representations , 2021.
Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep supervised domain
adaptation and generalization. In Proceedings of the IEEE International Conference on Computer Vision ,
pp. 5715–5725, 2017.
Norman Mu and Justin Gilmer. Mnist-c: A robustness benchmark for computer vision. arXiv preprint
arXiv:1906.02337 , 2019.
Hong-Wei Ng and Stefan Winkler. A data-driven approach to cleaning large face datasets. In 2014 IEEE
International Conference on Image Processing (ICIP) , pp. 343–347. IEEE, 2014.
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.
Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001.
https://distill.pub/2020/circuits/zoom-in.
Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Schölkopf. Learning inde-
pendent causal mechanisms. In International Conference on Machine Learning , pp. 4036–4044. PMLR,
2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in Neural Information Processing Systems , 32, 2019.
Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and
learning algorithms . The MIT Press, 2017.
17Under review as submission to TMLR
Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, and Edoardo Maria Ponti. Modular deep learning. arXiv preprint
arXiv:2302.11529 , 2023.
Steven Piantadosi and Richard Aslin. Compositional reasoning in early childhood. PLOS ONE , 11(9):
e0147734, 2016.
Tomaso A Poggio and Fabio Anselmi. Visual cortex and deep networks: learning invariant representations .
MIT press, 2016.
Linlu Qiu, Peter Shaw, Panupong Pasupat, Pawel Nowak, Tal Linzen, Fei Sha, and Kristina Toutanova.
Improving compositional generalization with latent structure and data augmentation. In Proceedings of the
2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies , 2022.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural
language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR, 2021.
S-A Rebuffi, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters. In Advances
in Neural Information Processing Systems , 2017.
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Efficient parametrization of multi-domain deep
neural networks. In CVPR, 2018.
Lukasz Romaszko, Christopher KI Williams, Pol Moreno, and Pushmeet Kohli. Vision-as-inverse-graphics:
Obtaining a rich 3d explanation of a scene from a single image. In Proceedings of the IEEE International
Conference on Computer Vision Workshops , pp. 851–859, 2017.
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. In
International Conference on Learning Representations , 2021.
Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. Disentanglement of
correlated factors via hausdorff factorized support. In International Conference on Learning Representations ,
2023.
Amélie Royer and Christoph Lampert. A flexible selection scheme for minimum-effort transfer learning. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pp. 2191–2200,
2020.
Akira Sakai, Taro Sunagawa, Spandan Madan, Kanata Suzuki, Takashi Katoh, Hiromichi Kobashi, Hanspeter
Pfister, Pawan Sinha, Xavier Boix, and Tomotake Sasaki. Three approaches to facilitate invariant neurons
and generalization to out-of-distribution orientations and illuminations. Neural Networks , 155:119–143,
2022. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2022.07.026.
Anirban Sarkar, Matthew Groth, Ian Mason, Tomotake Sasaki, and Xavier Boix. Deephys: Deep elec-
trophysiology, debugging neural networks under distribution shifts. arXiv preprint arXiv:2303.11912 ,
2023.
Jürgen Schmidhuber. Towards compositional learning in dynamic networks, technical report. 1990.
Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal,
and Yoshua Bengio. Towards causal representation learning. Proceedings of the IEEE , 109(5):612–634,
2021.
Lukas Schott, Julius Von Kügelgen, Frederik Träuble, Peter Gehler, Chris Russell, Matthias Bethge, Bernhard
Schölkopf, Francesco Locatello, and Wieland Brendel. Visual representation learning does not generalize
strongly within the same domain. In International Conference on Learning Representations , 2022.
Eric Schulz, Josh Tenenbaum, David K Duvenaud, Maarten Speekenbrink, and Samuel J Gershman. Probing
the compositionality of intuitive functions. In Advances in Neural Information Processing Systems , 2016.
18Under review as submission to TMLR
Kendrick Shen, Robbie M Jones, Ananya Kumar, Sang Michael Xie, Jeff Z HaoChen, Tengyu Ma, and Percy
Liang. Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation. In
International Conference on Machine Learning , pp. 19847–19878. PMLR, 2022.
Samarth Sinha and Adji Bousso Dieng. Consistency regularization for variational auto-encoders. Advances in
Neural Information Processing Systems , 34:12943–12954, 2021.
Irene Sperandio and Philippe A. Chouinard. The mechanisms of size constancy. Multisensory Research , 28
(3-4):253 – 283, 2015. doi: https://doi.org/10.1163/22134808-00002483.
Baochen Sun and Kate Saenko. Deep CORAL: Correlation alignment for deep domain adaptation. In
European Conference on Computer Vision , pp. 443–450. Springer, 2016.
Qingyao Sun, Kevin Murphy, Sayna Ebrahimi, and Alexander D’Amour. Beyond invariance: Test-time
label-shift adaptation for distributions with "spurious" correlations. arXiv preprint arXiv:2211.15646 , 2023.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 2818–2826, 2016.
Nassim Nicholas Taleb. Statistical consequences of fat tails: Real world preasymptotics, epistemology, and
applications. arXiv preprint arXiv:2001.10488 , 2020.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation.
InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 7167–7176, 2017.
Frank van der Velde, Gwendid T van der Voort van der Kleij, and Marc de Kamps. Lack of combinatorial
productivity in language processing with simple recurrent networks. Connection Science , 16(1):21–46, 2004.
Stéfan van der Walt, Johannes L. Schönberger, Juan Nunez-Iglesias, François Boulogne, Joshua D. Warner,
Neil Yager, Emmanuelle Gouillart, Tony Yu, and the scikit-image contributors. scikit-image: image
processing in python. PeerJ, 2:e453, jun 2014. ISSN 2167-8359. doi: 10.7717/peerj.453.
Lukas Vogelsang, Sharon Gilad-Gutnick, Evan Ehrenberg, Albert Yonas, Sidney Diamond, Richard Held, and
Pawan Sinha. Potential downside of high initial visual acuity. Proceedings of the National Academy of
Sciences, 115(44):11333–11338, 2018.
Julius Von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf, Michel Besserve,
and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from
style.Advances in Neural Information Processing Systems , 34:16451–16467, 2021.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint . Cambridge University
Press, 2019. doi: 10.1017/9781108627771.
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation equivariant
cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 849–858,
2018.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks:
Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 5028–5037, 2017.
Sirui Xie, Ari S Morcos, Song-Chun Zhu, and Ramakrishna Vedantam. COAT: Measuring object composi-
tionality in emergent representations. In International Conference on Machine Learning , 2022.
Moyuru Yamada, Vanessa D’Amario, Kentaro Takemoto, Xavier Boix, and Tomotake Sasaki. Transformer
Module Networks for systematic generalization in visual question answering. Technical Report CBMM
Memo No. 121, Ver.2, Center for Brains, Minds and Machines, 2023.
19Under review as submission to TMLR
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:
Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 6023–6032, 2019.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. In International Conference on Learning Representations , 2018.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations
for domain adaptation. In International conference on machine learning , pp. 7523–7532. PMLR, 2019.
Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.
20Under review as submission to TMLR
Appendix
Table of Contents
A Choice of Corruptions 22
A.1 Sampling and Commutativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.2 Corruption Severity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.3 The 3D Projection Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B Module Implementation and Interpretability 25
C Elemental and Composition Invariance Scores, a Worked Example 26
D Network Architecture Details 28
E Invariance Scores for All Datasets 28
F Variance Over Seeds 28
G Heat Maps - Full Granular Results 28
21Under review as submission to TMLR
A Choice of Corruptions
The choice of corruptions used in our compositional robustness task is quite subtle. We want to ensure a good
mixture of different types of corruptions and the compositions they form, but without creating a compositional
space that is so big that it becomes prohibitively expensive to evaluate. Due to the exponential increase in
the number of possible compositions as the number of elemental corruptions increases, and in order to reduce
computational costs, we make the following concessions: (i) we keep the total number of elemental corruptions
low whilst ensuring a good mixture of elemental corruptions; (ii) we include compositions constructed from
every combination of elemental corruptions but sample the possible permutations (orderings) of elemental
corruptions that make up a composition (see Appendix A.1); (iii) we do not consider the 3D projection
problem (see Appendix A.3).
As discussed in Section 3.1, along with the Identity (ID) data we consider the corruptions, Contrast (CO) ,
Gaussian Blur (GB) ,Impulse Noise (IM) ,Invert (IN) ,Rotate 90◦(R90)andSwirl (SW) , which can be seen
foremnistandcifar- 10in Figures 4 and 5 respectively. We consider two different behaviors that corrupting
functions may exhibit and select this set of corruptions to get a mixture of behaviors. Firstly, corruptions
can be local or long-ranged, where images under local corruptions (such as Invert) can be transformed to the
Identity image by applying a patch-wise operation. On the other hand, long-ranged corruptions (such as
Rotate 90◦) require a holistic understanding of the image. Additionally, corruptions can be lossless or lossy,
where lossless corruptions lose no information so can be perfectly inverted and lossy corruptions may lose
information due to randomness or the application of non-invertible corrupting functions.
A.1 Sampling and Commutativity
Our set of elemental corruptions allows us to consider compositions made up of up to six corruptions at once
(we do not allow for repeated application of elemental corruptions). As not all elemental corruptions are
commutative under composition (e.g. IM◦GB̸=GB◦IM), we must take into account the possible orderings
of elemental corruptions when constructing compositions. When taking into account possible orderings there
are6P2= 30possible orderings of two corruptions but6P6= 720possible ordering of six corruptions, where
nPr=n!/(n−r)!, counts the number of possible permutations. As we don’t want results to be dominated by
compositions of larger numbers of corruptions and to reduce the number of compositional test domains, we
samplethe possible orderings.
For compositions of two corruptions, we consider all possible orderings giving6P2= 30compositions. For
compositions of more than two corruptions we aim to get as close to6P2= 30test domains as possible whilst
maintaining a balance of the possible unique combinations of elemental corruptions. This means we first
calculate the number of unique combinations asnCr=n!/r!(n−r)!, wherenis the total number of elemental
corruptions and ris the number of elemental corruptions in the compositions we are considering. We then
sample the same number of possible orderings of each unique combination until we get as close as possible
Gaussian Blur (GB)Identity (ID)Contrast (CO)CO◦GBR90◦IM◦CO
IN◦R90◦CO◦SWIM◦INR90◦SW
SW◦IN◦GBImpulse Noise (IM)
Invert (IN)Swirl (SW)Rotate 90 (R90)GB◦R90◦SW◦COCO◦GB◦IM◦ IN◦R90◦SWElemental CorruptionsCompositions of Corruptions
Figure 4: The compositional robustness task for emnist.
22Under review as submission to TMLR
Gaussian Blur (GB)Identity (ID)Contrast (CO)CO◦GBR90◦IM◦CO
IN◦R90◦CO◦SWIM◦INR90◦SW
SW◦IN◦GBImpulse Noise (IM)
Invert (IN)Swirl (SW)Rotate 90 (R90)
GB◦R90◦SW◦COCO◦GB◦IM◦ IN◦R90◦SW
Elemental CorruptionsCompositions of Corruptions
Figure 5: The compositional robustness task for cifar- 10.
to30domains. As an example, for compositions of three corruptions6C3= 20, so we have twenty unique
combinations of three elemental corruptions. For each unique combination we sample two possible orderings,
giving forty test domains. For compositions of four corruptions we have fifteen unique combinations so we
again sample two possible orderings, for compositions of five corruptions we have six unique combinations so
we sample five orderings and for compositions of six corruptions there is only one unique combinations so we
sample thirty different orderings.
A.2 Corruption Severity
Our implementation allows for corruptions to be applied with differing severity, for example by adding
more or fewer random pixels for Impulse Noise or by increasing or decreasing the Gaussian filter size when
creating Gaussian Blur . In our main experiments we keep the severity fixed as varying the severity would
significantly increase the size of the compositional space. To show the impact of varying severity we perform
one additional ablation on EMNIST using the lowest severity settings in Figure 6, where we verify that the
relative performance of different approaches remains unchanged.
For corruptions with variable severity, the specific functions we use in our experiments are as follows: Contrast
calculates per-channel x∗s+ (1−s)¯xfor pixel value xand average pixel value ¯xusings= 0.2;Gaussian Blur
uses a Gaussian filter with standard deviation 2; Impulse Noise uses random salt and pepper noise affecting
27% of the pixels in an image; and Swirluses the scikit-image (van der Walt et al., 2014) implementation
of image swirling with strength 3 and radius equal to the image width in pixels divided by√
2. Whilst the
scale of severity is somewhat arbitrary, following the definitions of MNIST-C (Mu & Gilmer, 2019), we use
severities 2, 2, 5 for Contrast,Gaussian Blur andImpulse Noise ;Swirluses an original implementation with
severity 3 according to our severity scale.
A.3 The 3D Projection Problem
A final point of interest when choosing which elemental corruptions to consider is the problem of 3D projection.
There are certain corruptions that occur in natural data that are inherently 3-dimensional, yet we only see
the results as a projection onto a 2-dimensional image plane. This fact introduces complexity in the way
corruptions can be applied and composed if we are aiming to create a system with vision that is as robust as
humans.
To see the problem, consider the corruption Scale (SC) , where we create a zoomed out version of a base image
(see Figure 7). Imagine that we then also consider the composition of ScalewithGaussian Blur .SC◦GB
creates a very different image to GB◦SC, but more importantly these represent fundamentally different
processes in the 3D world. If scaling is applied before blurring this corresponds to the case where there is a
fixed amount of blur in the scene (e.g. because of an eye condition) and the object we care about is moved
further away from the viewer. On the other hand if blurring is applied before scaling this corresponds to the
23Under review as submission to TMLR
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling
(a) EMNIST, original severities
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling (b) EMNIST, lowest severities
Figure 6: Comparing performance of different methods when using different corruption severities. (a) is the
severities used in the main text in Figure 2a, (b) is using the lowest severity setting for each corruption. With
lower severities, performance is higher for all methods but relative performance remains the same
PPNear object appears bigger
Scale (SC)
Scene Level BlurObject Level BlurFar object appears smaller
Figure 7: The 3D projection problem. Corruptions of 2D images can represent 3D processes, for example
scaling an image represents moving the object further away from the viewer (top). When composing
corruptions, this can lead to different orderings of corruptions representing different 3D processes (bottom).
case where the object itself is blurry (e.g. because of damage around the edges). This process is depicted in
Figure 7.
Thepointofthisdiscussionistodemonstratethatapplyingacorruptionatthescenelevelcanbefundamentally
different from applying a corruption at the object level. Whilst this can be taken into account (e.g. by
changing the order of Gaussian Blur andScale), we aim to avoid this situation by only considering corruptions
where changing the ordering under composition does not change the composition from a scene level process
to an object level process (or vice versa). This makes our task more practical as we can apply it to any image
classification dataset. Since we may not even consciously perceive the effect of scaling accurately (Sperandio
& Chouinard, 2015; Köhler, 1970), future work may find that different processes in 3D space should be
handled in different ways or at different levels of abstraction.
24Under review as submission to TMLR
B Module Implementation and Interpretability
Figure 8 shows the training process for a module trained on the Invertcorruption. First a network is trained
onIdentity data to learn parameters θshared. These weights are then frozen (gray boxes in Figure 8) and
a module is trained to ‘undo’ the Invertcorruption in latent space (blue box in Figure 8). To train the
module, the contrastive loss is used to align representations of Identity data before the module is applied
with representations of Invertdata after the module is applied. As described in the main text, we also use
the cross entropy loss to ensure classification accuracy is maintained.
1Convolutions & Fully Connected (Frozen)Convolutions (Frozen)Convolutions & Fully Connected (Frozen)Convolutions (Frozen)Identity
Invert
Invert Speciﬁc Convolutions (Trainable)Feature  ExtractorHidden RepresentationClassiﬁerPredictionCorruption  ModuleHidden RepresentationContrastive Loss
Figure 8: Module training diagram. After pre-training on Identity data, shared network parameters are
frozen (gray boxes) and a module (blue box) is trained to align the representation of the corrupted Invert
image with the representation of the Identityimage using the contrastive loss. In this figure, apart from those
of the module, all parameters are identical between the top and bottom networks.
Using interpretability tool Deephys (Sarkar et al., 2023), we visualize the effect of modules trained in this way
in Figure 9 . We find neurons which are initially activated by very different class instances when comparing
Identity data with corrupted data, but after applying the module, neurons fire for similar class instances
between the Identity and corrupted data.
25Under review as submission to TMLR
Pre-ModulePost-ModuleIdentity Data
Figure 9: Training modules to undo elemental corruptions. Images in this grid represent some of the images
that maximally activate a neuron. The first column shows a neuron before the module is applied, this is
equivalent to the images that the neuron is tuned to for a network trained only on Identitydata. The second
column shows the same neuron after the module is applied. The third column shows how the Identity data
activates this neuron. By comparing across columns we can see that modules learns to align the hidden
representations so that neurons fire for similar class instances between Identity and corrupted data. Top to
bottom the corruptions in the rows are, Invert,Rotate 90◦andSwirl.
C Elemental and Composition Invariance Scores, a Worked Example
This section gives an exemplar activation grid and a worked example of calculating elemental and composition
invariance scores based on the techniques described by Madan et al. (2022).
Table 3 shows an exemplar activation grid for a single neuron for the test domain containing the composition
CO◦GBoncifar- 10. We see the 4rows consist of the composition alongside the elemental corruptions that are
relevant for CO◦GB, and the 10columns for each of the 10categories of cifar- 10, creating domain-category
pairs.
To calculate the invariance scores for this example we first find the preferred category as j∗=
argmaxj/summationtext3
i=1ai,j, which indicates that this neuron activates maximally for category 7. The elemental
invariance score is the worst case difference amongst the elemental corruption activations for this category
(the maximum is marked ∗, and the minimum is marked †).
26Under review as submission to TMLR
Table 3: An exemplar activation grid
Cat. 1 Cat. 2 Cat. 3 Cat. 4 Cat. 5 Cat. 6 Cat. 7 Cat. 8 Cat. 9 Cat. 10
CO 0.002 0.007 0.038 0.089 0.039 0.794 0.998 0.015 0.022 0.005
GB 0.011 0.021 0.070 0.144 0.061 0.733 0.955 †0.043 0.029 0.020
ID 0.020 0.004 0.051 0.090 0.039 0.791 1.000 ∗0.016 0.018 0.000
CO◦GB0.035 0.102 0.109 0.126 0.087 0.415 0.638 ‡0.078 0.116 0.138
Ie= 1−(max
iai,j∗−min
iai,j∗)
= 1−(1.000−0.955)
= 0.955
The composition invariance score finds the activation amongst the elemental corruptions that is closest to the
composition’s activation (marked ‡) for the preferred category.
Ic= 1−min{|ai,j∗−aE+1,j∗|}E
i=1
= 1−min{|0.998−0.638|,|0.995−0.638|,|1.000−0.638|}
= 0.683
For this particular neuron, we would deduce that the elemental corruptions have relatively invariant activations
whereas the activations are less invariant when we include the composition.
27Under review as submission to TMLR
D Network Architecture Details
This appendix gives the specific architecture of the simple convolutional network used for emnistexperiments
in Table 4. For cifar- 10we use ResNet18 and for facescrub Inception-v3. In both cases we use the official
PyTorch (Paszke et al., 2019) implementations of the architectures. Rather than giving a lengthy description
of the possible architectures for modules between every layer of these networks we refer the reader to the
associated code repository (file lib/networks.py). The architectures of the auto-encoders used in Section 4.4
can also be found in this file.
Table 4: The network architecture used for emnistexperiments. For convolutions, the weights-shape is:
number of input channels ×number of output channels ×filter height×filter width .
Block Weights-Shape Stride Padding Activation Dropout Prob.
Convolution 3×64×5×5 2 2 ReLU 0.1
Convolution 64×128×5×5 2 2 ReLU 0.3
Convolution 128×256×5×5 2 2 ReLU 0.5
Convolution 256×256×5×5 2 2 ReLU 0.5
Linear 1024×512 N/A N/A ReLU 0.5
Linear 512×Number of Classes N/A N/A Softmax 0
E Invariance Scores for All Datasets
Appendices E, F and G show a large number of plots over the following pages. This appendix contains further
plots correlating invariance scores with compositional robustness. To begin we show the invariance summary
plots for cifar- 10(Figure 10) and facescrub (Figure 11). These plots are the equivalent of Figure 3 for
emnistfrom the main text.
Following this, in Figures 12-20, we show the invariance summary plots (Figures 3, 10, 11) expanded over all
compositional test domains. That is, these plots include the plots for compositions containing more than three
corruptions. For compositions of more than three corruptions accuracy is often low, making it challenging to
uncover meaningful trends.
F Variance Over Seeds
This appendix shows the figures included in the main text for two further random seeds. In particular we
replicate Figures 2, 3, 10 and 11 in each case. The results in the main text come from the first random seed,
Figures 21-24 show the second random seed and Figures 25-28 show the third random seed.
G Heat Maps - Full Granular Results
Finally we show granular results, showing the individual accuracy for every elemental corruption and
composition. This is the raw data that is summarized by the box plots in Figures 2, 21 and 25. We show
heat maps for every dataset and every seed in Figures 29-37 to give a per-domain view of the differences in
behaviors for the different methods for compositional robustness.
28Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.81. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.14. p = 0.46.
Elemental Invariance Score
 Modular. r = -0.15. p = 0.42.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.78. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.04. p = 0.81.
Elemental Invariance Score
 Modular. r = -0.27. p = 0.09.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.50. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.41. p = 0.02.
Elemental Invariance Score
 Modular. r = -0.14. p = 0.47.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = -0.35. p = 0.06.
Elemental Invariance Score
 Contrastive. r = 0.49. p = 0.01.
Elemental Invariance Score
 Modular. r = -0.26. p = 0.17.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = -0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.63. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.16. p = 0.40.
Elemental Invariance Score
 Modular. r = -0.28. p = 0.13.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.55. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.16. p = 0.31.
Elemental Invariance Score
 Modular. r = -0.38. p = 0.02.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.28. p = 0.13.
Elemental Invariance Score
 Contrastive. r = 0.29. p = 0.12.
Elemental Invariance Score
 Modular. r = -0.30. p = 0.10.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.15. p = 0.44.
Elemental Invariance Score
 Contrastive. r = 0.25. p = 0.17.
Elemental Invariance Score
 Modular. r = -0.51. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.95. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.78. p = 0.00.
Composition Invariance Score
 Modular. r = 0.98. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.89. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.66. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.77. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.77. p = 0.00.
Composition Invariance Score
 Modular. r = 0.97. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.38. p = 0.04.
Composition Invariance Score
 Contrastive. r = 0.53. p = 0.00.
Composition Invariance Score
 Modular. r = 0.91. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.12. p = 0.54.
Composition Invariance Score
 Contrastive. r = -0.85. p = 0.00.
Composition Invariance Score
 Modular. r = 0.06. p = 0.77.
Corruptions in Composition = 6
Figure 10: Correlating invariance scores with compositional robustness for cifar- 10. These plots plot the
same relationships as in Figure 3.
29Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.06. p = 0.76.
Elemental Invariance Score
 Contrastive. r = 0.04. p = 0.84.
Elemental Invariance Score
 Modular. r = -0.16. p = 0.39.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.06. p = 0.71.
Elemental Invariance Score
 Contrastive. r = 0.07. p = 0.66.
Elemental Invariance Score
 Modular. r = -0.31. p = 0.05.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = -0.14. p = 0.46.
Elemental Invariance Score
 Contrastive. r = 0.08. p = 0.68.
Elemental Invariance Score
 Modular. r = -0.16. p = 0.40.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = -0.02. p = 0.90.
Elemental Invariance Score
 Contrastive. r = 0.20. p = 0.28.
Elemental Invariance Score
 Modular. r = -0.42. p = 0.02.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = -0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.03. p = 0.88.
Elemental Invariance Score
 Contrastive. r = -0.02. p = 0.93.
Elemental Invariance Score
 Modular. r = -0.26. p = 0.16.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.06. p = 0.71.
Elemental Invariance Score
 Contrastive. r = 0.02. p = 0.92.
Elemental Invariance Score
 Modular. r = -0.35. p = 0.03.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.19. p = 0.30.
Elemental Invariance Score
 Contrastive. r = -0.08. p = 0.69.
Elemental Invariance Score
 Modular. r = -0.28. p = 0.13.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.08. p = 0.67.
Elemental Invariance Score
 Contrastive. r = -0.10. p = 0.60.
Elemental Invariance Score
 Modular. r = -0.49. p = 0.01.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.99. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.99. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.96. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.95. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.94. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.92. p = 0.00.
Composition Invariance Score
 Modular. r = 0.96. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.80. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.83. p = 0.00.
Composition Invariance Score
 Modular. r = 0.96. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.58. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.53. p = 0.00.
Composition Invariance Score
 Modular. r = 0.44. p = 0.02.
Corruptions in Composition = 6
Figure 11: Correlating invariance scores with compositional robustness for facescrub . These plots plot the
same relationships as in Figure 3.
30Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.10. p = 0.60.
Elemental Invariance Score
 Contrastive. r = 0.07. p = 0.72.
Elemental Invariance Score
 Modular. r = -0.46. p = 0.01.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.30. p = 0.06.
Elemental Invariance Score
 Contrastive. r = 0.13. p = 0.42.
Elemental Invariance Score
 Modular. r = -0.55. p = 0.00.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.35. p = 0.06.
Elemental Invariance Score
 Contrastive. r = 0.14. p = 0.47.
Elemental Invariance Score
 Modular. r = -0.23. p = 0.22.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.38. p = 0.04.
Elemental Invariance Score
 Contrastive. r = -0.23. p = 0.21.
Elemental Invariance Score
 Modular. r = -0.11. p = 0.55.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
Figure 12: Correlating the elemental invariance score with compositional robustness for emnist. These plots
expand the first row of Figure 3 to show all compositional test domains.
31Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.10. p = 0.59.
Elemental Invariance Score
 Contrastive. r = 0.21. p = 0.26.
Elemental Invariance Score
 Modular. r = -0.70. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.01. p = 0.94.
Elemental Invariance Score
 Contrastive. r = -0.01. p = 0.96.
Elemental Invariance Score
 Modular. r = -0.83. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.11. p = 0.58.
Elemental Invariance Score
 Contrastive. r = -0.15. p = 0.43.
Elemental Invariance Score
 Modular. r = -0.85. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.50. p = 0.00.
Elemental Invariance Score
 Contrastive. r = -0.87. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.93. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = -0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
Figure 13: Correlating the elemental invariance score with the composition invariance score for emnist.
These plots expand the second row of Figure 3 to show all compositional test domains.
32Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.45. p = 0.01.
Composition Invariance Score
 Contrastive. r = 0.83. p = 0.00.
Composition Invariance Score
 Modular. r = 0.80. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.56. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.95. p = 0.00.
Composition Invariance Score
 Modular. r = 0.86. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.42. p = 0.02.
Composition Invariance Score
 Contrastive. r = 0.94. p = 0.00.
Composition Invariance Score
 Modular. r = 0.64. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = -0.40. p = 0.03.
Composition Invariance Score
 Contrastive. r = 0.60. p = 0.00.
Composition Invariance Score
 Modular. r = 0.30. p = 0.11.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.86. p = 0.00.
Composition Invariance Score
 Contrastive. r = -0.87. p = 0.00.
Composition Invariance Score
 Modular. r = 0.12. p = 0.52.
Corruptions in Composition = 6
Figure 14: Correlating the composition invariance score with compositional robustness for emnist. These
plots expand the third row of Figure 3 to show all compositional test domains.
33Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.81. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.14. p = 0.46.
Elemental Invariance Score
 Modular. r = -0.15. p = 0.42.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.78. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.04. p = 0.81.
Elemental Invariance Score
 Modular. r = -0.27. p = 0.09.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.50. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.41. p = 0.02.
Elemental Invariance Score
 Modular. r = -0.14. p = 0.47.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = -0.35. p = 0.06.
Elemental Invariance Score
 Contrastive. r = 0.49. p = 0.01.
Elemental Invariance Score
 Modular. r = -0.26. p = 0.17.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = -0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
Figure 15: Correlating the elemental invariance score with compositional robustness for cifar- 10. These
plots expand the first row of Figure 10 to show all compositional test domains.
34Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.63. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.16. p = 0.40.
Elemental Invariance Score
 Modular. r = -0.28. p = 0.13.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.55. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.16. p = 0.31.
Elemental Invariance Score
 Modular. r = -0.38. p = 0.02.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.28. p = 0.13.
Elemental Invariance Score
 Contrastive. r = 0.29. p = 0.12.
Elemental Invariance Score
 Modular. r = -0.30. p = 0.10.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.15. p = 0.44.
Elemental Invariance Score
 Contrastive. r = 0.25. p = 0.17.
Elemental Invariance Score
 Modular. r = -0.51. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
Figure 16: Correlating the elemental invariance score with the composition invariance score for cifar- 10.
These plots expand the second row of Figure 10 to show all compositional test domains.
35Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.95. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.78. p = 0.00.
Composition Invariance Score
 Modular. r = 0.98. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.89. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.66. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.77. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.77. p = 0.00.
Composition Invariance Score
 Modular. r = 0.97. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.38. p = 0.04.
Composition Invariance Score
 Contrastive. r = 0.53. p = 0.00.
Composition Invariance Score
 Modular. r = 0.91. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.12. p = 0.54.
Composition Invariance Score
 Contrastive. r = -0.85. p = 0.00.
Composition Invariance Score
 Modular. r = 0.06. p = 0.77.
Corruptions in Composition = 6
Figure 17: Correlating the composition invariance score with compositional robustness for cifar- 10. These
plots expand the third row of Figure 10 to show all compositional test domains.
36Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.06. p = 0.76.
Elemental Invariance Score
 Contrastive. r = 0.04. p = 0.84.
Elemental Invariance Score
 Modular. r = -0.16. p = 0.39.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.06. p = 0.71.
Elemental Invariance Score
 Contrastive. r = 0.07. p = 0.66.
Elemental Invariance Score
 Modular. r = -0.31. p = 0.05.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = -0.14. p = 0.46.
Elemental Invariance Score
 Contrastive. r = 0.08. p = 0.68.
Elemental Invariance Score
 Modular. r = -0.16. p = 0.40.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = -0.02. p = 0.90.
Elemental Invariance Score
 Contrastive. r = 0.20. p = 0.28.
Elemental Invariance Score
 Modular. r = -0.42. p = 0.02.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = -0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
Figure 18: Correlating the elemental invariance score with compositional robustness for facescrub . These
plots expand the first row of Figure 11 to show all compositional test domains.
37Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.03. p = 0.88.
Elemental Invariance Score
 Contrastive. r = -0.02. p = 0.93.
Elemental Invariance Score
 Modular. r = -0.26. p = 0.16.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.06. p = 0.71.
Elemental Invariance Score
 Contrastive. r = 0.02. p = 0.92.
Elemental Invariance Score
 Modular. r = -0.35. p = 0.03.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.19. p = 0.30.
Elemental Invariance Score
 Contrastive. r = -0.08. p = 0.69.
Elemental Invariance Score
 Modular. r = -0.28. p = 0.13.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.08. p = 0.67.
Elemental Invariance Score
 Contrastive. r = -0.10. p = 0.60.
Elemental Invariance Score
 Modular. r = -0.49. p = 0.01.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
Figure 19: Correlating the elemental invariance score with the composition invariance score for facescrub .
These plots expand the second row of Figure 11 to show all compositional test domains.
38Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.99. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.99. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.96. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.95. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.94. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.92. p = 0.00.
Composition Invariance Score
 Modular. r = 0.96. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.80. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.83. p = 0.00.
Composition Invariance Score
 Modular. r = 0.96. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.58. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.53. p = 0.00.
Composition Invariance Score
 Modular. r = 0.44. p = 0.02.
Corruptions in Composition = 6
Figure 20: Correlating the composition invariance score with compositional robustness for facescrub . These
plots expand the third row of Figure 11 to show all compositional test domains.
39Under review as submission to TMLR
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling
(a) EMNIST
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling (b) CIFAR-10
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling
ERM
Contrastive
Modular (c) FACESCRUB
Figure 21: Evaluating compositional robustness on different datasets (second random seed). This figure is
the same as Figure 2 with a different random seeding.
40Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.06. p = 0.76.
Elemental Invariance Score
 Contrastive. r = 0.03. p = 0.88.
Elemental Invariance Score
 Modular. r = -0.33. p = 0.08.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.28. p = 0.08.
Elemental Invariance Score
 Contrastive. r = 0.08. p = 0.61.
Elemental Invariance Score
 Modular. r = -0.43. p = 0.01.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.32. p = 0.09.
Elemental Invariance Score
 Contrastive. r = 0.11. p = 0.56.
Elemental Invariance Score
 Modular. r = -0.12. p = 0.52.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.35. p = 0.06.
Elemental Invariance Score
 Contrastive. r = -0.20. p = 0.29.
Elemental Invariance Score
 Modular. r = -0.18. p = 0.35.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.05. p = 0.80.
Elemental Invariance Score
 Contrastive. r = 0.18. p = 0.34.
Elemental Invariance Score
 Modular. r = -0.62. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.03. p = 0.86.
Elemental Invariance Score
 Contrastive. r = -0.06. p = 0.73.
Elemental Invariance Score
 Modular. r = -0.73. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.13. p = 0.50.
Elemental Invariance Score
 Contrastive. r = -0.19. p = 0.32.
Elemental Invariance Score
 Modular. r = -0.77. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.41. p = 0.02.
Elemental Invariance Score
 Contrastive. r = -0.89. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.87. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.39. p = 0.03.
Composition Invariance Score
 Contrastive. r = 0.86. p = 0.00.
Composition Invariance Score
 Modular. r = 0.83. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.47. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.96. p = 0.00.
Composition Invariance Score
 Modular. r = 0.89. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.37. p = 0.05.
Composition Invariance Score
 Contrastive. r = 0.93. p = 0.00.
Composition Invariance Score
 Modular. r = 0.66. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = -0.32. p = 0.09.
Composition Invariance Score
 Contrastive. r = 0.56. p = 0.00.
Composition Invariance Score
 Modular. r = 0.54. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.66. p = 0.00.
Composition Invariance Score
 Contrastive. r = -0.85. p = 0.00.
Composition Invariance Score
 Modular. r = 0.53. p = 0.00.
Corruptions in Composition = 6
Figure 22: Correlating invariance scores with compositional robustness for emnist(second random seed).
This is the same as Figure 3 with a different random seeding.
41Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.62. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.06. p = 0.76.
Elemental Invariance Score
 Modular. r = -0.16. p = 0.41.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.63. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.08. p = 0.63.
Elemental Invariance Score
 Modular. r = -0.31. p = 0.05.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.51. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.40. p = 0.03.
Elemental Invariance Score
 Modular. r = -0.14. p = 0.47.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = -0.16. p = 0.40.
Elemental Invariance Score
 Contrastive. r = 0.60. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.25. p = 0.19.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.43. p = 0.02.
Elemental Invariance Score
 Contrastive. r = 0.72. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.27. p = 0.15.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.35. p = 0.03.
Elemental Invariance Score
 Contrastive. r = 0.67. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.38. p = 0.01.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.44. p = 0.01.
Elemental Invariance Score
 Contrastive. r = 0.65. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.27. p = 0.15.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.09. p = 0.65.
Elemental Invariance Score
 Contrastive. r = 0.78. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.45. p = 0.01.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.97. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.53. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.90. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.39. p = 0.01.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.82. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.58. p = 0.00.
Composition Invariance Score
 Modular. r = 0.97. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.19. p = 0.31.
Composition Invariance Score
 Contrastive. r = 0.65. p = 0.00.
Composition Invariance Score
 Modular. r = 0.94. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.76. p = 0.00.
Composition Invariance Score
 Contrastive. r = -0.38. p = 0.04.
Composition Invariance Score
 Modular. r = -0.70. p = 0.00.
Corruptions in Composition = 6
Figure 23: Correlating invariance scores with compositional robustness for cifar- 10(second random seed).
This is the same as Figure 10 with a different random seeding.
42Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.15. p = 0.44.
Elemental Invariance Score
 Contrastive. r = 0.02. p = 0.90.
Elemental Invariance Score
 Modular. r = -0.16. p = 0.41.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.18. p = 0.28.
Elemental Invariance Score
 Contrastive. r = -0.01. p = 0.93.
Elemental Invariance Score
 Modular. r = -0.07. p = 0.66.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.02. p = 0.94.
Elemental Invariance Score
 Contrastive. r = -0.03. p = 0.89.
Elemental Invariance Score
 Modular. r = 0.09. p = 0.63.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.07. p = 0.73.
Elemental Invariance Score
 Contrastive. r = 0.11. p = 0.55.
Elemental Invariance Score
 Modular. r = -0.10. p = 0.59.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.13. p = 0.50.
Elemental Invariance Score
 Contrastive. r = -0.01. p = 0.96.
Elemental Invariance Score
 Modular. r = -0.23. p = 0.22.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.21. p = 0.20.
Elemental Invariance Score
 Contrastive. r = -0.02. p = 0.88.
Elemental Invariance Score
 Modular. r = -0.17. p = 0.30.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.01. p = 0.97.
Elemental Invariance Score
 Contrastive. r = -0.13. p = 0.50.
Elemental Invariance Score
 Modular. r = -0.17. p = 0.38.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.15. p = 0.42.
Elemental Invariance Score
 Contrastive. r = -0.03. p = 0.90.
Elemental Invariance Score
 Modular. r = -0.38. p = 0.04.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.99. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.99. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.95. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.96. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.91. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.95. p = 0.00.
Composition Invariance Score
 Modular. r = 0.90. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.56. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.95. p = 0.00.
Composition Invariance Score
 Modular. r = 0.91. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.03. p = 0.88.
Composition Invariance Score
 Contrastive. r = 0.44. p = 0.02.
Composition Invariance Score
 Modular. r = 0.73. p = 0.00.
Corruptions in Composition = 6
Figure 24: Correlating invariance scores with compositional robustness for facescrub (second random seed).
This is the same as Figure 11 with a different random seeding.
43Under review as submission to TMLR
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling
(a) EMNIST
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling (b) CIFAR-10
1 2 3 4 5 6
Corruptions in Composition020406080100Accuracy (%)
ChanceCeiling
ERM
Contrastive
Modular (c) FACESCRUB
Figure 25: Evaluating compositional robustness on different datasets (third random seed). This figure is the
same as Figure 2 with a different random seeding.
44Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.08. p = 0.68.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 0.99.
Elemental Invariance Score
 Modular. r = -0.43. p = 0.02.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.32. p = 0.04.
Elemental Invariance Score
 Contrastive. r = 0.10. p = 0.54.
Elemental Invariance Score
 Modular. r = -0.49. p = 0.00.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.35. p = 0.06.
Elemental Invariance Score
 Contrastive. r = 0.09. p = 0.63.
Elemental Invariance Score
 Modular. r = -0.29. p = 0.12.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.30. p = 0.10.
Elemental Invariance Score
 Contrastive. r = -0.11. p = 0.56.
Elemental Invariance Score
 Modular. r = -0.24. p = 0.19.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = -0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.08. p = 0.68.
Elemental Invariance Score
 Contrastive. r = 0.12. p = 0.52.
Elemental Invariance Score
 Modular. r = -0.70. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.02. p = 0.92.
Elemental Invariance Score
 Contrastive. r = -0.06. p = 0.69.
Elemental Invariance Score
 Modular. r = -0.74. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.18. p = 0.35.
Elemental Invariance Score
 Contrastive. r = -0.21. p = 0.26.
Elemental Invariance Score
 Modular. r = -0.72. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.41. p = 0.02.
Elemental Invariance Score
 Contrastive. r = -0.86. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.88. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.40. p = 0.03.
Composition Invariance Score
 Contrastive. r = 0.85. p = 0.00.
Composition Invariance Score
 Modular. r = 0.85. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.49. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.95. p = 0.00.
Composition Invariance Score
 Modular. r = 0.90. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.34. p = 0.07.
Composition Invariance Score
 Contrastive. r = 0.94. p = 0.00.
Composition Invariance Score
 Modular. r = 0.78. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = -0.35. p = 0.06.
Composition Invariance Score
 Contrastive. r = 0.58. p = 0.00.
Composition Invariance Score
 Modular. r = 0.54. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.64. p = 0.00.
Composition Invariance Score
 Contrastive. r = -0.34. p = 0.07.
Composition Invariance Score
 Modular. r = 0.33. p = 0.08.
Corruptions in Composition = 6
Figure 26: Correlating invariance scores with compositional robustness for emnist(third random seed). This
is the same as Figure 3 with a different random seeding.
45Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.73. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.62. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.02. p = 0.91.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.74. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.72. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.22. p = 0.17.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.58. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.67. p = 0.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 0.98.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = -0.22. p = 0.24.
Elemental Invariance Score
 Contrastive. r = 0.20. p = 0.29.
Elemental Invariance Score
 Modular. r = -0.25. p = 0.19.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = -0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.63. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.62. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.21. p = 0.28.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.56. p = 0.00.
Elemental Invariance Score
 Contrastive. r = 0.63. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.34. p = 0.03.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.42. p = 0.02.
Elemental Invariance Score
 Contrastive. r = 0.51. p = 0.00.
Elemental Invariance Score
 Modular. r = -0.10. p = 0.59.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.20. p = 0.29.
Elemental Invariance Score
 Contrastive. r = 0.31. p = 0.10.
Elemental Invariance Score
 Modular. r = -0.45. p = 0.01.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.98. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.78. p = 0.00.
Composition Invariance Score
 Modular. r = 0.98. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.93. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.65. p = 0.00.
Composition Invariance Score
 Modular. r = 0.98. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.89. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.75. p = 0.00.
Composition Invariance Score
 Modular. r = 0.98. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.58. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.74. p = 0.00.
Composition Invariance Score
 Modular. r = 0.93. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.11. p = 0.57.
Composition Invariance Score
 Contrastive. r = -0.41. p = 0.03.
Composition Invariance Score
 Modular. r = 0.52. p = 0.00.
Corruptions in Composition = 6
Figure 27: Correlating invariance scores with compositional robustness for cifar- 10(second random seed).
This is the same as Figure 10 with a different random seeding.
46Under review as submission to TMLR
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = 0.13. p = 0.51.
Elemental Invariance Score
 Contrastive. r = -0.01. p = 0.96.
Elemental Invariance Score
 Modular. r = -0.13. p = 0.51.
Corruptions in Composition = 2
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.14. p = 0.39.
Elemental Invariance Score
 Contrastive. r = -0.04. p = 0.81.
Elemental Invariance Score
 Modular. r = -0.12. p = 0.47.
Corruptions in Composition = 3
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = -0.02. p = 0.93.
Elemental Invariance Score
 Contrastive. r = -0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.02. p = 0.93.
Corruptions in Composition = 4
0.5 0.6 0.7 0.8 0.9 1.0020406080100
Elemental Invariance Score
 ERM. r = 0.05. p = 0.80.
Elemental Invariance Score
 Contrastive. r = 0.20. p = 0.28.
Elemental Invariance Score
 Modular. r = 0.21. p = 0.27.
Corruptions in Composition = 5
0.5 0.6 0.7 0.8 0.9 1.0020406080100Accuracy (%)
Elemental Invariance Score
 ERM. r = -0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = 0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = 0.07. p = 0.72.
Elemental Invariance Score
 Contrastive. r = -0.08. p = 0.68.
Elemental Invariance Score
 Modular. r = -0.21. p = 0.28.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = 0.07. p = 0.66.
Elemental Invariance Score
 Contrastive. r = -0.05. p = 0.77.
Elemental Invariance Score
 Modular. r = -0.23. p = 0.15.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.11. p = 0.57.
Elemental Invariance Score
 Contrastive. r = -0.07. p = 0.72.
Elemental Invariance Score
 Modular. r = -0.11. p = 0.56.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Elemental Invariance Score
 ERM. r = -0.25. p = 0.19.
Elemental Invariance Score
 Contrastive. r = -0.11. p = 0.57.
Elemental Invariance Score
 Modular. r = -0.42. p = 0.02.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Composition Invariance Score
Elemental Invariance Score
 ERM. r = -0.00. p = 1.00.
Elemental Invariance Score
 Contrastive. r = 0.00. p = 1.00.
Elemental Invariance Score
 Modular. r = -0.00. p = 1.00.
Corruptions in Composition = 6
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.99. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.98. p = 0.00.
Composition Invariance Score
 Modular. r = 0.99. p = 0.00.
Corruptions in Composition = 2
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.95. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.94. p = 0.00.
Composition Invariance Score
 Modular. r = 0.98. p = 0.00.
Corruptions in Composition = 3
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = 0.89. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.93. p = 0.00.
Composition Invariance Score
 Modular. r = 0.90. p = 0.00.
Corruptions in Composition = 4
0.0 0.2 0.4 0.6 0.8 1.0020406080100
Composition Invariance Score
 ERM. r = 0.64. p = 0.00.
Composition Invariance Score
 Contrastive. r = 0.87. p = 0.00.
Composition Invariance Score
 Modular. r = 0.67. p = 0.00.
Corruptions in Composition = 5
0.0 0.2 0.4 0.6 0.8 1.0020406080100Accuracy (%)
Composition Invariance Score
 ERM. r = -0.18. p = 0.34.
Composition Invariance Score
 Contrastive. r = -0.07. p = 0.73.
Composition Invariance Score
 Modular. r = 0.32. p = 0.09.
Corruptions in Composition = 6
Figure 28: Correlating invariance scores with compositional robustness for facescrub (second random seed).
This is the same as Figure 11 with a different random seeding.
47Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)89 89 89
88 88 89
88 88 89
86 87 88
89 89 89
87 87 88
88 88 89
84 83 30
84 83 63
43 62 60
85 85 5.5
84 82 80
84 82 80
57 58 88
57 58 88
85 84 88
85 84 88
62 72 59
82 81 8.6
83 82 85
83 82 85
43 43 86
43 43 86
79 80 79
80 80 86
80 76 85
79 77 85
55 55 86
54 54 86
80 80 54
79 80 86
50 51 87
50 51 87
83 84 88
83 84 68
61 62 85
63 64 85
7.9 8.2 29
7.9 8.2 29
32 33 2.7
11 18 6.4
26 32 5.1
71 70 3.6
10 13 63
10 13 63
3.6 5.3 53
15 16 3.3
67 67 21
60 62 48
17 20 84
17 19 84
4.5 5.3 47
4.5 5.2 48
40 46 9.4
67 67 6.1
18 9.4 7.3
18 9.4 7.3
30 28 24
77 76 12
61 62 68
66 65 43
14 15 70
17 18 81
75 73 34
17 16 49
25 26 42
18 20 81
68 64 54
67 64 54
14 17 82
14 17 82
65 64 54
61 59 83
14 16 83
14 16 46
5.9 7.1 77
5.9 7.1 77
5 4.7 2.9
12 9.4 3.4
2.2 2.9 8.3
3.2 3.7 2.3
2.8 3.3 2.1
2.7 3.3 3.8
6.7 7.8 26
6.5 7.7 6.6
10 6.2 1.7
13 7.3 11
8.1 13 11
49 52 2.5
11 11 9
54 51 22
4.3 5.4 18
4.9 5.5 63
5.4 6 35
5.5 6.2 37
7.9 8.9 3.1
7.3 8.4 3.1
5 6.2 4.8
2.4 3.1 13
53 53 13
20 20 20
6.3 8.1 19
6.3 8 20
7.6 9.4 38
6.9 8.2 11
8.2 9.4 4
9.6 9.9 3.7
2.6 3.1 2.2
2.6 3.1 2.2
2.5 3.1 6.2
2.5 3 2.4
2.4 3 1.8
1.9 2 2.5
2 2.1 2.4
4.3 4.5 1.9
2 2.5 8
2.2 2.4 7.1
2 2.4 2.8
2.5 3.4 2.7
2.4 3.3 2.9
2.2 3.1 2.5
2.1 2.5 2.7
9 6.5 2.1
8.9 6.7 3.7
11 6.7 2.6
4.1 4.4 4.8
7.5 5.2 2.6
2 2.6 2.6
4.1 4.8 4.8
3.9 4.8 2.2
2.1 2.4 2.7
2.2 2.5 2.7
4.7 5.8 2.9
4.7 5.5 11
4.9 5.5 11
2.2 2.7 3.7
4.5 5.1 2.9
2 2.2 2.1
2.2 2.9 1.8
2.2 3 2.4
2.3 3 2
2.4 2.9 2.3
2.2 3 2.4
2.1 2.2 2.3
2 2.3 2.1
2.2 3 2.5
1.9 2.1 2.3
2.4 3.2 2.2
2.4 2.9 2.5
2.3 2.9 2.5
2 2.4 3
2.1 2.3 3
2.3 2.8 2.3
1.8 2.3 3.1
2.2 2.9 2.4
2.4 3.1 2.5
2.4 2.9 2.2
2.1 2.2 2
2 2.4 2.1
2.3 2.9 2
2.3 2.9 2
2.2 2.9 2
2.4 2.9 2.5
2.4 2.9 2.5
2.1 2.3 2.2
2 2.3 2.1
2.4 3 2.5Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.3 0.32 0.31
0.33 0.36 0.32
0.34 0.37 0.31
0.4 0.4 0.33
0.3 0.33 0.3
0.37 0.4 0.37
0.33 0.36 0.32
0.47 0.54 2.6
0.47 0.54 1.4
2 1.4 1.5
0.45 0.46 5.1
0.53 0.66 1.1
0.53 0.66 1.1
1.4 1.6 0.39
1.4 1.6 0.39
0.44 0.48 0.34
0.44 0.48 0.34
1.2 0.91 1.5
0.52 0.59 4.6
0.51 0.58 0.49
0.51 0.58 0.49
2 2.3 0.46
2 2.3 0.46
0.61 0.65 0.67
0.61 0.64 0.42
0.64 0.93 0.46
0.65 0.91 0.46
1.7 1.8 0.45
1.7 1.8 0.45
0.6 0.65 1.7
0.65 0.64 0.4
1.8 2 0.43
1.8 2 0.43
0.49 0.51 0.37
0.49 0.51 1.7
1.4 1.5 0.58
1.3 1.4 0.6
4.4 4 2.7
4.4 4 2.7
2.5 2.6 6.6
3.9 3.2 5.8
2.9 2.7 4.7
0.95 1.1 4.8
3.7 3.4 1.8
3.7 3.4 1.8
5 4.1 1.9
3.3 3.3 7.4
1 1.1 3
1.3 1.3 1.9
3.6 3.5 0.66
3.7 3.5 0.64
5.7 4.7 2.1
5.7 4.7 2.1
2.3 2.1 3.9
1 1.2 4.4
3.1 4.1 4
3.1 4.1 4
2.6 2.9 3.1
0.8 0.94 4.2
1.2 1.3 1.2
1.1 1.2 2.5
4.4 4.2 1.2
3.9 3.9 0.86
1.1 1.4 2.5
3.3 3.4 1.9
3.3 3.1 2.3
4.1 3.9 0.77
1.1 1.4 2.1
1.1 1.4 2.1
3.6 3.2 0.81
3.6 3.2 0.81
1.2 1.4 1.8
1.3 1.6 0.69
4.6 4.2 0.78
4.6 4.2 2.3
5 4.2 0.95
5 4.2 0.95
4.2 3.8 8
3.3 3.7 6.1
5.3 4.2 5.4
4.9 4.1 7.2
4.9 4.8 6.6
4.9 4.8 5.7
4.4 3.8 2.8
4.4 3.8 3.6
3.6 4.5 5.7
3.4 4.3 3.5
4.1 3.5 4.2
1.7 1.8 5.1
3.7 3.6 3.7
1.7 2.1 3.1
5.8 4.5 3.2
5.6 4.5 1.6
5.4 4.3 2.5
5.4 4.3 2.4
4.4 3.8 5.5
4.6 4 5.9
4.5 3.9 5.2
5 4 3.9
1.6 1.8 3.6
3.1 3.2 3.2
3.9 3.5 3.3
3.9 3.5 3.3
4.6 3.7 2.6
4.7 3.7 3.7
4.6 4.1 5
4.2 4 5.1
4.9 4.7 6.7
4.9 4.7 6.7
4.9 4.7 4.1
5 4.8 5.6
5 4.8 5.7
5.5 4.2 6
5.5 4.2 6.1
5 4.1 5.6
5.4 4.3 4.5
5.4 4.3 4.6
4.7 4 6.5
4.6 4.4 8.5
4.6 4.4 7.6
4.5 4.3 8.7
4.7 4 6.5
3.5 3.8 5
3.5 3.8 5.1
3.4 4.1 6.1
4.3 3.9 4.5
3.7 4.1 5.6
5.1 4 5.2
5 4.1 4.5
5 4.1 5.4
5.2 4 5.2
5.1 4 5.2
4.2 3.8 4.5
4.2 3.8 3.6
4.2 3.8 3.6
4.7 3.9 4.2
4.2 3.8 4.5
4.7 4 7.4
4.6 4.3 6.7
4.6 4.3 7.1
4.6 4.4 6.4
4.5 4.3 7
4.6 4.3 7.1
4.7 4 6.8
4.7 4 5.9
4.4 4.2 6.4
4.7 4 5.9
4.6 4.4 7.2
4.7 4.5 5.6
4.7 4.5 5.6
4.7 4 5.3
4.7 4 5.3
4.6 4.4 7.1
4.7 4 5.4
4.5 4.1 6.5
4.6 4.4 6.2
4.7 4.5 7.2
4.8 4 7.9
4.7 4 7.8
4.6 4.3 6.4
4.6 4.4 6.5
4.6 4.4 6.5
4.7 4.5 5.6
4.7 4.5 5.6
4.7 4 5.9
4.7 4 5.9
4.7 4.5 5.6Loss Heatmap
1020304050607080
12345678
Figure 29: Per-domain heat map for emnist(first random seed). This shows the raw emnistdata from
Figure 2. Best viewed with zoom.
48Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)91 91 91
86 87 85
90 91 92
83 88 91
89 89 89
86 88 82
89 89 87
60 58 70
59 58 70
39 61 83
13 12 11
83 75 84
83 75 84
77 74 78
77 74 78
86 86 85
86 86 85
42 33 11
29 31 21
62 62 75
62 62 75
58 61 71
58 61 71
78 78 66
81 82 79
50 68 87
50 68 87
49 67 80
49 67 80
41 25 17
75 83 85
68 67 76
68 67 76
81 81 83
81 81 83
70 68 62
70 69 72
28 29 51
28 29 51
11 12 14
21 21 10
12 11 11
11 13 13
51 39 67
51 39 67
19 28 66
11 12 11
49 47 61
45 44 49
49 46 67
49 46 59
20 18 11
19 19 11
21 15 13
25 26 18
24 24 51
24 24 51
13 12 9.8
19 19 13
41 40 53
49 50 68
36 35 50
45 46 61
12 12 11
15 25 71
22 19 16
37 46 62
63 57 74
63 57 74
19 30 73
19 31 72
20 20 14
36 51 79
41 36 50
41 36 50
23 25 57
23 25 57
8.4 6.8 9.5
10 11 10
15 17 10
9.9 11 12
9.6 10 37
9.7 10 38
24 20 42
24 20 52
16 17 41
15 16 30
20 21 10
19 18 16
11 16 55
11 12 11
13 15 27
18 18 44
23 20 35
23 21 26
11 13 14
11 11 10
13 12 11
7.8 9.3 9.9
16 15 13
10 10 10
11 11 11
11 11 11
13 14 12
14 18 58
16 15 16
16 16 13
7.8 8.6 24
7.8 8.6 24
7.6 8.9 18
8.5 8.9 28
8.6 8.9 28
11 10 12
11 10 13
15 15 14
15 17 10
15 17 10
7.9 5.8 9
8.9 9.5 11
8.7 9.1 11
10 10 12
7.5 5.9 8.5
10 11 10
10 11 9.7
11 10 9.3
8.2 6.6 9.1
10 9.8 10
7.6 9.1 9.7
13 13 11
13 12 11
7.9 9.1 9.4
7.4 9 9.4
11 12 10
11 11 9.5
11 12 10
6.7 8.9 34
10 11 9.7
9.4 9.9 8.6
10 9.6 11
10 9.8 11
10 10 11
10 9.1 10
11 9.6 11
9.3 9.8 8.6
7.5 6 8.3
10 11 9.2
7.9 6.1 8.3
8.8 9.3 9.4
8.1 9 10
8.2 9 9.9
7.7 6.3 8.3
7.7 6.2 8.6
10 10 11
7.7 6.1 8.8
10 10 9.6
8.8 9 9.5
7.8 9.2 9.9
8 6.1 8.2
7.8 6 8.6
10 10 11
10 9.9 11
10 9.7 11
8 9.1 10
8.4 9.6 10
7.7 6.1 8.4
7.9 6.3 8.2
8.1 9.3 10Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.38 0.32 0.41
0.57 0.46 0.69
0.4 0.32 0.35
0.73 0.43 0.41
0.43 0.39 0.47
0.56 0.41 0.73
0.49 0.39 0.53
2 1.7 1.4
2 1.7 1.4
3.1 1.5 0.74
6.5 5 8.1
0.75 0.94 0.7
0.75 0.94 0.7
0.98 0.92 0.92
0.98 0.92 0.92
0.62 0.48 0.64
0.62 0.48 0.64
3 3.1 7.2
4 2.9 4.6
1.8 1.5 1.2
1.8 1.5 1.2
2 1.5 1.2
2 1.5 1.2
0.93 0.82 1.6
0.81 0.66 0.93
2.4 1.3 0.57
2.4 1.2 0.58
2.4 1.2 0.85
2.4 1.2 0.84
3 3.4 4.7
1.1 0.63 0.62
1.4 1.2 0.99
1.4 1.2 0.99
0.79 0.7 0.71
0.79 0.7 0.71
1.3 1.2 1.7
1.3 1.1 1.2
4.2 3.2 2.3
4.2 3.2 2.3
6.7 4.7 4.9
4.8 3.5 7.9
7.1 6 5.9
7.8 5.1 5.6
2.4 2.6 1.4
2.4 2.6 1.4
4.8 3.3 1.5
7.3 5.1 8.8
2.6 2.2 1.8
2.9 2.4 2.4
2.6 2.2 1.4
2.6 2.2 1.8
5.2 4.2 6.2
5.2 4.2 6.1
4.8 4.4 5.1
4.4 3.2 4.5
4.4 3.4 2.4
4.4 3.4 2.4
5.8 4.7 6.7
5 3.6 5.8
3.1 2.6 2.2
2.5 2.1 1.4
3.6 2.8 2.3
2.7 2.2 1.7
6.8 5.2 8.1
5.1 3.5 1.4
4.6 3.9 4.8
3.3 2.2 1.7
1.8 1.8 1.1
1.8 1.8 1.1
4.8 3.1 1.2
4.8 3.1 1.2
4.5 3.8 4.5
3.3 2.1 0.88
3.1 2.8 2.3
3.1 2.8 2.3
4.5 3.5 2
4.5 3.5 2
6.4 4.6 7
7.3 5.3 8.3
5 3.7 7
6.9 4.9 5.3
5.9 4.4 3.1
5.9 4.4 3.1
4.7 3.9 2.9
4.7 3.9 2.2
5.1 3.8 2.8
5.3 3.9 3.5
4.8 3.5 7.2
5 4.1 4.8
5.5 4 2.1
7.9 5.1 5
5.9 4.3 3.7
5.2 4.1 2.7
4.9 3.8 3.2
4.9 3.8 4
8.2 5.1 5.7
7.4 5.1 10
5.9 4.2 6.6
6.4 5 5.9
5.5 3.8 5.1
6 4.9 6
7.2 5.2 8.2
7.1 5.2 8.2
5.6 4.4 5.8
5.4 4 1.8
5.8 4.2 4.5
5.6 4 5.5
6.4 4.7 4
6.4 4.7 4
6.4 4.6 4.4
6.1 4.5 3.6
6.1 4.4 3.6
7.6 6.3 5.6
7.5 6.2 5.4
5.9 4.5 5.4
5 3.7 7.3
5 3.7 7.9
6.4 4.6 6.1
6.5 4.8 7.7
6.4 4.7 7.8
7 4.8 7
6.4 4.6 6
7.3 5.3 7.6
7.3 5.3 7.6
5.9 4.6 7
6.5 4.6 6.2
7 4.7 5.7
6.5 5.1 6.8
6.1 4.2 6.9
6 4.2 6.9
6.5 5.1 6.8
6.5 5.1 6.8
8.2 5.1 6
8.2 5.2 5.9
8.2 5.1 6
6.2 4.6 3.4
7.3 5.2 8.2
7.3 6.4 6
7 4.7 7.5
7.1 4.8 7.6
7.1 4.8 7.9
7.2 4.7 6.4
7 4.8 7.6
7.2 6.3 6.1
6.4 4.5 6.3
8.5 5.2 6.3
6.4 4.5 6.5
6.3 4.8 6.5
6.4 4.7 7.7
6.4 4.7 7.6
6.4 4.5 6.6
6.4 4.5 6.6
7.1 4.8 7.5
6.4 4.5 6.5
7.4 5.3 9.2
6.3 4.8 6.5
6.4 4.7 7.4
6.4 4.5 6.6
6.4 4.5 6.6
7 4.8 8
7 4.8 8
7 4.8 8
6.5 4.7 7.7
6.5 4.7 7.7
6.4 4.5 6.6
6.4 4.5 6.6
6.5 4.7 7.7Loss Heatmap
102030405060708090
246810
Figure 30: Per-domain heat map for cifar- 10(first random seed). This shows the raw cifar- 10data from
Figure 2. Best viewed with zoom.
49Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)96 96 96
95 96 95
96 97 96
95 96 96
96 96 95
95 96 92
94 96 95
87 89 84
87 89 83
77 85 93
1 0.27 0.45
93 94 89
93 94 89
90 93 88
90 93 88
93 95 94
93 95 94
75 66 40
29 29 7
92 93 88
92 93 73
86 87 86
86 87 86
89 90 63
90 91 92
55 74 93
54 74 92
89 93 90
90 92 90
7 1.2 1.1
91 93 94
91 91 87
91 91 87
93 94 94
93 94 94
76 70 59
76 71 60
14 14 61
14 14 61
0.86 0.47 0.31
9.2 11 12
0.33 0.21 0.27
0.96 0.49 0.55
63 73 59
63 73 59
9.7 25 80
0.55 0.23 0.33
61 58 70
64 69 44
48 46 51
48 44 50
23 22 27
22 23 27
0.66 0.41 0.35
14 13 2.4
52 62 32
52 62 17
4.4 14 25
35 28 8.9
80 79 51
82 78 79
26 15 22
35 28 46
5.8 1.7 1
1.4 7.5 65
0.94 0.53 0.35
49 47 53
85 88 81
85 88 81
6.8 22 78
6.4 23 79
7.9 3.2 1.3
30 46 85
56 43 53
56 43 53
61 52 66
61 52 48
0.53 0.45 2.1
0.47 0.29 0.41
0.64 0.61 6.9
0.23 0.31 0.27
1.3 1.1 7.2
1.2 1.1 7.5
10 9.7 23
10 9.8 23
22 22 6.7
24 28 13
4.5 3.5 3.8
15 16 0.88
0.8 2.1 43
1.6 0.84 0.62
8 2.3 16
11 4.3 14
1.7 1.2 11
1.7 1.2 11
0.37 0.21 0.33
0.33 0.21 0.29
5.7 3 1.6
0.59 0.41 15
22 18 1.7
1.9 4.5 14
0.78 0.33 0.55
0.57 0.41 0.76
0.68 0.33 0.27
0.9 1.7 36
1.7 0.86 2.8
1.7 1.8 1.2
0.41 0.41 3.9
0.41 0.41 2
0.39 0.39 1.7
0.47 0.33 3
0.47 0.33 1.4
0.33 0.21 0.31
0.31 0.21 0.31
0.35 0.47 0.35
0.43 0.49 2.7
0.41 0.51 1.8
0.47 0.27 1.4
0.45 0.41 1.1
0.41 0.41 0.45
0.21 0.23 0.53
0.49 0.29 1.1
0.39 0.23 0.37
0.39 0.25 0.23
3.1 4.4 1.3
0.41 0.31 0.94
0.27 0.39 0.25
0.53 0.41 5.1
1.2 0.59 1.4
1.2 0.45 0.62
0.47 0.43 3.6
0.45 0.31 2.8
0.25 0.23 0.31
0.25 0.23 0.35
0.33 0.23 0.39
0.51 0.21 5.4
0.31 0.23 0.35
0.16 0.21 0.21
0.18 0.21 0.29
0.2 0.27 0.25
0.33 0.31 0.33
0.29 0.21 0.47
0.33 0.25 0.31
0.21 0.21 0.29
0.49 0.25 0.55
0.33 0.21 0.43
0.41 0.27 0.55
0.31 0.31 0.43
0.35 0.29 0.27
0.35 0.23 0.39
0.45 0.2 0.47
0.43 0.25 0.53
0.29 0.25 0.43
0.43 0.21 0.33
0.27 0.2 0.31
0.25 0.31 0.33
0.31 0.31 0.39
0.47 0.2 0.45
0.33 0.18 0.41
0.21 0.23 0.41
0.31 0.23 0.33
0.2 0.21 0.33
0.41 0.35 0.33
0.29 0.35 0.37
0.33 0.16 0.47
0.43 0.25 0.39
0.33 0.27 0.39Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.26 0.16 0.21
0.29 0.19 0.26
0.23 0.15 0.2
0.29 0.18 0.22
0.26 0.17 0.23
0.3 0.18 0.4
0.32 0.19 0.23
0.74 0.48 0.79
0.74 0.48 0.79
1.2 0.65 0.34
6.5 9 6.9
0.42 0.24 0.56
0.42 0.24 0.56
0.52 0.31 0.54
0.52 0.31 0.54
0.4 0.23 0.27
0.4 0.23 0.27
1.4 1.5 2.8
3.8 3.6 5.3
0.45 0.32 0.63
0.45 0.32 1.3
0.74 0.57 0.63
0.74 0.57 0.63
0.61 0.44 1.8
0.51 0.38 0.38
2.4 1.1 0.38
2.4 1.1 0.38
0.56 0.33 0.46
0.56 0.34 0.46
5.5 7.7 6.4
0.47 0.3 0.27
0.48 0.37 0.62
0.48 0.37 0.62
0.4 0.26 0.32
0.4 0.26 0.32
1.2 1.3 1.9
1.2 1.3 1.9
4.7 5 1.8
4.7 5 1.8
6.8 7.9 7.2
5.1 5.5 4.9
6.5 9 7
6.6 8.4 6.7
1.9 1.3 1.9
1.9 1.3 1.9
5.1 4.1 0.94
6.7 9.4 7.1
2 1.8 1.4
1.8 1.3 2.6
2.6 2.5 2.3
2.6 2.5 2.3
4.1 4.2 3.7
4.1 4.2 3.8
6.6 8.7 6.9
4.8 5.1 6
2.5 1.7 3.4
2.5 1.7 4.4
5.8 5.2 3.7
3.4 3.7 5.1
1 0.91 2.3
0.96 0.99 0.96
3.8 4.6 4
3.3 3.7 2.6
5.6 7.4 6.5
6.4 6.1 1.6
6.4 8.3 7.2
2.5 2.4 2.2
0.85 0.51 0.88
0.85 0.5 0.88
5.5 4.4 1
5.5 4.4 0.98
5.4 6.8 6.3
3.7 2.6 0.7
2.2 2.7 2.2
2.2 2.7 2.2
2 2.3 1.6
2 2.3 2.5
6.8 8.5 6.1
6.9 9 6.9
6.4 7.9 5.5
6.9 8.3 7.2
6.1 7.6 5.4
6.1 7.6 5.4
5 5.3 3.9
5 5.2 3.9
4.1 4 5.4
3.9 3.6 4.6
5.6 6.7 5.9
4.7 4.8 6.7
6.6 7.3 2.7
6.3 8.1 6.6
5.2 6.6 4.4
5 6 4.8
6 6.9 4.9
6 7 5
6.8 9.2 7.3
6.7 9.5 7.3
5.6 7 6.4
6.7 8 4.7
4.2 4.8 6.3
6.3 6.6 4.5
6.8 9 7
6.8 9 7
6.6 8.2 7.2
6.5 6.9 3.1
6.1 7.5 6.1
6.2 7.1 6.5
6.5 7.9 5.8
6.5 7.9 6.3
6.5 8 6.3
6.5 8.2 6
6.5 8.2 6.4
6.5 9.1 6.9
6.5 9.1 6.9
6.5 7.9 7.2
6.6 8.1 6.1
6.6 8.1 6.3
6.8 8.8 6.4
6.6 8.3 6.4
6.6 8.3 7
7.1 8.5 6.8
6.8 8.7 6.4
7.2 9.2 6.8
7.2 9.2 6.9
5.8 6.4 6.3
6.8 8.6 6.4
6.9 8.4 6.9
6.7 8.4 5.7
6.4 8 6.3
6.3 8 6.9
6.7 8.4 5.9
6.7 8.4 6.1
7 9.1 7.4
7 9.1 7.4
7 9.1 7.4
6.8 8.8 5.7
7 9.2 7.3
6.8 9.1 7
6.8 8.8 7.3
7.1 8.6 6.8
7.1 8.6 6.8
6.8 8.8 6.7
7.1 8.6 6.9
6.8 9.1 7
6.8 8.8 6.7
7.4 9.3 6.6
6.8 8.9 6.7
6.6 8.4 6.6
6.8 8.7 6.9
6.8 8.7 6.6
6.8 8.9 6.5
6.8 8.9 6.5
7.1 8.6 6.8
6.8 8.8 6.5
7.5 9.2 6.8
6.6 8.4 6.9
6.8 8.7 6.6
6.8 8.8 6.7
6.8 8.8 6.7
7.2 8.6 7.2
7.1 8.6 6.9
7.1 8.6 6.9
6.8 8.6 6.9
6.8 8.6 6.9
6.8 8.8 6.5
6.8 8.8 6.5
6.8 8.6 6.6Loss Heatmap
20406080
2468
Figure 31: Per-domain heat map for facescrub (first random seed). This shows the raw facescrub data
from Figure 2. Best viewed with zoom.
50Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)89 89 89
88 88 88
88 88 89
87 86 88
89 89 89
87 86 88
88 88 89
84 84 7.3
84 84 15
44 61 66
85 85 5.8
84 82 79
84 82 79
56 58 88
56 58 88
84 84 88
84 84 88
62 72 67
82 82 6.7
84 82 84
84 82 84
42 43 86
42 43 86
80 79 81
80 79 86
78 77 83
78 77 83
53 54 86
53 54 86
79 80 53
79 80 86
50 52 87
50 52 87
83 84 88
83 84 67
60 62 85
62 64 84
6.9 8.1 3.9
6.9 8.1 3.9
33 32 2.8
11 18 11
25 31 5.2
71 68 3.9
12 13 70
12 13 70
3.7 5.3 61
15 17 3.5
67 68 5.4
61 63 15
17 20 83
16 19 83
4.4 5.5 61
4.5 5.6 61
40 44 11
67 66 6.2
20 12 5.6
20 12 5.6
30 30 20
77 77 14
59 63 67
65 67 40
13 15 71
16 19 79
74 73 39
17 17 51
24 27 41
18 20 81
65 64 57
65 64 57
16 18 80
16 18 80
63 65 50
60 60 80
14 16 82
14 16 41
6.6 8.2 77
6.6 8.2 77
5.3 4.7 2.7
14 10 3.1
2.1 2.5 13
3.1 3.4 1.8
2.8 3 2.9
2.8 3 4.2
7.2 8.3 32
7.1 8.3 10
12 7.5 4
14 8.7 11
7.7 13 14
50 51 2.7
11 11 12
53 52 32
4.9 5.9 17
5.4 6 60
4.8 6.3 11
4.9 6.5 11
7.9 8.9 2.8
7 8.4 2.7
5.7 7.2 7.5
2.5 2.8 12
50 54 14
20 21 14
7.5 9 26
7.7 8.7 26
7.7 9.3 33
7.3 8.5 18
8.1 9.4 5
9 10 4.1
2.8 3 2
2.8 3 2
2.8 3 7.8
2.6 2.7 2.6
2.6 2.7 1.5
1.8 2.2 2.4
1.8 1.9 2.6
4 4.1 1.2
2.2 2.3 10
2.1 2.4 9.1
2.1 2.4 2.5
2.6 2.9 2.1
2.8 2.9 2.2
2.4 3 2.2
2.2 2.3 2.3
9.2 7.1 2.4
9.8 7 2.8
13 8.3 4
4.3 4.2 4.2
8.1 6.1 2.2
2.2 2.6 3.5
4.4 5.2 6.2
4.4 5.2 2.8
2.1 2.4 3.4
2 2.5 3.2
5.2 6.1 6.3
5.4 6.3 18
5.2 6.4 18
2.2 2.8 6.6
5 5.8 5.7
2.2 2.3 2.3
2.4 2.9 2.1
2.2 2.7 2.2
2.2 2.8 2.1
2.4 3 2.1
2.3 2.8 2.2
2.2 2.3 2.2
2.1 2.4 2.3
2.4 3.1 2.2
2 2.3 2.2
2.7 3 2.3
2.5 2.7 2.1
2.5 2.7 2.1
2.1 2.2 2.3
1.9 2.5 2.3
2.5 2.8 2.2
2.2 2.3 2.3
2.4 2.8 2.2
2.7 2.9 2
2.5 2.7 2.2
2 2.2 2.3
2.1 2.4 2.3
2.2 2.8 2.1
2.3 2.6 2.2
2.4 2.7 2.2
2.5 2.6 2
2.4 2.6 2
2.1 2.4 2.3
2.2 2.3 2.5
2.6 2.7 2Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.3 0.32 0.3
0.33 0.36 0.33
0.34 0.37 0.31
0.39 0.4 0.33
0.3 0.33 0.3
0.37 0.4 0.38
0.33 0.36 0.32
0.46 0.51 4.2
0.46 0.52 3.4
2 1.4 1.2
0.44 0.47 4.6
0.52 0.66 1.1
0.52 0.66 1.1
1.5 1.6 0.41
1.5 1.6 0.41
0.44 0.48 0.35
0.44 0.48 0.35
1.2 0.92 1.2
0.52 0.59 4.9
0.49 0.56 0.52
0.49 0.56 0.52
2.1 2.3 0.51
2.1 2.3 0.51
0.6 0.66 0.62
0.6 0.65 0.45
0.68 0.86 0.53
0.69 0.85 0.53
1.8 1.8 0.46
1.7 1.9 0.46
0.62 0.66 1.7
0.65 0.64 0.4
1.9 2 0.44
1.9 2 0.44
0.51 0.5 0.38
0.51 0.5 1.6
1.5 1.5 0.59
1.4 1.4 0.61
4.6 3.9 5.3
4.6 3.9 5.3
2.5 2.7 7
4 3.2 4.3
3 2.7 4.4
0.94 1.2 4.2
3.5 3.4 1.6
3.5 3.4 1.6
5 4.1 1.5
3.4 3.3 5
1 1.1 4.6
1.2 1.3 3.6
3.8 3.5 0.72
3.9 3.6 0.7
5.9 4.7 1.6
5.9 4.7 1.6
2.3 2.1 3.7
1 1.2 4.3
3 3.8 4
3 3.8 4
2.6 2.8 3.3
0.79 0.91 4
1.3 1.3 1.3
1.1 1.2 2.5
4.6 4.2 1.2
4.1 3.9 0.98
1.1 1.3 2.3
3.3 3.4 1.8
3.5 3.1 2.4
4.3 3.9 0.79
1.2 1.4 2.1
1.2 1.4 2.1
3.4 3.2 0.9
3.4 3.2 0.9
1.2 1.3 2
1.3 1.6 0.79
4.6 4.2 0.81
4.6 4.2 2.4
4.8 4.1 0.99
4.8 4.1 0.99
4.2 3.9 7.1
3.3 3.6 6.4
5.4 4.2 4.1
5 4.1 6.6
4.8 4.5 4.5
4.8 4.5 4.6
4.3 3.8 2.7
4.2 3.8 3.5
3.6 4.1 4.9
3.4 3.9 3.4
4.3 3.4 3.8
1.6 1.9 5
3.7 3.6 3.5
1.8 2.1 2.6
5.6 4.5 3.2
5.4 4.4 1.7
5.6 4.3 4.1
5.6 4.3 4.1
4.5 3.8 4.7
4.6 3.9 4.8
4.4 3.8 4.2
5 4 3.7
1.7 1.8 3.6
3.1 3.2 3.5
3.7 3.5 3
3.7 3.5 3
4.6 3.8 2.7
4.6 3.8 3.2
4.8 4.1 4.4
4.4 3.9 4.4
4.8 4.4 5
4.8 4.4 5
4.8 4.4 3.9
4.9 4.5 4.7
4.9 4.5 5.6
5.6 4.3 5.2
5.6 4.3 5.2
5.1 4.1 4.7
5.4 4.2 4.1
5.5 4.2 4.1
4.6 4 6.6
4.5 4.2 9.5
4.5 4.2 8.8
4.5 4.2 8.9
4.6 4 6.6
3.5 3.8 5.2
3.5 3.8 5.5
3.4 3.8 6
4.3 3.9 5.2
3.7 3.9 6.1
5.1 4.1 4.4
4.8 4 4.1
4.8 4 4.6
5.1 4.1 4.5
5.1 4.1 4.5
4.1 3.7 3.9
4.1 3.7 3.3
4.1 3.7 3.3
4.6 4 3.8
4.1 3.7 4
4.6 4 6.3
4.5 4.1 7.3
4.6 4.2 7
4.6 4.2 7.8
4.5 4.1 6.7
4.5 4.2 7
4.6 4 6.4
4.7 4 5.6
4.4 4.1 6.8
4.7 4 5.6
4.5 4.2 7.2
4.6 4.3 7.3
4.6 4.3 7.3
4.7 4 5.8
4.7 4 5.7
4.5 4.2 7.1
4.7 4 5.8
4.4 4.1 6.9
4.5 4.2 6.8
4.6 4.3 7.2
4.7 4 6.4
4.7 4 6.4
4.6 4.2 7.7
4.6 4.2 7.7
4.6 4.2 7.7
4.6 4.3 7.3
4.6 4.3 7.3
4.7 4 5.6
4.7 4 5.6
4.6 4.3 7.3Loss Heatmap
1020304050607080
2468
Figure 32: Per-domain heat map for emnist(second random seed). This shows the raw emnistdata from
Figure 21. Best viewed with zoom.
51Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)91 92 90
86 87 85
90 91 92
85 86 91
90 89 89
87 89 82
88 89 89
62 62 71
62 62 71
50 54 85
11 11 11
83 76 84
83 76 84
77 75 80
77 75 80
86 86 86
86 86 86
47 36 11
23 30 21
63 68 75
63 68 75
58 62 74
58 62 74
78 77 68
80 82 79
61 67 87
61 67 87
58 61 81
58 61 80
26 24 12
79 80 86
69 60 78
69 60 78
82 81 84
82 81 84
72 68 77
72 69 77
26 30 61
26 30 61
11 12 14
22 23 9.8
10 11 10
11 12 12
51 37 70
51 37 70
22 24 73
10 11 11
50 49 61
47 44 52
49 45 73
48 45 74
19 18 10
19 18 10
17 16 11
20 26 16
26 28 54
26 28 54
16 15 10
16 20 13
41 38 56
50 54 69
35 32 56
46 46 66
11 12 10
19 24 75
17 16 11
42 41 74
65 57 76
64 58 76
26 24 75
25 25 75
18 18 12
45 50 82
44 32 70
44 32 70
25 24 64
25 24 64
7.7 7.7 9.7
10 10 10
16 17 9.8
11 11 12
9.9 10 45
9.9 10 45
25 19 59
26 19 59
19 19 45
17 15 38
21 22 9.8
21 18 18
13 16 61
11 12 11
14 12 47
19 18 56
22 21 42
22 21 43
11 11 12
10 11 10
12 13 12
8.7 9.6 10
15 15 13
13 13 10
10 11 10
10 11 10
14 14 11
17 16 67
14 17 15
13 17 15
8.9 8.2 31
8.9 8.2 31
8.7 8 31
9.3 8.8 37
9.3 8.7 37
10 10 9.8
10 10 10
16 14 16
15 17 10
15 17 9.8
6.7 6.5 10
9.5 9.4 10
9 9.2 10
9.7 9.3 9.3
6 6.9 10
10 10 10
10 10 10
12 11 11
7.4 7.4 9.9
10 9.4 8.5
8.5 9.1 9.7
11 13 11
11 12 11
8.7 9.7 9.6
8.5 9.4 9.8
10 11 11
11 11 11
11 11 11
7.8 9.1 46
10 11 10
10 9.9 10
9.7 8.6 8.7
9.7 9.2 8.6
10 9.5 8.6
10 8.6 8.4
9.8 9.2 8.3
9.9 9.7 10
6.7 6.6 10
10 10 10
6.6 6.7 11
9.5 8.9 10
8.9 8.9 9.9
8.5 8.9 9.9
6.6 7 11
6.4 6.5 11
9.5 9.2 8.4
6.7 6.7 10
10 10 10
9 8.8 10
9 9.2 10
6.9 6.7 11
6.7 6.5 10
9.7 9 8.2
9.6 9.2 8.6
9.9 9.4 8.3
9.4 8.9 9.7
8.7 8.8 9.8
6.6 6.7 10
6.7 6.8 11
8.8 9.3 9.8Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.37 0.29 0.42
0.57 0.44 0.67
0.39 0.31 0.36
0.61 0.48 0.42
0.42 0.37 0.49
0.53 0.39 0.74
0.48 0.38 0.5
1.8 1.5 1.4
1.8 1.5 1.4
2.5 1.8 0.68
6.7 4.9 7.3
0.76 0.88 0.7
0.76 0.88 0.7
0.98 0.87 0.86
0.98 0.87 0.86
0.58 0.48 0.61
0.58 0.48 0.61
2.6 2.8 7.1
4.8 2.8 4.9
1.7 1.2 1.2
1.7 1.2 1.2
1.9 1.4 1.1
1.9 1.4 1.1
0.93 0.83 1.5
0.82 0.65 0.92
1.8 1.2 0.58
1.8 1.2 0.57
1.9 1.5 0.83
1.9 1.5 0.83
4.4 3.3 6.2
0.91 0.71 0.59
1.3 1.4 0.96
1.3 1.4 0.96
0.76 0.7 0.69
0.76 0.7 0.69
1.2 1.2 1
1.2 1.1 1
4.3 3 1.8
4.3 3 1.8
6.5 5.4 5.8
4.3 3.4 7.4
7.3 5.3 6.9
6.5 5 6.5
2.4 2.6 1.4
2.4 2.6 1.4
4.7 3.3 1.1
7.2 4.9 7.1
2.4 2.1 1.9
2.7 2.3 2.5
2.6 2.1 1.2
2.6 2.2 1.2
5.3 4.2 8.5
5.3 4.2 8.5
5.6 4.1 6.6
5.3 3.1 5.2
4.1 3.3 2.2
4.1 3.3 2.2
5.4 4.4 7.1
5.6 3.5 5.9
3.1 2.7 2.1
2.5 1.8 1.4
3.7 2.9 2.1
2.7 2.2 1.6
6.7 4.8 7.6
4.6 3.4 1.2
5.7 3.9 9.4
2.8 2.4 1.1
1.6 1.7 1.1
1.6 1.7 1.1
4.2 3.3 1.1
4.2 3.3 1.1
5.3 3.8 5.8
2.8 2 0.84
2.8 2.9 1.3
2.8 2.9 1.3
4.4 3.5 1.7
4.4 3.5 1.7
5.6 4.3 6.8
6.6 4.7 7.9
4.9 3.8 9.3
6.7 5.4 6.4
5.8 4.3 2.7
5.8 4.3 2.7
4.5 3.7 2
4.4 3.8 2
4.7 3.7 2.9
5 4.1 3.2
4.4 3.5 7.2
4.8 4 4.7
5.3 3.9 1.9
6.5 4.9 6.8
5.7 4.4 2.6
5 3.9 2.1
5 3.6 3
4.9 3.6 3
6.9 4.9 6.7
7.3 4.9 7.6
6.8 4 6.5
6.6 4.9 9.6
5.3 3.8 5.5
5.7 4.5 7.2
7.2 4.9 7.5
7.2 4.9 7.4
6.2 4.2 9.6
5.1 4 1.5
6.1 3.7 6.5
6.4 3.8 6.3
6.2 4.7 3.5
6.2 4.7 3.5
6.2 4.7 3.5
6 4.5 3.2
6 4.5 3.2
7.7 5.4 11
7.6 5.5 11
5.4 4.4 5.3
4.9 3.8 11
4.9 3.8 11
5.7 4.3 11
5.6 4.9 6.9
5.7 4.9 6.9
6.6 5.6 8.3
5.7 4.3 11
6.7 4.7 7.9
6.7 4.7 7.9
5.6 4.7 5.7
5.6 4.3 6.6
6.5 5.6 6.8
6.6 4.8 11
6.8 4.2 6.7
6.8 4.2 6.8
6.6 4.9 11
6.6 4.9 11
6.9 5 7.5
6.9 5 7.5
6.9 5 7.5
6 4.3 2.6
7.2 4.9 8
7.8 5.6 13
6.5 5.4 8
6.5 5.7 8.2
6.5 5.7 8.1
6.4 5.2 8
6.6 5.6 8.3
7.6 5.7 13
5.7 4.3 14
6.6 5.1 7.6
5.7 4.3 14
6.1 4.9 6.2
5.8 4.9 7
5.7 4.9 6.9
5.7 4.3 14
5.7 4.3 14
6.5 5.7 8.2
5.7 4.3 14
6.9 4.8 7.8
6.2 4.9 6.2
5.7 4.9 6.9
5.7 4.3 14
5.7 4.3 14
6.6 5.7 8.3
6.6 5.7 8.2
6.6 5.6 8.3
5.7 5 7
5.7 5 7
5.7 4.3 14
5.7 4.3 14
5.7 5 7.1Loss Heatmap
102030405060708090
24681012
Figure 33: Per-domain heat map for cifar- 10(second random seed). This shows the raw cifar- 10data
from Figure 21. Best viewed with zoom.
52Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)95 97 96
95 96 95
96 97 96
95 96 96
95 96 96
94 96 92
94 96 95
87 92 91
87 92 91
81 88 92
0.59 0.25 0.49
93 95 94
93 95 94
90 93 90
91 93 90
92 95 94
92 95 94
74 72 0.41
26 22 8.5
92 93 92
92 93 55
86 88 89
86 88 90
88 90 66
90 92 93
69 82 92
69 82 0.53
90 92 90
90 92 90
4.5 0.86 0.82
92 94 94
91 91 89
91 91 89
93 95 94
93 95 94
77 72 1.8
78 72 37
19 24 73
19 24 73
0.74 0.37 0.61
13 17 0.25
0.55 0.25 0.29
0.94 0.49 0.35
72 72 81
72 72 81
17 20 74
0.39 0.29 0.29
62 67 83
67 71 45
47 48 29
47 47 1.5
29 22 0.25
28 22 0.29
0.78 0.27 0.39
13 10 3.7
56 67 69
56 67 13
10 18 0.39
29 24 4.7
81 79 65
82 80 88
27 18 12
34 35 27
1.7 2.5 2.9
3.7 14 0.27
1.3 0.55 0.31
51 49 1.6
86 87 91
86 87 91
19 40 0.27
19 39 0.31
4.9 3 0.41
46 59 0.33
58 46 1.5
58 46 1.5
64 56 85
64 56 31
0.21 0.55 0.27
0.31 0.21 0.33
0.78 0.41 0.37
0.39 0.29 0.35
3.3 1.4 5.7
3.3 1.4 5.7
17 12 0.88
17 12 20
23 28 4.4
28 30 35
4.8 6.1 0.27
12 18 4.1
1.2 4.5 0.27
1.4 1.2 0.29
9.2 3.4 0.88
12 7 0.55
2 1.6 4.9
1.9 1.5 0.49
0.43 0.27 0.29
0.43 0.29 0.27
4.9 2.6 1.5
0.88 0.55 0.23
18 16 3.2
3.9 6.7 0.29
0.39 0.37 1.2
0.45 0.33 0.35
0.9 0.33 0.27
2.6 5.1 0.27
1.4 1.1 0.39
1.3 2.3 0.41
0.74 0.64 5.2
0.74 0.64 1
0.88 0.68 0.21
0.62 0.68 6.1
0.62 0.68 0.57
0.39 0.27 0.29
0.35 0.29 0.27
0.51 0.43 0.62
0.35 0.59 0.21
0.39 0.49 0.25
0.039 0.29 0.25
0.57 0.29 1.4
0.7 0.39 1.2
0.35 0.2 0.31
0.039 0.25 0.31
0.37 0.29 0.27
0.27 0.23 0.31
3.5 5 2
0.059 0.35 0.27
0.23 0.23 0.35
0.39 0.45 0.33
0.96 0.62 0.66
0.94 0.43 0.47
0.49 0.35 0.27
0.49 0.43 0.29
0.31 0.29 0.31
0.35 0.2 0.23
0.41 0.29 0.29
0.33 0.35 0.29
0.35 0.27 0.35
0.27 0.27 0.29
0.29 0.31 0.43
0.25 0.21 0.27
0.29 0.25 0.33
0.35 0.23 0.12
0.27 0.23 0.31
0.23 0.29 0.25
0.059 0.29 0.25
0.25 0.29 0.25
0.14 0.31 0.27
0.47 0.41 0.23
0.49 0.51 0.43
0.47 0.35 0.51
0.16 0.35 0.27
0.12 0.29 0.21
0.25 0.25 0.31
0.078 0.29 0.2
0.21 0.29 0.25
0.47 0.47 0.2
0.39 0.43 0.14
0.098 0.31 0.25
0.078 0.33 0.21
0.25 0.2 0.2
0.35 0.29 0.49
0.29 0.21 0.31
0.47 0.39 0.53
0.39 0.37 0.45
0.078 0.2 0.27
0.059 0.27 0.33
0.45 0.43 0.41Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.27 0.16 0.2
0.3 0.19 0.24
0.25 0.15 0.2
0.3 0.18 0.22
0.27 0.17 0.22
0.31 0.19 0.38
0.33 0.18 0.24
0.75 0.37 0.44
0.75 0.37 0.44
1.1 0.52 0.41
6.2 8.1 6.8
0.39 0.24 0.32
0.39 0.24 0.32
0.53 0.31 0.48
0.53 0.31 0.48
0.42 0.23 0.27
0.42 0.23 0.27
1.4 1.2 6.8
3.9 4.1 5.3
0.45 0.33 0.4
0.45 0.33 2.1
0.75 0.51 0.49
0.75 0.51 0.49
0.65 0.46 1.6
0.53 0.35 0.33
1.7 0.81 0.42
1.7 0.8 6.6
0.55 0.34 0.44
0.55 0.34 0.45
5.7 7.3 6.5
0.48 0.27 0.29
0.48 0.4 0.52
0.48 0.4 0.52
0.4 0.24 0.29
0.4 0.24 0.29
1.2 1.2 6.3
1.2 1.2 3
4.3 4 1.3
4.3 4 1.3
6.5 7.8 6.9
4.8 4.6 7.1
6.3 8.6 7.1
6.3 7.4 7
1.4 1.3 0.94
1.4 1.3 0.94
4.4 4.4 1.2
6.3 8.4 7.4
1.9 1.4 0.79
1.8 1.3 2.5
2.7 2.4 3.5
2.7 2.4 6.4
3.7 4.2 7
3.7 4.2 7
6.3 8.3 6.8
4.8 5.3 5.9
2.2 1.5 1.5
2.2 1.5 4.8
5.2 4.7 6.7
3.8 3.9 5.7
1.1 0.88 1.7
0.95 0.89 0.6
3.8 4.3 4.8
3.4 3.1 3.7
6.1 6.7 5.9
6 5 7.1
6.1 8 7.3
2.5 2.3 6.4
0.75 0.56 0.46
0.75 0.56 0.46
4.4 3 7.2
4.4 3.1 7.2
5.6 6.3 6.8
2.8 1.8 7
2.2 2.5 6.4
2.2 2.5 6.4
1.8 2.1 0.73
1.8 2.1 3.4
6.7 8.3 6.8
6.5 8.2 6.8
6.5 8.2 7.3
6.8 8.1 7
5.7 7.1 5.8
5.7 7.1 5.8
4.5 4.9 6.7
4.5 4.9 4.1
4 3.6 5.7
3.7 3.4 3.2
5.6 6.1 7.4
4.9 4.5 5.9
6.4 6.5 7.3
6.3 7.1 6.8
5.1 6.2 6.9
4.9 5.6 6.8
6 6.7 5.6
6 6.7 7
6.4 8.1 7.4
6.3 8.4 7.4
5.6 6.6 6.7
6.5 8.4 7.2
4.4 4.7 6
5.9 6.2 7
6.4 8 6.5
6.4 8 7.3
6.3 7.8 7.1
6.1 6 7.3
6.2 7.2 7
6.2 6.6 7
6.4 7.8 5.5
6.4 7.8 6.6
6.4 7.8 7.1
6.4 7.9 5.4
6.4 7.9 6.8
6.3 8.6 7.3
6.3 8.6 7.2
6.5 7.5 6.8
6.7 8.6 7.2
6.7 8.7 7.4
6.9 9 7.1
6.4 8.3 6.6
6.4 8.3 6.7
7 8.5 7.1
6.9 9 7.1
6.6 8.3 7
6.6 8.3 7.1
5.8 6.1 6.3
6.8 8.6 7.4
6.9 8.4 7
6.7 8.8 7
6.4 7.5 6.8
6.4 7.5 6.8
6.8 9 6.8
6.8 8.9 7.3
6.6 8 7.1
6.5 8 7.1
6.6 8 7.3
6.8 8.8 7.2
6.5 8.2 7.1
6.4 8.5 7
6.8 8.2 6.9
7.1 8.5 7.2
7.1 8.5 7
6.8 8.2 7.1
7.1 8.5 7.1
6.4 8.5 7.2
6.9 9.1 7.2
6.7 8.4 7.1
6.9 9.1 7.2
6.7 8.4 7
6.6 8.6 6.9
6.6 8.6 6.8
6.9 9.1 7.4
6.9 9.1 7.4
7.1 8.5 7.1
6.9 9.1 7.4
6.6 8.5 7.4
6.7 8.4 7
6.6 8.6 7
6.9 9.1 6.9
6.9 9.1 7.2
7.1 8.5 6.9
7.1 8.5 7
7.1 8.5 7.2
6.6 8.6 6.9
6.6 8.6 6.9
6.9 9.1 7.4
6.9 9.1 7.4
6.6 8.6 6.9Loss Heatmap
20406080
123456789
Figure 34: Per-domain heat map for facescrub (second random seed). This shows the raw facescrub data
from Figure 21. Best viewed with zoom.
53Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)89 89 89
88 88 89
88 88 89
86 87 88
89 89 89
87 86 88
88 88 89
85 84 14
85 84 31
44 61 61
85 85 5.3
84 83 83
84 83 83
56 59 88
56 60 88
85 84 88
85 84 88
62 72 65
82 81 8
84 82 85
84 82 85
41 44 86
41 44 86
79 80 82
80 80 87
79 76 84
79 76 84
53 55 86
53 55 86
79 79 50
79 80 87
49 51 87
49 51 87
83 83 88
83 83 88
60 62 82
62 64 80
7.6 7.9 13
7.6 7.9 13
32 32 2.2
10 18 8.3
26 30 5
70 69 4
11 12 75
11 12 75
3.8 5 53
15 18 3.5
67 69 10
61 63 32
17 21 79
16 20 82
4.1 5.2 55
4.5 5.5 55
40 45 10
67 66 6.5
19 13 11
19 13 11
31 28 22
77 77 14
61 63 73
66 66 80
13 16 66
16 19 74
75 75 32
17 16 46
23 27 34
18 20 77
66 65 73
66 65 73
14 18 80
14 18 80
65 64 41
61 59 79
14 17 80
14 17 80
6.4 7.7 79
6.4 7.7 79
4.8 4.5 2.7
13 13 3.3
2 2.4 9.7
3.2 3.8 2.2
2.9 2.7 2.6
2.8 2.7 5.7
7.4 8.6 53
7.4 8.7 49
11 8.1 0.59
14 9.8 15
8.4 13 11
49 50 2.2
11 11 32
52 54 23
4.7 6 47
5.3 6.1 69
5.4 6.4 26
5.5 6.4 24
7.8 9.5 2.9
7.2 8.7 2.6
5.5 6.9 8
2.7 3.4 12
51 54 14
20 19 15
7.4 8.3 20
7.3 8.4 20
7.5 10 27
6.7 8.6 60
8.2 9.5 5.2
8.9 10 4.8
2.8 2.8 2.1
2.8 2.8 2.1
2.8 2.7 7.3
2.6 2.5 3.4
2.6 2.5 1.8
1.7 2 2.6
1.9 2.1 2.9
4.3 4.5 2.2
2.1 2.2 9.2
2.1 2.4 5.8
2.2 2.2 2.4
2.6 2.6 1.7
2.7 2.5 3
2.7 2.8 2
2.3 2.4 2.6
9.6 9.1 3.1
9.3 8.8 3
13 9.2 3.4
4.5 4 3.6
8.3 6.6 2.6
2.3 2.8 7.4
4.4 5.2 5
4.3 5.2 4.9
2 2.6 5.4
1.9 2.8 5.6
5.5 6.1 13
5.3 6.4 13
5.4 6.4 13
2.2 2.5 20
5.2 5.9 11
1.9 2.2 2.2
2.7 2.9 2.1
2.5 2.6 2.1
2.5 2.5 1.9
2.7 2.8 2.2
2.5 2.7 1.9
1.8 2.4 2.2
2.2 2.1 2.7
2.6 2.8 2.6
2 2.5 2.2
2.8 2.6 1.9
2.4 2.3 2.6
2.5 2.3 2.5
2.1 2.3 2.1
2.1 2.3 2.3
2.5 2.6 1.9
1.8 2.2 2.3
2.6 2.8 2.3
2.8 2.6 3.1
2.5 2.4 1.6
2 2.2 1.9
2.1 2.4 2.1
2.4 2.6 1.8
2.4 2.5 1.9
2.6 2.5 1.9
2.5 2.3 2.5
2.4 2.4 2.5
2 2.1 2.1
2 2.1 2.3
2.5 2.3 2.5Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.3 0.32 0.3
0.33 0.36 0.32
0.34 0.37 0.31
0.4 0.4 0.33
0.3 0.33 0.3
0.37 0.4 0.37
0.34 0.36 0.32
0.45 0.51 3.7
0.45 0.51 2.7
2 1.4 1.4
0.44 0.47 6.4
0.51 0.62 0.86
0.51 0.62 0.86
1.5 1.5 0.39
1.5 1.5 0.39
0.43 0.48 0.34
0.43 0.48 0.34
1.2 0.93 1.2
0.53 0.61 4.6
0.49 0.57 0.47
0.49 0.57 0.47
2.2 2.2 0.48
2.2 2.2 0.48
0.61 0.64 0.59
0.61 0.64 0.42
0.65 0.93 0.53
0.65 0.93 0.53
1.7 1.8 0.46
1.7 1.8 0.45
0.61 0.67 1.8
0.66 0.63 0.38
2 2 0.44
2 2 0.44
0.5 0.51 0.36
0.5 0.51 0.36
1.4 1.5 0.71
1.3 1.4 0.87
4.6 4 3.7
4.6 4 3.7
2.6 2.7 6.7
4 3.2 5.1
3 2.7 5.1
0.95 1.2 5.5
3.6 3.4 1.3
3.6 3.4 1.3
5.1 4 1.9
3.4 3.2 7.3
1 1.1 3.8
1.2 1.3 2.6
3.7 3.4 0.91
3.8 3.5 0.76
5.9 4.7 1.8
5.9 4.7 1.8
2.3 2.1 3.9
1.1 1.2 4.4
3.2 3.6 3.6
3.2 3.6 3.6
2.6 2.9 3.3
0.79 0.92 4
1.2 1.3 1
1.1 1.2 0.75
4.6 4.1 1.4
4.2 3.8 1.2
1.1 1.3 2.7
3.3 3.4 2
3.5 3.1 2.6
4.3 3.8 0.91
1.1 1.3 1.5
1.1 1.3 1.5
3.6 3.2 0.89
3.6 3.2 0.89
1.2 1.4 2.3
1.3 1.6 0.81
4.8 4.2 0.86
4.8 4.2 0.86
5 4.1 0.83
5 4.1 0.83
4.2 3.9 9.2
3.3 3.5 6.7
5.4 4.2 4.9
5 4.1 6.4
5.1 4.4 5.4
5.1 4.4 4.7
4.4 3.7 2.2
4.4 3.7 2.2
3.7 4 5.3
3.5 3.8 3.4
4.2 3.4 4.5
1.7 1.9 5.6
3.7 3.6 2.6
1.8 2 3
5.8 4.4 2.1
5.6 4.3 1.4
5.5 4.3 3
5.5 4.3 3
4.5 3.8 6.4
4.7 3.9 6.8
4.5 3.8 4.4
5 4 4
1.6 1.8 3.7
3.1 3.2 3.6
3.8 3.5 3.3
3.8 3.5 3.3
4.7 3.7 2.9
4.7 3.7 1.8
4.8 4 4.5
4.4 3.9 4.6
5.1 4.4 6.5
5.1 4.4 6.5
5.1 4.3 3.9
5.2 4.5 4.7
5.2 4.5 5.1
5.6 4.3 6.1
5.6 4.3 5.7
5.1 4.1 7.6
5.5 4.2 4.3
5.5 4.2 5.2
4.7 4 6.8
4.6 4.2 7.5
4.6 4.2 6.4
4.5 4.1 7
4.7 4 6.8
3.6 3.6 5.8
3.6 3.6 5.8
3.4 3.7 6.7
4.3 3.9 5.6
3.7 3.8 6.4
5.1 4 4.3
4.9 4 4.7
4.9 4 4.8
5.2 4 4.6
5.2 4 4.6
4.2 3.7 3.6
4.2 3.7 3.6
4.2 3.7 3.6
4.7 4 3.2
4.2 3.7 3.7
4.7 4 7.6
4.6 4.1 6.4
4.6 4.1 5.9
4.6 4.1 6.7
4.6 4.1 5.7
4.6 4.1 5.9
4.7 4 6.2
4.7 4 5.6
4.5 4 5.8
4.7 4 6.5
4.6 4.1 6
4.7 4.2 6
4.7 4.2 6
4.7 4 6.5
4.7 4 6.6
4.6 4.1 5.9
4.7 4 6.6
4.5 4 7.3
4.6 4.1 5.4
4.7 4.2 6.2
4.7 4 9.5
4.7 4 9.5
4.6 4.1 6.7
4.6 4.1 6.7
4.6 4.1 6.7
4.7 4.2 6
4.7 4.2 6
4.7 4 6.6
4.7 4 6.6
4.7 4.2 6.1Loss Heatmap
1020304050607080
2468
Figure 35: Per-domain heat map for emnist(third random seed). This shows the raw emnistdata from
Figure 25. Best viewed with zoom.
54Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)91 91 91
86 87 82
91 91 92
84 86 90
90 89 89
87 88 82
88 89 88
59 57 52
59 56 65
41 48 83
11 11 11
84 76 84
84 76 84
77 75 79
77 75 79
85 87 85
85 87 85
40 36 61
23 26 19
64 62 66
64 62 66
59 61 71
59 61 71
77 77 67
80 82 76
55 62 87
55 61 87
53 61 81
53 61 80
34 23 15
76 80 85
69 67 77
69 67 77
83 81 83
83 81 83
70 68 76
70 69 76
27 29 42
27 29 42
10 11 12
20 22 19
11 10 11
11 11 12
51 40 69
51 40 69
20 24 70
11 11 11
48 45 48
45 44 49
48 48 71
48 48 71
19 18 49
19 18 49
18 15 13
19 22 17
27 23 43
27 23 43
13 12 38
17 15 12
41 40 46
51 49 59
37 36 55
46 46 61
12 11 11
18 19 70
19 15 14
38 42 73
65 57 75
65 57 75
23 25 74
23 26 74
21 17 15
39 45 80
42 36 68
42 36 68
25 23 53
25 23 53
6.9 6.7 14
11 10 12
16 18 29
10 11 12
11 9.7 23
11 9.8 32
24 20 57
25 20 57
19 16 24
18 15 29
21 21 29
19 17 24
13 13 56
11 11 12
15 14 36
19 17 45
23 24 35
23 24 35
10 11 12
11 11 11
12 12 12
8.2 8.7 33
15 13 13
11 11 32
11 11 11
11 11 11
14 13 13
16 17 64
13 14 16
13 15 15
8.9 8.6 18
8.9 8.6 18
8.8 8.6 19
9.3 8.4 22
9.4 8.5 18
10 10 12
10 10 12
14 14 14
16 18 29
16 17 27
6.8 5.9 11
9.9 9.3 12
9.8 8.9 12
9.7 9.3 8.9
6 5.9 11
11 10 11
11 10 12
11 9.9 13
6.3 6.9 11
10 9.3 9.8
8 8.6 27
12 12 11
12 11 11
7.8 8.7 26
7.9 8.6 26
11 11 11
11 11 11
11 10 11
7.9 8.2 42
11 11 11
10 9.9 11
9.7 9 9
10 9.1 9.1
9.9 9.5 8.8
9.4 9.3 9.5
10 9.4 9.5
10 9.9 10
6.2 6.2 10
10 10 12
6.3 6.1 11
9.6 9 11
9.5 8.5 11
9.3 8.5 11
6.6 5.9 10
6.7 6 11
10 9.4 9
6.5 5.9 9.9
11 9.9 11
9.9 9.1 12
9.3 8.7 11
6.5 6 12
6.7 5.9 12
10 9.3 9.1
9.9 9.4 9.2
9.9 9.4 8.9
9.7 8.8 11
9.7 8.4 11
6.9 6 9.9
6.2 5.9 9.9
9.5 8.6 11Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.38 0.31 0.41
0.59 0.46 0.76
0.39 0.31 0.35
0.69 0.49 0.42
0.43 0.39 0.46
0.54 0.4 0.74
0.5 0.4 0.51
2 1.9 2.4
2 1.9 1.7
3.1 2.3 0.77
6.6 5.4 7.6
0.7 0.91 0.7
0.7 0.91 0.7
1 0.9 0.87
1 0.9 0.87
0.64 0.48 0.64
0.64 0.48 0.64
3.1 3 1.8
5.1 3.4 4.6
1.7 1.6 1.6
1.7 1.6 1.6
1.9 1.5 1.3
1.9 1.5 1.3
1 0.83 1.4
0.84 0.67 1
2.1 1.5 0.58
2.1 1.5 0.57
2.1 1.5 0.81
2.2 1.5 0.82
3.8 3.4 5
1 0.7 0.6
1.3 1.2 1
1.3 1.2 1
0.74 0.69 0.72
0.74 0.69 0.72
1.3 1.2 1
1.2 1.2 1
4.4 3.3 2.8
4.4 3.3 2.8
7.4 5.8 5.9
4.4 3.5 5.1
8.1 5.2 5
7.4 5.6 5.9
2.3 2.5 1.4
2.3 2.5 1.4
4.8 3.6 1.3
6.8 5.5 6.1
2.7 2.5 2.6
3 2.5 2.4
2.7 2.1 1.3
2.7 2.1 1.3
5.3 4.3 2.3
5.3 4.3 2.4
5.5 4.2 5.1
5.5 3.6 4.4
4.3 3.8 3
4.3 3.8 3
5.9 4.8 3.3
5.9 4.1 5
3.1 2.6 2.6
2.4 2.2 1.9
3.5 2.7 2.2
2.6 2.2 1.8
6.3 5.4 8
4.8 3.8 1.4
5.3 4.1 5.6
3.1 2.4 1.1
1.6 1.8 1.1
1.6 1.8 1.1
4.4 3.4 1.1
4.4 3.4 1.1
5 3.9 4.7
3.1 2.4 0.86
3 2.7 1.5
3 2.7 1.5
4.3 3.6 2.3
4.3 3.6 2.3
5.6 4.3 5.4
6.1 5.5 6.2
4.7 3.7 3.4
7.5 5.8 9.5
5.8 4.6 4.6
5.8 4.6 3.7
4.5 3.8 2
4.5 3.8 2
4.9 4.2 4.3
5.1 4.3 3.7
4.4 3.5 3.7
5.2 4.6 3.9
5.3 4.2 2.1
7.1 5.5 5.6
5.7 4.3 3.6
5 4.1 2.9
4.9 3.6 3.6
4.9 3.6 3.7
7.5 5.5 5.7
6.8 5.6 5.7
6.8 4.4 5.3
6.6 5.2 3.4
6 4.2 4.7
6.1 4.9 3.4
6.6 5.5 6.6
6.6 5.5 6.6
6.1 4.3 6.2
5.2 4.1 1.6
6.8 4.2 5.8
6.6 4.2 6.2
6.3 4.8 5.1
6.3 4.8 5.1
6.3 4.8 5.3
6 4.7 4.8
6 4.7 4.9
8.4 5.3 6.6
8.3 5.3 6.5
6 5.1 6.7
4.7 3.7 3.5
4.7 3.7 3.5
5.6 4.5 6
6.3 5.4 5.7
6.3 5.3 6.1
7.4 5.8 8.3
5.6 4.5 6
6.1 5.5 5.6
6.1 5.5 5.6
5.8 5.2 4.8
5.6 4.3 5.6
7.3 5.9 5.3
6.7 5.2 4
6.8 4.5 6.3
6.9 4.5 6.3
6.6 5.3 3.9
6.6 5.3 4
7.3 5.6 6.3
7.3 5.6 6.3
7.3 5.6 6.3
5.9 4.6 2.9
6.6 5.6 6.5
8.3 5.3 7.9
7.3 5.1 8.8
7.4 5.9 9
7.4 5.9 9
7.2 5.1 8.9
7.4 5.9 8.7
8.2 5.2 9.3
5.6 4.5 6.7
7.3 5.7 7.1
5.6 4.5 6.7
6.3 5.2 5.8
6.2 5.4 6.9
6.2 5.4 7
5.6 4.5 6.6
5.6 4.5 6.6
7.4 5.9 9
5.5 4.5 6.9
6.2 5.5 7.2
6.3 5.2 6.1
6.2 5.4 6.5
5.5 4.5 6
5.5 4.5 6
7.4 5.9 8.8
7.4 5.9 8.7
7.4 5.9 8.8
6.3 5.4 7.1
6.3 5.4 7
5.5 4.5 6.9
5.5 4.5 6.9
6.3 5.4 7Loss Heatmap
102030405060708090
2468
Figure 36: Per-domain heat map for cifar- 10(third random seed). This shows the raw cifar- 10data from
Figure 25. Best viewed with zoom.
55Under review as submission to TMLR
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)96 97 96
95 96 95
96 97 96
95 96 96
96 96 96
94 96 92
94 96 96
88 91 72
88 91 73
85 87 92
0.94 0.2 0.76
93 95 94
93 95 94
90 94 89
90 94 89
93 95 95
93 95 95
70 71 43
27 23 13
92 92 59
92 92 59
84 87 84
84 87 84
88 90 65
90 92 93
80 84 93
81 84 1.5
91 93 91
90 93 91
4.8 0.66 1.6
92 94 94
91 92 89
91 92 89
93 95 94
93 95 94
73 70 3.1
73 71 35
19 16 36
19 16 36
0.92 0.33 0.51
11 15 8.4
0.31 0.2 0.33
1 0.39 0.8
74 75 80
74 75 80
25 32 75
0.53 0.2 0.27
65 64 51
66 72 30
48 50 28
48 49 2.1
27 31 22
27 31 22
0.64 0.21 0.55
13 9.5 5.3
61 52 7.7
61 52 7.7
15 20 0.39
25 28 4.7
79 80 38
82 77 36
19 18 10
32 31 23
2.3 1.3 4.5
8.5 21 0.29
0.94 0.59 0.27
52 50 2.8
87 86 90
87 86 90
39 46 0.29
39 46 0.25
6.7 2.6 0.82
61 64 0.49
55 48 2.4
55 48 2.4
59 54 25
59 54 25
0.35 0.62 3.5
0.35 0.29 0.45
0.7 0.64 2.3
0.55 0.23 0.37
2.1 0.98 0.72
2.1 1.1 0.78
18 14 1.1
18 15 18
24 16 2
30 23 3.7
3.4 4.6 2.7
14 13 4.3
2.5 6.6 0.2
1.2 0.8 1.1
6.7 4.6 0.9
9.6 7.1 0.84
2.5 0.78 2
2.4 0.68 0.49
0.45 0.21 0.31
0.33 0.25 0.23
3.7 3.4 1.4
1.2 1.3 0.31
17 19 4.3
5.1 6.7 0.35
0.55 0.29 1
0.61 0.31 0.53
1.2 0.29 0.29
6.8 7.1 0.33
1.6 0.84 0.7
1.9 1.7 0.41
0.49 0.41 0.55
0.49 0.41 0.55
0.49 0.43 0.33
0.49 0.35 0.43
0.49 0.35 0.47
0.25 0.23 0.27
0.29 0.23 0.2
0.59 0.25 0.57
0.53 0.41 0.29
0.55 0.41 0.64
0.25 0.21 0.31
0.51 0.27 0.49
0.57 0.25 0.51
0.27 0.16 0.27
0.27 0.21 0.29
0.18 0.18 0.29
0.25 0.23 0.27
3.5 4.1 2.4
0.31 0.31 0.25
0.39 0.35 0.45
0.72 0.43 0.25
0.86 0.61 0.37
0.86 0.66 0.41
0.51 0.47 1
0.57 0.55 0.25
0.33 0.27 0.25
0.45 0.27 0.33
0.49 0.25 0.23
0.37 0.21 0.27
0.41 0.23 0.31
0.43 0.25 0.31
0.39 0.29 0.33
0.33 0.16 0.2
0.21 0.25 0.31
0.33 0.18 0.29
0.21 0.23 0.21
0.27 0.29 0.33
0.29 0.21 0.31
0.21 0.23 0.23
0.27 0.21 0.27
0.35 0.29 0.21
0.31 0.21 0.33
0.25 0.23 0.27
0.25 0.23 0.31
0.29 0.23 0.31
0.27 0.33 0.23
0.31 0.21 0.25
0.18 0.2 0.23
0.29 0.31 0.29
0.21 0.21 0.2
0.27 0.23 0.49
0.27 0.2 0.41
0.2 0.27 0.12
0.23 0.25 0.21
0.29 0.25 0.31
0.31 0.23 0.27
0.25 0.23 0.21
0.27 0.21 0.27
0.27 0.25 0.29
0.2 0.29 0.45Accuracy Heatmap
ERM Contrastive Modular
Training ApproachCO
GB
ID
IM
IN
R90
SW
CO-GB
GB-CO
CO-IM
IM-CO
CO-IN
IN-CO
CO-R90
R90-CO
CO-SW
SW-CO
GB-IM
IM-GB
GB-IN
IN-GB
GB-R90
R90-GB
GB-SW
SW-GB
IM-IN
IN-IM
IM-R90
R90-IM
IM-SW
SW-IM
IN-R90
R90-IN
IN-SW
SW-IN
R90-SW
SW-R90
CO-GB-R90
R90-CO-GB
CO-IM-GB
GB-CO-IM
CO-IM-SW
IM-SW-CO
CO-IN-R90
R90-IN-CO
CO-R90-IM
IM-CO-R90
CO-SW-GB
GB-CO-SW
CO-SW-R90
R90-SW-CO
GB-IM-R90
R90-GB-IM
GB-IM-SW
SW-IM-GB
GB-IN-CO
IN-GB-CO
GB-IN-IM
IM-IN-GB
GB-IN-SW
SW-GB-IN
GB-SW-R90
SW-GB-R90
IM-IN-CO
IN-CO-IM
IM-R90-SW
R90-SW-IM
IN-CO-SW
IN-SW-CO
IN-IM-R90
R90-IN-IM
IN-IM-SW
IN-SW-IM
IN-R90-SW
R90-SW-IN
R90-GB-IN
R90-IN-GB
CO-GB-IM-IN
GB-IM-CO-IN
CO-GB-IM-R90
CO-IM-R90-GB
CO-IN-R90-GB
IN-R90-GB-CO
CO-IN-R90-SW
CO-SW-IN-R90
CO-IN-SW-GB
GB-CO-IN-SW
CO-SW-GB-IM
IM-CO-GB-SW
CO-SW-IN-IM
IN-IM-CO-SW
GB-R90-SW-IN
IN-R90-SW-GB
GB-SW-R90-CO
R90-GB-CO-SW
IM-CO-SW-R90
SW-IM-R90-CO
IM-GB-IN-R90
IN-GB-IM-R90
IM-GB-IN-SW
IN-SW-GB-IM
IM-R90-CO-IN
IN-R90-IM-CO
IM-R90-IN-SW
SW-IN-IM-R90
R90-IM-GB-SW
R90-IM-SW-GB
CO-GB-IN-SW-R90
CO-IN-GB-SW-R90
IN-GB-CO-R90-SW
SW-GB-IN-CO-R90
SW-IN-CO-R90-GB
GB-CO-IM-SW-R90
GB-R90-CO-IM-SW
IM-SW-GB-R90-CO
R90-CO-GB-SW-IM
SW-R90-CO-GB-IM
GB-CO-IN-R90-IM
IM-CO-IN-GB-R90
IM-GB-CO-R90-IN
R90-CO-IN-IM-GB
R90-GB-IN-CO-IM
GB-SW-IN-IM-CO
IN-GB-SW-IM-CO
IN-IM-CO-GB-SW
IN-SW-GB-CO-IM
SW-CO-IM-GB-IN
GB-SW-IN-R90-IM
IM-IN-SW-R90-GB
IM-SW-GB-R90-IN
SW-GB-IM-R90-IN
SW-IN-GB-IM-R90
IM-CO-R90-SW-IN
IM-R90-IN-CO-SW
IN-IM-R90-CO-SW
R90-SW-CO-IN-IM
R90-SW-IM-CO-IN
CO-GB-IM-IN-SW-R90
CO-IM-GB-SW-IN-R90
CO-IM-R90-IN-SW-GB
CO-IM-SW-IN-R90-GB
CO-IN-IM-GB-R90-SW
CO-IN-R90-SW-IM-GB
GB-CO-IN-IM-R90-SW
GB-CO-R90-SW-IN-IM
GB-IM-IN-R90-CO-SW
GB-SW-R90-CO-IN-IM
IM-IN-R90-CO-GB-SW
IM-SW-R90-GB-CO-IN
IM-SW-R90-IN-GB-CO
IN-GB-SW-CO-R90-IM
IN-GB-SW-R90-CO-IM
IN-R90-CO-IM-SW-GB
IN-SW-GB-CO-IM-R90
IN-SW-GB-IM-CO-R90
R90-IM-GB-IN-SW-CO
R90-IM-IN-SW-CO-GB
SW-CO-GB-IM-R90-IN
SW-CO-GB-R90-IN-IM
SW-CO-IM-GB-IN-R90
SW-CO-IM-R90-IN-GB
SW-CO-IN-IM-R90-GB
SW-IM-GB-R90-IN-CO
SW-IM-R90-GB-CO-IN
SW-IN-GB-CO-R90-IM
SW-R90-IN-GB-CO-IM
SW-R90-IN-IM-GB-COT est Corruption(s)0.26 0.16 0.2
0.3 0.18 0.25
0.24 0.14 0.19
0.28 0.17 0.21
0.26 0.16 0.22
0.32 0.18 0.38
0.32 0.18 0.22
0.64 0.39 1.3
0.64 0.39 1.3
0.84 0.57 0.38
6.2 8.8 6.4
0.37 0.25 0.33
0.37 0.25 0.33
0.52 0.27 0.5
0.52 0.27 0.5
0.4 0.21 0.25
0.4 0.21 0.25
1.6 1.3 2.7
3.9 4 4.6
0.44 0.36 1.9
0.44 0.36 1.9
0.82 0.56 0.74
0.82 0.56 0.74
0.65 0.44 1.6
0.52 0.35 0.37
1.1 0.72 0.38
1.1 0.73 6.2
0.52 0.31 0.45
0.52 0.31 0.45
5.7 7.9 6
0.45 0.26 0.26
0.5 0.34 0.54
0.5 0.34 0.54
0.39 0.23 0.28
0.39 0.23 0.28
1.4 1.3 5.9
1.4 1.2 3.1
4.3 4.6 3.1
4.3 4.6 3.1
6.3 8.5 6.6
5 4.9 5.1
6.2 8.7 6.7
6.3 8.6 6.3
1.3 1.1 0.95
1.3 1.1 0.95
4 3.6 1.2
6.3 8.9 7.1
1.8 1.6 2.3
1.7 1.2 3.4
2.6 2.2 3.6
2.6 2.3 6.2
3.8 3.5 4
3.8 3.5 3.9
6.4 8.8 6.5
4.8 5.5 5.5
2 2.2 5.2
2 2.2 5.2
4.7 4.5 6.6
4 3.7 5.6
1.1 0.9 3
0.95 1 3.1
4.3 4.3 4.9
3.5 3.4 3.8
6 7.5 5.6
5.4 4.6 6.8
6.3 8.2 6.7
2.4 2.3 6
0.69 0.6 0.48
0.69 0.6 0.48
3.1 2.6 7.2
3.1 2.7 7.2
5.4 6.9 6.2
2 1.7 6.5
2.3 2.4 6
2.3 2.4 6
2.1 2.2 3.9
2.1 2.2 3.9
7.1 9 5.8
6.4 8.8 6.8
7.1 8.4 6.2
6.5 8.6 6.9
6.3 7.9 6.4
6.3 7.9 6.4
4.3 4.8 6.5
4.3 4.7 4.3
3.9 4.7 6
3.6 4.2 5.7
6 6.5 5.9
4.7 5 5.7
6.1 6.5 6.7
6.3 7.9 6.3
5.3 6 6.5
5.1 5.7 6.5
5.9 7.1 6.2
5.9 7.1 6.9
6.5 8.8 7
6.4 8.9 7
5.9 7 6.5
6.6 8 6.9
4.5 4.7 5.7
5.7 6.1 6.7
6.4 8.7 6.5
6.4 8.7 6.8
6.3 8.2 6.6
5.4 5.9 7.2
6.1 7.5 6.6
6.2 7.1 6.7
6.7 8.4 6.6
6.7 8.4 6.6
6.8 8.4 6.7
7.1 8.9 6.7
7.1 8.9 6.7
6.2 8.7 6.9
6.2 8.7 6.6
6.7 8.1 6.6
7.3 8.8 6.8
7.3 9 6.6
7.5 10 6.7
7.1 9.4 6.7
7.1 9.4 6.7
6.7 9.3 6.9
7.5 10 6.7
6.5 8.9 6.8
6.5 8.9 6.8
5.8 6.8 6
7.2 9.5 6.6
6.6 9.1 6.8
6.9 8.5 7.1
6.5 8.1 6.8
6.5 8.1 6.8
6.9 8.6 6.5
6.9 8.6 7.1
6.6 8.9 6.8
6.6 8.9 6.8
6.6 8.9 6.8
7.2 9.8 6.8
6.6 8.9 6.7
6.4 8.9 7.2
6.4 9.2 6.9
6.7 9.4 7
6.7 9.4 6.9
6.4 9.2 7
6.7 9.4 7
6.4 8.9 6.6
7.5 10 6.6
6.7 9.2 7.1
7.5 10 6.7
6.9 8.9 6.9
7.3 9.7 6.8
7.3 9.7 6.8
7.6 10 6.7
7.5 10 6.7
6.7 9.4 7
7.5 10 6.7
6.6 9.1 7.3
6.9 9 6.9
7.3 9.7 6.9
7.5 10 6.7
7.5 10 6.7
6.7 9.4 6.9
6.7 9.4 6.9
6.7 9.4 6.9
7.3 9.6 6.8
7.3 9.6 6.8
7.5 10 6.7
7.5 10 6.7
7.3 9.6 6.6Loss Heatmap
20406080
246810
Figure 37: Per-domain heat map for facescrub (third random seed). This shows the raw facescrub data
from Figure 25. Best viewed with zoom.
56