Published in Transactions on Machine Learning Research (02/2024)
The Fair Value of Data Under Heterogeneous Privacy
Constraints in Federated Learning
Justin S. Kang justin_kang@berkeley.edu
UC Berkeley
Ramtin Pedarsani ramtin@ece.ucsb.edu
UC Santa Barbara
Kannan Ramchandran kannanr@berkeley.edu
UC Berkeley
Reviewed on OpenReview: https: // openreview. net/ forum? id= ynG5Ak7n7Q
Abstract
Modern data aggregation often involves a platform collecting data from a network of users
with various privacy options. Platforms must solve the problem of how to allocate incentives
to users to convince them to share their data. This paper puts forth an idea for a fair
amount to compensate users for their data at a given privacy level based on an axiomatic
definition of fairness, along the lines of the celebrated Shapley value. To the best of our
knowledge, these are the first fairness concepts for data that explicitly consider privacy
constraints. We also formulate a heterogeneous federated learning problem for the platform
with privacy level options for users. By studying this problem, we investigate the amount
of compensation users receive under fair allocations with different privacy levels, amounts
of data, and degrees of heterogeneity. We also discuss what happens when the platform
is forced to design fair incentives. Under certain conditions we find that when privacy
sensitivity is low, the platform will set incentives to ensure that it collects all the data with
the lowest privacy options. When the privacy sensitivity is above a given threshold, the
platform will provide no incentives to users. Between these two extremes, the platform will
set the incentives so some fraction of the users chooses the higher privacy option and the
others chooses the lower privacy option.
1 Introduction
Frommediatohealthcaretotransportation, thevastamountofdatageneratedbypeopleeverydayhassolved
difficult problems across many domains. Nearly all machine learning algorithms, including those based on
deeplearningrelyheavilyondataandmanyofthelargestcompaniestoeverexistcentertheirbusinessaround
this precious resource of data. This includes directly selling access to data to others for profit, selling targeted
advertisements based on data, or by exploiting data through data-driven engineering, to better develop and
market products. Simultaneously, as users become more privacy conscious, online platforms are increasingly
providing privacy level options for users. Platforms may provide incentives to users to influence their privacy
decisions. This manuscript investigates how platforms can fairly compensate users for their data contribution
at a given privacy level. Consider a platform offering geo-location services with three privacy level options:
i) Users send no data to the platform — all data processing is local and private.
ii) An intermediate option with federated learning (FL) for privacy. Data remains with the users, but
the platform can ask for gradients with respect to a particular loss function.
iii) A non-private option, where all user data is stored and owned by the platform.
1Published in Transactions on Machine Learning Research (02/2024)
If users choose option (i), the platform does not stand to gain from using that data in other tasks. If the user
chooses (ii), the platform is better off, but still has limited access to the data via FL and may not be able to
fullyleverageitspotential. Therefore, theplatformwantstoincentivizeuserstochooseoption(iii). Thismay
be done by providing services, discounts or money to users that choose this option. Effectively, by choosing
an option, users are informally selling (or not selling) their data to platforms. Due to the lack of a formal
exchange, it can be difficult to understand if this sale of user data is fair. Are platforms making the cost of
choosingprivateoptionslike(i)or(ii)toohigh? Isthevalueofdatamuchhigherthantheplatformispaying?
A major shortcoming of the current understanding of data value is that it often fails to explicitly consider a
critical factor in an individual’s decision to share data—privacy. This work puts forth two rigorous notions
of the fair value of data in Section 3 that explicitly include privacy and make use of the axiomatic framework
of theShapley value from game theory (Shapley, 1952).
Figure 1: Depiction of interactions between plat-
formandusers. Usersgeneratedatawithphones,
cameras, vehicles, and drones. This data goes to
the platform but requires some level of privacy.
The platform uses this data to generate utility,
often by using the data for learning tasks. In re-
turn, the platform may provide the users with
payments in the form of access to services, dis-
counts on products, or monetary compensation.Compelledbytheimportanceofdatainourmodernecon-
omy and a growing social concern about privacy, this pa-
per presents frameworks for quantifying the fair value of
private data. Specifically, we consider a setting where
users are willing to provide their data to a platform in
exchange for some sort of payment and under some pri-
vacy guarantees depending on their level of privacy re-
quirements. The platform is responsible for running the
private learning algorithm on the gathered data and mak-
ing the fair payments with the objective of maximizing its
utility including statistical accuracy and total amount of
payments. Our goal is to understand fair mechanisms for
this procedure as depicted in Fig. 1.
1.1 Related Work
Economics With the widespread use of the internet,
interactions involving those that have data and those that
want it have become an important area of study (Balazin-
ska et al., 2011), and a practical necessity (Spiekermann
et al., 2015b). Among these interactions, the economics
of data from privacy conscious users has received signifi-
cantattentioninAcquistietal.(2016)andWieringaetal.
(2021). The economic and social implications of privacy
and data markets are considered in Spiekermann et al.
(2015a). In Acemoglu et al. (2019) the impact of data ex-
ternalities is investigated. The leakage of data leading to
the suppression of its market value is considered.
Privacy Currently, popular forms of privacy include federated learning (Kairouz et al., 2021) and differ-
ential privacy (DP) (Dwork, 2008; Bun & Steinke, 2016) either independently or in conjunction with one an-
other. Our work uses a flexible framework that allows for a rage of different privacy models to be considered.
OptimalDataAcquisition Onelineofliteraturestudies data acquisition , whereplatformsattempttocol-
lectdatafromprivacyconscioususers. Ghosh&Ligett(2013)considerthecaseofuniformprivacyguarantees
(homogeneousDP),whereusershaveuniqueminimumprivacyconstraints,focusingoncharacterizingequilib-
ria. Ghosh&Roth(2011)allowsforheterogeneousDPguaranteeswiththegoaltodesignadominantstrategy
truthful mechanism to acquire data and estimate the sum of users’ binary data. In Fallah et al. (2022) the au-
thors consider an optimal data acquisition problem in the context of private mean estimation in two different
local and central heterogeneous DP settings. It is assumed that players care about both the estimation error
of the common estimator generated by the platform and any payments made to them by the platform in their
decisionmaking. Byassuminglinearprivacysensitivityrepresentedbyscalarsanddrawnfromadistribution,
2Published in Transactions on Machine Learning Research (02/2024)
they devise a mechanism for computing the near-Bayes optimal privacy levels to provide to the players. Cum-
mings et al. (2023) focuses on the central setting, under both the linear privacy sensitivity and the privacy
constraints model, offering insights into the optimal solution. Hu & Gong (2020) goes beyond linear estima-
tion to consider FL, where each user has a unique privacy sensitivity function parameterized by a scalar vari-
able. Users choose their privacy level, and the platform pays them via a proportional scheme. For linear pri-
vacy sensitivity functions, an efficient way to compute the Nash equilibrium is derived. Roth & Schoenebeck
(2012); Chen et al. (2018); Chen & Zheng (2019) also follow Ghosh & Roth (2011) and design randomized
mechanisms that use user data with a probability that depends on their reported privacy sensitivity value.
Fairness In Jia et al. (2019), Ghorbani & Zou (2019) and Ghorbani et al. (2020) a framework for deter-
mining the fair value of data is proposed. These works extend the foundational principles of the Shapley
value (Shapley, 1952), which was originally proposed as a concept for utility division in coalitional games to
the setting of data. Our work takes this idea further and explicitly includes privacy in the definition of the
fair value of data, ultimately allowing us to consider private data acquisition in the context of fairness con-
straints. Finally, we note that we consider the concept of fairness in data valuation, not algorithmic fairness,
which relates to the systematic failure of machine learning systems to account for data imbalances.
1.2 Main Contributions
•We present an axiomatic notion of fairness that is inclusive of the platforms and the users in Theo-
rem 1. The utility to be awarded to each user and the platform is uniquely determined, providing a
useful benchmark for comparison.
•In the realistic scenario that fairness is considered between users, Theorem 2 defines a notion of
fairness based on axioms, but only places restriction on relative amounts distributed to the players.
This creates an opportunity for the platform to optimize utility under fairness constraints.
•Section 4 contains an example inspired by online platform advertisement to heterogeneous users. We
use our framework to fairly allocate payments, noticing how those payments differ among different
types of users, and how payments change as the degree of heterogeneity increases or decreases. We
numerically investigate the mechanism design problem under this example and see how heterogeneity
impacts the optimal behavior of the platform.
•Finally, Section 5 explores the platform mechanism design problem. In Theorem 3 we establish that
there are three distinct regimes in which the platform’s optimal behavior differs depending on the
common privacy sensitivity of the users. While existing literature has investigated how a platform
should design incentives for users to optimize its utility, this is the first work to consider fairness
constraintsontheplatform. Whenprivacysensitivityislow, theplatformwillsetincentivestoensure
that it collects all the data with the lowest privacy options. When the privacy sensitivity is above a
given threshold, the platform will provide no incentives to users. Between these two extremes, the
platform will set the incentives so some fraction of the users chooses the higher privacy option and
the remaining chooses the lower privacy option.
Notation Lowercase boldface xand uppercase boldface Xsymbols denote vectors and matrices respec-
tively. X⊙Yrepresents the element-wise product of XandY. We use R≥0for non-negative reals. Finally,
x≥ymeans that xi≥yi∀i. For a reference list of all symbols and their meaning, see Appendix A.
2 PROBLEM SETTING
2.1 Privacy Levels and Utility Functions
Definition 1. Aheterogeneous privacy framework on the space of random function A:XN→Yis:
1. A set of privacy levelsE⊆R≥0∪{∞}, representing the amount of privacy of each user. We use ρ
to represent an element of Ein the general case and ϵwhen the privacy levels are referring to DP
parameters (defined below).
3Published in Transactions on Machine Learning Research (02/2024)
2. A constraint set A(ρ)⊆{A:XN→Y}, representing the set of random functions that respect the
privacy levels ρi∈Efor alli∈[N]. If a function A∈A(ρ)then we call it a ρ-private algorithm.
We maintain this general notion of privacy framework because different notions of privacy can be useful in
differentsituations. Forexample, thelackofrigorassociatedwithnotionssuchasFL,maymakeitunsuitable
for high security applications, but it may be very useful in protecting users against data breaches on servers,
by keeping their data local. One popular choice with rigorous guarantees is DP:
Definition 2. Pure heterogeneous ϵ-DP, is a heterogeneous privacy framework with E=R≥0∪{∞}and
the constraint set A(ϵ) ={A: Pr(A(x)∈S)≤eϵiPr(A(x′)∈S)}for all measurable sets S.
Henceforth we will use the symbol ϵto represent privacy level when we are specifically referring
to DP as our privacy framework, but if we are referring to a general privacy level, we will use
ρ. Fig. 2, depicts another heterogeneous privacy framework. ρi= 0means the user will keep
their data fully private, ρi= 1is an intermediate privacy option where user data is securely ag-
gregated with other users before it is sent to the platform, which obfuscates it from the plat-
form. Finally, if ρi= 2, the users send a sufficient statistic for their data to the platform.
Figure 2: Users choose between three levels of
privacy. If ρi= 0, users send no data to the
platform. If ρi= 1, a user’s model is securely
combined with other users who also choose ρi=
1, and the platform receives only the combined
model. Ifρi= 2, users send their relevant infor-
mation directly to the platform.The platform applies an ρ-private algorithm Aρ:XN∝⇕⊣√∫⊔≀→
Yto process the data, providing privacy level ρito data
xi. The output of the algorithm y=Aρ(x)is used by
the platform to derive utility U, which depends on the
privacy level ρ.
For example, if the platform is estimating the mean of a
population, the utility could depend on the mean square
error of the private estimator.
Differencesfrompriorwork Thisformulationdiffers
from the literature of optimal data acquisition (i.e., Fal-
lah et al. (2022)), where privacy sensitivity is reported by
users, and the platform chooses privacy levels ρibased on
this sensitivity. Privacy sensitivity is the cost that a user
experiences by choosing a particular privacy level. Their
formulation allows for a relatively straightforward appli-
cation of notions like incentive compatibility and individ-
ual rationality from mechanism design theory. In this
work, we instead emphasize that users choose a privacy
level, rather than report a somewhat nebulously defined
privacy sensitivity. Despite this difference, the notions of
fairness described in the following section can be applied
more broadly.
2.2 The Data Acquisition Problem
The platform generates transferable and divisible utility U(ρ)from the user data. In exchange, distributes
a portion of the utility ti(ρi;ρ−i)to useri, where ρ−idenotes the vector of privacy levels ρwith theith
coordinate deleted. These incentives motivate users to lower their privacy level, but each user will also have
some sensitivity to their data being shared, modelled by a sensitivity function ci:E→ [0,∞),ci(0) = 0.
The behavior of users can be modelled with the help of a utility function:
ui(ρ) =ti(ρi,ρ−i)−ci(ρi). (1)
The payment to user iwill tend to increase with a lower privacy level, as the platform can better exploit
the data, but their sensitivity ciwill increase with ρi, creating a trade-off. By specifying ti(ρi;ρ−i), the
platform effectively creates a game among the users. This situation is depicted in Fig. 3. Each user’s action
is the level of privacy that they request for the data they share. Users (players) select their privacy level ρi
4Published in Transactions on Machine Learning Research (02/2024)
Figure 3: Users send their data xiand a privacy level ρito the central platform in exchange for payments
ti(ρi;ρ−i). The central platform extracts utility from the data at a given privacy level and optimizes
incentives to maximize the difference between the utility and the sum of payments U(ρ)−/summationtext
iti(ρ).
by considering their utility function uiand the potential actions of the other players. The platform’s goal is
to design the payments ti(ρi;ρ−i)that maximize its net utility U(ρ)−1Tt(ρ). One way to formulate this
problem is to consider maximizing this difference at equilibrium points:
maximize
t(·),ρU(ρ)−1Tt(ρ)
subject to ρ∈NE(t).(2)
NE(t)denotes the set of NE strategies induced by the payment function t, which is the vector with payment
functiontiat indexi. Recall that the NE is a stable state such that no user gains by unilaterally changing
their strategy. Depending on the circumstances, we may also want to consider equilibrium points in mixed
strategies (distributions) over the privacy space. This could make sense if we expect users to continually
interact with the platform, making a different choice each time, such that users ultimately converge to their
long-run average payoff. In such cases, we can formulate the problem for the platform as:
maximize
t(·),PU(P)−1Tt(P)
subject toP∈NE(t).(3)
where we have used the shorthand f(P) =Eρ∼P[f(ρ)]andPrepresents a distribution over the privacy
spaceE. Note that in both equation 3 and equation 2 restrictions must be placed on t, otherwise it can be
made arbitrarily negative. Individual rationality is a common condition in mechanism design that says that
a user can be made no worse off by participation. In Section 5, we consider a fairness constraint.
2.3 Model Limitations
Known sensitivity functions To solve equation 3, the platform requires the privacy sensitivity ciof each
user, and our solution in Section 5 depends on this information. This can be justified when platforms interact
with businesses. For example, an AI heath platform may interact with insurance companies and hospitals
and can invest significant resources into studying each of its partners. Another example is advertisement
platforms and sellers. Another justification is that the privacy sensitivity ciis learned by the platforms over
time, and we are operating in a regime where the estimates of cihave converged. An interesting future
direction could be investigating this learning problem.
Data-correlated sensitivity In Section 5 we treat the sensitivity function cias fixed and known, but a
practical concern is that cimay depends on the data xi. Sayxiis biological data pertaining to a disease.
Those users with the diseases may have higher ci. Without taking this into account, the collected data will
be biased. If our utility function is greatly increased by those users who do have the disease though, they
may receive far more payment, compensating for this correlation. We leave a investigation of data-correlated
sensitivity and fairness to future work.
5Published in Transactions on Machine Learning Research (02/2024)
Known transferable and divisible utility Solving equation 3 also requires knowledge of the utility
function. In some cases, the platform may dictate the utility entirely on its own, perhaps to value a diverse
set of users. In other cases, like in the estimation setting of Example 3.2, it may represent a more concrete
metric, like a risk function that is easily computed. In some cases, however, the utility function may not be
easily computed. For example, it may depend on the revenue of a company’s product, or the downstream
performance of a deep network. We also note that ti(ρi;ρ−i)may not represent a monetary transfer.
Individuals are often compensated for data via discounts or access to services. A shortcoming of our model is
that we assume a divisible and transferable utility, which may fail to capture these nuances of compensation.
Informed and Strategic Users We also assume that users can compute and play their equilibrium
strategy, which is a standard assumption in game theory. Practically this also means that the platform must
be transparent about the incentives, fully publishing this information to the users.
3 Axiomatic Fairness with Privacy
What is a fair way to distribute incentives? One approach is to view the users and platforms as a coalition
jointlygeneratingutility. Followinganaxiomaticapproachtofairness, thecelebratedShapleyvalue(Shapley,
1952) describes how to fairly divide utility among a coalition. In this section, we take a similar approach to
defining fairness. This coalitional perspective is not a complete characterization of the complex dynamics
between users and platforms, but we argue that it is still a useful one. One of the benefits of this concept
of fairness is that it deals with intrinsic value (i.e., how much of the utility comes from the data). This is
in contrast to the market value that users are willing to sell for (potentially depressed). This information is
particularly useful to economists, regulators, and investors, who are interested in characterizing the value of
data as capital for the purposes of analysis, taxation, and investment respectively.
3.1 Platform as a Coalition Member
We define a coalition of users and a platform as a collection of susers, with 0≤s≤Nand up to 1platform.
Letz∈{0,1}represent the action of the platform. Let z= 1when the platform chooses to join the coalition,
andz= 0otherwise. Let U(ρ)be as defined in Section 2. We augment the utility to take into account that
the utility is zero if the platform does not participate, and define ρSas follows:
U(z,ρ):=/braceleftigg
U(ρ)z= 1
0z= 0,[ρS]i:=/braceleftigg
ρii∈S
0else. (4)
Letϕp(z,ρ)andϕi(z,ρ),i∈[N]represent the “fair” amount of utility awarded to the platform and each
userirespectively, given zandρ, otherwise described as the “value” of a user. Note that these values depend
implicitly on both the private algorithm Aρand the utility function U, but for brevity, we avoid writing this
dependence explicitly. The result of Hart & Mas-Colell (1989) show that these values are unique and well
defined if they satisfy the following three axioms:
A.i)Two equally contributing users should be paid equally . For anyi,j∈[N] :U(z,ρS∪{i}) =
U(z,ρS∪{j})∀S⊂[N]\{i,j}=⇒ϕi(z,ρ) =ϕj(z,ρ).
In addition, for any user i∈[N],U(1,ρS∪{i})−U(1,ρS) = 0∀S⊂[N]\{i}=⇒ϕi(z,ρ) = 0.
A.ii)The sum of all payments is the total utility . The sum of values is the total utility U(z,ρ) =
ϕp(z,ρ) +/summationtext
iϕi(z,ρ).
A.iii)If two utility functions are combined, the payment for the combined task should be the
sum of the individual tasks . Letϕp(z,ρ)andϕi(z,ρ)be the value of the platform and users
respectively for the utility function U, under the ρ-privateAρ. LetVbe a separate utility function,
also based on the output of Aρ, and letϕ′
p(z,ρ)andϕ′
i(z,ρ)be the utility of the platform and
individuals with respect to V. Then under the utility U+V, the value of user iisϕi(z,ρ) +ϕ′
i(z,ρ)
and the value of the platform is ϕp(z,ρ) +ϕ′
p(z,ρ).
6Published in Transactions on Machine Learning Research (02/2024)
Theorem 1. Letϕp(z,ϵ)andϕi(z,ϵ)satisfying axioms (A.i-iii)represent the portion of total utility awarded
to the platform and each user ifrom utility U(z,ϵ). Then they are unique and take the form:
ϕp(z,ρ) =1
N+ 1/summationdisplay
S⊆[N]1/parenleftbigN
|S|/parenrightbigU(z,ρS), (5)
ϕi(z,ρ) =1
N+ 1/summationdisplay
S⊆[N]\{i}1/parenleftbigN
|S|+1/parenrightbig/parenleftbig
U(z,ρS∪{i})−U(z,ρS)/parenrightbig
. (6)
Theorem 1 is proved in Appendix B.2, and resembles the classic Shapley value result (Shapley, 1952).
3.2 Fairness Among Users
Though we can view the interactions between the platform and the users as a coalition, due to the asymmetry
that exists between the platform and the users, it also makes sense to discuss fairness among the users alone.
In this case, we can consider an analogous set of axioms that involve only the users.
B.i)Two equally contributing users should be paid equally . For anyi,j∈[N] :U(ρS∪{i}) =
U(ρS∪{j})∀S⊂[N]\{i,j}=⇒ϕi(ρ) =ϕj(ρ).
In addition, for any user i∈[N],U(ρS∪{i})−U(ρS) = 0∀S⊂[N]\{i}=⇒ϕi(ρ) = 0.
B.ii)The sum of all payments is an α(ϵ)fraction of the total utility . The sum of values is the
total utility α(ρ)U(ρ) =/summationtext
iϕi(ρ). Where ifU(ρ) =U(˜ρ)thenα(ρ) =α(˜ρ)and0≤α(ρ)≤1.
B.iii)If two utility functions are combined, the payment for the combined task should be the
sum of the individual tasks . Letϕi(ρ)be the value of users for the utility function U, under
theϵ-private algorithm Aρ. LetVbe a separate utility function, also based on the output of the
algorithmAϵ, and letϕ′
i(ρ)be the utility of the users with respect to V. Then under the utility
U+V, the value of user iisϕi(ρ) +ϕ′
i(ρ).
Anotabledifferencebetweentheseaxiomsand(A.i-iii)isthattheefficiencyconditionisreplacedwithpseudo-
efficiency. Under this condition, the platform may determine the sum of payments awarded to the players,
but this sum should in general depend only on the utility itself, and not on how that utility is achieved.
Theorem 2. Letϕi(ρ)satisfying axioms (B.i-iii)represent the portion of total utility awarded to each user
ifrom utility U(ρ). Then for α(ρ)that satisfies axiom (B.ii)ϕitakes the form:
ϕi(ρ) =α(ρ)
N/summationdisplay
S⊆[N]\{i}1/parenleftbigN−1
|S|/parenrightbig/parenleftbig
U(ρS∪{i})−U(ρS)/parenrightbig
. (7)
The proof of Theorem 2 can be found in Appendix B.2. This result is similar to the classic Shapley value
(Shapley, 1952), but differs in its novel asymmetric treatment of the platform.
Computational Complexity At first glance it may seem that both notions of fairness have exponential
computational complexity of N|E|N. This is only true for a worst-case exact computation. In order for
these notions to be useful in any meaningful way, we must be able to compute them. Thankfully, in practice,
Utypically has a structure that makes the problem more tractable. In Ghorbani & Zou (2019), Jia et al.
(2019), Wang & Jia (2023) and Lundberg & Lee (2017) special structures are used to compute the types
of Shapley value sums we are considering with significantly reduced complexity, particularly in cases where
theUis related to the accuracy of a deep network. This is critical because we want to compute fair values
for large number of users. For example, our platform could be a medical data network with hundreds of
hospitals as our users, or a smartphone company with millions of users, and we need to be able to scale
computation to accurately compute these fair values.
7Published in Transactions on Machine Learning Research (02/2024)
Example: Differentially Private Estimation In this example, we use DP as our heterogeneous privacy
framework. Let Xirepresentindependentandidenticallydistributeddataofuser irespectively, withPr (Xi=
1/2) =pand Pr (Xi=−1/2) = 1−p, withp∼Unif(0,1). The platform’s goal is to construct an ϵ-DP
estimator for µ:=E[Xi] =p−1/2that minimizes Bayes risk. There is no general procedure for finding the
Bayes optimal ϵ-DP estimator, so restrict our attention to ϵ-DP linear-Laplace estimators of the form:
A(X) =w(ϵ)TX+Z, (8)
whereZ∼Laplace (1/η(ϵ)). In Fallah et al. (2022) the authors argue that unbiased linear estimators
are nearly optimal in a minimax sense for bounded random variables. We assume a squared error loss
L(a,µ) = (a−µ)2and letAlin(ϵ)be the set of ϵ-DP estimators satisfying equation 8. Then, we define:
Aϵ= arg min
A∈Alin(ϵ)E[L(A(X),µ)]r(ϵ) =E[L(Aϵ(X),µ)]. (9)
In words,Aϵis an ϵ-DP estimator of the form equation 8, where w(ϵ)andη(ϵ)are chosen to minimize the
Bayes risk of the estimator, and r(ϵ)is the risk achieved by Aϵ. Since the platform’s goal is to accurately
estimate the mean of the data, it is natural for the utility U(ϵ)to depend on ϵthrough the risk function
r(ϵ). Note that if Uis monotone decreasing in r(ϵ), thenUis monotone increasing in ϵ. Let us now consider
the case of N= 2users, choosing from an action space of E={0,ϵ′}, for someϵ′>0. Furthermore, take U
to be an affine function of r(ϵ):U(ϵ) =c1r(ϵ) +c2. For concreteness, take U(0) = 0andsupϵ∈RU(ϵ) = 1.
Note that this ensures that Uis monotone increasing in ϵ, and is uniquely defined. Considering the example
of a binary privacy space E={0,∞}(ϵ′=∞), the utility can be written in matrix form as:
U=/bracketleftigg
U([0,0])U([0,ϵ′])
U([ϵ′,0])U([ϵ′,ϵ′])/bracketrightigg
=/bracketleftigg
0 2/3
2/3 1/bracketrightigg
. (10)
Derivations are available in Appendix B.1. Note from equation 5 and equation 6, it is clear that ϕp(0,ϵ) =
ϕi(0,ϵ) = 0. Let ΦpandΦ(1)
irepresent the functions ϕp(1,ϵ)andϕi(1,ϵ)in matrix form akin to U. Then
using equation 5 and equation 6, we find that the fair allocations of the utility are given by:
Φp=/bracketleftigg
0 1/3
1/3 5/9/bracketrightigg
,Φ(1)
1=/bracketleftigg
0 1/3
0 2/9/bracketrightigg
,Φ(1)
2=/bracketleftigg
0 0
1/3 2/9/bracketrightigg
. (11)
Consider the utility function defined in equation 10, for the N= 2user mean estimation problem with
E={0,∞}. By Theorem 2 the fair allocation satisfying (B.i-iii) must be of the form:
Φ(2)
1=A⊙/bracketleftigg
0 2/3
0 1/2/bracketrightigg
,Φ(2)
2=A⊙/bracketleftigg
0 0
2/3 1/2/bracketrightigg
,A=AT,0≤[A]ij≤1. (12)
4 Fair Incentives in Federated Learning
FLisadistributedlearningprocessusedwhendataiseithertoolargeortoosensitivetobedirectlytransferred
in full to the platform. Instead of combining all the data together and learning at the platform, each user
performs some part of the learning locally and the results are aggregated at the platform, providing some
level of privacy. Donahue & Kleinberg (2021) consider a setting where heterogeneous users voluntarily opt-
in to federation. A natural question to ask is: how much less valuable to the platform is a user that chooses
to federate with others as compared to one that provides full access to their data? Furthermore, how should
the platform allocate incentives to get users to federate? This section addresses these questions.
Each useri∈[N]has a unique mean and variance (θi,σ2
i)∼Θ, where Θis some global joint distribution.
Letθirepresent some information about the user critical for advertising. We wish to learn θias accurately
as possible to maximize our profits, by serving the best advertisements possible to each user. User idraws
nisamples i.i.d. from its local distribution Di(θi,σ2
i), that is, some distribution with mean θiand variance
8Published in Transactions on Machine Learning Research (02/2024)
Figure 4: Each user i∈[N]has mean and variance (θi,σ2
i)∼Θ, where Θis a global joint distribution. Let
s2= Var(θi)andr2=E[σ2
i]for alli. In this case s2is large relative to r2, and the data is very heterogeneous.
σ2
i. Lets2= Var(θi)represent the variance between users and r2=E[σ2
i]represent the variance within a
user’s data. When s2≫r2/nithe data is very heterogeneous, and it is generally not helpful to include much
information from the other users when estimating θi, however, if s2≪r2/ni, the situation is reversed, and
information from the other users will be very useful. The goal of the platform is to construct estimators ˆθp
i
while respecting the privacy level vector ρ:
EMSEi(ρ):=E/bracketleftbigg/parenleftig
ˆθp
i(ρ)−θi/parenrightig2/bracketrightbigg
. (13)
Fig. 2 summarizes our FL formulation. Users can choose from a 3-level privacy space E={0,1,2}. In this
case the privacy space is not related to DP, but instead encodes how users choose to share their data with the
platform. Let Njbe the number of users that choose privacy level j. Theheterogeneous privacy framework
is given in Table 1. Note that the error in estimating θidepends not just on the privacy level of the ith
Level Description Platform gets
ρi= 2Provide local estimator directly to the platform. ˆθi
ρi= 1Provide securely aggregated model with other users of same privacy level. ˆθf=1
N1/summationtext
i:ρi=1ˆθi
ρi= 0Provide no data to the platform. Nothing
Table 1: Privacy Level Description
userρi, but on the entire privacy vector. Let the users be ordered such that ρiis a non-increasing sequence.
Then for each ithe platform constructs estimators of the form:
ˆθp
i=wi0ˆθf+N2/summationdisplay
j=1wijˆθj, (14)
where,/summationtext
jwij= 1for alli. In Proposition 5, found in Appendix B.3, we calculate the optimal choice of wij
which depends on ρ. From these estimators, the platform generates utility U(ρ). The optimal wi0andwij
in equation 14 are well defined in a Bayesian sense if ρi>0for somei, but this does not make sense when
ρ=0. We can get around this by defining EMSEi(0):=r2+ 2s2. For the purposes of our discussion, we
assume a logarithmic form utility function. This logarithmic form is common in utility theories dating back
at least to Kelly (1956). In the following section, we make a diminishing returns assumption to derive our
theoretical result, which the logarithmic utility satisfies. The exact utility function we consider is:
U(ρ):=n/summationdisplay
i=1ailog/parenleftbigg(r2+ 2s2)
EMSEi(ρ)/parenrightbigg
. (15)
airepresents the relative importance of each user . This is important to model because some users
may spend more than others, and are thus more important to the platform i.e., the platform may care about
computing their θimore accurately than the other users. The argument of the logis increasing as the EMSE
decreases. The logmeans there is diminishing returns as each ˆθibecomes more accurate.
9Published in Transactions on Machine Learning Research (02/2024)
(a)
 (b)
Figure 5: (a)Plot ofdifference from the average utility per user U(ρ)/Nfor each of the four different types
of users, for three different regimes of s2= Var(θi)andr2=E[σ2
i], with heterogeneity decreasing from left
to right. In left (most heterogeneous) plot users who choose ρi= 2are more valuable compared to those
that choose ρ1= 1. In the center there is an intermediate regime, where all users are paid closer to the
average, with users with more data being favored slightly. In the rightmost graph, with little heterogeneity
users with more data are paid more, and privacy level has a lesser impact on the payments.
(b)In each case there is one user iwithai= 100(indicated with a star), while all other users j̸=ihave
aj= 1(airepresents the relative importance of the user in the utility function). In the two leftmost set of
bars, we see that the user with ρi= 2andni= 100receives by far the most payment, when heterogeneity
is high, but this becomes less dramatic as heterogeneity decreases. This shows that when users are very
heterogeneous, if aiis large for only user i, most of the benefit in terms of additional payments should go to
useri. Likewise, comparing the second from the left and the rightmost plots we see little difference, showing
that the opposite is true in the homogeneous case: any user can benefit from any other user having a large ai.
4.1 Fair Payments Under Optional Federation
In this section, we focus on our definition of fairness in Theorem 2 and analyze the fair values ϕi(ρ)that
are induced when using that notion of fairness. Let there be N= 10users.N1= 5of these users opt for
federating ( ρi= 1),N2= 4directly provide their data to the platform ( ρi= 2), and finally, N0= 1user
chooses to not participate ( ρi= 0). In this subsection (and Fig. 5), without loss of generality, we assume
α(ρ) = 1, and the results of this section can be scaled accordingly. The choices of ρifor each user depends
on their individual privacy sensitivity functions ci, but we defer that discussion to the next subsection.
Different Amounts of Data Fig 5a plots the difference from an equal distribution of utility, i.e., how
much each user’s utility differs from U(ρ)/N. We assume ai= 1for all users. In the bars furthest to the
left, where s2= 100andr2= 1, we are in a very heterogeneous environment. Intuitively, this means that a
userjwill have data that may not be helpful for estimating θiforj̸=i, thus those users that choose ρi= 2
are paid the most, since at the very least, the information they provide can be used to target their own
θi. Likewise, users that federate obfuscate where their data is coming from, making their data less valuable
(since their own θicannot be targeted), so users with ρi= 1are paid less than an even allocation. On the
right side, we have a regime where s2= 0.1andr2= 100, meaning users are similar and user data more
exchangeable. Now users with larger niare paid above the average utility per user, while those with lower ni
are paid less. Users with ρi= 2still receive more than those with ρi= 1whenniis fixed, and this difference
is significant when ni= 100. In the center we have an intermediate regime of heterogeneity, where s2= 1
andr2= 10. Differences in payments appear less pronounced, interpolating between the two extremes.
More Valuable Users Fig 5b is like Fig 5a, except now in each set of graphs, exactly one user has
ai= 100, meaning that estimating θifor useriis100times more important than the others. Looking at the
two leftmost sets of bars in Fig 5b we see that when user iwithρi= 2andni= 100is the most important
10Published in Transactions on Machine Learning Research (02/2024)
one, whens2is large compared to r2, it is useriwho receives most of the benefit in terms of its payment but
whens2is smaller, other users also benefit. This can be intuitively explained as follows: if users are very
heterogeneous, other users j̸=ido not have data that is helpful for determining θi, thus they do not benefit
when userihas a larger ai. Likewise, when s2is small compared to r2not just user ibenefits, but also all
those users that contribute more data, as those users with ρi= 1andni= 100are also paid over the average
utility per user. Another key point is the similarity between the second and fourth set of graphs. This tells
an interesting story: when users are not very heterogeneous, regardless of which user is has ai= 100, it is
those users with large nithat will benefit.
4.2 Platform and User Game - Mechanism Design
Figure 6: Plot of payments and user sensitivities cifor 5 different types of users with different sensitivity
functions. Some user types are repeated, (indicated in the title), such that there is a total of N= 10Users.
Users choose the privacy level ρito maximize the difference between their payment and privacy sensitivity
function. Astarineachplotmarkseachuser’stopchoice. Thepaymentfunctions ϕi(ρ)changesbasedonthe
privacy levels of all the users. At the given configuration, which matches the left-most plot in Fig 5b ( s2=
100,r2= 1), no user benefits by changing their choice of ρi, thus the configuration is a Nash Equilibrium.
Nash Equilibrium Under Fair Payments We have just discussed how the fair values ϕi(ρ)change
dependingonthemodelparameters, andthechoicesofprivacylevel ρi. Nowwediscuss howtheuserscometo
decide their ρi. We again focus on the notion of privacy in Theorem 2, but we now restrict α(ρ) =α∈[0,1].
Toavoidoverlycomplicatingthemodel, wetake ai= 1forallusers. User ichoosestheirprivacylevel ρibased
on both the payments that they receive from the platform αϕi(ρi;ρ−i)as well as their own unique privacy
sensitivityfunction ci(ρi). Inthiscontext,wecanvieweachuserasoptimizingtheirownlocalutilityfunction:
ui(ρi,ρ−i):=αϕ(ρi;ρ−i)−ci(ρi). (16)
Useriindividually optimizes this function to determine their best response (denoted BRi) to the choices of
the other users and the platform:
BRi(ρ−i,α):= arg max
ρiui(ρi,ρ−i) (17)
The full best response function BR(ρ,α):= [BR 1(ρ−1,α),..., BRN(ρ−N,α)]Tis a vector that collects all
users’ individual best response functions. The “fixed points” of the best response at a given αconstitute the
pure-strategy NEs:
NE(α):={ρ:ρ= BR( ρ,α)}. (18)
Fig 6 depicts a particular fixed point of the best response function (therefore a NE) in our federated mean
estimation example. Each of the N= 10users in assigned one of 5 unique sensitivity functions ci, which are
shown in the figure.
Solving the Fair Data Acquisition Problem The platform’s goal is to set αsuch that it maximizes
the total amount of utility it receives. Since the NE set depends on αthe platform has some control over
the behavior of the users. The mechanism design problem in this case reduces to:
maximizeα,ρ(1−α)U(ρ)
subject to ρ∈NE(α).(19)
11Published in Transactions on Machine Learning Research (02/2024)
(a) Homogeneous
 (b) Heterogeneous
Figure 7: (a) and (b) The top of both sub-figures plot the optimization landscape for the platform design
problem equation 19.The solid black line depicts the partial solution to equation 19, where we optimize
overρfor fixedα. Precisely that is: max ρ(1−α)U(ρ),ρ∈NE(α). The bottom of each plot depicts the
optimizing arg maxρ(1−α)U(ρ),ρ∈NE(α).ρi= 0means that the users send no data to the platform,
ρi= 1means the users securely aggregate their data with other users, and ρi= 2means the users send all
their data to the platform. Each row of bars corresponds to a unique user i. On the right-hand side, the
amount of data each user has niis given, as well as the color-code corresponding to the ciof each user. By
looking at the same color plot in Fig 6, the cifor the given user can be found. If there are multiple NEs
that achieve the maximum utility, one is shown arbitrarily.
(a)Is in a homogeneous regime. In this example it turns out that the optimal α∗for the platform is such that
both of the users with ni= 100chooseρi= 1, while the other users choose ρi= 0, showing the importance
of the amount of data, as opposed to higher privacy options.
(b)Is in a more heterogeneous setting. We find that the optimal α∗ensures that most users choose ρi= 2,
except one user with high privacy sensitivity that chooses ρi= 0, and one user that chooses ρi= 1.
Note that this is exactly equation 2, except we have used the fact that α(ρ)U(ρ) =/summationtext
iϕi(ρ). Solving
equation 19 is a daunting task, due to the complex structure of the constraint, however, it is numerically
tractable in this case. The key idea to efficiently solve equation 19 is to exploit symmetries in the utility
function. For instance, if ρ′is constructed from ρby permuting the values ρi,ρjwhereni=nj, then
U(ρ) =U(ρ′). This allows us compute ϕi(ρ)for a much smaller number of representative ρ, rather than
a full combinatorial search. Despite producing the same utility, ρandρ′may have very different stability
properties. To deal this this, we can implement an efficient tree search to identify the NEs within each group.
Once we have an efficient algorithm for computing NEs, we conduct a grid search to determine the optimal
α. Note that for an arbitrary problem, finding the equilibria can be a challenging task, and thus the more
difficult it is to compute the equilibria, the more difficult it will be to solve the design problem in equation 2.
We provide a full description of our solution in Appendix D.
Fig 7 shows the numerical solution to equation 19 for two different choices of s2andr2. Fig 7a is a more
homogeneous setting and we observe that the optimal α∗sets the payment just high enough so that both
of the users with ni= 100choose to participate at ρi= 1, while all other users choose ρi= 0. Essentially,
the platform identifies the two users with ni= 100, and focused on collecting their data, rather than the
data of the other 8 users. This configuration collects a majority of the data for relatively little payment.
Since the data is very homogeneous anyways, the platform is not losing much utility by allowing those two
users to choose ρi= 1rather than ρi= 2. As the platform increases α, more users choose to participate,
eventually with many of them choosing ρi= 2, however, the benefit is minimal, and it is outweighed by the
extra payments that are required to achieve that outcome. Fig 7b covers a more heterogeneous setting. In
this setting, we see that α∗is higher, and a larger fraction of the total utility is paid to users. For this α∗
only two of the users do not choose ρi= 2. There is also one user with ρi= 1, and one with ρi= 0that have
12Published in Transactions on Machine Learning Research (02/2024)
a higher privacy sensitivities. In the more heterogeneous setting, allowing users to choose a private option
is more costly, so the extra payment to ensure more user choose ρi= 2is worthwhile in this regime.
Other Formulations We conclude this section by remarking that this is just one particular formulation
of a federated learning problem where users have privacy choice. Another interesting formulation can be
found in Aldaghri et al. (2023), which comes from the literature on personalized federated learning Li et al.
(2021). In this formulation, all users join the federated learning process, but some users can choose between
a private option, where their data is protected via differential privacy, or a standard non-private option.
Exploring fair incentives in this, and other models could be an interesting direction for future work.
5 Fairness Constraints: Data Acquisition
In the previous section, we considered a complex model, numerically studying the equilibria of the users
based on fair payments from the platform, as well as how the platform optimally chooses α. In this section,
we consider a tractable model and theoretically study how the optimal α∗changes based on the privacy
sensitivity of users, under a fairness framework based on Theorem 2. This section addresses this problem by
investigating the incentives of a platform designing a mechanism under the constraint of fairness.
ConsiderN≥2users each with identical statistical marginal contribution, i.e., for any i,jwe haveS⊆
[N]\{i,j},U(ρS∪{i}) =U(ρS∪{j}). The platform is restricted to making fair payments satisfying axioms
(B.i-iii) with the additional constraint that α(ρ) =α∈[0,1]. Users choose one of two available privacy
levelsρi∈EN, withE={ρ′
1,ρ′
2}andρ′
2>ρ′
1. We can write the utility of the user ias
u(ρi,ρ−i) =αϕ(ρi;ρ−i)−c1{ρi=ρ′
2}. (20)
Users gain utility from incentives provided by the platform but incur a cost of cif they choose the less private
option. For now, we assume cis the same for all users; later we discuss the case where cis different. Note that
we can drop the index of ϕidue to the assumption of equal marginal contribution. To enrich the problem,
we allow users to employ a mixed strategy denoted by p= [p,(1−p)]T, where users choose the ρ′
1with
probability pandρ′
2with probability 1−p. This is justified because we expect users to repeatedly interact
with platforms and sample from their mixed strategy and ultimately converge to their expected utility.
The platform is also trying to maximize the fraction of the total expected utility U(p):=Eρ∼p[U(ρ)]that
it keeps as in equation 3. The platform’s goal is to choose a payment value αsuch that it optimizes:
maximizeα(1−α)U(p∗(α))
subject to p∗(α)∈NE(α).(21)
The objective is simplified compared to equation 3 by exploiting the pseudo-efficiency axiom, which says that
the sum of payments is αtimes the total utility. The constraint in equation 21 implicitly encodes the user
behavior governed by equation 20, and will change with the privacy sensitivity c. Theorem 3 characterizes
the solution of equation 21 for different values of c. To make equation 21 amenable to insightful analysis,
we make some mild assumptions.
Assumption 1. The utility Uis monotone: ρ(2)
S≥ρ(1)
S=⇒U(ρ(2)
S)>U(ρ(1)
S)∀S⊆[N].
Assumption 2. The utility Uhas diminishing returns. Let nprivate (ρS)represent the number of elements
ofi∈Ssuch thatρi=ρ′
1, i.e., the number of users choosing the higher privacy option. Furthermore, define
∆iU(ρS):=U(ρ(i+)
S)−U(ρS), where ρ(i+)
Sis equal to ρSexceptρ(i+)
i=ρ′
2. In other words, ∆iU(ρS)is the
marginal increase in utility when the ith user switches to the lower privacy option. Then Usatisfies:
nprivate (ρ(1)
S)≥nprivate (ρ(2)
S) =⇒∆iU(ρ(1))>∆iU(ρ(2)). (22)
It is helpful to define the expected relative payoff , where the expectation is taken with respect to the actions of
the other players. When all other users choose a mixed strategy p, the expected relative payoff is defined as:
γ(p):=ϕ(ρ′
2;p)−ϕ(ρ′
1;p) =Eρj∼p
j̸=i[ϕ(ρ′
2;ρ−i)−ϕ(ρ′
1;ρ−i)]. (23)
13Published in Transactions on Machine Learning Research (02/2024)
For convenience, we have defined γin terms of the scalar p, rather than the vector p= [p,(1−p)]T. This
quantity represents the expected gain in incentive (normalized to make it invariant to α) if a user switches
to a less private level from the more private level given everyone else plays the mixed strategy p.
Theorem 3. Consider a binary privacy level game with Nusers and a platform. If Usatisfies Assumptions
1 and 2, and the platform payments are fair as defined in Theorem 2 with constant αthen the optimal α∗
can be divided into three regimes depending on c. The boundaries of these regions are γmax:= maxpγ(p)
and somecth<γmaxsuch that:
1. Whenc>γmax,α∗= 0is the maximizer of 21.
2. Whencth<c<γmaxthenα∗is the minimizing α∈[0,1]such thatp∗(α)∈γ−1(c/α).
3. Whenc<cth:α∗is the smallest α∈[0,1]such thatp(α) = 0, where
cth= max/braceleftbigg
c/vextendsingle/vextendsingle/vextendsingle/vextendsingle1−c/γmin
1−α−U(p∗(α))
U(0)≥0∀α≤c/γmin/bracerightbigg
. (24)
Theorem 3 can be interpreted as follows. If privacy sensitivity is above γmaxfor the given task, it is not
worth the effort of the platform to participate. On the other hand, if privacy sensitivity is less than cth, the
platform should set αto be as small as possible, while still ensuring that all users choose the low privacy
setting. Finally, ifprivacysensitivitiesliesomewhereinbetween, α∗shouldbechosenbasedonthe γfunction,
and generally will lead to a mixed strategy with some proportion of users choosing each of the two options.
Comparison to other works Two key novelties of our work is that we (1) consider a constraint of fairness
and (2) have users choose a privacy level, rather than report their privacy sensitivity. This is different from
Fallah et al. (2022), and Cummings et al. (2023), which rely on incentive compatibility, and have users
report their privacy parameters. In Fallah et al. (2022), a computationally efficient algorithm is proposed for
computing user payments and privacy levels to assign users. Both of these works consider a mean estimation
problem, where users have i.i.d. samples, and so also have the “equal marginal contribution” assumption
that we have. Distinct from our model, users have an additional term in their utility where they benefit
from reduced error in the estimation problem. These works focus on maximizing the platform utility, and it
is very clear that the payments deviate significantly from the fair ones that satisfy the fairness axioms. Hu
& Gong (2020) is perhaps the work most relevant to ours. They consider an incentive design problem where
the platform fixes the total sum of payments Rand the amount each user receives is proportional to their
privacy level ρi, which the users choose. This proportional scheme, while potentially viewed as a type of
fairness,doesnotsatisfyouraxioms. Foraparticularutilityfunction,theydevelopacomputationallyefficient
algorithm to compute the equilibrium privacy levels ρibased on the privacy sensitivities of the users and the
total sum of payments R. In all of these works, users have a linear privacy sensitivity function with rate ci.
Figure8: Utilityofusersandplatformwhenplat-
form solves equation 21. The solution has three
separate regions as predicted by Theorem 3.Thoughthisseemsdifferentfromourbinaryprivacyprob-
lem, there is a direct correspondence here since we allow
mixed strategies, so in expectation, our sensitivity is also
reduced to a linear function of the mixed strategy: i.e.,
E[ci1{ρi=ρ′
2}] =ciPr(ρi=ρ′
2).
5.1 Mechanism Design: Mean Estimation Example
Let’s look at the utility function from equation 11, and
fairpaymentsthatwecalculatedfromTheorem2inequa-
tion 10. In this case there are N= 2users, and we will
assume each user has a sensitivity function as in equa-
tion 20. This setting satisfies the conditions of Theo-
rem 3, and so we can use our above result to character-
ize the optimal α∗to equation 21 for a range of different
cvalues. That is, as the privacy sensitivity parameter c
changes, how is the optimal strategy of the platform im-
pacted? Fig. 8 depicts the solution to equation 21. The
14Published in Transactions on Machine Learning Research (02/2024)
top plot shows the optimal α∗vs.c, while the bottom plot shows the total utility and the fraction of utility
paid to each user at the optimal point α∗for a range of c∈[0,1].
As predicted by Theorem 3, we find that the solution is clearly divided into three regions. Equation 24 tells
us thatcth=1
3andγmax=2
3, matching our observations in Fig. 8. In the first region when c≤1
3the
privacy sensitivity of the users is low, and the platform is able to capture most of the utility for itself, paying
less of it out to the users. In this region, α∗= 2c, growing linearly with the privacy sensitivity. We also
see that throughout this regime, the total utility is maximized, as predicted by the theory. In the region
wherec∈[1
3,2
3], optimalα∗is no longer growing linearly, and now grows as α∗=6c
3c+2no longer have
enough incentive to always choose the less private option, the total utility also begins to decrease, meaning
less utility is available for incentives. These factors lead to a decrease in total utility in this region. As α∗
continues to increase towards 1. Once α∗= 1atc=2
3, the platform is getting no utility, so may as well
chooseα∗= 0. Finally, for c≥2
3, the platform no longer attempts to incentivize the users, and the total
utility and payments fall to zero with α∗= 0.
For this particular example, it is possible to analytically solve the NE constraint in equation 21, exact
analytic expressions for the curves in Fig. 8 are given in Appendix B.4.1.
5.2 Considering Different Privacy Sensitivities
The computational burden in solving equation 21 is in characterizing the constraint, since the objective
reduces to a one-dimensional optimization over α∈[0,1]. In the previous section, with the knowledge that
the game is symmetric, we are able to easily characterize the equilibria as a function of α. If theci’s are all
different, for arbitrary utility functions, the problem essentially reduces to finding the equilibria in a general
game. To make this tractable, we will need some assumptions. In Hu & Gong (2020), the specific choice of
utility function and payments makes computation of the equilibrium tractable. If we have only two groups
of users with different cithat act together, and a finite privacy space, we can appeal to tools for enumerating
equilibria in matrix games (Avis et al., 2010). In this case if the privacy space is also binary, then the
equilibria have an analytical solution, which we provide in Appendix E. Like the symmetric case, there are
3 cases for each of the two users as well as corresponding thresholds that depend on c1andc2respectively,
resulting in 9 total cases. For example, in the case where payment is below the threshold of both users,
neither participate at the low-privacy level, when the payment is high enough both participate at the low
privacy level, and for the remaining intermediate cases, either only one user chooses the low privacy option,
or there is some asymmetric mixed strategy. Below, we numerically investigate this case:
Thisproblemdiffersfromequation21becausetheequilibriumisgovernedbyasymmetricusers. Forexample,
if user 1 and user 2 have privacy sensitivity c1andc2respectively, we have
u1(p1,p2) =pT
1Φ(2)
1p2−[0c1]Tp1, u 2(p1,p2) =pT
1Φ(2)
2p2−[0c2]Tp2. (25)
Consider a setting where there are only two users (these can be thought of as representing two groupsof
users) with utility function u1andu2listed above. Thus, when the platform is trying to optimize it’s own
utility, it must take into consideration that these two groups will play different strategies.
maximizeαpT
1Up2−(1−α)pT
1Up2
subject to ( p1,p2)∈NE(α).(26)
Fig. 9 plots the results of simulating the solution of 26. It shows that there is one region when c1andc2
are both small and close together ( <1/3), the platform chooses αto collect data from both users. If the
difference is large, even in this region, the users may be asymmetrically engaged. When c1>c2>1/3, the
platform chooses αsuch that only user 2chooses to participate, even if the difference is very small, and vice
versa ifc2>c1>1/3, as before, when c1,c2>2/3the sensitivity to too high and the platform can no longer
offer enough payment to the users.
Broader Impact Statement One of the unique defining characteristics of data is that its generation pro-
cess is inherently distributed, so no single entity exists to advocate for data sellers. In the past, platforms
15Published in Transactions on Machine Learning Research (02/2024)
Figure 9: (Left) The payments to user 2from the platform for a range of c1,c2. (Right) The platform’s share
of utility for the optimal α∗payments for a range of values c1,c2.
have been able to extract data from users, often with little to no compensation in return. As public con-
sciousness around privacy changes, a nuanced relationship around privacy between platforms and users must
develop. Transparency and understanding the value of user data is an important step in empowering regu-
lators, consumers, and platforms.
•Users making strategic decisions about when they share their data stand to gain from incentives.
•For regulators, understanding the amount of value that flows through the interactions between
platforms can enable better policies around data. Frameworks like those discussed in Theorem 1
and 2 can be a starting point in understanding exactly how much this value is.
•For platforms, understanding which data tasks are economically viable, and how they allocate in-
centive is important. Our discussion in Section 5, and our three regimes help shed light on this.
6 Conclusion
This paper introduces two formal definitions of fair payments in the context of acquisition of private data.
The first treats the users and the platform together and uses axioms like those of the Shapley value to
determine a unique fair distribution of utility. In the second, we define a notion of fairness between the users
only, leading to a definition of fairness that admits a range of values, of which the platform is free to choose
the most favorable. By formulating a federated mean estimation problem, we show that heterogeneous users
can have significantly different contributions to the overall utility, and that a fair incentive, according to our
second notion, must take into account the amount of data, privacy level as well as the degree of heterogeneity.
Weformulateandsolvethefairness-constrainedmechanismdesignprobleminthisfederatedmeanestimation
problem, and also find that data heterogeneity and user properties play an important role in the solution.
While previous literature has investigated how platforms should design incentives for users in order to
optimize its utility, the definitions of fairness we propose offers another important way to evaluate the
fairness of these mechanisms. This is a critical step towards future research in ensuring that data acquisition
mechanisms are bothfair for users and efficient for platforms.
Though we provide a characterization of optimal fair mechanisms when privacy sensitivity is the same across
users, designing mechanisms and developing theories that scale up these solutions to deal with platform
that interact with large and diverse groups of users is critical. Additionally, users may come and go, as
their sensitivities may change over time. Understanding how fluctuating users alter the model is of great
practical significance. Furthermore, there is subjectivity in the choice of axioms, and other choices may lead
to meaningful notions of fairness worthy of study. We have also assumed a non-divisible and transferable
utility, but in many cases, users are paid for their data in the form of access to services. Investigating the
impact of this will also be important for the practical application of a comprehensive theory for fairness.
16Published in Transactions on Machine Learning Research (02/2024)
References
Daron Acemoglu, Ali Makhdoumi, Azarakhsh Malekian, and Asuman Ozdaglar. Too much data: Prices and
inefficiencies in data markets. Working Paper 26296, National Bureau of Economic Research, September
2019. URL http://www.nber.org/papers/w26296 .
Alessandro Acquisti, Curtis Taylor, and Liad Wagman. The economics of privacy. Journal of Economic Lit-
erature, 54(2):442–92, June 2016. doi: 10.1257/jel.54.2.442. URL https://www.aeaweb.org/articles?
id=10.1257/jel.54.2.442 .
Nasser Aldaghri, Hessam Mahdavifar, and Ahmad Beirami. Federated learning with heterogeneous differen-
tial privacy, 2023.
David Avis, Gabriel D. Rosenberg, Rahul Savani, and Bernhard von Stengel. Enumeration of Nash equilibria
for two-player games. Economic Theory , 42(1):9–37, 2010. doi: 10.1007/s00199-009-0449-x. URL https:
//doi.org/10.1007/s00199-009-0449-x .
Magdalena Balazinska, Bill Howe, and Dan Suciu. Data markets in the cloud: An opportunity for the
database community. Proc. VLDB Endow. , 4(12):1482–1485, aug 2011. ISSN 2150-8097. doi: 10.14778/
3402755.3402801. URL https://doi.org/10.14778/3402755.3402801 .
Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower
bounds. In Martin Hirt and Adam Smith (eds.), Theory of Cryptography , pp. 635–658, Berlin, Heidelberg,
2016. Springer Berlin Heidelberg. ISBN 978-3-662-53641-4.
Syomantak Chaudhuri and Thomas A. Courtade. Mean estimation under heterogeneous privacy: Some
privacy can be free, 2023.
Yiling Chen and Shuran Zheng. Prior-free data acquisition for accurate statistical estimation. In Proceedings
of the 2019 ACM Conference on Economics and Computation , pp. 659–677, 2019.
Yiling Chen, Nicole Immorlica, Brendan Lucier, Vasilis Syrgkanis, and Juba Ziani. Optimal data acquisition
for statistical estimation. In Proceedings of the 2018 ACM Conference on Economics and Computation , EC
’18, pp. 27–44, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450358293.
doi: 10.1145/3219166.3219195. URL https://doi.org/10.1145/3219166.3219195 .
Shih-Fen Cheng, Daniel M Reeves, Yevgeniy Vorobeychik, and Michael P Wellman. Notes on equilibria in
symmetric games. In Proceedings of the 6th International Workshop On Game Theoretic And Decision
Theoretic Agents GTDT , 2004.
R.Cummings, H.Elzayn, E.Pountourakis, V.Gkatzelis, andJ.Ziani. Optimaldataacquisitionwithprivacy-
aware agents. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) , pp. 210–
224, Los Alamitos, CA, USA, feb 2023. IEEE Computer Society. doi: 10.1109/SaTML54575.2023.00023.
URL https://doi.ieeecomputersociety.org/10.1109/SaTML54575.2023.00023 .
Kate Donahue and Jon Kleinberg. Model-sharing games: Analyzing federated learning under voluntary
participation. In 2021 AAAI Conference on Artifical Intellegence , 2021. doi: 10.48550/ARXIV.2010.00753.
URL https://arxiv.org/abs/2010.00753 .
Cynthia Dwork. Differential privacy: A survey of results. In Manindra Agrawal, Dingzhu Du, Zhenhua Duan,
and Angsheng Li (eds.), Theory and Applications of Models of Computation , pp. 1–19, Berlin, Heidelberg,
2008. Springer Berlin Heidelberg. ISBN 978-3-540-79228-4.
Alireza Fallah, Ali Makhdoumi, Azarakhsh Malekian, and Asuman Ozdaglar. Optimal and differentially
private data acquisition: Central and local mechanisms, 2022. URL https://arxiv.org/abs/2201.
03968.
Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In
Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp. 2242–2251. PMLR,
09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ghorbani19c.html .
17Published in Transactions on Machine Learning Research (02/2024)
Amirata Ghorbani, Michael Kim, and James Zou. A distributional framework for data valuation. In
HalDauméIIIandAartiSingh(eds.), Proceedings of the 37th International Conference on Machine Learn-
ing, volume 119 of Proceedings of Machine Learning Research , pp. 3535–3544. PMLR, 13–18 Jul 2020.
URL https://proceedings.mlr.press/v119/ghorbani20a.html .
Arpita Ghosh and Katrina Ligett. Privacy as a coordination game. In 2013 51st Annual Allerton Conference
on Communication, Control, and Computing (Allerton) , pp. 1608–1615, 2013. doi: 10.1109/Allerton.2013.
6736721.
Arpita Ghosh and Aaron Roth. Selling privacy at auction. In Proceedings of the 12th ACM Conference
on Electronic Commerce , EC ’11, pp. 199–208, New York, NY, USA, 2011. Association for Computing
Machinery. ISBN 9781450302616. doi: 10.1145/1993574.1993605. URL https://doi.org/10.1145/
1993574.1993605 .
Sergiu Hart and Andreu Mas-Colell. Potential, value, and consistency. Econometrica , 57(3):589–614, 1989.
ISSN 00129682, 14680262. URL http://www.jstor.org/stable/1911054 .
Rui Hu and Yanmin Gong. Trading data for learning: Incentive mechanism for on-device federated learning.
InGLOBECOM 2020 - 2020 IEEE Global Communications Conference , pp. 1–6, 2020. doi: 10.1109/
GLOBECOM42002.2020.9322475.
Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gürel, Bo Li, Ce Zhang,
DawnSong,andCostasJ.Spanos. Towardsefficientdatavaluationbasedontheshapleyvalue. InKamalika
Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the Twenty-Second International Conference on
Artificial Intelligence and Statistics , volume 89 of Proceedings of Machine Learning Research , pp. 1167–
1176. PMLR, 16–18 Apr 2019. URL https://proceedings.mlr.press/v89/jia19a.html .
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hu-
bert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih
Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben
Hutchinson,JustinHsu,MartinJaggi,TaraJavidi,GauriJoshi,MikhailKhodak,JakubKonecný,Aleksan-
dra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar
Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana
Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian
Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu,
and Sen Zhao. Advances and open problems in federated learning. Foundations and Trends ®in Machine
Learning , 14(1–2):1–210, 2021. ISSN 1935-8237. doi: 10.1561/2200000083. URL http://dx.doi.org/
10.1561/2200000083 .
JohnLKelly. Anewinterpretationofinformationrate. the bell system technical journal , 35(4):917–926, 1956.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In International Conference on Machine Learning , pp. 6357–6368. PMLR, 2021.
Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceedings
of the 31st International Conference on Neural Information Processing Systems , NIPS’17, pp. 4768–4777,
Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
Aaron Roth and Grant Schoenebeck. Conducting truthful surveys, cheaply. In Proceedings of the 13th ACM
Conference on Electronic Commerce , EC ’12, pp. 826–843, New York, NY, USA, 2012. Association for
Computing Machinery. ISBN 9781450314152. doi: 10.1145/2229012.2229076. URL https://doi.org/
10.1145/2229012.2229076 .
Lloyd S. Shapley. A Value for N-Person Games . RAND Corporation, Santa Monica, CA, 1952. doi:
10.7249/P0295.
18Published in Transactions on Machine Learning Research (02/2024)
Sarah Spiekermann, Alessandro Acquisti, Rainer Böhme, and Kai-Lung Hui. The challenges of personal data
markets and privacy. Electronic Markets , 25(2):161–167, 2015a. doi: 10.1007/s12525-015-0191-0. URL
https://doi.org/10.1007/s12525-015-0191-0 .
Sarah Spiekermann, Rainer Böhme, Alessandro Acquisti, and Kai-Lung Hui. Personal data markets. Elec-
tronic Markets , 25(2):91–93, 2015b. doi: 10.1007/s12525-015-0190-1. URL https://doi.org/10.1007/
s12525-015-0190-1 .
Jiachen T. Wang and Ruoxi Jia. Data banzhaf: A robust data valuation framework for machine learning. In
Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent (eds.), Proceedings of The 26th International
Conference on Artificial Intelligence and Statistics , volume 206 of Proceedings of Machine Learning Re-
search, pp. 6388–6421. PMLR, 25–27 Apr 2023. URL https://proceedings.mlr.press/v206/wang23e.
html.
Jaap Wieringa, P.K. Kannan, Xiao Ma, Thomas Reutterer, Hans Risselada, and Bernd Skiera. Data ana-
lytics in a privacy-concerned world. Journal of Business Research , 122:915–925, 2021. ISSN 0148-2963.
doi: https://doi.org/10.1016/j.jbusres.2019.05.005. URL https://www.sciencedirect.com/science/
article/pii/S0148296319303078 .
19Published in Transactions on Machine Learning Research (02/2024)
A Notation Table
Symbol Definition Relevance
E privacy level space
AllN Number of users
ρi privacy level (generic) of user i
ϵi privacy level (DP only) of user i
U(·)platform utility function
ui(·)useri’s utility function
ci(·)privacy sensitivity function
NE(·)Nash Equilibrium
ti(·)generic payment function
ϕi(·)fair payment function for user iunder Theorem 1 & 2
ϕp(·)fair payment for platform under Theorem 2
α(·)fraction of utility used as payment in Theorem 2
ni useri’s amount of data
Section 4s2variance of user means
r2mean of user variances
ai relative user value in utility function
BRi(·)best response function
p prob. of playing more private option in mixed strategySection 5γ(·)expected relative payoff by changing strategy
Table 2: Table of Common Notations
B Missing Proofs
B.1 Proof of Equation 37
In this section, we present the calculations required to arrive at the utility values in equation 37. First let’s
treat the trivial case of ϵ1= 0,ϵ2= 0. The optimal ϵ-DP estimator is simply the optimal Bayes estimator
with no data, i.e., the prior mean. Let us define this estimator as ˆµ(0,0)= 0. Its risk function is
R(µ,ˆµ(0,0)) =E/bracketleftbig
L(ˆµ(0,0),µ)|µ/bracketrightbig
=µ2. (27)
The Bayes risk of ˆµ(0,0)is the expectation of this quantity taken using our prior:
r([0,0]) =E/bracketleftbig
µ2/bracketrightbig
=1
12. (28)
Next, consider the case where user ichooses privacy level ϵ1=ϵ′>0, and the other user chooses ϵ2= 0. In
this case the estimator depends on X1,ˆµ(ϵ′,0)=w1X1+Z. Then the risk function is:
R(µ,ˆµ(ϵ′,0)) =E/bracketleftig
(w1X1+Z−µ)2|µ/bracketrightig
=/parenleftbigg
µ+1
2/parenrightbigg/parenleftig
µ−w1
2/parenrightig2
+/parenleftbigg
−µ+1
2/parenrightbigg/parenleftig
µ+w1
2/parenrightig2
+2
η2.(29)
Now taking the expectation with respect to our prior over µ, we have:
E/bracketleftbig
R(µ,ˆµ(ϵ′,0))/bracketrightbig
=1
12/parenleftbig
3w2
1−2w1+ 1/parenrightbig
+2
η2, (30)
20Published in Transactions on Machine Learning Research (02/2024)
hereηis the inverse scale parameter for Z. Note that equation 30 is minimized when ηis maximized. The
ϵ-DP condition enforces the constraint η≤ϵ′
w1. This constraint will be met with equality for the optimal
w1. The optimal w∗
1=1
3+24
ϵ′2. Thus, we have:
ˆµ(ϵ′,0)=1
3 +24
ϵ′2X1+Z, Z∼Laplace/parenleftbiggϵ′
3ϵ′2+ 24/parenrightbigg
, (31)
and the resulting Bayes risk is:
r([ϵ′,0]) =r([0,ϵ′]) =1
12/parenleftigg
1−1
3 +24
ϵ′2/parenrightigg
. (32)
For the case with ϵ1=ϵ2=ϵ′we can repeat the same process by defining ˆµ(ϵ′,ϵ′)=w1X1+w2X2+Z. By
symmetry, we must have w1=w2, so we drop the index. Then the risk function and its expectation are:
R(µ,ˆµ(ϵ′,ϵ′)) = 2/parenleftbigg
µ+1
2/parenrightbigg/parenleftbigg
−µ+1
2/parenrightbigg
µ2+/parenleftbigg
µ+1
2/parenrightbigg2
(w−µ)2+/parenleftbigg
−µ+1
2/parenrightbigg2
(µ+w)2+2
η2
(33)
E/bracketleftbig
R(µ,ˆµ(ϵ′,ϵ′))/bracketrightbig
=1
12(8w2−4w+ 1) +2
η2. (34)
By a similar argument to the previous case, the Bayes optimal estimator and the corresponding Bayes risk is:
ˆµ(ϵ′,ϵ′)=1
4 +12
ϵ′2(X1+X2) +Z, Z∼Laplace/parenleftbiggϵ′
4ϵ′2+ 12/parenrightbigg
, (35)
r([ϵ′,ϵ′]) =1
12/parenleftigg
1−1
2 +6
ϵ′2/parenrightigg
. (36)
Finallyletting U(ϵ) =c1r(ϵ)+c2. TakeU(0) = 0 =⇒c1=−12c2. And max ϵU(ϵ) = 1 =⇒c1= 24(1−c2).
Simplifying gives us our desired result:
U=/bracketleftigg
U([0,0]T)U([0,ϵ′]T)
U([ϵ′,0]T)U([ϵ′,ϵ′]T)/bracketrightigg
=
0 2/parenleftig
3 +24
(ϵ′)2/parenrightig−1
2/parenleftig
3 +24
(ϵ′)2/parenrightig−1/parenleftig
1 +3
(ϵ′)2/parenrightig−1
 (37)
B.2 Proof of Theorem 1 and Theorem 2
We will begin with the proof of Theorem 2, which is standard and follows the typical proof of the Shapley
value. We begin by proving ϕi(ρ)as defined in equation 7 satisfies axioms (B.i-iii). First assume U(ρS∪{i}) =
U(ρS∪{j})∀S⊂[N]\{i,j}, then:
ϕi(ρ) =α(ρ)
N/summationdisplay
S⊆[N]\{i}U(ρS∪{i})−U(ρS)/parenleftbigN−1
|S|/parenrightbig (38)
=α(ρ)
N
/summationdisplay
S⊆[N]\{i,j}U(ρS∪{i})−U(ρS)/parenleftbigN−1
|S|/parenrightbig +/summationdisplay
S⊆[N]\{i,j}/parenleftbig
U(ρS∪{j}∪{i})−U(ρS∪{j})/parenrightbig
/parenleftbigN−1
|S|+1/parenrightbig
(39)
=α(ρ)
N
/summationdisplay
S⊆[N]\{i,j}U(ρS∪{j})−U(ρS)/parenleftbigN−1
|S|/parenrightbig +/summationdisplay
S⊆[N]\{i,j}/parenleftbig
U(ρS∪{i}∪{j})−U(ρS∪{i})/parenrightbig
/parenleftbigN−1
|S|+1/parenrightbig
(40)
=ϕj(ρ), (41)
21Published in Transactions on Machine Learning Research (02/2024)
proving axiom (B.i) is satisfied. For the proof that axiom (B.ii) is satisfied, we write:
/summationdisplay
iϕi(ρ) =α(ρ)
N/summationdisplay
i/summationdisplay
S⊆[N]\{i}U(ρS∪{i})−U(ρS)/parenleftbigN−1
|S|/parenrightbig (42)
=α(ρ)
N
/summationdisplay
i/summationdisplay
S⊆[N]\{i}U(ρS∪{i})/parenleftbigN−1
|S|/parenrightbig−/summationdisplay
i/summationdisplay
S⊆[N]\{i}U(ρS)/parenleftbigN−1
|S|/parenrightbig
 (43)
=α(ρ)U(ρ) +α(ρ)
N
/summationdisplay
i/summationdisplay
S⊆[N]\{i}
|S|<N−1U(ρS∪{i})/parenleftbigN−1
|S|/parenrightbig−/summationdisplay
i/summationdisplay
S⊆[N]\{i}U(ρS)/parenleftbigN−1
|S|/parenrightbig
(44)
=α(ρ)U(ρ) +α(ρ)
N
/summationdisplay
i/summationdisplay
S⊆[N]
i∈S
|S|<N−1U(ρS)/parenleftbigN−1
|S|−1/parenrightbig−/summationdisplay
S⊆[N]
|S|≤N−1(N−|S|)U(ρS)/parenleftbigN−1
|S|/parenrightbig
(45)
=α(ρ)U(ρ) +α(ρ)
N
/summationdisplay
S⊆[N]
|S|≤N−1|S|U(ρS)/parenleftbigN−1
|S|−1/parenrightbig−/summationdisplay
S⊆[N]
|S|≤N−1(N−|S|)U(ρS)/parenleftbigN−1
|S|/parenrightbig
(46)
=α(ρ)U(ρ), (47)
thus proving axiom (B.ii) is satisfied. Finally, we note that (B.iii) is satisfied by linearity. Next, we establish
the uniqueness of equation 7. To prove uniqueness, we take an approach that is standard in the literature
where we define the unanimity game, show the uniqueness of the ϕi(ρ)in that case, and then argue that
uniqueness follows from additivity (B.iii).
Define the unanimity utility, indexed by some T⊆[N]:
UT(ρ) =/braceleftigg
1ifT⊆supp(ρ)
0ifelse.(48)
{UT}T⊆[N]form a linear basis for utility function such that any utility Ucan be represented uniquely by
a set of values{bT}T⊆[N]. In addition, by direct application of the axioms, it is easy to see that for the
unanimity utility, the fair allocation ϕ(T)
i(ρ)is unique and is of the form:
ϕ(T)
i(ρ) =/braceleftiggα(ρ)
Tifi∈T
0ifelse.(49)
Thus, for any utility U, the fair value is represented uniquely by/summationtext
T⊆[N]bTϕ(T)
i(ρ), since this value is unique,
it must be equivalent to equation 7.
Now we consider the proof of Theorem 1. By a similar argument to the above, we can establish that:
ϕp(z,ρ) =1
N+ 1/summationdisplay
S⊆[N]U(z,ρS)−U(0,ρS)/parenleftbigN
|S|/parenrightbig (50)
as well as:
ϕi(z,ρ) =1
N+ 1/summationdisplay
S⊆[N]\{i}
z′∈{0,z}1/parenleftbigN
|S|+1(z′=1)/parenrightbig/parenleftbig
U(z′,ρS∪{i})−U(z′,ρS)/parenrightbig
(51)
(52)
22Published in Transactions on Machine Learning Research (02/2024)
Applying the definition U(0,ρ) = 0we have
ϕp(z,ρ) =1
N+ 1/summationdisplay
S⊆[N]U(z,ρS)/parenleftbigN
|S|/parenrightbig (53)
ϕi(z,ρ) =1
N+ 1/summationdisplay
S⊆[N]\{i}1/parenleftbigN
|S|+1/parenrightbig/parenleftbig
U(z,ρS∪{i})−U(z,ρS)/parenrightbig
, (54)
completing the proof.
B.3 Error Computation for Section 4
In this section we prove Proposition 4 and 5 from which exact error expressions follow.
Proposition4. For the federated mean estimation problem described in Section 4, the expected mean-squared
error is given by:
E/bracketleftbigg/parenleftig
ˆθp
i−θi/parenrightig2/bracketrightbigg
=
r2
N2/summationdisplay
j=1w2
ij·1
nj+1
N1w2
i01
¯n
+s2
N2/summationdisplay
j=1
j̸=iw2
ij+1
N2
1N2+N1/summationdisplay
j=N2+1
j̸=iw2
i0+
N2/summationdisplay
j=1
j̸=iwij+1
N1N2+N1/summationdisplay
j=N2+1
j̸=iwi0
2
,(55)
where ¯n=/parenleftigg
1
N1N1+N2/summationtext
j=N2+11
nj/parenrightigg−1
.
Proof.Consider an estimator of the form ˆθp
i=N/summationtext
i=1vijˆθj, where user jhasnsamples, and θjis the local
model of user j. By Theorem 4.2 of Donahue & Kleinberg (2021), the error can be written as:
E/bracketleftbigg/parenleftig
ˆθp
i−θi/parenrightig2/bracketrightbigg
=r2N/summationdisplay
j=1v2
ij·1
nj+s2
/summationdisplay
j̸=iv2
ij+
/summationdisplay
j̸=ivij
2
 (56)
Forj= 1,...,N 2, we havevij=wij. Forj=N2+ 1,...,N 2+N1, we havevij=wi0
N1. Finally, for
j >N 1+N2, we havevij= 0. Thus the first term can be written as:
r2N/summationdisplay
j=1v2
ij·1
nj=r2
N2/summationdisplay
j=1w2
ij1
nj+N2+N1/summationdisplay
j=N2+11
nj/parenleftbiggwi0
N1/parenrightbigg2
 (57)
=r2
N2/summationdisplay
j=1w2
ij1
nj+1
N1w2
i01
¯n
. (58)
Making these same substitutions to/summationtext
j̸=iv2
ijand/summationtext
j̸=ivijyields the desired result.
Proposition 5. The error expression equation 55 is minimized if ρi= 0with weights:
wi0=N1
N1+N2V0¯V, wij=V0/Vj
N1+N2V0¯V. (59)
Ifρi= 1equation 55 is minimized by:
wi0=N1
N1+N2V0¯V+N2
N1+N2V0¯Vs2
¯V, (60)
23Published in Transactions on Machine Learning Research (02/2024)
wij=V0/Vj
N1+N2V0¯V−1
N1+N2V0¯Vs2
Vj. (61)
Finally, ifρi= 2, equation 55 is minimized by:
wi0=N1
N1+N2V0¯V−N1
N1+N2V0¯Vs2
Vi, (62)
wij=V0/Vj
N1+N2V0¯V−V0/Vj
N1+N2V0¯Vs2
Vi(63)
wii=V0/Vi
N1+N2V0¯V+N1+N2V0¯V−V0
Vi
N1+N2V0¯Vs2
Vi(64)
Proof.First we will consider the case where ρi= 1. Considering the point where the derivative of equation 55
with respect to wik,k≥1is equal to zero gives:
2r2
nkwik−2r2
¯nN1
1−N2/summationdisplay
j=1wij
+s2
2wik−2N1−1
N2
1
1−N2/summationdisplay
j=1wij
+2
N2
1
N1−1 +N2/summationdisplay
j=1wij

= 0,(65)
/parenleftbiggr2
nk+s2/parenrightbigg
wik=/parenleftbiggr2
¯n+s2/parenrightbiggwi0
N1−s2
N1. (66)
It is easily verified from the second derivative that solving this equation gives us the unique minimum of
equation 55. For ease of notation, define Vk:=/parenleftig
r2
nk+s2/parenrightig
andV0:=/parenleftig
r2
¯n+s2/parenrightig
,¯V=/parenleftbigg
1
N2N2/summationtext
k=11
Vk/parenrightbigg−1
. Thus,
we have:
wik=V0wi0
N1−s2
N1
Vk. (67)
Noting that wi0+/summationtextN2
j=1wij= 1, we have:
wi0+N2
N1V0
¯Vwi0−N2
N1s2
¯V= 1, (68)
wi0=N1
N1+N2V0¯V+N2
N1+N2V0¯Vs2
¯V, (69)
wij=V0/Vj
N1+N2V0¯V−1
N1+N2V0¯Vs2
Vj. (70)
This completes the proof for those users isuch thatρi= 1. Whenρi= 2, the gradient condition with respect
tok≥1,k̸=iis:
wikVk=V0
N1wi0, (71)
and similarly, the gradient condition when k=iis:
wiiVi+wi0N2V0
N1¯V+s2
Vi= 1. (72)
Combining these together gives our desired result. ρi= 0
24Published in Transactions on Machine Learning Research (02/2024)
B.4 Proof of Theorem 3
The symmetric Nash equilibria of our game is characterized Cheng et al. (2004) by the minimizers of
min
p/summationdisplay
s∈E[u(s,p)−u(p,p)]2
+, (73)
whereu(s,p)is the utility a user when they choose privacy level ρi=s, and all other users play mixed
strategy p, andu(p,p) =Es∼p[u(s,p)]. Since our action space is binary, there are only two terms in this
sum. Applying the definition of uand writing out both terms of this sum yields:
/summationdisplay
s∈E[u(s,p)−u(p,p)]2
+= [u(ρ1,p)−u(p,p)]2
++ [u(ρ2,p)−u(p,p)]2
+(74)
= [c(1−p)−α(ϕ(p,p)−ϕ(ρ1,p))]2
++ [c(1−p)−α(ϕ(p,p)−ϕ(ρ2,p))]2
+(75)
= [(1−p)(c−αγ(p))]2
++ [−p(c−αγ(p))]2
+, (76)
where we define γ(p):=ϕ(ρ2,p)−ϕ(ρ1,p).γis an important quantity in this problem that described the
relative increase in payment a user receives for choosing a higher privacy level when the other users choose
mixed strategy p. In general, to say something about the equilibria, we must say something about γ. We
can now use Assumptions 1 and 2, as well as the definition of ϕ(·;·)to establish properties of γ. First we
showγ(p)≥0using monotonicity of U:
γ(p) =ϕ(ρ2,p)−ϕ(ρ1,p), (77)
=Eρj∼p
ρi=ρ′
2
1
N/summationdisplay
S⊆[N]\{i}1/parenleftbigN−1
|S|/parenrightbig/parenleftbig
U(ρS∪{i})−U(ρS)/parenrightbig

−Eρj∼p
ρi=ρ′
1
1
N/summationdisplay
S⊆[N]\{i}1/parenleftbigN−1
|S|/parenrightbig/parenleftbig
U(ρS∪{i})−U(ρS)/parenrightbig
,(78)
=1
N/summationdisplay
S⊆[N]\{i}1/parenleftbigN−1
|S|/parenrightbigEρj∼p
j̸=i/bracketleftig
U(ρ(i+)
S∪{i})−U(ρ(i−)
S∪{i})/bracketrightig
≥0. (79)
In equation 78 we have used the definition of the fair value from Theorem 2, and in equation 79, we have
simplified the expression, exchanged the sum and expectation, and used the fact that the expectation of a
non-negative random variable is non-negative.
Next, we will show that under Assumption 2 (and our assumption of equal marginal contribution) we also
haveγ′(p)≥0. Assumep2>p1, and letb(n,p) =/parenleftbigN
n/parenrightbig
pi(1−p)N−i:
γ(p2)−γ(p1) =1
N/summationdisplay
S⊆[N]\{i}1/parenleftbigN−1
|S|/parenrightbig/parenleftbigg
Eρj∼p2
j̸=i/bracketleftig
U(ρ(i+)
S∪{i})−U(ρ(i−)
S∪{i})/bracketrightig
−Eρj∼p1
j̸=i/bracketleftig
U(ρ(i+)
S∪{i})−U(ρ(i−)
S∪{i})/bracketrightig/parenrightbigg
(80)
=1
N/summationdisplay
S⊆[N]\{i}1/parenleftbigN−1
|S|/parenrightbigN/summationdisplay
n=0(b(n,p2)−b(n,p1))∆iU(ρ(n))s.t.nprivate (ρ(n)) =N−n(81)
Now note that b(n,p2)−b(n,p1)is zero-mean, and decreasing, furthermore, ∆iU(ρ(n))is non-negative and
non-increasing. Let n∗representthesmallestvalueof nsuchthatb(n,p2)−b(n,p1)isnegative. Thenwehave:
∆iU(ρ(n)) =n∗−1/summationdisplay
n=0(b(n,p2)−b(n,p1)) ∆iU(ρ(n)) +N/summationdisplay
n=n∗(b(n,p2)−b(n,p1)) ∆iU(ρ(n))(82)
≥/parenleftiggn∗−1/summationdisplay
n=0b(n,p2)−b(n,p1)/parenrightigg
(∆iU(ρ(n∗−1))−∆iU(ρ(n∗))) (83)
≥0. (84)
25Published in Transactions on Machine Learning Research (02/2024)
With the knowledge that γ(p)≥0andγ′(p)≥0we can compute p∗for three distinct cases. Defining
γmax:= maxpγ(p)andγmin:= minpγ(p), we have:
Case 1c−αγmax>0:/summationdisplay
s∈E[u(s,p)−u(p,p)]2
+= [(1−p)(c−αγ(p))]2
+(85)
Since this quantity is non-negative, it is clearly minimized when p∗= 1, where it is exactly 0. Furthermore,
sincec−αγmax>0is satisfied with strict inequality, it is the unique minimizer.
Case 2c/α∈[γmin,γmax]:
/summationdisplay
s∈E[u(s,p)−u(p,p)]2
+= [(1−p)(c−αγ(p))]2
++ [−p(c−αγ(p))]2
+, (86)
In the above case, this is minimized when p∗∈γ−1(c/α).
Case 3c−αγmin<0:/summationdisplay
s∈E[u(s,p)−u(p,p)]2
+= [−p(c−αγ(p))]2
+, (87)
In the above case, the expression is minimized when p∗= 0. To summarize, we have:
p∗(α) =

1 ifα<c
γmax
γ−1(c/α)ifα∈[c
γmax,c
γmin]
0 ifα>c
γmin. (88)
This establishes that the Nash equilibrium is cleanly separated into three regions. From this fact, we are
able to show that the optimal strategy of the platform is also separated into three regions. We consider a
platform that solves the following problem, where we define U(p):=Eρi∼p[U(ρ)]:
min
α(1−α)U(p∗(α)), (89)
Clearly, when privacy sensitivity is large, specifically, when c≥γmaxthenα∗= 0is the optimal solution,
sincep∗(α) = 1for allα<1, and forα>1the objective becomes negative.
Alternatively, when cis very small, we can determine the optimal value as follows. We first note that
Assumption 1 implies that U(p)is a decreasing function of p. Thus the condition for α∗=c
γminis:
1−c/γmin
1−α>U(p∗(α))
U(0)∀α<c/γmin. (90)
Since the left-hand side takes value1
1−αatc= 0, while the right-hand side is 1, as well as the fact that both
sides are continuous, by the Intermediate Value Theorem, (and our previous result, which implies that for c
large enough this condition does not hold), there is some minimum cth, where this condition fails. Thus we
conclude, there are three regions:
(1) a region where c≤cthis small, and α∗is the smallest αsuch thatp∗= 0, (2) an intermediate region
where a symmetric mixed strategy is played, and (3) a region where c≥γmax, andα∗= 0,p∗= 1
B.4.1 Exact Calculation for Example
In this section, we work thought our example in Section 5, showing that using Theorem 3 and some basic
calculus, we can determine valuable information about α∗The utility function is of the form:
U=/bracketleftigg
0 2/3
2/3 1/bracketrightigg
. (91)
26Published in Transactions on Machine Learning Research (02/2024)
Φ(2)
1=α/bracketleftigg
0 2/3
0 1/2/bracketrightigg
,Φ(2)
2=α/bracketleftigg
0 0
2/3 1/2/bracketrightigg
(92)
When users play a mixed strategy p, the utility U(p)can be written as
U(p) =−1
3p2−2
3p+ 1. (93)
The gamma function likewise can be computed as
γ(p) =1
2+1
6p. (94)
Thus, we find that γmax=2
3andγmin=1
2. This allows us to compute p∗(α) :
p∗(α) =

1 ifα<3c
2
6c
α−3if3c
2≤α≤2c
0 ifα>2c. (95)
From this and Theorem 3, we immediately know α∗= 0whenc≥2
3. Next, we can determine cth. First, we
computeU(p∗(α)):
U(p∗(α)) =

0 ifα<3c
2
8c
α−12c2
α2if3c
2≤α≤2c
1 ifα>2c. (96)
The threshold is concerned only with α≤2c. We note that for c <1
3, the function (1−α)U(p∗(α))is
monotone on 0≤α≤2c, and attains the value 1−2catα= 2c. However, for c<1
3, it exceeds 1−2cat
it’s maximum value at α∗=6c
3c+2. Thus,cthis1
3. To summarize, the optimal α∗is:
α∗=

2cifc<1
3
6c
3c+2if1
3≤c≤2
3
0ifc>2
3, (97)
and making the neccessary substitutions generates the plots in Fig 8.
C Monotonicity of Utility
When beginning this work, the dearth of algorithms that supported heterogeneous privacy constraints sur-
prised us, given the increasing number of privacy options available to users. All of the algorithms that did
exist were provably sub-optimal Hu & Gong (2020), or placed constraints on privacy parameters to prove ap-
proximate optimality Fallah et al. (2022). In both of these works, the pathology of the algorithm leads to er-
ror that is not monotonically decreasing in ρ. For DP-based notions of privacy, which both of the aforemen-
tioned works are, one can prove that an optimal error must be monotonic. This observation inspired a recent
work that studies a saturation phenomenon Chaudhuri & Courtade (2023). Similar ideas can also be found in
Cummingsetal.(2023). Theideaisthatanoptimalalgorithmwillsometimesgiveusersthatchoosealarge ϵi
more privacy than they asked for, to ensure that it still efficiently uses information from users jwithϵj≪ϵi.
D Solving the Federated Mean Estimation Mechanism Design Problem
In this section we discuss how we produce Fig 7. Algorithm 1 describes the process. In the first step, we
exploit the symmetry of the utility function by exploiting the fact that the utility does not change when ρi
is exchanges between two users with the same ni. This greatly reduces the number of payment functions
that need to be calculated. Next, we compute the payment functions, again exploiting symmetry whenever
possible to reduce calculations. Finally, for each partition, we efficiently search through the partition to see
if there is a ρthat leads to a NE.
27Published in Transactions on Machine Learning Research (02/2024)
Algorithm 1: Find optimal α
input :ni,ci:i= 1...,N
output:α∗
nArray(i)←ni,i= 1...,N;
cArray(i)←ci,i= 1...,N;
partitions←GetValidPartitions( ni:i= 1...,N ); /* all ρthat produce unique U*/
fori= 1tolen( partitions )do
ρ←partitions(i); /* one representative ρfrom the partition */
forj= 1toNdo
phi(j)←Shapley( ρ,j,nArray ); /* Actual code skips repeated calculations */
end
forα∈griddo
neExists←TreeSearch( α×phi,cArray );/* Check if any ρin the partition is NE */
ifneExiststhen
currUtil←(1−α)×Utility (ρ,nArray);
ifcurrUtil>maxUtilthen
α∗←α; /* Update α∗if needed */
maxUtil←currUtil;
end
end
end
end
Comment on Fig 7 The observant reader will notice there is almost always one user with ρi= 1. This is
an artifact of our model, where if only one user chooses ρi= 1, it essentially behaves the same from a utility
perspective as if it has chosen ρi= 2, but gets a reduced privacy sensitivity. It is possible to show that this
means that other than the all zeros NE, there will always be one user with ρi= 1.
E Equilibria for Binary Privacy Level with Two Different Privacy Sensitivities
Letp= [p(1−p)]Tbe the mixed strategy of user 1 and let q= [q(1−q)]Tbe the mixed strategy of user
2. When they play these respective strategies, the utility of user 1is:
u1(p,q) =pqαϕ (ρ′
1,ρ′
2) +p(1−q)αϕ(ρ′
1,ρ′
2) + (1−p)qαϕ(ρ′
2,ρ′
1) + (1−p)(1−q)αϕ(ρ′
2,ρ′
2) +−c1(1−p)
=pαϕ(ρ′
1,q) + (1−p)αϕ(ρ′
2,q)−c1(1−p)
=p(c1−αγ(q)) +αϕ(ρ′
2,q)−c1.
By a symmetric argument, we also have that
u2(p,q) =q(c2−αγ(p)) +αϕ(ρ′
2,p)−c2. (98)
We are interested in characterizing the best response maps:
BR1(q;α) = arg max
pu1(p,q) BR 2(p;α) = arg max
qu2(p,q), (99)
since their intersection characterize the set of NEs.
We begin with finding an analytic expression for BR1(q;α), which, we will break into three distinct cases:
Case 1:c1−αγmax>0
In this case, the constant factor in front of pis always positive (invoking the monotonicity and non-negativity
we proved in the previous section under the assumptions), thus the best response is:
BR1(q;α) = [1 0]T∀α<c1
γmax. (100)
28Published in Transactions on Machine Learning Research (02/2024)
Case 2:c1−αγmin<0
In this case, by a similar argument to before, the constant factor in front of pis always negative, thus the
best response is:
BR1(q;α) = [0 1]T∀α>c1
γmin. (101)
Case 3:α∈/bracketleftig
c1
γmax,c1
γmin/bracketrightig
In this case, the sign of the factor in front of pchanges with q. We can write the best response piece-wise as:
BR1(q;α) =

[1 0]Tifc1−αγ(q)>0
{[ab]T:a,b≥0, a+b= 1}ifc1−αγ(q) = 0
[0 1]Tifc1−αγ(q)<0(102)
This same analysis can be applied to BR2(p;α). The NE is characterized by the sets where these two maps
intersect. The following table summarize the equilibria p∗,q∗, written as scalars for readability.
α≤c1
γmaxα∈/bracketleftig
c1
γmax,c1
γmin/bracketrightig
α>c1
γmin
α≤c2
γmax(1,1) (0,1) (0 ,1)
α∈/bracketleftig
c2
γmax,c2
γmin/bracketrightig
(1,0)/braceleftbig
(1,0),(0,1),/parenleftbig
γ−1/parenleftbigc1
α/parenrightbig
,γ/parenleftbigc2
α/parenrightbig/parenrightbig/bracerightbig
(0,1)
α>c2
γmin(1,0) (1,0) (0,0)
Whenαis below the threshold for the two users (the top left entry), both c1andc2are too small for it to be
worthwhile for the users to participate at the lower privacy option. Conversely, if αis above the threshold
for both users, then both users choose the less private option. When neither of these extremes occur the
results are more nuanced.
29