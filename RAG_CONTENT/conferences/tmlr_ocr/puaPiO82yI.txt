Under review as submission to TMLR
Using Stochastic Gradient Descent to Smooth Nonconvex
Functions: Analysis of Implicit Graduated Optimization with
Optimal Noise Scheduling
Anonymous authors
Paper under double-blind review
Abstract
The graduated optimization approach is a heuristic method for ﬁnding globally optimal so-
lutions for nonconvex functions and has been theoretically analyzed in several studies. This
paper deﬁnes a new family of nonconvex functions for graduated optimization, discusses
their suﬃcient conditions, and provides a convergence analysis of the graduated optimiza-
tion algorithm for them. It shows that stochastic gradient descent (SGD) with mini-batch
stochastic gradients has the eﬀect of smoothing the function, the degree of which is deter-
mined by the learning rate and batch size. This ﬁnding provides theoretical insights on why
large batch sizes fall into sharp local minima, why decaying learning rates and increasing
batch sizes are superior to ﬁxed learning rates and batch sizes, and what the optimal learning
rate scheduling is. To the best of our knowledge, this is the ﬁrst paper to provide a theoret-
ical explanation for these aspects. Moreover, a new graduated optimization framework that
uses a decaying learning rate and increasing batch size is analyzed and experimental results
of image classiﬁcation that support our theoretical ﬁndings are reported.
1 Introduction
1.1 Background
The amazing success of deep neural networks (DNN) in recent years has been based on optimization by
stochastic gradient descent (SGD) ( Robbins & Monro ,1951) and its variants, such as Adam (Kingma & Ba ,
2015). These methods have been widely studied for their convergence (Moulines & Bach ,2011;Needell et al. ,
2014)(Fehrman et al. ,2020;Bottou et al. ,2018;Scaman & Malherbe ,2020;Loizou et al. ,2021;Zaheer et al. ,
2018;Zou et al. ,2019;Chen et al. ,2019;Zhou et al. ,2020;Chen et al. ,2021;Iiduka ,2022) and stability
(Hardt et al. ,2016;Lin et al. ,2016;Mou et al. ,2018;He et al. ,2019) in nonconvex optimization.
SGD updates the parameters as xt+1:=xt−η∇fSt(xt), where ηis the learning rate and ∇fStis the
stochastic gradient estimated from the full gradient ∇fusing a mini-batch St. Therefore, there is only
anωt:=∇fSt(xt)− ∇f(xt)diﬀerence between the search direction of SGD and the true steepest descent
direction. Some studies claim that it is crucial in nonconvex optimization. For example, it has been proven
that noise helps the algorithm to escape local minima ( Ge et al. ,2015;Jin et al. ,2017;Daneshmand et al. ,
2018;Harshvardhan & Stich ,2021), achieve better generalization ( Hardt et al. ,2016;Mou et al. ,2018), and
to ﬁnd a local minimum with a small loss value in polynomial time under some assumptions ( Zhang et al. ,
2017).
Kleinberg et al. (2018) also suggests that noise smooths the objective function. Here, at time t, letytbe the
parameter updated by the gradient descent (GD) and xt+1be the parameter updated by SGD, i.e.,
yt:=xt−η∇f(xt),
xt+1:=xt−η∇fSt(xt)
=xt−η(∇f(xt) +ωt).
1Under review as submission to TMLR
Then, we obtain the following update rule for the sequence {yt},
Eωt[yt+1] =Eωt[yt]−η∇Eωt[f(yt−ηωt)], (1)
where fis Lipschitz continuous and diﬀerentiable. Therefore, if we deﬁne a new function ˆf(yt) :=Eωt[f(yt−
ηωt)],ˆfcan be smoothed by convolving fwith noise (see Deﬁnition 2.1, also Wu(1996)), and its parameters
ytcan approximately be viewed as being updated by using the gradient descent to minimize ˆf. In other
words, simply using SGD with a mini-batch smooths the function to some extent and may enable escapes
from local minima. (The derivation of equation ( 1) is in Section A.)
Graduated Optimization. Graduated optimization is one of the global optimization methods, which
searches for the global optimal solution of diﬃcult multimodal optimization problems. The method generates
a sequence of simpliﬁed optimization problems that gradually approach the original problem through diﬀerent
levels of local smoothing operations. Itsolves the easiest simpliﬁed problem ﬁrst, as it should have nice
properties such as convexity or strong convexity; after that, it uses that solution as the initial point for
solving the second-simplest problem, then the second solution as the initial point for solving the third-
simplest problem and so on, as it attempts to escape from local optimal solutions of the original problem
and reach a global optimal solution.
This idea was ﬁrst established as graduated non-convexity (GNC) by Blake & Zisserman (1987) and has
since been studied in the ﬁeld of computer vision for many years. Similar early approaches can be found
inWitkin et al. (1987) and Yuille (1989), and the same concept hasappeared in the ﬁelds of numerical
analysis ( Allgower & Georg ,1990) and optimization ( Rose et al. ,1990;Wu,1996). Over the past 25 years,
graduated optimization has been successfully applied to many tasks in computer vision, such as early vision
(Black & Rangarajan ,1996), image denoising ( Nikolova et al. ,2010), optical ﬂow ( Sun et al. ,2010;Brox &
Malik ,2011), dense correspondence of images ( Kim et al. ,2013), and robust estimation ( Yang et al. ,2020;
Antonante et al. ,2022;Peng et al. ,2023). In addition, it has been applied to certain tasks in machine
learning, such as semi-supervised learning ( Chapelle et al. ,2006;Sindhwani et al. ,2006;Chapelle et al. ,
2008), unsupervised learning ( Smith & Eisner ,2004), and ranking Chapelle & Wu (2010). Moreover, score-
based generative models ( Song & Ermon ,2019;Song et al. ,2021b ) and diﬀusion models ( Sohl-Dickstein
et al. ,2015;Ho et al. ,2020;Song et al. ,2021a ;Rombach et al. ,2022), which are currently state-of-the-art
generative models, implicitly use the techniques of graduated optimization. A comprehensive survey on the
graduated optimization approach can be found in ( Mobahi & Fisher III ,2015b ).
While graduated optimization is popular, there is not much theoretical analysis on it. Mobahi & Fisher III
(2015a ) performed the ﬁrst theoretical analysis, but they did not provide a practical algorithm. Hazan et al.
(2016) deﬁned a family of nonconvex functions satisfying certain conditions, called σ-nice, and proposed
a ﬁrst-order algorithm based on graduated optimization. In addition, they studied the convergence and
convergence rate of their algorithm to a global optimal solution for σ-nice functions. Iwakiri et al. (2022)
proposed a single-loop method that simultaneously updates the variable that deﬁnes the noise level and the
parameters of the problem and analyzed its convergence. Li et al. (2023) analyzed graduated optimization
based on a special smoothing operation. Note that Duchi et al. (2012) pioneered the theoretical analysis
of optimizers using Gaussian smoothing operations for nonsmooth convex optimization problems. Their
method of optimizing with decreasing noise level is truly a graduated optimization approach.
1.2 Motivation
Equation ( 1) indicates that SGD smooths the function (Kleinberg et al. ,2018), but it is not clear to what
extent the function is smoothed or what factors are involved in the smoothing. Therefore, we decided to
clarify these aspects and identify what parameters contribute to the smoothing.
Although Hazan et al. (2016) proposed a σ-nice function, it is unclear how special a nonconvex function
theσ-nice function is. In some cases, there may be no function that satisﬁes the σ-nice property. Here, we
decided totry to deﬁne and analyze a new family of functions with clear suﬃcient conditions as replacements
for the σ-nice function.
2Under review as submission to TMLR
<latexit sha1_base64="MytryU0PTzfMOANBLybnl9um4CQ=">AAACuXichVG7SgNBFD2ur/iO2gg2YohYhVkRFW2CNpYaTRSSEHbXUUf3xe4mGJf8gJWdqJWChfgZNnZWFvkEsYxgY+HdzYJoiM6yM2fO3HPnzrmqrQvXY6zeIXV2dff0xvr6BwaHhkfio2M51yo7Gs9qlm45u6ricl2YPOsJT+e7tsMVQ9X5jnq8FpzvVLjjCsvc9qo2LxrKgSn2haZ4ROUKquGf1ErxBEuxcEy1AjkCCURjw4o/o4A9WNBQhgEOEx5hHQpc+vKQwWATV4RPnENIhOccNfSTtkxRnCIUYo9pPqBdPmJN2gc53VCt0S06/Q4pp5BkL+yeNdgTe2Cv7LNtLj/MEdRSpVVtarldGjmb2Pr4V2XQ6uHwW/WHQqXo9m8KMuqEgvpFmNMInfjbBQ/7WApfL8gNO2QCX7RmRZXTi8bWcibpz7Bb9kaO3LA6eyRPzMq7drfJM9fUUPl3+1pBbi4lL6TmN+cT6dWotTFMYhqz1L9FpLGODWTp1iOc4xJX0oqkSIfSUTNU6og04/gxJPcLPX6dVQ==</latexit>xthe learning rate<latexit sha1_base64="C4URmJuEHtKjwhyI8FTgL8dlw1g=">AAACt3ichVG7SgNBFD1ZX/EdtRFsxKBYhYkEX1XQxtJXVNAQdteJGdyXu5NADP6AhW0EKwUL8TNs7Kws/ASxVLCx8O5kQTQkzrIzZ87cc+fOuYZniUAy9hLTOjq7unvivX39A4NDw4mR0Z3ALfsmz5mu5fp7hh5wSzg8J4W0+J7nc902LL5rHK+G57sV7gfCdbZl1eN5Wz9yRFGYugypAy71QiLJUkyNyWaQjkAS0Vh3E084wCFcmCjDBocDSdiCjoC+faTB4BGXR404n5BQ5xxn6CNtmaI4RejEHtN8RLv9iHVoH+YMlNqkWyz6fVJOYpo9szv2zh7ZPXtlXy1z1VSOsJYqrUZDy73C8Pn41ue/KptWidKPqo3CoOjWbwozWoTC+oXKaSsn2rsgUcSier0gNzzFhL6YjYoqp/X3reXN6doMu2Fv5Mg1e2EP5IlT+TBvN/jmFTU0/bd9zWBnLpWeT2U2MsnsStTaOCYwhVnq3wKyWMM6cnRrCReo41Jb0gpaUSs1QrVYpBnDr6GdfAOSnJw8</latexit>⌘the batch size<latexit sha1_base64="fbCiLlWDQWvYZQIUwXhl5A/rzNQ=">AAACtHichVG7SgNBFD2u73fURrARg2IVZkWiWIk2lomaB6jI7jjRIftidxKIwR/QVrGwUrAQP8PGzsrCTxBLBRsL724WREPiLDtz5sw9d+6ca3qWDBRjLx1aZ1d3T29f/8Dg0PDIaGJsPB+4FZ+LHHct1y+aRiAs6YicksoSRc8Xhm1aomCW18PzQlX4gXSdbVXzxJ5tHDqyJLmhiMqa+4kkS7FoTDcDPQZJxCPjJp6wiwO44KjAhoADRdiCgYC+Hehg8IjbQ504n5CMzgVOMEDaCkUJijCILdN8SLudmHVoH+YMIjWnWyz6fVJOY5Y9szv2zh7ZPXtlXy1z1aMcYS01Ws2GVnj7o6eTW5//qmxaFY5+VG0UJkW3flOY0SIU1i+jnHbkRHsXFEpYjl4vyQ0vYkJfeKOi6vHl+9bK5mx9jt2wN3Lkmr2wB/LEqX7w26zYvKKG6n/b1wzyCyk9nVrMLiZX1+LW9mEKM5in/i1hFRvIIEe3CpzhHBdaWtvVuCYaoVpHrJnAr6E531bHmuo=</latexit>blarge
0smalllargesmallthe degree of smoothing<latexit sha1_base64="W9kTd46OgurOvqB2HAs/1kYKbX8=">AAACv3ichVG7SgNBFD2u73fURrBZDIpVmIioWPloLE00KhgJu5tJHDL7YHcSiNEf8AO0sPABFuJn2NhZWfgJYqlgY+HdzYKoqLPszJkz99y5c67pSREoxh5btNa29o7Oru6e3r7+gcHE0PBm4FZ9i+csV7r+tmkEXAqH55RQkm97PjdsU/Its7ISnm/VuB8I19lQdY/v2kbZESVhGYqo/IGeL3KpjIKtHxQSSZZi0dB/gnQMkojHmpu4Rx5FuLBQhQ0OB4qwhIGAvh2kweARt4sGcT4hEZ1zHKKHtFWK4hRhEFuhuUy7nZh1aB/mDCK1RbdI+n1S6phgD+yavbA7dsOe2PuvuRpRjrCWOq1mU8u9wuDR6PrbvyqbVoW9T9UfCpOif39TmFESCusXUU47cuJvFxRKmI9eL8gNL2JCX6xmRbX9k5f1hexEY5Jdsmdy5II9slvyxKm9WlcZnj2lhqa/t+8n2JxOpWdTM5mZ5OJy3NoujGEcU9S/OSxiFWvI0a0ejnGGc21JK2uO5jVDtZZYM4IvQ6t/AGEEn2A=</latexit>| m|
<latexit sha1_base64="qHNvFBEhEDdT7ej4onnHVvYS3/I=">AAACuXichVG7SgNBFD1ZX/EdtRFsxBCxChMJKtoEbSzzMDGQiOyuo07cF7uTQAz+gJWdqJWChfgZNnZWFvkEsVSwsfDuZkFU1Fl25syZe+7cOVdzDOFJxtoRpau7p7cv2j8wODQ8MhobGy95dt3VeVG3Ddsta6rHDWHxohTS4GXH5aqpGXxTO1jzzzcb3PWEbW3IpsO3THXPErtCVyVRpaq+Y0tvOxZnSRaM6Z8gFYI4wpG1Yw+oYgc2dNRhgsOCJGxAhUdfBSkwOMRtoUWcS0gE5xxHGCBtnaI4RajEHtC8R7tKyFq093N6gVqnWwz6XVJOI8Ee2Q17Yffslj2x919ztYIcfi1NWrWOljvbo8eThbd/VSatEvufqj8UGkX//iY/o0HIr18EOc3Aib9dkNjFUvB6QW44AeP7oncqahyevhSW84nWLLtiz+TIJWuzO/LEarzq1zmev6CGpr637ycozSdTC8l0Lh3PrIatjWIKM5ij/i0ig3VkUaRbazjBGc6VFUVV9pVaJ1SJhJoJfBmK9wHunJ0z</latexit>···<latexit sha1_base64="a70UlIckkBFUB4G87w7BlW5YliQ=">AAACtHichVG7SgNBFD1ZX/GZqI1gEwyKVZiVEMVKtLE0xkRBRXbHiQ6ZfbC7CWjwB7RVLKwULMTPsLGzsvATxDKCjYV3Nwuios6yM2fO3HPnzrmmq6QfMPaU0Do6u7p7kr19/QODQ6n08EjFd+oeF2XuKMfbMA1fKGmLciADJTZcTxiWqcS6WVsKz9cbwvOlY68FB67Ytow9W1YlNwKiitWddJblWDQyP4EegyziseKkH7CFXTjgqMOCgI2AsIIBn75N6GBwidtGkziPkIzOBY7QR9o6RQmKMIit0bxHu82YtWkf5vQjNadbFP0eKTOYZI/shrXYPbtlz+z911zNKEdYywGtZlsr3J3U8Vjp7V+VRWuA/U/VHwqTon9/U5hREQrrl1FOK3LibxcCVDEXvV6SG27EhL7wdkWNw/NWaX51sjnFrtgLOXLJntgdeWI3Xvl1UaxeUEP17+37CSozOb2Qyxfz2YXFuLVJjGMC09S/WSxgGSso060CJzjFmVbQtjSuiXaolog1o/gyNPsDYAea7g==</latexit>f
<latexit sha1_base64="8I7FvNGtb++AT6j7ntDAO7TG7IQ=">AAACx3ichVHLLgRBFD3aa7zH2EhsxIRYTWpEECthw0LiNUiMdLpbDRXVj3TXTNCZha0fsJBISATxGTZ2VhY+QSxJbCzc7ulEEFSnq06duufWrXNNT4pAMfZYp9U3NDY1p1pa29o7OrvS3ZnVwC37Fi9YrnT9ddMIuBQOLyihJF/3fG7YpuRr5u5MdL5W4X4gXGdF7Xt80za2HVESlqGI0tOZ4o6hwlJVD4tbXCpDn6/q6SzLsXj0/wT5BGSRjAU3fY8ituDCQhk2OBwowhIGAvo2kAeDR9wmQuJ8QiI+56iilbRliuIUYRC7S/M27TYS1qF9lDOI1RbdIun3SdmPQfbArtkLu2M37Im9/5orjHNEtezTata03NO7jnqX3/5V2bQq7Hyq/lCYFP37m6KMklBUv4hz2rETf7ugUMJE/HpBbngxE/li1SqqHBy/LE8uDYZD7Jw9kyNn7JHdkidO5dW6WORLJ9TQ/Pf2/QSrI7n8WG50cTQ7NZ20NoU+DGCY+jeOKcxiAQW6dQ+nuMSVNqe5WkXbq4VqdYmmB1+GdvgB1PCiww==</latexit>ˆf M
<latexit sha1_base64="XlfgmOMcIC6rcgpsvc1tCWlat80=">AAACx3ichVG7SsRAFD3G93vVRrARF8VqmRVRsVq00c7XquBKSOKsDjt5kMwurmELW3/AQhAURMXPsLGzsvATxFLBxsKbbEBU1AmZOXPmnjt3zjU9KQLF2GOD1tjU3NLa1t7R2dXd05vq618P3LJv8bzlStffNI2AS+HwvBJK8k3P54ZtSr5hluaj840K9wPhOmuq6vFt29h1RFFYhiJKT/UX9gwVFmt6WNjhUhn6RE1PpVmGxWP4J8gmII1kLLmpexSwAxcWyrDB4UARljAQ0LeFLBg84rYREucTEvE5Rw0dpC1TFKcIg9gSzbu020pYh/ZRziBWW3SLpN8n5TBG2QO7Zi/sjt2wJ/b+a64wzhHVUqXVrGu5p/ceDa6+/auyaVXY+1T9oTAp+vc3RRkloah+Eee0Yyf+dkGhiJn49YLc8GIm8sWqV1Q5OH5ZnV0ZDcfYOXsmR87YI7slT5zKq3WxzFdOqKHZ7+37CdYnMtmpzOTyZDo3l7S2DUMYwTj1bxo5LGAJebp1H6e4xJW2qLlaRduvh2oNiWYAX4Z2+AGWZaKo</latexit>ˆf 2
<latexit sha1_base64="5nf67s1tsojvPE4zJAisRH0zTps=">AAACx3ichVHLLgRBFD3a+z3YSGzEhFhNqkUQK2HDzmuQGOl0txoqU/1Id80EnVnY+gELiYREEJ9hY2dl4RPEksTGwu2eTgRBdbrq1Kl7bt061/KlCBVjj3VafUNjU3NLa1t7R2dXd6andy30yoHN87YnvWDDMkMuhcvzSijJN/yAm44l+bpVmovP1ys8CIXnrqp9n2855o4risI2FVFGprewa6qoWDWiwjaXyjT0qpHJshxLxuBPoKcgi3Qsepl7FLANDzbKcMDhQhGWMBHStwkdDD5xW4iICwiJ5JyjijbSlimKU4RJbInmHdptpqxL+zhnmKhtukXSH5ByEMPsgV2zF3bHbtgTe/81V5TkiGvZp9WqablvdB/1r7z9q3JoVdj9VP2hsCj69zfFGSWhuH6R5HQSJ/52QaGIqeT1gtzwEyb2xa5VVDk4flmZXh6ORtg5eyZHztgjuyVP3MqrfbHEl0+oofr39v0Ea2M5fSI3vjSenZlNW9uCAQxhlPo3iRnMYxF5unUPp7jElbageVpF26uFanWppg9fhnb4AZQUoqc=</latexit>ˆf 1
<latexit sha1_base64="5WmrttDee4vWbv0oQLE3vb+jc9c=">AAACv3ichVG7SgNBFD1Z389EbQQbMShWYSKiYuWjsdTERMGo7K6TOGRf7E6CcfEH/AAtLHyAhfgZNnZWFvkEsYxgY+HdzYKoGGfZmTNn7rlz51zNMYQnGavHlLb2js6u7p7evv6BwXhiaDjv2RVX5zndNmx3W1M9bgiL56SQBt92XK6amsG3tPJqcL5V5a4nbGtT1hy+a6olSxSFrkqiCgXN9I9O9gqeVN39RJKlWDjGf4N0BJKIxrqdeEIBB7ChowITHBYkYQMqPPp2kAaDQ9wufOJcQiI85zhBL2krFMUpQiW2THOJdjsRa9E+yOmFap1uMeh3STmOSfbM7liDPbJ79sI+/szlhzmCWmq0ak0td/bjp6PZ939VJq0Sh1+qFgqNov9+U5DRIBTUL8KcZuhEaxckilgIXy/IDSdkAl/0ZkXV4/NGdjEz6U+xG/ZKjlyzOnsgT6zqm367wTMX1ND0z/b9BvmZVHouNbsxm1xaiVrbjTFMYJr6N48lrGEdObrVwRkucaUsKyXFUpxmqBKLNCP4NpTaJ+IcoAU=</latexit>x?large
<latexit sha1_base64="DBvwizuhgnthxH0PiU2edVby4DA=">AAACzHichVFNLwNBGH67vuurOJC4NBri1MyKIE7CxUlQbSXKZneNmnT2I7vTBpteHfwBByfEQfArXNycHPoTxLESFwfvTjcRmjKbnXnmmfd5553nNVzOfEFILaa0tXd0dnX3xHv7+gcGE0PDOd8peybNmg53vG1D9ylnNs0KJjjddj2qWwaneaO0Ep7nK9TzmWNviWOX7lp60WYHzNQFUlpitGBYwVFVCwr7lAtdU6t7BV/onpZIkTSRI9kM1AikIBrrTuIZCrAPDphQBgso2CAQc9DBx28HVCDgIrcLAXIeIibPKVQhjtoyRlGM0JEt4VzE3U7E2rgPc/pSbeItHH8PlUmYJC/kltTJE7kjr+SzZa5A5ghrOcbVaGipqw2ejWU+/lVZuAo4/Fb9oTAwuvWbwowcUVg/kzkt6cTfLgg4gAX5eoZuuJIJfTEbFVVOzuuZxc3JYIpckTd05JLUyCN6YlfezZsNunmBDVV/t68Z5GbS6lx6dmM2tbQctbYbxmECprF/87AEq7AOWXnrNdzDg7KmCCVQqo1QJRZpRuDHUE6/AFK4pPE=</latexit>x? 1
<latexit sha1_base64="Ye/tnolbUzxuW67eRZQl9u1TAUE=">AAACzHichVFNLwNBGH67vqq+igOJS6MhTs1UGsSp4eIktKpN2trsrsHE7Ed2p43a9OrgDzg4IQ6CX+Hi5uTQnyCOJC4O3p1uIghmszPPPPM+77zzvLrDmScIaUWUjs6u7p5ob6yvf2BwKD48sunZNdegBcPmtlvSNY9yZtGCYILTkuNSzdQ5Ler7y8F5sU5dj9nWhmg4tGpquxbbYYYmkFLjYxXd9A+aql/Zplxo6mxzq+IJzVXjSZIiciR+gnQIkhCONTv+ABXYBhsMqIEJFCwQiDlo4OFXhjQQcJCrgo+ci4jJcwpNiKG2hlEUIzRk93HexV05ZC3cBzk9qTbwFo6/i8oETJFHckVeyD25Jk/k/ddcvswR1NLAVW9rqaMOHY/n3/5VmbgK2PtU/aHQMfr3NwUZOaKgfiZzmtKJv10QsAML8vUM3XAkE/hitCuqH5685BdzU/40OSfP6MgZaZE79MSqvxqX6zR3ig1Nf2/fT7A5m0rPpTLrmWR2KWxtFCZgEmawf/OQhRVYg4K89QJu4FZZVYTiK812qBIJNaPwZShHH1UPpPI=</latexit>x? 2
<latexit sha1_base64="5VDH5929+OX7alZL/GSpa7sEXDI=">AAACzHichVFNLwNBGH67vuurOJC4NJqKUzMVQZyEiwvRVluJ1mZ3TZmY/cjutMGmVwd/wMEJcRD8Chc3J4f+BHGsxMXBu9NNBMFsduaZZ97nnXeeV3c48wQhjYjS1t7R2dXdE+3t6x8YjA0NFzy76ho0b9jcdjd1zaOcWTQvmOB003GpZuqcFvX95eC8WKOux2xrQxw6tGxquxarMEMTSKmx0ZJu+gd11S/tUC40dbW+XfKE5qqxBEkROeI/QToECQjHuh17hBLsgA0GVMEEChYIxBw08PDbgjQQcJArg4+ci4jJcwp1iKK2ilEUIzRk93Hexd1WyFq4D3J6Um3gLRx/F5VxSJInck2a5IHckGfy/msuX+YIajnEVW9pqaMOnozl3v5VmbgK2PtU/aHQMfr3NwUZOaKgfiZzmtKJv10QUIF5+XqGbjiSCXwxWhXVjk6buYVs0p8kF+QFHTknDXKPnli1V+MqQ7Nn2ND09/b9BIXpVHo2NZOZSSwuha3thnGYgCns3xwswgqsQ17eegm3cKesKULxlXorVImEmhH4MpTjD5Q8pQ0=</latexit>x? M<latexit sha1_base64="QhHoRI/PkCXmOgPIfB1OXYfXXrI=">AAACxnichVG7SgNBFD2u72cSbQQbMSixCRMJKtoEbSx9RQUTw+460cF9sTtJ1BCw9gcsxEJBRfwMGzsrCz9BLBVsLLy7WRAN6iw7c+bMPXfunKs5hvAkY09NSnNLa1t7R2dXd09vXyQa61/z7JKr86xuG7a7oakeN4TFs1JIg284LldNzeDr2t68f75e5q4nbGtVHjg8b6o7ligKXZVEFaKxuUROM6v7ta2cJ1V31h0vROMsyYIx3AhSIYgjHIt29AE5bMOGjhJMcFiQhA2o8OjbRAoMDnF5VIlzCYngnKOGLtKWKIpThErsHs07tNsMWYv2fk4vUOt0i0G/S8phjLJHdsNe2T27Zc/s49dc1SCHX8sBrVpdy51C5Hhw5f1flUmrxO6X6g+FRtG/v8nPaBDy6xdBTjNw4m8XJIqYDl4vyA0nYHxf9HpF5cOT15WZ5dHqGLtgL+TIOXtid+SJVX7TL5f48ik1NPWzfY1gbSKZmkyml9LxzFzY2g4MYQQJ6t8UMljAIrJ0awVnuMK1sqBYSkmp1EOVplAzgG9DOfoEKo2hqA==</latexit>B(x?;r)
<latexit sha1_base64="t6v9IWUp4OdusIyytmZvUaODZUA=">AAAC1HicSyrIySwuMTC4ycjEzMLKxs7BycXNw8vHLyAoFFacX1qUnBqanJ+TXxSRlFicmpOZlxpaklmSkxpRUJSamJuUkxqelO0Mkg8vSy0qzszPCympLEiNzU1Mz8tMy0xOLAEKxQvI+2nEJOVWV9TGxRSXJBZZK6TEGyrUxKSk5pQkxhvWaMYLKBvoGYCBAibDEMpQZoCCgHyB8wwxDCkM+QzJDKUMuQypDHkMJUB2DkMiQzEQRjMYMhgwFADFYhmqgWJFQFYmWD6VoZaBC6i3FKgqFagiESiaDSTTgbxoqGgekA8ysxisOxloSw4QFwF1KjCoGlw1WGnw2eCEwWqDlwZ/cJpVDTYD5JZKIJ0E0ZtaEM/fJRH8naCuXCBdwpCB0IVHRxJQNW4/gUzMAbJA7s8Em5kLDgn8oVDCkMZgAfZ9JjA0CsAioHBJhriorGr652CrINVqNYNFBq+BIbLQ4KbBYWCY5JV9SV4amBo0GxihhujRh8kIM9IzNNMzCTRRdnCCRi0HgzSDEoMGMP7MGRwYPBgCGEKBtrYyrGbYwrCVKYyphqmZqRWilIkRqkeYAQUw9QEAqL+m8A==</latexit>N(x?;d1| 1|)
<latexit sha1_base64="p0pvudGJcVuGE5wiY3I4EUrN93g=">AAAC1HicSyrIySwuMTC4ycjEzMLKxs7BycXNw8vHLyAoFFacX1qUnBqanJ+TXxSRlFicmpOZlxpaklmSkxpRUJSamJuUkxqelO0Mkg8vSy0qzszPCympLEiNzU1Mz8tMy0xOLAEKxQvI+2nEJOVWV9TGxRSXJBZZK6TEGynUxKSk5pQkxhvVaMYLKBvoGYCBAibDEMpQZoCCgHyB8wwxDCkM+QzJDKUMuQypDHkMJUB2DkMiQzEQRjMYMhgwFADFYhmqgWJFQFYmWD6VoZaBC6i3FKgqFagiESiaDSTTgbxoqGgekA8ysxisOxloSw4QFwF1KjCoGlw1WGnw2eCEwWqDlwZ/cJpVDTYD5JZKIJ0E0ZtaEM/fJRH8naCuXCBdwpCB0IVHRxJQNW4/gUzMAbJA7s8Em5kLDgn8oVDCkMZgAfZ9JjA0CsAioHBJhriorGr652CrINVqNYNFBq+BIbLQ4KbBYWCY5JV9SV4amBo0GxihhujRh8kIM9IzNNMzCTRRdnCCRi0HgzSDEoMGMP7MGRwYPBgCGEKBtrYyrGbYwrCVKYyphqmZqRWilIkRqkeYAQUw9QEArW2m8g==</latexit>N(x?;d2| 2|)
<latexit sha1_base64="+GAHjPWFAYXHWlJfrfZD/VKdwe0=">AAAC1HichVG7ThtBFD0sCc8ABhqkFLGwQNBYY4QAQWORhsaIR2yQMKx21wOMmH1od2wBtqvIDQUtRSqQKBAt+YI0dFQp+IQoJUg0KXJ3vRICBMxqZ86cuefOnXNNT4pAMXbborV++NjW3tHZ1f2pp7cv0T9QCNyyb/G85UrXXzeNgEvh8LwSSvJ1z+eGbUq+Zu59Dc/XKtwPhOt8Uwce37SNHUdsC8tQROmJL4tjRdOu7te3ioEy/LlkSc8la8USl8rQc7VxPZFiaRaN5EuQiUEK8VhyEzcoogQXFsqwweFAEZYwENC3gQwYPOI2USXOJySic446ukhbpihOEQaxezTv0G4jZh3ahzmDSG3RLZJ+n5RJjLDf7ILdsWt2yf6wf6/mqkY5wloOaDWbWu7pfUdDqw/vqmxaFXYfVW8oTIp+/U1hRkkorF9EOe3IibddUNjGTPR6QW54ERP6YjUrqhye3K3OroxUR9kZ+0uOnLJb9os8cSr31vkyX/lBDc08b99LUJhIZ6bSk8uTqex83NoOfMYwxqh/08hiAUvI060NXOIKP7WCVtO+a41mqNYSawbxZGjH/wEr1qco</latexit>N(x?;dM| M|)
<latexit sha1_base64="ZEkpUei2GO1p3vKjB3Pi0xul78U=">AAACu3ichVG7SgNBFL1Z3/GRqI1gEwyKVZgVUREE0cbSVx4QQ9gdJ3HM7IPdSVCDP2BpY6GNgoX4GTZ2Vhb5BLGMYGPh3cmCaEi8y+6cOXPP3Tvnmq7gviSkEdF6evv6BwaHosMjo2Ox+PhExneqHmVp6gjHy5mGzwS3WVpyKVjO9ZhhmYJlzcpmcJ6tMc/njr0vT11WsIyyzUucGhKp3IFp1U/Oi3oxniQpoiLRDvQQJCGMbSf+AgdwCA5QqIIFDGyQiAUY4OOTBx0IuMgVoI6ch4ircwbnEEVtFbMYZhjIVvBbxl0+ZG3cBzV9pab4F4Gvh8oEzJJX8kCa5Jk8kjfy1bFWXdUIejnF1WxpmVuMXUztff6rsnCVcPSj6qIwMbvznYKKAlHQP1c1LeVEdxcklGBF3Z6jG65iAl9oq6Pa2VVzb3V3tj5H7sg7OnJLGuQJPbFrH/R+h+1e40D1v+NrB5mFlL6UWtxZTK5vhKMdhGmYgXmc3zKswxZsQ1rN7RKu4UZb06h2rIlWqhYJNZPwK7TqN9uYnfk=</latexit>x1
Figure 1: Conceptual diagram of new σ-nice function and its smoothed versions (see also the Notation 1).
In graduated optimization, the noise level is gradually reduced, eventually arriving at the original function,
but there are an inﬁnite number of ways to reduce the noise. For better optimization, the choice of noise
scheduling is a very important issue. Therefore, we also aimed to clarify the optimal noise scheduling
theoretically.
Once it is known what parameters of SGD contribute to smoothing and the optimal noise scheduling, an
implicit graduated optimization can be achieved by varying the parameters so that the noise level is optimally
reduced gradually. Our goal was thus to construct an implicit graduated optimization framework using the
smoothing properties of SGD to achieve global optimization of deep neural networks.
1.3 Contributions
1.3.1 SGD’s Smoothing Property
We show that the degree of smoothing by SGD depends on the ratioη√
bbetween the batch size and the
learning rate. Accordingly, the smaller the batch size band the larger the learning rate ηare, the more
smoothed the function becomes (see Figure 1). Also, we can say that halving the learning rate is the same
as quadrupling the batch size. ( Goyal et al. ,2017;Smith et al. ,2018;Xie et al. ,2021) also studied SGD
dynamics and demonstrated how the ratioη
baﬀect training dynamics. Note that our theory includes and
does not conﬂict with their results.
1.3.2 Why the Use of Large Batch Sizes Leads to Solutions Falling into Sharp Local Minima
In other words, from a smoothing perspective, if we use a large batch size and/or a small learning rate,
it is easy for the algorithm to fall into a sharp local minimum and experience a drop in generalization
performance, since it will optimize a function that is close to the original multimodal function. As is well
known, training with a large batch size leads to convergence to sharp local minima and poor generalization
performance, as evidenced by the fact that several prior studies ( Hoﬀer et al. ,2017;Goyal et al. ,2017;You
et al. ,2020) provided techniques that do not impair generalization performance even with large batch sizes.
Keskar et al. (2017) showed this experimentally, and our results provide theoretical support for it.
3Under review as submission to TMLR
1.3.3 Why Using Decaying Learning Rates and Increasing Batch Sizes is Superior to Using Fixed
Ones
Moreover, we can say that decreasing the learning rate and/or increasing the batch size during training is
indeed an implicit graduated optimization. Hence, using a decaying learning rate and increasing the batch
size makes sense in terms of avoiding local minima. Our results provide theoretical support for the many
positive ﬁndings on using decaying learning rates ( Wu et al. ,2014;Ioﬀe & Szegedy ,2015;Loshchilov &
Hutter ,2017;Hundt et al. ,2019;You et al. ,2019;Hundt et al. ,2019;Lewkowycz ,2021) and increasing
batch sizes ( Byrd et al. ,2012;Friedlander & Schmidt ,2012;Balles et al. ,2017;De et al. ,2017;Bottou et al. ,
2018;Smith et al. ,2018).
1.3.4 New σ-nice Function
We propose a new σ-nice function that generalizes the σ-nice function. All smoothed functions of the new
σ-nice function are σ-strongly convex in a neighborhood B(x⋆;dm|δm|)of the optimal solution x⋆that is
proportional to the noise level |δm|(see Figure 1). In contrast to Hazan et al. (2016), we show suﬃcient
conditions for a certain nonconvex function fto be a new σ-nice function as follows:
2Lgmax/braceleftBig/vextenddouble/vextenddoublex⋆
δm−x⋆/vextenddouble/vextenddouble,/vextenddouble/vextenddouble/vextenddoublex⋆
δm+1−x⋆/vextenddouble/vextenddouble/vextenddouble/bracerightBig
σ(1−γm)≤ |δm|=/vextendsingle/vextendsingleδ−
m/vextendsingle/vextendsingle.
where |δ−
m|>0, dm+1>1, γm∈/parenleftBig
1
dm+1,1/parenrightBig
, and m∈[M]⊂N.δmis the noise level of ˆfδm, which is a
smoothed version of f, and x⋆is the global optimal solution of the original function f. Furthermore, we
show that the graduated optimization algorithm for the Lf-Lipschitz new σ-nice function converges to an
ϵ-neighborhood of the globally optimal solution in O/parenleftBig
1/ϵ1
p+2/parenrightBig
(p∈(0,1])rounds.
1.3.5 Optimal Noise Scheduling
Let|δm|be the current noise level, and let the next noise level be determined by |δm+1|:=γm|δm|, where
γmis the decay rate of noise. We show theoretically that γmshould decay slowly from a value close to 1
for convergence to the globally optimal solution. To the best of our knowledge, ours is the ﬁrst paper to
provide theoretical results on optimal scheduling, both in terms of how to reduce the noise in graduated
optimization and how to decay the learning rate and increase the batch size in general optimization. Noise
scheduling also has an important role in score-based models ( Song & Ermon ,2020), diﬀusion models ( Chen ,
2023), panoptic segmentation ( Chen et al. ,2023), etc., so our theoretical ﬁndings will contribute to these
methodologies as well.
Furthermore, since the decay rate of noise in graduated optimization is equivalent to the decay rate of the
learning rate and rate of increase in batch size, we can say that it is desirable to vary them gradually from
a value close to 1. As for the schedule for decaying the learning rate, many previous studies have tried
cosine annealing (without restart) ( Loshchilov & Hutter ,2017), cosine power annealing ( Hundt et al. ,2019),
or polynomial decay ( Liu et al. ,2015;Chen et al. ,2018;Zhao et al. ,2017;Chen et al. ,2017), but it has
remained unclear why they are superior to ﬁxed rates. We provide theoretical support showing why they
are experimentally superior. In particular, we show that a polynomial decay with a power less than or equal
to 1 is the optimal learning rate schedule and demonstrate this in Section 4.
1.3.6 Implicit Graduated Optimization
We propose a new implicit graduated optimization algorithm. The algorithm decreases the learning rate
of SGD and increases the batch size during training. We show that the algorithm for the Lf-Lipschitz
newσ-nice function converges to an ϵ-neighborhood of the globally optimal solution in O/parenleftBig
1/ϵ1
p/parenrightBig
(p∈(0,1])
rounds. In Section 4, we show experimentally that methods that reduce noise outperform methods that use
a constant learning rate and constant batch size. We also ﬁnd that methods which increase the batch size
outperform those which decrease the learning rate when the decay rate of the noise is set at 1/√
2.
4Under review as submission to TMLR
2 Preliminaries
2.1 Deﬁnitions and Notation
The notation used in this paper is summarized in Table 1.
Table 1: Notation List
Notation Description
N The set of all nonnegative integers
[N] [N] :={1,2, . . . , N }(N∈N\{0})
RdAd-dimensional Euclidean space with inner product ⟨·,·⟩, which induces the norm ∥ · ∥
Eξ[X] The expectation with respect to ξof a random variable X
St Mini-batch of bsamples ziat time t
N(x⋆;ϵ) ϵ-neighborhood of a vector x⋆, i.e., N(x⋆;ϵ) :=/braceleftbig
x∈Rd:∥x−x⋆∥< ϵ/bracerightbig
B(x⋆;r) The Euclidian closed ball of radius rcentered at x⋆, i.e., B(x⋆;r) :=/braceleftbig
x∈Rd:∥x−x⋆∥ ≤r/bracerightbig
u∼B(x⋆;r)A random variable distributed uniformly over B(x⋆;r)
M The number of smoothed functions, i.e., M∈N
m Counts from the smoothest function, i.e., m∈[M]
δ The degree of smoothing of the smoothed function, i.e., δ∈R
δm The degree of smoothing of the m-th smoothed function, i.e., δm∈R
ˆfδ The function obtained by smoothing fwith a noise level δ
ˆfδm Them-th smoothed function obtained by smoothing fwith a noise level δm
xm+1 xm+1is deﬁned by ˆfδm(xm+1)≤ˆfδm(ˆxt), where (ˆxt)TF+1
t=1 is generated by GD
fi(x) A loss function for x∈Rdandzi
f(x) The total loss function for x∈Rd, i.e., f(x) :=|S|−1/summationtext
i∈Sfi(x)
ξ A random variable supported on Ξthat does not depend on x∈Rd
ξt ξ0, ξ1, . . .are independent samples and ξtis independent of (xk)t
k=0⊂Rd
ξt,i A random variable generated from the i-th sampling at time t
Gξt(x) The stochastic gradient of f(·)atx∈Rd
∇fSt(xt) The mini-batch stochastic gradient of f(xt)forSt, i.e., ∇fSt(xt) :=b−1/summationtext
i∈[b]Gξt,i(xt)
Deﬁnition 2.1 (Smoothed function) .Given an Lf-Lipschitz function f, deﬁne ˆfδto be the function obtained
by smoothing fas
ˆfδ(x) :=Eu∼B(0;1)[f(x−δu)], (2)
where δ∈Rrepresents the degree of smoothing and uis a random variable distributed uniformly over B(0; 1).
Also,
x⋆:= argmin
x∈Rdf(x)andx⋆
δ:= argmin
x∈Rdˆfδ(x).
Remark: For a general smoothing as in Deﬁnition 2.1, the distribution followed by the random variable
uneed not necessarily be uniform; it can be a normal distribution. In fact, several previous studies ( Wu,
1996;Iwakiri et al. ,2022) assumed that ufollows a normal distribution. Here, we assume that it follows a
uniform distribution because this is necessary for the analysis of the new σ-nice function. This is also true
for the analysis of the σ-nice function ( Hazan et al. ,2016).
There are a total of Msmoothed functions in this paper. The largest noise level is δ1and the smallest noise
level is δM+1= 0. Thus, ˆfδM+1=f.
Deﬁnition 2.2 (σ-nice function ( Hazan et al. ,2016)).A function f:Rd→Ris said to be σ-nice if the
following two conditions hold:
(i) For every δ > 0and every x⋆
δ, there exists x⋆
δ/2such that:
/vextenddouble/vextenddouble/vextenddoublex⋆
δ−x⋆
δ/2/vextenddouble/vextenddouble/vextenddouble≤δ
2.
5Under review as submission to TMLR
(ii) For every δ > 0, letrδ= 3δ; then, the function ˆfδ(x)over N(x⋆
δ;rδ)isσ-strongly convex.
Theσ-nice property implies that optimizing the smoothed function ˆfδis a good start for optimizing the next
smoothed function ˆfδ/2, which has been shown to be suﬃcient for graduated optimization ( Hazan et al. ,
2016).
2.2 Assumptions and Lemmas
We make the following assumptions:
Assumption 2.1. (A1) f:Rd→Ris continuously diﬀerentiable and Lg-smooth, i.e., for all x,y∈Rd,
∥∇f(x)− ∇f(y)∥ ≤Lg∥x−y∥.
(A2) f:Rd→RisLf-Lipschitz function, i.e., for all x,y∈Rd,
|f(x)−f(y)| ≤Lf∥x−y∥.
(A3) Let (xt)t∈N⊂Rdbe the sequence generated by SGD .
(i) For each iteration t,
Eξt[Gξt(xt)] =∇f(xt).
(ii) There exists a nonnegative constant C2such that
Eξt/bracketleftbig
∥Gξt(xt)− ∇f(xt)∥2/bracketrightbig
≤C2.
(A4) For each iteration t,SGD samples a mini-batch St⊂ S and estimates the full gradient ∇fas
∇fSt(xt) :=1
b/summationdisplay
i∈[b]Gξt,i(xt) =1
b/summationdisplay
{i:zi∈St}∇fi(xt).
Lemma 2.1. Suppose that (A3)(ii) and (A4) hold for all t∈N; then,
Eξt/bracketleftbig
∥∇fSt(xt)− ∇f(xt)∥2/bracketrightbig
≤C2
b.
The proof of Lemma 2.1can be found in Appendix B.1.
The following Lemmas concern the properties of smoothed functions ˆfδ. See Appendix Bfor their proofs.
Lemma 2.2. Suppose that (A1) holds; then, ˆfδdeﬁned by ( 2) is also Lg-smooth; i.e., for all x,y∈Rd,
/vextenddouble/vextenddouble/vextenddouble∇ˆfδ(x)− ∇ ˆfδ(y)/vextenddouble/vextenddouble/vextenddouble≤Lg∥x−y∥.
Lemma 2.3. Suppose that (A2) holds; then ˆfδis also an Lf-Lipschitz function; i.e., for all x,y∈Rd,
/vextendsingle/vextendsingle/vextendsingleˆfδ(x)−ˆfδ(y)/vextendsingle/vextendsingle/vextendsingle≤Lf∥x−y∥.
Lemmas 2.2and2.3imply that the Lipschitz constants Lfof the original function fandLgof∇fare taken
over by the smoothed function ˆfδand its gradient ∇ˆfδfor all δ∈R.
Lemma 2.4. Let ˆfδbe the smoothed version of f; then, for all x∈Rd,
/vextendsingle/vextendsingle/vextendsingleˆfδ(x)−f(x)/vextendsingle/vextendsingle/vextendsingle≤ |δ|Lf.
6Under review as submission to TMLR
Lemma 2.4implies that the larger the degree of smoothing is, the further away the smoothed function is
from the original function. Since the degree of smoothing is determined by the learning rate and batch size
(see Section 3.3), we can say that the optimal value obtained by using a large learning rate and/or small
batch size may be larger than the optimal value obtained by using a small learning rate and/or large batch
size. When decreasing the learning rate or increasing the batch size, the sharp decrease in function values
at that time depends on the change in the objective function (see also Figure 1), and this phenomenon is
especially noticeable in schedules that use the same noise level for multiple epochs, such as the step decay
learning rate (see Figures 7-9).
3 Main Results
3.1 New σ-nice function
Since the deﬁnition of the σ-nice function is inappropriate for large noise levels (see Section 3.2), we generalize
theσ-nice function and deﬁne a new σ-nice function that can be deﬁned even when the noise level is large.
Deﬁnition 3.1. Letδ1∈R.A function f:Rd→Ris said to be “new σ-nice” if the following two conditions
hold :
(i) For all m∈[M]and all γm∈(0,1), there exist δm∈Rwith|δm+1|:=γm|δm|andx⋆
δmsuch that
/vextenddouble/vextenddouble/vextenddoublex⋆
δm−x⋆
δm+1/vextenddouble/vextenddouble/vextenddouble≤ |δm| − |δm+1|.
(ii) For all m∈[M]and all γm∈(0,1), there exist δm∈Rwith|δm+1|:=γm|δm|anddm>1such that the
function ˆfδm(x)isσ-strongly convex on N(x⋆;dmδm).
The value δm∈Rin Deﬁnition 3.1is the degree of smoothing or noise level (see Deﬁnition 2.1) and γm∈(0,1)
is the decay rate of the noise, i.e., γm:=|δm+1|/|δm|.In the deﬁnition of the σ-nice function (Deﬁnition
2.2),γmis always 0.5. We have extended this condition to γm∈(0,1).We can show that, for the graduated
optimization algorithm to be successful, γmrequires a certain lower bound, which provides important insights
into the optimal noise scheduling (see Section 3.2).
The next propositions provide a suﬃcient condition for the function fto be a new σ-nice function. The
proofs of Propositions 3.1and3.2are in Section D.5andD.6, respectively.
Proposition 3.1. Letam>√
2for all m∈[M].Suppose that the function f:Rd→Risσ-strongly convex
onB(x⋆;r)for suﬃciently small r > 0and the noise level |δm|satisﬁes |δm|=|δ−
m|; then, the smoothed
function ˆfδmisσ-strongly convex on N(x⋆;amr), where
|δ−
m|:= sup
x∈N(x⋆;amr)\{x⋆}Eum∼B(0;1)/bracketleftBig/vextendsingle/vextendsingle/vextendsingle∥x⋆−x∥∥um∥cosθ−/radicalbig
∥x⋆−x∥2∥um∥2cos2θ−r2(a2m−1)/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
,
andθis the angle between umandx⋆−x.
Also, if we deﬁne dmasdm:=amr/|δ−
m|, then the smoothed function ˆfδmis also σ-strongly convex on
N(x⋆;dm|δm|).
Note that um∈Rdis a random variable used to deﬁne the smoothed function ˆfδm, which we assume follows a
uniform distribution (see Deﬁnition 2.1). In addition, am∈Ris only required for the analysis and (am)m∈[M]
is monotone decreasing. Proposition 3.1implies that the radius of the strongly convex region of the function
ˆfδmextends to amrif the sequence of noise (δm)m∈[M]added to the function fsatisﬁes |δm|=|δ−
m|for
allm∈[M]. Thus, if amr≥dm|δm|holds, then the smoothed function ˆfδmis also strongly convex in the
neighborhood N(x⋆;dm|δm|). Therefore, we deﬁne dm∈Rasdm:=amr/|δ−
m|.
Now, let us discuss dm. From the deﬁnition of dm,the lower and upper bounds of dmcan be expressed as
1<dm≤am/radicalbig
a2m−1−1. (3)
7Under review as submission to TMLR
Thus, the upper bound of dmgradually increases as amdecreases. The σ-nice function ( Hazan et al. ,2016)
always assumes dm= 3, but we see that this does not hold when amis large (see Figure 2in Section 3.2).
Proposition 3.2. Letdm>1for all m∈[M].Suppose that the function f:Rd→Risσ-strongly convex
andLg-smooth onB(x⋆;r)for suﬃciently small r > 0; a suﬃcient condition for fto be a new σ-nice
function is that the noise level |δm|satisﬁes the following condition :
For all m∈[M], suppose that x⋆
δm−1∈N(x⋆;dm|δm|),
2Lgmax/braceleftBig/vextenddouble/vextenddoublex⋆
δm−x⋆/vextenddouble/vextenddouble,/vextenddouble/vextenddouble/vextenddoublex⋆
δm+1−x⋆/vextenddouble/vextenddouble/vextenddouble/bracerightBig
σ(1−γm)≤ |δm|=/vextendsingle/vextendsingleδ−
m/vextendsingle/vextendsingle. (4)
Proposition 3.2shows that any function is a new σ-nice function if |δm|satisﬁes equations ( 4). Note that δ−
m
does not always exist. The probability p(am)thatδ−
mexists depends on the direction of the random variable
vector umand can be expressed as
0< p(am) :=arccos/parenleftBigg
r/radicalbig
a2m−1
∥x⋆−x∥∥um∥/parenrightBigg
π<arccos/parenleftBigg/radicalbig
a2m−1
am/parenrightBigg
π<1,
where r > 0, am>√
2,x∈N(x⋆;amr)\{x⋆}.Since the upper bound of p(am)approaches 0when amis
large, the probability p(am)approaches 0asamgets larger, but it never reaches 0. Therefore, the success of
Algorithm 1depends on the random variable um, especially when amis large, i.e., when δmis large.
The framework of graduated optimization for the new σ-nice function is shown in Algorithm 1. Algorithm
2is used to optimize each smoothed function.
Algorithm 1 Graduated Optimization
Require: ϵ >0, r∈(0,1), p∈(0,1],¯d > 0,x1, B2>0
δ1:=2Lg
σr
α0:= min/braceleftbigg
σr
8L2
f(1+¯d),√σr
2√
2Lg/bracerightbigg
, Mp:=1
α0ϵ
form= 1toM+ 1do
ifm̸=M+ 1then
ϵm:=σδ2
m, TF:= 2B2/σϵm
γm:=(M−m)p
{M−(m−1)}p
end if
xm+1:=GD(TF,xm,ˆfδm)
δm+1:=γmδm
end for
return xM+2
Algorithm 2 GD with decaying learning rate
Require: TF,ˆx1, F
fort= 1toTFdo
ηt:= 2/σt
ˆxt+1:=ˆxt−ηt∇F(ˆxt)
end for
return ˆxTF+1=GD(TF,ˆx1, F)
The smoothed function ˆfδmisσ-strongly convex in the neighborhood N(x⋆;dm|δm|). Thus, we should now
consider the convergence of GD for a σ-strongly convex function F=ˆfδm.Theorem 3.1is a convergence
analysis for when a decaying learning rate is used (The proof of Theorem 3.1is in Section D.1).
8Under review as submission to TMLR
Theorem 3.1 (Convergence analysis of Algorithm 2).Suppose that F:Rd→Ris aσ-strongly convex and
Lg-smooth function andηt:=2
σt. Then, the sequence (ˆxt)t∈Ngenerated by Algorithm 2satisﬁes
min
t∈[T](F(ˆxt)−F(x⋆))≤2B2
σT=O/parenleftbigg1
T/parenrightbigg
, (5)
where x⋆is the global minimizer of F, and B2>0is a nonnegative constant.
Theorem 3.1is the convergence analysis of Algorithm 2for any σ-strongly convex function F. It shows
that the algorithm can reach an ϵm-neighborhood of the optimal solution x⋆
δmofˆfδmin approximately
TF:= 2B2/σϵmiterations.
Remark: Algorithms 1and2represent explicit graduated optimization algorithms. Function smoothing
is accomplished explicitly by convolving random variables as in Deﬁnition 2.1, and the smoothed strongly
convex function is optimized by the gradient descent (Algorithm 2). However, in general, the integral
operation of the function fis not possible, so optimization by Algorithms 1and 2is not feasible. If
smoothing of the function fby Deﬁnition 2.1is possible and ˆfδis accessible, then Algorithm 2may be
SGD-type optimizer. For example, Algorithm 2can be the projected SGD generated by the sequence
(ˆxt)with ˆxt+1=Pm(ˆxt−ηt∇FSt(ˆxt)), where Pmis the projection onto B(x⋆;dmδm). Since ˆx0=x⋆
δm−1∈
B(x⋆;dmδm)is guaranteed by Proposition 3.3(ii), the sequence (ˆxt)generated by the projected SGD is always
inB(x⋆;dmδm). Using the proof of Theorem 3.1and the nonexpansivity of Pm(i.e.,∥Pm(x)−Pm(y)∥ ≤
∥x−y∥), we can show that the projected SGD satisﬁes
min
t∈[T]E[F(ˆxt)−F(x⋆)]≤2D2
σT,
where D1:= supt∈NE/bracketleftbig
∥∇F(ˆxt)∥2/bracketrightbig
andD2:=C2+D1.
The next theorem guarantees the convergence of Algorithm 1for the new σ-nice function (The proof of
Theorem 3.2is in Section D.2).
Theorem 3.2 (Convergence analysis of Algorithm 1).Letϵ∈(0,1)andfbe an Lf-Lipschitz new σ-nice
function. Suppose that we apply Algorithm 1; then, after O/parenleftBig
1/ϵ1
p+2/parenrightBig
rounds, the algorithm reaches an
ϵ-neighborhood of the global optimal solution x⋆.
Remark: In Algorithm 1and Theorem 3.2, we assume that we can access the full gradient of the smoothed
function ∇ˆfδm. Thus, our explicit graduated optimization by Algorithms 1and2is only valid for functions
ffor which the computation of ˆfδmby Deﬁnition 2.1and the access to its full gradient ∇ˆfδmare possible.
Hence, Algorithm 1and2are not applicable to DNN.
Note that Theorem 3.2provides a total complexity that integrates Algorithm 1and Algorithm 2because
Algorithm 1uses Algorithm 2at each m∈[M]. Theorem 3.2implies that convergence is faster when the
power of the polynomial decay pis large, and when p= 1, it takes at least O/parenleftbig
1/ϵ3/parenrightbig
rounds for new σ-nice
functions. Hazan et al. (2016) showed that their graduated optimization algorithm converges to a globally
optimal solution in O/parenleftbig
1/ϵ2/parenrightbig
iterations for a σ-nice function. However, explicit graduated optimization, such
as with our Algorithm 1and Algorithm 1 in Hazan et al. (2016), is not applicable to DNN due to the
impossibility of computing a smoothed function ˆfδm.
3.2 Optimal Noise Scheduling
The next proposition is crucial to the success of Algorithm 1(The proof is in Section D.7).
Proposition 3.3. Letdm>1for all m∈[M]and suppose that f:Rd→Ris a new σ-nice function.
(i) Then, the following always holds for all m∈[M],
/vextenddouble/vextenddoublex⋆
δm−x⋆/vextenddouble/vextenddouble< dm|δm|,
9Under review as submission to TMLR
(ii) If, γmsatisﬁes γm∈/parenleftBig
1
dm+1,1/parenrightBig
for all m∈[M], then the following holds for all m∈ {2,3,···, M},
/vextenddouble/vextenddouble/vextenddoublex⋆
δm−1−x⋆/vextenddouble/vextenddouble/vextenddouble< dm|δm|.
Proposition 3.3(i) implies that if the objective function fis the new σ-nice function, the optimal solution
x⋆
δmof the smoothed function ˆfδmis always contained in the σ-strongly convex region N(x⋆;dm|δm|)of
the function ˆfδm. Therefore, if the initial point of the optimization of the function ˆfδmis contained in the
σ-strongly convex region N(x⋆;dm|δm|), the sequence generated by Algorithm 2never leaves the σ-strongly
convex region. Also, assuming that Algorithm 2comes suﬃciently close to the optimal solution x⋆
δm−1after
more than TFiterations in the optimization of the ˆfδm−1,x⋆
δm−1is the initial point of the optimization of the
next function ˆfδm. Proposition 3.3(ii)therefore implies that the initial point of optimization of the function
ˆfδmis contained in the σ-strongly convex region of the function ˆfδm.Hence, Proposition 3.3guarantees
that if the initial point ¯x1of Algorithm 1is contained in the σ-strongly convex region N(x⋆;d1|δ1|)of the
smoothest function ˆfδ1, then the algorithm will always reach the globally optimal solution x⋆of the original
function f.Note that the decay rate γmused in Algorithm 1satisﬁes γm∈(1/dm+1,1). See the following
discussion.
2
 5 10 15 20
am0.11310am
a2
m11
a2
m11
am
dm
1
dm
Figure 2: The range sof possible values for dmand
1/dmarecolored blue and green, respectively. Note
that the vertical axis is logarithmic.
0 25 50 75 100 125 150 175 200
epoch0.000.020.040.060.080.10learning ratedecaying learning rate
cosine annealing
cosine power annealing with w=10
step decay
exponential decay
polynomial decay with p=0.5Figure 3: Existing decay learning rate versus number
of epochs. The schedule deﬁnitions are included in
equations ( 6) through ( 7).
According to Proposition 3.1, for a function to be a new σ-nice function, dmmust satisfy equation ( 3). Thus,
there is a range of possible values for 1/dm:
1<dm≤am/radicalbig
a2m−1−1and/radicalbig
a2m−1−1
am≤1
dm<1.
Therange is plotted in Figure 2. Recall that amis a value that appears only in the theoretical analysis and
it becomes smaller as mincreases and δmdecreases, since it satisﬁes dm|δm|=amr.
dmis involved in the radius of the strongly convex region N(x⋆;dm|δm|)of the smoothed function ˆfδm.
According to Figure 2, when amis large, i.e., when mis small and |δm|is large, dmcan only take almost 1.
From the deﬁnition of a σ-nice function ( Hazan et al. ,2016) (see Deﬁnition 2.2), a smoothed function ˆfδm
is strongly convex in a neighborhood N(x⋆
δm; 3δm). Then, since x⋆
δmisalways contained in N(x⋆;dm|δm|)
(see Proposition 3.3), we see that dm= 3does not always hold. That is, a σ-nice function cannot be deﬁned
when the noise level is large.
From Proposition 3.3and its proof, for Algorithm 1to be successful, 1/dm+1is required as a lower bound
forγm, i.e., γm∈/parenleftBig
1
dm+1,1/parenrightBig
. Recall that γmis the decay rate of the noise level |δm|, i.e., |δm+1|:=γm|δm|.
According to Figure 2, when amis large, i.e., when mis small and |δm|is large, 1/dmandγmcan only
10Under review as submission to TMLR
take almost 1. Therefore, γmshould vary very gradually from a value close to 1. From the deﬁnition of a
σ-nice function (see Deﬁnition 2.2),γmis always 0.5. When the noise level is large, a small decay rate such
as 0.5 cannot be used, so the deﬁnition of the σ-nice function is still not appropriate when the noise level
is large. Even when the noise level is large, our new σ-nice function can satisfy the conditions (Proposition
3.3) necessary for the success of Algorithm 1because the radius dmof the strongly convex region and the
decay rate γmvary with the noise level δm.
0 25 50 75 100 125 150 175 200
epoch0.00.20.40.60.81.0decay ratedecay rate of decaying learning rate
cosine annealing
cosine power annealing with w=10
step decay
exponential decay
polynomial decay with p=0.5
1
dm
Figure 4: Decay rate of the existing decaying learning rate schedule. The area colored green represents the
value that the decay rate γmmust satisfy for the graduated optimization approach to succeed. Here, the
green curve in this ﬁgure is a symmetric shift and parallel shift of the green curve in Figure 2to zero at
epoch 200.
Now let us see if there isa decaying learning rate schedule that satisﬁes the decay rate γmcondition. The
existing decaying learning rate schedule is shown in Figure 3(Methods that include an increase in the learning
rate, even partially, such as warm-up, are omitted). The following deﬁnes the update rules for all decaying
learning rates (ηt)t∈{0,1,···,T−1}, where Tmeans the number of epochs.
cosine annealing ( Loshchilov & Hutter ,2017):ηt:=ηmin+1
2(ηmax−ηmin)/parenleftbigg
1 + cos/parenleftbiggt
Tπ/parenrightbigg/parenrightbigg
(6)
cosine power annealing ( Hundt et al. ,2019):ηt:=ηmin+ (ηmax−ηmin)w1
2(1+cos (t
Tπ))+1−w
w2−w(w > 0)
step decay ( Lu,2022):ηt:=ηmaxd⌊t
n⌋(0< d < 1, n < T )
exponential decay ( Wu et al. ,2014):ηt:=ηmaxexp (−kt) (k > 0)
polynomial decay ( Chen et al. ,2018):ηt:= (ηmax−ηmin)/parenleftbigg
1−t
T/parenrightbiggp
+ηmin(p > 0) (7)
The curves in Figure 3are plotted for T= 200 , ηmin= 0, ηmax = 0.1, d= 0.5, n= 40, k= 0.94, w= 10 and
p= 0.5. The decay rates of these schedules are plotted in Figure 4. Figure 5and Figure 6are for polynomial
decays with diﬀerent parameters p. Note that ηminis 0, but since t∈[0,1,···, T−1],ηtwill never be 0
under any update rule. In Figure 4and 6, only the ﬁrst one is set to 1 artiﬁcially. Also, the value shown
at epoch trepresents the rate of decay from the learning rate used in epoch tto the learning rate used in
epoch t+ 1. Therefore, the graphs stop at 199 epochs.
11Under review as submission to TMLR
0 25 50 75 100 125 150 175 200
epoch0.000.020.040.060.080.10learning ratepolynomial decaying learning rate
p=0.1
p=0.5
p=1.0
p=2.0
p=4.0
p=8.0
Figure 5: Polynomial decay learning rate versus
epoch. The update rule for polynomial decay is de-
ﬁned by ( 7).
170 175 180 185 190 195 200
epoch0.00.20.40.60.81.0decay ratedecay rate of polynomial decaying learning rate
p=0.1
p=0.5
p=1.0
p=2.0
p=4.0
p=8.0
1
dmFigure 6: Decay rate of polynomial decay learning
rate schedule. The area colored green represents the
value that the decay rate γmmust satisfy for the grad-
uated optimization approach to succeed.
According to Figure 4and Figure 6, only a polynomial decay with small power psatisﬁes the conditions that
γmmust satisfy.
Finally, we would like to mention something about warm-up techniques ( Radford et al. ,2018;Liu et al. ,
2018;Gotmare et al. ,2019). Although warm-up techniques that increase the learning rate in the early stages
of learning are very popular, they are a distraction from the discussion of the decay rates shown in Figures
4and6; hence, we have focused on monotonically decreasing learning rates in this paper. Since the learning
rate determines the smoothing level of the function, increasing the learning rate in the early learning phase,
with a ﬁxed batch size, means temporarily smoothing the function signiﬁcantly and exploring that function
with a large learning rate. Therefore, we can say that the warm-up technique is a reasonable scheduling
that, as conventionally understood, deﬁnes the best starting point. However, we should also note that, since
Algorithm 3assumes that the learning rate is monotonically decreasing, Theorem 3.4may not hold if the
warm-up technique is used.
3.3 SGD’s smoothing property
This section discusses the smoothing eﬀect of using stochastic gradients. From Lemma 2.1, we have
Eξt[∥ωt∥]≤C√
b,
due to ωt:=∇fSt(xt)− ∇f(xt). The ωtfor which this equation is satisﬁed can be expressed as ωt=
C√
but,where ut∼ N/parenleftBig
0;1√
dId/parenrightBig
,N/parenleftBig
0;1√
dId/parenrightBig
is a normal distribution with mean 0and variance-covariance
matrix1√
dId, and Iddenotes the identity matrix in Rd. Note that, according to ( Zhang et al. ,2020), for
some deep learning models and datasets, the stochastic noise follows a normal distribution. Based on this,
we assume that the stochastic noise follows a normal distribution. Then, using Deﬁnition 2.1, we further
transform equation ( 1) as follows:
Eωt[yt+1] =Eωt[yt]−η∇Eωt[f(yt−ηωt)]
=Eωt[yt]−η∇Eut∼N/parenleftbig
0;1√
dId/parenrightbig/bracketleftbigg
f/parenleftbigg
yt−ηC√
but/parenrightbigg/bracketrightbigg
≈Eωt[yt]−η∇Eut∼B(0;1)/bracketleftbigg
f/parenleftbigg
yt−ηC√
but/parenrightbigg/bracketrightbigg
=Eωt[yt]−η∇ˆfηC√
b(yt), (8)
where we have used the fact that the standard normal distribution in high dimensions dis close to a uniform
distribution on a sphere of radius√
d(Vershynin ,2018, Section 3.3.3). This shows that Eωt[f(yt−ηωt)]
12Under review as submission to TMLR
is a smoothed version of fwith a noise level ηC/√
band its parameter ytcan be approximately updated
by using the gradient descent to minimize ˆfηC√
b. Therefore, we can say that the degree of smoothing by the
stochastic gradients in SGD is determined by the learning rate ηand batch size b. The ﬁndings gained from
this insight are immeasurable.
3.3.1 Why the Use of Large Batch Sizes Leads to Solutions Falling into Sharp Local Minima
It is known that training with large batch sizes leads to a persistent degradation of model generalization
performance. In particular, Keskar et al. (2017) showed experimentally that learning with large batch sizes
leads to sharp local minima and worsens generalization performance. According to equation ( 8), using a
large learning rate and/or a small batch size will make the function smoother. Thus, in using a small batch
size, the sharp local minima will disappear through extensive smoothing, and SGD can reach a ﬂat local
minimum. Conversely, when using a large batch size, the smoothing is weak and the function is close to the
original multimodal function, so it is easy for the solution to fall into a sharp local minimum. Thus, we have
theoretical support for what Keskar et al. (2017) showed experimentally. In addition, equation ( 8) implies
that halving the learning rate is the same as quadrupling the batch size. Note that Smith et al. (2018) argues
that reducing the learning rate by half is equivalent to doubling the batch size.
Remark: Note that our argument is based on the somewhat non-theoretical ﬁnding that ﬂat local solutions
have better generalizability than sharp local solutions ( Hochreiter & Schmidhuber ,1997;Keskar et al. ,2017;
Izmailov et al. ,2018;Li et al. ,2018). Since the function optimized by the optimizer is constructed from a
limited training sample, there should be some deviation from the function constructed with unknown data,
including the test data. Therefore, the intuitive explanation is that the ﬂatness around the local solution
prevents the deviation from degrading the generalizability.
3.3.2 Why Decaying Learning Rates and Increasing Batch Sizes are Superior to Fixed Learning Rates
and Batch Sizes
From equation ( 8), the use of a decaying learning rate or increasing batch size during training is equivalent to
decreasing the noise level of the smoothed function, so using a decaying learning rate or increasing the batch
size is an implicit graduated optimization. Thus, we can say that using a decaying learning rate ( Loshchilov
& Hutter ,2017;Hundt et al. ,2019;You et al. ,2019;Lewkowycz ,2021) or increasing batch size ( Byrd et al. ,
2012;Friedlander & Schmidt ,2012;Balles et al. ,2017;De et al. ,2017;Bottou et al. ,2018;Smith et al. ,
2018) makes sense in terms of avoiding local minima and provides theoretical support for their experimental
superiority.
3.3.3 Optimal Decay Rate of Learning Rate
As indicated in Section 3.2, gradually decreasing the noise from a value close to 1 is an optimal noise
scheduling for graduated optimization. Therefore, we can say that the optimal update rule for a decaying
learning rate and increasing batch size is varying slowly from a value close to 1, as in cosine annealing
(without restart) ( Loshchilov & Hutter ,2017), cosine power annealing ( Hundt et al. ,2019), and polynomial
decay ( Liu et al. ,2015;Chen et al. ,2018;Zhao et al. ,2017;Chen et al. ,2017). Thus, we have a theoretical
explanation for why these schedules are superior. In particular, a polynomial decay with small powers from
0 to 1 satisﬁes the conditions that the decay rate must satisfy (see also Figures 4and6in Section 3.2).
Therefore, we argue that polynomial decays with powers less than equal to 1 are the optimal decaying
learning rate schedule.
3.4 Implicit graduated optimization algorithm
Algorithm 3embodies the framework of implicit graduated optimization for the new σ-nice function. Algo-
rithm 4is used to optimize each smoothed function. The γmused in Algorithms 1and3is a polynomial
decay rate with powers from 0 to 1 which satisﬁes the condition that γmmust satisfy for the Algorithm
to succeed 3(see Proposition 3.3). The smoothed function ˆfδmisσ-strongly convex in the neighborhood
N(x⋆;dm|δm|). Also, the learning rate used by Algorithm 4to optimize ˆfδmis always constant. Therefore,
13Under review as submission to TMLR
let us now consider the convergence of GD with a constant learning rate for a σ-strongly convex function
F=ˆfδm. The proof of Theorem 3.3is in Section D.3.
Theorem 3.3 (Convergence analysis of Algorithm 4).Suppose that F:Rd→Ris aσ-strongly convex and
Lg-smooth function andη < min/braceleftBig
1
σ,2
Lg/bracerightBig
. Then, the sequence (ˆxt)t∈Ngenerated by Algorithm 4satisﬁes
min
t∈[T](F(ˆxt)−F(x⋆))≤H3
T=O/parenleftbigg1
T/parenrightbigg
, (9)
where x⋆is the global minimizer of F, and H3is a nonnegative constant.
Theorem 3.3is the convergence analysis of Algorithm 4for any σ-strongly convex and Lg-smooth function F.
It shows that Algorithm 4can reach an ϵm-neighborhood of the optimal solution x⋆
δmofˆfδmin approximately
TF:=H3/ϵmiterations. Proposition 3.3also holds for Algorithm 3. Therefore, if the initial point ¯x1is
contained in the σ-strongly convex region N(x⋆;d1|δ1|)of the smoothest function ˆfδ1, then the algorithm
will always reach the globally optimal solution x⋆of the original function f.
Remark: Algorithms 3and4represent implicit graduated optimization algorithms. Function smoothing is
accomplished implicitly by the stochastic noise in SGD. From Section 3.3, SGD is running for the objective
function f, but behind the scenes, GD can be regarded as running for the function ˆfηC√
b, which is smoothed
version of f, where ηandbare hyperparameters of SGD. That is why, our Algorithm 4can be GD. The
convergence analysis for this case is Theorem 3.3. On the other hand, another way of looking at it is possible.
The experiments in Section 4simply run SGD, which uses a decaying learning rate or increasing batch size,
and GD is not used explicitly. In this case, since bdata are handled in each step, it may be viewed as
ˆfηC√
b≈1
b/summationtextb
i=1fξi. Then Algorithm 4can be SGD since ˆfηC√
bvaries depending on the data chosen. If the
projection is computable to ensure that the SGD sequence does not leave the strongly convex region of
the function ˆfηC√
b, then convergence can be guaranteed as with Remark for Theorem 3.1. The relationship
between the loss function fifor the i-th data and the smoothed function ˆfηC√
bis still unknown. Then, there
may be some diﬀerences between theory and practice.
Algorithm 3 Implicit Graduated Optimization with SGD
Require: ϵ >0, p∈(0,1],¯d > 0,x1, η1, b1
δ1:=η1C√b1
α0:= min/braceleftbigg√b1
4Lfη1C(1+¯d),√b1 √
2ση1C/bracerightbigg
, Mp:=1
α0ϵ
form= 1toM+ 1do
ifm̸=M+ 1then
ϵm:=σ2δ2
m, TF:=H3/ϵm
γm:=(M−m)p
{M−(m−1)}p
κm/√λm=γm(κm∈(0,1], λm≥1)
end if
xm+1:=GD(TF,xm,ˆfδm, ηm)
ηm+1:=κmηm, bm+1:=λmbm
δm+1:=ηm+1C√
bm+1
end for
return xM+2
The next theorem guarantees the convergence of Algorithm 3with the new σ-nice function (The proof of
Theorem 3.4is in Section D.4).
Theorem 3.4 (Convergence analysis of Algorithm 3).Letϵ∈(0,1)andfbe an Lf-Lipschitz new σ-
nice function. Suppose that we run Algorithm 3; then after O/parenleftBig
1/ϵ1
p/parenrightBig
rounds, the algorithm reaches an
ϵ-neighborhood of the global optimal solution x⋆.
14Under review as submission to TMLR
Algorithm 4 GD with a constant learning rate
Require: TF,ˆx1, F, η
fort= 1toTFdo
ˆxt+1:=ˆxt−η∇F(xt)
end for
return ˆxTF+1=GD(TF,ˆx1, F, η )
Note that Theorem 3.4provides a total complexity including those of Algorithm 3and Algorithm 4, because
Algorithm 3uses Algorithm 4at each m∈[M]. Theorem 3.4implies that convergence is faster when the
power of the polynomial decay pis high, and when p= 1, it takes at least O(1/ϵ)rounds for new σ-nice
functions.
4 Numerical Results
The experimental environment was as follows: NVIDIA GeForce RTX 4090 ×2GPU and Intel Core i9
13900KF CPU. The software environment was Python 3.10.12, PyTorch 2.1.0 and CUDA 12.2. The code is
available at https://anonymous.4open.science/r/new-sigma-nice .
4.1 Implicit Graduated Optimization of DNN
We compared four types of SGD for image classiﬁcation:
1. constant learning rate and constant batch size,
2. decaying learning rate and constant batch size,
3. constant learning rate and increasing batch size,
4. decaying learning rate and increasing batch size.
We evaluated the performance of the four SGDs in training ResNet18 ( He et al. ,2016) on the CIFAR100
dataset ( Krizhevsky ,2009) (Figure 7), WideResNet-28-10 ( Zagoruyko & Komodakis ,2016) on the CIFAR100
dataset (Figure 8), and ResNet34 ( He et al. ,2016) on the ImageNet dataset ( Deng et al. ,2009) (Figure 9).
All experiments were run for 200 epochs. In methods 2, 3, and 4, the noise decreased every 40 epochs, with a
common decay rate of 1/√
2. That is, every 40 epochs, the learning rate of method 2 was multiplied by 1/√
2,
the batch size of method 3 was doubled, and the learning rate and batch size of method 4 were respectively
multiplied by√
3/2and 1.5. The initial learning rate was 0.1 for all methods, which was determined by
performing a grid search among [0.01,0.1,1.0,10]. The noise reduction interval was every 40 epochs, which
was determined by performing a grid search among [10,20,25,40,50,100]. A history of the learning rate or
batch size for each method is provided in the caption of each ﬁgure.
For methods 2, 3, and 4, the decay rates are all 1/√
2, and the decay intervals are all 40 epochs, so throughout
the training, the three methods should theoretically be optimizing the exact same ﬁve smoothed functions in
sequence. Nevertheless, the local solutions reached by each of the three methods are not exactly the same.
All results indicate that method 3 is superior to method 2 and that method 4 is superior to method 3 in both
test accuracy and training loss function values. This diﬀerence can be attributed to the diﬀerent learning
rates used to optimize each smoothing function. Among methods 2, 3, and 4, method 3, which does not
decay the learning rate, maintains the highest learning rate 0.1, followed by method 4 and method 2. In all
graphs, the loss function values are always small in this order; i.e., the larger the learning rate is, the lower
loss function values become. Therefore, we can say that the noise level |δ|, expressed asηC√
b, needs to be
reduced, while the learning rate ηneeds to remain as large as possible. Alternatively, if the learning rate
is small, then a large number of iterations are required. Thus, for the same rate of change and the same
number of epochs, an increasing batch size is superior to a decreasing learning rate because it can maintain
a large learning rate and can be made to iterate a lot when the batch size is small.
15Under review as submission to TMLR
1
0.1
0.01
0.002
loss function value for trainingTraining ResNet18 on CIFAR100 dataset
0 40 80 120 160 200
epoch4045505560657075accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
1
0.1
0.01
0.005
0.002
loss function value for trainingTraining ResNet18 on CIFAR100 dataset
0 50000 100000 150000 200000 250000
the number of parameter updates4045505560657075accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
Figure 7: Accuracy score for testing and loss function value for training versus the number of epochs ( left)
and the number of parameter updates ( right ) in training ResNet18 on the CIFAR100 dataset. The solid
line represents the mean value, and the shaded area represents the maximum and minimum over three runs.
In method 1, the learning rate and the batch size were ﬁxed at 0.1 and 128, respectively. In method 2, the
learning rate decreased every 40 epochs as/bracketleftBig
0.1,1
10√
2,0.05,1
20√
2,0.025/bracketrightBig
and the batch size was ﬁxed at 128.
In method 3, the learning rate was ﬁxed at 0.1, and the batch size was increased as [16,32,64,128,256]. In
method 4, the learning rate was decreased as/bracketleftBig
0.1,√
3
20,0.075,3√
3
80,0.05625/bracketrightBig
and the batch size was increased
as[32,48,72,108,162].
1
0.1
0.01
0.001
0.0006
loss function value for trainingTraining WideResNet-28-10 on CIFAR100 dataset
0 40 80 120 160 200
epoch4550556065707580accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
1
0.1
0.01
0.002
0.001
0.0006
loss function value for trainingTraining WideResNet-28-10 on CIFAR100 dataset
0 100000 200000 300000 400000 500000 600000
the number of parameter updates4550556065707580accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
Figure 8: Accuracy score for testing and loss function value for training versus the number of epochs ( left)
and the number of parameter updates ( right ) in training WideResNet-28-10 on the CIFAR100 dataset. The
solid line represents the mean value, and the shaded area represents the maximum and minimum over three
runs. In method 1, the learning rate and batch size were ﬁxed at 0.1 and 128, respectively. In method 2,
the learning rate was decreased every 40 epochs as/bracketleftBig
0.1,1
10√
2,0.05,1
20√
2,0.025/bracketrightBig
and the batch size was ﬁxed
at 128. In method 3, the learning rate was ﬁxed at 0.1, and the batch size increased as [8,16,32,64,128].
In method 4, the learning rate decreased as/bracketleftBig
0.1,√
3
20,0.075,3√
3
80,0.05625/bracketrightBig
and the batch size increased as
[8,12,18,27,40].
16Under review as submission to TMLR
0.712345
loss function value for trainingTraining ResNet34 on ImageNet dataset
0 40 80 120 160 200
epoch464850525456586062accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
0.712345
loss function value for trainingTraining ResNet34 on ImageNet dataset
0 20000 40000 60000 80000 100000 120000 140000 160000
the number of parameter updates464850525456586062accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
Figure 9: Accuracy score for testing and loss function value for training versus thenumber of epochs ( left)
and the number of parameter updates ( right ) in training ResNet34 on the ImageNet dataset. The solid line
represents the mean value, and the shaded area represents the maximum and minimum over three runs. In
method 1, the learning rate and batch size were ﬁxed at 0.1 and 256, respectively. In method 2, the learning
rate was decreased every 40 epochs as/bracketleftBig
0.1,1
10√
2,0.05,1
20√
2,0.025/bracketrightBig
and the batch size was ﬁxed at 256. In
method 3, the learning rate was ﬁxed at 0.1, and the batch size was increased as [32,64,128,256,512]. In
method 4, the learning rate was decreased as/bracketleftBig
0.1,√
3
20,0.075,3√
3
80,0.05625/bracketrightBig
and the batch size was increased
as[32,48,72,108,162].
Theoretically, the noise level |δm|should gradually decrease and become zero at the end, so in our algorithm
3, the learning rate ηmshould be zero at the end or the batch size bmshould match the number of data sets
at the end. However, if the learning rate is 0, training cannot proceed, and if the batch size is close to a
full batch, it is not feasible from a computational point of view. For this reason, the experiments described
in this paper are not fully graduated optimizations; i.e., full global optimization is not achieved. In fact,
the last batch size used by method 2 is around 128 to 512, which is far from a full batch. Therefore, the
solution reached in this experiment is the optimal solution for a function that has been smoothed to some
extent, and to achieve a global optimization of the DNN, it is necessary to increase only the batch size to
eventually reach a full batch, or increase the number of iterations accordingly while increasing the batch size
and decaying the learning rate.
Finally, we should note that graduated optimization with Algorithm 1is not applicable to DNN. Our
approach, Algorithm 3, allows implicit graduated optimization by exploiting the smoothness of SGD; the
experimental results provided in this section imply its success.
4.2 Experiments on Optimal Noise Scheduling
Section 3.3.3 shows that the optimal decaying learning rate is in theory a polynomial decay with small powers
from 0 to 1. To demonstrate this, we evaluated the performance of SGDs with several decaying learning
rate schedules in training ResNet18 and WideResNet-28-10 on CIFAR100 dataset. Figures 10and11plot
the accuracy in testing and the loss function value in training versus number of epochs. All experiments
were run for 200 epochs and the batch size was ﬁxed at 128. The learning rate was decreased per epoch; see
Section 3.2for the respective update rules.
Both results show that a polynomial decay with a power less than or equal to 1, which is the schedule that
satisﬁes the condition that γmmust satisfy, is superior in both test accuracy and training loss function value.
Furthermore, the loss function values and test accuracy worsen the further away from the green region that
the decay rate curve must satisfy (see Figure 4and Figure 6), and the order is in excellent agreement with
the order in which lower loss function values are achieved. According to Theorem 3.4, Algorithm 3reaches
anϵ-neighborhood of the globally optimal solution after O/parenleftBig
1/ϵ1
p/parenrightBig
iterations. Thus, theoretically, the closer
pis to 1, the fewer iterations are required. This explains why p= 0.1is not initially superior in both test
accuracy and loss function value for training in Figures 10and11.
17Under review as submission to TMLR
1
0.1
0.01
0.002
loss function value for trainingTraining ResNet18 on CIFAR100 dataset
0 40 80 120 160 200
epoch45505560657075accuracy score for testpolynomial (p=0.1)
polynomial (p=0.5)
polynomial (p=0.9)polynomial (p=2.0)
cosine annealing
exponential decay
Figure 10: Accuracy score for testing and loss function value for training versus epochs in training of ResNet18
on the CIFAR100 dataset. The solid line represents the mean value, and the shaded area represents the
maximum and minimum over three runs. See Figure 15in Appendix Ffor full results.
1
0.1
0.01
0.002
0.001
loss function value for trainingTraining WideResNet28-10 on CIFAR100 dataset
0 40 80 120 160 200
epoch45505560657075accuracy score for testpolynomial (p=0.1)
polynomial (p=0.5)
polynomial (p=0.9)polynomial (p=2.0)
cosine annealing
exponential decay
Figure 11: Accuracy score for testing and loss function value for training versus epochs in training of
WideResNet-28-10 on the CIFAR100 dataset. The solid line represents the mean value, and the shaded area
represents the maximum and minimum over three runs. See Figure 16in Appendix Ffor full results.
18Under review as submission to TMLR
5 Conclusion
We deﬁned a family of nonconvex functions: new σ-nice functions that prove that the graduated optimization
approach converges to a globally optimal solution. We also provided suﬃcient conditions for any nonconvex
function to be a new σ-nice function and performed a convergence analysis of the graduated optimization
algorithm for the new σ-nice functions. We proved that SGD with a mini-batch stochastic gradient has
the eﬀect of smoothing the function, and the degree of smoothing is greater with larger learning rates and
smaller batch sizes. This shows theoretically that smoothing with large batch sizes is makes it easy to
fall into sharp local minima, that using a decaying learning rate and/or increasing batch size is implicitly
graduated optimization, which makes sense in the sense that it avoids local solutions, and that the optimal
learning rate scheduling rule is a gradual scheduling with a decreasing rate, such as a polynomial decay with
small powers. Based on these ﬁndings, we proposed a new graduated optimization algorithm that uses a
decaying learning rate and increasing batch size and analyzed it. Finally, we conducted experiments whose
results showed the superiority of our recommended framework for image classiﬁcation tasks on CIFAR100
and ImageNet and that polynomial decay with small powers is an optimal decaying learning rate schedule.
References
Eugene L. Allgower and Kurt Georg. Numerical continuation methods -anintroduction, volume 13. Springer,
1990.
Pasquale Antonante, Vasileios Tzoumas, Heng Yang, and Luca Carlone. Outlier-robust estimation: Hardness,
minimally tuned algorithms, and applications. IEEE Transactions onRobotics, 38(1):281–301, 2022.
Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive batch sizes with learning rates. In
Proceedings ofthe33rd Conference onUncertainty inArtiﬁcial Intelligence, 2017.
Michael J. Black and Anand Rangarajan. On the uniﬁcation of line processes, outlier rejection, and robust
statistics with applications in early vision. International Journal ofComputer Vision, 19(1):57–91, 1996.
Andrew Blake and Andrew Zisserman. Visual Reconstruction. MIT Press, 1987.
Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
SIAM Review, 60(2):223–311, 2018.
Thomas Brox and Jitendra Malik. Large displacement optical ﬂow: Descriptor matching in variational
motion estimation. IEEE Transactions onPattern Analysis andMachine Learning, 33(3):500–513, 2011.
Richard H. Byrd, Gillian M. Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in optimization
methods for machine learning. Mathematical Programming, 134(1):127–155, 2012.
Olivier Chapelle and Mingrui Wu. Gradient descent optimization of smoothed information retrieval metrics.
Information retrieval, 13(3):216–235, 2010.
Olivier Chapelle, Mingmin Chi, and Alexander Zien. A continuation method for semi-supervised SVMs. In
Proceedings ofthe23rd International Conference onMachine Learning, volume 148, pp. 185–192, 2006.
Olivier Chapelle, Vikas Sindhwani, and S. Sathiya Keerthi. Optimization techniques for semi-supervised
support vector machines. Journal ofMachine Learning Research, 9:203–233, 2008.
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Clos-
ing the generalization gap of adaptive gradient methods in training deep neural networks. In
Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, volume 452,
pp. 3267–3275, 2021.
Liang-Chieh Chen, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Rethinking atrous convolution
for semantic image segmentation. http://arxiv.org/abs/1706.05587 , 2017.
19Under review as submission to TMLR
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
IEEE Transactions onPattern Analysis andMachine Learning, 40(4):834–848, 2018.
Ting Chen. On the importance of noise scheduling for diﬀusion models. https://arxiv.org/abs/2301.
10972 , 2023.
Ting Chen, Lala Li, Saurabh Saxena, Geoﬀrey E. Hinton, and David J. Fleet. A generalist framework for
panoptic segmentation of images and videos. In Proceedings oftheIEEE/CVF International Conference
onComputer Vision (ICCV), pp. 909–919, 2023.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of Adam-type
algorithms for non-convex optimization. Proceedings ofthe7thInternational Conference onLearning
Representations, 2019.
Hadi Daneshmand, Jonas Moritz Kohler, Aurélien Lucchi, and Thomas Hofmann. Escaping saddles with
stochastic gradients. In Proceedings ofthe35th International Conference onMachine Learning, volume 80,
pp. 1163–1172, 2018.
Soham De, Abhay Kumar Yadav, David W. Jacobs, and Tom Goldstein. Automated inference with adaptive
batches. In Proceedings ofthe20th International Conference onArtiﬁcial Intelligence andStatistics,
volume 54, pp. 1504–1513, 2017.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical
image database. In IEEE Computer Society Conference onComputer Vision andPattern Recognition,
pp. 248–255, 2009.
John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic opti-
mization. SIAM Journal onOptimization, 22(2):674–701, 2012.
Benjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradient
descent method for non-convex objective functions. Journal ofMachine Learning Research, 21:1–48, 2020.
Michael P. Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting. SIAM
Journal onScientiﬁc Computing, 34(3), 2012.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic gradient
for tensor decomposition. In Proceedings ofthe28th Conference onLearning Theory, volume 40, pp.
797–842, 2015.
Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep
learning heuristics: Learning rate restarts, warmup and distillation. In Proceedings ofthe7thInternational
Conference onLearning Representations, 2019.
Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour.
https://arxiv.org/abs/1706.02677 , 2017.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In Proceedings ofThe 33rd International Conference onMachine Learning, volume 48, pp.
1225–1234, 2016.
Harshvardhan and Sebastian U. Stich. Escaping local minima with stochastic noise. In the13th International
OPT Workshop onOptimization forMachine Learning inNeurIPS 2021, 2021.
Elad Hazan, Kﬁr Yehuda, and Shai Shalev-Shwartz. On graduated optimization for stochastic non-convex
problems. In Proceedings ofThe 33rd International Conference onMachine Learning, volume 48, pp.
1833–1841, 2016.
20Under review as submission to TMLR
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize well: The-
oretical and empirical evidence. In Proceedings ofthe32nd International Conference onNeural Information
Processing Systems, pp. 1141–1150, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
IEEE Conference onComputer Vision andPattern Recognition, pp. 770–778, 2016.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diﬀusion probabilistic models. In Proceedings ofthe
34th Conference onNeural Information Processing Systems, 2020.
Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. MIT Press, 9(1):1–42, 1997.
Elad Hoﬀer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap
in large batch training of neural networks. In Proceedings ofthe31st International Conference onNeural
Information Processing Systems, pp. 1731–1741, 2017.
Andrew Hundt, Varun Jain, and Gregory D. Hager. sharpDARTS: faster and more accurate diﬀerentiable
architecture search. https://arxiv.org/abs/1903.09900 , 2019.
Hideaki Iiduka. Appropriate learning rates of adaptive learning rate optimization algorithms for training
deep neural networks. IEEE Transactions onCybernetics, 52(12):13250–13261, 2022.
Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training by reduc-
ing internal covariate shift. In Proceedings ofthe32nd International Conference onMachine Learning,
volume 37, pp. 448–456, 2015.
Hidenori Iwakiri, Yuhang Wang, Shinji Ito, and Akiko Takeda. Single loop gaussian homotopy method
for non-convex optimization. In Proceedings ofthe36th Conference onNeural Information Processing
Systems, 2022.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Av-
eraging weights leads to wider optima and better generalization. In Proceedings ofthe34th Conference
onUncertainly inArtiﬁcial Intelligence, pp. 876–885, 2018.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordam. How to escape saddle
points eﬃciently. http://arxiv.org/abs/1703.00887 , 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
On large-batch training for deep learning: Generalization gap and sharp minima. In Proceedings ofthe
5thInternational Conference onLearning Representations, 2017.
Jaechul Kim, Ce Liu, Fei Sha, and Kristen Grauman. Deformable spatial pyramid matching for fast dense
correspondences. In IEEE Conference onComputer Vision andPattern Recognition, pp. 2307–2314, 2013.
Diederik P Kingma and Jimmy Lei Ba. A method for stochastic optimization. In Proceedings ofthe3rd
International Conference onLearning Representations, pp. 1–15, 2015.
Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape local minima?
InProceedings ofthe35th International Conference onMachine Learning, volume 80, pp. 2703–2712,
2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/
~kriz/learning-features-2009-TR.pdf , 2009.
Aitor Lewkowycz. How to decay your learning rate. https://arxiv.org/abs/2103.12682 , 2021.
Da Li, Jingjing Wu, and Qingrun Zhang. Stochastic gradient descent in the viewpoint of graduated opti-
mization. https://arxiv.org/abs/2308.06775 , 2023.
21Under review as submission to TMLR
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of
neural nets. In Proceedings ofthe31stAnnual Conference onNeural Information Processing Systems, pp.
6391–6401, 2018.
Junhong Lin, Raﬀaello Camoriano, and Lorenzo Rosasco. Generalization properties and implicit regulariza-
tion for multiple passes SGM. In Proceedings ofThe33rd International Conference onMachine Learning,
volume 48, pp. 2340–2348, 2016.
Songtao Liu, Di Huang, and Yunhong Wang. Receptive ﬁeld block net for accurate and fast object detection.
InProceedings ofthe15th European Conference onComputer Vision, volume 11215, pp. 404–419, 2018.
Wei Liu, Andrew Rabinovich, and Alexander C. Berg. Parsenet: Looking wider to see better. http:
//arxiv.org/abs/1506.04579 , 2015.
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for
SGD: An adaptive learning rate for fast convergence: An adaptive learning rate for fast convergence.
InProceedings ofthe24th International Conference onArtiﬁcial Intelligence andStatistics (AISTATS),
volume 130, 2021.
Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In Proceedings
ofthe5thInternational Conference onLearning Representations, 2017.
Jun Lu. Gradient descent, stochastic optimization, and other tales. https://arxiv.org/abs/2205.00832 ,
2022.
Hossein Mobahi and John W. Fisher III. A theoretical analysis of optimization by gaussian continuation. In
Proceedings ofthe39th AAAI Conference onArtiﬁcial Intelligence, pp. 1205–1211, 2015a.
Hossein Mobahi and John W. Fisher III. On the link between gaussian homotopy continuation and convex
envelopes. In Proceedings ofthe10th International Conference onEnergy Minimization Methods in
Computer Vision andPattern Recognition, volume 8932, pp. 43–56, 2015b.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of SGLD for non-convex
learning: Two theoretical viewpoints. In Proceedings ofthe31st Annual Conference onLearning Theory,
volume 75, pp. 605–638, 2018.
Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine
learning. In Proceedings ofthe25th Annual Conference onNeural Information Processing Systems,
volume 24, 2011.
Deanna Needell, Rachel A. Ward, and Nathan Srebro. Stochastic gradient descent, weighted sampling, and
the randomized kaczmarz algorithm. In Proceedings ofthe28th Annual Conference onNeural Information
Processing Systems, volume 27, pp. 1017–1025, 2014.
Mila Nikolova, Michael K. Ng, and Chi-Pan Tam. Fast nonconvex nonsmooth minimization methods for
image restoration and reconstruction. IEEE Transactions onImage Processing, 19(12), 2010.
Liangzu Peng, Christian Kümmerle, and René Vidal. On the convergence of IRLS and its variants in
outlier-robust estimation. In IEEE/CVF Conference onComputer Vision andPattern Recognition, pp.
17808–17818, 2023.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by
generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/
language-unsupervised/language_understanding_paper.pdf , 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals ofMathematical
Statistics, 22:400–407, 1951.
22Under review as submission to TMLR
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diﬀusion models. In IEEE/CVF Conference onComputer Vision andPattern
Recognition, 2022.
Kenneth Rose, Eitan Gurewitz, and Geoﬀrey Fox. A deterministic annealing approach to clustering. Pattern
Recognition Letters, 11(9):589–594, 1990.
Kevin Scaman and Cedric Malherbe. Robustness analysis of non-convex stochastic gradient descent using
biased expectations. In Proceedings ofthe34th Conference onNeural Information Processing Systems,
volume 33, pp. 16377–16387, 2020.
Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures onStochastic Programming -
Modeling andTheory. MOS-SIAM Series on Optimization. SIAM, 2009.
Vikas Sindhwani, S. Sathiya Keerthi, and Olivier Chapelle. Deterministic annealing for semi-supervised
kernel machines. In Proceedings ofthe23rd International Conference onMachine Learning, volume 148,
pp. 841–848, 2006.
Noah A. Smith and Jason Eisner. Annealing techniques for unsupervised statistical language learning. In
Proceedings ofthe42nd Annual Meeting oftheAssociation forComputational Linguistics, pp. 486–493,
2004.
Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don’t decay the learning rate,
increase the batch size. In Proceedings ofthe6thInternational Conference onLearning Representations,
2018.
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In Proceedings ofthe32nd International Conference on
Machine Learning, volume 37, pp. 2256–2265, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diﬀusion implicit models. In Proceedings of
the9thInternational Conference onLearning Represantations, 2021a.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In
Proceedings ofthe33rd International Conference onNeural Information Processing Systems, pp. 11895–
11907, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In
Proceedings ofthe34th Conference onNeural Information Processing Systems, 2020.
Yang Song, Jascha Sohl-Dickstein, Diederik P. kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic diﬀerential equations. In Proceedings ofthe9th
International Conference onLearning Represantations, 2021b.
Deqing Sun, Stefan Roth, and Michael J. Black. Secrets of optical ﬂow estimation and their principles. In
IEEE Conference onComputer Vision andPattern Recognition, pp. 2432–2439, 2010.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
Number 47. Cambridge University Press, 2018.
Andrew P. Witkin, Demetri Terzopoulos, and Michael Kass. Signal matching through scale space.
Intenational Jounral ofComputer Vision, 1(2):133–144, 1987.
Yuting Wu, Daniel J. Holland, Mick D. Mantle, Andrew Gordon Wilson, Sebastian Nowozin, Andrew Blake,
and Lynn F. Gladden. A bayesian method to quantifying chemical composition using NMR: application to
porous media systems. In Proceedings ofthe22nd European Signal Processing Conference (EUSIPCO),
pp. 2515–2519, 2014.
23Under review as submission to TMLR
Zhijun Wu. The eﬀective energy transformation scheme as a special continuation approach to global op-
timization with application to molecular conformation. SIAM Journal onOptimization, 6(3):748–768,
1996.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diﬀusion theory for deep learning dynamics: Stochastic
gradient descent exponentially favors ﬂat minima. In Proceedings ofthe9thInternational Conference on
Learning Represantations, 2021.
Heng Yang, Pasquale Antonante, Vasileios Tzoumas, and Luca Carlone. Graduated non-convexity for robust
spatial perception: From non-minimal solvers to global outlier rejection. IEEE Robotics andAutomation
Letters, 5(2):1127–1134, 2020.
Kaichao You, Mingsheng Long, Jianmin Wang, and Michael I. Jordam. How does learning rate decay help
modern neural networks? https://arxiv.org/abs/1908.01878 , 2019.
Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,
James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training
BERT in 76 minutes. In Proceedings ofthe8thInternational Conference onLearning Representations,
2020.
A. L. Yuille. Energy functions for early vision and analog networks. Biological Cybernetics, 61(2):115–123,
1989.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings oftheBritish Machine
Vision Conference, 2016.
Manzil Zaheer, Sashank J. Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Proceedings ofthe32nd International Conference onNeural Information
Processing Systems, volume 31, 2018.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sanjiv Kumar, and Suvrit Sra.
Why are adaptive methods good for attention models? In Proceedings ofthe33rd Annual Conference on
Neural Information Processing Systems, 2020.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient langevin
dynamics. In Proceedings ofthe30th Conference onLearning Theory, volume 65, pp. 1980–2022, 2017.
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing
network. In 2017 IEEE Conference onComputer Vision andPattern Recognition, pp. 6230–6239, 2017.
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. On the convergence
of adaptive gradient methods for nonconvex optimization. 12th Annual Workshop onOptimization for
Machine Learning, 2020.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A suﬃcient condition for convergences of
adam and rmsprop. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 11119–11127, 2019.
24Under review as submission to TMLR
A Derivation of equation ( 1)
Letytbe the parameter updated by gradient descent (GD) and xt+1be the parameter updated by SGD at
time t, i.e.,
yt:=xt−η∇f(xt),
xt+1:=xt−η∇fSt(xt)
=xt−η(∇f(xt) +ωt).
Then, we have
xt+1:=xt−η∇fSt(xt)
= (yt+η∇f(xt))−η∇fSt(xt)
=yt−ηωt, (10)
from ωt:=∇fSt(xt)− ∇f(xt). Hence,
yt+1=xt+1−η∇f(xt+1)
=yt−ηωt−η∇f(yt−ηωt).
By taking the expectation with respect to ωton both sides, we have, from Eωt[ωt] =0,
Eωt[yt+1] =Eωt[yt]−η∇Eωt[f(yt−ηωt)],
where we have used Eωt[∇f(yt−ηωt)] =∇Eωt[f(yt−ηωt)], which holds for the Lipschitz-continuous and
diﬀerentiable of f(Shapiro et al. ,2009, Theorem 7.49). In addition, from ( 10) and Eωt[ωt] =0, we obtain
Eωt[xt+1] =yt.
Therefore, on average, the parameter xt+1of the function farrived at by SGD coincides with the parameter
ytof the smoothed function ˆf(yt) :=Eωt[f(yt−ηωt)]arrived at by GD.
B Proofs of the Lemmas in Section 2.2
B.1 Proof of Lemma 2.1
Proof. (A3)(ii) and (A4) guarantee that
Eξt/bracketleftbig
∥∇fSt(xt)− ∇f(xt)∥2/bracketrightbig
=Eξt
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
bb/summationdisplay
i=1Gξt,i(xt)− ∇f(xt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

=Eξt
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
bb/summationdisplay
i=1Gξt,i(xt)−1
bb/summationdisplay
i=1∇f(xt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

=Eξt
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
bb/summationdisplay
i=1/parenleftbig
Gξt,i(xt)− ∇f(xt)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

=1
b2Eξt
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleb/summationdisplay
i=1/parenleftbig
Gξt,i(xt)− ∇f(xt)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

=1
b2Eξt/bracketleftBiggb/summationdisplay
i=1/vextenddouble/vextenddoubleGξt,i(xt)− ∇f(xt)/vextenddouble/vextenddouble2/bracketrightBigg
≤C2
b.
This completes the proof.
25Under review as submission to TMLR
B.2 Proof of Lemma 2.2
Proof. From Deﬁnition 2.1and (A1), we have, for all x,y∈Rd,
/vextenddouble/vextenddouble/vextenddouble∇ˆfδ(x)− ∇ ˆfδ(y)/vextenddouble/vextenddouble/vextenddouble=∥∇Eu[f(x−δu)]− ∇Eu[f(y−δu)]∥
=∥Eu[∇f(x−δu)]−Eu[∇f(y−δu)]∥
=∥Eu[∇f(x−δu)− ∇f(y−δu)]∥
≤Eu[∥∇f(x−δu)− ∇f(y−δu)∥]
≤Eu[Lg∥(x−δu)−(y−δu)∥]
=Eu[Lg∥x−y∥]
=Lg∥x−y∥.
This completes the proof.
B.3 Proof of Lemma 2.3
Proof. From Deﬁnition 2.1and (A2), we have, for all x,y∈Rd,
/vextendsingle/vextendsingle/vextendsingleˆfδ(x)−ˆfδ(y)/vextendsingle/vextendsingle/vextendsingle=|Eu[f(x−δu)]−Eu[f(y−δu)]|
=|Eu[f(x−δu)−f(y−δu)]|
≤Eu[|f(x−δu)−f(y−δu)|]
≤Eu[Lf∥(x−δu)−(y−δu)∥]
=Eu[Lf∥x−y∥]
=Lf∥x−y∥.
This completes the proof.
B.4 Proof of Lemma 2.4
Proof. From Deﬁnition 2.1and (A2), we have, for all x,y∈Rd,
/vextendsingle/vextendsingle/vextendsingleˆfδ(x)−f(x)/vextendsingle/vextendsingle/vextendsingle=|Eu[f(x−δu)]−f(x)|
=|Eu[f(x−δu)−f(x)]|
≤Eu[|f(x−δu)−f(x)|]
≤Eu[Lf∥(x−δu)−x∥]
=Eu[Lf|δ|∥u∥]
=|δ|Lf,
where we have used ∥u∥ ≤ 1. This completes the proof.
C Lemmas used in the proofs of the theorems
Lemma C.1. Suppose that F:Rd→Risσ-strongly convex and ˆxt+1:=ˆxt−ηtgt. Then, for all t∈N,
F(ˆxt)−F(x⋆)≤1−σηt
2ηtXt−1
2ηtXt+1+ηt
2∥gt∥2,
where gt:=∇F(ˆxt),Xt:=∥ˆxt−x⋆∥2, and x⋆is the global minimizer of F.
26Under review as submission to TMLR
Proof. Lett∈N. The deﬁnition of ˆxt+1guarantees that
∥ˆxt+1−x⋆∥2=∥(ˆxt−ηtgt)−x⋆∥2
=∥ˆxt−x⋆∥2−2ηt⟨ˆxt−x⋆,gt⟩+η2
t∥gt∥2.
From the σ-strong convexity of F,
∥ˆxt+1−x⋆∥2≤ ∥ˆxt−x⋆∥2+ 2ηt/parenleftBig
F(x⋆)−F(ˆxt)−σ
2∥ˆxt−x⋆∥2/parenrightBig
+η2
t∥gt∥2.
Hence,
F(ˆxt)−F(x⋆)≤1−σηt
2ηt∥ˆxt−x⋆∥2−1
2ηt∥ˆxt+1−x⋆∥2+ηt
2∥gt∥2.
This completes the proof.
Lemma C.2. Suppose that F:Rd→RisLg-smooth and ˆxt+1:=ˆxt−ηtgt. Then, for all t∈N,
ηt/parenleftbigg
1−Lgηt
2/parenrightbigg
∥∇F(ˆxt)∥2≤F(ˆxt)−F(ˆxt+1).
where gt:=∇F(ˆxt)andx⋆is the global minimizer of F.
Proof. From the Lg-smoothness of the Fand the deﬁnition of ˆxt+1, we have, for all t∈N,
F(ˆxt+1)≤F(ˆxt) +⟨∇F(ˆxt),ˆxt+1−ˆxt⟩+Lg
2∥ˆxt+1−ˆxt∥2
=F(ˆxt)−ηt⟨∇F(ˆxt),gt⟩+Lgη2
t
2∥gt∥2
≤F(ˆxt)−ηt/parenleftbigg
1−Lgηt
2/parenrightbigg
∥∇F(ˆxt)∥2.
Therefore, we have
ηt/parenleftbigg
1−Lgηt
2/parenrightbigg
∥∇F(ˆxt)∥2≤F(ˆxt)−F(ˆxt+1).
This completes the proof.
Lemma C.3. Suppose that F:Rd→RisLg-smooth, ˆxt+1:=ˆxt−ηtgt, and ηt:=η <2
Lg. Then, for all
t∈N,
1
TT/summationdisplay
t=1∥gt∥2≤2 (F(ˆx1)−F(x⋆))
η(2−Lgη)T,
where gt:=∇F(ˆxt)andx⋆is the global minimizer of F.
Proof. According to Lemma C.2, we have
η/parenleftbigg
1−Lgη
2/parenrightbigg
∥∇F(xt)∥2≤F(ˆxt)−F(ˆxt+1).
Summing over t, we ﬁnd that
η/parenleftbigg
1−Lgη
2/parenrightbigg1
TT/summationdisplay
t=1∥∇F(ˆxt)∥2≤F(ˆx1)−F(ˆxT+1)
T.
27Under review as submission to TMLR
Hence, from η <2
Lg,
1
TT/summationdisplay
t=1∥gt∥2=2 (F(ˆx1)−F(x⋆))
η(2−Lgη)T.
This completes the proof.
Lemma C.4. Suppose that F:Rd→RisLg-smooth, ˆxt+1:=ˆxt−ηtgt, and ηt:=2
σt. Then, for all t∈N,
∥gt∥2≤B2,
where gt:=∇F(ˆxt)andB2≥0is a nonnegative constant.
Proof. According to Lemma C.2, we have
ηt/parenleftbigg
1−Lgηt
2/parenrightbigg
∥∇F(ˆxt)∥2≤F(ˆxt)−F(ˆxt+1).
Summing over tfrom t=t0tot=T, we have
T/summationdisplay
t=t0ηt/parenleftbigg
1−Lgηt
2/parenrightbigg
∥∇F(ˆxt)∥2≤F(ˆxt0)−F(ˆxT),
where t0satisﬁes
∀t≥t0:ηt0<2
Lg.
Hence, we obtain
/parenleftbigg
1−Lgηt0
2/parenrightbiggT/summationdisplay
t=t0ηt∥∇F(ˆxt)∥2≤F(ˆxt0)−F(x⋆)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:B<∞.
Then,
T/summationdisplay
t=t0ηt∥∇F(ˆxt)∥2≤2B
2−Lgηt0<∞.
Therefore,
T/summationdisplay
t=1ηt∥∇F(ˆxt)∥2≤2B
2−Lgηt0+t0−1/summationdisplay
t=1ηt∥∇F(ˆxt)∥2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=:ˆB<∞. (11)
From ηT≤ηt:=2
σt,
2
σTT/summationdisplay
t=1∥∇F(ˆxt)∥2=ηTT/summationdisplay
t=1∥∇F(ˆxt)∥2≤T/summationdisplay
t=1ηt∥∇F(ˆxt)∥2≤ˆB. (12)
Then, if/parenleftbig
∥∇F(ˆxt)∥2/parenrightbig
is unbounded, we have
∀ϵ >0,∃t1∈N,∀t∈N:t≥t1⇒ ∥∇ F(ˆxt)∥2≥ϵ.
28Under review as submission to TMLR
Therefore, from ( 12),
ˆB≥2
σTT/summationdisplay
t=1∥∇F(ˆxt)∥2
=2
σT/parenleftBiggT/summationdisplay
t=t1∥∇F(ˆxt)∥2+t1−1/summationdisplay
t=1∥∇F(ˆxt)∥2/parenrightBigg
≥2
σT(T−t1+ 1)ϵ
=2
σ/parenleftbigg
1−t1−1
T/parenrightbigg
ϵ,
where we have used/summationtextt1−1
t=1∥∇F(ˆxt)∥2≥0. Hence, letting ϵ:=σˆB, we have
∃t1,∀T≥t1: 2/parenleftbigg
1−t1−1
T/parenrightbigg
ˆB≤ˆB.
Taking the limit of T→ ∞ , we have 2ˆB≤ˆB. This is a contradiction. Hence,/parenleftbig
∥∇F(ˆxt)∥2/parenrightbig
is bounded. Let
its upper boundary be B2. This completes the proof.
D Proof of the Theorems and Propositions
D.1 Proof of Theorem 3.1
Proof. Lemma C.1, Lemma C.4, and ηt:=2
σtguarantee that
F(ˆxt)−F(x⋆)≤1−σηt
2ηtXt−1
2ηtXt+1+ηt
2∥gt∥2
≤1
2ηt{(1−σηt)Xt−Xt+1}+ηtB2
2
=σ(t−2)
4Xt−σt
4Xt+1+B2
σt.
Therefore, we have
(t−1) (F(ˆxt)−F(x⋆))≤σ(t−2)(t−1)
4Xt−σ(t−1)t
4Xt+1+B2(t−1)
σt.
Summing over t, we ﬁnd that
T/summationdisplay
t=1(t−1) (F(ˆxt)−F(x⋆))≤σ·(−1)·0
4X1−σ(T−1)T
4XT+1+B2
σT/summationdisplay
t=1t−1
t
≤B2(T−1)
σ.
Then, we have
2
(T−1)TT/summationdisplay
t=1(t−1) (F(ˆxt)−F(x⋆))≤2B2
σT.
From the convexity of F,
F/parenleftBigg
2
T(T−1)T/summationdisplay
t=1(t−1)ˆxt/parenrightBigg
≤2
T(T−1)T/summationdisplay
t=1(t−1)F(ˆxt).
29Under review as submission to TMLR
Hence,
F/parenleftBigg
2
T(T−1)T/summationdisplay
t=1(t−1)ˆxt/parenrightBigg
−F(x⋆)≤2B2
σT=O/parenleftbigg1
T/parenrightbigg
.
In addition, since the minimum value is smaller than the mean, we have
min
t∈[T](F(ˆxt)−F(x⋆))≤2B2
σT=O/parenleftbigg1
T/parenrightbigg
.
This completes the proof.
D.2 Proof of Theorem 3.2
The following proof uses the proof technique of Hazan et al. (2016).
Proof. From Mp:=1
α0ϵ,δ1:=2Lg
σr, and γm+1:=(M−m)p
{M−(m−1)}p, we have
δM=δ1(γ1γ2···γM−1)
=δ1·(M−1)p
Mp·(M−2)p
(M−1)p·(M−3)p
(M−2)p···1
2p
=δ1·1
Mp
=δ1α0ϵ
=2Lgα0ϵ
σr.
According to Theorem 3.1,
ˆfδM(xM+1)−ˆfδM(x⋆
δM)≤ϵM
=σδ2
M
=/parenleftbigg2Lgα0ϵ√σr/parenrightbigg2
From Lemma 2.3and2.4,
f(xM+2)−f(x⋆) =/braceleftBig
f(xM+2)−ˆfδM(xM+2)/bracerightBig
+/braceleftBig
ˆfδM(x⋆)−f(x⋆)/bracerightBig
+/braceleftBig
ˆfδM(xM+2)−ˆfδM(x⋆)/bracerightBig
≤/braceleftBig
f(xM+2)−ˆfδM(xM+2)/bracerightBig
+/braceleftBig
ˆfδM(x⋆)−f(x⋆)/bracerightBig
+/braceleftBig
ˆfδM(xM+2)−ˆfδM(x⋆
δM)/bracerightBig
≤δMLf+δMLf+/braceleftBig
ˆfδM(xM+2)−ˆfδM(x⋆
δM)/bracerightBig
= 2δMLf+/braceleftBig
ˆfδM(xM+2)−ˆfδM(xM+1)/bracerightBig
+/braceleftBig
ˆfδM(xM+1)−ˆfδM(x⋆
δM)/bracerightBig
≤2δMLf+Lf∥xM+2−xM+1∥+/braceleftBig
ˆfδM(xM+1)−ˆfδM(x⋆
δM)/bracerightBig
.
Then, we have
f(xM+2)−f(x⋆)≤2δMLf+ 2LfdMδM+ϵM
≤2δMLf+ 2Lf¯dδM+ϵM
= 2LfδM/parenleftbig
1 + ¯d/parenrightbig
+ϵM,
30Under review as submission to TMLR
where we have used ∥xM+2−xM+1∥ ≤ 2dMδMsince xM+2,xM+1∈N(x⋆;dMδM), and dM≤¯d < +∞.
Therefore,
f(xM+2)−f(x⋆)≤2Lf/parenleftbig
1 + ¯d/parenrightbig2Lfα0ϵ
σr+/parenleftbigg2Lgα0ϵ√σr/parenrightbigg2
=4L2
f/parenleftbig
1 + ¯d/parenrightbig
α0ϵ
σr+/parenleftbigg2Lgα0ϵ√σr/parenrightbigg2
≤ϵ,
where we have used α0:= min/braceleftbigg
σr
8L2
f(1+¯d),√σr
2√
2Lg/bracerightbigg
.
LetTtotalbe the total number of queries made by Algorithm 1; then,
Ttotal =M+1/summationdisplay
m=12B2
σϵm=M+1/summationdisplay
m=12B2
σ2δ2m
=2B2
σ2δ2
1/parenleftbigg
1 +1
γ2
1+1
γ2
1γ2
2+···+2
γ2
1γ2
2···γ2
M−1/parenrightbigg
=2B2
σ2δ2
1/braceleftBigg/parenleftbig
γ2
1γ2
2···γ2
M−1/parenrightbig
+/parenleftbig
γ2
2γ2
3···γ2
M−1/parenrightbig
+···+γ2
M−1+ 2
γ2
1γ2
2···γ2
M−1/bracerightBigg
From γ1γ2···γM−1=1
Mp,
Ttotal =2B2
σ2δ2
1·1
M2p+1
(M−1)2p+1
(M−2)2p+···1
22p+ 2
1
M2p
≤2B2
σ2δ2
1·M2p(M+ 1)
=2B2
σ2δ2
1·M2p+1+2B2
σ2δ2
1·M2p
=2B2
σ2δ2
1/parenleftbigg1
α0ϵ/parenrightbigg1
p+2
+2B2
σ2δ2
1/parenleftbigg1
α0ϵ/parenrightbigg2
=O/parenleftbigg1
ϵ1
p+2/parenrightbigg
,
This completes the proof.
D.3 Proof of Theorem 3.3
Proof. Lemma C.1guarantees that
F(ˆxt)−F(x⋆)≤1−σηt
2ηtXt−1
2ηtXt+1+ηt
2∥gt∥2
=1−ση
2η(Xt−Xt+1)−σ
2Xt+1+η
2∥gt∥2.
31Under review as submission to TMLR
From η < min/braceleftBig
1
σ,2
Lg/bracerightBig
and Lemma C.3, by summing over twe ﬁnd that
1
TT/summationdisplay
t=1(F(ˆxt)−F(x⋆))≤1−ση
2ηT(X1−XT+1)−σ
2TT/summationdisplay
t=1Xt+1+η
2TT/summationdisplay
t=1∥gt∥2
≤1−ση
2ηTX1+η
2TT/summationdisplay
t=1∥gt∥2
≤(1−ση)X1
2η/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:H11
T+F(ˆx1)−F(x⋆)
(2−Lgη)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:H21
T
= (H1+H2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:H31
T
=H3
T,
where H3>0is a nonnegative constant. From the convexity of F,
F/parenleftBigg
1
TT/summationdisplay
t=1ˆxt/parenrightBigg
≤1
TT/summationdisplay
t=1F(ˆxt).
Hence,
F/parenleftBigg
1
TT/summationdisplay
t=1ˆxt/parenrightBigg
−F(x⋆)≤H3
T=O/parenleftbigg1
T/parenrightbigg
.
In addition, since the minimum value is smaller than the mean, we have
min
t∈[T](F(ˆxt)−F(x⋆))≤H3
T=O/parenleftbigg1
T/parenrightbigg
.
This completes the proof.
D.4 Proof of Theorem 3.4
The following proof uses the proof technique of Hazan et al. (2016).
Proof. According to δm+1:=ηm+1C√
bm+1andκm√λm=γm, we have
δm+1:=ηm+1C/radicalbig
bm+1
=κmηmC√λm√bm
=κm√λmδm
=γmδm.
32Under review as submission to TMLR
Therefore, from Mp:=1
α0ϵ,δ1:=η1C√b1, and γm:=(M−m)p
{M−(m−1)}p, then
δM=δ1(γ1γ2···γM−1)
=δ1·(M−1)p
Mp·(M−2)p
(M−1)p·(M−3)p
(M−2)p···1
2p
=δ1·1
Mp
=δ1α0ϵ
=η1Cα0ϵ√b1.
According to Theorem 3.3,
ˆfδM(xM+1)−ˆfδM(x⋆
δM)≤ϵM
=σδ2
M
=/parenleftbigg√ση1Cα0ϵ√b1/parenrightbigg2
As in the proof of Theorem 3.2, we have
f(xM+2)−f(x⋆)≤2LfδM/parenleftbig
1 + ¯d/parenrightbig
+ϵM.
Therefore,
f(xM+2)−f(x⋆)≤2Lf/parenleftbig
1 + ¯d/parenrightbigη1Cα0ϵ√b1+/parenleftbigg√ση1Cα0ϵ√b1/parenrightbigg2
=2Lf/parenleftbig
1 + ¯d/parenrightbig
η1Cα0ϵ√b1+/parenleftbigg√ση1Cα0ϵ√b1/parenrightbigg2
≤ϵ,
where we have used α0:= min/braceleftbigg√b1
4Lfη1C(1+¯d),√b1 √
2ση1C/bracerightbigg
.
LetTtotalbe the total number of queries made by Algorithm 3; then,
Ttotal =M+1/summationdisplay
m=1H3
ϵm=M+1/summationdisplay
m=1H3
σ2δ2m
=H3
σ2δ2
1+H3
σ2δ2
2+···+2H3
σ2δ2
M
≤H3(M+ 1)
σ2δ2
M
≤H3(M+ 1)
σ2δ2
M
=H3(M+ 1)
σ2δ2
11
M2p
=H3M2p(M+ 1)
σ2δ2
1
=H3M2p+1
σ2δ2
1+H3M2p
σ2δ2
1.
33Under review as submission to TMLR
From Mp:=1
α0ϵ,
Ttotal =H3/parenleftBig
1
α0ϵ/parenrightBig1
p+2
σ2δ2
1+H3/parenleftBig
1
α0ϵ/parenrightBig2
σ2δ2
1
≤2H3/parenleftBig
1
α0ϵ/parenrightBig1
p+2
σ2δ2
1
=2H3/parenleftBig
1
α0ϵ/parenrightBig1
p
σ2δ2
1(α0ϵ)2
=2H3
σ2δ2
1(α0ϵ)1
p+2
≤2H3
σ2δ2
1(α0ϵ)1
p
=O/parenleftbigg1
ϵ1
p/parenrightbigg
.
This completes the proof.
D.5 Proof of Proposition 3.1
Proof. For all x∈N(x⋆;amr)\{x⋆}(am≥0), and all um∼B(0; 1), the quadratic equation
δ2
m−2⟨x⋆−x,um⟩δm+ (a2
m−1)r2= 0 (13)
forδm∈Rhas solutions with probability p(am)when am>1and always has solutions when 0≤am≤1.
Let us derive p(am). When am>1, the condition for the discriminant equation of ( 13) to be positive is as
follows:
−1≤cosθ≤ −r/radicalbig
a2m−1
∥x⋆−x∥∥um∥,orr/radicalbig
a2m−1
∥x⋆−x∥∥um∥≤cosθ≤1, (14)
where θis the angle between um∼B(0; 1)andx⋆−x. Note that cosθcan be positive or negative because
δm∈R. Since the random variable umis sampled uniformly from the B(0; 1), the probability that um
satisﬁes ( 14) is less than
p(am) :=arccos/parenleftBigg
r/radicalbig
a2m−1
∥x⋆−x∥∥um∥/parenrightBigg
π,
forδm>0andδm<0, respectively.
Now let us consider the solution of the quadratic inequality,
∥um∥2δ2
m−2⟨x⋆−x,um⟩δm+ (a2
m−1)r2≤0 (15)
forδm∈R.
(i) When am>1, (15) has one or two solutions with probability p(am)or less. Whenr√
a2m−1
∥x⋆−x∥∥um∥≤cosθ≤1,
let the larger solution be D+
m>0and the smaller one be D−
m>0; we can express these solutions as follows:
D+
m(x,um):=∥x⋆−x∥∥um∥cosθ+/radicalbig
∥x⋆−x∥2∥um∥2cos2θ−r2(a2m−1),
D−
m(x,um):=∥x⋆−x∥∥um∥cosθ−/radicalbig
∥x⋆−x∥2∥um∥2cos2θ−r2(a2m−1).
34Under review as submission to TMLR
Moreover, we deﬁne δ+
mandδ−
mas follows:
δ+
m:= sup
x∈N(x⋆;amr)\{x⋆}Eum∼B(0;1)/bracketleftbig
D+
m(x,um)/bracketrightbig
,
δ−
m:= sup
x∈N(x⋆;amr)\{x⋆}Eum∼B(0;1)/bracketleftbig
D−
m(x,um)/bracketrightbig
.
Thus, the solution Dm(x,um)to (15) is
0< D−
m(x,um)< D m(x,um)< D+
m(x,um)
whenr√
a2m−1
∥x⋆−x∥∥um∥≤cosθ≤1, and
−D+
m(x,um)< D m(x,um)<−D−
m(x,um)<0
when −1≤cosθ≤ −r√
a2m−1
∥x⋆−x∥∥um∥. Hence, let δm:= sup
x∈N(x⋆;amr)\{x⋆}Eum∼B(0;1)[Dm(x,um)], then we have
|δ−
m|≤ |δm| ≤ |δ+
m|.
(ii) When am≤1, (15) always has one or two solutions. The two solutions are deﬁned as in (i). Then, the
solution to ( 15) is
D−
m(x,um)< D m(x,um)< D+
m(x,um)
whenr√
a2m−1
∥x⋆−x∥∥um∥≤cosθ≤1, and
−D+
m(x,um)< D m(x,um)<−D−
m(x,um) (−D+
m(x,um)<0,−D−
m(x,um)>0)
when −1≤cosθ≤ −r√
a2m−1
∥x⋆−x∥∥um∥. Hence, we have
|δm| ≤|δ−
m|.
From (i) and (ii), ( 15) may have a solution for all am>0when |δm|=|δ−
m|. Therefore, suppose |δm|=|δ−
m|;
then,
r2≥a2
mr2−2δm⟨x⋆−x,um⟩+|δm|2
≥a2
mr2−2δm⟨x⋆−x,um⟩+|δm|2∥um∥2
>∥x−x⋆∥2−2δm⟨x⋆−x,um⟩+|δm|2∥um∥2
=∥x+δmum−x⋆∥2.
This means that x+δmum∈N(x⋆;r) (δm∈R), where um∼B(0; 1). Hence, for all x,y∈N(x⋆;amr)⊂
Rd(am>√
2),
/angbracketleftBig
∇ˆfδm(x)− ∇ ˆfδm(y),x−y/angbracketrightBig
=⟨∇Eu[f(x+δmu)]− ∇Eu[f(y+δmu)],x−y⟩
=⟨Eu[∇f(x+δmu)]−Eu[∇f(y+δmu)],x−y⟩
=⟨Eu[∇f(x+δmu)− ∇f(y+δmu)],x−y⟩
=Eu[⟨∇f(x+δmu)− ∇f(y+δmu),x−y⟩]
≥Eu[σ∥(x+δmu)−(y+δmu)∥2]
=Eu[σ∥x−y∥2]
=σ∥x−y∥2.
This means that, if |δm|=|δ−
m|holds, then ˆfδmisσ-strongly convex on N(x⋆;amr) (am>√
2)when fis
σ-strongly convex on B(x⋆;r). Also, if we deﬁne dm:=amr
|δ−
m|, then dm|δm| ≤amrholds; i.e., ˆfδmisσ-strongly
convex on N(x⋆;dm|δm|). This completes the proof.
35Under review as submission to TMLR
Remark: In the end, |δm|must be equal to |δ−
m|. We can show that |δ−
m|is non-zero. Suppose that ( 14)
holds, then
|δ−
m|:= sup
x∈N(x⋆;amr)\{x⋆}Eum∼B(0;1)/bracketleftBig/vextendsingle/vextendsingle/vextendsingle∥x⋆−x∥∥um∥cosθ−/radicalbig
∥x⋆−x∥2∥um∥2cos2θ−r2(a2m−1)/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
> sup
x∈N(x⋆;amr)\{x⋆}Eum∼B(0;1)/bracketleftBig/vextendsingle/vextendsingle/vextendsingler/radicalbig
a2m−1−/radicalbig
a2mr2−r2(a2m−1)/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
=|r/radicalbig
a2m−1−r|
=r/parenleftBig/radicalbig
a2m−1−1/parenrightBig
>0,
where we have used ∥x⋆−x∥< amr,∥um∥ ≤ 1,r√
a2m−1
∥x⋆−x∥∥um∥≤ |cosθ| ≤1, and am>√
2.
D.6 Proof of Proposition 3.2
Proof. From Proposition 3.1, for all |δm|=|δ−
m|,ˆfδmisσ-strongly convex, i.e.,
σ∥x⋆−x⋆
δm−1∥2≤/angbracketleftBig
x⋆−x⋆
δm−1,∇ˆfδm(x⋆)− ∇ ˆfδm(x⋆
δm−1)/angbracketrightBig
≤/vextenddouble/vextenddouble/vextenddoublex⋆−x⋆
δm−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇ˆfδm(x⋆)−∇ˆfδm(x⋆
δm−1)/vextenddouble/vextenddouble/vextenddouble,
where we have used the Cauchy-Schwarz inequality and/vextenddouble/vextenddouble/vextenddouble∇ˆfδm(x⋆
δm)/vextenddouble/vextenddouble/vextenddouble=0. Accordingly, we have
/vextenddouble/vextenddouble/vextenddoublex⋆−x⋆
δm−1/vextenddouble/vextenddouble/vextenddouble/parenleftBig
σ/vextenddouble/vextenddouble/vextenddoublex⋆−x⋆
δm−1/vextenddouble/vextenddouble/vextenddouble−/vextenddouble/vextenddouble/vextenddouble∇ˆfδm(x⋆)−∇ˆfδm(x⋆
δm−1)/vextenddouble/vextenddouble/vextenddouble/parenrightBig
≤0.
Because/vextenddouble/vextenddouble/vextenddoublex⋆−x⋆
δm−1/vextenddouble/vextenddouble/vextenddouble≥0andLemma 2.2,
/vextenddouble/vextenddouble/vextenddoublex⋆−x⋆
δm−1/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble∇ˆfδm(x⋆)−∇ˆfδm(x⋆
δm−1)/vextenddouble/vextenddouble/vextenddouble
σ
≤Lg/vextenddouble/vextenddouble/vextenddoublex⋆
δm−1−x⋆/vextenddouble/vextenddouble/vextenddouble
σ.
Hence,
/vextenddouble/vextenddouble/vextenddoublex⋆
δm−x⋆
δm+1/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddoublex⋆
δm−x⋆/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddoublex⋆
δm+1−x⋆/vextenddouble/vextenddouble/vextenddouble
≤Lg
σ/parenleftBig/vextenddouble/vextenddoublex⋆
δm−x⋆/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddoublex⋆
δm+1−x⋆/vextenddouble/vextenddouble/vextenddouble/parenrightBig
≤2Lg
σmax/braceleftBig/vextenddouble/vextenddoublex⋆
δm−x⋆/vextenddouble/vextenddouble,/vextenddouble/vextenddouble/vextenddoublex⋆
δm+1−x⋆/vextenddouble/vextenddouble/vextenddouble/bracerightBig
≤ |δm|(1−γm)
=|δm| − |δm+1|
This completes the proof.
36Under review as submission to TMLR
D.7 Proof of Proposition 3.3
Proof. By using the triangle inequality, we have, for all m∈[M],
∥x⋆
δm−x⋆∥=∥x⋆
δm−x⋆
δm+1+x⋆
δm+1−x⋆∥
≤ ∥x⋆
δm−x⋆
δm+1∥+∥x⋆
δm+1−x⋆∥
≤ ∥x⋆
δm−x⋆
δm+1∥+∥x⋆
δm+1−x⋆
δm+2∥+···+∥x⋆
δM−x⋆
δM+1∥+∥x⋆
δM+1−x⋆∥
≤(|δm| − |δm+1|) + (|δm+1| − |δm+2|) +···+ (|δM| − |δM+1|) + 0
=|δm|, (16)
where we have used x⋆
δM+1=x⋆,δM+1= 0. Therefore, from dm>1, we have
∥x⋆
δm−x⋆∥< dm|δm|.
This completes the proof of Proposition 3.3(i).In addition, if γm∈(1
dm+1,1)holds, from ( 16),
∥x⋆
δm−1−x⋆∥ ≤|δm|
γm−1
< dm|δm|.
This completes the proof of Proposition 3.3(ii).
EAdditional Experimental Results
For the sake of fairness, we provide here a version of Figures 7-9with the number of gradient queries on the
horizontal axis. Since bstochastic gradients are computed per epoch, the number of gradient queries is Tb,
where Tmeans the number of steps and bmeans the batch size.
FFull Experimental Results for Section 4.2
37Under review as submission to TMLR
1
0.1
0.01
0.002
loss function value for trainingTraining ResNet18 on CIFAR100 dataset
0.0 0.2 0.4 0.6 0.8 1.0
the number of gradient queries 1e74045505560657075accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
Figure 12: Accuracy score for testing and loss function value for training versus the number of gradient
queries in training ResNet18 on the CIFAR100 dataset. The solid line represents the mean value, and the
shaded area represents the maximum and minimum over three runs. In method 1, the learning rate and
the batch size were ﬁxed at 0.1 and 128, respectively. In method 2, the learning rate decreased every 40
epochs as in/bracketleftBig
0.1,1
10√
2,0.05,1
20√
2,0.025/bracketrightBig
and the batch size was ﬁxed at 128. In method 3, the learning
rate was ﬁxed at 0.1, and the batch size was increased as [16,32,64,128,256]. In method 4, the learning
rate was decreased as/bracketleftBig
0.1,√
3
20,0.075,3√
3
80,0.05625/bracketrightBig
and the batch size was increased as [32,48,72,108,162].
This graph shows almost the same results as Figure 7.
38Under review as submission to TMLR
1
0.1
0.01
0.001
0.0006
loss function value for trainingTraining WideResNet-28-10 on CIFAR100 dataset
0.0 0.2 0.4 0.6 0.8 1.0
the number of gradient queries 1e74550556065707580accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
Figure 13: Accuracy score for testing and loss function value for training versus the number of gradient
queries in training WideResNet-28-10 on the CIFAR100 dataset. The solid line represents the mean value,
and the shaded area represents the maximum and minimum over three runs. In method 1, the learning rate
and batch size were ﬁxed at 0.1 and 128, respectively. In method 2, the learning rate was decreased every 40
epochs as/bracketleftBig
0.1,1
10√
2,0.05,1
20√
2,0.025/bracketrightBig
and the batch size was ﬁxed at 128. In method 3, the learning rate
was ﬁxed at 0.1, and the batch size increased as [8,16,32,64,128]. In method 4, the learning rate decreased as/bracketleftBig
0.1,√
3
20,0.075,3√
3
80,0.05625/bracketrightBig
and the batch size increased as [8,12,18,27,40].This graph shows almost
the same results as Figure 8.
39Under review as submission to TMLR
0.712345
loss function value for trainingTraining ResNet34 on ImageNet dataset
0.0 0.5 1.0 1.5 2.0 2.5
the number of gradient queries 1e8464850525456586062accuracy score for test1.constant lr and constant batch size
2.only decaying lr
3.only increasing batch size
4.hybrid
Figure 14: Accuracy score for testing and loss function value for training versus the number of gradient
queries in training ResNet34 on the ImageNet dataset. The solid line represents the mean value, and the
shaded area represents the maximum and minimum over three runs. In method 1, the learning rate and
batch size were ﬁxed at 0.1 and 256, respectively. In method 2, the learning rate was decreased every 40
epochs as/bracketleftBig
0.1,1
10√
2,0.05,1
20√
2,0.025/bracketrightBig
and the batch size was ﬁxed at 256. In method 3, the learning rate
was ﬁxed at 0.1, and the batch size was increased as [32,64,128,256,512]. In method 4, the learning rate
was decreased as/bracketleftBig
0.1,√
3
20,0.075,3√
3
80,0.05625/bracketrightBig
and the batch size was increased as [32,48,72,108,162].This
graph shows almost the same results as Figure 9.
40Under review as submission to TMLR
1
0.1
0.01
0.002
loss function value for trainingTraining ResNet18 on CIFAR100 dataset
0 40 80 120 160 200
epoch45505560657075accuracy score for testpolynomial (p=0.1)
polynomial (p=0.5)
polynomial (p=0.9)
polynomial (p=1.0)
polynomial (p=2.0)polynoimal (p=4.0)
polynoimal (p=8.0)
cosine annealing
exponential decay
Figure 15: Accuracy score for testing and loss function value for training versus epochs in training of ResNet18
on the CIFAR100 dataset. The solid line represents the mean value, and the shaded area represents the
maximum and minimum over three runs. This is the full version of Figure 10.
1
0.1
0.01
0.002
0.001
loss function value for trainingTraining WideResNet28-10 on CIFAR100 dataset
0 40 80 120 160 200
epoch45505560657075accuracy score for testpolynomial (p=0.1)
polynomial (p=0.5)
polynomial (p=0.9)
polynomial (p=1.0)
polynomial (p=2.0)polynoimal (p=4.0)
polynoimal (p=8.0)
cosine annealing
exponential decay
Figure 16: Accuracy score for testing and loss function value for training versus epochs in training of
WideResNet-28-10 on the CIFAR100 dataset. The solid line represents the mean value, and the shaded area
represents the maximum and minimum over three runs. This is the full version of Figure 11.
41