Published in Transactions on Machine Learning Research (04/2024)
Fixed Budget Best Arm Identification in Unimodal Bandits
Debamita Ghosh debamita.ghosh@iitb.ac.in
IITB-Monash Research Academy
Indian Institute of Technology Bombay, India
Manjesh K. Hanawal mhanawal@iitb.ac.in
Department of Industrial Engineering & Operations Research
Indian Institute of Technology Bombay, India
Nikola Zlatanov n.zlatanov@innopolis.ru
Faculty of Computer and Engineering Sciences
Innopolis University, Russia
Reviewed on OpenReview: https: // openreview. net/ forum? id= epcLNhkoEL& noteId= g5BMyjdTBG
Abstract
We consider the best arm identification problem in a fixed budget stochastic multi-armed
bandit in which arm means exhibit unimodal structure, i.e., there is only one local maximum.
We establish that the probability of misidentifying the optimal arm within a budget of Tis
lower bounded as O/parenleftbig
exp/braceleftbig
−T/¯H/bracerightbig/parenrightbig
, where ¯Hdepends on the sub-optimality gaps of arms
in the neighborhood of the optimal arm. In contrast to the lower bound for the unstructured
case, the error exponent in this bound does not depend on the number of arms Kand is
smaller by a factor logK, which captures the gain achievable by exploiting the unimodal
structure. We then develop an algorithm named Fixed Budget Best Arm Unimodal Bandits
(FB-BAUB )that exploits unimodality to achieve the gain. Specifically, we show that the error
probability of FB-BAUB is upper bounded as O/parenleftbig
log2Kexp/braceleftbig
−T∆2/bracerightbig/parenrightbig
, where ∆is the gap
between the neighboring arms and ¯H≤2∆−2. We demonstrate that FB-BAUB outperforms
the state-of-the-art algorithms through extensive simulations. Moreover, FB-BAUB is
parameter-free and simple to implement.
1 Introduction
Multi-armed bandit (MAB) is a popular setup to study decision-making under uncertainty. It has been
applied in drug trials, recommendation systems, auctions, communication networks, and the list is growing,
see Bouneffouf & Rish (2019). In MAB, a policy is any strategy that sequentially selects arms based on
past observations. The performance of a policy is evaluated using criteria such as cumulative regret, simple
regret, or best arm identification (BAI), depending on the application, refer to Lattimore & Szepesvári (2020)
for details. While the policies optimizing cumulative regret balance exploration and exploitation optimally,
the policies optimizing simple regret, or BAI, focus on optimal exploration. In BAI, the goal is to minimize
the sample complexity of identifying the best arm within a given tolerance on the error probability (fixed
confidence setting) or to minimize the error probability of not identifying the best arm within a given number
of rounds (fixed budget setting), see (Karnin et al., 2013; Carpentier & Locatelli, 2016; Atsidakou et al.,
2022). We focus on the fixed budget BAI setting.
The classical MAB setup considers the unstructured case, where reward distributions of the arms are
independent with no relation among their means. However, many practical problems exhibit structural
properties such as smoothness, linearity, unimodality, and convexity on the mean rewards, which can be
exploited to improve the performance of the MAB algorithms (Magureanu et al., 2014; Combes et al., 2017;
Yu & Mannor, 2011; Cheshire et al., 2021). Our objective in this paper is to develop algorithms that exploit
the unimodal structure of the arm means to improve learning performance.
1Published in Transactions on Machine Learning Research (04/2024)
In unimodality, the arms’ means are increasing and then decreasing in the arms’ indices, exhibiting a change
in the monotonicity pattern only at the globally optimal arm. Many real-life applications of unimodal
structure arise in network throughput, see Hashemi et al. (2018), sequential pricing, and bidding in online
sponsored search auctions, see Yu & Mannor (2011). Unimodality is exploited in Combes & Proutiere (2014);
Saber et al. (2020a) to improve the cumulative regret bounds, where it is shown that the regret bound is
asymptotically optimal and does not depend on the number of arms. Unimodlity structure is also exploited
in Blinn et al. (2021), where the transmitter identifies the best beam in the fixed confidence setting, which is
aligned with the receiver’s beam in the wireless network. However, in wireless networks where devices (e.g.,
base stations and mobiles) need to operate synchronously, fixed budget BAI is better suited as the devices can
know exactly when to switch from exploration to exploitation and hence require less information exchange
for synchronization. This simplifies the protocol design. To the best of our knowledge, the fixed budget BAI
with unimodal structure has not been studied in the literature. Also, as noted in Atsidakou et al. (2022),
fixed budget BAI is more challenging than their fixed confidence counterparts, and our work fills this gap.
We develop an algorithm named Fixed Budget Best Arm in Unimodal Bandits ( FB-BAUB )to address the
BAI problem with the unimodal structure. The algorithm is motivated by the Line Search Eliminations
(LSE)algorithm, introduced in Yu & Mannor (2011) and works in phases. In each phase of FB-BAUB ,
a portion of the arms is eliminated ( ∼33%) based on empirical means of the arms, thereby reducing the
search space for the optimal arm in the subsequent phases. For a given budget Tand the number of
armsK, we show that FB-BAUB achieves an error probability of the order of O(logKexp(−T∆2))under
the assumption of the minimum separation gap between neighboring arms is at least ∆>0. Under the
same minimum gap assumption, the best known achievable error probability (by Sequential Halving) is of
orderO/parenleftig
logKexp/parenleftig
−T∆2
logK/parenrightig/parenrightig
for the unstructured case, see Audibert et al. (2010). Thus, by exploiting the
unimodal structure, we reduced the error exponent by factor logK. We show that this reduction is the
best possible by establishing a lower bound. Thus, we quantify the gain one can achieve by exploiting the
unimodal structure. In establishing the lower bound, we generalize the “flipping constructions" of the bandit
instance from Carpentier & Locatelli (2016) to include distribution with unbounded support and adapt it to
unimodal bandits.
In summary, our contributions are as follows:
•We establish a lower bound for the unimodal bandits in a fixed budget BAI setting.
•We develop a parameter-free algorithm, named FB-BAUB , and derived an upper bound on its error
probability. We show that the error exponent in the bound does not depend on K. The lower bound
shows that the error exponent of FB-BAUB is of the right order.
•We empirically validate the superior performance of FB-BAUB compared to the other state-of-the-art
algorithms in the literature that exploit the unimodal structure.
Detailed proofs of all the statements are given in Appendix A.
2 Related Work
The BAI problem is well-studied in the unstructured bandits in both fixed confidence and fixed budget
settings, see Mannor & Tsitsiklis (2004); Even-Dar et al. (2006); Kalyanakrishnan et al. (2012); Karnin et al.
(2013); Kaufmann et al. (2016); Jamieson & Nowak (2014); Garivier & Kaufmann (2016); Atsidakou et al.
(2022); Wang et al. (2021). The authors in Gabillon et al. (2012) develop a unifying approach to analyse
both settings leading to a meta-algorithm that can be applied to both settings. The lower bounds for these
settings for unstructured bandits are developed by Chen & Li (2015); Carpentier & Locatelli (2016); Audibert
et al. (2010).
Improving Cumulative Regret: Several works exploit the structural properties of the bandits to minimize
cumulative regret. Abbasi-Yadkori et al. (2011); Chu et al. (2011); Dani et al. (2008) exploit linearity of
observed rewards. The authors in Cesa-Bianchi & Lugosi (2012); Combes et al. (2015) studied Combinatorial
bandits with bandit feedback exploit the combinatorial structure of the arms to learn the best subset of arms.
2Published in Transactions on Machine Learning Research (04/2024)
Unimodal structure is exploited in Yu & Mannor (2011); Combes & Proutiere (2014) to improve the regret
performance. Combes & Proutiere (2014) provide a lower bound on any policy exploiting unimodal structure
and develop an Optimal Sampling for Unimodal Bandits (OSUB) algorithm with a matching upper bound.
Saber et al. (2020b) developed algorithms that do not require force explorations as used in the OSUB. The
smoothness of the rewards expressed in terms of Lipschitz conditions are studied by Magureanu et al. (2014);
Valko et al. (2014); Hanawal et al. (2015). A generic framework for analysing the cumulative regret of bandits
exhibiting structural properties that include linearity, smoothness, and unimodality property is given in
Combes et al. (2017). All of the aforementioned works are in the cumulative regret minimization setting.
Improving Best Arm Identification: In the best arm identification setting, the authors in Soare et al.
(2014); Jedra & Proutiere (2020); Azizi et al. (2022) exploit the linearity structures, and the authors in Kocák
& Garivier (2020) exploit the smoothness structures to improve performance of BAI algorithms. The authors
in Wang et al. (2021) studied fixed confidence BAI problem and developed Frank-Wolfe-based Sampling
(FWS) whose sample complexity matches the lower bounds for a wide class of pure exploration problems.
They applied FWS to other structural bandits such as threshold, linear, and Lipschitz, but they did not
consider unimodal bandits. The authors in Garivier et al. (2017) consider threshold bandit problems (TBP),
where the means of the arms are monotonically increasing, and the goal is to identify the set of arms with
means above a given threshold. The authors characterise the sample complexity of the TBP in the fixed
confidence setting. The TBP is extended to include other structural properties such as unimodality and
concavity in Cheshire et al. (2020), and problem-independent bounds on the simple regret are derived. The
paper Cheshire et al. (2021) extended the analysis of the TBP with monotonicity and concavity to establish
problem-dependent bounds.
Lower Bounds: The lower bounds for the best arm identification are explored in Audibert et al. (2010);
Garivier & Kaufmann (2016), and Carpentier & Locatelli (2016). In the fixed budget setting, Audibert
et al. (2010) established a first lower bound, and it was improved in Garivier & Kaufmann (2016) using
‘flipping constructions’. Notably, these approaches assume a parametric form (Bernoulli and Gaussian) for the
underlying distributions and establish a minimax bound. Further, refining the flipping argument, Carpentier &
Locatelli (2016) derived a lower bound that matches with the upper bound of the Successive Reject algorithm
introduced in Audibert et al. (2010), thus establishing a tighter lower bound. The case of non-parametric
bandits is studied in Barrier et al. (2023), which concentrates on developing instance-independent bounds for
fixed budget scenarios. However, these bounds hold only asymptotically. The above work deals with the
unstructured case, whereas we deal with the structured (unimodal) bandits.
Our work is closer to Cheshire et al. (2020), Yu & Mannor (2011), and Carpentier & Locatelli (2016). Cheshire
et al. (2020) focuses on TBPs to exploit the unimodal property and provide a guarantee on the simple regret,
which is different from our setting. The LSE algorithm introduced in Yu & Mannor (2011) provides the
PAC guarantees in the fixed confidence setting for a continuous set of arms. The algorithm runs in phases,
and the arms played in each phase are selected based on the golden ratio. Though our algorithm adapts
the basic ideas of LSE, it differs in how arms are selected and eliminated. Also, it does not require any
problem-dependent information. The details are given in Subsection 5.1.
3 Problem Setup
3.1 Notations
We consider the stochastic multi-armed bandit setting where the learner explores a finite set A={1,2,...,K}
of arms over a fixed horizon T. The reward sequence for arm k∈Acorresponds to independently and
identically (i.i.d.) samples drawn from a distribution p(µk)with an unknown mean µk, i.e.,µk=EX∼p(µk)[X].
We assume p(µk)isβsub-Gaussian, where β >0,∀k∈A.
Letϵβdenote the set of all bandits instances that are βsub-Gaussian with distinct arm means. For
any instance p(µ) :={p(µ1),...,p (µK)}∈ϵβwith mean rewards of the arms as µ:={µ1,...,µK}, let
k∗:=k∗/parenleftbig
p(µ)/parenrightbig
=arg max
k∈Aµkdenote the optimal arm, i.e., the arm with the highest mean. We denote ϵUas
the set of bandits in ϵβsatisfying unimodality, defined below.
3Published in Transactions on Machine Learning Research (04/2024)
Definition 1. (Unimodality): A bandit instance p(µ)∈ϵβis said to be unimodal iff µ1<µ2<···<µk∗
andµk∗>µk∗+1>···>µK.
LetSbe the set of the arm means satisfies the unimodal structure, i.e.,
S=/braceleftbig
µ:µ1<µ2<···<µk∗>µk∗+1>···>µK/bracerightbig
.
3.2 Learning setup
We consider the following interaction between the learner and the environment over fixed rounds T >0. We
refer toTas the fixed budget. For any round 1≤t≤T, the learner chooses an arm kt∈Aand observes a
stochastic reward drawn from the distribution p(µkt). In each round t, the learner decides which arm to play
based on the samples observed in the past. At the end of T, the learner returns an arm kT∈A. A policyπ
is any strategy that selects an arm in each round, given the past observations. For a given policy π, letkπ
T
denote the arm output at the end of Tbudget. Let Πdenote the set of all policies that output an arm within
Tbudget on unimodal bandits instances.
Objective: The goal is to find a policy in Πthat exploits the unimodal structure of the mean rewards and
minimizes the probability that the arm output at the end of Tbudget is not the optimal arm. Specifically,
our objective is given as follows:
inf
π∈Πsup
µ∈SPp(µ)(kπ
T̸=k∗), (1)
where Pr(·)is over the randomness of the reward and the policy. We refer to the BAI setup given in (1)as
thefixed budget BAI for unimodal bandits .
3.3 Problem Dependent Complexity
For a given instance p(µ)∈ϵUwith arm means µ, we express the complexity as ¯H:=¯H/parenleftbig
p(µ)/parenrightbig
, and is given
by
¯H:=/summationdisplay
k∈{k∗−1,k∗+1}1
(µk∗−µk)2. (2)
We call ¯Hcomplexity as the characterisation of the hardness of understanding the problem, as we will see
later. Similar problem-dependent quantities are consider in Audibert et al. (2010); Carpentier & Locatelli
(2016); Jamieson & Nowak (2014), that characterize the complexity of bandit problems, e.g.,
H1=/summationdisplay
k̸=k∗1
(µk∗−µk)2andH2= sup
k>1k
(µk∗−µ(k))2,
whereµ(k)denotes the kthlargest mean of the arms. Following Audibert et al. (2010), it is easy to show the
following inequalities hold (see Appendix A.1 for proof)
min(H2,¯H)≤H1≤2 log(K)H2. (3)
We next consider the lower bound for fixed budget BAI with the unimodal structure.
4 Lower Bound for Fixed Budget BAI of Unimodal Bandits
A lower bound on the error probability for BAI in the fixed budget setting without assuming any structure is
established in Audibert et al. (2010); Garivier & Kaufmann (2016) and Carpentier & Locatelli (2016) using
different techniques for constructing bandit problems, and all of them have provided minimax lower bound
for the unstructured bandits. More specifically, Audibert et al. (2010) constructs K!bandit problems by
4Published in Transactions on Machine Learning Research (04/2024)
permutation of the arms and proposes that for any bandit problem, there exists a permutation such that any
algorithm will make an error with probability at least exp/parenleftig
−T
H2/parenrightig
. Garivier & Kaufmann (2016) proposed
the ‘flipping constructions’ and showed that there exists a bandit problem such that any algorithm will make
an error with probability at least exp/parenleftig
−T
H1/parenrightig
, whereH2<H 1. Carpentier & Locatelli (2016) improved the
flipping construction of Garivier & Kaufmann (2016) by providing further information to the algorithm. They
proposed that there exists a bandit problem such that any algorithm will make an error with a probability of
at least exp/parenleftig
−T
log2(K)H1/parenrightig
. The authors argue that in the fixed budget setting, unlike in the fixed confidence
setting, there is an additional log(K)price to pay for adaptation to H1in the absence of knowledge over this
quantity.
We adopt the lower bound proof of Carpentier & Locatelli (2016) for fixed budget BAI problems on unimodal
instances. Carpentier & Locatelli (2016) provided a lower bound using a particular choice of Bernoulli
rewards, whereas we do not assume any particular choice of bandit instances. Our only assumption is that an
unimodal structure exists over the mean rewards, and we consider Gaussian distributions for our case. Below,
we have given an overview of our construction.
Fixβ= 1. Letp(µ) :={p(µk)}k∈A∈ϵUbe a unimodal bandit instance such that p(µk) :=N(µk,1), where
µk∈[1/4,1/2]for allk∈Aandµk∗= 1/2. Letp′(µ′) :={p′(µ′
k)}k∈Abe another bandit instance where
p′(µ′
k) :=N(µ′
k,1)andµ′
k= 2µk∗−µkfor allk∈A. Usingp/parenleftbig
µ/parenrightbig
andp′/parenleftbig
µ′/parenrightbig
we construct two more bandit
instancepk∗−1(µk∗−1)andpk∗+1(µk∗+1)with meansµk∗−1andµk∗+1as follows.
µk∗−1
i =µi∀i̸=k∗−1andµk∗−1
i =µ′
ifori=k∗−1
µk∗+1
i =µi∀i̸=k∗+ 1andµk∗−1
i =µ′
ifori=k∗+ 1
It is easy to note that both bandit instances pk∗−1/parenleftbig
µk∗−1/parenrightbig
andpk∗+1/parenleftbig
µk∗+1/parenrightbig
are unimodal with the optimal
arms being k∗−1andk∗+ 1, respectively. Recall the definition of ¯H(p(µ))given in (2). We define ¯has
¯h:=/summationdisplay
i∈{k∗−1,k∗+1}1
(µk∗−µi)2¯H/parenleftbigg
pi/parenleftbig
µi/parenrightbig/parenrightbigg,
where ¯H/parenleftbigg
pi/parenleftbig
µi/parenrightbig/parenrightbigg
for bandit instance pi/parenleftbig
µi)is defined as
¯H/parenleftbigg
pi/parenleftbig
µi/parenrightbig/parenrightbigg
=/summationdisplay
k∈{i−1,i+1}1
(∆i
k)2,where ∆i
k=/braceleftigg
2µk∗−µi−µk,ifk̸=i,
µk∗−µi, ifk=i.
Note that the authors in Carpentier & Locatelli (2016) have defined the quantity h∗=/summationtext
i∈A,i̸=k∗1
(µk∗−µi)2¯H(pi(µi)). However, for the unimodality structure of the mean rewards, we can define
the quantity ¯hon the neighbourhood of the optimal arm k∗. We now provide the lower bound of the fixed
budget BAI problem for unimodal bandits, as stated in the following theorem.
Theorem 1. For any unimodal bandit strategy that returns arm kTafterTbudget, where T≥
max
i∈{k∗−1,k∗+1}/parenleftbigg
¯H/parenleftbig
p(µ)/parenrightbig
,¯H/parenleftbig
pi(µi)/parenrightbig¯h/parenrightbigg
4 log(6TK)
12, it holds that
max
i∈{k∗−1,k∗+1}/bracketleftigg
Ppi(µi)(kT̸=i) exp/parenleftigg
15T
¯H/parenleftbig
pi(µi)/parenrightbig/parenrightigg/bracketrightigg
≥1
6. (4)
Proof.The proof is given in Appendix A.2.
From the above theorem, we conclude that the lower bound for the fixed budget unimodal bandit is
O/parenleftig
1
6exp/braceleftig
−15T
¯H(p(µ)/bracerightig/parenrightig
. Note that the exponent does not depend on K, but only on the sub-optimal gaps of
5Published in Transactions on Machine Learning Research (04/2024)
the neighbours of the optimal arm. Our focus on improving the scaling in Kis motivated by the study of
unimodal bandits in the regret setting, where the unimodal property helps improve the scaling with respect
toK. Specifically, a similar observation is also made for unimodal bandits in the cumulative regret setting,
see Combes & Proutiere (2014).
5 FB-BAUB Algorithm
We propose an algorithm for unimodal bandit instances in the fixed budget BAI setting. The algorithm is
based on the Line Search Elimination (LSE) method developed in Yu & Mannor (2011)and we refer to it as
Fixed Budget Best Arm in Unimodal Bandits ( FB-BAUB ). It is a parameter-free algorithm that only needs
to knowKandT, and the arms are eliminated based on their empirical means.
FB-BAUB splits the total budget TintoL+ 1phases. Let Nlfor phasel= 1,2,...,L + 1, denotes the
number of samples in phase l, whereL+1/summationtext
l=1Nl=T. We have chosen Nlsuch that after the first two phases, the
number of samples increases by a factor of 3/2in each subsequent phase, which helps to distinguish between
the empirical means of the remaining arms, which are likely to be closer. Thus, Nlis given by
Nl=/braceleftigg
2L−2
3L−1Tforl= 1,2
2L−(l−1)
3L−(l−2)Tforl= 3,4,...,L + 1(5)
and satisfy the budget constraints, i.e,
2×2L−2T
3L−1+L+1/summationdisplay
l=32L−(l−1)T
3L−(l−2)=T. (6)
The arms are sampled and eliminated in each phase, so only one arm survives after the L+ 1phase.
The pseudo-code of FB-BAUB is given in ALGO 1. It works as follows: Let Bldenote the set of arms
available in phase landjl:=|Bl|is the number of arms in the set Bl. In each phase l= 1,2,...L, the
algorithm selects four arms Sl={kM,kA,kB,kN}∈Bl, which include the first, last and the two middle
arms uniformly spaced from them (lines 4-7). Each of the arms is sampled for Nl/4number of times (line 8).
At the end of the phase, their empirical means, denoted as ˆµk(line 9), are obtained as follows:
ˆµl
k=1
Nl/4Nl/4/summationdisplay
s=1Xl
k,s,∀k∈Sl, (7)
whereXl
k,sdenotes the sthsample from the ktharm in phase l. Based on these empirical means, we eliminate
at most 1/3rdof the number of arms from the remaining set.1Specifically, if the arms kMorkAhave the
highest empirical means, we eliminate all the arms succeeding kBin the setBl(line 12). Similarly, if the
armskBorkNhave the highest empirical means, we eliminate all the arms preceding kAin the setBl(line
14). Fig. 1 gives a pictorial representation of the elimination of arms in two possible cases. The remaining set
of arms is then transferred to the next phase. In phase L+ 1, we are left with three arms. Each is sampled
NL+1/3times, and the one with the highest empirical mean is output as the optimal arm (lines 18-22).
Remark 1. Arms between kM&kAorkB&kNare eliminated in each phase, and the arms between
kA&kBalways survive.
After phase l= 1,2,...,L,⌊2
3jl⌋of the arms survive. For ease of exposition, we will drop the ⌊⌋function
since this drop will influence only a few constants in the analysis. Thus, after the end of the Lphases, there
will be three arms as
(2/3)LK= 3 =⇒L=log2K/3
log23/2. (8)
Therefore, FB-BAUB outputs the best arm as ˆkL+1(i.e.,kT) after exploring for Trounds.
1If the number of arms in a phase is not a multiple of 4, then less than 1/3rdwill be eliminated in that phase.
6Published in Transactions on Machine Learning Research (04/2024)
ALGO 1: Fixed Budget Best Arm in Unimodal Bandits (FB-BAUB)
1:Input:TandK.
2:Initialise:B1=A,j1←|B 1|. Calculate Lfrom (8).
3:forl= 1toLdo
4:kM←First arm ofBl;
5:kN←Last arm ofBl;
6:kA←⌈jl/3⌉tharm ofBl;
7:kB←⌊2jl/3⌋tharm ofBl;
8:Sample each arm in Sl={kM,kA,kB,kN}forNl
4number of times from (5)
9:Obtain ˆµl
kM,ˆµl
kA,ˆµl
kB,ˆµl
kNby (7).
10:x∗
l= arg maxk∈Slˆµk.
11:ifx∗
l=={kM,kA}then
12:Bl+1←{k∈Bl:kM≤k≤kB}Shrink to left
13:else ifx∗
l=={kB,kN}then
14:Bl+1←{k∈Bl:kA≤k≤kN}Shrink to right
15:end
16:jl+1←|Bl+1|;
17:end for
18:forl=L+ 1do
19:BL+1={kM,kA,kN};
20:Sample each arm in {kM,kA,kN}forNL+1
3no. of times and obtain ˆµl
kM,ˆµl
kA,ˆµl
kN.
21:Obtain ˆkL+1= arg maxk∈BL+1ˆµk.
22:end for
23:Output:kT=ˆkL+1
Figure 1: Different cases of elimination in phase l.
5.1 Comparison between LSE and FB-BAUB
In Yu & Mannor (2011), LSE is developed for a continuous set of arms. However, they have also applied LSE
for a finite set of arms in the fixed confidence setting. FB-BAUB and LSE differ primarily in four aspects.
1.Elimination: LSE eliminates about 1−1/ϕfraction of arms, where ϕis the golden ratio. In contrast,
FB-BAUB eliminates 1/3rdof the available arms in each phase.
2.Input parameters: LSE needs a sequence of parameters ( ϵl,δl)for every phase as input, whereas no
such input parameters are required in FB-BAUB . Hence, it is a parameter-free algorithm, which is
highly desirable.
3.Number of samples : Inthediscretecase, LSEconsiderstheinstancewheremeanrewardsofneighboring
arms are separated at least by an amount DL, i.e., ∆>DL(Yu & Mannor (2011)[Assum 3.4]), and
uses this information in deciding the arm plays in each phase (Combes & Proutiere (2014)[Prop. 5.4]).
Whereas FB-BAUB does not require any problem-specific information and works as long means of
the neighboring arms are separated, i.e., ∆>0.
7Published in Transactions on Machine Learning Research (04/2024)
4.Arms selection policy : LSE adds one new arm based on the golden ratio in each phase. Whereas
FB-BAUB adds two new arms in each phase by uniformly dividing the space.
6 Performance Guarantee of FB-BAUB
This section provides the following theorem that gives the upper bound for the error probability of FB-BAUB .
Theorem 2. Letp(µ)∈ϵUfollowβsub-Gaussian with arm means µand∆ = min
2≤i≤K|µi−µi−1|>0denote
the minimum gap between the means of any two neighboring arms. For any T >K, the error probability of
FB-BAUB is bounded as
Pp(µ)(ˆkL+1̸=k∗)≤2 exp/braceleftigg
−TK
32/parenleftbigg∆
β/parenrightbigg2/bracerightigg
+ 2 exp/braceleftigg
−TK
72/parenleftbigg∆
β/parenrightbigg2/bracerightigg
+ 2 exp/braceleftigg
−T
24/parenleftbigg∆
β/parenrightbigg2/bracerightigg
+ 2(L−2) exp/braceleftigg
−T
8/parenleftbigg∆
β/parenrightbigg2/bracerightigg
. (9)
Proof.The proof is given in Appendix A.3.
The first two terms in the upper bound correspond to the probability of eliminating the optimal arm k∗in
phases 1 & 2. The 3rdterm bounds the probability of eliminating the optimal arm in phase L+ 1, and the
4thterm corresponds to the sum of the probabilities of eliminating the optimal arm in phases l= 3,...,L.
Note that, as K > 1,exp/braceleftbigg
−TK/parenleftig
∆
β/parenrightig2/bracerightbigg
<exp/braceleftbigg
−T/parenleftig
∆
β/parenrightig2/bracerightbigg
. As a result, the 3rdand4thterms dominate
the upper bound (9). Therefore, the error probability is of order O/parenleftbig
log2Kexp/braceleftbig
−T∆2/bracerightbig/parenrightbig
, where the error
exponent term exp/braceleftbig
−T∆2/bracerightbig
does not depend on K.
Comparison with Sequential Halving: For unstructured bandits, the complexity parameter H2, as
given in Subsection 3.3, can be upper bounded by H2<H 1=/summationtext
k̸=k∗1
(µk∗−µk)2≤/summationtext
k̸=k∗1
(k−k∗)2∆2≤π2
3∆2≤4
∆2.
Applying this inequality, the upper bound of the error probability of Sequential Halving as proposed in Karnin
et al. (2013)[Thm. 4.1] is revised as of order O/parenleftig
log2Kexp/braceleftig
−T∆2
logK/bracerightig/parenrightig
and is optimal as it matches with
the lower bound derived in Carpentier & Locatelli (2016) up to a multiplicative factor of log2K. Note that
the exponent term in the error bound of Sequential Halving has a log2Kfactor. For unimodal bandits, the
exponent term in the error bound of FB-BAUB does not depend on Kand is smaller by a factor of log2K.
As expected, the error probability for unimodal bandits should be smaller, and our analysis captures this
gain by shaving off the factor log2Kin the error bound.
Comparision with LSE: The following points will discuss the novelty of the analysis of FB-BAUB , compared
to the analysis of LSE, as follows:
1.Technical Challenges of FB-BAUB: The PAC-bound provided by the LSE algorithm in Yu &
Mannor (2011) is based on the known (ϵl,δl)-PAC bound of the Sampling Algorithm (refer to Thm.
4.1) for every iteration. However, FB-BAUB does not run any sub-algorithm but has to carefully
constructNlso that the budget constraint that balances the trade-off of elimination and exploration
of new arms is attained. Note that Nlare of different lengths over the phases, and the error bound
of the FB-BAUB is obtained by carefully applying Hoeffding’s inequality in each phase.
2.Minimum Gap Separation: For a finite set of arms, LSE assumes that the gap between the mean
rewards of the neighboring arms is separated by at least DL>0(Assumption 3.2 in Yu & Mannor
(2011)). More specifically, LSE requires knowledge of DL, and its sample complexity is expressed in
terms of (ϵl,δl)for each phase l. However, FB-BAUB only considers that ∆>0, i.e., the arm means
to be distinct. We do not need any assumptions on the minimum separation of the mean rewards
of the neighboring arms, i.e., FB-BAUB need not know DL.Therefore, FB-BAUB works when the
arm means are distinct but arbitrarily close to each other, whereas LSE analysis assumes that this
separation is at least DL.
8Published in Transactions on Machine Learning Research (04/2024)
3.Analysis of fixed-budget setting vs. fixed-confidence setting: Our analysis for the finite set
of arms differs from that of Yu & Mannor (2011), as we do not require the knowledge of DL. Since
DLis unknown, this is not the same as ’computing the sample size with a required estimation error’.
We have to carefully control the elimination and decide the duration of the samples to meet the
budget constraints. However, as rightly noted, in the case of Yu & Mannor (2011), ’computing the
sample size with a required estimation error’ applies as they use the knowledge of DLfor the finite
set of arms.
4.Novelty in choice of Nl:The nature of selecting an arm for sampling based on the golden ratio
in Yu & Mannor (2011) makes the separation between the arms selected for sampling non-uniform,
which in turn makes the number of arms eliminated in each phase a random quantity. Hence, LSE
is not a good strategy for fixed-budget settings where the number of samples is constrained, and
we need to have a good accounting of the remaining arms for analytical traceability. On the other
hand, in FB-BAUB ,2/3of the arms remain in each phase, and we increase the number of samples
collected in subsequent phases by a factor of 3/2. This helps us in two ways. (a) meet the budget
constraint exactly across the phases, and (b) the number of samples in phase independent of the
problem instance (like knowing DL).
We note that the error exponent of FB-BAUB differs from the optimal error exponent with respect to the
complexity terms as ¯H/parenleftbig
p(µ)/parenrightbig
≤2/∆2and hence is not optimal with respect to the specific problem instance.
It is an interesting open problem to develop an optimal algorithm in the fixed budget BAI setting with
unimodality. However, our main focus is to improve the scaling of the error probability bound w.r.t. K. We
have provided a near-optimal solution and have shown that the error exponent for both the lower and upper
bounds does not involve K, quantifying the gain achieved by exploiting the unimodal structure. We are
motivated by the study of unimodal bandits in the regret setting (Combes & Proutiere (2014)), where the
unimodal property helps improve the scaling of the regret bound with respect to K.
7 Simulation Results
In this section, we corroborate the theoretical guarantee of FB-BAUB by applying it to problem instances
of varying difficulty. As no algorithm exists for the BAI in unimodal bandits in the fixed budget setting,
we consider the following benchmark algorithms. The source code is available at https://github.com/
debamita-ghosh/FBBAUB .
Sequential Halving (Seq. Halv.) Karnin et al. (2013): This fixed budget BAI algorithm is for
unstructured bandits but is shown to be optimal by Carpentier & Locatelli (2016). A comparison with this
algorithm gives gains achieved by exploiting structure.
Successive Rejects (SR) Audibert et al. (2010): This fixed budget BAI algorithm for unstructured
bandits is parameter-free and optimal up to a logarithmic term. SR outperforms UCB-E in Audibert et al.
(2010); Shahrampour et al. (2017). Hence, we do not consider UCB-E for comparison. A comparison with
this algorithm gives gains achieved by exploiting structure.
Linear Search Elimination (LSE) Yu & Mannor (2011): We consider the discrete variant of LSE
proposed for a finite set of arms in the fixed confidence case and adopt it to the fixed budget setting. A
comparison of FB-BAUB with LSE is pertinent as it is a well-known algorithm for unimodal bandits.
7.1 Comparison with pure exploration algorithms for unimodal structure
We consider two experimental setups to compare the performance of FB-BAUB with other benchmark
algorithms. We consider KGaussian arms with known variances σ2
i=σ2= 5, assuming that the mean of
the best arm is µk∗=−252. Our simulations are averaged over 1000runs and are shown with confidence
intervals.
Experiment 1: µ1=−312, µk∗=−252, µK=−311, µ2:k∗−1=µ1+2(k−1)(µk∗−µ1)
k∗−1, andµk∗+1:K=
µk∗−2(k−k∗)(µk∗−µK)
K−k∗−1.
9Published in Transactions on Machine Learning Research (04/2024)
0250 500 7501000 1250 1500 1750 2000
Budget (T)0.00.10.20.30.40.50.60.70.80.9Error Probability
FB-BAUB, K = 80, Experiment 1
LSE, K = 80, Experiment 1
Seq. Halv., K = 80, Experiment 1
SR, K = 80, Experiment 1
FB-BAUB, K = 80, Experiment 2
LSE, K = 80, Experiment 2
Seq. Halv., K = 80, Experiment 2
SR, K = 80, Experiment 2
Figure 2: Error Probability vs T,K= 80.
0250 500 7501000 1250 1500 1750 2000
Budget (T)0.00.10.20.30.40.50.60.70.80.9Error Probability
FB-BAUB, K = 100, Experiment 1
LSE, K = 100, Experiment 1
Seq. Halv., K = 100, Experiment 1
SR, K = 100, Experiment 1
FB-BAUB, K = 100, Experiment 2
LSE, K = 100, Experiment 2
Seq. Halv., K = 100, Experiment 2
SR, K = 100, Experiment 2 Figure 3: Error Probability vs T,K= 100.
0250 500 7501000 1250 1500 1750 2000
Budget (T)0.00.10.20.30.40.50.60.7Error Probability
FB-BAUB, K=80
LSE, K=80
Seq. Halv., K=80
SR, K=80
FB-BAUB, K=100
LSE, K=100
Seq. Halv., K=100
SR, K=100
Figure 4: Exp. 3: Error Prob. vs T.
0250 500 7501000 1250 1500 1750 2000
Budget (T)0.00.10.20.30.40.50.60.7Error Probability
FB-BAUB, K=80
LSE, K=80
Seq. Halv., K=80
SR, K=80
FB-BAUB, K=100
LSE, K=100
Seq. Halv., K=100
SR, K=100 Figure 5: Exp. 4: Error Prob. vs T.
Experiment 2: µ1=−312, µk∗=−252, µK=−311,µ2:k∗−1=µ1+(k−1)(µk∗−µ1)
(k∗−1), andµk∗+1:K=
µk∗−(k−k∗)(µk∗−µK)
(K−k∗−1).
Fig. 2 and Fig. 3 illustrate the performance of each algorithm for Exp. 1 and Exp. 2. We examine each
setup for two values of K={80,100}. Exp. 1 and 2 are strictly unimodal in the sense that the optimal arm
is lying in the interior. In Exp. 1, the means of the successive arms are well-separated compared to that in
Exp. 2, i.e., ∆is lower for Exp. 2. As we decrease the gap between the means of the neighboring arms, the
means of the neighboring arms of the optimal arm come close to each other. Hence, identifying the best arm
becomes complicated, which increases the error probability. This makes Exp. 2 more challenging to identify
the optimal arm compared to Exp. 1, and thereby the error probability is higher for Exp. 2 than Exp. 1.
Moreover, the error probability increases as we increase the arm size. We compared the empirical number of
pulls of each arm for Exp. 1 and 2 for the state-of-the-art algorithms in Appendix A.4, see Fig. 7 and Fig. 8.
SR has the worst error performance for each setup. For a small number of arms, both Seq. Halv. and
FB-BAUB have comparable performance, but as the number of arms increases, FB-BAUB has a lesser error
probability compared to Seq. Halv. as evident from the case of K= 100. Moreover, FB-BAUB can identify
the best arm with a probability of more than 95% with a lesser budget than other state-of-the-art algorithms.
More specifically, in Exp. 1 FB-BAUB can identify the best arm with a probability of more than 95% within
100budget for 80arms, while the other state-of-the-art algorithms need at least a 500budget for executions.
Hence, FB-BAUB outperforms the state-of-the-art algorithms.
10Published in Transactions on Machine Learning Research (04/2024)
We note that the minimum budget requirement (as a function of K) for LSE is much smaller than both
FB-BAUB and Seq. Halv. for its feasible execution. However, as LSE samples for a fixed number of times
for each of the arms in every phase, the number of samples it runs for arms neighboring k∗is much lesser,
resulting in a higher error probability. Seq. Halv. needs at least Klog2(K)number of horizons to complete
one phase and has samples for all arms in every phase. Note that for each setup, Seq. Halv. requires at least
100and300rounds for K= 80andK= 100, respectively, to complete their execution, and hence, their
graph starts after those many rounds. Thereby, it has fewer rounds remaining to explore the best arm when
the algorithm is executed in the neighbourhood of k∗compared to FB-BAUB . Thus, the minimum budget
requirement for FB-BAUB as a function of Kis much less than that of Seq. Halv. In addition, FB-BAUB
has better error probability performance. This demonstrates the advantage of exploiting the unimodality of
the reward function.
7.2 Comparison with pure exploration algorithms for monotone structure
We have considered two more experimental set-ups to compare FB-BAUB with the state-of-the-art algorithms.
We consider KGaussian arms with known variances σ2
i=σ2= 0.5, assuming that the mean of the best arm
isµ1= 0.7. Our simulations are averaged over 1000runs and are shown with confidence intervals.
Experiment 3: µ1= 0.7andµ2:K=µ1−0.6(i−1)
K−1.
Experiment 4: µ1= 0.7andµ2:K=µ1−0.01/parenleftbig
1 +4
K/parenrightbigi−2.
Fig. 4 and Fig. 5 illustrate the performance of each algorithm for Exp. 3 and Exp. 4, respectively, for
two values of K={80,100}. Exp. 3 and Exp. 4 follow the monotone structure with the first arm as the
optimal arm, similar to that considered in Shahrampour et al. (2017); Audibert et al. (2010). The error
probability increases as we increase the arm size. In Exp. 3, the sub-optimal gap decreases in arithmetic
progression, whereas in Exp. 4, the sub-optimal gap decreases in geometric progression. This makes Exp. 4
more challenging to find the best arm within a fixed budget, and thereby, the error probability of FB-BAUB
is lower in Exp. 3 than in Exp. 4. We compared the empirical number of pulls of each arm for Exp. 3 and 4
for the state-of-the-art algorithms in Appendix A.4, see Fig. 9 and Fig. 10.
Furthermore, in Exp. 3, FB-BAUB can identify the optimal arm with a probability of more than 80% within
100 budget for 80arms, and in Exp. 4, it identifies the optimal arm with a probability of more than 75%
within 100 budget. Whereas, the other state-of-the-art algorithms need at least a 600budget for execution.
Hence, FB-BAUB outperforms the other state-of-the-art algorithms for Exp. 3 and 4.
8 Conclusion
We studied the fixed budget BAI problem with an unimodal structure on a finite set of Karm means. We
developed an algorithm named FB-BAUB to address the problem and derived an upper bound on its error
probabilities. The algorithm works in phases and identifies the best arm with high probability. We established
that the exponent in the error bound is independent of Kin contrast to the unstructured bandits. We
demonstrated that for any optimal algorithm, the error exponent should be independent of Kby establishing
a lower bound. Simulations validated the efficiency of FB-BAUB compared to state-of-the-art algorithms.
FB-BAUB is parameter-free and easy to implement.
Many interesting research directions could be further investigated. The exponent in the upper bound on the
error probability of FB-BAUB is optimal in TandK, but not in the problem-dependent complexity terms,
which are characterized in terms of the smallest gap between any neighbouring arms. However, the lower
bound is only dependent on the neighbours of the optimal arm. It is interesting to develop algorithms that
are also optimal with respect to the complexity terms.
Acknowledgement
Manjesh K. Hanawal thanks funding support from SERB, Govt. of India, through the Core Research Grant
(CRG/2022/008807) and MATRICS grant (MTR/2021/000645), and funding support from DST, Govt. of
India, through the DST-INRIA targeted programme and the DST-INRIA joint research targeted programme.
11Published in Transactions on Machine Learning Research (04/2024)
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.
Advances in neural information processing systems , 24, 2011.
Alexia Atsidakou, Sumeet Katariya, Sujay Sanghavi, and Branislav Kveton. Bayesian Fixed-Budget Best-Arm
Identification. arXiv preprint arXiv:2211.08572 , 2022.
Jean-Yves Audibert, Sébastien Bubeck, and Rémi Munos. Best arm identification in multi-armed bandits. In
COLT, pp. 41–53, 2010.
MohammadJavad Azizi, Branislav Kveton, and Mohammad Ghavamzadeh. Fixed-Budget Best-Arm
Identification in Structured Bandits. In Proceedings of the Thirty-First International Joint Conference on
Artificial Intelligence, IJCAI-22 , pp. 2798–2804, 7 2022.
Antoine Barrier, Aurélien Garivier, and Gilles Stoltz. On Best-Arm Identification with a Fixed Budget in
Non-Parametric Multi-Armed Bandits. In International Conference on Algorithmic Learning Theory , pp.
136–181. PMLR, 2023.
Nathan Blinn, Jana Boerger, and Matthieu Bloch. mmWave Beam Steering with Hierarchical Optimal
Sampling for Unimodal Bandits. In ICC 2021-IEEE International Conference on Communications , pp.
1–6. IEEE, 2021.
Djallel Bouneffouf and Irina Rish. A Survey on Practical Applications of Multi-Armed and Contextual
Bandits. CoRR, abs/1904.10040, 2019. URL http://arxiv.org/abs/1904.10040 .
Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification
bandit problem. In Conference on Learning Theory , pp. 590–604. PMLR, 2016.
Nicolò Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences ,
78(5):1404–1422, 2012. JCSS Special Issue: Cloud Computing 2011.
Lijie Chen and Jian Li. On the Optimal Sample Complexity for Best Arm Identification, 2015.
James Cheshire, Pierre Ménard, and Alexandra Carpentier. The influence of shape constraints on the
thresholding bandit problem. In Conference on Learning Theory , pp. 1228–1275. PMLR, 2020.
James Cheshire, Pierre, Ménard, and Alexandra Carpentier. Problem Dependent View on Structured
Thresholding Bandit Problems. In International Conference on Machine Learning , pp. 1846–1854. PMLR,
2021.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual Bandits with Linear Payoff Functions. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics , volume 15
ofProceedings of Machine Learning Research , pp. 208–214, Fort Lauderdale, FL, USA, 11–13 Apr 2011.
PMLR.
Richard Combes and Alexandre Proutiere. Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms.
InProceedings of the 31st International Conference on International Conference on Machine Learning -
Volume 32 , ICML’14, pp. I–521–I–529, 2014.
Richard Combes, Mazraeh Shahi Talebi, Sadegh Mohammad, Alexandre Proutiere, and Marc Lelarge.
Combinatorial Bandits Revisited. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.),
Advances in Neural Information Processing Systems , volume 28. Curran Associates, Inc., 2015.
Richard Combes, Stefan Magureanu, and Alexandre Proutiere. Minimal exploration in structured stochastic
bandits. Advances in Neural Information Processing Systems , 30, 2017.
Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic Linear Optimization under Bandit Feedback.
InConference on Learning Theory , 2008.
12Published in Transactions on Machine Learning Research (04/2024)
Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action Elimination and Stopping Conditions for the
Multi-Armed Bandit and Reinforcement Learning Problems. J. Mach. Learn. Res. , 7, 2006.
Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best Arm Identification: A Unified
Approach to Fixed Budget and Fixed Confidence. In Advances in Neural Information Processing Systems ,
volume 25, 2012.
A. Garivier, P. Ménard, L. Rossi, and P. Menard. Thresholding Bandit for Dose-ranging: The Impact of
Monotonicity. In Arxiv. PMLR, 2017. URL https://doi.org/10.48550/arXiv.1711.04454 .
Aurélien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In Conference
on Learning Theory , pp. 998–1027. PMLR, 2016.
Manjesh Kumar Hanawal, Venkatesh Saligrama, Michal Valko, and Rémi Munos. Cheap Bandits. In
Proceedings of the 32nd International Conference on International Conference on Machine Learning , 2015.
Morteza Hashemi, Ashutosh Sabharwal, C. Emre Koksal, and Ness B. Shroff. Efficient Beam Alignment in
Millimeter Wave Systems Using Contextual Bandits. In IEEE Conference on Computer Communications
(INFOCOM) , pp. 2393–2401, 2018.
Kevin Jamieson and Robert Nowak. Best-arm identification algorithms for multi-armed bandits in the fixed
confidence setting. In 48th Annual Conference on Information Sciences and Systems (CISS) , pp. 1–6.
IEEE, 2014.
Yassir Jedra and Alexandre Proutiere. Optimal best-arm identification in linear bandits. Advances in Neural
Information Processing Systems , 33:10007–10017, 2020.
Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC Subset Selection in Stochastic
Multi-Armed Bandits. In Proceedings of the 29th International Conference on International Conference on
Machine Learning (ICML) , pp. 227–234, 2012.
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In
International Conference on Machine Learning , pp. 1238–1246. PMLR, 2013.
Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On the Complexity of Best Arm Identification in
Multi-Armed Bandit Models. Journal of Machine Learning Research , 17:1–42, 2016.
Tomáš Kocák and Aurélien Garivier. Best arm identification in spectral bandits. In Proceedings of the
International Joint Conference on Artificial Intelligence, IJCAI-20 , 2020.
Tor Lattimore and Csaba Szepesvári. Bandit algorithms . Cambridge University Press, 2020.
Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bound and
optimal algorithms. In Conference on Learning Theory , pp. 975–999. PMLR, 2014.
Shie Mannor and John N. Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed Bandit
Problem. J. Mach. Learn. Res. , 5:623–648, dec 2004. ISSN 1532-4435.
Hassan Saber, Pierre Ménard, and Odalric-Ambrym Maillard. Forced-exploration free strategies for unimodal
bandits. arXiv preprint arXiv:2006.16569 , 2020a.
Hassan Saber, Pierre Ménard, and Odalric-Ambrym Maillard. Forced-exploration free Strategies for Unimodal
Bandits. working paper or preprint, 2020b. URL https://hal.archives-ouvertes.fr/hal-02883907 .
Shahin Shahrampour, Mohammad Noshad, and Vahid Tarokh. On sequential elimination algorithms for
best-arm identification in multi-armed bandits. IEEE Transactions on Signal Processing , 65(16):4281–4292,
2017.
Marta Soare, Alessandro Lazaric, and Remi Munos. Best-Arm Identification in Linear Bandits. In Advances
in Neural Information Processing Systems , volume 27, 2014.
13Published in Transactions on Machine Learning Research (04/2024)
Michal Valko, Rémi Munos, Branislav Kveton, and Tomas Kocak. Spectral Bandits for Smooth Graph ˇ
Functions. In Proceedings of the 31st International Conference on International Conference on Machine
Learning , 2014.
Po-An Wang, Ruo-Chun Tzeng, and Alexandre Proutiere. Fast pure exploration via frank-wolfe. Advances in
Neural Information Processing Systems , 34:5810–5821, 2021.
Jia Yuan Yu and Shie Mannor. Unimodal Bandits. In Proceedings of the 28th International Conference on
International Conference on Machine Learning , pp. 41–48, 2011.
14