Published in Transactions on Machine Learning Research (11/2023)
Data pruning and neural scaling laws: fundamental limita-
tions of score-based algorithms
Fadhel Ayed∗fadhel.ayed@gmail.com
Huawei Technologies France
Soufiane Hayou∗hayou@nus.edu.sg
National University of Singapore
Reviewed on OpenReview: https: // openreview. net/ forum? id= iRTL4pDavo
Abstract
Data pruning algorithms are commonly used to reduce the memory and computational cost
of the optimization process. Recent empirical results (Guo, B. Zhao, and Bai, 2022) reveal
that random data pruning remains a strong baseline and outperforms most existing data
pruning methods in the high compression regime, i.e. where a fraction of 30%or less of
the data is kept. This regime has recently attracted a lot of interest as a result of the role
of data pruning in improving the so-called neural scaling laws; see (Sorscher et al., 2022),
where the authors showed the need for high-quality data pruning algorithms in order to beat
the sample power law. In this work, we focus on score-based data pruning algorithms and
show theoretically and empirically why such algorithms fail in the high compression regime.
We demonstrate “No Free Lunch" theorems for data pruning and discuss potential solutions
to these limitations.
1 Introduction
Coreset selection, also known as data pruning, refers to a collection of algorithms that aim to efficiently
select a subset from a given dataset. The goal of data pruning is to identify a small yet representative
sample of the data that accurately reflects the characteristics of the entire dataset. Coreset selection is often
used in cases where the original dataset is too large or complex to be processed efficiently by the available
computational resources. By selecting a coreset, practitioners can reduce the computational cost of their
analyses and gain valuable insights more efficiently. Data pruning has many interesting applications, notably
neural architecture search (NAS), where models trained with a small fraction of the data serve as a proxy
to quickly estimate the performance of a given choice of hyper-parameters (Coleman et al., 2020). Another
application is continual (or incremental) learning in the context of online learning; To avoid the forgetting
problem, one keeps track of the most representative examples of past observations (Aljundi et al., 2019).
Coreset selection is typically performed once during training, and the selected coreset remains fixed until
the end of training. This topic has been extensively studied in classical machine learning and statistics
(Welling, 2009; Chen, Welling, and Smola, 2012; Feldman, Faulkner, and Krause, 2011; Huggins, Campbell,
and Broderick, 2016; Campbell and Broderick, 2019). Recently, many approaches have been proposed to
adapt to the challenges of the deep learning context. Examples include removing the redundant examples
from the feature space perspective (see Sener and Savarese, 2018), finding the hard examples, defined as
the ones for which the model is the least confident (Coleman et al., 2020), or the ones that contribute the
most to the error (Toneva et al., 2019). We refer the reader to Section 6 for a more comprehensive literature
review. Most of these methods use a score function that ranks examples based on their “importance". Given
a desired compression level r∈(0,1)(the fraction of data kept after pruning), the coreset is created by
retaining only the most important examples based on the scores to meet the required compression level. We
∗Equal contribution (Alphabetical order).
1Published in Transactions on Machine Learning Research (11/2023)
Figure 1: Logistic regression: Data distribution alteration due to pruning for different compression ratios.
Here we use GraNdas the pruning algorithm. Blue points correspond to Yi= 0, red points correspond to
Yi= 1. More details in Section 5.
refer to this type of algorithms as score-based pruning algorithms ( SBPA). A formal definition is provided in
Section 2.
1.1 Connection to Neural Scaling Laws
Recently, a stream of empirical works have observed the emergence of power law scaling in different machine
learning applications (see e.g. Hestness et al., 2017; Kaplan et al., 2020; Rosenfeld et al., 2020; Hernandez
et al., 2021; Zhai et al., 2022; Hoffmann et al., 2022). More precisely, these empirical results show that the
performance of the model (e.g. the test error) scales as a power law with either the model size, training
datasetsize, orcompute(FLOPs). InSorscheretal., 2022, theauthorsshowedthatdatapruningcanimprove
the power law scaling of the dataset size. The high compression regime (small r) is of major interest in this
case since it exhibits super-polynomial scaling laws on different tasks. However, as the authors concluded,
improving the power law scaling requires high-quality data pruning algorithms, and it is still unclear what
properties such algorithms should satisfy. Besides scaling laws, small values of rare of particular interest
for tasks such as hyper-parameters selection, where the practitioner wants to select a hyper-parameter from
a grid rapidly. In this case, the smaller the value of r, the better.
In this work, we argue that score-based data pruning is generally not suited for the high compression regime
(starting from r≤30%) and, therefore, cannot be used to beat the power law scaling. In this regime, it
has been observed (see e.g. Guo, B. Zhao, and Bai, 2022) that most SBPAalgorithms underperform random
pruning (randomly selected subset). To understand why this occurs, we analyze the asymptotic behavior of
SBPAalgorithms and identify some of their properties, particularly in the high compression level regime. To
the best of our knowledge, no rigorous explanation for this phenomenon has been reported in the literature.
Our work provides the first theoretical explanation for this behavior and offers insights on how to address it
in practice.
Intuitively, SBPAalgorithms induce a distribution shift that affects the training objective. This can, for
example, lead to the emergence of new local minima where performance deteriorates significantly. To give
a sense of this intuition, we use a toy example in Fig. 1 to illustrate the change in data distribution as the
compression level rdecreases, where we have used GraNd(Paul, Ganguli, and Dziugaite, 2021) to prune the
dataset.
We also report the change in the loss landscape in Fig. 2 as the compression level decreases and the resulting
scaling laws. The results show that such a pruning algorithm cannot be used to improve the scaling laws
since the performance drops significantly in the high compression regime and does not tend to significantly
decrease with sample size.
Motivated by these empirical observations, we aim to understand the behaviour of SBPAalgorithms in the
high compression regime. In Section 3, we analyze the impact of pruning of SBPAalgorithms on the loss
function in detail and link this distribution shift to a notion of consistency. We prove several results showing
the limitations of SBPAalgorithms in the high compression regime, which explains some of the empirical
results reported in Fig. 2. We also propose calibration protocols, that build on random exploration to
address this deterioration in the high compression regime (Fig. 2).
2Published in Transactions on Machine Learning Research (11/2023)
1.2 Contributions
Figure 2: Logistic regression: ris the compression
level,nthe total number of available data and wthe
learnable parameter. ( Left) The loss landscape trans-
formation due to pruning. ( Right) The evolution of
the performance gap as the data budget m:=r×nin-
creases (average over ten runs). Top figures illustrate
theperformanceof GraNd,bottomfiguresillustratethe
performance of GraNdcalibrated with our exact proto-
col: we use 90%of the data budget for the signal, i.e.
points selected by GraNd, and 10%of the data budget
for calibration through random exploration. See Sec-
tions 4 and 5 for more details.Our contributions are as follows:
•We propose a novel formalism to character-
ize the asymptotic properties of data prun-
ingalgorithmsintheabundantdataregime.
•We introduce Score-Based Pruning Algo-
rithms ( SBPA), a class of algorithms that
encompasses a wide range of popular ap-
proaches. By employing our formalism, we
analyze SBPAalgorithms and identify a phe-
nomenon of distribution shift, which prov-
ably impacts generalization error.
•WedemonstrateNo-Free-Lunchresultsthat
characterize when and why score-based
pruningalgorithmsperformworsethanran-
dom pruning. Specifically, we prove that
SBPAare unsuitable for high compression
scenarios due to a significant drop in perfor-
mance. Consequently, SBPAcannot improve
scaling laws without appropriate adapta-
tion.
•Leveraging our theoretical insights, solu-
tions can be designed to address these lim-
itations. As an illustration, we introduce
a simple calibration protocol to correct the
distribution shift by adding noise to the
pruning process. Theoretical and empiri-
cal results support the effectiveness of this
method on toy datasets and show promising
results on image classification tasks.1
2 Learning with Data Pruning
2.1 Setup
Consider a supervised learning task where the inputs and outputs are respectively in X⊂RdxandY⊂Rdy,
both assumed to be compact2. We denote byD=X×Ythe data space. We assume that there exists µ, an
atomless probability distribution on Dfrom which input/output pairs Z= (X,Y )are drawn independently
at random. We call such µadata generating process . We will assume that Xis continuous while Ycan be
either continuous (regression) or discrete (classification). We are given a family of models
Mθ={yout(·;w) :X→Y|w∈Wθ}, (1)
parameterised by the parameter space Wθ, a compact subspace of Rdθ, whereθ∈Θis a fixed hyper-
parameter . For instance,Mθcould be a family of neural networks of a given architecture, with weights w,
and where the architecture is given by θ. We will assume that youtis continuous onX×Wθ3. For a given
1It is important to note that the calibration protocol serves as an example to stimulate further research. We do not claim
that this method systematically allows to outperform random pruning nor to beat the neural scaling laws.
2We further require that the set Xhas no isolated points. This technical assumption is required to avoid dealing with
unnecessary complications in the proofs.
3This is generally satisfied for a large class of models, including neural networks.
3Published in Transactions on Machine Learning Research (11/2023)
continuous loss function ℓ:Y×Y→ R, the aim of the learning procedure is to find a model that minimizes
the generalization error, defined by
L(w)def=Eµℓ/parenleftbig
yout(X;w),Y/parenrightbig
. (2)
We are given a dataset Dncomposed of n≥1input/output pairs (xi,yi),iidsampled from the data
generating process µ. To obtain an approximate minimizer of the generalization error (Eq. (2)), we perform
an empirical risk minimization, solving the problem
min
w∈WθLn(w)def=1
nn/summationdisplay
i=1ℓ/parenleftbig
yout(xi;w),yi/parenrightbig
. (3)
The minimization problem (3) is typically solved using a numerical approach, often gradient-based, such
as Stochastic Gradient Descent (Robbins and Monro, 1951), Adam (Kingma and Ba, 2017), etc. We refer
to this procedure as the training algorithm . We assume that the training algorithm is exact, i.e. it will
indeed return a minimizing parameter w∗
n∈argminw∈WθLn(w).The numerical complexity of the training
algorithms grows with the sample size n, typically linearly or worse. When nis large, it is appealing to
extract a representative subsetofDnand perform the training with this subset, which would reduce the
computational cost of training. This process is referred to as data pruning. However, in order to preserve
the performance, the subset should retain essential information from the original (full) dataset. This is the
primary objective of data pruning algorithms. We begin by formally defining such algorithms.
Notation. IfZis a finite set, we denote by |Z|its cardinal number, i.e. the number of elements in Z. We
denote⌊x⌋the largest integer smaller than or equal to xforx∈R. For some Euclidean space E, we denote by
dthe Euclidean distance and for some set B⊂Eande∈E, we define the distance d(e,B) = infb∈Bd(e,b).
Finally, for two integers n1< n 2,[n1:n2]refers to the set{n1,n1+ 1,...,n 2}. We denote the set of all
finite subsets ofDbyC, i.e.C=∪n≥1{{z1,z2,...,zn},z1̸=z2̸=...̸=zn∈D}. We callCthe finite power
set ofD.
Definition 1 (Data Pruning Algorithm) We say that a function A:C×(0,1]→Cis a data pruning
algorithm if for all Z∈C,r∈(0,1], such that r|Z|is an integer4, we have the following
•A(Z,r)⊂Z
•|A(Z,r)|=r|Z|
where|.|refers to the cardinal number. The number ris called the compression level and refers to the fraction
of the data kept after pruning.
Among the simplest pruning algorithms, we will pay special attention to Randompruning, which selects
uniformly at random a fraction of the elements of Zto meet some desired compression level r.
2.2 ValidandConsistent Pruning Algorithms
Given a pruning algorithm Aand a compression level r, a subset of the training set is selected and the
model is trained by minimizing the empirical loss on the subset. More precisely, the training algorithm finds
a parameter wA,r
n∈argminw∈WθLA,r
n(w)where
LA,r
n(w)def=1
|A(Dn,r)|/summationdisplay
(x,y)∈A(Dn,r)ℓ/parenleftbig
yout(x;w),y/parenrightbig
.
4We make this assumption to simplify the notations. One can take the integer part of rninstead.
4Published in Transactions on Machine Learning Research (11/2023)
This usually requires only a fraction rof the original energy/time5cost or better, given the linear complexity
of the training algorithm with respect to the data size. In this work, we evaluate the quality of a pruning
algorithm by considering the performance gap it induces, i.e. the excess risk of the selected model
gapA,r
n=L(wA,r
n)−min
w∈WθL(w). (4)
In particular, we are interested in the abundant data regime: we aim to understand the asymptotic behavior
oftheperformancegapasthesamplesize ngrowstoinfinity. Wedefinethenotionof validpruningalgorithms
as follows.
Definition 2 (Valid pruning algorithm) For a parameter space Wθ, a pruning algorithm Ais valid at
a compression level r∈(0,1]iflimn→∞gapA,r
n= 0almost surely. The algorithm is said to be valid if it is
valid at any compression level r∈(0,1].
We argue that a valid data pruning algorithm for a given generating process µand a family of models Mθ
should see its performance gap converge to zero almost surely. Otherwise, it would mean that with positive
probability, the pruning algorithm induces a deterioration of the out-of-sample performance that does not
vanish even when an arbitrarily large amount of data is available. This deterioration would not exist without
pruning or if random pruning was used instead (Corollary 1). This means that with positive probability, a
non-valid pruning algorithm will underperform random pruning in the abundant data regime. In the next
result, we show that a sufficient and necessary condition for a pruning algorithm to be valid at compression
levelris thatwA,r
nshould approach the set of minimizers of the original generalization loss function as n
increases.
Proposition 1 (Characterization of valid pruning algorithms) A pruning algorithm Ais valid at a
compression level r∈(0,1]if and only if
d/parenleftig
wA,r
n,W∗
θ(µ)/parenrightig
→0a.s.
whereW∗
θ(µ) =argminw∈WθL(w)⊂Wθandd/parenleftig
wA,r
n,W∗
θ(µ)/parenrightig
denotes the euclidean distance from the point
wA,r
nto the setW∗
θ(µ).
With this characterization in mind, the following proposition provides a key tool to analyze the performance
of pruning algorithms. Under some conditions, it allows us to describe the asymptotic performance of any
pruning algorithm via some properties of a probability measure.
Proposition 2 LetAbe a pruning algorithm and r∈(0,1]a compression level. Assume that there exists a
probability measure νronDsuch that
∀w∈Wθ,LA,r
n(w)→Eνrℓ(yout(X;w),Y)a.s. (5)
Then, denotingW∗
θ(νr) =argminw∈WθEνrℓ(yout(X;w),Y)⊂Wθ, we have that
d/parenleftig
wA,r
n,W∗
θ(νr)/parenrightig
→0a.s.
Condition Eq. (5) assumes the existence of a limiting probability measure νrthat represents the distribution
of the pruned dataset in the limit of infinite sample size. In Section 3, for a large family of pruning algorithms
called score-based pruning algorithms (a formal definition will be introduced later), we will demonstrate the
existence of such limiting probability measure and derive its exact expression.
Let us now derive two important corollaries; the first gives a sufficient condition for an algorithm to be valid,
and the second a necessary condition. From Proposition 1 and Proposition 2, we can deduce that a sufficient
condition for an algorithm to be valid is that νr=µsatisfies equation (5). We say that such a pruning
algorithm is consistent .
5Here the original cost refers to the training cost of the model with the full dataset.
5Published in Transactions on Machine Learning Research (11/2023)
Definition 3 (Consistent Pruning Algorithms) We say that a pruning algorithm Ais consistent at
compression level r∈(0,1]if and only if it satisfies
∀w∈Wθ,LA,r
n(w)→Eµ[ℓ(yout(x,w),y)] =L(w)a.s. (6)
We say thatAis consistent if it is consistent at any compression level r∈(0,1].
Corollary 1 A consistent pruning algorithm Aat a compression level r∈(0,1]is also valid at compression
levelr.
A simple application of the law of large numbers implies that Randompruning is consistent and hence valid
for any generating process and learning task satisfying our general assumptions.
We bring to the reader’s attention that consistency is itself a property of practical interest. Indeed, it not
only ensures that the generalization gap of the learned model vanishes, but it also allows the practitioner to
accurately estimate the generalization error of their trained model from the selected subset. For instance,
consider the case where the practitioner is interested in Khyper-parameter values θ1,...,θK; these can
be different neural network architectures (depth, width, etc.). Using a pruning algorithm A, they obtain
a trained model wA,r
n(θk)for each hyper-parameter θk, with corresponding estimated generalization error
LA,r
n/parenleftig
wA,r
n(θk)/parenrightig
.Hence, the consistency property would allow the practitioner to select the best hyper-
parameter value based on the empirical loss computed with the set of retained points (or a random subset of
which used for validation). From Proposition 1 and Proposition 2, we can also deduce a necessary condition
for an algorithm satisfying (5) to be valid:
Corollary 2 LetAbe any pruning algorithm and r∈(0,1], and assume that (5)holds for a given probability
measureνronD. IfAis valid, thenW∗
θ(νr)∩W∗
θ(µ)̸=∅; or, equivalently,
min
w∈W∗
θ(νr)L(w) = min
w∈WL(w).
Corollary2willbeakeyingredientintheproofsonthenon-validityofagivenpruningalgorithm. Specifically,
for all the non-validity results stated in this paper, we prove that W∗
θ(νr)∩W∗
θ(µ) =∅. In other words, none
of the minimizers of the original problem is a minimizer of the pruned one, and vice-versa.
3 Score-Based Pruning Algorithms and their Limitations
3.1 Score-based Pruning Algorithms
A standard approach to define a pruning algorithm is to assign to each sample zi= (xi,yi)a scoregi=g(zi)
according to some score function g, wheregis a mapping from DtoR.gis also called the pruning criterion.
The score function gcaptures the practitioner’s prior knowledge of the relative importance of each sample.
This function can be defined using a teacher model that has already been trained, for example. In this
work, we use the convention that the lower the score, the more relevant the example. One could of course
adopt the opposite convention by considering −ginstead ofgin the following. We now formally define this
category of pruning algorithms, which we call score-based pruning algorithms.
Definition 4 (Score-based Pruning Algorithm ( SBPA))LetAbe a data pruning algorithm. We say
thatAis a score-based pruning algorithm ( SBPA) if there exists a function g:D→ Rsuch that for all
Z∈C, r∈(0,1), we have thatA(Z,r) ={z∈Z,s.t.g(z)≤gr|Z|},wheregr|Z|is(r|Z|)thorder statistic
of the sequence (g(z))z∈Z(first order statistic being the smallest value). The function gis called the score
function.
A significant number of existing data pruning algorithms are score-based (for example
Coleman2020Uncertainty ; Paul, Ganguli, and Dziugaite, 2021; Ducoffe and Precioso, 2018; Sorscher
et al., 2022), among which the recent approaches for modern machine learning. One of the key benefits of
6Published in Transactions on Machine Learning Research (11/2023)
these methods is that the scores are computed independently; these methods are hence parallelizable, and
their complexity scales linearly with the data size (up to log terms). These methods are tailored for the
abundant data regime, which explains their recent gain in popularity.
Naturally, the result of such a procedure highly depends on the choice of the score function g, and different
choices ofgmight yield completely different subsets. The choice of the score function in Definition 4 is not
restricted, and there are many scenarios in which the selection of the score function gmay be problematic.
For example, if ghas discontinuity points, this can lead to instability in the pruning procedure, as close data
points may have very different scores. Another problematic scenario is when gassigns the same score to a
large number of data points. To avoid such unnecessary complications, we define adaptedpruning criteria
as follows:
Definition 5 (Adapted score function) Letgbe a score function corresponding to some pruning algo-
rithmA. We say that gis an adapted score function if gis continuous and for any c∈g(D) :={g(z),z∈D},
we haveλ(g−1({c})) = 0, whereλis the Lebesgue measure on D.
In the rest of the section, we will examine the properties of SBPAalgorithms with an adapted score function.
3.2 Asymptotic Behavior of SBPA
Asymptotically, SBPAalgorithms have a simple behavior that mimics rejection algorithms. We describe this
in the following result.
Proposition 3 (Asymptotic behavior of SBPA)LetAbe aSBPAalgorithm and let gbe its corresponding
adapted score function. Consider a compression level r∈(0,1). Denote by qrtherthquantile of the random
variableg(Z)whereZ∼µ. DenoteAr={z∈D|g(z)≤qr}. Almost surely, the empirical measure of the
retained data samples converges weakly to νr=1
rµ|Ar, whereµ|Aris the restriction of µto the setAr. In
particular, we have that
∀w∈Wθ,LA,r
n(w)→Eνrℓ(yout(X;w),Y)a.s.
The result of Proposition 3 implies that in the abundant data regime, a SBPAalgorithmAacts similarly to a
deterministic rejection algorithm, where the samples are retained if they fall in Ar, and removed otherwise.
The first consequence is that a SBPAalgorithmAis consistent at compression level rif and only if
∀w∈Wθ,E1
rµ|Arℓ(yout(X;w),Y) =Eµℓ(yout(X;w),Y), (7)
The second consequence is that SBPAalgorithms ignore entire regions of the data space, even when we have
access to unlimited data, i.e. n→∞. Moreover, the ignored region can be made arbitrarily large for small
enough compression levels. Therefore, we expect that the generalization performance will be affected and
that the drop in performance will be amplified with smaller compression levels, regardless of the sample size
n. This hypothesis is empirically validated (see Guo, B. Zhao, and Bai, 2022 and Section 5).
In the rest of the section, we investigate the fundamental limitations of SBPAin terms of consistency and
validity; we will show that under mild assumptions, for any SBPAalgorithm with an adapted score function,
there exist compression levels rfor which the algorithm is neither consistent nor valid. Due to the prevalence
of classification problems in modern machine learning, we focus on the binary classification setting and give
specialized results in Section 3.3. In Section 3.4, we provide a different type of non-validity results for more
general problems.
3.3 Binary Classification Problems
In this section, we focus our attention on binary classification problems. The predictions and labels are in
Y= [0,1]. DenotePBthe set of probability distributions on X×{ 0,1}, such that the marginal distribution
on the input space Xis continuous (absolutely continuous with respect to the Lebesgue measure on X) and
for which
pπ:x∝⇕⊣√∫⊔≀→Pπ(Y= 1|X=x)
is upper semi-continuous for any π∈PB. We further assume that:
7Published in Transactions on Machine Learning Research (11/2023)
(i) the loss is non-negative and that ℓ(y,y′) = 0if and only if y=y′.
(ii) Forq∈[0,1],y∝⇕⊣√∫⊔≀→qℓ(y,1) + (1−q)ℓ(y,0)has a unique minimizer, denoted y∗
q∈[0,1], that is
increasing with q.
These two assumptions are generally satisfied in practice for the usual loss functions, such as the ℓ1,ℓ2,
Exponential or Cross-Entropy losses, with the notable exception of the Hinge loss for which (ii) does not
hold.
Under mild conditions that are generally satisfied in practice, we show that no SBPAalgorithm is consistent.
We first define a notion of universal approximation.
Definition 6 (Universal approximation) A family of continuous functions Ψhas the universal approx-
imation property if for any continuous function f:X→Yandϵ>0, there exists ψ∈Ψsuch that
maxx∈X|f(x)−ψ(x)|≤ϵ
The next proposition shows that if the set of all models considered ∪θ∈ΘMθhas the universal approximation
property, then no SBPAalgorithm is consistent.
Theorem 1 Consider any generating process for binary classification µ∈PB. LetAbe any SBPAalgorithm
with an adapted score function. If ∪θMθhas the universal approximation property and the loss satisfies
assumption (i), then there exist hyper-parameters θ∈Θfor which the algorithm is not consistent.
Even though consistency is an important property, a pruning algorithm can still be valid without being
consistent. Inthisclassificationsetting, wecanfurthershowthat SBPAalgorithmsalsohavestronglimitations
in terms of validity.
Theorem 2 Consider any generating process for binary classification µ∈PB. LetAbe a SBPAwith an
adapted score function gthat depends on the labels6. If∪θMθhas the universal approximation property and
the loss satisfies assumptions (i) and (ii), then there exist hyper-parameters θ1,θ2∈Θandr0∈(0,1)such
that the algorithm is not valid for r≤r0for any hyper-parameter θsuch thatWθ1∪Wθ2⊂Wθ.
This theorem sheds light on a strong limitation of SBPAalgorithms for which the score function depends on
the labels: it states that any solution of the pruned program will induce a generalization error strictly larger
than with random pruning in the abundant data regime. The proof builds on Corollary 2; we show that
for such hyper-parameters θ, the minimizers of the pruned problem and the ones of the original (full data)
problem do not intersect, i.e.
W∗
θ(νr)∩W∗
θ(µ) =∅.
SBPAalgorithms usually depend on the labels ( Coleman2020Uncertainty ; Paul, Ganguli, and Dziugaite,
2021; DucoffeandPrecioso, 2018)andTheorem2applies. InSorscheretal., 2022, theauthorsalsoproposeto
use a SBPAthat does not depend on the labels. For such algorithms, the acceptance region Aris characterized
by a corresponding input acceptance region Xr.SBPAindependent of the labels have a key benefit; the
conditional distribution of the output is not altered given that the input is in Xr. Contrary to the algorithms
depending on the labels, the performance will not necessarily be degraded for any generating distribution
given that the family of models is rich enough. It remains that the pruned data give no information outside
ofXr, andyoutcan take any value in X\Xrwithout impacting the pruned loss. Hence, these algorithms can
create new local/global minima with poor generalization performance. Besides, the non-consistency results
of this section and the No-Free-Lunch result presented in Section 3.4 do apply for SBPAindependent of the
labels. For these reasons, we believe that calibration methods (see Section 4) should also be employed for
SBPAindependent of the labels, especially with small compression ratios.
6The score function gdepends on the labels if there exists an input xin the support of the distribution of the input Xand
for whichg(x,0)̸=g(x,1)andP(Y= 1|X=x)∈(0,1)(both labels can happen at input x)
8Published in Transactions on Machine Learning Research (11/2023)
Applications: Neural Networks
To exemplify the utility of Theorem 1 and Theorem 2, we leverage the existing literature on the universal
approximation properties of neural networks to derive the important corollaries stated below
Definition 7 For an activation function σ, a real number R > 0, and integers H,K≥1, we denote by
FFNNσ
H,K(R)the set of fully-connected feed-forward neural networks with Hhidden layers, each with K
neurons with all weights and biases in [−R,R].
Corollary 3 (Wide neural networks) Letσbe any continuous non-polynomial function that is continu-
ously differentiable at (at least) one point, with a nonzero derivative at that point. Consider any generating
processµ∈PB. For any SBPAwith adapted score function and H≥1, there exists a radius R0and a width
K0such that the algorithm is not consistent on FFNNσ
H,K(R)for anyK≥K0andR≥R0. Besides, if
the score function depends on the labels, then it is also not valid on FFNNσ
H,K(R)for anyK≥K′
0and
R≥R′
0.
Corollary 4 (Deep neural networks) Consider a width K≥dx+ 2. Letσbe any continuous non-
polynomial function that is continuously differentiable at (at least) one point, with a nonzero derivative
at that point. Consider any generating process µ∈PB. For any SBPAwith an adapted score function,
there exists a radius R0and a number of hidden layers H0such that the algorithm is not consistent on
FFNNσ
H,K(R)for anyH≥H0andR≥R0. Besides, if the score function depends on the labels, then it is
also not valid on FFNNσ
H,K(R)for anyH≥H′
0andR≥R′
0
A similar result for convolutional architectures is provided in Appendix C. To summarize, these corollaries
showthatforlargeenoughneuralnetworkarchitectures, any SBPAisnon-consistent. Besides, forlargeenough
neural network architectures, any SBPAthat depends on the label is non-valid, and hence a performance gap
should be expected even in the abundant data regime.
3.4 General Problems
In the previous section, we leveraged the universal approximation property and proved non-validity and
non-consistency results that hold for any data-generating process. In this section, we show a different No-
free-Lunch result in the general setting presented in Section 2. This result does not require the universal
approximation property. More precisely, we show that under mild assumptions, given any SBPAalgorithm,
we can always find a data distribution µsuch that the algorithm is not valid (Definition 2). Since random
pruning is valid for any generating process, this means that there exist data distributions for which the SBPA
algorithm provably underperforms random pruning in the abundant data regime.
ForK∈N∗, letPK
Cdenote the set of generating processes for K-classes classification problems, for which
the inputXis a continuous random variable7, and the output Ycan take one of Kvalues inY(the same
set of values for all π∈PK
C). Similarly, denote PR, the set of generating processes for regression problems
for which both the input and output distributions are continuous. Let Pbe any set of generating processes
introduced previously for regression or classification (either P=PK
Cfor someK, orP=PR). In the
next theorem, we show that under minimal conditions, there exists a data generating process for which the
algorithms is not valid.
Theorem 3 LetAbe a SBPAwith an adapted score function. For any hyper-parameter θ∈Θ, if there exist
(x1,y1),(x2,y2)∈Dsuch that
argminw∈Wθℓ(yout(x1;w),y1)∩argminw∈Wθℓ(yout(x2;w),y2) =∅, (H1)
then there exists r0∈(0,1)and a generating process µ∈Pfor which the algorithm is not valid for r≤r0.
The rigorous proof of Theorem 3 requires careful manipulations of different quantities, but the intuition is
rather simple. Fig. 3 illustrates the main idea of the proof. We construct a distribution µwith the majority
7In the sense that the marginal of the input is dominated by the Lebesgue measure
9Published in Transactions on Machine Learning Research (11/2023)
𝑧2 𝑧1
𝑤𝑆𝑒𝑙𝑒𝑐𝑡𝑧1𝑎𝑛𝑑𝑧2𝑠𝑢𝑐ℎ𝑡ℎ𝑎𝑡𝑔𝑧1<𝑔(𝑧2)
𝑃𝑢𝑡𝑡ℎ𝑒𝑏𝑢𝑙𝑘𝑜𝑓𝜇𝑎𝑟𝑜𝑢𝑛𝑑𝑧2𝑃𝑢𝑡𝑎𝑠𝑚𝑎𝑙𝑙𝑚𝑎𝑠𝑠𝑜𝑓𝜇𝑎𝑟𝑜𝑢𝑛𝑑𝑧1
Figure 3: Graphical sketch of the proof of Theorem 3. The surface represents the loss function f(z,w) =
ℓ(yout(x),y)in 2D, where z= (x,y).
Data SBPA Subset
SBPA-CPx Subset
SBPACalibration  
Protocol (CPx)Standard  
method
Our proposed
methodData
Figure 4: An illustration of how the calibration protocols modify SBPAalgorithms.
of the probability mass concentrated around a point where the value of gis not minimal. Consequently,
for sufficiently small r, the distribution of the retained samples will significantly differ from the original
distribution. This shift in data distributions causes the algorithm to be non-valid. We see in the next
section how we can solve this issue via randomization. Finally, notice that Eq. (H1) is generally satisfied in
practice since usually for two different examples (x1,y1)and(x2,y2)in the datasets, the global minimizers
ofℓ(yout(x1;w),y1)andℓ(yout(x2;w),y2)are different.
4 Solving Non-Consistency via Randomization
We have seen in Section 3 that SBPAalgorithms inherently transform the data distribution by asymptotically
rejecting all samples in D\Ar. These algorithms are prone to inconsistency; the transformation of the
data distribution translates to a distortion of the loss landscape, potentially leading to a deterioration of
the generalization error. This effect is exacerbated for smaller compression ratios ras the acceptance region
becomes arbitrarily small and concentrated.
10Published in Transactions on Machine Learning Research (11/2023)
With this in mind, one can design practical solutions to mitigate the problem. For illustration, we propose
to resort to a Calibration Protocol to retain information from the previously discarded region D\Ar.The
calibration protocols can be thought of as wrapper modules that can be applied on top of any SBPAalgorithm
to solve the consistency issue through randomization (see Fig. 4 for a graphical illustration). Specifically, we
split the data budget rninto two parts: the first part, allocated for the signal, leverages the knowledge from
theSBPAand its score function g. The second part, allocated for exploration , accounts for the discarded
region and consists of a subset of the rejected points, selected uniformly at random. In other words, we
writer=rsignal +rexploration.With standard SBPAprocedures, rexploration = 0.We defineα=rsignal
rthe
proportion of signal in the overall budget. Accordingly, the set of retained points can be expressed as
¯A(Dn,r,α) =¯As(Dn,r,α)∪¯Ae(Dn,r,α),
where ¯Adenotes the calibrated version of A, and the indices ‘s’ and ‘e’ refer to signal and exploration
respectively. In this work, we consider the simplest approach. The “signal subset" is composed of the αrn
points with the highest importance according to g, i.e. ¯As(Dn,r,α) =A(Dn,rα). The “exploration subset",
¯Ae(Dn,r,α)is composed on average of (1−α)rnpoints selected uniformly at random from the remaining
samplesDn\A(Dn,rα), each sample being retained with probability pe=(1−α)r
1−αr, independently. The
calibrated loss is then defined as a weighted sum of the contributions of the signal and exploration budgets,
L¯A,r,α
n(w) =1
n
γs/summationdisplay
z∈¯As(Dn,r,α)f(z;w) +γe/summationdisplay
z∈¯Ae(Dn,r,α)f(z;w)
 (8)
wheref(z;w) =ℓ(yout(x),y)forz= (x,y)∈Dandw∈Wθ.The weights γsandγeare chosen so that the
calibrated procedure is consistent; they are inversely proportional to the probability of acceptance within
each region:
γs= 1
γe=1−αr
(1−α)r
Proposition 4 hereafter states that any SBPAcalibrated with this procedure is made consistent as long as a
non-zero budget is allocated to exploration. For this reason, we refer to this method as the Exact Calibration
protocol (EC). The proof builds on an adapted version of the law of large numbers for sequences of dependent
variables which we prove in the Appendix (Theorem 7).
Proposition 4 (Consistency of Exact Calibration+ SBPA)LetAbe a SBPAalgorithm. Using the Exact
Calibration protocol with signal proportion α, the calibrated algorithm ¯Ais consistent if 1−α>0, i.e. the
exploration budget is not null. Besides, under the same assumption 1−α > 0, the calibrated loss is an
unbiased estimator of the generalization loss at any finite sample size n>0,
∀w∈Wθ,∀r∈(0,1),EL¯A,r,α
n(w) =L(w).
The proposed EC protocol offers a simple yet effective approach to address the challenges of non-consistency
andnon-validity. Itcanbeseamlesslyappliedinconjunctionwithany SBPA.Thecoreconceptrevolvesaround
the implementation of soft-pruning: any data point is assigned a non-zero selection probability. Samples
with lower scores are given a higher acceptance rate. The contribution of each accepted data point to the
loss is then weighted accordingly. The EC protocol embodies one specific implementation of soft-pruning,
offering the advantage of a single interpretable tuning parameter, the signal proportion α∈[0,1]. By setting
αto 1 or 0, one can recover the SBPAandRandompruning as extreme cases.
Proposition 4 states that any SBPAcalibrated with EC is made consistent and valid as long as some budget
is allocated to exploration. This is empirically validated in Section 5 where we show promising results on a
Toy example (Logistic regression) and other image tasks. However, the exact calibration protocol does not
systematically allow to outperform random pruning.
11Published in Transactions on Machine Learning Research (11/2023)
Figure 5: Data distribution alteration due to pruning in the logistic regression setting. Here we use GraNd
as the pruning algorithm. Blue points correspond to Yi= 0, red points correspond to Yi= 1.
Besides, it is worth noting that different implementations of the same general recipe can be considered. It
is reasonable to expect that more tailored protocols can be designed to suit specific pruning algorithms and
problems. Nevertheless, addressing these questions falls outside the scope of the present work which focus
is to provide a framework to analyse data pruning algorithms, as well as to identify and understand their
fundamental limitations. We propose the EC protocol to illustrate how this understanding allows to design
simple yet efficient solutions to address these limitations.
5 Experiments
5.1 Logistic Regression
Figure 6: Evolution of the performance gap
as the data budget m=rnincreases (aver-
age over 10 runs).We illustrate the main results of this work on a logistic regres-
sion task. We consider the following data-generating process
Xi∼ U/parenleftbig
[−2.5,2.5]dx/parenrightbig
Yi|Xi∼ B/parenleftbigg1
1 +e−wT
0Xi/parenrightbigg
,
wherew0= (1,...,1)∈Rdx,UandBare respectively the
uniform and Bernoulli distributions. The class of models is
given by
M=/braceleftbigg
yout(·;w) :x∝⇕⊣√∫⊔≀→1
1 +e−wTXi|w∈W/bracerightbigg
,
whereW= [−10,10]dx. We train the models using stochastic
gradient descent with the cross entropy loss. For performance
analysis, we take dx= 20andn= 106. For the sake of
visualization, we take dx= 1when we plot the loss landscapes (so that the parameter wis univariate) and
dx= 2when we plot the data distributions.
We use GraNd(Paul, Ganguli, and Dziugaite, 2021) as a pruning algorithm in a teacher-student setting. For
simplicity, we use the optimal model to compute the scores, i.e.
g(Xi,Yi) =−∥∇wℓ(yout(Xi,w0),Yi)∥2,
which is proportional to −(yout(Xi;w0)−Yi)2.Notice that in this setting, GraNdandEL2N(Paul, Ganguli,
and Dziugaite, 2021) are equivalent8. We bring to the reader’s attention that r= 1corresponds to Random
pruning in our plots. Indeed, we compare models as a function of the data budget m=rn. But notice that
8This is different from the original version of GraNd, here, we have access to the true generating process, which is not the
case in practice.
12Published in Transactions on Machine Learning Research (11/2023)
Figure 8: Pruned data distribution for GraNdcalibrated with exact protocol with α= 90%. The top figures
represent the ’signal’ points. The bottom figures represent the ’exploration’ points. Blue markers correspond
toYi= 0, and red markers correspond to Yi= 1.
in the case of Random, for a given m, the values of randndo not affect the distribution of the accepted
datapoints, and this distribution is always the same as the original data distribution, i.e. when r= 1.
Distribution shift and performance degradation: In Section 3, we have seen that the pruning
algorithm induces a shift in the data distribution (Fig. 5). This alteration is most pronounced when ris
small; Forr= 20%, the bottom-left part of the space is populated by Y= 1and the top-right by Y= 0.
Notice that it was the opposite in the original dataset ( r= 1). This translates into a distortion of the loss
landscape and the optimal parameters wA,r
nof the pruned empirical loss becomes different from w0= 1.
Hence, even when a large amount of data is available, the performance gap does not vanish (Fig. 6).
Figure 7: Evolution of the performance gap
with calibrated GraNdas the data budget
m=rnincreases (average over 10 runs).Calibration with the exact protocol: To solve the distri-
bution shift, we resort to the exact protocol with α= 90%. In
otherwords, 10%ofthebudgetisallocatedtoexploration. The
signal points (top images in Fig. 8) are balanced with the ex-
ploration points (bottom images in Fig. 8). Even though there
are nine times fewer of them, the importance weights allow to
correct the distribution shift, as depicted in Fig. 2 (Introduc-
tion): the empirical losses overlap for all values of r, even for
small values for which the predominant labels are swapped (for
exampler= 20%). Hence, the performance gap vanishes when
enough data is available at any compression ratio (Fig. 7).
Impact of the quality of the pruning algorithm: The
calibration protocols allow the performance gap to eventually
vanish if enough data is provided. However, from a practical
point of view, a natural further requirement is that the pruning
method should be better than Random, in the sense that for a given finite budget rn, the error with the
pruning algorithm should be lower than the one of Random. We argue that this mostly decided by the
quality of the original SBPAand its score function. Let us take a closer look at what happens in the
13Published in Transactions on Machine Learning Research (11/2023)
logistic regression case. For a given Xi, denote ˜Yithe most probable label for the input, i.e. /tildewideYi= 1if
yout(Xi,w0)>1/2, and/tildewideYi= 0otherwise. As explained, in this setting, GraNdis equivalent to using the
score function g(Zi) =−|Yi−yout(Xi;w0)|. For a given value of r, considerqrtherthquantile of g(Z).
Notice that g(Z)≤qrif and only if
/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleyout(Xi;w0)−1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤qr+1
2/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Condition 1or/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleyout(Xi;w0)−1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle>/vextendsingle/vextendsingle/vextendsingle/vextendsingleqr+1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingleandYi̸=/tildewideYi/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Condition 2
Therefore, the signal acceptance region is the union of two disjoint sets. The first set is composed of all
samples that are close to the decision boundary, i.e. samples for which the true conditional probability
yout(Xi;w0)is close to 1/2. The second set is composed of samples that are further away from the decision
boundary, but the realized labels need to be the least probable ones ( Yi̸=/tildewideYi). These two subsets are visible
in Figs. 5 and 8 for r= 70%and even more for r= 50%. The signal points can be divided into two sets:
1. thesetofpointsclosetotheboundaryline y=−x, wherethecolorsmatchtheoriginalconfigurations
(mostly blue points under the line, red points over the line)
2. the set of points far away from the boundary line, for which the colors are swapped (only red under
the line, blue over the line).
Figure 9: Evolution of the performance gap
for a small value r= 0.1forGraNdand its
calibrated version with α= 90%.Hence, the signal subset corresponding to Condition 1 gives
valuable insights; it provides finer-grained visibility in the crit-
ical region. However, the second subset is unproductive, as it
only retains points that are not representative of their region.
Calibration allows mitigating the effect of the second subset
while preserving the benefits of the first subset; in Fig. 7, we
can see that the calibrated GraNdoutperforms random pruning
(which corresponds to the r= 1curve), requiring on average
two to three times fewer data to achieve the same generaliza-
tion error. However, as rbecomes lower, qrwill eventually fall
under−1/2, and the first subset becomes empty (for example,
r= 0.2in Fig. 8). Therefore, when rbecomes small, GraNd
does not bring valuable information anymore (for this particu-
lar setting). In Fig. 9, we compare GraNdand Calibrated GraNd
(with the exact protocol) to Randomwithr= 10%. We can see
that thanks to the calibration protocol, the performance gap
will indeed vanish if enough data is available. However, Randompruning outperforms both versions of GraNd
at this compression level. This underlines the fact that for high compression levels, (problem-specific) high-
quality pruning algorithms and score functions are required. Given the difficulty of the task, we believe that
in the high compression regime ( r≤10%here), one should allocate a larger budget to random exploration
(take smaller values of α).
14Published in Transactions on Machine Learning Research (11/2023)
5.2 Scaling Laws with Neural Networks
103105
training_size10203040Error (%)
Random
r=0.1
r=0.2
r=0.4
r=0.8
r=1.0
103105
training_size
Linear
GraNd
103105
training_size102030405060Error (%)
103105
training_size
103105
training_size
NonLinear+Noise
Calibrated GraNd
103105
Subset size1020304050Error (%)
103105
Subset size
103105
Subset size
NonLinear
Figure 10: Test error on a 3-layers MLP (details are
provided in Appendix D) on different pruned datasets
for compression levels r∈{0.1,0.2,0.4,0.8,1}where
thepruningprocedureisperformedwith Randomprun-
ing or GraNd. The caser= 1corresponds to no prun-
ing. Inalltheexperiments, thenetworkistraineduntil
convergence.The distribution shift is the primary cause of the
observed alteration in the loss function, resulting
in the emergence of new minima. Gradient descent
could potentially converge to a bad minimum, in
which case the performance is significantly affected.
To illustrate this intuition, we report in Fig. 10 the
observed scaling laws for three different synthetic
datasets. Let Ntrain = 106,Ntest= 3·104,d= 1000,
andm= 100. Thedatasets are generated as follows:
1.Lineardataset: we first generate a random vec-
torW∼N (0,d−1Id). Then, we generate Ntrain
training samples and Ntesttest samples with the
ruley= 1{W⊤x>0}, wherex∈Rdis simulated from
N(0,Id).
2.NonLinear dataset (Non-linearity): we first gen-
erate a random matrix Win∼ N (0,d−1Id×m)∈
Rd×mand a random vector Wout∼N (0,m−1Im).
The samples are then generated with the rule y=
1{Wout⊤ϕ(W⊤
inx)}, wherex∈Rdis simulated from
N(0,Id), andϕis the ReLU activation function.9
3.NonLinear+Noisy dataset: we first generate
a random vector W∼ N (0,d−1Id). Then, we
generateNtraintraining samples and Ntesttest
samples with the rule y= 1{sin(W⊤x+0.3ϵ)>0}, where
x∈Rdis simulated from N(0,Id)andϵis simulated from N(0,1)and ‘sin’ refers to the sine function.
In Fig. 10, we compare the test error of an 3-layers MLP trained on different subsets generated with either
Randompruning, or GraNd. As expected, with random pruning, the results are consistent regardless of the
compression level ras long as the subset size is the same. With GraNdhowever, the results depend on
the difficulty of the dataset. For the linear dataset, it appears that we can indeed beat the power law
scaling, provided that we have access to enough data. In contrast, GraNdseems to perform poorly on the
nonlinear and noisy datasets in the high compression regime. This is due to the emergence of new local
(bad) minima as rdecreases as evidenced in Fig. 1. Calibrated with the exact protocol, GraNdbecomes
valid: we can see that at any compression rate, the error converges to its minimum, which was not the
case forr≤20%. Whether calibration protocols can allow data pruning algorithms to beat the power law
scaling remains however an open question: further research is needed in this direction. It is also worth noting
that for the Nonlinear datasets, the scaling law pattern exhibits multi-phase behavior. For instance, for the
Nonlinear+Noisy dataset, we can (visually) identify two phases, each one of which follows a different power
law scaling pattern.
5.3 Image Tasks
Through our theoretical analysis, we have concluded that SBPAalgorithms are generally non-consistent.
This effect is most pronounced when the compression level ris small. In this case, the loss landscape can
be significantly altered due to the change in the data distribution caused by the pruning procedure. Given
aSBPAalgorithm, we argue that this alteration in distribution will inevitably affect the performance of the
model trained on the pruned subset, and for small r,Randompruning becomes more effective than the SBPA
algorithm.
9The ReLU activation function is given by ϕ(z) = max(z,0)forz∈R. Here, we abuse the notation a bit and write
ϕ(z) = (ϕ(z1),...,ϕ (zm))forz= (z1,...,zm)∈Rm.
15Published in Transactions on Machine Learning Research (11/2023)
In the following, we empirically investigate this behaviour. We evaluate the performance of different SBPA
algorithms from the literature and confirm our theoretical predictions with empirical evidence. We consider
the following SBPAalgorithms:
•GraNd(Paul, Ganguli, and Dziugaite, 2021): with this method, given a datapoint z= (x,y), the
score function gis given by g(z) =−Ewt∥∇wℓ(yout(x,wt),y)∥2, whereyoutis the model output and
wtare the model parameters (e.g. the weights in a neural network) at training step t, and where
the expectation is taken with respect to random initialization. GraNdselects datapoints with the
highest average gradient norm (w.r.t to initialization).
•Uncertainty (Coleman2020Uncertainty ): in this method, the score function is designed to cap-
ture the uncertainty of the model in assigning a classification label to a given datapoint10. Different
metrics can be used to measure this assignment uncertainty. We focus here on the entropy approach
in which case the score function gis given by g(z) =/summationtextC
i=1pi(x) log(pi(x))wherepi(x)is the model
output probability that xbelongs to class i. For instance, in the context of neural networks, we have
(pi(x))1≤i≤C=Softmax (yout(x,wt)), wheretis the training step where data pruning is performed.
•DeepFool (Ducoffe and Precioso, 2018): this method is rooted in the idea that in a classification
problem, data points that are nearest to the decision boundary are, in principle, the most valuable
for the training process. While a closed-form expression of the margin is typically not available,
the authors use a heuristic from the literature on adversarial attacks to estimate the distance to the
boundary. Specifically, given a datapoint z= (x,y), perturbations are added to the input xuntil
the model assigns the perturbed input to a different class. The amount of perturbation required to
change the label for each datapoint defines the score function in this case (see (Ducoffe and Precioso,
2018) for more details).
Weillustrate the limitationsof the SBPAalgorithms above forsmall r, and show that random pruning remains
a strong baseline in this case. We further evaluate the performance of our calibration protocols and show
that the signal parameter αcan be tuned so that the calibrated SBPAalgorithms outperform random pruning
for smallr. We conduct our experiments using the following setup:
•Datasets and architectures. Our framework is not constrained by the type of the learning task
or the model. However, for our empirical evaluations, we focus on classification tasks with neural
network models. We consider two image datasets: CIFAR10 with ResNet18 and CIFAR100 with
ResNet34 . More datasets and neural architectures are available in our code, which is based on
that of Guo, B. Zhao, and Bai, 2022. The code to reproduce all our experiments will be soon
open-sourced.
•Training. WetrainallmodelsusingSGDwithadecayinglearningrateschedulethatwasempirically
selected following a grid search. This learning rate schedule was also used in Guo, B. Zhao, and Bai,
2022. More details are provided in Appendix D.
•Selection epoch. The selection of the coreset can be performed at differnt training stages. We
consider data pruning at two different training epochs: 1, and 5. We found that going beyond epoch
5(e.g., using a selection epoch of 10) has minimal impact on the performance as compared to using
a selection epoch of 5.
•Pruningmethods. Weconsiderthefollowingdatapruningmethods: Random, GraNd, DeepFool,
Uncertainty . In addition, we consider the pruning methods resulting from applying the proposed
exact calibration protocol to a given SBPAalgorithm. We use the notation SBPA-CP1 to refer to the
resulting method. For instance, DeepFool-CP1 refers to the method resulting from applying (EC)
toDeepFool .
10Uncertainty is specifically designed to be used for classification tasks. This means that it is not well-suited for other types
of tasks, such as regression.
16Published in Transactions on Machine Learning Research (11/2023)
(a) ResNet18 on CIFAR10
 (b) ResNet34 on CIFAR100
Figure 11: Test accuracy for different pruning methods, fractions r, signal parameters α, and selection epochs
(se= 1or5). Confidence intervals based on 3 runs are shown.
Poor performance of SBPAin the high compression regime: Fig. 11 shows the results of the data
pruning methods described above with ResNet18 onCIFAR10 and ResNet34 onCIFAR100 . As expected,
we observe a consistent decline in the performance of the trained model when the compression ratio ris
small, typically in the region r<0.3. More importantly, we observe that SBPAmethods ( GraNd,DeepFool ,
Uncertainty in orange) perform consistently worse than Randompruning (in green), confirming our hypoth-
esis. We also observe that amongst the three SBPAmethods, DeepFool is generally the best in the region of
interest ofrand competes with random pruning when the subset selection is performed at training epoch 1.
We noticed that in that setting DeepFool is close to random pruning.
Effect of the calibration protocol Our proposed calibration protocol aim to correct the bias by injecting
some randomness in the selection process and keeping (on average) only a fraction αof the SBPAmethod. We
notice that the calibration protocol applied to different SBPAconsistently boosts the performance in the high
compression regime, as can be observed in Fig. 11. Fig. 12 shows that the calibrated SBPAperform better
than Randompruning for specific choices of α. However, the difference is not always significant. Besides,
finding the optimal proportion of signal αcan be difficult in practice.
17Published in Transactions on Machine Learning Research (11/2023)
(a) ResNet18 on CIFAR10
 (b) ResNet34 on CIFAR100
Figure 12: Test accuracy for different pruning methods, fractions r, and selection epochs ( se= 1or5). Best
αused for calibration. Different values of rmay have different αvalues.
6 Related Work
As we mentioned in the introduction. The topic of coreset selection has been extensively studied in classical
machine learning and statistics (Welling, 2009; Chen, Welling, and Smola, 2012; Feldman, Faulkner, and
Krause, 2011; Huggins, Campbell, and Broderick, 2016; Campbell and Broderick, 2019). These classical
approaches were either model-independent or designed for simple models (e.g. linear models). The recent
advances in deep learning has motivated the need for new adapted methods for these deep models. Many
approaches have been proposed to adapt to the challenges of the deep learning context. We will cover existing
methods that are part of our framework ( SBPAalgorithms) and others that fall under different frameworks
(non- SBPAalgorithms).
6.1 Score-based Methods
These can generally be categorized into four groups:
1. Geometry based methods: these methods are based on some geometric measure in the feature
space. The idea is to remove redundant examples in this feature space (examples that similar
representations). Examples include Herding ((Chen, Welling, and Smola, 2012)) which aims to
greedily select examples by ensuring that the centers of the coreset and that of the full dataset are
18Published in Transactions on Machine Learning Research (11/2023)
close. A similar idea based on the K-centroids of the input data was used in (Sener and Savarese,
2018; Agarwal et al., 2020; Sinha et al., 2020).
2. Uncertainty based methods: the aim of such methods is to find the most “difficult" examples, defined
as the ones for which the model is the least confident. Different uncertainty measures can be used
for this purpose, see (Coleman et al., 2020) for more details.
3. Error based methods: the goal is to find the most significant examples defined as the ones that
contribute the most to the loss. In Paul, Ganguli, and Dziugaite, 2021, the authors consider the
second norm of the gradient as a proxy to find such examples. Indeed, examples with the highest
gradient norm tends to affect the loss more significantly (a first order Taylor expansion of the loss
function can explain the intuition behind this proxy). This can be thought of as a relaxation of
a Lipschitz-constant based pruning algorithm that was recently introduced in Ayed and Hayou,
2022. Another method consider keeping the most forgettable examples defined as those that change
the most often from being well classified to being mis-classified during the course of the training
(Toneva et al., 2019). Other methods in this direction consider a score function based on the relative
contribution of each example to the total loss over all training examples (see Bachem, Lucic, and
Krause, 2015; Munteanu et al., 2021).
4. Decision boundary based: although this can be encapsulated in uncertainty-based methods, the idea
behind these methods is more specific. The aim is to find the examples near the decision boundary,
the points for which the prediction has the highest variation (e.g. with respect to the input space,
Ducoffe and Precioso, 2018; Margatina et al., 2021).
6.2 Non- SBPAMethods
Other methods in the literature select the coreset based on other desirable properties. For instance, one
could argue that preserving the gradient is an important feature to have in the coreset as it would lead to
similar minima (Killamsetty, Sivasubramanian, Ramakrishnan, De, et al., 2021; Mirzasoleiman, Bilmes, and
Leskovec, 2020). Other work considered the problem of corset selection as a two-stage optimization problem
where the subset selection can be seen also as an optimization problem (Killamsetty, Sivasubramanian,
Ramakrishnan, and Iyer, 2021; Killamsetty, X. Zhao, et al., 2021). Other methods consider conisder the
likelihood and its connection with submodular functions in order to select the subset ( kaushal2021prism ;
Kothawade et al., 2021).
It is worth noting that there exist other approaches to data pruning that involve synthesizing a new dataset
with smaller size that preserves certain desired properties, often through the brute-force construction of
samples that may not necessarily represent the original data. These methods are known as data distillation
methods (see e.g. Wang et al., 2020; B. Zhao, Mopuri, and Bilen, 2021; B. Zhao and Bilen, 2021) However,
these methods have significant limitations, including the difficulty of interpreting the synthesized samples
and the significant computational cost. The interpretability issue is particularly a these approaches to use
in real-world applications, particularly in high-stakes fields such as medicine and financial engineering.
7 Discussion and Limitations
7.1 Extreme Scenarios
Our framework provides insights in the case where both nandrnare large. As a result, there are cases
where this framework is not applicable. We call these cases extreme scenarios.
Extreme scenario 1: small n.Our asymptotic analysis can provide insights when a sufficient number of
samples are available. In the scarce data regime (small n), our theoretical results may not accurately reflect
the impact of pruning on the loss function. It is worth noting, however, that this case is generally not of
practical interest as there is no benefit to data pruning when the sample size is small.
19Published in Transactions on Machine Learning Research (11/2023)
Extreme scenario 2: large nwithr= Θ(n−1)).In this case, the “effective" sample size after pruning
isrn= Θ(1). Therefore, we cannot glean useful information from the asymptotic behavior of LA,r
nin this
case. It is also worth noting that the variance of LA,r
ndoes not vanish in the limit n→∞,r→0withrn=γ
fixed, and therefore the empirical mean does not converge to the asymptotic mean.
7.2 Asymptotic Results
Theorem 1 and 2, and the subsequent corollaries are asymptotic results. They essentially reveal the limita-
tions of SBPAfor "large enough models". We decided to take this direction to get results that are as general
as possible, showing that the discussed limitations will appear in most situations. Theorem 1 and 2 apply to
any configuration from a large variety of classes of models, SBPAs, generating processes, and loss functions.
The theory readily covers realistic architectures illustrated by Corollary 3, 4 and 5 (in the appendix) that
cover wide NN, deep neural NN, and convolutional NN with enough filters. However, these limitations could
appear for unrealistically large models. We acknowledge this drawback of the proposed theory and address
it in two ways. First, we experimentally show that for the usual settings (ResNet on Cifar), we already
observe this significant drop in the performance of SBPAs compared to random pruning. This also aligns
with other empirical observations (Guo, B. Zhao, and Bai, 2022). In addition, we provide Theorem 3 to
cover the cases where the class of functions is potentially not rich enough; even in that case, for any SBPA,
one can find datasets for which the pruning algorithm will fail for small compression ratios. Besides, to our
knowledge, the lines of work that allow one to derive explicit bounds for Neural Networks usually require
specific and often overly simplistic architectures (typically one hidden layer feed-forward). In our context,
we additionally expect similar strong restrictions to be required for the SBPAs and generating processes.
This could wrongfully lead practitioners to consider that using a different SBPAor class of models than the
one for which one could derive quantitative results would circumvent the limitations.
7.3 Overparameterized Regime
The scenario in which both the number of parameters pand the sample size ntend to infinity (e.g. with a
constant ratio γ=p/n) holds practical significance. Our framework does not cover this case and we would
like to elucidate the main point at which our proof machinery encounters challenges under this scenario.
The main issue resides in understanding the asymptotic behaviour of SBPA algorithms in this context,
particularly the extension of Proposition 3. While we can establish concentration with pconstant and n
growing large, achieving this with both nandpgoing to infinity, especially when the underlying model is
a neural network, becomes generally intractable. Nonetheless, under certain supplementary assumptions, it
remains feasible to demonstrate concentration.
RecallthatinProposition3, weessentiallyuseavariationofthelawoflargenumberstoshowtheconvergence
in the infinite sample size limit ( n→∞), whilep(and consequently, w) is fixed. However, in the scenario
where both pandntend to infinity, the dependency of wonpcomplicates matters, rendering the used
variation of the LLN inapplicable. An essential condition under such circumstances becomes the convergence
ofwin a certain sense as well, as p→∞. For this purpose, a pertinent tool is LLN for Triangular Arrays,
which takes the shape of:
Consider a Triangular Array (Xn,i)i∈[n],n≥1of random variables such that for each n, the variables (Xn,i)i∈[n]
are iid with mean µn. Then, under some assumptions (bounded second moments) we have
n−1/parenleftiggn/summationdisplay
i=1Xn,i−µn/parenrightigg
→∞.
However, note that in our case, the terms Xn,i=ℓ(yout(Xi;wp),Yi)1{iis accepted}are not necessarily inde-
pendent, and therefore more advanced LLN for trinagular arrays should be proven and used. We leave this
for future work.
20Published in Transactions on Machine Learning Research (11/2023)
7.4 Pruning Time Vs Training Time
In some cases, the pruning procedure might be compute-intensive, and requires more resources than the
actual training with the full dataset. This is the case when, for instance, the pruning criterion depends on
second order geometry (Hessian etc) and/or multiple perturbations of some quantity (DeepFool), or pruning
is performed in multi-shot settings (dynamical pruning). In this paper, we considered pruning criteria that
can be performed “one shot” and rely on criteria that involve either gradients (GraNd) or network outputs
(Uncertainty) or perturbations of the network output (DeepFool). For GraNd and Uncertainty pruning, the
pruning time is typically less than the time required for 1 epoch of training. For DeepFool however, the
pruning time might be comparable to 10 epochs of training.
References
Agarwal, S., H. Arora, S. Anand, and C. Arora (2020). “Contextual Diversity for Active Learning”. In:
European Conference on Computer Vision .
Aljundi, R., M. Lin, B. Goujaud, and Y. Bengio (2019). “Gradient Based Sample Selection for Online
Continual Learning”. In: Advances in Neural Information Processing Systems .
Arkhangel’skiˇ ı, A. V. (2001). “Fundamentals of General Topology: Problems and Exercises”. In: pp. 123–124.
Ayed, F. and S. Hayou (2022). “The Curse of (non)Convexity: The Case of an Optimization-Inspired Data
Pruning Algorithm”. In: I Can’t Believe It’s Not Better Workshop: Understanding Deep Learning Through
Empirical Falsification .
Bachem, O., M. Lucic, and A. Krause (2015). “Coresets for Nonparametric Estimation - the Case of DP-
Means”. In: International Conference on Machine Learning .
Campbell, T. and T. Broderick (2019). “Automated Scalable Bayesian Inference via Hilbert Coresets”. In:
The Journal of Machine Learning Research 20.1, pp. 551–588.
Chen, Y., M. Welling, and A. Smola (2012). Super-Samples from Kernel Herding . arXiv: 1203.3472 [cs.LG] .
Coleman, C., C. Yeh, S. Mussmann, B. Mirzasoleiman, P. Bailis, P. Liang, J. Leskovec, and M. Zaharia
(2020). “Selection via Proxy: Efficient Data Selection for Deep Learning”. In: International Conference on
Learning Representations .
Deimling, K. (2010). Nonlinear Functional Analysis . Courier Corporation.
Ducoffe, M. and F. Precioso (2018). Adversarial Active Learning for Deep Networks: a Margin Based Ap-
proach. arXiv: 1802.09841 [cs.LG] .
Feldman, D., M. Faulkner, and A. Krause (2011). “Scalable Training of Mixture Models via Coresets”. In:
Advances in neural information processing systems 24.
Guo, C., B. Zhao, and Y. Bai (2022). DeepCore: A Comprehensive Library for Coreset Selection in Deep
Learning . arXiv: 2204.08499 [cs.LG] .
Hernandez, D., J. Kaplan, T. Henighan, and S. McCandlish (2021). Scaling Laws for Transfer . arXiv: 2102.
01293 [cs.LG] .
Hestness, J., S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. M. A. Patwary, Y. Yang, and
Y. Zhou (2017). Deep Learning Scaling is Predictable, Empirically . arXiv: 1712.00409 [cs.LG] .
Hoffmann, J., S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de las Casas, L. A. Hen-
dricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy,
S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. W. Rae, and L. Sifre (2022). “An Empirical Analysis
of Compute-optimal Large Language Model Training”. In: Advances in Neural Information Processing
Systems.
Hornik, K. (1991). “Approximation Capabilities of Multilayer Feedforward Networks”. In: Neural Networks
4.2, pp. 251–257.
Huggins, J., T. Campbell, and T. Broderick (2016). “Coresets for Scalable Bayesian Logistic Regression”.
In:Advances in Neural Information Processing Systems 29.
Kaplan, J., S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and
D. Amodei (2020). Scaling Laws for Neural Language Models . arXiv: 2001.08361 [cs.LG] .
Kidger, P. and T. Lyons (2020). “Universal Approximation with Deep Narrow Networks”. In: Conference on
learning theory , pp. 2306–2327.
21Published in Transactions on Machine Learning Research (11/2023)
Killamsetty, K., D. Sivasubramanian, G. Ramakrishnan, A. De, and R. K. Iyer (2021). “GRAD-MATCH:
Gradient Matching based Data Subset Selection for Efficient Deep Model Training”. In: International
Conference on Machine Learning .
Killamsetty, K., D. Sivasubramanian, G. Ramakrishnan, and R. Iyer (2021). “GLISTER: Generalization
based Data Subset Selection for Efficient and Robust Learning”. In: Proceedings of the AAAI Conference
on Artificial Intelligence .
Killamsetty, K., X. Zhao, F. Chen, and R. Iyer (2021). RETRIEVE: Coreset Selection for Efficient and
Robust Semi-Supervised Learning . arXiv: 2106.07760 [cs.LG] .
Kingma, D. P. and J. Ba (2017). Adam: A Method for Stochastic Optimization . arXiv: 1412.6980 [cs.LG] .
Kothawade, S. N., N. A. Beck, K. Killamsetty, and R. K. Iyer (2021). “SIMILAR: Submodular Information
Measures Based Active Learning In Realistic Scenarios”. In: Advances in Neural Information Processing
Systems.
Margatina, K., G. Vernikos, L. Barrault, and N. Aletras (2021). Active Learning by Acquiring Contrastive
Examples . arXiv: 2109.03764 [cs.CL] .
Mirzasoleiman, B., J. Bilmes, and J. Leskovec (2020). “Coresets for Data-efficient Training of Machine
Learning Models”. In: International Conference on Machine Learning , pp. 6950–6960.
Munteanu, A., C. Schwiegelshohn, C. Sohler, and D. P. Woodruff (2021). On Coresets for Logistic Regression .
arXiv: 1805.08571 [cs.DS] .
Paul, M., S. Ganguli, and G. K. Dziugaite (2021). “Deep Learning on a Data Diet: Finding Important
Examples Early in Training”. In: Advances in Neural Information Processing Systems .
Robbins, H. and S. Monro (1951). “A Stochastic Approximation Method”. In: The annals of mathematical
statistics , pp. 400–407.
Rosenfeld, J. S., A. Rosenfeld, Y. Belinkov, and N. Shavit (2020). “A Constructive Prediction of the Gener-
alization Error Across Scales”. In: International Conference on Learning Representations .
Sener, O. and S. Savarese (2018). Active Learning for Convolutional Neural Networks: A Core-Set Approach .
arXiv: 1708.00489 [stat.ML] .
Sinha, S., H. Zhang, A. Goyal, Y. Bengio, H. Larochelle, and A. Odena (2020). “Small-GAN: Speeding up
GAN Training using Core-Sets”. In: International Conference on Machine Learning .
Sorscher, B., R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos (2022). “Beyond Neural Scaling Laws:
Beating Power Law Scaling via Data Pruning”. In: Advances in Neural Information Processing Systems .
Toneva, M., A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio, and G. J. Gordon (2019). An Empirical
Study of Example Forgetting during Deep Neural Network Learning . arXiv: 1812.05159 [cs.LG] .
Varadarajan, V. S. (1958). “On the Convergence of Sample Probability Distributions”. In: Sankhy¯ a: The
Indian Journal of Statistics (1933-1960) 19.1/2, pp. 23–26.
Wang, T., J.-Y. Zhu, A. Torralba, and A. A. Efros (2020). Dataset Distillation . arXiv: 1811.10959 [cs.LG] .
Welling, M. (2009). “Herding dynamical weights to learn”. In: Proceedings of the 26th Annual International
Conference on Machine Learning , pp. 1121–1128.
Zhai, X., A. Kolesnikov, N. Houlsby, and L. Beyer (2022). “Scaling Vision Transformers”. In: IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) .
Zhao, B. and H. Bilen (2021). “Dataset Condensation with Differentiable Siamese Augmentation”. In: Inter-
national Conference on Machine Learning .
Zhao, B., K. R. Mopuri, and H. Bilen (2021). “Dataset Condensation with Gradient Matching”. In: Inter-
national Conference on Learning Representations .
Zhou, D.-X. (2020). “Universality of Deep Convolutional Neural Networks”. In: Applied and computational
harmonic analysis 48.2, pp. 787–794.
8 Acknowledgement
We would like to thank the authors of DeepCore project (Guo, B. Zhao, and Bai, 2022) for open-sourcing
their excellent code11. The high flexibility and modularity of their code allowed us to quickly implement our
calibration protocols on top of existing SBPAalgorithms.
11The code by Guo, B. Zhao, and Bai, 2022 is available at https://github.com/PatrickZH/DeepCore .
22Published in Transactions on Machine Learning Research (11/2023)
A Proofs
A.1 Proofs of Section 2
Propositions 1 and 2 are built on the following lemma.
Lemma 1 Letπbe a distribution on Dand(wn)na sequence of parameters in Wθsatisfying
Eπℓ(yout(X;wn),Y)→min
w∈WθEπℓ(yout(X;w),Y).
Then, it comes that
d(wn,W∗
θ(π))→0.
Proof: DenoteLπthe function from WθtoRdefined by
Lπ(w) =Eπℓ(yout(X;w),Y).
Notice that under our assumptions, the dominated convergence theorem gives that Lπis continuous. This
lemma is a simple consequence of the continuity of Lπand the compacity of Wθ. Consider a sequence (wn)
such that
Lπ(wn)→min
w∈WθLπ(w).
We can prove the lemma by contradiction. Consider ϵ >0and assume that there exists infinitely many
indicesnkfor whichd/parenleftig
wnk,W∗
θ(π)/parenrightig
> ϵ.SinceWθis compact, we can assume that wnkis convergent (by
considering a subsequence of which if needed), denote w∞∈Wθits limit. The continuity of dthen gives
thatd/parenleftig
w∞,W∗
θ(π)/parenrightig
≥ϵ, and in particular
w∞̸∈W∗
θ(π) =argminw∈WθLπ(w).
But sinceLπis continuous, the initial assumption on (wn)translates to
min
w∈WθLπ(w) = lim
kLπ(wnk) =Lπ(w∞),
concluding the proof. □
Proposition 1. A pruning algorithm Ais valid at a compression ratio r∈(0,1]if and only if
d/parenleftig
wA,r
n,W∗
θ(µ)/parenrightig
→0a.s.
whereW∗
θ(µ) =argminw∈WθL(w)⊂Wθandd/parenleftig
wA,r
n,W∗
θ(µ)/parenrightig
denotes the euclidean distance from the point
wA,r
nto the setW∗
θ(µ).
Proof: This proposition is a direct consequence of Lemma 1. Consider a valid pruning algorithm A, a
compression ratio rand a sequence of observations (Xk,Yk)such that
L(wA,r
n)→min
w∈WθL(w).
We can apply Lemma 1 on the sequence (wA,r
n)with the distribution π=µto get the result. □
Proposition 2. LetAbe a pruning algorithm and r∈(0,1]a compression ratio. Assume that there exists
a probability measure νronDsuch that
∀w∈Wθ,LA,r
n(w)→Eνrℓ(yout(X;w),Y)a.s. (5)
Then, denotingW∗
θ(νr) =argminw∈WθEνrℓ(yout(X;w),Y)⊂Wθ, we have that
d/parenleftig
wA,r
n,W∗
θ(νr)/parenrightig
→0a.s.
23Published in Transactions on Machine Learning Research (11/2023)
Proof: Leveraging Lemma 1, it is enough to prove that
Eνrℓ(yout(X;wA,r
n),Y)−min
w∈WθEνrℓ(yout(X;w),Y)→0a.s.
To simplify the notations, we introduce the function ffromD×WθtoRdefined by
f(z,w) =ℓ(yout(x;w),y),
wherez= (x,y).SinceWθis compact, we can find w∗∈Wθsuch that Eνr[f(z,w∗)] = minwEνr[f(z,w)].
It comes that
0≤Eνr[f(z,wA,r
n)]−Eνr[f(z,w∗)]
≤Eνr[f(z,wA,r
n)]−1
rn/summationdisplay
z∈A(Dn,r)f(z,wA,r
n)
+1
rn/summationdisplay
z∈A(Dn,r)f(z,wA,r
n)−1
rn/summationdisplay
z∈A(Dn,r)f(z,w∗)
+1
rn/summationdisplay
z∈A(Dn,r)f(z,w∗)−Eνr[f(z,w∗)]
The last term converges to zero almost surely by assumption. By definition of wA,r
n, the middle term is
non-positive. It remains to show that the first term also converges to zero. With this, we can conclude that
limnEνr[f(z,wA,r
n)]−Eνr[f(z,w∗)] = 0
To prove that the first term converges to zero, we use the classical result that if every subsequence of a
sequence (un)has a further subsequence that converges to u, then the sequence (un)converges to u. Denote
un=Eνr[f(z,wA,r
n)]−1
rn/summationdisplay
z∈A(Dn,r)f(z,wA,r
n).
Bycompacity ofWθ, fromany subsequence of (un)wecan extractafurthersubsequencewithindicesdenoted
(nk)such thatw∗
nkconverges to some w∞∈Wθ. We will show that (unk)converges to 0. Letϵ>0, since
fis continuous on the compact set D×Wθ, it is uniformly continuous. Therefore, almost surely, for klarge
enough,
sup
z|f(z,w∗
nk)−f(z,w∞)|≤ϵ.
Denoting
vn=Eνr[f(z,w∞)]−1
rn/summationdisplay
z∈A(Dn,r)f(z,w∞),
the triangular inequality then gives that, almost surely, for klarge enough
|unk−vnk|≤2ϵ.
By assumption, the sequence vnkconverges to zero almost surely, which concludes the proof. □
We now prove Corollary 2, since Corollary 1 is a straightforward application of Proposition 2.
Corollary2. LetAbe any pruning algorithm and r∈(0,1], and assume that (5)holds for a given probability
measureνronD. IfAis valid, thenW∗
θ(νr)∩W∗
θ(µ)̸=∅; or, equivalently,
min
w∈W∗
θ(νr)L(w) = min
w∈WL(w).
24Published in Transactions on Machine Learning Research (11/2023)
Proof: This proposition is a direct consequence of Proposition 2 that states that
d(wA,r
n,W∗
θ(νr))→0a.s.
Since theLis continuous on the compact Wθ, it is uniformly continuous. Hence, for any ϵ>0, we can find
η>0such that if d(w,w′)≤η, then|L(w)−L(w′)|≤ϵfor any parameters w,w′∈Wθ. Hence, for nlarge
enough,d(wA,r
n,W∗
θ(νr))≤η, leading to
L(wA,r
n)≥min
w∈W∗
θ(r)L(w)−ϵ.
Since the algorithm is valid, we know that L(wA,r
n)converges to minw∈WθL(w)almost surely. Therefore,
for anyϵ>0,
min
w∈WθL(w)≥min
w∈W∗
θ(r)L(w)−ϵ.
which concludes the proof. □
A.2 Proof of Proposition 3
Proposition 3. [Asymptotic behavior of SBPA]
LetAbe a SBPAalgorithm and let gbe its corresponding score function. Assume that gis adapted, and
consider a compression ratio r∈(0,1). Denote by qrtherthquantile of the random variable g(Z)where
Z∼µ. DenoteAr={z∈D|g(z)≤qr}. Almost surely, the empirical measure of the retained data samples
converges weakly to νr=1
rµ|Ar, whereµ|Aris the restriction of µto the setAr. In particular, we have that
∀w∈Wθ,LA,r
n(w)→Eνrℓ(yout(X;w),Y)a.s.
Proof: ConsiderFthe set of functions f:D→ [−1,1]that are continuous. We will show that
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
|A(Dn,r)|/summationdisplay
z∈A(Dn,r)f(z)−1
r/integraldisplay
Arf(z)µ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle→0a.s. (9)
To simplify the notations, and since|A(Dn,r)|
rnconverges to 1, we will assume that rnis an integer. Denote
qr
nthe(rn)thordered statistic of/parenleftbig
g(zi)/parenrightbig
i=1,...,n, andqrtherthquantile of the random variable g(Z)where
Z∼µ.
We can upper bound the left hand side in equation (9) by the sum of two random terms AnandBndefined
by
Bn=1
rsup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
z∈Dnf(z)Ig(z)≤qrn−1
n/summationdisplay
z∈Dnf(z)Ig(z)≤qr/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Cn=1
rsup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
z∈Dnf(z)Ig(z)≤qr−/integraldisplay
f(z)Ig(z)≤qrµ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
To conclude the proof, we will show that both terms converge to zero almost surely.
25Published in Transactions on Machine Learning Research (11/2023)
For anyf∈F, denotingGnthe empirical cumulative density function (cdf) of (g(zi))andGthe cdf ofg(Z),
we have that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
z∈Dnf(z)Ig(z)≤qrn−1
n/summationdisplay
z∈Dnf(z)Ig(z)≤qr/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
n/summationdisplay
z∈Dn|f(z)|×/vextendsingle/vextendsingleIg(z)≤qrn−Ig(z)≤qr/vextendsingle/vextendsingle
≤1
n/summationdisplay
z∈Dn/vextendsingle/vextendsingleIg(z)≤qrn−Ig(z)≤qr/vextendsingle/vextendsingle
≤ |Gn(qr
n)−Gn(qr)|
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
r−Gn(qr)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=|G(qr)−Gn(qr)|.
Therefore, Bn≤supt∈R|G(t)−Gn(t)|which converges to zero almost surely by the Glivenko-Cantelli
theorem.
Similarly, the general Glivenko-Cantelli theorem for metric spaces (Varadarajan, 1958) gives that almost
surely,
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
z∈Dnf(z)−/integraldisplay
f(z)µ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle→0.
Considerk≥1. Sincegis continuous and Dis compact, the sets Ar(1−1/k)andAr=D\Arare disjoint
and closed subsets. Using Urysohn’s lemma (Theorem 8 in the Appendix), we can find fk∈Fsuch that
fk(z) = 1ifz∈Ar(1−1/k)andfk(z) = 0ifz∈Ar. Consider f∈F, it comes that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
z∈Dnf(z)Ig(z)≤qr−/integraldisplay
f(z)Ig(z)≤qrµ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
z∈Dnf×fk(z)−/integraldisplay
f×fk(z)µ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+1
n/summationdisplay
z∈DnIqr(1−1/k)≤g(z)≤qr
+/integraldisplay
Iqr(1−1/k)≤g(z)≤qrµ(z)dz
Hence, noticing that f×fk∈F, we find that
Cn≤supf∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
z∈Dnf(z)−/integraldisplay
f(z)µ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+|Gn(qr)−Gn(qr(1−1/k))|+r
k.
We can conclude the proof by noticing that |Gn(qr)−Gn(qr(1−1/k))|converges tor
kand taking k→∞.□
A.3 Proof of Theorem 1
In order to prove the theorem, we will need a few technical results that we state and prove first.
Lemma 2 Consider a set of continuous functions MfromXtoY. Consider ψ0a function in the closure
ofMfor theℓ∞norm. Then for any ϵ>0, there exists ψ∈Msuch that
supx,y∈D∥ℓ(ψ(x),y)−ℓ(ψ0(x),y)∥≤ϵ
Proof: Since the loss ℓis continuous on the compact Y×Y, it is uniformly continuous. We can therefore
findη >0such that for any y0,y,y′∈Y, if∥y−y′∥≤ηthen∥ℓ(y0,y)−ℓ(y0,y′)∥≤ϵ.We conclude the
proof using by selecting any ψ∈Mthat is at a distance not larger than ηfromψ0for theℓ∞norm. □
26Published in Transactions on Machine Learning Research (11/2023)
Lemma 3 Consider a SBPAA. LetMbe a set of continuous functions from XtoY. Consider r∈(0,1)
and assume thatAis consistent onMat levelr, i.e.
∀ψ∈M,1
|A(D,r)|/summationdisplay
(x,y)∈A(D,r)ℓ(ψ(x),y)→Eµℓ(ψ(X),Y)a.s.
Letψ∞be any measurable function from XtoY. If there exists a sequence of elements of Mthat converges
point-wise to ψ∞, then
E1
rµ|Arℓ(ψ∞(X),Y) =Eµℓ(ψ∞(X),Y). (10)
In particular, ifMhas the universal approximation property, then (10)holds for any continuous function.
Proof: Le(ψk)kbe a sequence of functions in Mthat converges point-wise to ψ∞. Consider k≥0, since
Ais consistent and that ψkis continuous and bounded, Proposition 3 gives that
E1
rµ|Arℓ(ψk(X),Y) =Eµℓ(ψk(X),Y).
Sinceℓis bounded, we can apply the dominated convergence theorem to both sides of the equation to get
the final result. □
Proposition 5 hereafter proves the final result for SBPAthat do not depend on the labels. The proof of
Theorem 1 that follows essentially deals with the remaining case of SBPAthat depend on the labels.
Proposition 5 LetAbe any SBPAwith an adapted score function gsatisfying
∃˜g:X→R+, g(x,y) = ˜g(x)a.s.
Assume that there exists two continuous functions f1andf2such that
Eµℓ(f1(X),Y)̸=Eµℓ(f2(X),Y).
If∪θMθhas the universal approximation property, then there exist hyper-parameters θ∈Θfor which the
algorithm is not consistent.
Proof: Considera compressionratio r∈(0,1). We willprovethe resultby meansof contradiction. Assume
that the SBPAis consistent on∪θMθ. From the universal approximation property and Lemma 3, we get that
1
rEµ|Arℓ(f1(X),Y) =Eµℓ(f1(X),Y),
from which we deduce that
Eµ/bracketleftig
ℓ(f1(X),Y)I(Z∈Ar)/bracketrightig
=rEµℓ(f1(X),Y) (11)
Eµ/bracketleftig
ℓ(f1(X),Y)I(Z∈D\Ar)/bracketrightig
= (1−r)Eµℓ(f1(X),Y) (12)
and similarly for f2.
Notice that since the score function gdoes not depend on Y, there existsXr⊂Xsuch thatAr=Xr×Y.
Consider the function defined by
f:x∝⇕⊣√∫⊔≀→f1(x)I(x∈Xr) +f2(x) (1−I(x∈Xr)),
we will show that
i)1
rEµ|Arℓ(f(X),Y)̸=Eµℓ(f(X),Y)
ii) There exists a sequence of elements in ∪θMθthat converges point-wise almost everywhere to f
27Published in Transactions on Machine Learning Research (11/2023)
The conjunction of these two points contradicts Lemma 3, which would conclude the proof.
The first point is obtained through simple derivations, evaluating both sides of the equation i).
1
rEµ|Arℓ(f(X),Y) =1
rEµℓ(f(X),Y)I(Z∈Xr×Y)
=1
rEµℓ(f(X),Y)I(X∈Xr)
=1
rEµℓ(f1(X),Y)I(X∈Xr)
=1
rEµℓ(f1(X),Y)I(Z∈Ar)
=Eµℓ(f1(X),Y),
where we successively used the definition of fand equation (11). Now, using the definition of f, we get that
Eµℓ(f(X),Y) = Eµℓ(f1(X),Y)I(X∈Xr) +Eµℓ(f2(X),Y) (1−I(X∈Xr))
=Eµℓ(f1(X),Y)I(Z∈Ar) +Eµℓ(f2(X),Y)I(Z∈D\Ar)
=rEµℓ(f1(X),Y) + (1−r)Eµℓ(f2(X),Y).
These derivations lead to
1
rEµ|Arℓ(f(X),Y)−Eµℓ(f(X),Y) = (1−r) [Eµℓ(f1(X),Y)−Eµℓ(f2(X),Y)]̸= 0,
by assumption on f1andf2.
For point ii), we will construct a sequence (ψk)kof functions in∪θMθthat converges point-wise to falmost
everywhere, using the definition of the universal approximation property and Urysohn’s lemma (Lemma 8 in
the Appendix). Consider k≥0and denote ϵk=1−r
k+1. Denoteqrandqr+ϵktherthand(r+ϵk)thquantile of
the random variable ˜g(X)where (X,Y )∼µ. DenoteXr={x∈X| ˜g(x)≤qr}andBr,k={x∈X| ˜gr(x)≥
qr+ϵk}. Since ˜gis continuous and Xis compact, the two sets are closed. Besides, since the random variable
˜g(X)is continuous ( gis an adapted score function), both sets are disjoint. Therefore, using Urysohn’s lemma
(Lemma 8 in the Appendix), we can chose a continuous function ϕk:X → [0,1]such thatϕk(x) = 1for
x∈Xrandϕk(x) = 0forx∈Br,k. Denotefkthe function defined by
¯fk(x) =f1(x)ϕk(x) +f2(x)(1−ϕk(x)).
Notice that (ϕk)kconverges point-wise to I(·∈Xr), and therefore (¯fk)kconverges point-wise to f. Besides,
since ¯fkis continuous, and ∪θMθhas the universal approximation property, we can chose ψk∈∪θMθsuch
that
supx∈X|ψk(x)−¯fk(x)|≤ϵk.
Hence, for any input x∈X, we can upper-bound |ψk(x)−f(x)|byϵk+|¯fk(x)−f(x)|, giving that ψk
converges pointwise to fand concluding the proof. □
We are now ready to prove the Theorem 1 that we state here for convenience.
Theorem 1. LetAbe any SBPAalgorithm with an adapted score function. If ∪θMθhas the universal
approximation property, then there exist hyper-parameters θ∈Θfor which the algorithm is not consistent.
Proof: We will use the universal approximation theorem to construct a model for which the algorithm is
biased. Denote supp (µ)the support of the generating measure µ. We can assume that there exists x∈X
such that (x0,0)∈supp(µ),(x0,1)∈supp(µ), andg(x0,1)̸=g(x0,0),otherwise one can apply Proposition 5
to get the result. Denote y0∈{0,1}such thatg(x0,y0)>g(x0,1−y0). Sincegis continuous, we can find
ϵ>0,r0∈(0,1)such that
∀x∈B(x0,ϵ), g(x,y0)>qr0>g(x,1−y0), (13)
28Published in Transactions on Machine Learning Research (11/2023)
whereqr0is therth
0quantile of g(Z)whereZ∼µ.
Since (x0,1−y0)∈supp(µ), it comes that
∆ =1−r0
2(1 +r0)P/parenleftig
X∈B(x0,ϵ),Y= 1−y0/parenrightig
ℓ(y0,1−y0)>0.
By assumption, the distribution of Xis dominated by the Lebesgue measure, we can therefore find a positive
ϵ′<ϵsuch that
P/parenleftig
X∈B(x0,ϵ)\B(x0,ϵ′)/parenrightig
<∆
2 maxℓ.
The setsK1=B(x0,ϵ′)andK2=X\Bo(x0,ϵ)are closed and disjoint sets, Lemma 8 in Appendix insures
the existance of a continuous function hsuch thath(x) =y0forx∈K1, andh(x) = 1−y0forx∈K2.
We use Lemma 2 to construct ψ∈∪θMθsuch that for any x,y∈D,|ℓ(ψ(x),y)−ℓ(h(x),y)|<∆/2.Let
f1(x,y) =ℓ(ψ(x),y)andf2(x,y) =ℓ(1−y0,y). Denotef=f1−f2. Notice that if we assume that the
algorithm is consistent on ∪θMθ, Lemma 3 gives that Ef(X,Y ) =1
r0Ef(X,Y ) 1g(X,Y)≤qr0.We will prove
the non-consistency result by means of contradiction, showing that instead we have
Ef(X,Y )<1
r0Ef(X,Y ) 1g(X,Y)≤qr0. (14)
To do so, we start by noticing three simple results that are going to be used in the following derivations
•∀x∈K2,y∈Y,f(x,y) = 0.
•∀x∈K1,f(x,y0) =−ℓ(1−y0,y0)andf(x,1−y0) =ℓ(y0,1−y0)
•∀x∈B(x0,ϵ)\B(x0,ϵ′),y∈Y,|f(x,y)|≤maxℓ
We start be upper bounding the left hand side of (14) as follows:
Ef(X,Y ) = Ef(X,Y )/bracketleftbig
1X∈K1+ 1X∈K2+ 1X∈B(x0,ϵ)\B(x0,ϵ′)/bracketrightbig
≤P/parenleftig
X∈K1,Y= 1−y0/parenrightig
ℓ(y0,1−y0)
−P/parenleftig
X∈K1,Y=y0/parenrightig
ℓ(1−y0,y0)
+P/parenleftig
X∈B(x0,ϵ)\B(x0,ϵ′)/parenrightig
maxℓ
<P/parenleftig
X∈K1,Y= 1−y0/parenrightig
ℓ(y0,1−y0) +∆
2
Using (13), we can lower bound the right hand side of (14) as follows:
1
r0Ef(X,Y ) 1g(X,Y)≤qr0=1
r0Ef(X,Y )/bracketleftbig
1X∈K1+ 1X∈K2+ 1X∈B(x0,ϵ)\B(x0,ϵ′)/bracketrightbig
1g(X,Y)≤qr0
≥1
r0P/parenleftig
X∈K1,Y= 1−y0/parenrightig
ℓ(y0,1−y0)
−1
r0P/parenleftig
X∈B(x0,ϵ)\B(x0,ϵ′)/parenrightig
maxℓ
>1
r0/bracketleftig
P/parenleftig
X∈K1,Y= 1−y0/parenrightig
ℓ(y0,1−y0)−∆
2/bracketrightig
>Ef(X,Y )
+/bracketleftbig1
r0−1/bracketrightbig
P/parenleftig
X∈K1,Y= 1−y0/parenrightig
ℓ(y0,1−y0)
−1
2/bracketleftbig1
r0+ 1/bracketrightbig
∆
>Ef(X,Y ),
where the last line comes from the definition of ∆. □
29Published in Transactions on Machine Learning Research (11/2023)
A.4 Proof of Theorem 2
DenotePBthe set of probability distributions on X×{ 0,1}, such that the marginal distribution on the
input space is continuous (absolutely continuous with respect to the Lebesgue measure on X) and for which
pπ:x∝⇕⊣√∫⊔≀→Pπ(Y= 1|X=x)
is upper semi-continuous. For a probability measure π∈PB, denoteπXthe marginal distribution on the
input. Denote γthe function from [0,1]×[0,1]toRdefined by
γ(p,y) =pℓ(y,0) + (1−p)ℓ(y,1).
Finally, denoteFthe set of continuous functions from Xto[0,1]. We recall the two assumptions made on
the loss:
(i) The loss is non-negative and that ℓ(y,y′) = 0if and only if y=y′
(ii) Forp∈[0,1],y∝⇕⊣√∫⊔≀→γ(p,y) =pℓ(y,1) + (1−p)ℓ(y,0)has a unique minimizer, denoted y∗
p∈[0,1], that
is increasing with p.
Lemma 4 Consider a loss ℓthat satisfies (ii). Then, for any p∈[0,1]andδ >0, there exists ϵ >0such
that for any y∈Y= [0,1],
γ(p,y)−γ(p,y∗
p)≤ϵ=⇒ |y−y∗
p|≤δ.
Proof: Considerp∈[0,1]andη > 0. Assume that for any ϵk=1
k+1there exists yk∈Ysuch that
|y−y∗
p|≥δand
pℓ(yk,1) + (1−p)ℓ(yk,0)−pℓ(y∗
p,1)−(1−p)ℓ(y∗
p,0)≤ϵk
SinceYis compact, we can assume that the sequence (yk)kconverges (taking, if needed, a sub-sequence of
the original one). Denote y∞this limit. Since ℓand|·|are continuous, it comes that |y∞−y∗
p|≥δand
pℓ(y∞,1) + (1−p)ℓ(y∞,0)−pℓ(y∗
p,1)−(1−p)ℓ(y∗
p,0) = 0,
contradicting the assumption that y∗
pis unique. □
Lemma 5 Ifψis a measurable map from Xto[0,1], then there exists a sequence of continuous functions
fn∈Fthat converges point-wise to ψ(for the Lebesgue measure)
Proof: This result is a direct consequence of two technical results, the Lusin’s Theorem (Theorem 5 in the
appendix), and the continuous extension of functions from a compact set (Theorem 6 in the appendix). □
Lemma 6 For a distribution π∈PB. defineψ∗
πthe function from Xto[0,1]by
∀x∈X, ψ∗
π(x) =y∗
pπ(x)
is measurable. Besides,
inff∈FEπℓ(f(X),Y) =Eπℓ(ψ∗
π(X),Y)
Proof: The function from [0,1]to[0,1]defined by
p∝⇕⊣√∫⊔≀→argminy∈[0,1]γ(p,y) =y∗
p,
is well defined and increasing from assumption (ii) on the loss. It is, therefore, measurable. Since pπ:
x∝⇕⊣√∫⊔≀→Pπ(Y= 1|X=x)is measurable, we get that ψ∗
πis measurable as the composition of two measurable
functions. For the second point, notice that by definition of ψ∗
π, for anyf∈F,
Eπℓ(f(X),Y) = EπXEπ/bracketleftig
ℓ(f(X),Y)|X/bracketrightig
≥EπXEπ/bracketleftig
ℓ(ψ∗
π(X),Y)|X/bracketrightig
≥Eπℓ(ψ∗
π(X),Y).
30Published in Transactions on Machine Learning Research (11/2023)
Using Lemma 5, we can take a sequence of continuous functions fn∈Fthat converge point-wise to ψ∗
π. We
can conclude using the dominated convergence theorem, leveraging that ℓis bounded. □
Lemma 7 LetAaSBPAwith an adapted score function gthat depends on the labels. Then there exists a
compression level r>0andε>0such that for any f0∈F, the two following statements exclude each other
(i)Eνrℓ(f0(X),Y)−inff∈FEνrℓ(f(X),Y)≤ε
(ii)Eµℓ(f0(X),Y)−inff∈FEµℓ(f(X),Y)≤ε
Proof: Sincegdepends on the labels, we can find x0∈Xin the support of µXsuch thatpµ(x0) =
Pµ(Y= 1|X=x0)∈(0,1)andg(x0,0)̸=g(x0,1). Without loss of generality, we can assume that
g(x0,0)<g(x0,1). Taker∈(0,1)such that
g(x0,0)<qr<g(x0,1)
By continuity of g, we can find a radius η>0such that for any xin the ballBη(x0)of centerx0and radius
η, we have that g(x,0)<qr<g(x,1). Besides, since pµis upper semi-continuous, we can assume that ηis
small enough to ensure that for any x∈Bη(x0),
pµ(x)<1 +pµ(x0)
2<1. (15)
Therefore, recalling that νr=1
rµ|Ar
•Pνr(X∈Bη(x0)) =1
rPµ(X∈Bη(x0),Y= 0)>0andPνr(Y= 1|X∈Bη(x0)) = 0.
•Pµ(X∈Bη(x0))>0andPµ(Y= 1|X∈Bη(x0))>0.
Denote ∆ =Pµ(X∈Bη(x0),Y= 1)>0.Consider the subset Vdefined by
V={x∈Bη(x0)s.t. pµ(x)≥∆
2}
We can derive a lower-bound on µX(V)as follows:
∆ =/integraldisplay
x∈Bη(x0)p(x)µX(dx)
=/integraldisplay
x∈Bη(x0)p(x) 1p(x)<∆
2µX(dx) +/integraldisplay
x∈Bη(x0)p(x) 1p(x)≥∆
2µX(dx)
≤/integraldisplay
x∈Bη(x0)∆
2µX(dx) +/integraldisplay
x∈VµX(dx)
≤∆
2+µX(V).
The last inequality gives that µX(V)≥∆/2>0. Moreover, we can lower-bound νX
r(V)using (15) as follows:
νX
r(V) =νr(V×{0})
=1
rµ(V×{0})
=1
r/integraldisplay
x∈V(1−pµ(x))µX(dx)
≥1−pµ(x0)
2rµX(V)
≥1−pµ(x0)
4r∆
>0.
31Published in Transactions on Machine Learning Research (11/2023)
Therefore, assumptions i) and ii) on the loss give that ψ∗
νr(x) = 0andψ∗
µ(x)≥y∗
∆
2>0for anyx∈V. Using
Lemma 4, take ϵ1>0such that
ℓ(y,0)≤ϵ1=⇒y≤y∗
∆
2
3. (16)
In the following, we will show that there exists ϵ2>0such that for any p≥∆
2,
y≤y∗
∆
2
3=⇒γ(p,y)−γ(p,y∗
p)≥ϵ2 (17)
Otherwise, leveraging the compacity of the sets at hand, we can find two converging sequences pk→p∞≥∆
2
andyk→y∞≤y∗
∆
2
3such that
γ(pk,yk)−min
y′γ(pk,y′)≤1
k+ 1.
Sinceγis uniformly continuous,
p∝⇕⊣√∫⊔≀→min
y′γ(p,y′)
is continuous. Taking the limit it comes that
γ(p∞,y∞)−min
y′γ(p∞,y′) = 0,
and consequently y∞=y∗
p∞. Sincep∞≥∆
2,
y∞=y∗
p∞≥y∗
∆
2>y∗
∆
2
3
reaching a contradiction.
Now, takeϵ1andϵ2satisfying (16) and (17) respectively. Put together, we have that for any p≥∆
2,
γ(0,y)−γ(0,y∗
0)≤ϵ1=⇒γ(p,y)−γ(p,y∗
p)≥ϵ2.
Using the definition of V, it comes that for any function f0andx∈V
γ(0,f0(x))≤ϵ1=⇒γ(pµ(x),f0(x))−γ(pµ(x),ψ∗
µ(x))≥ϵ2 (18)
Letε=rmin(ϵ1,ϵ2)νX
r(V)
4>0. Consider f0∈Fsatisfying
Eνrℓ(f0(X),Y)−inf
f∈FEνrℓ(f(X),Y)≤ε.
We will prove that
Eµℓ(f0(X),Y)−inf
f∈FEµℓ(f(X),Y)>ε
to conclude the proof. Denote Uf0is the subset of Vsuch that for any x∈Uf0,γ(0,f0(x))≤2ε
νXr(V).We get
that
ε≥Eνrℓ(f0(X),Y)−inf
f∈FEνrℓ(f(X),Y)
≥/integraldisplay
X/bracketleftbig
γ(pνr(x),f0(x))−γ(pνr(x),ψ∗
νr(x))/bracketrightbig
νX
r(dx)
≥/integraldisplay
Vγ(0,f0(x))νX
r(dx)
≥2ε
νXr(V)νX
r(V\Uf0)
32Published in Transactions on Machine Learning Research (11/2023)
Hence we get that νX
r(Uf0)≥νX
r(V)
2. Since2ε
νXr(V)≤ϵ1,the right hand side of (18) holds. In other words,
∀x∈Uf0,γ(pµ(x),f0(x))−γ(pµ(x),ψ∗
µ(x))≥ϵ2,
from which we successively obtain
Eµℓ(f0(X),Y)−inf
f∈FEµℓ(f(X),Y) =/integraldisplay
X/bracketleftbig
γ(pµ(x),f0(x))−γ(pµ(x),ψ∗
µ(x))/bracketrightbig
µX(dx)
≥/integraldisplay
U{′/bracketleftbig
γ(pµ(x),f0(x))−γ(pµ(x),ψ∗
µ(x))/bracketrightbig
µX(dx)
≥µX(Uf0)ϵ2
≥µ(Uf0×{0})ϵ2
=r ϵ2νX
r(Uf0)
≥rϵ2νX
r(V)
2
> ε.
□
We can now ready to prove Theorem 2.
Theorem 2. LetAaSBPAwith an adapted score function gthat depends on the labels. If ∪θMθhas
the universal approximation property and the loss satisfies assumptions (i) and (ii), then there exist two
hyper-parameters θ1,θ2∈Θsuch that the algorithm is not valid on Wθ1∪Wθ2.
Proof: Denote ˜Θ = Θ×Θ, and for ˜θ= (θ1,θ2)∈˜Θ,W˜θ=Wθ1∪Wθ2andM˜θ=Mθ1∪Mθ2. We
will leverage Proposition 1 and Lemma 7 show that there exist a compression ratio r∈(0,1)and a hyper-
parameter ˜θsuch that
min
w∈W∗
˜θ(r)L(w)>min
w∈W˜θL(w)
which would conclude the proof.
Using Lemma 7, we can find randϵ >0such that for any continuous function f0∈F, the two following
propositions exclude each other:
(i)Eµℓ(f0(X),Y)−inff∈FEµℓ(f(X),Y)≤ϵ
(ii)Eνrℓ(f0(X),Y)−inff∈FEνrℓ(f(X),Y)≤ϵ
Since∪Mθhas the universal approximation property, and that ψ∗
µandψ∗
νr(defined as in Lemma 6) are
measurable, we consecutively use Lemma 5 and Lemma 2 to find ˜θ= (θ1,θ2)such that
1. There exists ψ1∈Mθ1such that Eµℓ(ψ1(X),Y)−Eµℓ(ψ∗
µ(X),Y)≤ϵ/2
2. There exists ψ2∈Mθ2such that Eνrℓ(ψ2(X),Y)−Eνrℓ(ψ∗
νr(X),Y)≤ϵ/2
Takeψ1,ψ2∈M ˜θtwo such functions. Consider any parameter w∈argminw∈W∗
˜θ(r)L(w).By definition, it
comes that
Eνrℓ(yout(X;w),Y)−Eνrℓ(ψ∗
νr(X),Y)≤Eνrℓ(ψ2,Y)−Eνrℓ(ψ∗
νr(X),Y)
≤ϵ/2
Therefore, since Lemma 6 gives that inff∈FEνrℓ(f(X),Y) =Eνrℓ(ψ∗
νr(X),Y), we can conclude that
Eµℓ(yout(X;w),Y)−inf
f∈FEµℓ(f(X),Y)>ϵ,
33Published in Transactions on Machine Learning Research (11/2023)
from which we deduce that
Eµℓ(yout(X;w),Y)>inf
f∈FEµℓ(f(X),Y) +ϵ
>Eµℓ(ψ1(X),Y) +ϵ/2
≥ min
w′∈W˜θL(w′) +ϵ/2,
which gives the desired result. □
A.5 Proof of the Corollaries 3 and 4
These two corollaries are a straightforward application of Theorem 1 and Theorem 2 as well as the existing
literature on the universal approximation properties of Neural Networks: (Hornik, 1991) and (Kidger and
Lyons, 2020). We give the proof of the result for wide neural networks. Consider any number of hidden
layersH≥1fixed. Denote θ= (K,R)∈N×R= Θ. (Hornik, 1991) implies that ∪(K,R)∈ΘFFNNσ
H,K(R)
has the universal approximation property. Theorem 1 states that one can find a θ0= (K0,R0)such that the
SBPAis not consistent on FFNNσ
H,K 0(R0). Now from the definition of consistency, we get that if a SBPAis
not consistent on M, then it is not consistent on any superset M′that containsM. Therefore, we get the
non-consistency result by noticing that
FFNNσ
H,K 0(R0)⊂FFNNσ
H,K(R),
for anyK≥K0andR≥R0. Similarly, Theorem 2 states that there exist θ1= (K1,R1)andθ2= (K2,R2)
such that the model is not valid for any class of model such that
FFNNσ
H,K 1(R1)∪FFNNσ
H,K 2(R2)⊂M.
We can conclude noticing that FFNNσ
H,K(R)satisfies this condition if K≥max(K1,K2)andR≥
max(R1,R2).
A.6 Proof of Theorem 3
ForK∈N∗, denotePK
Cthe set of generating processes for K-classes classification problems, for which the
inputXis a continuous random variable (the marginal of the input is dominated by the Lebesgue measure),
and the output Ycan take one of Kvalues inY(the same for all π∈PK
C). Similarly, denote PR, the
set of generating processes for regression problems for which both the input and output distributions are
continuous. LetPbe any set of generating processes introduced previously for regression or classification
(eitherP=PK
Cfor someK, orP=PR).
Assume that there exist (x1,y1),(x2,y2)∈Dsuch that
argminw∈Wθℓ(yout(x1;w),y1)∩argminw∈Wθℓ(yout(x2;w),y2) =∅. (H1)
For any SBPAalgorithmAwith adapted criterion, we will show that there exists a generating process µ∈P
for whichAis not valid. More precisely, we will show that there exists r0∈(0,1)such that for any
compression ratio r≤r0, there exists a generating process µ∈Pfor whichAis not valid. To do so, we
leverage Corollary 2 and prove that for any r≤r0, there exists µ∈P, for whichW∗
θ(νr)∩W∗
θ(µ)̸=∅, i.e.
∃r0∈(0,1),∀r≤r0,∃µ∈Ps.t.∀w∗
r∈W∗
θ(νr),Lµ(w∗
r)>min
w∈WθLµ(w) (19)
We bring to the reader’s attention that νr=1
rµ|Ar=νr(µ)depends on µ, and so does the acceptance region
Ar=Ar(µ).
The rigorous proof of Theorem 3 requires careful manipulations of different quantities, but the idea is rather
simple. Fig. 13 illustrates the main idea of the proof. We construct a distribution µwith the majority of
the probability mass concentrated around a point where the value of gis not minimal.
34Published in Transactions on Machine Learning Research (11/2023)
𝑤𝑧
𝑧2 𝑧1
𝑤𝑆𝑒𝑙𝑒𝑐𝑡𝑧1𝑎𝑛𝑑𝑧2𝑠𝑢𝑐ℎ𝑡ℎ𝑎𝑡𝑔𝑧1<𝑔(𝑧2)
𝑃𝑢𝑡𝑡ℎ𝑒𝑏𝑢𝑙𝑘𝑜𝑓𝜇𝑎𝑟𝑜𝑢𝑛𝑑𝑧2𝑃𝑢𝑡𝑎𝑠𝑚𝑎𝑙𝑙𝑚𝑎𝑠𝑠𝑜𝑓𝜇𝑎𝑟𝑜𝑢𝑛𝑑𝑧1
Figure 13: Graphical sketch of the proof of Theorem 3. The surface represents the loss function f(z,w) =
ℓ(yout(x),y)in 2D, where z= (x,y).
We start by introducing further notations. For z= (x,y)∈D, andw∈Wθ, we denote by fthe function
defined by f(z,w) =ℓ(yout(x),y).We will use the generic notation ℓ2to refer to the Euclidean norm on
the appropriate space. We denote B(X,ρ)theℓ2ball with center Xand radius ρ. IfXis a set, then
B(X,ρ) =/uniontext
X∈XB(X,ρ).ForS⊂D, we denote argminwf(S,w) =/uniontext
X∈Sargminwf(X,w).
Notice that fis continuous on D×Wθ.Besides, the set data generating processes Pis i) convex and ii)
satisfies for all X0∈D,δ>0andγ <1, there exists a probability measure µ∈Psuch that
µ(B(X0,δ))>γ,
These conditions play a central role in the construction of a generating process for which the pruning
algorithm is not valid. In fact, the non-validity proof applies to any set of generating processes satisfying
conditions i) and ii). To ease the reading of the proof, we break it into multiple steps that we list hereafter.
Steps of the proof:
1. For allz0∈D, the setWz0=argminwf(z0,w)is compact (and non empty).
2. For allz0∈D,δ> 0, there exists ρ0>0such that for all ρ≤ρ0,
argminwf(B(z0,ρ),w)⊂B(Wz0,δ)
3. Under assumption (H1), there exists z1,z2∈Dsuch that i) g(X1)<g(X2)and ii)Wz1∩Wz2=∅
4. Forz1,z2as in 3, denoteW1=Wz1andW2=Wz2. There exists δ,ρ0>0such that for any ρ≤ρ0
andw1∈B(W1,δ),andw∗
2∈W 2
inf
z∈B(z2,ρ)f(z,w1)>sup
z∈B(z2,ρ)f(z,w∗
2)
5. For any r∈(0,1), there exits a generating process µ∈Psuch that any minimizer of the pruned
programw∗
r∈W∗
θ(νr)necessarily satisfies w∗
r∈B(W1,δ)and such that µ(B(z2,ρ))≥1−2rfor a
givenρ≤ρ0.
6.∃r0>0such that∀r≤r0,∃µ∈Psuch thatLµ(w∗
r)>minw∈WθLµ(w)for anyw∗
r∈W∗
θ(νr)
35Published in Transactions on Machine Learning Research (11/2023)
Proof: Result 1: LetWz0=argminwf(z0,w)⊂Wθ. SinceWθis compact and functions fz0:w∝⇕⊣√∫⊔≀→
f(z0,w)is continuous, it comes that Wz0is well defined, non-empty and closed (as the inverse image of a
closed set). Hence it is compact.
Result 2: Letz0∈Dandδ >0. We will prove the result by contradiction. Suppose that for any ρ >0,
there exists w∈argminw′f(B(z0,ρ),w′)such thatd(w,Wz0)≥δ.
It is well known that since fis continuous and that Wθis compact, the function
z∝⇕⊣√∫⊔≀→min
w∈Wθf(z,w),
is continuous. Therefore, for any k>0, we can find ρk>0such that for any z∈B(z0,ρk),
|inf
wf(z,w)−inf
wf(z0,w)|<1
k
For everyk > 0, letwk,zksuch thatzk∈B(z0,ρk),wk∈argminwf(zk,w)andd(wk,Wz0)≥δ. By
definition, limzk=z0. SinceWθis compact, we can assume that wkconverges to w∞without loss of
generality (taking a sub-sequence of the original one). Now, notice that
|f(zk,wk)−inf
wf(z0,w)|=|inf
wf(zk,w)−inf
wf(z0,w)|<1/k,
therefore, since fis continuous, f(z0,w∞) = infwf(z0,w)and sow∞∈Wz0, which contradicts the fact that
d(wk,w∞)≥δfor allk. Hence, we can find ρ>0such that for all argminwf(B(z0,ρ))⊂B(Wz0,δ).
Result 3: Letz1,z2as in (H1) such that g(z1) =g(z2). Sincedis continuous, and W1=Wz1andW2=Wz2
are compact, d(W1×W 2)is also compact. Hence, there exists δ>0such that
min
w1∈W1, w2∈W2d(w1,w2)≥δ.
Using the previous result, let ρsuch that argminwf(B(z1,ρ),w)⊂B(W1,δ/2),The triangular inequality
yieldsargminwf(B(z1,ρ),w)∩W2=∅. SincegisadaptedandB(z1,ρ)hasstrictlypositiveLebesguemeasure,
we can find z′
1∈B(z1,ρ)such thatg(z′
1)̸=g(z1). Therefore, the points z′
1,z2satisfy the requirements.
Result 4: SinceW1is compact and fz2is continuous, f(z2,W1)is compact, and since W1∩W 2=∅,
minf(z2,W1)>f(z2,w∗
2) = min
w∈Wθf(z2,w),
for anyw∗
2∈W 2. Denote ∆ = minf(z2,W1)−minwf(z2,w)>0.
Sincefis continuous on the compact space D×Wθ, it is uniformly continuous. We can hence take δ >0
such that for z,z′∈Dandw,w′∈Wθsuch that
∥z−z′∥≤δ,∥w−w′∥≤δ=⇒ |f(z,w)−f(z′,w′)|≤∆/3.
Using Result 2, we can find ρ0>0such that for all ρ≤ρ0,
argminwf(B(z1,ρ),w)⊂argminwf(B(z1,ρ0),w)⊂B(W1,δ)
We can assume without loss of generality that ρ0≤2δ. Letw1∈B(W1,δ). For anyw∗
2∈W 2, we conclude
that
min
z∈B(z2,ρ)f(z,w1)≥minf(z2,W1)−∆/3>f(z2,w∗
2) + ∆/3≥sup
z∈B(z2,ρ)f(z,w∗
2).
Result 5: Letρ0defined previously, k >1andr∈(0,1). Using the uniform continuity of f, we construct
0<ρk≤ρ0such that
∀w∈P,∀z,z′∈D,d(z,z′)≤ρk=⇒ |f(z,w)−f(z′,w)|≤1/k.
36Published in Transactions on Machine Learning Research (11/2023)
Considerµk∈Psuch thatµk/parenleftbig
B(z1,ρk)/parenrightbig
≥randµk/parenleftbig
B(z2,ρk)/parenrightbig
≥1−r−r/k. Letνk
r=νr(µk). It comes
thatνk
r(B(z1,ρk))≥1−1
k.Using a proof by contradiction, we will show that there exists k>1such that
argminwEνkrf(z,w)⊂B(W1,δ).
Suppose that the result doesn’t hold, we can define a sequence of minimizers wksuch that wk∈
argminwEνkrf(z,w)andd(wk,W1)>δ.DenoteM= supz,wf(z,w).Take anyw∗
1∈W 1,
Eνkrf(z,wk)≤Eνkrf(z,w∗
1) (20)
≤/parenleftbigg
f(z1,w∗
1) +1
k/parenrightbigg
νk(B(z1,ρk)) +M/parenleftbig
1−νk(B(z1,ρk))/parenrightbig
(21)
≤/parenleftbigg
f(z1,w∗
1) +1
k/parenrightbigg
+M
k(22)
≤/parenleftbigg
min
wf(z1,w) +1
k/parenrightbigg
+M
k(23)
Similarly, we have that
Eνk
rf(z,wk)≥/parenleftbigg
f(z1,wk)−1
k/parenrightbigg
νk(B(z1,ρk)) (24)
≥/parenleftbigg
f(z1,wk)−1
k/parenrightbigg
(1−1/k) (25)
≥/parenleftbigg
min
wf(z1,w)−1
k/parenrightbigg
(1−1/k). (26)
Putting the two inequalities together, we find
/parenleftbigg
min
wf(z1,w)−1
k/parenrightbigg
(1−1/k)≤/parenleftbigg
f(z1,wk)−1
k/parenrightbigg
(1−1/k)≤/parenleftbigg
min
wf(z1,w) +1
k/parenrightbigg
+M
k
SinceWθis compact, we can assume that limkwk=w∞∈Wθ(taking a sub-sequence of the original one).
And sincefz1is continuous, we can deduce that f(z1,w∞) = minwf(z1,w), which contradict the fact that
d(wk,w∞)>δfor allk.
Result 6: Letr∈(0,1)andδ,ρ0,ρ,µas in the previous results. Let wr∈W∗
θ(νr)From Result 5, we have
thatwr∈B(W1,δ).Forw∗
2∈W 2, Result 5 implies that
minz∈B(z2,ρ)f(z,wr)−supz∈B(z2,ρ)f(z,w∗
2)
≥minz∈B(z2,ρ0)f(z,wr)−supz∈B(z2,ρ0)f(z,w∗
2) = ∆
>0
Therefore,
Eµf(z,wr)≥ min
z∈B(z2,ρ0)f(z,w1)×µ(B(z2,ρr)) (27)
≥/parenleftigg
sup
z∈B(z2,ρ0)f(z,w∗
2) + ∆/parenrightigg
µ(B(z2,ρ)) (28)
≥Eµf(z,w∗
2) + ∆(1−2r)−2rM (29)
≥min
wEµf(z,w) + ∆(1−2r)−2rM. (30)
Therefore,
Lµ(wr)−min
w∈WθLµ(w)≥∆(1−2r)−2rM,
which is strictly positive for r<∆
2(M+∆)=r0 □
37Published in Transactions on Machine Learning Research (11/2023)
A.7 Proof of Proposition 4
Proposition 4. [Consistency of Exact Calibration+ SBPA]
LetAbe a SBPAalgorithm. Using the Exact Calibration protocol with signal proportion α, the calibrated
algorithm ¯Ais consistent if 1−α > 0, i.e. the exploration budget is not null. Besides, under the same
assumption 1−α > 0, the calibrated loss is an unbiased estimator of the generalization loss at any finite
sample size n>0,
∀w∈Wθ,∀r∈(0,1),EL¯A,r,α
n(w) =L(w).
Proof: Considerα<1. Letf(zi,w) =ℓ(yout(xi,w),yi), andpe=(1−α)r
1−αr. Fori∈{1,...,n}, consider the
independent Bernoulli random variables bi∼B(pe). Notice that
L¯A,r,α
n(w) =1
nn/summationdisplay
i=1/parenleftbigg
1zi∈A(Dn,αr)+bi
ps1zi̸∈A(Dn,αr)/parenrightbigg
f(zi,w),
which gives
EL¯A,r,α
n(w) =EDnE/bracketleftig
L¯A,r,α
n(w)|Dn/bracketrightig
=EDnLn(w) =L(w).
Define the random variables
Yn,i=/parenleftbigg
1zi∈A(Dn,αr)+bi
ps1zi̸∈A(Dn,αr)−1/parenrightbigg
f(zi,w),
LetFn,i=σ({Yn,j,j̸=i})be theσ-algebra generated by the random variables {Yn,j,j̸=i}. Let us now
show that the conditions of Theorem 7 hold with this choice of Yn,i.
•Letn≥1andi∈{1,...,n}. Similarly to the previous computation, we get that E[Yn,i|Fn,i] = 0.
•Using the compactness assumption on the space WθandD, we trivially have that supi,nEY4
n,i<∞.
•Trivially, for each n≥1, the variables{Yn,i}1≤i≤nare identically distributed.
Using Theorem 7 and the standard strong law of large numbers, we have that n−1/summationtextn
i=1Yn,i→0almost
surely, and n−1/summationtextn
i=1f(zi,w)→Eµf(z,w)almost surely, which concludes the proof for the consistency.
□
B Technical results
Theorem 4 (Universal Approximation Theorem, (Hornik, 1991)) LetC(X,Y )denote the set of
continuous functions from XtoY. Letϕ∈C(R,R). Then,ϕis not polynomial if and only if for ev-
eryn,m∈N, compactK⊂Rn,f∈C(K,Rm),ϵ >0, there exist k∈N,A∈Rk×n,b∈Rk,C∈Rm×k
such that
sup
x∈K∥f(x)−yout(x)∥≤ϵ,
whereyout(x) =C⊤σ(Ax+b).
Lemma 8 (Urysohn’s lemma, (Arkhangel’ski ˇı, 2001)) For any two disjoint closed sets AandBof a
topological space X, there exists a real-valued function f, continuous at all points, taking the value 0at all
points ofA, the value 1at all points of B. Moreover, for all x∈X,0≤f(x)≤1.
Theorem 5 (Lusin’s Theorem) IfXis a topological measure space endowed with a regular measure µ, if
Yis second-countable and ψ:X→Yis measurable, then for every ϵ>0there exists a compact set K⊂X
such thatµ(X\K)<ϵand the restriction of ψtoKis continuous.
38Published in Transactions on Machine Learning Research (11/2023)
Theorem 6 (Continuous extension of functions from a compact, (Deimling, 2010)) LetA⊂Rd
be compact and f:A→Rbe a continuous function. Then there exists a continuous extension ˜f:Rd→R
such thatf(x) =˜f(x)for allx∈A.
B.1 A generalized Law of Large Numbers
There are many extensions of the strong law of large numbers to the case where the random variables have
some form of dependence. We prove a strong law of large numbers for specific sequences of arrays that satisfy
a conditional zero-mean property.
Theorem 7 Let{Yn,i,1≤i≤n,n≥1}be a triangular array of random variables satisfying the following
conditions:
•For alln≥1andi∈[n],E[Yn,i|Fn,i] = 0, whereFn,i=σ({Yn,j,j̸=i}), i.e. theσ-algebra
generated by all the random variables in row nother thanYn,i.
•For alln≥1, the random variables (Yn,i)1≤i≤nare identically distributed (but not necessarily
independent).
•supn,iEY4
n,i<∞.
Then, we have that
1
nn/summationdisplay
i=1Yn,i→0, a.s.
Proof: The proof uses similar techniques to the standard proof of the strong law of large numbers, with
some key differences, notably in the use of the Chebychev inequality to upper-bound the fourth moment of
the mean. Let Sn=/summationtextn
i=1Yn,i. We want to show that P(limn→∞Sn/n= 0) = 1 . This is equivalent to
showing that for all ϵ>0,P(Sn>nϵfor infinitely many n) = 0. This event is nothing but the limsup of the
eventsAn={Sn>nϵ}. Hence, we can use Borel-Cantelli to conclude if we can show that/summationtext
nP(An)<∞.
Letϵ >0. Using Chebychev inequality with degree 4, we have that P(An)≤(ϵn)−4ES4
n. It remains to
bound ES4
nto conclude. We have that ES4
n=E/summationtext
1≤i,j,k,l≤nYn,iYn,jYn,kYn,l. Using the first condition
(zero-mean conditional distribution), all the terms of the form Yn,iYn,jYn,kYn,l,Y2
n,iYn,jYn,k, andY3
n,iYn,lfor
i̸=j̸=k̸=lvanish and we end up with ES4
n=nEY4
n,1+ 3n(n−1)EY2
n,1Y2
n,2, where we have used the fact
that the number of terms of the form Y2
n,iY2
n,jin the sum is given by/parenleftbign
2/parenrightbig
×/parenleftbig4
2/parenrightbig
=n(n−1)
2×6 = 3n(n−1).
Using the last condition of the fourth moment, we obtain that there exists a constant M > 0such that
ES4
n< Cn2. Using Chebychev inequality, we get that P(An)≤ϵ−4n−2, and thus/summationtext
nP(An)<∞. We
conclude using the Borel-Cantelli lemma. □
C Additional Theoretical Results
Convolutional neural networks: For an activation function σ, a real number R> 0, and integers J≥1
ands≥2denoteCNNσ
J,s(R)the set of convolutional neural networks with Jfilters of length s, with all
weights and biases in [−R,R]. More precisely, for a filter mask w= (w0,..,ws−1), and a vector x∈Rd, the
results of the convolution of wandx, denotedw∗xis a vector in Rd+sdefined by (w∗x)i=i/summationtext
k=i−s+1wi−kxk.
A network from CNNσ
J(R)is then defined recursively for x∈X:
•h(0)(x) =x
•Forj∈[1 :J],h(j)(x) =σ/parenleftbig
w(j)∗h(j−1)(x) +b(j)/parenrightbig
, where the filters and biases w(j)andb(j)are in
[−R,R]
•yout(x) =cTh(J)(x), where the vector chas entries in [−R,R]
39Published in Transactions on Machine Learning Research (11/2023)
Corollary 5 (Convolutional Neural Networks (Zhou, 2020)) Letσbe the ReLU activation function.
Consider a filter length s∈[2,dx].For any SBPAwith adapted score function, there exists a number of filters
J0and a radius R0such that the algorithm is not consistent on CNNσ
J,s(R), for anyJ≥J0andR≥R0.
Besides, if the algorithm depends on the labels, then it is also not valid on CNNσ
J,s(R), for anyJ≥J′
0and
R≥R′
0.
D Experimental details
Dataset CIFAR10 CIFAR100
Architecture ResNet18 ResNet34
Methods GraNd(10), Uncertainty ,
DeepFoolGraNd(10), Uncertainty ,
DeepFool
Selection LR 0.1 0.1
Training LR 0.1 0.1
Selection Epochs 1, 5 1, 5
Nb of exps 3 3
Training Epochs 160 160
Optimizer SGD SGD
Batch Size 128 128
The table above contains the different hyper-parameter we used to run the experiments. GraNd(10) refers
to using the GraNdmethod with 10different seeds (averaging over 10 different initializations). Selection LR
refers to the learning rate used for the coreset selection. The training LR follwos a cosine annealing schedule
given by the following:
ηt=ηmin+1
2(ηmax−ηmin)/parenleftbigg
1 + cos/parenleftbiggTcur
Tmaxπ/parenrightbigg/parenrightbigg
,
whereTcuris the current epoch, Tmaxis the total number of epochs, and ηmax= 0.1andηmin= 10−4.
These are the same hyper-parameter choices used by Guo, B. Zhao, and Bai, 2022.
D.1 MLP for Scaling laws experiments
We consider an MLP given by
y1(x) =ϕ(W1xin+b1),
y2(x) =ϕ(W2y1(x) +b2),
yout(x) =Wouty2(x) +bout,
wherexin∈R1000is the input, W1∈R128×1000,W2∈R128×128,Wout∈R2×128are the weight matrices and
b1,b2,boutare the bias vectors.
40