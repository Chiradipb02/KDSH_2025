Published in Transactions on Machine Learning Research (09/2024)
Rethinking Teacher-Student Curriculum Learning through
the Cooperative Mechanics of Experience
Manfred Diaz diazcabm@mila.quebec
Mila, University of Montreal
Liam Paull paulll@mila.quebec
Mila, University of Montreal
Canada CIFAR AI Chair
Andrea Tacchetti attachet@google.com
Google DeepMind
Reviewed on OpenReview: https://openreview.net/forum?id=qWh82br6KT
Abstract
Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework that draws inspi-
ration from human cultural transmission and learning. It involves a teacher algorithm shaping the
learning process of a learner algorithm by exposing it to controlled experiences. Despite its success,
understanding the conditions under which TSCL is effective remains challenging. In this paper,
we propose a data-centric perspective to analyze the underlying mechanics of the teacher-student
interactions in TSCL. We leverage cooperative game theory to describe how the composition of
the set of experiences presented by the teacher to the learner, as well as their order, influences the
performance of the curriculum that is found by TSCL approaches. To do so, we demonstrate that
for every TSCL problem, an equivalent cooperative game exists, and several key components of
the TSCL framework can be reinterpreted using game-theoretic principles. Through experiments
covering supervised learning, reinforcement learning, and classical games, we estimate the coopera-
tive values of experiences and use value-proportional curriculum mechanisms to construct curricula,
even in cases where TSCL struggles. The framework and experimental setup we present in this work
represents a novel foundation for a deeper exploration of TSCL, shedding light on its underlying
mechanisms and providing insights into its broader applicability in machine learning.
1 Introduction
Controlling the sequence of tasks that a learning algorithm is exposed to through curriculum has been shown to
potentially enhance learning efficiency (Elman, 1993; Krueger & Dayan, 2009; Bengio et al., 2009). One widely used
curriculum framework, known as Teacher-Student Curriculum Learning ( TSCL ) (Graves et al., 2017; Matiisen et al.,
2020), specifically gives a teacher algorithm the ability to control this sequence. While it is commonly understood
that presenting tasks with increasing difficulty can improve learning, the underlying dynamics and structure of teacher-
student interaction in this context are still relatively unexplored. Very few works have attempted to understand when , and
howTSCL works (Lee et al., 2021; Wu et al., 2020) while most have focused on providing algorithmic improvements to
the problem (Portelas et al., 2019a; Turchetta et al., 2020; Liu et al., 2020; Feng et al., 2021). In this paper, we propose
a novel data-centric perspective (Ng, 2021) to understand and analyze TSCL algorithms.
We begin by formalizing a general notion of units of experience to describe the teacher algorithm’s control objects
(consumed by the learner). Subsequently, our approach draws inspiration from work on feature attribution (Patel et al.,
2021), data valuation (Ghorbani & Zou, 2019; Yan & Procaccia, 2021) and explainability (Lundberg & Lee, 2017), and
leverages tools from cooperative game theory (V on Neumann & Morgenstern, 1944; Shapley, 1952) to analyze how the
1Published in Transactions on Machine Learning Research (09/2024)
compositions of these units impact teacher-student interactions. We show that, for every TSCL problem, there exists
an equivalent cooperative game where units of experience are players and teacher-student interactions approximate a
sequential coalition formation process (Sec. 4). As a result, the learning progression objective (Schmidhuber, 1991;
Oudeyer et al., 2007; Graves et al., 2017) and the teacher bandit policy (Gittins, 1979; Matiisen et al., 2020), two essential
components of TSCL , have alternative interpretations as an approximation of player (unit) marginal contribution (Weber,
1988) and a fair allocation mechanism, respectively (Sec. 4.2 & 4.3). Furthermore, because the order matters in the
case of curriculum learning (Krueger & Dayan, 2009; Bengio et al., 2009), traditional cooperative game-theoretic
arguments produce unintuitive results (Nowak & Radzik, 1994). Thus, we leverage generalized cooperative games
and their solution concepts (Nowak & Radzik, 1994; Sanchez & Bergantiños, 1997) to overcome these limitations and
formally extend these data-centric game-theoretic formulations to the curriculum learning setting.
To demonstrate the predictive power and range of problems where this game-theoretic and data-centric interpretation
ofTSCL applies, we build an experimental setting that evaluates the prospect of cooperation among units of experience
in problems spanning supervised learning ( SL), reinforcement learning ( RL), and classical games (Sec. 5). These
experiments simulate ordered and unordered coalition formation processes and approximate the cooperative games
we developed to describe TSCL . For every problem, we estimate units a priori value (e.g., Shapley or Nowak &
Radzik values) and demonstrate that these a priori values, although expensive to compute (Deng & Papadimitriou,
1994), are useful proxies to find curricula. To this end, we design unordered and ordered value-proportional curriculum
mechanisms inspired by value-proportional allocations (Bachrach et al., 2020). In most settings, the unordered
mechanism fails to find a reasonable curriculum, demonstrating the unsuitability of traditional game-theoretic tools
for the TSCL problem. However, the ordered mechanism consistently finds an optimal or near-optimal ordering (i.e.,
a curriculum) even when TSCL fails (Sec. 5.2). To understand what impacts the ability of TSCL in those settings,
we leverage another cooperative game-theoretic tool, namely, measures of interactions (Grabisch & Roubens, 1999;
Procaccia et al., 2014), and in particular the Value of a Player to other Player ( vPoP ) (Hausken & Mohr, 2001), to
quantify positive, neutral, or negative pairwise interactions among units. We show that in settings with considerable
unit interference, as characterized by their negative pairwise interactions, TSCL cannot produce useful curricula.
2 Preliminaries
2.1 Cooperative Game Theory
Cooperative Games. Cooperative games model problems where players interact to maximize collective gain (Roth,
1988). In a (traditional) cooperative game in characteristic function form among a set of players U, denoted by
G=⟨U,v⟩, the characteristic function v: 2U→Rassociates to each coalition C∈2U, belonging to the powerset 2U, a
real number that represents the benefits produced by the players in Cacting jointly. In a cooperative game, a solution
concept represents a mechanism that produces allocation vectors ϕ∈R|U|(Shubik, 1981). Particularly, Shapley’s
value (Shapley, 1952) allocates to each player u∈Uits average marginal contribution v(C+u)−v(C)to coalitions
C⊆U, where u∈U−C
ϕ(u) =/summationdisplay
C:u̸∈C|C|!(|U|−|C|−1)!
|U|![v(C+u)−v(C)] (1)
and uniquely satisfies the axioms of efficiency ,null-player ,symmetry , and linearity , which are generally considered to
be properties of a fair allocation mechanism (van den Brink & van der Laan, 1998).
Generalized Cooperative Games. When the order in which players join determines coalitional worth, traditional
cooperative games and their solution concepts (e.g., Shapley’s value) may produce unintuitive allocations (Nowak
& Radzik, 1994). In these games, the generalized characteristic function v:P(2U)→Rassigns to every ordered
coalition C∈P(2U)in the powerset of permutations P(2U)its worth if members join in the permutation order. Nowak
& Radzik (1994) and Sanchez & Bergantiños (1997) extended Shapley’s work and proposed solution concepts for these
generalized cooperative games. We focus on the former due to its intuitive formulation
ϕNR(u) =1
|U|!/summationdisplay
C∈P(2U)
C:u̸∈C[v(C:u)−v(C)] (2)
2Published in Transactions on Machine Learning Research (09/2024)
that averages, for all ordered coalitions C∈P(2U)where the unit u∈Uis appended last, its marginal contribution to
the newly formed ordered coalition C:u.
Measures of Interactions. In a cooperative game, a measure of interaction (Grabisch & Roubens, 1999; Procaccia
et al., 2014) computes players’ influences on other players’ outcomes. In particular, we leverage the value of a player
to another player (vPoP) (Hausken & Mohr, 2001). For the games above, vPoP constructs a matrix whose entries
ϕ(ui,uj)∈Rmeasure the influence player uiexerts over player uj. It measures how the Shapley value of a unit
changes in the absence of another. More precisely,
ϕ(ui,uj) =/summationdisplay
C⊆U
ui,uj∈C(|U|−|C|)(|C|−1)!
|U|![ϕ(uj,C)−ϕ(uj,C−ui)] (3)
whereϕ(uj,C)is the Shapley value of unit uj(Eq. 1) in the cooperative game restricted to players in C. This matrix
marginalϕ(ui) =/summationtext
jϕ(ui,uj)corresponds to each player’s Shapley value. We extend vPoP to games in generalized
characteristic function form by applying Eq. 3 mutatis mutandis using Nowak & Radzik (1994) value to provide an
ordered pairwise interaction metric ϕNR(ui,uj).
2.2 Bandit Algorithms
Multi-armed bandit algorithms provide a solution to problems of decision-making under uncertainty (Gittins, 1979;
Lattimore & Szepesvári, 2020) where, at each interaction, a decision must be made about which arm u∈Umust be
pulled. We are particularly interested in action-value-based algorithms that maintain empirical value estimates qk(u)
computed as
qk(u)≈1
Nu
kk−1/summationdisplay
i=1r(ui)Iui=u (4)
and that estimate the average reward received by the algorithm in the iterations Nu
k≤kwhere the u-arm has been
pulled. Bandit algorithms, like the ones Graves et al. (2017) and Matiisen et al. (2020) use in their work, transform the
estimated average contributions into arms interactions by deriving from estimated values a Boltzmann policy τk∈∆(U)
such that the probability of interaction is proportional to the value estimates:
τk(u)∝B(qk(u)) =eqk(u)
T
/summationtext
u′eqk(u′)
T(5)
More sophisticated approaches (e.g., the EXP3(Auer et al., 2003) used in our experiments) account for other factors,
like recency, bias, stochasticity, or non-stationarity (Lattimore & Szepesvári, 2020).
3 Experience to Control
The TSCL framework commonly operates under the assumption that tasks presented to a learning algorithm can
influence its learning dynamics. Modern iterative learning algorithms process tasks in discrete units. For instance,
SLandRLalgorithms operate over instances and transitions, respectively. But also, collections of these elementary
units, such as batches or episodes, datasets or environments , or more generally benchmarks or environment suites,
describe a hierarchy of aggregations of experience. Henceforth, we utilize the term unit of experience for referring to
any collection of discrete units that a teacher algorithm can use to control the dynamics of the learner algorithm.
Example 3.1. For an analysis, we may define a unit of experience as the set of instances of class in a SLclassification
problem. For example, in the MNIST dataset (LeCun & Cortes, 2010), there may be ten units of experience , namely,
classes ZERO,ONE,TWO,..., NINE .
Theunits of experience abstraction indistinctly applies to supervised or reinforcement learning problems. On either
paradigm, any iterative learning algorithm is a controllable system whose control inputs are units of experience.
3Published in Transactions on Machine Learning Research (09/2024)
Algorithm 1 Generalized Teacher-Student Curriculum Learning.
1:procedure GENTSCL
2: inputs policy:π0, algorithm:L, units: U, metric:J
3: teacher :τ0∈∆(U),targets :¯U, budget:K
4: fork= 1...K do
5: uk∼τk(u)
6:πk∼L(πk−1,uk)
7:rk←J (πk,¯U)−J(πk−1,¯U)
8:τk+1←UPDATE RULE(τk,uk,rk)
9: end for
10: output:πK
11:end procedure
Example 3.2. There are four control inputs in mini-batch gradient descent (Goodfellow et al., 2016): the mini-batch
{x1,...,xB}, the loss function ℓ, the parameters θ, and the learning rate ηsuch that:
θk+1=θk−η∇θkB/summationdisplay
i=1ℓ(θk,xi)
ATSCL -style algorithm, as presented in Alg. 1, solves a data-centric control problem. The learner algorithm
L(πk−1,uk)is ablack-box system (line 6) controlled by a teacher algorithm through units drawn with probability
uk∼τk(u). The learner output, at each iteration k, is policy or model πkwhose performance is measured by a metric
functionJthat quantifies the model’s performance on a set of evaluation units ¯U. The teacher aims to maximize the
cumulative learning progression reward (line 7). For the teacher ’sUPDATE RULE, we focus on multi-armed bandit
learning (see Sec. 2.2). We adopt a data-centric perspective to perform a systematic investigation of its components.
4 The Cooperative Mechanics of Experience
The ideal teacher-student interaction mechanics assume that the learner monotonically increases its performance on the
target task. We conjecture that a prerequisite for this idealistic curriculum learning dynamics (Matiisen et al., 2020) to
occur within TSCL -style algorithms is that experience (or data) presented to the learner should not interfere with each
other. In other words, units of experience should interact cooperatively. From a game-theoretic perspective, we explain
how these cooperative mechanics may emerge among units by examining the history of teacher-student interactions, the
reward function, and the bandit selection policy.
4.1 The Mechanics of Coalition Formation
We establish a cooperative game where each unit of experience u∈Uis a player. Next, we interpret the history of
k≤Kteacher-student interactions Hk={u1,..., uk}through their empirical frequencies pk(u)∈∆Uwhich form
unit vectors that lie in the |U|-probability simplex ∆(U). The effective support (i.e., non-zero probabilities) determines
an unordered coalition (i.e., a set) Ck⊆U(see Faigle (2022), Chapter 8), formed by the units presented to the learner
up to interaction k≤K. We study this interpretation through a cooperative game in characteristic function form
(Sec. 2.1).
Example 4.1. (Example 3.1 cont’d) In the class-as-unit equivalence on MNIST , an unordered training coalition,
e.g., the two-unit coalition C={ZERO,NINE}, describes teacher-student interactions limited to instances from those
classes .
Next, we note that the outcome of a coalition’s work is the policy or model πk. Thus, estimating the performance of
the policyπkthrough the metric function Jis akin to approximating the characteristic function v(Ck)(Alg. 1, line
7). Moreover, these approximations are conditioned on an evaluation (or target) unit ¯u∈¯U. We model the target-task
andmultiple-task settings (Graves et al., 2017) where units of experience should increase learner performance on an
evaluation unit (e.g., a task, or an environment) or on multiple evaluation units (e.g., a set of tasks or environments).
4Published in Transactions on Machine Learning Research (09/2024)
TSCL Cooperative Game Learning Example
Units Players u1,u2,u3
Sequence Coalition (u2,u1)
Policy Value Coalitional Worth J(π2) v(u2,u1)
Learning Progression Marginal Contribution J(π2)−J(π1)v(u2,u1)−v(u2)
Unit Value Player Value qk(u1) ϕ(u1)
Interactions Allocations τk(u)
Table 1: The mechanics of Teacher-Student Curriculum Learning are equivalent to a cooperative game among units of experience.
Consequently, every notion of coalitional worth is conditional on the evaluation units, thus generating a space of
cooperative games.
Definition 4.1. (TSCL Cooperative Games ) Let Udenote a set of units of experience u∈Uand¯Ua set of evaluation
units ¯u∈¯U. Every evaluation coalition ¯C∈2¯Uinduces a parameterized characteristic function v¯C(Ck)∈Rwhose
value measures the worth of a coalition Ckwhen the members of ¯Care the evaluation units . Therefore, the TSCL -family
of algorithms operate over a parameterized space of cooperative games:
G⟨U,·⟩=/braceleftbig
⟨U,v¯C⟩|¯C⊆¯U/bracerightbig
comprising 2|U|×2|¯U|possible games and where the target-task (i.e., ¯C=¯u) and the multiple-tasks (i.e., ¯C=¯U)
settings are special cases.
Example 4.2. If alearner algorithm is presented with units from C={ZERO,NINE}onMNIST , the following
condition is expected to hold:
v¯C={ZERO,NINE}(C)>v¯C={ZERO,ONE}(C)
4.2 Marginal Contributions to Learning
The notions of coalitions and coalitional worth above induce a game-theoretic interpretation of the learning progression
reward. At any iteration k≤K, this reward r(uk)∈R(Alg. 1, line 7) measures the improvement in policy performance
after the teacher presents a unit ukto the learner algorithm that produces a new policy πk∼L(πk−1,uk). Thus, we
can restate this reward in terms of a game in characteristic function form:
r(uk) =v(Ck)−v(Ck−1) =v(Ck−1+uk)−v(Ck−1) (6)
and note its equivalence to computing the marginal contribution (see Sec. 2 and Eq. 1) of aggregating the unit of
experience uk=uto the existing coalition Ck−1.
4.3 A Fair Allocation Mechanism
A principle of fair attribution in cooperative games is that players get assigned values proportional to their expected
marginal contribution. We note that under the learning progression objective, a bandit action-value estimate qk(u)
(Eq. 4) approximates every unit’s (or arm’s) average marginal contribution after kinteractions. Moreover, as discussed
in Sec. 2.2, multi-arm bandit algorithms may transform action-values through a Boltzmann projection (Eq. 5) that
converts the value estimates into units’ probabilities of interaction with the learner (i.e., the (stochastic) policy τk(u)).
Consequently, the units that, up to interaction k≤K, have produced more significant increases on learner performance
and would be allocated larger fractions of the remaining K−kinteractions.
Thus, a multi-armed bandit teacher implements a fair allocation mechanism that computes units’ values by approximating
their average marginal contributions and converts these approximations into the currency-like utility of the TSCL games,
namely, interactions with the learner.
Table 1 summarizes the equivalences between TSCL components and those of a cooperative game we have established
throughout this section.
5Published in Transactions on Machine Learning Research (09/2024)
5 An Experiment on The Prospect of Cooperation
We design an experimental setting to empirically verify the equivalences we draw between TSCL components and
cooperative game-theoretic concepts (e.g., do cooperative solution concepts capture some notion of curriculum?) and to
highlight the utility of this data-centric approach to understanding TSCL failure modes. To this end, we incrementally
build different parts of the equivalence in supervised learning, reinforcement learning, and classical game settings as
follows:
1.For any set of units of experience given to the teacher algorithm, we simulate their interactions and compute
each a priori Shapley orNowak & Radzik value (Sec. 5.1).
2.We build two value-proportional mechanisms (i.e., pre-computed teacher policies) leveraging the prior units’
values to validate whether a curriculum exists and to show that cooperative solution concepts retrieve such a
notion.
3.We leverage units’ estimated pairwise interactions to understand the value-proportional mechanism success
and TSCL failure from a data-centric perspective (i.e., the composition of the set of units).
5.1 A Simulation of Cooperation
We simulate two coalition formation processes where units of experience (e.g., classes, environments, or opponents) in
each coalition fairly share a finite interaction budget K∈Nand connect the value of the resulting learner’s policies
with coalitional’s worth.
5.1.1 Coalitional Mechanics and Worth
Cooperative (Non-Ordered) Game. To simulate a traditional cooperative game (Sec. 2), we design a coalition
formation process that draws at each interaction k≤Ka unit uk∈Cwith uniform probability τC(uk)∝|C|−1, from
a coalition C, and present it to a learner algorithm. We compute this procedure for every coalition of units C∈2U. The
uniform distribution reflects an a priori ignorance of units’ importance before measuring their effect on the learner .
Generalized (Ordered) Game. To test whether our formulation captures order for curriculum, we build a coalition
formation process that simulates a generalized cooperative game. In this setting, a unitu∈Cis continually presented to
thelearner , for⌊K/|C|⌋interactions, in its permutation order on an ordered coalition C. We repeat this procedure for
every C∈P(2U). As before, the ordered equipartition of interactions reflects our ignorance about units a priori effect.
Coalitional Worth & Characteristic Function. For every coalition, we obtain a model πK
CafterKinteractions with
the units in Cthrough either the traditional or generalized mechanics described above. Therefore, by the equivalence
we established between policy performance and (conditional) coalitional worth (Sec. 4.2), the evaluation of each policy
determines the characteristic function v(C). More importantly, because the resulting policy πK
Cis unbiased for the
evaluation units or coalitions (e.g., the uninformed priors in the mechanics), we estimate, using the same policy πK
C,
the coalitions worth for every characteristic function v¯C(C)parameterized by every evaluation or target coalition ¯C.
Example 5.1. (Example 4.1 cont’d) Assume a budget of K= 100 interactions and a subset (coalition) of classes
from MNIST , for instance, units (classes) ZERO and NINE . In the simulation of a traditional game, for a coalition
C={ZERO,NINE}, we uniformly draw instances from each unit with probability τ(u) =1
2. For a coalition
C= [ ZERO,NINE]in a generalized game, instances from unit ZERO are presented for the first k= 50 iterations
followed by k= 50 instances from NINE . The resulting policy πK
Cperformance is equivalent to the value of the
characteristic function v(C), evaluated at coalition C.
Marginal Contributions & Solution Concepts. The players marginal contributions are the central quantity of
cooperative solution concepts. What do marginal contributions capture in our experiments? First, in both coalition
formation processes, adding a unit uto a coalition Cwhile keeping the number of interactions Kconstant reduces the
learner algorithm’s interactions with the existing units. For instance, in the traditional cooperative game simulation,
adding a unit u′reduces the probability of drawing any unit already in u∈CfrompC(u) =|C|−1topC+u′(u) =
(|C|+ 1)−1, while in the the generalized game reduces units’ interactions from ⌊K/|C|⌋to⌊K/(|C|+ 1)⌋. Therefore,
6Published in Transactions on Machine Learning Research (09/2024)
in either simulation, a unit umarginal contribution v(C+u)−v(C)measures the change in performance produced by
increasing learner’s interactions with uwhile reducing those with the existing units in C.
Example 5.2. (Example 5.1 cont’d) In a traditional cooperative game simulation on MNIST , a marginal contribution
such asv({ZERO,NINE})−v({ZERO})measures the change in learner performance produced by exchanging approxi-
matelyK/2interactions with unit ZERO for interactions with NINE . However, in the generalized game simulation, the
same expression measures the change in performance produced by exchanging Kinteractions with unit {ZERO}for
K/2with ZERO first (pre-training) followed by K/2with NINE (fine-tuning).
In consequence, both solution concepts leveraging marginal contributions, namely, the Shapley value for traditional
games (Eq. 1) and the Nowak & Razik’s value for generalized games (Eq. 2) estimate, in our simulations, a unit’s
average marginal contribution to learning, and thus capture their helpfulness or cooperativeness for curriculum.
5.1.2 A Sanity Check Through Supervised Classification
Our running examples on MNIST inspire the first setting we examine to empirically validate the connections we have
established between TSCL and cooperative games. In this experiment, we considered instances aggregated in classes
asunits of experience and benefited from a trained classifier’s confusion matrix to extract ground-truth information
from unit interactions. For both MNIST andCIFAR10 (Krizhevsky, 2009), we trained a model on the complete
dataset (e.g., for 200epochs), extracted the confusion matrix on validation, and identify the top-k most confused pairs
of classes. On MNIST , we selected the five classes TWO,THREE,FOUR,FIVE and SEVEN belonging to the top-three
most confused pairs (see Appendix A, Fig. 4a), grouped their instances into five units of experience , and conduct the
approximations described in Sec. 5.1 for a traditional cooperative game (i.e., not considering order). Next, we followed
the same approach for CIFAR10 six classes with more significant pairwise confusion errors on validation, namely,
CAR,CAT,DEER ,DOG,FROG and HORSE (see Appendix A, Fig. 4b).
If the equivalences we drew are correct, one may expect a unit Shapley value to be higher when the evaluation target is
the same unit. Moreover, the pairwise interactions between units should approximately reflect the negative interactions
we extracted from the confusion matrices of each trained classifier.
Units’ Values. In effect, for either MNIST orCIFAR 10, each unit’s Shapley value estimated from the traditional
cooperative game simulations correctly matches the ground truth information. In the target-unit setting, where each
unit of experience is also used as evaluation unit, each unit of experience (or class) matching the evaluation unit (or
class) has the largest Shapley value , as depicted in Fig. 1a (first fivetargets) for MNIST and Fig. 1c (first sixtargets)
forCIFAR10 . For instance, on MNIST , unit u=TWO has the largest Shapley value ϕ(TWO) = 0.995when the
evaluation unit is ¯u=TWO . We observed a similar effect on CIFAR 10. Furthermore, for the all-units setting, every
unit’s Shapley value is approximately equal on both MNIST andCIFAR10 (Fig. 1a and 1c, allcolumn), matching the
intuition that, conditional on an allunits evaluation, every unit of experience should be equally valuable.
Measures of Interactions. We also computed the vPoP measure (see Sec. 2.1, Eq. 3) to verify whether its decom-
position of Shapley values into pairwise interaction values correctly identifies the most confused pairs of classes.
For both MNIST and CIFAR10 , their respective vPoP matrices, displayed in Fig. 1b and 1d, provide a reasonable
approximation to the ground-truth pairwise interactions extracted from the confusion matrices. For instance, on MNIST
theunits TWO and SEVEN have the lowest interaction value ϕ(TWO,SEVEN ) =ϕ(SEVEN,TWO) =−0.007which
corresponds to largest entry M(2,7) = 20 of the confusion matrix (see Appendix A, Fig. 4a). Also, in CIFAR 10the
units DOG and CAT have the lowest interaction value ϕ(DOG,CAT) =−0.0164 coinciding with the most confused
classesM(DOG,CAT) = 66 on validation (see Appendix A, Fig. 4b). However, we note that, for instance, on MNIST
confusion matrix M(2,7)̸=M(7,2)and, similarly, on CIFAR10 ’sM(DOG,CAT)̸=M(CAT,DOG). Nevertheless,
we interpret these values as reasonable proxies for units negative, positive, or neutral pairwise interactions.
A valuable aspect of our cooperative game-theoretic analysis of TSCL is that the equivalences we introduced are not
limited to the supervised classification setting where ground truth information is available. Next, we demonstrate the
broad applicability of these ideas through problems in RLand classical games, where finding ground-truth notions of
curriculum is non-trivial. Details to reproduce these experiments are provided in Appendix A.
7Published in Transactions on Machine Learning Research (09/2024)
Two Three Four Five Seven all
u
Two
Three
Four
Five
Sevenu n it s [class]
0.995 - 0.002 - 0.001 0.001 -0.002 0.207
- 0.001 0.995 - 0.000 - 0.003 -0.001 0.202
- 0.001 - 0.001 0.999 0.000 -0.000 0.198
- 0.000 - 0.003 -0.001 0.997 0.000 0.179
- 0.003 - 0.002 -0.000 0.000 0.995 0.205t ar g et s [ u ]
(a)
Two Three Four Five Seven
u(ui, uj)
Two
Three
Four
Five
Seven u n it s [class]0.2077 -0.0001 -0.0002 -0.0001 -0.0007
-0.0001 0.2034 -0.0001 -0.0006 -0.0003
-0.0002 -0.0001 0.1983 -0.0001 -0.0002
-0.0001 -0.0006 -0.0001 0.1797 -0.0002
-0.0007 -0.0003 -0.0002 -0.0002 0.2068u n it s [class] (b)
Car Cat Deer Dog F rog Horse all
u
Car
Cat
Deer
Dog
F rog
Horseu n it s [class]
0.928 -0.007 -0.004 -0.013 -0.013 -0.008 0.147
-0.012 0.691 -0.040 -0.095 -0.038 -0.028 0.080
-0.012 -0.039 0.768 -0.027 -0.034 -0.040 0.103
-0.009 -0.101 -0.034 0.729 -0.019 -0.038 0.088
-0.008 -0.044 -0.036 -0.019 0.843 -0.009 0.121
-0.008 -0.024 -0.044 -0.042 -0.014 0.812 0.113t ar g et s [ u ]
 (c)
Car Cat Deer Dog Frog Horse
u(ui, uj)
Car
Cat
Deer
Dog
Frog
Horseu n it s [class]0.1548 -0.0020 -0.0012 -0.0015 -0.0017 -0.0011
-0.0020 0.1162 -0.0067 -0.0164 -0.0064 -0.0051
-0.0012 -0.0067 0.1288 -0.0057 -0.0054 -0.0071
-0.0015 -0.0164 -0.0057 0.1210 -0.0029 -0.0062
-0.0017 -0.0064 -0.0054 -0.0029 0.1396 -0.0020
-0.0011 -0.0051 -0.0071 -0.0062 -0.0020 0.1348u n it s [class] (d)
Figure 1: We validated the prospect prior using the class-as-a-unit analogy on MNIST andCIFAR 10. In Figures (a) and (c), each
column represents units’ Shapley values ϕ(u)in each cooperative game parameterized by a target-unit ¯uand the target coalition
ofallunits. In Figures (b) and (d), we present the vPoP decomposition matrix (Eq. 3) measuring the pairwise interaction values
ϕ(ui,uj)among units in the all-units target.
5.1.3 Reinforcement Learning
We investigate the MINIGRID -ROOMS (Chevalier-Boisvert et al., 2018) set of three environments, namely, TWOROOMS ,
FOURROOMS , and SIXROOMS for which it is folk knowledge that an optimal curriculum exists.1We apply the prospect
prior simulation of a generalized game where we consider each environment a unit of experience . As a learner
algorithm, we used PPO (Schulman et al., 2017) with an interaction budget of K= 500,000steps, and estimated,
from the outcome of these simulations, the Nowak & Radzik values (Sec. 2, Eq. 2), with every environment and a
uniform distribution over all, as evaluation targets. In Fig. 2a, we show that the Nowak & Radzik values we estimate
match folk knowledge . First, there is no requirement for environments other than TWOROOMS as the only positive
valueϕ¯u(TWOROOMS ) = 0.423correctly measures. Then, for FOURROOMS , the values of ϕ¯u(TWOROOMS ) = 0.041
andϕ¯u(FOURROOMS ) = 0.107indicate that both environments are required. And finally, environments values of
ϕ¯u(FOURROOMS ) = 0.03andϕ¯u(SIXROOMS ) = 0.03indicate that both are needed for solving S IXROOMS .
5.1.4 Classical Games
We introduce an experimental setting, the Adversarial Sparse Iterated Prisoner’s Dilemma (A-SIPD ), that utilizes
the Prisioner’s Dilemma (Flood, 1952; Axelrod & Hamilton, 1981) classical two-player game as a base but in a more
challenging sparse and iterated version where at the end of a finite number of interactions (e.g., 200steps), a win-
draw-loss reward is given to the learner if it accrues more cumulative payoff than its opponent. Opponents are drawn
from a population of five well-known strategies: ALWAYS COOPERATE ,ALWAYS DEFECT ,WINSTAYLOSESWITCH ,
TITFORTAT(Axelrod, 1981) and a ZERODETERMINANT strategy (Hilbe et al., 2013). We apply the prospect prior
simulation of a generalized game where we consider each opponent aunit of experience . As a learner algorithm, we
used PPO (Schulman et al., 2017) with a budget of K= 100,000interactions.
We estimated the Nowak & Radzik values (Sec. 2, Eq. 2), conditioned on each opponent and a uniform mixture over all,
as evaluation targets. The results we present in Fig. 2c show that playing uniquely against TITFORTATis sufficient
across all evaluation targets, including the most challenging opponents, ALWAYS DEFECT andZERODETERMINANT .
This result contrasts with folk knowledge in population-based training (e.g., playing against the population’s Nash
strategy (Nash, 1950)). We defer to Sec. 5.3 a more in-depth discussion of this finding.
5.2 Curriculum from A Priori Values
For both the RL and classical games settings, the Nowak & Radzik value correctly ordered tasks and opponents,
respectively, by their contribution to learning. Nevertheless, we also investigated whether these values’ magnitude
indicated the proportion of interactions (i.e., a fraction of the Kinteractions budget) that should be allocated to each
unit and whether the combination of order and magnitude retrieved an approximate curriculum.
Therefore, inspired by value-proportional allocations (Bachrach et al., 2020), we developed two mechanisms that turn
units’ values into interactions with the learner by projecting any value vector ϕ(u)∈R|U|onto vectors τ(u)∈∆Uin a
|U|-simplex . While Bachrach et al. (2020) use a linear projection unable to handle negative marginal contributions, we
1The curriculum order between these three environments in RL is an intuition that, to our knowledge, has not been quantified before.
8Published in Transactions on Machine Learning Research (09/2024)
TwoRooms FourRoom s SixRoom s all
u
TwoRooms
FourRoom s
SixRoom su n it s [grid]
0.423 0.041 0.000 0.083
-0.010 0.107 0.033 0.052
-0.342 -0.136 -0.033 -0.111t ar g et s [ u ]
(a)Nowak & Radzik Values
0 100000 200000 300000 400000 500000
Step0.00.10.20.30.40.50.60.7returns
projection
tscl- all- exp3s
uniform - all- softm ax
now ak- all- sim plex
shapley- all- softm ax (b) Learning Curves [ all]
MINIGRID-ROOMS
Cooperator Defector TitForT atW inLoseSwicth ZD- Extortionerall
u
Cooperator
Defector
TitForT at
W inLoseSwicth
ZD- Extortioneru n it s [players]
0.206 -0.095 0.232 0.252 0.097 0.151
0.217 -0.463 -0.149 -0.507 -0.753 -0.306
0.112 0.340 0.423 0.609 0.874 0.444
0.154 0.017 0.228 0.527 0.204 0.253
0.219 -0.172 0.002 -0.260 -0.166 -0.092t ar g et s [ u ]
(c)Nowak & Radzik Values
0 20000 40000 60000 80000 100000
Step0.4
0.2
0.00.20.40.60.8returns
projection
tscl- all- exp3s
shapley- all- sim plex
uniform - all- softm ax
now ak- all- sim plex (d) Learning Curves [ all]
A-SIPD
Figure 2: Nowak & Radzik values (a, c) conditional on each single-unit and the multiple-units (all) evaluations, and the learning
curves (b,d) for our mechanisms and TSCL.
TwoRooms FourRooms SixRooms
u(ui, uj)
TwoRooms
FourRooms
SixRoomsu n it s [grid]0.1740 -0.0393 -0.0358
-0.0393 -0.0393 0.0256
-0.0358 0.0256 -0.0358u n it s [grid]
(a) Shapley-vPoP
TwoRooms FourRooms SixRooms
u(ui, uj)
TwoRooms
FourRooms
SixRoomsu n it s [grid]0.1187 0.0439 -0.0507
-0.0123 0.0439 -0.0096
-0.0233 -0.0355 -0.0507u n it s [grid] (b) Nowak&Radzik-vPoP
Cooperator Defector TitForT atW inLoseSwicth ZD-Extortioner
u(ui, uj)
Cooperator
Defector
TitForT at
W inLoseSwicth
ZD-Extortioneru n it s [players]0.2268 -0.0055 -0.0234 0.0000 -0.0979
-0.0055 -0.3501 -0.1071 -0.0091 0.1285
-0.0234 -0.1071 0.4136 -0.0121 -0.0494
0.0000 -0.0091 -0.0121 0.2977 -0.0265
-0.0979 0.1285 -0.0494 -0.0265 -0.2630u n it s [players] (c) Shapley-vPoP
Cooperator Defector TitForT at W inLoseSwicth ZD-Extortioner
u(ui, uj)
Cooperator
Defector
TitForT at
W inLoseSwicth
ZD-Extortioneru n it s [players]0.4016 -0.0217 -0.1332 -0.0971 -0.0914
0.0137 -0.4404 0.0769 0.0095 0.0065
-0.1012 0.0058 0.6102 -0.1058 0.0867
-0.1005 -0.0574 -0.0943 0.5307 0.0200
-0.0602 0.0662 0.0429 -0.0339 -0.2060u n it s [players] (d) Nowak&Radzik-vPoP
Figure 3: The vPoP decomposition of Shapley’s andNowak & Radzik values , conditioned on the all-units evaluation target, for the
MINIGRID-ROOMS (a, b) andA-SIPD (c, d) problem settings.
investigate the Boltzmann orsoftmax projection, commonly used in multi-arm bandit algorithms (see Sec. 2.2, Eq. 5),
and an Euclidean projection (Blondel et al., 2014) that projects to zero any unit with negative value.
When values ϕ(u)areShapley values , the projected vectors are used as pre-computed teacher policies (i.e., probability
distributions), mimicking TSCL ’s interactions but fixed a-priori knowledge of units’ value. However, when values
ϕ(u)areNowak & Radzik , vectorsτ∈∆Uare used as ordered compositional vectors (Aitchison, 1982) that represent
ordered fractions of Kinteractions. Thus, we construct a pre-computed teacher policy that, first orders units by their
Nowak & Radzik values , projects the ordered values onto τ∈∆K
U, and presents the i-th ranked unit ui∈Uto the
learner for the number of interactions indicated by τi∈N. This mechanism preserves the ordered values captured by
Nowak & Radzik ’s solution concept.
Fig. 2 shows that for both MINIGRID -ROOMS andA-SIPD theteacher policies obtained from these value-proportional
mechanisms , in particular the Euclidean projection of Nowak & Radzik values ( nowak-all-simplex ), consistently produce
learner-induced policies that solve the target tasks, and empirically validate our hypothesis that a priori values, in
particular the generalized Nowak & Radzik value we introduced, retrieve both the order of and the proportions of
interactions that should be allocated to each unit to produce curriculum.
TSCL Data-Centric Failures. Moreover, Fig. 2 also highlights the multi-arm bandit approach ( EXP3(Auer et al.,
2003; Graves et al., 2017)) to TSCL (i.e., tscl-all-exp3s ) failure to produce an effective curriculum. We found that in the
presence of units with non-cooperative interactions, bandit-driven TSCL fails. Fig. 3 presents the vPoP decomposition
ofShapley andNowak & Radzik values conditioned on the all-units evaluation. In both settings, the interactions
measured from the Shapley -based decomposition produce lower (negative) values than those obtained with Nowak &
Radzik . As we show through Sec. 4 and Sec. 5, we take the Shapley -based interaction values as fair approximations
ofTSCL interactions, and thus they provide a data-centric explanation to TSCL failures (see Appendix B). On the
other hand, the Nowak & Radzik -based values explain the success of nowak-all-simplex mechanism, along with the
elimination, through the Euclidean projection, of negatively-valued units. These pruning strategies are employed
effectively by data valuation techniques (Yan & Procaccia, 2021).
5.3 Single and Population-based Curriculum
These results offer an alternative approach to what population-based training approaches prescribe as curriculum (Lanctot
et al., 2017; Balduzzi et al., 2019; Garnelo et al., 2021; Liu et al., 2022). Generally, meta-strategy solvers for population-
based training leverage tools from non-cooperative game theory (V on Neumann & Morgenstern, 1944) to find, for
9Published in Transactions on Machine Learning Research (09/2024)
instance, the mixed Nash equilibrium (Nash, 1950) of the empirical meta-game (Wellman, 2006) played by the
population of opponents. In this sense, we may also understand TSCL as a cooperative meta-strategy solver that
prioritizes among a fixed population of (non-learning) opponents (units of experience) those that improve the learning
progression of a single learning player against one or more opponents of the same population.
In the sparse and iterated version of Prisoner’s Dilemma that we introduced, the Defector strategy remains the
(empirical) game Nash equilibrium . However, the ordered prospect prior results in Fig. 2c show that when evaluated
on the population Nash ¯u=Defector , the largest Nowak & Radzik value corresponded to the TitForTat strategy
ϕ¯u(TitForTat ) = 0.34. Playing against the TitForTat strategy remains the optimal solution across all evaluation targets,
meaning that TitForTat is the best proxy opponent to learn from and reach the Nash equilibrium strategy. We can
interpret this result from two perspectives. First, it could indicate that the sparse, iterated, and overtly adversarial version
of the game we constructed is a more complicated problem than the original, and the Nash equilibrium, the Defector
strategy, is a stronger opponent. However, these results may also indicate that a cooperative approach to meta-strategy
selection may improve performance in some scenarios. We believe that this connection warrants further investigation.
6 Limitations
The simulations we computed in our experiments are computationally expensive. It is well-established that cooperative
solution concepts are NP-hard (Deng & Papadimitriou, 1994; Elkind et al., 2009). However, better approximations are
possible (Yan & Procaccia, 2021; Mitchell et al., 2022) although we do not explore them in this work. Consequently,
we do not consider the prospect prior experiments and the value-proportional curriculum mechanism as algorithmic
innovations to replace TSCL . They represent a data-centric approach to study the limits imposed on TSCL -style
algorithms by the (non)cooperative mechanics among units of experience. However, we acknowledge that the mechanics
of units’ interactions also affect other aspects of TSCL . These aspects may include, for instance, the teacher ’s credit-
assignment problem (Gittins, 1979) or neural networks learning and forgetting dynamics (e.g., Lee et al. (2021)). We
control for these factors by keeping them constant in our experiments (see Appendix A) but do not undertake their
analysis here. Our work is a starting point for more thorough explorations of TSCL and curriculum learning, their
underlying mechanisms and broader applicability in machine learning.
7 Related Work
Curriculum Learning. Since the seminal works of Elman (1993); Krueger & Dayan (2009) and Bengio et al. (2009),
a large body of literature has been produced on curricula for machine learning algorithms. Excellent surveys presented
by Narvekar et al. (2022); Wang et al. (2022), and Soviany et al. (2022) offer a comprehensive state of recent advances in
the field. Our work closely inspects the TSCL framework concurrently introduced by Graves et al. (2017) and Matiisen
et al. (2020). While follow-up works have generally focused on either algorithmic innovations (Weinshall et al., 2018;
Portelas et al., 2020; Feng et al., 2021; Liu et al., 2020; Portelas et al., 2019b; Racaniere et al., 2020; Florensa et al.,
2017; Campero et al., 2021; Du et al., 2022; Florensa et al., 2018) or evaluation benchmarks (Chevalier-Boisvert
et al., 2019; Romac et al., 2021), the data-centric perspective (Ng, 2021; Karpathy & Abbeel, 2021) we proposed here
is less explored. The work by Wu et al. (2020) empirically verifies the same fundamental questions on why,when ,
andhow curriculum learning works. However, while our work is more specifically focused on TSCL , we provide a
game-theoretic grounding that could be further extended to analyze Wu et al. (2020) setting. On the other hand, Lee
et al. (2021) specifically explore TSCL but with a focus on the deep neural network dynamics leading to catastrophic
forgetting McCloskey & Cohen (1989). The evaluation mechanisms that we propose in the prospect prior simulation
and the game-theoretic characterization of experience interference we introduce here provide a more general grounding
for the problems studied there.
Machine Learning & Game Theory. Our work draws much of its inspiration from the tradition of cross-pollination
between machine learning and game theory research (Cesa-Bianchi & Lugosi, 2006). In recent years, game theory
research has fuelled work in optimization (Daskalakis & Panageas, 2018; Jin et al., 2019), generative modelling (Good-
fellow et al., 2014; Farnia & Ozdaglar, 2020; Mohebbi Moghaddam et al., 2023), or robustness (Madry et al., 2018;
Huang et al., 2022a). Also, game-theoretic arguments have been used to revitalize long-standing problems in ma-
chine learning. For instance, Gemp et al. (2021) reframed the century-old problem of Principal Component Analysis
(PCA) (F.R.S., 1901; Hotelling, 1933) as the Nash equilibrium (Nash, 1950) of a multiplayer game. Recently, Chang
10Published in Transactions on Machine Learning Research (09/2024)
et al. (2020) reformulated sequential decision-making problems (i.e., RL (Sutton & Barto, 2018)) as local economic
transactions among specialized self-interested agents. Moreover, and closer to our data-centric approach, Lundberg &
Lee (2017) pioneered work on feature attribution andexplainable machine learning models through cooperative game
theory. In particular, they approximate Shapley’s value (Shapley, 1952) (Eq. 1) to compute the influence input features
have on model predictions . In a way, our work extends Lundberg & Lee (2017) input-feature-as-a-player analogy
through the notion of units of experience and introduces substantial elements to discuss how the cooperative mechanics
of experience affects TSCL at different levels of abstraction (i.e., from features to tasks) and varied learning paradigms.
Active Learning. As Graves et al. (2017) and Matiisen et al. (2020) noted in their works, TSCL is an experience-
prioritization mechanism holding a solid connection to algorithms that actively select experience. The central problem in
active learning is to decide which experience to present to a learning algorithm by querying an expert or oracle (Settles,
2010). Areas of research that have also adopted this paradigm of actively sampling experience include experience
replay mechanisms in RLSchaul et al. (2016); Andrychowicz et al. (2017); Fedus et al. (2020) where transitions stored
in areplay buffer (Lin, 1992) are sampled according to some prioritization mechanism. This category of active methods
fits perfectly under the TU-game among units of experience we introduced in Sec. 4.
Multitask Learning. The cooperative game we designed also provides grounding to the problem of determining
which tasks should be learned together and what their optimal order is , originally from the literature of multitask
learning (Caruana, 1997). The discussion on experience interference in Sec. 4, the prospect prior simulation and our
computation of ordered andunordered Shapley values, and the vPoP metric are some novel tools we introduce that
could shed new light on this problem. Recent work has explored the different avenues of how task interference and
ordering affect learning outcomes (Standley et al., 2019; Shamsian et al., 2023; Zhang et al., 2021; Fifty et al., 2021;
Lin et al., 2019). Our work provides a novel grounding that relates easily to these efforts.
Continual Learning. Finally, another area of interest to our work is continual learning Parisi et al. (2019); De Lange
et al. (2022); Mundt et al. (2023). We note that the ordered prospect prior simulates a continual learning setting. Units
of experience are presented to the learning algorithm in order, and their importance (i.e., value in the cooperative game)
is estimated using Nowak & Radzik (1994) extension (Eq. 2) to Shapley’s value. This perspective offers a principled
approach to rank and order tasks by their importance, a critical aspect of the continual learning setting. We believe that
future extensions to the ordered prospect prior experiment may be part of a toolbox to understand continual learning
benchmarks (Lomonaco & Maltoni, 2017; Lin et al., 2021; Srinivasan et al., 2022), although we do not undertake that
task here.
8 Conclusions & Future Work
We reexamined TSCL through the lens of cooperative game theory. By drawing inspiration from work on data valuation,
feature attribution and explainability, we provide a novel data-centric perspective that re-frames several of its components
through alternative cooperative game-theoretic interpretations. Our experiments confirmed the appropriateness of
studying TSCL -style under this framework and highlighted the impact of units’ cooperative mechanics on this problem.
However, we only began to unveil the potential of allocation mechanisms, solution concepts, and measures of interactions
to explain some fundamental aspects of TSCL and hope our work inspires an influx of novel game-theoretic approaches
to the problem. Future work would explore more theoretically-grounded analysis of this problem through the connection
between convex games (Shapley, 1971) and super(sub)modularity in discrete combinatorial optimization (Dughmi,
2009; Bach, 2011; Krause & Guestrin, 2011) and the extension to continuous set of units through values of non-atomic
games (Aumann & Shapley, 1974).
Acknowledgements
MD would like to thank Eugene Vinitsky for insightful feedback on earlier versions of this manuscript. The NSERC
Discovery Grant and the Canada CIFAR AI Chair program supported MD and LP. Computing resources provided by
Mila, Québec AI Institute, partly enabled this research.
11Published in Transactions on Machine Learning Research (09/2024)
References
J Aitchison. The statistical analysis of compositional data. Journal of the Royal Statistical Society , 44(2):139–160,
January 1982. 9
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh
Tobin, Openai Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In I Guyon, U V on Luxburg,
S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett (eds.), Advances in Neural Information Processing
Systems , volume 30. Curran Associates, Inc., 2017. 11
Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM J. Comput. , 32(1):48–77, jan 2003. ISSN 0097-5397. doi: 10.1137/S0097539701398375. URL
https://doi.org/10.1137/S0097539701398375 . 3, 9, 20, 21
R. J. Aumann and L. S. Shapley. Values of Non-Atomic Games . Princeton University Press, 1974. URL http:
//www.jstor.org/stable/j.ctt13x149m . 11
R Axelrod and W D Hamilton. The evolution of cooperation. Science , 211(4489):1390–1396, March 1981. 8
Robert Axelrod. The emergence of cooperation among egoists. The American political science review , 75(2):306–318,
1981. 8, 21
Francis Bach. Learning with submodular functions: A convex optimization perspective. November 2011. 11
Yoram Bachrach, Richard Everett, Edward Hughes, Angeliki Lazaridou, Joel Z Leibo, Marc Lanctot, Michael Johanson,
Wojciech M Czarnecki, and Thore Graepel. Negotiating team formation using deep reinforcement learning. Artificial
intelligence , 288(103356):103356, November 2020. 2, 8
David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jaderberg, and Thore
Graepel. Open-ended learning in symmetric zero-sum games. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine
Learning Research , pp. 434–443. PMLR, 2019. 9
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the
26th Annual International Conference on Machine Learning , ICML ’09, pp. 41–48, New York, NY , USA, 2009.
Association for Computing Machinery. ISBN 9781605585161. URL https://doi.org/10.1145/1553374.
1553380 . 1, 2, 10
Lilian Besson. SMPyBandits: an Open-Source Research Framework for Single and Multi-Players Multi-Arms Bandits
(MAB) Algorithms in Python. Online at: github.com/SMPyBandits/SMPyBandits , 2018. URL https:
//github.com/SMPyBandits/SMPyBandits/ . Code at https://github.com/SMPyBandits/SMPyBandits/,
documentation at https://smpybandits.github.io/. 20, 21
Mathieu Blondel, Akinori Fujino, and Naonori Ueda. Large-Scale multiclass support vector machine training via
euclidean projection onto the simplex. In 2014 22nd International Conference on Pattern Recognition , pp. 1289–1294,
August 2014. 9
Andres Campero, Roberta Raileanu, Heinrich Kuttler, Joshua B. Tenenbaum, Tim Rocktäschel, and Edward Grefen-
stette. Learning with {amig}o: Adversarially motivated intrinsic goals. In International Conference on Learning
Representations , 2021. URL https://openreview.net/forum?id=ETBc_MIMgoX . 10
Rich Caruana. Multitask learning. Machine learning , 28:41–75, 1997. 11
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games . Cambridge University Press, 2006. doi:
10.1017/CBO9780511546921. 10
Michael Chang, Sid Kaushik, S Matthew Weinberg, Tom Griffiths, and Sergey Levine. Decentralized reinforcement
learning: Global Decision-Making via local economic transactions. In Hal Daumé Iii and Aarti Singh (eds.),
Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine
Learning Research , pp. 1437–1447. PMLR, 2020. 10
12Published in Transactions on Machine Learning Research (09/2024)
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for gymnasium,
2018. URL https://github.com/Farama-Foundation/Minigrid . 8, 20
Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen,
and Yoshua Bengio. BabyAI: First steps towards grounded language learning with a human in the loop. In
International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=
rJeXCo0cYX . 10
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/
139c3c1b7ca46a9d4fd6d163d98af635-Paper.pdf . 10
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne
Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern
analysis and machine intelligence , 44(7):3366–3385, July 2022. 11
Xiaotie Deng and Christos H. Papadimitriou. On the complexity of cooperative solution concepts. 19:257–266, 1994.
ISSN 0364-765X. 2, 10
Yuqing Du, P Abbeel, and Aditya Grover. It takes four to tango: Multiagent selfplay for automatic curriculum generation.
International Conference on Learning Representations , 2022. 10
Shaddin Dughmi. Submodular functions: Extensions, distributions, and algorithms. a survey. December 2009. 11
Edith Elkind, Leslie Ann Goldberg, Paul W Goldberg, and Michael Wooldridge. On the computational complexity of
weighted voting games. Annals of mathematics and artificial intelligence , 56(2):109–131, June 2009. 10
J L Elman. Learning and development in neural networks: the importance of starting small. Cognition , 48(1):71–99,
July 1993. 1, 10
Ulrich Faigle. Mathematical Game Theory . 2022. 4
Farzan Farnia and Asuman Ozdaglar. GANs may have no nash equilibria. arXiv e-prints , pp. arXiv:2002.09124,
February 2020. 10
William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark Rowland, and Will
Dabney. Revisiting fundamentals of experience replay. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the
37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pp.
3061–3071. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/fedus20a.html .
11
Dieqiao Feng, Carla P Gomes, and Bart Selman. A novel automated curriculum strategy to solve hard sokoban planning
instances. October 2021. 1, 10
Christopher Fifty, E Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings
for Multi-Task learning. Advances in neural information processing systems , 2021. 11
Merrill M. Flood. Some Experimental Games . RAND Corporation, Santa Monica, CA, 1952. 8
Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse curriculum generation
for reinforcement learning. In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg (eds.), Proceedings of the 1st
Annual Conference on Robot Learning , volume 78 of Proceedings of Machine Learning Research , pp. 482–495.
PMLR, 2017. 10
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning
agents. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning , volume 80 of Proceedings of Machine Learning Research , pp. 1515–1528. PMLR, 2018. 10
13Published in Transactions on Machine Learning Research (09/2024)
Karl Pearson F.R.S. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and
Dublin Philosophical Magazine and Journal of Science , 2(11):559–572, 1901. doi: 10.1080/14786440109462720.
URLhttps://doi.org/10.1080/14786440109462720 . 10
Marta Garnelo, Wojciech Marian Czarnecki, Siqi Liu, Dhruva Tirumala, Junhyuk Oh, Gauthier Gidel, Hado van Hasselt,
and David Balduzzi. Pick your battles: Interaction graphs as population-level objectives for strategic diversity. In
Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems , AAMAS ’21, pp.
1501–1503, Richland, SC, 2021. International Foundation for Autonomous Agents and Multiagent Systems. ISBN
9781450383073. 9
Ian Gemp, Brian McWilliams, Claire Vernade, and Thore Graepel. Eigengame: {PCA} as a nash equilibrium. In
International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=
NzTU59SYbNq . 10
Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. April 2019. 1
J C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society , 41(2):148–164,
January 1979. 2, 3, 10
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence,
and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems , volume 27. Curran Asso-
ciates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf . 10
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning . MIT press, 2016. 4
Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among players in cooperative
games. International Journal of Game Theory , 28(4):547–565, Nov 1999. ISSN 0020-7276. 2, 3
Alex Graves, Marc G Bellemare, Jacob Menick, Rémi Munos, and Koray Kavukcuoglu. Automated curriculum learning
for neural networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on
Machine Learning , volume 70 of Proceedings of Machine Learning Research , pp. 1311–1320. PMLR, 2017. 1, 2, 3,
4, 9, 10, 11, 20, 21
Kjell Hausken and Matthias Mohr. The value of a player in n-person games. Social choice and welfare , 18(3):465–483,
2001. ISSN 0176-1714. 2, 3
Christian Hilbe, Martin A. Nowak, and Karl Sigmund. Evolution of extortion in iterated prisoner’s dilemma games.
Proceedings of the National Academy of Sciences of the United States of America , 110(17):6913–6918, Apr 2013.
ISSN 0027-8424. 8, 21
Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of Educational
Psychology , 24:498–520, 1933. 10
Peide Huang, Mengdi Xu, Fei Fang, and Ding Zhao. Robust reinforcement learning as a stackelberg game via
adaptively-regularized adversarial training. February 2022a. 10
Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and João G.M.
Araújo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of
Machine Learning Research , 23(274):1–18, 2022b. URL http://jmlr.org/papers/v23/21-1342.html .
20, 21
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Minmax optimization: Stable limit points of gradient descent ascent
are locally optimal. arXiv.org , 2019. 10
Andrej Karpathy and Pieter Abbeel. The robot brains podcast: Andrej karpathy on the vi-
sionary ai in tesla’s autonomous driving. https://podcasts.apple.com/us/podcast/
andrej-karpathy-on-visionary-ai-in-teslas-autonomous/id1559275284?i=
1000513993723 , 2021. 10
14Published in Transactions on Machine Learning Research (09/2024)
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun
(eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings , 2015. URL http://arxiv.org/abs/1412.6980 . 19, 20, 21
Vince Knight, Owen Campbell, Marc, T J Gaffney, Eric Shaw, Vsn Reddy Janga, Nikoleta Glynatsi, James Campbell,
Karol M Langner, Sourav Singh, Julie Rymer, Thomas Campbell, Jason Young, MHakem, Geraint Palmer, Kristian
Glass, Daniel Mancia, edouardArgenson, Martin Jones, kjurgielajtis, Yohsuke Murase, Sudarshan Parvatikar, Melanie
Beck, Cameron Davidson-Pilon, Marios Zoulias, Adam Pohl, Paul Slavin, Timothy Standen, Aaron Kratz, and Areeb
Ahmed. Axelrod-Python/Axelrod: v4.12.0, October 2021. 21
Andreas Krause and Carlos Guestrin. Submodularity and its applications in optimized information gathering. ACM
Trans. Intell. Syst. Technol. , 2(4):1–20, July 2011. 11
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, April 2009. 7
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 19
Kai A Krueger and Peter Dayan. Flexible shaping: how learning in small steps helps. Cognition , 110(3):380–394,
March 2009. 1, 2, 10
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Pérolat, David Silver,
and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. Advances in neural
information processing systems , 30, 2017. 9
Tor Lattimore and Csaba Szepesvári. Bandit Algorithms . Cambridge University Press, July 2020. 3
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/
exdb/mnist/ . 3, 19
Sebastian Lee, Sebastian Goldt, and Andrew M. Saxe. Continual learning in the teacher-student setup: Impact of task
similarity. International Conference on Machine Learning , 2021. URL https://www.semanticscholar.
org/paper/57db7f24f15150ef7ea0db1fed20e6ee752792ec . 1, 10
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning ,
8(3):293–321, May 1992. 11
Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qingfu Zhang, and S Kwong. Pareto Multi-Task learning. Advances in neural
information processing systems , 2019. 11
Zhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The clear benchmark: Continual learning on real-world
imagery. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track
(Round 2) , 2021. 11
Rui Liu, Tianyi Wu, and Barzan Mozafari. Adam with bandit sampling for deep learning. October 2020. 1, 10
Siqi Liu, Luke Marris, Daniel Hennes, Josh Merel, Nicolas Heess, and Thore Graepel. NeuPL: Neural population
learning. In International Conference on Learning Representations , 2022. URL https://openreview.net/
forum?id=MIX3fJkl_1 . 9
Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In
Sergey Levine, Vincent Vanhoucke, and Ken Goldberg (eds.), Proceedings of the 1st Annual Conference on Robot
Learning , volume 78 of Proceedings of Machine Learning Research , pp. 17–26. PMLR, 13–15 Nov 2017. URL
https://proceedings.mlr.press/v78/lomonaco17a.html . 11
Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceedings of the 31st
International Conference on Neural Information Processing Systems , NIPS’17, pp. 4768–4777, Red Hook, NY , USA,
2017. Curran Associates Inc. ISBN 9781510860964. 1, 11
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning
models resistant to adversarial attacks. In International Conference on Learning Representations , 2018. URL
https://openreview.net/forum?id=rJzIBfZAb . 10
15Published in Transactions on Machine Learning Research (09/2024)
Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-Student curriculum learning. IEEE
transactions on neural networks and learning systems , 31(9):3732–3740, September 2020. 1, 2, 3, 4, 10, 11
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning
problem. In Gordon H Bower (ed.), Psychology of Learning and Motivation , volume 24, pp. 109–165. Academic
Press, January 1989. 10
Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for shapley value estimation.
Journal of machine learning research: JMLR , 23(43):1–46, 2022. 10
Monireh Mohebbi Moghaddam, Bahar Boroomand, Mohammad Jalali, Arman Zareian, Alireza Daeijavad, Moham-
mad Hossein Manshaei, and Marwan Krunz. Games of GANs: game-theoretical models for generative adversarial
networks. Artificial Intelligence Review , February 2023. 10
Martin Mundt, Yongwon Hong, Iuliia Pliushch, and Visvanathan Ramesh. A wholistic view of continual learning with
deep neural networks: Forgotten lessons and the bridge to active and open world learning. Neural networks: the
official journal of the International Neural Network Society , 160:306–336, March 2023. 11
Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning
for reinforcement learning domains: a framework and survey. Journal of machine learning research: JMLR , 21(1):
7382–7431, June 2022. 10
J F Nash. Equilibrium points in N-Person games. Proceedings of the National Academy of Sciences of the United States
of America , 36(1):48–49, January 1950. 8, 10
Andrew Ng. Mlops: From model-centric to data-centric ai. https://www.youtube.com/watch?v=
06-AZXmwHjo , 2021. 1, 10
Andrzej S Nowak and Tadeusz Radzik. The shapley value for n-person games in generalized characteristic function
form. Games and economic behavior , 6(1):150–161, January 1994. 2, 3, 11
Pierre-Yves Oudeyer, Frédéric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental
development. IEEE transactions on evolutionary computation , 11(2):265–286, 2007. 2
German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with
neural networks: A review. Neural networks: the official journal of the International Neural Network Society , 113:
54–71, May 2019. 11
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin
Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:
An imperative style, high-performance deep learning library. In Advances in Neural Information Processing
Systems 32 , pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf .
19, 20, 21
Roma Patel, Marta Garnelo, Ian Gemp, Chris Dyer, and Yoram Bachrach. Game-theoretic vocabulary selection via
the shapley value and banzhaf index. In Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies , pp. 2789–2798, Online, June 2021.
Association for Computational Linguistics. 1
Rémy Portelas, Cédric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum learning of
deep RL in continuously parameterized environments. October 2019a. 1
Rémy Portelas, Cédric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum learning of
deep RL in continuously parameterized environments. October 2019b. 10
Rémy Portelas, Clément Romac, Katja Hofmann, and Pierre-Yves Oudeyer. Meta automatic curriculum learning.
November 2020. 10
16Published in Transactions on Machine Learning Research (09/2024)
Ariel Procaccia, Nisarg Shah, and Max Tucker. On the structure of synergies in cooperative games. Proceedings of the
AAAI Conference on Artificial Intelligence , 28(1), Jun 2014. ISSN 2374-3468. doi: 10.1609/aaai.v28i1.8812. URL
https://ojs.aaai.org/index.php/AAAI/article/view/8812 . 2, 3
Sebastien Racaniere, Andrew Lampinen, Adam Santoro, David Reichert, Vlad Firoiu, and Timothy Lillicrap. Automated
curriculum generation through setter-solver interactions. In International Conference on Learning Representations ,
2020. 10
Clément Romac, Rémy Portelas, Katja Hofmann, and Pierre-Yves Oudeyer. TeachMyAgent: a benchmark for automatic
curriculum learning in deep RL. March 2021. 10
Alvin E Roth (ed.). The Shapley value: essays in honor of Lloyd S. Shapley . Cambridge University Press, October
1988. 2
Estela Sanchez and Gustavo Bergantiños. On values for generalized characteristic functions. Operations-Research-
Spektrum , 19(3):229–234, September 1997. 2
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In 4th International
Conference on Learning Representations , 2016. 11
J. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers . The MIT
Press, 1991. ISBN 9780262256674. 2
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017. 8, 20, 21
Burr Settles. Active learning literature survey. Machine learning , 15(2):201–221, 2010. 11
Aviv Shamsian, Aviv Navon, Neta Glazer, Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya. Auxiliary learning as an
asymmetric bargaining game. ArXiv , 2023. 11
Lloyd S Shapley. A value for N-Person games. Technical report, RAND Corporation, 1952. 1, 2, 11
Lloyd S Shapley. Cores of convex games. International Journal of Game Theory , 1(1):11–26, December 1971. 11
Martin Shubik. Game theory models and methods in political economy. In Kenneth J Arrow and Michael D Intriligator
(eds.), Handbook of Mathematical Economics , volume 1, pp. 285–330. Elsevier, 1981. 2
Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: A survey. International journal
of computer vision , 130(6):1526–1565, June 2022. 10
Tejas Srinivasan, Ting-Yun Chang, Leticia Pinto Alva, Georgios Chochlakis, Mohammad Rostami, and Jesse Thomason.
Climb: A continual learning benchmark for vision-and-language tasks. volume 35, pp. 29440–29453, 2022. 11
Trevor Standley, Amir R Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should
be learned together in multi-task learning? May 2019. 11
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction . The MIT Press, 2nd edition, 2018.
11
Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe reinforcement learning via
curriculum induction. Advances in neural information processing systems , 33:12151–12162, 2020. 1
René van den Brink and Gerard van der Laan. Axiomatizations of the normalized banzhaf value and the shapley value.
Social Choice and Welfare , 15(4):567–582, 1998. ISSN 01761714, 1432217X. URL http://www.jstor.org/
stable/41106281 . 2
John V on Neumann and Oskar Morgenstern. Theory of games and economic behavior . Princeton University Press,
Princeton, NJ, 1944. 1, 9
Xin Wang, Yudong Chen, and Wenwu Zhu. A survey on curriculum learning. IEEE transactions on pattern analysis
and machine intelligence , 44(9):4555–4576, September 2022. 10
17Published in Transactions on Machine Learning Research (09/2024)
Robert J Weber. Probabilistic values for games. In A E Roth (ed.), Essays on the Shapley Value and Its Applications ,
pp. 101–119. Cambridge University Press, 1988. 2
Daphna Weinshall, Gad Cohen, and Dan Amir. Curriculum learning by transfer learning: Theory and experiments with
deep networks. Technical report, 2018. 10
Michael P. Wellman. Methods for empirical game-theoretic analysis. In Proceedings of the 21st National Conference
on Artificial Intelligence - Volume 2 , AAAI’06, pp. 1552–1555. AAAI Press, 2006. ISBN 9781577352815. 10
Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? International Conference on Learning
Representations , 2020. 1, 10
Omry Yadan. Hydra - a framework for elegantly configuring complex applications. Github, 2019. URL https:
//github.com/facebookresearch/hydra . 19
Tom Yan and Ariel D Procaccia. If you like shapley then you’ll love the core. Proceedings of the ... AAAI Conference
on Artificial Intelligence. AAAI Conference on Artificial Intelligence , 35(6):5751–5759, May 2021. 1, 9, 10
Yipeng Zhang, Tyler L Hayes, and Christopher Kanan. Disentangling transfer and interference in Multi-Domain
learning. ArXiv , 2021. 11
18Published in Transactions on Machine Learning Research (09/2024)
A Prospect Prior Experiments Details
We provide for all problems, models or policies architectures, algorithm hyperparameters, and other reproducibility
details.2All models and architectures are implemented with PYTORCH (Paszke et al., 2019), are configured using
HYDRA (Yadan, 2019), and fit on a workstation equipped with a 16GBNVIDIA RTX A4000 GPU, 32GB of RAM, and
32CPU cores.
A.1 Supervised Learning
MNIST. We trained a model on the MNIST (LeCun & Cortes, 2010) supervised 10-digits classification task. Specifica-
tion of the model architecture and hyper-parameters selection are provided in Table 2.
Hyperparameter Value
optimizer ADAM (Kingma & Ba, 2015)
learning-rate 10−4
betas (0.9,0.999)
eps 10−8
batch-size 4
epochs 200
shuffle YesModel
CONV 2D(32,3,1)
RELU()
CONV 2D(64, 3, 1)
RELU()
MAXPOOL2D(2, 2)
DROPOUT(0.25)
FLATTEN ()
LINEAR (9216, 128)
RELU()
DROPOUT(0.5)
LINEAR (128, 10)
Table 2: Details on the learning algorithm hyperparameters ( left) and model architecture ( right ) used in the MNIST (LeCun &
Cortes, 2010) experiments. Model components and the optimizer are provided by PYTORCH (Paszke et al., 2019). These details
remained constant throughout the rest of the experiments with MNIST.
CIFAR10. The experiments on CIFAR 10(Krizhevsky et al., 2009) follow the same setting as those on MNIST . We
similarly trained a model on the supervised 10-classes task. Specification of the model architecture and hyperparameter
selection are provided in Table 3.
Hyperparameter Value
optimizer ADAM (Kingma & Ba, 2015)
learning-rate 10−4
betas (0.9,0.999)
eps 10−8
batch-size 4
epochs 200
shuffle YesModel
CONV 2D(3, 6, 5)
RELU()
MAXPOOL2D(2, 2)
CONV 2D(6, 16, 5)
RELU()
MAXPOOL2D(2, 2)
FLATTEN ()
LINEAR (400, 120)
RELU()
LINEAR (120, 84)
RELU()
LINEAR (84, 10)
Table 3: Details on the learning algorithm hyperparameters ( left) and model architecture ( right ) used in the CIFAR 10(Krizhevsky
et al., 2009) experiments. Model components and the optimizer are provided by PYTORCH (Paszke et al., 2019). These details
remained constant throughout the rest of the experiments with C IFAR 10.
2Regardless, we plan to release the complete source code of all our experiments.
19Published in Transactions on Machine Learning Research (09/2024)
Digit2 Digit3 Digit4 Digit5 Digit7
confusionsDigit2
Digit3
Digit4
Digit5
Digit7classes   0   14    2    1   20
  10    0    2   17   11
   2    0    0    1   13
   0   10    2    0    0
   1    3    3    3    0classes
Two Three Four Five Seven all
u
Two
Three
Four
Five
Sevenu n it s [class]
0.995 - 0.002 - 0.001 0.001 -0.002 0.207
- 0.001 0.995 - 0.000 - 0.003 -0.001 0.202
- 0.001 - 0.001 0.999 0.000 -0.000 0.198
- 0.000 - 0.003 -0.001 0.997 0.000 0.179
- 0.003 - 0.002 -0.000 0.000 0.995 0.205t ar g et s [ u ]
Two Three Four Five Seven
u(ui, uj)
Two
Three
Four
Five
Seven u n it s [class]0.2077 -0.0001 -0.0002 -0.0001 -0.0007
-0.0001 0.2034 -0.0001 -0.0006 -0.0003
-0.0002 -0.0001 0.1983 -0.0001 -0.0002
-0.0001 -0.0006 -0.0001 0.1797 -0.0002
-0.0007 -0.0003 -0.0002 -0.0002 0.2068u n it s [class]
(a) MNIST
Car Cat Deer Dog Frog Horse
confusionsCar
Cat
Deer
Dog
Frog
Horseclasses   0    0    0    0    0    0
   1    0   11   22   34   14
   3   23    0    9   11   20
   1   66   13    0   26   13
   3   12   11    2    0    0
   0   10    5    4    0    0classes
Car Cat Deer Dog F rog Horse all
u
Car
Cat
Deer
Dog
F rog
Horseu n it s [class]
0.928 -0.007 -0.004 -0.013 -0.013 -0.008 0.147
-0.012 0.691 -0.040 -0.095 -0.038 -0.028 0.080
-0.012 -0.039 0.768 -0.027 -0.034 -0.040 0.103
-0.009 -0.101 -0.034 0.729 -0.019 -0.038 0.088
-0.008 -0.044 -0.036 -0.019 0.843 -0.009 0.121
-0.008 -0.024 -0.044 -0.042 -0.014 0.812 0.113t ar g et s [ u ]
Car Cat Deer Dog Frog Horse
u(ui, uj)
Car
Cat
Deer
Dog
Frog
Horseu n it s [class]0.1548 -0.0020 -0.0012 -0.0015 -0.0017 -0.0011
-0.0020 0.1162 -0.0067 -0.0164 -0.0064 -0.0051
-0.0012 -0.0067 0.1288 -0.0057 -0.0054 -0.0071
-0.0015 -0.0164 -0.0057 0.1210 -0.0029 -0.0062
-0.0017 -0.0064 -0.0054 -0.0029 0.1396 -0.0020
-0.0011 -0.0051 -0.0071 -0.0062 -0.0020 0.1348u n it s [class]
(b) CIFAR10
Figure 4: The class-as-a-unit analogy applied to MNIST (a) and CIFAR 10(b) served as our ground truth. For each problem, we
derived the Shapley’s value from the precomputed priors ( left) [Eq. 1] on each cooperative game (Sec. 5). Our results verify that units
values on the target-unit settings approximately ordered the most confused pairs of classes. For instance, digits 2 & 7 in MNIST , or
dog& cat in C IFAR 10. When the target is allclasses, the vPoP decomposition ( right ) also (Sec. 2.1) identifies interfering pairs.
A.2 Reinforcement Learning
MINIGRID ROOMS .We utilized a sequence of TWOROOMS ,FOURROOMS , and SIXROOMS gridworlds provided on
MINIGRID (Chevalier-Boisvert et al., 2018) as units of experience . As the learning algorithm, we trained for 500,000
steps a PPO Schulman et al. (2017) agent, whose implementation we derived from CLEAN RLHuang et al. (2022b).
Policy and actor-critic architecture, with shared backbone, as well as other PPO hyperparameters details are presented
in Table 4. For the TSCL experiments, we leveraged Exp3S (Auer et al., 2003) implementation from Besson (2018)
with default hyperparameters α= 10−5andγ= 0.05, as defined in Graves et al. (2017).
Hyperparameter Value
optimizer ADAM Kingma & Ba (2015)
learning-rate 0.0025
annealing Yes
num-steps 128
total-timesteps 500,000
seeds 5
gamma 0.99
GAE-lambda 0.95
num-minibatches 4
update-epochs 4
advantage-normalization Yes
clip-value-loss Yes
clip-coeff 0.2
entropy-coeff 0.01
vf-coeff 0.5
max-grad-norm 0.5
target-kl NoActor Critic
CONV 2D(16, 2, 2)
RELU()
MAXPOOL2D(2, 2)
CONV 2D(16, 32, 2, 2)
RELU()
CONV 2D(16, 64, 2, 2)
RELU()
LINEAR (64,64) LINEAR (64,64)
TANH() TANH()
LINEAR (64,7) LINEAR (64,1)
Table 4: Details on the PPO hyperparameters ( left) and actor-critic architecture ( right ) used in the MINIGRID ROOMS (Chevalier-
Boisvert et al., 2018) experiments. Policy and critic components, and the optimizer, are provided by PYTORCH (Paszke et al., 2019).
Implementation and default hyperparameters are derived from CLEAN RL(Huang et al., 2022b). These details remained constant
throughout the rest of the experiments with M INIGRID ROOMS .
20Published in Transactions on Machine Learning Research (09/2024)
A.3 Populations & Games.
Adversarial SIPD. In our more challenging sparse and iterated version of Prisoner’s Dilemma, at the end of 200
interactions, inspired by Axelrod’s competition (Axelrod, 1981). A win-draw-loss rewardr={−1,0,1}is given to
a learning player if it beats a fixed opponent. Opponents are drawn from a population of five well-known strategies:
always cooperate ,always defect ,win-stay-lose-switch ,tit-for-tat , and a zero-determinant strategy (Axelrod, 1981; Hilbe
et al., 2013; Knight et al., 2021). We trained for 500episodes (or 100,000steps) a PPO (Schulman et al., 2017) agent
adapted from CLEAN RLHuang et al. (2022b) default implementation. Policy and actor-critic architecture, without
shared backbone, as well as other PPO hyperparameters details are presented in Table 5. For the TSCL experiments,
we leveraged Exp3S (Auer et al., 2003) implementation from Besson (2018) with default hyperparameters α= 10−5
andγ= 0.05, as defined in Graves et al. (2017).
Hyperparameter Value
optimizer ADAM (Kingma & Ba, 2015)
learning-rate 0.0025
annealing Yes
num-steps 128
timesteps 100,000
seeds 5
gamma 0.99
GAE-lambda 0.95
minibatches 4
epochs 4
advantage-norm Yes
clip-value-loss Yes
clip-coeff 0.2
entropy-coeff 0.01
vf-coeff 0.5
max-grad-norm 0.5
target-kl NoActor Critic
LINEAR (2,64) LINEAR (2,64)
ORTHO INIT() ORTHO INIT()
TANH() TANH()
LINEAR (64,64) LINEAR (64,64)
ORTHO INIT() ORTHO INIT()
TANH() TANH()
LINEAR (64,2) LINEAR (64,1)
Table 5: Details on the PPO hyperparameters ( left) and actor-critic architecture ( right ) used in the ADVERARIAL -SIPD experiments.
Policy and critic components, and the optimizer, are provided by PYTORCH (Paszke et al., 2019). Implementation and default
hyperparameters are derived from CLEAN RL(Huang et al., 2022b). These details remained constant throughout the rest of the
experiments.
21Published in Transactions on Machine Learning Research (09/2024)
B Extended Experiments Results
B.1 Value-Proportional Curriculum
0 100000 200000 300000 400000 500000
Step0.00.20.40.60.8returnsprojection
nowak-softmax
tscl-exp3s
nowak-simplexT woRooms
0 100000 200000 300000 400000 500000
Step0.00.10.20.30.4returnsprojection
nowak-softmax
tscl-exp3s
nowak-simplexFourRooms
0 100000 200000 300000 400000 500000
Step0.0000.0010.0020.0030.0040.005returnsprojection
nowak-softmax
tscl-exp3s
nowak-simplexSixRooms
(a) M INIGRID ROOMS
0 20000 40000 60000 80000 100000
Step0.00.20.40.60.81.0returnsprojection
shapley-simplex
shapley-softmax
nowak-softmax
tscl-exp3s
nowak-simplexCooperator
0 20000 40000 60000 80000 100000
Step1.0
0.8
0.6
0.4
0.2
0.0returnsprojection
shapley-simplex
shapley-softmax
nowak-softmax
tscl-exp3s
nowak-simplexDefector
0 20000 40000 60000 80000 100000
Step0.00.20.40.60.81.0returnsprojection
shapley-simplex
shapley-softmax
nowak-softmax
tscl-exp3s
nowak-simplexTitForT at
0 20000 40000 60000 80000 100000
Step1.0
0.5
0.00.51.0returnsprojection
shapley-simplex
shapley-softmax
nowak-softmax
tscl-exp3s
nowak-simplexWinLooseSwitch
0 20000 40000 60000 80000 100000
Step1.0
0.5
0.00.51.0returnsprojection
nowak-player4-softmax
shapley-simplex
shapley-softmax
tscl-exp3s
nowak-simplex
nowak-player4-simplexZD-Extortioner
(b) A DVERSARIAL -SIPD
Figure 5: We also investigated the prior-proportional curriculum in the target-unit setting. For each target unit, we allocate
to each training unit interactions proportional to their pre-computed values for each target. For the ADVERSARIAL -SIPD and
MINIGRID-ROOMS controlled their learning dynamics by presenting the units according to unordered andordered mechanisms
in Sec. 5. On each task, the value-proportional curriculum derived from the prospect priot outperforms TSCL (tscl-*-exp3s) . We
further investigate the reason for TSCL failures on this scenario.
22Published in Transactions on Machine Learning Research (09/2024)
B.2 TSCL Failures
0 20000 40000 60000 80000 100000
Step0.04
0.02
0.000.020.04returnsseed
2
0
4
3
1Cooperator
0 20000 40000 60000 80000 100000
Step0.08
0.06
0.04
0.02
0.000.020.04returnsseed
2
0
4
3
1Defector
0 20000 40000 60000 80000 100000
Step0.04
0.02
0.000.020.040.06returnsseed
2
0
4
3
1TitForT at
0 20000 40000 60000 80000 100000
Step0.10
0.05
0.000.050.10returnsseed
2
0
4
3
1WinLooseSwitch
0 20000 40000 60000 80000 100000
Step0.075
0.050
0.025
0.0000.0250.0500.075returnsseed
2
0
4
3
1ZD-Extortioner
(a) Individual Runs
Cooperator Defector TitForT at WinLoseSwicth ZD-Extortioner
u(ui,uj)
Cooperator
Defector
TitForT at
WinLoseSwicth
ZD-Extortionerunits [players]0.4557 -0.0608 -0.1017 -0.0850 -0.0588
-0.0652 0.4601 -0.0878 -0.0816 -0.0588
-0.0628 -0.0649 0.4191 -0.0468 -0.0619
-0.0641 -0.0712 -0.0323 0.4254 -0.0692
-0.0628 -0.0608 -0.0681 -0.0711 0.4620units [players]
Cooperator Defector TitForT at WinLoseSwicth ZD-Extortioner
u(ui,uj)
Cooperator
Defector
TitForT at
WinLoseSwicth
ZD-Extortionerunits [players]-0.1139 0.0377 -0.0450 -0.0293 0.0024
0.0250 -0.7227 0.1209 0.0360 0.0885
0.0048 0.0335 0.2605 -0.0532 0.0999
-0.0132 -0.0012 -0.0527 0.1558 0.0538
-0.0261 0.1311 0.0838 0.0023 -0.5046units [players]
Cooperator Defector TitForT at WinLoseSwicth ZD-Extortioner
u(ui,uj)
Cooperator
Defector
TitForT at
WinLoseSwicth
ZD-Extortionerunits [players]0.5482 -0.0371 -0.1405 -0.1246 -0.0883
-0.0300 -0.1302 0.0290 -0.0329 -0.0074
-0.1223 -0.0229 0.6262 -0.0860 0.0363
-0.1008 -0.0617 -0.0846 0.5546 -0.0168
-0.0626 0.0285 0.0099 -0.0527 0.0245units [players]
Cooperator Defector TitForT at WinLoseSwicth ZD-Extortioner
u(ui,uj)
Cooperator
Defector
TitForT at
WinLoseSwicth
ZD-Extortionerunits [players]0.7204 -0.0572 -0.2059 -0.1400 -0.1430
0.0857 -0.9349 0.1483 0.0926 0.0473
-0.2046 0.0338 0.8594 -0.1789 0.1765
-0.1973 -0.0495 -0.1510 0.7829 0.0591
-0.0577 0.1595 0.0917 -0.0108 -0.5499units [players]
Cooperator Defector TitForT at WinLoseSwicth ZD-Extortioner
u(ui,uj)
Cooperator
Defector
TitForT at
WinLoseSwicth
ZD-Extortionerunits [players]0.3683 -0.0074 -0.1778 -0.1448 -0.0793
-0.0310 -1.0074 0.1787 0.0201 0.1211
-0.0730 0.0218 0.9485 -0.1594 0.1429
-0.1074 -0.0581 -0.1619 0.7524 0.0474
-0.1327 0.1961 0.1124 -0.0525 -0.5713units [players]
(b)VPOP M ETRIC
Figure 6: To understand the failure modes of TSCL onADVERSARIAL -SIPD , we represented the individual runs (i.e., each of
the five seeds) on every target unit. TSCL (tscl-*-exp3s) (top row) is extremely brittle, unstable, and generally not robust to units
interference. We surmise that these failures are related to the exploration-exploitation dilemma. Exploratory steps presenting a
negatively-valued unit are hard to overcome (forgetting dynamics). This issue requires further investigation, and we defer it to future
work.
23Published in Transactions on Machine Learning Research (09/2024)
0 100000 200000 300000 400000 500000
Step0.010
0.005
0.0000.0050.0100.0150.020returnsseed
2
0
4
3
1T woRooms
0 100000 200000 300000 400000 500000
Step0.015
0.010
0.005
0.0000.0050.010returnsseed
2
0
4
3
1FourRooms
0 100000 200000 300000 400000 500000
Step0.04
0.02
0.000.020.04returnsseed
2
0
4
3
1SixRooms
T wo-Rooms FourRooms SixRooms
u(ui,uj)
T wo-Rooms
FourRooms
SixRoomsunits [grid]0.6089 -0.0121 -0.1817
-0.0501 0.0009 0.0217
-0.1364 0.0011 -0.1817units [grid]
T wo-Rooms FourRooms SixRooms
u(ui,uj)
T wo-Rooms
FourRooms
SixRoomsunits [grid]0.0629 0.0891 -0.0535
-0.0110 0.0891 -0.0289
-0.0110 -0.0713 -0.0535units [grid]
T wo-Rooms FourRooms SixRooms
u(ui,uj)
T wo-Rooms
FourRooms
SixRoomsunits [grid]0.0000 0.0277 -0.0111
0.0000 0.0277 -0.0111
0.0000 -0.0221 -0.0111units [grid]
(a) M INIGRID ROOMS
Figure 7: We found that TSCL presents a similar problem in MINIGRID ROOMS . When actions (units) need to be almost deterministi-
cally drawn for several steps, and other actions (units) have negative interference with the target, TSCL is unable to find a stable and
robust solution to the p roblem.
24