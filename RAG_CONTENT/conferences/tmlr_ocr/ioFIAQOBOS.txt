Published in Transactions on Machine Learning Research (11/2023)
Learning to reconstruct signals
from binary measurements alone
Julián Tachella julian.tachella@cnrs.fr
Physics Laboratory
CNRS & École Normale Supérieure de Lyon
Laurent Jacques laurent.jacques@uclouvain.be
ICTEAM
UCLouvain
Reviewed on OpenReview: https: // openreview. net/ forum? id= ioFIAQOBOS
Abstract
Recent advances in unsupervised learning have highlighted the possibility of learning to
reconstruct signals from noisy and incomplete linear measurements alone. These methods
play a key role in medical and scientific imaging and sensing, where ground truth data is
often scarce or difficult to obtain. However, in practice measurements are not only noisy
and incomplete but also quantized. Here we explore the extreme case of learning from
binary observations and provide necessary and sufficient conditions on the number of mea-
surements required for identifying a set of signals from incomplete binary data. Our results
are complementary to existing bounds on signal recovery from binary measurements. Fur-
thermore, we introduce a novel self-supervised learning approach, which we name SSBM,
that only requires binary data for training. We demonstrate in a series of experiments with
real datasets that SSBM performs on par with supervised learning and outperforms sparse
reconstruction methods with a fixed wavelet basis by a large margin.
1 Introduction
Continuous signals have to be quantized in order to be represented digitally with a limited number of
bits in a computer. In many real-world applications, such as radar (Alberti et al., 1991), wireless sensor
networks (Chen & Wu, 2015), and recommender systems (Davenport et al., 2014), the measured data is
quantized with just a few bits per observation. The extreme case of quantization corresponds to observing a
single bit per measurement. For example, single-photon detectors record the presence or absence of photons
at each measurement cycle (Kirmani et al., 2014), and recommendation systems often observe a binary
measurement of users’ preferences only ( e.g., via thumbs up or down).
The binary sensing problem is formalized as follows: we observe binary measurements y∈{− 1,1}mof a
signalx∈X⊂ Sn−1with unit norm1via the following forward model
y= sign (Ax) (1)
whereA∈Rm×nis a linear forward operator. Recovering the signal from the measurements is an ill-posed
inverse problem since there are many signals x∈Sn−1that are consistent with a given measurement vector y.
Moreover, oftenthemeasurementmatrixisincomplete m<n,e.g., asinone-bitcompressedsensing(Jacques
et al., 2013), which makes the signal recovery problem even more challenging.
It is possible to obtain a good estimation of xdespite the binary quantization, if the set of plausible signals
Xis low-dimensional (Bourrier et al., 2014), i.e., if it occupies a small portion of the ambient space Sn−1.
1Note that the sensing model in (1) provides no information about the norm of x, so it is commonly assumed that signals
verify∥x∥= 1.
1Published in Transactions on Machine Learning Research (11/2023)
unknown
signal set
self-supervised trainingbinary dataset
sensing
device(s)
linear inverse
Figure 1: We propose a method for learning to reconstruct binary measurement observations, using only
the binary observations themselves for training. The learned reconstruction function can discover unseen
patterns in the data (in this case the clothes of fashionMNIST - see the experiments in Section 5), which
cannotberecognizedinthestandardlinearreconstructions(nolearning). Wealsoprovidetheoreticalbounds
that characterize how well we can expect to learn the set of signals from binary measurement data alone.
A popular approach is to assume that Xis a single linear subspace or a union of subspaces (Jacques et al.,
2013), imposing sparsity over a known dictionary. For example, the well-known total variation regularization
assumes that the gradients of the signal are sparse (Rudin et al., 1992). However, in real-world settings,
the set of signals Xis generally unknown, and sparsity assumptions on an arbitrary dictionary yield a
loose description of the true set X, negatively impacting the quality of reconstructions obtained under this
assumption. This limitation can be overcome by learning the reconstruction mapping y∝⇕⊣√∫⊔≀→x(e.g., with
a deep neural network) directly from Npairs of measurements and associated signals— i.e., a supervised
learning scenario with a labeled dataset {(yi,xi)}N
i=1withNassumed sufficiently large. While this learning-
based approach generally obtains state-of-the-art performance, it is often impractical since it can be very
expensive or even impossible to obtain ground-truth signals xifor training. For example, recommender
systems generally do not have access to high-resolution user ratings on all items for training.
In this paper, we investigate the problems of identifying the signal set and learning reconstruction mapping
usingadatasetofbinarymeasurementsonly {yi}N
i=1. Inthissetting, ifthemeasurementprocessisincomplete
m<n, the matrix Ahas a non-trivial nullspace and there is no information in the measurement data about
the set of signals Xin the nullspace (Chen et al., 2021). As a consequence, there is not enough information
for learning the reconstruction function either. For example, the trivial pseudo-inverse reconstruction f(y) =
A⊤(AA⊤)−1yis perfectly consistent with the binary measurements, i.e.,sign (Af(y)) =y, but is generally
far from being optimal (Boufounos et al., 2015).
Here we show that it is still possible to (approximately) identify the signal set and learn to reconstruct the
binary measurements, if the measurement operator varies across observations, i.e.,
yi= sign (Agixi) (2)
where each signal xiis observed via one out of Goperatorsgi∈{1,...,G}, andi= 1,...,N. This sensing
assumption holds in various practical applications, where signals are observed through different operators
(e.g., recommendation systems access ratings about a different set of items for each user) or through an
operator which changes through time ( e.g., a sensor that changes its calibration). Moreover, this assumption
is also valid for the case where we obtain binary measurements via a single operator A, but the setXis
knowntobeinvarianttoagroupofinvertibletransformations {Tg}G
g=1, suchastranslationsorrotations. The
invarianceofXprovidesaccesstomeasurementsassociatedwithasetof(implicit)operators {Ag=ATg}G
g=1,
as we have that
y= sign/parenleftbig
ATgT−1
gx/parenrightbig
= sign (ATgx′) (3)
withx′=T−1
gx∈Xfor allg= 1,...,G. This observation has been exploited to perform fully unsu-
pervised learning on various linear inverse problems, such as magnetic resonance imaging and computed
tomography (Chen et al., 2021; 2022; Tachella et al., 2023).
2Published in Transactions on Machine Learning Research (11/2023)
Assumption onX⊆Sn−1None None boxdim<k
Assumption on Ag∈Rm×n,g∈[G]rank [A⊤
1,...,A⊤
G]<n None Gaussian
Identification error bounds δ>1 δ≳n
mGδ≲k+n/G
mlognm
k+n/G
Section Section 3.1 Section 3.1 Section 3.2
Table1: Summaryoftheglobalmodelidentificationerror δboundspresentedinthispaper. Theidentification
errorδcorresponds to the maximal error of the optimal estimation of the signal set from binary measurement
data alone (see Definition 3.1). The bounds depend on the size of the signals n, the number of binary
measurement operators Gwithmmeasurements, and the dimension of the signal set k.
The problem of recovering a signal from binary measurements under the assumption of a known signal set
has been extensively studied in the literature (Goyal et al., 1998; Jacques et al., 2013; Oymak & Recht, 2015).
These works provide practical bounds that characterize the recovery error as a function of the number of
measurements mfor different classes of signal sets. However, they assume that the signal set is known (or
that there is enough ground-truth training data to approximate it), which is not often the case in real-world
scenarios. Here we investigate the best approximation of the signal set that can be obtained from the binary
observations. This approximation lets us understand how well we can learn the reconstruction function from
binary data. To the best of our knowledge, the model identification problem has not been yet addressed,
and we aim to provide the first answers to this problem here. The main contributions of this paper are:
•We show that for any Gsensing matrices A1,...,AG∈Rm×nand any dataset size N, there exists
a signal set whose identification error (precisely defined in Section 3) from binary measurements
cannot decay faster than O(n
mG)whenmincreases.
•We prove that, if each operator Ag,g∈{1,...,G}, has iid Gaussian entries (a standard construction
in one-bit compressed sensing), it is possible to estimate a k-dimensional2signal set up to a global
error ofO(k+n/G
mlognm
k+n/G)with high probability.
•We determine the sample complexity of the related unsupervised learning problem, i.e., we find that,
forGoperators with Gaussian entries, the number of distinct binary observations for obtaining the
best possible approximation of a k-dimensional signal set XisN=O/parenleftbig
G(m√n
k)5k/parenrightbig
with controlled
probability, which reduces to N=O/parenleftbig
G(m
k)k/parenrightbig
ifXis a union of k-dimensional subspaces.
•We introduce a Self-Supervised learning loss for training reconstruction networks from Binary Mea-
surement data alone (SSBM), and show experimentally that the learned reconstruction function
outperforms classical binary iterative hard thresholding (Jacques et al., 2013) and performs on par
with fully supervised learning on various real datasets.
A summary of the model identification bounds presented in this paper is shown in Table 1.
Related Work
Unsupervised learning in inverse problems. Despite providing very competitive results, most deep
learning-based solvers require a supervised learning scenario, i.e., they need measurements and signal pairs
{(yi,xi)}, a labeled dataset, in order to learn the reconstruction function y∝⇕⊣√∫⊔≀→x. A first step to overcome
this limitation is due to Noise2Noise (Lehtinen et al., 2018), where the authors show that it is possible to
learn from only noisy data if two noisy realizations of the same signal {(xi+ni,xi+n′
i)}are available for
training. This approach has been extended to linear inverse problems with pairs of measurements {(Agixi+
ni,Ag′
ixi+n′
i)}(Yaman et al., 2020; Liu et al., 2020). The equivariant imaging framework (Chen et al.,
2021; 2022) shows that learning the reconstruction function from unpaired measurement data {Axi+ni}of a
single incomplete linear operator Ais possible if the signal model is invariant to a group of transformations.
This approach can also be adapted to the case where the signal model is not invariant, but measurements are
2The definition of dimension used in this paper is the upper box-counting dimension defined in Section 2.
3Published in Transactions on Machine Learning Research (11/2023)
Figure 2: Geometry of the 1-bit signal recovery problem with m= 5andn= 3.Left:The binary sensing
operator sign (A·)defines a tessellation of the sphere into multiple consistency cells , which are defined as
all vectors x∈S2associated with the same binary code. The consistency cell associated with a given
measurement yis shown in green. Each red line is a great circle defined by all points of S2perpendicular to
one row of A.Middle: If the signal set consists of all vectors in the sphere, i.e.,X=S2, the center of the
cell is the optimal reconstruction ˆf(y)(depicted with a blue cross) and the recovery error (denoted by δ) is
given by the radius of the cell. Right:If the signal set (depicted in black) occupies only a small subset of
S2,i.e., it has a small box-counting dimension, the optimal reconstruction corresponds to the center of the
intersection between the signal set and the consistency cell, and the resulting signal recovery error is smaller.
obtained via many different operators {Agixi+ni}(Tachella et al., 2022). Necessary and sufficient conditions
for learning in these settings are presented in Tachella et al. (2023), however under the assumption of linear
observations (no quantization). Here we extend these results to the non-linear binary sensing problem with
an unsupervised dataset with multiple operators {sign (Agixi)}andgi∈{1,...,G}, or with a single operator
and a group-invariant signal set {sign (Axi)}.
Quantized and one-bit sensing. Reconstructing signals from one-bit compressive measurements is a
well-studied problem (Goyal et al., 1998; Jacques et al., 2013; Oymak & Recht, 2015; Baraniuk et al., 2017),
both in the (over)complete case m≥n(Goyal et al., 1998), and in the incomplete setting m < n, either
under the assumption that the signals are sparse (Jacques et al., 2013), or more generally, that the signal set
has small Gaussian width (Oymak & Recht, 2015). Some of these results are summarized in Section 2. The
theoretical bounds presented in this paper complement those of signal recovery bounds from quantized data,
as they characterize the fundamental limitations of model identification from binary measurement data.
One-bit matrix completion and dictionary learning. Matrix completion consists of inferring missing
entries of a data matrix Y= [y1,...,yN], whose columns can be seen as partial observations of signals xi,
i.e.,yi= sign (Agixi)where the operators Agiselect a random subset of mentries of the signal xi. In order
to recover the missing entries, it is generally assumed that the signals xi(the columns of X= [x1,...,xN])
belong to a k-dimensional subspace with k≪n. Davenport et al. (2014) solve this learning problem via
convex programming and present theoretical bounds for the reconstruction error.
Zayyani et al. (2015) present an algorithm that learns a dictionary ( i.e., a union of k-dimensional subspaces)
from binary data alone in the overcomplete regime m>n. Rencker et al. (2019) presents a similar dictionary
learning algorithm with convergence guarantees. In this paper, we characterize the model identification error
for the larger class of low-dimensional signal sets, which includes subspaces and the union of subspaces as
specialcases. Moreover, weproposeaself-supervisedmethodthatlearnsthereconstructionmappingdirectly,
avoiding an explicit definition ( e.g., a dictionary) of the signal set.
4Published in Transactions on Machine Learning Research (11/2023)
2 Signal Recovery Preliminaries
We begin with some basic definitions related to the one-bit sensing problem. The diameter of a set is
defined as diam(S) = supu,v∈S∥u−v∥, and the radius is defined as half the diameter. Each row ai∈Rn
in the operator Adivides the unit sphere Sn−1into two hemispheres, i.e.,{x∈Sn−1:a⊤
ix≥0}and
{x∈Sn−1:a⊤
ix < 0}. Considering all rows, the operator sign (A·)defines a tesselation ofSn−1into
consistency cells , where each cell is composed of all the signals that are associated with a binary code y,
i.e.,{x∈Sn−1: sign (Ax) =y}. The radius and number of consistency cells play an important role in the
analysis of signal recovery and model identification. Figure 2 illustrates the geometry of the problem for
n= 3andm= 5.
The problem of recovering a signal from one-bit compressed measurements with a known signal set has been
well studied (Goyal et al., 1998; Jacques et al., 2013; Oymak & Recht, 2015; Baraniuk et al., 2017). These
works characterize the maximum estimation error across all signals obtained by an optimal reconstruction
function ˆf,i.e.,
δ= max
x∈X∥x−ˆf(sign (Ax))∥ (4)
asafunctionofthenumberofmeasurementsandcomplexityofthesignalmodel. Fromageometricviewpoint
(see Figure 2), the optimal reconstruction function with respect to the norm ∥·∥is given by the centroid
(with respect to the same norm ∥·∥) of the intersection between the consistency cell associated with the
measurement y=y(x) = sign (Ax),i.e.,Sy:={u∈Sn−1:y= sign (Au)}, and the signal set X,i.e.,
ˆf(y) =centroid (Sy∩X). (5)
while the maximum reconstruction error is given by the intersection with maximal radius, that is
δ= max
x∈Xradius (Sy(x)∩X). (6)
In the overcomplete case m > n, assuming that all unit vectors are plausible signals, i.e.,X=Sn−1,
the mean reconstruction error δis given by the consistency cell with maximal radius, which scales asn
m
(see Proposition 5). The optimal rate is achieved by measurement consistent reconstruction functions, i.e.,
those verifying y= sign (Af(y))(Goyal et al., 1998).
In the incomplete case m < n, non-trivial signal recovery is only possible if the set of signals occupies a
low-dimensional subset of the unit sphere Sn−1(Oymak & Recht, 2015). For example, a common assumption
is thatXis the set of k-sparse vectors (Jacques et al., 2013). In this paper, we characterize the class of
low-dimensional sets using a single intuitive descriptor, the box-counting dimension. The upper box-counting
dimension (Falconer, 2004, Chapter 2) is defined for a compact subset S⊂Rnas
boxdim (S) = lim sup
ϵ→0+logN(S,ϵ)
log 1/ϵ(7)
whereN(S,ϵ)is the minimum number of closed balls of radius ϵwith respect to the norm ∥·∥that are
required to cover S. This descriptor has been widely adopted in the inverse problems literature (Puy et al.,
2017; Tachella et al., 2023), and it captures the complexity of various popular models, such as smooth
manifolds (Baraniuk & Wakin, 2009) and union of subspaces (Blumensath & Davies, 2009; Baraniuk et al.,
2017). For example, the set of (k+ 1)-sparse vectors with unit norm has a box-counting dimension equal
tok. The upper box-counting dimension is particularly useful to obtain an upper bound on the covering
number of a set: if boxdim (X)<k, there exists a set-dependent constant ϵ0∈(0,1
2)for which
N(X,ϵ)≤ϵ−k(8)
holds for all ϵ≤ϵ0(Puy et al., 2017). The following theorem (proved in Appendix B) exploits this fact to
provide a bound on the number of measurements needed for recovering a signal with an error smaller than
δfrom generic binary observations.
5Published in Transactions on Machine Learning Research (11/2023)
Figure 3: Illustration of the model identification problem from binary measurements with n= 3,m= 4, and
G= 3. A signal set with box-counting dimension 1 is depicted in black. The red lines define the frontiers
of the consistency cells associated with operators A1,...,A 3.From left to right: The signal set, the
estimation of the signal set associated with A1,...,A 3and the overall estimate ˆX.
Theorem 1. LetAbe a matrix with iid entries sampled from a standard Gaussian distribution and assume
thatboxdim (X)< k, such that N(X,ϵ)≤ϵ−kfor allϵ < ϵ 0withϵ0∈(0,1
2). Forδ≤min{30√nϵ0,1
2}, if
the number of measurements verifies
m≥4
δ/parenleftbig
2klog30√n
δ+ log1
ξ/parenrightbig
(9)
then for all x,s∈X, we have that
sign (Ax) = sign (As) =⇒ ∥x−s∥<δ (10)
with probability greater than 1−ξ.
This result extends Theorem 2 in Jacques et al. (2013), which holds for k-sparse sets only, to general low-
dimensional sets and is included in Appendix B. For example, if Xis the intersection of L(s+1)-dimensional
subspaces with the unit sphere, Theorem 1 holds with constant ϵ0= (3sL)−1
k−sandk>s(Vershynin, 2018,
Chapter 4.2). This theorem tells us that we can recover sparse signals from binary measurements up to an
error of
O(k
mlognm
k)
which is sharp, up to the logarithmic factor (Jacques et al., 2013). Oymak and Recht (Oymak & Recht, 2015)
present a similar result, stated in terms of the Gaussian width3of the signal set instead of the box-counting
dimension.
3 Model Identification from Binary Observations
In this section, we study how well we can identify the signal set from binary measurement data associated
withGdifferent measurement operators A1,...,AG∈Rm×n. We focus on the problem of identifying the
setXfrom the binary sets {sign (AgX)}G
g=1. In practice, we observe a subset of each binary set sign (AgX),
however, in Section 3.4 we show that the number of elements in each of these sets is controlled by the
box-counting dimension of X, which is typically low in real-world settings (Hein & Audibert, 2005).
We start by analyzing how the different operators provide us with information about X. Each forward
operatorAgconstrains the signal space by the following set
ˆXg={v∈Sn−1:∃xg∈X,sign (Agv) = sign (Agxg)}. (11)
Eachset ˆXgisthuscomposedofallunitvectors vthatareconsistent withatleastonepoint xgofXaccording
to the binary mapping sign (Ag·). We thus conclude that ˆXgis essentially a dilationofX—and we clearly
haveX ⊂ ˆXg—whose extension is locally determined by specific cells of sign (Ag·). A three-dimensional
example with m= 4measurements and G= 3operators is presented in Figure 3. Note that, for a given
binary mapping sign (Ag·), each cell is characterized by one binary vector in the range of this mapping, so
3The Gaussian width of a set Sis defined as Es{supx∈Sx⊤s}wheresis distributed as a standard Gaussian vector.
6Published in Transactions on Machine Learning Research (11/2023)
that, as shown in this figure, all cells provide a different tesselation of Sn−1whose size and dimension will
play an important role in our analysis.
Since each ˆXgis a dilation ofX, we can infer the signal set from the following intersection
ˆX:=G/intersectiondisplay
g=1ˆXg, (12)
which can be expressed concisely as
ˆX=/braceleftbig
v∈Sn−1:∃x1,...,xG∈X,sign (Agv) = sign (Agxg),∀g= 1,...,G/bracerightbig
. (13)
Due to the binary quantization, the inferred set will be larger than the true set, i.e.,X⊂ ˆX. However, we
will show that it is possible to learn a slightly largersignal set, defined in terms of a global identification
errorδ>0,i.e., the openδ-tube
Xδ={v∈Sn−1: inf
x∈X∥x−v∥<δ} (14)
such that the inferred set is contained in it, i.e.,ˆX ⊂Xδ. We define the model identification error as the
smallestδsuch that ˆX⊂Xδholds:
Definition 3.1 (Model identification error) .The identification error of a signal set X⊂Sn−1from binary
sets{sign (AgX)}G
g=1is defined as min{δ≥0 :ˆX⊆Xδ}.
For our developments to be valid, we will further assume that Xis not too dense over Sn−1so that two
tubes ofXwith two distinct radii are distinct.
Assumption 1. The setXis closed and there exists a maximal radius 0<δ0<2for whichXδ⊊Xδ0for
any0<δ<δ 0.
This assumption amounts to saying that there exists at least one open ball in Sn−1that does not belong to
X ⊂Sn−1. For instance,X=Sn−1does not verify this assumption, and X=Sn−1∩{x∈Rn:x1≥0}
verifies it for δ0≤√
2sinceXδ=Sn−1for anyδ≥√
2. The next subsections provide lower and upper
bounds for δ.
3.1 A Lower Bound on the Identification Error
We first aim to find a lower bound on the best δachievable via the following oracle argument: if we had
oracle access to Gmeasurements of each point xinXthrough each of the Gdifferent operators, we could
stack them together to obtain a larger measurement operator, defined as

y1
...
yG
= sign/parenleftbig¯Ax/parenrightbig
with ¯A=
A1
...
AG
∈RmG×n. (15)
This oracle measurement operator provides a refined approximation of the signal set, specified as
ˆXoracle ={v∈Sn−1:∃x∈X,sign/parenleftbig¯Av/parenrightbig
= sign/parenleftbig¯Ax/parenrightbig
}, (16)
which is again a dilation of X.
Figure 4 shows an example with the oracle set ˆXoracle, which provides a better (or equal) approximation
of the signal set than (13), due to the fact that X ⊂ ˆXoracle⊆ˆXby the construction of these sets. As
the oracle estimate is composed of the cells associated with sign/parenleftbig¯A·/parenrightbig
which are intersected by the signal
set, the oracle approximation error depends on the diameter of the intersected cells. Given a certain oracle
tesselation of Sn−1, the worst estimate of Xis obtained when it intersects the largest cells in the tessellation.
The following proposition formalizes the intuition that the maximum consistency cell diameter— i.e., the
greatest distance separating two binary consistent vectors of Xaccording to ¯A—serves as a lower bound on
the model identification error δ.
7Published in Transactions on Machine Learning Research (11/2023)
Figure 4: Illustration of the oracle argument in the example of Figure 3. Left:The signal setX ⊂S2
is depicted in black. Middle: Cells intersected by the oracle system are indicated in green. Right:The
identified set ˆXis indicated in green, and is larger than the oracle counterpart.
Proposition 2. Given ¯A∈RmG×n, for any setX⊂Sn−1respecting Assumption 1 with 0<δ0<2, there
exists a rotation matrix R∈SO(n)such that the rotated set
X′={v∈Sn−1:v=Rx, x∈X} =RX (17)
verifies ˆX′
oracle̸⊂X′
δfor anyδ <min{d,δ0}where 0< d < 2is the largest cell diameter of the tesselation
induced by sign/parenleftbig¯A·/parenrightbig
.
Proof.Givenδ<δ 0, the proof consists in choosing an appropriate rotation matrix, such that we can find a
pointvwhich belongs to the oracle estimate ˆX′
oracleof the rotated set X′, but doesn’t belong to the δ-tube
X′
δof this set. From Assumption 1 and since the δ-tubeXδis open, there exists x∈Xandv̸∈Xδsuch
that∥x−v∥=δLetSdenote the largest cell in the tesselation of Sn−1induced by sign/parenleftbig¯A·/parenrightbig
, such that
d=diam (S). Ifδ < d, we can always pick a rotation R∈SO(n)such that both x′=Rxandv′=Rv
belong toS. Asx′∈S,X′intersectsSand we have that S⊆ˆX′
oracle, and thus that v′∈ˆX′
oracle.
In words, Proposition 2 shows that we can rotate any signal set Xsuch that it intersects the largest con-
sistency cell in the tesselation, obtaining a model identification error that is proportional to the maximum
cell diameter. The rotation is used to remove the best-case scenarios where the signal set only intersects
consistency cells that are smaller than the largest one.
In the rest of this subsection, we focus on bounding the maximum cell diameter, as it is directly related to
the model identification error through Proposition 2. We start with the following proposition which shows
that, if the stacked matrix is rank-deficient, all cells have the maximum possible diameter.
Proposition 3. Consider the tessellation defined by sign/parenleftbig¯A·/parenrightbig
with ¯A∈RmG×n. If
rank(¯A)<n (18)
all the cells in the tessellation have a diameter equal to 2.
Proof.If¯Ahas a rank smaller than n, it has a non-trivial nullspace. Let v∈Sn−1be an element in the
nullspace with unit norm. Consider a cell associated with the code sign/parenleftbig¯Ax/parenrightbig
for somex∈Rninside the
complement of this nullspace ( i.e., in the range of ¯A⊤). The pointsx+v
∥x+v∥,x−v
∥x−v∥∈Sn−1belong to this cell
since they share the same code. As ∥x±v∥=/radicalbig
∥v∥2+∥x∥2due to orthogonality, the distance between
these two points is
2∥v∥/radicalbig
∥v∥2+∥x∥2=2/radicalbig
1 +∥x∥2(19)
which tends to 2as∥x∥goes to zero, without modifying the cell code sign/parenleftbig¯Ax/parenrightbig
.
8Published in Transactions on Machine Learning Research (11/2023)
This result provides a practical necessary condition for model identification, which is summarized in the
following corollary:
Corollary 4. A necessary condition for the tesselation defined by sign/parenleftbig¯A·/parenrightbig
to have consistency cells with a
diameter smaller than 2 is that there are at least
m≥n/G
measurements per operator.
This proposition tells us that n/Gmeasurements are necessary in order to obtain non-trivial cell diameters,
and thus to obtain a non-trivial estimation of X.
Moreover, in practice, it is possible to compute the rank of the stacked matrix ¯Avia numerical approxima-
tions. The following theorem provides a more refined characterization of the oracle error for m≥n/G:
Proposition 5. Consider the tessellation defined by sign/parenleftbig¯A·/parenrightbig
with ¯A∈RmG×n. The largest cell in the
tessellation has a diameter of size at least2
3n
mG.
Proof.According to Thao & Vetterli (1996, Theorem A.7), the maximum number of cells C¯Ainduced by a
tessellation defined by sign/parenleftbig¯A·/parenrightbig
with ¯A∈RmG×ncan be upper bounded as
C¯A≤/parenleftbiggmG
n/parenrightbigg
2n.
As/parenleftbigmG
n/parenrightbig
≤(emG
n)n, we have that C¯A≤(2emG
n)n. We can inscribe all cells into spherical caps Si4of radius
δ/2, whereδis the maximum cell diameter. As shown in (Ball et al., 1997, Lemma 2.3), a spherical cap
of radiusδ/2has measure bounded by σ(Si)≤(δ
4)n−1σn−1whereσn−1is the measure of Sn−1. Since the
tessellation covers the unit sphere Sn−1, we have that Sn−1⊆∪C¯A
i=1Siand thus
/summationtextC¯A
i=1σ(Si)≥σn−1⇒(2emG
n)n(δ
4)nσn−1≥σn−1⇒δ≥2
3n
mG.
Remark The upper bound in this proposition is tight in the sense that there exist matrices that attain
this bound. As a special case of Theorem 1 with mG > n measurements and the box-counting dimension
ofXset ton, for anmG×nGaussian random matrix ¯A, and by choosing the minimal number of required
measurements in the condition of this theorem, we have with high probability that the largest cell of sign/parenleftbig¯A·/parenrightbig
has a diameter that decays like O(n
mG)up to log factors. By a standard boosting argument5, it thus means
that there exists a mG×nmatrix ¯Awith the same consistency cell diameter decay.
As stated in the following corollary, Proposition 5 shows that the model identification error cannot decrease
faster with the number of measurements and operators than O(n
mG), since the largest cell in any oracle
tesselation has a diameter of at least2
3n
mG.
Corollary 6. GivenGoperatorsA1,...,AG∈Rm×n, and any setX⊂Sn−1verifying Assumption 1 with
0<δ0<2, for any 0<δ < min(δ0,2
3n
mG), there exists a rotation Rsuch that the inferred signal set ˆX′of
X′=RXis not included in X′
δ,i.e.,ˆX′̸⊂X′
δ.
Proof.IfX⊂Sn−1respects Assumption 1 with 0<δ0<2, and ˆXoracleand ˆXare the oracle set associated
with ¯Aand the inferred set of X′, respectively, then, as derived previously, we know that X⊂ ˆXoracle⊂ˆX.
According to Proposition 2 and Proposition 5, there exists a rotation Rsuch that ˆX′
oracle =RˆXoracle̸⊂
X′
δ=RXδfor0< δ < min(δ0,2
3n
mG). Therefore, from the inclusion above, we thus see that there exists
ˆX′=RˆX̸⊂X′
δ.
4A spherical cap of radius raround a point v∈Sn−1is defined as{x∈Sn−1:∥x−v∥<r}.
5Assuming a Gaussian matrix will fail to have the desired cell size with probability at most ξ <1, the probability that 1
out ofrindependent Gaussian matrices will fail to have this property is at least 1−ξr, which can be made arbitrarily high by
increasingr.
9Published in Transactions on Machine Learning Research (11/2023)
At this stage, it is natural to ask if the condition in Corollary 4 is sufficient to upper bound the model
identificationerror δ. Theanswerisnegativesince, forcertainfamiliesofoperators, themaximumconsistency
cell associated with sign/parenleftbig¯A·/parenrightbig
does not decrease with the number of measurements mor operators G, as
illustrated by the following example inspired by a case considered in (Plan & Vershynin, 2013, Sec. 1.1).
Example 3.1. Consider the operators with binary6entriesA1,...,AG∈{− 1,1}m×n. Letxλ=e1+λe2∈
Rnwhereei∈Sn−1is theith canonical vector and λis a scalar. Due to the quantization of the operators,
we have that sign/parenleftbig¯Axλ/parenrightbig
= sign/parenleftbig¯Axλ′/parenrightbig
for any value of λ,λ′∈(−1,1). Thus, there exists a cell in the oracle
tesselation associated with sign/parenleftbig¯A·/parenrightbig
which contains the set of points {xλ
∥xλ∥}λ∈(−1,1)and thus has a diameter
equal to√
2, independently of the values of mandG.
In the next subsection, we obtain an upper bound on the model identification error which overcomes this
pathological example by sampling the operators from a continuous random distribution.
3.2 A Sufficient Condition for Model Identification
We now seek a sufficient condition on the number of measurements per operator that guarantees the iden-
tification ofXup to a global error of δ. As with the sufficient conditions ensuring signal recovery (see
Section 2), we assume that Xis low-dimensional to provide a bound that holds with high probability if the
entries of the operators are sampled from a Gaussian distribution.
Theorem 7. Given the operators A1,...,AG∈Rm×nwith entries i.i.d. as a standard Gaussian distribution,
a low-dimensional signal set X, with boxdim (X)<k, such that N(X,ϵ)≤ϵ−kfor allϵ<ϵ 0withϵ0∈(0,1
2).
For0< δ≤min{18√nϵ0,1}and some failure probability 0< ξ < 1, if the number of measurements per
operator verifies
m≥4
δ/bracketleftbig
(k+n
G) log54√n
δ+1
Glog1
ξ/bracketrightbig
(20)
then with probability at least 1−ξ, we have that ˆX⊆Xδ.
TheproofisincludedinAppendixC.Theorem7providesaboundon δ,i.e., howpreciselywecancharacterise
the signal setX, which we can compare with the lower bound in Proposition 5. From (20) we have that (see
Appendix C for a detailed derivation),
δ=O(k+n/G
mlognm
k+n/G). (21)
The bound in (21) is consistent with existing model identification bounds in the linear setting (Tachella
et al., 2023), which require m>k +n/Gmeasurements per operator for uniquely identifying the signal set.
3.3 Learning to Reconstruct
The best reconstruction function ˆfthat can be learned from binary measurements alone can be defined as
a function of the identified set ˆX, as defined in (5):
ˆf(y) =centroid (Sy∩ˆX) (22)
for a binary input ywith associated consistency cell Sy={v∈Sn−1: sign (Av) =y}. The reconstruction
error of ˆfis lower bounded by the radius of the set Sy∩ˆX. This error must be larger than the error of
a reconstruction function that has full knowledge about the signal set X. Intuitively, if we have a large
model identification error, ˆXwill be a bad approximation of Xand thus ˆfwill obtain large reconstruction
errors. The following proposition formalizes this intuition, showing that the reconstruction error of ˆfis lower
bounded by the model identification error.
Proposition 8. GivenGoperatorsA1,...,AG∈Rm×nand a setX⊂Sn−1with model identification error
equal toδ, there exist points xg∈Xforg= 1,...,Gsuch that the reconstruction error is
∥ˆf(sign (Agxg))−xg∥≥δ/2,
6This example can be generalized to operators with entries belonging to a discrete set Q, and show that there exist cells
with diameter equal to√
∆where ∆is the minimum distance between two elements in Q.
10Published in Transactions on Machine Learning Research (11/2023)
where ˆfis the optimal reconstruction function that can be learned from the measurement data
{sign (AgX)}G
g=1, as defined in (22).
Proof.Following Definition 3.1 of model identification error, there exists a point ˆx∈ˆXsuch that∥x−ˆx∥≥δ
for allx∈X. According to the construction of the inferred set ˆXin (13), there exist some x1,...,xG∈X
suchthat sign (Agˆx) = sign (Agxg)forallg= 1,...,G. Therefore, forany g∈{1,...,G}, thediameterofthe
setSsign(Agxg)∩ˆXis at least∥xg−ˆx∥since bothxgandˆxbelong to this set. As the optimal reconstruction
function outputs the centroid of the set (as defined in (22)), the reconstruction error of the point xgis at
least∥xg−ˆx∥/2≥δ/2.
Therefore, we can use the results on model identification developed in Section 3.1 to lower bound the
reconstruction error for the case where the function is learned from measurement data only. In particular,
combining this result with Corollary 6, we obtain that the (worst-case) reconstruction error should be larger
than1
3n
mG. It is worth noting that this result also holds for the case where we have a single operator and
group invariance, i.e., whenAg=ATgforg= 1,...,G.
An upper bound on the reconstruction error is harder to obtain. Unfortunately, Theorems 1 and 7 do
not automatically translate into a bound on the optimal reconstruction error of the reconstruction function
defined in (22). Theorem 7 implies that the optimal unsupervised reconstruction ˆf(sign (Agx))is at most
O(k+n/G
mlognm
k+n/G)away from the signal set X, but does not guarantee that it is close to x. Nonetheless,
we conjecture that this rate holds with high probability if the operators follow a Gaussian distribution:
Conjecture 9. Given binary measurements from the operators A1,...,AG∈Rm×nwith entries i.i.d. from
a standard Gaussian distribution, the optimal reconstruction function defined in (22)has a maximal recon-
struction error that is upper bounded as O(k+n/G
mlognm
k+n/G)with high probability.
Conjecture 9 hypothesizes that the optimal unsupervised reconstruction function should obtain a similar
performance than the supervised one, i.e.,O(k
mlognm
k)shown in Theorem 1, if the number of operators is
sufficiently large, i.e.,G>n/k . In the experiments in Section 5, we provide empirical evidence that supports
this hypothesis.
3.4 Sample complexity
We end our theoretical analysis of the unsupervised learning problem by bounding its sample complexity ,
i.e., we bound the number Nofdistinctbinary measurement vectors {yi}N
i=1that must be acquired for
obtaining the best approximation of the signal set Xfrom binary data.
Since we observe binary vectors y∈{± 1}m, there is a limited number of different binary observations. We
could naively expect to observe up to 2mdifferent vectors per measurement operator ( i.e., all possible binary
codes with mbits), requiring at most N≤G2msamples to fully characterize the best approximation of
the signal set ˆXdefined in (13). Fortunately, as already exploited in the proof of Proposition 5, this upper
bound can be significantly reduced if the signal set has a low box-counting dimension, as not all cells in the
tessellation will be intersected by the signal set (see Figure 3). We can thus obtain a better upper bound by
counting the number of intersected cells, denoted as |sign (AX)|.
IfXis the intersection of a single k-dimensional subspace with the unit sphere, (Thao & Vetterli, 1996,
Theorem A.7) tells us that, for any matrix A∈Rm×nwithm≥k, there are|sign (AX)|≤2k/parenleftbigm
k/parenrightbig
intersected
cells. More generally, if Xis a union of Lsubspaces, we have |sign (AX)|≤L2k/parenleftbigm
k/parenrightbig
. Thus, using the fact
that/parenleftbigm
k/parenrightbig
≤/parenleftbig3m
k/parenrightbigk, from theGmeasurement operators, we can observe up to
N≤GL(6m
k)k(23)
differentmeasurementvectors. However, thisresultonlyholdsforaunionofsubspaceshavingeachdimension
k. The following theorem extends this result to more general low-dimensional sets with small upper box-
counting dimension.
11Published in Transactions on Machine Learning Research (11/2023)
Theorem 10. Let the entries of A∈Rm×nbe sampled from a standard Gaussian distribution, and let
X ⊆Rnwith boxdim (X)< k. Ifk/(m√n)<min(ϵ0,1/2), then, in expectation, the cardinality of the
measurement set is bounded as
E|sign (AX)|≤/parenleftbigem√n
k/parenrightbigk. (24)
Moreover, given a failure probability 0<ξ< 1, if2k/(m√n)≤min(ϵ0,1/2), then, with probability 1−ξ, we
have
|sign (AX)|≤(1
ξ)4/parenleftbig3m√n
5k/parenrightbig5k. (25)
The proof is included in Appendix D. This result depends on the square root of the ambient dimension√n
due to the application of Lemma 11 and can be suboptimal for some signal sets. For example, the bound
in (23) avoids this dependency for the case where Xis a union of subspaces.
Inthesettingwhereweobservemeasurementsthrough Gindependentforwardoperators, wesumthenumber
of intersected cells for each operator, so that with probability exceeding 1−Gξfor0< ξ < 1(by a union
bound), the number of different binary measurement vectors is then bounded by
N=O/parenleftig
G/parenleftig
m√n
k/parenrightig5k/parenrightig
with a hidden multiplicative constant depending on ξ. Similarly to (23), this bound scales exponentially
only in the model dimension kbut not in the number of measurements mor operators G. In the setting of
a single operator and a k-dimensional invariant signal set, we have the upper bound N=O/parenleftig/parenleftig
m√n
k/parenrightig5k/parenrightig
.
4 Learning Algorithms
In this section, we present a novel algorithm for learning the reconstruction function f: (y,A)∝⇕⊣√∫⊔≀→xfromN
binary measurement vectors {(yi,Agi)}N
i=1, which is motivated by the analysis in Section 3. We parameterize
the reconstruction function using a deep neural network with parameters θ∈Rp. The learned function can
take into account the knowledge about the forward operator by simply applying a linear inverse at the first
layer,i.e.,fθ(y,A) = ˜fθ(A⊤y), or using more complex unrolled optimization architectures (Monga et al.,
2021).
In the case where we observe measurements associated with Gdifferent forward operators, we propose the
SSBM loss
arg min
θ∈RpN/summationdisplay
i=1/bracketleftig
LMC(yi,Agiˆxθ,i) +α/summationdisplay
s̸=gi∥ˆxθ,i−fθ(sign (Asˆxθ,i),As)∥2
2/bracketrightig
, (26)
where ˆxθ,i=fθ(yi,Agi), the costLMC(yi,Agiˆxθ,i)≥0enforcesmeasurement consistency (MC),i.e., require
thatyi= sign (Agiˆxθ,i), andα∈R+is a hyperparameter controlling the trade-off between the two terms
involved. In the setting where we have a single operator and the set Xis invariant to a group of transfor-
mations{Tg}G
g=1such as rotations or translations, we aim to learn a reconstruction function fθ:y∝⇕⊣√∫⊔≀→x(we
remove the dependence of fθonAto simplify the notation) via the following self-supervised loss:
arg min
θ∈RpN/summationdisplay
i=1/bracketleftig
LMC(yi,Aˆxθ,i) +αG/summationdisplay
g=1∥Tgˆxθ,i−fθ(sign (ATgˆxθ,i))∥2
2/bracketrightig
, (27)
where ˆxθ,i=fθ(yi)andα∈R+. In practice, we minimize (26) by mini-batching approaches ( e.g., stochastic
gradient descent) by using sampling one out of the Goperators at random per batch. In both cases, we
choose the measurement consistency term to be the logistic loss, i.e.,
LMC(y,ˆy) = log (1 + exp(−y◦ˆy)) (28)
which enforces sign-consistent predictions which are far from zero, as the logistic function tends asymptot-
ically towards zero as |ˆy|→∞. An empirical analysis in Section 5 shows that the logistic loss obtains the
best performance across various popular consistency losses.
12Published in Transactions on Machine Learning Research (11/2023)
Analysis of the proposed loss We focus on the multi-operator loss in (26), although a similar analysis
also holds for the equivariant setting. The first term of the loss enforces measurement consistency, i.e.,
requiresyi= sign (Agifθ(yi,Agi))for everyyiin the dataset. However, in the incomplete setting m<n, the
simple pseudo-inverse solution
f(y,Ag) =A†
gy (29)
withA†
g=A⊤
g(AgA⊤
g)−1, is measurement consistent for any number of operators Gand training data N.
Therefore, the first loss does not prevent learning a function fθ(y,Ag)which acts independently for each
operator(asiftherewere Gindependentlearningproblems). Thesecondloss bootstraps thecurrentestimates
ˆxi,θ=fθ(yi,Agi)as new ground truth references, mimicking the supervised loss
N/summationdisplay
i=1G/summationdisplay
s=1∥ˆxi,θ−fθ(sign (Asˆxi,θ),As)∥2, (30)
in order to enforce consistency across operators. Importantly, this additional loss avoids the trivial pseudo-
inverse solution in (29), as
A†
gy−A†
ssign/parenleftbig
AsA†
gy/parenrightbig
̸= 0 (31)
forg̸=sif the nullspaces of AgandAsare different, e.g., if the necessary condition in Corollary 4 is verified.
Model identification perspective The learning algorithm constructs a discrete approximation of the
signal set using the reconstructed dataset, i.e.,∪G
g=1˜Xgwhere ˜Xg=fθ(Yg,Ag)forg= 1,...,GandYgis the
subset of measurement vectors associated with the gth operator. From a model identification perspective,
the measurement consistency loss ensures that Yg=Ag˜Xgfor allg= 1,...,G. The second loss ensures
consistency across all operators, i.e.,˜Xg=fθ(As˜Xg,As)for all pairs s,g∈{1,...,G}, acting as a proxy for
˜Xg=˜Xs.
5 Experiments
For all experiments, we use measurement operators with entries sampled from a standard Gaussian distri-
bution and evaluate the performance of the algorithms using by computing the average peak-to-signal ratio
(PSNR) on a test set with N′ground-truth signals, that is:
1
N′/summationtextN′
i=1PSNR/parenleftig
x′
i,fθ(sign (Agix′
i),Agi)/parenrightig
, (32)
where the PSNR is computed after normalizing the reconstructed image such that it has the same norm as
the reference image, i.e.,
PSNR(x,ˆx) =−20 log∥x−ˆx∥x∥
∥ˆx∥∥. (33)
We choose fθ(y,A) = ˜fθ(A⊤y)where ˜fθis the U-Net network used in (Chen et al., 2021) with weights θ,
and train for 400 epochs with the Adam optimizer with learning rate 10−4and standard hyperparameters
β1= 0.9andβ2= 0.99.
5.1 MNIST experiments
WeevaluatethetheoreticalboundsusingtheMNISTdataset, whichconsistsofgreyscaleimageswith n= 784
pixels and whose box-counting dimension is approximately k≈12(Hein & Audibert, 2005). We use 6×104
images for training and 103for testing.
Multipleoperatorssetting. Westartbycomparingthelogisticconsistencylossin(28)withthefollowing
alternatives:
•Standardℓp-loss,LMC(y,ˆy) =∥y−ˆy∥p
p. Asthislossiszeroonlyif ˆy=y, itpromotessignconsistency,
sign (ˆy) =yand unit outputs |ˆy|= 1.
13Published in Transactions on Machine Learning Research (11/2023)
Figure 5: Evaluated training losses for enforcing sign measurement consistency sign (Aˆx) =yof reconstruc-
tionsfθ(y) = ˆx.Left:The loss functions are shown for the case y= 1.Right:Average test PSNR of
different measurement consistency losses on the MNIST dataset with G= 10operators.
•One-sidedℓp-loss,LMC(y,ˆy) =∥max(−y◦ˆy,0)∥p
pwhere◦denotes element-wise multiplication and
themaxoperation is performed element-wise. This loss is zero as long as sign (ˆy) =yregardless of
the value of|ˆy|.
For all losses except the logistic loss, setting the trade-off parameter α= 1obtained best results. For the
logistic loss, we performed a sweep over different values of αandm, finding that the optimal choice of α
decreases with m(see Appendix E for more details). Thus, we set α= 0.1form < nandα= 0.06for
m≥n. Figure 5 shows the different losses and the test performance for different values of measurements
usingG= 10operators. The logistic loss obtains the best performance across all sampling regimes, whereas
the one-sided ℓ2loss obtains the worst results.
Secondly, we compare the logistic loss with the following learning schemes:
•Linear inverse (no learning), defined as ˆxi=A⊤
giyi. This reconstruction can fail to be measurement
consistent (Goyal et al., 1998).
•Standard supervised learning loss, defined as/summationtextN
i=1∥xi−fθ(yi,Agi)∥2.We also evaluate this loss
together with the cross-operator consistency term in (26) which we denote as supervised+.
•Measurement consistency loss, defined as/summationtextN
i=1LMC(yi,Agifθ(yi,Agi))using the logistic loss.
•The binary iterative hard-thresholding (BIHT) reconstruction algorithm (Jacques et al., 2013) with
a Daubechies4 orthonormal wavelet basis. The step size and sparsity level of the algorithm were
chosen via grid search. It is worth noting that the best-performing sparsity level increases as the
number of measurements mis increased.
•Proposed SSBM loss in (26) using the logistic loss for measurement consistency.
Test PSNR values obtained for the case of G= 10operators are shown in the left subfigure of Figure 6,
where the PSNR in dB is plotted against m/nin log-scale representation. The measurement consistency
approach obtains performance similar to simply applying a linear inverse for the incomplete m/n< 1setting,
whereas it obtains a significant improvement over the linear inverse in the overcomplete case m/n≥1. This
gap can be attributed to the lack of measurement consistency of the linear reconstruction algorithm (Goyal
et al., 1998). The proposed loss obtains a performance that is several dBs above the linear inverse and
BIHT for all sampling regimes. BIHT relies on the wavelet sparsity prior, which does not capture well
enough the MNIST digits. SSBM performs similarly to supervised learning as the sampling ratio tends to
1, and perhaps surprisingly, it obtains slightly better performance than supervised learning for m/n = 1.28.
However, adding the cross-operator consistency loss to the supervised method ( i.e., the method supervised+
in Section 5.1) performs better for all sampling regimes than SSBM.
The right plot in Figure 6 compares the performance of the SSBM with the bounds in Proposition 8 and Con-
jecture9. Theseboundsbehavealmostlinearlyinthislog-logplotofboththeerror—throughthePSNR—and
14Published in Transactions on Machine Learning Research (11/2023)
Figure 6: Left:Average test PSNR of different supervised and unsupervised algorithms on the MNIST
dataset with G= 10operators. Right:The performance of the SSBM method follows closely the bounds
in Conjecture 9.
(a)
13
0.0640
10
0.51 0.26 0.13
(b)
Figure 7: ( a) Average test PSNR and ( b) reconstructed test images of the proposed unsupervised method
for different numbers of operators Gand measurements m.
the log-scale representation of m/n. We thus observe a good agreement between the predictions in Conjec-
ture 9 and the performance in practice.
Figure 7 shows the average test PSNR and reconstructed images obtained by the proposed self-supervised
method for different values of Gandm. The method fails to obtain good reconstructions when G= 1, as
the necessary condition in Corollary 4 is not fulfilled.
Noisy measurements In many sensing applications, the sensor data are subject to noise. In the setting
of binary measurements, noise affects the measurements by flipping the sign, as the observations can only
be−1or+1. We evaluate the proposed algorithm with m= 274measurements and G= 10operators and
different noise levels, according to the model
yi= sign (Agi,ixi+ϵi) (34)
whereϵi∼N(0,Iσ2)fori= 1,...,N. Figure 8 shows the performance of the SSBM algorithm for different
values ofσ. The learning algorithm is particularly robust to noise, obtaining a good performance for noise
levelsupto σ= 0.13. Thisnoiseleveltranslatestohavingapproximately15%ofbitsflippedpermeasurement
vector. It is worth noting that these results indicate that we can expect similarly good performances for
other noise distributions ( e.g., Poisson noise) for a similar average number of bit flips.
Equivariant setting using shifts. We evaluate the setting of learning with a single operator by using
the unsupervised equivariant objective in (27) with 2D shifts as the group of transformations (as the MNIST
dataset is approximately shift-invariant). Figure 9 shows the average test PSNR and reconstructed images as
15Published in Transactions on Machine Learning Research (11/2023)
Figure 8: Robustness of the proposed learning algorithm to noise in the measurement data. Left:Average
test PSNR as a function of the standard deviation of the noise. Right:Average test PSNR as a function of
the average percentage of flipped bits per measurement vector.
(a)
(b)
Figure 9: ( a) Average test PSNR and ( b) reconstructed test images by the compared algorithms with a
single operator Aas a function of the undersampling ratio m/n.
a function of the measurements mfor various algorithms. The proposed unsupervised method significantly
outperforms the linear inverse, BIHT, and the measurement consistent network in all sampling regimes, and
performs closely to supervised learning for m/n> 0.4.
5.2 Other Datasets
In order to demonstrate the robustness of the proposed method across datasets, we evaluate the proposed
unsupervised approach on the FashionMNIST (Xiao et al., 2017), CelebA (Liu et al., 2015) and Flow-
ers (Nilsback & Zisserman, 2008) datasets. The FashionMNIST dataset consists of 6×104greyscale images
with 28×28pixels which are divided across G= 10different forward operators. As with MNIST, we use
N= 6×103per operator for training and 103per operator for testing. For the CelebA dataset, we use
G= 10forward operators and choose a subset of 103images for each operator for training and another subset
of the same amount for testing. The Flowers dataset consists of 6149 color images for training and 1020
images for testing, all associated with the same forward operator. For both CelebA and Flowers datasets,
a center crop of 128×128pixels of each color image was used for training and testing. Section 5.2 shows
the average test PSNR of the proposed unsupervised method, standard supervised learning, BIHT, and the
linear inverse. For BIHT, we use the Daubechies4 orthonormal wavelet basis and optimize the step size and
sparsity level via grid search.
The self-supervised method obtains an average test PSNR which is only 1 to 2 dB below the supervised
approach. Figures 10 and 11 show reconstructed test images by the evaluated approaches for each forward
operator. The proposed unsupervised method is able to provide good estimates of the images, while only
having access to highly incomplete binary information. The supervised method obtains sharper images,
however at the cost of hallucinating details, whereas the proposed method obtains blurrier estimates with
less hallucinated details.
16Published in Transactions on Machine Learning Research (11/2023)
linear
inverse
proposedBIHT
supervised
ground
truth
Figure 10: Reconstructed test images using the FashionMNIST dataset. Each column corresponds to a test
image observed via a different forward operator Ag.
linear
inverse
proposedBIHT
supervised
ground
truth
Figure 11: CelebA results. Reconstructed test images using the CelebA dataset. Each column corresponds
to a test image observed via a different forward operator Ag.
6 Conclusions and Future Work
The theoretical analysis in this work characterizes the best approximation of a low-dimensional set that can
beobtainedfrombinarymeasurements. Themodelidentificationboundspresentedhereapplytoalargeclass
of signal models, as they only rely on the box-counting dimension, and complement those existing for signal
recovery from binary measurements (Goyal et al., 1998; Jacques et al., 2013). Moreover, the proposed self-
supervised loss provides a practical algorithm for learning to reconstruct signals from binary measurements
alone, which performs closely to fully supervised learning. This work paves the way for deploying machine
learning algorithms in scientific and medical imaging applications with quantized observations, where no
ground-truth references are available for training.
We leave the proof of Conjecture 9, and a study of the effect of noise in the observations and related dithering
techniques for future work. Another avenue of future research is the extension of Theorem 7 for the case of
operators related through the action of a group.
17Published in Transactions on Machine Learning Research (11/2023)
Dataset nmGLinear Inverse BIHT Supervised SSBM(ours)
FashionMNIST 78430010 6.38±0.23 10.68±0.31 17.63±0.33 16.47±0.22
CelebA 491529830 10 4.81±0.32 16.26±0.40 21.59±0.31 19.53±0.3
Flowers 491529830shifts 5.31±0.72 14.62±0.92 18.26±0.75 16.45±0.71
Table 2: Average test PSNR in dB obtained by the compared methods for the FashionMNIST, CelebA and
Flowers datasets.
Acknowledgments
Part of this research was supported by the Agence Nationale de la Recherche (Project UNLIP) and by the
Fonds de la Recherche Scientifique – FNRS under Grant T.0136.20 (Project Learn2Sense).
References
G Alberti, G Schirinzi, G Franceschetti, and V Pascazio. Time-domain convolution of one-bit coded radar
signals. In IEE Proceedings F (Radar and Signal Processing) , volume 138, pp. 438–444. IET, 1991.
Keith Ball et al. An elementary introduction to modern convex geometry. Flavors of geometry , 31(1-58):26,
1997.
R Baraniuk, S Foucart, D Needell, Y Plan, and M Wootters. One-bit compressive sensing of dictionary-
sparse signals. Information and Inference: A Journal of the IMA , 7(1):83–104, 08 2017. ISSN 2049-8764.
doi: 10.1093/imaiai/iax009. URL https://doi.org/10.1093/imaiai/iax009 .
Richard G Baraniuk and Michael B Wakin. Random projections of smooth manifolds. Foundations of
computational mathematics , 9(1):51–77, 2009.
T. Blumensath and M. E. Davies. Sampling theorems for signals from the union of finite-dimensional linear
subspaces. IEEE Transactions on Information Theory , 55(4):1872–1882, 2009. doi: 10.1109/TIT.2009.
2013003.
Petros T Boufounos, Laurent Jacques, Felix Krahmer, and Rayan Saab. Quantization and compressive
sensing. In Compressed Sensing and its Applications: MATHEON Workshop 2013 , pp. 193–237. Springer,
2015.
Anthony Bourrier, Mike Davies, TomerPeleg, PatrickPérez, and RémiGribonval. Fundamentalperformance
limits for ideal decoders in high-dimensional linear inverse problems. IEEE Transactions on Information
Theory, 60(12):7928–7946, 2014.
Ching-Hsien Chen and Jwo-Yuh Wu. Amplitude-aided 1-bit compressive sensing over noisy wireless sensor
networks. IEEE Wireless Communications Letters , 4(5):473–476, 2015.
Dongdong Chen, Julián Tachella, and Mike Davies. Equivariant imaging: Learning beyond the range space.
InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 4379–4388,
October 2021.
Dongdong Chen, Julián Tachella, and Mike Davies. Robust equivariant imaging: a fully unsupervised frame-
work for learning to image from noisy and partial measurements. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , 2022.
Mark A Davenport, Yaniv Plan, Ewout Van Den Berg, and Mary Wootters. 1-bit matrix completion.
Information and Inference: A Journal of the IMA , 3(3):189–223, 2014.
Kenneth Falconer. Fractal geometry: mathematical foundations and applications . John Wiley & Sons, 2004.
V.K. Goyal, M. Vetterli, and N.T. Thao. Quantized overcomplete expansions in RN: analysis, synthesis,
and algorithms. IEEE Transactions on Information Theory , 44(1):16–31, 1998. doi: 10.1109/18.650985.
18Published in Transactions on Machine Learning Research (11/2023)
Matthias Hein and Jean-Yves Audibert. Intrinsic dimensionality estimation of submanifolds in Rd. In
Proceedings of the 22nd international conference on Machine learning (ICML) , pp. 289–296, 2005.
Laurent Jacques, Jason N. Laska, Petros T. Boufounos, and Richard G. Baraniuk. Robust 1-bit compressive
sensing via binary stable embeddings of sparse vectors. IEEE transactions on information theory , 59(4):
2082–2102, 2013.
Ahmed Kirmani, Dheera Venkatraman, Dongeek Shin, Andrea Colaço, Franco NC Wong, Jeffrey H Shapiro,
and Vivek K Goyal. First-photon imaging. Science, 343(6166):58–61, 2014.
Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila,
et al. Noise2Noise. In International Conference on Machine Learning (ICML) . PMLR, 2018.
Jiaming Liu, Yu Sun, Cihat Eldeniz, Weijie Gan, Hongyu An, and Ulugbek S Kamilov. RARE: Image
reconstruction using deep priors learned without groundtruth. IEEE Journal of Selected Topics in Signal
Processing , 14(6):1088–1099, 2020.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Vishal Monga, Yuelong Li, and Yonina C Eldar. Algorithm unrolling: Interpretable, efficient deep learning
for signal and image processing. IEEE Signal Processing Magazine , 38(2):18–44, 2021.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.
In2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing , pp. 722–729. IEEE,
2008.
Samet Oymak and Ben Recht. Near-optimal bounds for binary embeddings of arbitrary sets. arXiv preprint
arXiv:1512.04433 , 2015.
Gilles Pisier. The volume of convex bodies and Banach space geometry , volume 94. Cambridge University
Press, 1999.
Yaniv Plan and Roman Vershynin. One-bit compressed sensing by linear programming. Communications
on pure and Applied Mathematics , 66(8):1275–1297, 2013.
Gilles Puy, Mike E Davies, and Rémi Gribonval. Recipes for stable linear embeddings from hilbert spaces
toRm.IEEE Transactions on Information Theory , 63(4):2171–2187, 2017.
Lucas Rencker, Francis Bach, Wenwu Wang, and Mark D Plumbley. Sparse recovery and dictionary learning
from nonlinear compressive measurements. IEEE Transactions on Signal Processing , 67(21):5659–5670,
2019.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms.
Physica D: nonlinear phenomena , 60(1-4):259–268, 1992.
JuliánTachella, DongdongChen, andMikeDavies. Unsupervisedlearningfromincompletemeasurementsfor
inverse problems. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances
in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=aV9WSvM6N3 .
Julián Tachella, Dongdong Chen, and Mike Davies. Sensing theorems for learning from incomplete measure-
ments.Journal of Machine Learning Research , 24(39):1–45, 2023.
Nguyen T Thao and Martin Vetterli. Lower bound on the mean-squared error in oversampled quantization
of periodic signals using vector quantization analysis. IEEE Transactions on Information Theory , 42(2):
469–479, 1996.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science , vol-
ume 47. Cambridge university press, 2018.
19Published in Transactions on Machine Learning Research (11/2023)
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta Ellermann, Kâmil Uğurbil, and
Mehmet Akçakaya. Self-supervised learning of physics-guided reconstruction neural networks without fully
sampled reference data. Magnetic resonance in medicine , 84(6):3172–3191, 2020.
Hadi Zayyani, Mehdi Korki, and Farrokh Marvasti. Dictionary learning for blind one bit compressed sensing.
IEEE Signal Processing Letters , 23(2):187–191, 2015.
A Technical Lemmas
We begin by introducing some technical results that play an important role in the main theorems of the
paper. We start with a result from (Jacques et al., 2013).
Lemma 11 (Lemma 9 in (Jacques et al., 2013)) .Given 0≤ϵ<1and two unit vectors ˜x,˜v∈Sn−1⊂Rn
anda∈Rnwithai∼i.i.d.N(0,1), we have
p0=P/bracketleftbig
∀x∈Bϵ(˜x),∀v∈Bϵ(˜v) : sign/parenleftbig
a⊤v/parenrightbig
= sign/parenleftbig
a⊤x/parenrightbig/bracketrightbig
≥1−d(˜x,˜v)−/radicalbigg
nπ
2ϵ (35)
p1=P/bracketleftbig
∀x∈Bϵ(˜x),∀v∈Bϵ(˜v) : sign/parenleftbig
a⊤v/parenrightbig
̸= sign/parenleftbig
a⊤x/parenrightbig/bracketrightbig
≥d(˜x,˜v)−/radicalbigg
nπ
2ϵ. (36)
whered(·,·)denotes the angular distance.
Remark: TheangulardistancesinLemma11canbetranslatedintoEuclideandistancesduetothefollowing
inequality:
d(˜x,˜v)≥2
πsin/parenleftbigπ
2d(˜x,˜v)/parenrightbig
=1
π∥˜x−˜v∥. (37)
LetC0(S)denote the set of continuous functions on the set S. This lemma has the following corollary:
Corollary 12. Given ˜x∈Sn−1,0<ϵ< 1/2,a∈Rnwitha∼i.i.d.N(0,1), we have
P/bracketleftig
sign/parenleftbig
a⊤·/parenrightbig
/∈C0/parenleftbig
Bϵ(˜x)∩Sn−1/parenrightbig/bracketrightig
≤√nϵ.
Proof.The proof can be derived from the complement of the event associated with p0in (35) when ˜x= ˜v.
Here is, however, a simplified proof for completeness. We first observe that sign/parenleftbig
a⊤·/parenrightbig
is discontinuous over
Bϵ(˜x)∩Sn−1iff|a⊤˜x
∥a∥|≤ϵ. Therefore, by the rotational invariance of the Gaussian distribution we can choose
˜x= [1,0,..., 0]⊤and the probability above amounts to computing
p:=P[|a1
∥a∥|≤ϵ] =P[a2
1≤ϵ2∥a∥2] =P[a2
1≤ϵ2
(1−ϵ2)(a2
2+...+a2
n)] =EξP[a2
1≤ϵ2
(1−ϵ2)ξ],
whereξ∼χ2(n−1). Since P[a2
1≤ϵ2
(1−ϵ2)ξ]≤√
2√πϵ√
1−ϵ2√ξ, andEξ√ξ≤/radicalbig
Eξξ≤√n−1≤√nby Jensen’s
inequality, we finally get p≤√
2√πϵ√
1−ϵ2√n≤2√
2√π√
3ϵ√n<ϵ√n.
B Signal Recovery Proof
Proof of Theorem 1. Proving this theorem amounts to showing that the probability of the failure of the
event
sign (Ax) = sign (As) =⇒ ∥x−s∥<δ
20Published in Transactions on Machine Learning Research (11/2023)
decays exponentially in mprovided that
m≥4
δ/parenleftbig
2klog30√n
δ+ log1
ξ/parenrightbig
holds. In other words, we want to upper bound
pδ:=P[∃x1,x2∈X,∥x1−x2∥>δ: sign (Ax1) = sign (Ax2)]
with such an exponential decay.
Asboxdim (X)< k, there exist a constant ϵ0∈(0,1
2)such that N(X,ϵ)≤ϵ−kfor allϵ≤ϵ0. Thus, there
is a covering set Qϵofϵ−kpoints, such that for every x∈X, there exists a point q∈Qϵwhich verifies
∥x−q∥<ϵ.
Thanks to this covering, we can upper bound pδas
pδ≤P[∃q1,q2∈Qϵ,∃x1∈Bϵ(q1),∃x2∈Bϵ(q2),∥x1−x2∥>δ: sign (Ax1) = sign (Ax2)].
However, since∥x1−x2∥> δ, we must have∥q1−q2∥≥∥x1−x2∥−2ϵ > δ−2ϵ. Therefore, defining
Qϵ,δ={(q,q′)∈Qϵ×Qϵ:∥q−q′∥>δ−2ϵ}, the previous upper bound can be enlarged as
pδ≤P[∃(q1,q2)∈Qϵ,δ,∃x1∈Bϵ(q1),∃x2∈Bϵ(q2) : sign (Ax1) = sign (Ax2)].
Given a fixed pair (q1,q2)∈Qϵ,δ, Lemma 11 shows that for a∈Rndrawn from a standard Gaussian
distribution
P/bracketleftbig
∀x1∈Bϵ(q1),∀x2∈Bϵ(q2) : sign/parenleftbig
a⊤x1/parenrightbig
̸= sign/parenleftbig
a⊤x2/parenrightbig/bracketrightbig
≥1
π∥q1−q2∥−/radicalbigπn
2ϵ>δ−2ϵ
π−/radicalbigπn
2ϵ.(38)
By settingϵ=ϵ(δ) =δ(4−π)
8+4π√
nπ/2and taking the probability of the complementary event we obtain
P/bracketleftbig
∃x1∈Bϵ(q1),∃x2∈Bϵ(q2) : sign/parenleftbig
a⊤x1/parenrightbig
= sign/parenleftbig
a⊤x2/parenrightbig/bracketrightbig
≤1−δ/4. (39)
Therefore, considering the mi.i.d. rows{ai}m
i=1⊂Rnof the matrix A= (a1,...,am)⊤∈Rm×ndrawn from
a standard Gaussian distribution, we have
P[∃x1∈Bϵ(q1),∃x2∈Bϵ(q2) : sign (Ax1) = sign (Ax2)]
≤/producttextm
i=1P[∃x1∈Bϵ(q1),∃x2∈Bϵ(q2) : sign/parenleftbig
a⊤
ix1/parenrightbig
= sign/parenleftbig
a⊤
ix2/parenrightbig
]
≤(1−δ/4)m.
Applying a union bound to all pairs (q1,q2)∈Qϵ(δ),δ⊂Qϵ×Qϵ, since there are no more that/parenleftbig|Qϵ|
2/parenrightbig
≤
|Qϵ|2≤ϵ−2ksuch pairs, we obtain
pδ≤/parenleftig8+4π√
πn/2
(4−π)δ/parenrightig2k
(1−δ/4)m≤exp/parenleftig
2klog(8+4π√
πn/2
(4−π)δ)−mδ
4/parenrightig
, (40)
where we used 1−δ/4≤exp(−δ/4)forδ>0.
Upper bounding this probability by 0≤ξ≤1as in the statement of Thm 1 and using the crude bound
(8+4π/radicalbig
πn/2)/(4−π)≤30√nforn≥1, we finally obtain 2klog30√n
δ+mδ
4≥logξwhich gives the sample
complexity bound (9)
m≥4
δ/parenleftig
2klog30√n
δ+ log1
ξ/parenrightig
, (41)
where the condition ϵ(δ)≤ϵ0holds ifδ≤30ϵ0√n.
21Published in Transactions on Machine Learning Research (11/2023)
C Model Identification Proof
Proof of Theorem 7. We want to identify the condition that m,Gand0< δ < 1must respect to induce
that ˆX ⊆ Xδholds with high probability with respect to a random draw of the operators A1,...,AG.
Equivalently, we need to show that, for this condition,
sign (Agxg) = sign (Agv),∀g= 1,...,G (42)
holds for some v∈Sn−1\Xδand somex1,...,xG∈Xwith probability at most ξwith respect to a random
draw of the Gaussian matrices A1,...,AG. This proof adapts some of the procedures given in (Jacques et al.,
2013) to our specific setting. We start by bounding this probability for ϵ-balls around vectors ˜v∈Sn−1\Xδ,
˜x1,..., ˜xG∈X, that is
p0:=P/bracketleftbig
∃(x1,...,xG)∈Bϵ(˜x1)×···×Bϵ(˜xG),∃v∈Bϵ(˜v) :∀g= 1,...,G, sign (Agv) = sign (Agxg)/bracketrightbig
.
We first notice that from the independence of the operators {Ag}G
g=1,
p0≤/producttextG
g=1P/bracketleftbig
∃xg∈Bϵ(˜xg),∃v∈Bϵ(˜v) : sign (Agv) = sign (Agxg)/bracketrightbig
.
Furthermore, as every row of each operator Agis i.i.d. as a standard Gaussian random vector ag, we have
p0≤/producttextG
g=1P/bracketleftbig
∃xg∈Bϵ(˜xg),∃v∈Bϵ(˜v) : sign/parenleftbig
a⊤
gv/parenrightbig
= sign/parenleftbig
a⊤
gxg/parenrightbig/bracketrightbigm(43)
=/producttextG
g=1/parenleftig
1−P/bracketleftbig
∀xg∈Bϵ(˜xg),∀v∈Bϵ(˜v) : sign/parenleftbig
a⊤
gv/parenrightbig
̸= sign/parenleftbig
a⊤
g,ixg/parenrightbig/bracketrightbig/parenrightigm
(44)
From Lemma 11, we know that
P/bracketleftbig
∀xg∈Bϵ(˜xg),∀v∈Bϵ(˜v) : sign/parenleftbig
a⊤
g,iv/parenrightbig
̸= sign/parenleftbig
a⊤
g,ixg/parenrightbig/bracketrightbig
≥1
π∥˜xg−˜v∥−/radicalbignπ
2ϵ. (45)
where the distance ∥˜xg−˜v∥can be bounded by δto obtain
P/bracketleftbig
∀xg∈Bϵ(˜xg),∀v∈Bϵ(˜v) : sign/parenleftbig
a⊤
g,iv/parenrightbig
̸= sign/parenleftbig
a⊤
g,ixg/parenrightbig/bracketrightbig
≥δ
π−/radicalbignπ
2ϵ. (46)
Plugging this into (44) and pickingδ
π−/radicalbignπ
2ϵ=δ
4, which means that
ϵ=ϵ(δ) = (4−π√
8π3)δ√n≤1
18δ√n,
we get
p0≤/parenleftbig
1−δ
π+/radicalbignπ
2ϵ/parenrightbigmG≤exp(−δ
4mG). (47)
We can extend this result to all vectors v∈Sn−1\Xδandx1,...,xG∈Xby applying a union bound over
a covering of the product set XG×(Sn−1\Xδ). Since we can cover Xwithϵ−kballs withϵ≤ϵ0due to the
assumption that boxdim (X)<k, and also cover Sn−1\Xδwith (3/ϵ)nballs (Pisier, 1999), we have
P[∃x1,...,xG∈X,∃v∈(Sn−1\Xδ) :∀g= 1,...,G, sign (Agv) = sign (Agxg)]≤ϵ−kG(ϵ/3)−np0(48)
Using the bound (47), the upper bound on ϵ, and bounding the resulting probability by ξ, we obtain
ξ≥ϵ−kG(ϵ
3)−nexp(−δ
4mG) = exp(kGlog(1
ϵ) +nlog(3
ϵ)−δ
4mG)
≥exp/bracketleftbig
kGlog(18√n
δ) +nlog(54√n
δ)−δ
4mG/bracketrightbig
.
Equivalently, m≥4
δ/bracketleftbig
klog(18√n
δ) +n
Glog(54√n
δ) +1
Glog(1/ξ)/bracketrightbig
, which holds if
m≥4
δ/bracketleftbig
(k+n
G) log(54√n
δ) +1
Glog(1
ξ)/bracketrightbig
.
Recalling, we must have ϵ<ϵ 0, we observe that this conditions is met if δ<18√nϵ0.
22Published in Transactions on Machine Learning Research (11/2023)
Derivation of δ.Here we aim to upper bound the minimum identification error, i.e., the minimum value
ofδ, for a fixed number of measurements m. The bound in Theorem 7, that is
m≥4
δ((k+n
G) log(54√n
δ) +1
Glog(1
ξ)) (49)
can be rewritten as a function of δas
log(δ) +δa≥b (50)
where
a=m
4(k+n
G),andb= log 54√n+1
(Gk+n)log1
ξ.
Notice that b≥1, anda≥1from (49) since 0<δ< 1. The expression in (50) holds if
δ≥1
a(log(a) +b). (51)
Indeed, (51) implies that log(δ) +δa≥log(δ) + log(a) +b= log(aδ) +b. However, again from (51), we get
aδ≥log(a) +b≥1sincea,b≥1. Therefore, log(δ) +δa≥log(aδ) +b≥b.
Finally, picking the smallest δrespecting (51), we get for large m,nandG,
δ=O/parenleftbigk+n
G
mlogmn
k+n
G/parenrightbig
(52)
which, forn/G≪kreads
δ=O/parenleftbigk
mlogmn
k/parenrightbig
. (53)
D Sample Complexity Proof
Proof of Theorem 10. We aim to bound the number of different cells associated with the binary mapping
sign (A·)which contain at least one element from the signal set X,i.e.,|sign (AX)|. Our strategy consists
in obtaining a global bound on the number of discontinuities of the binary mapping (or in other words, sign
changes) over the image of a covering of X, which can then be related to the number of different cells that
contain at least one element of X.
Forϵ < ϵ 0, letQϵ⊂Xbe an optimal ϵcovering ofX. Ifboxdim (X)< k, then there exists an ϵ0∈(0,1
2)
such that|Qϵ|≤ϵ−kfor allϵ <ϵ 0. Let us define the number Z(S)of its discontinuous components of the
binary mapping sign (A·)over a setS⊂Sn−1,i.e.,
Z(S) :=/vextendsingle/vextendsingle{i: sign/parenleftbig
a⊤
i·/parenrightbig
/∈C0(S)}/vextendsingle/vextendsingle.
From the independence of the {ai}m
i=1, we observe that Z(S) =/summationtextm
i=1Zi(S)is a binomial random variable,
that is the sum of mi.i.d. binary random variables with probability
p:=P/bracketleftig
sign/parenleftbig
a⊤·/parenrightbig
/∈C0(S)/bracketrightig
,
whereais a standard Gaussian random vector. From Corollary 12, we have p≤√nϵfor any set of the form
S=Sq,ϵ:=Bϵ(q)∩Sn−1withq∈Rn. Using Bernstein inequality (Vershynin, 2018) on the random variable
Z(Sq,ϵ)withEZ(Sq,ϵ) =mp≤m√nϵ, we obtain
P[Z(Sq,ϵ)>2m√nϵ]≤P[Z > 2EZ]≤exp(−3
8m√nϵ).
Therefore, since|Qϵ|≤ϵ−k, we get from a union bound
P[∀q∈Qϵ:Z(Sq,ϵ)≤2m√nϵ]≤1−exp(klog(1
ϵ)−3
8m√nϵ). (54)
Let us fixϵby setting a failure probability 0<ξ< 1such thatξ= exp(klog(1
ϵ)−3
8m√nϵ),i.e.,
2m√nϵ=16
3/bracketleftbig
log(1
ξ) +klog(1
ϵ)/bracketrightbig
. (55)
23Published in Transactions on Machine Learning Research (11/2023)
This implicitly imposes3
8m√nϵ>k log(1
ϵ), and since ϵ<min(ϵ0,1/2), we get3
8m√nϵ>k log 2, or
8klog 2
3m√n<ϵ. (56)
Since the left-hand side of (56) has to be smaller than min(ϵ0,1/2), we have that 2k/(m√n)<min(ϵ0,1/2)
(using the fact that 2>8
3log 2).
For any set S⊆Sn−1, the number of cells generated by sign (A·)in this set cannot exceed 2 to the power of
the number of discontinuous components in this mapping, i.e.,|sign (AS)|≤2Z(S). Thus, with probability
1−ξ, and given q∈Qϵ,
|sign (ASq,ϵ)|≤216
3[log(1
ξ)+klog(1
ϵ)]= (1
ξ)16 log 2
3(1
ϵ)16 log 2
3k<(1
ξ)4(1
ϵ)4k.
Since there are at most ϵ−kballs in the covering, and using (56), we obtain the bound
|sign (AX)|≤/summationtext
q∈Xϵ|sign (ASq,ϵ)|<(1
ξ)4(1
ϵ)5k<(1
ξ)4/parenleftbig3m√n
8klog 2/parenrightbig5k<(1
ξ)4/parenleftbig3m√n
5k/parenrightbig5k.
We now prove a bound on the expected number of intersected cells. We first observe that
E|sign (AX)|≤/summationtext
q∈XϵE|sign (ASq,ϵ)|,
and, for any set S⊆Sn−1, the independence of the random variables Zi(S)provides
E|sign (AS)|≤E2Z(S)=E/bracketleftbig
2/summationtextm
i=1Zi(S)/bracketrightbig
=E/bracketleftbig/productdisplay
i2Zi(S)/bracketrightbig
=/productdisplay
iE2Zi(S).
Moreover, considering the previous covering QϵofX, ifS=Sq,ϵ, for someq∈Qϵ,
E2Zi(S)= 20P(Zi(S) = 0) + 21P(Zi(S) = 1) = (1−p) + 2p= 1 +p≤1 +√nϵ≤e√nϵ.
Therefore, E|sign (ASq,ϵ)|≤em√nϵ, and
E|sign (AX)|≤ϵ−kem√nϵ=eklog(1
ϵ)+m√nϵ.
The function klog(1
ϵ) +m√nϵis convex in ϵand reaches its minimum on ϵ=k
m√n. Therefore, by setting
ϵ=k/(m√n)and imposing k/m√n≤min(ϵ0,1/2), we get
klog(m√n
k) +k=k(1 + log(m√n
k)),
which finally gives
E|sign (AX)|≤ek(1+log(m√n
k))= (em√n
k)k.
E Choice of trade-off parameter
We evaluate the impact of the trade-off parameter of the proposed SSBM algorithm for different sampling
ratiosm/non the MNIST dataset. In all cases, we use G= 10operators. Figure 12 shows the average test
PSNR of the learned networks. The optimal choice of αdecreases with the number of measurements m. In
the experiments, we choose α= 0.1ifm<nandα= 0.06otherwise.
24Published in Transactions on Machine Learning Research (11/2023)
Figure 12: Impact of the trade-off parameter αof the SSBM learning algorithm as a function of the sampling
ratiom/nnetwork for the MNIST problem with G= 10operators.
25