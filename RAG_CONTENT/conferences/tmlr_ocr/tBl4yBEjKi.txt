Published in Transactions on Machine Learning Research (01/2023)
Separable Self-attention for Mobile Vision Transformers
Sachin Mehta
Apple Inc.
Mohammad Rastegari
Apple Inc.
Reviewed on OpenReview: https: // openreview. net/ forum? id= tBl4yBEjKi
Abstract
Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across
several mobile vision tasks, including classification and detection. Though these models have
fewer parameters, they have high latency as compared to convolutional neural network-based
models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention
(MHA) in transformers, which requires O(k2)time complexity with respect to the number of
tokens (or patches) k. Moreover, MHA requires costly operations (e.g., batch-wise matrix
multiplication) for computing self-attention, impacting latency on resource-constrained
devices. This paper introduces a separable self-attention method with linear complexity, i.e.
O(k). A simple yet effective characteristic of the proposed method is that it uses element-wise
operations for computing self-attention, making it a good choice for resource-constrained
devices. The improved model, MobileViTv2 , is state-of-the-art on several mobile vision
tasks, including ImageNet object classification and MS-COCO object detection. With about
three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet
dataset, outperforming MobileViT by about 1% while running 3.2×faster on a mobile device.
Our source code is available at: https://github.com/apple/ml-cvnets .
1 Introduction
Vision transformers (ViTs) of Dosovitskiy et al. (2021) have become ubiquitous for a wide variety of visual
recognition tasks (Touvron et al., 2021; Liu et al., 2021), including mobile vision tasks (Mehta & Rastegari,
2022). At the heart of the ViT-based models, including mobile vision transformers, is the transformer block
(Vaswani et al., 2017). The main efficiency bottleneck in ViT-based models, especially for inference on
resource-constrained devices, is the multi-headed self-attention (MHA). MHA allows the tokens (or patches)
to interact with each other, and is a key for learning global representations. However, the complexity of
self-attention in transformer block is O(k2), i.e., it is quadratic with respect to the number of tokens (or
patches)k. Besides this, computationally expensive operations (e.g., batch-wise matrix multiplication; see
Fig. 1) are required to compute attention matrix in MHA. This, in particular, is concerning for deploying
ViT-based models on resource-constrained devices, as these devices have reduced computational capabilities,
restrictive memory constraints, and a limited power budget. Therefore, this paper seeks to answer this
question: can self-attention in transformer block be optimized for resource-constrained devices?
Several methods (e.g., Child et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020; Wang et al., 2020) have
been proposed for optimizing the self-attention operation in transformers (not necessarily for ViTs). Among
these, a widely studied approach in sequence modeling tasks is to introduce sparsity in self-attention layers,
wherein each token attends to a subset of tokens in an input sequence (Child et al., 2019; Beltagy et al., 2020).
Though these approaches reduces the time complexity from O(k2)toO(k√
k)orO(klogk), the cost is a
performance drop. Another popular approach for approximating self-attention is via low-rank approximation.
Linformer (Wang et al., 2020) decomposes the self-attention operation into multiple smaller self-attention
1Published in Transactions on Machine Learning Research (01/2023)
Transformer (Total time = 12.3 ms)
Linformer (Total time = 13.8 ms)
Ours (Total time = 7.7 ms)Top-5 operations in each layerTheoretical time complexity
CPU latency vs. Tokens
<latexit sha1_base64="I1wmkFxj3dvBneHHWWVgR2fppBg=">AAADEnicbVLLbtNAFB2bVwmvAEs2I1JQ2UR2VB7Lim5YgFqkpq0Uh2g8vk5HmYc1cw2NLH8AW+Br2CG2/AAfA2KSWqFOe6WRjs6599y5dyYtpHAYRb+D8MrVa9dvbNzs3Lp95+697v0Hh86UlsOQG2nsccocSKFhiAIlHBcWmEolHKWz3YV+9BGsE0Yf4LyAsWJTLXLBGXpq0v2TpDAVukKWlpLZupK87tAmEjSFLSWM4v7zAscr3gtwimlevTMZyJo+XRG7RhUSTgXOa5+V/HdSImucBi2rA8u0y41VYL0N3dzbmn0YPNs8X/tW6FXCQm+re6V1lwpJahCNWr9/AjpbTTvp9qJ+tAx6EcQN6JEm9ifdv0lmeKlAI5fMuVEceeOKWRRcQt1JSgcF4zM2hZGHmilw42r5TDV94pmM+ln80UiX7PmKiinn5ir1mYrhiVvXFuRl2qjE/NW4ErooETQ/a5SXkqKhizenmbDAUc49YNwKf1fKT5hlHP3PaHVJjZn55bjWJJUqJQprPtUdv7B4fT0XweGgH7/ob7/f7u28bla3QR6Rx2SLxOQl2SFvyD4ZEh7w4HPwJfgafgu/hz/Cn2epYdDUPCStCH/9A6XA6Yo=</latexit>Model ComplexityTransformerO(k2)LinformerO(k)OursO(k)
<latexit sha1_base64="FKPOTpWmrtIx/FBC+Zno4CdIQlc=">AAAECHicjVPLbtNAFJ3aPNrwaApLNiMiEBssO21C6aqCDRKqVKSmrRRH0Xh8k44y47HmgYgs/wAfwBY+gR1iy1/wBfwG48StaBIIV7J0de8599w540lyzrQJw58bnn/j5q3bm1uNO3fv3d9u7jw41dIqCj0quVTnCdHAWQY9wwyH81wBEQmHs2TyuuqfvQelmcxOzDSHgSDjjI0YJcaVhjveVpzAmGWFIYnlRJUFV6ps4IWIjcyV5dCPgk5uBiv68MEko+JE5s87WOagZvN1iZ/iWFhumFvViqyIyoKWxRWcCSjXYI5ASDV1qDhelhUsrddqr9wLEwPZwYEQTqMb7GKhXeLOgI+SVfNqOElTB2t32kGI7RrGyGpI46ECbuOhO0WFjnYviZ2omvH2X2LaVsu9jC4pawlzkf3u/2jEiTRGir9dXQxZenXzw2YrDMJZ4OUkqpMWquN42PwVp5JaAZmhnGjdj0InUBDlbpJD2YidNTmhEzKGvkszIkAPitk/W+InlW94JJX7MoNn1T8ZBRFaT0XikIKYC73Yq4qren1rRvuDgmW5dW7RudDIWWYkrh4ATpkCavjUJYQq5nbF9IIoQo17JtdUEiknzhxdORMt+rCcnLaDqBvsvdtrHb6qPdpEj9Bj9AxF6AU6RG/QMeoh6uXeJ++z98X/6H/1v/nf51Bvo+Y8RNfC//EbnjsgRg==</latexit>Top-5 operations Time Memoryaten::mm 6.3 ms 1.5 Mbaten::add 252.0 us 1.5 Mbfusedrelumul 113.0 us 512.0 Kbaten::sum 91.0 us 2.0 Kbaten::mul 86.0 us 512.0 Kb
<latexit sha1_base64="K7ZmNecfsZiUaO78zFC9KeVDASI=">AAAEW3ichVNNb9MwGHbTAqMMtoI4wcGiYuIAVVJaKDtN4wCXSUNat0lNVdmO01n1R2Q7o1WUE7+GK/waDvwX3DSd6Mc0S5EevR/P876PY5xwZqzv/6l41dq9+w92HtYf7T5+srffeHpuVKoJ7RPFlb7EyFDOJO1bZjm9TDRFAnN6gSef5/mLa6oNU/LMzhI6FGgsWcwIsi40angvQ0zHTGYW4ZQjnWdc67wO105oVaJTTgdBq5vY4ZY8nVocZ2cqedeFKqG64Dc5PIChSLllbtRUyCzIM5JnN+VM0PyOmhMqlJ65qjDclBUsKsdqb50LIkvl4aEQTqPX8qEwDrQdOMHb+GCo1ffC1OyL4hGVWkV5yYELkqD1aUnSvY1kUR+OjIqtQFNX/L7bcZrpHeKLPqKSWTiaN/ndZZN/VxOKojl3t7cic7zVM6ysVeK22wypjG5+htF+00kXB26CoARNUJ7TUaNyEEaKpIJKSzgyZhD4TiFD2t0up3k9TA1NEJmgMR04KJGgZpgVlufwtYtEMFbafdLCIvp/R4aEMTOBXaVA9sqs5+bBbblBauPeMGMySZ1bZCEUpxxaBeePAkZMU2L5zAFENHOzQnKFNCLWPZ0VFazUxLljVjZZ+jV9W4xsMV9dNLpmiSlXnS52dc4G6z5ugvN2K/jQ6nzrNI+OS493wAvwCrwBAfgIjsBXcAr6gHg/vJ/eL+939W+tWqvXdhelXqXseQZWTu35P4syPuE=</latexit>Top-5 operations Time Memoryaten::mm 8.0 ms 2.0 Mbaten::bmm1.9 ms2.5 Mbaten::softmax 354.0 us 2.0 Mbaten::copy305.0 us 0.0 Mbaten::add 258.0 us 2.0 MB
<latexit sha1_base64="D25nWpdJ7G+zuU9bp5dS3vuXcNI=">AAAEW3icjVPLbtNAFHWcACUUaECsYDEiomIBlp00aemqggVsKhWpLymOopnxOB1lHtbMuCSyvOJr2MLXsOBfGDtuRV4qI1m6uvfcc+4940EJo9r4/u+aW2/cu/9g62Hz0fbjJ093Ws/OtUwVJmdYMqkuEdSEUUHODDWMXCaKQI4YuUCTT0X94pooTaU4NbOEDDkcCxpTDI1NjVruqxCRMRWZgShlUOUZUypvgqUTGpmolJFB4PUSM1xTJ1OD4uxUJu97QCZElfw6B7sg5Ckz1I6acpEFeYbz7BZOOcnvwBwTLtXMosJwVZbTqBqrs3YuAA0Rh4ecW4197wBwbYOO54NjtI4PhEp+K03NPksWEaFklFccqCTpev05SdfrbSKZ48ORlrHhcFqAfd9qpneIV7OmrIB96P93B6FCp8VwQb970+RvbAqRNEbyTbcZEhHd/gyjnbYlKg9YDYIqaDvVORm1arthJHHKiTCYQa0HgW8VMqjs7TKSN8NUkwTiCRyTgQ0F5EQPs9LyHLyxmQjEUtlPGFBm/+3IINd6xpFFcmiu9HKtSK6rDVITHwwzKpLUGobnQrH12UhQPAoQUUWwYTMbQKyonRXgK6ggNvbpLKggKSfWHb2wyY1f03flyAaxxUWja5roatXpfFfrbLDs42pw3vGCvrf3da999LHyeMt56bx23jqBs+8cOV+cE+fMwe5394f70/1V/9OoN5qN7TnUrVU9z52F03jxF0jMP0M=</latexit>Top-5 operations Time Memoryaten::mm 7.8 ms 2.0 Mbaten::bmm3.6 ms3.5 Mbaten::softmax 300.0 us 2.0 Mbaten::mul 296.0 us 2.0 Mbaten::einsum 163.0 us 0.0 Mb
Figure 1: Comparison between different attention units. Transformer and Linformer use costly
operations (batch-wise matrix multiplication) for computing self-attention. With increase in number of tokens,
the cost of computing self-attention ( aten::bmm operation) outweighs the cost of linear layers; making it
the main bottleneck (as also noted in previous works such as Linformer) for efficient inference on resource-
constrained devices. The proposed method does not use such operations, thus accelerating inference on
resource-constrained devices. Leftcompares top-5 operations (sorted by CPU time) in a single layer of
different attention units for k= 256tokens.Top Right compares complexity of different attention units.
Bottom Right compares the latency of different attention units as a function of the number of tokens k.
These results are computed on a single CPU core machine with a 2.4 GHz 8-Core Intel Core i9 processor,
d= 512(token dimensionality), h= 8(number of heads; for Transformer and Linformer), and p= 256
(projected tokens in Linformer) using a publicly available profiler in PyTorch Paszke et al. (2019).
1 2 3 4 5 6 7 8 9 10
Inference time (in ms)6870727476788082T op-1 accuracy 
 (in %)
 6.4%
better
3.2× faster
MobileViTv1
MobileViTv2
(a) ImageNet-1k classification
2 4 6 8 10 12 14 16 18
Inference time (in ms)202224262830Mean Average Precision 
 (in %)
 5.4%
better
3× faster
MobileViTv1
MobileViTv2 (b) MS-COCO object detection
5 10 15 20 25 30 35 40
Inference time (in ms)737475767778798081Mean Intersection over Union 
 (in %)
 6.7% 
better
3.1× fasterMobileViTv1
MobileViTv2 (c) PASCAL VOC segmentation
Figure 2: MobileViTv2 models are faster and better than MobileViTv1 models of (Mehta &
Rastegari, 2022) across different tasks. MobileViTv2 models are constructed by replacing multi-headed
self-attention in MobileViTv1 with the proposed separable self-attention (Section 3.2). Here, inference time is
measured on an iPhone12 for an input resolution of 256×256,512×512, and 320×320for classification,
segmentation, and detection respectively.
2Published in Transactions on Machine Learning Research (01/2023)
operations via linear projections, and reduces the complexity of self-attention from O(k2)toO(k). However,
Linformer still uses costly operations (e.g., batch-wise matrix multiplication; Fig. 1) for learning global
representations in MHA, which may hinder the deployment of these models on resource-constrained devices.
This paper introduces a novel method, separable self-attention , withO(k)complexity for addressing the
bottlenecks in MHA in transformers. For efficient inference, the proposed self-attention method also replaces
the computationally expensive operations (e.g., batch-wise matrix multiplication) in MHA with element-wise
operations (e.g., summation and multiplication). Experimental results on standard vision datasets and tasks
demonstrates the effectiveness of the proposed method (Fig. 2).
2 Related work
Improving self-attention Improving the efficiency of MHA in transformers is an active area of research.
The first line of research introduces locality to address the computational bottleneck in MHA (e.g., Child
et al., 2019; Beltagy et al., 2020; Parmar et al., 2018; Qiu et al., 2019; Bello, 2021). Instead of attending to
allktokens, these methods use predefined patterns to limit the receptive field of self-attention from all k
tokens to a subset of tokens, reducing the time complexity from O(k2)toO(k√
k)orO(klogk). However,
such methods suffer from large performance degradation with moderate training/inference speed-up over
the standard MHA in transformers. To improve the efficiency of MHA, the second line of research uses
similarity measures to group tokens (Kitaev et al., 2020; Vyas et al., 2020; Wang et al., 2021a). For instance,
Reformer (Kitaev et al., 2020) uses locality-sensitive hashing to group the tokens and reduces the theoretical
self-attention cost from O(k2)toO(klogk). However, the efficiency gains over standard MHA are noticeable
only for large sequences ( k>2048) (Kitaev et al., 2020). Because k<1024in ViTs, these approaches are not
suitable for ViTs. The third line of research improves the efficiency of MHA via low-rank approximation
(Wang et al., 2020; Choromanski et al., 2020). The main idea is to approximate the self-attention matrix
with a low-rank matrix, reducing the computational cost from O(k2)toO(k). Even though these methods
speed-up the self-attention operation significantly, they still use expensive operations for computing attention,
which may hinder the deployment of these models on resource-constrained devices (Fig. 1).
In summary, existing methods for improving MHA are limited in their reduction of inference time and memory
consumption, especially for resource-constrained devices. This work introduces a separable self-attention
method that is fast and memory-efficient (see Fig. 1), which is desirable for resource-constrained devices.
Improving transformer-based models There has been significant work on improving the efficiency of
transformers (Liu et al., 2021; Mehta & Rastegari, 2022; Mehta et al., 2021; Wu et al., 2021; Heo et al.,
2021). The majority of these approaches reduce the number of tokens in the transformer block using different
methods, including down-sampling (Ryoo et al., 2021; Heo et al., 2021) and pyramidal structure (Liu et al.,
2021; Wang et al., 2021b; Mehta & Rastegari, 2022). Because the proposed separable self-attention module
is a drop-in replacement to MHA, it can be easily integrated with any transformer-based model to further
improve its efficiency.
Other methods Transformer-based models performance can be improved using different methods, including
mixed-precision training (Micikevicius et al., 2018), efficient optimizers (Dettmers et al., 2022; Zhai et al.,
2021a), and knowledge distillation (Touvron et al., 2021; Kundu & Sundaresan, 2021). These methods are
orthogonal to our work, and by default, we use mixed-precision during training.
3MobileViTv2
MobileViT of Mehta & Rastegari (2022) is a hybrid network that combines the strengths of CNNs and ViTs.
MobileViT views transformers as convolutions, which allows it to leverage the merits of both convolutions (e.g.,
inductive biases) and transformers (e.g., long-range dependencies) to build a light-weight network for mobile
devices. Though MobileViT networks have significantly fewer parameters and deliver better performance as
compared to light-weight CNNs (e.g., MobileNets (Sandler et al., 2018; Howard et al., 2019)), they have high
latency. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA; Fig. 3a).
3Published in Transactions on Machine Learning Research (01/2023)
Broadcasted element-wise multiplication<latexit sha1_base64="r3RhxPibzptum5p4iWVmnNh7d98=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyKqMegF48RzAOSJcxOZpMhM7PLPISw5Be8eFDEqz/kzb9xNtmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtnVhFaIskPFHdCGvKmaQtwwyn3VRRLCJOO9HkLvc7T1RplshHM01pKPBIspgRbHKpr60YVGt+3Z8DrZKgIDUo0BxUv/rDhFhBpSEca90L/NSEGVaGEU5nlb7VNMVkgke056jEguowm986Q2dOGaI4Ua6kQXP190SGhdZTEblOgc1YL3u5+J/Xsya+CTMmU2uoJItFseXIJCh/HA2ZosTwqSOYKOZuRWSMFSbGxVNxIQTLL6+S9kU9uKoHD5e1xm0RRxlO4BTOIYBraMA9NKEFBMbwDK/w5gnvxXv3PhatJa+YOYY/8D5/ADPjjlk=</latexit>XElement-wise sum<latexit sha1_base64="N/EBWZoOVLkLENSRb/DG5XWv4dg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWjW4v6zUb/I4inACp3AOAVxBHe6gAU0g8AjP8ApvnvJevHfvY9Fa8PKZY/gD7/MHnlePKQ==</latexit> SoftmaxDot-productConcatenation
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dLinearLinearLinearLinearLinearLinear
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dhLinearLinearLinear<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="N/EBWZoOVLkLENSRb/DG5XWv4dg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWjW4v6zUb/I4inACp3AOAVxBHe6gAU0g8AjP8ApvnvJevHfvY9Fa8PKZY/gD7/MHnlePKQ==</latexit> <latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
LinearAttention matrix<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dTranspose<latexit sha1_base64="gMTgjs7J9T7tfl8I4J/4iGZ8J/U=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APbUjLpnTY0kxmSjFiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybnHjwXXxnW/ncLK6tr6RnGztLW9s7tX3j9o6ihRDBssEpFq+1Sj4BIbhhuB7VghDX2BLX98k/mtR1SaR/LeTGLshXQoecAZNVZ66IbUjPwgfZr2yxW36s5AlomXkwrkqPfLX91BxJIQpWGCat3x3Nj0UqoMZwKnpW6iMaZsTIfYsVTSEHUvnSWekhOrDEgQKfukITP190ZKQ60noW8ns4R60cvE/7xOYoKrXsplnBiUbP5RkAhiIpKdTwZcITNiYgllitushI2ooszYkkq2BG/x5GXSPKt6F9Xzu/NK7TqvowhHcAyn4MEl1OAW6tAABhKe4RXeHO28OO/Ox3y04OQ7h/AHzucPADaRJQ==</latexit>x
<latexit sha1_base64="Orsd8Uetr8OD0z/KWA5V57CS7gM=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae2Q8mkmTY0kwxJRhiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybkniDnTxnW/ndLa+sbmVnm7srO7t39QPTzqaJkoQttEcql6AdaUM0HbhhlOe7GiOAo47QbT29zvPlGlmRQPJo2pH+GxYCEj2FjpcRBhMwnCLJ0NqzW37s6BVolXkBoUaA2rX4ORJElEhSEca9333Nj4GVaGEU5nlUGiaYzJFI9p31KBI6r9bJ54hs6sMkKhVPYJg+bq740MR1qnUWAn84R62cvF/7x+YsJrP2MiTgwVZPFRmHBkJMrPRyOmKDE8tQQTxWxWRCZYYWJsSRVbgrd88irpXNS9y3rjvlFr3hR1lOEETuEcPLiCJtxBC9pAQMAzvMKbo50X5935WIyWnGLnGP7A+fwBAbuRJg==</latexit>y<latexit sha1_base64="LbIiugsfjMiuQbZM+HeSITVF930=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQxjIB84BsCLOTu8mQ2Qczd8WwbGPjr9hYKGLrP9j5N85uUmjigYHDOfc1x40EV2hZ30ZhZXVtfaO4Wdra3tndK+8ftFUYSwYtFopQdl2qQPAAWshRQDeSQH1XQMed3GR+5x6k4mFwh9MI+j4dBdzjjKKWBuVjB+EB8zmJhGGaOD7FMaMiaabpoFyxqlYOc5nYc1IhczQG5S9nGLLYhwCZoEr1bCvCfkIlciYgLTmxgoiyCR1BT9OA+qD6Sb49NU+1MjS9UOoXoJmrvzsS6is19V1dmd2oFr1M/M/rxehd9RMeRDFCwGaLvFiYGJpZJOaQS2AopppQJrm+1WRjKilDHVxJh2AvfnmZtM+r9kW11qxV6tfzOIrkiJyQM2KTS1Int6RBWoSRR/JMXsmb8WS8GO/Gx6y0YMx7DskfGJ8/RAaZtg==</latexit>Q<latexit sha1_base64="b2P15Xg35B2Uc1NdmMfGE2pXebI=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQRrCJYB6QXcLs5CYZMvtg5q4Ylm1s/BUbC0Vs/Qc7/8bJZgtNPDBwOOe+5niR4Aot69soLC2vrK4V10sbm1vbO+XdvZYKY8mgyUIRyo5HFQgeQBM5CuhEEqjvCWh746up374HqXgY3OEkAtenw4APOKOopV750EF4wGxOIqGfJo5PccSoSG7StFeuWFUrg7lI7JxUSI5Gr/zl9EMW+xAgE1Sprm1F6CZUImcC0pITK4goG9MhdDUNqA/KTbLtqXmslb45CKV+AZqZ+rsjob5SE9/TldMb1bw3Ff/zujEOLtyEB1GMELDZokEsTAzNaSRmn0tgKCaaUCa5vtVkIyopQx1cSYdgz395kbROq/ZZtXZbq9Qv8ziK5IAckRNik3NSJ9ekQZqEkUfyTF7Jm/FkvBjvxsestGDkPfvkD4zPHzrimbA=</latexit>K<latexit sha1_base64="Ta0kqL4N5jGMEBTHhbK9R3DfFLU=">AAACBXicbVDLSsNAFJ34rPVVdamLYBFclUSKuiy6cVnBPqAtZTK5aYdOJmHmRiwhGzf+ihsXirj1H9z5N07bLLT1wMDhnPua48WCa3Scb2tpeWV1bb2wUdzc2t7ZLe3tN3WUKAYNFolItT2qQXAJDeQooB0roKEnoOWNrid+6x6U5pG8w3EMvZAOJA84o2ikfumoi/CA0zmpAj9LuyHFIaMibWZZv1R2Ks4U9iJxc1ImOer90lfXj1gSgkQmqNYd14mxl1KFnAnIit1EQ0zZiA6gY6ikIeheOt2e2SdG8e0gUuZJtKfq746UhlqPQ89UTm7U895E/M/rJBhc9lIu4wRBstmiIBE2RvYkEtvnChiKsSGUKW5utdmQKsrQBFc0IbjzX14kzbOKe16p3lbLtas8jgI5JMfklLjkgtTIDamTBmHkkTyTV/JmPVkv1rv1MStdsvKeA/IH1ucPS6SZuw==</latexit>V
(a) MHA in Transformers
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dLinearLinearLinearLinearLinearLinear
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>hLinearLinearLinear
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="N/EBWZoOVLkLENSRb/DG5XWv4dg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWjW4v6zUb/I4inACp3AOAVxBHe6gAU0g8AjP8ApvnvJevHfvY9Fa8PKZY/gD7/MHnlePKQ==</latexit> <latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
LinearAttention matrix<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dTranspose<latexit sha1_base64="gMTgjs7J9T7tfl8I4J/4iGZ8J/U=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APbUjLpnTY0kxmSjFiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybnHjwXXxnW/ncLK6tr6RnGztLW9s7tX3j9o6ihRDBssEpFq+1Sj4BIbhhuB7VghDX2BLX98k/mtR1SaR/LeTGLshXQoecAZNVZ66IbUjPwgfZr2yxW36s5AlomXkwrkqPfLX91BxJIQpWGCat3x3Nj0UqoMZwKnpW6iMaZsTIfYsVTSEHUvnSWekhOrDEgQKfukITP190ZKQ60noW8ns4R60cvE/7xOYoKrXsplnBiUbP5RkAhiIpKdTwZcITNiYgllitushI2ooszYkkq2BG/x5GXSPKt6F9Xzu/NK7TqvowhHcAyn4MEl1OAW6tAABhKe4RXeHO28OO/Ox3y04OQ7h/AHzucPADaRJQ==</latexit>x
<latexit sha1_base64="Orsd8Uetr8OD0z/KWA5V57CS7gM=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae2Q8mkmTY0kwxJRhiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybkniDnTxnW/ndLa+sbmVnm7srO7t39QPTzqaJkoQttEcql6AdaUM0HbhhlOe7GiOAo47QbT29zvPlGlmRQPJo2pH+GxYCEj2FjpcRBhMwnCLJ0NqzW37s6BVolXkBoUaA2rX4ORJElEhSEca9333Nj4GVaGEU5nlUGiaYzJFI9p31KBI6r9bJ54hs6sMkKhVPYJg+bq740MR1qnUWAn84R62cvF/7x+YsJrP2MiTgwVZPFRmHBkJMrPRyOmKDE8tQQTxWxWRCZYYWJsSRVbgrd88irpXNS9y3rjvlFr3hR1lOEETuEcPLiCJtxBC9pAQMAzvMKbo50X5935WIyWnGLnGP7A+fwBAbuRJg==</latexit>y<latexit sha1_base64="LbIiugsfjMiuQbZM+HeSITVF930=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQxjIB84BsCLOTu8mQ2Qczd8WwbGPjr9hYKGLrP9j5N85uUmjigYHDOfc1x40EV2hZ30ZhZXVtfaO4Wdra3tndK+8ftFUYSwYtFopQdl2qQPAAWshRQDeSQH1XQMed3GR+5x6k4mFwh9MI+j4dBdzjjKKWBuVjB+EB8zmJhGGaOD7FMaMiaabpoFyxqlYOc5nYc1IhczQG5S9nGLLYhwCZoEr1bCvCfkIlciYgLTmxgoiyCR1BT9OA+qD6Sb49NU+1MjS9UOoXoJmrvzsS6is19V1dmd2oFr1M/M/rxehd9RMeRDFCwGaLvFiYGJpZJOaQS2AopppQJrm+1WRjKilDHVxJh2AvfnmZtM+r9kW11qxV6tfzOIrkiJyQM2KTS1Int6RBWoSRR/JMXsmb8WS8GO/Gx6y0YMx7DskfGJ8/RAaZtg==</latexit>Q<latexit sha1_base64="b2P15Xg35B2Uc1NdmMfGE2pXebI=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQRrCJYB6QXcLs5CYZMvtg5q4Ylm1s/BUbC0Vs/Qc7/8bJZgtNPDBwOOe+5niR4Aot69soLC2vrK4V10sbm1vbO+XdvZYKY8mgyUIRyo5HFQgeQBM5CuhEEqjvCWh746up374HqXgY3OEkAtenw4APOKOopV750EF4wGxOIqGfJo5PccSoSG7StFeuWFUrg7lI7JxUSI5Gr/zl9EMW+xAgE1Sprm1F6CZUImcC0pITK4goG9MhdDUNqA/KTbLtqXmslb45CKV+AZqZ+rsjob5SE9/TldMb1bw3Ff/zujEOLtyEB1GMELDZokEsTAzNaSRmn0tgKCaaUCa5vtVkIyopQx1cSYdgz395kbROq/ZZtXZbq9Qv8ziK5IAckRNik3NSJ9ekQZqEkUfyTF7Jm/FkvBjvxsestGDkPfvkD4zPHzrimbA=</latexit>K<latexit sha1_base64="Ta0kqL4N5jGMEBTHhbK9R3DfFLU=">AAACBXicbVDLSsNAFJ34rPVVdamLYBFclUSKuiy6cVnBPqAtZTK5aYdOJmHmRiwhGzf+ihsXirj1H9z5N07bLLT1wMDhnPua48WCa3Scb2tpeWV1bb2wUdzc2t7ZLe3tN3WUKAYNFolItT2qQXAJDeQooB0roKEnoOWNrid+6x6U5pG8w3EMvZAOJA84o2ikfumoi/CA0zmpAj9LuyHFIaMibWZZv1R2Ks4U9iJxc1ImOer90lfXj1gSgkQmqNYd14mxl1KFnAnIit1EQ0zZiA6gY6ikIeheOt2e2SdG8e0gUuZJtKfq746UhlqPQ89UTm7U895E/M/rJBhc9lIu4wRBstmiIBE2RvYkEtvnChiKsSGUKW5utdmQKsrQBFc0IbjzX14kzbOKe16p3lbLtas8jgI5JMfklLjkgtTIDamTBmHkkTyTV/JmPVkv1rv1MStdsvKeA/IH1ucPS6SZuw==</latexit>V
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="D9dv/kQhc4rGfkzamaQeeya8K1s=">AAACPXicbVDLSsNAFJ34rPXV6tJNsCgupCRS1GXRjcsW7APaUCaTSTt0kgkzN6Ul9Avc6vf4HX6AO3Hr1mmahWm9cOFw7r3cc44bcabAsj6Mjc2t7Z3dwl5x/+Dw6LhUPmkrEUtCW0RwIbsuVpSzkLaAAafdSFIcuJx23PHjYt6ZUKmYCJ9hFlEnwMOQ+Yxg0FQzGpQqVtVKy1wHdgYqKKvGoGxc9j1B4oCGQDhWqmdbETgJlsAIp/NiP1Y0wmSMh7SnYYgDqpwkVTo3LzTjmb6QukMwU/bvRYIDpWaBqzcDDCO1OluQ/816Mfj3TsLCKAYakuUjP+YmCHNh2/SYpAT4TANMJNNaTTLCEhPQ4eS+uEKMAbsq5yTRTMyxnF6nksHleaPehEUqszpdetXJ2qs5roP2TdW+rdaatUr9Icu4gM7QObpCNrpDdfSEGqiFCKLoBb2iN+Pd+DS+jO/l6oaR3ZyiXBk/v0SrsCU=</latexit>pToken ProtectionToken ProtectionToken Protection<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="D9dv/kQhc4rGfkzamaQeeya8K1s=">AAACPXicbVDLSsNAFJ34rPXV6tJNsCgupCRS1GXRjcsW7APaUCaTSTt0kgkzN6Ul9Avc6vf4HX6AO3Hr1mmahWm9cOFw7r3cc44bcabAsj6Mjc2t7Z3dwl5x/+Dw6LhUPmkrEUtCW0RwIbsuVpSzkLaAAafdSFIcuJx23PHjYt6ZUKmYCJ9hFlEnwMOQ+Yxg0FQzGpQqVtVKy1wHdgYqKKvGoGxc9j1B4oCGQDhWqmdbETgJlsAIp/NiP1Y0wmSMh7SnYYgDqpwkVTo3LzTjmb6QukMwU/bvRYIDpWaBqzcDDCO1OluQ/816Mfj3TsLCKAYakuUjP+YmCHNh2/SYpAT4TANMJNNaTTLCEhPQ4eS+uEKMAbsq5yTRTMyxnF6nksHleaPehEUqszpdetXJ2qs5roP2TdW+rdaatUr9Icu4gM7QObpCNrpDdfSEGqiFCKLoBb2iN+Pd+DS+jO/l6oaR3ZyiXBk/v0SrsCU=</latexit>pToken ProtectionToken ProtectionToken Protection
<latexit sha1_base64="D9dv/kQhc4rGfkzamaQeeya8K1s=">AAACPXicbVDLSsNAFJ34rPXV6tJNsCgupCRS1GXRjcsW7APaUCaTSTt0kgkzN6Ul9Avc6vf4HX6AO3Hr1mmahWm9cOFw7r3cc44bcabAsj6Mjc2t7Z3dwl5x/+Dw6LhUPmkrEUtCW0RwIbsuVpSzkLaAAafdSFIcuJx23PHjYt6ZUKmYCJ9hFlEnwMOQ+Yxg0FQzGpQqVtVKy1wHdgYqKKvGoGxc9j1B4oCGQDhWqmdbETgJlsAIp/NiP1Y0wmSMh7SnYYgDqpwkVTo3LzTjmb6QukMwU/bvRYIDpWaBqzcDDCO1OluQ/816Mfj3TsLCKAYakuUjP+YmCHNh2/SYpAT4TANMJNNaTTLCEhPQ4eS+uEKMAbsq5yTRTMyxnF6nksHleaPehEUqszpdetXJ2qs5roP2TdW+rdaatUr9Icu4gM7QObpCNrpDdfSEGqiFCKLoBb2iN+Pd+DS+jO/l6oaR3ZyiXBk/v0SrsCU=</latexit>p (b) MHA in Linformer
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dLinearLinear
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>kContext  vectorLinear
<latexit sha1_base64="N/EBWZoOVLkLENSRb/DG5XWv4dg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWjW4v6zUb/I4inACp3AOAVxBHe6gAU0g8AjP8ApvnvJevHfvY9Fa8PKZY/gD7/MHnlePKQ==</latexit> ReLU<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="NDYzpDPf4NReujBVQrH7hehsVpg=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzU8Prlilt15yCrxMtJBXLU++Wv3iBmaYTSMEG17npuYvyMKsOZwGmpl2pMKBvTIXYtlTRC7WfzQ6fkzCoDEsbKljRkrv6eyGik9SQKbGdEzUgvezPxP6+bmvDGz7hMUoOSLRaFqSAmJrOvyYArZEZMLKFMcXsrYSOqKDM2m5INwVt+eZW0LqreVdVrXFZqt3kcRTiBUzgHD66hBvdQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBfJWMuw==</latexit>1<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
Context Scores<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="r3RhxPibzptum5p4iWVmnNh7d98=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyKqMegF48RzAOSJcxOZpMhM7PLPISw5Be8eFDEqz/kzb9xNtmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtnVhFaIskPFHdCGvKmaQtwwyn3VRRLCJOO9HkLvc7T1RplshHM01pKPBIspgRbHKpr60YVGt+3Z8DrZKgIDUo0BxUv/rDhFhBpSEca90L/NSEGVaGEU5nlb7VNMVkgke056jEguowm986Q2dOGaI4Ua6kQXP190SGhdZTEblOgc1YL3u5+J/Xsya+CTMmU2uoJItFseXIJCh/HA2ZosTwqSOYKOZuRWSMFSbGxVNxIQTLL6+S9kU9uKoHD5e1xm0RRxlO4BTOIYBraMA9NKEFBMbwDK/w5gnvxXv3PhatJa+YOYY/8D5/ADPjjlk=</latexit>X<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dLinear<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="gMTgjs7J9T7tfl8I4J/4iGZ8J/U=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APbUjLpnTY0kxmSjFiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybnHjwXXxnW/ncLK6tr6RnGztLW9s7tX3j9o6ihRDBssEpFq+1Sj4BIbhhuB7VghDX2BLX98k/mtR1SaR/LeTGLshXQoecAZNVZ66IbUjPwgfZr2yxW36s5AlomXkwrkqPfLX91BxJIQpWGCat3x3Nj0UqoMZwKnpW6iMaZsTIfYsVTSEHUvnSWekhOrDEgQKfukITP190ZKQ60noW8ns4R60cvE/7xOYoKrXsplnBiUbP5RkAhiIpKdTwZcITNiYgllitushI2ooszYkkq2BG/x5GXSPKt6F9Xzu/NK7TqvowhHcAyn4MEl1OAW6tAABhKe4RXeHO28OO/Ox3y04OQ7h/AHzucPADaRJQ==</latexit>x
<latexit sha1_base64="Orsd8Uetr8OD0z/KWA5V57CS7gM=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae2Q8mkmTY0kwxJRhiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybkniDnTxnW/ndLa+sbmVnm7srO7t39QPTzqaJkoQttEcql6AdaUM0HbhhlOe7GiOAo47QbT29zvPlGlmRQPJo2pH+GxYCEj2FjpcRBhMwnCLJ0NqzW37s6BVolXkBoUaA2rX4ORJElEhSEca9333Nj4GVaGEU5nlUGiaYzJFI9p31KBI6r9bJ54hs6sMkKhVPYJg+bq740MR1qnUWAn84R62cvF/7x+YsJrP2MiTgwVZPFRmHBkJMrPRyOmKDE8tQQTxWxWRCZYYWJsSRVbgrd88irpXNS9y3rjvlFr3hR1lOEETuEcPLiCJtxBC9pAQMAzvMKbo50X5935WIyWnGLnGP7A+fwBAbuRJg==</latexit>y<latexit sha1_base64="b2P15Xg35B2Uc1NdmMfGE2pXebI=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQRrCJYB6QXcLs5CYZMvtg5q4Ylm1s/BUbC0Vs/Qc7/8bJZgtNPDBwOOe+5niR4Aot69soLC2vrK4V10sbm1vbO+XdvZYKY8mgyUIRyo5HFQgeQBM5CuhEEqjvCWh746up374HqXgY3OEkAtenw4APOKOopV750EF4wGxOIqGfJo5PccSoSG7StFeuWFUrg7lI7JxUSI5Gr/zl9EMW+xAgE1Sprm1F6CZUImcC0pITK4goG9MhdDUNqA/KTbLtqXmslb45CKV+AZqZ+rsjob5SE9/TldMb1bw3Ff/zujEOLtyEB1GMELDZokEsTAzNaSRmn0tgKCaaUCa5vtVkIyopQx1cSYdgz395kbROq/ZZtXZbq9Qv8ziK5IAckRNik3NSJ9ekQZqEkUfyTF7Jm/FkvBjvxsestGDkPfvkD4zPHzrimbA=</latexit>K<latexit sha1_base64="Ta0kqL4N5jGMEBTHhbK9R3DfFLU=">AAACBXicbVDLSsNAFJ34rPVVdamLYBFclUSKuiy6cVnBPqAtZTK5aYdOJmHmRiwhGzf+ihsXirj1H9z5N07bLLT1wMDhnPua48WCa3Scb2tpeWV1bb2wUdzc2t7ZLe3tN3WUKAYNFolItT2qQXAJDeQooB0roKEnoOWNrid+6x6U5pG8w3EMvZAOJA84o2ikfumoi/CA0zmpAj9LuyHFIaMibWZZv1R2Ks4U9iJxc1ImOer90lfXj1gSgkQmqNYd14mxl1KFnAnIit1EQ0zZiA6gY6ikIeheOt2e2SdG8e0gUuZJtKfq746UhlqPQ89UTm7U895E/M/rJBhc9lIu4wRBstmiIBE2RvYkEtvnChiKsSGUKW5utdmQKsrQBFc0IbjzX14kzbOKe16p3lbLtas8jgI5JMfklLjkgtTIDamTBmHkkTyTV/JmPVkv1rv1MStdsvKeA/IH1ucPS6SZuw==</latexit>V<latexit sha1_base64="iLkvU3wtIpFtLSPmMsHyo5TUMdM=">AAACWnicbVBNS8NAEN3G7+/6cdNDsCgepCQi6lH0orcKVoW2lMlmqks32bA7kZaQi7/Gq/4bwR/jNs3BVgcWHm9m9s17QSKFIc/7qjgzs3PzC4tLyyura+sb1c2tB6NSzbHJlVT6KQCDUsTYJEESnxKNEAUSH4P+9aj/+IraCBXf0zDBTgTPsegJDmSpbnWvTTig4p9MY5hn7QjohYPMbvO8W615da8o9y/wS1BjZTW6m5XDdqh4GmFMXIIxLd9LqJOBJsEl5svt1GACvA/P2LIwhghNJyvkc/fAMqHbU9q+mNyC/b2RQWTMMArs5OhIM90bkf/1Win1LjqZiJOUMOZjoV4qXVLuKBM3FBo5yaEFwLWwt7r8BTRwsslNqARK9QkCM+Eks0wqQQ+Oi5MpkJNGw1eRmNLqYOzVJutP5/gXPJzU/bP66d1p7fKqzHiR7bJ9dsR8ds4u2Q1rsCbj7I29sw/2Wfl2HGfJWRmPOpVyZ5tNlLPzAxYxuNo=</latexit>I (c) Separable self-attention (ours)
Figure 3: Different self-attention units. (a) is a standard multi-headed self-attention (MHA) in
transformers. (b)extends MHA in (a) by introducing token projection layers, which project ktokens to a
pre-defined number of tokens p, thus reducing the complexity from O(k2)toO(k). However, it still uses
costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency
on resource-constrained devices (Fig. 1). (c)is the proposed separable self-attention layer that is linear in
complexity, i.e., O(k), and uses element-wise operations for faster inference.
MHA uses scaled dot-product attention to capture the contextual relationships between ktokens (or patches).
However, MHA is expensive as it has O(k2)time complexity. This quadratic cost is a bottleneck for
transformers with a large number of tokens k(Fig. 1). Moreover, MHA uses computationally- and memory-
intensive operations (e.g., batch-wise matrix multiplication and softmax for computing attention matrix;
Fig. 1); which could be a bottleneck on resource-constrained devices. To address the limitations of MHA for
efficient inference on resource-constrained devices, this paper introduces separable self-attention with linear
complexity (Fig. 3c).
The main idea of our separable self-attention approach, shown in Fig. 4b, is to compute context scores with
respect to a latent token L. These scores are then used to re-weight the input tokens and produce a context
vector, which encodes the global information. Because the self-attention is computed with respect to a latent
token, the proposed method can reduce the complexity of self-attention in the transformer by a factor k.
A simple yet effective characteristic of the proposed method is that it uses element-wise operations (e.g.,
summation and multiplication) for its implementation, making it a good choice for resource-constrained
devices. We call the proposed attention method separable self-attention because it allows us to encode global
information by replacing the quadratic MHA with two separate linear computations. The improved model,
MobileViTv2 , is obtained by replacing MHA with separable self-attention in MobileViT.
In the rest of this section, we first briefly describe MHA (Section 3.1), and then elaborate on the details of
separable self-attention (Section 3.2) and MobileViTv2 architecture (Section 3.3).
3.1 Overview of multi-headed self-attention
MHA (Fig. 3a) allows transformer to encode inter-token relationships. Specifically, MHA takes an input
x∈Rk×dcomprising of k d-dimensional token (or patch) embeddings. The input xis then fed to three
branches, namely query Q, keyK, and valueV. Each branch (Q,K, andV) is comprised of hlinear layers
(or heads), which enables the transformer to learn multiple views of the input. The dot-product between
the output of linear layers in QandKis then computed simultaneously for all hheads, and is followed
by a softmax operation σto produce an attention (or context-mapping) matrix a∈Rk×k×h. Another
dot-product is then computed between aand the output of linear layers in Vto produce weighted sum output
yw∈Rk×dh×h, wheredh=d
his the head dimension . The outputs of hheads are concatenated to produce a
tensor with k d-dimensional tokens, which is then fed to another linear layer with weights WO∈Rd×dto
4Published in Transactions on Machine Learning Research (01/2023)
ij<latexit sha1_base64="kKU4slhRI+NaZu6rgC+4JkqVhio=">AAACj3icbVDLbhMxFHWGVwmvBJZIyCJBYkM0gyrKCkWwobsikbYoE0XXnpvUjcce2ddto1G+je/gA9jCL+AkI0RarmTp6Jz78Dmi0spTmv5oJbdu37l7b+9++8HDR4+fdLpPj70NTuJIWm3dqQCPWhkckSKNp5VDKIXGE7H4tNZPLtB5Zc1XWlY4KWFu1ExJoEhNO99ygXNlagIRNLhVrVftnPCK6kNj0L2pnC2CJC6QLhHNiuc53+pkF2h4X/U5mIL3z/txEE3xd9O000sH6ab4TZA1oMeaOpp2Wy/ywspQoiGpwftxllY0qcGRkhrj+uCxArmAOY4jNFCin9SbDFb8VWQKPrMuPkN8w/47UUPp/bIUsbMEOvPXtTX5P20caPZ+UitTBUIjt4dmQXOyfB0oL5RDSXoZAUin4l+5PAMHkmLsO1eEtYuYjt9xUpdBk3L2ctdfcaEq3zi82lpsx0Sz6/ndBMdvB9m7wf6X/d7wY5PtHnvOXrLXLGMHbMg+syM2YpJ9Zz/ZL/Y76SYHyYdkuG1NWs3MM7ZTyeEf4B/Mbw==</latexit>Inner-product betweentokeniandjj<latexit sha1_base64="zRcTmo6/KyQ8ShvUKIHWxZASHAU=">AAACNXicbVDLSsNAFJ34rPXV6lKQYBHERUmkqMuiG5ct2Ae0oUwm03boZCbM3FRL6Be41W/xW1y4E7f+gtM2C9N64MLh3Hu59xw/4kyD43xYa+sbm1vbuZ387t7+wWGheNTUMlaENojkUrV9rClngjaAAaftSFEc+py2/NH9rN8aU6WZFI8wiagX4oFgfUYwGKl+2SuUnLIzh71K3JSUUIpar2iddgNJ4pAKIBxr3XGdCLwEK2CE02m+G2saYTLCA9oxVOCQai+Zfzq1z40S2H2pTAmw5+rfjQSHWk9C30yGGIZ6uTcT/+t1YujfegkTUQxUkMWhfsxtkPbMth0wRQnwiSGYKGZ+tckQK0zAhJO54ks5AuzrjJMkjDkwJZ+y/oIxi3Tq8HlhMW8SdZfzWyXNq7J7Xa7UK6XqXZptDp2gM3SBXHSDqugB1VADEUTRC3pFb9a79Wl9Wd+L0TUr3TlGGVg/vzHTrEw=</latexit>⇤<latexit sha1_base64="ZX3R0tpnoWyEDov8q1UhyJUzgHM=">AAACSXicbVDLTgJBEJwFH4gvkKOJmUhMPBiya4h6JHrx4AETeSSAZHaYhZHZnc1ML0o2fItX/Ra/wM/wZjw5wB4E7KSTSnV3uqrcUHANtv1ppdJr6xubma3s9s7u3n4uf1DXMlKU1agUUjVdopngAasBB8GaoWLEdwVruMOb6bwxYkpzGTzAOGQdn/QD7nFKwFDdXKHtExi4Xky7evIYP53hu0k3V7RL9qzwKnASUERJVbt566jdkzTyWQBUEK1bjh1CJyYKOBVskm1HmoWEDkmftQwMiM90J56pn+ATw/SwJ5XpAPCM/XsRE1/rse+azalWvTybkv/NWhF4V52YB2EELKDzR14kMEg8jQL3uGIUxNgAQhU3WjEdEEUomMAWvrhSDoG4esFJ7EcCuJLPi/56Ix7qxOHL3GLWJOos57cK6ucl56JUvi8XK9dJthl0iI7RKXLQJaqgW1RFNUTRGL2iN/RufVhf1rf1M19NWclNAS1UKv0LmD2zWw==</latexit>csj,L<latexit sha1_base64="vJakunoZWF//T0puXfAbsX9ypv0=">AAACrHicbVFdb9MwFHXC1+gG6+ARCVm0CB5QlaAJ9jjBCw88DKndJjWlsp2bzqtjR/ZNt8rKD0Xix+C0EaIbV7J0dM+5X8e8UtJhkvyK4gcPHz1+sve0t3/w7Plh/+jFuTO1FTARRhl7yZkDJTVMUKKCy8oCK7mCC7782vIXK7BOGj3GdQWzki20LKRgGFLz/irjsJDaI+O1YrbxqullCLfox2YJmg6vh+8cXTFVg6PMAnWCKcgbmmV0q+NrKoxuYeBMUAyzkuEVL7yYu+anv/5AvzfD0BV0/nfMvD9IRskm6H2QdmBAujibH0Wvs9yIugSNQjHnpmlS4cwzi1IoCO1rBxUTS7aAaYCaleBmfmNQQ9+GTE4LY8PTSDfZfys8K51blzwo293dXa5N/o+b1liczLzUVY2gxXZQUSuKhrZu01xaEKjWATBhZdiViitmmcDwJztTuDHL4I7bucSXtUJpzc3ufflKVq678HZ7Yi84mt717z44/zhKP42OfxwPTr903u6RV+QNeU9S8pmckm/kjEyIIL+jONqPDuJRPI6n8WwrjaOu5iXZibj4A00H1UE=</latexit>Tokenj’s values are scaledby context scorecsj,L
12
31’2’
3’
<latexit sha1_base64="eFAanIKgMqnVzBBLNknN3NCMCYc=">AAACMHicbVBNSwMxFMzW7/pV9eglWEQPUnalqMeiF48KVoV2Ldn0bRua3SzJW7Use/DXeFV/jZ7Eqz9CTGsP1joQGGbeyyQTJFIYdN03pzA1PTM7N79QXFxaXlktra1fGpVqDnWupNLXATMgRQx1FCjhOtHAokDCVdA7GfhXt6CNUPEF9hPwI9aJRSg4Qyu1SptNhHsc3pMFMoU8YzeZt0e9nTxvlcpuxR2CThJvRMpkhLNW6avZVjyNIEYumTENz03Qz5hGwSXkxWZqIGG8xzrQsDRmERg/G4bndNsqbRoqbU+MdKj+3shYZEw/CuxkxLBr/noD8T+vkWJ45GciTlKEmP8EhamkqOigEdoWGjjKviWMa2HfSnmXacbR9jaWEijVQxaYsZ9kUSpRaHWXF21h3t96JsnlfsU7qFTPq+Xa8ai6ebJJtsgu8cghqZFTckbqhJMH8kieyLPz4rw6787Hz2jBGe1skDE4n99+A6r0</latexit>a1,10
<latexit sha1_base64="Z0OK05MF+4fBuGU0uLONeWJsBIs=">AAACMHicbVBNSwMxFMz6WetX1WMvwSJ6kLIroh6LXjxWsLbQriWbvtXQ7GZJ3qpl2YO/xqv6a/QkXv0RYlp7sOpAYJh5L5NMkEhh0HVfnanpmdm5+cJCcXFpeWW1tLZ+YVSqOTS4kkq3AmZAihgaKFBCK9HAokBCM+ifDP3mDWgjVHyOgwT8iF3FIhScoZW6pXIH4Q5H92SBTCHP2GXm7dK97Tzvlipu1R2B/iXemFTIGPVu6bPTUzyNIEYumTFtz03Qz5hGwSXkxU5qIGG8z66gbWnMIjB+NgrP6ZZVejRU2p4Y6Uj9uZGxyJhBFNjJiOG1+e0Nxf+8dorhkZ+JOEkRYv4dFKaSoqLDRmhPaOAoB5YwroV9K+XXTDOOtreJlECpPrLATPwki1KJQqvbvGgL837X85dc7FW9g+r+2X6ldjyurkDKZJPsEI8ckho5JXXSIJzckwfySJ6cZ+fFeXPev0ennPHOBpmA8/EFf7aq9Q==</latexit>a1,20
<latexit sha1_base64="jwm2tioidFKHgyz5b6qN1mfH4es=">AAACMHicbVA9T8MwFHT4pnwFGLtYVAgGVCVQASOChbFIlFZqS+W4r2DViSP7BaiiDPwaVuDXwIRY+REIN3SgwEmWTnfv+ewLYikMet6rMzE5NT0zOzdfWFhcWl5xV9cujEo0hxpXUulGwAxIEUENBUpoxBpYGEioB/2ToV+/AW2Eis5xEEM7ZFeR6AnO0Eodt9hCuMP8njSQCWQpu0z9Hbq3lWUdt+SVvRz0L/FHpERGqHbcz1ZX8SSECLlkxjR9L8Z2yjQKLiErtBIDMeN9dgVNSyMWgmmneXhGN63SpT2l7YmQ5urPjZSFxgzCwE6GDK/Nb28o/uc1E+wdtlMRxQlCxL+DeomkqOiwEdoVGjjKgSWMa2HfSvk104yj7W0sJVCqjywwYz9Jw0Si0Oo2K9jC/N/1/CUXu2V/v1w5q5SOjkfVzZEi2SDbxCcH5IickiqpEU7uyQN5JE/Os/PivDnv36MTzmhnnYzB+fgCgWmq9g==</latexit>a1,30
<latexit sha1_base64="GrEsGoBbYMapICRe/nCHmBlpaC4=">AAACUnicbVJNSwMxEM3W7/pV9ahIsIgepOyqqEfRgx4VrAptLdl0qqHZZElma8uyF3+NV/0tXvwrnkzbPVh1IPB4M5OX90gYS2HR9z+9wsTk1PTM7FxxfmFxabm0snprdWI4VLmW2tyHzIIUCqooUMJ9bIBFoYS7sHM+6N91wVih1Q32Y2hE7FGJtuAMHdUsbdQReji8J70wACpL2UO6v0cPdrKsWSr7FX9Y9C8IclAmeV01V7zNekvzJAKFXDJra4EfYyNlBgWXkBXriYWY8Q57hJqDikVgG+lQPqPbjmnRtjbuKKRD9udGyiJr+1HoJiOGT/Z3b0D+16sl2D5ppELFCYLiI6F2IilqOsiEtoQBjrLvAONGuLdS/sQM4+iSG1MJte4gC+2YkzRKJAqjn8f9tboitrnD3shi0SUa/M7vL7jdrwRHlcPrw/LpWZ7tLFknW2SXBOSYnJJLckWqhJMX8kreyLv34X0V3C8ZjRa8fGeNjFVh4RsXp7UU</latexit>a2,30
<latexit sha1_base64="xf+AJy+5AIod1nVSqDYDwlmNCXQ=">AAACUnicbVJNSwMxEM3Wr1q/Wj0qEiyiBym7RdSj6EGPClaFtpZsOtXQbLIks2pZ9uKv8aq/xYt/xZNpuwerDgQeb2by8h4JYyks+v6nV5ianpmdK86XFhaXllfKldVrqxPDocG11OY2ZBakUNBAgRJuYwMsCiXchP3TYf/mEYwVWl3hIIZ2xO6V6AnO0FGd8kYL4RlH96RnBkBlKbtL63s02MmyTrnq1/xR0b8gyEGV5HXRqXibra7mSQQKuWTWNgM/xnbKDAouISu1Egsx4312D00HFYvAttORfEa3HdOlPW3cUUhH7M+NlEXWDqLQTUYMH+zv3pD8r9dMsHfUToWKEwTFx0K9RFLUdJgJ7QoDHOXAAcaNcG+l/IEZxtElN6ESat1HFtoJJ2mUSBRGP0366z6K2OYOn8cWSy7R4Hd+f8F1vRYc1PYv96vHJ3m2RbJOtsguCcghOSbn5II0CCcv5JW8kXfvw/squF8yHi14+c4amajC4jcT/7US</latexit>a2,10<latexit sha1_base64="0KA703VpA+uRMIpPpMQDEl/4Qkg=">AAACUHicbVBNbxMxEJ1NKYS00ARuICGrEWoPKNqFinKM4MIxSORDSkLk9U5SK971yp5NE61W6q/ptf0tvfFPuIHzgUQSRrL89GbGz++FqZKWfP+nVzp4dPj4Sflp5ej42fOTau1Fx+rMCGwLrbTphdyikgm2SZLCXmqQx6HCbjj9sux3Z2is1Ml3WqQ4jPkkkWMpODlqVH01IJzT6p3cYFTk/Ef+4R0LzopiVK37DX9VbB8EG1CHTbVGNe/NINIiizEhobi1/cBPaZhzQ1IoLCqDzGLKxZRPsO9gwmO0w3wlXrC3jonYWBt3EmIr9t+NnMfWLuLQTcacruxub0n+r9fPaPxpmMskzQgTsRYaZ4qRZstEWCQNClILB7gw0v2ViStuuCCX25ZKqPWUeGi3nORxpkgafb3tL5rJ1G4cztcWKy7RYDe/fdB53wg+Ni6+XdSbnzfZluE1nMI5BHAJTfgKLWiDgBu4hTu49x68X97vkrce/XvDS9iqUuUPjqK1SA==</latexit>a3,10
<latexit sha1_base64="8Oh3R0cyhhcFCyperBwDNduZXUA=">AAACUHicbVBNSyNBEK2JumpWd6PeFKTZIHqQMJMV9Sh68ahgVEhi6OmpaJOe6aG7Rg3DgL9mr+tv8eY/8aadD8GoBU0/XlX16/fCVElLvv/slaamZ37Mzs2Xfy4s/vpdWVo+tzozAhtCK20uQ25RyQQbJEnhZWqQx6HCi7B3NOhf3KKxUidn1E+xHfPrRHal4OSoTmW1RXhPw3dyg1GR86v87zarbxZFp1L1a/6w2FcQjEEVxnXSWfLWW5EWWYwJCcWtbQZ+Su2cG5JCYVFuZRZTLnr8GpsOJjxG286H4gXbcEzEutq4kxAbsh83ch5b249DNxlzurGfewPyu14zo+5+O5dJmhEmYiTUzRQjzQaJsEgaFKT6DnBhpPsrEzfccEEutwmVUOse8dBOOMnjTJE0+m7SX3QrUzt2eD+yWHaJBp/z+wrO67Vgt7ZzulM9OBxnOwdr8Ae2IIA9OIBjOIEGCHiAf/AfHr0n78V7LXmj0fcbVmCiSuU3kHa1SQ==</latexit>a3,20
<latexit sha1_base64="nBDHjvohuoWIGe/u/kWtq7VsF3U=">AAACUHicbVBNTxsxEJ0NLYRQIMANpMoiqtoDinZJBBwjeukRpAaQkjTyeidgxbte2bOBaLUSv4Yr/BZu/Se9gfNRqYGOZPnpzYyf3wtTJS35/m+vtPTh4/JKebWy9ml9Y7O6tX1hdWYEtoVW2lyF3KKSCbZJksKr1CCPQ4WX4fD7pH85QmOlTn7SOMVezK8TOZCCk6P61d0u4R1N38kNRkXOf+WNA9b4WhT9as2v+9Ni70EwBzWY11l/y/vcjbTIYkxIKG5tJ/BT6uXckBQKi0o3s5hyMeTX2HEw4THaXj4VL9gXx0RsoI07CbEp++9GzmNrx3HoJmNON/Ztb0L+r9fJaHDSy2WSZoSJmAkNMsVIs0kiLJIGBamxA1wY6f7KxA03XJDLbUEl1HpIPLQLTvI4UySNvl30F41kaucO72YWKy7R4G1+78HFYT04qjfPm7XW6TzbMuzBPnyDAI6hBT/gDNog4B4e4BGevGfvj/dS8majf2/YgYUqVV4Bkkq1Sg==</latexit>a3,30
<latexit sha1_base64="oP6iIk5zs1r/eDQM975boGseCSI=">AAACUnicbVJNSwMxEM3W7/pV9ahIsIgepOwWUY+iBz1WsFVoa8mmUw3NJksyq5ZlL/4ar/pbvPhXPJm2e7DqQODxZiYv75EwlsKi7396hanpmdm5+YXi4tLyymppbb1hdWI41LmW2tyGzIIUCuooUMJtbIBFoYSbsH8+7N88grFCq2scxNCO2L0SPcEZOqpT2mohPOPonvTCAKgsZXdp9YBW97KsUyr7FX9U9C8IclAmedU6a952q6t5EoFCLpm1zcCPsZ0yg4JLyIqtxELMeJ/dQ9NBxSKw7XQkn9Fdx3RpTxt3FNIR+3MjZZG1gyh0kxHDB/u7NyT/6zUT7J20U6HiBEHxsVAvkRQ1HWZCu8IARzlwgHEj3Fspf2CGcXTJTaiEWveRhXbCSRolEoXRT5P+uo8itrnD57HFoks0+J3fX9CoVoKjyuHVYfn0LM92nmySHbJPAnJMTsklqZE64eSFvJI38u59eF8F90vGowUv39kgE1VY+gYV07UT</latexit>a2,20Query tokensKey tokens
<latexit sha1_base64="jrC/j5Xv7brc+Hbkkc8yQ09POBg=">AAACYXicbVBNSwMxEE3X7/rV6rEgwSIIQtmVoh5FLx4VbBXaUmbTrIYmmyWZrS1Lf4S/xqv+DM/+EbPtHqw6EHi8N5OZ98JECou+/1nylpZXVtfWN8qbW9s7u5XqXtvq1DDeYlpq8xiC5VLEvIUCJX9MDAcVSv4QDq9z/WHEjRU6vsdJwnsKnmIRCQboqH7lpIt8jNm1VkmKnAIij3OFKkAjxnRKuw49h1EG036l7jf8WdG/IChAnRR126+WDroDzVLl/mQSrO0EfoK9DAwKJvm03E0tT4AN4Yl3HIxBcdvLZq6m9MgxAxpp416MdMb+nMhAWTtRoevMT7S/tZz8T+ukGF30MhHnjmM2XxSlkqKmeUR0IAxnKCcOADPC3UrZMxhg6IJc2BJqPUQI7YKTTKUShdEvi/4GI5HYwuF4brHsEg1+5/cXtE8bwVmjedesX14V2a6TGjkkxyQg5+SS3JBb0iKMvJI38k4+Sl/ehlfx9uatXqmY2ScL5dW+AXu1uwM=</latexit>Compute attention matrixa
(a) Self-attention in transformers
123Input tokens
L
<latexit sha1_base64="8q1Ps2eM7n8OC6rhUlk92JoEfvA=">AAACYXicbVBNS8NAEN3Gr1q/aj0WZLEIglASKeqx2IvHCrYKTSmb7UaX7mbD7qS2hP4If41X/Rme/SNu2hyMOqc3b2aY914QC27AdT9Lztr6xuZWebuys7u3f1A9rPWNSjRlPaqE0o8BMUzwiPWAg2CPsWZEBoI9BJNONn+YMm24iu5hHrOhJE8RDzklYKlR9dwHNoO0o2ScAMNURVmPp4yC0niBfUngOQhTOpouRtWG23SXhf8CLwcNlFd3dFg69seKJpJFQAUxZuC5MQxTooFTwRYVPzEsJnRCntjAwohIZobp0tUCn1pmjEMrI7Sq8JL9eZESacxcBnYzE2l+zzLyv9kggfB6mPIocxzR1aMwERgUziLCY66tfTG3gFDNrVZMn4kmFGyQhS+BUhMggSk4SWUigGv1UvQ3nvLY5A5nK4sVm6j3O7+/oH/R9C6brbtWo32TZ1tGdXSCzpCHrlAb3aIu6iGKXtEbekcfpS9n26k6tdWqU8pvjlChnPo3gbC7Bw==</latexit>Compute context vectorcv
<latexit sha1_base64="aHfWEKiAVGS+vJisi00U66pH9CI=">AAACW3icbVBNSysxFE3H7/pVFVeCBIvgQsqMFN9bynPjwoWCVaGtJUnvaGhmMiR3fJYwv8Bf41Z/iQv/i2k7C6seCBzOvTf3nsMzJS2G4XslmJmdm19YXKour6yurdc2Nq+tzo2AltBKm1vOLCiZQgslKrjNDLCEK7jhg9NR/eYRjJU6vcJhBt2E3acyloKhl3q1/Q7CE47/cVzlULhOwvCBx070bHHnokN6XhS9Wj1shGPQnyQqSZ2UuOhtVHY7fS3yBFIUilnbjsIMu44ZlEJBUe3kFjImBuwe2p6mLAHbdeM7CrrvlT6NtfEvRTpWv044llg7TLjvHB1rv9dG4m+1do7x366TaZYjpGKyKM4VRU1H4dC+NCBQDT1hwkh/KxUPzDCBPsKpLVzrATJup5y4JFcojf4/7a//KDNbOnyaWKz6RKPv+f0k10eN6LjRvGzWT/6V2S6SHbJHDkhE/pATckYuSIsI8kxeyCt5q3wEM0E1WJm0BpVyZotMIdj+BH7YuLE=</latexit>cs1,L
<latexit sha1_base64="t9PN/CnU0tDvdtgnKc967oTZzGo=">AAACXHicbVBNSyNBEO2M3/ErruBlQRqD4kHCjIjuUfSwHjy4YFRIYujp1JgmPd1Dd40amvkH/hqvu39kL/4WO8kcjPqg4fGqqqveizMpLIbh/0owMzs3v7C4VF1eWV1br238uLE6NxyaXEtt7mJmQQoFTRQo4S4zwNJYwm08OB/Vbx/BWKHVNQ4z6KTsQYlEcIZe6tb22gjPOP7H/TYAqnDtlGE/Thzv2uLeHR7Qy6Lo1uphIxyDfiVRSeqkxFV3o7Ld7mmep6CQS2ZtKwoz7DhmUHAJRbWdW8gYH7AHaHmqWAq248aHFHTXKz2aaOOfQjpWP044llo7TGPfOTrWfq6NxO9qrRyTXx0nVJYjKD5ZlOSSoqajdGhPGOAoh54wboS/lfI+M4yjz3BqS6z1AFlsp5y4NJcojH6a9td7FJktHT5PLFZ9otHn/L6Sm8NGdNw4+nNUPz0rs10kP8kO2ScROSGn5IJckSbh5IW8kr/kX+UtmA2Wg9VJa1ApZzbJFIKtdzcduQU=</latexit>cs2,L1’2’3’Key tokens
<latexit sha1_base64="o+44f8/mvQjaFw1rq4ED37OSRt0=">AAACQHicbVDLSsNAFJ34rPXV6lKQYBFclUSKuiy6cVnBPqAJZTKZtEMnmTBzUy2hv+FWv8W/8A/ciVtXTtosTOuFC4dz7+Wec7yYMwWW9WGsrW9sbm2Xdsq7e/sHh5XqUUeJRBLaJoIL2fOwopxFtA0MOO3FkuLQ47Trje+yeXdCpWIieoRpTN0QDyMWMIJBU44TYhh5QUoGk9mgUrPq1rzMVWDnoIbyag2qxqnjC5KENALCsVJ924rBTbEERjidlZ1E0RiTMR7SvoYRDqly07nomXmuGd8MhNQdgTln/16kOFRqGnp6MxOplmcZ+d+sn0Bw46YsihOgEVk8ChJugjCzBEyfSUqATzXARDKt1SQjLDEBnVPhiyfEGLCnCk7SMOHApHgq+vMnLFa5w+eFxbJO1F7ObxV0Luv2Vb3x0Kg1b/NsS+gEnaELZKNr1ET3qIXaiKAYvaBX9Ga8G5/Gl/G9WF0z8ptjVCjj5xersLGO</latexit>cv<latexit sha1_base64="aHfWEKiAVGS+vJisi00U66pH9CI=">AAACW3icbVBNSysxFE3H7/pVFVeCBIvgQsqMFN9bynPjwoWCVaGtJUnvaGhmMiR3fJYwv8Bf41Z/iQv/i2k7C6seCBzOvTf3nsMzJS2G4XslmJmdm19YXKour6yurdc2Nq+tzo2AltBKm1vOLCiZQgslKrjNDLCEK7jhg9NR/eYRjJU6vcJhBt2E3acyloKhl3q1/Q7CE47/cVzlULhOwvCBx070bHHnokN6XhS9Wj1shGPQnyQqSZ2UuOhtVHY7fS3yBFIUilnbjsIMu44ZlEJBUe3kFjImBuwe2p6mLAHbdeM7CrrvlT6NtfEvRTpWv044llg7TLjvHB1rv9dG4m+1do7x366TaZYjpGKyKM4VRU1H4dC+NCBQDT1hwkh/KxUPzDCBPsKpLVzrATJup5y4JFcojf4/7a//KDNbOnyaWKz6RKPv+f0k10eN6LjRvGzWT/6V2S6SHbJHDkhE/pATckYuSIsI8kxeyCt5q3wEM0E1WJm0BpVyZotMIdj+BH7YuLE=</latexit>cs1,L
<latexit sha1_base64="t9PN/CnU0tDvdtgnKc967oTZzGo=">AAACXHicbVBNSyNBEO2M3/ErruBlQRqD4kHCjIjuUfSwHjy4YFRIYujp1JgmPd1Dd40amvkH/hqvu39kL/4WO8kcjPqg4fGqqqveizMpLIbh/0owMzs3v7C4VF1eWV1br238uLE6NxyaXEtt7mJmQQoFTRQo4S4zwNJYwm08OB/Vbx/BWKHVNQ4z6KTsQYlEcIZe6tb22gjPOP7H/TYAqnDtlGE/Thzv2uLeHR7Qy6Lo1uphIxyDfiVRSeqkxFV3o7Ld7mmep6CQS2ZtKwoz7DhmUHAJRbWdW8gYH7AHaHmqWAq248aHFHTXKz2aaOOfQjpWP044llo7TGPfOTrWfq6NxO9qrRyTXx0nVJYjKD5ZlOSSoqajdGhPGOAoh54wboS/lfI+M4yjz3BqS6z1AFlsp5y4NJcojH6a9td7FJktHT5PLFZ9otHn/L6Sm8NGdNw4+nNUPz0rs10kP8kO2ScROSGn5IJckSbh5IW8kr/kX+UtmA2Wg9VJa1ApZzbJFIKtdzcduQU=</latexit>cs2,L
<latexit sha1_base64="O0FUQn+gceBp1dpR/cJ8h+IUh2o=">AAACWnicbVBNTxsxEHWWUvJBS6DckCqLtFIPVbTbIuAY0QsHDkEiJFKSRl7vLFjxrlf2LBBZ+wf4NVzbf1KpPwYn2QMJPMnS05sZz7wXZlIY9P1/FW/j3eb7rWqt3tj+8HGnubt3bVSuOfS4kkoPQmZAihR6KFDCINPAklBCP5z+mtf7d6CNUOkVzjIYJ+wmFbHgDJ00aX4ZITzg4h+rISrsKGF4G8aWT0zx2/78Ti+KYtJs+W1/AfqaBCVpkRLdyW7l8yhSPE8gRS6ZMcPAz3BsmUbBJRT1UW4gY3zKbmDoaMoSMGO7OKOgX50S0Vhp91KkC/XlhGWJMbMkdJ3zY816bS6+VRvmGJ+OrUizHCHly0VxLikqOs+GRkIDRzlzhHEt3K2U3zLNOLoEV7aESk2RhWbFiU1yiUKr+1V/0Z3ITOnwYWmx7hIN1vN7Ta5/tIPj9tHlUatzVmZbJQfkkHwjATkhHXJOuqRHOHkkT+QP+Vv573lezWssW71KOfOJrMDbfwaHn7g8</latexit>cs3,L
<latexit sha1_base64="O0FUQn+gceBp1dpR/cJ8h+IUh2o=">AAACWnicbVBNTxsxEHWWUvJBS6DckCqLtFIPVbTbIuAY0QsHDkEiJFKSRl7vLFjxrlf2LBBZ+wf4NVzbf1KpPwYn2QMJPMnS05sZz7wXZlIY9P1/FW/j3eb7rWqt3tj+8HGnubt3bVSuOfS4kkoPQmZAihR6KFDCINPAklBCP5z+mtf7d6CNUOkVzjIYJ+wmFbHgDJ00aX4ZITzg4h+rISrsKGF4G8aWT0zx2/78Ti+KYtJs+W1/AfqaBCVpkRLdyW7l8yhSPE8gRS6ZMcPAz3BsmUbBJRT1UW4gY3zKbmDoaMoSMGO7OKOgX50S0Vhp91KkC/XlhGWJMbMkdJ3zY816bS6+VRvmGJ+OrUizHCHly0VxLikqOs+GRkIDRzlzhHEt3K2U3zLNOLoEV7aESk2RhWbFiU1yiUKr+1V/0Z3ITOnwYWmx7hIN1vN7Ta5/tIPj9tHlUatzVmZbJQfkkHwjATkhHXJOuqRHOHkkT+QP+Vv573lezWssW71KOfOJrMDbfwaHn7g8</latexit>cs3,L
<latexit sha1_base64="3A/mVRfJCaZ9GpsUYMykQS+za3M=">AAACYXicbVDLSgNBEJys7/iK8SjIYBAEIeyKqEcxF48KRoUkhNlJrw6ZxzLTq4YlH+HXeNXP8OyPOJvswagFAzXV3XRXxakUDsPwsxLMzS8sLi2vVFfX1jc2a1v1W2cyy6HNjTT2PmYOpNDQRoES7lMLTMUS7uJhq6jfPYF1wugbHKXQU+xBi0Rwhl7q1w67CC+Yt4xKMwTKjS7+1HFjwdEx7SqGj3GS874b92uNsBlOQP+SqCQNUuKqv1XZ7Q4MzxRo5JI514nCFHs5syi4hHG1mzlIGR+yB+h4qpkC18snrsZ03ysDmhjrn0Y6UX9O5Ew5N1Kx7yyOdL9rhfhfrZNhctbLhS4caz5dlGSSoqFFRHQgLHCUI08Yt8LfSvkjs4yjD3JmS2zMEFnsZpzkKpMorHme9Td4EqkrHb5MLVZ9otHv/P6S26NmdNI8vj5unF+U2S6THbJHDkhETsk5uSRXpE04eSVv5J18VL6ClaAW1KetQaWc2SYzCHa+AXS5uwA=</latexit>Compute context scorescs<latexit sha1_base64="zRcTmo6/KyQ8ShvUKIHWxZASHAU=">AAACNXicbVDLSsNAFJ34rPXV6lKQYBHERUmkqMuiG5ct2Ae0oUwm03boZCbM3FRL6Be41W/xW1y4E7f+gtM2C9N64MLh3Hu59xw/4kyD43xYa+sbm1vbuZ387t7+wWGheNTUMlaENojkUrV9rClngjaAAaftSFEc+py2/NH9rN8aU6WZFI8wiagX4oFgfUYwGKl+2SuUnLIzh71K3JSUUIpar2iddgNJ4pAKIBxr3XGdCLwEK2CE02m+G2saYTLCA9oxVOCQai+Zfzq1z40S2H2pTAmw5+rfjQSHWk9C30yGGIZ6uTcT/+t1YujfegkTUQxUkMWhfsxtkPbMth0wRQnwiSGYKGZ+tckQK0zAhJO54ks5AuzrjJMkjDkwJZ+y/oIxi3Tq8HlhMW8SdZfzWyXNq7J7Xa7UK6XqXZptDp2gM3SBXHSDqugB1VADEUTRC3pFb9a79Wl9Wd+L0TUr3TlGGVg/vzHTrEw=</latexit>⇤
<latexit sha1_base64="zRcTmo6/KyQ8ShvUKIHWxZASHAU=">AAACNXicbVDLSsNAFJ34rPXV6lKQYBHERUmkqMuiG5ct2Ae0oUwm03boZCbM3FRL6Be41W/xW1y4E7f+gtM2C9N64MLh3Hu59xw/4kyD43xYa+sbm1vbuZ387t7+wWGheNTUMlaENojkUrV9rClngjaAAaftSFEc+py2/NH9rN8aU6WZFI8wiagX4oFgfUYwGKl+2SuUnLIzh71K3JSUUIpar2iddgNJ4pAKIBxr3XGdCLwEK2CE02m+G2saYTLCA9oxVOCQai+Zfzq1z40S2H2pTAmw5+rfjQSHWk9C30yGGIZ6uTcT/+t1YujfegkTUQxUkMWhfsxtkPbMth0wRQnwiSGYKGZ+tckQK0zAhJO54ks5AuzrjJMkjDkwJZ+y/oIxi3Tq8HlhMW8SdZfzWyXNq7J7Xa7UK6XqXZptDp2gM3SBXHSDqugB1VADEUTRC3pFb9a79Wl9Wd+L0TUr3TlGGVg/vzHTrEw=</latexit>⇤
<latexit sha1_base64="zRcTmo6/KyQ8ShvUKIHWxZASHAU=">AAACNXicbVDLSsNAFJ34rPXV6lKQYBHERUmkqMuiG5ct2Ae0oUwm03boZCbM3FRL6Be41W/xW1y4E7f+gtM2C9N64MLh3Hu59xw/4kyD43xYa+sbm1vbuZ387t7+wWGheNTUMlaENojkUrV9rClngjaAAaftSFEc+py2/NH9rN8aU6WZFI8wiagX4oFgfUYwGKl+2SuUnLIzh71K3JSUUIpar2iddgNJ4pAKIBxr3XGdCLwEK2CE02m+G2saYTLCA9oxVOCQai+Zfzq1z40S2H2pTAmw5+rfjQSHWk9C30yGGIZ6uTcT/+t1YujfegkTUQxUkMWhfsxtkPbMth0wRQnwiSGYKGZ+tckQK0zAhJO54ks5AuzrjJMkjDkwJZ+y/oIxi3Tq8HlhMW8SdZfzWyXNq7J7Xa7UK6XqXZptDp2gM3SBXHSDqugB1VADEUTRC3pFb9a79Wl9Wd+L0TUr3TlGGVg/vzHTrEw=</latexit>⇤<latexit sha1_base64="+jqneO7rnvdYuFH6gonTHQiKhfE=">AAACOHicbVDLSsNAFJ34rPXV6lKQYBFclUSKuiy6cVnBPqANZTKZtkNnMmHmTrWE/oJb/Rb/xJ07cesXmLRZmNYDFw7n3su95/gRZxoc58NaW9/Y3Nou7BR39/YPDkvlo5aWRhHaJJJL1fGxppyFtAkMOO1EimLhc9r2x3dpvz2hSjMZPsI0op7Aw5ANGMGQSj1tRL9UcarOHPYqcTNSQRka/bJ12gskMYKGQDjWuus6EXgxVsAIp7Niz2gaYTLGQ9pNaIgF1V48f3ZmnydKYA+kSioEe67+3Yix0Hoq/GRSYBjp5V4q/tfrGhjceDELIwM0JItDA8NtkHbq3A6YogT4NCGYKJb8apMRVphAkk/uii/lGLCvc05iYTgwJZ/y/oIJi3Tm8HlhsZgk6i7nt0pal1X3qlp7qFXqt1m2BXSCztAFctE1qqN71EBNRNAIvaBX9Ga9W5/Wl/W9GF2zsp1jlIP18wtqP63x</latexit>X (b) Proposed separable self-attention method
Figure 4: Example illustrating the interaction between tokens to learn global representations
in different attention layers. In (a), each query token computes the distance with all key tokens via
dot-product. These distances are then normalized using softmax to produce an attention matrix a, which
encodes contextual relationships. In (b), the inner product between input tokens and latent token Lis
computed. The resultant vector is normalized using softmax to produce context scores cs. These context
scores are used to weight key tokens and produce a context vector cv, which encodes contextual information.
produce the output of MHA y∈Rk×d. Mathematically, this operation can be described as:
y=Concat
⟨σ/parenleftbig
⟨xWQ0,xWK0⟩/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
a0∈Rk×k,xWV0⟩,···,⟨σ/parenleftig
⟨xWQh,xWKh⟩/parenrightig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
ah∈Rk×k,xWVh⟩
WO(1)
where WQi∈Rd×dh,WKi∈Rd×dh, and WVi∈Rd×dhare the weights of the i-th linear layer (or head) in
Q,K, andVbranches respectively. The symbol ⟨·,·⟩denotes the dot-product operation.
3.2 Separable self-attention
The structure of separable self-attention is inspired by MHA. Similar to MHA, the input xis processed using
three branches, i.e., input I, keyK, and valueV. The input branch Imaps eachd-dimensional token in xto
a scalar using a linear layer with weights WI∈Rd. The weights WIserves as the latent node Lin Fig. 4b.
This linear projection is an inner-product operation and computes the distance between latent token Land
x, resulting in a k-dimensional vector. A softmax operation is then applied to this k-dimensional vector to
produce context scores cs∈Rk. Unlike transformers that compute the attention (or context) score for each
token with respect to all ktokens, the proposed method only computes the context score with respect to a
latent token L. This reduces the cost of computing attention (or context) scores from O(k2)toO(k).
The context scores csare used to compute a context vector cv. Specifically, the input xis linearly projected
to ad-dimensional space using key branch Kwith weights WK∈Rd×dto produce an output xK∈Rk×d.
The context vector cv∈Rdis then computed as a weighted sum of xKas:
cv=k/summationdisplay
i=1cs(i)xK(i) (2)
The context vector cvis analogous to the attention matrix ain Eq. (1) in a sense that it also encodes the
information from all tokens in the input x, but is cheap to compute.
The contextual information encoded in cvis shared with all tokens in x. To do so, the input xis linearly
projected to a d-dimensional space using a value branch Vwith weights WV∈Rd×d, followed by a ReLU
5Published in Transactions on Machine Learning Research (01/2023)
Table 1:Effect of different self-attention methods on the performance of MobileViT on the ImageNet-1k
dataset. Here, all models have similar number of parameters and FLOPs, and latency is measured on iPhone12
(CPU and neural engine (NE)).
Attention unitLatency↓↓Top-1↑↑
NE CPU
Self-attention in Transformer (Fig. 3a) 9.9 ms 78.6 ms 78.4
Self-attention in Linformer (Fig. 3b) 10.2 ms 83.2 ms 78.2
Separable self-attention (Ours; Fig. 3c) 3.4ms38.6ms 78.1
activation to produce an output xV∈Rk×d. The contextual information in cvis then propagated to xVvia
broadcasted element-wise multiplication operation. The resultant output is then fed to another linear layer
with weights WO∈Rd×dto produce the final output y∈Rk×d. Mathematically, separable self-attention can
be defined as:
y=
/summationdisplay
cs∈Rk
/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright
σ(xWI)∗xWK

/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
cv∈Rd∗ReLU (xWV)
WO (3)
where∗and/summationtextare broadcastable element-wise multiplication and summation operations, respectively.
Comparison with self-attention methods Fig. 1 compares the proposed method with Transformer and
Linformer1. Because time complexity of self-attention methods do not account for the cost of operations that
are used to implement these methods, some of the operations may become bottleneck on resource-constrained
devices. For holistic understanding, module-level latency on a single CPU core with varying kis also measured
in addition to theoretical metrics. The proposed separable self-attention is fast and efficient as compared to
MHA in Transformer and Linformer.
Besides these module-level results, when we replaced the MHA in the transformer with the proposed self-
separable attention in the MobileViT architecture, we observe 3×improvement in inference speed with similar
performance on the ImageNet-1k dataset (Table 1). These results show the efficacy of the proposed separable
self-attention at the architecture-level. Note that self-attention in Transformer and Linformer yields similar
results for MobileViT. This is because the number of tokens kin MobileViT is fewer ( k≤1024) as compared
to language models, where Linformer is significantly faster than the transformer.
Relationship with additive addition The proposed approach resembles the attention mechanism of
Bahdanau et al. (2014), which also encodes the global information by taking a weighted-sum of LSTM outputs
at each time step. Unlike Bahdanau et al. (2014), where input tokens interact via recurrence, the input tokens
in the proposed method interact only with a latent token.
3.3 MobileViTv2 architecture
To demonstrate the effectiveness of the proposed separable self-attention on resource-constrained devices,
we integrate separable self-attention with a recent ViT-based model, MobileViT (Mehta & Rastegari, 2022).
MobileViT is a light-weight, mobile-friendly hybrid network that delivers significantly better performance than
other competitive CNN-based, transformer-based, or hybrid models, including MobileNets (Howard et al.,
1Inspired by sampling-based methods (e.g., DynamicViT (Rao et al., 2021)) for reducing the computational cost of self-
attention recent works, we also studied the proposed method with different sampling methods (random, uniform, top-k). We
observe that these approaches are effective in reducing the FLOPs of the model with little or no drop in performance. However,
these sampling-based methods have high latency on mobile devices as compared to the models without these methods (see
Appendix E). This is likely because these approaches have high memory access cost (because of tensor memory re-ordering) and
constitutes to large portion of latency.
6Published in Transactions on Machine Learning Research (01/2023)
2017; Sandler et al., 2018; Howard et al., 2019). To avoid ambiguity, we refer to MobileViT as MobileViTv1
in the rest of the paper.
Specifically, we replace MHA in the transformer block in the MobileViTv1 with the proposed separable
self-attention method. We call the resultant architecture MobileViTv2 . We also do not use the skip-connection
and fusion block in the MobileViT block (Fig. 1b in (Mehta & Rastegari, 2022)) as it improves the performance
marginally (Fig. 12 in (Mehta & Rastegari, 2022)). Furthermore, to create MobileViTv2 models at different
complexities, we uniformly scale the width of MobileViTv2 network using a width multiplier α∈{0.5,2.0}.
This is in contrast to MobileViTv1 which trains three specific architectures (XXS, XS, and S) for mobile
devices. More details about MobileViTv2 ’s architecture are given in Appendix A.
4 Experimental results
4.1 Object classification on the ImageNet dataset
Training on ImageNet-1k from scratch We train MobileViTv2 for 300 epochs with an effective batch
size of 1024 images (128 images per GPU ×8 GPUs) using AdamW of Loshchilov & Hutter (2019) on the
ImageNet-1k dataset (Russakovsky et al., 2015) with 1.28 million and 50 thousand training and validation
images respectively. We linearly increase the learning rate from 10−6to0.002for the first 20k iterations.
After that, the learning rate is decayed using a cosine annealing policy (Loshchilov & Hutter, 2017). To
reduce stochastic noise during training, we use exponential moving average (EMA) (Polyak & Juditsky, 1992)
as we find it helps larger models. We implement our models using CVNets(Mehta & Rastegari, 2022; Mehta
et al., 2022), and use their provided scripts for data processing, training, and evaluation.
Pre-training on ImageNet-21k-P and finetuning on ImageNet-1k We train on the ImageNet-21k
(winter’21 release) that contains about 13 million images across 19k classes. Specifically, we follow Ridnik
et al. (2021) to pre-process (e.g., remove classes with fewer samples) the dataset and split it into about 11
million and 522 thousand training and validation images spanning over 10,450 classes, respectively. Following
Ridnik et al. (2021), we refer to this pre-processed dataset as ImageNet-21k-P. Note that the ImageNet-21k-P
validation set does not overlap with the validation and test sets of ImageNet-1k.
We follow Ridnik et al. (2021) for pre-training MobileViTv2 on ImageNet-21k-P. For faster convergence, we
initialize MobileViTv2 models with ImageNet-1k weights and finetune it on ImageNet-21k-P for 80 epochs
with an effective batch size of 4096 images (128 images per GPU x 32 GPUs). We do not use any linear
warm-up. Other settings follow ImageNet-1k training.
We finetune ImageNet-21k-P pre-trained models on ImageNet-1k for 50 epochs using SGD with momentum
(0.9) and cosine annealing policy with an effective batch size of 256 images (128 images per GPU ×2 GPUs).
Finetuning at higher resolution MobileViTv2 is a hybrid architecture that combines convolution
and separable self-attention to learn visual representations. Unlike many ViT-based models (e.g., DeiT),
MobileViTv2 does not require adjustment to patch embeddings or positional biases for different input
resolutions and is simple to finetune. We finetune MobileViTv2 models at higher resolution (i.e., 384×384)
for 10 epochs with a fixed learning rate of 10−3using SGD.
Comparison with existing methods Table 2 and Fig. 2 compares MobileViTv2 ’s performance with
recent methods. We make following observations:
•When MHA in MobileViTv1 is replaced with separable self-attention, the resultant model, MobileViTv2 ,
is faster and better (Fig. 2); validating the effectiveness of the proposed separable self-attention method
for mobile ViTs.
•Compared to transformer-based (including hybrid) models, MobileViTv2 models are fast on mobile devices.
For example, MobileViTv2 is about 8×faster on a mobile device and delivers 2.5% better performance
on the ImageNet-1k dataset than MobileFormer (Chen et al., 2021b), even though MobileFormer is
FLOP efficient (R9 vs. R10). However, on GPU, both MobileFormer and MobileViTv2 run at a similar
7Published in Transactions on Machine Learning Research (01/2023)
Table 2:Classification performance on the ImageNet-1k validation set . Here, NS means that we are
not able to measure the latency on mobile device as some operations (e.g., cyclic shifts) are not supported on
mobile devices. Following Mehta & Rastegari (2022), latency is measured on iPhone12 with a batch size of 1.
Similar to Liu et al. (2021) and Liu et al. (2022), throughput is measured on NVIDIA V100 GPUs with a
batch size of 128. The rows are grouped by network parameters.
Row # Model TypeNeural Extra Image# Params↓↓FLOPs↓↓Latency↓↓Throughput↑↑Top-1↑↑
search? data size (in ms) (images/ sec) (in %)
R1 MobileViT-XXS Hybrid ✗ None 25621.3 M 0.4 G 4.8 4225 69.0
R2 MobileViTv2 -0.5 Hybrid ✗ None 25621.4 M 0.5 G 1.6 4595 70.2
R3 MobileFormer-52 Hybrid ✗ None 22423.6 M 52 M 7.1 4445 68.7
R4 MobileViTv2 -1.0 Hybrid ✗ None 25624.9 M 1.8 G 3.4 2351 78.1
R5 EfficientNet-b0 CNN ✓ None 22425.3 M 422 M 1.6 4619 77.1
R6 DeiT-Tiny Transformer ✗ None 22425.5 M 1.3 G 3.4 4541 72.2
R7 MobileViT-S Hybrid ✗ None 25625.6 M 2.0 G 9.9 1986 78.4
R8 EfficientNet-b2 CNN ✓ None 28829.1 M 1.2 G 3.8 2032 80.1
R9 MobileViTv2 -1.5 Hybrid ✗ None 256210.6 M 4.0 G 5.1 1418 80.4
R10 MobileFormer-294 Hybrid ✗ None 224211.8 M 294 M 40.7 1402 77.9
R11 MobileViTv2 -2.0 Hybrid ✗ None 256218.5 M 7.5 G 7.5 1105 81.2
R12 Swin-T Hybrid ✗ None 224228.3 M 4.5 G NS 1390 81.3
R13 ConvNext-T CNN ✗ None 224228.6 M 4.5 G 3.7 1800 82.1
R14 DeiT-Base Transformer ✗ None 224286.6 M 17.6 G 13.2 958 81.8
R15 MobileViTv2 -2.0 Hybrid ✗ImageNet-21k-P 256218.5 M 7.5 G 7.5 1105 82.4
R16 ConvNext-T CNN ✗ ImageNet-21k 224228.6 M 4.5 G 3.7 1800 82.9
R17 MobileViTv2 -2.0 Hybrid ✗ImageNet-21k-P 384218.5 M 16.1 G 17.0 488 83.4
R18 ConvNext-T CNN ✗ ImageNet-21k 384228.6 M 13.1 G 8.6 645 84.1
speed. The discrepancy in FLOPs and speed of MobileFormer across devices is primarily because of its
architectural design. MobileFormer has conditional operations between mobile and former blocks. Such
conditional operations, especially on resource-constrained devices, have a low degree of parallelism and
create memory bottlenecks, resulting in a high latency network. Ma et al. (2018) also makes a similar
observation for CNN-based architectures.
•MobileViTv2 bridges the latency gap between CNN- and ViT-based models on mobile devices while
maintaining performance with similar or fewer parameters. For example, on a mobile device, ConvNexT
(Liu et al., 2022) (CNN-based model) is 2×and3.6×faster than MobileViTv2 (hybrid model) and DeiT
(transformer-based model) for similar performance respectively (see R11, R13, and R14). The low latency
of fully CNN-based models on mobile devices can be attributed to several device-level optimizations that
have been done for CNN-based models over the past few years (e.g., dedicated hardware implementations
for convolutions and folding batch normalization with convolutions). ViT-based models still lack such
optimizations and therefore, the resultant inference graphs are sub-optimal. Though MobileViTv2 bridges
the latency gap between CNNs and ViTs, we believe the latency of ViT-based models will improve in the
future with similar optimizations.
•The delta in speed (on GPU) between ConvNext and MobileViTv2 (R15-R18) at higher model complexities
reduces from 1.6×to1.3×when input resolution is increased from 224×224(or256×256) to384×384,
suggesting ViT-based (including hybrid) models exhibit better scaling properties as compared to CNNs.
This is because of a higher degree of parallelism that ViT-based models offer at a large scale (Dosovitskiy
et al., 2021; Brown et al., 2020). Our results on down-stream tasks in Section 4.2 and previous work on
scaling ViTs (Dosovitskiy et al., 2021; Zhai et al., 2021b) further supports this observation.
See Appendix for more results on the task of image classification (e.g., Appendix B for MobileViTv2 ’s
classification performance on ImageNet-1k/21k-P, Appendix C for comparison with light-weight models, and
Appendix E for ablations, including the effect of augmentation, loss functions, multiple latent tokens, and
locality-based sampling methods on FLOP efficiency).
8Published in Transactions on Machine Learning Research (01/2023)
Table 3:Semantic segmentation results on the ADE20k and the PASCAL VOC 2012 datasets.
Here, throughput, network parameters, and FLOPs are measured on the ADE20k dataset for an input with a
spatial resolution of 512×512. mIoU (mean intersection over union) score is calculated for a single scale only.
Throughput is calculated using a batch size of 32 images on a single NVIDIA V100 GPU with 32 GB memory
and is an average of over 50 iterations (excluding 10 iterations for warmup). We do not report latency on a
mobile device as some of the operations (e.g., pyramid pooling in PSPNet) are not optimally implemented
for mobile devices. The baseline results are from MMSegmentation (2020). Rows are grouped by network
parameters.
Seg. ImageNet-1k Image Throughput ↑↑# Params↓↓FLOPs↓↓ mIoU↑↑
Model Backbone Size (images/sec) (in millions) (in billions) ADE20k PASCAL VOC
PSPNetMobileViTv2 -0.5 (Ours) 5122439 3.6 M 15.4 G 31.8 74.6
MobileNetv2 5122276 13.7 M 53.1 G 29.7 –
PSPNetMobileViTv2 -1.75 (Ours) 5122114 22.5 M 95.9 G 39.8 80.2
ResNet-50 5122119 49.1 M 179.1 G 41.1 76.8
DeepLabv3MobileViTv2 -0.75 (Ours) 5122241 9.6 M 40.0 G 34.7 75.1 (α=0.5)
MobileNetv2 5122246 18.7 M 75.4 G 34.1 –
DeepLabv3MobileViTv2 -2.0 (Ours) 512290 34.0 M 147.0 G 40.9 80.3(α=1.5)
ResNet-50 5122103 68.2 M 270.3 G 42.4 79.1
4.2 Evaluation on down-stream tasks
Semanticsegmentation Weintegrate MobileViTv2 withtwostandardsegmentationarchitectures, PSPNet
(Zhao et al., 2017) and DeepLabv3 (Chen et al., 2017), and study it on two standard semantic segmentation
datasets, ADE20k (Zhou et al., 2017) and PASCAL VOC 2012 (Everingham et al., 2015). For training details
including hyper-parameters, see supplementary material.
Table 3 and Fig. 2c compares the segmentation performance in terms of validation mean intersection over
union (mIOU) of MobileViTv2 with different segmentation methods. MobileViTv2 delivers competitive
performance at different complexities while having significantly fewer parameters and FLOPs. Interestingly,
the inference speed of MobileViTv2 models is comparable to CNN-based models, including light-weight
MobileNetv2 and heavy-weight ResNet-50 (He et al., 2016) model. This is consistent with our observation in
Section 4.1 (R17 vs. R18; Table 2) where we also observe that ViT-based models scale better than CNN’s at
higher input resolutions and model complexities.
Object detection We integrate MobileViTv2 with SSDLite of Sandler et al. (2018) (SSD head (Liu et al.,
2016) with separable convolutions) for mobile object detection, and study its performance on MS-COCO
dataset (Lin et al., 2014). We follow Mehta & Rastegari (2022) for training detection models. Table 4 and
Fig. 2b compares SSDLite’s detection performance in terms of validation mean average precision (mAP) using
different ImageNet-1k backbones. MobileViTv2 delivers competitive performance to models with different
capacities, further validating the effectiveness of the proposed self-separable attention method.
5 Visualizations of self-separable attention scores
Fig. 5 visualizes what the context scores learn at different output strides2ofMobileViTv2 network. We
found that separable self-attention layers pay attention to low-, mid-, and high-level features, and allow
MobileViTv2 to learn representations from semantically relevant image regions.
6 Robustness analysis of self-separable attention layer
Previous works have shown that vision transformer (ViTs) models generalizes better and robust as compared
to CNNs. A natural question arises is: Do separable self-attention layer exhibit similar properties? To study
the robustness of MobileViTv2 , we evaluate the performance of MobileViTv2 on the ImageNetv2 dataset
2Output stride is the ratio of the spatial dimension of the input to the feature map.
9Published in Transactions on Machine Learning Research (01/2023)
Table 4:Object detection using SSDLite on the MS-COCO dataset. Here, throughput is measured
with a batch of 128 images on the NVIDIA V100 GPU, and is an average over 50 iterations (excluding 10
iterations for warmup). Latency on a mobile device is not reported as some operations (e.g., hard swish) are
not optimally implemented for such devices. Rows are grouped by network parameters.
ImageNet-1k Image Throughput ↑↑# Params↓↓FLOPs↓↓mAP↑↑backbone Size (images/sec) (in millions) (in billions)
MobileViTv1-XXS 32022246 1.7 M 0.9 G 19.9
MobileViTv2 -0.5 (Ours) 32022782 2.0 M 0.9 G 21.2
MobileViTv2 -0.75 (Ours) 32021876 3.6 M 1.8 G 24.6
Mobilenetv2 32023052 4.3 M 0.8 G 22.1
MobileNetv3 32023884 5.0 M 0.6 G 22.0
MobileNetv1 32024330 5.1 M 1.3 G 22.2
MobileViTv2 -1.75 3202780 14.9 M 9.0 G 29.5
ResNet-50 3002744 22.9 M 20.2 G 25.2
Figure 5: Context score maps at different output strides (OS) of MobileViTv2 model. Observe how
context scores pay attention to semantically relevant image regions. ( Left to right: input image, context
scores at OS=8, context scores at OS=16, and context scores at OS=32). For more examples and details
about context score map generation, see Appendix D.
Model # Params ↓↓ImageNet Top-1 ↑↑ImageNetv2 Top-1 ↑↑
DeiT-T 5 M 72.2 60.4
MobileViT-S 5.5 M 78.4 66.5
MobileViTv2-1.5 4.9 M 78.1 66.4
DeiT-S 22 M 79.8 68.5
MobileViTv2-2.0 18.5 M 81.2 70.0
Table 5: MobileViTv2 models are robust and generalizes to harder images.
(matched frequency split) (Recht et al., 2019). Table 5 shows that MobileViTv2 models exhibit similar
robustness properties to transformer-based models (DeiT and MobileViT) and generalizes to harder images.
7 Conclusions
Transformer-based vision models are slow on mobile devices as compared to CNN-based models because
multi-headed self-attention is expensive on resource-constrained devices. In this paper, we introduce a
separable self-attention method that has linear complexity and can be implemented using hardware-friendly
10Published in Transactions on Machine Learning Research (01/2023)
element-wise operations. Experimental results on standard datasets and tasks demonstrate the effectiveness
of the proposed method over multi-headed self-attention.
In this paper, we do not use methods (e.g., distillation and neural architecture search), which have found
to be effective in improving the accuracy and efficiency of recent efficient vision transformer models (Li
et al., 2022). In the future, we plan to study such methods to further improve the accuracy and efficiency of
MobileViTv2 models.
Acknowledgements
WearegratefultoAliFarhadi, PeterZatloukal, OncelTuzel, RickChang, FartashFaghri, FarzadAbdolhosseini,
Lailin Chen, and Max Horton for their helpful comments. We are also thankful to Apple’s infrastructure
and open-source teams for their help with training infrastructure and open-source release of the code and
pre-trained models.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to
align and translate. arXiv preprint arXiv:1409.0473 , 2014.
Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint
arXiv:2102.08602 , 2021.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 , 2020.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. CrossVit: Cross-attention multi-scale vision transformer
for image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV), 2021a.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution
for semantic image segmentation. arXiv preprint arXiv:1706.05587 , 2017.
Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu.
Mobile-former: Bridging mobilenet and transformer. arXiv preprint arXiv:2108.05895 , 2021b.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,
Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.
arXiv preprint arXiv:2009.14794 , 2020.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data
augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops , pp. 702–703, 2020.
Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:
Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697 ,
2021.
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization.
InInternational Conference on Learning Representations , 2022. URL https://openreview.net/forum?
id=shpkpVXzo3h .
11Published in Transactions on Machine Learning Research (01/2023)
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations , 2021. URL https://openreview.net/forum?id=YicbFdNTTy .
Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function
approximation in reinforcement learning. Neural Networks , 107:3–11, 2018.
Mark Everingham, SM Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes challenge: A retrospective. International journal of computer vision , 111
(1):98–136, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking
spatial dimensions of vision transformers. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , pp. 11936–11945, 2021.
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,
Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pp. 1314–1324, 2019.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861 , 2017.
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint
arXiv:2001.04451 , 2020.
Souvik Kundu and Sairam Sundaresan. Attentionlite: Towards efficient self-attention models for vision. In
ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ,
pp. 2225–2229. IEEE, 2021.
Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian
Ren. Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191 , 2022.
Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to vision
transformers. arXiv preprint arXiv:2104.05707 , 2021.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer
vision, pp. 740–755. Springer, 2014.
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexan-
der C Berg. Ssd: Single shot multibox detector. In European conference on computer vision , pp. 21–37.
Springer, 2016.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 10012–10022, 2021.
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet
for the 2020s. arXiv preprint arXiv:2201.03545 , 2022.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations , 2017.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations , 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .
12Published in Transactions on Machine Learning Research (01/2023)
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient
cnn architecture design. In Proceedings of the European conference on computer vision (ECCV) , pp.
116–131, 2018.
Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision
transformer. In International Conference on Learning Representations , 2022. URL https://openreview.
net/forum?id=vh-0sUt8HlG .
Sachin Mehta, Mohammad Rastegari, Linda Shapiro, and Hannaneh Hajishirzi. Espnetv2: A light-weight,
power efficient, and general purpose convolutional neural network. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 9190–9200, 2019.
Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Delight:
Deep and light-weight transformer. In International Conference on Learning Representations , 2021. URL
https://openreview.net/forum?id=ujmgfuxSLrO .
Sachin Mehta, Farzad Abdolhosseini, and Mohammad Rastegari. Cvnets: High performance library for
computer vision. CoRR, 2022.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training.
InInternational Conference on Learning Representations , 2018. URL https://openreview.net/forum?
id=r1gs9JgRZ .
Contributors MMSegmentation. MMSegmentation: Openmmlab semantic segmentation toolbox and bench-
mark. https://github.com/open-mmlab/mmsegmentation , 2020.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin
Tran. Image transformer. In International Conference on Machine Learning , pp. 4055–4064. PMLR, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems , 32, 2019.
Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal
on control and optimization , 30(4):838–855, 1992.
Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention
for long document understanding. arXiv preprint arXiv:1911.02972 , 2019.
Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient
vision transformers with dynamic token sparsification. In Advances in Neural Information Processing
Systems (NeurIPS) , 2021.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize
to imagenet? In International Conference on Machine Learning , pp. 5389–5400. PMLR, 2019.
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses.
arXiv preprint arXiv:2104.10972 , 2021.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision , 115(3):211–252, 2015.
Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner:
Adaptive space-time tokenization for videos. Advances in Neural Information Processing Systems , 34, 2021.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 4510–4520, 2018.
13Published in Transactions on Machine Learning Research (01/2023)
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pp. 1–9, 2015.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.
Training data-efficient image transformers & distillation through attention. In International Conference on
Machine Learning , pp. 10347–10357. PMLR, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
Apoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with clustered attention.
Advances in Neural Information Processing Systems , 33:21665–21674, 2020.
Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu.
Cluster-former: Clustering-based sparse transformer for question answering. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 , pp. 3958–3968, 2021a.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear
complexity. arXiv preprint arXiv:2006.04768 , 2020.
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling
Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 568–578, 2021b.
Ross Wightman, Hugo Touvron, and Hervé Jégou. Resnet strikes back: An improved training procedure in
timm.arXiv preprint arXiv:2110.00476 , 2021.
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing
convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 22–31, 2021.
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and
Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings
of the IEEE/CVF international conference on computer vision , 2021.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:
Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF
international conference on computer vision , pp. 6023–6032, 2019.
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers, 2021a.
URL https://arxiv.org/abs/2106.04560 .
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. CoRR,
abs/2106.04560, 2021b.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. arXiv preprint arXiv:1710.09412 , 2017.
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network.
InProceedings of the IEEE conference on computer vision and pattern recognition , pp. 2881–2890, 2017.
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In
Proceedings of the AAAI conference on artificial intelligence , 2020.
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing
through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 633–641, 2017.
14Published in Transactions on Machine Learning Research (01/2023)
A Detailed architecture of MobileViTv2
MobileViTv2 ’s architecture follows MobileViTv1 (Mehta & Rastegari, 2022) and is given in Table 6.
MobileViTv2 block, shown in Fig. 6, makes two changes to the MobileViTv1 block: (1) it replaces the
multi-headed self-attention with the proposed separable self-attention to learn global representations and (2)
it does not use fusion block and skip-connection (see Fig. 1b in Mehta & Rastegari (2022)) as they improve
the performance marginally (see Fig. 12 in Mehta & Rastegari (2022)). The expansion factor in MobileNetv2
(Sandler et al., 2018) blocks and feed-forward layers is two. Similar to Mehta & Rastegari (2022), we use
Swish (Elfwing et al., 2018) as a non-linear activation function. Unlike MobileViTv1 that creates three specific
architectures (XXS, XS, and S) for mobile devices, we uniformly scale the width of MobileViTv2 network
using a width multiplier α∈0.5,2.0to create models at different complexities.
BMobileViTv2 ’s classification performance
ImageNet-1k Table 7 shows the results of MobileViTv2 on the ImageNet-1k dataset. Finetuning
MobileViTv2 models at higher resolution ( 384×384) shows improvement across the board. For exam-
ple, the performance of MobileViTv2 -0.50 with 1.4 million parameters improves by about 2% when finetuned
at higher resolution (R1 vs. R2). Similarly, pre-training on the ImageNet-21k-P dataset helps improve the
performance of MobileViTv2 models. For example, ImageNet-21k-P pretraining improves the performance
ofMobileViTv2 -2.0 improves by 1.2% (R17 vs. R18). Notably, MobileViTv2 models pretrained on the
xDepth-wise
convPoint-wise
convUnfoldSeparable
self-attention/circleplustextFeed-forward
network/circleplustext
FoldPoint-wise
convyB×
Figure 6: MobileViTv2 block. Here, depth-wise convolution uses a kernel size of 3×3to encode local
representations. Similar to MobileViTv1, unfolding and folding operations uses a patch height and width of
two respectively. The separable self-attention and feed-forward layers are repeated B×before applying the
folding operation.
Table 6: MobileViTv2 architecture. Here,drepresents dimensionality of the input to the separable
self-attention layer, Bdenotes the repetition of transformer block with separable self-attention inside the
MobileViTv2 block (Fig. 6), and MV2 indicates MobileNetv2 block. Similar to MobileViTv1 block, we set
kernel size as three and spatial dimensions of patch (height hand widthw) as two in the MobileViTv2 block.
Layer Output size Output stride Repeat Output channels
Image 256×256 1
Conv- 3×3,↓2128×128 21 32α
MV2 1 64α
MV2,↓264×64 41 128α
MV2 2 128α
MV2,↓232×32 81 256α
MobileViTv2 block (Fig. 6; B= 2) 1 256∗α(d= 128α)
MV2,↓216×16 161 384α
MobileViTv2 block (Fig. 6; B= 4) 1 384α(d= 192α)
MV2,↓2
8×8 321 512α
MobileViTv2 block (Fig. 6; B= 3) 1 512α(d= 256α)
Global pool1×1 256 1512α
Linear 1000
15Published in Transactions on Machine Learning Research (01/2023)
Table 7:Classification performance of MobileViTv2 on the ImageNet-1k dataset. Here,†indicates
finetuning at higher resolution.
Row # Model Image size Extra data # Params ↓↓FLOPs↓↓Top-1↑↑
R1 MobileViTv2 -0.50 2562None 1.4 M 0.5 G 70.2
R2 MobileViTv2 -0.50†3842None 1.4 M 1.0 G 72.1
R3 MobileViTv2 -0.75 2562None 2.9 M 1.0 G 75.6
R4 MobileViTv2 -0.75†3842None 2.9 M 2.3 G 77.0
R5 MobileViTv2 -1.00 2562None 4.9 M 1.8 G 78.1
R6 MobileViTv2 -1.00†3842None 4.9 M 4.1 G 79.7
R7 MobileViTv2 -1.25 2562None 7.5 M 2.8 G 79.6
R8 MobileViTv2 -1.25†3842None 7.5 M 6.3 G 80.9
R9 MobileViTv2 -1.50 2562None 10.6 M 4.0 G 80.4
R10 MobileViTv2 -1.50 2562ImageNet-21k-P 10.6 M 4.0 G 81.5
R11 MobileViTv2 -1.50†3842None 10.6 M 9.1 G 81.5
R12 MobileViTv2 -1.50†3842ImageNet-21k-P 10.6 M 9.1 G 82.6
R13 MobileViTv2 -1.75 2562None 14.3 M 5.5 G 80.8
R14 MobileViTv2 -1.75 2562ImageNet-21k-P 14.3 M 5.5 G 81.9
R15 MobileViTv2 -1.75†3842None 14.3 M 12.3 G 82.0
R16 MobileViTv2 -1.75†3842ImageNet-21k-P 14.3 M 12.3 G 82.9
R17 MobileViTv2 -2.00 2562None 18.5 M 7.2 G 81.2
R18 MobileViTv2 -1.75 2562ImageNet-21k-P 18.5 M 7.2 G 82.4
R19 MobileViTv2 -2.00†3842None 18.5 M 16.1 G 82.2
R20 MobileViTv2 -1.50†3842ImageNet-21k-P 18.5 M 16.1 G 83.4
Table 8: Performance of MobileViTv2 on the ImageNet-21k-P validation set.
Width factor α# Params↓↓FLOPs↓↓Top-1↑↑Top-5↑↑
1.50 17.9 M 4.1 G 44.5 74.5
1.75 22.7 M 5.5 G 45.8 75.8
2.00 28.1 M 7.2 G 46.4 76.6
ImageNet-21k-P are able to achieve the similar performance with fewer FLOPs to models finetuned on
ImageNet-1k with a higher resolution (e.g., R10 vs. R11; R14 vs. R15; R18 vs. R19 in Table 7).
ImageNet-21k-P Table 8 shows the results on the ImageNet-21k-P validation dataset. The performance
ofMobileViTv2 improves with increase in model size.
C Comparisons with light-weight networks on the ImageNet-1k dataset
Comparison with light-weight CNNs. Fig. 7a shows that MobileViTv2 outperforms light-weight CNNs
across different network sizes (MobileNetv1 (Howard et al., 2017), MobileNetv2 (Sandler et al., 2018),
ShuffleNetv2 (Ma et al., 2018), ESPNetv2 (Mehta et al., 2019), and MobileNetv3 (Howard et al., 2019)).
Comparison with light-weight ViTs. Fig. 7b shows that MobileViTv2 achieves better performance
than previous light-weight ViT-based models acorss different network sizes (DeIT (Touvron et al., 2021),
T2T (Yuan et al., 2021), CrossViT (Chen et al., 2021a), LocalViT (Li et al., 2021), ConViT (d’Ascoli et al.,
2021), and Mobile-former (Chen et al., 2021b)).
D Visualizations of separable self-attention scores
The MobileViTv2 block, Fig. 6, unfolds the input x∈Rd×H×Wto obtain xu∈Rd×M×N, whereN=HW
hw
are the number of patches, each patch with width wand height h(M=hwpixels per patch). This unfolded
feature map is fed to separable self-attention module to learn non-local representations. To better understand
how separable self-attention processes xu, we visualize context scores cs.
The separable self-attention in MobileViTv2 block computes context scores csforMpixels simultaneously
acrossNpatches. Therefore, cshas a dimensions of M×N. To visualize context scores, we fold cs∈RM×Nto
16Published in Transactions on Machine Learning Research (01/2023)
0 1 2 3 4 5 6 7 8
# Parameters (in million)556065707580T op-1 accuracy (%)
Mobilenetv1
MobileNetv2
MobileNetv3
ShuffleNetv2
ESPNetv2
MobileViTv2 (Ours)
(a) Comparison with light-weight CNNs
2 4 6 8 10
# Parameters (in million)646668707274767880T op-1 accuracy (%)
DeIT
ConViT
Mobile-former
CrossViT
LocalViT
T2T
MobileViTv2 (Ours) (b) Comparison with light-weight ViTs
Figure 7: Comparison with light-weight CNN- and ViT-based models. MobileViTv2 is smaller and
better, which is desirable for mobile devices.
the same spatial dimensions as the input and obtain context score map cm∈RH×W. For ease of visualization,
we scale cmusing min-max normalization.
The context score maps for different input images at different output strides of MobileViTv2 model are
shown in Fig. 8. These visualizations show that the proposed separable self-attention method is able to (1)
aggregate information from entire image under different settings, including complex backgrounds, illumination
& view-point changes, and different objects, and (2) learn high-, mid-, and low-level representations.
EMobileViTv2 ’s ablation studies on the ImageNet-1k dataset
In this section, we study the effect on different methods on the performance of MobileViTv2 models, including
augmentation methods.
Standard vs. advanced augmentation We study two different augmentation methods: (1) standard
augmentation that uses Inception-style augmentation Szegedy et al. (2015), i.e., random resized cropping and
horizontal flipping and (2) advanced augmentation that uses RandAugment (Cubuk et al., 2020), CutMix
(Yun et al., 2019), MixUp (Zhang et al., 2017), and RandomErase (Zhong et al., 2020) along with standard
augmentation methods. The effect of these augmentations on the performance of MobileViTv2 is shown
in Figure 9. Smaller models ( <4.5million parameters) benefit from standard augmentation while larger
models (≥4.5million parameters) benefit from advanced augmentation. For simplicity, we use advanced
augmentation for all variants of MobileViTv2 in this paper.
Loss functions CutMix and Mixup augmentations mixes the samples in a batch. As a result, each
sample has multiple labels. Therefore, in presence of these augmentations, ImageNet classification can be
thought as a multi-label classification task. Similar to Wightman et al. (2021), we trained MobileViTv2 by
minimizing binary cross-entropy loss. Unlike Wightman et al. (2021), we did not observe any improvements
in the performance when cross-entropy loss with label smoothing is replaced with binary cross-entropy loss.
Therefore, we use cross-entropy with label smoothing for training MobileViTv2 models.
Effect of multiple latent tokens Similar to multi-head attention in transformers, the proposed separable
self-attention can have multiple latent tokens. When we changed the number of latent tokens from 1to8,
the performance improvements on the ImageNet-1k dataset were negligible (within ±0.1top-1 accuracy).
Therefore, we use only one latent token in our experiments.
We note that changing the number of heads from 4to1in multi-headed self-attention in the transformer
block of the MobileViTv1-S architecture dropped the top-1 accuracy by 0.7%. This observation is similar to
Vaswani et al. (2017), who also found that multiple heads in multi-headed self-attention improve transformers
performance on the task of neural machine translation.
17Published in Transactions on Machine Learning Research (01/2023)
Improving FLOP-efficiency via pixel- and patch-sampling The MobileViTv1 model Mehta &
Rastegari (2022) unfolds an input feature map into Npatches, each patch with M=hwpixels and applies
a transformer block for each pixel in a patch independently, where handware patch’s height and width
respectively. Because pixels in a patch are spatially correlated, one can sub-sample mpixels from Mpixels
and learn non-local representations by applying self-attention layers on mpixels only. Such sub-sampling
methods should help in reducing model FLOPs.
We tried following sampling methods at pixel- as well as patch-level:
•Random sampling , whereinmpixels (ornpatches) from Mpixels (orNpatches) are randomly selected
during training and uniformly during validation.
•Top-m(or top-n) sampling , wherein top- mpixels (or top- npatches) are selected based on their
magnitude computed using L2 norm.
•Uniform sampling , whereinmpixels (ornpatches) are sampled uniformly from Mpixels (orNpatches).
We found that these methods can reduce the FLOPs by 1.2×to1.6×with little or no drop in top-1 accuracy
on the ImageNet-1k dataset for both MobileViTv1 (with multi-headed self-attention) and MobileViTv2 (with
the proposed separable self-attention) models. However, these improvements in FLOPs did not translate to
latency improvements on a mobile device. In fact, models with these sampling methods were significantly
slower than the models without these methods. The high-latency of models with these sampling methods on
mobile devices can be attributed to their high memory access cost, as these methods change the memory order
of tensor. Because of their high-latency on mobile devices, we did not use these methods in the MobileViTv2
model.
FMobileViTv2 training configurations
Configurations for training and finetuning MobileViTv2 -2.0 on the ImageNet-1k and ImageNet-21k-P datasets
are given in Table 9 and Table 10 respectively while configurations for finetuning MobileViTv2 on downstream
tasks are given in Table 11.
18Published in Transactions on Machine Learning Research (01/2023)
Training config MobileViTv2 -2.0
Dataset ImageNet-1k ImageNet-21k-P
# Training samples 1.28 M 11 M
# Validation samples 50 k 523 k†
Train resolution 256×256 256×256
Val resolution 256×256 256×256
RandAug ✓ ✓
CutMix ✓ ✓
MixUp ✓ ✓
Random resized crop ✓ ✓
Random horizontal flip ✓ ✓
Random erase ✓ ✓
Stochastic depth ✗ ✗
Label smoothing ✓ ✓
Loss CE CE
Optimizer AdamW AdamW
Weight decay 0.05 0.05
Scheduler Cosine Cosine
Warm-up iterations 20 k None
Warm-up init LR 1e−6None
Warm-up scheduler Linear None
Base LR 0.002 0.0003
Epochs 300 80
Batch size 1024 4096
Layer-wise LR decay ✗ ✗
Grad. clip 10 10
Exp. moving average ✓ ✓
Weight init Random ImageNet-1k
Table 9: Configuration for training MobileViTv2 -2.0 on the ImageNet-1k/22k-P datasets.†The validation
set in ImageNet-21k-P does not overlap with ImageNet-1k validation set, and is created following Ridnik
et al. (2021).
19Published in Transactions on Machine Learning Research (01/2023)
Input Image
Context score map cmat an output stride of 8
Context score map cmat an output stride of 16
Context score map cmat an output stride of 32
(a)
Input Image
Context score map cmat an output stride of 8
Context score map cmat an output stride of 16
Context score map cmat an output stride of 32
(b)
Input Image
Context score map cmat an output stride of 8
Context score map cmat an output stride of 16
Context score map cmat an output stride of 32
(c)
Input Image
Context score map cmat an output stride of 8
Context score map cmat an output stride of 16
Context score map cmat an output stride of 32
(d)
Figure 8: Layer-wise visualization of context score maps cmat different output strides. Recall that
MobileViTv2 (Fig. 6 and Table 6) applies B= 2,B= 4, andB= 3separable self-attention layers at an
output strides of 8, 16, and 32 respectively. Therefore, we have 2, 4, and 8 context score maps at an output
stride of 8, 16, and 32 respectively
20Published in Transactions on Machine Learning Research (01/2023)
0.5 2.5 4.5 6.5 8.510.5 12.5 14.5 16.5 18.5
Parameters (in millions)70727476788082T op-1 accuracy (in %)
Standard
Advanced
Figure 9: Impact of data augmentation on the performance of MobileViTv2 models on the
ImageNet-1k dataset. For smaller models ( <4.5million parameters), standard augmentation works best
while larger models ( ≥4.5million parameters) benefit from advanced augmentation.
Training config MobileViTv2 -2.0
Dataset ImageNet-1k ImageNet-1k ImageNet-1k
# Training samples 1.28 M 1.28 M 1.28 M
# Validation samples 50 k 50 k 50 k
Train resolution 384×384 256×256 384 ×384
Val resolution 384×384 256×256 384 ×384
Weight init ImageNet-1k ImageNet-21k-P ImageNet-21k-P-1k†
RandAug ✗ ✓ ✓
CutMix ✗ ✓ ✓
MixUp ✗ ✓ ✓
Random resized crop ✓ ✓ ✓
Random horizontal flip ✓ ✓ ✓
Random erase ✗ ✓ ✓
Stochastic depth ✗ ✗ ✗
Label smoothing ✓ ✓ ✓
Loss CE CE CE
Optimizer SGD SGD SGD
Weight decay 4e−54e−54e−5
Scheduler Fixed Cosine Fixed
Warm-up iterations None None None
Warm-up init LR None None None
Warm-up scheduler None None None
Base LR 0.001 0.01 0.001
Epochs 10 50 10
Batch size 128 256 128
Layer-wise LR decay ✗ ✗ ✗
Grad. clip 10 10 10
Exp. moving average ✓ ✓ ✓
Table 10: Configuration for finetuning MobileViTv2 -2.0 on the ImageNet-1k dataset. Here,†denotes that
the ImageNet-21k-P model finetuned on the ImageNet-1k dataset at 256×256image resolution is used for
initializing the weights.
21Published in Transactions on Machine Learning Research (01/2023)
Training config SSDLite- MobileViTv2 -1.75 DeepLabv3- MobileViTv2 -1.75 DeepLabv3- MobileViTv2 -1.75
Dataset MS-COCO ADE20k PASCAL VOC 2012
Extra Data None None COCO
Task Detection Segmentation Segmentation
# Training samples 117 k 20 k 128 k
# Validation samples 5 k 2 k 1.45 k
Train resolution 320×320 512 ×512 512 ×512
Val resolution 320×320 Shortest side 512 Shortest side 512
Weight init ImageNet-1k ImageNet-1k ImageNet-1k
SSD Cropping ✓ ✗ ✗
Photometric distortion ✓ ✓ ✓
Random horizontal flip ✓ ✓ ✓
Resize ✓ ✗ ✗
Random short size resize ✗ ✓ ✓
Random Crop ✗ ✓ ✓
Random Gaussian blur ✗ ✓ ✓
Random rotation ✗ ✓ ✓
Loss Smooth L1 + CE CE CE
Optimizer AdamW SGD AdamW
Weight decay 0.05 1e−40.05
Scheduler Cosine Cosine Cosine
Warm-up iterations 500 None 500
Warm-up init LR 9e−5None 5e−5
Warm-up scheduler Linear None Linear
Base LR 0.0009 0.02 0.0005
Epochs 200 120 50
Batch size 128 16 128
Layer-wise LR decay ✗ ✗ ✗
Grad. clip 10 10 10
Exp. moving average ✓ ✓ ✓
Table 11: Configuration for finetuning MobileViTv2 on downstream tasks. For Ade20k, we found that SGD
was more stable as compared to AdamW across different MobileViTv2 configurations, and therefore, we used
SGD for finetuning on Ade20k dataset. The configurations for MobileViTv2 with PSPNet are the same as
Deeplabv3 on both PASCAL VOC and Ade20k datasets.
22