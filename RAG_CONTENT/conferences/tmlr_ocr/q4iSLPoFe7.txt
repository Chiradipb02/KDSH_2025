Published in Transactions on Machine Learning Research (02/2024)
Revisiting Generalized p-Laplacian Regularized Framelet
GCNs: Convergence, Energy Dynamic and as Non-Linear
Diffusion
Dai Shi∗20195423@student.westernsydney.edu.au
Western Sydney University
Zhiqi Shao∗zhiqi.shao@sydney.edu.au
University of Sydney
Yi Guo y.guo@westernsydney.edu.au
Western Sydney University
Qibin Zhao qibin.zhao@riken.jp
AIP RIKEN
Junbin Gao junbin.gao@sydney.edu.au
University of Sydney
Reviewed on OpenReview: https: // openreview. net/ forum? id= q4iSLPoFe7
Abstract
This paper presents a comprehensive theoretical analysis of the graph p-Laplacian reg-
ularized framelet network (pL-UFG) to establish a solid understanding of its properties.
We conduct a convergence analysis on pL-UFG, addressing the gap in understanding its
asymptotic behaviors. Further, by investigating the generalized Dirichlet energy of pL-UFG,
we demonstrate that the Dirichlet energy remains non-zero throughout convergence, ensuring
the avoidance of over-smoothing issues. Additionally, we elucidate the energy dynamic
perspective, highlighting the synergistic relationship between the implicit layer in pL-UFG
and graph framelets. This synergy enhances the model’s adaptability to both homophilic
and heterophilic data. Notably, we reveal that pL-UFG can be interpreted as a generalized
non-linear diffusion process, thereby bridging the gap between pL-UFG and differential
equations on the graph. Importantly, these multifaceted analyses lead to unified conclusions
that offer novel insights for understanding and implementing pL-UFG, as well as other
graph neural network models. Finally, based on our dynamic analysis, we propose two novel
pL-UFG models with manually controlled energy dynamics. We demonstrate empirically
and theoretically that our proposed models not only inherit the advantages of pL-UFG but
also significantly reduce computational costs for training on large-scale graph datasets.
1 Introduction
Graph neural networks (GNNs) have emerged as a popular tool for the representation learning on the
graph-structured data (Wu et al., 2020). To enhance the learning power of GNNs, many attempts have been
made by considering the propagation of GNNs via different aspects such as optimization (Zhu et al., 2021;
Wei et al., 2022), statistical test (Xu et al., 2019) and gradient flow (Bodnar et al., 2022; Di Giovanni et al.,
2023). In particular, treating GNNs propagation as an optimization manner allows one to assign different
types of regularizers on the GNNs’ output so that the variation of the node features, usually measured by
∗Dai Shi and Zhiqi Shao are with equal contributions
1Published in Transactions on Machine Learning Research (02/2024)
so-called Dirichlet energy, can be properly constrained (Zhu et al., 2021; Chen et al., 2023). The underlying
reason for this regularization operation is due to the recently identified computational issue of GNNs on
different types of graphs, namely homophily and heterophily (Zheng et al., 2022a). With the former, most of
nodes are connected with those nodes with identical labels, and for the latter, it is not (Pei et al., 2019).
Accordingly, an ideal GNN shall be able to produce a rather smoother node features for homophily graph
and more distinguishable node features when the input graph is heterophilic (Pei et al., 2019; Bi et al., 2022).
Based on the above statement, a proper design of the regularizer that is flexible enough to let GNN fit both
two types of graphs naturally becomes the next challenge. Recent research Fu et al. (2022) proposes a new
energy-based regularizer, namely the p-Laplacian-based regularizer, to the optimization of GNN and results in
an iterative algorithm to approximate the so-called implicit layer induced from the solution of the regularized
optimization problem. To engage a more flexible design of p-Laplacian GNN, Shao et al. (2023) further
proposed p-Laplacian based graph framelet GNN (pL-UFG) to assign the p-Laplacian based regularization
acting on multiscale GNNs (e.g., graph framelet). While remarkable learning accuracy has been observed
empirically, the underlying properties of the models proposed in (Shao et al., 2023) are still unclear. In this
paper, our primary focus is on pL-UFG (see Section 2 for the formulation). Our objective is to analyze
pL-UFG from various perspectives, including convergence of its implicit layer, the model’s asymptotic energy
behavior, changes in the model’s dynamics due to the implicit layer, and relationship with existing diffusion
models. To the best of our knowledge, these aspects have not been thoroughly explored in the context of
p-Laplacian based GNNs, leaving notable knowledge gaps. Accordingly, we summarize our contribution as
follows:
•We rigorously prove the convergence of pL-UFG, providing insights into the asymptotic behavior of
the model. This analysis addresses a crucial gap in the understanding of GNN models regularized
with p-Laplacian based energy regularizer.
•We show that by assigning the proper values of two key model parameters (denoted as µandp) of
pL-UFG based on our theoretical analysis, the (generalized) Dirichlet energy of the node feature
produced from pL-UFG will never converge to 0; thus the inclusion of the implicit layer will prevent
the model (graph framelet) from potential over-smoothing issue.
•We demonstrate how the implicit layer in pL-UFG interacts with the energy dynamics of the graph
framelet. Furthermore, we show that pL-UFG can adapt to both homophily and heterophily graphs,
enhancing its versatility and applicability.
•We establish that the propagation mechanism within pL-UFG enables a discrete generalized non-linear
graph diffusion. The conclusions based on our analysis from different perspectives are unified at the
end of the paper, suggesting a promising framework for evaluating other GNNs.
•Based on our theoretical results, we propose two generalized pL-UFG models with controlled
model dynamics, namely pL-UFG low-frequency dominant model (pL-UFG-LFD) and pL-UFG high
frequency dominant model (pL-UFG-HFD). We further show that with controllable model dynamics,
the computational cost of pL-UFG is largely reduced, making our proposed model capable of handling
large-scale graph datasets.
•We conduct extensive experiments to validate our theoretical claims. The empirical results not only
confirm pL-UFG’s capability to handle both homophily and heterophily graphs but also demonstrate
that our proposed models achieve comparable or superior classification accuracy with reduced
computational cost. These findings are consistent across commonly tested and large-scale graph
datasets.
The remaining sections of this paper are structured as follows. Section 2 presents fundamental notations
related to graphs, GNN models, graph framelets, and pL-UFG. In Section 3, we conduct a theoretical analysis
on pL-UFG, focusing on the aforementioned aspects. Specifically, Section 3.1 presents the convergence
analysis, while Section 3.2 examines the behavior of the p-Laplacian based implicit layer through a generalized
Dirichlet energy analysis. Furthermore, Section 3.3 demystifies the interaction between the implicit layer and
2Published in Transactions on Machine Learning Research (02/2024)
graph framelets from an energy dynamic perspective. We provide our proposed models (pL-UFG-LFD and
pL-UFG-HFD) in Section 3.4. Lastly, in Section 3.5, we demonstrate that the iterative algorithm derived
from the implicit layer is equivalent to a generalized non-linear diffusion process on the graph. Additionally,
in Section 4, we further verify our theoretical claims by comprehensive numerical experiments. Lastly, in
Section 5, we summarize the findings of this paper and provide suggestions for future research directions.
2 Preliminaries
In this section, we provide the necessary notations and formulations utilized in this paper. We list the
necessary notations with their meanings in Table 1.
Table 1: Necessary Notations
Notations Brief Interpretation
H(G) Heterophily index of a given graph G
X Initial node feature matrix
F(k)Feature representation on k-th layer of GNN model
fi Individual row of F
Fi,: One or more operation acts on each row of F
/hatwideA Normalized adjacency matrix
/tildewideL Normalized Laplacian matrix
W Graph weight matrix
W Framelet decomposition matrix
I Index set of all framelet decomposition matrices.
/hatwiderW Learnable weight matrix in GNN models
/tildewiderW,Ω,/hatwiderW Learnable weight matrices in defining generalized Dirichlet energy
Y Feature propagation result for the pL-UFG defined in (Shao et al., 2023).
θ N-dimensional vector for diagonal scaling ( diag(θ)) in framelet models.
EPF(F)Generalized Dirichlet energy for node feature induced from implicit layer
We also provide essential background information on the developmental history before the formulation of
certain models, serving as a concise introduction to the related works.
Graph, Graph Convolution and Graph Consistency We denote a weighted graph as G= (V,E,W)
with nodes setV={v1,v2,···,vN}of totalNnodes, edge setE⊆V×V and graph adjacency matrix W,
where W= [wi,j]∈RN×Nandwi,j= 1if(vi,vj)∈E, else,wi,j= 0. The nodes feature matrix is X∈RN×c
forGwith each row xi∈Rcas the feature vector associated with node vi. For a matrix A, we denote its
transpose as A⊤, and we use [N]for set{1,2,...,N}. Throughout this paper, we will only focus on the
undirect graph and use matrix AandWinterchangeably for graph adjacency matrix1. The normalized
graph Laplacian is defined as /tildewideL=I−D−1
2(W+I)D−1
2, where D=diag(d1,1,...,dN,N)is a diagonal degree
matrix with di,i=/summationtextN
j=1wi,jfori= 1,...,N, and Iis the identity matrix. Let {λi}N
i=1in decreasing order
be all the eigenvalues of /tildewideL, also known as graph spectra, and λi∈[0,2]. For any given graph, we let ρ/tildewideLbe
the largest eigenvalue of /tildewideL. Lastly, for any vector x= [x1,...,xc]∈Rc,∥x∥2= (/summationtextc
i=1x2
i)1
2is the L 2-norm of
x, and similarly, for any matrix M= [mi,j], denote by∥M∥:=∥M∥F= (/summationtext
i,jm2
i,j)1
2the matrix Frobenius
norm.
Graph convolution network (GCN) (Kipf & Welling, 2016) produces a layer-wise (node feature) propagation
rule based on the information from the normalized adjacency matrix as:
F(k+1)=σ/parenleftbig/hatwideAF(k)/hatwiderW(k)/parenrightbig
, (1)
1We initially set Was the graph adjacency matrix while Wis a generic edge weight matrix in align with the notations used
in (Fu et al., 2022; Shao et al., 2023)
3Published in Transactions on Machine Learning Research (02/2024)
where F(k)is the embedded node feature, /hatwiderW(k)the weight matrix for channel mixing (Bronstein et al.,
2021), and σany activation function such as sigmoid. The superscript(k)indicates the quantity associated
with layerk, and F(0)=X. We write/hatwideA=D−1
2(W+I)D−1
2, the normalized adjacency matrix of G. The
operation conducted in GCN before activation can be interpreted as a localized filter by the graph Fourier
transform, i.e., F(k+1)=U(In−Λ)U⊤F(k), where U,Λare from the eigendecomposition /tildewideL=UΛU⊤. In
fact,UFis known as the Fourier transform of graph signals in F.
Over the development of GNNs, most of GNNs are designed under the homophily assumption in which
connected (neighbouring) nodes are more likely to share the same label. The recent work by Zhu et al. (2020)
identifies that the general topology GNN fails to obtain outstanding results on the graphs with different
class labels and dissimilar features in their connected nodes, such as the so-called heterophilic graphs. The
definition of homophilic and heterophilic graphs are given by:
Definition 1 (Homophily and Heterophily) .The homophily or heterophily of a network is used to define
the relationship between labels of connected nodes. The level of homophily of a graph can be measured by the
positive scoreH(G)=Evi∈V[|{vj}j∈Ni,yi=yi|/|Ni|], where|{vj}j∈Ni,yi=yi|denotes the number of neighbours
ofvi∈Vthat share the same label as vi, i.e.yi=yj. A scoreH(G)close to 1 corresponds to stronger
homophily while a score H(G)nearly 0 indicates stronger heterophily. We say that a graph is a homophilic
(heterophilic) graph if it has stronger homophily (heterophily) or simply strong homophily (heterophily).
Graph Framelet. The main target for this paper to explore is pL-UFG defined in (Shao et al., 2023) in
which the p-Laplacian-based implicit layer is combined with so-called graph framelet or framelets in short.
Framelets are a type of wavelet frames arising from signal processing that can be extended for analyzing graph
signals. Dong (2017) developed tight framelets on graphs by approximating smooth functions with filtered
Chebyshev polynomials. Graph framelets decompose graph signals and re-aggregating them effectively, as
shown in the study on graph noise reduction by Zhou et al. (2021) Recently, Yang et al. (2022) suggested a
simple method for building more versatile and stable framelet families, known as Quasi-Framelets. In this
study, we will introduce graph framelets using the same architecture described in (Yang et al., 2022). To
begin, we define the filtering functions for Quasi-framelets.
Definition 2. A set ofR+ 1positive functions F={g0(ξ),g1(ξ),...,gR(ξ)}defined on the interval [0,π]is
considered as (a set of) Quasi-Framelet scaling functions, if these functions adhere to the following identity
condition:
g0(ξ)2+g1(ξ)2+···+gR(ξ)2≡1,∀ξ∈[0,π]. (2)
The identity condition eq. (2) ensures a perfect reconstruction of a signal from its spectral space to the spatial
space, see (Yang et al., 2022) for a proof. Particularly we are interested in the scaling function set in which g0
descents from 1 to 0, i.e., g0(0) = 1andg0(π) = 0andgRascends from 0 to 1, i.e., gR(0) = 0andgR(π) = 1.
The purpose of setting these conditions is for g0to regulate the highest frequency and for gRto control the
lowest frequency, while the remaining functions govern the frequencies lying between them.
With a given set of framelet scaling functions, the so-called Quasi-Framelet signal transformation can be
defined by the following transformation matrices:
W0,J=Ug0(Λ
2m+J)···g0(Λ
2m)U⊤, (3)
Wr,0=Ugr(Λ
2m)U⊤,forr= 1,...,R, (4)
Wr,ℓ=Ugr(Λ
2m+ℓ)g0(Λ
2m+ℓ−1)···g0(Λ
2m)U⊤, (5)
forr= 1,...,R,ℓ = 1,...,J,
whereFis a given set of Quasi-Framelet functions satisfying eq. (2) and J≥0is a given level on a graph
G= (V,E)with normalized graph Laplacian /tildewideL=U⊤ΛU.W0,Jis defined as the product of J+ 1Quasi-
Framelet scaling functions g0applied to the Laplacian spectra Λat different scales. Wr,0is defined as gr(Λ
2m)
applied to spectra Λ, wheremis the coarsest scale level which is the smallest value satisfying 2−mλN≤π.
4Published in Transactions on Machine Learning Research (02/2024)
For1≤r≤Rand1≤ℓ≤J,Wr,ℓis defined as the product of ℓQuasi-Framelet scaling functions g0and
one Quasi-Framelet scaling functions grapplied to spectra Λat relevant scaling. Let W= [W0,J;W1,0; ...;
WR,0]be the stacked matrix. It can be proven that WTW=I, see (Yang et al., 2022), which provides a
signal decomposition and reconstruction process based on W. This is referred to as the graph Quasi-Framelet
transformation.
Since the computation of the Quasi-framelet transformation matrices requires the eigendecomposition of
graph Laplacian, to reduce the computational cost, Chebyshev polynomials are used to approximate the
Quasi-Framelet transformation matrices. The approximated transformation matrices are defined by replacing
gr(ξ)in eq. (3)-eq. (5) with Chebyshev polynomials Tr(ξ)of a fixed degree, which is typically set to 3. The
Quasi-Framelet transformation matrices defined in eq. (3) - eq. (5) can be approximated by,
W0,J≈T0(1
2m+J/tildewideL)···T 0(1
2m/tildewideL), (6)
Wr,0≈Tr(1
2m/tildewideL),forr= 1,...,R, (7)
Wr,ℓ≈Tr(1
2m+ℓ/tildewideL)T0(1
2m+ℓ−1/tildewideL)···T 0(1
2m/tildewideL), (8)
forr= 1,...,R,ℓ = 1,...,J.
Based on the approximated Quasi-Framelet transformation defined above, two types of graph framelet
convolutions have been developed recently:
1.The Spectral Framelet Models (Zheng et al., 2021; 2022b; Yang et al., 2022; Shi et al., 2023a):
F(k+1)=σ/parenleftig
W⊤diag(θ)WF(k)/hatwiderW(k)/parenrightig
:=σ
/summationdisplay
(r,ℓ)∈IW⊤
r,ℓdiag(θr,ℓ)Wr,ℓF(k)/hatwiderW(k)
,(9)
whereθr,ℓ∈RN,/hatwiderW(k)are learnable matrices for channel/feature mixing, and I={(r,j) :r=
1,...,R,ℓ = 0,1,...,J}∪{ (0,J)}is the index set for all framelet decomposition matrices.
2.The Spatial Framelet Models (Chen et al., 2023):
F(k+1)=σ
W⊤
0,J/hatwideAW0,JF(k)/hatwiderW(k)
0,J+/summationdisplay
r,ℓW⊤
r,ℓ/hatwideAWr,ℓF(k)/hatwiderW(k)
r,ℓ
. (10)
The spectral framelet models conduct framelet decomposition and reconstruction on the spectral domain of
the graph. Clearly θr,ℓ∈RNcan be interpreted as the frequency filters, given that a framelet system provides
a perfect reconstruction on the input graph signal (i.e., W⊤W=I). Instead of frequency domain filtering,
the spatial framelet models implement the framelet-based propagation via the spatial (graph adjacency)
domain. There is a major difference between the two schemes. In the spectral framelet methods, the weight
matrix/hatwiderW(k)is shared across different (filtered) frequency domains, while in the spatial framelet methods, an
individual weight matrix /hatwiderW(k)
r,ℓis applied to each (filtered) spatial domain to produce the graph convolution.
It is worth noting that the theoretical exploration of the learning advantage of graph framelet models, as well
as the difference between two types of framelet models, have been studied in (Han et al., 2022; Zheng et al.,
2021; Chen et al., 2023; Shi et al., 2023b).
Generalized p-Laplacian Regularized Framelet GCN. In this part, we provide several additional
definitions to formulate the model (pL-UFG) that we are interested in analyzing. As a generalized framelet
model incorporating the so-called p-Laplacian energy regularizer, the pL-UFG, as we are going to define it
later, has shown great flexibility in terms of adapting different types of graphs (i.e., homophily and heterophily)
by efficiently adjusting the penalty strength from the regularizer, resulted in superior learning performance
across various benchmark datasets (Shao et al., 2023). We started by defining the p-Laplace operator on the
graph as follows.
5Published in Transactions on Machine Learning Research (02/2024)
Definition 3 (Graph p-Laplacian) .Given a graphG= (V,E,W)and a multiple channel signal function
F∈FV, the graph p-Laplacian is an operator ∆p:FV→FV, defined by:
∆pF:=−1
2div(∥∇F∥p−2∇F),forp≥1. (11)
where divand∇are the graph gradient and divergence operators (Fu et al., 2022). ∥·∥p−2is element-wise
power over the node gradient ∇F2.
The corresponding p-Dirichlet form can be denoted as:
Sp(F) =1
2/summationdisplay
(vi,vj)∈E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/radicalbiggwi,j
dj,jfj−/radicalbiggwi,j
di,ifi/vextenddouble/vextenddouble/vextenddouble/vextenddoublep
, (12)
where we adopt the definition of p-norm as (Fu et al., 2022). It is not difficult to verify that once we set
p= 2, we recover the graph Dirichlet energy (Zhou & Schölkopf, 2005) that is widely used to measure the
difference between node features along the GNN propagation process.
Remark 1 (Dirichlet Energy, Graph Homophily and Heterophily) .Graph Dirichlet energy (Fu et al., 2022;
Bronstein et al., 2021) has become a commonly applied measure of variation between node features via GNNs.
It has been shown that once the graph is highly heterophilic where the connected nodes are not likely to
share identical labels, one may prefer GNNs that exhibit nodes feature sharpening effect, thus increasing
Dirichlet energy, such that the final classification output of the connected nodes from these GNNs tend to be
different. Whereas, when the graph is highly homophilic, a smoothing effect (thus a decrease of Dirichlet
energy) is preferred.
Shao et al. (2023) further generalized the p-Dirichlet form in eq. (12) as:
Sp(F) =1
2/summationdisplay
(vi,vj)∈E∥∇WF([i,j])∥p
=1
2/summationdisplay
vi∈V

/summationdisplay
vj∼vi∥∇WF([i,j])∥p
1
p
p
=1
2/summationdisplay
vi∈V∥∇WF(vi)∥p
p, (13)
wherevj∼vistands for the node vjthat is connected to node viand∇WF(vi) =(∇WF([i,j]))vj:(vi,vj)∈Eis
the node gradient vector for each node viand∥·∥pis the vector p-norm. Moreover, we can further generalize
the regularizerSp(F)by considering any positive convex function ϕas:
Sϕ
p(F) =1
2/summationdisplay
vi∈Vϕ(∥∇WF(vi)∥p). (14)
There are many choices of ϕandp. Whenϕ(ξ) =ξp, we recover the p-Laplacian regularizer. Interestingly,
by settingϕ(ξ) =ξ2, we recover the so-called Tikhonov regularization which is frequently applied in image
processing. When ϕ(ξ) =ξ, i.e. identity map written as id, andp= 1,Sid
1(F)becomes the classic total
variation regularization. Last but not the least, ϕ(ξ) =r2log(1 +ξ2/r2)gives nonlinear diffusion. We note
that there are many other choices on the form of ϕ. In this paper we will only focus on those mentioned in
(Shao et al., 2023) (i.e., the smooth ones). As a result, the flexible design of the p-Laplacian based energy
regularizer in eq. (14) provides different penalty strengths in regularizing the node features propagated
from GNNs.
Accordingly, the regularization problem proposed in (Shao et al., 2023) is:
F= arg min
FSϕ
p(F) +µ∥F−W⊤diag(θ)WF∥2
F, (15)
2We provide detailed formulation on graph p-Laplacian in Appendix A.1.1.
6Published in Transactions on Machine Learning Research (02/2024)
where we let Y:=W⊤diag(θ)WFstands for the node feature generated by the spectral framelet models (9)
without activation σ. This is the implicit layer proposed in (Shao et al., 2023). As the optimization problem
defined in eq. (15) does not have a closed-form solution when p̸= 2, an iterative algorithm is developed
in (Shao et al., 2023) to address this issue. The justification is summarized by the following proposition
(Theorem 1 in (Shao et al., 2023)):
Proposition 1. For a given positive convex function ϕ(ξ)(ϕ(ξ)≥0on the domain R+), define
Mi,j=wi,j
2∥∇WF([i,j])∥p−2·/bracketleftbiggϕ′(∥∇WF(vi)∥p)
∥∇WF(vi)∥p−1
p+ϕ′(∥∇WF(vj)∥p)
∥∇WF(vj)∥p−1
p/bracketrightigg
, (16)
αii=1/
/summationdisplay
vj∼viMi,j
di,i+ 2µ
, βii= 2µαii, (17)
and denote the matrices M= [Mi,j],α=diag(α11,...,αNN)andβ=diag(β11,...,βNN). Then problem (15)
can be solved by the following message passing process
F(k+1)=α(k)D−1/2M(k)D−1/2F(k)+β(k)Y, (18)
with an initial value, e.g., F(0)=0orY. Note,kdenotes the discrete time index (iteration) and M(k)is
calculated according to eq. (16) with F(k).
Finally, in this paper, we call the iterative algorithm defined in eq. (18) for realizing the implicit layer defined
by the solution of problem eq. (15) as the pL-UFG model. Although remarkable performance has been
observed from pL-UFG, there are still some key properties of the model that require to be theoretically
explored. For example, the convergence of the iterative algorithm eq. (18) for the implicit layer in eq. (15),
and how the implicit layer changes and interacts with the energy dynamic of the original framelet, what is
the relationship between the propagation within the implicit layer and other propagations such as diffusion
on node features. We will explicitly show our theoretical results in the coming sections.
3 Theoretical Analysis of the pL-UFG
In this section, we show a detailed analysis of the convergence (Section 3.1) and energy behavior (Section 3.2)
of the iterative algorithm in solving the implicit layer
presented in eq. (18). In addition, we will also present some results regarding the interaction between the
implicit layer and graph framelet in Section 3.3 via the energy dynamic aspect based on the conclusion from
Section 3.2. Lastly, in Section 3.5, we will verify that the iterative algorithm induced from the p-Laplacian
implicit layer admits a discretized non-linear diffusion process, thereby connecting the discrete iterative
algorithm to the differential equations on the graph.
First, we consider the form of matrix Min eq. (18). Write
ζϕ
i,j(F) =1
2/bracketleftigg
ϕ′(∥∇WF(vi)∥p)
∥∇WF(vi)∥p−1
p+ϕ′(∥∇WF(vj)∥p)
∥∇WF(vj)∥p−1
p/bracketrightigg
. (19)
Mi,jcan be simplified as
Mi,j=ζϕ
i,j(F)wi,j∥∇WF([i,j])∥p−2. (20)
ζϕ
i,j(F)is bounded as shown in the following lemma.
Lemma 1. For the given pused inSϕ
p(see eq. (14)), assume that
ϕ′(ξ)
ξp−1≤C, (21)
for a suitable constant C, then we have|ζϕ
i,j(F)|≤C.
7Published in Transactions on Machine Learning Research (02/2024)
The proof is trivial thus we omit it here. In the sequel, we use ζi,j(F)forζϕ
i,j(F)instead.
Remark 2. The assumed condition in (21) in Lemma 1 is appropriate to ensure ζi,j(F)is bounded. This
condition is satisfied for all ϕfunctions used in (Shao et al., 2023).
•Whenϕ(ξ) =ξp(1≤p≤∞)and take the same pintoSϕ
p(F)), then the resulting ζi,j(F)is bounded
for all such p;
•Whenϕ(ξ) =ξ2and for any 0<p≤2inSϕ
p(F), then the resulting ζi,j(F)is bounded when Fis
bounded. This is becauseϕ′(ξ)
ξp−1=2ξ
ξp−1= 2ξ2
ξpwhich is bounded for finite ξ>0;
•Whenϕ(ξ) =ξ, thenϕ′(ξ)
ξp−1=ξ
ξp−1, indicating ζi,j(F)is bounded for all 0<p≤1;
•Whenϕ(ξ) =/radicalbig
ξ2+ϵ2−ϵ, we haveϕ′(ξ)
ξp−1=(ξ2+ϵ2)1/2ξ
ξp−1≤Cξ
ξp−1. Therefore ζi,j(F)is bounded for
all0<p≤2;
•Whenϕ(ξ) =r2log(1 +ξ2
r2), the result ofϕ′(ξ)
ξp−1yieldsr21
1+ξ2
r2·2
r2ξ
ξp−1≤2ξ
ξp−1. Henceζi,j(F)remain
bounded for all 0<p≤2.
In summary, for all forms of ϕwe included in the model, ζi,j(F)is bounded with the appropriate choice of p.
The boundedness of ζi,j(F)from Lemma 1 is useful in the following convergence analysis.
3.1 Convergence Analysis of pL-UFG
We show the iterative algorithm presented in eq. (18) will converge with a suitable choice of µ. We further
note that although the format of Theorem 1 is similar to Theorem 2 in (Fu et al., 2022), our message passing
scheme presented in eq. (18) is different compared to the one defined in (Fu et al., 2022) via the forms of M,
αandβ. In fact, the model defined in (Fu et al., 2022) can be considered as a special case where ϕ(ξ) =ξp.
As a generalization of the model proposed in (Fu et al., 2022), we provide a uniform convergence analysis for
the pL-UFG.
Theorem 1 (Shrinking Property of the Proposed Model Iteration) .Given a graphG(V,E,W) with node
features X, ifα(k),β(k),M(k)andF(k)are updated according to eq. (18), then there exist some real positive
valueµ, which depends on the input graph ( G,X) and the quantity of p, updated in each iteration, such that:
Lϕ
p(F(k+1))≤Lϕ
p(F(k)),
whereLϕ
p(F) :=Sϕ
p(F) +µ∥F−Y∥2
F.
We include detailed proof in Appendix A.2.1. Theorem 1 shows that with an appropriately chosen value
ofµ, the iteration scheme eq. (18) for the implicit layer eq. (15) is guaranteed to decrease the objective
function values (or not increasing). This inspires us to explore further the variation of the node feature
produced from the implicit layer asymptotically. Recall that to measure the difference between node features,
one common choice is to analyze its Dirichlet energy, which is initially considered in the setting p= 2in
eq. (12). It is known that the Dirichlet energy of the node feature tends to approach 0 after a sufficiently
large number of iterations in many GNN models (Kipf & Welling, 2016; Wu et al., 2020; Chamberlain et al.,
2021a; Di Giovanni et al., 2023), known as the over-smoothing problem. However, as we will show in the next
section, by taking large µor smallp, the iteration from the implicit layer will always lift up the Dirichlet
energy of the node features, and the over-smoothing issue can be resolved completely in pL-UFG.
Remark 3. Additionally, Theorem 1 guarantees that the objective Lϕ
pis bounded, which means all the
iterates generated from eq. (18) are bounded (as ∥F(k)−Y∥2
Fis bounded). Hence, for the sequence {F(k)}∞
k=1,
there must be an accumulated point F∗, such that there is a convergent sub-sequence {F(kn)}∞
n=1. Accordingly,
the iteration produces a weak convergence sequence eq. (18) in the sense of sub-convergent.
Remark 4. In practice, the implicit layer eq. (15) is replaced by a given number (e.g., 20 in our experiments)
of iteration eq. (18). Theorem 1 ensures the objective value is not increasing. Thus this shrinkage property is
more important than the convergence in practice.
8Published in Transactions on Machine Learning Research (02/2024)
3.2 Energy Behavior of the pL-UFG
In this section, we show the energy behavior of the p-Laplacian based implicit layer. Specifically, we are
interested in analyzing the property of the generalized Dirichlet energy defined in (Bronstein et al., 2021).
We start by denoting generalized graph convolution as follows:
F(k+τ)=F(k)+τσ/parenleftig
−F(k)Ω(k)+/hatwideAF(k)/hatwiderW(k)−F(0)/tildewiderW(k)/parenrightig
, (22)
where Ω(k),/hatwiderW(k)and/tildewiderW(k)∈Rc×cact on each node feature vector independently and perform channel
mixing. When τ= 1, and Ω(k)=/tildewiderW(k)=0, it returns to GCN (Kipf & Welling, 2016). Additionally, by
setting Ω(k)̸= 0, we have the anisotropic instance of GraphSAGE (Xu et al., 2019). To quantify the quality
of the node features generated by eq. (22), specifically, Bronstein et al. (2021) considered a new class of
energy as defined below,
E(F) =1
2N/summationdisplay
i=1⟨fi,Ωfi⟩−1
2N/summationdisplay
i,j=1/hatwideAi,j⟨fi,/hatwiderWfj⟩+φ(0)(F,F(0)), (23)
in whichφ(0)(F,F(0))serves a function of that induces the source term from ForF(0). It is worth noting that
by setting Ω=/hatwiderW=Icandφ(0)= 0, we recover the classic Dirichlet energy when setting p= 2in eq. (12)
that is, E(F) =1
2/summationtext
(vi,vj)∈E/vextenddouble/vextenddouble/vextenddouble/radicaligwi,j
dj,jfj−/radicaligwi,j
di,ifi/vextenddouble/vextenddouble/vextenddouble2
. Additionally, when we set φ(0)(F,F(0))=/summationtext
i⟨fi,/tildewiderWf(0)
i⟩,
eq. (23) can be rewritten as:
E(F) =/angbracketleftbigg
vec(F),1
2(Ω⊗IN−/hatwiderW⊗/hatwideA)vec(F) + (/tildewiderW⊗IN)vec(F(0))/angbracketrightbigg
. (24)
Recall that eq. (18) produces the node feature F(k+1)according to the edge diffusion αD−1/2MD−1/2
onF(k)and the scaled source term 2µαF(0)where F(0)can be set to Y. To be specific, in (24), we set
Ω=/hatwiderW=/tildewiderW=Icand replace the edge diffusion/hatwideAwith αD−1/2MD−1/2and set the identity matrix INin
the residual term to be the diagonal matrix 2µα. Finally, we propose
Definition 4 (The Generalized Dirichlet Energy) .Based on the previous notation setting, the generalized
Dirichlet energy for the node features F(k+1)in eq. (18) is:
EPF(F(k+1)) =/angbracketleftig
vec(F(k+1)),
1
2/parenleftig
Ic⊗IN−Ic⊗/parenleftig
α(k+1)D−1/2M(k+1)D−1/2/parenrightig/parenrightig
vec(F(k+1)) + (Ic⊗2µα(k+1))vec(F(0))/angbracketrightbigg
,(25)
where the superscript “PF” is short for p-Laplacian based framelet models.
It is worth noting that the generalized Dirichlet energy defined in eq. (25) is dynamic along the iterative
layers due to the non-linear nature of the implicit layer defined in eq. (15). We are now able to analyze the
energy ( EPF(F)) behavior of the pL-UFG, concluded as the following proposition.
Proposition 2 (Energy Behavior) .AssumeGis connected, unweighted, and undirected. There exists
sufficiently large value of µor small value of psuch that EPF(F)will stay away above 0 at each iterative
layerkand increases with the increase of µor the decrease of p.
We leave the detailed proof in Appendix A.2.2. Proposition 2 shows that, for any of our framelet convolution
models, the p-Laplacian based implicit layer will not generate identical node features across graph nodes, and
thus the so-called over-smoothing issue will not appear. Furthermore, it is worth noting that the result from
Proposition 2 provides the theoretical justification of the empirical observations in (Shao et al., 2023), where
a large value of µor small value of pis suitable for fitting heterophily datasets which commonly require the
output of GNN to have higher Dirichlet energy.
9Published in Transactions on Machine Learning Research (02/2024)
Remark 5 (Regarding to the quantity of p).The conclusion of Proposition 2 is under sufficient large of
µor small of p. However, it is well-known that the quantity of pcannot be set as arbitrary values, and in
fact, it is necessary to have p≥1so that the iteration for the solution of the optimization problem defined
in eq. (15) can converge. Therefore, it is not hard to see that the effect of pis weaker than µin terms of
analyzing the asymptotic energy behavior of the model (i.e., via eq. (46) in Appendix A.2.2). Accordingly, in
practice, one shall prefer to let µbe the major target and pbe the sub-target to adjust for fitting pL-UFG
to different types of graphs. Without loss of generality, in the sequel, when we analyze the property of the
model with conditions on µandp, we mainly target the effect from µ, and one can check µandpare in the
opposite effect on the model.
3.3 Discussion on Dynamic Interactions
The results on the energy behavior of the p-Laplacian implicit layer, as discussed in the last section, encourage
further exploration into the interactions between the implicit layer and graph framelets. The dynamics of
energy in graph framelets have become a popular topic in recent works (Shi et al., 2023a; Han et al., 2022;
2023). The studies by Han et al. (2022) and Shi et al. (2023a) demonstrate that, under mild conditions, the
propagation dynamics of graph framelets can be dominated by one of its frequency domains, as exemplified by
the Graph Neural Network (GNN) Low-frequency-dominance (LFD) and High-frequency-dominance (HFD).
Taking spectral framelet convolution eq. (9) with Haar-type filter (i.e. R= 1in the case of scaling function
set) as an example, one can manually set θ0,1=1Nandθ1,1=θ1N, and ifθ∈[0,1), the spectral framelet
convolution is LFD, meaning that although the feature propagation via high-frequency domain remains to
sharpen node features, the mainstream of framelet is dominated by the dynamic via low-frequency domain
in which node features are homogenized by the low-pass filter. Therefore, asymptotically all node features
will still become identical, resulting in over-smoothing issues. However, this situation can be reversed by
settingθ>1, and the resulting HFD dynamic in the framelet can ensure that all features are sharpened in
the whole propagation process. Therefore, the over-smoothing issue will not appear. It is worth noting that
there are many other settings that can induce LFD/HFD dynamic of framelet, our example is here as an
illustration purpose.
Now recall that based on Definition 1, if Gis homophilic, connected nodes are more likely to share the
same label, in this case, one may prefer framelet model to induce LFD dynamic to enhance smoothing. On
the other hand, if Gis heterophilic, the model is expected to induce an HFD dynamic so that even in the
adjacent nodes, their predicted labels still tend to be different. Based on the aforementioned settings on
θ, one can observe that graph framelet can induce both LFD and HFD dynamics so that automatically
capable of fitting both types of graphs. We strictly formulate the LFD and HFD dynamic of framelet in
Appendix A.1.2. Together with the result in Proposition 2, it can be verified that if the framelet is LFD, the
additional p-Laplacian implicit layer will ensure the Dirichlet energy of the node feature remains nonzero,
thus preventing the model from the over-smoothing issue. On the other hand, if the framelet is HFD, node
features are initially sharpened from the framelet and further serve as a source term in the implicit layer
(i.e., 2µαF). This setting provides the potential to induce a further sharpening of the node feature via the
implicit layer, resulting in an “enhanced” HFD dynamic on the framelet to further fit heterophilic graphs.
We empirically verify this observation in Section 4.2.
3.4 Proposed Model with Controlled Dynamics
Based on the aforementioned conclusions regarding energy behavior and the interaction between the implicit
layer and framelet’s energy dynamics, it becomes evident that irrespective of the homophily index of any
given graph input, one can readily apply the condition of θ(s) in Proposition 3 to facilitate the adaptation
of the pL-UFG model to the input graph by simply adjusting the quantities of µandp. This adjustment
significantly reduces the training cost of the graph framelet. For instance, consider the case of employing a
Haartype frame with ℓ= 1, where we have only one low-pass and one high-pass domain. In this scenario,
the trainable matrices for this model are θ0,1,θ1,1, and/hatwiderW. Based on our conclusions, we can manually
set both θ0,1andθ1,1to our requested quantities, thereby inducing either LFD or HFD. Consequently, the
only remaining training cost is associated with /hatwiderW, leading to a large reduction in the overall training cost
10Published in Transactions on Machine Learning Research (02/2024)
Figure 1: Illustration of the working flow of pL-UFG-LFD and pL-UFG-HFD under the Haartype frame
withℓ= 1. The input graph features are first decomposed onto two frequency domains and further filtered by
the diagonal matrix θ0,1andθ1,1. With controlled model dynamics from Proposition 3 i.e., θ0,1=1Nand
θ1,1=θθ0,1, framelet can induce both LFD and HFD dynamics resulting as different level of Dirichlet energy
of the produced node features. It is straightforward to check that when the framelet is LFD, the level of node
Dirichlet energy is less than its HFD counterpart. The generated node features from the graph framelet is
then inputted into p-Laplacian (with graph gradient as one component) based implicit layer.
while preserving the model’s capability of handling both types of graphs. Accordingly, we proposed two
additional pL-UFG variants with controlled model dynamics, namely pL-UFG-LFD andpL-UFG-HFD .
More explicitly, the propagation of graph framelet with controlled dynamic takes the form:
F(k+1)=σ/parenleftig
W⊤
0,1diag(1N)W0,1F(k)/hatwiderW+W⊤
1,1diag(θ1N)W1,1F(k)/hatwiderW/parenrightig
.
After that, the output node features will be propagated through certain iterative layers as defined in eq. (18)
for the implicit layer eq. (15), and the resulting node feature will be forwarded to the next graph framelet
convolution and implicit layer propagation. To properly represent the Dirichlet energy of node features, we
borrow the concept of electronic orbital energy levels in Figure 1. The shaded outermost electrons correspond
to higher energy levels, which can be analogously interpreted as higher variations in node features. Conversely,
the closer the electrons are to the nucleus, the lower their energy levels, indicating lower variations in node
features.
3.5 Equivalence to Discrete Non-Linear Diffusion
Diffusion on graph has gained its popularity recently (Chamberlain et al., 2021b; Thorpe et al., 2022; Han
et al., 2023) by providing a framework (i.e., PDE) to understand the GNNs architecture and as a principled
way to develop a broad class of new methods. To the best of our knowledge, although the GNNs induced
from discretized linear diffusion on graph (Chamberlain et al., 2021b;a; Thorpe et al., 2022; Shi et al., 2024)
have been intensively explored, models built from non-linear diffusion have not attracted much attention in
general3. In this section, we aim to verify that the iteration eq. (18) admits a (discretized) nonlinear diffusion
3FormoredetailsincludingtherelationshipbetweenGNNdynamicandcontinuousdiffusionprocess, pleasecheck(Chamberlain
et al., 2021a; Han et al., 2023)
11Published in Transactions on Machine Learning Research (02/2024)
with a source term. To see this, recall that the p-Laplacian operator defined in eq. (11) has the form:
∆pF:=−1
2div(∥∇F∥p−2∇F),forp≥1. (26)
Plugging in the definition of graph gradient and divergence (Specifically defined in eq. (30) and eq. (32)) into
the above equation, one can compactly write out the form of p-Laplacian as:
(∆pF)(i) =/summationdisplay
vj∼vi/radicalbiggwi,j
di,i∥∇WF([i,j])∥p−2/parenleftbigg/radicalbiggwi,j
di,ifi−/radicalbiggwi,j
dj,jfj/parenrightbigg
. (27)
If we treat the iteration equation eq. (18) as the Euler discretization of one continuous non-linear diffusion
process on the graph, we have
F(k+1)−F(k)
τ=α(k)D−1/2M(k)D−1/2F(k)−F(k)+β(k)Y,
=/parenleftig
α(k)D−1/2M(k)D−1/2−I/parenrightig
F(k)+β(k)Y. (28)
We setτ= 1for the rest of the analysis for convenience reasons. With all these setups, we summarize our
results in the following:
Lemma 2 (Non-Linear Diffusion) .AssumingGis connected, the forward Euler scheme presented in eq. (28)
admits a generalized non-linear diffusion on the graph. Specifically, we have:
/parenleftig
α(k)D−1/2M(k)D−1/2−I/parenrightig
F(k)+β(k)Y=α/parenleftig
div(∥∇F(k)∥p−2∇F(k))/parenrightig
+ 2µα(k)DF(k)+ 2µα(k)F(0).
(29)
We leave the proof of the Lemma in Appendix A.2.3. Based on the conclusion of Lemma 2, it is clear
that the propagation via the p-Laplacian implicit layer admits a scaled non-linear diffusion with two source
terms. We note that the form of our non-linear diffusion coincides with the one developed in (Chen et al.,
2022). However, in Chen et al. (2022), the linear operator is assigned via the calculation of graph Laplacian
whereas in our model, the transformation acts over the whole p-Laplacian. Finally, it is worth noting that
the conclusion in Lemma 2 can be transferred to the implicit schemes4. We omit it here.
Remark 6. With sufficiently large µor smallp, one can check that the strength of the diffusion, i.e.
div(∥∇F(k)∥p−2∇F(k)), is diluted. Once two source terms 2µα(k)DF(k)+ 2µα(k)F(0)dominant the whole
process, the generated node features approach to DF(k)+F(0), which suggests a framelet together with two
source terms. The first term can be treated as the degree normalization of the node features from the last
layer and the second term simply maintains the initial feature embedding. Furthermore, this observation
suggests our conclusion on the energy behavior of pL-UFG (Proposition 2); the interaction within pL-UFG
described in Section 3.3, and lastly, the conclusion from Lemma 2 can be unified and eventually forms a
well-defined framework in assessing and understanding the property of pL-UFG.
4 Experiment
Experiment outlines In this section, we present comprehensive experimental results on the claims that
we made from the theoretical aspects of our model. All experiments were conducted in PyTorch on NVIDIA
Tesla V100 GPU with 5,120 CUDA cores and 16GB HBM2 mounted on an HPC cluster. In addition, for the
sake of convenience, we listed the summary of each experimental section as follows:
•In Section 4.1, we show how a sufficiently large/small µcan affect the model’s performance on
heterophilic/homophilic graphs, and the results are almost invariant to the choice of p.
4With a duplication of terminology, here the term “implicit” refers to the implicit scheme (i.e., backward propagation) in the
training of the diffusion model.
12Published in Transactions on Machine Learning Research (02/2024)
•In Section 4.2, we show some tests regarding the results of the model’s dynamics. Specifically, we
verified the observations in Section 3.3 with controlled model dynamics (quantity of θ) of framelet
to illustrate how the p-Laplacian based implicit layer interacts with the framelet model.
•In Section 4.3 we test the performances of pL-UFG-LFD and pL-UFG-HFD via real-world graph
benchmarks versus various baseline models. Furthermore, as these two controllable pL-UFG models
largely reduced the computational cost (as we claimed in Section 3.4), we show pL-UFG-LFD
and pL-UFG-HFD can even handle the large-scale graph datasets and achieve remarkable learning
accuracies.
Hyper-parameter tuning We applied exactly the same hyper-parameter tunning strategy as (Shao et al.,
2023) to make a fair comparison. In terms of the settings for graph framelets, the framelet type is fixed as
Haar((Yang et al., 2022)), and the level Jis set to 1. The dilation scale s∈{1,1.5,2,3,6}, and forn, the
degree of Chebyshev polynomial approximation is drawn from {2, 3, 7}. It is worth noting that in graph
framelets, the Chebyshev polynomial is utilized for approximating the spectral filtering of the Laplacian
eigenvalues. Thus, a d-degree polynomial approximates d-hop neighbouring information of each node of the
graph. Therefore, when the input graph is heterophilic, one may prefer to have a relatively larger das node
labels tend to be different between directly connected (1-hop) nodes.
4.1 Synthetic Experiment on Variation of µ
Setup In this section, we show how a sufficiently large/small of µcan affect the model’s performance
on hetero/homophilic graphs. In order to make a fair comparison, all the parameters of pL-UFG followed
the settings included in (Shao et al., 2023). For this test, we selected two datasets: Cora(heterophilic
index: 0.825, 2708 nodes and 5278 edges) and Wisconsin (heterophilic index: 0.15, 499 nodes and 1703
edges) from https://www.pyg.org/ . We assigned the quantity of p={1,1.5,2,2.5}combined with a set of
µ={0.1,0.5,1,5,10,20,30,50,70}. The number of epochs was set to 200, and the test accuracy (in %) was
obtained as the average test accuracy of 10 runs.
Results and Discussion The experimental results are presented in Figure 2. When examining the results
obtained through the homophily graph (Figure 2a), it is apparent that all variants of pL-UFGs achieved the
best performance when µ= 0.1, which is the minimum value of µ. As the value of µincreased, the learning
accuracy decreased. This suggests that a larger sharpening effect was induced by the model, as stated in
Section 3.3 and Proposition 2, causing pL-UFGs to incorporate higher amounts of Dirichlet energy into their
generated node features. Consequently, pL-UFGs are better suited for adapting to heterophily graphs. This
observation is further supported by the results in Figure 2b, where all pL-UFG variants achieved their optimal
performance with a sufficiently large µwhen the input graph is heterophilic.
Additional interesting observation on the above result is despite the fact that all model variants demonstrated
superior learning outcomes on both homophilic and heterophilic graphs when assigned sufficiently large or
small values of µ, it can be observed that when the quantity of pis small, pL-UFG requires a smaller value of
µto fit the heterophily graph (blue line in Figure 2b). On the other hand, when the models have a relatively
large value of p(i.e.,p= 2.5), it is obvious that these models yielded the most robust results when there is
an increase of µ(red line in Figure 2a). These phenomena further support the notion that pandµexhibit
opposite effects on the model’s energy behavior as well as its adaptation to homophily and heterophily graphs.
4.2 Synthetic Experiment on Testing of Model’s Dynamics
Now, we take one step ahead. Based on the observation in Section 3.3, the inclusion of p-Laplacian based
implicit layer has the potential for further enhancing framelet’s LFD and HFD dynamics. This suggests that
one can control the entries of θbased on the conditions provided in Proposition 3 and by only changing the
quantity of µandpto test the model’s adaption power on both homophily and heterophily graphs. Therefore,
in this section, we show how a (dynamic) controlled framelet model can be further enhanced by the assistant
from the p-Laplacian regularizer. Similarly, we applied the same setting to the experiments in (Shao et al.,
2023).
13Published in Transactions on Machine Learning Research (02/2024)
(a) Accuracy on Coravia different combinations of µ
andp.
(b) Accuracy on Wisconsin via different combinations
ofµandp.
Figure 2: Performance of pL-UFG with various combinations of the values of µandp.
Setup and Results To verify the claims on in Section 3.3, we deployed the same settings mentioned
in Proposition 3. Specifically, we utilized Haar frame with ℓ= 1and set θ0,1=IN,θ0,1=θIN. For
heterophilic graphs ( Wisconsin ),θ= 2, and for the homophilic graph ( Cora),θ= 0.2. The result of the
experiment is presented in Figure 3. Similar to the results observed from Section 4.1, it is shown that when
the relatively large quantity of µis assigned, the model’s capability of adapting homophily/heterophily graph
decreased/increased. This directly verifies that the p-Laplacian based implicit layer interacts and further
enhances the (controlled) dynamic of the framelet by the value of pandµ, in terms of adaptation.
(a) Accuracy on Cora
 (b) Accuracy on Wisconsin
Figure 3: Average Accuracy(%) with Changing µandpunder (manually fixed) LFD/HFD framelet models.
All framelet model in Figure 3a are LFD dynamic with θ0,1=IN,θ1,1=θ1N,θ= 0.2. On Figure 3b, all
framelet models are HFD with θ0,1=IN,θ1,1=θ1N,θ= 2.
4.3 Real-world Node Classification and Scalability
Previous synthetic numerical results show predictable performance of both pL-UFG-LFD and pL-UFG-HFD.
In this section, we present the learning accuracy of our proposed models via real-world homophily and
heterophily graphs. Similarly, we deployed the same experimental setting from (Shao et al., 2023). In addition,
to verify the claim in Remark 3.4, we tested our proposed model via a large-scale graph dataset ( ogbn-arxiv )
to show the proposed model’s scalability, which is rarely explored. We include the summary statistics of the
datasets in Table 2. All datasets are split according to (Hamilton et al., 2017).
14Published in Transactions on Machine Learning Research (02/2024)
Table 2: Statistics of the datasets, H(G)represent the level of homophily of overall benchmark datasets.
Datasets Class Feature Node Edge H(G)
Cora 7 1433 2708 5278 0.825
CiteSeer 6 3703 3327 4552 0.717
PubMed 3 500 19717 44324 0.792
Computers 10 767 13381 245778 0.802
Photo 8 745 7487 119043 0.849
CS 15 6805 18333 81894 0.832
Physics 5 8415 34493 247962 0.915
Arxiv 23 128 169343 1166243 0.681
Chameleon 5 2325 2277 31371 0.247
Squirrel 5 2089 5201 198353 0.216
Actor 5 932 7600 26659 0.221
Wisconsin 5 251 499 1703 0.150
Texas 5 1703 183 279 0.097
Cornell 5 1703 183 277 0.386
For the settings of µ,pandθwithin pL-UFG-LFD and pL-UFG-HFD, we assigned µ={0.1,0.5,1,2.0},
p={1,1.5,2,2.5}andθ={0.2,0.5,0.8}for pL-UFG-LFD in order to fit the homophily graphs, and for
pL-UFG-HFD, we assigned µ={10,20,30},p={1,1.5,2,2.0,2.5}andθ={5,7.5,10}for heterophily graphs.
The learning accuracy is presented in Table 3 and 4. Furthermore, rather than only reporting the average
accuracy and related standard deviation, to further verify the significance of the improvement, we also
computed the 95% confidence interval under t-distribution for the highest learning accuracy of the baselines
and mark∗to our model’s learning accuracy if it is outside the confidence interval.
We include a brief introduction to the baseline models used in this experiment:
•MLP: Standard feed-forward multiple layer perceptron.
•GCN(Kipf & Welling, 2016): GCN is the first of its kind to implement linear approximation to
spectral graph convolutions.
•SGC(Wu et al., 2019): SGC reduces GCNs’ complexity by removing nonlinearities and collapsing
weight matrices between consecutive layers. Thus serves as a more powerful and efficient GNN
baseline.
•GAT(Veličković et al., 2018): GAT generates one attention coefficient matrix that is element-wisely
multiplied on the graph adjacency matrix according to the node feature-based attention mechanism
via each layer to propagate node features via the relative importance between them.
•JKNet(Xu et al., 2018): JKNet offers the capability to adaptively exploit diverse neighbourhood
ranges, facilitating enhanced structure-aware representation for individual nodes.
•APPNP (Gasteiger et al., 2019): APPNP leverages personalized PageRank to disentangle the neural
network from the propagation scheme, thereby merging GNN functionality.
•GPRGNN (Chien et al., 2021): The GPRGNN architecture dynamically learns General Pagerank
(GPR) weights to optimize the extraction of node features and topological information from a graph,
irrespective of the level of homophily present.
•p-GNN (Fu et al., 2022):p-GNN is a p-Laplacian based graph neural network model that incorporates
a message-passing mechanism derived from a discrete regularization framework. To make a fair
comparison, we test the p-GNN model with different quantities of p.
•UFG(Zheng et al., 2022b): UFG, a class of GNNs built upon framelet transforms utilizes framelet
decomposition to effectively merge graph features into low-pass and high-pass spectra.
15Published in Transactions on Machine Learning Research (02/2024)
•pL-UFG (Shao et al., 2023): pL-UFG employs a p-Laplacian-based implicit layer to enhance the
adaptability of multi-scale graph convolution networks (i.e., UFG) to filter-based domains, effectively
improving the model’s adaptation to both homophily and heterophily graphs. Furthermore, as two
types of pL-UFG models are proposed in (Shao et al., 2023), we test both two pL-UFG variants as
our baseline models. For more details including the precise formulation of the model, please check
(Shao et al., 2023).
Table 3: Test accuracy (%) on homophilic graphs, the top learning accuracy is highlighted in boldand the
second accuracy is underlined. The term OOM means out of memory.
Method Cora CiteSeer PubMed Computers Photos CS Physics Arxiv
MLP 66.04±1.11 68.99±0.48 82.03±0.24 71.89±5.36 86.11±1.35 93.50±0.24 94.56±0.11 55.50±0.78
GCN 84.72±0.38 75.04±1.46 83.19±0.13 78.82±1.87 90.00±1.49 93.00±0.12 95.55±0.09 70.07±0.79
SGC 83.79±0.37 73.52±0.89 75.92±0.26 77.56±0.88 86.44±0.35 92.18±0.22 94.99±0.13 71.01±0.30
GAT 84.37±1.13 74.80±1.00 83.92±0.28 78.68±2.09 89.63±1.75 92.57±0.14 95.13±0.15 OOM
JKNet 83.69±0.71 74.49±0.74 82.59±0.54 69.32±3.94 86.12±1.12 91.11±0.22 94.45±0.33 OOM
APPNP 83.69 ±0.71 75.84±0.64 80.42±0.29 73.73±2.49 87.03±0.95 91.52±0.14 94.71±0.11 OOM
GPRGNN 83.79 ±0.93 75.94±0.65 82.32±0.25 74.26±2.94 88.69±1.32 91.89±0.08 94.85±0.23 OOM
UFG 80.64±0.74 73.30±0.19 81.52±0.80 66.39±6.09 86.60±4.69 95.27±0.0495.77±0.04 71.08±0.49
PGNN1.084.21±0.91 75.38±0.82 84.34±0.33 81.22±2.62 87.64±5.05 94.88±0.12 96.15±0.12 OOM
PGNN1.584.42±0.71 75.44±0.98 84.48±0.21 82.68±1.15 91.83±0.77 94.13±0.08 96.14±0.08 OOM
PGNN2.084.74±0.67 75.62±1.07 84.25±0.35 83.40±0.68 91.71±0.93 94.28±0.10 96.03±0.07 OOM
PGNN2.584.48±0.77 75.22±0.73 83.94±0.47 82.91±1.34 91.41±0.66 93.40±0.07 95.75±0.05 OOM
pL-UFG11.084.54±0.62 75.88±0.60 85.56±0.18 82.07±2.78 85.57±19.92 95.03±0.22 96.19±0.06 70.28±9.13
pL-UFG11.584.96±0.38 76.04±0.85 85.59±0.18 85.04±1.06 92.92±0.37 95.03±0.22 96.27±0.0671.25±8.37
pL-UFG12.085.20±0.42 76.12±0.82 85.59±0.1785.26±1.15 92.65±0.65 94.77±0.27 96.04±0.07 OOM
pL-UFG12.585.30±0.60 76.11±0.82 85.54±0.18 85.18±0.88 91.49±1.29 94.86±0.14 95.96±0.11 OOM
pL-UFG21.084.42±0.32 74.79±0.62 85.45±0.18 84.88±0.84 85.30±19.50 95.03±0.19 96.06±0.11 71.01±7.28
pL-UFG21.585.60±0.3675.61±0.60 85.59±0.18 84.55±1.57 93.00±0.6195.03±0.19 96.14±0.09 71.21±6.19
pL-UFG22.085.20±0.42 76.12±0.8285.59±0.1785.27±1.1592.50±0.40 94.77±0.27 96.05±0.07 OOM
pL-UFG-LFD 85.64±1.3677.39∗±1.5985.08±1.3385.36∗±1.3993.17∗±1.3096.13∗±1.0896.49∗±1.0471.96±1.25
Discussion on the Results, Scalability and Computational Complexity From both Table 3 and 4,
it is clear that our proposed model (pL-UFG-LFD and pL-UFG-HFD) produces state-of-the-art learning
accuracy compared to various baseline models. For the datasets (i.e., Pubmedand Squirrel ) on which
pL-UFG-LFD and pL-UFG-HFD are not the best, one can observe that pL-UFG-LFD and pL-UFG-HFD still
have nearly identical learning outcomes compared to the best pL-UFG results. This suggests even within the
pL-UFG with controlled framelet dynamics, by adjusting the values of µandp, our proposed models are still
able to generate state-of-the-art learning results with the computational complexity largely reduced compared
to the pL-UFG and UFG. This observation directly verifies the observation in Section 3.3. In addition, due
to the reduction of computational cost, our dynamic controlled models (pL-UFG-LFD and pL-UFG-HFD)
16Published in Transactions on Machine Learning Research (02/2024)
Table 4: Test accuracy (%) on heterophilic graphs. the top learning accuracy is highlighted in boldand the
second accuracy is underlined.
Method Chameleon Squirrel Actor Wisconsin Texas Cornell
MLP 48.82±1.43 34.30±1.13 41.66±0.8393.45±2.09 71.25±12.99 83.33±4.55
GCN 33.71±2.27 26.19±1.34 33.46±1.42 67.90±8.16 53.44±11.23 55.68±10.57
SGC 33.83±1.69 26.89±0.98 32.08±2.22 59.56±11.19 64.38±7.53 43.18±16.41
GAT 41.95±2.65 25.66±1.72 33.64±3.45 60.65±11.08 50.63±28.36 34.09±29.15
JKNet 33.50 ±3.46 26.95±1.29 31.14±3.63 60.42±8.70 63.75±5.38 45.45±9.99
APPNP 34.61 ±3.15 32.61±0.93 39.11±1.11 82.41±2.17 80.00±5.38 60.98±13.44
GPRGNN 34.23 ±4.09 34.01±0.82 34.63±0.58 86.11±1.31 84.38±11.20 66.29±11.20
UFG 50.11±1.67 31.48±2.05 40.13±1.11 93.52±2.36 84.69±4.87 83.71±3.28
PGNN1.049.04±1.16 34.79±1.01 40.91±1.41 94.35±2.16 82.00±11.31 82.73±6.92
PGNN1.549.12±1.14 34.86±1.25 40.87±1.47 94.72±1.91 81.50±10.70 81.97±10.16
PGNN2.049.34±1.15 34.97±1.41 40.83±1.81 94.44±1.75 84.38±11.52 81.06±10.18
PGNN2.549.16±1.40 34.94±1.57 40.78±1.51 94.35±2.16 83.38±12.95 81.82±8.86
pL-UFG11.056.81±1.69 38.81±1.97 41.26±1.66 96.48±0.9486.13±7.47 86.06±3.16
pL-UFG11.556.89±1.17 39.73±1.2240.95±0.93 96.48±1.07 87.00±5.16 86.52±2.29
pL-UFG12.056.24±1.02 39.72±1.86 40.95±0.93 96.59±0.72 86.50±8.84 85.30±2.35
pL-UFG12.556.11±1.25 39.38±1.78 41.04±0.99 95.34±1.64 89.00±4.9983.94±3.53
pL-UFG21.055.51±1.53 36.94±5.69 29.28±19.25 93.98±2.94 85.00±5.27 87.73±2.49
pL-UFG21.557.22±1.1939.80±1.4240.89±0.75 96.48±0.94 87.63±5.32 86.82±1.67
pL-UFG22.056.19±0.99 39.74±1.66 41.01±0.80 96.14±1.16 86.50±8.84 85.30±2.35
pL-UFG22.555.69±1.15 39.30±1.68 40.86±0.74 95.80±1.44 86.38±2.98 84.55±3.31
pL-fUFG1.055.80±1.93 38.43±1.26 32.84±16.54 93.98±3.47 86.25±6.89 87.27±2.27
pL-fUFG1.555.65±1.96 38.40±1.52 41.00±0.99 96.48±1.29 87.25±3.61 86.21±2.19
pL-fUFG2.055.95±1.29 38.33±1.71 41.25±0.84 96.25±1.25 88.75±4.97 83.94±3.78
pL-fUFG2.555.56±1.66 38.39±1.48 40.55±0.50 95.28±2.24 88.50±7.37 83.64±3.88
pL-UFG-HFD 58.60∗±1.7439.63±2.0144.63∗±2.75 96.64±1.77 89.31±8.40 88.97∗±3.36
show a strong capability of handling the large-scale graph dataset, which is a challenging issue (scalability)
for some GNNs especially multi-scale graph convolutions such as framelets (Zheng et al., 2022b) without
additional data pre-processing steps. Accordingly, one can check that pL-UFG-LFD outperforms all included
baselines on Arxivdatasets. Lastly, one can also find that most of the improvements between the learning
accuracy produced from our model and the baselines are significant.
4.4 Limitation of the Proposed Models and Future Studies
First, we note that our analysis of the convergence, energy dynamic, and equivalence between our proposed
model can be applied or partially applied to most existing GNNs. Based on what we have claimed regarding
the theoretical perspective of pL-UFG, although we assessed model property via different perspectives,
eventually, all theoretical conclusions come to the same conclusion (i.e., the asymptotic behavior of pL-UFG).
17Published in Transactions on Machine Learning Research (02/2024)
Therefore, it would be beneficial to deploy our analyzing framework to other famous GNNs. Since the main
purpose of this paper is to re-assess the property of pL-UFG, we leave this to future work.
In addition, to induce LFD/HFD to pL-UFG, we set the value of θas constant according to Proposition 3.
However, due to the large variety of real-world graphs, it is challenging to determine the most suitable θwhen
we fix it as a constant. This suggests the exploration of controlling the model’s dynamic via selecting θis still
rough. Moreover, based on Definition 1, the homophily index of a graph is a summary statistic over all nodes.
However, even in the highly homophilic graph, there are still some nodes with their neighbours with different
labels. This suggests the index is only capable of presenting the global rather than local labelling information
of the graph. Accordingly, assigning a constant θto induce LFD/HFD might not be able to equip pL-UFG
with enough power to capture detailed labelling information of the graph. Therefore, another future research
direction is to potentially explore the design of θvia the local labelling information of the graph. Finally,
we note that another consequence of setting θ0,1andθ1,1as constant is such setting narrows the model’s
parameter space, as one can check the only learnable matrix left via explicit part of pL-UFG (eq. (9)) is /hatwiderW.
Accordingly, the narrowed parameter space might make the solution of the model optimization apart from
the desired solution as before, causing potential increases in learning variance.
5 Concluding Remarks
In this work, we performed theoretical analysis on pL-UFG. Specifically, we verified that by choosing suitable
quantities of the model parameters ( µandp), the implicit propagation induced from p-Laplacian is capable of
amplifying or shrinking the Dirichlet energy of the node features produced from the framelet. Consequently,
such manipulation of the energy results in a stronger energy dynamic of graph framelets and, therefore,
enhances the model’s adaption power on both homophilic and heterophilic graphs. We further explicitly
showed the proof of the convergence of pL-UFG, which, to the best of our knowledge, fills the knowledge gap,
at least in the field of p-Laplacian-based multi-scale GNNs. Moreover, we showed the equivalence between
pL-UFG and the non-linear graph diffusion, indicating that pL-UFG can be trained via various training
schemes. Finally, it should be noted that for the simplicity of the analysis, we have made several assumptions
and only focus on the Haartype frames. It suffices in regard to the scope of this work. However, it would be
interesting to consider more complex energy dynamics by reasonably dropping some of the assumptions or
from other types of frames, we leave this to future work.
References
Wendong Bi, Lun Du, Qiang Fu, Yanlin Wang, Shi Han, and Dongmei Zhang. Make heterophily graphs
better fit GNN: A graph rewiring approach. arXiv:2209.08264 , 2022.
Cristian Bodnar, Francesco Di Giovanni, Benjamin Paul Chamberlain, Pietro Liò, and Michael M Bronstein.
Neural Sheaf diffusion: A topological perspective on heterophily and oversmoothing in GNNs. Advances in
Neural Information Processing Systems , 35, 2022.
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. Geometric deep learning: grids, groups,
graphs, geodesics, and gauges. arXiv:2104.13478 , 2021.
Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele
Rossi. GRAND: Graph neural diffusion. In Proceedings of the International Conference on Machine
Learning , pp. 1407–1418. PMLR, 2021a.
Benjamin Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Xiaowen Dong, and
Michael Bronstein. Beltrami flow and neural diffusion on graphs. Advances in Neural Information Processing
Systems, 34:1594–1609, 2021b.
Jialin Chen, Yuelin Wang, Cristian Bodnar, Rex Ying, Pietro Lio, and Yu Guang Wang. Dirichlet energy
enhancement of graph neural networks by framelet augmentation. arXiv:2311.05767 , 2023.
18Published in Transactions on Machine Learning Research (02/2024)
Qi Chen, Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Optimization-induced graph implicit
nonlinear diffusion. In Proceedings of the International Conference on Machine Learning , pp. 3648–3661.
PMLR, 2022.
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph
neural network. In Proceedings of the International Conference on Learning Representations , 2021.
Fan RK Chung. Spectral graph theory , volume 92. American Mathematical Soc., 1997.
Francesco Di Giovanni, James Rowbottom, Benjamin Paul Chamberlain, Thomas Markovich, and Michael M
Bronstein. Understanding convolution on graphs via energies. Transactions on Machine Learning Research ,
2023.
Bin Dong. Sparse representation on graphs by tight wavelet frames and applications. Applied and Computa-
tional Harmonic Analysis , 42(3):452–479, 2017. doi: 10.1016/j.acha.2015.09.005.
Pavel Drábek and Stanislav I Pohozaev. Positive solutions for the p-Laplacian: application of the fibrering
method. Proceedings of the Royal Society of Edinburgh Section A: Mathematics , 127(4):703–726, 1997.
Guoji Fu, Peilin Zhao, and Yatao Bian. p-Laplacian based graph neural networks. In Proceedings of the
International Conference on Machine Learning , pp. 6878–6917. PMLR, 2022.
JP García Azorero and I Peral Alonso. Existence and nonuniqueness for the p-Laplacian. Communications
in Partial Differential Equations , 12(12):126–202, 1987.
Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph
neural networks meet personalized pagerank. In Proceedings of the International Conference on Learning
Representations , 2019.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances
in Neural Information Processing Systems , 30, 2017.
Andi Han, Dai Shi, Zhiqi Shao, and Junbin Gao. Generalized energy and gradient flow via graph framelets.
arXiv:2210.04124 , 2022.
Andi Han, Dai Shi, Lequan Lin, and Junbin Gao. From continuous dynamics to graph neural networks:
Neural diffusion and beyond. arXiv:2310.10121 , 2023.
Bernd Kawohl and Jiri Horak. On the geometry of the p-Laplacian operator. Discrete and Continuous
Dynamical Systems - S , 10(4):799–813, 2017.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. Proceed-
ings of the International Conference on Learning Representations , 2016.
Remigijus Paulavičius and Julius Žilinskas. Analysis of different norms and corresponding lipschitz constants
for global optimization. Technological and Economic Development of Economy , 12(4):301–306, 2006.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-GCN: Geometric graph
convolutional networks. In Proceedings of the International Conference on Learning Representations , 2019.
Zhiqi Shao, Dai Shi, Andi Han, Andrey Vasnev, Yi Guo, and Junbin Gao. Enhancing framelet GCNs with
generalized p-Laplacian regularization. International Journal of Machine Learning and Cybernetics , pp.
1–21, 2023. URL https://link.springer.com/article/10.1007/s13042-023-01982-8 .
Dai Shi, Yi Guo, Zhiqi Shao, and Junbin Gao. How curvature enhance the adaptation power of framelet
GCNs.arXiv:2307.09768 , 2023a.
Dai Shi, Zhiqi Shao, Yi Guo, and Junbin Gao. Frameless graph knowledge distillation. arXiv:2307.06631 ,
2023b.
19Published in Transactions on Machine Learning Research (02/2024)
Dai Shi, Andi Han, Lequan Lin, Yi Guo, Zhiyong Wang, and Junbin Gao. Design your own universe: A
physics-informed agnostic method for enhancing graph neural networks. arXiv:2401.14580 , 2024.
Matthew Thorpe, Tan Minh Nguyen, Hedi Xia, Thomas Strohmer, Andrea Bertozzi, Stanley Osher, and
Bao Wang. GRAND++: Graph neural diffusion with a source term. In Proceedings of the International
Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=EMxu-dzvJk .
César Torres. Boundary value problem with fractional p-Laplacian operator. Advances in Nonlinear Analysis ,
5(2):133–146, 2016.
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph attention networks. In Proceedings of the International Conference on Learning Representations ,
2018.
Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process in linear graph
convolutional networks. Advances in Neural Information Processing Systems , 34:5758–5769, 2021.
Quanmin Wei, Jinyan Wang, Jun Hu, Xianxian Li, and Tong Yi. OGT: Optimize graph then training
GNNs for node classification. Neural Computing and Applications , 34(24):22209–22222, 2022. URL
https://doi.org/10.1007/s00521-022-07677-5 .
Felix Wu, Tianyi Zhang, Amauri Holanda de Souza, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger.
Simplifying graph convolutional networks. In Proceedings of the International Conference on Machine
Learning , pp. 6861–6871. PMLR, 2019.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive
survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems , 32(1):
4–24, 2020.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka.
Representation learning on graphs with jumping knowledge networks. In Proceedings of the International
Conference on Machine Learning , 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In
Proceedings of the International Conference on Learning Representations , 2019. URL https://openreview.
net/forum?id=ryGs6iA5Km .
Mengxi Yang, Xuebin Zheng, Jie Yin, and Junbin Gao. Quasi-framelets: Another improvement to graph
neural networks. arXiv:2201.04728 , 2022.
Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks for graphs
with heterophily: A survey. arXiv:2202.07082 , 2022a.
Xuebin Zheng, Bingxin Zhou, Junbin Gao, Yuguang Wang, Pietro Lió, Ming Li, and Guido Montufar. How
framelets enhance graph neural networks. In Proceedings of the International Conference on Machine
Learning , pp. 12761–12771. PMLR, 2021.
Xuebin Zheng, Bingxin Zhou, Yu Guang Wang, and Xiaosheng Zhuang. Decimated framelet system on
graphs and fast g-framelet transforms. Journal of Machine Learning Research , 23:18–1, 2022b.
Bingxin Zhou, Ruikun Li, Xuebin Zheng, Yu Guang Wang, and Junbin Gao. Graph denoising with framelet
regularizer. arXiv:2111.03264 , 2021.
Dengyong Zhou and Bernhard Schölkopf. Regularization on discrete spaces. In Joint Pattern Recognition
Symposium , pp. 361–368. Springer, 2005.
Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily
in graph neural networks: Current limitations and effective designs. Advances in Neural Information
Processing Systems , 33:7793–7804, 2020.
Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural networks
with an optimization framework. In Proceedings of the Web Conference 2021 , pp. 1215–1226, 2021.
20Published in Transactions on Machine Learning Research (02/2024)
A Appendix
A.1 Detailed Formulations
In this section, we provide detailed formulation as the preliminaries of the graph p-Laplacian operator shown
in Definition 3
A.1.1 About p-Laplace Operator
Definition 5 (The p-Laplace Operator (Drábek & Pohozaev, 1997)) .LetΩ⊂Rdbe a domain and uis a
function defined on Ω. The p-Laplace operator ∆over functions is defined as
∆u:=∇·(∥∇u∥p−2∇u)
where∇is the gradient operator and ∥·∥is the Euclidean norm and pis a scalar satisfying 1<p< +∞.
The p-Laplace operator is known as a quasi-linear elliptic partial differential operator.
There is a line of research on the properties of p-Laplacian in regard to its uniqueness and existence
(García Azorero & Peral Alonso, 1987), the geometrical property (Kawohl & Horak, 2017), and boundary
conditions on so-called p-Laplacian equation (Torres, 2016).
The concept of p-Laplace operator can be extended for discrete domains such as graph (nodes) based on
the concepts of the so-called graph gradient and divergence, see below. One of the recent works (Fu et al.,
2022) considers assigning an adjustable p-Laplacian regularizer to the (discrete) graph regularization problem
that is conventionally treated as a way of producing GNN outcomes (i.e., Laplacian smoothing) (Zhou &
Schölkopf, 2005). In view of the fact that the classic graph Laplacian regularizer measures the graph signal
energy along edges under L2metric, it would be beneficial if the GNN training process could be regularized
underLpmetric in order to adapt to different graph inputs. Following these pioneer works, Shao et al.
(2023) further integrated graph framelet and a generalized p-Laplacian regularizer to develop the so-called
generalized p-Laplacian regularized framelet model. It involves a regularization problem over the energy
quadratic form induced from the graph p-Laplacian. To show this, we start by defining graph gradient as
follows:
To introduce graph gradient and divergence, we define the following notation:
Given a graphG= (V,E,W), letFV:={F|F:V→Rd}be the space of the vector-valued functions defined
onVandFE:={g|g:E→Rd}be the vector-valued function space on edges, respectively.
Definition 6 (Graph Gradient (Zhou & Schölkopf, 2005)) .For a given function F∈FV, its graph gradient
is an operator∇W:FV→FEdefined as for all (vi,vj)∈E,
(∇WF)([i,j]) :=/radicalbiggwi,j
dj,jfj−/radicalbiggwi,j
di,ifi, (30)
where fiandfjare the signal vectors on nodes viandvj, i.e., the rows of F.
For simplicity, we denote ∇WFas∇Fas the graph gradient. The definition of the (discrete) graph gradient
is analogized from the notion of gradient from the continuous space. Similarly, we can further define the
so-called graph divergence:
Definition 7 (Graph Divergence (Zhou & Schölkopf, 2005)) .The graph divergence is an operator div:FE→
FVwhich is defined in the following way. For a given function g∈FE, the resulting div(g)∈FVsatisfies the
following condition, for any functions F∈FV,
⟨∇F,g⟩=⟨F,−div(g)⟩. (31)
It is easy to check that the graph divergence can be computed by:
div(g)(i) =N/summationdisplay
j=1/radicalbiggwi,j
di,i(g[i,j]−g[j,i]). (32)
21Published in Transactions on Machine Learning Research (02/2024)
With the formulation of graph gradient and divergence, we can define the graph p-Laplacian operator and
the corresponding p-Dirichlet form (Zhou & Schölkopf, 2005; Fu et al., 2022) as illustrated in Definition 3.
A.1.2 About LFD and HFD Dynamic
We provide some detailed formulation on the so-called Low frequency dominance (LFD) and high frequency
dominance (HFD) of graph framelet. In the work (Di Giovanni et al., 2023), the propagation of GNNs was
considered as the gradient flow of the Dirichlet energy that can be formulated as:
E(F) =1
2N/summationdisplay
i=1N/summationdisplay
j=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/radicalbiggwi,j
dj,jfj−/radicalbiggwi,j
di,ifi/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
, (33)
and similarly by setting the power from 2 to p, we recover the p-Dirichlet form presented in eq. (12). The
gradient flow of the Dirichlet energy yields the so-called graph heat equation (Chung, 1997) as ˙F(k)=
−∇E(F(k)) =−/tildewideLF(k). Its Euler discretization leads to the propagation of linear GCN models (Wu et al.,
2019; Wang et al., 2021). Following this observation, the work (Han et al., 2022; Di Giovanni et al., 2023;
Shi et al., 2023a) also shows even with the help of the non-linear activation function and the weight matrix
via classic GCN (eq. (1)), the process described is still dominated by the low frequency (small Laplacian
eigenvalues) of the graph, hence eventually converging to the kernel of /tildewideL, for almost every initialization. To
quantify such behavior, Di Giovanni et al. (2023) and Han et al. (2022) considered a general dynamic as
˙F(k)=GNNθ(F(k),k), with GNNθ(·)as an arbitrary graph neural network function, and also characterizes
its behavior by low/high-frequency-dominance (L/HFD).
Definition 8 ((Di Giovanni et al., 2023)) .˙F(k)=GNNθ(F(k),k)is Low-Frequency-Dominant (LFD) if
E/parenleftbig
F(k)/∥F(k)∥/parenrightbig
− →0ask− →∞, and is High-Frequency-Dominant (HFD) if E/parenleftbig
F(k)/∥F(k)∥/parenrightbig
− →ρ/tildewideL/2as
t− →∞.
Lemma 3 ((Di Giovanni et al., 2023)) .A GNN model is LFD (resp. HFD) if and only if for each tj− →∞,
there exists a sub-sequence indexed by kjl− →∞andF∞such that F(kjl)/∥F(kjl)∥− →F∞and/tildewideLF∞= 0
(resp./tildewideLF∞=ρ/tildewideLF∞).
To employ the similar notions of LFD/HFD, Han et al. (2022) developed a framelet Dirichlet energy and
analyzed the energy behavior of both spectral (eq. (9)) and spatial framelet (eq. (10)) convolutions. Specifically,
let
EFr
r,ℓ(F) =1
2Tr/parenleftbig
(Wr,ℓF)⊤Wr,ℓFΩr,ℓ/parenrightbig
−1
2Tr/parenleftbig
(Wr,ℓF)⊤diag(θ)r,ℓWr,ℓF/hatwiderW/parenrightbig
for all (r,ℓ)∈I. The generated framelet energy is given by:
EFr(F) =EFr
0,J(F) +/summationdisplay
r,ℓEFr
r,ℓ(F)
=1
2/summationdisplay
(r,ℓ)∈I/angbracketleftig
vec(F),/parenleftig
Ωr,ℓ⊗W⊤
r,ℓWr,ℓ−/hatwiderW⊗W⊤
r,ℓdiag(θ)r,ℓWr,ℓ/parenrightig
vec(F)/angbracketrightig
, (34)
where the superscript “Fr” stands for the framelet convolution. This definition is based on the fact that the
total Dirichlet energy is conserved under framelet decomposition (Han et al., 2022; Di Giovanni et al., 2023).
By analyzing the gradient flow of the framelet energy5defined above, Han et al. (2022) concluded the energy
dynamic of framelet as:
Proposition 3 ((Han et al., 2022)) .The spectral graph framelet convolution eq. (9) with Haar-type filter
(i.e.R= 1in the case of scaling function set) can induce both LFD and HFD dynamics. Specifically, let
θ0,ℓ=1Nandθr,ℓ=θ1Nforr= 1,...,L,ℓ = 1,...,Jwhere 1Nis a sizeNvector of all 1s. Whenθ∈[0,1),
the spectral framelet convolution is LFD, and when θ>1, the spectral framelet convolution is HFD.
Remark 7. Proposition 3 suggests that graph framelet can naturally induce both two types of dynamic
without further modification, and this supports our claim on the property of graph framelet in Section 3.3.
5Similar to the requirement on our p-Laplacian based framelet energy ( EPF(F(k+1)), to thoroughly verify the framelet
energy in eq. (34) is a type of energy, we shall further require: ∇2EFr
r,ℓ(F) =Ωr,ℓ⊗W⊤
r,ℓWr,ℓ−/hatwiderW⊗W⊤
r,ℓ/tildewideLWr,ℓis symmetric,
which can be satisfied by requiring both Ωand/hatwiderWare symmetric.
22Published in Transactions on Machine Learning Research (02/2024)
A.2 Formal Proofs
A.2.1 Detailed Proof of Proposition 1
Theorem 2 (Repeat of Theorem 1) .Given a graphG(V,E,W) with node features X, ifα(k),β(k),M(k)
andF(k)are updated according to eq. (18), then there exist some real positive value µ, which depends on the
input graph (G,X) and the quantity of p, updated in each iteration, such that:
Lϕ
p(F(k+1))≤Lϕ
p(F(k)),
whereLϕ
p(F) :=Sϕ
p(F) +µ∥F−Y∥2
F.
Proof.First, write:
M(k)
i,j=wi,j
2/vextenddouble/vextenddouble/vextenddouble∇WF(k)([i,j])/vextenddouble/vextenddouble/vextenddoublep−2
·/bracketleftbiggϕ′(∥∇WF(k)(vi)∥p)
∥∇WF(k)(vi)∥p−1
p+ϕ′(∥∇WF(k)(vj)∥p)
∥∇WF(k)(vj)∥p−1
p/bracketrightigg
. (35)
The derivative of the regularization problem is defined in eq. (15) is:
∂Lϕ
p(F)
∂Fi,:/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
F(k)=2µ(F(k)
i,:−Yi,:) +/summationdisplay
vj∼viM(k)
ij1/radicalbig
diiwij∇WF(k)([j,i])
=2µ(F(k)
i,:−Yi,:) +/summationdisplay
vj∼viM(k)
ij/parenleftigg
1
diiF(k)
i,:−1/radicalbig
diidjjF(k)
j,:/parenrightigg
=(2µ+/summationdisplay
vj∼viM(k)
ij/dii)F(k)
i,:−2µYi,:−/summationdisplay
vj∼viM(k)
ij/radicalbig
diidjjF(k)
j,:
=1
α(k)
iiF(k)
i,:−1
α(k)
ii
β(k)
iiYi,:+α(k)
ii/summationdisplay
vj∼viM(k)
ij/radicalbig
diidjjF(k)
j,:
(36)
Thus, according the update rule of F(k+1)in eq. (18), we have
∂Lϕ
p(F)
∂Fi,:/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
F(k)=F(k)
i,:−F(k+1)
i,:
α(k)
ii. (37)
For our purpose, we denote the partial derivative at F(∗)of the objective function with respect to the node
feature Fi,;as
∂Lϕ
p(F(∗)
i,:) :=∂Lϕ
p(F)
∂Fi,:/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
F(∗)(38)
23Published in Transactions on Machine Learning Research (02/2024)
For alli,j∈[N], letv∈R1×cbe a disturbance acting on node i. Define the following:
N(k)
i,j=Wi,j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/radicaligg
Wi,j
Di,iF(k)
i,:−/radicaligg
Wi,j
Dj,jF(k)
j,:/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublep−2
N′(k)
i,j=Wi,j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/radicaligg
Wi,j
Di,i(F(k)
i,:+v)−/radicaligg
Wi,j
Dj,jF(k)
j,:/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublep−2
M(k)
i,j=N(k)
ijζi,j(F(k)), M′(k)
i,j=N′(k)
ijζi,j(F(k)+v)
α′(k)
ii= 1/
/summationdisplay
vj∼viM′(k)
i,j
Di,i+ 2µ
, β′(k)
ii= 2µα′(k)
ii
F′(k+1)
i,: =α′(k)
i,i/summationdisplay
vj∼viM′(k)
i,j/radicalbig
Di,iDj,jF(k)
j,:+β′(k)Yi,:,(39)
whereζij(F)is defined as eq. (19) and F(k)+vmeans that vonly applies to the i-th of F(k)6.
Similar to eq. (37), we compute
∂Lϕ
p(F(k)
i,:+v) =1
α′(k)
i,i/parenleftig
(F(k)
i,:+v)−F′(k+1)
i,:/parenrightig
. (40)
Hence from both eq. (37) and eq. (40) we will have
/vextenddouble/vextenddouble/vextenddouble∂Lϕ
p(F(k)
i,:+v)−∂Lϕ
p(F(k)
i,:)/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
α′(k)
i,i/parenleftig
(F(k)
i,:+v)−F′(k+1)
i,:/parenrightig
−1
α(k)
i,i/parenleftig
F(k)
i,:−F(k+1)
i,:/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤1
α′(k)
i,i∥v∥+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
α′(k)
i,i/parenleftig
F(k)
i,:−F′(k+1)
i,:/parenrightig
−1
α(k)
i,i/parenleftig
F(k)
i,:−F(k+1)
i,:/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=1
α′(k)
i,i∥v∥+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftigg
1
α′(k)
i,i−1
α(k)
i,i/parenrightigg
F(k)
i,:−1
α′(k)
i,iF′(k+1)
i,:+1
α(k)
i,iF(k+1)
i,:/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=1
α′(k)
i,i∥v∥+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
vj∼vi/parenleftigg
M′(k)
i,j
Di,i−M(k)
i,j
Di,i/parenrightigg
F(k)
i,:−/summationdisplay
vj∼vi/parenleftigg
M′(k)
i,j/radicalbig
Di,iDj,j/parenrightigg
F′(k)
j,:+/parenleftigg
M(k)
i,j/radicalbig
Di,iDj,j/parenrightigg
F(k)
j,:/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=
/summationdisplay
vj∼viM(k)
i,j
Di,i+ 2µ
∥v∥+/summationdisplay
vj∼vi/parenleftigg
M′(k)
i,j−M(k)
i,j
Di,i/parenrightigg
∥v∥
+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
vj∼vi/parenleftigg
M′(k)
i,j−M(k)
i,j
Di,i/parenrightigg
F(k)
i,:−/summationdisplay
vj∼vi/parenleftigg
M′(k)
i,j−M(k)
i,j/radicalbig
Di,iDj,j/parenrightigg
F(k)
j,:/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble.
Note that in eq. (39), ∥·∥p−2is the matrix L2norm raised to power p−2, that is∥X∥p−2=/parenleftig
(/summationtext
i,jx2
i,j)1
2/parenrightigp−2
.
It is known that the matrix L2norm as a function is Lipschitz (Paulavičius & Žilinskas, 2006), so is its
exponential to p−2. Furthermore, it is easy to verify that ∥N′−N∥≤c∥v∥due to the property of Nand
N′. Hence, according to Lemma 1, the following holds
|M′(k)
i,j−M(k)
i,j|≤C|N′(k)
i,j−N(k)
i,j|≤C′∥v∥.
6With slightly abuse of notation, we denote N′as the matrix after assigning the disturbance vto the matrix N.
24Published in Transactions on Machine Learning Research (02/2024)
Combining all the above, we have
/vextenddouble/vextenddouble/vextenddouble∂Lϕ
p(F(k)
i,:+v)−∂Lϕ
p(F(k)
i,:)/vextenddouble/vextenddouble/vextenddouble≤
/summationdisplay
vj∼viM(k)
i,j
Di,i+ 2µ+o(G,v,X,p)
∥v∥, (41)
whereo(G,v,X,p)is bounded. It is worth noting that the quantity of o(G,v,X,p)is bounded by
/summationdisplay
vj∼vi/parenleftigg
M′(k)
i,j−M(k)
i,j
Di,i/parenrightigg
∥v∥+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
vj∼vi/parenleftigg
M′(k)
i,j−M(k)
i,j
Di,i/parenrightigg
F(k)
i,:−/summationdisplay
vj∼vi/parenleftigg
M′(k)
i,j−M(k)
i,j/radicalbig
Di,iDj,j/parenrightigg
F(k)
j,:/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble.
Leto=o(G,v,X,p),γ={γ1,...γN}⊤, and η∈RN×c. By the Taylor expansion theorem, we have:
Lϕ
p(F(k)
i,:+γiηi,:) =Lϕ
p(F(k)
i,:) +γi/integraldisplay1
0⟨∂Lϕ
p(F(k)
i,:+ϵγiηi,:),ηi,:⟩dϵ∀i
=Lϕ
p(F(k)
i,:) +⟨∂Lϕ
p(F(k)
i,:),ηi,:⟩+γi/integraldisplay1
0/angbracketleftig
∂Lϕ
p/parenleftig
F(k)
i,:+ϵγiηi,:−∂Lϕ
p/parenleftig
F(k)
i,:/parenrightig/parenrightig
,ηi,:/angbracketrightig
dϵ
≤Lϕ
p(F(k)
i,:) +⟨∂Lϕ
p(F(k)
i,:),ηi,:⟩γi+γi/integraldisplay1
0/vextenddouble/vextenddouble/vextenddouble∂Lϕ
p/parenleftig
F(k)
i,:+ϵγiηi,:−∂Lϕ
p/parenleftig
F(k)
i,:/parenrightig/parenrightig/vextenddouble/vextenddouble/vextenddouble∥ηi,:∥dϵ
≤Lϕ
p(F(k)
i,:) +⟨∂Lϕ
p(F(k)
i,:),ηi,:⟩γi+/parenleftigg
1
α(k)
i,i+o/parenrightigg
γ2
i∥ηi,:∥2
where the last inequality comes from eq. (41).
Takingγi=α(k)
iiandηi,:=−∂Lϕ
p(F(k)
i,:)in the above inequality gives
Lϕ
p/parenleftig
F(k)
i,:−α(k)
ii∂Lϕ
p(F(k)
i,:)/parenrightig
≤Lϕ
p(F(k)
i,:)−α(k)
ii/angbracketleftig
∂Lϕ
p(F(k)
i,:),∂Lϕ
p(F(k)
i,:)/angbracketrightig
+1
2/parenleftigg
1
α(k)
i,i+o/parenrightigg
α2(k)
i,i∥∂Lϕ
p(F(k)
i,:)∥2
=Lϕ
p(F(k)
i,:)−1
2α(k)
i,i/parenleftig
1−α(k)
i,io/parenrightig
∥∂Lϕ
p(F(k)
i,:)∥2. (42)
Given that ois bounded, if we choose a large µ, e.g., 2µ>o, we will have
1−α(k)
i,io= 1−o
/summationtext
vj∼viM(k)
i,j
Di,i+ 2µ>0.
Thus the second term in eq. (42) is positive. Hence we have
Lϕ
p(F(k+1)
i,:) :=Lϕ
p/parenleftig
F(k)
i,:−α(k)
ii∂Lϕ
p(F(k)
i,:)/parenrightig
≤Lϕ
p(F(k)
i,:).
This completes the proof.
A.2.2 Detailed Proof of Proposition 2
Proposition 4 (Repeat of Proposition 2) .AssumeGis connected, unweighted, and undirected. There exists
a sufficiently large value of µor small value of psuch that EPF(F)will stay away above 0 at each iterative
layerkand increases with the increase of µor the decrease of p.
Proof.We start with the definition of the generalized Dirichlet energy above, we can re-write EPF(F(k+1))in
the following inner product between vec(F(k+1))andvec(F(0)), based on M,α,βand the iterative scheme
25Published in Transactions on Machine Learning Research (02/2024)
defined in eq. (18):
EPF(F(k+1)) =/angbracketleftig
vec(F(k+1)),
1
2/parenleftig
Ic⊗IN−Ic⊗/parenleftig
α(k+1)D−1/2M(k+1)D−1/2/parenrightig/parenrightig
vec(F(k+1)) + (Ic⊗2µα(k+1))vec(F(0))/angbracketrightbigg
=1
2/angbracketleftig
vec(F(k+1)),vec(F(k+1))/angbracketrightig
−1
2/angbracketleftig
vec(F(k+1)),Ic⊗/parenleftig
α(k+1)D−1/2M(k+1)D−1/2/parenrightig
vec(F(k+1))
−(Ic⊗2µα(k+1))vec(F(0))/angbracketrightig
. (43)
Based on the form of eq. (43), it is straightforward to see that to let EPF(F(k+1))>0and further increase
with the desired quantities of µandp, it is sufficient to require7:
Ic⊗/parenleftig
α(k+1)D−1/2M(k+1)D−1/2/parenrightig
vec(F(k+1))−(Ic⊗2µα(k+1))vec(F(0))<0. (44)
To explicitly show how the quantities of µandpaffect the term in eq. (44), we start with the case when
k= 0. Whenk= 0, eq. (44) becomes:
Ic⊗/parenleftig
α(1)D−1/2M(1)D−1/2/parenrightig
vec(F(1))−(Ic⊗2µα(1))vec(F(0))
=Ic⊗/parenleftig
α(1)D−1/2M(1)D−1/2/parenrightig
vec/parenleftig
α(0)D−1/2M(0)D−1/2F(0)+ 2µα(0)F(0)/parenrightig
−(Ic⊗2µα(1))vec(F(0)),
=Ic⊗/parenleftig
α(1)D−1/2M(1)D−1/2/parenrightig/parenleftig
Ic⊗/parenleftig
α(0)D−1/2M(0)D−1/2+ 2µα(0)/parenrightig
vec(F(0))/parenrightig
−(Ic⊗2µα(1))vec(F(0)),
=Ic⊗/parenleftigg1/productdisplay
s=0α(s)D−1/2M(s)D−1/2+/parenleftig
α(1)D−1/2M(1)D−1/22µα(0)/parenrightig
−2µα(1)/parenrightigg
vec(F(0)). (45)
Wenotethat, ineq.(45),the i-thelementof/parenleftig/producttext1
s=0α(s)D−1/2M(s)D−1/2+/parenleftbig
α(1)D−1/2M(1)D−1/22µα(0)/parenrightbig
−2µα(1)/parenrightig
can be computed as:
1/productdisplay
s=0α(s)
i,id−1/2
i,iM(s)
i,jd−1/2
j,j +/parenleftig
α(1)
i,id−1/2
i,iM(1)
i,jd−1/2
j,j2µα(0)
i,i/parenrightig
−2µα(1)
i,i
=1/productdisplay
s=0

1/
/summationdisplay
vj∼viM(s)
i,j
di,i+ 2µ

/parenleftigg/vextenddouble/vextenddouble∇WF(s)([i,j])/vextenddouble/vextenddoublep−2
/radicalbig
di,idj,j/parenrightigg
+


1/
/summationdisplay
vj∼viM(1)
i,j
di,i+ 2µ

/parenleftigg/vextenddouble/vextenddouble∇WF(1)([i,j])/vextenddouble/vextenddoublep−2
/radicalbig
di,idj,j/parenrightigg
2µ/
/summationdisplay
vj∼viM(0)
i,j
di,i+ 2µ



−
2µ/
/summationdisplay
vj∼viM(1)
i,j
di,i+ 2µ

,
=
/vextenddouble/vextenddouble∇WF(0)([i,j])/vextenddouble/vextenddoublep−2
/parenleftbigg/summationtext
vj∼viM(0)
i,j
di,i+ 2µ/parenrightbigg
·/radicalbig
di,idj,j

/vextenddouble/vextenddouble∇WF(1)([i,j])/vextenddouble/vextenddoublep−2
/parenleftbigg/summationtext
vj∼viM(1)
i,j
di,i+ 2µ/parenrightbigg
·/radicalbig
di,idj,j
+

/vextenddouble/vextenddouble∇WF(1)([i,j])/vextenddouble/vextenddoublep−2
/parenleftbigg/summationtext
vj∼viM(1)
i,j
di,i+ 2µ/parenrightbigg
·/radicalbig
di,idj,j·2µ/
/summationdisplay
vj∼viM(0)
i,j
di,i+ 2µ

−
2µ/
/summationdisplay
vj∼viM(1)
i,j
di,i+ 2µ

.(46)
7Strictly speaking, one shall further require all elements in F(k+1)larger than or equal to 0. As this can be achieved by
assigning a non-linear activation function (i.e., ReLU) to the framelet, we omit it here in our main analysis.
26Published in Transactions on Machine Learning Research (02/2024)
Now we see that by assigning a sufficient large of µor small value of p, we can see terms
like∥∇WF(1)([i,j])∥p−2
/parenleftbigg/summationtext
vj∼viM(1)
i,j
di,i+2µ/parenrightbigg
·√
di,idj,jin eq. (46) are getting smaller. Additionally, we have both
2µ//parenleftbigg/summationtext
vj∼viM(0)
i,j
di,i+ 2µ/parenrightbigg
and2µ//parenleftbigg/summationtext
vj∼viM(1)
i,j
di,i+ 2µ/parenrightbigg
≈1. Therefore, the summation result of eq. (46)
tends to be negative. Based on eq. (43), EPF(F(k+1))will stay above 0.
For the case that k≥1, by taking into the iterative algorithm eq. (18), eq. (45) becomes:
Ic⊗/parenleftigg/parenleftiggk+1/productdisplay
s=0α(s)D−1/2M(s)D−1/2+k+1/summationdisplay
s=0/parenleftiggk+1/productdisplay
l=k−sα(l)D−1/2M(l)D−1/2/parenrightigg/parenleftig
2µα(l−1)/parenrightig
−2µα(k+1)/parenrightigg/parenrightigg
vec(F(0)).
Applying the same reasoning as before, one can verify that both/producttextk+1
s=0α(s)D−1/2M(s)D−1/2and/summationtextk+1
s=0/parenleftig/producttextk+1
l=k−sα(l)D−1/2M(l)D−1/2/parenrightig/parenleftbig
2µα(l−1)/parenrightbig
approach to zero due to their forms defined in eq. (17),
and2µα(k+1)goes to 1. Therefore, the above equation tends to be negative, yielding a positive EPF(F(k+1)),
and that completes the proof.
A.2.3 Detailed Proof of Lemma 2
Lemma 4 (Repeat of Lemma 2) .AssumingGis connected, the forward Euler scheme presented in eq. (28)
admits a generalized non-linear diffusion on the graph. Specifically, we have:
/parenleftig
α(k)D−1/2M(k)D−1/2−I/parenrightig
F(k)+β(k)Y=α/parenleftig
div(∥∇F(k)∥p−2∇F(k))/parenrightig
+ 2µα(k)DF(k)+ 2µα(k)F(0).
(47)
Proof.The proof can be done by verification. We can explicitly write out the computation on the i-th row of
the L.H.S of the above equation becomes: First let us denote the rows of F(k)asf(k)(i)’s.
/summationdisplay
vj∼vi/parenleftig
α(k)
i,id−1/2
iiM(k)
i,jd−1/2
jj/parenrightig
f(k)(j)−f(k)(i) +β(k)
i,iY(i)
=α(k)
i,i
/summationdisplay
vj∼vi/parenleftigg
Mij√dii/radicalbig
djjf(k)(j)/parenrightigg
−1
α(k)
i,if(k)(i)
+ 2µα(k)
i,if(0)(i)
=α(k)
i,i
/summationdisplay
vj∼vi/parenleftigg
Mij√dii/radicalbig
djjf(k)(j)/parenrightigg
−/summationdisplay
vj∼vi/parenleftbiggMij
dii+ 2µ/parenrightbigg
f(k)(i)
+ 2µα(k)
i,if(0)(i)
=α(k)
i,i
/summationdisplay
vj∼vi/radicalbiggwi,j
di,i∥∇WF([i,j])∥p−2/parenleftbigg/radicalbiggwi,j
dj,jf(k)
j−/radicalbiggwi,j
di,if(k)
i/parenrightbigg
+ 2µ/summationdisplay
vj∼vif(k)
i

+ 2µα(k)
i,if(0)(i)
=α(k)
i,i((∆pF)(i)) + 2µα(k)
i,idiif(k)
i+ 2µα(k)
i,if(0)
i (48)
Whenitakes from 1 to N, it gives the result of the R.H.S of the Lemma according to eq. (26) and eq. (27).
Thus we complete the proof.
27