Published in Transactions on Machine Learning Research (10/2022)
Explicit Group Sparse Projection with Applications to
Deep Learning and NMF
Riyasat Ohib riyasat.ohib@gatech.edu
TReNDS Center, Georgia Institute of Technology
Nicolas Gillis nicolas.gillis@umons.ac.be
University of Mons
Niccolò Dalmasso niccolo.dalmasso@jpmchase.com
J.P. Morgan AI Research
Sameena Shah sameena.shah@jpmchase.com
J.P. Morgan AI Research
Vamsi K. Potluru vamsi.k.potluru@jpmchase.com
J.P. Morgan AI Research
Sergey Plis s.m.plis@gmail.com
TReNDS Center
Reviewed on OpenReview: https: // openreview .net/ forum? id= jIrOeWjdpc
Abstract
We design a new sparse projection method for a set of vectors that guarantees a desired
average sparsity level measured leveraging the popular Hoyer measure (an affine function of
the ratio of the ℓ1andℓ2norms). Existing approaches either project each vector individually
or require the use of a regularization parameter which implicitly maps to the average ℓ0-
measure of sparsity. Instead, in our approach we set the Hoyer sparsity level for the whole set
explicitly and simultaneously project a group of vectors with the Hoyer sparsity level of each
vector tuned automatically. We show that the computational complexity of our projection
operator is linear in the size of the problem. Additionally, we propose a generalization of this
projection by replacing the ℓ1norm by its weighted version. We showcase the efficacy of our
approach in both supervised and unsupervised learning tasks on image datasets including
CIFAR10 and ImageNet. In deep neural network pruning, the sparse models produced by
our method on ResNet50 have significantly higher accuracies at corresponding sparsity values
compared to existing competitors. In nonnegative matrix factorization, our approach yields
competitive reconstruction errors against state-of-the-art algorithms.
1 Introduction
Sparsity is a crucial property in signal processing and learning representations, exemplified by breakthroughs
in compressed sensing (Donoho, 2006; Candès et al., 2006), low-rank matrix approximations (d’Aspremont
et al., 2007) and sparse dictionary learning (Aharon et al., 2006; Hoyer, 2004). A natural advantage of
sparseness is that it diminishes the effects of random noise since it suppresses arbitrary combinations of
measured signals (Donoho, 1995; Hyvärinen, 1999; Elad, 2006). The l0-norm is a natural measure of sparsity
but directly optimizing for it is typically NP-hard. In practice, ℓ1-norm is typically used as a proxy for
obtaining sparse solutions. This is similar to the LASSO problem (Tibshirani, 1996) in the regression setting
where one constrains the number of non-zeros in the solution which can be accomplished explicitly as a
1Published in Transactions on Machine Learning Research (10/2022)
constraint or implicitly through ℓ1-norm regularization. However, this necessitates the search over the
regularization parameter which corresponds to the solution with the user-defined sparsity.
In this paper, we design a new sparse projection method for a set of feature vectors {ci∈Rni}r
i=1to achieve
a desired average sparsity level that is measured using the sparsity measure introduced by Hoyer (2004). For
x̸= 0andn>1, the Hoyer sparsity of xis defined as follows
sp(x) =√n−∥x∥1
∥x∥2√n−1∈[0,1]. (1)
We have that sp(x) = 0⇐⇒ ∥x∥1=√n∥x∥2⇐⇒ |x(j)|=bfor alljand for some constant b, while
sp(x) = 1⇐⇒ ∥x∥0= 1. We refer to Section 2.1 for more detail on this measure. In summary, given a
set of vectors{ci∈Rni}r
i=1and a desired average sparsity level s∈[0,1], we will compute a set of vectors
{xi∈Rni}r
i=1that are closest to {ci∈Rni}r
i=1for eachi(see Section 3 for the details), while the average
Hoyer sparsity of the vectors xi’s is larger than s, that is,1
r/summationtextr
i=1sp(xi)≥s.
This projection is inspired by the work of Thom et al. (2015) in which each feature vector ciis independently
projected, and used for sparse dictionary learning. Although a range of approaches exist to induce sparsity,
this particular measure has been shown to enjoy many attractive properties (Hurley & Rickard, 2009) while
also being very amenable for optimization. The key difference with our projection from previous works
by Potluru et al. (2013); Thom et al. (2015) is that the feature vectors achieve an average target sparsity
level: some may end up dense, while others extremely sparse based on the problem. Therefore, our approach
has three main advantages:
1. Only one sparsity parameter has to be chosen, namely s∈[0,1].
2.The sparsity levels of the projected vectors are automatically tuned to achieve the desired average
sparsity; hence, allowing these vectors to have different sparsity levels.
3.Our projection has more degrees of freedom and consequently will generate sparse feature vectors
that are closer to the original ones.
Our novel projection operator will rely on duality and Newton’s method to compute the unique solution under
mild assumptions. This new approach to project a set of vectors could be used in numerous applications where
more than one sparse vector has to be learned, e.g., in dictionary learning (Thom et al., 2015) and sparse
low-rank matrix approximations (d’Aspremont et al., 2007). In this paper, we will explore our novel projection
operator in the settings of pruning deep neural networks and sparse non-negative matrix factorization (NMF).
1.1 Related Work
Projection onto the ℓ1ball, and onto the intersection of ℓ1,ℓ2ballsℓ1-ball projections have a rich
history in the literature and in particular have been considered earlier by Gafni & Bertsekas (1984). Recent
versions are optimal with run times linear in size of the input (Duchi et al., 2008; Condat, 2016). Projections
onto the intersection of ℓ1,ℓ2-ball constraints was introduced by Hoyer (2004) and subsequently addressed in
a series of works by Yu et al. (2012); Potluru et al. (2013); Thom et al. (2015); Liu et al. (2019), resulting in
essentially the same optimal run times as shown in the ℓ1settings. Note that none of the approaches above
can directly handle the group sparsity problem that arises in Sparse NMF and neural network models.
Sparse NMF In nonnegative matrix factorization, an input data matrix Y∈Rm×nis approximated by a
low-rank matrix ˜Y= XHwhere X∈Rm×r
+andH∈Rr×n
+, andris the factorization rank. The most popular
formulation of NMF uses the Frobenius norm to evaluate the quality of the solution as follows:
min
X∈Rm×r,H∈Rr×n∥Y−XH∥2
F
such that X≥0andH≥0.(2)
2Published in Transactions on Machine Learning Research (10/2022)
Many algorithms have been proposed to tackle this problem such as by Kim & Park (2007) and Potluru et al.
(2013). Most of them use an alternating strategy optimizing XforHfixed and then HforXfixed, since the
corresponding subproblems are convex; see Section 5.1 for more detail.
In practice, it is particularly useful to have sparse Xand/or Hto take prior information into account, leading
to more robust, identifiable and more interpretable decomposition (Hoyer, 2004). We will discuss in detail
sparse NMF formulations in Section 5.1.
Neural Network Pruning It has been known since the 1980s that a significant number of parameters can
be eliminated from neural networks without appreciable loss in accuracy (Janowsky, 1989; LeCun et al., 1990;
Reed, 1993). It is indeed an attractive proposition to prune such large networks for real-time applications
especially, on edge devices with resource constraints. Pruning large networks could substantially reduce the
computational demands of inference when used with appropriate libraries (Elsen et al., 2020) or hardwares
designed to exploit sparsity (Pool et al., 2021; Cerebras, 2019). In recent times, the Lottery Ticket Hypothesis
(Frankle & Carbin, 2019) was proposed that details the presence of sub-networks within a larger network,
which are capable of training to full accuracy in isolation. This resulted in a renewed interest in sparse deep
learning and model pruning (Renda et al., 2020; Chen et al., 2020; 2021) and more recently in the area of
sparse reinforcement learning (Arnob et al., 2021; Sokar et al., 2021). There are a range of techniques in
the literature to prune deep neural networks and find sub-networks at various stages of training: techniques
that prune before training (Lee et al., 2019; Wang et al., 2020), during training (Zhu & Gupta, 2018; Ma
et al., 2019) and after training (Han et al., 2015). The most common among these techniques is to prune the
network after training using some sort of predefined criterion that captures the significance of the parameters
of the network to the objective function. A range of classical works on pruning used the second derivative
information of the loss function (LeCun et al., 1990; Hassibi et al., 1993). Perhaps the most intuitive of these
approaches is magnitude based pruning, where following training, a subset of the parameters below some
threshold is pruned and the rest of the parameters are retrained (Han et al., 2015; Guo et al., 2016), and
regularization based methods (Yang et al., 2020; Ma et al., 2019; Louizos et al., 2018; Yun et al., 2019) which
induces sparsity in the network during the optimization process.
However, we still lack a way to introduce sparsity in the layers of the network which is both controllable
and interpretable. For example, in the case of regularizer imposed sparsity in neural networks (Yang et al.,
2020), there is no way to relate the regularization weight with the actual sparsity of the result. Moreover, in
practice we have to tune the regularizer weight differently for each task and architecture. Hence a form of a
grid search for that parameter is unavoidable, which is costly both in terms of time and resources.
1.2 Contributions and Outline
Our contributions can be summarized as follows:
1. We define a novel grouped sparse projection (GSP) with a single sparsity parameter (Section 2).
•We provide an efficient algorithm (linear in the size of the problem) to compute this projection,
based on the Newton’s method (Section 3).
•We extend our approach to perform weighted grouped sparse projection using the weighted ℓ1
norm (Section 4 and Appendix C).
2. We evaluate GSP on sparse NMF and pruning deep neural network tasks (Section 5).
•For NMF, GSP competes with state-of-the-art methods on an image dataset and outperforms
them on a synthetic dataset (Section 5.1).
•In pruning deep neural networks, GSP achieves higher accuracies at corresponding sparsity values
compared to competing methods on the CIFAR10 dataset. On the Imagenet task, it outperforms
a range of pruning methods in terms of sparsity versus accuracy trade-off (Section 5.2).
•GSP can also recover competitive accuracy with a single projection of large pre-trained models
followed by finetuning, altogether skipping the training with regularization phase. (Section 5.3).
Please check the appendix for detailed proofs, implementation details, and additional experiments.
3Published in Transactions on Machine Learning Research (10/2022)
Notation We denote Rnthe set ofn-dimensional vectors in R,Rn
+=Rn∩{x|x≥0}wherex≥0means
that the vector xis component-wise nonnegative, Rn
0=Rn\{0}andRn
0,+=Rn
+\{0}where 0is the vector of
zeros of appropriate dimension. For x∈Rn, we denote sign(x)the vector of signs of the entries of x,|x|the
component-wise absolute value of the vector x,[x]+=max(0,x)the projection onto the nonnegative orthant,
x(i)theith entry of x,∥x∥0the number of nonzero entries of x, that is, the ℓ0norm,∥x∥1=/summationtextn
i=1|x(i)|
theℓ1norm ofx, and∥x∥2=/radicalig/summationtextn
i=1x(i)2theℓ2norm ofx. We also denote ◦the component-wise product
between two vectors, that is, z=x◦y⇐⇒z(i)=x(i)y(i)∀i,1the vector of all ones of appropriate
dimension, and J1,rK={1,2,...,r}.
2 Background on Projection with the Hoyer Sparsity Measure
In this section, we define the problem of sparse projection and formulate the sparse projection task for a
single vector.
2.1 Hoyer sparsity measure
Given a vector x∈Rn, a meaningful way to measure its sparsity is to consider the Hoyer sparsity defined
in(1). A main advantage of sp(x)compared to∥x∥0is that sp(x)is smooth except on a set of measure
zero, namely when x(i) = 0for somei, since∥x∥1is not smooth at these points. However, it is continuous
everywhere except at the origin, x= 0, where it is not defined. Moreover, when considering only the
nonnegative orthant, that is, x∈Rn
+, then sp(x)is smooth everywhere except at the origin, since ∥x∥1=1Tx
forx≥0. As we will see in Sections 2.2 and 3, see in particular (4)and(GSP), one can restrict the search
space to the nonnegative orthant which makes sp(x)even more convenient to work with. These are crucial
properties that allows one to project efficiently onto the set of vectors of a given Hoyer sparsity (Thom et al.,
2015). Note that sp(x)is invariant to scaling, that is, sp(x) = sp(αx)for anyα̸= 0.
Another useful property of sp(x)is its nonincreasingness under the soft thresholding operator: Given a vector
xand a parameter λ≥0, the soft-thresholding operator is defined as
st(x,λ) = sign(x)◦[|x|−λ1]+.
This property will be particularly useful later when deriving our novel projection of a set of vectors. Note
that forλbetween the largest and second largest entry of |x|,st(x,λ)is 1-sparse with sp (st(x,λ))= 1, hence
sp (st(x,λ))is constant. Interestingly, for x=b1for some constant b, it is not possible to sparsify x(because
we cannot differentiate between its entries) and st(x,λ)is constant for all λ < b. Note that while the ℓ0
norm can also be used directly as a sparsity measure (Bolte et al., 2014; Pock & Sabach, 2016), it does not
enjoy some of the nice properties of the Hoyer-sparsity measure; see Appendix A.1, Hurley & Rickard (2009)
and Thom et al. (2015) for more detailed discussions.
2.2 Single Vector Sparse Projection
Let us first present the sparse projection problem for a single vector, along with a reformulation that will
be useful to project a set of vectors. These derivations are similar to that of Hoyer (2004); Potluru et al.
(2013); Thom et al. (2015). Given c∈Rn
0and a sparsity level s∈[0,1], the sparse projection problem can be
formulated as follows
min
z∈Rn∥c−z∥2such that sp (z)≥s. (3)
Let us use the change of variable z=αx, withα=∥z∥2≥0and∥x∥2= 1. Note that sp(z) =sp(αx) =sp(x)
since sp(.)is invariant to scaling. Hence αdoes not appear in the sparsity constraints. Moreover, αcan be
optimized easily. By expanding the ℓ2norm we have:
α∗= argminα≥0∥c−αx∥2= max(0,xTc),
since∥x∥2= 1. Forα∗>0, we have
∥c−α∗x∥2
2=∥c∥2
2−2α∗xTc+ (α∗)2=∥c∥2
2−(xTc)2.
4Published in Transactions on Machine Learning Research (10/2022)
We notice that the sign of the entries of xcan be chosen freely since the constraints are not influenced by
flipping the sign of entries of x. This implies that, at optimality,
•the entries of xwill have the same sign as the entries of c, and
•α∗>0sincec̸= 0andx̸= 0.
Therefore, (3) can be reformulated as
max
x∈Rn
0xT|c|
such that∥x∥2= 1,x≥0and sp (x)≥s.(4)
In fact, the optimal solution of (3)is given by z∗= (|c|Tx∗)sign(c)◦x∗, wherex∗is an optimal solution
of(4). Although this reformulation is relatively straightforward, it was not present in the literature, as far as
we know.
3 Grouped Sparse Projection (GSP)
We extend the argument of Section 2.2 to a set of vectors and present our novel approach to grouped sparse
projection.
3.1 Formulation of the Problem
Let{ci∈Rni
0}r
i=1be a set of non-zero vectors. The main goal of sparse projection is to find a set of
non-zero unit-norm vectors {xi∈Rni
0}r
i=1that has an average target sparsity larger than a given s∈[0,1].
Mathematically, let X={xi∈Rni
0,i∈J1,rK:xi≥0,∥xi∥2= 1∀i}. We propose the following novel grouped
sparse projection problem:
max
Xr/summationdisplay
i=1xT
i|ci|such that1
rr/summationdisplay
i=1sp (xi)≥s. (GSP)
The main reason for the choice of this formulation is that it makes GSP much faster to solve. In fact, as for (3)
that was studied by Thom et al. (2015), we will be able to reduce this problem to the root finding problem of
a nonincreasing function in one variable. In particular, using the objective function minX/summationtextr
i=1∥ci−xi∥2(or/summationtextr
i=1∥ci−xi∥2
2) would not allow such an effective optimization scheme.
Remark 3.1 (Abuse of terminology) .The solution to the problem (GSP)is not projection, as it does not
provide a point within a set closest in some norm to a given point. However, for simplicity, and since it
extends the projection in the case of a single vector, see (3)and(4), we abuse the terminology and refer
to (GSP) as a projection.
Let us reformulate GSP focusing on the sparsity constraint: we have
r/summationdisplay
i=1sp (xi) =r/summationdisplay
i=1√ni−∥xi∥1√ni−1=r/summationdisplay
i=1√ni√ni−1−r/summationdisplay
i=1∥xi∥1√ni−1≥rs,
where we used the fact that ∥xi∥2= 1for alliin the feasible set X. Denoting ks=/summationtextr
i=1√ni√ni−1−rsand
βi=1√ni−1, GSP can be reformulated as follows
max
Xr/summationdisplay
i=1xT
i|ci|such thatr/summationdisplay
i=1βi1Txi≤ks. (5)
We used∥xi∥1=1Txisincexi≥0. Note that the maximum of (5)is attained since the objective function is
continuous and the feasible set is compact (extreme value theorem).
5Published in Transactions on Machine Learning Research (10/2022)
3.2 Lagrange Dual Formulation
In order to solve (5), we follow a standard dual approach, similarly as done by Thom et al. (2015) for the
projection of a single vector. However, our derivations are rather different because the vectors to be projected
share the same Lagrange dual variable, while we need to carefully treat the case when some of the vectors
are projected onto a 1-sparse vector. Let us introduce the Lagrange variable µ≥0associated with the
constraint/summationtextr
i=1βi1Txi≤ks. The Lagrange dual function with respect to µis given by
ℓ(µ) = max
Xr/summationdisplay
i=1xT
i|ci|−µ/parenleftiggr/summationdisplay
i=1βixT
i1−ks/parenrightigg
= max
Xr/summationdisplay
i=1xT
i(|ci|−βiµ1) +µks. (6)
The dual problem is given by minµ≥0ℓ(µ). The optimization problem to be solved to compute ℓ(µ)is
separable in variables xi’s and consequently can be solved individually for each xi. Let us denote xi[µ]the
optimal solution of (6). For each i, there are two possible cases, depending on the value of µ:
1.|ci|−µβi1>0: the optimal xi[µ]is given by
xi[µ] =/bracketleftbig
|ci|−µβi1/bracketrightbig
+/vextenddouble/vextenddouble/vextenddouble/bracketleftbig
|ci|−µβi1/bracketrightbig
+/vextenddouble/vextenddouble/vextenddouble
2=st(|ci|,µβi)/vextenddouble/vextenddoublest(|ci|,µβi)/vextenddouble/vextenddouble
2.
This formula can be derived from the first-order optimality conditions. Note that this formula is
similar to that of Thom et al. (2015). The difference is that the xi’s share the same Lagrange
variableµ.
2.|ci|−µβi1≤0: the optimal xi[µ]is given by the 1-sparse vector whose nonzero entry corresponds
to the largest entry of |ci|−µβi1, that is, of|ci|. Note that if the largest entry of |ci|is attained
for several indexes, then the optimal 1-sparse solution x∗
iis not unique. Note also that this case
coincides with the case above for µin the interval between the largest and second largest entry of ci.
3.3 Characterizing the Optimal Solution xi[µ]
Let˜µbe the smallest value of µsuch thatxi[˜µ]are all 1-sparse. There are three scenarios based on the value
ofµ: (a)µ= 0, (b)µ≥˜µand (c) 0<µ< ˜µ.
(a)Forµ= 0, we havexi[0]=|ci|
∥ci∥2. Ifxi[0]is feasible, that is,/summationtextr
i=1βi1Txi[0]≤ks, then it is optimal
since the error of GSP is zero: this happens when the ci’s are already sparse enough and do not need
to be projected.
(b) Forµ≥˜µ, allxi[˜µ]are all 1-sparse so that 1Txi[˜µ] = 1hence
g(˜µ) =r/summationdisplay
i=1βi−ks
=r/summationdisplay
i=11√ni−1−r/summationdisplay
i=1√ni√ni−1+rs
=r(s−1)≤0. (7)
The value ˜µis given by the second largest entry among the vectors |ci|/βi’s. In fact, if µis larger
than the second largest entry of |ci|, thenxi[µ]is 1-sparse. Note that if the largest and second largest
entry of a vector |ci|are equal, then g(µ)is discontinuous. This is an unavoidable issue when one
wants to make a vector sparse: if the largest entries are equal to one another, one has to decide
which one will be set to zero. (For example, [1,0]and[0,1]are equally good 1-sparse representation
of[1,1].)
6Published in Transactions on Machine Learning Research (10/2022)
Algorithm 1 GSP({ci∈Rni}r
i=1,s,ϵ,rl)
1:input:{ci∈Rni}r
i=1, the average sparsity s∈[0,1], the accuracy ϵ, the parameter rl∈[1/2,1)
2:output:{zi∈Rni}r
i=1with average sparsity in [s−ϵ,s+ϵ]
3:µ= 0,¯µ= ˜µ,µ∗= 0,∆ = ¯µ−µ.
4:while|g(µ∗)|>rϵ do
5:µold=µ∗
6:µ∗=µ∗+k−g(µ∗)
g′(µ∗)▷Newton’s step
7:ifµ∗/∈[µ,¯µ]thenµ∗=µ+¯µ
2end if ▷Bisection method if Newton’s step fails
8:ifg(µ∗)>0thenµ=µ∗else ¯µ=µ∗end if ▷Update feasible interval
9:if¯µ−µ>rl∆and|µold−µ∗|<(1−rl)∆then
10:µ∗=µ+¯µ
2▷If feasible interval is not reduced use bisection again
11: ifg(µ∗)>0thenµ=µ∗else ¯µ=µ∗end if ▷Update feasible interval
12: end if
13: ∆ = ¯µ−µ ▷Update diameter ∆
14: if∆<ϵµ∗and|µ∗−µ|<ϵµ∗for someµ∈Dthen
15:break; ▷Dcontains the set of discontinuous points, stop GSP
16: end if
17:end while
18:returnz∗
i={|ci|Txi[µ∗] sign (ci)◦xi[µ∗]}r
i=1▷Computexi[µ∗](see Section 3.2) and return projections
(c) For 0<µ< ˜µ, the constraint is active at optimality and we need to find the value µsuch that
g(µ) =r/summationdisplay
i=1βi1Txi[µ]−ks= 0,
that is, find a root of g(µ). Theorem 3.2 guarantees that the solution µ∗is unique in this setting,
and Corollary 3.3 proves the respective projection x[µ∗] =/bracketleftbig
x1[µ∗],...,xr[µ∗]/bracketrightbig
is also unique.
Theorem 3.2 (Uniqueness of µ∗).The function g(µ)is strictly decreasing for 0<µ< ˜µ. Hence, it is not
discontinuous around g(µ) = 0and attains a unique root µ∗.
Proof sketch. The function g(µ)is continuous as it is a continuous function of x(µ), which is continuous.
Hence, it suffices to show that g′(µ)<0for0<µ< ˜µ. See Appendix C.3 for a detailed derivation in a more
general case.
Corollary 3.3 (Uniqueness of projection x∗).If the largest entry of each |ci|is attained for only one entry,
the projection x[µ∗]is unique.
Proof sketch. Ifµ= 0andµ>˜µ,x(µ)is unique provided the largest and second entry of each |ci|are not
equal (see Section 3.3). For 0<µ< ˜µ, since the optimal µ∗is unique it suffices to prove xi[µ]is a one-to-one
mapping, i.e., that if xi(µ1) =xi(µ2)thenµ1=µ2. Detailed derivations shows that xi[µ]is a one-to-one
mapping unless all entries of ciare equal, which is in contrast with the assumption (see Appendix B).
Once the optimal µ∗and the respective projections xi[µ∗]are computed, one can retrieve the sparse projections
viaz∗
i= (|ci|Txi[µ∗]) sign(ci)◦xi[µ∗]fori∈J1,rK.
3.4 Implementation and Computational Cost
In order to compute the dual variable µ, which leads to the desired average Hoyer sparsity for the vectors
xi[µ]’s, we resort to a standard approach to compute the root of the nonlinear function, g(µ), namely Newton’s
method which has quadratic convergence (given that we are sufficiently close to a root, and that the function
is sufficiently smooth). Because g(µ)is not smooth everywhere, we have implemented the bisection method
7Published in Transactions on Machine Learning Research (10/2022)
as a fallback procedure, which guarantees the algorithm’s computational complexity to be upper-bounded by
O(NlogN). The use of bisection is necessary if Newton’s method either goes outside of the current feasible
interval [µ,¯µ]containing the solution µ∗or stagnates locally and does not converge (although we have not
observed the latter behavior in practice). This is a a standard textbook strategy in numerical analysis.
Algorithm 1 summarizes the GSP algorithm. The main computational cost of GSP is to compute g(µ)and
g′(µ)at each iteration, which requires O(N)operations where N=/summationtextr
i=1ni. Most of the computational
cost resides in computing the xi[µ]’s and some inner products. The total computational cost is O(tN),
wheretis the number of Newton’s method iterations, hence making GSP linear in the size of the problem. In
practice, we observed that Netwon’s method converges very fast and does not require many iterations t(see
Appendix A.4 for a synthetic data example with vectors of dimension ni= 1000, in which Newton’s achieves
convergence in t= 4iterations or less). In our experiments we have observed that using Newton’s method
with initial point µ= 0performs well, which is in line with the results of Thom et al. (2015), as points where
the function is not differentiable typically form a set of measure zero (Kummer et al., 1988). The reason to
chooseµ= 0as the initial point is because g(µ)decreases initially fast (all entries of xicorresponding to a
nonzero entry of ciare decreasing) while it tends to saturate for a larger µ. In particular, we cannot initialize
µat values larger than ˜µsinceg(µ)is constant ( g′(µ) = 0) for allµ≥˜µ. Finally, we have included a method
to detect discontinuities, corresponding to situations when the largest entries of the ci’s are not unique. In
that case we require GSP to return a µsuch thatg(µ+ϵµ)<0<g(µ−ϵµ), whereϵis a desired accuracy
(which in practice can be set to ϵ= 10−4).
4 Weighted Grouped Sparse Projection
In some settings, one may want to minimize a weighted sum of the entries of the xi’s, that is,/summationtextr
i=1wT
ixi≤k
for somewi∈Rni
+. This amounts to replacing the ℓ1-norm in the sparsity measure sp(.)by a weighted ℓ1
norm. We introduce the notion of weighted sparsity : For a vector x∈Rn
0and givenw∈Rn
0,+, we define
spw(x) =∥w∥2−∥Wx∥1
∥x∥2
∥w∥2−miniw(i)∈[0,1], (8)
whereW=diag(w). We use the weighted sparsity in (8)to define the w-sparse projection problem of the set
of vectors{ci∈Rni}r
i=1. Let{wi∈Rni
0,+}r
i=1be the nonzero nonnegative weight vectors associated with the
ci’s. Given a target average weighted sparsity sw∈[0,1], we formulate the weighted group sparse projection
(WGSP) problem as follows:
max
Xr/summationdisplay
i=1xT
i|ci|such that1
rr/summationdisplay
i=1spwi(xi)≥sw. (WGSP)
We have included properties of the proposed weighted sparsity, along with full derivation of the WGSP
problem, the optimal solutions and experiments in Appendix C.
5 Experiments
In this section, we evaluate the utility of GSP in two applications: sparse NMF and deep learning network
pruning. The goal is to evaluate the effect of performance at desired sparsity in the unsupervised and
supervised setting.
5.1 gspNMF: Projection-based Sparse NMF
As explained in Section 1.1, the standard NMF problem is formulated as in (2). As studied in detail in the
recent book by Gillis (2020), the most widely used and efficient algorithms for NMF are block coordinate
descent algorithms, and two of the most popular and effective ones are the following:
1.A-HALS updates alternatively the columns of Xand the rows of Husing a closed-form solution (Gillis
& Glineur, 2012),
8Published in Transactions on Machine Learning Research (10/2022)
2.NeNMF updates alternatively the full factors XandH. The update of each factor requires to solve
convex non-negative least squares problems that NeNMF solves approximately using a few steps of
Nesterov fast gradient method (FGM), an optimal first-order method (Guan et al., 2012).
Sparse NMF enforces an additional sparsity constraint on Xand our GSP leads to a natural sparse NMF
formulation: given an average sparsity level for the columns of X,s, solve
min
X∈Rm×r,H∈Rr×n∥Y−XH∥2
F
such that X≥0,H≥0and1
rr/summationdisplay
k=1sp (X(:,k))≥s,(9)
where X(:,k)is thekth column of X. To solve (9), we use the same update as A-HALS for Has it is not
affected by the sparsity requirement. For updating X, we adapt the update of NeNMF (Guan et al., 2012) by
replacing the projection onto the nonnegative orthant with GSP; we refer to this new sparse NMF algorithm
as group sparse projection NMF (gspNMF). Note that our feasible set is not convex, hence FGM is not
guaranteed to converge. However, we use a restarting scheme and keep the best iterate in memory. We
compare gspNMF with
1.The same algorithm where the projection is performed column-wise with the columns constrained
to have the same sparsity level, as done by Thom et al. (2015). This algorithm solves the same
problem (9)where the last constraint is replaced by sp (X(:,k))≥sfor allk. We refer to this
algorithm as cspNMF.
2. A-HALS with ℓ1penalty described by Gillis (2012) that uses the formulation
min
X∈Rm×r,H∈Rr×n∥Y−XH∥2
F+k/summationdisplay
k=1λk∥X(:,k)∥1
such that X≥0,H≥0,and∥X(:,k)∥∞= 1fork∈J1,rK,(10)
where theℓ1penalty parameters for each column of X,λk’s, are automatically tuned to achieve the
same desired ℓ0sparsity level1; see the code provided by the author. We refer to this algorithm as ℓ1
A-HALS.
We compare all algorithms in terms of error per iteration. Since all algorithms have almost the same
computational complexity per iteration2, we believe it is fair and more insightful to compare these five
algorithms with respect to the iteration number. In all experiments, we perform 500iterations (updates of X
andH) of each algorithm. See Appendix D for details about the experimental parameters and datasets used.
gspNMF on synthetic data We first perform experiments on synthetic data sets where WandHare
generated randomly, with Whaving about 50% of its entries equal to zero, and with setting m=n= 100
andr= 10; see Appendix D.1 for the details. We note an interesting finding: if gspNMF is given the sparsity
of an exact factorization, it converges to an exact solution much faster than A-HALS and NeNMF. In other
words, gspNMF is able to use the prior information to its advantage. Although A-HALS and NeNMF are less
constrained, as they solve (2)instead of (9), they converge significantly slower. From Figure 1 we observe
1Although ℓ1A-HALS does not use the same sparsity measure as gspNMF and cspNMF, these two measures are close to one
another in most cases. For example, in the synthetic data experiments (see below), we will generate n-dimensional vectors using
the Gaussian distribution and setting the negative entries to zero. Such vectors have an expected ℓ0sparsity of 50%. Generating
100 such vectors, their average Hoyer sparsity is 48.31% while their average ℓ0sparsity is 49.9%.
2The main computation cost resides in computing YHT(resp. YX) when updating X(resp. H) which requires O(mnr)
operations. gspNMF and cspNMF are slightly more expensive because of the projection step, although this is not the main
computational cost (the projection cost is linear in mandr). For example, on synthetic data sets for nsufficiently large (see
below), gspNMF and cspNMF took on average the same computational time than NeNMF and A-HALS (in fact, for some runs
with n= 104, gspNMF surprisingly ran slightly faster than NeNMF and A-HALS, due to inaccuracy in accounting for the
computational time).
9Published in Transactions on Machine Learning Research (10/2022)
Figure 1: Comparison of NMF and sparse NMF algorithms. On the left: Average relative error
100∥Y−XH∥F−emin
∥Y∥Fobtained with different NMF algorithms over 50 synthetic data sets. On the right: Average
relative error in percent to which the lowest error eminobtained among all algorithms and all initializations
is subtracted (hence the error should go to zero for the best algorithm), that is, 100∥Y−XH∥F−emin
∥Y∥F, over 10
random initializations on the CBCL data set with r= 49.
that gspNMF performs better than A-HALS, which in turn outperforms NeNMF. gspNMF reduces the error
towards zero much faster, although having a higher initial error as it is more constrained.
Since cspNMF is more constrained, it is not able to perform as well as gspNMF. This illustrates the significant
benefit of using an average sparsity instead of a single sparsity level for all columns of X. For similar reasons,
ℓ1A-HALS performs worse as it attempts to achieve the same given sparsity level for all columns of X.
gspNMF on CBCL facial image data We test our method on one of the most widely used datasets in
the NMF literature, the CBCL facial images used in the seminal work by Lee & Seung (1999) with r= 49.
NeNMF and A-HALS for NMF (2)generate solution with average Hoyer sparsity about 70%. We run the
sparse NMF techniques with sparsity set at 85%, hence we expect sparse NMF to have higher approximation
errors but have higher sparsity. Using 10 random initializations, Figure 1 reports the evolution of the average
relative error. In this case, gspNMF performs similar to the state-of-the-art algorithms, cspNMF and ℓ1
A-HALS, because the columns of Xhave similar sparsity levels. We include the basis elements obtained by
each different methods in Fig 5 of Appendix-D.2.
5.2 Pruning Deep Neural Networks with GSP
We evaluate performance of GSP in pruning each layer of a modern convolutional neural network using a
fixed value of the sparsity parameter s. Although we chose to prune each layer with the same sparsity to
simplify the comparison, GSP can easily be used with different sparsity levels for parameters of different
layers, providing fine control over the sparsity of the network.
To simplify comparison with related work we express the sparsity of the pruned network in terms of the
number of zeroed parameters instead of sp(x)(1). Since sp(x)is a differentiable approximation of the ℓ0
norm, applying GSP with sparsity sto a layer of the network pushes sfraction of the parameters to zero or
near zero, thus retaining interpretability of the parameter s.
We run experiments on the CIFAR-10 (Krizhevsky et al., 2009) dataset with the VGG16 model and on the
ILSVRC2012 Imagenet dataset (Russakovsky et al., 2015) with the ResNet50 model (He et al., 2016)3. For
the fully connected layers, we project the connections in each layer separately, with a target sparsity s. This
ensures the weights of that particular layer have sp(x) =s. For the convolutional layers, we project each
c×k×kfilter treating each as a vector xi∈Rnin our formulation, where n=c×k2andcis the number of
input channels. All the filters in a particular layer are projected at once.
3The code is available at https://github.com/riohib/gsp-for-deeplearning
10Published in Transactions on Machine Learning Research (10/2022)
VGG16 on CIFAR-10 Model Sparsity
Methods 80% 85% 90% 95% 97%
Dense Baseline 92.82 % - - - -
Random Pruning 83.7% 82.75% 81.56 % 78.18% 76.3%
Magnitude Pruning
(Han et al., 2015)92.78% 92.74% 92.5% 91.41% 10%
DeepHoyer
(Yang et al., 2020)91.23% 91.2% 91.42% 91.47% 91.54%
GSP 92.37% 92.28% 92.39% 92.32% 91.92%
Table 1: Test Accuracy of pruned VGG16 network using GSP vs random pruning and other pruning methods
for sparsity varying in the 80%to97%range.
We test two pruning strategies using GSP. The first strategy consists in projecting the layers of a model every
specified number of iterations, then pruning the weights post-training followed by finetuning of the surviving
weights – we call this ”induced-GSP” training. In the second strategy, we start with a pretrained model,
project the layers once, and finetune the model. The second approach skips the sparsity inducing training
phase, which is a requirement for popular regularization-based sparse techniques. We denote this approach as
"single-shot GSP".
Table 1 reports the test accuracy of induced-GSP at different sparsity levels against (a) random pruning,
which randomly selects (1−s)fraction of the model weights and retrains them, (b) magnitude pruning,
where the top (1−s)fraction of the parameters are selected post-training and retrained and (c) DeepHoyer
(DH, Yang et al. 2020), a high-performance regularization-based pruning technique. GSP clearly outperforms
random pruning and finds out important connections in the network. While magnitude pruning performs
slightly better than GSP at lower sparsity levels, GSP achieves comparable and higher accuracies in the
sparsity region s>90%. Finally, GSP achieves higher accuracy than DH throughout the different sparsity
levels tested. We note that since DH requires a regularization parameter to be tweaked using naive search,
we trained upwards of 60models to achieve the reported performance. This quickly becomes infeasible for
training ImageNet level tasks without using significant resources.
20 30 40 50 60 70 80 90 100
Sparsity %−15.0−12.5−10.0−7.5−5.0−2.50.02.55.0Accuracy Δ %GSP
SynFlow
Magnitude
DeepHoyer
SNIP
GraSP
ThiNet
NISP
DCP
Figure 2: Accuracy change relative the dense model at different levels of sparsity for multiple state-of-the-art
network pruning techniques applied to ResNet50 on the ImageNet dataset. Values towards top-right are
better. Notably, GSP provides superior sparsity vs accuracy trade-off.
11Published in Transactions on Machine Learning Research (10/2022)
ResNet50 on ImageNet Testing pruning techniques on the ImageNet dataset (Russakovsky et al., 2015)
is becoming a standard in the field of model compression as it is a large-scale complex dataset. Therefore,
we test the efficacy of GSP on the ImageNet dataset and compare it with a range of pruning techniques
from the literature. We compare against regularization-based (Yang et al., 2020), magnitude-based (Han
et al., 2016), and structure-based pruning (Luo et al., 2017; Yu et al., 2018; Zhuang et al., 2018). We also
test GSP against pruning at initialization techniques by Lee et al. (2019); Wang et al. (2020); Tanaka et al.
(2020), that have the added benefit of training a sparse model but the disadvantage of having almost no
training information to guide pruning. We project the layers of a Res-Net50 model with GSP every fixed
iterations and subsequently prune the parameters identified by our method according to set sparsity values.
Pruning a network is a trade-off between sparsity and accuracy. Hence, in Figure 2 we report the sparsity and
accuracy of GSP against other pruning techniques. We observe that even at relatively high sparsity levels,
GSP competes with and outperforms all pruning techniques we compared with, dropping only 0.41,0.8,1.43
and2.2percentage points at 70,80,85and90percent sparsity respectively.
5.3 Single Shot Network Pruning
GSP can also be utilized to generate sparse models by a single projection step instead of repeated projections
throughout the training process. Thus, we can first take a pretrained model, project the model once layerwise
with our choice of sparsity, and finally finetune the surviving connections. We refer to this approach as
“Single-Shot GSP”. This is in contrast to popular regularization-based techniques of Yang et al. (2020) and
Ma et al. (2019) which need to train with the regularizer first, then prune the weights and finally finetune
the surviving weights to generate the final sparse model. The ability to project a pretrained model directly
without going through a regularized training phase would be an attractive ability to have, e.g. in transfer
learning.
Table 2 shows the accuracy and sparsity value of induced GSP and single-shot GSP against DeepHoyer (DH)
on the ResNet-56 and ResNet-110 architectures and tested on the CIFAR-10 Dataset. The drop in accuracy
compared to their respective baselines is reported as well. Noticeably, single-shot GSP performs comparably
or better than DH. Particularly, we notice that for the ResNet-56 architecture, single-shot GSP achieves
a higher sparsity of 85.75% with the same accuracy drop of 0.42% compared to DH. For the ResNet-110
architechture, we find that even with a higher sparsity of 85.90% we achieve lower accuracy drop compared
to DH. Subsequently, even after pushing the single-shot GSP sparsity to 90.72%, the accuracy drop was only
1.17%, still lower than DH’s 1.24% at 83.94% sparsity. This result is even more salient as DH relies on an
extra sparsity-inducing regularizer phase with 164epochs of training to first induce sparsity, and then prunes
the model with a threshold followed by finetuning for similar number of epochs. In contrast, we use a preset
sparsity value for GSP, project a pretrained model once, and finetune the surviving weights.
Summary of the numerical experiments We have shown that GSP in the NMF application performs
competitively with state-of-the-art approaches on image data sets without the need to tune the sparsity
parameters and outperforms them on synthetic and sparse data sets. In deep network pruning, the use of GSP
leads to sparser and more accurate networks without extra training time needed to tune sparsity parameters.
On ImageNet data with ResNet50 model GSP outperforms a large number of existing approaches. It also
demonstrates superior performance when used for single-shot pruning.
6 Conclusion
We proposed a novel grouped sparse projection algorithm which allows us to project a set of vectors to a set
of sparse vectors whose average sparsity is controlled via a single interpretable parameter. This parameter
should be carefully chosen by the user to obtain a good tradeoff between the sparsity of the projected vectors
and how well they approximate the original input vectors; see, e.g., Table 1 and Figure 2 in the context of
pruning deep neural networks. We provided theoretical guarantees for the uniqueness of the solution of the
dual problem which is also shown to correspond to the uniqueness of the primal solution. Additionally, we
proposed an efficient algorithm whose running time is shown to be linear in the size of the problem. We
demonstrate the efficacy of the projection in two important learning applications, namely in pruning deep
12Published in Transactions on Machine Learning Research (10/2022)
Methods Architecture Sparsity% Accuracy% Drop %
Induced GSP ResNet-56 88.33 92.04 1.09
DH ResNet-56 84.64 92.71 0.42
Single-shot GSP ResNet-56 85.78 92.71 0.42
Induced GSP Resnet-110 95.18 92.67 0.95
DH ResNet-110 83.94 92.76 1.24
Single-Shot GSPResNet-110 85.90 92.86 0.76
ResNet-110 90.72 92.45 1.17
Table 2: Sparsity vs drop in accuracy for ResNet-56 and ResNet-110 for single-shot and induced GSP
(our algorithms) in comparison to DeepHoyer (DH) in the CIFAR-10 dataset. GSP with a single projection
produces a model with comparable or better sparsity vs accuracy trade-off than a full pipeline of DeepHoyer’s
regularized training and finetuning. The best sparsity-accuracy trade-off compared to DH are reported in
bold.
neural networks and sparse non-negative matrix factorization and validate it on a wide variety of datasets.
However, our projection is not limited to the listed applications and could also be used in dictionary learning,
sparse PCA, and in different types of neural network architectures. We expect the proposed group sparse
projection operator to have wide applicability given the interpretability of the sparsity measure, practical
efficiency of the projection, and strong theoretical guarantees.
Acknowledgement
We thank the action editor and the three reviewers for their insightful comments that helped us improve the
paper significantly.
This study was in part supported by NIH MH129047, NIH DA040487, NIH MH121885 and NSF 2112455.
Nicolas Gillis acknowledges the support by the Fonds de la Recherche Scientifique - FNRS and the Fonds
Wetenschappelijk Onderzoek - Vlanderen (FWO) under EOS Project no O005318F-RG47, and by the Francqui
Foundation.
Niccolò Dalmasso, Sameena Shah and Vamsi K. Potluru at the time of publishing are at JP Morgan AI
Research. This paper was prepared for information purposes by the AI Research Group of JPMorgan Chase
& Co and its affiliates (“J.P. Morgan”), and is not a product of the Research Department of J.P. Morgan.
J.P. Morgan makes no explicit or implied representation and warranty and accepts no liability, for the
completeness, accuracy or reliability of information, or the legal, compliance, financial, tax or accounting
effects of matters contained herein. This document is not intended as investment research or investment
advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument,
financial product or service, or to be used in any way for evaluating the merits of participating in any
transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation
under such jurisdiction or to such person would be unlawful.
References
Aharon, M., Elad, M., and Bruckstein, A. K-SVD: An algorithm for designing overcomplete dictionaries for
sparse representation. IEEE Transactions on signal processing , 54(11):4311–4322, 2006.
Arnob, S. Y., Ohib, R., Plis, S., and Precup, D. Single-shot pruning for offline reinforcement learning. arXiv
preprint arXiv:2112.15579 , 2021.
Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences , 2(1):183–202, 2009.
Bolte, J., Sabach, S., and Teboulle, M. Proximal alternating linearized minimization for nonconvex and
nonsmooth problems. Math. Program. , 146(1-2):459–494, 2014. doi: 10 .1007/s10107-013-0701-9.
Candès, E., Romberg, J., and Tao, T. Robust uncertainty principles: Exact signal reconstruction from highly
incomplete frequency information. IEEE Transactions on Information Theory , 52(2):489–509, 2006.
13Published in Transactions on Machine Learning Research (10/2022)
Cerebras. Wafer Scale Engine: Why We Need Big Chips for Deep Learning. https://cerebras .net/blog/
cerebras-wafer-scale-engine-why-we-need-big-chips-for-deep-learning/ , 2019.
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M. The lottery ticket hypothesis
for pre-trained bert networks. Advances in neural information processing systems , 33:15834–15846, 2020.
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Carbin, M., and Wang, Z. The lottery tickets hypothesis
for supervised and self-supervised pre-training in computer vision models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 16306–16316, 2021.
Condat, L. Fast projection onto the simplex and the ℓ1ball.Mathematical Programming , 158(1):575–585,
2016.
d’Aspremont, A., El Ghaoui, L., Jordan, M., and Lanckriet, G. A direct formulation for sparse PCA using
semidefinite programming. SIAM Review , 49(3):434–448, 2007.
Donoho, D. Compressed sensing. IEEE Transactions on Information Theory , 52(4):1289–1306, 2006.
Donoho, D. L. De-noising by soft-thresholding. IEEE transactions on information theory , 41(3):613–627,
1995.
Duchi, J., Shalev-Shwartz, S., Singer, Y., and Chandra, T. Efficient projections onto the ℓ1-ball for learning
in high dimensions. In Proceedings of the 25th international conference on Machine learning , pp. 272–279,
2008.
Elad, M. Why simple shrinkage is still relevant for redundant representations? IEEE transactions on
information theory , 52(12):5559–5569, 2006.
Elsen, E., Dukhan, M., Gale, T., and Simonyan, K. Fast sparse convnets. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pp. 14629–14638, 2020.
Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In
7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019, 2019.
Gafni, E. M. and Bertsekas, D. P. Two-metric projection methods for constrained optimization. SIAM
Journal on Control and Optimization , 22(6):936–964, 1984.
Gillis, N. Sparse and unique nonnegative matrix factorization through data preprocessing. Journal of Machine
Learning Research , 13(Nov):3349–3386, 2012.
Gillis, N. Nonnegative Matrix Factorization . SIAM, Philadelphia, 2020.
Gillis, N. and Glineur, F. Accelerated multiplicative updates and hierarchical ALS algorithms for nonnegative
matrix factorization. Neural computation , 24(4):1085–1105, 2012.
Guan, N., Tao, D., Luo, Z., and Yuan, B. NeNMF: An optimal gradient method for nonnegative matrix
factorization. IEEE Transactions on Signal Processing , 60(6):2882–2898, 2012.
Guo, Y., Yao, A., and Chen, Y. Dynamic network surgery for efficient DNNs. In Proceedings of the 30th
International Conference on Neural Information Processing Systems , NIPS’16, pp. 1387–1395, Red Hook,
NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.
Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural
networks. arXiv preprint arXiv:1506.02626 , 2015.
Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural network with pruning,
trained quantization and huffman coding. In Bengio, Y. and LeCun, Y. (eds.), 4th International Conference
on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings , 2016.
14Published in Transactions on Machine Learning Research (10/2022)
Hassibi, B., Stork, D. G., and Wolff, G. J. Optimal brain surgeon and general network pruning. In IEEE
international conference on neural networks , pp. 293–299. IEEE, 1993.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Ho, N.-D. Nonnegative Matrix Factorization - Algorithms and Applications . PhD thesis, Université catholique
de Louvain, 2008.
Hoyer, P. O. Non-negative matrix factorization with sparseness constraints. Journal of machine learning
research, 5(Nov):1457–1469, 2004.
Hurley, N. and Rickard, S. Comparing measures of sparsity. IEEE Transactions on Information Theory , 55
(10):4723–4741, 2009.
Hyvärinen, A. Sparse code shrinkage: Denoising of nongaussian data by maximum likelihood estimation.
Neural computation , 11(7):1739–1768, 1999.
Janowsky, S. A. Pruning versus clipping in neural networks. Physical Review A , 39(12):6600, 1989.
Journée, M., Nesterov, Y., Richtárik, P., and Sepulchre, R. Generalized power method for sparse principal
component analysis. Journal of Machine Learning Research , 11(Feb):517–553, 2010.
Kim, H. and Park, H. Sparse non-negative matrix factorizations via alternating non-negativity-constrained
least squares for microarray data analysis. Bioinformatics , 23(12):1495–1502, 2007.
Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.
Kummer, B. et al. Newton’s method for non-differentiable functions. Advances in mathematical optimization ,
45(1988):114–125, 1988.
LeCun, Y., Denker, J. S., and Solla, S. A. Optimal brain damage. In Advances in neural information
processing systems , pp. 598–605, 1990.
Lee, D. and Seung, H. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):
788, 1999.
Lee, N., Ajanthan, T., and Torr, P. H. S. Snip: Single-shot network pruning based on connection sensitivity.
In7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 , 2019.
Liu, H., Wang, H., and Song, M. A unified approach for projections onto the intersection of ℓ1andℓ2balls or
spheres. arXiv preprint arXiv:1911.03946 , 2019.
Louizos, C., Welling, M., and Kingma, D. P. Learning sparse neural networks through ℓ0regularization. In
6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -
May 3, 2018, Conference Track Proceedings , 2018.
Luo, J.-H., Wu, J., and Lin, W. Thinet: A filter level pruning method for deep neural network compression.
InProceedings of the IEEE international conference on computer vision , pp. 5058–5066, 2017.
Ma, R., Miao, J., Niu, L., and Zhang, P. Transformed ℓ1regularization for learning sparse deep neural
networks. Neural Networks , 119:286–298, 2019.
Pock, T. and Sabach, S. Inertial proximal alternating linearized minimization (ipalm) for nonconvex and
nonsmooth problems. SIAM J. Imaging Sci. , 9(4):1756–1787, 2016. doi: 10 .1137/16M1064064.
Pool, J., Sawarkar, A., and Rodge, J. Accelerating Inference with Sparsity Using the NVIDIA Ampere
Architecture and NVIDIA TensorRT. https://developer .nvidia.com/blog/accelerating-inference-
with-sparsity-using-ampere-and-tensorrt/ , 2021.
15Published in Transactions on Machine Learning Research (10/2022)
Potluru, V. K., Plis, S. M., Roux, J. L., Pearlmutter, B. A., Calhoun, V. D., and Hayes, T. P. Block
coordinate descent for sparse NMF. In Bengio, Y. and LeCun, Y. (eds.), 1st International Conference
on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Conference Track
Proceedings , 2013.
Reed, R. Pruning algorithms-a survey. IEEE transactions on Neural Networks , 4(5):740–747, 1993.
Renda, A., Frankle, J., and Carbin, M. Comparing rewinding and fine-tuning in neural network pruning.
arXiv preprint arXiv:2003.02389 , 2020.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla,
A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10 .1007/s11263-015-0816-y.
Sokar, G., Mocanu, E., Mocanu, D. C., Pechenizkiy, M., and Stone, P. Dynamic sparse training for deep
reinforcement learning. arXiv preprint arXiv:2106.04217 , 2021.
Tanaka, H., Kunin, D., Yamins, D. L., and Ganguli, S. Pruning neural networks without any data by
iteratively conserving synaptic flow. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
(eds.),Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.
Thom, M., Rapp, M., and Palm, G. Efficient dictionary learning with sparseness-enforcing projections.
International Journal of Computer Vision , 114(2-3):168–194, 2015.
Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological) , 58(1):267–288, 1996.
Wang, C., Zhang, G., and Grosse, R. B. Picking winning tickets before training by preserving gradient flow.
In8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020 , 2020.
Yang, H., Wen, W., and Li, H. Deephoyer: Learning sparser neural network with differentiable scale-invariant
sparsity measures. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 , 2020.
Yu, A. W., Su, H., and Fei-Fei, L. Efficient euclidean projections onto the intersection of norm balls. In
Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland,
UK, June 26 - July 1, 2012 . icml.cc / Omnipress, 2012.
Yu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V. I., Han, X., Gao, M., Lin, C.-Y., and Davis, L. S. Nisp:
Pruning networks using neuron importance score propagation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pp. 9194–9203, 2018.
Yun, J., Zheng, P., Yang, E., Lozano, A., and Aravkin, A. Trimming the ℓ1regularizer: Statistical analysis,
optimization, and applications to deep learning. In International Conference on Machine Learning , pp.
7242–7251. PMLR, 2019.
Zhu, M. and Gupta, S. To prune, or not to prune: Exploring the efficacy of pruning for model compression.
In6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Workshop Track Proceedings , 2018.
Zhuang, Z., Tan, M., Zhuang, B., Liu, J., Guo, Y., Wu, Q., Huang, J., and Zhu, J. Discrimination-aware
channel pruning for deep neural networks. arXiv preprint arXiv:1810.11809 , 2018.
16Published in Transactions on Machine Learning Research (10/2022)
Appendices
A Hoyer Sparsity Measure and Grouped Sparse Projection
A.1 The Hoyer Sparsity Measure
In section 2.1 we discussed the Hoyer sparsity measure (Hoyer, 2004) for a given vector x∈Rnand forx̸= 0
andn>1, we defined the sparsity of xas
sp(x) =√n−∥x∥1
∥x∥2√n−1∈[0,1], (11)
The above measure has the following properties, which we list here for completeness:
•We have that sp(x) = 0⇐⇒ ∥x∥1=√n∥x∥2⇐⇒x(i) =bfor alliand for some constant b, while
sp(x) = 1⇐⇒ ∥x∥0= 1, where∥x∥0counts the number of nonzero entries of x.
•One of the main advantage of sp(x)compared to∥x∥0is that sp(x)is smooth over Rnapart from a
measure-zero set, i.e., when x= 0orx(i) = 0for anyi. For example, with this measure, the vector
[1,10−6,10−6]is sparser than [1,1,0], which makes sense numerically as [1,10−6,10−6]is very close
to the 1-sparse vector [1,0,0].
•sp(x)is invariant to scaling, that is, sp(x) = sp(αx)for anyα̸= 0.
•Note that for any two vectors wandz,sp(w)≤sp(z)⇐⇒∥w∥1
∥w∥2≥∥z∥1
∥z∥2.
•Note also that sp(x)is not defined at 0, nor for n= 1.
•It is non-increasing under the soft thresholding operator: Given a vector xand a parameter λ≥0, it
is defined as st(x,λ) =sign(x)◦[|x|−λ1]+where◦is the component-wise multiplication, and [.]+is
the projection onto the non-negative orthant, that is, max(0,.).
A.2 Comparison to typical formulations
We discuss how the GSP formulation compares with two typical approaches to sparsify a set of vectors:
•The most popular method to make a set of vectors sparse is arguably to use ℓ1penalty terms, solving
min
xi∈Rni
0,1≤i≤rr/summationdisplay
i=11
2∥ci−xi∥2
2+λi∥xi∥1,
for which the solution is given by the soft thresholding operator, x∗
i=st(ci,λi) 1≤i≤r. This is
widely used in algorithms for compressed sensing and for solving inverse problems (in particular, to
find sparse solutions to an underdetermined linear system); see, e.g., Beck & Teboulle (2009). The
use ofℓ1penalty to obtain sparse factors in low-rank matrix approximations is also arguably the
most popular approach; see, e.g., Kim & Park (2007); Journée et al. (2010).
The main drawback of this approach is that the parameters λineeds to be tuned to obtain a desired
sparsity level, As we noted, our projection resolves this drawback.
•Using the method of Thom et al. (2015) that projects a vector onto a vector with a desired level of
sparsity, that is, solves (3), we could either project each vector ciindependently but then we would
have to choose a priori the sparsity level of each projection xi. We could also project the single
vector [c1;c2;...;cr]∈R/summationtextr
i=1ni, stacking the ci’s on top of one another. However, some vectors ci
17Published in Transactions on Machine Learning Research (10/2022)
could be projected onto zero, which is not desirable in some applications; for example, in low-rank
matrix approximation problems this would make the matrix with xi’s as its columns rank deficient.
More pertinently, in the scenario of learning sparse deep models, stacking all the layers together and
projecting them this way would result in the projection of all the parameters of a layer onto zero,
also known as layer-collapse (Tanaka et al., 2020), which makes the model untrainable.
A.3 Properties of g(µ)and edge cases
In section 3 we discussed that the optimization problem to be solved to compute ℓ(µ)in(6)is separable in
the variables x′
is, and that it can be solved individually for each xi. We denoted the optimal solution of the
(6)withxi[µ]and noted that there are two possible cases, depending on the value of µ. We noted that unless
the solution for µ= 0is feasible (which can be checked easily), we need to find the value of µsuch that
g(µ) =r/summationdisplay
i=1βi1Txi[µ]−ks= 0,
that is, we need to find a root of g(µ)(ifµ>˜µthen thexi[µ]are all 1-sparse).
In section-3 we explored possible ways to find this root of g(µ), including using bisection and Newton’s
method. The reason to use bisection is mostly to avoid points of non-differentiability and local stagnation of
Newton’s method because of discontinuity. The discontinuous points can be pre-computed and corresponds to
the largest entries of the ci’s when they are not uniquely attained. We have denoted Dthe set of discontinuous
points ofg(µ)in Algorithm 3: if a discontinuous point is encountered, the algorithm returns µsuch that
g(µ+ϵµ)<0<g(µ−ϵµ). Note that the accuracy ϵdoes not need to be high in practice, say 0.001, since
there will not be a significant difference between a vector of sparsity sand sparsity s±0.001.
Finally, we also provide a fast vectorized GPU-compatible implementation of the Algorithm 1 in PyTorch
along with the supplementary materials, where we parallelly project all the vectors together for application in
deep neural networks. Since, the Newton’s method converges quickly (see Table 3), empirically we observed a
fast execution of the the projection of deep networks.
A.4 Computational Cost
As detailed in Section 3.4, the computational cost of the algorithm is O(tN), whereN=/summationtextr
i=1niandtis
the number of iterations in Newton’s method. The computational cost per iteration is therefore linear in
the size of the problem. In practice, we observed that Netwon’s method converge very fast and does not
require much iterations, as showcased by the following synthetic data example. Let us take ni= 1000for all
i∈J1,100Kand generate each entry of the ci’s using the normal distribution N(0,1). For each sparsity level,
we generate 100 such data points and Table 3 reports the average and maximum number of iterations needed
by Algorithm 1. In all cases, it requires less than 4 iterations for a target accuracy of ϵ= 10−4.
s0s= 0.7s= 0.8s= 0.9s= 0.95s= 0.99
Average 20.86% 3.88 3.78 3.98 3.75 3.77
Maximum 21.01% 4 4 4 4 4
Table 3: Average and maximum number of iteration for Algorithm 1 to perform grouped sparse projection of
100 randomly generated vectors of length 1000, with precision ϵ= 10−4. The column s0gives the average
and maximum initial sparsity of the 100 randomly generated vectors xi’s.
B Grouped Sparse Projection
In this section we detail the calculations in the proof of Corollary 3.3.
Proof.Ifµ= 0andµ>˜µ, the projection is unique provided the largest and second entry of each |ci|are not
equal (see Section 3.3). For 0<µ< ˜µ, since the optimal µ∗is unique it suffices to prove xi[µ]is a one-to-one
18Published in Transactions on Machine Learning Research (10/2022)
mapping, i.e., that if xi[µ1]=xi[µ2]thenµ1=µ2. As noted in Section 3.2, when |ci|−µβi1≤0, the solution
xi[µ]is unique — corresponding to the largest entry of |ci|— as long as the largest entry is unique.
We now analyse the last scenario, in which |ci|−µβi1>0(with 0<µ< ˜µ). For simplicity of notation, we
assume all indexes jof the vector|ci|are active, i.e.,|ci(j)|−µβi>0∀j. The derivations would follow in an
analogous way in the case of some indexes being zero-d out, with sums and norms only including the active
indexes. Let’s consider µ1,µ2such that 0< µ 1,µ2<˜µ, and consider the j-th index of xi[µ1]andxi[µ2]
respectively.
xi[µ1](j) =xi[µ2](j)
=⇒|ci(j)|−µ1βi
∥|ci|−µ1βi∥2=|ci(j)|−µ2βi
∥|ci|−µ2βi∥2
=⇒(|ci(j)|−µ1βi)2−(|ci|−µ2βi)T(|ci|−µ2βi) = (|ci(j)|−µ2βi)2(|ci|−µ1βi)T(|ci|−µ1β1)
=⇒[ci(j)2+µ2
1β2
i−2|ci(j)|µ1βi]/parenleftbig
∥ci∥2
2+µ2
2β2
in−2µ2βi|ci|Te/parenrightbig
=
[ci(j)2+µ2
2β2
i−2|ci(j)|µ2βi]/parenleftbig
∥ci∥2
2+µ2
1β2
in−2µ1βi|ci|Te/parenrightbig
=⇒ci(j)2β2
in(µ2
2−µ2
1) + 2ci(j)2βi|ci|Te(µ1−µ2) +β2
i∥ci∥2
2(µ2
1−µ2
2) + 2µ1µ2β3
i|ci|Te(µ2−µ1)
+ 2|ci(j)|∥ci∥2
2βi(µ2−µ1) + 2µ1µ2|ci(j)|β3
in(µ1−µ2) = 0
=⇒(µ2
2−µ2
1)β2
i/bracketleftbig
ci(j)2n−∥ci∥2
2/bracketrightbig
(12)
+ 2βi(µ1−µ2)/bracketleftbig
ci(j)2∥ci∥1−µ1µ2β2
i∥ci∥1−|ci(j)|∥ci∥2
2+µ1µ2|ci(j)|β2
in/bracketrightbig
= 0
=⇒(µ2
2−µ2
1)β2
i/bracketleftbig
ci(j)2n−∥ci∥2
2/bracketrightbig
+ 2µ1µ2β3
i(µ1−µ2) [|ci(j)|n−∥ci∥1] (13)
+ 2βi(µ1−µ2)/bracketleftbig
ci(j)2∥ci∥1−|ci(j)|∥c∥2
2/bracketrightbig
= 0
=⇒(µ1−µ2)
(µ1+µ2)β2
i/bracketleftbig
∥ci∥2
2−ci(j)2n/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(A)+2µ1µ2β3
i[|ci(j)|n−∥ci∥1]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(B)+2βi/bracketleftbig
ci(j)2∥ci∥1−|ci(j)|∥c∥2
2/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(C)
= 0.
For the equation above to be equal to zero, given µ1,µ2,βi>0, either the terms (A) = (B) = (C) = 0
orµ1=µ2. Note that if (A)and(B)are equal to 0then (C) = 0, hence we just need to check when
(A) = (B) = 0. We have that:
/braceleftigg
(A) = 0
(B) = 0=⇒/braceleftigg
∥ci∥2
2−ci(j)2n= 0
|ci(j)|n−∥ci∥1= 0=⇒/braceleftigg
ci(j) =∥ci∥2√n
|ci(j)|=∥ci∥1
n.
This implies the vector cineeds to have all entries equal to the same value in every active index j. As by
assumption this cannot be case — the largest entry of cineeds to be unique and distinct — we have that
µ1=µ2and we have proved that [µ]is a one-to-one mapping.
C Weighted Grouped Sparse Projection
C.1 Weighted Sparsity
In section 4 we introduced the notion of weighted sparsity : For a vector x∈Rn
0and givenw∈Rn
0,+, as
following
spw(x) =∥w∥2−∥Wx∥1
∥x∥2
∥w∥2−miniw(i)∈[0,1],whereW= diag(w)1. (14)
Let us make some observations about this quantity
19Published in Transactions on Machine Learning Research (10/2022)
•We have
∥w∥2= max
∥y∥2≤1∥Wy∥1and min
iw(i) = min
∥y∥2=1∥Wy∥1,
which implies the fact that spw(x)∈[0,1]for anyx.
•Forw=1, we have∥W(·)∥1=∥(·)∥1,∥w∥2=√nandminiw(i) = 1so that sp(.) = sp1(.).
•spw(x) = 1if and only if xis 1-sparse and its non-zero entry corresponds to (one of) the smallest
entry ofw. Therefore, a vector xcan be 1-sparse but have a low weighted sparsity spw(x)if the
corresponding entry of wis large.
•∥Wx∥1is a norm if and only if w>0, which we do not require in this paper but use this notation
for simplicity.
This notion of weighted sparsity allows to give more or less importance to the entries of xto measure its
sparsity. For example, having w(j) = 0means that there is no need for x(j)to be sparse (that is, close or
equal to zero) as it is not taken into account in spw(x), while taking w(j)large will enforce x(j)to be (close
to) zero if spw(x)is large (that is, close to one). In the remainder of this paper, we will say that a vector xis
w-sparse if it has a large weighted sparsity spw(x). The following example provides a potential application of
projecting a set of vectors onto w-sparse vectors.
C.2 Weighted Group Sparse Projection
We defined the w-sparse projection problem of the set of vectors {ci∈Rni}r
i=1in(WGSP) , where{wi∈
Rni
0,+}r
i=1was the nonzero nonnegative weight vectors associated with the ci’s with a target average weighted
sparsitysw∈[0,1].
As for 5, the maximum of (WGSP) is attained. Similarly, as for (5), we can derive formula for xidepending
on the Lagrange variable µassociated with the inequality constraint:
1. For|ci|−µβw
iwi≰0, we have
xi[µ] =/bracketleftbig
|ci|−µβw
iwi/bracketrightbig
+/vextenddouble/vextenddouble/vextenddouble/bracketleftbig
|ci|−µβw
iwi/bracketrightbig
+/vextenddouble/vextenddouble/vextenddouble
2.
2.For|ci|−µβw
iwi≤0,xi[µ]is 1-sparse. The nonzero entry of xi[µ]corresponds to the largest entry
of|ci|−µβw
iwi. In this case, since the entries of wimight be distinct, this non-zero entry does not
necessarily correspond to the largest entry of |ci|and may change as µincreases. In particular, for µ
large enough, the nonzero entry of xi[µ]will correspond to the smallest entry of wi.
We need/summationtextr
i=1βw
iwT
ixi[µ]to be smaller than kw
s, that is,
gw(µ) =r/summationdisplay
i=1βw
iwT
ixi[µ]−kw
s≤0.
Ifgw(0)≤0, thenxi[0]=|ci|
∥ci∥2is optimal and the problem is solved (the ci’s have average w-sparsity larger
thansw). Otherwise, the constraint will be active and we need to find a root µ∗ofgw(µ). Unfortunately,
as opposed to GSP, the function gw(µ)is not necessarily strictly decreasing for µsufficiently small: when
|ci|−µβw
iwi≤0and asµincreases, the w-sparsity of cimay change abruptly as the index of the maximum
entry of|ci|−µβw
iwimay change as µincreases. In that case, the term corresponding to ciingw(µ)is
piece-wise constant. For example, let ci= [4,1]andwi= [2,1], we have
•Forβw
iµ∈[1,2),spw(xi[µ]) = spw([1,0]) =√
5−2√
5−1= 0.19.
•Forβw
iµ≥2,spw(xi[µ]) = spw([0,1]) = 1.
However, when|ci|−µβw
iwi≰0for somei,gw(µ)is strictly decreasing.
20Published in Transactions on Machine Learning Research (10/2022)
Figure 3: Basis elements obtained with sparse NMF (left) and weighted sparse NMF (right).
C.2.1 Example Application on Facial Images
Let us consider the case when the xi’s represent a set of basis vectors for (vectorized) facial images. In this
case, one may want these basis vectors to be sparser on the edges since edges are less likely to contain facial
features; see, e.g., Ho (2008). Let us illustrate this on the ORL face data set (400 facial images, each 112 by
92 pixels). Each column of the input data matrix Yrepresents a vectorized facial image, and is approximated
by the product of XHwhere XandHare nonnegative. Each column of Wis a basis elements for the facial
images. We first apply sparse NMF with average sparsity of 60% for the columns of basis matrix X. The
basis elements are displayed on the left of Figure 3.
Given a basis image 112 by 92 pixels, we define the weight of the pixel at position (i,j)ase∥(i,j)−(56.5,46.5)∥2/σ
withσ= 5as in Chapter 6 of Ho (2008). The further away from the middle of the image, the more weights
we assign to the pixel so that the basis images are expected to be sparser on the edges. According to these
weights, the average weighted sparsity of the sparse NMF solution is 89% (in fact, we notice that most basis
images are already relatively sparse on the edges). Then, we run weighted sparse NMF (WSNMF) with
average weighted sparsity 95%. The basis elements are displayed on the right of Figure 3. We observe that,
as expected, the edges are in average even sparser compared to the unweighted case (note that the only
WSNMF basis element that is not sparse on the edges, the third image, has darker pixels in the middle of
the image to compensate for the relatively lighter pixels on the edges). This is confirmed by Figure 4 which
displays the average of the columns of the squared error for both solutions, that is, it displays the average of
the columns of the squared residual (Y−XH)2
ij. The residual of WSNMF is brighter on the middle of the
image (since the basis elements are less constrained to be sparse in this area) and darker at the corners. In
fact, most of the error of WSNMF is concentrated in the four corners.
Observe also that the basis elements of WSNMF are denser: in fact, the (unweighted) sparsity of WSNMF is
only 50%. The relative errors, that is,∥Y−XH∥F
∥Y∥F, for both approaches are similar, 20.34% for sparse NMF
and 20.77% for WSNMF.
21Published in Transactions on Machine Learning Research (10/2022)
Figure 4: Average squared error obtained with sparse NMF (left) and WSNMF (right) –the darker, the higher
the error.
C.3 Theoretical Results
We now provide theoretical proofs for the solutions of the problem 4, as well as proof of Theorem 3.2 for
GSP. Lemma C.1 provides a proof for the solution of the dual problem for a general weighted sum of x[µ],
from which both results follow.
Lemma C.1. Letw∈Rn
0,+andx∈Rn
+. Let alsofx(γ) =wTx[γ]where
•Ifx−γw≰0, that is, if γ <˜γ= maxjx(j)
w(j), then
¯x[γ] =[x−γw]+
∥[x−γw]+∥2.
•Otherwise x[γ]is a 1-sparse vector, with its nonzero entry equal to one and at position j∈
argmaxjx(j)−γw(j).
For0≤γ <˜γ,fx(γ)is strictly decreasing, unless xis a multiple of win which case it is constant. For γ≥˜γ,
fx(γ)is nonincreasing and piece-wise constant.
Proof.The caseγ≥˜γis straightforward since fx(γ) =wjforj∈argmaxj(x(j)−γw(j)): asγincreases,
the selected wjcan only decrease since w≥0.
Let us now consider the case 0≤γ <˜γ. Clearly,fx(γ)is continuous since it is a linear function of x[γ]which
is continuous, and it is differentiable everywhere except for γ=x(j)
w(j)for somej. Therefore, it suffices to show
thatf′(γ)is negative for all γ̸=x(j)
w(j). Note that f(γ)is strictly decreasing if and only if c1f(γc2)is strictly
decreasing for any constants c1,c2>0. Therefore, we may assume without loss of generality (w.l.o.g.) that
||w||2= 1(replacingwbyw/||w||2). We may also assume w.l.o.g. that x > γwotherwise we restrict the
problem to x(J)whereJ(γ) ={j|x(j)−γw(j)>0}sincefdepends only on the indices in Jin the case
γ <˜γ. Under these assumptions, we have
f(γ) =wT(x−γw)
||x−γw||2=wTx−γ
||x−γw||2,
since||w||2
2=wTw= 1, and
f′(γ) =−||x−γw||2+ (wTx−γ)wT(x−γw)||x−γw||−1
2
||x−γw||2
2
=−||x−γw||2
2+ (wTx−γ)2
||x−γw||3
2.
22Published in Transactions on Machine Learning Research (10/2022)
It remains to show that ||x−γw||2
2≥(wTx−γ)2implyingf′(γ)<0. We have
||x−γw||2
2=||x||2
2−2γwTx+γ2,
and
(wTx−γ)2= (wTx)2−2γwTx+γ2,
which gives the result since |wTx|<||w||2||x||2=||x||2forxnot a multiple of w.
Proof of Theorem 1. The proof follows from Lemma C.1 by setting w=1and noting that g(µ) =/summationtextr
i=1f|ci|(βiµ)−ks. Note that since w(j) = 1∀j, the index of the maximum element remains the same.
Corollary C.2. The function gw(µ) =/summationtextr
i=1βw
iwT
ixi[µ]−kw
sas defined above is nonincreasing. Moreover,
if|ci|−µβw
iwi≰0for somei, it is strictly decreasing.
Proof.This follows from Lemma C.1 since gw(µ) =/summationtextr
i=1f|ci|(βw
iµ)−kw
s.
Therefore, as opposed to g(µ),gw(µ)could have an infinite number of roots µ∗. However, the corresponding
xi(µ∗)is unique so that non-uniqueness of µ∗is irrelevant. Moreover, this situation is rather unlikely to
happen in practice since it requires |ci|−µβw
iwi≤0for alliat the root of gw(µ)hence it requires swto
be close to one (that is, kw
sto be large). Similarly as for g,gwwill be discontinuous at the points where
maxjc(j)
βiwi(j)is not uniquely attained: as µincreases, the two (or more) last non-zero entries of xi[µ]become
zero simultaneously. Finally, to solve WGSP, we can essentially use the same algorithm as for GSP, that is,
we can easily adapt Algorithm 1 to find a root of gw(µ).
D Sparse NMF: Experiment Details
In this section, we provide experiment parameters and details for the sparse NMF experiments.
D.1 Synthetic data sets
For the experiment on synthetic data, we take m=n= 100andr= 10. We generate each entry of Xusing
the normal distribution with mean 0 and variance 1 and then set the negative entries to zero, so that Xwill
have sparsity around 50%. We generate each entry of Husing the normal distribution in the interval [0,1].
To run gspNMF and cgspNMF, we compute the average sparsity of the columns of the true Xand use it as
an input. We generate 50 such matrices, and use the same initial matrices for all algorithms, which were
generated using the uniform distribution for each entry. In Figure 1 of the paper, we reported the evolution
of the average of the relative error obtained by the different algorithms among the 50 randomly generated
matrices.
D.2 Image data set
For this set of experiments we test on the widely used data set in the NMF literature, the CBCL facial images.
This dataset consists 2429 images of 19×19pixels, and was used in the seminal paper by Lee & Seung
(1999) with r= 49. We run the sparse NMF techniques with sparsity 85% Using 10 random initializations,
Figure 1 (in paper) reports the evolution of the average relative error, and Figure 5 displays the basis elements
obtained by different methods.
E Deep Network Pruning with GSP: Experiments, settings and hyperparameters
E.1 Experiments on CIFAR-10
We use the CIFAR-10 dataset (Krizhevsky et al., 2009) to train and test the VGG16, Resnet-56 and Resnet-110
models for our experiments. The CIFAR-10 dataset was accessed through the dataset API of the torchvision
23Published in Transactions on Machine Learning Research (10/2022)
Figure 5: Basis elements obtained with NeNMF (top left), PSNMF with sparsity 0.85 (top right), ℓ1A-HALS
with sparsity 0.85 (bottom left), cPSNF with sparsity 0.85 (bottom right).
package of PyTorch. We performed the standard preprocessing on the data which included horizontal flip,
random crop and normalization on the training set. With the CIFAR-10 dataset we perform two different
types of experiments. First, we perform an experiment with layerwise induced GSP integrated with the
training phase. We also perform single shot pruning with a single projection of the model weights followed by
the finetuning phase in section 5.3.
For the experiments with intermittent projections during the training phase, we project the weights of the
VGG16 model using GSP with sparsity level s, perform a forward pass on the projected weights and finally
update the model parameters using backpropagation every 150iterations for 200epochs, starting from epoch
40. We reduce the learning rate by a factor of 0.1at the milestone epochs of 80,120and160. Next, we set the
sfraction of the lowest parameters of the model to zero. At this point the model is sparse with a layerwise
hoyer sparsity ofs. However, since we project intermittently and with hoyer−sparsity being a differentiable
approximation to the ℓ0norm, we then prune the surviving weights that are close to zero or zero, keeping
the largest 1−˜sfraction of the parameters, where ˜sis the final sparsity of the model. We fix these pruned
parameters (do not train them) with a mask. Finally, we finetune the surviving parameters for 200 epochs
with a learning rate of 0.01and dropping the rate by 0.1in the same milestones as the sparsity inducing run.
In the case of single shot GSP, we take a pretrained model, make a single projection of the layers with a
sparsitysand then prune and finetune the model with similar parameters as above.
24Published in Transactions on Machine Learning Research (10/2022)
E.2 Experiments on ImageNet
In these set of experiments, we use the ImageNet dataset (Russakovsky et al., 2015) to train and test the
Resnet-50 model for our experiments. ImageNet is augmented by normalizing per channel, selecting a patch
with a random aspect ratio between 3/4and4/3and a random scale between 8%to100%, cropping to
224x224, and randomly flipping horizontally.
For the experiments with induced-GSP, we project the weights of the ResNet50 model in a similar technique
to the experiments performed with the CIFAR-10 dataset. We first project the layers of the model using GSP
withs= 0.80, perform a forward pass on the projected weights and finally update the model parameters
using backpropagation every 500iterations for 120epochs. We start the projection of the model from epoch
40and keep projecting every 500iterations till the final epoch. In both the cases of CIFAR-10 and ImageNet
we choose the iteration interval of projection in such a way so that there are 3 projections per epoch. We
reduce the learning rate by a factor of 0.1at the milestone epochs of 70and100. Next, we set the sfraction
of the lowest parameters of the model to zero. We next prune the surviving weights that are close to zero or
zero, keeping the largest 1−˜sfraction of the parameters, where ˜sis the final sparsity of the model. Finally,
we finetune the surviving parameters for 140 epochs with a learning rate of 0.001and dropping the rate by
0.1in the same milestones as the inducing-GSP run.
25