UnderreviewassubmissiontoTMLRDirectNeuralNetworkTrainingonSecurelyEncodedDatasetsAnonymousauthorsPaperunderdouble-blindreviewAbstractInﬁeldswheredataprivacyandsecrecyarecritical,suchashealthcareandbusinessintel-ligence,securityconcernshavereducedtheavailabilityofdataforneuralnetworktraining.Arecentlydevelopedtechniquesecurelyencodestraining,test,andinferenceexampleswithanaggregatenon-orthogonalandnonlineartransformationthatconsistsofstepsofrandompadding,randomperturbation,andrandomorthogonalmatrixtransformation,enablingartiﬁcialneuralnetwork(ANN)trainingandinferencedirectlyonencodeddatasets.Here,theperformancecharacteristicsandprivacyaspectsofthemethodarepresented.Theindividualtransformationsofthemethod,whenappliedalone,donotsigniﬁcantlyreducevalidationaccuracywithfully-connectedANNs.Trainingondatasetstransformedbysequentialpadding,perturbation,andorthogonaltransformationresultsinslightlylowervalidationaccuraciesthanthoseseenwithunmodiﬁedcontroldatasets,withnodiﬀerenceintrainingtimeseenbetweentransformedandcontroldatasets.Thepresentedmethodshaveimplicationsformachinelearninginﬁeldsrequiringdatasecurity.Keywords:securemachinelearning,datasecurity,encodedtrainingdata,encodedtestdata,non-orthogonaltransformation,nonlineartransformation,artiﬁcialneuralnetworks.1IntroductionPrivacyanddatasecurityareparamountinareassuchashealthcareandbusinessintelligence.Inhealthcare,asanillustrativeexample,complexregulatorystructuresexistindiﬀerentjurisdictionsthatweredesignedtoprotecttheprivacyofdatathatmightidentifyanindividualandtheirpersonalinformation(Theodos&Sittig,2020;Phillips,2018).Thedatasecurityrequirementsintheseareashavecreateddemandfornewtechnologiesthatcansecurelyencodedataformachinelearningtrainingandinferenceusingcloudresources.Massiveamountsofdataremainsiloedinhospitalandothersystemsbecauseofprivacy,datasecurity,andregulatoryconcerns(Kostkovaetal.,2016),andthesedataremaininaccessibletothecloudresourcesandtechnicalexpertiseneededformodernneuralnetworktraining.Theexistingbarrierstocloud-basedmachinelearningandcollaborationusingthiswealthofhealthcaredataareparticularlyimportantinlightofrecentdramaticresultsthathavebeenobtainedbytrainingneuralnetworksonthelimitednumberofdatasetsthathavebeenavailabletodate.Forexample,deepnetworktrainingonretinalphotographsfromdiabeticscreeningprogramsyieldedanaccuracyforthedetectionofdiabeticretinopathythatisonparwithboard-certiﬁedophthalmologists(Gulshanetal.,2016).Inaddition,useofthesameretinalimagingdatasetsfacilitatedtrainingofneuralnetworkstomakearangeofsurprisinglyaccurateinferencesrelatedtobloodpressure,smokinghistory,age,andgender(Poplinetal.,2018).Thesamegroupmorerecentlytrainedneuralnetworksonexternaleyephotographs,andthesenetworkscouldaccuratelypredictlevelsofthelaboratorymeasureshemoglobinA1C(ameasureofdiabetesmellitusanditscontrol)andserumcholesterol(Babenkoetal.,2022),neitherofwhichwerepreviouslyknowntobeassociatedwithanyfeaturesvisibleinexternalphotographsoftheeye.1UnderreviewassubmissiontoTMLRThereshouldbeanimportantroleforneuralnetworkstodramaticallyenhanceclinicaldiagnosis(Poplinetal.,2018;Babenkoetal.,2022),butmachinelearningtechniqueshaveyettobewidelyadoptedacrosshealthcare.Whiletheslowpaceofadoptionmaybeinpartrelatedtoaslow-to-adaptcultureinhealthcareandacomplexregulatoryenvironment,datasiloinghascertainlylimitedtheavailabilityofdataandcreatedbarrierstotransmittingprivatedatatocloudresourcesfortrainingandinference.Solutionsarethereforeneededtosecurelyencodedatainhealthcareandotherﬁeldssuchthatencodeddatamaybedirectlyusedinmachinelearningapplications.Anidealencodingmethodtoestablishsecurityformachinelearningsystemswouldsecurelyencodetraining,test,andinferencedatainafashionthatallowsexistingarchitecturestobeusedontheencodeddata,withminimallossofaccuracyandcomparabletrainingtimes.Suchanimplementationwouldalsorequiretestandinferenceexamplestobesimilarlyencodedtotrainingexamples,whichwouldhavethebeneﬁtofmaintainingthebusinessvalueoftrainingdataincollaborations.Here,theperformanceofanovelmethodofsecuredataencodingfordirectusewithneuralnetworks(Anonymized,2022)isdescribedandtested.Themethodinvolvesacombinationofdiﬀerentformsofpadding,perturbation,andorthogonaltransformation(e.g.,ﬁxedshuﬄing)tocreateanaggregatenon-orthogonalandnonlineartransformationthatisappliedtotraining,test,andinferencedata,withneuralnetworkstraineddirectlyontheencodeddata.Weﬁndthattrainingafully-connectedANNondataencodedbythisaggregatetransformationachievesalevelofvalidationaccuracythatissimilartoresultsobtainedwhentrainingwithanunencodeddataset.2ThreatmodelTheencodingtransformationsdescribedindetailbelowaredesignedtoprotectdataprivacyinthecontextofaspeciﬁcthreatmodel,asshowninFigure1.
Figure1:Threatmodel.A.Trainingstage.PartyA(thedataowner)possessestheoriginaldataset(Xd)andsetoflabels(Yd).PartyAgeneratesanencodeddataset(Zd),whichistransmittedalongwithYdtoPartyB(ANNtraining).PartyC(attacker)interceptstransmissionofZdandYd.B.Inferencephase.PartyC(attacker)submitsanoriginal(unencoded)example(x′)thatisnotamemberoftheoriginaldataset(Xd)fromtraining.PartyA(dataowner)receives(x′)andprivatelyencodesitto(z′)withthesamealgorithmusedduringtraining.Inferenceperformedon(z′)usingthetrainedneuralnetworkyieldslabel(y′),whichisreturnedtoPartyC.Onlytheinferenceresultlabel(y′)istransmittedbacktoPartyC,andafterinferenceisperformed,PartyAdestroystheencodedexample(z′).2UnderreviewassubmissiontoTMLRAsillustratedinFigure1A,duringtraining,adataowner(PartyA)possessesanoriginaldataset(Xd)withnexamplesandacorrespondingsetofnlabels(Yd).PartyAappliestherandomfunctionsdescribedbelowtogenerateanencodeddataset(Zd)withthesamennumberofexamplesandanunchangedsetofnlabels(Yd).PartyAtransmitsZdandYdtoPartyBforperformanceofANNtraining,andPartyBusesZdandYdtotrainaneuralnetwork(Figure1A).Atthisstage,weassumethatPartyCisabletointerceptXdandYdduringtransmission(although,inpractice,thisconﬁgurationwouldallowPartyAandPartyBtoencryptZdandYdfortransmissionbystandard2-keyencryptionmethods).AnalternativescenariowithinthethreatmodelisthatPartyCmightbeaﬃliatedwithPartyBandthusabletoobtainZdandYdwithoutaneedtointerceptthedataduringtransmission.Ineitherscenario,PartyCwillhaveaccesstotheneuralnetworktrainedusingZdandYd,eitherbecausetheyareaﬃliatedwithPartyBandhaveaccesstothenetworktrainedbyPartyB,orbecausetheyinterceptedZdandYdandproceededtotraintheirownneuralnetworkusingtheencodeddata(Zd)andassociatedlabels(Yd).AsillustratedinFigure1B,duringinference,anattacker(PartyC)maypresent,asauserofsoftwaredesignedtoperforminference,anoriginaldataexample(x′)toPartyA.PartyAthenprivatelyencodesthisexample(x′)usingthesameencodingprocessusedfortrainingtoyieldanencodedexample(z′).BecausePartyCdoesnothaveaccesstooriginalexamplesinthetrainingdataset(Xd),thisoriginalexample(x′)presentedbyPartyCisnotamemberofXdandmustbegeneratedforthispurposebyPartyCwithoutknowledgeoftheoriginalexamplesinXd.InPartyA’spossession,theencodedexample(z′)ispresentedtothetrainedneuralnetwork(obtainedfromPartyBafterthetrainingprocess)toyieldalabelcategoryprediction(y′).PartyAreturnsthelabelcategoryprediction(y′)totheuser(inthisscenario,theattacker,PartyC),withoutdisclosingtheencodedexample(z′),andPartyAdestroystheencodedexample(z′)afterinferenceisperformed.Fromthesetwoscenarios,asillustratedinFigure1,weobtainthethreatmodel,inwhichananattacker(PartyC)cancometopossessthefollowing:•Anencodedtrainingdataset(Zd),withnencodedexamples,andncorrespondinglabels(Yd).•Atrainedneuralnetwork,obtainedbytrainingonZdandYd.•Asetoforiginal(unencoded)examples(x′a,x′b,···)NOTpresentintheoriginalset(Xd),withassociatedlabels(y′a,y′b,···)obtainedbyinferencefromthetrainedneuralnetwork.•ForthescenarioinwhichPartyCispartofPartyB,PartyCmayhavegeneralknowledgeofthetypeofdatasharingprojectbetweenPartyAandPartyB.Forthelastsupposition,theknowledgeofthetypeofdatasharingprojectcouldimplysomeunderstandingoftheformoforiginalexamples,butitwouldnotincludeactualtrainingortestexamplesinXd.Knowledgeoftheformofexamplesinagivenprojectmightrepresentlimitedknowledgeofwhattypeofimagesarebeingusedfortheproject(theattackermightknow,forexample,thataprojectconcernscomputedtomographic(CT)imagesoftheabdomen).Insuchascenario,theattackermightknowthatallunencodedexamplesareexpectedtohaveasurroundingareaofblackornear-blackpixelscorrespondingtothedensityofaironaCT,buttheywouldnothavedataregardingtheunencodeddatathemselvesandwouldnothaveknowledgeoftheshapeofthosedata,orthelengthofunencodedﬂattenedoriginalvectorsinXd,priortotheencodingprocess.3DescriptionoftheencodingmethodsThecentralmethodofthesecureencodingapproachpresentedhereisarandomnon-orthogonalandnonlineartransformationcomprisedofoneormorerandomnon-orthogonalandnonlineartransformationstogetherwithoneormorerandomorthogonaltransformations.Atleastoneoftheincludedrandomfunctionsappliedtothedataisarandomorthogonaltransformation,whichcanusearandomorthogonalmatrixofappropriatesize,orcanapplyanexample-wiseﬁxedshuﬄing(indexshuﬄing)transformation,whichrepresentsasubsetofthebroadersetofrandomorthogonalmatrixtransformations.ForatensorofrankN,sequentialtransformations3UnderreviewassubmissiontoTMLRareappliedtotheinputtensor,includingatleastonestepthatshuﬄeseachoftheexamplesinaﬁxedfashionusingastoredshuﬄedindexarray(oratleastonestepthatdotsarandomorthogonalmatrixofappropriatesizewiththeinputvectors).Alltensorsinagivenproject’strainingandtestsets,aswellasanyinferenceexamples,aretransformedinsimilarfashion.Toillustratethemethod,threediﬀerentformsofmanipulation,whichmaybeappliedinvaryingcombi-nations,aredescribedbelow:padding,perturbation,andrandomorthogonaltransformation.Whileeachofthesetransformationsmaybeappliedtotensorsofarbitraryrankwithanydatatypesuitableforanytypeofmachinelearningoperation,inthispaper,methodillustrationsandperformanceexperimentswillusesupervisedtrainingoffully-connectedartiﬁcialneuralnetworks(ANNs)ontheMNISTdatasetofgrayscale(n,28,28)handwrittendigits(Lecunetal.,1998)asanexemplar,withconﬁrmatoryanalysesperformedonFashion-MNIST(Xiaoetal.,2017)andCIFAR-10(Krizhevsky).3.1PaddingPaddingofadatasetinvolvesgeneratingarandompadofdatasuchthattheexamplesofthesourcetensorareincreasedinsize(anincreaseintheﬂattenedvectordimensionsoftheexamples),withresultingexamplesofuniformshapeacrosstheresultingtensor.Therandompaddingmayrepresentoneormorerowsofdata,oroneormorecolumnsofdata,oracombinationofoneormorerowsandcolumns(asshownintheL-shapedpadinFigure2B),orbeanadditionofanynumberofpixelswithrandomvaluesinvariouslocationsinsideoroutsideoftheexampleimages.Diﬀerentformsofpaddingmaybeusedthatvaryinthewaytheyareappliedtotheinputtensor.Threevariationsonrandompaddingwillbedescribedhereandperformance-testedbelow:ﬁxedpadding,randomnon-ﬁxedpadding,andadjustednon-ﬁxedpadding.3.1.1FixedpaddingWithﬁxedpadding,asinglepadisrandomlygeneratedforasourceexample,typicallybyrandomchoiceforeachpixel,withintherangeofpixelvaluesofthesourcetensor.Thissamepadisappendedinidenticalfashiontoeachexampleacrossthenexamplesofthesourcetensor,withthesamepadusedforallexamplesofthetrainingtensor,thetesttensor,andanyinferenceexamples.3.1.2Non-ﬁxedpaddingNon-ﬁxedpaddingisanyrandompaddingapproachthatvariesfromexampletoexample.Inoneversionofnon-ﬁxedpadding,randomnon-ﬁxedpadding,acompletelydiﬀerentrandompadisgeneratedforeachexampleacrossthetraining,test,andinferencetensors.Inanotherversionofnon-ﬁxedpadding,adjustedpaddingstartswithaﬁxedpadofappropriateshapethatisstoredinmemory.Thisstoredpadinmemoryisthenadjusted,pixelbypixel,byarandomamountinaspeciﬁedrange(byananalogousprocesstothatappliedintheperturbationtransformationdiscussedbelow),anddiﬀerentlyadjustedpadsarethenappliedtoeachexampleacrossthetraining,test,andinferencetensors.Otherformsofnon-ﬁxedpadding,notillustratedhere,includemethodssuchasgeneratingagroupofrandompadsthatissmallerthanthenumberofexamplesinaninputtensorandthenusingrandomchoicefromthispoolofrandompadstopadtheexamplesinatensor.3.2PerturbationPerturbationofadatasetinvolvesapplyingasetofrandomfunctionsthatperturbthepixelvalueateachpositionintheexamplesofaninputtensor.Inthepresentexperiments,theversionofthisperturbationshownisanalterationofthepixelvaluesbydiﬀerentpercentagesofthepixelvaluerange.First,anarrayofrandomvaluesfromachosenminimumtoachosenmaximum(e.g.,min=-5%tomax=+5%)isgenerated,wherethearraysizeismatchedtothesizeofanexampleintheinputtensor.(Intheexperimentsshowninthispaper,sourcedataaretransformedfrom8-bitdepth[range0-255]to32-bitﬂoating-pointprecision[range0.0to1.0]toenablegradedchangesbetweentheoriginal8-bitvalues.)Foreachexampleintheinputtensor,thepixelvalueatagivenlocationisperturbedbytherandomlychosenpercentageamountatthecorrespondingpositionintheperturbationarray.4UnderreviewassubmissiontoTMLR
Figure2:Randompadding,perturbation,andorthogonaltransformationperformedseriallypriortotrainingofanartiﬁcialneuralnetwork(ANN).A.Illustrationofanoriginaldataset(Xd),a(n,28,28)tensor,theMNISThandwrittendigitsdatasetwithnexamples,withanillustratedexampleshowingahandwrittendigitcorrespondingtothenumber8,withthelabelcategoryof8intheassociatedlabelset(Yd).B.Applicationofarandompaddingtransformation.Inthiscase,anL-shapedpadofrandomlychosenpixelsisgeneratedwith2rowsand2columns.Thiscanbeﬁxedpaddingacrosstheexamplesofthetensor(onerandomlygeneratedpadappendedinidenticalfashiontoeveryexample),oritcanvaryacrosstheexamples(non-ﬁxedpadding:adiﬀerentpadappendedtoeachoftheexamples).Paddingmaybeanyadditionofrandomxrowsorycolumnsoranyadditionofrandomextrinsicpixelsoutsideorinsideoftheexamples,aslongasthetransformedresulthasaconsistentshape.Theresultinthepresentexampleisatransformedtensorofshape(n,30,30).C.Applicationofarandomperturbationtransformation.Arandomarrayofperturbationvalues(intheillustratedcase,fromtherangeof-5%to+5%)iscreatedandusedtoalterthepixelsbyapercentageoftheoriginalpixelvaluesoftheexamplesofthe(n,30,30)tensor.Casesinwhichpixelvalueswouldbetakenbeyondtheminimumormaximumpixelvalueatagivenlocationcanbehandledindiﬀerentways-inthiscase,pixelsexceedingtheminimumormaximumvaluesareclippedtotheminimumormaximumvalue.Theeﬀectoftheﬁxedperturbationtransformationcanbeseenintheﬁgureasaﬁneamountoflightgraypixelnoisethatismostnotablewhencomparingthewhitepixelsin[B]tocorrespondingpositionsin[C].Theperturbationtransformationmaybeaﬁxedperturbation(thesamesetofmathematicalperturbationsappliedtoeveryexample)oranon-ﬁxedperturbation(diﬀerentsetsofmathematicalperturbationsappliedtoeachexample).D.Applicationofarandomorthogonaltransformation.Inthiscase,aﬁxedindexshuﬄingisappliedtoshuﬄethepixelindexpositionsofeachexampleinidenticalfashion.Morebroadly,anyrandomorthogonaltransformationmaybeappliedbydottingallexamplesbyarandomorthogonalmatrixofappropriatesize(e.g.fora30x30input,a[(30x30)x(30x30)]=[900x900]randomorthogonalmatrix).E.Therandomfunctionsappliedseriallyfrom[B]through[D]areindependentoftheassociatedlabelsinYdandproduceanaggregaterandomnon-orthogonalandnon-lineartransformation,andtheresultofthistransformationisusedtotrainamultilayerANN.
5UnderreviewassubmissiontoTMLRAspixelvaluesarerandomlyperturbedupwardordownwardbythistechnique,perturbationbeyondtherelevantminimumandmaximumvaluesmustbeexplicitlyhandled.Whilemanyapproachescanbeused,inthispaper,twoapproachesaretested:clippingandreﬂection.Clippingassignstheminimumormaximumvaluetoanypixelvaluesbroughtbeyondthesevaluesbytheperturbationalgorithm.Forexample,ifapixelvalueonaﬂoatscalefrom0.0to1.0startedat0.99andwouldbebroughttoavalueof1.03byaperturbation,clippingchangestheresultingvalueto1.0.Reﬂectionbringsthevaluetoacorrespondingamountwithinthemin/maxboundaries.Forexample,ifapixelvalueonaﬂoatscalefrom0.0to1.0startedat0.99andwouldbebroughtto1.03byaperturbation,theamountexceedingthemaximumvalueis(1.03-1.0=0.03)andtheassignedvalueusingreﬂectionwouldbe(1.0-0.03=0.97).Figures2Cand3Billustratetheeﬀectofapplyinga-5%to+5%boundedpercentageperturbationwithclippingtotheexamplesofaninputtensor.Withverylargeperturbationsettings,itispossibleforreﬂectiontooccurbackandforthbetweenthemaxandminvalues.Throughoutthispaper,thedefaultsettingforhandlingperturbationsbeyondthemaximumandminimumboundswillbeclippingunlessstatedotherwise.Diﬀerentformsofperturbationmaybeappliedthatvaryintheirapplicationtotheinputtensor.Twovariationsonperturbationwillbedescribedhereandperformance-tested:ﬁxedperturbation,inwhichallexamplesaremodiﬁedinsimilarfashion,andnon-ﬁxedperturbation,inwhicheachexampleismodiﬁedinadiﬀerentfashion.3.2.1FixedPerturbationInﬁxedperturbation,asinglerandomperturbationarrayisgenerated,andthesetofpixelvalueperturbationsusingtheperturbationarrayisappliedinidenticalfashiontoeachoftheexamplesintheinputtensor,withthesameﬁxedperturbationtransformationappliedtoalloftheexamplesofthetrainingtensorandthetesttensor,aswellasanyinferenceexamples.3.2.2Non-ﬁxedperturbationInnon-ﬁxedperturbation,adiﬀerentrandomperturbationarrayisgeneratedandappliedtoeachexample.Forexample,adiﬀerentsetofpixelvariationfunctionsmaybegeneratedforeachexamplewithaminimumtomaximumrangeforthedegreeofpotentialperturbation(e.g.min=-5%tomax=+5%perturbationofpixelvalues).Withnon-ﬁxedperturbation,diﬀerentperturbationtransformationarraysareappliedtoeachoftheexamplesofthetrainingtensorandtesttensor,aswellastoanyinferenceexamples.3.3OrthogonalTransformationApplicationofanorthogonalmatrixtransformationofinputvectorshasbeenproposedasamethodtoencodedataforusewithartiﬁcialneuralnetworks(Chen,2019),becauseorthogonalmatrixtransformationmaintainsvectorlengthandanglesbetweenvectorsandthusmachinelearningaccuracyshouldbemaintainedbythisapproach.IntheproposedmethodofChen(2019),asquareimageﬁleofappropriatesizeisusedasakeythatissubjectedtoQRfactorizationtoyieldanorthogonalmatrix.Theorthogonalmatrixthusobtainedisusedtoencodetrainingandtestdata.Inthemethodspresentedhere,theorthogonalmatrixtransformationsappliedareeitherdrawnrandomlyfromtheO(N)Haardistributionorappliedbyﬁxedshuﬄing(indexshuﬄing)ofpixelvaluesineachexampleandusedincombinationwithstepsofrandompaddingandrandomperturbationtoyieldanaggregatenon-orthogonalandnonlineartransformation.Twosubtypesoforthogonaltransformationareusedhereincombinationwiththetransformationsofpaddingandperturbationdescribedabove.Inthebroadestmethod,arandomorthogonalmatrixofappropriatesizeisdrawnrandomlyfromtheO(N)Haardistributionandusedtodotinputvectors.Inaspeciﬁcsubtypeofrandomorthogonalmatrixtransformation,ﬁxedshuﬄing(indexshuﬄing)isappliedtothepixelsofinputvectors.3.3.1RandomorthogonalmatrixtransformationTotransforminputvectorsbyrandomorthogonaltransformation,arandomorthogonalmatrixisgenerated,eitherbyrandomchoiceofanorthogonalmatrixofappropriateshapedrawnfromtheO(N)Haardistribution,6UnderreviewassubmissiontoTMLRor,alternatively,byinitialgenerationofarandommatrixofappropriateshapethatisthensubjectedtoQRfactorizationtoobtainarandomorthogonalmatrixofappropriateshape.Oncearandomorthogonalmatrixisgeneratedbyeithermethod,itisstoredinmemoryandusedtotransforminputvectorsbytakingthevector·matrixdotproductofeachinputvectorandthestoredrandomorthogonalmatrix.Thesamerandomorthogonalmatrixisusedtotransformtraining,test,andinferencedatainthesamefashion.3.3.2FixedshuﬄingFixedshuﬄing,orindexshuﬄing,ofadatasetinvolvesshuﬄingthepixelindexlocationsoftheexamplesofaninputtensorsuchthatthesameshuﬄingrearrangementisappliedinidenticalfashiontoeachexampleinthetensor.Fixedshuﬄingisasubsetofthebroadersetofrandomorthogonalmatrixtransformations.Fixedshuﬄingcanbeaccomplishedbyrandomlygeneratingashuﬄingindexarraythatmatchestheshapeofanexampleintheinputtensor,whichstorestheinstructionsforrearrangingthepixellocationsineachexample.Forexample,ashuﬄingindexarraymightindicatethatthepixelresidingatlocation(0,0)ineachexampleberepositionedtoposition(16,9)ineachexampleintheﬁxedshuﬄedtensor,withacorrespondingrepositioningofeverypixelindexlocationineachexample.Theidenticalresultmay,ofcourse,beobtainedbyﬂatteningtheexamples,usingaﬂatshuﬄingindexarrayofcorrespondinglength,thenreshapingtheresultbacktotheoriginal(x,y)exampleshape,andﬁxedshuﬄingmaybeappliedtoexamplesofanydimensionintensorsofanyrank.Alternatively,ﬁxedshuﬄingmaybeappliedbygeneratingasquareorthogonalmatrixﬁlledwithzeroesexceptforsingleonesinuniquecolumnpositionsacrossthematrixrows,andthismatrixmaybeusedtohavethesameindexshuﬄing(coordinateaxispermutation)eﬀect.Thesameﬁxedshuﬄingtransformationisappliedtotheexamplesofthetrainingtensorandthetesttensor,aswellasanyinferenceexamples.Figure2Dshowstheeﬀectofapplyingaﬁxedshuﬄingtransformationtotheexamplesofaninputtensor,inthiscasetheresultoftherandompaddingtransformationshowninFigure2BfollowedbytherandomperturbationtransformationshowninFigure2C.3.3.3Non-ﬁxedshuﬄingNon-ﬁxedshuﬄing(inwhichadiﬀerentshuﬄeisappliedtoeachexampleofthetrainingandtestdatasets)isoflimitedpracticalutilityinthecontextoftrainingmachinelearningsystemsforinference—separateexample-wiseshuﬄingdisruptsthespatialrelationshipsofthedataelementsacrossexamples,thusrequiringamachinelearningsystemtotrainonnon-spatialdataalone.Thiseﬀectis,however,ofacademicinterest,becauseitallowsforaquantitativeassessmentoftheextenttowhichaparticularmachinelearningsystemusesspatialvs.non-spatialinformationtotrain.Speciﬁcally,trainingonaﬁxedshuﬄeddatasetinvolvesbothspatialinformation(e.g.,thearrangementofpixelsintheoriginaldataset)andnon-spatialinformation(e.g.,thepercentageofdarkerpixelsinsomeexamplescomparedwithothers,whichcaninturnassociatewiththeexamplelabel),whiletrainingonanon-ﬁxedshuﬄeddatasetinvolvesonlynon-spatialinformation.Acomparisonbetweentheresultsofthesetwotreatments(ﬁxedshuﬄingvs.non-ﬁxedshuﬄing)thusallowsforaquantitativeassessmentofhowthesetwoaspectsofagivendataset(spatialvsnon-spatialinformation)contributetonetworktraining.3.4SerialcombinationsoftransformationsAsshowninFigure2,thetransformationsofpadding,perturbation,andorthogonaltransformationmaybeperformedsequentially,invaryingcombinations.(Thesequentialapplicationofmultipletransformationscan,ofcourse,beestablishedasasinglealgorithmictransformationthathasthesameeﬀect).Atleastonerandomorthogonaltransformationstep,asdescribedabove,isused,andtypicallyanorthogonaltransformationstepisdeployedattheﬁnalstageofthechainoftransformations,asillustratedinFigure2.Multipleroundsofrandomfunctionsmaybeapplied,includingmultipleversionsofeachcategoryoftransformation;e.g.,bothrandomorthogonalmatrixtransformationandﬁxedshuﬄingmaybesequentiallyapplied.Theeﬀectofcombiningthenon-orthogonalandnonlineartransformationstepsofpadding,perturbation,oracombinationofthese,istocreateanaggregaterandomnon-orthogonalandnonlineartransformation.7UnderreviewassubmissiontoTMLR4Methods4.1ExperimentalsetupTrainingperformanceoftheencodingstepsofpadding,perturbation,andorthogonaltransformation,aloneandindiﬀerentcombinations,wastestedherebytrainingandvalidationwiththeMNISTdataset(Lecunetal.,1998),withtheusual60Ktrainingand10Kvalidationsplit.Trainingandvalidationwereperformedusingthesamefully-connectedANNstructureandhyperparametersforalltrainedmodels,usingTensorFlow/KerasinPython3.6onGoogleColab+,runningonaNVIDIATeslaT4orP100GPU,dependingontheserverconnection.Alltrainingsessionsfortransformeddatasetsandpairedcontrolswereperformedonthesameserverconnection,usingthesameavailableGPU.Forthetrainingtimecomparisonsdescribedbelow,aserverconnectionrunningaNVIDIATeslaT4GPUwasused.ANNstructurewas(1)aninputlayercorrespondingtotheexampledatashapewithorwithoutchangeinshapeproducedbyencoding,(2)twofully-connectedhiddenlayersof128neuronseachwithReLUactivation,with0.25Dropout,Flatten,and0.5Dropout,and(3)anoutputlayerof10neuronscorrespondingtothe0-9digitlabelsofMNIST,withSoftmaxactivation.Thelossfunctionwascategoricalcrossentropy,andtheoptimizerwasRMSprop,withalearningrateof0.001,rhoof0.9,andepsilonof1e-07.Trainingbatchsizewas128,with12trainingepochsforalltrainingruns.Tofacilitateappropriatecomparisonsofvalidationaccuracyandoftrainingtime,modelparameterswereﬁxedforallexperimentsandnotindividuallyoptimized—theonlydiﬀerencefrommodeltomodelwasanyrequiredincreaseininputshapeproducedbytheencodingprocess(speciﬁcallytheincreaseininputshapeproducedbypadding).Toachieveappropriateprecisionofﬁxedperturbationtransformations,allpixelvaluesforallmodelswereconvertedtoﬂoatingpointprecision.Eachencodingprocessthatwastested,whetherindividualencodingstepsormultipleencodingstepsindiﬀerentcombinations,wasperformedatotalof15separatetimes(seesamplesizecalculationsbelow),withseparatefully-connectedANNmodelstrainedforeachfromscratch.Forexample,totestorthogonaltransformationalone,asshowninFigure3C,anewrandomorthogonalmatrixwasgeneratedtotransformthetrainingandvalidationsplitsofMNIST,andthenthemodelunderwenttraining.Thiswasrepeated14moretimes,eachtimedeletingallmodelvariables,generatinganewrandomorthogonalmatrix,transformingthetensorsdenovo,andtraininganewmodelfromscratch.Duringthesamesession,whileconnectedtothesameGoogleColab+cloudserverwiththesameavailableGPU,controlmodelswereindependentlytrainedusingMNISTwithoutanyencodingsteps.Foreachcomparison,15controlmodelsweretrained,withpresentedcontrolspairedtothesameserversessionusedfortheencodingprocessbeingtested,toensuresimilarconditions(suchasavailableGPU)andtoavoidmultiplecomparisonsinstatisticalanalysis.Additionalanalyseswereperformedwithsimilarnetworkstructuresusingadditionaldatasets(Fashion-MNIST(Xiaoetal.,2017),CIFAR-10(Krizhevsky)),withminormodiﬁcationstonetworkstructureasdescribedintheResults.4.2StatisticsandsamplesizecalculationForcomparisonofvalidationaccuracybetweencontrolsandagivenencodingprocess,thenonparamet-ricKruskal-Wallisequality-of-populationsranktestwasperformed,withPvaluefortheChi-squaretestwithouttiesreportedforeachcomparison(similarresultswereobtainedwiththeparametricStudent’st-test).Forhistogramanalysisofpixelvaluedistributioninoriginalandencodeddatasets,exampleswereﬂattened,withﬂoating-pointprecision,scaledto0.0to1.0,andhistogramswith100binseachweregener-atedwithnumpy.histograminPythonandﬁttingofencodedhistogramdatatotheGaussiandistributionwasperformedinGraphPad8.4(GraphPadSoftware,SanDiego,CA),withgoodness-of-ﬁtreportedasR2.Formeasurementofinformationentropyinoriginalandencodedexamples,exampleswereﬂattened,withﬂoating-pointprecision,scaledto0.0to1.0,andﬂattenedandscaledsignalswereconvertedtoprobabilitiessummingto1.0,thentotalentropyacrosseachexamplewasdeterminedinnats(1/ln(2)shannons)byS=-sum(pk*ln(pk).LinearregressionoforiginalandencodedentropyforpairsofexampleswithinlabelgroupswasperformedinGraphPadwithgoodness-of-ﬁtreportedasR2.Samplesizepoweranalysiswasperformedfortwo-samplemeansusingaseriesof20controlruns(unencodedMNIST)onthesameANN,whichshowedacontrolmeanvalidationaccuracyof0.977andstandarddevi-8UnderreviewassubmissiontoTMLRationofthevalidationaccuracyof0.001,yieldingaverysmallsamplesizebecauseoftheextremelytightdistributionofresultsreﬂectedintheverysmallstandarddeviation.Conservativelyassumingastandarddeviation50xlargerthanthis(0.05)forbothgroupsyieldsasamplesizeestimationof10modeltrainingsneededpergroup(controlvs.encoded)todetectadiﬀerenceinaccuracyof0.007(0.977forcontrol,0.970forencoded)withpowerof0.8andalphaof0.05.Keepingtheverylowobservedstandarddeviationforthecontrolgroup(0.001)butconservativelyassuminga50xhigherstandarddeviationfortheencodedgroup(0.05)forthesamedetectionofa0.007diﬀerenceinaccuracyyieldsasamplesizeestimationof7modeltrainingspergroup.Basedontheabovecalculations,aconservativechoiceof15trainingrunspergroupwasusedforexperimentscomparingperformancebetweencontrolandencodeddata,andfortrainingtimeexperiments,20trainingrunspergroupwasused.Fortrainingtimeexperiments,elapsedtimewasmeasuredfromthestartoftheﬁrsttrainingepochtotheendofthelasttrainingepoch.StatisticaltestsandsamplesizecalculationswereperformedwithStata/MP16.1(StataCorp,CollegeStation,TX)andGraphPad.IllustrationsandgraphsweregeneratedusingthePythonlibrariesMatplotlibandToyplotinGoogleColab+andusingGraphPad.5Results5.1SingletransformationsFigure3showsmodeltrainingperformanceforeachoftherandomfunctionsdescribedabove(padding,perturbation,andorthogonaltransformation)appliedasseparate,single-steptransformationsandcomparedwithtrainingwithunencodedMNIST(control).Forpaddingtransformation(Figure3A1-3),theresultsofﬁxedpadding(applicationofthesamerandompadtoeveryexample)areshowninFigure3A2,andtheresultsofnon-ﬁxedpadding(applicationofadiﬀerentrandompadtoeveryexample)areshowninFigure3A3.Forperturbationtransformation(Figure3B1-3),theresultsofﬁxedperturbation(applicationofthesamesetofperturbationtransformationstoeveryexample)areshowninFigure3B2,andtheresultsofnon-ﬁxedperturbation(applicationofadiﬀerentsetofperturbationtransformationstoeveryexample)areshowninFigure3B3.Fororthogonaltransformation(Figure3C1-3),resultsofrandomorthogonalmatrixtransformationareshowninFigure3C2,andtheresultsofﬁxedshuﬄingareshowninFigure3C3.Foreachofthetransformationsappliedindividually,nosigniﬁcantdiﬀerenceinthevalidationaccuracywasdetectedbetweenmodelstrainedontransformeddatavs.control(Figure3).5.1.1Non-ﬁxedvs.ﬁxedshuﬄingAsdiscussedabove,non-ﬁxedshuﬄing(applicationofadiﬀerentindexshuﬄetoeachexampleacrossthetrainingandtesttensors)isnotgenerallyofpracticaluse,asthismanipulationdisruptsthespatialrelation-shipsbetweendataelements(pixels)andthusforcesthenetworktotrainonlyonnon-spatialinformation.Thiseﬀectdoes,however,allowforanassessmentoftheextenttowhichamachinelearningsystemusesspatialvs.non-spatialinformationtotrain.Comparingnetworktrainingbetweennon-ﬁxedshuﬄedandﬁxedshuﬄedMNISTshowedthatthenon-ﬁxedshuﬄeddata(lackingspatialinformation)yieldedameanvalidationaccuracyof0.2608±0.0049comparedwith0.9769±0.001forﬁxedshuﬄedcontrols(P<0.001).5.2VariationinpaddingsizeThesizeoftheappendedrandompadisoneofseveraladjustableparametersinthemethodstestedhere,sotheimpactofvaryingthesizeoftheappendedpadonvalidationaccuracywastestedforbothﬁxedandnon-ﬁxedpadding.Figure4showstheresultsofincreasingpadsizefrom0rows(control)to10rowsofpadding,withexamplesillustratedinFigure4A.Forbothﬁxedpaddingandnon-ﬁxedpadding,increasingsizeoftheappendedpaddidnotreducevalidationaccuracy(Figure4B).5.3VariationinperturbationrangeAnotheradjustableparameterinthepresentmethodsistherangeofpotentialperturbationofpixelsintheexamples.Intheaboveillustrations(Figures2Cand3B1),aperturbationrangeof-5%to+5%isshown,9UnderreviewassubmissiontoTMLR
Figure3:Separatetransformationscomparedwithcontrols.A1.Illustrationofrandompadding(a2-rowrandompadappendedtotheMNISTexamples).A2.Modeltrainingonﬁxedpaddeddata(samerandompadforeveryexample).Meanvalidationaccuracywas0.9763±0.001forpaddeddata(Processed),and0.9770±0.001forcontrol(P=0.14).A3.Modeltrainingonnon-ﬁxedpaddeddata(adiﬀerentrandompadforeachexample).Meanvalidationaccuracywas0.9762±0.0012comparedwith0.9769±0.0009forcontrol(P=0.08).B1.Illustrationofrandomperturbation(applyingarandomperturbationarray,constrainedfrom-5%to+5%totheexamplesofMNIST).B2.Modeltrainingafterﬁxedperturbation(samerandomperturbationarrayappliedtoeveryexample).Meanvalidationaccuracywas0.9767±0.0008(Processed)comparedwith0.9770±0.0009(Control)(P=0.23).B3.Modeltrainingafternon-ﬁxedperturbation(adiﬀerentarrayofrandomperturbationsappliedtoeachexample).Meanvalidationaccuracywas0.9768±0.001(Processed)comparedwith0.9772±0.001(Control)(P=0.44).C1.Illustrationofrandomorthogonaltransformation.Asinglerandomorthogonalmatrixisusedtotrans-formeveryexample.C2.Modeltrainingafterrandomorthogonalmatrixtransformation(asillustratedinC1).Meanvalidationaccuracywas0.9767±0.0009(Processed)comparedwith0.9767±0.0008(Control)(P=0.62).C3.Modeltrainingafterﬁxedshuﬄing(asubsetofthebroadersetofrandomorthogonaltransformations).Meanvalidationaccuracywas0.9768±0.001(Processed)comparedwith0.9769±0.0009(Control)(P=0.71).10UnderreviewassubmissiontoTMLR
Figure4:Variationofpaddingsize.A.Illustrationofincreasingpadsizefrom0padrowsontheleft(control)upto10padrowsontheright.B.Validationaccuracyofmodelstrainedwithasingle-steptransformationusingdiﬀerentnumbersofpadrows,from0to10padrows.Whitecirclesshowtheresultsofvaryingpadsizeusingﬁxedpadding(thesamerandompadappliedtoeveryexample)andgraycirclesshowtheresultsofvaryingpadsizeusingnon-ﬁxedpadding(adiﬀerentrandompadappliedtoeachexample).Horizontaldottedlineindicatesthebaselinefromcontrol.butthisrangeisarbitraryandcanbevariedacrosssmallerorlargerranges,creatingdiﬀerentdegreesofdistortionforhumanviewersdependingontherangechoice.Whileonlysmallrangesofperturbationwouldtypicallybedesirabletoachievesecuritygoalstogetherwithpaddingandorthogonaltransformation,itisimportanttoestablishhowvaryingdegreesofperturbationimpactmodeltraining.Asdiscussedabove,pixelperturbationbeyondtheboundsoftheavailablepixelvaluesmaybehandledinseveraldiﬀerentways,includingclipping(settingthevaluetotheboundaryvalue)andreﬂection(reﬂectingthevalueabouttheboundaryvalue),and,asexpected,thesetwodiﬀerentapproachesyieldvisiblydiﬀerentresultswhenverylargeperturbationrangesareused,asseeninFigure5A(clipping)vs.Figure5C(reﬂec-tion).TheimpactofvaryingtheavailableperturbationrangeonvalidationaccuracyisshowninFigure5B(clipping)andFigure5D(reﬂection).Verylargeperturbationrangeswithclippingcauseanincreasingnumberofpixellocationstobeclipped,andasexpected,thisleadstoaprogressivedecreaseinaccuracy,buttoalesserextentthanmightbepredictedbasedonvisualinspectionoftheencodedresults.AsshowninFigure5B,perturbationrangesupto-50%to+50%donotappreciabledecreasevalidationaccuracy,andevenverylargeperturbationrangesabove-150%to+150%thatrendertheexamplesuninterpretabletohumaninspectionstillproducevalidationaccuraciesaround0.95withMNIST.Verylargeperturbationrangeswithreﬂectionareexpectedtoretainmoreusefultraininginformationthanclipping,asclippingisintrinsicallyalossymanipulation,whilereﬂectionneednotbe.Consistentwiththis,increasingtheperturbationrangewithreﬂectioncausesasmallerdecreaseinvalidationaccuracythanthatseenwithclipping,reachingaplateauofvalidationaccuracyaround0.965(Figure5D).5.4Serialapplicationofpadding,perturbation,andorthogonaltransformationThepresentmethodinvolvessequentialapplicationofrandompadding,randomperturbation,andrandomorthogonaltransformationtoyieldanaggregatenon-orthogonalandnonlineartransformation.Whilethe11UnderreviewassubmissiontoTMLR
Figure5:Variationofperturbationrange.A.Illustrationofperturbationrangesfrom0%(control)to200%(-200%to+200%),processedwithclippingtohandlevaluesbeyondthepixelvaluerange.B.Validationaccuracyofmodelstrainedwithasingle-steptransformationusingﬁxedperturbationwithclipping,from0%to200%.Horizontaldottedlineindicatesthecontrolbaseline.C.Illustrationofperturbationrangesfrom0%to200%,processedwithreﬂectiontohandlevaluesbeyondthepixelvaluerange.D.Validationaccuracyofmodelstrainedwithasingle-steptransformationusingﬁxedperturbationwithreﬂection,from0%to200%.Horizontaldottedlineindicatesthecontrolbaseline.12UnderreviewassubmissiontoTMLRresultspresentedinFigure3showthatsingletransformationsofpadding,perturbation,ororthogonaltrans-formationdonotdecreasevalidationaccuracy,itiscriticaltotesttheimpactofdiﬀerentformsofpaddingandperturbationperformedinserieswithorthogonaltransformation,inatypicalsequenceof(padding→perturbation→orthogonaltransformation[usingtheﬁxedshuﬄingformoforthogonaltransformationintheexamplesshown]).5.4.1DiﬀerentpaddingapproachescombinedwithﬁxedperturbationandorthogonaltransformationThediﬀerentformsofrandompaddingdescribedabove(ﬁxedpadding,adjustedpadding,andnon-ﬁxedpadding)maybeappliedtogetherwithﬁxedperturbationandorthogonaltransformation,soeachoftheseapproacheswastestedtotrainmodelsondataencodedwiththesequence(padding→ﬁxedperturbation→orthogonaltransformation).AsshowninFigure6A,applicationofﬁxedpaddingtogetherwithﬁxedperturbationandﬁxedshuﬄingproducesaverysmallbutsigniﬁcantdecreaseinvalidationaccuracy(0.9754fortheencodeddatacomparedwith0.9769forcontrol,arelativedecreaseinvalidationaccuracyof0.15%).Adjustedpadding(seeabovefordetaileddescription)usingapotentialadjustmentof-20%to+20%togetherwithﬁxedperturbationandﬁxedshuﬄingyieldedasimilarvalidationaccuracy(0.975).Applicationofnon-ﬁxedpadding(adiﬀerentrandompadforeverytrainingandtestexample)togetherwithﬁxedperturbationandﬁxedshuﬄingyieldedaslightlylowervalidationaccuracy(0.9742fortheencodeddata,arelativedecreaseinvalidationaccuracyof0.29%).Similarresultswereobtainedwhenrandomorthogonalmatrixtransformationwasusedinsteadofﬁxedshuﬄingintheabovecombinationsofpaddingandﬁxedperturbation(datanotshown).
Figure6:Diﬀerentpaddingapproachescombinedwithﬁxedperturbationandﬁxedshuﬄing.A.Fixedpadding(thesame2x28padappendedtoeveryexample),followedbyﬁxedperturbation(-5%to+5%),followedbyﬁxedshuﬄing.Meanvalidationaccuracywas0.9754±0.001(Processed)comparedwith0.9769±0.0011(Control)(P=0.004).B.Adjustedpadding(a2x28padthatisrandomlyperturbedwithinthebounds[-20%to+20%]priortoappendingtoeachexample),followedbyﬁxedperturbationoftheexamples(-5%to+5%),followedbyﬁxedshuﬄing.Meanvalidationaccuracywas0.9756±0.0011(Processed)comparedwith0.9771±0.0014(Control)(P=0.005).C.Non-ﬁxedpadding(adiﬀerent2x28randompadappendedtoeachexample),followedbyﬁxedperturbation(-5%to+5%),followedbyﬁxedshuﬄing.Meanvalidationaccuracywas0.9742±0.001(Processed)comparedwith0.9770±0.0012(Control)(P<0.001).13UnderreviewassubmissiontoTMLR5.4.2Diﬀerentpaddingapproachescombinedwithnon-ﬁxedperturbationandorthogonaltransformationThediﬀerentformsofrandompadding(ﬁxedpadding,adjustedpadding,andnon-ﬁxedpadding)mayalsobeappliedtogetherwithnon-ﬁxedperturbationandorthogonaltransformation,soeachoftheseapproacheswastestedtotrainmodelsondataencodedwiththesequence(padding→non-ﬁxedperturbation→orthogonaltransformation[usingtheﬁxedshuﬄingformoforthogonaltransformationintheexamplesshown]).AsshowninFigure7A,applicationofﬁxedpaddingtogetherwithnon-ﬁxedperturbationandﬁxedshuﬄingproducesasmallbutsigniﬁcantdecreaseinvalidationaccuracy(0.9750,arelativedecreaseinvalidationaccuracyof0.23%).Adjustedpaddingusingapotentialadjustmentof-20%to+20%togetherwithnon-ﬁxedperturbationandﬁxedshuﬄingyieldedasimilarvalidationaccuracy(0.9750,arelativedecreaseinvalidationaccuracyof0.21%).Applicationofnon-ﬁxedpaddingtogetherwithnon-ﬁxedperturbationandﬁxedshuﬄingyieldedalowervalidationaccuracy(0.9738fortheencodeddata,arelativedecreaseinvalidationaccuracyof0.35%).Similarresultswereobtainedwhenrandomorthogonalmatrixtransformationwasusedinsteadofﬁxedshuﬄingintheabovecombinationsofpaddingandnon-ﬁxedperturbation(datanotshown).
Figure7:Diﬀerentpaddingapproachescombinedwithnon-ﬁxedperturbationandﬁxedshuﬄing.A.Fixedpadding,followedbynon-ﬁxedperturbation(-5%to+5%),followedbyﬁxedshuﬄing.Meanvalidationaccuracywas0.9750±0.001(Processed)comparedwith0.9772±0.001(Control)(P<0.001).B.Adjustedpadding,followedbynon-ﬁxedperturbationoftheexamples(-5%to+5%),followedbyﬁxedshuﬄing.Meanvalidationaccuracywas0.9750±0.0008(Processed)comparedwith0.9771±0.0010(Control)(P<0.001).C.Non-ﬁxedpadding,followedbynon-ﬁxedperturbation(-5%to+5%),followedbyﬁxedshuﬄing.Meanvalidationaccuracywas0.9738±0.0014(Processed)comparedwith0.9772±0.0011(Control)(P<0.001).5.5TrainingtimeTrainingtimesweretestedtocomparecomputationtimeforcontrolunencodedMNISTwiththesamemodelspeciﬁcationtrainedusingMNISTencodedwithsequentialpadding,perturbation,andﬁxedshuﬄing.ModeltrainingwithMNISTencodedwithﬁxedpaddingfollowedbyﬁxedperturbationfollowedbyﬁxedshuﬄingshowednosigniﬁcantdiﬀerenceintrainingtimeswhencomparedwithcontrolMNIST(25.92±0.57secfortransformedMNISTvs.26.06±0.65secforcontrol,Figure8).Themeanprocessingtimerequiredtoperformencodingwiththesethreetransformationsacrossthetrainingandtesttensors(representingaone-timeprocessingtimeforagivenproject)was22.49±0.23sec,whichrepresentsabout0.3msecperMNISTexample.Ofnote,thecodeusedtoperformtheseserialtransformationswasnotoptimizedtominimizetherequiredprocessingtimeforencoding,asthebriefprocessingtimesrequiredherewereacceptableforthepresentwork.14UnderreviewassubmissiontoTMLR
Figure8:Modeltrainingtime.Modeltrainingforaﬁxedpadded(2x28),ﬁxedperturbed(-5%to+5%),andﬁxedshuﬄeddataset(Pro-cessed)was25.92±0.57sec,comparedwith26.06±0.65secforcontrol(P=0.67).Forthissetoftransforma-tions(ﬁxedpadding,ﬁxedperturbation,andﬁxedshuﬄing),processingofMNISTpriortomodeltrainingtook22.49±0.23sec,whichrepresentsaone-timeprocessingsteppriortotrainingforagivenproject.Pro-cessingtimesweretestedonaGoogleColab+serverrunningonaNVIDIATeslaT4GPU,with20separatemodeltrainingspercondition.5.6TestingonalternativedatasourcesToexplorewhethersimilarresultsmaybeobtainedwithdatasetsknowntobemorechallengingthanMNIST,additionalexperimentstestedfully-connectedANNtrainingonencodedandcontrolversionsofFashion-MNIST(Xiaoetal.,2017)andCIFAR-10(Krizhevsky).Encodingusedontheadditionaldatasetswas2-rownon-ﬁxedpadding,non-ﬁxedperturbation(-5%to+5%),andﬁxedshuﬄing.TrainingontheFashion-MNISTdatabaseyieldedthefollowingvalidationaccuracies:control0.8999±0.0032,encoded0.8874±0.0025(P<0.001).TrainingonCIFAR-10yielded:control0.5505±0.0174,encoded0.5289±0.033(P<0.001).ForFashion-MNISTandCIFAR-10,modelstructuresweresimilartothoseusedforMNIST,butwith256batchsize,24epochs,and256neuronsintheﬁrstlayer.Thetypicalapproachtoobtainbetteraccuracieswiththesedatasets,particularlywithCIFAR-10,involvestheuseofCNNstructures,whichwillbepossibletointegratewithourencodingmethodinthefuture(seebelow),butarebeyondthescopeofthisinitialreport.Finally,oursyntheticchallengedataset(58x58pixelimagesasencoded,50Ktrain,10Ktest,seebelowfordetails)yieldedvalidationaccuraciesof:control0.9849±0.0016,encoded0.9726±0.0028,P<0.001.Forthemodelstrainedonthechallengedataset,thestructuresweresimilartothoseusedforMNIST,butwith682neuronsinbothlayers.TheabsolutedecreaseinvalidationaccuracyforeachdatasetfromcontroltoencodedversionisshowninTable1.Table1:AbsolutedecreaseinvalidationaccuracyacrossdatasetsDatasetPercentaccuracydecreaseMNIST0.51Fashion-MNIST1.25CIFAR-102.16ChallengeDataset1.235.7PrivacyAnalysisAsdescribedabove,ourthreatmodelhasapotentialattackerinpossessionofanencodedtrainingdataset(Zd)andncorrespondinglabels(Yd);atrainedneuralnetwork,obtainedbytrainingonZdandYd;anda15UnderreviewassubmissiontoTMLRsetofattacker-providedoriginal(unencoded)examples(x′a,x′b,···)notpresentintheoriginaldataset(Xd),withassociatedlabels(y′a,y′b,···)obtainedbyinferencefromthetrainedneuralnetwork.Ourprivacyschemeisdesignedtoprotectagainstanattackerusingtheseelementstoreverseourencodingrandomfunctionf(Xd)→Zdbydiscoveringtheinversefunctionf-1(Zd)→Xd,oracloseapproximationthereof.Therandomfunctionsthatcreateanaggregatenon-orthogonalandnonlineartransformationinourmethodproduceanextraordinarilylargesearchspaceinwhichapotentialattackerwouldhavetodeterminetheencodingtransformation.Forexample,indexshuﬄingofagivenarrayyieldsasingleresultatrandomoutofaverylargesetofpossibleshuﬄedarrays,withthenumberofpossiblearraysrepresentedbythefactorialofthelengthofthearray.Thus,evenforsmallexamplearrayssuchasthoseinMNIST,with(28x28=784pixel)examples,thetotalnumberofpossibleindexshuﬄesforanexampleis784!,whichis3.19x101930possibleindexshuﬄes,anumbervastlylargerthanthenumberofguessesrequiredtobrute-forceAES-256(2256,orroughly1077).Beyondtheparticularcaseofindexshuﬄing,thespaceofallpossiblerandomorthogonalmatrixtransformationsforagiveninputvectorsizeisfargreater.Randompadding,eitherﬁxedornon-ﬁxed,changesthenumberofvectordimensionsbetweenXdandZd,inarandomfashionthatisunknowntoPartyC(theattacker)inthethreatmodel.AsshownaboveinFigure4,anarbitrarynumberofvectordimensionsmaybeaddedtoXd,andthelocation,number,andrandompixelvaluesofthesenewdimensionsintheresultingvectorsinZdareallunknowntotheattacker.Thevaluesstoredintheaddeddimensionsarerandomlydeterminedforallformsofpadding,andfornon-ﬁxedpaddingoradjustedpadding,thevaluesstoredintheaddeddimensionsrandomlyvaryfromexampletoexample.Becausealargenumberofaddeddimensionswithrandomvaluesmaybeaddednon-deterministicallybetweenXdandZd,thiscreatesa‘needleinahaystack’problem:forencodedvectorsinZdofknownlength,len(z),theattackermustﬁndtheoriginalvectorsofunknownlength,len(x)«len(z).Thepaddingcomponentoftheencodingprocess,whichcanaddrandompixelsatanypositioninsideoroutsideoftheoriginalinputexamples,introducesanothersourceofrandomnesswithaverylargesearchspace:fortheadditionof(10x28)randompixelstoanoriginal28x28MNISTexample,thiscanbeapproximatedby[(784+280)combinations784]=5.8x10264.Whenpaddingisputtogetherwithanindexshuﬄe,thesearchspaceinwhichanattackermustdiscovertheoriginalvectorintherandompaddedvectorcanbeapproximatedby[(784+280)permutations784]=1.8x102195.Bothofthesecalculationssubstantiallyunderestimatethethetask,giventhatinourthreatmodeltheattackerdoesnothaveknowledgeoftheoriginalnumberofvectordimensions.Perturbationaddsrandomextrinsicnoiseacrossexamplevectorsthatfurtherservestoimpedeattemptstoreducethesearchspace.TheimpactofperturbationonthetransitionofexamplesfromXdandZdisnotadeterministicfunctionbetweenXdandZd:everypixelisvariedrandomlyindiﬀerentfashion,acrossarangeofperturbationvaluesthatisunknowntotheattacker.Non-ﬁxedperturbationensuresthatevenbeforeorthogonaltransformationisapplied,nopixelvalueintheoriginalexamplesXdmapsdirectlytoacorrespondingpixelvalueintheencodedexamplesZd.Finally,alloftherandomfunctionsdiscussedabove(padding,perturbation,andorthogonaltransformation)arefullyindependentofthelabelcategoriesinYd—inthenextsection,thiswillbeexaminedindetailinthecontextofourthreatmodel.5.8InformationLeakageTheinformationleakageinourthreatmodelasshowninFigure1isthespeciﬁcknowledgeoftherelationshipbetweenencodedexamplesinZdandlabelsinYdandbetweennon-encodedinferenceexamplesx’andinferencelabelsy’generatedbyamodeltrainedonencodedexamples(Figure1).Becauseallofthetransformationsappliedinthepresentmethodarerandomfunctionsthatarefullyinde-pendentofthelabelcategoriesinYd,itcanbeshownanalyticallythattheinformationleakedinthethreatmodeldoesnotassistinanattempttoreversethetransformationofXdtoZd.Asdescribedindetailabove,thedesignoftheencodingprocesspresentedhereinvolvesserialapplicationofrandomfunctionstothedata,beginningwithXdandyieldingZd,inafashionthatisindependentofthe16UnderreviewassubmissiontoTMLRlabelcategoryinYd(Figure9).Putanotherway,alloftherandomfunctionsusedareappliedacrossthetensorsandareagnostictolabelvalue.
Figure9:Independenceofrandomfunctiontransformationsfromlabelcategories.Independenceofrandomtransformationsfromlabelcategory,illustratedwithatransformationofnon-ﬁxedpadding,ﬁxedperturbation,andrandomorthogonalmatrixtransformationappliedtoanexamplexnfromFashion-MNIST.A.ArandomfunctionFaactingonexamplesxninXdappliesanon-ﬁxedpaddingtoeachexamplexntoyieldatransformedexamplex′n,wherethefunctionisrandomperpixelofeachoftheexamples.TherandomfunctionFaisindependentoflabelyn.B.ArandomfunctionFbactingonexamplesx′nappliesaﬁxedperturbationofeachexamplex′ntoyieldtransformedexamplesx′′n,wherethefunctionisrandomperexampleacrossthetensor.TherandomfunctionFbisindependentoflabelyn.C.ArandomfunctionFcactingonexamplesx′′nappliesarandomorthogonalmatrixtransformationtoeachexamplex′′ntoyieldatransformedexampleznintheresultingtransformeddatasetZd,wherethefunctionisrandomperexampleinthedataset.TherandomfunctionFcisindependentoflabelyn.BecauseallrandomfunctionsinthepresentmethodareindependentofthevalueoflabelsinYd(Figure9),theinformationleakageinourthreatmodel(knowledgeoftherelationshipbetweenZdandYdandbetweeninferenceexamplesx′andinferencelabelsy′)doesnotprovideinformationonthenatureoftherandomfunctionsappliedbetweenXdandZd.Totestthisassertionempirically,originalimagesfromFashion-MNISTweretransformedbypadding(5x28non-ﬁxedpadding,randomatthepixellevelforallexamples),ﬁxedperturbation(min=-5%tomax=+5%,withreﬂection),andrandomorthogonalmatrixtransformation(asillustratedinFigure9),andtheoriginalandencodedexamplesweresubjectedtoanalysisaccordingtolabelcategory.AsshowninFigure10,theaveragedistributionofpixelvaluesintheoriginalFashion-MNIST(Xd)diﬀerfromlabeltolabelinYd,aswouldbeexpectedforimagesofdiﬀerenttypesofobjects(e.g.,T-shirts[label0]vs.Sandals[label5]).Whenencodingbythesequenceofnon-ﬁxedpadding,ﬁxedperturbation,andrandomorthogonalmatrixtransformation(asillustratedinFigure9)isappliedtoyieldanencodeddataset(Zd),thedistributionofpixelvalueswithinlabelcategoriesinYdisGaussian(withR2goodness-of-ﬁttotheGaussiandistribution>0.997foralllabelcategories)anddoesnotvaryfromlabeltolabel.ThisobservedlackofvariationacrosslabelcategoriesisexpectedfromtheapplicationofrandomfunctionsthattransformXdintoZdinafashionthatisindependentoflabelsinYd.TofurthershowthatvariabilityinXdacrosslabelcategoriesinYddoesnotcarryovertovariabilityinZdacrosslabelcategoriesinYd,informationentropywasmeasuredforpairsoforiginal(Xd)andencoded(Zd)examples,groupedaccordingtolabelcategory(Yd).AsshowninFigure11,entropyvariesconsiderablyacrosslabelcategoriesforexamplesinXd(Figure11A),andafterencodingbytheserialrandomfunctionsthatareindependentofYd,entropyrisestoahigher,andfarmorenarrowlydistributed,level(Figure11B1and11B2).Linearregressionbetweenentropyoforiginalexamplesandpairedencodedexamples(beforeandaftertransformationbyrandomfunctionsindependentofYd)showsnorelationshipbetweenvariationinentropybeforeencodingandthevariationinentropyafterencodingwithinlabelcategoriesinYd(Figure11C).Thisobservedlackofarelationshipbetweenentropyvariationbeforeencodingandentropy17UnderreviewassubmissiontoTMLR
Figure10:AveragedistributionofpixelvaluesinoriginalandencodedFashion-MNISTtrainingexamplesaccordingtolabelcategory.AllexamplesaregroupedaccordingtolabelvalueinYd,subjectedtohistogramanalysisofpixelvalueswith100bins,andtheresultinghistogramsareaveragedwithinlabelgroups.A.AveragehistogramsoforiginalFashion-MNISTexamples(Xd)accordingtolabelcategoryinYd,showingexpectedvariationinpixelvaluedistributionaccordingtolabel.B.AveragehistogramsoftransformedFashion-MNISTexamples(Zd)accordingtolabelcategoryinYd,showingasimilarGaussiandistributionofpixelvaluesaccordingtolabel.R2goodness-of-ﬁt>0.997fortheGaussiandistributionforalllabelcategories.18UnderreviewassubmissiontoTMLRvariationafterencodingwithinlabelcategoryisexpectedfromtheapplicationofrandomfunctionsthatareindependentoflabelsinYd.TheaboveanalysesofpixeldistributionandentropyvariationbothsupporttheaprioriassertionthatknowledgeoftherelationshipbetweenZdandYddoesnotprovideinformationabouttherandomfunctionsthatareappliedbetweenXdandZdinafashionthatisindependentofYd.BecauseanattackercannotusetheinformationleakedregardingYdtogaininformationabouttheencodingrandomfunctionf(Xd)→Zd,anattacker’sattempttodiscovertheinversefunctionf-1(Zd)→Xdisprobabilisticallysubjecttotheextremelylargesearchspacesdescribedabove.5.9EncodedDatasetChallengeAnexampledatasetencodedwiththeabovemethods(beginningwithasyntheticunknowndataset)isavailableat:https://drive.google.com/drive/folders/1Q0xspKzQCgP4ouYWcHGJcsLCa4o6pHjdTheencodeddatasetisstoredasseparate.npyﬁlesforx_train,y_train,x_test,andy_test,andconsistsofa60,000train/10,000testsplitwith(58x58)pixelgrayscaleexamplesandlabelsin10categories(0-9).Thereare6,000examplesineachofthetraincategories,and1,000examplesineachofthetestcategories.Loadingthe.npyﬁletypeintoPythonmaybeperformedwithnumpy.load(https://numpy.org/doc/stable/reference/generated/numpy.load.html).Tomatchthethreatmodel,wheretheattackermaybeamemberofPartyBandthusmayhaveaccesstoamodeltrainedon(Zd)and(Yd),aKerasmodeltrainedontheencodedchallengedataset(Zd=x_train)and(Yd=y_train)isalsoincludedinthesamefolderinHDF5(.h5)format,whichmaybeloadedwithtf.keras.models.load_model(https://keras.io/api/models/model_saving_apis/).6Discussion6.1SummaryAmethodforsecureencodingofdatasetsformachinelearningoperationsisdescribedhereindetailandshowsverylittlereductioninfully-connectedANNtrainingperformancewhencomparedtotrainingwithunencodeddatasets.Theindividualtransformationsofthemethodappliedseparatelydonotsigniﬁcantlyreducevalidationaccuracy.Trainingonseveraldatasetstransformedbysequentialapplicationofpaddingfollowedbyperturbationfollowedbyorthogonaltransformationresultsinvalidationaccuraciesthatareonlyslightlylowerthanthoseseenwithunmodiﬁedcontroldatasets.Trainingtimeissimilarbetweenencodedandcontroldatasets.6.2SecurityaspectsofthemethodTheprivacyestablishedbyourencodingmethodsinthecontextofourthreatmodel(Figure1)aredis-cussedandassessedanalyticallyinsections5.7’Privacyanalysis’and5.8’Informationleakage’above,withaccompanyingFigures9,10,and11.6.2.1SecurityparametersPracticalsecurityschemeshavesecurityparametersthatcanbevariedtochangethestrengthoftheencodingscheme,suchasthekeysizeinAES(NationalInstituteofStandardsandTechnology,2001).Smallkeysizesmightallowtheschemetobeeasilybrokenwithmoderncomputingresources,whilelargerkeysizessuchasthecurrentAESstandardof256bitkeysprovidesecuritybymakingtheproblemcomputationallyintractable.Varyingsecurityparametersinsuchschemestypicallyinvolvetradeoﬀs;forexample,largerkeysizesyieldgreatersecuritybutmakeencodingcomputationslower.Inthepresentmethod,thereareseveralsecurityparametersthatcanbeadjustedtovarythelevelofsecurityforagivenprojectwithinthedeﬁnedthreatmodelasshownaboveinFigure1:•Typeofpaddingapplied(e.g.,ﬁxedpadding,adjustedpadding,non-ﬁxedpadding).19UnderreviewassubmissiontoTMLR
Figure11:EntropyanalysisofpairedoriginalandencodedFashion-MNISTtrainingexamples,accordingtolabelcategory.TotalentropyperexamplecalculatedforeachFashion-MNISToriginalexampleinXdandeachencodedFashion-MNISTexampleinZd,groupedbylabelcategoryinYd.A.ScatterplotshowingtheentropydistributionoforiginalexamplesinXdaccordingtolabelcategoryinYd.B1.ScatterplotshowingtheentropydistributionofencodedexamplesinZdaccordingtolabelcategoryinYd,usingthesameY-axisscaleforentropyasshownin(A),showingthatentropyincreasesfromXdtoZdandthatentropyvariationdramaticallydecreasesfromXdtoZd.B2.Samescatterplotasshownin(B1)butwithzoomed-inY-AxistoshowvariationofentropyvaluesineachlabelcategoryintheencodedsetZdoveramuchsmallerrange.C.Linearregressionoforiginalentropy(X-axis)againstencodedentropy(Y-axis)forpairedexamplesbeforeandaftertransformationfromXdtoZd.Best-ﬁtregressionlinesindiﬀerentlabelcategorieshavepositive,near-zero,ornegativeslopes,andallregressionshaveextremelypoorR2goodness-of-ﬁtvalues(lowest<0.001,highest0.051),indicatingalackofrelationshipbetweenentropyvariationintheoriginals(Xd)andentropyvariationafterencoding(Zd).20UnderreviewassubmissiontoTMLR•Extentofpaddingappliedperexample(npixels).•Locationofpaddingrelativetooriginalpixels(e.g.,intrinsic,extrinsic,contiguous,non-contiguous).•Typeofperturbationapplied(e.g.,ﬁxedperturbation,non-ﬁxedperturbation).•Choiceofperturbationfunction(e.g.,changeinpixelvaluebyrandomamountoranothermathe-maticalfunctionwitharandomvariable).•Perturbationrange(e.g.,randomperturbationofeachpixelwithboundsfrom-A%to+B%).•Variationofperturbationrange(wherethevaluesforAandBabovearevariedfrompixeltopixelpriortorandomchoiceofaperturbationamountbetweenAandBforthatpixel).•Handlingofedgevalueswithperturbation(e.g.,clipping,reﬂection).•Choiceoforthogonaltransformation(e.g.,ﬁxedshuﬄing,randomorthogonalmatrixtransforma-tion).•Iterativeapplicationofmultipletransformations(theabilitytoperformmultipleroundsofpadding,perturbation,andorthogonaltransformationwiththeoptiontovarytheabovesecurityparametersineachround).Furtherdetaileddiscussionofsecurityinthecontextofourthreatmodelareincludedinsection5.7-PrivacyAnalysisandsection5.8-InformationLeakage,above.Independentsecuritytestingmaybeperformedonapubliclyaccessibleencodedchallengedataset(seesection5.9-EncodedDatasetChallenge,above).6.3AdaptationtoconvolutionalneuralnetworksThecurrentpaperexplorestheperformanceofvariousaspectsofthepadding,perturbation,andorthogonaltransformationmethodinthecontextoffully-connectedANNs.Itisalsopossibletoadaptthisapproachtoencodingforusewithconvolutionalneuralnetworks(CNNs)(Anonymized,2022).TheoperationsofconvolutionsandpoolinginCNNsleveragethespatialrelationshipsofdataelements,sotheseoperationscannotbeblindlyperformedafteranorthogonaltransformationsuchasﬁxedshuﬄinginafashionthatisagnostictotheﬁxedshuﬄingalgorithm.Instead,convolutionandpoolingoperationsmaybeperformedpriortoapplicationofpadding,perturbation,andorthogonaltransformation,withtensorscreatedthatstoremultipleencodedresultsofconvolutionandpooling(Anonymized,2022).Alternatively,thealgorithmusedtoapplypadding,perturbation,andanorthogonaltransformationsuchasﬁxedshuﬄingmaybeencryptedbyconventionalmethods,transmittedsecurely,thendecryptedbyauthorizeduserstoapplyconvolutionandpoolingoperations,usingthedecryptedalgorithmasalookuptablethatenablestheseoperationsonencodeddata(Anonymized,2022).SuchintegrationofourencodingmethodwithCNNstructuresisbeyondthescopeofthisinitialreport.6.4AlternativeapproachesVariousalternativeapproachestotheproblemofsecuringdataformodeltraininghavebeenproposed,withrecentworkfocusedontheareasoffederatedlearning,theuseofdatatreatedwithfullyhomomorphicencryption,andseveralproposalsforinstanceencoding,includingtheInstaHidemethod(Huangetal.,2021),theNeuraCryptmethod(Yalaetal.,2021),theDarKnightscheme(Hashemietal.,2020),andapplicationofsingle-steporthogonaltransformationbasedonanimagekey(Chen,2019).6.4.1FederatedlearningFederatedlearningisawell-establishedapproachforperformingdistributedmodelupdatesusingdataonlocaldevices,whereintrainingdatadonothavetobetransmittedfromthelocaldevicetoacentralserver.Inthefederatedlearningframework,amodelandweightsaretransmittedtothelocaldevice,weightsareupdatedbyadditionallocaltraining,andthentheupdatedmodelsandweightsreturnedtothecentralserver(McMahanetal.,2017;Kairouzetal.,2021;Lietal.,2020;Rodríguez-Barrosoetal.,2020).Whilethisapproachhastheadvantageofnotneedingtotransmitdatafromalocaldevicetoacentralserver,andhasfoundwideapplicationinupdatingmodelsusingrichdataonmorecomputing-constrainedsystemssuch21UnderreviewassubmissiontoTMLRasmobiledevices,ithascertaindisadvantageswhenitcomestotheproblemofmodeltrainingofprivatedata.Softwareprovidedbythecentralservermustberuntotrainorupdatemodelsonlocalsystemsthatmaintaintheprivatedata,andthissoftwarehasdirectaccesstotheprivatedata.Thismeansthatthepartymaintainingtheprivatedata(‘PartyA’)musttrustthepartyprovidingthesoftware(‘PartyB’).PartyAmusttrustPartyBtorunsoftwarebehindthePartyA’sﬁrewallandadditionallytrustthissoftwaretohavedirectaccesstoPartyA’sprivatedata.PartyAadditionallymusttrustPartyBnottotransmitanyaspectofPartyA’sprivatedatabacktoPartyB’sserversalongwiththeupdatedmodelsandweightsthathavetobetransmittedfrombehindPartyA’sﬁrewalltoPartyB’sserver.Inadditiontothesecurityissuesdiscussedabove,federatedlearningcanhaveanimportantadversebusinessramiﬁcation:becausefederatedlearningisparticularlywell-suitedforupdatingmodelsonawiderangeofdatasources,thismeansthatthebusinessvalueofprivatedataprovidedbyanyoneorganizationwilltendtobedevaluedbydilution.6.4.2FullyhomomorphicencryptionFullyhomomorphicencryption(FHE)representsafamilyofapproachesthatallowmathematicalmanipu-lationofencrypteddatasuchthatwhenamanipulatedciphertextisdecrypted,theresultisidenticaltoananalogousmathematicalmanipulationperformedontheoriginaldatapriortoencryption(Lauter,2021;Munjal&Bhatia,2022;Alloghanietal.,2019).TherehavebeeninitialexplorationsofusingFHEinthecontextofsecuremachinelearning(Bostetal.,2014;Graepeletal.,2013;Lauter,2021;Nandakumaretal.,2019;Papernotetal.,2016),butFHErequiresprofoundalterationstomachinelearningalgorithmsthatyieldcomputetimesthataremanyordersofmagnitudehigherthanrequiredforworkonunencrypteddatausingconventionalmethods(Nandakumaretal.,2019).Forexample,trainingonMNISTinaninitialexplorationofusingFHEwithaneuralnetworkrequireddownsamplingoftheMNISTdatasetto8x8pixelexamplesanduseofacyclotomicringencryptionconsideredtonotprovidesuﬃcientsecurity(φ(m)=600),anddespitethesesimpliﬁcations,computetimesfortheneuralnetworkrangedfrom40minutesto1.5days(Nandaku-maretal.,2019).WhenamoresecureFHEalgorithmwastested(φ(m)=27,000),thetimetoprocessasingleneuronintheﬁrstlayerwas15minutes(Nandakumaretal.,2019).InanotherexperimenttoperformunsupervisedlearningwithK-means-clusteringonanFHE-encrypteddataset,single-threadruntimeswereestimatedtobeonthetimescaleofmonths(Jäschke&Armknecht,2019).6.4.3TheInstaHidemethodOneformofinstance-hidingencodingdesignedforprivatemachinelearningthathasbeenproposedrecentlyistheInstaHidemethod(Huangetal.,2021).InstaHideﬁrstappliestheapproachofmixup(Zhangetal.,2018),performingalinearcombinationofseveralimagesfromthetrainsetandalargepublicdatabasetogeneratemultipleexamplesderivedfromcombinationsoftheoriginalexamples.Next,theInstaHidemethodextendsimagepixelvaluestoanegative-to-positiverange,thenappliesarandompatternofsigninversionstoeachimage(Huangetal.,2021).Subsequentworkbyothers(Carlinietal.,2021a)hasshownthismethodtobevulnerabletoanattackthattakesadvantageofthespeciﬁcstepsknowntooccurintheInstaHidemethod,inparticular,takingtheabsolutevalueofallpixelvalues(toremovethesign-ﬂippinginInstaHide),clusterencodingthedatasetbytraininganeuralnetwork“todetectwhentwoencodingsweregeneratedfromthesameimage,”andsolvinganunder-determinedsystemofequationsbygradientdescenttorecoveranapproximationoftheoriginalimages(Carlinietal.,2021a).ThespeciﬁcsourcesofrandomnessinInstaHidethatarethebasisofthismethod’sclaimtoprivacyallowedfortheattackbyCarlini,etal.:performinglinearcombinationsusingmultipleimages,inturnyieldingmultipleimages,andperformingasimplerandomalteration(randomsignﬂipping)(Carlini,2020).ThemethodappliedbyInstaHide,andthecorrespondingattackbyCarlini,etal.,areinseveralimportantwaysunrelatedtotheapproachwetakeinthispaper.First,InstaHideuseslinearcombinationsofmultipleimages,includingimagesfromalargepublicdataset,toyieldencodedimages.ThislevelofcomplexityactuallycreatestheweaknessexploitedbyCarlini,etal.,astheypointoutinarelatedblogpost(Carlini,2020).Itisthislinearcombinationofimagesthatallowsthesecondstep(clusterencoding)to“detectwhentwoencodingsweregeneratedfromthesameimage”(Carlinietal.,2021a).Ourapproachdoesnothavethisattackvulnerabilitybecauseitdoesnotinanywaycombinemultipleimages,whichisnecessarytoenable22UnderreviewassubmissiontoTMLRthisstepoftheCarliniattack.Moretrivially,thespeciﬁctransformationofapplyingrandomsigninversionsasusedinInstaHideandnulliﬁedbyCarlinibyapplicationoftheabsolutevalueofallpixelsintheﬁrststepoftheirattackisalsonotrelevanttoourapproach.Inourapproach,everyexampleintheoriginaldatasethasasinglecorrespondingexampleintheencodeddataset,andthereareseveralsourcesofrandomnessusedtoalterthedata,withallsourcesofrandomnessindependentoflabelcategoryandkeptsecretfromapotentialattacker.6.4.4TheNeuraCryptmethodAnotherformofexampleencodingforprivatemachinelearningproposedrecentlyistheNeuraCryptmethod(Yalaetal.,2021).NeuraCryptperformsrandomencodingofanoriginaldatasetbysamplingarandomneuralnetworkwithconvolutionallayers,batchnormalization,andReLU,usingthisrandomlychosennet-worktoencodethedata,togetherwithrandompositionalembeddingandpermutationofimagepatches,to“encodepositionalinformationintothefeaturespacewhilehidingspatialstructure”(Yalaetal.,2021).ThearchitectureofthenetworksusedinNeuraCryptis“closelyinspiredbythedesignofpatch-embeddingmodules”ofVisionTransformernetworks(Dosovitskiyetal.,2021).InasubsequentpapershowingthatitispossibletofullybreaktheprivacyoftheNeuraCryptmethod,theattackleveragestheparticularstructureofNeuraCrypt,namelytheuseofpatch-embeddingandinformationleakagefromthepositionencodingused(Carlinietal.,2021b).6.4.5TheDarKnightmethodAnotherproposedapproachtotrainingsensitivedatausinguntrustedcloudhardwareistheDarKnightapproach,whichpassestraditionallyencryptedexamplestothecloud,wheretheexamplesaredecryptedwithinatrustedexecutionenvironment(TEE)andbasicMLoperationsarebegunwithinthisTEE(Hashemietal.,2020).InordertoperformintensivelinearandnonlinearoperationsonuntrustedcloudGPUs,theDarKnightschemeblindsthedatapassedoutsidetheTEEtotheuntrustedhardwareasavirtualbatch,whereanumberofinputexamplesarelinearlycombinedtoformanumberofencodedexamples(Hashemietal.,2020),similartothelinearcombinationmixupapproachusedaspartoftheInstaHidemethoddiscussedabove(Huangetal.,2021).ItispossiblethatoneaspectoftheattackusedbyCarlini,etal.,onInstaHide,leveragingclusterencodingtodetectlinearcombinationsincludingthesameexample(Carlinietal.,2021a),mightrepresentapotentialattackvectoragainstDarKnightaswell,buttoourknowledgethishasnotbeenexploredtodate.6.4.6Single-steporthogonaltransformationOrthogonaltransformationhasbeenproposedasaone-stepencodingformachinelearningapplications,speciﬁcallybyperformingQRfactorizationonadigitalimageﬁleofappropriatesizeandusingtheresultingorthogonalmatrixtotransforminputvectorsintoencodedvectors(Chen,2019).Usinganorthogonalmatrixtotransforminputvectorsintoencodedvectorsmaintainstheanglesbetweenvectorsandthelengthsofvectors,bytransformingtherelativepositionofthevectorsinthecoordinatesystem(representingarotationand/orreﬂectionofthecoordinatesystem,orinthespeciﬁccaseofindexshuﬄing,shuﬄingofthecoordinateaxes).Becausetherelationshipsbetweenvectoranglesandvectorlengthsarepreservedinthistransformation,machinelearningaccuracyisexpectedtobeunaﬀected,andweshowherethatthisisindeedthecaseinpractice.Speciﬁcapproachesareavailabletoobtainanorthogonalmatrixforuseinencoding,andtheseapproacheshaveimportantdiﬀerences.RandomsamplingoforthogonalmatricesfromtheO(N)Haardistributionyieldsarandomorthogonalmatrix,whilerandomﬁxedshuﬄingyieldsarandomselectionofanindexshuﬄewithinthemuchbroaderspaceoforthogonalmatrices.TheprocessofQRfactorizationofastartingnon-orthogonalmatrixtoyieldanorthogonalmatrixandanuppertriangularmatrixyieldsarandomorthogonalmatrixifthestartingmatrixischosenrandomly,whichisnotthecasewhenstartingwithanimageﬁle.IfonestartswitharandommatrixandappliesQRfactorization,therandomorthogonalmatrixresultingfromthisprocessisrandom,butisdrawnfromadistinctdistributionwithintheoverallspaceofpotentialorthogonalmatrices.Finally,thechoiceofmethodforgeneratingtheorthogonalmatrixforencodinghaspractical23UnderreviewassubmissiontoTMLRimplications,assamplingtheHaardistributionandQRfactorizationofarandommatrixareroughlycubicintimerelativetoinputvectordimensions,whileindexshuﬄingisroughlylinearintimeandmaybemoreappropriateforlargerexamples.Inourpresentmethod,weusethemoregeneralapproachestorandomorthogonalmatrixgeneration(e.g.,samplingfromtheHaardistributionorindexshuﬄing)andfurtherstrengthenprivacybycombiningoneormoreoftheseorthogonaltransformationswiththenon-orthogonalandnonlineartransformationsofpaddingandperturbation,yieldinganaggregatenon-orthogonalandnonlineartransformationthatchangesvectordimensionsbetweenXdandZd.6.4.7OtherapproachesOthervariationsonthethemeofhomomorphicencryptionwithadditionalconstraintshavebeentestedwithneuralnetworkswithimprovedperformanceresults,suchastheapproachofleveledhomomorphicencryption(Bosetal.,2013),togetherwithtailoringofnetworkstructurestothespeciﬁcencryptionapproachesusedinordertomaketherequiredcomputationsmoretractableforneuralnetworktraining(Gilad-Bachrachetal.,2016).Anotherproposedapproachadaptsmulti-partycomputationtechniques(Goldreich,2004),inwhichen-crypteddataispassedbackandforthbetweenparties,updatingcalculationslayerbylayerinstepwisefashion,withthesendingpartyrequiredateachsteptodecryptthedata,applyanonlineartransformation,encrypttheresultandreturnthistotheotherparty(Barnietal.,2006;Orlandietal.,2007).Whilerecentadaptationsofthistechniquehaveimprovedthecomputationaleﬃciency,theapproachstillrequiresbidirec-tionaltransmissionofverylargeamountsofdata,substantiallylongertrainingtimeswhencomparedwithplaintexttraining,andtechnicalsophisticationonbothendsofthedataexchangetoimplementthemethod(Keller&Sun,2021).Finally,abroadareaofcomputerscienceresearchdealswiththetopicofprivacy-preservingdataanalysis(theattempttoachieveabalancebetweendatautilityforanalysis,particularlyintheaggregate,whilemaintainingprivacy,particularlyattheindividuallevel.Thenotionofdiﬀerentialprivacyisamodernapproachtoprivacy-preservingdataanalysisthatquantiﬁesthetradeoﬀbetweendatautilityandprivacywhentrainingonoriginal,sensitivedata(Dwork&Roth,2014).Thegoalprivacyguaranteeindiﬀerentialprivacyisthatanattackerdoesnotlearnsigniﬁcantlymoreaboutanindividual(orexample)ifthatindividual(orexample)ispresentinatrainingdatasetthanwhentheyarenotpresentinthetrainingdataset(Dworketal.,2006).6.5AdvantagesofthepresentedmethodsComparedtoalternativeapproaches,themethodpresentedherehascertainadvantages.Therandomen-codingprocessdescribedhere,whenusedwithfully-connectedANNs,allowsforuseofunchangedmachinelearningsystems,withoutneedforfundamentalalterationofexistingframeworks.Themethodalsoallowsforcustomizationoftheencodingprocess,withtheoptiontovarymultiplediﬀerentsecurityparameters,sothatthetrade-oﬀbetweensecurityandsmalldecrementsinaccuracymaybeadjustedaccordingtoagivenproject’sneeds.Thereisnolossintrainingtimewhenusingtheencodingdescribedhere,andonlyminimalone-timecomputationsareneededtoperformtheencodingsteps.Importantly,thespeciﬁcencodingstepsusedareappliedinsimilarfashiontothetrainingandtesttensorsandanyinferenceexamples.Thisrequirementtoencodeinferencedataprotectsthebusinessvalueoftheencodedtrainingdatausedinacollaboration,becausethecollaborationmaybesetupsuchthatthepartythatcontributesencodeddatamaintainsthesecretoftheencodingprocess,whichmustbesecurelyaccessedinordertoperforminference.6.6ConclusionAtechniqueforsecurelyencodingexampleswithnon-orthogonalandnonlineartransformationispresentedthatenablesdirectANNtrainingonencodeddatasets.Whenusedwithanon-convolutional,fully-connectedANNasshowninthepresentreport,nomodiﬁcationstostandardneuralnetworkarchitectureareneeded.24UnderreviewassubmissiontoTMLRIndividual,single-steptransformationsofthemethoddonotsigniﬁcantlyreducevalidationaccuracy,andtrainingondatasetstransformedbysequentialpadding,perturbation,andrandomorthogonaltransformationyieldsonlyslightlyreducedvalidationaccuracies.Nodiﬀerencesintrainingtimesareseenbetweenencodedandcontroldatasets.BecauseapotentialattackercannotusetheinformationleakedinourthreatmodelregardingtherelationshipbetweenZdandYdtogaininformationabouttheencodingrandomfunctionf(Xd)→Zd,anattacker’sattempttodiscovertheinversefunctionf-1(Zd)→Xdisprobabilisticallysubjecttoextremelylargesearchspaces.Themethodsdescribedheremayusefulinabroadrangeofmachinelearningprojectsrequiringdatasecurity.ReferencesMohamedAlloghani,MohammedM.Alani,DhiyaAl-Jumeily,TharBaker,JamilaMustaﬁna,AbirHussain,andAhmedJ.Aljaaf.Asystematicreviewonthestatusandprogressofhomomorphicencryptiontech-nologies.JournalofInformationSecurityandApplications,48:102362,October2019.ISSN2214-2126.doi:10.1016/j.jisa.2019.102362.AuthorsAnonymized.NonprovisionalpatentapplicationonﬁlewithUSPTO.February2022.BorisBabenko,AkinoriMitani,IlanaTraynis,NahoKitade,PreetiSingh,AprilY.Maa,JorgeCuadros,GregS.Corrado,LilyPeng,DaleR.Webster,AvinashVaradarajan,NaamaHammel,andYunLiu.Detectionofsignsofdiseaseinexternalphotographsoftheeyesviadeeplearning.NatureBiomedicalEngineering,pp.1–14,March2022.ISSN2157-846X.doi:10.1038/s41551-022-00867-5.M.Barni,C.Orlandi,andA.Piva.Aprivacy-preservingprotocolforneural-network-basedcomputation.InProceedingsofthe8thWorkshoponMultimediaandSecurity,MM&amp;Sec’06,pp.146–151,NewYork,NY,USA,September2006.AssociationforComputingMachinery.ISBN978-1-59593-493-2.doi:10.1145/1161366.1161393.JoppeW.Bos,KristinLauter,JakeLoftus,andMichaelNaehrig.ImprovedSecurityforaRing-BasedFullyHomomorphicEncryptionScheme.InMartijnStam(ed.),CryptographyandCoding,LectureNotesinComputerScience,pp.45–64,Berlin,Heidelberg,2013.Springer.ISBN978-3-642-45239-0.doi:10.1007/978-3-642-45239-0_4.RaphaelBost,RalucaAdaPopa,StephenTu,andShaﬁGoldwasser.MachineLearningClassiﬁcationoverEncryptedData.TechnicalReport331,2014.NicholasCarlini.InstaHideDisappointinglyWinsBellLabsPrize,2ndPlace.https://nicholas.carlini.com/writing/2020/,2020.NicholasCarlini,SamuelDeng,SanjamGarg,SomeshJha,SaeedMahloujifar,MohammadMahmoody,ShuangSong,AbhradeepThakurta,andFlorianTramer.IsPrivateLearningPossiblewithInstanceEncoding?,April2021a.NicholasCarlini,SanjamGarg,SomeshJha,SaeedMahloujifar,MohammadMahmoody,andFlorianTramer.NeuraCryptisnotprivate,August2021b.MonchuChen.Methodsandprocessesofencrypteddeeplearningservices,March2019.AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUn-terthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,andNeilHoulsby.AnImageisWorth16x16Words:TransformersforImageRecognitionatScale,June2021.Comment:Fine-tuningcodeandpre-trainedmodelsareavailableathttps://github.com/google-research/vision_transformer.ICLRcamera-readyversionwith2smallmodiﬁcations:1)Addedadiscus-sionofCLSvsGAPclassiﬁerintheappendix,2)FixedanerrorinexaFLOPscomputationinFigure5andTable6(relativeperformanceofmodelsisbasicallynotaﬀected).CynthiaDworkandAaronRoth.TheAlgorithmicFoundationsofDiﬀerentialPrivacy.FoundationsandTrendsR©inTheoreticalComputerScience,9(3–4):211–407,August2014.ISSN1551-305X,1551-3068.doi:10.1561/0400000042.25UnderreviewassubmissiontoTMLRCynthiaDwork,FrankMcSherry,KobbiNissim,andAdamSmith.CalibratingNoisetoSensitivityinPrivateDataAnalysis.InShaiHaleviandTalRabin(eds.),TheoryofCryptography,LectureNotesinComputerScience,pp.265–284,Berlin,Heidelberg,2006.Springer.ISBN978-3-540-32732-5.doi:10.1007/11681878_14.RanGilad-Bachrach,NathanDowlin,KimLaine,KristinLauter,MichaelNaehrig,andJohnWernsing.CryptoNets:ApplyingNeuralNetworkstoEncryptedDatawithHighThroughputandAccuracy.InProceedingsofThe33rdInternationalConferenceonMachineLearning,pp.201–210.PMLR,June2016.OdedGoldreich.FoundationsofCryptography:Volume2,BasicApplications.CambridgeUniversityPress,Cambridge,UK;NewYork,1steditionedition,May2004.ISBN978-0-521-83084-3.ThoreGraepel,KristinLauter,andMichaelNaehrig.MLConﬁdential:MachineLearningonEncryptedData.InTaekyoungKwon,Mun-KyuLee,andDaesungKwon(eds.),InformationSecurityandCryptology–ICISC2012,pp.1–21,Berlin,Heidelberg,2013.Springer.ISBN978-3-642-37682-5.doi:10.1007/978-3-642-37682-5_1.VarunGulshan,LilyPeng,MarcCoram,MartinC.Stumpe,DerekWu,ArunachalamNarayanaswamy,SubhashiniVenugopalan,KasumiWidner,TomMadams,JorgeCuadros,RamasamyKim,RajivRaman,PhilipC.Nelson,JessicaL.Mega,andDaleR.Webster.DevelopmentandValidationofaDeepLearningAlgorithmforDetectionofDiabeticRetinopathyinRetinalFundusPhotographs.JAMA,316(22):2402–2410,December2016.ISSN1538-3598.doi:10.1001/jama.2016.17216.HaniehHashemi,YongqinWang,andMuraliAnnavaram.DarKnight:ADataPrivacySchemeforTrainingandInferenceofDeepNeuralNetworks,October2020.YangsiboHuang,ZhaoSong,KaiLi,andSanjeevArora.InstaHide:Instance-hidingSchemesforPrivateDistributedLearning,February2021.Comment:ICML2020.AngelaJäschkeandFrederikArmknecht.UnsupervisedMachineLearningonEncryptedData.InCarlosCidandMichaelJ.JacobsonJr.(eds.),SelectedAreasinCryptography–SAC2018,pp.453–478,Cham,2019.SpringerInternationalPublishing.ISBN978-3-030-10970-7.doi:10.1007/978-3-030-10970-7_21.PeterKairouz,H.BrendanMcMahan,BrendanAvent,AurélienBellet,MehdiBennis,ArjunNitinBhagoji,KallistaBonawitz,ZacharyCharles,GrahamCormode,RachelCummings,RafaelG.L.D’Oliveira,Hu-bertEichner,SalimElRouayheb,DavidEvans,JoshGardner,ZacharyGarrett,AdriàGascón,BadihGhazi,PhillipB.Gibbons,MarcoGruteser,ZaidHarchaoui,ChaoyangHe,LieHe,ZhouyuanHuo,BenHutchinson,JustinHsu,MartinJaggi,TaraJavidi,GauriJoshi,MikhailKhodak,JakubKonečný,AleksandraKorolova,FarinazKoushanfar,SanmiKoyejo,TancrèdeLepoint,YangLiu,PrateekMittal,MehryarMohri,RichardNock,AyferÖzgür,RasmusPagh,MarianaRaykova,HangQi,DanielRam-age,RameshRaskar,DawnSong,WeikangSong,SebastianU.Stich,ZitengSun,AnandaTheerthaSuresh,FlorianTramèr,PraneethVepakomma,JianyuWang,LiXiong,ZhengXu,QiangYang,Fe-lixX.Yu,HanYu,andSenZhao.AdvancesandOpenProblemsinFederatedLearning,March2021.Comment:PublishedinFoundationsandTrendsinMachineLearningVol4Issue1.See:https://www.nowpublishers.com/article/Details/MAL-083.MarcelKellerandKeSun.SecureQuantizedTrainingforDeepLearning,July2021.Comment:17pages.PattyKostkova,HelenBrewer,SimondeLusignan,EdwardFottrell,BenGoldacre,GrahamHart,PhilKoczan,PeterKnight,CorinneMarsolier,RachelA.McKendry,EmmaRoss,AngelaSasse,RalphSullivan,SarahChaytor,OliviaStevenson,RaquelVelho,andJohnTooke.WhoOwnstheData?OpenDataforHealthcare.FrontiersinPublicHealth,4,2016.ISSN2296-2565.AlexKrizhevsky.CIFAR-10andCIFAR-100datasets.https://www.cs.toronto.edu/~kriz/cifar.html.KristinE.Lauter.PrivateAI:MachineLearningonEncryptedData.TechnicalReport324,2021.Y.Lecun,L.Bottou,Y.Bengio,andP.Haﬀner.Gradient-basedlearningappliedtodocumentrecognition.ProceedingsoftheIEEE,86(11):2278–2324,November1998.ISSN1558-2256.doi:10.1109/5.726791.26UnderreviewassubmissiontoTMLRLiLi,YuxiFan,MikeTse,andKuo-YiLin.Areviewofapplicationsinfederatedlearning.Computers&IndustrialEngineering,149:106854,November2020.ISSN0360-8352.doi:10.1016/j.cie.2020.106854.H.BrendanMcMahan,EiderMoore,DanielRamage,SethHampson,andBlaiseAgüerayArcas.Communication-EﬃcientLearningofDeepNetworksfromDecentralizedData,February2017.Com-ment:Thisversionupdatesthelarge-scaleLSTMexperiments,alongwithotherminorchanges.Inearlierversions,aninconsistencyinourimplementationofFedSGDcausedustoreportmuchlowerlearningratesforthelarge-scaleLSTM.Wererantheseexperiments,andalsofoundthatfewerlocalepochsoﬀersbetterperformance,leadingtoslightlybetterresultsforFedAvgthanpreviouslyreported.KundanMunjalandRekhaBhatia.Asystematicreviewofhomomorphicencryptionanditscontributionsinhealthcareindustry.Complex&IntelligentSystems,May2022.ISSN2198-6053.doi:10.1007/s40747-022-00756-z.KarthikNandakumar,NaliniRatha,SharathPankanti,andShaiHalevi.TowardsDeepNeuralNetworkTrainingonEncryptedData.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops,pp.0–0,2019.NationalInstituteofStandardsandTechnology.AdvancedEncryptionStandard(AES).(U.S.Depart-mentofCommerce,Washington,DC),FederalInformationProcessingStandardsPublication(FIPS)197.https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.197.pdf,2001.ClaudioOrlandi,AlessandroPiva,andMauroBarni.ObliviousNeuralNetworkComputingviaHomo-morphicEncryption.EURASIPJournalonInformationSecurity,2007(1):1–11,December2007.ISSN1687-417X.doi:10.1155/2007/37343.NicolasPapernot,PatrickMcDaniel,AruneshSinha,andMichaelWellman.TowardstheScienceofSecurityandPrivacyinMachineLearning.arXiv:1611.03814[cs],November2016.MarkPhillips.Internationaldata-sharingnorms:FromtheOECDtotheGeneralDataProtectionReg-ulation(GDPR).HumanGenetics,137(8):575–582,August2018.ISSN1432-1203.doi:10.1007/s00439-018-1919-7.RyanPoplin,AvinashV.Varadarajan,KatyBlumer,YunLiu,MichaelV.McConnell,GregS.Corrado,LilyPeng,andDaleR.Webster.Predictionofcardiovascularriskfactorsfromretinalfundusphotographsviadeeplearning.NatureBiomedicalEngineering,2(3):158–164,March2018.ISSN2157-846X.doi:10.1038/s41551-018-0195-0.NuriaRodríguez-Barroso,GoranStipcich,DanielJiménez-López,JoséAntonioRuiz-Millán,EugenioMartínez-Cámara,GerardoGonzález-Seco,M.VictoriaLuzón,MiguelAngelVeganzones,andFranciscoHerrera.FederatedLearningandDiﬀerentialPrivacy:Softwaretoolsanalysis,theSherpa.aiFLframe-workandmethodologicalguidelinesforpreservingdataprivacy.InformationFusion,64:270–292,December2020.ISSN1566-2535.doi:10.1016/j.inﬀus.2020.07.009.KimTheodosandScottSittig.HealthInformationPrivacyLawsintheDigitalAge:HIPAADoesn’tApply.PerspectivesinHealthInformationManagement,18(Winter):1l,December2020.ISSN1559-4122.HanXiao,KashifRasul,andRolandVollgraf.Fashion-MNIST:ANovelImageDatasetforBench-markingMachineLearningAlgorithms,September2017.Comment:Datasetisfreelyavailableathttps://github.com/zalandoresearch/fashion-mnistBenchmarkisavailableathttp://fashion-mnist.s3-website.eu-central-1.amazonaws.com/.AdamYala,HomaEsfahanizadeh,RafaelG.L.D’Oliveira,KenR.Duﬀy,ManyaGhobadi,TommiS.Jaakkola,VinodVaikuntanathan,ReginaBarzilay,andMurielMedard.NeuraCrypt:HidingPrivateHealthDataviaRandomNeuralNetworksforPublicTraining,June2021.HongyiZhang,MoustaphaCisse,YannN.Dauphin,andDavidLopez-Paz.Mixup:BeyondEmpiricalRiskMinimization,April2018.Comment:ICLRcamerareadyversion.ChangesvsV1:ﬁxrepoURL;addablationstudies;addmixup+dropoutetc.27