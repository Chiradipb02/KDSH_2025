Published in Transactions on Machine Learning Research (10/2024)
Towards Provable Log Density Policy Gradient
Pulkit Katdare
University of Illinois at Urbana-Champaign
Anant A. Joshi
University of Illinois at Urbana-Champaign
Katherine Driggs-Campbell
University of Illinois at Urbana-Champaign
Reviewed on OpenReview: https://openreview.net/pdf?id=qIWazsRaTR
Abstract
Policy gradient methods are a vital ingredient behind the success of modern reinforcement
learning. Modern policy gradient methods, although successful, introduce a residual error
in gradient estimation. In this work, we argue that this residual term is significant and
correcting for it could potentially improve sample-complexity of reinforcement learning
methods. To that end, we propose log density gradient to estimate the policy gradient,
which corrects for this residual error term. Log density gradient method computes policy
gradient by utilising the state-action discounted distributional formulation. We first present
the equations needed to exactly find the log density gradient for a tabular Markov Decision
Processes (MDPs). For more complex environments, we propose a temporal difference (TD)
method that approximates log density gradient by utilizing backward on-policy samples.
Since backward sampling from a Markov chain is highly restrictive we also propose a min-max
optimization that can approximate log density gradient using just on-policy samples. We also
prove uniqueness, and convergence under linear function approximation, for this min-max
optimization. Finally, we show that the sample complexity of our min-max optimization to
be of the order of m−1/2, wheremis the number of on-policy samples. We also demonstrate
a proof-of-concept for our log density gradient method on gridworld environment, and
observe that our method is able to improve upon the classical policy gradient method by a
clear margin, thus indicating a promising novel direction to develop reinforcement learning
algorithms that require fewer samples.
1 Introduction
Policygradient(PG)methodsareavitalingredient behindthesuccessofmodernreinforcementlearning(Silver
et al., 2017; John Schulman et al., 2023; Haarnoja et al.; Kakade, 2001). The success of PG methods stems
from their simplicity and compatibility with neural network-based function approximations (Sutton et al.,
1999; Baxter & Bartlett, 2001). Although modern policy gradient methods like PPO and TRPO, have
achieved excellent results in various on-policy tasks (Schulman et al., 2017; 2015), they require extensive
hyper-parameter tuning. Additionally, it has been shown by Ilyas et al. (2020) that the estimation error
between policy gradient estimated by the methods like PPO and the true policy gradient increases significantly
during the training process. Classically in reinforcement learning, two types of problems are well known and
well studied: discounted reward and average reward, see for example Sutton & Barto (2018). It is well known
that for the discounted reward scenario, the Bellman operator is a contraction, and solving the Bellman
equation iteratively is a straightforward numerical procedure. However, the same contraction property fails
to hold for the average reward scenario. Therefore, classical policy gradient methods typically approximate
gradient of the policy using Q-function estimated with discount factor strictly less than 1, which leads to a
1Published in Transactions on Machine Learning Research (10/2024)
0 25 50 75 100 125 150 175 200
Episodes2500
2000
1500
1000
500
0Average Rewards
Classical Policy Gradient (10 x 10)
Log Density Gradient (10 x 10)
Classical Policy Gradient (5 x 5)
Log Density Gradient (5 x 5)
Figure 1: For the average reward scenario, performance of classical policy gradient (blue) algorithm as
compared to log density gradient (green) algorithm over a n×ngridworld environment, for n= 5,10. We
observe that log density gradient algorithm consistently converges to better policy performance. Theoretical
calculated solutions are used for implementation.
error in gradient estimation (Morimura et al., 2010). In this paper, we empirically demonstrate that this
error in indeed significant, see for instance, Figure 1. We further propose a novel algorithm to estimate policy
gradient that corrects for this residual error, which could potentially lead to sample efficient reinforcement
learning, thus enabling their deployment over a wide variety of complex scenarios. We call our method, log
density gradient. We show that log density gradient method can be used to estimate the policy gradient
for all values of discounting factor – including the average reward scenario . Log density gradient method
is based on the average state-action stationary distribution formulation of reinforcement learning, which
allows for the estimation of policy gradient as a multiplication of the gradient of log density and the reward
function (Nachum et al., 2019; Uehara et al.). This separation results in an improved correlation with the
true policy gradient and requires fewer hyperparameters. We show that our method is consistent with the
classical policy gradient theorem (Sutton, 1988) and also prove convergence properties and sample complexity.
Our main contributions are as follows. 1.A novel method to provably calculate policy gradient by using the
average state-action discounted formulation for all values of the discounting factor. We will show that policy
gradient estimated in this manner for average reward scenario will correct for the residual error in policy
gradient estimation, which is widely ignored in empirical implementations of policy gradients (as shown in
Figure 1). 2.A model-free Temporal Difference (TD) method for approximating policy gradient. We provide
proof of contraction as well as convergence. However, there is a major drawback that it requires samples from
the backward Markov chain (described in detail in the paper) which motivates the next contribution. 3.A
min-max optimization which yields the gradient of log density for all values of the discounting factor including
the average reward scenario and a model free TD method to implement it, with proof of convergence. We also
show that this min-max optimization has a closed form solution under linear function class assumptions, thus
enabling their practical use with linear MDP problems (Zhang et al., 2022). We additionally show sample
complexity of the order O(m−1/2)for the projected version of the proposed TD method, where mis the
number of on-policy samples. Our method is competitive with the sample complexity of classical vanilla
policy gradient methods (Yuan et al., 2022).
Section 2 starts with problem formulation and motivation behind this paper. Section 3, discusses prior work
in policy gradient methods, temporal difference methods, and min-max problems in off-policy evaluation
and compares our work with existing works to situate our paper in the literature. Our main contributions
are discussed in detail starting from Section 5 which starts with rigorously defining log density gradient.
Additionally we also propose a TD approach to estimate log density gradient under strict reversibility
assumptions, and we describe the issue caused by this assumption. In section 6, to overcome this issue to
2Published in Transactions on Machine Learning Research (10/2024)
propose a min-max variant that allows us to estimate log density gradient algorithm using empirical samples.
We finally demonstrate a proof-of-concept of our algorithm in Section 7 which shows that log density gradient
can be potentially sample efficient as compared to classical policy gradient methods.
2 Background and Motivation
Notation: we let (·)Tdenote matrix transpose, and let erepresent the vector of ones, the size of which
would be clear from context.
We define Markov Decision Process (MDP) as a 6-tuple of (S,A,P,r,γ,d 0). Here,Sis a finite state space of
the MDP,Ais a finite action space, Pis the transition probability matrix, ris the reward function and d0
is the initial distribution. The reinforcement learning problems is optimise for a policy π:S→ ∆(A)that
maximizesJγ(π), defined as
Jγ(π) := (1−γ)E[∞/summationdisplay
t=0γtr(st,at)|s0∼d0,at∼π(·|st),st+1∼P(·|st,at)],forγ∈[0,1)
J1(π) := lim
T→∞E[1
TT/summationdisplay
t=0γtr(st,at),s0∼d0,at∼π(·|st),st+1∼P(·|st,at)],forγ= 1,
whereγ∈[0,1]is the discounting factor which accounts for the impact of future rewards in present decision
making. When γ= 1,J1(π)the scenario is called the average reward formulation. Most practical problems
in reinforcement learning typically aim to solve for an optimal policy π∗=arg maxπJ1(π)(See Figure 1
Haarnoja et al.). Modern reinforcement learning methods aim to parameterise policy with a set of parameters
θ∈Rn, wherenis the dimensions of the parameter space. We refer to such paremterisation as πθ. This kind
of parameterisation enables us search for optimal set of parameters θ∗instead of a search over S×Awhich
in practice could be very large. We define
θ∗:= arg max
θ∈RnJ1(πθ).
The Q-function Qπθγis commonly used function used to describe the performance of an RL agent. Q-function
calculates the long term (discounted) rewards accumulated by an agent following a fixed policy πθwhile
starting from a state s∈Sand taking an action a∈A
Qπθγ(s,a) :=E[∞/summationdisplay
t=0γtr(st,at)|s0=s,a0=a,at∼πθ(·|st),st+1∼P(·|st,at)] (1a)
=r(s,a) +γEs′∼P(·|s,a),a′∼πθ(·|s′)[Qπθγ(s′,a′)] (1b)
Where, equation 1b is called the Bellman Equation. Bellman equation is popularly used to estimate the Q-
functionusingjustempiricaldatacollectedontheMDP.Q-functionapproximationmethodstypicallyuse γ <1
for stable estimation of the Q-function. We also similarly define value function Vπθγ(s) =Ea∼πθ(·|s)[Qπθγ(s,a)].
Modern RL algorithms generally solve for θ∗by estimating the gradient of policy performance Jγ(πθ)with
respect to policy parameters θ. This is also commonly referred to as the policy gradient theorem ∇θJ1(πθ)
(Sutton et al., 1999) which says
∇θJγ(πθ) =E(s,a)∼dπθγ[Qπθγ(s,a)·∇θlogπθ(a|s)], γ∈[0,1]. (2)
Here,dπθγis the average state-action discounted stationary distribution, which is defined as the cumulative
sum of discounted state-action occupancy across the time horizon.
dπθγ(s,a) := (1−γ)∞/summationdisplay
t=0γtP(st=s,at=a|s0∼d0,at∼πθ(st),st+1∼P(·|st,at),γ < 1) (3a)
dπθ
1(s,a) := lim
T→∞1
TT/summationdisplay
t=0P(st=s,at=a|s0∼d0,at∼πθ(st),st+1∼P(·|st,at),γ= 1).(3b)
3Published in Transactions on Machine Learning Research (10/2024)
In this paper we make a standard assumption that the Markov chain induced by policy πθis ergodic. In
particular, this implies that dπθγ(s,a)>0for all state action pairs (s,a)(Puterman, 2014). In scenarios
where we are trying to optimize for J1(π), estimating the policy gradient becomes difficult. This is because
the Bellman equation cannot be used to estimate Q-function for γ= 1. As a compromise policy gradient for
average reward scenarios are instead approximated by calculating the Q-function for a discounting factor
γ <1, but close to 1, and using that estimate in the policy gradient equation 2
ˆ∇θJ1(πθ) =E(s,a)∼dπθ
1[Qπθ
1(s,a)·∇θlogπθ(a|s)], (4a)
≈E(s,a)∼dπθ
1[Qπθγ(s,a)·∇θlogπθ(a|s)], γ < 1. (4b)
In this paper we argue that the policy gradient calculated in this manner induces a significant residual error,
which keeps on compounding as the reinforcement learning training proceeds even leading to a sub optimal
solution. The following equation, derived in Proposition 2 characterizes that error,
∇θJ1(πθ) =E(s,a)∼dπθ
1[Qπθγ(s,a)·∇θlogπθ(a|s)] + (1−γ)E(s,a)∼dπθ
1[∇θlogdπθ
1(s)·Vπθγ(s)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Residual Error(5)
In this paper, we prove that this residual error is significant. What more, we also propose another method
to exactly obtain the policy gradient, for all values of the discounting factor including γ= 1which we call
as the log density gradient. Our estimation of log density gradient utilises average state-action discounted
distributional formulation of a reinforcement learning problem which re-states Jγ(π)as expectation under
dπθγ(Nachum et al., 2019; Uehara et al.) as
Jγ(π) =E(s,a)∼dπθγ[r(s,a)].
Under this formulation, policy gradient can similarly be obtained using log derivative trick as follows,
∇θJγ(π) =E(s,a)∼dπθγ[∇θlogdπθγ(s,a)·r(s,a)]. (6)
We refer to∇θlogdπθγas the log density gradient. A key advantage of log density gradient is that it would
allow us to approximate policy gradient for average reward scenarios in a provable manner. In this work, we
show that log density gradient can be approximated even under average reward scenarios ( γ= 1).
3 Survey of Related Work and Comparison
In this section we will discuss existing studies in policy gradient methods including the framework of log
density gradient first introduced by Morimura et al. (2010). We also briefly discuss density ratio learning
methods which have been very popular in off-policy evaluation. A short discussion on Temporal Difference
(TD) learning methods may be found in Appendix 9.1.
3.1 Policy Gradient Methods
Literature survey: Policy gradient methods are a widely studied topic in reinforcement learning. One
of the earliest works in this area proposed a closed-form solution for evaluating policy gradients called the
policy gradient theorem (Sutton et al., 1999). Initially implementations for policy gradient methods used
episodic estimates to update policy parameters (Williams, 1992) and GPOMDP (Baxter & Bartlett, 2001).
Unfortunately this way of implementing policy gradient suffered from high variance, thus inhibiting scalability
to large problem spaces (Schulman et al., 2016). To address this problem, actor-critic methods approximate
the Q-function or advantage function using an additional neural network, which are then used to update the
policy (Mnih et al., 2016; Schulman et al., 2016). Furthermore policy gradient methods are also designed to
be compatible with deterministic policies (Lillicrap et al., 2016; Silver et al., 2014). Recently Trust region
methods, such as Trust Region Policy Optimization (TRPO) Schulman et al. (2015) and Proximal Policy
Optimization (PPO) Schulman et al. (2017) have been introduced which update policies while ensuring
monotonic performance improvement. To the best of our knowledge, Log density gradient has only been
4Published in Transactions on Machine Learning Research (10/2024)
discussed in Morimura et al. (2010) in which a TD method to estimate log density gradient for average
reward scenarios by using reversible backward Markov chain is proposed.
Comparison: In our paper, we re-introduce the idea of log density gradient introduced by Morimura et al.
(2010) for estimating gradient in the average reward scenario. Morimura et al. (2010) was also the first
work to find out the residual error in policy gradient approximation (Proposition 2). Additionally, this work
proposes estimating log density gradient specifically for average reward scenarios ( γ= 1) using a TD update,
with additional extensions for linear function approximation.
Our work not only fixes many technical gaps evident in the theory of log density gradient as proposed by
Morimura et al. (2010) but also builds on them to make log density gradient practical. We first define log
density gradient (equation 10) over a range of discounting factor γ∈[0,1], which also includes the average
reward scenario. Using Lemma 2 and 3 we then prove mathematical conditions under which the log density
gradient is unique and can be exactly calculated. We further use this relation to propose a TD form of
updates (equation 14) for log density gradient estimation for all values of discounting factor γ∈[0,1]as
against the average reward scenario proposed by Morimura et al. (2010). In Lemma 4 we further prove
that these TD updates converge to a unique solution for all values of discounting factor γexcept 1. Thus,
effectively demonstrating that TD-updates proposed by Morimura et al. (2010) does not converge to the
true log density gradient, further limiting their use for large scale problems. Additionally, to make log density
gradient estimation viable for practical problems, we propose a min-max optimization approach (equation 17)
that allows us to estimate log density gradient using empirical samples. We also demonstrate that under linear
function approximation settings, this min-max optimization not only has a closed form but also converges to
a unique solution (Theorem 1). Under weighted updates, as proposed in algorithm 1 we also show a bound
on sample complexity of log density gradient estimation of the order of O(1√n).
3.2 Density Ratio Learning
Literature survey: Off-policy evaluation estimates the performance of a target policy πusing an offline
dataset generated by a behavior policy µ(Voloshin et al., 2019; Katdare et al.). This is done by estimating
the average state-action density ratiodπ
γ
dµ, which allows approximation of the target policy’s performance. In
this work, we are primarily interested in the DICE class of off-policy evaluation algorithms (Zhang et al.,
2020b; Nachum et al., 2019; Zhang et al., 2020a). These algorithms typically approximate the divergence
(for somef−divergence of their choice) between the two distributions dπanddµin their convex dual form,
eliminating the need to obtain samples from dπ, which results in a min-max form optimization problem.
Comparison: Inspired by the DICE class of algorithms we too propose a min-max form of estimating the
log density gradient. We show that such a method of estimating log density gradient converges to the true
policy under linear function approximations assumptions. We also show that the sample complexity of such
an estimator is of the order (n−1/2), withnbeing the number of on-policy samples.
4 Roadmap
Before we begin the technical exposition we would like to give a high level idea of the building blocks involved,
and how each step connects to the next.
To that end, we finally propose a min-max version of log density gradient which allows to approximate
∇θlogdπθusing empirical samples. Our experimental further show that this manner of policy gradient
estimation can potentially make reinforcement learning sample efficient, thus helping them scale.
In Section 5, we first propose a model based approach to approximate the log density gradient for tabular
scenarios. To that end, we propose an optimisation problem, the solution to which is the log density gradient.
Then we prove some properties about the solution of the optimisation problem, in particular. Next we suggest
a temporal difference type method to solve the optimisation problem using samples from the controlled
Markov chain. We prove certain uniqueness and convergence properties about the solution obtained from
the TD method. However, the TD method works under a very restrctive assumption: it requires backward
samples from the Markov chain. To circumvent this assumption, we proceed to the next section.
5Published in Transactions on Machine Learning Research (10/2024)
In Section 6, we first convert the original optimisation problem into a min-max problem using Fenchel
duality. Then we propose a TD-type algorithm to solve the min-max problem. This new TD algorithm
needs only samples from the forward Markov chain, and subverts the need for backward samples of the
previous algorithm. We conclude the section by proving results about the solution of the TD algorithm, and
introducing a function approximation framework for the TD method (with proofs of convergence).
5 Log Density Gradient
In this section, we introduce log density gradient approach to policy gradient estimation. By deriving the
gradient of log density, we demonstrate that traditional policy gradient methods are not well-suited at
approximating policy gradient through average reward scenarios. The log density gradient approach addresses
this limitation, allowing us to estimate both the average reward and discounted reward formulations. We first
propose a model-based approach to log density gradient estimation which relies on Bellman-type recursion
to exactly calculate policy gradient. We then extend this to a Temporal difference (TD) version of policy
gradient estimation which will allow us to estimate gradient only using on-policy samples.
5.1 Model Based Log Density Gradient
The log density gradient method attempts to estimate the gradient of log of average state-action discounted
stationery distribution dπθγ. We start by observing that dπθγsatisfies an identity called the Bellman flow
equation Liu et al. (2018); Nachum et al. (2019).
Lemma 1. The average state-action density distribution dπθγsatisfies the Bellman flow equation ∀(s,a,s′)×
S×A×S ,
dπθγ(s′) = (1−γ)d0(s′) +γ/summationdisplay
s,adπθγ(s,a)P(s′|s,a)∀γ∈[0,1] (7)
A detailed proof for the same can be found in the Appendix 9.3. Note that the Bellmam flow equation 7
equation is similar to the classical Bellman contraction except that it is reversed. Unlike the classical Bellman
contraction, the Bellman flow equation shows us the relationship of the next state s′with previous state-action
pair(s,a). In that sense, the Bellman flow equation is reversed and requires samples from a reversed MDP to
estimatedπ
γaccurately. In the following Lemma, we present an optimisation problem, the solution of which is
same as the solution of equation 7. This optimisation problem will be used in later development.
Lemma 2. Choose an arbitrary but fixed λ>0. For allγ∈[0,1]the solution to the following optimisation
is unique and equal to dπθγ,
arg min
w:S→R/summationdisplay
s′/parenleftigg
w(s′)−(1−γ)d0(s′) +γ/summationdisplay
s,aw(s)πθ(a|s)P(s′|s,a)/parenrightigg2
+λ
2(/summationdisplay
sw(s)−1)2(8)
Detailed proof can be found in the appendix 9.4. Intuitively speaking, the termλ
2(/summationtext
sw(s)−1)2is redundant
forγ <1, and only becomes useful for average reward scenarios wherein γ= 1thus helping ensure uniqueness.
Recall that we are interested in estimating the log density gradient, which is the gradient of the log of density
with the policy parameters ∇θlogdπθγ. Similar to the Bellman flow equation equation 7, the log density
gradient also follows a similar recursion which can be obtained by taking the gradient of the Bellman flow
equation 7 with respect to the policy parameters θas follows:
dπθγ(s′)∇θlogdπθγ(s′) =γ/summationdisplay
s,adπθγ(s,a)P(s′|s,a)∇θlogdπθγ(s,a). (9)
Multiplying both sides by πθ(a′|s′)and recalling that dπθγ(s,a) =dπθγ(s)πθ(a|s), we obtain
dπθγ(s′,a′)(∇θlogdπθγ(s′,a′)−∇θlogπ(a′|s′)) =γ/summationdisplay
s,adπθγ(s,a)∇θlogdπθγ(s,a)P(s′|s,a)πθ(a′|s′)(10)
6Published in Transactions on Machine Learning Research (10/2024)
Furthermore, if we have exact knowledge of the transition matrix P, we can exactly calculate the log density
gradient by solving for the following optimization problem
min
w:S×A→ Rn/braceleftbigg
E(s′,a′)∼dπθγ∥ν(s′,a′)∥2+λ
2/vextenddouble/vextenddouble/vextenddoubleE(s,a)∼dπθγ[w(s′,a′)]/vextenddouble/vextenddouble/vextenddouble2/bracerightbigg
(11)
ν(s′,a′) :=dπθγ(s′,a′)(w(s′,a′)−∇θlogπθ(a′|s′))−γ/summationdisplay
s,adπθγ(s,a)P(s′|s,a)πθ(a′|s′)w(s,a)
Lemma 3. Forλ>0, the solution to equation 11 is unique and equal to ∇θlogdπθγfor allγ∈[0,1].
We describe the proof for this Lemma in the appendix 9.5. Note that the constraintλ
2∥E(s,a)∼dπθγ[w(s′,a′)]∥2
is redundant for γ <1and only becomes useful for average reward scenarios γ= 1. For a finite state and
action space, the proof follows from the fact that the optimal solution to equation 11 requires solving for
a linear equation A·w=b. The remaining part of the proof demonstrates that this matrix Ais invertible
for∀γ∈[0,1]. It is worth reiterating that, once we have an estimate ∇θlogdπθγ, we can use this estimate to
approximate the policy gradient using equation 6. We will now recall two important properties of log density
gradient.
Proposition 1. The policy gradient method as recalled from equation 6
∇θJγ(π) =E(s,a)∼dπθγ[∇θlogdπθγ(s,a)·r(s,a)].
is exactly equal to the classical policy gradient (Sutton et al., 1999) recalled from equation 2
∇θJγ(πθ) =E(s,a)∼dπθγ[Qπθγ(s,a)·∇θlogπθ(a|s)], γ∈[0,1].
Detailed proof for this proposition can be found in Appendix 9.2. In essence this results shows us that
log density gradient approach to policy gradient estimates the exact same gradient but using a different
formulation. Additionally log density gradient formulation allows us to estimate policy gradient for average
reward formulations. It is well known that, the classical policy gradient theorem is in-sufficient to estimate
gradient for average reward scenarios. This is because the Bellman equation for reward case ( γ= 1) is
not a contraction, making it harder to approximate Q-function using neural networks. As a compromise,
reinforcement learning practioners typically approximate policy gradient for average reward scenario using
a discounting factor slightly less than 1 and use that to update policy. Our next result shows that such a
way to approximating policy gradient ignores a residual error, which is not necessarily small. Correcting
for this residual error, can not improve policy gradient estimation but also potentially help improve sample
complexity of reinforcement learning algorithms.
Proposition 2. The following identity (recalled from equation 5) is true
∇θJ1(πθ) =E(s,a)∼dπθ
1[Qπθγ(s,a)·∇θlogπθ(a|s)] + (1−γ)E(s,a)∼dπθ
1[∇θlogdπθ
1(s)·Vπθγ(s)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Residual Error
Proof.From the definition of log density gradient equation 6 we have ∇θJ1(πθ) =E(s,a)∼dπθ
1[∇θlogdπθ
1(s,a)·
r(s,a)]. Letγ <1, and we use the Bellman equation 1b to obtain
∇θJ1(πθ) =E(s,a)∼dπθ
1[∇θlogdπθ
1(s,a)·(Qπθγ(s,a)−γEs′∼P(·|s,a)[Vπθγ(s′)])] (12a)
=E(s,a)∼dπθ
1[(∇θlogdπθ
1(s) +∇θlogπθ(a|s))·(Qπθγ(s,a)−γEs′∼P(·|s,a)[Vπθγ(s′)])] (12b)
=E(s,a)∼dπθ
1[Qπθγ(s,a)·∇θlogπθ(a|s)] + (1−γ)Es∼dπθ
1[∇θlogdπθ
1(s,a)·Vπθ(s)] (12c)
Here, we go from equation 12a to 12b by utilizing ∇θlogdπθγ(s,a) =∇θlogdπθγ(s) +∇θlogπθ(a|s). We finally
go from 12b to 12c by using a key identity of log density gradient equation 10.
Note that the first term in equation 12b is the practical instantiation of classical policy gradient theorem,
while the second term (1−γ)Es∼dπθ
1[∇θlogdπθ
1(s)·Vπθγ(s)]being the residual error. This completes the
proof.
7Published in Transactions on Machine Learning Research (10/2024)
To recap, in this section, we propose a Bellman-like relationship for the log density gradient, which further
allows us estimate the gradient using a simple but known optimization. Next, we propose a Temporal
difference (TD) type method to estimate log density gradient using samples.
5.2 Temporal Difference Log Density Gradient
In this section, we begin to estimate log density gradient algorithm using just on-policy samples. To that end,
as a first step, we propose a temporal difference approach to log density gradient estimation. We refer to
our method as TD(0) method.1To get an update equation for our TD(0) method, consider a re-arranged
version of equation 10 for log density gradient,
∇θlogdπθγ(s′,a′) =∇θlogπθ(a′|s′) +γ/summationdisplay
s,adπθγ(s,a)P(s′|s,a)πθ(a′|s′)
dπθγ(s′,a′)∇θlogdπθγ(s,a) (13)
As mentioned earlier, this recursion is backwards as compared to the Bellman contraction. In order to see
that, we will first define the backwards distribution Pb:S×A→ ∆(S,A)such that
Pb(s,a|s′,a′):=dπθγ(s,a)P(s′|s,a)πθ(a′|s′)
dπθγ(s′,a′)=dπθγ(s,a)Pπθ(s′,a′|s,a)
dπθγ(s′,a′).
This backwards distribution Pbestimates the probability of the previous state (s,a)given the next state
(s′,a′). The mathematical form of the backwards distribution is thus, consequence of Bayes’ rule. The
summation in equation 13 therefore becomes an expectation under Pb
∇θlogdπθγ(s′,a′) =∇θlogπθ(a′|s′) +γE(s,a)∼Pb(·|s,a)[logdπθγ(s,a)].
The log density gradient is therefore said to follow a backward recursion and it requires samples from backward
conditional probability Pbto estimate log density gradient2. Assuming that we have samples from this
backward distribution Pbwe now propose TD updates to estimate log density gradient wfor all discounting
factorγ∈[0,1],
w(s′,a′)←w(s′,a′) +α[γw(s,a) +g(s′,a′)−w(s′,a′)], (14)
with (s′,a′)∼dπθγ,(s,a)∼Pb(·|s′,a′)andg(s′,a′) :=∇θlogπθ(a′|s′). It is worth noting that this TD(0)
update is a generalization of Morimura et al. (2010), who only propose these updates for average reward
scenario (γ= 1).
Next we define an operator Yγto capture the behaviour of the update rule equation 14 after taking expectation
as,
(Yγ·w)(s′,a′) :=γE(s,a)∼Pb(·|s′,a′)[w(s,a)] +g(s′,a′).
We can write this in matrix form as follows,
Yγ·W=γD−1
πθP⊤
πθDπθW+G (15)
where,W∈R|S|·|A|×nis the matrix with every row corresponding to w(s,a)for each state-action pair (s,a).
Similarly,G∈R|S|·|A|×nhas its rows as∇θlogπθfor each state-action pair. Let Pπθ,Dπθ∈R|S|·|A|×|S|·|A|
where (Pπθ)((s,a),(s′,a′))=Pπθ(s′,a′|s,a)andDπθis a diagonal matrix whose every element correspond to dπθγ
for each state-action pair. We use this matrix form for the operator Yγin the proof of the following lemma.
Lemma 4. Letw0∈∆(S,A)be an arbitrary initial guess. Let wk=Yγ·wk−1for all natural numbers k≥1.
Forγ∈[0,1), the operator Yγis a contraction, and {wk}k≥0converges to a unique fixed point ∇θlogdπθγ.
1There is a family of TD methods which incorporate prior traces in a discounted manner called TD( λ) (Tesauro, 1992).
2Although for γ= 1we can use samples from Pas well (Morimura et al., 2010)
8Published in Transactions on Machine Learning Research (10/2024)
Detailed proof of Lemma 4 can be found in Appendix 9.6. The key idea behind this Lemma is to show that
successive application of the above mentioned TD updates will help us estimate the log density gradient
accurately. In-fact we also show that these TD updates are a contraction, thus effectively ensuring convergence.
Extension of Lemma 4 to linear function approximation, and proof of convergence for the same, can be found
in the Appendix 9.7.
Although TD methods are known to converge, they still suffer from two problems. One, the access to samples
from backward conditional probability. Two, scalability to large problem spaces. We attempt to solve both of
these problems in the next section where we propose a min-max optimization procedure for estimating the
log density gradient.
6 Min-Max Log Density Gradient
In this section, we propose min-max optimization approach to evaluate log density gradient for all values of
the discounting factor including the average reward scenario ( γ= 1). Doing so removes the need for samples
from backward distribution, which was limiting scalability of log density gradient method described in the
previous section. Min-max optimizations also allow us to use a large variety of function classes like neural
networks to approximate log density gradient.
Letusreturntothelossfunctionthatweinitiallyproposeinequation11. Classicalmachinelearningalgorithms
usually posit loss function as Empiricial Risk Minimization (ERM), which allows us to approximate the loss
using samples from that distribution. In order to bring our key optimization in an ERM form, consider a
modified form of the optimization proposed in equation 11 (the modification is that δ(s′,a′)is divided by
dπθγ(s′,a′)where the ergodicity assumption ensures this operation is well defined),
arg min
w∈S×A→ RnE(s′,a′)∼dπθγ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleν(s′,a′)
dπθγ(s′,a′)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
+λ
2∥E(s,a)∼dπθγ[w(s,a)]∥2(16)
ν(s′,a′) :=dπθγ(s′,a′)(w(s′,a′)−∇θlogπθ(a′|s′))−γ/summationdisplay
s,adπθγ(s,a)P(s′|s,a)π(a′|s′)w(s,a)
The denominator term dπθγis added to ensure that the final optimization form can be written in form of an
expectation, which we shall see soon. This allows us to use samples to approximate our optimization function.
We also add the regularization termλ
2∥E(s,a)∼dπθγ[w(s,a)]∥2which allows us to estimate log density gradient
even for all values of the discounting factor including average reward scenarios ( γ= 1).
Since, equation equation 16 is a re-weighting of equation 11 with the (dπθγ(s,a))−1. the optimal solution for
the both the equation is the same, thus ensuring uniqueness of the optimization.
By exploiting the Fenchel-duality, we can re-write this optimization in the minimax form (Rockafellar, 2015;
Zhang et al., 2020b) as follows,.
arg min
w:S×A→ Rdmax
f:S×A→ Rd,τ∈RdLγ(w,f,τ ) :=/braceleftbigg
E(s′,a′)∼dπθγ[f(s′,a′)·w(s′,a′)]
−E(s′,a′)∼dπθγ[f(s′,a′)·∇θlogπθ(a′|s′)]−γE(s,a)∼dπθγ[Es′∼P(·|s,a),a′∼πθ(·|s′)[f(s′,a′)]·w(s,a)]
−1
2E(s,a)∼dπθγ[∥f(s′,a′)∥2] +λ(τ·E(s,a)∼dπθγ[w(s,a)]−1
2∥τ∥2)/bracerightbigg
(17)
In many cases searching over all function mappings S×A→ Rdis not possible, hence we search over a
smaller and more tractable function classes W,Fand the aim is to approximate
∇θlogdπθγ≈arg min
w∈Wmax
f∈F,τ∈RnLγ(w,f,τ ).
Such a practical consideration allows us to use different types of function approximators like linear function
approximation, neural networks, and reproducible kernel Hilbert spaces (RKHS).
9Published in Transactions on Machine Learning Research (10/2024)
In the remainder of this section, we will focus on linear function approximation, and provide an update rule
to solve equation 17 under linear function approximation. For that we choose a feature map Φ :S×A→ Rd
and parameters α,β∈Rd×nthat need to be learnt, so that we can approximate the optima of equation 17,
w∗(s,a)andf∗(s,a)withαTΦ(s,a), andβTΦ(s,a)respectively, for each state action pair (s,a). The update
rule is
δt= ΦtΦT
t−γΦt(Φ′
t)T(18a)
αT
t+1=αT
t−εt(βTδt+λ(τΦT
t)) (18b)
βT
t+1=βT
t+εt(αT
tδt−gtΦT
t−βT
tΦtΦT
t) (18c)
τt+1=τt+εt(λ(αT
tΦt−τt)) (18d)
where, Φt:= Φ(st,at)is the feature encountered at time t,gt:=∇θlogπθ(at|st), and Φ′
t:= Φt(s′
t,a′
t)
for(st,at)∼dπθγ,s′
t∼P(·|st,at),a′
t∼πθ(a′
t|s′
t). We first re-write the updates in equation 18 in form of
dt= [αt,βt,τT
t]so that the updates can be written in matrix form dt+1=dt+εt(Gt+1dt+ht+1), where,
Gt+1,ht+1are as follows,
Gt+1:=
0−At−λΦt
At−Ct 0
λΦT
t 0−λ
, ht+1:=
0
−Bt
0

andAt:= (ΦtΦT
t−γΦt(Φ′
t)T),Bt:= ΦgT
t,Ct:= ΦtΦT
t. We can calculate the expectation for each of these
matrices as follows,
G:=Ep[Gt+1] =
0−A−λΨDπθe
A C 0
λeTDπθΨT0−λ
, h :=E(s,a)∼dπθγ[ht+1] =
0
−B
0

Here, each column of Ψ∈R|S|·|A|×nis the feature vector Φ(s,a), for each (s,a)∈S×Aande∈Rnis a vector
of 1’s at every element. We can similarly write A= ΨDπθ(I−γPπθ)ΨT,B= ΨDπθGT,C= ΨDπθΨTand
Ep[·] :=E(s,a)∼dπθγ,s′∼P(·|s,a),a′∼πθ(·|s′)[·]. We can now prove the convergence of linear function approximation
under the following key assumptions.
Assumption 1. 1. The matrix Ψhas linearly independent columns.
2. The matrix Ais non-singular or the regularizer λ>0.
3. The feature matrix Φhas uniformly bounded second moments.
Theorem 1. Under the assumptions 1, the update equation 18 converges in probability to a unique solution.
That is, limt→∞dt=G−1hin probability.
The detailed proof is provided in Appendix 9.8. The proof is similar to (Zhang et al., 2020b, Theorem 2)
and invokes theorem 2.2 Borkar & Meyn (2000).
We provide a sample complexity analysis for a projected version of the update rule equation 18. To that end,
we propose Algorithm 1 called the Projected Log Density Gradient. We choose closed, bounded and convex
setsX⊂Rd×n,Y⊂Rd×n,Z⊂R1×nand define a projection operator ΠX,ΠY,ΠZthat project our variables
αt,βt,τtontoX,Y,Zrespectively. Moreover, we choose a learning rate {εt}m
t=1where we run the algorithm
formsteps. The details of the choice of learning rate are found in Appendix 9.9.
Theorem 2. Under assumptions 1 for (¯α,¯β,¯τ)obtained from Algorithm 1 after msteps, the optimality gap
ϵg(¯α¯β,¯τ)(defined below) is bounded with probability 1−δas follows,
ϵg(¯α,¯β,¯τ) := max
(β,τ)∈Y×ZL(¯α,β,τ )−min
α∈XL(α,¯β,¯τ)≤C0/radicalbigg
5
m(8 + 2 log2
δ)w.p. 1−δ
where,C0is a constant which is a function of the sets X,Y,Z, and the second moment of Φ.
We present the proof of this result in appendix 9.9. This result essentially shows us that the upper-bound
for log density gradient estimation requires O(1√m)(wheremis the number of steps the algorithm runs for)
samples to learn an accurate estimation.
10Published in Transactions on Machine Learning Research (10/2024)
Algorithm 1 Projected Log Density Gradient
1:fort= 1,2,...,mdo:
2:δt= ΦtΦT
t−γΦt(Φ′
t)T
3:αT
t+1= ΠX(αT
t−εt(βTδt+λ(τΦT
t)))
4:βT
t+1= ΠY(βT
t+εt(αT
tδt−gtΦT
t−βT
tΦtΦT
t))
5:τt+1= ΠZ(τt+εt(λ(αT
tΦt−τt)))
6:Return ¯α,¯β,¯τ
Where, ¯α=/summationtextn
i=1εiαi/summationtextn
i=0εi,¯β=/summationtextn
i=1εiβi/summationtextn
i=0εi,¯τ=/summationtextn
i=1εiτi/summationtextn
i=0εi
Algorithm 2 Linear Log Density Gradient
1:fort= 1,2,...,mdo:
2:α∗= arg min α‘:∈Rd×nmaxβ:∈Rd×n,τ∈R1×nLγ(w,f,τ )(equation 17)
3:∇θJ1(πθ) =E(s,a)∼dπθ
1[∇θlogdπθ
1(s,a)·r(s,a)]
4:Returnθ
7 Experiments
In this section, we present a proof of concept for our log density gradient estimation on two sets of
environments 5×5and3×3gridworld environment (Towers et al., 2023). For the gridworld experiments, we
approximate log density gradient by using linear function approximation (Algorithm 2). Here, the features
areϕ:S×A→ R|S|·|A|such that it maps every state to the corresponding standard basis vector. Our results
for5×5are in Figure 2 and for 3×3in Figure 3.
We compare our algorithm against 3 different baselines. The first is theoretical log density gradient as
described in Lemma 3. The second baseline implements REINFORCE algorithm, which is the practical
rendition of the policy gradient theorem (Williams, 1992). The third is theoretical policy gradient method
which exactly computes the classical policy gradient theorem, as in equation 4b (Sutton et al., 1999).
We observe in that both log density gradient approaches are more sample efficient than both policy gradient
approaches. This is because policy gradient methods approximate the gradient for average reward scenarios
(γ= 1) by estimating a Q-function for a discounting factor less than 1. Moreover, we observe that our
method tends to outperform REINFORCE with much reduced variance. Our approach is always very close
in performance to the theoretical log density gradient which serves to validate correctness of our algorithm.
In5×5gridworld we also observe our algorithm to outperforms theoretical log density gradient. This is
because, theoretical log density gradient suffers from some numerical computation issues arising from average
reward scenarios.
8 Conclusion and Future Work
We present log density gradient algorithm that estimates policy gradient using state-action discounted
formulation of a reinforcement learning problem. We observe that policy gradient estimated in this manner,
corrects for a residual error common in many reinforcement learning tasks. We show that with a known
model, we can exactly calculate the gradient of the log density by solving two sets of linear equations. We
further propose a TD(0) algorithm to implement the same, but it needs samples from the backward Markov
chain, which becomes too restrictive. Therefore, we propose a min-max optimization that estimates log
density gradient using just on-policy samples. We not only prove theoretical properties like convergence and
uniqueness but also experimentally demonstrate that our method is sample efficient as compared to classical
policy gradient methods like REINFORCE. This approach looks promising, and further studies of log density
gradient will focus on scaling their performance to complex tasks.
Limitations: Currently most of our experimental results require Linear function approximation to estimate
logdensitygradient. Thisisbecauseunderlinearfunctionapproximationconditions, ourmin-maxoptimization
equation 17 becomes a quadratic program and thus has a closed form solution. To scale log density gradient
11Published in Transactions on Machine Learning Research (10/2024)
0 25 50 75 100 125 150 175 200
Episodes0255075100125150175200Average Timesteps
0 25 50 75 100 125 150 175 200
Epsiodes1000
800
600
400
200
0Average Rewards102030
150
100
50
Min-Max Log Density Gradient (ours) Theoretical Log Density Gradient REINFORCE Theoretical Policy Gradient
Figure 2: For 5×5gridworld, comparison of Log Density Gradient algorithms (in light green) as compared
to REINFORCE (light red), theoretical policy gradient (gray) and theoretical log density gradient (blue). We
observe that our empirical algorithm comfortably outperforms the other baselines.
0 20 40 60 80 100 120 140
Episodes020406080100120Average Timesteps
0 20 40 60 80 100 120 140
Epsiodes300
250
200
150
100
50
0Average Rewards51015202530
50
40
30
20
10
0Min-Max Log Density Gradient (ours) Theoretical Log Density Gradient REINFORCE Theoretical Policy Gradient
Figure 3: For 3×3gridworld, comparison of Log Density Gradient algorithms (in light green) as compared
to REINFORCE (light red), theoretical policy gradient (gray) and theoretical log density gradient (blue). We
observe that our empirical algorithm comfortably outperforms the other baselines.
algorithms to complex cases requires evaluating high quality features which will also allow us to estimate
gradient accurately Zhang et al. (2022).
12Published in Transactions on Machine Learning Research (10/2024)
References
Baxter, J. and Bartlett, P. L. Infinite-horizon policy-gradient estimation. J. Artif. Int. Res. , 2001.
Borkar, V. S. and Meyn, S. P. The o.d. e. method for convergence of stochastic approximation and
reinforcement learning. 2000.
Boyan, J. A. Least-squares temporal difference learning. In ICML, pp. 49–56, 1999.
Bradtke, S. J. and Barto, A. G. Linear least-squares algorithms for temporal difference learning. Mach.
Learn., 1996.
Gelada, C. and Bellemare, M. G. Off-policy deep reinforcement learning by bootstrapping the covariate shift.
InThe Thirty-Third AAAI Conference on Artificial Intelligence, AAAI , 2019.
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on
Machine Learning, ICML 2018 .
Hallak, A. and Mannor, S. Consistent on-line off-policy evaluation. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017 , 2017.
Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., and Madry, A. A closer look at
deep policy gradients. In 8th International Conference on Learning Representations, ICLR 2020 , 2020.
John Schulman, Barret Zoph, C. K. et al. Chatgpt: Optimizing language models for dialogue, 2023.
Kakade, S. M. A natural policy gradient. In Dietterich, T. G., Becker, S., and Ghahramani, Z. (eds.),
Advances in Neural Information Processing Systems 14 , 2001.
Katdare, P., Liu, S., and Campbell, K. D. Off environment evaluation using convex risk minimization. In
2022 International Conference on Robotics and Automation, ICRA 2022 .
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. In 4th International Conference on Learning Representations,
ICLR, 2016.
Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M. Finite-sample analysis of proximal
gradient TD algorithms. In Meila, M. and Heskes, T. (eds.), Proceedings of the Thirty-First Conference on
Uncertainty in Artificial Intelligence, UAI 2015 .
Liu, Q., Li, L., Tang, Z., and Zhou, D. Breaking the curse of horizon: Infinite-horizon off-policy estimation.
InAdvances in Neural Information Processing Systems 31 , 2018.
Maei, H. R. Gradient temporal-difference learning algorithms. In Ph.D thesis , 2011.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D., and Kavukcuoglu,
K. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33nd International
Conference on Machine Learning, ICML , 2016.
Morimura, T., Uchibe, E., Yoshimoto, J., Peters, J., and Doya, K. Derivatives of logarithmic stationary
distributions for policy gradient reinforcement learning. Neural Comput. , 2010.
Nachum, O., Chow, Y., Dai, B., and Li, L. Dualdice: Behavior-agnostic estimation of discounted stationary
distribution corrections. In Advances in Neural Information Processing Systems 32 , 2019.
Nemirovski, A., Juditsky, A. B., Lan, G., and Shapiro, A. Robust stochastic approximation approach to
stochastic programming. SIAM J. Optim. , 2009.
Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming . John Wiley and
Sons. 2014.
13Published in Transactions on Machine Learning Research (10/2024)
Robbins, H. and Monro, S. A Stochastic Approximation Method. The Annals of Mathematical Statistics , 22
(3):400 – 407, 1951. doi: 10.1214/aoms/1177729586. URL https://doi.org/10.1214/aoms/1177729586 .
Rockafellar, R. T. Convex Analysis . Princeton University Press, 2015.
Schulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. Trust region policy optimization. CoRR,
abs/1502.05477, 2015. URL http://arxiv.org/abs/1502.05477 .
Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P. High-dimensional continuous control using
generalized advantage estimation. In 4th International Conference on Learning Representations, ICLR ,
2016.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms.
CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347 .
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. A. Deterministic policy gradient
algorithms. In Proceedings of the 31th International Conference on Machine Learning, ICML , 2014.
Silver, D., Schrittwieser, J., Simonyan, K., and thers. Mastering the game of go without human knowledge.
Nat., 2017.
Sutton, R. S. Learning to predict by the methods of temporal differences. Mach. Learn. , 1988.
Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction . The MIT Press, second edition,
2018. URL http://incompleteideas.net/book/the-book-2nd.html .
Sutton, R. S., McAllester, D. A., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural Information Processing Systems , 1999.
Tesauro, G. Practical issues in temporal difference learning. Mach. Learn. , 1992.
Towers, M., Terry, J. K., Kwiatkowski, A., Balis, J. U., Cola, G. d., Deleu, T., Goulão, M., Kallinteris, A.,
KG, A., Krimmel, M., Perez-Vicente, R., Pierré, A., Schulhoff, S., Tai, J. J., Shen, A. T. J., and Younis,
O. G. Gymnasium, March 2023. URL https://zenodo.org/record/8127025 .
Uehara, M., Huang, J., and Jiang, N. Minimax weight and q-function learning for off-policy evaluation. In
Proceedings of the 37th International Conference on Machine Learning, ICML 2020 .
Voloshin, C., Le, H. M., Jiang, N., and Yue, Y. Empirical study of off-policy policy evaluation for reinforcement
learning. 2019.
Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Mach. Learn. , 1992.
Yuan, R., Gower, R. M., and Lazaric, A. A general sample complexity analysis of vanilla policy gradient. In
International Conference on Artificial Intelligence and Statistics, AISTATS 2022 , Proceedings of Machine
Learning Research, 2022.
Zhang, R., Dai, B., Li, L., and Schuurmans, D. Gendice: Generalized offline estimation of stationary values.
CoRR, abs/2002.09072, 2020a. URL https://arxiv.org/abs/2002.09072 .
Zhang, S., Liu, B., and Whiteson, S. Gradientdice: Rethinking generalized offline estimation of stationary
values.CoRR, abs/2001.11113, 2020b. URL https://arxiv.org/abs/2001.11113 .
Zhang, T., Ren, T., Yang, M., Gonzalez, J., Schuurmans, D., and Dai, B. Making linear mdps practical via
contrastive representation learning. In International Conference on Machine Learning, ICML , Proceedings
of Machine Learning Research, 2022.
14