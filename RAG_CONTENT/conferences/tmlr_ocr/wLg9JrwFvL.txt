Under review as submission to TMLR
Normalized/Clipped SGD with Perturbation for Diﬀeren-
tially Private Non-Convex Optimization
Anonymous authors
Paper under double-blind review
Abstract
By ensuring diﬀerential privacy in the learning algorithms, one can rigorously mitigate
the risk of large models memorizing sensitive training data. In this paper, we study two
algorithms for this purpose, i.e., DP-SGD and DP-NSGD, which ﬁrst clip or normalize
per-sample gradients to bound the sensitivity and then add noise to obfuscate the exact
information. We analyze the convergence behavior of these two algorithms in the non-
convex empirical risk minimization setting with two common assumptions and achieve a
rateO/parenleftbigg
4/radicalBig
dlog(1/δ)
N2/epsilon12/parenrightbigg
of the gradient norm for a d-dimensional model, Nsamples and (/epsilon1,δ)-
DP, which improves over previous bounds under much weaker assumptions. Speciﬁcally, we
introduce a regularizing factor in DP-NSGD and show that it is crucial in the convergence
proof and subtly controls the bias and noise trade-oﬀ. Our proof deliberately handles the
per-sample gradient clipping and normalization that are speciﬁed for the private setting.
Empirically, we demonstrate that these two algorithms achieve similar best accuracy while
DP-NSGD is comparatively easier to tune than DP-SGD.
1 Introduction
Modern applications of machine learning strongly rely on training models with sensitive datasets, including
medical records, real-life locations, browsing histories and so on. These successful applications raise an
unavoidable risk of privacy leakage, especially when large models are shown to be able to memorize training
data (Carlini et al., 2020). Diﬀerential Privacy (DP) is a powerful and ﬂexible framework (Dwork et al.,
2006b) to quantify the inﬂuence of each individual and reduce the privacy risk. Speciﬁcally, we study the
machine learning problem in the formalism of minimizing empirical risk privately:
min
x∈Rdf(x),Eξ[/lscript(x,ξ)] =1
NN/summationdisplay
i=1/lscript(x,ξi), (1)
where the objective f(x)is an empirical average of losses evaluated at each data point ξandξis sampled
uniformly from the given dataset {ξi,1≤i≤N}¬.
In order to provably achieve the privacy guarantee, one popular algorithm is diﬀerentially private stochastic
gradient descent or DP-SGD for abbreviation, which clips per-sample gradients with a preset threshold and
perturbs the gradients with Gaussian noise at each iteration. Formally, given a set of gradients {g(i),i∈S⊂
[N]}whereScan be thought of as a set of indices of gradients in a mini-batch and g(i)=∇x/lscript(x,ξi)is the
gradient computed with some data point i, a threshold c>0, a learning rate η>0and a noise multiplier σ,
the updating rule of DP-SGD goes from xto the following
x+=x−η/parenleftBigg
1
|S|/summationdisplay
i∈S¯h(i)g(i)+¯z/parenrightBigg
, (2)
¬Our method later uses uniform sub-sampling without replacement to construct mini-batches. Thus, the second equality in
1 is exact.
1Under review as submission to TMLR
where ¯z∼N(0,c2σ2Id)is an isotropic Gaussian noise and ¯h(i)= min{1,c//bardblg(i)/bardbl}is the per-sample clipping
factor. Intuitively speaking, the per-sample clipping procedure controls the inﬂuence of one individual. DP-
SGD (Abadi et al., 2016) has made a benchmark impact in deep learning with diﬀerential privacy, which is
also referred to as the gradient perturbation approach. Despite being applied into many ﬁelds (Hassan et al.,
2019; Ji et al., 2014), it has also been extensively studied from many aspects, e.g., convergence (Bassily
et al., 2014; Yu et al., 2020), privacy analysis (Abadi et al., 2016), adaptive clipping threshold (Asi et al.,
2021; Andrew et al., 2019; Pichapati et al., 2019), hyperparameter choices (Li et al., 2021; Papernot et al.,
2020a; Papernot & Steinke, 2021; Mohapatra et al., 2021) and so forth (Bu et al., 2020; 2021; Papernot
et al., 2020b).
Another natural option to achieve diﬀerential privacy is normalized gradient with perturbation , which we
coin “DP-NSGD”. It normalizes per-sample gradients to control individual contribution and then adds noise
accordingly. The update formula is the same as (2) except replacing ¯h(i)with aper-sample normalization
factor
h(i)=1
r+/bardblg(i)/bardbl, (3)
and replacing ¯zwithz∈N(0,σ2Id)since each sample’s inﬂuence is normalized to be 1. In (3), we introduce
aregularizer r>0, whichnotonlyaddressestheissueofill-conditioneddivisionbutalsocontrolsthebiasand
noise trade-oﬀ as we will see in the analysis. A concurrent work of ours Bu et al. (2023) also proposed exactly
the same method, but their theoretical results are based on diﬀerent assumptions, while the experimental
ﬁndings are mutually supportive.
AnintuitivethoughtthatfavorDP-NSGDisthattheclippingthresholdishardtochooseduetothechanging
statistics of the gradients over the training trajectory (Andrew et al., 2019; Pichapati et al., 2019). In more
details, the injected Gaussian noise η¯zin (2) is proportional to the clipping threshold cand this noise
component would dominate over the gradient component/summationtext¯h(i)g(i)/|S|, when the gradients /bardblg(i)/bardbl/lessmuchcare
getting small as optimization algorithm iterates, thus hindering the overall convergence. DP-NSGD aims to
alleviate this problem by replacing ¯h(i)in (2) with a per-sample gradient normalization factor h(i)in (3),
thus enhancing the signal component g(i)when it is small.
It is obvious that both clipping and normalization introduce bias­that might prevent the optimizers from
converging (Chen et al., 2020; Zhao et al., 2021). Most of previous works on the convergence of DP-SGD
(Bassily et al., 2014; Asi et al., 2021; Yu et al., 2020) neglect the eﬀect of such biases by assuming a global
gradient upper bound of the problem, which does not exist for the cases of deep neural network models.
Chen et al. (2020) have made a ﬁrst attempt to understand gradient clipping, but their results strongly rely
on a symmetric assumption which is not that realistic.
In this paper, we consider both the eﬀect of per-sample normalization/clipping and the injected Gaus-
sian perturbation in the convergence analysis. If properly setting the hyperparameters, we achieve an
O/parenleftbigg
4/radicalBig
dlog(1/δ)
N2/epsilon12/parenrightbigg
convergence rate of the gradient norm for the general non-convex objective with a d-
dimensional model, Nsamples and (/epsilon1,δ)-DP, under only two weak assumptions (L0,L1)-generalized smooth-
ness (Zhang et al., 2020b) and (τ0,τ1)-bounded gradient variance. These assumptions are very mild as they
allow the smoothness coeﬃcient and the gradient variance growing with the norm of gradient, which is widely
observed in the setting of deep learning.
Our contributions are summarized as follows.
•For thediﬀerentially private empirical risk minimization , we establish the convergence rate of the
DP-NSGD and the DP-SGD algorithms for general non-convex objectives with (L0,L1)-smoothness
condition and (τ0,τ1)-bounded gradient variance, and explicitly characterizes the bias of the per-
sample clipping or normalization. In particular, our utility bounds match the best convergence rates
that are available, even under weakened conditions.
­Here biasmeans that the expected descent direction diﬀers from the true gradient ∇f.
2Under review as submission to TMLR
Table 1: Expected gradient norm bounds (the smaller, the better) for non-convex empirical risk minimization with/without (/epsilon1,δ)-DP
guarantee. All the algorithms assume certain bound on gradient noises, which may be diﬀerent from one another. Notations: N,Tand
dare the number of samples, iterations and parameters, respectively. All bounds should be read as O(·)andlog1
δis omitted.
Algorithm SmoothnessCondition on gradient
estimate gBias handled
in analysis/diamondmath Gradient norm bound
SGD
Ghadimi & Lan (2013)L E/bardblg−∇f/bardbl2≤V N/A O/parenleftBig
1√
T+√
V
4√
T/parenrightBig
Clipped SGD
Zhang et al. (2020b)(L0,L1) E/bardblg−∇f/bardbl2≤V√O/parenleftBig
V2
√
T+V3/2
4√
T/parenrightBig
DP-NormFedAvg
Das et al. (2021)L quasar-Convex√O/parenleftBig
DX√
d
N/epsilon1+Ei/bardblx∗
i−x∗/bardbl/parenrightBig†
DP-GD
Wang et al. (2019a)L /bardblg/bardbl≤τa.s. × O/parenleftbig
4/radicalbig
d
N2/epsilon12/parenrightbig
DP-SRM
Wang et al. (2023)L /bardblg/bardbl≤τa.s. × O/parenleftbig
4/radicalbig
d
N2/epsilon12/parenrightbig
DP-GD/RMSprop/Adam
Zhou et al. (2020)L /bardblg/bardbl≤τa.s. × O/parenleftbig
4/radicalbig
d
N2/epsilon12/parenrightbig
DP-(N)SGD
Bu et al. (2023) (concurrent)LE/bardblg−∇f/bardbl2≤V
gcentrally symmetric around its mean√O/parenleftbig
4/radicalbig
d
N2/epsilon12/parenrightbig
DP-(N)SGD (Ours) (L0,L1)/bardblg−∇f/bardbl≤τ0+τ1/bardbl∇f/bardbla.s.√O/parenleftbig
4/radicalbig
d
N2/epsilon12/parenrightbig‡
†More remarks on this line are packed in Appendix A.
‡To be preciser, Corollary 3.5 suggests DP-SGD achieving exactly this rate, but Corollary 3.3 also adds another non-vanishing term for
DP-NSGD.
/diamondmathWhen analyzing the utility theoretically, most previous works in the literature of diﬀerentially private optimization do not address the
inﬂuence of gradient clipping/regularization. Accordingly, they usually assume a much stronger condition that /bardblg/bardbl≤τa.s.
•For the DP-NSGD algorithm, we introduce a regularizing factor which turns out to be crucial in the
convergence analysis and induces interesting trade-oﬀ between the bias due to normalization and
the decaying rate of the upper bound.
•We identify one key diﬀerence in the proofs of DP-NSGD and DP-SGD. As the gradient norm
approaches zero, DP-NSGD cannot guarantee the function value to drop along the expected descent
direction, and introduce a non-vanishing term that depends on the regularizer and the gradient
variance.
•We evaluate the empirical performance of DP-NSGD and DP-SGD respectively on deep models with
(/epsilon1,δ)-DP and show that they both can achieve comparable accuracy but the former is easier to tune
than the later.
Thepaperisorganizedasfollows. AfterintroducingtheproblemsetupinSection2, wepresentthealgorithms
and theorems in Section 3 and show numerical experiments on vision and language tasks in Section 4. We
make concluding remarks in Section 5.
2 Problem Setup
2.1 Notations
Denote the private dataset as D={ξi,1≤i≤N}. The loss/lscript(x,ξi)is deﬁned for every model parameter
x∈Rdand data record ξi. In the sequel,/bardblx/bardblis denoted as the /lscript2norm of a vector x∈Rd, without other
speciﬁcations. From time to time, we interchangeably use ∇x/lscript(x,ξi)andg(i)to denote the gradient of /lscript(·,·)
w.r.t. xevaluated at (x,ξi). We are given an oracle to draw a mini-batch Bof data for each iteration. Our
target is to minimize the empirical average loss (1) satisfying (/epsilon1,δ)-diﬀerential privacy .
Deﬁnition 2.1 ((/epsilon1,δ)-DP, (Dwork et al., 2006a)) .A randomized mechanism Mguarantees (/epsilon1,δ)-
diﬀerentially privacy if for any two neighboring input datasets D∼D/prime(D/primediﬀer from Dby substituting
one record of data) and for any subset of output Sit holds that Pr [M(D)∈S]≤e/epsilon1Pr[M(D/prime)∈S] +δ.
3Under review as submission to TMLR
Besides, we also deﬁne the following notations to illustrate the bound we derived. We write f(·) =O(g(·)),
f(·) = Ω(g(·))to denotef(·)/g(·)is upper or lower bounded by a positive constant. We also write f(·) =
Θ(g(·))to denote that f(·) =O(g(·))andf(·) = Ω(g(·)). Throughout this paper, we use Eto represent
taking expectation over the randomness of optimization procedures: drawing noisy gradients estimates g
and adding extra Gaussian perturbation z. In the meantime, Ektakes conditional expectation given xk, the
k-th iterate of our optimization algorithm.
In the settings of non-convex loss functions f(x)inx, we measure the utility of some algorithm via bounding
the expected minimum gradient norm E[min 0≤k<T/bardbl∇f(xk)/bardbl]. If we want to measure utility via bounding
function values, an extra convex condition or its weakened versions are essential. See Section 2.3 for more
discussion on the last point.
2.2 Assumptions on Smoothness and Variance
Deﬁnition 2.2. We say that a continuously diﬀerentiable function f(x)is(L0,L1)-generalized smooth, if
for all x,y∈Rd, we have/bardbl∇f(x)−∇f(y)/bardbl≤(L0+L1/bardbl∇f(x)/bardbl)/bardblx−y/bardbl.
Firstly appearing in Zhang et al. (2020b), a similar condition is derived via empirical observations that
the operator norm /bardbl∇2f(x)/bardblof the Hessian matrix increases with the gradient norm /bardbl∇f(x)/bardblin training
language models. If we set L1= 0, then Deﬁnition 2.2 turns into the usually assumed L-smoothness. Our
ﬁrst assumption below comprises of this relaxed notion of smoothness, Deﬁnition 2.2, and that f(x)is lower
bounded.
Assumption 2.1. We assume that f(x)is(L0,L1)-generalized smooth, as deﬁned in Deﬁnition 2.2. We
also set the function value to be lower bounded, f∗= inf x∈Rdf(x). For notational convenience, write
Df,f(x0)−f∗<∞as the gap in function value between the initialization x0and the lower bound.
Many optimization studies necessitate the initialization point x0, to be suﬃciently close to the optimal point
x∗, in terms of Euclidean distance; that is, they require an upper bound on DX,/bardblx0−x∗/bardbl. However, this
assumption may obscure some dependence on the dimension, as DXtends to scale with the dimension of
the model. In our context, such assumptions are not necessary.
Moreover, tohandlethestochasticityingradientestimates, weemploythefollowing almost sure upperbound
on the gradient variance as another assumption.
Assumption 2.2. For all x∈Rd,E[∇x/lscript(x,ξ)] =∇f(x). Furthermore, there exists τ0>0and0≤τ1<1,
such that it holds /bardbl∇x/lscript(x,ξ)−∇f(x)/bardbl≤τ0+τ1/bardbl∇f(x)/bardblalmost surely for ξdrawn from the data distribution.
Since we are focusing on the problem of empirical risk minimization, Assumption 2.2 turns out to be a
condition onto the dataset D={ξi,1≤i≤N}that there exist constants (τ0,τ1)uniform in xsuch that
/bardbl∇x/lscript(x,ξi)−∇f(x)/bardbl≤τ0+τ1/bardbl∇f(x)/bardblfor all 1≤i≤N. We note that similar almost-surely bounds on
the gradient noises have been assumed in Wang et al. (2019a; 2023); Zhou et al. (2020). In comparison,
Assumption 2.2 is a weakened version: it allows the deviation /bardbl∇x/lscript(x,ξ)−∇f(x)/bardblgrows with respect to
the gradient norm /bardbl∇f(x)/bardbl, which matches practical observation more closely. This almost-surely type
of assumption seems unavoidable for analyzing DP optimization algorithms otherwise the sensitivity of an
individual is out of control. One alternative option in literature is adding light-tailed conditions on the
distribution of∇x/lscript(x,ξ)(Fang et al., 2022).
Outside the context of empirical risk minimization, we can also ﬁnd meaningful examples for which Assump-
tion 2.2 holds and is more reasonable to the concrete setting.
Example 2.3. We provide a natural setting in which our Assumption 2.2 provably holds. Consider a linear
modelv=wTx∗+uand MSE loss /lscript(x,v,w) =1
2|v−wTx|2, wherev∈Ris the response and w∈Rd
comprises of predictors. Here udenotes mean-zero uncorrelated noise E[uw] = 0,E[u] = 0. By normalizing,
we assume|v| ≤Cvand/bardblw/bardbl ≤Cware both bounded, and E[wwT]has positive minimum eigenvalue
λmin>0.
In this setup, we ﬁnd
∇x/lscript(x,v,w) = (wTx−v)w=wwT(x−x∗)−uw
4Under review as submission to TMLR
and∇L(x) =Ev,w∇x/lscript(x,v,w) =E[wwT](x−x∗). Consequently, we will have
/bardbl∇x/lscript(x,v,w)/bardbl≤Cw(Cw/bardblx∗/bardbl+Cv) +C2
w/bardblx−x∗/bardbl≤τ0+τ1/bardbl∇L(x)/bardbl
almost surely for v,w, whereτ0=Cw(Cw/bardblx∗/bardbl+Cv)andτ1=C2
w/λmin.
2.3 Related Works
Apart from the literature mentioned in Section 1, there are a large body of works related to our study. We
brieﬂy review part of them as follows.
Private Deep Learning: Many papers have made attempts to theoretically analyze gradient perturbation
approaches in various settings, including (strongly) convex (Chaudhuri & Monteleoni, 2009; Wang et al.,
2017; Kuru et al., 2020; Yu et al., 2020; Asi et al., 2021; Kamath et al., 2022; Wang et al., 2022) or non-
convex (Wang et al., 2019a; 2022; Zhou et al., 2020; Wang et al., 2023) objectives. However, these papers
did not take gradient clipping into consideration, and simply treat DP-SGD as SGD with extra Gaussian
noise. Chen et al. (2020) made a ﬁrst attempt to understand gradient clipping, but their results strongly
rely on a symmetric assumption which is considered as unrealistic, which is used in a concurrent work (Bu
et al., 2023) of ours to establish the convergence of DP-NSGD.
As for algorithms involving normalizing , Das et al. (2021) studied DP-NormFedAvg, a client-level DP opti-
mizer. More detailed remarks are packed up in Appendix A.
These mentioned results are hard to compare due to the diﬀerences of the settings, assumptions and algo-
rithms. WepresentapartoftheminTable1. FulldiscussionsandcomparisonsarepackedupinAppendixA.
Non-Convex Stochastic Optimization: Ghadimi & Lan (2013) established the convergence of random-
ized SGD for non-convex optimization. The objective is assumed to be L-smooth and the randomness on
gradients is assumed to be light-tailed with factor V. We note that the rate O(V/4√
T)has been shown to
be optimal in the worst-case under the same condition (Arjevani et al., 2019).
Outside the privacy community, understanding gradient normalization and clipping is also crucial in analyz-
ing adaptive stochastic optimization methods, including AdaGrad (Duchi et al., 2011), RMSProp (Hinton
et al., 2012), Adam (Kingma & Ba, 2014) and normalized SGD (Cutkosky & Mehta, 2020). However,
with the average of a mini-batch of gradient estimates being clipped, this batchgradient clipping diﬀers
greatly from the per-sample gradient clipping in the private context. Zhang et al. (2020b) and Zhang et al.
(2020a) showed the superiority of batch gradient clipping with and without momentum respectively under
(L0,L1)-smoothness condition for non-convex optimization. Due to a strong connection between clipping
and normalization, we also assume this relaxed condition in our analysis. We further explore this condition
for some speciﬁc cases in great details. Zhang et al. (2020c) studied SGD with gradient clipping under
heavy-tailed condition for gradient estimation. Cutkosky & Mehta (2021) found that a ﬁne integration of
clipping, normalization and momentum, can overcome heavy-tailed gradient variances via a high-probability
bound. Jin et al. (2021) discovered that normalized SGD with momentum is also distributionally robust.
Signiﬁcant advancements have been made in understanding convergence rates for non-convex objectives from
the perspective of diﬀerentially private Riemannian optimization, as highlighted by Han et al. (2022); Utpala
et al. (2022a;b) in their improved diﬀerentially private frameworks. Notably, many problems are, in fact,
geodesically convex, and these frameworks oﬀer utility bounds on the expected empirical excess risk.
3 Normalized/Clipped Stochastic Gradient Descent with Perturbation
In this section, we ﬁrst present the algorithms DP-NSGD and DP-SGD, and their privacy guarantees. Then
we establish their convergences, respectively, with proof sketches. In the end, we analyze the biases of these
algorithms and verify them with experiments.
3.1 Algorithms and Their Privacy Guarantees
5Under review as submission to TMLR
Algorithm 1 Diﬀerentially Private Normalized Stochastic Gradient Descent, DP-NSGD
Input:initial point x0; number of epochs T; default learning rates ηk; mini batch size B; noise multiplier
σ; regularizer r.
fork= 0toT−1do
Draw a mini-batch Skof sizeBand compute individual gradients gi
kat point xkwherei∈Sk.
Fori∈Sk, compute per-sample normalizing factor
h(i)
k=1
r+/bardblg(i)
k/bardbl.
Draw zk∼N(0,σ2Id)and update the parameters by
xk+1=xk−ηk/parenleftBigg
1
B/summationdisplay
i∈Skh(i)
kg(i)
k+zk/parenrightBigg
.
end for
Since no literature formally displays DP-NSGD in a centralized setting, we present it in Algorithm 1. Com-
pared to the usual SGD update, DP-NSGD contains two more steps: per-sample gradient normalization, i.e.,
multiplying g(i)
kwithh(i)
k, and noise injection, i.e., adding zk. The normalization well controls each sample’s
contribution to the update and the noise obfuscates the exact information.
The well-known DP-SGD (Abadi et al., 2016) replaces the normalization with clipping, i.e., replacing h(i)
k
with ¯h(i)
k= min{1,c//bardblgk/bardbl}and replacing zkwith ¯zk∼N(0,c2σ2Id)in Algorithm 1. DP-SGD introduces a
new hyper-parameter, the clipping threshold c.
To facilitate the common practice in private deep learning, we adopt uniform sub-sampling without replace-
mentfor both theory and experiments, instead of Poisson sub-sampling originally adopted in DP-SGD
(Abadi et al., 2016). Due to this diﬀerence, the following lemma shares the same expression as Theorem 1 in
Abadi et al. (2016), but requires a new proof. Deferred in Appendix E, this simple proof combines ampliﬁed
privacy accountant by sub-sampling in Bun et al. (2018) with the tight composition theorem for Renyi DP
(Mironov et al., 2019).
Lemma 3.1 (Privacy Guarantee) .Provided that B < 0.1N, there exists absolute constants c1,c2>0so
that DP-SGD and DP-NSGD are (/epsilon1,δ)-diﬀerentially private for any /epsilon1<c 1B2T/N2andδ >0if we choose
σ≥c2√
Tlog(1/δ)
N/epsilon1.
3.2 Convergence Guarantee of DP-NSGD
For the DP-NSGD (Algorithm 1), we have the following convergence result.
Theorem 3.2. Suppose that the objective f(x)satisﬁes Assumption 2.1 and 2.2. Given any noise multiplier
σand a regularizer r>τ 0, we run DP-NSGD (Algorithm 1) using constant learning rate
η=/radicalBigg
2
(L1(r+τ0) +L0)Tdσ2, (4)
with suﬃciently many iterations T(larger than some constant determined by (σ2,d,L,τ,r ), as speciﬁed in
Lemma C.2). We can obtain the following upper bound on gradient norm
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤C/parenleftBigg
4/radicalbigg
(Df+ 1)2r3dσ2
T+4/radicalbigg
1
Tr3dσ2/parenrightBigg
+8(r+ 2τ0)τ2
0
r(r+τ0)(1−τ1)3, (5)
whereCis a constant depending on the gradient variance coeﬃcients (τ0,τ1)and the objective smoothness
coeﬃcients (L0,L1).
6Under review as submission to TMLR
Theorem 3.2 is a general convergence for normalized SGD with perturbation. To achieve (/epsilon1,δ)-diﬀerential
privacy, we can choose proper noise multiplier σand iterations T.
Corollary 3.3. Under the same conditions of Theorem 3.2, we use σ=c2/radicalBig
Tlog1
δ/(N/epsilon1)withc2from
Lemma 3.1 and set T≥ O (N2/epsilon12/(r3dlog1
δ)). If we have suﬃciently many samples (larger than some
constant determined by (/epsilon1,δ,d,L,τ,r,B ), as speciﬁed in Lemma C.3), there holds the following privacy-
utility trade-oﬀ
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤C/prime4/radicalbigg
dr3log(1/δ)
N2/epsilon12+8(r+ 2τ0)τ2
0
r(r+τ0)(1−τ1)3, (6)
whereC/primeis a constant depending on the gradient variance coeﬃcients (τ0,τ1), the objective smoothness
coeﬃcients (L0,L1), the function value gap Dfand the batch size B.
There are two major obstacles to prove this theorem. One is to handle the normalized gradients, which is
solved by carefully using rand dividing the range of /bardbl∇f(xk)/bardblinto two cases. The other is to handle the
Gaussian perturbation z, whose variance σ2could even grow linearly with T. This is solved by setting the
learning rate ηproportionally to 1/σin (4). Combining two steps together, we reach the privacy-utility
trade-oﬀ in Corollary 3.3.
Proof Sketch of Theorem 3.2. We ﬁrstly establish a descent inequality as in Lemma B.2 via exploiting the
(L0,L1)-generalized smooth condition in Assumption 2.1,
Ek[f(xk+1)]−f(xk)≤−ηEk[/angbracketlefthk∇f(xk),gk/angbracketright]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A+L0+L1/bardbl∇f(xk)/bardbl
2η2/parenleftBig
dσ2+Ek/bardblhkgk/bardbl2/parenrightBig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B.
In the above expression, we use Ekto denote taking expectation of {g(i)
k,i∈Sk}andzkconditioned on the
past, especially xk. Next, in Lemma C.1, we upper bound the second order term Bby a constantO(η2)
plus a term like ηEk/bracketleftbig
hk/bardbl∇f(xk)/bardbl2/bracketrightbig
, which is compatible to A. In order to ﬁnd simpliﬁed lower bound for
A, we separate the time index {0,1,···,T−1}into two casesU:=/braceleftBig
0≤k<T :/bardbl∇f(xk)/bardbl≥τ0
1−τ1/bracerightBig
and
Uc. Speciﬁcally, in Lemma B.3, we ﬁnd that for k∈U, the ﬁrst order term AisΩ(η/bardbl∇f(xk)/bardbl)(see (17)
in Appendix B); for k /∈U, the ﬁrst order term AisΩ(η(/bardbl∇f(xk)/bardbl2/r−τ3
0/r2))(see (18) in Appendix B).
Then our result follows from summing up descent inequalities and scaling ηdeliberately.
There is rich literature investigating the convergence properties of normalized gradient methods in the non-
private non-convex optimization setting. These results heavily rely on the following inequality to control the
amount of descent
−/angbracketleftbigg
∇f(xk),gk
/bardblgk/bardbl/angbracketrightbigg
≤−/bardbl∇f(xk)/bardbl+ 2/bardblgk−∇f(xk)/bardbl. (7)
Based on this inequality, one unavoidably needs to control the error term /bardblgk−∇f(xk)/bardblwell to have the
overall convergence. In practice, You et al. (2019) used large batch size B∼O(T)to reduce the variance.
However, this trick cannot be applied in the private setting due to per-sample gradient processing , e.g.,
clipping or normalization. Cutkosky & Mehta (2020) and Jin et al. (2021) use momentum techniques with a
properly scaled weight decay and obtain a convergence rate E/bardbl∇f(xT)/bardbl=O(1/4√
T), which is comparable
with the usual SGD in the non-convex setup (Ghadimi & Lan, 2013). However, momentum techniques do not
apply well in the private setting either, because we only have access to previous descent directions with noise
due to the composite diﬀerential privacy requirement. As far as we know, there is no successful application
of momentum in the private community, either practically or theoretically. Our analysis is able to deal with
this issue, mainly because clipping / normalization is applied in a per-sample manner.
In this paper, we view the regularizer ras a tunable hyperparameter, and make our upper bound decay
as fast as possible O(1/4√
T)by tuningr. However, due to the restrictions imposed by privacy protection,
we are unable to properly exploit large batch size or momentum to reduce variance as in previous works,
thus leaving a strictly positive term O(τ2
0/r)in the right hand side of Theorem 3.2 and Corollary 3.3.
7Under review as submission to TMLR
Another observation is that rtrades oﬀ between the non-vanishing bound O(τ2
0/r)and the decaying term
O/parenleftBig
4/radicalBig
dr3log1
δ/(N2/epsilon12)/parenrightBig
.
3.3 Convergence Guarantee of DP-SGD
We now turn our attention to DP-SGD with per-sample gradient clipping, whose convergence is given by
the following theorem.
Theorem 3.4. Suppose that the objective f(x)satisﬁes Assumption 2.1 and 2.2. Given any noise multiplier
σ>0and any clipping threshold c>2τ0/(1−τ1), we run DP-SGD using constant learning rate
η=/radicalBigg
2
(L1(c+τ0) +L0)Tdc2σ2, (8)
with suﬃciently many iterations T(larger than some constant determined by (σ2,d,L,τ,c )respectively,
speciﬁed in Lemma D.2). We can obtain the following upper bound on gradient norms
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤C/parenleftBigg
4/radicalbigg
(Df+ 1)2c3dσ2
T+4/radicalBigg
1
Tc2(c+τ0)dσ2/parenrightBigg
, (9)
where we employ a constant Conly depending on the gradient variance coeﬃcients (τ0,τ1)and the objective
smoothness coeﬃcients (L0,L1).
Combining Theorem 3.4 and Lemma 3.1, we have a characterization for (/epsilon1,δ)-DP.
Corollary 3.5. Under the same conditions of Theorem 3.4, we use σ=c2/radicalbig
Tlog(1/δ)/(N/epsilon1)withc2from
Lemma 3.1 and set T≥ O (N2/epsilon12/(c3dlog1
δ)). If we have suﬃciently many samples (larger than some
constant determined by (/epsilon1,δ,d,L,τ,c,B ), as speciﬁed in Lemma D.3), there holds the following privacy-
utility trade-oﬀ
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤C/prime4/radicalbigg
dc3log(1/δ)
N2/epsilon12, (10)
whereC/primeis a constant depending on the gradient variance coeﬃcients (τ0,τ1), the objective smoothness
coeﬃcients (L0,L1), the function value gap Dfand the batch size B.
By comparing Corollary 3.5 and Corollary 3.3, the most signiﬁcant distinction of DP-SGD from DP-NSGD
is that clipping does not induce a non-vanishing term O(τ2
0/r)as what we obtained in Corollary 3.3. This
distinction is because A:=ηE[/angbracketleft∇f,hg/angbracketright]and¯A:=ηE/bracketleftbig
/angbracketleft∇f,¯hg/angbracketright/bracketrightbig
behave quite diﬀerently in some cases (see
details in Lemmas B.3 & B.5 of Appendix B).
Speciﬁcally, when /bardbl∇f/bardblis larger than τ0/(1−τ1), we know/angbracketleft∇f,g/angbracketright≥0. Therefore, the following ordering
c
c+/bardblg/bardbl≤min/braceleftbigg
1,c
/bardblg/bardbl/bracerightbigg
≤2c
c+/bardblg/bardbl(11)
guarantees Aand ¯Ato be equivalent to Ω(η/bardbl∇f/bardbl), where (11) can be argued by considering two cases
c >/bardblg/bardblandc≤/bardblg/bardblseparately. When the gradient norm /bardbl∇f/bardblis small, the inner-product /angbracketleft∇f,g/angbracketrightcould
be of any sign, and we can only have A= Ω/parenleftbig
η/parenleftbig
/bardbl∇f/bardbl2/r−τ3
0/r2/parenrightbig/parenrightbig
and ¯A= Ω/parenleftbig
η/parenleftbig
/bardbl∇f/bardbl2/parenrightbig/parenrightbig
instead. As
Acontrols the amount of descent within one iteration for DP-NSGD, the non-vanishing term appears. We
here provide a toy example of the distribution of gto further illustrate the diﬀerent behaviors of Aand¯A.
Example 3.1. Consider a simple distribution on e,g−∇f:
P/parenleftbigg
e=τ0∇f
/bardbl∇f/bardbl/parenrightbigg
=1
3,P/parenleftbigg
e=−τ0∇f
2/bardbl∇f/bardbl/parenrightbigg
=2
3.
This distribution certainly satisﬁes Assumption 2.2 with τ1= 0. We calculate the explicit formula of Afor
this case,
A=η(/bardbl∇f/bardbl3+ (3r+τ0/2)/bardbl∇f/bardbl2−τ2
0/bardbl∇f/bardbl/2)
3(r+τ0+/bardbl∇f/bardbl)(r+τ0/2−/bardbl∇f/bardbl).
8Under review as submission to TMLR
For/bardbl∇f(xk)/bardbl≤τ2
0/(10r), we have A<0. The function value may not decrease along E[hg]in this case and
the learning curves are expected to ﬂuctuate adversely. This example also supports that the lower bound
Ω/parenleftbig
η/parenleftbig
/bardbl∇f/bardbl2/r−τ3
0/r2/parenrightbig/parenrightbig
onAis optimal. In contrast, for the clipping operation, as long as /bardbl∇f/bardbl≤c−τ0,
we have ¯h≡1and therefore ¯A=η/bardbl∇f/bardbl2.
As pointed out in the Example 3.1, the training trajectories of DP-NSGD ﬂuctuate more adversely than
DP-SGD, since Acan be of any sign while ¯Astays positive. This diﬀerence is also observed empirically (see
Figure 1) that the training loss of normalized SGD with r= 0.01(close to normalization) ﬂuctuates more
than that of the clipped SGD with c= 1®.
Comparison to a concurrent work. In the meantime of submitting our research to ArXiv, another group
of researchers Bu et al. (2023) also achieved the same order of convergence rates but under markedly diﬀerent
assumptions. As displayed in Table 1, they use a stronger and more conventional notion of L-smoothness
for the objective function. On the other hand, the assumptions on the gradient noise cannot be compared
directly between theirs and ours. Our Assumption 2.2 is “stricter" than theirs in the sense that g−∇fis
bounded almost surely, whereas Bu et al. (2023) only require a bounded second moment. However, they
make an additional distributional assumption that gis centrally symmetric around its mean, which ﬁrstly
appears in Chen et al. (2020) with supportive empirical evidence.
3.4 On the Biases from Normalization and Clipping
In this section, we further discuss how gradient normalization andclipping aﬀect the overall convergences
of the private algorithms. The inﬂuence is two-folded: one is that clipping/normalization induces bias, i.e.,
the gap between true gradient ∇fand clipped/normalized gradient; the other is that the added Gaussian
noise for privacy may scale with the regularizer rand the clipping threshold c.
The Induced Bias. When writing the objective as an empirical average f(x) =/summationtext
i/lscript(x,ξi)/N, the true
gradient is∇f(x) =1
N/summationtextN
i=1g(i). Then both expected descent directions of Normalized SGD and Clipped
SGD
E[hg] =1
NN/summationdisplay
i=1g(i)
r+/bardblg(i)/bardbl,E[¯hg] =1
NN/summationdisplay
i=1g(i)min/braceleftbigg
1,c
/bardblg(i)/bardbl/bracerightbigg
,
deviate from the true gradient ∇f(x). This means that the normalization or the clipping induces biases
compared with the true gradient. A small regularizer ror a small clipping threshold cinduces large biases,
while a large rorccould reduce such biases. This can be seen from the training loss curves of diﬀerent
values ofcandrin Figure 1, whose implementation details are in Section 4. Although theoretically the bias
itself hinders convergence, the biasesaﬀect the accuracy curves diﬀerently for clipped SGD and normalized
SGD. The accuracy curves of clipped SGD vary with the value of cwhile the value of rmakes almost no
impact on the accuracy curves of normalized SGD. This phenomenon extends to the private setting (look at
the accuracy curves in Figure 2).
A qualitative explanation would be as follows. After several epochs of training, well-ﬁtted samplesξ(those
alreadybeenclassiﬁedcorrectly)yieldsmallgradients gandnot-yet-ﬁtted samplesξ/prime(thosenotbeencorrectly
classiﬁed) yield large gradients g/prime. Typically, cis set on the level of gradient norms, while ris for regularizing
the division. As the training goes, more and more samples are ﬁtted well enough and their gradients
would become small (He et al., 2023). While both normalized SGD and clipped SGD tend to amplify the
signiﬁcance of instances with small gradients ( well-ﬁtted samples ), the ampliﬁcation eﬀect of normalized SGD
is generally more pronounced than that of clipped SGD unless the clipping threshold is set to be extremely
small. Moreover, normalized SGD achieves comparable accuracy but incurs higher loss than clipped SGD.
We call for a future investigation towards understanding this phenomenon thoroughly. Speciﬁc to the setting
r≈0,c= 1in Figure 1, normalized SGD normalizes all g(i)to be with a unit norm, while clipped SGD
would not change gradients with small norms.
From a theoretical perspective, to give a ﬁner-grained analysis of the bias, imposing further assumptions to
controlγmay be a promising future direction. For example, Chen et al. (2020) made an attempt towards
®c= 1makes the magnitude of clipped SGD similar as normalized SGD and hence the comparison is meaningful.
9Under review as submission to TMLR
0 20 40 60 80 100
epoch0.00.51.01.52.02.5lossclip 1.0 & lr 0.5
clip 10.0 & lr 0.5
020406080100
accuracyClipped SGD
0 20 40 60 80 100
epoch0.00.51.01.52.02.5loss
regularizer 0.01 & lr 0.5
regularizer 1.0 & lr 0.5
020406080100
accuracyNormalized SGD
Figure 1: Left: Training loss and training accuracy curves of Clipped SGD. Right: Training loss and training
accuracy curves of Normalized SGD. Both are trained with ResNet20 on CIFAR10 task.
0 20 40 60 80 100
epoch0.00.51.01.52.02.5lossclip 1.0 & lr 0.5
clip 10.0 & lr 0.5
102030405060708090100
accuracyDP-SGD
0 20 40 60 80 100
epoch0.00.51.01.52.02.5lossregularizer 0.01 & lr 0.5
regularizer 1.0 & lr 0.5
020406080100
accuracyDP-NSGD
Figure 2: Left: Training loss and training accuracy curves of DP-SGD. Right: Training loss and training
accuracy curves of DP-NSGD. Task: ResNet20 on CIFAR10 with /epsilon1= 8,δ=1e-5.
10Under review as submission to TMLR
this aspect, but their assumption is a bit artiﬁcial and not intuitive. Sankararaman et al. (2020) proposed
a concept gradient confusion , deﬁned as γ=−min{/angbracketleftg(i),g(j)/angbracketright:i/negationslash=j}to approximately quantify how
per-sample gradients alignto each other.
The Added Noises for Privacy Guarantee . For gradient clipping, the added noise (Gaussian perturba-
tion) ¯z∼N (0,c2σ2Id)is proportional to c, while for gradient normalizing z∼N (0,σ2Id)keeps invariant
withr. This suggests that when tuning DP-SGD, ηneeds to vary with c, in order to control the noise
component η¯zin each update. In contrast, DP-NSGD is robust under diﬀerent scales of r, and thus is easier
to tune intuitively. Extensive experiments in Section 4 support this intuition empirically.
4 Experiments
This section conducts experiments to demonstrate the eﬃcacy of Algorithm 1 and compare the behavior of
DP-SGD and DP-NSGD empirically.
4.1 Tuning Vision Models
One example for the proof of concept is training a ResNet20 (He et al., 2016) with CIFAR-10 dataset. As in
literature (Yousefpour et al., 2021; Davody et al., 2020; Yu et al., 2020), we replace all batch normalization
layers with group normalization (Wu & He, 2018) layers for easily computing the per-sample gradients. The
non-private accuracy for CIFAR-10 is 90.4%. We compare the performances of DP-NSGD and DP-SGD
with a wide range of hyper-parameters and diﬀerent learning rate scheduling rules. All experiments can be
run on a single Tesla V100 with 16GB memory. The ResNet20 has 270K trainable parameters.
Hyperparameter choices. We ﬁrst ﬁx the privacy budget /epsilon1={2.0,4.0,8.0},δ= 10−5, which corresponds
to setting the noise multiplier σ={3.6,2.0,1.2}for the case of batch size 1000 and number of epochs 100
with Rényi diﬀerential privacy accountant (Abadi et al., 2016; Mironov et al., 2019). There are tighter
privacy accountants (Gopi et al., 2021) that can save /epsilon1for the same noise multiplier. We then ﬁx the weight
decay to be 0 and use the classical learning rate scheduling strategy that multiplies the initial lrwith 0.1
at epoch 50 and 0.01at epoch 75 respectively. The hyperparameters to tune are the initial learning rate
lrand the clip threshold cfor DP-SGD, where lrtakes values{0.05,0.1,0.2,0.4,0.8,1.6,3.2}andctakes
values{0.1,0.4,1.6,6.4,12.8}. At the same time, the hyperparameters to tune for DP-NSGD are the initial
learning rate lrand the regularizer r, wherelrtakes values{0.05,0.1,0.2,0.4,0.8,1.6,3.2}andrtakes values
{0.0001,0.001,0.01,0.1,1.0}. We compare the validation accuracy of DP-SGD and DP-NSGD via heatmaps
of the above hyperparameter choices in Figure 3. We can see that the performance of DP-NSGD is rather
stable for the regularizer taking values from 10−4to1.0and it is mostly aﬀected by the learning rate. This
is in sharp contrast with the case of DP-SGD where the performance depends on both the learning rate and
the clip threshold in a complicated way. This suggests that it may be easier to tune the hyperparameters
for DP-NSGD than that for DP-SGD, which may help save the privacy budget for tuning hyperparameters
(Papernot & Steinke, 2021).
We also run the above setting with the cyclic learning rate scheduling with min-lr = 0.02and max-lr = 1.0.
The best accuracy number are of DP-NSGD and DP-SGD can be as good as 66, which is comparable with
the number achieved with model architecture modiﬁcation in Papernot et al. (2020b).
4.2 Fine-tuning Large Language Models
We use the pretrained RoBERTa model (Liu et al., 2019)¯, which has 125M parameters (RoBERTa-Base)
and ﬁne-tune them except the embedding layer for SST-2 classiﬁcation task (Wang et al., 2018). We adopt
the setting as in Li et al. (2021): full-precision training with the batch size 1000 and the number of epochs
10.
Hyperparameter choice: For privacy parameters, we use /epsilon1= 8,δ=1e-5. With Renyi diﬀerential privacy
accountant, this corresponds to setting the noise multiplier 0.635. We compare the behavior of DP-SGD
¯The model and checkpoints can be found at https://github.com/pytorch/fairseq/tree/master/examples/roberta.
11Under review as submission to TMLR
0.0001 0.001 0.01 0.1 1.0
regularizer0.05 0.1 0.2 0.4 0.8 1.6 3.2lr
46.12 46.22 46.56 46.79 46.5948 48.18 48.07 48.64 48.7449.88 49.91 50.51 50.34 50.4149.38 49.86 49.44 49.7 49.6949.29 50.13 49.61 49.3 48.847.24 48.03 47.05 47.22 44.4942.49 43.06 43.03 40.66 40.9=2.0
3035404550556065
0.0001 0.001 0.01 0.1 1.0
regularizer0.05 0.1 0.2 0.4 0.8 1.6 3.2lr
47.59 47.62 47.86 48.21 48.8750.63 51.07 51.61 52.01 53.0352.93 53.25 53.79 54.68 55.2254.23 54.62 54.31 55.71 55.9454.91 56.81 56.6 56.4 56.5855.81 55.77 55.11 55.14 54.7853.34 53.46 52.89 52.33 51.12=4.0
3035404550556065
0.0001 0.001 0.01 0.1 1.0
regularizer0.05 0.1 0.2 0.4 0.8 1.6 3.2lr
48.64 48.86 49.01 49.36 49.8251.94 52.25 52.62 53.09 53.957.53 57.92 58.51 58.74 59.0859.69 59.83 60.29 60.47 60.9360.71 60.96 60.77 61.24 61.7862.05 61.8 61.69 61.76 62.3662.64 62.34 63.22 62.07 60.85=8.0
3035404550556065
0.1 0.4 1.0 1.6 6.4 12.8
clip0.05 0.1 0.2 0.4 0.8 1.6 3.2lr
32.32 41.92 46.75 48.28 50.49 47.337.3 45.45 48.74 50.19 49.44 43.3441.9 47.92 50.99 50.4 47.15 33.8745.43 50.05 50.17 50.81 39.93 26.4347.93 50.17 50.4 47.89 31.12 23.1749.68 50.19 47.02 43.04 26.76 20.3349.72 47.88 39.58 38.94 24.16 19.42=2.0
3035404550556065
0.1 0.4 1.0 1.6 6.4 12.8
clip0.05 0.1 0.2 0.4 0.8 1.6 3.2lr
32.91 43.32 48.53 51.84 55.37 55.7237.72 47 52.35 54.6 56.94 50.7743.24 51.65 55.16 55.23 55.65 47.2746.63 54.18 55.63 57.01 52.6 39.3951.31 55.85 56.92 57.08 43.09 28.9553.58 56.91 55.35 54.57 34.79 24.0354.73 57.79 51.43 52.05 28.99 24.68=4.0
3035404550556065
0.1 0.4 1.6 6.4 12.8
clip0.05 0.1 0.2 0.4 0.8 1.6 3.2lr
32.8 43.5 52.68 61.18 61.4237.52 48.29 58.02 62.64 61.643.46 52.18 60.56 62.49 54.3848.02 57.77 62.48 59.87 47.1151.92 60.21 63 56.08 38.4757.26 62 60.89 47.48 29.5759.88 63 59.79 36.83 26.92=8.0
3035404550556065
Figure3: ExperimentsforResNet20onCIFAR10task. Upper: AccuracyheatmapofDP-NSGDwithvarying
lrs and regularizers. Lower: Accuracy heatmap of DP-SGD with varying lrs and clipping thresholds. The
DP parameters are δ= 1e−5and/epsilon1= 2.0,4.0,8.0from left to right.
and that of DP-NSGD. For DP-SGD, we search the clipping threshold cfrom{0.1,0.5,2.5,12.5,50.0}and
thelrfrom{0.05,0.1,0.2,0.4,0.8,1.6}. For the DP-NSGD, we search the learning rate lrover the same set
of DP-SGD and the regularizer rfrom{1e-3, 1e-2, 1e-1, 1., 10.0 }.
0.001 0.01 0.1 1.0 10.0
regularizer0.05 0.1 0.2 0.4 0.8 1.6lr
89.32 89.55 90 90.23 90.3490.85 91.02 91.02 91.7 91.2591.7 91.82 91.82 92.05 91.792.27 92.39 92.44 92.5 92.0590.38 91.7 92.16 92.39 92.0589.58 89.81 84.73 89.58 89.73
50556065707580859095
0.1 0.5 2.5 12.5 50.0
clip0.05 0.1 0.2 0.4 0.8 1.6lr
50.98 88.75 91.59 91.82 79.8550.98 90.45 92.05 91.59 69.6288.52 91.36 92.27 82.35 50.9889.55 91.82 91.48 76.63 51.4490.8 92.39 81.33 50.98 51.7491.59 91.02 77.92 51.14 51.74
50556065707580859095
Figure 4: Experiments of ﬁne-tuning RoBERTa on SST-2 task. Left: Accuracy heatmap of DP-NSGD with
varying learning rates and regularizers. Right: Accuracy heatmap of DP-SGD with varying learning rates
and clipping thresholds.
We have similar observation in Figure 4 that the performance of DP-NSGD is rather stable for the regularizer
and the learning rate, which indicates that it could be easier to tune than DP-SGD.
12Under review as submission to TMLR
Notably, the concurrent study by Bu et al. (2023) has conducted extensive experiments, supporting observa-
tions that both DP-SGD and DP-NSGD achieve comparable performance. Furthermore, it was also found
that DP-NSGD is comparatively easier to tune.
5 Concluding Remarks
In this paper, we have studied the convergence of two algorithms, i.e., DP-SGD and DP-NSGD, for diﬀeren-
tially private non-convex empirical risk minimization. We have achieved a rate that signiﬁcantly improves
over previous literature under similar setup and have analyzed the bias induced by the clipping or normal-
izing operation. As for future directions, it is very interesting to consider the convergence theorems under
stronger assumptions on the gradient distribution.
Acknowledgments
All acknowledgements go at the end of the paper before appendices and references. Moreover, you are
required to declare funding (ﬁnancial activities supporting the submitted work) and competing interests
(related ﬁnancial activities outside the submitted work). More information about this disclosure can be
found on the TMLR website.
References
MartinAbadi, AndyChu, IanGoodfellow, HBrendanMcMahan, IlyaMironov, KunalTalwar, andLiZhang.
Deep learning with diﬀerential privacy. In ACM SIGSAC Conference on Computer and Communications
Security, 2016.
Galen Andrew, Om Thakkar, H Brendan McMahan, and Swaroop Ramaswamy. Diﬀerentially private learn-
ing with adaptive clipping. arXiv preprint arXiv:1905.03871 , 2019.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower
bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365 , 2019.
Hilal Asi, John C. Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient
methods for convex optimization. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th Inter-
national Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of
Proceedings of Machine Learning Research , pp. 383–392. PMLR, 2021.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Diﬀerentially private empirical risk minimization:
Eﬃcient algorithms and tight error bounds. Annual Symposium on Foundations of Computer Science ,
2014.
Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. Deep learning with gaussian diﬀerential privacy. Harvard
data science review , 2020(23), 2020.
Zhiqi Bu, Hua Wang, Qi Long, and Weijie J Su. On the convergence of deep learning with diﬀerential
privacy.arXiv preprint arXiv:2106.07830 , 2021.
Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Automatic clipping: Diﬀerentially private
deep learning made easier and stronger. In Thirty-seventh Conference on Neural Information Processing
Systems, 2023. URL https://openreview.net/forum?id=e8i7OaPj0q .
Mark Bun and Thomas Steinke. Concentrated diﬀerential privacy: Simpliﬁcations, extensions, and lower
bounds. In Theory of Cryptography Conference , 2016.
Mark Bun, Cynthia Dwork, Guy N Rothblum, and Thomas Steinke. Composable and versatile privacy via
truncated cdp. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing ,
2018.
13Under review as submission to TMLR
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam
Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language
models.arXiv preprint arXiv:2012.07805 , 2020.
Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances in Neural
Information Processing Systems , 2009.
Xiangyi Chen, Steven Z Wu, and Mingyi Hong. Understanding gradient clipping in private sgd: A geometric
perspective. Advances in Neural Information Processing Systems , 33, 2020.
Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International Conference on
Machine Learning , pp. 2260–2268. PMLR, 2020.
Ashok Cutkosky and Harsh Mehta. High-probability bounds for non-convex stochastic optimization with
heavy tails. Advances in Neural Information Processing Systems , 34, 2021.
Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, and Inderjit S Dhillon. Dp-normfedavg: Normalizing client
updates for privacy-preserving federated learning. arXiv preprint arXiv:2106.07094 , 2021.
AliDavody,DavidIfeoluwaAdelani,ThomasKleinbauer,andDietrichKlakow. Ontheeﬀectofnormalization
layers on diﬀerentially private training of deep neural networks. arXiv preprint arXiv:2006.10919 , 2020.
JohnDuchi, EladHazan, andYoramSinger. Adaptivesubgradientmethodsforonlinelearningandstochastic
optimization. Journal of machine learning research , 12(7), 2011.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, our-
selves: Privacy via distributed noise generation. In Annual International Conference on the Theory and
Applications of Cryptographic Techniques , 2006a.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of cryptography conference , 2006b.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of diﬀerential privacy. Foundations and
Trends in Theoretical Computer Science , 9(3-4):211–407, 2014a.
Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze gauss: optimal bounds for
privacy-preserving principal component analysis. In Proceedings of the forty-sixth annual ACM symposium
on Theory of computing , 2014b.
Huang Fang, Xiaoyun Li, Chenglin Fan, and Ping Li. Improved convergence of diﬀerential private sgd with
gradient clipping. In The Eleventh International Conference on Learning Representations , 2022.
Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of diﬀerential privacy. arXiv
preprint arXiv:2106.02848 , 2021.
Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Diﬀerentially private riemannian optimiza-
tion.arXiv preprint arXiv:2205.09494 , 2022.
Muneeb Ul Hassan, Mubashir Husain Rehmani, and Jinjun Chen. Diﬀerential privacy techniques for cyber
physical systems: a survey. IEEE Communications Surveys & Tutorials , 22(1):746–789, 2019.
Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai
Yu, and Jiang Bian. Exploring the limits of diﬀerentially private deep learning with group-wise clipping. In
The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.
net/forum?id=oze0clVGPeX .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , 2016.
14Under review as submission to TMLR
Geoﬀrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a
overview of mini-batch gradient descent. Cited on, 14(8):2, 2012.
Lijie Hu, Shuo Ni, Hanshen Xiao, and Di Wang. High dimensional diﬀerentially private stochastic optimiza-
tion with heavy-tailed data. In Proceedings of the 41st ACM SIGMOD-SIGACT-SIGAI Symposium on
Principles of Database Systems , pp. 227–236, 2022.
Zhanglong Ji, Zachary C Lipton, and Charles Elkan. Diﬀerential privacy and machine learning: a survey
and review. arXiv preprint arXiv:1412.7584 , 2014.
Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. Non-convex distributionally robust optimization:
Non-asymptotic analysis. CoRR, abs/2110.12459, 2021. URL https://arxiv.org/abs/2110.12459 .
Gautam Kamath, Vikrant Singhal, and Jonathan Ullman. Private mean estimation of heavy-tailed distri-
butions. In Conference on Learning Theory , pp. 2204–2235. PMLR, 2020.
Gautam Kamath, Xingtu Liu, and Huanyu Zhang. Improved rates for diﬀerentially private stochastic convex
optimization with heavy-tailed data. In International Conference on Machine Learning , pp. 10633–10660.
PMLR, 2022.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Nurdan Kuru, Ş İlker Birbil, Mert Gurbuzbalaban, and Sinan Yildirim. Diﬀerentially private accelerated
optimization algorithms. arXiv preprint arXiv:2008.01989 , 2020.
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong
diﬀerentially private learners. arXiv preprint arXiv:2110.05679 , 2021.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv
preprint arXiv:1907.11692 , 2019.
Andrew Lowy, Ali Ghafelebashi, and Meisam Razaviyayn. Private non-convex federated learning without a
trusted server. arXiv preprint arXiv:2203.06735 , 2022.
Ilya Mironov. Rényi diﬀerential privacy. In IEEE Computer Security Foundations Symposium , 2017.
Ilya Mironov, Kunal Talwar, and Li Zhang. Rényi diﬀerential privacy of the sampled gaussian mechanism.
arXiv, 2019.
Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive
optimizers for honest private hyperparameter selection. arXiv preprint arXiv:2111.04906 , 2021.
Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi diﬀerential privacy. arXiv preprint
arXiv:2110.03620 , 2021.
Nicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Ulfar Erlingsson. Making the shoe
ﬁt: Architectures, initializations, and tuning for learning with privacy, 2020a. URL https://openreview.
net/forum?id=rJg851rYwH .
Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and Úlfar Erlingsson. Tempered sigmoid
activations for deep learning with diﬀerential privacy. arXiv preprint arXiv:2007.14191 , 2020b.
Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X Yu, Sashank J Reddi, and Sanjiv Kumar.
Adaclip: Adaptive clipping for private sgd. arXiv preprint arXiv:1908.07643 , 2019.
Karthik Abinav Sankararaman, Soham De, Zheng Xu, W Ronny Huang, and Tom Goldstein. The im-
pact of neural network overparameterization on gradient confusion and stochastic gradient descent. In
International Conference on Machine Learning , pp. 8469–8479. PMLR, 2020.
15Under review as submission to TMLR
Saiteja Utpala, Andi Han, Pratik Jawanpuria, and Bamdev Mishra. Improved diﬀerentially private rieman-
nian optimization: Fast sampling and variance reduction. Transactions on Machine Learning Research ,
2022a.
Saiteja Utpala, Praneeth Vepakomma, and Nina Miolane. Diﬀerentially private fréchet mean on the manifold
ofsym-metricpositivedeﬁnite(spd)matriceswithlog-euclideanmetric. Transactions on Machine Learning
Research , 2022b.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461 , 2018.
Di Wang, Minwei Ye, and Jinhui Xu. Diﬀerentially private empirical risk minimization revisited: Faster and
more general. In Advances in Neural Information Processing Systems , 2017.
Di Wang, Changyou Chen, and Jinhui Xu. Diﬀerentially private empirical risk minimization with non-convex
loss functions. In International Conference on Machine Learning , pp. 6526–6535. PMLR, 2019a.
Di Wang, Hanshen Xiao, Srinivas Devadas, and Jinhui Xu. On diﬀerentially private stochastic convex
optimization with heavy-tailed data. In International Conference on Machine Learning , pp. 10081–10091.
PMLR, 2020.
Lingxiao Wang, Bargav Jayaraman, David Evans, and Quanquan Gu. Eﬃcient privacy-preserving stochastic
nonconvex optimization. In Uncertainty in Artiﬁcial Intelligence , pp. 2203–2213. PMLR, 2023.
Puyu Wang, Yunwen Lei, Yiming Ying, and Hai Zhang. Diﬀerentially private sgd with non-smooth losses.
Applied and Computational Harmonic Analysis , 56:306–336, 2022.
Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled rényi diﬀerential privacy
and analytical moments accountant. In International Conference on Artiﬁcial Intelligence and Statistics ,
2019b.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer
vision (ECCV) , 2018.
Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. Reducing BERT
pre-training time from 3 days to 76 minutes. CoRR, abs/1904.00962, 2019. URL http://arxiv.org/
abs/1904.00962 .
Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek,
John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, et al. Opacus: User-friendly diﬀerential
privacy library in pytorch. arXiv preprint arXiv:2109.12298 , 2021.
Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Gradient perturbation is underrated for
diﬀerentially private convex optimization. In Proc. of 29th Int. Joint Conf. Artiﬁcial Intelligence , 2020.
Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms for non-
convex optimization. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems (NeurIPS) , 2020a.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A
theoretical justiﬁcation for adaptivity. In International Conference on Learning Representations (ICLR) ,
2020b.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar,
and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural Information
Processing Systems , 33:15383–15393, 2020c.
16Under review as submission to TMLR
Xinwei Zhang, Xiangyi Chen, Mingyi Hong, Zhiwei Steven Wu, and Jinfeng Yi. Understanding clipping
for federated learning: Convergence and client-level diﬀerential privacy. arXiv preprint arXiv:2106.13673 ,
2021.
Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic normalized
gradient descent. Science China Information Sciences , 64(3):1–13, 2021.
Yingxue Zhou, Xiangyi Chen, Mingyi Hong, Zhiwei Steven Wu, and Arindam Banerjee. Private stochas-
tic non-convex optimization: Adaptive algorithms and tighter generalization bounds. arXiv preprint
arXiv:2006.13501 , 2020.
17Under review as submission to TMLR
Appendices
A More Literature on Private Optimization
Private Non-convex Empirical Risk Minimization: This line of works (Wang et al., 2019a; 2023;
Zhou et al., 2020) study GD, RMSprop and Adam for centralized diﬀerentially private non-convex empirical
risk minimization minxf(x) =/summationtextN
i=1/lscript(x,ξi)/N. As our Table 1 suggests, all these algorithms achieve the
state-of-the-art on the utility upper bound O/parenleftBig
4/radicalbig
d/(N2/epsilon12)/parenrightBig
, under the following assumption.
Assumption A.1 (Previous assumption for non-convex DP ERM) .There exists L,G> 0such that for any
ξ, the loss function x/mapsto→/lscript(x,ξ)isL-smooth (/bardbl∇2
x/lscript(x,ξ)/bardbl≤L) andG-Lipschitz (/bardbl∇x/lscript(x,ξ)/bardbl≤G).
In direct comparison, our assumptions are much weaker:
•Assumption2.1onlyrequirestheexpectedlossfunction f(x)tosatisfycertainsmoothnesscondition;
•Assumption 2.2 with τ0= 2G,τ 1= 0covers the cases of Assumption A.1.
In these works, the Lipschitz condition of the loss gradient ∇x/lscript(x,ξ)is vital since it allows to ignore the
eﬀect of gradient clipping when analyzing convergence, thus unable to provide theoretical understanding
towards this tunable hyper parameter.
We manage to obtain best available utility bounds for the problem of non-convex DP ERM, even under
weakened assumptions. More importantly, our weakened assumptions are not only closer to real-life neural
network training, but also more suitable to show the distinctions between DP-SGD and DP-NSGD, as shown
in Corollaries 3.5 & 3.3.
Private Convex Optimization with Heavy-tailed Data: In comparison with DP-ERM, private convex
optimization (DP-SCO) (Bassily et al., 2014) privately minimizes the population risk
min
x˜f(x) :=Eξ∼P[/lscript(x,ξ)],
given i.i.d. samples ξ1,...,ξN∼P, wherePis the underlying true distribution not solely the empirical
one. Typically, the loss x/mapsto→/lscript(x,ξ)is assumed to be convex for any ξ. For this problem, the utility is thus
measured by EξN
1,M[˜f(xoutput )], where expectation is taken over both the dataset and the algorithm itself.
Many papers have investigated the problem of DP-SCO (Chaudhuri & Monteleoni, 2009; Wang et al., 2017;
Kuru et al., 2020; Yu et al., 2020; Asi et al., 2021), but all of them adopt the Lipschitz condition on the
loss gradient∇x/lscript(x,ξ)to avoid considering gradient clipping. In contrast to our Assumption 2.2, recent
progresses (Wang et al., 2020; Kamath et al., 2022; Hu et al., 2022) in DP SCO, weakened the Lipschitz
condition to a heavy-tailed assumption, namely the k-th bounded moment condition on each coordinate
∇x/lscript(x,ξ)j,j∈[d]of the gradient estimation (for example, (Kamath et al., 2022, Deﬁnition 2.11)). These
works still have to assume the expected gradients to be uniformly bounded Eξ∼P[∇x/lscript(x,ξ)]for every x, e.g.
the 6th assertion in (Kamath et al., 2022, Assumption 2.11). In contrast, our setting is able to illustrate the
diﬀerences of clipping and normalizing based methods.
Fromanalgorithmicperspective, Wangetal.(2020);Kamathetal.(2022);Huetal.(2022)proposedmethods
involving a sophisticated mean oracle in Kamath et al. (2020) to estimate the expected gradients, while this
mean oracle still employs clipping with respect to a preset threshold. Speciﬁcally, to handle the ill-behaved
gradients, methods from Kamath et al. (2022) process each coordinate separately, by ﬁrstly partitioning the
selected batch and then taking median over the clipped means of each disjoint parts. These techniques might
be helpful in addressing non-convex DP ERM with heavy-tailed conditions.
Private Federated Learning In recent years, private federated learning also draws lots of attention. Since
our work focuses on centralized DP, we only mention a few related works in this direction. Das et al.
18Under review as submission to TMLR
(2021) studied DP-NormFedAvg, a client-level DP optimizer in a federated setting. Their optimizer, DP-
NormFedAvg, uses vanilla GD for each client and normalizes the contribution of every client to unit-norm.
Sharing similar motivation with our centralized DP-NSGD, their contributions are roughly credited to DP-
NSGD. Their convergence analysis is based on one-point/quasar convexity and L-smoothness. Speciﬁcally
for the bound shown in the 4th line of Table 1, /bardblx(i)−x∗/bardblmeasures heterogeneity via the distance between
thei-th client’s local minimizer x(i)to the global minimizer x∗, andDX,/bardblx0−x∗/bardbl.
Following Chen et al. (2020), DP-FedAvg with clipping is analyzed in Zhang et al. (2021), with symmetric
gradient distribution assumption. Additionally, a recent preprint (Lowy et al., 2022) also investigates private
non-convex federated learning, but based on local/shuﬄe diﬀerential privacy.
B Prerequisite Lemmas
The following is a standard lemma for (L0,L1)-generalized smooth functions, and it can be obtained via
Taylor’s expansion. Throughout this appendix, we use Ekto denote taking expectation of {g(i)
k,i∈Sk}and
zkconditioned on the past, especially xk.
Lemma B.1 (Lemma C.4, Jin et al. (2021)) .A function f:Rd→Rdis(L0,L1)-generalized smooth, then
for any x,x+∈Rd,
f(x+)≤f(x) +/angbracketleftbig
∇f(x),x+−x/angbracketrightbig
+L0+L1/bardbl∇f(x)/bardbl
2/bardblx+−x/bardbl2.
Lemma B.2. For anyk≥0, we use gkto denote another realization of the underlying distribution behind
the set of i.i.d. unbiased estimates {g(i)
k:i∈Sk}. If we run DP-NSGD iteratively, the trajectory would
satisfy the following bound:
Ek[f(xk+1)]−f(xk)≤−ηEk[/angbracketlefthk∇f(xk),gk/angbracketright] +L0+L1/bardbl∇f(xk)/bardbl
2η2/parenleftBig
dσ2+Ek/bardblhkgk/bardbl2/parenrightBig
.(12)
Proof.The updating rule of our iterative algorithm could be summarized as
xk+1=xk−η/parenleftBigg
1
B/summationdisplay
i∈Skh(i)
kg(i)
k+zk/parenrightBigg
,zk∼N(0,σ2Id).
By taking expectation Ekconditioned on the past, we rewrite the ﬁrst-order term in Lemma B.1 into
Ek[/angbracketleft∇f(x),xk−xk+1/angbracketright] =ηEk[/angbracketlefthk∇f(xk),gk/angbracketright]. (13)
In the same manner, we bound the second-order term by
Ek/bardblxk+1−xk/bardbl2=η2dσ2+η2
B2Ek/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
i∈Skh(i)
kg(i)
k/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤η2/parenleftBig
dσ2+Ek/bardblhkgk/bardbl2/parenrightBig
, (14)
where the last inequality follows from an elementary Cauchy-Schwarz inequality,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
i∈Skh(i)
kg(i)
k/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤B/summationdisplay
i∈Sk/vextenddouble/vextenddouble/vextenddoubleh(i)
kg(i)
k/vextenddouble/vextenddouble/vextenddouble2
.
Plug (13) and (14) into Lemma B.1 to obtain the desired result.
Remark B.1.This lemma implies that mini batch size Bdoes not aﬀect expected upper bounds, due to
per-sample gradient normalization. We need to point out that Bcould still inﬂuence high-probability upper
bounds, and call for future investigations.
19Under review as submission to TMLR
In Section C.1, we will upper bound the second-order termL0+L1/bardbl∇f(xk)/bardbl
2η2/parenleftBig
dσ2+Ek/bardblhkgk/bardbl2/parenrightBig
by a sum
ofαηhk/bardbl∇f(xk)/bardbl(for some 0< α < 1) and another term of O(η2)via a proper scaling of η. We ﬁrstly
present the following lemma to provide a simpliﬁed lower bound for the ﬁrst-order terms
ηEk[/angbracketlefthk∇f(xk),gk/angbracketright]−ηαEk[hk]/bardbl∇f(xk)/bardbl2. (15)
Lemma B.3 (Lower bound ﬁrst-order terms for normalizing) .Deﬁne a function A:R+→Ras
A(s) =

/parenleftbiggτ0
r(1−τ1) + 2τ0−α
1−τ1/parenrightbigg
s, ifs≥τ0
1−τ1;
(1−α)(1−τ1)
r(1−τ1) + 2τ0s2−4τ3
0
r(r+τ0)(1−τ1)3,otherwise.(16)
Then we have
ηEk[/angbracketlefthk∇f(xk),gk/angbracketright]−ηαEk[hk]/bardbl∇f(xk)/bardbl2≥ηA(/bardbl∇f(xk)/bardbl).
Proof.We prove this lemma via separating the range of /bardbl∇f(xk)/bardbl. When/bardbl∇f(xk)/bardbl≥τ0/(1−τ1), then
/angbracketleft∇f(xk),gk/angbracketright=/bardbl∇f(xk)/bardbl2+/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright
≥(1−τ1)/bardbl∇f(xk)/bardbl2−τ0/bardbl∇f(xk)/bardbl≥0,
followed by
ηEk[/angbracketlefthk∇f(xk),gk/angbracketright]−ηαEk[hk]/bardbl∇f(xk)/bardbl2
=Ek/bracketleftbiggη/angbracketleft∇f(xk),gk/angbracketright
r+/bardblgk/bardbl/bracketrightbigg
−Ek/bracketleftbiggαη
(r+/bardblgk/bardbl)/bardbl∇f(xk)/bardbl2/bracketrightbigg
≥Ek/bracketleftbiggη/angbracketleft∇f(xk),gk/angbracketright
r+τ0+ (1 +τ1)/bardbl∇f(xk)/bardbl/bracketrightbigg
−α
1−τ1η/bardbl∇f(xk)/bardbl
=η/bardbl∇f(xk)/bardbl2
r+τ0+ (1 +τ1)/bardbl∇f(xk)/bardbl−α
1−τ1η/bardbl∇f(xk)/bardbl
≥/parenleftbiggτ0
r(1−τ1) + 2τ0−α
1−τ1/parenrightbigg
η/bardbl∇f(xk)/bardbl. (17)
When/bardbl∇f(xk)/bardbl<τ0/(1−τ1), we have/bardblgk−∇f(xk)/bardbl≤τ0+τ1/bardbl∇f(xk)/bardbl≤τ0/(1−τ1)as well. Then we
decompose the ﬁrst-order terms by
ηEk[/angbracketlefthk∇f(xk),gk/angbracketright]−ηαEk[hk]/bardbl∇f(xk)/bardbl2
=(1−α)ηEk[hk]/bardbl∇f(xk)/bardbl2+Ek/bracketleftbiggη/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright
r+/bardblgk/bardbl/bracketrightbigg
.
On one hand, we know
hk=1
r+/bardblgk/bardbl≥1
r+τ0+ (1 +τ1)/bardbl∇f(xk)/bardbl≥1−τ1
r(1−τ1) + 2τ0.
On the other hand, we also have
Ek/bracketleftbiggη/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright
r+/bardblgk/bardbl/bracketrightbigg
=Ek/bracketleftbiggη/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright
r+τ0+ (1 +τ1)/bardbl∇f(xk)/bardbl/bracketrightbigg
+Ek/bracketleftbiggη[(1 +τ1)/bardbl∇f(xk)/bardbl+τ0−/bardblgk/bardbl]/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright
(r+/bardblgk/bardbl)(r+τ0+ (1 +τ1)/bardbl∇f(xk)/bardbl)/bracketrightbigg
≥−η4τ3
0
r(r+τ0)(1−τ1)3.
20Under review as submission to TMLR
Therefore, we have
ηEk[/angbracketlefthk∇f(xk),gk/angbracketright]−ηαEk[hk]/bardbl∇f(xk)/bardbl2
≥η/bracketleftbigg(1−α)(1−τ1)
r(1−τ1) + 2τ0/bardbl∇f(xk)/bardbl2−4τ3
0
r(r+τ0)(1−τ1)3/bracketrightbigg
. (18)
Combine both cases to derive the required lemma.
We then establish similar results for gradient clipping based algorithms, whose updating rule is described by
(2). Firstly, we mimic the proof of Lemma B.2 and derive without proof the following lemma.
Lemma B.4. For anyk≥0, we use gkto denote another realization of the underlying distribution behind
the set of i.i.d. unbiased estimates {g(i)
k:i∈Sk}. If we run DP-SGD iteratively, the trajectory would satisfy
the following bound:
Ek[f(xk+1)]−f(xk)≤−ηEk/bracketleftbig
/angbracketleft¯hk∇f(xk),gk/angbracketright/bracketrightbig
+L0+L1/bardbl∇f(xk)/bardbl
2η2/parenleftBig
dc2σ2+Ek/vextenddouble/vextenddouble¯hkgk/vextenddouble/vextenddouble2/parenrightBig
.(19)
Then, we provide another lemma for clipping, similar to Lemma B.3.
Lemma B.5 (Lower bound ﬁrst-order terms for clipping) .Deﬁne a function B:R+→Ras
B(s) =

/parenleftbiggτ0c
c(1−τ1) + 2τ0−α
1−τ1/parenrightbigg
s,ifs≥τ0
1−τ1;
(1−α)s2, otherwise.(20)
If we takec≥2τ0/(1−τ1), then
ηEk/bracketleftbig
/angbracketleft¯hk∇f(xk),gk/angbracketright/bracketrightbig
−ηαEk[¯hk]/bardbl∇f(xk)/bardbl2≥ηB(/bardbl∇f(xk)/bardbl).
Proof.Recall that ¯hkis deﬁned as
¯hk= min/braceleftbigg
1,c
/bardblgk/bardbl/bracerightbigg
≥c
c+/bardblgk/bardbl.
Again, we take the strategy of separting the range of /bardbl∇f(xk)/bardbl. When/bardbl∇f(xk)/bardbl≥τ0/(1−τ1), we know
/angbracketleft∇f(xk),gk/angbracketright≥0, followed by
ηEk/bracketleftbig
/angbracketleft¯hk∇f(xk),gk/angbracketright/bracketrightbig
−ηαEk[¯hk]/bardbl∇f(xk)/bardbl2
≥Ek/bracketleftbiggηc/angbracketleft∇f(xk),gk/angbracketright
c+/bardblgk/bardbl/bracketrightbigg
−Ek/bracketleftbiggαη
/bardblgk/bardbl/bardbl∇f(xk)/bardbl2/bracketrightbigg
≥Ek/bracketleftbiggηc/angbracketleft∇f(xk),gk/angbracketright
c+τ0+ (1 +τ1)/bardbl∇f(xk)/bardbl/bracketrightbigg
−α
1−τ1η/bardbl∇f(xk)/bardbl
=ηc/bardbl∇f(xk)/bardbl2
c+τ0+ (1 +τ1)/bardbl∇f(xk)/bardbl−α
1−τ1η/bardbl∇f(xk)/bardbl
≥/parenleftbiggτ0c
c(1−τ1) + 2τ0−α
1−τ1/parenrightbigg
η/bardbl∇f(xk)/bardbl.
Otherwise, when /bardbl∇f(xk)/bardbl< τ 0/(1−τ1)≤(c−τ0)/(1 +τ1)where the second inequality follows from
c≥2τ0/(1−τ1), we know
/bardblgk/bardbl≤τ0+ (1 +τ1)/bardbl∇f(xk)/bardbl≤c.
Therefore in this case, ¯hk= 1and
ηEk/bracketleftbig
/angbracketleft¯hk∇f(xk),gk/angbracketright/bracketrightbig
−ηαEk[¯hk]/bardbl∇f(xk)/bardbl2= (1−α)η/bardbl∇f(xk)/bardbl2.
Combine both cases to conclude the desired lemma.
21Under review as submission to TMLR
C Proofs for DP-NSGD
In this section, we provide a rigorous convergence theory for normalized stochastic gradient descent with
perturbation . An unavoidable error between gkand∇f(xk)is a central distinction between stochastic and
deterministic optimization methods. We begin with an explicit decomposition for (12)
Ek[f(xk+1)]−f(xk)
≤−ηEk[hk]/bardbl∇f(xk)/bardbl2−ηEk[/angbracketlefthk∇f(xk),gk−∇f(xk)/angbracketright]
+L0+L1/bardbl∇f(xk)/bardbl
2η2/parenleftBig
Ek/bracketleftBig
h2
k/bardbl∇f(xk)/bardbl2/bracketrightBig
+ 2Ek/bracketleftbig
h2
k/angbracketleftgk−∇f(xk),∇f(xk)/angbracketright/bracketrightbig/parenrightBig
+L0+L1/bardbl∇f(xk)/bardbl
2η2/parenleftBig
dσ2+Ek/bracketleftBig
h2
k/bardblgk−∇f(xk)/bardbl2/bracketrightBig/parenrightBig
. (21)
C.1 Upper Bound Second-order Terms
In theory, we need to carefully distinguish these terms accourding to their orders of η, as the ﬁrst order term
Ek[/angbracketlefthk∇f(xk),gk/angbracketright]controls the amount of descent mainly. We show that the second order terms could be
bounded by ﬁrst order terms via a proper scaling of η, in the following technical lemma.
Lemma C.1. For any 0<α< 1to be determined explicitly later, if
η≤min/parenleftbigg(r−τ0)α
4L0,(1−τ1)α
4L1,α
6L1dσ2/parenrightbigg
(22)
then we have
L0+L1/bardbl∇f(xk)/bardbl
2η2dσ2≤L0+L1(r+τ0)
2η2dσ2+αηhk
4/bardbl∇f(xk)/bardbl2, (23)
(L0+L1/bardbl∇f(xk)/bardbl)η2h2
k/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright≤(L0(1−τ1) +L1τ0)τ2
0
r2(1−τ1)3η2+αηhk
4/bardbl∇f(xk)/bardbl2,(24)
L0+L1/bardbl∇f(xk)/bardbl
2η2h2
k/bardblgk−∇f(xk)/bardbl2≤(L0(1−τ1) +L1τ0)τ2
0
2r2(1−τ1)3η2+αηhk
4/bardbl∇f(xk)/bardbl2, (25)
L0+L1/bardbl∇f(xk)/bardbl
2η2h2
k/bardbl∇f(xk)/bardbl2≤αηhk
4/bardbl∇f(xk)/bardbl2. (26)
Remark C.1.These bounds are proved by separating the range of /bardbl∇f(xk)/bardbl. When it is smaller than some
threshold, we can obtain an upper bound of O(η2). Otherwise, when /bardbl∇f(xk)/bardblis greater than the threshold,
hkis of order Ω(1//bardbl∇f(xk)/bardbl), then the left hand terms are all of order O(η2hk/bardblf(x)/bardbl2). Therefore we scale
ηsmall enough to make left hand terms smaller than αηhk/bardblf(x)/bardbl2/4. At last, we sum up the respective
upper bounds together to conclude the lemma.
Remark C.2.Moreover, we remark that the thresholds chosen during proof ( r+τ0for proving (23) and
τ0/(1−τ1)for proving (24) and (25)) are quite artiﬁcial. A thorough investigation towards these thresholds
would deﬁnitely improve the dependence on the constants (L0,L1,τ0,τ1), but would not aﬀect our main
argument.
Proof.In fact, this lemma can be proved in an obvious way by separating into diﬀerent cases.
(i)If/bardbl∇f(xk)/bardbl≤r+τ0, it directly follows that L1/bardbl∇f(xk)/bardblη2dσ2/2≤L1(r+τ0)η2dσ2/2; otherwise, if
/bardbl∇f(xk)/bardbl>r+τ0, we know
hk=1
r+/bardblgk/bardbl≥1
r+τ0+ (τ1+ 1)/bardbl∇f(xk)/bardbl≥1
3/bardbl∇f(xk)/bardbl,
22Under review as submission to TMLR
thereforeη≤α/(6L1dσ2)directly yields
L1/bardbl∇f(xk)/bardblη2dσ2
2≤ηαhk/bardbl∇f(xk)/bardbl2
4.
Then (23) follows from summing up these two cases.
(ii)If/bardbl∇f(xk)/bardbl≤τ0/(1−τ1), then/bardblgk−∇f(xk)/bardbl≤τ0+τ1/bardbl∇f(xk)/bardbl≤τ0/(1−τ1)andhk≤1/r, which
yield
(L0+L1/bardbl∇f(xk)/bardbl)η2h2
k/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright≤(L0(1−τ1) +L1τ0)τ2
0
r2(1−τ1)3η2,
L0+L1/bardbl∇f(xk)/bardbl
2η2h2
k/bardblgk−∇f(xk)/bardbl2≤(L0(1−τ1) +L1τ0)τ2
0
2r2(1−τ1)3η2.
Otherwise, if/bardbl∇f(xk)/bardbl>τ0/(1−τ1), we note that/bardblgk/bardbl≥−τ0+ (1−τ1)/bardbl∇f(xk)/bardbland
hk(L0+L1/bardbl∇f(xk)/bardbl)≤L0+L1/bardbl∇f(xk)/bardbl
r−τ0+ (1−τ1)/bardbl∇f(xk)/bardbl≤max/parenleftbiggL0
r−τ0,L1
1−τ1/parenrightbigg
.
Consequently, once η≤α
4min/parenleftbiggr−τ0
L0,1−τ1
L1/parenrightbigg
, we have
(L0+L1/bardbl∇f(xk)/bardbl)η2h2
k/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright
≤max/parenleftbiggL0
r−τ0,L1
1−τ1/parenrightbigg
η2hk/bardbl∇f(xk)/bardbl2≤ηαhk
4/bardbl∇f(xk)/bardbl2,
and
L0+L1/bardbl∇f(xk)/bardbl
2η2h2
k/bardblgk−∇f(xk)/bardbl2
≤1
2max/parenleftbiggL0
r−τ0,L1
1−τ1/parenrightbigg
η2hk/bardbl∇f(xk)/bardbl2≤ηαhk
4/bardbl∇f(xk)/bardbl2.
We obtain (24) and (25) via summing up respective bounds for two cases.
(iii)The last bound (26) can be derived directly by
L0+L1/bardbl∇f(xk)/bardbl
2η2h2
k/bardbl∇f(xk)/bardbl2≤max/parenleftbiggL0
r−τ0,L1
τ1/parenrightbiggη2
2hk/bardbl∇f(xk)/bardbl2≤ηαhk
4/bardbl∇f(xk)/bardbl2
via setting η≤α
2min/parenleftbiggr−τ0
L0,τ1
L1/parenrightbigg
.
In conclusion, it suﬃces to set η≤min/parenleftbigg(r−τ0)α
4L0,(1−τ1)α
4L1,α
6L1dσ2/parenrightbigg
to obtain these bounds.
In the sequel, we will use this lemma only with
α=α0:=τ0(1−τ1)
2r(1−τ1) + 4τ0<1
4(27)
Lemma C.2. In the statement of Theorem 3.2, we take η=/radicalBig
2
L1(r+τ0)Tdσ2. Then the condition (22) in
Lemma C.1 holds as long as we run the algorithm long enough i.e. T≥C/parenleftbig
σ2,τ,L,d,r/parenrightbig
.
23Under review as submission to TMLR
Proof.We see that
η=/radicalBigg
2
L1(r+τ0)Tdσ2≤min/parenleftbigg(r−τ0)α0
4L0,(1−τ1)2α0
4L1,α0
6L1dσ2/parenrightbigg
is equivalent to
T≥max/parenleftbigg32L2
0
(r−τ0)2α2
0L1(r+τ0)dσ2,32L1
τ2
1α2
0(r+τ0)dσ2,72L1d
α2
0(r+τ0)/parenrightbigg
.
C.2 Final Procedures in Proof
Proof of Theorem 3.2. Equipped with Lemmas B.3 ,C.1 and C.3, we further wrote the one step inequality
(21) into
ηA(/bardbl∇f(xk)/bardbl)
≤f(xk)−Ek[f(xk+1)] +η2/parenleftbigg(L1(r+τ0) +L0)dσ2
2+3(L0(1−τ1) +L1τ0)τ2
0
2r2(1−τ1)3η2/parenrightbigg
.(28)
We then separate the time index into
U=/braceleftbigg
k<T :/bardbl∇f(xk)/bardbl≥τ0
1−τ1/bracerightbigg
andUc={0,1,···,T−1}\U. Given this, we derive from (17) that for any k∈U,
τ0
2r(1−τ1) + 4τ0η/bardbl∇f(xk)/bardbl
≤Ek[f(xk+1)]−f(xk) +η2/parenleftbigg(L1(r+τ0) +L0)dσ2
2+3(L0(1−τ1) +L1τ0)τ2
0
2r2(1−τ1)3/parenrightbigg
.
Similarly, together with α0≤1/4, (18) deduces that for any k∈Uc,
η/bracketleftbigg3(1−τ1)
4r(1−τ1) + 8τ0/bardbl∇f(xk)/bardbl2−4τ3
0
r(r+τ0)(1−τ1)3/bracketrightbigg
≤Ek[f(xk+1)]−f(xk) +η2/parenleftbigg(L1(r+τ0) +L0)dσ2
2+3(L0(1−τ1) +L1τ0)τ2
0
2r2(1−τ1)3/parenrightbigg
.
Sum these inequalities altogether to have
1
r+ 2τ0max/braceleftBigg
τ0
2T/summationdisplay
k∈U/bardbl∇f(xk)/bardbl,3(1−τ1)
4T/summationdisplay
k/∈U/bardbl∇f(xk)/bardbl2/bracerightBigg
≤Df
Tη+η(L1(r+τ0) +L0)dσ2
2+η3(L0(1−τ1) +L1τ0)τ2
0
2r2(1−τ1)3+4τ3
0
r(r+τ0)(1−τ1)3|Uc|
T.
In order to optimize the ﬁrst two terms, we set
η=/radicalBigg
2
(L1(r+τ0) +L0)Tdσ2. (29)
We then deﬁne
∆ = (Df+ 1)/radicalbigg
(L1(r+τ0) +L0)dσ2
2T+3(L0+L1τ0)τ2
0
2r2(1−τ1)3/radicalBigg
2
(L1(r+τ0) +L0)Tdσ2+4τ3
0
r(r+τ0)(1−τ1)3.
24Under review as submission to TMLR
Recall that σ2can have some dependence on T, so actually these three terms are O(σ/√
T),O(1/(√
Tσ))
andO(1)respectively. Then we further have
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤E
min

/radicalBigg
1
|U|/summationdisplay
k∈Uc/bardbl∇f(xk)/bardbl2,1
|U|/summationdisplay
k/∈U/bardbl∇f(xk)/bardbl



≤max/braceleftBigg/radicalBigg
8(r+ 2τ0)
3(1−τ1)∆,2(r+ 2τ0)
τ0∆/bracerightBigg
, (30)
where the second inequality follows from the fact that either |U|≥T/2or|Uc|≥T/2. In the end, we capture
the leading terms in this upper bound to have
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤O/parenleftBigg
4/radicalBigg
(Df+ 1)2(L1(r+τ0) +L0)(r+ 2τ0)2dσ2
T(1−τ1)2/parenrightBigg
+O
/radicaltp/radicalvertex/radicalvertex/radicalbt/radicalBigg
2(r+ 2τ0)2
(L1(r+τ0) +L0)Tdσ23(L0+L1τ0)τ2
0
2r2(1−τ1)4+8(r+ 2τ0)τ2
0
r(r+τ0)(1−τ1)3
.
Lemma C.3. In the statement of Corollary 3.3, we take η=/radicalBig
2
(L1(r+τ0)+L0)Tdσ2,σ=c2/radicalBig
Tlog1
δ/(N/epsilon1)
andT≥O(N2/epsilon12/(r3dlog1
δ)). Then the condition in Lemma C.2 holds as long as we have enough samples
i.e.N≥C(/epsilon1,δ,τ,L,B,d,r ).
Proof.It is more straight-forward to verify the condition (22) directly. Firstly, we plug σ2=c2
2Tlog(1/δ)
N2/epsilon12from
Lemma 3.1 into the formula of ηto have
η=/radicalBigg
2
(L1(r+τ0) +L0)Tdσ2=N/epsilon1
c2T/radicalBigg
2
(L1(r+τ0) +L0)dlog(1/δ)≤α0N2/epsilon12
6L1dc2
2Tlog(1/δ)=α0
6L1dσ2
as long as we have enough samples
N≥6c2L1
/epsilon1α0/radicalBigg
2dlog(1/δ)
L1(r+τ0) +L0.
Other conditions
η=N/epsilon1
c2T/radicalBigg
2
(L1(r+τ0) +L0)dlog(1/δ)≤min/braceleftbigg(r−τ0)α0
4L0,(1−τ1)α0
4L1/bracerightbigg
holds as long as we run the algorithm long enough
T
N≥min/braceleftbiggL0
r−τ0,L1
τ1/bracerightbigg4/epsilon1
α0c2/radicalBigg
2
(L1(r+τ0) +L0)dlog(1/δ). (31)
The last requirement (31) naturally holds due to T≥O(N2/epsilon12/(r3dlog1
δ)).
Proof of Corollary 3.3. Moreover, we take the limit T≥O(N2/epsilon12/(B2r3dlog1
δ))to derive the privacy-utility
trade-oﬀ
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤O/parenleftBigg
4/radicalbigg
(Df+ 1)2(L1(r+τ0) +L0)dlog(1/δ)
N2/epsilon12+8(r+ 2τ0)τ3
0
r(r+τ0)(1−τ1)3/parenrightBigg
,
ending the proof.
25Under review as submission to TMLR
D Proofs for DP-SGD
In this section, we prove the convergence theorem for DP-SGD following the roadmap outlined in Section C.
To start with, we extend (19) into
Ek[f(xk+1)]−f(xk)
≤−ηEk/bracketleftbig¯hk/bracketrightbig
/bardbl∇f(xk)/bardbl2−ηEk/bracketleftbig
/angbracketleft¯hk∇f(xk),gk−∇f(xk)/angbracketright/bracketrightbig
+L0+L1/bardbl∇f(xk)/bardbl
2η2/parenleftBig
Ek/bracketleftBig
¯h2
k/bardbl∇f(xk)/bardbl2/bracketrightBig
+ 2Ek/bracketleftbig¯h2
k/angbracketleftgk−∇f(xk),∇f(xk)/angbracketright/bracketrightbig/parenrightBig
+L0+L1/bardbl∇f(xk)/bardbl
2η2/parenleftBig
dc2σ2+Ek/bracketleftBig
¯h2
k/bardblgk−∇f(xk)/bardbl2/bracketrightBig/parenrightBig
. (32)
D.1 Upper Bound Second-order Terms
In the same spirit as Lemma C.1, we provide an upper bound for the second-order terms in the following
lemma.
Lemma D.1. For any 0<α< 1to be determined explicitly later, if
η≤min/braceleftbiggα
6L1dcσ2,α(1−τ1)
2L0(1−τ1) + 4L1τ0,ατ0(1−τ1)
4c(L0(1−τ1) + 2L1τ0)/bracerightbigg
, (33)
then we have
L0+L1/bardbl∇f(xk)/bardbl
2η2dc2σ2≤L1(c+τ0) +L0
2η2dc2σ2+αη¯hk
4/bardbl∇f(xk)/bardbl2, (34)
(L0+L1/bardbl∇f(xk)/bardbl)η2¯h2
k/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright≤2(L0(1−τ1) + 2L1τ0)τ2
0
(1−τ1)3η2+αη¯hk
4/bardbl∇f(xk)/bardbl2,(35)
L0+L1/bardbl∇f(xk)/bardbl
2η2¯h2
k/bardblgk−∇f(xk)/bardbl2≤2(L0(1−τ1) + 2L1τ0)τ2
0
(1−τ1)3η2+αη¯hk
4/bardbl∇f(xk)/bardbl2, (36)
L0+L1/bardbl∇f(xk)/bardbl
2η2¯h2
k/bardbl∇f(xk)/bardbl2≤αη¯hk
4/bardbl∇f(xk)/bardbl2. (37)
Proof.In fact, this lemma can be proved in an obvious way by separating into diﬀerent cases.
(i)If/bardbl∇f(xk)/bardbl≤c+τ0, it directly follows that L1/bardbl∇f(xk)/bardblη2dσ2/2≤L1(c+τ0)η2dc2σ2/2; otherwise, if
/bardbl∇f(xk)/bardbl>c+τ0, we know
¯hk= min/braceleftbigg
1,c
/bardblgk/bardbl/bracerightbigg
≥c
c+/bardblgk/bardbl≥c
c+τ0+ (τ1+ 1)/bardbl∇f(xk)/bardbl≥c
3/bardbl∇f(xk)/bardbl
thereforeη≤α/(6L1dcσ2)directly yields
L1/bardbl∇f(xk)/bardblη2dc2σ2
2≤ηα¯hk/bardbl∇f(xk)/bardbl2
4.
Then (34) follows from summing up these two cases.
(ii)If/bardbl∇f(xk)/bardbl≤2τ0/(1−τ1), then/bardblgk−∇f(xk)/bardbl≤τ0+τ1/bardbl∇f(xk)/bardbl≤2τ0/(1−τ1)and¯hk≤1, which
yield
(L0+L1/bardbl∇f(xk)/bardbl)η2¯h2
k/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright≤2(L0(1−τ1) + 2L1τ0)τ2
0
(1−τ1)3η2,
L0+L1/bardbl∇f(xk)/bardbl
2η2¯h2
k/bardblgk−∇f(xk)/bardbl2≤2(L0(1−τ1) + 2L1τ0)τ2
0
(1−τ1)3η2.
26Under review as submission to TMLR
Otherwise, if/bardbl∇f(xk)/bardbl>2τ0/(1−τ1), we note that/bardblgk−∇f(xk)/bardbl≤1+τ1
2/bardbl∇f(xk)/bardbland/bardblgk/bardbl≥
1−τ1
2/bardbl∇f(xk)/bardbl. Moreover,
¯hk(L0+L1/bardbl∇f(xk)/bardbl)≤c(L0+L1/bardbl∇f(xk)/bardbl))
/bardblgk/bardbl≤2c(L0+L1/bardbl∇f(xk)/bardbl))
(1−τ1)/bardbl∇f(xk)/bardbl≤cL0
τ0+2cL1
1−τ1.
Consequently, once η≤α
4τ0(1−τ1)
c(L0(1−τ1) + 2L1τ0), we have
(L0+L1/bardbl∇f(xk)/bardbl)η2¯h2
k/angbracketleft∇f(xk),gk−∇f(xk)/angbracketright
≤/parenleftbiggcL0
τ0+2cL1
1−τ1/parenrightbigg
η2¯hk/bardbl∇f(xk)/bardbl2≤ηα¯hk
4/bardbl∇f(xk)/bardbl2,
and
L0+L1/bardbl∇f(xk)/bardbl
2η2¯h2
k/bardblgk−∇f(xk)/bardbl2
≤1
2max/parenleftbiggcL0
τ0+2cL1
1−τ1/parenrightbigg
η2¯hk/bardbl∇f(xk)/bardbl2≤ηα¯hk
4/bardbl∇f(xk)/bardbl2.
We obtain (35) and (36) via summing up respective bounds for two cases.
(iii)We ﬁrstly derive a bound on ¯hk(L0+L1/bardbl∇f(xk)/bardbl). When/bardbl∇f(xk)/bardbl≤2τ0/(1−τ1), we know ¯hk(L0+
L1/bardbl∇f(xk)/bardbl)≤L0(1−τ1)+2L1τ0
1−τ1. Otherwise, we know ¯hk(L0+L1/bardbl∇f(xk)/bardbl)≤cL0
τ0+2cL1
1−τ1. The last
bound (37) can be derived directly by
L0+L1/bardbl∇f(xk)/bardbl
2η2¯h2
k/bardbl∇f(xk)/bardbl2
≤max/parenleftbiggL0(1−τ1) + 2L1τ0
1−τ1,cL0
τ0+2cL1
1−τ1/parenrightbiggη2
2¯hk/bardbl∇f(xk)/bardbl2≤ηα¯hk
4/bardbl∇f(xk)/bardbl2
via setting η≤α
2min/parenleftbigg1−τ1
L0(1−τ1) + 2L1τ0,τ0(1−τ1)
c(L0(1−τ1) + 2L1τ0)/parenrightbigg
.
In general, the four inequalities hold as long as we ensure (33).
Explanations in Remarks C.1 and C.2 also explain the motivations behind this proof. In the sequel, we will
use this lemma only with
α=α0:=τ0(1−τ1)
c(1−τ1) + 2τ0<1
2. (38)
Lemma D.2. In the statement of Theorem 3.4, we take η=/radicalBig
2
(L1(c+τ0)+L0)Tdc2σ2. Then the condition (33)
in Lemma D.1 holds as long as we run the algorithm long enough i.e. T≥C/parenleftbig
σ2,τ,L,d,c/parenrightbig
.
Proof.We see that
η=/radicalBigg
2
(L1(c+τ0) +L0)Tdc2σ2≤min/braceleftbiggα0
6L1dcσ2,α0(1−τ1)
2L0(1−τ1) + 4L1τ0,α0τ0(1−τ1)
4c(L0(1−τ1) + 2L1τ0)/bracerightbigg
is equivalent to
T≥2
(L1(c+τ0) +L0)dc2σ2max/braceleftbigg6L1dcσ2
α,2L0(1−τ1) + 4L1τ0
α(1−τ1),4c(L0(1−τ1) + 2L1τ0)
ατ0(1−τ1)/bracerightbigg2
.
27Under review as submission to TMLR
D.2 Final Procedures in Proof
Proof of Theorem 3.4. Equipped with Lemmas B.5, D.1 and D.3, we further wrote the one step inequality
(32) into
ηB(/bardbl∇f(xk)/bardbl)≤f(xk)−Ek[f(xk+1)]
+η2/parenleftbigg(L1(c+τ0) +L0)dc2σ2
2+4(L0(1−τ1) + 2L1τ0)τ2
0
(1−τ1)3η2/parenrightbigg
. (39)
We then separate the time index into
U=/braceleftbigg
k<T :/bardbl∇f(xk)/bardbl≥τ0
1−τ1/bracerightbigg
andUc={0,1,···,T−1}\U. Given this, we derive from (39) that for any k∈U,
τ0(c−1)
c(1−τ1) + 2τ0η/bardbl∇f(xk)/bardbl
≤f(xk)−Ek[f(xk+1)] +η2/parenleftbigg(L1(c+τ0) +L0)dc2σ2
2+4(L0(1−τ1) + 2L1τ0)τ2
0
(1−τ1)3η2/parenrightbigg
.
Similarly, together with α0≤1/2, (18) deduces that for any k∈Uc,
1
2η/bardbl∇f(xk)/bardbl2
≤f(xk)−Ek[f(xk+1)] +η2/parenleftbigg(L1(c+τ0) +L0)dc2σ2
2+4(L0(1−τ1) + 2L1τ0)τ2
0
(1−τ1)3η2/parenrightbigg
.
Sum these inequalities altogether to have
max/braceleftBigg
τ0(c−1)
c(1−τ1) + 2τ01
T/summationdisplay
k∈U/bardbl∇f(xk)/bardbl,1
2T/summationdisplay
k/∈U/bardbl∇f(xk)/bardbl2/bracerightBigg
≤(f(x0)−f∗)
Tη+η(L1(c+τ0) +L0)dc2σ2
2+η4(L0(1−τ1) + 2L1τ0)τ2
0
(1−τ1)3.
We minimize the sum of ﬁrst two terms by setting
η=/radicalBigg
2
(L1(c+τ0) +L0)Tdc2σ2. (40)
We then deﬁne
∆ = (Df+ 1)/radicalbigg
(L1(c+τ0) +L0)dc2σ2
2T+4(L0+ 2L1τ0)τ2
0
(1−τ1)3/radicalBigg
2
(L1(c+τ0) +L0)Tdc2σ2.(41)
Recallσ2can grow with T, so these two terms are O(σ/√
T),O(1/(σ√
T))respectively. Then we further
have
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤E
min

/radicalBigg
1
|U|/summationdisplay
k∈Uc/bardbl∇f(xk)/bardbl2,1
|U|/summationdisplay
k/∈U/bardbl∇f(xk)/bardbl



≤max/braceleftbigg√
4∆,2(c+ 2τ0)
τ0(c−1)∆/bracerightbigg
, (42)
28Under review as submission to TMLR
where the second inequality follows from the fact that either |U|≥T/2or|Uc|≥T/2. In the end, we capture
the leading terms in this upper bound to have
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤O/parenleftBigg
4/radicalbigg
(Df+ 1)2(L1(c+τ0) +L0)dc2σ2
2T/parenrightBigg
+O
/radicaltp/radicalvertex/radicalvertex/radicalbt/radicalBigg
2
(L1(c+τ0) +L0)Tdc2σ2(L0+ 2L1τ0)τ0
(1−τ1)3
.
Lemma D.3. In the statement of Corollary 3.5, we take η=/radicalBig
2
(L1(c+τ0)+L0)Tdc2σ2,σ=
c2B/radicalbig
Tlog(1/δ)/(N/epsilon1)andT≥O(N2/epsilon12/(B2c3dlog1
δ)). Then the condition in Lemma D.2 holds as long as
we have enough samples i.e. N≥C(/epsilon1,δ,τ,L,B,d,c ).
Proof.It is more straight-forward to verify the condition (22) directly. Firstly, we plug σ2=c2
2Tlog(1/δ)
N2/epsilon12from
Lemma 3.1 into the formula of ηto have
η=/radicalBigg
2
(L1(c+τ0) +L0)Tdc2σ2=N/epsilon1
c2T/radicalBigg
2
(L1(c+τ0) +L0)dc2log(1/δ)≤α0N2/epsilon12
6L1dc2c2
2Tlog(1/δ)=α0
6L1dc2σ2
as long as we have enough samples
N≥6L1cc2
/epsilon1α0/radicalBigg
2dlog(1/δ)
L1(c+τ0) +L0.
Other conditions
η=N/epsilon1
cc2T/radicalBigg
2
(L1(c+τ0) +L0)dlog(1/δ)≤min/braceleftbiggα0(1−τ1)
2L0(1−τ1) + 4L1τ0,α0τ0(1−τ1)
4c(L0(1−τ1) + 2L1τ0)/bracerightbigg
holds as long as we run the algorithm long enough
T
N≥min/braceleftbigg
1,2c
τ0/bracerightbigg2(L0(1−τ1) + 2L1τ0)/epsilon1
cc2α0(1−τ1)/radicalBigg
2
(L1(c+τ0) +L0)dlog(1/δ). (43)
The last requirement (43) naturally holds due to T≥O(N2/epsilon12/(B2c3dlog1
δ)).
Proof of Corollary 3.5. Moreover, we take the limit T≥O(N2/epsilon12/(B2c3dlog1
δ))to derive the privacy-utility
trade-oﬀ
E/bracketleftbigg
min
0≤k<T/bardbl∇f(xk)/bardbl/bracketrightbigg
≤O/parenleftBigg
4/radicalbigg
(Df+ 1)2(L1(c+τ0) +L0)dc2log(1/δ)
N2/epsilon12/parenrightBigg
,
ending the proof.
E Proof for Privacy Guarantee
This section presents a simple proof for Lemma 3.1. To begin with, we formally introduce the functional
view of Renyi Diﬀerential Privacy below. Deﬁne a functional as
/epsilon1M(α),sup
D,D/primeDα(M(D)/bardblM(D/prime)) = sup
D,D/prime1
α−1logEθ∼M(D/prime)/bracketleftbigg/parenleftbiggM(D)(θ)
M(D/prime)(θ)/parenrightbiggα/bracketrightbigg
,α≥1 (44)
whereM(D)denotes the distribution of the output with input DandM(D)(θ)refers to the density at θof
this distribution. The following propositions clarify several notions of diﬀerential privacy in the literature.
29Under review as submission to TMLR
Proposition E.1. LetMbe a randomized mechanism.
(i)If and only if /epsilon1M(∞)≤/epsilon1, thenMis/epsilon1-(pure)-DP (Dwork et al., 2014a).
(ii)If and only if /epsilon1M(α)≤/epsilon1, thenMis(α,/epsilon1)-RDP (Renyi diﬀerential privacy) (Mironov, 2017).
(iii)If and only if δ≥exp[(α−1)(/epsilon1M(α)−/epsilon1)]for someα≥1, thenMis(/epsilon1,δ)-DP (Dwork et al., 2014b).
(iv)If and only if /epsilon1M(α)≤ραfor anyα≥1, thenMisρ-zCDP (zero-concentrated diﬀerential privacy)
(Bun & Steinke, 2016).
(v)If and only if /epsilon1M(α)≤ραfor anyα∈(1,ω), thenMis(ρ,ω)-tCDP (truncated concentrated diﬀerential
privacy) (Bun et al., 2018).
We remark that Proposition E.1(iii) is adapted from the second assertion of Theorem 2 in Abadi et al. (2016),
while the literature prefers to use the converse argument for this assertion, Proposition 3 in Mironov (2017).
Here we also restate the composition theorem for Renyi diﬀerential privacy.
Proposition E.2 (Proposition 1, Mironov (2017)) .LetM=MT◦MT−1◦···◦M 1be deﬁned in an
interactively compositional way, then for any ﬁxed α≥1,
/epsilon1M(α)≤T/summationdisplay
i=1/epsilon1Mi(α).
DP-SGD and DP-NSGD under our consideration can both be decomposed into Tcomposition of sub-
sampled Gaussian mechanism with uniform sampling without replacement , denoted as Gaussian (σ)◦
subsample (N,B ). We write the privacy-accountant functional, (44), of this building-block mechanism as
ˆ/epsilon1(α).
It is widely known that the sole Gaussian mechanism has /epsilon1Gaussian (σ)(α) =α/(2σ2), Table II in Mironov
(2017), when the /lscript2-sensitivity of the unperturbed mechanism is normalized to 1. The sub-sampled Gaussian
mechanism is much more complicated and draws many previous eﬀorts. In particular, Abadi et al. (2016);
Mironov et al. (2019) study Poisson sub-sampling , which is less popular in practical sub-sampling; Wang
et al. (2019b) proposed a general bound for any uniformly sub-sampled RDP mechanisms, but their bound
is a bit loose when restricted to Gaussian mechanisms. Thankfully, Bun et al. (2018) developed a general
privacy-ampliﬁcation bound for any uniformly sub-sampled tCDP mechanisms, which is satisfying for our
later treatment. Speciﬁcally, we specify Theorem 11 in Bun et al. (2018) to the Gaussian mechanism, to get
the following proposition.
Proposition E.3 (Privacy Ampliﬁcation by Uniform Sub-sampling without Replacement) .For the very
mechanism Gaussian (σ)◦subsample (N,B )withB < 0.1N, we have the following privacy accountant
ˆ/epsilon1(α)≤7γ2α
σ2,∀α≤σ2
2log/parenleftbigg1
γ/parenrightbigg
, (45)
withγ=B/N.
Proof of Lemma 3.1. We denote whole composited mechanism as M. We view the summation of
clipped/normalized gradients as the unperturbed mechanism, so the Gaussian noise we add is N(0,σ2B2c2)
for DP-SGD and N(0,σ2B2). However, their respective privacy guarantee are still the same, since DP-SGD
has/lscript2sensitivitycwhile DP-NSGD has /lscript2sensitivity 1. By Propositions E.2 and E.3, we have
/epsilon1M(α)≤7Tγ2α
B2σ2,∀α≤B2σ2
2log/parenleftbigg1
γ/parenrightbigg
.
Further by Proposition E.1(iii), DP-SGD is (/epsilon1,δ)-DP if there exists α≤B2σ2
2log/parenleftBig
1
γ/parenrightBig
such that
7Tγ2α/(B2σ2)≤/epsilon1/2,
exp(−(α−1)/epsilon1/2)≤δ.
30Under review as submission to TMLR
Plus, we ﬁnd that when /epsilon1=c1γ2T, we can satisfy all these conditions by setting
σ≥c2γ/radicalbig
Tlog(1/δ)
B/epsilon1
for some explicit constants c1andc2.
31