Published in Transactions on Machine Learning Research (12/2024)
Inductive Global and Local Manifold Approximation
and Projection
Jungeum Kim Jungeum.kim@chicagobooth.edu
Booth School of Business
University of Chicago
Xiao Wang wangxiao@purdue.edu
Statistics Department
Purdue University
Reviewed on OpenReview: https: // openreview. net/ forum? id= p9pxeNupQ5& noteId= VEi3S62pV4
Abstract
Nonlinear dimensional reduction with the manifold assumption, often called manifold learn-
ing, has proven its usefulness in a wide range of high-dimensional data analysis. The signif-
icant impact of t-SNE and UMAP has catalyzed intense research interest, seeking further
innovations toward visualizing not only the local but also the global structure information
of the data. Moreover, there have been consistent eﬀorts toward generalizable dimensional
reduction that handles unseen data. In this paper, we ﬁrst propose GLoMAP, a novel man-
ifold learning method for dimensional reduction and high-dimensional data visualization.
GLoMAP preserves locally and globally meaningful distance estimates and displays a pro-
gression from global to local formation during the course of optimization. Furthermore,
we extend GLoMAP to its inductive version, iGLoMAP, which utilizes a deep neural net-
work to map data to its lower-dimensional representation. This allows iGLoMAP to provide
lower-dimensional embeddings for unseen points without needing to re-train the algorithm.
iGLoMAP is also well-suited for mini-batch learning, enabling large-scale, accelerated gra-
dient calculations. We have successfully applied both GLoMAP and iGLoMAP to the
simulated and real-data settings, with competitive experiments against the state-of-the-art
methods.
Keywords : Data visualization, deep neural networks, nonlinear dimensional reduction, inductive algorithm
1 Introduction
Data visualization, which belongs to exploratory data analysis, has been promoted by John Tukey since the
1970s as a critical component in scientiﬁc research (Tukey, 1977). However, visualizing high-dimensional
data is a challenging task. The lack of visibility in the high-dimensional space gives less intuition about what
assumptions would be necessary for compactly presenting the data on the reduced dimension. A common
assumption that can be made without speciﬁc knowledge of the high-dimensional data is the manifold as-
sumption. It assumes that the data are distributed on a low-dimensional manifold within a high-dimensional
space. Nonlinear dimensional reduction (DR) with the manifold assumption, often called manifold learning ,
has proven its usefulness in a wide range of high-dimensional data analysis (Meilă & Zhang, 2023). Various
research eﬀorts have been made to develop DR tools for data visualization, and several leading algorithms
include MDS (Cox & Cox, 2008), Isomap (Tenenbaum et al., 2000), t-SNE (Van der Maaten & Hinton,
2008), UMAP (McInnes et al., 2018), PHATE (Moon et al., 2017), PacMAP (Wang et al., 2021), and many
others. It is not uncommon to perform further statistical analysis on the reduced dimensional data, including
visualized data, through methods such as regression (Cheng & Wu, 2013), functional data analysis (Dai &
Müller, 2018), classiﬁcation (Belkina et al., 2019), and generative models (Qiu & Wang, 2021).
1Published in Transactions on Machine Learning Research (12/2024)
Figure 1: The visualization of the spheres dataset (Moor et al., 2020) by UMAP and GLoMAP (both trans-
ductive). GLoMAP shows a progression of the representation from global to local during the optimization.
All ten inner clusters are identiﬁed as well as the larger cluster on the outer shell (purple).
The purpose of this work is to address a key challenge in manifold learning: Can a single algorithm capture
both the global structures and local details of high-dimensional data? To answer this question, we develop
a new nonlinear manifold learning method called GLoMAP, which is short for Global and Local Manifold
Approximation and Projection. An essential component of GLoMAP is its locally adaptive global distance
construction, where locality is based on K-nearest neighbors, and global distance refers to the distance
between any general pair of data points. GLoMAP locallyapproximates the data manifold in a data-adaptive
way with many small low-dimensional Euclidean patches similar to UMAP (McInnes et al., 2018). As such
an approximation can result in multiple diﬀerent local distances of the same pairs, we ﬁrst reconcile the local
incompatibilities by taking the maximum of the multiple (ﬁnite) local distances. Then, we take a shortest
path search over those local maximums to obtain a coherent globalmetric. Our heuristic gives a single
topological space that reﬂects the global geometry of the data manifold without incompatible distances.
Our work presents two signiﬁcant algorithmic advancements. First, to address the crowding problem in visu-
alization highlighted by Van der Maaten & Hinton (2008), we adopt a UMAP-like loss function. Optimizing
this loss function with a global distance matrix (rather than a sparse matrix of K-nearest neighbors) presents
a challenge. To handle the challenge, we develop a novel sampling scheme that leverages an unbiased esti-
mator of the loss (Proposition 1) within a stochastic gradient descent framework. Second, our annealing-like
process for scaling—similar to cooling in temperature-is a novel and interesting development. It enhances the
optimization process by enabling a gradual progression from global shape to local detail. This progressive
global-to-local reﬁnement is unique in the literature and addresses a longstanding dilemma: global meth-
ods often lack local detail, while local methods rely on initialization for global coherence. Speciﬁcally, by
tempering with a global distance rescaler τ, the algorithm ﬁrst identiﬁes global structures (with a larger τ)
and subsequently reﬁnes local details (with a smaller τ). This eﬀect, illustrated in Figure 1, reveals a clear
progression from global structure identiﬁcation to ﬁne-grained local details, where all ten inner clusters are
distinguished, as well as the larger cluster on the outer shell (purple). We emphasize that this annealing
approach does not require carefully crafted initialization—random noise suﬃces. We ﬁnd that combining
annealing with global distance scaling was particularly eﬀective, as similar meaningful structures did not
emerge with rescaled local distances alone.
To achieve an inductive dimensional reduction mapping for the generalization of visualization to an unseen
novel data point, we extend GLoMAP to its inductive version named iGLoMAP (inductive GLoMAP). The
low dimensional embedding, denoted by z, for a high dimensional data point xis obtained by z=Qθ(x),
whereQθis modeled by a deep neural network (DNN) with parameters θ. When the map Qθis updated
by using a gradient from a pair of points, the low dimensional representation of the other pairs is also
aﬀected. We found that it is very diﬃcult to optimize the GLoMAP loss function with a typical deep
learning method where the visualization representations are by a neural network’s output. Therefore, as one
of our key contributions, we develop a particle-based algorithm which mimics the stable optimization process
2Published in Transactions on Machine Learning Research (12/2024)
of transductive learning. By this, we can formulate an eﬀective stochastic gradient descent algorithm for
learning the inductive map.
Our proposed global distance itself, constructed through a new local distance, also can be seen as an advance
inthearrayofglobalpreservationalgorithms. MDSkeepsthemetricinformationbetweenallthepairsofdata
points, andtherebyaimstopreservetheglobalgeometryofthedataset. RecognizingthatEuclideandistances
between non-neighboring points may not always be informative in high-dimensional settings, Isomap applies
MDS instead to geodesic distance estimates computed via the shortest path search. Eﬀorts to reﬁne this
geodesic distance estimator include improving the Euclidean distances among nearby points to better reﬂect
the local manifold, for instance, through a conformal embedding (Silva & Tenenbaum, 2002), a spherelets
argument (Li & Dunson, 2019), or the tangential Delaunay complex (Arias-Castro & Chau, 2020). On the
reﬁned local distances, these approaches apply the shortest path search to estimate global distances. As our
local distance estimator is in a closed form, the distance computation is a scalable alternative to Li & Dunson
(2019) and Arias-Castro & Chau (2020). Similar to our approach, the local distance in Silva & Tenenbaum
(2002) is computed by multiplying a closed-form local rescaling constant to the Euclidean distances. As
compared in Section S9 in Appendix, when our distance is instead used under the framework of Silva &
Tenenbaum (2002), the visualization result shows separation between clusters similar to Figure 1.
Now we discuss related works or compare our framework with the previous works. The seminal t-SNE
and UMAP preserve the distance among neighboring data points. It is known that these methods may
lose global information, leading to a misunderstanding of the global structure (Coenen & Pearce, 2019;
Rudin et al., 2022). To overcome the loss of global information, much eﬀort has been made, for example,
by good initialization (McInnes et al., 2018; Kobak & Berens, 2019; Kobak & Linderman, 2021) or by an
Euclidean distance preservation between selected non-neighboring points (Fu et al., 2019; Wang et al., 2021).
Our proposed GLoMAP improves this global structure preservation by estimating the global distances with
adopting a UMAP-like loss function. When it comes to the local distances, both UMAP uses rescale factors
that may enable local adaptability of estimated distances. Our local approximation diﬀers from that of
UMAP as our local distance estimator is in a closed form and has a consistent property. Additionally, UMAP
adopts a fuzzy union to allow their inconsistent coexistence in a single representation of the data manifold,
while we use the shortest path search seeking a coherent global metric. Many eﬀorts have been made to
extend existing visualization methods to utilize DNNs for generalizability or for handling optimization with
lower computational cost, especially when the data size is large (Van Der Maaten, 2009; Gisbrecht et al.,
2015; Pai et al., 2019; Sainburg et al., 2021). Our sampling schemes and particle-based algorithm may beneﬁt
these approaches in stabilizing the optimization.
The main contributions of this paper are as follows:
(1) We introduce GLoMAP, a novel dimensionality reduction method that captures both global and
local structures. The key inventions include locally-adaptive global distances and an annealing-
like process for scaling. These innovations enable GLoMAP to produce a progression from global
structure to local details during optimization, a feature not present in previous methods.
(2) We introduce iGLoMAP, an inductive version of GLoMAP, which avoids the need to re-run the
entire algorithm when a new data point is introduced. Our particle-based algorithm leverages
the advantages of inductive formulation while maintaining the stability of optimization, similar to
transductive learning.
(3) We establish the consistency of our new local geodesic distance estimator, which serves as the
building block for the global distance used in both the GLoMAP and iGLoMAP algorithms. The
global distance is shown to be an extended metric.
(4) We demonstrate the usefulness of both GLoMAP and iGLoMAP by applying them to simulated and
real data and conducting comparative experiments with state-of-the-art methods. All implementa-
tions are available at https://github.com/JungeumKim/iGLoMAP .
The rest of this article is structured as follows. In Section 2, we provide a brief review of some leading
DR methods. For a more complete review, we refer to Rudin et al. (2022). The GLoMAP and iGLoMAP
3Published in Transactions on Machine Learning Research (12/2024)
algorithms are introduced in Section 3. The theoretical analysis and justiﬁcations of our distance metric
are presented in Section 4. The numerical analysis in Section 5 demonstrates iGLoMAP’s competency in
handling both global and local information. This article concludes in Section 6.
2 Background and the Related Work
In manifold learning, the key design components of DR are: (a) What information to preserve; (b) How to
calculate it; (c) How to preserve it. From this perspective, in this section, we review several techniques, such
as Isomap, MDS, t-SNE, UMAP in common notation, and discuss their limitations and our improvements.
We also discuss PacMAP and PHATE in Section S2.
Letx1,...,xn∈X ⊆ Rpdenote the original dataset. Write X= [x1,...,xn]∈Rn×p. Our goal is to
ﬁnd embeddings z1,...,zn∈Rd, whered/lessmuchpandziis the corresponding representation of xiin the low-
dimensional space. Under many circumstances, d= 2is used for visualization. Write Z= [z1,...,zn]∈
Rn×d.
2.1 Global methods
MDS is a family of algorithms that solves the problem of recovering the original data from a dissimilarity
matrix. Dissimilarity between a pair of data points can be deﬁned by a metric, by a monotone function on
the metric, or even by a non-metric functions on the pair. For example, in metric MDS, the objective is to
minimize stress, whichisdeﬁnedby stress (z1,...,zn) =/parenleftbig/summationtext
i/negationslash=j(dz(zi,zj)−f(dx(xi,xj)))2/parenrightbig1/2foramonotonic
rescalingfunction f. Dependingonhowweseetheinputspacegeometry, thechoicesofdissimilaritymeasures
fordxanddzwould vary. The common choice of the L2distance corresponds to an implicit assumption
that the input data is a linear isometric embedding of a lower-dimensional set into a high-dimensional space
(Silva & Tenenbaum, 2002).
Isomap can be seen as a special case of metric MDS where the dissimilarity measure is the geodesic distance,
i.e., Isomap tries to preserve the geometry of the data manifold by keeping the pairwise geodesic distances.
For this, Isomap ﬁrst constructs a K-nearest neighbor (KNN) graph where the edges are weighted by the
L2distance among the KNNs. Then it estimates the geodesic distance by applying a shortest path search
algorithm, such as Dijkstra’s algorithm (Tenenbaum et al., 2000). The crux of Isomap is the use of a
shortest path search to approximate global geodesic distances, with the underlying assumption that the
manifold is ﬂat and without intrinsic curvature. There have been multiple attempts to improve the geodesic
distance estimator under more relaxed conditions. For instance, Isomap has been extended to conformal
Isomap (Silva & Tenenbaum, 2002) to accommodate manifolds with curvature. Both their approach and
ours construct global distances by ﬁnding the shortest path over locally rescaled distances (although we use
diﬀerent rescalers). Our work extends this locally adaptive global approach beyond the MDS framework,
addressing the crowding problem identiﬁed by Van der Maaten & Hinton (2008). Similarly, other eﬀorts
have also sought to handle general manifolds by calibrating local distances. For example, Arias-Castro
& Chau (2020) employed the tangential Delaunay complex and Li & Dunson (2019) used a spherelets
argument with the decomposition of a local covariance matrix near each data point. In our work, we use
a computationally eﬃcient local distance estimator albeit the global distance construction becomes more
heuristic. Nevertheless, we believe that these distances from previous work could also be integrated into our
framework, providing better theoretical understanding of the chosen global distance, pending improvements
in eﬃcient computation.
2.2 Local methods
t-SNE, as introduced by Van der Maaten & Hinton (2008), is one of the most cited works in manifold
learning for visualization. Instead of equally weighting the diﬀerence between the distances on the input and
embedding spaces, t-SNE adaptively penalizes this distance gap. More speciﬁcally, for each pair (xi,xj),
pijdeﬁnes a relative distance metric in the high-dimensional space, where the relative distance qijin the
low-dimensional space is optimized to match pijaccording to the Kullback-Leibler (KL) divergence. For any
4Published in Transactions on Machine Learning Research (12/2024)
two points xiandxj, deﬁne the conditional probability
pj|i=exp(−/bardblxi−xj/bardbl2/2σ2
i)/summationtext
k/negationslash=iexp(−/bardblxi−xk/bardbl2/2σ2
i), (1)
whereσiis tuned by solving the equation log2perplexity =−/summationtext
jpj|ilog2pj|i. Intuitively, perplexity is a
smooth measure of the eﬀective number of neighbors. Deﬁne the symmetric probability as pij=pi|j+pj|i
2n.
In the embedding space, t-SNE adopts a t-distribution formula for the latent probability, deﬁned by qij=
(1 +d2
ij)−1/Z, wheredij=/bardblzi−zj/bardbl, and the normalizing constant is Z=/summationtext
l/summationtext
k/negationslash=l(1 +d2
kl)−1. The loss
function of t-SNE is the KL divergence between pandq,KL(p/bardblq) =/summationtext
i/negationslash=jpijlogpij
qij. The weight matrix of
a graph, such as (pij)or(qij), which encodes proximity, is called an aﬃnity matrix. To handle datasets with
heterogeneous density, Van Assel et al. (2024) extends t-SNE by maintaining constant entropy for each row
in the aﬃnity matrix using an optimal transport framework, deﬁning a doubly stochastic matrix (normalized
by both rows and columns) for both input and visualization space.
Another highly cited visualization technique is UMAP (McInnes et al., 2018), which is often considered a
new algorithm based on t-SNE (Rudin et al., 2022), known for being faster and more scalable (Becht et al.,
2019). UMAP achieves a signiﬁcant computational improvement by changing the probability paradigm of
t-SNE to Bernoulli probability on every single edge between a pair. This removes the need for normalization
in t-SNE in both distributions {pij}and{qij}that require summations over the entire dataset. Another
innovation brought about by UMAP is a theoretical viewpoint on the local geodesic distance as a rescale of
the existing metric on the ambient space (Lemma 1 in McInnes et al. (2018)). McInnes et al. (2018) further
develops a local geodesic distance estimator near xideﬁned as
dumap
i(xi,xj) =/braceleftbigg∞ forj/negationslash∈Niorj=i,
(/bardblxi−xj/bardbl−ρi)/ˆσiotherwise,(2)
whereNiis the index set of the KNN of xi,ρiis the distance between xiand its nearest neighbor, and ˆσiis
an estimator of the rescale parameter for each i. In our work, we address the previously unexplained choices
ofˆσiandρiin (2) by proposing a new local geodesic distance estimator and theoretically establishing its
consistency.
To embrace the incompatibility between local distances, as indicated by dumap
i(xi,xj)/negationslash=dumap
j(xi,xj),
McInnes et al. (2018) treats the distances as uncertain , so that those incompatible dumap
i(xi,xj)and
dumap
j(xi,xj)may co-exist by a fuzzy union. UMAP deﬁnes a weighted graph with the edge weight, or
the membership strength, for each edge (i,j)asνij=pij+pji−pijpji, wherepij= exp{−dumap
i(xi,xj)}.
Meanwhile, on the low-dimensional embedding space, the edge weight for each edge (i,j)is deﬁned by
qij=/parenleftbig
1+a/bardblzi−zj/bardbl2b/parenrightbig−1, whereaandbare hyperparameters. Then, UMAP tries to match the membership
strength of the representation graph of the embedding space to that of the input space by using a fuzzy set
cross entropy, which can be seen as a sum of KL divergences between two Bernoulli distributions such as
L(Z) =/summationdisplay
ijKL(νij|qij) =/summationdisplay
ijνijlogνij
qij+ (1−νij) log(1−νij)
(1−qij). (3)
In UMAP, the global structure is preserved through a good initialization, e.g., through the Laplacian Eigen-
maps initialization (McInnes et al., 2018). In our work, we seek a visualization method that does not depend
on initialization for global preservation. Another diﬀerence of our work is the way to merge the incompatible
local distances. We alter the fuzzy union of UMAP by ﬁrst taking the maximum among ﬁnite local distances
and then merging the local information by the shortest path search, thereby obtaining global distances.
3 Methodology
In this section, we ﬁrst present a new framework, called GLoMAP, which is a transductive algorithm for
nonlinear manifold learning. GLoMAP consists of three primary phases: (1) global metric computation; (2)
representation graph construction by information reduction; (3) optimization of the low-dimensional embed-
ding by a stochastic gradient descent algorithm. During the optimization process, the data representation
5Published in Transactions on Machine Learning Research (12/2024)
continuously evolves, initially revealing global structures and subsequently delineating more detailed local
structures. Furthermore, we introduce iGLoMAP, an inductive version of GLoMAP, which employs a map-
per to replace vector representations of data. iGLoMAP is trained using a proposed particle-based inductive
algorithm, designed to mimic the stable optimization process of GLoMAP. Once the mapper is trained, a
new data point is easily mapped to its lower dimensional representation without any additional optimization.
3.1 Global metric computation
Recall that{xi}n
i=1denotes the original data in Rpand{zi}n
i=1are the corresponding embeddings in Rd.
Our goal is to construct two graphs GXandGZthat, respectively, represent the geometry on the input
data manifold and the embedding space. We then formulate an objective function as a dissimilarity measure
between the graphs GXandGZto project the input space geometry onto the embedding space. The
key information to construct the representation graph GXof the input space is the global distance matrix
between allpairs of data points. The global distance will be determined based on the local distances,
where the locality is deﬁned by the K-nearest neighbor Ki={X(k)(xi)}K
k=1for eachxi. When we obtain
the estimate of the local geodesic distance by a rescaled L2distance, two neighboring points can have two
possible rescalers because each of the two points provides its own local view. Therefore, we handle this
ambiguity by considering the minimum of the two rescalers, which corresponds to taking the maximum of
the two distances. Consequently, the local distance estimate between xiandxjis given by
ˆdloc(xi,xj) =/braceleftBigg/bardblxi−xj/bardbl
min{ˆσxi,ˆσxj},ifxj∈Kiorxi∈Kj,
∞, otherwise,(4)
where ˆσxis the local normalizing estimator deﬁned by
ˆσ2
x=1
KK/summationdisplay
k=1/bardblx−X(k)(x)/bardbl2. (5)
The theoretical rationale for the choice of the local normalizing constant ˆσ2
xwill be provided in Section 4
under the local Euclidean assumption. Using ˆdlocas the local building block, we can construct a global
distance matrix. We construct a weighted graph, denoted by Gloc, in whichxiandxjare connected by an
edge with the weight ˆdloc(xi,xj)ifˆdloc(xi,xj)is ﬁnite. Given the weighted graph (Gloc,ˆdloc), we apply a
shortest path search algorithm, e.g., Dijkstra’s algorithm, to construct the global distance between any two
data points. Note that ˆdloc(xi,xj)<∞whenxj∈Kiorxi∈Kj. Therefore, the shortest path search on
Gloccan be seen as an undirected shortest path search on a KNN graph, where the distances are locally
rescaled. As a result, for any two data points xandy, the search gives
ˆdglo(x,y) = min
P/parenleftbigˆdloc(x,u1) +···+ˆdloc(up,y)/parenrightbig
, (6)
wherePvaries over the graph path between xandy. This process is outlined in Algorithm 1. In line 4,
D2, sets the distances to zero for all but the K-nearest neighbors, indicating disconnections on the neighbor
graph. Consequently, the elementwise-max operation in Line 8 yields the intended ˆdlocin equation 4. Note
that for the distance of the disconnected elements (nodes), a shortest path search algorithm assigns ∞. This
implies that when two points are disconnected based on the local graph Gloc, they are regarded to be on
diﬀerent disconnected manifolds, so that the global distance is deﬁned as ∞.
The search for the shortest path on a neighbor graph with Euclidean distances is well studied by Tenenbaum
et al. (2000). However, with our locally adaptive building blocks, comprehending the properties of the
resultant global distance becomes challenging. Therefore, the distance construction in equation 6 is heuristic
in its nature. In Section 4.2, the construction of the global distance will be explained through the lens of
the coequalizer on extended pseudometric spaces. The eﬀectiveness of this approach in capturing the global
structure is demonstrated in Examples 1 and 2, as well as in Section 5.
3.2 Graph construction and optimization
Having constructed our global distance ˆdglobetween all data pairs, we can construct an undirected graph GX
that represents the data manifold. For the embedding space, assuming it is Euclidean with the L2distance,
6Published in Transactions on Machine Learning Research (12/2024)
Algorithm 1 Global distance construction
1:Input:then×npairwiseL2distance matrix from Xdenoted by D2
2:Fixed input : the number of neighbor K(default: 15)
3:Initialization
4:Construct the KNN distance matrix DKfromD2
5:Rescale
6:Calculatendistinct ˆσ2by a row-wise mean among ﬁnite squared distances of DK.
7:(Rescale) Deﬁne D=DK[i,j]/min{ˆσ[i],ˆσ[j]}
8:(Symmetrization) Dloc= max{D,DT}, where max is elementwise-max
9:Distance construction
10:RunDglo=S(Dloc), whereSis an undirected shortest path search algorithm
Algorithm 2 GLoMAP (Transductive dimensional reduction)
1:Fixed input : distance matrix Dglo, trade-oﬀ parameter λe, number of epochs nepoch, learning rate
scheduleαschand tempering schedule τsch,c= 4.
2:Learnable input: randomly initialized representation Z
3:Optimization
4:fort= 1,...,n epoch,
5: Updateα=αsch[t]andτ=τsch[t]
6: Compute the membership strength µij= exp(−Dglo[i,j]/τ), µi·=/summationtext
jµij
7: fork= 1,...,niter,
8: Sample mini-batch indices S={i1,...,im}
9: Sample a neighbor xjifromµj|ifori∈S
10: ˆLglo=−/summationtext
i∈Sµi·logqiji−λe/summationtext
i∈S/summationtext
j∈S(1−µij) log (1−qij)
11: Z←optimizer (Z,α,∇ZˆLglo,clip=c)
12: end for
13:end for
we can construct another undirected graph GZrepresenting the embedding space. Speciﬁcally, consider the
data matrix X= [x1,...,xn]and the embedding matrix Z= [z1,...,zn]. The shared index i∈I={1,...,n}
identiﬁes the data point xiand its embedding zi. Here we construct two weighted graphs of the nodes I
denoted by GX= (I,Ex)andGZ= (I,Ez). Deﬁne the undirected (symmetric) weighted adjacency matrix
EXwith a temperature τby
EX[i,j]≡µij= exp{−ˆdglo(xi,xj)/τ} (7)
fori/negationslash=j, where the global distance ˆdglois deﬁned in (6). Note that if the KNN graph Glocused to construct
ˆdglois not a single connected graph, but several, then there exists (i,j)s.t. ˆdglo=∞. In this case, the
weight of the edge (i,j)is0by (7). Similarly, with the L2distance for the embedding graph, deﬁne the
undirected weighted adjacency matrix EZby
EZ[i,j]≡qij=/parenleftbig
1 +a/bardblzi−zj/bardbl2b/parenrightbig−1(8)
fori/negationslash=j, wherea= 1.57694andb= 0.8951are tunable hyperparameters. This deﬁnition of qijand thea
andbvalues were originally used by McInnes et al. (2018).
To force the two graphs GXandGZto be similar, that is, to force the edge weights to be similar, we optimize
the sum of KL divergences between the Bernoulli edge probabilities µijandqij. Letj= 0,...,nibe the
index thatµij>0. Then,
Lglo(Z) =/summationdisplay
i,j∈IKL(µij|qij) =−n/summationdisplay
i=1n/summationdisplay
j=1µijlogqij−λen/summationdisplay
i=1n/summationdisplay
j=1(1−µij) log (1−qij). (9)
We call the ﬁrst term in (9) the positive term because decreasing −logqijforces the pair i,jcloser, and the
second term the negative term because decreasing −log(1−qij)works oppositely. We multiply the negative
7Published in Transactions on Machine Learning Research (12/2024)
Figure 2: The transductive visualization of the hierarchical dataset (Wang et al., 2021). Top: The results
of UMAP. Bottom: From the random initialization, GLoMAP ﬁnds ﬁrst the macro, then meso and then all
the micro clusters a progressive way during the optimization.
term byλe>0as a tunable weight because in some applications, having control over attractive forces (the
positive term) and repulsive forces (the negative term) can be preferred for improved aesthetics (Belkina
et al., 2019; Kobak & Berens, 2019). We set λe= 1as default. This loss construction with the Exponential-
and-t formulation is similar to t-SNE and UMAP, where each probability of the pair (i,j)approaches one
when the points (xi,xj)or(zi,zj)are near with each other, and approaches zero when they are far away
from each other. We optimize equation 9 through its stochastic version in the following proposition.
Proposition 1. An unbiased estimator of Lglo(Z)up to a constant multiplication is
ˆLglo(Z) =−/summationdisplay
i∈Sµi·logqiji−λe/summationdisplay
i∈S/summationdisplay
j∈S(1−µij) log (1−qij), (10)
whereµi·=/summationtextn
j=1µijandSis a uniformly sampled index set from I={1,...,n}andjiis sampled from a
conditional distribution µj|i=µij/summationtextn
j=1µij.
ProofSee Section S1.1.
We apply stochastic gradient descent (SGD) to optimize the loss in (10) with respect to Z, decaying the
learning rate αand temperature τ. The tempering through decreasing τshifts the focus from global to local,
therebycatalyzingtheprogressionofvisualizationfromaglobaltoalocalperspectiveinasingleoptimization
process. For a more in-depth discussion of tempering τ, refer to Section S5.1. To stabilize SGD, we adopt
two optimization techniques from McInnes et al. (2018). First, the gradient of each summand of ˆLiglo(Z)
is clipped by a ﬁxed constant c. Second, the optimizer ﬁrst updates Zw.r.t. the negative term, then
again updates w.r.t. the positive term, which is calculated with the updated Z. The described transductive
GLoMAP algorithm is in Algorithm 2.
This stochastic optimization approach is highly sought after, especially in our global distance context. The
optimization ofLglo(Z)in (9) is challenging because the number of pairs with nonzero µijcan beO(n2).
Note that although the loss formulation Lglo(Z)in (9) resembles that of t-SNE and UMAP, they do not have
the same computational burden because the number of all pairs with non-zero µijisO(nK) =O(n), and
thus their schemes consider allsuch pairs at once or sequentially. Therefore, we optimize Lglo(Z)through
ˆLiglo(Z)in equation 10 with a mini-batch sampling. The caution may arise as each iteration does not
necessarily involve the entire dataset. This problem is handled by an inductive formulation in the following
section.
Example 1. We apply GLoMAP to two exemplary simulated datasets with both global and local structures.
The spheres dataset (Moor et al., 2020) has ten inner clusters and an eleventh spread on a large outer shell.
Moor et al. (2020) found that UMAP identiﬁes all inner clusters but not the outer shell one. The hierarchical
dataset (Wang et al., 2021) features a three-layer tree with ﬁve child clusters per parent, across ﬁve trees.
8Published in Transactions on Machine Learning Research (12/2024)
Wang et al. (2021) showed UMAP captures micro-level clusters but not meso- or macro-level ones. Detailed
data descriptions and hyperparameters are in Section S3 and Section S4 respectively. Figure 1 and 2 show
UMAP and iGLoMAP representations at diﬀerent optimization stages. UMAP uses spectral embedding for
initialization, while GLoMAP starts randomly. In Figure 1, GLoMAP on the spheres dataset shows a clear
transition from global to local structure during optimization. At epoch 125, it separates the outer shell
(purple) from the ten inner clusters, which then become distinct in subsequent epochs. This illustrates how
GLoMAP evolves from global to detailed local structures. Similarly, Figure 2 for the hierarchical dataset
reveals a progression: by epoch 50, GLoMAP distinguishes all macro-level clusters, then identiﬁes meso-level
clusters, and ﬁnally, all micro-level clusters, while preserving higher-level structures. We attribute this to
GLoMAP’s distance metrics that balance global and local information. For the spheres dataset, the distinct
separation of the outer shell and inner clusters is due to improved local distance estimation and the use of a
smaller rescaler in equation 4, discussed further in Remark 2. The theoretical analysis of our local distance
estimate will be provided in Section 4. The eﬀect of the aesthetics parameter λeis detailed in Section S5.2.
3.3 iGLoMAP: The particle-based inductive algorithm
To achieve an inductive dimensional reduction mapping, we parameterize the embedding vector using a
mapperQθ:X→Rd, whereQθcan be modeled by a deep neural network, and the resulted embeddings are
zi=Qθ(xi)fori= 1,...,n. Then the loss in equation 9 becomes Lglo(Qθ(x1),...,Qθ(xn))and is optimized
with respect to Qθ. Due to Proposition 1, we can use an unbiased estimator of ˆLglo(Qθ(x1),...,Qθ(xn))
with ˆLglodeﬁned in equation 10 for stochastic optimization. We develop a particle-based algorithm to stably
optimize the inductive formulation. The idea is that the evaluated particle z=Qθ(x)is ﬁrst updated in a
transductive way (as updating zin Algorithm 2), and then Qθis updated accordingly by minimizing the
squared error between the original and updated z. The proposed particle-based inductive learning is in
Algorithm 3. Note that this particle-based approach does not increase any computational cost in handling
the DNN compared to a typical deep learning optimization; the DNN is evaluated/diﬀerentiated only one
time over the entire mini-batch. At the same time, it individually regularizes the gradient of each pair due
to the transductive step in Line 12 in Algorithm 3. In our implementation, the optimizer for θis set as the
Adam optimizer (Kingma & Ba, 2015) with its default learning hyperparameters, i.e., β= (0.9,0.999)with
learning rate decay 0.98. The Adam optimizer is renewed every 20 epochs.
In the iGLoMAP algorithm, the entire dataset is aﬀected by every step of stochastic optimization through
the mapper Qθ. This is in contrast to the previous transductive GLoMAP algorithm, where an embedding
update of a pair of points does not aﬀect the embeddings of the other points. Due to the generalization
property of the mapper, we empirically found that the iGLoMAP algorithm needs fewer iterations than the
transductive GLoMAP algorithm, as will be shown in the following example. Note that the particle-based
algorithm of iGLoMAP is distinguished from two stage-wise algorithms that ﬁrst complete the transductive
embedding and then train a neural network to learn the embedding (for example, Duque et al. (2020)). In
such cases, the transductive embedding stage cannot enjoy the generalization property of the neural network
during training.
Example 2. We apply iGLoMAP on the spheres and hierarchical datasets in Example 1 with the same
hyperparameter settings. As a mapper, we train a basic fully connected neural network as described in
Section 5. The visualization results of the training set and also the generalization performance on the test
set are presented in Figure 3. In the spheres dataset, the ten inner clusters remain distinct from the outer
shell (colored purple), and this separation is still evident in the generalization. For the hierarchical dataset,
the test set is generated with diﬀerent level of random corruption (in noise level, micro level, or meso level)
from the training set. As these levels increase, the generalization appears diﬀerent, yet various levels of
clusters can still be observed. We make some other empirical observations. First, as mentioned above, the
iGLoMAP algorithm needs fewer iterations compared to the transductive GLoMAP algorithm. We trained
it for 150 epochs, in contrast to the 300 epochs for the transductive algorithm. Second, we found that even
a small starting τdoes not hinder the discovery of global structures; for an analogous ﬁgure to Figure S10,
see Figure S29 in Section S8. These ﬁndings suggest that the deep neural network might inherently encode
some global information. For example, with iGLoMAP on the hierarchical dataset using the default K= 15
neighbors, which is generally too few for meso level cluster connectivity, the macro and meso clusters are
9Published in Transactions on Machine Learning Research (12/2024)
Figure 3: The visualization and generalization performance of iGLoMAP on the hierarchical and spheres
dataset. Spheres: All ten inner clusters are identiﬁed with the outer shell (purple) scattered around. Hier-
archical: all levels of clusters are identiﬁed.
Algorithm 3 iGLoMAP (Inductive dimensional reduction)
1:Fixed input : distance matrix Dglo, trade-oﬀ parameter λe, number of epochs nepoch, learning rate
scheduleαschand tempering schedule τsch,c= 4.
2:Learnable input: randomly initialized encoder Qθparametrized by θ
3:Optimization
4:fort= 1,...,n epoch,
5: Updateα=αsch[t], η=ηsch[t]andτ=τsch[t]
6: Compute the membership strength µij= exp(−Dglo[i,j]/τ), µi·=/summationtext
jµij
7: fork= 1,..., n_iter,
8: Sample mini-batch indices S={i1,...,im}
9: Sample a neighbor xjkfromµjk|ikforik∈S
10: Z={zik,zjk}m
k=1wherezik=Qθ(xik), zjk=Qθ(xjk)
11: ˆLglo=−/summationtext
ik∈Sµik·logqikjk−λe/summationtext
i∈S/summationtext
j∈S(1−µij) log (1−qij)
12: ˜Z=optimizer (Z,α,∇ZˆLglo,clip=c)
13: ˆLz=/bardblZ−˜Z/bardbl2
Fwhere/bardbl·/bardblFis the Frobenius norm
14: θ←optimizer (θ,η,∇θˆLz)w.r.t.θ, where ˜Zis regarded as constant
15: end for
16:end for
partially preserved even from the start, as shown in Figure S31 in Section S8. Furthermore, only within 5
epochs, all level of clusters are identiﬁed. For the sensitivity of λe, see Figure S32 in Section S8. This ﬁgure
demonstrates λe’s impact, similar to that in the transductive case, but with a much milder eﬀect.
Remark 1. The two main hyperparameters of our method are the negative weight λeand the tempering
schedule of τ. Smallerλeleads to tighter clustering, while larger values disperse clusters more, as demon-
strated in Examples 1 and 2 (Figures S11, S12, and S32 in Section S8). We typically ﬁx λe= 1, and
recommend λe= 0.1for a tighter clustering and λe= 10for more relaxed cluster shapes. For tempering,
since the impact τ’s varies between datasets, we normalize the distances throughout this paper, aiming for a
median of 3. Our standard practice begins with τ= 1, reducing to τ= 0.1. This range may not suit every
dataset, and thus we provide guidance for adjusting τ. Our method evolves from random noise to global, then
local shapes. If it remains noisy for a long time, e.g., over half the epochs, it suggests a too high initial τ,
giving insuﬃcient time for local detail development. In such cases, it is advisable to reduce the starting τ; in
10Published in Transactions on Machine Learning Research (12/2024)
fact, halving it or even reducing it to a quarter may be beneﬁcial. If meaningful shapes still emerge towards
the end, the ﬁnal τmight also be too high, indicating the need to halve the ﬁnal τ.
4 Theoretical Analysis
One of the key innovations of GLoMAP compared with existing methods is the construction of the distance
matrix. Roughly speaking, our distance is designed to combine the best of both worlds: like Isomap, the
distances are globally meaningful by adopting its shortest path search; like t-SNE and UMAP, the distance is
locally adaptive because the shortest path search is conducted over locally adaptive distances. In Section 4.1,
we establish that our local distances are consistent geodesic distance estimators under the same assumption
utilized in UMAP. Then, in Section 4.2, we provide theoretical insights into our heuristic, which constructs a
single global metric space by coherently combining local geodesic estimates. Recall that a geodesic distance
dM, given a manifold M, is deﬁned by dM(x,y) = infs{length(s)}, wheresvaries over the set of (piecewise)
smooth arcs connecting xtoyinM.
4.1 Local geodesic distance
This section aims to justify the local geodesic distance estimator on a small local Euclidean patch of the
manifoldM. Consider a dataset in Rp, distributed on a d-dimensional Riemannian manifold M1, where
d≤p. We make the following assumptions on (M,g)and the data distribution.
A1. Assume that, for a given point u∈M, there exists a geodesically convex neighborhood Uu⊂M
such that (Uu,g)composes a d-dimensional Euclidean patch with scale λu>0, i.e.,g=λug∗, where
g∗is a Euclidean metric restricted to a d-dimensional subspace.
A2. Assume that Mis compact. The data are then assumed to be i.i.d. samples drawn from a uniform
distribution on M.
A1 characterizes a local neighborhood around a ﬁxed point on Mthat near the point u,Mbehaves locally
as ad-dimensional Euclidean space (up to scaling). A2 assumes that the data are uniformly distributed on
the manifold. We assume compactness to ensure ﬁnite volume, allowing us to deﬁne a uniform distribution.
Alternatively, we could assume that the manifold Mhas ﬁnite volume under the metric g. A2 is beneﬁcial
for theoretical reasons, as pointed out in the work of Belkin and Nuyogi on Laplacian eigenmaps (Belkin
& Niyogi, 2001; 2003) and also observed by McInnes et al. (2018). Assumptions A1 and A2 together are
similar to those of UMAP, which interprets a denser neighborhood as a local manifold approximation with a
largerλuand a sparser neighborhood as one with a smaller λu. Intuitively, this brings an eﬀect of projecting
(ﬂattening) the manifold locally onto a Euclidean space. Now we present a distance theorem that connects
the local geodesic distance with the L2distance.
Theorem 1. Denotex∼Pthe uniform probability distribution on Mthat satisﬁes A2.
(a) Assume A1 regarding a point u∈Mand its open convex neighborhood Uu. For any pair (x,y)that
belongs toUuandλu>0,dM(x,y) =√λu/vextenddouble/vextenddoublex−y/vextenddouble/vextenddouble
2.
(b) Assume A1 and A2 regarding a point u∈Mwith its neighbor Uuandλu. LetV=vol(M). Assume,
for anm∈[0,1],B2(u,G P,m(u))⊂Uu, whereGP,t:x∈M/mapsto→inf{r > 0;P(B2(x,r))≥t}and
B2(x,r)is anL2ball2centered at xwith radius r. Then,√λu=C/δ P,m(u), whereCis a universal
constant that does not depend on uorλubut only on d,m,V, and
δP,m:x∈M/mapsto→/radicalBigg
1
m/integraldisplaym
0G2
P,tdt, (11)
1Given manifold M, aRiemannian metric onMis a family of inner products, {/angbracketleft·,·/angbracketrightu:u∈M}, on each tangent space,
TuM, such that/angbracketleft·,·/angbracketrightudepends smoothly on u∈M. A smooth manifold with a Riemannian metric is called a Riemannian
manifold . The Riemannian metric is often denoted by g= (gu)u∈M. Using local coordinates, we often use the notation
g=/summationtextd
i=1/summationtextd
j=1gijdxi⊗dxj, wheregij(u) =/angbracketleft(∂
∂xi)u,(∂
∂xj)u/angbracketrightu.
2The ambient metric is used to deﬁne B2(x,r).
11Published in Transactions on Machine Learning Research (12/2024)
is the distance function to measure (DTM) w.r.t. a distribution P.
ProofSee Section S1.2.
Theorem 1 (a) marginally generalizes Lemma 1 from McInnes et al. (2018), which originally introduced the
concept of viewing the local geodesic distance as a rescale of the existing metric on the ambient space. In
Theorem 1 (b), we connect λuto DTM (Chazal et al., 2011) by noticing that under the uniform distribution
assumption, the probability of a set is closely related to its volume. By this result, for any pair (x,y)in a
geodesically convex Ux, we have3
dM(x,y)≡C
δP,m(x)/bardblx−y/bardbl2. (12)
This formulation of dM(x,y)through DTM δP,m(x)is important because its unbiased estimator is identical
toˆσxin equation 5. Therefore, by the result in Theorem 1 (b), we know that ˆσx/Cis an unbiased estimator
of1/√λx. Now, by using ˆσx, we deﬁne a local geodesic distance estimator of dM(x,y)/Cas
ˆdx(x,y) =/braceleftbigg1
ˆσx/bardblx−y/bardbl,ify∈Ux,
∞, otherwise.(13)
The next theorem establishes the consistency of ˆdx.
Theorem 2. Assume A1 and A2 regarding a point x∈Mwith its neighbor Uxand sample size n. Consider
a ﬁxedm∈[0,1]such thatB2(x,G P,m(x))⊂Ux. For anyξ >0, asn→∞with withKas the smallest
natural number greater than or equal to mn,
P/parenleftBig
∃y∈Ux,s.t./vextendsingle/vextendsingle/vextendsingle1−d2
M(x,y)/C2
ˆd2x(x,y)/vextendsingle/vextendsingle/vextendsingle≥ξ/parenrightBig
−→0.
ProofSee Section S1.3.
Theorem 2 demonstrates that ˆdx(x,y)converges in probability to1
CdM(x,y), whereCis a universal constant
independent of the local center x’s choice. Thus, ˆdx(x,y)estimates the geodesic distance up to a constant
multiplication. Note that for dimension reduction, the scale of the input space is relatively unimportant,
and thus, we regard C= 1. Consequently, by establishing an equivalence relationship dM(x,y) =/bardblx−y/bardbl2
δP,m(x),
we can assert that ˆdx(x,y)consistently estimates dM(x,y)locally for any x∈M.
4.2 Construction of global metric space in extended-pseudo-metric spaces
When we apply the local geodesic distance estimation as described in equation 13, treating each data point as
the local center, we eﬀectively obtain ndiﬀerent local approximations of the data manifold. In this section,
our goal is to coherently glue these approximations together. By treating these as ndiﬀerent metric systems
on the shared space Xwith|X|=n, we employ an equivalence relation analogous to Spivak’s coequalizer
in extended-pseudo-metric spaces. This method presents an alternative to the fuzzy union approach used in
McInnes et al. (2018), with both methods aiming to merge local information amidst potential inconsistencies
among local assumptions. While the fuzzy union encapsulates the global manifold by aggregating all local
distances into asingle set, our shortest path approach intricately interweaves thesedistances to form auniﬁed
(extended) metric system.
LetAbe the set of all indices of data points X, and for all a∈A, letXa= (X,da)denote the space deﬁned
through the localized metric daof UMAP in equation 2 or our approach in equation 13. As shown in McInnes
et al. (2018), this localized space Xafor anya∈Ais an extended-pseudo-metric space. Metaphorically
speaking, the points in Xathat are within a ﬁnite distance from aunderdacan be thought of as composing
an “island” centered around a(SeeXain Figure S28, the red-colored part). We deﬁne faas an operator that
mapsfromXtoXa. Now, wesimplymergetheseextended-pseudo-metricspacesintooneas XA=/coproducttext
a∈AXa,
combining all local information. Let dA(y,y/prime) =da(y,y/prime)ify,y/prime∈XaanddA(y,y/prime) =∞otherwise. This
3Note that instead of δP,m(u), in (12), we can use δP,m(z)for anyz∈Uusuch thatB2(z,G P,m(z))⊂Uu.
12Published in Transactions on Machine Learning Research (12/2024)
space again is an extended-pseudo-metric space and satisﬁes the universal property for a coproduct (Spivak
(2009), Lemma 2.2). Recalling the island analogy, XAnow contains nislands, as depicted in Figure S28
(in the blue box labeled XA). Note that for a,a/prime∈A, whileXaandXa/primeare distinct extended-pseudo-
metric spaces, they share a very important commonality. They all share the same underlying data points
sinceXa= (X,da)andXa/prime= (X,da/prime). Givenx∈X, however, the two points xa=fa(x)∈Xaand
xa/prime=fa/prime(x)∈Xa/primeare considered distinct points in XAwithdA(xa,xa/prime) =∞. We will sew those inherently-
identical points into one in a coherent way through a needle of equivalence relation.
Spivak (2009) considers a coequalizer diagram of sets
AZY,g1
g2q(14)
whereq= [−]is the equivalence relation on Zwithx∼x/primeif there exists a∈Awithx=g1(a)andx/prime=g2(a).
GivendZas a metric on Z,deﬁne a metric dYonYby
dY([z],[z/prime]) = inf(dZ(p1,q1) +dZ(p2,q2) +···+dZ(pn,qn)), (15)
where the inﬁmum is taken over all pairs of sequences (p1,...,pn),(q1,...,qn)of elements of Z,such that
p1∼z,qn∼z/prime, andpi+1∼qifor all 1≤i≤n−1. According to Spivak (2009), when (Z,dZ)is a extended-
pseudo-metric space, (Y,dY)is another extended-pseudo-metric space and satisﬁes the universal property of
a coequalizer. His construction of coequalizer through equivalence relation can provide in our setting a tool
to merge two possibly-conﬂicting distances into one coherent distance. In the above diagram let Z=XA
and letgibe the function that maps Xto the subset in XAthat is induced by Xai, where we enumerate A
throughA={a1,...,an}.First, consider g1andg2. The coequalizer diagram equation 14 regarding g1and
g2merges the subspaces in XAthat correspond to Xa1andXa2into a smaller space (Y,dY), eliminating the
redundancy discussed above. As illustrated in Figure S28, the disconnected two inherently-identical points
now are identiﬁed as identical, and the two islands are linked, forming a larger island (in the sense that more
points are connected with ﬁnite distance to others).
Under the design of Spivak (2009) in equation 15, two possibly inconsistent local distances are merged into
one by taking the minimum of them. We consider instead to take the maximum between the two as follows.
Deﬁne a ﬁnite max operator (a maximum among ﬁnite elements), denoted by
fmaxa∈A{f(a)}:=/braceleftbigg
maxa∈Af{f(a)},ifAf/negationslash=∅,
∞, otherwise,(16)
whereAf={a∈A|f(a)<∞}. Recalling that Z=XA, we deﬁne the local building block as df(p,q) :=
fmaxx∼p,y∼q/braceleftbig
dZ(x,y)/bracerightbig
to reconcile the incompatibility. Then, the merged distance on Yin equation 15 is
replaced as
dY([z],[z/prime]) = inf(df(p1,q1) +df(p2,q2) +···+df(pn,qn)). (17)
Now, we consider all g1,...,gn. We extend the equivalence relation [−], deﬁningx∼x/primeif there exist some
a∈Aand indices i,jsuch thatgi(a) =xandgj(a) =x/prime.In this case, dYin equation 17 satisﬁes the
conditions of an extended-pseudo-metric.
Proposition 2. The distance dYdeﬁned in equation 17 is an extended-pseudo-metric, which satisﬁes: 1)
dY(x,y)≥0; 2)dY(x,y) =dY(y,x); 3)dY(x,y)≤dY(x,w) +dY(w,y)ordY(x,y) =∞.
ProofSee Section S1.3.1.
OnY,dYdeﬁnes the distances between all pairs in the dataset without inconsistencies among distances.
Now, the following theorem says that the global distance of GLoMAP is a special case of the metric dYin
equation 17, given that two neighboring points are neighbors to each other. Furthermore, it states that in
this case,dYis an extended metric, which is a stronger notion than an extended pseudo-metric.
Theorem 3. Assumex∈Kyiﬀy∈Kxforx,y∈X, whereKxis the K-nearest neighbor of x. Consider
(Xa,da)constructed by the local geodesic distance estimator deﬁned in equation 13 with Uxapproximated by
Kx. ThendYin equation 17 is identical to ˆdgloin equation 6 and an extended metric.
13Published in Transactions on Machine Learning Research (12/2024)
Figure 4: The 3D datasets are generated from the displayed 2D rectangles. Left: S-curve dataset, Middle:
Severed Sphere dataset, Right: Eggs dataset.
ProofSee Section S1.3.2.
Remark 2. If a minimum operator is used instead of the fmax operator, the distance in equation 17 corre-
sponds to the distance deﬁned by Spivak (2009) as in equation 15. The diﬀerence between these two distances
is subtle, yet the modiﬁcation in equation 17 reﬂects our view that when each of two points has a diﬀerent
local scale, the smaller scale should be employed to deﬁne the distance between them. In other words, the
larger local distance should be used. Conversely, the fuzzy union approach of UMAP results in a ﬁnal lo-
cal distance that is shorter than either of the two conﬂicting local distances.4Therefore, our approach and
that of McInnes et al. (2018) reﬂect diﬀerent philosophical perspectives. We believe that the impact of this
philosophical divergence becomes signiﬁcant in practical applications. For instance, in the Spheres dataset
in Examples 1 and 2, points on the outer shell seem very distant from the perspective of the inner small
clusters, while from the outer shell’s perspective, the inner clusters do not appear comparatively far. We
believe that the choice of perspective signiﬁcantly inﬂuences the visualization outcome, as demonstrated in
Figure 1, where GLoMAP aligns with the former perspective.
5 Numerical Results
In this section, we compare our approach with existing manifold learning methods and demonstrate its
eﬀectiveness in preserving both local and global structures. We consider two scenarios that allow for a fair
comparison: 1)caseswherelower-dimensionalpointsaresmoothlytransformedintoahigh-dimensionalspace,
withthegoalbeingtorecoverthelower-dimensionalpoints; and2)scenarioswherelabelinformationrevealing
the data structure is available. We provide the GLoMAP and iGLoMAP implementation as a Python
package. The detailed learning conﬁgurations are in Section S4. Our analysis begins with transductive
learning, followed by inductive learning.
5.1 Transductive Learning
WecompareGLoMAPwithexistingmanifoldlearningmethods, includingIsomap, t-SNE,UMAP,PaCMAP,
and PHATE, which inherently employ transductive learning. First, we consider a number of three-
dimensional datasets to visually compare the DR results, while also quantitatively measuring the perfor-
mance. Second, we study DR for three high-dimensional datasets that inherently form clusters so that the
KNN classiﬁcation result can be used to measure the DR performance.
4As described in Section 2.2, the membership strength given by a fuzzy union is deﬁned by νij=pij+pji−pijpji,
wherepij= exp{−dumap
i(xi,xj)}andpji= exp{−dumap
j(xi,xj)}. Hence, it can be seen as νij= exp{−x}for somex≤
min{dumap
i(xi,xj),dumap
j(xi,xj)}.
14Published in Transactions on Machine Learning Research (12/2024)
Figure 5: The dimensional reduction for the S-curve, Severed Sphere, Eggs dataset.
5.1.1 Manifolds embedded in three dimensional spaces
Dataset and performance measure. We study DR for three datasets: S-curve, Severed Sphere, and
Eggs, shown in Figure 4. All three datasets are obtained by embedding data points on a two-dimensional
box into a three-dimensional space using a smooth function. Detailed data descriptions are presented in
Section S3. We adopt two performance measures. The ﬁrst one is the correlation between the original
two-dimensional and the dimensional reduction L2distances. The second one is the KL-divergence between
distance-to-measure (DTM) type distributions, as used in Moor et al. (2020), which is deﬁned by
fX
σ(x) :=/summationdisplay
y∈Xexp/parenleftBig
−dist(x,y)2
σ/parenrightBig
//summationdisplay
x∈X/summationdisplay
y∈Xexp/parenleftBig
−dist(x,y)2
σ/parenrightBig
,
whereσ > 0represents a length scale parameter. For distinfX
σ, we use the L2distance on the original
two-dimensional space, and for distinfZ
σ, we use the L2distance on the two-dimensional embedding space.
The denominator is for normalization so that/summationtext
x∈XfX
σ(x) = 1. The KL-divergence given σis
KLσ(X,Z) =/summationdisplay
ifX
σ(xi) logfX
σ(xi)
fZσ(zi).
We varyσfrom 0.001 to 10. A larger σfocuses more on global preservation, while a smaller σfocuses more
on local preservation.
Results. The visualization results are shown in Figure 5, where GLoMAP demonstrates clear and infor-
mative results. Across all three datasets, we observe rectangular shapes and color alignments, indicative of
successful preservation of both global and local structures. Isomap and GLoMAP consistently demonstrate
visually compatible recovery across all three datasets. PacMAP also achieves this in the Severed Spheres and
Eggs datasets. The visualization by PacMAP of the S-curve demonstrates local color alignment although
it shows an S-shape. A similar pattern (but with a U-shape) is observed with t-SNE and UMAP in the
S-curve. Although the global connectivity of the Eggs dataset is not displayed by t-SNE and UMAP, they
demonstrate local structure preservation by local color alignments for each egg shape. Consistent with these
visual observations, the quantitative measures, plotted in Figure S30, reveal that GLoMAP has the compet-
itive results overall. Depending on the run, the visualization of GLoMAP of the S-curve and Eggs dataset
can be twisted (Figure S36 (c) and (d)). Even when it happens, the performance measures are similar, as
the overall global shape is quite similar.
15Published in Transactions on Machine Learning Research (12/2024)
Figure 6: Top: the Spheres. GLoMAP separates points on the larger outer shell from the smaller inner
clusters. Middle: the hierarchical dataset. GLoMAP catches all the global, meso and micro level data
structures. Bottom: the MNIST. GLoMAP clearly displays the ten digit clusters.
5.1.2 High dimensional cluster structured datasets
Dataset. We consider the case where the label information revealing the data structure is available. We
apply GLoMAP on hierarchical data (Wang et al., 2021), Spheres data (Moor et al., 2020), and MNIST
(LeCun et al., 2010). The hierarchical data have ﬁve macro clusters, each macro cluster contains ﬁve meso
clusters, and each meso cluster contains 5 micro clusters. Therefore, the 125 clusters can be seen as 25 meso
clusters, where 5 meso clusters also compose one macro cluster. One micro cluster contains 48 observations,
and the dimension of the hierarchical dataset is 50. The Spheres data by (Moor et al., 2020) are consisted
of 10000 points in 101 dimensional space. Half of the data is uniformly distributed on the surface of a large
sphere with radius 25, composing an outer shell. The rest reside inside the shell relatively closer to the
origin, composing 10 smaller Spheres of radius 5. Detailed data descriptions are presented in Section S3.
The MNIST database is a large database of handwritten digits 0∼9that is commonly used to train various
image processing systems. The MNIST database contains 70,000 28×28grey images with the class (label)
information, and is available at http://yann.lecun.com/exdb/mnist/ . The MNIST dataset is regarded
as the most important and widely used real dataset for evaluating the data structure preservation of a DR
method (Van der Maaten & Hinton, 2008; McInnes et al., 2018; Wang et al., 2021). Since in the later section,
iGLoMAP generalizes to unseen data points, we use 60,000images for training (and in the later section,
the other 10,000images for generalization). The baseline methods are also applied to the 60,000training
images.5
Performance measure. As a performance measure, we use KNN classiﬁcation accuracy as described in
the followings: Since a data observation comes with a label, we can calculate the classiﬁcation precision by
ﬁtting a k-nearest neighbor (KNN) classiﬁer on the embedding low-dimensional vectors (based on the L2
distance). For the hierarchical dataset, the KNN can measure both local and global preservation; The KNN
based on the micro labels shows local preservation, while the KNN based on the macro labels demonstrate
global preservation. For the Spheres, KNN is a local measure for the small inner clusters, and a proxy for
global separation between the outer shell and the inner clusters. For the MNIST, the KNN measures local
preservation. If we assume that the data points within the same class have a stronger proximity than between
classes, the KNN classiﬁcation accuracies on the DR results are a reasonable measure of local preservation.
5Our computational memory could not handle Isomap on the 60,000MNIST images. Therefore, we applied Isomap only on
the ﬁrst 6,000MNIST images.
16Published in Transactions on Machine Learning Research (12/2024)
Figure 7: The KNN measures for increasing K.
Note that the labels of the MNIST dataset allow global interpretation by human perceptual understandings
of the similarity between numbers. For example, a handwritten 9 is often confused with a handwritten 4.
Results. The comparison of the visualization with other leading DR methods is presented in Figure 6 and
performance measure in Figure 7. On the Spheres, the proposed GLoMAP shows very intuitive visualization
results similar to what one would probably draw on a paper based on the data description. The inner ten
clusters are enclosed by the other points that make up a large disk. Similarly, for the hierarchical dataset,
we can clearly identify all levels of the hierarchy structure. We can see that although the inner ten clusters
are less clearly identiﬁed, Isomap also gives the global shape, such that the outer points are well spread
out. For methods such as t-SNE, PaCMAP, and UMAP, the outer shell points of the Spheres dataset stick
with the inner points making ten clusters. For the hierarchical dataset, no baseline method catches the
nested clustering structure; where either the global structure or the meso level clusters is missed. These
visual observations are corroborated by the KNN classiﬁcation plot in Figure 7, which demonstrates the
eﬀective KNN classiﬁcation performance on GLoMAP’s DR. For the hierarchical dataset, only GLoMAP
shows almost perfect meso and macro level classiﬁcation. Also, for MNIST, GLoMAP achieves more than
97% of the KNN accuracy numbers, demonstrating GLoMAP’s competence in local preservation.
5.2 Inductive Learning
We apply iGLoMAP and compare it with other leading parametric visualization methods, including Para-
metric UMAP (PUMAP) (Pai et al., 2019), TopoAE (Moor et al., 2020), and DIMAL (Sainburg et al.,
2021), which can be seen as a parametric Isomap. These methods employ deep neural networks for map-
ping; they are detailed in further detail in Section S2. For iGLoMAP’s mapper, we utilize a fully connected
ReLU network with three hidden layers, each of width 128. Following each hidden layer, we incorporate
a batch normalization layer and ReLU activation. The network’s ﬁnal hidden layer is transformed into 2
dimensions via a linear layer. For the other methods, we either employ the default networks provided or
replicate the network designs used in iGLoMAP. Speciﬁcally, for MNIST, PUMAP, and TopoAE are given
the conﬁgurations (including hyperparameters and network designs) recommended by the original authors.
The DR results are presented in Figure 8.6iGLoMAP exhibits similar visual qualities to GLoMAP but
with more clearly identiﬁable global and meso-level clusters for the hierarchical and MNIST datasets. For
instance, from the MNIST results in Figure 9, we observe that similar numbers form groups, such as (7,9,4),
(0,6), and (8,3,5,2), while the ten distinct local clusters corresponding to the ten-digit numbers are evident.
The generalization capability of iGLoMAP is depicted in Figure 9 for MNIST and in Figure S33 in Section
S8 for the S-curve, Severed, and Eggs datasets, demonstrating almost identical visualizations for unseen
data points. Interestingly, the enhanced performance of parametric UMAP over (transductive) UMAP,
despite sharing the same framework, reinforces our conjecture in Example 2 that incorporating DNNs aids
in preserving global information. Nonetheless, PUMAP’s performance on the spheres dataset serves as a
cautionary example, indicating that the application of DNNs is not a universal solution for bridging the
gap in global information representation. On the Eggs dataset, PUMAP often displayed a more pronounced
disconnection than that shown in Figure 8, as shown in Figure S36 (a) in Section S8. We also observed that
depending on the speciﬁc run, the Eggs and S-curve visualizations of iGLoMAP can appear twisted, as in
Figure S36 (b) in Section S8. The numerical performance metrics are presented in Figures S34 and S35 in
6DIMAL collapsed on the hierarchical dataset and is thus not included in Figure 8.
17Published in Transactions on Machine Learning Research (12/2024)
Figure 8: Comparison with other inductive models that utilize deep neural network.
Figure 9: The iGLoMAP generalizations for the MNIST. The generalization on the test set is almost identical
to the original DR on the training set.
Section S8, demonstrating that iGLoMAP is compatible with or outperforms other methods on the datasets
used.
6 Conclusion
In this paper, we proposed GLoMAP, a uniﬁed framework for high-dimensional data visualization capable of
both global and local preservation. By tempering τ, we observed a transition from global formation to local
18Published in Transactions on Machine Learning Research (12/2024)
detail within a single optimization process. This is attributed to the global distance construction with locally
adaptive distances as the building blocks, oﬀering an alternative to UMAP’s fuzzy union. Furthermore, our
algorithms, which are randomly initialized, do not rely on optimal initialization for global preservation.
Additionally, we extend the GLoMAP algorithm to its inductive variant, iGLoMAP, by incorporating deep
learning techniques to learn a dimensionality reduction mapping.
Wenow mentionseveral future research directions on GLoMAP and iGLoMAP. First, the full impact of using
DNNs remains to be explored. We have observed that the use of DNNs can encode some global information
as shown with iGLoMAP on hierarchical dataset with small K(Example 2 and Figure S31) and parametric
UMAP on various datasets as well (Figure 8). Also, in Section 5, the output of iGLoMAP is generally similar
to that of GLoMAP, but not exactly the same. More detailed investigation seems necessary. Additionally,
reducingthecomputationaldemandsofouralgorithmisapriority. Whilewehavereducedthecomputational
cost for local distance estimation, managing the global distances, conversely, increases the cost. The shortest
path search step represents the most computationally intensive aspect of our framework. Depending on the
algorithm, the shortest path search can cost up to O(n3K). In our experiment, when the number of points
is more than 60,000, we have experienced a signiﬁcant computational bottleneck. One possible resolution
could be a landmark extension such as that of t-SNE or Isomap (Van der Maaten & Hinton, 2008; Silva &
Tenenbaum, 2002). Moreover, the neighbor sampling scheme, which involves summing membership scores
overndistances when all data points are connected, incurs costs comparable to the normalization of t-SNE.
Currently, this issue can be addressed through approximations, as discussed in Section S5.3, but further
development to improve computational eﬃciency would be advantageous. Currently, our implementation
is solely in Python, and it could beneﬁt from optimization through advanced languages and interfaces like
Numba, Cython, or C++. See Figure S20 for a comparison of the current computational time with other
leading baselines. In this paper, we did not consider the problem of noise in data. Robustness against noise
is an important problem of dimensional reduction, which is worth a subsequent study, but is beyond the
scope of current work. Additionally, the uniform assumption applied in this study may be too rigid. Finally,
the current applicability of our proposed methods is limited to datasets without missing data. Determining
how to eﬀectively incorporate missing data into the algorithm remains an intriguing and challenging area for
future research. We believe that our results nevertheless serve as a valuable addition towards visualization
of both global and local structures in data, useful in practice.
References
Ery Arias-Castro and Phong Alain Chau. Minimax estimation of distances on a surface and minimax
manifold learning in the isometric-to-convex setting. arXiv preprint arXiv:2011.12478 , 2020.
Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai Guan
Ng, Florent Ginhoux, and Evan W Newell. Dimensionality reduction for visualizing single-cell data using
UMAP.Nature biotechnology , 37(1):38–44, 2019.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and cluster-
ing. InProceedings of the 14th International Conference on NIPS: Natural and Synthetic , pp. 585–591,
Cambridge, MA, USA, 2001. MIT Press.
MikhailBelkinandParthaNiyogi. Laplacianeigenmapsfordimensionalityreductionanddatarepresentation.
Neural Computation , 15(6):1373–1396, 2003. doi: 10.1162/089976603321780317.
Anna C Belkina, Christopher O Ciccolella, Rina Anno, Richard Halpert, Josef Spidlen, and Jennifer E
Snyder-Cappione. Automated optimized parameters for t-distributed stochastic neighbor embedding im-
prove visualization and analysis of large datasets. Nature communications , 10(1), 2019.
Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad
Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Ar-
naud Joly, Brian Holt, and Gaël Varoquaux. API design for machine learning software: experiences from
the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning ,
pp. 108–122, 2013.
19Published in Transactions on Machine Learning Research (12/2024)
Frédéric Chazal, David Cohen-Steiner, and Quentin Mérigot. Geometric inference for probability measures.
Foundations of Computational Mathematics , 11(6):733–751, 2011.
Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Bertrand Michel, Alessandro Rinaldo, Alessandro Rinaldo,
and Larry Wasserman. Robust topological inference: Distance to a measure and kernel distance. The
Journal of Machine Learning Research , 18(1):5845–5884, 2017.
Ming-Yen Cheng and Hau-tieng Wu. Local linear regression on manifolds and its geometric interpretation.
Journal of the American Statistical Association , 108(504):1421–1434, 2013.
Andy Coenen and Adam Pearce. Understanding UMAP. Google PAIR , 2019.
MichaelAACoxandTrevorFCox. Multidimensionalscaling. In Handbook of data visualization , pp.315–347.
Springer, 2008.
Xiongtao Dai and Hans-Georg Müller. Principal component analysis for functional data on riemannian
manifolds and spheres. The Annals of Statistics , 46(6B):3334–3361, 2018.
AndrésFDuque, SachaMorin, GuyWolf, andKevinMoon. Extendableandinvertiblemanifoldlearningwith
geometry regularized autoencoders. In 2020 IEEE International Conference on Big Data , pp. 5027–5036.
IEEE, 2020.
Cong Fu, Yonghui Zhang, Deng Cai, and Xiang Ren. Atsne: Eﬃcient and robust visualization on gpu
through hierarchical optimization. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining , pp. 176–186, 2019.
Andrej Gisbrecht, Alexander Schulz, and Barbara Hammer. Parametric nonlinear dimensionality reduction
using kernel t-SNE. Neurocomputing , 147:71–82, 2015.
Jacob M Graving and Iain D Couzin. Vae-sne: a deep generative model for simultaneous dimensionality
reduction and clustering. BioRxiv, 2020.
Takuya Isomura and Taro Toyoizumi. Dimensionality reduction to maximize prediction generalization ca-
pability. Nature Machine Intelligence , 3(5):434–446, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the 3rd
International Conference on Learning Representations , 2015.
Dmitry Kobak and Philipp Berens. The art of using t-SNE for single-cell transcriptomics. Nature commu-
nications , 10(1):5416, 2019.
Dmitry Kobak and George C Linderman. Initialization is critical for preserving global data structure in both
t-SNE and UMAP. Nature biotechnology , 39(2):156–157, 2021.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [Online].
Available: http://yann. lecun. com/exdb/mnist , 2:18, 2010.
Didong Li and David B Dunson. Geodesic distance estimation with spherelets. arXiv preprint
arXiv:1907.00296 , 2019.
Leland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approximation and projection
for dimension reduction. arXiv preprint arXiv:1802.03426 , 2018.
Marina Meilă and Hanyu Zhang. Manifold learning: what, how, and why. Annual Review of Statistics and
Its Application , 11, 2023.
Kevin R Moon, David van Dijk, Zheng Wang, William Chen, Matthew J Hirn, Ronald R Coifman, Natalia B
Ivanova, Guy Wolf, and Smita Krishnaswamy. Phate: a dimensionality reduction method for visualizing
trajectory structures in high-dimensional biological data. BioRxiv, pp. 120378, 2017.
20Published in Transactions on Machine Learning Research (12/2024)
MichaelMoor, MaxHorn, BastianRieck, andKarstenBorgwardt. Topologicalautoencoders. In International
conference on machine learning , pp. 7045–7054. PMLR, 2020.
Gautam Pai, Ronen Talmon, Alex Bronstein, and Ron Kimmel. Dimal: Deep isometric manifold learning
using sparse geodesic sampling. In 2019 IEEE Winter Conference on Applications of Computer Vision
(WACV) , pp. 819–828. IEEE, 2019.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011.
Yixuan Qiu and Xiao Wang. ALMOND: Adaptive latent modeling and optimization via neural networks
and langevin diﬀusion. Journal of the American Statistical Association , 116(535):1224–1236, 2021. doi:
10.1080/01621459.2019.1691563. URL https://doi.org/10.1080/01621459.2019.1691563 .
Peter J Rousseeuw. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis.
Journal of computational and applied mathematics , 20:53–65, 1987.
Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable
machine learning: Fundamental principles and 10 grand challenges. Statistic Surveys , 16:1–85, 2022.
Tim Sainburg, Leland McInnes, and Timothy Q Gentner. Parametric UMAP embeddings for representation
and semisupervised learning. Neural Computation , 33(11):2881–2907, 2021.
Vin Silva and Joshua Tenenbaum. Global versus local methods in nonlinear dimensionality reduction.
Advances in neural information processing systems , 15, 2002.
David I Spivak. Metric realization of fuzzy simplicial sets. Preprint, pp. 4, 2009.
Joshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290(5500):2319–2323, 2000.
John W. Tukey. Exploratory Data Analysis . Addison-Wesley, 1977.
Hugues Van Assel, Titouan Vayer, Rémi Flamary, and Nicolas Courty. Snekhorn: Dimension reduction with
symmetric entropic aﬃnities. Advances in Neural Information Processing Systems , 36, 2024.
Laurens Van Der Maaten. Learning a parametric embedding by preserving local structure. In Artiﬁcial
intelligence and statistics , pp. 384–391. PMLR, 2009.
Laurens Van der Maaten and Geoﬀrey Hinton. Visualizing data using t-SNE. Journal of machine learning
research, 9(11), 2008.
Jarkko Venna and Samuel Kaski. Neighborhood preservation in nonlinear projection methods: An experi-
mental study. In International conference on artiﬁcial neural networks , pp. 485–491. Springer, 2001.
Yingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. Understanding how dimension
reduction tools work: An empirical approach to deciphering t-SNE, UMAP, TriMap, and pacmap for
data visualization. Journal of Machine Learning Research , 22(201):1–73, 2021. URL http://jmlr.org/
papers/v22/20-1061.html .
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Zelin Zang, Shenghui Cheng, Linyan Lu, Hanchen Xia, Liangyu Li, Yaoting Sun, Yongjie Xu, Lei Shang,
Baigui Sun, and Stan Z Li. Evnet: An explainable deep network for dimension reduction. IEEE Transac-
tions on Visualization and Computer Graphics , 2022.
21Published in Transactions on Machine Learning Research (12/2024)
Appendix
S1 Proofs of Theorems and Propositions
In this Appendix we prove the following theorem from Section 3 and Section 4.
S1.1 Proof of Proposition 1
We only need to show that Lglo(Z) =cES[ˆLiglo(Z)]for some constant c>0, where
Lglo(Z) =−n/summationdisplay
i=1n/summationdisplay
j=1µijlogqij−λen/summationdisplay
i=1n/summationdisplay
j=1(1−µij) log (1−qij). (S2)
We ﬁrst rewrite the ﬁrst term in (S2) by using an expectation form. We say i∼µi·whenifollows a discrete
distribution over {1,...,n}with probability (µ1·,...,µn·). Likewise, we say j|i∼µj|iwhenj|ifollows a
discrete distribution over {1,...,n}with probability (µ1|i,...,µn|i)whereµj|i:=µij/summationtext
jµij.Then we can rewrite
(S2) as
n/summationdisplay
i=1n/summationdisplay
j=1µijlogqij=EiEj|i/bracketleftbig
logqij/bracketrightbig
,
wherej|i∼µj|iandi∼µi·. Now, for easy sampling, we further consider U, a uniform distribution over
{1,...,n}. Then, now we have a formulation for importance sampling
EiEj|i/bracketleftbig
logqij/bracketrightbig
=Ei∼U/bracketleftBigµi·
1/nEj|i/bracketleftbig
logqij/bracketrightbig/bracketrightBig
=nEi∼U/bracketleftBig
µi·Ej|i/bracketleftbig
logqij/bracketrightbig/bracketrightBig
.
Its one-sample unbiased estimator is nµi·logqijiwherei∼Uandji∼µj|i. Now,Sis simply a multi-sample
version of the one-sample case. Therefore,/summationtext
i∈Sµi·logqijiis an unbiased estimator of/summationtextn
i=1µi·logqijiup
to a constant multiplication, i.e., c=n/|S|.
Now, rewrite the second term in (S2) again by using an expectation form. This time, however, we use two
independent uniform distributions i∼Uandj∼U.That is,
n/summationdisplay
i=1n/summationdisplay
j=1(1−µij) log (1−qij) =n2Ei∼UEj∼U/bracketleftbig
(1−µij) log (1−qij)/bracketrightbig
. (S3)
When we have two independent copies i∼Uandj∼U,an unbiased estimator of (S3) is n2(1−
µij) log (1−qij). To extend this idea, when we have i⊥{j1,...,jm−1}wherejk’s are i.i.d. from U, again,
an unbiased estimator of (S3) is
n2
m−1m−1/summationdisplay
k=1(1−µijk) log (1−qijk).
Note thatS={i1,...,im}is a set of independent draws from U. Therefore, for any ik,we have independency
ik⊥{i1,..,ik−1,ik+1,...,im}.Therefore, we have another unbiased estimator of (S2), that is
n2
m(m−1)/summationdisplay
i∈S/summationdisplay
j∈S\i(1−µij) log (1−qij).
Note thatqii= 0so that log(1−qii) = 0. Therefore, above estimator is simpliﬁed without any change in
value to
n2
m(m−1)/summationdisplay
i∈S/summationdisplay
j∈S(1−µij) log (1−qij).
This proves that/summationtext
i∈S/summationtext
j∈S(1−µij) log (1−qij)is an unbiased estimator of/summationtextn
i=1/summationtextn
j=1(1−µij) log (1−qij)
up to a constant multiplication, i.e., c=n2/m(m−1).
1Published in Transactions on Machine Learning Research (12/2024)
Note that, for the two terms, the constant multiplications applied for unbiasedness are diﬀerent. However,
becauseλeis a ﬂexible tuning hyperparameter, we can simply redeﬁne λeby multiplying the ratio of the
two constant multiplications.
S1.2 Proof of Theorem 1
Proof[Proof of (a)] Denote the Euclidean metric on Rpby¯gand that of scale λuasλu¯g. SinceUuis
Euclidean and convex, the shortest geodesic on Uuthat connects any x,y∈Uuis a straight line, that is a
straight line in the Euclidean geometry. We deﬁne
ΓM={γ/vextendsingle/vextendsingleγ(0) =x,γ(1) =y,smooth curve on M},
ΓU={γ/vextendsingle/vextendsingleγ(0) =x,γ(1) =y,smooth curve on Uu}.
Then, for any x,y∈Uu,
dM(x,y) = inf
γ∈ΓM{length (γ)}= inf
γ∈ΓU{length (γ)}= inf
γ∈ΓU/integraldisplay1
0/radicalbig
g( ˙γ(t),˙γ(t))dt
= inf
γ∈ΓU/integraldisplay1
0/radicalbig
λu¯g( ˙γ(t),˙γ(t))dt=/radicalbig
λuinf
γ∈ΓU/integraldisplay1
0/bardbl˙γ(t)/bardbldt=/radicalbig
λud2(x,y).
In the second equation, we changed the inﬁmum over ΓMtoΓU. This is because by the geodesic convex-
ity ofUu, the shortest geodesic arc that connects x,yis inΓU(which is the shortest arc that connects x,y.)
Proof[Proof of (b)] Under A2,X1∼Unif (M), i.e., pdff(x) =1
V1x∈M, whereV=Vol(M). Form∈[0,1],
deﬁne the global constants Cd:=2πd/2
Γ(d
2+1)andCd,m,V :=/parenleftBig/radicalBig
d+2
d/parenleftBig
Cd
Vm/parenrightBig1
d/parenrightBig−1
. AssumeB2(x,G P,m)∈Uuon
whichg=λu¯g. SinceUuisd-dimensional Euclidean, we can consider new coordinates x1,...,xdas spanning
Uuso that the volume element is dV=/radicalbig
det(g)dx1∧···∧dxd. Then, the volume of Vol(B2(x,t))is
Vol(B2(x,t)) =/integraldisplay
B2(x,t)/radicalbig
det(g)dx1∧···∧dxd
=λud
2/integraldisplay
B2(x,t)dx1∧···∧dxd
=λud
22πd/2
Γ(d
2+ 1)td
=λud
2tdCd
Therefore, for B2(x,t)∈Uu,
P(B2(x,t)) =Vol(B2(x,t))
V=λud
2tdCd
V.
In our setting,
P(B2(x,t))≥u⇔t≥1√
λ/parenleftBiguV
Cd/parenrightBig1
d,
and therefore,
GP,u=1√
λ/parenleftBiguV
Cd/parenrightBig1
d.
2Published in Transactions on Machine Learning Research (12/2024)
Therefore, the DTM is
δ2
P,m(x) =1
m/integraldisplaym
0/parenleftBig1√
λ/parenleftBiguV
Cd/parenrightBig1
d/parenrightBig2
du
=1
m1
λ/parenleftBigV
Cd/parenrightBig2
d/integraldisplaym
0u2
ddu
=d
d+ 21
m1
λ/parenleftBigV
Cd/parenrightBig2
dm1+2
d
=d
d+ 21
λ/parenleftBigV
Cd/parenrightBig2
dm2
d.
Therefore,
1√λu=δP,m(x)/radicalbigg
d+ 2
d/parenleftBigCd
Vm/parenrightBig1
d=δP,m(x)
Cd,m,V.
S1.3 Proof of Theorem 2
Proof. In this proof, we make use of the limiting distribution theorem regarding the empirical DTM in
Chazal et al. (2017). First, we restate the theorem.
Theorem 4 (Theorem 5 in Chazal et al. (2017)) .LetPbe some distribution in Rd. For some ﬁxed x,
assume that Fxis diﬀerentiable at F−1
x(m), form∈(0,1), with positive derivative F/prime
x(F−1
x(m)).Deﬁne
ˆδPn,m(x) =/summationtextK
k=1/bardblx−X(k)(x)/bardbl2/KforK=⌈∗⌉mn.Then we have
√n(ˆδ2
Pn,m(x)−δP,m(x))d−→N(0,σ2
x),
whereσ2
x=1
m2/integraltextF−1
x(m)
0/integraltextF−1
x(m)
0[Fx(s∧t)−Fx(s)Fx(t)]ds dt.
Now, assume A1 and A2 regarding a point x∈M. From A2, we have
dM(x,y)≡/bardblx−y/bardbl2Cd,m,V
δP,m(x).
/vextendsingle/vextendsingle/vextendsingle1−d2
M(x,y)
C2
d,m,Vˆd2x(x,y)/vextendsingle/vextendsingle/vextendsingle≥/epsilon1
⇔/vextendsingle/vextendsingle/vextendsingle1−ˆσ2
x
δ2
P,m(x)/vextendsingle/vextendsingle/vextendsingle≥/epsilon1
⇔/vextendsingle/vextendsingle/vextendsingleδP,m(x)2−ˆσ2
x/vextendsingle/vextendsingle/vextendsingle≥/epsilon1δ2
P,m(x)
Here,/epsilon1x:=/epsilon1δP,m(x)2is a ﬁxed nonnegative number. Therefore,
P/parenleftBig
∃y∈Ux,s.t./vextendsingle/vextendsingle/vextendsingle1−1
C2
d,m,Vd2
M(x,y)
ˆd2x(x,y)/vextendsingle/vextendsingle/vextendsingle≥/epsilon1/parenrightBig
=P/parenleftBig/vextendsingle/vextendsingle/vextendsingleδ2
P,m(x)−ˆσ2
x/vextendsingle/vextendsingle/vextendsingle≥/epsilon1x/parenrightBig
By Theorem 4, when Fxis diﬀerentiable at F−1
x(m)form∈(0,1), with positive derivative F/prime
x(F−1
x(m)),
whereFx(t) =P/parenleftBig/vextenddouble/vextenddouble/vextenddoubleX−x/vextenddouble/vextenddouble/vextenddouble2
≤t/parenrightBig
,
√n(δ2
P,m(x)−ˆσ2
x)d−→N(0,η2
x),
3Published in Transactions on Machine Learning Research (12/2024)
for some ﬁxed η2
x. We ﬁrst check if the condition satisﬁes our setting. Note that by the deﬁnition of GP,m(x),
we know that P(B2(x,G P,m(x)))≥m, i.e.,Fx(GP,m(x))≥m. SinceFx(t)is a non-decreasing function of t,
let us consider t≤GP,m(x).By the proof of Theorem 1, we know that
Fx(t) =P/parenleftBig/vextenddouble/vextenddouble/vextenddoubleX−x/vextenddouble/vextenddouble/vextenddouble2
≤t/parenrightBig
=P/parenleftBig
B2(x,√
t)/parenrightBig
=ctd/2,
for some constant c > 0.Therefore, Fx(t)is diﬀerentiable for t≤GP,m(x), and its derivative is always
positive as long as t >0. Note that F−1
x(m)>0whenm∈(0,1). Therefore, we can apply Theorem 4 in
our setting, so that δ2
P,m(x)−ˆσ2
x=op(1). Therefore,
P/parenleftBig/vextendsingle/vextendsingle/vextendsingleδ2
P,m(x)−ˆσ2
x/vextendsingle/vextendsingle/vextendsingle≥/epsilon1x/parenrightBig
→0.
S1.3.1 Proof of Proposition 2
Deﬁne forx,y∈Y,A(x,y)as a set of all sequences {pi,qi}n
i=1of elements of XA,that satisfy pi+1∼qifor
all1≤i≤n−1and[p1] =x,[qn] =y.
1. Since each dfis non-negative, by deﬁnition dYis non-negative.
2. For a sequence in A(x,y), its reverse exists and is also a sequence in A(y,x). Now, the symmetry of
dfmakes the path distance of any ordered sequence {pi,qi}n
i=1∈A(x,y)the same as that of the reversed
sequence{˜pi,˜qi}n
i=1∈A(y,x), wherepj= ˜qn+1−jandqj= ˜pn+1−j. In other words,
df(p1,q1) +···+df(pn,qn) =df(˜p1,˜q1) +···+df(˜pn,˜qn).
Therefore, we have dY(x,y) =dY(y,x)because
inf
A(x,y)df(p1,q1) +···+df(pn,qn) = inf
A(y,x)df(˜p1,˜q1) +···+df(˜pn,˜qn).
3. LetdY(x,y)<∞.Choose any w∈Y. If any of dY(x,w)ordY(w,y)is∞,then the inequality is
true. Now, consider both dY(x,w)anddY(w,y)to be ﬁnite. Then, there exist minimizing sequences
{ˆpi,ˆqi}n
i=1∈A(x,w)and{˜pi,˜qi}n
i=1∈A(w,y)suchthatdY(x,w) =df(ˆp1,ˆq1)+···df(ˆpn,ˆqn)anddY(w,y) =
df(˜p1,˜q1) +···df(˜pn,˜qn)with [ˆqn] = [˜p1] =w. Note that when we merge the two paths into one, we have
a path connecting xandy. Now, similarly to A(x,y), deﬁneA2(x,y)as a set of all sequences {pi,qi}2n
i=1of
elements of XA,that satisfy pi+1∼qifor all 1≤i≤n−1andp1∼x,q2n∼y.
dY(x,y) : = inf
A(x,y)/parenleftbig
df(p1,q1) +···+df(pn,qn)/parenrightbig
= inf
A2(x,y)/parenleftbig
df(p1,q1) +···+df(p2n,q2n)/parenrightbig
≤df(ˆp1,ˆq1) +···df(ˆpn,ˆqn) +df(˜p1,˜q1) +···+df(˜pn,˜qn)
=dY(x,w) +dY(w,y).
The second equality is because the shortest distance can always be achieved by a path of length nsince
card(X) =card(Y) =n.
S1.3.2 Proof of Theorem 3
We haveg1,...,gn, and each x∈Xis mapped to a distinct point gi(x)∈XA, i.e.,gi(x)/negationslash=gj(x)ifi/negationslash=j.
Therefore, for each p∈XA, we can uniquely identify the index iandx∈Xs.t.p=gi(x), i.e.,x=g−1
i(p).
Since the image of gidoes not overlap i.e., gi(X)∩gj(X) =∅fori/negationslash=j, we can deﬁne g−1, which is a uniﬁed
4Published in Transactions on Machine Learning Research (12/2024)
functionofall g−1
1,...,g−1
n, i.e., forp∈XA,g−1(p) :=g−1
i(p)forisuchthatp∈gi(X). Now, forp,q∈XA, we
havedf(p,q)<∞only wheng−1(p)∈Kg−1(q)org−1(q)∈Kg−1(p). By the assumption of Theorem 3, when
g−1(p)∈Kg−1(q), we always have g−1(q)∈Kg−1(p). This ensures that when dg−1(p)(g−1(p),g−1(q))<∞,
we havedg−1(q)(g−1(p),g−1(q))<∞. Therefore, in this case,
df(p,q) = max{dg−1(p)(g−1(p),g−1(q)),dg−1(q)(g−1(p),g−1(q))}
=ˆdloc(g−1(p),g−1(q)),
forˆdlocdeﬁned in equation 4. Thus, for x,y∈Y, we have
dY(x,y) : = inf
A(x,y)(df(p1,q1) +df(p2,q2) +···+df(pn,qn))
= inf
A(x,y)(dloc(g−1(p1),g−1(q1)) +···+dloc(g−1(pn),g−1(qn)))
= min
P(˜x,˜y)/parenleftbigˆdloc(˜x,u1) +···+ˆdloc(up,˜y)/parenrightbig
=ˆdglo(˜x,˜y)
with ˆdglo(x,y)deﬁned in equation 6, where ˜x=g−1(p1)∈Xand˜y=g−1(qn)∈X. The second equality is
due tog−1(p) =g−1(q)forp∼q. Note that the x,y∈Yare identiﬁed by ˜x,˜y∈X, so with a slight abuse
of notation, we can say dY(x,y) =ˆdglo(x,y).
Now, we show that dYin this case is an extended-metric. Given Proposition 2, we only need to show that
dY(x,y) = 0iﬀx=y. Sincedasatisﬁesda(x,y) = 0iﬀx=y, we havedf(p,q) = 0iﬀp=q. Given this,
considerx,y∈Y. It is obvious that dY(x,y) = 0ifx=y; we can take piandqiso thatpi=qiand[pi] =
[qi] =xfori= 1,...,n, whichgives 0≤dY(x,y)≤df(p1,q1)+df(p2,q2)+···+df(pn,qn) = 0.IfdY(x,y) = 0,
there must exist a sequence {pi,qi}n
i=1∈A(x,y)such thatdf(p1,q1)+df(p2,q2)+···+df(pn,qn) = 0. Since
ifpi/negationslash=qi,thendf(pi,qi)>0,we havepi=qifori= 1,...,n. Sincepi+1∼qiby the deﬁnition of A(x,y)
and since [qi] = [pi], we conclude that x= [p1] =···= [pn] =y.
S2 Other Related Work
In this section, we discuss additional related work.
PaCMAP (Wang et al., 2021) propose PaCMAP to preserve both the local and global structure. The loss
of PaCMAP is derived from an observational study of state-of-the-art dimension reduction methods such
as t-SNE (Van der Maaten & Hinton, 2008) and UMAP (McInnes et al., 2018). The global preservation
of PaCMAP is based on initialization through PCA and some optimization scheduling techniques. Our
development of global distances may be adopted by PacMAP instead of its current use of Euclidean distance.
PHATE (Moon et al., 2017) propose PHATE to preserve a diﬀusion-based information distance, where
the long-term transition probability catches the global information. Intuitively, PHATE generates a random
walk on the data points with the transition probability matrix PwhereP[i|j]∝eγ/bardblxi−xj/bardblfor some global
rescalerγ.. Aftertwalks, a large transition probability pt[i|j]wherePT=/producttextPwill imply that iandj
are relatively closer than those pairs of low transition probability. The diﬀusion-based design of PHATE
is especially eﬀective to visualize biological data, which has some development according to time, such as
single-cell RNA sequencing of human embryonic stem cells.
Distance to Measure. Distance to measure (DTM) was ﬁrst introduced by Chazal et al. (2011) and its
limiting distributions are investigated in (Chazal et al., 2017). These works are in the context of topological
dataanalysis, whereDTMisarobustalternativeof distance to support (DTS).DTSisadistancefromapoint
xto the support of the data distribution, not a distance between two points. DTS is used to reconstruct the
data manifold, for example by r-oﬀset (a union of radius r-balls centered at the data points), or together with
persistent topology (increasing r) to infer topological features such as Betti numbers. DTM is introduced
as an alternative to DTS because the empirical estimator of DTS depends on only one observation in the
dataset, which might be sensitive to outliers. These works do not share the same context with our work
5Published in Transactions on Machine Learning Research (12/2024)
because we are not aiming to infer topological features or recover topology. However, we found that there
is a mathematical connection between our local distance rescale and DTM. By connecting the two, we can
easily obtain the consistency of our geodesic distance estimators by their limiting distribution theorem.
Topological Autoencoder Moor et al. (2020) propose Topological Autoencoder as a new way of reg-
ularizing the latent representations of autoencoders by forcing that the topological features (persistence
homology) of the input data be preserved in the latent space, adding the interpretability of autoencoders in
terms of meaningful latent representation. The information of persistence homology is captured by calcu-
lating distances between persistence pairings, so that such distances of the input space and latent space are
regularized to be similar. Besides the motivation of autoencoder regularization, Moor et al. (2020) show com-
petingtwo-dimensionallatentspacevisualizationsagainstexistingmanifoldlearning(visualization)methods.
Therefore, we compare our iGLoMAP with TopoAE.
Deep isometric manifold learning Pai et al. (2019) proposes Deep isometric manifold learning (DI-
MAL), which is an extension of Isomap to exploit deep neural networks to learn a dimensional reduction
mapper. DIMAL shares the same spirit of MDS as Isomap does in the sense that it seeks to minimize the
quadraticerrorbetweenthegeodesicdistanceintheinputspaceandthe L2distanceonthelowerdimensional
representation. DIMAL uses a network design called Siamese networks (or a twin network). As the geodesic
distance to be preserved in DIMAL is the same as that in Isomap, the key diﬀerence between GLoMAP and
DIMAL is the key diﬀerence between iGLoMAP and Isomap; the geodetic distance estimates and the loss
function (the quadratic loss and KL-divergence loss).
Parametric UMAP Sainburg et al. (2021) extend UMAP to a parametric version, which we call PUMAP,
using a deep neural network such as a autoencoder. PUMAP shares the same graphical representation of
a dataset with UMAP, but the optimization process is modiﬁed to exploit a mini-batch based stochastic
gradient algorithm for training a deep neural network. Like Sainburg et al. (2021), we also use a deep neural
network to learn a dimensional reduction function. The key diﬀerence between GLoMAP and UMAP is also
the key diﬀerence between iGLoMAP and PUMAP.
EVNet Zang et al. (2022) propose EVNet, a dimensionality reduction method that utilizes deep neural
networks, with the purpose of 1) preserving both global and local data information, and 2) enhancing
the interpretability of the dimensionality reduction results. EVNet maps the input data twice: ﬁrst, to a
latent space, and then from this space to a lower-dimensional (visualization) space. The distances in the
visualization space are regularized to be similar to those in the latent space. We believe that comparing our
method with EVNet would be interesting, pending the availability of its public implementation.
Predictive principal component analysis Isomura & Toyoizumi (2021) propose predictive principal
component analysis , which ﬁnds a lower-dimensional representation at time t+1as a function of the sequence
of dependent data points of time {1,...,t}.The lower dimensional representation then is used for predicting
the data point at time t+ 1. It is an interesting problem to condense the dependency structure among data
points as a time series into a single lower dimensional representation for the purpose of prediction. We think
extending our framework to handle this problem would be an interesting direction.
VAE-SNE Graving & Couzin (2020) propose VAE-SNE that incorporates t-SNE to regularize the latent
space of the variational autoencoder (VAE). VAE-SNE is similar to Moor et al. (2020) in that it seeks
to regularize the latent space of an autoencoder to have a meaningful and probably more interpretable
latent representation. The regularization term of VAE-SNE that is added to the (variant of) VAE loss is a
probabilistic expression of the t-SNE loss. Therefore, the design of VAE-SNE aims to learn an autoencoder
that has a latent representation similar to t-SNE. In our numerical study, we focus on comparing with t-SNE.
6Published in Transactions on Machine Learning Research (12/2024)
S3 The Datasets
We give details about the datasets used in our numerical study. The Python package of iGLoMAP includes
the data generating code for all simulated datasets.
Scurve We adopted the Scurve dataset from the Python package scikit-learn (Pedregosa et al., 2011). Let
tbe a vector of uniform samples from (−3π
2,3π
2).Also, let ube uniform samples from (0,2)of the same
size. We can think that the original data in the lower dimensional space (2-dimensional) is a matrix [t,y].
Now we transform this data matrix into higher dimension (3-dimensional). The ﬁrst dimension xis deﬁned
byx= sin t,where the sin is an element-wise sin operator. The second dimension is simply y=u. The
third dimension is deﬁned by a vector z= sin( t)∗(cos(t)−1).The resulting 3-dimensional data is a matrix
[x,y,z].We usen= 6000.
Severed Sphere The Severed Sphere dataset is also obtained by the Python package scikit-learn (Pe-
dregosa et al., 2011). Let ube a vector of uniform samples from (−0.55,2π−0.55).Also, let tbe a
vector of uniform samples from (0,2π).The 2-dimensional data matrix [t,y]is transformed into three
dimension ﬁrst by transform, and then by point selection. First, the transformation is [x,y,z] =
[sin(t) cos(u),sin(t) sin(u),cos(u)].Now only the rows of the data matrix such thatπ
8< t <7π
8are se-
lected fort∈t. We usen= 6000.
EggsThe Eggs data is made up of 5982data points in the three-dimensional space. It has 12 half-spheres
(empty inside) and a rectangle with 12 holes. When we locate the 12 half-spheres on the holes on the
rectangle, we obtain the shape of a two-dimensional surface, where the half spheres and the boundaries
of the wholes are seamlessly connected. On the rectangular surface, the points on the ﬂat area are little
sparser than in the curved area; we uniformly distribute 2130points on the rectangle [−16,16]×[−4,4]and
remove the points on the 12 circles with radius 1, centered at [(−13,−2),(−13,2),(−8,−2),(−3,2),(2,+2),
(7,−2),(12,2),(−8,+2),(−3,−2),(2,−2),(7,+2),(12,−2)].Depending on the random uniform generation,
the amount of points removed can vary. Therefore, in our setting, we use a ﬁxed random generating seed
number so that the training set is always the same, and always the number of remaining points is 1266.
Each half-sphere of radius 1 has 348 uniformly distributed points. Therefore, the dataset in total has
12×348 + 1266 = 5982 .
Hierarchy The hierarchy dataset is developed by Wang et al. (2021). The ﬁve macro centers are generated
fromN(0,1002I50)whereI50is a 50x50 identity matrix. We call each macro center as Mifori= 1,...,5.
Now, for each macro center, 5 meso centers are generated from N(Mi,1000I50).We call the meso centers as
Mijwherej= 1,..,5. Now, for each meso center, 5 micro centers are generated from N(Mij,100I50).We
call the micro centers Mijk,wherek= 1,...,5.Now for each micro center, a local cluster is generated from
N(Mijk,10I5).In our experiment, each local cluster consists of 48 points so that in total n= 6000.
Spheres The spheres dataset is designed by Moor et al. (2020). It has one large outer sphere on which
half of the dataset is uniformly distributed and ten smaller inner spheres on which the remaining points are
uniformly distributed. Let the center an inner sphere be denoted by ui, wherei= 1,...,10. Then, uiis
generated from N(0,0.5I101). Now, 5% of the dataset is uniformly distributed on the sphere of radius one
centered at ui. The large outer sphere is centered at the origin and has a radius of 25. The 50% of the data
set is uniformly distributed on this outer sphere. In our case, we use n= 10000.
7Published in Transactions on Machine Learning Research (12/2024)
Figure S10: GLoMAP withoutτtempering on the hierarchical dataset. A larger ﬁxedτﬁnds more global
shapes, and smaller ﬁxedτ’s ﬁnd more local shapes while missing global clusters.
S4 Learning Conﬁgurations
Learning Conﬁguration for Example 1. We apply UMAP and GLoMAP, where for both methods, the
number of neighbors is set as K= 15(default) for the spheres, and as K= 250for the hierarchical dataset.
The latter is large to make the graph connected at least within each of the macro-level clusters. We ﬁx all
λeto 1 (default), while τis scheduled to decrease from 1 to 0.1 (default).
Learning Conﬁguration for Section 5. Typically, we set λeat 1 (default) to showcase performance
without tuning. However, for MNIST, we opt for tighter clustering by setting λeat 0.1. In the case of
iGLoMAP applied to the S-curve, the shape initially remains linear, then expands suddenly at the end.
To address this, we adjusted the starting τto be four times smaller as outlined in Remark 1. For similar
reasons,wemadeadjustmentsontheEggsandSevereddatasets,allowingshapestoformearlierandproviding
suﬃcient time for detail development. In the MNIST dataset, where both GLoMAP and iGLoMAP tend
to linger in the noise, we reduced the starting τby a quarter. Apart from these speciﬁc cases, we maintain
the default τschedule (from 1 to 0.1). All other learning hyperparameters are set to their defaults in the
iGLoMAP package (learning rate decay = 0.98, Adam’s initial learning rate = 0.01, initial particle learning
rate = 1, number of neighbors K=15, and mini-batch size=100). GLoMAP was optimized for 300 epochs
(500 epochs for MNIST), and iGLoMAP was trained for 150 epochs.
S5 Additional Discussions
S5.1 The eﬀect of tempering
In Example 1, it seems tempering is vital to discover the hierarchical clustering structure in the data. Figure
S10 (a) depicts the eﬀect of decreasing temperatures on µτ(d) = exp{−d/τ}. For a given d, a decreasing τ
reducesµτ(d), requiring the corresponding pair on the visualization space to be more distanced. This makes
the local clusters stay close at the early stage while forming the global shape, and separates them at the late
stages, forming the detailed local structures. The same progression from global shape to detailed localization
does not happen without the tempering (when τis kept as a constant) as Figure S10 (b-d). A larger ﬁxed
τ(τ= 1) ﬁnds global shapes, and smaller ﬁxed τ’s ﬁnd more local shapes while missing the global clusters.
S5.2 The eﬀect of the aesthetics parameter λe
For Example 1, to see the eﬀect of the aesthetics parameter λein controlling the attractive and repulsive
forces, we vary λefor a wide range in Figure S11 and S12. We observe that similar development patterns
occur across a wide range of λevalues. However, a smaller λemore strongly condenses data points within the
cluster, and vice versa. For an extremely large λe, the cluster shape may not appear, suggesting a possibility
that optimization did not occur properly.
8Published in Transactions on Machine Learning Research (12/2024)
Figure S11: The sensitivity of λeof the transductive GLoMAP on the hierarchical dataset (Example 1). A
largerλetends to make a local cluster more spread. However, the general tendency of the development from
global structures to local structures is similar across a wide range of λevalues. The τscale was reduced
from 1 to 0.1
9Published in Transactions on Machine Learning Research (12/2024)
Figure S12: The sensitivity of λeof the transductive GLoMAP on the spheres dataset (Example 1). A larger
λetend to make a local cluster more spread. However, the general tendency of the development from global
to local structures is similar across a wide range of λevalues. The τscale was reduced from 1 to 0.1.
10Published in Transactions on Machine Learning Research (12/2024)
Figure S13: GLoMAP faster variants results (Section S5.3). Top: the exact loss, Middle: simpliﬁed Dglois
used ( ˜K-nearest neighbor distance matrix of Dglois used instead of Dglo), Bottom: simpliﬁed Dglois used
and(1−µij)is approximated by 1. Number of data points: Spheres 10,000, S-curve 8,000, Eggs 6,000,
Severed 4,000. We set ˜K= 15m, wherem=⌊n/100⌋.
S5.3 Computational cost reduction
While we have reduced the computational cost for local distance estimation, managing the global distances,
conversely, increases the cost. Besides the shortest path search, the neighbor sampling scheme (line 9
in Algorithm 2) incurs costs comparable to the normalization of t-SNE; The sampling involves summing
membership scores over ndistances, given that all data points are connected. The cost of the sampling
scheme can be reduced by instead considering the ˜K-nearest neighbor distance matrix of Dglo, for example,
with ˜K= 20K. To further reduce the computational cost, we could also ignore the diﬀerence between
(1−µij)values, but consider them all as 1. This saves time in retrieving the µijvalues from the n×n
(sparse) matrix. Since (µi·)is only ann×1vector, it does not take much time to look up. In such a case, the
loss function is ˆL=−/summationtext
i∈Sµi·logqiji−λe/summationtext
i∈S/summationtext
j∈Slog (1−qij).For examples of these approximations’
results, see Figure S13 for the visualization and Figure S14 for the computational time comparison. These
ﬁgures demonstrate a high similarity to the results obtained from the exact loss computation, and the
reduction in computational cost becomes more evident for larger datasets.
S6 Additional Experiments
In this section, we discuss the performance of our methods on an interesting dataset, called Fishbowl. In
addition, we also empirically investigate the sensitivity of iGLoMAP against the capacity choice of deep
neural networks for the mapper.
Fishbowl Data. It is an interesting problem how a dimensional reduction method performs on a dataset,
which is not homeomorphic to R2,for example, a sphere. In such a case, there is no way to map onto
the two dimensional space while preserving all the pairwise distances in the data. Now, we slightly change
the problem. We consider that we are given data on a half-sphere. This is the famous crowding problem,
and in this case also, there is no way to map onto the two dimensional space while preserving all the
pairwise distances. Intuitively, we may want to smash (or, stretch) the half sphere to make it ﬂat on the
11Published in Transactions on Machine Learning Research (12/2024)
Figure S14: Run time comparison of GLoMAP and faster variants in Section S5.3. Full: the exact loss,
Partial: ˜K-nearest neighbor distance matrix of Dglois used instead of Dglo, Partial and mu approx: ˜K-
nearest neighbor distance matrix and (1−µij)is approximated by 1. The eﬀect of using ˜Kbecomes more
evident for larger datasets.
Figure S15: The ﬁshbowl dataset (n=6000).
two dimensional space. Now, we turn our attention to a dataset on something between a sphere and a
half-sphere, called a ﬁshbowl (Silva & Tenenbaum, 2002), which has more than half of the sphere preserved.
We generate a uniformly distributed dataset on a ﬁshbowl Fγ={(x,y,z )∈R3|x2+y2+z2= 1,z≤γ}
forγ∈{0.5,0.6,0.7,0.8,0.9,0.95,0.975,1}.The data is illustrated in Figure S15. When γ= 1, the data are
equivalenttoauniformsampleonasphere. Althoughthereisnoconsensusonwhattypeofvisualizationmust
be ideal, in this case the exact pairwise distance is impossible. All learning parameters and hyperparameters
are set to their default values, except for the initial τvalue for iGLoMAP, which was reduced to 0.5. This
adjustment was necessary because, under the default value, iGLoMAP remains in a linear (non-meaningful)
state until very late (see Remark 1). The results of GLoMAP and iGLoMAP are shown in Figure S25. For
GLoMAP, we see some recovery of the original disk until γ≤0.95. Then, when γ≥0.975(when the ﬁshbowl
is closer to a sphere), GLoMAP simply shows a side image. For iGLoMAP, the ﬂat disc shape is made until
γ= 0.975. The result on a sphere is in Figure S25 (e) for GLoMAP and Figure S25 (j) for iGLoMAP.
Sensitivity analysis on the deep neural network capacity In order to see the impact of network
size, we use the same fully connected deep neural network (four layers) with varying network width. The
network width (which controls the network size) varies in {2q|q= 2,4,6,8,10,11}, of which the range is
intentionally chosen to be extremely wide. We enumerate the corresponding capacities as 0 to 5. Width 4 is
impractically small; When networks are this small, the network may not be expressive. Width 2048 is the
extreme opposite. Note that width 4 has 66 parameters to learn in the network, width 16 has 646, width 64
12Published in Transactions on Machine Learning Research (12/2024)
Figure S16: The result of GLoMAP on the ﬁshbowl dataset. The number on each panel indicates the height
of ﬁsh bowl (the threshold on the z-axis).
has 8,706, width 256 has 133,122, width 1024 has 2,105,346, and width 2048 has 8,404,994. We recall that
in Section 5, we used width 64 for all simulation datasets. We set the hyperparameters of iGLoMAP same
as in Section 5.2, except halving the starting τto 0.5 for Spheres since for smaller networks, the expansion
of visualization is observed too late.
The visualizations of the results are in Figure S17. We see that increasing the network size improves the
visualizationqualityfromwidth4towidth64. Ontheotherhand, amonglargernetworks, weseeverysimilar
results, concluding that for a good range of network widths the visualizations are fairly similar. Therefore,
we see that for a wide range of network width (of the fully connected deep neural network that we use here),
the performance of iGLoMAP is stable. Those visual inspections are supported by the numerical measures
shown in Figures S18 and S19, which generally show slightly better performance of a larger network. This
result implies that we should avoid using too small networks, but much less caution is needed about using
too large networks.
13Published in Transactions on Machine Learning Research (12/2024)
Figure S17: Sensitivity analysis on the deep neural network capacity (n=6000). The same fully connected
deep neural network (four layers) is used with varying layer width. The range of width is intentionally
extreme (from 4 to 2048).
Figure S18: Sensitivity analysis on the deep neural network capacity. x-axis: the number of neighbors K.
y-axis: the KNN measure as analogous to Figure 7. The numbers in the legend are the layer width of the
corresponding deep neural network. From Spheres, we see that a larger capacity is generally better.
14Published in Transactions on Machine Learning Research (12/2024)
Figure S19: Sensitivity analysis on the deep neural network capacity. The KL divergence (ﬁrst row) and
correlation (second row) deﬁned in Section 5.1.1. The numbers in the legend are the layer width of the
corresponding deep neural network. From S-curve and Severed Sphere, it looks a larger capacity is generally
better, while the performance measures show similar patterns.
Figure S20: Comparison of computational time. There is room for improvement in the GLoMAP implemen-
tation, as it currently relies solely on Python without optimization through languages/interfaces like Numba,
Cython, or C++. Additionally, the neighborhood search algorithm is a basic version rather than a faster
approximation.
15Published in Transactions on Machine Learning Research (12/2024)
S7 Additional Measures
In Van Assel et al. (2024), the Silhouette coeﬃcient (Rousseeuw, 1987) and the trustworthiness score (Venna
& Kaski, 2001) are used to assess the quality of dimensional reduction. The Silhouette coeﬃcient measures
both the cohesion within clusters and separation between clusters, deﬁned as b−a/max(a,b), whereais the
mean intra-cluster distance, and bis the mean nearest-cluster distance. The trustworthiness score quantiﬁes
how well the K-nearest neighbors in the low-dimensional representation aligns with the rank of among the
Euclidean distances in the input space. Both scores are computed using the Scikit-learn Python package
Buitinck et al. (2013).
The full results are shown in Figure S27 and S26, while Table S1 shows the case when n= 6000withK= 5
for trustworthiness measure. In Figure S27, For the Hierarchical dataset, when Kis small, Isomap performs
poorly under the Trustworthiness measure, while for larger Kvalues, UMAP exhibits lower performance.
GLoMAP demonstrates competitive performance overall. This consistent competence is also shown in the
Silhouette coeﬃcient as in Figure S26. For the Spheres dataset, however, GLoMAP performs the worst on
this Trustworthiness measure and improves on the Silhouette coeﬃcient. The corresponding visualization is
presented in Figure S21, which we believe remains acceptable or even desirable.
Figure S21: Comparison of visualizations for increasing data sizes on the Spheres dataset. The corresponding
Silhouette coeﬃcients and trustworthiness scores are presented in Figure S27 and Table S1.
16Published in Transactions on Machine Learning Research (12/2024)
Figure S22: Comparison of visualizations for increasing data sizes on the Scurve dataset. The corresponding
Silhouette coeﬃcients and trustworthiness scores are presented in Figure S27 and Table S1.
Figure S23: Comparison of visualizations for increasing data sizes on the Severed dataset. The corresponding
Silhouette coeﬃcients and trustworthiness scores are presented in Figure S27 and Table S1.
17Published in Transactions on Machine Learning Research (12/2024)
Figure S24: Comparison of visualizations for increasing data sizes on the Hierarchical dataset. The
corresponding Silhouette coeﬃcients and trustworthiness scores are presented in Figure S27 and Table
S1. For GLoMAP, Isomap, UMAP, to make the graph connected among higher-level clusters, we use
n_neighbors=250.
Trustworthiness Silhouette
Method GLoMAP Isomap UMAP t-SNE GLoMAP Isomap UMAP t-SNE
Eggs 0.998 0.999 1.000 1.000 . . . .
Hierarchical (Macro) 0.997 0.989 0.997 0.999 0.413 0.424 -0.059 0.226
Hierarchical (Meso) 0.997 0.989 0.997 0.999 0.741 0.614 -0.198 0.059
Hierarchical (Micro) 0.997 0.989 0.997 0.999 0.907 0.489 0.991 0.930
Scurve 0.998 1.000 1.000 1.000 . . . .
Severed 0.999 1.000 1.000 1.000 . . . .
Spheres 0.615 0.633 0.660 0.681 0.016 -0.028 -0.022 0.002
Table S1: The trustworthiness score and Silhouette coeﬃcient for various datasets with n= 6000. The
full results are in Figure S27. The Silhouette coeﬃcient is computed only for the Hierarchical and Spheres
datasets, as it requires the class information.
S8 Additional Visualizations
18Published in Transactions on Machine Learning Research (12/2024)
Figure S25: Comparison of visualizations for increasing data sizes on the Eggs dataset. The corresponding
Silhouette coeﬃcients and trustworthiness scores are presented in Figure S27 and Table S1.
FigureS26: TheSilhouettecoeﬃcient(Rousseeuw,1987;Buitincketal.,2013)areevaluatedforanincreasing
number of samples. For the Hierarchical dataset, GLoMAP shows a consistently competitive result for all
levels of clusters.
19Published in Transactions on Machine Learning Research (12/2024)
S9 Comparison with C-Isomap
To see the usefulness of our distance in equation 6, we plug-in our global distance into the Isomap framework,
i.e., apply an MDS onto our global distance matrix. As a baseline, we choose C-Isomap (Silva & Tenenbaum,
2002) among many Isomap variants to improve the local distance because C-Isomap is a scalable option as
it also provides a closed-form local distance estimator. C-Isomap applies shortest path search over the local
distances and then Multidimensional Scaling (MDS). Since MDS does not allow any pair having as the
distance, we imputed 1.5 times of the maximum of pairwise distances for such pairs. As shown in Figure
S38, for C-Isomap, when K is large then we see separation between the purple and the other classes. When
K is small, the smaller classes are doted, and each class may compose multiple clusters in visualization
depending on the size of . On the other hand, when C-Isomap is using our distance, we can see separation.
Our GLoMAP corresponds to Algorithm 1 + Algorithm 2. Our choice of designing the loss similar to that
of UMAP (as in Algorithm 2) was also signiﬁcant, and novel if we see it from the perspective of Isomap.
When comparing to Figure 1, we can see that using Algorithm 2 instead of MDS brings a better clustering
eﬀect for those ten inner clusters.
20Published in Transactions on Machine Learning Research (12/2024)
Figure S27: The trustworthiness scores (Venna & Kaski, 2001; Buitinck et al., 2013) are evaluated for an
increasing number of samples. This score depends on a hyperparameter, K, which measures the correspon-
dence ofK-nearest neighbors between the input and visualization spaces.
21Published in Transactions on Machine Learning Research (12/2024)
Figure S28: The eﬀect of equivalence relation as a coequalize. For simplicity and visual clarity, we illustrate
the coproduct between only XaandXa/prime, rather than all npseudo-metric spaces..
Figure S29: iGLoMAP without τtempering. Even when small τis used for the entire optimization, the
global information is still found.
22Published in Transactions on Machine Learning Research (12/2024)
Figure S30: The global preservation measures. Top: KLσ(smaller is better), Bottom: distance correlation
(larger is better)
Figure S31: iGLoMAP on the hierarchical dataset using the default K= 15neighbors, which is generally
too few for meso level cluster connectivity. However, the macro and meso clusters are still preserved from
the start. This suggests the deep neural network might inherently encode some location information. (a)
Therandominitialization of the deep neural network. It shows many levels of clusters. (b) At the epoch 5,
already all the levels of clusters are found.
23Published in Transactions on Machine Learning Research (12/2024)
Figure S32: Hyperparameter sensitivity of iGLoMAP. Initial τis 1 and sent to 0.1 for all cases (default).
Figure S33: The iGLoMAP generalizations for the S-curve, Severed Sphere, Eggs dataset. The generalization
on the test set is almost identical to the original DR on the training set.
24Published in Transactions on Machine Learning Research (12/2024)
Figure S34: Numerical comparison with other models that utilize deep neural network.
Figure S35: Numerical comparison with other models utilizing deep neural networks. For the hierarchical
dataset, macro and meso-level performance metrics are omitted, as iGLoMAP, PUMAP, and TopoAE all
achieve 100% performance.
25Published in Transactions on Machine Learning Research (12/2024)
Figure S36: (a) Parametric UMAP: The majority runs of parametric UMAP resulted such a shape, so we
cherry picked the best one for comparison as in Figure 8. (b-d) Depending on the run or choice of the
hyperparameters, GLoMAP and iGLoMAP may result in a twisted visualization. However, we did not
observe any case having two separated partial visualizations.
Figure S37: The iGLoMAP result on the Fashion MNIST dataset (Xiao et al., 2017). Left: The visualization
result on the training data (n=60000). Right: Generalization result on the test data (n=10000).
26Published in Transactions on Machine Learning Research (12/2024)
Figure S38: The eﬀect of applying our new global distance to C-Isomap instead of its original global distance
estimates, which corresponds to the distance computation by Algorithm 1 and applying MDS in the resulting
distance matrix. C-Isomap shows the separation of the purple class when K(n_neighbor) is set as large.
27