Published in Transactions on Machine Learning Research (02/2024)
Optimization with Access to Auxiliary Information
El Mahdi Chayti el-mahdi.chayti@epfl.ch
EPFL
Sai Praneeth Karimireddy sp.karimireddy@berkeley.edu
UC Berkeley
Reviewed on OpenReview: https: // openreview. net/ forum? id= kxYqgSkH8I
Abstract
We investigate the fundamental optimization question of minimizing a targetfunctionf(x),
whose gradients are expensive to compute or have limited availability, given access to some
auxiliary side function h(x)whose gradients are cheap or more available. This formulation
captures many settings of practical relevance, such as i) re-using batches in SGD, ii) transfer
learning, iii) federated learning, iv) training with compressed models/dropout, Et cetera. We
propose two generic new algorithms that apply in all these settings; we also prove that we
can benefit from this framework under the Hessian similarity assumption between the target
and side information. A benefit is obtained when this similarity measure is small; we also
show a potential benefit from stochasticity when the auxiliary noise is correlated with that
of the target function.
1 Introduction
Motivation. Stochastic optimization methods such as SGD (Robbins & Monro, 1951b) or Adam (Kingma
& Ba, 2014) are arguably at the core of the success of large-scale machine learning (LeCun et al., 2015;
Schmidhuber, 2015). This success has led to significant (perhaps even excessive) research efforts dedicated to
designing new variants of these methods (Schmidt et al., 2020). In all these methods, massive datasets are
collected centrally on a server, and immense parallel computational resources of a data center are leveraged
to perform training (Goyal et al., 2017; Brown et al., 2020). Meanwhile, modern machine learning is moving
away from this centralized training setup with new paradigms emerging, such as i) distributed/federated
learning, ii) semi-supervised learning, iii) personalized/multi-task learning, iv) model compression, Et cetera.
Relatively little attention has been devoted to these more practical settings from the optimization community.
In this work, we focus on extending the framework and tools of stochastic optimization to bear on these novel
problems.
At the heart of these newly emergent training paradigms lies the following fundamental optimization question:
We want to minimize a target loss function f(x), but computing its stochastic gradients is either very
expensive or unreliable due to limited data. However, we assume having access to some auxiliary loss function
h(x)whose stochastic gradient computation is relatively cheaper or more available. For example, in transfer
learning,f(x)would represent the downstream task we care about and for which we have very little data
available, whereas h(x)would be the pretraining task for which we have plenty of data (Yosinski et al., 2014).
Similarly, in semi-supervised learning, f(x)would represent the loss over our clean labeled data, whereas
h(x)represents the loss over unlabeled or noisily labeled data (Chapelle et al., 2009). Our challenge is the
following question:
How can we leverage an auxiliary h(x)to speed up the optimization of our target loss function f(x)?
Of course, if f(x)andh(x)are entirely unrelated, our task is impossible, and we cannot hope for any speedup
over simply running standard stochastic optimization methods (e.g., SGD) on f(x). Thus, the additional
1Published in Transactions on Machine Learning Research (02/2024)
question before us is to define and take advantage of useful similarity measures between f(x)andh(x). In
this work, we will mainly consider Hessian similarity (defined later in Assumption 3.3) and leave devising
more practical similarity measures as a future direction.
Contributions. The main results in this work are
•We formulate the following as stochastic optimization with auxiliary information: i)Re-using batches
in SGD, ii) Semi-supervised learning, iii) transfer learning, iv) Federated Learning, v) personalized
learning, and vi) training with sparse models.
•We show a useful and simple trick (Eq3) to construct biased gradients using gradients from an
auxiliary function.
•Based on the above trick, we design a biased gradient estimator of f(x), which reuses stochastic
gradients of f(x)and combines it with gradients of h(x).
•We then use this estimator to develop algorithms for minimizing smooth non-convex functions. Our
methods improve upon known optimal rates that don’t use any side information.
Relatedwork. Optimizingonefunction fwhileaccessinganotherfunction h(oritsgradients)isanimportant
idea that has only been considered in specific cases in machine learning and optimization communities. To
the best of our knowledge, this problem has never been regarded in all of its generality before this time.
For this reason, we are limited to only citing works that used this idea in particular cases. Lately, masked
training of neural networks was considered, for example, in (Alexandra et al., 2019; Amirkeivan et al.,
2021); this approach is a special case of our framework, where the auxiliary information is given by the
sub-network (or mask). In distributed optimization, (Shamir et al., 2013) define sub-problems based on
available local information; the main problem with this approach is that the defined sub-problems need to be
solved precisely in theory and to high precision in practice. In Federated Learning (Konecny et al., 2016;
McMahan et al., 2017a; Mohri et al., 2019), the local functions (constructed using local datasets) can be seen
as side information. Applying our framework recuperates an algorithm close to MiMe (Karimireddy et al.,
2020a). In personalization, Chayti et al. (2021) study the collaborative personalization problem where one
user optimizes its loss by using gradients from other available users (that are willing to collaborate); again,
these collaborators can be seen as side information, one drawback of the approach in (Chayti et al., 2021) is
that they need the same amount of work from the main function and the helpers, in our case we alleviate
this by using the helpers more.
There is also auxiliary learning (Baifeng et al.; Aviv et al.; Xingyu et al.) that is very similar to what we are
proposing in this work. Auxiliary learning also has the goal of learning one given task using helper tasks,
however, all these works come without any theoretical convergence guarantees, furthermore, our approach is
more general.
The proposed framework is general enough to include all the above problems and more. More importantly,
we don’t explicitly make assumptions on how the target function fis related to the auxiliary side information
h(potentially a set of functions) like in Distributed optimization or Federated learning where we assume fis
the average of the side-information h. Also, it is not needed to solve the local problems precisely as expected
by DANE (Shamir et al., 2013).
2 General Framework
Our main goal is to solve the following optimization problem:
min
x∈Rdf(x), (1)
and we suppose that we have access to an auxiliary function h(x)that is related to fin a sense that we
don’t specify at this level.
Specifically, we are interested in the stochastic optimization framework. We assume that the target function is
of the form f(x) :=Eξf/bracketleftbig
f(x;ξf)/bracketrightbig
overx∈Rdwhile the auxiliary function has the form h(x) :=Eξh/bracketleftbig
h(x;ξh)/bracketrightbig
defined over the same parameter space. We will refer to these two functions simply as fandhand stress
that they should not be confused with f(·;ζf)andh(·;ζh). It is evident that if both functions fandh
2Published in Transactions on Machine Learning Research (02/2024)
are unrelated, we can’t hope to benefit from the auxiliary information h. Hence, we need to assume some
similarity between fandh. In our case, we propose to use the hessian similarity (defined in assumption 3.3).
Many optimization algorithms can be framed as sequential schemes that, starting from an estimate xof the
minimizer, compute the next (hopefully better estimate) x+by solving a potentially simpler problem
x+∈argmin
z∈Rd{ˆf(z;x) +λR(z;x)}, (2)
where ˆf(·;x)is an approximation of faround the current state x,R(z;x)is a regularization function that
measures the quality of the approximation and λis a parameter that trades off the two terms.
For example, the gradient descent algorithm is obtained by choosing ˆf(z;x) =f(x) +∇f(x)⊤(z−x),
R(z;x) =1
2∥z−x∥2
2andλ=1
ηwhereηis the stepsize. Mirror descent uses the same approximation ˆfbut a
different regularizer R(z;x) =Dϕ(z;x)whereDϕis the Bregman divergence of a certain strongly-convex
functionϕ.
We take inspiration from this approach in this work. However, we would like to take advantage of the
existence of the auxiliary function h. We will mainly focus on first-order approximations of fthroughout this
work; we will also fix the regularization function R(z;x) =1
2∥z−x∥2
2, we note that our ideas can be easily
adapted to other choices of Rand more involved approximations of f.
For any function h, we can always write fas
f(z) :=h(z)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
cheap+f(z)−h(z)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
expensive,
we call the first term “cheap”, but this should not be understood strictly; it can also, for example, mean
more available.
A very straightforward approach is to use fwhenever it is available and use has its proxy whenever it is not;
we call this approach the naive approach . This approach is equivalent to simply ignoring (in other words,
using a zeroth-order approximation of) the “expensive” part.
A more involved strategy is to approximate the “expensive” part f(z)−h(z)as well but not as much as the
“cheap” part h(z). We can do this by approximating f(z)−h(z)around x( a global state, or a snapshot,
the idea is that it is the state of f) and approximating h(z)around y(a local state in the sense that it is
updated by h). Doing this, we get the following update rule:
y+∈argmin
z∈Rd{ˆf(z;y,x) +1
2η∥z−y∥2
2},
where ˆf(z;y,x) :=h(y) +f(x)−h(x) +∇h(y)⊤(x−y) + (∇h(y)−∇h(x) +∇f(x))⊤(z−x).
This is equivalent to
y+=y−η(∇h(y)−∇h(x) +∇f(x)) (3)
We will refer to (3)as a local step because it uses a new gradient of hto update the state. The idea is that
for each state xoff, we perform a number of local steps, then update the state xbased on the last “local”
steps.
Control variates and SVRG. (3)can be understood as a generalization of the control variate idea used in
SVRG (Johnson & Zhang, 2013). To optimize a function f(x) :=1
n/summationtextn
i=1fi(x), SVRG uses the modified
gradient gSVRG =∇fi(y)−∇fi(x) +∇f(x)where xis a snapshot that is updated less frequently and iis
sampled randomly so that this new gradient is still unbiased. The convergence of SVRG is guaranteed by
the fact that the “error” of this new gradient is E∥gSVRG−∇f(x)∥2
2=O(∥y−x∥2
2)so that if y−x→0,
then convergence is guaranteed without needing to take small step-sizes. The main idea of our work is to use
instead offianother function hthat is related to f; this means using a gradient g=∇h(y)−∇h(x) +∇f(x),
then if we can still guarantee that the error is O(∥y−x∥2
2)everything should still work fine.
3Published in Transactions on Machine Learning Research (02/2024)
Other Variance reduction techniques. given the form of (3)that is very similar to the SVRG gradient, it
is natural to ask what would happen when using a form similar to other variance reduction techniques such as
SARAH (Nguyen et al., 2017) (this will amount to choosing a gradient gt=∇h(yt)−∇h(yt−1) +gt−1with
g0=∇f(y0:=x)). Unfortunately, this choice leads to the same theoretical rate obtained by the SVRG-like
choice, the main reason being that on top of the biasedness of SARAH, the fact that his potentially different
fromf(even in average) introduces another biasedness, which limits the potential gain; moreover, it is not
evident how to treat the case where we only have access to stochastic gradients of f.
What if we can’t access the true gradient of f?Whenfis not a finite average, we can’t access
its true gradient; in this case, we propose replacing the “correction” ∇f(x)−∇h(x)by a quantity mf−h
that is a form of momentum (takes into account past observed gradients of f−h), the idea of using
momentum is used to stabilize the estimate of the quantity ∇f(x)−∇h(x)as momentum can be used
to reduce the variance. Specifically, we use g=∇h(y) +mf−h. We note that for this estimate we have
E∥g−∇f(y)∥2
2=O(∥y−x∥2
2+∥mf−h−∇f(x) +∇h(x)∥2
2)which means that gapproximates∇f(y)as
long as yis not far from xandmf−his a good estimate of the quantity ∇f(x)−∇h(x).
Local steps or a subproblem? we note that another possible approach is, instead of defining local steps
based onh, to define a subproblem that gives the next estimate of fdirectly by solving
x+∈argmin
y∈Rd{h(y) +m⊤
f−h(y−x) +1
2η∥y−x∥2
2}. (4)
Our results can be understood as approximating a solution to this sub-optimization problem (4).
Notation. For a given function J, we denote gJ(·,ξJ)an unbiased estimate of the gradient of Jwith
randomness ξJ.
General meta-algorithm. Based on the discussion above, we propose the (meta)-Algorithm 1: at the
beginning of each round t, we have an estimate xt−1of the minimizer. We sample a new ζf−h, and compute
gf−h(xt−1,ζf−h), a noisy unbiased estimate of the gradient of f−h; we then update mt
f−ha momentum
off−h. We transfer both xt−1andmt
f−hto the helper hwhich uses both to construct a set of biased
gradients dt
kof∇f(yt
k−1)that are updated by h. These biased gradients are then used to update the “local”
states yt
k, thenfupdates its own state xtbased on the last local states yt
0≤k≤K; throughout this work, we
simply take xt=yt
K.
Algorithm 1 stochastic optimization of fwith access to the auxiliary h
Require: x0,η,T,K
fort= 1toTdo
sample gf−h(xt−1,ξt
f−h)≈∇f(xt−1)−∇h(xt−1)
update mt
f−h≈∇f(xt−1)−∇h(xt−1)§ momentum
define yt
0=xt−1
fork= 1toKdo
sample gh(yt
k−1,ξt,k
h)≈∇h(yt
k−1)
use it and mtto form dt
k≈∇f(yt
k−1)
yt
k=yt
k−1−ηdt
k
end for
update xt
end for
For our purposes we can take dt
k=gh(yt
k−1,ξt,k
h)+mt
f−hwhich should be a good approximation of ∇f(yt
k−1)
andmt
f−his a momentum of f−h, we will consider two options: classical momentum orMVR(for
momentum based variance reduction (Cutkosky & Orabona, 2019)).
Decentralized auxiliary information. More generally, we can assume having access to Nauxiliary
functionshi(x) :=Eξhi/bracketleftbig
hi(x;ξhi)/bracketrightbig
. While we can treat this case by taking h= (1/N)/summationtextN
i=1hi, we propose a
more interesting solution that also works if the helpers hiare decentralized and cannot live in the same server.
4Published in Transactions on Machine Learning Research (02/2024)
In this case, we can sample a set Stof helpers, each hi∈Stwill do the updates exactly as in Algorithm1,
but this time xtwill be constructed using all {yt
i,K,i∈St}. In our case, we propose to use the average
xt= (1/S)/summationtext
i∈Styt
i,K.
3 Algorithms and Results
We discuss here some particular cases of Algorithm 1 based on choices of mt
f−handdt
k, which we kept a
little bit vague purposefully.
We will consider mainly two approaches. We call the first one the Naive approach and the second one we
refer to as Bias correction .
We remind the reader that we can take dt
k=gh(yt
k−1,ξt,k
h) +mt
f−h.
Naive approach. This approach is exactly as suggested by its name, naive; it simply ignores the part f−h
or, in other words, sets mt
f−h= 0. The main idea is to use gradients (or gradient estimates) of fwhenever
they are available and use gradients of hwhen gradients of fare not available. In our case, we alternate
between one step using a gradient of fand(K−1)-steps using gradients of hwithout any correction. We will
show that this approach suffers heavily from the bias between the gradients of fandh. It is worth noting
that in federated learning, this approach corresponds to Federated averaging (McMahan et al., 2017a).We
remind the reader that we can take dt
k=gh(yt
k−1,ξt,k
h) +mt
f−h.
Bias correction. In the absence of noise, this approach simply implements (3). Specifically, the inner loop
in Algorithm 1 does the following:
yt
k=yt
k−1−η(∇h(yt
k−1)−∇h(xt−1) +∇f(xt−1)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
:=dt
k).
In the noisy case (when we can only have access to noisy gradients of f), we approximate the above step in
the following way:
yt
k=yt
k−1−η(gh(yt
k−1,ξt
k−1) +mt
f−h/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
:=dt
k),
where mt
f−his a momentum of f−h. We consider two methods for defining this momentum:
mt
f−h= (1−a)mt−1
f−h+agf−h(xt−1,ξt−1
f−h) (AuxMOM ), (5)
mt
f−h= (1−a)mt−1
f−h+agf−h(xt−1,ξt−1
f−h)
+ (1−a)/parenleftbig
gf−h(xt−1,ξt−1
f−h)−gf−h(xt−2,ξt−1
f−h)/parenrightbig
(AuxMVR ). (6)
AuxMOM simply uses the classical momentum, whereas AuxMVR uses the momentum-based variance
reduction technique introduced in (Cutkosky & Orabona, 2019).
Assumptions. To analyze our algorithms, we will make the following assumptions on the target fand the
helperh.
Assumption 3.1. (Smoothness.) We assume that fhasL-Lipschitz gradients and satisfy
∥∇f(x)−∇f(y)∥2≤L∥x−y∥2.
Assumption 3.2. (Variance.) The stochastic gradients gf(x;ζf),gh(x;ζh)andgf−h(x;ζf−h)are unbiased,
and satisfy
EζJ∥gJ(x;ζJ)−∇J(x)∥2
2≤σ2
J,J∈{f,h,f−h}.
5Published in Transactions on Machine Learning Research (02/2024)
In Assumption 3.2, we assume that we directly have access to unbiased gradient estimates of f−h; this does
not restrict in any way our work since gf−ghis such an estimate; however, this last estimate has a variance
ofσ2
f−h=σ2
f+σ2
h, in general, it is possible to have a correlated estimate such that σ2
f−h<σ2
f+σ2
h. If the
batchesξfandξhare drawn such that gfandghare positively correlated, then it is even possible to have
σ2
f−h<σ2
f.
Assumption 3.3. Hessian similarity. Finally, we will assume that for some δ∈[0,2L]we have
∥∇2f(x)−∇2h(x)∥2≤δ.
Note that if h(·)is also smooth (satisfies Assumption 3.1), then we would have Hessian similarity with δ≤2L
since
∥∇2f(x)−∇2h(x)∥2≤∥∇2f(x)∥2+∥∇2h(x)∥2≤2L.
As sanity checks, h= 0corresponds to δ=L(this case should not lead to any benefit), and h=fgives
δ= 0; we will consider these two cases to verify our convergence rates.
Discussion of the assumptions. Assumptions 3.1 and 3.2 are very common assumptions in the optimization
literature. Under these two assumptions, it is well-known that SGD (Robbins & Monro, 1951a; Kiefer &
Wolfowitz, 1952) has an optimal convergence rate. Assumption 3.3 was used extensively in Federated Learning
(Karimireddy et al., 2020a;b) and was considered for personalization (Chayti et al., 2021).
Relaxing the Hessian similarity. Jingzhao et al. propose a generalized smoothness assumption where the
norm of the Hessian can grow with the norm of the gradient. We believe it possible to extend our theory to
accommodate such an assumption. In the same spirit, we can let ∥∇2f(x)−∇2h(x)∥2grow with∥∇f(x)∥2.
More important than this is finding similarity measures that apply to some of the potential applications we
cite in Section 5.
4 Results
We start by showing that the convergence rate of the naive approach is dominated by the gradient bias. We
then show the convergence rate of our momentum variant AuxMOM that will be compared to SGD/GD.
We will also state the convergence rate of our AuxMVR variant and compare it to MVR/GD.
Notation. We assume fis bounded from below and denote f⋆= inf
x∈Rdf(x).
Remark. We would like to note that while we state our results for the case of one helper h, they also apply
without needing any additional assumption when we have Ndecentralized helpers h1,...,hNfrom which we
sampleS= 1helper at random. In the case where S >1, we need an additional weak-convexity assumption
to deal with the averaging performed at the end of each step; this general case is treated in Appendix C.4.
4.1 Naive approach
In this section, we show the convergence results using the naive approach that uses a gradient of ffollowed
byK−1gradients of h. For the analysis of this case (and only of this case), we need to make an assumption
on the gradient bias between fandh.
Assumption 4.1. The gradient bias between fandhis(m,ζ2)−bounded:∀x∈Rd:∥∇f(x)−∇h(x)∥2
2≤
m∥∇f(x)∥2
2+ζ2.
Theorem 4.2. There exists fandhsatisfying assumptions 3.1,3.2,3.3,4.1 with δ= 0,σf=σh= 0
such that1
KT/summationtextT
t=1∥∇f(xt−1)∥2
2= Ω(ζ2).
Using the biased SGD analysis in (Ajalloeian & Stich, 2020), it is easy to prove an upper bound, but we
only need a lower bound for our purposes. Theorem 4.2 shows that this naive approach cannot guarantee
convergence to less than ζ2(up to some constant).
Note.Theorem 4.2 is very loose for Federated Averaging as, in this case, the function fis directly tied
to the helper entities (it is the average). Nevertheless, it’s worth noting that heterogeneity and client drift,
6Published in Transactions on Machine Learning Research (02/2024)
which are akin to gradient bias, are recognized as factors that can constrain the performance of FedAVG.
Consequently, there have been efforts to mitigate these issues through approaches such as those discussed in
(Karimireddy et al., 2020b;a).
In what follows, we will show that our two proposed algorithms solve this bias problem.
4.2 Momentum based approach
We consider the instance of Algorithm 1 with the momentum choice in (5). For clarity, a detailed Algorithm
can be found in the Appendix Algorithm 3.
Convergence rate. We prove the following theorem that gives the convergence rate of this algorithm in the
non-convex case.
Theorem 4.3. Under assumptions A3.1, 3.2,3.3. For m0such thatE0≤σ2
f−h/T,a=max(1
T,36δKη)
andη= min(1
L,1
192δK,/radicalig
F0
144LβK2Tσ2
f),we get the following :
1
KTT/summationdisplay
t=1K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(yt
k)∥2
2/bracketrightbig
≤O/parenleftig/radicaligg
LβF0σ2
f
T+(L+δK)F0
KT+σ2
f−h
T+σ2
h
K/parenrightig
.
WhereF0=f(x0)−f⋆,E0=E[∥m0−∇f(x0) +∇h(x0)∥2
2]andβ=O/parenleftbigδ
Lσ2
f−h
σ2
f+1
K(1 +δ
L)σ2
h
σ2
f/parenrightbig
.
Note.the condition E0≤σ2
f−h/Tcan be ensured by using a batch size Ttimes larger to estimate m0, this
will at most result in doubling the number of steps T. In practice, we did not need to ensure this condition.
We also note the term σ2
h/K, which corresponds to the error of solving the inner problem 4. Remarkably,
although we are using SGD steps of the helper h, we get a 1/Kerror instead of 1/√
K; this can be explained
by the fact that we initialize using the (approximate) solution of the last inner problem which accelerates
convergence; we can potentially get faster rates by using variance reduction methods on h.
We will compare this rate to that of SGD under the same amount of work asked from f:O/parenleftbigg/radicalig
LF0σ2
f
T+LF0
T/parenrightbigg
which corresponds to O/parenleftigLF0σ2
f
ε2+LF0
ε/parenrightig
stochastic gradient calls of fnecessary to get an ε-stationary point ˆx
in expectation i.e. a point ˆxsuch that E[∥∇f(ˆx)∥2
2]≤ε(the expectation is taken over the algorithm that
generated ˆx).
In comparison, based on Theorem 4.3, we can show the following corollary:
Corollary 4.4 (Iteration complexity of AuxMOM) .Letˆxbe chosen uniformly at random from the
iterates generated by AuxMOM. To guarantee E[∥∇f(ˆx)∥2
2]≤ε, AuxMOM needs at most
O/parenleftigδF0σ2
f−h
ε2+δF0
ε+σ2
f−h
ε+σ2
h
LF0+ 1σh̸=0LF0
ε/parenrightig
(stochastic) gradient calls of f.
In particular, we see that in the dominating order of 1/ε, when access to gradients of fis stochastic ( noisy
case), we replaced Lσ2
fbyδσ2
f−hwhich might be very small, either because δ≪Lorσ2
f−h≪σ2
f. In the
noiseless case (when we have full access to gradients of f), we replaced Lbyδ.
Of course, the gain that we obtain is not for free; it comes at the cost of using K=O/parenleftig
σ2
h
ε+ 1δ̸=0L
δ+ 1/parenrightig
inner steps of the helper h.
Sanity checks. Forh=f, we haveδ= 0, we get the iteration complexity O/parenleftig
σ2
ε+LF0
ε/parenrightig
which corresponds
to the rate of KT-steps of SGD. For h= 0, we haveδ=L; in this case, we don’t gain anything as should be
the case.
7Published in Transactions on Machine Learning Research (02/2024)
SVRG in the non-convex setting. In particular, because Theorem 4.3 also applies for the case where
we have multiple helpers, and we sample each time S= 1helpers, we get that SVRG converges in this case
asO/parenleftbig(L+δK)F0
KT/parenrightbig
, which matches the known SVRG rate (Reddi et al., 2016) (up to δbeing small). More
interestingly, we obtain the same convergence rate by using only one batch (no need to sample) if the batch
is representative enough of the data (i.e., satisfies our Hessian similarity Assumption 3.3).
Local steps help in federated learning. By employing the decentralized version of this theorem (as
detailed in Appendix C.4), we can ascertain that local steps (denoted as Kin our context) indeed provide a
beneficial contribution to Federated Learning. This finding aligns with the results presented in (Karimireddy
et al., 2020a).
4.3 MVR based approach
We consider now the instance of Algorithm 1, which uses the MVR momentum in in (6). The detailed
algorithm can be found in the Appendix Algorithm 5.
Stronger assumptions. For the analysis of this variant, we need a stronger similarity assumption
Assumption 4.5. Stronger Hessian similarity.
∃δ∈[0,L]∀ζf−h:gf−h(·,ζf−h)isδ-Lipschitz.
Assumption 4.5 is stronger than its counterpart Assumption 3.3 used for AuxMOM. It is reasonable to need
a stronger assumption since, already, when using the MVR momentum, a stronger smoothness assumption
has to hold.
Convergence of this variant. We prove the following theorem that gives the convergence rate of this
algorithm in the non-convex case.
Theorem 4.6. Under assumptions A3.1, 3.2,4.5. For m0such thatE0≤σ2
f−h/T, fora=
max(1
T,1156δ2K2η2)andη=min/parenleftig
1
L,1
192δK,1
K/parenleftig
F0
18432δ2Tσ2
f−h/parenrightig1/3
,/radicalig
F0
KT(L/2+8δK)/parenrightig
.This choice gives
us the rate :
1
KTT/summationdisplay
t=1K−1/summationdisplay
k=1E/bracketleftbig
∥∇f(yt
k)∥2
2/bracketrightbig
=O/parenleftig/parenleftbigδF0σf−h
T/parenrightbig2/3+/radicalbigg
(L+δ)F0σ2
h
KT+(L+δK)F0
KT+σ2
f−h
T+σ2
h
K/parenrightig
.
Baseline. Under the same assumptions and for the same amount of work, MVR or STORM (Cutkosky &
Orabona, 2019) has the rate: O/parenleftbigg/parenleftbigLF0σf
T/parenrightbig2/3+LF0
T/parenrightbigg
, this rate corresponds to needing at most O/parenleftbigg
LF0σf
ε3/2+
LF0
ε/parenrightbigg
(stochastic) gradient calls of fto reach a point ˆxsuchE[∥∇f(ˆx)∥2
2]≤εorε-stationary point. This
rate/iteration complexity is known to be optimal under the strong smoothness assumption: f(·,ξ)isL-smooth
for allξalmost surely.
Using Theorem 4.6, it is easy to show the following corollary:
Corollary 4.7 (Iteration complexity of AuxMVR) .Letˆxbe chosen uniformly at random from the
iterates generated by AuxMVR. To guarantee E[∥∇f(ˆx)∥2
2]≤ε, AuxMVR needs at most
O/parenleftigδF0σf−h
ε3/2+δF0
ε+σ2
f−h
ε+σ2
h
LF0+ 1σh̸=0LF0
ε/parenrightig
(stochastic) gradient calls of f.
The same conclusions as for AuxMOM are valid here. In the noisy case , we replaced Lσ2
fbyδσ2
f−hwhich
might be very small, either because δ≪Lorσ2
f−h≪σ2
f. In thenoiseless case (when we have full access
to gradients of f), we replaced Lbyδ.
Again, this potential gain is obtained at the cost of using K=O/parenleftig
σ2
h
ε+ 1δ̸=0L
δ+ 1/parenrightig
inner steps of the helper h.
8Published in Transactions on Machine Learning Research (02/2024)
5 Potential applications
The optimization with access to auxiliary information proposed is general enough that we can use it in many
applications where we either have access to auxiliary information explicitly, such as in auxiliary learning or
transfer learning, or implicitly, such as in semi-supervised learning. We present here a non-exhaustive list of
potential applications.
Reusing batches in SGD training and core-sets. In Machine Learning, the empirical risk minimization
consisting of minimizing a function of the form f(x) =1
N/summationtextN
i=1L(x,ξi)is ubiquitous. In many applications,
we want to summarize the data-set {ξi}N
i=1by a smaller potentially weighted subset CS={(ξij,wj)}M
j=1,
for positive weights (wj)M
j=1that add up to one, this is referred to as a core-set (Bachem et al., 2017). In
this case we can set h(x) =/summationtext
(w,ξ)∈CSwL(x,ξ). An even sampler problem is when we sample a batch
B⊂{ξi}N
i=1of sizeb≤N, one question we can ask is how can we reuse this same batch to optimize f? In
this case we set h(x) = (1/b)/summationtext
ξ∈BL(x,ξ). In the case where we have many batches {Bi}i∈I, we can set
hi(x) = (1/|Bi|)/summationtext
ξ∈BiL(x,ξ)for eachi∈Iand use our decentralized framework to sample each time a
helperh.
Note.In casehis obtained using a subset Bof the dataset defining f, there is a-priori a trade-off between
the similarity between fandhmeasured by the hessian similarity parameter δ(B)and the cheapness of the
gradients of h. A-priori, the bigger the size of Bis, the easier it is to obtain a small δ(B), but the more
expensive it is to compute the gradients of h.
Semi-supervised learning. In Semi-supervised Learning (Zhu, 2005), We have a small set of carefully
cleaned dataZdirectly related to our target task and a large set of unlabeled data ˜Z. Let us also assume
there is an auxiliary pre-training task defined over the source data, e.g., this can be the popular learning
with contrastive loss (Chen et al., 2020). In this setting, we have a set of transformations Twhich preserves
the semantics of the data, two unlabeled data samples ˜ζ1,˜ζ2∈˜Z, and a feature extractor ϕx(·) :˜Z→Rk
parameterized by x. Then, the contrastive loss is of the form: ˜ℓ(ϕx(˜ζ1), ϕx(T(˜ζ1)), ϕx(˜ζ2) )where the
loss tries to minimize the distance between the representations ϕx(˜ζ1)andϕx(T(˜ζ1)), while simultaneously
maximizing distance to ϕx(˜ζ2). Similarly, we also have a target loss ℓ:Z→R, which we care about; then,
we can define
f(x) =Eζ∈Z/bracketleftbig
ℓ(x;ζ)/bracketrightbig
and
h(x) =E˜ζ1,˜ζ2,T/bracketleftbig˜ℓ(ϕx(˜ζ1),ϕx(T(˜ζ1)),ϕx(˜ζ2))/bracketrightbig
.
Another way our framework can be used for semi-supervised learning is by endowing the unlabeled data with
synthetic labels (either random labels as we will see for logistic regression or via Pseudo-labeling (Lee, 2013))
and defining the helper has the loss over the unlabeled data with its assigned labels.
Transfer learning. For a survey, see (Zhuang et al., 2020). In this case, in addition to a cleaned data set Z
we have access to ˜Za pre-training source ˜Z. Given a fixed mask M(for masking deep layers in a neural
network) and a loss ˜ℓ, we set
f(x) =Eζ∈Z/bracketleftbig
ℓ(x;ζ)/bracketrightbig
andh(x) =E˜ζ∈˜Z/bracketleftbig˜ℓ(M⊙x,˜ζ)/bracketrightbig
.
Federated learning. Consider the problem of distributed/federated learning where data is decentralized
overNworkers (McMahan et al., 2017b). Let {F1(x),...,FN(x)}represent the Nloss functions defined over
their respective local datasets. In such settings, communication is much more expensive (say Mtimes more
expensive) than the minibatch gradient computation time (Karimireddy et al., 2018). In this case, we set
f(x) =1
NN/summationdisplay
i=1Fi(x)andhi(x) =Fi(x).
Thus, we want to minimize the target function f(x)defined over all the workers’ data but only have access
to the cheap loss functions defined over local data. The main goal, in this case, is to limit the number
of communications and use as many local steps as possible. Our proposed two variants are very close to
MiMeSGDm and MiMeMVR from (Karimireddy et al., 2020a).
Personalized learning. This problem is a combination of the federated learning and transfer learning
9Published in Transactions on Machine Learning Research (02/2024)
problems described above and is closely related to multi-task learning (Ruder, 2017). Here, there are N
workers, each with a task {F1(x),...,FN(x)}, and without loss of generality, we describe the problem from
the perspective of worker 1. In contrast to the delayed communication setting above, in this scenario, we only
care about the local lossF1(x), whereas all the other worker training losses {F2(x),...,FN(x)}constitute
auxiliary data:
f(x) =/bracketleftbig
F1(x) =Eζ1[F1(x;ζ1)]/bracketrightbig
and fori>2
hi(x) =/bracketleftbig
Fi(x) =Eζi[Fi(x;ζi)]/bracketrightbig
.
In this setting, our main concern is the limited amount of training data available on any particular worker—if
this was not an issue, we could have simply directly minimized the local loss F1(x).
Training with compressed models. Here, we want to train a large model parameterized by x∈Rd. To
decrease the cost (both time and memory) of computing the backprop, we instead mask (delete) a large part
of the parameters and perform backprop only on the remaining small subset of parameters (Sun et al., 2017;
Yu & Huang, 2019). Suppose that our loss function ℓis defined over sampled minibatches ξand parameters
x. Also, let us suppose we have access to some sparse/low-rank masks {1M1,..., 1Mk}from which we can
choose. Then, we can define the problem as
f(x) =Eξ/bracketleftbig
ℓ(x;ξ)/bracketrightbig
andh(x) =Eξ,M/bracketleftbig
ℓ(1M⊙x;ξ)/bracketrightbig
.
Thus, to compute a minibatch stochastic gradient of h(x)requires only storing and computing a significantly
smaller model 1M⊙xwhere most of the parameters are masked out. Let DM=diag(1M)be a diagonal
matrix with the same mask as 1M. The similarity condition then becomes
∥∇2f(x)−EM/bracketleftbig
DM∇2f(1M⊙x)DM/bracketrightbig
∥2≤δ.
The quantity above first computes the Hessian on the masked parameters 1M⊙xand then is averaged over
the various possible masks to compute EM/bracketleftbig
DM∇2f(1M⊙x)DM/bracketrightbig
. Thus, the parameter δhere measures the
decomposability of the full Hessian ∇2f(x)along the different masked components. Again, we do not need the
functionsfandhto be related to each other in any other way beyond this condition on the Hessians—they
may have completely different gradients and optimal parameters.
Does the Hessian similarity hold in these examples? In general, the answer is a no since already
smoothness does not necessarily hold. Also, this will depend on the models we have and should be treated on
a case-by-case basis. However, we can see from the experiment section that our algorithms perform well even
for deep learning models. This work is a first attempt to unify these frameworks; we do not pretend that
our answers or algorithms are the final attempt. We hope to spark further research in this direction, both
theoretically and practically.
Examples where it holds. There are two special and simple examples where this similarity assumption
holds. In both semi-supervised linear and logistic regressions, we note that the hessian does not depend
on the label distribution; for this reason, we can endow the unlabeled data with any label distribution and
construct the helper h(based on unlabeled data) with δ= 0(under the assumption that unlabeled data come
from the same distribution as that of labeled data).
6 Experiments
Baselines. We will consider fine-tuning and the naive approach as baselines. Fine-tuning is equivalent to
using the gradients of the helper all at the beginning and then only using the gradients of the main objective
f. We note that in our experiments, K= 1corresponds to SGD with momentum; this means we are also
comparing with SGDm.
6.1 Toy example
We consider a simple problem that consists in optimizing a function f(x) =1
2x2by enlisting the help of the
functionh(x) =1
2(1 +δ)(x−ζ/(1 +δ))2forx∈R.
10Published in Transactions on Machine Learning Research (02/2024)
0 10 20 30 40 50101
100101102103f(x)Naive
0 10 20 30 40 50
iterations1027
1022
1017
1012
107
102
AuxMom
0 10 20 30 40 501028
1023
1018
1013
108
103
102FT
zeta=1
zeta=10
zeta=100
Figure 1: Effect of the bias ζ(zeta in the figure) on the naive approach (Naive), AuxMOM and Fine Tuning (FT) for
K= 10,δ= 1andη=min(1/2,1/(δK)). We can see that the naive approach fails to converge for large bias values,
whereas AuxMOM converges all the time, no matter the value of the bias. Fine Tuning converges much slower for
small values of δ, but beats AuxMOM for δ= 10.
Effect of the bias ζ.Figure 1 shows that indeed our algorithm AuxMOM does correct for the bias. We
note that in this simple example, having a big value of ζmeans that the gradients of hpoint opposite to
those off, and hence, it’s better to not use them in a naive way. However, our approach can correct for this
automatically and hence does not suffer from increasing values of ζ. In real-world data, it is very difficult to
quantifyζ, thus why we can still benefit a little bit (in non-extreme cases) using the naive way.
0 10 20 30 40 50100101f(x)Naive
0 10 20 30 40 50
iterations1028
1023
1018
1013
108
103
AuxMom
0 10 20 30 40 501028
1023
1018
1013
108
103
102FT
delta=0.1
delta=1
delta=10
Figure 2: Effect of the similarity δ(delta in the figure) on both the naive approach (Naive), AuxMOM and Fine
Tuning for K= 10,ζ= 10andη= 0.5/(1 +δ)for Naive and AuxMOM and η= 0.5/(1 +δ)theneta= 0.5for Fine
Tuning. We can see that the naive approach fails to benefit from small values of δ; AuxMOM does not suffer from the
same problem, whereas Fine Tuning is slower than AuxMOM.
Effect of the similarity δ.Figure 2 shows how the three approaches compare when changing δ. We note
thatδ= 0.1corresponds to L/δ=K= 10the value used in our experiment; our theory predicts that for
such values of δ, the convergence is as if we were using only gradients of f.
6.2 Leveraging noisy or mislabeled data
We show here how our approach can be used to leverage data with questionable quality, like the case where
some of the inputs might be noisy or, in general, transformed in a way that does not preserve the labels. A
second example is when part of the data is either unlabeled or has wrong labels.
Rotated features. We consider a simple feed-forward neural network ( Linear (28∗28,512)→ReLU→
Linear (512,512)→ReLU→Linear (512,10)) to classify the MNIST dataset (LeCun & Cortes, 2010) , which
is the main task f. As a helper function h, we rotate MNIST images by a certain angle ∈{0,45,90,180};
the rotation plays the role of heterogeneity; it is worth noting that this is not simple data augmentation as
numbers “meanings” are not conserved under such a transformation. We used a 256 batch size for fand a 64
batch size for h; in our experiments changing the batch size of hto256or512led to similar results. We plot
the test accuracy that is obtained using both the naive approach and AuxMOM .
First of all, Figure 3 shows that we indeed benefit from using larger values of Kup to a certain level (this is
predicted by our theory), this suggests that we have somehow succeeded in making a new gradient of fout
of each gradient of hwe had.
Next, Figure 4 shows how much the rotation angle affects the performances of the naive approach and
AuxMOM. Astonishingly, AuxMOM seems to not suffer from increasing the value of the angle (which we
should increase the bias).
Figure 5 shows a comparison with the fine-tuning approach as well.
11Published in Transactions on Machine Learning Research (02/2024)
0 20 40
#iterations/500.20.40.60.81.0accuracy
K=1
K=5
K=10
Figure 3: Effect of K(K−1is the number of times we use the helper h) on the test accuracy of the main task (for an
angle = 45). We can see that our approach, as our theory predicts, benefits from bigger values of K.
0 20 40
#iterations/500.20.40.60.81.0accuracyNaive
0 20 400.20.40.60.81.0AuxMom
angle=0
angle=45
angle=90
angle=180
Figure 4: Test accuracy obtained using different angles as helpers, for K= 10, step sizeη= 0.01and momentum
parametera= 0.1. We see that, astonishingly, AuxMOM does not suffer much from the change in the angle, whereas,
as expected, the bigger the angle, the worse the accuracy on the main task for the naive approach.
0 20 40
#iterations/500.20.40.60.81.0accuracyAUXMOM
NaiveAlg
Fine Tuning
Figure 5: comparison of The Naive approach, AuxMOM, and Fine Tuning for an angle = 90. Again, we see that while
not suffering from the added bias, Fine Tuning is slower than AuxMOM.
Mislabeled data. In a similar experiment, the helper his given again by MNIST images, but this time, we
choose a wrong label for each image with a probability p. Figure 6 shows the results.
0 20 40
#iterations/500.20.40.60.81.0accuracyNaive
0 20 400.20.40.60.81.0AuxMom
p=0.2
p=0.5
p=0.8
p=1
Figure 6: Test accuracy obtained using different probabilities pas helpers, for K= 10, step size η= 0.01and
momentum parameter a= 0.1. Again, astonishingly, AuxMOM does not suffer much from the change in the angle,
whereas, as expected, the bigger the angle, the worse the accuracy on the main task.
12Published in Transactions on Machine Learning Research (02/2024)
0 10 20 30 40 50
#iterations/500.20.40.60.81.0accuracytest accurracy for different algorithms
Coreset + AuxMOM
Coreset
W/O
AuxMOM - Coreset
Figure 7: Comparing Coresets with and without AuxMOM. We see that with AuxMOM, we reach a higher accuracy,
similar to using the true dataset (W/O).
0 5 10 15 20 25 30 35 40
#iterations/500.10.20.30.40.50.6accuracytest accurracy for different algorithms (CIFAR10)
Coreset + AuxMOM
Coreset
W/O
AuxMOM - Coreset
0 5 10 15 20 25 30 35 40
#iterations/500.000.050.100.150.200.25accuracytest accurracy for different algorithms (CIFAR100)
Coreset + AuxMOM
Coreset
W/O
AuxMOM - Coreset
Figure 8: CIFAR10 and CIFAR100 experiments. Again, using AuxMOM leads to higher accuracy, similar to using the
true dataset (W/O), whereas using the coreset alone leads to a loss in accuracy.
6.3 Training with Coresets
MNIST experiment. We consider again the MNIST dataset. This time, we randomly sample a subset that
has one-fifth the size of the original training set; this subset will play the role of the coreset.
We compare running SGDm solely on the subset (referred to as Coreset) to training using AuxMOM with
the coreset as a helper (referred to as Coreset + AuxMOM). We also consider as baselines running SGDm on
the same total amount of work performed by AuxMOM and SGDm on the coreset (referred to as W/O) and
running on the same amount of work done by the main function on AuxMOM (referred to as AuxMOM -
Coreset).
The results of this experiment are shown in Figure 7. We can see that Coreset loses around 2%accuracy
compared to W/O, whereas Coreset + AuxMOM reaches the same accuracy as W/O (it even beats it a
little). Again, we see that AuxMOM bests training alone (AuxMOM - Coreset here).
CIFAR10/100 experiments. We performed the same experiments on the CIFAR10 and CIFAR100
datasets; again, we considered a randomly sampled coreset one-fifth the size of the original training set. We
note that in these experiments, we used simple convolutional neural networks (two convolutions, followed
each by a max-pooling layer, followed by three linear layers, to which a ReLu activation is applied, except for
the last layer) and a constant step-size strategy, which shouldn’t lead to state-of-the-art performance.
The results depicted in Figure 8 show that, similar to the experiment with the MNIST dataset, we observe
that by using AuxMOM, we rival the performance of W/O; on the other hand, Coreset loses nearly 10%
accuracy.
Varying the size of the coreset. We consider the CIFAR10 dataset and assess the performance of
AuxMOM and training on the coreset alone for different coreset sizes ( 5%,10%, and 25%). Figure 9 shows
how AuxMOM is only influenced in that it needs more iterations to reach the same accuracy for a smaller
13Published in Transactions on Machine Learning Research (02/2024)
0 10 20 30 40 50 60
#iterations/500.00.10.20.30.40.50.60.7accuracyCIFAR10 - 5% size coreset
Coreset + AuxMOM
Coreset
0 10 20 30 40 50 60
#iterations/500.00.10.20.30.40.50.60.7accuracyCIFAR10 - 10% size coreset
Coreset + AuxMOM
Coreset
0 5 10 15 20 25 30 35 40
#iterations/500.00.10.20.30.40.50.60.7accuracyCIFAR10 - 25% size coreset
Coreset + AuxMOM
Coreset
Figure 9: Effect of varying the coreset size on AuxMOM and pure coreset training. We see that the smaller the size of
the coreset is, the worse the performance of training on the coreset alone; however, AuxMOM is only affected in terms
of the number of iterations it needs to reach a given accuracy, which increases as the size of the coreset decreases.
coreset size, whereas training on the coreset alone is highly affected by the size of the corset as we see clearly
that the smaller this size is, the lower the reached accuracy is.
Overhead of AuxMOM over training alone. The only additional time AuxMOM needs is the time
necessary for sampling a new batch from the original dataset, computing its gradient, and updating the
momentum used by AuxMOM; it should be negligible in practice.
6.4 Semi-supervised logistic regression
We consider a semi-supervised logistic regression task on the “Mushrooms” dataset from the libsvmtools
repository (Chang & Lin, 2011), which has 8124samples, each with 112features. We divide this dataset
into three equal parts: one for training and, one for testing, and the third one is unlabeled. In this context,
the helper task his constituted of the unlabeled data to which we assigned random labels. Figure 10 shows
indeedAuxMOM accelerates convergence on the training set. More importantly, it also leads to a smaller
loss on the test set, which suggests a generalization benefit coming from using the unlabeled set.
0 2000 4000 6000 8000
stoch. grad of f10−510−410−310−210−1100/bardbl∇ftrain/bardblGradient norm
0 2000 4000 6000 8000
stoch. grad of f10−510−410−310−210−1100ftrain−f⋆
trainTrain loss
0 2000 4000 6000 8000
stoch. grad of f10−310−210−1100ftest−f⋆
testTest loss
K=1 (SGDm)
AUXMOM K=5
AUXMOM K=10
Figure 10: Gradient norm, train loss, and test loss. We set the parameter a= 0.1, and the stepsizes were optimized
using grid-search for best training loss.
7 Discussion
Fine Tuning. Fine Tuning uses the helper gradients first and then uses the gradients of f; in this sense,
fine-tuning has the advantage that it can be used for different functions fwithout having to go through the
first phase each time. AuxMOM seems to beat it (especially for small values of the similarity δ); however, it
does not enjoy the same advantage. Can we reconcile both worlds?
Better measures of similarity? Quantifying similarity between functions of the previous stochastic form
is an open problem in Machine Learning that touches many domains such as Federated learning, transfer
learning, curriculum learning, and continual learning. Knowing what measure is most appropriate for each
case is an interesting problem. In this work, we don’t pretend to solve the latter problem.
Higher order strategies. We believe our work can be “easily” extended to higher order strategies by using
proxies off−hbased on higher order derivatives. We expect that, in general, if we use a given Taylor
approximation, we will need to make assumptions about its error. For example, if we use a 2nd order Taylor
14Published in Transactions on Machine Learning Research (02/2024)
approximation, we expect we will need to bound the difference of the third derivatives of fandh; this will
be similar to the analysis of the Newton algorithm with cubic regularization(Nesterov & Polyak, 2006).
Dealing with the noise of the snapshot. In this work, we proposed to deal with the noise of the snapshot
gradient of f−husing momentum, other approaches such as using batch sizes of varying sizes with training
(typically a batch size that increases as convergence is near) are possible, such an approach was used in
SCSG(Lihua et al.).
Positively correlated noise. We showed in this work that we might benefit if we could sample gradients of
hthat are positively correlated with gradients of f, but we did not mention how this can be done. This is an
interesting question that we intend to follow in the future.
8 Conclusion
We studied the general problem of optimizing a target function with access to a set of potentially decentralized
helpers. Our framework is broad enough to recover many machine learning and optimization settings (ignoring
the applicability of the similarity assumptions). While there are different ways of solving this problem in
general, we proposed two variants AuxMOM andAuxMVR that we showed improve on known optimal
convergence rates. We also showed how we could go beyond the bias correction that we have proposed; this
can be potentially accomplished by using higher-order approximations of the difference between target and
helper functions. Furthermore, we only considered the hessian similarity assumption in this work, but we
think it is possible to use other similarity measures depending on the solved problem; finding such measures
is outside of the scope of this work, but it might be a good future direction.
9 Acknowledgements
The authors would like to express their gratitude to Martin Jaggi and the MLO team at EPFL for their
valuable discussions. Additionally, we appreciate the helpful feedback provided by the TMLR reviewers.
15Published in Transactions on Machine Learning Research (02/2024)
References
Ahmad Ajalloeian and Sebastian U. Stich. On the convergence of sgd with biased gradients. arXiv:2008.00051
[cs.LG], 2020.
Peste Alexandra, Iofinova Eugenia, Vladu Adrian, and Alistarh Dan. Ac/dc: Alternating com-
pressed/decompressed training of deep neural networks. arXiv:2106.12379 [cs.LG] , 2019.
Mohtashami Amirkeivan, Jaggi Martin, and U. Stich Sebastian. Masked training of neural networks with
partial gradients. arXiv:2106.08895 [cs.LG] , 2021.
Navon Aviv, Achituve Idan, Maron Haggai, Chechik Gal, and Fetay Ethan. Auxiliary learning by implicit
differentiation. ICLR 2021. URL https://arxiv.org/pdf/2007.02693.pdf .
Olivier Bachem, Mario Lucic, and Krause Andreas. Practical coreset constructions for machine learning.
arXiv:1703.06476 [stat.ML] https: // arxiv. org/ abs/ 1703. 06476 , 2017.
Shi Baifeng, Hoffman Judy, Saenko Kate, Darrell Trevor, and Xu Huijuan. Auxiliary task reweighting for
minimum-data learning. 34th Conference on Neural Information Processing Systems (NeurIPS 2020),
Vancouver, Canada. URL https://arxiv.org/pdf/2010.08244.pdf .
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165 , 2020.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology , 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.
edu.tw/~cjlin/libsvm .
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.;
2006)[book reviews]. IEEE Transactions on Neural Networks , 20(3):542–542, 2009.
El Mahdi Chayti, Sai Praneeth Karimireddy, Sebastian U. Stich, Nicolas Flammarion, and Martin Jaggi.
Linear speedup in personalized collaborative learning. arXiv:2111.05968 [cs.LG] , 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning , pp. 1597–1607. PMLR,
2020.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.
arXiv:1905.10018 [cs.LG] , 2019.
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour.
arXiv preprint arXiv:1706.02677 , 2017.
Zhang Jingzhao, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A
theoretical justification for adaptivity. arXiv:1905.11881 [math.OC] . URL https://arxiv.org/abs/1905.
11881.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
NeurIPS , 2013.
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich,
and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning.
arXiv:2008.03606 [cs.LG] , 2020a.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for on-device federated learning.
In37th International Conference on Machine Learning (ICML) , 2020b.
16Published in Transactions on Machine Learning Research (02/2024)
Sai Praneeth Reddy Karimireddy, Sebastian Stich, and Martin Jaggi. Adaptive balancing of gradient and
update computation times using global geometry and approximate subproblems. In International Conference
on Artificial Intelligence and Statistics , pp. 1204–1213. PMLR, 2018.
J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. Ann. Math.
Statist. Volume 23, Number 3, 462-466 , 1952.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Jakub Konecny, H. Brendan McMahan, Daniel Ramage, and Peter Richtarik. Federated optimization :
Distributed machine learning for on-device intelligence. arxiv.org/abs/1610.02527 , 2016.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/ .
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural
networks. ICML, 2013.
Lei Lihua, Ju Cheng, Chen Jianbo, and I. Jordan Michael. Non-convex finite-sum optimization via scsg
methods. arXiv:1706.09156 [math.OC] . URL https://arxiv.org/abs/1706.09156 .
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of
deep networks from decentralized data. In Proceedings of AISTATS, pp. 1273–1282 , 2017a.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. Communication-
efficient learning of deep networks from decentralized data. In Proceedings of AISTATS , pp. 1273–1282,
2017b.
M. Mohri, G. Sivek, and A. T. Suresh. Agnostic federated learning. arXiv preprint arXiv:1902.00146 , 2019.
Yurii Nesterov and B.T. Polyak. Cubic regularization of newton method and its global performance.
https: // link. springer. com/ content/ pdf/ 10. 1007/ s10107-006-0706-8. pdf , 2006.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takáč. Sarah: A novel method for machine learning
problems using stochastic recursive gradient. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70 , pp. 2613–2621. JMLR. org, 2017.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction
for nonconvex optimization. arXiv:1603.06160 [math.OC] https: // arxiv. org/ abs/ 1603. 06160 , 2016.
Herbert Robbins and Sutton Monro. A stochastic approximation method the annals of mathematical statistics.
Vol. 22, No. 3. pp. 400-407 , 1951a.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics , pp. 400–407, 1951b.
Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098 ,
2017.
Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks , 61:85–117, 2015.
Robin M Schmidt, Frank Schneider, and Philipp Hennig. Descending through a crowded valley–benchmarking
deep learning optimizers. arXiv preprint arXiv:2007.01547 , 2020.
Ohad Shamir, Nathan Srebro, and Tong Zhang. Communication efficient distributed optimization using an
approximate newton-type method. arXiv:1312.7853 [cs.LG] , 2013.
17Published in Transactions on Machine Learning Research (02/2024)
Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng Wang. meprop: Sparsified back propagation for
accelerated deep learning with reduced overfitting. In International Conference on Machine Learning , pp.
3299–3308. PMLR, 2017.
Lin Xingyu, Singh Baweja Harjatin, Kantor George, and Held David. Adaptive auxiliary task weighting
for reinforcement learning. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019),
Vancouver, Canada. URL https://openreview.net/pdf?id=rkxQFESx8S .
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural
networks? arXiv preprint arXiv:1411.1792 , 2014.
Jiahui Yu and Thomas S Huang. Universally slimmable networks and improved training techniques. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1803–1811, 2019.
Xiaojin Jerry Zhu. Semi-supervised learning literature survey. Technical report, University of Wisconsin-
Madison Department of Computer Sciences, 2005.
Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing
He. A comprehensive survey on transfer learning. arXiv:1703.06476 [stat.ML] https: // arxiv. org/ abs/
1911. 02685 , 2020.
A Code
The code for our experiments is available at https://github.com/elmahdichayti/OptAuxInf .
B Basic lemmas
Lemma B.1.∀a,b∈Rd, c> 0 :∥a+b∥2
2≤(1 +c)∥a∥2
2+ (1 +1
c)∥b∥2
2.
Proof.The difference of the two quantities above is exactly ∥√ca−1√cb∥2
2≥0.
Lemma B.2.∀N∈N,∀a1,...,aN∈Rd:∥/summationtextN
i=1ai∥2
2≤N/summationtextN
i=1∥ai∥2
2.
Lemma B.3. It is common knowledge that if fisL-smooth, .i.e. satisfies A3.1 then :
∀x,y∈Rd:f(y)−f(x)≤∇f(x)⊤(y−x) +L
2∥y−x∥2
2.
C Missing proofs
C.1 Naive approach
As a reminder, the naive approach uses one unbiased gradient of fat the beginning of each cycle followed by
K−1unbiased gradients of h.
We prove the following theorem :
Theorem C.1. Under Assumptions 3.1,3.2,4.1. Starting from x0and using a step size ηwe have :
1
2KTT/summationdisplay
t=1E[∥∇f(xt−1)∥2
2] +1−m
2KTT/summationdisplay
t=1K/summationdisplay
k=2E[∥∇f(yt
k−1)∥2
2]≤F0
KTη+Lσ2
2η+K−1
Kζ2.
Forσ2=σ2
f+(K−1)σ2
h
Kthe average variance, F0=E[f(x0)]−f⋆.
18Published in Transactions on Machine Learning Research (02/2024)
Algorithm 2 Naive (f,h)
Require: x0,m0,η,T,K
fort= 1toTdo
Sampleξt
f;compute gf(xt−1,ξt
f)
mt=gf(xt−1,ξt
f)
yt
0=xt−1
fork= 0toK−1do
ifk=0then
dt
k=mt
else
Sampleξt,k
h; Compute gh(yt
k,ξt,k
h)
dt
k=gh(yt
k,ξt,k
h)
end if
yt
k+1=yt
k−ηdt
k
end for
Update xt=yt
K
end for
Proof.The proof is based on biased SGD theory. From (Ajalloeian & Stich, 2020) we have that if we the
gradients g(x)that we are using are such that :
E[g(x)] =∇f(x) +b(x)
For a target function fand a gradient bias b(x). Then we have for η≤1
L:
f(yt
k+1)−f(yt
k)≤η
2/parenleftbig
−∥∇f(yt
k)∥2
2+∥b(yt
k)∥2
2/parenrightbig
+Lη2
2σ2
k
In our case, by Assumption4.1 : ∥b(yt
k)∥2
2≤/parenleftbig
ζ2+m∥∇f(yt
k)∥2
2/parenrightbig
1k>0andσ2
k= 1k=0σ2
f+ 1k>0σ2
h.
Summing the above inequality from t= 1,k= 0tot=T,k=K−1and then dividing by KTwe get the
statement of the theorem.
Lower bound. It is not difficult to prove that we cannot do better using the naive strategy. We can for
example pick in 1da target function f(x) =1
2x2and an auxiliary function h(x) =1
2(x−ζ)2. Both functions
are1-smooth and satisfy hessian similarity with δ= 0, however,ζis not (necessarily zero), in particular, this
shows that hessian similarity (Assumption3.3) and gradient dissimilarity (Assumption4.1) are orthogonal. To
show the lower bound we can consider the perfect case where we have access to full gradients of fandh.
The dynamics of the naive approach can be written in the form:
xt+1= (1−η)xt+ηζ1t̸=0mod(K)
Which implies
xt= (1−η)tx0+ζt−1/summationdisplay
i=0η(1−η)t−i−11i̸=0mod(K)
= (1−η)tx0+ Ω(ζ)
This sequence does not converge to zero no matter the choice of η<1.
19Published in Transactions on Machine Learning Research (02/2024)
C.2 Momentum with auxiliary information
C.2.1 Algorithm description
The algorithm that we proposed proceeds in cycles, at the beginning of each cycle we have xt−1and
takeKiterations of the form yt
k=yt
k−1−ηdt
kwhere yt
0=xt−1,dt
k=gh(yt
k−1,ξt
h,k) +mtandmt=
(1−a)mt−1+agf−h(xt−1,ξt−1
f−h). Then we set xt=yt
K.
Algorithm 3 AuxMOM (f,h)
Require: x0,m0,η,a,T,K
fort= 1toTdo
Sampleξt
f−h;compute gf−h(xt−1,ξt−1
f−h)
mt= (1−a)mt−1+agf−h(xt−1,ξt−1
f−h)
yt
0=xt−1
fork= 0toK−1do
Sampleξt,k
h; Compute gh(yt
k,ξt,k
h)
dt
k=gh(yt
k,ξt,k
h) +mt
yt
k+1=yt
k−ηdt
k
end for
Update xt=yt
K
end for
We will introduce ¯dt
k=∇h(yt
k−1) +mtand we note that E[dt
k|yt
k−1] =¯dt
kandE[∥dt
k−¯dt
k∥2
2]≤σ2
h. We will
be using this fact and sometimes we will condition on yt
k−1implicitly to replace dt
kby¯dt
k.
Note.Algorithm 4 is another version of AuxMOM that works very well in practice and seems more robust
to the choice of the step size than Algorithm 3. The main difference with Algorithm 3 is that the momentum
is applied on fonly. For this Algorithm, we could not prove any benefit from the similarity δ.
Algorithm 4 AuxMOM (f,h)−V0
Require: x0,m0,η,a,T,K
fort= 1toTdo
Sampleξt
f;compute gf(xt−1,ξt−1
f)
mt= (1−a)mt−1+agf(xt−1,ξt−1
f)
yt
0=xt−1
fork= 0toK−1do
Sampleξt,k
h; Compute gh(yt
k,ξt,k
h),gh(xt−1,ξt,k
h)
dt
k=gh(yt
k,ξt,k
h)−gh(xt−1,ξt,k
h) +mt
yt
k+1=yt
k−ηdt
k
end for
Update xt=yt
K
end for
C.2.2 Convergence rate
We prove the following theorem that gives the convergence rate of this algorithm in the non-convex case.
20Published in Transactions on Machine Learning Research (02/2024)
TheoremC.2. Under assumptions A3.1, 3.2,3.3. For a= 36δKηandη=min(1
L,1
144δK,/radicalbigg
˜F
128LβKTσ2
f).
This choice gives us the rate :
1
8KTT/summationdisplay
t=1K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(yt
k)∥2
2/bracketrightbig
≤24/radicaligg
Lβ˜Fσ2
f
T+(L+ 192δK)˜F
KT+σ2
h
6K.
where ˜F=F0+E0
8δandβ=δ
L/parenleftbigg
σ2
f−h
σ2
f+1
18Kσ2
h
σ2
f/parenrightbigg
+1
288Kσ2
h
σ2
f,F0=f(x0)−f⋆andE0=E[∥m0−
∇f(x0) +∇h(x0)∥2
2].
Furthermore, if we use a batch-size Ttimes bigger for computing an estimate m0of∇f(x0)−∇h(x0),
then by taking a= max(1
T,36δKη)and replacing ˜FbyF0in the expression of η, we get :
η= min(1
L,1
192δK,/radicaligg
F0
144LβK2Tσ2
f).
we get the following :
1
8KTT/summationdisplay
t=1K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(yt
k)∥2
2/bracketrightbig
≤24/radicaligg
LβF0σ2
f
T+(L+ 192δK)F0
KT+8σ2
f−h
T+σ2
h
6K.
C.2.3 Bias in helpers updates and Notation
Lemma C.3. Under theδ-Bounded Hessian Dissimilarity assumption, we have :
∀x,y∈Rd:∥∇h(y)−∇h(x)−∇f(y) +∇f(x)∥2≤δ2∥y−x∥2.
Proof.A simple application of the δ-Bounded Hessian Dissimilarity assumption.
Notation : We will use the following notations : et=mt−∇f(xt−1) +∇h(xt−1)to denote the momentum
error andEt=E[∥et∥2
2]to denote its expected squared norm. ∆t
k=E[∥yt
k−xt−1∥2]will denote the
progress made up to the k-th round of the t-th cycle, and for the progress in a whole cycle we will use
∆t=E[∥xt−xt−1∥2]. We also denote ¯dt
k=dt
k−gh(yt
k,ξt,k
h) +∇h(yt
k−1) =∇h(yt
k−1) +mt.
C.2.4 Change during each cycle
Variance of ¯dt
k.We would like ¯dt
kto be∇f(yt
k−1), but it is not. In the next lemma, we control the error
resulting from these two quantities being different.
Lemma C.4. Under assumption A3.3, we have the following inequality :
E[∥¯dt
k−∇f(yt
k−1)∥2
2]≤2δ2∆t
k−1+ 2Et
Proof.We have
¯dt
k−∇f(yt
k−1) =∇h(yt
k−1)−∇f(yt
k−1) +mt
=h(yt
k−1)−∇f(yt
k−1)−∇f(yt
k−1) +∇f(xt−1) +mt−∇f(xt−1) +∇h(xt−1)
=h(yt
k−1)−∇f(yt
k−1)−∇f(yt
k−1) +∇f(xt−1) +et
Using Lemma B.1 with c= 1, we get :
E[∥¯dt
k−∇f(yt
k−1)∥2
2]≤2E[∥h(yt
k−1)−∇f(yt
k−1)−∇f(yt
k−1) +∇f(xt−1)∥2
2] + 2E[∥et∥2
2]
≤2δ2∥yt
k−1−xt−1∥2
2+ 2E[∥et∥2
2]
= 2δ2∆t
k−1+ 2Et
21Published in Transactions on Machine Learning Research (02/2024)
Distance moved in each step.
Lemma C.5. forη≤1
5δKwe have:
∆t
k≤(1 +1
K)∆t
k−1+ 12Kη2Et+ 6Kη2E[∥∇f(yt
k−1)∥2
2] +η2σ2
h.
Proof.
∆t
k=E[∥yt
k−xt−1∥2
2]
=E[∥yt
k−1−ηdt
k−xt−1∥2
2]
=E[∥yt
k−1−η¯dt
k−xt−1∥2
2] +η2E[∥dt
k−¯dt
k∥2
2]
≤(1 +1
2K)∆t
k−1+ (2K+ 1)η2E[∥¯dt
k∥2
2] +η2σ2
h
= (1 +1
2K)∆t
k−1+ 3Kη2E[∥¯dt
k±∇f(yt
k−1)∥2
2] +η2σ2
h
≤(1 +1
2K)∆t
k−1+ 6Kη2E[∥¯dt
k−∇f(yt
k−1)∥2
2] + 6Kη2E[∥∇f(yt
k−1)∥2
2] +η2σ2
h
≤(1 +1
2K)∆t
k−1+ 6Kη2(2δ2∆t
k−1+ 2Et) + 6Kη2E[∥∇f(yt
k−1)∥2
2] +η2σ2
h
= (1 +1
2K+ 12δ2Kη2)∆t
k−1+ 12Kη2Et+ 6Kη2E[∥∇f(yt
k−1)∥2
2] +η2σ2
h
The condition η≤1
5δKensures 12δ2Kη2≤1
2Kwhich finishes the proof.
Progress in one step.
Lemma C.6. Forη≤min(1
L,1
192δK), under assumptions A3.1 and A3.3, the following inequality is true:
E/bracketleftbig
f(yt
k) +δ(1 +2
K)K−k∆t
k/bracketrightbig
≤E/bracketleftbig
f(yt
k−1) +δ(1 +2
K)K−(k−1)∆t
k−1/bracketrightbig
−η
4E[∥∇f(yt
k−1)∥2
2] + 2ηEt+ (L
2+ 8δ)η2σ2
h.
Proof.TheL-smoothness of fguarantees :
f(yt
k)−f(yt
k−1)≤−η∇f(yt
k−1)⊤dt
k+Lη2
2∥dt
k∥2
2.
By taking expectation conditional to the knowledge of yt
k−1we have:
E[f(yt
k)−f(yt
k−1)]≤−ηE[∇f(yt
k−1)⊤¯dt
k] +Lη2
2E[∥¯dt
k∥2
2] +Lη2
2σ2
h.
Using the identity −2ab= (a−b)2−a2−b2, we have :
E[f(yt
k)−f(yt
k−1)]≤−η
2E[∥∇f(yt
k−1)∥2
2] +η
2E[∥dt
k−∇f(yt
k−1)∥2
2] +Lη2−η
2E[∥dt
k∥2
2] +Lη2
2σ2
h.
Usingη≤1
Lwe can get rid of the last term in the above inequality.
22Published in Transactions on Machine Learning Research (02/2024)
Using LemmaC.4, we get :
E[f(yt
k)−f(yt
k−1)]≤−η
2E/bracketleftbig
∥∇f(yt
k−1)∥2
2/bracketrightbig
+η
2E/bracketleftbig
∥dt
k−∇f(yt
k−1)∥2
2/bracketrightbig
+Lη2
2σ2
h
≤−η
2E/bracketleftbig
∥∇f(yt
k−1)∥2
2/bracketrightbig
+η
2(2δ2∆t
k−1+ 2Et) +Lη2
2σ2
h
=δ2η∆t
k−1+ηEt−η
2E/bracketleftbig
∥∇f(yt
k−1)∥2
2/bracketrightbig
+Lη2
2σ2
h.
Now we multiply LemmaC.5 by δ(1 +2
K)K−k. Note that 1≤(1 +2
K)K−k≤8.
δ(1 +2
K)K−k∆t
k≤δ(1 +2
K)K−k/parenleftbig
(1 +1
K)∆t
k−1+ 12Kη2Et+ 6Kη2E[∥∇f(yt
k−1)∥2
2]/parenrightbig
+δ(1 +2
K)K−kη2σ2
h
≤δ(1 +2
K)K−(k−1)∆t
k−1−δ
K(1 +2
K)K−k∆t
k−1+ 96Kδη2Et
+ 48Kδη2E[∥∇f(yt
k−1)∥2
2] + 8δη2σ2
h
≤δ(1 +2
K)K−(k−1)∆t
k−1−δ
K∆t
k−1+ 96Kδη2Et
+ 48Kδη2E[∥∇f(yt
k−1)∥2
2] + 8δη2σ2
h
Adding the last two inequalities, we get :
E[f(yt
k)] +δ(1 +2
K)K−k∆t
k≤E[f(yt
k−1)] +δ(1 +2
K)K−(k−1)∆t
k−1
+ (δ2η−δ
K)∆t
k−1
+ (η+ 96Kδη2)Et
+ (−η
2+ 48Kδη2)E[∥∇f(yt
k−1)∥2
2]
+ (L/2 + 8δ)η2σ2
h
Forη≤1
192δKwe haveδ2η−δ
K≤0,η+ 96Kδη2≤2ηand−η
2+ 48Kδη2≤−η
4which gives the lemma.
Distance moved in a cycle.
Lemma C.7. Forη≤1
5Kδand under assumptions A3.1 and A3.3 with Gt=1
K/summationtextK−1
k=0E/bracketleftbig
∥∇f(yt
k)∥2
2/bracketrightbig
, we
have :
∆t/parenleftig
:=E/bracketleftbig
∥xt−xt−1∥2
2/bracketrightbig/parenrightig
≤36K2η2Et+ 18K2η2Gt+ 3Kη2σ2
h.
Proof.We use the fact xt=yt
K, which means ∆t= ∆t
K. The recurrence established in LemmaC.5 implies :
∆t≤K/summationdisplay
k=1(1 +1
K)K−k/parenleftig
12Kη2Et+ 6Kη2E[∥∇f(yt
k−1)∥2
2] +η2σ2
h/parenrightig
≤36K2η2Et+ 18K2η21
K/summationdisplay
kE/bracketleftbig
∥∇f(yt
k)∥2
2/bracketrightbig
+ 3Kη2σ2
h
= 36K2η2Et+ 18K2η2Gt+ 3Kη2σ2
h.
Where we used the fact that (1 +1
K)K−k≤3.
Momentum variance. Here we will bound the quantity Et.
23Published in Transactions on Machine Learning Research (02/2024)
Lemma C.8. Under assumptions A3.1,A3.2 and for a≥12Kδη, we have :
Et≤(1−a
2)Et−1+36δ2K2η2
aGt−1+a2σ2
f−h+a
24Kσ2
h.
Proof.
Et=E/bracketleftbig
∥mt−∇f(xt−1) +∇h(xt−1)∥2
2/bracketrightbig
=E/bracketleftbig
∥(1−a)(mt−1−∇f(xt−1) +∇h(xt−1)) +a(gf(xt−1) +gh(xt−1)−∇f(xt−1) +∇h(xt−1))∥2
2/bracketrightbig
≤(1−a)2E/bracketleftbig
∥mt−1±(∇f(xt−2)−∇h(xt−2))−(∇f(xt−1)−∇f(xt−1))∥2
2/bracketrightbig
+a2σ2
f−h
≤(1−a)2(1 +a/2)Et−1+ (1−a)2(1 + 2/a)E/bracketleftbig
∥∇f(xt−2)−∇h(xt−2)−∇f(xt−1) +∇h(xt−1)∥2
2/bracketrightbig
+a2σ2
f−h
≤(1−a)Et−1+2δ2
a∆t−1+a2σ2
f−h
Where we used the L-smoothness of fto get the last inequality. We have also used the inequalities
(1−a)2(1 +a/2)≤(1−a)and(1−a)2(1 + 2/a)≤2/atrue for all a∈(0,1].
We can now use LemmaC.7 to have :
Et≤(1−a+72δ2K2η2
a)Et−1+36δ2K2η2
aGt−1+a2σ2
f−h+6Kδ2η2
aσ2
h
By takinga≥12δKηwe ensure72δ2K2η2
a≤a
2, So :
Et≤(1−a
2)Et−1+36δ2K2η2
aGt−1+a2σ2
f−h+a
24Kσ2
h
Progress in one round.
Lemma C.9. Under the same assumptions as in LemmaC.6, we have :
η
4Gt≤Ft−1−Ft
K−δ
K∆t+ 2ηEt+ (L
2+ 8δ)η2σ2
h.
WhereFt=E[f(xt)]−f⋆.
Proof.We use the inequality established in LemmaC.6, which can be rearranged in the following way :
η
4E[∥∇f(yt
k−1)∥2
2]≤E/bracketleftbig
f(yt
k−1) +δ(1 +2
K)K−(k−1)∆t
k−1/bracketrightbig
−/parenleftig
E/bracketleftbig
f(yt
k) +δ(1 +2
K)K−k∆t
k/bracketrightbig/parenrightig
+ 2ηEt+ (L
2+ 8δ)η2σ2
h.
We sum this inequality from k= 1tok=K, this will give:
Kη
4Gt≤E/bracketleftbig
f(yt
0) +δ(1 +2
K)K∆t
0/bracketrightbig
−/parenleftig
E/bracketleftbig
f(yt
K) +δ∆t
K/bracketrightbig/parenrightig
+ 2ηKEt+ (L
2+ 8δ)Kη2σ2
h.
We note that yt
0=xt−1andyt
K=xt, which means ∆t
0= 0and∆t
K= ∆t. So we have :
η
4Gt≤Ft−1−Ft
K−δ
K∆t+ 2ηEt+ (L
2+ 8δ)η2σ2
h.
24Published in Transactions on Machine Learning Research (02/2024)
Let’s derive now the convergence rate.
We have : /braceleftigg
η
4Gt≤Ft−1−Ft
K+ 2ηEt+ (L
2+ 8δ)η2σ2
h,
Et≤(1−a
2)Et−1+36δ2K2η2
aGt−1+a2σ2
f−h+a
24Kσ2
h.
We will add to both sides of the first inequality the quantity4η
aEt.
So :
η
4Gt+4η
aEt≤Ft−1−Ft
K+2ηEt+(L
2+8δ)η2σ2
h+4η
a/parenleftig
(1−a
2)Et−1+36δ2K2η2
aGt−1+a2σ2
f−h+a
24Kσ2
h/parenrightig
,
Which gives for a≥36δKη:
η
4Gt−η
8Gt−1≤Φt−1−Φt+ 4ηaσ2
f−h+ (L
2+ 8δ)η2σ2
h+η
6Kσ2
h, (7)
For a potential Φt=Ft
K+ (4η
a−2η)Et≤Ft
K+4η
aEt.
Summing the inequality 7 over t, gives :
1
8TT/summationdisplay
t=1Gt≤Φ0
ηT+ 4aσ2
f−h+ (L
2+ 8δ)ησ2
h+σ2
h
6K,
≤F0
ηKT+4
aTE0+ 4aσ2
f−h+ (L
2+ 8δ)ησ2
h+σ2
h
6K,
Now taking a= 36δKη≤1, we get :
1
8TT/summationdisplay
t=1Gt≤F0
ηKT+1
8δKηTE0+ 144δKησ2
f−h+ (L
2+ 8δ)ησ2
h+σ2
h
6K,
=˜F
ηKT+ 144δKησ2
f−h+ (L
2+ 8δ)ησ2
h+σ2
h
6K,
=˜F
ηKT+ 144LβKησ2
f+σ2
h
6K,
For˜F=F0+E0
8δandβ=δ
L/parenleftbigg
σ2
f−h
σ2
f+1
18Kσ2
h
σ2
f/parenrightbigg
+1
288Kσ2
h
σ2
f.
Taking into account all the conditions on ηthat were necessary, we can take :
η= min(1
L,1
192δK,/radicaligg
˜F
144LβK2Tσ2
f).
This choice gives us the rate :
1
8TT/summationdisplay
t=1Gt≤24/radicaligg
Lβ˜Fσ2
f
T+(L+ 192δK)˜F
KT+σ2
h
6K.
Dealing with the term E0.If we use a batch-size Stimes bigger for generating m0than the other
batch-sizes, then we have E0≤σ2
f−h
S. In particular, for S=T,E0≤σ2
f−h
T.
Now by taking a= max(1
T,36δKη), we ensureE0
aT≤E0≤σ2
f−h
T, then
25Published in Transactions on Machine Learning Research (02/2024)
1
8TT/summationdisplay
t=1Gt≤F0
ηKT+4
aTE0+ 4aσ2
f−h+ (L
2+ 8δ)ησ2
h+σ2
h
6K,
≤F0
ηKT+ 4E0+4σ2
f−h
T+ 144Kδησ2
f−h+ (L
2+ 8δ)ησ2
h+σ2
h
6K,
≤F0
ηKT+8σ2
f−h
T+ 144Kδησ2
f−h+ (L
2+ 8δ)ησ2
h+σ2
h
6K,
=F0
ηKT+ 144LβKησ2
f+8σ2
f−h
T+σ2
h
6K,
Taking
η= min(1
L,1
192δK,/radicaligg
F0
144LβK2Tσ2
f).
we get the following :
1
8TT/summationdisplay
t=1Gt≤24/radicaligg
LβF0σ2
f
T+(L+ 192δK)F0
KT+8σ2
f−h
T+σ2
h
6K.
C.3 MVR with auxiliary information
C.3.1 Algorithm description
The algorithm that we proposed proceeds in cycles, at the beginning of each cycle we have states xt−2and
xt−1. To update these states, we take K+ 1iterations of the form yt
k=yt
k−1−ηdt
kwhere yt
0=xt−1,dt
k=
gh(yt
k−1,ξt
h,k)+mtandmt= (1−a)mt−1+agf−h(xt−1,ξt−1
f−h)+(1−a)/parenleftig
gf−h(xt−1,ξt−1
f−h)−gf−h(xt−2,ξt−1
f−h)).
Then we set xt=yt
K.
Algorithm 5 AUXMVR (f,h)
Require: x0,m0,η,a,T,K
x−1=x0
fort= 1toTdo
Sampleξt−1
f−h
gf−h
prev=gf−h(xt−2,ξt−1
f−h)
gf−h=gf−h(xt−1,ξt−1
f−h)
mt= (1−a)mt−1+agf−h+ (1−a)(gf−h−gf−h
prev)§update momentum
yt
0=xt−1
fork= 0toK−1do
Sampleξt,k
h; Compute gh(yt
k,ξt,k
h)
dt
k=gh(yt
k,ξt,k
h) +mt
yt
k+1=yt
k−ηdt
k
end for
Update xt=yt
K
end for
We will use the same notations as section C.2.
C.3.2 Convergence of MVR with auxiliary information
We prove the following theorem that gives the convergence rate of this algorithm in the non-convex case.
26Published in Transactions on Machine Learning Research (02/2024)
Theorem C.10. Under assumptions A3.1, 3.2,4.5. For m0such thatE0≤σ2
f−h/T, fora=
max(1
T,1156δ2K2η2)andη=min/parenleftig
1
L,1
192δK,1
K/parenleftig
F0
18432δ2Tσ2
f−h/parenrightig1/3
,/radicalig
F0
KT(L/2+8δK)/parenrightig
.This choice gives
us the rate :
1
8KTT/summationdisplay
t=1K−1/summationdisplay
k=1E/bracketleftbig
∥∇f(yt
k)∥2
2/bracketrightbig
≤40/parenleftigδF0σf−h
T/parenrightig2/3
+2/radicalbigg
(L/2 + 8δ)F0σ2
h
KT+(L+ 192δK)F0
KT+8σ2
f−h
T+σ2
h
48K.
WhereF0=f(x0)−f⋆andE0=E∥m0−(∇f(x0)−∇h(x0))∥2.
C.3.3 Proof
Given that the form of dt
kis the same as in AuxMOM (Algorithm 3), the same Lemmas still hold, except for
Lemma C.8 which changes to the following Lemma.
Momentum variance of AUXMVR. Here we will bound the quantity Et.
Lemma C.11. Under assumptions A4.5,A3.2 , we have :
Et≤(1−a)Et−1+ 2δ2∆t−1+ 2a2σ2
f−h.
Combining this inequality with Lemma C.7, we get for η≤1
5Kδanda≥144K2δ2η2:
Et≤(1−a
2)Et−1+ 36K2δ2η2Gt−1+ 2a2σ2
f−h+ 6Kδ2η2σ2
h.
Proof.First, we notice that
et=mt−∇f(xt−1) +∇h(xt−1)
= (1−a)mt−1+agf−h(xt−1,ξt−1
f)
+ (1−a)/parenleftig
gf−h(xt−1,ξt−1
f−h)−gf−h(xt−2,ξt−1
f−h)/parenrightig
−∇f(xt−1) +∇h(xt−1)
= (1−a)et−1+a(gf−h(xt−1,ξt−1
f−h)−∇f(xt−1) +∇h(xt−1))
+ (1−a)/parenleftbig
gf−h(xt−1,ξt−1
f−h)−∇f(xt−1) +∇h(xt−1)−gf−h(xt−2,ξt−1
f−h)
+∇f(xt−2)−∇h(xt−2)/parenrightbig
Notice that et−1is independent of the rest of the formulae which is itself centered (has a mean equal to zero),
so :
Et≤(1−a)2Et−1+ 2a2σ2
f−h+ 2δ2∆t−1.
Now forη≤1
5Kδ, we can use Lemma C.7 and we get
Et≤(1−a)Et−1+ 2a2σ2
f−h+ 2δ2/parenleftbig
36K2η2Et+ 18K2η2Gt+ 3Kη2σ2
h/parenrightbig
≤(1−a+ 74K2δ2η2)Et−1+ 2a2σ2
f−h+ 36K2δ2η2Gt−1+ 6Kδ2η2σ2
h
It is easy to verify that for a≥144K2δ2η2, we have 74K2δ2η2≤a/2which finishes the proof.
Let’s now derive the convergence rate of AUXMVR.
We have : /braceleftbiggη
4Gt≤Ft−1−Ft
K+ 2ηEt+ (L
2+ 8δ)η2σ2
h,
Et≤(1−a
2)Et−1+ 36K2δ2η2Gt−1+ 2a2σ2
f−h+ 6Kδ2η2σ2
h.
We will add to both sides of the first inequality the quantity4η
aEt.
So :
η
4Gt+4η
aEt≤Ft−1−Ft
K+2ηEt+(L
2+8δ)η2σ2
h+4η
a/parenleftig
(1−a
2)Et−1+36K2δ2η2Gt−1+2a2σ2
f−h+6Kδ2η2σ2
h/parenrightig
,
27Published in Transactions on Machine Learning Research (02/2024)
Let’s define the potential Φt=Ft
K+ (4η
a−2η)Et≤Ft
K+4η
aEt.
Then
η
4Gt≤Φt−1−Φt+ (L
2+ 8δ)η2σ2
h+144K2δ2η3
aGt−1+ 8aησ2
f−h+24Kδ2η3
aσ2
h,
fora≥1152K2δ2η2, we have144K2δ2η3
a≤η/8, which means
η
4Gt−η
8Gt−1≤Φt−1−Φt+ (L
2+ 8δ)η2σ2
h+ 8aησ2
f−h+24Kδ2η3
aσ2
h,
Summing the last inequality over tgives :
1
8TT/summationdisplay
t=1Gt≤Φ0
ηT+ 8aσ2
f−h+ (L
2+ 8δ)ησ2
h+24Kδ2η2
aσ2
h,
≤F0
ηKT+4
aTE0+ 8aσ2
f−h+ (L
2+ 8δ)ησ2
h+24Kδ2η2
aσ2
h,
If we use a batch Ttimes larger at the beginning, we can ensure E0≤σ2
f−h
T, so:
1
8TT/summationdisplay
t=1Gt≤F0
ηKT+4σ2
f−h
aT2+ 8aσ2
f−h+ (L
2+ 8δ)ησ2
h+24Kδ2η2
aσ2
h,
Now taking a= max(1
T,1152δ2K2η2), we get :
1
8TT/summationdisplay
t=1Gt≤F0
ηKT+4σ2
f−h
T+8σ2
f−h
T+ 9216δ2η2K2σ2
f−h+ (L
2+ 8δ)ησ2
h+σ2
h
48K,
Taking into account all the conditions on ηthat were necessary, we can take :
η= min/parenleftig1
L,1
192δK,1
K/parenleftigF0
18432δ2Tσ2
f−h/parenrightig1/3
,/radicaligg
F0
KT(L/2 + 8δK)/parenrightig
.
This choice gives us the rate :
1
8TT/summationdisplay
t=1Gt≤40/parenleftigδF0σf−h
T/parenrightig2/3
+ 2/radicalbigg
(L/2 + 8δ)F0
KT+(L+ 192δK)F0
KT+8σ2
f−h
T+σ2
h
48K.
C.4 Generalization to multiple decentralized helpers.
We consider now the case where we have Nhelpers:h1,...,hN. This case can be easily solved by merging
all the helpers into one helper h=1
N/summationtextN
i=1hifor example (it is easy to see that if each hiisδi-BHD from
fthat their average hwould be1
N/summationtextN
i=1δi-BHD from f ). However, this is not possible if the helpers are
decentralized (are not in the same place and cannot be made to be for privacy reasons for example). For this
reason, we consider a Federated version of our optimization problem in the presence of auxiliary decentralized
information.
In this case, we consider that all functions hiare such that :
∀x,ξ:∥∇2f(x)−∇2hi(x)∥2≤δ.
We will also need an additional assumption on f:
Assumption C.12. (Weak convexity.) fisδ- weakly convex i.e. x∝⇕⊣√∫⊔≀→f(x) +δ∥x∥2
2is convex.
28Published in Transactions on Machine Learning Research (02/2024)
Lemma C.13. Under Assumption C.12 the following is true :
∀N∀x,x1,...,xN∀α≥δ:f(1
NN/summationdisplay
i=1xi) +α∥1
NN/summationdisplay
i=1xi−x∥2
2≤1
NN/summationdisplay
i=1/parenleftig
f(xi) +α∥xi−x∥2
2/parenrightig
.
About the need for Assumption C.12. Assumption C.12 becomes strong for δsmall which is not good,
as this should be the easiest case. however, we would like to point out that we only need Assumption C.12 to
deal with the averaging that we perform to construct the new state xt. In the case where we sample each
time one and only one helper (i.e. S= 1), we don’t need such an assumption. This assumption was made in
the context of Federated Learning for example in (Karimireddy et al., 2020a).
C.4.1 Decentralized momentum version
As in C.2.1 we start from xt−1, we sample (randomly) a set StofShelpers, we sample ξt
f, compute
gf(xt−1,ξt
f), share it with all of the helpers for hi,i∈St, which each sample ξt
h(potentially correlated with
ξt
f) and then update mt
i, then perform Ksteps of yt
i,k=yt
i,k−1−ηdt
i,k, once this finishes yt
i,Kis sent back
tofwhich then does xt=1
S/summationtext
i∈Styt
i,K.
For eachiwe will denote Et
i=E[∥mt
i−∇f(xt−1) +∇hi(xt−1)∥2
2]andEt=1
S/summationtext
i∈StEt
i.
Theorem C.14. Under assumptions A3.1, 3.2,3.3 (and assumption C.12 if S >1). Fora= 36δKη
andη= min(1
L,1
144δK,/radicalbigg
˜F
128LβKTσ2
f).This choice gives us the rate :
1
8KSTT/summationdisplay
t=1K−1/summationdisplay
k=1/summationdisplay
i∈StE/bracketleftbig
∥∇f(yt
i,k)∥2
2/bracketrightbig
≤24/radicaligg
Lβ˜Fσ2
f
T+(L+ 192δK)˜F
KT+σ2
h
6K.
where ˜F=F0+E0
8δandβ=δ
L/parenleftbigg
σ2
f−h
σ2
f+1
18Kσ2
h
σ2
f/parenrightbigg
+1
288Kσ2
h
σ2
f,F0=f(x0)−f⋆andE0=E[∥m0−
∇f(x0) +∇h(x0)∥2
2].
Furthermore, if we use a batch-size Ttimes bigger for computing an estimate m0
iof∇f(x0)−∇hi(x0),
then by taking a= max(1
T,36δKη)and replacing ˜FbyF0in the expression of η, we get :
η= min(1
L,1
192δK,/radicaligg
F0
144LβK2Tσ2
f).
we get the following :
1
8KSTT/summationdisplay
t=1K−1/summationdisplay
k=1/summationdisplay
i∈StE/bracketleftbig
∥∇f(yt
i,k)∥2
2/bracketrightbig
≤24/radicaligg
LβF0σ2
f
T+(L+ 192δK)F0
KT+8σ2
f−h
T+σ2
h
6K.
Proof.The proof follows the same lines as the proof of in C.2.1.
Two changes should be made to the proof: Gtneeds to be updated to Gt=1
SK/summationtext
i∈St,kE/bracketleftbig
∥∇f(yt
k)∥2
2/bracketrightbig
in
both Lemmas C.7 and C.9
29Published in Transactions on Machine Learning Research (02/2024)
In fact, in this case xt=1
S/summationtext
i∈Styt
i,K, which means using convexity of the squared norm :
∆t≤1
S/summationdisplay
i∈St∆t
i,K(:=E/bracketleftbig
∥yt
i,K−xt−1∥2
2/bracketrightbig
)
≤1
S/summationdisplay
i∈St/summationdisplay
k(1 +1
K)K−k/parenleftig
12Kη2Et
i+ 6Kη2E[∥∇f(yt
i,k−1)∥2
2] +η2σ2
h/parenrightig
≤36K2η2Et+ 18K2η21
KS/summationdisplay
i∈St/summationdisplay
kE/bracketleftbig
∥∇f(yt
i,k)∥2
2/bracketrightbig
+ 3Kη2σ2
h
= 36K2η2Et+ 18K2η2Gt+ 3Kη2σ2
h.
And in the descent lemma (That modifies LemmaC.9) we will have :
∀i:Kη
41
K/summationdisplay
kE/bracketleftbig
∥∇f(yt
i,k)∥2
2/bracketrightbig
≤E/bracketleftbig
f(yt
i,0) +δ(1 +2
K)K∆t
i,0/bracketrightbig
−/parenleftig
E/bracketleftbig
f(yt
i,K) +δ∆t
i,K/bracketrightbig/parenrightig
+ 2ηKEt+ (L
2+ 8δ)η2σ2
h.
Where ∆t
i,0= 0and using LemmaC.13 we have :
Ft+δ∆t≤1
S/summationdisplay
i∈St/parenleftig
E/bracketleftbig
f(yt
i,K) +δ∆t
i,K/bracketrightbig/parenrightig
.
Using the last inequality it is easy to get :
Kη
4Gt≤Ft−1−Ft−δ∆t+ 2ηKEt+ (L
2+ 8δ)η2σ2
h.
All the rest is the same.
C.4.2 Decentralized MVR version
As in C.3.1 we start from xt−1, we sample (randomly) a set StofShelpers, we sample ξt
f, compute
gf(xt−1,ξt
f), share it with all of the helpers for hi,i∈St, which each sample ξt
h(potentially correlated with
ξt
f) and then update mt
i, then perform Ksteps of yt
i,k=yt
i,k−1−ηdt
i,k, once this finishes yt
i,Kis sent back
tofwhich then does xt=1
S/summationtext
i∈Styt
i,K.
We can prove the following theorem under the same changes as in the momentum case.
Theorem C.15. Under assumptions A3.1, 3.2,3.3 (and assumption C.12 if S > 1). For mo-
mentum initialization m0
isuch that E0
i≤σ2
f−h/T, fora=max(1
T,1152δ2K2η2), andη=
min/parenleftig
1
L,1
192δK,1
K/parenleftig
F0
18432δ2Tσ2
f−h/parenrightig1/3
,/radicalig
F0
KT(L/2+8δK)/parenrightig
,we get:
1
8KSTT/summationdisplay
t=1/summationdisplay
i∈StK−1/summationdisplay
k=1E/bracketleftbig
∥∇f(yt
i,k)∥2
2/bracketrightbig
≤40/parenleftigδF0σf−h
T/parenrightig2/3
+ 2/radicalbigg
(L/2 + 8δ)F0
KT+(L+ 192δK)F0
KT
+8σ2
f−h
T+σ2
h
48K.
WhereF0=f(x0)−f⋆andE0
i=∥m0
i−(∇f(x0)−∇hi(x0))∥2
2.
30