Published in Transactions on Machine Learning Research (07/2024)
Variational Bayesian Imaging with an Efficient
Surrogate Score-based Prior
Berthy T. Feng bfeng@caltech.edu
Computing and Mathematical Sciences
California Institute of Technology
Katherine L. Bouman
Computing and Mathematical Sciences
California Institute of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= db2pFKVcm1
Abstract
We propose a surrogate function for efficient yet principled use of score-based priors in
Bayesian imaging. We consider ill-posed inverse imaging problems in which one aims for
a clean image posterior given incomplete or noisy measurements. Since the measurements
do not uniquely determine a true image, a prior is needed to constrain the solution space.
Recent work turned score-based diffusion models into principled priors for solving ill-posed
imaging problems by appealing to an ODE-based log-probability function. However, eval-
uating the ODE is computationally inefficient and inhibits posterior estimation of high-
dimensional images. Our proposed surrogate prior is based on the evidence lower bound of
a score-based diffusion model. We demonstrate the surrogate prior on variational inference
for efficient approximate posterior sampling of large images. Compared to the exact prior in
previous work, our surrogate accelerates optimization of the variational image distribution
by at least two orders of magnitude. We also find that our principled approach gives more
accurate posterior estimation than non-variational diffusion-based approaches that involve
hyperparameter-tuning at inference. Our work establishes a practical path forward for using
score-based diffusion models as general-purpose image priors.
1 Introduction
Ill-posed image reconstruction arises when measurements of an object of interest are incomplete or noisy,
making it impossible to uniquely determine the true image. Imaging in this setting requires a prior to
constrain images according to desired statistics. From a Bayesian perspective, the prior influences both the
uncertainty and the richness of the estimated image. Although diffusion-based generative models represent
rich image priors, leveraging these priors for Bayesian image reconstruction remains a challenge. True
posterior sampling with an unconditional diffusion model is intractable, so most previous methods either
heavily approximate the posterior (Chung et al., 2023; Jalal et al., 2021; Kawar et al., 2022; Song et al.,
2023a) or perform non-Bayesian conditional sampling (Choi et al., 2021; Chung et al., 2022a;b; Chung & Ye,
2022; Graikos et al., 2022; Song et al., 2022; Adam et al., 2022).
Recent work demonstrated how to turn score-based diffusion models into probabilistic priors ( score-based
priors) for variational Bayesian imaging (Feng et al., 2023). This method requires the exact probability of
a proposed image to be evaluated with a computationally-expensive ordinary differential equation (ODE),
requiring days to a week to reconstruct even a 32×32image. Even so, this approach is appealing because
it offers the same theoretical guarantees as traditional variational inference. This can sometimes be worth
the computational cost for imaging applications in which measurements are expensive or difficult to obtain
(e.g., black-hole imaging (EHTC, 2019), cryo-electron microscopy (Egelman, 2016; Levy et al., 2022), and
1Published in Transactions on Machine Learning Research (07/2024)
X-ray crystallography (Woolfson, 1997; Miao et al., 2008)). Still, it would be beneficial to have an efficient
“surrogate” for this approach that can be used in the development and testing stages or simply to reduce
computational costs. We present a method for variational Bayesian inference that is both principled and
computationally efficient thanks to a surrogate score-based prior.
Computing exact probabilities under a diffusion model is inefficient or even intractable, but computing the
evidence lower bound (ELBO) (Song et al., 2021a; Ho et al., 2020) is computationally efficient and feasible for
high-dimensional images. We propose to use this lower bound as a surrogate for the exact score-based prior.
In particular, we use the ELBO function as a substitute for the exact log-probability function, and it can
be plugged into any inference algorithm that requires the value or gradient of the posterior log-density. For
variational inference of an image posterior, we find at least two orders of magnitude in speedup of optimizing
the variational distribution. Furthermore, our approach reduces GPU memory requirements, as there is no
need to evaluate and backpropagate through an ODE. These efficiency improvements make it practical to
perform principled inference with score-based priors.
In this paper, we describe our variational-inference approach to estimate a posterior with a surrogate score-
based prior.1We provide experimental results to validate the proposed surrogate, including high-dimensional
posterior samples of sizes up to 256×256. In the setting of accelerated MRI, we quantify time- and memory-
efficiency improvements of the surrogate over the exact prior. We also demonstrate that our approach
achieves more accurate posterior estimation and higher-fidelity image reconstructions than diffusion-based
methods that deviate from true Bayesian inference. Finally, we demonstrate how our approach can be used
for black-hole imaging, in which images must be carefully reconstructed from sparse telescope measurements.
2 Related work
2.1 Bayesian inverse imaging
Imaging can be framed as an inverse problem in which a hidden image x∗∈RDmust be recovered from
measurements y∈RM, where
y=f(x∗) +n.
Usually the forward model f:RD→RMand statistics of the noise n∈RMare assumed to be known.
Bayesian imaging accounts for the uncertainty added by the measurement process by formulating a posterior
distribution p(x|y), whose log-density can be decomposed into a likelihood term and a prior term:
logp(x|y) = logp(y|x) + logp(x) +const. (1)
Given a log-likelihood function logp(y|x)and a prior log-probability function logp(x), we can use estab-
lished techniques for sampling from the posterior, such as Markov chain Monte Carlo (MCMC) (Brooks
et al., 2011) or variational inference (VI) (Blei et al., 2017). MCMC algorithms generate a Markov chain
whose stationary distribution is the posterior, but they are generally slow to converge for high-dimensional
data like images. VI instead approximates the posterior with a tractable distribution. The variational dis-
tribution is usually parameterized and thus can be efficiently optimized to represent high-dimensional data
distributions. Deep Probabilistic Imaging (DPI) (Sun & Bouman, 2021; Sun et al., 2022) proposed an ef-
ficient VI approach specifically for computational imaging with image regularizers. In DPI, the variational
distribution is a discrete normalizing flow (Kobyzev et al., 2020), which is an invertible generative model
capable of representing complex distributions.
The log-prior term in Eq. 1 is often defined through a regularizer. Traditional imaging approaches use
hand-crafted regularizers including total variation (TV) (Vogel & Oman, 1996), maximum entropy (Gull &
Skilling, 1984; Narayan & Nityananda, 1986), and the L1 norm (Candes & Romberg, 2007) or data-driven
regularizers such as through Gaussian mixture models (Zoran & Weiss, 2011; 2012). Various deep-learned
regularizers have been proposed, including adversarial regularizers (Lunz et al., 2018) and input-convex
neural networks (Mukherjee et al., 2020; Shumaylov et al., 2023), but they impose training or architecture
restrictions that may limit generalizability and expressiveness.
1Code is available at https://github.com/berthyf96/score_prior .
2Published in Transactions on Machine Learning Research (07/2024)
We note that this perspective of Bayesian imaging with standalone priors differs from that of other learning-
based approaches to inverse problems. Many approaches train an end-to-end neural network on a paired
dataset of measurements and ground-truth images (Pathak et al., 2016; Iizuka et al., 2017; Yu et al., 2019;
Zhang et al., 2022; 2020; Yin et al., 2021; Saharia et al., 2022; Delbracio et al., 2021; Delbracio & Milanfar,
2023). However, for every new measurement distribution, a new paired dataset would have to be obtained,
and the network would have to be re-trained. This can become cumbersome when one wishes to analyze
differentmeasurementsettingswiththesameprior(e.g., whenexploringdifferentMRIacquisitionstrategies).
It also precludes analyzing the effects of different priors (e.g., by imposing different assumptions in imaging
a black hole (EHTC, 2019)) since the implicit prior of the network changes every time it is re-trained. Our
approach, on the other hand, only requires a dataset of clean images for the prior, and the same learned
prior can be used across different inverse problems by being paired with different measurement-likelihood
functions in a modular way. Large-scale datasets of clean images exist for many applications, such as the
fastMRI (Zbontar et al., 2018) dataset for accelerated MRI. It is also usually possible to create a clean
dataset through costly acquisition of ideal images or simulation.
2.2 Diffusion models for inverse problems
Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song & Ermon, 2019; Song et al., 2019; 2021b)
learn to model a rich image distribution that could be useful as a prior for image reconstruction. A diffusion
model generates an image by starting from an image of noise and gradually denoising it until it becomes a
clean image. We discuss this process, known as reverse diffusion , in more detail in Sec. 3.1.
Given an inverse problem, simply adapting a trained diffusion model to sample from the posterior instead of
the learned prior is intractable (Feng et al., 2023). Therefore, most diffusion-based approaches do not infer
a true posterior. Some project images onto a measurement-consistent subspace (Song et al., 2022; Chung
et al., 2022b; Chung & Ye, 2022; Choi et al., 2021; Chung et al., 2022a), but the projection does not account
for measurement noise and might pull images away from the true posterior. Others follow a gradient toward
higher measurement-likelihood throughout reverse diffusion (Chung et al., 2023; Jalal et al., 2021; Graikos
et al., 2022; Kawar et al., 2022; Adam et al., 2022; Song et al., 2023a; Mardani et al., 2023), but they heavily
approximate the posterior. Overall, these diffusion-based methods require hyperparameter-tuning of the
measurement weight. As soon as hyperparameters are introduced, there is no guarantee of sampling from a
posterior that represents the true uncertainty. These diffusion-based methods are not principled enough for
scientific and medical applications that require accurate uncertainty quantification.
Score-based priors Recent work proposed an alternative perspective: turning a score-based diffusion
model into a standalone, probabilistic prior ( score-based prior ) that can be paired with any measurement-
likelihood function and plugged into established variational-inference approaches. Rigorous variational in-
ference is usually more computationally intensive than the aforementioned diffusion-based approaches, so
this type of approach is best-suited for applications in which finding the best posterior for a given set of
measurements is worth the computational overhead.
However, the approach of Feng et al. (2023) is too memory-intensive to feasibly handle large images. Even
for small images, it is too time-intensive to enable fast development. This is due to a log-density function
based on an ODE (see Sec. 3.2) that is computationally expensive. Our work proposes an efficient surrogate
that makes this type of principled approach practical for high-dimensional inference. While other efforts have
been made to speed up diffusion-model sampling, such as by reducing the number of function evaluations
(Karras et al., 2022; Zhang & Chen, 2022; Song et al., 2023b), performing latent diffusion (Rombach et al.,
2022; Avrahami et al., 2023), or learning more efficient reverse processes (Lipman et al., 2022; De Bortoli
et al., 2021), these approaches either break the tractability of image probabilities or only incrementally
improve speed. We aim to fundamentally improve efficiency without altering the diffusion model.
3Published in Transactions on Machine Learning Research (07/2024)
3 Background
In this section, we review background on score-based diffusion models with an emphasis on evaluating
probabilities of images under a trained diffusion model. We then describe how a diffusion process gives rise
to an efficient denoising-based lower bound on these image probabilities.
3.1 Score-based diffusion models
Thecoreideaofadiffusionmodelisthatittransformsasimpledistribution πtoacompleximagedistribution
through a gradual process. We follow the popular framework of denoising diffusion models, which transform
noise samples from π=N(0,I)to clean samples from the data distribution pdatathrough gradual denoising.
With knowledge of the noise distribution and denoising process, we can assess the probability of a novel
image under this generative model.
The transformation from a simple distribution to a complex one occurs over many steps. To determine how
the data distribution should look at each step of denoising, we turn to a stochastic differential equation
(SDE) that describes a diffusion process from clean images to noise (Song et al., 2021b). The diffusion SDE
is defined on the time interval t∈[0,T]as
dx=f(x,t)dt+g(t)dw, (2)
where w∈RDdenotes Brownian motion; g(t)∈Ris the diffusion coefficient, which controls the rate of noise
increase; and f(·,t) :RD→RDis the drift coefficient, which controls the deterministic evolution of x(t). By
defining a stochastic trajectory {x(t)}t∈[0,T], this SDE gives rise to a time-dependent probability distribution
pt, which is the marginal distribution of x(t). We construct f(·,t)andg(t)so that ifp0=pdata, thenpT≈π.
Image generation amounts to reversing the diffusion, which requires the gradient of the data log-density
(score) at every noise level in order to nudge images toward high probability under pdata. A convolutional
neural network sθknown as a score model learns to approximate the true score: sθ(x,t)≈∇ xlogpt(x).
Sampling with a reverse-time SDE Once trained, sθ(x,t)is used to reverse diffusion and generate
clean images from noise. This results in a distribution pSDE
θ, denoted as such because it is determined by a
reverse-time SDE (Song et al., 2021b):
dx=/bracketleftbig
f(x,t)−g(t)2sθ(x,t)/bracketrightbig
dt+g(t)d¯ w, (3)
where ¯ w∈RDdenotes Brownian motion, and f(·,t)andg(t)are the same as in Eq. 2. To generate an
image, we first sample x(T)∼N (0,I)and then numerically solve the reverse-time SDE for x(0). The
marginal distribution of x(0)is denoted by pSDE
θ, which is close to pdatawhen the score model is well-
trained. Intuitively, Eq. 3 explains how to denoise an image x(t)using the score model sθin order to get
closer to the clean image distribution. Indeed, Eq. 3 is exactly the reverse of the forward-time SDE (Eq. 2)
(Anderson, 1982) when replacing ∇xlogpt(x)withsθ(x,t), which means that removing noise corresponds
to reaching higher probability under the data distribution.
3.2 Image probabilities
The generated image distribution pSDE
θtheoretically assigns density to any image. However, reverse diffusion
does not lead to an image distribution with tractable probabilities. In this subsection, we describe two
workarounds: one based on an ODE and the other based on an ELBO related to denoising score-matching.
To compute the probability of an image xunderpSDE
θ, we need to invert it from x(0) = xtox(T). However,
this is not tractable through the SDE: just as it is intractable to reverse a random walk, it is intractable
to account for all the possible starting points x(T)that could have resulted in x(0)through the stochastic
process. Probability computation calls for an invertible process that lets us map any point from pdatato a
point fromN(0,I)and vice versa.
4Published in Transactions on Machine Learning Research (07/2024)
Probability flow ODE Theprobability flow ODE (Song et al., 2021b) defines an invertible sampling
function for a distribution pODE
θtheoretically the same as pSDE
θ. It is given by
dx
dt=f(x,t)−1
2g(t)2sθ(x,t) =:˜fθ(x,t). (4)
The absence of Brownian motion makes it possible to solve this ODE in both directions of time. To compute
the log-probability of an image x, we map x(0) = xto its corresponding noise image x(T). Under the
framework of neural ODEs (Chen et al., 2018), the log-probability is given by the log-probability of x(T)
underN(0,I)plus a normalizing factor accounting for the change in density through time:
logpODE
θ(x(0)) = logπ(x(T)) +/integraldisplayT
0∇·˜fθ(x(t),t)dt (5)
forx(0) = x. Although tractable to evaluate with an ODE solver, this log-probability function is compu-
tationally expensive, requiring hundreds to thousands of discrete ODE time steps to accurately evaluate.
Additional time and memory costs are incurred by backpropagation through the ODE and Hutchinson-
Skilling trace estimation of the divergence.
Equivalence of pSDE
θandpODE
θSong et al. (2021a) proved that if sθ(x,t)≡ ∇ xlogpt(x,t)for all
t∈[0,T]andpT=π, thenpODE
θ =pSDE
θ=pdata. In our work, we assume that sθ(x,t)≈∇ xlogpt(x,t)for
almost all x∈RDandt∈[0,T]and thatpT≈N (0,I), so thatpODE
θ≈pSDE
θ≈pdata. This assumption
empirically performed well in previous work that appealed to pODE
θas the exact probability distribution of
the diffusion model (Feng et al., 2023; Song et al., 2021b).
3.2.1 Evidence lower bound
In lieu of an exact log-probability function, Song et al. (2021a) derived an evidence lower bound bSDE
θfor
pSDE
θsuch thatbSDE
θ(x)≤logpSDE
θ(x)for any proposed image x. Essentially, this lower bound corresponds
to how well the diffusion model is able to denoise a given image: an image with high probability under the
diffusion model is easy to denoise, whereas a low-probability image is difficult.
The lower bound, or the negative “denoising score-matching loss” (Song et al., 2021a), is defined as
bSDE
θ(x) :=Ep0T(x′|x)[logπ(x′)]−1
2/integraldisplayT
0g(t)2h(t)dt, (6)
where
h(t) :=Ep0t(x′|x)/bracketleftbigg
∥sθ(x′,t)−∇ x′logp0t(x′|x)∥2
2−∥∇ x′logp0t(x′|x)∥2
2−2
g(t)2∇x′·f(x′,t)/bracketrightbigg
.(7)
p0t(x′|x)denotes the transition distribution from x(0) = xtox(t) =x′. For a drift coefficient that is
linear in x, this transition distribution is Gaussian: p0t(x′|x) =N(x′;α(t)x,β(t)2I). This means that the
gradient∇x′logp0t(x′|x)is directly proportional to the Gaussian noise that is subtracted from x′to get x.
Eq. 6 is efficient to compute since we can evaluate it by adding Gaussian noise to xwithout having to solve
an initial-value problem as with the ODE. In fact, Eq. 6 is closely related to the denoising score-matching
objective used to train diffusion models (Song et al., 2021b).
Intuitively, wecaninterpretEq.6asassociatinganimage’sprobabilitywithhowwellthescoremodel sθcould
denoisethatimageifitunderwentdiffusion. Thisisrepresentedbythefirsttermin h(t)(Eq.7). Toassessthe
probabilityofanimage x, weperturbitwithGaussiannoisetoget x′andthenaskthescoremodeltoestimate
the noise that was added. If sθ(x,t)accurately estimates the noise, then ∥sθ(x′,t)−∇ x′logp0t(x′|x)∥2
2
is small, and the value of bSDE
θ(x)becomes larger. The remaining terms in h(t)are normalizing factors
independent of θ, andEp0T(x′|x)[logπ(x′)]accounts for the probabilities of the noise images x(T)that could
result from xbeing entirely diffused.
5Published in Transactions on Machine Learning Research (07/2024)
4 Method
Inspired by previous theoretical work (Song et al., 2021a), we apply bSDE
θas an efficient surrogate for the
exact score-based prior in Bayesian imaging. In this section, we describe our approach for efficient inference
of an approximate posterior given a score-based prior.
4.1 Approximating the posterior with VI
Given measurements y∈RM(with a known log-likelihood function) and a score-based diffusion model with
parameters θas the prior, our goal is to sample from the image posterior pθ(x|y). Following VI, we optimize
the parameters of a variational distribution to approximate pθ(x|y).
Letqϕdenote the variational distribution with parameters ϕ. We would like to optimize ϕto minimize the
KL divergence from qϕto the target posterior:
ϕ∗= arg min
ϕDKL(qϕ∥pθ(·|y)) (8)
= arg min
ϕEx∼qϕ/bracketleftbig
−logp(y|x)−logpODE
θ(x) + logqϕ(x)/bracketrightbig
. (9)
qϕcan be any parameterized tractable distribution. It could be a Gaussian distribution with a diagonal
covariance so that ϕ:= [µ⊤,σ⊤]⊤, whereµ∈RDandσ∈RD(σ>0)are the mean and pixel-wise standard
deviation.qϕcould also be a RealNVP normalizing flow as it is in DPI (Sun & Bouman, 2021).
To circumvent the computational challenges of evaluating the prior term logpODE
θ(x), we replace it with the
surrogatebSDE
θ(x). This results in the following objective:
ϕ∗= arg min
ϕEx∼qϕ/bracketleftbig
−logp(y|x)−bSDE
θ(x) + logqϕ(x)/bracketrightbig
. (10)
We can also think of bSDE
θas replacing the intractable logpSDE
θin Eq. 8. Since−logpSDE
θ≤−bSDE
θ,our
surrogate objective minimizes the upper bound of the KL divergence involving pSDE
θ.
4.2 Implementation details
Evaluating bSDE
θ(x)The formula for bSDE
θ(x)(Eq. 6) contains a time integral and expectation over p0t(x′|
x)thatcanbeestimatedwithnumericalmethods. FollowingSongetal.(2021a), weuseimportancesampling
with time samples t∼p(t)for the time integral and Monte-Carlo approximation with noisy images x′∼
N(α(t)x,β(t)2I)for the expectation. The proposal distribution p(t) :=g(t)2
β(t)2Zwas empirically verified to
result in lower variance in the estimation of bSDE
θ(x)(Song et al., 2021a). We provide the following formula
used in our implementation, which estimates the time integral with importance sampling and the expectation
with Monte-Carlo approximation, for reference:
bSDE
θ(x)≈1
NzNz/summationdisplay
j=1logπ/parenleftbig
x′
j/parenrightbig
−1
2NtNzNt/summationdisplay
i=1Zβ(t)2Nz/summationdisplay
j=1/bracketleftbigg/vextenddouble/vextenddouble/vextenddoublesθ(x′
ij,ti) +zij
β(ti)/vextenddouble/vextenddouble/vextenddouble2
2−/vextenddouble/vextenddouble/vextenddoublezij
β(ti)/vextenddouble/vextenddouble/vextenddouble2
2−2
g(ti)2∇x′
ij·f(x′
ij,ti)/bracketrightbigg
(11)
s.t.ti∼p(t),zij∼N(0,I),x′
ij=α(ti)x+β(ti)zij,
x′
j∼N(α(T)x,β(T)2I)∀i= 1,...,Nt,j= 1,...,N z.
Nttime samples and Nznoise samples are taken to approximate the time integral and expectation over
p0t(x′|x), respectively. In our experiments, Nt=Nz= 1. We find that increasing the number of time and
noise samples does not efficiently decrease variance in the estimated value of bSDE
θ(x).
Optimization We use stochastic gradient descent to optimize ϕ, Monte-Carlo-approximating the expecta-
tion in Eq. 10 with a batch of x∼qϕ. Estimating bSDE
θ(x)has higher variance than estimating logpODE
θ(x).
For instance, in Fig. 4, bSDE
θ(x)withNt= 2048,Nz= 1shows higher variance than logpODE
θ(x)with 16 trace
estimators. A lower learning rate can help mitigate training instabilities caused by variance. For example,
in Fig. 3b the learning rate with the exact prior was 0.0002, while the learning rate with the surrogate prior
was0.00001. Please refer to Appendix B for more optimization details.
6Published in Transactions on Machine Learning Research (07/2024)
Dimensionality
Memory [GB]DimensionalityOptimization TimeOptimization Memory
PSNRSSIMImage-Restoration Quality
Dimensionality39 hrs.43 hrs.126 hrs.19 mins.19 mins.45 mins.2 hrs.9 hrs.16 GB31 GB89 GB8 GB12 GB17 GB44 GB142 GBTime [hr]= Exact= Surrogate= Exact= Surrogate
Figure 1: Computational efficiency of proposed surrogate prior vs. exact prior. For each image size, we
estimated a posterior of images conditioned on 4×-accelerated MRI measurements of a knee image, using a
Gaussiandistributionwithdiagonalcovarianceasthevariationaldistribution. Thehardwarewas4xNVIDIA
RTX A6000. The surrogate prior allows for variational inference of image sizes that are prohibitively large
for the exact prior. For image sizes supported by the exact prior, the surrogate improved total optimization
time by over 120×while using less memory and scaling better with image size. “Image-Restoration Quality”
verifies that optimization with the surrogate was done fairly, as the PSNR and SSIM of the converged
posterior (averaged over 128 samples) are at least as high as with the exact prior.
5 Experiments
We validate our approach on the tasks of accelerated MRI, denoising, reconstruction from low spatial fre-
quencies, and black-hole interferometric imaging. See Appendix A for details about the forward models.
5.1 Efficiency improvements
Image size Surrogate Exact
16×16 0.029 19.5
32×32 0.038 41.9
64×64 0.090 123
128×128 0.294 N/A
256×256 1.115 N/A
Table 1: Iteration time [sec/step].
A step of gradient-based optimiza-
tion of the variational distribution
istwotothreeordersofmagnitude
faster with the surrogate prior.In Tab. 1 and Fig. 1, we quantify the efficiency improvements of the
surrogate prior for an accelerated MRI task at different image resolu-
tions. We drew a test image from the fastMRI knee dataset (Zbontar
et al., 2018) and resized it to 16×16,32×32,64×64,128×128, and
256×256. For each image size, we trained a score model on images
of the corresponding size from the fastMRI training set of single-coil
knee scans. We then optimized a Gaussian distribution with diagonal
covariance to approximate the posterior. The batch size was 64for the
surrogate and 32for the exact prior (a smaller batch size was needed
to fit 64×64optimization into GPU memory). Convergence was de-
fined via a minimum acceptable change in the estimated posterior mean
between optimization steps.
We find at least two orders of magnitude in time improvement with the
surrogate prior. Tab. 1 compares the iteration time between the two priors. Fig. 1 compares the total
time it takes to optimize the variational distribution. The surrogate also improves memory consumption,
which in turn enables optimizing higher-dimensional posteriors. Following standard practice, we just-in-time
(JIT) compile the optimization step to reduce time/step at the cost of GPU memory. Fig. 1 shows how
the surrogate prior significantly reduces memory requirements and scales better with image size. The exact
prior could only handle up to 64×64before exceeding GPU memory (we tested on 4x 48GB GPUs). While
memory could be reduced with a smaller batch size, this would make optimization more time-consuming. On
the other hand, our surrogate prior supports much larger images, as we demonstrate in Fig. 2 for 256×2562
MRI with a Gaussian-approximated posterior. This type of principled inference of high-dimensional image
posteriors was not possible before with the exact score-based prior.
2Larger images may be feasible, but their larger memory footprint might restrict the possible batch size and complexity of
the variational distribution.
7Published in Transactions on Machine Learning Research (07/2024)
ObservedPosterior SamplesOriginal16x-accel.MRI256x2564x-accel.MRI256x256<latexit sha1_base64="7fz4lbnt1FWpDNg3QS24RMrBU0Y=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoseiF48VbC22oWy2m3bpZhN2X4QS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrHuBNRwKRRvoUDJO4nmNAokfwjGN7n/8MS1EbG6x0nC/YgOlQgFo2ilx15EcRSE2WTar9bcujsDWSZeQWpQoNmvfvUGMUsjrpBJakzXcxP0M6pRMMmnlV5qeELZmA5511JFI278bJZ4Sk6sMiBhrO1TSGbq742MRsZMosBO5gnNopeL/3ndFMMrPxMqSZErNv8oTCXBmOTnk4HQnKGcWEKZFjYrYSOqKUNbUsWW4C2evEzaZ3Xvou7endca10UdZTiCYzgFDy6hAbfQhBYwUPAMr/DmGOfFeXc+5qMlp9g5hD9wPn8AAB+RIQ==</latexit>y<latexit sha1_base64="68Hba4evaXWTPZttK/t7lB5R0WI=">AAAB83icbVDLSgMxFL3js9ZX1aWbYBHERZkRRZdFNy4r2Ad0xpJJM21oJhOSjFiG/oYbF4q49Wfc+Tdm2llo64HA4Zx7uScnlJxp47rfztLyyuraemmjvLm1vbNb2dtv6SRVhDZJwhPVCbGmnAnaNMxw2pGK4jjktB2ObnK//UiVZom4N2NJgxgPBIsYwcZKvh9jMwyj7GnycNqrVN2aOwVaJF5BqlCg0at8+f2EpDEVhnCsdddzpQkyrAwjnE7KfqqpxGSEB7RrqcAx1UE2zTxBx1bpoyhR9gmDpurvjQzHWo/j0E7mGfW8l4v/ed3URFdBxoRMDRVkdihKOTIJygtAfaYoMXxsCSaK2ayIDLHCxNiayrYEb/7Li6R1VvMuau7debV+XdRRgkM4ghPw4BLqcAsNaAIBCc/wCm9O6rw4787HbHTJKXYO4A+czx8fkZG8</latexit>x⇤
<latexit sha1_base64="KHEYI2SIpi8t7LhPQSObPQ91CLA=">AAACE3icbVDLSsNAFJ3UV62vqEs3g0WoLkoiii6LblxWsA9oQplMJu3QmSTMTMQQ8g9u/BU3LhRx68adf+O0jVBbDwycOede7r3HixmVyrK+jdLS8srqWnm9srG5tb1j7u61ZZQITFo4YpHoekgSRkPSUlQx0o0FQdxjpOONrsd+554ISaPwTqUxcTkahDSgGCkt9c0ThyM19ILsIXck5TCuzQic+r+/ND/um1Wrbk0AF4ldkCoo0OybX44f4YSTUGGGpOzZVqzcDAlFMSN5xUkkiREeoQHpaRoiTqSbTW7K4ZFWfBhEQr9QwYk625EhLmXKPV05XlHOe2PxP6+XqODSzWgYJ4qEeDooSBhUERwHBH0qCFYs1QRhQfWuEA+RQFjpGCs6BHv+5EXSPq3b53Xr9qzauCriKIMDcAhqwAYXoAFuQBO0AAaP4Bm8gjfjyXgx3o2PaWnJKHr2wR8Ynz98d58q</latexit>x⇠p(x|y)
Figure 2: High-dimensional Bayesian inference with a surrogate score-based prior. Here we show posterior
samplesforacceleratedMRIof 256×256kneeimages, approximatedviavariationalinferencewithasurrogate
score-based prior. The first row shows reconstruction from 16×-reduced MRI measurements. The second
row shows reconstruction given more κ-space measurements, i.e., 4×-reduced MRI. Bayesian imaging at this
image resolution is computationally infeasible with the previous ODE-based approach (Feng et al., 2023).
Our proposed surrogate enables efficient yet principled inference with diffusion-model priors, resulting in
inferred posteriors where the true image is within three standard deviations of the posterior mean for 96%
and 99% of the pixels for 16×- and 4×-acceleration, respectively.
-0.35MeanStd. Dev.TrueEst.(Surrogate)
Est.(Exact)0.89
0.100.16
(a) Ground-truth (Gaussian) posterior.
SurrogateExactMeanStd. Dev.OriginalObservedOriginalObservedMeanStd. Dev.0.01
0.120.010.1(i) CelebAdenoising(ii) CIFAR-10 denoising (b) Complex posteriors.
Figure 3: Estimated posteriors under surrogate vs. exact prior. For each task, the variational distribution
is a RealNVP, and the score model is the same between both prior functions. (a)Both prior functions
help recover the correct (Gaussian) posterior. The score-based prior was trained on samples from a known
Gaussian distribution (originally fit to 16×16face images), and the measurements are the lowest 6.25%
spatial frequencies of a test image from the prior. Since the prior and likelihood are both Gaussian, we know
the ground-truth Gaussian posterior. (b)Estimated posteriors for (i) denoising a CelebA image and (ii)
denoising a CIFAR-10 image. Std. dev. is averaged across the three color channels. The score-based prior
was trained on CelebA in (i) and CIFAR-10 in (ii). Both prior functions result in comparable image quality;
visual differences appear mostly in the image background.
5.2 Posterior estimation under surrogate vs. exact
The surrogate prior bSDE
θmay not be an identical substitute for the exact prior logpODE
θ. Importantly,
though, we verify in Fig. 3a that both the surrogate and exact prior recover a ground-truth Gaussian
posterior derived from a Gaussian likelihood and prior. The variational distribution is a RealNVP. The score
model (used by both the surrogate and exact prior) was trained on samples from the known prior.
8Published in Transactions on Machine Learning Research (07/2024)
Nonetheless, the surrogate could result in a different locally-optimal variational posterior, particularly if
the posterior leads to many local minima in the variational objective. Fig. 3b compares posteriors (with
unknown true distributions) approximated by a RealNVP under the surrogate versus exact prior. For each
task (CelebA denoising and CIFAR-10 denoising), both prior functions used the same trained score model.
We observe in these comparisons that most of the differences appear in the image background and that both
priors result in a plausible mean reconstruction and uncertainty.
Visualizing the bound gap throughout optimization helps shed light on why the two priors converge
to different solutions even with the same underlying score model. Fig. 4 shows probabilities of samples
generated by qϕ(in this case a RealNVP) as optimization progresses. At each checkpoint of qϕ, we plot
bSDE
θ(x)versus logpODE
θ(x)for samples x∼qϕcoming from both the exact and surrogate optimization
ofqϕ. We find that the surrogate is a valid bound for the ODE log-density: bSDE
θ(x)≤logpODE
θ(x)for
allx∼qϕ, except for some outliers due to variance of bSDE
θ(x). However, optimization follows a different
trajectory depending on the prior. With the surrogate, samples x∼qϕtend toward a region where the bound
gap is small. Meanwhile, the exact prior follows a loss landscape whose structure appears to be independent
of the lower bound. Note that samples from qϕoptimized under the exact prior obtain higher values of
bSDE
θ(x)than samples obtained under the surrogate. Fig. 4 suggests that gradients under the surrogate tend
to push the qϕdistribution along the boundary of equality between bSDE
θandlogpODE
θ. This constrains the
path taken through gradient descent and subsequently the converged solution.
ExactSurrogateStep 20000
Step 1000000Step 500Step 100
Step 5000Step 25000
Step 100Step 500Step 20K
Step 5KStep 25KStep 1MPlotting trajectories on same scale<latexit sha1_base64="zVurDO9TRSko1A7q3GoGLQ970UQ=">AAACA3icbVDJSgNBEO2JW4zbqDe9NAbBU5gRRY/BBbwZwSyQiaGnU0ma9Cx014hhCHjxV7x4UMSrP+HNv7GzHDTxQcHjvSqq6vmxFBod59vKzM0vLC5ll3Mrq2vrG/bmVkVHieJQ5pGMVM1nGqQIoYwCJdRiBSzwJVT93vnQr96D0iIKb7EfQyNgnVC0BWdopKa948moQ+Omh11Adpd6CA+YXl9cDgZNO+8UnBHoLHEnJE8mKDXtL68V8SSAELlkWtddJ8ZGyhQKLmGQ8xINMeM91oG6oSELQDfS0Q8Dum+UFm1HylSIdKT+nkhZoHU/8E1nwLCrp72h+J9XT7B92khFGCcIIR8vaieSYkSHgdCWUMBR9g1hXAlzK+VdphhHE1vOhOBOvzxLKocF97jg3Bzli2eTOLJkl+yRA+KSE1IkV6REyoSTR/JMXsmb9WS9WO/Wx7g1Y01mtskfWJ8/WMiX+Q==</latexit>logpODE✓(               )<latexit sha1_base64="xW93/YyrfIlA8iFfqMqPfnccWEI=">AAAB/HicbVDJSgNBEO2JW4zbaI5eBoPgKcyIosfgAh4jmgUy49DTqUma9Cx014hhiL/ixYMiXv0Qb/6NneWg0QcFj/eqqKoXpIIrtO0vo7CwuLS8Ulwtra1vbG6Z2ztNlWSSQYMlIpHtgCoQPIYGchTQTiXQKBDQCgbnY791D1LxJL7FYQpeRHsxDzmjqCXfLAe+i31AeuciPGB+c3E58s2KXbUnsP4SZ0YqZIa6b3663YRlEcTIBFWq49gpejmVyJmAUcnNFKSUDWgPOprGNALl5ZPjR9a+VrpWmEhdMVoT9edETiOlhlGgOyOKfTXvjcX/vE6G4amX8zjNEGI2XRRmwsLEGidhdbkEhmKoCWWS61st1qeSMtR5lXQIzvzLf0nzsOocV+3ro0rtbBZHkeySPXJAHHJCauSK1EmDMDIkT+SFvBqPxrPxZrxPWwvGbKZMfsH4+AbvPJTz</latexit>bSDE✓(         )<latexit sha1_base64="oPUrUeiPUAiQl2O3PR65FmlFHoU=">AAACDXicbVDJSgNBEO1xN26jHr00RiFewowoegwu4M0IxgQyMfR0apImPQvdNWIY8gNe/BUvHhTx6t2bf2NnOajxQcHjvSqq6vmJFBod58uamp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBBUUKKGWKGChL6Hqd08HfvUOlBZxdI29BBoha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDDt+kN3395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rrtOgo2MKRRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUUg+NGJqIkRYj4aFGQSooxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>logpODE✓(x)<latexit sha1_base64="4UWrp2slWB+4HXCtY4nwexig0mE=">AAACCHicbVDJSgNBEO1xjXEb9ejBwSDES5gRRY/BBTxGNAtkYujp1CRNeha6ayRhyNGLv+LFgyJe/QRv/o2d5aCJDwoe71VRVc+LBVdo29/G3PzC4tJyZiW7ura+sWlubVdUlEgGZRaJSNY8qkDwEMrIUUAtlkADT0DV614M/eoDSMWj8A77MTQC2g65zxlFLTXNPa/pYgeQ3rsIPUxvL68GeTeg2PH8tDc4bJo5u2CPYM0SZ0JyZIJS0/xyWxFLAgiRCapU3bFjbKRUImcCBlk3URBT1qVtqGsa0gBUIx09MrAOtNKy/EjqCtEaqb8nUhoo1Q883Tk8UU17Q/E/r56gf9ZIeRgnCCEbL/ITYWFkDVOxWlwCQ9HXhDLJ9a0W61BJGerssjoEZ/rlWVI5KjgnBfvmOFc8n8SRIbtkn+SJQ05JkVyTEikTRh7JM3klb8aT8WK8Gx/j1jljMrND/sD4/AEJV5n6</latexit>bSDE✓(x)<latexit sha1_base64="oPUrUeiPUAiQl2O3PR65FmlFHoU=">AAACDXicbVDJSgNBEO1xN26jHr00RiFewowoegwu4M0IxgQyMfR0apImPQvdNWIY8gNe/BUvHhTx6t2bf2NnOajxQcHjvSqq6vmJFBod58uamp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBBUUKKGWKGChL6Hqd08HfvUOlBZxdI29BBoha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDDt+kN3395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rrtOgo2MKRRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUUg+NGJqIkRYj4aFGQSooxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>logpODE✓(x)<latexit sha1_base64="oPUrUeiPUAiQl2O3PR65FmlFHoU=">AAACDXicbVDJSgNBEO1xN26jHr00RiFewowoegwu4M0IxgQyMfR0apImPQvdNWIY8gNe/BUvHhTx6t2bf2NnOajxQcHjvSqq6vmJFBod58uamp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBBUUKKGWKGChL6Hqd08HfvUOlBZxdI29BBoha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDDt+kN3395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rrtOgo2MKRRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUUg+NGJqIkRYj4aFGQSooxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>logpODE✓(x)<latexit sha1_base64="4UWrp2slWB+4HXCtY4nwexig0mE=">AAACCHicbVDJSgNBEO1xjXEb9ejBwSDES5gRRY/BBTxGNAtkYujp1CRNeha6ayRhyNGLv+LFgyJe/QRv/o2d5aCJDwoe71VRVc+LBVdo29/G3PzC4tJyZiW7ura+sWlubVdUlEgGZRaJSNY8qkDwEMrIUUAtlkADT0DV614M/eoDSMWj8A77MTQC2g65zxlFLTXNPa/pYgeQ3rsIPUxvL68GeTeg2PH8tDc4bJo5u2CPYM0SZ0JyZIJS0/xyWxFLAgiRCapU3bFjbKRUImcCBlk3URBT1qVtqGsa0gBUIx09MrAOtNKy/EjqCtEaqb8nUhoo1Q883Tk8UU17Q/E/r56gf9ZIeRgnCCEbL/ITYWFkDVOxWlwCQ9HXhDLJ9a0W61BJGerssjoEZ/rlWVI5KjgnBfvmOFc8n8SRIbtkn+SJQ05JkVyTEikTRh7JM3klb8aT8WK8Gx/j1jljMrND/sD4/AEJV5n6</latexit>bSDE✓(x)
<latexit sha1_base64="4UWrp2slWB+4HXCtY4nwexig0mE=">AAACCHicbVDJSgNBEO1xjXEb9ejBwSDES5gRRY/BBTxGNAtkYujp1CRNeha6ayRhyNGLv+LFgyJe/QRv/o2d5aCJDwoe71VRVc+LBVdo29/G3PzC4tJyZiW7ura+sWlubVdUlEgGZRaJSNY8qkDwEMrIUUAtlkADT0DV614M/eoDSMWj8A77MTQC2g65zxlFLTXNPa/pYgeQ3rsIPUxvL68GeTeg2PH8tDc4bJo5u2CPYM0SZ0JyZIJS0/xyWxFLAgiRCapU3bFjbKRUImcCBlk3URBT1qVtqGsa0gBUIx09MrAOtNKy/EjqCtEaqb8nUhoo1Q883Tk8UU17Q/E/r56gf9ZIeRgnCCEbL/ITYWFkDVOxWlwCQ9HXhDLJ9a0W61BJGerssjoEZ/rlWVI5KjgnBfvmOFc8n8SRIbtkn+SJQ05JkVyTEikTRh7JM3klb8aT8WK8Gx/j1jljMrND/sD4/AEJV5n6</latexit>bSDE✓(x)<latexit sha1_base64="oPUrUeiPUAiQl2O3PR65FmlFHoU=">AAACDXicbVDJSgNBEO1xN26jHr00RiFewowoegwu4M0IxgQyMfR0apImPQvdNWIY8gNe/BUvHhTx6t2bf2NnOajxQcHjvSqq6vmJFBod58uamp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBBUUKKGWKGChL6Hqd08HfvUOlBZxdI29BBoha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDDt+kN3395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rrtOgo2MKRRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUUg+NGJqIkRYj4aFGQSooxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>logpODE✓(x)
<latexit sha1_base64="oPUrUeiPUAiQl2O3PR65FmlFHoU=">AAACDXicbVDJSgNBEO1xN26jHr00RiFewowoegwu4M0IxgQyMfR0apImPQvdNWIY8gNe/BUvHhTx6t2bf2NnOajxQcHjvSqq6vmJFBod58uamp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBBUUKKGWKGChL6Hqd08HfvUOlBZxdI29BBoha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDDt+kN3395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rrtOgo2MKRRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUUg+NGJqIkRYj4aFGQSooxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>logpODE✓(x)
<latexit sha1_base64="oPUrUeiPUAiQl2O3PR65FmlFHoU=">AAACDXicbVDJSgNBEO1xN26jHr00RiFewowoegwu4M0IxgQyMfR0apImPQvdNWIY8gNe/BUvHhTx6t2bf2NnOajxQcHjvSqq6vmJFBod58uamp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBBUUKKGWKGChL6Hqd08HfvUOlBZxdI29BBoha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDDt+kN3395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rrtOgo2MKRRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUUg+NGJqIkRYj4aFGQSooxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>logpODE✓(x)<latexit sha1_base64="4UWrp2slWB+4HXCtY4nwexig0mE=">AAACCHicbVDJSgNBEO1xjXEb9ejBwSDES5gRRY/BBTxGNAtkYujp1CRNeha6ayRhyNGLv+LFgyJe/QRv/o2d5aCJDwoe71VRVc+LBVdo29/G3PzC4tJyZiW7ura+sWlubVdUlEgGZRaJSNY8qkDwEMrIUUAtlkADT0DV614M/eoDSMWj8A77MTQC2g65zxlFLTXNPa/pYgeQ3rsIPUxvL68GeTeg2PH8tDc4bJo5u2CPYM0SZ0JyZIJS0/xyWxFLAgiRCapU3bFjbKRUImcCBlk3URBT1qVtqGsa0gBUIx09MrAOtNKy/EjqCtEaqb8nUhoo1Q883Tk8UU17Q/E/r56gf9ZIeRgnCCEbL/ITYWFkDVOxWlwCQ9HXhDLJ9a0W61BJGerssjoEZ/rlWVI5KjgnBfvmOFc8n8SRIbtkn+SJQ05JkVyTEikTRh7JM3klb8aT8WK8Gx/j1jljMrND/sD4/AEJV5n6</latexit>bSDE✓(x)
<latexit sha1_base64="4UWrp2slWB+4HXCtY4nwexig0mE=">AAACCHicbVDJSgNBEO1xjXEb9ejBwSDES5gRRY/BBTxGNAtkYujp1CRNeha6ayRhyNGLv+LFgyJe/QRv/o2d5aCJDwoe71VRVc+LBVdo29/G3PzC4tJyZiW7ura+sWlubVdUlEgGZRaJSNY8qkDwEMrIUUAtlkADT0DV614M/eoDSMWj8A77MTQC2g65zxlFLTXNPa/pYgeQ3rsIPUxvL68GeTeg2PH8tDc4bJo5u2CPYM0SZ0JyZIJS0/xyWxFLAgiRCapU3bFjbKRUImcCBlk3URBT1qVtqGsa0gBUIx09MrAOtNKy/EjqCtEaqb8nUhoo1Q883Tk8UU17Q/E/r56gf9ZIeRgnCCEbL/ITYWFkDVOxWlwCQ9HXhDLJ9a0W61BJGerssjoEZ/rlWVI5KjgnBfvmOFc8n8SRIbtkn+SJQ05JkVyTEikTRh7JM3klb8aT8WK8Gx/j1jljMrND/sD4/AEJV5n6</latexit>bSDE✓(x)
<latexit sha1_base64="4UWrp2slWB+4HXCtY4nwexig0mE=">AAACCHicbVDJSgNBEO1xjXEb9ejBwSDES5gRRY/BBTxGNAtkYujp1CRNeha6ayRhyNGLv+LFgyJe/QRv/o2d5aCJDwoe71VRVc+LBVdo29/G3PzC4tJyZiW7ura+sWlubVdUlEgGZRaJSNY8qkDwEMrIUUAtlkADT0DV614M/eoDSMWj8A77MTQC2g65zxlFLTXNPa/pYgeQ3rsIPUxvL68GeTeg2PH8tDc4bJo5u2CPYM0SZ0JyZIJS0/xyWxFLAgiRCapU3bFjbKRUImcCBlk3URBT1qVtqGsa0gBUIx09MrAOtNKy/EjqCtEaqb8nUhoo1Q883Tk8UU17Q/E/r56gf9ZIeRgnCCEbL/ITYWFkDVOxWlwCQ9HXhDLJ9a0W61BJGerssjoEZ/rlWVI5KjgnBfvmOFc8n8SRIbtkn+SJQ05JkVyTEikTRh7JM3klb8aT8WK8Gx/j1jljMrND/sD4/AEJV5n6</latexit>bSDE✓(x)
<latexit sha1_base64="4UWrp2slWB+4HXCtY4nwexig0mE=">AAACCHicbVDJSgNBEO1xjXEb9ejBwSDES5gRRY/BBTxGNAtkYujp1CRNeha6ayRhyNGLv+LFgyJe/QRv/o2d5aCJDwoe71VRVc+LBVdo29/G3PzC4tJyZiW7ura+sWlubVdUlEgGZRaJSNY8qkDwEMrIUUAtlkADT0DV614M/eoDSMWj8A77MTQC2g65zxlFLTXNPa/pYgeQ3rsIPUxvL68GeTeg2PH8tDc4bJo5u2CPYM0SZ0JyZIJS0/xyWxFLAgiRCapU3bFjbKRUImcCBlk3URBT1qVtqGsa0gBUIx09MrAOtNKy/EjqCtEaqb8nUhoo1Q883Tk8UU17Q/E/r56gf9ZIeRgnCCEbL/ITYWFkDVOxWlwCQ9HXhDLJ9a0W61BJGerssjoEZ/rlWVI5KjgnBfvmOFc8n8SRIbtkn+SJQ05JkVyTEikTRh7JM3klb8aT8WK8Gx/j1jljMrND/sD4/AEJV5n6</latexit>bSDE✓(x)<latexit sha1_base64="4UWrp2slWB+4HXCtY4nwexig0mE=">AAACCHicbVDJSgNBEO1xjXEb9ejBwSDES5gRRY/BBTxGNAtkYujp1CRNeha6ayRhyNGLv+LFgyJe/QRv/o2d5aCJDwoe71VRVc+LBVdo29/G3PzC4tJyZiW7ura+sWlubVdUlEgGZRaJSNY8qkDwEMrIUUAtlkADT0DV614M/eoDSMWj8A77MTQC2g65zxlFLTXNPa/pYgeQ3rsIPUxvL68GeTeg2PH8tDc4bJo5u2CPYM0SZ0JyZIJS0/xyWxFLAgiRCapU3bFjbKRUImcCBlk3URBT1qVtqGsa0gBUIx09MrAOtNKy/EjqCtEaqb8nUhoo1Q883Tk8UU17Q/E/r56gf9ZIeRgnCCEbL/ITYWFkDVOxWlwCQ9HXhDLJ9a0W61BJGerssjoEZ/rlWVI5KjgnBfvmOFc8n8SRIbtkn+SJQ05JkVyTEikTRh7JM3klb8aT8WK8Gx/j1jljMrND/sD4/AEJV5n6</latexit>bSDE✓(x)<latexit sha1_base64="oPUrUeiPUAiQl2O3PR65FmlFHoU=">AAACDXicbVDJSgNBEO1xN26jHr00RiFewowoegwu4M0IxgQyMfR0apImPQvdNWIY8gNe/BUvHhTx6t2bf2NnOajxQcHjvSqq6vmJFBod58uamp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBBUUKKGWKGChL6Hqd08HfvUOlBZxdI29BBoha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDDt+kN3395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rrtOgo2MKRRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUUg+NGJqIkRYj4aFGQSooxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>logpODE✓(x)
<latexit sha1_base64="oPUrUeiPUAiQl2O3PR65FmlFHoU=">AAACDXicbVDJSgNBEO1xN26jHr00RiFewowoegwu4M0IxgQyMfR0apImPQvdNWIY8gNe/BUvHhTx6t2bf2NnOajxQcHjvSqq6vmJFBod58uamp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBBUUKKGWKGChL6Hqd08HfvUOlBZxdI29BBoha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDDt+kN3395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rrtOgo2MKRRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUUg+NGJqIkRYj4aFGQSooxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>logpODE✓(x)
Figure 4:bSDE
θ(x)vs.logpODE
θ(x)for samples x∼qϕas optimization of ϕprogresses. The task is from
Fig. 3b(i). For each plot, we took 128 samples x∼qϕand performed 20 estimates each of logpODE
θ(x)
andbSDE
θ(x)(approximated with Nt= 2048for reduced variance). The density map is a KDE plot of all
128·20 = 2560 values; the 128 scatter points represent the mean estimate for each x. The black line indicates
perfect agreement between bSDE
θ(x)andlogpODE
θ(x). We expect all points to lie below this black line for
bSDE
θto be a lower bound. We find that bSDE
θ(x)≤logpODE
θ(x)(up to variance error), but the optimization
progresses differently depending on the prior. Gradients under the surrogate push qϕ(x)along the black line
to increasebSDE
θ(x)without exceeding logpODE
θ(x). Optimization under the exact prior proceeds more freely,
although eventually achieves higher bSDE
θ(x)at convergence. This visualization may help explain differences
in the posterior estimated with the surrogate vs. exact prior.
5.3 Quality of posterior estimation with Bayesian approach vs. diffusion-based approaches
A popular class of methods is diffusion-based approaches (discussed in Sec. 2.2), which attempt to sample
from the posterior by incorporating measurements throughout the reverse diffusion process of the diffusion
model that has been trained on images from the prior. Such approaches provide fast conditional sampling,
9Published in Transactions on Machine Learning Research (07/2024)
TrueDPI + exactDPI + surr.SDE+ProjScore-ALDDPSmeas. weightKL vs. meas. weightSDE+ProjOurs (DPI + surr.)Score-ALDDPSOurs (DPI + surr.)Ours (DPI + surr.)-0.80.8
-0.80.8x1x2lowestoraclehighestlowestoraclehighest
lowestoraclehighest
0.0018.3prior
<latexit sha1_base64="bfK5Eg4s3fvjtadqsrDZt6AqEPI=">AAACCXicdVDLSgMxFM3UV62vUZdugkVwNaRTS+tCKLpxWcE+oK0lk2ba0ExmSDJiGbp146+4caGIW//AnX9jpg9Q0QMXDufcy733eBFnSiP0aWWWlldW17LruY3Nre0de3evocJYElonIQ9ly8OKciZoXTPNaSuSFAcep01vdJH6zVsqFQvFtR5HtBvggWA+I1gbqWfD8VknwHro+Qme3HSY0FQSzBfa3aRn55FTRpVisQSR47oIITclJfe07MKCg6bIgzlqPfuj0w9JHFChCcdKtQso0t0ES80Ip5NcJ1Y0wmSEB7RtqMABVd1k+skEHhmlD/1QmhIaTtXvEwkOlBoHnulML1S/vVT8y2vH2q90EyaiWFNBZov8mEMdwjQW2GeSEs3HhmAimbkVkiGWmJg8VM6EsPgU/k8arlMoOejqJF89n8eRBQfgEByDAiiDKrgENVAHBNyDR/AMXqwH68l6td5mrRlrPrMPfsB6/wI0P5tR</latexit>y=a|xerrors out
Figure 5: Comparing our VI approach with a surrogate score-based prior to baselines on a bimodal poste-
rior. In this example, the prior is a bimodal mixture-of-Gaussians, and the likelihood is Gaussian, making
the posterior a bimodal mixture-of-Gaussians (shown in “True”). Assuming access to the true prior score
function, we tested how well each method recovers the true posterior. Diffusion-based methods depend on
hand-tuned meas. weights. Even the meas. weight giving the best KL divergence (“oracle”) does not rival
using our hyperparameter-free VI approach (“DPI + surr.”). Note that this “oracle” weight would not be
accessible in practice, as it is determined by comparing to the ground-truth posterior. Diffusion-based base-
lines either (1) incorrectly place equal weight on both posterior modes or (2) miss one of the modes. DPI
with either the surrogate or the exact score-based prior recovers the relative weights of both modes. (KL
vs.meas.weight) Regardlessof hyperparameters, diffusion-basedmethodsdo not reachour KL divergence.
KL (↓)time/optimization step ( ↓)
DPI + exact 0.030 130 ms
Ours: DPI + surr. 0.037 22 ms
DPS (oracle) 0.064
Score-ALD (oracle) 0.10
SDE+Proj (oracle) 0.12
Table 2: Quantitative results for Fig. 5. A two-component Gaussian mixture model (GMM) was fit to
estimated samples to obtain a PDF. “Ours” achieves a much lower KL div. (i.e., reverse KL from estimated
posterior to true posterior) than diffusion-based baselines at their best. Time/step for DPI optimization is
lower with our surrogate than with the exact score-based prior without sacrificing much accuracy.
but they may severely mischaracterize the posterior. Although our approach also approximates the posterior
(duetousingbothasurrogatepriorandavariationaldistribution), wefindthatbeinggroundedinvariational
inference helps us obtain a more accurate posterior andimages that more accurately reflect the true image
than diffusion-based methods that make ad-hoc approximations of the posterior.
In the following experiments, we compare to three diffusion-based baselines: SDE+Proj (Song et al., 2022),
Score-ALD (Jalal et al., 2021), and Diffusion Posterior Sampling ( DPS) (Chung et al., 2023). SDE+Proj
projects images onto a measurement subspace. Score-ALD and DPS strongly approximate the posterior
throughout reverse diffusion. All baselines involve a measurement-weight hyperparameter. Our approach is
DPI with the surrogate prior (i.e., using a RealNVP as the variational distribution).
10Published in Transactions on Machine Learning Research (07/2024)
Avg. PSNR
Avg. SSIM= Ours   (DPI + Surrogate) = SDE+Proj= DPS= Score-ALDAcceleration factorAcceleration factor= TV Regularization
(a) Image-restoration metrics.
0-Filled Recon.(16x-accel.)
Score-ALD
DPS
SDE+Proj
Ours(DPI + Surr.)
Original
TV Reg.
(b) Example image reconstructions for 16×acceleration.
Figure 6: Accelerated MRI. (a)For each accel. factor ( 4×,8×,16×), we estimated posteriors for ten knee
images measured at that accel. rate. For each method, we computed the average PSNR and SSIM of 128
estimated posterior samples. The line plot shows the average result across the ten tasks; the shaded region
shows one std. dev. above and below the average. (b)An example of 16×-accel. MRI. The cropped region
exemplifies how diffusion-based baselines hallucinate more features than necessary. (a) and (b) are evidence
that a principled Bayesian approach can get closer to the true image than previous unsupervised methods.
5.3.1 Accuracy of posterior
A simple 2D example illustrates the accuracy gap between the diffusion-based baselines and our VI approach.
We consider a bimodal posterior: the prior is a bimodal mixture-of-Gaussians and the forward model a linear
projection with Gaussian noise, making the posterior a bimodal mixture-of-Gaussians. This setup lets us
evaluate with a true posterior and over a reasonable space of hyperparameters for baselines.
We tested how well each of the methods could recover the ground-truth posterior when given the true score
function of the bimodal prior (thus avoiding potential error caused by learning the score function). Fig. 5
shows estimated probability density functions (PDFs) for the evaluated methods. None of the diffusion-based
baselines correctly recover the bimodal posterior for any hyperparameter value. In particular, they struggle
to find the correct balance between the two posterior modes — in the best case, they incorrectly place equal
weight on each mode; in the worst case, they only recover one mode. Our VI approach with the exact or
surrogate score-based prior recovers both modes in correct proportion. As shown in Fig. 5 and Tab. 2, even
the best KL divergence obtained by the diffusion-based baselines does not rival that of VI with a score-based
prior. We emphasize that the hyperparameter values resulting in the “best” KL for diffusion-based methods
can only be found with knowledge of the ground-truth, which is inaccessible in real-world scenarios. In
contrast, our method automatically finds a better KL divergence by following the Bayesian formula.
5.3.2 Image-reconstruction quality
It would be reasonable to assume that the diffusion-based baselines, though less principled, may lead to
better visual quality than a Bayesian approach. However, we find that in addition to providing more reliable
uncertainty, our approach achieves higher-fidelity reconstructions. We note that similarity to a ground-truth
image does not indicate a correct posterior. Still, for a good prior, it might be desirable for posterior samples
to accurately recover the original image.
We performed multiple MRI tasks at different acceleration rates and compared our approach to the diffusion-
based baselines, as well as a total variation (TV) baseline. Implementations and hyperparameter settings for
11Published in Transactions on Machine Learning Research (07/2024)
SDE+Proj and Score-ALD were provided by Song et al. (2022). For DPS, we followed the implementation
of Chung et al. (2023) and performed a hyperparameter search on an 8×-acceleration test image to find the
optimal PSNR. For the TV baseline, we performed DPI with TV regularization with a regularization weight
of105instead of the surrogate score-based prior.
We simulated MRI at three acceleration factors for ten test images, resulting in thirty posteriors to be
estimated. As baseline implementations do not assume measurement noise, we gave the baselines noiseless
measurements and set a near-zero measurement noise for our method. The test images were randomly drawn
from the fastMRI dataset and resized to 64×64. The score model sθwas trained on 64×64fastMRI knee
images and stayed fixed across all methods.
Our method achieves a marked improvement in PSNR and SSIM (Fig. 6). Across all acceleration factors and
diffusion-based baselines, our method improves PSNR by between 2.7and8.5dB. Even though our method
and the diffusion-based methods all use the same score model, restoration quality depends on how the prior
is used for inference; whereas baselines loosely approximate the posterior and involve hyperparameters, our
approach treats the diffusion model as a standalone prior in Bayesian inference. Furthermore, Fig. 6 confirms
that a diffusion-model prior far outperforms a traditional regularizer like TV.
5.4 Application case study: black-hole interferometric imaging
Computational imaging of distant black holes is possible with very-long-baseline interferometry (VLBI)
(Thompson et al., 2017), by which a network of telescopes measures spatial frequencies of the sky’s image.
The Event Horizon Telescope (EHT) Collaboration used this technique to image the black hole at the center
of the galaxy M87 (EHTC, 2019; 2024) and the black hole at the center of our galaxy, SgrA* (EHTC, 2022).
Imaging from EHT measurements requires a prior since telescope observations are corrupted and sparsely
sample the low spatial-frequency space. Previously, the EHT Collaboration imposed hand-crafted regu-
larizers to obtain the M87* and SgrA* images. Here we demonstrate the applicability of our method to
black-hole imaging from EHT measurements. In contrast to the regularized maximum-likelihood (RML)
approaches used by the EHT Collaboration (EHTC, 2019; 2024; 2022), our approach is hyperparameter-free
and provides a rich data-driven posterior.
Black-hole imaging exemplifies an application that calls for a theoretically sound imaging approach. A
set of telescope measurements must be carefully analyzed not only because it is expensive to collect but
also because the inferred confidence intervals inform downstream scientific analysis. Therefore, an imaging
approach like ours that estimates the posterior in a principled way is often desirable, even if it requires more
computational resources.
5.4.1 Interferometry background
In VLBI, each pair of telescopes i,jprovides a Fourier measurement called a visibilityvij(van Cittert,
1934; Zernike, 1938). To overcome amplitude and phase errors, we form robust closure quantities out of the
measured visibilities (Blackburn et al., 2020). A closure phase is formed from a triplet of telescopes i,j,k
and robust to phase errors. A log closure amplitude is formed from a set of four telescopes i,j,k,ℓand robust
to amplitude errors. They are given respectively as
ycp
ijk=∠(vijvjkvki)andylogca
ijkℓ= log/parenleftbigg|vij||vkℓ|
|vik||vjℓ|/parenrightbigg
. (12)
For the inverse problem, our measurements are all the non-redundant closure phases and log closure am-
plitudes, which have a nonlinear forward model and Gaussian thermal noise. Fig. 7a illustrates the VLBI
measurements obtained by an EHT telescope array. These particular measurements were generated from an
imageofablack-holesimulation. The (u,v)coverageissparseandconstrainedtolowspatialfrequencies. The
“dirty” image, which is a naïve reconstruction from the measured visibilities, is completely un-interpretable.
The target image is the original image blurred to the maximum resolution achievable by the EHT. Recon-
structing an image beyond this intrinsic resolution is considered super-resolution.
12Published in Transactions on Machine Learning Research (07/2024)
u(u, v) Coveragev9.5 Gλ-9.5 Gλ9.5 GλOriginalTarget
Dirty
.01-.03
3.90.0
3.90.0
Brightness Temperature [1010 K]
(a) Simulated VLBI measurements.
3.90.0
0.0
0.2SampleMeanStd. Dev.
Brightness Temperature [1010 K]3.90.0
 (b) Estimated image posterior.
Figure 7: Black-hole imaging. (a)We simulated EHT VLBI measurements with realistic noise of a synthetic
black-hole image. “ (u,v)Coverage” shows which points in the complex 2D Fourier plane are measured by
the EHT array of radio telescopes. “Dirty” is a naïve reconstruction from the sparse measured visibilities.
“Target” represents the best-possible image reconstruction if the low spatial frequencies (up to the intrinsic
resolution of the EHT) were to be fully and perfectly measured. “Original” is the actual underlying image
that generated the simulated measurements. (b)Using our variational-inference approach with a score-based
prior trained on simulations of black holes, we approximated the image posterior. The posterior samples
recover the ring and brightness asymmetry of the original image. The pixel-wise mean and std. dev. are also
shown. The std. dev. indicates regions of uncertainty, such as possible wisps inside and outside the thin ring.
5.4.2 Simulated-data example
We used our method to estimate an image posterior given the simulated measurements visualized in Fig. 7a.
The variational distribution is a RealNVP, and the score-based prior was trained on 64×64images of fluid-
flow simulations of black holes (Wong et al., 2022). Fig. 7b shows a sample from the estimated posterior, as
well as the mean and standard deviation. By using a score-based prior trained on black-hole simulations, we
are able to reconstruct images that resemble these detailed simulations while still fitting the measurements.
Our proposed surrogate prior made it possible to approximate this posterior in a reasonable amount of time.
Optimization with the exact prior (Feng et al., 2023) would have taken 151GB of GPU memory and 1.8
minutes per iteration. In contrast, optimization with the surrogate prior used 46GB of GPU memory and
177milliseconds per iteration on the same hardware (4x 48GB NVIDIA RTX A6000).
6 Conclusion
We have presented a surrogate function that provides efficient access to score-based priors for variational
Bayesian inference. Specifically, the evidence lower bound bSDE
θ(x)≤logpSDE
θ(x)serves as a proxy for the
log-prior of an image in the Bayesian log-posterior. Our experiments with variational inference show at
least two orders of magnitude in runtime improvement and significant memory improvement over the ODE-
based prior. We also establish that a principled approach like ours outperforms diffusion-based methods
on posterior approximation and image restoration, evidence that following a traditional Bayesian approach
results in more reliable image reconstructions. This advantage is crucial for applications that call for a
reliable image posterior while still leveraging the expressiveness of a score-based prior.
Broader impacts There are many applications, such as in the medical and astronomical domains demon-
strated in this paper, in which there is incomplete information in measurements that makes priors necessary
to produce interpretable images. Our work proposes a way to efficiently incorporate rich diffusion-model
priors in a rigorous imaging framework. In general, however, generative models should be used with caution.
They have been used to create deepfakes (Ricker et al., 2022) and may induce spurious hallucinations. Dif-
fusion models have also been shown to memorize training data (Carlini et al., 2023; Somepalli et al., 2023),
in which case one should be careful to preserve privacy when deploying a diffusion model as an image prior.
13Published in Transactions on Machine Learning Research (07/2024)
Acknowledgments
The authors would like to thank Yang Song for his many technical insights and helpful feedback on the paper.
BTF and KLB acknowledge funding from NSF Awards 2048237 and 1935980 and the Amazon AI4Science
Partnership Discovery Grant. BTF is supported by the NSF GRFP.
References
Alexandre Adam, Adam Coogan, Nikolay Malkin, Ronan Legin, Laurence Perreault-Levasseur, Yashar Heza-
veh, andYoshuaBengio. Posteriorsamplesofsourcegalaxiesinstronggravitationallenseswithscore-based
priors.arXiv preprint arXiv:2211.03812 , 2022.
Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications ,
12(3):313–326, 1982.
Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Transactions on Graphics
(TOG), 42(4):1–11, 2023.
Lindy Blackburn, Dominic W. Pesce, Michael D. Johnson, Maciek Wielgus, Andrew A. Chael, Pierre Chris-
tian, and Sheperd S. Doeleman. Closure statistics in interferometric data. The Astrophysical Journal ,
894(1):31, may 2020. doi: 10.3847/1538-4357/ab8469. URL https://dx.doi.org/10.3847/1538-4357/
ab8469.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American statistical Association , 112(518):859–877, 2017.
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of markov chain monte carlo .
CRC press, 2011.
EmmanuelCandesandJustinRomberg. Sparsityandincoherenceincompressivesampling. Inverse problems ,
23(3):969, 2007.
Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle,
Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX
Security Symposium (USENIX Security 23) , pp. 5253–5270, 2023.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. NeurIPS , 31, 2018.
Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning
method for denoising diffusion probabilistic models. In ICCV. IEEE, 2021.
HyungjinChungandJongChulYe. Score-baseddiffusionmodelsforacceleratedmri. Medical Image Analysis ,
80:102479, 2022.
Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse
problems using manifold constraints. arXiv preprint arXiv:2206.00941 , 2022a.
Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional
diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 12413–12422, 2022b.
Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffu-
sion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on
Learning Representations , 2023. URL https://openreview.net/forum?id=OnD9zGAGT0k .
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrödinger bridge
with applications to score-based generative modeling. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.
Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34,
pp. 17695–17709. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/
paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf .
14Published in Transactions on Machine Learning Research (07/2024)
M. Delbracio, H. Talebei, and P. Milanfar. Projected distribution loss for image enhancement. In 2021
IEEE International Conference on Computational Photography (ICCP) , pp. 1–12, Los Alamitos, CA,
USA, may 2021. IEEE Computer Society. doi: 10.1109/ICCP51581.2021.9466271. URL https://doi.
ieeecomputersociety.org/10.1109/ICCP51581.2021.9466271 .
Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising diffusion
for image restoration. arXiv preprint arXiv:2303.11435 , 2023.
Edward H Egelman. The current revolution in cryo-em. Biophysical journal , 110(5):1008–1012, 2016.
Event Horizon Telescope Collaboration EHTC. First sagittarius a* event horizon telescope results. iii.
imaging of the galactic center supermassive black hole. The Astrophysical Journal Letters , 930(2):L14,
may 2022. doi: 10.3847/2041-8213/ac6429. URL https://dx.doi.org/10.3847/2041-8213/ac6429 .
The Event Horizon Telescope Collaboration EHTC. First m87 event horizon telescope results. iv. imaging
the central supermassive black hole. The Astrophysical Journal Letters , 875(1):L4, apr 2019. doi: 10.3847/
2041-8213/ab0e85. URL https://dx.doi.org/10.3847/2041-8213/ab0e85 .
The Event Horizon Telescope Collaboration EHTC. The persistent shadow of the supermassive black hole
of m 87-i. observations, calibration, imaging, and analysis. Astronomy & Astrophysics , 681:A79, 2024.
Berthy T Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L Bouman, and William T
Freeman. Score-baseddiffusionmodelsasprincipledpriorsforinverseimaging. In International Conference
on Computer Vision (ICCV) . IEEE, 2023.
Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-
play priors. In Thirty-Sixth Conference on Neural Information Processing Systems , 2022. URL https:
//arxiv.org/pdf/2206.09012.pdf .
Stephen F Gull and John Skilling. Maximum entropy method in image processing. In Iee proceedings f
(communications, radar and signal processing) , volume 131, pp. 646–659. IET, 1984.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems , 33:6840–6851, 2020.
Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion.
ACM Transactions on Graphics (ToG) , 36(4):1–14, 2017.
Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jonathan I Tamir. Robust
compressed sensing mri with deep generative priors. NeurIPS , 2021.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based
generative models. arXiv preprint arXiv:2206.00364 , 2022.
Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In
Advances in Neural Information Processing Systems , 2022.
Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review
of current methods. IEEE transactions on pattern analysis and machine intelligence , 43(11):3964–3979,
2020.
Axel Levy, Gordon Wetzstein, Julien NP Martel, Frederic Poitevin, and Ellen Zhong. Amortized inference
for heterogeneous reconstruction in cryo-em. Advances in Neural Information Processing Systems , 35:
13038–13049, 2022.
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for
generative modeling. arXiv preprint arXiv:2210.02747 , 2022.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
15Published in Transactions on Machine Learning Research (07/2024)
Sebastian Lunz, Ozan Öktem, and Carola-Bibiane Schönlieb. Adversarial regularizers in inverse problems.
Advances in neural information processing systems , 31, 2018.
Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse
problems with diffusion models. arXiv preprint arXiv:2305.04391 , 2023.
Jianwei Miao, Tetsuya Ishikawa, Qun Shen, and Thomas Earnest. Extending x-ray crystallography to allow
the imaging of noncrystalline materials, cells, and single protein complexes. Annu. Rev. Phys. Chem. , 59:
387–410, 2008.
Subhadip Mukherjee, Sören Dittmer, Zakhar Shumaylov, Sebastian Lunz, Ozan Öktem, and Carola-Bibiane
Schönlieb. Learned convex regularizers for inverse problems. arXiv preprint arXiv:2008.02839 , 2020.
Ramesh Narayan and Rajaram Nityananda. Maximum entropy image restoration in astronomy. Annual
review of astronomy and astrophysics , 24(1):127–170, 1986.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:
Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 2536–2544, 2016.
Jonas Ricker, Simon Damm, Thorsten Holz, and Asja Fischer. Towards the detection of diffusion model
deepfakes. arXiv preprint arXiv:2210.14571 , 2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 10684–10695, 2022.
Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.
Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2022.
Zakhar Shumaylov, Jeremy Budd, Subhadip Mukherjee, and Carola-Bibiane Schönlieb. Provably convergent
data-driven convex-nonconvex regularization. arXiv preprint arXiv:2310.05812 , 2023.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In Int. Conf. Machine Learning , pp. 2256–2265. PMLR, 2015.
Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and
mitigating copying in diffusion models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine (eds.), Advances in Neural Information Processing Systems , volume 36, pp. 47783–47803. Cur-
ran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
9521b6e7f33e039e7d92e23f5e37bbf4-Paper-Conference.pdf .
Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models
for inverse problems. In International Conference on Learning Representations , 2023a. URL https:
//openreview.net/forum?id=9_gsMA8MRKQ .
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In
NeurIPS , pp. 11895–11907, 2019.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to
density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
Intelligence, UAI , pp. 204, 2019. URL http://auai.org/uai2019/proceedings/papers/204.pdf .
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based
diffusion models. In Thirty-Fifth Conference on Neural Information Processing Systems , 2021a.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. In ICLR, 2021b. URL https:
//openreview.net/forum?id=PxTIG12RRHS .
16Published in Transactions on Machine Learning Research (07/2024)
Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with
score-based generative models. In ICLR, 2022. URL https://openreview.net/forum?id=vaRCHVj0uGI .
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint
arXiv:2303.01469 , 2023b.
He Sun and Katherine L Bouman. Deep probabilistic imaging: Uncertainty quantification and multi-modal
solution characterization for computational imaging. In AAAI, pp. 2628–2637, 2021.
He Sun, Katherine L Bouman, Paul Tiede, Jason J Wang, Sarah Blunt, and Dimitri Mawet. alpha-deep
probabilistic inference (alpha-dpi): efficient uncertainty quantification from exoplanet astrometry to black
hole feature extraction. arXiv preprint arXiv:2201.08506 , 2022.
A Richard Thompson, James M Moran, and George W Swenson. Interferometry and synthesis in radio
astronomy . Springer Nature, 2017.
Muhammad Usman and Philipp G Batchelor. Optimized sampling patterns for practical compressed mri.
InSAMPTA’09 , pp. Poster–session, 2009.
Pieter Hendrik van Cittert. Die wahrscheinliche schwingungsverteilung in einer von einer lichtquelle direkt
oder mittels einer linse beleuchteten ebene. Physica, 1(1-6):201–210, 1934.
Curtis R Vogel and Mary E Oman. Iterative methods for total variation denoising. SIAM Journal on
Scientific Computing , 17(1):227–238, 1996.
George N Wong, Ben S Prather, Vedant Dhruv, Benjamin R Ryan, Monika Mościbrodzka, Chi-kwan Chan,
Abhishek V Joshi, Ricardo Yarza, Angelo Ricarte, Hotaka Shiokawa, et al. Patoka: Simulating electro-
magnetic observables of black hole accretion. The Astrophysical Journal Supplement Series , 259(2):64,
2022.
Michael M Woolfson. An introduction to X-ray crystallography . Cambridge University Press, 1997.
Tianwei Yin, Zihui Wu, He Sun, Adrian V Dalca, Yisong Yue, and Katherine L Bouman. End-to-end
sequential sampling and reconstruction for mri. arXiv preprint arXiv:2105.06460 , 2021.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Free-form image inpainting
with gated convolution. In Proceedings of the IEEE/CVF international conference on computer vision ,
pp. 4471–4480, 2019.
Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron
Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, et al. fastmri: An open dataset and benchmarks
for accelerated mri. arXiv preprint arXiv:1811.08839 , 2018.
Frederik Zernike. The concept of degree of coherence and its application to optical problems. Physica, 5(8):
785–795, 1938.
Jinwei Zhang, HangZhang, Alan Wang, Qihao Zhang, MertSabuncu, PascalSpincemaille, ThanhD Nguyen,
and Yi Wang. Extending loupe for k-space under-sampling pattern optimization in multi-coil mri. In
International Workshop on Machine Learning for Medical Image Reconstruction , pp. 91–101. Springer,
2020.
Kaihao Zhang, Wenqi Ren, Wenhan Luo, Wei-Sheng Lai, Björn Stenger, Ming-Hsuan Yang, and Hongdong
Li. Deep image deblurring: A survey. International Journal of Computer Vision , 130(9):2103–2130, 2022.
Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv
preprint arXiv:2204.13902 , 2022.
Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration.
InICCV, pp. 479–486. IEEE, 2011.
Daniel Zoran and Yair Weiss. Natural images, gaussian mixtures and dead leaves. NeurIPS , 25, 2012.
17Published in Transactions on Machine Learning Research (07/2024)
A Forward models
In this appendix, we describe the forward models of the inverse problems explored in the main text: ac-
celerated MRI, denoising, and reconstruction from low spatial frequencies (“deblurring”). These tasks have
forward models of the form
y=Ax+ϵ, ϵ∼N(0,σ2
y,I), (13)
with the corresponding log-likelihood function
logp(y|x)∝−1
2σ2y∥y−Ax∥2
2. (14)
A.1 Accelerated MRI
Accelerated MRI collects sparse spatial-frequency measurements in κ-space of an underlying anatomical
image. As the acceleration rate increases, the number of measurements decreases. The forward model can
be written as
y=M⊙F(x∗) +ϵ, ϵ∼N(0,σ2
yI), (15)
where x∈CDandy∈CM.Fdenotes the 2D Fourier transform, and M∈{0,1}Dis a binary sampling
mask that reduces the number of non-zero measurements to M <<D . Oftenσyis assumed to be small (e.g.,
corresponding to an SNR of at least 30 dB). We use Poisson-disc sampling (Usman & Batchelor, 2009) for
the sampling mask. 16×-acceleration, for example, corresponds to a sampling mask with only 1/16nonzero
elements.
Experimental setup In our experiments, we assumed that |σy|is0.05%of the DC (zero-frequency)
amplitude. This corresponds to a maximum SNR of 40 dB. The only exception is for comparison to baselines
(Fig. 6), since baseline methods do not account for measurement noise. In this case, we let |σy|= 0.1%of
the DC amplitude along the horizontal direction of the true image, which amounts to a very low level of
noise.
A.2 Denoising
The denoising forward model is simply
y=x+ϵ, ϵ∼N(0,σ2
y,I), (16)
where x∈RD, andσydetermines the level of i.i.d. Gaussian noise added to the clean image to get y∈RD.
Experimental setup In our presented experiments on denoising, σy= 0.2, which is 20%of the dynamic
range.
A.3 Deblurring
We refer to the task of reconstruction from the lowest spatial frequencies as deblurring. The forward model
is given by
y=Dx+ϵ, ϵ∼N(0,σ2
y,I), (17)
where x∈CD,y∈CM, and D∈CM×Dperforms a 2D discrete Fourier transform (DFT) with only the
firstMDFT components.
Experimental setup In our presented experiments on deblurring, the measurements are the lowest 6.25%
of the DFT components, and |σy|= 1.
18Published in Transactions on Machine Learning Research (07/2024)
B Experiment details
For the sake of reproducibility, we detail the experimental setup behind each figure. Our code will be made
publicly available. Some common implementation details are that the exact prior ( logpODE
θ) was always
estimated with 16 trace estimators. The RealNVP had 32 affine-coupling layers unless stated otherwise.
B.1 Variational distributions
We first describe the two types of variational distributions considered in our experiments: a RealNVP
normalizing flow and a multivariate Gaussian with diagonal covariance.
RealNVP The architecture of the RealNVP is determined by the number of affine-coupling layers and the
width of each layer. For images up to 64×64, we use 32 affine-coupling layers and set the number of hidden
neurons in the first layer to 1/8of the image dimensionality (e.g., 32·32·3/8for32×32RGB images). We
use batch normalization in the network. Please refer to the original DPI (Sun & Bouman, 2021) PyTorch
implementation3for details on the architecture. Our implementation is an adaptation of this codebase in
JAX.
Gaussian Other experiments use a multivariate Gaussian distribution with a diagonal covariance matrix
as the variational family. In this case, the parameters are the mean image and the pixel-wise standard
deviation. We initialize the mean at 0.5 and the standard deviation at 0.1 for all pixels. To sample, we take
the absolute value of the standard deviation and construct the diagonal covariance matrix.
B.2 MRI efficiency experiment (Fig. 1, Tab. 1)
Score model For each image size, the score model was an NCSN++ architecture with 64 filters in the
first layer and trained with the VP SDE with βmin= 0.1,βmax= 10.
Variational optimization For each task (i.e., each image size and prior), the variational distribution was
a multivariate Gaussian with diagonal covariance. The batch size was 64, learning rate 0.0002, and gradient
clip 1. A convergence criterion based on the loss value is difficult to define due to high variance of the loss (we
used 1 time sample to estimate bθ(x)). We defined a convergence criterion based on the change in the mean
of the variational distribution. Specifically, every 10000 steps, we evaluated a snapshot of the variational
Gaussian and computed δ=∥µcurr−µprev∥/∥µprev∥, whereµcurrandµprevare the current and previous
snapshot means, respectively. If δ < εfor some threshold εtwo snapshots in a row, then the optimization
was considered converged. Since convergence rate depends on the image size and the prior used, we set a
differentεfor each task:
•16×16(surrogate): ε= 0.002
•32×32(surrogate): ε= 0.003
•64×64(surrogate): ε= 0.005
•128×128(surrogate): ε= 0.007
•256×256(surrogate): ε= 0.009
•16×16(exact):ε= 0.0025
•32×32(exact):ε= 0.0027
•64×64(exact):ε= 0.005
We were conservative in defining the convergence and checked that optimization under the surrogate actually
achieved better sample quality than optimization under the exact prior (see Fig. 1).
3https://github.com/HeSunPU/DPI
19Published in Transactions on Machine Learning Research (07/2024)
DataThe test image is from the fastMRI (Zbontar et al., 2018) single-coil knee test dataset and was
resized to 64×64with antialiasing.
B.3 256x256 MRI examples (Fig. 2)
The4×-acceleration result is from the efficiency experiment (Fig. 1 and Tab. 1) on the 256×256test image.
The16×-acceleration result came from a similar setup, where the variational distribution was Gaussian with
diagonal covariance. Optimization was done with a batch size of 64, learning rate of 0.00001, and gradient
clip of 0.0002. We ran optimization for 270K steps (optimization for 4×-acceleration was done in 100K steps
with the convergence criterion).
In the figure caption, we report that the true image is within three standard deviations of the inferred
posterior mean for 96%and99%of the pixels for 16×- and 4×-acceleration, respectively. This was computed
based on the mean and standard deviation of 128 samples from the inferred posterior. We find the same
result when using the exact mean and standard deviation of the inferred posterior: with respect to the
inferred posterior, the true image is within three standard deviations of the mean for 96.7%and99.0%of
the pixels for 16×- and 4×-acceleration, respectively.
B.4 Ground-truth posterior (Fig. 3a)
DataThe mean and covariance of the ground-truth Gaussian prior were fit with PCA (with 256 principal
components) to training data from the CelebA dataset (Liu et al., 2015). The CelebA images were resized
to16×16with antialiasing.
Score model The score model was based on the DDPM++ deep continuous archictecture of Song et al.
(2021b) with 128 filters in the first layer. It was trained with the VP SDE with βmin= 0.1andβmax= 20
for 100K steps.
Variational optimization The variational distribution was a RealNVP. Under the surrogate prior, opti-
mization was done with a learning rate of 0.00005 and gradient clip of 1. Under the exact prior, the learning
rate was 0.0002 and gradient clip 1. Both priors used a batch size of 64.
B.5 32x32 image denoising (Fig. 3b)
Variational optimization For both CelebA denoising (i) and CIFAR-10 denoising (ii), the variational
distribution was a RealNVP. Optimization under the exact prior was done with a learning rate of 0.0002
and gradient clip of 1 for 20K steps. Optimization under the surrogate prior was done with a learning rate
of 0.00001 and gradient clip of 1. For CelebA, the batch size was 64 and training was done for 1.72M steps
(convergence was probably achieved earlier, but we continued training to be conservative). For CIFAR-10,
the batch size was 128 and training was done for 550K steps.
Score model For both (i) and (ii), the score model had an NCSN++ architecture with 64 filters in the
first layer. For the CelebA prior, it was trained with the VP SDE with βmin= 0.1andβmax= 20and with
images that were resized without antialiasing. For the CIFAR-10 prior, it was trained with the VP SDE
withβmin= 0.1andβmax= 10.
DataThe CelebA and CIFAR-10 images are both 32×32. The CelebA image was resized without an-
tialiasing.
B.6 Bound gap (Fig. 4)
Visualization of the bound gap is shown for optimization of the RealNVP from Fig.3b(i) (i.e., 32×32CelebA
denoising). For the plots comparing the lower-bound to the ODE log-probability, we used 2048 time samples
to estimate bθ(x).
20Published in Transactions on Machine Learning Research (07/2024)
B.7 Accuracy of posterior (Fig. 5, Tab. 2)
Variational optimization For both the exact score-based prior and surrogate score-based prior, the
variational distribution was a RealNVP with 16 affine-coupling layers, and it was optimized for 12K iter-
ations with a batch size of 2560 and learning rate of 10−5. For the surrogate score-based prior, bθ(x)was
approximated with Nt=Nz= 1.
Baselines For this 2D experiment, we implemented the diffusion-based baselines exactly according to
their proposed algorithms. For SDE+Proj, we tested the following values for the measurement weight
λ:linspace(0.001, 0.5, num=100) . For Score-ALD, we distilled all hyperparameters into one global
hyperparameter 1/γTand tested the following values for γT:linspace(100, 0.8, num=100) . For DPS, we
tested the following values for the scale parameter ζ:exp(linspace(log(0.001), log(0.15), num=100)) .
Evaluation Since the diffusion-based approaches only provide samples (not probability densities), we ap-
proximated the probability density function (PDF) from the estimated posterior samples. For each method,
we fit a two-component Gaussian mixture model (GMM) to 10000 samples. The reverse KL divergence
was approximated with the log-density function of the fitted GMM and the log-density function of the true
posterior, evaluated on these 10000 samples.
B.8 Image-restoration metrics (Fig. 6)
Score model The score model was the same as the one used for the 64×64image in the MRI efficiency
experiment (Fig. 1).
Variational optimization The variational distribution was a RealNVP. Optimization was done with a
learning rate of 0.00001 and gradient clip of 0.0002. We used the same convergence criterion as the one used
in the MRI efficiency experiment with ε= 0.005. The same convergence criterion was also used for the TV
results but with a maximum number of steps of 50000. The TV regularization weight was 105.
Baseline hyperparameters For SDE+Proj, we used the projection CS solver provided by Song et al.
(2022) with the hyperparameters snr=0.517, coeff=1 . For Score-ALD, we used the langevin CS solver
with the hyperparameters n_steps_each=3, snr=0.212, projection_sigma_rate=0.713 . For DPS, we
used scale=0.5 . This was the best scale out of [10,1,0.9,0.5,0.3,0.1,0.001]for a test image in terms of
PSNR with respect to the true image.
B.9 Black-hole interferometric imaging (Fig. 7)
Score model The score model was trained on images of general relativistic magneto-hydrodynamic
(GRMHD) simulations (Wong et al., 2022) resized to 64×64. During training, the images were randomly
flipped horizontally, and they were randomly zoomed so that the ring diameter would vary between 35and
48µas. The score model had an NCSN++ architecture with 64filters in the first layer; it was trained with
the VP SDE with βmin= 0.1andβmax= 20for100K steps.
Variational optimization The variational distribution with a RealNVP. Optimization was done with a
learning rate of 0.00001and gradient clip of 1for100K steps.
Interferometricimagingassumptions Imagingwasdoneforafieldofviewof 160µas. Aflux-constraint
loss was added to the DPI optimization objective so that all posterior images would have a total flux around
173(computed as the median total flux of images sampled from the score-based GRMHD prior).
21