Published in Transactions on Machine Learning Research (01/2023)
Online Learning for Prediction via Covariance Fitting:
Computation, Performance and Robustness
Muhammad Osama muhammad.osama@it.uu.se
Department of Information Technology,
Uppsala University
Dave Zachariah dave.zachariah@it.uu.se
Department of Information Technology,
Uppsala University
Petre Stoica ps@it.uu.se
Department of Information Technology,
Uppsala University
Thomas B. Schön thomas.schon@it.uu.se
Department of Information Technology,
Uppsala University
Reviewed on OpenReview: https: // openreview. net/ forum? id= nAr9PhyEbQ
Abstract
We consider the problem of online prediction using linear smoothers that are functions of
a nominal covariance model with unknown parameters. The model parameters are often
learned using cross-validation or maximum-likelihood techniques. But when training data
arrives in a streaming fashion, the implementation of such techniques can only be done in
an approximate manner. Even if this limitation could be overcome, there appears to be no
clear-cut results on the statistical properties of the resulting predictor.
Here we consider a covariance-ﬁtting method to learn the model parameters, which was
initially developed for spectral estimation. We ﬁrst show that the use of this approach
results in a computationally eﬃcient online learning method in which the resulting predictor
can be updated sequentially. We then prove that, with high probability, its out-of-sample
error approaches the optimal level at a root- nrate, where nis the number of data samples.
This is so even if the nominal covariance model is misspeciﬁed. Moreover, we show that the
resultingpredictor enjoys two robustnessproperties. First, it corresponds to apredictor that
minimizes the out-of-sample error with respect to the least favourable distribution within
a given Wasserstein distance from the empirical distribution. Second, it is robust against
errors in the covariate training data. We illustrate the performance of the proposed method
in a numerical experiment.
1 Introduction
We consider scenarios in which we observe a streamof randomly distributed data
Dn={(x1,y1),..., (xn,yn)}n= 1,2,3,...
1Published in Transactions on Machine Learning Research (01/2023)
Given covariate xn+1in a spaceX, our goal is to predict the outcome yn+1inR. A large class of predictors
(also known as linear smoothers) can be described as a weighted combination of observed outcomes:
/hatwidey(x;λ) =n/summationdisplay
i=1wi(x;λ)yi, (1)
where xdenotes any test point and the weights {wi(x;λ)}are parameterized by λ. The sensitivity of such a
predictor function to noise in the training data is often characterized by how close the in-sample prediction
/hatwidey(xi;λ)is toyiand quantiﬁed by the sum of in-sample weights,
0< dfn,n/summationdisplay
i=1wi(xi;λ), (2)
also known as the ‘eﬀective’ degrees of freedom (Ruppert et al., 2003; Wasserman, 2006; Hastie et al., 2009).
These degrees of freedom are often tuned to avoid overﬁtting to the irreducible noise in the training data
with the aim of achieving good out-of-sample performance. This includes learning the parameters λfrom
Dnvia distribution-free cross-validation or distribution-based maximum likelihood methods, which however
can typically be implemented only approximately in the online scenario.
In this paper, we consider an alternative method using a covariance-based criterion ﬁrst proposed in the
context of spectral estimation (Stoica et al., 2010a;b). We show that this method
•enables sequential computation of a predictor with learned parameters,
•approaches an optimal out-of-sample performance at a root- nrate,
•enjoys two types of robustness properties.
For illustration of the online learning method, we include a numerical experiment.
Notation:/bardblZ/bardblW=/radicalbig
tr{Z/latticetopWZ}is a weighted Frobenious norm of matrix Zusing a positive deﬁnite weight
matrix W. The element-wise Hadamard product between zandz/primeis denoted z⊙z/prime.
2 Problem formulation
The linear smoother predictor (1) can be written compactly as
/hatwidey(x;λ) =w/latticetop(x;λ)y,where w/latticetop(x;λ) = [w(x1;λ),...,w (xn;λ)]andy= [y1,...,yn]/latticetop.(3)
We will investigate a class of model-based weights for which (3) can be computed sequentially from the
streamDn. Speciﬁcally, suppose yis modeled as a zero-mean stochastic process with a nominal covariance
function parameterized as
Cov[y,y/prime|x,x/prime;λ] =λ0δ(x,x/prime) +d/summationdisplay
k=1λkφk(x)φk(x/prime), (4)
where xandx/primeare two arbitrary covariates, δ(x,x/prime)is the Kronecker delta function and {φk(x)}are real-
valued features of x. Hereλis theunknown set ofd+ 1nonnegative model covariance parameters. When
xbelongs to a vector space, then periodic Fourier-type features provide a convenient choice due to their
excellent covariance approximating properties (Ruppert et al., 2003; Rahimi & Recht, 2007; Stein, 2012;
Hensman et al., 2017; Solin & Särkkä, 2020). Using the notation above, a set of optimal weights can be
written as
w(x;λ) =C−1
λΦΛφ(x), (5)
(Bishop, 2006; Rasmussen & Williams, 2006; Stein, 2012) where
φ(x) =
φ1(x)
...
φd(x)
, Φ=
φ/latticetop(x1)
...
φ/latticetop(xn)

2Published in Transactions on Machine Learning Research (01/2023)
contain the features and
Cλ=ΦΛΦ/latticetop+λ0In/follows0 (6)
is a nominal covariance matrix where Λ=diag(λ1,...,λd). The predictor function above includes a variety
of penalized regression methods (see the references cited above). The degrees of freedom of (3) are controlled
byλand we have that
0<dfn(λ) =tr/braceleftbig
ΦΛΦ/latticetopC−1
λ/bracerightbig
=tr/braceleftbig
ΦΛΦ/latticetop(ΦΛΦ/latticetop+λ0In)−1/bracerightbig
≤min(n,d) (7)
(Stoica & Stanasila, 1982; Ruppert et al., 2003).
Letλndenote the parameters that are ﬁttedto the data stream Dnin some way. The concern of this paper
is two-fold: (i) sequential computation of /hatwidey(x;λn)and (ii) performance guarantees that hold even when the
model class is not well-speciﬁed. That is, the nominal covariance model (4) using {φk(x)}may not match
the unknown covariance structure of the data. Nevertheless, we aim to derive meaningful guarantees that
remain valid also in this misspeciﬁed case.
Cross-validation or maximum likelihood methods are two popular approaches to ﬁtting λn. However, these
methods have challenges in the online learning setting. First, computing λnfor the covariance model above
is a non-convex problem that can be riddled with multiple local minima. Second, for each additional training
data point (xn+1,yn+1), the parameters λn+1will have to be reﬁtted and therefore /hatwidey(x;λn)recomputed from
scratch or approximated (see below). Third, to the best of our knowledge, ﬁnite out-of-sample prediction
performance guarantees for /hatwidey(x;λn)neither exist for cross-validation nor maximum likelihood λn.
We will consider an alternative covariance-based ﬁtting criterion for λ, used in another context, viz. spectral
estimation (Stoica et al., 2010a;b). The main contribution is to derive performance guarantees of a sequen-
tially computable predictor, thereby establishing predictive properties of the covariance-ﬁtting methodology.
Similar results do not appear to be available in the previous literature.
3 Other approaches to online model-based prediction
We identify two main lines of work that enable online or fast implementations of model-based linear
smoothers, which include Gaussian process regression and kriging methods: The ﬁrst line considers spectral
model approximations (e.g., (Rahimi & Recht, 2007; Hensman et al., 2017; Solin & Särkkä, 2020)) which
is covered by the class of covariance models in (4). These methods also enable eﬃcient online computation
of/hatwidey(x;λ), but for a ﬁxed set of model parameters λ. The second line considers sparse variational approx-
imations (e.g., (Titsias, 2009; Bui et al., 2017; Stanton et al., 2021)). These methods can recompute the
predictor/hatwidey(x;λ)eﬃciently when new data arrives, but again for ﬁxed λ.
Fittingλnand computing /hatwidey(x;λn)in a sequential manner requires recomputing past covariance quantities
and is computationally prohibitive. For the covariance model above, it means recomputing (5) and the
inverse of (6) at each new sample . Work on this problem appears to be scarce. Stanton et al. (2021) consider
seeking maximum likelihood estimates λnby a type of gradient-based search that projects past quantities
onto a lower-dimensional space. However, neither a convergence analysis towards the sought maximum
likelihood estimate nor any resulting predictive properties are provided. By contrast, Cai & Yuan (2012)
provide an asymptotic performance guarantee of a linear smoother with a learned regularization parameter,
but their approach is restricted to the oﬄine setting.
Our focus below is on deriving ﬁnite-sample performance guarantees for a sequentially computable predictor.
4 Learning via covariance ﬁtting
The parameters λcan be learned by ﬁtting the model covariance matrix Cλin (5) to the empirical covariance
matrix yy/latticetop. Speciﬁcally, we will use the following ﬁtting criterion, known as Spice(Stoica et al., 2010a;b),
λn= arg min
λ≥0/vextenddouble/vextenddoubleyy/latticetop−Cλ/vextenddouble/vextenddouble2
C−1
λ, (8)
3Published in Transactions on Machine Learning Research (01/2023)
whereλnis a function of the datastream Dn. Since this criterion is convex in λ, we can be sure that a global
minimizer can be determined.
4.1 Sequential computation
The covariance-ﬁtting criterion in (8) can be connected to an alternative convex problem and this connection
can be utilized to perform parameter estimation online, as shown in (Zachariah & Stoica, 2015). We ﬁrst
leverage this connection to compute /hatwidey(x;λn)in a sequential manner, and subsequently we derive a series of
predictive performance guarantees of the proposed method.
Theorem 1 The predictor function /hatwidey(x;λn+1)can be updated from /hatwidey(x;λn)in a constant runtime O/parenleftbig
d2/parenrightbig
.
The total memory requirement of the method is also on the order of O/parenleftbig
d2/parenrightbig
.
Proof 1 We ﬁrst note that the predictor (3) has an equivalent form
/hatwidey(x;λ) =φ/latticetop(x)ΛΦ/latticetopC−1
λy/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
/hatwideθ(λ)(9)
For later use, we also note that (9) is invariant to a uniform rescaling of λ, since
/hatwideθ(λ)≡/hatwideθ(αλ),∀α>0 (10)
The covariance-ﬁtting criterion in (8) can be expanded into
/vextenddouble/vextenddoubleyy/latticetop−Cλ/vextenddouble/vextenddouble2
C−1
λ=y/latticetopC−1
λy·/bardbly/bardbl2+tr{Cλ}+K, (11)
whereKis a constant. Thus λnis also a minimizer of
V(λ) =y/latticetopC−1
λy+/bardbly/bardbl−2·tr{Cλ}
Next, we follow Zachariah & Stoica (2015, Appendix A) and consider the following augmented convex crite-
rion,
V/prime(θ,λ) =1
λ0/bardbly−Φθ/bardbl2
2+θ/latticetopΛ−1θ+tr{Cλ}, (12)
and show that its minimizers produce the predictor /hatwidey(x;λn) =φ/latticetop(x)/hatwideθ(λn).
The ﬁrst argument of (12) is minimized by
/hatwideθ(λ) =/parenleftbig
λ−1
0Φ/latticetopΦ+Λ−1/parenrightbig−1Φ/latticetopλ−1
0y=ΛΦ/latticetopC−1
λy,
where the second equality follows from using the matrix inversion lemma. Inserting the minimizer it back
into (12), we have a concentrated cost function:
V/prime(/hatwideθ(λ),λ) =y/latticetopC−1
λy+tr{Cλ}
Let us now consider the minimizing λ. By rescaling the parameters by α=/bardbly/bardbl−1>0, we have that
α·V/prime(/hatwideθ(αλ),αλ) =α·/parenleftbig
y/latticetop(αCλ)−1y+·tr{αCλ}/parenrightbig
=V(λ)
whereα=/bardbly/bardbl−1>0. It follows that αλnis a minimizer of (12), since
V/prime(/hatwideθ(αλn),αλn) =1
αV(λn)≤1
αV(λ) =V/prime(/hatwideθ(αλ),αλ)∀λ≥0.
4Published in Transactions on Machine Learning Research (01/2023)
This scaled minimizer can be related to the predictor with ﬁtted parameters: /hatwidey(x;λn) =φ/latticetop(x)/hatwideθ(λn)≡
φ/latticetop(x)/hatwideθ(αλn)using (10).
We now change the order of the minimization of V/prime(θ,λ)to arrive at an alternative way of computation.
The minimizing parameters are
λk(θ) =/braceleftBigg/bardbly−Φθ/bardbl2√n, k = 0
|θi|√nψk, k = 1,...,d(13)
whereψk=/radicalBig
1
n/summationtextn
i=1φ2
k(xi). Inserting (13) into (12) yields the following equivalent convex cost function
arg min
θ/radicalbigg
1
n/bardbly−Φθ/bardbl2
2+1√n/bardblψ⊙θ/bardbl1, (14)
whereψ= [ψ1···ψd]/latticetop. Letθndenote the minimizer of (14), then /hatwidey(x;λn)≡φ/latticetop(x)θn.
Eq. (14) is a weighted square-root LASSO problem (Belloni et al., 2011) that can be solved in a runtime on
the order ofO(d2)using variables
An=n/summationdisplay
i=1φ(xi)φ/latticetop(xi),bn=n/summationdisplay
i=1φ/latticetop(xi)yi, cn=n/summationdisplay
i=1y2
i, (15)
of ﬁxed dimension that are updated recursively. Thus the memory requirement is dominated by the storing
of thed×d-matrix An.
The pseudocode for the method is provided in the appendix.
4.2 Out-of-sample performance
We now turn to evaluating the out-of-sample performance of the predictor learned from the data stream Dn.
Speciﬁcally, we consider the mean-squared error
Mse =E/bracketleftBig/parenleftbig
yn+1−/hatwidey(xn+1;λn)/parenrightbig2/bracketrightBig
, (16)
for thesubsequent sample (xn+1,yn+1)in the stream. The expectation is conditional on Dn, thus the Mse
will depend on the particular realization of the stream since the learned predictor is a function of all past
samples.
To provide a performance reference, we note that all predictors of the form (3) with (5) belong to the
following class of predictor functions
F,/braceleftBigg
f(x) =d/summationdisplay
k=1φk(x)θk:θ∈Rd/bracerightBigg
, (17)
i.e., a linear combination of all features {φk(x)}in the nominal model (4). We can now benchmark /hatwidey(x;λn)
against the minimal achievable error among all predictors in F, even when the nominal covariance model
(4) is misspeciﬁed. Speciﬁcally, we provide the following ﬁnite out-of-sample performance guarantee for the
learned predictor.
Theorem 2 Assume the outcome yand features are φ(x)bounded, and that the features are such that/summationtextn
i=1|φk(xi)|2>0. If the data pairs (xi,yi)in the stream are drawn i.i.d., then the out-of-sample error of
/hatwidey(x;λn)is given by
E/bracketleftBig/parenleftbig
yn+1−/hatwidey(xn+1;λn)/parenrightbig2/bracketrightBig
≤min
/hatwidey∈FE/bracketleftBig/parenleftbig
yn+1−/hatwidey(xn+1)/parenrightbig2/bracketrightBig
+K/radicalbigg
1
nln2(d+ 1)2
ε+bn(18)
with probability of at least 1−ε, whereKis a constant and bnis bounded asO(n−3/4). That is, with high
probability, the out-of-sample error approaches the minimum achievable error at a root- nrate. Note that the
number of features dincreases only the second term at a logarithmic rate.
5Published in Transactions on Machine Learning Research (01/2023)
Proof 2 For notational simplicity, let (x,y)denote the random (n+ 1)th sample. Let /hatwideybe any predictor in
Fand express its out-of-sample mean-square error as:
R(/hatwidey)≡E/bracketleftBig/parenleftbig
y−φ/latticetop(x)θ/parenrightbig2/bracketrightBig
=E
/bracketleftbiggθ
−1/bracketrightbigg/latticetop/bracketleftbiggφ(x)
y/bracketrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
z/bracketleftbiggφ(x)
y/bracketrightbigg/latticetop/bracketleftbiggθ
−1/bracketrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
/tildewideθ
=/tildewideθ/latticetopΣ/tildewideθ, (19)
where Σ=E[zz/latticetop]. Similarly, the in-sample error can be expressed as Rn(/hatwidey) =/tildewideθ/latticetop/hatwideΣ/tildewideθ, where/hatwideΣ=n−1(z1z/latticetop
1+
···+znz/latticetop
n). The gap between in- and out-of-sample errors can be bounded as:
|Rn(/hatwidey)−R(/hatwidey)|=|/tildewideθ/latticetop(/hatwideΣ−Σ)/tildewideθ|
≤d+1/summationdisplay
i=1d+1/summationdisplay
j=1|/tildewideθi/bardbl/tildewideθj||/hatwideΣij−Σij|
≤(/bardblθ/bardbl1+ 1)2·max
i,j|/hatwideΣij−Σij|
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
/tildewideσ(20)
Next, we bound /tildewideσ(see also Greenshtein & Ritov (2004)). Since yandφ(x)are bounded random variables,
we have that|zizj|≤Bfor someBand using Hoeﬀding’s inequality
Pr/braceleftBig
|/hatwideΣij−Σij|≥σ/bracerightBig
≤2 exp/parenleftbigg
−nσ2
2B2/parenrightbigg
(21)
Combining this result with the union bound over all (d+ 1)2variables in/tildewideσ, we have that
Pr{/tildewideσ≥σ}≤(d+ 1)2·2 exp/parenleftbigg
−nσ2
2B2/parenrightbigg
,ε (22)
Consequently, we can replace /tildewideσby
σ=B/radicalbigg
2
n/radicalbigg
ln2(d+ 1)2
ε(23)
in (20) so that
|Rn(/hatwidey)−R(/hatwidey)|≤(/bardblθ/bardbl1+ 1)2σ,
holds for any predictor in Fwith a probability of at least 1−ε. Thus ifθis bounded,/bardblθ/bardbl1≤Pfor someP,
then
R(/hatwidey)−(P+ 1)2σ≤Rn(/hatwidey)≤R(/hatwidey) + (P+ 1)2σ (24)
holds with a probability of at least 1−ε.
Let us now study two speciﬁc predictor functions in F: An optimal predictor that minimizes the out-of-
sample error y⋆(x) =φ/latticetop(x)θ⋆, whereθ⋆=E[φ(x)φ/latticetop(x)]†E[φ(x)y]is a bounded vector. The learned
predictory⋆(x;λn) =φ/latticetop(x)θn, whereθnis a minimizer of (14). This vector is also bounded because the
minimizer of (14) coincides with that of
arg min
θ:/bardblψ⊙θ/bardbl1≤γ/bardbly−Φθ/bardbl2
for some value of 0≤γ <∞. Thus both/bardblθ⋆/bardbl1and/bardblθn/bardbl1are bounded by some Pand (24) applies to the
optimal and learned predictors, denoted by y⋆and/hatwideynfor brevity.
Sinceθnminimizes the criterion in (14), it follows that
/radicalbig
Rn(/hatwideyn) +n−1/2/bardblψ⊙θn/bardbl1≤/radicalbig
Rn(y⋆) +n−1/2/bardblψ⊙θ⋆/bardbl1,∀n
6Published in Transactions on Machine Learning Research (01/2023)
After rearranging, we have
/radicalbig
Rn(/hatwideyn)−/radicalbig
Rn(y⋆)≤n−1/2(/bardblψ⊙θ⋆/bardbl1−/bardblψ⊙θn/bardbl1)
≤n−1/2/bardblψ⊙θ⋆/bardbl1
≤n−1/2βP,(25)
whereβ=/bardblψ/bardbl∞. Multiplying both sides of the equality by the positive quantity (/radicalbig
Rn(/hatwideyn) +/radicalbig
Rn(y⋆)), we
have
Rn(/hatwideyn)−Rn(y⋆)≤(/radicalbig
Rn(/hatwideyn) +/radicalbig
Rn(y⋆))n−1/2βP
≤(2/radicalbig
Rn(y⋆) +n−1/2βP)n−1/2βP,(26)
where the second inequality follows from using (25). Finally, by deﬁnition R(y⋆)≤R(/hatwideyn)and we have that
R(/hatwideyn)≤Rn(/hatwideyn) + (P+ 1)2σ
≤Rn(y⋆) + (P+ 1)2σ+ (2/radicalbig
Rn(y⋆) +n−1/2βP)n−1/2βP
≤R(y⋆) + 2(P+ 1)2σ+ (2/radicalbig
R(y⋆) + (P+ 1)2σ+n−1/2βP)n−1/2βP
=R(y⋆) + 2(P+ 1)2B√
2·/radicalbigg
1
nln2(d+ 1)2
ε+O(n−3/4),(27)
with a probability of at least 1−ε, where (24) was used in the ﬁrst and third inequality and (26) was used in
the second inequality.
4.3 Distributional robustness
In the previous section, we showed that the out-of-sample error of /hatwidey(x;λn)approaches the minimum achiev-
ableMseat a root-nrate. We will now see that this predictor also provides robustness against distributional
uncertainty for ﬁnite n.
The feature vector φ(x) :X → Rdfor any predictor in Fleads to a distribution of the random variables
(φ,y)which we denote p(φ,y). The out-of-sample Msecan then be written
E/bracketleftBig/parenleftbig
yn+1−/hatwidey(xn+1)/parenrightbig2/bracketrightBig
≡Ep/bracketleftBig
(yn+1−φ/latticetop
n+1θ)2/bracketrightBig
, (28)
Usingni.i.d. samples (φi,yi), we can deﬁne a predictor in Fthat minimizes the Mseunder the least
favourable distribution among all plausible distributions that are consistent with the data. Such a predictor
is called ‘distributionally robust’, see, e.g., Duchi & Namkoong (2018). To formalize a set of plausible
distributions, we ﬁrst deﬁne the empirical distribution
pn(φ,y) =1
nn/summationdisplay
i=1δ(φ−φi, y−yi) (29)
Then we consider a set of distributions
{p:D(pn,p)≤/epsilon1n}, (30)
whereD(pn,p)is some divergence measure. A distributionally robust predictor minimizes the Mseunder
the least-favourable distribution in the set (30), viz.
max
p:D(pn,p)≤/epsilon1nEp/bracketleftbig
(yn+1−φ/latticetop
n+1θ)2/bracketrightbig
(31)
Several diﬀerent divergence measures D(pn,p)have been considered in the literature, including Kullback-
Leibler divergence, chi-square divergence, and so on. One popular divergence measure is the Wasserstein
distance (Blanchet et al., 2019), which is deﬁned as
D(pn,p) = inf
πEπ/bracketleftbig
c(φ,y,φ/prime,y/prime)/bracketrightbig
, (32)
7Published in Transactions on Machine Learning Research (01/2023)
wherec(φ,y,φ/prime,y/prime)is a nonnegative cost function and πis a joint distribution over (φ,y,φ/prime,y/prime)whose
marginals equal pn(φ,y)andp(φ/prime,y/prime), respectively. Thus D(pn,p)can be interpreted as measuring the
expected cost of moving probability mass from one distribution to the other.
Theorem 3 Suppose the observed features are standardized so that1
n/summationtextn
i=1φ2
k(xi) = 1. Then/hatwidey(x;λn)
corresponds to a predictor that minimizes the out-of-sample Mseunder the least favourable distribution in
the set{p:D(pn,p)≤n−2}, deﬁned by a Wasserstein distance (32) with a cost function
c(φ,y,φ/prime,y/prime) =/braceleftBigg
/bardblφ−φ/prime/bardbl2
∞y=y/prime,
∞ otherwise.(33)
Thus the predictor is robust against distributional uncertainties in the features φ, which may be high-
dimensional. Note that the size of the distribution set shrinks with n.
Proof 3 When the features are standardized, then ψ=1and (14) becomes
arg min
θ/radicalbigg
1
n/bardbly−Φθ/bardbl2
2+1√n/bardblθ/bardbl1. (34)
Using Theorem 1 in (Blanchet et al., 2019), (34) corresponds to a predictor that minimizes (31) with diver-
gence bound /epsilon1n=n−2.
4.4 In-sample robustness
When learning /hatwidey(x;λn)it is possible that the observed covariates themselves are subject to errors so that
the dataset is:
/tildewideDn={(/tildewidex1,y1),..., (/tildewidexn,yn)}
Then the true feature vector φi=φ(xi)can be viewed as a perturbed version of the observed vector
/tildewideφi=φ(/tildewidexi), where the perturbation δi=φi−/tildewideφiis unknown. This problem (aka. errors-in-variables) leads
to yet another interpretation of the predictor /hatwidey(x;λn).
Theorem 4 Consider the bounded set of possible in-sample perturbations:
Sn=/braceleftBig
δ1,...,δn:Epn/bracketleftbig
δ2
k/bracketrightbig
≤n−1Epn/bracketleftBig
/tildewideφ2
k/bracketrightBig
,∀k= 1,...,d/bracerightBig
Then/hatwidey(x;λn)corresponds to a predictor that minimizes the in-sample root- Mseunder the least-favourable
perturbations inSn:
max
{δi}∈S n/radicalBig
Epn/bracketleftbig
(y−(/tildewideφ+δ)/latticetopθ)2/bracketrightbig
, (35)
where/hatwidey= (/tildewideφ+δ)/latticetopθ∈F.
Proof 4 The problem in (35) can be written as:
max
{δi}∈S1√n/bardbly−(/tildewideΦ+∆)θ/bardbl2,where ∆=
δ/latticetop
1
...
δ/latticetop
n
 (36)
Let[∆]kdenote thekthcolumn of the matrix ∆. We can then upper bound the error as
max
{δi}∈S1√n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley−/tildewideΦθ−d/summationdisplay
k=1[∆]kθk/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤max
{δi}∈S1√n/bardbly−/tildewideΦθ/bardbl2+1√nd/summationdisplay
k=1/bardbl[∆]kθk/bardbl2,
≤1√n/bardbly−/tildewideΦθ/bardbl2+ max
{δi}∈S1√nd/summationdisplay
k=1/bardbl[∆]k/bardbl2|θk|,
≤1√n/bardbly−/tildewideΦθ/bardbl2+1√nd/summationdisplay
k=1/radicalBig
Epn[/tildewideφ2
k]|θk|.(37)
8Published in Transactions on Machine Learning Research (01/2023)
where the bound is attainable when
[∆]k=/radicalBig
Epn[/tildewideφ2
k]y−/tildewideΦθ
||y−/tildewideΦθ||2.
But the bound is of the same form as the cost function in (14). Thus solving problem (14) implies the mini-
mization of (36). Theorem 1 in Xu et al. (2009) established a connection between a square-root maximization
problem in the form of (36) and the penalized form on the right-hand side of (37).
5 Numerical Experiment
In the previous sections we have showed several computational and theoretical properties of the predictor
function/hatwidey(x;λn)which we shall call the Spice-predictor. In this section we present a numerical experiment
for sake of illustration.
5.1 Setup
We observe a stream of nsamples generated by the following (unknown) process
x∼Uniform ([0,10]2),
y|x∼GP(0,k(x,x/prime) +σ2δ(x,x/prime)),(38)
where
k(x,x/prime) =σ2/parenleftbigg
1 +√
3
l/bardblx−x/prime/bardbl2/parenrightbigg
exp/parenleftbigg
−√
3
l/bardblx−x/prime/bardbl2/parenrightbigg
.
with noise variance σ= 2and scalel= 7. In other words, xis a two-dimensional covariate drawn from a
uniform distribution and yis drawn from a Gaussian process (GP) with zero mean and a Matérn covariance
function. A realization of the process above and ntraining data points are shown in Figures 1a and 1e.
We consider a class Fwithd= 100periodic feaatures {φk(x)}using the Laplacian eigenfunction basis (Solin
& Särkkä, 2020). Note that this corresponds to a misspeciﬁed covariance model (4). We are interested in
the online learning of predictors in F, and use the least-squares ( Ls) and ridge regression methods as
baseline references. Both methods can be implemented in an online fashion, but the latter requires ﬁxing a
regularization parameter. Here we simply set this parameter to 0.1 based on visual inspection.
5.2 Out-of-sample performance
For illustration, consider the predictions produced by the Ls, ridge regression and Spicemethods, see
Figure 1. As expected, the Lsprovides poor results at these sample sizes. Ridge regression with a ﬁxed
regularization parameter and Spicewith adaptively learned parameters appear to perform similarly here.
To evaluate their out-of-sample errors, we compare the Mseagainst that of the oracle predictor based on
the (oracle) Gaussian process repression ( Gpr) predictor in (38). Table 1 shows that the out-of-sample error
ofSpiceis lower than that of Lsand ridge regression, and that the chosen class Fis capable of predicting
the GP in (38) well.
Following the discussion of eﬀective degrees of freedom dfnin Ruppert et al. (2003), we also provide a
comparison between Ls,Spiceand the oracle Gprpredictors in Figure 2. While Lsattains the maximum
dfnatn= 100,Spicemoderates its growth rate in a data-adaptive and online manner. The degrees of
freedom of the oracle predictor increases gracefully and remains below its maximum value, even when n
increases beyond d.
5.3 Run-time
We report the runtimes of Ridge(since Lsis virtually identical) and Spice, and consider a well-speciﬁed
Gprpredictor with covariance parameters learned using the maximum likelihood method as reference. As
9Published in Transactions on Machine Learning Research (01/2023)
0 5 100246810
-2-101
(a)
0 5 100246810
-2-1.5-1-0.500.5 (b)
0 5 100246810
-1.5-1-0.50 (c)
0 5 100246810
0500010000 (d)
0 5 100246810
-2-101
(e)
0 5 100246810
-2-1.5-1-0.500.5 (f)
0 5 100246810
-1.5-1-0.500.5 (g)
0 5 100246810
-2024 (h)
Figure1: Contourplots. Firstcolumnshowsa realization of yin(38) alongwith samplingpatterns {xi}n
i=1∈
Xforn= 100andn= 250(top and bottom rows, respectively). Second, third and fourth columns show the
contour plots of the Spice-predictor, ridge regression and the Ls-predictor, respectively. All three predictors
belong toF.
Mse/Mse*
n Ls Ridge Spice
50 4.38×1041.71 1.11
100 21.12 1.47 1.09
250 1.47 1.19 1.06
500 1.11 1.06 1.02
Table 1: Mean-square error ( Mse) forLsandSpicemethods, normalized by Mse* of an oracle predictor
which is given the unknown covariance function in (38). For a given set of training data Dn, we compute
the averaged squared error over 250test points. The mean of this error is the Mseand was approximated
using 100diﬀerent realizations of Dn.
50 100 150 200020406080100120
Figure 2: Plot of degrees of freedom dfnagainst number of data points nforLs,Spiceand oracle GP
predictors.
can be seen in Table 2, the computational complexity of Gpris considerably higher than that of the online
alternatives. Ridgehas a lower run-time than Spicebut both perform as O(n). A visualization of the
trends for 0<n≤500is given in Figure 3.
10Published in Transactions on Machine Learning Research (01/2023)
Run times (ms)
n Gpr Ridge Spice
50 59 0.15 6.0
100 113.8 0.16 11.6
250 231.3 0.20 29.2
500 1507.2 0.22 58.6
Table 2: Run times for oracle Gpr,ridgeandSpice. Note that the run times are computed for a single
realization.
10210-1100101102103104
GP
Ridge
Spice
Figure 3: Plot of run time of a single run for learning parameters and prediction against versus of data
pointsnforridge,Spiceand oracle GP predictors.
5.4 Robustness
In Theorems 3 and 4 established two diﬀerent types of robustness results with respect to the features φ(x).
We therefore study the predictive performance when the training and test distributions over (φ,y)diverge.
Speciﬁcally, the covariates in the training data are drawn as /tildewidex|x∼N (x,σ2
xI), where xfollows (38). This
results in a distribution shift over the features (aka. errors-in-variables). Figure 4 evaluates the test Mse
when data drawn from (38). We see that the out-of-sample error for RidgeandSpiceincreases consistently
withσx, while Lsproduces very poor and inconsistent results. Spiceis notably more robust against this
distributional shift and these results corroborated the derived robustness properties.
6 Conclusion
We considered the problem of learning model-based linear smoothers online. If the model parameters were
ﬁxed, the resulting predictor – which includes Gaussian process regression and kriging methods – can readily
be computed sequentially. Since the model parameters unknown, however, they must be learned from data,
using, e.g., maximum likelihood or cross-validation methods. But implementing them when the data arrives
as a stream requires either recomputing the predictor which is computationally prohibitive or resorting to
approximations. In either case, these approaches do not oﬀer clear-cut results on the statistical properties
of the resulting predictor.
We applied a covariance-ﬁtting method to learn the model parameters, which was initially developed for
spectral estimation. We ﬁrst used its computational properties to show that the resulting predictor can be
computed sequentially. We then derived ﬁnite out-of-sample performance guarantees of the resulting predic-
tor and showed that its error approaches the minimum achievable level at root- nrate. Finally, we established
connections to the distributional robustness literature by showing that the predictor is robust against distri-
11Published in Transactions on Machine Learning Research (01/2023)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 111.522.533.5
LS
Ridge
Spice
Figure 4: Out-of-sample error when the distribution of training data diverges from that of the test data. Mse
(normalized by minimal Mse*) versus perturbation noise level σxon training data ( n= 100). Evaluation
based on 500Monte Carlo runs.
butional uncertainties and errors in the covariate training data. The performance, computational complexity
and robustness of the proposed method were illustrated in a numerical experiment.
Acknowledgments We thank the anonymous reviewers from TMLR for their constructive feedback. This
research was partially supported by the Swedish Research Council under contracts 2018-05040 and 2021-
05022, Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by Knut and Alice
Wallenberg Foundation, and by Kjell och Märta Beijer Foundation.
References
Alexandre Belloni, Victor Chernozhukov, and Lie Wang. Square-root lasso: pivotal recovery of sparse signals
via conic programming. Biometrika , 98(4):791–806, 2011.
Christopher Bishop. Pattern recognition and machine learning . Springer, 2006.
Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein proﬁle inference and applications to
machine learning. Journal of Applied Probability , 56(3):830–857, 2019.
Thang D Bui, Cuong Nguyen, and Richard E Turner. Streaming sparse gaussian process approximations.
Advances in Neural Information Processing Systems , 30, 2017.
T Tony Cai and Ming Yuan. Minimax and adaptive prediction for functional linear regression. Journal of
the American Statistical Association , 107(499):1201–1216, 2012.
John Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust
optimization. arXiv preprint arXiv:1810.08750 , 2018.
Eitan Greenshtein and Ya’Acov Ritov. Persistence in high-dimensional linear predictor selection and the
virtue of overparametrization. Bernoulli , 10(6):971–988, 2004.
Trevor Hastie, Robert Tibshirani, and Jerome H Friedman. The elements of statistical learning: data mining,
inference, and prediction , volume 2. Springer, 2009.
James Hensman, Nicolas Durrande, and Arno Solin. Variational fourier features for gaussian processes. J.
Mach. Learn. Res. , 18(151–1), 2017.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In NIPS, volume 3, pp.
5. Citeseer, 2007.
12Published in Transactions on Machine Learning Research (01/2023)
Carl Edward Rasmussen and Chris Williams. Gaussian processes for machine learning , volume 1. MIT press
Cambridge, 2006.
David Ruppert, Matt P Wand, and Raymond J Carroll. Semiparametric regression . Number 12. Cambridge
university press, 2003.
Arno Solin and Simo Särkkä. Hilbert space methods for reduced-rank gaussian process regression. Statistics
and Computing , 30(2):419–446, 2020.
Samuel Stanton, Wesley Maddox, Ian Delbridge, and Andrew Gordon Wilson. Kernel interpolation for
scalable online gaussian processes. In International Conference on Artiﬁcial Intelligence and Statistics ,
pp. 3133–3141. PMLR, 2021.
Michael L Stein. Interpolation of spatial data: some theory for kriging . Springer Science & Business Media,
2012.
Petre Stoica and Octavian Stanasila. Some spectral properties of the matrix b/a. Bul. Inst. Politehnic
Bucuresti, ser. Electro. , 44(3):3–8, 1982.
Petre Stoica, Prabhu Babu, and Jian Li. New method of sparse parameter estimation in separable models
and its use for spectral analysis of irregularly sampled data. IEEE Transactions on Signal Processing , 59
(1):35–47, 2010a.
Petre Stoica, Prabhu Babu, and Jian Li. Spice: A sparse covariance-based estimation method for array
processing. IEEE Transactions on Signal Processing , 59(2):629–638, 2010b.
Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In Artiﬁcial intel-
ligence and statistics , pp. 567–574. PMLR, 2009.
Larry Wasserman. All of nonparametric statistics . Springer Science & Business Media, 2006.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. In Advances in Neural
Information Processing Systems , pp. 1801–1808, 2009.
Dave Zachariah and Petre Stoica. Online hyperparameter-free sparse estimation method. IEEE Transactions
on Signal Processing , 63(13):3348–3359, 2015.
A Appendix: Sequential computation
Here we provide a pseudocode for computing /hatwidey(x;λn)≡φ/latticetop(x)θnsequentially. This is accomplished via a
cyclic minimization of the convex problem (14).
First, deﬁne the kth column as ck= [Φ]kand/tildewideyk=y−/summationtext
j/negationslash=kcjθj. Then the cost function in (14) can be
equivalently expressed as
V(θk) = (/bardbl/tildewideyk−ckθk/bardbl2
2)1/2+ψk|θk|+Ck, (39)
and minimized cyclically, one coordinate k= 1,2,...,dat a time. It was shown in (Zachariah & Stoica,
2015) that the minimizer of (39) is given by
/hatwideθk=/braceleftBigg
skrk,if√n−1γk>/radicalbig
αkβk−γ2
k
0,else(40)
where
αk=/bardbl/tildewideyk/bardbl2
2, βk=/bardblck/bardbl2
2, γk=|c/latticetop
k/tildewideyk| (41)
and
sk=sign(c/latticetop
k/tildewideyk), rk=γk
βk−1
βk/parenleftbiggαkβk−γ2
k
n−1/parenrightbigg
(42)
13Published in Transactions on Machine Learning Research (01/2023)
The key observation here is that the variables (41) are all expressible using the recursively computable
quantities in (15) (along with a prior parameter iterate θ/primewhich is initialized at 0).
To arrive at this conclusion, deﬁne the following global variables
z=y−Φθ/prime, g 0=/bardblz/bardbl2
2,g=Φ/latticetopz (43)
which can be expressed using (15). Then we have that /tildewideyk≡z+ckθ/prime
k, so that we can express (41) in terms
of the global variables:
αk=g0+An
kkθ2
k+ 2gkθ/prime
k, βk=An
kk, γk=|gk+An
kkθ/prime
k|, sk=sign(gk+An
kkθ/prime
k) (44)
Finally we can express the cyclic minimization approach in recursive form as outlined in Algorithm 1,
initializingθ/prime=0atn= 1. Additional computational considerations are developed in Sec. III of (Zachariah
& Stoica, 2015). Link to implementation is provided here: https://github.com/Muhammad-Osama .
Algorithm 1 :Spice-predictor
1:Input: (xn,yn)andx(and initialθ/prime)
2:Update recursive variables An,bnandcnin (15)
3:Set global variables g0=cn+θ/prime/latticetopAnθ/prime−2θ/prime/latticetopbnandg=bn−Anθ/prime
4:repeat
5:k= 1,...,d
6:Computeαk,βk,γkandskin (44)
7:Compute/hatwideθkin (40)
8:Update global variables g0:=g0+An
kk(θ/prime
k−/hatwideθk)2+ 2(θ/prime
k−/hatwideθk)gkandg:=g+ [An]k(θ/prime
k−/hatwideθk)
9:Updateθ/prime
k:=/hatwideθk
10:untilnumber of iterations equals L
11:Output:/hatwidey(x) =φ/latticetop(x)/hatwideθ
14