Published in Transactions on Machine Learning Research (09/2023)
Quantization Robust Federated Learning for Efficient Infer-
ence on Heterogeneous Devices
Kartik Gupta†∗, Marios Fournarakis‡, Matthias Reisser‡, Christos Louizos‡, Markus Nagel‡
†Australian National University,‡Qualcomm AI Research
†kartik.gupta@anu.edu.au
‡{mfournar,mreisser,clouizos,markusn}@qti.qualcomm.com
Reviewed on OpenReview: https://openreview.net/forum?id=lvevdX6bxm
Abstract
Federated Learning ( FL) is a machine learning paradigm to distributively learn machine
learning models from decentralized data that remains on-device. Despite the success of
standard Federated optimization methods, such as Federated Averaging ( FedAvg ) inFL,
the energy demands and hardware induced constraints for on-device learning have not been
considered sufficiently in the literature. Specifically, an essential demand for on-device learn-
ing is to enable trained models to be quantized to various bit-widths based on the energy
needs and heterogeneous hardware designs across the federation. In this work, we introduce
multiplevariantsoffederatedaveragingalgorithmthattrainneuralnetworksrobusttoquan-
tization. Such networks can be quantized to various bit-widths with only limited reduction
in full precision model accuracy. We perform extensive experiments on standard FLbench-
marks to evaluate our proposed FedAvg variants for quantization robustness and provide
a convergence analysis for our Quantization-Aware variants in FL. Our results demonstrate
that integrating quantization robustness results in FLmodels that are significantly more
robust to different bit-widths during quantized on-device inference.
1 Introduction
Federated Learning ( FL) is a distributed machine learning paradigm, where a large number of clients, such
as consumer smartphones, personal computers or smart home devices learn collaboratively. Clients train
on their private local data, which is never shared with other participants in the federation, such as other
clients or the server. Despite learning happening on-device, FLresults in a single global model at the end
of training. The privacy of local client data is an important requirement in modern machine learning and
acts as a central motivator for FL.
Several challenges arise in the FLsetting. For example, different clients might have different computational
constraints based on their hardware design specifications. One practically relevant heterogeneous hardware
characteristic is the supported quantization bit-width of the hardware accelerator. A suitably trained model
should exhibit no significant performance degradation after quantization to various bit-widths represented
in the heterogeneous device landscape. Quantization robustness aims to achieve this objective where a single
model is trained with the constraint of robustness to various quantization bit-widths on-the-go at inference
time without re-training or finetuning.
Recent works (Shkolnik et al. (2020); Alizadeh et al. (2020); Kim et al. (2020)) introduced novel ways to
train quantization robust models in the centralized training setting, but the application of such quantization
robustness mechanisms has not been a focus in FLyet. In this work, we address the problem of learning
quantization robust models trained using the standard federated learning algorithm, known as Federated
∗Work done during internship at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies,
Inc. and/or its subsidiaries.
1Published in Transactions on Machine Learning Research (09/2023)
Averaging ( FedAvg ) (McMahan et al. (2017)). To this end, we introduce multiple variants of FedAvg
algorithms to incorporate quantization robustness in FL; this is done either via regularization-based meth-
ods for quantization robustness or modified quantization-aware training methods. Firstly, we propose the
integration of a standard quantization robustness approach known as Kurtosis Regularization ( KURE),
which involves a regularization term in the local clients’ loss function, to enforce uniform distribution on
the weights and activations. Secondly , we present a quantization robustness approach that involves adding
random pseudo-quantization noise during the training procedure. The adopted mechanism is also inspired
by the introduction of additive pseudo-quantization noise (Défossez et al. (2021)) for QATthat discards
the Straight Through Estimator ( STE) approximation (Courbariaux et al. (2015)) for non-differentiable
uniform quantization. Straight Through Estimator ( STE) approximation is de-facto method in QAT, that
allows the backpropagation through a non-differentiable quantization operator by assuming it as an identity
function.
Quantization-Aware Training ( QAT) methods (Zhou et al. (2016); Krishnamoorthi (2018); Esser et al.
(2020); Nagel et al. (2021)) have been successful at training quantized models with ultra-low bit-widths.
QATmodels perform very well for the target bit-width they have been trained on but can lead to significant
degradation for other bit-widths, even for full precision (Kim et al., 2020). To address this limitation of
conventional QATmethods, we further introduce Multi-bit Quantization-Aware Training ( MQAT), a novel
QATframework that achieves quantized models robust to multiple bit-widths without re-training.
InMQAT forFL, a random bit-width is sampled for each client from the set of considered quantiza-
tion bit-widths before performing a standard QATprocedure during the client training phase. This small
modification enables models trained using the federated regime to be robust to multiple bit-widths during
quantized inference. Furthermore, since QATinvolves certain heuristics ( STE) for computing gradients
(due to the non-differentiable rounding operation), we theoretically analyse the convergence behaviour of
the global model in the non-convex setting when clients perform local QAT.
Below we summarize the contributions in this paper:
•We introduce multiple quantization robustness methods such as Kurtosis Regularization ( KURE), Addi-
tive Pseudo-Quantization Noise ( APQN) into the federated learning setup to achieve quantization robust
models that can be used for efficient inference at multiple bit-widths.
•As the standard form of QATintegration into federated learning fails to generalise across multiple bit-
width, weproposeMulti-bitQuantization-AwareTraining( MQAT),anewvarianttoachievequantization
robust models in FL.
•We study the theoretical convergence properties for our QATvariants in federated learning and show
that the convergence rate for the server-side weights is similar to traditional FL, albeit with a QAT-
method-specific error floor.
•We perform extensive experimental evaluations of baselines and our FedAvg variants on CIFAR-10 ,
CIFAR-100 ,FEMNIST and TinyImageNet with different network architectures. We empirically demon-
strate that our proposed modifications yield models that are robust to quantization at multiple bit-widths
without significant reduction on the model’s full-precision accuracy.
2 Preliminaries
We first provide some brief background on FedAvg and neural network quantization robustness.
2.1 Federated Averaging
Federated learning is a distributed learning paradigm where multiple clients collaboratively learn a shared
model. In this machine learning framework, the local client data is not shared with other clients or the
server. The problem of federated learning can be formulated as an optimization objective
min
wF(w) =Ei∼P[Fi(w)], (1)
2Published in Transactions on Machine Learning Research (09/2023)
whereFi(w,Di) =Eξ∼Di[fi(w,ξ)]is the the local objective function at client i,w∈RDrepresents the
parameters for the global model, and Pdenotes a distribution over the population of clients I. The local
data distribution Dioften varies between clients, resulting in data heterogeneity.
Federated Averaging ( FedAvg ) (McMahan et al. (2017)) is the standard algorithm for federated learning.
It operates via a series of roundswhere each round is divided into a client update phase and server update
phase. We denote the total number of rounds as T. At the beginning of each round t, a subset of clients
Stis sampled from the pool of clients. The server model is then shared with the sampled clients. During
the client update phase, each sampled client runs local SGDforKsteps with learning rate ηcusing their
own private data. We denote the i-th client’s parameters at the k-th local step of the t-th round by wi
t,k.
During the server update phase, the updates of the sampled clients are averaged to build the server-side
update ∆t. The server then applies that update with learning rate ηsto receive the next round’s parameters
wt+1. Reddi et al. (2020) describe a generalization of the server-side update rule to include more advanced
adaptive optimizers.
2.2 Quantization Robustness
The objective for robust quantization is to learn a single model that can be quantized to different bit-widths
without significant degradation in the full precision performance. Given a neural network parameterized by
wthat is optimized using a standard loss function F, such as the cross entropy, quantization robustness
aims at minimizing the following loss:
min
w/summationdisplay
b∈BF(Q(w,b),D). (2)
Here, quantizer Q(·)with bit-width band a quantization step size ∆b, we have that
Q(w,b) = ∆b·clip/parenleftbigg/floorleftbiggw
∆b/ceilingrightbigg
,−2b−1,2b−1−1/parenrightbigg
, (3)
where⌊·⌉denotes the rounding-to-nearest integer operation, and clip (·)clamps its input such that it lies in
the range [−2b−1,2b−1−1]. The quantization step size can be estimated either post-training or learnt using
QAT(Krishnamoorthi (2018); Esser et al. (2020); Nagel et al. (2021)). The above objective intends to learn
a neural network that is robust to various bit-widths in the quantization bit set B.Bcould also include
the high precision 32-bit floating-point format (FP32). Note that the above formulation explicitly enforces
robustness to different bit-widths for weight quantization only. It is straightforward to enforce quantization
robustness for activations in a similar manner. Recent works (Alizadeh et al. (2020); Shkolnik et al. (2020))
have exploredalternatewaysof satisfyingtheaboveobjectiveby addinga regularizationtermin thestandard
training procedure instead of directly solving the aforementioned optimization problem.
3 A Federated Learning Framework with Quantization Robustness
In quantization robust federated learning we aim to solve an optimization problem of the following form:
min
wF(w) =Ei∼P[F∗
i(w)], (4)
whereF∗
i(w,Di) =Eξ∼Di/summationtext
b∈B[fi(Q(w,b),ξ)]is a modified local loss to encourage quantization robustness
at clientiandBis the set of target quantization bit-widths. Instead of directly optimizing this loss, which
involvesmultipleforward-backwardpassesthroughthesamebatchforeachbit-width, weincorporatevarious,
more efficient ways for quantization robustness in the FedAvg framework.
RegularizationMethod. RegularizationmethodssuchasKurtosisregularization(Shkolniketal.(2020)),
which enforce a uniform distribution on the weight tensors, can be incorporated in the FedAvg framework
by modifying the loss function Fifor each client as follows:
F∗
i(w,Di) =Eξ∼Di[fi(w,ξ)] +λ·LKURE (w). (5)
3Published in Transactions on Machine Learning Research (09/2023)
The Kurtosis regularization term for an M- layered neural network can be expressed as
LKURE =1
MM/summationdisplay
i=1|K(wi)−Kτ|2,K(w) =E/bracketleftbigg/parenleftbiggw−µ
σ/parenrightbigg4/bracketrightbigg
. (6)
Here,µandσare the mean and standard deviation of tensor wandKτdenotes a scalar value that defines the
distribution enforced on the tensors. Similar to Shkolnik et al. (2020), we employ Kτ= 1.8to enforce uniform
distribution. We provide the pseudo-code for FedAvg with Kurtosis regularization, FedAvg -KURE, in
Algorithm 1.
Algorithm 1 FedAvg ,FedAvg -KURE
1:Require (w0,ηc,ηs,λ)
2:fort= 0,...,T−1do
3:sample a subsetSof clients
4:for alli∈Sin parallel do
5: wi
t,0←wt{broadcast server state to client}
6:fork= 0,...,K−1do
7:gi
t,k←∇fi(wi
t,k;ξm
t,k)
8:f∗
i(wi
t,k;ξm
t,k) =fi(wi
t,k;ξm
t,k)+λ·LKURE(wi
t,k)
9:gi
t,k←∇f∗
i(wi
t,k;ξm
t,k)
10: wi
t,k+1←wm
t,k−ηc·gi
t,k{client update}
11:end for
12:end for
13: ∆t=1
|S|/summationtext
i∈S(wi
t,K−wi
t,0)
14: wt+1←wt+ηs·∆t{server update}
15:end for
Additive Pseudo-Quantization Noise (APQN). The quantization robustness problem has similarities
with adversarial robustness in the sense that both aim to keep predictions unaltered in the presence of some
form of bounded additive noise. Adversarially robust models aim to be robust to noised-up input, whereas
with quantization robustness the noise is added to either the weight tensor or the intermediate activations.
We present a quantization robustness approach that involves adding random pseudo-quantization noise
during the training procedure. This is motivated by the recent success of Randomized Smoothing (Cohen
et al. (2019)) in the adversarial robustness literature, where a model is learnt with input data samples
corrupted with Gaussian noise. An adaptation similar to ours has been presented in the recently introduced
Differentiable Quantizer (Défossez et al. (2021)) as a replacement of the commonly used Straight Through
Estimator ( STE) based quantizer. Their proposed quantizer has only been used to achieve quantized models
for a single target bit-width or mixed-precision with fixed computational budget. Since APQNinvolves
additive pseudo quantization noise and the noise does not have any learnable parameters, backpropagation
for the rest of the parameters is straightforward.
We propose to learn models that are robust to varying levels of quantization noise and thus can be quantized
to different bit-widths. The local loss function in this case can be reformulated as
F∗
i(w,Di) =Eξ∼Di[fi(˜Q(w,b),ξ)]. (7)
Here, ˜Q(·)is a pseudo-quantizer with bit-width bthat adds noise sampled from the uniform distribution
U[−∆b/2,∆b/2], and can be defined as
˜Q(w,b) =w+U/bracketleftbigg
−∆b
2,∆b
2/bracketrightbigg
. (8)
Provided that ∆bis sufficiently large, the sampled pseudo-quantization noise can have support for vari-
ous target bit-widths, thus encouraging quantization robustness. The pseudo-code for FedAvg -APQNis
provided in Algorithm 2.
4Published in Transactions on Machine Learning Research (09/2023)
QAT and MQAT. A standard way of learning a network with low bit-widths is to constrain the parame-
ters and/or activations of the model to fixed quantization levels. This procedure of training a neural network
with standard Projected Gradient Descent ( PGD) algorithm with quantization function as projection, is of-
ten termed Quantization-Aware Training ( QAT). To perform “quantization-aware” FL, we can adopt the
QATprocedure for the local optimization at each client to learn a global model that can be quantized to a
specific bit-width. In this “quantization-aware” FL, the client-level loss function can be reformulated as:
F∗
i(w,Di) =Eξ∼Di[fi(Q(w,b),ξ)]. (9)
The quantization step-size ∆bcan be either learnt as a parameter (Esser et al. (2020)) or be estimated
before the start of training and kept fixed thereafter. Note that the backward pass of the network involves
a gradient estimate through the non-differentiable rounding operation of Q(·). To this end, similar to prior
QATliterature we use the standard STEapproximation (Bengio et al. (2013)), which approximates the
gradient of the rounding operator as 1:
∂⌊x⌉
∂x= 1. (10)
By using this STEapproximation for QAT, we can train models that can be quantized to specific bit-widths.
In order to use the trained model for bit-widths other than b, we can analytically estimate the quantization
step-size using the ranges for bit-width bas follows:
∆a=2b−1
2a−1∆b. (11)
Here,arefers to any target bit-width during inference stage and brefers to bit-width for which the model is
trained on. The pseudo-code for FedAvg -QATis provided in Algorithm 2.
Although QATtrains models that perform favourably at specific bit-widths, they often suffer from perfor-
mance degradation when quantizing to other bit-widths (Kim et al. (2020)). For this reason, QATalone is
not suitable for tackling the heterogeneous hardware requirements that one can encounter in a cross-device
FLdeployment. We propose MQATto realize quantization-robust FL.MQATaims to learn models that
are robust to a set of bit-widths B, by selecting a bitwidth b∈Beither randomly during local training or
by fixing it based on each client’s hardware requirements. The sampled bit-width is then kept same for all
the layers.
Similar to QAT, the quantization step-size ∆bfor different bit-widths can be either learnt or estimated
before the start of training and kept fixed thereafter. The quantization step-size for different bit-widths is
then shared along the model parameters with all clients. We provide the pseudo-code for FedAvg -MQAT
in Algorithm 2.
Convergence analysis. APQN,QATandMQAT modify the forward pass of the model, either by
adding uniform noise in the case of APQNor approximating the gradient of the non-differentiable rounding
operation in the case of QATandMQAT. Therefore, it is important to study how these modifications /
heuristics affect the convergence behaviour of the global model in the federated setting. We do not discuss
the convergence behaviour of KURE, asKUREuses standard unbiased gradients of a regularized objective
and therefore the standard FedAvg guarantees apply for this objective. What follows now is a convergence
analysis in the non-convex setting with the help of the following assumptions:
Assumption 1 (Lipschitz Gradient) .Each local loss function FsisL-smooth∀s∈S,i.e.,∥∇Fs(x)−
∇Fs(y)∥≤L∥x−y∥,∀x,y∈I RD.
Assumption 2 (Bounded variance) .EachFshas bounded local variance, i.e.,E[∥∇fs(w,ϵ)−∇Fs(w)∥2]≤
σ2
l, wherefsis a stochastic estimate of the local loss based on a w∈I RDandϵis a random mini-batch.
Furthermore, the global variance is also bounded, i.e.,1
S/summationtext
s∥∇Fs(w)−∇F(w)∥2≤σ2
g,∀w∈I RD.
Assumption 3 (Bounded quantization noise) .Letw∈I RD,jbe any of its dimensions and Q(·)the
quantization operation with a step size ∆∈I R. The quantization noise rj∈I Radded towj,i.e.,rj=
Q(wj)−wj, is bounded by half the step size of Q(·),i.e.,rj≤∆
2.
5Published in Transactions on Machine Learning Research (09/2023)
Algorithm 2 FedAvg -APQN,FedAvg -QAT, and FedAvg -MQAT
1:Require (w0,ηc,ηs,b,B)
2:fort= 0,...,T−1do
3:sample a subsetSof clients
4:for alli∈Sin parallel do
5: wi
t,0←wt{broadcast server state to client}
6:b′←U[B]
7:fork= 0,...,K−1do
8:gi
t,k←∇wfi(˜Q(wi
t,k,b);ξm
t,k)
9:gi
t,k←∇wfi(Q(wi
t,k,b);ξm
t,k)
10:gi
t,k←∇wfi(Q(wi
t,k,b′);ξm
t,k)
11: wi
t,k+1←wm
t,k−ηc·gi
t,k{client update}
12:end for
13:end for
14: ∆t=1
|S|/summationtext
i∈S(wi
t,K−wi
t,0)
15: wt+1←wt+ηs·∆t{server update}
16:end for
The first two assumptions are common in the non-convex optimization literature for FL(Reddi et al.
(2020)) and the third is automatically satisfied in our quantization schemes, provided the ranges are set
up appropriately. Based on these assumptions, we can then make the following statement.
Theorem 1. LetKbe the local iterations of each client and w∈I RDbe the global model parameter vector.
Under assumptions 1, 2 and 3, if the client ( ηc) and server ( ηs) learning rates are chosen such that
ηc≤1
10LK, ηc≤1
8LKηs, (12)
we have that the FedAvg-{APQN,QAT,MQAT} server updates satisfy
min
1≤t≤T∥∇F(wt)∥2≤F(w1)−F(w∗)
TηsηcA
+ηc
ηsA(Bσ2
l+ ΓKσ2
g+HL2DR2), (13)
where we define
A=K
4−2LηsηcK2, (14)
B= 4ηsηcK2L2+Lη2
s/parenleftbigg
2K2+K
6/parenrightbigg
, (15)
Γ = 24ηsηcK2L2+Lη2
sK, (16)
H=4ηs
3ηcK+ 6Lη2
sK2(17)
andR=∆b√
12forAPQN,R=∆b
2forQATandR=/radicalbigg
Eb/bracketleftig∆2
b
4/bracketrightig
forMQAT.
The proof is delegated to appendix due to space constraints. It follows Reddi et al. (2020) while handling
quantization noise, such as Li et al. (2017). We can see that the convergence rate for the (non-quantized)
server side weights is similar to traditional FL(Reddi et al. (2020)), albeit with an additional error floor due
to the quantization noise. Some of this quantization error can be reduced by decreasing the learning rates,
up to an irreducible factor of O(L2DR2)that depends on the bit-width. Both the convergence term, i.e.,
F(w1)−F(w∗), and the the error term scale with the number of local iterations K, with the first decaying
withKbut for the second some of the error terms increase with K, due to each step contributing additional
quantization noise and drift between the local and the server weights.
6Published in Transactions on Machine Learning Research (09/2023)
4 Related Work
Post-training quantization. PTQ is fast and efficient way of achieving neural network quantization by
using little or no data at inference time. Recent literature in PTQ focused on post-training quantization
of LLMs and transformers. Frantar & Alistarh (2022) extend the Optimal Brain Surgeon (OBS) frame-
work to efficiently quantize and prune NNs in a unified setting. Their method is time and space-efficient
while achieving high accuracy in vision and language models. Frantar et al. (2023) propose an extension
of OBC Frantar & Alistarh (2022) that is optimized for efficient and accurate quantization of generative
pretrained models. This one-shot weight quantization method can quantize GPT models with 175 billion
parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with
negligible accuracy degradation relative to the uncompressed baseline. Liu et al. (2023a) address the issue of
quantizing heavy-tailed activations in vision transformers. They discover that for a given quantizer adding
a fixed uniform noisy bias to the values being quantized can significantly reduce the quantization error. By
adding a noisy bias to each layer they are able to actively alter the activations distribution and make it
more quantization-friendly. Yao et al. (2022) proposes a method for efficient and accurate PTQ of large-
scale transformers. It comprises of a fine-grained hardware-friendly quantization scheme for both weight
and activations employing layer-by-layer knowledge distillation algorithm (LKD) without the access to the
original training data. ZeroQuant can achieve 8-bit weight/activation quantization of GPT-3-style models
with minimal accuracy impact.
Quantization-aware training. QATis one of the more effective and widely used methods for achieving
low-bit weight and activation quantization. It relies on simulating the quantization operation operation
during training and requires access to labelled training data. Esser et al. (2020) first introduced the idea of
learning the quantization step-size jointly with the weight achieving near floating-point accuracy in ResNets
even with 3-bit quantization. Since then, further advances in quantization-aware training (Han et al., 2021;
J. Lee, 2021; Gong et al., 2019; Bhalgat et al., 2020; Park & Yoo, 2020) have pushed the envelop and enabled
ultra low-bit quantization ( 2-4bits) for a wide range of networks and tasks. Recently, Nagel et al. (2022)
observe that oscillating latent weights can prevent neural networks from converging to optimal solutions
during QAT. They propose freezing oscillating weights or oscillation dampening through regularization and
thus improve quantized accuracy in efficient ConvNets. Shin et al. (2023) propose training with pseudo-noise
quantization to prevent unstable convergence induced by the straight-through-estimator (STE) in QAT. The
NIPQ formulation allows for naturally learning the bitwidth and quantization parameters leading to more
accurate and efficient mixed-precision quantized NNs. Liu et al. (2023b) investigate QAT for LLMs and
propose a data-free distillation method that leverages generations produced by the pre-trained model, which
better preserves the original output distribution and allows quantizing any generative model independent
of its training data, similar to PTQ methods. They experiment with LLaMA models of sizes 7B, 13B, and
30B, at quantization levels down to 4 bits.
Quantization robustness. A drawback of QATis that it can make the trained model highly dependent
on the chosen bit-width and quantization parameters. To address this, robust quantization aims at training
a single set of weights that are robust to a wider range of quantization choices and bit-widths. Alizadeh
et al. (2020) model quantization noise as an additive perturbation and show that they can improve quan-
tization robustness by regularizing the L1-norm of gradients. Since this type of gradient regularization is
computationally expensive due to the second-order gradient information, a simpler alternative regularization
procedure was proposed in follow-up work (Shkolnik et al., 2020). Shkolnik et al. (2020) trains the network
with Kurtosis Regularization ( KURE) on the weights to improve robustness and use the LAPQ (Nahshan
et al., 2020) algorithm to find the optimal quantization parameters post-training. It has been shown in Shkol-
nik et al. (2020) that the uniform distributions on the weight tensors achieve better quantization robustness,
instead of the Gaussian-like distributions attained during the standard training procedure. Recently, Défos-
sez et al. (2021) introduce additive uniform noise to simulate quantization during training for the purpose
ofQAT; this method can be easily extended for the purpose of quantization robustness as we show in this
paper. Kim et al. (2020) introduces a training method that leads to more “quantization-friendly” weights,
by scaling the gradient depending on the distance of the weights from the quantization grid.
7Published in Transactions on Machine Learning Research (09/2023)
Quantization in Federated Learning (FL). Quantization in the context of FLhas been studied mostly
inthecontextofcompressingcommunication, e.g.,(Amirietal.,2020;Reisizadehetal.,2020;Triastcynetal.,
2021). Equally important to reducing the communication overhead is the reduction of the computational cost
of training and inference on-device. Existing work in this direction can roughly be divided into designing
more efficient models through e.g.sparsification (Caldas et al., 2018b; Jiang et al., 2019; Louizos et al.,
2021) and more effective algorithms (Reddi et al., 2020; He et al., 2020) that reduce the number of training
rounds. Structurally sparse models can reduce training and inference costs (Horvath et al. (2021) targets
heterogeneous computational resources) and may additionally reduce communication (Louizos et al., 2021).
Diao et al. (2021) proposes to assign to each client a subset of the global model depending on their resources.
A weaker client receives only a subset of hidden layers and as such, it is suited for training as well as
inference across a heterogenuous compute landscape. Such an approach is orthogonal to MQAT, similar to
how sparsification and quantization are being successfully combined in federated training and centralized
settings alike. Quantization, the focus of this work, is orthogonal to these and as of yet understudied in
the literature. QuPeL (Ozkara et al., 2021) is very close in scope to our work. A core proposition of their
work however is the flexibility of the quantization mechanism to be non-uniform and the set of quantized
values that the model weights can occupy to be learnable. Such a method is highly performant in theory
but is entirely unsuitable to low-bit accelerators as can be found in today’s hardware. Furthermore, QuPeL
addresses hardware heterogeneity through personalization, meaning that each client needs to have access a
local dataset and perform a finetuning operation in order to select the appropriate centroids for their budget.
This is in contrast to our work where the quantization happens “zero-shot”, i.e., the client does not need
any data (i.e., it could be a new client in the federation, only interested in inference) and can just quantize
the server model to a specific, hardware friendly, bit-width. To the best of our knowledge, this work is the
first to investigate the robustness of models to different levels of quantization in the FLsetting.
5 Experiments
We evaluate the quantization robustness of different proposed variants of FedAvg using standard bench-
marks in FL. In this work, we mainly focus on weight quantization robustness of various trained models but
we also provide additional experimental comparisons for activation quantization and quantizing both weight
and activations. For all the results in the paper, we present the accuracy plots at different quantization
bit-widths and we refer the reader for exact numbers to Appendix.
Experimental Setup. For the experimental comparisons, we use federated versions of the CIFAR-10 ,
CIFAR-100 (Krizhevsky et al., 2009), TinyImageNet1andFEMNIST (Caldas et al., 2018a) datasets. We split
the data into 100 ( CIFAR-10 ), 500 ( CIFAR-100 ), 500 (TinyImageNet), 3500 ( FEMNIST ) clients in a non-i.i.d
way following Hsu et al. (2019), where in each round only 10 clients participate for all datasets except
TinyImageNet dataset, where 100 clients participate. We train different models for 5000 ( CIFAR-10 using
ResNet-20 ), 2000 ( CIFAR-10 using LeNet-5), 10000 ( CIFAR-100 ), 4500 (TinyImageNet), and 6000 ( FEMNIST )
rounds. We use small local batch sizes for all clients in our experiments for different datasets: 64 ( CIFAR-
10), 20 ( CIFAR-100 , TinyImageNet, FEMNIST ). For all our experiments, we use ADAM optimizer for server
training phase and SGDoptimizer for client training phase. We use single epoch of local client training for
each client participating in a round for all our experiments. For data augmentation, we normalize CIFAR-
10and CIFAR-100 and TinyImageNet to per-channel zero-mean and standard deviation of one. CIFAR-100
further undergoes random cropping to 28pixels height and width with zero-padding, followed by random
horizontal flipping with 50%probability. FEMNIST requires no preprocessing.
In order to simulate a non-i.i.d. data split that we would expect in the federated scenario, we artificially
split CIFAR-10 ,CIFAR-100 and TinyImageNet by their label. For CIFAR-10 and TinyImageNet, the label
proportions on each client are computed by sampling from a Dirichlet distribution with α−1.0(Hsu et al.
(2019)). For CIFAR-100 we use the coarse labels provided with the dataset and follow Reddi et al. (2020).
For our FEMNIST experiments, the federated split is naturally determined by the writer-id for each client.
1https://tiny-imagenet.herokuapp.com/
8Published in Transactions on Machine Learning Research (09/2023)
32 8 6 4 3 2
Quantization Bit-Width10203040506070Global Validation Accuracy
CIFAR-10 ResNet-20
(a)
32 8 6 4 3 2
Quantization Bit-Width10203040506070Global Validation Accuracy
CIFAR-10 LeNet-5
HeteroFL (b)
32 8 6 4 3 2
Quantization Bit-Width2030405060Global Validation Accuracy
CIFAR-10 CCT (c)
32 8 6 4 3 2
Quantization Bit-Width01020304050Global Validation Accuracy
CIFAR-100 ResNet-20
Baseline
FedAvg-KURE
FedAvg-APQN (W-4)
FedAvg-APQN (W-2)
FedAvg-QAT (W-4)
FedAvg-QAT (W-2)
FedAvg-MQAT-W
(d)
32 8 6 4 3 2
Quantization Bit-Width010203040Global Validation Accuracy
TinyImageNet ResNet-18 (e)
Figure 1: Global validation accuracy of the proposed FedAvg variants at different bit-widths of weight
quantization for models trained on (a) CIFAR-10 using ResNet-20 , (b) CIFAR-10 using LeNet-5, (c) CIFAR-
100using ResNet-20 , and (d) TinyImageNet using the ResNet-18 architecture. Here, an abbreviation of “W”
indicates that we performed weight-quantization only, whereas “W- #” refers to quantization at #bits.
We use LeNet-5,ResNet-20 and Compact Convolutional Transformers (CCT) (Hassani et al., 2021) archi-
tecture for evaluation on CIFAR-10 and LeNet-5onFEMNIST . For experiments with CIFAR-100 and Tiny-
ImageNet, we use ResNet-20 and ResNet-18 network architectures respectively. We replace batch-norm with
group-norm in ResNet-20 since batch-norm induces training instabilities in FL(Reddi et al., 2020). We
report the accuracy of the quantized server-side model whenever comparing the models at different quanti-
zation bit-widths. To compare quantization robustness, we use bit-widths from the set {32,8,6,4,3,2}. We
provide the details regarding training hyperparameters in Appendix. For the APQNandKUREmethods,
we employ post-training quantization methods to set the quantization ranges. Specifically, we find ranges
that minimize the mean-squared error (Nagel et al. (2021)). We found that the “Scaler” without “static
Batch Normalization” lead to unstable training in LeNet-5. For our HeteroFL (Diao et al., 2021) implemen-
tation, we therefore report results without Scaler or Batch Normalization. Batch Normalization negates the
scaler’s effect, and the absence of Batch Normalization causes the instability. For QATbased methods,
range estimation is done at the start of the training and then the same ranges are used at inference time.
Our experiments are performed using NVIDIA Tesla V100 GPUs and code is in PyTorch.
5.1 Weight Quantization
Firstly, we compare all weight-only quantization-robust FedAvg variants introduced in this work, i.e.,
we keep the activations in full precision. We report validation accuracy at the different levels of weight
quantization for various datasets and network combinations in Fig. 1.
CIFAR-10 .Fig. 1a shows the performance of FedAvg variants on CIFAR-10 with ResNet-20 . We see
that the classification acurracy for the baseline FedAvg trained model drops considerably when weights
are quantized to low bit-widths ( 2and3). Both, FedAvg -KUREandFedAvg -APQNperform similarly
to the baseline with only slight improvements at low bit-widths. Our FedAvg -QATvariants outperforms
the other FedAvg variants. A signficant drop in validation accuracy is observed as the FedAvg -QAT
9Published in Transactions on Machine Learning Research (09/2023)
models are quantized to bit-widths other than the target bit-width they have been trained for. This is a
known issue with QATtrained models in the context of quantization robustness. Our proposed FedAvg -
MQATvariant directly targets this issue. The FedAvg -MQATmodel outperforms all other variants at
different level of quantizations consistently. Furthermore, it improves validation accuracy at full precision as
well. Further investigation revealed that for an over-parameterized network such as ResNet-20 , theFedAvg
baseline overfits the training set. Our proposed FedAvg -MQATimplicitly regularizes the FLmodel and
avoids the issue of overfitting in this experimental setup and thus achieves better full precision accuracy. We
further investigate overfitting and the implicit regularization phenomenon of FedAvg -MQATin Appendix.
Fig. 1b shows performance on CIFAR-10 with the LeNet-5architecture. Since LeNet-5is a relatively small
network, nooverfittingisobservedforthe FedAvg baseline. Itisimportanttonotethatthe LeNet-5(achieves
48-55% accuracy at 2bits) is more robust to weight quantization compared to ResNet-20 (achieves 28-35%
accuracy at 2bits) for the baseline FedAvg trained model on CIFAR-10 . Similar to previous comparisons,
FedAvg -KUREandFedAvg -APQNachieve marginal gains at low bit-widths. The FedAvg -MQAT
produces better validation accuracy (improvement of ≈12%at ultra low bit-width of 2bits but with
considerable loss ( ≈3%) in accuracy at full precision. We believe this is because a small network, such as
LeNet-5onCIFAR-10 , is inherently hard to compress (quantize) and the gains at low bit-widths come at the
cost of considerable degradation in full precision accuracy. Fig. 1c shows performance on CIFAR-10 with
the transformer architecture namely CCT (Hassani et al., 2021). Similar to convolutional neural networks,
FedAvg -MQATconsistently outperforms all other other variants at different level of quantizations.
In order to compare our quantization robust mechanism in FL against existing literature, we implemented
HeteroFL (Diao et al., 2021). HeteroFL propose to achieve efficient inference by pruning subset of hidden
layers based on computer budget. To compare HeteroFL to our proposed methods, we compute the bit
operations per second (BOPs) (Van Baalen et al., 2020) for the full-precision activation LeNet-5at different
bit-widths for the weight tensors and chose pruning ratios for HeteroFL that result in the same BOPs.
Instead of bits, the HeteroFL curve in Fig. 1b thus presents the evaluation accuracy of the pruned model
at different ratios while being close in BOPs to the corresponding quantized LeNet-5model. We notice that
pruning as an alternative approach to quantization generally underperforms quantization for this model
and dataset. Since these approaches are orthogonal, we expect future work to explore the combination of
quantization and pruning to ensure efficient inference in FL.
32 8 6 4 3 2
Quantization Bit-Width020406080Global Validation Accuracy
CIFAR-10 ResNet-20
Baseline
FedAvg-KURE
FedAvg-APQN (A-4)
FedAvg-APQN (A-2)
FedAvg-QAT (A-4)
FedAvg-QAT (A-2)
FedAvg-MQAT-A
(a)
32 8 6 4 3 2
Quantization Bit-Width01020304050Global Validation Accuracy
CIFAR-100 ResNet-20 (b)
Figure 2: Global validation accuracy of the proposed FedAvg variants at different bit-widths of activation
quantization for models trained on (a) CIFAR-10 using ResNet-20 , and (b) CIFAR-100 using ResNet-20 ar-
chitecture. Here, an abbreviation of “A” indicates that we performed activation-quantization only, whereas
“A-#” refers to quantization at #bits.
CIFAR-100 .Fig. 1d shows the performance of our FedAvg variants with weight-only quantization on
CIFAR-100 using the ResNet-20 architecture. Similar to our CIFAR-10 setup, we observe a large drop in
the baseline model accuracy at low bits. Our FedAvg -MQATvariant achieves significant gains, especially
at low bit-widths; ≈35%at2-bits and≈10%at3-bits, compared to the baseline FedAvg model. It is
10Published in Transactions on Machine Learning Research (09/2023)
32 8 6 4 3 2
Quantization Bit-Width10203040506070Global Validation Accuracy
CIFAR-10 ResNet-20
Baseline
Baseline (Dropout)
FedAvg-QAT (WA-4)
FedAvg-QAT (WA-2)
FedAvg-MQAT-WA
Figure 3: Global validation accuracy of the proposed FedAvg variants at different bit-widths of weight and
activation quantization for models trained on CIFAR-10 using ResNet-20 architecture. An abbreviation of
“WA” indicates that we performed both weight and activation quantization.
important to note that these improvements on low bit-widths come at the cost of a small loss of accuracy
(≈2%) at full precision in comparison to the baseline.
TinyImageNet. We also performed an experimental evaluation on a more challenging FLsetup; clas-
sification on TinyImageNet with the ResNet-18 architecture. The results for all of our proposed FedAvg
variants can be seen in Fig. 1e. Despite being the most challenging task with 200 classes and only 500
training samples for each class, our FedAvg -MQATvariant remains robust to bit-widths as low as 2-bits
without any loss in full-precision accuracy. In comparison to the baseline, it improves the 2-bit, 3-bit quan-
tization accuracy by a significant margin ( 20−35%). We would like to point out that despite being trained
for different set of bit-width, our FedAvg -MQATvariant can achieve accuracy of FedAvg -QATfor their
target bit-widths while preserving the full precision model accuracy.
FEMNIST .The FEMNIST dataset has become one of the standard datasets used to evaluate FLalgo-
rithms. For the sake of completeness, we performed quantization robustness experiments on FEMNIST using
theLeNet-5architecture. We observe that the baseline FedAvg trained model is already robust to various
quantization levels, even up to 2-bits. Compared to our other tasks, we believe classification on FEMNIST
with LeNet-5, is relatively easier and more robust to quantization.
5.2 Activation Quantization
To further demonstrate the effectiveness of our FedAvg variants, we analyze the task of quantizing acti-
vations on CIFAR-10 and CIFAR-100 with the ResNet-20 architecture. For FedAvg -KURE, we impose the
regularization term on the activations and, in a similar manner, for FedAvg -{APQN,QATandMQAT},
the noise / quantizer is on the activations.
Fig. 2a and Fig. 2b show the validation accuracy at different bit-widths for CIFAR-10 and CIFAR-100 re-
spectively. For both CIFAR-10 and CIFAR-100 , we observe considerable decline in the full precision model
accuracy after Kurtosis regularization on activations compared to the baseline. It sould be noted that the
original work on KURE(Shkolnik et al. (2020)) considered weight-quantization only. FedAvg -MQAT
achieves significant gains at 2-bit (≈55%onCIFAR-10 and≈24%onCIFAR-100 ) and 3-bit (≈26%on
CIFAR-10 and≈6%onCIFAR-100 ) quantization for both CIFAR-10 andCIFAR-100 .
Perhaps surprisingly, we can see that our FedAvg -MQATtrained model outperforms the respective Fe-
dAvg-QATmodels trained on their own respective bitwidth for 2and4-bits. For CIFAR-100 dataset, we
observe a considerable decrease in model accuracy for FedAvg -MQATvariant at higher bit-widths (and
full precision).
11Published in Transactions on Machine Learning Research (09/2023)
Bit ConfigFederated Averaging (FedAvg)
Baseline MQAT- WMQAT∗-W
W-32 70.16 74.46 70.44
W-8 69.86 74.54 70.88
W-6 70.02 74.60 68.24
W-4 68.44 74.58 70.02
W-3 64.14 75.64 67.58
W-2 28.02 72.58 68.20
Table 1: Global validation accuracy after quantization at various bit-widths for different FedAvg variants
trained on CIFAR-10 dataset using ResNet-20 architecture. Here,∗indicates the client-specific bit-width is
chosen at the begining of training and then kept fixed throughout.
5.3 Weight and Activation Quantization
Thecombinationofactivationandweightquantizationpromisestofullyharnesstheadvantagesofspecialized
hardware accelerators. Fig. 3 illustrates the impact of our quantization robustness variants for joint weight
and activation quantization on CIFAR-10 with the ResNet-20 architecture. In this setting, both, weights and
activations are quantized before each matrix multiplication. We see that FedAvg -MQAToutperforms the
FedAvg baseline as well as the bit-specific FedAvg -QATacross all bit-widths, including full-precision. As
noted before, ResNet-20 exhibits overfitting on CIFAR-10 . Thus, we include another baseline which introduces
dropout ( 50%) before the final fully-connected layer, which marginally improves performance.
5.4 Per-client fixed bit-width.
Performing a forward-pass in low bit-width during Quantization-Aware Training ( QAT) does have the ad-
ditional benefit of reduced computational requirements also during training. A good assumption to make
is that each client implements efficient hardware acceleration for a specific bit-width that remains constant
during its participation in the federated learning process. While in the main experimental setting we con-
sidered per-round sampling of the bit-width for MQAT, here we sample a client-specific bit-width at the
beginning of training and keep it fixed throughout.
As we can see in Table 1, ex-ante fixed bit-widths lead to some degradation in performance, albeit at the
aforementioned benefit of spead-up training.
6 Discussion
Real-world deployments of FL, necessarily require catering to a heterogeneous device landscape.
Quantization-robust server models, i.e., models that have robust performance on arbitrary target bit-widths,
are a step towards effectively navigating such a landscape. In this work, we introduced several variants of
theFedAvg algorithm that encourage quantization-robustness for the server model. Experimentally, we
demonstrated that our FedAvg variants can achieve good performance on several target bit-widths, with-
out significant accuracy degradation for the full precision model. No method clearly outperforms in all
settings, although we see MQATperforming well in most situations, especially for lower bit-widths. Theo-
retically, we showed that quantization-aware local training on the clients provides a convergence rate that is
similar to traditional FL, albeit with an extra error floor that depends on the parameters of the quantization
procedure and the model characteristics.
In the future, we would focus on end-to-end quantized training which can enable efficiency for both client-
server communication as well as on-device training. Further axes of heterogeneity than bit-width should be
considered, such as non-uniform quantization, different quantization strategies (symmetric vs. asymmetric)
and more subtle differences in inference engines across devices. We believe that the scenario where a client’s
hardware characteristics are considered fixed throughout training opens up the possibility for advanced client
12Published in Transactions on Machine Learning Research (09/2023)
sub-sampling strategies and necessitates a discussion on the impact of client-specific model performance
as a trade-off between efficiency and representation. Device capabilities correlate with the heterogeneous
socioeconomic landscape of participating devices and each client’s dataset’s influence on the learned function
will be different.
References
Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, TijmenBlankevoort, and Max Welling.
Gradient l1 regularization for quantization robustness. ICLR, 2020.
MohammadMohammadiAmiri, DenizGunduz, SanjeevRKulkarni, andHVincentPoor. Federatedlearning
with quantized global model updates. arXiv preprint arXiv:2006.10672 , 2020.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-
bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) Workshops , June 2020.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečn` y, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint
arXiv:1812.01097 , 2018a.
Sebastian Caldas, Jakub Konečny, H Brendan McMahan, and Ameet Talwalkar. Expanding the reach of
federated learning by reducing client resource requirements. arXiv preprint arXiv:1812.07210 , 2018b.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing.
InInternational Conference on Machine Learning , pp. 1310–1320. PMLR, 2019.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural net-
works with binary weights during propagations. Advances in neural information processing systems , 28,
2015.
Alexandre Défossez, Yossi Adi, and Gabriel Synnaeve. Differentiable model compression via pseudo quanti-
zation noise. arXiv preprint arXiv:2104.09987 , 2021.
Enmao Diao, Jie Ding, and Vahid Tarokh. Hetero{fl}: Computation and communication efficient federated
learning for heterogeneous clients. In International Conference on Learning Representations , 2021. URL
https://openreview.net/forum?id=TNkPBBYFkXg .
Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S.
Modha. Learned step size quantization. International Conference on Learning Representations (ICLR) ,
2020.
Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quan-
tization and pruning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),
Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=
ksVGCOlOEba .
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for gen-
erative pre-trained transformers. In The Eleventh International Conference on Learning Representations ,
2023. URL https://openreview.net/forum?id=tcbBPnfwxS .
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie
Yan. Differentiablesoftquantization: Bridgingfull-precisionandlow-bitneuralnetworks. 2019 IEEE/CVF
International Conference on Computer Vision (ICCV) , pp. 4851–4860, 2019.
13Published in Transactions on Machine Learning Research (09/2023)
Tiantian Han, Dong Li, Ji Liu, Lu Tian, and Yi Shan. Improving low-precision network quantization via bin
regularization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) ,
pp. 5261–5270, October 2021.
Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and Humphrey Shi. Escaping
the big data paradigm with compact transformers. arXiv preprint arXiv:2104.05704 , 2021.
Chaoyang He, Murali Annavaram, and Salman Avestimehr. Group knowledge transfer: Federated learning
of large cnns at the edge. arXiv preprint arXiv:2007.14513 , 2020.
Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos I Venieris, and Nicholas D
Lane. Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout.
arXiv preprint arXiv:2102.13451 , 2021.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution
for federated visual classification. arXiv preprint arXiv:1909.06335 , 2019.
B. Ham J. Lee, D. Kim. Network quantization with element-wise gradient scaling. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021.
YuangJiang,ShiqiangWang,VictorValls,BongJunKo,Wei-HanLee,KinKLeung,andLeandrosTassiulas.
Model pruning enables efficient federated learning on edge devices. arXiv preprint arXiv:1909.12326 , 2019.
Jangho Kim, KiYoon Yoo, and Nojun Kwak. Position-based scaled gradient for model quantization and
pruning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems , volume 33, pp. 20415–20426. Curran Associates, Inc., 2020. URL https:
//proceedings.neurips.cc/paper/2020/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf .
Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper.
arXiv preprint arXiv:1806.08342 , 2018.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized
nets: A deeper understanding. In Proceedings of the 31st International Conference on Neural Information
Processing Systems , pp. 5813–5823, 2017.
Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. Noisyquant: Noisy
bias-enhanced post-training activation quantization for vision transformers, 2023a.
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,
Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for
large language models, 2023b.
Christos Louizos, Matthias Reisser, Joseph Soriaga, and Max Welling. An expectation-maximization per-
spective on federated learning. arXiv preprint arXiv:2111.10192 , 2021.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics , pp. 1273–1282. PMLR, 2017.
Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen
Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295 , 2021.
MarkusNagel, MariosFournarakis, YelyseiBondarenko, andTijmenBlankevoort. Overcomingoscillationsin
quantization-aware training. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang
Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning ,
volume 162 of Proceedings of Machine Learning Research , pp. 16318–16330. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/nagel22a.html .
14Published in Transactions on Machine Learning Research (09/2023)
Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M. Bronstein, and
Avi Mendelson. Loss aware post-training quantization, 2020.
Kaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Diggavi. Qupel: Quantized personalization with
applications to federated learning. arXiv preprint arXiv:2102.11786 , 2021.
Eunhyeok Park and Sungjoo Yoo. PROFIT: A novel training method for sub-4-bit mobilenet models. In
Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV
2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI , volume 12351
ofLecture Notes in Computer Science , pp. 430–446. Springer, 2020. doi: 10.1007/978-3-030-58539-6\_26.
URL https://doi.org/10.1007/978-3-030-58539-6_26 .
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečn` y, Sanjiv
Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295 ,
2020.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fed-
paq: A communication-efficient federated learning method with periodic averaging and quantization. In
International Conference on Artificial Intelligence and Statistics , pp. 2021–2031. PMLR, 2020.
Juncheol Shin, Junhyuk So, Sein Park, Seungyeop Kang, Sungjoo Yoo, and Eunhyeok Park. Nipq: Noise
proxy-based integrated pseudo-quantization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 3852–3861, June 2023.
Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, and Uri Weiser.
Robust quantization: One model to rule them all. NeurIPS , 2020.
Aleksei Triastcyn, Matthias Reisser, and Christos Louizos. Dp-rec: Private & communication-efficient fed-
erated learning. arXiv preprint arXiv:2111.05454 , 2021.
Mart Van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort,
and Max Welling. Bayesian bits: Unifying quantization and pruning. Advances in neural information
processing systems , 33:5741–5752, 2020.
Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zero-
quant: Efficient and affordable post-training quantization for large-scale transformers. In Alice H. Oh,
Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing
Systems, 2022. URL https://openreview.net/forum?id=f-fVCElZ-G1 .
Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefa-net: Training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 ,
2016.
A Appendix
Appendix
B Convergence analysis for quantization aware local training in federated learning
In this appendix we provide a convergence analysis for quantization-aware training in combination with
federated learning. We follow the analysis presented in Reddi et al. (2020), while handling the quantization
noise, such as Li et al. (2017). This appendix tries to be verbose so as to be easy to follow along. Theorem
3.1 contains the formal claim for this proof. On a high level, we will aim to upper-bound the gradient
magnitude of the FL objective (Eq. 1 in the paper), ∇wF(wt)for different perspectives on t. We begin the
proof by making explicit assumptions about the objective as well as the nature of quantization. In terms of
techniques we rely only on some general inequalities that we detail before the actual proof. We advise the
studious reader to keep a second copy of the pdf open to refer back to these inequalities. Furthermore we
preface this section by re-iterating notation.
15Published in Transactions on Machine Learning Research (09/2023)
B.1 Notation and definitions
w∈I RD...is the flattened vector of model parameters.
wt
sk∈I RD...model parameters at the k’th (out ofK) iteration of local updates
on clientsin communication round t.
S...is the set of all clients (’shards’), of size S=|S|
F(w) =1
SS/summationdisplay
sFs(w),We use1
S/summationdisplay
sFs(w)as shorthand.
Fs(w) =1
DsDs/summationdisplay
ifs(w,ϵi),whereDs=|Ds|is the size of the local dataset Dsat clients.
fs(w,ϵi)...is a loss function evaluated on a random mini-batch ϵion clientswith
model parameters w.
Et[F(wt+1)]...is the loss evaluated on parameters wat roundt+ 1, in expectation over the
randomness at round tthat influences the transition of parameters from round t
to roundt+ 1.
E[F(w)]...is the loss evaluated on parameters w,averaged over all possible sources of
randomness. Precise definition given in context below.
∇wF(w)...is the gradient of Fwith respect to wevaluated at w.
We use the shorthand ∇F(w), similarly for∇Fs(w)and∇fs(w).
∆b...is the step size of the quantizer / magnitude of the quantization noise for a
given bit-width b.
B.2 Assumptions
Assumption 1 (Lipschitz Gradient) .Each local loss function FsisL-smooth∀s∈S,i.e.,∥∇Fs(x)−
∇Fs(y)∥≤L∥x−y∥,∀x,y∈I RD.
Assumption 2 (Bounded variance) .EachFshas bounded local variance, i.e.,E[∥∇fs(w,ϵ)−∇Fs(w)∥2]≤
σ2
l, wherefsis a stochastic estimate of the loss based on an w∈I RDandϵis a random mini-batch.
Furthermore, the global variance is also bounded, i.e.,1
S/summationtext
s∥∇Fs(w)−∇F(w)∥2≤σ2
g,∀w∈I RD.
Assumption 3 (Bounded quantization noise) .Letw∈I RD,jbe any of its dimensions and Q(·)the
quantization operation with a step size ∆∈I R. The quantization noise rj∈I Radded towj,i.e.,rj=
Q(wj)−wj, is bounded by half the step size of Q(·),i.e.,rj≤∆
2.
B.3 Auxiliary lemmata / inequalities
In this section we will provide some inequalities and lemmata that will be useful for the proof of our main
theorem.
Lemma 2. For anyγ >0we have that±2αβ≤γα2+1
γβ2
Proof.
0≤(√γα±1√γβ)2→0≤γα2+1
γβ2±2αβ→±2αβ≤γα2+1
γβ2.
Corollary 3. For anyγ >0we have that (α±β)2≤(1 +γ)α2+ (1 +1
γ)β2
16Published in Transactions on Machine Learning Research (09/2023)
Proof.
(α±β)2=α2+β2±2αβ (18)
From Lemma 2
≤α2+β2+γα2+1
γβ2= (1 +γ)α2+ (1 +1
γ)β2(19)
Lemma 4. For random variables zr,...,zrwe have that
E[∥z1+···+zr∥2]≤rE[∥z1∥2+···+∥zr∥2]. (20)
Proof.TheprooffollowsfromexpandingthesquareandapplyingLemma2toeach 2zizjtermwithγ= 1.
Eb[E[∥r∥2]] =/summationdisplay
dEb[E[r2
d]]≤/summationdisplay
dEb/bracketleftbigg∆2
b
4/bracketrightbigg
=D
4Eb/bracketleftbig
∆2
b/bracketrightbig
(21)
Lemma 5. Letrbe the quantization noise added to wsatisfying assumption Eq. (3). When performing
QAT,MQAT,APQNwe have that
E[∥r∥2]≤DR2, (22)
whereR=∆b
2forQAT,R=/radicalbigg
Eb/bracketleftig∆2
b
4/bracketrightig
forMQATandR=∆b√
12forAPQN.
Proof.ForQATwe have that
E[∥r∥2] =E[D/summationdisplay
d=1r2
d]≤E[D/summationdisplay
d=1∆2
b
4] =D/parenleftbigg∆b
2/parenrightbigg2
, (23)
due to the bounded quantization noise assumption. For MQATwhere we consider random bitwidths, we
have that
Eb[E[∥r∥2]] =/summationdisplay
dEb[E[r2
d]]≤/summationdisplay
dEb/bracketleftbigg∆2
b
4/bracketrightbigg
=DEb/bracketleftbigg∆2
b
4/bracketrightbigg
. (24)
ForAPQNwe have that
E[∥r∥2] =E[D/summationdisplay
d=1r2
d] =D/summationdisplay
d=1E[r2
d] =D/summationdisplay
d=1∆2
b
12=D/parenleftbigg∆b√
12/parenrightbigg2
, (25)
due tord∼U/bracketleftbig
−∆b
2,∆b
2/bracketrightbig
andE[r2
d] = Var[rd] +E[rd]2=∆2
b
12.
Lemma 6. For any local learning rate ηc≤1
10LK, we can bound the difference between the local shadow
weights and the server weights for any k∈{0,...,K−1}andK≥1at a given federated training iteration
tvia
1
S/summationdisplay
sE∥wt
sk−wt∥2≤4Kη2
c(σ2
l+ 6Kσ2
g) + 32K2η2
cL2DR2+ 24K2η2
c∥∇F(wt)∥2. (26)
17Published in Transactions on Machine Learning Research (09/2023)
Proof.We begin by noting that
E∥wt
sk−wt∥2=E∥wt
s,k−1−wt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
a−ηc(∇fs(wt
s,k−1+rt
s,k−1)−∇Fs(wt
s,k−1+rt
s,k−1)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
b
+∇Fs(wt
s,k−1+rt
s,k−1)−∇Fs(wt)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
c+∇Fs(wt)−∇F(wt)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
d+∇F(wt)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
e)∥2,(27)
where we introduced several shorthand notations for easier manipulation of the terms. By expanding the
norm using the multinomial theorem we have that
=E[∥a∥2] +η2
cE[∥b∥2] +η2
cE[∥c∥2] +η2
cE[∥d∥2] +η2
cE[∥e∥2]
−2E[aT(ηc(b+c+d+e))] + 2η2
cE[bT(c+d+e)]
+ 2η2
cE[cT(d+e)] + 2η2
cE[dTe]. (28)
We can now use the fact that the expectation of bis zero, since∇fs(wt
s,k−1+rt
s,k−1)is an unbiased estimate
of∇Fs(wt
s,k−1+rt
s,k−1). In this way, we can simplify the above to
=E[∥a∥2] +η2
cE[∥b∥2] +η2
cE[∥c∥2] +η2
cE[∥d∥2] +η2
cE[∥e∥2]
−2E[aT(ηc(c+d+e))] + 2η2
cE[cT(d+e)] + 2η2
cE[dTe]. (29)
We can now use Lemma 2 with a γ= 2K−1in order to “split” the 2E[aT(ηc(c+d+e))]term
≤E[∥a∥2] +η2
cE[∥b∥2] +η2
cE[∥c∥2] +η2
cE[∥d∥2] +η2
cE[∥e∥2]
+1
2K−1E[∥a∥2] + (2K−1)η2
cE[∥c+d+e∥2]
+ 2η2
cE[cT(d+e)] + 2η2
cE[dTe]. (30)
Following that, we can see that several terms cancel, due to E[∥c+d+e∥2] =E[∥c∥2] +E[∥d∥2] +E[∥e∥2] +
2E[cT(d+e)] + 2E[dTe]
=/parenleftbigg
1 +1
2K−1/parenrightbigg
E[∥a∥2] +η2
cE[∥b∥2] + 2Kη2
cE[∥c+d+e∥2]. (31)
Finally, we will apply Lemma 4 in order to split E[∥c+d+e∥2]and thus end up with
≤/parenleftbigg
1 +1
2K−1/parenrightbigg
E[∥a∥2] +η2
cE[∥b∥2] + 6Kη2
cE[∥c∥2]
+ 6Kη2
cE[∥d∥2] + 6Kη2
cE[∥e∥2], (32)
=/parenleftbigg
1 +1
2K−1/parenrightbigg
E[∥wt
s,k−1−wt∥2]
+η2
cE[∥∇fs(wt
s,k−1+rt
s,k−1)−∇Fs(wt
s,k−1+rt
s,k−1)∥2]
+ 6Kη2
cE[∥∇Fs(wt
s,k−1+rt
s,k−1)−∇Fs(wt)∥2]
+ 6Kη2
cE[∥∇Fs(wt)−∇F(wt)∥2] + 6Kη2
cE[∥∇F(wt)∥2], (33)
where we replaced the shorthand notations with their original terms. To proceed, we will make use of
assumptions 2 and 1 to arrive at
≤/parenleftbigg
1 +1
2K−1/parenrightbigg
E[∥wt
s,k−1−wt∥2] +η2
cσ2
l
+ 6Kη2
cL2E[∥wt
s,k−1+rt
s,k−1−wt∥2]
+ 6Kη2
cE[∥∇Fs(wt)−∇F(wt)∥2] + 6Kη2
cE[∥∇F(wt)∥2]. (34)
18Published in Transactions on Machine Learning Research (09/2023)
We can now make use of corollary 3 with a γ= 3in order to separate the quantization error rs,k−1from the
difference between the local (shadow) weight and the server weight
≤/parenleftbigg
1 +1
2K−1/parenrightbigg
E[∥wt
s,k−1−wt∥2] +η2
cσ2
l
+ 24Kη2
cL2E[∥wt
s,k−1−wt∥2] + 8Kη2
cL2E[∥rt
s,k−1∥2]
+ 6Kη2
cE[∥∇Fs(wt)−∇F(wt)∥2] + 6Kη2
cE[∥∇F(wt)∥2], (35)
and then use Lemma 5 in order to bound the squared norm of rs,k−1
≤/parenleftbigg
1 +1
2K−1/parenrightbigg
E[∥wt
s,k−1−wt∥2] +η2
cσ2
l
+ 24Kη2
cL2E[∥wt
s,k−1−wt∥2] + 8Kη2
cL2DR2
+ 6Kη2
cE[∥∇Fs(wt)−∇F(wt)∥2] + 6Kη2
cE[∥∇F(wt)∥2]. (36)
To proceed and make further use of our assumptions, we will average the aformentioned inequality over the
clients and thus have that
1
S/summationdisplay
sE[∥wt
sk−wt∥2]≤/parenleftbigg
1 +1
2K−1+ 24Kη2
cL2/parenrightbigg1
S/summationdisplay
sE[∥wt
s,k−1−wt∥2] +η2
cσ2
l
+ 8Kη2
cL2DQ2+ 6Kη2
c1
S/summationdisplay
sE[∥∇Fs(wt)−∇F(wt)∥2] (37)
+ 6Kη2
cE[∥∇F(wt)∥2], (38)
and then make use of assumption 2 in order to bound the “global” variance
≤/parenleftbigg
1 +1
2K−1+ 24Kη2
cL2/parenrightbigg1
S/summationdisplay
sE[∥wt
s,k−1−wt∥2] +η2
cσ2
l
+ 8Kη2
cL2DQ2+ 6Kη2
cσ2
g+ 6Kη2
cE[∥∇F(wt)∥2]. (39)
Finally, given our assumption that ηc≤1
10LK, we have that (1 +1
2K−1+ 24Kη2
cL2)≤(1 +1
K−1)and thus
we can simplify the upper bound even further
1
S/summationdisplay
sE[∥wt
sk−wt∥2]≤/parenleftbigg
1 +1
K−1/parenrightbigg1
S/summationdisplay
sE[∥wt
s,k−1−wt∥2] +η2
c(σ2
l+ 6Kσ2
g)
+ 8Kη2
cL2DQ2+ 6Kη2
cE[∥∇F(wt)∥2]. (40)
We have now arrived at a point where the average difference between the local shadow weight at iteration
kand the server weight is upper bounded by two things; the average difference at iteration k−1along
with some constant terms that are independent of the actual weights or iteration. We can thus continue
further by sequentially applying the bound at Eq. 40 on each weight difference, up until we end up at the
server weight wt(since local optimization started from that point) where the difference is zero. Notice that
each application of this bound “adds” additional non-negative terms, so we have that the upper bound of
Kiterations would upper bound the bound on K−1iterations. Therefore, the “worst-case” upper bound
is the one where k=K. In this case, we can unroll the recursion and have that
1
S/summationdisplay
sE[∥wt
sk−wt∥2]≤K−1/summationdisplay
j=0/parenleftbigg
1 +1
K−1/parenrightbiggj/parenleftbig
η2
c(σ2
l+ 6Kσ2
g) + 8Kη2
cL2DQ2
+ 6Kη2
cE[∥∇F(wt)∥2]/parenrightbig
. (41)
To simplify even further, we can use the fact that (1 +1
K−1)jis monotonic in jand that there are Kterms
in the sum, thus get
≤K/parenleftbigg
1 +1
K−1/parenrightbiggK/parenleftbig
η2
c(σ2
l+ 6Kσ2
g) + 8Kη2
cL2DQ2+ 6Kη2
cE[∥∇F(wt)∥2]/parenrightbig
(42)
19Published in Transactions on Machine Learning Research (09/2023)
and since (1 +1
K−1)K≤4for anyK > 1
≤4Kη2
c(σ2
l+ 6Kσ2
g) + 32K2η2
cL2DQ2+ 24K2η2
cE[∥∇F(wt)∥2], (43)
which proves our claim.
B.4 Proof of Theorem 3.1
We begin by noting that the server-side update rule of the model in the case when the clients perform Quan-
tization Aware (QA) SGD with a learning rate of ηcand the server does SGD with a learning rate ηs. The
extension of the proof to more involved server-side update rules as in Reddi et al. (2020) is straightforward.
wt+1−wt=ηsGt,Gt=−ηc
S/summationdisplay
s/summationdisplay
k∇fs(wt
sk+rt
sk), (44)
wheresindexes the clients and Sis the total number of clients. kindexes the local client iteration number,
and we assume that there are Klocal iterations in total. wt
sk∈I RDcorresponds to a real valued local
shadow weight at iteration tandrt
sk∈I RDcorresponds to the quantization noise that is added to it in each
iteration of the local optimization (as the weights are rounded / noised before the forward pass).
Using theL-smoothness of the global loss function,
F(wt+1)≤F(wt) +ηs∇F(wt)TGt+L
2∥ηsGt∥2(45)
=F(wt) +ηs∇F(wt)TGt+Lη2
s
2∥Gt∥2. (46)
We then take an expectation over all randomness at time step t,
Et[F(wt+1)]≤F(wt) +ηs∇F(wt)TEt[Gt]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T1t+Lη2
s
2Et[∥Gt∥2]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T2t, (47)
and work towards upper bounding the T1t,T2tterms separately.
Bounding T1t
∇F(wt)TEt[Gt] =∇F(wt)TEt[Gt−ηcK∇F(wt) +ηcK∇F(wt)] (48)
=−ηcK∥∇F(wt)∥2+∇F(wt)TEt[Gt+ηcK∇F(wt)]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T3t. (49)
We will now work towards upper bounding T3t
T3t=∇F(wt)TEt[−ηc
S/summationdisplay
s/summationdisplay
k∇fs(wt
sk+rt
sk) +ηcK∇F(wt)] (50)
=∇F(wt)TEt[−ηc
S/summationdisplay
s/summationdisplay
k∇Fs(wt
sk+rt
sk) +ηcK∇F(wt)] (51)
=ηc∇F(wt)TEt[−1
S/summationdisplay
s/summationdisplay
k∇Fs(wt
sk+rt
sk) +1
S/summationdisplay
s/summationdisplay
k∇Fs(wt)]. (52)
Now by using Lemma 2 with γ=Kwe have that
T3t≤ηcK
2∥∇F(wt)∥2+ηc
2KEt[∥1
S/summationdisplay
s/summationdisplay
k∇Fs(wt
sk+rt
sk)−1
S/summationdisplay
s/summationdisplay
k∇Fs(wt)∥2] (53)
=ηcK
2∥∇F(wt)∥2+ηc
2KS2Et[∥/summationdisplay
s/summationdisplay
k(∇Fs(wt
sk+rt
sk)−∇Fs(wt))∥2]. (54)
20Published in Transactions on Machine Learning Research (09/2023)
By then using Lemma 4 to push the squared norm inside the sum
≤ηcK
2∥∇F(wt)∥2+ηcSK
2KS2Et[/summationdisplay
s/summationdisplay
k∥∇Fs(wt
sk+rt
sk)−∇Fs(wt))∥2] (55)
and from the Lipschitz gradient assumption
≤ηcK
2∥∇F(wt)∥2+ηc
2SEt[/summationdisplay
s/summationdisplay
k∥L(wt
sk+rt
sk−wt)∥2] (56)
=ηcK
2∥∇F(wt)∥2+ηcL2
2/summationdisplay
k1
S/summationdisplay
sEt[∥wt
sk+rt
sk−wt∥2]. (57)
We will now once more use Lemma 4 to separate the shadow weight difference from the quantization noise
≤ηcK
2∥∇F(wt)∥2+ηcL2/summationdisplay
k1
S/summationdisplay
sEt[∥wt
sk−wt∥2] +ηcL2/summationdisplay
k1
S/summationdisplay
sEt[∥rt
sk∥2] (58)
and from Lemma 5
≤ηcK
2∥∇F(wt)∥2+ηcL2/summationdisplay
k1
S/summationdisplay
sEt[∥wt
sk−wt∥2] +ηcKL2DR2. (59)
We see that in order to proceed, we need to upper bound the difference between the local shadow weight at
any iteration kand the server weight. This is were we will use our Lemma 6 in order to proceed
≤ηcK
2∥∇F(wt)∥2+KηcL2(4Kη2
c(σ2
l+ 6Kσ2
g)) +KηcL2(32K2η2
cL2DQ2)
+KηcL2(24K2η2
c∥∇F(wt)∥2) +ηcKL2DR2(60)
=/parenleftbiggηcK
2+ 24K3η3
cL2/parenrightbigg
∥∇F(wt)∥2+ 4K2η3
cL2(σ2
l+ 6Kσ2
g) + (ηcK+ 32η3
cK3L2)L2DR2.(61)
Finally, we can make use of our assumption ηc≤1
10LKwhich leads to 24η3
cL2K3≤1
4ηcKand32η3
cL2K3≤
1
3ηcK. In this way, we can arrive at our final bound for T3t
T3t≤3ηcK
4∥∇F(wt)∥2+4
3ηcKL2DR2+ 4K2η3
cL2(σ2
l+ 6Kσ2
g). (62)
Now we can apply this bound to T1tin order to get
T1t≤−ηcK∥∇F(wt)∥2+3ηcK
4∥∇F(wt)∥2+4
3ηcKL2DR2+ 4K2η3
cL2(σ2
l+ 6Kσ2
g)(63)
=−ηcK
4∥∇F(wt)∥2+4
3ηcKL2DR2+ 4K2η3
cL2(σ2
l+ 6Kσ2
g) (64)
Which is the final upper bound on T1t.
Bounding T2tWe begin by noting that
Lη2
s
2Et[∥Gt∥2] =Lη2
s
2Et[∥Gt−ηcK∇F(wt) +ηcK∇F(wt)∥2], (65)
and then we can split the squared norm via Corollary 3 with γ= 1
≤Lη2
s(Et[∥Gt+ηcK∇F(wt)∥2]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T4t+η2
cK2∥∇F(wt)∥2). (66)
To continue, we will move towards upper bounding T4tby expanding the terms inside the squared norm
T4t=Et[∥−ηc
S/summationdisplay
s/summationdisplay
k∇fs(wt
sk+rt
sk) +ηc
S/summationdisplay
s/summationdisplay
k∇Fs(wt)∥2] (67)
=η2
c
S2Et[∥/summationdisplay
s/summationdisplay
k/parenleftbig
∇fs(wt
sk+rt
sk)−∇Fs(wt
sk+rt
sk) +∇Fs(wt
sk+rt
sk)−∇Fs(wt)/parenrightbig
∥2].(68)
21Published in Transactions on Machine Learning Research (09/2023)
We will then apply Lemma 4 in order to move the squared norm inside the sums
≤η2
cSK
S2/summationdisplay
s/summationdisplay
kEt[∥∇fs(wt
sk+rt
sk)−∇Fs(wt
sk+rt
sk) +∇Fs(wt
sk+rt
sk)−∇Fs(wt)∥2](69)
and will apply Corollary 3 with γ= 1in order to split the norm
≤2η2
cK
S/summationdisplay
s/summationdisplay
k/parenleftbig
Et[∥∇fs(wt
sk+rt
sk)−∇Fs(wt
sk+rt
sk)∥2] +Et[∥∇Fs(wt
sk+rt
sk)
−∇Fs(wt)∥2]/parenrightbig
. (70)
To proceed, we will make use of our assumptions 1, 2 in order to get
≤2η2
cK2σ2
l+ 2η2
cK/summationdisplay
k1
S/summationdisplay
sEt[∥L(wt
sk+rt
sk−wt)∥2] (71)
and we will apply Corollary 3 with γ= 1one more time in order to split the norm of the weight difference
and the quantization error
≤2η2
cK2σ2
l+ 4η2
cKL2/summationdisplay
k1
S/summationdisplay
sEt[∥wt
sk−wt∥2] + 4η2
cKL2/summationdisplay
k1
S/summationdisplay
sEt[∥rt
sk∥2] (72)
so that we can apply Lemma 5 in order to bound the latter
≤2η2
cK2σ2
l+ 4η2
cKL2/summationdisplay
k1
S/summationdisplay
sEt[∥wt
sk−wt∥2] + 4η2
cK2L2DR2. (73)
By observing the above, we see that we again end up with the average difference between the shadow weights
at each iteration kand the server weight. As a result, we can apply Lemma 6 in order to proceed further
≤2η2
cK2σ2
l+ 4η2
cK2L2DQ2
+ 4η2
cK2L2(4Kη2
c(σ2
l+ 6Kσ2
g) + 32K2η2
cL2DR2+ 24K2η2
cE[∥∇F(wt)∥2]) (74)
= 2η2
cK2σ2
l+ 4η2
cK2L2DQ2+ 16K3η4
cL2(σ2
l+ 6Kσ2
g) + 128η4
cL4K4DR2
+ 96η4
cK4L2∥∇F(wt)∥2. (75)
In order to simplify the aforementioned inequality we will make a use of our assumption on ηc, namely that
ηc≤1
10LK. In this way, we will have that 16K3η4
cL2≤1
6Kη2
calong with 128η4
cK4L2≤3
2η2
cK2. Taking
these into account, we have that
≤(2η2
cK2+1
6Kη2
c)σ2
l+K2η2
cσ2
g+ (4η2
cK2+3
2η2
cK2)L2DR2+ 96η4
cK4L2∥∇F(wt)∥2(76)
and due to 4 +3
2<6
≤(2η2
cK2+1
6Kη2
c)σ2
l+K2η2
cσ2
g+ 6η2
cK2L2DR2+ 96η4
cK4L2∥∇F(wt)∥2, (77)
which constitutes our final upper bound on T4t. With this bound at hand, we can move back to bounding
T2tand thus get
T2t≤Lη2
s((2η2
cK2+1
6Kη2
c)σ2
l+K2η2
cσ2
g+ 6η2
cK2L2DR2+ 96η4
cK4L2∥∇F(wt)∥2
+η2
cK2∥∇F(wt)∥2) (78)
=Lη2
s((2η2
cK2+1
6Kη2
c)σ2
l+K2η2
cσ2
g+ 6η2
cK2L2DR2)
+Lη2
s(96η4
cK4L2+η2
cK2)∥∇F(wt)∥2. (79)
22Published in Transactions on Machine Learning Research (09/2023)
Having bounded T1tandT2t, we can now apply these bounds to the inequality at Eq. 47 and thus get
Et[F(wt+1)]≤F(wt) +ηs∇F(wt)TEt[Gt]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T1t+Lη2
s
2Et[∥Gt∥2]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T2t(80)
≤F(wt)−ηsηcK
4∥∇F(wt)∥2+4
3ηsηcKL2DR2+ 4ηsK2η3
cL2(σ2
l+ 6Kσ2
g)
+Lη2
s((2η2
cK2+1
6Kη2
c)σ2
l+K2η2
cσ2
g+ 6η2
cK2L2DR2)
+Lη2
s(96η4
cK4L2+η2
cK2)∥∇F(wt)∥2. (81)
To simplify the aforementioned bound, we can once again make use of our condition ηc≤1
10LKwhich leads
to96η4
cK4L2≤η2
cK2. In this way, we get that
Et[F(wt+1)]≤F(wt)−ηs(ηcK
4−2Lηsη2
cK2)∥∇F(wt)∥2
+ (4ηsK2L2η3
c+Lη2
s(2η2
cK2+1
6Kη2
c))σ2
l
+ (24ηsK2L2η3
c+Lη2
sη2
cK)Kσ2
g+ (4
3ηsηcK+ 6Lη2
sη2
cK2)L2DR2(82)
=F(wt)−ηsηc(K
4−2LηsηcK2)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A∥∇F(wt)∥2
+η2
c(4ηsK2L2ηc+Lη2
s(2K2+K
6))
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Bσ2
l
+η2
c(24ηsK2L2ηc+Lη2
sK)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
ΓKσ2
g+η2
c(4ηs
3ηcK+ 6Lη2
sK2)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
HL2DR2, (83)
where we introduced several shorthand notations for easier manipulation of the inequalities. We can thus
now re-arrange the inequality to
Et[F(wt+1)]−F(wt)≤−ηsηcA∥∇F(xt)2∥+η2
c(Bσ2
l+ ΓKσ2
g) +η2
cHL2DR2. (84)
In order to consider the entire training trajectory, we will use a telescoping sum, i.e., we will sum this
inequality over all rounds and take the expectation at each time-step
T/summationdisplay
t=1(Et[F(wt+1)]−Et−1[F(wt)])≤−ηsηcAT/summationdisplay
t=1∥∇F(wt)∥2+Tη2
c(Bσ2
l+ ΓKσ2
g)
+Tη2
cHL2DR2. (85)
In doing that, most of the terms on the left-hand-side will cancel across subsequent time-steps and thus we
will end up with
ET[F(wT+1)]−F(w1)≤−ηsηcAT/summationdisplay
t=1∥∇F(wt)∥2+Tη2
c(Bσ2
l+ ΓKσ2
g) +Tη2
cHL2DR2.(86)
We can now re-arrange the terms to get
ηsηcAT/summationdisplay
t=1∥∇F(wt)∥2≤F(w1)−ET[F(wT+1)] +Tη2
c(Bσ2
l+ ΓKσ2
g) +Tη2
cHL2DR2.(87)
23Published in Transactions on Machine Learning Research (09/2023)
Dataset Network ηsηcϵsRounds (T)Batch Size
CIFAR-10 LeNet-5 1e−3 5e−2 1e−8 2000 64
CIFAR-10 ResNet-20 1e−3 5e−2 1e−75000 64
CIFAR-100 ResNet-20 1e−3 5e−2 1e−710000 20
FEMNIST LeNet-5 1e−3 5e−2 1e−86000 20
TinyImageNet ResNet-18 1e−2 1e−2 1e−34500 20
Table 2: Hyperparameters used for the experimental evaluations in the paper. Here ηs,ηcdenote the server
and client learning rate and ϵsrefers to the correction term in value in ADAM optimizer of server.
and by considering w∗to be the parameters of lowest loss, ET[F(wT+1)]≥F(w∗), we have that
ηsηcAT/summationdisplay
t=1∥∇F(wt)∥2≤F(w1)−F(w∗) +Tη2
c(Bσ2
l+ ΓKσ2
g) +Tη2
cHL2DR2. (88)
In order to proceed, we have to impose a condition on A, namely that it has to be positive (otherwise,
dividing by Areverses the inequality). For this to happen we need that
K
4≥2LηsηcK2→1
4≥2LηsηcK→ηc≤1
8LKηs. (89)
Assuming that this condition is satisfied, we have that
1
TT/summationdisplay
t=1∥∇F(wt)∥2≤F(w1)−F(w∗)
TηsηcA+ηc
ηsA(Bσ2
l+ ΓKσ2
g+HL2DR2). (90)
(91)
Finally, in order to obtain the result of Theorem 3.1, we make use of the fact that min1≤t≤T∥∇F(wt)∥2≤
1
T/summationtextT
t=1∥∇F(wt)∥2and thus arrive at
min
1≤t≤T∥∇F(wt)∥2≤F(w1)−F(w∗)
TηsηcA+ηc
ηsA(Bσ2
l+ ΓKσ2
g+HL2DR2), (92)
which completes the proof.
C Final Hyperparameters
We provide the final hyperparameters used for all the experiments in Table 2. Since the grid-search for FLis
expensive, we tune the hyperparameters such as client learning rate ( ηc), server learning rate ( ηs) and epsilon
term (ϵs) used in ADAM optimizer only for the baselines and then use the same set of hyperparameters for all
ourproposedvariants. For FedAvg -KURE,wetunedλfromthegrid [1e+1,1e+0,1e−1,1e−2,1e−3,1e−4],
foundλ= 1e−1to be optimal and use that for all our experiments. We also provide the details about
bit-set used to perform FedAvg -MQATvariants in Table 3.
D Additional Results
Implicit Regularization of MQAT. As discussed in the main paper, ResNet-20 onCIFAR-10 exhibits
overfitting, which we further investigate here. It was observed in the main text that FedAvg -MQAThas
an implicit regularisation effect and thus achieves better validation accuracy for the full-precision model
by avoiding overfitting despite that not being the main objective. We show experimental comparisons of
our proposed MQATto other standard methods of regularization with different strength of weight decay
(measured by regularisation term λWD) and drop-out before the last full-connected layer of the network in
24Published in Transactions on Machine Learning Research (09/2023)
Dataset NetworkFederated Averaging (FedAvg)
MQAT-WMQAT-AMQAT-WA
CIFAR-10 LeNet-5 [4,6,8] - -
CIFAR-10 ResNet-20 [2,3,4,6,8,32] [2,3,4,6,8,32] [2,3,4,6,8,32]
CIFAR-100 ResNet-20 [2,3,4,6,8,32] [2,3,4,6,8,32] -
TinyImageNet ResNet-18 [2,3,4,6,8] - -
Table 3: Quantization Bit-Set used to perform FedAvg-MQATon different experimental setup in the paper.
Table 4. Despite not being its primary objective, our FedAvg -MQATvariant achieves considerable gains
in full-precision accuracy in comparison to standard approaches used to avoid overfitting. Fig. 4 shows how
the validation loss of the baseline is increasing after 2.5krounds and the corresponding validation accuracy
on the right. For MQAT, we observe no overfitting according to the validation loss.
0 1000 2000 3000 4000 5000
Rounds0.751.001.251.501.752.002.252.50Global Validation LossCIFAR-10 ResNet-20
Baseline
FedAvg-MQAT-W
FedAvg-MQAT-A
FedAvg-MQAT-WA
(a)
0 1000 2000 3000 4000 5000
Rounds3040506070Global Validation AccuracyCIFAR-10 ResNet-20
Baseline (b)
Figure 4: Global validation (a) loss and (b) accuracy curves for Baseline and different FedAvg-MQAT
variants trained on CIFAR-100 using ResNet-20 architecture. For FedAvg-MQATvariants the validation
loss refers to the loss after quantization at lowest bit-width in the bit-set B. While Baseline model clearly
suffers from the overfitting issue, our FedAvg-MQATvariants clearly manage to avoid it. Further, the
validation accuracy curves on the baselines reveals that overfitting cannot be prevented even by early stopping.
Detailed results from the main text In this section, we provide the exact values used for plotting
various figures in the main text in Table 5-11.
25Published in Transactions on Machine Learning Research (09/2023)
FedAvg Accuracy
Baseline 70.16
Baseline (Dropout) 72.02
λWD= 1e−2 44.94
λWD= 1e−3 71.62
λWD= 1e−4 70.76
λWD= 1e−5 70.06
λWD= 1e−6 70.10
MQAT-W 74.46
MQAT-A 76.28
MQAT-WA 74.90
Table 4: Global validation accuracy for full-precision models learnt in federation using FedAvg variants
trained on CIFAR-10 dataset with ResNet-20 architecture. Here Windicates the client-specific bit-width is
chosen at the begining of training and then kept fixed throughout. Here, an abbreviation of “W”, “A” and
“WA” indicate weight quantization, activation quantization and both weight-activation.
Bit ConfigFederated Averaging (FedAvg)
Baseline KURE APQN (W-4) APQN (W-2) QAT (W-4) QAT (W-2) MQAT-W
W-32 70.16 69.38 70.5 70.04 72.38 29.06 74.46
W-8 69.86 69.38 70.72 70.19 71.08 11.68 74.54
W-6 70.02 69.08 70.62 70.16 71.6 12.68 74.60
W-4 68.44 68.12 69.44 69.36 72.58 18.5 74.58
W-3 64.14 64.28 63.46 65.66 71.22 39.02 74.64
W-2 28.02 31.28 29.64 33.36 52.32 72.64 72.58
Table 5: Global validation accuracy after weight quantization of various quantization robustness variants for
Federated Averaging ( FedAvg) on federated version of CIFAR-10 dataset using ResNet-20 architecture.
Bit ConfigFederated Averaging (FedAvg)
Baseline KURE APQN (W-4) APQN (W-2) QAT (W-4) QAT (W-2) MQAT-W
W-32 49.17 49.57 50.77 50.8 49.19 13.62 48.47
W-8 49.24 49.52 50.86 50.88 46.46 1.7 48.21
W-6 48.99 49.66 50.27 50.81 47.34 1.92 48.28
W-4 45.37 47.75 45.76 46.34 49.7 3.05 47.18
W-3 36.42 41.04 34.87 37.04 44.44 11.68 45.88
W-2 6.62 8.28 8.41 7.62 24.77 42.92 42.27
Table 6: Global validation accuracy after weight quantization of various quantization robustness variants for
Federated Averaging ( FedAvg) on federated version of CIFAR-100 dataset using ResNet-20 architecture.
Bit ConfigFederated Averaging (FedAvg)
Baseline KURE APQN (W-4) APQN (W-2) QAT (W-4) QAT (W-2) MQAT-W
W-32 37.21 37.51 36.48 37.85 36.83 4.39 37.89
W-8 37.29 37.61 36.41 37.89 36.73 2.3 37.56
W-6 36.61 37.17 35.9 37.35 36.98 2.73 37.23
W-4 31.09 33.29 31.4 33.37 37.43 5.99 37.12
W-3 17.64 17.61 20.08 19.25 31.14 15.66 36.84
W-2 0.86 0.9 1.02 0.89 7.28 34.53 35.45
Table 7: Global validation accuracy after weight quantization of various quantization robustness variants for
Federated Averaging ( FedAvg) on federated version of TinyImageNet dataset using ResNet-18 architecture.
26Published in Transactions on Machine Learning Research (09/2023)
Bit ConfigFederated Averaging (FedAvg)
Baseline KURE APQN (W-4) APQN (W-2) QAT (W-4) QAT (W-2) MQAT-W
W-32 69 69.4 69.66 68.92 65.68 27.74 65.68
W-8 69.02 69.4 69.66 68.86 55.1 11.52 65.66
W-6 68.72 69.38 69.54 68.54 59.42 11.74 66.26
W-4 68.24 68.8 68.66 68.26 66.74 12.62 66.02
W-3 66.82 67.12 67.06 66.94 58.4 15.36 62.51
W-2 48.68 51.94 51.3 53.14 61.44 61.5 59.32
Table 8: Global validation accuracy after weight quantization of various quantization robustness variants for
Federated Averaging ( FedAvg) on federated version of CIFAR-10 dataset using LeNet-5architecture.
Bit ConfigFederated Averaging (FedAvg)
Baseline KURE APQN (A-4) APQN (A-2) QAT (A-4) QAT (A-2) MQAT-A
A-32 70.16 66.82 70.42 50.56 63.74 41.04 76.28
A-8 70.06 66.88 70.3 50.55 73.98 53.24 76.38
A-6 70 66.78 70.1 49.98 73.72 53.22 76.42
A-4 64.5 63.38 65.66 41.49 71.26 53.1 76.14
A-3 47.92 50.03 51.48 22.72 51.52 53.1 74.68
A-2 12.66 26.2 15.2 3.8 11.06 58.2 67.78
Table 9: Global validation accuracy after activation quantization of various quantization robustness variants
for Federated Averaging ( FedAvg) on federated version of CIFAR-10 dataset using ResNet-20 architecture.
Bit ConfigFederated Averaging (FedAvg)
Baseline KURE APQN (A-4) APQN (A-2) QAT (A-4) QAT (A-2) MQAT-A
A-32 49.17 37.36 50.94 50.56 18.13 4.74 40.71
A-8 49.37 37.22 50.86 50.55 45.62 23.91 43.79
A-6 48.73 37.25 50 49.98 45.57 23.82 44.23
A-4 41.36 32.5 40.6 41.4 43.06 23.13 42.59
A-3 24.33 20.16 22.22 22.72 25.31 24.14 30.5
A-2 4.59 3.4 4.13 3.8 1.24 7.81 28.79
Table 10: Global validation accuracy after activation quantization of various quantization robustness variants
for Federated Averaging ( FedAvg) on federated version of CIFAR-100 dataset using ResNet-20 architecture.
Bit ConfigFederated Averaging (FedAvg)
Baseline Baseline (Dropout) QAT (WA-4) QAT (WA-2) MQAT-WA
WA-32/32 70.16 72.02 60.64 26.61 74.9
WA-8/8 69.86 71.62 71 28.08 77.3
WA-6/6 69.92 70.58 71.38 29.42 76.8
WA-4/4 63.12 64.08 70.04 36.44 75.54
WA-3/3 42.98 41.8 42.54 50.12 72.9
WA-2/2 11.04 14.5 9.58 40.18 66.08
Table 11: Global validation accuracy after activation quantization of various quantization robustness variants
for Federated Averaging ( FedAvg) on federated version of CIFAR-10 dataset using ResNet-20 architecture.
27