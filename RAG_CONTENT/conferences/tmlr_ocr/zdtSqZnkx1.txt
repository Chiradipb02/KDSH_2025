Published in Transactions on Machine Learning Research (05/2024)
Continual HyperTransformer:
A Meta-Learner for Continual Few-Shot Learning
Max Vladymyrov mxv@google.com
Google Research
Andrey Zhmoginov azhmogin@google.com
Google Research
Mark Sandler sandler@google.com
Google Research
Reviewed on OpenReview: https: // openreview. net/ forum? id= zdtSqZnkx1
Abstract
We focus on the problem of learning without forgetting from multiple tasks arriving sequen-
tially, where each task is deﬁned using a few-shot episode of novel or already seen classes.
We approach this problem using the recently published HyperTransformer (HT), a
Transformer-based hypernetwork that generates specialized task-speciﬁc CNN weights di-
rectly from the support set. In order to learn from a continual sequence of tasks, we propose
to recursively re-use the generated weights as input to the HT for the next task. This way,
the generated CNN weights themselves act as a representation of previously learned tasks,
and the HT is trained to update these weights so that the new task can be learned without
forgettingpasttasks. Thisapproachisdiﬀerentfrommostcontinuallearningalgorithmsthat
typically rely on using replay buﬀers, weight regularization or task-dependent architectural
changes. We demonstrate that our proposed Continual HyperTransformer method
equipped with a prototypical loss is capable of learning and retaining knowledge about past
tasks for a variety of scenarios, including learning from mini-batches, and task-incremental
and class-incremental learning scenarios.
1 Introduction
Continual few-shot learning involves learning from a continuous stream of tasks described by a small number
of examples without forgetting previously learned information. This type of learning closely resembles how
humans and other biological systems acquire new information, as we can continually learn novel concepts
with a small amount of information and retain that knowledge for an extended period of time. Algorithms for
continual few-shot learning can be useful in many real-world applications where there is a need to classify a
large number of classes in a dynamic environment with limited observations. Some practical applications can
includeenablingrobotsto continuallyadapttochangingenvironmentsbasedonanincomingstreamofsparse
demonstrations or allowing for privacy-preserving learning, where the model can be trained sequentially on
private data sharing only the weights without ever exposing the data.
To tackle this problem, we propose using HyperTransformer (HT; Zhmoginov et al. 2022), a recently
published few-shot learning method that utilizes a large hypernetwork (Ha et al., 2016) to meta-learn from
episodes sampled from a large set of few-shot learning tasks. The HTis trained to directly generate weights
of a much smaller specialized Convolutional Neural Network (CNN) model using only few labeled examples.
This works by decoupling the domain knowledge model (represented by a Transformer; Vaswani et al. 2017)
from the learner itself (a CNN), generated to solve only a given speciﬁc few-shot learning problem.
1Published in Transactions on Machine Learning Research (05/2024)
We present a modiﬁcation to HTmethod, called Continual HyperTransformer (CHT), that is aimed
at exploring the capability of the HTtosequentially update the CNN weights with the information from a
new task, while retaining the knowledge about the tasks that were already learned. In other words, given
the CNN weights θt−1generated after seeing some previous tasks 0,...,t−1and a description of the new
taskt, theCHTgenerates the weights θtthat are suited for all the tasks 0,...,t.
In order for the CHTto be able to absorb a continual stream of tasks, we modiﬁed the loss function from
a cross-entropy that was used in the HTto a more ﬂexible prototypical loss (Snell et al., 2017), that uses
prototypes as a learned representation of every class from all the tasks. As the tasks come along, we maintain
and update a set of prototypes in the embedding space. The prototypes are then used to predict the class
and task attributes for a given input sample.
We evaluate CHTin three realistic scenarios where a continual few-shot learning model like ours might be
used: the mini-batch version, where every task consists of the same classes; the lifelong learning version,
where classes for all the tasks are drawn from the same overall distribution; and the heterogeneous task
semantic version, where every task has its own unique distribution of classes.
We also test CHTin two diﬀerent continual learning scenarios: task-incremental learning (predicting class
attributes using the task information) and class-incremental learning (predicting class attributes without
access to task information; also known as lifelong learning). Moreover, we show empirically that a model
trained for class-incremental learning can also perform well in task-incremental learning, similar to a model
speciﬁcally trained for task-incremental learning.
Our approach has several advantages. First, as a hypernetwork, the CHTis able to generate and update
the weights of the CNN on the ﬂy with no training required. This is especially useful for applications
That require generating a lot of custom CNN models (such as user-speciﬁc models based on their private
data). A trained Transformer holds the domain world-knowledge and can generalize from limited few-shot
observations.
Second, we did not observe catastrophic forgetting in CHT models when evaluating them on up to 5 tasks
using Omniglot andtieredImageNet datasets. We even see cases of the positive backward transfer for
smaller generated CNN model, where the performance on a given task actually improves for subsequently
generated weights.
Third, while the CHTis trained to optimize for Ttasks, the model can be stopped at any point t≤T
during the inference with weights θtthat are suited for all the tasks 0≤τ≤t. Moreover, the performance
of a given weight θtimproves when the CHTis trained on more tasks T.
Finally, we designed the CHTmodel to be independent from a speciﬁc step and operate as a recurrent
system. It can be used to learn a larger number of tasks it was originally trained for.
2 Related work
Few-shot learning Many few-shot learning methods can be divided into two categories: metric-based
learning and optimization-based learning. First, metric-based methods (Vinyals et al., 2016; Snell et al.,
2017; Sung et al., 2018; Oreshkin et al., 2018) train a ﬁxed embedding network that works universally for
any task. The prediction is based on the distances between the known embeddings of the support set and
the embeddings of the query samples. These methods are not speciﬁcally tailored for the continual learning
problem, since they treat every task independently and have no memory of the past tasks.
Second,optimization-based methods (Finn et al., 2017; Nichol & Schulman, 2018; Antoniou et al., 2019; Rusu
et al., 2019) propose to learn an initial ﬁxed embedding, which is later adapted to a speciﬁc task using a few
gradient-based steps. They are not able to learn continually, as simply adapting the embedding for a new
task will result in the catastrophic forgetting of previously learned information. In addition, in contrast to
these methods, our proposed CHTgenerates the CNN weights directly on the ﬂy, with no additional gradient
adaptations required. This means that for a given task we do not have to use second-order derivatives, which
improves stability of our method and reduces the size of the computational graph.
2Published in Transactions on Machine Learning Research (05/2024)
Figure 1: In continual few-shot learning, the model learns from Ttasks sequentially. For the ﬁrst task (task
0), the CNN weights θ0are generated using only the support set S(0). For each subsequent task t, the
Continual HyperTransformer (CHT) uses the support set S(t)and the previously generated weights
θt−1to generate the weights θt. To update the weights ψof the CHT, the loss is calculated by summing
the individual losses computed for each generated weight θtwhen evaluated on the query set of all the prior
tasks (Q(τ))T
τ=0.
Continual learning Most continual learning methods can be grouped into three categories based on
their approach to preventing catastrophic forgetting when learning a new task: rehearsal, regularization
and architectural (see Pasunuru et al. 2021; Biesialska et al. 2020 for an overview). Rehearsal methods
work by injecting some amount of replay data from past tasks while learning the new task (Lopez-Paz &
Ranzato, 2017; Riemer et al., 2018; Rolnick et al., 2019; Gupta et al., 2020; Wang et al., 2021a) or distilling
a part of a network using task-conditioned embeddings (Mandivarapu et al., 2020; Von Oswald et al., 2019).
Regularization methods introduce an explicit regularization function when learning new tasks to ensure that
old tasks are not forgotten (Kirkpatrick et al., 2017; Zenke et al., 2017). Architectural methods modify the
network architecture with additional task-speciﬁc modules (Rusu et al., 2016), ensembles (Wen et al., 2020)
or adapters (Pfeiﬀer et al., 2020) that allow for separate routing of diﬀerent tasks.
We believe that our approach requires the least conceptual overhead compared to the techniques above,
since it does not impose any additional explicit constraints to prevent forgetting. Instead, we reuse the same
principle that made HTwork in the ﬁrst place: decoupling the specialized representation model (a CNN)
from the domain-aware Transformer model. The Transformer learns how to best adapt the incoming CNN
weights in a way that the new task is learned and the old tasks are not forgotten. In this sense, the closest
analogy to our approach would be slow and fast weights (Munkhdalai & Yu, 2017), with the Transformer
weights being analogous to the slow weights that accumulate the knowledge and generate CNN weights as
fast weights.
Incremental few-shot learning A related, but distinct area of research is incremental few-shot learning
(Gidaris & Komodakis, 2018; Ren et al., 2019; Perez-Rua et al., 2020; Chen & Lee, 2020; Tao et al., 2020;
Wang et al., 2021b; Shi et al., 2021; Zhang et al., 2021; Mazumder et al., 2021; Lee et al., 2021; Yin et al.,
2022). There, the goal is to adapt a few-shot task to an existingbase classiﬁer trained on a largedataset,
without forgetting the original data. The typical use case for this would be a single large classiﬁcation model
that needs to be updated once in a while with novel classes. These methods do not work when the existing
base classiﬁer is not available or hard to obtain. Another issue is applicability. These methods have to be
retrained for each new sequence of tasks, which severely limits their practical application. In contrast, CHT
is meta-trained on a large public set of many few-shot classes. After training, it can generate task-dependent
θin a matter of seconds for many novel sequences.
Consider the following use-case scenario. There is a large public database of many few-shot classes available
on the server, and a large number of users with private sequential few-shot data. There is a whole range of
applications one can think of in this space, for example smartphone hotword detection or robot calibration
3Published in Transactions on Machine Learning Research (05/2024)
of unique environments in users’ homes. Incremental few-shot learning would not be able to either train a
reliable base classiﬁer due to the large number of classes on the server, nor eﬀectively deal with user’s private
data, as it needs to be re-trained each time on the users’ devices, which is a lengthy process.
On the other hand, our Continual HyperTransformer can be trained on a large public dataset on the
server (with as many classes as needed, as it trains episodically and does not require a single monolithic
classiﬁer) and then shipped to devices. Each user can then generate or update their model on the ﬂy without
any training required, in a matter of seconds. In the paper we have demonstrated the ability of the CHT
to update the user’s private classiﬁers with novel instances of already existing classes (Section 6.1) as well
as new classes (Section 6.2 and 6.3) on the ﬂy without retraining the system. This would be impossible or
require lengthy retraining with any existing approach.
Perhaps the closest to our setting is the paper by Antoniou et al. (2020) which focuses on the general problem
deﬁnition of the continual few-shot learning, but falls short of providing a novel method to solve it. Another
related method is Wang et al. (2023) which proposes a heterogeneous framework that uses visual as well as
additional semantic textual concepts. In our paper we only focus on visual input for model training and
predictions.
3 Continual few-shot learning
We consider the problem of continual few-shot learning, where we are given a series of Ttasks, where each
taskt:={S(t),Q(t)}is speciﬁed via a K-wayN-shot support set S(t):= (x(t)
i,y(t)
i)NK
i=0and a query set
Q(t):= (ˆx(t)
i,ˆy(t)
i)ˆNK
i=0, whereKis the number of classes in each task, Nis the number of labeled examples
for each class, and ˆN(typically ˆN/greatermuchN) is the number of query examples to be classiﬁed.
We assume that the classes composing each individual task are drawn from the same distribution uniformly
at random without replacement. However, we consider diﬀerent ways in which classes for diﬀerent tasks
are chosen. First, each task may include exactly the same set of classes. This is similar to mini-batch
learning with Titerations, where each batch contains exactly Nexamples of each of Kclasses1. Second,
each task might include a diﬀerent set of classes, but drawn from the same overall distribution of classes.
This corresponds to a lifelong learning scenario, where tasks can be thought of as observations that allow us
to learn more about the world as we encounter new classes during the inference. Finally, each task might have
its own unique semantic meaning and the classes for diﬀerent tasks are drawn from diﬀerent distributions.
We will evaluate all of these scenarios in our experiments.
Figure 1 illustrates the process of learning of a continual few-shot problem. For each of the tasks t∈0,...,T,
a learneraψ(parameterized by ψ) needs to produce CNN weights θtbased on the support set S(t)of taskt
and previously generated weights θt−1(except for the ﬁrst task, where θtis generated only using S(0)):
θt:=aψ(S(t),θt−1), (1)
such thatθtcan predict the classes from all the tasks τ∈0,...,t. Notice that when learning from task t,
the learner does not have access to the support set of past tasks and must rely solely on the input weights
θt−1as a source of information from previous tasks.
After the weights θtare generated, we can use the query set Q(τ)of all tasks τ∈0,...,tto evaluate the
prediction quality of the θtand calculate the loss Lψwith respect to the learner parameters ψ. In this work,
we consider two types of predictions given the weights θt:
•Task-incremental learning , in which the goal is to identify the class attribute given the sample and
its task attribute: p(ˆy=k|ˆx,τ).
•Class-incremental learning , in which the goal is to identify both class and task attributes of the
samples:p(ˆy=k,τ|ˆx).
1This scenario does not require continual learning per se, as the classes do not change between the tasks.
4Published in Transactions on Machine Learning Research (05/2024)
Figure 2: The information ﬂow of the HyperTransformer (HT) model ( left) compared to the proposed
Continual HyperTransformer (CHT) model ( right). In the original HT, the input weight embeddings
are initialized with empty placeholders. In contrast, the proposed CHTmodel incorporates information
from past tasks when generating weights for the current task. The weight slice information from previously
learned tasks is passed as input to the new iteration of the CHT. The CHTuses the support set for the
current task and the input weight information to generate the weights. This allows the CHTto retain
knowledge about past tasks and avoid forgetting when learning new tasks.
Finally, we can test the performance of the trained model aψon episodes sampled from a holdout set of
classesCtest. Notice that, in general, the total number of tasks for the test Ttestmight be diﬀerent from T.
4 Continual HyperTransformer
Notice that for T= 1, the continual learning problem above reduces to a standard few-shot learning problem
deﬁnedbyasinglefew-shotlearningtask t0={S(0),Q(0)}. Onemethodthathasbeeneﬀectiveinsolvingthis
type of problem is HyperTransformer (HT, Zhmoginov et al., 2022) that uses a self-attention mechanism
to generate CNN weights θdirectly from the support set of the few-shot learning problem (see Figure 2,
left).
We ﬁrst describe the HTarchitecture. CNN weights are constructed layer by layer using the embeddings
of the support set and the activations of the previous layer. After the weights have been generated, the
cross-entropy loss Lψ(fθ(ˆx),ˆy)is calculated by running the query set (ˆx,ˆy)through the generated CNN.
To encode the knowledge of the training task distribution HTis using Transformer model that generates the
resulting CNN layer-by-layer. The input of the Transformer consists of concatenation of image embeddings,
activation embeddings and support sample labels. The image embedding is produced by a convolutional
feature extractor, this time shared among all the layers. The activation embedding is given by another
feature extractor that is applied to the activation of the previous label (or the input, in case of the ﬁrst
layer). Together, the parameters of Transformer, image embedding and activation embedding constitute the
HTparameters ψ.
Ourproposed Continual HyperTransformer (CHT)naturallyextends HTtohandleacontinualstream
oftasksbyusingthegeneratedweightsfromalreadylearnedtasksasinputweightembeddingsintotheweight
generator for a new task (see Figure 2, right). In this way, the learned weights themselves act as both the
input and the output of the CHT, performing a dual function: storing information about the previous tasks
as well as serving as the weights for the CNN when evaluating on tasks that have already been seen.
For each task t, the CHTtakes as input the support set of that task S(t)as well as the weights from
the previous tasks θt−1, and generates the weights using the equation (1) that are suited for all the tasks
τ∈0,...,t. Therefore, for each step twe want to minimize the loss on the query sets of every task up to t:
Jt(ψ) =t/summationdisplay
τ=0Lψ/parenleftBig
fθt(ˆx(τ)),ˆy(τ)/parenrightBig
. (2)
5Published in Transactions on Machine Learning Research (05/2024)
The overall loss function is simply the sum of the losses for all tasks:
arg min
ψT/summationdisplay
t=0Jt(ψ). (3)
TheCHTgenerates a sequence of weights {θτ}t
τ=0, such that each weight is suited for all tasks up to the
current task: θ0performs well only on task 0,θ1performs well on tasks 0and1, and so on. This allows
the model to eﬀectively learn and adapt to a stream of tasks, while also maintaining good performance on
previously seen tasks.
This design allows for a “preemptive” approach to continual learning, where the CHTmodel can be trained
onTtasks, and run for any number of tasks τ <T, producing well-performing weights θτfor all the tasks
seen up to that point. An alternative approach would be to specify the exact number of tasks in the sequence
in advance, and only consider the performance after the ﬁnal task T. This would correspond to minimizing
only the last term JT(ψ)in the equation (3). However, in our experiments, we did not observe any signiﬁcant
improvement using this approach compared to the one we have described above.
Another desirable property of the proposed CHTarchitecture is its ability to be recurrent. The parameters
of the HTdo not depend on task information, and only take the weights θand the support set as input.
This means that it is not only possible to preempt CHTat some earlier task, but also extend the trained
model to generate weights for additional tasks beyond the ones it was trained. We will demonstrate this
ability in the experimental section.
Algorithm 1 Class-incremental learning using HyperTransformer with Prototypical Loss.
Input:Trandomly sampled K-wayN-shot episodes:{S(t);Q(t)}T
t=0.
Output: The loss value Jfor the generated set of tasks.
1:J←0 ⊿Initialize the loss.
2:θ−1←0 ⊿Initialize the weights.
3:fort←0toTdo
4:θt←aψ(S(t),θt−1) ⊿Generate weight for current task.
5: fork←0toKdo ⊿Compute prototypes for every class of the current task.
6:ctk←1
N/summationtext
(x,y)∈S(t)fθt(x)1y=k
7: end for
8: forτ←0totdo ⊿Update the loss with every seen query set using the equation (6).
9: fork←0toKdo
10: J←J−/summationtext
(ˆx,ˆy)∈Q(τ)logp(ˆy=k,τ|ˆx)1ˆy=k
11: end for
12: end for
13:end for
4.1 Prototypical loss
The last element of the algorithm that we have left to discuss is the exact form of loss function Lψ(·)in the
equation (2). The original HTused the cross-entropy loss, which is not well suited for continual learning
because the number of classes that it predicts is tied to the number of parameters in the head layer of the
weightsθ. This means that as the number of tasks increases, the architecture of CNN needs to be adjusted,
which goes against our design principle of using a recurrent CHTarchitecture. Another option would be
to ﬁx the head layer to the K-way classiﬁcation problem across all the tasks and only predict the class
information within tasks (a problem known as domain-incremental learning; Hsu et al., 2018). However,
this would cause classes with the same label but diﬀerent tasks to be minimized to the same location in
the embedding space, leading to collisions. Additionally, since class labels are assigned at random for each
training episode, the collisions would occur randomly, making it impossible for CHTlearn the correct class
assignment. In the Appendix A.1, we show that the accuracy of this approach decreases dramatically as the
number of tasks increases and becomes impractical even for just two tasks.
6Published in Transactions on Machine Learning Research (05/2024)
To make the method usable, we need to decouple the class predictions of every task while keeping the
overall dimensionality of the embedding space ﬁxed. One solution is to come up with a ﬁxed arrangement
ofTKpoints, but any kind of such arrangement is suboptimal because it is not possible to place TKpoints
equidistant from each other in a ﬁxed-dimensional space for large T. A much more elegant solution is to
learn the location of these class prototypes from the support set itself, e.g. with a prototypical loss (Snell
et al., 2017). The prototypes are computed by averaging the embeddings of support samples from a given
classkandtaskτ:
cτk:=1
N/summationdisplay
(x,y)∈S(τ)fθτ(x)1y=k. (4)
We can use the prototypes in two diﬀerent continual learning scenarios. First, for the task-incremental
learning, we are assumed to have access to the task we are solving and need to predict only the class
information. The probability of the sample belonging to a class kgiven the task τis then equal to the
softmax of the /lscript2distance between the sample and the prototype normalized over the distances to the
prototypes from all the classes from τ:
p(ˆy=k|ˆx,τ) :=exp(−/bardblfθt(ˆx)−cτk/bardbl2)/summationtext
k/primeexp(−/bardblfθt(ˆx)−cτk/prime/bardbl2). (5)
Second, for more general class-incremental learning , we need to predict class attributes across all seen tasks.
The probability of a sample belonging to class kof taskτis equal to the softmax of the /lscript2distance between
the sample and the prototype, normalized over the distances to the prototypes from all classes for all tasks:
p(ˆy=k,τ|ˆx) :=exp(−/bardblfθt(ˆx)−cτk/bardbl2)/summationtext
τ/primek/primeexp(−/bardblfθt(ˆx)−cτ/primek/prime/bardbl2). (6)
The ﬁnal loss function is given by minimizing the negative log probability of the chosen softmax over the
query set. The pseudo-code for the entire CHTmodel is described in Algorithm 1.
Empirically, we noticed that the CHTmodels trained with the class-incremental learning objective (6)
perform equally well in both class-incremental and task-incremental settings, while models trained with
the task-incremental objective (5) perform well only in the task-incremental setting and rarely outperform
models trained with the equation (6). Therefore, we will focus on CHTmodels trained with the equation
(6) and evaluate them for both task- and class-incremental learning scenarios.
Notice that the prototypes are computed using the current weights θτin the equation (4) for task τ, but they
are used later to compare the embeddings produced by subsequent weights θtin equation (6). Ideally, once
the new weights θtare generated, the prototypes should be recomputed as well. However, in true continual
learning, we are not supposed to reuse the support samples after the task has been processed. We have
found that freezing the prototypes after they are computed provides a viable solution to this problem, and
the diﬀerence in performance compared to recomputing the prototypes every step is marginal.
Finally, we want to highlight an important use-case where recomputing the prototypes might still be possible
or even desirable. The weights θtare not aﬀected by this issue and are computed in a continual learning
manner from the equation (1) without using information from the previous task. The support set is only
needed to update the prototypes through generated weights, which is a relatively cheap operation. This
means that it is possible to envision a privacy-preserving scenario in which the weights are updated and
passed from client to client in a continual learning manner, and the prototypes needed to “unlock” those
weights belong to the clients that hold the actual data.
5 Connection Between Prototypical Loss and MAML
While the core idea behind the prototypical loss is very natural, this approach can also be viewed as a special
case of a simple 1-step MAML-like learning algorithm. This can be demonstrated by considering a simple
classiﬁcation model q(x;φ) =s(Wfθ(x) +b)withφ= (W,b,θ), wherefθ(x)is the embedding and s(·)
7Published in Transactions on Machine Learning Research (05/2024)
is a softmax function. MAML algorithm identiﬁes such initial weights φ0that any task τwith just a few
gradient descent steps initialized at φ0brings the model towards a task-speciﬁc local optimum of Lτ.
Notice that if any label assignment in the training tasks is equally likely, it is natural for q(x;φ0)to not prefer
any particular label over the others. Guided by this, let us choose W0andb0that are label-independent .
Substituting φ=φ0+δφintoq(x;φ), we then obtain
q/lscript(x;φ) =q/lscript(x;φ0) +s/prime
/lscript(·)/parenleftbigg
δW/lscriptfθ0(x) +δb/lscript+W0
/lscript∂f
∂θ(x;θ0)δθ/parenrightbigg
+O(δφ2),
where/lscriptis the label index and δφ= (δW,δb,δθ). The lowest-order label-dependent correction to q/lscript(x;φ0)
is given simply by s/prime
/lscript(·)(δW/lscriptfθ0(x) +δb/lscript). In other words, in the lowest-order, the model only adjusts the
ﬁnal logits layer to adapt the pretrained embedding fθ0(x)to a new task.
For a simple softmax cross-entropy loss (between predictions q(x)and the groundtruth labels y), a single
step of the gradient descent results in the following logits weight and bias updates:
δWi,·=γ
n/summationdisplay
(x,y)∈S/parenleftbigg
1y=k−1
|C|/parenrightbigg
fθ0(x), δbk=γ
n/summationdisplay
(x,y)∈S/parenleftbigg
1y=k−1
|C|/parenrightbigg
, (7)
where the 1/|C|term results from normalization in the softmax operation. Here γis the learning rate, n
is the total number of support-set samples, |C|is the number of classes and Sis the support set. In other
words, we see that the label assignment imposed by δWandδbfrom the equation (7) eﬀectively relies on
computing a dot-product of fθ0(x)with “prototypes” ck:=N−1/summationtext
(x,y)∈Sfθ0(x)1y=k.
6 Experiments
Most of our experiments were conducted using two standard benchmark problems using Omniglot and
tieredImageNet datasets. The generated weights for each task θtare composed of four convolutional
blocks and a single dense layer. Each of the convolutional blocks consist of a 3×3convolutional layer, batch
norm layer, ReLU activation and a 2×2max-pooling layer. Preliminary experiments have showed that batch
norm is important to achieve the best accuracy. For Omniglot we used 8ﬁlters for convolutional layers
and20-dim FC layer to demonstrate how the network works on small problems, and for tieredImageNet
we used 64ﬁlters for convolutional and 40-dim for the FC layer2to show that the method works for large
problems as well. The models were trained in an episodic fashion, where the examples for each training
iteration are sampled from a given distribution of classes. The reported accuracy was calculated from 1024
random episodic evaluations from a separate test distribution, with each episode run 16times with diﬀerent
combinations of input samples.
For the HTarchitecture, we tried to replicate the setup used in the original paper as closely as possible. We
used a 4-layer convolutional network as a feature extractor and a 2-layer convolutional model for computing
activation features. For Omniglot we used a 3-layer, 2-head Transformer and for tieredImageNet , we
used a simpliﬁed 1-layer Transformer with 8 heads. In all our experiments, we trained the network on a
single GPU for 4Msteps with SGD with an exponential LR decay over 100 000steps with a decay rate of
0.97. We noticed some stability issues when increasing the number of tasks and had to decrease the learning
rate to compensate: for Omniglot experiments, we used a learning rate 10−4for up to 4tasks and 5×10−5
for5tasks. For tieredImageNet , we used the same learning rate of 5×10−6for training with any number
of tasksT. We trained the CHTmodels with the class-incremental objective (6), but evaluated them for
both task-incremental and class-incremental scenarios.
2In contrast with cross-entropy, we do not need to have the head layer dimension to be equal to the number of predicted
labels when using the Prototypical Loss.
8Published in Transactions on Machine Learning Research (05/2024)
6.1 Learning from mini-batches
We ﬁrst consider a case where every task includes the same set of classes. Speciﬁcally, we compared the
following three models using a set of four 5-way 1-shot support set batches S(1),...,S(4)that consist of the
same set of classes from tieredImageNet :
θ(a)≡aψ(S(1)+S(2)+S(3)+S(4),θ0),
θ(b)≡aψ(S(3)+S(4),aψ(S(1)+S(2),θ0)),
θ(c)≡aψ(S(4),aψ(S(3),aψ(S(2),aψ(S(1),θ0)))),
where +operation denotes a simple concatenation of diﬀerent support set batches. For this experiment, we
used the cross-entropy loss (since the label set was the same for all S(i)) and each support set batch S(i)
contained a single example per class. We observed that the test accuracies for θ(a),θ(b)andθ(c)were equal
to67.9%,68.0%and68.3%respectively, all within the statistical error range ( ±0.4%). At the same time,
HTtrained with just S(1)orS(1)+S(2)(with 1 or 2 samples per class respectively) performed signiﬁcantly
worse, reaching the test accuracies of 56.2%and62.9%respectively. This demonstrates that the proposed
mechanism of updating generated CNN weights using information from multiple support set batches can
achieve performance comparable to processing all samples in a single pass with HT.
6.2 Learning from tasks within a single domain
Next, we consider a scenario where the tasks consist of classes drawn from a single overall distribution. We
present the results of two models: one trained on 20-way, 1-shot tasks with classes sampled from Omniglot
dataset, and anther trained on 5-way, 5-shot tasks with classes sampled from tieredImageNet dataset.
We compare the performance of CHTto two baseline models. The ﬁrst is a Constant ProtoNet
(ConstPN ), which represents a vanilla Prototypical Network, as described in Snell et al. (2017). In this
approach, a universal ﬁxed CNN network is trained on episodes from Ctrain. This constant network can be
applied to every task separately by projecting the support set as prototypes for that task and computing
the prediction with respect to these prototypes. Strictly speaking, this is not a continual learning method,
since it treats every task independently and has no memory of previous tasks. For the best results on this
baseline, we had to increase the number of classes by a factor of 5during training (e.g. for 20-wayOmniglot
evaluation we have trained it with 100-way problems).
The second baseline we used speciﬁcally for the class-incremental learning is a Merged HyperTrans-
former (MergedHT ), where we combine all the tasks and train a single original HTinstance as a single
task. This method does not solve a continual learning problem, since it has the information about all the
tasks from the beginning, but it produces a solution for every class and task that we can still be compared
to the weights generated by the CHT.
Each trained model is applied to both task-incremental (Figure 3) and class-incremental (Figure 4) settings.
To understand the eﬀect of continual learning with multiple tasks, each column represents a separate run of
theCHTtrained onT= 2,3,4or5tasks in total (for training a higher T, see the results in the Appendix).
To demonstrate the recurrence of the method, we extended the number of tasks to 5for the evaluation
regardless of how many tasks it was trained on. Each plot shows 5 curves corresponding to the CHT, split
into two groups: bullet marker ( •) for tasks that the model was trained for and diamond marker ( /diamondmath) for
extrapolation to more tasks.
Task-incremental learning. We start by analysing the task-incremental learning results. For the Om-
niglotdataset, we saw no signs of catastrophic forgetting for the CHT. In fact, we observed a positive
backwardknowledgetransfer, wheretheperformanceonpasttasks improved asmoreweightsweregenerated.
For example, in most cases, the performance of θ1(green markers) was higher than θ0(orange markers), and
θ2was higher than both θ1andθ0. Additionally, as the number of tasks increased, the overall performance
of the CHTalso increased, with the model trained on T= 5tasks performing better than the one trained
onT= 2tasks.
9Published in Transactions on Machine Learning Research (05/2024)
T= 2tasks T= 3tasks T= 4tasks T= 5tasks
Omniglot , 8 channelsAccuracy
012340.840.850.860.870.88
01234
01234
01234
tieredImageNet , 64 channels
012340.680.700.720.74
01234
01234
01234
Task Name
θ0θ1θ2θ3θ4ConstPN
Figure 3: Task-incremental learning on Omniglot andtieredImageNet . Each column represents a
diﬀerent CHTtrained with a total of T= 2,3,4or5tasks. The tasks marked with a bullet symbol ( •)
correspond to the terms in the objective function (3) that are being minimized. The lines marked with the
diamond symbol ( /diamondmath) show the extrapolation of the trained CHTto a larger number of tasks. The conﬁdence
intervals do not exceed 0.5%.
T= 2tasks T= 3tasks T= 4tasks T= 5tasks
Omniglot , 8 channelsAccuracy
00-10-20-30-40.650.700.750.800.850.90
00-10-20-30-4
00-10-20-30-4
00-10-20-30-4
tieredImageNet , 64 channels
00-10-20-30-40.600.650.700.75
00-10-20-30-4
00-10-20-30-4
00-10-20-30-4
Task Name
θ0θ1θ2θ3θ4ConstPN MergedHT
Figure 4: Class-incremental learning on Omniglot andtieredImageNet . Each column represents a
diﬀerent CHTtrained with a total of T= 2,3,4or5tasks. The tasks marked with a bullet symbol ( •)
correspond to the terms in the objective function (3) that are being minimized. The lines marked with the
diamond symbol ( /diamondmath) show the extrapolation of the trained CHTto a larger number of tasks. The conﬁdence
intervals do not exceed 0.5%.
10Published in Transactions on Machine Learning Research (05/2024)
Table 1: Performance comparison of diﬀerent methods for learning up to 5 tasks on Omniglot dataset.
Method Task name
00-10-20-30-4
Pretraining 38.421.18.34.83.9
MAML++ 81.458.233.524.419.8
EWC 33.420.49.04.73.7
Continual HT (ours) 87.280.075.176.869.3
For the tieredImageNet dataset, the results were better than the ConstPN baseline, but the positive
backward knowledge eﬀect eﬀect was not as pronounced as it was for the Omniglot dataset. The perfor-
mance for every training task remained roughly the same for all generated weights, indicating that the model
did not suﬀer from catastrophic forgetting.
Overall, the CHTconsistently outperformed the ConstPN baseline, particularly when applied to the same
or lower number of tasks it was trained on. Although the accuracy of the CHTdid decrease slightly when
it was applied to more tasks than it was trained on, this decrease was not signiﬁcant. In fact, even when
CHTwas trained on only T= 3tasks, generating weights for one of two additional tasks still resulted in
better performance than the ConstPN baseline.
Class-incremental learning. In the class-incremental learning setting, the task name is given by two
numbers indicating the range of tasks we used for evaluation (e.g. task name 0-3 corresponds to four tasks
from 0 to 3). The black constant dashed line is the baseline performance of the ConstPN , which uses a
ﬁxed embedding and does not diﬀerentiate between tasks. The starred blue markers represent a separate
run of the HTfor a particular conﬁguration of merged tasks.
As one can see in the Figure 4, the accuracy of all the models decreased as more tasks were included in
the prediction. This was expected because the size of the generated CNN did not change, but the number
of classes that needs to be predicted was increasing. For Omniglot dataset we again saw the positive
backwards transfer taking place, with CHTmodels trained on more tasks Tperforming better overall. For
a given model trained on a ﬁxed T, the performance was comparable. This demonstrates the preemptive
property of the CHT, where models trained for a certain number of tasks can still be run for any smaller
number of tasks with similar performance.
When comparing the results to the baselines, the CHThad better results than the ConstPN up to the
number of tasks Tit was trained for, and the extrapolation results improved as Tincreases. Interestingly,
for the case of T= 5theCHTwas able to outperform even the MergedHT baseline for the Omniglot ,
even though the MergedHT had access to information about all tasks from the beginning. This suggests
that having more classes to classify makes the learning problem diﬃcult for the original HT, as the image
embeddings may not be able to learn good embeddings. This is particularly evident in the tieredImageNet
dataset, where the performance of the MergedHT is so low that it falls below 60%, even for the 0-1 task.
We have also compared the results of CHTwith other, more traditional baselines from few-shot learning
and continual learning literature. Speciﬁcally, we consider the following methods:
•Pretraining . Pretraining a network on a full training set, then ﬁne-tuning it on an input sequence
of tasks from the test set.
•MAML++ . An example of a Few-Shot Learning method (Antoniou et al., 2019).
•EWC. An example of a Continual Learning method, we run EWC (Kirkpatrick et al., 2017) on a
pretrained network from above.
Table 1 shows the results. Notice that MAML++ performed well on the ﬁrst task, but not able to retain
the knowledge for subsequent tasks, because of catastrophic forgetting. Pretraiing methods and EWC were
not able to learn suﬃciently well even the ﬁrst task, because they were not able to learn from few-examples.
6.3 Learning from tasks across multiple domains
In the experiments described above, the support and query sets for each task were drawn from the same
general distribution, and the image domain remained consistent across all tasks. If the tasks were drawn
11Published in Transactions on Machine Learning Research (05/2024)
from diﬀerent distributions and diﬀerent image domains, we would expect task-agnostic ConstPN approach
to suﬀer in accuracy because it would need to ﬁnd a universal representation that works well across all image
domains. In contrast, the CHTapproach could adapt its sample representations diﬀerently for diﬀerent
detected image domains, leading to improved performance.
We verify this by creating a multi-domain episode generator that includes tasks from various image datasets:
Omniglot ,Caltech101 ,CaltechBirds2011 ,Cars196 ,OxfordFlowers102 andStanfordDogs .
We compared the accuracy of the ConstPN andCHTon this generator using episodes containing two
tasks with 5-way, 1-shot problems. The generated CNN model had 16channels with 32channels for the ﬁnal
layer. Other parameters were the same as those used in the tieredImageNet experiments. The ConstPN
achieved the accuracy of 53%for task 0,52.8%for task 1and 50.8%for combined tasks. The CHT
achieved the accuracy of 56.2%for task 0,55.2%for task 1and53.8%for combined tasks. The accuracy
gap of nearly 3%between these two methods, which is larger than the gap observed in the Omniglot
andtieredImageNet experiments, suggests that the CHTis better at adapting to a multi-domain task
distribution.
7 Discussion
While the computational cost of training the hypernetwork can be quite large, the training process is a one-
time investment on a meta-training distribution of tasks. Once trained, the HyperTransformer can generate
target CNN architectures for new sets of tasks in a matter of seconds (e.g., 10 seconds as demonstrated
in our experiments). This eﬃciency in generating new architectures is a key advantage when operating in
dynamic task environments.
Since HyperTransformer requires a separate Transformer network in order to generate a target CNN, whose
architecture and optimization parameters can be considered hyper-parameters of our method. However, in
scenarios with novel datasets, a practitioner may need to explore hyperparameter tuning to optimize perfor-
mance. A separate sensitivity analysis needs to be done to understand the role of various hyperparatemeters
of a generative Transformer network.
8 Conclusions
The proposed Continual HyperTransformer model has several attractive features. As an eﬃcient
few-shot learner, it can generate CNN weights on the ﬂy with no training required, using only a small set
of labeled examples. As a continual learner, it is able to update the weights with information from new
tasks by iteratively passing them through HT. Empirically, we have shown that the learning occurs without
catastrophic forgetting and may even result in positive backward transfer. By modifying the loss function
from cross-entropy to the prototype loss, we deﬁned a learning procedure that optimizes the location of the
prototypes of all the classes of every task. A single trained CHTmodel can be used in both task-incremental
and class-incremental scenarios.
9 Broader Impact Statement
This paper is focused on developing new methods for continual few-shot learning. We do not envision these
methods being used for harmful purposes more so than other algorithms in a class of few-shot or continual
learning.
The authors of this paper are committed to making sure that the methods we develop are used with safety
concerns in mind. We believe that continual few-shot learning has the potential to be a powerful tool for
good, but it is important to use it responsibly. While the technology itself does not directly facilitate harm
to living beings or raise immediate safety concerns, its broader impact needs to be carefully considered
in light of potential ethical, societal, and environmental implications. We are committed to working with
the research community to mitigate potential risks. We welcome any feedback from the reader regarding a
potential misuse of the methods described in the paper that we did not describe in this section.
12Published in Transactions on Machine Learning Research (05/2024)
10 Acknowledgements
The authors would like to thank Nolan Miller, Gus Kristiansen, Jascha Sohl-Dickstein and Johannes von
Oswald for their valuable insights and feedback throughout the project.
References
Antreas Antoniou, Harrison Edwards, and Amos J. Storkey. How to train your MAML. In 7th Interna-
tional Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net, 2019.
Antreas Antoniou, Massimiliano Patacchiola, Mateusz Ochal, and Amos Storkey. Deﬁning benchmarks for
continual few-shot learning. arXiv preprint arXiv:2004.11967 , 2020.
MagdalenaBiesialska, KatarzynaBiesialska, andMartaRCosta-Jussa. Continuallifelonglearninginnatural
language processing: A survey. arXiv preprint arXiv:2012.09823 , 2020.
Kuilin Chen and Chi-Guhn Lee. Incremental few-shot learning via vector quantization in deep embedded
space. In International Conference on Learning Representations , 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on
Machine Learning , volume 70 of Proceedings of Machine Learning Research , pp. 1126–1135. PMLR, 06–11
Aug 2017.
Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pp. 4367–4375, 2018.
Gunshi Gupta, Karmesh Yadav, and Liam Paull. Look-ahead meta learning for continual learning. Advances
in Neural Information Processing Systems , 33:11588–11598, 2020.
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106 , 2016.
Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning scenar-
ios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488 , 2018.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic
forgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526, 2017.
Eugene Lee, Cheng-Han Huang, and Chen-Yi Lee. Few-shot and continual learning with attentive indepen-
dent mechanisms. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.
9455–9464, 2021.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in
neural information processing systems , 30, 2017.
Jaya Krishna Mandivarapu, Blake Camp, and Rolando Estrada. Self-Net: Lifelong learning via continual
self-modeling. Frontiers in Artiﬁcial Intelligence , 3:19, 2020.
Pratik Mazumder, Pravendra Singh, and Piyush Rai. Few-shot lifelong learning. arXiv preprint
arXiv:2103.00991 , 2021.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine Learning ,
pp. 2554–2563. PMLR, 2017.
Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint
arXiv:1803.02999 , 2(3):4, 2018.
13Published in Transactions on Machine Learning Research (05/2024)
Boris N. Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. TADAM: task dependent adaptive metric
for improved few-shot learning. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montréal, Canada , pp. 719–729, 2018.
Ramakanth Pasunuru, Veselin Stoyanov, and Mohit Bansal. Continual few-shot learning for text classiﬁca-
tion. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp.
5688–5702, 2021.
Juan-Manuel Perez-Rua, Xiatian Zhu, Timothy M Hospedales, and Tao Xiang. Incremental few-shot object
detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
13846–13855, 2020.
Jonas Pfeiﬀer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun
Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers. arXiv preprint
arXiv:2007.07779 , 2020.
Mengye Ren, Renjie Liao, Ethan Fetaya, and Richard Zemel. Incremental few-shot learning with attention
attractor networks. Advances in Neural Information Processing Systems , 32, 2019.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro.
Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint
arXiv:1810.11910 , 2018.
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay
for continual learning. Advances in Neural Information Processing Systems , 32, 2019.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671 , 2016.
Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and
Raia Hadsell. Meta-learning with latent embedding optimization. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 , 2019.
Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming Zhan, and Xiao-Ming Wu. Overcoming catastrophic
forgetting in incremental few-shot learning by ﬁnding ﬂat minima. Advances in Neural Information Pro-
cessing Systems , 34:6747–6761, 2021.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in
neural information processing systems , 30, 2017.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales. Learning
to compare: Relation network for few-shot learning. In 2018 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018 , pp. 1199–1208. IEEE
Computer Society, 2018. doi: 10.1109/CVPR.2018.00131.
Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, and Yihong Gong. Few-shot class-
incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 12183–12192, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,
December 4-9, 2017, Long Beach, CA, USA , pp. 5998–6008, 2017.
14Published in Transactions on Machine Learning Research (05/2024)
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks
for one shot learning. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and
Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pp. 3630–3638,
2016.
Johannes Von Oswald, Christian Henning, João Sacramento, and Benjamin F Grewe. Continual learning
with hypernetworks. arXiv preprint arXiv:1906.00695 , 2019.
Liyuan Wang, Qian Li, Yi Zhong, and Jun Zhu. Few-shot continual learning: a brain-inspired approach.
arXiv preprint arXiv:2104.09034 , 2021a.
Xin Wang, Yue Liu, Jiapei Fan, Weigao Wen, Hui Xue, and Wenwu Zhu. Continual few-shot learning with
transformer adaptation and knowledge regularization. In Proceedings of the ACM Web Conference , volume
2023, 2023.
Yu Wang, Nicholas J Bryan, Mark Cartwright, Juan Pablo Bello, and Justin Salamon. Few-shot continual
learning for audio classiﬁcation. In ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pp. 321–325. IEEE, 2021b.
Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to eﬃcient ensemble
and lifelong learning. arXiv preprint arXiv:2002.06715 , 2020.
Li Yin, Juan M Perez-Rua, and Kevin J Liang. Sylph: A hypernetwork framework for incremental few-
shot object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 9035–9045, 2022.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In
International Conference on Machine Learning , pp. 3987–3995. PMLR, 2017.
Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, and Yinghui Xu. Few-shot incremental learning
with continually evolved classiﬁers. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 12455–12464, 2021.
Andrey Zhmoginov, Mark Sandler, and Max Vladymyrov. Hypertransformer: Model generation for super-
vised and semi-supervised few-shot learning. arXiv preprint arXiv:2201.04182 , 2022.
15Published in Transactions on Machine Learning Research (05/2024)
0 1 2 3 4
Task name0.20.40.60.8Accuracy
θ0θ1θ2θ3θ4ConstPN
Figure 5: The accuracy of the HTmodel trained
forT= 5using the cross-entropy loss. The ac-
curacy of the ﬁrst weight θ0is high and is better
than the accuracy of the ConstPN model’s em-
beddings. However, when more tasks are added,
the accuracy drops dramatically due to collisions
between the same classes for diﬀerent tasks in the
cross-entropy loss.
Accuracy
0 1 2 3 40.870.88
0 0-1 0-2 0-3 0-40.60.70.80.9
Task name
min eq. (5):
θ0θ1θ2θ3θ4
min eq. (6):
θ0θ1θ2θ3θ4
Figure 6: CHTtrained using task-incremental ob-
jective (5) vs. class-incremental objective (6).
A Additional experiments and ﬁgures
A.1 Learning with cross-entropy loss
Figure 5 shows the results of an attempt to do learn multiple tasks using a HTwith a cross-entropy loss.
Since the size of the last layer’s embedding is not increased, the model can only predict the class labels
within the task and not the task themselves, which corresponds to the domain-incremental learning setup.
Additionally, the same class from diﬀerent tasks are mapped to the same location in the embedding space,
leading to collisions when more tasks are added. This is why the accuracy drops signiﬁcantly as the number
of tasks increases. On the other hand, ConstPN model is more ﬂexible because the prototypes for each
task are computed from the support set of that task and do not have to be ﬁxed to a one-hot vector as in
the cross-entropy loss.
A.2 Training using task-incremental and class-incremental objectives
Figure 6 compares the accuracy of two diﬀerent models trained with task-incremental (using equation (5))
and class-incremental (using equation (6)) objectives. The performance of both models on task-incremental
problems are similar, while the model trained with the class-incremental objective performs better on class-
incremental problems.
A.3 Analysis of prototypical embeddings using UMAP
To better understand the quality of the learned prototypes, we conducted an experiment on a tieredIma-
geNet5-way, 5-shot problem. We selected a random episode of 5 tasks and ran them through the model,
producing weights θ0toθ4along with the prototypes for each class of every task. We then computed the
logits of the query sets, which consisted of 20samples per class for each task. The resulting 40-dim em-
beddings for the prototypes and query sets were concatenated and projected onto a 2D space using UMAP
(Figure 7). Note that the prototypes from the earlier tasks remain unchanged for the later task, but their
2DUMAPprojections are diﬀerent, because UMAPis a non-parametric method and it must be re-run for
every newθk. We tried our best to align the embedding using the Procrustes alignment method.
The plot shows that the embeddings of the tasks are well separated in the logits space, which helps explain
why the model performs well for both task- and class-incremental learning. Normalizing the softmax over
the classes within the same tasks or across all tasks made little diﬀerence when the tasks are so far away
from each other. On the right of Figure 7, we show the projection of the ConstPN embedding of the same
16Published in Transactions on Machine Learning Research (05/2024)
θ0 θ1 θ2 θ3 θ4 ConstPN
Figure 7: Left 5 plots: theUMAP projection of the CHTprototypes and the query set embeddings for
diﬀerent generated weights, where the points are colored based on the task information. The query set
points are connected with their corresponding prototypes using dashed lines. Right plot: UMAPprojection
of the ConstPN embedding for 25diﬀerent classes from tieredImageNet . Embeddings are aligned using
Procrustes alignment.
θ0 θ1 θ2 θ3 θ4
Figure 8: The UMAPprojections of 20-dimensional embeddings of the prototypes and query set for diﬀerent
weights obtained from incremental HTtraining. The query set points are connected to their corresponding
prototypes using dashed lines. In the top plot, the points are colored according to their class information,
while in the bottom plot they are colored according to their task information. Embeddings are aligned using
Procrustes alignment.
25classes. The ConstPN model does not make a distinction between tasks and treats each class separately.
The fact that 3clusters emerge has to do purely with the semantics of the chosen clusters and the way the
ConstPN model groups them. This also helps to explain why the CHTmodel performs better than the
ConstPN , as it separates the tasks before separating the classes within each task.
TheUMAPembedding for the Omniglot dataset using ProtoNet (Figure 8) appears to be diﬀerent from
similar embedding projection of tieredImageNet dataset. In particular, the embeddings from diﬀerent
tasks seem to overlap, while in the tieredImageNet embedding they are separated. This may be due to the
factthattheclassesinthe Omniglot datasetaremorecloselyconnectedthanthoseinthe tieredImageNet
dataset. Interestingly, despite the overlap between the classes from diﬀerent tasks, the ﬁnal accuracy is still
high and only slightly degrades as more tasks are added.
A.4 Learning with more tasks
Our analysis primarily focused on the performance of the CHTon up to 5 tasks. However, as shown in
Figure 9 the CHTmodel is capable of handling a much larger number of tasks T. Similar to the results in
17Published in Transactions on Machine Learning Research (05/2024)
Omniglot , 8 channel Omniglot , 32 channel
0
0-1
0-2
0-3
0-4
0-5
0-6
0-7
0-8
0-9
0-10
0-11
0-12
0-13
0-14
0-15
0-16
0-17
0-18
0-19
Task name0.50.60.70.8AccuracyNumber of tasks 10
Number of tasks 15
Number of tasks 20
0
0-1
0-2
0-3
0-4
0-5
0-6
0-7
0-8
0-9
0-10
Task name0.800.850.900.95AccuracyNumber of tasks 8
Number of tasks 9
Number of tasks 10
Number of tasks 11
Figure 9: Omniglot with 8 or 32 channels trained with a diﬀerent number of tasks T.
012345678910111213141516171819
Task name0.8600.8650.870Accuracyθ5
θ19
0-1
0-2
0-3
0-4
0-5
0-6
0-7
0-8
0-9
0-10
0-11
0-12
0-13
0-14
0-15
0-16
0-17
0-18
0-19
Task name0.50.60.70.8Accuracyθ5
θ19
Figure 10: Omniglot with 8 channels trained for T= 20tasks. Here we show the ﬁnal weight θ19generated
along with an intermediate θ5for task-incremental ( left) and class-incremental ( right) learning.
the main text, the nearly overlapping curves in the graph indicate that the model trained for Ttasks can
maintain the same level of accuracy when applied to a larger number of tasks.
Figure 10 shows 8-channel Omniglot evaluated on task-incremental and class-incremental objectives.
A.5 Continual HyperTransformer vs MergedHT for tieredImageNet
Figure 11 shows a zoomed out view of the results presented in Figure 4). It illustrates the signiﬁcant
diﬀerence in performance between the MergedHT and the CHTmodels.
A.6 Additional ﬁgures for Omniglot task-incremental and class-incremental learning
Figures 12, 13, 14 and 15 show additional experiments with the Omniglot dataset using diﬀerent number
of channels in the CNN.
T= 2tasks T= 3tasks T= 4tasks T= 5tasksAccuracytieredImageNet , 64 channels
00-10-20-30-40.30.40.50.60.7
00-10-20-30-4
00-10-20-30-4
00-10-20-30-4
Task Name
θ0θ1θ2θ3θ4ConstPN MergedHT
Figure 11: Zoomed out view of Figure 4 so that the results of the MergedHT is visible.
18Published in Transactions on Machine Learning Research (05/2024)
T= 2tasks T= 3tasks T= 4tasks T= 5tasks
Task-incremental learningAccuracy
012340.6250.6500.675
012340.6250.6500.675
012340.6250.6500.675
012340.6250.6500.675
Class-incremental learning
00-10-20-30-40.40.6
00-10-20-30-40.40.6
00-10-20-30-40.40.6
00-10-20-30-40.40.6
Task Name
θ0θ1θ2θ3θ4
Figure 12: Task-incremental and class-incremental learning on the Omniglot dataset with 4-channels con-
volutions.
T= 2tasks T= 3tasks T= 4tasks T= 5tasks
Task-incremental learningAccuracy
012340.780.80
012340.780.80
012340.780.80
012340.780.80
Class-incremental learning
00-10-20-30-40.40.60.8
00-10-20-30-40.40.60.8
00-10-20-30-40.40.60.8
00-10-20-30-40.40.60.8
Task Name
θ0θ1θ2θ3θ4
Figure 13: Task-incremental and class-incremental learning on the Omniglot dataset with 6-channels con-
volutions.
19Published in Transactions on Machine Learning Research (05/2024)
T= 2tasks T= 3tasks T= 4tasks T= 5tasks
Task-incremental learningAccuracy
012340.930.940.95
012340.930.940.95
012340.930.940.95
012340.930.940.95
Class-incremental learning
00-10-20-30-40.800.850.900.95
00-10-20-30-40.800.850.900.95
00-10-20-30-40.800.850.900.95
00-10-20-30-40.800.850.900.95
Task Name
θ0θ1θ2θ3θ4
Figure 14: Task-incremental and class-incremental learning on the Omniglot dataset with 16-channels
convolutions.
T= 2tasks T= 3tasks T= 4tasks T= 5tasks
Task-incremental learningAccuracy
012340.9500.9550.960
012340.9500.9550.960
012340.9500.9550.960
012340.9500.9550.960
Class-incremental learning
00-10-20-30-40.850.900.95
00-10-20-30-40.850.900.95
00-10-20-30-40.850.900.95
00-10-20-30-40.850.900.95
Task Name
θ0θ1θ2θ3θ4
Figure 15: Task-incremental and class-incremental learning on the Omniglot dataset with 32-channels
convolutions.
20