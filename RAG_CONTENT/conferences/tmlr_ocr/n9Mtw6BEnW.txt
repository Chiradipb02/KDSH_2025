Under review as submission to TMLR
Bias-inducing geometries: an exactly solvable data model
with fairness implications
Anonymous authors
Paper under double-blind review
Abstract
Machine learning (ML) may be oblivious to human bias but it is not immune to its
perpetuation. Marginalisation and iniquitous group representation are often traceable
in the very data used for training, and may be reflected or even enhanced by the
learningmodels. Inthepresentwork, weaimtoclarifytheroleplayedbydatageometry
in the emergence of ML bias. We introduce an exactly solvable high-dimensional model
of data imbalance, where parametric control over the many bias-inducing factors allows
for an extensive exploration of the bias inheritance mechanism. Through the tools of
statistical physics, we analytically characterise the typical properties of learning models
trained in this synthetic framework and obtain exact predictions for the observables
that are commonly employed for fairness assessment. Simplifying the nature of the
problem to its minimal components, we can retrace and unpack typical unfairness
behaviour observed on real-world datasets. Finally, we focus on the effectiveness of
bias mitigation strategies, first by considering a loss-reweighing scheme, that allows
for an implicit minimisation of different unfairness metrics and a quantification of the
incompatibilities between existing fairness criteria. Then, we propose a mitigation
strategy based on a matched inference setting that entails the introduction of coupled
learning models. Our theoretical analysis of this approach shows that the coupled
strategy can strike superior fairness-accuracy trade-offs.
Introduction
Machine Learning (ML) systems are actively being integrated into multiple aspects of our lives, making
the question about their failure points of utmost importance. Recent studies (Buolamwini & Gebru, 2018;
Weidinger et al., 2021) have shown that these systems may have a significant disparity in failure rates across
the multiple sub-populations targeted in the application. ML systems appear to perpetuate discriminatory
biases that align with those present in our society (Benjamin, 2019; Noble, 2018; Eubanks, 2018; Broussard,
2018).
Bias could originate at many levels in the ML pipeline, from the problem definition to data collection, to the
training and deployment of the ML algorithm (Suresh & Guttag, 2021). Without minimising the importance
of the other factors, we will focus this study on data itself, which often represents a critical source of bias
(Perez, 2019). A dataset can inadvertently contain the record of a history of discriminatory behaviour, tangled
in complex dependencies which are hardly eradicated even when the explicit discriminatory attribute is
removed. The root of the discrimination can indeed be hidden in the structural properties of the dataset,
since different sub-populations are almost inevitably heterogeneously represented. Thus, an important open
question is when and how such heterogeneity can induce bias in ML systems.
Disproportional numerical representation of the different sub-populations in a dataset is of course the most
visible – but not only possible – form of representation heterogeneity. Learning with an unbalanced dataset,
where some classes are underrepresented, has been shown to drastically bias the outcome of a classifier
(Kotsiantis et al., 2006; Wang et al., 2021). Furthermore, imbalances in the relative representation can become
particularly problematic in the high-dimensional, feature-rich regime (Chen & Wasikowski, 2008). In this
work, however, we aim at identifying the many other geometrical properties of data that can systematically
lead to biased trained models.
ML bias can be prevented or removed by implementing targeted heuristics in the training pipeline. A vast
literature focuses on the study of bias mitigation methods in the context of real-world data, either by revising
the data sampling step or by adjusting the optimisation objective. Several methods have been shown to be
effective in correcting for class imbalances in standard classification settings, including oversampling (Chawla
et al., 2002), undersampling (Liu et al., 2008) and reweighing strategies. In the general framework, the class
1Under review as submission to TMLR
label and sub-population membership do not necessarily overlap but some of these ideas can be adapted to
allow for bias mitigation (Wang et al., 2020; Idrissi et al., 2022). Despite many empirical successes, a large
gap remains in the theoretical understanding of bias-induction mechanisms and how to counteract them. The
introduction of a controlled minimal setting , where these phenomena can be characterised exactly, could allow
for a better theoretical grasp of these nuanced interactions.
In this work, we aim to address this theory gap by introducing the Teacher-Mixture (T-M) model, a novel
exactly-solvable generative model producing high-dimensional correlated data. This model offers a controlled
setting where data imbalances and the emergence of bias become more transparent and can be better
understood, allowing also for the design of theoretically grounded and effective solutions. The model is
designed to capture common observations about the data structure of real datasets, with a particular focus
on the coexistence of non-trivial correlations, both among inputs and between inputs and labels, induced
by the presence of a sub-population structure. Surprisingly, the few ingredients encoded in the model are
capable of generating a rich and realistic ML bias phenomenology.
The rest of the work is structured as follows: in Sec. 1 we describe the T-M model and derive an analytical
characterisation of the typical performance of solutions in the high-dimensional limit. Sec. 2 examines
the different sources of bias (shown in Fig. 1B) and their role in the bias-induction mechanism in a sub-
population-agnostic shallow network. This leads also to the identification of a positive transfer effect among
the sub-populations within the dataset: despite their distinct characteristics, which make it tempting to
split the dataset and use different classifiers, the shared underlying features can be leveraged to enhance the
performance of a single classifier on both groups. Finally in Sec. 3, we focus on the problem of mitigating bias
when the membership information is accessible. We theoretically analyse the effects of a sample reweighing
mitigation strategy, highlighting the trade-offs between different definitions of fairness. We also propose and
analyse a model-matched mitigation strategy, where two coupled networks are jointly trained, allowing for
specialisation on different sub-populations as well as transfer of valuable cross-population information.
1 Modelling Data Imbalance
Drug testing provides a historically significant example of the potential consequences of unchecked data
imbalance: substantial evidence (Hughes, 2007; 2019; Perez, 2019) shows that the scarcity of data points
corresponding to female individuals in drug-efficiency studies resulted in a larger number of side effects in
their group. This historical data gap has often been justified on the basis of a “simplification” criterion: due
to the inherent variance of the female sub-population (caused e.g. by fluctuating hormonal levels), their
inclusion in medical trials can introduce complex interactions that are instead absent in the “standard” male
sub-population. However, ignoring biological sex as a discriminative factor in the analysis can induce serious
adverse effects on the female sub-population, ranging from over-dosage to ineffectiveness of treatment.
The Teacher-Mixture (T-M) model is designed to allow a theoretical characterisation of the impact of such
data imbalances on the inference process (e.g., determining a discriminative rule for administering the drug to
the patients). While retaining analytical tractability, the T-M model retraces the main features of real data
with multiple coexisting sub-populations and allows for a richer phenomenology than previously analysed
data models. In Fig. 1A, we sketch a 2-dimensional cartoon of the T-M data distribution, framed in the
context of drug testing.
The T-M combines aspects of two common modelling frameworks for supervised learning, namely the
Gaussian-Mixture (GM) and the Teacher-Student (TS) setups (Zdeborová & Krzakala, 2016). The GM
is a simple model of clustered input data, where each data point is sampled from one out of a narrow set
of high-dimensional Gaussian distributions. Instead, the T-M inherits from the TS setup a simple model
of input-label correlation, where the ground-truth labels are produced by a realisable "teacher" rule, to be
inferred by the trained model during the learning process. For simplicity, in this study, we will only consider
linear labelling rules. In the T-M, however, we allow for the existence of group-specific rules: at inference,
the model will have to strike a compromise between them. Many different factors, parametrically controlled
in the T-M, can generate bias in a classifier, T-M allows to explore different realisation of the problem as
shown in Fig. 1B.
2Under review as submission to TMLR
Figure 1: The Teacher-Mixture (T-M) model can account for several types of data imbalance .
Panel A The T-M model is a generative model of high-dimensional structured data. Inputs are sampled
from a combination of multivariate Gaussian distributions, with different centroids and covariances for each
sub-population in the dataset. The probability of sampling from each sub-population can be tuned, giving
rise to representation imbalance. In particular, the cartoon shows a larger relative representation for the
male population ( ♂), which also has a smaller variance. The cyan and yellow shaded regions (green in their
intersection) denote the decision boundaries of the labelling rules for the different data sub-populations, which
in principle can be misaligned. Panel B The panel exemplifies how manipulating the parameters of the
T-M model can alter the data distribution: B.1 represents the balanced condition with equally represented,
distributed and labelled samples; B.2 shows scarcityof data points in both clusters; B.3 displays an example of
rule misalignment ; B.4 shows different sub-population variances ; B.5 shows relative representation imbalance;
B.6 represents the case of unbalanced labels ; B.7 shows a case of positive group-label correlation .
In the sketch in Fig. 1A, the female and male sub-populations are represented as partially overlapping data
clouds, with different variances and group-dependent offsets (the two features in the sketch could represent
some combination of clinical values recorded during the trial). Note that the female population is numerically
under-represented, as in the above-described real-world scenarios. The shaded areas represent the true regions
of the effectiveness of the tested drug for female/male subjects (cyan/yellow shades). As depicted in Fig. 2A,
the goal of the inference model is to infer a decision boundary for the administration of the drug based on
the observations of its effectiveness on the test subjects. While the vast majority of the subjects would be
identically classified according to the two different labelling rules (green region), some false positives could
occur if the inference only accounts for a single sub-population.
Fig. 2 shows a representation of student trained on the T-M model. If no explicit bias mitigation strategy is
employed, a heterogeneous representation of the two sub-groups will inevitably lead to a biased classifier.
In panels B and C of Fig. 2, we show that the classification accuracy on the two sub-populations, as the
fraction of data points belonging to each group (the relative representation) is varied, is biased in favour of
the majority group.
For simplicity, the results discussed in this paper will focus on the case of two groups, but the analysis could
be extended to multiple sub-populations.
Formal definition We consider a synthetic dataset of nsamplesD={xµ,yµ}n
µ=1, with xµ∈Rd,yµ∈
{1,−1}. We define the O(1)ratioα=n/dand we refer to it as the dataset size parameter. Each
input vector is i.i.d. sampled from a mixture of two symmetric Gaussians with variances ∆ ={∆+,∆−},
x∼N(±v/√
d,∆±Id×d), with respective probabilities ρand(1−ρ). The shift vector vis a Gaussian vector
with i.i.d. entries with zero mean and variance 1. The 1/√
dscaling corresponds to the high-noise noise
regime, where the two Gaussian clouds are overlapping and hard to disentangle (Mignacco et al., 2020;
Saglietti & Zdeborová, 2022), e.g. as in the case of CelebA and MEPS shown in the Appendix C. The
ground-truth labels, instead, are provided by two Gaussian teacher vectors, namely W+
TandW−
T, with
respective bias terms b+
Tandb−
T, normalized to the d-dimensional sphere1
dE[∥W±
T∥2] = 1and with mutual
overlap1
dE[W+
T·W−
T] =qT. Each teacher produces labels for the inputs with the corresponding group-
membership, namely yµ=sign/parenleftig
Wcµ
T·xµ/√
d+bcµ
T/parenrightig
, withcµ∈{+,−}. The teacher bias terms are included
in the model to control the fraction of positive and negative samples within the two sub-populations. Overall,
the geometric picture of the data distribution (sketched in Fig. 1A) is summarised by three sufficient statics,
3Under review as submission to TMLR
Figure 2: Training on T-M model and comparison between error on synthetic and real data .
Panel A Given a vector of input features and a group membership (male/female), the ground-truth label is
assigned by the associated 1-layer teacher network (represented by one of the vectors W±
T). The decision
boundaries are demarked in blue and yellow (while their intersection is coloured in green)). The labelling rules
can be aligned, i.e. the decision rule does not depend on the group membership, or misaligned as in panel A.
A 1-layer student network is given inputs xµand labelsyµ, and trained to produce the correct outputs ˆyvia
gradient descent on the loss ℓ(ˆy,y).Panel B shows the test performance (on the two sub-populations) for
a student network trained on mixed data instances with variable relative representations. Unsurprisingly,
when one sub-population is largely predominant in the dataset, the classifier becomes biased to have higher
accuracy on it. The plot shows the match between the analytic curves described in Sec. 1 (solid lines), and
numerical simulations on the synthetic framework (dots). Panel C contains a similar experiment, but with
data from the ‘CelebA’ dataset (Liu et al., 2015). Details in the Appendix C.
m±
T=1
dWWW±
T·vvvandqT, that respectively quantify the alignment of the teacher labelling rules with respect
to the shift vector, controlling the group-label correlation, and the alignment between the teacher vectors,
controlling the correlation between labels assigned to similar inputs belonging to different communities.
Given the synthetic dataset D, we study the properties of a single-layer network W, with bias term b,
producing outputs ˆyµ=sign/parenleftig
W·xµ/√
d+b/parenrightig
, and trained via empirical risk minimisation (ERM) with loss:
L(W,b) =/summationdisplay
µ∈Dℓ(W,b;xµ,yµ) +λ||WWW||2
2
2(1)
whereℓis assumed to be convex in student’s parameters and λis an external parameter that regulates the
intensity of the L2regularisation.
Given this framework, we derive a theoretical characterisation of the training performance of this learning
model and consider the possible implications from an ML fairness perspective. In particular, we aim to study
the role of data geometry and cardinality in the training of a fair classifier. To quantify the level of bias in
the predictions of the trained model, we need to choose a metric of fairness. We will employ disparate impact
(DI) (Feldman et al., 2015), an ML analog of the 80% rule (Commission et al., 1979), which allows a simple
assessment of the over-specialisation of the classifier on one of the sub-populations. In our framework, we
characterise bias against sub-population +using the following definition of
DI=p(ˆy=y|+)
p(ˆy=y|−), (2)
evaluating the ratio between test accuracy in sub-population +and sub-population −. Note that how to
measure bias is itself an active line of research, and the DI alone cannot return a full picture of the unfairness.
In Sec. 3, we compare these results with those obtained with other metrics. Notice, that the T-M model
allows to parametrically move from a model-mismatched scenario ( qT<1) where the rule to be inferred is
not in the function space of learnable rules, to a model-matched scenario ( qT= 1) where the rule is actually
learnable but, as we will discuss further in Sec. 2, the model may systematically fail to identify it. We will
discuss in detail when these failure modes occur and why.
Finally, the T-M model has, at the same time, the advantage of being simple, allowing a better understanding
of the many facets of ML bias, and the disadvantage of being simple, since some modelling assumptions
might not reflect the complexity of real-world data. For example, we ignore any type of correlation among
4Under review as submission to TMLR
the inputs other than the clustering structure. However, this modelling approach continues a long tradition
of research in statistical physics (Charbonneau et al., 2023), which has shown that theoretical insights gained
in prototypical settings can often be helpful in disentangling and interpreting the complexity of real-world
behaviour.
Remark 1 By looking at the available degrees of freedom in the T-M, several possible sources of bias naturally
emerge from the model:
•therelative representation ,ρ=n+/(n++n−), withncthe number of points in group c,c∈{+,−}.
•thegroup variance ,∆c, determining the width of the clusters.
•thegroup label frequencies , controlled through the bias terms bc
T.
•thegroup-label correlation ,mc
T.
•theinter-group similarity ,qT, which measures the alignment between the two teachers, i.e. the linear
discriminators that assign the labels to the two groups of inputs.
•thedataset size ,α, representing the ratio between the number of inputs and the input dimension.
Theoretical analysis in high-dimensions. In principle, solving Eq. 1 requires finding the minimiser of
a complex non-linear, high-dimensional, quenched random function. However, statistical physics (Mézard
et al., 1987) showed that in the limit n,d→∞,n/d=α, a large class of problems, including the T-M
model, becomes analytically tractable. In fact, in this proportional high-dimensional regime, the behaviour
of the learning model becomes deterministic and trackable due to the strong concentration properties of
a narrow set of descriptors that specify the relevant geometrical properties of the ERM estimator. The
original high-dimensional learning problem can be reduced to a simple system of equations that depends
on a set of scalar sufficient statistics, Θ ={Q=1
dWWW·WWW,m=1
dWWW·vvv,R±=1
dWWW·WWW±
T,δq,b}, respectively
representing the typical norm of the trained estimator, its magnetisation in the direction of the cluster centres,
its alignment with the two teacher vectors of the T-M, the rescaled variance of the self-overlap (details in
Appendix B), and the student bias term.
The results below summarise the main findings of the replica analysis, while all the technical details are
reported in Appendix B.
Analytical result 1 Given a specific setup of the T-M model (formal definition of the model given above,
model parameters recapped in Appendix A), in the high dimensional limit when n,d→ ∞at a fixed
ratioα=n/d, the scalar descriptors Θ ={Q,m,R±,δq,b}of the vector Wobtained by empirical risk
minimisation Eq. 1 with a generic convex loss ℓ, and their Lagrange multipliers ˆΘ={ˆQ,ˆm,ˆR±,δˆq}, converge
to deterministic quantities given by the unique fixed point of the system: Q=−2∂s(ˆΘ;λ)
∂δˆq;m=∂s(ˆΘ;λ)
∂ˆm;
R±=∂s(ˆΘ;λ)
∂ˆR±;δq= 2∂s(ˆΘ;λ)
∂ˆQ;ˆQ= 2α∂e(Θ;∆)
∂δq; ˆm=α∂e(Θ;∆)
∂m;ˆR±=α∂e(Θ;∆±)
∂R±;δˆq= 2α∂e(Θ;∆)
∂Q. with:
s(ˆΘ;λ) =1
2 (δˆq+λ)/bracketleftigg
ˆQ+
ˆm+/summationdisplay
c∈{±}mc
TˆRc
2
+/summationdisplay
c∈{±}/parenleftbig
1−(mc
T)2/parenrightbigˆR2
c+ 2
qT−/productdisplay
c∈{±}mc
T
/productdisplay
c∈{±}ˆRc/bracketrightigg
(3)
e(Θ; ∆) = Ec/bracketleftigg
Ez/summationdisplay
y=±1H/parenleftigg
−y√Q(cmc
T+bc
T) +√∆cRcz/radicalbig
∆c(Q−R2c)/parenrightigg
v(y,c,Θ)/bracketrightigg
(4)
wherec∈{+,−}∼ Bernoulli (ρ),z∼N(0,1),H(·) =1
2erfc(·/√
2)is the Gaussian tail function. The function
v(y,c,Θ), appearing in Eq. 4, depends parametrically on the scalar descriptors and entails a 1-dimensional
optimization problem: v(y,c,Θ) = maxw/bracketleftig
−w2
2−ℓ/parenleftig
y,/radicalbig
∆cδqw+/radicalbig
∆cQz+cm+b/parenrightig/parenrightig
]. The student bias
termbimplicitly solves the equation ∂be(Θ; ∆) = 0 . Eqs. 3 and 4 represent the so-called entropic and
energetic contributions appearing in the quenched free-entropy of the system (details in Appendix B).
5Under review as submission to TMLR
relative representation relative representation
DI<0.8cut
group +group -
relative representation relative representationgroup-label correlation
rule similarity
Figure 3:Simple geometrical properties cause the emergence of bias. Each point in the left diagrams
shows, for different values of the model parameters, the Disparate Impact (DI) of the trained model (darker
colours represent stronger biases). In particular, in the left diagrams, on the x-axis we vary the relative
representation ρ, while on the y-axis we explore possible values of the rule similarity qTforPanel A and
the group-label correlation m±
TforPanel B .The corresponding figures on the right show the values of the
accuracy for the two sub-populations in correspondence of the cut represented by the dashed line on the left.
The yielded fixed point values for the scalar descriptors, Θ, can be used to obtain deterministic predictions for
commonmodelevaluationmetrics, suchasthe confusion matrix orthegeneralisation error , inhigh-dimensional
realizations of the system.
The presented result was obtained through the non-rigorous replica method from statistical physics (Mézard
et al., 1987; Engel & Van den Broeck, 2001; Zdeborová & Krzakala, 2016). The derivation details are deferred
to the Appendix B. We remark that, in convex settings, the replica method was rigorously proven to yield
exact results in a range of different model settings. In particular, a lengthy but straightforward generalization
of the proofs presented in (Thrampoulidis et al., 2015; Mignacco et al., 2020; Loureiro et al., 2021) could be
derived for the T-M case, but this is out of the scope of the present work. In this manuscript, we verify the
validity of our replica theory by comparison with numerical simulations, as shown e.g. in Fig. 2B.
Analytical result 2 In the same limit as in Analytical result 1, the entries of the confusion matrix, repre-
senting the probability of classifying as ˆyan instance sampled from sub-population cwith true label y, are
given by:
p(ˆy|y;c) =Ez/bracketleftigg
Heav/parenleftig
y/parenleftig/radicalbig
∆cz+cmc
T+bc
T/parenrightig/parenrightig
H/parenleftigg
−ˆy(cm+b) +√∆cRcz/radicalbig
∆c(Q−R2c)/parenrightigg/bracketrightigg
, (5)
wherez∼N(0,1)andHeav (·)is the Heaviside step function. The generalisation error, representing the
fraction of wrongly labelled instances, can then be obtained as ϵg=Ec/bracketleftig/summationtext
ˆy̸=yp(ˆy|y;c)/bracketrightig
.
This second result yields a fully deterministic estimate of the accuracy of the trained model on the different
data sub-populations. These scores will be used in the following sections to investigate the possible presence
of bias in the classification output of the model. In particular, they will be useful to estimate numerator
and denominator of the DI, Eq. 2. Note that the results 1 and 2 allow for an extremely efficient and exact
evaluation of the learning performance in the T-M, remapping the original high-dimensional optimisation
problem onto a system of deterministic scalar equations that can be easily solved by recursion.
2 Investigating the sources of bias
With these analytical results in hand, we now turn to systematically investigating the effect of the sources of
bias identified in remark 1, which potentially mine the design of a fair classifier. We specialise on cross-entropy
loss and perform three separate experiments to summarise some distinctive features of the fairness behaviour
in the T-M: namely, the impact of the correlation between the labelling rules and the group structure, the
interplay between relative representation and group variance, and the different accuracy trade-offs between
the sub-populations at different dataset sizes. The parameters of the experiments, if not specified in the
caption, are detailed in the Appendix B.1.
6Under review as submission to TMLR
Figure 4:Emergence of bias even in balanced datasets. We show the disparate impact as the distribution
of the two subpopulations is changed by altering their variances ( ∆+and∆−). The diagonal line gives the
configurations where the two subpopulations have the same variance. The two figures consider different levels
of representation, from left to right ρ= 0.1,0.3,0.5. The latter is the situation with both subpopulations
being equally represented in the dataset. We use the red and blue colours to quantify the disparate bias
against sub-population +and−(respectively).
2.1 Group-label correlation.
In Fig. 3A, we consider a scenario where the labelling rules for the two groups are not perfectly aligned, i.e.
WWW+
T̸=WWW−
T(and/orb+
T̸=b−
T). Note that, in this case, we have a clear mismatch between the learning model,
a single linear classifier, and the true input-output structure in the data: the learning model cannot reach
perfect generalisation for both sub-populations at the same time. For simplicity, we set an equal correlation
between the two teacher vectors and the shift vector, m+
T=m−
T>0, and isolate the role of rule similarity
qT. The upper-left panel shows a phase diagram of the DI (DI <1indicating a lower accuracy on group
+), as function of the similarity of the teachers and the fraction of +samples in the dataset. As intuitively
expected, the induced bias exceeds the 80% rule when the labelling rules are misaligned and the group sizes
are numerically unbalanced (small qTandρ). Indeed, in the cut displayed in the upper-right panel, by
lowering the group-label correlation m±
Tthe gap between the measured accuracies on the two sub-populations
becomes smaller. However:
Remark 2 Even when qT= 1and the task is solvable (i.e. the classifier can learn the input-output mapping),
the trained model can still be biased.
This is shown in Fig. 3B, where a large high-bias region (DI <80%) exists. In particular, the lower-left panel
shows the cause of this effect in the presence of a non-zero group-label correlation m±
T, and in the lower-right
panel we see how this effect is more pronounced in the data-scarce regime. In all four panels, as ρreaches 0.5,
the two sub-populations become equally represented and the classifier achieves the same accuracy for both.
2.2 Bias and variance.
In Fig. 4, we plot the DI as a function of the group variances ∆±, for different values of the fraction of +
samples. One finds that the model might need a disproportionate number of samples in the two groups to
obtain comparable accuracies. We can see that:
Remark 3 Balancing the group relative representation does not guarantee a fair training outcome.
In fact, the quality of a group’s representation in the dataset can increase if the number of points is kept
constant but the group variance is reduced. The blue regions in the left panel indicate a higher accuracy for
the smaller sub-population even if the dataset only contains 10%of samples belonging to it. This exemplifies
the fact that a very focused distribution (low ∆±) actually requires less samples. The right panel ( ρ= 0.5)
shows the scenario one would expect a priori: on the diagonal line the DI is balanced, but by setting ∆+>∆−
(or viceversa) one induces a bias in the classification.
7Under review as submission to TMLR
dataset size
dataset size
dataset size
Figure 5: Performance benefits for both subpopulations under shared training. With 10%of
the data points in sub-population +(ρ= 0.1), we compare the performance with different levels of rule
similarity (qT) as the size of the dataset is increased, showing the disparate impact in the left figure and the
individual accuracies in central and right ones. In central and right figures, the baselines –plotted in black–
show the accuracies attained when the model is trained only on the corresponding group data. The inset of
the rightmost figure highlights the differences in accuracy in the small dataset regime. When the rules are
sufficiently aligned, joint training on both groups will induce a better accuracy on the smaller sub-population
providedαis not too small. Moreover, at intermediate values of αalso the larger group can benefit from the
information transfer.
2.3 Positive transfer.
If mixing different sub-populations in the same dataset can induce unfair behaviour, one could think of
splitting the data and train independent models. In Fig. 5, we show that a positive transfer effect (Gerace
et al., 2022) can yet be traced between the two groups when the rules are sufficiently similar. This means
that the accuracy on the under-represented group is enhanced when information is shared across the two
sub-populations.
Remark 4 The performance on the smaller sub-population tends to further deteriorate if the dataset is split
according to the sub-group structure.
To clarify this point, in the left plot of Fig. 5 we show the DI as a function of the dataset size α, for several
values of the rule similarity qTand at fixed ρ= 0.10. In the centre and right plots in Fig. 5, we also display
the gain in accuracy on each sub-population when the model is trained on the full dataset, comparing with
a baseline classifier (black lines) trained only on the respective data subsets ( +in the central panel, −in
the lower panel). These two plots elucidate the positive transfer effect: for sufficiently similar rules (large
qT), both populations can benefit from shared training at intermediate dataset sizes. If the dataset is too
small (lowαregime), the lack of data combined with a high variance in the input distribution can induce
over-fitting, with a larger drop in performance for the smaller group. On the other hand, as the dataset size
becomes sufficiently large, the positive transfer effect is eventually lost for the large sub-population (large α
regime).
Connecting the results of this section to the drug testing examples, after observing that the distribution of
side effects in the female population presented a larger variance, the solution was simply to collect more data
of female subjects. Moreover, the results shown in Fig. 4 indicate the need for a higher relative representation
of female subjects in the dataset to achieve an unbiased classifier. While implementing this solution might
have introduced higher variance in the results due to the intrinsic high-variability of the data, it would have
significantly reduced the risk of administering drugs with limited testing on half of the population.
3 Mitigation strategies
To assess or ensure the fairness of a ML model on a given data distribution, a plethora of different fairness
criteria have been designed (Speicher et al., 2018; Castelnovo et al., 2022). In convex settings, any of these
criteria can be separately enforced via a hard constraint during the optimisation process (Agarwal et al.,
2018; 2019; Celis et al., 2019). However, it was proved that some criteria are completely incompatible and
cannot be exactly achieved simultaneously (Kleinberg et al., 2016; Corbett-Davies & Goel, 2018; Barocas
et al., 2019). In the same spirit of (Speicher et al., 2018), we drop the hard constraint and instead quantify
exactly how far a given trained model is from meeting the criteria. Each criterion requires the probability
of obtaining a specific classification outcome Eto be the same across the sub-populations. For example,
8Under review as submission to TMLR
FAIRNESS METRIC CONDITION
Statistical Parity P[ˆY=y|C=c] =P[ˆY=y]∀y,c
Equal Opportunity P[ˆY= 1|C=c,Y= 1] = P[ˆY= 1|Y= 1]∀c
Equal Accuracy P[ˆY=y|C=c,Y=y] =P[ˆY=y|Y=y]∀y,c
Equal OddsP[ˆY= 1|C=c,Y= 1] = P[ˆY= 1|Y= 1]∩
P[ˆY= 1|C=c,Y=−1] =P[ˆY= 1|Y=−1]∀c
Predicted Parity P[Y= 1|C= +,ˆY=y] =P[Y= 1|C=−,ˆY=y] =P[Y= 1|ˆY=y]∀y
Table 1:List of Fairness Metrics. Statistical Parity : Equal fractions of each group should be treated as
belonging to the positive class (Dwork et al., 2012; Kleinberg et al., 2016; Corbett-Davies et al., 2017). Equal
Opportunity : Each group needs to achieve equal true positive rate (Hardt et al., 2016). Equal Accuracy : Each
group is required to achieve the same level of accuracy. Equal Odds : Each group should achieve equal true
positive and false positive rates(Feldman et al., 2015; Zafar et al., 2017). Predicted Parity . Given inputs that
are classified by the model with label y, the fraction of input with true label y∗should be consistent across
sub-populations. This gives two sub-criteria: predicted parity 1 requires the condition only for y∗= 1, while
predicted parity 10 requires the condition for both y∗= 1andy∗=−1(Chouldechova, 2017).
coupled NNs
coupling strength
CelebA
coupling strength
Figure 6: Fairness-accuracy trade-off with reweighing and coupled architecture. Panel A The
figures show the effect of re-weighting and coupled architectures de-biasing methods in a instance of the
T-M model. The lowers figures shows the accuracy for subpopulation +and subpopulation −and the upper
figures show the mutual information for the several fairness metrics defined in Table 1, namely statistical
parity, equal opportunities, equal accuracy, equal odds, predicted parity 1, and predicted parity 10. The
goal of the algorithm is to identify regions with high accuracy (lower figures) and low mutual information
(higher figures) for all metrics: this would imply that fairness is approximately achieved under all the criteria.
The first three group of figures refer to the reweighing strategy, forcing higher relevance for a certain label
in each panel ( w1= 0.1,0.5,0.9) and the relative importance of a given subpopultation (parameter w+) on
the x-axis. The last panels instead refer to the proposed coupled networks strategy and the x-axis represent
the strength of the coupling γ. The figures clearly show that our strategy achieves a higher accuracy in
both subpopulations while preserving a higher level of fairness. Interestingly the minimum of the mutual
information roughly correspond to the same parameter of the coupling strength, contrarily to what observed
in the reweighing strategy. Panel B The two panels, show an example from the CelebA dataset splitting and
classifying according to the attributes “Wearing_Lipstick” and “Wavy_Hair” respectively, more details are
provided in the Appendix C.1 and C. The observations made for the synthetic model applies also in this
real-world case.
according to the definition of Equal Opportunity (Table 1), the true positive rateP(E= (ˆY= 1|Y= 1))
should not depend on the group-membership C. A natural measure of the observed dependence between E
9Under review as submission to TMLR
andCis given by the Mutual Information (MI):
I(E;C) =DKL(P[E,C]/vextendsingle/vextendsingleP[E]P[C]) =ElogP[E,C]
P[E]P[C]. (6)
The fairness condition is exactly verified only when the joint distribution factorizes, i.e. P[E,C] =P[E]P[C],
and the mutual information goes to zero. Table 1 provides some other examples of classification events E, for
some well-established fairness criteria. Note that some criteria might not be sensible in specific settings (e.g.,
Statistical Parity is unlikely to be guaranteed in a drug-testing scenario).
In the following, we consider two simple bias mitigation strategies that can be analysed within our analytical
framework. The required generalisations of the results shown in Sec. 1 are detailed in the Appendix B. First,
we study the de-biasing effect of a sample reweighing strategy where the relevance of each sample is varied
based on its label and group membership (Kamiran & Calders, 2012; Plecko & Meinshausen, 2020; Lum &
Johndrow, 2016). By adjusting the weights, one can indirectly minimise the MI relative to any given fairness
measure. We use the simultaneous quantitative predictions on the various metrics to assess the compatibility
between different fairness definitions. Then, we propose a theory-based mitigation protocol, along the lines
of protocols used in the context of multi-task learning (Rusu et al., 2016), that couples two architectures
trained in parallel.
Loss Reweighing. Recent literature shows that some fairness constraints cannot be satisfied simultaneously.
ML systems are instead forced to accept trade-offs between them (Kleinberg et al., 2016). This sort of
compromise is well-captured in the simple framework of the T-M model. The first three panels of Fig. 6a show
accuracies and MI measured with respect to the various fairness criteria while varying the two reweighing
parameters, w1andw+, which up-weigh data points with true label 1and in group +, respectively. Thus,
each loss term in Eq.1 is reweighed as:
W(c,y) =

w1w+ ifc= +,y= 1
w1(1−w+) ifc=−,y= 1
(1−w1)w+ ifc= +,y=−1
(1−w1)(1−w+)ifc=−,y=−1.(7)
By changing these relative weights one can force the model to pay more attention to some types of errors and
re-establish a balance between the accuracies on the two sub-populations. The goal is to identify a classifier
that achieves high accuracy (lower panels) while minimising the MI for different fairness metrics. Notably,
given a weight w1, these minima occur for different values of the weight w+. Onlyw1= 0.1seems to have a
value ofw+close to several minima of the MI, but this point correspond to a sharp decrease in accuracy in
both subpopulations, thus fairness is achieved but at the expense of accuracy. These results are in agreement
with rigorous results in the literature (Barocas et al., 2019), but also show how the incompatibilities between
the different constraints extend to regimes where the fairness criteria are not exactly satisfied.
Coupled Networks. The emergence of classification bias in the T-M traces back to the clear mismatch
between the generative model of data and the learning model. In order to move towards a matched inference
setting, we need to enhance the learning model to account for the presence of multiple sub-populations and
labelling rules. This inspires a mitigation strategy that we call coupled neural networks , consisting in the
simultaneous training of multiple neural networks, each one seeing a different subset of the data associated
with a different sub-population. This idea is represented by the following modified loss
Lcnn(w) =/summationdisplay
c∈±/summationdisplay
µ∈Dcℓ(Wc,bc;xµ,yµ) +λ
2/parenleftbig
||WWW+||2
2+||WWW−||2
2/parenrightbig
+γ
2||WWW+−WWW−||2
2 (8)
whereWWW±are the weights of the two networks, b±their associated bias terms, and ˆyµ
±are their respective
estimation of label yµ. The networks exchange information through the elastic penalty γthat mutually
attracts them, and the intensity of this elastic interaction is obtained by cross-validation. This approach
shares some ideas with other methods present in the literature: (Zenke et al., 2017; Saglietti et al., 2021) add
an elastic penalty term to the loss to bias the training trajectory, while (Calders & Verwer, 2010) proposes
to combine estimations from different Bayesian classifiers. However, note that these prior approaches were
tailored for different learning protocols or problem domains, and could not be applied in the problem setting
considered in this paper. Other similar optimisation strategies include simultaneous linear regression (Myers
& Myers, 1990) and multiple factor analysis, but to our knowledge, these approaches have not yet been
applied in the bias mitigation context.
10Under review as submission to TMLR
Remark 5 The coupled neural networks method allows for higher expressivity and specialisation on the
various sub-populations, while also encouraging positive transfer between similarly labelled sub-populations,
leading to better fairness-accuracy trade-offs.
The upper rightmost plots of Fig. 6a, displaying the behaviour of the mutual information as a function of
the coupling parameter for different fairness metrics, shows the key advantage of using this method. We
observe a more robust consistency among the various fairness metrics: the positions of the different minima
are now very close to each other. Moreover, the value of the coupling parameter achieving this agreement
condition is also the one that minimises the gap in terms of test accuracy between the two sub-populations,
as shown in the lower plot, without hindering the performance on the larger group. Notice that this result
does not contradict the impossibility theorem (Barocas et al., 2019) which states that statistical parity, equal
odds, and predicted parity cannot be satisfied altogether. In fact, our result only concerns soft minimisation
of each fairness metrics. The result is in agreement with (Dutta et al., 2020) whose results show that the
trade-off between fairness and accuracy vanishes when the true distribution of data is capture. Leveraging
the universal approximation property of neural networks, the coupled networks method seems a promising
direction for applications. In the panels of Fig. 6b we show promising preliminary results in the realistic
dataset CelebA1. We stress that real data often presents more complex correlations than those modelled in
the T-M, which may hinder the effectiveness of this strategy in unexpected ways.
The method of the coupled networks can be generalised to an arbitrary number of classes and sub-populations,
and can be combined with standard clustering methods when the group membership label is not available.
A future research direction will be to better investigate its range of applicability and, consequently, its
limitations in real-world scenarios. In the Appendix B and B.1 we provide additional results for this method
and we discuss the effect of training the networks on data subsets that only partially correlate with the true
group structure.
4 Discussion
The goal of this study was to design a novel generative model of high-dimensional correlated data that
allows the study of the effect of data geometry in the bias induction mechanism, in isolation from real-world
confounding factors. While a focus on each specific dataset might be required to ensure fairness in applications
with high societal impact, we believe that the study of the ML bias phenomenology in controlled synthetic
settings might allow more coherent advancements in the understanding and prevention of bias induction.
TheTeacher-Mixture (T-M) model captures non-trivial correlations among inputs and between inputs and
labels, representing various imbalances appearing in real datasets when different sub-populations coexist in
the sample. Surprisingly, with few modelling ingredients, the T-M can generate a rich and realistic ML bias
phenomenology. We derive an analytical characterisation of its performance in the high-dimensional limit,
showing agreement with numerical simulations and producing realistic unfairness behaviour. By isolating
different sources of bias, we gain insights into situations where unfairness may persist despite apparent
data balance, cautioning against relying solely on simple rebalancing techniques. We identify a positive
transfer effect among diverse sub-populations, leveraging shared underlying features to enhance performance
across groups. Additionally, we analysed the trade-offs between different ways of quantifying the model
fairness, focusing on a sample reweighing mitigation strategy that can be analytically characterised within
our framework. We also proposed a theory-based mitigation strategy that effectively promotes fairness
without compromising overall performance, as demonstrated in the T-M model. Instead of imposing an hard
constraint on a desired fairness metric which would incur in the incompatibility theorem (Barocas et al.,
2019), the coupled networks strategy minimises several fairness metrics simultaneously only approximately.
Furthermore, the strategy seems to avoid the typical fairness-accuracy trade-off. This result is in agreement
with the findings of (Dutta et al., 2020) showing that it is possible to construct a Bayes optimal classifier
that is not affected by the trade-off.
Moving forward, our model is extremely simplified with the respect to real data and practical architectures.
Future directions for our research include incorporating more complex elements into the data model cosindering
model complex data structures, for instance assuming that data live in a low-dimensional manifold of the
input space (Goldt et al., 2020) or moving away from Gaussian setting (Adomaityte et al., 2024), and
introducing the effect of feature dependencies (e.g., proxy variables) in the generated data. Another important
but challenging address for further work is to move to the non-convex optimisation setting and to more
1The illustrated checkpoints are used only to show the similarity of behaviour in synthetic data and realistic data (CelebA),
and not used or recommended to use in any face recognition systems or scenarios.
11Under review as submission to TMLR
complex model architectures, where at this time some of the analytic techniques employed in this work fail to
generalise. Howver, recent works succeeded in addressing some of this limitations, in particular considering
the multi-label classification problem (Cornacchia et al., 2023) and more than a single layer (Loureiro et al.,
2021)—despite still limited to random projection—these results could be included in future iterations of the
work. Moreover, further explorations of the efficacy and limitations of the coupled networks strategy in the
context of deep networks and more complex datasets is called for. By investigating its performance in deeper
architectures and diverse real-world datasets, and connecting to the existing literature (Sagawa et al., 2020;
Bell & Sagun, 2023), we can assess the scalability and generalisability of this approach for addressing fairness
concerns.
References
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey
Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan,
Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang
Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.
tensorflow.org/ . Software available from tensorflow.org.
Urte Adomaityte, Gabriele Sicuro, and Pierpaolo Vivo. Classification of heavy-tailed features in high
dimensions: a superstatistical approach. Advances in Neural Information Processing Systems , 36, 2024.
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. A reductions
approach to fair classification. In International Conference on Machine Learning , pp. 60–69. PMLR, 2018.
Alekh Agarwal, Miroslav Dudík, and Zhiwei Steven Wu. Fair regression: Quantitative definitions and
reduction-based algorithms. In International Conference on Machine Learning , pp. 120–129. PMLR, 2019.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning . fairmlbook.org, 2019.
http://www.fairmlbook.org .
Samuel James Bell and Levent Sagun. Simplicity bias leads to amplified performance disparities. In Proceedings
of the 2023 ACM Conference on Fairness, Accountability, and Transparency , pp. 355–369, 2023.
Ruha Benjamin. Race after technology: Abolitionist tools for the new jim code. Social Forces , 2019.
Lynn A. Blewett, Julia A. Rivera Drew, Risa Griffin, Natalie Del Ponte, and Pat Convey. IPUMS health
surveys: Medical expenditure panel survey, version 2.1 [dataset]. Minneapolis, MN: IPUMS , 2021. URL
https://doi.org/10.18128/D071.V2.1 .
Meredith Broussard. Artificial unintelligence: How computers misunderstand the world . mit Press, 2018.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender
classification. In Conference on fairness, accountability and transparency , pp. 77–91. PMLR, 2018.
Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification. Data
mining and knowledge discovery , 21(2):277–292, 2010.
Alessandro Castelnovo, Riccardo Crupi, Greta Greco, Daniele Regoli, Ilaria Giuseppina Penco, and An-
drea Claudio Cosentini. A clarification of the nuances in the fairness metrics landscape. Scientific Reports ,
12(1):1–21, 2022.
L Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K Vishnoi. Classification with fairness constraints:
A meta-algorithm with provable guarantees. In Proceedings of the conference on fairness, accountability,
and transparency , pp. 319–328, 2019.
Patrick Charbonneau, Enzo Marinari, Marc Mézard, Giorgio Parisi, Federico Ricci-Tersenghi, Gabriele
Sicuro, and Francesco Zamponi. Spin Glass Theory and Far Beyond . WORLD SCIENTIFIC, 2023. doi:
10.1142/13341. URL https://www.worldscientific.com/doi/abs/10.1142/13341 .
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority
over-sampling technique. Journal of artificial intelligence research , 16:321–357, 2002.
12Under review as submission to TMLR
Xue-wen Chen and Michael Wasikowski. Fast: a roc-based feature selection metric for small samples and
imbalanced data classification problems. In Proceedings of the 14th ACM SIGKDD international conference
on Knowledge discovery and data mining , pp. 124–132, 2008.
François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pp. 1251–1258, 2017.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments. Big data, 5(2):153–163, 2017.
US Equal Employment Opportunity Commission et al. Questions and answers to clarify and provide a
common interpretation of the uniform guidelines on employee selection procedures. US Equal Employment
Opportunity Commission: Washington, DC, USA , 1979.
Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review of fair
machine learning. arXiv preprint arXiv:1808.00023 , 2018.
Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making
and the cost of fairness. In Proceedings of the 23rd acm sigkdd international conference on knowledge
discovery and data mining , pp. 797–806, 2017.
Elisabetta Cornacchia, Francesca Mignacco, Rodrigo Veiga, Cédric Gerbelot, Bruno Loureiro, and Lenka
Zdeborová. Learning curves for the multi-class teacher–student perceptron. Machine Learning: Science
and Technology , 4(1):015019, 2023.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee,
2009.
Emily Denton, Ben Hutchinson, Margaret Mitchell, and Timnit Gebru. Detecting bias with generative
counterfactual face attribute augmentation. 2019.
Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, and Kush Varshney. Is there a trade-
off between fairness and accuracy? a perspective using mismatched hypothesis testing. In International
conference on machine learning , pp. 2803–2813. PMLR, 2020.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference , pp. 214–226,
2012.
Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning . Cambridge University Press,
2001.
Virginia Eubanks. Automating inequality: How high-tech tools profile, police, and punish the poor . St. Martin’s
Press, 2018.
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian.
Certifyingandremovingdisparateimpact. In proceedings of the 21th ACM SIGKDD international conference
on knowledge discovery and data mining , pp. 259–268, 2015.
Federica Gerace, Luca Saglietti, Stefano Sarao Mannelli, Andrew Saxe, and Lenka Zdeborová. Probing
transfer learning with a model of synthetic correlated datasets. Machine Learning: Science and Technology ,
2022.
Sebastian Goldt, Marc Mézard, Florent Krzakala, and Lenka Zdeborová. Modeling the influence of data
structure on learning in neural networks: The hidden manifold model. Physical Review X , 10(4):041044,
2020.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in
neural information processing systems , 29, 2016.
Robert N Hughes. Sex does matter: comments on the prevalence of male-only investigations of drug effects
on rodent behaviour. Behavioural pharmacology , 18(7):583–589, 2007.
13Under review as submission to TMLR
Robert N Hughes. Sex still matters: has the prevalence of male-only studies of drug effects on rodent
behaviour changed during the past decade? Behavioural pharmacology , 30(1):95–99, 2019.
Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing
achieves competitive worst-group-accuracy. In Conference on Causal Learning and Reasoning , pp. 336–351.
PMLR, 2022.
Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination.
Knowledge and information systems , 33(1):1–33, 2012.
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of
risk scores. arXiv preprint arXiv:1609.05807 , 2016.
Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas, et al. Handling imbalanced datasets: A review.
GESTS international transactions on computer science and engineering , 30(1):25–36, 2006.
Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. Exploratory undersampling for class-imbalance learning. IEEE
Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) , 39(2):539–550, 2008.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka
Zdeborová. Learning curves of generic features maps for realistic datasets with a teacher-student model.
Advances in Neural Information Processing Systems , 34:18137–18151, 2021.
Kristian Lum and James Johndrow. A statistical framework for fair predictive algorithms. arXiv preprint
arXiv:1610.08077 , 2016.
Marc Mézard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to
the Replica Method and Its Applications , volume 9. World Scientific Publishing Company, 1987.
Francesca Mignacco, Florent Krzakala, Yue Lu, Pierfrancesco Urbani, and Lenka Zdeborova. The role of
regularization in classification of high-dimensional noisy gaussian mixture. In International Conference on
Machine Learning , pp. 6874–6883. PMLR, 2020.
Raymond H Myers and Raymond H Myers. Classical and modern regression with applications , volume 2.
Duxbury press Belmont, CA, 1990.
Safiya Umoja Noble. Algorithms of oppression . New York University Press, 2018.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011.
Caroline Criado Perez. Invisible women: Data bias in a world designed for men . Abrams, 2019.
Drago Plecko and Nicolai Meinshausen. Fair data adaptation with quantile preservation. Journal of Machine
Learning Research , 21(242):1–44, 2020.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671 , 2016.
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameteri-
zation exacerbates spurious correlations. In International Conference on Machine Learning , pp. 8346–8356.
PMLR, 2020.
Luca Saglietti and Lenka Zdeborová. Solvable model for inheriting the regularization through knowledge
distillation. In Mathematical and Scientific Machine Learning , pp. 809–846. PMLR, 2022.
Luca Saglietti, Stefano Sarao Mannelli, and Andrew Saxe. An analytical theory of curriculum learning in
teacher-student networks. arXiv preprint arXiv:2106.08068 , 2021.
14Under review as submission to TMLR
Till Speicher, Hoda Heidari, Nina Grgic-Hlaca, Krishna P Gummadi, Adish Singla, Adrian Weller, and
Muhammad Bilal Zafar. A unified approach to quantifying algorithmic unfairness: Measuring individual
&group unfairness via inequality indices. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining , pp. 2239–2248, 2018.
Harini Suresh and John Guttag. Understanding potential sources of harm throughout the machine learning
life cycle. 2021.
Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A precise analysis
of the estimation error. In Conference on Learning Theory , pp. 1683–1709. PMLR, 2015.
Le Wang, Meng Han, Xiaojuan Li, Ni Zhang, and Haodong Cheng. Review of classification methods on
unbalanced data sets. IEEE Access , 9:64606–64628, 2021.
Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga
Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8919–8928, 2020.
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng,
Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models.
arXiv preprint arXiv:2112.04359 , 2021.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness
constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics , pp. 962–970. PMLR,
2017.
Lenka Zdeborová and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances
in Physics , 65(5):453–552, 2016.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In
International Conference on Machine Learning , pp. 3987–3995. PMLR, 2017.
15Under review as submission to TMLR
A Symbols and notation
xµµth data point in the training set
yµµth label in the training set
d Input dimension
n Number of data points
n+ Number of data points from sub-population +
n− Number of data points from sub-population −
α Ration between number of data points and input dimension n/d
ℓ Loss (in particular cross-entropy loss used in the analysis)
λ Ridge-regulariser’s strength
ρ Relative representation of sub-population +,ρ=n+/n
W+
T Teacher vector associated with sub-population +
W−
T Teacher vector associated with sub-population −
W±
T Shortcut notation to indicate both teacher vectors
v Shift vector separating the two sub-populations
∆+ Variance of the sub-population +
∆− Variance of the sub-population −
b+
T Threshold (or bias term) associated with teacher +
b−
T Threshold associated with teacher −
b±
T Shortcut notation to indicate thresholds associated with both teachers
qT Teacher-teacher overlap qT=1
dW+
T·W−
T
m+
T Overlap between teacher +and shift vector m+
T=1
dW+
T·v(group-label
correlation)
m−
T Overlap between teacher −and shift vector m−
T=1
dW−
T·v
m±
T Shortcut notation to indicate both overlaps between teachers and shift
vector
W Student weight vector
Ws Weight vectors for the coupled student networks
b Student bias term
bs Bias terms for the coupled student networks
Q Student-student overlap, Q=1
dW·W
m Overlap between student and shift vector, Q=1
dW·v
R+ Overlap between student and teacher associated with sub-population
+,Q=1
dW·W+
T
R− Overlap between student and teacher associated with sub-population
+,Q=1
dW·W−
T
R± Shortcut notation to indicate overlap between student and both teachers
δq Variance of the self-overlap at finite temperature (see Appendix B)
ω+ In the re-weighing strategy, indicates the coefficient associated with
data from sub-population +
ω1 In the re-weighing strategy, indicates the coefficient associated with
data labelled 1
γ In the coupled networks strategy, indicates the coupling coefficient
B Replica analysis
We will directly present the most general setting for this calculation, where the learning model is composed
of two linear classifiers (“students” in the following), coupled by an elastic penalty of intensity γ. This allows
us to characterise the novel mitigation strategy proposed in this work, while the standard case with a single
learning model can be obtained by setting γ= 0. Each student, denoted by the index s= 1,2, is assumed to
16Under review as submission to TMLR
be trained on a fraction of the full dataset Ds. Note that, in principle, the data split could not be aligned
with the group structure of the dataset.
The loss function for the coupled learning model reads:
L(W1,W2) =/summationdisplay
s=1,2/summationdisplay
µ∈Dsℓ/parenleftbiggWcµ
T·xµ
√
d+bcµ
T,Ws·xµ
√
d+bs/parenrightbigg
+/summationdisplay
s=1,2λ
2/parenleftiggd/summationdisplay
i=1W2
s,i/parenrightigg
−γ
2∥W1−W2∥2(B.1)
and we will focus in the following on the cross-entropy loss:
ℓ(y,q) =−Heav (y) logσ(q)−(1−Heav (y)) log (1−σ(q)) (B.2)
where Heav (·)is the Heaviside step function, which outputs 1for positive arguments and 0for negative ones,
andσ(x)=(1 + exp (−x))−1is the sigmoid activation function. The calculation also holds for alternative
convex losses, e.g. the Hinge loss or the MSE loss, since the only affected part is the numerical optimisation
of the proximal operator, as shown below.
Teacher partition function
In the T-M model, the label distribution is non-trivially dependent on the mutual alignment of the shift
vector v, determining the means of the two Gaussians in the input mixture, and the two teacher vectors W±
T.
Since we are allowed to fix a Gauge for one of these vectors (compatible with its distribution), we choose for
simplicity v=1to be a vector with all entries equal to 1(still normalized on the sphere of radius d). We
define the teacher partition function:
ZT=/integraldisplay
dµ(W+
T,W−
T) =/integraldisplay/productdisplay
c=±/bracketleftig
dµ(Wc
T)δ/parenleftig
|Wc
T|2−d/parenrightig
δ(Wc
T·1−dmc
T)/bracketrightig
δ/parenleftbig
W+
T·T−−dqT/parenrightbig
,
where the measures µ(T±)are in this case assumed to be factorised normal distributions. The Dirac’s
δ-functions ensure that the geometrical disposition of the model vectors is the one defined by the chosen
magnetisations m±
Tand the overlap qT, and that the vectors are normalised to the d-sphere.
At this point, and throughout this section, we use the integral representation of the δ-function:
δ(x−ad) =/integraldisplaydˆa
2π/de−iˆa(x
d−a), (B.3)
where ˆais a so-called conjugate field that plays a role similar to a Lagrange multiplier, enforcing the constraint
contained in the δ-function. We can rewrite:
ZT=/integraldisplay/productdisplay
c=±dˆQc
T
2π/d/integraldisplay/productdisplay
c=±dˆmc
T
2π/d/integraldisplaydˆqT
2π/dedΦT({m±
T,qT},{ˆQ±
T,ˆm±
T,ˆqT}), (B.4)
where the action ΦTrepresents the entropy of configurations for the teacher that satisfy the chosen geometrical
constraints. Given that the components of the teacher vectors are i.i.d., the entropy can be factorised over
them. In high dimensions, i.e. when d→∞, the integral will be dominated by "typical" configurations for
the vectors, and the integral ZTcan be computed through a saddle-point approximation. We Wick rotate
the fields in order to avoid dealing explicitly with imaginary quantities, and decompose ΦT=gTi+gTs:
gTi=−/parenleftigg/summationdisplay
cˆmc
Tmc
T+/summationdisplay
cˆQc
T+ ˆqTqT/parenrightigg
, (B.5)
gTs= log/integraldisplay
DT+/integraldisplay
DT−exp/parenleftigg/summationdisplay
cˆQc
TT2
c+/summationdisplay
cˆmc
TTc+ ˆqTT+T−/parenrightigg
.
After a few Gaussian integrations the computation of the second term yields:
gTs=/parenleftig
1−2ˆQ−
T/parenrightig
( ˆm+
T)2+/parenleftig
1−2ˆQ+
T/parenrightig
( ˆm−
T)2+ 2ˆqTˆm+
Tˆm−
T
2/parenleftig/parenleftig
1−2ˆQ−
T/parenrightig/parenleftig
1−2ˆQ+
T/parenrightig
−ˆq2
T/parenrightig−1
2log/parenleftig/parenleftig
1−2ˆQ+
T/parenrightig/parenleftig
1−2ˆQ−
T/parenrightig
−ˆq2
T/parenrightig
.
17Under review as submission to TMLR
Now, in order to complete the computation of the partition function ZT, we have impose the saddle point
condition for ΦT, which is realised when the entropy is extremised with respect to the fields we introduced.
From the associated saddle point equations one can find two useful identities:
1−mc
T2=/parenleftig
1−2ˆQ−c
T/parenrightig
/parenleftig/parenleftig
1−2ˆQ−
T/parenrightig/parenleftig
1−2ˆQ+
T/parenrightig
−ˆq2
T/parenrightig (B.6)
qT−m+
Tm−
T=ˆqT/parenleftig/parenleftig
1−2ˆQ−
T/parenrightig/parenleftig
1−2ˆQ+
T/parenrightig
−ˆq2
T/parenrightig (B.7)
Free entropy of the learning model
In this subsection we aim to achieve analytical characterisation of typical learning performance in the T-M,
i.e. to describe the solutions of the following optimisation problem:
W⋆
1,W⋆
2= argmin
W1,W2L(W1,W2;D), (B.8)
whereDrepresents a realisation of the data and L(·)was defined in Eq. B.1. In typical statistical physics
fashion, we can associate this problem with a Boltzmann-Gibbs probability measure, over the possible
configurations of the student model parameters:
P(W1,W2;D) =e−βL(W1,W2;D)
ZW, (B.9)
where the lossLplays the role of an the energy function, βis an inverse temperature and ZWis the partition
function (normalisation of the Boltzmann-Gibbs measure).
Since the loss is convex in the student parameters, when the inverse temperature is sent to infinity, β→∞,
the probability measure focuses on the unique minimiser of the loss, representing the solution of the learning
problem. In the asymptotic limit d→∞, the behaviour of this model becomes predictable since the
overwhelming majority of the possible dataset realisations (with the same configuration of the generative
parameters) will produce solutions with the same macroscopic properties (norm, test performance, etc). We
therefore need to consider a self-averaging quantity, which is independent of the specific realisation of the
dataset so that the typical learning scenario can be captured.
Thus, we aim to compute the average free-energy:
ΦW= lim
d→∞lim
β1
βd⟨logZW(WWW1,WWW2;D1,D2)⟩D1,D2. (B.10)
This type of quenched average is not easily computed because of the logfunction in the definition. The replica
trick, based on the simple identity limr→0(xr−1)/r=log(x), provides a method to tackle this computation.
One can replicate the partition function, introducing rindependent copies of the original system. Each of
them, however, sees the same realisation of the data D(the "disorder" of the system, in the statistical physics
terminology). When one takes the average over D, therreplicas become effectively coupled, and can be
intuitively interpreted as i.i.d. samples from the Boltzmann-Gibbs measure of the original problem. At the
end of the computation, one takes the analytic continuation of the integer rto the real axis and computes
the limit limr→0, re-establishing the logarithm and the initial expression.
We start by computing the replicated volume (product over the rpartition functions) Ωr(D), which is still
explicitly dependent on the sampled dataset:
Ωr(D) =/integraldisplaydµ(W+
T,W−
T)
ZT/integraldisplay/productdisplay
s,a
dba
sdWa
se−βγ
2∥Wa
1−Wa
2∥2/productdisplay
µ∈Dse−βℓ/parenleftig
Wcµ
T·xµ
√
d+bcµ
T,Wa
s·xµ
√
d+ba
s/parenrightig
,(B.11)
wheres= 1,2indexes the two coupled student models and a= 1,...,ris the replica index.
18Under review as submission to TMLR
To make progress we have to take the disorder average, i.e. the expectation over the distribution of xµas
defined in the T-M model. We can exploit δ-functions in order to replace with dummy variables, uµandλa
µ,
the dot products in the loss and isolate the input dependence in simpler exponential terms:
1 =/integraldisplay/productdisplay
µduµδ/parenleftbigg
uµ−Wcµ
T·xµ
√
d/parenrightbigg/integraldisplay/productdisplay
a,s,µ∈Dsdλa
µδ/parenleftbigg
λa
µ−Wa
s·xµ
√
d/parenrightbigg
(B.12)
=/integraldisplay/productdisplay
µduµdˆuµ
2πeiˆuµ/parenleftbigg
uµ−/summationtextd
i=1Wcµ
T,ixµ
i√
d/parenrightbigg
/integraldisplay/productdisplay
a,s,µ∈Dsdλa
µdˆλa
µ
2πeiˆλa
µ/parenleftig
λa
µ−/summationtextd
i=1Wa
s,ixµ
i√
d/parenrightig
(B.13)
We can now evaluate the expectation over the input distribution, collecting all the terms where each given
input appears. By neglecting terms that vanish in the N→∞limit, for each pattern µwe get:
Exµe−i/summationtext
aˆλµ
a/summationtextN
i=1Wa
sµ,ixµ
i√
d−iˆuµ/summationtextd
i=1Wcµ
T,ixµ
i√
d= (B.14)
=N/productdisplay
i=1e−icµ/parenleftbigg/summationtext
aˆλµ
aWa
sµ,ivi
d+ˆuµWcµ
T,ivµ
i
d/parenrightbigg
Ezµ
ie−i/parenleftbigg/summationtext
aˆλµ
aWa
sµ,i√
d+ˆuµWcµ
T,i√
d/parenrightbigg
zµ
i
=e−icµ/parenleftbigg/summationtext
aˆλµ
a/summationtext
iWa
sµ,i
d+ˆuµ/summationtext
iWcµ
T,i
d/parenrightbigg
−∆cµ
2/parenleftbigg/summationtext
abˆλµ
aˆλµ
b/summationtext
iWa
sµ,iWb
sµ,i
d+2ˆuµ/summationtext
aˆλµ
a/summationtext
iWa
sµ,iWcµ
T,i
d+(ˆuµ)2/summationtext
i(Wcµ
T,i)2
d/parenrightbigg
.
(B.15)
To get Eq. B.15, we used the fact that the noise zµis i.i.d. sampled from centred Gaussians of variance
determined by the group, and explicitly used our Gauge choice v=1. In this expression, the relevant order
parameters of the model appear, describing the overlaps between the student vectors, the shift vector and the
teacher vectors. We are thus going to introduce via δ-functions the following parameters:
•ma
s=Wa
s·1
d,mc
T=Wc
T·1
d: magentisations in the direction of the +group centre of the students and
the teachers.
•qab
s=/summationtext
iwa
siwb
si
d: self-overlap between different replicas of each student.
•Ra
sc=/summationtext
iWa
s,iWc
T,i
d: overlap between student and teacher vectors.
•qc
T=/summationtext
iT2
ci
d: norm of the teacher vectors (equal to 1by assumption).
After the introduction of these order parameters (via the integral representation of the δ-function) the
replicated volume can be expressed as:
Ωd=/integraldisplay/productdisplay
s,adma
sdˆma
s
2π/d/integraldisplay/productdisplay
sc,adRa
scdˆRa
sc
2π/d/integraldisplay/productdisplay
s,abdqab
sdˆqab
s
2π/d/integraldisplay/productdisplay
cdba
cGd
IGd
S/productdisplay
scGE(s,c)αc,sd(B.16)
whereαc,sNindicates the number of patterns from group ccontained in the data slice Dsgiven to student s.
We also introduced the interaction, the entropic and the energetic terms:
GI= exp
−/summationdisplay
s,aˆma
sma
s−/summationdisplay
s,abˆqab
sqab
s−/summationdisplay
sc,aˆRa
scRa
sc
 (B.17)
GS=/integraldisplay/productdisplay
cDTcexp/parenleftigg/summationdisplay
cˆQc
TT2
c+/summationdisplay
cˆmc
TTc+ ˆqTT+T−/parenrightigg
19Under review as submission to TMLR
×/integraldisplay/productdisplay
s,adµ(wa
s)e−βγ(wa
1−wa
2)2exp
/summationdisplay
s,aˆma
swa
s+/summationdisplay
s,abˆqab
swa
swb
s+/summationdisplay
sc,aˆRa
scwa
sTc
 (B.18)
GE(s,c) =/integraldisplaydudˆu
2πeiuˆu/integraldisplay/productdisplay
a/parenleftigg
dλadˆλa
2πeiλaˆλa/parenrightigg
e−∆c
2/summationtext
abˆλaˆλbqab
s−∆cˆu/summationtext
aˆλaRa
sc−∆c
2(ˆu)2
×/productdisplay
ae−βℓ(u+cmc
T+bc
T,λa+cma
s+ba
s)(B.19)
The shorthand notation Dx=e−x2
2√
2πis used to indicate a normal Gaussian measure. Note that, after the
factorization in the GS, the variables Tcandwa
sdenote a component of the vectors Wc
TandWa
srespectively.
Replica symmetric ansatz. To make further progress, we have to make an assumption for the structure
of the introduced order parameters. Given the convex nature of the optimisation objective B.1, the simplest
possible ansatz, the so-called replica symmetric (RS) ansatz, is fortunately exact. Replica symmetry introduces
a strong constraint for the overlap parameters, requiring the rreplicas of the students to be indistinguishable
and the free entropy to be invariant under their permutation. Mathematically, the RS ansatz implies that:
•ma
s=msfor alla= 1,...,r(same for the conjugate)
•Ra
sc=Rscfor alla= 1,...,r(same for the conjugate)
•qab
s=qsfor alla>b,qab
s=Qsfor alla=b(same for the conjugate)
•ba
s=bsfor alla= 1,...,r
Moreover, since we want to describe the minimisers of the loss, we are going to take the β→∞limit in the
Gibbs-Boltzmann measure. The replicas, which represent independent samples from it, will collapse on the
unique minimum. This is represented by the following scaling law with βfor the order parameters, which will
be used below:
Q−q=δq/β;ˆQ−ˆq=−βδˆq; ˆq∼β2ˆq; ˆm∼βˆm;ˆR∼βˆR (B.20)
Interaction term. We now proceed with the calculation of the different terms in B.16, where we can
substitute the RS ansatz. In the interaction term, neglecting terms of O(n2), we get:
Gi= exp/parenleftigg
−n/parenleftigg/summationdisplay
s/parenleftigg
ˆmsms+/summationdisplay
cˆRscRsc+ˆQsQs
2−ˆqsqs
2/parenrightigg/parenrightigg/parenrightigg
(B.21)
In theβ→∞limit the expression becomes:
log(Gi)/d=gi=−β/parenleftigg/summationdisplay
s/parenleftigg
ˆmsms+/summationdisplay
cˆRscRsc+1
2(ˆqsδqs−δˆqsqs)/parenrightigg/parenrightigg
(B.22)
Entropic term
In the entropic term the computation is more involved, due to the couplings between the Gaussian measures
for the teachers and for those of the students. We substitute the RS ansatz in expression B.18 to get:
GS=/integraldisplay
DT+/integraldisplay
DT−exp/parenleftigg/summationdisplay
cˆQc
TT2
c+/summationdisplay
cˆmc
TTc+ ˆqTT+T−/parenrightigg/integraldisplay/productdisplay
s,adµ(wa
s)e−γ
2(wa
1−wa
2)2
×/productdisplay
sexp
ˆms/summationdisplay
awa
s+1
2/parenleftig
ˆQs−ˆqs/parenrightig/summationdisplay
a(wa
s)2+1
2ˆqs/parenleftigg/summationdisplay
awa
s/parenrightigg2
+/summationdisplay
cˆRsc/summationdisplay
awa
sTc
 (B.23)
20Under review as submission to TMLR
We perform a Hubbard-Stratonovich transformation to remove the squared sum in the previous equation,
introducing the Gaussian fields zs. Then, we rewrite coupling term between the teachers as ˆqTT+T−=
ˆqT
2(T++T−)2−ˆqT
2(T2
++T2
−), and perform a second Hubbard-Stratonovich transformation, with field ˜z, to
remove the explicit coupling between T+andT−. Similarly, the elastic coupling between the students can be
turned into a linear term with fields za
12:
=/integraldisplay
D˜z/integraldisplay/productdisplay
sDzs/integraldisplay
dTc√
2πexp/parenleftigg
−1
2/summationdisplay
c/parenleftbig
1−2ˆQc
T+ ˆqT/parenrightbig
T2
c+/summationdisplay
c/parenleftig
ˆmc
T+/radicalbig
ˆqT˜z/parenrightig
Tc/parenrightigg/integraldisplay/productdisplay
aDza
12
×/integraldisplay/productdisplay
s,adµ(wa
s)/productdisplay
sexp/parenleftigg
1
2/parenleftbigˆQs−ˆqs/parenrightbig/summationdisplay
a(wa
s)2+/parenleftigg
ˆms+/summationdisplay
cˆRscTc+/radicalbig
ˆqszs+is√γza
12/parenrightigg/summationdisplay
awa
s/parenrightigg
(B.24)
After rescaling the variances of the teacher measures and centring them, one can factorise over the replica
index and take the r→0limit, obtaining the following expression for gS= logGS/d:
gS=A+/integraldisplay/productdisplay
sDzs/integraldisplay/productdisplay
cDTc/integraldisplay
D˜zlog/integraldisplay
Dz12/integraldisplay/productdisplay
sdµ(ws) exp/parenleftbigg1
2/parenleftig
ˆQs−ˆqs/parenrightig
w2
s+Bsws/parenrightbigg
(B.25)
where:
A=/summationtext
cˆmc
T2/parenleftig
1−2ˆQ−c
T/parenrightig
+ 2ˆqT(/summationtext
cˆmc
T)2
2/parenleftig/parenleftig
1−2ˆQ+
T/parenrightig/parenleftig
1−2ˆQ−
T/parenrightig
−ˆq2
T/parenrightig (B.26)
Bs=bs(T±,z±,˜z,zs) +is√γz12 (B.27)
bs= ˆms+/radicalbig
ˆqszs+/summationdisplay
c
mc
TˆRsc+ˆRsc/radicalbigg/parenleftig
1−2ˆQc
T+ ˆqT/parenrightigTc+√ˆqT/radicalbigg
1−/summationtext
c′ˆqT
(1−2ˆQc′
T+ˆqT)ˆRsc/parenleftig
1−2ˆQc
T+ ˆqT/parenrightig˜z

(B.28)
In theβ→∞limit, and considering the L2-regularisation on the student weights dµ(w)=dw√
2πe−βλ
2w2we
get:
gS=A+/integraldisplay/productdisplay
sDzc/integraldisplay/productdisplay
cDTc/integraldisplay
D˜zlog/integraldisplay
Dz12exp/parenleftigg/summationdisplay
smax
ws/parenleftbigg
−λ+δˆqs
2w2
s+Bsws/parenrightbigg/parenrightigg
(B.29)
and the maximisation gives:
w⋆
s=Bs
(λ+δˆqs); max
ws/parenleftbigg
−λ+δˆqs
2w2
s+Bsws/parenrightbigg
=B2
s
2 (λ+δˆqs)(B.30)
Substituting the above described scaling laws for the order parameters in the β→∞limit one finds that the
Aterm becomes sub-dominant and can be ignored. The remaining steps are quite tedious, but the procedure
to obtain the final result for the entropic channel is straightforward:
•Expand the sums in Eq.B.29.
•Perform the z12Gaussian integration and take the logof the result.
•Identify the terms that have even powers in the Hubbard-Stratonovich Gaussian fields and in the
teacher variables. The Gaussian integrations will kill all the remaining cross terms, so they can be
ignored.
•Perform the remaining Gaussian integrations.
•Use identities B.6 and B.7 to remove the dependence on the conjugate fields appearing in the Teacher
measure and only retain a dependence on mc
T,Qc
T, andqT.
21Under review as submission to TMLR
The final expression reads:
gS=β
2/parenleftbig/producttext
s(λ+γ+δˆqs)−γ2/parenrightbig/bracketleftigg/parenleftigg/summationdisplay
s/parenleftigg
ˆms+/summationdisplay
smc
TˆRsc/parenrightigg2
(λ+γ+δˆq¬s) + 2γ/productdisplay
s/parenleftigg
ˆms+/summationdisplay
cmc
TˆRsc/parenrightigg/parenrightigg
(B.31)
+/parenleftigg/summationdisplay
sˆqs(λ+γ+δˆq¬s)/parenrightigg
+/parenleftigg/summationdisplay
c/parenleftbig
1−mc
T2/parenrightbig/parenleftigg/summationdisplay
sˆR2
sc(λ+γ+δˆq¬s) + 2γ/productdisplay
sˆRsc/parenrightigg/parenrightigg
(B.32)
+/parenleftigg
2/parenleftbig
qT−m+
Tm−
T/parenrightbig/parenleftigg/summationdisplay
s/parenleftigg/productdisplay
cˆRsc(λ+δˆq¬s)/parenrightigg
+γ/parenleftigg/productdisplay
c/parenleftigg/summationdisplay
sˆRsc/parenrightigg/parenrightigg/parenrightigg/parenrightigg/bracketrightigg
(B.33)
where the notation ¬sdenotes the other student index with respect to the one used in the corresponding sum or
product.
Energetic term. We can compute the energetic channel for a generic student sand a generic data group c.
Each term will be multiplied by αc,s, determining the fraction of inputs from group cin the datasetDsof
students. For simplifying the notation in this subsection we drop the indices s,c, with the understanding
that the all the order parameters, and model parameters, appearing in the following expressions are those
corresponding to a specific pair of these indices.
Substituting the RS ansatz in Eq. B.19 we get:
GE=/integraldisplaydudˆu
2πeiuˆu/integraldisplay/productdisplay
a/parenleftigg
dλadˆλa
2πeiλaˆλa/parenrightigg
e−∆
2/summationtext
abˆλaˆλbq−∆ˆuR/summationtext
aˆλa−∆
2(ˆu)2qT(B.34)
×/productdisplay
ae−βℓ(u+c˜m+˜b,λa+cm+b)(B.35)
We can start by evaluating the Gaussian in ˆu, then performing a Hubbard-Stratonovich transformation, with
fieldz, to remove the squared sums on the replica index. Following up with the Gaussian integration in ˆλwe
find that the argument of the integrations factorises over the replica index. Up to first order in rwhenr→0,
we find for gE= logGE/d:
gE=/integraldisplay
Dz/integraldisplay
Dulog/integraldisplay
Dλe−βℓ/parenleftbigg√
∆qTu+c˜m+˜b,√
∆(Q−q)λ+√
∆R√qTu+/radicalig
∆(q−R2)
qTz+cm+b/parenrightbigg
(B.36)
and in the the β→∞limit we can solve the integral by saddle-point:
log/integraldisplay
Dλe−βℓ/parenleftbigg√
∆qTu+c˜m+˜b,√
∆(Q−q)λ+√
∆R√qTu+/radicalig
∆(q−R2)
qTz+cm+b/parenrightbigg
=−βM (B.37)
with:
M= min
λλ2
2+ℓ/parenleftigg/radicalbig
∆qTu+c˜m+˜b,/radicalbig
∆δqλ+√
∆R√qTu+/radicaligg
∆(q−R2)
qTz+cm+b/parenrightigg
(B.38)
To simplify further, we can shift√
∆R√qTu+/radicalig
∆(q−R2)
qTz→/radicalbig
∆qz′. Then, given the definition of the logistic
loss B.2, we can split the uintegration over the intervals/radicalbig
∆qTu+c˜mc>0and/radicalbig
∆qTu+c˜mc<0and
eventually get (re-establishing the s,cindices):
gE(s,c) =/summationdisplay
y/integraldisplay
DzH
−yqscmc
T+bc
T√
1+√∆cRscz
/radicalbig
∆c(qs−R2sc)
ME(y,s,c ) (B.39)
WhereH(x) =1
2erfc(x/√
2)is the Gaussian tail function and we defined the proximal:
ME(y,s,c ) = max
λ−λ2
2−ℓ/parenleftig
y,/radicalbig
∆cδqsλ+/radicalbig
∆cqsz+cms+bs/parenrightig
(B.40)
22Under review as submission to TMLR
Note that this simple 1D optimisation problem has to be solved numerically in correspondence of each point
evaluated in the integral.
The reweighing strategy is easily embedded in this calculation by explicitly changing the definition of ℓ,
adding a different weight Wc,yfor each combination of label and group membership. Defining a one-hot
encoding vector for the teacher-produced label, Y∈R2, and a output probability (constructed from the
sigmoid function) for the student, P(ˆY), the reweighed cross-entropy loss can be written as:
L(D) =/summationdisplay
c=±/summationdisplay
y=0,1(W)(c,y)YylogP(ˆYy). (B.41)
For the sake of simplicity we reduced the degrees of freedom to two, parameterising these weights as:
W= 2/parenleftbigg
w+w1w+(1−w1)
(1−w+)w1(1−w+)(1−w1)/parenrightbigg
(B.42)
wherew+,w1∈[0,1]can be used to increase the relative weight of a misclassification errors in the group +
and label 1respectively.
Different losses could be chosen instead of the cross-entropy and, again, only the numerical optimisation of
the proximal would be affected.
Saddle-point of the free-entropy
We thus have found that the free-entropy ΦWcan be written as a simple function of few scalar order
parameters. In the high-dimensional limit, the integral in B.16 is dominated by the typical configuration
of the order parameters, which is found by extremising the free-entropy with respect to all the overlap
parameters:
ΦW= extr
o.p./braceleftigg
gI+gS+/summationdisplay
s,cαs,cgE(s,c)/bracerightigg
(B.43)
The saddle-point is typically found by fixed-poimnt iteration: setting each derivative, with respect to the
order parameters, to zero returns a saddle-point condition for the conjugate parameters, and vice-versa.
The fixed-point is uniquely determined by the value of the generative parameters, m±
TandqT, and the pattern
densitiesαs,c. In the main text, for simplicity, we parameterise αs,cthrough the fraction η, which represents
the percentage of patterns from group +assigned to the first student model.
The special case of a single student model is obtained from this calculation by setting γ= 0and assigning all
the inputs in the first dataset D1.
Test accuracy. All the performance assessment metrics employed in this paper can be derived from the
confusion matrix, which measures the TP, FP, TN, FN rates on new samples from the T-M. These quantities
can be evaluated analytically and are easily expressed as a function of the saddle-point order parameters
obtained in the previous paragraphs.
Suppose we obtain a new data point with label yfrom group c, then probability of obtaining an output ˆy
from the trained model sis given by:
P/parenleftig
Y=y,ˆY= ˆy/parenrightig
=Ex(c)/angbracketleftbigg
Θ/parenleftbigg
y/parenleftbiggWc
T·x(c)√
d+˜b/parenrightbigg/parenrightbigg
Θ/parenleftbigg
ˆy/parenleftbiggWs·x(c)√
d+b/parenrightbigg/parenrightbigg/angbracketrightbigg
µ(WT,W)(B.44)
=Ex(c)/angbracketleftigg/integraldisplaydudˆu
2πeiˆu/parenleftig
u−/summationtextd
i=1WT,ixi√
d/parenrightig/integraldisplaydλdˆλ
2πeiˆλ/parenleftbig
λ−/summationtextd
i=1Wixi√
d/parenrightbig/angbracketrightigg
Θ/parenleftbig
y/parenleftbig
u+˜b/parenrightbig/parenrightbig
Θ (ˆy(λ+b))(B.45)
where, following the same lines as in the free-entropy computation, we used δ-functions to extract the
dependence on the input, to facilitate the expectation:
Ex(c)/angbracketleftbigg
e−iˆλWs·x(c)√
d−iˆuWT·x(c)√
d/angbracketrightbigg
(B.46)
23Under review as submission to TMLR
=e−ic(ˆλm+ˆu˜m)e−∆
2(ˆλ2Q+2ˆuˆλR+ˆu2). (B.47)
We have substituted the overlaps that come out of the average with their typical values in the Boltzmann-Gibbs
measure of the T-M. Note that we can substitute q=Qsince in the β→∞limit they are equal up to the
first order.
The Gaussian integrals can be computed and one gets the final expression:
P/parenleftig
Y=y,ˆY= ˆy/parenrightig
=/integraldisplay∞
−∞DuΘ/parenleftig
y/parenleftig/radicalbig
∆cu+cmc
T+bc
T/parenrightig/parenrightig
H/parenleftigg
−ˆy√∆cRscu+cms+bs/radicalbig
∆c(qs−R2sc)/parenrightigg
(B.48)
Similarly, one can also obtain e.g. the label 1frequency:
P(Y= 1) =ρH/parenleftigg
−m+
T+b+
T/radicalbig
∆+/parenrightigg
+ (1−ρ)H/parenleftigg
m−
T−b−
T/radicalbig
∆−/parenrightigg
(B.49)
and the generalisation error:
ϵg=/integraldisplay∞
−∞DuH/parenleftigg
sign/parenleftig/parenleftig/radicalbig
∆cu+cmc
T+bc
T/parenrightig/parenrightig√∆cRscu+cms+bs/radicalbig
∆c(qs−R2sc)/parenrightigg
. (B.50)
B.1 Parameters used in the figures
The following list contains the parameters of the T-M model used to plot the figures of the paper.
•Fig. 1d: ∆+= 0.5,∆−= 20.5,α= 2.5,qT−0.2.
•Fig.??a (upper):m±= 0.2,α= 0.5,∆+= 0.5,∆−= 0.5,b+= 0,b−= 0.
•Fig.??a (lower):α= 0.5,∆+= 0.5,∆−= 0.5,b+= 0,b−= 0.
•Fig.??b:α= 0.5,qT= 1,m= 0.5,b+= 0,b−= 0.
•Fig.??c:ρ= 0.1,m= 0.2,∆+= 0.5,∆−= 0.5,b+= 0,b−= 0.
•Fig. 6a:ρ= 0.1,qT= 0.8,∆+= 2.0,∆−= 0.5,α= 0.5,m+= 0.3,m−= 0.1,b+= 0.5,b−= 0.5.
C Real data validation
In the next two sections, we demonstrate the ability of the Teacher-Mixture model to mimic unfairness
scenarios in real-world applications. In particular, we perform this validation through a set of numerical
experiments on the CelebA dataset Liu et al. (2015). This dataset consists of a collection of face images
of celebrities, equipped with metadata indicating the presence of specific attributes in each picture. As
can be seen in Fig. C.3, the consistent amount of these attributes allows to explore many possible learning
scenarios in unfairness conditions. This feature of CelebA together with its size and the high-dimensional
nature of face pictures, makes it a good candidate for validating the Teacher-Mixture model on real datasets.
Moreover, as shown in Fig. C.2 through a PCA clustering, the different sub-populations associated to a
given CelebA attribute are overlapping and hard to disentangle. This situation precisely corresponds to
the high-noise regime the Teacher-Mixture model is meant to describe. Interestingly, the picture emerging
from the simulations on CelebA turned out to be quite general and further extendable to lower-dimensional
datasets such as the Medical Expenditure Panel Survey (MEPS) dataset Blewett et al. (2021). More details on
both datasets are discussed in Sec. C.2 and Sec. C.3. Here we provide a general overview on the experimental
framework applied to CelebA.
24Under review as submission to TMLR
10
 5
 0 5 10
7.5
5.0
2.5
0.02.55.07.5
0.00 0.25 0.50 0.75 1.00
0.200.220.240.260.28test errorsubpopulation +
subpopulation 
Figure C.1: Relative representation and bias . Numerical experiments on a sub-sample of the CelebA
dataset. (Left)A 2D projection of the pre-processed dataset, obtained from PCA, where the colours
represent the two sub-populations. (Right)Per community test error, as the fraction of samples from the two
subpopulations is varied (dataset dimension is fixed).
C.1 Model motivation
We construct a dataset by sub-sampling CelebA and by preprocessing the selected images through an Xception
network Chollet (2017) trained on ImageNet Deng et al. (2009). As depicted in the scatter plot in Fig. C.1,
the first two principal components of the obtained data clearly reveal a clustered structure. Many attributes
contained in the metadata are highly correlated with the split into these two sub-populations. For example,
in the figure we colour the points according to the attribute "Wearing_Lipstick". Now, suppose we are
interested in predicting a different target attribute, which is not as easily determined by just looking at
the group membership, e.g. "Wavy_Hair"2. What happens to the model accuracy if one alters the relative
representation of the two groups, e.g. when one varies the fraction of points that belong to the orange group?
The right panel of Fig. C.1 shows the outcome of this experiment. As we can see from the plot, the fact that
a group is under-represented induces a gap in the generalisation performance of the model when evaluated on
the different sub-populations. The presence of a gap is a clear indicator of unfairness, induced by an implicit
bias towards the over-represented group.
Many factors might play a role in determining and exacerbating this phenomenon. This is precisely why
designing a general recipe for a fair / unbiased classifier is a very challenging, if solvable, problem. Some bias
inducing factors are linked to the sampling quality of the dataset, as in the case of the overall number of
datapoints and the balance between the sub-populations frequencies. Other factors are controlled by the
different degree of variability in the input distributions of each group. In other cases the imbalance is hidden
and can only be recognised by looking at the joint distribution of inputs and labels. For example, the balance
between the positive/negative labels might differ among the groups and may be strongly correlated with the
group membership. Even similar individuals with different group memberships might be labelled differently.
The present work aims at modelling the data structure observed in these types of experiments, to obtain
detailed understanding of the various sources of bias in these problems.
C.2 Additional details on the CelebA experiments
The CelebA dataset is a collection of 202.599face images of various celebrities, accompanied by 40binary
attributes per image (for instance, whether a celebrity features black hairs or not) Liu et al. (2015). To obtain
the results presented in the main text we apply the following pre-processing pipeline: We first downsample
CelebA up to 20.000images. Notice that this is done with the purpose of considering settings with limited
amount of available data. Indeed, as we have seen in the main manuscript, data scarcity is one of the main
bias-inducing ingredients. We are thus not interested to consider the entire CelebA dataset, especially for
simple classification tasks like the one described in the main text. By exploiting the deep learning framework
provided by Tensorflow Abadi et al. (2015), we then pre-process the dataset using the features extracted
from an Xception convolutional network Chollet (2017) pre-trained on Imagenet Deng et al. (2009). Finally,
we collect the extracted features together with the associated binary attributes in a json file.
2To be mindful on the Ethical Considerations of using the CelebA datast, we don’t use protected attributes like binary
genders and age Denton et al. (2019)
25Under review as submission to TMLR
Figure C.2: Clustering CelebA according to attributes. We show 6 of the 40 attributes in CelebA
demonstrating a neat clustering.
By applying PCA on the pre-processed dataset, we observe a clustering structure in the data when projected
to the space of the PCA principal components. The clusters appear to reflect a natural correspondence with
the binary attributes associated to each input data point, however this is not a general implication and many
datasets show clustering with a non interpretable connection to the attributes. The clusters can be clearly
seen in Fig. C.2, where we use colours to show whether a celebrity features a given attribute (green dots)
or not (orange dots). In the plot, the axes correspond to the directions traced by the two PCA leading
eigenvectors. As we can see from Fig. C.2, the two sub-populations are overlapping and hard to disentangle.
This situation precisely corresponds to the high-noise regime the T-M model is meant to describe. Among the
various clustering depicted in Fig. C.2, we decided to disregard those corresponding to ethically questionable
attributes, such as "Attractive", "Male" or "Young". Finally, we chose as sensitive attribute – determining
the membership in the subpopulations – the "Wearing_Lipstick" feature since it gives a more homogeneous
distribution of the data points in the two clusters.
Anyone of the other attributes can be considered as a possible target, and thus be used to label the data
points. The final pre-processing step consist in downsampling further the data in order to have the same ratio
of0and1labels in the two subpopulations. This step helps mitigating bias induced by the different ratio of
label in the two subpopulations and simplifies the identification of the other sources of bias. The general case
can be addressed in the T-M model, in Sec. D we comment more on the bias induced by different label ratios.
As Fig. C.3 illustrates, there is a large number of possible outcomes concerning the behaviour of the test
error as a function of the relative representation. Indeed, as we have seen in the main text, the presence and
the position of the crossing point strictly depends on both the cluster variances and the amount of available
data. Despite all these behaviours are fully reproducible in the T-M model by means of its corresponding
parameters, we here decided to chose the “Wavy_Hair" as target feature because it shows a nicely symmetric
profile of the test error that is more suitable for illustration purposes. To get the learning curves in Fig. C.3,
we train a classifier with logistic regression and L2-regularisation. In particular, we use the LogisticRegression
class from scikit-learn Pedregosa et al. (2011). This class implements several logistic regression solvers, among
which the lbfgsoptimizer. This solver implements a second order gradient descent optimization which can
consistently speed-up the training process. The training algorithm stops either if the maximum component of
the gradient goes below a certain threshold, or if a maximum number of iterations is reached. In our case, we
set the threshold at 1e−15and the maximum number of iterations to 105. The parameter penaltyof the
26Under review as submission to TMLR
0.00 0.25 0.50 0.75 1.00
0.180.200.22test errorStraight_Hair
0.00 0.25 0.50 0.75 1.00
0.200.250.30test errorHigh_Cheekbones
0.00 0.25 0.50 0.75 1.00
0.160.180.200.22test errorArched_Eyebrows
0.00 0.25 0.50 0.75 1.00
0.0040.0050.0060.0070.008test errorEyeglasses
0.00 0.25 0.50 0.75 1.00
0.0900.0950.100test errorBushy_Eyebrows
0.00 0.25 0.50 0.75 1.00
0.240.260.280.30test errorMouth_Slightly_Open
0.00 0.25 0.50 0.75 1.00
0.140.160.180.20test errorBlack_Hair
0.00 0.25 0.50 0.75 1.00
0.280.300.32test errorPointy_Nose
0.00 0.25 0.50 0.75 1.00
0.0060.0070.0080.009test errorGray_Hair
0.00 0.25 0.50 0.75 1.00
0.080.090.10test errorWearing_Earrings
0.00 0.25 0.50 0.75 1.00
0.0060.0080.0100.012test errorWearing_Hat
0.00 0.25 0.50 0.75 1.00
0.0450.0500.0550.060test errorReceding_Hairline
Figure C.3: Relative representation across attributes. The panels show the generalisation error
depending on the relative representation in different attributes. The sub-populations +(green)−(orange)
are obtained splitting according to the attribute "Wearing_Lipstick". The simulations are averaged over 100
samples.
LogisticRegression class is a flag determining whether an L2-regularisation needs to be added to the training
or not. The C hyper-parameter corresponds instead to the inverse of the regularisation strength. In our
experiments, we chose the value of the regularisation strength by cross-validation in the interval (10−3,103)
with 30points sampled in logarithmic scale.
C.3 Other datasets
The observations made on the CelebA dataset are quite general and can be further extended to lower-
dimensional datasets. As example of this, we considered the Medical Expenditure Panel Survey (MEPS)
dataset. This is a dataset containing a large set of surveys which have been conducted across the United
States in order to quantify the cost and use of health care and health insurance coverage. The dataset
consists of about 150features, including sensitive attributes, such as age or medical sex, as well as attributes
describing the clinical status of each patient. The label is instead binary and measures the expenditure on
medical services of each individual, assessing whether the total amount of medical expenses is below or above
a certain threshold. As it can be seen in Fig. C.4, the behaviour is qualitatively similar to the one already
observed in the CelebA dataset of celebrity face images. Indeed, even in this case, PCA shows the presence
of two distinct clusters when considering the age as the sensitive attribute and then splitting the dataset in
two sub-populations, according to the middle point of the age distribution. Moreover, the generalisation error
per community exhibits a crossing according to the relative representation.
27Under review as submission to TMLR
0 2 4
5.5
5.0
4.5
4.0
3.5
3.0
2.5
2.0
0.00 0.25 0.50 0.75 1.00
0.140.150.160.170.18test error+
Figure C.4: MEPS dataset. (Left) Clustering in the MEPS dataset, according to be above or below the
average age. (Right) Crossing of the generalisation error as the relative representation ρis changed. The
simulations are averaged over 100samples.
0.5 1.0
qT0.10.20.30.40.5
m+=m=0.0
0.5 1.0
qT
m+=m=0.2
0.5 1.0
qT
m+=m=0.5
0.800.850.900.951.00
0.80.91.0
0.60.70.80.91.0
Figure D.1: Bias with two different rules to be learned. The three phase diagrams give the DI depending
onρ(y-axis) and qT(x-axis). Moving from the left panel to the right panel m+andmiare increased. The
other parameters are: α= 0.5,∆+= 0.5,∆−= 0.5,b+= 0,b−= 0.
D Exploration of the parameter space
D.1 Supporting results
This section presents supporting results on the sources of bias. In Fig. D.1, we re-propose the the study of
the disparate impact (DI) depending on the relative representation ρand the rule similarity qT, paying close
attention to the role of the group-label correlation m+,m−. Interestingly, if m+=m−= 0, when the rules
become identical ( qT= 1) the bias is removed. However if m+=m−̸= 0this is no longer true. This shows
once again that it is not sufficient for a classifier to be able of reproducing the rule, as bias can appear in
reason of other concurring factors.
The main difference with respect to the case with qT̸= 1is that, ifqT= 1, increasing the amount of training
data can be a solution. In fact, bias at qT= 1is due to overfitting with respect ot the largest sub-population,
and this effect can be cured by increasing in α. This is illustrated in Fig. D.2, that extends the figure of the
main text showing the effect of α. Moving from left to right, αincreases and the area where the 80%rule is
violated shrinks down.
The results shown until this point are agnostic with respect to the relative fraction of labels inside the
sub-populations. When this quantity is strongly varied across the groups, it can contribute to an additional
source of bias, especially if combined with a small relative representation. Indeed, the classifier can simply
bias its prediction towards the most likely outcome reaching an accuracy that apparently exceeds random
guessing, without effectively doing any informed prediction. Many factors play a role in deciding the relative
28Under review as submission to TMLR
0.5 1.0
m0.10.20.30.40.5
=0.1
0.5 1.0
m
=0.5
0.5 1.0
m
=1.0
0.20.40.60.81.0
0.20.40.60.81.0
0.60.81.0
Figure D.2: Bias with a learnable rule. We show the accuracy gain as function of the proportion of
group +(ρ) and the correlation between label and group ( m+,m−). The different figures show how of
increasing the dataset size (increasing from left to right) mitigates the bias. The other parameters are:
qT= 1.0,∆+= 0.5,∆−= 0.5,b+= 0,b−= 0.
2
 0 2
b
2
1
012b+=0.5
0.551.0
0.55
2
 0 2
b
2
1
012b+=0.3
0.401.0
0.69
2
 0 2
b
2
1
012b+=0.1
0.031.0
0.74
2
 0 2
b
2
1
012b+mean label in +
0.20.40.60.8
2
 0 2
b
2
1
012b+mean label in 
0.20.40.60.8
2
 0 2
b
2
1
012b+difference
0.5
0.00.5
Figure D.3: Labels within groups and classifier bias. Thefirst row shows the DI as faction of b+and
b−with ∆+= ∆−= 0.5,α= 0.5,m+=m−= 0.5. From left to right, the relative representation ρmoves
from equally represented groups to having group +under-represented. The 80% threshold is denoted by the
dotted line. The dashed line indicates equal within-group label fraction. The second row shows the average
labelling in +(left),−(centre), and their difference (right). Notice that these diagrams are independent of ρ
and therefor apply to the three settings shown in the first row.
fraction of labels in the T-M model, the bias terms ( b+andb−) are the most relevant since they directly shift
the decision boundaries. We consider these two parameters in Fig. D.3 to exemplify this concept.
When the sub-populations are equally represented ρ= 0.5, the separations between bias towards +or−is
clearly marked by two straight lines. One separation is simply given by the line of equal label fraction, the
other is given by the uncertainty of the classifier, receiving contrasting inputs from the two groups. As the
relative representation ρdecreases, the classifier accommodates the inputs from the largest group and the
separation line is distorted. Finally, observe that the line of equal label fraction (bottom right panel) is not
centred in the diagram because m+=m−̸= 0.
29Under review as submission to TMLR
E Mitigation strategies
Real data. In Fig. 6 of the main text, we show the effect of reweighing in the synthetic model. The
same analysis can be applied to real data, yielding similar results. In particular, in line with the other
validations, we present in Fig. E.1 the result for the CelebA dataset when the splitting is done according to
the "Wearing_Lipstick" and the target feature is "Wavy_Hair".
Figure E.1: Mitigation using re-weighting on real data. The four panels show the same quantities as in
Fig. 6A but applied to the CelebA dataset. Each panels shows in upper figure the mutual information on the
fairness metrics – statistical parity, equal opportunities, equal accuracy, equal odds, predicted parity 1, and
predicted parity 10– and in the lower figure the accuracy of for subpopulation +and subpopulation −. The
first 3 panels show the effect on the reweighing strategy for different values of the label weight (from left to
rightwl= 0.1,0.5,0.9). The last panel shows the performance of the coupled neural network strategy.
Similarly to what observa in the synthetic dataset, the coupled neural network strategy allows for a better
performance on all the fairness metrics while retraining a high accuracy for both subpopulations.
Additional results varying group membership. Some strategies require information concerning the
group membership of each data point. Depending on the situation, this information may contain errors
or it may even be unavailable. Consequently we should take into account the robustness of the mitigation
strategies with respect to these errors. Call ηthe fraction of points for which the group was correctly assessed.
The phase diagrams in Fig. E.1a show the DI under the reweighing mitigation scheme (controlling the group
importance in the loss) and the coupled classifier mitigation. We can clearly observe a greater resilience to
the error rates in the case of our strategy. The reweighing strategy appears to have low DI only in extreme
cases, where the accuracy on the largest sub-population is greatly deteriorated.
We can understand the larger picture by looking at the different fairness metrics described in the main text,
Fig. E.1b, for which the same observations apply. Since ηis not an actual hyper-parameter, but rather
represents an imperfect imputation of the group structure, we consider the maximum for each value of η.
The picture seems quite robust on the side of reweighing (upper group): for every ηthe maximum is achieved
for different values of the parameters. Instead, the picture changes for the coupled classifiers (lower group):
the method is robust to this perturbation until a critical value (roughly 25% of mismatched inputs), where
the minima of the MI become inconsistent and therefore the fairness metrics cannot be optimised all at once.
30Under review as submission to TMLR
0.00.20.40.60.81.0
statistical parity
 equal opportunity
 predited parity 1 accuracy group +
0.250.500.75
fractional weight w+0.00.20.40.60.81.0
equal accuracy
0.250.500.75
fractional weight w+equal odds
0.250.500.75
fractional weight w+predited parity 10
0.250.500.75
fractional weight w+accuracy group 
0.0060.0080.010
0.00020.0004
0.0060.0070.0080.009
0.0060.0080.010
0.0000500.0000750.0001000.0001250.000150
0.00120.00140.0016
0.700.720.74
0.700.720.74
(a)MI with errors in the group membership under community re-weighting strategy.
0.000.250.500.751.00
statistical parity
 equal opportunity
 predited parity 1 accuracy group +
105
101
103
0.000.250.500.751.00
equal accuracy
105
101
103
equal odds
105
101
103
predited parity 10
105
101
103
accuracy group 
0.0050.010
0.00010.00020.0003
0.00250.00500.0075
0.0050.0100.015
0.00020.0004
0.00050.00100.00150.0020
0.700.720.74
0.700.720.74
(b)MI with errors in the group membership under coupled neural networks strategy.
Figure E.1: Effect of noise in the attribute of the sub-communities. In the heatmaps we show in colors
how having imperfect information concerning the sub-community membership affect each fairness metrics
(six left figures) and the accuracy (right two plots). The vertical axis of the figures represents the probability
of mismatch η, while the horizontal axis refer to the parameter of the strategy ( w+andγresepctively). For
every value of the mismatch probability, we denote with red points the minima of the mutual information for
each fairness metrics.
31