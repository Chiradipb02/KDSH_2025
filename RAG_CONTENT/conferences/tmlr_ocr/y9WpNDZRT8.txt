Under review as submission to TMLR
Equitable Federated Learning with Activation Clustering
Anonymous authors
Paper under double-blind review
Abstract
Federated learning is a prominent distributed learning paradigm that incorporates collab-
oration among diverse clients, promotes data locality, and thus ensures privacy. These
clients have their own technological, cultural, and other biases in the process of data gen-
eration. However, the present standard often ignores this bias/heterogeneity, perpetuating
bias against certain groups rather than mitigating it. In response to this concern, we pro-
pose an equitable clustering-based framework where the clients are categorized/clustered
based on how similar they are to each other. We propose a unique way to construct the
similarity matrix that uses activation vectors. Furthermore, we propose a client weighing
mechanism to ensure that each cluster receives equal importance and establish O/parenleftig
1√
K/parenrightig
rate of convergence to reach an ϵ-stationary solution. We assess the effectiveness of our pro-
posed strategy against common baselines, demonstrating its efficacy in terms of reducing
the bias existing amongst various client clusters and consequently ameliorating algorithmic
bias against specific groups.
1 Introduction
With the advent of distributed learning paradigms, Federated Learning (FL) emerged as a promising mech-
anism for collaborative learning among diverse clients. In FL, the diversity of the clients gives rise to both
system and statistical heterogeneity McMahan et al. (2017); Li et al. (2020). The system heterogeneity
is attributable to the different computational capabilities of the participating clients, uplink and downlink
communication channel bandwidths, and faults. Statistical heterogeneity exists due to the variability in the
data distribution coming from the different clients. In our work, our primary aim is to tackle the statistical
heterogeneity in the data distribution arising due to the technological, cultural, and other biases in the data
generation process native to each client.
In a distributed learning setting, algorithms use the participating clients’ local updates and perform global
weighted aggregation. The goal is to create a global model that performs equally well on all clients. This
leads to a notion of group fairness Dwork et al. (2012) that incentivizes the participation of diverse groups of
clients Zhan et al. (2021), thus mitigating the bias in performance during the learning process. The end goal
of such algorithms is to ensure that the generated model is not biased towards any particular group of clients
and performs well for all of them. This is achieved through various techniques, such as FL, where the model
is trained on the data from all participating clients without compromising the privacy of individual clients.
Ultimately, the success of distributed learning algorithms depends on their ability to balance the needs and
preferences of all participating clients to generate a global model that is accurate, efficient, and fair. A
celebrated FL method, FedAvg McMahan et al. (2017), showcases success in the case of IID distribution of
data among the clients. However, its performance in the case of non-IID data distribution is the subject of
interest. Several studies such as Li et al. (2020); Karimireddy et al. (2020) have been conducted and provide
better convergence of FL on non-IID scenarios. Although these algorithms provide better results and tackle
the problem of data heterogeneity, they do not account for the inherent structure among the participating
clients.
Thispaperpresentsanovelapproachtoward dealing withthenon-IIDscenarioinFL.Itproposesaclustering
paradigm based on activation vectors (See Definition 1 ) that promote group fairness. In the context of FL,
theideaofclientsbelongingtoaclusterisnotastrictassumption. Instead, itisalreadypresentinthecurrent
1Under review as submission to TMLR
paradigm, for instance, when clients from different countries participate. In this broader context, the clients
representing distinct clusters, such as different countries, can benefit from the shared knowledge in FL. The
primary goal of the proposed algorithm is to assign equal weightage to each participating cluster, where
each cluster contains a varying number of clients, for global model aggregation. This approach promotes an
equitable and fair treatment of clients, utilizing the underlying similarities among the participating clusters.
Therefore, it can be considered as a server-side debiasing method, ensuring all clients benefit from the shared
knowledge.
1.1 Contribution
Our contributions are summarized as follows:
•Our proposed solution is a novel clustering framework that employs activation vectors as the primary
mechanism to group clients based on their similarities. This approach addresses the challenges that
arisewhenclientsfromdiversebackgrounds participateintheclusteringprocess. By usingactivation
vectors, our framework can effectively capture each client’s unique characteristics and preferences
while also identifying commonalities that allow for meaningful clustering.
•Our proposed approach involves leveraging this side information to enhance the model aggregation
process. This novel scheme can be seamlessly integrated with any existing client scheduler, allowing
for improved performance and more efficient utilization of resources.
•We present the convergence analysis for our algorithm, termed Equitable-FL , and show that it
enjoys a convergence rate of O/parenleftig
1√
K/parenrightig
to reach an ϵ-stationary solution under mild assumptions.
•We have conducted thorough experiments to compare our framework against baseline algorithms
using popular vision datasets like MNIST, CIFAR-10, CIFAR-100, and FEMNIST. Our findings
demonstrate that our algorithm not only delivers high accuracy, but it also minimizes client dis-
agreements, thus mitigating the algorithmic bias against certain groups.
1.2 Organization
The rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 introduces
the relevant concepts and preliminaries. Section 4 introduces our proposed approach and discusses its
convergence properties. Section 5 corroborates our method through extensive experiments while concluding
remarks are stated in Section 6.
2 Related Work
Centralized and Consensus-based Methods. Fairness has long been a concern in machine learning.
Traditional centralized machine learning approaches use pre-processing and post- processing techniques to
ensure fairness since the central server typically has access to the data Grgić-Hlača et al. (2018); Zhang
et al. (2018); Lohia et al. (2019); Kim et al. (2019); Mehrabi et al. (2021). However, these methods do
not suit paradigms like FL, which prioritize data locality and privacy. Achieving fairness in FL remains
a critical research area. This paper focuses on traditional FL settings where collaborative model training
happens across multiple clients while preserving the data privacy. Researchers have proposed extensions to
FedAvg McMahan et al. (2017), including model-level regularization Li et al. (2020); Durmus et al. (2021),
featurealignmentbetweenlocalandglobalmodelsLietal.(2021), alignmentoflocalandaglobalmeta-model
via gradient correction Acar et al. (2021), dividing clients into simple and complex type to train different
network architectures collaboratively Acar & Saligrama (2022), and momentum-based updates at the server
and client levels to reduce variance Karimireddy et al. (2021); Das et al. (2022). Despite these advancements,
FL methods often train a global model that performs well on average across all clients, neglecting individual
group performance. This limitation calls for new methods that accommodate groups’ demands.
Clustering and Fairness in Federated Learning. Preserving privacy by not sharing data or sensitive
client information with the server or other clients is a crucial aspect of FL, posing challenges for developing
2Under review as submission to TMLR
learning algorithms. Some work Cho et al. (2020); Goetz et al. (2019); Li et al. (2019a) prioritize client
selection to boost performance using local validation losses. We demonstrate client discrepancies using
the global model’s performance at the end of each epoch, though this information is not essential for our
algorithm’s efficiency. Other approaches Fraboni et al. (2021); Balakrishnan et al. (2022); Jiménez et al.
(2024) group the clients based on their representations using similarity matrices or submodular sets. These
methods assume a fixed number of clients per round, deviating from FL standards. Approaches like Lyu
et al. (2020); Wang et al. (2021) prioritize highly contributing clients, undermining the consensus-based
nature of FL. The method in Mohri et al. (2019) minimizes the maximum loss across all data samples to
avoid bias towards any data distribution, differing from our server-side debiasing approach using activation
vector information. Additionally, Cheng et al. (2024) proposes domain adaptation for client groups with
similar characteristics, whereas we focus on disjoint client groups participating in the collaboration. Other
works such as Ezzeldin et al. (2023) aim to mitigate the bias between the data samples with a client’s data
to promote group fairness, whereas we focus on fairness among groups of clients where the goal is to create
equitable learning scenarios among different groups. A recent study Yue et al. (2023) also focuses on ensuring
fair performance for both groups of clients and individual clients. However, prior knowledge of the client
groups is required, while our approach automatically clusters the clients based on the activation vectors.
Another concurrent work Chen & Vikalo (2024) orthogonal to ours uses the bias’s gradients in the neural
network’s last layer to construct clusters and then perform heterogeneity-aware client sampling. This work
poses an immense computational overhead, requiring computing the similarity matrix amongst all clients to
sample the participating clients. Other clustering- based works Vahidian et al. (2023); Sattler et al. (2020)
perform clustered federated learning to promote personalization, which is not the objective of our work.
3 Background and Preliminaries
In this section, we start by defining the standard terminologies introduced in McMahan et al. (2017); Li
et al. (2020) and proceed towards extending the idea to the setting used in our paper.
3.1 Federated Learning Setup
FL involves a central server collaborating with nclients, each maintaining its unique data distribution Di
and sample size Ni. The central server aims to train a machine learning model using data from its clients
without accessing their local data directly, thus preserving data locality. The expected loss function for
clienti,fi(w), depends on samples xdrawn fromDiand the client’s loss function ℓ, with model parameters
w∈Rd. The server minimizes the weighted loss f(w)across all clients:
f(w) :=n/summationdisplay
i=1pifi(w),where fi(w) =Ex∼Di[ℓ(x,w)], (1)
wherepi=Ni/summationtextn
i=1Nias suggested by McMahan et al. (2017). Each client performs Elocal SGD steps per
communication round k≤K. In partial client participation, r≤nclients are randomly selected without
replacement in each round. Each client calculates the unbiased stochastic gradient /tildewide∇fi(w;B)over a batch
B.The FL process iterates through three steps: downlink communication, local computation, and uplink
communication, formally described below.
Downlink Communication : At the start of each round, the server sends global model parameters to the
selected clients:
wi
k,0=wk,∀i= 1,...,n. (2)
Local Computation : Clients train locally using their datasets:
wi
k,τ+1=wi
k,τ−ηk/tildewide∇fi(wi
k,τ;Bi
k,τ),∀τ= 0,1,...,E−1. (3)
Increasing local epochs, especially with non-IID data, causes client drift Li et al. (2020); Karimireddy et al.
(2020); Das et al. (2022). To address this, Li et al. (2020) introduced FedProx, adding a proximal term to
the update:
wi
k,τ+1=wi
k,τ−ηk/parenleftbig/tildewide∇fi(wi
k,τ;Bi
k,τ) +µ(wi
k,τ−wk)/parenrightbig
,∀τ= 0,1,...,E−1. (4)
3Under review as submission to TMLR
Algorithm 1 Equitable-FL
1:Input:Initial model weights w0, # of communication rounds K, periodE, learning rates{ηk}K−1
k=0, and
global batch size r, # of clusters C.
2:Output: wK
3:fork= 0,...,K−1do
4:Server send wkto a set ofrclients chosen uniformly at random without replacement denoted by Sk,
5:forclienti∈Skdo
6:Downlink communication: Setwi
k,0=wk.
7:forτ= 0,...,E−1do
8: Pick a random batch of samples in client i,Bi
k,τ. Compute the stochastic gradient of fiat
wi
k,τoverBi
k,τ, viz./tildewide∇fi(wi
t,τ;Bi
k,τ).
9: Update wi
k,τ+1=wi
k,τ−ηk/parenleftbig/tildewide∇fi(wi
k,τ;Bi
k,τ) +µ(wi
k,τ−wk)/parenrightbig
.
10: Uplink communication: Sendwi
k,Eandai
k,E.
11:S=Similarity/parenleftig
A={a1
k,E,a2
k,E,...,ar
k,E}⊤/parenrightig
.
12:pk=getprobs (S,C).
13:Update wk+1=/summationtext
i∈Skpi
kwi
k,E.
14:Function :Similarity (A)
15:Return:A×A⊤.
16:Function :getprobs (S,C)
17:Pick the eigen vectors of Scorresponding to the Clargest eigen values.
18:Use K-Means to cluster the clients.
19:forclienti∈Skdo
20:pi
k=1
C×# of clients in each cluster.
21:Return:pk.
Uplink Communication : Clients send their updated parameters to the server:
wk+1=/summationdisplay
i∈Skpiwi
k,E. (5)
This iterative process continues for Kcommunication rounds.
4 Equitable Clustering
In this section, we present our novel approach named Equitable-FL , which aims to tackle the issue of
algorithmic bias in FL. Our solution involves implementing a server-side debiasing mechanism that leverages
the activation vectors (see Definition 1) to identify and cluster clients based on their similarities. By doing
so, we are able to update how the central server aggregates the local model updates received from clients.
This approach ensures that the participation across each group of clients is more equitable, minimizing the
potential for bias to occur during the learning process.
4.1 Problem statement and Motivation
In Federated Learning (FL), the k-th communication round’s model aggregation involves computing the
weighted average of the model updates from each client, given by wk+1=/summationtext
i∈Skpiwi
k,E, whereSkis the set
of clients in round k. Algorithms such as those proposed by McMahan et al. (2017); Li et al. (2020) weigh
clients differently based on factors like the number of data samples, Ni, a client possesses. In this scenario, a
client’s weight is proportional to their data sample size, pi=Ni/summationtext
i∈SkNi. Alternatively, clients can be weighed
uniformly, irrespective of their data sample size, with each client’s weight being1
|Sk|. Accurately weighing
each client’s contribution is crucial for fair and unbiased distributed learning. Simply weighing clients based
4Under review as submission to TMLR
on data sample size can lead to biases, favoring clients with more data and giving them disproportionate
influence on the global model. This undermines the collaborative nature of FL, which aims to incentivize
equal participation. On the other hand, uniformly weighing clients disregards individual contributions,
allowing clients with stronger local models to dominate while those with weaker models are suppressed.
Both approaches can lead to unfair outcomes, compromising the effectiveness and equity of the learning
algorithm.
4.2 Framework: Equitable-FL
Our proposed framework, Equitable-FL, addresses these limitations. First, we define what an activation
vector is in our context.
Definition 1 (Activation vector ).An activation vector within a neural network refers to the output
generated by any given layer after it undergoes transformations like linear combinations (involving inputs,
weights, and biases) and activation through a non-linear function. This output captures critical features of
the input data, which are vital for the following layers in tasks like classification or prediction. For our
purpose, we use the activation vectors, specifically ai
k,E, that are generated as outputs of the pre-final layer
of the model architecture for E−thlocal epoch and k−thcommunication round for client i(See Figure 1).
At the beginning of the training process, we send the global model wkto all the clients. Using
this global model, each client runs local iterations and transmits the updated model wi
k+1and the
activation vector ai
k,Eto the central server. The activation vectors are essentially dimensionality-
Figure 1: Activation vectorsreduced representations of the client’s data distri-
bution. Figure 1 describes how we retrieve these
ai
k,E. The process starts with the Input, which
moves through several Deep layers , including vari-
ous neural network layers like convolutional, pool-
ing, and fully connected layers. The data then
reaches the Activation layer , where an activation
function,ϕ(·)(such as ReLU), applies non-linearity.
The activated data continues to the Classification
layer, which generates the network’s final Output.
Simultaneously, the output from the activation layer
is processed through a log softmax function, Ψ(·), to
calculate the Activation vector . We use these ac-
tivation vectors to construct a similarity matrix Aas presented in line 13 of Algorithm 1. Using the K-Means
algorithm, this similarity matrix Sis then used to perform spectral clustering Von Luxburg (2007). Spectral
clustering helps us determine the number of clients belonging to each cluster, which we use to create an
equitable client weighing scheme, namely
pi
k=1
C×# of clients in each cluster. (6)
In other words, we use clustering to obtain the weighing probabilities to aggregate the local model for the
k+ 1-th communication round. By doing so, we ensure that all clients are given equal importance and
contribute equally to the overall model. So, we formally re-define eq. (1) for our setting as,
f(w):=C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqfi(w). (7)
Remark 1. Sending the additional ai
k,Eto the server does not pose a severe communication cost in com-
parison to the model parameters because they are mostly of size O(102)in practice, which is considerably
smaller than typical model sizes.
Remark 2. The client weighting scheme pi
k,Edescribed in eq. (6)is straightforward yet effective, as demon-
strated in the experiments section, and further is amenable to theoretical analysis. However, one could develop
5Under review as submission to TMLR
0 20 40 60 80 100
Communication rounds0.60.70.80.91.0NMIMNIST
C=2
0 20 40 60 80 100
Communication rounds0.30.40.50.60.70.80.91.0NMICIFAR10
C=2
0 20 40 60 80 100
Communication rounds0.700.750.800.850.900.951.00NMICIFAR100
C=3
0 20 40 60 80 100
Communication rounds0.950.960.970.980.991.001.01NMIFEMNIST
C=5
Figure 2: NMIcomparison across various vision datasets using ResNet-18 model architectures. We parti-
tioned the data among clients to form clusters. The Cvalues in each figure indicate the actual number of
clusters we divided the clients into for each dataset. Our observations reveal that the NMIs are close to 1,
suggesting that the algorithm’s performance aligns closely with the true cluster labels of the clients.
other weighting strategies based on specific objectives, such as personalization. This paper demonstrates that
clients can be clustered effectively by utilizing activation vectors, leading to a fair consensus-based approach.
In Figure 2, we present the normalized mutual information (NMI) for various datasets with different cluster
sizes and show the effectiveness of our proposed algorithm.
Remark 3. We emphasize that while K-Means relies on specifying the number of clusters as a hyperparam-
eter, it is not unique in this requirement. Many other clustering algorithms, e.g. hierarchical clustering, also
depend on a thresholding mechanism to determine cluster formation, which similarly requires fine-tuning for
optimal results. Thus, the need for parameter adjustment is a common aspect across clustering techniques.
4.3 Main Assumptions
In this section, we discuss the standard assumptions that we make in order to provide theoretical guarantees
on the convergence of the proposed method.
Assumption 1 (Smoothness ).ℓ(x,w)isL-smooth with respect to w, for all x. Thus, each fi(w)(i∈[n])
isL-smooth, and so is f(w).
∥∇fi(w1)−∇fi(w2)∥≤L∥w1−w2∥;for anyi,w1,w2.
The assumption stated in Assumption 1 is frequently used while analyzing the convergence of algorithms
that employ gradient-based optimization. This assumption has been referenced in several publications such
as Chellapandi et al. (2023); Shi et al. (2023); Das et al. (2022). It aims to limit abrupt changes in the
gradients.
Assumption 2 (Non-negativity ).Eachfi(w)is non-negative and therefore, f∗
i≜minfi(w)≥0.
TheassumptionstatedinAssumption2isusuallyfulfilledbythelossfunctionsthatareemployedinpractical
applications. Nevertheless, if a loss function happens to have a negative value, this assumption can still be
met by introducing a constant offset.
Assumption 3 (Bounded Variance ).The variance of the stochastic gradient for all clients i= 1,...,n
is bounded, where B(i)
k,τrepresents the random batch of samples in client iforτthlocal iteration.
E[||/tildewide∇fi(w(i)
k,τ;B(i)
k,τ)−∇fi(w(i)
k,τ)||2]≤σ2.
Assumption 3 is often utilized to assess the convergence of gradient descent-based algorithms, as demon-
strated in various works, including Shi et al. (2023); Li et al. (2019b); Nguyen et al. (2018). However, some
other studies assume uniformly bounded stochastic gradients, where E[||/tildewide∇fi(w(i)
k,τ;B(i)
k,τ)||2]≤σ2. This as-
sumption is stronger than Assumption 3 and is also shown to be untrue for convex loss functions in Nguyen
et al. (2018).
Assumption 4 (Existence of Clusters ).Assuming a system comprising Cclusters to which all nclients
are allocated, this assumption aligns with the inherent system partitions, for instance, clients segmented
6Under review as submission to TMLR
by diverse demographic regions. Each cluster, denoted by γq, encapsulates a subset of participants, with q
signifying the specific cluster. In the course of the kthcommunication round, a selection of rclients is made
to partake, and they are subsequently distributed into Cclusters by our algorithm, ensuring a minimum
representation of one client per cluster q.
Remark4. We ensure a minimum of one participant per cluster for theoretical analysis purposes, though this
constraint is relaxed during experimental execution. Notably, Assumption 4 is not a stringent requirement,
as the scenario where a cluster lacks participant representation is deemed excessively pessimistic.
4.4 Convergence Analysis
We present the convergence analysis of the proposed Algorithm 1. The detailed proof of Theorem 1 is present
in Appendix A.
Theorem 1 (Smooth non-convex case of Equitable-FL ).Suppose Assumptions 1, 2, 3, and 4 hold
true for Equitable-FL (refer Algorithm 1). In Equitable-FL set η=1
4E√
3LK. Define a distribution Pfor
k∈{0,...,K−1}such that P(k) =(1+ζ−1)(K−1−k)/summationtextK−1
k=0(1+ζ)kwhereζ:=η2E2/parenleftig
9ηL2E+ 4ηµE + 6L/parenleftig
1 +4η2µ2E2
18/parenrightig/parenrightig
.
Samplek∗fromP. Then for ηLE≤1
2,ηµE≤1
2,µ<1, andK≥max/parenleftig
3L
32,µ2
12L,µ2
108L3/parenrightig
; we have:
E[∥∇f(wk∗)∥2]≤16√
3Lf(w0)
(1−µ)√
K+√
Lσ2
2√
3KE(1−µ)+σ2
36E2K(1−µ)+(2 +E)Lσ2
36K(1−µ)+µσ2
6ELK (1−µ)
+σ2
18CELK (1−µ)/parenleftig
L2+µ
2/parenrightigC/summationdisplay
q=11
|γq|.(8)
and the expectation is with respect to the randomness in all stochastic gradients and the random selection of
kaccording to the distribution P.
As stated above, the analysis is conducted without restrictive assumptions such as convexity and bounded
clientdissimilarityKarimireddyetal.(2020);Upadhyay&Hashemi(2023). Broadlyspeaking, wecanobserve
that it consists of two terms in eq. (8). In particular, the first term captures the impact of initialization,
and the second set of terms results from the noisy stochastic updates of the clients. So, by setting the ηas
described in the theorem, we see that Equitable-FL enjoys a convergence of O/parenleftig
1√
K/parenrightig
to reach an ϵ-stationary
solution for our setting.
5 Experiments
In this section, we present the findings of our framework and compare it with several baselines. We evaluate
our algorithm and baselines on an extensive suite of datasets in FL with varying client partition and cluster
sizes to show its efficacy.
5.1 Datasets and Model Architecture
We conduct deep learning experiments on datasets such as MNIST (Deng, 2012), CIFAR-10, CIFAR-
100 (Hinton, 2007), and FEMNIST (Cohen et al., 2017; Caldas et al., 2018). These datasets are standard
datasets used in FL experimentation. To showcase the effectiveness of our algorithm, we partition the data
in a non-IID fashion. In this partition, we try to emulate the clustering scenario by creating groups among
clients and giving them only specific labels. We train a simple CNN model architecture and ResNet-18 (He
et al., 2016) for all these datasets. In Table 1, we show how we have planted the clusters. For example, in
the case of the MNIST dataset, there are n= 10clients divided into C= 2clusters where r= 4clients are
participating in each round. We now describe the data partition and the model architecture for the datasets
mentioned above.
MNIST. In this case, we train a simple neural network on the MNIST dataset. The total number of clients
isn=10, and the data is distributed among these 10 clients. As we described earlier, the division of data
7Under review as submission to TMLR
is in a non-IID fashion. Since we do not know the true distribution of data, we created the non-IID and
an inherent clustering scenario by distributing the data based on classes. In particular, we ensure that
there are two sets of clients where the data possession is entirely orthogonal. So, the first 4 out of the 10
clients get the images from the first 4 out of 10 classes, and the rest classes go to the remaining 6 clients.
Table 1: Data partition
Dataset C n r
MNIST 2 10 4
CIFAR-10 2 10 4
CIFAR-100 3 10 4
FEMNIST 5 90 18A client has 800 images of each class, which leads to a client
from the first category having 3200 images and a client from
the second category having 4800 images, respectively. This
approach leads to the formation of two clusters with hetero-
geneity in terms of the number of samples in each cluster and
the nature of samples present in each cluster. From a practical
perspective, we tried to emulate the partial client participation
scenario. We conducted experiments with 40%of the total
participants. The model architecture consists of three fully connected layers, with the first layer accepting
flattened input images of size 28 ×28 (784 features). The subsequent hidden layers have 200 units each, and
ReLU activation functions are applied after the first two layers. The final layer outputs predictions for the
classes in the MNIST dataset.
CIFAR-10. For this dataset, we train a CNN model. The total number of participants is n=10. The data
partition strategy to introduce non-IIDness follows the same strategy as in the case of the MNIST dataset.
Similar to the MNIST dataset, we create 2 clusters where the first 4 clients have the data from the first four
classes, and the following six clients have the data from the following six classes. So, each client in the first
cluster has 3200 images, and the clients from the following cluster have 4800 images each. We experiment
with partial client participation where only 40 %of the total clients participate. The model architecture is
a CNN network with two convolutional layers and three fully connected layers. The first layer is a 5×5
convolutional layer with 3 and 6 input and output channels, respectively. This is followed by a 5x5 kernel
convolutional layer with 16 output channels. A ReLU activation and a max pooling layer succeed each
convolutional layer. The resulting output is flattened, traversing two fully connected layers featuring ReLU
activations. Finally, the output is directed to the last fully connected layer.
CIFAR-100. In CIFAR-100, we follow a similar partition strategy to MNIST and CIFAR-10. The total
number of participants is n= 10, forming 3 clusters. The first 2 clients have data from the first 20 classes,
each having 4400 samples. The second set of 3 clients has the data from the next 30 classes, where each client
has4800samples, andthethirdsetof5clientshasthedatafromthefollowing50classesofCIFAR-100, where
each client has 5000 samples. In each round, only 40%of the clients participate, thus presenting the partial
client participation scenario. The model architecture begins with five convolutional layers, each followed by
ReLU activation to introduce non-linearity and three max-pooling layers for downscaling the feature maps.
The network concludes with two fully connected layers, with the final layer reducing feature dimensions to
512 and the last layer mapping these to 100 classes, aligning with the CIFAR-100 dataset specifications. The
network’s forward pass involves processing through these layers, outputting the raw features from the last
fully connected layer and the log softmax of these features, catering to feature extraction and classification
tasks.
FEMNIST. In FEMNIST, we have 5 clusters, and the total number of participants is n= 90. The number
ofclientsineachoftheseclustersis8, 36, 16, 18, and12, respectively, andeachoftheseclustershasdatafrom
the 2, 8, 15, 20, and 17 classes, respectively. This means the first 8 clients have data from the first 2 classes.
The following 36 clients get the data from the following 8 classes, and so on. These numbers are chosen
randomly, and there is no correlation between them. In each round, only 20%of the clients participate, thus
presenting the partial client participation scenario. This model comprises two convolutional layers, each
followed by a max-pooling layer. The convolutional layers use 32 and 64 filters, respectively, with a kernel
size of 5. The network also includes a dropout layer with a dropout probability of 0.2 for regularization. The
fully connected part of the network consists of two linear layers, with the first linear layer transforming the
input from 1024 features to 512 features, followed by ReLu and the final output layer producing 62 classes,
corresponding to the EMNIST dataset.
8Under review as submission to TMLR
Centralized Fedprox Cluster2 pow-d Fairfed GIFAIR Equitable-FL
0 50 100 150 200 250
Communication rounds0123456Client disagreement
MNIST
0 50 100 150 200 250
Communication rounds012345678Client disagreement
CIFAR10
0 50 100 150 200 250
Communication rounds0.00.51.01.52.02.53.03.5Client disagreement
CIFAR100
0 50 100 150 200 250
Communication rounds0123456Client disagreement
FEMNIST
0 20 40 60 80 100
Communication rounds0.00.51.01.52.02.53.03.5Client disagreement
MNIST
0 20 40 60 80 100
Communication rounds012345Client disagreement
CIFAR10
0 20 40 60 80 100
Communication rounds0123456Client disagreement
CIFAR100
0 20 40 60 80 100
Communication rounds0.00.51.01.52.02.53.03.54.0Client disagreement
FEMNIST
Figure 3: Client disagreement comparison on different vision datasets using two different model archi-
tectures. The plots in the first row are generated using a simple CNN model architecture, and the plots in the
second row are generated using the ResNet-18 model architecture. Equitable-FL consistently outperforms
other baselines, invariant to the model architecture, datasets, and number of clusters.
5.2 Experiment Setup
Baselines. We evaluate our proposed approach against several seminal works in the FL area for tackling
client heterogeneity. The baselines include algorithms that tackle client heterogeneity, such as Fedprox Li
et al. (2020), and client selection to improve the representation of clients, such as Cluster2 Fraboni et al.
(2021). Other algorithms that we compare against propose different model aggregation strategies for groups
of clients to promote group fairness, i.e., FairFed Ezzeldin et al. (2023) and GIFAIR-FL-Global Yue et al.
(2023). We also compare our method against a biased client selection, pow-d Cho et al. (2020) algorithm
that is solely driven by improving accuracy on average. The details are as follows,
•Centralized : The training happens in a centralized fashion, where all the clients share their data
with the server.
•Fedprox (Li et al., 2020): It adds a proximal term to the local client update to mitigate the effect
of client drift.
•Fedprox + Cluster2 (Fraboni et al., 2021): The Cluster2 algorithm uses representative gradients,
i.e., the difference between the client’s updated model and the global model, to construct the simi-
larity matrix and then sample the clients utilizing it. We use the sampling strategy along with the
Fedprox algorithm.
•Fedprox + pow-d (Cho et al., 2020): In pow-d the server starts by selecting dclients and then
selectsasubsetof rclientswithhighestlocallosses. WeuseFedProxalongwiththepow-dalgorithm.
•GIFAIR-FL-Global (Yue et al., 2023): GIFAIR-FL-Global aims to achieve group fairness by
adding the differences in the loss between clusters of clients as a regularization term to the objective
function.
•Fedprox + FairFed (model reweighing) (Ezzeldin et al., 2023): We utilize the model reweighing
strategy proposed in FairFed.
In GIFAIR Yue et al. (2023), it requires an actual number of clients participating from each cluster; thus,
it violates the principle of not knowing the client’s identity. To conduct the experiments, we provided
those details to the algorithm, and in that scenario as well, the algorithm’s performance in reducing the
9Under review as submission to TMLR
Table 2: Performance of Algorithms. We tabulate the accuracy of different algorithms against
Equitable-FL . The report shows the average test accuracy and σAccof 3 independent runs over 250 global
communication rounds. The results are produced using a simple CNN model architecture. Bold numbers
indicate the best results. Additionally, results in the centralized setting row are just for reference; hence, we
have not indicated them as best results.
MethodMNIST CIFAR-10 CIFAR-100 FEMNIST
Acc σAcc(↓) Acc σAcc(↓) Acc σAcc(↓) Acc σAcc(↓)
Centralized 99.22±0.03 0.17±0.03 58.74±0.75 7.18±0.28 47.01±0.18 2.27±0.16 91.18±0.015 11.70±0.08
Fedprox 96.97±0.20 2.26±0.21 47.36±0.50 18.01±0.68 33.04±0.14 13.90±0.43 74.18±0.12 22.99±0.08
Cluster2 95.38±0.18 4.51±0.23 47.17±0.80 25.52±0.38 32.66±0.13 15.37±0.21 76.12±0.01 21.37±0.13
Pow-d 96.03±0.33 3.61±0.40 47.08±0.50 22.04±0.68 33.07±0.41 14.50±0.22 74.34±0.14 22.80±0.13
FairFed 82.10±1.75 20.2±2.12 44.74±0.84 33.52±3.01 26.79±1.63 17.57±1.80 54.41±0.61 41.56±1.05
GIFAIR 96.98±0.12 2.34±0.13 47.41±0.83 18.88±0.98 33.26±0.43 14.28±0.48 74.13±0.12 22.96±0.25
Equitable-FL 97.75±0.20 1.17±0.25 49.74±0.56 6.07±0.31 33.34±0.10 10.57±0.13 75.54±0.25 16.21±0.04
Table 3: Performance of Algorithms. We tabulate the accuracy of different algorithms against
Equitable-FL . The report shows the average test accuracy and σAccof 3 independent runs over 100 global
communication rounds. The results are produced using the ResNet-18 model architecture. Bold numbers
indicate the best results. Additionally, results in the centralized setting row are just for reference; hence, we
have not indicated them as the best results.
MethodMNIST CIFAR-10 CIFAR-100 FEMNIST
Acc σAcc(↓) Acc σAcc(↓) Acc σAcc(↓) Acc σAcc(↓)
Centralized 99.43±0.02 0.10±0.02 83.60±0.10 5.41±0.21 65.90±0.21 0.10±0.16 90.51±0.07 11.67±0.01
Fedprox 96.01±0.52 2.84±0.48 66.72±0.80 15.70±1.36 55.19±0.11 16.85±0.23 67.61±1.07 30.29±0.35
Cluster2 94.53±0.10 5.20±0.03 63.46±0.59 26.61±0.70 53.99±0.83 18.71±1.25 68.12±0.76 30.27±0.54
Pow-d 95.05±0.46 4.55±0.73 65.06±1.25 21.48±1.85 54.91±0.39 17.53±0.65 67.67±0.20 30.02±0.13
FairFed 85.85±2.92 15.41±3.81 57.10±2.11 31.98±7.50 46.05±0.48 24.98±0.18 53.27±1.344 38.92±2.22
GIFAIR 96.02±0.33 2.86±0.31 64.41±0.56 20.25±1.21 55.39±0.14 17.08±0.13 69.55±0.62 25.53±0.61
Equitable-FL 96.95±0.40 1.65±0.53 69.40±0.48 7.83±0.71 55.65±0.15 13.67±0.39 67.02±0.42 24.11±0.33
disagreement between clients is inferior to ours. Moreover, in Fairfed Ezzeldin et al. (2023), we only utilize
their model/client weighing scheme, and it turns out that their strategy can generate negative weights (refer
to the section on Computing Aggregation Weights for FairFed in Ezzeldin et al. (2023)). So, to mitigate the
issue, we lower-bound these aggregation weights to 0.
Implementation details. We implement all algorithms in PyTorch using an Nvidia A100 GPU. We make
Assumption 4 solely for convergence analysis, not for experimental evaluations. The figures and tables
present results averaged over three independent runs. Following the standard FL learning regime, we start
with a global communication round of K= 250for a simple CNN architecture and K= 100for ResNet-
18, with local epochs set to E= 5unless stated otherwise. We perform hyperparameter tuning with
η∈{0.001,0.01,0.1}andµ∈{0.001,0.01,0.1,1}, and present results for the best outcomes. We use the
cumulative moving average of the results in our graphical representations to enhance clarity.
Evaluation metric. To demonstrate the effectiveness of our algorithm, we measure client disagreement
using test loss. Client disagreement is based on the principle that the global model should perform equally
well on each client’s dataset to mitigate algorithmic bias. Specifically, we define client disagreement as the
average absolute difference in loss values between two participating clients during a communication round
using the updated global model, expressed as:
CDk+1=/summationtext
i∈Sk/summationtext
j∈Sk|fi(wk+1)−fj(wk+1)|/parenleftbigr
2/parenrightbig . (9)
As discussed, CDassesses algorithm performance and efficiency. It is not essential for the algorithm’s
function but is a useful evaluation tool. Additionally, we evaluate the standard deviation between the global
test accuracy and each client’s accuracy, denoted as σAcc, as an additional performance metric to measure
client performance discrepancy (refer to Yue et al. (2023)).
10Under review as submission to TMLR
Centralized Fedprox Cluster2 pow-d Fairfed GIFAIR Equitable-FL
0 50 100 150 200 250
Communication rounds01020304050σAcc
MNIST
0 50 100 150 200 250
Communication rounds5101520253035σAcc
CIFAR10
0 50 100 150 200 250
Communication rounds0.02.55.07.510.012.515.017.520.0σAcc
CIFAR100
0 50 100 150 200 250
Communication rounds1015202530354045σAcc
FEMNIST
0 20 40 60 80 100
Communication rounds010203040σAcc
MNIST
0 20 40 60 80 100
Communication rounds510152025303540σAcc
CIFAR10
0 20 40 60 80 100
Communication rounds0510152025σAcc
CIFAR100
0 20 40 60 80 100
Communication rounds101520253035404550σAcc
FEMNIST
Figure 4:σAcccomparison on different vision datasets using two different model architectures. The plots
in the first row are generated using simple CNN model architecture, and the plots in the second row are
generated using the ResNet-18 model architecture. Equitable-FL consistently outperforms other baselines,
invariant to the model architecture, datasets, and number of clusters.
5.3 Main Results
In this study, we evaluate the performance of our algorithm against established baselines. To ensure compre-
hensive results, we conducted experiments on various datasets, including the widely used MNIST, CIFAR-10,
CIFAR-100, and FEMNIST datasets. The datasets are distributed among clients such that the clients form
clusters. Our findings demonstrate that Equitable-FL consistently outperforms other baselines in reducing
client disagreement in highly heterogeneous settings, as illustrated in Figure 3. Additionally, we show that
the effectiveness of our framework in mitigating bias is independent of the model architecture used. The
average accuracy and σAccare presented in Tables 2 and 3 as well as in Figures 4 and 5. Our results indicate
that the Equitable-FL significantly outperforms other baselines, except the FEMNIST dataset (regarding
average accuracy). Overall, our results suggest that Equitable-FL is a promising solution for addressing
challenges associated with distributed machine learning.
Table 4: We tabulate the accuracy and σAccof our algorithm using the CIFAR-100 dataset for ResNet-18
with varying CandE. The table shows the average test accuracy of 3 independent runs over 50 global
communication rounds. Bold numbers indicate the best results.
MethodC= 2 C= 3 C= 4
Acc σAcc(↓) Acc σAcc(↓) Acc σAcc(↓)
Equitable-FL 49.71±1.16 18.02±1.16 49.89±0.66 14.92±1.72 49.13±0.63 19.04±0.80
MethodE= 1 E= 5 E= 10
Acc σAcc(↓) Acc σAcc(↓) Acc σAcc(↓)
Equitable-FL 31.93±0.78 13.47±1.50 49.90±0.65 14.92±1.72 52.90±0.67 14.45±1.60
5.4 Abalation study
We evaluate our proposed framework on the CIFAR-100 dataset using the ResNet-18 architecture. During
the experiment, we vary the number of clusters, C, the number of local epochs, E, and the proximal term,
µ. In Table 4, we show that with correct cluster assignment, i.e., C= 3, our framework significantly reduces
the disagreement among clients as well as improves the test accuracy. Additionally, we show in Table 4
that with increasing local iterations, our algorithm consistently manages to reduce the disagreement among
11Under review as submission to TMLR
Centralized Fedprox Cluster2 pow-d Fairfed GIFAIR Equitable-FL
0 50 100 150 200 250
Communication rounds60708090100T est accuracy
MNIST
0 50 100 150 200 250
Communication rounds2030405060T est accuracy
CIFAR10
0 50 100 150 200 250
Communication rounds010203040T est accuracy
CIFAR100
0 50 100 150 200 250
Communication rounds102030405060708090T est accuracy
FEMNIST
0 20 40 60 80 100
Communication rounds20406080100T est accuracy
MNIST
0 20 40 60 80 100
Communication rounds1020304050607080T est accuracy
CIFAR10
0 20 40 60 80 100
Communication rounds0102030405060T est accuracy
CIFAR100
0 20 40 60 80 100
Communication rounds20406080T est accuracy
FEMNIST
Figure 5: Test accuracy comparison on different vision datasets using two different model architectures.
The plots in the first row are generated using the model architecture described in section 5.1, and the
plots in the second row are generated using the ResNet-18 model architecture. Equitable-FL consistently
outperforms other baselines, invariant to the model architecture, datasets, and number of clusters.
0 10 20 30 40 50
Communication rounds0.40.60.81.01.21.41.61.8Client disagreementCIFAR100
C=2
C=3
C=4
(a)
0 10 20 30 40 50
Communication rounds0.20.40.60.81.01.21.4Client disagreementCIFAR100
E=1
E=5
E=10 (b)
0 10 20 30 40 50
Communication rounds0.00.20.40.60.81.01.2Client disagreementCIFAR100
μ=1
μ=0.1
μ=0.01
μ=0.001 (c)
Figure 6: Impact of hyperparameters on client disagreement: perturbations in C,Eandµ.
clients, mitigating the effect of client drift. In Table 5, we show that if we increase the µ, the disagreement
or the effect of client drift will reduce but at the price of accuracy. In Figure 6, we show the CDcomparison
for these 3 cases.
Table5: Wetabulatetheaccuracyand σAccofouralgorithmusingtheCIFAR-100datasetforResNet-18with
varyingµ. The table shows the average test accuracy of 3 independent runs over 50 global communication
rounds. Bold numbers indicate the best results.
Methodµ= 0.001 µ= 0.01 µ= 0.1 µ= 1
Acc σAcc(↓) Acc σAcc(↓) Acc σAcc(↓) Acc σAcc(↓)
Equitable-FL 49.89±0.65 14.92±1.72 49.44±0.23 15.06±1.83 35.18±0.61 12.92±1.27 5.24±0.07 4.32±0.30
6 Conclusion and Future Work
In this paper, we presented an equitable learning framework for FL, which reduces the bias against a diverse
set of participants. We utilize the side information offered by the activation vectors to cluster the clients
into groups based on their similarity and use this to propose a weighing mechanism that promotes fairness.
12Under review as submission to TMLR
Additionally, we established a rate of convergence to reach a stationary solution for Equitable-FL. We vi-
sualized the efficacy of our proposed framework on various vision datasets and showed that it consistently
outperforms the baseline in mitigating bias. As previously mentioned, privacy is a cornerstone of Federated
Learning (FL). However, activation vectors can lead to information leakage if the server is compromised.
Despite this risk, privacy-preserving mechanisms exist to enable secure learning without compromising pri-
vacy Schaffer et al. (2012); Qiao et al. (2024). This paper aims to demonstrate the effectiveness of using
activation vectors to cluster client groups and develop a fair weighting mechanism for these groups. We also
propose that this clustering approach can be applied to personalized federated learning Long et al. (2023);
Ghosh et al. (2020). This method effectively clusters client groups and ensures fair solutions for each group,
maintaining fairness for all participants.
References
Durmus Alp Emre Acar and Venkatesh Saligrama. Fedhen: Federated learning in heterogeneous networks.
arXiv preprint arXiv:2207.03031 , 2022.
Durmus Alp Emre Acar, Yue Zhao, Ruizhao Zhu, Ramon Matas, Matthew Mattina, Paul Whatmough,
and Venkatesh Saligrama. Debiasing model updates for improving personalized federated training. In
International conference on machine learning , pp. 21–31. PMLR, 2021.
Ravikumar Balakrishnan, Tian Li, Tianyi Zhou, Nageen Himayat, Virginia Smith, and Jeff Bilmes. Di-
verse client selection for federated learning via submodular maximization. In International Conference on
Learning Representations , 2022.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečn` y, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint
arXiv:1812.01097 , 2018.
Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, and Stanislaw H Żak. On the convergence
of decentralized federated learning under imperfect information sharing. IEEE Control Systems Letters ,
2023.
Huancheng Chen and Haris Vikalo. Heterogeneity-guided client sampling: Towards fast and efficient non-
iid federated learning. (arXiv:2310.00198), October 2024. doi: 10.48550/arXiv.2310.00198. URL http:
//arxiv.org/abs/2310.00198 . arXiv:2310.00198 [cs].
Shu-Ling Cheng, Chin-Yuan Yeh, Ting-An Chen, Eliana Pastor, and Ming-Syan Chen. Fedgcr: Achieving
performance and fairness for federated learning with distinct client types via group customization and
reweighting. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pp. 11498–
11506, 2024.
Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence analysis
and power-of-choice selection strategies. arXiv preprint arXiv:2010.01243 , 2020.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to
handwritten letters. In 2017 international joint conference on neural networks (IJCNN) , pp. 2921–2926.
IEEE, 2017.
Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S Dhillon, and Ufuk Topcu. Faster
non-convex federated learning via global and local momentum. In Uncertainty in Artificial Intelligence ,
pp. 496–506. PMLR, 2022.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine , 29(6):141–142, 2012.
Alp Emre Durmus, Zhao Yue, Matas Ramon, Mattina Matthew, Whatmough Paul, and Saligrama
Venkatesh. Federated learning based on dynamic regularization. In International conference on learn-
ing representations , 2021.
13Under review as submission to TMLR
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference , pp. 214–226,
2012.
Yahya H Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara, and A Salman Avestimehr. Fairfed: Enabling
group fairness in federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 37, pp. 7494–7502, 2023.
Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi. Clustered sampling: Low-variance
and improved representativity for clients selection in federated learning. In International Conference on
Machine Learning , pp. 3407–3416. PMLR, 2021.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered
federated learning. Advances in Neural Information Processing Systems , 33:19586–19597, 2020.
Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active federated
learning. arXiv preprint arXiv:1909.12641 , 2019.
Nina Grgić-Hlača, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian Weller. Beyond distributive
fairness in algorithmic decision making: Feature selection for procedurally fair learning. In Proceedings of
the AAAI conference on artificial intelligence , volume 32, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Geoffrey E. Hinton. Learning multiple layers of representation. Trends in Cognitive Sciences , 11(10):428–434,
October 2007. ISSN 13646613. doi: 10.1016/j.tics.2007.09.004. URL https://linkinghub.elsevier.
com/retrieve/pii/S1364661307002173 .
Andrés Catalino Castillo Jiménez, Ege C Kaya, Lintao Ye, and Abolfazl Hashemi. Submodular maximization
approaches for equitable client selection in federated learning. arXiv preprint arXiv:2408.13683 , 2024.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International
conference on machine learning , pp. 5132–5143. PMLR, 2020.
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian U Stich,
and Ananda Theertha Suresh. Breaking the centralized barrier for cross-device federated learning. Ad-
vances in Neural Information Processing Systems , 34:28663–28676, 2021.
Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in
classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society , pp. 247–254,
2019.
Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , pp. 10713–10722, 2021.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning.
arXiv preprint arXiv:1905.10497 , 2019a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. Proceedings of Machine learning and systems , 2:429–450, 2020.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg
on non-iid data. arXiv preprint arXiv:1907.02189 , 2019b.
Pranay K Lohia, Karthikeyan Natesan Ramamurthy, Manish Bhide, Diptikalyan Saha, Kush R Varshney,
and Ruchir Puri. Bias mitigation post-processing for individual and group fairness. In ICASSP 2019 -
2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 2847–
2851. IEEE, 2019.
14Under review as submission to TMLR
Guodong Long, Ming Xie, Tao Shen, Tianyi Zhou, Xianzhi Wang, and Jing Jiang. Multi-center federated
learning: clients clustering for better personalization. World Wide Web , 26(1):481–500, 2023.
Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. Collaborative fairness in federated learning. Federated
Learning: Privacy and Incentive , pp. 189–204, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics , pp. 1273–1282. PMLR, 2017.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on
bias and fairness in machine learning. ACM computing surveys (CSUR) , 54(6):1–35, 2021.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International
Conference on Machine Learning , pp. 4615–4625. PMLR, 2019.
Lam Nguyen, Phuong Ha Nguyen, Marten Dijk, Peter Richtárik, Katya Scheinberg, and Martin Takác. Sgd
and hogwild! convergence without the bounded gradients assumption. In International Conference on
Machine Learning , pp. 3750–3758. PMLR, 2018.
Dong Qiao, Chris Ding, and Jicong Fan. Federated spectral clustering via secure similarity reconstruction.
Advances in Neural Information Processing Systems , 36, 2024.
Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. Clustered federated learning: Model-agnostic
distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and
learning systems , 32:3710–3722, 2020.
Péter Schaffer, Károly Farkas, Ádám Horváth, Tamás Holczer, and Levente Buttyán. Secure and reliable
clustering in wireless sensor networks: a critical survey. Computer Networks , 56(11):2726–2741, 2012.
Yifan Shi, Li Shen, Kang Wei, Yan Sun, Bo Yuan, Xueqian Wang, and Dacheng Tao. Improving the model
consistency of decentralized federated learning. arXiv preprint arXiv:2302.04083 , 2023.
Antesh Upadhyay and Abolfazl Hashemi. Improved convergence analysis and snr control strategies for
federated learning in the presence of noise. IEEE Access , 2023.
Saeed Vahidian, Mahdi Morafah, Weijia Wang, Vyacheslav Kungurtsev, Chen Chen, Mubarak Shah, and
Bill Lin. Efficient distribution similarity identification in clustered federated learning via principal angles
between client data subspaces. In Proceedings of the AAAI conference on artificial intelligence , volume 37,
pp. 10043–10052, 2023.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing , 17:395–416, 2007.
Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen, Cheng Wang, and Rongshan Yu. Federated
learning with fair averaging. arXiv preprint arXiv:2104.14937 , 2021.
Xubo Yue, Maher Nouiehed, and Raed Al Kontar. Gifair-fl: A framework for group and individual fairness
in federated learning. INFORMS Journal on Data Science , 2(1):10–23, 2023.
Yufeng Zhan, Jie Zhang, Zicong Hong, Leijie Wu, Peng Li, and Song Guo. A survey of incentive mechanism
design for federated learning. IEEE Transactions on Emerging Topics in Computing , 10(2):1035–1044,
2021.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial
learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society , pp. 335–340,
2018.
15Under review as submission to TMLR
Appendix
A Proof of Theorem 1
We base our convergence analysis on a framework similar to that of Das et al. (2022). However, our
approach introduces distinct variations due to the addition of a proximal term in local updates and the
equitable reweighting of these updates across clients, leading to differences in the analysis compared to Das
et al. (2022).
Proof.From Lemma 1, for ηkLE≤1
2,ηkµE≤1
2, we upper bound the per-round progress as:
E[f(wk+1)]≤E[f(wk)]−ηkE(1−µ)
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+η2
kE2/parenleftbigg
9ηkL2E+ 4ηkµE+ 6L/parenleftbigg
1 +4η2
kµ2E2
18/parenrightbigg/parenrightbiggC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+η2
kE/parenleftbigg
L/parenleftbigg
1 +2ηk
3/parenrightbigg
+8ηkL2E(2 +E)
9+4ηkµE(1 +ηkµE)
9
+4ηkE
3C/parenleftig
L2+µ
2/parenrightigC/summationdisplay
q=11
|γq|/parenrightigg
σ2.(10)
Now using L-smoothness and 2 of fi’s, we get:
C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈[γq]E[∥∇fi(wk)∥2]≤C/summationdisplay
q=12L
|γq|×C/summationdisplay
i∈[γq](E[fi(wk)]−f∗
i)
C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈[γq]E[∥∇fi(wk)∥2]≤2LE[f(wk)]−C/summationdisplay
q=12L
|γq|×C/summationdisplay
i∈[γq]f∗
i
C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈[γq]E[∥∇fi(wk)∥2]≤2nLE[f(wk)]. (11)
Using eq. (11) in eq. (10), we get for a constant learning rate of ηk=η:
E[f(wk+1)]≤/parenleftbigg
1 +η2E2/parenleftbigg
9ηL2E+ 4ηµE + 6L/parenleftbigg
1 +4η2µ2E2
18/parenrightbigg/parenrightbigg/parenrightbigg
E[f(wk)]−ηE(1−µ)
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+η2E/parenleftig
L/parenleftbigg
1 +2η
3/parenrightbigg
+8ηL2E(2 +E)
9+4ηµE (1 +ηµE)
9+4ηE
3C/parenleftig
L2+µ
2/parenrightigC/summationdisplay
q=11
|γq|/parenrightig
σ2.(12)
For clarity, define ζ:=η2E2/parenleftig
9ηL2E+ 4ηµE + 6L/parenleftig
1 +4η2µ2E2
18/parenrightig/parenrightig
and
ζ2:=η2E/parenleftigg
L/parenleftbigg
1 +2η
3/parenrightbigg
+8ηL2E(2 +E)
9+4ηµE (1 +ηµE)
9+4ηE
3C/parenleftig
L2+µ
2/parenrightigC/summationdisplay
q=11
|γq|/parenrightigg
.
Then unfolding the recursion from k= 0tok=K−1, we get:
E[f(wK)]≤(1 +ζ1)KE[f(wk)]−ηE(1−µ)
2K−1/summationdisplay
k=0(1 +ζ)(K−1−k)E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+η2Eζ2σ2K−1/summationdisplay
k=0(1 +ζ)(K−1−k).(13)
16Under review as submission to TMLR
Let us define pk:=(1+ζ)(K−1−k)/summationtextK−1
k′=0(1+ζ1)(K−1−k′). Then, set µ < 1and re-arranging eq. (13) using the fact that
E[f(wK)]≥0, we get:
K−1/summationdisplay
k=0pkE[∥∇f(wk)∥2]≤2 (1 +ζ)Kf(w0)
ηE(1−µ)/summationtextK−1
k′=0(1 +ζ)k′+2ηζ2
(1−µ)σ2(14)
=2ζf(w0)
ηE(1−µ)/parenleftig
1−(1 +ζ)−K/parenrightig+2ηζ2
(1−µ)σ2, (15)
where the eq. (15) follows by using the fact that/summationtextK−1
k′=0(1 +ζ)k′=(1+ζ)K−1
ζ. Now,
(1 +ζ1)−K<1−ζK+ζ2K(K+ 1)
2<1−ζK+ζ2K2
=⇒1−(1 +ζ)−K>ζK (1−ζK).
Plugging this in eq. (15), we have for ζK < 1:
K−1/summationdisplay
k=0pkE[∥∇f(wk)∥2]≤2f(w0)
ηEK (1−µ)(1−ζK)+2ηζ2
(1−µ)σ2. (16)
Now, note that the optimal step size will be η=O/parenleftig
1
E√
LK/parenrightig
. So, then let us pick η=1
4E√
3LK. We need to
haveηLE≤1
2andηµE≤1
2; which happens for K≥max/parenleftig
L
12,µ2
12L/parenrightig
. Furthermore, lets ensure that ζK <1
2;
which happens for K≥max/parenleftig
3L
32,µ2
108L3,µ2
72L/parenrightig
. Therefore we should have K≥max/parenleftig
3L
32,µ2
12L,µ2
108L3/parenrightig
. So,
plugging in η=1
4E√
3LKand1−ζK≥1
2in eq. (16); we get,
K−1/summationdisplay
k=0pkE[∥∇f(wk)∥2]≤16√
3Lf(w0)
(1−µ)√
K+√
Lσ2
2√
3KE(1−µ)
+σ2
36E2K(1−µ)+(2 +E)Lσ2
36K(1−µ)+µσ2
6ELK (1−µ)
+σ2
18CELK (1−µ)/parenleftig
L2+µ
2/parenrightigC/summationdisplay
q=11
|γq|.(17)
This finishes the proof.
B Supporting Lemmas
Lemma 1. ForηkLE≤1
2and ForηkµE≤1
2,
E[f(wk+1)]≤E[f(wk)]−ηkE(1−µ)
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+η2
kE2/parenleftbigg
9ηkL2E+ 4ηkµE+ 6L/parenleftbigg
1 +4η2
kµ2E2
18/parenrightbigg/parenrightbiggC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+η2
kE/parenleftbigg
L/parenleftbigg
1 +2ηk
3/parenrightbigg
+8ηkL2E(2 +E)
9+4ηkµE(1 +ηkµE)
9
+4ηkE
3C/parenleftig
L2+µ
2/parenrightigC/summationdisplay
q=11
|γq|/parenrightigg
σ2.(18)
17Under review as submission to TMLR
Proof.Define
wi
k,τ=wk−ηkτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
−ηkµτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig
, (19)
wk,τ=wk−ηkC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
−ηkµC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig
,(20)
wk+1=wk−ηkC/summationdisplay
q=11
|γqk|×C/summationdisplay
i∈γqkE−1/summationdisplay
τ=0/tildewide∇fi/parenleftbig
wi
k,τ;Bi
k,τ/parenrightbig
−ηkµC/summationdisplay
q=11
|γqk|×C/summationdisplay
i∈γqkE−1/summationdisplay
τ=0/parenleftbig
wi
k,τ−wk/parenrightbig
. (21)
For any two vectors aandb, we have
⟨a,b⟩=1
2/parenleftbig
∥a∥2+∥b∥2−∥a−b∥2/parenrightbig
. (22)
Also, note
E{Bi
k,t}|γq|
i=1
1
|γq|/summationdisplay
i∈γq/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
=1
|γq|/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,t/parenrightbig
, (23)
E{Bi
k,t}|γq|,τ−1
i=1,t=0
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleτ−1/summationdisplay
t=01
|γq|/summationdisplay
i∈γq/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=ττ−1/summationdisplay
t=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γq|/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+τσ2
|γq|,(24)
E{Bi
k,t}τ−1
t=0
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=ττ−1/summationdisplay
t=0E/bracketleftbig
∥∇fi/parenleftbig
wi
k,t/parenrightbig
∥2/bracketrightbig
+τσ2. (25)
Sinceσ2is the maximum variance of the stochastic gradients, eqs. (24) and (25) follow due to the indepen-
dence of the noise in each local update of each client. Now using the L-smoothness of fand eq. (21), we
get
E[f(wk+1)]≤E[f(wk)] + (A) + (B) + (M), (26)
where
A=−ηkE
/angbracketleftigg
∇f(wk),C/summationdisplay
q=11
|γqk|×C/summationdisplay
i∈γqkE−1/summationdisplay
τ=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,τ/parenrightbig/angbracketrightigg
,
B=−ηkµE
/angbracketleftigg
∇f(wk),C/summationdisplay
q=11
|γqk|×C/summationdisplay
i∈γqkE−1/summationdisplay
τ=0/parenleftbig
wi
k,τ−wk/parenrightbig/angbracketrightigg
,
and
M=η2
kL
2E/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γqk|×C/summationdisplay
i∈γqkE−1/summationdisplay
τ=0/tildewide∇fi/parenleftbig
wi
k,τ;Bi
k,τ/parenrightbig
+µC/summationdisplay
q=11
|γqk|×C/summationdisplay
i∈γqkE−1/summationdisplay
τ=0/parenleftbig
wi
k,τ−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
.
18Under review as submission to TMLR
Starting with (A) by taking an expectation over the set γqk,
A=−ηkE−1/summationdisplay
τ=0E
/angbracketleftigg
∇f(wk),C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,τ/parenrightbig/angbracketrightigg

=−ηkE
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
−ηk
2E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+ηk
2E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(wk)−C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (27)
≤−ηkE
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+ηk
2E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(wk)−C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
A1.(28)
Note eq. (27) follows due to eq. (23), linearity of inner product and eq. (22) while eq. (28) follows by dropping
the second term on the right side of eq. (27). Now using A1,
A1=ηk
2E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(wk)−C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,τ/parenrightbig
−∇f(wk,τ) +∇f(wk,τ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

≤ηkE−1/summationdisplay
τ=0E/bracketleftig
∥∇f(wk)−∇f(wk,τ)∥2/bracketrightig
+ηkE−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(wk,τ)−C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (29)
≤ηkL2E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
+ηkE−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(wk,τ)−C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(30)
≤ηkL2E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
+ηkE−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq/parenleftbig
∇fi(wk,τ)−∇fi/parenleftbig
wi
k,τ/parenrightbig/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (31)
≤ηkL2E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
+ηkL2
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewk,τ−wi
k,τ/vextenddouble/vextenddouble2/bracketrightig
. (32)
Here, eq. (29) follows using Young’s inequality, eq. (30) follows from L-smoothness of f, and eq. (32) follows
using Jensen’s inequality. So, Abecomes,
A≤−ηkE
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+ηkL2E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
+ηkL2
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewk,τ−wi
k,τ/vextenddouble/vextenddouble2/bracketrightig
.(33)
19Under review as submission to TMLR
Now using Band taking an expectation over set γqk,
B=−ηkµE−1/summationdisplay
τ=0E
/angbracketleftigg
∇f(wk),C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq/parenleftbig
wi
k,τ−wk/parenrightbig/angbracketrightigg

=−ηkµE
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
−ηkµ
2E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq/parenleftbig
wi
k,τ−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+ηkµ
2E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇f(wk)−C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq/parenleftbig
wi
k,τ−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (34)
=−ηkµE
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
−ηkµ
2E−1/summationdisplay
τ=0E/bracketleftig
∥wk,τ−wk∥2/bracketrightig
+ηkµ
2E−1/summationdisplay
τ=0E/bracketleftig
∥∇f(wk)−wk,τ+wk∥2/bracketrightig
(35)
≤ηkµE
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+ηkµ
2E−1/summationdisplay
τ=0E/bracketleftig
∥wk,τ−wk∥2/bracketrightig
. (36)
Note eq. (34) follows due to eq. (22), while eqs. (35) and (36) follows from eq. (20) and Young’s inequality
respectively. Now using M,
M≤η2
kLE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γqk|×C/summationdisplay
i∈γqkE−1/summationdisplay
τ=0/tildewide∇fi/parenleftbig
wi
k,τ;Bi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+η2
kµ2LE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γqk|×C/summationdisplay
i∈γqkE−1/summationdisplay
τ=0/parenleftbig
wi
k,τ−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (37)
≤η2
kLE
CC/summationdisplay
q=1E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γqk|/summationdisplay
i∈γqk/tildewide∇fi/parenleftbig
wi
k,τ;Bi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+η2
kµ2LE
CC/summationdisplay
q=1E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γqk|/summationdisplay
i∈γqk/parenleftbig
wi
k,τ−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (38)
≤η2
kLE
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/tildewide∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+η2
kµ2LE
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
(39)
≤η2
kLE
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
+η2
kLE2σ2
+η2
kµ2LE
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
. (40)
Here, eq. (37) follows from Young’s inequality. Additionally, eq. (38) follows from Jensen’s inequality, and
Young’s inequality, while eq. (39) follows from taking expectation w.r.t γk. Now, putting A,B, andMback
20Under review as submission to TMLR
in eq. (26), we get
E[f(wk+1)]≤E[f(wk)]−ηkE(1−µ)
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+η2
kLE2σ2
+ηk(L2+µ
2)E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(X)+ηkL2
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewk,τ−wi
k,τ/vextenddouble/vextenddouble2/bracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(Y)
+η2
kLE
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(Z)+η2
kµ2LE
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(W).(41)
UsingXalong with Lemma 2:
X≤4η3
kE2
3C(L2+µ
2)C/summationdisplay
q=1E−1/summationdisplay
τ=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γq|/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+4η3
kE2
3C(L2+µ
2)C/summationdisplay
q=1σ2
|γq|
≤4η3
kE2
3C(L2+µ
2)C/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
+4η3
kE2
3C(L2+µ
2)C/summationdisplay
q=1σ2
|γq|(42)
≤8η3
kE3
C(L2+µ
2)C/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+4η3
kE2
3(L2+µ
2)/parenleftigg
2
3+1
CC/summationdisplay
q=11
|γq|/parenrightigg
σ2(43)
Equation (42) follows from Jensen’s inequality and eq. (43) follows from lemma 5. Now, using Yalong
with Lemma 3, we get
Y≤8η3
KL2E3
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+8η3
KL2E2(1 +E)
9σ2. (44)
UsingZ:
Z=η2
kLE
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
≤6η2
kLE2
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+2η2
kLE
3σ2. (45)
Equation (45) follows from Lemma 5. Now using W,
W=η2
kµ2LE
CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
≤4η4
kµ2LE4
3CC/summationdisplay
q=11
|γq|/summationdisplay
i∈γqE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+44η4
kµ2LE3
9σ2. (46)
Equation (46) follows due to Lemma 4 and Lemma 5. Putting X,Y,Z, andWback in eq. (41), we get
E[f(wk+1)]≤E[f(wk)]−ηkE(1−µ)
2E/bracketleftig
∥∇f(wk)∥2/bracketrightig
+η2
kE2/parenleftbigg
9ηkL2E+ 4ηkµE+ 6L/parenleftbigg
1 +4η2
kµ2E2
18/parenrightbigg/parenrightbiggC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
21Under review as submission to TMLR
+η2
kE/parenleftbigg
L/parenleftbigg
1 +2ηk
3/parenrightbigg
+8ηkL2E(2 +E)
9+4ηkµE(1 +ηkµE)
9
+4ηkE
3C/parenleftig
L2+µ
2/parenrightigC/summationdisplay
q=11
|γq|/parenrightigg
σ2.
Lemma 2. ForηkLE≤1
2,
E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
≤4η2
kE2
3CE−1/summationdisplay
τ=0C/summationdisplay
q=1E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γq|/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+4η2
kE2
3CC/summationdisplay
q=1σ2
|γq|
Proof.
E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
=E/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublewk−wk+ηkC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t)/parenrightbig
+ηkµC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
(47)
=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηkC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t)/parenrightbig
+ηkµC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

≤2η2
kE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 2η2
kµ2E/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
(48)
≤2η2
k
CC/summationdisplay
q=1E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γq|/summationdisplay
i∈γqτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 2η2
kµ2ττ−1/summationdisplay
t=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
(49)
≤2η2
kτ
Cτ−1/summationdisplay
t=0C/summationdisplay
q=1E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γq|/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+2η2
kτ
CC/summationdisplay
q=1σ2
|γq|+ 2η2
kµ2ττ−1/summationdisplay
t=0E/bracketleftig
∥wk−wk,t∥2/bracketrightig
. (50)
Equation (47) follows from eq. (20), and eq. (48) follows from Young’s inequality, eq. (49) follows from
Jensen’s inequality, eq. (20) and Young’s inequality. Furthermore, eq. (50) follows from eq. (24). Now,
summing up eq. (50) for all τ∈{0,...,E−1}, we get:
E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
≤η2
kE2
CE−1/summationdisplay
τ=0C/summationdisplay
q=1E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γq|/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+η2
kE2
CC/summationdisplay
q=1σ2
|γq|+η2
kµ2E2E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
(51)
E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
≤η2
kE2
(1−η2
kµ2E2)CE−1/summationdisplay
τ=0C/summationdisplay
q=1E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γq|/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+η2
kE2
(1−η2
kµ2E2)CC/summationdisplay
q=1σ2
|γq|
(52)
22Under review as submission to TMLR
ForηkLE≤1
2in eq. (52), we get
E−1/summationdisplay
τ=0E/bracketleftig
∥wk−wk,τ∥2/bracketrightig
≤4η2
kE2
3CE−1/summationdisplay
τ=0C/summationdisplay
q=1E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|γq|/summationdisplay
i∈γq∇fi/parenleftbig
wi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+4η2
kE2
3CC/summationdisplay
q=1σ2
|γq|.
Lemma 3. ForηkµE≤1
2,
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk,τ/vextenddouble/vextenddouble2/bracketrightig
≤8η2
kE2C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+8η2
kE2(1 +E)
9σ2.
Proof.
E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk,τ/vextenddouble/vextenddouble2/bracketrightig
=E/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/braceleftigg
wk−ηkτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
−ηkµτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig/bracerightigg
−/braceleftigg
wk−ηkC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
−ηkµC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig/bracerightigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
(53)
=η2
kE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleτ−1/summationdisplay
t=0
C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
−/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig

+µτ−1/summationdisplay
t=0
C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqwi
k,t−wi
k,t
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

≤2η2
kττ−1/summationdisplay
t=0E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γq/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
−/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+ 2η2
kµ2ττ−1/summationdisplay
t=0
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleC/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqwi
k,t−wi
k,t/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (54)
≤2η2
kττ−1/summationdisplay
t=0C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE/bracketleftig
∥/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
∥2/bracketrightig
+ 2η2
kµ2ττ−1/summationdisplay
t=0/bracketleftig/vextenddouble/vextenddoublewi
k,t−wk,t/vextenddouble/vextenddouble2/bracketrightig
(55)
≤2η2
kττ−1/summationdisplay
t=0C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE/bracketleftig/vextenddouble/vextenddouble∇fi(wi
k,t)/vextenddouble/vextenddouble2/bracketrightig
+ 2η2
kτ2σ2+ 2η2
kµ2ττ−1/summationdisplay
t=0/bracketleftig/vextenddouble/vextenddoublewi
k,t−wk,t/vextenddouble/vextenddouble2/bracketrightig
.(56)
Here, eq. (53) follows from eqs. (19) and (20) and eq. (54) follows from Young’s inequality. Now, eq. (55)
follows using the fact that the second moment is greater than or equal to the variance and eq. (20).
Again, eq. (56) follows from eq. (25). Then, summing up eq. (56) for all τ∈{0,...,E−1}, we get,
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk,τ/vextenddouble/vextenddouble2/bracketrightig
≤η2
kE2E−1/summationdisplay
τ=0C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE/bracketleftig/vextenddouble/vextenddouble∇fi(wi
k,τ)/vextenddouble/vextenddouble2/bracketrightig
+2η2
kE3
3σ2(57)
23Under review as submission to TMLR
+η2
kµ2E2E−1/summationdisplay
τ=0/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk,τ/vextenddouble/vextenddouble2/bracketrightig
(58)
ForηkµE≤1
2in eq. (57), we get
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk,τ/vextenddouble/vextenddouble2/bracketrightig
≤4η2
kE2
3C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi(wi
k,τ)/vextenddouble/vextenddouble2/bracketrightig
+8η2
kE3
9σ2.(59)
Putting the results from Lemma 5 back in eq. (59), we get
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk,τ/vextenddouble/vextenddouble2/bracketrightig
≤8η2
kE3C/summationdisplay
q=11
|γq|×C/summationdisplay
i∈γqE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+8η2
kE2(1 +E)
9σ2.
Lemma 4. ForηkµE≤1
2,
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
≤4η2
kE2
3E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
+4η2
kE2σ2
3.
Proof.
E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublewk−ηkτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
−ηkµτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig
−wk/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
 (60)
=E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleηkτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig
+ηkµτ−1/summationdisplay
t=0/parenleftbig
wi
k,t−wk/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

≤2η2
kE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleτ−1/summationdisplay
t=0/tildewide∇fi/parenleftbig
wi
k,t;Bi
k,t/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 2η2
kµ2ττ−1/summationdisplay
t=0E/bracketleftig/vextenddouble/vextenddouble/parenleftbig
wi
k,t−wk/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
(61)
≤2η2
kττ−1/summationdisplay
t=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,t/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
+ 2η2
kτσ2+ 2η2
kµ2ττ−1/summationdisplay
t=0E/bracketleftig/vextenddouble/vextenddouble/parenleftbig
wi
k,t−wk/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
.(62)
In eq. (60), follows due to eq. (19), and in eq. (61), follows from Young’s inequality. We use eq. (25) to
get eq. (62). Now, summing up eq. (62) for all τ∈{0,...,E−1}, we get,
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
≤η2
kE2E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
η2
kE2σ2+η2
kµ2E2E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble/parenleftbig
wi
k,τ−wk/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
≤η2
kE2
(1−η2
kµ2E2)E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
+η2
kE2σ2
(1−η2
kµ2E2). (63)
ForηkµE≤1
2, we get
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
≤4η2
kE2
3E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
+4η2
kE2
3σ2.
Lemma 5. ForηkLE≤1
2,
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
= 6E−1/summationdisplay
τ=0E/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+2
3σ2.
24Under review as submission to TMLR
Proof.
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
=E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi(wi
k,τ)−∇fi(wk) +∇fi(wk)/vextenddouble/vextenddouble2/bracketrightig
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
≤2EE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+ 2E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi(wi
k,τ)−∇fi(wk)/vextenddouble/vextenddouble2/bracketrightig
(64)
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
≤2EE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+ 2L2E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddoublewi
k,τ−wk/vextenddouble/vextenddouble2/bracketrightig
. (65)
Equation (64) follows from Young’s inequality and eq. (65) follows from L-smoothness of fi. Now, putting
the results of Lemma 4 back in eq. (65), we get
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
≤2EE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+8η2
kL2E2
3E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
+8η2
kL2E2
3σ2.(66)
ForηkLE≤1
2, we get
E−1/summationdisplay
τ=0E/bracketleftig/vextenddouble/vextenddouble∇fi/parenleftbig
wi
k,τ/parenrightbig/vextenddouble/vextenddouble2/bracketrightig
≤6EE/bracketleftig
∥∇fi(wk)∥2/bracketrightig
+2
3σ2. (67)
25