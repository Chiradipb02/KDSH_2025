Under review as submission to TMLR
Gradient Descent Temporal Diﬀerence-diﬀerence Learning
Anonymous authors
Paper under double-blind review
Abstract
Though widely useful in reinforcement learning, “semi-gradient" methods—including TD( λ)
and Q-learning—do not converge as robustly as gradient-based methods. Even in the case of
linear function approximation, convergence cannot be guaranteed for these methods when
they are used with oﬀ-policy training, in which an agent uses a behavior policy that diﬀers
from the target policy in order to gain experience for learning. To address this, alternative
algorithms that are provably convergent in such cases have been introduced, the most well
known being gradient descent temporal diﬀerence (GTD) learning. This algorithm and
others like it, however, tend to converge much more slowly than conventional temporal
diﬀerence learning. In this paper we propose gradient descent temporal diﬀerence-diﬀerence
(Gradient-DD) learning in order to improve GTD2, a GTD algorithm (Sutton et al., 2009b),
by introducing second-order diﬀerences in successive parameter updates. We investigate this
algorithm in the framework of linear value function approximation, theoretically proving
its convergence by applying the theory of stochastic approximation. Studying the model
empirically on the random walk task, the Boyan-chain task, and the Baird’s oﬀ-policy
counterexample, we ﬁnd substantial improvement over GTD2 and, in several cases, better
performance even than conventional TD learning.
1 Introduction
Many of the recent practical successes of reinforcement learning have been achieved using “semi-gradient"
methods—including TD( λ) and Q-learning—in which bootstrapping is used to quickly estimate a value
function. However, because they introduce bias by learning a bootstrapped estimate of the target rather than
the target itself, semi-gradient methods do not converge as robustly as gradient-based methods (Sutton &
Barto, 2018). Even in the relatively simple case of linear function approximation, convergence cannot be
guaranteed for these methods when they are used with oﬀ-policy training (Baird, 1995), in which an agent
uses a behavior policy that diﬀers from the target policy in order to gain experience for learning.
To address this shortcoming and to ground value prediction in the framework of stochastic gradient descent, the
gradient-based temporal diﬀerence algorithms GTD and GTD2 were introduced (Sutton et al., 2009a;b). These
algorithms are compatible with both linear function approximation and oﬀ-policy training, ensuring stability
with computational complexity scaling linearly with the size of the function approximator. Despite this
theoretical assurance, empirical evidence suggests that their convergence is notably slower than conventional
temporal diﬀerence (TD) learning, limiting their practical utility (Ghiassian et al., 2020; White & White,
2016). Building on this work, extensions to the GTD family of algorithms (see (Ghiassian et al., 2018) for
a review) have allowed for incorporating eligibility traces (Maei & Sutton, 2010; Geist & Scherrer, 2014),
non-linear function approximation such as with a neural network (Maei, 2011), and reformulation of the
optimization as a saddle point problem (Liu et al., 2015; Du et al., 2017). However, due to their slow
convergence, none of these stable oﬀ-policy methods are commonly used in practice.
In this work, we introduce a new gradient descent algorithm for temporal diﬀerence learning with linear
value function approximation. This algorithm, which we call gradient descent temporal diﬀerence-diﬀerence
(Gradient-DD) learning, is an acceleration technique that employs second-order diﬀerences in successive
parameter updates. The basic idea of Gradient-DD is to modify the error objective function by additionally
considering the prediction error obtained in the last time step, then to derive a gradient-descent algorithm
1Under review as submission to TMLR
based on this modiﬁed objective function. In addition to exploiting the Bellman equation to get the solution,
this modiﬁed error objective function avoids drastic changes in the value function estimate by encouraging
local search around the current estimate. Algorithmically, the Gradient-DD approach only adds an additional
term to the update rule of the GTD2 method, and the extra computational cost is negligible. We prove
its convergence by applying the theory of stochastic approximation. This result is supported by numerical
experiments, which also show that Gradient-DD obtains better convergence in many cases than conventional
TD learning.
1.1 Related Work
In related approaches to ours, some previous studies have attempted to improve Gradient-TD algorithms
by adding regularization terms to the objective function. These approaches have used l1regularization on
weights to learn sparse representations of value functions Liu et al. (2012), or l2regularization on weights
Ghiassian et al. (2020). Our work is diﬀerent from these approaches in two ways. First, whereas these
previous studies investigated a variant of TD learning with gradient corrections, we take the GTD2 algorithm
as our starting point. Second, unlike these previous approaches, our approach modiﬁes the error objective
function by using a distance constraint rather than a penalty on weights. The distance constraint works by
restricting the search to some region around the evaluation obtained in the most recent time step. With
this modiﬁcation, our method provides a learning rule that contains second-order diﬀerences in successive
parameter updates.
Our approach is similar to trust region policy optimization (Schulman et al., 2015) or relative entropy policy
search (Peters et al., 2010), which penalize large changes being learned in policy learning. In these methods,
constrained optimization is used to update the policy by considering the constraint on some measure between
the new policy and the old policy. Here, however, our aim is to ﬁnd the optimal value function, and the
regularization term uses the previous value function estimate to avoid drastic changes in the updating process.
Our approach bears similarity to the natural gradient approach widely used in reinforcement learning (Amari,
1998; Bhatnagar et al., 2009; Degris et al., 2012; Dabney & Thomas, 2014; Thomas et al., 2016), which uses
the metric tensor to correct for the local geometry of the parameter space, and also features a constrained
optimization form. However, Gradient-DD is distinct from the natural gradient. The essential diﬀerence
is that, unlike the natural gradient, Gradient-DD is a trust region method, which deﬁnes the trust region
according to the diﬀerence between the current value and the value obtained from the previous step. From
the computational cost viewpoint, unlike natural TD (Dabney & Thomas, 2014), which needs to update an
estimate of the metric tensor, the computational cost of Gradient-DD is essentially the same as that of GTD2.
2 Gradient descent method for temporal diﬀerence learning
2.1 Problem deﬁnition and background
In this section, we formalize the problem of learning the value function for a given policy under the Markov
decision process (MDP) framework. In this framework, the agent interacts with the environment over a
sequence of discrete time steps, t= 1,2,.... At each time step the agent observes a state st∈Sand selects
an actionat∈A. In response, the environment emits a reward rt∈Rand transitions the agent to its next
statest+1∈S. The state and action sets are ﬁnite. State transitions are stochastic and dependent on the
immediately preceding state and action. Rewards are stochastic and dependent on the preceding state and
action, as well as on the next state. The process generating the agent’s actions is termed the behavior policy.
In oﬀ-policy learning, this behavior policy is in general diﬀerent from the target policy π:S→A. The
objective is to learn an approximation to the state-value function under the target policy in a particular
environment:
V(s) =Eπ/bracketleftBig/summationdisplay∞
t=1γt−1rt|s1=s/bracketrightBig
, (1)
whereγ∈[0,1)is the discount rate.
In problems for which the state space is large, it is practical to approximate the value function. In this paper
we consider linear function approximation, where states are mapped to feature vectors with fewer components
2Under review as submission to TMLR
than the number of states. Speciﬁcally, for each state s∈Sthere is a corresponding feature vector x(s)∈Rp,
withp≤|S|, such that the approximate value function is given by
Vw(s) :=w/latticetopx(s). (2)
The goal is then to learn the parameters wsuch thatVw(s)≈V(s).
2.2 Gradient temporal diﬀerence learning
A major breakthrough for the study of the convergence properties of MDP systems came with the introduction
of the GTD and GTD2 learning algorithms (Sutton et al., 2009a;b). We begin by brieﬂy recapitulating the
GTD algorithms, which we will then extend in the following sections. To begin, we introduce the Bellman
operator Bsuch that the true value function V∈R|S|satisﬁes the Bellman equation:
V=R+γPV=:BV,
where Ris the reward vector with components E(rn|sn=s), and Pis a matrix of the state transition
probabilities under the target policy. In temporal diﬀerence methods, an appropriate objective function
should minimize the diﬀerence between the approximate value function and the solution to the Bellman
equation.
Having deﬁned the Bellman operator, we next introduce the projection operator Π, which takes any value
function Vand projects it to the nearest value function within the space of approximate value functions of
the form Eqn. (2). Letting Xbe the matrix whose rows are x(s), the approximate value function can be
expressed as Vw=Xw. The projection operator is then given by
Π=X(X/latticetopDX)−1X/latticetopD,
where the matrix Dis diagonal, with each diagonal element dscorresponding to the probability of visiting
statesunder the behavior policy. We consider a general setting as in Sutton et al. (2009b;a), where the ﬁrst
state of each transition is chosen i.i.d. according to an arbitrary distribution that may be unrelated to P.
This setting deﬁnes a probability over independent triples of state, next state, and reward random variables,
denoted (sn,sn+1,rn), with associated feature-vector random variables xn=xsnandxn+1=xsn+1.
The natural measure of how closely the approximation Vwsatisﬁes the Bellman equation is the mean-squared
Bellman error:
MSBE (w) =/bardblVw−BVw/bardbl2
D, (3)
where the norm is weighted by D, such that/bardblV/bardbl2
D=V/latticetopDV. However, because the Bellman operator
follows the underlying state dynamics of the Markov chain, irrespective of the structure of the linear function
approximator, BVwwill typically not be representable as Vwfor any w. An alternative objective function,
therefore, is the mean squared projected Bellman error (MSPBE), which is deﬁned by Sutton et al. (2009b) as
J(w) =/bardblVw−ΠBV w/bardbl2
D. (4)
Following Sutton et al. (2009b), our objective is to minimize this error measure. As usual in stochastic
gradient descent, the weights at each time step are then updated by ∆w=−α∇J(w), whereα>0, and
−1
2∇J(wn) =−E[(γxn+1−xn)x/latticetop
n][E(xnx/latticetop
n)]−1E(δnxn). (5)
We have also introduced the temporal diﬀerence error δn=rn+ (γxn+1−xn)/latticetopwn. Let ηndenote the
estimate of [E(xnx/latticetop
n)]−1E(δnxn)at the time step n. Because the factors in Eqn. (5) can be directly sampled,
the resulting updates in each step are
δn=rn+ (γxn+1−xn)/latticetopwn
ηn+1=ηn+βn(δn−x/latticetop
nηn)xn
wn+1=wn−αn(γxn+1−xn)(x/latticetop
nηn). (6)
3Under review as submission to TMLR
These updates deﬁne the GTD2 learning algorithm, which we will build upon in the following section.
While the algorithm described above would be appropriate for on-policy learning, we are interested in the
case of oﬀ-policy learning, in which actions are selected based on a behavior policy diﬀerent from the target
policy. If value estimation consists of the estimation of the expected returns, this oﬀ-policy setting involves
estimating an expectation conditioned on one distribution with samples collected under another. GTD2 can
be extended to make oﬀ-policy updates by using importance sampling ratios ρn=π(an|sn)/b(an|sn)≥0
whereandenotes the action taken at state sn. The resulting modiﬁcations to the equations for updating ηn
andwnare as follows:
ηn+1=ηn+βn(ρnδn−x/latticetop
nηn)xn
wn+1=wn−αnρn(γxn+1−xn)(x/latticetop
nηn). (7)
3 Gradient descent temporal diﬀerence-diﬀerence learning
In this section we modify the objective function by additionally considering the diﬀerence between Vwand
Vwn−1, which denotes the value function estimate at step n−1of the optimization. We propose a new
objectiveJGDD(w|wn−1), where the notation “ w|wn−1" in the parentheses means that the objective is deﬁned
given Vwn−1of the previous time step n−1. Speciﬁcally, we modify Eqn. (4) as follows:
JGDD(w|wn−1) =J(w) +κ/bardblVw−Vwn−1/bardbl2
D, (8)
whereκ≥0is a parameter of the regularization, and we assume that κis constant. We show in Section A.1
of the appendix that minimizing Eqn. (8) is equivalent to the following optimization
arg min
wJ(w)s.t./bardblVw−Vwn−1/bardbl2
D≤µ (9)
whereµ>0is a parameter which becomes large when κis small, so that the MSPBE objective is recovered
asµ→∞, equivalent to κ→0in Eqn. (8).
 
Figure 1: Schematic diagram of Gradient-DD learn-
ing with w∈R2. Rather than updating wdirectly
along the gradient of the MSPBE (black arrow), the
update rule selects the direction starting from wn(red
star) that minimizes the MSPBE while satisfying the
constraint/bardblVw−Vwn−1/bardbl2
D≤µ(shaded ellipse).
Rather than simply minimizing the optimal prediction from the projected Bellman equation, the agent makes
use of the most recent update to look for the solution, choosing a wthat minimizes the MSPBE while
following the constraint that the estimated value function should not change too greatly, as illustrated in
Fig. 1. In eﬀect, the regularization term encourages searching around the estimate at previous time step,
especially when the state space is large.
Eqn. (9) shows that the regularized objective is a trust region approach, which seeks a direction that attains
the best improvement possible subject to the distance constraint. The trust region is deﬁned by the value
4Under review as submission to TMLR
distance rather than the weight distance, meaning that Gradient-DD also makes use of the natural gradient
of the objective around wn−1rather than around wn(see Section A.2 of the appendix for details). In this
sense, our approach can be explained as a trust region method that makes use of natural gradient information
to prevent the estimated value function from changing too drastically.
For comparison with related approaches using natural gradients, in Fig. 9 of the appendix we compare the
empirical performance of our algorithm with natural GTD2 and natural TDC Dabney & Thomas (2014) using
the random walk task introduced below in Section 5. In addition, we compared our approach of regularizing
the objective using the diﬀerence in successive value estimates ( κ/bardblVwn−Vwn−1/bardbl2
D)vs.using the diﬀerence
in successive parameters ( κ/bardblwn−wn−1/bardbl2). We found that, unlike Gradient-DD, the latter approach does
not yield an improvement compared to GTD2 (Fig. 11 of the appendix).
With these considerations in mind, the negative gradient of JGDD(w|wn−1)is
−1
2∇JGDD(wn|wn−1)
=−E[(γxn+1−xn)x/latticetop
n][E(xnx/latticetop
n)]−1E(δnxn)−κE[(x/latticetop
nwn−x/latticetop
nwn−1)xn]. (10)
Because the terms in Eqn. (10) can be directly sampled, the stochastic gradient descent updates are given by
δn=rn+ (γxn+1−xn)/latticetopwn
ηn+1=ηn+αn(δn−x/latticetop
nηn)xn
wn+1=wn−καn(x/latticetop
nwn−x/latticetop
nwn−1)xn−αn(γxn+1−xn)(x/latticetop
nηn). (11)
Similar to the case of GTD2 for oﬀ-policy learning in (7), the modiﬁcations to the equations for updating ηn
andwnfor oﬀ-policy learning with Gradient-DD are as follows:
ηn+1=ηn+αn(ρnδn−x/latticetop
nηn)xn
wn+1=wn−καnρn(x/latticetop
nwn−x/latticetop
nwn−1)xn−αnρn(γxn+1−xn)(x/latticetop
nηn). (12)
These update equations deﬁne the Gradient-DD method, in which the GTD2 update equations (6) are
generalized by including a second-order update term in the third update equation, where this term originates
from the squared bias term in the objective (8). Since Gradient-DD is not sensitive to the step size of
updating η(see Fig. 8 in the appendix), the updates of Gradient-DD only have a single shared step size αn
rather than two step sizes αn,βnas GTD2 and TDC used. It is worth noting that the computational cost of
our algorithm is essentially the same as that of GTD2. In the following sections, we shall analytically and
numerically investigate the convergence and performance of Gradient-DD learning.
4 Convergence Analysis
In this section we establish that the asymptotic convergence guarantees of the original GTD methods also
apply to the Gradient-DD algorithm. Denote Gn=/bracketleftbigg−xnx/latticetop
n−xn(xn−γxn+1)/latticetop
(xn−γxn+1)x/latticetop
n 0/bracketrightbigg
,andHn=
/bracketleftbigg0 0
0 xnx/latticetop
n/bracketrightbigg
. We rewrite the update rules in Eqn. (11) as a single iteration in a combined parameter
vector with 2ncomponents, ρn= (η/latticetop
n,w/latticetop
n)/latticetop, and a new reward-related vector with 2ncomponents,
gn+1= (rnx/latticetop
n,0/latticetop)/latticetop, as follows:
ρn+1=ρn−καnHn(ρn−ρn−1) +αn(Gnρn+gn+1), (13)
Theorem 1. Consider the update rules (13) with step-size sequences αn. Let the TD ﬁxed point be w∗, such
thatVw∗=ΠBV w∗. Suppose that (A0) αn∈(0,1),/summationtext∞
n=1αn=∞,/summationtext∞
n=1α2
n<∞, (A1) (xn,rn,xn+1)
is an i.i.d. sequence with uniformly bounded second moments, (A2) E[(xn−γxn+1)x/latticetop
n]andE(xnx/latticetop
n)are
non-singular, (A3) supn/bardblρn+1−ρn/bardblis bounded in probability, (A4) κis a constant such that 0≤κ<∞.
Then asn→∞,wn→w∗with probability 1.
5Under review as submission to TMLR
Proof sketch. Due to the second-order diﬀerence term in Eqn. (13), the analysis framework in (Borkar &
Meyn, 2000) does not directly apply to the Gradient-DD algorithm when (A0) holdes, i.e., step size is tapered.
Likewise, the two-timescale convergence analysis (Bhatnagar et al., 2009) is also not directly applicable.
Deﬁning un+1=ρn+1−ρn, we rewrite the iterative process in Eqn. (13) into two parallel processes which
are given by
ρn+1=ρn−καnHnun+αn(Gnρn+gn+1), (14)
un+1=−καnHnun+αn(Gnρn+gn+1). (15)
We analyze the parallel processes Eqns. (14) & Eqn. (15) instead of directly analyzing Eqn. (13). Our
proofs have three steps. First we show supn/bardblρn/bardblis bounded by applying the stability of the stochastic
approximation (Borkar & Meyn, 2000) into the recursion Eqn. (14). Second, based on this result, we shall
show that ungoes to 0 in probability by analyzing the recursion Eqn. (15). At last, along with the result
thatungoes to 0 in probability, by applying the convergence of the stochastic approximation (Borkar &
Meyn, 2000) into the recursion Eqn. (14), we show that ρngoes to the TD ﬁxed point which is given by the
solution of Gρ+g= 0. The details are provided in Section A.3 of the Appendix.
Theorem 1 shows that Gradient-DD maintains convergence as GTD2 under some mild conditions. The
assumptions (A0), (A1), and (A2) are standard conditions in the convergence analysis of Gradient TD
learning algorithms (Sutton et al., 2009a;b; Maei, 2011). The assumption (A3) is weak since it means only
that the incremental update in each step is bounded in probability. The assumption (A4) requires that κis a
constant, meaning κ=O(1). Given this assumption, the contribution of the term κHnunis controlled by αn
asn→∞.
5 Empirical Study
In this section, we assess the practical utility of the Gradient-DD method in numerical experiments. To validate
performance of Gradient-DD learning, we compare Gradient-DD learning with GTD2 learning, TDC learning
(TD with gradient correction (Sutton et al., 2009b)), TDRC learning (TDC with regularized correction
(Ghiassian et al., 2020)) and TD learning in both tabular representation and linear representation. We
conductedthreetasks: asimplerandomwalktask, theBoyan-chaintask, andBaird’soﬀ-policycounterexample.
In each task, we evaluate the performance of a learning algorithm by empirical root mean-squared (RMS)
error:/radicalbig/summationtext
s∈Sds(Vwn(s)−V(s))2. The reason we choose the empirical RMS error rather than root projected
mean-squared error or other measures as Ghiassian et al. (2018; 2020) used is because it is a direct measure
of concern in practical performance.
5.1 Random walk task
As a ﬁrst test of Gradient-DD learning, we conducted a simple random walk task (Sutton & Barto, 2018).
The random walk task has a linear arrangement of mstates plus an absorbing terminal state at each end.
Thus there are m+ 2sequential states, S0,S1,···,Sm,Sm+1, wherem=10, 20, or 40. Every walk begins in
the center state. At each step, the walk moves to a neighboring state, either to the right or to the left with
equal probability. If either edge state ( S0orSm+1) is entered, the walk terminates. A walk’s outcome is
deﬁned to be r= 0atS0andr= 1atSm+1. Our aim is to learn the value of each state V(s), where the
true values are (0,1/(m+ 1),···,m/(m+ 1),1). In all cases the approximate value function is initialized to
the intermediate value V0(s) = 0.5. We consider tabular representation of the value function, as it eliminates
the impact of value function approximation. We consider that the learning rate αnis tapered according to
the schedule αn=α(103+ 1)/(103+n). We tuneα∈{10−12/4,10−11/4,···,10−1/4,1}, where larger values
were not used since the algorithms tend to diverge when αis large (Fig. 2). We obtain similar results in
the case where step sizes are constant (Fig. 6 of the appendix). For GTD2 and TDC, we set βn=ζαn
withζ∈{1/45,1/44,1/43,1/42,1/4,1,4,42,43}. For Gradient-DD, we set κ= 1. We also investigate the
sensitivity to κin Fig. 7 of the appendix, where we show that κ= 1is a good choice in the empirical studies.
To compare algorithms, we begin by plotting the empirical RMS error as a function of step size α. To assess
convergence performance, we ﬁrst plot the ﬁnal empirical RMS error, averaged over the ﬁnal 100 episodes, as
6Under review as submission to TMLR
a function of step size αin the upper panel of Fig. 2. We also plot the average empirical RMS error of all
episodes as a function of αand report these results in the upper panel of Fig. 5 of the Appendix. Note that
20,000 episodes are used. From these ﬁgures, we can make several observations. (1) Gradient-DD clearly
performs better than GTD2. This advantage is consistent in various settings, and gets bigger as the state
space becomes large. (3) Gradient-DD performs similarly to TDRC and conventional TD learning, with
a similar dependence on α, although Gradient-DD exhibits greater sensitivity to the value of αin the log
domain than these other algorithms. In addition, Gradient-DD clearly outperforms TDC when p= 20,40for
the range of ζthat we tested. However, the better comparison is to TD learning since TDC approaches to
conventional TD learning when ζgoes to 0, and TDC appears to oﬀer no advantage over TD learning in this
task. In summary, Gradient-DD exhibits clear advantages over the GTD2 algorithm, and its performance is
also as good as that of TDRC and conventional TD learning.
0.0010.0100.1001.0000.00.20.40.6
αEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
0.0010.0100.1001.0000.00.20.40.6
αEmpirical RMS error
0.0010.0100.1001.0000.00.20.40.6
αEmpirical RMS error
05000 150000.000.150.30
episodeEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
05000 150000.00.20.4
episodeEmpirical RMS error
05000 150000.00.20.4
episodeEmpirical RMS error
Figure 2: The random walk task with tabular representation and tapering step size αn=α(103+ 1)/(103+n).
Upper: Mean error from the ﬁnal 100 episodes for diﬀerent values of α. Lower: Performance over all episodes,
whereαis tuned to minimize the mean error from the ﬁnal 100 episodes. In each row, state space size 10
(left), 20 (middle), or 40 (right). The curves are averaged over 50 runs, with error bars denoting the standard
error of the mean, though most are vanishingly small.
Next we look closely at the performance during training in the lower panel of Fig. 2. For each method,
we tunedα∈{10−12/4,···,10−1/4,1}by minimizing the ﬁnal empirical RMS error, averaged over the last
100 episodes. We also compare the performance when αis tuned by minimizing the average error of all
episodes (lower panel of Fig. 5 of the appendix). From these results, we draw several observations. (1) For all
conditions tested, Gradient-DD converges much more rapidly than GTD2. The advantage of Gradient-DD
grows as the state space increases in size. (2) When evaluating performance based on ﬁnal episodes and
tuningαaccordingly (Fig. 2), Gradient-DD exhibits an evidently faster convergence rate than TDRC and
conventional TD learning, while demonstrating similar performance in terms of ﬁnal empirical RMS error.
Whenαis tuned based on the average error of all episodes (Fig. 5), Gradient-DD achieves a slightly smaller
error than TDRC and conventional TD learning, even its convergence rate is slighter faster, when the state
7Under review as submission to TMLR
space size takes 20 and 40. (3) Gradient-DD has consistent and good performance under both the constant
step size setting (Fig. 6) and under the tapered step size setting. In summary, the Gradient-DD learning
curves in this task show improvements over other gradient-based methods and performance that matches
conventional TD learning.
Like TDRC, the updates of Gradient-DD only have a single shared step size αn, i.e.,βn=αn, rather than
two independent step sizes αnandβnas in the GTD2 and TDC algorithms. A possible concern is that the
performance gains in our second-order algorithm could just as easily be obtained with existing methods by
adopting this two-timescale approach, where the value function weights are updated with a smaller step
size than the second set of weights. Hence, in addition to investigating the eﬀects of the learning rate, size
of the state space, and magnitude of the regularization parameter, we also investigate the eﬀect of using
distinct values for the two learning rates, αnandβn, although we set βn=ζαnwithζ= 1. To do this,
we setβn=ζαnfor Gradient-DD, with ζ∈{1/64,1/16,1/4,1,4}, and report the results in Fig. 8 of the
appendix. The results show that comparably good performance of Gradient-DD is obtained under these
variousζ, providing evidence that the second-order diﬀerence term in our approach provides an improvement
beyond what can be obtained with previous gradient-based methods using the two time scale approach.
5.2 Boyan-chain task
We next investigate Gradient-DD learning on the Boyan-chain problem, which is a standard task for testing
linear value-function approximation (Boyan, 2002). In this task we allow for 4p−3states, each of which is
represented by a p-dimensional feature vector, with p= 20,50,or100. Thep-dimensional representation for
every fourth state from the start is [1,0,···,0]for states1,[0,1,0,···,0]fors5,···, and [0,0,···,0,1]for
the terminal state s4p−3. The representations for the remaining states are obtained by linearly interpolating
between these. The optimal coeﬃcients of the feature vector are (−4(p−1),−4(p−2),···,0)/5. In each
state, except for the last one before the end, there are two possible actions: move forward one step or move
forward two steps, where each action occurs with probability 0.5. Both actions lead to reward -0.3. The
last state before the end just has one action of moving forward to the terminal with reward -0.2. We tune
α∈{10−2,10−1.5,10−1,10−3/4,10−1/2,10−1/4,10−1/8,1,101/8,101/4}for each method by minimizing the
average error of the ﬁnal 100 episodes. All algorithms, with the exception of TD, tend to diverge frequently
whenα≥101/2. TD also experiences frequent divergence when α≥103/4. Thus, we set the maximum value
ofαas101/4. The step size is tapered according to the schedule αn=α(2×103+ 1)/(2×103+n). For
GTD2 and TDC, we set βn=ζαnwithζ∈{1/64,1/16,1/4,1,4}. In this task, we set γ= 1. As in the
random-walk task, we set κ= 1.
We report the performance as a function of αand the performance over episodes in Fig. 3, where we tune α
by the performance based on the average error of the last 100 episodes. We also compare the performance
based on the average error of all episodes during training and report the results in Fig. 10 of the appendix.
These ﬁgures lead to conclusions similar to those already drawn in the random walk task. (1) Gradient-DD
has much faster convergence than GTD2andTDC, and generally converges to better values. (However,
similar to the random walk task, we note that TDC appears to perform worse than TD learning in this task
for nonzero values of ζ, so comparison to TD learning is more informative than comparison to TDC.) (2)
Gradient-DD is competitive with TDRCand conventional TD learning despite being somewhat slower at the
beginning episodes when αis tuned based on the average error of all episodes. (3) The improvement over
GTD2 or TDC grows as the state space becomes larger.
5.3 Baird’s oﬀ-policy counterexample
We also verify the performance of Gradient-DD on Baird’s oﬀ-policy counterexample (Baird, 1995; Sutton &
Barto, 2018), illustrated schematically in Fig. 4, for which TD learning famously diverges. We show results
from Baird’s counterexample with N= 7,20states. The reward is always zero, and the agent learns a linear
value approximation with N+ 1weightsw1,···,wN+1: the estimated value of the j-th state is 2wj+wN+1
forj≤N−1and that of the N-th state is wN+ 2wN+1. In the task, the importance sampling ratio for
the dashed action is (N−1)/N, while the ratio for the solid action is N. Thus, comparing diﬀerent state
sizes illustrates the impact of importance sampling ratios in these algorithms. The initial weight values are
8Under review as submission to TMLR
0.010.050.201.0002468
αEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
0.010.050.201.0005101520
αEmpirical RMS error
0.010.050.201.00010203040
αEmpirical RMS error
05000 150000.00.51.01.52.0
episodeEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
05000 150000246810
episodeEmpirical RMS error
05000 15000010203040
episodeEmpirical RMS error
Figure 3: The Boyan Chain task with linear approximation and tapering step size αn=α(2×103+ 1)/(2×
103+n). Upper: Performance as a function of α; Lower: performance over episodes. In each row, the feature
size is 20 (left), 50 (middle), or 100 (right). The curves are averaged over 50 runs, with error bars denoting
the standard error of the mean, though most are vanishingly small across runs.
(1,···,1,10,1)/latticetop. Constant αis used in this task and is tuned in the region {10−16/4,10−15/4,···,10−1/4,1}.
We setγ= 0.99. For TDC and GTD2, thus we tune ζ∈{4−2,4−1,1,42,43}. Meanwhile we tune αfor TDC
in a wider region {10−24/4,10−23/4,···,10−1/4,1}. For Gradient-DD, we tune κ∈{4−1,1,4}. We tuneα
separately for each algorithm by minimizing the average error from the ﬁnal 100 episodes.
Fig. 4 demonstrates that Gradient-DD works better on this counterexample than GTD2, TDC, and TDRC. It
is worthwhile to observe that when the state size is 20, TDRC become unstable, meaning serious unbalance of
importance sampling ratios may cause TDRC unstable. We also note that, because the linear approximation
leaves a residual error in the value estimation due to the projection error, the RMS errors of GTD2, TDC,
and TDRC in this task do not go to zero. In contrast to other algorithms, the errors from our Gradient-DD
converge to zero.
6 Conclusion and discussion
In this work, we have proposed Gradient-DD learning, a new gradient descent-based TD learning algorithm.
The algorithm is based on a modiﬁcation of the projected Bellman error objective function for value function
approximation by introducing a second-order diﬀerence term. The algorithm signiﬁcantly improves upon
existing methods for gradient-based TD learning, obtaining better convergence performance than conventional
linear TD learning.
Since GTD learning was originally proposed, the Gradient-TD family of algorithms has been extended to
incorporate eligibility traces and learning optimal policies (Maei & Sutton, 2010; Geist & Scherrer, 2014), as
well as for application to neural networks (Maei, 2011). Additionally, many variants of the vanilla Gradient-TD
9Under review as submission to TMLR
(a) Illustration of the extended Baird’s oﬀ-
policy counterexample. The “solid" action
usually goes to the N-th state, and the
“dashed" action usually goes to one of the
otherN−1states, each with equal probabil-
ity.
1e−06 1e−04 1e−02012345
αEmpirical RMS errorTDC
TDRC
GTD2
GDD
1e−06 1e−04 1e−0201234
αEmpirical RMS error
040008000012345
episodeEmpirical RMS errorTDC
TDRC
GTD2
GDD
040008000012345
episodeEmpirical RMS error
(b) The performance of various algorithms.
Figure 4: Baird’s oﬀ-policy counterexample. Upper in (b): Performance as a function of α; Lower in (b):
performance over episodes. From left to right in (b): 7-state and 20-state.
methods have been proposed, including HTD (Hackman, 2012) and Proximal Gradient-TD (Liu et al., 2016).
Because Gradient-DD just modiﬁes the objective error of GTD2 by considering an additional squared-bias
term, it may be extended and combined with these other methods, potentially broadening its utility for more
complicated tasks.
One potential limitation of our method is that it introduces an additional hyperparameter relative to similar
gradient-based algorithms, which increases the computational requirements for hyperparameter optimization.
This is somewhat mitigated by our ﬁnding that the algorithm’s performance is not particularly sensitive to
values ofκ, and thatκ∼1was found to be a good choice for the range of environments that we considered.
The second limitation lies in the absence of a convergence rate analysis demonstrating superior performance
compared to GTD2 in empirical studies, in addition to addressing convergence in this paper. While analyzing
asymptotic convergence rates, similar to the approach in Devraj et al. (2019), could be a viable way to assess
the proposed Gradient-DD algorithm, such analysis extends beyond the scope of this paper and is deferred
to future work. Another potential limitation is that we have focused on value function prediction in the
two simple cases of tabular representations and linear approximation. An especially interesting direction for
future study will be the application of Gradient-DD learning to tasks requiring more-complex representations,
including neural network implementations. Such approaches are especially useful in cases where state spaces
are large, and indeed we have found in our results that Gradient-DD seems to confer the greatest advantage
over other methods in such cases. Intuitively, we expect that this is because the diﬀerence between the
optimal update direction and that chosen by gradient descent becomes greater in higher-dimensional spaces
(cf. Fig. 1). This performance beneﬁt in large state spaces suggests that Gradient-DD may be of practical
use for these more challenging cases.
References
S. Amari. Natural gradient works eﬃciently in learning. Neural Computation , 10(2):251–276, 1998.
10Under review as submission to TMLR
L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proceedings of the
12 th International Conference on Machine Learning , pp. 30–37, 1995.
S. Bhatnagar, R.S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor-critic algorithm. Automatic , 73:
2471–2482, 2009.
V.S. Borkar and S.P. Meyn. The ODE method for convergence of stochastic approximation and reinforcement
learning. SIAM Journal on Control and Optimization , 38(2):447–469, 2000.
Justin A. Boyan. Technical update: least-squares temporal diﬀerence learning. Machine Learning , 49:233–246,
2002.
W. Dabney and P. Thomas. Natural temporal diﬀerence learning. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , 2014.
T. Degris, P.M. Pilarski, and R.S. Sutton. Model-free reinforcement learning with continuous action in
practice. In Proceedings of the 2012 American Control Conference , 2012.
A.M. Devraj, A. Bušić, and S. Meyn. On matrix momentum stochastic approximation and applications to
q-learning. In 57th Annual Allerton Conference on Communication, Control, and Computing , 2019.
S. S. Du, J. Chen, L. Li, L. Xiao, and D. Zhou. Stochastic variance reduction methods for policy evaluation.
InProceedings of the 34 th International Conference on Machine Learning , 2017.
M. Geist and B. Scherrer. Oﬀ-policy learning with eligibility traces: A survey. Journal of Machine Learning
Research , 15:289–333, 2014.
S. Ghiassian, A. Patterson, M. White, R.S. Sutton, and A. White. Online oﬀ-policy prediction.
arXiv:1811.02597, 2018.
S. Ghiassian, A. Patterson, S. Garg, D. Gupta, A. White, and M. White. Gradient temporal-diﬀerence
learning with regularized corrections. In International Conference on Machine Learning , 2020.
L. Hackman. Faster gradient-TD algorithms. Master’s thesis, University of Alberta, Edmonton, 2012.
B. Liu, S. Mahadevan, and J. Liu. Regularized oﬀ-policy TD-learning. In Advances in Neural Information
Processing Systems , 2012.
B. Liu, J. Liu, M. Ghavamzadeh, S. Mahadevan, and M. Petrik. Finite-sample analysis of proximal gradient
TD algorithms. In Proceedings of the 31st International Conference on Uncertainty in Artiﬁcial Intelligence ,
pp. 504–513, 2015.
B. Liu, J. Liu, M. Ghavamzadeh, S. Mahadevan, and M. Petrik. Proximal gradient temporal diﬀerence
learning algorithms. In The 25th International Conference on Artiﬁcial Intelligence (IJCAI-16), , 2016.
H.R. Maei. Gradient temporal-diﬀerence learning algorithms . PhD thesis, University of Alberta, Edmonton,
2011.
H.R. Maei and R.S. Sutton. GQ(λ): A general gradient algorithm for temporal-diﬀerence prediction learning
with eligibility traces. In Proceedings of the 3rd Conference on Artiﬁcial General Intelligence , pp. 91–96,
2010.
J. Peters, K. Mülling, and Y. Altün. Relative entropy policy search. In AAAI Conference on Artiﬁcial
Intelligence , 2010.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy
optimization. In Proceedings of the 32nd International Conference on Machine Learning , 2015.
R. S. Sutton, Cs. Szepesvári, and H. R. Maei. A convergent O(n) algorithm for oﬀ-policy temporal diﬀerence
learning with linear function approximation. In Advances in Neural Information Processing Systems 21 ,
2009a.
11Under review as submission to TMLR
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . The MIT Press, second
edition edition, 2018.
R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. Szepesvári, and E. Wiewiora. Fast gradient-
descent methods for temporal-diﬀerence learning with linear function approximation. In Proceedings of the
26th International Conference on Machine Learning , 2009b.
P. Thomas. GeNGA: A generalization of natural gradient ascent with positive and negative convergence
results. In Proceedings of the 31st International Conference on Machine Learning , 2014.
P. Thomas, B.C. Silva, C. Dann, and E. Brunskill. Energetic natural gradient descent. In Proceedings of the
33th International Conference on Machine Learning , 2016.
A. White and M. White. Investigating practical linear temporal diﬀerence learning. In International
Conference on Autonomous Agents and Multi-Agent Systems , 2016.
12Under review as submission to TMLR
A Appendix
A.1 On the equivalence of Eqns. (8) & (9)
The Karush-Kuhn-Tucker conditions of Eqn. (9) are the following system of equations


d
dwJ(w) +κd
dw(/bardblVw−Vwn−1/bardbl2
D−µ) = 0;
κ(/bardblVw−Vwn−1/bardbl2
D−µ) = 0;
/bardblVw−Vwn−1/bardbl2
D≤µ;
κ≥0.
These equations are equivalent to


d
dwJ(w) +κd
dw/bardblVw−Vwn−1/bardbl2
D= 0andκ>0,
if/bardblVw−Vwn−1/bardbl2
D=µ;
d
dwJ(w) = 0andκ= 0,if/bardblVw−Vwn−1/bardbl2
D<µ.
Thus, for any µ>0, there exists a κ≥0such thatd
dwJ(w) +µd
dw/bardblVw−Vwn−1/bardbl2
D= 0.
A.2 The relation to natural gradients
In this section, we shall show that Gradient-DD is related to, but distinct from, the natural gradient. We
thank a reviewer for pointing out the connection between Gradient-DD and the natural gradient.
Following Amari (1998) or Thomas (2014), the natural gradient of J(w)is the direction obtained by solving
the following optimization:
lim
/epsilon1→0arg min
∆J(w+/epsilon1∆)s.t./epsilon12∆/latticetopX/latticetopDX∆≤µ. (A.1)
We can note that this corresponds to the ordinary gradient in the case where the metric tensor X/latticetopDXis
proportional to the identity matrix.
Now we rewrite Eqn. (9) as
/bardblVw−Vwn−1/bardbl2
D= (w−wn−1)/latticetopX/latticetopDX(w−wn−1).
Denote/epsilon1∆ =w−wn−1, where/epsilon1is the radius of the circle of waround wn−1and∆is a unit vector. Thus,
we have
/bardblVw−Vwn−1/bardbl2
D=/epsilon12∆/latticetopX/latticetopDX∆.
For the MSPBE objective, we have
J(w) =J(wn−1+w−wn−1) =J(wn−1+/epsilon1∆).
Minimizing Eqn. (9) is equivalent to the following optimization
arg min
∆J(wn−1+/epsilon1∆)s.t./epsilon12∆/latticetopX/latticetopDX∆≤µ. (A.2)
In the limit as /epsilon1→0, the above optimization is equivalent to
arg min
∆∆/latticetop∇J(wn−1)s.t./epsilon12∆/latticetopX/latticetopDX∆≤µ.
Thus, given the metric tensor G=X/latticetopDX,−G−1∇J(wn−1)is the direction of steepest descent, i.e.the
natural gradient, of J(wn−1). The natural gradient of J(w), on the other hand, is the direction of steepest
descent at w, rather than at wn−1.
Therefore, our Gradient-DD approach makes use of the natural gradient of the objective around wn−1rather
than around wnin Eqn. (A.1). This explains the distinction of the updates of our Gradient-DD approach
from the updates of directly applying the natural gradient of the objective w.
13Under review as submission to TMLR
A.3 Proof of Theorem 1
We introduce an ODE result on stochastic approximation in the following lemma, then show the convergence
of our Gradient-DD approach in Theorem 1 by applying this result.
Lemma 1. (Theorems 2.1 & 2.2 of (Borkar & Meyn, 2000)) Consider the stochastic approximation algorithm
described by the d-dimensional recursion
yn+1=yn+an[f(yn) +Mn+1].
Suppose the following conditions hold: (c1) The sequence {αn}satisﬁes 0< αn<1,/summationtextn
n=1αn=∞,/summationtextn
n=1α2
n<∞. (c2) The function fis Lipschitz, and there exists a function f∞such that limr→∞fr(y) =
f∞(y), where the scaled function fr:Rd→Rdis given byfr(y) =f(ry)/r. (c3) The sequence {Mn,Fn}, with
Fn=σ(yi,Mi,i≤n), is a martingale diﬀerence sequence. (c4) For some c0<∞and any initial condition
y0,E(/bardblMn+1/bardbl2|Fn)≤c0(1 +/bardblyn/bardbl2). (c5) The ODE ˙y=f∞(y)has the origin as a globally asymptotically
stable equilibrium. (c6) The ODE ˙y(t) =f(y(t))has a unique globally asymptotically stable equilibrium y∗.
Then (1) under the assumptions (c1-c5), supnyn<∞in probability. (2) under the assumptions (c1-c6), as
n→∞,ynconverges to y∗with probability 1 .
Now we investigate the stochastic gradient descent updates in Eqn. (13), which is recalled as follows:
ρn+1=ρn−καnHn(ρn−ρn−1) +αn(Gnρn+gn+1). (A.3)
The iterative process in Eqn. (A.3) can be rewritten as
(ρn+1−ρn) =−καnHn(ρn−ρn−1) +αn(Gnρn+gn+1). (A.4)
Deﬁning
un+1=ρn+1−ρn.
Eqn. (A.4) becomes
un+1=−καnHnun+αn(Gnρn+gn+1).
Thus, the iterative process in Eqn. (A.3) is rewritten as two parallel processes that are given by
ρn+1=ρn−καnHnun+αn(Gnρn+gn+1), (A.5)
un+1=−καnHnun+αn(Gnρn+gn+1). (A.6)
Our proofs have three steps. First we shall show supn/bardblρn/bardblis bounded by applying the ordinary diﬀerential
equation approach of the stochastic approximation (the 1st result of Lemma 1) into the recursion Eqn. (A.5).
Second, based on this result, we shall show that ungoes to 0 in probability by analyzing the recursion
Eqn. (A.6). At last, along with the result that ungoes to 0 in probability, by applying the 2nd result of
Lemma 1 into the recursion Eqn. (A.5), we show that ρngoes to the TD ﬁxed point, which is given by the
solution of Gρ+g= 0.
First, we shall show supn/bardblρn/bardblis bounded. Eqn. (A.5) is rewritten as
ρn+1=ρn+αn(f(ρn) +Mn+1), (A.7)
wheref(ρn) = (Gρn+g)−κHunandMn+1= ((Gn−G)ρn+gn+1−g)−κ(Hn−H)un. LetFn=
σ(u0,ρ0,M0,u1,ρ1,M1,···,un,ρn,Mn)beσ-ﬁelds generated by the quantities ui,ρi,Mi,i≤n.
Now we verify the conditions (c1-c5) of Lemma 1. Condition (c1) is satisﬁed under the assumption of the
step sizes. Clearly, f(u)is Lipschitz and f∞(ρ) =Gρ, meaning Condition (c2) is satisﬁed. Condition (c3) is
satisﬁed by noting that (Mn,Fn)is a martingale diﬀerence sequence, i.e., E (Mn+1|Fn) = 0.
We next investigate E (/bardblMn+1/bardbl2|Fn). From the triangle inequality, we have that
/bardblMn+1/bardbl2≤2/bardbl(Gn−G)/bardbl2/bardblρn/bardbl2+ 2/bardblκ(Hn−H)/bardbl2/bardblun/bardbl2. (A.8)
14Under review as submission to TMLR
From Assumption A3 in Theorem 1 that /bardblun/bardblis bounded and the Assumption A1 that ( xn,rn,xn+1) is an
i.i.d. sequence with uniformly bounded second moments, there exists some constant c0such that
/bardblGn−G/bardbl2≤c0/2,and/bardblκ(Hn−H)/bardbl2/bardblun/bardbl2≤c0/2.
Thus, Condition (c4) is satisﬁed.
Note that Gis deﬁned in (Maei, 2011). From (Sutton et al., 2009a) and (Maei, 2011), the eigenvalues of
the matrix Gare strictly negative under the Assumption A2. Therefore, Condition (c5) is satisﬁed. Thus,
applying the 1st part of Lemma 1 shows that supn/bardblρn/bardblis bounded in probability.
Second, we investigate the recursion Eqn. (A.6). Let yn+1= (Gnρn+gn+1). Then
un+1=αn[−κHnun+yn+1]
=αnyn+1+αnαn−1(−κHn)yn+αnαn−1αn−2(−κHn)(−κHn−1)yn−1
+···+αnn−1/productdisplay
k=0αk(−κHk+1)y1+n/productdisplay
k=0αk(−κHk)u0. (A.9)
Note that/bardblHn/bardbl≤1/κdue to/bardblxn/bardbl≤1/κand that there exists a constant csuch that/bardblρn/bardbl≤cdue to the
above result that supn/bardblρn/bardbl<∞in probability. Without loss of generality, we assume that /bardblxn/bardbl≤1/κ.
Eqn. (A.9) implies that
/bardblun+1/bardbl≤c/parenleftBigg
αn+αnαn−1+αnαn−1αn−2+···+n/productdisplay
k=0αk/parenrightBigg
+n/productdisplay
k=0αk/bardblu0/bardbl. (A.10)
Under Assumption A0, αn→0asn→0. Based on this, Lemma 2 (given in the following section) tells us
thatαn+αnαn−1+αnαn−1αn−2+···+/producttextn
k=0αk→0asn→0. Thus, Eqn. (A.10) implies that un→0in
probability.
Finally, for applying the 2nd part of Lemma 1, we just need to verify Condition (c6). Because ungoes to 0
with probability 1, Eqn. (A.7) tells us that the associated ODE corresponds to
Gρ+g= 0.
Thus, Condition (c6) is satisﬁed. The theorem is proved.
A.4 A Lemma
Lemma 2. Denote/epsilon1n=αn+αnαn−1+···+αnαn−1···α0. Ifαn→0asn→∞, then/epsilon1n→0asn→∞.
Proof.Becauseαn→0asn→∞, there exists α∈(0,1)and some integer Nsuch thatαn≤α<1when
n≥N. Deﬁne a sequence εnsuch that
εn= 1 +αεn−1forn≥N+ 1;
εN=/epsilon1N.
Obviously,
/epsilon1n≤εn,∀n≥N. (A.11)
Now we investigate the sequence εn.
εn= 1 +αεn−1= 1 +α(1 +αεn−2) =···= 1 +α+···+αn−N−1+αn−NεN
≤/summationdisplay∞
k=0αk+αn−NεN= 1/(1−α) +αn−NεN.
Thus, we have that
sup
n≥Nεn<∞. (A.12)
15Under review as submission to TMLR
From Eqns. (A.11) & (A.12), we have
sup
n≥0/epsilon1n<∞. (A.13)
From the deﬁnition of /epsilon1n, we have that /epsilon1n=αn+αn/epsilon1n−1. It follows that
αn=/epsilon1n
1 +/epsilon1n−1≥/epsilon1n
1 + supk≥0/epsilon1k.
From the assumption αn→0asn→∞and Eqn. (A.13), we have /epsilon1n→0asn→∞.
A.5 Additional empirical results
0.0010.0100.1001.0000.00.20.40.6
αEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
0.0010.0100.1001.0000.00.20.40.6
αEmpirical RMS error
0.0010.0100.1001.0000.00.20.40.6
αEmpirical RMS error
05000 150000.000.150.30
episodeEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
05000 150000.00.20.4
episodeEmpirical RMS error
05000 150000.00.20.4
episodeEmpirical RMS error
Figure 5: The random walk task with tabular representation. The setting is similar to Fig. 2, but the
performance is evaluated by the average error of all episodes, and αis tuned by minimizing the average error
of all episodes. Upper: Performance as a function of α; Lower: performance over episodes. From left to right:
state space size 10 (left), 20 (middle), or 40 (right).
16Under review as submission to TMLR
0.0010.0100.1001.0000.00.20.40.6
αEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
05000 150000.00.20.4
episodeEmpirical RMS error
Figure 6: The random walk task with the tabular representation. The setting is similar to Fig. 2, but constant
step size is used. The state size is 20. The curves are averaged over 50 runs, with error bars denoting the
standard error of the mean, though most are vanishingly small.
0.51.02.00.000 0.004 0.008
κEmpirical RMS error
GTD2
GDD
0.51.02.00.00 0.04 0.08
κEmpirical RMS error
0.51.02.00.00.10.20.30.4
κEmpirical RMS error
Figure 7: Performance of Gradient-DD in the random walk task in the tabular representation with κ∈
{0.25,0.5,1,2,4}. From left to right: state space size 10 (left), 20 (middle), or 40 (right). In each ﬁgure, αis
tuned for each algorithm by minimizing the average error of the last 100 episodes. Results are averaged over
50 runs, with error bars denoting standard error of the mean.
0.020.100.502.000.000 0.004 0.008 0.012
ζEmpirical RMS error
GTD2
GDD
0.020.100.502.000.000.050.100.150.20
ζEmpirical RMS error
0.020.100.502.000.00.10.20.30.4
ζEmpirical RMS error
Figure 8: The random walk task in the tabular representation. Performance for various βn=ζαn, with
ζ∈{4−3,4−2,4−1,1,4}. From left to right in each row: the size of the state space is m= 10,m= 20, and
m= 40. In each case αis tuned by minimizing the average error of the last 100 episodes according to the
their performance of corresponding algorithms. Results are averaged over 50 runs, with error bars denoting
standard error of the mean.
17Under review as submission to TMLR
0.020.100.502.000.000.050.100.150.20
ζEmpirical RMS errorTDC
GTD2
GDD
NTDC
NGTD
Figure 9: Performance of natural TDC and natural GTD2 in the random walk task with the tabular
representation and m= 20. Performance for various βn=ζαn, withζ∈{4−3,4−2,4−1,1,4}. In each case
αis tuned by minimizing the average error of the last 100 episodes according to the their performance of
corresponding algorithms. “NGTD" and “NTDC" denote the natural gradient-based algorithm of GTD2 and
TDC, respectively. Results are averaged over 50 runs, with error bars denoting standard error of the mean.
0.010.050.201.0002468
αEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
0.010.050.201.0005101520
αEmpirical RMS error
0.010.050.201.00010203040
αEmpirical RMS error
05000 150000.00.51.01.52.0
episodeEmpirical RMS errorTD
TDC
TDRC
GTD2
GDD
05000 150000246810
episodeEmpirical RMS error
05000 15000010203040
episodeEmpirical RMS error
Figure 10: The Boyan Chain task with linear approximation. The setting is similar to Fig. 3, but we evaluate
the performance the average error of all episodes, and αis tuned by minimizing the average error of all
episodes. Upper: Performance as a function of α; Lower: performance over episodes.
18Under review as submission to TMLR
1e−08 1e−04 1e+0005101520
αEmpirical RMS errorGTD2
GDD
0.05
0.2
0.5
2
Figure 11: Results from the Boyan-chain task with a feature size of 50 when regularizing the objective in
the parameter space, unlike Gradient-DD, which regularizes in the space of the value function. The various
shades of gray represent diﬀerent values of the regularization parameter κin this regularization scheme.
19