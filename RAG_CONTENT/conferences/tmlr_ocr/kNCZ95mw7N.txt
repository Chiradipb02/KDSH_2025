Published in Transactions on Machine Learning Research (02/2024)
A VAE-based Framework for Learning
Multi-Level Neural Granger-Causal Connectivity
Jiahe Lin
Machine Learning Research, Morgan Stanley
Huitian Lei
Lyft, Inc.
George Michailidis⋆
Department of Statistics and Data Science
University of California, Los Angeles
Reviewed on OpenReview: https://openreview.net/forum?id=kNCZ95mw7N
Abstract
Grangercausalityhasbeenwidelyusedinvariousapplicationdomainstocapturelead-lagre-
lationships amongst the components of complex dynamical systems, and the focus in extant
literature has been on a single dynamical system. In certain applications in macroeconomics
and neuroscience, one has access to data from a collection of relatedsuch systems, wherein
the modeling task of interest is to extract the shared common structure that is embedded
acrossthem, aswellastoidentifytheidiosyncrasieswithinindividualones. Thispaperintro-
duces a Variational Autoencoder (VAE) based framework that jointlylearns Granger-causal
relationships amongst components in a collection of related-yet-heterogeneous dynamical
systems, and handles the aforementioned task in a principled way. The performance of
the proposed framework is evaluated on several synthetic data settings and benchmarked
against existing approaches designed for individual system learning. The method is further
illustrated on a real dataset involving time series data from a neurophysiological experiment
and produces interpretable results.
1 Introduction
The concept of Granger causality introduced in Granger (1969) leverages the temporal ordering of time series
data. It is defined in terms of predictability of future values of a time series; namely, whether the inclusion
of past information (lag values) of other time series as well as its own (self lags) leads to a reduction in the
variance of the prediction error of the time series under consideration. Since its introduction, it has become
a widely-used approach in the analysis of economic (Stock & Watson, 2001), financial (Hong et al., 2009)
and neuroimaging (Seth et al., 2015) time series data. The standard setting in these applications is that one
is interested in estimating Granger causal relationships in a dynamical system (e.g., a national economy, a
brain) comprising of pvariables.
Granger causality can also be expressed through the language of graphical models (Dahlhaus & Eichler,
2003; Eichler, 2012). The node set of the graph corresponds to the pvariables at different time points;
directed edges between nodes at past time points to those at the present time capture Granger causal
relationships (for more details and a pictorial illustration see Section C.1). Traditionally, Granger causality
was operationalized through linear vector autoregressive (VAR) models (Granger, 1969), in which case the
entries of the estimated transition matrices correspond precisely to the edges of the Granger causal graph.
⋆Corresponding Author. 〈gmichail@ucla.edu 〉
1Published in Transactions on Machine Learning Research (02/2024)
More recent work has explored how Granger causal relationships can be learned through nonlinear models;
e.g., see review paper Shojaie & Fox (2022) and references therein.
In certain application domains, one has access to data from a collection of relateddynamical systems. A
motivating example is described next. Consider electroencephalography (EEG) recordings obtained from p
electrodes placed on the scalp of a subject (e.g., a patient or an animal). The resulting time series data
constitute measurements from a complex neurophysiological dynamical system (Stam, 2005). On many
instances, one has access to such measurements for a collection of Mrelatedsubjects (or “entities”, equiva-
lently); for example, they may be performing the same cognitive task (e.g., visual counting, geometric figure
rotation) or exhibit a similar neurological disorder (e.g., epilepsy, insomnia, dementia). In such a setting, one
can always opt to perform separate analyses on each subject’s data; however, it would be useful to develop
methodology that models the data from all subjects jointly, so as to simultaneously extract the embedded
structure shared across subjects and identify the idiosyncrasies (heterogeneity) in any single one. In other
words, if one views all subjects as belonging to a common group, the quantities of interest are the shared
group-level connectivity structure (amongst nodes) and the entity-level ones.
Conceptually, the above-mentioned modeling task is not difficult to accomplish in a linear setting where one
can decompose the transition matrices into a “shared” component and an idiosyncratic (entity-specific) one,
with some orthogonality-type constraint to enforce identifiability of the parameters (more details provided
in Section C.3). However, the task becomes more challenging and involved in non-linear settings where one
hopes to use flexible models to capture the underlying complex dynamics. In particular, a decomposition-
based approach, which requires the exact specification of the functional form of the shared component or how
the shared and the idiosyncratic components interact, would be rather restrictive. To this end, we adopt a
generative model-based approach, which circumvents the issue by encoding the Granger causal relationships
through graphs. By postulating a model with a hierarchical structure between the shared and entity-specific
components, the problem can be addressed in a flexible, yet principled manner.
Summary of contributions. Wedevelopatwo-layerVariationalAutoencoder(VAE)basedframeworkfor
estimating Granger-causal connections amongst nodes in a collection of related dynamical systems — jointly
for the common group-level and the entity-level ones — in the presence of entity-specific heterogeneity. De-
pending on the assumed connection type (continuous or binary) amongst the nodes, the proposed framework
can accommodate the scenario accordingly by imposing a commensurate structure on the encoded/decoded
distributions, leveraging conjugacy between pairs of distributions. The proposed model enables extracting
the embedded common structure in a principled way, without resorting to any ad-hoc or post-hoc aggrega-
tion. Finally, the framework can be generalized to the case where multiple levels of nested groups are present
and provides estimates of the group-level connectivity for all levels of groups.
The remainder of the paper is organized as follows. In Section 2, we provide a review of related literature on
Granger-causality estimation, with an emphasis on neural network-based methods. The main building block
used in the proposed framework, namely, a multi-layer VAE is also briefly introduced. Section 3 describes in
detailtheproposedframework, includingtheencoder/decodermodulesandthetraining/inferenceprocedure.
In Section 4, model performance is assessed on synthetic datasets and benchmarked against several existing
methods. An application to a real dataset involving EEG signals from 22 subjects is discussed in Section 5.
Finally, Section 6 concludes the paper.
2 Related Work and Preliminaries
In this section, we review related work on inferring Granger causality based on time series data, with an
emphasis on deep neural network-based approaches. Further, as the proposed framework relies on variational
autoencoders (VAE) with a hierarchical structure, we also briefly review VAEs in the presence of multiple
latent layers.
2Published in Transactions on Machine Learning Research (02/2024)
2.1 Inference of Granger causality
Linear VAR models have historically been the most popular approach for identifying Granger causal rela-
tionships. Within the linear setting, hypothesis testing frameworks with theoretical guarantees have been
developed (Granger, 1980; Geweke, 1984), while more recently regularized approaches have enabled the es-
timation in the high-dimensional setting (Basu et al., 2015). Recent advances in neural network techniques
have facilitated capturing non-linear dynamics and identifying Granger causality accordingly, as discussed
next.
Note that estimation of Granger causality is an unsupervised task, in the sense that the connectivity as
captured by the underlying graph is not observed and thus cannot serve as the supervised learning target.
Depending on the model family that the associated estimation procedure falls into, existing approaches
suitable for estimating Granger causality based on neural networks (Montalto et al., 2015; Nauta et al.,
2019; Wu et al., 2020; Khanna & Tan, 2020; Tank et al., 2021; Marcinkevičs & Vogt, 2021; Löwe et al., 2022)
can be broadly categorized into prediction-based and generative model-based ones. We selectively review
some of them next. In the remainder of this subsection, we use xi,tto denote the value of node iat time
t,xt:= (x1,t,···,xp,t)the collection of node values of the dynamical system, and x:={x1,···,xT}the
trajectory over time.
Within the predictive modeling framework, recent representative works include Khanna & Tan (2020); Tank
et al. (2021); Marcinkevičs & Vogt (2021), where the Granger-causal relationship is inferred from coefficients
that govern the dynamics of the time series, and the coefficients are learned by formulating prediction tasks
that can be generically represented as xt=f(xt−1,...,xt−q) +εt, with xt∈Rpbeing the multivariate
time series signal and εtthe noise term. In Tank et al. (2021), coordinates of the response are considered
separately, that is, xi,t=fi(xt−1,...,xt−q)+εi,t, andfiis parameterized using either multi-layer perceptrons
(MLP) or LSTM (Hochreiter & Schmidhuber, 1997). In the case of an L-layer MLP,
/hatwidexi,t=WLhL−1
t+bL;hl
t=σ/parenleftig
Wlhl−1
t+bl/parenrightig
, l= 2,···,L;h1
t=σ/parenleftig/summationdisplayq
k=1W1kxt−k+b1/parenrightig
;
the Granger-causal connection from the jth node to the ith node is then encoded in some “summary” (e.g.,
Frobenius norm) of {W11
:j,···,W1q
:j}, with each component corresponding to the first hidden layer weight of
lagsxj,t−1,···,xj,t−q. Various regularization schemes are considered and incorporated as penalty terms in
the loss function, to encourage sparsity and facilitate the identification of Granger-causal connections. The
case of LSTM-based parameterization is handled analogously. Marcinkevičs & Vogt (2021) parameterizes
fas an additive function of the lags, i.e., xt=/summationtextq
k=1Ψk(xt−k)xt−k+εt; the output of Ψk:Rp∝⇕⊣√∫⊔≀→Rp×p
contains the generalized coefficients of xt−k, whose (i,j)entry corresponds to the impact of xj,t−kon
xi,tandΨkis parameterized through MLPs. The Granger causal connection between the jth node and
theith node is obtained by aggregating information from the coefficients of all lags {Ψk(xt−k)ij}, i.e.,
max 1≤k≤q{medianq+1≤t≤T(|Ψk(xt−k)ij|)}. Finally, an additional stability-based procedure where the model
is fit to the time series in the reverse order is performed for the final selection of the connections.1It
is worth noting that for both of the above-reviewed approaches, the ultimately desired node jto nodei
(∀i,j∈{1,···,p}) Granger causal relationship is depicted by a scalar value, whereas in the modeling
stage, such a connection is collectively captured by multiple “intermediate” quantities— {W1k
:j,k= 1,···q}
in Tank et al. (2021) and {Ψk(xt−k)ij,k= 1,···,q}in Marcinkevičs & Vogt (2021); hence, an information
aggregation step becomes necessary to summarize the above-mentioned quantities to a single scalar value.
For generative model-based approaches, the starting point is slightly different. Notable ones include Löwe
et al. (2022) that builds upon Kipf et al. (2018), and the focus is on relational inference . The postulated
generative model assumes that the trajectories are collectively governed by an underlying latent graph z,
which effectively encodes Granger-causal connections:
p(x|z) =p({xT+1,···,x1}|z) =/productdisplayT
t=1p(xt+1|xt,···,x1,z).
1This stability-based step amounts to finding an optimal thresholding level for the “final” connections: the same model is fit
to the time series in the reverse order, and “agreement” is sought between the Granger causal connections obtained respectively
based on the original and the reverse time series, over a sequence of thresholding levels; the optimal one is determined by the
one that maximizes the agreement measure. We refer interested readers to the original paper and references therein for more
details.
3Published in Transactions on Machine Learning Research (02/2024)
Specifically, in their setting, xi,t∈Rdis vector-valued and zijcorresponds to a categorical “edge type”
between nodes iandj. For example, it can be a binary edge type indicating presence/absence, or a more
complex one having more categories. To simultaneously learn the edge types and the temporal dynam-
ics, the model is formalized through a VAE that maximizes the evidence lower bound (ELBO), given by
Eqϕ(z|x)(logpθ(x|z))−KL(qϕ(z|x)/vextenddouble/vextenddoublepθ(z)), whereqϕ(z|x)is the probabilistic encoder, pθ(x|z)the decoder, and
pθ(z)the prior distribution. Concretely, the probabilistic encoder is given by qϕ(z|x) =softmax/parenleftbig
fenc,ϕ(x)/parenrightbig
and it infers the type for each entry of z; the function fenc,ϕis parameterized by neural networks. The de-
coderpθ(x|z) =/producttextT
t=1pθ(xt+1|z,xτ:τ≤t)projects the trajectory based on past values and z—specifically,
the distributional parameters for each step forward. For example, if a Gaussian distribution is assumed, in
the Markovian case, pθ(xt+1|xt,z) =N(mean,variance ), where mean =f1
dec,θ(xt,z),variance =f2
dec,θ(xt,z)
andf1
dec,θ,f2
dec,θare parameterized by some neural networks. Finally, maximizing the ELBO loss can be
alternatively done by minimizing
−Eqϕ(z|x)(logpθ(x|z)) + KL(qϕ(z|x)/vextenddouble/vextenddoublepθ(z)) :=negative log-likelikehood +H/parenleftbig
qϕ(z|x)/parenrightbig
+const ;
the negative log-likelihood corresponds to the reconstruction error of the entiretrajectory coming out of the
decoder, and the KL divergence term boils down to the sum of entropies denoted by H(·)if the prior pθ(z)
is assumed to be a uniform distribution over edge types.
In summary, at the formulation level, generative model-based approaches treat Granger-causal connections
(relationships) as a latent graph and learn it jointly with the dynamics, whereas predictive ones extract
Granger-causal connections from the parameters that govern the dynamics in a post-hoc manner. The
former can readily accommodate vector-valued nodes whereas for the latter, it becomes more involved and
further complicates how the connections can be extracted/represented based on the model parameters. At
the task level, to learn the model parameters, prediction-based approaches rely on tasks where the predicted
values of the future one-step-ahead timestamp are of interest, whereas generative approaches amount to
reconstructing the observed trajectories; prediction and reconstruction errors constitute part of the empirical
risk minimization loss and the ELBO loss, respectively.
2.2 Multi-layer variational autoencoders
With a slight abuse of notation, in this subsection, we use xto denote the observed variable and zl,l=
1,···,Lthe latent ones for Llayers.
A “shallow” VAE with one latent layer is considered in the seminal work of Kingma & Welling (2014), where
the generative model is given by pθ(x,z1) =pθ(x|z1)pθ(z1), withpθ(z1)denoting the prior distribution.
Later works (Kingma et al., 2014; Burda et al., 2016; Sønderby et al., 2016) consider the extension into
multiple latent layers, where the generative model can be represented through a cascading structure as
follows:
pθ(x,{zl}L
l=1) =pθ(x|z1)/parenleftig/productdisplayL−1
l=1pθ(zl|zl+1)/parenrightig
pθ(zL);
the corresponding inference model (encoder) is given by qϕ(z1,···,zL|x) =qϕ(z1|x)/producttextL
i=1qϕ(zl|zl−1). The
variational lower bound on logp(x)can be written as
Eqϕ({z}L
l=1|x)/parenleftig
logpθ(x|{z}L
l=1)/parenrightig
−KL/parenleftig
qϕ({z}L
l=1|x)/vextenddouble/vextenddoublepθ({z}L
l=1)/parenrightig
, (1)
with the first term corresponding to the reconstruction error.
Conjugacy adjustment. Under the above multi-layer setting, Sønderby et al. (2016) considers an infer-
ence model that recursively merges information from the “bottom-up” encoding and “top-down” decoding
steps. Concretely, inthecasewhereeachlayerisspecifiedbyaGaussiandistribution, theoriginaldistribution
at layerlafter encoding is given by qϕ(zl|zl−1)∼N(µq,l,σ2
q,l)and the distribution at the same layer after de-
coding is given by pθ(zl|zl+1)∼N(µp,l,σ2
p,l). The adjustment amounts to a precision-weighted combination
thatcombinesinformationfromthedecoderdistributionintotheencoderone, thatis, qϕ(zl|·)∼N/parenleftbig
˜µq,l,˜σ2
q,l/parenrightbig
,
where ˜µq,l= (µq,lσ−2
q,l+µp,lσ−2
p,l)/(σ−2
q,l+σ−2
p,l)and˜σ2
q,l= 1/(σ−2
q,l+σ−2
p,l). This information-sharing mechanism
4Published in Transactions on Machine Learning Research (02/2024)
leads to richer latent representations and improved approximation of the log-likelihood function. A similar
objective is also considered in Burda et al. (2016) and operationalized through importance weighting.
It is worth noting that the precision-weighted adjustment in Sønderby et al. (2016) is in the spirit of the
conjugate analysis in Bayesian statistics. In particular, in Bayesian settings where the data likelihood is
assumed Gaussian with a fixed variance parameter and the prior distribution is also assumed Gaussian, the
posterior distribution possesses a closed-form Gaussian distribution (and hence conjugate w.r.t. the prior)2.
For this reason, we term such an adjustment as the “conjugacy adjustment”, which will be used later in our
technical development.
Finally, utilizing multiple layers possessing a hierarchy as discussed above resembles the framework adopted
in Bayesian hierarchical modeling. We provide a brief review of the topic in Section C.2. We also sketch in
Section C.3 a modeling formulation under this framework for collection of linear VARs.
3 The Proposed Framework
Given a collection of trajectories for the same set of pvariables (nodes) from Mdynamical systems (entities),
we are interested in estimating the Granger causal connections amongst the nodes in each system (i.e., entity-
level connections), as well as the common “backbone” connections amongst the nodes that are shared across
the entities (i.e., group-level connections).
To this end, we propose a two-layer VAE-based framework, wherein Granger-causal connections are treated
as latent variables with a hierarchical structure, and they are learned jointly with the dynamics of the
trajectories. In Section 3.1, we present the posited generative process that is suitable for the modeling
task of interest, and give an overview of the proposed VAE-based formulation; the details of the components
involvedandtheirexactmodelingconsiderationsarediscussedinSection3.2. Section3.3providesasummary
of the end-to-end training process and the inference tasks that can be performed based on the trained model.
The generalization of the proposed framework to the case of multiple levels of grouping across entities is
deferred to Appendix F, where the grandcommon and the groupcommon structures can be simultaneously
learned with those of the entities.
3.1 An overview of the formulation
¯z
z[1]z[2]z[M−1]z[M]
x[1]x[2]x[M−1]x[M]pθ⋆(z[m]|¯z)
pθ⋆(x[m]|z[m])
Figure 1: Diagram for the postulated top-down
generative process.Consider a setting where there are Mentities, each of them having
the same set of pnodes, that evolve as a dynamical system. Let
x[m]
i,tdenote the value of node iof entitym∈{1,···,M}at time
t. It can be either scalar or vector-valued, with scalar node values
being prevalent in traditional time-series settings; in the latter case,
the nodes can be thought of as being characterized by vector-valued
“features”3. Let x[m]
t:= (x[m]
1,t,···,x[m]
p,t)be the collection of node
values at time tfor entitym, and x[m]:={x[m]
1,···,x[m]
T}the corre-
sponding trajectory over time. Further, let z[m]∈Rp×pdenote the
Granger-causal connection matrix of entity mand¯z:= [¯zij]∈Rp×p
the common structure embedded in z[1],···,z[M], and note that it
doesnotnecessarily correspond to the arithmetic mean of the z[m]’s.
In the remainder of this paper, we may refer to these matrices as
“graphs” interchangeably.
Depending on the modeling scenario, the entity-level Granger-causal
connections z[m]can either be binary or continuous. In the former case, it corresponds precisely to the
2Note that in the case where the data likelihood is Gaussian, but the variance is no longer a fixed parameter, the Normal-
Normal conjugacy does not necessarily go through.
3For example, in the Springs experiment in Kipf et al. (2018) (which is also considered in this paper; see Appendix B.1), the
features correspond to a 4-dimensional vector, with the first two coordinates being the 2D velocity and the last two being the
2D location
5Published in Transactions on Machine Learning Research (02/2024)
aggregate Granger causal graph defined in Dahlhaus & Eichler (2003); in the latter case, its (i,j)-entry
(scalar value) reflects the strength of the relationship between the past value(s) of node jthe present value
of nodei; see Appendix C.1 and Remark 6 for a detailed discussion.
The posited generative process, whose true parameters are denoted by θ⋆, is given by:
pθ⋆/parenleftig
{x[m]}M
m=1,{z[m]}M
m=1,¯z/parenrightig
=pθ⋆/parenleftig
{x[m]}M
m=1|{z[m]}M
m=1/parenrightig
·pθ⋆/parenleftig
{z[m]}M
m=1|¯z/parenrightig
·pθ⋆(¯z)
=/productdisplayM
m=1pθ⋆(x[m]|z[m])/productdisplayM
m=1pθ⋆(z[m]|¯z)/productdisplay
1≤i,j≤ppθ⋆(¯zij).(2)
The decomposition is based on the following underlying assumptions (see also Figure 1 for a pictorial illus-
tration):
•conditional on the entity-specific graphs z[m], their trajectories x[m]’s areindependent of the grand
common ¯z, and they are conditionally independent from each other given their respective entity-
specific graphs z[m]’s
•the entity-specific graphs z[m]are conditionally independent given the common graph ¯z
•the prior distribution pθ⋆(¯z)factorizes over the edges.
The proposed model creates a hierarchy between the common graph and the entity-specific ones, which in
turn naturally provides a coupling mechanism amongst the latter. The grand common structure can be
estimated as one learns all the latent components jointly with the dynamics of the system through a VAE.
LetX:={x[1],···,x[m]},Z:={¯z,z[1],···,z[m]},qϕ(Z|X)denote the encoder, pθ(X|Z)the decoder and
pθ(Z)the prior distribution. Then, the ELBO is given by
Eqϕ(Z|X)/parenleftig
logpθ(X|Z)/parenrightig
−KL/parenleftig
qϕ(Z|X)/vextenddouble/vextenddoublepθ(Z)/parenrightig
,
and serves as the objective function for the end-to-end encoding-decoding procedure as depicted in Figure 2.
{x[m]} {z[m]}|{x[m]}
sampled ¯zpθ(¯z)
{z[m]}|· {ˆx[m]}qϕ(z[m]|xm)
qϕ(¯z|{¯zm})
pθ({z[m]}|¯z)
pθ({x[m]}|{z[m]})(merge info)
(merge info)(observed)
encoding
decoding
(reconstructed)(prior)
Figure 2: Diagram for the end-to-end encoding-decoding procedure. Solid paths with arrows denote modeling the corresponding
distributions during the encoding/decoding process; dashed paths with arrows correspond to information merging based on (weighted)
conjugacy adjustment. Quantities obtained after each step are given inside the circles/rectangles. {x[m]}is short for the collection
{x[m]}M
m=1;{z[m]}is analogously defined.
Remark 1 (On the proposed formulation) .(1) Depending on the assumption on the entity-level Granger-
causal connections z[m]—either binary or continuous—encoder/decoder distributions can then be selected
accordingly. Inparticular,distributionsthatformconjugatepairs(e.g.,Gaussian-Gaussianforthecontinuous
case and Beta-Bernoulli for the binary case) can facilitate computations. (2) The proposed framework
naturally allows estimation of positive/negative connections in a principled way without resorting to ad-hoc
aggregationschemes. Italsoenablesincorporationofexternalinformationpertainingtothepresence/absence
of connections through the decoder. (3) In settings where a large collection of entities is available, but each
entity has limited sample size, the joint learning framework can be advantageous over an individual entity
learning one.
6Published in Transactions on Machine Learning Research (02/2024)
3.2 Modeling details
Next, we provide details on the specification of the encoder and the decoder, the sampling steps, and the
loss function calculations for model (2).
3.2.1 Encoder
Thegoaloftheencoderistoinferthelatentgraphs Z:={¯z,z[1],···,z[M]}basedontheobservedtrajectories
X:={x[1],···,x[M]}.
Letϕdenote the collection of parameters in the encoder qϕ(Z|X). To delineate the dependency between the
trajectories and the graphs, the following assumptions are imposed:
•conditioning on {z[m]}M
m=1,¯zis independent of {x[m]}M
m=1and the conditional probability
qϕ/parenleftbig
¯z|{z[m]}M
m=1/parenrightbig
factorizes across edges (i,j);
•the entity-specific graphs are conditionally independent given their corresponding trajectories, i.e.,
qϕ/parenleftbig
{z[m]}M
m=1|{x[m]}M
m=1/parenrightbig
factorizes across entities.
These assumptions are in line with the structure of the model in (2), in that the conditional dependencies
posited in the generative model are respected during the “bottom-up” encoding process.
Consequently, the encoder can be decomposed into the following product components:
qϕ/parenleftbig
Z/vextendsingle/vextendsingleX/parenrightbig
=qϕ/parenleftig
¯ z/vextendsingle/vextendsingle{z[m]}M
m=1/parenrightigM/productdisplay
m=1qϕ/parenleftig
z[m]/vextendsingle/vextendsinglex[m]/parenrightig
=/productdisplay
1≤i,j≤pqϕ/parenleftig
¯zij|{z[m]
ij}M
m=1/parenrightigM/productdisplay
m=1qϕ/parenleftig
z[m]/vextendsingle/vextendsinglex[m]/parenrightig
.
Therearetwotypesoftermsintheaboveexpression: qϕ(z[m]|x[m])thatinferseachentity’slatentgraphbased
on its trajectory, and qϕ(¯zij|{z[m]
ij}M
m=1)that obtains the grand common based on the entity-level graphs,
in an edge-wise manner. Note that for qϕ(¯zij|{z[m]
ij}M
m=1), together with modeling pθ(z[m]
ij|¯zij), resembles
prior-posterior calculations in Bayesian statistics using conjugate pairs of distributions; hence, depending on
the underlying structural assumptions (continuous or binary) on the z[m]’s, one can choose emission heads
(or equivalently, the output functional form) accordingly.
At the high level, the encoder can be abstracted into 3 modules, parameterized through fx→h,fh→zand
fz→¯z, respectively:
(enc-a) trajectory to hidden representation x[m]→h[m]:=fx→h(x[m]), with h[m]
ijcorresponding to the
edge-specific one;
(enc-b) hidden representation to the entity-specific graph: h[m]→z[m]:=fh→z(h[m]);
(enc-c) entity-level graphs to the grand common (edge-wise): {z[m]
ij}M
m=1→¯zij:=fz→¯z({z[m]
ij}M
m=1).
Modules (enc-a) and (enc-b) combined, model qϕ(z[m]|x[m])and correspond to “Trajectory2Graph” opera-
tions, while module (enc-c) models qϕ(¯zij|{z[m]
ij}M
m=1)and captures the “Entity2Common” one. On the other
hand, given the above-mentioned conjugate pair consideration, the choices of fh→zandfz→¯zare considered
jointly.
Formally,for fx→h,weuseasimilarapproachtothatinKipfetal.(2018),where fx→hentailsmessage-passing
operations that are widely adopted in the literature related to graph neural networks (Scarselli et al., 2008;
Gilmer et al., 2017). At a high level, these operations entail “node2edge” (concatenating the representation
of the node stubs) and “edge2node” (aggregating the representation of incoming edges) iteratively and non-
linear functions (e.g., MLPs) in between. The operation ultimately leads to {h[m]
ij}, with h[m]
ij∈Rnhidbeing
anhid-dimensional hidden representation corresponding to z[m]
ij. Full details are provided in Appendix A.1
wherein we also provide a pictorial illustration for the operations.
Once the h[m]
ij’s are obtained, subsequent modeling in modules (enc-b) and (enc-c) can be generically repre-
sented as
z[m]
ij|h[m]
ij∼qz(·;δ[m]
q,ij),and ¯zij|{z[m]
ij}∼q¯z(·;¯δij),
7Published in Transactions on Machine Learning Research (02/2024)
whereqz(·;δ[m]
q,ij)is some distribution with parameter δ[m]
q,ij:=fh→z(h[m]
ij)being the function output of fh→z.
Similarly,q¯z(·;¯δq,ij)is some distribution with parameter ¯δq,ij:=fz→¯z({z[m]
ij})being the function output of
fz→¯z. The exact choices for fh→zandfz→¯zbifurcate depending on the scenario:
•Case 1, z[m]’s entries being continuous: in this case, we consider a Gaussian-Gaussian emission head
pair. Consequently, δ[m]
q,ij={µ[m]
q,ij,(σ[m])2
q,ij},¯δq,ij={¯µq,ij,¯σ2
q,ij};
qz∼N/parenleftig
µ[m]
q,ij,(σ[m])2
q,ij/parenrightig
;µ[m]
q,ij:=f1
h→z(h[m]
ij),(σ[m])2
q,ij:=f2
h→z(h[m]
ij); (3)
q¯z∼N/parenleftig
¯µq,ij,¯σ2
q,ij/parenrightig
; ¯µq,ij:=f1
z→¯z({z[m]
ij}),¯σ2
q,ij:=f2
¯z→z({z[m]
ij}). (4)
f1
h→z,f2
h→zare component functions of fh→z, each with an nhid-dimensional input and a scalar
output; they can be simple linear functions with f2
h→zhaving an additional softplus operation to
ensure positivity. Similarly, f1
z→¯z,f2
z→¯zcomprisefz→¯z, each with an m-dimensional input and a
scalar output; in practice their functional form can be as simple as taking the sample mean and
standard deviation, respectively.
•Case 2, z[m]’s entries being binary: in this case, we consider a Beta-Bernoulli emission head pair,
i.e.,
qz∼Ber/parenleftig
δ[m]
q,ij/parenrightig
;δ[m]
q,ij:=fh→z(h[m]
ij), (5)
q¯z∼Beta/parenleftig
¯αq,ij,¯βq,ij/parenrightig
; ¯αq,ij:=f1
z→¯z({z[m]
ij}),¯βq,ij:=f2
z→¯z({z[m]
ij}). (6)
The output of fh→zcorresponds to the Bernoulli success probability and it is parameterized with
an MLP with the last layer performing sigmoid activation to ensure that the output lies in (0,1).
f1
z→¯zandf2
z→¯zare component functions of fz→¯z. Similar to the Gaussian case, their choice need
not be complicated and is chosen based on moment-matching.
Note that the prior distribution pθ(¯zij)is also selected according to the underlying scenario, with a standard
Normal distribution used in the continuous case and a Beta(1,1)in the binary case. Once the distribution
parameters for ¯zijare obtained based on (4) or (6), we apply conjugacy adjustment to incorporate also the
information from the prior, before the sampling step takes place.
3.2.2 Decoder
The goal of the decoder pθ(X|Z)is to reconstruct the trajectories based on the entity and group level graphs,
and its components follow from the generative process described in (2), that is,
pθ(X|Z) =pθ/parenleftig
{x[m]}M
m=1|{z[m]}M
m=1/parenrightig
·pθ/parenleftig
{z[m]}M
m=1|¯z/parenrightig
=/productdisplayM
m=1pθ(x[m]|z[m])/productdisplayM
m=1pθ(z[m]|¯z),
whereθdenotes the collections of parameters in the decoder. The two components pθ(z[m]|¯z)and
pθ(x[m]|z[m]), respectively capture the dependency between the entity-specific graphs z[m]’s and their grand
common ¯z, and the evolution of the trajectories given z[m]. Consequently, the decoder can be broken into
two modules, parameterized through g¯z→zandgz→x:
(dec-a)pθ(z[m]|¯z), the grand common to entity-specific graphs z→z[m]:=g¯z→z(¯z), withg¯z→z(·)acting on
the sampled ¯z(edge-wise). Samples drawn from this distribution will be used to guide the evolution
of the trajectories of the corresponding entity;
(dec-b)pθ(x[m]|z[m]), graph to trajectory z[m]→xm; concretely,
pθ(x[m]|z[m]) =pθ(x[m]
1|z[m])/productdisplayT
t=2pθ/parenleftig
x[m]
t|x[m]
t−1,...,x[m]
1,z[m]/parenrightig
,
withpθ(x[m]
t|x[m]
t−1,...,x[m]
1,z[m])modeled through gz→x(x[m]
t−1,···,x[m]
t−q,z[m])assuming a fixed con-
text length of q(orq-lag dependency, equivalently).
We refer to these two modules as “Common2Entity” and “Graph2Trajectory”, respectively.
8Published in Transactions on Machine Learning Research (02/2024)
Common2Entity. We consider a weighted conjugacy adjustment that merges the information from the
encoder distribution into the decoder one, so that it contains both the grand common and the entity-specific
information. Concretely, for some pre-specified weight ω∈[0,1],
•Case 1, in the continuous case, let pθ(z[m]
ij|¯zij)∼N (µ[m]
p,ij,(σ[m])2
p,ij)withµ[m]
p,ij:=g1
¯z→z(¯z[m]
ij)and
(σ[m])2
p,ij:=g2
¯z→z(¯z[m]
ij);g1
¯z→z,g2
¯z→z:R∝⇕⊣√∫⊔≀→Rare component functions of g¯z→z. This gives the “un-
adjusted” distribution that contains only the grand common information. With µ[m]
q,ijand(σ[m])2
q,ij
obtained in (3), the weighted adjustment gives pθ(z[m]
ij|·)∼N/parenleftig
˜µ[m]
p,ij,(˜σ[m])2
p,ij/parenrightig
, where
˜µ[m]
p,ij:=ωµ[m]
q,ij(σ[m])−2
q,ij+ (1−ω)µ[m]
p,ij(σ[m])−2
p,ij
ω(σ[m])−2
q,ij+ (1−ω)(σ[m])−2
p,ij,(˜σ[m])2
p,ij:=1
ω(σ[m])−2
q,ij+ (1−ω)(σ[m])−2
p,ij.(7)
•Case 2, in the binary case, let pθ(z[m]
ij|¯zij)∼Ber(δ[m]
p,ij), whereδ[m]
p,ij:=g¯z→z(¯zij). Withδ[m]
q,ijobtained
in (5), the weighted adjustment gives
pθ(z[m]
ij|·)∼Ber/parenleftig
˜δ[m]
p,ij/parenrightig
;˜δ[m]
p,ij=1
ω/δ[m]
q,ij+ (1−ω)/δ[m]
p,ij. (8)
Similar to the function fz→¯zin the encoder, here g¯z→zcorresponds to fz→¯z’s “reverse-direction” counterpart
and its choice can be rather simple4.
Remark 2 (On the role of ω).It governs the mixing percentage of the entity-specific and the common
information: when ω= 1, the “tilde” parameters of the post-adjustment distribution effectively collapse into
the encoder ones (e.g., ˜δp,ij≡δ[m]
q,ijand analogously for ˜µp,ij,˜σ2
p,ij); correspondingly, samples drawn from
pθ(z[m]
ij|·)essentially ignore the sampled ¯zand hence they can be viewed as entirely entity-specific. At the
other extreme, for ω= 0, the tilde parameters coincide with the unadjusted ones; therefore, apart from the
grand common information carried in the sampled ¯z, no entity-specific one is passed onto the sampled z[m].
By varying ωbetween (0,1), one effectively controls the level of heterogeneity and how strongly the sampled
entity-specific graphs deviate from the grand common one.
Graph2Trajectory. Module (dec-b) pertains to modeling the dynamics of the trajectory x[m]given
the sampled z[m]. Here, we focus on one-step Markovian dependency, i.e., q= 1and thus
pθ(x[m]
t|x[m]
t−1,...,x[m]
1,z[m])≈gz→x(x[m]
t−1,z[m]). The extension to longer lag dependencies ( q > 1) can
be readily obtained by pre-processing the input accordingly, as discussed in Appendix A.2.
We consider the following parameterization of gz→x. At the high level, given that z[m]
ijcorresponds to the
Granger-causal connection from node jto nodei, it should serve as a “gate” controlling the amount of
information that can be passed from x[m]
j,t−1tox[m]
i,t. To this end, each response coordinate x[m]
i,tis modeled
as follows:
u[m],j
i,t−1:= ˇx[m]
j,t−1◦z[m]
ij(gating ),u[m]
i,t−1={u[m],1
i,t−1,···,u[m],p
i,t−1},and ˇu[m]
i,t−1:=MLP (u[m]
i,t−1); (9)
x[m]
i,t∼N/parenleftbig
µ[m]
x,it,(σ[m])2
x,it/parenrightbig
,whereµ[m]
x,it:=Linear (ˇu[m]
i,t−1),(σ[m])2
x,it=Softplus/parenleftbig
Linear (ˇu[m]
i,t−1)/parenrightbig
.(10)
Note that in the gating operation in (9), we use ˇx[m]
j,t−1to denote the output after some potential numerical
embedding step (e.g., Gorishniy et al. (2022)) of x[m]
j,t−1; in the absence of such embedding, ˇx[m]
j,t−1≡x[m]
j,t−1.
Through the gating step5,x[m]
j,t−1exerts its impact on x[m]
i,tentirely through u[m],j
i,t−1. The continuous case and
the binary case z[m]
ijcan be treated in a unified manner: in the former case, the value of z[m]
ijcorresponds to
the strength; in the latter case, it performs masking. Subsequently, u[m]
i,t−1collects the u[m],j
i,t−1’s of all nodes
j= 1,···,p, and serves as the predictor for x[m]
i,t. Finally, if one simply sums all u[m],j
i,t−1’s to obtain the mean
ofx[m]
i,t, then it effectively coincides with the operation in a linear VAR system, with z[m]
ijcorresponding
precisely to the entries in the transition matrix.
4In our experiments, we use an identity function and it has been effective across the settings considered.
5Note that z[m]
ijis a scalar and is applied to all coordinates of ˇx[m]
j,t−1in the case the latter is a vector.
9Published in Transactions on Machine Learning Research (02/2024)
Remark 3.The above-mentioned choice of gz→xcan be viewed as a “node-centric” one, wherein entries z[m]
ij
control the information passing directly through the nodes. As an alternative, one can consider an “edge-
centric” one, which leverages the idea of message-passing in GNNs and entails “node2edge” and “edge2node”
operations. This resembles the technology adopted in Kipf et al. (2018); Löwe et al. (2022) that consider
primarily having graph entries corresponding to categorical edge types, which, after some adaptation, can
be used to handle the numerical case. In practice, we observe that the edge-centric graph2trajectory decoder
can lead to instability for time series signals6. A more detailed comparison can be found in Appendix A.2,
where additional illustrations are provided for the two.
3.2.3 Sampling
Given the stochastic nature of the sampled quantities, drawing samples from the encoded/decoded distri-
butions requires special handling to enable the gradient to back propagate. Depending on whether entries
ofz[m]are continuous or binary, there are three possible types of distributions involved; for notational
simplicity, here we use zto represent generically the random variable under consideration.
•Normalz∼N (µ,σ2). In this case, the “standard” reparameterization trick (Kingma & Welling,
2014) can be used, that is, z=µ+σ◦ϵ,ϵ∼N(0,1).
•Bernoulliz∼Ber(δ). In this case, the discrete distribution is approximated by its continuous
relaxation (Maddison et al., 2017). Concretely, z=softmax ((log(π) +ϵ)/τ)whereϵ∈R2whose
coordinates are i.i.d. samples from Gumbel (0,1),π= (1−δ,δ)is the binary class probability and
τis the temperature.
•Betaz∼Beta(α,β). In this case, implicit reparameterization of the gradients (Figurnov et al., 2018)
is leveraged and the construction of the reparameterized samples becomes much more involved. We
refer interested readers to Figurnov et al. (2018); Jankowiak & Obermeyer (2018) for an in-depth
discussion on how parameterized random variables can be obtained and become differentiable.
3.2.4 Loss function
The loss function is given by the negative ELBO, that is,7
−Eqϕ(Z|X)/parenleftig
logpθ(X|Z)/parenrightig
+ KL/parenleftig
qϕ(Z|X)/vextenddouble/vextenddoublepθ(Z)/parenrightig
=:reconstruction error +KL;
the first term corresponds to the reconstruction error that measures the deviation between the original
trajectories and the reconstructed ones, while the KL term measures the “consistency” between the encoded
and the decoded distributions, and can be viewed as a type of regularization.
Letµ[m]
x,t:= (µ[m]
x,1t,···,µ[m]
x,pt)⊤andΣx[m]
t:=diag((σ[m])2
x,1t,···,(σ[m])2
x,pt)⊤with the components defined
in (10). The reconstruction error is the negative Gaussian log-likelihood loss given by
M/summationdisplay
m=1/parenleftigT/summationdisplay
t=2/parenleftbig
x[m]
t−µ[m]
x,t/parenrightbig⊤Σ−1
x[m]
t/parenleftbig
x[m]
t−µ[m]
x,t/parenrightbig
+ log|Σx[m]
t|/parenrightig
. (11)
The KL term can be simplified after some algebra to (see Appendix A.3 calculation):
Eqϕ(Z|X)/bracketleftig
KL/parenleftig
qϕ(¯z|{z[m]})/vextenddouble/vextenddoublepθ(¯z)/parenrightig/bracketrightig
+Eqϕ(Z|X)/bracketleftig
KL/parenleftig
qϕ({z[m]}|{x[m]})/vextenddouble/vextenddoublepθ({z[m]}|¯z)/parenrightig/bracketrightig
;(12)
both terms can be viewed as “consistency matching” terms that measure the divergence between the distribu-
tions obtained in the encoder pass and that from the decoder pass. Finally, note that in the implementation,
the quantities involved are replaced by their conjugacy adjusted counterparts wherever applicable, and this
is similar to the treatment in Sønderby et al. (2016).
6to contrast with the physical system (e.g., Springs) considered in the experiments of Kipf et al. (2018).
7Recall thatX:={x[m];m= 1,···, M}andZ:={¯z,z[m];m= 1,···, M}.
10Published in Transactions on Machine Learning Research (02/2024)
3.3 Training and inference
The functions in the encoder ( fx→h,fh→zandfz→¯z) and those in the decoder ( g¯z→zandgz→x) are shared
across all entities m= 1,···,M, and thus the model is trained based on the “pooled” data of all entities,
while keeping track of the entity id that each data block is associated with. The steps involved in the
end-to-end training under the proposed framework are summarized in Exhibit 1.
Exhibit 1: Outline of steps for training under the two-layer VAE-based framework
Input:observed trajectories {x[1],···,x[M]}, hyperparameters. Let ⟨M⟩:={1,···,M}.
– Forward pass, encoder: {x[m]}→{ z[m]}→¯z
0.[Traj2Graph] m∈⟨M⟩: obtain the encoded distribution for entity-specific graphs qϕ(z[m]|x[m]);
1.m∈⟨M⟩: sample z[m]fromqϕ(z[m]|x[m]);
2.[Entity2Common] based on{z[m]}M
m=1, obtain the encoded distribution for the common graph
qϕ(¯z|{z[m]});
– Forward pass, decoder: ¯z→{z[m]}→{ xm}
3. merge prior info pθ(¯z)intoqϕ(¯z|{z[m]})then sample ¯z;
4.[Common2Entity] m∈⟨M⟩: obtain the decoded distribution for entity-specific graphs pθ(z[m]|¯z);
5.m∈⟨M⟩: merge entity-specific encoded info qϕ(z[m]|x[m])intopθ(z[m]|¯z), then sample (z[m]|·);
6.[Graph2Traj] m∈⟨M⟩: using z[m]and the lag info x[m]
t−1, decode to get ˆx[m]
t;t= 2,···,T.
– Loss calculation
7. calculate the ELBO loss by summing up (11) and (12);
– Backward pass : update neural network parameters based on gradients (back-propagation)
Output: Trained encoder and decoder
Several pertinent remarks follow. (1) The data typically consist of “long” trajectories that contain all
the available observations (time points); one needs to partition them to “short” ones of length T(that
are typically between 20-50), which constitute the samples used in model training. See Appendix A.5 for
additional illustration. (2) In the case where one has external information regarding presence or absence of
edges in the z[m]’s, it can be incorporated by enforcing the corresponding entries to zero after the former are
sampled in Step 5. (3) Once the encoder (inference model) and the decoder (generative model) are trained,
the latent graphs can be obtained by applying the trained encoder on the trajectories. For entity-specific
graphs z[m]’s, the inference model gives the encoded distribution qϕ(z[m]|x[m])’s. In practice, the graph
of interest is extracted by calculating the “mode” of the distribution; the grand common graph ¯zcan be
analogouslyhandled. Itisworthnotingthatforcontinuous z[m]’s, theproposedframeworknaturallyprovides
signed estimates and thus positive/negative Granger causal connections can be readily differentiated (see
Appendix E for a detailed discussion). (4) The trained decoder can be utilized to quantify also the predictive
strength of the Granger-causal connection, as discussed in Appendix A.4.
4 Synthetic Data Experiments
We evaluate the performance of the proposed framework, together with benchmarking methods on several
synthetic data settings. For all experiments, we start from a common graph that corresponds to ¯z, add
perturbations to it for individual entities to produce heterogeneous Granger-causal connections (i.e., the
z[m]’s), then simulate trajectories {x[m]}corresponding to each entity based on their respective z[m]’s and
the specified dynamics. The estimated entity-specific and grand common graphs are then evaluated against
the underlying truth, for both the proposed and competing methods.
Prediction model-based competitors8include NGC(Tank et al., 2021), GVAR(Marcinkevičs & Vogt, 2021)
and TCDF(Nauta et al., 2019), and a regularized linear VAR model based estimator ( Linear; e.g., Basu &
Michailidis (2015)). For generative model-based ones, we consider variations of Löwe et al. (2022). Note
8The selection of these competitors is based on the results reported in Marcinkevičs & Vogt (2021). Specifically, we picked
the ones that were demonstrated to be competitive. The code implementations for these competitors (except for the regularized
Linear VAR) are directly taken from the repositories accompanying the papers.
11Published in Transactions on Machine Learning Research (02/2024)
that the original paper and the accompanying code implementation only handles the case where each entry
in the latent graph is a categorical variable denoting the “edge type”. Consequently, we adapt the method
and make necessary modifications to the code, so that it can handle numerical values9. Besides using the
edge-centric graph2trajectory decoder adopted in Kipf et al. (2018); Löwe et al. (2022), we also consider
another variant based on the proposed node-centric one. These two benchmarks are referred to as One-edge
and One-node . Note that none of the above-mentioned methods readily handles the multi-entity setting
where all graphs are estimated jointly; hence, for comparison purposes, the estimated grand common graph
for the competitors is simply obtained by averaging the estimated entity ones.
4.1 Data generating mechanisms
The data generating mechanisms used are based on: (1) a linear VAR, (2) a non-linear VAR, and (3) multi-
species Lotka-Volterra systems. Two additional mechanisms corresponding to the Lorenz96 and the Springs
systems are also considered; their description and results are presented in Appendix B. Consistent with
extant notation, pdenotes the number of nodes and Mthe number of entities.
Linear VAR. The dynamics of a linear VAR(1) model are determined by xt=Axt−1+εt,xt∈Rp,
whereinA∈Rp×pis the transition matrix and coincides with the Granger-causal graph; for notational
convenience, let ¯A:=¯zdenote the grand common and A[m]:=z[m]the entity-specific graphs. For this
mechanism, we set p= 30andM= 20, while the noise term εthas i.i.d entries drawn from a standard
Gaussian distribution.
We first discuss the generation of the “initial” common graph ¯A(0), whose skeletonS¯A(0)(i.e., support set) is
determined by independent draws from Ber(0.1); nonzero entries are first drawn from Unif(−2,−1)∪(1,2),
then scaled so that the spectral radius (i.e., the maximum in absolute value eigenvalue) of ¯A(0)is 0.5. Next,
we generate perturbations of ¯A(0)by “relocating” 10% of the entries (denote their index set by Sptrb) inS¯A(0)
to random locations in the non-support set Sc
¯A(0). This step generates the corresponding A[m]’s. Note that
the perturbation mechanism ensures that Sptrb⊂S ¯A(0). Further, the positions of the 10% of entries selected
at random remain fixed for all Mentities, and only the “new” locations are randomly selected and hence
differ across the entities, thus inducing heterogeneity across the A[m]’s. As a result of the perturbation, for
¯A(0), entries inSptrbare essentially “flipped” to zero, and this gives rise to the final grand common graph
¯A; see also Figure 3a.
Non-Linear VAR. For this mechanism, we set p= 20andM= 10. We first describe how ¯zandz[m]
are generated, as they dictate the connections and determine how the dynamics are specified. First, let ¯z(0)
be the “initial” common graph, set to a banded matrix that has non-zero entries on the diagonal and the
adjacent upper and lower diagonals. Next, we perturb ¯z(0)as follows: for all rows not divisible by 3 (e.g.,
rows, 1, 2, 4, etc.), the two off-diagonal entries are relocated to other positions at random within the same
row. This is repeated for all m’s to generate z[m]’s. The perturbation creates a zigzag pattern for the final
¯z, since whenever a perturbation is present, the original off-diagonal entries on the ±1band are guaranteed
to get flipped to zero – see Figure 3b for an illustration. Within any entity m, response nodes indexed by
i= 2,···,p−1have 3 parents; denote their indices by k1
i<k2
i<k3
iwith subscript icorresponding to the
response node id and superscript the parent id, and k2
i≡iby construction.
The trajectories are generated as follows. For i= 2,···,p−1, letxi,t= 0.25xi,t−1+ sin(xk1
i,t−1·xk3
i,t−1) +
cos(xk1
i,t−1+xk3
i,t−1) +εi,t,εi,t∼N(0,0.25). For the first node iand the last node p, their dynamics are
slightly different given that they only have one “neighbor”10. The choice of such dynamics (in particular,
using sine/cosine functions) is somewhat ad-hoc, but aim to induce non-linearities, while ensuring that the
system is stable given that these functions are uniformly bounded. Finally, note that we omit the superscript
[m]that indexes the entities, as the dynamic specification applies to the dynamical systems of all entities;
the parent set for each response node iof entitymis dictated by row iofz[m].
9see Appendix A.2 for how the adaptation can be conducted.
10Fori= 1, the dynamics is given by x1,t= 0.4x1,t−1−0.5x2,t−1+ε1,t; for i=p, the dynamics is given by xp,t=
0.4xp,t−1−0.5xp−1,t−1+εp,t
12Published in Transactions on Machine Learning Research (02/2024)
Multi-species Lotka-Volterra system. It comprises of coupled ordinary different equations (ODE) that
model the population dynamics of multiple predators and preys based on their interactions, specified by the
corresponding Granger causal graphs. We consider p= 20andM= 10. Thepnodes are separated
equally into preys and predators (i.e.,p
2preys and predators each). Let xt:= (u⊤
t,v⊤
t)⊤with ut:=
(u1,t,···,up/2,t)⊤∈Rp/2andvt:= (v1,t,···,vp/2,t)⊤∈Rp/2denoting the population size of the preys
and the predators at time t, respectively; ui:={ui,t}corresponds to the continuous-time trajectory for the
ith coordinate and vjis analogously defined. The dynamics for each coordinate are specified through the
following ODE system:
dui
dt=αui−βui(/summationdisplay
j∈Pivj)−α(ui/η)2;dvj
dt=δvj(/summationdisplay
i∈Pjui)−γvj.(13)
The parameters are set to α= 1.1,β= 0.2,γ= 1.1,δ= 0.2andη= 200. Once again, we omit superscript
[m]as this specification applies to all m= 1,···,M. The heterogeneity at the entity level is contingent on
their graphs z[m]’s that dictate the coupling mechanism; in particular, PiandPjare the parent set of nodes
iandj, and are respectively dictated by the support set of the ith andjth rows of the corresponding z[m].
The generation mechanism of ¯zandz[m]are described next. The common graph ¯zis generated identically
to the one considered in Marcinkevičs & Vogt (2021), where the 20 nodes can be separated into 5 decoupled
systems, each containing 2 predators and 2 preys. We add random perturbations to ¯zto arrive at the
z[m]’s, by adding additional entries. These additional entries in the upper right/lower left blocks need to be
symmetric w.r.t. the diagonal so that the predator-prey correspondence is respected, and they also provide
coupling across the originally decoupled 5×4systems – see also Figure 3c for an illustration.
4.2 Performance evaluation
For all settings, we consider sample sizes of 10K. We run 5 data replicates and report the mean and standard
deviation of the AUROC and AUPRC metrics for the competing methods considered. Given that the
underlying true Granger-causal graphs in the examined settings are sparse, we also report the best attainable
F1 score for each method after thresholding the entries of the group and entity-specific graphs. Results for
two other experimental settings, —the Lorenz96 and the Springs systems—, are presented in Appendix B.1.
Additional metrics such as true positive rate (TPR), true negative rate (TNR) and accuracy (ACC) based on
different thresholding levels are deferred to Appendix B.2, together with visual illustrations of the estimates
obtained by good performing competitors.
Table 1 displays the results for all methods. The proposed framework is referred to as Multi-node and Multi-
edge, corresponding to the multi-entity joint learning approaches using the node- and edge-centric decoders,
respectively; a visualization of the estimated ¯zandz[1],z[2]for illustration purposes is provided in Figure 3
for the former.
The main findings are as follows: (1) the proposed joint-learning approach clearly outperforms its individual
learning counterpart (e.g., Multi-node vs. One-node), both at the entity level and the group level (i.e.,
the common graph). (2) The node-centric decoder consistently outperforms its edge-centric counterpart
(e.g., Multi-node vs.Multi-edge ). (3) If one focuses only on individual learning methods, the ones based
on prediction models tend to exhibit superior performance (e.g., GVAR/NGCvs. One-node ). In addition,
despite the presence of non-linear dynamics, the regularized linear VAR model exhibits surprisingly good
performance, especially for the common structure. (4) For practical purposes, post-hoc averaging of the
entity-specific Granger causal graphs is reasonably effective for extracting the common structure.
Remark 4 (On the robustness with respect to sample size) .The proposed joint-learning framework is ad-
equately robust to sample sizes. In particular, in the case where the training sample size reduces to 3000,
Multi-node shows little degradation in its performance in recovering ¯z(within 1% across all settings in
AUROC), and its performance degradation in recovering the entity-level z[m]’s are within 2% for the same
metric. On the other hand, One-node shows a material deterioration in performance especially for the esti-
mated ¯z(as large as 5% for more challenging settings such as the Lotka-Volterra system), although at the
individual entity level, the deterioration is of a smaller magnitude at around 3%. In Appendix B.4 additional
comments on the “minimum sample size required” are provided from a practitioner’s perspective.
13Published in Transactions on Machine Learning Research (02/2024)
(a) Linear VAR; p= 30,M= 20.
(b) Non-linear VAR; p= 20,M= 10. Note that as the non-linearity is induced via sinusoidal functions, we do not know the true sign of the cross lead-lag
dependency; as such, the entries corresponding to edges that are present are colored in black.
(c) Multi-species Lotka-Volterra; p= 20,M= 10.
Figure 3: True (shaded panel on the left) and estimated (non-shaded panel on the right) Granger-causal connections using the proposed
framework with node-centric decoder (Multi-node); from left to right: ¯z,z[1]andz[2]and their estimated counterparts.Nonzero entries
inz[1],z[2](and/hatwidez[1],/hatwidez[2], resp.) that overlap with those in ¯z(/hatwide¯z) have been grayed-out so that the idiosyncratic ones stand out.
Table 1: Performance evaluation for the estimated ¯zandz[m]’s: “common” corresponds to ¯zand “entity(avg)” the z[m]’s after averaging
the performance metric across m= 1,···,M. Numbers are in % and rounded to integers, and correspond to the mean results based
on 5 data replicates; standard deviations are reported in the parenthesis.
Generative model-based Prediction model-based
Multi-node Multi-edge One-node One-edge NGC-cMLP GVAR TCDF Linear
Linear VAR
common AUROC 100(0.0) 100(0.0) 95(6.6) 98(4.8) 100(0.4) 100(0.0) 79(2.0) 100(0.0)
AUPRC 100(0.0) 100(0.0) 83(20.4) 91(15.9) 99(1.3) 100(0.0) 50(7.6) 100(0.0)
F1(best) 100(0.0) 100(0.0) 81(17.4) 88(15.9) 96(3.5) 100(0.0) 52(5.1) 100(0.0)
entity AUROC 100(0.1) 99(0.6) 100(0.1) 100(0.1) 96(1.8) 100(0.0) 77(1.4) 100(0.0)
(avg) AUPRC 99(0.3) 95(2.4) 99(0.2) 98(0.4) 86(4.4) 99(0.1) 36(5.5) 100(0.0)
F1(best) 97(0.8) 90(3.5) 96(0.6) 95(1.0) 79(4.7) 99(0.4) 44(3.4) 100(0.0)
Non-linear VAR
common AUROC 99(0.2) 82(1.7) 97(0.2) 93(0.8) 90(0.7) 99(0.1) 75(1.0) 99(0.1)
AUPRC 96(0.9) 58(1.1) 80(0.8) 80(8.0) 64(1.1) 98(0.2) 53(0.5) 98(0.1)
F1(best) 94(0.6) 60(0.7) 74(1.0) 83(6.9) 61(0.9) 98(0.7) 56(1.2) 98(0.7)
entity AUROC 98(0.3) 85(0.9) 94(0.4) 95(0.5) 94(0.5) 99(0.3) 73(0.9) 96(0.7)
(avg) AUPRC 93(1.0) 75(0.8) 76(0.2) 89(0.6) 87(0.6) 96(0.6) 44(1.8) 96(0.7)
F1(best) 86(1.5) 73(1.0) 70(0.3) 86(0.8) 82(0.4) 91(0.8) 50(1.5) 97(0.6)
Lotka-Volterra
common AUROC 100(0.0) 100(0.0) 97(1.1) 87(8.4) 100(0.0) 100(0.0) 79(0.8) 100(0.1)
AUPRC 100(0.0) 100(0.1) 92(3.0) 73(10.5) 100(0.0) 100(0.0) 58(1.2) 100(0.4)
F1(best) 100(0.7) 99(0.8) 87(5.4) 69(9.0) 100(0.4) 97(1.2) 53(1.4) 94(3.5)
entity AUROC 89(1.0) 84(1.3) 83(1.6) 75(1.3) 92(1.0) 93(0.6) 72(0.8) 77(1.0)
(avg) AUPRC 80(1.5) 70(2.0) 69(1.8) 51(2.6) 87(1.2) 89(1.0) 41(1.0) 71(1.2)
F1(best) 74(1.4) 65(2.0) 63(1.4) 53(2.2) 84(0.8) 84(0.7) 46(0.3) 71(0.7)
14Published in Transactions on Machine Learning Research (02/2024)
Finally, we remark that GVARexhibits consistently strong performance amongst the methods under consid-
eration. On the other hand, it is observed during evaluation time that given the magnitude of the estimated
entries, the quality of the graph skeleton is sensitive to the exact choice of the thresholding level, whereas
the proposed framework is more robust. This has implications on the difficulty of choosing a good threshold
in practice — see also Table 4 and additional discussion and remarks in Appendix B.2.
5 Application to a Multi-Subject EEG Dataset
The dataset in consideration corresponds to electroencephalogram (EEG) measurements obtained from 72
active electrodes placed on the scalp of 22 subjects (entities), and they are publicly available; see Trujillo
et al. (2017). Prior investigation on this dataset primarily centers around understanding the information
provided by different connectivity measures that are available in the literature, rather than the connectivity
patterns themselves.
The EEG experiment pertains to a stimulus procedure performed on the subjects comprising of 1-min
interleaved sessions with eyes open (EO) or closed (EC). Such experiments aim to provide insights into the
brain’s functional segregation and integration (Barry et al., 2007; Rubinov & Sporns, 2010; Miraglia et al.,
2016). Note that (1) the experiment is integrated, but the data are collated separately for the eyes-open and
the eyes-closed interleaving sessions, which results in two datasets (EO and EC, respectively); and (2) due
to the design of the experiment, the dynamics governing the data within the EO sessions (respectively, EC
sessions) are stable and stay largely unchanged.
We select to analyze the data from 31 specific EEG channels (and hence p= 31) located at the back of
the scalp (see Figure 4), where the primary visual cortex is located. For both datasets, we restrict the
analysis to entities that have at least 40000 observations (total number of time points)11, and the whole
trajectory is further partitioned into training/validation data, with the latter having 2000 time points. Here
the validation data is used to select the best hyperparameters such that the reconstruction or prediction
error is minimum over the search grid, depending on the method. Four methods are considered, including the
proposed joint-learning one with a node-centric decoder ( Multi-node ), its individual-learning counterpart
(One-node ) and prediction model-based GVARandNGC.
(a) Eyes Open (EO)
 (b) Eyes Closed (EC)
Figure 4: Multi-node results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normal-
ization and subsequent thresholding at 0.15. Red edges correspond to positive connections and blue edges correspond to negative ones;
the transparency of the edges is proportional to the strength of the connection. Larger node sizes correspond to higher in-degree
(incoming connectivity), and the top 6 nodes are colored in gray.
The estimated common Granger-causal connections based on Multi-node andGVARare depicted in Figures 4
and 5, respectively. The results based on One-node andNCGare delegated to Appendix D12. For all methods,
we threshold the raw estimates to remove very small entries; the thresholding values are chosen so that each
11This restriction has reduced the number of entities to 21 for the EO dataset while the number of entities for the EC dataset
remains at 22.
12Note that the Granger causal connections estimated by Multi-node and One-node are up to a “complete sign flip” (see,
e.g., discussion in Appendix E); nonetheless, these methods are effective in distinguishing positive (negative) connections from
negative (positive) ones. Further, NCGdoes not provide signed estimates (positive/negative) of the Granger causal connections,
unlike the other three methods.
15Published in Transactions on Machine Learning Research (02/2024)
(a) Eyes Open (EO)
 (b) Eyes Closed (EC)
Figure 5: GVARresults: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization
and subsequent thresholding at 0.05. Red edges correspond to positive connections and blue edges correspond to negative ones; the
transparency ofthe edgesis proportionalto the strengthof theconnection. Larger node sizes correspond to higher in-degree (incoming
connectivity), and the top 6 nodes are colored in gray.
method has around 400 total number of edges for the EC session to facilitate comparisons across them.
Results from these methods exhibit commonalities and differences, as discussed next.
The following common observations are noted across most methods: (1) based on the results by Multi-node ,
GVARand One-node , the overall Granger causal connectivity level is markedly higher for the EC session
compared to the EO one; this is consistent with results in studies in the literature (Barry et al., 2007; Marx
et al., 2004; Das et al., 2016; Trujillo et al., 2017), albeit using different connectivity measures. On the other
hand, results from NCGshow the reverse pattern, i.e., higher connectivity for the EO session compared to
the EC session. (2) For both the EO and EC sessions, the in-degree of nodes in the mid-line channels (i.e.
OZ, POZ, PZ, CPZ) tends to be higher than that of the nodes to the left and right parts of the brain. This
is broadly comparable to results in the literature—see, e.g., Barry et al. (2007) for adult subjects and Barry
et al. (2009) for children, though the problem under consideration and thus the analysis is different in their
work. (3) As it is observed in Multi-node ,One-Node , and NGC, the OZ channel exhibits different degree of
connectivity for the EO and the EC sessions; in particular, it is Granger causal for many other channels in
the former, i.e., being the emitter of edges and exhibit higher node out-degree; this becomes significantly
less so in the latter—see also Hatton et al. (2023).
The four methods also exhibit certain discrepancies in their results. (1) As mentioned above, the EO session
exhibits an overall decrease in connectivity when compared against the EC session. The drop in connectivity,
however, isnotuniformacrossnodesontheleftandtherightpartsofthebrain. Thisisreportedbygenerative
model-based methods Multi-node andOne-node , and a result also mentioned in the literature (Barry et al.,
2007; 2009; Modarres et al., 2023). Such discrepancy—in terms of the differential change in connectivity level
between the nodes on the left and those on the right—is significantly less pronounced in GVARandNGC. (2)
A strong bi-directional Granger causal link between channels M1 and M2 in both the EO and EC sessions is
observed according to GVAR. This strong connection is somewhat harder to interpret, since these two channels
correspond to mastoid (behind the ears) locations and their connectivity is customarily modulated through
the midline positioned ones (OZ, PZ, POZ, CPZ) (Das et al., 2022). (3) As a minor remark, for GVAR, we
observe strong autoregressive connections (i.e., dominant diagonals in the estimates)13; for NGC, the overall
connectivity level in the raw estimates is significantly higher and thus requires stronger thresholding. Both
observations are also noted in selected synthetic data experiments.
In summary, all methods with the exception of NGCare in agreement regarding the decrease in Granger
causal connections from the EC to the EO session. There is also concordance across methods regarding
the observation that this decrease is not uniform across the left and right parts of the brain. Both of these
results are in accordance to previous ones in the literature, although based on different analysis techniques
and connectivity measures.
13this is not shown in the plot (to avoid self-loops) for aesthetic purposes. Note also that visually, the edges are overall more
“faint” in the plot, as a result of the dominant diagonals and the corresponding normalization.
16Published in Transactions on Machine Learning Research (02/2024)
6 Discussion
This paper proposes a multi-layer VAE-based framework for jointly estimating the group and entity-level
Granger-causal graphs, in the presence of connectivity heterogeneity across entities. The framework is based
on a hierarchical generative structure that couples the group and entity-specific graphs. The model is learned
via an end-to-end encoding-decoding procedure that minimizes the negative ELBO loss. The results of the
numerical experiments show that the performance of the proposed framework is broadly robust to sample
size, especially for the common graph. Further, the joint learning paradigm has a clear advantage over its
“individual learning” generative model-based counterpart, which then leads to more accurate quantification
for both the common connectivity patterns and the idiosyncratic ones. This advantage becomes more
pronouncedinsettingswhereonehaslimitedsamplesizeandlargecollectionsofrelatedsystems. Inaddition,
the joint learning paradigm can be useful in situations, where one may be interested in detecting “outlier”
dynamical systems in the collection under consideration, or in identifying clusters of such systems. These
tasks can be accomplished by close examination and analysis of the entity specific graphs.
Although “prediction models plus post-hoc aggregation” heuristics can sometimes exhibit competitive per-
formance, the embedded common structure across entities is completely neglected at the formulation level.
In addition, existing models within this framework are also limited to scalar-valued nodes, partly due to
their reliance on performing ad-hoc extraction/aggregation on intermediate quantities (e.g., neural network
weight matrices during training) to infer the Granger causality.
In the presence of non-linearity, a key advantage of generative model-based approaches is that the Granger-
causal relationships are solely encoded through the latent graph that serves as the gateway for information
propagation. This provides a clean way to model relationships between connectivity patterns — either stat-
ically or dynamically. The setting considered in this work is a static one, and the type of such relationship
manifests as common-idiosyncratic connectivity patterns. A potential extension to the generative process
under consideration, suitable for more complex real-world dynamical systems, is to allow for time-varying
connectivity patterns. For example, Graber & Schwing (2020) extends the work in Kipf et al. (2018) to a
dynamic setting. With appropriate modifications to the proposed approach, such as expanding the condi-
tional relationship of the graphs dictated in (2) so that they also depend on their past, this modeling task
can be handled in a straightforward manner.
Code and Data Availability
The code repository is available at https://github.com/georgemichailidis/vae-multi-level-neural-GC-official .
The multi-subject EEG dataset is available at https://dataverse.tdl.org/dataverse/rsed2017 , as provided by
Trujillo et al. (2017).
Acknowledgements
The authors thank the Action Editor and three anonymous reviewers for their careful review of the work,
and their constructive comments and suggestions.
George Michailidis was supported in part by NSF grants DMS 2348640 and DMS 2334735.
17Published in Transactions on Machine Learning Research (02/2024)
References
Barry, R. J., Clarke, A. R., Johnstone, S. J., Magee, C. A., and Rushby, J. A. EEG differences between
eyes-closed and eyes-open resting conditions. Clinical Neurophysiology , 118(12):2765–2773, 2007.
Barry, R. J., Clarke, A. R., Johnstone, S. J., and Brown, C. R. EEG differences in children between
eyes-closed and eyes-open resting conditions. Clinical Neurophysiology , 120(10):1806–1811, 2009.
Basu, S. and Michailidis, G. Regularized estimation in sparse high-dimensional time series models. The
Annals of Statistics , 43(4):1535–1567, 2015. doi: 10.1214/15-AOS1315.
Basu, S., Shojaie, A., and Michailidis, G. Network Granger causality with inherent grouping structure. The
Journal of Machine Learning Research , 16(1):417–453, 2015.
Berliner, L. M. Hierarchical Bayesian time series models. In Maximum Entropy and Bayesian Methods:
Santa Fe, New Mexico, USA, 1995 Proceedings of the Fifteenth International Workshop on Maximum
Entropy and Bayesian Methods , pp. 15–22. Springer, 1996.
Burda, Y., Grosse, R., and Salakhutdinov, R. Importance weighted autoencoders. In International Confer-
ence on Learning Representations , 2016.
Dahlhaus, R. and Eichler, M. Causality and graphical models in time series analysis. Oxford Statistical
Science Series , pp. 115–137, 2003.
Das, A., Mandel, A., Shitara, H., Popa, T., Horovitz, S. G., Hallett, M., and Thirugnanasambandam, N.
Evaluating interhemispheric connectivity during midline object recognition using EEG. PloS One , 17(8):
e0270949, 2022.
Das, R., Maiorana, E., and Campisi, P. EEG biometrics using visual stimuli: A longitudinal study. IEEE
Signal Processing Letters , 23(3):341–345, 2016.
Eichler, M. Graphical modelling of multivariate time series. Probability Theory and Related Fields , 153:
233–268, 2012.
Erdős, P. and Rényi, A. On random graphs I. Publ. math. debrecen , 6(290-297):18, 1959.
Feller, A. and Gelman, A. Hierarchical models for causal effects. Emerging Trends in the Social and
Behavioral Sciences: An interdisciplinary, searchable, and linkable resource , pp. 1–16, 2015.
Figurnov, M., Mohamed, S., and Mnih, A. Implicit reparameterization gradients. Advances in Neural
Information Processing Systems , 31, 2018.
Gelman, A. and Hill, J. Data analysis using regression and multilevel/hierarchical models . Cambridge
University Press, 2006.
Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. Bayesian data analysis . Chapman and Hall/CRC,
3rd edition, 2014.
Geweke, J. Inference and causality in economic time series models. Handbook of Econometrics , 2:1101–1144,
1984.
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum
chemistry. In International Conference on Machine Learning , pp. 1263–1272. PMLR, 2017.
Gorishniy, Y., Rubachev, I., and Babenko, A. On embeddings for numerical features in tabular deep learning.
Advances in Neural Information Processing Systems , 35:24991–25004, 2022.
Graber, C. and Schwing, A. Dynamic neural relational inference for forecasting trajectories. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops , pp. 1018–1019,
2020.
Granger, C. W. Investigating causal relations by econometric models and cross-spectral methods. Econo-
metrica: Journal of the Econometric Society , pp. 424–438, 1969.
Granger, C. W. Testing for causality: A personal viewpoint. Journal of Economic Dynamics and Control ,
2:329–352, 1980.
Hatton, S. L., Rathore, S., Vilinsky, I., and Stowasser, A. Quantitative and qualitative representation of
introductory and advanced EEG concepts: An exploration of different EEG setups. Journal of Under-
graduate Neuroscience Education , 21(2):A142, 2023.
18Published in Transactions on Machine Learning Research (02/2024)
Heller, K. A. and Ghahramani, Z. Bayesian hierarchical clustering. In Proceedings of the 22nd International
Conference on Machine learning , pp. 297–304, 2005.
Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation , 9(8):1735–1780, 1997.
Hong, Y., Liu, Y., and Wang, S. Granger causality in risk and detection of extreme risk spillover between
financial markets. Journal of Econometrics , 150(2):271–287, 2009.
Jankowiak, M.andObermeyer, F. Pathwisederivativesbeyondthereparameterizationtrick. In International
Conference on Machine Learning , pp. 2235–2244. PMLR, 2018.
Kerin, J. and Engler, H. On the Lorenz’96 model and some generalizations. Discrete and Continuous
Dynamical Systems - B , 27(2):769–797, 2022. ISSN 1531-3492. doi: 10.3934/dcdsb.2021064.
Khanna, S. and Tan, V. Y. F. Economy statistical recurrent units for inferring nonlinear granger causality.
InInternational Conference on Learning Representations , 2020.
Kingma, D. P. and Welling, M. Auto-encoding variational Bayes. In International Conference on Learning
Representations , 2014.
Kingma, D. P., Mohamed, S., Jimenez Rezende, D., and Welling, M. Semi-supervised learning with deep
generative models. Advances in Neural Information Processing Systems , 27, 2014.
Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. Neural relational inference for interacting
systems. In International Conference on Machine Learning , pp. 2688–2697. PMLR, 2018.
Kyung, M., Gill, J., Ghosh, M., and Casella, G. Penalized regression, standard errors, and Bayesian Lassos.
Bayesian Analysis , 5(2):369–411, 2010. ISSN 1931-6690. doi: 10.1214/10-BA607.
Lindley, D. V. and Smith, A. F. Bayes estimates for the linear model. Journal of the Royal Statistical Society
Series B: Statistical Methodology , 34(1):1–18, 1972.
Lorenz, E.N. Predictability: Aproblempartlysolved. In Proc. Seminar on Predictability , volume1.Reading,
1996.
Löwe, S., Madras, D., Zemel, R., and Welling, M. Amortized causal discovery: Learning to infer causal
graphs from time-series data. In Conference on Causal Learning and Reasoning , pp. 509–525. PMLR,
2022.
Maddison, C. J., Mnih, A., and Teh, Y. W. The concrete distribution: A continuous relaxation of discrete
random variables. In International Conference on Learning Representations , 2017.
Marcinkevičs, R. and Vogt, J. E. Interpretable models for Granger causality using self-explaining neural
networks. In International Conference on Learning Representations , 2021.
Marx, E., Deutschländer, A., Stephan, T., Dieterich, M., Wiesmann, M., and Brandt, T. Eyes open and
eyes closed as rest conditions: impact on brain activation patterns. Neuroimage , 21(4):1818–1824, 2004.
Miraglia, F., Vecchio, F., Bramanti, P., and Rossini, P. M. EEG characteristics in “eyes-open” versus “eyes-
closed” conditions: Small-world network architecture in healthy aging and age-related brain degeneration.
Clinical Neurophysiology , 127(2):1261–1268, 2016.
Modarres, M., Cochran, D., Kennedy, D. N., and Frazier, J. A. Comparison of comprehensive quantita-
tive EEG metrics between typically developing boys and girls in resting state eyes-open and eyes-closed
conditions. Frontiers in Human Neuroscience , 17, 2023.
Montalto, A., Stramaglia, S., Faes, L., Tessitore, G., Prevete, R., and Marinazzo, D. Neural networks with
non-uniform embedding and explicit validation phase to assess Granger causality. Neural Networks , 71:
159–171, 2015.
Nauta, M., Bucur, D., and Seifert, C. Causal discovery with attention-based convolutional neural networks.
Machine Learning and Knowledge Extraction , 1(1):19, 2019.
Rubinov, M. and Sporns, O. Complex network measures of brain connectivity: uses and interpretations.
Neuroimage , 52(3):1059–1069, 2010.
Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model.
IEEE Transactions on Neural Networks , 20(1):61–80, 2008.
19Published in Transactions on Machine Learning Research (02/2024)
Seth, A. K., Barrett, A. B., and Barnett, L. Granger causality analysis in neuroscience and neuroimaging.
Journal of Neuroscience , 35(8):3293–3297, 2015.
Shojaie, A. and Fox, E. B. Granger causality: A review and recent advances. Annual Review of Statistics
and Its Application , 9:289–319, 2022.
Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., and Winther, O. Ladder variational autoencoders.
Advances in Neural Information Processing Systems , 29, 2016.
Stam, C. J. Nonlinear dynamical analysis of EEG and MEG: review of an emerging field. Clinical Neuro-
physiology , 116(10):2266–2301, 2005.
Stock, J. H. and Watson, M. W. Vector autoregressions. Journal of Economic Perspectives , 15(4):101–115,
2001.
Tank, A., Covert, I., Foti, N., Shojaie, A., and Fox, E. B. Neural Granger causality. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 44(8):4267–4279, 2021.
Teh, Y. W. and Jordan, M. I. Hierarchical Bayesian nonparametric models with applications. Bayesian
Nonparametrics , 1:158–207, 2010.
Trujillo, L. T., Stanfield, C. T., and Vela, R. D. The effect of electroencephalogram (EEG) reference
choice on information-theoretic measures of the complexity and integration of EEG signals. Frontiers in
Neuroscience , 11:425, 2017.
Wikle, C. K., Berliner, L. M., and Cressie, N. Hierarchical Bayesian space-time models. Environmental and
Ecological Statistics , 5:117–154, 1998.
Wu, T., Breuel, T., Skuhersky, M., and Kautz, J. Discovering nonlinear relations with minimum predictive
information regularization. arXiv preprint arXiv:2001.01885 , 2020.
20Published in Transactions on Machine Learning Research (02/2024)
A Additional Modeling and Implementation Details
In this section, we provide a description for some additional modeling details. In Sections A.1 and A.2, we
omit superscript [m]that indexes the entities whenever there is no ambiguity, as the descriptions therein
apply to all m’s independently unless otherwise specified.
A.1 Encoder
We provide details for the encoder sub-module that is abstracted as fx→h, wherein based on the node
trajectories, one obtains the hidden representations for the edges {hij}:=fx→h(x); see also Section 3.2,
module (enc-a).
As the most basic building blocks of message-passing operations, “node2edge” and “edge2node” operate
based off a complete graph, and can be generically represented as:
eij←concat (xi,xj)(node2edge) ;xi←/summationdisplay
jeij(edge2node) ,
withxidenoting the node representation and eijthe edge one. fx→his then parameterized through the L
passes of such operations:
(init emb) : ˇx(0)
i←emb(xi),∀i= 1,···,p
ˇx→e:e(l)
ij←MLP/parenleftbig
node2edge (ˇx(l−1)
i,ˇx(l−1)
j)/parenrightbig
;l= 1,···,L
e→ˇx: ˇx(l−1)
i←MLP/parenleftbig
edge2node (e(l)
ij;j= 1,···,p)/parenrightbig
;l= 2,···,L
Here xicorresponds to the trajectory of node iover time, that is, xi= (xi,1,···,xi,T)and the final hidden
representation is given by hij:=e(L)
ij,i,j= 1,···,p.
Concretely, the embedding module can be as simple as entailing only Linear-ReLU type operations; the
input trajectory xiof a nodei,∀i∈{1,···,p}, is processed via the following steps outlined in Figure 6:
xi Linear ReLU Dropout Linear ˇx(0)
i(flatten)
embedding module
Figure 6: Example for the embedding operation in the encoder according to MLP style. Blocks with trainable parameters are outlined
in red. Note that the flattening step is only required when the nodes are vector-valued.
Note that this also coincides with the LR-type embedding functions in Gorishniy et al. (2022). In regards to
the MLP block, it is obtained by stacking the sub-blocks as illustrated in Figure 7.
input Linear ReLU Dropout Linear ReLU Dropout Linear output
MLP sub-block MLP sub-block
Figure 7: An MLP block obtained by stacking the sub-blocks. Constituent blocks with trainable parameters are outlined in red.
Figure 8 provides a pictorial illustration for the sequential operations entailed in the Trajectory2Graph
encoder module.14Note that this is effectively the MLPEncoder used in Kipf et al. (2018) and the description
is given here for the sake of completeness. We refer interested readers to Kipf et al. (2018) for some other
encoders considered therein.
14In our experiments, all the MLP blocks used in the Trajectory2Graph operations are kept simple with only one single
sub-block; the hidden dimension is set at 128 or 256, depending on the exact experiments.
21Published in Transactions on Machine Learning Research (02/2024)
x1
x2
xp−1
xpˇx(0)
1
ˇx(0)
2
ˇx(0)
p−1
ˇx(0)
ph11h12···h1p
h21h22···h2p
............
hp1hp2···hppz11z12···z1p
z21z22···z2p
............
zp1zp2···zpp
node2edgeGNN-MLP1edge2nodeGNN-MLP2node2edgeGNN-MLP3
EmbeddingMLP
GNN-style message passing
Figure 8: Diagram for the Trajectory2Graph encoder operations. Blocks with trainable parameters are outlined in red.
A.2 Decoder
We divide this subsection into two parts, that respectively (1) discuss how the structure adopted in a
node-centric Graph2trajectory module described in Section 3.2 can readily accommodate the presence of
dependence on more than 1 lags; and (2) provide a brief discussion on how the original edge-centric decoder
adopted in Kipf et al. (2018); Löwe et al. (2022) can be revised to adapt to the case of a numerical graph,
and compare it with the node-centric one, although architectural choices are not the focus of this paper.
Extension to multiple lag dependency. The extension of a node-centric decoder to accommodate the
presence of more than 1 lags (i.e., q > 1) is straightforward, largely due to the fact that the node value
at timet−1, denoted by xj,t−1is not limited to be scalar-valued in the first place. In the case of q-lag
dependency, one can simply replace xj,t−1by concat (xj,t−1,···,xj,t−q)and proceed with the remainder of
the operations as outlined in (9) and (10). In particular, with the presence of more lags, as an alternative
to a (optional) numerical embedding step, one can instead consider 1D-CNN as a preprocessing module on
the “new”xj,t−1, before an element-wise gate represented by zijis applied to control the information flow.
Adaptation of the edge-centric decoder. The original edge-centric decoder adopted in Kipf et al.
(2018) handles the case where each entry in zijcorresponds to an edge type (categorical), and it entails the
following operations:
1. node2edge for each time step, that is eij,t−1:=concat (xi,t−1,xj,t−1)to arrive at the edge represen-
tation at time t−1;
2. foreachedge type of interest, run eij,t−1’s through its corresponding edge type-specific function
(e.g., MLP) to get the “enriched” representation ˇeij,t−1;
3. aggregate the enriched edge representations back to nodes via an edge2node operation, giving rise
tovi,t−1’s,i= 1,···,p;vi,t−1then serves as the predictor for time- tresponsexi,t.
In order for the above module to accommodate the case of a numeric zij, the following simple modification
to step 2 is introduced:
2’ runeij,t−1’s through some function (e.g., MLP) to get the “enriched” representation ˇeij,t−1, and
further update it through a gating mechanism as dictated by zij, that is, ˇeij,t−1←ˇeij,t−1◦zij.
The information propagation path from node jtoican be represented as:
xj,t−1node2edge→eij,t−1MLP→ˇeij,t−1gating→ˇeij,t−1◦zijedge2node→ vi,t−1→xi,t; (14)
one can easily verify that for zij= 0, there is no path from xj,t−1toxi,t.
As a final remark, for the node-centric decoder, the gating through zijdirectly operates on the node repre-
sentation, and the path is given by
xj,t−1emb→ˇxj,t−1gating→ˇxj,t−1◦zijelement of→ ui,t−1→xi,t;
see also Figure 9 for a pictorial illustration.
22Published in Transactions on Machine Learning Research (02/2024)
x1
x2
xp−1
xpˇx1
ˇx2
ˇxp−1
ˇxpzi1
zi2
zi(p−1)
zipu1
i
u2
i
up−1
i
up
iMLP xi◦
◦
◦
◦past time≤(t−1)timet
numerical embedding
Figure 9: Diagram for the node-centric Graph2Trajectory Decoder operations, with node ibeing the response node in this illustration.
The corresponding entries of z’s (shaded in gray, obtained by sampling and is fed into the Graph2Trajectory Decoder as input) perform
the gating operation (denoted by ◦). Blocks with trainable parameters are outlined in red and are shared across all response nodes
i= 1,···,p.
Tocontrast, fortheedge-centricdecoder, asindicatedin(14), entriesin zijdeterminethelead-laginformation
passing from j→iviaeij,t−1, and therefore such a gating mechanism is somewhat circumstantial.
A.3 Loss calculation
A derivation of (12) is given next.
KL/parenleftig
qϕ(Z|X)/vextenddouble/vextenddoublepθ(Z)/parenrightig
=Eqϕ(Z|X)log/bracketleftigqϕ(Z|X)
pθ(Z)/bracketrightig
=Eqϕ(Z|X)/bracketleftig
logqϕ(¯z|{z[m]})
pθ(¯z)+ logqϕ({z[m]}|{x[m]})
pθ({z[m]}|¯z)/bracketrightig
=/integraldisplay/integraldisplay
qϕ/parenleftbig
¯z|{z[m]}/parenrightbig
qϕ/parenleftbig
{z[m]}|{x[m]}/parenrightbig
log/bracketleftigqϕ/parenleftbig
¯z|{z[m]}/parenrightbig
pθ/parenleftbig
¯z/parenrightbig/bracketrightig
d¯zd{z[m]}
+/integraldisplay/integraldisplay
qϕ/parenleftbig
¯z|{z[m]}/parenrightbig
qϕ/parenleftbig
{z[m]}|{x[m]}/parenrightbig
log/bracketleftigqϕ/parenleftbig
{z[m]}|{x[m]}/parenrightbig
pθ/parenleftbig
{z[m]}|¯z/parenrightbig/bracketrightig
d¯zd{z[m]}
=/integraldisplay
qϕ({z[m]}|{x[m]})/braceleftig/integraldisplay
qϕ/parenleftbig
¯z|{z[m]}/parenrightbig
log/bracketleftigqϕ/parenleftbig
¯z|{z[m]}/parenrightbig
pθ/parenleftbig
¯z/parenrightbig/bracketrightig
d¯z/bracerightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
KL/parenleftig
qϕ(¯z|{z[m]})/vextenddouble/vextenddoublepθ(¯z)/parenrightigd{z[m]}
+/integraldisplay
qϕ/parenleftbig
¯z|{z[m]},{x[m]}/parenrightbig/braceleftig/integraldisplay
qϕ/parenleftbig
{z[m]}|{x[m]}/parenrightbig
log/bracketleftigqϕ/parenleftbig
{z[m]}|{x[m]}/parenrightbig
pθ/parenleftbig
{z[m]}|¯z/parenrightbig/bracketrightig
d{z[m]}/bracerightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
KL/parenleftig
qϕ({z[m]}|{x[m]})/vextenddouble/vextenddoublepθ({z[m]}|¯z)/parenrightigd¯z
(a)=Eqϕ({z[m]}|X)/bracketleftig
KL/parenleftig
qϕ(¯z|{z[m]})/vextenddouble/vextenddoublepθ(¯z)/parenrightig/bracketrightig
+Eqϕ(¯z|X)/bracketleftig
KL/parenleftig
qϕ({z[m]}|{x[m]})/vextenddouble/vextenddoublepθ({z[m]}|¯z)/parenrightig/bracketrightig
.
For (a), the first term is straightforward, the second term goes through since
/integraldisplay
p(x|y,z)/braceleftig/integraldisplay
p(y|z) logp(y|z)
q(y|x)dy/bracerightig
dx=/integraldisplay/integraldisplay
p(y|z)p(x|y,z) logp(y|z)
q(y|x)dxdy
=EY|ZEX|Z,Ylogp(y|z)
q(y|x)=EY|ZEX|Zlogp(y|z)
q(y|x)=EX|Z/bracketleftig
EY|Zlogp(y|z)
q(y|x)/bracketrightig
;
and the last equality holds as a result of the Fubini-Tonelli theorem.
A.4 Evaluating the predictive strength of Granger causal relationships
Next, we briefly discuss how the trained decoder can be used to measure the predictive strength of the
Granger causal connections.
Once the model is trained, using the inference procedure described in Section 3.3, one obtains estimates ˆz[m]
for all entity-specific graphs. Further, a trained Graph2Trajectory module, abstracted as ˆgz→x, also becomes
23Published in Transactions on Machine Learning Research (02/2024)
available. The predictive strength of any connection entry (i,j)— corresponding to the lead-lag relationship
fromjtoi— can then be assessed by nullifying the corresponding entry. Throughout the remainder of the
discussion, we omit superscript [m]for ease of presentation, as the procedure is applicable to an arbitrary
entity of interest.
Let˜z(ij)be identical to ˆzexcept that the (i,j)entry is set to zero (nullified). The reconstructed trajectories,
based on the estimated and the nullified graphs are given by ˆx= ˆgz→x(ˆz,x1)15and˜x(ij)= ˆgz→x(˜z(ij),x1),
respectively. The predictive strength can then be evaluated based on the difference in the residual-sum-
of-squares (RSS), with the latter obtained by evaluating the reconstructed trajectory against the observed
values. Concretely, RSS (ˆx)can be obtained by1
T−1/summationtextT
t=2∥xt−ˆxt∥2and that for ˜x(ij)can be analogously
obtained; the predictive strength of the (i,j)connection can then be calculated as RSS (ˆx)−RSS(˜x(ij)). This
procedure can be generalized to a set of connections, where instead of nullifying a single entry, multiple
entries are nullified simultaneously and the remainder of the evaluation follows. Note that the proposed
procedure resembles that of testing for the presence/absence of Granger causality in linear VAR models,
where an F-test is used (Geweke, 1984). The calculated difference RSS (ˆx)−RSS(˜x(ij))also appears in the
numerator of the aforementioned F-statistic.
A.5 Construction of samples
We briefly explain how samples are constructed from observed data trajectories. We omit the superscript
[m]that corresponds to the entity ID, since the construction is generally applicable.
The available data can either correspond to a collection of long trajectories (e.g., traditional time series
setting where observations for different variables are collected over time), or to multiple collections of (long)
trajectories, where each collection corresponds to temporal observations over time from repeated measure-
ments (e.g., in the context of a neurophysiological experiment, a subject is exposed to a stimulus (eyes open
or eyes close) a number of times). In both cases, the trajectories are parsed into shorter ones of length T,
which is the context window considered in the modeling. Concretely, let {x0,x1,···,x/tildewideT}be the long trajec-
tory, with/tildewideTdenoting the total number of observations. The samples, indexed by n, are shorter trajectories
of lengthT, with each consisting of observations X(n):={xsn,xsn+1,···,xsn+T−1}, wheresis the stride
size that dictates the overlapping between samples with adjacent indices. A long trajectory of length /tildewideTgives
rise to⌊(/tildewideT−T)/s+ 1⌋samples, which are then used during mini-batch training.
B Additional Synthetic Data Experiments and Results
B.1 Lorenz96 and Springs5 experiments
To explore the applicability of the proposed framework to selected special cases, there are two other settings
considered in our synthetic data experiments: the Lorenz96 and the Springs5 systems. Unlike the settings
presented in the numerical experiments in Section 4 wherein the entity-level heterogeneity manifests itself
primarily in the form of perturbations to the skeleton of the shared common graph, for these two systems,
the entity-specific skeletons are either identical across all Mentities and only the magnitude of the entries
changes (Lorenz96), or they manifest their heterogeneity through a probabilistic mechanism (Springs), as
explained in the sequel.
Similar to those presented earlier, for both settings, we run the experiments on 5 data replicates and report
the metrics after averaging across the 5 runs, with their respective standard deviation included in the
parentheses.
15Recall that throughout the main sections, we use x:={x1,···,xT}to denote the trajectory; here ˆxis effectively its
“reconstructed” couterpart.
24Published in Transactions on Machine Learning Research (02/2024)
B.1.1 The Lorenz96 system
The Lorenz96 system (Lorenz, 1996) has been previously investigated in Tank et al. (2021); Marcinkevičs &
Vogt (2021). The dynamics for a p-variable system evolve according to the following ODE:
dxi
dt= (xi+1−xi−2)xi−1−xi+F, i = 1,···,p, (15)
Figure 10: Lorenz96: ¯zandz[0], showing only the
skeleton.where xi:={xi,t}denotes the continuous time trajectory of
nodeiwith x0:=xp,x−1:=xp−1andxp+1:=x1. Such a sys-
tem corresponds to a Granger-causal structure shown in Fig-
ure 10 that depicts its skeleton. The representation in (15) can
be obtained from Kerin & Engler (2022):
dxi
dt=α(xi+1−xi−2)xi−1−βxi+γ, (16)
byreparameterizing α=β,λ=α/βandsetting F=αγ/β2.F
is the forcing constant that controls the degree of non-linearity;
in particular, given the relationship between (15) and (16), as
Fvaries, the strength of the Granger-causality changes despite an invariant skeleton. In other words, to
induce heterogeneity across entities, we can only change the parameter Fthat induces heterogeneity in the
magnitudes of the Granger causal connections, while the skeleton of the Granger causal graph remains the
same. We consider a setting with p= 20andM= 5entities, with the forces taking the following values:
F∈{10.0,17.5,25.0,32.5,40.0}.
Table 2: Performance evaluation for the estimated ¯zandz[m]’s for setting Lorenz96. Numbers are in % and rounded to integers, and
correspond to the mean results based on 5 data replicates; standard deviations are reported in the parentheses.
Generative model-based Prediction model-based
Multi-node Multi-edge One-node One-edge NGC-cMLP GVAR TCDF Linear
common AUROC 100(0.1) 100(0.7) 100(0.1) 90(19.7) 97(0.0) 100(0.1) 82(0.9) 99(0.1)
AUPRC 100(0.4) 99(1.6) 100(0.3) 82(32.5) 87(0.1) 100(0.2) 65(0.9) 97(0.5)
F1(best) 97(1.5) 96(3.4) 97(1.3) 80(25.7) 87(0.8) 98(1.0) 59(1.3) 89(0.2)
entity AUROC 95(1.3) 85(3.7) 96(1.0) 88(1.9) 96(0.1) 97(0.8) 79(0.8) 99(0.1)
(avg) AUPRC 89(2.3) 76(4.6) 91(2.0) 78(2.9) 85(0.3) 90(1.5) 62(0.7) 96(0.3)
F1(best) 82(3.2) 71(3.5) 84(2.6) 72(3.1) 83(0.4) 83(0.2) 58(0.5) 88(0.3)
The results are shown in Table 2 and the main findings are: (1) consistent with the results in Section 4, the
node-centric decoder outperforms the edge-centric one; (2) the proposed joint-learning approach Multi-node
matches the performance of GVARand outperforms all other competitors for the common graph; (3) for the
entity-specific graphs, interestingly, the linear VAR exhibits a slight edge over all competing methods, while
the performance of the proposed model is broadly on-par with the remaining competitors.
Figure 11: Estimated ¯zand z[m]’s with different F’s using the proposed joint-learning framework with a node-centric decoder
(Multi-node ).
Finally, the common and the five entity-specific Granger causal graphs for the Multi-node method are
depicted in Figure 11. It can be seen that the performance deteriorates for systems with larger external force
F.
25Published in Transactions on Machine Learning Research (02/2024)
B.1.2 Springs5 system
Figure 12: Springs5: ¯zand z[0].z[0]is binary
(and symmetric) with entries generated according to
Bernoulli distributions.This setting is investigated in Kipf et al. (2018); Löwe et al.
(2022), and in this work we consider a “multi-entity” version of
it. In the original setting, particles (i.e., nodes) are connected
(pairwise) by springs at random with probability 0.5; in the
case where the connection between particles iandjis present,
they interact according to Hooke’s law Fij=−k(ri−rj), where
Fijis the force applied to particle iby particle j,kis the
spring constant and riis the location vector of particle iin 2-
dimensional space. With some initial location and velocity, the
trajectories can be simulated by solving Newton’s equations of
motion (see also Kipf et al. (2018), Appendix B for details).
Crucially, (1) the Granger-causal graph is essentially a realiza-
tion of the homogeneous Erdős-Rényi graph (Erdős & Rényi,
1959) with edge probability being 0.5, and (2) each node’s trajectory is multivariate with 4 dimensions, that
is,xi,t∈Rd,d= 4; the first 2 dimensions correspond to the velocity and the last 2 to the location in the
2-dimensional space.
The extension to the “multi-entity” case that is suitable for the setup considered in this paper is described
next, and it differs primarily from the original one in how the Granger-causal connections across nodes
are generated. Specifically, we start from ¯z, whose entries (i,j)in its upper-triangular part are generated
independently from Beta(1,1); then set ¯zji≡¯zij,i < jso that it’s symmetric. For the z[m]’s, let z[m]
ij∼
Ber(¯zij),i < j, and then set z[m]
ji≡z[m]
ij,∀m= 1,···,M. Once z[m]’s are generated, they dictate the
connections between nodes in their respective systems, and one can proceed with the same procedure as in
the original setting to simulate the trajectories. Note that (1) each entity’s Granger-causal graph corresponds
to a realization of a heterogeneous Erdős-Rényi graph; the edge probability differs across node pairs and
depends on the corresponding entry in ¯zthat is a realization from the Beta distribution, and (2) the grand
common structure possesses a “probabilistic” interpretation, in that it effectively captures the expectation of
an edge being present/absent across all entities. In this experiment, we set p= 5andM= 10.
None of the competitors based on the prediction models can readily handle this setting16, and therefore we
only present results for those based on generative models. Note that in this experiment, despite that the
underlying true graphs are symmetric, we do notuse this information during our estimation.
Table 3 shows the results for the above-mentioned systems, using both the node- and the edge-centric de-
coders. A visualization of the estimates is provided in Figure 13. Overall, the proposed joint learning
framework outperforms individual learning for entity-level graphs, while the performance is largely compa-
rable for the common graph estimate. Given the physics system nature of this dataset (vis-a-vis time series
signals), the edge-centric decoder has a small advantage over the node-centric one; this is manifested by the
fact that under the joint learning framework, the two decoders show comparable performance, whereas the
edge-centric decoder is clearly superior in the case of single-entity separate learning. Note that this points
to another potential advantage of the joint-learning framework, in that it is more robust and exhibits less
volatility than individual learning.
Table 3: Performance evaluation for the estimated ¯z(error in Frobenius norm) and z[m]’s (accuracy and F1 score after thresholding at
0.5, averaged across all entities) for the Springs5 system.
quantity metric Multi-node Multi-edge One-node One-edge
common ERR-fnorm 1.00(0.259) 0.92(0.294) 1.30(0.412) 0.79(0.217)
entity(avg) ACC% 99.3(0.84) 99.3(0.76) 87.5(6.45) 96.3(3.99)
entity(avg) F1Score% 99.5(0.79) 99.4(0.73) 88.2(7.45) 96.3(4.78)
16There are two issues that the prediction model-based competitors can not readily handle and would require major changes:
(1) all of them assume that the Granger-causality to be estimated is numeric and therefore does not naturally handle the binary
case, and (2) at any point in time, each node is assumed to have a scalar value, akin to classical time-series settings, whereas
here each node is vector valued; consequently, the existing code does not readily handle it.
26Published in Transactions on Machine Learning Research (02/2024)
Figure 13: Estimated ¯zandz[m]’s (showing the first five) using the proposed framework with node-centric decoder ( Multi-node ).
B.2 Additional performance evaluation results and their visualization
Table 4 presents additional evaluation metrics (TPR, TNR and ACC) for the proposed method and its strong
competitors, after the estimates of the Granger causal graphs are thresholded at various levels no greater
than 0.5 (after normalization). We only show the results for the estimated common graph ¯z, since the results
for the entity-level ones exhibit similar patterns.
As briefly mentioned in Section 4, prediction model-based methods ( NGC/GVAR) are more sensitive to the
value of the threshold, manifested by a sudden jump in accuracy once the threshold exceeds a certain level.
On the other hand, the change in ACC for the ones based on generative models is more gradual. Given that
in practice it is common to use a moderate threshold to eliminate small entries of the initial estimates of the
Granger causal graphs to determine their skeleton, the above-mentioned susceptibility can adversely impact
the quality of the final estimate used for interpretation purposes and in downstream analytical tasks.
Table 4: Performance evaluation for the support set of the estimated common graph ¯zat various threshold levels (left-most column).
Numbers are in %, and correspond to the mean results based on 5 data replicates.
Multi-node One-node NGC-cMLP GVAR Linear
TPR TNR ACC TPR TNR ACC TPR TNR ACC TPR TNR ACC TPR TNR ACC
Linear VAR
0.10 100 92.1 92.9 98.1 50.3 55.1 100 0.0 10.0 100 0.0 10.0 100 99.9 99.9
0.20 100 99.9 99.9 95.8 78.9 80.6 100 0.0 10.0 100 0.0 10.0 100 100 100
0.30 100 100 100 91.2 90.9 91.0 100 0.0 10.0 100 2.9 12.7 100 100 100
0.4099.6 100 100 81.8 96.0 94.6 100 48.9 54.2 100 57.4 61.9 100 100 100
0.5092.7 100 99.3 67.6 98.5 95.4 79.4 99.9 97.9 96.9 100 99.7 98.7 100 99.9
Non-linear VAR
0.10 100 74.2 76.7 100 59.3 63.1 100 0.0 9.5 100 0.0 9.5 99.5 57.1 61.1
0.2098.4 89.2 90.0 100 82.9 84.5 100 0.0 9.5 100 0.0 9.5 97.4 99.8 99.5
0.3094.7 91.7 92.0 96.3 89.3 90.0 100 0.0 9.5 100 85.4 86.8 92.1 100 99.2
0.4089.5 99.4 98.5 72.1 91.8 89.9 99.5 47.9 52.8 71.1 100 97.2 68.9 100 97.0
0.5073.2 100 97.5 60.5 95.6 92.2 47.4 95.7 91.2 61.1 100 96.3 60.5 100 96.2
Lotka-Volterra
0.05 100 72.8 76.8 99.0 40.5 49.3 100 58.4 64.7 34.0 100 90.1 33.3 100 90.0
0.10 100 97.4 97.8 96.3 73.9 77.3 99.7 100 100 33.3 100 90.0 33.3 100 90.0
0.1599.3 99.8 99.8 90.0 92.4 92.0 90.7 100 98.6 33.3 100 90.0 33.3 100 90.0
0.3067.0 100 95.0 50.3 100 92.5 33.7 100 90.0 33.3 100 90.0 33.3 100 90.0
0.5033.3 100 90.0 33.3 100 90.0 33.3 100 90.0 33.3 100 90.0 33.3 100 90.0
Lorenz96
0.0595.2 99.5 98.7 93.8 100 98.8 100 0.0 20.0 100 99.8 99.8 95.8 94.1 94.5
0.1058.8 100 91.8 39.5 100 87.9 100 0.0 20.0 96.8 100 97.0 50.0 100 90.0
0.1527.2 100 85.5 25.0 100 85.0 100 0.0 20.0 72.8 100 94.5 25.0 100 85.0
0.3025.0 100 85.0 25.0 100 85.0 100 79.2 83.4 25.0 100 85.0 25.0 100 85.0
0.5025.0 100 85.0 25.0 100 85.0 93.0 93.4 93.3 25.0 100 85.0 25.0 100 85.0
An illustration of the recovered Granger-causal connections (after “optimal” thresholding) is shown in Fig-
ure 14. Note that NGCcan only produce the “unsigned” version of the connections and hence all its estimates
are shown as positive, whereas for other methods, the entries are “signed” with red denoting the positive
and blue the negative ones.
One interesting observation is that for the Lotka-Volterra system, all methods have incorrectly estimated
the signs of the diagonals, in that the underlying true dependencies on their own lags are positive for the
preys and negative for the predators, whereas all methods fail to identify such discrepancy — although for
27Published in Transactions on Machine Learning Research (02/2024)
(a) Linear VAR: estimated ¯z(or transition matrix ¯A, equivalently)
(b) Non-linear VAR: estimated ¯z
(c) Lotka-Volterra. Top panel: estimated ¯z; bottom panel: estimated ¯zafter suppressing the diagonals
(d) Lorenz96: estimated ¯z
Figure 14: Estimated ¯z(after normalization) for various methods. The displayed f1score corresponds to the best attainable one (after
thresholding) for each method. Red:( +); blue:(−). Note that NGCdoes not produce signed estimates and hence all its estimates are
shown in red, with the shades corresponding to the magnitude of the entries after normalization.
the prediction model-based ones all dependencies show as positive and generative model-based ones have the
opposite sign. This could be partially driven by the fact that during trajectory generation, the Runge–Kutta
method (specifically, RK4) has been used and thus it renders the presence of a self-lag linear term with
coefficient 1in the recursion; in addition, a small noise term has also been injected.
For this setting, given that the estimated diagonals have dominating magnitude for GVARand Linear, we
also provide a visual display of the estimates with the diagonals suppressed.
Remark 5.A dichotomous behavior is observed between the unsigned and the signed estimates obtained
from the code implementation of GVAR17, with the former typically being 5-10% better (in absolute values,
for reported metrics such as AUC, ACC that are between 0-100%). In all the tables, we have reported the
performance of the superior one (unsigned), whereas Figure 14 is produced based on the signed estimate
to show the positive/negative recovery. The best attainable F1 scores after thresholding (corresponding to
the result of the specific data replicate being displayed) for these signed estimates are labeled in the title of
17Repository for GVAR: https://github.com/i6092467/GVAR
28Published in Transactions on Machine Learning Research (02/2024)
the figures; e.g., 0.75 for the non-linear VAR setting, 0.95 and 0.86 for the Lotka-Volterra and the Lorenz96
setting, respectively.
B.3 The impact of the degree of heterogeneity
Toevaluatetherobustnessandpotentialsusceptibilityoftheproposedframeworktothelevelofheterogeneity
present across entities, we conduct additional experiments based on the Linear VAR and Non-linear VAR
settings described in Section 4.1. To recap, the following dynamics are considered for each individual system
ofpnodes, xt= (x1,t,···,xp,t)⊤∈Rp:
•Linear VAR: xt=Axt−1+εt. The Granger-causal graph zcoincides with A.
•Non-linear VAR: each response coordinate 2≤i≤(p−1)depends on the lag of its own that of two
othercoordinatesindexedby k1
iandk3
i,thatis,xi,t= 0.25xk2
i,t−1+sin(xk1
i,t−1·xk3
i,t−1)+cos(xk1
i,t−1+
xk3
i,t−1) +εi,t, withk1
i<k2
i≡i<k3
i. The dynamics for the first and the last coordinates depend
only on their respective adjacent coordinate, i.e., for i= 1,x1,t= 0.4x1,t−1−0.5x2,t−1+ε1,t; for
i=p,xp,t= 0.4xp,t−1−0.5xp−1,t−1+εp,t. The Granger-causal graph zdictates the exact locations
of thek1
i’s andk3
i’s.
For both settings, the Granger-causal graphs z[m]’s of theMentities are obtained by a “perturbation” with
respect to the initial common Granger-causal graph ¯z(0)(or¯A(0)equivalently, in a linear setting), and the
magnitude of such perturbation determines the degree of heterogeneity across entities and the final common
graph ¯z. The perturbation logic resembles the one described in Section 4.1.
Specifically, for the linear VAR setting, we let the skeleton of ¯A(0)have 30% density, that is, the support
setS¯A(0)is determined by independent draws from Ber (0.3), and the magnitude of the perturbation is
controlled by the percentage of “relocated” entries. For the Non-Linear VAR setting, the magnitude of the
perturbation is controlled by the number of rows whose off-diagonal entries are kept unchanged from those in
the initial common Granger causal graph.18The sub-settings (S1-S5 with increasing degree of heterogeneity,
respectively for linear and non-linear VAR setups) are depicted in Figures 15 and 16, where the percentage
of relocation and the unchanged entries, respectively, are given in the sub-captions. Note that under the
non-linear VAR setup, S1 and S5 correspond to the two extreme cases: no entity-level heterogeneity and
almost fully heterogeneous.
We focus on generative model-based methods with a node-centric decoder, i.e., Multi-node (proposed frame-
work) and One-node , and evaluate the performance of the estimates, obtained by training the model on
different sample sizes. For the linear VAR setting, the sample size is set to 200 and 1000, while for the
non-linear VAR setting to 500 and 2000. The selection of these sample sizes was based on the following
three considerations: (1) non-linear dynamics are typically more challenging to learn and thus require larger
networks and more samples to train; and (2) instead of choosing a “large” sample size where both methods
perform well and thus little differentiation is shown, additional insights can be gained by assessing the per-
formance of the model in settings where the available sample size is getting close to the information-theoretic
limit (at the conceptual level).
Tables 5 and 6 display the results of the Linear/Non-linear VAR settings based on the same set of metrics as
in Section 4.2, and they correspond to the average of 3 data replicates with the standard deviations displayed
in parentheses. Major observations are: (1) for Multi-node , the estimation of ¯zis reasonably robust to the
varying degree of heterogeneity across sub-settings. In particular, little deterioration is observed across sub-
settings, although for sub-setting S5, given the very few common entries, the presented metrics become not
not particularly meaningful. (2) Regarding the quality of individual entity estimates, Multi-node exhibits
some deterioration in the non-linear setting when the model is getting close to being mis-specified (S5
versus S1-S4). (3) In the settings under consideration, where the sample size starts becoming rather small,
Multi-node starts exhibiting an advantage over One-node by a wide margin. Specifically, for the estimated
¯z,One-node shows performance degradation as the level of heterogeneity increases across sub-settings S1 to
S5 (even for the linear case), and the overall performance is inferior to that of Multi-node . The latter is
18Recall, in the original settings presented in Section 4.1, for the linear VAR setting, the percentage of “relocated" entries is
10%; for the non-linear VAR setting, every 3rd row is left unchanged.
29Published in Transactions on Machine Learning Research (02/2024)
(a) relocation = 10%
 (b) relocation = 25%
 (c) relocation = 50%
 (d) relocation = 75%
 (e) relocation = 90%
Figure 15: 30×30Linear VAR system with a total number of M= 20entities. Sub-settings are displayed vertically with increasing
level of heterogeneity (from left to right). In the figure, only ¯zandz[1],z[2]are displayed.
(a) fix everything
 (b) fix every other row
 (c) fix every third row
 (d) fix diagonals +corners
 (e) fix first +last rows
Figure 16: 20×20Non-linear VAR system with a total number of M= 10entities. Sub-settings are displayed vertically with increasing
level of heterogeneity (from left to right). In the figure, only ¯zandz[1],z[2]are displayed. Similar to those in Section 4, as the
non-linearity is induced via sinusoidal functions, we do not know the true sign of the cross lead-lag dependency; as such, the entries
corresponding to entries that are present are colored in black.
somewhat expected: Multi-node performs joint estimation over samples across all entities and thus borrows
information across; as such, it can rely on fewer number of samples19to attain estimates of similar accuracy.
19Here the number of samples is expressed in relative terms, that is, train size, corresponding to the number of trajectories
used in training for each entity.
30Published in Transactions on Machine Learning Research (02/2024)
Table 5: Performance evaluation for the estimated ¯zandz[m]’s under settings S1-S5 of Linear VAR. Numbers are in %, and correspond
to the mean results based on 5 data replicates; standard deviations are reported in the parentheses.
Multi-node One-node
S1 S1 S3 S4 S5 S1 S2 S3 S4 S5
Linear VAR ; train size 200
common AUROC 100(0.0) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 90(2.1) 90(0.6) 90(4.7) 83(4.3) 85(1.7)
AUPRC 100(0.0) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 85(1.4) 82(0.2) 77(8.1) 56(5.7) 44(10.1)
F1(best) 100(0.3) 100(0.0) 100(0.2) 100(0.0) 100(0.0) 75(2.1) 72(1.3) 70(8.1) 53(6.1) 43(10.0)
entity AUROC 94(1.4) 94(1.2) 94(1.4) 95(1.6) 95(1.6) 88(3.1) 89(2.4) 89(3.1) 88(3.4) 88(2.6)
(avg) AUPRC 92(1.9) 92(1.8) 92(2.3) 92(2.3) 92(2.4) 83(4.2) 84(3.4) 84(4.6) 82(4.4) 83(3.4)
F1(best) 84(2.4) 84(2.2) 84(2.9) 84(2.7) 84(2.9) 75(4.1) 76(3.0) 75(4.1) 74(4.1) 74(3.2)
Linear VAR ; train size 1000
common AUROC 100(0.0) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 97(1.3) 96(0.6) 95(1.3) 91(1.2) 93(0.6)
AUPRC 100(0.0) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 95(1.6) 93(0.3) 88(3.9) 72(4.1) 59(9.9)
F1(best) 100(0.4) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 89(1.7) 85(0.5) 80(6.5) 67(3.0) 57(8.6)
entity AUROC 95(1.4) 95(1.3) 95(1.5) 95(1.7) 95(1.6) 94(1.4) 94(1.4) 94(1.5) 94(1.6) 94(1.6)
(avg) AUPRC 92(2.0) 92(2.0) 92(2.4) 92(2.3) 92(2.3) 92(2.0) 92(2.1) 91(2.4) 92(2.3) 92(2.3)
F1(best) 84(2.9) 84(2.4) 84(2.9) 85(2.6) 85(2.8) 84(2.7) 84(2.5) 84(3.1) 84(2.7) 84(2.9)
Table 6: Performance evaluation for the estimated ¯zandz[m]’s under settings S1-S5 of Non-linear VAR. Numbers are in %, and
correspond to the mean results based on 5 data replicates; standard deviations are reported in the parentheses.
Multi-node One-node
S1 S1 S3 S4 S5 S1 S2 S3 S4 S5
Non-linear VAR ; train size 500
common AUROC 98(0.4) 98(1.4) 96(0.4) 100(0.0) 100(0.0) 92(1.0) 84(1.3) 75(1.7) 98(0.2) 98(0.9)
AUPRC 81(1.4) 84(10.9) 73(3.2) 98(0.4) 100(0.0) 69(0.1) 64(0.6) 49(9.0) 89(0.6) 46(31.5)
F1(best) 84(3.3) 78(7.3) 69(1.8) 92(1.9) 100(0.0) 74(0.6) 69(0.0) 60(5.6) 90(2.1) 50(31.0)
entity AUROC 97(0.1) 97(0.8) 92(0.3) 98(0.5) 77(1.0) 92(0.4) 74(1.3) 63(1.6) 71(2.9) 51(2.9)
(avg) AUPRC 79(0.4) 82(6.0) 67(2.1) 88(1.9) 41(1.9) 68(0.6) 54(0.2) 39(3.4) 52(2.6) 18(0.9)
F1(best) 79(0.6) 77(3.1) 68(1.1) 81(2.6) 55(1.3) 67(1.2) 53(0.3) 45(1.1) 51(1.4) 28(1.5)
Non-linear VAR ; train size 2000
common AUC 99(0.1) 100(0.2) 98(0.3) 100(0.0) 100(0.0) 95(0.1) 95(0.0) 95(0.4) 99(0.2) 100(0.0)
AUPRC 94(0.5) 96(4.3) 84(1.9) 99(0.3) 100(0.0) 74(0.1) 75(0.1) 77(1.1) 92(1.0) 100(0.0)
F1(best) 95(0.0) 96(1.7) 77(1.1) 96(1.9) 100(0.0) 77(0.0) 69(0.0) 72(2.2) 92(0.0) 100(0.0)
entity AUROC 99(0.1) 99(0.2) 95(0.6) 99(0.1) 80(0.6) 95(0.3) 93(0.2) 90(0.3) 93(0.5) 73(1.1)
(avg) AUPRC 92(0.4) 93(2.1) 82(0.7) 95(0.3) 53(2.9) 78(0.9) 71(0.3) 67(0.2) 71(1.1) 32(0.8)
F1(best) 87(0.6) 88(0.9) 76(1.2) 89(0.9) 61(1.5) 76(0.4) 70(1.0) 63(0.4) 64(1.2) 47(0.5)
Finally, note that in the proposed framework, at the decoder stage the Common2Entity step where the
encoder distribution is merged via weighted conjugacy adjustment, the hyper-parameter ωcontrols the
mixing percentage between the common and the entity-specific information. Conceptually, its choice varies
according to the degree of heterogeneity present across entities: in the extreme case where the common
structure is de facto absent, ω= 1; the other end of the extreme corresponds to ω= 0when there is no
heterogeneity. It is worth noting that we have observed empirically that the proposed framework is not
sensitive to the choice of ω, since in most cases its specific value makes little difference to the quality of the
estimated ¯zandz[m]’s, as long as it was selected from a reasonable range (e.g., between [0.25,0.75]). For
example, in all the experiments above, we have fixed ωat 0.5.
B.4 Some remarks on sample size
We give a brief account of the performance of the proposed framework in small sample size regimes. Note
that in practical settings, model performance hinges on multiple factors, such as sample size, the size of the
problem—including both the number of nodes and the number of entities, given the joint learning strategy—
and how complex the temporal dynamics of the underlying systems are. The goal of this section is to provide
guidance on the “minimum number of samples required”—from a practitioner’s perspective—in settings of
comparable size to the ones considered herein.
Specifically, we focus on the same set of time series settings for systems with non-linear dynamics considered
inSection4andAppendixB.1,namely,theNon-LinearVAR,multi-speciesLotka-Volterra,andtheLorenz96.
In all three settings there are 20 nodes in their respective entity-level dynamical systems, and the collection
31Published in Transactions on Machine Learning Research (02/2024)
contains 5 or 10 entities. Recall that for the first setting the non-linear dynamics are induced through some
sinusoidal function, while the other two settings are ODE-based systems.
Table 7 presents the performance evaluation of the estimated ¯zandz[m]’s based on Multi-node , when
training sample sizes are 3000, 1000 and 500, respectively.
Table 7: Performance evaluation for /hatwide¯zand/hatwidez[m]’s based on Multi-node under different settings with various training sample sizes.
Numbers are in %, and correspond to the mean results based on 5 data replicates; standard deviations are reported in the parentheses.
Non-linear VAR Lotka-Volterra Lorenz96
3000 1000 500 3000 1000 500 3000 1000 500
common AUROC 98(0.1) 97(0.2) 96(0.4) 100(0.0) 97(2.2) 90(8.0) 99(0.4) 95(1.3) 89(3.5)
AUPRC 89(0.5) 80(1.3) 74(3.1) 100(0.2) 95(3.2) 81(7.9) 98(0.8) 92(2.0) 85(2.8)
F1(best) 79(1.4) 75(1.3) 69(1.7) 100(0.4) 94(4.3) 78(4.6) 94(0.7) 87(2.3) 84(1.9)
entity AUROC 96(0.5) 94(0.6) 92(0.7) 88(0.8) 81(2.1) 64(1.4) 92(1.5) 87(1.3) 85(1.3)
(avg) AUPRC 86(0.4) 76(1.2) 69(2.3) 79(1.2) 66(3.2) 43(3.8) 85(2.4) 79(2.1) 76(2.8)
F1(best) 78(0.3) 73(1.4) 69(1.6) 75(1.1) 64(4.0) 42(2.4) 78(2.3) 73(2.5) 70(3.6)
The main observations are: (1) for the common graph ¯z, as sample size reduces from 3000 to 1000, the
proposed method’s performance metrics stay above a reasonable range, even though a certain degradation
is present, and its magnitude varies across settings. (2) For the entity-specific z[m]’s, the degradation in
performance is more pronounced as the sample size reduces, and the model clearly suffers from not having
access to an adequate number of samples.20
Basedontheseobservations, webroadlyconcludethefollowingforpracticalsettingsofcomparablesizetothe
onesexaminedabove: inthecasewheretheprimaryfocusisonthecommongraph ¯z, theproposedframework
would likely yield reasonable recovery even with about 1000 samples. On the other hand, if individual entity-
level estimates are also of interest, sample sizes below 3000 would become rather challenging for the method
to exhibit a satisfactory performance.
B.5 Lotka-Volterra with perturbation: some characterization
We provide a characterization/justification for the “perturbed” Lotka-Volterra system, pertaining to how to
validate a Lotka-Volterra system based on the “perturbed” interaction matrix being stable.
The general form of p-multi-species Lotka-Volterra equations are given by
dxi
dt=rixi/parenleftbig
1 +p/summationdisplay
j=1Aijxj/parenrightbig
, (17)
whereri>0is theinherent per-capita growth rate of species xi,i= 1,···,pandA∈Rp×pthe species
interaction matrix. The system considered in (13) can then be put in this canonical form, by assuming that
the firstp/2species are preys and the last p/2species predators.
Specifically, for the preys the corresponding equation in the canonical form becomes
dxi
dt=αxi/bracketleftbigg/parenleftbig
1−1
η2xi/parenrightbig
−β/α/summationdisplay
j∈Pprey
ixj/bracketrightbigg
, i = 1,···,p/2
whereri=α,Aii=−1
η2,Aij=−β/αfor allj∈Pprey
iotherwise 0;Pprey
idenotes the support set of the
prey indexed by i. Analogously, for the predators the corresponding equation in the canonical form becomes
dxi
dt=−γxi/parenleftbig
1−δ/γ/summationdisplay
j∈Ppredator
ixj/parenrightbig
, i =p/2 + 1,···,p
20For these small sample size experiments, we use the same set of hyper-parameters as the ones in earlier experiments with
much larger sample sizes (1e4). One can potentially expect improved performance with more carefully tuned hyper-parameters,
although the improvement would likely be limited.
32Published in Transactions on Machine Learning Research (02/2024)
whereri=−γ,Aii= 0,Aij=−δ/γfor allj∈Ppredator
iotherwise 0;Ppredator
idenotes the support set of
the predator indexed by i.
It can be seen that fixed points of the set of equations in (17) can be found by setting d xi/dt= 0for alli,
which translates to the vector equation
r+Ax= 0, r∈Rp,x∈Rp,A∈Rp×p.
Consequently, fixed points exist if Ais invertible and are given by x=−A−1r. Note that xi= 0is a trivial
fixed point. Further, the fixed point may contain both positive and negative values, which implies that there
is no stable attractor for which the populations of all species are positive. The eigenvalues of Adetermine
thestability of the fixed point. By the stable manifold theorem, if its eigenvalues are less than 1, then the
fixed point is stable. This can be easily verified once the “perturbed” Granger-causal matrix z’s (which
determines thePi’s and hence the corresponding A) are generated.
C Granger Causality and Graphical Models, Bayesian Hierarchical Modeling, and
Linear VARs
This section comprises of three parts that provide background information on different topics mentioned in
the main paper. Section C.1 illustrates how the framework of graphical models can be used to capture the
concept of Granger causality. Section C.2 provides a brief overview of the Bayesian hierarchical modeling
frameworkandoutlineshowitsharesbroadsimilaritiestothemodelingframeworkusedinthepaper. Finally,
Section C.3 discusses possible ways of accomplishing the modeling task via a collection of linear VARs, either
using a frequentist formulation, or a Bayesian hierarchical modeling one.
C.1 Granger causality and graphical models
Consider a dynamical system, comprising of a p-dimensional stationary time series xt:= (x1,t,···,xp,t),
withxi,tdenoting the value of node iat timet. Further, let V={x1,···,xp}denote the node set of the p
nodes/time series of the system.
AGranger causal time series graph (Dahlhaus & Eichler, 2003) has node set V=V×Zand edge set
E⊆V×V, wherein an edge (xi,t−s)→(xj,t)̸∈E, if and only if s≤0orxi,t−s⊥ ⊥xj,t|Xt\xi,t−s, where
Xt={xt′,t′<t}denotes the entire past process of the time series at time t,⊥ ⊥probabilistic independence
and\the set difference operator. The above definition implies that the edge set Econtains directed edges
from past time points to present ones, only if xi,t−sandxj,tare dependent, conditioned on all other past
nodes inVexcludingxi,t−s.
1
2
3
4
51
2
3
4
51
2
3
4
5
timet−2 timet−1 timet
(a) Example for a Granger causal graph with 2-lag dependency
andV={1,2,3,4,5}. For the edges in E, those originate from
timet−2are denoted in dash and those from time t−1are in
solid arrows, respectively.“or”
t−2t
t−1t
past< tpresentt
(b) Connection matrices corresponding to the graph in Figure 17a, where columns cor-
respond to emitters (past) and rows corresponding to receivers (present). Colored cells
denote the presence of a connection. The right-most matrix corresponds to the aggregate
Granger-causal connection matrix that summarizes and indicates present-past dependen-
cies.
Figure 17: Pictorial illustration for Granger causal time series graph, aggregate Granger causal graph and their corresponding matrix
representation.
Anaggregate Granger causal graph (Dahlhaus & Eichler, 2003) has vertex set Vand edge setE, wherein an
edge (xi→xj)̸∈Eif and only if (xi,t−s)→(xj,t)̸∈Efor allu > 0, t∈Z; i.e,absenceof the edge
(xi→xj)from the aggregate Granger causal graph implies absenceof Granger causality from node (time
33Published in Transactions on Machine Learning Research (02/2024)
series)xito nodexj, whilepresence of that edge implies that one or more time lags of node xiare Granger
causal of node xj.
Figure 17 illustrates pictorially both the Granger causal time series graph (Figure 17a), and the matrix
representation (in the form of heatmaps) for the aggregate Granger causal graph (the right-most heatmap
in Figure 17b).
In the case of a linear VAR system of order qgiven by xt=/summationtextq
k=1Akxt−k+et, the edge set of the
Granger causal time series graph corresponds to E={(Ak)ij|(Ak)ij̸= 0, i,j∈V, k = 1,···,q}, while
E={Bij|Bij=1(/summationtextq
k=1(abs(Ak)ij)̸= 0), i,j∈V}, with 1(·)denoting the indicator function. The aggregate
Granger causal graph with edge set Eis anunweighted one, namely, its edges take values 0 (absence) or 1
(presence) and consequently reflect absence/presence of Granger causality between the time series.
Remark6 (On the estimated Granger-causal graph) .Under the proposed framework, in the binary case, the
Granger connectivity graph corresponds exactly to the aggregate Granger causal graph defined above (see
Section 3 and Remark 1), which is also in the same spirit as how different edge types are modeled in Kipf
et al. (2018). In the continuous case, it corresponds to a weighted version of the aggregate Granger causal
graph, wherein the weights correspond to the size of the “gate” through which the information from the
past flows to the present. Admittedly, in the presence of non-linear modules (such as MLP) after the gating
operation in the decoder, the weights no longer correspond to the “predictive strength” as defined in the
original paper by Granger (1969). Nonetheless, at the conceptual level, the weights reflect the “strength” of
the underlying relationships, as measured through the “permissible information flow”.
C.2 Bayesian hierarchical modeling
Given the prevalent usage of hierarchical modeling in the case where observational units form a hierarchy—
e.g., in our motivating example, the observed time series are at the entity level and the entities form a
group—we briefly review the Bayesian hierarchical modeling framework next.
SinceitsinitialintroductioninLindley&Smith(1972)forlinearmodels, theBayesianhierarchicalframework
has been expanded and used for many other classes of statistical models. The book by Gelman et al. (2014)
provides a description of the general framework and outlines the role of exchangeability for constructing prior
distributions for statistical models with hierarchical structure. The framework has been operationalized and
used for many statistical models, including regression and multilevel models (Gelman & Hill, 2006), time
series (Berliner, 1996) and spatio-temporal models (Wikle et al., 1998), in causal analysis (Feller & Gelman,
2015), cluster analysis (Heller & Ghahramani, 2005), in nonparametric modeling (Teh & Jordan, 2010), and
so forth. At the modeling level, the outline of the framework for a hierarchy comprising of two levels is as
follows. Data for entities m= 1,···,Mare generated according to some probability distribution
p(x[m];θ[m],ϕ) =p(x[m]|θ[m])·p(θ[m]|ϕ)·p(ϕ).
θ[m]’s are entity-specific parameters, and they are assumed to be generated exchangeably from a common
population, whose distribution is governed by a common parameter ϕ, and can be specified as p(θ[m]|ϕ).
The common parameter ϕcan be fairly complex (for an example, see Section C.3) and possesses a prior
distribution p(ϕ), which depending on the nature of ϕcan be fairly involved. The prior distribution for the
parameter (θ[m],ϕ)thatgovernsthedatagenerationmechanismforentity mjointly,canthenbecharacterized
byp(θ[m],ϕ) =p/parenleftbig
θ[m]|ϕ/parenrightbig
p(ϕ).
Note that the above specification exhibits differences to the generative process of a multi-level VAE presented
in Section 2.2. Specifically, in the VAE specification, there are observed and latent random variables,
modeledaccordingtoaprobabilitydistributionwith fixedparameters θ⋆, whereasintheBayesianhierarchical
modeling formulation, the parameters of the data generating distribution are randomvariables themselves
and respect a hierarchical specification as previously mentioned.
C.3 Modeling via a collection of linear VARs
We illustrate how the modeling task at hand can be handled when the dynamics are assumed linear. In
particular, the dynamical systems can be characterized by a collection of linear VAR models; we show how
34Published in Transactions on Machine Learning Research (02/2024)
the common structure can be modeled by decomposing the transition matrix or using hierarchical modeling,
respectively in a frequentist and a Bayesian setting. For ease of exposition, in the sequel, we assume the
collection of linear VAR models have lag of order 1, and they are given by
x[m]
t=A[m]x[m]
t−1+εt, m = 1,···,M.
Frequentist formulation. Suppose that the transition matrices can be decomposed as A[m]=A0+B[m],
i.e., into a common component A0and anentity-specific oneB[m]. For model identifiability purposes, an
“orthogonality” constraint is imposed; for example, in the form of A0B[m]=0∈Rp×p. In settings where
the transition matrices A[m]are additionally assumed sparse (see, e.g., the numerical experiments in Section
4), such a constraint is typically in the form of support (A0)∩support (B[m]) =∅, namely that the matrices
A0andB[m]do not share non-zero entries.
Bayesian hierarchical modeling formulation. We consider a collection of linear VAR models as above.
The probability distribution of the data is p/parenleftbig
{x[m]
t}M
m=1|{A[m]}M
m=1,ϕ/parenrightbig
, whereϕis a vector of additional
parameters specified next. To construct the prior distribution of the model parameters ({A[m]},ϕ)we
proceed as follows. Note that at the modeling level, a simple hierarchy is defined for each (i,j)-th element
of the transition matrix across all Mentities/models; i.e., we consider p2such hierarchies independently . To
use the Bayesian hierarchical modeling framework, let ⃗ cij= (A[m]
ij,···,A[M]
ij)′be anM-dimensional vector
containing the (i,j)-th element of all Mtransition matrices. The following distributions are imposed on ⃗ cij’s
and the parameters associated with their priors:
⃗ cij|(Ψ,τij)∼N(0,τijΨ); (18)
τij∼Gamma (M+ 1/2,λij),
Ψ∼Inverse Wishart (S0,γ0).
The prior distributions in (18) are independent over index (i,j);τijis an (i,j)-element specific scaling factor,
andΨanM×Mmatrix that captures similarities between the Mmodels. The parameters λij,S0,γ0can
be either fixed to some pre-specified values (e.g., a fixed S0can reflect prior knowledge on the similarity
between the Mmodels), or equipped with diffuse prior distributions. Further, note that if Ψ≡Ithe identity
matrix, then the above specification reduces to the Bayesian group lasso of Kyung et al. (2010). Based on
the above exposition, it can be seen that ϕ:= (Ψ,{τ}ij,i,j= 1,···,p). In summary, we have the following
two-level modeling specification: at the first level, we have the data distribution, while at the second level
the distribution on the elements of the transition matrices that are “coupled" across the Mmodels through
ϕand its prior distribution specification. Obviously, more complicated prior specifications can be imposed,
for example by “coupling” whole rows of the transition matrices A[m]across the entities.
D Additional Results for the EEG Dataset
The estimated common Granger-causal connections based on One-node andNGCare depicted in Figures 18
and 19, respectively. The increase in the overall Granger causal connectivity in the EC session compared
to that in the EO session observed for Multi-node and GVARis also present in the results from One-node ,
whereas it is reversed in the results of the NCG. Further, the observed increase in the overall connectivity
pattern between the EO session compared to the EC session, exhibits differences between the left and right
parts of the brain, something also observed in the results of Multi-node . Further, note that NCGdoes not
produce signed estimates and hence all Granger causal connections are colored grey in Figure 19. This
limitation of the method can hinder scientific insights that could be obtained from the analysis of a dataset
byNGC.
E On Respecting the Sign Distinction of the Connections
This section provides some explanation to how the proposed methodology (Multi-node)—modulo estimation
error that can introduce inaccuracies—recovers the sign of the underlying truth up to a complete sign flip ,
35Published in Transactions on Machine Learning Research (02/2024)
(a) Eyes Open (EO)
 (b) Eyes Closed (EC)
Figure18: One-node results: estimated common Granger-causalconnectionsforEO(leftpanel)andEC(rightpanel)afternormalization
and subsequent thresholding at 0.50. Red edges correspond to positive connections and blue edges correspond to negative ones; the
transparency ofthe edgesis proportionalto the strengthof theconnection. Larger node sizes correspond to higher in-degree (incoming
connectivity), and the top 6 nodes are colored in gray.
(a) Eyes Open (EO)
 (b) Eyes Closed (EC)
Figure 19: NGCresults: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and
subsequent thresholding at 0.45. All edges are colored gray, since NGCdoes not provide signed estimates of Granger causal connections.
The transparency of the edges is proportional to the strength of the connection. Larger node sizes correspond to higher in-degree
(incoming connectivity), and the top 6 nodes are colored in gray.
that is,
SIGN (ˆz) = (±)SIGN (z); (19)
with SIGN (·)operating in an entry-wise fashion on zorˆz. In (19), zgenerically refers either to the grand-
common Granger-causal graph ¯zor entity specific ones z[m], and ˆzis the corresponding estimate. This is
equivalent to saying that there is no guarantee that for each individual entry, sign (zij) =sign(/hatwidezij)always
holds; however, all positive (negative) signed connections are identified as having the same sign. In this
regard, the signs of the estimates obtained from the procedure can be interpreted in a meaningful way, in
that the positive/negative connections can be differentiated ; see Figure 20 for an illustration.
✓complete sign flip
✓no sign flip
est, no sign flip truth est, complete sign flip
Figure 20: Pictorial illustration for the concept of “up to complete sign flip”. In both the no-sign-flip and the complete-sign-flip case,
the estimate always “groups" the positive/negative connections together in a way that is in accordance with the truth.
As a result of (19), the following also readily holds for any two entries indexed by (i1,j1)and(i2,j2),
∀i1,j1,i2,j2∈{1,···,p}:
sign(zi1j1)sign(zi2j2) =sign(/hatwidezi1j1)sign(/hatwidezi2j2);
i.e., if two connections have the same/opposite signs in z, they continue having the same/opposite signs in
ˆz. We shall refer to this property as “respecting the sign distinction”.
36Published in Transactions on Machine Learning Research (02/2024)
The goal of this section is to provide some intuition on how the above mentioned is enabled through the
encoder-decoder learning—in particular, in the presence of non-linear modules. Note that the subsequent
arguments do not constitute a formal end-to-end proof.
In the sequel, we focus on the single-entity case and ignore modules related to the coupling between entity-
level graphs and their grand-common counterpart, as these modules are not pertinent to this specific discus-
sion. Concretely, the relevant modules in the ensuing discussion are:
•q(z[m]|x[m])as captured by (enc-a) and (enc-b) combined; i.e., the “Trajectory2Graph” encoder.
•p(x[m]|z[m])as captured by (dec-b); i.e., the “Graph2Trajectory” decoder.
The superscript [m]will be omitted henceforth.
Outline of the argument. The argument consists of two parts:
1. The decoder, by utilizing a sharedMLP across all response coordinates, ensures that the sign distinc-
tion is respected across the rows, along any column. Specifically, see, e.g., equations (9) and (10),
wherein the MLP and the subsequent operations (in particular, their parameters) are shared by all
response coordinates i’s.
2. The encoder, in the case of supervised training, disallows any partial(row or column) sign flip.
(1) and (2) jointly ensure that ˆzrespects the sign of zup to acomplete sign flip, and this is operationalized
via the end-to-end training where the parameters are jointly learned and the data likelihood maximized.
At the high level, the shared MLP mechanism in the decoder ensures that it will not generate estimates
that show “row sign flip” relative to the underlying truth. Specifically, for any fixed column, if one looks at
the estimates along the columns (i.e., vertically) and across the rows, the estimates would respect their sign
distinction in a pairwise fashion. However, it does not preclude cases where along the rows (i.e., horizontally)
and across the columns, signs in the estimates can be flipped (i.e., column sign flip). On the other hand,
row sign flip
×
truth est
(a) Example for row sign-flip: signs of the 2nd row is flipped (right versus.
left)column sign flip
?
truth est
(b) Example for column sign-flips: signs of the 1st, 3rd and 4th columns are
flipped (right versus. left)
Figure 21: Pictorial illustration for the concept of “row sign flip” and “column sign flip”, picking the first two rows from Figure 20.
Note that the former is prohibited by the shared MLP mechanism in the decoder construction.
to generate estimates that recover the sign of the underlying truth up to a complete sign flip, both row and
column sign flips need to be precluded. The latter is facilitated by the encoder module during the end-to-end
training: when the decoder fixes the “sign-orientation” vertically across the rows, the encoder would favor
estimates that do not exhibit any partial sign flip during learning.
The details for each component are given next.
E.1 Decoder
Claim: By using a shared MLP across all response coordinates, for any fixed column j∈{1,···,p}, the
decoder respects the sign distinction across the rows of z, that is,
sign(/hatwidezi1j)sign(/hatwidezi2j)≡sign(zi1j)sign(zi2j),∀i1,i2∈{1,···,p}. (20)
The same cannot be guaranteed, however, if different MLPs are used for different response coordinates.
For illustration purposes, we focus on the case where the feature dimension is 1 (i.e., classical time series
setting). Consider a simple two-layer MLP whose hidden layer has hneurons. Let fMLP :Rp∝⇕⊣√∫⊔≀→Rbe
represented as
fMLP(u) =W(2)σ/parenleftbig
W(1)u+b(1)/parenrightbig
+b(2), u∈Rp;
37Published in Transactions on Machine Learning Research (02/2024)
W(1)∈Rh×p,b(1)∈Rh×1,W(2)∈R1×h,b(2)∈R;σ(·)is some activation function. Specifically in the
Graph2Trajectory decoder, the function input of the MLP is in the form of ui,t−1, whosejth coordinate is
given byxj,t−1◦zij, assuming the absence of any numerical embedding (see, e.g., expressions in (9) with
superscript [m]dropped). Tofurthersimplifynotation,weignoresubscript t−1,andlet y= (y1,···,yp)∈Rp
denote the time- ttarget. Effectively, at decoding time, an approximation of the following form is considered
for all timestamps:
yi≈fMLP(ui),∀i= 1,···,p
=W(2)σ

W(1)
11W(1)
12···W(1)
1p
............
W(1)
h1W(1)
h2···W(1)
hp

x1◦zi1
...
xp◦zip
+b(1)
+b(2),(21)
wherex1,···,xpare inputs directly available through training data, (zi1,···,zip)′constitutes the ith row
of matrix z. Crucially, fMLPis shared across all i’s.
In the actual end-to-end learning, zij’s are sampled from a distribution whose parameters are dictated by the
encoding step. The parameters of the encoders are jointly learned with those of the decoders, by minimizing
the reconstruction error and the KL term. Here to further delineate the issue pertaining specifically to
whether with the use of a shared MLP, the learnedzij’s can respect the sign distinction, we ignore the
encoding step, and simplifies the question as follows:
Can the learning procedure—by minimizing the prediction error based on (21)— that jointly learns the W’s,
b’s and entries of z’s give rise to learned /hatwidezij’s, such that the /hatwidezij’s respect the sign distinction?
The answer is affirmative for any fixed column j= 1,···,p. To see this, expand the matrix product in (21),
which gives (here we ignore approximation error and assume the model is well-specified):
yi=h/summationdisplay
s=1W(2)
sσ/parenleftigp/summationdisplay
j=1/parenleftbig
W(1)
sj◦zij/parenrightbig
xj+b(1)/parenrightig
+b(2).
The predicted /hatwideyiis given by
/hatwideyi=h/summationdisplay
s=1/hatwiderW(2)
sσ/parenleftigp/summationdisplay
j=1/parenleftbig/hatwiderW(1)
sj◦/hatwidezij/parenrightbig
xj+/hatwideb(1)/parenrightig
+/hatwideb(2),
where/hatwiderW(1),/hatwiderW(2),/hatwideb(1)and/hatwideb(2)are estimated weights and bias terms. By minimizing the prediction error, /hatwideyi
is close toyi, foranyvalues ofx1,x2,···,xpand for alli’s. This amounts to having the estimated coefficients
in front of the xj’s sufficiently close to the truth—in particular, modulo estimation error, the following holds:
W(1)
sjzij=/hatwiderW(1)
sj/hatwidezij,for alli= 1,···,p. (22)
This further gives
(W(1)
sj)2zi2jzi2j= (/hatwiderW(1)
sj)2/hatwidezi1j/hatwidezi2j,∀i1,i2∈{1,···,p}, (23)
and therefore (20) follows since (/hatwiderW(1)
sj)2>0.
Note that in the case where different MLPs are used for different response coordinates, (23) becomes
(W(i1,1)
sjW(i2,1)
sj )zi2jzi2j= (/hatwiderW(i1,1)
sj/hatwiderW(i2,1)
sj )/hatwidezi1j/hatwidezi2j, which no longer leads to (20).
Toy data experiments. To verify this empirically, we consider a toy data example, where the trajectories
are generated according to a 2-dimensional linear VAR system, that is,
xt=Axt−1+et,whereA=/bracketleftbigg
0.5−0.25
−0.25 0.5/bracketrightbigg
; (24)
38Published in Transactions on Machine Learning Research (02/2024)
coordinates of etare drawn i.i.d. from N(0,0.5). Note that given the linear setup, the transition matrix
corresponds precisely to the true Granger-causal graph, and therefore z≡A.
We run end-to-end training based on two configurations of the decoder:
(a) a single MLP shared across all response coordinates;
(b) separate MLPs for different response coordinates.
In both configurations, the MLPs are 2-layer ones with a hidden layer of dimension 64. The experiment is
run over a single data replicate but repeated using 10 independent seeds.
(a) Normalized truth
 (b) Estimates based on Configuration (a) - a shared MLP
 (c) Estimates based on Configuration (b) - separate MLPs
Figure 22: Toy data experiment decoder results: heatmaps for z(truth, normalized) and ˆz(estimates, normalized) under the shared
and separate-MLP configurations. Panel (a) corresponds to zafter normalization; panel (b) correspond to normalized ˆz(from runs
with different seeds) obtained under Configuration (a); panel (c) correspond to normalized ˆzobtained under Configuration (b).
Figure 22 displays the estimated zcorresponding to 3 different seeds for each configuration. Amongst all
10 runs, Configuration(a) preserves the sign distinction at all times—in this particular case, diagonals in
ˆzalways have the same sign and anti-diagonals have the opposite. Note that results from run seed 324
(left-most figure in Figure 22b) correspond to the case where the estimate yields a complete sign flip of the
underlying truth. For Configuration (b), it fails in 2 out of the 10 runs—showing 2 failures (seed 324 and
644) and 1 success (seed 764) in Figure 22c, as the estimates can fail to preserve the sign distinction amongst
the edges.
E.2 Encoder
Claim: the encoder is able to perform “effective” learning based on labels up to a complete sign flip, but
learning becomes problematic when the labels entail any partialsign flip.
Similar to the case of the decoder, to delineate the issue pertaining to the encoder, instead of considering
end-to-end training where the two models are jointly learned, we consider a simplified setting, where we
use the encoder module for a supervised learning task, based on data whose true generating mechanism is
associated with the Granger causal graph z. The question posed is the following:
The true trajectories are generated based on z. For a supervised learning task where the training labels are
provided and the learning is enabled by the encoder module, is the encoder able to perform “effective”
learning,
1. when the label used during training is some partial (column or row) sign flip of z?
2. when the label used during training is a complete sign flip of z, namely−z?
This is explored via synthetic data experiments, where the data generating mechanism is identical to the
one considered in Section E.1.
Concretely, let z♯denote the quantity that is provided as the target (label) during the supervised training;
note that the data is generated according to (24), with z≡A=/bracketleftbig0.5−0.25
−0.25 0.5/bracketrightbig
, irrespective of the labels
provided. The following four training scenarios are considered:
(a) No sign flip: z♯=/bracketleftbig0.5−0.25
−0.25 0.5/bracketrightbig
, that is, z♯=z;
(b) Complete sign flip: z♯=/bracketleftbig−0.5 0.25
0.25−0.5/bracketrightbig
, that is, z♯=−z;
(c) Column sign flip: z♯=/bracketleftbig0.5 0.25
−0.25−0.5/bracketrightbig
, that is, z♯
:,1=z:,1,z♯
:,2=−z:,2;
39Published in Transactions on Machine Learning Research (02/2024)
(d) Row sign flip: z♯=/bracketleftbig0.5−0.25
0.25−0.5/bracketrightbig
, that is, z♯
1,:=z1,:,z♯
2,:=−z2,:.
We run encoder-only training for the above four scenarios. Results21are displayed in Figure 23, with the
estimated zdisplayed in the top panel and the label z♯used for supervision during training displayed in the
bottom panel.
(a) Scenario (a)
 (b) Scenario (b) - complete flip
 (c) Scenario (c) - column flip
 (d) Scenario (d) - row flip
Figure 23: Toy data experiment encoder-only results: heatmaps for ˆz(estimates, top panel) and and z♯(training label, bottom panel)
for scenarios (a) to (d) respectively. Note that the underlying ground truth (i.e., the zthat governs the dynamics of the trajectories)
for all these experiments are identical to the one in Scenario (a).
As the results show, the encoder learns almost perfectly (relative to the provided labels) in scenarios (a)
and (b), despite the latter being a complete sign flip. On the other hand, it struggles to learn in the case
of partial sign flips (i.e., Scenarios (c) and (d)), as manifested by the essentially-zero estimated values. This
empirically corroborates our claim.
Finally, it is worth noting that the claim examined in this subsection is under the supervised learning setup,
namely, it establishes the fact that the encoder only permits no or complete sign flip, under a setting where
the training target is explicitly provided. In practice, the learning is end-to-end, that is, there is no “real”
supervision on the encoder available. As such, at the conceptual level, the learning relies on the decoder to
fix the vertical sign-orientation as well as the encoder to preclude potential row sign flip—our experiment
results in Section E.1 also corroborates this.
F Generalization to Multiple Levels of Grouping
We discuss the generalization of the proposed framework to the case where multiple levels of grouping are
present and the corresponding group-common graphs at different levels of the hierarchy are of interest.
ConsiderL-levels of nestedgrouping where the group assignments become increasingly granular as the level
index increases. Specifically, there is a single level-0 group that encompasses all entities, and M(degenerate)
level-Lgroups, with each group mhaving a singleton member being the entity m; all other levels are cases in
between – see also Figure 24 for a pictorial illustration. Note that the case discussed in the main manuscript
corresponds to the special case with L= 1. As an example for the case of L= 2levels, consider the data
analyzed in Section 5. Suppose that the subjects can be partitioned into 3groups according to their ages —
e.g., less than 30 years old, 30-60 years old, over 60. In such a setting, the single level-0 group comprises of
all subjects; the level-1 groups correspond to subjects falling into different age strata; the level-2 groups are
the subjects themselves. The quantities of interest are the connectivity patterns shared by subjects within
their respective groups at all levels.
21Here we are displaying results for the test data; the results for training data lead to the same conclusion qualitatively.
40Published in Transactions on Machine Learning Research (02/2024)
Entities Level 2 Group Level 1 Group Level 0 Group
Figure 24: Diagram for a 3-level grouping. Neurons corresponds to Gl
k’s that collects the indices of the entities belonging to that group.
Solid lines with arrows indicate how small groups from an upper level form larger groups at a lower level.
LetGl:={Gl
1,···,Gl
|Gl|}denote the collection of groups of level l; eachGl
kis the index set for the entities
belongingtogroup katlevellandthegroupmembershipisnon-overlapping, thatis, Gl
k1∩Gl
k2=∅,∀k1,k2∈
{1,···,|Gl|}. The quantities of interest are the entity-specific graphs z[m], as well as the group-level common
structure for all groups at all levels, that is ¯zGl
k, denoting the group-common structure amongst all entities
that belong to the kth group, with level- lgrouping;l= 0,···,L−1indexes the group level; k= 1,···,|Gl|
indexes the group id within each level. Finally, we let ¯z≡¯zG0, which is consistent with its definition in the
main text and it corresponds to the grand-common structure across all entities.
Without getting into the details of each step, the end-to-end learning procedure can be summarized in
Figure 25. Compared with the two-level case, the generalization amounts to additional intermediate en-
coded/decoded distributions in the form of qϕ(z[Gl−1
k]|z[Gl
k]),pθ(z[Gl
k]|z[Gl−1
k])andpθ(z[Gl
k]|·)(post conjugacy
adjustment/merging information); l= 2,···,L;k= 1,···,|Gl|.
{x[m]} {z[m]}|{x[m]}{z[GL−1
k]}|{z[m]}{z[G1
k]}|{z[G2
k]}
sampled ¯zpθ(¯z)
{ˆx[m]} {z[m]}|·{z[GL−1
k]}|· {z[G1
k]}|·qϕ(z[m]|xm)
qϕ(¯z|{¯z[G1
k]})
pθ({z[G1
k]}|¯z)
pθ({x[m]}|{z[m]})(merge info)
(merge info) (merge info) (merge info)(observed)
encoding
decoding
(reconstructed)(prior)
Figure 25: Diagram for the end-to-end encoding-decoding procedure in the presence of multiple levels of grouping.
41