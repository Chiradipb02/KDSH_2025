Under review as submission to TMLR
A Unified Survey on Anomaly, Novelty, Open-Set, and Out-
of-Distribution Detection: Solutions and Future Challenges
Anonymous authors
Paper under double-blind review
Abstract
Machine learning models often encounter samples that are diverged from the training distri-
bution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign
that sample to an in-class label, significantly compromises the reliability of a model. The
problem has gained significant attention due to its importance for safety deploying mod-
els in open-world settings. Detecting OOD samples is challenging due to the intractability
of modeling all possible unknown distributions. To date, several research domains tackle
the problem of detecting unfamiliar samples, including anomaly detection, novelty detec-
tion, one-class learning, open set recognition, and out-of-distribution detection. Despite
having similar and shared concepts, out-of-distribution, open-set, and anomaly detection
have been investigated independently. Accordingly, these research avenues have not cross-
pollinated, creating research barriers. While some surveys intend to provide an overview
of these approaches, they seem to only focus on a specific domain without examining
the relationship between different domains. This survey aims to provide a cross-domain
and comprehensive review of numerous eminent works in respective areas while identify-
ing their commonalities. Researchers can benefit from the overview of research advances
in different fields and develop future methodology synergistically. Furthermore, to the
best of our knowledge, while there are surveys in anomaly detection or one-class learn-
ing, there is no comprehensive or up-to-date survey on out-of-distribution detection, which
our survey covers extensively. Finally, having a unified cross-domain perspective, we dis-
cuss and shed light on future lines of research, intending to bring these fields closer to-
gether. All the implementations and benchmarks reported in the paper can be found here :
https://anonymous.4open.science/r/osr-ood-ad-methods-DBFC/README.md (R2)
1 Introduction
Machine learning models commonly make the closed-set assumption, where the test data is drawn i.i.dfrom
the same distribution as the training data. Yet in practice, all types of test input data—even those on
which we have not trained the classifier—can be encountered. Unfortunately, models can assign misleading
confidencevaluesforunseentestsamplesSunetal.(2020);Milleretal.(2021);Oza&Patel(2019);Yoshihashi
et al. (2019); Yu & Aizawa (2019). This leads to concerns about the reliability of classifiers, particularly for
safety-critical applications Hendrycks et al. (2021a). In the literature, several fields attempt to address the
issueofidentifyingtheunknowns/anomalies/out-of-distributiondataintheopen-worldsetting. Inparticular,
the problems of anomaly detection (AD), Novelty Detection (ND), One-Class Classification (OCC), Out-of-
Distribution (OOD) detection, and Open-Set Recognition (OSR) have gained significant attention owing to
their fundamental importance and practical relevance. They have been used for similar tasks, although the
differences and connections are often overlooked.
Specifically, OSR trains a model on Kclasses of a Nclass training dataset; then, at the test time, the
model is faced with Ndifferent classes of which N−Kare not seen during training. OSR aims to assign
correct labels to seen test-time samples while detecting the unseen samples. Novelty detection or one-class
classification is an extreme case of open-set recognition, in which Kis 1. In the multi-class classification
setting, the problem of OOD detection is canonical to OSR: accurately classify in-distribution (ID) samples
1Under review as submission to TMLR
into the known categories and detect OOD data that is semantically different and therefore should not be
predicted by the model. However, OOD detection encompasses a broader spectrum of learning tasks (e.g.,
multi-label classification, reinforcement learning) and solution space (e.g., density estimation), which we
comprehensively review in this paper.
While all the aforementioned domains hold the assumption of accessing an entirely normal training dataset,
anomaly detection assumes the training dataset is captured in a fully unsupervised manner without applying
any filtration; therefore, it might contain some abnormal samples too. However, as abnormal events barely
occur, AD methods have used this fact and proposed filtering them during the training process to reach a
final semantic space that fully grasps normal features. Despite previous approaches that are mostly used
in object detection and image classification domains, this setting is more common in the industrial defect
detection tasks, in which abnormal events are rare and normal samples have a shared concept of normality.
Fig. 1 depicts an overview of the mentioned domains, in which the differences are shown visually. Note that
even if there are differences in the formulation of these domains, they have so much in common, making
them used interchangeably.
As an important research area, there have been several surveys in the literature Bulusu et al. (2020); Perera
et al. (2021); Ruff et al. (2021); Pang et al. (2020); Chalapathy & Chawla (2019), focusing on each domain
independently or providing a very general notion of anomaly detection to cover all different types of datasets.
Instead, we offer in-depth explanations for methodologies in respective areas. We make cross-domain bridges
by which ideas can be easily propagated and inspire future research. For instance, the idea of using some out-
lier samples from different datasets to improve task-specific features is called Outlier Exposure in Hendrycks
et al. (2019a) or background modeling in Dhamija et al. (2018), and is very similar to semi-supervised
anomaly detection in Ruff et al. (2019). Despite the shared idea, all are considered to be novel ideas in their
respective domains.
In this survey, we identify the commonalities that address different but related fields. Furthermore, we
try to categorize different methods based on general justifications. For instance, some surveys Ruff et al.
(2021) categorize methods into different classes, such as distance-based methods and density-based methods.
As distance-based methods can be simply converted to density-based ones, one can find this classification
ambiguous. Although some of the mentioned tasks have very close methodological setups, they differ in their
testing protocols. Therefore, a comprehensive review of all the methods can better reveal their limitations
in practical applications. (R1) Key to our survey, we describe methods both mathematically and visually to
give better insights to both newcomers and experienced researchers. Finally, comprehensive future lines of
research are provided both practically and fundamentally to not only address the issues of current methods
but also shed light on critical applications of these methods in different fields.
In summary, our main contributions are as follows :
1. Identify the relationship between different research areas that, despite being highly correlated with
each other, have been examined separately.
2. Comprehensive methodological analysis of recent eminent research, and providing a clear theoretical
and visual explanation for methods reviewed.
3. Performing comprehensive tests on existing baselines in order to provide a solid ground for current
and future lines of research.
4. Providing plausible future lines of research and specifying some fundamental necessities of the meth-
ods that will be presented in the future such as fairness, adversarial robustness, privacy, data-
efficiency, and explainability.
2 General Perspectives of Method Categorizations
We consider a dataset with training samples (x1,y1),(x2,y2),...from the joint distribution PX,Y, whereX
andYare random variables on an input space X=Rdand a label (output) space Yrespectively. In-class
2Under review as submission to TMLR
Novelty Detection
Open -Set Recognition
Out-Of-Distribution Detection
A
CB
Multi -class 
classification 
loss
Multi -class 
classification 
loss
One-class 
classification 
lossND
OSR
DA[ ]vs[∪ ] BC
A[∪ ]vs[ ] B C
OOD
B C A D [∪ ]vs[∪ ]
Figure 1: Problem setup for ND, OSR, and ODD from a unified perspective based on the common routine
followed in the respective fields. (A), (B) and (C) are sampled from the same training dataset, while (D)
is sampled from different datasets. Typically, in ND, all training samples are deemed normal, and share a
common semantic (green region), while samples diverged from such distribution are considered as anomalous.
Although samples in area (D) can be regarded as potential outliers, only areas (B) and (C) are used as
anomalies for the evaluation phase. In OSR, more supervision is available by accessing the label of normal
samples. For example "car", "dog", "cat", and "airplane" classes i.e, the union of (A) and (B), are considered
as normal while (C) is open-set distribution(see right Figure). Same as ND, (D) is not usually considered
an open-set distribution in the field, while there is no specific constraint on the type of open-set distribution
in the definition of OSR domain. In the OOD detection, multiple classes are considered as normal, which is
quite similar to the OSR. For example, (A), (B), and (C) comprise the normal training distributions, and
another distribution that shows a change to some extent with respect to the training distribution is said to
be out-of-distribution, which can be (D) in this case. (R1, R2, R3)
(also called "seen" or "normal") domain refers to the training data. In AD or ND, the label space Yis a binary
set, indicating normal vs. abnormal. During testing, provided with an input sample x, the model needs
to estimate P(Y=normal/seen/in-class |X=x)in the cases of one-class setting. In OOD detection and
OSR for multi-class classification, the label space can contain multiple semantic categories, the model needs
to additionally perform classification on normal samples based on the posterior probability p(Y=y|x).
It is worth mentioning that in AD, input samples could contain some noises (anomalies) combined with
normals; thus, the problem is converted into a noisy-label one-class classification problem; however, the
overall formulation of the detection task still remains the same.
To model the conditional probabilities, the two most common and known perspectives are called generative
modeling anddiscriminative modeling . While discriminative modeling might be easy for OOD detection
or OSR settings since there is access to the labels of training samples; nevertheless, the lack of labels makes
AD, ND (OCC) challenging. This is because one-class classification problems have a trivial solution to map
each of their inputs regardless of being normal or anomaly to the given label Yand, consequently, minimize
their objective function as much as possible. This issue can be seen in some of the recent approaches such as
DSVDDRuff et al. (2018), which maps every input to a single point regardless of being normal or abnormal
when it is trained for a large number of training epochs.
3Under review as submission to TMLR
Table 1: The table summarizes the most notable recent deep learning-based works in different fields, specified
by the informative methodological keywords and decision score used in each work. For the methods that have
common parts, a shared set of keywords are used to place them on a unitary ground as much as possible.
The pros and cons of each methodology are described in detail in the method explanation. (R2, R3)
Taxonomy Methods Refrence Decision Score Keywords
NDALOCC Sabokrou et al. (2018a) reconstruction error for input discriminative-generative, reconstruction-based, adversarial learning, denoising AutoEncoder, Refinement and Detection
Mem-AE Gong et al. (2019) reconstruction error for input generative, reconstruction-based, Memory-based Representation, sparse addressing technique, hard shrinkage operator
DeepSvdd Ruff et al. (2018) distance of the input to the discriminative, extension of SVDD, compressed features, minimum volume hyper-sphere, Autoencoder
center of the hypersphere
GT Golan & El-Yaniv (2018) sum of Dirichlet probabilities for different self-supervised learning, auxiliary task, self-labeled dataset, geometric transformations, softmax response vector
transformations of input
CSI Tack et al. (2020) cosine similarity to the nearest training sample self-supervised learning, contrastive learning, negative samples, shifted distribution, hard augmentations
multiplied by the norm of representation
Uninformed Students Bergmann et al. (2020) an ensemble of student networks’ knowledge distillation, teacher-student based, transfer learning, self supervised learning, Descriptor Compactness
variance and regression error
CutPaste Li et al. (2021) log-density of trained Gaussian density estimator self-supervised Learning, generative classifiers, auxiliary task, extracting patch-level representations, cutout and scar
Multi-KD Salehi et al. (2021b) discrepancy between the intermediate teacher and knowledge distillation, teacher-student, transfer the intermediate knowledge, pretrained network, mimicking different layers
student networks’ activation values
OSROpenMax Bendale & Boult (2016) after computing OpenMax probability per channel Extreme Value Theorem, activation vector, overconfident scores, weibull distribution, Meta-Recognition
maximum probability is then selected
OSRCI Neal et al. (2018) probability assigned to class with label k+ 1, minus generative, unknown class generation, classification based, Counterfactual Images, learning decision boundary
maximum probability assigned to first kclass
C2AE Oza & Patel (2019) minimum reconstruction error under different matchgenerative, Reconstruction based, class conditional Autoencoder, Extreme Value Theory, feature-wise linear modulations
vectors, compared with the predetermined threshold
CROSR Yoshihashi et al. (2019) after computing OpenMax probability per channeldiscriminative-generative, classification-reconstruction based , Extreme Value Theorem, hierarchical reconstruction nets
maximum probability is then selected
GDFR Perera et al. (2020) by passing the augmented input to the classifier discriminative-generative, self-supervised learning, reconstruction based, auxiliary task, geometric transformation
network, and finding maximum activation
OpenGan Kong & Ramanan (2021) the trained discriminator utilized as discriminative-generative, unknown class generation, adversarially synthesized fake data, crucial to use a validation set, model selection
an open-set likelihood function
MLS Vaze et al. (2021) negation of maximum logit score discriminative, classification based, between the closed-set and open-set, Fine-grained datasets, Semantic Shift Benchmark(SSB)
OODMSP Hendrycks & Gimpel (2016) Negation of maximum softmax probability outlier detection metric, softmax distribution, maximum class probability, classification based, various data domains
ODIN Liang et al. (2017) Negation of maximum softmax probability outlier detection metric, softmax distribution, temperature scaling, adding small perturbations, enhancing separability
with temperature scaling
A Simple Unified etc. Lee et al. (2018) Mahalanobis distance to the closest distance based, simple outlier detection metric, small controlled noise, class-incremental learning, Feature ensemble
class-conditional Gaussian
OE Hendrycks et al. (2018) negation of maximum softmax probability outlier exposure , auxiliary dataset, classification based, uniform distribution over kclasses, various data domains
Using SSL etc. Hendrycks et al. (2019a) negation of maximum softmax probability self-supervised learning, auxilary task, robustness to adversarial perturbations, MSP, geometric transformation
G-ODIN Hsu et al. (2020) softmax output’s categorical distribution outlier detection metric, softmax distribution, learning temperature scaling, Decomposed Confidence
Energy-based OOD Liu et al. (2020b) the energy function outlier detection metric, hyperparameter-free, The Helmholtz free energy , energy-bounded learning, the Gibbs distribution
(a scalar is derived from the logit outputs)
MOS Wang et al. (2021) lowest othersscore among all groups softmax distribution, group-based Learning, large-scale images, category others, pre-trained backbone
ReAct Sun et al. (2021) after applying ReAct different OOD detection methods activation truncation, reducing model overconfidence, compatible with different OOD scoring functions
could be used. they default to using the energy score.
Some approaches Golan & El-Yaniv (2018); Bergman & Hoshen (2020), however, have made some changes
in the formulation of P(Y|X)to solve this problem. They apply a set of affine transformations on the
distribution of Xsuch that the normalized distribution does not change. Then the summation/summationtext|T|
i=1P(Ti|
Ti(X))is estimated, calculating the aggregated probability of each transformation Tibeing applied on the
inputXgiven the transformed input Ti(X), which is equal to |T|P(Y|X). This is similar to estimating
P(Y|X)directly; however, it would not collapse and can be used instead of estimating one-class conditional
probability. This simple approach circumvents the problem of collapsing; however, it makes the problem
depend on the transformations since transformed inputs must not have an intersection with each other as
much as possible to satisfy the constraint of normalized distribution consistency. Therefore, as we will show
later, OSR methods can employ AD approaches with classification models to overcome their issues. A similar
situation holds for the OOD domain.
In generative modeling, AE (Autoencoder)-based, GAN (Generative Adversarial Network)-based, and ex-
plicit density estimation-based methods such as auto-regressive and flow-based models are used to model
the data distribution. For AEs, there are two important assumptions.
If the auto-encoder is trained solely normal training samples:
4Under review as submission to TMLR
•Theywouldbe able to reconstruct unseen normal test-time samples as precisely as training time
ones.
•Theywould not be able to reconstruct unseen abnormal test-time samples as precisely as normal
inputs.
Nevertheless, recently proposed AE-based methods show that the above-mentioned assumptions are not
always true Salehi et al. (2021a); Gong et al. (2019); Zaheer et al. (2020). For instance, even if AE can
reconstruct normal samples perfectly, nonetheless with only a one-pixel shift, reconstruction loss will be
high.
Likewise, GANs as another famous model family, are widely used for AD, ND, OCC, OSR, and OOD
detection. If a GAN is trained on fully normal training samples, it operates on the following assumptions:
•If the input is normal then there isa latent vector that, if it is generated, has a low discrepancy
with the input.
•If the input is abnormal then there is nota latent vector that, if it is generated, has a low
discrepancy with the input.
Here, the discrepancy can be defined based on the pixel-level MSE loss of the generated image and test time
input or more complex functions such as layer-wise distance between the discriminator’s features when it
is fed generated image and test-time input. Although GANs have proved their ability to capture semantic
abstractions of a given training dataset, they suffer from mode-collapse, unstable training process, and
irreproducible result problems Arjovsky & Bottou (2017).
Finally, auto-regressive and flow-based models can be used to explicitly approximate the data density and
detect abnormal samples based on their assigned likelihood. Intuitively, normal samples must have higher
likelihoods compared to abnormal ones; however, as will be discussed later, auto-regressive models even
assign a higher amount of likelihoods to abnormal samples while they have not seen any of them at their
training process, which results in a weak performance in AD, ND, OSR, and OOD detection. We will be
reviewing solutions to this problem in subsequent sections. To address this issue, several remedies have been
proposed in the OOD domain, which can be used in OSR, AD, and ND; however, considering the fact that
OOD detection’s prevalent testing protocols might be pretty different from other domains such as AD or
ND, more evaluations on their reliability are needed. In Table 1, a summary of the most notable papers
reviewed in this work is presented, and their contributions with respect to the methodological categorization
provided in the paper are specified. (R2, R3)
3 Non Deep Learning-Based Approaches (R3)
In this section, some of the most notable traditional methods used to tackle the problem of anomaly detection
are explained. All other sections are based on the new architectures that are mainly based on deep learning.
3.1 Isolation Forests Liu et al. (2008):
Like Random Forests, Isolation Forests (IF) are constructed using decision trees. It is also an unsupervised
model because there are no labels that have been predefined. Isolation Forests were created with the notion
that anomalies are "few and distinct" data points. An Isolation Forest analyzes randomly sub-sampled data
in a tree structure using attributes chosen at random. Because further-reaching samples required more
cuttings to separate, they are less likely to be outliers. Similarly, samples that end up in shorter branches
exhibit anomalies because the tree could distinguish them from other data more easily. Fig. 2 depicts the
overall procedure of this work.
5Under review as submission to TMLR
Figure 2: An overview of the isolation forest method is shown. Anomalies are more susceptible to isolation
and hence have short path lengths. A normal point xirequires twelve random partitions to be isolated
compared to an anomaly xothat requires only four partitions to be isolated.
3.2 DBSCAN Ester et al. (1996):
Given a set of points in a space, it combines together points that are densely packed together (points with
numerous nearby neighbors), identifying as outliers those that are isolated in low-density regions. The steps
of the DBSCAN algorithm are as follows: (1) Identify the core points with more than the minimum number
of neighbors required to produce a dense zone. (2) On the neighbor graph, find the related components of
core points, ignoring all non-core points. (3) If the cluster is a neighbor, assign each non-core point to it,
otherwise assign it to noise. Fig. 3 depicts the overall procedure of this work.
Figure 3: The minimal number of points required to generate a dense zone in this figure is four. Because the
area surrounding these points in a radius contains at least four points, Point A and the other red points are
core points (including the point itself). They constitute a single cluster because they are all reachable from
one another. B and C are not core points, but they may be reached from A (through other core points) and
hence are included in the cluster. Point N is a noise point that is neither a core nor reachable directly.
6Under review as submission to TMLR
3.3 LOF: Identifying Density-Based Local Outliers Breunig et al. (2000):
The local outlier factor is based on the concept of a local density, in which locality is determined by the
distance between K nearest neighbors. One can detect regions of similar density and points that have a
significantly lower density than their neighbors by comparing the local density of an object to the local
densities of its neighbors. These are classified as outliers. The typical distance at which a point can
be "reached" from its neighbors is used to estimate the local density. The LOF notion of "reachability
distance" is an extra criterion for producing more stable cluster outcomes. The notions of "core distance"
and "reachability distance", which are utilized for local density estimation, are shared by LOF and DBSCAN.
3.4 OC-SVMSchölkopf et al. (1999):
Primary AD methods would use statistical approaches such as comparing each sample with the mean of
the training dataset to detect abnormal inputs, which imposes an ungeneralizable and implicit Gaussian
distribution assumption on the training dataset. In order to reduce the number of presumptions or relieve
the mentioned deficiencies of traditional statistical methods, OC-SVM Schölkopf et al. (1999) was proposed.
As the name implies, OC-SVM is a one-class SVM maximizing the distance of training samples from the
origin using a hyper-plane, including samples on one side and the origin on its other side. Eq. 1 shows the
primal form of OC-SVM that attempts to find a space in which training samples lie on just one side, and
the more distance of origin to the line, the better solution is found for the optimization problem.
min
w,ρ,ξi1
2∥w∥2+1
vnn/summationdisplay
1ξi−ρ
subject to (w·Φ(xi))⩾ρ−ξi,i∈1,...,n
ξi⩾0,i∈1,...,n(1)
Finding a hyper-plane appears to be a brilliant approach for imposing a shared normal restriction on the
training samples. But unfortunately, because half-space is not compact enough to grasp unique shared
normal features, it generates a large number of false negatives. (R3) Therefore, similarly, Support Vector
Data Description(SVDD) tries to find the most compact hyper-sphere that includes normal training samples.
This is much tighter than hyper-plan, thus finds richer normal features, and is more robust against unwanted
noises as Eq. 2 shows. Both of the mentioned methods offer soft and hard margin settings, and in the former,
despite the latter, some samples can cross the border and remain outside of it even if they are normal. (R3)
OC-SVM has some implicit assumptions too; for example, it assumes training samples obey a shared concept,
which is conceivable due to the one-class setting. Also, it works pretty well on the AD setting in which the
number of outliers is significantly lower than normal ones however fails on high dimensional datasets.
min
R,aCn/summationdisplay
i=1ξi+R2
subject to∥ϕ(xi)−a∥2⩽R2+ξi,i∈1,...,n
ξi⩾0,i∈1,...,n(2)
4 Anomaly and Novelty Detection
Anomaly Detection (AD) and Novelty Detection (ND) have been used interchangeably in the literature,
with few works addressing the differences Perera et al. (2019); Xia et al. (2015); Wang et al. (2019). There
are specific inherent challenges with anomaly detection that contradict the premise that the training data
includes entirely normal samples. In physical investigations, for example, measurement noise is unavoidable;
asaresult, algorithmsinanunsupervisedtrainingprocessmustautomaticallydetectandfocusonthenormal
samples. (R3) However, this is not the case for novelty detection problems. There are many applications in
7Under review as submission to TMLR
which providing a clean dataset with minimum supervision is an easy task. While these domains have been
separated over time, their names are still not appropriately used in literature. We review them here in a
shared section.
Interests in anomaly detection go back to 1969 Grubbs (1969), which defines anomaly/outlier as “samples
thatappeartodeviatemarkedlyfromothermembersofthesampleinwhichitoccurs" , explicitly
assuming the existence of an underlying shared pattern that a large fraction of training samples follow. This
definition has some ambiguity. For example, one should define a criterion for the concept of deviation or
make the term “markedly" more quantitative.
To this end, there has been a great effort both before and after the advent of deep learning methods to make
the mentioned concepts more clear. To find a sample that deviates from the trend, adopting an appropriate
distance metric is necessary. For instance, deviation could be computed in a raw pixel-level input or in a
semantic space that is learned through a deep neural network. Some samples might have a low deviation from
others in the raw pixel space but exhibit large deviations in representation space. Therefore, choosing the
right distance measure for a hypothetical space is another challenge. Finally, the last challenge is choosing
the threshold to determine whether the deviation from normal samples is significant.
4.1 Anomaly Detection With Robust Deep Auto-encoders Zhou & Paffenroth (2017):
This work trains an AutoEncoder (AE) on a dataset containing both inliers and outliers. The outliers are
detected and filtered during training, under the assumption that inliers are significantly more frequent and
have a shared normal concept. This way, the AE is trained only on normal training samples and consequently
poorly reconstructs abnormal test time inputs. The following objective function is used:
min
θ||LD−Dθ(Eθ(LD))||2+λ||S||1
s.t.X−LD−S= 0,(3)
where E and D are encoder and decoder networks, respectively. LDis supposed to be the inlier part and S
the outlier part of the training data X. However, the above optimization is not easily solvable since Sandθ
needtobeoptimizedjointly. Toaddressthisissue, theAlternatingDirectionMethodofMultipliers(ADMM)
is used, which divides the objective into two (or more) pieces. At the first step, by fixing S, an optimization
problem on the parameters θis solved such that LD=X−S, and the objective is ||LD−Dθ(Eθ(LD))||2.
Then by setting LDto be the reconstruction of the trained AE, the optimization problem on its norm is
solved when Sis set to be X−LD. Since the L1norm is not differentiable, a proximal operator is employed
as an approximation of each of the optimization steps as follows:
proxλ,L1(xi) =

xi−λ x i>λ
xi+λ xi<−λ
0−λ≤xi≤λ

(4)
Such a function is known as a shrinkage operator and is quite common in L1optimization problems. The
mentioned objective function with ||S||1separates only unstructured noises, for instance, Gaussian noise
on training samples, from the normal content in the training dataset. To separate structured noises such
as samples that convey completely different meaning compared to the majority of training samples, L2,1
optimization norm can be applied as:
n/summationdisplay
j=1||xj||2=n/summationdisplay
j=1(m/summationdisplay
i=1|xij|2)1/2, (5)
with a proximal operator that is called block-wise soft-thresholding function Mosci et al. (2010). At the test
time, the reconstruction error is employed to reject abnormal inputs.
8Under review as submission to TMLR
4.2 Adversarially Learned One-Class Classifier for Novelty Detection (ALOCC) Sabokrou et al.
(2018a):
In this work, it is assumed that a fully normal training sample is given, and the goal is to train a novelty
detection model using them. At first, ( R) as a Denoising Auto Encoder (DAE) is trained to (1) decrease
the reconstruction loss and (2) fool a discriminator in a GAN-based setting. This helps the DAE to produce
high-quality images instead of blurred outputs Larsen et al. (2016). This happens because, on the one hand,
the AE model loss explicitly assumes independent Gaussian distributions for each of the pixels. And on the
other hand, true distributions of pixels are usually multi-modal, which forces the means of Gaussians to settle
in between different modes. Therefore, they produce blurry images for complex datasets. To address this
issue, AEs can be trained in a GAN-based framework to force the mean of each of Gaussians to capture just
one mode of its corresponding true distribution. Moreover, by using the discriminator’s output ( D) instead
of the pixel-level loss, normal samples that are not properly reconstructed can be detected as normal. This
loss reduces the False Positive Rate (FPR) of the vanilla DAE significantly. The objective function is as
follows:
LR+D= min
Rmax
D/parenleftbigg
EX∼pt[log(D(X))]+
E/tildewideX∼pt+Nσ([log(1−D(R(/tildewideX)))]/parenrightbigg
LR=||X−X′||2
L=LR+D+LR,(6)
whereX′is the reconstructed output of the decoder, and ptis the distribution of the target class (i.e.,
normal class). This helps the model to not only have the functionality of AEs for anomaly detection but
also produces higher quality outputs. Furthermore, detection can be done based on D(R(X))as mentioned
above. Fig. 4 depicts the overall architecture of this work.
An extended version of ALOCC, where the R(X)network has been formed as a Variational AE is presented
in Sabokrou et al. (2020). Besides, the ALOCC can not process the whole of its input data (images or
video frames) at one step and needs the test samples to divide into several patches. Processing the patches
makes the method computationally expensive. To address this problem, AVID Sabokrou et al. (2018b) was
proposed to exploit a fully convolutional network as a discriminator (i.e., D) to score (and hence detect) all
abnormal regions in the input frame/image all at once.
Figure 4: An overview of the ALOCC method is shown. This work trains an autoencoder that fools a
discriminator in a GAN-based setting. This helps the AE to make high-quality images instead of blurred
outputs. Besides, by using the discriminator’s output, a more semantic similarity loss is employed instead
of the pixel-level L2reconstruction loss.
9Under review as submission to TMLR
4.3 One-Class Novelty Detection Using GANs With Constrained Latent Representations (OC-GAN)
Perera et al. (2019):
As one of the challenges, AE trained on entirely normal training samples could reconstruct unseen abnormal
inputs with even lower errors. To solve this issue, this work attempts to make the latent distribution of the
encoder (EN (·)) similar to the uniform distribution in an adversarial manner:
llatent =−(Es∼U(−1,1)[log(Dl(s))]+
Ex∼px[log(1−Dl(EN(x+n)))]),(7)
wheren∼N(0,0.2), andDlis the latent discriminator. The discriminator forces the encoder to produce
uniform distribution on the latent space. Similarly, the decoder (De (·)) is forced to reconstruct in-class
outputs for any latent value sampled from the uniform distribution as follows :
lvisual =−(Es∼U(−1,1)[log(Dv(De(s)))]+
Ex∼pl[log(1−Dv(x))]),(8)
whereDvis called visual discriminator.
Intuitively, the learning objective distributes normal features in the latent space such that the reconstructed
outputs entirely or at least roughly resemble the normal class for both normal and abnormal inputs. Also,
another technique called informative negative sample mining is employed on the latent space to actively seek
regions that produce images of poor quality. To do so, a classifier is trained to discriminate between the
reconstructed outputs of the decoder and fake images, which are generated from randomly sampled latent
vectors. To find informative negative samples, the algorithm (see Fig. 5) starts with a random latent-space
sample and uses the classifier to assess the quality of the generated image. After that, the algorithm solves
an optimization in the latent space to make the generated image such that the discriminator detects it as
fake. Finally, the negative sample is used to boost the training process.
As in previous AE-based methods, reconstruction loss is employed in combination with the objective above.
Reconstruction error is used as the test time anomaly score.
4.4 Latent Space Autoregression for Novelty Detection (LSA) Abati et al. (2019):
This work proposes a concept called "surprise" for novelty detection, which specifies the uniqueness of input
samples in the latent space. The more unique a sample is, the less likelihood it has in the latent space,
and subsequently, the more likely it is to be an abnormal sample. This is beneficial, especially when there
are many similar normal training samples in the training dataset. To minimize the MSE error for visually
identical training samples, AEs often learn to reconstruct their average as the outputs. This can result in
fuzzy results and large reconstruction errors for such inputs. (R3) Instead, by using the surprise loss in
combination with the reconstruction error, the issue is alleviated. Besides, abnormal samples are usually
more surprising, and this increases their novelty score. Surprise score is learned using an auto-regressive
model on the latent space, as Fig. 6 shows. The auto-regressive model ( h) can be instantiated from different
architectures, such as LSTM and RNN networks, to more complex ones. Also, similar to other AE-based
methods, the reconstruction error is optimized. The overall objective function is as follows:
L=LRec(θE,θD) +λ·LLLK(θE,θh)
=EX/bracketleftbig
||x−ˆx||2−λ·log(h(z;θh))/bracketrightbig (9)
4.5 Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection (Mem-AE) Gong
et al. (2019):
This work challenges the second assumption behind using AEs. It shows that some abnormal samples could
be perfectly reconstructed even when there are not any of them in the training dataset. Intuitively, AEs
10Under review as submission to TMLR
Figure 5: The training process of OC-GAN method Perera et al. (2019).
Figure 6: An overview of the LSA method is shown. A surprise score is defined based on the probability
distribution of embeddings. The probability distribution is learned using an auto-regressive model. Also,
reconstruction error is simultaneously optimized on normal training samples.
may not learn to extract uniquely describing features of normal samples; as a result, they may extract
some abnormal features from abnormal inputs and reconstruct them perfectly. This motivates the need
for learning features that allow only normal samples to be reconstructed accurately. To do so, Mem-AE
employs a memory that stores unique and sufficient features of normal training samples. During training,
the encoder implicitly plays the role of a memory address generator . The encoder produces an embedding,
and memory features that are similar to the generated embedding are combined. The combined embedding
is then passed to a decoder to make the corresponding reconstructed output. Also, Mem-AE uses a sparse
addressing technique that uses only a small number of memory items. Accordingly, the decoder in Mem-AE
is restricted to performing the reconstruction using a small number of addressed memory items, rendering
the requirement for efficient utilization of the memory items. Furthermore, the reconstruction error forces
the memory to record prototypical patterns that are representative of the normal inputs. To summarize the
training process, Mem-AE (1) finds address Enc (x) =zfrom the encoder’s output; (2) measures the cosine
11Under review as submission to TMLR
similarityd(z,mi)of the encoder output zwith each of the memory elements mi; (3) computes attention
weights w, where each of its elements is computed as follows:
wi=exp(d(z,mi))/summationtextN
j=1exp(d(z,mj)); (10)
(4) applies address shrinkage techniques to ensure sparsity:
ˆwi=max(wi−λ,0).wi
|wi−λ|+ϵ(11)
E(ˆwt) =T/summationdisplay
i=1−ˆwi.log( ˆwi), (12)
and finally, the loss function is defined as (13), where Ris the reconstruction error and is used as the test
time anomaly score. Fig. 7 shows the overview of architecture.
L(θe,θd,M) =1
TT/summationdisplay
t=1/parenleftbig
R(xt,ˆxt) +αE(ˆwt)/parenrightbig(13)
Figure 7: An overview of the Mem-AE method is shown. Each sample is passed through the encoder, and a
latent embedding zis extracted. Then using the cosine similarity, some nearest learned normal features are
selected from the memory, and the embedding ˆzis made out of them using a weighted averaging. At last,
the reconstruction error of the decoded ˆzand input is considered as the novelty score.
4.6 Redefining the Adversarially Learned One-Class Classifier Training Paradigm (Old-is-Gold) Zaheer
et al. (2020):
This work extends the idea of ALOCC Sabokrou et al. (2018a). As ALOCC is trained in a GAN-based
setting, it suffers from stability and convergence issues. On one hand, the over-training of ALOCC can
confuse the discriminator Dbecause of the realistically generated fake data. On the other hand, under-
training detriments the usability of discriminator features. To address this issue, a two-phase training
process is proposed. In the first phase, a similar training process as ALOCC is followed :
L=LR+D+LR (14)
12Under review as submission to TMLR
As phase one progresses, a low-epoch generator model Goldfor later use in phase two of the training is saved.
The sensitivity of the training process to the variations of the selected epoch is discussed in the paper.
During the second phase, samples ˆX=Gare considered high-quality reconstructed data. Samples ˆXlow=
Gold(X)are considered as low-quality samples. Then, pseudo anomaly samples are created as follows:
ˆ¯X=Gold(Xi) +Gold(Xj)
2
ˆXpseudo=G(ˆ¯X)(15)
After that, the discriminator is trained again to strengthen its features by distinguishing between good
quality samples such as {X,ˆX}and low-quality or pseudo anomaly ones such as {ˆXlow,ˆXpseudo}as follows:
max
D/parenleftig
α·EX[log(1−D(X))] + (1−α)·EˆX[log(1−D(ˆX))]
+ (β·EˆXlow[log(1−D(ˆXlow))]
+ (1−β)·EˆXpseudo [log(1−D(ˆXpseudo))]/parenrightig(16)
Figure 8: An overview of the Old-Is-Gold method is shown. The architecture is similar to ALOCC; however,
it saves the weights of the network Gduring the training process, which are called Gold. Then,Goldis
employed to make some low-quality samples that the discriminator is expected to distinguish as normal.
Also, some pseudo abnormal samples are generated by averaging each pair of normal low-quality samples,
which the discriminator is supposed to distinguish as fake inputs. This makes the discriminator’s features
rich and stables the training process.
In this way, Ddoes not collapse similar to ALOCC, and D(G(X))is used as the test time criterion. Fig. 8
shows the overall architecture of this method.
4.7 Adversarial Mirrored Autoencoder (AMA) Somepalli et al. (2020):
The overall architecture of AMA is similar to ALOCC. However, it challenges the first assumption of AEs.
It has been shown that lpnorms are not suitable for training AEs in the anomaly detection domain since
they cause blurry reconstructions and subsequently increase the error of normal samples. To address this
problem, AMA proposes to minimize the Wasserstein distance between distributions PX,XandPX,ˆX. The
objective function is as follows:
13Under review as submission to TMLR
W(PX,X,PX,ˆX) = max
D∈Lip−1Ex∼PX[D(X,X )−D(X,ˆX)] (17)
where ˆXis the reconstructed image.
Berthelot et al. (2018) showed that by forcing the linear combination of latent codes of a pair of data points
to look realistic after decoding, the encoder learns a better representation of data. Inspired by this, AMA
makes use of ˆXinter—obtained by decoding the linear combination of some randomly sampled inputs:
min
Gmax
D∈Lip−1Ex∼PX[D(X,X )−D(X,ˆXinter)] (18)
To further boost discriminative abilities of D, inspired by Choi & Jang (2018) proposing normal samples
resideinthetypicalsetCover&Thomas(2006)whileanomaliesresideoutsideofit, aGaussianregularization
equal toL2norm is imposed on the latent representation. Then using the Gaussian Annulus Theorem
Vershynin (2018) stating in a d-dimensional space, the typical set resides with high probability at a distance
of√
dfrom the origin, synthetic negatives (anomalies), are obtained by sampling the latent space outside
and closer to the typical set boundaries. Therefore, the final objective function is defined as follows:
min
Gmax
D∈Lip−1Lnormal−λneg·Lneg
Lnormal =Ex∼PX/bracketleftig
D(X,X )−D(X,ˆX)
+λinter·(D(X,X )−D(X,ˆXinter)) +λreg·∥E(X)∥/bracketrightig
Lneg=Ex∼/tildewideQX/bracketleftig
D(X,X )−D(X,ˆXneg)/bracketrightig(19)
Fig. 9 shows an overview of the method and the test time criterion is ||f(X,X )−f(X,G(E(X))||1wheref
is the penultimate layer of D.
Figure 9: An overview of AMA method is shown. The architecture is similar to ALOCC; however, it does not
try to minimize reconstruction error between xandˆx. Instead, a discriminator is trained with Wasserstein
Loss to minimize the distance between the distribution (x,x)and(x,ˆx). In this way, it forces ˆxto be similar
toxwithout using any lpnorm that causes blurred reconstructed outputs. Moreover, some negative latent
vectors are sampled from low probability areas, which the discriminator is supposed to distinguish as fake
ones. This makes the latent space consistent compared to the previous similar approaches.
4.8 Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker
Discovery (AnoGAN) Schlegl et al. (2017):
This work trains a GAN on normal training samples, then at the test time, solves an optimization problem
that attempts to find the best latent space zby minimizing a discrepancy. The discrepancy is found by using
14Under review as submission to TMLR
a pixel-level loss of the generated image and input in combination with the loss of discriminator’s features
at different layers when the generated and input images are fed. Intuitively, for normal test time samples, a
desired latent vector can be found despite abnormal ones. Fig. 10 shows the structure of the method.
As inferring the desired latent vector through solving an optimization problem is time-consuming, some
extensions of AnoGAN have been proposed. For instance, Efficient-GAN Zenati et al. (2018) tries to sub-
stitute the optimization problem by training an encoder Esuch that the latent vectors z′approximate the
distribution of z. In this way, Eis used to produce the desired latent vector, significantly improving test
time speed. Fig. 11 shows differences. The following optimization problem (20) is solved at the test time to
findz. The anomaly score is assigned based on how well the found latent vector can minimize the objective
function.
min
z(1−λ)·/summationdisplay
|x−G(z)|+λ·/summationdisplay
|D(x)−D(G(z))| (20)
Figure 10: An overview of AnoGAN method. At first, a generator network Gand a discriminator Dare
trained jointly on normal training samples using the standard training loss, which yields a semantic latent
representation space. Then at the test time, an optimization problem that seeks to find an optimal latent
embedding zthat mimics the pixel-level and semantic-level information of the input is solved. Intuitively,
for normal samples, a good approximation of inputs can be found in the latent space; however, it is not
approximated well for abnormal inputs.
Figure 11: Figure A shows the architecture of AnoGAN. Figure B shows the architecture of Efficient-GAN.
The encoder Emimics the distribution of the latent variable z. The discriminator Dlearns to distinguish
between joint distribution of (x,z)instead ofx.
15Under review as submission to TMLR
4.9 Deep One-Class Classification (DeepSVDD) Ruff et al. (2018):
This method can be seen as an extension of SVDD using a deep network. It assumes the existence of
shared features between training samples, and tries to find a latent space in which training samples can be
compressed into a minimum volume hyper-sphere surrounding them. Fig. 12 shows the overall architecture.
The difference w.r.ttraditional methods is the automatic learning of kernel function ϕby optimizing the
parameters W. To find the center cof the hyper-sphere, an AE is first trained on the normal training
samples, then the average of normal sample latent embeddings is considered c. After that, by discarding the
decoder part, the encoder is trained using the following objective function:
min
W1
nn/summationdisplay
i=1||ϕ(xi;W)−c||2
2+λ
2L/summationdisplay
l=1||Wl||2
F, (21)
whereWlshows the weights of encoder’s lth layer. At the test time, anomaly score is computed based on
||ϕ(x;W∗)−c||2whereW∗denotes trained parameters.
Figure 12: An overview of the DSVDD method is shown. It finds a minimum volume hyper-sphere that
contains all training samples. As the minimum radius is found, more distance to the center is expected for
abnormal samples compared to normal ones.
4.10 Deep Semi-Supervised Anomaly Detection Ruff et al. (2019):
This is the semi-supervised version of DSVDD which assumes a limited number of labeled abnormal samples.
The loss function is defined to minimize the distance of normal samples from a pre-defined center of a hyper-
sphere while maximizing abnormal sample distances. The objective function is defined as follows:
min
W1
n+mn/summationdisplay
i=1||ϕ(xi;W)−c||2+
η
n+mm/summationdisplay
j=1(||ϕ( ˆxi;W)−c||2)ˆyj+λ
2L/summationdisplay
l=1||Wl||2
F(22)
Note that, as mentioned above, there is access to ( ˆx1,ˆy1),...,( ˆxm,ˆym)∈X×YwithY={−1,+1}where
ˆy= 1denotes known normal samples and ˆy=−1otherwise. cis specified completely similar to DSVDD
by averaging on the latent embeddings of an AE trained on normal training samples. From a bit theoretical
point of view, AE’s objective loss function helps encoder maximizes I(X;Z)thatXdenotes input variables
andZdenotes latent variables. Then, it can be shown that (22) minimizes the entropy of normal sample
latent embeddings while maximizing it for abnormal ones as follows:
16Under review as submission to TMLR
H(Z) =E[−log(P(Z)] =−/integraldisplay
Zp(z) logp(z)dz
≤1
2log((2πe)ddet(Σ))∝logσ2(23)
As the method makes normal samples compact, it forces them to have low variance, and consequently, their
entropy is minimized. The final theoretical formulation is approximated as follows:
max
p(z|x)I(X;Z) +β(H(Z−)−H(Z+)) (24)
4.11 Deep Anomaly Detection Using Geometric Transformations (GT) Golan & El-Yaniv (2018):
GT attempts to reformulate the one-class classification problem into a multi-class classification. GT defines a
set of transformations that do not change the data distribution, then trains a classifier to distinguish between
them. Essentially, the classifier is trained in a self-supervised manner. Finally, a Dirichlet distribution
is trained on the confidence layer of the classifier to model non-linear boundaries. Abnormal samples are
expectedtobeinlow-densityareassincethenetworkcannotconfidentlyestimatethecorrecttransformation.
At the test time, different transformations are applied to the input, and the sum of their corresponding
Dirichlet probabilities is assigned as the novelty score. An overview of the method is shown in Fig. 13.
Figure 13: An overview of the GT method is shown. A set of transformations is defined, then a classifier must
distinguish between them. Having trained the classifier in a self-supervised manner, a Dirichlet distribution
is trained on the confidence layer to model their boundaries.
4.12 Effective End-To-End Unsupervised Outlier Detection via Inlier Priority of Discriminative
Network Wang et al. (2019):
In this work, similar to GT, a self-supervised learning (SSL) task is employed to train an anomaly detector
except in the presence of a minority of outliers or abnormal samples in the training dataset. Suppose the
following training loss is used as the objective function:
LSS(xi|θ) =−1
KK/summationdisplay
y=1log(P(y)(x(y)
i|θ)), (25)
wherex(y)
iis obtained by applying a set of transformations O(.|y), andyindicates each transformation. The
anomaly detection is obtained based on the objective function score. We take the rotation prediction task
17Under review as submission to TMLR
as an example. During training, the classifier learns to predict the amount of rotation for normal samples.
During test time, different rotations are applied on inputs, the objective function scores for normal samples
would be lower than abnormal ones.
However, due to the presence of abnormal samples in the training dataset, the objective score for abnormal
samples may not always be higher. To address this issue, it is shown that the magnitude and direction of
gradient in each step have a significant tendency toward minimizing inlier samples’ loss function. Thus the
network produces lower scores compare to abnormal ones. Wang et al. (2019) exploits the magnitude of
transformed inliers and outliers’ aggregated gradient to update wc, i.e.||∇(in)
wcL||and||∇(out)
wcL||, which are
shown to follow this approximation:
E(||∇(in)
wc·L||2)
E(||∇(out)
wc·L||2)≈N2
in
N2
out(26)
whereE(·)denotes the probability expectation. As Nin≫Nout, normal samples have more effect on the
training procedure. Also, by projecting and averaging the gradient in the direction of each of the training
sample’s gradient (−∇θL(x)·−∇θL(xi)
||−∇θL(xi)||), the stronger effect of inlier distribution vs outlier one is observed
again. Fig. 14 shows the effect empirically.
Figure 14: The average magnitude of the gradient for inliers and outliers with respect to the number of
iterations. The class used as inliers is in brackets.
4.13 Classification-Based Anomaly Detection for General Data (GOAD) Bergman & Hoshen (2020):
This work is very similar to GT. It trains a network to classify between different transformations; however,
instead of using cross-entropy loss or training a Dirichlet distribution on the final confidences, it finds a
center for each transformation and minimizes the distance of each transformed data with its corresponding
center as follows :
P(m′|T(x,m)) =e−||f(T(x,m))−cm′||2
/summationtext
ˆme−||f(T(x,m))−cˆm||2 (27)
where the centers cmare given by the average feature over the training set for every transformation i.e.
cm=1
N/summationtext
x∈Xf(T(x,m)). For training f, two options are used. The first one is using a simple cross
entropy on P(m′|T(x,m))values, and the second one is using the center triplet loss He et al. (2018) as
follows:
/summationdisplay
imax(0,||f(T(xi,m))−cm||2+s
−min
m′̸=m||f(T(xi,m))−cm′||2)(28)
wheresis a margin regularizing the distance between clusters.
18Under review as submission to TMLR
The idea can be seen as the combination of DSVDD and GT in which GT’s transformations are used, and
different compressed hyperspheres are learned to separate them. Mdifferent transformations transform each
sample at the test time, and the average of correct label probabilities is assigned as the anomaly score.
4.14 CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances Tack
et al. (2020):
This work attempts to formulate the problem of novelty detection into a contrastive framework similar to
SimCLR Chen et al. (2020b). The idea of contrastive learning is to learn an encoder fθto extract the
necessary information to distinguish similar samples from the others. Let xbe a query, x+, andx−be
a set of positive and negative samples respectively, zbe the encoder’s output feature or the output of an
additional projection layer gϕ(fθ(x))for each input, and suppose sim (z,z′)is cosine similarity. Then, the
primitive form of the contrastive loss is defined as follows:
Lcon(x,x+,x−) :=−1
|x+|log/summationtext
x′∈x+esim(z(x),z(x′))/τ
/summationtext
x′∈x+∪x−esim(z(x),z(x′))/τ(29)
Specifically for SimCLR, the contrastive loss above is converted into the formulation below:
LSimCLR (B;T) :=1
2BB/summationdisplay
i=1Lcon(ˆx(1)
i,ˆx(2)
i,ˆB−i)
+Lcon(ˆx(2)
i,ˆx(1)
i,ˆB−i)(30)
where (ˆx(1)
i,ˆx(2)
i) = (T1(xi),T2(xi))for the transformations T1andT2from a set of transformations T,
B:={xi}B
i=1, and ˆB−i:={ˆx(1)
j}j̸=i∪{ˆx(2)
j}j̸=i.
However, contrastive learning requires defining a set of negative samples. To this end, a collection of trans-
formations that shift the distribution of training samples (S)is specified to make the desired negative set
when applied on each input. For instance, rotation or patch permutation completely shifts the original input
samples’ distribution; therefore, they can be used as negative samples. Also, another set of transformations
called align transformations (T)is defined as not changing the distribution of training images as much as
(S). Then the Contrasting shifted instances (CSI) loss can be defined as follows:
Lcon−SI:=LSimCLR/parenleftbig
∪s∈SBs;T/parenrightbig
(31)
whereBs:={s(xi)}B
i=1. Here, CSI regards each distributionally-shifted sample as an OOD with respect
to the original sample. The goal is to discriminate an in-distribution sample from other OOD i.e., (s∈S)
samples.
Further, to facilitate fθto discriminate each shifted instance, a classification loss for classifying shifted
instances is defined in combination with Lcon-SI. To do so, a linear layer for modeling an auxiliary softmax
classifierpcls-SI(yS|x)is added to fθas in GT or GOAD:
Lcls-SI =1
2B1
K/summationdisplay
s∈S/summationdisplay
ˆxs∈Bs−logpcls-SI(ys=s|ˆxs), (32)
where ˆBsis the batch augmented from Bs. In testing, the cosine similarity to the nearest training sample in
{xm}multiplied by the norm of representation ||z(x)||is used. The contrastive loss increases the norm of in-
distribution samples, as it is an easy way to minimize the cosine similarity of identical samples by increasing
the denominator of (29). To further improve the scoring criterion, ensembling over random augmentations
or utilizing shifting transformations similar to GOAD can be employed.
19Under review as submission to TMLR
4.15 DA-Contrastive: Learning and Evaluating Representations for Deep One-class Classification
Sohn et al. (2020): (R1)
This research tries to put the problem of novelty detection into a contrastive framework, similar to the
other methods mentioned previously in Tack et al. (2020) and Golan & El-Yaniv (2018). Tack et al. (2020)
investigates that all of the augmentations that are utilized in the contrastive loss presented at 29 are not
necessarily effective for the one-class learning. As a result, rotated versions of the input are treated as
separate distributions in this study, and are accounted as negative samples in the contrastive loss. Positive
samples are also created by applying augmentations to each distribution, such as color jittering. Finally, the
one-class classifier are trained on the positive and negative samples in a contrastive learning manner. After
that, a KDE or OC-SVM Schölkopf et al. (1999) is trained on the learned representation of normal training
samples, which is used at the test time. An overview of the method can be seen in Fig. 15.
 
Figure 15: An overview of the DA-Contrastive method is represented. This work trains a one-class classifier
employing a novel variant of contrastive learning with distribution augmentations such that the class col-
lision between examples from the same class is reduced. This implies discriminating different distributions
generated by augmentations of different rotations from the original input (see the right side), which results
in less uniformity and more compactness in the feature space (see the left side).
4.16 Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent
Embeddings Bergmann et al. (2020):
This work trains a teacher network using metric learning and knowledge distillation techniques to provide
a semantic and discriminative feature space. The teacher Tis obtained by first training a network ˆTthat
embeds patch-sized images pinto a metric space. Then, fast dense local feature extraction for an entire
input image can be achieved by a deterministic network transformation from ˆTtoT, as described in Bailer
et al. (2018). To train ˆT, a large number of training patches pare obtained by randomly cropping an image
database, for instance, ImageNet. Then using the following knowledge distillation loss, the knowledge of a
pre-trained network Pis distilled into ˆTas follows:
Lk(ˆT) =||D(ˆT(p))−P(p)||2, (33)
whereDis used to align the size of output spaces. This helps the use of computationally efficient network
ˆTinstead ofPat the test time while the required knowledge is distilled.
To further enrich the feature space of ˆT, an SSL method is employed such that the feature space of patches
that are obtained by applying small translations, small changes in the image luminance, and the addition
of Gaussian noise to pbe similar. This set is called p+as opposed to p−, which is obtained using random
cropping regardless of the neighborhood to the patch p. The SSL loss is defined as follows:
δ+=||ˆT(p)−ˆT(p+)||2
δ−= min{||ˆT(p)−ˆT(p−)||2,||ˆT(p+)−ˆT(p−)||2}
Lm(ˆT) = max{0,δ+δ+−δ−}(34)
20Under review as submission to TMLR
whereδ > 0denotes the margin parameter. Finally, to reduce the redundancy of feature maps of ˆT, a
compactness loss which minimizes their cross-correlation is used as follows:
Lc(ˆT) =/summationdisplay
i̸=jcij (35)
therefore, the final loss is Lc(ˆT) +Lm(ˆT) +Lk(ˆT).
Having a teacher Ttrained comprehensively to produce a ddimensional feature maps for each pixel of an
input image, an ensemble of student networks is forced to approximate the feature maps of Tfor each pixel
locating at the row rand column cas follows:
L(Si) =1
wh/summationdisplay
(r,c)||µSi
(r,c)−(yT
(r,c)−µ)diag(σ)−1||2
2 (36)
whereµandσare used for data normalization, note that the receptive field of each student is limited to a
local image region p(r,c), this helps the students to obtain dense predictions for each image pixel with only
a single forward pass without having actually to crop the patches p(r,c). At the test time, as students have
only learned to follow their teacher on normal training samples, their average ensemble error can be used to
detect abnormal samples. Intuitively, they wouldn’t follow their teacher on abnormal samples and produce
a high average error.
4.17 Self-Supervised Learning for Anomaly Detection and Localization (CutPaste) Li et al. (2021):
This work designs a simple SSL task that to capture local, pixel-level regularities instead of global, semantic-
level ones. While GT or GOAD utilize transformations such as rotation, translation, or jittering, CutPaste
cuts a patch of its training input and copies it in another location. The network is trained to distinguish
between defected samples and intact ones. Extra auxiliary tasks such as cutout and Scar can be used in
combination with the cut-paste operation. After training, a KDE or Gaussian density estimator is trained
on the confidence scores of normal training samples, which is used at the test time. Due to the simplicity of
the method, it may overfit easily on the classification task. However; several experiments in the paper show
contrary to this assumption. An overview of the method can be seen in Fig. 16.
Figure 16: An overview of CutPaste. A network is trained to classify artificially made defected samples from
the original ones. Despite the simplicity of the method, it does not overfit on MVTecAD dataset.
4.18 Multiresolution Knowledge Distillation for Anomaly Detection (Multi-KD) Salehi et al. (2021b):
Generative models are suited for detecting pixel-level anomalies; however, they may fail on complex,
semantic-level ones. In contrast, discriminative models are good at capturing semantics. Designing an
21Under review as submission to TMLR
SSL task that captures both semantic and syntax is not easy. To address this issue, Multi-KD attempts
to mimic the intermediate layers of a VGG pre-trained network—called intermediate knowledge—into a
simpler network using knowledge distillation. This way, multi-resolution modeling of the normal training
distribution is obtained and can be used to detect both pixel-level and semantic-level anomalies at the test
time. Here, the concept of knowledge is defined to be the length and direction of a pre-trained network on
ImageNet Deng et al. (2009). A cloner network has a simple yet similar overall architecture compared to the
source, making its knowledge similar to the source on normal training samples. At the test time, the cloner
can follow the source on the normal test time samples; however, it fails for the abnormal ones. This results
in a high discrepancy that can be used at the test time. Fig. 17 shows the overall architecture.
Similar methods exist, which model the distribution of different layers of a pre-trained network using multi-
variate Gaussian descriptors such as PaDiM Defard et al. (2020) or Rippel et al. (2021). An overview of the
architecture of Rippel et al. (2021) is shown in the Fig. 18.
Figure 17: An overview of the Multi-KD method is shown. It distills multi-resolution knowledge of a pre-
trained source network on the normal training distribution into a simpler cloner one. The structure of source
and cloner is similar to each other. Therefore, the cloner distills the source’s knowledge in the corresponding
layers. At the test time, their discrepancy would be low for normal inputs despite abnormal ones.
5 Open-set Recognition
Open-set recognition (OSR) receives more supervision than AD or ND. In this setting, Knormal classes
are given at the training time. In testing, Nclasses with N−Kunknown and Kknown classes exist. The
objective is to identify unknown classes while classifying the known ones. It has a lot of applications in
which labeling normal dataset is more feasible, or gathering a cleaned dataset without having any abnormal
sample is possible. As there is more supervision, training data can be categorized into four classes:
•known known classes (KKC): Training samples that we know they are known. They are already
given and labeled.
•known unknown classes (KUC): Training samples that we know they are not known. This means,
they do not belong to the known categories. For example, background images, or any image that
we know is not categorized into the known classes are in this group. They are already given and
labeled.
•unknown known classes (UKC): Trainingsamplesthatwedonotknowtheyareknown. Forexample,
known test time samples are in this group. These are not given at the training phase.
•unknown unknown classes (UUC): Training samples that we do not know they are not known. For
example, unknown test time samples are in this group. These are not given at the training phase.
The space that is far from known classes, including KKC and KUC, is called open space OScheirer et al.
(2012). Labeling any sample in this space has a risk value denoted by RO. The risk factor is usually
22Under review as submission to TMLR
Figure 18: An overview of Rippel et al. (2021). It models the different layers’ distribution of a pre-trained
network using Gaussian descriptors. Then, at the test time, the probability of being normal is computed
using the training time means and variances.
represented as Eq. 37 in which Sois the overall measuring space, and fis the measurable recognition
(classification) function Scheirer et al. (2012). The value of fis1if the input belongs to KKC otherwise 0.
RO(f) =/integraltext
Of(x)dx/integraltext
Sof(x)dx(37)
AsdiscussedinGengetal.(2020), therearesomedifficultiesindefiningthepracticalformulationofopenness;
therefore, we use the definition of Geng et al. (2020) as in Eq. 38, where CTRis the set of classes used in
training, and CTEis the set of classes used in the testing.
O= 1−/radicaligg
2×|CTR|
|CTR|+|CTE|(38)
Larger values of Ocorrespond to more open problems, and 0is assigned to a completely closed one. OSR
problem can be defined as finding recognition functions fin such a way that it minimizes the risk factor RO.
5.1 Towards Open Set Deep Networks (OpenMax) Bendale & Boult (2016):
This work addresses the problem that classification models produce overconfident scores for unseen test time
samples. Duetothenormalizationinsoftmaxcomputation, twosampleswithcompletelydifferentlogitscores
may have the same confidence score distribution. Instead of using the confidence score, OpenMax resorts
to the logit scores that are shown by Activation Vector (AV). AV of each sample captures the distribution
over classes. The mean AV (MAV) is defined to be the average of AV values across all samples. As for each
input sample, the value in AV corresponding to the ground truth is supposed to be high; its distance to
23Under review as submission to TMLR
the corresponding value of MAV would be high too. Considering the distance of each element in AVs from
the corresponding element in MAV as a random variable, correctly classified inputs would have the highest
distances for ground truth elements. This might happen for a few classes that are not ground truth while
having a strong relationship with ground truth. For instance, the class leopard as the ground truth and
cheetah as a near class. To model the distribution of these highest values, the Extreme Value Theorem
(EVT) can be used as follows:
EVT : Let ( s1,s2, ...,sn) be a sequence of i.i.d samples. Let Mn= max(s1, ...,sn). If a sequence of pairs
of real numbers ( an,bn) exists such that each 0≤anand
lim
n→∞P/parenleftbiggMn−bn
an≤x/parenrightbigg
=F(x) (39)
Then ifFis non-degenerate distribution function, it belongs to one of three extreme value distributions
Pickands III et al. (1975). For this work, it is Weibull distribution. By modeling the distribution of extremes
for each class, one can easily compute the probability of each test input being an extreme and discard
remaining ones.
In summary, during training, the CDF of extreme values for each class or element of AVs are approximated
using EVT and correctly classified training samples. For each test-time input sample, the top αelements
in AV are selected, representing ground truth and near classes; then, their CDFs are assigned to variables
ws(i)(x)as Fig. 19 shows. These probabilities represent the chance of each AV’s element vj(x)to be the
maximum. After that, an arbitrary class is added to the existing ones as the representative of unknown
unknown samples, let’s call class 0, and all activation values are normalized as follows:
ˆv(x) =v(x)◦w(x)
ˆv0(x) =/summationdisplay
ivi(x)(1−wi(x))(40)
The normalization is performed so that the activation vector of UUC would be high if ws(i)(x)s are small.
Finally, softmax is computed based on the normalized activation vector, and unknown or uncertain inputs
are rejected. All hyper-parameters such as ϵ,αare tuned using a validation set including both seen and
unseen samples.
5.2 Generative OpenMax for Multi-Class Open Set Classification (G-OpenMax) Ge et al. (2017):
This work is similar to OpenMax except for artificially generating UUC samples with GANs and fine-tuning
OpenMax. This removes the need to have a validation dataset. To generate UUCs, a conditional GAN is
trained on the KKCs as follows:
min
ϕmax
θ=Ex,c∼Pdata[log(Dθ(x,c))]
+Ez∼Pz,c∼Pc[log(1−Dθ(Gϕ(z,c),c))](41)
New samples are generated by interpolating the KKCs’ latent space. If the generated sample is not classified
as one of the mixing labels, the image is considered UUC. Finally, another classifier called NetG(see Fig.
20) is trained using both of the UUC and KKC samples such that UUCs can be assigned to the class 0of the
classifier. The inference steps are similar to OpenMax except that the normalization process for the class 0
and other classes is the same.
5.3 Open Set Learning with Counterfactual Images (OSRCI) Neal et al. (2018):
This work follows the idea of generating UUC samples as in G-OpenMax. A generated input is made
similar to a KKC yet not to be assigned to the same class. Such generated inputs are called counter-factual
24Under review as submission to TMLR
Figure 19: The inference algorithm of open-max. The class 0is added in addition to other classes and
considered as the unknown class; then, its probability is computed for each of the input samples, and the
probabilities of top k values are normalized with respect to the unknown class’s probability. At last, if the
most significant value is for class 0or the confidence of top kvalues is less than a specific amount, the sample
is discarded.
Figure 20: The overall architecture of G-OpenMax compared to the base OpenMax. As it is shown, every-
thing is completely the same except the unknown unknown sample generation, which is done using a GAN.
examples. As these samples are near the boundary of UUCs, they can better help approximate the real UUC
distribution.
Specifically, a classifier is trained on kclasses. A GAN-based method such as ALOCC is trained. The
discriminator is trained using a Wasserstein critic with gradient penalty as follows:
LD=/summationdisplay
x∈XD(G(E(x)))−D(x) +P(D)
LG=/summationdisplay
x∈X||x−G(E(x))||1−D(G(E(x))),(42)
whereP(D) =λ·(||∇ˆxD(ˆx)−1||2). The counter-factual samples are generated by optimizing a latent vector
z, which has a small distance to the latent vector of an arbitrary training sample xwhile its decoded image
produces low confidence scores for each of the known classes:
z∗= min
z||z−E(x)||2
2+ log/parenleftbig
1 +K/summationdisplay
i=1eCK(G(z))i/parenrightbig, (43)
25Under review as submission to TMLR
whereCKis the classifier and CK(G(z))iis the logit of the counterfactual image G(z)for classi. The
generated UUCs are used in combination with KKCs to train a new classifier CK+1, which is used at the
test time.
5.4 Reducing Network Agnostophobia Dhamija et al. (2018):
In applications such as object detection, there is usually a class called background. On the internet, a
large number of samples can be crawled, which can be used as “background” for a specific task. This work
employs background samples as an auxiliary KUC distribution to train a classifier. The training facilitates
KUC to have small feature magnitudes while KKCs have larger magnitudes with a defined margin. Also, the
entropyoftheconfidencelayerismaximizedforthebackgroundsamples, whichisequivalenttoincreasingthe
classifier’s uncertainty for such inputs. The training employs a simple entropic open-set loss that maximizes
the entropy of confidence scores, along with the objectosphere loss minimizing the L2norm of final features.
Fig. 21 shows the effect of each loss on the geometric location of each class’s samples at the final layer. At
the test time, the thresholding is based on Sc(x).||F(x)||, whereF(x)is the representation from penultimate
layer.
Figure 21: The effect of different loss functions on the last layer of a classifier. The classifier is trained on the
MNIST dataset. The background class is considered to be NIST letters Grother & Hanaoka (2016). Black
dots show samples from Devanagari Pant et al. (2012) as UUCs, and the gray lines show the boundaries of
different classes. (a) shows the last layer when the network is trained only with softmax loss. (b) shows
similar setting to (a); however, background samples are used as a separate class. (c) shows the last layer
when the network is trained with the objectosphere loss. The figures in the bottom are histograms of softmax
probability values for samples of KKCs with green color and UUCs with the red one.
5.5 Class Conditioned Auto-Encoder for Open-Set Recognition (C2AE) Oza & Patel (2019):
ThesecondpremisebehindemployingAEsisusedinthisstudy, whichstatesthatabnormaltesttimesamples
should not be reconstructed as well as normal ones. Nevertheless, despite AD and ND, in OSR, training
labels are used to increase AE abilities. (R3) Here, an AE is used as the meta-recognition function while
its encoder plays the role of a classifier for the recognition task. Intuitively, this work wants the encoder to
classify each passed sample correctly and provide embeddings by which the reconstruction of the original
input is possible. Furthermore, it imposes other constraints to force encoder embeddings not to be easily
converted to each other, e.g., by applying linear transformations, which prevents the AE from utilizing the
learned features to reconstruct abnormal/unseen inputs.
To this end, at first, the encoder that is a classifier is trained and fixed. Then for a given input Xia match
vectorlm=lym
i∈{− 1,1}KwhereKis the number of classes is defined such that lis equal to 1for the
yith element and−1otherwise. Similarly, some none-match vectors lnm=lynm
jfor any random ynm
j̸=yi
sampled randomly from labels are considered. After that, two neural networks HγandHβwith parameters
26Under review as submission to TMLR
ΘγandΘβare defined to receive match and none-match vectors and produce some linear transformations
by which the encoder embeddings are transformed. Finally, match transformations must be constructed
perfectly; however, none-match ones are forced to have high reconstruction loss as follows:
zi=F(Xi),
γym
i=Hγ(lym
i), γ ynm
i=Hγ(lynm
i)
βym
i=Hβ(lym
i), β ynm
i=Hβ(lynm
i)
zilm=γym
i◦zi+βym
i, zilnm=γynm
i◦zi+βynm
i
ˆXm
i=G(zlm). ˆXnm
i=G(zlnm).
Lm
r=1
NN/summationdisplay
i=1||Xi−ˆXm
i||1
Lnm
r=1
NN/summationdisplay
i=1||Xnm
i−ˆXnm
i||1(44)
where ˆXnm
is are sampled randomly based on randomly sampled none-match vectors, and the objective loss
is:
min
Θγ,Θβ,ΘGα·Lm
r+ (1−α)·Lnm
r (45)
The embedding transformation technique is called FiLM Perez et al. (2018). This means that a given input
would be reconstructed well only when the corresponding match vector is used. Therefore, for each test-time
input, the reconstruction error under different match vectors is computed, and the rejection decision is made
based on their minimum value. If the minimum value is greater than a threshold τit is discarded; else, the
encoder’s output is assigned. To obtain τin a principled way, the optimum threshold is found using EVT
on the distribution of match and none-match reconstruction error values i.e ||Xi−ˆXm
i||1and||Xi−ˆXnm
i||1
for eachiand randomly sampled none-match vectors. Assuming the prior probability of observing unknown
samples ispu, the probability of error as a function of threshold τis shown in Eq. 46 where GmandGmn
are the extreme value distributions of the match and none-match errors.
τ∗= min
τPerror(τ)
= min
τ[(1−pu)∗Pm(r≥τ) +pu∗Pnm(−r≤−τ)]
= min
τ[(1−pu)∗(1−Gm(τ)) +pu∗(1−Gnm(τ))](46)
5.6 Deep Transfer Learning For Multiple Class Novelty Detection (DTL) Perera & Patel (2019):
This work also follows the idea of using a background dataset (called reference dataset). Similar to Dhamija
et al. (2018), DTL addresses the deficiencies of using softmax loss in OSR. A new loss function called
membership loss isproposed. Specifically, eachactivationscorevalue fiofthepenultimatelayerisnormalized
into [0,1]using the sigmoid function. The normalized scores can be interpreted as the probability of the
input image belonging to each individual class. Ideally, given the label y,f(x)ishould be 1wheny=iand
0otherwise. The loss function is defined as Eq. 47.
LM(x,y) = [1−σ(f(x)y)]2+λ·1
c−1c/summationdisplay
i=1,i̸=y[σ(f(x)i)]2(47)
27Under review as submission to TMLR
Another technique for improving the detection performance is based on the “globally negative filters” . Filters
that provide evidence for a particular class are considered as positive filters and vice versa. For pre-trained
neural networks, it has been shown that only a small fraction of final feature maps are activated positively.
Furthermore, some filters are always activated negatively, indicating irrelevance for all known classes. By
discardinginputsthatactivategloballynegativefilters, anovelsampleislesslikelytoproducehighactivation
scores. To learn such filters for domain-specific task, DTL trains two parallel networks with shared weights
up to the last layer—the first one solves a classification task on the reference dataset, and the second one
solves the domain-specific classification tasks in combination with membership loss. If the reference and
domain-specific datasets do not share much information, they provide negative filters for each other. Also,
since the reference dataset consists of diverse classes, those learned filters can be considered globally negative
filters. Finally, filters of the parallel network in combination with the confidence scores of the domain-specific
classifier are used for novelty detection. Fig. 47 shows the overall network architecture.
Figure 22: An overview of DTL method. Two parallel networks are trained to produce confidence scores and
globally negative filters. The network above solves a simple classification task on the reference dataset, and
the one below solves membership loss in combination with the domain-specific classification task. Test time
detection is done by putting a threshold on the confidence of maximum class.
5.7 Classification-Reconstruction Learning for Open-Set Recognition (CROSR) Yoshihashi et al.
(2019):
This work follows the similar idea as C2AE. In particular, CROSR employs an encoder network for classi-
fication and producing the latent vectors for reconstruction task. Importantly, the latent vector zused for
the reconstruction task and penultimate layer yused for the classification task are not shared. The reason
is that there is an excessive amount of information loss in the penultimate layer, which makes distinguishing
between unknown and known samples hard. The overall procedure can be described as follows:
(y,z) =f(x),
p(Ci|x) =Softmaxi(y),
ˆx=g(z)(48)
Moreover, to preserve information at different levels of abstraction, each layer of the encoder is compressed
into a latent vector zi. The latent vector is then decoded to minimize the reconstruction error of the
corresponding layer as follows:
xl+1=fl(xl),
zl=hl(xl),
ˆxl=gl(ˆxl+1+ˆhl(zl))(49)
28Under review as submission to TMLR
whereflandglare layers of the encoder and decoder, respectively. ˆhlis a reprojection to the original space
ofxl. The autoencoding structure is based on a ladder network Rasmus et al. (2015).
The final latent vector zis the concatenation of each zi. EVT is employed as in OpenMax Bendale & Boult
(2016). However, the distance is not only computed on the penultimate layer ybut also on the latent vector
z. Specifically, EVT is applied on the joint [y,z]. Test-time detection is performed similar to OpenMax.
Fig. 23 shows an overview of the method.
Figure 23: An overview of the proposed method compared to similar methods in the literature. As it is
obvious, an encoder is used for the classification task, and the reconstruction task is done in a ladder network.
5.8 Generative-Discriminative Feature Representations For Open-Set Recognition (GDFR) Perera
et al. (2020):
Similar to CROSR, this work trains a discriminative model in combination with a generative one. Discrimi-
nativeapproachesmayloseimportantfeaturesthatareutilitarianfordistinguishingbetweenseenandunseen
samples. Generative modeling can provide complementary information. Similar to GT, GDFR employs SSL
to improve the features of the discriminator. A shared network performs both the classification and SSL
tasks, predicting the geometric transformations applied to the input.
Moreover,agenerativemodelsuchasAEisused,producingreconstructedoutputs ˆxforagiveninput x. Then
the collection of input-reconstruction pairs (x,ˆx)are passed to the discriminator network for classification
and SSL tasks. The disparity between ˆxandxfor unseen samples better helps the discriminator network
detect them. Fig. 24 shows the method.
5.9 Conditional Gaussian Distribution Learning for Open Set Recognition (CGDL) Sun et al. (2020):
The main idea of this research is very similar to CROSR. However, CGDL uses the probabilistic ladder
network based on variational encoding and decoding Sønderby et al. (2016). The overall encoding process
for thelth layer is as follows:
xl=Conv (xl−1)
hl=Flatten (xl)
µl=Linear (hl)
σ2
l=Softplus (Linear (hl))(50)
where the "Softplus" operation is log(1 +exp(·)); and, the final representation vector zis defined as µ+σ◦ϵ
whereϵ∼N(0,I). Similarly, for the decoding process we have:
29Under review as submission to TMLR
Figure 24: An overview of GDFR. As it can be seen, a classification network is trained on both SSL and
classification tasks to make the learned features richer. Also, to benefit from generative modeling, each input
is passed to an AE, and further tasks are applied on the joint of reconstructed and input images.
ˆcl+1=Unflatten (ˆzl+1)
ˆxl+1=ConvT (ˆcl+1)
ˆhl+1=Flatten (ˆxl+1)
ˆµl=Linear (ˆhl+1)
ˆσ2
l=Softplus (Linear (ˆhl+1))
q-µl=ˆµl+ ˆσ−2
l+µl+σ−2
l
ˆσ−2
l+σ−2
l
q-σ2
l=1
ˆσ−2
l+σ−2
l
ˆzl=q-µl+q-σ2
l◦ϵ(51)
During training, samples are passed into the encoder to estimate µandσfor each layer. The mean and
variance can be used as priors for the corresponding decoding layer. The final embedding zof the encoder’s
top layer is used for the joint classification task and decoding process. The distribution of the encoder’s final
layer is forced to be similar to different multivariate Gaussian pk
θ(z) =N(z;µk,I), wherekis the index of
known classes and µkis obtained by a fully-connected layer which maps the one-hot encoding of the input’s
label to the latent space. Each layer of the decoder is a Gaussian probability distribution in which a prior
of its mean and variance is added by the corresponding layer of encoder statistics. Putting it together, the
30Under review as submission to TMLR
training objective function is as follows:
LKL=−1
L/bracketleftig
KL/parenleftbig
qϕ(z|x,k)||pk
θ(z)/parenrightbig
+K−1/summationdisplay
l=1KL/parenleftbig
qθ(ˆxl|ˆxl+1,x)||qθ(ˆxl|ˆxl+1)/parenrightbig/bracketrightig
Lr=||x−ˆx||1
L=−(Lr+λ·Lc+β·LKL)(52)
whereLcis the classification error and
qθ(ˆxl|ˆxl+1,x) =N(ˆxl;q-µl,q-σ2
l)
qθ(ˆxl|ˆxl+1) =N(ˆxl; ˆµl,ˆσ2
l)(53)
At the test time, both the probability of final latent space with respect to pk
θ(z)and reconstruction error for
each test input are used for detection. If an input is preserved, the output of classification is considered as
the true class. As Fig. 25 shows, the latent vector zis considered as a prior of which the distribution of all
classes should have a low KL distance. This is similar to the role of N(0,I)in the baseline VAE.
Figure 25: An overview of CGDL method. Compared to CROSR a probabilistic ladder network is used
instead of a deterministic one; however, the classification task is similar. The probability distributions of
the decoder network contain a prior from the corresponding layer statistics of the encoder network. Also,
Despite VAE in which there is a prior N(0,I)on the latent space; here, the prior is learned, and each class
distribution is tried to have a low KL distance with it.
5.10 Hybrid Models for Open Set Recognition Zhang et al. (2020):
In this work, a classification network is trained in combination with a flow-based generative model. Genera-
tive models in the pixel-level space may not produce discernible results for unseen and seen samples, and they
31Under review as submission to TMLR
are not robust against semantically irrelevant noises. To address this issue, a flow-based model is applied to
the feature representation space instead of pixel-level space (see Fig. 26). The reason for using flow-based
models is their handy and comprehensive theoretical abilities. The training loss is a simple cross-entropy
loss in combination with negative log-likelihood used for the training of the flow-based model. At the test
time, the thresholding is applied to the likelihood of each input, and if it is preserved, the classifier’s output
is assigned as the in-class label.
Figure 26: An overview of the hybrid models for open set recognition. As it can be seen, a classification
network is trained in combination with a generative flow-based model. At the test time, the probability of
the latent vector is considered as the criterion of rejection, and the classifier assigns a label if the input is
not rejected.
5.11 Learning Open Set Network With Discriminative Reciprocal Points (RPL) Chen et al. (2020a):
Similar to Mem-AE, the idea of prototype features is used in this work. The goal is to learn a set of
prototypes or reciprocal points, which can assign labels to each input based on the distance w.r.t each
prototype. RPL helps the model better adjust the boundaries of different classes compared to softmax or
OpenMax, and decreases the risk factor. Initially, random reciprocal points are chosen. The location of
reciprocal points and the weights of a classifier network are adjusted to minimize the classification loss. This
forces the network to locate features of each class near some specific reciprocal points to yield the desired
class boundary using at least a set of points. To decrease the risk factor, samples of each class are forced to
have a margin with respect to their reciprocal points, which is learned during the training process. Eq. 54
shows the mathematical formulation:
d(fθ(x),Pk) =1
MM/summationdisplay
i=1||fθ(x)−pk
i||2
2
p(y=k|x,fθ,P) =eγd(fθ(x),Pk)
/summationtextN
i=1eγd(fθ(x),Pi)
Lo=1
MM/summationdisplay
i=1||d(fθ(x),pk
i)−Rk||2
2,(54)
wherePkisthesetof kthclassreciprocalpoints, pk
iisareciprocalpoint, Misthenumberofreciprocalpoints
for each class, Nis the number of classes, Rkis the margin for each class, and γis a tunable hyper-parameter.
5.12 A Loss for Distance-Based Open Set Recognition (CAC) Miller et al. (2021):
The idea of this work is similar to RPL and GOAD. For each class, CAC defines an anchor vector of
dimensionN—the number of classes. For each vector, the element corresponding to the class label is 1and
0otherwise. For each training sample, the learning process forces its logit scores to be in a compact ball
w.r.t true class anchor vector while having a large distance from anchors of other classes. CAC can also be
seen as a multi-class DSVDD. The training loss function is as follows.
32Under review as submission to TMLR
d=e(z,C) = (||z−c1||2,...,||z−cN||2)
LT(x,y) = log(1 +N/summationdisplay
j̸=yedy−dj)
LA(x,y) =dy=||f(x)−cy||2
LCAC(x,y) =LT(x,y) +λ·LA(x,y)(55)
5.13 Few-Shot Open-Set Recognition Using Meta-Learning (PEELER) Liu et al. (2020a):
In this work, the idea of meta-learning is combined with open set recognition. Meta-learning enable learning
general features that can be easily adapted to any unseen task. Meta-learning is also called learning to
learn. Due to the ability to work in few-shot settings, meta-learning can be useful in low data regimes. At
the meta iteration i, meta-model his initialized with the one produced by the previous meta-iteration. Let
(Ss
i,Ts
i)Ns
i=1be a meta-training dataset with Nsnumber of training problems, two steps are performed. First,
an estimate h′of the optimal model for the training set Ss
iis produced. Then the test set Ts
iis used for
finding a model with a suitable loss function Las the Eq. 56 shows.
h∗=arg minh/summationdisplay
(xk,yk)∈Ts
iL[yk,h′(xk)] (56)
To adopt meta-learning in OSR, a classification loss in combination with open-set loss is used. Moreover,
the test set Ts
iis augmented with some unseen samples. The overall loss function is defined as Eq. 57, where
Lcis a simple classification loss, and Lomaximizes the entropy of unknown samples on the known classes.
h∗=arg minh/braceleftbigg/summationdisplay
(xk,yk)∈Cs
i∈Ts
i|ykLc[yk,h′(xk)]
+λ·/summationdisplay
(xk,yk)∈Ts
i|yk∈Cu
iLo[h′(xk)]/bracerightbigg (57)
At the test time, the average features of correctly classified samples is obtained as a prototype point and
used for the rejection of unseen samples.
5.14 Learning Placeholders for Open-Set Recognition (PROSER) Zhou et al. (2021):
This work tries to train a classifier that can place between target class and non-target classes. A dummy
classifier is added to the softmax layer of the model with a shared feature extractor. Then it is forced to
have the second maximum value for the correctly classified samples. When the classifier encounters novel
inputs, the dummy classifier produces high values since all known classes are non-targets. Dummy classifier
can be seen as the instance-dependent threshold which can well fit every known class. The loss function is
defined as the Eq. 58, where ˆf((x)/ymeans making the most probable class zero.
L1=/summationdisplay
(x,y)∈Dtrl(ˆf(x),y) +βl(ˆf((x)/y,k + 1) (58)
Moreover, the mixup technique Verma et al. (2019) is added to the loss function to boost unseen samples
detection. The mixup representations are introduced to approximate the unseen sample’s distribution and
should be classified as the dummy class k+ 1. Finally, rejecting each test time sample is done based on the
probability of the dummy class.
33Under review as submission to TMLR
5.15 Counterfactual Zero-Shot and Open-Set Visual Recognition Yue et al. (2021):
This work attempts to make abnormal samples in a counter-factual faithful way. As the paper mentions,
most of the generative approaches such as G-OpenMax do not produce desired fake samples, the distributions
of which do not resemble real distribution of unseen samples. To this end, a β-VAE Higgins et al. (2016) is
used to make sample attribute variable Zand the class attribute variable Yindependent. The β-VAE loss
function is similar to simple VAE; however, the KLterm is induced by a coefficient β. This is shown to
be highly effective in learning a disentangled sample attribute ZHiggins et al. (2016). For disentangling Y
fromZ, the proposed approach makes counter-factual samples by changing the variable Yto have a large
distance with the given input xin spite of samples that are generated by changing the variable Z. To make
counter-factual samples faithful, a Wasserstein GAN Arjovsky et al. (2017) loss is used for a discriminator
D(X,Y ), which verifies the correspondence between the generated counter-factual image and the assigned
label. At last, generated samples can be used to boost the performance of any OSR problem.
5.16 OpenGAN: Open-Set Recognition via Open Data Generation Kong & Ramanan (2021): (R1)
Thisworkarguesthatusinganeasilyaccessibleauxiliaryoutlierdatasetcanimprovetheopen-setrecognition
performance as mentioned in Ruff et al. (2019) and Hendrycks et al. (2018). However, the auxiliary dataset
might poorly generalize to diverse open-set data that are even likely to be faced. Therefore, close to Zaheer
et al. (2020) a GAN-based approach is employed to generate fake samples that are supposed to mimic the
distribution of the open-set data. Although, as mentioned in Zaheer et al. (2020), GAN-based methods
suffer from instability when used in the one-class training setup, in the OSR setup, labeled closed-set data
is available which can be used to find the best stopping point during training. Furthermore, fake samples
can be generated according to the feature space of closed-set samples passed through a pre-trained K-
way classification model, which significantly reduces the dimensionality of the problem and helps stability.
This work shows that a well-selected GAN-discriminator using a validation set achieves SOTA on OSR.
Fig. 27 shows that at first, a closed-set classifier is trained on the given training dataset and is kept fixed
afterward. Then, a GAN-based framework is trained on the embeddings of the closed-set and outlier images,
which forces the generator to produce semantically fake outputs that are hard to be detected as fake for
the discriminator. Eq. 59 shows the mathematical formulation of the loss function, where λGcontrols the
contribution of generated fake samples generated by G, and λoadjusts the effect of auxiliary outlier dataset.
max
Dmin
GEx∼Pclosed/bracketleftig
log(D(x))/bracketrightig
+λo·Eˆx∼Popen/bracketleftbig
log(1−D(ˆx)/bracketrightbig
+λG·Ez∼N/bracketleftbig
log(1−D(G(z))/bracketrightbig
(59)
Figure 27: An overview of the OpenGAN method. Unlike pure GAN-based methods or Outlier Exposure,
fake samples are generated in the latent space while a discriminator is trained to distinguish between the
closed-set samples’ embedding and outlier inputs. Closed-set embeddings are produced using a trained K-
way closed-set classification model that is kept frozen afterward. In this way, the fake generator Gis pushed
towards making features, matching outlier images’ embeddings.
5.17 Open-set Recognition: A Good Closed-set Classifier Is All You Need? Vaze et al. (2021): (R1)
This work provides a comprehensive investigation through the correlation of closed-set accuracy and open-set
performance of different deep architectures. It is shown that not only the closed-set accuracy of a model is
34Under review as submission to TMLR
highly correlated with its OSR performance, but also this correlation holds across a variety of loss objectives
and architectures. This is surprising because stronger closed-set classifiers are more likely to overfit to the
training classes and perform poorly in OSR. Fig. 28 illustrates the mentioned relationship for different
architectures and within the ResNet family architectures. Another contribution of this work is providing
a Semantic Shift Benchmark (SSB), which unlike common approaches of utilizing a single test set for the
evaluation, makestwo’Easy’and’Hard’setsbasedonthesemanticsimilarityoftheopen-setcategoriestothe
training classes. The ability of a model to detect semantic novelty as opposed to the low-level distributional
shift is better captured in this manner.
Figure 28: (a) The performance of different deep model architectures on the ImangeNet dataset. The
ImageNet-21K-P dataset is used to generate ’Easy’ and ’Hard’ OSR splits. (b) The performance of different
ResNet families on the ImageNet.
6 Out-of-distribution Detection
OOD detection aims to identify test-times samples that are semantically different from the training data
categories, and therefore should not be predicted into the known classes. For instance, one could train the
model on CIFAR-10 (as in-distribution data), and then evaluating on CIFAR-100 Netzer et al. (2011) as an
out-of-distribution dataset, as CIFAR-10 and CIFAR-100 have mutually exclusive classes. In the multi-class
setting, the problem of OOD detection is canonical to OSR: accurately classifying samples from the known
classes while detecting the unknowns. However, OOD detection encompasses a broader spectrum of learning
tasks (e.g., multi-label classification) and solution space (e.g., density estimation without classification).
Some approaches relax the constraints imposed by OSR and achieve strong performance. We next review
some of recent OOD detection works and their differences.
6.1 A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks
Hendrycks & Gimpel (2016):
This work coined “out-of-distribution detection” and showed how to evaluate deep learning out-of-
distribution detectors. Whereas previous anomaly detection works for deep classifiers often had low-quality
or proprietary datasets, this repurposed existing datasets to create out-of-distribution datasets, enabling
easier evaluation. It proposes using the maximum softmax probability (MSP) to detect out-of-distribution
samples, namely maxkp(y=k|x). A test sample with a large MSP score is detected as an in-distribution
(ID) example rather than out-of-distribution (OOD) example. It showed that a simple maximum probability
score can be useful for detection in vision, natural language processing, and speech recognition settings, but
there is much room for improvement. It also showed p(y|x)models can be useful for out-of-distribution
detection and that p(x)models are not necessarily needed. Until now, it still serves as a general-purpose
baseline that is nontrivial to surpass. Concurrent OSR work proposed additional modifications to softmax
probabilities for detection Bendale & Boult (2016).
35Under review as submission to TMLR
6.2 Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks (ODIN)
Liang et al. (2017):
In this work, a technique called temperature scaling was employed. Although it has been used in other
domains such as knowledge distillation Hinton et al. (2015), the main novelty of this work is showing the
usefulness of this technique in the OOD domain. In temperature scaling, the softmax score is computed
as in Eq. 60. OOD samples are then detected at the test time based on thresholding the maximum class
probability. This simple approach, in combination with adding a small controlled noise, has shown significant
improvement compared to the baseline approach MSP. ODIN further shows that adding one step gradient
to the inputs in the direction of improving the maximum score has more effect on the in-class samples, and
pushes them to have larger margin with the OOD samples.
Si(x;T) =exp(fi(x)/T)/summationtextN
j=1exp(fj(x)/T)(60)
Thepaperalsoprovidedmathematicalexplanationfortheeffectoftemperaturescalingonout-of-distribution
detection. This can be seen in the Taylor approximation of the softmax score (expanded around the largest
logit output fˆy(x)):
Si(x;T) =exp(fi(x)/T)/summationtextN
j=1exp(fj(x)/T)
=1/summationtextN
j=1exp(fj(x)−fi(x)
T)
≈1
N−1
T/summationtextN
j=1[fˆy(x)−fj(x)] +1
2T2/summationtextN
j=1[fˆy(x)−fj(x)]2(61)
A sufficiently large temperature Thas a strong smoothing effect that transforms the softmax score back
to the logit space—which effectively distinguishes ID vs. OOD. In particular, the ID/OOD separability is
determinedbythe U1=/summationtextN
j=1,j̸=ˆy[fˆy(x)−fj(x)]andU2=/summationtextN
j=1,j̸=ˆy[fˆy(x)−fj(x)]2. Theformermeasuresthe
extent to which the largest unnormalized output of the neural network deviates from the remaining outputs;
while the latter measures the extent to which the remaining smaller outputs deviate from each other. For the
in-class samples, U1andE[U2|U1]are higher than OOD ones. Mathematically and empirically, ODIN is
not sensitive to Twhen it is large enough to satisfy the Taylor approximation. For example, the paper shows
that simply applying T= 1000can yield effective performance boost without hyper-parameter tuning. In
general, using temperature scaling can improve the separability more significantly than input preprocessing.
Note that ODIN differs from confidence calibration, where a much milder Tis employed. While calibration
focuses on representing the true correctness likelihood of ID data only, the ODIN score is designed to
maximize the gap between ID and OOD data and may no longer be meaningful from a predictive confidence
standpoint. As seen in Fig. 29, the ODIN scores are closer to 1/NwhereNis the number of classes.
Score distributionBaselineTemperature scaling only
Temperature scaling
& Input preprocessing
1/N 1
Softmax scoreIn-distribution
Out-distribution
Figure 29: An overview of ODIN, a post-hoc method that uses temperature scaling and input perturbation
to amplify the ID/OOD separability.
36Under review as submission to TMLR
6.3 A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks
Lee et al. (2018):
This work was inspired from the idea of Linear Discriminant Analysis (LDA) in which P(X=x|Y=y)
is considered to be a multivariate Gaussian distribution. In order for P(Y=y|X=x)to be similar to a
softmax form, it is assumed that the feature space of the penultimate layer follows the Gaussian distribution.
Therefore, a mean and variance vector is simply estimated from features of each class, and a multivariate
Gaussian is fit to them. In order to check validity of the assumptions, it uses the Mahalanobis distance of
the test time images to perform the classification instead of the softmax function. Surprisingly, the results
are comparable or better than softmax, which supports the assumptions. It performs OOD detection using
Mahalanobis distance to the closest class-conditional Gaussian distribution. Furthermore, to improve the
performance, features in different layers are ensembled and a small controlled noise is added to test samples
as shown in Eq. 62, where M(x)is the Mahalanobis distance with mean of the closest class-conditional
Gaussian distribution. A similar idea has been discussed earlier in Rippel et al. (2021).
ˆx=x+ϵ·sign(∇xM(x)) (62)
6.4 Predictive Uncertainty Estimation via Prior Networks (DPN) Malinin & Gales (2018):
This work discusses three different sources of uncertainty: (1) data uncertainty, (2) distributional uncer-
tainty, and (3) model uncertainty. The importance of breaking down the final uncertainty into these terms
was discussed. For instance, model uncertainty might happen because of the model’s lack of capacity to
approximate the given distribution well. On the other hand, data uncertainty might happen because of the
intrinsic intersection of similar classes. For instance, classifying between different kinds of dogs has more
data uncertainty than solving a classification problem with completely separate classes. Distributional un-
certainty is related to the problem of AD, ND, OSR, and OOD detection. The goal of this work was to
estimate the distributional uncertainty for each input and compare it with the data and model uncertainties.
Data uncertainty P(wc|x∗,θ)can be defined by the posterior distribution over class labels given a set of
parameterθ. Model uncertainty P(θ|D)is defined by the posterior distribution over the parameter given
dataD. These two types of uncertainties could be combined to give rise to the distributional uncertainty,
as shown in Eq. 63.
P(wc|x∗,D) =/integraldisplay
P(wc|x∗,θ)P(θ|D)dθ (63)
As computing the integral is not tractable, the above formula is usually converted into the Eq. 64, where
q(θ)is an approximation of P(θ|D).
P(wc|x∗,D)≈1
MM/summationdisplay
i=1P(wc|x∗,θi),θi∼q(θ) (64)
EachP(wc|x∗,D)can then be seen as a categorical distribution located on a simplex, as shown in Fig. 30.
Figure 30: The figure shows a distribution over the categorical distributions for modeling uncertainty using
both model uncertainty and data uncertainty.
Nowtoextractdistributionaluncertaintyfromthemodeluncertainty,Eq. 64isdecomposedintothefollowing
Eq. 65.
37Under review as submission to TMLR
P(wc|x∗,D) =/integraldisplay/integraldisplay
P(wc|µ)P(µ|x∗,θ)P(θ|D)dµdθ, (65)
whereP(wc|µ)is a categorical distribution given a realization from a Dirichlet distribution, P(µ|x∗,θ)
is the Dirichlet distribution given the input and model parameters θ, andP(θ|D)is the distribution over
model parameters given the dataset D. For simplicity, in this work P(θ|D)is equal to δ(θ−ˆθ)and
is produced by the output of a deep network. Therefore P(µ|x∗,θ) =P(µ|x∗,ˆθ) =Dir(µ|α), and
α=f(x∗,ˆθ), wheref(.,.)is represented by a deep neural network.
At the training time, the Dirichlet Prior Network (DPN) is expected to yield a flat distribution over the
simplex for OOD samples, indicating large uncertainty in the mapping from xtoy. Some out-of-distribution
data is used to minimize the KL distance of Dir (µ|α)and the flat Dirichlet distribution. For the in-class
samples, the KL divergence between Dir (µ|α)and a sharp, sparse Dirichlet distribution is minimized. The
objective Dirichlet distributions are obtained by pre-setting their parameters during training process. During
test time, different criterion such as max probability, last layer’s entropy, and distributional uncertainty as
in Eq. 66 are used for OOD detection.
I(y,µ|x∗,D) =H[EP(µ|x∗,D)[P(y|µ)]]
−EP(µ|x∗,D)[H[P(y|µ)]](66)
6.5 Confidence-calibrated Classifiers for Detecting Out-of-distribution Samples Lee et al. (2017):
This work attempted to maximize the entropy of confidence scores for OOD samples, similar to Dhamija
et al. (2018). Similar to Neal et al. (2018), it generates OOD samples by jointly training a GAN and a
classifier. As shown in Eq. 67, the first term solves a classification task on in-class samples, the second
term uses KL divergence to make the confidence score distribution of generated OOD samples uniform. The
remainder terms train the GAN on the in-class samples. Note that, the GAN is forced to generate high-
quality OOD samples that produce high uncertainty when they are passed to the classifier. Therefore, the
generated samples are located on the boundaries of in-class and outlier distributions. The paper also shows
that leveraging on-boundary in-class samples significantly improves its confidence calibration.
min
Gmax
Dmin
θEPin(ˆx,ˆy)[−logPθ(y= ˆy|ˆx)]
+βEPG(x)[KL(U(y)||Pθ(y|x))] +EPin(ˆx)[logD(x)]
+EPG(x)[log(1−D(x))](67)
Test time OOD detection is done based on thresholding of the maximum softmax value.
6.6 Deep Anomaly Detection with Outlier Exposure (OE) Hendrycks et al. (2018):
This work introduced Outlier Exposure (OE) and reported extensive experiments on its usefulness for various
settings. When applied to classifiers, the Outlier Exposure loss encourages models to output a uniform
softmax distribution on outliers, following Lee et al. (2017). More generally, the Outlier Exposure objective
is
E(x,y)∼Din[L(f(x),y) +λ·Ex′∼DOE
out[LOE(f(x′),f(x),y)]],
assuming a model f, the original learning objective L; when labeled data is not available, ycan be ignored.
Models trained with this objective can have their maximum softmax probabilities Hendrycks & Gimpel
(2016) better separate in- and out-of-distribution examples. To create DOE
out, data unlike the training data
will need to be scraped or curated or downloaded. Samples from DOE
outare gathered from already existing and
available datasets that might not be directly related to the task-specific objective function; however, they
can significantly improve the performance because they contain many diverse variations. Concurrent work
Dhamija et al. (2018) explores a similar intuition for small-scale image classification, while Outlier Exposure
shows how to improve OOD detection for density estimation models, natural language processing models,
and small- and large-scale image classifiers.
38Under review as submission to TMLR
6.7 Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty Hendrycks et al.
(2019a):
This work investigated the benefits of training supervised learning tasks in combination with SSL methods
in improving robustness of classifiers against simple distributional shifts and OOD detection tasks. To do
so, an auxiliary rotation prediction was added to a simple supervised classification. The work measures
robustness against simple corruptions such as Gaussian noise, shot noise, blurring, zooming, fogging etc. It
has been observed that although auxiliary SSL tasks do not improve the classification accuracy, the model’s
robustness and detection abilities are significantly improved. Additionally, when the total loss function
is trained in an adversarially robust way, the robust accuracy is improved. Finally, the method is tested
in the ND setting using rotation prediction, and horizontal and vertical translation prediction similar but
simpler than GT or GOAD. They also test in the setting of multiclass classification setting and find that
auxiliary self-supervised learning objectives improves the maximum softmax probability detector Hendrycks
& Gimpel (2016). In addition, similar to Dhamija et al. (2018) and Lee et al. (2017), they attempted to make
distribution of the confidence layer for some background or outlier samples uniform. Outliers are selected
from other accessible datasets, as in Outlier Exposure Hendrycks et al. (2018).
6.8 Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy Yu & Aizawa
(2019):
This work relies on a surprising fact—two classifiers trained with different random initializations can act
differently on unseen test time samples at their confidence layers. Motivated by this, the work attempts
to increase the discrepancy on unseen samples and reduce the discrepancy on seen ones. The discrepancy
loss is the difference between the first classifier’s last layer entropy and that of the second one. This forces
the classifiers to have the same confidence scores for in-class inputs, yet increases their discrepancy for the
others. Fig. 31 shows the overall architecture. First, two classifiers are trained on the in-class samples and
are encouraged to produce the same confidence scores. Second, an unlabeled dataset containing both OOD
and in-class data is employed to maximize their discrepancy on outliers while preserving their consistency
on inliers.
Figure 31: The overall architecture of Yu & Aizawa (2019). As it can be seen, at the first step, both of
the classifiers are trained. Then, at the next step, an auxiliary discrepancy loss is added to the supervised
classification to adjust the boundaries of in-class and OOD samples.
39Under review as submission to TMLR
6.9 Why ReLU Networks Yield High-Confidence Predictions Far Away From the Training Data Hein
et al. (2019):
This work proved that ReLU networks produce piece-wise affine functions; therefore, they can be written as
f(x) =Vlx+alon the polytope Q(x)as follows:
Γl,i={z∈Rd|∆l(x)(Vl
iz+al
i≥0)
Q(x) =∩l=1,...,L∩i=1,...,nlΓl,i,(68)
nlandLare the number of hidden units in the lth layer and the total number of layers, respectively. The
following theorem proves the deficiency of ReLU networks.
Theorem. 1 LetRd=∪R
l=1Qlandf(x) =Vlx+albe the piecewise affine representation of the output of a
ReLU network on Ql. Suppose that Vldoes not contain identical rows for all l= 1,...,RThen for almost
anyx∈Rdandϵ≥0there exists an αand a class k∈{1,...,K}such that for z=αxit holds
efk(z)
/summationtextK
r=1efr(z)≥1−ϵ (69)
The equation goes to 1ifα→∞. From this, we can imply that for ReLU networks there exist infinitely
many inputs which yield high confidence predictions. Note that arbitrarily high confidence prediction can
not be obtained due to the bounded domain of inputs. In order to relieve this problem, a technique that is
called confidence enhancing data augmentation is used, as the Eq. 70 shows.
1
NN/summationdisplay
i=1LCE(yi,f(xi)) +λ·E[Lpout(f,Z)]
Lpout= max
l=1,...,Klog/parenleftigg
efl(z)
/summationtextK
k=1efk(z)/parenrightigg (70)
wherepoutandpinare in-class and out-of-distribution distributions respectively, which we are certain that
the set of the intersection of their supports has zero or close to zero probability mass. An example of such
an out-distribution pout would be the uniform distribution on [0,1]w×hor other noise distributions.
The above training objective needs many samples to enforce low confidence on the entire out-distribution,
an alternative technique called adversarial confidence enhancing training (ACET) is used. ACET uses the
idea of adversarial robust training to not only minimize the objective function at each point but also the
worst case in a neighborhood of the point as Eq. 71 shows.
1
NN/summationdisplay
i=1LCE(yi,f(xi)) +λ·E[ max
||u−Z||p≤ϵLpout(f,u)] (71)
At the test time, a thresholded confidence score is used to distinguish between inliers and outliers.
6.10 Do Deep Generative Models Know What They Don’t Know? Nalisnick et al. (2018):
This work shows that generative models surprisingly assign higher likelihood scores to outliers. This holds
for VAEs, auto-regressive models, and different kinds of flow-based methods. In the generative modeling,
usually, a parametric model on the data distribution is assumed as pθ(x). Then, it finds the best θthat
minimizestheKLdistancebetweenthetruebutunknowndistribution p∗andp, whichisequaltomaximizing
the likelihood of pθ(x)on the input distribution, as the Eq. 72 shows.
40Under review as submission to TMLR
KL[p∗||pθ(x)] =/integraldisplay
p∗(x) logp∗(x)
pθ(x)dx≈−1
Nlogpθ(X)−H[p∗] (72)
Also, assuming the existence of a latent space Zthe integrals can be written as Eq. 73 where ZandXare
hidden and input variables, respectively. Let fbe a diffeomorphism from the data space to a latent space,
which commonly happens in flow-based modelings, where |∂f
∂x|is known as the volume element.
/integraldisplay
zpz(z)dz=/integraldisplay
xpz(f(x))/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂f
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingledx=/integraldisplay
xpx(x)dx (73)
The parameters of pcan be decomposed as θ={ϕ,ψ}withϕbeing the diffeomorphism’s parameters, i.e.
f(x;ϕ), andψbeing the auxiliary distribution’s parameters, i.e. p(z;ψ). Then the objective function can be
written as the Eq. 74.
θ∗=arg maxθlogpθ(X) =
arg maxϕ,ψN/summationdisplay
i=1logpz(f(xn,ϕ),ψ) + log/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂fϕ
∂xn/vextendsingle/vextendsingle/vextendsingle/vextendsingle(74)
An interesting aspect of the objective function above is that it encourages the function fto have high
sensitivity to small changes of input samples xn. It is mentioned in the paper that if we plot the effect
of each term separately, the first term shows the desired behavior for inliers and outliers, but the second
term causes the problem. While changing fto constant-volume (CV) transformations Dinh et al. (2014)
can alleviate the problem, but not entirely. Finally, using the second-order expansion of the log-likelihood
aroundaninteriorpoint x0, ithasbeenshownthattheassignedlikelihoodshaveadirectrelationtothemodel
curvature and data’s second moment. Therefore, the problem of generative models might be fundamental.
Eq. 75 shows the details of expansion where Trmeans trace operation.
0≤Eq[logp(x,θ)]−Ep∗[logp(x,θ)]≈
∇x0logp(x0,θ)T(Eq[x]−Ep∗[x])
+1
2Tr{∇2
x0logp(x0,θ)(Σq−Σp∗)}(75)
6.11 Likelihood Ratios for Out-of-Distribution Detection Ren et al. (2019):
This paper employs likelihood ratio to alleviate the problem of OOD detection in generative models. The key
idea is to model the background and fore-ground information separately. Intuitively, background information
is assumed to be harmed less than fore-ground information when semantically irrelevant information are
added to the input distribution. Therefore, two autoregressive models are trained on noisy and original
input distribution, and their likelihood ratio is defined as Eq. 76.
LLR(x) = logpθ(X)
pθ0(X)= logpθ(XB)pθ(XF)
pθ0(XB)pθ0(XF)
logpθ(XF)
pθ0(XF)= logpθ(XF)−logpθ0(XF)(76)
At the test time, a thresholding method is used on the likelihood ratio score.
41Under review as submission to TMLR
6.12 Generalized ODIN Hsu et al. (2020):
As an extension to ODIN Liang et al. (2017), this work proposes a specialized network to learn temperature
scaling and a strategy to choose perturbation magnitude. G-ODIN defines an explicit binary domain variable
d∈{din,dout}, representing whether the input xis inlier (i.e, x∼pin). The posterior distribution can be
decomposed into p(y|din,x) =p(y,din|x)
p(din|x). Note that in this formulation, the reason of assigning overconfident
scores to outliers seems more obvious, since the small values of p(y,din|x)andp(din|x)produce the high
value ofp(y|din,x). Therefore, they are decomposed and estimated using different heads of a shared feature
extractor network as hi(x)andg(x)forp(y|din,x)andp(din|x)respectively. This structure is called
dividend/divisor and the logit fi(x)for classican be written ashi(x)
g(x). The objective loss function is a simple
cross entropy, similar to previous approaches. Note that the loss can be minimized by increasing hi(x)or
decreasingg(x). For instance, when the data is not in the high-density areas of in-distribution, hi(x)might
be small; therefore, g(x)is forced to be small to minimize the objective function. In the other case, g(x)
are encouraged to have larger values. Therefore, they approximate the role of aforementioned distributions
p(y|din,x)andp(din|x). At the test time maxihi(x)org(x)are used. Fig. 32 shows an overview of the
method.
Figure 32: The overall architecture of Generalized ODIN. As it is obvious, different heads are applied on the
penultimate layer to model the mentioned distributions.
6.13 Background Data Resampling for Outlier-Aware Classification Li & Vasconcelos (2020):
As mentioned before, in AD, ND, OSR, and OOD detection, some methods use a background or outlier
dataset to boost their performances. However, to avoid different kinds of biases, the size of auxiliary datasets
becomes important. In this work, a re-sampling technique is proposed to select an optimal number of
training samples from the outlier dataset such that on-boundary samples play a more influential role in the
optimization task. The work first provided an interesting probabilistic interpretation on outlier exposure
technique. The loss function can be written as Eq. 79, where LclsandLuniare shown in Eq. 77 and Eq. 78
respectively.
Lcls(f(x;θ),y) =−logfy(x;θ) (77)
Luni(f(x;θ)) =−1
KK/summationdisplay
k=1logfk(x;θ)−logK (78)
L(θ;p,q) =EX,Y∼p(.,.)[Lcls(f(X;θ);Y)]
+αEX∼q(·)[Luni(f(X;θ))].(79)
42Under review as submission to TMLR
By expanding Eq. 79 and taking its gradient w.r.t. classifier logits, the optimial classifier is obtained as Eq.
80 shows.
f∗
k(x) =c(x)pY|X(k|x) +1−c(x)
K
c(x) =pX(x)
pX(x) +αqX(x),(80)
wherec(x)can be seen as the relative probability of a sample xbeing in-class distribution por background
distribution qwith the ratio α. SupposeD′
bis the re-sampled dataset and Dbis the given background one,
OOD detection loss after reweighting can be written as:
Lout(θ;w) =1
|D′
b|/summationdisplay
(x,y)∈D′
bLuni(f(x;θ))
=1
/summationtext|Db|
i=1wiLuni(f(x;θ))(81)
The optimal parameters θ∗are learned as follows:
θ∗(w) =arg minθL(θ;D,w)
=arg minθLin(θ;D) +αLout(θ;w)(82)
Therefore, an iterating optimization is solved between finding θtandwtat each step by fixing one and solving
the optimization problem for the other. At last, the largest values of weights can be selected regarding the
dataset’s desired size.
6.14 Input Complexity and Out-of-Distribution Detection With Likelihood-Based Generative Models
Serrà et al. (2019):
This work further investigated the problem of generative models assigning higher likelihood values to OOD
samples. In particular, this work finds a strong tie between the OOD samples’ complexity and likelihood
values. Simpler input can lead to higher the likelihood value. Fig. 34 shows this phenomenon. Furthermore,
another experiment to support the claim is designed in such a way that we start from a random noise, on
which a mean average pooling is applied at each step. To preserve the dimension, an upscaling is done
after average poolings. Surprisingly, simpler images on which more average poolings are applied to achieve
higher likelihoods. Motivated by this, the work proposed to detect OOD samples by accounting for input
complexity in combination with likelihood values. Since it is hard to compute the input complexity, the
paper instead calculates an upper bound using a lossless compression algorithm Cover & Thomas (2006).
Given a set of inputs x coded with the same bit depth, the normalized size of their compressed versions,
L(x)(in bits per dimension), is used as the complexity measurement. Finally, the OOD score is defined as
Eq. 83:
S(x) =−lM(x)−L(x), (83)
wherelM(x)is the log-likelihood of an input xgiven a model M.
Intuitively, considering M0as a universal compressor, then p(x|M0) = 2−L(x)and Consequently, the OOD
score can be defined as follows :
S(x) =−log2p(x|M) + log2p(x|M0) = log2p(x|M0)
p(x|M)(84)
In the cases where a simple OOD sample is fed into the model, the universal compressor M0assigns a high
probability to it and effectively corrects the high likelihood wrongly given by the learned model M. Similar
interpretation holds for the complex OOD samples too.
43Under review as submission to TMLR
Figure 33: The assigned likelihood values of a simple generative model to different datasets when is trained
on CIFAR10. As it is obvious, simpler datasets achieve higher values.
6.15 Energy-based Out-of-distribution Detection Liu et al. (2020b):
This work proposes using the energy score derived from the logit outputs for OOD detection, and demon-
strated superiority over softmax score. Energy-based models map each input xto a single deterministic
point that is called energy LeCun et al. (2006). A set of energy values E(x,y)could be turned into a density
functionp(x)through the Gibbs distribution:
p(y|x) =e−E(x,y)/T
/integraltext
y′e−E(x,y′)/T=e−E(x,y)/T
e−E(x)/T(85)
E(x)is called Helmholtz free energy and is equal to:
E(x) =−T·log/integraldisplay
y′e−E(x,y′)/T(86)
In deep networks, by considering E(x,y) =−fy(x), one can express the free energy function in terms of the
denominator of the softmax activation:
E(x;f) =−T·logK/summationdisplay
iefi(x)/T(87)
The paper also shows that cross-entropy loss facilitates pulling down the energy for in-distribution data
during the training process. Moreover, softmax scores can be analyzed through an energy-based perspective,
as Eq. 88 shows. During the optimization, E(x;f)is forced to be small for in-distribution samples while
being shifted by fmax(x)to satisfy the maximum function. Consequently, this results in a biased scoring
function that is not suitable for OOD detection.
max
yp(y|x) = max
yefy(x)
/summationtext
iefi(x)=efmax(x)
/summationtext
iefi(x)
=1/summationtext
iefi(x)−fmax(x)
=⇒log max
yp(y|x) =E(x;f(x)−fmax(x))
=E(x;f) +fmax(x)(88)
Energy score is hyperparameter-free, easy to compute and achieves strong performance compared to the
softmax score. Beyond post hoc OOD detection, the paper further demonstrated that energy score can be
44Under review as submission to TMLR
utilized for model regularization. Different from outlier exposure which forces the uniform softmax distribu-
tion for outlier training samples, energy-based regularization directly optimizes the energy gap between ID
and OOD:
min
θE(x,y)∼Dtrain
in[−logFy(x)] +λ·Lenergy
Lenergy =E(xin,y)∼Dtrain
in[max(0,E(xin)−min)2]
+Exout∼Dtrain
out[max(0,mout−E(xout))2](89)
The optimization results on stronger performance than OE. At the test time, OOD samples are detected
based on a threshold on −E(x;f).
CNNEnergy FunctionNegative EnergyFrequency
thresholdτin-distributionout-of-distributionf(x;θ)E(x;f)x
Figure 34: The energy-based OOD detection framework. The energy function maps the logit outputs to a
scalar through a convenient logsumexp operator. Test samples with lower energy are considered ID and vice
versa.
6.16 Likelihood Regret: An Out-of-Distribution Detection Score for Variational Autoencoder Xiao
et al. (2020):
Previous works showed that VAEs can reconstruct OOD samples perfectly, resulting in the difficulty in
detecting OOD samples. The average test likelihoods of VAE across different datasets have a much smaller
range than PixelCNN Oord et al. (2016) or GlowKingma & Dhariwal (2018), showing that distinguishing
between OOD and inlier samples is much harder in VAE. The reason might be because of different ways
they model the input distribution. Auto-regressive and flow-based methods model their input at pixel level,
while the bottleneck structure in VAE forces the model to ignore some information. To address this issue,
a criterion called likelihood regret is proposed. It measures the discrepancy between a model trained to
maximize the average likelihood of a training dataset, for instance, a simple VAE, and a model maximizing
the likelihood of a single input image. The latter is called an ideal model for each sample. Intuitively, the
likelihood difference between the trained model and the ideal one might not be high; however, this does not
hold for OOD inputs. Suppose the following optimization is performed to train a simple VAE :
(ϕ∗,θ∗)≈arg maxϕ,θ1
nn/summationdisplay
i=1L(xi;θ,τ(xi,ϕ)), (90)
whereϕandθare the parameters of encoder and decoder respectively, and τ(xi,ϕ)denotes the sufficient
statistics (µx,σx)ofqϕ(z|x). For obtaining the ideal model, we can fix the decoder part and solve an
optimization problem on τ(xi,ϕ)such that its individual ELBO is maximized:
ˆτ=arg maxτL(x;θ∗,τ) (91)
Finally, the likelihood regret is defined as follows:
45Under review as submission to TMLR
LR(x) =L(x;θ∗,ˆτ(x))−L(x;θ∗,ϕ∗) (92)
The optimization of Eq. 91 could be started from the initial point that encoder produces for each input, and
a threshold is used on LR values at the test time.
6.17 Understanding Anomaly Detection With Deep Invertible Networks Through Hierarchies of
Distributions and Features Schirrmeister et al. (2020):
This work studied the problem of flow-based generative models for OOD detection. It was noted that local
feature such as smooth local patches can dominate the likelihood. Consequently, smoother datasets such
as SVHN achieves higher likelihoods than a less smooth one like CIFAR-10, irrespective of the training
dataset. Another exciting experiment shows the better performance of a fully connected network than a
convolutional Glow network in detecting OOD samples using likelihood value. This again supports the
existence of a relationship between local statistics like continuity and likelihood values. Fig. 35 shows the
similarity of different dataset local statistics that are computed based on the difference of a pixel value to
the mean of its 3×3neighboring pixels.
Figure 35: (A) shows the distribution of local pixel value differences. As it can be seen, distributions are
highly overlapped. (B) Likelihoods extracted from the local pixel value differences correlate with CIFAR10-
Glow likelihoods.
Furthermore, let us call the summation of the likelihood of pixel value differences computed in a local 3×3
neighboring pixels using a histogram with 100 equally distanced bins pseudo-likelihoods. A strong Spearman
correlation is found between the pseudo-likelihoods and the exact values of likelihoods, supporting the above
assumptions. To address this problem, the following three steps are used:
•Train a generative network on a general image distribution like 80Million Tiny Images
•Train another generative network on images drawn from the in-distribution, e.g., CIFAR-10
•Use their likelihood ratio for OOD detection
Also, to improve the efficacy, the following outlier loss, which uses OOD samples, can be added to the
maximum likelihood objective function:
Lo=−λ.log/parenleftbig
σ/parenleftbiglog(pg(x))−log(pin(x))
T/parenrightbig/parenrightbig
, (93)
whereσis sigmoid function.
6.18 Self-Supervised Learning for Generalizable Out-of-Distribution Detection Mohseni et al. (2020):
In this work, a self-supervised learning method is employed to use the information of an unlabeled outlier
dataset such that the OOD detection utility of an in-distribution classifier is improved. To do so, at first, the
46Under review as submission to TMLR
classifier is trained on in-class training samples until the desired performance is achieved. Then, additional
outputs (a set of kreject classes) are added to the last layer. Each training batch consists of ID data and
some outlier samples. The following loss function is used:
min(EPin(ˆx,ˆy)[−log(Pθ(y= ˆy|ˆx))])
+λEPout(x,target )[−log(Pθ(y=target|x))],(94)
where the target is selected based on the following rules:
if arg max (Pθ(x))∈kthen
target←arg max (Pθ(x))
else
target←random (k)(95)
This is similar to unsupervised deep k-means in Caron et al. (2018). If an unlabeled sample in the outlier
dataset resembles in-class samples, it is assigned a random reject class each time; otherwise, it is assigned to
a specific reject class. This helps unlabeled inliers to be separated from outliers. At the test time, the sum
of the softmax output of the OOD classes is used as the detection score.
6.19 SSD: A Unified Framework for Self-Supervised Outlier Detection Sehwag et al. (2021):
This work has a very similar idea to GDFR ( c.f.Section 5.8). It incorporates SSL methods so to alleviate
the need for labeling in-class samples. This is different from several aforementioned methods that require
solving a classification task. As a result, SSD can be flexibly used in different settings such as ND, OSR, and
OOD detection. The main idea is to employ the contrastive learning as in Chen et al. (2020b), and learns
semantically meaningful features. After representation learning, a k-means clustering is applied to estimate
class centers with mean and covariance (µm,Σm). Then for each test time sample, the Mahalanobis distance
to the closest class centroid is used as the OOD detection score:
sx= min
m(zx−µm)TΣ−1
m(zx−µm) (96)
The contrastive learning objective function is simple. Using image transformations, it first creates two views
of each image, commonly referred to as positives. Next, it optimizes to pull each instance close to its positive
instances while pushing away from other images, commonly referred to as negatives:
Lbatch =1
2N2N/summationdisplay
i=1−logeuT
iuj/τ
/summationtext2N
k=1I(k̸=i)euT
iuj/τ, (97)
whereui=h(f(xi))
||h(f(xi))||2is the normalized feature vector, (xi,xj)are positive pairs for the ith image from a
batch ofNimages, and h(·)is the projection head. Moreover, when a few OOD samples are available, the
following scoring function can be used:
sx= (zx−µin)TΣ−1
in(zx−µin)
−(zx−µood)TΣ−1
ood(zx−µood)(98)
This framework can be extended to the supervised settings when the labels of in-class distribution are
available. SSD employs the supervised contrastive learning objective proposed in Khosla et al. (2020):
Lbatch =1
2N2N/summationdisplay
i=1−log1
2Nyi−1/summationtext2N
k=1I(yk=yi)euT
iuj/τ
/summationtext2N
k=1I(k̸=i)euT
iuj/τ, (99)
whereNyiis the number of images with label yiin the batch.
47Under review as submission to TMLR
6.20 MOOD: Multi-level Out-of-distribution Detection Lin et al. (2021):
This work first investigated the computational efficiency aspect of OOD detection. Intuitively, some OOD
samples can be detected using only low-level statistics without any need for complex modelings. To this end,
multiple intermediate classifiers are trained, operating at different depths of a trained network, as Fig. 36
shows. To find the desired existing depth, an approximation of the input complexity is needed. To address
this issue, similar to Serrà et al. (2019), the number of bits used to encode the compressed image L(x)is
employed. Consequently, exit depth I(x)is determined based on the complexity range a sample belongs to:
I(x) = min(⌈Lnormalized (x)×K⌉,K), (100)
whereKis the number of classes.
The OOD scoring function is defined similarly to Liu et al. (2020b), using an energy function on logits as
follows:
E(x;θi) =−logC/summationdisplay
j=1ef(j)(x;θi)
i, (101)
whereCis the number of classes of in-distribution data, and f(j)
i(x;θi)is the logit output corresponding to
classj∈1,2,...,Cfor the intermediate classifier at exit i. The mentioned criterion has different scales at
different depths; therefore, it is normalized as:
Eadjusted (x;θi) =−E(x;θi)−Ex∈Din[−E(x;θi)] (102)
Finally, OOD samples are detected based on simple thresholding on the energy score of the desired depth.
Figure 36: The overall architecture of MOOD. As the figure shows, some OOD samples can be detected by
applying simple low-level computations, as opposed to some others that need more complex modelings. Test
time computations can be done dynamically and on demand, which significantly reduces computational cost.
6.21 MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space Huang & Li
(2021):
MOS first revealed that the performance of OOD detection can degrade significantly when the number of in-
distribution classes increases. For example, analysis reveals that a common baseline’s average false positive
rate (at 95% true positive rate) would rise from 17.34%to76.94%as the number of classes increases from 50
to1,000on ImageNet1k. To overcome the challenge, the key idea of MOS is to decompose the large semantic
space into smaller groups with similar concepts, which allows simplifying the decision boundaries between
known vs. unknown data. Specifically, MOS divides the total number of Ccategories into Kgroups,G1,G2,
...,GK. Grouping is done based on the taxonomy of the label space if it is known, applying k-means using
the features extracted from the last layer of a pre-trained network or random grouping. Then the standard
groupwise softmax for each group Gkis defined as follows:
48Under review as submission to TMLR
pk
c(x) =efk
c(x)
/summationtext
c′∈Gkefk
c′(x),c∈Gk (103)
wherefk
c(x)andpk
c(x)denote output logits and the softmax probability for class cin groupGk, respectively.
Fig. 37 shows the overall architecture. The training loss function is defined as below:
LGS=−1
NN/summationdisplay
n=1K/summationdisplay
k=1/summationdisplay
c∈Gkyk
clog(pk
c(x)) (104)
whereyc
kandpc
krepresent the label and the softmax probability of category cinGk, andNis the total
number of training samples.
A key component in MOS is to utilize a category othersin each group, which measures the probabilistic
score for an image to be unknown with respect to the group. The proposed OOD scoring function, Minimum
Others Score (MOS), exploits the information carried by the otherscategory. MOS is higher for OOD
inputs as they will be mapped to otherswith high confidence in all groups, and is lower for in-distribution
inputs. Finally, test time detection is done based on the following score:
SMOS(x) =−min
1≤k≤K(pk
others (x)) (105)
Figure 37: The overall architecture of MOS. As the figure shows, each sample is labeled as "others" for
groups to which it does not belong, except the correct group. At the test time, detection is done based on
the minimum of "others" class scores.
6.22 Can Multi-Label Classification Networks Know What They Don’t Know? Wang et al. (2021):
In this work, the ability of OOD detectors in the multi-label classification setting was investigated. In the
multi-label classification setting, each input sample might have one or more corresponding labels, which
make the problem harder since the joint distribution across labels can be intractable to model. This work
proposes the JointEnergy criterion as a simple and effective method, which estimates the OOD indicator
scores by aggregating label-wise energy scores from multiple labels. Also, they show that JointEnergy can
be mathematically interpreted from a joint likelihood perspective. Similar to what has been discussed in
Liu et al. (2020b) P(yi= 1|x)can be written ase−E(x,yi)
e−E(x), then by defining label-wise free energy that is a
special case of K-class free energy with K= 2:
Eyi(x) =−log(1 +efyi(x)) (106)
JointEnergy can be defined as:
49Under review as submission to TMLR
EJoint(x) =K/summationdisplay
i=1−Eyi(x) (107)
Through derivations, the JointEnergy can be decomposed into three terms:
Ejoint(x) = logp(x|y1= 1,...,yK= 1)
+ (K−1)·logp(x) +Z(108)
wherethefirsttermtakesintoaccountjointlikehoodacrosslabels; andthesecondtermreflectstheunderlying
data density, which is supposed to be higher for in-distribution data x. The summation overall results in
stronger separability between ID and OOD. The architecture overview is provided in Fig. 38.
f(x;θ)
Out-of-DistributionPlaneCarBirdDeerDogFrogHorseCatShipTruckCarMaxSum
JointEnergy (ours) MaxLogit
Out-of-DistributionIn-Distribution
In-Distribution
aggregating over manyxx
Figure 38: An overview of JointEnergy for OOD detection in multi-label classification networks. During
inference time, input xis passed through a classifier, and label-wise scores are computed for each label.
OOD indicator scores are either the maximum-valued score (denoted by green outlines) or the sum of all
scores. Taking the sum results in a larger difference in scores and more separation between in-distribution
and OOD inputs (denoted by red lines), resulting in better OOD detection. Plots in the bottom right depict
the probability densities of MaxLogit versus JointEnergy.
6.23 On the Importance of Gradients for Detecting Distributional Shifts in the Wild Huang et al.
(2021) :
This work proposes a simple post hoc OOD detction method GradNorm, which utilizes the vector norm of
gradients with respect to weights, backpropagated from the KL divergence between the softmax output and
a uniform probability distribution. The GradNorm is generally higher for in distribution (ID) data than
that for OOD data. Therefore, it can be used for OOD detection. Specifically, the KL divergence is defined
as follows:
DKL(u||Softmax (f(x)) =−1
CC/summationdisplay
c=1log/parenleftigg
efc(x)/T
/summationtextC
j=1efj(x)/T/parenrightigg
, (109)
where uis the uniform distribution, and Cis the number of in-distribution classes. Then, the OOD score is
defined asS(x) =||∂DKL
∂W||pwhereWcan be (1) a block of parameters, (2) all parameters, and (3) last layer
parameters. It was mentioned that the third approach is better than other ones and achieves significant
results.
50Under review as submission to TMLR
6.24 ReAct: Out-of-distribution Detection With Rectified Activations Sun et al. (2021): (R1)
The internal activations of neural networks are examined in depth in this study, and it is discovered that they
have considerably different signature patterns for OOD distributions, which can generalize successfully to
varied network architectures and OOD detection scores. The distribution of activations in the penultimate
layer of ResNet-50, which is trained on the ImageNet, is depicted in Fig. 39. Each point on the horizontal
axis represents a single unit of measurement. The mean and standard deviation are depicted by the solid
line and shaded area, respectively, on the graph. The mean activation for the ID data (blue) is well-behaved,
with a mean and standard deviation that are both nearly constant. Contrary to this, when looking at the
raw OOD data (gray), the mean activation shows substantially greater variations across units and is biased
towards having sharp positive values. The unintended consequence of having such a high unit activation is
that it can present itself in the output of the model, resulting in overconfident predictions on the OOD data.
Other OOD datasets exhibit a similar distributional property, which is consistent with previous findings.
Therefore, a technique called Rectified Activations (also known as ReAct) is presented as a solution to this
problem, in which the outsized activation of a few selected hidden units can be attenuated by rectifying
the activations at an upper limit c > 0. It is demonstrated that ReAct can generalize effectively to a
variety of network designs and that it is compatible with a variety of OOD detection methods, including
MSP Hendrycks & Gimpel (2016), ODIN Liang et al. (2017), and energy score Liu et al. (2020b), among
others.
Figure 39: a) The distribution of ID and OOD uncertainty scores before truncation when ImageNet and
iNaturalist are used as in-class and out-of-distribution samples, b) the distribution of per-unit activations
in the penultimate layer for ID and OOD data, and c) the distribution of OOD scores after rectification for
the ID and OOD data are depicted in the plots above. When ReAct is used, it significantly enhances the
separation of the ID and OOD information.
7 Dataset
7.1 Semantic-Level Datasets
Below we summarize datasets that can be used to detect semantic anomalies. Semantic anomalies are those
kinds of anomalies that the variation of pixels leads to the change of semantic content. Datasets such as
MNIST, Fashion-MNIST, SVHN, and COIL-100 are considered as toy datasets. CIFAR-10, CIFAR-100,
LSUN, and TinyImageNet are hard datasets with more variations on color, illumination, and background.
Finally, Flowers and Birds are fine-grained semantic datasets, which makes the problem even harder.
MNIST LeCun & Cortes (2010): This dataset includes 28 ×28 grayscale handwritten digits from 0-9
and consists of 60k training images and 10k testing ones.
Fashion MNIST Xiao et al. (2017): This dataset comprises of 28 ×28 grayscale images of 70k clothing
items from 10 categories. The training set has 60k images and the test set has 10k images.
51Under review as submission to TMLR
CIFAR-10 Krizhevsky et al. (2009): The CIFAR-10 has 60k natural images. It consists 32x32 RGB
images of 10 classes,There are 50k training images and 10k test images.
CIFAR-100 Krizhevsky et al. (2009): This dataset is very similar to CIFAR-10, but it has 100 classes
containing 600 images each.The 100 classes are grouped into 20 super classes. each class has 500 training
images and 100 testing images.
TinyImageNet Deng et al. (2009): The Tiny ImageNet dataset consists of a subset of ImageNet images
. It contains 10,000 test images from 200 different classes. Also, two more datasets, TinyImageNet (crop)
and TinyImageNet (resize) can be constructed, by either randomly cropping image patches of size 32 ×32
or downsampling each image to size 32 ×32.
LSUN Yu et al. (2015): The Large-scale Scene UNderstanding dataset (LSUN) has a testing set of
10,000 images of 10 different scene categories such as bedroom, kitchen room, living room, etc. Similar
to TinyImageNet, two more datasets, LSUN (crop) and LSUN (resize), can be reconstructed by randomly
cropping and downsampling the LSUN testing set, respectively.
COIL-100 Nene et al. (1996): COIL-100 is a dataset of colorful images of 100 objects. It comprises
7200 128 ×128 images. Images are captured from objects placed on a motorized turntable against a black
background, and there are 72 images of each object in different poses.
SVHN Netzer et al. (2011): SVHN can be seen as similar in flavor to MNIST (e.g., the images are of
small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images)
and comes from a significantly harder, unsolved, real-world problem (recognizing digits and numbers in
natural scene images). SVHN is obtained from house numbers in Google Street View images.
Flowers Nilsback & Zisserman (2008): Flowers is a 102 category dataset, consisting of 102 flower
categories. The flowers chosen to be flower commonly occuring in the United Kingdom. Each class consists
of between 40 and 258 images. The images have large scale, pose and light variations. In addition, there are
categories that have large variations within the category and several very similar categories.
Birds Welinder et al. (2010): CUB-200-2011 is a bird classification task with 11,788 images across 200
wild bird species. There is roughly equal amount of train and test data. It is generally considered one of the
most challenging datasets since each species has only 30 images for training.
7.2 Pixel-Level Datasets
In these datasets unseen samples, outliers or anomalies do not have semantic difference with inliers. This
means an area of the original image is defected; however, the original meaning is still reachable, yet has
been harmed.
MVTecADBergmannetal.(2019) :Thisdatasetisanindustrialdataset, itprovides5354high-resolution
images divided into ten objects and five texture categories. it contains 3629 training images. test set contains
467 normal images and 1258 abnormal images having various kinds of defects.
PCB Huang & Wei (2019): PCB dataset containing 1386 images with 6 kinds of defects for the use of
detection, classification and registration tasks. Images are captured in high-resolution.
LaceAD Yan et al. (2021): LaceAD contains 9,176 images from the top 10 lace fabric manufacturing
companies worldwide, where the images are captured in the real production environment by a high-resolution
DSLR camera, They are categorized into 17 subsets based on their patterns. Each image has the size of 512
×512 and has been labeled by professional workers.
Retinal-OCT Kermany et al. (2018): This consists of 84,495 X-Ray images in 4 categories CNV,
DME, DRUSEN, and NORMAL, each of which has subtle differences with respect to others.
CAMELYON16Bejnordietal.(2017): Detectingmetastasesoflymphnodesisanextremelyimportant
variable in the diagnosis of breast cancers. Tissue with metastasis may differ from healthy one only by
texture, spatial structure, or distribution of nuclei, and can be easily confused with normal tissue. The
52Under review as submission to TMLR
training dataset of Camelyon16 consists of 110 whole-slide images (WSIs) contained tumors, and 160 are
not, and testing dataset with 80 regular slides and 50 slides containing metastases.
Chest X-Rays Wang et al. (2017); Irvin et al. (2019); Rajpurkar et al. (2017): Chest X-Ray
datasets are medical imaging datasets which comprises a large number of frontal-view X-ray images of many
unique patients (collected from the year of 1992 to 2015). The datasets include eight to fourteen common
disease labels, mined from the text radiological reports via NLP techniques. Images are not registered and
captured in different pose and contrast, which makes the detection task challenging.
Species Hendrycks et al. (2020): This dataset consists of organisms that fall outside ImageNet-21K.
Consequently, ImageNet-21K models can treat these images as anomalous.
ImageNet-O Hendrycks et al. (2021c): ImageNet-O is a dataset of adversarially filtered examples for
ImageNet out-of-distribution detectors. To create this dataset, at first, ImageNet-22K is downloaded, and
shared examples from ImageNet-1K are deleted. With the remaining ImageNet-22K examples that do not
belong to ImageNet-1K classes, examples that are classified by a ResNet-50 as an ImageNet-1K class with a
high confidence are kept. Finally, visually clear images are selected. This creates a dataset of OOD examples
that are hard for a ResNet-50. These examples are challenging for other models to detect, including Vision
Transformers.
 
 
Figure 40: Sample visualization of MNIST, Fashion-MNIST, SVHN, COIL-100, Birds, Chest X-Rays, Cifar-
10, TinyImageNet, LSUN, Flowers, MVTecAD, PCB, Retinal-OCT, CAMELYON16, LaceAD, MNIST-C,
ImageNet-C, and ImageNet-P.
7.3 Synthetic Datasets
These datasets are usually made using semantic-level datasets; however, the amount of pixel-variations is
under control such that unseen, novel, or abnormal samples are designed to test different aspects of trained
models while preserving semantic information. For instance, MNIST-c includes MNIST samples added
different kinds of noises such as shot noise and impulse noise, which are random corruptions that may occur
during the imaging process. These datasets could be used to not only test the robustness of our models,
but also for training models in the AD setting instead of novelty detection or open-set recognition. Due to
the lack of comprehensive research in the field of anomaly detection, these datasets can be very beneficial.
53Under review as submission to TMLR
MNIST-C Mu & Gilmer (2019): MNIST-C dataset is a comprehensive suite of 15 corruptions applied
to the MNIST test set for benchmarking out-of-distribution robustness in computer vision. Through several
experiments and visualizations, it is shown that the corruptions significantly degrade the performance of
state-of-the-art computer vision models while preserving the semantic content of the test images.
ImageNet-C,ImageNet-PHendrycks&Dietterich(2019): ThiscanbeseenastheImageNetversion
of MNIST-C. For the ImageNet-C, a set of 75 common visual corruptions are applied on each image, and
for the ImageNet-P, a set of perturbed or subtly differing ImageNet images are introduced. It is shown that
although these perturbations are not chosen by an adversary, currently existing networks exhibit surprising
instability on common perturbations.
An overall visualization of the mentioned datasets can be found in Fig. 40.
8 Evaluation Protocols
8.1 AUC-ROC
Receiver Operating Characteristics (ROC) is a well-known criterion. Given a test dataset including positive
and negative (or seen and unseen ) samples, it characterizes the relation between the false positive rate
(FPR) and the true positive rate (TPR) at different detection thresholds. AUC-ROC is the area under the
ROC curve, which is a threshold-independent metric. The highest value of ROC is 1, and 0.5 indicates a
model assigns the positive label with random guessing.
In the literature, AD and ND are usually tested in one-vs-all setting that considers one class as normal
and the rest of the classes as anomaly, unseen or unknown. For OOD detection, the in-distribution data
is considered as positive and OOD data is considered as negative. For instance, one can train a model on
CIFAR-10 and considers MNIST as outlier at the test time. This means training and testing datasets have a
large contrast with each other. Sometimes instead of MNIST, uniform noise or Gaussian noise can be used.
8.2 FPR@TPR
Although AUC-ROC is a common metric, in practice, models have to select a specific threshold to perform
detection. To address this issue, an operating point on the ROC which is desired with respect to applications
is selected. A commonly-used metric is FPR@TPR x, which measures the FPR when the TPR is x= 0.95.
8.3 AUPR
AUPR is the Area under the Precision-Recall curve, which is another threshold independent metric. The
PR curve depicts the precision=TP
TP+FPand recall=TP
TP+FNunder different thresholds. In some literature,
the metrics AUPR-In and AUPR-Out denote the area under the precision-recall curve where in-distribution
and out-of-distribution images are specified as positives, respectively. This metric is usually used in OOD
detection setting.
8.4 Accuracy
This metric is usually used in OSR, which is a common choice for evaluating classifiers under the closed-set
assumption, as follows:
A=/summationtextC
i=1(TPi+TNi)
/summationtextC
i=1(TPi+TNi+FPi+FNi)(110)
This can be easily extended to open-world assumption in which UUCs must be classified correctly:
AO=/summationtextC
i=1(TPi+TNi) +TU
/summationtextC
i=1(TPi+TNi+FPi+FNi) + (TU+FU)(111)
54Under review as submission to TMLR
Although accuracy is a common metric; however, it is very sensitive to imbalanced number of samples, which
is not the case in metrics such as AUC-ROC. To cope with this issue, a normalized accuracy (NA), which
weights the accuracy for KKCs (AKS) and the accuracy for UUCs (AUS) is defined as follows:
NA=λrAKS + (1−λr)AUS (112)
where
AKS =/summationtextC
i=1(TPi+TNi)
/summationtextC
i=1(TPi+TNi+FPi+FNi)
AUS =TU
TU+FU(113)
and0≤λr≤1is a regularization constant.
8.5 F-measure
The F-measure or F-score is the harmonic mean of precision Pand recallR:
F= 2×P×R
P+R(114)
Note thatFmust be computed in the same way as the multi-class closed set scenario. This is because the
correct classifications of UUCs would be considered as true positive classifications; however, it makes no
sense since there is no UUC sample in the training process. Therefore, the computations of Precision and
Recall only for KKCs are modified to give a relatively reasonable F-measure for OSR. The new measures are
called macro-F-measure and micro-F-measure respectively as follows:
Pma=1
CC/summationdisplay
i=1TPi
TPi+FPi, R ma=1
CC/summationdisplay
i=1TPi
TPi+FNi
Pmi=/summationtextC
i=1TPi/summationtextC
i=1(TPi+FPi), R mi=/summationtextC
i=1TPi/summationtextC
i=1(TPi+FNi)(115)
Note that although the precision and recall only consider the KKCs; however, by computing FNiandFPi
false UUCs and false KKCs are taken into account Júnior et al. (2017).
8.6 Comparing Evaluating Procedures of Each Domain (R1)
Each domain assessment process establishes the practical restrictions that must be met before the claimed
performanceofproposedmethodswithinthatdomaincanbetrusted. Furthermore, insomedomains, suchas
OSR and OOD detection, highlighting the differences in evaluation protocols utilized in each domain might
help to gain a better expectation about the reported performances when applied in practical applications. As
a result, given an arbitrary dataset such as CIFAR-10 Krizhevsky et al. (2009), the following training/testing
setup is employed at each domain :
AD/ND : (1) Select a random class out of given 10 classes as normal distribution. (2) Train a one-class
feature extractor according to the discussed approaches. (3) Test the trained feature extractor on the entire
test-set, which implies the implicit assumption of 10% likelihood for the occurrence of normal events and
90% for the abnormal ones.
OOD Detection : (1) Use the whole training dataset to train a multi-class feature extractor. (2) As the
out-of-distribution dataset, choose another dataset, such as one of the datasets listed above. (3) Consider
55Under review as submission to TMLR
the entire test set to be normal and select some random samples from the out-of-distribution dataset so that
the chance of abnormal occurrences is lower than normal when all the samples are combined. As can be
observed, this assessment methodology makes an implicit assumption about the rarity of anomalies, which
OSR does not.
OSR : (1) As the normal distribution, pick Krandom classes. (2) Train a multi-class feature extractor
according to the discussed approaches. (3) Test the trained feature extractor on the entire test-set. As
previously stated, Kis decided based on the openness score . This suggests that abnormal events may
have a higher probability than normal events, because anomaly means not being a member of the normal
sets. For instance, in the self-driving car application, a model might be only responsible for detecting people
and cars as two small sets of many definable sets existing in the agent environment. This is a key distinction
between the OOD detection and OSR viewpoints.
9 Future Challenges
Here we provide plausible future directions, which might be of interest to both practitioners and academic
researchers.
9.1 Evaluating Baselines and the Evaluation Protocols of OOD Detection
The evaluation protocols for OOD detection has room for improvement. For instance, Ahmed & Courville
(2020) trains a mixture of three Gaussians on the CIFAR-10 dataset (as ID), and evaluated against OOD
datasets including TinyImagenet (crop), TinyImagenet (resize), LSUN, LSUN(resize) and iSUN. The model
is trained channel-wise at a pixel-level. Tab. 2 shows the detection results on different datasets. The results
are comparable with SOTA despite the simplicity. In particular, LSUN performs poorly since the majority of
them have uniform color and texture, with little variation and structure. Similar to what has been observed
in likelihood-based methods, LSUN “sits inside” CIFAR-10 with a similar mean but lower variance, and ends
up being more likely under the wider distribution.
We also provide a better insight into the performance of OOD detection baselines, evaluated on both near
and far out-of-distribution datasets. For model trained on CIFAR-10, we use CIFAR-100 as the near OOD
datasets. The results are presented in Tables 3, 4, and 6. As it is shown, none of the methods are good at
detectingbothnearandfarOODsamples, exceptOEapproachesthatuseanextraauxiliarydatasettodothe
task. Using Mahalanobis distance can improve the performance of most of the methods at detecting far OOD
samples, while degrading the performance of near OOD detection. Besides, as it can have poor performance
at detecting even some of far OOD samples due to inaccurate Gaussisn density estimation. Moreover, its
performance varies significantly when OOD dataset is resized or cropped, showing its dependency on low-
level statistics. For instance, notice the SVHN column of Table 6. This is in correspondence with what has
been shown recently by Ren et al. (2021) on the deficiencies of Mahalanobis distance as well.
One solution to resolve the issue might be applying input pre-processing techniques such as ODIN to alleviate
the effect of first and second-order statistics in assigning OOD scores; however, it increases the execution
speed by the sum of an extra forward and backward pass during testing. Additionally, techniques such as
ensembling or MC-Dropout Gal & Ghahramani (2016) might be slightly better than others on some OOD
datasets; yet, they need multiple forward passes, increasing the execution time significantly. For example,
the reported MC-Dropout is 40 times slower than a simple MSP. In summary, we encourage future works to
evaluate OOD detection on both near and far OOD datasets.
9.2 AD Needs to Be Explored More
As mentioned earlier, AD and ND are not completely the same, both historically and fundamentally. An
essential and practical category of problems in the real-world application are those that can not be cleaned
easily, and consequently, contain different kinds of noises such as, label noise ordata noise . This is the
case in complex and hazardous systems such as modern nuclear power plants, military aircraft carriers, air
traffic control, and other high-risk systems Hendrycks et al. (2021b). Recently proposed methods in ND
56Under review as submission to TMLR
Table 2: The performance of a simple method using only low-level features on different datasets Ahmed &
Courville (2020).
OOD Dataset Average Precision
TinyImageNet(crop) 96.8
TinyImageNet(resize) 99.0
LSUN 58.0
LSUN(resize) 99.7
iSUN 99.2
need to be evaluated in AD settings using the proposed synthetic datasets, and new solutions need to be
proposed. As the openness score is usually high for AD detectors, having a high recall while providing a low
false alarm rate is necessary for their practicality Cvach (2012).
Furthermore, almost all the AD or ND methods are evaluated in the one-vs-all setting. This results in having
a normal class with a few distribution modes, which is not a good approximation of real-world scenarios.
Therefore, evaluating AD or ND methods in multi-class settings similar to OSR domain while having no
access to labels could give a more clear perspective on the practicality of SOTA methods. Table 7 reports
the performance of the most notable works using the standard evaluation protocols explained before. All
the results are reported from the main papers. (R1)
9.3 OSR Methods for Pixel-Level Datasets
Almost all the methods existing in OSR are evaluated on semantic datasets. As class boundaries in such
datasets usually are far from each other, discriminative or generative methods can model their differences
effectively. However, in many applications such as Chest X-ray datasets, variations are subtle. Existing
methods can result in poor performance on such tasks. For instance, a model may be trained on 14 known
chest diseases. A new disease, for example, COVID-19, may emerge as unknowns. In this case, our model
must detect it as a new illness instead of classifying it into pre-existing disease categories. Also, in many
clinical applications where medical datasets are gathered, usually, disease images are more accessible than
healthy ones; thus, OSR problems must be learned on sickness as our normal images and detect healthy ones
as abnormal inputs.
Table 5 shows the performance of a simple MSP baseline on MVTecAD dataset when some frequent faults
are considered as normal classes. In such scenarios, the goal is to detect and classify well-known faults
while distinguishing rare ones as outliers that need to be treated differently. Although this is a common
and practical industrial setting, the baseline does not perform better than random, casting doubt on their
generality for safety-critical applications.
Recently, Cheng & Vasconcelos (2021) has shown the effectiveness of using a prior Gaussian distribution on
the penultimate layer of classifier networks, similar to what has been done in several before-mentioned works,
in tasks in which the distribution of seen classes are very similar to each other, for instance, Flowers or Birds
datasets that introduced in the previous sections. However, more research should be done in this setting
since it is more practical and quite harder than the traditional ones. Table 8 reports the performance of the
most notable works using the standard evaluation protocols explained before. All the results are reported
from the main papers. (R1)
9.4 Small Sample Size
Learning with a small sample size is always challenging, but desirable. One way of approaching the problem
couldbetoexploitmeta-learningalgorithmsFinnetal.(2017);Nicholetal.(2018), andtolearngeneralizable
features that can be easily adapted to AD, ND, OSR, or OOD detection using a few training samples Lu
et al. (2020). One challenge in meta-learning is handling distribution shift between training and adaptation
phase, which might result in producing one-class meta-learning algorithms such as Frikha et al. (2020). Also,
other approaches explored generating a synthetic OOD dataset to improve few-shot classification of in-class
57Under review as submission to TMLR
samples Jeong & Kim (2020). Although, the combination of meta-learning and AD, ND, OOD detection,
and OSR has gained significant attention recently, some important aspects—such as generalizing to detect
UUCs using only a few KUC and the convergence of meta-learning algorithms in one-class setting—remain
underexplored.
9.5 Adversarial Robustness
Carefully designed imperceptible perturbations fooling deep learning-based models to make incorrect predic-
tions are called adversarial attacks Yuan et al. (2019). Up to this time, it has been shown that classifiers are
susceptible to adversarial attacks, such that their performance degrades significantly at the test time. As in
OOD detection, OSR, AD and ND, being robust against adversarial attacks is crucial. Recent works in OSR
Shao et al. (2020); Rozsa et al. (2017), ND Salehi et al. (2021a; 2020), and OOD detection Meinke et al.
(2021); Chen et al. (2021) have investigated the effects of adversarial attacks on models; however, more is
needed. For instance, as anomalies in AD or UUCs in OSR are inaccessible at the training time, therefore,
achieving a robust model on attacked anomalies or UUCs would not be trivial.
The relation of different defense approaches against adversarial attacks with novelty detection can also
reveal some important insights about the internal mechanisms of our models. For instance, membership
attack Shokri et al. (2017) attempts to infer whether an input sample has been used during the training
process or not, which can be seen as designing novelty detectors without having any generalization to UKC
samples. Also, Du et al. (2019) investigates the relation of detecting poisoning attacks and novelty detectors.
Poisoning examples that are intentionally added by attackers to achieve backdoor attacks could be treated
as one type of “outliers” in the training dataset. It is claimed that differential privacy not only improves
outlier detection and novelty detection, but also backdoor attack detection of ND models.
From an entirely different point of view, as it is mentioned in Ilyas et al. (2019), adversarial robust training
can be employed to boost learned feature space in a semantic way. This path has been followed in ARAE
Salehi et al. (2021a) and Puzzle-AE Salehi et al. (2020) to improve the performance of AEs in detecting
unseen test time samples. Similar intention is followed in the one-class learning method Goyal et al. (2020)
that shows robustness is beneficial for detecting novel samples. This path also needs to be explored more, for
instance despite standard adversarial attacks in classification tasks Madry et al. (2017), attacks do not need
to be imperceptible anymore in AD or ND and sometimes perceptible ones improve detection performance
more.
9.6 Fairness and Biases of Models
Research on fairness has witnessed a significant growth recently Zemel et al. (2013); Burrell (2016); Dwork
et al. (2012):. It has been shown that models get biased towards some sensitive variables during their training
process. For instance, Wang et al. (2020) show that for attribute classification task in the CelebA Liu et al.
(2015) dataset, attribute presence is correlated with the gender of people in the image, which is obviously
not desirable. Attributes such as gender in the mentioned example are called protected variables. In OOD
detection literature, a recent work Ming et al. (2021) systematically investigates how spurious correlation
in the training set impacts OOD detection. The results suggest that the OOD detection performance is
severely worsened when the correlation between spurious features and labels is increased in the training
set. For example, a model that exploits the spurious correlation between the water background and label
waterbird for prediction. Consequently, a model that relies on spurious features can produce a high-
confidence prediction for an OOD input with the same background (i.e., water) but a different semantic
label (e.g., boat).
Fairness and AD or ND seems to have fundamental contrast with each other. In fairness , we tend to make
unbiased models in which equality constraints between minority samples and majority ones hold while the
goal of AD models is to assign higher anomaly scores to rarely happen events. To address this issue Shekhar
et al. (2020); Zhang & Davidson (2021) proposed fairness-aware ADs while using the label of protected
variables as an extra supervision in the training process.
58Under review as submission to TMLR
Table 3: OOD example detection for the maximum softmax probability (MSP) baseline detector, maximum
logit value, the MSP detector after fine-tuning with Outlier Exposure (OE), the maximum logit value after
fine-tuning with Outlier Exposure (OE), ensemble of 3 models, Mahalanobis distance, and Monte Carlo
dropout. Inlier distribution is considered as cifar10. All results are average percentages of 10 runs.
Method Criterion Gaussian Rademacher Blob TinyImageNet(crop) TinyImageNet(resize) LSUN(crop) LSUN(resize) iSUN SVHN CIFAR-100
MSPFPR95 14.53 94.78 70.50 17.06 40.10 12.65 29.23 36.22 28.37 43.27
AUROC 94.78 79.85 94.63 94.64 88.30 96.45 91.40 90.00 91.94 87.77
AUPR 70.50 32.21 74.23 75.09 58.15 83.16 65.36 62.46 67.10 55.68
MLVFPR95 52.60 73.27 11.67 9.59 47.67 4.93 27.28 36.42 43.54 56.52
AUROC 75.48 70.08 96.85 97.84 89.16 98.93 94.05 93.38 91.11 87.13
AUPR 27.07 25.47 83.56 90.31 65.65 95.35 78.18 73.99 72.08 61.47
MSP-OEFPR95 0.71 0.50 0.58 6.61 13.00 1.32 5.16 5.64 4.77 28.36
AUROC 99.60 99.78 99.84 98.77 97.27 99.70 98.95 98.87 98.42 93.29
AUPR 94.25 97.36 98.94 95.06 88.08 98.56 94.56 94.20 89.33 76.19
MLV-OEFPR95 0.69 0.43 0.49 4.98 11.17 1.11 4.10 4.52 4.08 30.38
AUROC 99.62 99.79 99.86 98.96 97.58 99.74 99.11 99.02 98.61 93.10
AUPR 94.30 97.46 99.07 95.72 89.10 98.71 95.15 94.68 90.11 76.36
EnsembleFPR95 6.84 16.71 16.71 15.99 100 12.34 25.04 100.00 16.71 100.00
AUROC 97.37 86.94 91.20 93.18 85.69 95.23 90.21 88.00 92.05 83.90
AUPR 82.32 41.71 64.52 71.49 56.32 78.07 64.99 61.03 67.29 53.00
MahalanobisFPR95 1.35 2.01 7.38 35.82 48.38 28.61 27.98 39.02 24.79 48.40
AUROC 99.57 99.60 98.21 87.78 87.75 87.10 92.25 90.40 90.86 86.71
AUPR 96.49 97.95 90.63 46.79 55.33 41.59 65.14 62.17 53.36 54.06
MC-DropoutFPR95 15.31 33.58 16.54 20.75 38.77 16.81 28.44 34.62 28.73 37.48
AUROC 93.89 83.41 94.73 93.55 88.52 95.09 91.36 89.73 91.07 88.43
AUPR 63.52 35.40 74.91 71.65 58.19 77.26 65.34 61.76 62.41 56.84
ODINFPR95 0.00 0.00 99.4 04.30 07.50 04.80 03.80 06.10 51.00 51.40
AUROC 100.00 99.90 42.50 99.10 98.50 99.00 99.20 98.80 89.90 88.3
AUPR 63.52 35.40 74.91 71.65 58.19 77.26 65.34 61.76 62.41 56.84
From a different point of view Ye et al. (2021), introduces a significantly important bias in semi-supervised
anomaly detection methods such as DSAD Ruff et al. (2019). Suppose DSAD has been implemented in
law enforcement agencies to spot suspicious individuals using surveillance cameras. As a few number of
training samples has been used as abnormal samples during the process, the trained model might have been
biased towards detecting special type of anomalies more than others. For instance, if the auxiliary abnormal
training dataset includes more men than women, boundaries of detecting abnormal events as men might
be looser than women at the test time. This could also happen in the classification settings such as OOD
detection or OSR. Seyyed-Kalantari et al. (2020) reports the existence of unfair biases toward some unrelated
protected variables in detecting chest diseases for a classifier trained on Chest X-Ray datasets. From what
has been said, it seems fairness and AD, ND, OSR, and OOD detection are strongly correlated because of
some critical applications in which they are used and researching more on their correlation is necessary for
having practical models.
9.7 Multi-Modal Datasets
In many situation, training dataset consists of multi-modal training samples, for instance in Chest-X-Ray
datasets, labelsofimagesarefoundautomaticallybyapplyingNLPmethodsontheprescribesofradiologists.
In these situations, joint training of different modes could help models to learn better semantic features.
However, in this way, models need to be robust in different modes too. For example, in visual Question
Answering tasks, we expect our model not to produce any answer for out-of-distribution input texts or
images. Note that the correlation between different modes must be attended here, and independent training
of AD, ND, OOD detection, or OSR models for different modes results in sticking in local minimas. To
cope with this issue, Lee et al. (2020) has explored the performance of VQA models in detecting unseen
test time samples. However, there are many more challenges need to be investigated in this way. For
instance, the robustness and fairness of VQA OOD models is significantly more challenging compared to
single mode datasets, besides due to heavy training process of these models, inventing few-shot methods
might be demanding in the fields.
59Under review as submission to TMLR
Table 4: OOD example detection for the maximum softmax probability (MSP) baseline detector, maximum
logit value, the MSP detector after fine-tuning with Outlier Exposure (OE), the maximum logit value after
fine-tuning with Outlier Exposure (OE), ensemble of 3 models, Mahalanobis distance, and Monte Carlo
dropout. Inlier distribution is considered as cifar100. All results are average percentages of 10 runs.
Method Criterion Gaussian Rademacher Blob TinyImageNet(crop) TinyImageNet(resize) LSUN(crop) LSUN(resize) iSUN SVHN CIFAR-10
MSPFPR95 54.32 39.08 57.11 43.34 65.88 47.32 62.98 63.34 69.12 65.14
AUROC 64.66 79.27 75.61 86.34 74.56 85.56 75.59 75.73 71.43 75.12
AUPR 19.69 30.05 29.99 56.98 33.71 56.49 34.11 33.88 30.44 33.92
ODINFPR95 01.20 13.90 13.70 09.20 37.60 07.20 32.30 36.40 37.00 76.4
AUROC 99.50 92.60 95.90 97.90 90.80 98.30 91.90 90.50 89.00 73.20
AUPR 98.70 83.70 94.50 97.70 89.90 98.20 90.90 87.80 86.30 70.60
MLVFPR95 71.89 72.35 81.09 22.51 66.17 22.20 61.30 60.86 67.01 64.41
AUROC 44.24 46.22 53.62 94.72 77.72 95.09 79.54 79.19 74.03 77.55
AUPR 13.82 14.20 15.98 79.10 38.00 81.11 39.14 37.27 31.99 37.30
MSP-OEFPR95 12.41 16.89 12.04 22.02 69.42 13.27 60.89 62.42 43.10 62.57
AUROC 95.69 93.01 97.11 95.69 76.04 97.55 80.94 79.96 86.86 75.41
AUPR 71.13 56.81 85.91 85.34 39.57 90.99 48.52 45.86 53.27 32.28
MLV-OEFPR95 10.71 16.66 8.09 17.34 73.95 08.50 56.02 60.73 32.59 64.91
AUROC 96.12 91.86 97.94 96.38 75.84 98.31 83.33 81.89 88.91 73.74
AUPR 72.81 52.03 88.60 86.55 39.72 92.90 51.06 48.21 54.72 30.48
EnsembleFPR95 22.72 43.51 48.07 44.68 100.00 47.26 100.00 91.44 57.18 57.18
AUROC 89.15 68.64 79.24 82.90 70.47 82.39 70.66 71.08 73.61 75.13
AUPR 46.45 21.61 35.80 44.31 28.98 44.12 28.75 28.66 31.76 32.62
MahalanobisFPR95 0.82 0.10 2.70 73.79 43.40 76.42 37.94 42.07 32.05 70.80
AUROC 99.78 99.98 99.48 57.77 87.62 54.35 90.12 88.91 92.76 70.99
AUPR 98.63 99.88 97.64 17.27 60.18 16.11 65.97 62.82 75.87 27.12
MC-DropoutFPR95 54.45 41.41 46.64 47.32 68.05 55.38 63.53 65.03 75.98 63.33
AUROC 62.35 76.51 80.59 85.23 73.66 82.24 74.93 74.70 68.89 76.87
AUPR 18.74 27.14 35.08 54.12 32.57 49.08 33.04 32.26 28.63 36.31
9.8 Explainablity Challenge
Explainable AI (XAI) has found a seriously important role in the recently proposed deep network architec-
tures, especially when they are used in safety-critical applications Arrieta et al. (2020). In AD, OSR, ND
and OOD detection due to some of their critical applications, we should be able to explain the reason of
decisions our models make Hendrycks et al. (2021a). For instance, if a person is distinguished as suspicious
in the surveillance cameras, there must be good reasons for that by which the model has made its decision.
The challenges of explainability can be defined into two different approaches. First, we should explain
why a sample is normal, known or in-distribution. Secondly, we should explain why a sample is abnormal,
unknown or out-of-distribution. There are a lot of different techniques to explain the decisions of models
in the literature such as Grad-cam Selvaraju et al. (2017), Smoothfgrad Smilkov et al. (2017), which have
been used in Multi-KD Salehi et al. (2021b), CutPaste Li et al. (2021) and Venkataramanan et al. (2020).
However, they only have been used to explain normal, seen or in-distribution samples and their results are
not as accurate as enough for unseen or abnormal inputs. To cope with the issue, Liu et al. (2020c) proposed
a VAE based method which can provide the reasons of abnormality of input samples while performing
accuratelyon explainingnormal samplesas well. However, itdoes notwork well on complex training datasets
such as CIFAR-10, which shows the need of conducting more research toward mitigating the problem.
Another important challenge of explainability can be seen in one-class classification or ND approaches, in
which there is only access to one-label at the training time. Therefore, Gradcam or Smoothgrad which use
the availability of fine-grain labels can not be used anymore. To address this issue, Liznerski et al. (2020)
proposed a fully convolutional architecture with a heatmap upsampling algorithm that is called receptive
field upsampling, which starts from a sample latent vector and reverse the effect of applied convolution
operators to find important regions in the given input sample. However, explainable OCC models are still
largely unexplored and more investigations in this direction are still necessary.
60Under review as submission to TMLR
Table 5: OOD example detection for the maximum softmax probability (MSP) baseline detector. Inlier
distribution is a set of faults in MVTecAD dataset, and outliers are rare faults. All results are average
percentages of 10 runs.
Class Name Normal Sets AUC FPR AUPR
Cable good,combined, missing cable, poke insulation 51.70 1 24.20
Capsule good, poke, faulty imprint, crack 56.40 1 15.80
Wood good, color, scratch, liquid 53.30 1 86.40
Carpet good, metal, contamination, hole 50.20 1 14.70
9.9 Multi-Label OOD Detection and Large-Scale Datasets
While OOD detection for multi-class classification has been extensively studied, the problem for multi-label
networks remain underexplored Wang et al. (2021). This means each input has more than one true label by
which must be recognized. This is more challenging since multi-label classification tasks have more complex
class boundaries, and unseen behaviors could happen in a subset of input sample labels. Another challenge
of multi-label datasets can be explored in anomalous segmentation tasks. Different from classification in
which one can report an entire image as an abnormal input, the specific anomalous part must be specified
here.
Current methods have been primarily evaluated on small data sets such as CIFAR. It’s been shown that ap-
proaches developed on the CIFAR benchmark might not translate effectively into ImageNet benchmark with
a large semantic space, highlighting the need to evaluate OOD detection in a large-scale real-world setting.
Therefore, we encourage future research to evaluate on ImageNet-based OOD detection benchmark Huang
& Li (2021), and test the limits of the method developed.
9.10 Data Augmentation
One source of uncertainty in classifying known or normal training samples could be the lack of generalization
performance. For instance, if one rotates a bird image, its content is not harmed and must be distinguished
as the bird again. Some of the mentioned works try to embed this ability into models by designing different
SSL objective functions. However, there is also another way to do that, using data augmentations. Data
augmentation is a common technique to enrich the training dataset. Some approaches DeVries & Taylor
(2017); Zhang et al. (2017); Yun et al. (2019); Hendrycks et al. (2019b); Salehi et al. (2021a) improve
generalization performance using different data augmentation techniques.
From another point of view, Pourreza et al. (2021) attempts to generate unseen abnormal samples and use
them to convert a one-class learning problem into a simple two-class classification task. Similarly, however
in the OSR setting, Kong & Ramanan (2021) and Ditria et al. (2020) follow the same idea. All These works
also can be seen as working on a training dataset to make it richer for the further detection task. From what
has been said, it is obvious that working on data instead of models could achieve very effective results and
must be investigated more in the sense of different trade-offs in the future.
9.11 Open-World Recognition
Although detecting novel, unknown or out-of-distribution samples is enough in controlled lab environments,
novel categories must be continuously detected and then added to the recognition function in real-world
operational systems. This becomes even more challenging when considering the fact that such systems
require minimal downtime, even to learn Bendale & Boult (2015). While there has been much research on
incremental Rebuffi et al. (2017) and life-long learning Parisi et al. (2019) for addressing the problem of
adding new knowledge to a pre-existing one, open-world recognition needs a few more steps. This means
61Under review as submission to TMLR
Table 6: OOD example detection for the maximum softmax probability (MSP) baseline detector, maximum
logit value (MLV), the MSP detector after fine-tuning with Outlier Exposure (OE), the maximum logit value
after fine-tuning with Outlier Exposure (OE), ensemble of 3 models, Mahalanobis distance, and Monte Carlo
dropout. Inlier distribution is considered as TinyImageNet. All results are average percentages of 10 runs.
Method Criterion Gaussian Rademacher Blob LSUN(crop) LSUN(resize) iSUN SVHN
MSPFPR95 72.34 47.60 90.31 29.33 44.37 45.68 44.75
AUROC 33.36 70.52 22.79 93.66 86.16 85.94 89.05
AUPR 12.27 22.76 10.55 77.91 50.79 51.16 67.21
ODINFPR95 43.70 59.40 74.60 14.80 38.90 38.90 23.70
AUROC 70.00 50.80 46.20 96.80 87.10 87.60 93.90
AUPR 56.60 45.10 43.10 96.60 83.10 87.60 92.80
MLVFPR95 67.38 21.56 97.58 10.96 28.53 27.80 27.51
AUROC 45.34 90.24 15.96 97.75 91.71 91.98 94.09
AUPR 14.15 49.31 9.77 91.22 64.00 66.12 79.28
MSP-OEFPR95 45.32 49.53 0.05 0.53 0.12 0.12 0.39
AUROC 76.30 65.11 99.99 99.76 99.97 99.97 99.83
AUPR 28.32 19.97 99.93 98.37 99.82 99.79 98.16
MLV-OEFPR95 11.21 46.46 0.05 0.52 0.11 0.12 0.38
AUROC 95.45 68.30 99.99 99.81 99.97 99.97 99.83
AUPR 66.66 21.45 99.93 98.48 99.81 99.79 98.16
EnsembleFPR95 71.09 45.96 78.16 46.55 57.62 58.94 54.60
AUROC 35.75 74.09 51.86 83.72 76.54 75.70 77.09
AUPR 12.61 25.61 15.70 50.66 34.67 33.48 34.89
MahalanobisFPR95 66.87 48.15 22.23 98.46 72.04 79.93 96.83
AUROC 48.74 70.28 92.41 13.33 71.11 66.65 27.59
AUPR 14.78 22.60 62.45 9.48 28.17 25.19 10.81
MC-DropoutFPR95 76.09 56.14 91.36 30.44 43.25 47.22 47.67
AUROC 30.38 58.92 21.31 93.13 85.68 84.45 87.44
AUPR 11.82 17.54 10.39 76.53 48.49 46.56 62.24
Table 7: AUROC results of the most notable works for anomaly/novelty detection. The performance is
averaged for each dataset in the one-vs-all setting. (R1)
Method MNIST Fashion-MNIST CIFAR-10 CIFAR-100 MVTec AD
OC-GAN 97.50 - 65.60 - -
LSA 97.50 - 64.10 - 73.00
AnoGan 91.30 - 61.79 - 55.00
OC-SVM 96.00 92.80 58.56 - 47.90
DeepSVDD 94.80 92.80 64.81 67.00 72.00
GT - 93.50 86.00 78.70 67.06
CSI - - 94.30 89.55 63.6
U-std 99.35 - 81.96 - 85.70
Multiresolution 98.71 94.49 87.18 - 87.74
Contrastive DA - 94.50 92.40 86.50 86.50
novel classes must be found continuously, and the system must be updated to include these new classes in
its multi-class open-set recognition algorithm.
The mentioned process poses many different challenges, from the scalability of current open-set recognition
algorithms to designing new learning algorithms to avoid problems such as catastrophic forgetting Rebuffi
et al. (2017) of OSR classifiers. Furthermore, all the previously mentioned future works can be reformulated
62Under review as submission to TMLR
Table8: AUROCresultsofthemostnotableworksforOSR.Theperformanceisaveragedacrosseachdataset
using the explained evaluation procedures for 10 random trials. (MLS stands for Vaze et al. (2021)). (R1)
Method MNIST SVHN CIFAR-10 CIFAR + 10 CIFAR + 50 TinyImageNet
OpenMax 98.10 89.40 81.10 81.70 79.60 81.10
G-OpenMax 98.40 89.60 67.60 82.70 81.90 58.00
OSRCI 98.80 91.00 69.90 83.80 82.70 58.60
C2AE 98.90 92.20 89.50 95.50 93.70 74.80
CROSR 99.20 89.90 88.30 - - 58.90
GDFR - 93.50 80.70 92.80 92.60 60.80
RPL 99.60 96.80 90.10 97.60 96.80 80.90
OpenGan 99.90 98.80 97.30 - - 90.70
MLS 99.30 97.10 93.60 97.90 96.50 83.00
Table 9: OSR AUROC (R1) results of baselines such as the maximum softmax probability (MSP), Openmax,
and CROSR compared to ViT-B16. Known distribution is considered a random selection of 6 classes of the
respective datasets and unknown ones are the rest. All results are percentages and the average of 5 runs.
Method SVHN MNIST
Softmax 88.60 97.80
Openmax 89.40 98.10
CROSR 89.90 99.10
ViT-B16 82.18 94.89
in open-world recognition problems again, which means by considering a few existing works in these subjects,
it needs to be explored more.
9.12 Vision Transformers in OOD Detection and OSR
Vision Transformers (ViTs) Dosovitskiy et al. (2020) have recently been proposed to replace CNNs and
have shown a great performance in different applications such as object detection Carion et al. (2020),
medical image segmentation Valanarasu et al. (2021), and visual tracking Chen et al. (2020a). Similarly,
some methods have recently reported the benefits of ViTs in OOD detection Koner et al. (2021); Fort et al.
(2021) and have shown their capabilities at detecting near OOD samples. For instance, Fort et al. (2021) has
reported the significant superiority of ViTs compared to previous works when they are trained on CIFAR-10
andtestedonCIFAR-100asinlierandoutlierdatasets, respectively. However, asViTsusuallygetpre-trained
on extra-large datasets such as ImageNet-22K that has a large intersection with training and testing datasets,
the integrity of the train-test mismatch does not hold anymore, and the problem would be converted to “how
much does it remember from pretraining”. This means ViTs should be evaluated on datasets that have no
intersection with the pre-trained knowledge.
To address this issue, we have evaluated ViT-B16Dosovitskiy et al. (2020) on SVHN and MNIST when
six randomly selected classes are considered as normal and the remaining ones as outliers or unseens. We
consider MSP to detect unknown samples. As Table 9 shows, ViT-B16 that is pre-trained on ImageNet-22K
is not even as good as other baselines that are trained from scratch. As all the experiments are evaluated in
a near OOD detection setting, they support the before-mentioned deficiency of ViTs. From what has been
said, a future line of research could be evaluating ViTs in a more controlled situation such that their real
benefits would be more precise. Indeed, the recent Species dataset collects examples that do not fall under
any of the ImageNet-22K classes and is a first step to rectify this problem Hendrycks et al. (2020).
63Under review as submission to TMLR
9.13 Identifiablity Problems
Providing a method whereby unfamiliar samples are detected is not always trivial. Garg et al. (2021)
investigates the theoretical foundations of the problem, demonstrating that determining the accuracy is
just as difficult as identifying the ideal predictor, and so the success of any method is dependent on the
implicit assumptions about the nature of the shift, which is characterized by mismatches between the source
(training) and target (test) distributions. It is demonstrated that no approach of measuring accuracy will
work in general without assumptions on the source classifier or the nature of the shift. Average Thresholded
Confidence (ATC) is a simple approach for predicting model performance using softmax probability. It learns
a threshold for model confidence on the validation source data and predicts the target domain accuracy as
the proportion of unlabeled target points with a score greater than the threshold. This work advances
the positive answer to the question of a feasible technique for selecting a threshold that enables prediction
accuracy with the thresholded model confidence.(R2)
10 Conclusion
In many applications, it is not feasible to model all kinds of classes occurring during testing; thus, scenarios
existing in domains such as OOD detection, OSR, ND (one-class learning), and AD become ubiquitous. Up
to this time, these domains, despite having the same intention and a large intersection, have been followed
roughly independently by researchers.
To address the need, this paper gives a comprehensive review on existing techniques, datasets, evaluation
criteria, and future challenges. More importantly, we analyzed and discussed the limitations of approaches
and pointed out some promising research directions. We hope this helps the research community build a
broader and cross-domain perspective.
64Under review as submission to TMLR
References
Davide Abati, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Latent space autoregression for nov-
elty detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 481–490, 2019.
Faruk Ahmed and Aaron Courville. Detecting semantic anomalies. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 34, pp. 3154–3162, 2020.
Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial networks.
arXiv preprint arXiv:1701.04862 , 2017.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning , pp. 214–223. PMLR, 2017.
Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Al-
berto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. Explainable
artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai.
Information Fusion , 58:82–115, 2020.
Christian Bailer, Tewodros Habtegebrial, Didier Stricker, et al. Fast feature extraction with cnns with
pooling layers. arXiv preprint arXiv:1805.03096 , 2018.
Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico Karssemeijer,
Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson, Maschenka Balkenhol,
et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women
with breast cancer. Jama, 318(22):2199–2210, 2017.
Abhijit Bendale and Terrance Boult. Towards open world recognition. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pp. 1893–1902, 2015.
Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pp. 1563–1572, 2016.
Liron Bergman and Yedid Hoshen. Classification-based anomaly detection for general data. arXiv preprint
arXiv:2005.02359 , 2020.
PaulBergmann, MichaelFauser, DavidSattlegger, andCarstenSteger. Mvtecad–acomprehensivereal-world
dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 9592–9600, 2019.
Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-
teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pp. 4183–4192, 2020.
David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow. Understanding and improving interpolation
in autoencoders via an adversarial regularizer. arXiv preprint arXiv:1807.07543 , 2018.
Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. Lof: identifying density-based
local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data ,
pp. 93–104, 2000.
Saikiran Bulusu, Bhavya Kailkhura, Bo Li, P Varshney, and Dawn Song. Anomalous instance detection
in deep learning: A survey. Technical report, Lawrence Livermore National Lab.(LLNL), Livermore, CA
(United States), 2020.
Jenna Burrell. How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data
& Society , 3(1):2053951715622512, 2016.
65Under review as submission to TMLR
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vi-
sion, pp. 213–229. Springer, 2020.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised
learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV) , pp.
132–149, 2018.
Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey. arXiv preprint
arXiv:1901.03407 , 2019.
Guangyao Chen, Limeng Qiao, Yemin Shi, Peixi Peng, Jia Li, Tiejun Huang, Shiliang Pu, and Yonghong
Tian. Learning open set network with discriminative reciprocal points. arXiv preprint arXiv:2011.00178 ,
2020a.
Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distribution
detectionusingoutliermining. In Proceedings of European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases (ECML PKDD) , 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning , pp. 1597–1607. PMLR,
2020b.
Jiacheng Cheng and Nuno Vasconcelos. Learning deep classifiers consistent with fine-grained novelty de-
tection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
1664–1673, 2021.
Hyunsun Choi and Eric Jang. Generative ensembles for robust anomaly detection. 2018.
ThomasM.CoverandJoyA.Thomas. Elements of Information Theory (Wiley Series in Telecommunications
and Signal Processing) . Wiley-Interscience, USA, 2006. ISBN 0471241954.
Maria Cvach. Monitor alarm fatigue: an integrative review. Biomedical instrumentation & technology , 46
(4):268–277, 2012.
Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. Padim: a patch distribution
modeling framework for anomaly detection and localization. arXiv preprint arXiv:2011.08785 , 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee,
2009.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with
cutout.arXiv preprint arXiv:1708.04552 , 2017.
Akshay Raj Dhamija, Manuel Günther, and Terrance E Boult. Reducing network agnostophobia. arXiv
preprint arXiv:1811.04110 , 2018.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation.
arXiv preprint arXiv:1410.8516 , 2014.
Luke Ditria, Benjamin J Meyer, and Tom Drummond. Opengan: Open set generative adversarial networks.
InProceedings of the Asian Conference on Computer Vision , 2020.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly detection and backdoor attack detection via differential
privacy.arXiv preprint arXiv:1911.07116 , 2019.
66Under review as submission to TMLR
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference , pp. 214–226,
2012.
Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering
clusters in large spatial databases with noise. In kdd, volume 96, pp. 226–231, 1996.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In International Conference on Machine Learning , pp. 1126–1135. PMLR, 2017.
Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection.
arXiv preprint arXiv:2106.03004 , 2021.
Ahmed Frikha, Denis Krompaß, Hans-Georg Köpken, and Volker Tresp. Few-shot one-class classification via
meta-learning. arXiv preprint arXiv:2007.04146 , 2020.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In international conference on machine learning , pp. 1050–1059. PMLR, 2016.
Saurabh Garg, Sivaraman Balakrishnan, Zachary Chase Lipton, Behnam Neyshabur, and Hanie Sedghi.
Leveraging unlabeled data to predict out-of-distribution performance. In NeurIPS 2021 Workshop on
Distribution Shifts: Connecting Methods and Applications , 2021.
ZongYuan Ge, Sergey Demyanov, Zetao Chen, and Rahil Garnavi. Generative openmax for multi-class open
set classification. arXiv preprint arXiv:1707.07418 , 2017.
Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. Recent advances in open set recognition: A survey.
IEEE transactions on pattern analysis and machine intelligence , 2020.
Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. arXiv preprint
arXiv:1805.10917 , 2018.
DongGong, LingqiaoLiu, VuongLe, BudhadityaSaha, MoussaRedaMansour, SvethaVenkatesh, andAnton
van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for
unsupervised anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 1705–1714, 2019.
Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri, and Prateek Jain. Drocc: Deep
robust one-class classification. In International Conference on Machine Learning , pp. 3711–3721. PMLR,
2020.
Patrick Grother and Kayee Hanaoka. Nist special database 19 handprinted forms and characters 2nd edition.
National Institute of Standards and Technology, Tech. Rep , 2016.
Frank E Grubbs. Procedures for detecting outlying observations in samples. Technometrics , 11(1):1–21,
1969.
Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai. Triplet-center loss for multi-view 3d
object retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.
1945–1954, 2018.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-confidence
predictions far away from the training data and how to mitigate the problem. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 41–50, 2019.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. arXiv preprint arXiv:1903.12261 , 2019.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples
in neural networks. arXiv preprint arXiv:1610.02136 , 2016.
67Under review as submission to TMLR
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure.
arXiv preprint arXiv:1812.04606 , 2018.
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can
improve model robustness and uncertainty. arXiv preprint arXiv:1906.12340 , 2019a.
Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan.
Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint
arXiv:1912.02781 , 2019b.
Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joseph Kown, Mohammadreza Mostajabi,
Jacob Steinhardt, and Dawn Xiaodong Song. Scaling out-of-distribution detection for real-world settings.
arXiv: 1911.11132 , 2020.
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety.
ArXiv, abs/2109.13916, 2021a.
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety.
arXiv preprint arXiv:2109.13916 , 2021b.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial ex-
amples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
15262–15271, 2021c.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed,andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational
framework. 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-distribution
image without learning from out-of-distribution data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 10951–10960, 2020.
Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 8710–8719,
2021.
Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts
in the wild. Advances in Neural Information Processing Systems , 2021.
Weibo Huang and Peng Wei. A pcb dataset for defects detection and classification. arXiv preprint
arXiv:1901.08204 , 2019.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
Adversarial examples are not bugs, they are features. arXiv preprint arXiv:1905.02175 , 2019.
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Mark-
lund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset
with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intel-
ligence, volume 33, pp. 590–597, 2019.
Taewon Jeong and Heeyoung Kim. Ood-maml: Meta-learning for few-shot out-of-distribution detection and
classification. Advances in Neural Information Processing Systems , 33, 2020.
Pedro R Mendes Júnior, Roberto M De Souza, Rafael de O Werneck, Bernardo V Stein, Daniel V Pazinato,
Waldir R de Almeida, Otávio AB Penatti, Ricardo da S Torres, and Anderson Rocha. Nearest neighbors
distance ratio open-set classifier. Machine Learning , 106(3):359–386, 2017.
68Under review as submission to TMLR
Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter,
Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al. Identifying medical diagnoses and treatable
diseases by image-based deep learning. Cell, 172(5):1122–1131, 2018.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,
Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint arXiv:2004.11362 , 2020.
Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. arXiv
preprint arXiv:1807.03039 , 2018.
Rajat Koner, Poulami Sinhamahapatra, Karsten Roscher, Stephan Günnemann, and Volker Tresp. Ood-
former: Out-of-distribution detection transformer. arXiv preprint arXiv:2107.08976 , 2021.
Shu Kong and Deva Ramanan. Opengan: Open-set recognition via open data generation. arXiv preprint
arXiv:2104.02939 , 2021.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding
beyond pixels using a learned similarity metric. In International conference on machine learning , pp.
1558–1566. PMLR, 2016.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/ .
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based learning.
Predicting structured data , 1(0), 2006.
Doyup Lee, Yeongjae Cheon, and Wook-Shin Han. Regularizing attention networks for anomaly detection
in visual question answering. arXiv preprint arXiv:2009.10054 , 2020.
Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting
out-of-distribution samples. arXiv preprint arXiv:1711.09325 , 2017.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-
distribution samples and adversarial attacks. arXiv preprint arXiv:1807.03888 , 2018.
Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. Cutpaste: Self-supervised learning for
anomaly detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 9664–9674, 2021.
Yi Li and Nuno Vasconcelos. Background data resampling for outlier-aware classification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 13218–13227, 2020.
Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. arXiv preprint arXiv:1706.02690 , 2017.
Ziqian Lin, Sreya Dutta Roy, and Yixuan Li. Mood: Multi-level out-of-distribution detection. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 15313–15323, 2021.
Bo Liu, Hao Kang, Haoxiang Li, Gang Hua, and Nuno Vasconcelos. Few-shot open-set recognition using
meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 8798–8807, 2020a.
FeiTonyLiu, KaiMingTing, andZhi-HuaZhou. Isolationforest. In 2008 eighth ieee international conference
on data mining , pp. 413–422. IEEE, 2008.
Weitang Liu, Xiaoyun Wang, John D Owens, and Yixuan Li. Energy-based out-of-distribution detection.
arXiv preprint arXiv:2010.03759 , 2020b.
69Under review as submission to TMLR
Wenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir Bhanu, Richard J Radke, and
Octavia Camps. Towards visually explaining variational autoencoders. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 8642–8651, 2020c.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus-Robert
Müller. Explainable deep one-class classification. arXiv preprint arXiv:2007.01760 , 2020.
Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, and Yang Wang. Few-shot scene-adaptive anomaly
detection. In European Conference on Computer Vision , pp. 125–141. Springer, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.
Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. arXiv preprint
arXiv:1802.10501 , 2018.
Alexander Meinke, Julian Bitterwolf, and Matthias Hein. Provably robust detection of out-of-distribution
data (almost) for free. arXiv preprint arXiv:2106.04260 , 2021.
Dimity Miller, Niko Sunderhauf, Michael Milford, and Feras Dayoub. Class anchor clustering: A loss for
distance-based open set recognition. In Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision , pp. 3570–3578, 2021.
Yifei Ming, Hang Yin, and Yixuan Li. On the impact of spurious correlation for out-of-distribution detection.
arXiv preprint arXiv:2109.05642 , 2021.
Sina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for generalizable
out-of-distribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34,
pp. 5216–5223, 2020.
SofiaMosci, LorenzoRosasco, MatteoSantoro, AlessandroVerri, andSilviaVilla. Solvingstructuredsparsity
regularization with proximal methods. In Joint European conference on machine learning and knowledge
discovery in databases , pp. 418–433. Springer, 2010.
Norman Mu and Justin Gilmer. Mnist-c: A robustness benchmark for computer vision. arXiv preprint
arXiv:1906.02337 , 2019.
Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deep
generative models know what they don’t know? arXiv preprint arXiv:1810.09136 , 2018.
Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set learning with
counterfactual images. In Proceedings of the European Conference on Computer Vision (ECCV) , pp.
613–628, 2018.
Sameer A. Nene, Shree K. Nayar, and Hiroshi Murase. object image library (coil-100. Technical report,
1996.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint
arXiv:1803.02999 , 2018.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.
In2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing , pp. 722–729. IEEE,
2008.
70Under review as submission to TMLR
AaronvandenOord, NalKalchbrenner, OriolVinyals, LasseEspeholt, AlexGraves, andKorayKavukcuoglu.
Conditional image generation with pixelcnn decoders. arXiv preprint arXiv:1606.05328 , 2016.
Poojan Oza and Vishal M Patel. C2ae: Class conditioned auto-encoder for open-set recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 2307–2316, 2019.
Guansong Pang, Chunhua Shen, Longbing Cao, and Anton van den Hengel. Deep learning for anomaly
detection: A review. arXiv preprint arXiv:2007.02500 , 2020.
Ashok Kumar Pant, Sanjeeb Prasad Panday, and Shashidhar Ram Joshi. Off-line nepali handwritten char-
acter recognition using multilayer perceptron and radial basis function neural networks. In 2012 Third
Asian Himalayas International Conference on Internet , pp. 1–5. IEEE, 2012.
German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong
learning with neural networks: A review. Neural Networks , 113:54–71, 2019.
Pramuditha Perera and Vishal M Patel. Deep transfer learning for multiple class novelty detection. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11544–11552,
2019.
Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. Ocgan: One-class novelty detection using gans with
constrained latent representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 2898–2906, 2019.
PramudithaPerera, VladIMorariu, RajivJain, VarunManjunatha, CurtisWigington, VicenteOrdonez, and
Vishal M Patel. Generative-discriminative feature representations for open-set recognition. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11814–11823, 2020.
Pramuditha Perera, Poojan Oza, and Vishal M Patel. One-class classification: A survey. arXiv preprint
arXiv:2101.03064 , 2021.
Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning
with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 32, 2018.
James Pickands III et al. Statistical inference using extreme order statistics. Annals of statistics , 3(1):
119–131, 1975.
MasoudPourreza, BahramMohammadi, MostafaKhaki, SamirBouindour, HichemSnoussi, andMohammad
Sabokrou. G2d: Generate to detect anomaly. In Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision , pp. 2003–2012, 2021.
Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti
Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist-level pneumonia detection on chest
x-rays with deep learning. arXiv preprint arXiv:1711.05225 , 2017.
Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised learn-
ing with ladder networks. arXiv preprint arXiv:1507.02672 , 2015.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental
classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition , pp. 2001–2010, 2017.
JieRen, PeterJLiu, EmilyFertig, JasperSnoek, RyanPoplin, MarkADePristo, JoshuaVDillon, andBalaji
Lakshminarayanan. Likelihood ratios for out-of-distribution detection. arXiv preprint arXiv:1906.02845 ,
2019.
Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A
simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022 ,
2021.
71Under review as submission to TMLR
Oliver Rippel, Patrick Mertens, and Dorit Merhof. Modeling the distribution of normal data in pre-trained
deep features for anomaly detection. In 2020 25th International Conference on Pattern Recognition
(ICPR), pp. 6726–6733. IEEE, 2021.
Andras Rozsa, Manuel Günther, and Terrance E Boult. Adversarial robustness: Softmax versus openmax.
arXiv preprint arXiv:1708.01697 , 2017.
Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder,
Emmanuel Müller, and Marius Kloft. Deep one-class classification. In International conference on machine
learning, pp. 4393–4402. PMLR, 2018.
Lukas Ruff, Robert A Vandermeulen, Nico Görnitz, Alexander Binder, Emmanuel Müller, Klaus-Robert
Müller, and Marius Kloft. Deep semi-supervised anomaly detection. arXiv preprint arXiv:1906.02694 ,
2019.
Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grégoire Montavon, Wojciech Samek, Marius
Kloft, Thomas G Dietterich, and Klaus-Robert Müller. A unifying review of deep and shallow anomaly
detection. Proceedings of the IEEE , 2021.
Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially learned
one-class classifier for novelty detection. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pp. 3379–3388, 2018a.
Mohammad Sabokrou, Masoud Pourreza, Mohsen Fayyaz, Rahim Entezari, Mahmood Fathy, Jürgen Gall,
and Ehsan Adeli. Avid: Adversarial visual irregularity detection. In Asian Conference on Computer
Vision, pp. 488–505. Springer, 2018b.
Mohammad Sabokrou, Mahmood Fathy, Guoying Zhao, and Ehsan Adeli. Deep end-to-end one-class classi-
fier.IEEE transactions on neural networks and learning systems , 32(2):675–684, 2020.
MohammadrezaSalehi,AinazEftekhar,NioushaSadjadi,MohammadHosseinRohban,andHamidRRabiee.
Puzzle-ae: Novelty detection in images through solving puzzles. arXiv preprint arXiv:2008.12959 , 2020.
MohammadrezaSalehi, AtrinArya, BarbodPajoum, MohammadOtoofi, AmirrezaShaeiri, MohammadHos-
sein Rohban, and Hamid R Rabiee. Arae: Adversarially robust training of autoencoders improves novelty
detection. Neural Networks , 2021a.
Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, and Hamid R Rabiee.
Multiresolution knowledge distillation for anomaly detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 14902–14912, 2021b.
Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E Boult. Toward open set
recognition. IEEE transactions on pattern analysis and machine intelligence , 35(7):1757–1772, 2012.
Robin Tibor Schirrmeister, Yuxuan Zhou, Tonio Ball, and Dan Zhang. Understanding anomaly detec-
tion with deep invertible networks through hierarchies of distributions and features. arXiv preprint
arXiv:2006.10848 , 2020.
Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Un-
supervised anomaly detection with generative adversarial networks to guide marker discovery. In Interna-
tional conference on information processing in medical imaging , pp. 146–157. Springer, 2017.
Bernhard Schölkopf, Robert C Williamson, Alexander J Smola, John Shawe-Taylor, John C Platt, et al.
Support vector method for novelty detection. In NIPS, volume 12, pp. 582–588. Citeseer, 1999.
Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised outlier
detection. arXiv preprint arXiv:2103.12051 , 2021.
72Under review as submission to TMLR
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In
Proceedings of the IEEE international conference on computer vision , pp. 618–626, 2017.
Joan Serrà, David Álvarez, Vicenç Gómez, Olga Slizovskaia, José F Núñez, and Jordi Luque. Input
complexity and out-of-distribution detection with likelihood-based generative models. arXiv preprint
arXiv:1909.11480 , 2019.
Laleh Seyyed-Kalantari, Guanxiong Liu, MatthewMcDermott, IreneY Chen, andMarzyehGhassemi. Chex-
clusion: Fairnessgapsindeepchestx-rayclassifiers. In BIOCOMPUTING 2021: Proceedings of the Pacific
Symposium , pp. 232–243. World Scientific, 2020.
Rui Shao, Pramuditha Perera, Pong C Yuen, and Vishal M Patel. Open-set adversarial defense. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XVII 16, pp. 682–698. Springer, 2020.
Shubhranshu Shekhar, Neil Shah, and Leman Akoglu. Fairod: Fairness-aware outlier detection. arXiv
preprint arXiv:2012.03063 , 2020.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against
machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP) , pp. 3–18. IEEE, 2017.
Daniel Smilkov, Nikhil Thorat, BeenKim, Fernanda Viégas, and MartinWattenberg. Smoothgrad: removing
noise by adding noise. arXiv preprint arXiv:1706.03825 , 2017.
Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, and Tomas Pfister. Learning and evaluating repre-
sentations for deep one-class classification. arXiv preprint arXiv:2011.02578 , 2020.
Gowthami Somepalli, Yexin Wu, Yogesh Balaji, Bhanukiran Vinzamuri, and Soheil Feizi. Unsupervised
anomaly detection with adversarial mirrored autoencoders. arXiv preprint arXiv:2003.10713 , 2020.
Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. How to train
deep variational autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282 , 3,
2016.
Xin Sun, Zhenning Yang, Chi Zhang, Keck-Voon Ling, and Guohao Peng. Conditional gaussian distribution
learning for open set recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 13480–13489, 2020.
Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations.
Advances in Neural Information Processing Systems , 34, 2021.
Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive learning
on distributionally shifted instances. arXiv preprint arXiv:2007.08176 , 2020.
Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer:
Gated axial-attention for medical image segmentation. arXiv preprint arXiv:2102.10662 , 2021.
Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set
classifier is all you need. arXiv preprint arXiv:2110.06207 , 2021.
Shashanka Venkataramanan, Kuan-Chuan Peng, Rajat Vikram Singh, and Abhijit Mahalanobis. Atten-
tion guided anomaly localization in images. In European Conference on Computer Vision , pp. 485–503.
Springer, 2020.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and
Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In International
Conference on Machine Learning , pp. 6438–6447. PMLR, 2019.
73Under review as submission to TMLR
Roman Vershynin. High-dimensional probability: An introduction with applications in data science , vol-
ume 47. Cambridge university press, 2018.
Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Can multi-label classification networks know
what they don’t know? Advances in Neural Information Processing Systems , 2021.
Siqi Wang, Yijie Zeng, Xinwang Liu, En Zhu, Jianping Yin, Chuanfu Xu, and Marius Kloft. Effective
end-to-end unsupervised outlier detection via inlier priority of discriminative network. 2019.
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-
ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and local-
ization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 2097–2106, 2017.
Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga
Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8919–8928, 2020.
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro
Perona. Caltech-ucsd birds 200. 2010.
Yan Xia, Xudong Cao, Fang Wen, Gang Hua, and Jian Sun. Learning discriminative reconstructions for
unsupervised outlier removal. In Proceedings of the IEEE International Conference on Computer Vision ,
pp. 1511–1519, 2015.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Zhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection score for
variational auto-encoder. arXiv preprint arXiv:2003.02977 , 2020.
Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng. Learning semantic context
from normal samples for unsupervised anomaly detection. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 35, pp. 3110–3118, 2021.
Ziyu Ye, Yuxin Chen, and Haitao Zheng. Understanding the effect of bias in deep anomaly detection. arXiv
preprint arXiv:2105.07346 , 2021.
RyotaYoshihashi, WenShao, ReiKawakami, ShaodiYou, MakotoIida, andTakeshiNaemura. Classification-
reconstructionlearningforopen-setrecognition. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 4016–4025, 2019.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Con-
struction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365 , 2015.
Qing Yu and Kiyoharu Aizawa. Unsupervised out-of-distribution detection by maximum classifier discrep-
ancy. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 9518–9526,
2019.
Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: Attacks and defenses for deep
learning. IEEE transactions on neural networks and learning systems , 30(9):2805–2824, 2019.
Zhongqi Yue, Tan Wang, Qianru Sun, Xian-Sheng Hua, and Hanwang Zhang. Counterfactual zero-shot and
open-set visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 15404–15414, 2021.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:
Regularizationstrategytotrainstrongclassifierswithlocalizablefeatures. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 6023–6032, 2019.
74Under review as submission to TMLR
Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, and Seung-Ik Lee. Old is gold: Redefining the
adversarially learned one-class classifier training paradigm. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 14183–14193, 2020.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In
International conference on machine learning , pp. 325–333. PMLR, 2013.
Houssam Zenati, Chuan Sheng Foo, Bruno Lecouat, Gaurav Manek, and Vijay Ramaseshan Chandrasekhar.
Efficient gan-based anomaly detection. arXiv preprint arXiv:1802.06222 , 2018.
Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. In European
Conference on Computer Vision , pp. 102–117. Springer, 2020.
Hongjing Zhang and Ian Davidson. Towards fair deep anomaly detection. In Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Transparency , pp. 138–148, 2021.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. arXiv preprint arXiv:1710.09412 , 2017.
Chong Zhou and Randy C Paffenroth. Anomaly detection with robust deep autoencoders. In Proceedings of
the 23rd ACM SIGKDD international conference on knowledge discovery and data mining , pp. 665–674,
2017.
Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Learning placeholders for open-set recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4401–4410, 2021.
A Appendix
You may include other additional sections here.
75