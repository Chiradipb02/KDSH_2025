Under review as submission to TMLR
Parametric Entropic Locally Linear Embedding for Small
Sample Size Classification
Anonymous authors
Paper under double-blind review
Abstract
Manifold learning algorithms are powerful non-linear dimensionality reduction methods for
unsupervised metric learning. The locally linear embedding (LLE) method uses the local ge-
ometryofthelinearneighborhoodspacestoestimateoptimalreconstructionweightsforeach
sample. In the present paper, we propose the parametric entropic LLE (PELLE) method,
which adopts the relative entropy instead of the pointwise Euclidean metric to build local
entropic covariance matrices. This methodological improvement increases the robustness
of the method regarding noise and outliers. Moreover, state-of-the-art algorithms such as
UMAP require a large number of samples for convergence to good results due to numer-
ical optimization methods (gradient descent). Results considering 25 distinct real-world
datasets indicate that the proposed method is capable of generating superior clustering and
classification accuracies compared to existing state-of-the-art methods for dimensionality
reduction-based metric learning, especially in datasets with a limited number of samples.
1 Introduction
Metric learning refers to the building of adaptive distance functions to a dataset prior to a classification
step. Manifold learning algorithms are capable of finding more compact and meaningful representations for
the observed data while preserving the intrinsic non-Euclidean geometry of the data. The locally linear
embedding (LLE) is one of the first algorithms of this class to be employed to classification tasks through
non-linear dimensionality reduction (Roweis and Saul, 2000). However, one of the main caveats of the LLE
is that such a method is remarkably sensitive to noise and outliers (Wang et al., 2019). Thus, the LLE
method performs poorly in data sets that do not lead to smooth manifolds. The existing literature contains
many LLE extensions proposed to overcome such a limitation.
Hessian eigenmaps replace the local covariance matrix used in the computation of the optimal reconstruction
weights by the Hessian matrix (i.e. matrix of the second order derivatives), encoding curvature information
(Donoho and Grimes, 2003; Wang, 2012; Xing et al., 2016). The local tangent space alignment (LTSA)
method was proposed as a generalization of the LLE method. The main difference between the LTSA and
LLE method is that in the former the locally linear patch is built by approximating the tangent space at
a particular sample through the application of principal component analysis (PCA) on the neighborhood
system. As each neighbor contains its own coordinate representation in the tangent space, we use them as
a low dimensional representation of the patch. Therefore, all local representations (i.e. patches) are then
aligned to a global one (Zhang and Zha, 2004). In addition, the modified LLE (MLLE) still extends the
LLE method by introducing multiple linearly independent local weight vectors for each neighborhood of the
k-nearest neighbors (KNN) graph. It has been demonstrated that the local geometry of the MLLE is more
stable compared to the LLE method (Zhang and Wang, 2006).
Nonetheless, all of these LLE methodological extensions adopt, at some degree, the Euclidean distance as the
similarity measure within a linear patch. Thus, in order to incorporate the relative entropy in the estimation
of the optimal reconstruction weights, in the present paper we then propose the parametric entropic LLE
(PELLE) method. There are three main contributions of the PELLE method to the literature in comparison
with existing methods. Firstly, the pointwise Euclidean distance is replaced by a patch-based information-
1Under review as submission to TMLR
theoretic distance (KL-divergence), making the method less sensitive to the presence of noise and outliers.
Secondly, the multicollinearity phenomenon in data patches degrades the performance of the LLE algorithm,
since it is, among other things, one of the causes of ill-conditioned matrices in the estimation of the optimal
reconstruction weights. With the incorporation of the relative entropy (KL-divergence), we can mitigate
this problem. Finally, results using several real-world data sets indicate that the PELLE method produces
superior clustering and classification accuracies for different supervised classifiers compared to the original
LLE as well as state-of-the-art manifold learning algorithms, such as the ISOMAP (Tenenbaum et al., 2000)
and UMAP (McInnes et al., 2018).
The remainder of the paper is organized as follows. Section 2 mentions related work, focusing on the
derivation of the regular LLE algorithm. Section 3 details the proposed PELLE method algorithm. Section
4 reports computational experiments and related results. Section 5 concludes and suggests future research
directions in metric learning-based dimensionality reduction.
2 Related work
In this section, we discuss the LLE algorithm, detailing its motivations and mathematical derivation.
2.1 Locally linear embedding (LLE) algorithm
The LLE algorithm consists of a local method, in which the new coordinates of any ⃗ xi∈Rmdepends only
on the neighborhood of the respective point. The main hypothesis behind the LLE is that of a sufficiently
high density of samples, being expected that a vector ⃗ xiand its neighbors define a linear patch, all belonging
to an Euclidean subspace (Roweis and Saul, 2000). Thus, we may characterize the local geometry by linear
coefficients, as follows:
ˆ⃗ xi≈/summationdisplay
jwij⃗ xjfor⃗ xj∈N(⃗ xi) (1)
that is, we may reconstruct a vector as a linear combination of its neighbors.
The LLE algorithm requires as inputs an n×mdata matrix X, with rows ⃗ xi, a certain number of dimensions
d<m, and an integer k>d + 1to find local neighborhoods. The output is an n×dmatrixY, with rows ⃗ yi.
The LLE algorithm may be divided into three main steps (Roweis and Saul, 2000; Saul and Roweis, 2003):
Firstly, from each ⃗ xi∈Rm, it finds the knearest neighbors. Subsequently, it finds the weight matrix W,
which minimizes the reconstruction error for each data point ⃗ xi∈Rm. Lastly, it finds the coordinates Y
which minimize the reconstruction error using the optimum weights. In the following section, we describe
how to obtain the solution to each step of the LLE algorithm.
2.1.1 Finding the local linear neighborhoods
A relevant aspect of the LLE is that this algorithm is capable of recovering embedding whose intrinsic
dimensionality dis smaller that the number of neighbors k. Moreover, the assumption of a linear patch
enforces the existence of an upper bound on k. For instance, in highly curved datasets, it is not reasonable
to have a large k, or that assumption would be then violated. In the uncommon situation where k >m, it
has been demonstrated that each sample may be perfectly reconstructed from its neighbors. However, an
additional problem then arises, namely the reconstruction weights are not unique anymore. To overcome
such a limitation, some regularization is required in order to break the degeneracy (Saul and Roweis, 2003).
2.1.2 Least-squares estimation of weights
The second step of the LLE algorithm consists of reconstructing each data point from its nearest neighbors.
The optimal reconstruction weights may be computed in closed form. Without loss of generality, the local
reconstruction error at point ⃗ ximay be expressed as follows:
2Under review as submission to TMLR
E(⃗ w) =/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
jwj(⃗ xi−⃗ xj)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(2)
=/summationdisplay
j/summationdisplay
kwjwk(⃗ xi−⃗ xj)T(⃗ xi−⃗ xk)
Defining the local covariance matrix Cas:
Cjk= (⃗ xi−⃗ xj)T(⃗ xi−⃗ xk) (3)
we find the following expression for the local reconstruction error:
E(⃗ w) =/summationdisplay
j/summationdisplay
kwjCjkwk=⃗ wTC⃗ w (4)
Regarding the constraint/summationtext
jwj= 1, it may be understood in two different ways: geometrical and probabilis-
tically. From a geometric point of view, it provides invariance under translation, adding a constant vector
⃗ cto⃗ xiand all of its neighbors while the reconstruction error remains unchanged. In terms of probability,
enforcing the weights to sum to one results in Wbecoming a stochastic transition matrix (Saul and Roweis,
2003). The estimation of the matrix Wreduces toneigenvalue problems. As there are no constraints across
the rows of W, we may separately find the optimal weights for each sample ⃗ xi, drastically simplifying the
computations. Thus, there are nindependent constrained optimization problems expressed as follows:
arg min
⃗ wi⃗ wT
iCi⃗ wis.t.⃗1T⃗ wi= 1 (5)
fori= 1,2,...,n. Using Lagrange multipliers, we write the Lagrangian function as follows:
L(⃗ wi,λ) =⃗ wT
iCi⃗ wi−λ(⃗1T⃗ wi−1) (6)
Taking the derivatives with relation to ⃗ wi:
∂
∂⃗ wiL(⃗ wi,λ) = 2Ci⃗ wi−λ⃗1 = 0 (7)
which leads to
Ci⃗ wi=λ
2⃗1 (8)
This is equivalent to solving the following linear system:
Ci⃗ wi=⃗1 (9)
and then normalizing the solution to guarantee that/summationtext
jwi(j) = 1by dividing each coefficient of the vector
⃗ wiby the sum of all the coefficients:
wi(j) =wi(j)/summationdisplay
jwi(j)forj= 1,2,...,m (10)
3Under review as submission to TMLR
If the number of neighbors kis greater than the number of features mthen, in general, the space spanned
bykdistinct vectors consists of the whole space. It means that ⃗ ximay be written precisely as a linear
combination of its k-nearest neighbors. In fact, if k>m, then there are generally infinitely many solutions to
⃗ xi=/summationtext
jwj⃗ xj, because there would be more unknowns kthan equations m. In such a case, the optimization
problemwouldbeill-posed, andregularizationshouldberequired. Inallexperimentsdescribedinthepresent
paper, we set the regularization parameter as α= 0.001.
2.1.3 Finding the coordinates
If the local neighborhoods are small enough compared to the curvature of the manifold, the optimal re-
construction weights in the embedding space as well as the weights reconstruction on the manifold are
approximately the same. In fact, the two sets of weights are identical for linear subspaces, and for general
manifolds they may be brought arbitrarily close to each other by shrinking the neighborhood. The key
idea behind the third step of the LLE algorithm is to use the optimal reconstruction weights estimated by
least-squares as the proper weights on the manifold and then solve for the local manifold coordinates. Thus,
fixing the weight matrix W, the objective is to solve the following quadratic minimization problem:
Φ(Y) =n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble⃗ yi−/summationdisplay
jwij⃗ yj/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(11)
In other words, we need to address the question about which coordinates ⃗ yi∈Rd(approximately on the
manifold) are reconstructed by such weights W. In order to avoid degeneracy, we have to impose the two
following constraints:
1. The mean of the data in the transformed space is zero, otherwise we would have an infinite number
of solutions;
2. The covariance matrix of the transformed data is the identity matrix (i.e. there is no correlation
between the components of ⃗ y∈Rd). This is a statistical constraint to assess that the output space
is Euclidean, defined by an orthogonal basis.
However, differently from the estimation of the weights W, finding the coordinates does not simplify into
nindependent problems, because each row of Yappears in Φmultiple times, once as the central vector yi
and also as one of the neighbors of other vectors. Thus, firstly, we rewrite equation equation 11 in a more
meaningful manner using matrices:
Φ(Y) =n/summationdisplay
i=1
⃗ yi−/summationdisplay
jwij⃗ yj
T
⃗ yi−/summationdisplay
jwij⃗ yj
 (12)
Applying the distributive law and expanding the summation, we then have:
Φ(Y) =n/summationdisplay
i=1⃗ yT
i⃗ yi−n/summationdisplay
i=1/summationdisplay
j⃗ yT
iwij⃗ yj
−n/summationdisplay
i=1/summationdisplay
j⃗ yT
jwji⃗ yi+n/summationdisplay
i=1/summationdisplay
j/summationdisplay
k⃗ yT
jwjiwik⃗ yk (13)
Denoting by Ythed×nmatrix in which each column ⃗ yifori= 1,...,nstores the coordinates of the i-th
sample in the manifold and knowing that ⃗ wi(j) = 0unless⃗ yjis one of the neighbors of ⃗ yi, we may write
Φ(Y)as follows:
4Under review as submission to TMLR
Φ(Y) =Tr(YT(I−W)T(I−W)Y) (14)
By defining the n×nmatrixMas follows:
M= (I−W)T(I−W) (15)
we get the following optimization problem:
arg min
YTr(YTMY)subject to1
nYTY=I (16)
Therefore, the Lagrangian function is given by:
L(Y,λ) =Tr(YTMY)−λ/parenleftbigg1
nYTY−I/parenrightbigg
(17)
Lastly, differentiating and setting the result to zero leads to:
MY =βY (18)
whereβ=λ
n, showing that the Ymust be composed by the eigenvectors of the matrix M. As we have
a minimization problem, we need to select the deigenvectors associated to the dsmallest eigenvalues to
composeY. Notice that Mconsisting of an n×nmatrix, it contains neigenvalues and northogonal
eigenvectors. Although the eigenvalues are real and non-negative, the smallest of them is always zero, with
theconstanteigenvector ⃗1. Thisbottomeigenvectorcorrespondstothemeanof Yandshouldbediscardedto
enforce the constraint that/summationtextn
i=1⃗ yi= 0(de Ridder and Duin, 2002). Therefore, to get ⃗ yi∈Rd, whered<m,
we must select the d+ 1smallest eigenvectors and discard the constant eigenvector with zero eigenvalue.
Specifically, we must select the deigenvectors associated to the bottom non-zero eigenvalues.
3 Parametric entropic LLE (PELLE)
The main motivation of the proposed parametric entropic LLE (PELLE) method is to find a surrogate for
the local matrix Cifor each sample of the dataset. Our method is a generalization of the regular LLE that
deals with local Gaussian densities estimated along the patches of the KNN graph. It is worth noticing that,
originally,Ci(j,k)is computed as the inner product between ⃗ xi−⃗ xjand⃗ xi−⃗ xk, meaning that the we employ
the Euclidean geometry in the estimation of the optimal reconstruction weights. In the proposed method, a
non-linear distance function is adopted in the definition of such a matrix, being the relative entropy between
Gaussian densities estimated within different patches of the KNN graph. Our inspiration is the parametric
PCAmethod, aninformation-theoreticextensionofthePCAmethodthatusestheKL-divergencetocompute
a surrogate for the covariance matrix - i.e. entropic covariance matrix (Levada, 2020).
LetX={⃗ x1,⃗ x2,...,⃗ xn}, with⃗ xi∈Rm, be our data matrix. The first step of the proposed method consists
of building the KNN graph from X. At this early stage, we employ the extrinsic Euclidean distance to
compute the nearest neighbors of each sample ⃗ xi. Denoting by ηithe neighborhood system of ⃗ xi, a patch
Piis defined as the set {⃗ xi∪ηi}. Notice that the number of elements of PiisK+ 1, fori= 1,2,...,n, being
a patchPigiven by the following m×(k+ 1)matrix:
5Under review as submission to TMLR
Pi=
xi(1)xi1(1)... xik(1)
xi(2)xi1(2)... xik(2)
............
............
xi(m)xi1(m)... xik(m)
(19)
Theideabehindtheproposedmethodistoconsidereachcolumnofthematrix Piasasampleofamultivariate
Gaussian random variable of size k+ 1. Then, we compute the maximum likelihood estimators of the model
parameters ⃗ µi(mean) and Σi(covariance matrix) as follows:
⃗ µi=1
k+ 1k+1/summationdisplay
j=1⃗ xij (20)
Σi=1
kk+1/summationdisplay
j=1(⃗ xij−⃗ µi)(⃗ xij−⃗ µi)T(21)
Letp(x)andq(x)be multivariate Gaussian densities, N(⃗ µ1,Σ1)andN(⃗ µ2,Σ2). Then, the relative entropy
DKL(p∥q)becomes:
DKL(p∥q) =1
2/bracketleftbigg
log/parenleftbigg|Σ2|
|Σ1|/parenrightbigg
+Tr/bracketleftbig
Σ−1
2Σ1/bracketrightbig
+ (⃗ µ2−⃗ µ1)TΣ−1
2(⃗ µ2−⃗ µ1)−m/bracketrightbigg
(22)
As the relative entropy is not symmetric, it is then possible to compute its symmetrized counterpart as
follows:
Dsym
KL(p∥q) =1
2[DKL(p,q) +DKL(q,p)] (23)
which contains the following closed-form expression:
Dsym
KL(p∥q) =1
2/bracketleftbigg1
2Tr/parenleftbig
Σ−1
1Σ2+ Σ−1
2Σ1/parenrightbig
(24)
+1
2(⃗ µ1−⃗ µ2)TΣ−1
1(⃗ µ1−⃗ µ2)
+1
2(⃗ µ2−⃗ µ1)TΣ−1
2(⃗ µ2−⃗ µ1)−m/bracketrightbigg
In the proposed method, our goal is to approximate the multivariate normal density of the patch Pias:
pi=/summationdisplay
jwijpj (25)
wherepj∈N(pi)denotes the multivariate normal densities from neighboring patches. Hence, we have to
minimize the following quadratic error:
E(⃗ wi) =
pi−/summationdisplay
jwijpj
2
=
/summationdisplay
jwij(pi−pj)
2
(26)
6Under review as submission to TMLR
since the summation of the weights must be equal to one.
Defining the difference between two multivariate normal densities piandpjas the symmetrized relative
entropy, that is, pi−pj=dij=DKL(pi∥pj)we have:
E(⃗ wi) =/summationdisplay
j/summationdisplay
kwijdijdikwik (27)
We compute the entropic matrix Cias follows:
Ci(j,k) =dijdik=DKL(pi∥pj)DKL(pi∥pk) (28)
leading to the quadratic form:
E(⃗ wi) =⃗ wiCi⃗ wi (29)
Thus, we have the same optimization problem of regular LLE. To accelerate the computation of the proposed
algorithm, webuildavectorofrelativeentropiesbetween piandalltheneighboringpatches pj, denotedby ⃗di.
The matrix Ciis computed by the outer product of ⃗diwith itself. It is worth noticing that distinctively from
the standard LLE method, which employs the pairwise Euclidean distance, the proposed PELLE method
employs a patch-based distance (i.e. relative entropy), becoming less sensitive to the presence of noise and
outliers in the observed data. In the following section, we present computational experiments comparing the
performance of the PELLE method against several popular manifold learning algorithms.
4 Fisher information and relative entropy
Letp(X;⃗θ)be a probability density function, where ⃗θ= (θ1,...,θk)∈Θis the vector of parameters. The
Fisher information matrix is the natural Riemannian metric of the parametric space (Amari, 1985; 2000;
Arwini and Dodson, 2008), being defined for i,j= 1,...,kas follows:
I(⃗θ)ij=E/bracketleftbigg/parenleftbigg∂
∂θilog p (X;⃗θ)/parenrightbigg/parenleftbigg∂
∂θjlog p (X;⃗θ)/parenrightbigg/bracketrightbigg
(30)
The Fisher information matrix is the metric tensor that equips the underlying parametric space. It consists
of the mathematical structure that defines the inner products in the local tangent spaces. The metric tensor
enables the expression of the square of an infinitesimal displacement in the manifold ds2as a function of an
infinitesimal displacement in the tangent space (Nielsen, 2020). In the case of a 2D manifold, it is given by
a vector [du,dv ]. Assuming a matrix notation we have:
ds2=/bracketleftbig
du dv/bracketrightbig/bracketleftbiggA B
B C/bracketrightbigg/bracketleftbiggdu
dv/bracketrightbigg
=Adu2+ 2Bdudv +Cdv2(31)
where the matrix of coefficients A,B, eCis the metric tensor. If the metric tensor is a positive definite
matrix, the manifold is is known as Riemannian. It is worth noticing that in the Euclidean case, the metric
tensor refers to the identity matrix - i.e., the space is flat, and we have the well-known Pythagorean relation
ds2=du2+dv2.
A relevant connection between the Fisher information and relative entropy is that the for nearby densities
p(x;⃗θ)andp(x;⃗θ+ ∆⃗θ), the KL-divergence becomes the Fisher information. Thus, it may be applied
to approximate geodesic distances. The KL-divergence between two infinitesimally close densities may be
expressed by a quadratic form, which coefficients are given by the elements of the Fisher information matrix.
First, we recall that the symmetric KL-divergence between p(x)andq(x)is given by:
7Under review as submission to TMLR
DKL(p∥q) =/integraldisplay
(p(x)−q(x))log/parenleftbiggp(x)
q(x)/parenrightbigg
dx (32)
Assuming we have a family of distributions parametrized by ⃗θ= (θ1,...,θk)and⃗θ′= (θ1+ ∆θ1,θ2+
∆θ2,...,θk+ ∆θk)where ∆θiis an infinitesimal displacement, then the symmetrized KL divergence is given
by:
DKL/parenleftig
(p(x;⃗θ)∥p(x;⃗θ′)/parenrightig
=
=1
2/integraldisplay/bracketleftig
p(x;⃗θ)−p(x;⃗θ′)/bracketrightig
log/parenleftigg
p(x;⃗θ)
p(x;⃗θ′)/parenrightigg
dx (33)
By definition, let the variation ∆p(x;⃗θ)be:
∆p(x;⃗θ) =p(x;⃗θ)−p(x;⃗θ′) (34)
which leads to:
DKL/parenleftig
(p(x;⃗θ)∥p(x;⃗θ′)/parenrightig
= (35)
=1
2/integraldisplay∆p(x;⃗θ)
p(x;⃗θ)log/parenleftigg
p(x;⃗θ)
p(x;⃗θ′)/parenrightigg
p(x;⃗θ)dx (36)
It is worth mentioning that the argument of the logarithm may be expressed as follows:
p(x;⃗θ)
p(x;⃗θ′)=p(x;⃗θ)
p(x;⃗θ)−∆p(x;⃗θ)= (37)
=p(x;⃗θ) + ∆p(x;⃗θ)
p(x;⃗θ)= 1 +∆p(x;⃗θ)
p(x;⃗θ)
Applying a Taylor approximation for the logarithm, for small values of xwe have:
log(1 +x)≈x (38)
Thus, we may write the following approximation:
log/parenleftigg
1 +∆p(x;⃗θ)
p(x;⃗θ)/parenrightigg
≈∆p(x;⃗θ)
p(x;⃗θ)(39)
which leads to:
DKL/parenleftig
(p(x;⃗θ)∥p(x;⃗θ′)/parenrightig
= (40)
=1
2/integraldisplay/parenleftigg
∆p(x;⃗θ)
p(x;⃗θ)/parenrightigg2
p(x;⃗θ)dx
8Under review as submission to TMLR
Considering that ∆p(x;⃗θ)is the arc length in the parametric space (manifold), we may express it in the
tangent space as the following dot product:
∆p(x;⃗θ)≈k/summationdisplay
i=1/bracketleftbigg∂
∂θip(x;⃗θ)∆θi/bracketrightbigg
(41)
The above equation is the dot product between the gradient (local tangent coordinates), being ∆⃗θthe
displacement vector. Hence, we have:
∆p(x;⃗θ)
p(x;⃗θ)≈1
p(x;⃗θ)k/summationdisplay
i=1∂
∂θip(x;⃗θ)∆θi
=k/summationdisplay
i=1∂
∂θilog p (x;⃗θ)∆θi (42)
Then, we may express equation equation 40 as:
DKL/parenleftig
(p(x;⃗θ)∥p(x;⃗θ′)/parenrightig
=1
2k/summationdisplay
i=1k/summationdisplay
j=1gij∆θi∆θj (43)
where
gij=/integraldisplay/parenleftbigg∂
∂θilog p (x;⃗θ)/parenrightbigg/parenleftbigg∂
∂θjlog p (x;⃗θ)/parenrightbigg
p(x;⃗θ)dx
=E/bracketleftbigg/parenleftbigg∂
∂θilog p (x;⃗θ)/parenrightbigg/parenleftbigg∂
∂θjlog p (x;⃗θ)/parenrightbigg/bracketrightbigg
=I(⃗θ)ij (44)
In matrix vector notation, we have:
DKL/parenleftig
(p(x;⃗θ)∥p(x;⃗θ′)/parenrightig
=1
2∆⃗θTI(⃗θ)∆⃗θ (45)
whereI(⃗θ)is the Fisher information matrix.
Therefore, adopting the symmetrized relative entropy as the similarity measure in PELLE means that we are
approximating the geodesic distances between the multivariate Gaussian densities from neighboring patches
along the KNN graph. As the variation between neighboring patches is smooth, the densities are similar.
5 Results
To test and evaluate the proposed method, we performed a set of experiments to compare the average clas-
sification accuracy obtained by four different supervised classifiers (KNN, decision trees, Bayesian classifier
under Gaussian hypothesis, and random forest classifiers), after dimensionality reduction to 2D spaces. We
then directly compare the proposed PELLE method against the PCA (Jolliffe, 2002), ISOMAP (Tenenbaum
et al., 2000), LLE (Roweis and Saul, 2000), Hessian eigenmaps or HLLE (Donoho and Grimes, 2003), local
tangent space alignment or LTSA (Zhang and Zha, 2004), and UMAP (McInnes et al., 2018) - which is
considered as the state-of-the-art approach for manifold learning when the number of samples is large.
Subsequently to performing the dimensionality reduction-based metric learning for each dataset, we use
50% of the samples to train the supervised classifiers. Then, each one of them is used to classify the 50%
9Under review as submission to TMLR
Table1: AverageclassificationaccuraciesproducedbythePCA,ISOMAP,LLE,HessianLLE,LTSA,UMAP,
and PELLE method considering 25 data sets (2D case)
Data set PCA LLE ISOMAP HLLE LTSA UMAP PELLE
Bolts 0.75 0.65 0.862 0.775 0.775 0.687 0.9
Parity5 0.453 0.375 0.391 0.343 0.344 0.390 0.593
Tic-tac-toe 0.621 0.593 0.622 0.647 0.652 0.770 0.908
Hayes-roth 0.575 0.640 0.666 0.723 0.723 0.594 0.731
Prnn_crabs 0.605 0.655 0.600 0.640 0.637 0.747 0.867
AIDS 0.38 0.29 0.36 0.340 0.350 0.38 0.500
Corral 0.831 0.762 0.846 0.846 0.859 0.812 0.918
Analcatdata_wildcat 0.78 0.746 0.756 0.719 0.720 0.698 0.799
Monks-problem-1 0.584 0.643 0.583 0.581 0.598 0.655 0.733
Vineyard 0.75 0.74 0.769 0.750 0.788 0.769 0.846
Plasma_retinol 0.534 0.553 0.541 0.533 0.534 0.530 0.603
Visualizing_enviromental 0.642 0.634 0.651 0.602 0.602 0.643 0.71
Wine 0.960 0.727 0.952 0.848 0.848 0.918 0.983
Mu284 0.929 0.85 0.936 0.556 0.568 0.907 0.966
Tae 0.434 0.473 0.519 0.477 0.470 0.483 0.539
Ar1 0.959 0.95 0.942 0.741 0.737 0.918 0.967
Sa-heart 0.651 0.658 0.683 0.634 0.633 0.663 0.712
Kidney 0.598 0.644 0.684 0.703 0.704 0.657 0.704
Haberman 0.736 0.692 0.745 0.722 0.723 0.707 0.753
Lupus 0.75 0.755 0.784 0.818 0.818 0.761 0.807
Acute-inflammations 0.933 0.8 0.954 0.783 0.770 0.945 0.991
Chscase_geyser1 0.871 0.752 0.878 0.754 0.756 0.853 0.896
Breast-tissue 0.410 0.452 0.424 0.495 0.518 0.518 0.556
Conference_attendence 0.835 0.839 0.839 0.847 0.841 0.825 0.862
Thoracic_surgery 0.798 0.807 0.804 0.750 0.694 0.803 0.828
Average 0.695 0.667 0.712 0.665 0.666 0.705 0.787
Median 0.736 0.658 0.745 0.719 0.704 0.707 0.807
Minimum 0.380 0.290 0.360 0.340 0.344 0.380 0.500
Maximum 0.96 0.95 0.954 0.848 0.859 0.945 0.991
remaining samples from the test data set, being the average accuracy computed to evaluate the behavior
of the dimensionality reduction in classification tasks. The results are reported in Table 1. In 24 out of
25 datasets, the proposed PELLE method obtains the highest classification accuracy, representing 96% of
the cases. Despite being one of the best methods described in the literature, UMAP has a limitation: as
it requires numerical optimization, it demands a sufficiently large number of samples for convergence to
good results, making it suffer in datasets with limited number of samples. In this context, the proposed
PELLEalgorithmcanbeviewedasaninterestingalternativefornon-linearfeatureextractioninclassification
problems with a limited number of samples (small sample size problems).
To test whether the classification accuracy obtained through the proposed PELLE method are statistically
superior to those obtained through the existing methods, we perform a non-parametric Friedman test (Fried-
man, 1937; Marozzi, 2014). Considering a significance level α= 0.01, there is evidence to reject the null
hypothesis that all groups are identical ( p= 1.36×10−11). In order to check which groups are significantly
different, we then performed a Nemenyi post-hoc test (Hollander et al., 2015). This test indicates that the
PELLE method provides significantly higher classification accuracy levels compared to the PCA ( p<10−3),
ISOMAP (p= 0.002), LLE (p<10−3), Hessian LLE ( p<10−3), LTSA (p<10−3), and UMAP ( p<10−3).
Despite the superior results, especially in small sample size classification problems, it is worth mentioning
that the proposed method has some caveats. A negative aspect of manifold learning algorithms, including
10Under review as submission to TMLR
Figure 1: Upper: Data visualization comparing clusters obtained through the LLE. Bottom: PELLE
method (K = 14) for the corral data set.
the proposed method, consists of the out-of-sample problem. Most unsupervised metric learning algorithms
are not capable of dealing with new samples that are not part of the training data set in a straightforward
manner. Another caveat of the proposed method concerns the definition of the parameter K(i.e. number of
neighbors) that controls the patch size. Our experiments reveal that the classification accuracy is sensitive
to changes in such a parameter.
In summary, in the present study we employ the following strategy: for each data set, we build the KKN
graphs for all values of K in the interval [2,40]. We then select the best model as the one that maximizes
the classification accuracy among all values of K. It is worth mentioning that we are using class labels to
perform model selection, however the dimensionality reduction-based metric learning is fully unsupervised.
A visual comparison of the clusters obtained through the LLE and PELLE method for the corral data set
is depicted in Figure 1. It is worth noticing that the discrimination between the two classes is more evident
through the results produced by the proposed PELLE method compared to the regular LLE, as there is less
overlap between the blue and red clusters. An advantage of the proposed method is its capacity to learn
good metrics even from small samples, while UMAP commonly requires more data to provide reasonably
good results.
11Under review as submission to TMLR
6 Conclusion
Unsupervised metric learning and manifold learning extract non-linear features from data, avoiding the
Euclidean distance. Several manifold learning algorithms have been proposed in the pattern recognition
literature, being the LLE method among the pioneering ones. Many LLE extensions have been devised to
overcome limitations of the original method, such as the Hessian LLE (also known as Hessian eigenmaps),
modified LLE, and LTSA. However, one yet unsolved problem refers to the fact that most LLE extensions
use the Euclidean metric to measure the similarity between samples.
In the present paper, a parametric entropic LLE (PELLE) method is proposed to incorporate the relative
entropy between local Gaussian distributions into the KNN adjacency graph. The rationale is to replace the
pointwsie Euclidean distance by a patch-based information-theoretic distance to increase the robustness of
the method against the presence of noise and outliers in the data. Our claim is that the proposed PELLE
method is a promising alternative to the existing manifold learning algorithms, especially in small sample
size classification problems. Our computational experiments support two main points. Firstly, the quality of
the clusters produced by the PELLE method may be superior to those obtained by state-of-the-art manifold
learning algorithms. Secondly, the proposed method features may be more discriminative in supervised
classification than features obtained by state-of-the-art manifold learning algorithms.
Future works might explore the use of additional information-theoretic distances, such as the Bhattacharyya
distance, Hellingerdistance, andgeodesicdistancesbasedintheFisherinformationmatrix. Anotherpossibil-
ity is the non-parametric estimation of the local densities using kernel density estimation (KDE) techniques.
In such a case, non-parametric versions of the information-theoretic distances might be employed to compute
a distance function between the patches of the KNN graph. The ϵ-neighborhood rule might also be used
to build adjacency relations that define the discrete approximation for the manifold, leading to non-regular
graphs. Furthermore, a supervised parametric entropic LLE might be created by removing the edges of the
KNN graph for which the endpoints belong to different classes, enforcing the optimal reconstruction weights
to use only the neighbors that belong to the same sample class.
References
Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,
290:2323–2326, 2000.
Justin Wang, Raymond K.W. Wong, and Thomas C.M. Lee. Locally linear embedding with additive noise.
Pattern Recognition Letters , 123:47–52, 2019.
David L. Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-
dimensional data. Proceedings of the National Academy of Sciences , 100(10):5591–5596, 2003.
Jianzhong Wang. Hessian locally linear embedding. In Geometric Structure of High-Dimensional Data and
Dimensionality Reduction , pages 249–265. Springer, New York, 2012.
Xianglei Xing, Sidan Du, and Kejun Wang. Robust hessian locally linear embedding techniques for high-
dimensional data. Algorithms , 9:36, 2016.
Z. Y. Zhang and H. Y. Zha. Principal manifolds and nonlinear dimensionality reduction via tangent space
aligment. SIAM Journal on Scientific Computing , 26(1):313–338, 2004.
Zhenyue Zhang and Jing Wang. MLLE: modified locally linear embedding using multiple weights. In
Bernhard Schölkopf, John C. Platt, and Thomas Hofmann, editors, Advances in Neural Information
Processing Systems 19, Proc. of the 20th Conf. on Neural Information Processing Systems , pages 1593–
1600, 2006.
Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290:2319–2323, 2000.
Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection
for dimension reduction, 2018. URL http://arxiv.org/abs/1802.03426 .
12Under review as submission to TMLR
Lawrence Saul and Sam Roweis. Think globally, fit locally: Unsupervised learning of low dimensional
manifolds. Journal of Machine Learning Research , 4:119–155, 2003.
Dick de Ridder and Robert P.W. Duin. Locally linear embedding for classification. Technical report, Delft
University of Technology, 2002.
Alexandre L. M. Levada. Parametric PCA for unsupervised metric learning. Pattern Recognition Letters ,
135:425–430, 2020.
S. Amari. Differential-geometrical methods in statistics (Lecture notes in statistics) . Springer-Verlag, New
York, 1985.
S. Amari. Methods of information geometry (Translations of mathematical monographs v. 191) . American
Mathematical Society, New York, 2000.
K. A. Arwini and C. T. J. Dodson. Information Geometry: Near Randomness and Near Independence .
Springer, New York, 2008.
Frank Nielsen. An elementary introduction to information geometry. Entropy, 22(10), 2020.
I. T. Jolliffe. Principal Component Analysis . Springer, New York, 2 edition, 2002.
Milton Friedman. The use of ranks to avoid the assumption of normality implicit in the analysis of variance.
Journal of the American Statistical Association , 32(200):675–701, 1937.
Marco Marozzi. Testing for concordance between several criteria. Journal of Statistical Computation and
Simulation , 84(9):1843–1850, 2014.
Myles Hollander, Douglas A. Wolfe, and Eric Chicken. Nonparametric Statistical Methods . Wiley, New York,
3 ed. edition, 2015.
13