Under review as submission to TMLR
Minimax Lower Bounds for Estimating Distributions on Low-
dimensional Spaces
Anonymous authors
Paper under double-blind review
Abstract
Recent statistical analyses of Generative Adversarial Networks (GAN) suggest that the error
in estimating the target distribution in terms of the β-Hölder Integral Probability Metric
(IPM) scales asO/parenleftbigg
n−β
dM+δ∨n−1/2logn/parenrightbigg
. HeredMis the upper Minkowski dimension of
the corresponding support Mof the data distribution and δis a positive constant. It is,
however, unknown as to whether this rate is minimax optimal, i.e. whether there are estima-
tors that achieve a better test-error rate. In this paper, we show that the minimax rate for
estimating unknown distributions in the β-Hölder IPM on Mscales as Ω/parenleftbigg
n−β
dM−δ∨n−1/2/parenrightbigg
,
wheredMis the lower Minkowski dimension of M. Thus if the low-dimensional structure
Mis regular in the Minkowski sense, i.e. dM=dM, GANs are roughly minimax optimal
in estimating distributions on M. We also show that the minimax estimation rate in the
p-Wasserstein metric scales as Ω/parenleftig
n−1
dM−δ∨n−1/(2p)/parenrightig
.
Nonparametric density estimation, aimed at approximating a probability distribution from a finite collection
of identically and independently distributed (i.i.d.) samples, holds extensive application in the realms of
statistics and machine learning. Nonparametric density estimation finds application in various fields such
as mode estimation (Parzen, 1962), nonparametric classification (Rigollet, 2007; Chaudhuri et al., 2008),
Monte Carlo computational methods (Doucet et al., 2001), and clustering (Chaudhuri & Dasgupta, 2010;
Chakraborty et al., 2021; Rinaldo & Wasserman, 2010), among others. Typical techniques for nonparametric
density estimation encompass the histogram method, kernel method, k-nearest Neighbor (kNN) method
(Devroye & Wagner, 1977; Bhattacharya & Mack, 1987; Zhao & Lai, 2022), wavelet-based methods (Donoho
et al., 1996) and more. Notably, the recent advancements in deep learning have led to the groundbreaking
concept of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which has revolutionised the
field of nonparametric density estimation to obtain superhuman performance, especially for handling vision
data.
The empirical successesof GANs have motivated researchers tostudy theirtheoretical guarantees. Biau et al.
(2020) analyzed the asymptotic properties of vanilla GANs along with parametric rates. Biau et al. (2021)
also analyzed the asymptotic properties of WGANs. Liang (2021) explored the min-max rates for WGANs
for different non-parametric density classes and under a sampling scheme from a kernel density estimate of
the data distribution; while Schreuder et al. (2021) studied the finite-sample rates under adversarial noise.
Uppal et al. (2019) derived the convergence rates for Besov discriminator classes for WGANs. Luise et al.
(2020) conducted a theoretical analysis of WGANs under an optimal transport-based paradigm. Recently,
Asatryan et al. (2023) and Belomestny et al. (2021) improved upon the works of Biau et al. (2020) to
understand the behaviour of GANs for Hölder class of density functions. Arora et al. (2017) showed that
generalisation might not hold in standard metrics. However, they show that under a restricted “neural-net
distance”, the GAN is indeed guaranteed to generalize well. Recently, Arora et al. (2018) showed that GANs
and their variants might not be well-equipped against mode collapse.
Although significant progress has been made in our theoretical understanding of GAN, some limitations of
the existing results are yet to be addressed. For instance, the generalisation bounds frequently suffer from
the curse of dimensionality. In practical applications, data distributions tend to have high dimensionality,
1Under review as submission to TMLR
making the convergence rates that have been proven exceedingly slow. However, high-dimensional data,
such as images, texts, and natural languages, often possess latent low-dimensional structures that reduce
the complexity of the problem. For example, it is hypothesised that natural images lie on a low-dimensional
structure, in spite of its high-dimensional pixel-wise representation (Pope et al., 2020). Though in classical
statistics there have been various approaches, especially using kernel tricks and Gaussian process regression
that achieve a fast rate of convergence that depends only on their low intrinsic dimensionality (Bickel &
Li, 2007; Kim et al., 2019), such results are largely unexplored in the context of GANs. Recently, Huang
et al. (2022) expressed the generalisation rates for GAN when the data has low-dimensional support in the
Minkowski sense and the latent space is one-dimensional; while Dahal et al. (2022) derived the convergence
rates under the Wasserstein-1 distance in terms of the manifold dimension. It is important to note that the
compact Riemannian manifold assumption of the support of the target distribution and the assumption of a
bounded density of the target distribution on this manifold by Dahal et al. (2022) is a very strong assumption
that might not hold in practice.
Despite these recent advances, it remains uncertain whether GAN estimates of the target distribution are
optimal in the minimax sense for estimating distributions that are supported on a low-dimensional structure.
In this paper, we address this gap in the current literature by providing a comprehensive analysis of the
minimax lower bound for estimating and demonstrate that when nindependent and identically distributed
samples are available from any target distribution on a set M, the convergence rate for any estimator is
at least Ω/parenleftbigg
n−β
dM−δ∨n−1/2/parenrightbigg
, whereδis a positive constant in the range of (0, dM), anddMis the lower
Minkowskidimensionoftheset M. Thus, whentheset MisMinkowskiregular, GANsalmostmatchthisrate,
when the networks are properly chosen. In particular, Huang et al. (2022) showed that when the generator
and discriminators are realized by a feed-forward neural network with ReLU activation, respective depths
Lgen,Ldisand widths Wgen,Wdis, then one can choose W2
genLgen≾nandWdisLdis≾nD/(2¯dM+2δ)log2n
(Ddenotes the dimension of the data space) to ensure that the error rate for the GAN estimate of the
distribution scales as O(n−β/(¯dM+δ)∨n−1/2logn). In this case, the discriminator network has to satisfy a
regularity condition in terms of its maximum Lipschitz constant. We refer the reader to Theorem 19 of
Huang et al. (2022) for more details. Additionally, we demonstrate that the minimax estimation rate in the
p-Wasserstein metric decreases in proportion to Ω/parenleftig
n−1
dM−δ∨n−1/(2p)/parenrightig
.
1 Background
1.1 Related Work
Recent research has explored the minimax rates under the Wasserstein distances under various settings.
Singh & Póczos (2018) demonstrated the minimax convergence rates assuming the distribution is compactly
supported. In a related context, Liang (2021) and Uppal et al. (2019) established minimax convergence rates
for the Wasserstein- 1distance under a smoothness assumption on the corresponding density. It has been
demonstrated that estimating under the Integral Probability Metric (IPM) with smooth functions can lead to
enhanced rates of convergence for empirical measures Kloeckner (2020). Niles-Weed & Berthet (2022) estab-
lished the minimax convergence rates for Besov densities for the Wasserstein- pmetric. For smooth densities,
the derived minimax rates can be improved (McDonald, 2017; Liang, 2021) in the sense that estimating a
smooth density is easier than estimating a non-smooth one. However, all the aforementioned findings pri-
marily consider the minimax rates when the corresponding distribution varies across all probability measures
on a compact set, resulting in rates of O(n−1/D)or similar for estimating distributions of RD. Recently,
Tang & Yang (2023) derived the minimax rates when the data is supported on a smooth sub-manifold with a
positively bounded reach, and has a smooth density w.r.t. the volume measure on this manifold. In contrast
to the work of Tang and Yang (2023), our analysis does not impose a smooth manifold structure on the
data support, allowing for highly non-smooth and irregular geometries. Moreover, we do not assume the
existence of a density for the distribution on this potentially irregular low-dimensional support, enabling
us to accommodate singular distributions on this structure within our framework. While both our work
and the one by Tang and Yang (2023) leverage the principles of minimax error bounds via Fano’s method,
their reliance on smooth manifolds and the existence of smooth densities simplifies the corresponding testing
2Under review as submission to TMLR
Table 1: A comparison of different upper and lower bounds for distribution estimation on the D-dimensional
unit hypercube ( D≥3). The upper bounds do not show possible poly-log factors in n.
Result Assumption on Distri-
bution SupportAssumption
on DensityResult Type Metric Bound
Liang (2021) None α-Hölder Upper
and Lower
Boundβ-Hölder IPM n−α+β
2α+D∨n−1/2
Niles-Weed
& Berthet
(2022)None s-Besov Upper
and Lower
Boundp-Wasserstein n−1+s/p
D+s∨n−1/2p
Huang et al.
(2022)Minkowski dimension
dNone Upper
Boundβ-Hölder IPM n−β/(d+δ)∨n−1/2
Tang&Yang
(2023)d-dimensional γ-
smooth (γ≥ 2)
sub-manifold with a
lower-bounded reachα-Hölder
and lower
boundedUpper
and Lower
Boundβ-Hölder IPM n−βγ
2α+d∨n−α+β
2α+d∨
n−1/2
This work Minkowski dimension dNone Lower Boundβ-Hölder IPM n−β/(d−δ)∨n−1/2
p-Wasserstein n−1/(d−δ)∨n−1/(2p)
problem for rate derivation. In contrast, our approach circumvents these assumptions by constructing point
measures on the low-dimensional structure, enabling a more general analysis under minimal assumptions.
Further, while Tang and Yang(2023) derive the minimax error bounds for β-Hölder IPMs, our analyses also
cover the Wasserstein p-distances. Table 1 compares the error rates found in some of the prominent works
in literature and puts our results in context.
In addition to the mathematical results (Huang et al., 2022) that the error rates for GANs depend on the
intrinsic data dimension, empirical results also indicate such phenomenon occurs in practice. For example,
the recent works by Chakraborty & Bartlett (2024a) show that the test error rates in the Wasserstein-1
distance for Wasserstein Autoencoders (Tolstikhin et al., 2018) depend only on the Minkowski dimension
of the data support. Similar results have also been established both theoretically and empirically for deep
regression (Nakada & Imaizumi, 2020) and federated learning models (Chakraborty & Bartlett, 2024b).
1.2 Preliminaries and Notations
Beforewegointothedetailsofthetheoreticalresults, weintroducesomenotationandrecallsomepreliminary
concepts.
We use the notation x∨y:= max{x,y}.Bϱ(x,r)denotes the open ball of radius raroundx, with respect
to (w.r.t.) the metric ϱ. For any measure γ, the support of γis defined as, supp (γ) ={x:γ(Bϱ(x,r))>
0,for allr>0}. For any function f:S→R, and any measure γonS, let∥f∥Lp(γ):=/parenleftbig/integraltext
S|f(x)|pdγ(x)/parenrightbig1/p,
if0< p <∞. Also let,∥f∥L∞(γ):= ess supx∈supp(γ)|f(x)|. We sayAn≾Bn(also written as An=
O(Bn)⇐⇒Bn= Ω(An)) if there exists C > 0, independent of n, such that An≤CBn. We sayAn≍Bn,
ifAn≾BnandBn≾An. For anyk∈N, we let [k] ={1,...,k}. 1(·)denotes the indicator function. For
any measure µ,µ⊗ndenotes the n-product measure of µ. We also recall some useful definitions as follows.
Definition 1 (Covering and Packing Numbers) .For a metric space (S,ϱ), theϵ-covering number w.r.t. ϱ
is defined as:N(ϵ;S,ϱ) = inf{n∈N:∃x1,...xnsuch that∪n
i=1Bϱ(xi,ϵ)⊇S}.Similarly, the ϵ-packing
number is defined as: M(ϵ;S,ϱ) = sup{m∈N:∃x1,...xm∈Ssuch thatϱ(xi,xj)≥ϵ,for alli̸=j}.
Definition 2 (Hölder functions) .Letf:S→Rbe a function, where S⊆RD. For a multi-index s=
(s1,...,sD), let,∂sf=∂|s|f
∂xs1
1...∂xsD
D, denote the weak partial derivative of f, where,|s|=/summationtextD
ℓ=1sℓ. We say
3Under review as submission to TMLR
that a function f:S→Risβ-Hölder (for β >0) if
∥f∥Hβ:=/summationdisplay
s:0≤|s|≤⌊β⌋∥∂sf∥∞+/summationdisplay
s:|s|=⌊β⌋sup
x̸=y∥∂sf(x)−∂sf(y)∥
∥x−y∥β−⌊β⌋<∞.
Iff:RD→RD′, then we define∥f∥Hβ=/summationtextD′
j=1∥fj∥Hβ. For notational simplicity, let, Hβ(S1,S2,C) ={f:
S1→S2:∥f∥Hβ≤C}. Here, both S1andS2are both subsets of real vector spaces. If S1= [0,1]D,S2=R
andC= 1, we write Hβin stead of Hβ(S1,S2,C).
Next, we recall the definitions of Total Variation and Wasserstein- pdistances as well as Integral Probability
Metrics (IPMs).
Definition 3 (Total Variation Distance) .LetΩbe a Polish space and suppose that µandνare two
probability measures defined on Ω. Then, the total variation distance between µandνis defined as,
TV(µ,ν) = sup
B∈B(Ω)|µ(B)−ν(B)|= inf
γ∈Γ(µ,ν)P(X,Y)∼γ(X̸=Y). (1)
Here,B(Ω)denotes the Borel σ-algebra on ΩandΓ(µ,ν)denotes the set of all measure couples between µ
andν. The reader is referred to Proposition 4.7 of Levin & Peres (2017) for a proof of the second equality
in (1).
Definition 4 (Wasserstein p-distance).Let(Ω,dist)be a Polish space and let µandνbe two probability
measures on the same with finite p-moments. Then the p-Wasserstein distance between µandνis defined
as:
Wp(µ,ν) =/parenleftbigg
inf
γ∈Γ(µ,ν)E(X,Y)∼γ(dist(X,Y ))p/parenrightbigg1/p
.
In what follows, we take dist to be the ℓ2-norm on RD.
Definition 5 (Integral Probability Metric) .For a function class F, theF-Integral Probability Metric (IPM)
between two probabiloty measures µandνis defined as,
∥µ−ν∥F= sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
fdµ−/integraldisplay
fdν/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
1.3 Minkowski Dimension
Often, real data is hypothesized to lie on a lower-dimensional structure within the high-dimensional repre-
sentative feature space. To characterize this low-dimensionality of the data, researchers have defined various
notions of the effective dimension of the underlying measure from which the data is assumed to be generated.
Among these approaches, the most popular ones use some sort of rate of increase of the covering number,
in the log-scale, of most of the support of this data distribution. Let (S,ϱ)be a compact Polish space and
letµbe a probability measure defined on it. Throughout the remainder of the paper, we take ϱto be the
ℓ∞-norm. We characterize this low-dimensional nature of the data, through the Minkowski dimension of the
support ofµ. We recall the definition of Minkowski dimensions (Falconer, 2004),
Definition 6 (Minkowski dimension) .For a bounded metric space (S,ϱ), the upper Minkwoski dimension
ofSis defined as
dS= lim sup
ϵ↓0logN(ϵ;S, ϱ)
log(1/ϵ).
Similarly, the lower Minkowski dimension of Sis given by,
dS= lim inf
ϵ↓0logN(ϵ;S, ϱ)
log(1/ϵ).
IfdS=dS, we say that Sis Minkowski regular and a has Minkowski dimension of dS= limϵ↓0logN(ϵ;S,ϱ)
log(1/ϵ).
4Under review as submission to TMLR
The Minkowski dimension essentially measures how the covering number of Sis affected by the radius of
the covering balls. Since this notion of dimensionality depends only on the covering numbers and does not
assume the existence of a smooth correspondence to a smaller dimensional Euclidean space, this notion not
only incorporates smooth manifolds but also covers highly non-smooth sets such as fractals. In the literature,
Kolmogorov&Tikhomirov(1961)providedacomprehensivestudyonthedependenceofthecoveringnumber
of different function classes on the underlying Minkowski dimension of the support. Nakada & Imaizumi
(2020) showed how deep learners can incorporate this low-dimensionality of the data that is also reflected
in their convergence rates. Recently, Huang et al. (2022) showed that WGANs can also adapt to this low-
dimensionality of the data. In particular they showed that when the data is independent and identically
distributed from a distribution µ, for the GAN estimate for the density (denoted as ˆµGAN),∥µ−ˆµGAN∥Hβ
decays at a rate of O/parenleftbigg
n−β
dM+δ∨n−1/2logn/parenrightbigg
, where Mis the support of µandδ>0is a pre-fixed constant.
In the following section, we attempt to understand whether this rate is optimal or not.
2 Theoretical Analysis
Suppose that M⊆[0,1]Dand let ΠMdenote the set of all probability distributions on M. We assume that
one has access to nsamples,X1,...,Xn, generated independently from µ∈ΠM. The goal is to understand
how well any estimate of µ, based on the data, performs. We characterise this performance in terms of
theβ-Hölder IPM or the p-Wasserstein distance, i.e. for an estimate ˆµ, its performance is measured as
∥µ−ˆµ∥HβorWp(ˆµ,µ). To characterise this notion of best-performing estimator, researchers use the concept
of minimax risk i.e. the risk of the best-performing estimator that achieves the minimum risk with respect
to all members in ΠM. Formally, the minimax risk for the problem is given by,
Mn= inf
ˆµsup
µ∈ΠMEµ∥ˆµ−µ∥HβorMn= inf
ˆµsup
µ∈ΠMEµWp(ˆµ,µ),
where the infimum is taken over all measurable estimates of µ, i.e. on{ˆµ: (X1,...,Xn)→Π[0,1]D:
ˆµis measurable}. Here, we write Eµto denote that the expectation is taken with respect to the joint
distribution of X1,...,Xn, which are independently and identically distributed as µ. Theorem 7 states the
main lower bound of this paper, which lower bounds Mnin terms of the lower-Minkowski dimension of M
and the number of samples n, whennis large. The proof of this result is given in Section 3.
Theorem 7 (Main Result) .Suppose that X1,...,Xnare i.i.d.µand letδ∈(0,dM). Then there exists an
n0∈Nsuch that if n≥n0,
inf
ˆµsup
µ∈ΠMEµ∥ˆµ−µ∥Hβ≿n−1
dM−δ∨n−1/2, (2)
where the infimum is taken over all measurable estimates of µ, based on the data, X1,...,Xn. Furthermore,
inf
ˆµsup
µ∈ΠMEµWp(ˆµ,µ)≿n−1
dM−δ∨n−1
2p. (3)
IfMis Minkowski regular, from Theorem 7, we note that for any δ∈(0,dM), we observe that,
inf
ˆµsup
µ∈ΠMEµ∥ˆµ−µ∥Hβ≿n−β
dM−δ∨n−1/2.
From the results derived by Huang et al. (2022), GANs can achieve a rate of convergence of
O/parenleftig
n−β
dM+δ∨n−1/2logn/parenrightig
, implying that GANs are almost optimal in learning distributions when the data
is low-dimensional in the Minkowski sense, barring poly-log factors in the sample size.
Itisimportanttonotethatthelowerboundin(3)closelyresemblestheonesderivedbyNiles-Weed&Berthet
(2022) for distributions with Besov densities. Furthermore, from Theorem 1 of Weed & Bach (2019), we
note that the empirical distribution ˆµnscales as EWp(ˆµn,µ)≾n−1/(d∗
p(µ)+δ), whered∗
p(µ)denotes the p-
upper Wasserstein dimension of µ. Sinceµis supported on M, by Proposition 2 of Weed & Bach (2019),
d∗
p(µ)≤¯dM, whendM≥2p. Thus, EWp(ˆµn,µ)≾n−1/(¯dM+δ). Hence, when dM>2p, we can choose δ >0,
5Under review as submission to TMLR
such that, infˆµsupµ∈ΠMEµWp(ˆµ,µ)≿n−1/(dM−δ). Hence, in this case, the empirical distribution almost
achieves this minimax optimal rate when dM>2pandMis Minkowski regular.
Remark 1 (δ-term in Theorem 7) .We observe that the δ-term is an artefact of the definition of the
Minkowski dimension. If N(ϵ,M,ℓ∞)≿ϵ−d, for somed∈(0,D], i.e. when the limit in the computation of
the lower Minkowski dimension can be achieved exactly, following the proof of Theorem 7 (see Section 3),
we note that under the same assumptions Mn≿n−β/d∨n−1/2, fornlarge. Theδ-term in the lower bound
is only an artefact of the definition of the lower Minkowski dimension and can be removed by assuming the
lower bound for the covering number. Further, if one assumes that N(ϵ,M,ℓ∞)≾ϵ−¯d, then the analyses
by Huang et al. (2022) shows that the error rate for GANs for estimating a distribution µ, supported on M
scales as (n−β/¯d∨n−1/2) logn(see Theorem 19 of Huang et al. (2022)). Thus, when N(ϵ,M,ℓ∞)≍ϵ−d, the
error rates for GANs match the minimax rate except for an excess log-factor in the number of samples.
Inference for distributions supported on a Manifold When the support is regular, one can say that
the minimax rate for estimating distributions decays at a rate whose exponent is inversely proportional to
its regularity dimension. We recall that a set Misd-regular w.r.t. the d-dimensional Hausdorff measure Hd
if
Hd(Bϱ(x,r))≍rd,
for allx∈M(see Definition 6 of Weed & Bach (2019)). Recall that the d-Hausdorff measure of a set Sis
defined as,
Hd(S) := lim inf
ϵ↓0/braceleftigg∞/summationdisplay
k=1rd
k:S⊆∞/summationdisplay
k=1Bϱ(xk,rk),rk≤ϵ,∀k/bracerightigg
.
It is known (Mattila, 1999) that if Misd-regular, then dM=d. Thus, when Misd-regular, the minimax rate
roughly scales at Ω(n−β/d). Since, compact d-dimensional differentiable manifolds are d-regular (Weed &
Bach, 2019, Proposition 9), this implies that for when Mis a compact differentiable d-dimensional manifold,
the error rates scale as Ω(n−β/d). This result underscores that GANs are nearly minimax optimal, given
that the corresponding upper bounds derived by Dahal et al. (2022) match this minimax rate, albeit under
some additional assumptions regarding the smoothness of the manifold. A similar result holds when Mis
a nonempty, compact convex set spanned by an affine space of dimension d; the relative boundary of a
nonempty, compact convex set of dimension d+ 1; or a self-similar set with similarity dimension das all
these sets are d-regular by (Weed & Bach, 2019, Proposition 9).
3 Proof of the Main Result (Theorem 7)
As a first step for deriving a minimax bound, we first show that the Hölder IPM can be lower bounded by
the total variation distance and the minimum separation of the support of the distributions. For any finite
set, we use the notation, sep (Ξ) = inf ξ,ξ′∈Ξ:ξ̸=ξ′∥ξ−ξ′∥∞.
Lemma 8. LetΞbe a finite subset of Rdand let,P,Q∈ΠΞ. Then, we can find a constant π1(that might
depend onβ) such that,∥P−Q∥Hβ(RD,R,1)≥π1(sep(Ξ))β∥P−Q∥TV.
Proof.Letb(x) = exp/parenleftig
1
x2−1/parenrightig
1{|x|≤1}bethestandardbumpfunctionon R. Foranyx∈RDandδ∈(0,1],
we let,hδ(x) =aδβ/producttextd
j=1b(xj/δ). Hereais such that ab(x)∈Hβ(R,R,C). It is easy to observe that
hδ∈Hβ(RD,R,1). LetPandQbe two distributions on Ξ ={ξ1,...,ξk}. Letδ=1
3mini̸=j∥ξi−ξj∥∞. We
defineh⋆(x) =/summationtextk
i=1αihδ(x−ξi), withαi∈{− 1,+1}, to bechosen later. Sincethe individualterms in h⋆are
members of Hβ(RD,R,1)and have disjoint supports, h⋆∈Hβ(RD,R,1). We takeαi= 2 1(P(ξ)≥Q(ξ))−1.
Thus,
∥P−Q∥Hβ(RD,R,1)≥/integraldisplay
h∗dP−/integraldisplay
h∗dQ=k/summationdisplay
i=1aδβαi(P(ξi)−Q(ξi)) =aδβk/summationdisplay
i=1|P(ξi)−Q(ξi)|
=2aδβ∥P−Q∥TV
6Under review as submission to TMLR
=2a
3β(sep(Ξ))β∥P−Q∥TV.
Takingπ1=2a
3βgives us the desired result.
Similar to Lemma 8, we also show that on a discrete space, the p-Wasserstein metric is lower bounded by
the total variation distance.
Lemma 9. LetΞbe a finite subset of Rdand let,P,Q∈ΠΞ. Then, Wp(P,Q)≥sep(Ξ)∥P−Q∥1/p
TV.
Proof.LetX∼PandY∼Q. We note that∥X−Y∥2≥ 1{X̸=Y}sep(Ξ). Thus,
(E∥X−Y∥p
2)1/p≥(P(X̸=Y))1/psep(Ξ).
Taking infimum w.r.t. all measure couples between PandQgives us the desired result.
With the above two lemmas at our disposal, we are now ready to prove the main result of this paper. Recall
that ifP≪Q, the KL-divergence between PandQis given by, KL(P∥Q) =/integraltext
log(dP/dQ )dP. Similarly,
theχ2-divergence is given by, χ2(P∥Q) =/integraltext
(dP/dQ )2−1.
3.1 Proof of Theorem 7
With Lemmata 8 and 9, we are now ready to prove Theorem 7. We use Fano’s method to obtain the
minimax lower bound. We refer the reader to Chapter 15 of Wainwright (2019) for a detailed exposition. Let,
s=dM−δ. Thus, we can find ϵ0∈(0,1), such that if ϵ∈(0,ϵ0],N(ϵ,M,ℓ∞)≥ϵ−s=⇒ M (ϵ,M,ℓ∞)≥ϵ−s.
We taken≥n0= (128(ϵ0)−s)∨8192. Supposeϵ= (n/128)−1/s. Let Θ ={θ1,...,θk}be aϵ-separated set
inM. For the above choices of nandϵ, we observe that we can take, k=ϵ−s=n/128≥64andn≥64k.
Letϕj(x) = 1{x=θj}− 1{x=θ⌊k/2⌋+j}, for allj= 1,...,⌊k/2⌋. Let, ω∈{0,1}⌊k/2⌋. We define the
probability mass function on Θ,
Pω(x) =1
k+δk
k⌊k/2⌋/summationdisplay
j=1ωjϕj(x),
withδk∈(0,1/2]. By construction, Pω∈ΠM.
Furthermore,
∥Pω−Pω′∥TV=δk
k∥ω−ω′∥1.
By the Varshamov-Gilbert bound (Tsybakov, 2009, Lemma 2.9), let Ω⊆{0,1}⌊k/2⌋be such that|Ω|≥
21
8⌊k/2⌋and∥ω−ω′∥1≥1
8⌊k/2⌋, for all ω̸=ω′both in Ω. Thus for any ω̸=ω′, both in Ω,
∥Pω−Pω′∥TV≥δk⌊k/2⌋
8k. (4)
Hence, by Lemma 8, ∥Pω−Pω′∥Hβ(RD,R,1)≥π1ϵβδk⌊k/2⌋
k. Similarly, by Lemma 9, we note that
Wp(Pω,Pω′)≥ϵ/parenleftig
δk⌊k/2⌋
k/parenrightig1/p
. Furthermore, we observe that
KL(P⊗n
ω∥P⊗n
ω′) =nKL(Pω∥Pω′)≤nχ2(Pω∥Pω′) =nk/summationdisplay
i=1(Pω(ξi)−Pω′(ξi)2
Pω(ξi)≤2nkk/summationdisplay
i=1(Pω(ξi)−Pω′(ξi))2
≤2nk⌊k/2⌋(2δk/k)2
=8n⌊k/2⌋δ2
k
k.
7Under review as submission to TMLR
Thus,
1
|Ω|2/summationdisplay
ω,ω′∈ΩKL(P⊗n
ω∥P⊗n
ω′)≤8n⌊k/2⌋δ2
k
k.
LetP={Pω:ω∈Ω}. LetJ∼Unif(Ω) andZ|J=ω∼Pω. By the convexity of KL divergence (see
equation 15.34 of Wainwright (2019)), we know that,
I(Z;J)≤1
|Ω|2/summationdisplay
ω,ω′∈ΩKL(P⊗n
ω∥P⊗n
ω′)≤8n⌊k/2⌋δ2
k
k.
Thus,
I(Z;J) + log 2
log|Ω|≤8I(Z;J) + log 2
⌊k/2⌋log 2≤64nδ2
k
klog 2+8
⌊k/2⌋≤64nδ2
k
klog 2+1
4. (5)
The last inequality follows since k≥64. We takeδk=1
16/radicalig
klog 2
n. Clearly,ϵ≤1/2asn≥64k. This choice
ofϵmakes,
I(Z;J) + log 2
log|Ω|≤1
2.
Thus, by Theorem 15.12 of Wainwright (2019) (taking Φ(x) =xandρ(P,Q) =∥P−Q∥Hβ),
inf
ˆµsup
µ∈ΠMEµ∥ˆµ−µ∥Hβ≥π1ϵβδk⌊k/2⌋
k=π1ϵβ⌊k/2⌋
k1
16/radicalbigg
klog 2
n=π1ϵβ⌊k/2⌋
k1
16/radicalbigg
log 2
128
≥π2ϵβ≍n−β/s. (6)
Similarly, inf
ˆµsup
µ∈ΠMEµWp(ˆµ,µ)≥ϵ/parenleftbiggδk⌊k/2⌋
k/parenrightbigg1/p
≿n−1/s. (7)
To show that infˆµsupµ∈ΠM∥ˆµ−µ∥Hβ≿n−1/2, we use Le Cam’s method (Wainwright, 2019, Chapter 15.2).
Letθ0,θ1∈Mbe such that∥θ0−θ1∥∞≥diam(M)/2. LetP0(θ0) =P0(θ1) = 1/2andP1(θ0) = 1−P1(θ1) =
1/2−δwithδ∈(0,1/4). Clearly, TV(P0,P1) =δ.Thus, by Lemma 8, we observe that
∥P1−P0∥Hβ≿(diam( M)/2)βδ≿δ.
Similarly, Wp(P1,P0)≥(diam( M)/2)δ1/p≿δ1/p. Again,
KL(P⊗n
1∥P⊗n
0) =nKL(P1∥P0)≤nχ2(P1∥P0) = 4nδ2.
By Pinsker’s inequality (Tsybakov, 2009, Lemma 2.5), we note that,
TV(P⊗n
1,P⊗n
0)≤/radicalbigg
1
2KL(P⊗n
1∥P⊗n
0) = 2δ√n= 1/4,
ifδ=1
8√n. Thus from equation 15.14 of Wainwright (2019), we observe that,
inf
ˆµsup
µ∈ΠMEµ∥ˆµ−µ∥Hβ≿δ≍1/√n. (8)
Similarly, inf
ˆµsup
µ∈ΠMEµWp(ˆµ,µ)≿δ1/p≍n−1
2p. (9)
The result now follows from combining (6) and (8). The minimax rate for the Wasserstein distance follows
from combining (7) and (9).
8Under review as submission to TMLR
4 Conclusion
In this paper, we aimed to study the fundamental question of whether GANs are optimal in providing
accurate estimates of target distributions, especially when the data exhibit a low-dimensional structure.
We characterise this notion of low-dimensionality through the so-called Minkowski dimension. We have
demonstrated that, in scenarios where nindependent and identically distributed samples are available from
a target distribution on a set M, the convergence rate for any estimator is bounded by Ω/parenleftbigg
n−β
dM−δ∨n−1/2/parenrightbigg
in theβ-Hölder IPM and Ω/parenleftig
n−1
dM−δ∨n−1
2p/parenrightig
in the Wasserstein p-metric. When the support is regular in
the Minkowski sense, the convergence rates for GANs closely resemble this lower bound (in the β-Hölder
IPM), when the networks are properly chosen. Some future results in this direction might render fruitful
avenues for understanding similar lower bounds, especially with a different notion of the dimensionality of
the data distribution, such as the Wasserstein dimension (Weed & Bach, 2019). Further, while GANs are
known to achieve optimal rates in the Wasserstein-1 distance, it remains an open question whether this result
extends to Wasserstein- pdistances for p≥2.
References
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in
generative adversarial nets (gans). In International Conference on Machine Learning , pp. 224–232. PMLR,
2017.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics.
InInternational Conference on Learning Representations , 2018. URL https://openreview.net/forum?
id=BJehNfW0- .
Hayk Asatryan, Hanno Gottschalk, Marieke Lippert, and Matthias Rottmann. A convenient infinite dimen-
sional framework for generative adversarial learning. Electronic Journal of Statistics , 17(1):391 – 428,
2023. doi: 10.1214/23-EJS2104. URL https://doi.org/10.1214/23-EJS2104 .
Denis Belomestny, Eric Moulines, Alexey Naumov, Nikita Puchkin, and Sergey Samsonov. Rates of conver-
gence for density estimation with gans. arXiv preprint arXiv:2102.00199 , 2021.
P K Bhattacharya and Y P Mack. Weak convergence of k-nn density and regression estimators with varying
k and applications. Annals of Statistics , 15(3):976–994, 1987.
Gérard Biau, Benoît Cadre, Maxime Sangnier, and Ugo Tanielian. Some theoretical properties of gans. The
Annals of Statistics , 48(3):1539–1566, 2020.
Gérard Biau, Maxime Sangnier, and Ugo Tanielian. Some theoretical insights into wasserstein gans. Journal
of Machine Learning Research , 22(1):5287–5331, 2021.
Peter J Bickel and Bo Li. Local polynomial regression on unknown manifolds. Lecture Notes-Monograph
Series, pp. 177–186, 2007.
Saptarshi Chakraborty and Peter Bartlett. A statistical analysis of wasserstein autoencoders for intrinsically
low-dimensional data. In The Twelfth International Conference on Learning Representations , 2024a. URL
https://openreview.net/forum?id=WjRPZsfeBO .
Saptarshi Chakraborty and Peter L. Bartlett. A statistical analysis of deep federated learning for intrinsically
low-dimensional data, 2024b. URL https://arxiv.org/abs/2410.20659 .
Saptarshi Chakraborty, Debolina Paul, and Swagatam Das. Automated clustering of high-dimensional data
with a feature weighted mean shift algorithm. In Proceedings of the AAAI Conference on Artificial Intel-
ligence, volume 35, pp. 6930–6938, 2021.
Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree. Advances in neural
information processing systems , 23, 2010.
9Under review as submission to TMLR
Probal Chaudhuri, Anil K Ghosh, and Hannu Oja. Classification based on hybridization of parametric
and nonparametric classifiers. IEEE transactions on pattern analysis and machine intelligence , 31(7):
1153–1164, 2008.
Biraj Dahal, Alexander Havrilla, Minshuo Chen, Tuo Zhao, and Wenjing Liao. On deep generative models for
approximation and estimation of distributions on manifolds. Advances in Neural Information Processing
Systems, 35:10615–10628, 2022.
Luc P Devroye and Terry J Wagner. The strong uniform consistency of nearest neighbor density estimates.
The Annals of Statistics , pp. 536–540, 1977.
David L Donoho, Iain M Johnstone, Gérard Kerkyacharian, and Dominique Picard. Density estimation by
wavelet thresholding. The Annals of statistics , pp. 508–539, 1996.
Arnaud Doucet, Nando De Freitas, and Neil Gordon. An introduction to sequential monte carlo methods.
Sequential Monte Carlo methods in practice , pp. 3–14, 2001.
Kenneth Falconer. Fractal geometry: mathematical foundations and applications . John Wiley & Sons, 2004.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing
Systems, volume 27. Curran Associates, Inc., 2014.
Jian Huang, Yuling Jiao, Zhen Li, Shiao Liu, Yang Wang, and Yunfei Yang. An error analysis of generative
adversarial networks for learning distributions. Journal of Machine Learning Research , 23(116):1–43, 2022.
URL http://jmlr.org/papers/v23/21-0732.html .
Jisu Kim, Jaehyeok Shin, Alessandro Rinaldo, and Larry Wasserman. Uniform convergence rate of the
kernel density estimator adaptive to intrinsic volume dimension. In International Conference on Machine
Learning , pp. 3398–3407. PMLR, 2019.
Benoît R Kloeckner. Empirical measures: regularity is a counter-curse to dimensionality. ESAIM: Probability
and Statistics , 24:408–434, 2020.
Andrey N Kolmogorov and Vladimir Mikhaılovich Tikhomirov. ϵ-entropy and ϵ-capacity of sets in function
spaces.Translations of the American Mathematical Society , 17:277–364, 1961.
David A Levin and Yuval Peres. Markov chains and mixing times , volume 107. American Mathematical
Society, 2017.
Tengyuan Liang. How well generative adversarial networks learn distributions. Journal of Machine Learning
Research , 22(1):10366–10406, 2021.
Giulia Luise, Massimiliano Pontil, and Carlo Ciliberto. Generalization properties of optimal transport gans
with latent distribution learning. arXiv preprint arXiv:2007.14641 , 2020.
Pertti Mattila. Geometry of sets and measures in Euclidean spaces: fractals and rectifiability . Number 44.
Cambridge University Press, 1999.
DanielMcDonald. Minimaxdensityestimationforgrowingdimension. In Artificial Intelligence and Statistics ,
pp. 194–203. PMLR, 2017.
Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep neural network
with intrinsic dimensionality. Journal of Machine Learning Research , 21(174):1–38, 2020. URL http:
//jmlr.org/papers/v21/20-002.html .
Jonathan Niles-Weed and Quentin Berthet. Minimax estimation of smooth densities in Wasserstein distance.
The Annals of Statistics , 50(3):1519 – 1540, 2022. doi: 10.1214/21-AOS2161. URL https://doi.org/
10.1214/21-AOS2161 .
10Under review as submission to TMLR
Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathematical
statistics , 33(3):1065–1076, 1962.
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension
of images and its impact on learning. In International Conference on Learning Representations , 2020.
Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster assumption.
Journal of Machine Learning Research , 8(7), 2007.
Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. Annals of Statistics , 38(5):
2678–2722, 2010.
Nicolas Schreuder, Victor-Emmanuel Brunel, and Arnak Dalalyan. Statistical guarantees for generative
models without domination. In Algorithmic Learning Theory , pp. 1051–1071. PMLR, 2021.
Shashank Singh and Barnabás Póczos. Minimax distribution estimation in wasserstein distance. arXiv
preprint arXiv:1802.08855 , 2018.
Rong Tang and Yun Yang. Minimax rate of distribution estimation on unknown submanifolds under adver-
sarial losses. The Annals of Statistics , 51(3):1282–1308, 2023.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders. In
International Conference on Learning Representations , 2018.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation . Springer Series in Statistics. Springer,
Springer New York, NY, 1 edition, 2009. ISBN 978-0-387-79051-0. doi: 10.1007/b13794. URL https:
//doi.org/10.1007/b13794 . Published: 26 November 2008.
Ananya Uppal, Shashank Singh, and Barnabás Póczos. Nonparametric density estimation & convergence
rates for gans under besov ipm losses. Advances in neural information processing systems , 32, 2019.
Martin J Wainwright. High-dimensional statistics: A Non-asymptotic Viewpoint , volume 48. Cambridge
University Press, 2019.
Jonathan Weed and Francis Bach. Sharp Asymptotic and Finite-Sample Rates of Convergence of Empirical
Measures in Wasserstein Distance. Bernoulli , 25(4A):2620–2648, 2019. doi: 10.3150/18-BEJ1065. URL
https://doi.org/10.3150/18-BEJ1065 .
Puning Zhao and Lifeng Lai. Analysis of knn density estimation. IEEE Transactions on Information Theory ,
68(12):7971–7995, 2022.
11