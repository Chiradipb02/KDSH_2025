Under review as submission to TMLR
Nonasymptotic Laplace approximation under model mis-
speciﬁcation
Anonymous authors
Paper under double-blind review
Abstract
In this note, we present non-asymptotic two-sided bounds to the log-marginal likelihood
in Bayesian inference. The classical Laplace approximation is recovered as the leading
term. Our derivation permits model misspeciﬁcation and allows the parameter dimension
to grow with the sample size. We do not make any assumptions about the asymptotic
shape of the posterior, and instead require certain regularity conditions on the likelihood
ratio and that the posterior is suﬃciently concentrated. We envision the derived bounds to
be widely applicable in establishing model selection consistency of Bayesian procedures in
non-conjugate settings, especially when the true model potentially lies outside the class of
candidate models considered.
1 Introduction
Suppose data Yis modeled according to a probability distribution Pθ, with the parameter space Θ⊆/Rfracturd
a closed convex set. For each θ, suppose Pθadmits a density pθ= (dPθ/dµ)with respect to a common
σ-ﬁnite measure µon the sample space Y. Assume the map (y,θ)/mapsto→pθ(y)is jointly measurable, and let
/lscript(θ) = logpθ(Y)be the log-likelihood function. Let π(·)be a continuous proper prior on Θand letγ(·)
denote the corresponding posterior distribution so that for any measurable set B,
γ(B) =/integraltext
Be/lscript(θ)π(θ)dθ
Zγ,Zγ=/integraldisplay
Θe/lscript(θ)π(θ)dθ. (1)
The posterior normalizing constant Zγin (1) is commonly referred to as the marginal likelihood or evidence
(Robert, 2007). The marginal likelihood is an ubiquitous tool for model comparison and selection in Bayesian
statistics as it encapsulates an automatic penalty for model complexity.
Barring conjugate settings, the multivariate integral in (1) is rarely available in closed form, necessitating
approximations to the marginal likelihood for computation as well as theoretical analysis. Laplace’s integral
approximation method, commonly referred to as the Laplace approximation (Tierney & Kadane, 1986), is
arguably the most well-known and widely used approximation; see Robert (2007); Ghosh et al. (2007) for
book level treatments. In regular parametric models with nindependent and identically distributed samples
and/hatwideθthe maximum likelihood estimator, the Laplace approximation takes the form logZγ≈/lscript(/hatwideθ)−dlogn/2.
The quantity on the right hand side is, up to a scale factor, the celebrated Bayesian information criterion
(Schwarz, 1978), which is thus realized as an asymptotic approximation to the log-marginal likelihood.
Throughout the article, we reserve the phrase Laplace approximation to exclusively refer to the above and
not the closely related problem of approximating posterior expectations of functionals (Tierney & Kadane,
1986; Tierney et al., 1989; Miyata, 2004; Ruli et al., 2016).
TheusualheuristicderivationoftheLaplaceapproximationproceedsbyperformingaTaylorseriesexpansion
of the log-likelihood function on a neighborhood of the maximum likelihood estimator or the posterior mode
to reduce the integral to a Gaussian integral. This argument can be made rigorous (Chen, 1985; Kass
et al., 1990) under the assumptions of a Bernstein–von Mises theorem guaranteeing the posterior assuming
a Gaussian shape asymptotically; see also Remark 1.4.5. of Ghosh & Ramamoorthi (2003) for an exposition
1Under review as submission to TMLR
along these lines. Shun & McCullagh (1995) showed that if the dimension is comparable with the sample
size, then the usual Laplace approximation is not valid.
In this article, we present a derivation of the Laplace approximation without assuming an asymptotic Gaus-
sian shape of the posterior. Speciﬁcally, we obtain non-asymptotic two-sided bounds on logZγwith the same
leading term, valid with high probability under the true data distribution. While the assumption of a true
data generating distribution is standard in existing derivations (Kass et al., 1990; Cavanaugh & Neath, 1999),
we refrain from assuming the model to be correctly speciﬁed. In such misspeciﬁed settings, the parameter
value in the model class closest to the true distribution in Kullback–Leibler divergence assumes the role of
the true parameter in well-speciﬁed settings.
Ourderivationcruciallyexploitstheconcentrationoftheposteriordistribution(Kleijn&vanderVaart,2006)
around this pseudo-true parameter, which typically requires milder assumptions compared to asymptotic
normality. For example, even in the linear regression setup, one needs strong prior ﬂatness conditions
for asymptotic normality when the parameter dimension grows with the sample size (Bontemps, 2011).
We show that the concentration phenomenon is suﬃcient to localize the assumptions on the likelihood
surface on a neighborhood around the pseudo-true parameter, unlike the global assumptions in Cavanaugh
& Neath (1999). We verify our conditions in the setting of a generalized linear model with growing parameter
dimension,andthesametemplatecanbeusedinothersettingssuchasquantileregressionandmoregenerally,
for model selection beyond the Gaussian linear model (Rossell & Rubio, 2018).
2 Main result
As noted in the introduction, we operate in misspeciﬁed framework allowing the true data distribution Pto
lie outside the model class {Pθ:θ∈Θ}. Without loss of generality, assume P/lessmuchµand letp(·) =dP/dµ(·).
We shall reserve the symbol Eto denote an expectation with respect to P. Let
θ∗= arg min
θ∈ΘD(p||pθ) = arg max
θ∈ΘE/lscript(θ) (2)
betheclosestKullback–Leiblerpointtothetruthinsidetheparameterspace, with D(p||q) =Ep(logp/q)the
Kullback–Leibler divergence between densities pandq. In a misspeciﬁed setting, the pseudo-true parameter
θ∗plays the role of the true parameter in well-speciﬁed models.
We now lay down the assumptions underlying our main result. For any θ,θ†∈Θ, we let/lscript(θ,θ†) =/lscript(θ)−/lscript(θ†)
denote the log-likelihood ratio. Throughout C,C 1,C2,...denote global positive constants. Let /lscriptr(θ) =
/lscript(θ)−E/lscript(θ)andB∗≡B∗
W,R={θ∈Θ : (θ−θ∗)TW(θ−θ∗)≤Rd}for a ﬁxed positive deﬁnite matrix W
and a constant R> 0.
Assumption 1 (Likelihood ratio: deterministic part) .There exists a ﬁxed d×dpositive deﬁnite matrix H
and a constant c∈(1/2,1)such that for all θ∈B∗,
(θ−θ∗)TH(θ−θ∗)/(2c)≥−E/lscript(θ,θ∗)≥(θ−θ∗)TH(θ−θ∗)/2. (3)
Assumption 2 (Likelihood ratio: stochastic part) .There exists a positive constant Cand˜δ∈(0,1/4)such
thatP/braceleftbig
supθ∈B∗|/lscriptr(θ)−/lscriptr(θ∗)|≤Cd/bracerightbig
≥1−˜δ.
Assumption 3 (Prior) .The prior distribution πis continuous and nowhere zero on Θ.
Assumption 4 (Posterior concentration) .There exists constants η,δ∈(0,1/4)such that P/braceleftbig
γ(B∗)≥
1−η/bracerightbig
≥1−δ.
Assumptions 1 and 2 together posit conditions on the log-likelihood ratio /lscript(θ,θ∗)on a neighborhood B∗of
θ∗. We separate the conditions into stochastic and deterministic components by writing /lscript(θ,θ∗) =E/lscript(θ,θ∗)+
/lscriptr(θ)−/lscriptr(θ∗).
Assumption 1 posits that −E/lscript(θ,θ∗)can be approximated by a quadratic form in (θ−θ∗)in a local neighbor-
hood ofθ∗. This is a standard assumption in parametric models; see, e.g. Spokoiny (2012a). If θ/mapsto→E/lscript(θ)
is twice diﬀerentiable, a natural choice to ﬁnd His to perform a Taylor expansion. Since ∇E/lscript(θ∗) = 0,
2Under review as submission to TMLR
the condition (3) is satisﬁed if c−1H&−∇2E/lscript(θ)&Hfor allθ∈B∗, whereA1&A2denotesA1−A2
is nonnegative deﬁnite. Thus in well-speciﬁed regular models, the matrix Hplays the role of the Fisher
Information matrix. Another particular simpliﬁcation arises for well-speciﬁed models where −E/lscript(θ,θ∗)is
the Kullback–Leibler divergence D(pθ∗||pθ), which is known to be locally equivalent to a weighted Euclidean
metric in many parametric models.
Assumption 2 requires control over the supremum of the centered empirical process /lscriptr(θ)asθvaries over the
setB∗. Inspeciﬁcexamples, thiscanbeachievedbyﬁrstboundingtheexpectedsupremum Esupθ∈B∗/lscriptr(θ)−
/lscriptr(θ∗)using a standard chaining argument and then use a concentration inequality for the supremum around
its expectation. Refer to Talagrand (2006); Boucheron et al. (2013); Vershynin (2018) for such arguments
for general empirical processes and van de Geer (2006); Spokoiny (2012b) for a more specialized statistical
context. We also mention the more recent work (Dirksen, 2015) which directly obtains a high-probability
bound for the supremum of an empirical process using generic chaining. Some smoothness assumption on the
likelihood surface is necessary to apply these results, which may be posed on the increments or alternatively,
on the gradient, of the likelihood process. We provide some speciﬁc examples in the next section.
Assumption 3 is broadly satisﬁed and Assumption 4 requires the posterior distribution γ(·)to place suﬃcient
mass around the pseudo-true parameter θ∗. A set of general conditions for posterior concentration in
misspeciﬁed models can be found in Kleijn & van der Vaart (2006); see also De Blasi & Walker (2013);
Sriram et al. (2013); Ramamoorthi et al. (2015); Atchadé (2017); Bhattacharya et al. (2019). We prove a
general theorem for misspeciﬁed high-dimensional generalized linear models in the Appendix. With these
ingredients in place, we state a two-sided bound on the log-marginal likelihood in Theorem 1 below.
Theorem 1. RecallZγfrom(1), and assume Assumptions 1–4 are satisﬁed. Then, with P-probability at
least (1−δ−˜δ), the following bounds in (4)and(5)hold:
logZγ≤/lscript(θ∗)−log|H|
2+/bracketleftbigg
C1d+ log/braceleftbigg
supθ∈B∗π(θ)
1−η/bracerightbigg
+ logP(/bardblξ/bardbl2≤Rd)/bracketrightbigg
, (4)
whereC1=C+ log(2π)/2andξ∼Nd(0,W1/2H−1W1/2). Also,
logZγ≥/lscript(θ∗)−log|H|
2+/braceleftbigg
C2d+ log inf
θ∈B∗π(θ) + logP(/bardblξ/bardbl2≤Rc−1d)/bracerightbigg
, (5)
whereC2=−C+ log(2π)/2 +c/2andξis the same as before.
A proof of Theorem 1 is provided in the Appendix. An inspection of the proof will reveal that the con-
centration of the posterior in Assumption 4 is only utilized for the upper bound. Some additional remarks
regarding the result are in order. We state the bounds in terms of /lscript(θ∗), and not/lscript(/hatwideθ), for convenience of
theoretical analysis. When comparing models, this helps to get rid of one layer on randomness stemming
from the respective /hatwideθfor each model. It is straightforward to modify the argument and state the bounds
in terms of /lscript(/hatwideθ)as detailed in the proof. In regular parametric models with nindependent and identically
distributed samples, |H|/equivasymptoticn−d/2, leading to the recognizable −dlogn/2penalty in the Bayesian information
criterion. Lv & Liu (2014) deﬁned a generalized Bayesian information criterion for misspeciﬁed models with
an additional term containing the sandwich covariance appearing in the asymptotic distribution of the max-
imum likelihood estimator under misspeciﬁcation. However, the sandwich covariance term does not appear
in the asymptotic limit of the posterior under misspeciﬁcation (Kleijn & Van der Vaart, 2012), and also does
not show up in our calculations.
3 Veriﬁcation of assumptions
In this section, we verify the Assumptions in §2 for a generalized linear model, which subsumes a wide array
of examples encountered in practice. For a more direct approach for the special case of i.i.d. exponential
family models, refer to Schwarz (1978); Haughton (1988). Consider covariate-response pairs {(yi,xi)}n
i=1
withyi∈/Rfracturandxi∈/Rfracturd. We consider the moderately high-dimensional regime where dis less than n,
but allowed to grow with n. Lety= (y1,...,yn)Tand letXdenote the n×dmatrix of covariates. We
assume a model on yiconditional on the covariates xiindependently according to a generalized linear model
3Under review as submission to TMLR
PxT
iβin canonical form, with the log-likelihood /lscript(β) = logpβ(y) =/summationtextn
i=1/braceleftbig
yixT
iβ−a(xT
iβ)/bracerightbig
, whereβ∈/Rfracturd
is the unknown vector of regression parameters. The function a:/Rfractur→/Rfracturis convex; we shall denote its ﬁrst
and second derivatives by a(1)anda(2)respectively. We operate in a misspeciﬁed framework and do not
assume the existence of a true regression parameter, and instead only make tail assumptions on the true
data distribution. The pseudo-true parameter β∗satisﬁes
∇E/lscript(β∗) =n/summationdisplay
i=1{Eyi−a(1)(xT
iβ∗)}xi= 0d. (6)
3.1 Veriﬁcation of Assumptions 1 and 2
Let us consider Assumption 1 ﬁrst. We have,
−E/lscript(β,β∗) =n/summationdisplay
i=1/braceleftbigg
a(xT
iβ)−a(xiβ∗)−xT
i(β−β∗)a(1)(xT
iβ∗)/bracerightbigg
=1
2(β−β∗)T/braceleftbiggn/summationdisplay
i=1a(2)(xT
i˜β)xixT
i/bracerightbigg
(β−β∗),
for some ˜βin the line segment joining βandβ∗. The ﬁrst equality in the above display utilizes the
identity (6). Letting u2
i= infβ∈B∗a(2)(xT
iβ)andv2
i= supβ∈B∗a(2)(xT
iβ)fori= 1,...,n, we have
(β−β∗)T/braceleftbig/summationtextn
i=1u2
ixixT
i/bracerightbig
(β−β∗)≤ −E/lscript(β,β∗)≤(β−β∗)T/braceleftbig/summationtextn
i=1v2
ixixT
i/bracerightbig
(β−β∗)Tfor allβ∈B∗.
Thus, we can set H=/summationtextn
i=1u2
ixixT
iandc= miniu2
i/v2
ito satisfy Assumption 1.
The quantity /lscriptr(β)−/lscriptr(β∗)appearing in Assumption 2 equals /angbracketlefty−Ey,X(β−β∗)/angbracketrightin the present context.
Deﬁne an index set T={x∈/Rfracturd:/bardblx/bardbl≤ 1}, and a stochastic process Zα=/angbracketlefty−Ey,Xα/angbracketrightforα∈T.
Observe that for any β/negationslash=β∗∈B∗,
/vextendsingle/vextendsingle/angbracketlefty−Ey,X(β−β∗)/angbracketright/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/angbracketlefty−Ey,X(β−β∗)
/bardblβ−β∗/bardbl/angbracketright/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bardblβ−β∗/bardbl
≤/parenleftbigg
sup
α∈Sd−1|/angbracketlefty−Ey,Xα/angbracketright/vextendsingle/vextendsingle/parenrightbigg
R/parenleftbiggd
n/parenrightbigg1/2
,
whereSd−1={x∈/Rfracturd:/bardblx/bardbl= 1}. Lettingα0= 0d, we can thus bound supβ∈B∗|/lscriptr(β)−/lscriptr(β∗)|≤
R(d/n)1/2/parenleftbig
supα∈T|Zα−Zα0|/parenrightbig
. The veriﬁcation of Assumption 2 thus requires control over the supremum of
the stochastic process (Zα), which in turn depends on the moment assumptions on the true data distribution.
We illustrate this through two diﬀerent examples below.
As a ﬁrst example, assume that (y−Ey)is a centered sub-Gaussian random variable (Vershynin, 2018),
that is, there exists a constant τ > 0such that for any v∈/Rfracturn,Eexp/angbracketlefty−Ey,v/angbracketright≤ exp(τ2/bardblv/bardbl2/2). If
the coordinates yiare independent, one may take τ= maxi/bardblyi−Eyi/bardblψ2to be the maximum of the sub-
Gaussian norms of (yi−Eyi); see Vershynin (2018) for deﬁnition of the sub-Gaussian norm /bardbl·/bardblψ2. However,
independence is not necessary for the above condition to hold and it can be veriﬁed for various dependence
structures. In particular, if yhas a joint Gaussian distribution, then τequals the largest eigenvalue of
cov(y). Under the above sub-Gaussian assumption, the process (Zα)has sub-Gaussian increments, since for
anyλ∈/Rfractur,
Eeλ(Zα−Z˜α)≤eλ2τ2/bardblXα−X˜α/bardbl2/2≤eλ2τ2/bardblX/bardbl2
2/bardblα−˜α/bardbl2,
where/bardblX/bardbl2is the operator norm of X. For processes with sub-Gaussian increments, a convenient high-
probability bound for the supremum was developed in Liaw et al. (2017, Theorem 4.1) as a corollary to the
more general tail bound of Dirksen (2015). In preparation for applying their bound, we have /bardblZα−Z˜α/bardblψ2≤
τ/bardblX/bardbl2/bardblα−˜α/bardblfor anyα,˜α∈T. Also, diam (T) = supα,˜α∈T/bardblα−˜α/bardbl≤2and the Gaussian width of T,
Esupα∈T/angbracketleftg,α/angbracketrightforg∼Nd(0,Id), is in the order of d1/2. Thus, with probability at least 1−e−d,supα∈T|Zα−
Zα0|≤Cτ/bardblX/bardbl2d1/2. It then follows that with probability at least 1−e−d,supβ∈B∗|/lscriptr(β)−/lscriptr(β∗)|≤Cd.
4Under review as submission to TMLR
Alternatively, suppose (yi−Eyi)are independent sub-exponential (Vershynin, 2018) random variables, so
that there exist gi>0andνisuch that
Eeλ(yi−Eyi)≤eλ2ν2
i/2,|λ|<gi, i= 1,...,n.
Fixλsuch that|λ|≤minigi:= ¯g−1. Under the above assumption, we have, for any α,˜α∈Tthat
EeλZα−Z˜α
/bardblXα−X˜α/bardbl=n/productdisplay
i=1EeλxT
i(α−˜α)
/bardblXα−X˜α/bardbl(yi−Eyi)≤eλ2/summationtextn
i=1ν2
i{xT
i(α−˜α)}2
/bardblXα−X˜α/bardbl2
≤eλ2ν2/2≤eλ2ν2/{2(1−|λ|¯g)},
whereν= maxiνi. From the second to the third step, we used that |xT
i(α−˜α)|//bardblXα−X˜α/bardbl≤1. Hence
Zαis a centered process on Twith sub-exponential increments. Deﬁne a norm d(α1,α2) =/bardblXα1−Xα2/bardbl.
Clearly,d(α1,α2)≤/bardblX/bardbl2forα1,α2∈T. From Theorem 2.1 of Baraud (2010),
P/bracketleftBig
sup
α∈T|Zα−Zα0|>/bardblX/bardbl2√
1 +x+ ¯gx/bracketrightBig
≤2e−x,x> 0,
thereby verifying Assumption 2 by setting x=d.
3.2 Veriﬁcation of Assumptions 3 and 4
Although literature on posterior contraction of regression parameters in linear models is abundant, both in
moderately high-dimensional and ultra-high dimensional settings; see the introduction of Gao et al. (2015)
for a general list of references; analogous results for generalized linear models are comparatively sparse, with
the exception of Jiang et al. (2007). However, special cases including high dimensional logistic regression
using a pseudo likelihood (Atchadé, 2017) and high-dimensional logistic regression using shrinkage priors
(Wei & Ghosal, 2020) are available. Although it is possible to use such results directly to verify Assumption
4, this would typically come with additional restriction necessitated by the speciﬁc goals targeted in these
papers. Jiang et al. (2007) operated in a well-speciﬁed setting where the use of a Gaussian prior leads to
a restrictive assumption on the growth of the true coeﬃcients; refer to the assumptions of Theorem 1 in
pg. 1493. Atchadé (2017) considered a Laplace-type prior for the coeﬃcients which obviated the need for
such a restriction, but their results are speciﬁc to logistic regression. We focus on extending the result
of Atchadé (2017) to accommodate other families and allow for model misspeciﬁcation in the moderately
high-dimensional regime with no sparsity assumption on the coeﬃcients. We prove this result (Theorem 2)
in the C; a sketch of the main ingredients is given below.
A posterior contraction result requires non-local versions of Assumptions 1 and 2 in the complement of the
neighborhood under consideration. A fundamental technique (Ghosal et al., 2000) to prove such a result
is to enforce that the likelihood ratio is appropriately small in (B∗)cand that the prior assigns suﬃcient
probability around the true parameter in B∗. The ﬁrst condition ensures that the numerator of the posterior
probability of (B∗)cis small and the second condition prevents the denominator from becoming too small.
The separation of the likelihood in (B∗)crelies on the decomposition /lscript(β,β∗) =/lscriptr(β)−/lscriptr(β∗) +E/lscript(β,β∗)
and then ensuring that E/lscript(β,β∗)is suﬃciently negative to oﬀset the stochastic variation in /lscriptr(β)−/lscriptr(β∗).
Although E/lscript(β,β∗)has a local quadratic shape for any member of the generalized linear model in B∗,
E/lscript(β,β∗)fails to be so outside B∗for certain members in the family. For instance, E/lscript(β,β∗)is approximately
linear outside B∗for logistic regression. Hence a suitable modiﬁcation to the lower bound in Assumption
1 in required. This can be encapsulated through an assumption on aasa(t+h)≥a(t) +ha(1)(t) +
r(|h|)a(2)(t)/2for allt,h, where r(·)is arate function (Atchadé, 2017) from R+toR+satisfying i) r(0) = 0,
ii)limh→0r(h) = 0and iii)r(h)≥h2/(r1+ r2h)forr1,r2≥0not simultaneously 0. This class of afunctions
includes the Gaussian a(t) =t2,r(h) =h2; logistica(t) =−log(1 +e−t),r(h) =h2/(h+ 2); and Poisson
a(t) =et,r(h) =h2, among others. Using such a lower bound on ait is possible to develop sharp lower
bounds for−E/lscript(β,β∗)on(B∗)cleading to a dominating negative term in the numerator. The stochastic
term on the other hand can be controlled by assuming y−Eyto be sub-Gaussian, in a very similar way the
term/lscriptr(β)−/lscriptr(β∗)is controlled in B∗.
5Under review as submission to TMLR
The treatment of the denominator needs extra care to avoid any assumption on the growth of the true
coeﬃcients. Motivated by Atchadé (2017); Castillo & van der Vaart (2012); Castillo et al. (2015), we
consider a Laplace-type prior ( ∝exp{−κh}, wherehis Lipschitz outside a neighborhood around zero and
κ>0is a constant) on the regression coeﬃcients β. The right amount of tail thickness associated with such
priors leads to an assumption free estimation of the regression coeﬃcients β.
4 Discussion
Consider the general setup of Bayesian model selection (Bishop, 2006; Hoeting et al., 1999), where we are
given a set of Kcandidate models {Mk}K
k=1with prior probabilities {pk}K
k=1. Thekth model postulates a
probability model fk(Y|θk,Mk)for the data with model parameters θk∈Θk, which is endowed with a
priorπk(·). Then, the posterior probability of the kth model given data Yis given by
pk(Y) : =pkmk(Y)/summationtext
jpjmj(Y), mk(Y) =/integraldisplay
fk(Y|θk,Mk)πk(dθk).
In particular, the maximum a posteriori model/hatwidek= arg maxkpk(Y)reports the model with the highest
posterior probability. Note that one can express 1/pk(Y) = 1 +/summationtext
j/negationslash=k(pj/pk)BFjk, where BF jk(Y) =mj(Y)
mk(Y)
is theBayes factor between models jandk, which is simply the ratio of the marginal likelihoods. Asymptotic
analysisoftheposteriormodelprobabilitiesnecessarilyrequirecontroloverthebehaviorof logBFjk(Y)under
the true data distribution, which may lie outside any of the candidate models considered. In such scenarios,
our bounding technique can be generally applied.
References
Y.A. Atchadé. On the contraction properties of some high-dimensional quasi-posterior distributions. The
Annals of Statistics , 45(5):2248–2273, 2017.
Yannick Baraud. A Bernstein-type inequality for suprema of random processes with applications to model
selection in non-Gaussian regression. Bernoulli , 16(4):1064–1085, 2010.
A. Bhattacharya, D. Pati, and Y. Yang. Bayesian fractional posteriors. The Annals Statistics , 47(1):39–66,
02 2019. doi: 10.1214/18-AOS1712.
Christopher M Bishop. Pattern recognition and machine learning , volume 4. Springer, 2006.
Dominique Bontemps. Bernstein–von Mises theorems for Gaussian regression with increasing number of
regressors. The Annals of Statistics , 39(5):2557–2584, 2011.
S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of indepen-
dence. Oxford university press, 2013.
I. Castillo and A.W. van der Vaart. Needles and straw in a haystack: Posterior concentration for possibly
sparse sequences. The Annals of Statistics , 40(4):2069–2101, 2012.
Ismaël Castillo, Johannes Schmidt-Hieber, and Aad Van der Vaart. Bayesian linear regression with sparse
priors.The Annals of Statistics , 43(5):1986–2018, 2015.
J.E. Cavanaugh and A.A. Neath. Generalizing the derivation of the Schwarz information criterion. Commu-
nications in Statistics-Theory and Methods , 28(1):49–66, 1999.
C-F. Chen. On asymptotic normality of limiting density functions with Bayesian implications. Journal of
the Royal Statistical Society: Series B (Methodological) , 47(3):540–546, 1985.
P. De Blasi and S. G. Walker. Bayesian asymptotics with misspeciﬁed models. Statistica Sinica , pp. 169–187,
2013.
S. Dirksen. Tail bounds via generic chaining. Electronic Journal of Probability , 20, 2015.
6Under review as submission to TMLR
Chao Gao, Aad W van der Vaart, and Harrison H Zhou. A general framework for Bayes structured linear
models.The Annals of Statistucs , 2015. (to appear).
S. Ghosal, J. K. Ghosh, and A. W. van der Vaart. Convergence rates of posterior distributions. The Annals
of Statistics , 28(2):500–531, 2000.
J. K. Ghosh and R.V. Ramamoorthi. Bayesian Nonparametrics . Springer–Verlag New York, Inc., 2003.
J.K. Ghosh, M. Delampady, and T. Samanta. An introduction to Bayesian analysis: theory and methods .
Springer Science & Business Media, 2007.
Dominique MA Haughton. On the choice of a model to ﬁt data from an exponential family. The Annals of
Statistics , 16(1):342–355, 1988.
Jennifer A Hoeting, David Madigan, Adrian E Raftery, and Chris T Volinsky. Bayesian model averaging: a
tutorial (with comments by m. clyde, david draper and ei george, and a rejoinder by the authors. Statistical
science, 14(4):382–417, 1999.
Wenxin Jiang et al. Bayesian variable selection for high dimensional generalized linear models: convergence
rates of the ﬁtted densities. The Annals of Statistics , 35(4):1487–1511, 2007.
R.E. Kass, L. Tierney, and J.B. Kadane. The validity of posterior expansions based on Laplace’s method.
Bayesian and Likelihood Methods in Statistics and Econometrics: Essays in Honor of George A. Barnard ,
7:473–491, 1990.
B. J. K. Kleijn and A. W. van der Vaart. Misspeciﬁcation in inﬁnite-dimensional Bayesian statistics. Ann.
Statist., pp. 837–877, 2006.
B.J.K. Kleijn and A.W. Van der Vaart. The Bernstein von–Mises theorem under misspeciﬁcation. Electronic
Journal of Statistics , 6:354–381, 2012.
Christopher Liaw, Abbas Mehrabian, Yaniv Plan, and Roman Vershynin. A simple tool for bounding the
deviation of random matrices on geometric sets. In Geometric aspects of functional analysis , pp. 277–299.
Springer, 2017.
J. Lv and J.S. Liu. Model selection principles in misspeciﬁed models. Journal of the Royal Statistical Society:
Series B (Statistical Methodology) , 76(1):141–167, 2014.
Y. Miyata. Fully exponential Laplace approximations using asymptotic modes. Journal of the American
Statistical Association , 99(468):1037–1049, 2004.
R. V. Ramamoorthi, K. Sriram, and R. Martin. On posterior concentration in misspeciﬁed models. Bayesian
Analysis, 10(4):759–789, 2015.
C. Robert. The Bayesian choice: from decision-theoretic foundations to computational implementation .
Springer Science & Business Media, 2007.
David Rossell and Francisco J Rubio. Tractable Bayesian variable selection: beyond normality. Journal of
the American Statistical Association , 113(524):1742–1758, 2018.
E. Ruli, N. Sartori, and L. Ventura. Improved Laplace approximation for marginal likelihoods. Electronic
Journal of Statistics , 10(2):3986–4009, 2016.
G. Schwarz. Estimating the dimension of a model. The Annals of Statistics , 6(2):461–464, 1978.
Zhenming Shun and Peter McCullagh. Laplace approximation of high dimensional integrals. Journal of the
Royal Statistical Society: Series B (Methodological) , 57(4):749–760, 1995.
V. Spokoiny. Parametric estimation. ﬁnite sample theory. The Annals of Statistics , 40(6):2877–2909, 2012a.
V. Spokoiny. Supplement to parametric estimation. ﬁnite sample theory. The Annals of Statistics , 40(6):
2877–2909, 2012b.
7Under review as submission to TMLR
K. Sriram, R.V. Ramamoorthi, and P. Ghosh. Posterior consistency of Bayesian quantile regression based
on the misspeciﬁed asymmetric Laplace density. Bayesian Analysis , 8(2):479–504, 2013.
M. Talagrand. The generic chaining: upper and lower bounds of stochastic processes . Springer Science &
Business Media, 2006.
L.TierneyandJ.B.Kadane. Accurateapproximationsforposteriormomentsandmarginaldensities. Journal
of the American Statistical Association , 81(393):82–86, 1986.
L. Tierney, R.E. Kass, and J.B. Kadane. Fully exponential Laplace approximations to expectations and
variances of nonpositive functions. Journal of the American Statistical Association , 84(407):710–716,
1989.
S. van de Geer. Empirical Processes in M-estimation . Cambridge UP, 2006.
R. Vershynin. High-dimensional probability: An introduction with applications in data science , volume 47.
Cambridge University Press, 2018.
Ran Wei and Subhashis Ghosal. Contraction properties of shrinkage priors in logistic regression. Journal of
Statistical Planning and Inference , 2020.
A Appendix
B Proof of Theorem 1
LetYgdenote the subset of the sample space Ywhere the events in Assumptions 1 and 2 both hold. We
shall work inside the set Yg, with P(Yg)≥1−δ−˜δby Bonferroni’s inequality.
We ﬁrst prove the upper bound (4). By Assumption 1,
(1−η)≤γ(B∗) =/integraltext
B∗e/lscript(θ,θ∗)π(θ)dθ/integraltext
Θe/lscript(θ,θ∗)π(θ)dθ.
Rearranging terms, this gives
logZγ≤/lscript(θ∗) + log/parenleftbigg1
1−η/parenrightbigg
+ log/integraldisplay
B∗e/lscript(θ,θ∗)π(θ)dθ.
We now bound the integral in the right hand side of the above display. We have,
/integraldisplay
B∗e/lscript(θ,θ∗)π(θ)dθ=/integraldisplay
B∗e/lscriptr(θ)−/lscriptr(θ∗)+E/lscript(θ,θ∗)π(θ)dθ≤eCd/integraldisplay
B∗e−(θ−θ∗)TH(θ−θ∗)/2π(θ)dθ
≤{sup
θ∈B∗π(θ)}eCd(2π)d/2|H|−1/2/integraldisplay
B∗φd(θ;θ∗,H−1)dθ,
whereφd(x;µ,Σ)denotes ad-variate normal density with mean µand covariance Σevaluated at x∈/Rfracturd.
The bound (4) follows since/integraltext
B∗φd(θ;θ∗,H−1)dθ=P(/bardblξ/bardbl2<Rd )forξ∼Nd(0,W1/2H−1W1/2).
For the lower bound, we use
logZf=/lscript(θ∗) +/integraldisplay
Θe/lscript(θ,θ∗)π(θ)dθ≥/lscript(θ∗) +/integraldisplay
B∗e/lscript(θ,θ∗)π(θ)dθ.
We now bound the integral in the right hand side of the above display from below. We have,
/integraldisplay
B∗e/lscript(θ,θ∗)π(θ)dθ=/integraldisplay
B∗e/lscriptr(θ)−/lscriptr(θ∗)+E/lscript(θ,θ∗)π(θ)dθ
8Under review as submission to TMLR
≥e−Cd/integraldisplay
B∗e−(θ−θ∗)TH(θ−θ∗)/(2c)π(θ)dθ
≥{inf
θ∈B∗π(θ)}e−Cd(2π)d/2cd/2|H|−1/2/integraldisplay
B∗φd(θ;θ∗,cH−1)dθ.
Finally, we have/integraltext
B∗φd(θ;θ∗,cH−1)dθ=P(/bardblξ/bardbl2<Rc−1d)whereξ∼Nd(0,W1/2H−1W1/2).
C Posterior concentration in generalized linear models
Consider a generalized linear model with the canonical parameterization: yiind.∼PxT
iβfori= 1,...,n, and
the log-likelihood as L(β) := logpβ(y) =/summationtextn
i=1/braceleftbig
yixT
iβ−a(xT
iβ)/bracerightbig
, where (yi,xi)∈/Rfractur×/Rfracturp,β∈/Rfracturpis the
parameter of interest, and ais a real valued convex function. We allow the true density p0(y)ofyito be
misspeciﬁed and let E(yi) =Aiand Var (Y) = Σ. We note some important properties of the model.
C.1 Properties of various aspects of the model
We deﬁne the pseudo-true parameter β∗asβ∗=arg maxβ∈RpEL(β). UnderPxT
iβ∗,E(yi) =a(1)(xT
iβ∗)and
Var(yi) =a(2)(xT
iβ∗). Also,∇EL(β∗) = 0which implies/summationtextn
i=1{Ai−a(1)(xT
iβ∗)}xi= 0.V2
0:=Var{∇L(β∗)}
andD2
0:=−E{∇2L(β∗)}=XTWX, whereW=diag{a(2)(xT
1β∗),...,a(2)(xT
nβ∗)}. Next, we look at some
important divergences/distance measures deﬁned as follows. A subscript 0will indicate the divergence
measure to be misspeciﬁed.
D0(β∗,β) := E/braceleftbigg
logpβ∗(y)
pβ(y)/bracerightbigg
=n/summationdisplay
i=1/braceleftbig
a(xT
iβ)−a(xT
iβ∗)−a(1)(xT
iβ∗)xT
i(β−β∗)/bracerightbig
=D(β∗,β),
V0(β∗,β) := E/braceleftbigg
logpβ∗(y)
pβ(y)−D0(β∗,β)/bracerightbigg2
,
D0,α(β∗,β) :=1
α−1logA0,α(β∗,β) :=1
α−1log/integraldisplay/braceleftbiggpβ(y)
pβ∗(y)/bracerightbiggα
p0(y)dy,
A0,α(β∗,β) = Eexp/bracketleftbig
α/angbracketlefty−Ey,X(β−β∗)/bracketrightbig
exp{−αD(β∗,β)}.
Note that we deﬁne the misspeciﬁed divergences only for the pair β∗,βwhich forces D0(β∗,β)≥0.
D0,α(β∗,β)is not necessarily a divergence and we shall impose assumptions on the true distribution of
yiwhich allows D0,α(β∗,β)≥0. For anyβ1,β2,H2(β1,β2) := 1−A1/2(β1,β2). Noting that
logp∗
β(y)
pβ(y)= (β−β∗)TXTY−n/summationdisplay
i=1[a(xT
iβ)−a(xT
iβ∗)]
andVar(Y) = Σ, we haveV0(β∗,β)≤(β−β∗)TXTΣX(β−β∗).Note thatD0(β∗,β)≤K(β,β∗)n/bardblβ∗−β/bardbl2,
whereK(β,β∗) = sup ˜β∈L(β∗,β)λp{XTW(˜β)X/n},
W(˜β) =diag{a(2)(xT
1˜β),...,a(2)(xT
n˜β)}andL(β∗,β)is the line-segment connecting β∗andβ.
C.2 Assumptions on the generalized linear model
We set our model assumptions to control the log-likelihood ratio
pβ(y)
pβ∗(y)= exp/bracketleftbig
/angbracketlefty−Ey,X(β−β∗)−D(β∗,β)/bracketrightbig
. (7)
The ﬁrst part in the right hand side of (7) is a stochastic term which can be controlled using appropriate sub-
Gaussian assumption on y−Ey. The second term involves a deterministic quantity which can be bounded
using an appropriate condition on the second derivative of the afunction. The following assumptions achieve
this in a concrete fashion.
9Under review as submission to TMLR
Assumption 5. Letκ1:=λ1(XTWX/n )>0.
Assumption 6. Assumeasatisﬁesa(t+h)≥a(t) +ha(1)(t) + r(|h|)a(2)(t)/2for allt,h, where r(·)is a
rate function fromR+toR+satisfying i) r(0) = 0, ii) limh→0r(h) = 0and iii)r(h)≥h2/(r1+ r2h)for
r1,r2≥0not simultaneously 0.
Assumption 7. K:= supβ:/bardblβ−β∗/bardbl≤/epsilon1nK(β,β∗)<∞where/epsilon1nis the rate of posterior convergence.
Remark 1. Assumption 6 can be used to provide a lower bound for D(β∗,β)in the following manner. If a
satisﬁes Assumption 6,
D(β∗,β) =n/summationdisplay
i=1/braceleftbig
a(xT
iβ)−a(xT
iβ∗)−a(1)(xT
iβ∗)xT
i(β−β∗)/bracerightbig
≥n/summationdisplay
i=1r(|xT
i(β−β∗)|)a(2)(xT
iβ∗).
Deﬁning k(h) =h2/r(h),
D(β∗,β)≥(β−β∗)T/bracketleftbiggn/summationdisplay
i=1a(2)(xT
iβ∗)
k(|xT
i(β−β∗)|)xixT
i/bracketrightbigg
(β−β∗)
≥(β−β∗)TXTWX(β−β∗)
r1+ r2/bardblX/bardbl∞√
d/bardblβ−β∗/bardbl,
where the last inequality follows from the fact that k(h)≤r1+ r2hand|xT
i(β−β∗)|≤/bardblX/bardbl∞√
d/bardblβ−β∗/bardbl.
C.3 Assumption on the data generating distribution
We assume that (y−Ey)is a centered sub-Gaussian random vector.
Assumption 8. Assume that there exists a constant τ >0such that for any v∈Rn,
Eexp/angbracketlefty−Ey,v/angbracketright≤eτ2/bardblv/bardbl2/2.
Remark 2. For example, if yhas a joint Gaussian distribution with covariance matrix Σ, then we may take
τ=/bardblΣ/bardbl2. Under this assumption, we revisit the quantity /angbracketlefty−Ey,X(β−β∗)/angbracketright. We claim the following: with
probability at least 1−e−d,
|/angbracketlefty−Ey,X(β−β∗)/angbracketright|≤τ/bardblX/bardbl2√
d/bardblβ−β∗/bardbl,∀β∈Rd.
Note that the probability statement is uniform in β. The proof uses a majorizing measure theorem (see
Theorem 4.1 of Liaw et al. (2017)). To prepare for the proof, note ﬁrst that for any β∈Rd,
|/angbracketlefty−Ey,X(β−β∗)/angbracketright|≤/parenleftbigg
sup
u∈T|/angbracketlefty−Ey,Xu/angbracketright|/parenrightbigg
/bardblβ−β∗/bardbl,
withT=Sd−1∪{0d}. Deﬁne a stochastic process Wu=/angbracketlefty−Ey,Xu/angbracketrightforu∈T. Note that
sup
u∈T|/angbracketlefty−Ey,Xu/angbracketright|= sup
u∈T|Wu−W0|≤sup
u,˜u∈T|Wu−W˜u|.
We shall invoke Theorem 4.1 to obtain a high probability bound to the quantity in the right most side of the
above display. The process Whas sub-Gaussian increments. We have, for any u,˜u∈T,
Eeλ(Wu−W˜u)≤eλ2τ2/bardblXu−X˜u/bardbl2/2≤eλ2τ2/bardblX/bardbl2
2/bardblu−˜u/bardbl2/2.
Hence, for any u,˜u∈T,
/bardblWu−W˜u/bardblψ2≤τ/bardblX/bardbl2/bardblu−˜u/bardbl2.
This implies the constant Min their theorem can be taken as M=τ/bardblX/bardbl2. Finally, note that diam (T)≤2
and the Gaussian width of T,Esupx∈T/angbracketleftg,x/angbracketrightforg∼N(0,Id), is of the order√
d. The proof is completed by
takingu=√
d.
10Under review as submission to TMLR
Assumption 9. There exists φ∈(0,1)such that
n≥τr2d/bardblX/bardbl2/bardblX/bardbl∞
φκ1,(1−φ)r1
φ(r2/bardblX/bardbl∞√
d)≥/epsilon1n.
Since/bardblX/bardbl2≤√n/bardblX/bardbl∞, Assumption 9 allows dto increase with nat a rate slightly slower than√n.
C.4 Assumptions on the prior
We assume that πis a product of ddensities of the form e−κh, for a function hsatisfying for some constant
c>0,
|h(x)−h(y)|≤D+D|x−y|,∀x,y∈R,
for some constant D> 0. This covers Laplace and Student densities, which corresponds to uniformly Lips-
chitzh. It also covers other smooth densities with polynomial tails, and densities of the form cαexp{−κ|x|α}
for someα∈(0,1]which corresponds to Lipschitz houtside a neighborhood of the origin. On the other
hand the standard normal density is ruled out.
C.5 Main result on posterior contraction
In the following, we state our main theorem on posterior contraction using the assumptions on the data
generating process in §C.3, the model in §C.2 with the prior in §C.4.
Theorem 2. Assume Assumption 8 on the data generation mechanism and Assumptions 7, 5, 9 and
6 on model and the prior assumptions in §C.4. Then there exists positive constants C1,C2, such that
for anyη∈(0,1)there exists δ=e−C2d/ηsuch that P/braceleftbig
γ(B∗)≥1−η/bracerightbig
≥1−δ, where the set
B∗={β:/bardblβ−β∗/bardbl≤C1/radicalbig
d/n}.
In Theorem 2 we make no sparsity assumptions on βand let the dimension dto increase with nat a rate
slightly slower than√n. Notably, the convergence rate we obtained is sharp minimax (/radicalbig
d/n) without any
logarithmic term. Our non-asymptotic version of the Laplace approximation as Theorem 1 in the main
document is valid for dgrowing at this rate. This is in stark contrast with Shun & McCullagh (1995) who
showed that the remainder terms of the Laplace approximation do not vanish unless d3/n→0. This is
due to diﬀerence in assumptions in the likelihood and the prior. Also our Laplace approximation does not
require maximizing the likelihood as in Shun & McCullagh (1995), instead we evaluate the likelihood at the
pseudo-true parameter θ∗.
Another salient feature of our result is the absence of any assumption on the norm of βwhich is possible
due to the use of a heavier tailed prior distribution on β. We conjecture that the use of a Gaussian prior
will lead to a degradation of the convergence rate unless the norm of the true coeﬃcients is appropriately
bounded.
C.6 Proof of Theorem 2
We divide up the proof into two separate parts.
Treatment of the denominator: In the following, we lower bound the normalizing constant of the
posterior distribution. The technical details are fairly standard modiﬁcation of
Lemma 1. Under Assumption 7 and assuming πsatisﬁes the assumption in §C.4, we have for any sequence
of numbers /epsilon1ngoing to 0,
/integraldisplaypβ(y)
pβ∗(y)π(β)dβ≥cd
κe−κ/summationtextd
j=1{h(β∗
j)+D}/integraldisplay
/bardblz/bardbl≤/epsilon1ne−Kn/bardblz/bardbl2/2−κD/summationtextd
j=1|zj|dz,
wherecκis the normalizer of πford= 1.
11Under review as submission to TMLR
Treatment of the numerator: In this section, we assume Assumptions 8, 7, 5, 9 and 6 and the prior
assumptions in §C.4 Deﬁne Ωnbe the set
sup
u∈T|/angbracketlefty−Ey,Xu/angbracketright|≤τ/bardblX/bardbl2√
d.
We ﬁrst detail our main result for test construction. Deﬁne a mapping from pβto the space of ﬁnite measures
as
pβ/mapsto→qβ:=p0
pβ∗pβ 1Ωn
For any/epsilon1>0, deﬁneB(β1;/epsilon1) ={pβ:/bardblβ−β1/bardbl</epsilon1}. Denote by conv{B(β1;/epsilon1)}the convex hull of B(β1;/epsilon1).
Pick anyβ1such that/bardblβ1−β∗/bardbl=r. Then the following holds.
Lemma 2. Assume Assumption 8 on the data generation mechanism and Assumptions 7, 5, 9 and 6 on
model. Then for r≥(1−φ)r1
φ(r2/bardblX/bardbl∞√
d), there exists measurable functions 0≤Φn,β1≤1such that for every n≥1
sup
pβ∈conv{B(β1;r/2)}E0Φn,β1+Eqβ(1−Φn,β1)≤exp/braceleftbigg
−nκ1(1−φ)α/bardblβ1−β∗/bardbl
2r2/bardblX/bardbl∞√
d/bracerightbigg
.
Now consider the following decomposition for any sequence of measurable functions ˜Φn(functions of y(n)),
E0γ{(B∗)c}≤E0˜Φn+E0/braceleftbig
γ{(B∗)c} 1Ωn(1−˜Φn)/bracerightbig
+P0(Ωc
n), (8)
where P0(Ωc
n)≤e−dfrom Remark 2. Let D(β∗) =cd
κe−κ/summationtextd
j=1{h(β∗
j)+D}/integraltext
e−K/bardblz/bardbl2/2−κD/summationtextd
j=1|zj|dz. Writing
for ﬁxedM > 0,
U:={β:/bardblβ−β∗/bardbl>M/epsilon1n}=∞/uniondisplay
j=1Uj,n (9)
whereUj,n={β:jM/epsilon1n</bardblβ−β∗/bardbl<(j+ 1)M/epsilon1n}, the second term in the rhs of (8) can be further
decomposed as
E0/braceleftbig
γ{(B∗)c} 1Ωn(1−˜Φn)/bracerightbig
≤D(β∗)−1∞/summationdisplay
j=1/integraldisplay
Uj,nE0/bracketleftBig
1Ωn(1−˜Φn)pβ(y)
pβ∗(y)/bracketrightBig
π(β)dβ. (10)
LetNj,n:=N(jM/epsilon1n/2,Uj,n,/bardbl·/bardbl)denote thejM/epsilon1n/2-covering number of Uj,nwith respect to/bardbl·/bardbl. For each
j≥1, letSjbe a maximal jM/epsilon1n/2-separated points in Uj,nand for each point ˜βk∈Sjwe can construct a
test function Φn,˜βkas in Lemma 2, with r=jM/epsilon1n. Then we set ˜Φnto
˜Φn= sup
j≥1max
˜βk∈SjΦn,˜βk.
From Lemma 2 since(1−φ)r1
φ(r2/bardblX/bardbl∞√
d)≥/epsilon1n,
E0{˜Φn} ≤/summationdisplay
j=1Nj,nexp/braceleftbigg
−nκ1(1−φ)αjM/epsilon1n
2r2/bardblX/bardbl∞√
d/bracerightbigg
,
E0/braceleftbig
γ(B∗) 1Ωn(1−˜Φn)/bracerightbig
≤∞/summationdisplay
j=1/braceleftbiggΠ(Uj,n)
D(β∗)/bracerightbigg
exp/braceleftbigg
−nκ1(1−φ)αjM/epsilon1n
2r2/bardblX/bardbl∞√
d/bracerightbigg
.
ClearlyNj,n≤9dand
Π(Uj,n)
D(β∗)≤I−1×eκdD/integraldisplay
Uj,neκ/summationtextd
j=1{h(β∗
j)−h(βj)}dβ
12Under review as submission to TMLR
whereI=/integraltext
/bardblz/bardbl≤/epsilon1ne−Kn/bardblz/bardbl2/2−κD/summationtextd
j=1|zj|dz. Also the assumption in §C.4 entails
d/summationdisplay
j=1{h(β∗
j)−h(βj)}≤dD+D/bardblβ−β∗/bardbl1≤dD+ 2D√
d/bardblβ−β∗/bardbl2−D/bardblβ−β∗/bardbl1.
Hence
Π(Uj,n)
D(β∗)≤Ie2κdD+2D√
d(j+1)M/epsilon1n/integraltext
Uj,ne−D/bardblβ−β∗/bardbl1dβ
≤(Id
2/I) exp{2κdD + 2D√
d(j+ 1)M/epsilon1n}. (11)
whereI2=/integraltext
Re−D|x|dx. Note that
I≥e−√
d/epsilon1n/integraldisplay
/bardblz/bardbl≤/epsilon1ne−Kn/bardblz/bardbl2dz=e−√
d/epsilon1n/integraldisplay/epsilon1n
0e−Knr2rd−1dr
=1
2/epsilon1d
n(Kn/epsilon12
n)−d/2/bracketleftbig
Γ(d/2)−Γ(d/2,Kn/epsilon12
n)/bracketrightbig
, (12)
where Γ(a,x)is the incomplete Gamma function deﬁned by/integraltext∞
xta−1e−tdt. From (8)-(11), and noting from
(12) thatI≥exp{−cdlogn}for some constant c>0, we have
E0/braceleftbig
γ{(B∗)c} 1Ωn(1−˜Φn)/bracerightbig
≤∞/summationdisplay
j=1(Id
2/I) exp/braceleftBig
2κdD + (13)
2D√
d(j+ 1)M/epsilon1n−nκ1αjM/epsilon1n
2r2/bardblX/bardbl∞√
d/bracerightBig
(14)
≤∞/summationdisplay
j=1exp/braceleftBig
−C1nκ1(1−φ)αjM/epsilon1n
r2/bardblX/bardbl∞√
d/bracerightBig
(15)
and
E0˜Φn≤∞/summationdisplay
j=19dexp/braceleftBig
−nκ1αjM/epsilon1n
r2/bardblX/bardbl∞√
d/bracerightBig
(16)
≤∞/summationdisplay
j=1exp/braceleftBig
−C2nκ1αjM/epsilon1n
r2/bardblX/bardbl∞√
d/bracerightBig
(17)
for some constants C1,C2>0, by Assumption 9. Hence E0γ(B∗)>1−e−Cdfor some constant C > 0. An application
of Markov’s inequality concludes the proof of Theorem 2.
D Some auxiliary results
D.1 Proof of Lemma 2
Setλd:=τ/bardblX/bardbl2√
dandUr+λd:=nκ1/{r2/bardblX/bardbl∞√
d}. ThenUr+λd≥(1−φ)nκ1
r2/bardblX/bardbl∞√
d, then
(Ur+λd)r1
nκ1−(Ur+λd)√
dr2/bardblX/bardbl∞≥(1−φ)r1
φ(r2/bardblX/bardbl∞√
d)=Lr
Due to Assumption 9, Ur≥(1−φ)nκ1
r2/bardblX/bardbl∞√
dand hence for x≥Lr,
λd−nκ1x
r1+ r2/bardblX/bardbl∞√
dx<−(1−φ)nκ1x
r2/bardblX/bardbl∞√
d.
Then from Remark 2, it follows if /bardblβ−β∗/bardbl>Lr,
/integraldisplay
Ωn/parenleftbigg
pβ
pβ∗/parenrightbiggα
p0dy≤exp/braceleftbigg
αλd/bardblβ−β∗/bardbl−nκ1α/bardblβ−β∗/bardbl2
r1+ r2/bardblX/bardbl∞√
d/bardblβ−β∗/bardbl/bracerightbigg
13Under review as submission to TMLR
≤exp/braceleftbigg
−nκ1α(1−φ)/bardblβ−β∗/bardbl
r2/bardblX/bardbl∞√
d/bracerightbigg
.
By Theorem 6.1 of Kleijn & van der Vaart (2006), there exists test functions Φn,β1such that for every n≥1
sup
pβ∈conv{B(β1;r/2)}E0Φn,β1+Eqβ(1−Φn,β1)≤ sup
pβ∈conv{B(β1;r/2)}/integraldisplay
Ωn/parenleftbigg
pβ
pβ∗/parenrightbiggα
p0dy
≤exp/braceleftbigg
−nκ1α(1−φ)/bardblβ1−β∗/bardbl
2r2/bardblX/bardbl∞√
d/bracerightbigg
.
D.2 Proof of Lemma 1
The proof follows along the lines of Lemma 11 of Atchadé (2017). We have,
/integraldisplay
pβ(y)
pβ∗(y)π(β)dβ=cd
κ/integraldisplay
pβ(y)
p0(y)e−κ/summationtextd
j=1h(βj)dβ
≥cd
κ/integraldisplay
β:/bardblβ−β∗/bardbl≤/epsilon1ne/braceleftbig/summationtextn
i=1(yi−Eyi)xT
i(β−β∗)−Kn/bardblβ−β∗/bardbl2/2−κ/summationtextd
j=1h(βj)/bracerightbig
dβ.
Substituting z=β−β∗, we obtain
/integraldisplay
pβ(y)
pβ∗(y)π(β)≥cd
κ/integraldisplay
/bardblz/bardbl≤/epsilon1ne/braceleftbig/summationtextn
i=1(yi−Eyi)xT
iz−Kn/bardblz/bardbl2/2−κ/summationtextd
j=1h(zj+β∗
j)/bracerightbig
dz
≥cd
κe−κ/summationtextd
j=1{h(β∗
j)+D})/integraldisplay
/bardblz/bardbl≤/epsilon1ne/braceleftbig/summationtextn
i=1(yi−Eyi)xT
iz−Kn/bardblz/bardbl2/2−κD/summationtextd
j=1|zj|/bracerightbig
dz
≥cd
κe−κ/summationtextd
j=1{h(β∗
j)+D}/integraldisplay
/bardblz/bardbl≤/epsilon1ne−Kn/bardblz/bardbl2/2−κD/summationtextd
j=1|zj|dz (18)
×exp/integraldisplay
/bardblz/bardbl≤/epsilon1n/braceleftbign/summationdisplay
i=1(yi−Eyi)xT
iz/bracerightbig
˜π(z)dz, (19)
where the second last inequality follows from the fact that h(zj+β∗
j)≤h(β∗
j)+D|zj|+D. The last inequality follows
from an application of Jensen’s where the expectation is taken with respect to ˜πgiven by
˜π(z) =e−Kn/bardblz/bardbl2/2−κD/summationtextd
j=1|zj|
/integraltext
/bardblz/bardbl≤/epsilon1ne−Kn/bardblz/bardbl2/2−κD/summationtextd
j=1|zj|dz1/bardblz/bardbl≤/epsilon1n.
Noting that the integrand in (19) is an odd function, we have obtained the ﬁnal result.
14