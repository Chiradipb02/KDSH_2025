Published in Transactions on Machine Learning Research (01/2023)
Proportional Fairness in Federated Learning
Guojun Zhang, Saber Malekmohammadi, Xi Chen
{guojun.zhang, saber.malekmohammadi, xi.chen4}@huawei.com
Huawei Noah’s Ark Lab
Yaoliang Yu
yaoliang.yu@uwaterloo.ca
University of Waterloo
Reviewed on OpenReview: https: // openreview. net/ forum? id= ryUHgEdWCQ
Abstract
With the increasingly broad deployment of federated learning (FL) systems in the real
world, it is critical but challenging to ensure fairness in FL, i.e. reasonably satisfactory
performances for each of the numerous diverse clients. In this work, we introduce and study
a new fairness notion in FL, called proportional fairness (PF), which is based on the relative
change of each client’s performance. From its connection with the bargaining games, we
proposePropFair , a novel and easy-to-implement algorithm for finding proportionally fair
solutions in FL, and study its convergence properties. Through extensive experiments on
vision and language datasets, we demonstrate that PropFair can approximately find PF
solutions, and it achieves a good balance between the average performances of all clients
and of the worst 10% clients. Our code is available at https://github.com/huawei-noah/
Federated-Learning/tree/main/FairFL .
1 Introduction
Federated learning (FL, McMahan et al. 2017) has attracted an intensive amount of attention in recent years,
due to its great potential in real world applications such as IoT devices (Imteaj et al., 2021), healthcare (Xu
et al., 2021) and finance (Long et al., 2020). In FL, different clients collaboratively learn a global model that
presumably benefits all, without sharing the local data.
However, clients differ. Due to the heterogeneity of client objectives and resources, the benefit each client
receives may vary. How can we make sure each client is treated fairlyin FL?
To answer this question, we first need to define what we mean by fairness. Similar to fairness in other fields
(Jainetal.,1984;Sen,1986;Rawls,1999;Barocasetal.,2017), inFL,thereisnounifieddefinitionoffairness.
In social choice theory, two of the most popular definitions are utilitarianism andegalitarianism . The goal
of utilitarian fairness is to maximize the utility of the total society; while egalitarian fairness requires the
worst-off people to receive enough benefits. Coincidentally, they correspond to two of the fair FL algorithms:
Federated Averaging (FedAvg, McMahan et al. 2017) and Agnostic Federated Learning (AFL, Mohri et al.
2019). In FedAvg (AFL), we minimize the averaged (worst-case) loss function, respectively. Utilitarian and
egalitarian might be in conflict with each other: one could improve the worst-case clients, but better-off
clients would be degraded to a large extent.
To achieve some balance between utilitarian and egalitarian fairness, other notions of fairness have been
studied. Inspired by α-fairness from telecommunication (Mo & Walrand, 2000), Li et al. (2020c) proposed
q-Fair Federated Learning ( q-FFL). By replacing the client weights with the softmax function of the client
losses, Li et al. (2020a) proposed Tilted Empirical Risk Minimization (TERM). However, it remains vague
what type of balance these algorithms are trying to yield.
1Published in Transactions on Machine Learning Research (01/2023)
In this work, we bring another fairness notion into the zoo of fair FL, called proportional fairness (PF, Kelly
1997). It also balances between utilitarian and egalitarian fairness, but is more intuitive. As a illustrative
example, suppose we only have two clients and if we can improve the performance of one client relatively
by 2% while decreasing another one by 1%, then the solution is more proportionally fair. In practice, this
view of relative change is quotidian. In stock market, people care more about how much they gain/lose
compared to the cost; in telecommunication, people worry about the data transmission speed compared to
the bandwidth. In a word, PF studies the relative change of each client, rather than the absolute change .
Under convexity, PF is equivalent to the Nash bargaining solution (NBS, Nash 1950), a well-known concept
from cooperative game theory. Based on the notion of PF and its related NBS, we propose a new FL
algorithm called PropFair . Our contributions are the following:
•With the utilityperspective and Nash bargaining solutions, we propose a surrogate loss for achieving
proportionally fair FL. This provides new insights to fair FL and is distinct from existing literature
which uses the lossperspective for fairness (see Section 2).
•Theoretical guarantee: we prove the convergence of PropFair to a stationary point of our objec-
tive, under mild assumptions. This proof can generalize to any other FL algorithm in the unified
framework we propose.
•Empirical viability: we test our algorithm on several popular vision and language datasets, and
modern neural architectures. Our results show that PropFair not only approximately obtains pro-
portionally fair FL solutions, but also attains more favorable balance between the averaged and
worst-case performances.
•Compared to previous works (Mohri et al., 2019; Li et al., 2020c; 2021), we provide a comprehensive
benchmark for popular fair FL algorithms with systematic hyperparameter tuning. This could
facilitate future fairness research in FL.
Note that we mainly focus on fairness in federated learning . Perhaps more widely known and orthogonal to
fair FL, fairness has also been studied in general machine learning (Appendix F.3.3), such as demographic
parity (Dwork et al., 2012), equalized odds (Hardt et al., 2016) and calibration (Gebel, 2009). These
definitions require knowledge of sensitive attributes and true labels. Although it is possible to adapt these
fairness definitions into FL, by e.g., treating each sensitive attribute as a client, the adaptation may not
always be straightforward due to the unique challenge of privacy in FL. Such adaptation can be interesting
future work and we do not consider it here.
Notations. We useθto denote the model parameters, and ℓ(θ,(x,y))to represent the prediction loss
ofθon the sample (x,y).ℓSdenotes the average prediction loss on batch S. For each client i, the data
distribution isDiand the expected loss of θonDiisfi. We denote f= (f1,...,fn)withnthe number
of clients, and use pias the linear weight of client i. Usually, we choose pi=ni/Nwithnithe number of
samples of client i, andNthe total number of samples across all clients. We use Φ :Rn→Rto denote the
scalarization of fandφ:R→Rfor some scalar function that operates on each fi. We denote λ∈Rnas
the dual parameter, and Aφas Kolmogorov’s generalized mean. The utilities of each client iisui∈Rwhose
exact definition depends on the context, and u= (u1,...,un)denotes the vector all client utilities. A more
complete notation table can be found in Appendix A.
2 A Unified Framework of Fair FL Algorithms
Suppose we have nclients, and a model parameterized by θ. Because of data heterogeneity, for each client
ithe data distribution Diis different. The corresponding loss function becomes:
fi(θ) :=E(x,y)∼Di[ℓ(θ,(x,y))], (2.1)
whereℓis the prediction loss (such as cross entropy) of model θfor each sample. The goal of FL is
essentially to learn a model θthat every element in the vector f= (f1,...,fn)is small, a.k.a. multi-
objective optimization (MOO, Jahn et al. 2009). Hu et al. (2022) took this approach and used Multiple
Gradient Descent Algorithm (MGDA) to find Pareto stationary points.
2Published in Transactions on Machine Learning Research (01/2023)
Another popular approach to MOO is scalarization of f(Chapter 5, Jahn et al., 2009), by changing the
vector optimization to some scalar optimization: minθ(Φ◦f)(θ)with Φ :Rn→R. In this work, we mainly
focus on Φbeing a (additively) separable function:
min
θ/summationdisplay
ipi(φ◦fi)(θ), φ :R→R. (2.2)
The linear weights pi’s are usually pre-defined and satisfy pi≥0,/summationtext
ipi= 1. In FL, a usual choice of pi
ispi=ni/Nwithnithe number of samples for client iandNthe total number of samples. Here φis a
monotonically increasing function, since if any fiincreases, the total loss should also increase.
In order to properly locate proportional fairness in the fairness literature, we first review existing fairness
definitions that have been applied to FL. In the following subsections, we show that different choices of scalar
functionφlead to different fair FL algorithms with their respective fairness principles.
2.1 Utilitarianism
The simplest choice of φwould be the identity function, φ(fi) =fi:
min
θF(θ) :=/summationdisplay
ipifi(θ),withpi≥0pre-defined,and/summationdisplay
ipi= 1. (2.3)
This corresponds to the first FL algorithm, Federated Averaging (FedAvg, McMahan et al. 2017). Combined
with eq. 2.1, the objective eq. 2.3 is equivalent to centralized training with all the client samples in one place.
Fromafairnessperspective, eq.2.3canbecalled utilitarianism , whichcanbetracedbacktoatleastBentham
(1780). From a utilitarian perspective, a solution θis fair if it maximizes an average of the client utilities.
(Here we treat client i’s utilityuias−fi. In general, ui∈Ris some value client iwishes to maximize.)
2.2 Egalitarianism (Maximin Criterion)
In contrast to FedAvg, Agnostic Federated Learning (AFL, Mohri et al. 2019) does not assume a pre-defined
weight for each client, but aims to minimize the worst convex combination:
min
θmax
p/summationdisplay
ipifi(θ),with 1⊤p= 1,andp≥0. (2.4)
Note thatp∈Rnis a vector on the probability simplex. An equivalent formulation is:
min
θmax
ifi(θ). (2.5)
Inotherwords, weminimizetheworst-caseclientloss. Insocialchoice, thiscorrespondstotheegalitarianrule
(or more specifically, the maximin criterion, see Rawls 1974). In MOO, this corresponds to Φ(f) = maxifi
(above eq. 2.2). There is one important caveat of AFL worth mentioning: the generalization. In practice,
each client loss fiis in fact the expected loss on the empirical distribution /hatwideDi, i.e.,
/hatwidefi(θ) =E(x,y)∼/hatwideDi[ℓ(θ,(x,y))]. (2.6)
In FL, some clients may have few samples and the empirical estimate /hatwidefimay not faithfully reflect the
underlying distribution. If such a client happens to be the worst-case client, then AFL would suffer from
defective generalization. We provide a concrete example in Appendix C, and this phenomenon has also been
observed in our experiments.
2.3α-Fairness
Last but not least, we may slightly modify the function φin FedAvg to be φ(fi) =fq+1
i/(q+ 1):
min
θ1
q+ 1/summationdisplay
ipifq+1
i(θ),withpi≥0pre-defined,and/summationdisplay
ipi= 1. (2.7)
3Published in Transactions on Machine Learning Research (01/2023)
This is called q-Fair Federated Learning ( q-FFL, Li et al. 2020c), and q≥0is required. If q= 0, then
we retrieve FedAvg; if q→∞, then the client who has the largest loss fiwill be emphasized more, which
corresponds to AFL. In general, q-FFL interpolates between the two. From a fairness perspective, q-FFL
can relate to α-fairness (Mo & Walrand, 2000), a popular concept from the field of communication. Suppose
each client has utility ui∈Randu= (u1,...,un)∈U⊆ Rn, withUthe feasible set of client utilities, then
α-fairness associates with the following problem:
max
u∈U/summationdisplay
ipiϕα(ui),with pre-defined pi≥0andϕα(ui) =/braceleftigg
logui ifα= 1,
u1−α
i/(1−α)ifα>0andα̸= 1.(2.8)
q-FFL modifies the α-fairness with two changes: (1) take α=−q, and allow α≤0; (2) replace uiwith the
lossfi. Therefore, q-FFL is an analogy of α-fairness. However, the objective eq. 2.7 misses the important
case withα= 1, also known as proportional fairness (PF, Kelly et al. 1998), which we will study in § 3. Note
that the formulation eq. 2.7 is not fit for studying PF, since if we take q→− 1(corresponding to α= 1),
then we obtain/summationtext
ipilogfi, which need not be convex even when each fiis (see also § 3.1.1).
2.4 Dual View of Fair FL Algorithms
In this subsection, we show that many existing fair FL algorithms can be treated in a surprisingly unified
way. In fact, eq. 2.2 is equivalent to minimizing the Kolmogorov’s generalized mean (Kolmogorov, 1930):
Aφ(f(θ)) :=φ−1/parenleftiggn/summationdisplay
i=1piφ(fi(θ))/parenrightigg
. (2.9)
Examples include φ(fi) =fi(FedAvg),φ(fi) =fq+1
i(q-FFL,q≥0) andφ(fi) = exp(αfi)(α≥0). The
last choice is known as Tilted Empirical Risk Minimization (TERM, Li et al. 2020a).
We can now supply a dual view of the aforementioned FL algorithms that is perhaps more revealing. Con-
cretely, let φbe (strictly) increasing, convex and thrice differentiable. Then, the generalized mean function
Aφis convex iff−φ′/φ′′is convex (Theorem 1, Ben-Tal & Teboulle, 1986). Applying the convex conjugate
ofAφwe obtain the equivalent problem:
min
θAφ(f(θ))≡min
θmax
λ≥0/summationdisplay
iλifi(θ)−A∗
φ(λ),A∗
φ(λ) := sup
fλ⊤f−Aφ(f), (2.10)
where A∗
φ(λ)is theconvex conjugate ofAφ. Note that f≥0and thus we require λ≥0. Under strong
duality, we may find the optimal dual variable λ∗, with which our fair FL algorithms are essentially FedAvg
with the fine-tuned weighting vector λ∗.
Constraints of λ.Solving the convex conjugate A∗
φoften gives additional constraints on λ. For example,
for FedAvg we can find that A∗
φ(λ) = 0ifλi=pifor alli∈[n]andA∗
φ(λ) =∞otherwise. For φ(fi) =fq+1
i,
we obtain the conjugate function corresponding to q-FFL:
A∗
φ(λ) = 0,ifλ≥0and/summationdisplay
ip−1/q
iλ(q+1)/q
i≤1,and∞otherwise. (2.11)
Bringing eq. 2.11 into eq. 2.10 and using Hölder’s inequality we obtain the maximizer λi∝pifq
i. Similarly,
we can derive the convex conjugate of TERM (Li et al., 2020a) as:
A∗
φ(λ) =/summationdisplay
iλi
αlogλi
piifλ≥0,λ⊤1= 1,and∞otherwise. (2.12)
The maximizer is achieved at λi∝pieαfi. In other words, TERM gives a higher weight to clients with worse
losses. Detailed derivations of the convex conjugates can be found in Appendix E.
In Table 1, we summarize all the algorithms we have discussed, including their motivating principles, objec-
tives as well as the constraints of λinduced by A∗
φ. Although the fair FL algorithms are motivated from
different principles, most of them achieve a balance between utilitarianism and egalitarianism, thus allowing
us to compare them on the same ground (§ 5).
4Published in Transactions on Machine Learning Research (01/2023)
Table 1: Different fairness concepts and their corresponding FL algorithms. fiis the loss function for the
ithclient. The requirement of λcan be found in § 2.4 and § 3.2. We defer the description and the dual view
of PropFair to § 3.
FL algorithm Principle Objective Constraints of λ
FedAvg Utilitarian/summationtext
ipifi λi=pi
AFL Egalitarian maxifi λ≥0,1⊤λ≤1
q-FFL α-fairness/summationtext
ipifq+1
i λi∝pifq
i,λ≥0
TERM n/a/summationtext
ipieαfiλi∝pieαfi,λ≥0,1⊤λ= 1
PropFair Proportional −/summationtext
ipilog(M−fi)λi∝pi
M−fi,/producttext
i(λi/pi)pi= 1
3 Adapting Proportional Fairness to FL
Now we study how to add the missing piece mentioned in Section 2.3 to FL: proportional fairness. From a
utility perspective, eq. 2.8 with α= 1reduces to:
max
u∈U/summationdisplay
ipilogui,withpi≥0pre-defined,and/summationdisplay
ipi= 1. (3.1)
Note that we now specify the domain of uto beU⊆Rn
++. The objective in eq. 3.1 is sometimes known as
theNash product (up to logarithmic transformation), and the maximizer u∗is also called the Nash bargaining
solution (NBS, Nash 1950). Axiomatic characterizations of the Nash bargaining solution are well-known, for
instance by the following four axioms: Pareto optimality, symmetry, scale equivariance and monotonicity
(e.g., Maschler et al., 2020, Theorem 16.35). Moreover, Figure 1 gives an illustration of the NBS. Among all
the solutions that maximize the total utility, the Nash bargaining solution achieves equal utility for the two
players, and the largest worst-case utility.
The first-order optimality condition (Bertsekas, 1997) of eq. 3.1 can be written as:
⟨u−u∗,∇n/summationdisplay
i=1pilogu∗
i⟩≤0,for anyu∈U, (3.2)
resulting in the following definition of proportional fairness (Kelly et al., 1998):
u∗∈Uis proportionally fair if/summationdisplay
ipiui−u∗
i
u∗
i≤0,for anyu∈U. (3.3)
Intuitively, (ui−u∗
i)/u∗
iis the relative utility gain for player igiven its utility switched from u∗
itoui. PF
simply states that at the solution u∗, the average relative utility cannot be improved. For instance, for two
players with p1=p2= 1/2we have:
u1−u∗
1
u∗
1≤−u2−u∗
2
u∗
2, (3.4)
which says that if by deviating from the optimal solution (u∗
1,u∗
2), player 2could gainppercentage more in
terms of utility, then player 1will have to lose a percentage at least as large as p.
The Nash bargaining solution is equivalent to the PF solution according to the following proposition:
Proposition 3.1 (equivalence , e.g. Kelly 1997; Boche & Schubert 2009) .For any convex set U∈Rn
++,
a pointu∈Uis the Nash bargaining solution iff it is proportionally fair. If Uis non-convex, then a PF
solution, when exists, is a Nash bargaining solution.
A PF solution, whenever exists, is a Nash bargaining solution over U. While the converse also holds if Uis
convex, for nonconvex U, PF solutions may not exist. In contrast, NBS always exists if Uis compact, and
5Published in Transactions on Machine Learning Research (01/2023)
Figure 1: Figure inspired by Nash (1950). U: the feasible set of utilities. Blue line: maximizers of the total
utility, on which the Nash bargaining solution u∗stands out as the fairest.
thus we solve eq. 3.1 as a necessary condition of PF. From Jensen’s inequality, we can show that:
/summationdisplay
ipilogui≤log/summationdisplay
ipiui. (3.5)
In other words, solving the NBS yields a lower bound of the averaged utility. On the other hand, if any of
the utilities is close to zero, then the left hand side of eq. 3.5 would decrease to −∞. Therefore, the NBS
does not yield extremely undesirable performance for any client. In a nutshell, the NBS achieves a balance
between maximizing the average and the worst-case utilities.
3.1 The PropFair algorithm for federated learning
In order to realize proportional fairness in FL, we need to solve eq. 3.1. With parametrization of ui, the
utility setUbecomes the set of all possible choices of (u1(θ),...,un(θ)), and our goal is to find a global
modelθto solve eq. 3.1:
max
θ/summationdisplay
ipilogui(θ). (3.6)
3.1.1 What is the right choice of utilities?
One immediate question is: how do we define these utilities in FL? Ideally, the utility should be the test
accuracy, which is unfortunately not amenable to optimize. Instead, we could use the training loss fi. There
are a few alternatives:
•Replaceuiwithfias done inq-FFL, and minimize the aggregate loss,/summationtext
ipilogfi;
•Replaceuiwithfias done inq-FFL, and maximize the aggregate utility,/summationtext
ipilogfi;
•Chooseui=M−fi,andmaximize/summationtext
ipilog(M−fi),withMsomehyperparametertobedetermined.
The first approach will encourage the client losses to be even more disparate. For instance, suppose p1=
p2=1
2, and then (f1,f2) = (1
3,2
3)has smaller product than (f1,f2) = (1
2,1
2). The second approach is not
a choice either as it is at odds with minimizing client losses. Therefore, we are left with the third option.
By contrast, for any M≥1andp1=p2= 1/2, one can show that (f1,f2) = (1
2,1
2)always gives a better
solution than (f1,f2) = (1
3,2
3). The resulting objective becomes:
min
θπ(θ) :=−/summationdisplay
ipilog(M−fi(θ)). (3.7)
3.1.2 Huberization
However, the objective eq. 3.7 also raises issues: what if M−fiis small and blows up the gradient, or
even worse, what if M−fiis negative and the logarithm does not make sense at all? Inspired by Huber’s
6Published in Transactions on Machine Learning Research (01/2023)
Algorithm 1: PropFair
1Input:global epoch T, client number n, loss function fifor clienti, number of samples nifor clienti,
initial global model θ0, local step number Ki, baselineM, threshold ϵ,pi=ni/N, batch size m,
learning rate η
2fortin0,1...T−1do
3randomly selectCt⊆[n]
4θ(i)
t,0=θtfori∈Ct,N=/summationtext
i∈Ctni
5 foriinCtdo// in parallel
6j= 1, drawKimini-batches of samples from client i
7 forSiintheKibatches do
8 ℓSi(θ) =1
|Si|/summationtext
(x,y)∈Siℓ(θ,(x,y))
9 flog
i(θ) =−log[ϵ](M−ℓSi(θ))
10 θ(i)
t,j←θ(i)
t,j−1−η∇flog
i(θ(i)
t,j−1),j←j+ 1
11θt+1=/summationtext
i∈Stpiθ(i)
t,Ki
12Output: global model θT
approach of robust estimation (Huber, 1964), we propose a “huberized” version of eq. 3.7:
min
θ−/summationdisplay
ipilog[ϵ](M−fi(θ)),with log[ϵ](M−t) :=/braceleftigg
log(M−t),ift≤M−ϵ,
logϵ−1
ϵ(t−M+ϵ),ift>M−ϵ.(3.8)
Essentially, log[ϵ](M−t)is a robustC1extension of log(M−t)from [0,M−ϵ]toR+: its linear part ensures
that att=M−ϵ, both the value and the derivative are continuous. If any fiis close or greater than M,
then eq. 3.8 switches from logarithm to its linear version. Based on eq. 3.8 we propose Algorithm 1 called
PropFair . It modifies FedAvg (McMahan et al., 2017) with a simple drop-in replacement, by replacing the
loss of each batch ℓi
Swith log[ϵ](M−ℓi
S(θ)). This allows easy adaptation of PropFair with minimal change
into any of the current FL platforms, such as Flower (Beutel et al., 2020) and Tensorflow Federated.1Also
note that in Algorithm 1 we average over the batch before the composition with log[ϵ]. This order cannot be
switched since otherwise the local variance will be mtimes larger (see eq. B.52).
Remark. WhenM→∞andfi(θ)is small compared to M, the loss function for client ibecomes:
flog
i(θ) =−log(M−fi(θ))≈−logM+fi(θ)
M.
Thus, FedAvg can be regarded as a first-order approximation of PropFair. We utilize this approximation in
our implementation. Another way to obtain FedAvg is to take ϵ=Mand thus log[ϵ](M−t)always uses the
linear branch. In contrast, if ϵ→0, then eq. 3.8 becomes more similar to eq. 3.7.
3.2 Dual view of PropFair
With the dual view from Section 2.4, we can also treat PropFair as minimizing a weighted combination of
loss functions (plus constants), similar to other fair FL algorithms. Note that if φ(fi) =−log(M−fi)in
eq. 2.9, then we have PropFair (see Table 1):
Proposition 3.2 (dual view of PropFair ).The generalized mean eq. 2.9 for PropFair can be written as:
Aφ(f) = max
λ≥0,/producttext
i(λi/pi)pi≥1λ⊤f−M(λ⊤1−1), (3.9)
Solving the inner maximization of eq. 2.10 gives/producttextn
i=1/parenleftig
λi
pi/parenrightigpi
= 1andλi∝pi
M−fi.
Similar to TERM/ q-FFL, PropFair puts a larger weight on worse-off clients with a larger loss.
1https://www.tensorflow.org/federated
7Published in Transactions on Machine Learning Research (01/2023)
4 The optimization side of PropFair
In this section, we discuss the convexity of our PropFair objective and show the convergence guarantee of
Algorithm 1. This gives formal fairness guarantee for our algorithm, and potentially for the convergence of
many others in the scalarization class, eq. 2.2. For simplicity we only study the case when fi≤M−ϵfor
alli.
4.1 Convexity of the PropFair objective
Convexity is an important and desirable property in optimization (Boyd & Vandenberghe, 2004). With
convexity, every stationary point is optimal (Bertsekas, 1997). From the composition rule, if fiis convex
for each client i, thenM−fiis concave, and thus/summationtext
ipilog(M−fi(θ))is concave as well (e.g., Boyd &
Vandenberghe, 2004). In other words, for convex losses, our optimization problem eq. 3.7 is still convex as
we are maximizing over concave functions. Moreover, our PropFair objective is convex evenwhenfi’s are
not. For example, this could happen if fi(θ) =M−exp(θ⊤Aiθ)and each Aiis a positive definite matrix.
In fact, it suffices to require each M−fito be log-concave (Boyd & Vandenberghe, 2004).
4.2 Adaptive learning rate and curvature
Denoteφ(t) =−log(M−t). We can compute the 1st- and 2nd-order derivatives of φ◦fi:
∇(φ◦fi) =∇fi
M−fi,∇2(φ◦fi) =(M−fi)∇2fi+ (∇fi)(∇fi)⊤
(M−fi)2. (4.1)
This equation tells us that at each local gradient step, the gradient ∇(φ◦fi)has the same direction as
∇fi, and the only difference is the step size. Compared to FedAvg, PropFair automatically has an adaptive
learning rate for each client. When the local client loss function fiis small, the learning rate is smaller; when
fiis large, the learning rate is larger. This agrees with our intuition that to achieve fairness, a worse-off
client should be allowed to take a more aggressive step, while a better-off client moves more slowly to “wait
for” other clients.
In the Hessian∇2(φ◦fi), an additional positive semi-definite (p.s.d.) term (∇fi)(∇fi)⊤is added. Thus,
∇2(φ◦fi)can be p.s.d. even if the original Hessian ∇2fiis not. Moreover, the denominator (M−fi)2has
a similar effect of coordinating the curvatures of various clients as in the gradients.
4.3 Convergence results
Let us now formally prove the convergence of PropFair by bounding its progress, using standard assumptions
(Lietal.,2019;Reddietal.,2020)suchasLipschitzsmoothnessandboundedvariance. Everynormdiscussed
in this subsection is Euclidean (including the proofs in Appendix B).
In fact, PropFair can be treated as an easy variant of FedAvg, with the local objective fireplaced with flog
i.
Therefore, we just need to prove the convergence of FedAvg and the convergence of PropFair would follow
similarly. In general, similar results also hold for objectives in the form of eq. 2.2.
Let us state the assumptions first. Since in practice we use stochastic gradient descent (SGD) for training,
we consider the effect of mini-batches. We also assume that the (local) variance of mini-batches and the
(global) variance among clients are bounded.
Assumption 4.1 (Lipschitz smoothness and bounded variances ).Each function fiisL-Lipschitz
smooth, i.e., for any θ,θ′∈Rdand anyi∈[n], we have∥∇fi(θ)−∇fi(θ′)∥≤L∥θ−θ′∥; For anyi,j∈[n],
fi−fjisσ-Lipschitz continuous and E(x,y)∼Di∥∇ℓ(θ,(x,y))−∇fi(θ)∥2≤σ2
i∀θ∈Rd.
Following the notations of Reddi et al. (2020), we use σ2andσ2
ito denote the global and local variances for
clienti. This assumption allows us to obtain the convergence result for FedAvg (see Algorithm 2). For easy
reference, we include FedAvg (McMahan et al., 2017) in Algorithm 2, whose goal is to optimize the overall
performance. At each round, each client takes local SGD steps to minimize the loss function based on the
client data. Afterwards, the server computes a weighted average of the parameters of these participating
clients, and shares this average among them. Note that for client i, the number of local steps is Kiwith
8Published in Transactions on Machine Learning Research (01/2023)
Algorithm 2: FedAvg
1Input: global epoch T, client number n, loss function fi, number of samples nifor clienti, initial
global model θ0, local step number Kifor clienti, batch size m, learning rate η,pi=ni/N
2fortin0,1...T−1do
3randomly selectCt⊆[n]
4θ(i)
t=θtfori∈Ct,N=/summationtext
i∈Ctni
5 foriinCtdo// in parallel
6 starting from θ(i)
t, takeKilocal SGD steps on fito findθ(i)
t+1
7θt+1=/summationtext
i∈Ctpiθ(i)
t+1
8Output: global model θT
learning rate η. In line 3 of Algorithm 2, if Ct= [n]then we call it full participation , otherwise it is called
partial participation . We prove the following convergence result of FedAvg. Note that we defined Fin eq. 2.3,
andmis the batch size.
Theorem 4.2 (FedAvg).Given Assumption 4.1, assume that the local learning rate satisfies ηKi≤1
6Lfor
anyi∈[n]and
η≤1
L/radicaligg
1
24(e−2)(/summationtext
ip2
i)(/summationtext
iK4
i). (4.2)
Running Algorithm 2 for Tglobal epochs we have:
min
0≤t≤T−1E∥∇F(θt)∥2≤12
(11µ−9)η/parenleftbiggF0−F∗
T+ Ψσ/parenrightbigg
,
withµ=/summationtext
ipiKifor full participation and µ= miniKifor partial participation, F0=F(θ0),F∗=
minθF(θ)the optimal value, and
Ψσ=η∥p∥2/bracketleftiggn/summationdisplay
i=1K2
i/parenleftbiggLησ2
i
2m+σ2/parenrightbigg
+ (e−2)η2L2n/summationdisplay
i=1K3
i/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg/bracketrightigg
,p= (p1,...,pn).
Our result is quite general: we allow for both full and partial participation; multiple and heterogeneous local
steps; and non-uniform aggregation with weight pi. The variance term Ψσdecreases with smaller local steps
Ki, which agrees with our intuition that each Kishould be as small as possible given the communication
constraint. Moreover, to minimize ∥p∥2we should take pi= 1/nfor each client i, which means if the samples
are more evenly distributed across clients, the error is smaller. In presence of convexity, we can see that
FedAvg converges to a neighborhood of the optimal solution, and the size of the neighborhood is controlled
by the heterogeneity of clients and the variance of mini-batches. When we have the global variance term
σ= 0, our result reduces to the standard result of stochastic gradient descent (e.g., Ghadimi & Lan 2013),
since we have
min
0≤t≤T−1E∥∇F(θt)∥2≤12
(11µ−9)ηF0−F∗
T+O(η),
and by taking η=O(1/√
T), we obtain min 0≤t≤T−1E∥∇F(θt)∥2=O(1/√
T).
We note that we are not the first to prove the convergence of FedAvg. For instance, Li et al. (2019) assumes
that each function fiis strongly convex and each client takes the same number of local steps; Karimireddy
et al. (2020) assumes the same number of local steps, gradient bounded similarity and uniform weights
pi= 1/n. These assumptions may not reflect the practical use of FedAvg. For example, usually each client
has a different number of samples and they may take different numbers of local updates. Moreover, for
neural networks, (global) strong convexity is usually not present. Compared to these results, we consider
9Published in Transactions on Machine Learning Research (01/2023)
different local client steps, heterogeneous weights and partial participation in the non-convex case, which is
more realistic.
Based onTheorem 4.2, we can similarlyprovethe convergence ofother FL algorithms which minimizeeq. 2.2,
if there are some additional assumptions. For the PropFair algorithm, as an example, we need to additionally
assume the Lipschitzness and bounded variances for the client losses:
Assumption 4.3 (boundedness, Lipschitz continuity and bounded variances for client losses ).
For anyi∈[n],θ∈Rdand any batch Si∼Dm
iofmi.i.d. samples, we have:
0≤ℓSi(θ) :=1
|Si|/summationdisplay
(x,y)∈Siℓ(θ,(x,y))≤M
2,
and for anyθ,θ′∈Rd,∥fi(θ)−fi(θ′)∥≤L0∥θ−θ′∥holds. We also assume that for any i,j∈[n]and
θ∈Rd,∥fi(θ)−fj(θ)∥2≤σ2
0andE(x,y)∼Di∥ℓ(θ,(x,y))−fi(θ)∥2≤σ2
0,ihold.
we can obtain the convergence guarantee of PropFair to a neighborhood of some stationary point:
Theorem 4.4 (PropFair ).Denote/tildewideL=4
M2(3
2ML+L2
0)andpi=ni
N. Given Assumptions 4.1 and 4.3,
assume that the local learning rate satisfies:
η≤min/braceleftigg
min
i∈[n]1
6/tildewideLKi,1
8˜L/radicaligg
1
(e−2)(/summationtext
ip2
i)(/summationtext
iK4
i)/bracerightigg
. (4.3)
By running Algorithm 1 for Tglobal epochs we have:
min
0≤t≤T−1E∥∇π(θt)∥2≤12
(11µ−9)η/parenleftbiggπ0−π∗
T+/tildewideΨσ/parenrightbigg
,
withµ=/summationtext
ipiKifor full participation and µ= miniKifor partial participation, π0=π(θ0),π∗= minθπ(θ)
the optimal value, and
/tildewideΨσ=η∥p∥2/bracketleftbiggn/summationdisplay
i=1K2
i/parenleftbigg/tildewideσ2
i
m+ 2/tildewideσ2/parenrightbigg
+ 16(e−2)η2˜L2n/summationdisplay
i=1K4
i/parenleftbigg/tildewideσ2
i
m+/tildewideσ2/parenrightbigg/bracketrightbigg
where/tildewideσ2
i=8
M4(9M2σ2
i+ 4L2
0σ2
0,i)and/tildewideσ=4
M/parenleftbig3
2σ+L0
Mσ0/parenrightbig
.
Our Theorem 4.4 inherits similar advantages from Theorem 4.2. One major difference is that when ˜σ= 0,
one cannot retrieve the same rate of SGD. This is expected since each batch φ◦ℓSiis no longer an unbiased
estimatorφ◦fidue to the composition. Nevertheless, due to data heterogeneity in FL, the global variance
˜σis often large, in which case the local variance term ˜σ2
i/min/tildewideΨσcan be comparable to ˜σ2by controlling
the batch size m.
5 Experiments
In this section, we verify properties of PropFair by answering the following questions: (1) can PropFair
achieve proportional fairness as in eq. 3.3? (2) what balance does PropFair achieve between the average and
worst-case performances? We report them separately in Section 5.2 and Section 5.3.
5.1 Experimental setup
We first give details on our datasets, models and hyperparameters, which are in accordance with existing
works. See Appendix D for additional experimental setup. A comprehensive survey of benchmarking FL
algorithms can be found in e.g. Caldas et al. (2018); He et al. (2020).
Datasets. We follow standard benchmark datasets as in the existing literature, including CIFAR-{10,
100} (Krizhevsky et al., 2009), TinyImageNet (Le & Yang, 2015) and Shakespeare (McMahan et al., 2017).
10Published in Transactions on Machine Learning Research (01/2023)
Figure 2: The relative improvement/deterioration (ui−u∗
i)/u∗
iof the test accuracy uiof each client iof
other baseline algorithms compared to PropFair. The dataset is CIFAR-10. The (weighted) average of the
relative changes for FedAvg, AFL, q-FFL and TERM are respectively: −2.21%,−1.32%,−12.95%,−1.79%.
We choose the hyperparameters based on Table 4 in Appendix D.
For vision datasets (CIFAR-{10, 100}/TinyImageNet), the task is image classification, and following Wang
et al. (2019b) we use Dirichlet allocation to split the dataset into different clients. For the language dataset
(Shakespeare), the task is next-character prediction. We use the default realistic partition based on different
users. We first partition the dataset into different clients, and further split each client dataset into its own
training and test sets. This reflects the real scenario, where each client evaluates the performance by itself.
Models, optimizer and loss function. For vision datasets we use ResNet-18 (He et al., 2016) with
Group Normalization (Wu & He, 2018). As discussed by Hsieh et al. (2020), Group Normalization (with
num_groups=2 ) works better than batch normalization, especially in the federated settings. For the Shake-
speare dataset, we use LSTM (Hochreiter & Schmidhuber, 1997). We find the best learning rates through
grid search (see Appendix D).
Other hyperparameters. We implement full participation and one local epoch throughout (with many
local steps for each client). Due to data heterogeneity, the number of local steps Kifor each client ivaries.
For CIFAR-{10, 100} we partition the data into 10 clients; for TinyImageNet/Shakespeare we choose 20
clients.
Evaluation metrics. We validate proportional fairness eq. 3.3 of our PropFair algorithm, where we treat
eachuias the test accuracy of client i. To show that PropFair achieves a proper balance between utilitarian
and egalitarian fairness, we use the average and the worst 10% test accuracies. These are standard fairness
metrics used in the literature (e.g. Li et al., 2020a;c). In Appendix D we also present other standard metrics
such as standard deviation and worst 20%.
5.2 Verification of proportional fairness
In this subsection, we show that PropFair can, to some extent, achieve proportional fairness as defined in
eq. 3.3. We treat uias the test accuracy of client i, and compute
/summationdisplay
ipiui−u∗
i
u∗
i, (5.1)
withpi=ni/Nandu∗:= (u∗
1,...,u∗
n)the test accuracies obtained by the PropFair model. Although we
cannot verify eq. 5.1 for every u, we can at least validate the negativity for some competitive u’s, of, e.g.,
models learned by other fair FL algorithms.
5.2.1 CIFAR-10
We first compute eq. 5.1 where u∗is the test accuracies obtained by PropFair and uis the test accuracies
found by one of the other fair FL algorithms, including FedAvg, AFL, q-FFL and TERM. Figure 2 shows
the relative changes of each client, (ui−u∗
i)/u∗
i, from which we can see that compared to the solution found
by PropFair, for another fair FL solution, most clients are degraded by a large relative amount, and only a
few clients are improved by a small amount.
11Published in Transactions on Machine Learning Research (01/2023)
Figure 3: The relative improvement/deterioration (ui−u∗
i)/u∗
iof the test accuracy of each client i, pretrained
with PropFair and fine-tuned with another baseline. The dataset is CIFAR-100. The average of the relative
changes for FedAvg, AFL, q-FFL and TERM are respectively: −0.86%,+0.05%,−0.67%,−1.01%.
Figure 4: The relative improvement/deterioration (ui−u∗
i)/u∗
iof test accuracy of each client i, pretrained
with another baseline and fine-tuned with PropFair. The dataset is CIFAR-100. The average of the relative
changes over FedAvg, AFL, q-FFL and TERM are respectively: 10.88%,13.93%,11.58%,8.67%.
5.2.2 CIFAR-100
In fact, we may compute eq. 5.1 with a stronger u. For CIFAR-100, we still treat u∗as the test accuracies
obtained by PropFair. The difference is that we use u∗as the initialization, and fine-tune with other fair
FL algorithms, to find u. If other fair FL algorithms cannot improve the proportional fairness of u∗, then
eq. 5.1 should be negative. As we see in Figure 3, this result indeed holds approximately (except the slight
improvement for AFL).
By contrast, none of the baseline fair FL algorithms can achieve the same level of proportional fairness as
our PropFair. In Figure 4, we see that if we start from a model pretrained with a baseline fair FL algorithm,
and fine-tune with our Propfair, most client performances are improved, sometimes by a large margin.
5.3 Comparison between PropFair and existing fair FL algorithms on other metrics
In Figure 5, we compare PropFair with existing fair FL algorithms using the average and the worst 10% test
accuracies across clients, including FedAvg (McMahan et al., 2017), q-FFL (Li et al., 2020c), AFL (Mohri
et al., 2019) and TERM (Li et al., 2020a).
Average performance. From Figure 5 we can see that PropFair does not always yield the best average
performance, e.g., compared to q-FFL on TinyImageNet. This is expected, since maximizing the Nash
product does not necessarily give the best average performance. Nevertheless, PropFair remains competitive.
Somewhat surprisingly, FedAvg does not always achieve the best average performance, which might be due
to optimization issues (Pathak & Wainwright, 2020).
Worst 10% performance. We also compare the worst 10% performance of various fair FL algorithms. We
observe that PropFair achieves the state-of-the-art in terms of the worst 10% performance, across various
12Published in Transactions on Machine Learning Research (01/2023)
Figure5: Meanandworst10%testaccuraciesfordifferentalgorithms. Theaccuraciesareinpercentage. ( top
left): CIFAR-10; ( top right ): CIFAR-100; ( bottom left ): TinyImageNet; ( bottom right ): Shakespeare.
All subfigures share the same legends and axis labels.
vision and language datasets. This is within our expectation, since from eq. 3.5 and eq. 3.7 we can see that
low utility in anyof the clients would result in a small Nash product.
Specifically, although AFL directly maximizes the worst-case loss function, it does not always achieve the
best worst-case performances (see Table 6 in Appendix D), especially for vision datasets. This might be due
to the generalization issue of AFL (see Appendix C).
6 Related Work in Fair FL
We review recent related work for fair federated learning. In additional to AFL (Mohri et al., 2019), q-FFL
(Li et al., 2020c) and TERM (Li et al., 2020a), there have been other approaches for fairness in FL. For
example, FedMGDA+ (Hu et al., 2022) defines fairness as achieving the Pareto frontier and they proposed
to use the MGDA algorithm. As another example, GIFAIR-FL (Yue et al., 2022) encourages the similarity
of the client losses by adding a regularization term of the pairwise ℓ1distances. Last but not least, Ditto
(Li et al., 2021) proposed a personalization approach to obtain fairness and robustness. A comprehensive
recent survey of fairness in FL can be found in Shi et al. (2021), and we have included additional papers of
fairness (in FL and in general) in Appendix F.
7 Conclusions
Basedonthenecessityofconsideringrelativechanges, weintroducetheconceptofProportionalFairness(PF)
into the field of federated learning (FL), which is deeply rooted in cooperative game theory. By showing the
connection between PF and the Nash bargaining solution, we propose PropFair that maximizes the product
of client utilities, where the total relative utility cannot be improved. This guarantees PropFair to have good
worst-case performance without sacrificing the total utility much. We verify proportional fairness and the
balance between utilitarian and egalitarian fairness in our extensive experiments. As we have shown, many
fair FL algorithms, including PropFair, can be unified using Kolmogorov’s generalized mean, the deeper
understanding of which may lead to future design of fair FL algorithms.
13Published in Transactions on Machine Learning Research (01/2023)
Broader Impact Statement
With the wide deployment of federated learning, how to ensure fairness in FL algorithms has become a
major concern. In this work, we study proportional fairness in FL to make FL systems fairer and thus
more trustworthy. This could have important positive social impacts as well. We are not aware of potential
negative societal impacts yet but we welcome discussions on them.
Acknowledgments
We thank the reviewers and the action editor for constructive comments that largely improved our draft. GZ
would like to thank Changjian Shui for his constructive feedback on an earlier draft, and Mahdi Beitollahi
for pointing out typos. YY is supported by NSERC and WHJIL.
References
Charles Audet, Gilles Savard, and Walid Zghal. Multiobjective optimization through a series of single-
objective formulations. SIAM Journal on Optimization , 19(1):188–210, 2008.
Pranjal Awasthi, Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Beyond individual and group
fairness. arXiv preprint arXiv:2008.09490 , 2020.
Sina Baharlouei, Maher Nouiehed, Ahmad Beirami, and Meisam Razaviyayn. Rényi fair inference. In
International Conference on Learning Representations , 2019.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. NIPS tutorial , 2017.
Aharon Ben-Tal and Marc Teboulle. Expected utility, penalty functions, and duality in stochastic nonlinear
programming. Management Science , 32(11):1445–1466, 1986. URL https://doi.org/10.1287/mnsc.32.
11.1445.
Jeremy Bentham. An Introduction to the Principles of Morals and Legislation . For T. Payne and Son, at
the Mews Gate, 1780.
Dimitri Bertsekas and Robert Gallager. Data networks . Athena Scientific, 1987.
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society , 48(3):334–334,
1997.
Dimitris Bertsimas, Vivek F Farias, and Nikolaos Trichakis. The price of fairness. Operations research , 59
(1):17–31, 2011.
Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, Pedro PB de Gusmão,
and Nicholas D Lane. Flower: A friendly federated learning research framework. arXiv preprint
arXiv:2007.14390 , 2020.
Holger Boche and Martin Schubert. Nash Bargaining and proportional fairness for wireless systems.
IEEE/ACM Transactions on Networking , 17(5):1453–1466, October 2009.
Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečn` y, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. LEAF: A benchmark for federated settings. arXiv preprint
arXiv:1812.01097 , 2018.
Mina Dashti, Paeiz Azmi, and Keivan Navaie. Harmonic mean rate fairness for cognitive radio networks
withheterogeneoustraffic. Transactions on Emerging Telecommunications Technologies , 24(2), 2013. ISSN
2161-3915. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.2541 .
14Published in Transactions on Machine Learning Research (01/2023)
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference , ITCS ’12,
pp. 214–226, New York, NY, USA, January 2012. Association for Computing Machinery. ISBN 978-1-
4503-1115-1. doi: 10.1145/2090236.2090255. URL https://doi.org/10.1145/2090236.2090255 .
Martin Gebel. Multivariate calibration of classifier scores into the probability space . PhD thesis, Citeseer,
2009.
Arthur M Geoffrion. Proper efficiency and the theory of vector maximization. Journal of mathematical
analysis and applications , 22(3):618–630, 1968.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pp. 2672–2680, 2014.
Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. October 2016.
URL https://arxiv.org/abs/1610.02413v1 .
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth
Vepakomma, Abhishek Singh, Hang Qiu, et al. FedML: A research library and benchmark for federated
machine learning. arXiv preprint arXiv:2007.13518 , 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780,
1997.
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons. The non-iid data quagmire of de-
centralized machine learning. In International Conference on Machine Learning , pp. 4387–4398. PMLR,
2020.
Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu. Federated learning meets multi-objective
optimization. IEEE Transactions on Network Science and Engineering , 9(4):2039–2051, 2022. doi: 10.
1109/TNSE.2022.3169117.
Peter J. Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics , 35(1):
73 – 101, 1964. doi: 10.1214/aoms/1177703732. URL https://doi.org/10.1214/aoms/1177703732 .
Ahmed Imteaj, Urmish Thakker, Shiqiang Wang, Jian Li, and M Hadi Amini. A survey on federated learning
for resource-constrained IoT devices. IEEE Internet of Things Journal , 9(1):1–24, 2021.
Johannes Jahn et al. Vector optimization . Springer, 2009.
Rajendra K Jain, Dah-Ming W Chiu, William R Hawe, et al. A quantitative measure of fairness and
discrimination. Eastern Research Laboratory, Digital Equipment Corporation, Hudson, MA , 1984.
Jiawen Kang, Zehui Xiong, Dusit Niyato, Yuze Zou, Yang Zhang, and Mohsen Guizani. Reliable federated
learning for mobile networks. IEEE Wireless Communications , 27(2):72–80, April 2020.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: stochastic controlled averaging for federated learning. In In-
ternational Conference on Machine Learning , pp. 5132–5143. PMLR, 2020.
FPKelly,AKMaulloo,andDKHTan. Ratecontrolforcommunicationnetworks: shadowprices,proportional
fairness and stability. Journal of the Operational Research Society , 49(3):237–252, March 1998. ISSN 1476-
9360. URL https://doi.org/10.1057/palgrave.jors.2600523 .
15Published in Transactions on Machine Learning Research (01/2023)
Frank Kelly. Charging and rate control for elastic traffic. European transactions on Telecommunications , 8
(1):33–37, 1997.
Andrey Kolmogorov. On the notion of mean. Atti della Academia Nazionale dei Lincei , 12(9):388–391,
1930. URL https://link.springer.com/book/9789027727961 . reprintedin“SelectedWorksIofAndrey
Kolmogorov: Mathematics and Mechanics”, pp. 144–146, 1991.
AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages,2009. Technical
report.
Matt Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Proceedings of
the 31st International Conference on Neural Information Processing Systems , pp. 4069–4079, 2017.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231n, 2015.
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical risk minimization. In
International Conference on Learning Representations , 2020a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. Proceedings of Machine Learning and Systems , 2:429–450, 2020b.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning.
InInternational Conference on Learning Representations , 2020c.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In ICML, volume 139 of Proceedings of Machine Learning Research , pp. 6357–
6368. PMLR, 2021.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of FedAvg
on non-iid data. In International Conference on Learning Representations , 2019.
Guodong Long, Yue Tan, Jing Jiang, and Chengqi Zhang. Federated learning for open banking. In Federated
learning, pp. 240–254. Springer, 2020.
FA Lootsma, TW Athan, and PY Papalambros. Controlling the search for a compromise solution in multi-
objective optimization. Engineering Optimization+ A35 , 25(1):65–81, 1995.
Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun Ma, Jiong Jin, Han Yu, and
Kee Siong Ng. Towards fair and privacy-preserving federated deep models. IEEE Transactions on Parallel
and Distributed Systems , 31(11):2524–2541, November 2020.
Michael Maschler, Eilon Solan, and Shmuel Zamir. Game theory. Cambridge University Press, Cambridge,
second edition , 2020.
Eric Maskin. A theorem on utilitarianism. The Review of Economic Studies , 45(1):93–96, 1978.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics , pp. 1273–1282. PMLR, 2017.
Jeonghoon Mo and Jean Walrand. Fair end-to-end window-based congestion control. IEEE/ACM Trans-
actions on Networking , 8(5):556–567, October 2000. ISSN 1063-6692. doi: 10.1109/90.879343. URL
https://doi.org/10.1109/90.879343 .
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International
Conference on Machine Learning , pp. 4615–4625. PMLR, 2019.
John Forbes Nash. The bargaining problem. Econometrica , 18(2):155–162, 1950.
Reese Pathak and Martin J Wainwright. FedSplit: an algorithmic framework for fast federated optimization.
Advances in Neural Information Processing Systems , 33:7057–7066, 2020.
16Published in Transactions on Machine Learning Research (01/2023)
Judea Pearl. Models, reasoning and inference. Cambridge University Press , 19, 2000.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon M Kleinberg, and Kilian Q Weinberger. On fairness and
calibration. In NIPS, 2017.
John Rawls. Some reasons for the maximin criterion. The American Economic Review , 64(2):141–146, 1974.
John Rawls. A theory of justice: Revised edition . Harvard university press, 1999.
Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečn` y, Sanjiv
Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on
Learning Representations , 2020.
Amartya Sen. Chapter 22 Social choice theory. volume 3 of Handbook of Mathematical Eco-
nomics, pp. 1073–1181. Elsevier, 1986. URL https://www.sciencedirect.com/science/article/pii/
S1573438286030047 . ISSN: 1573-4382.
Hanbyul Seo and Byeong Gi Lee. Proportional-fair power allocation with CDF-based scheduling for fair and
efficient multiuser OFDM systems. IEEE Transactions on Wireless Communications , 5(5):978–983, May
2006.
William Shakespeare. The complete works of William Shakespeare. 1614. URL https://www.gutenberg.
org/ebooks/100 .
Yuxin Shi, Han Yu, and Cyril Leung. A survey of fairness-aware federated learning. arXiv preprint
arXiv:2111.01872 , 2021.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated
learning with matched averaging. In International Conference on Learning Representations , 2019a.
Kangkang Wang, Rajiv Mathews, Chloé Kiddon, Hubert Eichner, Françoise Beaufays, and Daniel Ramage.
Federated evaluation of on-device personalization. October 2019b. URL https://arxiv.org/abs/1910.
10252v1. arXiv: 1910.10252.
Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. A principled approach to data
valuationforfederatedlearning. InQiangYang, LixinFan, andHanYu(eds.), Federated Learning: Privacy
and Incentive , Lecture Notes in Computer Science, pp. 153–167. Springer International Publishing, 2020.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer
vision (ECCV) , pp. 3–19, 2018.
Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei Wang. Federated learning for
healthcare informatics. Journal of Healthcare Informatics Research , 5(1):1–19, 2021.
Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit Niyato, and Qiang Yang. A
fairness-aware incentive scheme for federated learning. In Proceedings of the AAAI/ACM Conference on
AI, Ethics, and Society , AIES ’20, pp. 393–399, New York, NY, USA, February 2020. Association for
Computing Machinery.
Po-Lung Yu and Milan Zeleny. The set of all nondominated solutions in linear cases and a multicriteria
simplex method. Journal of Mathematical Analysis and Applications , 49(2):430–468, 1975.
XuboYue, MaherNouiehed, andRaedAlKontar. GIFAIR-FL:Aframeworkforgroupandindividualfairness
in federated learning. INFORMS Journal on Data Science , 0(0):null, 2022. doi: 10.1287/ijds.2022.0022.
URL https://doi.org/10.1287/ijds.2022.0022 .
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness
beyond disparate treatment & disparate impact: learning classification without disparate mistreatment.
Proceedings of the 26th International Conference on World Wide Web , pp. 1171–1180, April 2017. doi:
10.1145/3038912.3052660. URL http://arxiv.org/abs/1610.08452 . arXiv: 1610.08452.
17Published in Transactions on Machine Learning Research (01/2023)
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In
Proceedings of the 30th International Conference on Machine Learning , pp. 325–333. PMLR, May 2013.
URL https://proceedings.mlr.press/v28/zemel13.html . ISSN: 1938-7228.
Han Zhao and Geoff Gordon. Inherent tradeoffs in learning fair representations. Advances in neural infor-
mation processing systems , 32:15675–15685, 2019.
18Published in Transactions on Machine Learning Research (01/2023)
Notation Meaning
θ model parameters
n the number of clients
m batch size
x∈Rdraw input
y∈[C] output label
ni the number of samples from client i
N=/summationtext
ini the total number of samples from all clients
Si a batch of samples from client i
Di data distribution of client i
ℓ(θ,(x,y)) prediction loss (e.g. cross entropy) of model θon sample (x,y)
ℓSi average loss over batch Si
fi expected loss over distribution Di
f= (f1,...,fn) vector of client losses
ui utility of client i
u= (u1,...,un) vector of client utilities
Ki the number of local steps of client i
pi pre-defined weight of each client i, usuallypi=ni/N
p= (p1,...,pn) vector of client weights
Φ :Rn→R scalarization function of f
φ:R→R a scalar function that acts on each client loss fi
Aφ Kolmogorov’s generalized mean with scalar function φ
λ= (λ1,...,λn) dual parameter
η local learning rate
σ2
i local variance on distribution i
σ2global variance among clients
F=/summationtext
ipifi objective of FedAvg
log[ϵ](M−t) huberization of log(M−t), see eq. 3.8
π=−/summationtext
ipilog[ϵ](M−fi) objective of PropFair
L Lipschitz constant of all ∇fi’s
e natural logarithm
Rn
++ (strictly) positive orthant of Rn
Table 2: Notation table.
A Notations
We include a notation table for easy navigation. The reader can refer to Table 2 for quick access to the
notations.
19Published in Transactions on Machine Learning Research (01/2023)
B Proofs
Proposition 3.1 (equivalence , e.g. Kelly 1997; Boche & Schubert 2009) .For any convex set U∈Rn
++,
a pointu∈Uis the Nash bargaining solution iff it is proportionally fair. If Uis non-convex, then a PF
solution, when exists, is a Nash bargaining solution.
Proof.The Nash bargaining solution u∗is equivalent to the maximum of the following:
max
u∈Un/summationdisplay
i=1pilogui. (B.1)
SinceUis convex and/summationtextn
i=1piloguiis concave in u, the necessary and sufficient optimality condition (e.g,.
Bertsekas, 1997) is:
⟨u−u∗,∇n/summationdisplay
i=1pilogu∗
i⟩≤0,for anyu∈U, (B.2)
or equivalently, eq. 3.3. If Uis non-convex, then the optimality condition eq. B.2 also holds for the convex
hull ofU. Therefore,u∗is a maximizer of/summationtextn
i=1piloguiin the convex hull of Uand thusU.
Theorem 4.2 (FedAvg).Given Assumption 4.1, assume that the local learning rate satisfies ηKi≤1
6Lfor
anyi∈[n]and
η≤1
L/radicaligg
1
24(e−2)(/summationtext
ip2
i)(/summationtext
iK4
i). (4.2)
Running Algorithm 2 for Tglobal epochs we have:
min
0≤t≤T−1E∥∇F(θt)∥2≤12
(11µ−9)η/parenleftbiggF0−F∗
T+ Ψσ/parenrightbigg
,
withµ=/summationtext
ipiKifor full participation and µ= miniKifor partial participation, F0=F(θ0),F∗=
minθF(θ)the optimal value, and
Ψσ=η∥p∥2/bracketleftiggn/summationdisplay
i=1K2
i/parenleftbiggLησ2
i
2m+σ2/parenrightbigg
+ (e−2)η2L2n/summationdisplay
i=1K3
i/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg/bracketrightigg
,p= (p1,...,pn).
Proof.We first assume full participation in the following theorem. The partial participation version is an
easy extension and we discuss it in the end. We use θ(i)
t,jto denote the model parameters of client iat global
epochtand local step j. Due to the synchronization step, we have θ(i)
t,0=θt, the global model at step t, and
θt+1=n/summationdisplay
i=1piθ(i)
t,Ki, pi=ni
N, (B.3)
whereKiis the local number of steps of client i. We also have:
θ(i)
t,j=θ(i)
t,j−1−ηg(i)
t,j,for allj∈[Ki]. (B.4)
whereg(i)
t,j=∇ℓSj
i(θ(i)
t,j−1)is an unbiased estimator of ∇fi(θ(i)
t,j−1)forj∈[Ki], withSj
ithejthbatch from
clienti. Combining eq. B.3 and eq. B.4 we have:
θt+1=θt−ηn/summationdisplay
i=1piKi/summationdisplay
j=1g(i)
t,j. (B.5)
20Published in Transactions on Machine Learning Research (01/2023)
Part I Since eachfiisL-Lipschitz smooth, so is their average F=/summationtext
ipifi, from which we obtain that:
F(θt+1)≤F(θt) +⟨∇F(θt),θt+1−θt⟩+L
2∥θt+1−θt∥2. (B.6)
Plugging in eq. B.5 yields:
F(θt+1)≤F(θt)−η/angbracketleftigg
∇F(θt),n/summationdisplay
i=1piKi/summationdisplay
j=1g(i)
t,j/angbracketrightigg
+Lη2
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1g(i)
t,j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
. (B.7)
From the identity g(i)
t,j=g(i)
t,j−∇F(θt) +∇F(θt),we write eq. B.7 as:
F(θt+1)≤F(θt)−ηn/summationdisplay
i=1piKi∥∇F(θt)∥2−η/angbracketleftigg
∇F(θt),n/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/angbracketrightigg
+
+Lη2
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt)) +n/summationdisplay
i=1piKi∇F(θt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
. (B.8)
By further expanding the last term we have:
F(θt+1)≤F(θt)−ηn/summationdisplay
i=1piKi∥∇F(θt)∥2−η/angbracketleftigg
∇F(θt),n/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/angbracketrightigg
+
+Lη2
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+Lη2
2/parenleftiggn/summationdisplay
i=1piKi/parenrightigg2
∥∇F(θt)∥2+
+Lη2/angbracketleftiggn/summationdisplay
i=1piKi∇F(θt),n/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/angbracketrightigg
. (B.9)
For simplicity we use µas a shorthand for/summationtextn
i=1piKi. Grouping similar terms together gives:
F(θt+1)≤F(θt)−ηµ/parenleftbigg
1−Lη
2µ/parenrightbigg
∥∇F(θt)∥2
−η(1−Lηµ)/angbracketleftigg
∇F(θt),n/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/angbracketrightigg
+Lη2
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
.(B.10)
Taking the expectation on both sides and with Cauchy–Schwarz inequality, we have:
EF(θt+1)≤EF(θt)−ηµ/parenleftbigg
1−Lη
2µ/parenrightbigg
E∥∇F(θt)∥2+
+η(1−Lηµ)E
∥∇F(θt)∥·/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
+
+Lη2
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤EF(θt) +/parenleftbigg
−ηµ/parenleftbigg
1−Lη
2µ/parenrightbigg
+1
2η(1−Lηµ)/parenrightbigg
E∥∇F(θt)∥2+
+Lη2
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+1
2η(1−Lηµ)E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
,
(B.11)
21Published in Transactions on Machine Learning Research (01/2023)
where we used Eg(i)
t,j=∇fi(θ(i)
t,j−1)and the inequality ab≤1
2(a2+b2)in the second line. Let us now study
the two coefficients separately. From the assumption, 6ηLKi≤1for anyi∈[n], and thus 6Lµη≤1. Hence
we have:
−ηµ/parenleftbigg
1−Lη
2µ/parenrightbigg
+1
2η(1−Lηµ)≤−η/parenleftbigg
µ−1
2−Lη
2µ2+1
2Lηµ/parenrightbigg
≤−η/parenleftbigg11µ−6
12+Lηµ
2/parenrightbigg
≤−η11µ−6
12. (B.12)
Therefore, eq. B.11 becomes:
EF(θt+1)≤EF(θt)−η11µ−6
12E∥∇F(θt)∥2+Lη2
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+
+1
2η(1−Lηµ)E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
. (B.13)
Part II With the following identity:
g(i)
t,j−∇F(θt) =g(i)
t,j−∇fi(θ(i)
t,j−1) +∇fi(θ(i)
t,j−1)−∇F(θt), (B.14)
the second last term of eq. B.11 can be simplified as:
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇fi(θ(i)
t,j−1))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
, (B.15)
where we note that g(i)
t,jis an unbiased estimator of ∇fi(θ(i)
t,j−1). We first bound the first term of eq. B.15:
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇fi(θ(i)
t,j−1))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤E
n/summationdisplay
i=1piKi/summationdisplay
j=1/vextenddouble/vextenddouble/vextenddoubleg(i)
t,j−∇fi(θ(i)
t,j−1)/vextenddouble/vextenddouble/vextenddouble
2
≤E∥p∥2n/summationdisplay
i=1
Ki/summationdisplay
j=1/vextenddouble/vextenddouble/vextenddoubleg(i)
t,j−∇fi(θ(i)
t,j−1)/vextenddouble/vextenddouble/vextenddouble
2
≤E∥p∥2n/summationdisplay
i=1KiKi/summationdisplay
j=1/vextenddouble/vextenddouble/vextenddoubleg(i)
t,j−∇fi(θ(i)
t,j−1)/vextenddouble/vextenddouble/vextenddouble2
=∥p∥2n/summationdisplay
i=1KiKi/summationdisplay
j=1E/vextenddouble/vextenddouble/vextenddoubleg(i)
t,j−∇fi(θ(i)
t,j−1)/vextenddouble/vextenddouble/vextenddouble2
≤∥p∥2n/summationdisplay
i=1K2
iσ2
i
m, (B.16)
where in the first line, we used triangle inequality; in the second and third lines, we used the Cauchy–Schwarz
inequality; in the fourth line we used the linearity of expectation; in the final line we note that given the last
part of Assumption 4.1, we have:
ESi∼Dm
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|Si|/summationdisplay
(x,y)∈Si∇ℓ(θ,(x,y))−∇fi(θ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=1
|Si|2/summationdisplay
(x,y)∈SiE(x,y))∼Di∥∇ℓ(θ,(x,y))−∇fi(θ)∥2≤σ2
i
m,
(B.17)
22Published in Transactions on Machine Learning Research (01/2023)
where we used the property that each (x,y)is an i.i.d. sample from Di, and that the estimation is unbiased
(by the definition of fi). Similarly we bound the second term of eq. B.15:
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤∥p∥2n/summationdisplay
i=1KiKi/summationdisplay
j=1E/vextenddouble/vextenddouble/vextenddouble∇fi(θ(i)
t,j−1)−∇F(θt)/vextenddouble/vextenddouble/vextenddouble2
.(B.18)
With the following identity:
∇fi(θ(i)
t,j−1)−∇F(θt) =∇fi(θ(i)
t,j−1)−∇fi(θt) +∇fi(θt)−∇F(θt), (B.19)
and taking the squared norm on both sides, we have:
∥∇fi(θ(i)
t,j−1)−∇F(θt)∥2≤2∥∇fi(θ(i)
t,j−1)−∇fi(θt)∥2+ 2∥∇fi(θt)−∇F(θt)∥2
≤2L2∥θ(i)
t,j−1−θt∥2+ 2σ2, (B.20)
where we note that:
∥∇fi(θt)−∇F(θt)∥=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇fi(θt)−n/summationdisplay
j=1pj∇fj(θt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
j=1pj(∇fi(θt)−∇fj(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤n/summationdisplay
j=1pj∥∇fi(θt)−∇fj(θt)∥
≤σ, (B.21)
where in the third line we used the triangle inequality and in the last line we used Assumption 4.1. Plugging
eq. B.20 into eq. B.18 yields:
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤2∥p∥2n/summationdisplay
i=1K2
iσ2+ 2L2∥p∥2n/summationdisplay
i=1KiKi/summationdisplay
j=1E∥θ(i)
t,j−1−θt∥2.(B.22)
Bringing eq. B.16 and eq. B.22 into eq. B.15 we write:
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤∥p∥2n/summationdisplay
i=1K2
i/parenleftbiggσ2
i
m+ 2σ2/parenrightbigg
+ 2L2∥p∥2n/summationdisplay
i=1KiKi/summationdisplay
j=1E∥θ(i)
t,j−1−θt∥2
≤∥p∥2n/summationdisplay
i=1K2
i/parenleftbiggσ2
i
m+ 2σ2/parenrightbigg
+ 2L2∥p∥2n/summationdisplay
i=1KiKi−1/summationdisplay
j=0E∥θ(i)
t,j−θt∥2.(B.23)
Part III Now let us give an upper bound for E∥θ(i)
t,j−θt∥2. From eq. B.23, we only need to focus on
Ki≥2andj= 1,...,Ki−1sinceθ(i)
t,0=θt. Forj∈[Ki−1], we have from eq. B.4:
E∥θ(i)
t,j−θt∥2=E∥θ(i)
t,j−1−θt−ηg(i)
t,j∥2
=E∥θ(i)
t,j−1−θt−η∇fi(θ(i)
t,j−1) +η∇fi(θ(i)
t,j−1)−ηg(i)
t,j∥2
=E∥θ(i)
t,j−1−θt−η∇fi(θ(i)
t,j−1)∥2+Eη2∥∇fi(θ(i)
t,j−1)−g(i)
t,j∥2
=E∥θ(i)
t,j−1−θt−η∇fi(θ(i)
t,j−1)∥2+η2σ2
i
m, (B.24)
23Published in Transactions on Machine Learning Research (01/2023)
where in the third line we note that g(i)
t,jis an unbiased estimator of ∇fi(θ(i)
t,j−1)and in the last line we used
eq. B.17. The first term in the last line above can be bounded as:
E∥θ(i)
t,j−1−θt−η∇fi(θ(i)
t,j−1)∥2≤/parenleftbigg
1 +1
2Ki−1/parenrightbigg
E∥θ(i)
t,j−1−θt∥2+ 2Kiη2∥∇fi(θ(i)
t,j−1)∥2,(B.25)
where we used∥a+b∥2≤(1 +1
α)∥a∥2+ (1 +α)∥b∥2for any vectors a,bwith the same dimension and α>0.
Since
∇fi(θ(i)
t,j−1) = (∇fi(θ(i)
t,j−1)−∇fi(θt)) + (∇fi(θt)−∇F(θt)) +∇F(θt), (B.26)
taking the squared norm on both sides we have (note that (a+b+c)2≤3(a2+b2+c2)):
∥∇fi(θ(i)
t,j−1)∥2≤3∥∇fi(θ(i)
t,j−1)−∇fi(θt)∥2+ 3∥∇fi(θt)−∇F(θt)∥2+ 3∥∇F(θt)∥2
≤3L2∥θ(i)
t,j−1−θt∥2+ 3σ2+ 3∥∇F(θt)∥2, (B.27)
where in the second line we used eq. B.21 and Assumption 4.1. Plugging eq. B.27 into eq. B.25 we find:
E∥θ(i)
t,j−1−θt−η∇fi(θ(i)
t,j−1)∥2≤/parenleftbigg
1 +1
2Ki−1+ 6Kiη2L2/parenrightbigg
E∥θ(i)
t,j−1−θt∥2+ 6Kiη2σ2+ 6Kiη2E∥∇F(θt)∥2.
(B.28)
Combined with eq. B.24, we obtain:
E∥θ(i)
t,j−θt∥2≤/parenleftbigg
1 +1
2Ki−1+ 6Kiη2L2/parenrightbigg
E∥θ(i)
t,j−1−θt∥2+η2/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg
+ 6Kiη2E∥∇F(θt)∥2.
(B.29)
Recall that we assumed η≤mini{1
6KiL}. ForKi≥2(note the assumption at the beginning of Part III), we
have:
1 +1
2Ki−1+ 6Kiη2L2≤1 +1
2Ki−1+1
6Ki≤1 +1
Ki. (B.30)
Therefore, eq. B.29 becomes:
E∥θ(i)
t,j−θt∥2≤/parenleftbigg
1 +1
Ki/parenrightbigg
E∥θ(i)
t,j−1−θt∥2+η2/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg
+ 6Kiη2E∥∇F(θt)∥2.(B.31)
We can treat{aj=E∥θ(i)
t,j−θt∥2∥}Ki−1
j=1as a sequence. Unrolling this sequence and with a0= 0, we have:
E∥θ(i)
t,j−θt∥2≤/parenleftig
1 +1
Ki/parenrightigj
−1
1 +1
Ki−1/parenleftbigg
η2/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg
+ 6Kiη2E∥∇F(θt)∥2/parenrightbigg
=Ki/parenleftigg/parenleftbigg
1 +1
Ki/parenrightbiggj
−1/parenrightigg/parenleftbigg
η2/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg
+ 6Kiη2E∥∇F(θt)∥2/parenrightbigg
. (B.32)
Summing over j= 0,1,...,Ki−1gives:
Ki−1/summationdisplay
j=0E∥θ(i)
t,j−θt∥2≤K2
i/parenleftigg/parenleftbigg
1 +1
Ki/parenrightbiggKi
−2/parenrightigg/parenleftbigg
η2/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg
+ 6Kiη2E∥∇F(θt)∥2/parenrightbigg
≤(e−2)K2
i/parenleftbigg
η2/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg
+ 6Kiη2E∥∇F(θt)∥2/parenrightbigg
= (e−2)K2
iη2/parenleftbiggσ2
i
m+ 6Kiσ2+ 6KiE∥∇F(θt)∥2/parenrightbigg
(B.33)
where in the first line we used the geometric series formula 1 +q+···+qn−1=qn−1
q−1; in the second line we
used the fact that/parenleftig
1 +1
Ki/parenrightigKi
≤eforKi≥1, withethe natural logarithm.
24Published in Transactions on Machine Learning Research (01/2023)
Part IV We finally put things together and finish our proof. From eq. B.13 we have:
EF(θt+1)≤EF(θt)−η11µ−6
12E∥∇F(θt)∥2+Lη2
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+
+1
2η(1−Lηµ)E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=EF(θt)−η11µ−6
12E∥∇F(θt)∥2+Lη2
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(g(i)
t,j−∇fi(θ(i)
t,j−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+
+/parenleftbiggLη2
2+1
2η(1−Lηµ)/parenrightbigg
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤EF(θt)−η11µ−6
12E∥∇F(θt)∥2+Lη2
2∥p∥2n/summationdisplay
i=1K2
iσ2
i
m+η
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇fi(θ(i)
t,j−1)−∇F(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤EF(θt)−η11µ−6
12E∥∇F(θt)∥2+Lη2
2∥p∥2n/summationdisplay
i=1K2
iσ2
i
m+
+η
2
2∥p∥2n/summationdisplay
i=1K2
iσ2+ 2L2∥p∥2n/summationdisplay
i=1KiKi/summationdisplay
j=1E∥θ(i)
t,j−1−θt∥2

≤EF(θt)−η11µ−6
12E∥∇F(θt)∥2+ 6(e−2)η3L2∥p∥2n/summationdisplay
i=1K4
iE∥∇F(θt)∥2+ Ψσ, (B.34)
where in the third line we used eq. B.15; in the fifth line we used eq. B.16 and note that
Lη2
2+1
2η(1−Lηµ) =η
2+Lη2
2(1−µ)≤η
2; (B.35)
in the seventh line we used eq. B.22; and in the final line we used eq. B.33 and denoted
Ψσ=η∥p∥2/bracketleftiggn/summationdisplay
i=1K2
i/parenleftbiggLησ2
i
2m+σ2/parenrightbigg
+ (e−2)η2L2n/summationdisplay
i=1K3
i/parenleftbiggσ2
i
m+ 6Kiσ2/parenrightbigg/bracketrightigg
. (B.36)
Since we assumed:
η≤1
L/radicaligg
1
24(e−2)∥p∥2(/summationtextn
i=1K4
i), (B.37)
we have:
11µ−6
12−6(e−2)L2∥p∥2n/summationdisplay
i=1K4
iη2≥11µ−6
12−1
4≥11µ−9
12. (B.38)
Therefore, eq. B.34 becomes:
EF(θt+1)≤EF(θt)−η11µ−9
12E∥∇F(θt)∥2+ Ψσ. (B.39)
With some algebra we obtain:
η11µ−9
12E∥∇F(θt)∥2≤E[F(θt)−F(θt+1)] + Ψσ. (B.40)
25Published in Transactions on Machine Learning Research (01/2023)
Summing both sides over t= 0,...,T−1and dividing by T, we have:
η11µ−9
121
TT−1/summationdisplay
t=0E∥∇F(θt)∥2≤F(θ0)−F∗
T+ Ψσ, (B.41)
which gives:
min
0≤t≤T−1E∥∇F(θt)∥2≤12
(11µ−9)η/parenleftbiggF(θ0)−F∗
T+ Ψσ/parenrightbigg
, (B.42)
withF∗= minθF(θ)the optimal value.
Finally, for the partial participation, it suffices to replace the client set {1,...,n}with its subset. Note
that after this substitution, the new variance term satisfies Ψ′
σ≤Ψσsince this term increases with more
participants, and eq. B.38 still holds since we subtract a smaller term with partial participation. We also
need to modify eq. B.38 so we further lower bound eq. B.38 with µ≥miniKi.
Theorem 4.4 (PropFair ).Denote/tildewideL=4
M2(3
2ML+L2
0)andpi=ni
N. Given Assumptions 4.1 and 4.3,
assume that the local learning rate satisfies:
η≤min/braceleftigg
min
i∈[n]1
6/tildewideLKi,1
8˜L/radicaligg
1
(e−2)(/summationtext
ip2
i)(/summationtext
iK4
i)/bracerightigg
. (4.3)
By running Algorithm 1 for Tglobal epochs we have:
min
0≤t≤T−1E∥∇π(θt)∥2≤12
(11µ−9)η/parenleftbiggπ0−π∗
T+/tildewideΨσ/parenrightbigg
,
withµ=/summationtext
ipiKifor full participation and µ= miniKifor partial participation, π0=π(θ0),π∗= minθπ(θ)
the optimal value, and
/tildewideΨσ=η∥p∥2/bracketleftbiggn/summationdisplay
i=1K2
i/parenleftbigg/tildewideσ2
i
m+ 2/tildewideσ2/parenrightbigg
+ 16(e−2)η2˜L2n/summationdisplay
i=1K4
i/parenleftbigg/tildewideσ2
i
m+/tildewideσ2/parenrightbigg/bracketrightbigg
where/tildewideσ2
i=8
M4(9M2σ2
i+ 4L2
0σ2
0,i)and/tildewideσ=4
M/parenleftbig3
2σ+L0
Mσ0/parenrightbig
.
Proof.The proof follows similarly the proof of FedAvg (Theorem 4.2). Denote φ(t) =−log(M−t). The
changes of PropFair compared to FedAvg as follows:
•The aggregate loss for each client iis notfi, butφ◦fi;
•The objective function is not F=/summationtext
ipifi, butπ=/summationtext
ipiφ◦fi;
•For each batch Si∼Dm
ifrom client i, the batch loss is not ℓSi, butφ◦ℓSi.
Note that in Assumption 4.1 we implicitly required eq. 2.1:
fi(θ) =E(x,y)∼Di[ℓ(θ,(x,y))],
or in other words, ℓ(θ,(x,y))is anunbiased estimator of fi. This is no longer true if we replace ℓwithφ◦ℓ
andfiwithφ◦fi. Similarly, φ◦ℓSiis no longer an unbiased estimator of φ◦fi. We will take care of this
pitfall in our proof.
First, from the Lipschitzness assumption in Assumption 4.3 we can obtain an upper bound for the gradient:
∥∇fi(θ)∥≤L0for anyθ∈Rd. We will use this result, as well as the rest of Assumption 4.3, to derive
similar bounds as in Assumption 4.1:
26Published in Transactions on Machine Learning Research (01/2023)
•the Lipschitz constant of ∇φ◦fi;
•the Lipschitz constant of φ◦fi−φ◦fj;
•the variance of each batch ∇φ◦ℓSi.
For the Lipschitz smooth constant of φ◦fi, we write:
∥∇(φ◦fi)(θ)−∇(φ◦fi)(θ′)∥=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇fi(θ)
M−fi(θ)−∇fi(θ′)
M−fi(θ′)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM(∇fi(θ)−∇fi(θ′))−∇fi(θ)fi(θ′) +∇fi(θ′)fi(θ)
(M−fi(θ))(M−fi(θ′))/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤4
M2(M∥∇fi(θ)−∇fi(θ′)∥+∥∇fi(θ)fi(θ′)−∇fi(θ′)fi(θ)∥)
≤4
M2(ML∥θ−θ′∥+∥∇fi(θ)fi(θ′)−∇fi(θ′)fi(θ)∥). (B.43)
The second term in the parenthesis above can be computed as:
∥∇fi(θ)fi(θ′)−∇fi(θ′)fi(θ))∥=∥∇fi(θ)fi(θ′)−∇fi(θ)fi(θ) +∇fi(θ)fi(θ)−∇fi(θ′)fi(θ))∥
≤∥∇fi(θ)fi(θ′)−∇fi(θ)fi(θ)∥+∥∇fi(θ)fi(θ)−∇fi(θ′)fi(θ)∥
=∥∇fi(θ)∥·∥fi(θ′)−fi(θ)∥+∥∇fi(θ)−∇fi(θ′)∥·∥fi(θ)∥
≤L2
0∥θ′−θ∥+LM
2∥θ′−θ∥, (B.44)
where in the second line we used triangle inequality; in the fourth line we used Assumptions 4.1 and 4.3.
Plugging in back to eq. B.43 we have:
∥∇(φ◦fi)(θ)−∇(φ◦fi)(θ′)∥≤4
M2/parenleftbigg3
2ML+L2
0/parenrightbigg
∥θ−θ′∥. (B.45)
Let us now figure out the variance terms. For the global variance term, we similarly write:
∥∇(φ◦fi)(θ)−∇(φ◦fj)(θ)∥=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇fi(θ)
M−fi(θ)−∇fj(θ)
M−fj(θ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM(∇fi(θ)−∇fj(θ))−∇fi(θ)fj(θ) +∇fj(θ)fi(θ)
(M−fi(θ))(M−fj(θ))/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤4
M2(M∥∇fi(θ)−∇fj(θ)∥+∥∇fi(θ)fj(θ)−∇fj(θ)fi(θ)∥)
≤4
M2(Mσ+∥∇fi(θ)fj(θ)−∇fj(θ)fi(θ)∥). (B.46)
The second term in the parenthesis above can be computed as:
∥∇fi(θ)fj(θ)−∇fj(θ)fi(θ))∥=∥∇fi(θ)fj(θ)−∇fi(θ)fi(θ) +∇fi(θ)fi(θ)−∇fj(θ)fi(θ)∥
≤∥∇fi(θ)fj(θ)−∇fi(θ)fi(θ)∥+∥∇fi(θ)fi(θ)−∇fj(θ)fi(θ)∥
=∥∇fi(θ)∥·∥fj(θ)−fi(θ)∥+∥∇fi(θ)−∇fj(θ)∥·∥fi(θ)∥
≤L0σ0+M
2σ, (B.47)
where in the second line we used triangle inequality; in the last line we used Assumptions 4.1 and 4.3.
Plugging eq. B.47 into eq. B.46 we find:
∥∇(φ◦fi)(θ)−∇(φ◦fj)(θ)∥≤4
M/parenleftbigg3
2σ+L0
Mσ0/parenrightbigg
. (B.48)
27Published in Transactions on Machine Learning Research (01/2023)
Let us finally compute the new local variance term for each batch. Recall that we denoted ℓSi(θ) :=
1
|Si|/summationtext
(x,y)∈Siℓ(θ,(x,y)), withSi∼Dm
i. We can write
∥∇(φ◦fi)(θ)−∇(φ◦ℓSi)(θ)∥=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇fi(θ)
M−fi(θ)−∇ℓSi(θ)
M−ℓSi(θ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤4
M2/parenleftbigg3M
2∥∇fi(θ)−∇ℓSi(θ)∥+L0∥fi(θ)−ℓSi(θ)∥/parenrightbigg
,(B.49)
and the derivation follows similarly as eq. B.46. Taking the square on both sides and taking the expectation
overSi∼Dm
i, we obtain:
ESi∼Dm
i∥∇(φ◦fi)(θ)−∇(φ◦ℓSi)(θ)∥2≤16
M4/parenleftigg
29M2
4ESi∼Dm
i∥∇fi(θ)−∇ℓSi(θ)∥2+
+ 2L2
0ESi∼Dm
i∥fi(θ)−ℓSi(θ)∥2/parenrightigg
≤8
M4/parenleftigg
9M2σ2
i
m+ 4L2
0σ2
0,i
m/parenrightigg
, (B.50)
where in the first line we used (a+b)2≤2(a2+b2)and in the second line we used Assumptions 4.1 and 4.3.
For convenience we will use the following notations:
/tildewideL=4
M2/parenleftbigg3
2ML+L2
0/parenrightbigg
,/tildewideσ2
i=8
M4/parenleftbig
9M2σ2
i+ 4L2
0σ2
0,i/parenrightbig
,˜σ=4
M/parenleftbigg3
2σ+L0
Mσ0/parenrightbigg
,(B.51)
which are the new Lipschitz constant of ∇φ◦fi, the new local variance term of ∇φ◦ℓSi, and the new
Lipschitz constant of φ◦fi−φ◦fj. Note that if we average after the composition, then the local variance
would be:
ESi∼Dm
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
|Si|/summationdisplay
(x,y)∈Si∇φ◦ℓ(θ,(x,y))−∇φ◦fi(θ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤1
|Si|/summationdisplay
(x,y)∈SiE(x,y)∼Di∥∇φ◦ℓ(θ,(x,y))−∇φ◦fi(θ)∥2
≤/tildewideσ2
i, (B.52)
where we can only use Cauchy–Schwarz inequality since φ◦ℓis biased. Therefore, if we do it in this way, the
variance (upper bound) will be mtimes larger than the current way, which will slow down the convergence.
Let us now follow the proof of FedAvg (Theorem 4.2) to prove the convergence of PropFair. Our proof
follows the one of Theorem 4.2. Note that the global update now is:
θt+1=θt−ηn/summationdisplay
i=1piKi/summationdisplay
j=1/tildewideg(i)
t,j, (B.53)
with/tildewideg(i)
t,j=∇φ◦ℓSj
i(θ(i)
t,j−1)andSj
ithejthbatch from client i. Similar to eq. B.10 we obtain:
π(θt+1)≤π(θt)−ηµ/parenleftbigg
1−Lη
2µ/parenrightbigg
∥∇π(θt)∥2
−η(1−Lηµ)/angbracketleftigg
∇π(θt),n/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇π(θt))/angbracketrightigg
+Lη2
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇π(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
.(B.54)
28Published in Transactions on Machine Learning Research (01/2023)
However, since /tildewideg(i)
t,jis no longer unbiased, we need to rewrite eq. B.11 as:
Eπ(θt+1)≤Eπ(θt)−ηµ/parenleftbigg
1−Lη
2µ/parenrightbigg
E∥∇π(θt)∥2+η(1−Lηµ)E
∥∇π(θt)∥·/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇π(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
+
+Lη2
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇π(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤Eπ(θt) +/parenleftbigg
−ηµ/parenleftbigg
1−Lη
2µ/parenrightbigg
+1
2η(1−Lηµ)/parenrightbigg
E∥∇π(θt)∥2+
+/parenleftbiggLη2
2+1
2η(1−Lηµ)/parenrightbigg
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇π(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤Eπ(θt)−η11µ−6
12E∥∇π(θt)∥2+η
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇π(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
, (B.55)
where we recycled eq. B.12 and eq. B.35. Similar to eq. B.14 we write:
/tildewideg(i)
t,j−∇π(θt) =/tildewideg(i)
t,j−∇φ◦fi(θ(i)
t,j−1) +∇φ◦fi(θ(i)
t,j−1)−∇π(θt), (B.56)
and using∥a+b∥2≤2(∥a∥2+∥b∥2)eq. B.55 becomes:
Eπ(θt+1)≤Eπ(θt)−η11µ−6
12E∥∇π(θt)∥2+ηE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇/tildewidefi(θ(i)
t,j−1))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+
+ηE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇/tildewidefi(θ(i)
t,j−1)−∇π(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
, (B.57)
with ˜fia shorthand for φ◦fi. With eq. B.50 and similar to eq. B.16, we have:
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇/tildewidefi(θ(i)
t,j−1))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤∥p∥2n/summationdisplay
i=1K2
i/tildewideσ2
i
m, (B.58)
and similar to eq. B.22, we obtain:
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇/tildewidefi(θ(i)
t,j−1)−∇π(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤2∥p∥2n/summationdisplay
i=1K2
i˜σ2+ 2˜L2∥p∥2n/summationdisplay
i=1KiKi/summationdisplay
j=1E∥θ(i)
t,j−1−θt∥2.(B.59)
Forj∈[Ki−1], we can write similarly to eq. B.25:
E∥θ(i)
t,j−θt∥2=E∥θ(i)
t,j−1−θt−η/tildewideg(i)
t,j∥2≤/parenleftbigg
1 +1
2Ki−1/parenrightbigg
E∥θ(i)
t,j−1−θt∥2+ 2Kiη2E∥/tildewideg(i)
t,j∥2.(B.60)
With the following equality:
/tildewideg(i)
t,j= (/tildewideg(i)
t,j−∇˜fi(θ(i)
t,j−1)) + (∇˜fi(θ(i)
t,j−1)−∇˜fi(θt)) + (∇˜fi(θt)−∇π(θt)) +∇π(θt),(B.61)
we use∥a+b+c+d∥2≤4(∥a∥2+∥b∥2+∥c∥2+∥d∥2)to obtain:
E∥/tildewideg(i)
t,j∥2≤4E∥/tildewideg(i)
t,j−∇˜fi(θ(i)
t,j−1)∥2+ 4E∥∇˜fi(θ(i)
t,j−1)−∇˜fi(θt)∥2+ 4E∥∇˜fi(θt)−∇π(θt)∥2+ 4E∥∇π(θt)∥2
≤4/tildewideσ2
i
m+ 4˜L2E∥θ(i)
t,j−1−θt∥2+ 4˜σ2+ 4E∥∇π(θt)∥2. (B.62)
29Published in Transactions on Machine Learning Research (01/2023)
Plugging it back into eq. B.60 we have:
E∥θ(i)
t,j−θt∥2≤/parenleftbigg
1 +1
2Ki−1+ 8Kiη2˜L2/parenrightbigg
E∥θ(i)
t,j−1−θt∥2+ 8Kiη2/parenleftbigg/tildewideσ2
i
m+ ˜σ2/parenrightbigg
+ 8Kiη2E∥∇π(θt)∥2
≤/parenleftbigg
1 +1
Ki/parenrightbigg
E∥θ(i)
t,j−1−θt∥2+ 8Kiη2/parenleftbigg/tildewideσ2
i
m+ ˜σ2/parenrightbigg
+ 8Kiη2E∥∇π(θt)∥2
≤Ki/parenleftigg/parenleftbigg
1 +1
Ki/parenrightbiggj
−1/parenrightigg/parenleftbigg
8Kiη2/parenleftbigg/tildewideσ2
i
m+ ˜σ2/parenrightbigg
+ 8Kiη2E∥∇π(θt)∥2/parenrightbigg
, (B.63)
where in the second line we used η≤mini{1
6Ki˜L}, and the last line is telescoping. Similar to eq. B.33,
summing over j= 0,1,...,Ki−1gives:
Ki−1/summationdisplay
j=0E∥θ(i)
t,j−θt∥2≤8(e−2)K3
iη2/parenleftbigg/tildewideσ2
i
m+ ˜σ2+E∥∇π(θt)∥2/parenrightbigg
. (B.64)
From eq. B.57 we have:
Eπ(θt+1)≤Eπ(θt)−η11µ−6
12E∥∇π(θt)∥2+ηE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(/tildewideg(i)
t,j−∇/tildewidefi(θ(i)
t,j−1))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+
+ηE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1piKi/summationdisplay
j=1(∇/tildewidefi(θ(i)
t,j−1)−∇π(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤Eπ(θt)−η11µ−6
12E∥∇π(θt)∥2+η∥p∥2n/summationdisplay
i=1K2
i/tildewideσ2
i
m+ 2η∥p∥2n/summationdisplay
i=1K2
i˜σ2+
+ 2η˜L2∥p∥2n/summationdisplay
i=1KiKi/summationdisplay
j=1E∥θ(i)
t,j−1−θt∥2
≤Eπ(θt)−η11µ−6
12E∥∇π(θt)∥2+η∥p∥2n/summationdisplay
i=1K2
i/parenleftbigg/tildewideσ2
i
m+ 2˜σ2/parenrightbigg
+
+ 2η˜L2∥p∥2n/summationdisplay
i=18(e−2)K4
iη2/parenleftbigg/tildewideσ2
i
m+ ˜σ2+E∥∇π(θt)∥2/parenrightbigg
=Eπ(θt)−η/parenleftigg
11µ−6
12−16(e−2)η2˜L2∥p∥2n/summationdisplay
i=1K4
i/parenrightigg
E∥∇π(θt)∥2+/tildewideΨσ
≤Eπ(θt)−η11µ−9
12E∥∇π(θt)∥2+/tildewideΨσ, (B.65)
where in the second inequality we used eq. B.58 and eq. B.59; in the third inequality we used eq. B.64, in
the second last inequality, we denoted:
/tildewideΨσ=η∥p∥2/bracketleftbiggn/summationdisplay
i=1K2
i/parenleftbigg/tildewideσ2
i
m+ 2/tildewideσ2/parenrightbigg
+ 16(e−2)η2˜L2n/summationdisplay
i=1K4
i/parenleftbigg/tildewideσ2
i
m+/tildewideσ2/parenrightbigg/bracketrightbigg
; (B.66)
and in the last line, we note that:
11µ−6
12−16(e−2)η2˜L2∥p∥2n/summationdisplay
i=1K4
i≤11µ−9
12, (B.67)
since we assumed:
η≤1
8˜L/radicaligg
1
(e−2)∥p∥2(/summationtextn
i=1K4
i). (B.68)
30Published in Transactions on Machine Learning Research (01/2023)
Figure 6: The visualization of the distribution shown in eq. C.1. The plus sign means the positive label
y= 1and the negative sign means the negative label y=−1.
Similar to eq. B.42 we obtain:
min
0≤t≤T−1E∥∇π(θt)∥2≤12
(11µ−9)η/parenleftbiggπ(θ0)−π∗
T+/tildewideΨσ/parenrightbigg
. (B.69)
C A Failure Case of Agnostic Federated Learning
In this section we show that AFL might suffer from the generalization issue, in the case when some of the
clients have very few samples that are outliers. Suppose the input space is R2and the classification task
is binary with a linear classifier. We assume the simple case where every client has the same underlying
distribution:
p(x|y) =/braceleftigg
0.9U([−1,0]×[−1,1]) + 0.1U([0,1]×[−1,1])ify= 1,
0.9U([0,1]×[−1,1]) + 0.1U([−1,0]×[−1,1])ify=−1.(C.1)
Note thatU(I)represents the density of the uniform distribution on interval I. A visualization of eq. C.1
can be found in Figure 6.
In practice, we draw samples from each of the client. However, if one of the clients do not have enough
samples, AFL might have an issue. For instance, two clients could have to opposite sample sets:
S1={((0.5,±0.5),1),((−0.5,±0.5),−1)}, S2={((0.5,±0.5),−1),((−0.5,±0.5),1)}.
In this case, AFL could give an unfavorable generalization error, since the optimal training error is 50%. For
example, this optimal AFL solution can be reached if one chooses the linear classifier to be perpendicular
to thex1-axis, resulting in the test error to be 50%. However, there exists an optimal classifier w= (−1,0)
such that the test error is 10%.
We can also verify this claim from the proof of Theorem 1 in Appendix C.2, Mohri et al. (2019). If one of
the clients has too few samples (i.e., some mkis small), then the generalization bound on the right can be
very large or even vacuous.
Note that our PropFair algorithm does not suffer from this generalization problem, since if some client ihas
too few samples, then the corresponding weight pi=ni/Nwill be small, and thus according to equation 3.7
the overall performance will not be heavily affected.
31Published in Transactions on Machine Learning Research (01/2023)
D Additional Experiments
In this section, we provide more details about our experimental results. Results for all experiments are
provided based on an average over three runs with different seeds.
D.1 Datasets and models
We describe the benchmark datasets in this subsection. For all datasets we fix the batch size to be 64.
CIFAR-{10, 100} (Krizhevsky et al., 2009) are standard image classification datasets. There are 50000
samples with 10/100 balanced classes for CIFAR-{10, 100}. By doing Dirichlet allocation (Wang et al.,
2019a) we achieve the heterogeneity of label distributions. For all samples in each class k, denoted as the
setSk, we splitSk=Sk,1∪Sk,2...Sk,nintonclients according a symmetric Dirichlet distribution Dir(β).
Then we gather the samples for client jasS1,j∪S2,j...SC,jif we haveCclasses in total. We note that some
of the clients might have too few samples (a few hundred). In this case the FL algorithm might overfit for
such clients and we regenerate the data split. We choose the number of clients to be 10 for both CIFAR-{10,
100}. For each of the client dataset, we split it further into 80% training data and 20% test data.
TinyImageNet is from the course project of Stanford CS231N.2It contains 200 classes and each class has
500 images. Our FL split setting is the same as CIFAR-{10, 100}, except that we choose 20 clients and the
Dirichlet parameter β= 0.05.
Shakespeare (Shakespeare, 1614; McMahan et al., 2017) is a text dataset of Shakespeare dialogues, and
we use it for the task of next character prediction. We treat each speaking role as a client resulting in a
natural heterogeneous partition. We first filter out the clients with less than 10,000 samples and sample 20
clients from the remaining. Also, each client’s dataset is split into 50% for training and 50% for test.
In Table 3, we summarize these datasets, our partition methods, as well as the models we implement.
Table 3: Details of the experiments and the used datasets. ResNet-18 is the residual neural network defined
in He et al. (2016). GN: Group Normalization (Wu & He, 2018); FC: fully connected layer; CNN: Convolu-
tional Neural Network; Conv: convolution layer; RNN: Recurrent Neural Network; LSTM: Long Short-Term
Memory layer. The plus sign means composition.
Datasets Training set size Test set size Partition method # of clients Model
CIFAR-10 39963 10037 Dirichlet partition ( β= 0.5) 10 ResNet-18 + GN
CIFAR-100 39764 10236 Dirichlet partition ( β= 0.1) 10 ResNet-18 + GN
TinyImageNet 78044 20135 Dirichlet partition ( β= 0.05) 20 ResNet-18 + GN
Shakespeare 178796 177231 realistic partition 20 RNN (1 LSTM + 1 FC)
D.2 Algorithms to compare and tuning hyperparameters
We compare our PropFair algorithm with common FL baselines, including FedAvg (McMahan et al., 2017),
q-FFL (Li et al., 2020c) and AFL (Mohri et al., 2019). For each dataset and each algorithm (algorithms
with different hyperparameters are counted as different), we find the best learning rate from a grid. Here
are the grids we used for each dataset:
•CIFAR-10: {5e-3, 1e-2, 2e-2, 5e-2} ;
•CIFAR-100: {5e-3, 1e-2, 2e-2, 5e-2} ;
•TinyImageNet: {5e-3, 1e-2, 2e-2, 5e-2} ;
2http://cs231n.stanford.edu/
32Published in Transactions on Machine Learning Research (01/2023)
•Shakespeare: {1e-1, 5e-1, 1, 2} ;
Table 4: The best values of hyperparameters used for different datasets, chosen based on grid search.
Algorithm Hyperparameter CIFAR-10 CIFAR-100 TinyImageNet Shakespeare
q-FFL q 0.1 0.1 0.1 0.1
TERM α 0.5 0.5 0.5 0.5
GIFAIR-FL λ/λ max 0.9 0.1 0.1 0.5
FedMGDA + ϵ 0.5 0.05 0.05 0.5
PropFair M 5.0 2.0 2.0 2.0
Table 5: The best learning rates used for different datasets and algorithms, based on grid search.
Datasets FedAvg q-FFL AFL PropFair TERM GIFAIR-FL FedMGDA+
CIFAR-10 5e-3 5e-2 1e-2 5e-2 1e-2 1e-2 1e-2
CIFAR-100 5e-3 2e-2 1e-2 1e-2 5e-3 1e-2 1e-2
TinyImageNet 2e-2 2e-2 2e-2 5e-2 2e-2 2e-2 1e-2
Shakespeare 2 2 2 2 2 2 2
We adapt hierarchical TERM from Li et al. (2020a), with client-level fairness ( α > 0) and no sample-
level fairness ( τ= 0). For each dataset, we tune α(user-level parameter) from {0.01,0.1,0.5}. Table 4
shows the optimal value of αused for different datasets is 0.5. For AFL we tune the learning rate γw
from the corresponding grid and choose the default hyperparameter γλ= 0.1. Forq-FFL, we run the q-
FedAvg algorithm from Li et al. (2020c) with the default Lipschitz constant L= 1/ηfrom where ηis the
learning rate.3For each dataset we tune qfrom{0.1,1.0,5.0}. For all datasets we find q= 0.1has the best
performance. We also find that q= 5often leads to divergence during training.
For PropFair we fix ϵ= 0.2and tuneM(Algorithm 1) from M= 2,3,4,5. Table 4 shows the optimal values
ofMused for different datasets. A rule of thumb is to first take a large M(sayM= 10) and then gradually
reduce this value so as to obtain better performance. Given a learning rate η, we use the learning rate ηϵ
M
when the loss is greater than M−ϵ, andηotherwise.
In addition to the fair FL algorithms in the main text, we compare with two additional baselines in our
appendices: GIFAIR-FL (Yue et al., 2022) and FedMGDA+ (Hu et al., 2022). For GIFAIR-FL we first com-
puteλmaxand choose λfrom{0.1λmax,0.5λmax,0.9λmax}. For FedMGDA+, we choose ϵfrom{0.05,0.1,0.5}
as implemented in Hu et al. (2022). One minor difference is that we fix the global learning rate to be 1.0.
After finding the best hyperparameters for each algorithm, we record the best learning rates in Table 5.
For CIFAR-10/CIFAR-100/TinyImageNet/Shakespeare, we take 100/400/400/100 communication rounds
respectively, in which cases we find most fair FL algorithms converge.
D.3 Detailed results
In Table 6, we report different statistics across clients, for all the algorithms and datasets we study in this
work. These statistical quantities include:
•The mean of test accuracies of all clients;
•The standard deviation of client accuracies;
•The worst test accuracy among the clients;
•The mean of test accuracies across the worst 10% clients;
3https://github.com/litian96/fair_flearn/tree/master/flearn/trainers
33Published in Transactions on Machine Learning Research (01/2023)
•The best test accuracy among the clients.
•The mean of test accuracies across the best 10% clients.
For each algorithm we take three different runs and report the mean and standard deviation of different
statistical indices. In all the experiments we have used 64 as the default batch size. Table 6 shows that
PropFair is comparable with state-of-the-art algorithms across various datasets.
Table 6: Comparison among federated learning algorithms on CIFAR-10, CIFAR-100, TinyImageNet and
Shakespeare datasets with test accuracies (%) from clients. All algorithms are fine-tuned. Mean: the
average of performances across all clients; Std: standard deviation of client test accuracies; Worst/Best :
the worst/best test accuracy from clients; Worst (10%)/Best(10%) : the average of performance across
the worst/best 10% clients. Note that for CIFAR-{10, 100} the worst (best) case accuracy is the same as
the worst (best) 10% accuracy since we have 10 clients.
Dataset Algorithm Mean Std Worst Worst (10%) Best Best (10%)
FedAvg 63.63 ±0.485.38±0.4353.49±1.67 53.49±1.67 72.37±0.5372.37±0.53
q-FFL 57.27 ±0.475.68±0.1647.28±0.26 47.28±0.26 66.71±1.2466.71±1.24
AFL 64.29 ±0.404.48±0.7056.16±1.56 56.16±1.56 71.55±0.8471.55±0.84
CIFAR-10 TERM 63.81 ±0.624.96±0.4256.22±1.24 56.22±1.24 71.51±0.4271.51±0.42
GIFAIR-FL 63.81 ±0.235.05±0.0454.24±1.14 54.24±1.14 72.41±0.8872.41±0.88
FedMGDA+ 61.92 ±0.934.93±0.4452.84±1.12 52.84±1.12 70.42±1.7270.42±1.72
PropFair 64.75 ±0.104.46±0.6358.14 ±0.8958.14 ±0.8972.72 ±2.3572.72 ±2.35
FedAvg 29.94 ±0.814.06±0.3725.26±1.50 25.26±1.50 40.29±0.8540.29±0.85
q-FFL 28.53 ±0.584.53±0.1123.33±0.72 23.33±0.72 39.82±1.0239.82±1.02
AFL 30.33 ±0.273.68±0.4025.49±1.12 25.49±1.12 39.21±0.9839.21±0.98
CIFAR-100 TERM 30.35 ±0.283.50±0.3726.46±0.36 26.46±0.36 39.39±0.9039.39±0.90
GIFAIR-FL 30.63 ±0.373.58±0.1726.99±0.38 26.99±0.38 40.03±0.6240.03±0.62
FedMGDA+ 23.69 ±0.983.52±0.3319.01±0.87 19.01±0.87 32.51±1.8632.51±1.86
PropFair 31.84 ±0.673.10±0.4728.85 ±0.9428.85 ±0.9440.12 ±1.8040.12 ±1.80
FedAvg 16.14 ±0.592.33±0.0711.07±0.78 11.81±0.67 20.23±1.1119.91±0.90
q-FFL 18.84 ±0.023.23±0.2512.12±0.58 13.06±0.6624.19 ±0.2523.69 ±0.19
AFL 16.43 ±0.582.34±0.0411.34±1.24 12.32±0.66 20.70±0.6420.21±0.49
TinyImageNet TERM 16.41 ±0.292.75±0.2710.67±0.47 11.55±0.40 21.75±1.1920.97±0.71
GIFAIR-FL 16.54 ±0.412.70±0.1711.34±0.47 11.92±0.15 22.28±0.4621.47±0.50
FedMGDA+ 13.94 ±0.202.70±0.309.45±0.03 9.73±0.12 19.15±0.0618.62±0.53
PropFair 18.04 ±0.742.69±0.0812.63 ±1.5713.51 ±1.19 23.68±0.4923.02±0.30
FedAvg 50.54 ±0.121.22±0.0748.18±0.17 48.26±0.17 52.33±0.2952.15±0.12
q-FFL 50.69 ±0.141.05±0.0248.74±0.21 48.83±0.22 52.35±0.0852.25±0.13
AFL 52.54 ±0.081.25±0.0549.86 ±0.2950.13 ±0.1254.47 ±0.2954.22 ±0.18
Shakespeare TERM 50.90 ±0.111.27±0.0348.10±0.15 48.45±0.20 52.65±0.3952.47±0.26
GIFAIR-FL 50.67 ±0.281.25±0.0448.22±0.26 48.32±0.30 52.50±0.2452.45±0.21
FedMGDA+ 44.17 ±0.180.99±0.0242.42±0.10 42.67±0.05 46.30±0.2046.10±0.20
PropFair 52.28 ±0.081.20±0.0449.50±0.41 49.76±0.20 54.10±0.1153.88±0.12
D.4 Additional evaluation metrics
Inthissubsection,weperformcomparisonwithbaselinealgorithmsonCIFAR-100usingadditionalevaluating
metrics, including worst 20% and 30% test accuracies. One can see that our algorithm remains the state-of-
the-art among a large variety of algorithms.
34Published in Transactions on Machine Learning Research (01/2023)
Table 7: Comparison using worst 20% and 30% test accuracies on the CIFAR-100 dataset. The hyperpa-
rameters and learning rates are the same as in Table 4 and Table 5.
Metric PropFair AFL FedAvg TERM q-FFL GIFAIR-FL FedMGDA+
worst 20% 29.08 ±0.7726.15±0.6925.68±1.6626.90±0.3323.94±0.51 27.33±0.35 19.77±0.87
worst 30% 29.29 ±0.6326.63±0.2426.26±1.4827.25±0.2524.53±0.53 27.66±0.23 20.33±1.10
E Dual View of Fair FL Algorithms
In this section we derive the convex conjugates of the generalized means for each algorithm. We sometimes
extend the domain of fto obtain a clear form of A∗
φ, while ensuring the equality of eq. 2.10.
E.1 Dual View of FedAvg
For FedAvg, we have φ(t) =tand the generalized mean can be written as:
Aφ(f) =/summationdisplay
ipifi, (E.1)
where we extend the domain of fto beRn. The convex conjugate can be written as:
A∗
φ(λ) = sup
f∈Rn(λ−p)⊤f (E.2)
Solving it yields:
A∗
φ(λ) =/braceleftigg
0ifλ=p,
∞otherwise.(E.3)
Bringing the equation above to eq. 2.10 we obtain the original form of FedAvg.
E.2 Dual View of q-FFL and AFL
Let us now derive the conjugate function for q-FFL. With φ(t) =tq+1(q>0) we have:
Aφ(f) =/parenleftigg/summationdisplay
ipifq+1
i/parenrightigg 1
q+1
, (E.4)
where we assume domf=Rn
+. The convex conjugate can thus be written as:
A∗
φ(λ) = sup
f≥0λ⊤f−/parenleftigg/summationdisplay
ipifq+1
i/parenrightigg 1
q+1
. (E.5)
If/summationtext
ip−1/q
iλ(q+1)/q
i>1, we can take fi=λ1/q
ip−1/q
itand the maximand of eq. E.5 becomes:
λ⊤f−/parenleftigg/summationdisplay
ipifq+1
i/parenrightigg 1
q+1
=/summationdisplay
iλ(q+1)/q
ip−1/q
it−/parenleftigg/summationdisplay
iλ(q+1)/q
ip−1/q
i/parenrightigg 1
q+1
t
=
/summationdisplay
iλ(q+1)/q
ip−1/q
i−/parenleftigg/summationdisplay
iλ(q+1)/q
ip−1/q
i/parenrightigg 1
q+1
t. (E.6)
35Published in Transactions on Machine Learning Research (01/2023)
By takingt→∞we have A∗
φ(λ) =∞. Therefore we must constrain/summationtext
ip−1/q
iλ(q+1)/q
i≤1. In this case, we
can utilize Hölder’s inequality to obtain A∗
φ(λ) = 0. In summary, the convex conjugate for λ≥0is:
A∗
φ(λ) =/braceleftigg
0,if/summationtext
ip−1/q
iλ(q+1)/q
i≤1,
∞otherwise.(E.7)
Takingq→∞the function above becomes the one for AFL:
A∗
φ(λ) =/braceleftigg
0,if∥λ∥1≤1,
∞otherwise.(E.8)
E.3 Dual View of TERM
We continue to derive the convex conjugate of the generalized mean of TERM. Recall that φ(t) =eαtwith
α>0. The generalized mean can be written as:
Aφ(f) =1
αlog/parenleftigg/summationdisplay
ipieαfi/parenrightigg
, (E.9)
where we extend the domain of fto beRn. The convex conjugate is:
A∗
φ(λ) = sup
f∈Rnλ⊤f−1
αlog/parenleftigg/summationdisplay
ipieαfi/parenrightigg
. (E.10)
If anyλi<0, we can take the corresponding fi→−∞and thus A∗
φ(λ) =∞. Ifλ⊤1̸= 1, we can impose
f=t1and obtain:
λ⊤f−1
αlog/parenleftigg/summationdisplay
ipieαfi/parenrightigg
= (λ⊤1−1)t. (E.11)
By takingt→∞ort→−∞we get A∗
φ(λ) =∞. Now let us assume λ≥0andλ⊤1= 1. By requiring
stationarity in eq. E.10 we find the necessary and sufficient optimality condition:
λi=pieαfi
/summationtext
ipieαfi, (E.12)
which can always to satisfied with our assumption. Denote c=/summationtext
ipieαfiwe can solve eq. E.12 to obtain
fi=1
αlog/parenleftig
cλi
pi/parenrightig
. Bringing it back to eq. E.10 the convex conjugate becomes:
A∗
φ(λ) =/summationdisplay
iλi
αlog/parenleftbiggcλi
pi/parenrightbigg
−1
αlogc
=/summationdisplay
iλi
αlogλi
pi, (E.13)
where we used the condition λ⊤1= 1. Since we have the constraint that λ≥0, eq. 2.10 still holds.
Therefore, we get:
A∗
φ(λ) =/braceleftigg/summationtext
iλi
αlogλi
piifλ≥0,λ⊤1= 1,
∞ otherwise.(E.14)
36Published in Transactions on Machine Learning Research (01/2023)
E.4 Dual View of PropFair
Let us derive the dual of the generalized mean for PropFair in the same framework as in Section 2.4. Note
that
φ(t) =−log(M−t), (E.15)
and therefore the generalized mean is:
Aφ(f) =φ−1/parenleftigg/summationdisplay
ipiφ(fi)/parenrightigg
=M−n/productdisplay
i=1(M−fi)pi, (E.16)
where we require f≤M1. We observe that Aφis a convex function, since it is composition of the generalized
geometric mean (which is concave) and affine transformation. Now we compute the dual function
A∗
φ(λ) = sup
f≤M1λ⊤f−Aφ(f)
= sup
f≤M1λ⊤f+n/productdisplay
i=1(M−fi)pi−M (E.17)
If any entry λiis non-positive, clearly we can let fi→−∞so that A∗
φ(λ)→∞. For positive λ, and
/producttextn
i=1/parenleftig
λi
pi/parenrightigpi
<1, we can take fi=M−cpi
λiand get:
λ⊤f+n/productdisplay
i=1(M−fi)pi−M=n/summationdisplay
i=1(Mλi−cpi) +n/productdisplay
i=1/parenleftbiggcpi
λi/parenrightbiggpi
−M
=M(λ⊤1−1) +/parenleftigg/productdisplay
i/parenleftbiggpi
λi/parenrightbiggpi
−1/parenrightigg
c (E.18)
Sincec≥0is arbitrary, we can take c→∞and thus A∗
φ(λ) =∞. Otherwise, if/producttextn
i=1/parenleftig
λi
pi/parenrightigpi
≥1, then we
have:
λ⊤f+n/productdisplay
i=1(M−fi)pi−M=n/productdisplay
i=1(M−fi)pi−λ⊤(M1−f) +M(λ⊤1−1)
≤n/productdisplay
i=1(M−fi)pi−n/productdisplay
i=1/parenleftbiggλi
pi/parenrightbiggpin/productdisplay
i=1(M−fi)pi+M(λ⊤1−1)
≤M(λ⊤1−1), (E.19)
where in the second line we used the AM-GM inequality and in the last line we used/producttextn
i=1/parenleftig
λi
pi/parenrightigpi
≥1. This
equality can always be achieved by taking f=M1. In summary, we have:
A∗
φ(λ) =/braceleftigg
M(λ⊤1−1),ifλ≥0and/producttextn
i=1/parenleftig
λi
pi/parenrightigpi
≥1,
∞, otherwise.(E.20)
We remark that A∗
φis closed (since its domain is closed). If we want to enforce f≥0when computing the
dual function, we simply apply the convolution formula:
A∗
φ(λ) = inf
λ≤γA∗
s(γ). (E.21)
However, the formula for A∗
φsuffices for our purpose so we need not compute the above explicitly.
37Published in Transactions on Machine Learning Research (01/2023)
Applying the above conjugation result we can rewrite PropFair’s generalized mean as:
min
θAφ(f(θ)) = min
θmax
λ≥0λ⊤f(θ)−A∗
φ(λ). (E.22)
We focus on the inner maximization so that we know the weights we put on each client:
max
λ≥0λ⊤f(θ)−A∗
φ(λ) = max
λ≥0,/producttextn
i=1(λi/pi)pi≥1λ⊤f−(λ⊤1−1)M
= max
λ≥0,/producttextn
i=1(λi/pi)pi≥1M−λ⊤(M1−f). (E.23)
Using the AM-GM inequality we have:
λ⊤(M1−f)≥n/productdisplay
i=1/parenleftbiggλi
pi/parenrightbiggpin/productdisplay
i=1(M−fi)pi≥n/productdisplay
i=1(M−fi)pi, (E.24)
where the equality is attained iff/producttextn
i=1/parenleftig
λi
pi/parenrightigpi
= 1and
λi∝pi
M−fi. (E.25)
Thus, we verify again that the optimal value of eq. E.23 is:
M−n/productdisplay
i=1(M−fi)pi=Aφ(f), (E.26)
and we retrieve our original objective. eq. E.25 tells us that we are essentially solving a linearly weighted
combination of f1,...,fn, but with more weights on the worse-off clients, sincepi
M−fiis larger for larger fi.
F More Related Work
In this appendix we introduce more related work, including multi-objective optimization, fairness in FL, as
well as various definitions of fairness from multiple fields.
F.1 Multi-objective optimization
Multi-Objective Optimization (MOO) has been intensively studied in the field of operation research (Geof-
frion, 1968; Yu & Zeleny, 1975; Jahn et al., 2009). The goal of MOO is to minimize a series of objectives
f1,f2,...,fnbased on their best trade-offs. This is directly related to federated learning (Hu et al., 2022)
because one can treat the loss function of each client as an objective.
In MOO, Pareto optimality is often desired. To find a Pareto optimum, one way is to use an aggregating
objective (a.k.a. scalarizing function, Lootsma et al. 1995). We list some common choices of this aggregating
objective:
•Linear weighting method (Geoffrion, 1968): this method converts MOO into the problem of mini-
mizing the convex combination of client objectives:
min
x∈Xn/summationdisplay
i=1λifi(x), (F.1)
withλ∈∆n−1in the (n−1)-simplex, andXthe domain of x. Such solution is always Pareto
optimal and the method has been used in FedAvg (McMahan et al., 2017). A well-known difficulty
is that it cannot generate point in the nonconvex part of the Pareto front (Audet et al., 2008).
38Published in Transactions on Machine Learning Research (01/2023)
•Reference point (Audet et al., 2008): This method requires proximity to the ideal point :r=
(minx∈Xf1(x),..., minx∈Xfn(x)), measured by ℓq-norm:
min
x∈X∥f(x)−r∥q
q:=n/summationdisplay
i=1(fi(x)−ri)q, (F.2)
withf(x) := (f1(x),...,fn(x))and∥·∥qtheℓq-norm (q≥1). This method has been applied to
federated learning as q-FFL (Li et al., 2020c) (by assuming r=0).
•Weighted geometric mean (Lootsma et al., 1995): this method converts MOO to a single-objective
formulation by maximizing the weighted geometric mean between elements of the nadir point and
the client objectives:
max
x∈Xn/productdisplay
i=1(qi−fi(x))λi,such thatfi(x)≤qifor anyiandx∈X, (F.3)
whereqis called a nadir point , defined as (Lootsma et al., 1995):
qi= max
j=1,2,...,nfi(x∗
j), (F.4)
withx∗
j= arg minx∈Xfj(x)the optimizer of function fj. Theλi’s are the weights for each client
and they are positive. If we take λ= (λ1,...,λn) =1, then it resembles our objective in eq. 3.7.
F.2 Fairness in Federated Learning
As FL has been deployed to more and more real-world applications, it has become a major challenge to
guarantee that FL models has no discrimination against certain clients and/or sensitive attributes. Since
different participants may contribute differently to the final model’s quality, it is necessary to provide a fair
mechanism to encourage user participation.
Besides the related work we mentioned in the main paper (McMahan et al., 2017; Mohri et al., 2019; Li
et al., 2020b), another direction of research tries to directly encourage the involvement of user participation,
by providing some rewards to fairly recognize the contributions of clients. For example, Lyu et al. (2020)
designed a local credibility mutual evaluation mechanism to enforce good contributors get more credits.
Concretely, each client computes the contribution of every other client by investigating the label similarities
of the synthetic samples generated by the clients’ differential private GANs (Goodfellow et al., 2014). Kang
et al. (2020) proposed a pairwise measurement of contribution. Reputation scores are kept at each client
for all other clients, and are updated by a multi-weight subjective logic model. Yu et al. (2020) proposed
a Federated Learning Incentivizer (FLI) payoff-sharing scheme, which dynamically divides a given budget
among clients by optimizing their joint utility while minimizing their discrepancy. The objective function
takes into account the amount of payoff and the waiting time to receive the payoff. Wang et al. (2020)
analyzed the contribution from the data side, and proposed the federated Shapley Value (SV) for data
valuation. While preserving the desirable properties of the canonical SV, this federated SV can be calculated
with no extra communication overhead, making it suitable for the FL scenarios.
The above methods already applied some objective functions that reflect the concept of proportional fair-
ness, e.g., payoff proportional to the contribution. However, they mostly apply fixed contribution-reward
assignment rules, without explicit definitions of proportional fairness or theoretical guarantee.
F.3 Definitions of fairness
Fairness has been a perennial topic in social choice (Sen, 1986), communication (Jain et al., 1984), law
(Rawls, 1999) and machine learning (Barocas et al., 2017). Whenever we have multiple agents and limited
resources, we need fairness to allocate the resources. There have been many definitions of fairness, such
as individual fairness (Dwork et al., 2012), demographic fairness, counterfactual fairness and proportional
fairness.
In this section, we introduce definitions of fairness from various perspectives including social choice, com-
munication and machine learning, and study the implications in the setting of FL.
39Published in Transactions on Machine Learning Research (01/2023)
F.3.1 Social Choice and Law
We review some principles for fairness and justice in social choice (Sen, 1986) and law (Rawls, 1999), which
resembles FL: we can treat the shared global model as a public policy and clients as social agents.
•Utilitarian rule (Maskin, 1978): suppose we have nclients and their loss functions are fi, the
utilitarian rule aims to minimize the sum of the loss functions, e.g.,
min
θ/summationdisplay
ifi(θ), (F.5)
withθthe global model parameters. This utilitarian rule represents the utilitarian philosophy: as
long as the overall performance of the whole society is optimal, we call the society to be fair. A
utilitarian policy is Pareto-optimal but not vice versa. With model homogeneity, equation eq. F.5 is
nothing but the objective for FedAvg (McMahan et al., 2017), although the FedAvg algorithm may
not always converge to the global optimum even in linear regression (Pathak & Wainwright, 2020).
•Egalitarian rule (Rawls, 1974; 1999): The egalitarian rule, also known as the maximin criterion
represents egalitarianism in political philosophy. Instead of maximizing the overall performance as
in eq. F.5, an egalitarian wants to maximizing the performance of the worst-case client, i.e., we solve
the following optimization problem:
min
θmax
ifi(θ). (F.6)
This accords with Agnostic FL (Mohri et al., 2019). The egalitarian problem eq. F.6 may not always
be Pareto optimal, e.g., (f1,f2,f3) = (1,1,1)and(f1,f2,f3) = (1,0.9,0.8)can both be the optimal
solution of eq. F.6, but the former is not Pareto optimal.
F.3.2 Fairness in wireless communications
Since resource allocation is common in communication, different notions of fairness have also been proposed
and studied. We review some common fairness definitions in communication:
•Max-min fairness / Pareto optimal (Bertsekas & Gallager, 1987): this definition says at the fair
solution, one cannot simultaneously improve the performance of all clients, which is equivalent to
the definition of Pareto optimal. The corresponding algorithm in FL for finding a Pareto optimum
is FedMGDA+ (Hu et al., 2022).
•Proportional-fair rule (Kelly, 1997; Bertsimas et al., 2011): proportional fairness aims to find a
solutionθ∗such that for all θin the domain:
/summationdisplay
iui(θ)−ui(θ∗)
ui(θ∗)≤0, (F.7)
withuithe utility function of client i, e.g., the test accuracy. This problem aims to find a policy
such that the total relative utility cannot be improved. Proportional fairness has been studied in
communication (e.g. Seo & Lee, 2006) for scheduling but the application in FL has not been seen.
•Harmonic mean (Dashti et al., 2013): the method maximizes the harmonic mean of the utility
functions of each client, that is, we solve the following optimization problem:
max
θn/summationtext
iui(θ)−1(F.8)
In a similar vein we can find its optimality condition, assuming the utility set Uis convex:
n/summationdisplay
i=1ui−u∗
i
(u∗
i)2≤0,for allu∈U. (F.9)
Compared to proportional fairness, it simply amounts to squaring the denominator.
40Published in Transactions on Machine Learning Research (01/2023)
F.3.3 Fairness in machine learning
Fairness has been studied in machine learning for almost a decade (Barocas et al., 2017). A large body
of work focuses on proposing machine learning algorithms for achieving different definitions of fairness.
These definitions are often incompatible with each other, i.e., one cannot achieve two definitions of fairness
simultaneously. Let us review some common definitions, using classification as an illustrating example:
•Group fairness / statistical parity / demographic parity (DP, Dwork et al., 2012; Zemel et al., 2013):
thisdefinitionrequiresthatthepredictionisindependentofthesubgroup(e.g., race, gender). Denote
Yas the prediction and Sas the sensitive attribute, this definition requires Y⊥S, where the symbol
⊥denotes statistical independence. This is the simplest definition of fairness, and probably what
people think of at a first thought. However, this definition can be problematic. For instance,
suppose a subgroup of clients have poor performance (e.g. due to communication, memory), and
then to achieve better group fairness one can deliberately lower the performance of high-performing
clients, and thus the overall performance is lower. Moreover, DP would forbid us to achieve the
optimal performance if the true labels are not independent of the sensitive attribute (Hardt et al.,
2016; Zhao & Gordon, 2019).
•Equalized odds (EO) (Hardt et al., 2016): this defintion requires demographic parity given each
true label class. Define Tas the random variable for the true label. Equalized odds requires that
Y⊥S|TforanyTand equal opportunity requires that Y⊥S|Tforsome T. Different from DP, this
conditioning allows the prediction to align with the true label. In the binary setting, EO and DP
cannot be simultaneously achieved (Barocas et al., 2017).
•Calibration / Predictive Rate Parity (Gebel, 2009): this definition requires that among the samples
having a prediction score Y, the expectation of the true label Tshould match the prediction score,
i.e.,E[T|Y] =Y. In the context of fairness, calibration says that T⊥S|Y. Under mild assumptions,
calibration and EO cannot be simultaneously achieved (Pleiss et al., 2017). Similarly, calibration
and DP cannot be simultaneously achieved.
•Individual fairness (Dwork et al., 2012): this concept requires that similar samples, as measured by
some metric, should have similar predictions.
•Counterfactual fairness (Kusner et al., 2017): this definition requires that from any sample, the
prediction should be the same had the sensitive attribute taken different values. It follows the
notion of counterfactual from casual inference (Pearl, 2000).
•Accuracy parity (Zafar et al., 2017): the accuracy for each group remains the same.
Since many concepts conflict with each other (Barocas et al., 2017), there is no unified definition of fairness.
In light of this, a dynamical definition of fairness has been proposed (Awasthi et al., 2020). Algorithms
for achieving different definitions of fairness include mutual information (Zemel et al., 2013), representation
learning (Zemel et al., 2013; Zhao & Gordon, 2019) and Rényi correlation (Baharlouei et al., 2019).
41