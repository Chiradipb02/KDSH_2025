Published in Transactions on Machine Learning Research (01/2023)
Defense Against Reward Poisoning Attacks in Reinforce-
ment Learning
Kiarash Banihashem∗kiarash@umd.edu
University of Maryland
Adish Singla adishs@mpi-sws.org
Max Planck Institute for Software Systems
Goran Radanovic gradanovic@mpi-sws.org
Max Planck Institute for Software Systems
Reviewed on OpenReview: https: // openreview. net/ forum? id= goPsLn3RVo
Abstract
We study defense strategies against reward poisoning attacks in reinforcement learning.
As a threat model, we consider cost-eﬀective targeted attacks—these attacks minimally
alter rewards to make the attacker’s target policy uniquely optimal under the poisoned
rewards, with the optimality gap speciﬁed by an attack parameter. Our goal is to design
agents that are robust against such attacks in terms of the worst-case utility w.r.t. the
true, unpoisoned, rewards while computing their policies under the poisoned rewards. We
propose an optimization framework for deriving optimal defense policies, both when the
attack parameter is known and unknown. For this optimization framework, we ﬁrst provide
characterization results for generic attack cost functions. These results show that the
functional form of the attack cost function and the agent’s knowledge about it are critical
for establishing lower bounds on the agent’s performance, as well as for the computational
tractability of the defense problem. We then focus on a cost function based on /lscript2norm,
for which we show that the defense problem can be eﬃciently solved and yields defense
policies whose expected returns under the true rewards are lower bounded by their expected
returns under the poison rewards. Using simulation-based experiments, we demonstrate the
eﬀectiveness and robustness of our defense approach.
1 Introduction
One of the key challenges in designing trustworthy AI systems is ensuring that they are technically robust and
resilient to security threats European Commission (2019). Amongst many requirements that an AI system
ought to satisfy in order to be deemed trustworthy is robustness to adversarial attacks Hamon et al. (2020).
Standard approaches to reinforcement learning (RL) Sutton & Barto (2018) have shown to be susceptible to
adversarial attacks which manipulate the feedback that an agent receives from its environment. These attacks
broadly fall under two categories: a) test-time attacks, which manipulate an agent’s input data at test-time
without changing its policy Huang et al. (2017); Lin et al. (2017); Tretschk et al. (2018); Kumar et al. (2021),
and b)training-time attacks that manipulate an agent’s input data at training-time, inﬂuencing the agent’s
learned policy Zhang & Parkes (2008); Ma et al. (2019); Huang & Zhu (2019); Rakhsha et al. (2020; 2021);
Zhangetal.(2020b);Sunetal.(2020);Liu&Lai(2021);Rangietal.(2022). Inthispaper, wefocusontraining-
time attacks, and more speciﬁcally, on targeted reward poisoning attacks that modify (i.e., poison) rewards
to force an agent into adopting a targetpolicy Ma et al. (2019); Rakhsha et al. (2021); Rangi et al. (2022).
Prior work on reward poisoning attacks on RL primarily focuses on designing optimal attacks. In this paper,
∗This work was done as a part of an internship project at Max Planck Institute for Software Systems.
1Published in Transactions on Machine Learning Research (01/2023)
we take a diﬀerent perspective on targetedreward poisoning attacks, and focus on designing defense strategies
against such attacks. This is challenging, given that the attacker is typically unconstrained in poisoning the
rewards to force the target policy, while the agent’s performance is measured under the true reward function,
which is unknown. The key idea that we exploit in our work is that the poisoning attacks have an underlying
structure arising from the attacker’s objective to minimize the cost of the attack needed to force the target
policy. We therefore ask the following question:
Can we design an eﬀective defense strategy against targeted reward poisoning attacks by exploiting the cost-
eﬀective nature of these attacks?
s0s1s2s3
01 01 1 0 1 0-2.50-2.50
0.500.50
0.500.50
-0.50-0.50
(a)R,π∗s0s1s2s3
01 01 01 01-2.50-2.50
0.500.13
0.070.61
-0.500.19
(b)/hatwideR,π†
s0s1s2s3
01 01 0.940.06 01-2.50-2.50
0.500.13
0.070.61
-0.500.19
(c)/hatwideR,πDMDPPolicyπ∗π†πD
R 0.34-0.420.03
/hatwideR -0.030.110.03
(d) Scores of policies π∗,π†,πDin MDPs w.r.t. R,/hatwideR
Figure 1: A simple chain environment with 4 states and two possible actions: leftandright.s0is the initial
state. The agent goes in the direction of its action with probability 90%, and otherwise the next state is
selected uniformly at random from the other 3 states. Weights on edges indicate rewards for the action taken.
For example in Fig. 1a, if the agent takes leftin states1, it receives 0.5. We denote the true rewards by R,
the poisoned rewards by /hatwideR, the optimal policy under Rbyπ∗, the target policy (which is uniquely optimal
under/hatwideR) byπ†, and the defense policy (which is derived from our framework) by πD.(a)showsRand
π∗. In particular, the numbers above the arrows and the diﬀerent shades of gray show the probabilities of
taking actions leftandrightunderπ∗.(b)shows/hatwideRandπ†.(c)showsπDthat our optimization framework
derived from/hatwideR, and by reasoning about the goal of the attack ( π†). In particular, our optimization framework
maximizes the worst-case performance under R: while the optimization procedure does not know R, it can
constrain the set of plausible candidates for Rusing/hatwideR.(d)Table. 1d): Each entry in the table indicates the
score of a (policy, reward function) pair, where the score is a scaled version of the total discounted return
(see Section 3). For example, the score of policy π†equals−0.42and0.11under/hatwideRandRrespectively. Our
defense policy signiﬁcantly improves upon this and achieves a score of 0.03. For comparison, the score of π∗
equals 0.34. Moreover, unlike for the target policy π†, the score of our defense policy πDunderRis always at
least as high as its score under /hatwideR, as predicted by our results (see Theorem 5.1). The results are obtained
with parameters /epsilon1†= 0.1,/epsilon1D= 0.2andγ= 0.99(see Section 3).
In this paper, we study this question in depth and under diﬀerent assumption on the agent’s knowledge about
the attack cost function. Perhaps surprisingly, the answer to this question is sometimes aﬃrmative. While an
agent only has access to the poisoned rewards, it may still be able infer some information about the true
reward function, using the fact that the attack is cost-eﬀective. By maximizing the worst-case utility over the
set of plausible candidates for the true reward function, the agent can substantially limit the inﬂuence of the
attack. The approach we study can be understood from Figure 1 which uses the chain environment from
Rakhsha et al. (2021) to demonstrate the main ideas.
Contributions. We formalize this reasoning, and characterize the utility of our novel framework for designing
defense policies. In summary, the key contributions include:
•We formalize the problem of designing defense policies against targeted and cost-eﬀective reward poisoning
attacks, which minimally modify the original reward function to achieve their goal (force a target policy).
2Published in Transactions on Machine Learning Research (01/2023)
•We introduce a novel optimization framework for ﬁnding optimally robust defense policies—this framework
focuses on optimizing the agent’s worst-case utility among the set of reward functions that are plausible
candidates of the true reward function.
•We provide characterization results that establish feasibility and computational complexity of ﬁnding
optimally robust defense policies for diﬀerent classes of attack cost functions. These results show that
the functional form of the attack cost function and the agent’s knowledge about it play a critical role
in deriving optimally robust defense policies with provable performance guarantees.
•Focusing on a cost function based on /lscript2norm, we show that optimally robust defense policies can be
eﬃciently computed. We further establish lower bounds on the true performance of defense policies derived
from our framework and computable from the poisoned rewards.
•We empirically demonstrate the eﬀectiveness and robustness of our approach using numerical simulations.
To our knowledge, this is the ﬁrst framework for studying this type of defenses against reward poisoning
attacks that try to force a target policy at a minimal cost.
2 Related Work
While this paper is broadly related to the literature on adversarial machine learning (e.g., Huang et al. (2011)),
we recognize four themes in supervised learning (SL) and reinforcement learning (RL) that closely connect to
our work.
Poisoning attacks in SL and RL. This paper is closely related to data poisoning attacks, ﬁrst introduced
and extensively studied supervised learning Biggio et al. (2012); Xiao et al. (2012); Mei & Zhu (2015); Xiao
et al. (2015); Li et al. (2016); Koh & Liang (2017); Biggio & Roli (2018). These attacks are also called
training-time attacks , and unlike test time attacks Szegedy et al. (2014); Pinto et al. (2017); Behzadan &
Munir (2017); Zhang et al. (2020a); Moosavi-Dezfooli et al. (2016); Nguyen et al. (2015); Madry et al. (2018),
which attack an already trained agent, they change data points during the training phase, which in turn
aﬀects the parameters of the learned model. Data poisoning attacks have also been studied in the bandits
literature Jun et al. (2018); Ma et al. (2018); Liu & Shroﬀ (2019) and in RL (see Section 1).
Defenses against poisoning attacks in SL. In supervised learning, defenses against data poisoning
attacks are often based on data sanitization that removes outliers from the training set Cretu et al. (2008);
Paudice et al. (2018), trusted data points that support robust learning Nelson et al. (2008); Zhang et al.
(2018), or robust estimation Charikar et al. (2017); Diakonikolas et al. (2019). Recently, Wu et al. (2022)
have considered aggregation based defenses that can certify an RL agent’s policy against a limited number of
changes in the training dataset. While such defenses can mitigate some attack strategies, they are in general
susceptible to data poisoning attacks Steinhardt et al. (2017); Koh et al. (2018).
Robustness to model uncertainty. There is a rich literature that studies robustness to uncertainties in
reward functions McMahan et al. (2003); Regan & Boutilier (2010), and transition models Nilim & El Ghaoui
(2005); Iyengar (2005); Bagnell et al. (2001) for MDP models. Typically, these works consider settings
in which instead of knowing the exact parameters of the MDP, the agent has access to a set of possible
parameters (uncertainty set). These works design policies that perform well in the worst case. More recent
works have proposed ways to scale up these approaches via function approximation Tamar et al. (2014), as
well as utilize them in online settings Lim et al. (2013). While our work uses the same principles of robust
optimization, we do not assume that the uncertainty set, i.e., the set of all possible rewards, is directly given.
Instead, we show how to derive it from the poisoned reward function.
Robustness to corrupted episodes. Another important line of work is the literature on robust learners
that receive corrupted input during their training phase. Such learners have recently been designed for
bandits and experts settings Lykouris et al. (2018); Gupta et al. (2019); Bogunovic et al. (2020); Amir et al.
(2020), and episodic reinforcement learning Lykouris et al. (2019); Zhang et al. (2021). Typically, these works
consider an attack model in which the adversary can arbitrarily corrupt a limited number of episodes. As we
operate in the non-episodic setting and do not assume a limit in the attacker’s poisoning budget, these works
are orthogonal to the aspects we study in this paper. Instead, we utilize the structure of the attack in order
to design a defense algorithm.
3Published in Transactions on Machine Learning Research (01/2023)
3 Formal Setting
In this section, we describe our formal setting, and identify relevant background details on reward poisoning
attacks, as well as our problem statement. The problem formulation speciﬁes our objectives that we establish
and formally analyze in the next sections.
3.1 Preliminaries
We consider a standard reinforcement learning setting in which the environment is described by a discrete-time
discounted Markov Decision Processes (MDP) Puterman (1994), deﬁned as M= (S,A,R,P,γ,σ ), where:
Sis the state space, Ais the action space, R:S×A→Ris the reward function, P:S×A×S→[0,1]
is the transition model with P(s,a,s/prime)deﬁning the probability of transitioning to state s/primeby taking action
ain states,γ∈[0,1)is the discount factor, and σis the initial state distribution. We consider state and
action spaces, i.e., SandA, that are ﬁnite and discrete, and due to this we can adopt a vector notation for
quantities dependent on states or state-action pairs. W.l.o.g., we assume that |A|≥2.
A generic (stochastic) policy is denoted by π, and it is a mapping π:S→P(A), whereP(A)is the probability
simplex over action space A. We useπ(a|s)to denote the probability of taking action ain states. While
deterministic policies are a special case of stochastic policies, when explicitly stating that a policy πis
deterministic, we assume that it is a mapping from states to actions, i.e., π:S→A. We denote the set of
all policies by Πand the set of all deterministic policies by Πdet. For policy π, we deﬁne its score,ρπ, as
E/bracketleftbig
(1−γ)/summationtext∞
t=1γt−1R(st,at)|π,σ/bracketrightbig
, where state s1is sampled from the initial state distribution σ, and then
subsequent states stare obtained by executing policy πin the MDP. The score of a policy is therefore its
total expected return scaled by a factor of 1−γ.
Finally, we consider occupancy measures. We denote the state-action occupancy measure in the Markov
chain induced by policy πbyψπ(s,a) =E/bracketleftbig
(1−γ)/summationtext∞
t=1γt−11[st=s,at=a]|π,σ/bracketrightbig
. Given the MDP M, the
set of realizable state-action occupancy measures under any (stochastic) policy π∈Πis denoted by Ψ. Score
ρπandψπsatisfyρπ=/angbracketleftψπ,R/angbracketright, where/angbracketleft.,./angbracketrightcomputes the dot product between two vectors of sizes |S|·|A|.
We denote by µπ(s) =E/bracketleftbig
(1−γ)/summationtext∞
t=1γt−11[st=s]|π,σ/bracketrightbig
the state occupancy measure in the Markov chain
induced by policy π∈Π. State-action occupancy measure ψπ(s,a)and state occupancy measure µπ(s)satisfy
ψπ(s,a) =µπ(s)·π(a|s). We focus on ergodicMDPs, which in turn implies that µπ(s)>0for allπands
Puterman (1994). This is a standard assumption in this line of work (e.g, see Rakhsha et al. (2021)) and is
used to ensure the feasibility of the attacker’s optimization problem.
3.2 Reward Poisoning Attacks
We consider reward poisoning attacks on an oﬄine learning agent that optimally change the original reward
function with the goal of deceiving the agent to adopt a deterministic policy π†∈Πdet, calledtarget policy .
This type of attack has been extensively studied in the literature, and here we utilize the attack formulation
based on the works of Ma et al. (2019); Rakhsha et al. (2020; 2021); Zhang et al. (2020b). In the following,
we introduce the necessary notation, the attacker’s model, and the agent’s model (without defense).
Notation. We useMto denote the trueororiginalMDP with true, unpoisoned, reward function R, i.e.,
M= (S,A,R,P,γ,σ ). We use/hatwiderMto denote the modiﬁed orpoisoned MDP with poisoned reward function /hatwideR,
i.e.,/hatwiderM= (S,A,/hatwideR,P,γ,σ ). Note that only the reward function Rchanges across these MDPs. Quantities
that depend on reward functions have analogous notation. For example, the score of policy πunderRis
denoted by ρπ, whereas its score under /hatwideRis denoted by /hatwideρπ. We denote an optimal policy under Rbyπ∗, i.e.,
π∗∈arg maxπ∈Πρπ.
Attack model. The attacker we consider in this paper has full knowledge of M. It can be modeled by a
functionA(c,R/prime,π†,/epsilon1†)that returns a set of poisoned rewards functions for a given attack cost function c,
reward function R/prime, target policy π†, and a desired attack parameter /epsilon1†.1In particular, the attack problem is
deﬁned by the following optimization problem:
min
Rc(R,R/prime)s.t.ρπ†≥ρπ+/epsilon1†∀π∈Πdet\{π†}. (P1)
1For cost functions cpdeﬁned by (1) with ﬁnite p>1, this set has a single element.
4Published in Transactions on Machine Learning Research (01/2023)
A common class of cost functions are /lscriptp-norms of manipulations Ma et al. (2019); Rakhsha et al. (2020; 2021),
i.e.,
c(R,R/prime) =cp(R,R/prime) =/bardblR−R/prime/bardblp, (1)
withp≥1. As shown by Rakhsha et al. (2021), the attack problem (P1)is feasible for this class of cost
functions and ergodic MDPs. Furthermore, instead of considering all deterministic policies, it is suﬃcient to
consider policies that diﬀer from π†in a single action. Using π†{s;a}to denote a policy that follows a/negationslash=π†(s)
in statesandπ†(˜s)in states ˜s/negationslash=s, (P1) can be rewritten as follows:
min
Rc(R,R/prime)s.t.ρπ†≥ρπ†{s;a}+/epsilon1†∀s,a/negationslash=π†(s). (P1’)
The equivalence between (P1)and(P1’)is shown by Rakhsha et al. (2021), but we also provide further
details in Appendix. Intuitively, (P1)and(P1’)are equivalent because in order for a policy to be optimal it
is suﬃcient (and necessary) that the policy is better than any of its neighbor policies. This fact allows us to
reduce the number of constraints. Whereas the number of constraints in (P1)is exponential in |S|and|A|,
the number of constraint in (P1’) is polynomial in |S|and|A|, which in turn implies that (P1’) is tractable
(for a ﬁxed p).
To better understand the optimization problem (P1’), we can consider /lscript2attack cost (i.e., c2) deﬁned as the
Euclidean distance between R/primeandR. By solving this problem, i.e., setting /hatwideR∈A(c2,R,π†,/epsilon1†), the attacker
ﬁnds the closest reward function to Rfor whichπ†is a uniquely optimal policy (with attack parameter /epsilon1†).
Remark 3.1.Note that the optimization problem (P1)may not be feasible if we lift the assumption that
underlying MDP is ergodic. This can be seen from the constraints of the optimization problem (P1’): if
target policy π†does not visit a certain state s, then it has the same score as its neighbor policies π†{s;a}.
Hence, the primary reason for assuming ergodicity is to make the attack problem feasible.
Agent without defense. The agent receives the poisoned MDP /hatwiderM:= (S,A,/hatwideR,P,γ,σ )where the underlying
true reward function R(unknown to the agent) has been poisoned to /hatwideR. In the existing works on reward
poisoning attacks, an agent naively optimizes score /hatwideρ(score w.r.t. /hatwideR). Because of this, the agent ends up
adopting policy π†.
3.3 Problem Statement
Perhaps unsurprisingly, the agent without defense, could perform arbitrarily badly under the true reward
functionR. Our goal is to design a robust agent that derives its policy using the poisoned MDP /hatwiderM:=
(S,A,/hatwideR,P,γ,σ ), but has provable worst-case guarantees w.r.t. R. This agent has access to the poisoned
reward vector /hatwideR∈A(c,R,π†,/epsilon1†), butR,π†, and/epsilon1†are not given to the agent. Figure 2 illustrates a generic
problem setting studied in this paper. In general, the agent does not know c, but is given a class of cost
functionsCthat contains c, i.e.,c∈C.Crepresents’ the agent’s knowledge about the attack cost function;
in a special case when Ccontains only one element, the agent knows the cost function. Notice that π†is
obtainable by solving the optimization problem arg maxπ/hatwideρπasπ†is uniquely optimal in /hatwiderM. On the other
hand,Ris unknown to the agent. In terms of /epsilon1†, we will focus on two cases, the case when /epsilon1†is known to
the agent, and the case when it is not.
Intheﬁrstcase, wecanformulatethefollowingoptimizationproblemofmaximizingtheworstcaseperformance
of the agent, given that Ris unknown:
max
πmin
R,c∈Cρπs.t./hatwideR∈A(c,R,π†,/epsilon1†). (P2a)
In other words, we calculate the set of all possible reward functions Rsuch that an attack on Rcouldlead to
the solution/hatwideR, i.e.,/hatwideR∈A(c,R,π†,/epsilon1†). We then ﬁnd a policy πfor which the worst-case score, i.e., minR,cρπ
is maximized, where the minimum is over all reward functions Rcalculated previously and all cost functions
c∈C.
5Published in Transactions on Machine Learning Research (01/2023)
True reward function 𝑅"Poisoned reward function 𝑅#
Attack 𝒜(cost 𝑐, target 𝜋')
Defense 𝒟(score under 𝑅")Defense policy 𝜋𝒟
Poisoned reward function 𝑅#
Defense policy 𝜋𝒟
Find the set of reward functions consistent with 𝑅#and 𝒜
Find a policy that maximizes the worst-case score over this set
Figure 2: The problem setting studied in this paper. The attack Amodiﬁes the original (true) reward
functionRto forceπ†while minimizing its cost. The defense Daims to optimize the agent’s score under R,
but it only sees the poisoned reward /hatwideR. Nevertheless, it can (in principle) ﬁnd the set of all reward functions
consistent with /hatwideRandA, and search for a policy that maximizes the worst-case score of the agent over this
consistent set.
Knowledge about attack cost Guarantees on the value Complexity
GeneralC(e.g., s.t.cconst∈C)Nofor any/hatwideR, Proposition 4.2 —
C={/bardblR−R/prime/bardblps.t.p∈[1,∞)} Yes, Theorem 4.4 NP-hard , Theorem 4.5
C={/bardblR−R/prime/bardbl∞} Nofor some/hatwideR, Theorem 4.3 NP-hard , Appendix
C={/bardblR−R/prime/bardbl2} Yes, Section 5 Convex, Section 5
C={/bardblR−R/prime/bardbl1} Yes, Appendix Convex, Appendix
Table 1: Characterization results for diﬀerent levels of the agent’s knowledge about the attack cost function,
expressed through C. In general, ifCcan be arbitrary, the optimization problem (P2a)may be unbounded
from below regardless of /hatwideR. For some classes C, e.g., that contain /lscriptp-norm attack costs ( p/negationslash=∞),(P2a)has
the optimal solution, but this solution may be computationally hard. When the attack cost function is known,
properties of problem (P2a)depend on the functional form of the attack cost function, as indicated by the
/lscript1-norm,/lscript2-norm, and /lscript∞-norm attack costs.
For the case when the agent does not know /epsilon1†, we use the following optimization problem:
max
πmin
R,/epsilon1,c∈Cρπs.t./hatwideR∈A(c,R,π†,/epsilon1)and0</epsilon1≤/epsilon1D. (P2b)
where the agent uses /epsilon1Das an upper bound on /epsilon1†. We denote solutions to the optimization problems (P2a)
and(P2b)byπD, and it will be clear from the context which optimization problem we are referring to with
πD.
Remark 3.2.We note that some structural assumptions on the attack model are needed to guarantee
robustness. For example, prior work typically considers untargeted attacks with budget constraints, e.g.,
that put an upper limit on the cost of the attack or the number of episodes in which the attacker can attack
Lykouris et al. (2019); Zhang et al. (2021). This paper focuses on targeted attacks that minimize the cost of
the attack. Given the strategic nature of the attacker, the structural assumptions on the attack model that
(P2a) and (P2b) rely on are fairly natural.
4 Characterization Results for Generic Attack Cost
In this section, we provide characterization results showing the importance of the agent’s knowledge about
the attack cost function. The overview of the characterization results is shown in Table 1. To corresponding
proofs can be found in Appendix.
Remark 4.1.The results presented in this section are stated for the optimization problem (P2a). However,
the same results also hold for the optimization problem (P2b).
4.1 General Attack Cost
We start by stating what is perhaps an expected result: if the attack cost function can be arbitrary, then no
defense can achieve any provable guarantee. It is relatively easy to see why this claim should hold. If the
6Published in Transactions on Machine Learning Research (01/2023)
agent believes that the attack cost function can be constant, then from the agent’s perspective, Rcan be any
reward function. Since rewards are not bounded, this in turn implies that no matter which policy the agent
selects, no worst-case guarantees are possible. More formally, we obtain the following claim.
Proposition 4.2. Letcconst(R,R/prime)be a constant cost function, and assume that cconst∈C. Then the
optimization problem (P2a)is unbounded from below.
Given this result, it is clear that for provable defenses: a) the attack cost function cannot not be arbitrary
in that/hatwideRhas to be informative about R; b) the agent should have some knowledge about the attack cost
function. In the next subsection, we consider cost functions based on /lscriptpnorms, i.e., cp, commonly adopted
by prior work on reward poisoning attacks Ma et al. (2019); Rakhsha et al. (2020; 2021).
4.2/lscriptpAttack Cost
In contrast to constant cost functions, the strategic nature of the attacker is more apparent when it optimizes
cp, so the agent may infer some information about the true reward function Rfrom the poisoned rewards /hatwideR
which it can access. Our ﬁrst result shows that for p=∞the success of such an inference procedure depends
on/hatwideR. This is formally captured by the following theorem.
Theorem 4.3. There exists an instance of the problem setting, i.e., MDP M= (S,A,/hatwideR,P,γ,σ )for which
the optimization problem (P2a)is unbounded from below when c∞∈C.
This theorem paints a relatively bleak picture for the possibility of achieving provable guarantees. Note two
important observations. First, the impossibility result is a weaker variant of the result stated in Proposition
4.2 as it holds only for some MDPs. Second, c∞is measuring the maximum modiﬁcation of reward function
R, so critical information about Rmay be lost—this also provides intuition behind the impossibility results.
In contrast, when p/negationslash=∞,cpis aﬀected by all the modiﬁcations of R. In fact, when pis restricted to take
values in [1,∞), a lower bound on the optimal value of (P2a) can always be derived from /hatwideR.
Theorem 4.4. Consider any policy π/negationslashπ†s.t.π/negationslashπ†(π†(s)|s) = 0. ForC={cps.t.p∈[1,∞)}, the optimal
value of the optimization problem (P2a)is bounded from below by /hatwideρπ/negationslashπ†.
A direct consequence of Theorem 4.4 is that the optimal solution to (P2a)always exists, and its worst case
performance under Ris at least/hatwideρπ/negationslashπ†. Note that we can easily ﬁnd a (deterministic) policy π/negationslashπ†that maximizes
the lower bound by solving maxπs.t.π(s)/negationslash=π†(s)/hatwideρπ.2Furthermore, for any policy π/negationslashπ†s.t.π/negationslashπ†(π†(s)|s) = 0we
have thatρ/negationslashπ†≥/hatwideρ/negationslashπ†. This means that we can eﬃciently ﬁnd a defense policy with provable performance
guarantees. However, such a defense policy may not be optimally robust in that its performance lower bound
would not match the optimal one. We now turn to computational complexity challenges in deriving optimally
robust defense policies: the next theorem provides a hardness result for C={cps.t.p∈[1,∞)}, which
admits guarantees on the optimal solution to (P2a).
Theorem 4.5. ForC={cps.t.p∈[1,∞)}, it is NP-hard to determine whether the optimal value of the
optimization problem (P2a)is greater than or equal to /hatwideρπ†.
Given that even for this natural choice of cost functions, C={cps.t.p∈[1,∞)}, the problem of ﬁnding
optimally robust policies is computationally hard, in the next section we focus on /lscript2attack cost that is known
to the agent, i.e., C={c2}. In Appendix, we provide similar analysis for /lscript1attack cost, i.e.,C={c1}.
5 Characterization Results for /lscript2Attack Cost
In this section, we consider the attack cost function c2, and assume that it is known to the agent ( C={c2}).
In the ﬁrst part, we focus on characterization results for the case when the attack parameter /epsilon1†is known to
the agent, i.e., the optimization problem (P2a). In the second part, we focus on the optimization problem
(P2b), and generalize the results from the ﬁrst part to the unknown attack parameter setting. The proofs of
our formal results can be found in Appendix. Figure 3 provides intuition behind the results of this section.
2Stochastic policies are not necessary in this case since we can think of this problem as searching for an optimal policy over a
truncated actions space (because actions π†(s)are not admissible), so an optimal deterministic policy always exists.
7Published in Transactions on Machine Learning Research (01/2023)
In particular, it illustrates attack and defense strategies for a single-state MDP with action set {a1,...a 7}and
the/lscript2cost function. We refer the reader to Appendix for more details.
a1a2a3a4a5a6a70246810
ActionsR
π∗1000000
(a)R,π∗a1a2a3a4a5a6a70246810
Actions/hatwideR
π†0001000
(b)/hatwideR,π†a1a2a3a4a5a6a70246810
Actions/hatwideR
πD1
3001
3001
3
(c)/hatwideR,πD
Figure 3: A single-state environment environment with 7 actions and state s. In each ﬁgure, the denoted
policy is uniform over actions on or above the dashed line. (a)showsRandπ∗. Here, the optimal policy
selects action a1.(b)shows/hatwideRand target policy π†with/epsilon1†= 1. Here, the target policy selects action a4./hatwideR
is obtained by solving (P1)(or(P1’)) with the/lscript2cost function and R/prime=R. In this case, the attack only
modiﬁes the rewards of three actions, a1,a4, anda7.(c)shows/hatwideRandπDwith/epsilon1D= 2. The defense strategy
only sees poisoned rewards /hatwideR. It ﬁrst calculates the optimal action and the set of all second-best actions
under/hatwideR, in this case{a1,a7}. If the reward of the second-best actions are no worse than /epsilon1D, they form the
setΘ/epsilon1={(s,a1),(s,a7)}or simply Θ/epsilon1
s={a1,a7}. We show in Appendix that the defense strategy, i.e., the
solution to (P2b), selects an action uniformly at random from the set {π†(s)}∪Θ/epsilon1
s={a1,a4,a7}. As we
show in this section, the expected reward of the defense policy under Ris at least as much as its expected
reward under /hatwideR, which can be easily veriﬁed from the ﬁgures: in this case, both are equal to 22/3.
5.1 Known Parameter Setting
We begin by analyzing the optimization problem (P2a). Denote by Θ/epsilon1state-action pairs (s,a)for which the
diﬀerence between /hatwideρπ†and/hatwideρπ†{s;a}is equal to /epsilon1, i.e., Θ/epsilon1=/braceleftbig
(s,a) :/hatwideρπ†{s;a}−/hatwideρπ†=−/epsilon1/bracerightbig
.3For the results
of this section, Θ/epsilon1with/epsilon1=/epsilon1†plays a critical role—as we show in our formal analysis, it characterizes the
feasible set of the optimization problem (P2a). In particular, in our analysis we show that /hatwideRis the solution
to the attack problem for an underlying reward function R, i.e.,/hatwideR=A(R,π†,/epsilon1†), if and only if Rcan be
expressed as
R=/hatwideR+/summationdisplay
(s,a)∈Θ/epsilon1†αs,a·/parenleftBig
ψπ†{s;a}−ψπ†/parenrightBig
,
withαs,a≥0. To see the importance of this result, let us instantiate R=Rand calculate ρπ:
ρπ=/angbracketleftbig
ψπ,R/angbracketrightbig
=/angbracketleftBig
ψπ,/hatwideR/angbracketrightBig
+/summationdisplay
(s,a)∈Θ/epsilon1†αs,a·/angbracketleftBig
ψπ†{s;a}−ψπ†,ψπ/angbracketrightBig
.
When the occupancy measure of πDis positively aligned with vectors ψπ†{s;a}−ψπ†, the performance of
πDunder the original reward function Ris at least/hatwideρπD=/angbracketleftBig
ψπD,/hatwideR/angbracketrightBig
. Hence, constraining πDto satisfy
/angbracketleftbig
ψπ†{s;a}−ψπ†,ψπD/angbracketrightbig
≥0for alls,a∈Θ/epsilon1†yields a guarantee on the score ρπD, i.e.,ρπD≥/hatwideρπD. These
insights are formalized by Theorem 5.1, which also describes a procedure for solving (P2a). In Appendix, we
provide intuition behind our analysis using a special case of our setting.
3In practice, Θ/epsilon1should be calculated with some tolerance due to numerical imprecision (See Section 6).
8Published in Transactions on Machine Learning Research (01/2023)
Theorem 5.1. Consider the following optimization problem parameterized by /epsilon1:
max
ψ∈Ψ/angbracketleftBig
ψ,/hatwideR/angbracketrightBig
s.t./angbracketleftBig
ψπ†{s;a}−ψπ†,ψ/angbracketrightBig
≥0∀s,a∈Θ/epsilon1. (P3)
For/epsilon1=/epsilon1†, this optimization problem is always feasible, and its optimal solution ψmaxspeciﬁes an optimal
solution to the optimization problem (P2a)forC={c2}with
πD(a|s) =ψmax(s,a)/summationtext
a/primeψmax(s,a/prime). (2)
The score of πD(a|s)is lower bounded by ρπD≥/hatwideρπD.
In addition to providing a characterization of the solution to (P2a), the above theorem provides an eﬃcient
algorithm for ﬁnding this solution using linear programming. As we discuss in Appendix D.3, the set of
vectorsψπ†andψπ†{s;a}can be precomputed since the agent is given a poisoned model /hatwiderM(and hence,
knows the transition probabilities); for any policy π, the state-action occupancy measure ψπsatisﬁes
ψπ(s,a) =µπ(s)·π(a|s)where the state occupancy measure µπis the unique solution to the following
Bellman identity
µπ(s) = (1−γ)σ(s) +γ/summationdisplay
˜s,˜aµπ(˜s)π(˜a|˜s)P(˜s,˜a,s).
In addition, the set of valid occupancy measures Ψis the set of all vectors ψ∈R|S|·|A|satisfying the following
Bellman ﬂow constraints
∀s:/summationdisplay
aψ(s,a) = (1−γ)σ(s) +/summationdisplay
˜s,˜aγ·P(˜s,˜a,s)·ψ(˜s,˜a),
∀(s,a) :ψ(s,a)≥0.
Therefore, since occupancy measures ψπ†{s;a}andψπ†can be precomputed, the optimization problem (P3)
can be eﬃciently solved using linear programming. In other words, computing optimally robust defense
policy is computationally tractable. In order to calculate each µπ, we need to solve a |S|×|S|system
of linear equations, which requires |S|3operations in the worst case. Therefore, ﬁnding each ψπfor
π∈{π†}∪{π†{s;a}:a/negationslash=π†(s)}requires at most |S|4·|A|operations. The linear program (P3)takes
O(|S|4·|A|4)time in the worst case as it has |S|·|A|constrains and variables. Therefore, the overall complexity
of the approach is O(|S|4·|A|4)in the worst case.
Theorem 5.1 also provides a performance guarantee of the defense policy w.r.t. the true reward function, i.e.,
ρπD≥/hatwideρπD. Such a bound is important in practice since it provides a certiﬁcate of the worst-case performance
under the true reward function R, even though the agent can only optimize over /hatwideR. In contrast to the lower
bound in Theorem 4.4, the lower bound in Theorem 5.1 is optimal.
5.2 Unknown Parameter Setting
In this subsection, we focus on the optimization problem (P2b). First, note the structural diﬀerence between
(P2a)and(P2b). In the former case, /epsilon1†is given, and hence, the defense can infer possible values of Rby
solving an inverse problem to the attack problem (P1). In particular, we know that the original reward
functionRhas to be in the set {R:/hatwideR∈A(R,π†,/epsilon1†)}. In the latter case, /epsilon1†is not known, and instead we use
parameter/epsilon1Das an upper bound on /epsilon1†. We distinguish two cases:
•Overestimating Attack Parameter : If/epsilon1†≤/epsilon1D, thenweknowthat Risintheset{R:/hatwideR∈A(R,π†,/epsilon1)s.t.0<
/epsilon1≤/epsilon1D}. Note that this set is a super-set of {R:/hatwideR∈A(R,π†,/epsilon1†)}, which means that it is less informative
aboutR.
•Underestimating Attack Parameter : If/epsilon1†> /epsilon1D, then the set{R:/hatwideR∈A(R,π†,/epsilon1)s.t.0< /epsilon1≤/epsilon1D}will
have only a single element, i.e., /hatwideR. In other words, this set typically contains no information about R.
We analyze these two cases separately, ﬁrst focusing on the former one.
9Published in Transactions on Machine Learning Research (01/2023)
5.2.1 Overestimating Attack Parameter
When/epsilon1D≥/epsilon1†, our formal analysis builds on the one presented in Section 5.1, and we highlight the main
diﬀerences. Given that /epsilon1†is not exactly known, we cannot directly operate on the set Θ/epsilon1†. However, since
/epsilon1Dupper bounds /epsilon1†, the defense can utilize the procedure from the previous section (Theorem 5.1) with
appropriately chosen /epsilon1to solve (P2b) as we show in the following theorem.
Theorem 5.2. Assume that /epsilon1D≥/epsilon1†, and deﬁne/hatwide/epsilon1=mins,a/negationslash=π†(s)/bracketleftbig
/hatwideρπ†−/hatwideρπ†{s;a}/bracketrightbig
. Then, the optimization
problem(P3)with/epsilon1=min{/epsilon1D,/hatwide/epsilon1}is feasible and its optimal solution ψmaxidentiﬁes an optimal policy πDfor
the optimization problem (P2b)withC={c2}via Equation (2). This policy πDsatisﬁesρπD≥/hatwideρπD.
To interpret the bounds, let us consider three cases:
•R/negationslash=/hatwideR: If the attack indeed poisoned R, then the smallest /epsilon1/prime∈(0,/epsilon1D]such that Θ/epsilon1/prime/negationslash=∅corresponds
to/epsilon1†. In this case, it turns out that /epsilon1†=/hatwide/epsilon1, and somewhat surprisingly, the defense policies of (P2a)and
(P2b) coincide. (Note that this analysis assumes that /epsilon1D≥/epsilon1†.)
•R=/hatwideRand/epsilon1D</hatwide/epsilon1: This corresponds to the case when the attack did not poison Rand there is no
/epsilon1/prime∈(0,/epsilon1D]such that Θ/epsilon1/prime/negationslash=∅. In this case, it turns out that the optimal solution to the optimization
problem (P2b) is πD=π†(indeedπ†is uniquely optimal under R).
•R=/hatwideRand/epsilon1D≥/hatwide/epsilon1: This corresponds to the case when the attack did not poison Rand there is /epsilon1/prime∈(0,/epsilon1D]
such that Θ/epsilon1/prime/negationslash=∅. In fact,/hatwide/epsilon1is the smallest such /epsilon1/prime. In this case, it turns out that, in general, the optimal
solution to the optimization problem (P2b) is πD/negationslash=π†, even though π†is uniquely optimal under R.
These three cases also showcase the importance of choosing /epsilon1Dthat is a good upper bound on /epsilon1†. When
R=/hatwideR, the agent should select /epsilon1Dthat is strictly smaller than /hatwide/epsilon1. On the other hand, when R/negationslash=/hatwideR, the agent
should select /epsilon1D≥/hatwide/epsilon1, as it will be apparent from the result of the next subsection, in particular Theorem 5.3.
While the agent knows /hatwide/epsilon1, it does not know if R=/hatwideRorR/negationslash=/hatwideR.
5.2.2 Underestimating Attack Parameter
In this subsection, we analyze the case when /epsilon1D< /epsilon1†. We ﬁrst state our result, and then discuss its
implications.
Theorem 5.3. If/epsilon1†> /epsilon1D, thenπD=π†is the unique solution of the optimization problem (P2b)with
C={c2}.
Therefore, together with Theorem 5.2, Theorem 5.3 is showing the importance of having a good prior
knowledge about the attack parameter /epsilon1†. In particular:
•When the attack did not poison the reward function (i.e., /hatwideR=R), overestimating /epsilon1†implies that πDmight
not be equal to π†for larger values of /epsilon1D, even though π†is uniquely optimal under R. This can have a
detrimental eﬀect since in this case ρπD<ρπ†=ρπ∗.
•When the attack did poison the reward function R(i.e.,/hatwideR/negationslash=R), underestimating /epsilon1†impliesπD=π†, but
π†might be suboptimal. In this case, the defense policy does not limit the negative inﬂuence of the attack
at all, i.e.,ρπD=ρπ†≤ρπ∗.
We further discuss nuances to selecting /epsilon1Din Section 7.
6 Experimental Evaluation
In this section we evaluate our defense strategy in an experimental setting in order to better understand its
eﬃcacy and robustness. We focus on the setting from Section 5.2: c2attack cost functions, which is known
to the agent, i.e., C={c2}, with an unknown attack parameter /epsilon1†.
Giventhe resultsin Section5 (Theorems 5.2and 5.3), we use thelinear programmingformulation (P3)together
with the CVXPY solver Diamond & Boyd (2016); Agrawal et al. (2018) for calculating the solution to the
defense optimization problem (P2b). In the experiments, due to limited numerical precision, Θ/epsilon1is calculated
10Published in Transactions on Machine Learning Research (01/2023)
withatoleranceparameter, setto 10−4bydefault.4. Inotherwords, Θ/epsilon1={(s,a) :|/hatwideρπ†−/hatwideρπ†{s;a}−/epsilon1|≤10−4}.
s0s1s2s3s4s5s6
s7
s8
SG
0.1 0.3 0.5 0.7 0.9
Defense parameter ( /epsilon1D)0.1 0.3 0.5 0.7 0.9Attack parameter ( /epsilon1†)
−0.150.000.150.30
0.1 0.3 0.5 0.7 0.9
Defense parameter ( /epsilon1D)0.1 0.3 0.5 0.7 0.9Attack parameter ( /epsilon1†)
−1.6−1.4−1.2−1.0−0.8
(a) (b) (c) (d)
Figure 4: Experimental environments: Figures (a)and(b)show the Navigation and Grid world environment
respectively while ﬁgrues (c)and(d)showρπDin these environments. For comparison, in the navigation
environment, ρπ†=−0.26andρπ∗= 0.45while in the grid world environment, ρπ†=−1.75andρπ∗=−0.70.
0.0 0.1 0.2 0.3 0.4 0.5
Perturbation parameter-0.26-0.080.100.280.46Score w.r.t R
πD
πD+π†
π∗
(a) Navigation, PreAttack
0.0 0.1 0.2 0.3 0.4 0.5
Perturbation parameter-0.26-0.080.100.280.46Score w.r.t R
πD
πD+π†
π∗ (b) Navigation, PostAttack
0.0 0.1 0.2 0.3 0.4 0.5
Perturbation parameter-1.72-1.46-1.20-0.94-0.68Score w.r.t R
πD
πD+π†
π∗ (c) Grid world, PreAttack
0.0 0.1 0.2 0.3 0.4 0.5
Perturbation parameter-1.72-1.46-1.20-0.94-0.68Score w.r.t R
πD
πD+π†
π∗ (d) Grid world, PostAttack
Figure 5: Robustness of the defense policy against random perturbation. Results are based on average of 100
runs for each data point. Error bars around the data points indicate standard error.
Navigation environment . Our ﬁrst environment, shown in Figure 4a is the Navigation environment
taken from Rakhsha et al. (2021). The environment has 9 states and 2 possible actions. The reward
function is action independent and has the following values: R(s0,.) =R(s1,.) =R(s2,.) =R(s3,.) =−2.5,
R(s4,.) =R(s5,.) = 1andR(s6,.) =R(s7,.) =R(s8,.) = 0. When the agent takes an action, it will
successfully navigate in the direction shown by the arrows with probability 0.9; otherwise, the next state will
be sampled uniformly at random. The bold arrows in the ﬁgure indicate the attacker’s target policy. The
initial state is s0and the discounting factor γequals 0.99.
Grid world environment. For our second environment, shown in Figure 4b, we use the grid world
environment from Ma et al. (2019) with slight modiﬁcations in order to ensure ergodicity — we add a 10%
failure probability to each action, sampling the next state randomly in case of failure. The environment has
18 states and 4 actions: up,down,rightandleft. The white, gray and blue cells in the ﬁgure represent the
states and the black cells represent walls. In the white and gray states, the agent will attempt to go in the
direction speciﬁed by its action if there is a neighboring state in that direction. If there is no such state,
the agent will attempt to stay in its own place. In the blue state G, the agent will attempt to stay in its
own place regardless of the action taken. In all states, each attempt will succeed with probability 0.9; with
probability 0.1, the next state will be sampled uniformly at random. In the gray and white states, the agent’s
reward is a function of the state it is attempting to visit. Attempting to visit a gray, white and blue state
will yield a reward of −10,−1and2respectively. If the agent is in a blue state, it will always receive a
reward of 0. The bold arrows in the ﬁgure specify the attacker’s target policy. The initial state is Sand the
discounting factor γequals 0.9.
Policy score for diﬀerent values of parameters. We ﬁrst analyze the score of our defense policy in
4The value was chosen because the CVXPY solver uses a precision of 10−5.
11Published in Transactions on Machine Learning Research (01/2023)
both environments with diﬀerent values of /epsilon1†and/epsilon1D. For comparison, we also report the scores of the target
policy (π†) and the optimal policy ( π∗). The results are shown in Figures 4c and 4d. As seen in the ﬁgures,
as long as/epsilon1D≥/epsilon1†, our defense policy signiﬁcantly improves the agent’s score compared to π†.
Robustness to perturbations. We now analyze our algorithm’s robustness towards uncertainties in the
reward functions used by the attacker and the defender. For our ﬁrst experiment, which we call PreAttack, we
randomly perturb the attacker’s input. In particular, the input to the defender’s optimization problem /hatwideRis
sampled fromA(c2,R+N(0,σ2I),π†,/epsilon1†)whereIis the identity matrix, Ndenotes the multivariate normal
distribution and σis the perturbation parameter varied in the experiment. For our second experiment, called
PostAttack, we randomly perturb the reward vector after the attack, sampling the defender’s input from
A(c2,R,π†,/epsilon1†) +N(0,σ2I). In both experiments we use /epsilon1†= 0.1and/epsilon1D=∞. As explained below, when
calculating Θ/epsilon1, we also experiment with a larger tolerance parameter of 10−1, denoting the defense policy in
this case with πD+.
The results can be seen in Figure 5. As seen in the ﬁgures, our defense policy πDconsistently improves on
the baseline obtained with no defense (i.e, π†). It is also clear that the PostAttack perturbations have a
greater negative impact on our defense strategy’s score. Results for πD+indicate that this is due to random
perturbations prohibiting our algorithm from identifying all of the elements in Θ/epsilon1. While having a higher
tolerance parameter helps with robustness, it can also lead to a lower performance when there is no noise
because Θ/epsilon1would falsely include additional elements. We leave choosing the tolerance parameter in a more
systematic way for future work.
7 Concluding Discussions
In this paper, we introduced an optimization framework for designing defense strategies against reward
poisoning attacks that change an agent’s reward structure in order to steer the agent to adopt a target policy.
We analyzed the utility of using such defense strategies, providing characterization results and provable
guarantees on their performance. Moving forward we see several interesting future research directions.
Beyond the worst-case utility. In this paper, we deﬁned the defense objective as the maximization of
the agent’s worst-case utility. While this is a sensible objective, there are other objectives that one could
analyze. For example, instead of focusing on the absolute performance, one can try to optimize performance
relative to the target policy. Notice that this is a somewhat diﬀerent, and possibly a weaker goal, given
that the target policy can have arbitrarily bad utility under R. Additionally, one could study the agent’s
sub-optimality gap , i.e., the diﬀerence between its score and the score of the optimal policy π∗, and compare
it to the sub-optimality gap of the target policy. Going back to the example in Figure 3, we can see that the
defense strategy can signiﬁcantly reduce the suboptimality gap relative to not having a defense (in this case,
from 10−3 = 7to10−22/3≈2.7). However, when the original reward function already forces the target
policy, i.e., when R=/hatwideR, the suboptimality gap of the target policy is equal to 0. In Figure 3, this happens if
the target policy is action a1. Nevertheless, in this case, the suboptimality gap of the defense strategy would
also be low, equal to 1/2(since the defense would randomly select a1ora7), and generally upper bounded by
/epsilon1D= 2if the rewards of sub-optimal actions {a2,...,a 7}were diﬀerent. Providing a full theoretical treatment
for the general setting is an interesting research direction.
Informed prior. We did not model prior knowledge that an agent might have about the attacker or the
underlying reward function. In practice, the agent may have some information about the underlying true
reward function. Incorporating such considerations calls for a Bayesian approach that could improve the
agent’s defense by, e.g., ruling out implausible candidates for Rin the agent’s inference of Rgiven/hatwideR.
Selecting /epsilon1Dand non-oblivious attacks. The results in Section 5.2 indicate that choosing good /epsilon1Dis
important for having a functional defense. In practice, a selection procedure for /epsilon1Dshould take into account
the cost that the attacker has for diﬀerent choices of /epsilon1†, as well as game-theoretic considerations: attacks
might not be oblivious in that the strategy for selecting /epsilon1†might depend on the strategy for selecting /epsilon1D.
Namely, a direct consequence of Theorem 5.2 is that the attack optimization problem (P1)can successfully
achieve its goal if it chooses a large value of /epsilon1†to large enough values. However, the cost of the attack also
grows with /epsilon1†, so the attack (if strategic) also needs to reason about /epsilon1Dwhen selecting /epsilon1†. We leave the full
game-theoretic characterization of the parameter selection problem for the future work.
Unknown-model and scalability . Following prior work, we focused on attacks and defenses that have
12Published in Transactions on Machine Learning Research (01/2023)
access to an accurate transition model and operate in tabular settings. Given Theorems 5.1, 5.2 and 5.3,
the defense problem can be solved using the linear program (P3), which is similar in size to the attack
optimization problem (P1’). As such, we expect our method to be computationally scalable as long as the
attack optimization problem can be solved.
For many RL problems however, the transition and reward functions are not known and the MDP is too large
to be modeled in tabular settings. RL solutions for such problems typically rely on function approximation.
For future work, it would be interesting to study the problem, for both attack and defense, in this more
realistic scenario. An immediate question is how the attack problem would need to change in order to
generalise to this setting. In terms of defense, while the new setting would likely pose new challenges,
we expect the general min-max formulation in Section 3, as well as the techniques used for solving the
optimization problems (P2a) and (P2b), to remain useful.
Acknowledgments
The authors would like to thank the anonymous reviewers for their valuable comments and suggestions. This
research was, in part, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
– project number 467367360.
References
Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for convex
optimization problems. Journal of Control and Decision , 5(1):42–60, 2018.
Idan Amir, Idan Attias, Tomer Koren, Roi Livni, and Yishay Mansour. Prediction with corrupted expert
advice.CoRR, abs/2002.10286, 2020.
J Andrew Bagnell, Andrew Y Ng, and Jeﬀ G Schneider. Solving uncertain markov decision processes.
Technical report, Carnegie Mellon University, 2001.
Kiarash Banihashem, Adish Singla, Jiarui Gan, and Goran Radanovic. Admissible policy teaching through
reward design. CoRR, abs/2201.02185, 2022.
Vahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it stronger.
CoRR, abs/1712.09344, 2017.
Dimitri P Bertsekas. Control of uncertain systems with a set-membership description of the uncertainty. PhD
thesis, Massachusetts Institute of Technology, 1971.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.
Pattern Recognition , 84:317–331, 2018.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In
ICML, 2012.
Ilija Bogunovic, Arpan Losalka, Andreas Krause, and Jonathan Scarlett. Stochastic linear bandits robust to
adversarial attacks. CoRR, abs/2007.03285, 2020.
Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In STOC, pp. 47–60,
2017.
Gabriela F Cretu, Angelos Stavrou, Michael E Locasto, Salvatore J Stolfo, and Angelos D Keromytis. Casting
out demons: Sanitizing training data for anomaly sensors. In IEEE Symposium on Security and Privacy ,
pp. 81–95. IEEE, 2008.
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A
robust meta-algorithm for stochastic optimization. In ICML, pp. 1596–1606, 2019.
13Published in Transactions on Machine Learning Research (01/2023)
Steven Diamond and Stephen Boyd. Cvxpy: A python-embedded modeling language for convex optimization.
The Journal of Machine Learning Research , 17(1):2909–2913, 2016.
Christos Dimitrakakis, David C Parkes, Goran Radanovic, and Paul Tylkin. Multi-view decision processes:
The helper-ai problem. In NeurIPS , pp. 5443–5452, 2017.
European Commission. Ethics Guidelines for Trustworthy Artiﬁcial Intelligence. URL: https://ec.europa.
eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai , 2019. [Online; accessed
15-January-2021].
Ahana Ghosh, Sebastian Tschiatschek, Hamed Mahdavi, and Adish Singla. Towards deployment of robust
cooperative ai agents: An algorithmic framework for learning adaptive policies. In AAMAS, pp. 447–455,
2020.
Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with adversarial
corruptions. In COLT, pp. 1562–1578, 2019.
Ronan Hamon, Henrik Junklewitz, and Ignacio Sanchez. Robustness and explainability of artiﬁcial intelligence.
Publications Oﬃce of the European Union , 2020.
Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. Adversarial
machine learning. In ACM workshop on Security and artiﬁcial intelligence , pp. 43–58, 2011.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on
neural network policies. CoRR, abs/1702.02284, 2017.
Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipulations on cost
signals. In GameSec , pp. 217–237, 2019.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research , 30(2):257–280, 2005.
Kwang-Sung Jun, Lihong Li, Yuzhe Ma, and Xiaojin Zhu. Adversarial attacks on stochastic bandits. In
NeurIPS , pp. 3644–3653, 2018.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions. In ICML, pp.
1885–1894. PMLR, 2017.
Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization
defenses. CoRR, abs/1811.00741, 2018.
Aounon Kumar, Alexander Levine, and Soheil Feizi. Policy smoothing for provably robust reinforcement
learning. arXiv preprint arXiv:2106.11420 , 2021.
Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based
collaborative ﬁltering. In NeurIPS , pp. 1885–1893, 2016.
Shiau Hong Lim, Huan Xu, and Shie Mannor. Reinforcement learning in robust markov decision processes.
InNeurIPS , pp. 701–709, 2013.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics of
adversarial attack on deep reinforcement learning agents. In IJCAI, pp. 3756–3762, 2017.
Fang Liu and Ness B. Shroﬀ. Data poisoning attacks on stochastic bandits. In ICML, pp. 4042–4050, 2019.
Guanlin Liu and Lifeng Lai. Provably eﬃcient black-box action poisoning attacks against reinforcement
learning. Advances in Neural Information Processing Systems , 34, 2021.
Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial
corruptions. In STOC, pp. 114–122, 2018.
Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Corruption robust exploration in
episodic reinforcement learning. CoRR, abs/1911.08689, 2019.
14Published in Transactions on Machine Learning Research (01/2023)
Yuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu. Data poisoning attacks in contextual bandits. In
GameSec , pp. 186–204, 2018.
Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement learning and
control. In NeurIPS , pp. 14543–14553, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. In ICLR, 2018.
H Brendan McMahan, Geoﬀrey J Gordon, and Avrim Blum. Planning in the presence of cost functions
controlled by an adversary. In ICML, pp. 536–543, 2003.
Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine
learners. In AAAI, pp. 2871–2877, 2015.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In CVPR, pp. 2574–2582, 2016.
Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini,
Charles A Sutton, J Doug Tygar, and Kai Xia. Exploiting machine learning to subvert your spam ﬁlter.
LEET, 8:1–9, 2008.
Anh Nguyen, Jason Yosinski, and Jeﬀ Clune. Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In CVPR, pp. 427–436, 2015.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain transition
matrices. Operations Research , 53(5):780–798, 2005.
Andrea Paudice, Luis Muñoz-González, Andras Gyorgy, and Emil C Lupu. Detection of adversarial training
examples in poisoning attacks through anomaly detection. CoRR, abs/1802.03041, 2018.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement
learning. In ICML, pp. 2817–2826, 2017.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming . John Wiley &
Sons, Inc., 1994.
Goran Radanovic, Rati Devidze, David Parkes, and Adish Singla. Learning to collaborate in markov decision
processes. In ICML, pp. 5261–5270, 2019.
Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching via
environment poisoning: Training-time adversarial attacks against reinforcement learning. In ICML, 2020.
Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching in reinforce-
ment learning via environment poisoning attacks. Journal of Machine Learning Research , 22(210):1–45,
2021.
Anshuka Rangi, Haifeng Xu, Long Tran-Thanh, and Massimo Franceschetti. Understanding the limits of
poisoning attacks in episodic reinforcement learning. arXiv preprint arXiv:2208.13663 , 2022.
Kevin Regan and Craig Boutilier. Robust policy computation in reward-uncertain mdps using nondominated
policies. In AAAI, volume 24, 2010.
Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certiﬁed defenses for data poisoning attacks. In NeurIPS,
pp. 3520–3532, 2017.
Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl with
unknown dynamics. In International Conference on Learning Representations , 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
15Published in Transactions on Machine Learning Research (01/2023)
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In
ICML, pp. 1032–1039, 2008.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In ICLR, 2014.
Csaba Szepesvári. The asymptotic convergence-rate of q-learning. In NeurIPS , volume 10, pp. 1064–1070,
1997.
Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation. In ICML,
pp. 181–189, 2014.
Edgar Tretschk, Seong Joon Oh, and Mario Fritz. Sequential attacks on agents for long-term adversarial
goals.CoRR, abs/1805.12487, 2018.
Fan Wu, Linyi Li, Chejian Xu, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, and
Bo Li. Copa: Certifying robust policies for oﬄine reinforcement learning against poisoning attacks. arXiv
preprint arXiv:2203.08398 , 2022.
Han Xiao, Huang Xiao, and Claudia Eckert. Adversarial label ﬂips attack on support vector machines. In
ECAI, pp. 870–875, 2012.
Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is feature
selection secure against training data poisoning? In ICML, pp. 1689–1698, 2015.
Haoqi Zhang and David C. Parkes. Value-based policy teaching with active indirect elicitation. In AAAI,
2008.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Robust deep
reinforcement learning against adversarial perturbations on observations. CoRR, abs/2003.08938, 2020a.
Xuezhou Zhang, Xiaojin Zhu, and Stephen Wright. Training set debugging using trusted items. In AAAI,
volume 32, 2018.
Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks against
reinforcement learning. In ICML, 2020b.
Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. Robust policy gradient against strong data
corruption. CoRR, abs/2102.05800, 2021.
A List of Appendices
In this section we provide a brief description of the content provided in the appendices of the paper.
•Appendix B provides an intuition of our results for the /lscript2attack cost using special MDPs in which the
agent’s actions do not aﬀect the transition dynamics. The proofs of the results presented in this Appendix
can be found in Appendix I.
•Appendix C provides additional details regarding the experiments.
•Appendix D contains some background on reward poisoning attacks, and a brief overview of the MDP
properties that are important for proving our formal results.
•Appendix E contains characterization results for the attack optimization problem (P1).
•Appendix F contains proofs of the formal results in Section 5.
–The proof of Theorem 5.1 is in Section F.1.
–The proof of Theorem 5.2 is in Section F.2.
–The proof of Theorem 5.3 is in Section F.3.
16Published in Transactions on Machine Learning Research (01/2023)
•Appendix G contains the proofs for the results in Section 4 relating to Guarantees on values. The appendix
also includes an optimization framework for solving the defense optimization problem (P2a)for the/lscript1
norm as well as additional characterization results for the defense optimization problem for more general
cost functions which are used for proving the complexity results in Section 4.
–The characterization result for the defense optimization problem with for the /lscript1norm is provided in
Section G.1.
–Proof of Theorem 4.4 is provided in Section G.2.
–Additional characterization results for the defense optimization problem are provided in Section G.3.
–Proof of Proposition 4.2 is provided in Section G.4
–Proof of Theorem 4.3 is provided in Section G.5.
•Appendix H contains the proofs of the formal results in Section 4 relating to the computational complexity
of the defense optimization problem as well as the computational complexity result for the /lscript∞attack cost.
–The hardness result for the /lscript∞norm is provided in Section H.1.
–Proof of Theorem 4.5 is provided in Section H.2
•Appendix I contains a formal treatment of the results presented in Appendix B
B Intuition of Results using Special MDPs
In this Appendix, we describe characterization results on the /lscript2attack cost for special MDPs, in which the
agent’s actions do not aﬀect the transitions, that is, we assume that
P(s,a,s/prime) =P(s,a/prime,s/prime)∀s,a,a/prime,s/prime. (3)
Variants of the above condition have been studied in the literature (e.g., Szepesvári (1997); Dimitrakakis et al.
(2017); Sutton & Barto (2018); Radanovic et al. (2019); Ghosh et al. (2020)). Note that this assumption
implies that any two policies πandπ/primehave equal state occupancy measures, so we simplify the notation by
denotingµ=µπ=µπ/prime.
While the results from the previous sections incorporate this special case, we study this setting because: i) the
optimal solutions to the defense problem have a simple form, enabling us to provide intuitive explanations of
our main results from the previous sections, ii) using this setting, we show a tightness result for Theorem 5.2.
A more formal exposition of our results for this setting inlcuding the proofs can be found in Appendix I.
B.1 Optimal Defense Policy
In this subsection, we provide the intuition behind defense policies for the unknown parameter setting
with/epsilon1D≥/epsilon1†(Section 5.2.1). The key point about the assumption in Equation (3)is that it allows us to
consider each state separately in the defense optimization problems. In particular, it can be shown that the
optimization problem (P3) is equivalent to solving |S|optimization problems of the form
max
π(·|s)∈P(A)/angbracketleftBig
π(·|s),/hatwideR(s,·)/angbracketrightBig
(P3b)
π(a|s)≥π/parenleftbig
π†(s)|s/parenrightbig
∀a∈Θ/epsilon1
s,
where Θ/epsilon1
s={a:/hatwideR(s,a)−/hatwideR(s,π†(s)) =−/epsilon1
µ(s)}.5If we instantiate Theorem 5.2 for special MDPs by putting
/epsilon1=min{/epsilon1D,/hatwide/epsilon1}, the set Θ/epsilon1
shas an intuitive description: it is the set of all “second-best” actions (w.r.t /hatwideR) in
statessuch that their poisoned reward is greater than or equal to /hatwideR(π†(s))−/epsilon1
µ(s). It turns out that the
defense policy for state sselects an action uniformly at random from the set Θ/epsilon1
s∪{π†(s)}. In other words,
the defense policy πDis given by:
πD(a|s) =/braceleftBigg
1
|Θ/epsilon1s|+1ifa∈Θ/epsilon1
s∪{π†(s)}
0 otherwise.
5See Lemma 8 in Banihashem et al. (2022).
17Published in Transactions on Machine Learning Research (01/2023)
To see why, note that the objective in (P3b)only improves as we put more probability on selecting π†(s)(since
π†(s)is optimal under /hatwideR). However, the constraints in (P3b)require that the selection probability of any
action in Θ/epsilon1
shas to be at least as high as the selection probability of π†(s), which in turn give us the uniform
at random selection rule. Figure 3 illustrates attack and defense policies for special MDPs using a single-state
MDP with action set {a1,...a 7}. To obtain defense policy πD, we can solve the optimization problem (P3b),
which implies that πDshould select an action uniformly at random from the set {π†(s)}∪Θ/epsilon1
s={a1,a4,a7}
C Additional Details Regarding Experiments
In this section we provide additional details regarding the experiments. The source code for our experiments,
as well as instructions for replicating our results can be found in the Supplementary Material.
C.1 Implementation details
Both the attacker optimization problem (P1), and the defense optimization problems (P2a)and(P2b)are
convex since, by Theorems 5.1 and 5.2, the defense optimization problem reduces to the linear program (P3).
We use CVXPY to calculate their solutions. The code for solving these optimization problems can be found
in the ﬁle MDP.py. The speciﬁc functions used for solving these problems are as follows:
•The function attackimplements the attacker’s optimization problem (P1). As explained in the main text,
this is done by solving (P1’) since (P1’) is equivalent to (P1).
•The function defend_known implements the optimization problem (P2a). As explained in Section 6, the
tolerance parameter is set to 10−4(default value).
•The function defend_unknown implements the optimization problem (P2b).
C.2 Running time
Following prior work Rakhsha et al. (2021), to test the running times, we use the chain environment from
Rakhsha et al. (2021), but with diﬀerent number of states (additional states are added between s2ands3,
and the corresponding transitions and rewards are deﬁned analogously to those for s2). The attack and
defense parameters are set to /epsilon1†= 0.1and/epsilon1D= 0.2. Table 2 shows the average running times (across 10
runs) of the attack optimization problem (P1’)and the defense optimization problem (P2b)for diﬀerent sizes
of the chain environment.
It should be noted that the attack and defense optimization problems are similar in size, both solve a problem
with at most|S|·(|A|−1)constraints on R|S|.|A|. However, solving the defense problem takes more time,
partly because π†,/hatwide/epsilon1andΘ/epsilon1need to be identiﬁed before (P3) can be solved.
The machine used for obtaining these results is a Macbook Pro personal computer with 4 Gigabytes of
memory and a 2.4 GHz Intel Core i5 processor.
D Background and Additional MDP Properties
In this section we brieﬂy outline the background and MDP properties that we utilize in our proofs.
D.1 Reward Poisoning Attacks
In this section, we provide some background on the cost-eﬃcient reward poisoning attacks, focusing on the
results from Rakhsha et al. (2021).
The setting studied in Rakhsha et al. (2021) incorporates both the average and the discounted reward
optimality criteria in a discrete-time Markov Decision Process (MDP), with ﬁnite state and action spaces.
Our MDP setting is equivalent to their MDP setting under the discounted reward optimality criteria. This
criteria can be speciﬁed by score ρ. As deﬁned in the main text, scoreρπof policyπis the total expected
18Published in Transactions on Machine Learning Research (01/2023)
|S|ProblemAttack Defense
4 0.01s±0.5ms 0.05s±1.6ms
10 0.01s±0.2ms 0.09s±1.5ms
20 0.01s±0.1ms 0.17s±4.8ms
30 0.02s±2.0ms 0.27s±9.7ms
50 0.04s±6.8ms 0.56s±34.6ms
70 0.07s±3.0ms 1.02s±69.7ms
100 0.13s±5.4ms 1.83s±91.2ms
Table 2: Run time of the attack and defense optimization problems for the chain environment with varied
number of states |S|. Reported numbers are average of 10 runs; standard error is shown with ±.
return scaled by factor 1−γ:
ρπ=E/bracketleftBigg
(1−γ)∞/summationdisplay
t=1γt−1R(st,at)|π,σ/bracketrightBigg
,
where the state s1is sampled from the initial state distribution σ, and subsequent states stare obtained by
executing policy πin the MDP. Actions atare sampled from policy π.
As explained in the main text, the following result is important for our analysis, since it allows us to simplify
the optimization problem (P1) into the optimization problem (P1’).
Lemma D.1. (Lemma 1 in Rakhsha et al. (2021)) The score of a policy π†is at least/epsilon1†greater than all
other deterministic policies if and only if its score is at least /epsilon1†greater than the score of any policy π†{s;a}.
In other words,
/parenleftBig
∀π∈Πdet\{π†}:ρπ†≥ρπ+/epsilon1†/parenrightBig
⇐⇒/parenleftBig
∀s,a/negationslash=π†(s) :ρπ†≥ρπ†{s;a}+/epsilon1†/parenrightBig
.
Remark D.2.As explained in Rakhsha et al. (2021), this lemma implies that the optimization problem (P1)
is equivalent to (P1’). Furthermore, the optimization problem is always feasible since any policy can be made
optimal with suﬃcient perturbation of the reward function as formally shown by Rakhsha et al. (2021) and
Ma et al. (2019).
D.2 Overview of Important Quantities
Next, we provide an overview of standard MDP quantities and the quantities introduced in the main text
that are important for our analysis.
In addition to score ρ, we consider state-action value function, or Q-value function, deﬁned as
Qπ(s,a) =E/bracketleftBigg∞/summationdisplay
t=1γt−1R(st,at)|s1=s,a1=a,π/bracketrightBigg
.
In other words, Qπ(s,a)is the total expected return when the ﬁrst state is s, the ﬁrst action is a, while
subsequent states stand actions atare obtained by executing policy πin the MDP.
We consider two occupancy measures. By ψπwe denote the state-action occupancy measure in the Markov
chain induced by policy π:
ψπ(s,a) =E/bracketleftBigg
(1−γ)∞/summationdisplay
t=1γt−11[st=s,at=a]|π,σ/bracketrightBigg
.
19Published in Transactions on Machine Learning Research (01/2023)
Given MDP M, the set of realizable occupancy measures under any (stochastic) policy π∈Πis denoted by
Ψ. Note that the following holds:
ρπ=/angbracketleftψπ,R/angbracketright, (4)
where/angbracketleft.,./angbracketrightin the above equation computes a dot product between two vectors of size |S|·|A|(i.e., two vectors
inR|S|·|A|). We also denote by µπthe state occupancy measure in the Markov chain induced policy π∈Π,
i.e.:
µπ(s) =E/bracketleftBigg
(1−γ)∞/summationdisplay
t=1γt−11[st=s]|π,σ/bracketrightBigg
.
Note that
/summationdisplay
s,aψπ(s,a) =/summationdisplay
sµπ(s) = 1.
State-action occupancy measure and state occupancy measure satisfy
ψπ(s,a) =µπ(s)·π(a|s), (5)
which for deterministic πis equivalent to
ψπ(s,a) =1[π(s) =a]·µπ(s). (6)
Apart from the standard MDP quantities mentioned above, we also mention quantities introduced in the
main text. We denote by Θ/epsilon1state-action pairs (s,a)for which the margin between /hatwideρπ†and/hatwideρπ†{s;a}is equal
to/epsilon1, i.e.:
Θ/epsilon1=/braceleftBig
(s,a) :/hatwideρπ†{s;a}−/hatwideρπ†=−/epsilon1/bracerightBig
, (7)
which can be expressed through reward function /hatwideRusing state-action occupancy measures ψ:
Θ/epsilon1=/braceleftBig
(s,a) :/angbracketleftBig
ψπ†{s;a}−ψπ†,/hatwideR/angbracketrightBig
=−/epsilon1/bracerightBig
.
Finally, quantity Γ{s;a}(π)measures how well the occupancy measure of πis aligned with ψπ†{s;a}relative to
ψπ†:
Γ{s;a}(π) =/angbracketleftBig
ψπ†{s;a}−ψπ†,ψπ/angbracketrightBig
. (8)
D.3 Occupancy Measures as Linear Constraints
In this subsection, we introduce the Bellman ﬂow linear constraints that characterize ψπandµπ. In order to
characterize ψπ, we require the following constraints:
∀s:/summationdisplay
aψ(s,a) = (1−γ)σ(s) +/summationdisplay
˜s,˜aγ·P(˜s,˜a,s)·ψ(˜s,˜a). (9)
∀(s,a) :ψ(s,a)≥0. (10)
The importance of these constraints is reﬂected in the following lemma.
Lemma D.3. (Theorem 2 in Syed et al. (2008)) Let ψbe a vector that satisﬁes the Bellman ﬂow constraints
(9)and(10). Deﬁne policy πas
π(a|s) =ψ(s,a)/summationtext
˜aψ(s,˜a). (11)
Thenψis the state-action occupancy measure of π, in other words ψ=ψπ. Conversely, if π∈Πis a policy
with state-action occupancy measure ψ(i.e,ψ=ψπ) thenψsatisﬁes the Bellman ﬂow constraints (9)and
(10), as well as Equation (11).
20Published in Transactions on Machine Learning Research (01/2023)
As forµπ, it is well-known (e.g., see Rakhsha et al. (2021)) that a vector µis the state occupancy measure
for policyπ(i.e.,µ=µπ), if and only if
µ(s) = (1−γ)σ(s) +γ/summationdisplay
˜s,˜aµ(˜s)π(˜a|˜s)P(˜s,˜a,s). (12)
E Attack Characterization Results
E.1 Characterization results for the /lscript2attack cost
In this section we provide characterization results for the attack optimization problem (P1)for the/lscript2norm,
i.e,C={c2}. We will later use these results for proving the formal results presented in Section 5.1 and Section
5.2. In addition, these results provide intuition for our results about the more general /lscriptpnorms, which we will
discuss in the next sections. In particular, the main result of this appendix is a set of Karush–Kuhn–Tucker
(KKT) conditions that characterize the solution to the optimization problem (P1). As we focus on the cost
functionc=c2in this section, we will drop the dependence on cinA(c,R,π†,/epsilon1†).
To compactly express the KKT characterization results, let us introduce state occupancy diﬀerence matrix
Φ∈R|S|·(|A|−1)×|S|·|A|as a matrix with rows consisting of the vectors ψπ†{s;a}−ψπ†for all neighboring
policiesπ†{s;a}. Additionally, for all s,a/negationslash=π†(s), we use Φ(s,a)to denote the transpose of the row of
Φcorresponding to (s,a). Note that Φ(s,a)is a column vector. In this notation, given Remark D.2 and
Equation (4) , the optimization problem (P1) is equivalent to
min
R1
2/bardblR−R/prime/bardbl2
2(P1")
s.t. Φ·R4−/epsilon1†·1,
where 1is a|S|·(|A|−1)vector whose each element equal to 1, and 4speciﬁes that the left hand side is
element-wise less than or equal to the right hand side. Given this notation, the following lemma states the
KKT conditions for a reward function R(i.e., an|S|·|A|vector) to be an optimal solution to the optimization
problem (P1).
Lemma E.1. (KKT characterization) Ris a solution to the optimization problem (P1)if and only if there
exists an|S|·|A|vectorλsuch that
(R−R/prime) +ΦT·λ=0stationarity ,
Φ·R+/epsilon1†·140primal feasibility ,
λ<0dual feasibility ,
∀(s,a/negationslash=π†(s)) :λ(s,a)·(Φ(s,a)T·R+/epsilon1†) =0complementary slackness ,
where 0denotes an|S|·|A|vector whose each element equal to 0, and likewise, 1denotes an|S|·|A|vector
whose each element equal to 0.
Proof.Since(P1)is always feasible (Remark D.2) and all of the constrains are linear, strong duality holds.
Now, the Lagrangian of the optimization problem is equal to
L=1
2/bardblR−R/prime/bardbl2
2+λT(Φ·R+/epsilon1†·1),
and taking the gradient with respect to Rgives us
∇RL= (R−R/prime) +ΦT·λ.
The statement then follows by applying the standard KKT conditions.
Remark E.2.(Uniqueness) The solution to the optimization problem (P1)is unique since the objective
1
2||R−R/prime||2
2is strongly convex.
21Published in Transactions on Machine Learning Research (01/2023)
The above lemma, has the following important consequence.
Lemma E.3. Reward function Rsatisﬁes/hatwideR=A(R,π†,/epsilon1†)if and only if there exists some αs,a≥0such
that
R=/hatwideR+/summationdisplay
(s,a)∈Θ/epsilon1†αs,a·/parenleftBig
ψπ†{s;a}−ψπ†/parenrightBig
.
.
Proof.To prove the statement, we use Lemma E.1. The primal feasibility condition in the lemma always
holds as/hatwideR∈A(R,π†,/epsilon1†). Therefore/hatwideR∈A(R,π†,/epsilon1†)if and only if there exists λsuch that the other three
conditions hold. Note that the complementary slackness condition is equivalent to
∀(s,a/negationslash=π†(s)) :λ(s,a) = 0∨Φ(s,a)T·R+/epsilon1†= 0⇐⇒ ∀ (s,a)/∈Θ/epsilon1†:λ(s,a) = 0.
Therefore from dual feasibility, stationarity and complemantary slackness it follows that /hatwideR∈A(R,π†,/epsilon1†)if
and only if there exists λsuch that
λ<0,
R=/hatwideR+/summationdisplay
(s,a)λ(s,a)·/parenleftBig
ψπ†{s;a}−ψπ†/parenrightBig
,
∀(s,a)/∈Θ/epsilon1†:λ(s,a) = 0.
The Lemma therefore follows by setting αs,a=λ(s,a)since setting λ(s,a) = 0for all (s,a)/∈Θ/epsilon1†is equivalent
to not summing over the terms corresponding to (s,a)/∈Θ/epsilon1†in the stationarity condition.
A direct consequence of this lemma is the following result.
Corollary E.4. Assume that/hatwideR=A(R,π†,/epsilon1†)and/hatwideR/negationslash=R. It follows that
/hatwide/epsilon1=/epsilon1†,
where
/hatwide/epsilon1= min
s,a/negationslash=π†(s)/bracketleftBig
/hatwideρπ†−/hatwideρπ†{s;a}/bracketrightBig
.
Proof.Assume to the contrary that /hatwide/epsilon1/negationslash=/epsilon1†. Given the primal feasibility condition in Lemma E.1, /hatwide/epsilon1≥/epsilon1†.
Therefore/hatwide/epsilon1>/epsilon1†. It follows that
∀s,a/negationslash=π†(s) :/hatwideρπ†−/hatwideρπ†{s;a}>/epsilon1†=⇒Θ/epsilon1†=∅.
Given Lemma E.3, this implies that R=/hatwideR, which contradicts the initial assumption R/negationslash=/hatwideR.
F Proofs of Section 5.1
This section of the appendix contains the proofs of the formal results presented in Section 5.
F.1 Proof of Theorem 5.1
Before proving the theorem we prove some results that we need for the proof of this theorem, as well as for
the results in later sections.
Lemma F.1. Consider policy πwith state-action occupancy measure ψπ. Solution ρπ
minto the following
optimization problem:
min
Rρπs.t./hatwideR=A(R,π†,/epsilon1†), (P4)
22Published in Transactions on Machine Learning Research (01/2023)
satisﬁes:
ρπ
min=/braceleftBigg
/hatwideρπif∀s,a∈Θ/epsilon1†: Γ{s;a}(π)≥0
−∞otherwise.
Proof.We separately analyze the two cases: the case when Γ{s;a}(π)≥0for all (s,a)∈Θ/epsilon1†holds, and the
case when it does not.
Case 1: IfΓ{s;a}(π)≥0for all (s,a)∈Θ/epsilon1†, then by using Equation (4) and Lemma E.3 we obtain that
ρπ−/hatwideρπ=/angbracketleftBig
ψπ,R−/hatwideR/angbracketrightBig
=/summationdisplay
(s,a)∈Θ/epsilon1†αs,a·/angbracketleftBig
ψπ,ψπ†{s;a}−ψπ†/angbracketrightBig
≥0.
Therefore, ρπ≥/hatwideρπ. Furthermore, from Lemma E.3, we know that R=/hatwideRsatisﬁes the constraint in the
optimization problem (P4), so the score of the optimal solution to (P4) is ρπ
min=/hatwideρπ.
Case 2: Now, consider the case when Γ{s;a}(π)<0for a certain state-action pair (s,a)∈Θ/epsilon1†. Letαs,abe
an arbitrary positive number. From Lemma E.3, we know that
R=/hatwideR+αs,a·/angbracketleftBig
ψπ,ψπ†{s;a}−ψπ†/angbracketrightBig
satisﬁes the constraint in the optimization problem (P4), and hence is a solution to (P4). Moreover, by using
this solution together with Equation (4), we obtain
ρπ−/hatwideρπ=/angbracketleftBig
ψπ,R−/hatwideR/angbracketrightBig
=αs,a·/angbracketleftBig
ψπ,ψπ†{s;a}−ψπ†/angbracketrightBig
=αs,a·Γ{s;a}(π). (13)
Sinceαs,acan be arbitrarily large and Γ{s;a}(π)<0, while/hatwideρπis ﬁxed,ρπcan be arbitrarily small. Hence,
the score of the optimal solution to (P4) is unbounded from below, i.e., ρπ
min=−∞.
We can now prove Theorem 5.1, that is the following statement.
Statement: Consider the following optimization problem parameterized by /epsilon1:
max
ψ∈Ψ/angbracketleftBig
ψ,/hatwideR/angbracketrightBig
(P3)
s.t./angbracketleftBig
ψπ†{s;a}−ψπ†,ψ/angbracketrightBig
≥0∀s,a∈Θ/epsilon1.
For/epsilon1=/epsilon1†, this optimization problem is always feasible, and its optimal solution ψmaxspeciﬁes an optimal
solution to the optimization problem (P2a)with
πD(a|s) =ψmax(s,a)/summationtext
a/primeψmax(s,a/prime). (14)
The score of πD(a|s)is lower bounded by ρπD≥/hatwideρπD.
Proof.The feasibility of the problem follows from Theorem 4.46. Note that ψmaxalways exists since
(P3)is maximizing a continuous function over a closed and bounded set. Concretely, the constraints/angbracketleftbig
ψπ†{s;a}−ψπ†,ψ/angbracketrightbig
≥0and Equations (9)and(10)each deﬁne closed sets, and since ||ψ||1= 1, the set Ψis
bounded.
In order to see why ψmaxspeciﬁes an optimal solution to (P2a), note that we can rewrite (P2a) as
max
πρπ
min,
6The proof of Theorem 4.4 does not rely on this result .
23Published in Transactions on Machine Learning Research (01/2023)
whereρπ
minis the solution to the optimization problem (P4). Due to Lemma F.1, this could be rewritten as
max
π/hatwideρπ
s.t.Γ{s;a}(π)≥0∀(s,a)∈Θ/epsilon1†.
Namely, maximizing a function f(x)subject to constraint x∈X(whereX/negationslash=∅) is equivalent to maximizing
˜f(x), where
˜f(x) =/braceleftBigg
f(x)ifx∈X
−∞o.w..
Due to (4) and (8), the constrained optimization problem above can be rewritten as
max
π/angbracketleftBig
ψπ,/hatwideR/angbracketrightBig
s.t./angbracketleftBig
ψπ†{s;a}−ψπ†,ψπ/angbracketrightBig
∀(s,a)∈Θ/epsilon1†.
Therefore, given Lemma D.3, ψmaxspeciﬁes a solution to (P2a) via (2).
Finally, note that the constraints of the optimization problem (P3)ensure that a policy πwhose occupancy
measure is equal to ψmaxwill have Γ{s;a}(π)≥0— in other words, Γ{s;a}(πD)is non-negative for all
(s,a)∈Θ/epsilon1†. Due to Lemma F.1, we know that such policy πwill have the worst case utility equal to /hatwideρπ.
Therefore,ρπ≥/hatwideρπ.
Remark F.2.Given Lemma D.3, the constraint ψ∈Ψcan equivalently be replaced with constraints (9)and
(10), making the optimization problem (P3) a linear program.
F.2 Proof of Theorem 5.2
The proof of the theorem is similar to the proof of Theorem 5.1 and builds on two lemmas which we introduce
in this section.
Lemma F.3. Set/epsilon1= min{/epsilon1D,/hatwide/epsilon1}, where
/hatwide/epsilon1= min
s,a/negationslash=π†(s)/bracketleftBig
/hatwideρπ†−/hatwideρπ†{s;a}/bracketrightBig
.
Reward function Rsatisﬁes/hatwideR=A(R,π†,˜/epsilon1)for some ˜/epsilon1∈(0,/epsilon1D]if any only if
R=/hatwideR+/summationdisplay
(s,a)∈Θ/epsilon1αs,a·/parenleftBig
ψπ†{s;a}−ψπ†/parenrightBig
,
for someαs,a≥0.
Proof.We divide the proof into two parts, respectively proving the suﬃciency and the necessity of the
condition.
Part 1 (Necessity): Assume that /hatwideR=A(R,π†,˜/epsilon1)for some ˜/epsilon1∈(0,/epsilon1D]. From the stationariry and dual
feasibility conditions in Lemma E.1, we deduce
∃λ<0 :R=/hatwideR+/summationdisplay
s,a/negationslash=π†(s)λ(s,a)·(ψπ†{s;a}−ψπ†). (15)
We claim that λ(s,a) = 0for all (s,a)/∈Θ/epsilon1. Note that this would imply the lemma’s statement by setting
αs,a=λ(s,a), since the terms corresponding to (s,a)/∈Θ/epsilon1could be skipped in the summation of (15).
24Published in Transactions on Machine Learning Research (01/2023)
To see why the claim holds, assume that λ(s,a)/negationslash= 0for some (s,a)wherea/negationslash=π†(s). From complementary
slackness, we know that Φ(s,a)T·R+ ˜/epsilon1= 0, which implies that
/hatwide/epsilon1= min
˜s,˜a/negationslash=π†(˜s)(−Φ(˜s,˜a)T·R)≤−Φ(s,a)T·R= ˜/epsilon1. (16)
However, ˜/epsilon1≤/hatwide/epsilon1holds by primal feasibility. Therefore, all the inequalities are equalities, which implies ˜/epsilon1=/hatwide/epsilon1.
Since ˜/epsilon1≤/epsilon1D, we conclude that ˜/epsilon1=min{/epsilon1D,/hatwide/epsilon1}=/epsilon1. Since all of the inequalities in (16)are indeed equalities,
we conclude
−Φ(s,a)T·R=/epsilon1=⇒(s,a)∈Θ/epsilon1,
which proves the claim.
Part 2 (Suﬃciency): Assume that
R=/hatwideR+/summationdisplay
(s,a)∈Θ/epsilon1αs,a·/parenleftBig
ψπ†{s;a}−ψπ†/parenrightBig
,
for someαs,a≥0. Set ˜/epsilon1=/epsilon1and note that ˜/epsilon1≤/epsilon1Dby deﬁnition. Set
λ(s,a) =/braceleftBigg
αs,aif(s,a)∈Θ/epsilon1
0o.w..
We now verify all the conditions of Lemma E.1 hold. Stationarity and dual feasibility hold because
R=/hatwideR+/summationtext
s,aλ(s,a)·Φ(s,a)andλ<0. Primal feasibility holds because ˜/epsilon1=min{/epsilon1D,/hatwide/epsilon1}≤/hatwide/epsilon1. Finally,
complementary slackness holds because
λ(s,a)/negationslash= 0 =⇒(s,a)∈Θ/epsilon1=⇒Φ(s,a)TR+/epsilon1= 0.
Lemma F.4. Letρπ
minbe the solution to the following optimization problem
min
Rρπs.t./hatwideR=A(R,π†,˜/epsilon1)∧0<˜/epsilon1≤/epsilon1D. (P5)
Then
ρπ
min=/braceleftBigg
/hatwideρπif∀(s,a)∈Θ/epsilon1: Γ{s;a}(π)≥0
−∞o.w.,
where/epsilon1= min{/epsilon1D,/hatwide/epsilon1}, and
/hatwide/epsilon1= min
s,a/negationslash=π†(s)/bracketleftBig
/hatwideρπ†−/hatwideρπ†{s;a}/bracketrightBig
.
Proof.The proof is similar to the proof of Lemma F.1. We separately analyze the two cases: the case when
Γ{s;a}(π)≥0for all (s,a)∈Θ/epsilon1holds, and the case when it does not.
Case 1: IfΓ{s;a}(π)≥0for all (s,a)∈Θ/epsilon1, then by using Equation (4) and Lemma F.3 we obtain that
ρπ−/hatwideρπ=/angbracketleftBig
ψπ,R−/hatwideR/angbracketrightBig
=/summationdisplay
(s,a)∈Θ/epsilon1αs,a·/angbracketleftBig
ψπ,ψπ†{s;a}−ψπ†/angbracketrightBig
≥0.
Therefore, ρπ≥/hatwideρπ. Furthermore, from Lemma F.3, we know that R=/hatwideRsatisﬁes the constraint in the
optimization problem (P5), so the score of the optimal solution to (P5) is ρπ
min=/hatwideρπ.
25Published in Transactions on Machine Learning Research (01/2023)
Case 2: Now, consider the case when Γ{s;a}(π)<0for a certain state-action pair (s,a)∈Θ/epsilon1. Letαs,abe
an arbitrary positive number. From Lemma F.3, we know that
R=/hatwideR+αs,a·/angbracketleftBig
ψπ,ψπ†{s;a}−ψπ†/angbracketrightBig
satisﬁes the constraint in the optimization problem (P5), and hence is a solution to (P5). Moreover, by using
this solution together with Equation (4), we obtain
ρπ−/hatwideρπ=/angbracketleftBig
ψπ,R−/hatwideR/angbracketrightBig
=αs,a·/angbracketleftBig
ψπ,ψπ†{s;a}−ψπ†/angbracketrightBig
=αs,a·Γ{s;a}.
Sinceαs,acan be arbitrarily large and Γ{s;a}<0, while/hatwideρπis ﬁxed,ρπcan be arbitrarily small. Hence, the
score of the optimal solution to (P5) is unbounded from below, i.e., ρπ
min=−∞.
We are now ready to prove Theorem 5.2.
Statement: Assume that /epsilon1D≥/epsilon1†, and deﬁne/hatwide/epsilon1=mins,a/negationslash=π†(s)/bracketleftbig
/hatwideρπ†−/hatwideρπ†{s;a}/bracketrightbig
. Then, the optimization
problem(P3)with/epsilon1=min{/epsilon1D,/hatwide/epsilon1}is feasible and its optimal solution ψmaxidentiﬁes an optimal policy πDfor
the optimization problem (P2b)via Equation (2). This policy πDsatisﬁesρπD≥/hatwideρπD.
Proof.The proof is divide into two parts, respectively proving the ﬁrst and the second claim in the theorem
statement.
Part 1 (Solution to (P2b)):We prove that the optimization problem (P3)is feasible, its optimal solution
ψmaxidentiﬁes an optimal solution to (P2b) via Equation (2), and satisﬁes ρπD≥/hatwideρπD.
The feasibility of the problem follows from Theorem 4.4. Note that ψmaxalways exists since (P3)is maximizing
a continuous function over a closed and bounded set. Concretely, the constraints/angbracketleftbig
ψπ†{s;a}−ψπ†,ψ/angbracketrightbig
≥0and
Equations (9) and (10) each deﬁne closed sets and since ||ψ||1= 1, the set Ψis bounded.
In order to see why ψmaxspeciﬁes an optimal solution to (P2b), note that we can rewrite (P2b) as
max
πρπ
min,
whereρπ
minis the solution to the optimization problem (P5). Due to Lemma F.4, this could be rewritten as
max
π/hatwideρπ
s.t.Γ{s;a}(π)≥0∀(s,a)∈Θ/epsilon1,
where/epsilon1=min{/epsilon1D,/hatwide/epsilon1}. Namely, maximizing a function f(x)subject to constraint x∈X(whereX/negationslash=∅) is
equivalent to maximizing ˜f(x), where
˜f(x) =/braceleftBigg
f(x)ifx∈X
−∞o.w..
Due to (4) and (8), the constrained optimization problem above can be rewritten as
max
π/angbracketleftBig
ψπ,/hatwideR/angbracketrightBig
s.t./angbracketleftBig
ψπ†{s;a}−ψπ†,ψπ/angbracketrightBig
∀(s,a)∈Θ/epsilon1.
Therefore, given Lemma D.3, ψmaxspeciﬁes a solution to (P2b)via(2). Finally, given Lemma F.4, ψπD
satisﬁes the constraints of (P5) and therefore /hatwideρπDis a lower bound on ρπD.
26Published in Transactions on Machine Learning Research (01/2023)
F.3 Proof of Theorem 5.3
Statement: If/epsilon1†>/epsilon1D, thenπ†is the unique solution of the optimization problem (P2b), henceπD=π†.
Proof.As in Theorem 5.2, set /epsilon1= min{/hatwide/epsilon1,/epsilon1D}where
/hatwide/epsilon1= min
s,a/negationslash=π†(s)/bracketleftBig
/hatwideρπ†−/hatwideρπ†{s;a}/bracketrightBig
.
From the feasibility of the attack, we have that
∀s,a/negationslash=π†(s) :/hatwideρπ†−/hatwideρπ†{s;a}≥/epsilon1†>/epsilon1D≥/epsilon1=⇒Θ/epsilon1=∅.
Therefore, given Lemma F.3, the constraint in the optimization problem (P2b)is satisﬁed only for R=/hatwideR.
This reduces the optimization problem (P2b) to maxπ/hatwideρπ, which has a unique optimal solution: π†.
G Proofs of Section 4
In this section, we provide the proofs of the results of Section 47as well as additional results that characterize
the defense optimization problem. While our results are stated for the defense optimization problem (P2a),
all results hold for (P2b) as well with /epsilon1= min{/epsilon1D,/hatwide/epsilon1}.
G.1 Solution to the defense optimization problem for the /lscript1attack cost
In this section, we present a convex optimization framework for solving the defense optimization problem
(P2a)forC={c1}. Our main result is the following theorem, the proof of which is presented in Section G.3.
Theorem G.1. LetΦ∈R|S|·(|A|−1)×|S|·|A|be a matrix with rows consisting of the vectors ψπ†{s;a}−ψπ†as
in Section E and let Φθbe the sub-matrix of Φconsisting of the rows corresponding to Θ/epsilon1†. Deﬁne the set
U⊆S×Aas the set of state action pairs (˜s,˜a)for which the following optimization problem is feasible.
∀s,a:/vextendsingle/vextendsingle/parenleftbig
ΦT
θλ/parenrightbig
(s,a)/vextendsingle/vextendsingle≤1and/parenleftbig
ΦT
θλ/parenrightbig
(˜s,˜a) =−1andλ<0.
where <denotes coordinate-wise inequality. Consider the following optimization problem:
max
π/hatwideρπ,R,s.t.π(a|s) = 0∀s,a∈U. (17)
The optimization problem (17)is always feasible and its solution is a solution to the defense optimization
problem (P2a)withC={c1}.
G.2 Proof of Theorem 4.4
Statement: Consider any policy π/negationslashπ†s.t.π/negationslashπ†(π†(s)|s) = 0. ForC={cps.t.p∈[1,∞)}, the optimal value
of problem (P2a)is bounded from below by /hatwideρπ/negationslashπ†.
Proof.We claim that /hatwideR(s,a)≤R(s,a)for allRsatisfying/hatwideR∈A(R)and alls,a/negationslash=π†(s). To see why, assume
that if this not the case for some ˜s,˜aand deﬁne/tildewideRas
/tildewideR(s,a) =/braceleftBigg
R(s,a)ifs,a= ˜s,˜a
/hatwideR(s,a)o.w.
It is clear that/vextenddouble/vextenddouble/vextenddouble/tildewideR−R/vextenddouble/vextenddouble/vextenddouble
p</vextenddouble/vextenddouble/vextenddouble/hatwideR−R/vextenddouble/vextenddouble/vextenddouble
pfor allp∈[1,∞). We claim that /tildewideRis feasible for the attack optimization
problem (P1)with parameters cp,R,π†,/epsilon1†. This would contradict the assumption, /hatwideR∈A(R)as it would
mean/hatwideRis not an optimal solution to (P1), ﬁnishing the proof.
7The results related to computational hardness are discussed separately in Appendix H.
27Published in Transactions on Machine Learning Research (01/2023)
To prove the claim, since (P1) is equivalent to (P1’), we need to show that
ρ/tildewideR,π†{˜s;˜a}−ρ/tildewideR,π†≤−/epsilon1†,
for all ˜s,˜a/negationslash=π†(˜s). By deﬁnition of /tildewideRhowever,ρ/tildewideR,π†{˜s;˜a}=ρ/hatwideR,π†{˜s;˜a}as
ρ/tildewideR,π†{˜s;˜a}−ρ/hatwideR,π†{˜s;˜a}=/angbracketleftBig
/tildewideR−/hatwideR,ψπ†{˜s;˜a}/angbracketrightBig
= 0, (18)
where the ﬁrst equality follows from Equation (4), and the second equality follows from the fact that /tildewideRonly
diﬀers from/hatwideRin(s,a),ψπ†{˜s;˜a}(s,a) = 0. Simliarly ρ/tildewideR,π†≥ρ/hatwideR,π†as
ρ/tildewideR,π†−ρ/hatwideR,π†=/angbracketleftBig
/tildewideR−/hatwideR,ψπ†/angbracketrightBig
≥0,
where the second inequality follows from the fact that /tildewideRis never less than /hatwideR. Therefore,
ρ/tildewideR,π†{˜s;˜a}−ρ/tildewideR,π†≤ρ/hatwideR,π†{˜s;˜a}−ρ/hatwideR,π†≤−/epsilon1†,
where the second inequality follows from the assumption that /hatwideR∈A(R). We haver therefore shown (18),
ﬁnishing the proof.
G.3 Characterization of the inner minimization problem in (P2a)
We begin by providing characterisation results for the inner minimization problem in (P2a)for diﬀerent
known cost functions. These results can be seen as extensions of Lemma F.1. Formally, for ﬁxed p, consider
the following optimization problem
min
Rρπs.t./hatwideR=A(cp,R,π†,/epsilon1†), (P4)
The following lemmas characterize the value of the above optimization problem for diﬀerent values of p.
In stating these lemmas, we use Φ∈R|S|·(|A|−1)×|S|·|A|be a matrix with rows consisting of the vectors
ψπ†{s;a}−ψπ†as in Section E and let Φθbe the sub-matrix of Φconsisting of the rows corresponding to Θ/epsilon1†.
Lemma G.2. Letπbe a ﬁxed policy and assume that 1<p<∞is a ﬁxed number. Deﬁne the function
up:R→Rasup(x) =sgn(x)·|x|1
p−1wheresgn(x) =1[x>0]−1[x<0]and letup:Rn→Rnbe its
coordinate-wise extension to Rn, i.e,up(x)i=up(xi).
The solution to (P4)equals/hatwideρπif
/angbracketleftbig
ψπ,up(ΦT
θλ)/angbracketrightbig
≥0∀λ<0,
and equals−∞otherwise.
Lemma G.3. Letπbe a ﬁxed policy. Deﬁne the function u∞:R→Ras
u∞(x) =/braceleftBigg
−1ifx≤0
1o.w.,
and letu∞:Rn→Rnbe its coordinate-wise extension to Rn, i.e,u∞(x)i=u∞(x)i. The solution to (P4)
equals/hatwideρπif
/angbracketleftbig
ψπ,u∞(ΦT
θλ)/angbracketrightbig
≥0∀λ<0s.t.λ/negationslash= 0.
and equals−∞otherwise.
Lemma G.4. Letπbe a ﬁxed policy and let U⊆S×Abe the set of state action pairs (˜s,˜a)for which the
following optimization problem is feasible.
∀s,a:/vextendsingle/vextendsingle/parenleftbig
ΦT
θλ/parenrightbig
(s,a)/vextendsingle/vextendsingle≤1and/parenleftbig
ΦT
θλ/parenrightbig
(˜s,˜a) =−1andλ<0. (19)
Then the solution to (P4)is−∞ifπ(˜a|˜s)>0for some ˜s,˜a∈Uand is/hatwideρπotherwise.
28Published in Transactions on Machine Learning Research (01/2023)
When it is clear from context, we will drop the dependence on pinup. The proof of Lemmas G.2, G.3 and
G.4 are provided below. Note that Lemma G.4 immediately implies Theorem G.1 since the feasibility of (17)
already follows from Theorem 4.4.
Proof of Lemma G.2. Throughout the proof, we will drop the dependence on cp,/epsilon1†andπ†inA. The proof
follows a similar structure as the results for the /lscript2norm; namely, Lemmas E.1, E.3 and F.1.
We begin by analyzing the constraint A(R) =/hatwideRusing the KKT conditions. Since 1<p<∞, we can change
the objective to1
p/vextenddouble/vextenddoubleR−R/vextenddouble/vextenddoublep
pfor convenience. Of the four KKT conditions, primal feasiblity holds if and only
ifΦTR4−/epsilon1. For the stationarity condition, forming the lagrangian of (P1), we obtain
L=1
p/vextenddouble/vextenddoubleR−R/vextenddouble/vextenddoublep
p+λT(ΦR−/epsilon1†) =/summationdisplay1
p(R(s,a)−R(s,a))p+λTΦR−/epsilon1†λT1
Taking the gradient,
∇RL= 0⇐⇒sgn(R(s,a)−R(s,a))·/vextendsingle/vextendsingleR(s,a)−R(s,a)/vextendsingle/vextendsinglep−1+/parenleftbig
ΦTλ/parenrightbig
(s,a) = 0∀s,a
⇐⇒/parenleftbig
ΦTλ/parenrightbig
(s,a) =sgn(R(s,a)−R(s,a))·/vextendsingle/vextendsingleR(s,a)−R(s,a)/vextendsingle/vextendsinglep−1∀s,a
⇐⇒/vextendsingle/vextendsingle/parenleftbig
ΦTλ/parenrightbig
(s,a)/vextendsingle/vextendsingle1
p−1·sgn/parenleftbig/parenleftbig
ΦTλ/parenrightbig
(s,a)/parenrightbig
=R(s,a)−R(s,a)∀s,a
(i)⇐⇒u(ΦTλ) =R−R,
where for (i)we have used the deﬁnition of u. the complementary slackness condition states that
λ(s,a) = 0∀(s,a)/∈Θ/epsilon1†.
Finally, dual feasible states that λ<0.
Returning to the condition A(R) =/hatwideR, primal feasibility always holds as /hatwideR=A(R). We therefore conclude
that a vector RsatisﬁesA(R) =/hatwideRif and only if the following problem is feasible
R=/hatwideR+u(ΦTλ)
λ<0
λ(s,a) = 0∀(s,a)/∈Θ/epsilon1†
By deﬁnition of Φθ, this means that
A(R) =/hatwideR⇐⇒R=/hatwideR+u(ΦT
θλ)for someλ<0.
Returning back to (P4), the problem can be rewritten as
min
R,λ/angbracketleftψπ,R/angbracketright
s.t.λ<0
R=u(ΦT
θλ) +/hatwideR.
or equivalently,
min
λ<0/angbracketleftbig
ψπ,u(ΦT
θλ)/angbracketrightbig
+/hatwideρπ. (20)
Now, assume that/angbracketleftbig
ψπ,u(ΦT
θλ)/angbracketrightbig
<0for someλ<0. Observe that u(c·x) =c1
1−p·xfor positive constants
c>0. Therefore,
/angbracketleftbig
ψπ,u/parenleftbig
ΦT
θ(c·λ)/parenrightbig/angbracketrightbig
=c1
p−1·/angbracketleftbig
ψπ,u/parenleftbig
ΦT
θλ/parenrightbig/angbracketrightbig
Since 1<p<∞, lettingc→∞, the value of/angbracketleftbig
ψπ,u/parenleftbig
ΦT
θ(c·λ)/parenrightbig/angbracketrightbig
can be made arbitrarily low. Therefore, the
value of (20)equals−∞. Conversely, assume that/angbracketleftbig
ψπ,u(ΦT
θλ)/angbracketrightbig
≥0for allλ<0. It follows that the value
of (20) is bigger than equal to /hatwideρπ. Sinceλ= 0is feasible for (20), the value is exactly /hatwideρπ.
29Published in Transactions on Machine Learning Research (01/2023)
Before we prove Lemma G.3, we will state and prove the following Lemma which characterizes the subgradient
of the/lscript∞norm.
Lemma G.5. The vectors w,zi∈Rnsatisfyw∈∂/bardblz/bardbl∞if and only if (a)z= 0and/bardblw/bardbl1≤1or (b)
/bardblw/bardbl1= 1andz∈/tildewideu(w)where
/tildewideu(w) :=

x∈Rn:xi∈

{c}ifwi>0
{−c}ifwi<0
[−c,c]ifwi= 0for somec≥0


Proof.By Proposition A.22 in Bertsekas (1971),
∂/bardblz/bardbl∞= conv{ws.t/bardblw/bardbl1≤1,zTw=/bardblz/bardbl∞}={ws.t/bardblw/bardbl1≤1,zTw=/bardblz/bardbl∞}. (21)
analyzing the above result, note that
zTw=/summationdisplay
ziwi
(a)
≤/summationdisplay
|wi|/bardblz/bardbl∞
=/bardblw/bardbl1·/bardblz/bardbl∞
(b)
≤/bardblz/bardbl∞.
SincezTw=/bardblz/bardbl∞, equality holds in (b), implying/bardblw/bardbl1= 1orz= 0and in (a), implyingziwi=|wi|/bardblz/bardbl∞
for alli. This means that if |zi|</bardblz/bardbl∞, thenwi= 0, and if|zi|=/bardblz/bardbl∞, thenwi≥0ifzi≥0andwi≤0if
zi≤0.
In other words (this time conditioning on w), if/bardblw/bardbl1<1, thenz= 0. Otherwise, if wi>0thenzi=/bardblz/bardbl∞,
ifwi<0, thenzi=−/bardblz/bardbl∞and ifwi= 0thenzican be any value in [−/bardblz/bardbl∞,/bardblz/bardbl∞]. Therefore, the proof
follows by setting c=/bardblz/bardbl∞. (the last condition is obviously true but it is important to make explicit as will
become clear later). This brings us to the following result.
We can now prove Lemma G.3.
Proof of Lemma G.3. The main ideas of the KKT analysis in the proof of Theorem G.2 still hold and the
only condition that changes is the stationarity slackness condition. Formally, the lagrangian equals
/vextenddouble/vextenddoubleR−R/vextenddouble/vextenddouble
∞+λT(ΦR−/epsilon1†).
Therefore, the stationarity condition can be written as
0∈ΦTλ+∂/vextenddouble/vextenddoubleR−R/vextenddouble/vextenddouble
∞⇐⇒ − ΦTλ∈∂/vextenddouble/vextenddoubleR−R/vextenddouble/vextenddouble
∞(22)
By Lemma G.5, and given the fact that /tildewideu(−x) =−/tildewideu(x), the above condition holds if and only if
/parenleftbig
R−R∈/tildewideu(ΦTλ)∧/vextenddouble/vextenddoubleΦTλ/vextenddouble/vextenddouble
1= 1/parenrightbig
∨/parenleftbig/vextenddouble/vextenddoubleΦTλ/vextenddouble/vextenddouble
1≤1∧R−R= 0/parenrightbig
Combining with the complementary slackness and dual feasibility, (P4) can be rewritten as
min
v,λ/hatwideρπ+/angbracketleftψπ,v/angbracketright
s.tλ<0
/parenleftbig
v∈/tildewideu(ΦT
θλ)∧/vextenddouble/vextenddoubleΦT
θλ/vextenddouble/vextenddouble
1= 1/parenrightbig
∨/parenleftbig/vextenddouble/vextenddoubleΦT
θλ/vextenddouble/vextenddouble
1≤1∧v= 0/parenrightbig
Now, observe that /tildewideu(c·x) =/tildewideu(x)for allc>0. Therefore, the/vextenddouble/vextenddoubleΦT
θλ/vextenddouble/vextenddouble
1= 1inside the ﬁrst clause of the last
constraint is equivalent to ΦT
θλ/negationslash= 0since if ΦT
θλ/negationslash= 0and/vextenddouble/vextenddoubleΦT
θλ/vextenddouble/vextenddouble/negationslash= 1, then/vextenddouble/vextenddoubleΦT
θλ/vextenddouble/vextenddouble= 1can be satisﬁed by
30Published in Transactions on Machine Learning Research (01/2023)
replacingλwith1
/bardblΦT
θλ/bardbl1·λ. Similarly, the/vextenddouble/vextenddoubleΦT
θλ/vextenddouble/vextenddouble
1≤1inside the second clause is redundant. Therefore, the
last constraint can be rewritten as
/parenleftbig
v∈/tildewideu(ΦT
θλ)∧ΦT
θλ/negationslash= 0/parenrightbig
∨v= 0
Now, observe that ΦT
θλ/negationslash= 0is equivalent to λ/negationslash= 0as the rows of Φθare independent; this is becasue the
only row with nonzero (s,a/negationslash=π†(s))is the one corresponding to (s,a/negationslash=π†(s)). Therefore, the optimization
problem can be rewritten as
min
v,λ/hatwideρπ+/angbracketleftψπ,v/angbracketright
s.tλ<0
/parenleftbig
v∈/tildewideu(ΦT
θλ)∧λ/negationslash= 0/parenrightbig
∨v= 0
Assume that there exists λ/negationslash= 0satisfyingλ<0such that/angbracketleftbig
u(ΦT
θλ),ψπ/angbracketrightbig
<0. Then for any c > 0,
c·u(ΦT
θλ)∈/tildewideu(ΦT
θλ)and therefore, the value of the optimization problem is −∞. Otherwise, the value is
lower bounded by /hatwideρπand the bound is attained with v= 0, concluding the proof.
Proof of Lemma G.4. We begin by analyzing (P1) as before. Forming the Lagrangian,
L=/vextenddouble/vextenddoubleR−R/vextenddouble/vextenddouble
1+λT(ΦR−/epsilon1†)
Therefore, the stationarity condition states that
ΦTλ∈−∂/vextenddouble/vextenddoubleR−R/vextenddouble/vextenddouble
1
As before, the primal feasibility condition holds, dual feasibility states that λ<0and complementary
slackness states that λ(s,a) = 0for all (s,a)/∈Θ/epsilon1†. Therefore,A(R) =/hatwideRif and only if
ΦT
θλ∈−∂/vextenddouble/vextenddouble/vextenddouble/hatwideR−R/vextenddouble/vextenddouble/vextenddouble
1(23)
Note however that vectors z,wsatisfyw∈∂/bardblz/bardbl1if and only if
wi

= 1ifzi>0,
=−1ifzi<0,
∈[−1,1]ifzi= 0⇐⇒zi

≥0ifwi= 1
≤0ifwi=−1
= 0ifwi∈(−1,1)
Now, assume that π(˜a|˜s)>0for some (˜s,˜a)for which (19)is feasible and λbe the vector satisfying (19). By
(23), we conclude that the vector R(s,a) =/hatwideR(s,a)−t·1[(s,a) = (˜s,˜a)]satisﬁesA(R) =/hatwideRfor anyt>0.
Therefore, if π(˜a|˜s)>0, the solution to (P4)is−∞. Conversely, if (19)is not feasible for any such ˜s,˜a, then
it follows that R(s,a)≥/hatwideR(s,a)for allR,s,asatisfyingA(R) =/hatwideRandπ(a|s)>0. Therefore, the value of
the optimization problem is lower bounded by /hatwideρπ. SinceA(/hatwideR) =/hatwideR, the lower bound is attainble which proves
the lemma.
G.4 Proof of Proposition 4.2
Statement: Letcconst(R,R/prime)be a constant cost function, and assume that cconst∈C. Then problem (P2a)
is unbounded from below.
Proof.We will show that for any π, the value of the inner optimization problem of (P2a)equals−∞, thereby
proving the result.
Letπbe an arbitrary policy and consider an arbitrary vector R. By deﬁnition of cconstand the fact that /hatwideRis
feasible for (P1),/hatwideR∈A(cconst,R,π†,/epsilon1†). Therefore, R,cconstare feasible for (P2a). SinceRwas arbitrary, it
can be chosen such that the ρπis arbitrarily low, proving the claim.
31Published in Transactions on Machine Learning Research (01/2023)
G.5 Proof of Theorem 4.3
Statement: There exists MDP M= (S,A,/hatwideR,P,γ,σ )for which problem (P2a)is unbounded from below
whenc∞∈C.
Proof.Letπbe an arbitrary policy. We will build an example where the value of the inner minimization
problem of (P2a) is −∞.
Consider a single-state MDP with the reward function /hatwideR= (1,0,0,0)whereπ†is the ﬁrst action and /epsilon1†= 1.
Since/hatwideRissymmetricwithrespectto a2,a3,a4, weassumewithoutlossofgeneralitythat π(a2)≤π(a3)≤π(a4).
For anyx, consider the reward function R/prime= (1−x,x,−x,−x)with actions a1,a2,a3,a4. We claim that
/hatwideR∈A(c∞,R/prime,π†,/epsilon1†). To see why, consider the attack optimization problem (P1). The cost of the attack
optimization problem is at least xbecauseR/prime(π†)−R/prime(a2) = 1−2xand any feasible /tildewideRneeds to satisfy
/tildewideR(π†)−/tildewideR(a2)≥1, which implies
/vextenddouble/vextenddouble/vextenddoubleR/prime−/tildewideR/vextenddouble/vextenddouble/vextenddouble
∞≥max{/tildewideR(π†)−R/prime(π†),R/prime(a2)−/tildewideR(a2)}
≥1
2/parenleftBig
/tildewideR(π†)−R/prime(π†) +R/prime(a2)−/tildewideR(a2)/parenrightBig
=1
2(2x−1 + 1) =x
Since/vextenddouble/vextenddouble/vextenddoubleR/prime−/hatwideR/vextenddouble/vextenddouble/vextenddouble
∞=x, we conclude /hatwideR∈A(c∞,R/prime,π†,/epsilon1†)as claimed.
We now consider the score of the policy πunder the reward function R/primeand show that, with the proper
choice ofx, it is unbounded from below. Since /hatwideR∈A(c∞,R/prime,π†,/epsilon1†), this would show that the value of the
inner minimization in (P2a) equals −∞, proving the theorem.
By deﬁnition, ρR/prime,πcan be written as
ρR/prime,π=π(a1) +x(π(a2)−π(a1)−π(a3)−π(a4)) =π(a1) +x(π(a2)−(1−π(a2))) =π(a1) +x(2π(a2)−1).
Sinceπ(a2)≤π(a3)≤π(a4)however, we conclude that π(a2)<0.5. Therefore, tending xto inﬁnity ﬁnishes
the proof.
H Computational hardness results
In this section, we provide computational hardness results for diﬀerent choices of C, showing that the defense
optimization problem (P2a)is NP-hard in diﬀerent cases. While our results are stated for the defense
optimization problem (P2a), all results hold for (P2b)as well with /epsilon1=min{/epsilon1D,/hatwide/epsilon1}. Our proofs rely on the
results of Appendix G and we therefore refer to this and rely on notation introduced in this Appendix; namely,
the notation Φθandupas introduced in Lemma G.2.
H.1 Hardness result for p=∞
4.5 We begin by considering the case of c∞. By reducing the 3SAT problem to the defense optimization
problem (P2a), we will show that (P2a) is NP-hard. More formally, we prove the following theorem.
Theorem H.1. ForC={c∞}, it is NP-hard to determine whether the optimal value of problem (P2a)is
greater than or equal to /hatwideρπ†.
We assume without loss of generality that the clauses do not contain duplicate variables as the 3SAT instance
remains NP-hard in this case.
Before proving the theorem, we prove a weaker result which essentially proves hardness assuming we can set
the matrix Φθand vectorµπ†can be set arbitrarily, without the restriction that they correspond to an MDP.
32Published in Transactions on Machine Learning Research (01/2023)
PropositionH.2. Assume we are given an instance of 3SAT with clauses c1,...,cmand variables (x1,...xm).
Deﬁnek= 2n+ 1and/lscript= 4n+m+ 1. It is possible to build, in polynomial time, a matrix /tildewiderM∈Rk×/lscript, a
vector/tildewideµ∈R/lscriptsuch that/tildewideµis strictly positive and
•If the 3SAT instance is satisﬁable, then there exists a λ/negationslash= 0∈Rksatisfyingλ<0such that
/angbracketleftBig
u∞(/tildewiderMTλ),/tildewideµ/angbracketrightBig
<−1
4.
In addition,/tildewiderMTλdoes not contain any zero entries.
•If the 3SAT instance is not satisﬁable, then for any λ/negationslash= 0∈Rksatisfyingλ<0,
/angbracketleftBig
u∞(/tildewiderMTλ),/tildewideµ/angbracketrightBig
>1
4
Proof.We begin by an empty matrix and vector and in each step, add a value to /tildewideµand add a column of size
kto/tildewiderM.
We will let (a0,a1,...,an,b1,...,bn)be placeholder names for the coordinates of λSince the optimization
problem we are considering involves the matrix product /tildewiderMTλ, for ease of notation, we will specify each
column of/tildewiderMby the linear map forming the corresponding coordinate of /tildewiderMTλ. As an example, a0+ 4a3+ 5b3
is a column that has value 1 in a0, 4 ina3, 5 inb3and 0 everywhere else.
Deﬁningc:= 500k, we ﬁrst add the columns below.
c/parenleftBigg
a0+n/summationdisplay
i=1ai+n/summationdisplay
i=1bi/parenrightBigg
(24)
0.9a0−(ai+bi)∀i∈[n] (25)
−1.1a0+ (ai+bi)∀i∈[n] (26)
4 (0.9a0−(ai−bi))∀i∈[n] (27)
4 (0.9a0−(bi−ai))∀i∈[n] (28)
The value of/tildewideµfor(24),(25),(26),(27)and(28)is(6n2+m−1
2),3n,3n,1and1respectively. Note that
(24)is the all-ones vector. The constant cin(24)and the constant 4in(27)and(28)will not matter for the
proof, but will be useful for later.
Next for each clause with variables (xi,xj,xk), we will add the following column where didenotesaiif the
variablexiappears as positive in the clause and biif the variable appears as negative.
0.95·a0−(di+dj+dk) (29)
The value of/tildewideµin the above is 1.
We now prove that /tildewideµ,/tildewiderMhave the mentioned properties.
If the 3SAT instance is satisﬁable, this is easy to show. Set a0= 1and set (ai,bi)to(1,0)ifxiis
set to true in the satisﬁable arrangement and to (0,1)ifxiis set to 0. It is clear that the coordinates
ofu(/tildewiderMTλ)corresponding to (24),(25)and(26)equal 1,−1and−1respectively. Furthermore, since
{ai−bi,bi−ai}={1,−1}, exactly half of the coordinates of u(/tildewiderMTλ)corresponding to (27)and(28)equal 1,
while the other half equal −1. Furthermore, all of the coordinates corresponding to (29)equal -1. Therefore,
the inner product/angbracketleftBig
/tildewideµ,u(/tildewiderMTλ)/angbracketrightBig
equals
(6n2+m−1
2)−n·3n−n·3n+ 0−m=−1
2
which proves the claim.
33Published in Transactions on Machine Learning Research (01/2023)
Conversely, assume that the 3SAT instance is not satisﬁable. Let s1,...s/lscriptdenote the coordinates of /tildewideµwiths1
corresponding to (24)and letu[s]denote/parenleftBig
u(/tildewiderMTλ)/parenrightBig
(s). Letλ/negationslash= 0satisfyingλ<0be an arbitrary vector.
First note that since λ/negationslash= 0,/parenleftBig
/tildewiderMTλ/parenrightBig
(s1)is strictly positive and therefore u[s1] = 1. Since/tildewideµ(s1) = 6n2+m−1
2,
we need to show that
/angbracketleftBig
u(/tildewiderMTλ),/tildewideµ/angbracketrightBig
>1
4⇐⇒/summationdisplay
s/negationslash=s1/tildewideµ(s)·u[s]>−6n2−m+1
2+1
4(30)
⇐⇒/summationdisplay
s/negationslash=s1/tildewideµ(s)·(u[s] + 1)
2>1
2·/parenleftbigg
−6n2−m+3
4+ 2n·3n+ 2n·1 +m/parenrightbigg
(31)
⇐⇒/summationdisplay
s/negationslash=s1/tildewideµ(s)·1[u[s] = 1]>n+3
8(32)
Now note that since the value of /tildewideµin coordinates corresponding to (25)and(26)is>n+3
8, ifu[s]equals
one in any of these coordinates, then the claim is proved.
Therefore, we assume w.l.o.g that for any i,
0.9a0≤ai+bi≤1.1a0.
This implies that if any of {ai,bi}i≥1is strictly positive, so is a0. Since at least one of {a0,ai,bi}needs to be
strictly positive by the assumption λ/negationslash= 0, we conclude that a0is strictly positive.
Now note that u[s]is1in more than half of the coordinates corresponding to (27)and(28), then(32)will
hold. We can therefore assume w.l.o.g that at least half of these coordinates equal −1. We note however that
ifu[s]equals−1forboth(27) and (28) for some 1≤i≤n, then
0.9a0≤ai−bi∧0.9a0≤bi−ai=⇒a0≤0 =⇒a0= 0,
which is not possible. We can therefore conclude that for any ﬁxed i,u(/tildewiderMTλ)equals 1 in exactly one of the
two coordinates corresponding to (27)and(28). Therefore, either ai−bi≥0.9a0orbi−ai≥0.9a0. Since
ai+bi≥0.9a0, we get that either ai≥0.9a0orbi≥0.9a0. Sinceai+bi≤1.1a0, the bigger one is ≥0.9a0
and the smaller one is ≤0.2a0.
Finally, note that if u[s]equals 1 for any of the coordinates corresponding to (29), then(32)would hold since
u[s]was already 1 for half of the coordinates corresponding to (27)and(28). We can therefore assume that
u[s]is−1for all the coordinates corresponding to (29). Note however that if di+dj+dk≥0.95a0, then at
least one of the dimust have been >0.2. This means that all of the clauses must hold true (in the 3SAT
sense) with xiset to
xi=/braceleftBigg
1,ifai≥0.5
−1,otherwise.
This is not possible however as we assumed that the 3SAT instance was not satisﬁable.
Claim H.3. Deﬁne/tildewiderMas above and let s1denote the column of the matrix corresponding to (24). For all
rows/tildewidevin/tildewiderM, we have
/tildewidev(s1)≥50/summationdisplay
i>1|/tildewidev(si)|,
which further implies/summationtext
i≥1|/tildewidev(si)|≤(1 + 1/50)/tildewidev(s1).
Proof.The claim follows trivially from the fact that as /tildewidev(s1) =cand all other entries in /tildewidevare less than 4.
Next, we prove the following lemma which essentially states that for any desired value of Θ/epsilon1†, we can ﬁnd a
plausible reward function /hatwideRsuch that Θ/epsilon1†equals this value.
34Published in Transactions on Machine Learning Research (01/2023)
Lemma H.4. LetMbe an ergodic MDP with unspeciﬁed reward function and let π†be a policy in this MDP.
For any value of /epsilon1†>0and any set of state-action pairs Θ⊆S×Asatisfying Θ∩{(s,π(s)) :s∈S}=∅,
there is a reward function /hatwideRsuch that
1./hatwideRis feasible for the attack problem (P1), i.e, ΦT/hatwideR4−/epsilon1†.
2.Θ/epsilon1†= Θ.
Proof.Consider the following reward function,
/hatwideR(s,a) =

0 ifa=π†(s)
−/epsilon1†
µπ†{s;a}(s)ifa∈Θ
−2·/epsilon1†
µπ†{s;a}(s)o.w.
It is clear that ρπ†= 0and
ρπ†{s;a}=/braceleftBigg
−/epsilon1†if(s,a)∈Θ
−2/epsilon1†o.w.,for all (s,a/negationslash=π†(s)).
which proves the claim.
We now use the above results to construct an MDP, formally proving Theorem H.1. Given an instance of
3SAT, let/tildewiderM,/tildewideµdenote the values speciﬁed in Proposition H.2. Recall that /tildewiderM∈Rk×/lscriptand/tildewideµ∈R/lscriptwhere
k= 2n+ 1and/lscript= 4n+m+ 1. Letδ >1/2,γ∈[1/2,1]be parameters to be speciﬁed later. (see Claims
H.11, H.7, and H.5.) Intuitively, we need δto be close to zero and γto be close to one.
Given these values, we will build an MDP with reward vector /hatwideRwith/lscript+ 3states andk+ 1actions for which
|Θ/epsilon1|=kand the 3SAT instance is satisﬁable if and only if
∃λ/negationslash= 0∈Rk:λ≥0∧/angbracketleftbig
u∞(ΦT
θλ),ψπ†/angbracketrightbig
<0. (33)
In our construction, the states si≥1will each correspond to the columns of /tildewiderMwhile the states s−2,s−1,s0
will be new. Furthermore, in state s0, each of the actions a/negationslash=π†(s0)will correspond to a row of /tildewiderM.
LetMdenote the submatrix of Φθwith only columns corresponding to (s,π†(s)). We ﬁrst observe that (33)
can be simpliﬁed because ψπ†(s,a)is 0 for alls,a/negationslash=π†(s). Therefore, (33) is equivalent to
∃λ/negationslash= 0∈Rk:λ≥0∧/angbracketleftbig
u∞(MTλ),µπ†/angbracketrightbig
<0. (34)
In order to specify this MDP, we ﬁrst specify the transition probabilities of π†. In states0, followingπ†leads
tosifori∈{− 1,−2}with probability δ/2and leadssifori≥1with the probability (1−δ)/hatwideµ(si)where
/hatwideµ=/tildewideµ
/bardbl/tildewideµ/bardbl1. (35)
In statess/negationslash=s0, followingπ†will lead back to swith probability 1. The initial distribution σof the MDP is
chosen asσ(s0) = 1andσ(si) = 0fori/negationslash= 0.
It is straightforward to see that
µπ†(s0) = (1−γ),∀i∈{− 2,−1}:µπ†(si) =γ·δ
2,∀i≥1 :µπ†(si) =γ·(1−δ)·/tildewideµ(si)
/bardbl/tildewideµ/bardbl1.
Now, for each of the rows in /tildewiderMlike/tildewidev, we add an action ato states0with the following transition probabilities.
P(s0,a,sj) =

P(s0,π†(s0),sj) ifj= 0
P(s0,π†(s0),sj)−δ
4θ·(/summationtext
i/tildewidev(si))ifj∈{− 2,−1}
P(s0,π†(s0),sj) +δ
2θ·/tildewidev(sj)ifj≥1. (36)
35Published in Transactions on Machine Learning Research (01/2023)
whereθis taken to be large enough such that for all /tildewidev,
θ·min
i/hatwideµ(si)≥/bardbl/tildewidev/bardbl1.
Note that the value of θis the same for all the rows of matrix; it is set to the maximum of1
mini/hatwideµi·/bardbl/tildewidev/bardbl1
across all rows /tildewidevof/tildewiderM.
Claim H.5. Ifδ≤1
10, the transition probabilities speciﬁed in (35)arevalid, i.e., they are in the range [0,1]
and/summationtext
s/primeP(s0,a,s/prime) = 1.
Proof.In order to make sure these transition probabilities are valid, they need to sum to one and they all
need to be non-negative. They sum to one by deﬁnition. As for being non-negative, it holds trivially for
j= 0and holds for j/negationslash= 0by deﬁnition of δ. Formally, for j≤−1,
P(s0,π†(s0),s0)−δ
4·/parenleftBigg/summationdisplay
iv(si)/parenrightBigg
=δ·/parenleftbigg1
2−1
4·/summationtext
i/tildewidev(si)
θ/parenrightbigg
≥δ/parenleftbigg1
2−1
4·(min
i/hatwideµi)·/summationtext/tildewidev(si)
/bardbl/tildewidev/bardbl1/parenrightbigg
≥δ(1
2−1
4)
>0
and forj≥1,
P(s0,π†(s0),sj) +δ
2·/tildewidev(sj)
θ≥P(s0,π†(s0),sj)−δ
2·|/tildewidev(sj)|
θ
≥(1−δ) min
i/hatwideµi−δ
2·/tildewidev(sj)
/bardbl/tildewidev/bardbl1·/parenleftBig
min
i/hatwideµi/parenrightBig
≥(min
i/hatwideµi)·(1−3δ
2)>0
Now, using Lemma H.4, set the reward for the MDP such that Θ/epsilon1†consists of the added actions in s0.
Note that there are multiple actions in the states si/negationslash=0as well; however, given Lemma H.4, their transition
probabilities are not important and can be set arbitrarily as long as the MDP remains ergodic.
Claim H.6. For alla,/negationslash=π†(s0),
(MTλ)(a) =/parenleftbigg
−δ
4θ·γ·(/summationdisplay
/tildewidev(sj)),−δ
4θ·γ·(/summationdisplay
/tildewidev(sj)),−(1−γ), γ·δ
2θ·/tildewidev/parenrightbigg
,
where/tildewidevis the row corresponding to aand we have abused notation by using γ·δ
2·/tildewidevto denote the last k
entries of the vector.
Proof.As before, it is straightforward to see that µπ†{s0;a}(s0) = (1−γ)and
∀i∈{− 2,−1}:µπ†{s0;a}(si) =γ·/parenleftbiggδ
2−δ
4θ·(/summationdisplay
/tildewidev(sj))/parenrightbigg
,
and
∀i≥1 :µπ†{s0;a}(si) =γ·/parenleftbigg
(1−δ)·/hatwideµ(si) +δ
2θ·/tildewidev(si)/parenrightbigg
.
36Published in Transactions on Machine Learning Research (01/2023)
Claim H.7. Ifmax{δ,1−γ}≤1
4/bardbl/tildewideµ/bardbl1,then
/bardbl/tildewideµ/bardbl1
γ(1−δ)
0/summationdisplay
j=−2µπ†(sj)
≤1/8
Proof.
/bardbl/tildewideµ/bardbl1
γ(1−δ)·
0/summationdisplay
j=−2µπ†(sj)
=δ·γ+ (1−γ)
γ(1−δ)·/bardbl/tildewideµ/bardbl1 (37)
≤1
4·/bardbl/tildewideµ/bardbl1·(δ+ (1−γ))≤1
8(38)
Claim H.8. The 3SAT instance is satisﬁable if and only if (34)holds.
Proof.Given Claim H.6, the sub-matrix of Mcorresponding to columns sj≥1equals
γ·δ
2·θ·/tildewiderM.
Letu∞[s]denoteu∞/parenleftbig
(MTλ)(s)/parenrightbig
. For anyλ,
/angbracketleftbig
µπ†,u(MTλ)/angbracketrightbig
=0/summationdisplay
j=−2µπ†(sj)u∞[sj] +/angbracketleftbiggγ·(1−δ)
/bardbl/tildewideµ/bardbl1·/tildewideµ,u(γ·δ
2·θ·/tildewiderMTλ)/angbracketrightbigg
=0/summationdisplay
j=−2µπ†(sj)u∞[sj] +/angbracketleftbiggγ·(1−δ)
/bardbl/tildewideµ/bardbl1·/tildewideµ,u(/tildewiderMTλ)/angbracketrightbigg
Multiplying both sides by/bardbl/tildewideµ/bardbl1
γ(1−δ), it follows that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bardbl/tildewideµ/bardbl1
γ(1−δ)·/angbracketleftbig
µπ†,u(MTλ)/angbracketrightbig
−/angbracketleftBig
/tildewideµ,u(/tildewiderMTλ)/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/bardbl/tildewideµ/bardbl1
γ(1−δ)·
0/summationdisplay
j=−2|µπ†(s)u∞[s]|
 (39)
≤/bardbl/tildewideµ/bardbl1
γ(1−δ)·
0/summationdisplay
j=−2µπ†(s)
 (40)
≤1
8(41)
Therefore, by Lemma H.4, if the 3SAT is satisﬁable, then there exists a λsuch that
/bardbl/tildewideµ/bardbl1
γ(1−δ)·/angbracketleftbig
µπ†,u(MTλ)/angbracketrightbig
≤−1
4+1
8<0.
If the 3SAT is not satisﬁable, then by Lemma H.4, for all feasible λ,
/bardbl/tildewideµ/bardbl1
γ(1−δ)·/angbracketleftbig
µπ†,u(MTλ)/angbracketrightbig
≥1
4−1
8>0
which proves the claim.
37Published in Transactions on Machine Learning Research (01/2023)
H.2 Proof of Theorem 4.5
Statement: ForC={cps.t.p∈[1,∞)}, it is NP-hard to determine whether the optimal value of problem
(P2b)is greater than or equal to /hatwideρπ†We will prove the result using the same MDP as the previous Section.
We need to show the 3SAT instance is satisﬁable if and only if
∃p∈[1,∞),λ∈Rk:λ≥0∧/angbracketleftbig
up(ΦT
θλ),ψπ†/angbracketrightbig
<0.
As before, let Mdenote the submatrix of Φθwith only columns corresponding to (s,π†(s)). As in the
previous section we note that because ψπ†(s,a)is 0 for alls,a/negationslash=π†(s), the above condition is equivalent to
∃p∈[1,∞),λ∈Rk:λ≥0∧/angbracketleftbig
up(MTλ),µπ†/angbracketrightbig
<0. (42)
We split the proof into two lemmas.
Lemma H.9. If the 3SAT instance is satisﬁable, then (42)holds.
Proof.In this case, then the same proof as before basically holds. Formally, consider the λused before in
the proof and consider the vector MTλ. We need to show that there exists pfor which/angbracketleftbig
µπ†,up(MTλ)/angbracketrightbig
is
negative. Note however that
/angbracketleftbig
µπ†,up(MTλ)/angbracketrightbig
=k/summationdisplay
i=−2µπ†(si)·up/parenleftbig
(MTλ)(si)/parenrightbig
For ﬁxedx/negationslash= 0,up(x)is a continuous function of pandlim∞up(x) =u∞(x). If we show that all of the
coordinates in MTλare non-zero, this would imply that/angbracketleftbig
µπ†,up(MTλ)/angbracketrightbig
converges to/angbracketleftbig
µπ†,u∞(MTλ)/angbracketrightbig
<0
for large enough pwhich proves the claim. Therefore,/angbracketleftbig
µπ†,up(MTλ)/angbracketrightbig
is continuous in pand letting pbe
large enough proves the claim.
It remains to verify that all of the coordinates in MTλused in the above proof were non-zero. Since all of
the coordinates of /tildewiderMTλwere non-zero by our construction of λ, we conclude that MTλis non-zero on si≥1.
Fori∈{− 2,−1}, given claim H.3, we have/summationtext
i≥1/tildewidev(si)>0, for all rows/tildewidevof/tildewiderM, which means MTλis strictly
negative on these states. Finally, the entry corresponding to s0equals−µπ†(s0)in all rows of M, which
implies/parenleftbig
MTλ/parenrightbig
(s0)is strictly negative, proving the claim.
Lemma H.10. If(42)holds, then the 3SAT instance is satisﬁable.
Proof.We assume that λ/negationslash= 0without loss of generality; if λ= 0then/angbracketleftbig
µπ†,up(MTλ)/angbracketrightbig
= 0which is not
negative.
Lettingup[s]denoteup/parenleftbig
(MTλ)(si)/parenrightbig
anddp[s]denoteµπ†(s)·up[s], then/angbracketleftbig
µπ†,up(MTλ)/angbracketrightbig
can be rewritten as
/angbracketleftbig
µπ†,up(MTλ)/angbracketrightbig
=/lscript/summationdisplay
i=−2µπ†(si)·up[si] (43)
=0/summationdisplay
i=−2dp[si] +/summationdisplay
(24)dp[s] +/summationdisplay
(25)dp[s] +/summationdisplay
(26)dp[s] +/summationdisplay
(27)dp[s] +/summationdisplay
(28)dp[s] +/summationdisplay
(29)dp[s](44)
where in the above, we have broken the sum in diﬀerent parts, depending on what sicorresponds to and we
have abused the notation by using the set(24)denote all states that correspond to Equation (24). Note that
the sum corresponding to (24) consists of a single state. We will also use s1to denote this state.
We begin by stating some properties of upanddp.
Claim H.11. Letγbe close enough to 1 such that
cδ
4θ≥(1−γ).
38Published in Transactions on Machine Learning Research (01/2023)
Then for all rows /tildewidevfrom/tildewiderM,
∀s/negationslash=s1:|up[s]|≤up[s1] (45)
Proof.We need to show that for all rows vinM,|v(s1)|≥|v(s)|for alls/negationslash=s1. Fori≥1, this follows
immediately from Claim H.3 and Claim H.6.
As fors0, we need to show that
v(s1)≥|v(s0)| ⇐⇒γ·δ
2θ/tildewidev(s1)≥(1−γ)
Note however that
γ·δ
2θ/tildewidev(s1)≥δ
4θ/tildewidev(s1)≥(1−γ)
by the assumption on γ.
Claim H.12. For anyi∈[n], letting ˜siand˜s/prime
idenote the states corresponding to (27)and(28)respectively.
dp[˜si] +dp[˜s/prime
i]≥0.
Proof.To prove this, observe that
0.9a0−(ai−bi) + 0.9a0−(bi−ai) = 1.8a0≥0
There are therefore two possibilities: (a)Both 0.9a0−(ai−bi)and0.9a0−(bi−ai)are non-negative.
In this case, both dp[˜si]anddp[˜s/prime
i]are non-negative and the claim follows. (b)0.9a0−(ai−bi)≥0
and0.9a0−(bi−ai)≤0or vice versa. Assume w.l.o.g that 0.9a0−(ai−bi)≥0, i.e,dp[˜si]≥0. In
this case,|0.9a0−(ai−bi)|≥|0.9a0−(bi−ai)|and therefore, since µπ†(˜si) =µπ†(˜s/prime
i), we conclude that
dp[˜si]≥|dp[˜s/prime
i]|and therefore the claim follows.
We split the proof into several cases.
Case 1: There exists s∈(25)∪(26) such that dp[s]≥0.
Letsbe such a state In this case, deﬁne /tildewidedp[s]as follows
/tildewidedp[s] =

0 ifs=s
0 ifs∈(27)∪(28)
dp[s]ifs=s1
−|dp[s]|o.w.
We ﬁrst claim that/summationtextdp[s]≥/summationtext/tildewidedp[s]. To prove this, note that dp[s]≥/tildewidedp[s]for alls /∈(27)∪(28). We
therefore need to prove that
/summationdisplay
s∈(27)∪(28)dp[s]≥0,
which follows from Claim H.12. by summing over all i∈[n].
39Published in Transactions on Machine Learning Research (01/2023)
Deﬁning/tildewideS:=S\({s1,s}∪(27)∪(28)), we note that
/summationdisplay
s/tildewidedp[s] =dp[s1]−/summationdisplay
s∈/tildewideS|dp[si]|
=µπ†(s1)·up[s1]−/summationdisplay
s∈/tildewideSµπ†(s)·|up[si]|
(45)
≥µπ†(s1)·up[s1]−/summationdisplay
s∈/tildewideSµπ†(s)·|up[s1]|
=|up[s1]|
µπ†(s1)−/summationdisplay
s∈/tildewideSµπ†(s)

Deﬁning/tildewideS+:=/tildewideS\{s−2,s−1,s0},
/tildewideµ(s1)−/summationdisplay
s∈/tildewideS+/tildewideµ(s) = 6n2+m−1
2−(2n−1)·3n−m
= 6n2+m−1
2−6n2+ 3n−m
= 3n−1
2>1
4,
Claim H.7 now gives a contradiction as the inner product in (42) becomes positive.
Case 2:dp[s]<0for alls∈(25)∪(26) and|aj−bj|≤0.9a0for somej∈[n].
In this case, we conclude that
ai+bi∈[0.9,1.1]a0∀i∈[1,n]
Since either aiorbimust be strictly positive for some i(becauseλ/negationslash= 0), we conclude that a0>0.
Similar to case 1, we introduce a new vector /tildewidedpsuch that/summationtext
sdp[s]≥/summationtext
s/tildewidedp[s].
Let/tildewidesjand/tildewides/prime
jdenote the states corresponding to (27)and(28)respectively for i=j. Note that by assumption,
up[/tildewidesj],up[/tildewides/prime
j]≥0. Assume without loss of generality that up[/tildewidesj]≥up[/tildewides/prime
j]. Deﬁne the vector /tildewidedpas
/tildewidedp[s] =

0 ifs∈(27)∪(28)\{/tildewidesj}
dp[s]ifs∈{s1,/tildewidesj}
−|dp[s]|o.w.
As before,/summationtextdp[s]≥/summationtext/tildewidedp[s]. More formally, for (27)and(28), we use Claim H.12 and for all the other
states, we have dp[s]≥/tildewidedp[s].
We now claim that
∀s∈(25)∪(26)∪(29):|up[s]|≤up[/tildewidesj]. (46)
This is because
(MTλ)(/tildewidesj)≥(MTλ)(/tildewidesj) + (MTλ)(/tildewides/prime
j)
2= 3.6a0,
while/vextendsingle/vextendsingle(MTλ)(s)/vextendsingle/vextendsingle≤2.35a0for alls∈(25)∪(26)∪(29). This is because ai+bi∈[0.9·a0,1.1·a0].
40Published in Transactions on Machine Learning Research (01/2023)
Deﬁning/tildewideS:=(25)∪(26)∪(29), we conclude that
/summationdisplay
sdp[s]≥/summationdisplay
s/tildewidedp[s]
=µπ†(s1)·up[s1] +µπ†(/tildewidesj)·up[/tildewidesj] +/summationdisplay
s∈/tildewideSµπ†(s)·−|up[s]|+0/summationdisplay
i=−2µπ†(si)·−|up[si]|
≥µπ†(s1)·up[s1] +µπ†(/tildewidesj)·up[/tildewidesj] +/summationdisplay
s∈/tildewideSµπ†(s)·−|up[/tildewidesj]|+0/summationdisplay
i=−2µπ†(si)·−|up[s1]|
=/parenleftBigg
µπ†(s1)−0/summationdisplay
i=−2µπ†(si)/parenrightBigg
·up[s1] +µπ†(/tildewidesj)·up[/tildewidesj] +/summationdisplay
s∈/tildewideSµπ†(s)·−|up[/tildewidesj]|
≥/parenleftBigg
µπ†(s1)−0/summationdisplay
i=−2µπ†(si)/parenrightBigg
·up[/tildewidesj] +µπ†(/tildewidesj)·up[/tildewidesj] +/summationdisplay
s∈/tildewideSµπ†(s)·−|up[/tildewidesj]|
=up[/tildewidesj]·
µπ†(s1) +µπ†(/tildewidesj)−0/summationdisplay
i=−2µπ†(si)−/summationdisplay
s∈/tildewideSµπ†(s)

Here, for the ﬁnal inequality we have used the fact that µπ†(s1)−/summationtext0
i=−2µπ†(si)≥0,which follows from
Claim H.7.
As before, note that
/tildewideµ(s1) +/tildewideµ(/tildewidesj)−/summationdisplay
s∈/tildewideS/tildewideµ(s) = 6n2+m−1
2+ 1−2n·3n−m≥1
2
Claim H.7 now gives a contradiction as the inner product in (42) becomes positive.
Case 3:dp[s]<0for alls∈(25)∪(26) and|ai−bi|≥0.9a0for all 1≤i≤n.
Similar as before, deﬁne /tildewidedpas
/tildewidedp[s] =

0 ifs∈(27)∪(28)
dp[s] ifs=s1
min{dp[s],0}ifs∈(29)
−|dp[s]|o.w.
As before,/summationtext
sdp[s]≥/summationtext
s/tildewidedp[s]. Therefore,
/summationdisplay
sdp[s]≥/summationdisplay
s/tildewidedp[s]
=µπ†(s1)·up[s1]−/summationdisplay
s∈(25)∪(26)µπ†(s)·|up[s]|+/summationdisplay
s∈(29)µπ†(s)·min{up[s],0}−0/summationdisplay
i=−2µπ†(si)·|up[si]|
≥µπ†(s1)·up[s1]−/summationdisplay
s∈(25)∪(26)µπ†(s)·|up[s1]|−/summationdisplay
s∈(29)µπ†(s)·|up[s1]|1[up[s]<0]−0/summationdisplay
i=−2µπ†(si)·|up[s1]|
=up[s1]·
µπ†(s1)−/summationdisplay
s∈(25)∪(26)µπ†(s)−/summationdisplay
s∈(29)µπ†(s)1[up[s]<0]−0/summationdisplay
i=−2µπ†(si)

Assume for the sake of contradiction that the 3SAT instance is not satisﬁable. There are two possibilities.
The ﬁrst is that for at least one s∈(29), we have up[s]≥0. In that case,
/tildewideµ(s1)−/summationdisplay
s∈(25)∪(26)/tildewideµ(s)−/summationdisplay
s∈(29)/tildewideµ(s)1[up[s]<0]≥1
2
41Published in Transactions on Machine Learning Research (01/2023)
As before, Claim H.7 gives a contradiction. Otherwise, up[s]<0for alls∈(29). In that case, we have
/angbracketleftBig
u∞(/tildewiderMTλ),/tildewideµ/angbracketrightBig
= 6n2+m−1/2−2n·3n−m<−1/2,
which contradicts Proposition H.2 as λ/negationslash= 0.
I Proofs of Appendix B
In this section, we provide a more formal treatment of the results in Appendix B, formally stating and proving
these results.
Proposition I.1. Assume that condition (3)holds. Set/hatwide/epsilon1as
/hatwide/epsilon1= min
s,a/negationslash=π†(s)/bracketleftBig
/hatwideρπ†−/hatwideρπ†{s;a}/bracketrightBig
.
Consider the following policy
πD(a|s) =1[a∈Θ/epsilon1
s∪{π†(s)}]
|Θ/epsilon1s|+ 1. (47)
Equation (47)characterizes the solution to the optimization problems (P2a)and(P2b)with parameters /epsilon1=/epsilon1†
and/epsilon1= min{/hatwide/epsilon1,/epsilon1D}respectively. Furthermore, in both cases ρπD=/hatwideρπD
Proof.Given Theorem 5.1, Theorem 5.2, and Lemma D.3, it suﬃces to show that if /epsilon1≤/hatwide/epsilon1, the solution to the
optimization problem
max
ψ∈Ψ/angbracketleftBig
ψ,/hatwideR/angbracketrightBig
(P4)
s.t./angbracketleftBig
ψπ†{s;a}−ψπ†,ψ/angbracketrightBig
≥0∀s,a∈Θ/epsilon1,
corresponds to the occupancy measure of policy πDdeﬁned by Equation (47). Namely, the optimization
problems (P2a)and(P2b)correspond to the optimization problem (P3)with parameters /epsilon1=/epsilon1†and
/epsilon1=min{/hatwide/epsilon1,/epsilon1D}respectively. Since /hatwide/epsilon1≤/epsilon1†, the primal feasibility condition in Lemma E.1 implies that the
solution to the above optimization problem characterizes both cases ((P2a) and (P2b)).
Now, due to Lemma D.3, we have
ψ∈Ψ⇐⇒ψ<0∧∀s:/summationdisplay
aψ(s,a) = (1−γ)σ(s) +γ/summationdisplay
˜s,˜aP(˜s,˜a,s)ψ(˜s,˜a).
SinceP(˜s,˜a,s)is independent of ˜a, the second condition is equivalent to
∀s:/summationdisplay
aψ(s,a) = (1−γ)σ(s) +γ/summationdisplay
˜s/parenleftBig
P(˜s,π†(˜s),s)(/summationdisplay
˜aψ(˜s,˜a))/parenrightBig
,
which, due to (12), is equivalent to
/summationdisplay
aψ(s,a) =µ(s).
Furthermore, given the independence of the transition distributions from policies, we have the following
/parenleftbig
ψπ†{s;a}−ψπ†/parenrightbig
(˜s,˜a) =

µ(s)if(˜s,˜a) = (s,a)
−µ(s)if(˜s,˜a) = (s,π†(s))
0o.w. (48)
42Published in Transactions on Machine Learning Research (01/2023)
Therefore, the constraint/angbracketleftbig
ψπ†{s;a}−ψπ†,ψ/angbracketrightbig
≥0is equivalent to ψ(s,a)≥ψ(s,π†(s)). Furthermore, note
that
(s,a)∈Θ/epsilon1⇐⇒/hatwideρπ†{s;a}−/hatwideρπ†=−/epsilon1†
⇐⇒/angbracketleftBig
ψπ†{s;a}−ψπ†,/hatwideR/angbracketrightBig
≤−/epsilon1†
⇐⇒/hatwideR(s,a)−/hatwideR(s,π†(s)) =−/epsilon1†
µ(s)
⇐⇒a∈Θ/epsilon1
s.
Putting it all together, the optimization problem (P3) is equivalent to
max
ψ/angbracketleftBig
/hatwideR,ψ/angbracketrightBig
s.t.ψ(s,π†(s))≤ψ(s,a)∀s,a∈Θ/epsilon1
s/summationdisplay
aψ(s,a) =µ(s)∀s∈S
ψ(s,a)≥0∀(s,a).
Note that the maximization is now over all vectors ψ∈R|S|.|A|as the constraint ψ∈Ψhas been made
explicit. Furthermore, given Lemma D.3 and Equation (5), any vector ψsatisfying the last two constraints
(the bellman constraints) corresponds to a policy πthrough
π(a|s) =ψ(s,a)
µ(s).
In other words, probability of choosing ain statesis proportional to ψ(s,a).
Now, let us analyze the solution to this optimization problem which we will denote by ψmax. This solution
ψmaxexists, since the optimization problem is maximizing a continuous function on a closed and bounded set.
We ﬁrst claim that if a /∈Θ/epsilon1
s∪{π†(s)}, thenψmax(s,a) = 0. If this is not the case, then ψmaxis not optimal.
Concretely, consider the following vector ψ
ψ(˜s,˜a) =

ψmax(˜s,˜a) +1
|Θ/epsilon1s|+1ψmax(s,a)if˜s=s∧˜a∈Θ/epsilon1
s∪{π†(s)}
0if˜s=s∧˜a=a
ψmax(˜s,˜a)o.w..
In other words, we uniformly spread the probability of choosing action ain statesover the set Θ/epsilon1
s∪{π†(s)}.
The vector ψstill satisﬁes the constraints: if ˜a∈Θ/epsilon1
s,ψ(s,π†(s))−ψ(s,˜a) =ψmax(s,π†(s))−ψmax(s,˜a)and
the objective has strictly improved because
/hatwideρπ†{s;a}−/hatwideρπ†≤−/hatwide/epsilon1≤−/epsilon1=⇒/hatwideR(s,a)≤/hatwideR(s,π†(s))−/epsilon1
µ(s).
Sincea /∈Θ/epsilon1
s, the inequality is strict and therefore
∀˜a∈Θ/epsilon1
s∪{π†(s)}:/hatwideR(s,˜a)>/hatwideR(s,a).
This means that ψwas not optimal, contradicting the initial assumption.
Now note that if ψmax(s,a)> ψ max(s,π†(s))for somea∈Θ/epsilon1
s, then again ψmaxisn’t optimal as we could
replace it with
ψ(˜s,˜a) =

ψmax(˜s,˜a) +ψmax(s,a)−ψmax(s,π†(s))
|Θ/epsilon1s|+ 1if˜s=s∧˜a∈Θ/epsilon1
s∪{π†(s)}\{a}
ψmax(˜s,˜a)−|Θ/epsilon1
s|(ψmax(s,a)−ψmax(s,π†(s)))
|Θ/epsilon1s|+ 1if˜s=s∧˜a=a
ψmax(˜s,˜a)o.w..
43Published in Transactions on Machine Learning Research (01/2023)
Intuitively, since the action awas being chosen with strictly higher probability than action π†(s), we have
uniformly spread this excess probability among the set Θ/epsilon1
s∪{π†(s)}. This vector would still be feasible as
ψ(s,a) =ψ(s,π†(s))and would be strictly better in terms of utility as /hatwideR(s,π†(s))>/hatwideR(s,a). This contradicts
our initial assumption and therefore ψmax(s,a) =ψmax(s,π†(s))for alla∈Θ/epsilon1
s.
Since the occupancy measure ψmaxsatisﬁesψmax(s,a) = 0for alla /∈Θ/epsilon1
s∪{π†(s)}andψmax(s,a) =ψ(s,π†(s))
for alla∈Θ/epsilon1
s, we conclude that it is the occupancy measure for the policy πDas deﬁned in Equation (47).
In order to prove ρπD=/hatwideρπD, ﬁrst note that for (s,a)∈Θ/epsilon1
/angbracketleftBig
ψπ†{s;a}−ψπ†,ψπD/angbracketrightBig
=µ(s)(ψπD(s,a)−ψπD(s,π†(s))
=µ(s)2(πD(a|s)−πD(π†(s)|s)) = 0,
where we used Equation (48) and Equation (47). Therefore
ρπD−/hatwideρπD=/angbracketleftBig
R−/hatwideR,ψπD/angbracketrightBig
(i)=/summationdisplay
(s,a)∈Θ/epsilon1αs,a/angbracketleftBig
ψπ†{s;a}−ψπ†,ψπD/angbracketrightBig
=/summationdisplay
(s,a)∈Θ/epsilon1αs,a·0
= 0,
where (i)follows from Lemma E.3 in the known parameter case and Lemma F.3 in the unknown parameter
case.
J Additional experiments
In this section, we provide the worst-case score of the policy from the defender’s perspective , i.e., the value
of the inner minimization in (P2b), for our policy. Given the results in Section 5.2 (Theorems 5.3 and 5.2),
by construction of πD, this minimum is achieved for R=/hatwideRand therefore the worst-case score equals /hatwideρπD.
As mentioned in Section 5.2, /hatwideρπDisa certiﬁcate in that we are guaranteed ρπD≥/hatwideρπDas long as/epsilon1D≥/epsilon1†.
The results are shown in Figure 6. As shown in the Figure, the value of /hatwideρπDis larger when /epsilon1D</epsilon1†. This is
in line with results of Section 5.2 as in this case, πD=π†andπ†is the optimal policy in /hatwideR. In addition,
when/epsilon1D</epsilon1†, the value of /hatwideρπDincreases with /epsilon1†. This is because when we increase the attack parameter /epsilon1†,
the policyπ†needs to become optimal in /hatwideRwith a larger margin, which causes its score to increase.
We note that for both environments, when no defense is employed, i.e, when using the policy π†, we will
obtain the same worst-case value when /epsilon1D< /epsilon1†asπD=π†, and we will obtain the worst-case score −∞
when/epsilon1D≥/epsilon1†.
0.1 0.3 0.5 0.7 0.9
Defense parameter ( /u1D716D)0.1 0.3 0.5 0.7 0.9Attack parameter ( /u1D716†)
123
0.1 0.3 0.5 0.7 0.9
Defense parameter ( /u1D716D)0.1 0.3 0.5 0.7 0.9Attack parameter ( /u1D716†)
0510
Figure 6: The value of the worst-case score from the defense perspective, i.e., /hatwideρπDunder diﬀerent values of
/epsilon1†,/epsilon1Dfor the Navigation (left) and Grid world (right) environments.
44