Published in Transactions on Machine Learning Research (April/2023)
Spectral Regularization Allows Data-frugal Learning over
Combinatorial Spaces
Amirali Aghazadeh amiralia@gatech.edu
School of Electrical and Computer Engineering
Georgia Institute of Technology
Nived Rajaraman nived.rajaraman@berkeley.edu
Department of Electrical Engineering & Computer Science
University of California, Berkeley
Tony Tu ttu32@gatech.edu
School of Computer Science
Georgia Institute of Technology
Kennan Ramchandran kannanr@eecs.berkeley.edu
Department of Electrical Engineering & Computer Sciences
University of California, Berkeley
Reviewed on OpenReview: https: // openreview. net/ forum? id= mySiFHCeAl& noteId= WhfpRCk8Wz
Abstract
Data-driven machine learning models are being increasingly employed in several important
inference problems in biology, chemistry, and physics, which require learning over combina-
torial spaces. Recent empirical evidence (see, e.g., Tseng et al. (2020); Aghazadeh et al.
(2021);Haetal.(2021))suggeststhatregularizingthespectralrepresentationofsuchmodels
improves their generalization power when labeled data is scarce. However, despite these em-
pirical studies, the theoretical underpinning of when and how spectral regularization enables
improved generalization is poorly understood. In this paper, we focus on learning pseudo-
Boolean functions and demonstrate that regularizing the empirical mean squared error by
theL1norm of the spectral transform of the learned function reshapes the loss landscape
and allows for data-frugal learning under a restricted secant condition on the learner’s em-
pirical error measured against the ground truth function. Under a weaker quadratic growth
condition, we show that stationary points, which also approximately interpolate the training
data points achieve statistically optimal generalization performance. Complementing our
theory, we empirically demonstrate that running gradient descent on the regularized loss
results in a better generalization performance compared to baseline algorithms in several
data-scarce real-world problems.
1 Introduction
Machine learning (ML) models have become increasingly commonplace in learning pseudo-Boolean functions
f(·), which map a d-dimensional binary vector x∈{± 1}dto a real number f(x)∈R. In biology, ML
models are used to infer the functional properties of macro-molecules (e.g., proteins) from a small set of
mutations (Riesselman et al., 2018; Gelman et al., 2021). In physics, ML models are being used to infer the
thermodynamicpropertiesofcombinatorialsystemsdefinedoverasetofbinary(Ising)statevariables(Carleo
et al., 2019; Noé et al., 2019). These highly nonlinear and complex ML models are trained using modern
techniques in continuous optimization. Yet they inherit the elegant properties of pseudo-Boolean functions
over the discrete binary hypercube {±1}d. In particular, it is known that the spectral representation of
1Published in Transactions on Machine Learning Research (April/2023)
a pseudo-Boolean function is defined as the Walsh-Hadamard transform (WHT) of the resulting vector of
combinatorial function evaluations, that is, Hf(X), where His a2d×2dWalsh matrix and the function
evaluation vector f(X) = [f(x) :x∈X]Tis sorted in the lexicographic ordering of all the length- dbinary
strings in X. The WHT enables writing the pseudo-Boolean function f(x)as a multi-linear polynomial
f(x) =/summationtext
S⊆[d]αS/producttext
i∈Sxi, where [d] ={1,2,...,d}andαS∈Ris the WHT coefficient corresponding to
the monomial/producttext
i∈Sxi(Boros & Hammer, 2002).1
Recent studies in biology (e.g., protein function prediction) have measured the output of several of these
real-world functions f(x)to all the 2denumerations of the input xand analyzed their spectral representation.
These costly high-throughout experiments on combinatorially complete datasets demonstrate a curious phe-
nomenon: such pseudo-Boolean functions often have low dimensional structures that manifest in the form of
an approximately-sparse polynomial representation (i.e., approximately-sparse WHT) with the top- kcoeffi-
cients corresponding to physically-meaningful interactions (Poelwijk et al., 2019; Eble et al., 2020; Brookes
et al., 2022) (see Appendix A for empirical evidence on protein functions).
The problem of learning pseudo-Boolean functions with sparse polynomial representations has a rich history
from theoretical and empirical viewpoints. In particular, since the pseudo-Boolean functions being learned in
their polynomial representations are linear functions in their coefficients, one may think about this problem
as a special instance of sparse linear regression in dimension 2d. There are many algorithms for sparse linear
regression, such as LARS (Efron et al., 2004), OMP (Tropp & Gilbert, 2007), AMP (Donoho et al., 2009),
and FISTA (Beck & Teboulle, 2009). In particular, one may apply LASSO to get statistical guarantees
for this problem which only scale linearly in the sparsity of the underlying ground truth polynomial kand
logarithmically in the problem dimension log(2d)(Candes et al., 2006). Likewise, from another perspective,
one may view the polynomial instead as a vector of 2doutcomesf(X); the objective of the learner is to
approximate f(X), when the learner can observe any chosen set of a few entries of this vector (corrupted by
noise), under the assumption that f(X)itself has a sparse WHT. In the literature, this problem has been
referred to by several names, and we refer to it as the sparse WHT problem . There are many computationally
and statistically efficient algorithms for solving this problem, such as SPRIGHT (Li & Ramchandran, 2015)
and Sparse-FFT (Amrollahi et al., 2019), among others.
A common issue with these alternate views of the problem, such as sparse linear regression or sparse WHT is
that theresulting algorithms are notsuited for function approximation . Inparticular, real-world physical and
biologicalfunctionsoftenhaveadditionalstructureswhichareeitherunknown a prioriorcannotbedescribed
succinctly, and which nonlinear function classes are often implicitly biased towards learning. Indeed, several
deep neural networks (DNNs) have been shown to exhibit strong generalization performance in biological
and physical problems (Gelman et al., 2021; Ching et al., 2018; Sarkisyan et al., 2016). This leads to a
fundamental disconnect between algorithms for which strong theoretical guarantees are known (e.g., LASSO)
and practical ML models based on nonlinear and complex function classes which are well-suited for function
approximation.
Another issue is specific to algorithms for solving the sparse WHT problem: some of these approaches
require observing f(·)at any chosen input (Li & Ramchandran, 2015; Li et al., 2015). In many practical
applications, where data is prohibitively expensive to collect, one is often forced to work with a handful of
random samples. For example, in the case of proteins, a common approach to measure biological functions is
through a procedure called random mutagenesis , which allows only a random sampling of the combinatorial
sequence space Sarkisyan et al. (2016). This constraint renders several algorithms for the sparse WHT
problem ill-suited for learning, even though they admit near-optimal computational and sample complexity
guarantees.
From a more practical point of view, several recent approaches for learning over combinatorial spaces (see,
e.g., (Tseng et al., 2020; Aghazadeh et al., 2021; Ha et al., 2021; Li et al., 2020)) follow similar ideas
for improving empirical generalization. Instead of directly learning the 2d-dimensional polynomial, they
have converged on explicitly promoting sparsity in the spectral representation of the learned compactly-
represented nonlinear function—also known as spectral regularization. These empirical studies motivate
several important theoretical questions regarding the underlying mechanism of spectral regularization in
1We use these terms interchangeably for pseudo-Boolean functions: spectral, Fourier, and Walsh-Hadamard.
2Published in Transactions on Machine Learning Research (April/2023)
improving generalization in practice. This paper focuses on the question: When and how does spectral
regularization improve generalization? The answer to this question is important from theoretical and
practical perspectives. Theoretically, it helps us understand how the loss landscape is reshaped as the result
of spectral regularization in favor of data-frugal learning. Practically, it tells us when and how to use spectral
regularization in learning real-world combinatorial functions.
Contributions . In this paper, we theoretically analyze spectral regularization of the empirical risk from the
perspective of learning pseudo-Boolean functions. We demonstrate that regularizing with the L1norm of the
spectrum of the learned function improves the generalization performance of the learner. In particular, under
a particular Restricted Secant Inequality (RSI) condition on the learner’s empirical error, measured against
the ground truth, stationary points of the regularized loss admit optimal generalization performance. We re-
lax this to a weaker quadratic growth (QG) condition and show that, under an additional data-interpolation
condition, stationary points of the regularized loss also admit statistical optimality. We empirically demon-
strate that these conditions are satisfied for a wide class of nonlinear functions. Our analysis provides a new
generalization bound when the underlying learned functions are sparse in the spectral domain. Empirical
demonstrations on several real-world data sets complement our theoretical findings.
2 Learning Pseudo-Boolean Functions
Problem statement . We consider the problem of learning structured pseudo-Boolean functions on the
binary hypercube {±1}d. In the finite sample setting, we assume that the learner has access to a dataset Dn
comprised of nlabeled data points (xi,yi)n
i=1wherexi∈{± 1}dare drawn from an input distribution Dand
the real-valued output yi∈Ris a noisy realization of an unknown pseudo-Boolean function of the input2.
Consider a rich nonlinear function class F={fθθθ:θθθ∈Rm}parameterized by θθθ, wherefθθθ:{±1}d→R. We
study the realizable setting, where the ground-truth labels are generated as yi=fθθθ∗(xi)+Zifor an unknown
parameterθθθ∗∈Rm, whereZiis the noise in the measurement for input xi, assumed to be independent and
normally distributed ∼N(0,σ2)3. The objective of the learner is to learn θθθ∗.
Multi-linear polynomial representation . Any pseudo-Boolean function f(·)can be uniquely represented as a
multi-linear polynomial, f(x) =/summationtext
z∈{±1}dαz/producttext
i:zi=+1xi, where the scalar αz∈Ris the coefficient corre-
sponding to the monomial/producttext
i:zi=+1xi, with order/summationtextd
i=11(zi= +1)(Boros & Hammer, 2002). The problem
of learning a pseudo-Boolean function f(·)is equivalent to finding the 2dunknown coefficients αz. In par-
ticular, the evaluations of fon all vertices of the binary hypercube {±1}2dresults in a linear system of
equations over the αzvariables,
/bracketleftbig
f(x) :x∈{± 1}d/bracketrightbigT=H/bracketleftbig
αz:z∈{± 1}d/bracketrightbigT, (1)
where the variables xandzenumerate the vertices of the binary hypercube {±1}din lexicographic order,
andH≡H2ddenotes the 2d×2dscaled Hadamard matrix defined recursively as,
H2d+1=1√
2/bracketleftigg
H2dH2d
H2d−H2d/bracketrightigg
, (2)
where H1=/bracketleftbig+1/bracketrightbig
. Thus the vector of function evaluations can be obtained by taking the WHT of the vector
of coefficients in the polynomial representation of f. Furthermore, by inverting the above linear system (note
thatHis an orthonormal matrix), the vector of polynomial coefficients [αz:z∈{± 1}d]can, in turn, be
obtained by taking the WHT of the vector/bracketleftbig
f(x) :x∈{± 1}d/bracketrightbigT. For brevity of notation, for a function f,
we definef(X)as/bracketleftbig
f(x) :x∈{± 1}d/bracketrightbigTwhere in the LHS the binary strings are enumerated in lexicographic
order. The coefficients in the polynomial representation of f(·)can be collected in the vector Hf(X). For
functions, we use “sparsity in WHT” and “sparse polynomial representation” interchangeably.
2We use the subscript xito refer to the ithdigit in the binary string x= (x1,x2,· · ·,xd).
3In fact, our results only require the noise to be independent subgaussian random variables with variance parameter σ2, but
for ease of exposition, we impose the Gaussian noise condition.
3Published in Transactions on Machine Learning Research (April/2023)
Sparsity in real-world functions . Even in the noiseless setting, to perfectly learn a general pseudo-Boolean
functionf(·), itsevaluationsonallbinaryinputvectors x∈{± 1}darerequired, whichmaybeveryexpensive.
In the presence of additional structures, this significant statistical and computational cost can be mitigated.
One such structure that has been observed in real-world biological and physical functions is sparsity in
WHT (Poelwijk et al., 2019; Eble et al., 2020; Brookes et al., 2022). In particular, in the theoretical analysis
of this paper, we assume that the ground-truth function fθθθ∗has asparse polynomial representation composed
of at mostkmonomials. Namely, ∥Hfθθθ∗(X)∥0≤k, wherek≪2dis typically unknown.
Spectral regularization . In attempting to learn pseudo-Boolean functions with a sparse polynomial rep-
resentation, a natural question to ask is: how can one encourage the learned functions also to have sparse
polynomialrepresentations. Onesolutionistoregularizethetraininglosswithanadditionalfunctionalwhich
promotes sparsity in the polynomial representation of the learned function /hatwidef. Denoting the polynomial rep-
resentation of /hatwidefby/summationtext
z∈{±1}d/hatwideαz/producttext
i:zi=+1xi, a natural regularization functional would be ∥/hatwideα∥0≡∥H/hatwidef(X)∥0
which is also the sparsity of fin WHT.
However, since∥·∥ 0is not a continuous function, we proxy the ∥·∥ 0by theL1norm. The resulting
regularization functional is, ∥H/hatwidef(X)∥1. When learning over a parameterized function family, /hatwidef=fθθθ, the
resulting regularized ERM we consider in this paper is,
min
θθθLn(θθθ) +R(θθθ),whereR(θθθ)≜λ√
2d∥Hfθ(X)∥1, (OBJ)
whereLn(θθθ)≜1
n/summationtextn
i=1/bracketleftig/parenleftbig
fθθθ(xi)−yi/parenrightbig2/bracketrightig
is the empirical mean squared error (MSE). Regularizing by R(θθθ)of
the above form is known as spectral regularization (SP) .
Remark 1. The scaling of the regularization parameter,λ√
2d, is motivated from the linearly parameterized
setting ofF={⟨θθθ,x⟩:θθθ∈Rd}. An explicit computation gives,λ√
2d∥Hfθθθ(X)∥1=λ∥θθθ∥1, which is the scaling
of the regularization parameter as used in LASSO.
The regularization weight λ>0strikes a balance between the empirical MSE and the spectral regularization,
and is set empirically using cross validation. Since the aggregate loss Ln(θθθ) +R(θθθ)is subdifferentiable with
respect to the model parameters θ, we can apply stochastic (sub-)gradient methods on the aggregate loss.
Notehoweverthat, ingeneral, asafunctionoftheparameter θθθ, boththeMSE,aswellastheSPregularization
are non-convex functions.
3 Related Works
A recent line of theoretical works on learning pseudo-Boolean functions demonstrate a staircase-like property
of gradient descent in learning DNNs in that the WHT coefficients corresponding to higher order monomials
(e.g.,x1x2x3) are reachable from lower order ones along increasing chains (i.e., x1x2andx1), and are thus
learnable in polynomial time and sample cost (Abbe et al., 2021). Other works have shown that under certain
distributions, loworderparityfunctionsarelearnablebymeansofgradientdecentondepth- 2networks, while
they cannot be learned efficiently using linear methods (Daniely & Malach, 2020). These analyses are limited
to certain DNN architectures or assume (linear) approximations to DNNs (e.g., neural tangent kernels) which
entirely disallows the analysis of spectral regularization as they manifest only in nonlinear function classes.
SpectralbiasofDNNshavebeenthesubjectofseveralotherempiricalandtheoreticalworks(Yang&Salman,
2019). Approximations to the Fourier transform of two-layer (Zhang et al., 2019) and multi-layer (Rahaman
et al., 2019) ReLU networks show that these networks have a learning bias towards low frequency func-
tions (Xu et al., 2019). To improve the limitations of DNNs in learning high frequency components, empir-
ical works use Fourier features explicitly as part of the input (Tancik et al., 2020). A parametrization of
polynomial DNNs has also been shown to speed up the learning of higher frequency components in two-layer
networks (Choraria et al., 2022). Different notions of spectral priors have also been empirically investigated
in graph neural networks (Li et al., 2020) and elsewhere (Yoshida & Miyato, 2017). These results support
the implicit bias of DNNs towards dense and low-frequency spectral representation and only motivate our
analysis of sparsity as an explicit spectral prior.
4Published in Transactions on Machine Learning Research (April/2023)
4 Theoretical Analysis
To develop some intuition about the general problem, consider the special case of learning over the set of
linear functions. Namely, F={⟨θθθ,·⟩:θθθ∈Rd}. The polynomial representation of any linear function
fθθθ(x) =⟨θθθ,x⟩is simply/summationtext
i∈[d]θixiwhere only the order- 1coefficients are non-zero. Thus, the assumption
that the polynomial representation of fθθθis composed of at most kmonomials is equivalent to saying that
θθθisk-sparse. Thus in the linear setting, the problem boils down to sparse linear regression (Candes et al.,
2006). Furthermore, in the linear setting, spectral regularization has an explicit representation in terms of
its parameter θθθ. In particular, here R(θθθ) =λ√
2d∥Hfθθθ(X)∥1=λ∥θθθ∥1. Thus, the proposed objective function,
(OBJ), when specialized to the linear setting boils down to the LASSO objective,1
n/summationtextn
i=1(⟨xi,θθθ⟩−yi)2+
λ∥θθθ∥1, which can be efficiently solved by gradient descent, and is known to be statistically optimal in the
finite sample regime (Raskutti et al., 2011).
4.1 Going beyond the linear setting - Restricted Secant Inequality
In the linear setting, a sufficient condition for the finite sample statistical optimality of LASSO is the
restricted eigenvalue condition on the empirical covariance matrix Σn=1
n/summationtextn
i=1xixiT. The restricted
eigenvalue condition is automatically satisfied if the smallest eigenvalue of Σnis bounded away from 0. The
condition induces a particular sense of curvature around the true parameter θθθ∗of the loss Errn(θθθ,θθθ∗), that
is the MSE of the learned function compared to the ground truth,
Errn(θθθ,θθθ∗) =1
nn/summationdisplay
i=1(fθθθ(xi)−fθθθ∗(xi))2. (3)
In contrast, when Fis a non-linear function family, we circumvent these sufficient conditions, and directly
study assumptions which induce curvature in the loss Errn(θθθ,θθθ∗). These assumptions, however, do not
concern the structure of the spectral regularizer, which unlike in the linear case, can no longer be represented
as a closed-form function of θθθ. A major technical challenge in the analysis relates to circumventing the use
of this explicit representation, which we expand upon later.
Definition 1 (Restricted secant inequality (RSI) (Zhang & Yin, 2013)) .The set of functions satisfying the
restricted secant inequality with parameter C, denoted RSI(C)is defined as follows. A function g:Rm→R
belongs to RSI(C)iff for some z∗∈arg minz∈Rmg(z)and for all z∈Rm,
⟨∇g(z),z−z∗⟩≥C∥z−z∗∥2
2. (4)
Inotherwords, therestrictedsecantinequalityimpliesthatthegradientofthelossatapointiswellcorrelated
with thez−z∗, the line joining the current point to the minimizer of g.
Remark 2. The RSI is known to generalize several extensions of convexity, such as quasar-star convexity
(Hinder et al., 2020) and strong star-convexity (Lee & Valiant, 2015). Refer to Karimi et al. (2016) for a
comparison with other constraints such as the Polyak-Lojasiewicz (PL) inequality, and the quadratic growth
condition which we study in Section 4.2. In general, the RSI is a significantly weaker condition than global
strong convexity.
The first main assumption we study is when the Errn(θθθ,θθθ∗)satisfies the RSI. This assumption captures a
notion of curvature, in which the gradients of Errn(θθθ,θθθ∗)are informative about (i.e., positively correlated
with) the error in the parameter space θθθ−θθθ∗.
From this behavior, it may be expected that all stationary points of functions which satisfy the RSI are
global minima, which is indeed true (Karimi et al., 2016). However, note that we impose the RSI condition
onErrn(θθθ,θθθ∗)(which cannot be computed by the learner) and not the empirical risk Ln(θθθ). Even under the
RSI condition on Errn(θθθ,θθθ∗), it is not trivial to find global minima of the training objective, Ln(θθθ) +R(θθθ)
which is composed of two non-convex (in θθθ) functions and is non-smooth. Thus, we restrict our attention to
the analysis of first-order stationary points of this objective.
5Published in Transactions on Machine Learning Research (April/2023)
Assumption 1(a). Assume the function Errn(θθθ,θθθ∗)satisfies the RSI with high probability over the dataset.
Namely, there is a constant C∗
n,δ>0such that with probability at least 1−δ, for everyθθθ∈Rm,
⟨θθθ−θθθ∗,∇θErrn(θθθ,θθθ∗)⟩≥C∗
n,δ∥θθθ−θθθ∗∥2
2. (5)
Remark 3. The RSI is true for linear families F={⟨θθθ,·⟩:θθθ∈Rm}as long as with probability at least
1−δ, the data covariance satisfies Ex∼Unif(Dn)[xxT]⪰C∗
n,δI, i.e., is well conditioned. The quadratic growth
condition in Section 4.2 also holds under the same conditions with parameter C∗
n,δ.
In addition to the RSI, we make some mild regularity assumptions on the function classes we study. We
emphasize that none of these assumptions imply convexity or smoothness of the training objective Ln(θθθ) +
R(θθθ)orErrn(θθθ,θθθ∗). First, we assume that the function class Fis sufficiently smooth, having Lipschitz
continuous gradients (Cisse et al., 2017). Note that we study the case when the function class Fis smooth,
and do not impose this condition on the empirical risk, Ln(θθθ), as is often assumed (Jin et al., 2018).
Assumption 2 (Lipschitz continuous gradients on F).For eachx∈Xand constant µ, assume that fθθθ(x)
is twice differentiable in θθθand the Hessian of fθθθ(x)satisfies−µI⪯∇2fθθθ(x)⪯µI.
The final assumption we impose assumes that the ground truth function θθθ∗has well behaved gradients in
that a certain covariance matrix induced by the gradients of the matrix is bounded.
Assumption 3 (Bounded average gradient norm at θθθ∗).Under this assumption,
Ex∼Unif(X)/bracketleftbig
∇fθθθ∗(x)(∇fθθθ∗(x))T/bracketrightbig
⪯L2I. (6)
Assumption 3 can be interpreted as a one-point Lipschitzness condition, showing that on average across the
inputsx∈X, the gradients of fθθθ(x)are well behaved at the singular point θθθ=θθθ∗. Under the previous
assumptions, we prove a bound on the error in the parameter space made by the learner. This theorem is a
consequence of a more general result we prove in Appendix C for an arbitrary regularization scale λ>0.
Theorem 1. Suppose Assumptions 1(a), 2 and 3 are satisfied, λis chosen as 12σ/radicalig
d+log(1/δ)
nand the size
of the dataset is sufficiently large, namely n>n 0, defined as,
n0≜C1σ2/parenleftbig
d2µ2+L2k/parenrightbig
(d+ log(1/δ))
(C∗
n,δ)2, (7)
for a large constant C1<10004. Consider a learner which returns a first order stationary point, ˆθθθ, of the
lossLn(θθθ) +R(θθθ). Then, with probability ≥1−2δ,
∥ˆθθθ−θθθ∗∥2≲σL
C∗
n,δ/radicalbigg
k(d+ log(1/δ))
n. (8)
Note that in the dependence on dandk, the sample size threshold n0scales asymptotically as d3+dk.
4.2 Going beyond the RSI - The Quadratic Growth condition
While RSI is a much weaker condition than convexity, at a high level, it assumes that the gradients of
Errn(θθθ,θθθ∗)are informative about the error in the parameter space θθθ−θθθ∗. We can further relax this assump-
tion. In this section, we consider a weakening of this assumption which only supposes that Errn(θθθ,θθθ∗)grows
at least quadratically in ∥θθθ−θθθ∗∥2. This is known as the quadratic growth (QG) condition (Anitescu, 2000).
This condition no longer characterizes the behavior of the gradients of the function Errn(θθθ,θθθ∗), much less
those of the empirical loss Ln. Stationary points of a function satisfying the QG condition are no longer
global minima, in contrast with the behavior under the RSI (Definition 1).
4Note that we did not optimize the value of this constant. With a more careful analysis, its value can be brought down.
6Published in Transactions on Machine Learning Research (April/2023)
Definition 2 (Quadratic Growth (QG) condition (Anitescu, 2000)) .The set of functions satisfying the QG
condition with parameter C, denoted QG(C)is defined by the inclusion, g:Rm→Rbelongs to QG(C)iff
for somez∗∈arg minz∈Rmg(z)and for all z∈Rm,
g(z)−g(z∗)≥C∥z∗−z∥2
2. (9)
ForC > 0, functions in QG(C)have a unique global minimizer.
As the name suggests, the quadratic growth condition implies that the function ghas local curvature around
z∗. However, it is important to note that this does not discount the possibility that ghas many spurious
local minima. In this section, we go beyond the RSI assumption on Errn(θθθ,θθθ∗)and discuss the case where
it satisfies the quadratic growth condition.
Assumption 1(b) (Quadratic Growth (QG) condition (Anitescu, 2000)) .The function family F={fθθθ:
θθθ∈Rm}is assumed to satisfy the QG condition with parameter C∗
n,δ, if with probability at least 1−δover
the dataset Dn,
∀θθθ∈Rm,Errn(θθθ,θθθ∗)≥C∗
n,δ∥θθθ−θθθ∗∥2
2. (10)
Remark 4. (Anitescu, 2000, Theorem 2) Restricting to functions fwhich have L-Lipschitz continuous
gradients, if f∈RSI(C), then it implies that f∈QG(2C/L). This implies that up to the value of the
parameter, the QG condition is weaker than RSI, under a smoothness constraint on the considered functions.
Note that under the QG condition, gradients of Errn(θθθ,θθθ∗)are no longer constrained to be positively cor-
related with the θθθ−θθθ∗. In fact, the absence of this feature proves to be a significant barrier for first-order
methods to generalize well. To circumvent this issue, we motivate and introduce the notion of approximate
first-order stationary interpolators in the next section, and characterize their statistical performance. This
imposes a stronger requirement on the algorithm than just returning an arbitrary stationary point of the
training objective L(θθθ) +R(θθθ).
Approximate First-order Interpolators. WhenFis an expressive family of nonlinear models, it has
been observed empirically that standard stochastic gradient methods can be run until the model begins to
perfectly interpolate the training data, without hurting test-time performance. This phenomenon has been
referred to in the literature as benign overfitting (Bartlett et al., 2020), and has been seen to hold frequently
in the training of DNNs in practice, when stochastic gradient descent is run for sufficiently many epochs.
In fact, in all our experiments (see Figs. 5 and 6 in Appendix A), we observe that upon running stochastic
gradient descent for sufficiently long on the aggregate loss Ln(θθθ)+R(θθθ), the parameter ˆθθθeventually converges
to a solution which interpolates the labelled examples well , i.e., the mean square error Ln(ˆθθθ)is upper bounded
by a sufficiently small ∆>0. Note that we do notassume that ∆is so small that the condition essentially
imposes that ˆθθθis an approximate global minimizer of the training loss. In theory, our results also hold under
the stronger condition that Ln(ˆθθθ) +R(ˆθθθ)is upper bounded by ∆, which is the loss function being optimized
by (stochastic) gradient descent. This phenomenon motivates the following definition of approximate first
order stationary interpolators.
Definition 3 (∆-approximatefirstorderstationaryinterpolator) .A point ˆθθθis defined to be a ∆-approximate
first-order stationary interpolator if,
1.ˆθθθis a first order stationary point of the aggregate loss Ln(θθθ) +R(θθθ). Namely,
0∈(∇Ln)(ˆθθθ) + (∇R)(ˆθθθ). (11)
2.fˆθθθapproximately interpolates the observed training data. Namely, the empirical mean squared error
ofˆθθθevaluated on the training dataset satisfies,
Ln(ˆθθθ)≤∆. (12)
Note that our results hold under the stronger condition Ln(ˆθθθ) +R(ˆθθθ)≤∆, the training loss function
being minimized by the (stochastic) gradient methods in our experiments.
7Published in Transactions on Machine Learning Research (April/2023)
We are ready to establish a guarantee on the statistical performance of approximate first-order stationary
interpolators.
Theorem 2. Supposeλis chosen = 12σ/radicalig
d+log(1/δ)
n. Assume that the size of the dataset is sufficiently
large, namely, n>n 0(as defined in Equation (7)). Consider a learner returns ˆθθθwhich is a ∆-approximate
first order stationary interpolator, where ∆≤C1(C∗
n,δ/µ)2for a sufficiently large constant C1. Then, with
probability≥1−2δ,
∥ˆθθθ−θθθ∗∥2≲σL
C∗
n,δ/radicalbigg
k(d+ log(1/δ))
n. (13)
Remark 5. For pseudo-Boolean functions, the log-covering number in any norm up to log factors is ≈
log/parenleftbig2d
k/parenrightbig
≈kd. Therefore, an algorithm which returns a function which is an exact minimizer of the mean
squared error among all functions with a k-sparse polynomial representation, θθθLS
kadmits the guarantee ∥θθθLS
k−
θθθ∗∥2≲/radicalbig
kd/nup to scaling constants, with high probability, up to log factors. However, when ∆is large, the
learner considered in Theorem 2 is not constrained to return the exact minimizer of the squared error (subject
to the sparsity constraint). Thus, under the additional condition of first-order stationarity, Theorem 2
imposes a much weaker condition, Ln(ˆθθθ)≲∆rather than ˆθθθ∈arg minLn(θθθ).
Finally, in the context of the previous two results, we prove a lower bound showing statistical optimality
under the imposed assumptions.
Theorem 3. Supposed≥3andk≤2d/4. Then, there exists a parameter class Θ, and an associated
function classF={fθθθ:θθθ∈Θ}such that for any learner ˆθθθ, there exists a ground truth function fθθθ∗:
{±1}d→Rhaving ak-sparse polynomial representation, such that given a sufficiently large dataset of n
samples,
1. Assumptions 1(a), 1(b), 2 and 3 are satisfied with constants L= 1,µ= 0andCn,δ∗∈/bracketleftbig1
2,3
2/bracketrightbig
with
probability at least 1−δ.
2.E/bracketleftig
∥ˆθθθ−θθθ∗∥2/bracketrightig
≳σ/radicalig
kd
n, whereσ2denotes the noise variance in the observed labels.
Extensions to the case of non-unique minima. While we focus on general parametric function classes
in Theorems 1 and 2, for the specific case of DNNs, the issue of “permutation invariance” can appear.
In particular, a permutation (i.e., relabeling) of the neurons in the same layer of a network can result
in a different network with the same functional relationship between the input and the output. In this
case, Assumptions 1(a) and 1(b) cannot hold globally for all θθθ∈Rm, as the loss Errn(θθθ,θθθ∗)and its gradient
become 0atanyparameter θθθ∗
σ, whereθθθ∗
σdenotestheweightmatricescorrespondingtoafunctionallyinvariant
permutation σof the neurons of the base network with parameter θθθ∗. However, even in this case, we can
extract a guarantee from Theorem 1 and Theorem 2 by modifying the underlying assumptions slightly.
In particular, let Θ∗⊂Rmdenote the set of parameters that are functionally equivalent to θθθ∗, in that
fθθθ∗=fθθθfor allθ∈Θ∗. These parameters are local minimizers of Errn(θθθ,θθθ∗). In particular, we define a
modified RSI and QG condition, as below,
⟨θθθ−θθθ∗
proj,∇θθθErrn(θθθ,θθθ∗)⟩≥C∗
n,δ∥θθθ−θθθ∗
proj∥2
2, (Modified Assumption 1(a))
Errn(θθθ,θθθ∗)≥C∗
n,δ∥θθθ−θθθ∗
proj∥2
2,, (Modified Assumption 1(b))
whereθθθ∗
proj= arg minθθθ∈Θ∗∥θθθ∗−θθθ∥is the projection of θθθontoΘ∗in some metric∥·∥. In particular, for
eachθθθ∈Θ∗, letKθθθdenote the subset of Rmsuch that for all θθθ′∈Kθθθ,θθθ= arg minθθθ∈Θ∗∥θθθ∗−θθθ∥. In other
words,Kθθθare the set of parameters which are closest to θθθamong all the functionally equivalent parameters
θθθ∗. Then, modified Assumptions 1(a) and 1(b) impose the RSI and QG conditions in a local neighborhood
(Kθθθ) of eachθθθ∈Θ∗.
Corresponding to the new modified assumptions, a learner which returns any ˆθθθwhich is a stationary point
under modified Assumption 1(a) (resp. approximate first order stationary interpolator, under modified As-
sumption 1(b)), guarantees to approximate θθθ∗
proj, the nearest local minimizer to θθθinΘ∗. The proofs of these
8Published in Transactions on Machine Learning Research (April/2023)
number of samples200 150 100 50 200 150 100 50 200 150 100 50order of interactions
power law coefficient
power law coefficient1
3
5
1
3
5MSE
MSE+SPMSE MSE
MSE+SP MSE+SP
01/41/23/41monomial power law staircasex3x2x6x7x1x4
WHT WHT WHTx2 +  x2x5/α + x2x3x6/α2 + ...
2
11.5
2
11.53
2x1x7 +  x2x5/α + x3x6/α2 + ...
3
2
Figure 1: The plots demonstrate the generalization power of a depth- 4feed-forward neural network in
learning 3classes of sparse pseudo-Boolean functions f(x) :{±1}13→Rusing the mean squared error
(MSE) loss before (first row) and after (second row) adding the spectral (SP) regularizer, as a function of
the number of training samples. Monomial are1-sparse functions in Walsh-Hadamard transform (WHT)
with an increasing order of interactions ( 1to5).Power law are10-sparse functions in WHT with second
order interactions and coefficients set using a power law function. Staircase are18-sparse functions in
WHT with 3first order, 6second order, and 9third order interactions, each with an equal coefficients and
set using a power law function. The heat maps show the fraction of times (among 5random independent
trials) the models generalize on unseen data with the coefficient of determination larger than R2≥0.45on
the test data points.
results follow identically to Theorem 1 and Theorem 2. In particular, replacing θθθ∗by the parameter θθθ∗
proj,
and following the same argument completes the proof of the result showing that θθθ∗
proj(which is functionally
equivalent to θθθ∗) can be recovered approximately.
In summary, rather than assuming that the loss Errn(θθθ,θθθ∗)is globally bowl shaped as in Assumption 1(b),
an assumption which cannot hold globally for example in neural networks because of permutation invariance,
it suffices to assume that the loss function is locally bowl shaped around functionally invariant permutations
θθθ∈Θ∗of the ground truth parameter θθθ∗. This line of reasoning can be extended to incorporate other
symmetries in the function class which prevent exact parameter recovery, as in the case of permutation
invariance of neural networks. The corresponding parameter recovery guarantee is also modified to be up to
this symmetry.
4.3 Computational complexity of minimizing the regularized loss (OBJ)
In this section we discuss the computational complexity of finding stationary points of the regularized loss
Ln(θθθ) +R(θθθ)in (OBJ). Note that Aghazadeh et al. (2021) propose an algorithm for minimizing the regu-
larized loss in (OBJ) based on an alternating minimization based approach and sparse WHT algorithms. In
this section, we provide different insights on how first-order methods can be used to minimize the regularized
loss, based on computing sparse WHTs.
To begin with, the cost of computing the gradient of Ln(θθθ)for many function classes scales as O(n·poly(m)),
wherem=dim(θθθ). Note that we can also compute a stochastic gradient in time which scales as O(poly(m))
by simply computing the gradient of (fθθθ(xi)−yi)2for a single (xi,yi)pair. The variance of this stochastic
gradient estimate largely depends on the nature of fθθθ, as well as the training data.
On the other hand, consider the regularization term R(θθθ) =λ√
2d∥Hfθθθ(X)∥1. In general, the complexity
of exactly computing the gradient of Ris exponential in dwithout any further assumptions, as it involves
the summation of 2dterms. While this might prove to be a challenge, empirically we observe that a
two-step training process can help alleviate this issue. We first carry out weight initialization , running a
few iterations of gradient descent on the unregularized loss, Ln(θθθ)which generalizes moderately well, and
is therefore somewhat sparse in the spectral domain. Warm-starting from this initialization and running
9Published in Transactions on Machine Learning Research (April/2023)
Figure 2: We estimate the empirical lower bound ˆC∗
n,δin the quadratic growth (QG) condition by finding
the minimum change in the network’s output as a result of perturbations to the weights with noise drawn
from the normal distribution W∼N (0,σ2). Experiments on 2fully connected networks (FCNs) and a
convolutional neural network (CNN) reveals empirical lower bounds.
gradient descent on the regularized loss in (OBJ), it is empirically observed that the iterates of gradient
descent remain sparse throughout the training trajectory. The key implication of this result is that one can
now use sparse WHT based techniques, such as Li et al. (2015) to now compute an approximation of the
gradient of the regularizer at time t,∇R(θθθt)at a computational cost scaling polynomially in m, and linearly
in the sparsity of fθθθtin the spectral domain, which is small at each time tin the second phase.
5 Empirical Studies
We design our experiments in a way to address these questions:
•Does SP improve generalization accuracy in learning sparse polynomials?
•When does QG hold for common DNN architectures? what is the empirical lower bound ˆC∗
n,δ?
•Does SP improve generalization accuracy in real-world problems?
•How doesL1-regularization compare to SP-regularization in practice in terms of generalization?
Sparse polynomials . We compare the generalization performance of a depth- 4fully connected network
with and without the SP regularization on 3classes of sparse polynomials in Fig. 1: 1) Monomial are
randomly-drawn 1-sparse functions in WHT with increasing order of interactions from 1to5. 2)Power
Laware randomly-drawn 10-sparse functions in WHT all with order- 2interactions and coefficients set based
on a power law function with decreasing exponent. 3) Staircase are randomly-drawn 18-sparse functions in
the WHT with 3first order, 6second order, and 9third order interactions, each with equal coefficients and
set based on a power law function. These synthetic sparse polynomials are inspired by physical models for
real-world pseudo-Boolean functions (e.g., protein functions (Brookes et al., 2022; Qian et al., 2001; Qin &
Colwell, 2018)). We observe clear transitions in generalization power: it is harder in terms of sample cost to
learn nonlinear models with higher order interactions and lower sparsity exponent (i.e., denser functions in
WHT). This analysis adds a new axis to the accuracy-vs-sample-cost phase transition curves in compressed
sensing (Donoho et al., 2011). SP regularization consistently improves the transition curves for generalization
power as a function of order and sample cost.
QG condition . To empirically estimate the lower bound ˆC∗
n,δ, we follow this procedure. We collect a set of
training data points (xi,yi)n=1000
i=1, initialize a DNN at θ∗(see below for more detail), and repeatedly perturb
the weights with Gaussian noise to generate θpert,k=θ∗+Wk, whereW∼N (0,σ2I)is independent and
normally distributed, for k= 1,···,K. For each k, we compute the ratio/summationtextn
i=1(fθpert,k(xi)−fθ∗(xi))2/σ2
and report the minimum value across k∈[K]as a function of σ2in Fig. 2. Next, we train the DNN
with SP regularization for certain number of epochs to arrive at a new θ∗and repeat the same procedure
10Published in Transactions on Machine Learning Research (April/2023)
MSE+SP MSE RF XGB LASSO SVM0.160.180.200.22RMSE
Protein
MSE+SP MSE RF XGB LASSO SVM0.420.480.540.60
T cell
MSE+SP MSE RF XGB LASSO SVM0.400.480.560.64
Cancer
Figure 3: The plot demonstrates the generalization error of depth- 4neural networks with the mean squared
error (MSE) loss before and after adding the spectral (SP) regularizer as compared to Random Forest,
XGBoost, LASSO, and SVM. SP drastically improves the generalization error of neural network (reduced
mean and variance) and allows for a superior performance over LASSO with L1norm regularization over
the polynomial representation directly.
again to observe how the lower bound changes with training. The outputs yare generated from binary
vectorsx∈{± 1}13using the (randomly-drawn) sparse polynomial f(x) = 3x1+ 4x2x3+ 5x4x5+x12.
We choose sufficiently small architectures compared to the number of perturbations to ensure a reliable
empiricalestimatefor ˆC∗
n,δ. Fig.2showstheresultsforXavier-initialized,depth- 2FCNswith 222parameters,
perturbedK= 100(FCN-1) and K= 500(FCN-2) times, with different ranges of σ2(learning rate
= 5×10−3). Fig. 2 also shows the result for a depth- 4CNN with 1551parameters, initialized by a trained
network using MSE loss and perturbed K= 6000times (learning rate = 10−3). The plots demonstrate that
the QG condition is satisfied for these instances with empirical lower bounds 6.02and19.5, respectively.
The bound also improves drastically with training; moreover, initializing the networks with trained models
consistently improves the lower bound. See Appendix A for more detailed empirical studies and discussions
of these results.
Real world experiments . We compare the generalization error with and without the SP regularization to
standard baseline algorithms for data-scarce learning in the real-world (Fig. 3). Protein is a dataset which
measures the fluorescence level of 213protein sequences that link two variants of the Entacmaea quadricolor
proteins different at exactly 13amino acids (Poelwijk et al., 2019). T cellis a dataset which measures the
DNA repair outcome of T cells (average length of deletions) on 1521sites on human genome after applying
double-strand breaks (DSBs) using CRISPR (Leenay et al., 2019). Cancer is a similar dataset on 287
sites on cancer genome (Leenay et al., 2019). In the two latter datasets, a one-hot-encoded context sequence
of size 20around the DSB is used as the input to predict the DNA repair outcome. Following the low- n
experimental setting in (Biswas et al., 2021), we use a subset of 30sequences drawn uniformly at random
for training and validation and use the rest for testing. We repeat each experiments 10×with independent
random splits of the data and report the RMSE in predicting the phenotype. We initialize DNNs using
Xavier (equal seeds). We use the default hyperparamters in scikit-learn for the baselines. Despite minimal
hyperparameter tuning and no architecture search, SP allows for a competitive performance. In particular,
the SP-regularized model outperforms LASSO which applies the L1norm penalty on the coefficients in the
polynomial representation (Fig. 8 in Appendix A compares SP with different regularizers).
6 Summary, Discussion, and Future Vision
In the paper, we focused on learning sparse pseudo-Boolean functions with explicit regularization in the spec-
tral domain and established the conditions on general nonlinear functions under which the stationary points
which approximately interpolate the training data achieve statistically optimal generalization performance.
11Published in Transactions on Machine Learning Research (April/2023)
We demonstrated how the assumptions can be extended to nonlinear functions with multiple local minimas.
Real-world experiments with neural networks demonstrated the utility of these assumptions. Naturally,
some of the bounds might result in larger constants for very deep neural network (due to higher Lipschitz
constants). Future works involve analyzing the algorithms that can achieve such statistical performance. It
would be tempting to ask under what conditions stochastic gradient descend achieves stationary points with
restricted secant and quadratic growth conditions (for which we have clear empirical evidences). Further,
the statistical analysis of the computationally-efficient optimization algorithms for spectral regularization
is still poorly understood. On the algorithmic side, spectral algorithms for pseudo-Boolean functions can
be extended to generalized Fourier transform to accommodate a larger class of data-frugal combinatorial
problems. Overall, our work provides a concrete framework to connect combinatorial algorithms with strong
theoretical guarantees and nonlinear machine learning models with strong generalization power.
Broader Impact Statement
We do not anticipate this work to have any potential negative impact on a user of this research.
References
E. Abbe, E. Boix Adsera, M. Brennan, G. Bresler, and D. Nagaraj. The staircase property: How hierarchical
structure can guide deep learning. Advances in Neural Information Processing Systems , 34, 2021.
A. Aghazadeh, H. Nisonoff, O. Ocal, D. Brookes, Y. Huang, O. Koyluoglu, J. Listgarten, and K. Ramchan-
dran. Epistatic net allows the sparse spectral regularization of deep neural networks for inferring fitness
functions. Nature Communications , 12(1):1–10, 2021.
A. Amrollahi, A. Zandieh, M. Kapralov, and A. Krause. Efficiently learning Fourier sparse set functions.
Advances in Neural Information Processing Systems , 32, 2019.
M. Anitescu. Degenerate nonlinear programming with a quadratic growth condition. SIAM Journal on
Optimization , 10(4):1116–1135, 2000.
P. Bartlett, P. Long, G. Lugosi, and A. Tsigler. Benign overfitting in linear regression. Proceedings of the
National Academy of Sciences , 117(48):30063–30070, 2020.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences , 2(1):183–202, 2009.
S. Biswas, G. Khimulya, E. Alley, K. Esvelt, and G. Church. Low-N protein engineering with data-efficient
deep learning. Nature Methods , 18(4):389–396, 2021.
E. Boros and P. Hammer. Pseudo-Boolean optimization. Discrete Applied Mathematics , 123(1-3):155–225,
2002.
D. Brookes, A. Aghazadeh, and J. Listgarten. On the sparsity of fitness functions and implications for
learning. Proceedings of the National Academy of Sciences , 119(1), 2022.
E. Candes, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Math-
ematical Sciences , 59(8):1207–1223, 2006.
G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborová.
Machine learning and the physical sciences. Reviews of Modern Physics , 91(4):045002, 2019.
T. Ching, D. Himmelstein, B. Beaulieu-Jones, A. Kalinin, B. Do, G. Way, E. Ferrero, P. Agapow, M. Zietz,
M. Hoffman, et al. Opportunities and obstacles for deep learning in biology and medicine. Journal of The
Royal Society Interface , 15(141):20170387, 2018.
M.Choraria, L.Dadi, G.Chrysos, J.Mairal, andV.Cevher. Thespectralbiasofpolynomialneuralnetworks.
arXiv preprint arXiv:2202.13473 , 2022.
12Published in Transactions on Machine Learning Research (April/2023)
M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving robustness
to adversarial examples. In International Conference on Machine Learning , pp. 854–863. PMLR, 2017.
A. Daniely and E. Malach. Learning parities with neural networks. Advances in Neural Information Pro-
cessing Systems , 33:20356–20365, 2020.
D. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing. Proceedings
of the National Academy of Sciences , 106(45):18914–18919, 2009.
D. Donoho, A. Maleki, and A. Montanari. The noise-sensitivity phase transition in compressed sensing.
IEEE Transactions on Information Theory , 57(10):6920–6941, 2011.
H. Eble, M. Joswig, L. Lamberti, and W. Ludington. Higher-order interactions in fitness landscapes are
sparse.arXiv preprint arXiv:2009.12277 , 2020.
Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. 2004.
Á. Figula. and V. Kvaratskhelia. Some numerical characteristics of Sylvester and Hadamard matrices, 2015.
S. Gelman, S. Fahlberg, P. Heinzelman, P. Romero, and A. Gitter. Neural networks to learn protein se-
quence–function relationships from deep mutational scanning data. Proceedings of the National Academy
of Sciences , 118(48), 2021. doi: 10.1073/pnas.2104878118.
W. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu. Adaptive wavelet distillation from neural networks
through interpretations. Advances in Neural Information Processing Systems , 34, 2021.
O. Hinder, A. Sidford, and N. Sohoni. Near-optimal methods for minimizing star-convex functions and
beyond. In Conference on learning theory , pp. 1894–1938. PMLR, 2020.
C. Jin, L. Liu, R. Ge, and M. Jordan. On the local minima of the empirical risk. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information
Processing Systems , volume 31. Curran Associates, Inc., 2018.
H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods
under the polyak-łojasiewicz condition. In Joint European conference on machine learning and knowledge
discovery in databases , pp. 795–811. Springer, 2016.
J. Lee and P. Valiant. Optimizing star-convex functions, 2015.
R. Leenay, A. Aghazadeh, J. Hiatt, D. Tse, T. Roth, R. Apathy, E. Shifrut, J. Hultquist, N. Krogan, Z. Wu,
et al. Large dataset enables prediction of repair after CRISPR–Cas9 editing in primary T cells. Nature
Biotechnology , 37(9):1034–1037, 2019.
M. Li, Z. Ma, Y. Wang, and X. Zhuang. Fast Haar transforms for graph neural networks. Neural Networks ,
128:188–198, 2020.
X. Li and K. Ramchandran. An active learning framework using sparse-graph codes for sparse polynomials
and graph sketching. Advances in Neural Information Processing Systems , 28, 2015.
X. Li, J. Bradley, S. Pawar, and K. Ramchandran. SPRIGHT: A fast and robust framework for sparse
Walsh-Hadamard transform. arXiv preprint arXiv:1508.06336 , 2015.
P. Loh and M. Wainwright. Regularized m-estimators with nonconvexity: Statistical and algorithmic theory
for local optima, 2015.
F. Noé, S. Olsson, J. Köhler, and H. Wu. Boltzmann generators: Sampling equilibrium states of many-body
systems with deep learning. Science, 365(6457), 2019.
F. Poelwijk, M. Socolich, and R. Ranganathan. Learning the pattern of epistasis linking genotype and
phenotype in a protein. Nature Communications , 10(1):1–11, 2019.
13Published in Transactions on Machine Learning Research (April/2023)
J. Qian, N. Luscombe, and M. Gerstein. Protein family and fold occurrence in genomes: power-law behaviour
and evolutionary model. Journal of Molecular Biology , 313(4):673–681, 2001.
C. Qin and L. Colwell. Power law tails in phylogenetic systems. Proceedings of the National Academy of
Sciences, 115(4):690–695, 2018.
N. Rahaman, A. Baratin, D. Arpit, F. Draxler, M. Lin, F. Hamprecht, Y. Bengio, and A. Courville. On
the spectral bias of neural networks. In International Conference on Machine Learning , pp. 5301–5310.
PMLR, 2019.
G. Raskutti, M. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regression
overℓq-balls.IEEE Transactions on Information Theory , 57(10):6976–6994, 2011. doi: 10.1109/TIT.
2011.2165799.
A. Riesselman, J. Ingraham, and D. Marks. Deep generative models of genetic variation capture the effects
of mutations. Nature Methods , 15(10):816–822, 2018.
J. Rohn. Computing the norm ∥A∥∞,1is NP-hard. Linear and Multilinear Algebra , 47:195–204, 05 2000.
K. Sarkisyan, D. Bolotin, M. Meer, D. Usmanova, A. Mishin, G. Sharonov, D. Ivankov, N. Bozhanova,
M. Baranov, O. Soylemez, et al. Local fitness landscape of the green fluorescent protein. Nature, 533
(7603):397–401, 2016.
M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi,
J. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional
domains. Advances in Neural Information Processing Systems , 33:7537–7547, 2020.
J. Tropp and A. Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE
Transactions on Information Theory , 53(12):4655–4666, 2007.
J. A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends ®in Machine
Learning , 8(1-2):1–230, 2015.
A. Tseng, A. Shrikumar, and A. Kundaje. Fourier-transform-based attribution priors improve the inter-
pretability and stability of deep learning models for genomics. Advances in Neural Information Processing
Systems, 33:1913–1923, 2020.
Z. Xu, Y. Zhang, T. Luo, Y. Xiao, and Z. Ma. Frequency principle: Fourier analysis sheds light on deep
neural networks. arXiv preprint arXiv:1901.06523 , 2019.
G. Yang and H. Salman. A fine-grained spectral perspective on neural networks. arXiv preprint
arXiv:1907.10599 , 2019.
Y. Yoshida and T. Miyato. Spectral norm regularization for improving the generalizability of deep learning.
arXiv preprint arXiv:1705.10941 , 2017.
H. Zhang and W. Yin. Gradient methods for convex minimization: better rates under weaker conditions.
arXiv preprint arXiv:1303.4645 , 2013.
Y. Zhang, Z. Xu, T. Luo, and Z. Ma. Explicitizing an implicit bias of the frequency principle in two-layer
neural networks. arXiv preprint arXiv:1905.10264 , 2019.
14Published in Transactions on Machine Learning Research (April/2023)
A Additional Empirical Validations
A.1 Visualization of real-world functions in WHT
In this subsection, we visualize the combinatorial function evaluations and the WHT of the evaluation vector,
that is, the left and right hand sides of the equation below:
/bracketleftbig
f(x) :x∈{± 1}d/bracketrightbigT=H/bracketleftbig
αz:z∈{± 1}d/bracketrightbigT, (14)
for the experimental data obtained from the fluorescence protein (Poelwijk et al., 2019). In addition to
these plots, we have attached videos as supplemental materials where we visualize the learning trajectory
of a DNN initialized using Xavier initialization under data-scarce (red) and data-sufficient (blue) regimes.
DNNs start from a local minima that does not have a well-structured WHT representation, and then gets
sparser with training. However, if a sufficient amount of data is not available for training, the network does
not converge to a good solution. Spectral regularisation enable DNN converge to the sparse solution in the
data-scarce regime.
0 2000 4000 6000 8000
protein index0.51.01.5brightnessfunction value
0 2000 4000 6000 8000
WHT index0.00.40.8coefficientspectrum
Figure 4: Combinatorial visualization of the brightness of 213proteins (top) and their Walsh-Hadamard
transform (bottom) linking two variants of a fluorescence protein that are different in 13locations on their
amino acid chain (Poelwijk et al., 2019). The brightness is a pseudo-Boolean function which maps from
f:{±1}13→R. The sparse spectrum reveals low and high order interactions among amino acid sites on
the protein.
A.2 Convergence: training and validation
In this subsection, we include additional empirical results which focus on the convergence properties of
spectral regularization for training DNNs. The first objective we study is in validating whether stochastic
gradient descent (SGD) indeed converges to approximate first order stationary interpolators (Definition 3).
15Published in Transactions on Machine Learning Research (April/2023)
The goal of these experiments are to see whether, upon running SGD, the MSE loss gets sufficiently small
even when the training loss, against which stochastic gradients are computed, is augmented with spectral
regularization. In Fig. 5, we plot the empirical training and validation loss of DNNs trained both with the
MSE loss and the additional spectral regularization. We use a depth- 4fully connected network (learning
rate= 1×10−1) to train on the fluorescence protein (Poelwijk et al., 2019) dataset (also used in the main
paper).
0 200 400 600 800 1000
epoch0.000.080.160.24training loss (MSE)
0 200 400 600 800 1000
epoch0.150.300.45validation loss (MSE)MSE+SP
MSE
Figure 5: We plot the training and validation error in terms of the empirical mean squared error (MSE)
in predicting the brightness of a fluorescence protein (Poelwijk et al., 2019) using DNNs trained with and
without the spectral regularization. We use a depth- 4fully connected neural network with Xavier initializa-
tion in both cases. The plots demonstrate that 1) the training MSE eventually gets sufficiently small, even
when the original loss function on which SGD is run is augmented with spectral regularization, even when
network uses a random initialization and 2) spectral regularization consistently allows for a significantly
better generalization gap all along the trajectory of training process.
0 200 400 600 800 1000
epoch0.000.080.160.24training loss (MSE)
0 200 400 600 800 1000
epoch0.20.30.4validation loss (MSE)MSE+SP
MSE
Figure 6: We repeat the experiment above with the only difference that instead of a Xavier initialization,
we carry out what is known as “weight initialization” (Definition 4). Here, we train the DNN with spectral
regularization (blue curve) in comparison with a DNN without spectral regularization (orange curve), but
with weight initialization. The plots demonstrate that even with weight intialization, spectral regularization
doesnotresultinanincreaseintraining, andresultsinadecreaseinvalidationMSE.Weightinitializationhas
the advantage that the QG constant encountered along the training trajectory of SGD are higher (Figure 7)
compared to with a Xavier initialization
16Published in Transactions on Machine Learning Research (April/2023)
Figure 7: We plot the minimum ratio mink∈[K]/summationtextn
i=1(fθpert,k(xi)−fθ∗(xi))2/σ2againstσ2for 4-layer CNN
models trained initialized from (i) Xavier initialization, and (ii) weight initialization, with a learning rate of
10−3. Similar to the experiments plotted in Fig. 2, we perturb θθθ∗,K= 6000times to generate θθθpert,k.
A.3 Validating assumption 1(b): additional plots
In this subsection, we include additional plots on empirical experiments on validating Assumption 1(b) .
The goal of this experiment is to also provide more details about the new weight initialization method
discussed in the Empirical Studies section of the main paper. Note that Assumption 1(b) requires showing
that for any θ,
Ex∼Unif(Dn)/bracketleftig
(fθθθ(x)−fθθθ∗(x))2/bracketrightig
≥C∗
n,δ∥θθθ−θθθ∗∥2
2. (15)
However, since it is prohibitively expensive to check this for all choices of θ, and moreover since the models we
consider exhibit some smoothness, we resort to checking this condition only for randomly sampled θθθaround
17Published in Transactions on Machine Learning Research (April/2023)
MSE+SP MSE MSE+L2 MSE+L10.160.180.200.22RMSE
Protein
MSE+SP MSE MSE+L2 MSE+L10.420.480.540.60
T cell
MSE+SP MSE MSE+L2 MSE+L10.400.480.560.64
Cancer
Figure 8: Comparison of the generalization error of different regularization schemes: spectral (SP), L1, and
L2 norm regularization.
the reference θθθ∗. In particular, we collect a set of training data points (xi,yi)n=1000
i=1from an arbitrary sparse
polynomial, f(x) :{±1}13→R, defined as 3x1+ 4x2x3+ 5x4x5+x12.
First we train a DNN using SGD on the unregularized MSE, and define this as θ∗. Then, we repeatedly
perturb the weights Ktimes independently with Gaussian noise to generate θpert,k=θ∗+Wk, where
W∼N (0,σ2I)is independent and normally distributed, for k= 1,···,K. By concentration of measure,
under the Gaussian perturbations, note that ∥θθθpert,k−θθθ∗∥2
2concentrates around Mσ2, whereMis the
number of parameters in the network. Therefore, instead of computing the ratio,
C∗
n,δmin
θθθEx∼Unif(Dn)/bracketleftig
(fθθθ(x)−fθθθ∗(x))2/bracketrightig
∥θθθ−θθθ∗∥2
2(16)
we instead approximate it by the ratio (up to scaling by M),
min
k∈[K]/summationtextn
i=1/bracketleftbig
(fθpert,k(xi)−fθ∗(xi))2/bracketrightbig
σ2(17)
asanapproximateproxyforperturbationsatthescaleof σaroundθθθ∗inL∞distance, whichismoreaccurate,
asKgrows larger. In Fig. 2, we plot the ratio in eq. (17) as a function of σ.
We repeat this experiment when θθθ∗is trained starting from weight initialization, and plot the estimated
ratio in Fig. 7.
Definition 4 (Weight initialization) .Weight initialization, refers to initialization of the network by first
running 100epochs of SGD against the unregularized MSE. From this starting point, subsequently we “turn
on” the regularization and run SGD against the MSE with spectral regularization.
Each subplot shows the estimated QG ratio (eq. (17)) for models initialized with weight initialization, at the
100, 200, 300 and 400 epochs mark respectively. The plots demonstrate that our weight initialization method
significantly improves the QG constant C∗
n,δalong sample trajectory encountered by the SGD, compared to
models trained with spectral regularization starting from a random Xavier initialization.
Remark 6. The guarantees hold for a specific choice of regularization parameter λ, which may also be adap-
tively changed over the course of optimization in the computation of gradients. In practice, the regularization
parameterλis chosen by cross-validation.
A.4 Comparison to other regularization schemes
In this subsection, we compare the performance of neural network under SP regularization with other regu-
larization schemes which directly penalize the ℓ1andℓ2norm of the weights of the neural network. The goal
18Published in Transactions on Machine Learning Research (April/2023)
number of samples200 150 100 50 200 150 100 50 200 150 100 50order of interactions
power law coefficient
power law coefficient1
3
5
1
3
5MSE+SP
01/41/23/41monomial power law staircasex3x2x6x7x1x4
WHT WHT WHTx2 +  x2x5/α + x2x3x6/α2 + ...
2
11.5
2
11.53
2x1x7 +  x2x5/α + x3x6/α2 + ...
3
2MSE+SPMSE MSE
MSE+SP MSE+SPMSE
Figure 9: The plots demonstrate the generalization power of a depth- 4feed-forward neural network in
learning 3classes of sparse pseudo-Boolean functions f(x) :{±1}13→Rusing the mean squared error
(MSE) loss before (first row) and after (second row) adding the spectral (SP) regularizer, as a function of
the number of training samples. Monomial are1-sparse functions in Walsh-Hadamard transform (WHT)
with an increasing order of interactions ( 1to5).Power law are10-sparse functions in WHT with second
order interactions and coefficients set using a power law function. Staircase are18-sparse functions in
WHT with 3first order, 6second order, and 9third order interactions, each with an equal coefficients and
set using a power law function. The heat maps show the average of the coefficient of determination among
5random independent trials on unseen data.
of these experiments is to show that promoting sparsity among the weights does not have the same effect
as promoting sparsity in the spectral representation of neural networks. We test the generalization power of
these networks using the datasets in Fig. 3 under the same experimental conditions. In Fig. 8 we demonstrate
that while direct weight-regularization schemes such as ℓ1andℓ2norm improve the generalization gap of
neural networks, the generalization gap is significantly larger for SP regularization.
B Technical lemmas
In this section we outline and prove the lemmas used for the proof of the main theorems.
We first state the subgradient of the regularizer R(θθθ) =λ√
2d∥Hfθθθ(X)∥1.
Proposition 1. The subgradient of the regularizer R(θθθ) =λ√
2d∥Hfθθθ(X)∥1is,
(∇R)(θθθ) =/braceleftbiggλ√
2d(∇fθθθ(X))Hz:z∈sgn(Hfθθθ(X))/bracerightbigg
(18)
Here, forz∈R,sgn(z) =

{+1}ifz>0
{−1}ifz<0
[−1,+1]otherwiseand applied on a vector z= (z1,···,zn)in the Cartesian
product of the sets sgn(z1)×···× sgn(zn), and∇fθθθ(X)is matrix with columns ∇fθθθ(x)for eachx∈X.
Proof.The proof follows by direct computation. Observe that, ∇∥Hfθθθ(X)∥1=/summationtext2d
i=1∇|⟨ei,Hfθθθ(X)⟩|=
/summationtext2d
i=1zi·(∇fθθθ(X))HTei= (∇fθθθ(X))Hzwhere the last equation uses the symmetry of H.
Lemma 1.∥fθθθ∗(X)−fˆθθθ(X)∥2≤2µ√
2d∥ˆθθθ−θθθ∗∥2
2+ 2L√
2d∥ˆθθθ−θθθ∗∥2.
Proof.By the Lagrange form of the Taylor series expansion, for any x, there exists a θθθxsuch that,
fˆθθθ(x)−fθθθ∗(x) = ( ˆθθθ−θθθ∗)T/parenleftbig
∇2fθθθx(x)/parenrightbig
(ˆθθθ−θθθ∗) +/angbracketleftig
∇fθθθ∗(x),ˆθθθ−θθθ∗/angbracketrightig
(19)
19Published in Transactions on Machine Learning Research (April/2023)
Then, for each x∈X,
/parenleftbig
fˆθθθ(x)−fθθθ∗(x)/parenrightbig2≤2µ2∥ˆθθθ−θθθ∗∥4
2+ 2/angbracketleftig
∇fθ∗(x),ˆθθθ−θθθ∗/angbracketrightig2
(20)
Summing over x∈Xresults in,
∥ν∥2
2=∥fˆθθθ(X)−fθθθ∗(X)∥2
2≤µ22d+1∥ˆθθθ−θθθ∗∥4
2+L22d+1∥ˆθθθ−θθθ∗∥2
2. (21)
where recall the assumption,1
2d/summationtext
x∈X∇fθ∗(x)(∇fθ∗(x))T⪯L2.
Lemma 2. maxv:∥v∥∞≤1∥Hv∥1≤(2d+ 1)√
2d.
Proof.Note that maxv:∥v∥∞≤1∥Hv∥1= maxv∈{−1,+1}2d∥Hv∥1, by (Rohn, 2000, Proposition 1). Therefore,
it suffices to maximize over v∈{− 1,+1}2d. It is known from Figula. & Kvaratskhelia (2015) that ˆϱ(d)≤d2d
where ˆϱ(d)≜maxm∈[2d]√
2d∥ΦH1m∥1where Φis an arbitrary set of vectors from the unit ∥·∥1ball in R2d,
1mis the vector with the first mentries 1and the remaining entries 0. Since Φcan be chosen as any
permutation matrix, this result implies that maxv∈{0,1}2d∥Hv∥1≤d2d. To compare with the previous
statement, note that the number of non-zeros in vhere equals min the optimization problem defining ˆϱ(d).
Then, we have that,
max
v∈{−1,+1}2d∥Hv∥1= max
v∈{0,1}2d∥H(2v−1)∥1 (22)
(i)
≤2∥Hv∥1+∥H1∥1 (23)
≤2d√
2d+√
2d, (24)
where (i)follows by triangle inequality.
Lemma 3. Under Assumptions 1(a) and 2, for any θθθ∈Rdand any subgradient G∈(∇R)(θθθ), under the
eventErsi:⟨θθθ−θθθ∗,Errn(θθθ,θθθ∗)⟩≥C∗
n,δ∥θθθ−θθθ∗∥2
2which happens with probability ≥1−δ,
/angbracketleftig
θθθ−θθθ∗,∇Ln(θθθ) +G/angbracketrightig
−2Ex∼Unif(Dn)[(fθθθ∗(x)−y(x)))⟨θθθ−θθθ∗,∇fθθθ(x)⟩] (25)
≥/parenleftbig
C∗
n,δ−(2d+ 1)λµ/parenrightbig
∥θθθ−θθθ∗∥2
2+R(θθθ)−R(θθθ∗) (26)
Proof.Observe that,
⟨θθθ−θθθ∗,∇Ln(θθθ)⟩= 2Ex∼Unif(Dn)[(fθθθ(x)−y(x))⟨θθθ−θθθ∗,∇fθθθ(x)⟩] (27)
Plugging this in below,
⟨θθθ−θθθ∗,∇Ln(θθθ)⟩−2Ex∼Unif(Dn)[(fθθθ∗(x)−y(x)))⟨θθθ−θθθ∗,∇fθθθ(x)⟩] (28)
= 2Ex∼Unif(Dn)[(fθθθ(x)−y(x))⟨θθθ−θθθ∗,∇fθθθ(x)⟩−(fθθθ∗(x)−y(x))⟨θθθ−θθθ∗,∇fθθθ(x)⟩] (29)
= 2Ex∼Unif(Dn)[(fθθθ(x)−fθθθ∗(x))⟨θθθ−θθθ∗,∇fθθθ(x)⟩] (30)
=/angbracketleftbig
θθθ−θθθ∗,∇Ex∼Unif(Dn)/bracketleftbig
(fθθθ(x)−fθθθ∗(x))2/bracketrightbig/angbracketrightbig
(31)
≥C∗
n,δ∥θθθ−θθθ∗∥2
2 (32)
where the last inequality follows by the RSI condition imposed on Errn(θθθ,θθθ∗)in Assumption 1(a). Putting
together eq. (32) with Lemma 5, uner the event Ersi,
⟨θθθ−θθθ∗,∇Ln(θθθ) +G⟩−2Ex∼Unif(Dn)[(fθθθ∗(x)−y(x)))⟨θθθ−θθθ∗,∇fθθθ(x)⟩] (33)
≥C∗
n,δ∥θθθ−θθθ∗∥2
2+R(θθθ)−R(θθθ∗)−(2d+ 1)λµ∥θθθ−θθθ∗∥2
2 (34)
This completes the proof.
20Published in Transactions on Machine Learning Research (April/2023)
In the remaining proofs, we will assume a lower bound on the regularization parameter to ensure that the
regularization actually plays a role. In particular,
Condition 1. Assume that the regularization parameter λsatisfies,
λ≥12σ/radicalbigg
d+ log(1/δ)
n. (35)
Lemma 4. Supposeλsatisfies the lower bound in Condition 1. Define E2as the event that, for all θθθ∈Rd,
Ex∼Unif(Dn)[(fθθθ∗(x)−y(x)))⟨θθθ∗−θθθ,∇fθθθ(x)⟩] (36)
≤λ
4/radicalbigg
1
2d∥H(fθθθ∗(X)−fθθθ(X))∥1+λµ(2d+ 1)
4∥θθθ−θθθ∗∥2
2. (37)
Then, Pr(E2)≥1−δ.
Proof.For each sample x′∈Dn, define the noise in the sample as z(x′) =fθθθ∗(x′)−y(x′). Likewise, define the
noise vector zDn={Ex∼Unif(Dn)[z(x′)I(x′=x)] :x∈[2d]}. Note that the coordinates of zDncorresponding
to inputs unobserved in the dataset Dnare0.
By Taylor series expanding, fθθθ∗(x)−fθθθ(x) =⟨θθθ∗−θθθ,∇fθθθ(x)⟩+ (θθθ∗−θθθ)T/parenleftbig
∇2fθθθx(x)/parenrightbig
(θθθ∗−θθθ)for some
θθθx∈conv({θθθ,θθθ∗}). Therefore,
Ex∼Unif(Dn)[(fθθθ∗(x)−y(x)))⟨θθθ∗−θθθ,∇fθθθ(x)⟩] (38)
=Ex∼Unif(Dn)/bracketleftbig
(fθθθ∗(x)−y(x)))/parenleftbig
fθθθ∗(x)−fθθθ(x)−(θθθ−θθθ∗)T/parenleftbig
∇2fθθθx(x)/parenrightbig
(θθθ−θθθ∗)/parenrightbig/bracketrightbig
(39)
=⟨zDn,fθθθ∗(X)−fθθθ(X)−A(θθθ,θθθ∗)⟩, (40)
where A(θθθ,θθθ∗)denotes the vector/braceleftbig
(θθθ∗−θθθ)T∇2fθθθx(x)(θθθ∗−θθθ) :x∈[2d]/bracerightbig
.
By an application of Holder’s inequality and triangle inequality of the L1-norm, we have,
Ex∼Unif(Dn)[(fθθθ∗(x)−y(x)))⟨θθθ∗−θθθ,∇fθθθ(x)⟩] (41)
≤∥HzDn∥∞/vextenddouble/vextenddoubleH/parenleftbig
fθθθ∗(X)−fˆθθθ(X)/parenrightbig/vextenddouble/vextenddouble
1+∥HzDn∥∞∥HA(θθθ,θθθ∗)∥1 (42)
Note that for each fixed row i∈[2d],⟨Hi,zDn⟩=/summationtext
j∈2dHijzDn(j). Note that the coordinates of zDnare
independently distributed and subgaussian. Therefore, by Bernstein’s inequality,
Pr
⟨Hi,zDn⟩≥3/radicaltp/radicalvertex/radicalvertex/radicalbt/parenleftig/summationtext
j∈[2d]Var(zDn(j))/parenrightig
log(1/δ)
2d
≤δ (43)
Note that the coordinate of zDnlabelled by x∈{± 1}d,Ex∼Unif(Dn)[z(x′)I(x′=x)], is the sum of Dn(x)
independentN(0,σ2)Gaussians scaled by 1/n, whereDn(x), defined as the number of times xis sampled in
Dn. Therefore, Var(zDn(i)) =σ2Dn(x)
n2. By union bounding over the 2drows of H, with probability ≥1−δ,
Pr
∥HzDn∥∞≥3/radicaltp/radicalvertex/radicalvertex/radicalbt/parenleftig/summationtext
x∈[2d]σ2Dn(x)
n2/parenrightig
log(2d/δ)
2d
≤δ. (44)
Note that/summationtext
x∈[2d]Dn(x) =n, and therefore, with probability ≥1−δ, the eventE1, defined below, is satisfied,
E1:∥HzDn∥∞≤3σ/radicalbigg
1
2d/radicalbigg
d+ log(1/δ)
n≤λ
4√
2d. (45)
21Published in Transactions on Machine Learning Research (April/2023)
where the last inequality follows by the assumption on λin Condition 1.
Note that∥A(θθθ,θθθ∗)∥∞≤µ∥θθθ−θθθ∗∥2
2. Thus, the term ∥HA(θθθ,θθθ∗)∥1can be upper bounded by
supv:∥v∥∞≤µ∥θθθ−θθθ∗∥2
2∥Hv∥1. ThisitselfcanbefurtherupperboundedusingLemma2by µ∥θθθ−θθθ∗∥2
2(2d+1)√
2d.
All in all, combining this argument and eq. (45) with eq. (42), under the event E1, with probability ≥1−δ,
Ex∼Unif(Dn)[(fθθθ∗(x)−y(x)))⟨θθθ∗−θθθ,∇fθθθ(x)⟩] (46)
≤λ
4/radicalbigg
1
2d/vextenddouble/vextenddoubleH/parenleftbig
fθθθ∗(X)−fˆθθθ(X)/parenrightbig/vextenddouble/vextenddouble
1+λµ(2d+ 1)
4∥θθθ−θθθ∗∥2
2. (47)
Lemma 5. Under the Lipschitz gradient condition, Assumption 2, consider any θθθ∈Rdand any subgradient
G∈(∇R)(θθθ). Then,
⟨θθθ−θθθ∗,G⟩≥R(θθθ)−R(θθθ∗)−(2d+ 1)λµ∥θθθ−θθθ∗∥2
2. (48)
Proof.Recall that the regularization function R(θθθ)is defined asλ√
2d∥Hfθθθ(X)∥1. Consider any subgradient
G∈(∇R)(θθθ)of the regularization function R(θθθ). By Proposition 1, this is of the formλ√
2d(∇fθθθ(X))Hz,
wherez∈sgn(Hfθθθ(X)). In particular, using this representation, we have the equation,
⟨θθθ−θθθ∗,G⟩=λ√
2d(θθθ−θθθ∗)T(∇fθθθ(X))Hz. (49)
Sincefθθθ(x)is in general non-linear in θθθ, we can relate (θθθ−θθθ∗)T∇fθθθ(x)tofθθθ(x)−fθθθ∗(x)by using a Taylor
series expansion. In particular, for each x∈[2d]and eachθθθandθθθ∗, by the Lagrange form of the Taylor
series expansion, there exists a θθθx∈conv({θθθ,θθθ∗})such thatfθθθ∗(x)−fθθθ(x) =⟨∇fθθθ(x),θθθ∗−θθθ⟩+ (θθθ∗−
θθθ)T∇2fθθθx(x)(θθθ∗−θθθ). In addition, note from Assumption 2 that −µI⪯∇2fθθθ(x)⪯µI. This results in the
following set of inequalities,
⟨θθθ−θθθ∗,G⟩=λ√
2d
fθθθ(x)−fθθθ∗(x) + (θθθ−θθθ∗)T∇2fθθθx(x)(θθθ−θθθ∗)
...

x∈[2d]Hz (50)
≥λ√
2d(fθθθ(X)−fθθθ∗(X))THz−λ√
2dµ∥θθθ−θθθ∗∥2
2∥Hz∥1 (51)
≥λ√
2d(Hfθθθ(X)−Hfθθθ∗(X))Tz−(2d+ 1)λµ∥θθθ−θθθ∗∥2
2 (52)
where the last inequality follows from Lemma 2, where we show that for any vector v:∥v∥∞≤1(a condition
which is satisfied by the sign vector z),∥Hv∥1≤(2d+ 1)√
2d.
A naive attempt to prove such a bound turns out to result in a loose bound. Indeed, ∥Hv∥1≤√
2d∥Hv∥2=√
2d∥v∥2≤√
2d√
2d= 2d. The improvement of one of the√
2dfactors to (2d+ 1)turns to be quite a deep
mathematical fact, and we invoke a result of Figula. & Kvaratskhelia (2015) to prove this result in Lemma 2.
Finally, using the convexity of ∥·∥ 1, for anyz∈sgn(Hfθθθ(X)),
(Hfθθθ(X)−Hfθθθ∗(X))Tz=∥Hfθθθ(X)∥1−(Hfθθθ∗(X))Tz (53)
≥∥Hfθθθ(X)∥1−sup
v:∥v∥∞≤1⟨v,Hfθθθ∗(X)⟩ (54)
≥∥Hfθθθ(X)∥1−∥Hfθθθ∗(X)∥1 (55)
Combining this with eq. (52) and using the definition of R(θθθ)completes the proof.
22Published in Transactions on Machine Learning Research (April/2023)
Lemma 6. Under Assumption 2, for any θθθ∈Rmand any subgradient G∈(∇R)(θθθ),
/angbracketleftig
θθθ−θθθ∗,∇Ln(θθθ) +G/angbracketrightig
+ 2Ex∼Unif(Dn)[(fθθθ∗(x)−y(x))) (fθθθ∗(x)−fθθθ(x))] (56)
≥2Errn(θθθ,θθθ∗) +R(θθθ)−R(θθθ∗)−/parenleftig
(2d+ 1)λµ+ 2µ/radicalbig
Ln(θθθ)/parenrightig
∥θθθ−θθθ∗∥2
2 (57)
Proof.The proof of this result builds on the analysis in Lemma 3. Observe that,
⟨θθθ−θθθ∗,∇Ln(θθθ)⟩=Ex∼Unif(Dn)[2(fθθθ(x)−y(x))⟨θθθ−θθθ∗,∇fθθθ(x)⟩] (58)
For eachx∈[2d]and eachθθθandθθθ∗, there exists a θθθx∈conv({θθθ,θθθ∗})such thatfθθθ(x)−fθθθ∗(x) =
⟨∇fθθθ(x),θθθ−θθθ∗⟩−(θθθ−θθθ∗)T∇2fθθθx(x)(θθθ−θθθ∗). Plugging this in, and using the assumption that −µI⪯
∇2fθθθ(x)⪯µI,
⟨θθθ−θθθ∗,∇Ln(θθθ)⟩−2Ex∼Unif(Dn)[(fθθθ∗(x)−y(x))) (fθθθ(x)−fθθθ∗(x))] (59)
=Ex∼Unif(Dn)[2(fθθθ(x)−y(x))⟨θθθ−θθθ∗,∇fθθθ(x)⟩−2(fθθθ∗(x)−y(x))(fθθθ(x)−fθθθ∗(x))] (60)
≥Ex∼Unif(Dn)[2(fθθθ(x)−y(x))(fθθθ(x)−fθθθ∗(x))−2(fθθθ∗(x)−y(x))(fθθθ(x)−fθθθ∗(x))] (61)
−2µ∥θθθ−θθθ∗∥2
2Ex∼Unif(Dn)[|fθθθ(x)−y(x)|] (62)
≥2Ex∼Unif(Dn)/bracketleftbig
(fθθθ(x)−fθθθ∗(x))2/bracketrightbig
−2µ∥θθθ−θθθ∗∥2
2/parenleftig/radicalbig
Ln(θθθ)/parenrightig
(63)
where the last inequality follows by Jensen’s inequality.
Putting together Lemma 5 and eq. (63), for any subgradient G∈(∇R)(θθθ), we have,
⟨θθθ−θθθ∗,∇Ln(θθθ) +G⟩−2Ex∼Unif(Dn)[(fθθθ∗(x)−y(x))) (fθθθ(x)−fθθθ∗(x))] (64)
≥2Ex∼Unif(Dn)/bracketleftbig
(fθθθ(x)−fθθθ∗(x))2/bracketrightbig
+λ√
2d(∥Hfθθθ(X)∥1−∥Hfθθθ∗(X)∥1) (65)
−/parenleftig
(2d+ 1)λµ+ 2µ/radicalbig
Ln(θθθ)/parenrightig
∥θθθ−θθθ∗∥2
2 (66)
This completes the proof.
Lemma 7. Suppose the regularization parameter satisfies the lower bound in Condition 1. Define E3as the
event that for all θθθ∈Rm,
Ex∼Unif(Dn)[(fθθθ∗(x)−y(x))) (fθθθ∗(x)−fθθθ(x))]≤λ
2/radicalbigg
1
2d∥H(fθθθ∗(X)−fθθθ(X))∥1(67)
Then, Pr(E3)≥1−δ.
Proof.The proof of this result is similar to that of Lemma 4, and we include the details for completeness.
For each sample x′∈Dn, define the noise in the sample as z(x′) =fθθθ∗(x′)−y(x). Likewise, define the noise
vector zDn={Ex∼Unif(Dn)[z(x′)I(x′=x)] :x∈[2d]}. Then, by an application of Holder’s inequality, we
have,
Ex∼Unif(Dn)/bracketleftbig
(fθθθ∗(x)−y(x)))/parenleftbig
fθθθ∗(x)−fˆθθθ(x)/parenrightbig/bracketrightbig
≤∥HzDn∥∞/vextenddouble/vextenddoubleH/parenleftbig
fθθθ∗(X)−fˆθθθ(X)/parenrightbig/vextenddouble/vextenddouble
1.(68)
Note that for each fixed row i∈[2d],⟨Hi,zDn⟩=/summationtext
j∈2dHijzDn(j). Note that the coordinates of zDnare
independently distributed and subgaussian. Therefore, by Bernstein’s inequality,
Pr
⟨Hi,zDn⟩≥3/radicaltp/radicalvertex/radicalvertex/radicalbt/parenleftig/summationtext
j∈[2d]Var(zDn(j))/parenrightig
log(1/δ)
2d
≤δ (69)
23Published in Transactions on Machine Learning Research (April/2023)
Note that the coordinate of zDnlabelled by x∈{± 1}d,Ex∼Unif(Dn)[z(x′)I(x′=x)], is the sum of Dn(x)
independentN(0,σ2)Gaussians scaled by 1/n, whereDn(x), defined as the number of times xis sampled in
Dn. Therefore, Var(zDn(i)) =σ2Dn(x)
n2. By union bounding over the 2drows of H, with probability ≥1−δ,
Pr
∥HzDn∥∞≥3/radicaltp/radicalvertex/radicalvertex/radicalbt/parenleftig/summationtext
x∈[2d]σ2Dn(x)
n2/parenrightig
log(2d/δ)
2d
≤δ. (70)
Note that/summationtext
x∈[2d]Dn(x) =n, and therefore, with probability ≥1−δ,
∥HzDn∥∞≤3σ/radicalbigg
1
2d/radicalbigg
d+ log(1/δ)
n≤λ
4√
2d. (71)
where the last inequality follows by the assumption on λ. This implies that with probability ≥1−δ,
Ex∼Unif(Dn)/bracketleftbig
(fθθθ∗(x)−y(x)))/parenleftbig
fθθθ∗(x)−fˆθθθ(x)/parenrightbig/bracketrightbig
≤λ
2/radicalbigg
1
2d/vextenddouble/vextenddoubleH/parenleftbig
fθθθ∗(X)−fˆθθθ(X)/parenrightbig/vextenddouble/vextenddouble
1(72)
Lemma 8. Assume that d≥3andk≤2d/4. Then, there exists a set of 2(d−1)⌊k/2⌋binary vectors of length
2d, denotedCk, each having at most kones, such that: the hamming distance between any pair of vectors is
at least⌊k
2⌋.
Proof.The number of vectors within hamming distance κ=⌊k/2⌋of any vector is/parenleftbig2d
κ/parenrightbig
. Note that k−κ≥κ.
We can greedily construct a packing of size at least/parenleftbig2d
k/parenrightbig
//parenleftbig2d
κ/parenrightbig
≥(2d−κ)!
(2d−k)!≥2d(k−κ)/parenleftbig3
4/parenrightbigk−κ≥2dκ/parenleftbig3
4/parenrightbigκ≥
2(d−1)κvectors before running out of binary vectors to choose. By construction, every pair of vectors has
Hamming distance at least κ.
C Proof of Theorem 1 - Statistical performance under RSI
In this section we discuss the proof of Theorem 1. We prove a slightly more general result which characterizes
the performance of stationary points of the MSE with spectral regularization under the RSI, when the
regularization parameter is chosen arbitrarily.
Theorem 4. Suppose the regularization parameter λsatisfies Condition 1. Namely,
λ≥12σ/radicalbigg
d+ log(1/δ)
n(73)
Consider a learner which returns any first order stationary point of the loss Ln(θθθ) +R(θθθ). Under Assump-
tions 1(a), 2 and 3, ifC∗
n,δ
2−3
2(2d+ 1)λµ−3λL√
k>0, with probability ≥1−2δ,
∥ˆθθθ−θθθ∗∥2≤6λL√
k
C∗
n,δ(74)
Proof.Plugging in θθθ=ˆθθθinto Lemma 3, and noting that 0∈∇Ln(ˆθθθ) + (∇R)(ˆθθθ), choosingGappropriately
we obtain, conditioned on Ersi,
2Ex∼Unif(Dn)/bracketleftig
(fθθθ∗(x)−y(x)))/angbracketleftig
θθθ∗−ˆθθθ,∇fˆθθθ(x)/angbracketrightig/bracketrightig
≥R(ˆθθθ)−R(θθθ∗) +/parenleftbig
C∗
n,δ−(2d+ 1)λµ/parenrightbig
∥ˆθθθ−θθθ∗∥2
2 (75)
24Published in Transactions on Machine Learning Research (April/2023)
Now, we upper bound the LHS of this equation using Lemma 4. Plugging Lemma 4 into eq. (75) with the
choice ofθθθ=ˆθθθ, and rearranging both sides, under the event ErsiandE2which jointly occur with probability
≥1−2δ,
/parenleftbigg
C∗
n,δ−3
2(2d+ 1)λµ/parenrightbigg
∥ˆθθθ−θθθ∗∥2
2
(i)
≤λ√
2d∥Hfθθθ∗(X)∥1−λ√
2d/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1+λ
2√
2d/vextenddouble/vextenddoubleH/parenleftbig
fθθθ∗(X)−fˆθθθ(X)/parenrightbig/vextenddouble/vextenddouble
1(76)
(ii)
≤λ√
2d/parenleftbigg
∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1+1
2∥Hfθθθ∗(X)∥1+1
2/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbigg
(77)
=λ
2√
2d/parenleftbig
3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbig
(78)
where (i)follows from the definition of the regularization term, R(θθθ) =λ√
2d∥Hfθθθ(X)∥1. On the other hand,
(ii)follows by triangle inequality of the norm ∥·∥1. By the assumption1
2C∗
n,δ≥3
2(2d+ 1)λµ+ 3λL√
k, the
LHS of eq. (78) is non-negative. Plugging this into eq. (78) results in the inequality,
λ
2√
2d/parenleftbig
3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbig
≥0. (79)
Conditioned onE2andErsi.
Next, we apply (Loh & Wainwright, 2015, Lemma 5) to the function ρλ(·) =∥·∥ 1, and note that by
assumption Hfθθθ∗(X)isk-sparse. Define ν=H(fθθθ∗(X)−fˆθθθ(X))andAas the set of klargest indices of ν
in absolute value. Using the fact that 3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1≥0,
λ
2√
2d/parenleftbig
3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbig(i)
≤λ√
2d(3∥νA∥1−∥νAc∥1) (80)
≤3λ√
k
2√
2d∥νA∥2 (81)
≤3λ√
k
2√
2d∥ν∥2. (82)
Plugging this back into eq. (78) results in the following inequality, conditioned on the event E2andErsi,
/parenleftbigg
C∗
n,δ−3
2(2d+ 1)λµ/parenrightbigg
∥ˆθθθ−θθθ∗∥2
2≤3λ√
k
2√
2d∥fθθθ∗(X)−fˆθθθ(X)∥2 (83)
Finally, we relate ∥fθθθ∗(X)−fˆθθθ(X)∥2to∥ˆθθθ−θθθ∗∥2.
Plugging the bound in Lemma 1 into eq. (83) results in the following inequality, conditioned on E2andErsi,
/parenleftbigg
C∗
n,δ−3
2(2d+ 1)λµ/parenrightbigg
∥ˆθθθ−θθθ∗∥2
2≤3µλ√
k∥ˆθθθ−θθθ∗∥2
2+ 3λL√
k∥ˆθθθ−θθθ∗∥2. (84)
Furthermore, recalling the assumption thatC∗
n,δ
2−3
2(2d+ 1)λµ−3µλ√
k≥0, this results in the overall
bound,
∥ˆθθθ−θθθ∗∥2≤3λL√
k
C∗
n,δ−3
2(2d+ 1)λµ−3µλ√
k≤6λL√
k
C∗
n,δ, (85)
under the events E2andErsiwhich jointly occur with probability ≥1−2δ.
25Published in Transactions on Machine Learning Research (April/2023)
Proof of Theorem 1. Theorem 1 follows from Theorem 4 by choosing λas its lower bound in Condition 1,
equal to 12σ/radicalig
d+log(1/δ)
n. It is easily verified that when the size of the dataset, n, is larger than the quantity
n0as defined in the statement of Theorem 1, the conditionC∗
n,δ
2−3
2(2d+ 1)λµ−3λL√
k>0as required in
Theorem 4 is satisfied.
Notes on the constants. We did not choose to optimize the constants in the theorem statements to keep
the proofs simple. With more careful analysis they can be brought down further.
D Proof of Theorem 2 - Statistical performance under QG condition
In this section we discuss the proof of Theorem 2. This result is a consequence of a more general result
which characterizes the performance of stationary points of the MSE with spectral regularization when the
regularization parameter, λ, is chosen arbitrarily.
Theorem 5. Define ∆ =C∗
n,δ
2µ−(2d+1)λ
2−3
2λ√
kand assume ∆>0. Suppose the regularization parameter
λsatisfies Condition 1. Namely,
λ≥12σ/radicalbigg
d+ log(1/δ)
n(86)
Consider a learner which returns a ∆2-approximate first order stationary interpolator (Definition 3) of the
lossLn(θθθ) +R(θθθ). Then, under Assumptions 1(b), 2 and 3, with probability ≥1−2δ,
∥ˆθθθ−θθθ∗∥2≤3λL√
k
C∗
n,δ. (87)
Proof.Plugging in θθθ=ˆθθθinto Lemma 6, and noting that the learner returns a first order stationary point of
the regularized loss, 0∈∇Ln(ˆθθθ) + (∇R)(ˆθθθ), choosingGappropriately we have, that under the event Eqg,
2Ex∼Unif(Dn)/bracketleftbig
(fθθθ∗(x)−y(x)))/parenleftbig
fθθθ∗(x)−fˆθθθ(x)/parenrightbig/bracketrightbig
(88)
≥2Errn(ˆθθθ,θθθ∗) +R(ˆθθθ)−R(θθθ∗)−/parenleftbigg
(2d+ 1)λµ+ 2µ/radicalig
Ln(ˆθθθ)/parenrightbigg
∥ˆθθθ−θθθ∗∥2
2 (89)
Under assumption (A1), recall that we assume that ˆθθθis a sufficiently good interpolator in that, Ln(ˆθθθ)≤/parenleftigC∗
n,δ
2µ−(2d+1)λ
2−3
2√
kλ/parenrightig2
. Simplifying eq. (89) under this assumption gives,
2Ex∼Unif(Dn)/bracketleftbig
(fθθθ∗(x)−y(x)))/parenleftbig
fθθθ∗(x)−fˆθθθ(x)/parenrightbig/bracketrightbig
≥2Errn(ˆθθθ,θθθ∗) +R(ˆθθθ)−R(θθθ∗)−/parenleftig
C∗
n,δ−3µλ√
k/parenrightig
∥ˆθθθ−θθθ∗∥2
2 (90)
Next we bound the LHS of eq. (90) using Lemma 7.
Plugging Lemma 7 into (90) and rearranging both sides, under the event E3,
2Errn(ˆθθθ,θθθ∗)−/parenleftig
C∗
n,δ−3λµ√
k/parenrightig
∥ˆθθθ−θθθ∗∥2
2
(i)
≤λ√
2d∥Hfθθθ∗(X)∥1−λ√
2d/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1+λ
2√
2d/vextenddouble/vextenddoubleH/parenleftbig
fθθθ∗(X)−fˆθθθ(X)/parenrightbig/vextenddouble/vextenddouble
1(91)
(ii)
≤λ√
2d/parenleftbigg
∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1+1
2∥Hfθθθ∗(X)∥1+1
2/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbigg
(92)
=λ
2√
2d/parenleftbig
3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbig
(93)
where (i)follows from the definition of the regularization term, R(θθθ) =λ√
2d∥Hfθθθ(X)∥1. On the other hand,
(ii)follows by triangle inequality of the norm ∥·∥1. Next we focus on the LHS of the above expression and
26Published in Transactions on Machine Learning Research (April/2023)
simplifyitfurther. Bythequadraticgrowthconditioninassumption1(b), undertheevent Eqg: Errn(ˆθθθ,θθθ∗)≥
C∗
n,δ∥ˆθθθ−θθθ∗∥2
2, which is assumed to happen with probability ≥1−δ. Therefore, under E3andEqg,
2Errn(ˆθθθ,θθθ∗)−/parenleftig
C∗
n,δ−3λµ√
k/parenrightig
∥ˆθθθ−θθθ∗∥2
2 (94)
≥∥ˆθθθ−θθθ∗∥2
2/parenleftig
C∗
n,δ+ 3λµ√
k/parenrightig
(95)
≥0. (96)
Plugging this into eq. (93) results in the inequality,
λ
2√
2d/parenleftbig
3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbig
≥0. (97)
Under the events E3andEqg.
Next, we apply (Loh & Wainwright, 2015, Lemma 5) to the function ρλ(·) =∥·∥ 1, and note that by
assumption Hfθθθ∗(X)isk-sparse. By defining ν=H(fθθθ∗(X)−fˆθθθ(X))andAas the set of klargest indices
ofνin absolute value.
λ
2√
2d/parenleftbig
3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbig(i)
≤λ√
2d(3∥νA∥1−∥νAc∥1) (98)
≤3λ√
k
2√
2d∥νA∥2 (99)
≤3λ√
k
2√
2d∥ν∥2. (100)
here, (i)uses the fact that 3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1≥0from eq. (97).
Finally, we plug the relation between ∥v∥2=∥fθθθ∗(X)−fˆθθθ(X)∥2to∥ˆθθθ−θθθ∗∥2proved in Lemma 1 into
eq. (100). This results in the inequality,
λ
2√
2d/parenleftbig
3∥Hfθθθ∗(X)∥1−/vextenddouble/vextenddoubleHfˆθθθ(X)/vextenddouble/vextenddouble
1/parenrightbig
≤3µλ√
k∥ˆθθθ−θθθ∗∥2
2+ 3λL√
k∥ˆθθθ−θθθ∗∥2 (101)
Plugging this back into eq. (93), under E3andEqg,
2Errn(ˆθθθ,θθθ∗)−/parenleftig
C∗
n,δ−3λµ√
k/parenrightig
∥ˆθθθ−θθθ∗∥2
2≤3µλ√
k∥ˆθθθ−θθθ∗∥2
2+ 3λL√
k∥ˆθθθ−θθθ∗∥2. (102)
Resulting in the bound,
2Errn(ˆθθθ,θθθ∗)≤3λL√
k∥ˆθθθ−θθθ∗∥2+C∗
n,δ∥ˆθθθ−θθθ∗∥2
2. (103)
Under the quadratic growth condition, by the event E3in Assumption 1(b), Errn(ˆθθθ,θθθ∗)≥C∗
n,δ∥ˆθθθ−θθθ∗∥2
2.
Therefore, under the events E3andEqgwhich jointly occur with probability ≥1−2δ,
∥ˆθθθ−θθθ∗∥2≤3λL√
k
C∗
n,δ. (104)
Proof of Theorem 2. Theorem 2 follows from Theorem 5 by choosing λas its lower bound in Condition 1,
equal to 12σ/radicalig
d+log(1/δ)
n. It is easily verified that when n>n 0, as defined in the statement of Theorem 2,
the condition ∆ =C∗
n,δ
2−1
2(2d+ 1)λµ−3
2λL√
k>0as required in Theorem 5 is satisfied.
27Published in Transactions on Machine Learning Research (April/2023)
E Proof of Theorem 3
In this section, we prove a lower bound on the statistical error of parameter estimation. Loosely speaking,
the objective is to show that for every learner ˆθθθ,
sup
θθθ∗E/bracketleftig
∥ˆθθθ−θθθ∗∥2/bracketrightig
≳σ/radicalbigg
kd
n. (105)
Proof.WefirstintroduceanauxiliaryresultrelatedtopackingbinaryvectorswithboundedHammingweight
in Lemma 8. Now define the function space G={fθθθ(·) :θθθ∈R2d}, wherefθθθ(x)is defined as the polynomial
with coefficients specified by θθθ. Namely,fθθθ(x) =/summationtext
S⊆[d]θS/producttext
i∈Sxi, where we index the 2dcoefficients of θ
by the 2dsubsets of [d]. In an alternate notation, we may represent,
fθθθ(x) =/angbracketleftig
θ,2[x]/angbracketrightig
(106)
where 2[x]denotes the 2dlength vector whose element indexed by some subset S⊆[d]is/producttext
i∈Sxi.
Furthermore, we assume that the data generating distribution independently samples npairs (xi,yi)where
xi∼Unif({±1}d)andyi=fθθθ∗(xi) +ZiwhereZi∼N(0,σ2). DenoteDn={x1,···,xn}.
By Lemma 8, the binary vectors belonging to Ckcan be used to construct a subset of the function space G,
defined asSk,
Sk={fθθθ(·) :θθθ∈∆Ck}, (107)
where ∆>0is a scaling factor and ∆Ck={∆θθθ:θθθ∈Ck}.
Henceforth, we will consider ourselves with learning functions (resp. parameters) in the class Sk(resp. ∆Ck).
First we show the properties on Errn(θθθ,θθθ∗)andGin the statement of Theorem 3. Note that Gis a linear
family by the representation in eq. (106), and therefore µ= 0.
In addition, note that,
Errn(θθθ,θθθ∗) =Ex∼Dn/bracketleftbig
(fθθθ(x)−fθθθ∗(x))2/bracketrightbig
(108)
=Ex∼Dn/bracketleftbigg/angbracketleftig
θθθ−θθθ∗,2[x]/angbracketrightig2/bracketrightbigg
(109)
= (θθθ−θθθ∗)TEx∼Dn/bracketleftig
2[x](2[x])T/bracketrightig
(θθθ−θθθ∗) (110)
Note thatAx= 2[x](2[x])Tis a matrix whose entries (indexed by pairs of subsets of [d]) can be described as
Ax(S,T) =/producttext
i∈Sxi/producttext
j∈Sxj. Note that in expectation over x∼Unif(X), we have that,
Ex∼Unif(X)[Ax(S,T)] =I(S=T) (111)
This is because if S̸=T, there exists an element i∈(S\T)∪(T\S)(i.e. the symmetric difference of the
two sets) and since Ex∼Unif(X)[xi] = 0, we get the required statement. Therefore,
Ex∼Unif(X)[Ax] =I (112)
Now, given nsamples from the uniform distribution, Ex∼Unif(Dn)[Ax]is expected to concentrate around its
expectation Ex∼Unif(X)[Ax]. In particular, by invoking the matrix Bernstein inequality Tropp et al. (2015),
we have that,
Pr/parenleftig/vextenddouble/vextenddoubleEx∼Unif(Dn)[Ax]−Ex∼Unif(X)[Ax]/vextenddouble/vextenddouble
op≥t/parenrightig
≤2(2d) exp/parenleftbig
−nt2/2L2/parenrightbig
(113)
28Published in Transactions on Machine Learning Research (April/2023)
whereLis an almost sure upper bound on ∥Ax∥op. Note that∥Ax∥op=∥2[x]∥2=√
2dand therefore we
may choose L=√
2d. This results in the bound,
Pr/parenleftbigg/vextenddouble/vextenddoubleEx∼Unif(Dn)[Ax]−I/vextenddouble/vextenddouble
op≥1
2/parenrightbigg
≤2(2d) exp/parenleftbig
−n/2d+3/parenrightbig
(114)
Therefore, if n≳(d+ log(1/δ))2d+3, with probability ≥1−δ,
1
2I⪯Ex∼Unif(Dn)[Ax]⪯3
2I. (115)
In eq. (110), this implies that, with probability ≥1−δ,
Errn(θθθ,θθθ∗)≥1
2∥θθθ−θθθ∗∥2
2 (116)
And likewise, from the linear representation in eq. (106),
Ex∼Unif(X)/bracketleftbig
∇fθθθ∗(x)(∇fθθθ∗(x))T/bracketrightbig
=Ex∼Unif(X)/bracketleftig
2[x](2[x])T/bracketrightig
=I (117)
where the last equation follows from eq. (112).
To generate the lower bound instance, suppose the ground truth parameter θθθ∗is sampled uniformly from
∆Ck. Suppose the learner outputs a candidate parameter ˆθθθ. The population level mean squared error of the
learner is lower bounded by the testing error,
sup
θθθ∗∈R2dE/bracketleftig
∥ˆθθθ−θθθ∗∥2/bracketrightig
≥Eθθθ∗∼Unif(∆Ck)/bracketleftig
E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleˆθθθ−θθθ∗/vextenddouble/vextenddouble/vextenddouble
2/bracketrightig/bracketrightig
(118)
≥1
4/radicalbig
⌊k/2⌋∆ inf
ΨEθθθ∗∼Unif(Ck)[I(Ψ(Dn))̸=θθθ∗)] (119)
where the infimum is over all tests functions which return a function in Ck. This uses the fact that any
estimator ˆθθθinduces a testing function for θθθ∗by returning the θθθ∈∆Cksuch that∥θθθ−ˆθθθ∥2is smallest. If this
test makes a mistake, then ˆθθθmust have predicted >1
2/radicalbig
⌊k/2⌋entries ofθθθ∗as<∆/2instead of ∆or>∆/2
instead of 0(making an error of at least ∆/2on these coordinates).
The optimal hypothesis testing error can be lower bounded by Fano’s inequality as follows,
inf
ΨEθθθ∗∼Unif(∆Ck)[I(Ψ(Dn))̸=θθθ∗)]≥1−I(θθθ∗;Dn) + log(2)
log|∆Ck|(120)
For each parameter θθθ∈∆Ck, define the distribution,
Pθθθ(Dn) =n/productdisplay
i=11√
2πexp/parenleftbigg
−1
2σ2(yi−fθθθ(xi))2/parenrightbigg
×/parenleftbigg1
2d/parenrightbigg
. (121)
which captures the data distribution when the underlying ground truth parameter is θθθ. Note that Dn∼Pθθθ∗;
marginally xiis uniformly distributed on the hypercube and conditionally yiis normally distributed with
meanfθθθ∗(xi)and variance σ2. Note that the mutual information can be bounded as,
I(θθθ∗,Dn) = inf
QEθθθ∗∼Unif(∆Ck)[KL(Pθθθ∗∥Q)] (122)
≤Eθθθ∗∼Unif(∆Ck)[KL(Pθθθ∗∥P0)] (123)
whereP0is the distribution of Dnwhen the ground truth function fθθθin eq. (121) is chosen as 0everywhere.
Then,
KL(Pθθθ∗∥P0) =1
2σ2EDn∼Pθθθ∗/bracketleftiggn/summationdisplay
i=12yifθθθ∗(xi)−(g∗
θθθ(xi))2/bracketrightigg
(124)
=1
2σ2EDn∼Pθθθ∗/bracketleftiggn/summationdisplay
i=1(fθθθ∗(xi))2/bracketrightigg
(125)
=n
2σ2Exi∼Unif({±1}d)/bracketleftbig
(fθθθ∗(xi))2/bracketrightbig
(126)
29Published in Transactions on Machine Learning Research (April/2023)
where the last equation just uses the fact that in Pθθθ∗, the marginal distribution of xiis uniform. For each θθθ∗,
by Parseval’s theorem, Exi∼Unif({±1}d)/bracketleftbig
(fθθθ∗(xi))2/bracketrightbig
=k∆2. Overall, plugging into eq. (126) and subsequently
into eq. (123),
I(θθθ∗,Dn)≤Eθθθ∗∼Unif(∆Ck)[KL(Pθθθ∗∥P0)] =nk∆2
2σ2(127)
Putting these bounds into eq. (120) and subsequently into eq. (119) results in,
Eθθθ∗∼Unif(∆Ck)/bracketleftig
E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleˆθθθ−θθθ∗/vextenddouble/vextenddouble/vextenddouble
2/bracketrightig/bracketrightig
≥1
4/radicalbig
⌊k/2⌋∆/parenleftbigg
1−nk∆2/2σ2+ log(2)
log 2(d−1)⌊k/2⌋/parenrightbigg
(128)
≥1
4/radicalbig
⌊k/2⌋∆/parenleftbigg
1−10(nk∆2/σ2+ 1)
(d−1)⌊k/2⌋/parenrightbigg
(129)
Choosing ∆ = 8ϵ//radicalbig
⌊k/2⌋, forasufficientlylargeconstant C, wegetthatforanylearner, if n≤Cσ2d
∆2≍σ2kd
ϵ2,
Eθθθ∗∼Unif(∆Ck)/bracketleftig
E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleˆθθθ−θθθ∗/vextenddouble/vextenddouble/vextenddouble
2/bracketrightig/bracketrightig
≥ϵ≍σ/radicalbigg
kd
n. (130)
30