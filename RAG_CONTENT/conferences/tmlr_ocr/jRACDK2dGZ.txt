Under review as submission to TMLR
Sharpness-Aware Minimization Scaled by Outlier Normaliza-
tion for Improving Robustness on Noisy DNN Accelerators
Anonymous authors
Paper under double-blind review
Abstract
Energy-efficient deep neural network (DNN) accelerators are prone to non-idealities that de-
grade DNN performance at inference time. To mitigate such degradation, existing methods
typically add perturbations to the DNN weights during training to simulate inference on
noisyhardware. However, thisoftenrequiresknowledgeaboutthetargethardwareandleads
to a trade-off between DNN performance and robustness, decreasing the former to increase
the latter. In this work, we first show that applying sharpness-aware training, by optimiz-
ing for both the loss value and loss sharpness, significantly improves robustness to noisy
hardware at inference time without relying on any assumptions about the target hardware.
Then, we propose a new adaptive sharpness-aware method that conditions the worst-case
perturbation of a given weight not only on its magnitude but also on the range of the weight
distribution. This is achieved by performing sharpness-aware minimization scaled by outlier
minimization (SAMSON). Our extensive results on several models and datasets show that
SAMSON increases model robustness to noisy weights without compromising generalization
performance in noiseless regimes.
1 Introduction
The success of deep neural networks (DNNs) has been accompanied by an increase in training complexity and
computational demands, prompting efficient DNN designs (Zhao et al., 2023; Lebovitz et al., 2023). With
the slowing down of Moore’s law and the ending of Dennard scaling, power consumption is now the key
design constraint for DNN accelerators (Sze et al., 2020), which calls for new hardware and algorithms. In
particular, in-memory computing approaches (Le Gallo et al., 2018; Sebastian et al., 2020; Yin et al., 2020;
Sakr & Shanbhag, 2021) are promising directions to improve the energy consumption and throughput of
existingDNNsbycircumventingtheneedformemoryaccesses, whichrepresentanenergy-intensiveprocessin
conventional hardware implementations (Pedram et al., 2017). In-memory computing is especially important
for running DNNs on edge devices which usually possess low-power constraints (Gupta et al., 2023).
Despite being highly energy-efficient, in-memory computing solutions require performing computations in
the analog domain, which is inherently prone to variabilities (Xu et al., 2013). This leads to perturbations
in the DNN weights after deploying it in the target hardware, ultimately resulting in a degradation in per-
formance (Joshi et al., 2020; Kern et al., 2022; Spoon et al., 2021; Tambe et al., 2021). Hence, adopting such
energy-efficient hardware designs requires developing DNN methods that improve robustness to hardware
noise at inference time. The main approach for improving the hardware robustness of DNNs has been to
apply weight perturbations during training (Hacene et al., 2019; Chang et al., 2019; Gokmen et al., 2019;
Henwood et al., 2020; Joshi et al., 2020). However, these robustness methods provide a trade-off between
DNN performance and DNN robustness, decreasing the former to increase the latter. Moreover, such ap-
proaches typically rely on information about the hardware device that the DNN will be deployed to (at
inference time) to perform noise simulations at training time. However, this is not always possible and
presents a limitation on the type of target hardware that can be used for inference.
The goal of this work is to increase model robustness without decreasing DNN performance and without rely-
ing on any noise simulations from the target hardware. By doing so, we do not compromise the applicability
1Under review as submission to TMLR
of our approach, neither by reducing the original DNN performance nor by tailoring it to a specific hardware
design. To achieve this, we propose a novel sharpness-aware minimization method that is applied during
training to promote accurate DNN at inference time and after deployment on noisy, yet energy-efficient,
hardware.
The benefit of converging to a smoother loss landscape has been primarily tied to improving generalization
performance (Hochreiter & Schmidhuber, 1994; Keskar et al., 2016; Dziugaite & Roy, 2017; Neyshabur et al.,
2017; Chaudhari et al., 2017; Izmailov et al., 2018). With this goal in mind, Foret et al. (2021) recently
proposed sharpness-aware minimization (SAM) by minimizing both the loss value and loss sharpness within
a maximization region around each parameter during training. By showing a high correlation between loss
sharpness and test performance, SAM has ignited several follow-up works since its proposal. Particularly,
adaptive SAM (ASAM) (Kwon et al., 2021) reformulated sharpness to be invariant to weight scaling by
conditioning the neighborhood region of each weight based on its magnitude.
In this work, we propose to perform sharpness-aware minimization scaled by outlier normalization (SAM-
SON) to increase robustness in DNNs without compromising performance. SAMSON reformulates adaptive
sharpness to consider not only the weight magnitude but also the range of the weight distribution. By pro-
moting sharpness adaptivity based on the outlier weights, we show that SAMSON’s sharpness measure has
a high correlation with model robustness. This suggests that the minimization of such sharpness measure
by SAMSON’s objective during training is an effective way to promote robustness after training, i.e.during
inference. This is observed on a generic noise model on multiple DNN architectures and datasets as well as
on accurate noise simulations from real hardware. Overall, our results showcase the extensive practicality of
our approach by improving DNN robustness in noisy settings without affecting generalization performance
in noiseless regimes.
2 Related work
The deployment of pre-trained models on noisy hardware for highly efficient inference is known to introduce
non-idealities. This is caused by noise inherent to the device (Tsai et al., 2019) such as programming noise
after weight transfer to the target hardware and read noise every time the programmed weights are accessed.
Without robustness measures, such hardware noise significantly hinders the performance of neural networks.
To promote robustness after deployment in noisy hardware at inference time, existing methods typically
inject noise or faults to DNN weights during training (Ambrogio et al., 2018; Spoon et al., 2021; Li et al.,
2019; Ambrogio et al., 2019; Mackin et al., 2019). In particular, adding weight noise (Joshi et al., 2020) and
promoting redundancy by performing aggressive weight clipping (Stutz et al., 2021a; 2022) have been shown
to be effective methods for increasing DNN robustness (Chitsaz et al., 2023). However, existing robustness
methods often lead to a decrease of DNN performance for promoting robustness. Moreover, they typically
rely on noise measurements from the target hardware to improve the performance and robustness trade-off.
Here, we aim to increase DNN robustness in noisy settings without sacrificing DNN performance in the
noiseless regime without relying on any information about the target hardware.
Sharpness-aware training has recently gathered increased interest (Sun et al., 2021; Jiang et al., 2020; Foret
et al., 2021; Chen et al., 2022). Particularly, SAM has sparked a lot of new follow-up works due to the
significant increase in generalization performance presented in the original paper. Variants mainly focus
on increasing the efficiency (Du et al., 2022b;a; Zhou et al., 2022; Liu et al., 2022; Zhao et al., 2022),
performance (Zhuang et al., 2022; Kim et al., 2022; Kwon et al., 2021), or understanding (Andriushchenko
& Flammarion, 2022) of sharpness-aware training. Efforts have also been made to extend SAM to specific
use-cases such as quantization-aware training (Liu et al., 2021b) or data imbalance settings Liu et al. (2021a).
Several works (Kwon et al., 2021; Dinh et al., 2017) have also highlighted the importance of scale-invariant
sharpness measures, including in the context of model robustness against adversarial examples (Stutz et al.,
2021b).
In a similar vein to our work, Sun et al. (2021) recently related the sharpness of the loss landscape with
robustness to adversarial noise perturbations. This was further observed by Kim et al. (2022). Stutz et al.
(2021b) also recently studied the flatness of the (robust) loss landscape on the basis of adversarial training
withperturbedexamples(Madryetal.,2018). Inparticular, theytackletheproblemofrobustoverfitting(He
2Under review as submission to TMLR
et al., 2017), i.e.having high robustness to adversarial examples seen during training but generalizing poorly
to new adversarial examples at test time, through the lens of flat minima. Our work, on the other hand,
studies the relationship between the sharpness of the loss landscape and the robustness to hardware noise
perturbations.
Particularly, our goal is to promote hardware robustness by retaining as much performance as possible while
being exposed to hardware intrinsic variabilities that are reflected in terms of weight noise. This greatly
differs from the malicious attacks performed in the context of adversarial robustness (Madry et al., 2018),
even though both adversarial and hardware robustness share the common ground of degradation of the DNNs
parameters. In the context of hardware robustness, however, the noise injected on the DNN weights depends
on the variabilities of the target hardware to which the DNN is being deployed to. In other words, after
trainingaDNNonreliablehardware, westudyhowdeployingthatpre-trainedDNNtohighlyenergy-efficient
but noisy hardware would affect its performance at inference time.
3 Sharpness-aware minimization (SAM)
The goal of sharpness-aware minimization or SAM is to promote a smoother loss landscape by optimizing
for both the loss value and loss sharpness during training. Generally speaking, given a parameter w, the
goal is to find a region in the loss landscape where not only does whave a low training loss Lbut also do its
neighbor points. Considering the L2norm and discarding the regularization term in the original algorithm
for simplicity, SAM uses the following objective:
LSAM(w) = min
wmax
∥ϵ∥2≤ρL(w+ϵ), (1)
where the size of the neighborhood region is defined by a sphere with radius ρand the optimal ˆϵmay be
efficiently estimated via a first-order approximation, leading to:
ϵϵϵ∗
SAM(www) =ρ∇L(www)
||∇L(www)||2. (2)
By building on the strong correlation between sharpness and generalization performance, SAM is generally
used in practice to achieve better test performance. However, there are two main drawbacks. The first is
that, despite its efficiency in estimating the worst-case weight perturbations, SAM’s update requires two
backward passes. To mitigate this added complexity, the authors propose to leverage distributed training.
Another drawback of SAM is that the sharpness calculation is not independent from weight scaling. This
allows the manipulation of sharpness values by applying scaling operators to the weights such that weight
values change without altering the model’s final prediction (Dinh et al., 2017; Stutz et al., 2021b).
3.1 Adaptive SAM (ASAM)
To tackle the scale variance issue, adaptive sharpness-aware minimization or ASAM was proposed by Kwon
et al. (2021). By taking into account scaling operators that do not change the model’s loss, ASAM creates a
new notion of adaptive sharpness that is invariant to parameter scaling, contrarily to SAM. This is reflected
in ASAM’s objective:
LASAM (w) = min
wmax
∥ϵ/|w|∥2≤ρL(w+ϵ), (3)
where|w|represents the absolute value of a given weight w. With ASAM, different neighborhood sizes
are applied to different weights, depending on their magnitude; high-magnitude weights withstand higher
perturbations than low-magnitude weights. This adaptive sharpness formulation also leads to a change in
the neighborhood shape, which is now ellipsoidal instead of spherical. The worst-case perturbation ϵϵϵ∗
ASAM
is defined as
ϵϵϵ∗
ASAM (www) =ρw2∇L(w)
||w∇L(w)||2.(elementwise ops.) (4)
In practice, the adaptive sharpness that ASAM introduced shows a higher correlation with generalization
performance and overall improved convergence by using larger maximization regions for larger weights.
3Under review as submission to TMLR
4 Sharpness-aware minimization scaled by outlier normalization (SAMSON)
In this work, we propose a novel sharpness- and range-aware method called sharpness-aware minimization
scaled by outlier normalization or SAMSON. In essence, our approach considers not only the weight magni-
tudebutalsotherangeoftheweightdistributiontodeterminetheperturbation ϵofaweightw. Conditioning
sharpness by weight magnitude and the dynamic range of the weight distribution leads to the neighborhood
sizes being normalized across all layers. This is particularly important when training with batch normal-
ization, since the scales of the weight distributions across different layers may greatly differ leading to a
discrepancy in the applied weight perturbations across the entire network.
We propose to take into account the outlier weight, i.e.the maximum absolute weight of a given layer, by
simply scaling the effective neighborhood size of a weight wby thep-norm of all the weights wwwin a given
layer:
LSAMSON (w) = min
wmax
∥ϵ∥www∥p/|w|∥2≤ρL(w+ϵ), (5)
which leads to the following per-weight worst-case perturbation:
ϵϵϵ∗
SAMSON (www) =ρ(w∥www∥−1
p)2∇L(w)
||w∥www∥−1p∇L(w)||2.(elementwise ops.) (6)
We note that the p-norm affects the impact of outlier weights in the applied worst-case perturbation. This
differs from the norm ablations in Foret et al. (2021), where different norms are used to define the fixed
(non-adaptive) neighborhood regions of all weights, with ℓ2-norm performing the best in practice. Without
changing this default ℓ2-norm, our method uses different norms to control the importance of the outlier
weights in the adaptive neighborhood region of each weight. In our study, we experiment with using p=
{2,∞}. For ease of presentation, we often refer to the variants with p= 2andp=∞as SAMSON 2and
SAMSON∞, respectively, throughout the paper.
0.3
 0.2
 0.1
 0.0 0.1 0.2 0.3
w0.000.020.040.060.080.100.12*
SAMSON2SAMSON
 SAM ASAM
Figure 1: Worst-case perturbations of SAMSON, ASAM, and SAM. Each vertical, dotted line represents a
different weight range [- c,c], withc∈{0.05,0.1,0.15,0.2}.
We illustrate the applied worst-case perturbation considering a given weight value with SAMSON, ASAM,
and ASAM in Fig. 1, assuming ∇L(w) = 1for simplicity. To showcase that our method is adaptive not
only to the weight magnitude but also to the weight range, we apply different symmetric ranges to the
original weight distribution. For the purpose of this illustration, we generate weight values between -0.3
and 0.3 using an interval of 0.005 and represent different weight clipping ranges ( ±0.2,±0.15,±0.1,and
±0.05) by vertical lines with different colors. We re-use the color to plot the effect that a given range has
on the worst-case perturbations in SAMSON. We observe that ϵ∗
SAMis independent of the weight value and
range, being represented as a straight line that is defined solely by ρ. Sinceϵ∗
ASAMdepends on ρand the
4Under review as submission to TMLR
weight magnitude, larger weights are more perturbed. However, since ASAM is independent of the weight
range, there is no change in ASAM’s perturbations when changing the range of the weight distribution. On
the other hand, SAMSON is both range- and weight magnitude-dependent, taking into account the weight
value,ρ, and outlier weights for its perturbations. This results in the observed changes in ϵ∗
SAMSON over
the different ranges, with SAMSON 2putting less emphasis on outlier weights and SAMSON ∞emphasizing
them.
Despite not depending on any form of weight clipping, SAMSON is inherently suited to be used in combina-
tion with methods that restrict the weight distribution range. For example, training with aggressive weight
clipping (Stutz et al., 2021a) to improve robustness at inference time. When applying weight clipping, cis
the clipping range. With aggressive weight clipping, the weights are forced to be inside a small range to
promote robustness: i.e.c∈R: 0< c < 1. A pseudo-code implementation of SAMSON combined with
aggressive weight clipping is presented in Algorithm 1.
Algorithm 1 SAMSON combined with weight clipping.
Require: initial weight w0w0w0, aggressive clipping range c, learning rate α, neighborhood size ρ, normp
www←w0w0w0
whilenot converged do
Sample minibatch s
ϵϵϵ=ρ(w∥www∥−1
p)2∇Ls(w)
||w∥www∥−1
p∇Ls(w)||2▷elementwise ops.
www←www−α∇Ls(www+ϵϵϵ) ▷weight update
www←clip(www,c) ▷weight clipping (optional)
end while
5 Generalization performance
Before studying DNN robustness, we first analyze if using SAMSON negatively impacts generalization per-
formance in the noiseless setting. Since this is the most common setting in practice, such a performance
decrease would significantly reduce the applicability of our method. To test this, we first analyze the gen-
eralization performance of SAMSON, ASAM, SAM, and SGD on ResNet-34 (He et al., 2016), ResNet-50,
MobileNetV2 (Sandler et al., 2018), VGG-13 (Simonyan & Zisserman, 2014), and DenseNet-40 (Huang et al.,
2017) models trained on CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). All models were trained
for 200 epochs with a batch size of 128, starting with a learning rate of 0.1 and dividing it by 10 every 50
epochs.
We used the default neighborhood sizes for SAM and ASAM, as proposed in their original papers: we set
ρ= 0.05andρ= 0.5for SAM and ASAM on CIFAR-10, respectively, and ρ= 0.1andρ= 1.0for SAM and
ASAM on CIFAR-100, respectively. For a direct method comparison, we report the results using the same
defaultρas ASAM for our method variants. When applicable, we report the mean and standard deviation
over 3 runs. Additional details are presented in Appendix A.
The test accuracy comparisons between the different methods on CIFAR-10 and CIFAR-100 are shown in
Tables 1 and 2, respectively. Overall, we observe that SAMSON does not lead to a decrease in generalization
performance. and, in most cases, at least one of our variants (SAMSON 2or SAMSON∞) even shows slight
improvementsoverSGD,SAM,andASAMintermsoftestaccuracy, withthebestperforming pbeingdataset
and architecture dependent. The only instance where a SAMSON variant is not the best-performing method
is when using DenseNet-40 trained on CIFAR-10. However, both of our variants are within the standard
deviation of ASAM, rendering the difference between the method performances statistically insignificant.
To further expand our exploration of models, datasets, and training settings, we finetuned a ResNet-18
model on ImageNet (Russakovsky et al., 2015) provided by PyTorch for a total of 10 epochs using SGD
with momentum (0.9), a batch size of 400, a learning rate of 0.001, and a weight decay of 0.0001. Since
no defaultρis reported in the original ASAM’s paper for finetuning on ImageNet, we iterate over different
neighborhood ranges (details are provided in Appendix D) and report the best performing ρfor SAM,
5Under review as submission to TMLR
Table 1: Generalization performance (test accuracy %) of the different methods on several models trained
on CIFAR-10.
Method ResNet-34 ResNet-50 MobileNetV2 VGG-13 DenseNet-40
SGD 95.84±0.13 94.36±0.09 94.62±0.06 94.19±0.04 91.76±0.11
SAM 95.80±0.07 94.24±0.13 94.91±0.07 94.52±0.07 92.27±0.30
ASAM 95.85±0.22 94.42±0.57 95.37±0.04 94.68±0.07 92.57±0.06 92.57±0.06 92.57±0.06
SAMSON 2 95.96±0.34 95.96±0.34 95.96±0.34 95.09±0.21 95.09±0.21 95.09±0.21 95.29±0.17 94.73±0.12 94.73±0.12 94.73±0.12 92.54±0.14
SAMSON∞ 95.76±0.29 94.94±0.09 94.94±0.09 94.94±0.09 95.41±0.09 95.41±0.09 95.41±0.09 94.66±0.02 92.49±0.13
Table 2: Generalization performance (test accuracy %) of the different methods on several models trained
on CIFAR-100.
Method ResNet-34 ResNet-50 MobileNetV2 VGG-13 DenseNet-40
SGD 74.32±1.32 74.35±1.23 75.44±0.07 72.78±0.22 68.52±0.25
SAM 75.62±0.33 75.36±0.01 76.81±0.18 73.86±0.40 69.14±0.36
ASAM 76.91±0.44 77.88±0.85 77.28±0.10 74.12±0.01 70.21±0.25
SAMSON 2 77.68±0.57 77.68±0.57 77.68±0.57 78.22±0.67 78.22±0.67 78.22±0.67 77.24±0.13 74.77±0.23 74.77±0.23 74.77±0.23 69.94±0.36
SAMSON∞ 77.60±0.78 77.60±0.78 77.60±0.78 77.81±1.32 77.61±0.23 77.61±0.23 77.61±0.23 74.59±0.15 74.59±0.15 74.59±0.15 70.34±0.37 70.34±0.37 70.34±0.37
ASAM, and SAMSON. In the end, the best performances were obtained using ρ= 0.05for SAM,ρ= 0.2for
SAMSON, and ρ= 0.5for ASAM. Moreover, we also trained ResNet-18 and MobileNetV3 (Howard et al.,
2019) models from scratch for 90 epochs using the same setup but with a learning rate of 0.1 decayed by 10
every 30 epochs. Results are presented in Table 3.
Table 3: Generalization performance (test accuracy %) of the different methods with ResNet-18 and Mo-
bileNetV3 on ImageNet.
Finetuned Trained from scratch
Method ResNet-18 ResNet-18 MobileNetV3
top-1 top-5 top-1 top-5 top-1 top-5
SGD 69.758 89.078 69.91±.0489.21±.0569.30±.0189.01±.01
SAM 70.356 89.480 70.01±.0689.28±.0669.32±.0288.89±.02
ASAM 70.348 89.428 70.15±.0689.24±.0769.57±.0888.90±.06
SAMSON 270.35870.35870.358 89.48689.48689.486 70.16±.08 70.16±.08 70.16±.0889.38±.10 89.38±.10 89.38±.1069.62±.01 69.62±.01 69.62±.0189.14±.01 89.14±.01 89.14±.01
SAMSON∞70.36670.36670.366 89.50489.50489.504 70.23±.06 70.23±.06 70.23±.0689.35±.05 89.35±.05 89.35±.0569.57±.0388.99±.03
Once again, we observe that our variants do not degrade generalization performance, showing slight improve-
ments over the compared methods when both fine-tuning or training from scratch. These results highlight
theefficacyofourapproachinachievingmorerobustDNNs(aswillbediscussedinthenextsections)without
degrading generalization performance in several training settings.
5.1 Large-scale Transformer models
To further assess SAMSON’s ability to achieve competitive generalization performance in larger models, we
present additional results using Transformers (Vaswani et al., 2017). Moreover, we also extend our covered
tasks to include machine translation. As the Adam optimizer (Kingma & Ba, 2015) is the common practice
in Transformers for text, we combined our method with Adam for the machine translation setup, further
extending our analysis to applying SAMSON to additional optimizers on top of SGD.
6Under review as submission to TMLR
For image classification, we finetuned a vision Transformer (ViT) model (Dosovitskiy et al., 2021), particu-
larly a ViT-Base model (86.6M parameters) with 16×16image patches, referred to as ViT-B-16, that was
pre-trained on ImageNet-21K (Ridnik et al., 2021) and finetuned with SGD on ImageNet-1K resulting in a
top-1 validation accuracy of 81.79%. In our experiments, we used the different sharpness-aware minimization
methods to fine-tune this model for an additional 15 epochs. We used the same setup as previously described
for ImageNet (including the previously presented ρfor each method) except for the batch size which was
set to 480 to maximize resource utilization. Results are presented in Table 4a. We observe that both of
our method variants improve the generalization performance of the pre-trained model. On the other hand,
ASAM was unable to recover the generalization performance of the original model, whereas SAM obtained
a small improvement. Importantly, we see that both SAMSON variants outperform all the other methods.
For machine translation, we replicated the setup originally presented in ASAM (Kwon et al., 2021) and used
an encoder-decoder, 12-layer Transformer (39.4M parameters) trained from scratch on IWSLT’14. Similarly
to Kwon et al. (2021), we conducted a search over ρfor our variants using the validation set (the range is
presented in Appendix D), with ρ= 0.5performing the best for both of our variants. For SAM and ASAM
we used the best ρreported in Kwon et al. (2021). Results obtained over 3 seeds are shown in Table 4b. We
observe that both of our method variants outperform all previous methods when evaluating the validation
BLEU score. Moreover, SAMSON ∞outperforms all methods based on BLEU score using the test set.
Overall, these results indicate that SAMSON can be successfully applied to achieve high generalization
performance with large-scale Transformer models on image and language tasks.
Table 4: Generalization performance (validation accuracy % and validation and test BLEU scores) of the
different methods with ViT-B-16 on finetuned ImageNet and an encoder-decoder Transformer trained on
IWSLT’14. For IWSLT’14, the reported results (∗) for Adam, Adam+SAM, and Adam+ASAM are taken
from Kwon et al. (2021).
Finetuned
Method ViT-B-16
val. accuracy
SGD 81.79
SAM 81.80
ASAM 81.77
SAMSON 2 81.8381.8381.83
SAMSON∞ 81.8281.8281.82
(a) ImageNet.Trained from scratch
Method Transformer
val. BLEU test BLEU
Adam 35.34∗
±<.01 34.86∗
±<.01
Adam+SAM 35.52∗
±.01 34.78∗
±.01
Adam+ASAM 35.66∗
±<.01 35.02∗
±<.01
Adam+SAMSON 2 35.70±<.01 35.70±<.01 35.70±<.01 34.94±<.01
Adam+SAMSON ∞ 35.73±<.01 35.73±<.01 35.73±<.01 35.06±<.01 35.06±<.01 35.06±<.01
(b) IWSLT’14.
6 Model robustness to noisy weights
We will now focus on analyzing how sharpness-aware training promotes DNN robustness compared to stan-
dard SGD training. In particular, we will focus on improving robustness in the context of noisy hardware
accelerators that exploit the energy-reliability trade-off to improve energy efficiency at the cost of noisy
weights. As our use-case, we consider memristor-based DNN implementations, which present a promising
direction in energy-efficient DNN inference accelerators (Joshi et al., 2020; Kern et al., 2022). In such a
setting, the weights of all fully-connected or convolutional layers of a pre-trained DNN are linearly mapped
to the range of possible conductance values from 0 to Gmax. More concretely, the ideal conductance values
Gl
T,ijfor the weights Wl
ijof layerlare
Gl
T,ij=Wl
ij×Gmax
Wlmax, (7)
whereWl
maxislayerl’smaximumabsoluteweight. However, aspointedoutpreviously, Gl
T,ijisnotachievable
in practice since conductance errors δijare originated from programming and read noise (Tsai et al., 2019)
7Under review as submission to TMLR
as well as conductance drift over time (Ambrogio et al., 2019). Hence, in the general case, the non-ideal
conductance values Gl
ijmay be defined as
Gl
ij=Gl
T,ij×δij, (8)
withδij∼N (1,σ2
c). Following Joshi et al. (2020), σcrepresents the conductance variation of Gl
ijrelative
toGl
T,ij. This generic noise model may be used to accurately estimate inference accuracy in noise models
derived from measurements of existing noisy hardware implementations.
We tested robustness in a variety of networks – VGG-13 trained on CIFAR-10, MobileNetV2 trained on
CIFAR-100, and ResNet-18 finetuned on ImageNet – following the same training procedure describe above.
We tried a range of neighborhood sizes for the various methods since different a ρprovides a distinct trade-off
between performance and robustness. Additional details are provided in Appendix D. Overall, we found that
ρ= 0.5orρ= 1.0tend to provide the best trade-offs for both SAMSON and ASAM and ρ= 0.05orρ= 0.1
for SAM. To promote a cleaner visualization, we only report the best ρfor each method. Lastly, we note
thatσc= 0.0in our experiments refers to the special case where no noise is applied to the DNN weights.
6.1 Baseline robustness methods
On top of a simple baseline trained with vanilla SGD, we experimented with two methods: the additive
noise approach proposed by Joshi et al. (2020) and aggressive weight clipping (Stutz et al., 2021a). More
specifically, the first method applies additive Gaussian noise to DNN weights, whereas the second method
clips the DNN weights into a small range of possible values. The models are trained from scratch and use
the training settings previously described.
The additive random noise proposed by Joshi et al. (2020) is sampled from a Gaussian distribution N(0,σ2
n),
where
σn=Wl
max×σG
Gmax, (9)
withσGrepresenting the standard deviation of hardware non-idealities observed in practice. Both σGand
Gmaxaredevicecharacteristicsthataresetto 0.94and25, respectively, followingtheempiricalmeasurements
on 1 million of phase-change memory devices (Joshi et al., 2020). Since the amount of added noise is
proportional to the maximum absolute weight value of a given layer, we perform weight clipping after each
weight update; we used the range/bracketleftbig
−α×σWl,α×σWl/bracketrightbig
, whereσWlis the standard deviation of the weights of
layerlandαisapredefinedhyper-parameterdefaultedto 2.0. Wetriedadifferentrangeof α∈{1.5,2.0,2.5},
but the best performance for all CIFAR-10/100 models was achieved with the default αvalue of 2.0. For
finetuning on ImageNet, we used α= 2.5, as originally suggested (Joshi et al., 2020).
For aggressive weight clipping, we tried the values for the clipping range c, as performed by the original
authors (Stutz et al., 2021a): {±0.05,±0.10,±0.15,±0.20}. A lower weight range induced by a smaller c
leads to highly robust networks. However, they may lack generalization performance in the noiseless to low-
noise regimes due to outlier distortion. Hence, manipulating cprovides a trade-off between performance and
robustness. In our experiments, we observed that 0.2 (and in some cases 0.15) achieved the best trade-off
and was used on most of the reported networks. Please see Appendix D for additional details.
To reduce the impact of hardware non-idealities in the DNN performance, Joshi et al. (2020) also proposed
adaptive batch normalization statistics (AdaBS), which updates the batch normalization statistics using a
calibration set. More specifically, the running mean and running variance of all batch normalization layers
are updated using the statistics computed during inference on a calibration set using noisy weights. We used
the originally suggested hyper-parameters and applied AdaBS to all networks.
6.2 Robustness to different conductance variation
The robustness of the models trained with SAMSON, ASAM, SAM, and SGD in combination with aggressive
weight clipping at different conductance variation levels is shown in Fig. 2. We also include training with
SGD and additive Gaussian noise as an additional baseline. For visualization clarity, we include training
with additive noise on top of the sharpness-aware training variants in Appendix B.
8Under review as submission to TMLR
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
91929394test accuracy (%)
VGG-13 (CIFAR-10)
93.7594.0094.2594.5094.75
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
505560657075test accuracy (%)
MobileNetV2 (CIFAR-100)
757677
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
455055606570test accuracy (%)
ResNet-18 (ImageNet)
69.570.0
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
455055606570test accuracy (%)ResNet-18 (ImageNet)
ASAM 
ASAM + clippingSAM 
SAM + clippingSAMSON (p=2) 
SAMSON (p=2) + clippingSAMSON (p= ) 
SAMSON (p= ) + clipping
SGD
SGD + clippingSGD + noise
Figure 2: Performance of the different methods under a range of random conductance variations. We plot
the mean and standard deviation over 10 and 3 inference runs for CIFAR-10/100 and ImageNet, respectively.
We observe that SAMSON variants primarily compose the Pareto frontier across all models and datasets.
Ultimately, this means that training a DNN with SAMSON with and without aggressive weight clipping
provides the best performance and robustness trade-off across all noisy regimes. This is also observed in the
noiseless regime ( σc= 0.0), where we see that there is always at least one SAMSON variant that achieves
the best test accuracy, as discussed in section 5. The difference in model robustness between the various
methods is more subtle on ImageNet, likely due to all methods starting with the same pre-trained model and
being only finetuned for 10 epochs. Nevertheless, we see that SAMSON is the only method able to provide
significant improvements in terms of robustness in highly noisy regimes, e.g.σc= 0.4.
Overall, we observe that sharpness-aware training variants (SAMSON, ASAM, and SAM) clearly outperform
SGD, with SAMSON promoting the highest robustness, generally followed by ASAM and then SAM. This
is seen in terms of not only robustness at different noise levels but also in the best performances achieved
in the noiseless regime. Moreover, the improvement in robustness is especially amplified when combining
sharpness-aware methods with aggressive weight clipping, representing a simple yet effective alternative to
training with noise. We note that, as expected, the performance on the clean network drops when applying
both weight clipping or additive noise, as observed in the zoomed-in patches. This mitigates the robustness
benefits while using these methods in lower noisy settings but proves to be remarkably beneficial in highly
noisy regimes.
6.2.1 Large-scale Transformer models
We further assessed the robustness of different conductance variations in our encoder-decoder Transformer
model by applying noise to all the fully-connected layers. In this setting, we compare the different methods
in combination with the Adam optimizer instead of SGD. The robustness results without aggressive weight
clipping are presented in Fig. 3 (left). We observe that SAMSON ∞achieves the best robustness out of all the
comparedmethodsacrossallnoisyregimes. Importantly, SAMSON 2alsoachievesthesecond-bestrobustness
in low to mid-range noisy settings, which are often the desirable noise ranges due to the maintenance of an
adequate model performance for real-world tasks. Finally, in the zoomed-in patch, we see that both of our
variants achieve the best generalization performance in the noiseless regime. (We note that we were unable
to replicate the exact SAM and ASAM results reported in the original ASAM paper (Kwon et al., 2021)
which explains the subtle difference in test BLEU scores presented here and Table 4b.)
We also experimented with applying aggressive weight clipping on this new model and task. Results are
presented in Fig. 3 (right). In this setting, we observe that aggressive weight clipping did not help with
increasing model robustness, but increased generalization performance. We hypothesize that this suggests
that a smaller minimum and maximum clipping value may be used for this setting, instead of c=±0.2as
9Under review as submission to TMLR
used in the reported results and also throughout the paper for image classification tasks. Notwithstand-
ing, SAMSON 2is the most viable robustness method when all the noise range is considered. Moreover,
SAMSON∞is also among the top-performing methods in the low to mid-noise regime. Lastly, we observe
that once again both variants of our method achieve the best generalization performance in the noiseless
setting, as presented in the zoomed-in patch.
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
3132333435test BLEU score
Transformer (IWSLT 14)
34.535.0
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
3132333435test BLEU score
Transformer (IWSLT 14)
34.7535.0035.25
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
3132333435test BLEU score
Transformer (IWSLT 14)
Adam
Adam + clippingAdam+ASAM
Adam+ASAM + clippingAdam+SAM
Adam+SAM + clippingAdam+SAMSON (p=2)
Adam+SAMSON (p=2) + clippingAdam+SAMSON (p=)
Adam+SAMSON (p=) + clipping
Figure 3: Performance of the different methods under a range of random conductance variations on Trans-
former with (right) and without (left) aggressive weight clipping. We plot the mean over 3 inference runs.
6.3 Sharpness and robustness correlation
For measuring sharpness, we use the m-sharpness metric proposed by Foret et al. (2021), which stems from
the original SAM formulation (Eq. (1)), and further extend it to SAMSON’s objective (Eq. (5)). Considering
a training set ( Strain) composed of nminibatches Sof sizem, we compute the difference of the loss lsof a
given sample swith and without a worst-case perturbation ϵonw. SAMSON’s m-sharpness is calculated as
1
n/summationdisplay
S∈Strainmax
∥ϵ∥www∥−1
p/|w|∥2≤ρ1
m/summationdisplay
s∈Sls(w+ϵ)−ls(w). (10)
In our experiments, we used m= 400andm= 128for measuring the sharpness of models finetuned on
ImageNet and trained on CIFAR-10/100, respectively.
We treat robustness as the performance gap measured by the difference in test accuracy between the noise-
less models, i.e.with no conductance variation applied to the weights ( σc= 0.0), and the noisy model
configurations with the highest tested conductance variation ( σc= 0.4). We present the relation between
sharpness and robustness of all the tested models using SAMSON’s m-sharpness with p= 2in Fig. 4. We
observe a strong correlation within each training configuration, i.e.training each method with and without
additive noise or aggressive weight clipping, across all architectures and datasets.
We provide visualizations of m-sharpness as calculated using SAMSON ∞, SAM and ASAM’s objectives and
the metric proposed by Keskar et al. (2016) in Appendix E. We observe that SAMSON ∞’sm-sharpness
also shows a high correlation compared to the compared methods. Such findings showcase the ability of
SAMSON’s m-sharpness in acting as a generic robustness metric. Importantly, this suggests that training
with SAMSON’s objective, especially when combined with existing robustness methods such as aggressive
weight clipping, is an effective way of promoting more robust DNNs at inference time.
6.4 Robustness to noise from real hardware
To convey how the performance on the generic noise model translates to existing hardware implementations,
we performed experiments using an inference simulator on real hardware provided by IBM’s analog hardware
10Under review as submission to TMLR
0.001 0.002 0.003
sharpness1.01.52.02.53.03.54.0performance gap (%)VGG-13 (CIFAR-10)
0.004 0.006 0.008 0.010 0.012
sharpness10152025performance gap (%)MobileNetV2 (CIFAR-100)
0.002 0.003 0.004 0.005
sharpness14161820222426performance gap (%)ResNet-18 (ImageNet)
0.04
 0.02
 0.00 0.02 0.040.04
0.02
0.000.020.04normal weight clipping additive noise
Figure 4: Correlation between SAMSON 2’sm-sharpness (Eq. (10), ρ= 0.5,p= 2) and robustness, i.e.the
performance gap between the noise realizations at σc= 0.0and atσc= 0.4. We plot the mean and standard
deviation over 10 and 3 inference runs for CIFAR-10/100 and ImageNet, respectively.
acceleration kit (Rasch et al., 2021). This simulator uses the empirical measurements from 1 million phase-
change memory devices (Nandakumar et al., 2019) to accurately simulate how hardware noise affects the
DNN weights (Joshi et al., 2020). Specifically, by taking into account the programming and read noise, we
report the performance of the different methods combined with aggressive weight clipping measured 1 year
after deployment on the target hardware in Fig. 5. We observe that even though all sharpness-aware training
methodsoutperformSGDintermsofrobustness, theSAMSONvariantsretainthemostperformance. Thisis
important in scenarios where often reprogramming the DNN weights on the memristor device is not feasible.
SAMSON
SAMSON2 SAM ASAM SGD62646668707274test accuracy (%)69.4269.88 70.05 70.02 69.92
65.31 65.20 65.1364.79
63.55ResNet-18 (ImageNet)
After training
1 year after weight transfer
Figure 5: Performance of the different methods with aggressive weight clipping on ResNet-18 finetuned on
ImageNet 1 year after weight transfer to the target hardware. We plot the mean and standard deviation
over 10 inference runs.
7 Conclusion
In this work, we propose a new adaptive sharpness-aware training method that conditions the individual
worst-case perturbation of a given weight based on not only its absolute value but also on the weight range
distribution of a particular layer. Our results on different architectures, datasets, training regimes, and
noisy scenarios showcase the benefits of using SAMSON to increase DNN robustness without compromising
DNN performance in noiseless settings. One limitation of SAMSON which stems directly from SAM is the
increase in training complexity. Notwithstanding, our approach may be combined with existing efficient
SAM implementations (Du et al., 2022a; Liu et al., 2022) to further mitigate this issue.
11Under review as submission to TMLR
References
S. Ambrogio, M. Gallot, K. Spoon, H. Tsai, C. Mackin, M. Wesson, S. Kariyappa, P. Narayanan, C.-C. Liu,
A. Kumar, A. Chen, and G. W. Burr. Reducing the impact of phase-change memory conductance drift on
the inference of large-scale hardware neural networks. In International Electron Devices Meeting , 2019.
Stefano Ambrogio, Pritish Narayanan, Hsinyu Tsai, Robert M Shelby, Irem Boybat, Carmelo Di Nolfo,
Severin Sidler, Massimo Giordano, Martina Bodini, Nathan CP Farinha, et al. Equivalent-accuracy accel-
erated neural-network training using analogue memory. Nature, 2018.
Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization.
InInternational Conference on Machine Learning , 2022.
H.-Y. Chang, P. Narayanan, S. C. Lewis, N. C. P. Farinha, K. Hosokawa, C. Mackin, H. Tsai, S. Ambrogio,
A. Chen, and G. W. Burr. AI hardware acceleration with analog memory: Microarchitectures for low
energy at high speed. IBM Journal of Research and Development , 2019.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jen-
nifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient descent into wide
valleys. In International Conference on Learning Representations , 2017.
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision Transformers outperform ResNets without
pre-training or strong data augmentations. In International Conference on Learning Representations ,
2022.
Kamran Chitsaz, Goncalo Mordido, Jean-Pierre David, and François Leduc-Primeau. Training DNNs re-
silient to adversarial and random bit-flips by learning quantization ranges. Transactions on Machine
Learning Research , 2023.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep
nets. InInternational Conference on Machine Learning , 2017.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil
Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International
Conference on Learning Representations , 2021.
Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent
Tan. Efficient sharpness-aware minimization for improved training of neural networks. In International
Conference on Learning Representations , 2022a.
Jiawei Du, Daquan Zhou, Jiashi Feng, Vincent Tan, and Joey Tianyi Zhou. Sharpness-aware training for
free.Advances in Neural Information Processing Systems , 2022b.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep
(stochastic)neural networks withmany more parameters thantraining data. In Conference on Uncertainty
in Artificial Intelligence , 2017.
Mojtaba Faramarzi et al. PatchUp: A Feature-Space Block-Level Regularization Technique for CNNs. In
AAAI, 2022.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for
efficiently improving generalization. In International Conference on Learning Representations , 2021.
Tayfun Gokmen, Malte J. Rasch, and Wilfried Haensch. The marriage of training and inference for scaled
deep learning analog hardware. In International Electron Devices Meeting , 2019.
Kartik Gupta, Marios Fournarakis, Matthias Reisser, Christos Louizos, and Markus Nagel. Quantization ro-
bust federated learning for efficient inference on heterogeneous devices. Transactions on Machine Learning
Research , 2023.
12Under review as submission to TMLR
Ghouthi Boukli Hacene, François Leduc-Primeau, Amal Ben Soussia, Vincent Gripon, and François Gagnon.
Training modern deep neural networks for memory-fault robustness. In International Symposium on
Circuits and Systems , 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Conference on Computer Vision and Pattern Recognition , 2016.
Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example defense:
Ensembles of weak defenses are not strong. In USENIX Workshop on Offensive Technologies , 2017.
Sébastien Henwood, François Leduc-Primeau, and Yvon Savaria. Layerwise noise maximisation to train low-
energy deep neural networks. In International Conference on Artificial Intelligence Circuits and Systems ,
2020.
Sepp Hochreiter and Jürgen Schmidhuber. Simplifying neural nets by discovering flat minima. Advances in
Neural Information Processing Systems , 1994.
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,
Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for MobileNetv3. In IEEE/CVF Interna-
tional Conference on Computer Vision , 2019.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolu-
tional networks. In IEEE Conference on Computer Vision and Pattern Recognition , 2017.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Aver-
aging weights leads to wider optima and better generalization. In Conference on Uncertainty in Artificial
Intelligence , 2018.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic gener-
alization measures and where to find them. In International Conference on Learning Representations ,
2020.
Vinay Joshi, Manuel Le Gallo, Simon Haefeli, Irem Boybat, Sasidharan Rajalekshmi Nandakumar,
Christophe Piveteau, Martino Dazzi, Bipin Rajendran, Abu Sebastian, and Evangelos Eleftheriou. Accu-
rate deep neural network inference using computational phase-change memory. Nature Communications ,
2020.
Jonathan Kern, Sébastien Henwood, Gonçalo Mordido, Elsa Dupraz, Abdeldjalil Aïssa-El-Bey, Yvon Savaria,
and François Leduc-Primeau. MemSE: Fast MSE prediction for noisy memristor-based DNN accelerators.
InInternational Conference on Artificial Intelligence Circuits and Systems , 2022.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
On large-batch training for deep learning: Generalization gap and sharp minima. In International Con-
ference on Learning Representations , 2016.
Minyoung Kim, Da Li, Shell X Hu, and Timothy Hospedales. Fisher SAM: Information geometry and
sharpness aware minimisation. In International Conference on Machine Learning , 2022.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images., 2009.
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: Adaptive sharpness-aware
minimization for scale-invariant learning of deep neural networks. In International Conference on Machine
Learning , 2021.
Manuel Le Gallo, Abu Sebastian, Giovanni Cherubini, Heiner Giefers, and Evangelos Eleftheriou. Com-
pressed sensing with approximate message passing using in-memory computing. Transactions on Electron
Devices, 2018.
13Under review as submission to TMLR
Luzian Lebovitz, Lukas Cavigelli, Michele Magno, and Lorenz K Muller. Efficient inference with model
cascades. Transactions on Machine Learning Research , 2023.
Can Li, Zhongrui Wang, Mingyi Rao, Daniel Belkin, Wenhao Song, Hao Jiang, Peng Yan, Yunning Li, Peng
Lin, Miao Hu, et al. Long short-term memory networks in memristor crossbar arrays. Nature Machine
Intelligence , 2019.
Hong Liu, Jeff Z. HaoChen, Adrien Gaidon, and Tengyu Ma. Self-supervised learning is more robust to
dataset imbalance. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applica-
tions, 2021a.
Jing Liu, Jianfei Cai, and Bohan Zhuang. Sharpness-aware quantization for deep neural networks. arXiv
preprint arXiv:2111.12273 , 2021b.
YongLiu, SiqiMai, XiangningChen, Cho-JuiHsieh, andYangYou. Towardsefficientandscalablesharpness-
aware minimization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022.
Charles Mackin, Hsinyu Tsai, Stefano Ambrogio, Pritish Narayanan, An Chen, and Geoffrey W Burr. Weight
programming in DNN analog hardware accelerators in the presence of NVM variability. Advanced Elec-
tronic Materials , 2019.
AleksanderMadry, AleksandarMakelov, LudwigSchmidt, DimitrisTsipras, andAdrianVladu. Towardsdeep
learning models resistant to adversarial attacks. In International Conference on Learning Representations ,
2018.
SR Nandakumar, Irem Boybat, Vinay Joshi, Christophe Piveteau, Manuel Le Gallo, Bipin Rajendran,
Abu Sebastian, and Evangelos Eleftheriou. Phase-change memory models for deep learning training and
inference. In International Conference on Electronics, Circuits and Systems , 2019.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in
deep learning. Advances in Neural Information Processing Systems , 2017.
Ardavan Pedram, Stephen Richardson, Mark Horowitz, Sameh Galal, and Shahar Kvatinsky. Dark memory
and accelerator-rich system optimization in the dark silicon era. IEEE Des. Test , 2017.
Malte J Rasch, Diego Moreda, Tayfun Gokmen, Manuel Le Gallo, Fabio Carta, Cindy Goldberg, Kaoutar
El Maghraoui, Abu Sebastian, and Vijay Narayanan. A flexible and fast PyTorch toolkit for simulating
training and inference on analog crossbar arrays. In International Conference on Artificial Intelligence
Circuits and Systems , 2021.
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik. ImageNet-21K pretraining for the masses. In
Neural Information Processing Systems Track on Datasets and Benchmarks , 2021.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge.
International Journal of Computer Vision , 2015.
Charbel Sakr and Naresh R. Shanbhag. Signal processing methods to enhance the energy efficiency of
in-memory computing architectures. IEEE Transactions on Signal Processing , 2021.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2:
Inverted residuals and linear bottlenecks. In Conference on Computer Vision and Pattern Recognition ,
2018.
Abu Sebastian, Manuel Le Gallo, Riduan Khaddam-Aljameh, and Evangelos Eleftheriou. Memory devices
and applications for in-memory computing. Nature Nanotechnol. , 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
14Under review as submission to TMLR
Katie Spoon, Hsinyu Tsai, An Chen, Malte J. Rasch, Stefano Ambrogio, Charles Mackin, Andrea Fasoli,
Alexander M. Friz, Pritish Narayanan, Milos Stanisavljevic, and Geoffrey W. Burr. Toward software-
equivalent accuracy on Transformer-based deep neural networks with analog memory devices. Frontiers
in Computational Neuroscience , 2021.
David Stutz, Nandhini Chandramoorthy, Matthias Hein, and Bernt Schiele. Bit error robustness for energy-
efficient DNN accelerators. Machine Learning and Systems , 2021a.
David Stutz, Matthias Hein, and Bernt Schiele. Relating adversarially robust generalization to flat minima.
InInternational Conference on Computer Vision , 2021b.
David Stutz, Nandhini Chandramoorthy, Matthias Hein, and Bernt Schiele. Random and adversarial bit
error robustness: Energy-efficient and secure DNN accelerators. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2022.
Xu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan Luo, and Liangyou Li. Exploring the vulnerability of
deep neural networks: A study of parameter corruption. AAAI Conference on Artificial Intelligence , 2021.
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural networks.
Synthesis Lectures on Computer Architecture , 2020.
Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh,
Paul Whatmough, Alexander M Rush, David Brooks, et al. EdgeBERT: Sentence-level energy optimiza-
tions for latency-aware multi-task NLP inference. International Symposium on Microarchitecture , 2021.
H. Tsai, S. Ambrogio, C. Mackin, P. Narayanan, R. M. Shelby, K. Rocki, A. Chen, and G. W. Burr.
Inference of long-short term memory networks at software-equivalent accuracy using 2.5M analog phase
change memory devices. In Symposium on VLSI Technology , 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems , 2017.
Cong Xu, Dimin Niu, Naveen Muralimanohar, Norman P. Jouppi, and Yuan Xie. Understanding the trade-
offs in multi-level cell ReRAM memory design. In Annual Design Automation Conference , 2013.
Shihui Yin, Zhewei Jiang, Minkyu Kim, Tushar Gupta, Mingoo Seok, and Jae-Sun Seo. Vesti: Energy-
efficient in-memory computing accelerator for deep neural networks. IEEE Trans. on Very Large Scale
Integr., 2020.
XingxuanZhang,RenzheXu,HanYu,HaoZou,andPengCui. Gradientnormawareminimizationseeksfirst-
order flatness and improves generalization. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2023.
Kang Zhao, Yijun Tan, Kai Han, Ting Hu, Hanting Chen, Tao Yuan, Yunhe Wang, and Jun Yao. Comple-
mentary sparsity: Accelerating sparse CNNs with high accuracy on general-purpose computing platforms.
Transactions on Machine Learning Research , 2023.
Yang Zhao, Hao Zhang, and Xiuyuan Hu. SS-SAM: Stochastic scheduled sharpness-aware minimization for
efficiently training deep neural networks. arXiv preprint arXiv:2203.09962 , 2022.
Wenxuan Zhou, Fangyu Liu, Huan Zhang, and Muhao Chen. Sharpness-aware minimization with dynamic
reweighting. In Findings of EMNLP , 2022.
JuntangZhuang, BoqingGong, LiangzheYuan, YinCui, HartwigAdam, NichaCDvornek, sekhartatikonda,
James s Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training. In
International Conference on Learning Representations , 2022.
15Under review as submission to TMLR
A Training details
We trained the CIFAR-10/100 models using one RTX8000 NVIDIA GPU and 1 CPU core, the ResNet-
18 models using one A100 40GB GPU and 6 CPU cores, and the Transformer models using one A100
80GB GPU and 6 CPU cores. For CIFAR-10/100, we used the architecture implementations in https:
//github.com/kuangliu/pytorch-cifar . For ImageNet, we used the ResNet-18 implementation provided
by PyTorch1. For ViT-B-16 finetuned on ImageNet, we used the checkpoint publicly available at https:
//huggingface.co/timm/vit_base_patch16_224.orig_in21k_ft_in1k . For machine translation, we used
the implementation in https://github.com/facebookresearch/fairseq .
B Additional robustness baselines
We also present the robustness results when combining the sharpness-aware training variants (SAM, ASAM,
and SAMSON) with additive Gaussian noise in Fig. 6. Even though we observe an increase in robustness
in certain configurations, training with aggressive weight clipping tends to provide the overall best trade-off
between performance and robustness compared to training with additive noise.
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
91929394test accuracy (%)
VGG-13 (CIFAR-10)
93.7594.0094.2594.5094.75
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
505560657075test accuracy (%)
MobileNetV2 (CIFAR-100)
757677
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
455055606570test accuracy (%)
ResNet-18 (ImageNet)
69.069.570.0
0.0 0.1 0.2 0.3 0.4
conductance variation (c)
455055606570test accuracy (%)ResNet-18 (ImageNet)
ASAM 
ASAM + clipping
ASAM + noiseSAM 
SAM + clipping
SAM + noiseSAMSON (p=2) 
SAMSON (p=2) + clipping
SAMSON (p=2) + noiseSAMSON (p= ) 
SAMSON (p= ) + clipping
SAMSON (p= ) + noise
SGD
SGD + clipping
SGD + noise
Figure 6: Performance of the different methods under a range of random conductance variations and com-
bined with either weight clipping or additive noise. We plot the mean and standard deviation over 10 and 3
inference runs for CIFAR-10/100 and ImageNet, respectively.
C Additional robustness settings
To further expand the applicability of SAMSON, we also test its efficacy in promoting model robustness
to out-of-distribution (OOD) examples and post-training quantization. Even though this falls outside the
scope of hardware robustness that has been discussed so far, SAMSON is a generic robustness method that
may be applied to different use cases such as robustness to OOD examples and quantization. We explore
these additional robustness settings below.
C.1 Out-of-distribution examples
To test robustness in OOD settings, we used 8 input perturbations as presented in Faramarzi et al. (2022),
covering multiplescale, shear, androtation transformations. Weappliedsuch perturbations to allpre-trained
CIFAR-10 and CIFAR-100 models reported in Section 5, resulting in 40 scenarios per dataset. We report
the number of times each method achieved the best test accuracy over 3 seeds and without any fine-tuning
1https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html
16Under review as submission to TMLR
Table 5: DenseNet-40 OOD experiments on CIFAR-10.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 78.81±0.6278.38±0.5379.49±0.93 79.25±0.38 80.33±0.82 80.33±0.82 80.33±0.82
rotate 40 56.86±0.7756.51±0.6957.51±1.53 57.33±1.73 58.73±0.94 58.73±0.94 58.73±0.94
shear 28.6 79.47±0.4980.22±0.6981.41±0.56 80.31±0.85 81.65±0.64 81.65±0.64 81.65±0.64
shear 57.3 54.49±0.2454.05±0.6956.08±1.33 54.89±1.82 56.69±0.34 56.69±0.34 56.69±0.34
zoom 120 69.95±1.8870.08±1.7970.08±1.65 71.49±1.97 71.49±1.97 71.49±1.97 70.64±1.40
zoom 140 39.18±2.6339.23±3.9338.65±1.61 39.47±0.34 39.47±0.34 39.47±0.34 39.17±1.13
zoom 60 69.91±0.5371.26±0.3771.84±1.43 72.72±0.47 72.72±0.47 72.72±0.47 72.60±0.74
zoom 80 85.53±0.3986.01±0.2786.48±0.24 86.75±0.39 87.08±0.47 87.08±0.47 87.08±0.47
in Fig. 7. We observe that our variants consistently outperform the existing methods on both datasets.
Individual results for each OOD scenario in terms of test accuracies for CIFAR-10 and CIFAR-100 are
shown in Tables 5 to 9 and Tables 10 to 14, respectively.
SGD SAM ASAMSAMSON2SAMSON
05101520Cnt. best top1 acc.CIFAR-10
CIFAR-100
Figure 7: OOD robustness across 40 scenarios per dataset.
C.2 Post-training quantization
To test robustness against post-training quantization (no fine-tuning), we used a linear quantization scheme
without quantizing the first layer in all models. Results over 3 seeds using pre-trained MobileNetV2 and
ResNet-18modelsarepresentedinFig.8. WhiletherobustnessofourvariantsissimilartoASAMatmedium
to high bit-width, SAMSON ∞retains the most performance at the lowest bit-width for both models.
4 5 6 7 8
quantization (# bits)627078test accuracy (%)
MobileNetV2 (CIFAR-100)
5 6 7 8
quantization (# bits)5055606570test accuracy (%)
ResNet-18 (ImageNet)
5.0 5.5 6.0 6.5 7.0 7.5 8.0
quantization (# bits)50.052.555.057.560.062.565.067.570.0test accuracy (%)ResNet-18 (ImageNet)
ASAM SAM SAMSON (p=2) SAMSON (p= ) 
 SGD
Figure 8: Robustness to quantization at different bit-widths.
D Hyper-parameter tuning
The considered ranges for the different hyper-parameters are presented in Table 15. The configurations with
the best performance and robustness trade-off for the models trained CIFAR-10, CIFAR-100, and ImageNet
17Under review as submission to TMLR
Table 6: MobileNetV2 OOD experiments on CIFAR-10.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 86.89±0.4587.02±0.0488.21±0.06 88.21±0.06 88.21±0.06 87.47±0.42 87.43±0.48
rotate 40 64.25±0.0466.16±1.4168.05±0.14 68.05±0.14 68.05±0.14 66.24±0.04 67.17±0.37
shear 28.6 85.49±0.0486.82±0.1887.70±0.16 87.70±0.16 87.70±0.16 87.40±0.57 87.30±0.30
shear 57.3 59.93±0.1863.09±1.64 63.09±1.64 63.09±1.6462.56±1.51 62.10±0.64 62.56±0.70
zoom 120 77.56±1.1381.46±1.87 81.46±1.87 81.46±1.8779.80±1.02 80.97±1.41 80.97±1.35
zoom 140 47.20±2.4150.24±3.09 50.24±3.09 50.24±3.0949.28±0.18 48.62±2.03 48.62±1.92
zoom 60 76.46±1.6875.59±0.1277.81±1.33 78.61±1.16 78.61±1.16 78.61±1.16 78.61±0.30 78.61±0.30 78.61±0.30
zoom 80 90.11±0.4590.69±0.2291.53±0.21 91.53±0.21 91.53±0.21 91.16±0.10 91.16±0.04
Table 7: ResNet-34 OOD experiments on CIFAR-10.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 88.15±0.3789.11±0.6088.18±0.94 90.07±0.66 90.07±0.66 90.07±0.66 89.29±0.58
rotate 40 66.29±1.9266.19±1.2165.95±1.70 68.02±1.75 68.02±1.75 68.02±1.75 66.81±0.40
shear 28.6 86.76±0.5788.46±0.5886.88±0.94 89.05±1.19 89.05±1.19 89.05±1.19 87.92±0.16
shear 57.3 60.01±1.6162.00±0.6959.10±2.89 63.63±1.68 63.63±1.68 63.63±1.68 62.12±0.60
zoom 120 77.92±1.3984.09±2.03 84.09±2.03 84.09±2.0378.98±2.07 77.14±1.23 80.40±0.61
zoom 140 45.08±2.5051.81±3.21 51.81±3.21 51.81±3.2146.74±2.57 44.22±1.59 48.41±3.74
zoom 60 76.24±0.2378.16±0.6875.94±1.42 78.64±1.17 78.65±0.21 78.65±0.21 78.65±0.21
zoom 80 90.78±0.1191.65±0.7491.05±0.39 92.44±0.74 92.44±0.74 92.44±0.74 92.09±0.43
Table 8: ResNet-50 OOD experiments on CIFAR-10.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 85.46±0.0684.79±0.5284.40±1.60 86.30±0.94 86.30±0.94 86.30±0.94 85.78±0.83
rotate 40 65.45±1.47 65.45±1.47 65.45±1.4761.05±1.2061.83±2.18 64.15±1.18 64.83±1.30
shear 28.6 86.54±0.54 86.54±0.54 86.54±0.5484.95±0.4784.67±1.06 86.40±0.80 86.44±0.58
shear 57.3 61.63±1.2058.37±1.9459.97±0.90 62.33±0.54 62.33±0.54 62.33±0.54 62.24±0.68
zoom 120 77.05±4.0370.94±1.7874.80±1.47 77.17±2.81 80.09±4.54 80.09±4.54 80.09±4.54
zoom 140 51.10±7.88 51.10±7.88 51.10±7.8838.27±0.9942.32±2.46 43.47±4.56 48.46±6.04
zoom 60 73.57±0.8174.70±0.5770.48±2.99 74.92±1.37 76.30±0.23 76.30±0.23 76.30±0.23
zoom 80 88.94±0.2889.29±0.2888.84±0.74 89.62±0.80 90.56±0.14 90.56±0.14 90.56±0.14
Table 9: VGG-13 OOD experiments on CIFAR-10.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 85.00±0.6986.56±0.2886.81±0.08 86.81±0.21 87.19±0.36 87.19±0.36 87.19±0.36
rotate 40 64.46±1.7167.05±0.5267.06±0.90 67.06±0.90 67.06±0.90 66.83±0.93 66.41±0.73
shear 28.6 84.52±0.4086.49±0.4687.14±0.85 87.14±0.85 87.14±0.85 86.66±0.32 86.61±0.34
shear 57.3 58.56±0.4359.18±0.3661.14±1.25 61.14±1.25 61.14±1.25 58.76±0.30 59.33±0.84
zoom 120 84.12±0.0985.73±1.4684.58±0.69 86.46±1.48 86.46±1.48 86.46±1.48 86.46±1.53 86.46±1.53 86.46±1.53
zoom 140 58.97±0.4961.31±2.2059.15±1.62 61.58±1.83 61.58±1.83 61.58±1.83 61.58±2.35 61.58±2.35 61.58±2.35
zoom 60 72.11±0.6174.34±2.18 74.34±2.18 74.34±2.1872.65±0.80 72.92±0.56 72.92±0.91
zoom 80 88.37±0.3789.66±0.0389.66±0.35 90.06±0.23 90.06±0.23 90.06±0.23 90.06±0.15 90.06±0.15 90.06±0.15
18Under review as submission to TMLR
Table 10: DenseNet-40 OOD experiments on CIFAR-100.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 49.09±0.2949.74±0.5550.19±0.52 50.51±1.40 50.51±1.40 50.51±1.40 50.11±0.28
rotate 40 31.89±0.1833.08±0.2732.74±0.15 33.23±1.46 33.23±1.46 33.23±1.46 32.63±0.84
shear 28.6 52.59±0.2753.94±0.3554.19±0.30 55.36±0.98 55.36±0.98 55.36±0.98 54.22±0.53
shear 57.3 34.60±1.2235.91±0.2636.00±0.33 36.00±1.03 36.50±0.42 36.50±0.42 36.50±0.42
zoom 120 41.99±1.7141.52±0.9342.55±1.90 42.55±1.90 42.55±1.90 41.91±0.66 42.11±0.63
zoom 140 16.93±0.36 16.93±0.36 16.93±0.3615.24±0.1715.82±0.70 15.82±0.69 15.96±1.04
zoom 60 37.85±2.0638.46±2.6439.28±1.06 40.67±0.68 40.67±0.68 40.67±0.68 39.24±0.91
zoom 80 58.49±0.5559.35±0.2260.01±0.14 60.66±0.37 60.66±0.37 60.66±0.37 60.40±0.40
Table 11: MobileNetV2 OOD experiments on CIFAR-100.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 62.28±0.6362.76±0.0863.47±0.40 64.59±0.52 64.59±0.52 64.59±0.52 64.09±0.32
rotate 40 42.97±0.3642.95±0.1844.09±0.46 44.31±0.11 44.31±0.11 44.31±0.11 44.28±0.37
shear 28.6 61.50±0.4364.42±0.5964.88±0.29 65.01±0.37 65.01±0.37 65.01±0.37 64.72±0.09
shear 57.3 42.13±0.8842.80±1.7042.62±0.48 43.89±0.76 43.89±0.76 43.89±0.76 42.98±0.09
zoom 120 54.60±0.9953.32±3.6160.53±1.21 60.53±1.21 60.53±1.21 56.40±1.53 55.13±2.88
zoom 140 26.69±1.3624.88±2.4034.06±2.05 34.06±2.05 34.06±2.05 27.29±2.28 27.46±2.83
zoom 60 45.51±2.2143.48±0.5942.58±2.23 42.89±2.65 46.33±0.23 46.33±0.23 46.33±0.23
zoom 80 66.62±0.3667.85±0.1869.30±0.39 68.93±0.55 69.65±0.29 69.65±0.29 69.65±0.29
Table 12: ResNet-34 OOD experiments on CIFAR-100.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 62.62±0.6364.40±0.2265.34±0.45 65.17±0.50 66.01±0.57 66.01±0.57 66.01±0.57
rotate 40 43.59±0.1444.03±1.0945.49±0.24 45.32±0.30 45.77±0.64 45.77±0.64 45.77±0.64
shear 28.6 62.63±0.7065.03±0.5365.44±1.03 66.06±0.29 66.20±0.38 66.20±0.38 66.20±0.38
shear 57.3 40.27±1.1540.81±0.7443.63±0.57 44.79±0.59 44.79±0.59 44.79±0.59 43.56±0.39
zoom 120 59.78±0.9261.12±2.4262.42±0.44 64.59±2.41 64.59±2.41 64.59±2.41 60.50±1.19
zoom 140 37.58±2.8537.17±3.6838.50±0.80 42.03±4.63 42.03±4.63 42.03±4.63 35.37±2.76
zoom 60 43.14±1.0745.32±0.9746.27±0.42 47.24±2.00 47.24±2.00 47.24±2.00 46.00±1.62
zoom 80 66.09±0.7168.34±0.6770.22±0.56 70.22±0.56 70.22±0.56 70.10±0.84 69.57±0.35
Table 13: ResNet-50 OOD experiments on CIFAR-100.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 59.44±0.8060.36±0.7060.37±1.46 64.53±1.58 64.53±1.58 64.53±1.58 61.84±0.57
rotate 40 40.10±1.5340.47±1.2140.45±1.61 44.73±1.44 44.73±1.44 44.73±1.44 42.80±0.50
shear 28.6 59.70±1.8762.47±0.6862.71±1.01 65.63±0.27 65.63±0.27 65.63±0.27 63.30±1.05
shear 57.3 38.96±1.0540.56±0.7942.45±0.27 45.57±1.82 45.57±1.82 45.57±1.82 41.57±1.72
zoom 120 53.62±2.0957.13±1.4954.54±5.03 57.97±0.00 57.97±0.00 57.97±0.00 57.05±1.07
zoom 140 27.24±3.4729.51±0.75 29.51±0.75 29.51±0.7526.04±6.95 29.35±0.95 29.21±0.86
zoom 60 40.36±3.1245.47±1.6045.19±2.65 47.96±1.16 47.96±1.16 47.96±1.16 44.71±1.99
zoom 80 63.45±2.5766.28±0.3267.63±1.65 69.77±0.33 69.77±0.33 69.77±0.33 67.78±1.91
19Under review as submission to TMLR
Table 14: VGG-13 OOD experiments on CIFAR-100.
Transform SGD SAM ASAM SAMSON 2SAMSON∞
rotate 20 56.80±0.4558.52±0.8659.77±0.36 59.87±0.30 59.87±0.30 59.87±0.30 59.85±0.43
rotate 40 38.53±0.7140.14±0.2540.68±0.40 40.68±0.40 40.68±0.40 40.45±0.43 40.30±0.47
shear 28.6 58.88±0.0860.45±0.4560.68±0.15 60.97±0.48 60.97±0.48 60.97±0.48 60.70±0.21
shear 57.3 36.70±0.2438.53±0.4738.29±0.39 38.58±0.27 39.85±1.23 39.85±1.23 39.85±1.23
zoom 120 56.94±0.7660.30±1.6361.03±0.12 61.03±0.12 61.03±0.12 59.71±0.49 59.06±1.36
zoom 140 33.18±1.0037.00±2.70 37.00±2.70 37.00±2.7035.98±1.00 35.09±0.23 34.04±1.31
zoom 60 35.52±0.4238.30±0.1539.10±1.14 40.72±0.71 41.33±1.23 41.33±1.23 41.33±1.23
zoom 80 64.11±0.8565.33±0.4766.28±0.14 66.03±0.12 66.48±0.30 66.48±0.30 66.48±0.30
Table 15: Hyper-parameter choices for the different methods.
Hyper-parameter Choices
SAM’sρ {0.05, 0.1, 0.2, 0.5}
ASAM’sρ {0.5, 1.0, 1.5, 2.0}
SAMSON’s p {2,∞}
SAMSON’s ρ {0.1, 0.2, 0.5, 1.0}
c {±0.05,±0.10,±0.15,±0.20}
α {1.5, 2.0, 2.5}
are presented in tables 16, 17, and 18, respectively. These configurations were the ones used to report the
results in the main paper.
E Additional sharpness experiments
We also provide correlation results with additional sharpness metrics. Particularly, we analyze the m-
sharpness as formulated per SAM and ASAM’s objectives. For SAM, m-sharpness is calculated as
1
n/summationdisplay
S∈Strainmax
∥ϵ∥2≤ρ1
m/summationdisplay
s∈Sls(w+ϵ)−ls(w), (11)
whereas for ASAM, m-sharpness is obtained by
1
n/summationdisplay
S∈Strainmax
∥ϵ/|w|∥2≤ρ1
m/summationdisplay
s∈Sls(w+ϵ)−ls(w). (12)
To avoid repetition, we refer to the main paper for notations. Visual correlations between loss sharpness
and model robustness using SAMSON ∞, SAM, and ASAM’s m-sharpness are presented in figs. 9, 10, and
11, respectively. Results using Keskar et al. (2016)’s sharpness are also shown in Fig. 12. Overall, we see
that SAMSON ∞shows the highest visual correlation, comparatively with the SAMSON 2shown in the main
paper. Moreover, we observe that both SAM’s and ASAM’s m-sharpness show better visual correlation than
Keskar et al. (2016)’s notion of sharpness. This suggests that optimizing for low sharpness during training
by using existing sharpness-aware training methods is an effective way to promote robustness at inference
time, as discussed in the main paper.
20Under review as submission to TMLR
Table 16: Best hyper-parameter configurations for VGG-13 trained on CIFAR-10.
Method Best configuration
SGD + noise α= 2.0
SGD + clipping c=±0.15
SAM ρ= 0.1
SAM + noise ρ= 0.1,α= 2.0
SAM + clipping ρ= 0.1,c=±0.2
ASAM ρ= 0.5
ASAM + noise ρ= 0.5,α= 2.0
ASAM + clipping ρ= 0.5,c=±0.2
SAMSON 2 ρ= 0.2,p= 2
SAMSON 2+ clipping ρ= 0.5,p= 2,c=±0.2
SAMSON 2+ noise ρ= 0.1,p= 2,α= 2.0
SAMSON∞ ρ= 1.0,p=∞
SAMSON∞+ clipping ρ= 0.5,p=∞,c=±0.2
SAMSON∞+ noise ρ= 0.1,p=∞,α= 2.0
Table 17: Best hyper-parameter configurations for MobileNetV2 trained on CIFAR-100.
Method Best configuration
SGD + noise α= 2.0
SGD + clipping c=±0.2
SAM ρ= 0.2
SAM + noise ρ= 0.2,α= 2.0
SAM + clipping ρ= 0.2,c=±0.2
ASAM ρ= 1.0
ASAM + noise ρ= 1.0,α= 2.0
ASAM + clipping ρ= 1.0,c=±0.2
SAMSON 2 ρ= 1.0,p= 2
SAMSON 2+ clipping ρ= 0.5,p= 2,c=±0.2
SAMSON 2+ noise ρ= 0.2,p= 2,α= 2.0
SAMSON∞ ρ= 1.0,p=∞
SAMSON∞+ clipping ρ= 1.0,p=∞,c=±0.2
SAMSON∞+ noise ρ= 0.2,p=∞,α= 2.0
21Under review as submission to TMLR
Table 18: Best hyper-parameter configurations for ResNet-18 finetuned on ImageNet.
Method Best configuration
SGD + noise α= 2.5
SGD + clipping c=±0.2
SAM ρ= 0.1
SAM + noise ρ= 0.05,α= 2.5
SAM + clipping ρ= 0.1,c=±0.2
ASAM ρ= 1.0
ASAM + noise ρ= 0.5,α= 2.5
ASAM + clipping ρ= 1.0,c=±0.2
SAMSON 2 ρ= 0.2,p= 2
SAMSON 2+ clipping ρ= 0.5,p= 2,c=±0.2
SAMSON 2+ noise ρ= 0.1,p= 2,α= 2.5
SAMSON∞ ρ= 0.5,p=∞
SAMSON∞+ clipping ρ= 1.0,p=∞,c=±0.2
SAMSON∞+ noise ρ= 0.1,p=∞,α= 2.5
0.002 0.004 0.006 0.008 0.010
sharpness1.01.52.02.53.03.54.0performance gap (%)VGG-13 (CIFAR-10)
0.01 0.02 0.03
sharpness10152025performance gap (%)MobileNetV2 (CIFAR-100)
0.005 0.010 0.015
sharpness14161820222426performance gap (%)ResNet-18 (ImageNet)
0.04
 0.02
 0.00 0.02 0.040.04
0.02
0.000.020.04normal weight clipping additive noise
Figure 9: Correlation between SAMSON ∞’sm-sharpness (Eq. (10), ρ= 0.5,p=∞) and robustness, i.e.
the performance gap between the noise realizations at σc= 0.0and atσc= 0.4. We plot the mean and
standard deviation over 10 and 3 inference runs for CIFAR-10/100 and ImageNet, respectively.
22Under review as submission to TMLR
0.0004 0.0006 0.0008 0.0010 0.0012
sharpness1.01.52.02.53.03.54.0performance gap (%)VGG-13 (CIFAR-10)
0.00075 0.00100 0.00125 0.00150 0.00175
sharpness10152025performance gap (%)MobileNetV2 (CIFAR-100)
0.0008 0.0010 0.0012 0.0014
sharpness14161820222426performance gap (%)ResNet-18 (ImageNet)
0.04
 0.02
 0.00 0.02 0.040.04
0.02
0.000.020.04normal weight clipping additive noise
Figure10: CorrelationbetweenSAM’s m-sharpness(Eq.(11), ρ= 0.05)androbustness, i.e.theperformance
gap between the noise realizations at σc= 0.0and atσc= 0.4. We plot the mean and standard deviation
over 10 inference runs.
0.002 0.003 0.004
sharpness1.01.52.02.53.03.54.0performance gap (%)VGG-13 (CIFAR-10)
0.005 0.010 0.015
sharpness10152025performance gap (%)MobileNetV2 (CIFAR-100)
0.002 0.003 0.004 0.005
sharpness14161820222426performance gap (%)ResNet-18 (ImageNet)
0.04
 0.02
 0.00 0.02 0.040.04
0.02
0.000.020.04normal weight clipping additive noise
Figure 11: Correlation between ASAM’s m-sharpness (Eq. (12), ρ= 0.5) and robustness, i.e.the perfor-
mance gap between the noise realizations at σc= 0.0and atσc= 0.4. We plot the mean and standard
deviation over 10 inference runs.
10000 15000 20000 25000 30000
sharpness1.01.52.02.53.03.54.0performance gap (%)VGG-13 (CIFAR-10)
4000 5000 6000 7000 8000
sharpness10152025performance gap (%)MobileNetV2 (CIFAR-100)
0 2 4 6 8
sharpness 1e614161820222426performance gap (%)ResNet-18 (ImageNet)
0.04
 0.02
 0.00 0.02 0.040.04
0.02
0.000.020.04normal weight clipping additive noise
Figure 12: Correlation between Keskar et al. (2016)’s sharpness and robustness, i.e.the performance gap
between the noise realizations at σc= 0.0and atσc= 0.4. We plot the mean and standard deviation over
10 inference runs.
23Under review as submission to TMLR
E.1 Pearson correlations
We present the Pearson correlations and p-values of the different m-sharpness methods and generalization
gaps in Table 19. Overall, we observe a high Pearson coefficient and small p-values, suggesting a statistically
significant correlation between the sharpness-aware minimization objectives of the different methods and
robustness, in particular with our variants.
Table 19: Pearson coefficients and p-values between different sharpness measures and robustness on different
models, datasets, and training regimes: normal (N), additive noise (AN), and weight clipping (WC).
CIFAR-10 CIFAR-100 ImageNet
m-sharpness VGG-13 MobileNetV2 ResNet-18
N AN WC N AN WC N AN WC
SAM .59p=.28.93p=.02.89p=.04.87p=.05.78p=.11.90p=.03.90p=.03.90p=.03.94p=.01.94p=.01.94p=.01.88p=.04.94p=.01.94p=.01.94p=.01
ASAM .92p=.02.92p=.02.92p=.02.93p=.01.87p=.05.98p<.01.98p<.01.98p<.01.92p=.02.73p=.15.85p=.06.76p=.13.84p=.07
SAMSON 2.92p=.02.92p=.02.92p=.02.93p=.01.86p=.05.95p=.01.95p=.01.95p=.01.95p=.01.72p=.16.84p=.07.76p=.13.84p=.07
SAMSON∞.91p=.02.96p<.01.96p<.01.96p<.01.89p=.03.89p=.03.89p=.03.97p<.01.80p=.09.83p=.07.86p=.05.91p=.02.91p=.02.91p=.02.94p=.01.94p=.01.94p=.01
F Additional discussions
As we propose a simple extension to the ASAM method, we seek to propose an adaptive sharpness that
is invariant to parameter scaling across the considered parameter tensor. With respect to ASAM, which is
invariant to parameter scaling on a per parameter basis, our approach defines a neighbourhood region in the
loss landscape that is influenced by the statistics of the parameters tensor. It encourages a high ϵfor small
weights if the weight tensor it belongs to has values in the same range, whereas ASAM would simply not
apply much perturbation to it. One scenario where the range of the weight tensor is small and the weight
elements share close values is when using aggressive weight clipping to promote better robustness. With
aggressive weight clipping, i.e.c∈R: 0< c < 1,ϵ∗
SAMSON∞(w)is strictly bigger than ϵ∗
ASAM (w)at the
sameρ:
ρ(wc−1)2∇L(w)
||wc−1∇L(w)||2>ρw2∇L(w)
||w∇L(w)||2(13)
1√
c2>1. (14)
However, achieving a good trade-off between low loss and low sharpness is inherent to any sharpness-aware
minimization method due to the minmax nature of the objective. Hence, we note that controlling the amount
of perturbations may be beneficial to achieve a better trade-off at the end of training. In our approach, this
can be done by using p= 2, which softens SAMSON’s worst-case perturbation when compared to p=∞.
Since the loss-sharpness trade-off is task and model-specific, it is recommended in practice to treat the
p-norm as a hyperparameter to account for different training dynamics.
Another scenario is when the weight ranges of different layers are not in similar intervals due to the use of
batch normalization for example. To illustrate, let us define a 2-layer DNN with parameters w1andw2. We
further suppose that throughout the training of the DNN, the parameters in w1observe a large magnitude,
and those of w2have a small magnitude, e.g. close to zero, with respect to the average values of both tensors:
that is||w1||2>>||w2||2. In ASAM’s formulation, the perturbation applied to w2would be close to zero,
whereasw1would have large perturbations, i.e. ||ϵ∗
ASAM (w1)||>>||ϵ∗
ASAM (w2)||. SAMSON aim to put the
perturbation on the same scale for w1andw2by scaling the neighbourhood to account for the statistics of
w1andw2, that is||ϵ∗
SAMSON (w1)||≈||ϵ∗
SAMSON (w2)||. In other words, SAMSON aims to close a gap in
the definition of ASAM by automatically tuning the neighborhood size depending on the parameter tensor
statistics.
24Under review as submission to TMLR
Considering our use-case of DNN deployment on memristor devices, where parameters are scaled on a per
tensor basis, the range of the tensor defines the maximum relative impact of the hardware noise. The tighter
the range, the less of an effect the hardware noise will have. SAMSON reflects this behaviour by scaling ϵ
with respect to the range and as such is more related to such hardware setting than ASAM or SAM.
F.1 Limitations
A limitation of our method is regarding the types of sharpness considered. Particularly, SAMSON’s objective
optimizes for the m-sharpness measure presented in Eq. (10). Such measure depends on the worst-case
perturbation within the neighborhood size ρwhich which can be characterized as minimizing for zeroth-
order flatness. Such flatness has been recently shown by Zhang et al. (2023) to achieve a lower maximum
eigenvalue of the Hessian and Hessian trace compared to the first-order flatness proposed by the previous
work. This leads to minima that may have sharp curvature in certain directions instead of low curvature
across all directions. Combining our approach with such first-order flatness solution is an interesting future
work.
25