Published in Transactions on Machine Learning Research (03/2023)
Estimating the Density Ratio between Distributions with
High Discrepancy using Multinomial Logistic Regression
Akash Srivastava∗akashsri@mit.edu
MIT-IBM Watson AI Lab and IBM Research
Seungwook Han∗swhan@mit.edu
MIT
Kai Xu xuk@amazon.com
Amazon
Benjamin Rhodes ben.rhodes@ed.ac.uk
School of Informatics, University of Edinburgh
Michael U. Gutmann∗michael.gutmann@ed.ac.uk
School of Informatics, University of Edinburgh
Reviewed on OpenReview: https: // openreview. net/ forum? id= jM8nzUzBWr
Abstract
Functions of the ratio of the densities p/qare widely used in machine learning to quantify
the discrepancy between the two distributions pandq. For high-dimensional distributions,
binary classification-based density ratio estimators have shown great promise. However,
when densities are well separated , estimating the density ratio with a binary classifier is
challenging. In this work, we show that the state-of-the-art density ratio estimators per-
form poorly on well separated cases and demonstrate that this is due to distribution shifts
between training and evaluation time. We present an alternative method that leverages
multi-class classification for density ratio estimation and does not suffer from distribution
shift issues. The method uses a set of auxiliary densities {mk}K
k=1and trains a multi-class
logistic regression to classify the samples from p,qand{mk}K
k=1intoK+ 2classes. We
show that if these auxiliary densities are constructed such that they overlap with pandq,
then a multi-class logistic regression allows for estimating logp/qon the domain of any of
theK+ 2distributions and resolves the distribution shift problems of the current state-of-
the-art methods. We compare our method to state-of-the-art density ratio estimators on
both synthetic and real datasets and demonstrate its superior performance on the tasks of
density ratio estimation, mutual information estimation, and representation learning. Code:
https://www.blackswhan.com/mdre/
1 Introduction
Quantification of the discrepancy between two distributions underpins a large number of machine learning
techniques. For instance, distribution discrepancy measures known as f-divergences (Csiszár, 1964), which
are defined as expectations of convex functions of the ratio of two densities, are ubiquitous in many domains
of supervised and unsupervised machine learning. Hence, density ratio estimation is often a central task
in generative modeling, mutual information and divergence estimation, as well as representation learning
(Sugiyamaetal.,2012;Gutmann&Hyvärinen,2010;Goodfellowetal.,2014;Nowozinetal.,2016;Srivastava
et al., 2017; Belghazi et al., 2018; Oord et al., 2018; Srivastava et al., 2020). However, in most problems
∗equal contribution
1Published in Transactions on Machine Learning Research (03/2023)
of interest, estimating the density ratio by modeling each of the densities separately is significantly more
challenging than directly estimating their ratio for high dimensional densities (Sugiyama et al., 2012). Hence,
direct density ratio estimators are often employed in practice.
One of the most commonly used density ratio estimators (DRE) utilizes binary classification via logistic
regression (BDRE). Once trained to discriminate between the samples from the two densities, BDREs have
been shown to estimate the ground truth density ratio between the two densities (e.g. Gutmann & Hyväri-
nen, 2010; Gutmann & Hirayama, 2011; Sugiyama et al., 2012; Menon & Ong, 2016). BDREs have been
tremendously successful in problems involving the minimization of the density-ratio based estimators of dis-
crepancy between the data and the model distributions even in high-dimensional settings (Nowozin et al.,
2016; Radford et al., 2015). However, they do not fare as well when applied to the task of estimating the dis-
crepancy between two distributions that are far apart or easily separable from each other . This issue has been
characterized recently as the density-chasm problem by Rhodes et al. (2020). We demonstrate this in Figure
1 where we employ a BDRE to estimate the density ratio between two 1-D distributions, p=N(−1,0.1)
andq=N(1,0.2)shown in panel (a). Since pandqare considerably far apart from each other, solving the
classification problem is relatively simple as illustrated by the visualization of the decision boundary of the
BDRE. However, as shown in panel (b), even in this simple setup, BDRE completely fails to estimate the
ratio. Kato & Teshima (2021) have also confirmed that most DREs, especially those implemented with deep
neural networks, tend to overfit to the training data in some way when faced with the density-chasm prob-
lem. Since BDRE-based plug-in estimators are often used in many high-dimensional tasks such as mutual
information estimation, representation learning, energy-based modeling, co-variate-shift resolution, and im-
portance sampling (Rhodes et al., 2020; Choi et al., 2021b;a; Sugiyama et al., 2012), resolving density-chasm
is an important problem of high practical relevance.
A recently introduced solution to the density-chasm problem, telescopic density-ratio estimation (TRE;
Rhodes et al., 2020), tackles it by replacing the easier-to-classify, original logistic regression problem, by a set
of harder-to-classify logistic regression problems. In short, TRE constructs a set of Kauxiliary distributions
({mk}K
k=1) to bridge the two target distributions ( p=:m0andq=:mK+1) of interest and then trains a
set ofK+ 1BDREs on every pair of consecutive distributions (mk−1andmkfork= 1,...,K), which are
assumed to be close enough (i.e. not easily separable) for BDREs to work well. After that, an overall density
ratio estimate is obtained by taking the cumulative (telescopic) product of all individual estimates.
In this work, we argue that the aforementioned solution to the density chasm problem has an inherent issue
ofdistribution shift that can lead to significant inaccuracies in the final density ratio estimation. Notice that
thei-th BDRE in the chain of BDREs that TRE constructs is only trained on the samples from distributions
miandmi+1. However, post-training, it is typically evaluated on regions where the distributions from the
original density ratio estimation problem (i.e. pandq) have non-negligible mass. If the high-probability
regions ofp,qand the auxiliary distributions mido not overlap, the training and evaluation distributions
forthei-thBDREaredifferent. Becauseofthisdistributionshiftbetweentrainingandevaluation, theoverall
density ratio estimation can end up being inaccurate (see Figure 2 and Section 2.1 for further details). We
here provide another solution to the density-chasm problem that avoids this distribution shift.
We present Multinomial Logistic Regression based Density Ratio Estimator (MDRE), a novel
methodfordensityratioestimationthatsolvesthedensity-chasmproblemwithoutsufferingfromdistribution
shift. This is done by using auxiliary distributions and multi-class classification .MDREreplaces the easy
binary classification problem with a singleharder multi-class classification problem. MDREfirst constructs
a set ofKauxiliary distributions {mk}K
k=1that overlap with pandqand then uses multi-class logistic
regression on the K+ 2distributions to obtain a density ratio estimator of logp/q. We will show that the
multi-class classification formulation avoids the distribution shift issue of TRE.
The key contributions of this work are as follows:
1. We study the state-of-the-art solution to the density-chasm problem (TRE; Rhodes et al., 2020)
and identify its limitations arising from distribution shift. We illustrate that this inherent issue can
significantly degrade its density ratio estimation performance.
2Published in Transactions on Machine Learning Research (03/2023)probability
0 20 40 60 80 100
Iteration0246810Train Loss
10
 8
 6
 4
 2
 0 2 4
Samples500
400
300
200
100
0100200Log RatioTrue p/q
BDRE p/q
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Iteration0246810Test Loss
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
Samples012345Probp
q
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
(a) BDRE class probability
log-ratio
0 20 40 60 80 100
Iteration0246810Train Loss
10
 8
 6
 4
 2
 0 2 4
Samples500
400
300
200
100
0100200Log RatioTrue p/q
BDRE p/q
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Iteration0246810Test Loss
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
Samples012345Probp
q
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
(b) BDRE log density-ratio
probability
0 5000 10000 15000 20000 25000 30000 35000 40000
Iteration0246810Train Loss
10
 8
 6
 4
 2
 0 2 4
Samples500
400
300
200
100
0100200Log RatioTrue p/q
CoB p/q
0 100 200 300 400 500 600 700 800
Iteration0246810Test Loss
0 10000 20000 30000 40000
Iteration0246810Train Loss
1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
Samples012345Probp
m
q
0 10000 20000 30000 40000
Iteration0246810Test Loss
0 10000 20000 30000 40000
Iteration0246810Train Loss
6
 4
 2
 0 2 4 6 8 10
Samples1000
010002000300040005000Log RatioTrue p/q
TRE p/q
0 10000 20000 30000 40000
Iteration0246810Test Loss
(c)MDREclass probability
log-ratio
0 5000 10000 15000 20000 25000 30000 35000
Iteration0246810Train Loss
10
 8
 6
 4
 2
 0 2 4
Samples500
400
300
200
100
0100200Log RatioTrue p/q
MDRE p/q
0 100 200 300 400 500 600 700
Iteration0246810Test Loss
0 10000 20000 30000 40000
Iteration0246810Train Loss
1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5
Samples012345Probp
m
q
0 10000 20000 30000 40000
Iteration0246810Test Loss
0 10000 20000 30000 40000
Iteration0246810Train Loss
6
 4
 2
 0 2 4 6 8 10
Samples1000
010002000300040005000Log RatioTrue p/q
TRE p/q
0 10000 20000 30000 40000
Iteration0246810Test Loss
(d)MDRElog density-ratio
Figure 1: BDRE vs proposed MDRE on estimation of logdensity ratio where p=N(−1,0.1)andq=
N(1,0.2). For MDRE, the auxillary distribution mis CauchyC(0,1). Plots (a) and (c) show the class
probabilities P(Y|x)learned for BDRE and MDRE respectively overlayed on the plots of p,qandm.
Plots (b) and (d) show the estimated log density-ratio by BDRE and MDRErespectively. Using auxiliary
distribution mallows MDREto better estimate the log density-ratio.
2. We formally establish the link between multinomial logistic regression and density ratio estimation
and propose a novel method ( MDRE) that uses auxiliary distributions to train a multi-class clas-
sifier for density ratio estimation. MDREresolves the aforementioned distribution shift issue by
construction and effectively tackles the density chasm problem.
3. We construct a comprehensive evaluation protocol that significantly extends on benchmarks used in
prior works. We conduct a systematic empirical evaluation of the proposed approach and demon-
strate the superior performance of our method on a number of synthetic and real datasets. Our
results show that MDREis often markedly better than the current state-of-the-art of density ratio
estimation on tasks such as f-divergence estimation, mutual information estimation, and represen-
tation learning in high-dimensional settings.
2 Related Work
Telescopic density-ratio estimation (TRE, Rhodes et al., 2020) uses a two step, divide-and-conquer strategy
to tackle the density-chasm problem. In the first step, they construct Kwaymark distributions{mk}K
k=1
by gradually transporting samples from ptowards samples from q. Then, they train KBDREs, one for
each consecutive pair of distributions. This allows for estimating the ratio rp/qas the product of K+ 1
BDREs,rp/q:=p
q=p
m1×···×mK
q. Rhodes et al. (2020) introduced two schemes for creating waymark
distributions that ensure that consecutive pairs of distributions are packed closely enough so that none of
theK+ 1BDREs suffer from the density-chasm issue. Hence, TRE addresses the density-chasm issue by
replacing the ratio between pandqwith a product of K+ 1intermediate density ratios that, by design of
the waymark distribution, should not suffer from the density-chasm problem. In a new work, Choi et al.
(2021b) introduced DRE- ∞, a method that takes the number of waymark distributions in TRE to infinity
and derives a limiting objective that leads to a more scalable version of TRE.
F-DRE is other interesting related work that comes from Choi et al. (2021a). F-DRE uses a FLOW-
based model (Rezende & Mohamed, 2015) which is trained to project samples from a mixture of the two
distributions onto a standard Gaussian. They then train a BDRE. It is easy to show that any bijective
map will preserve the original density ratio rp/qin the feature space as the Jacobian correction term simply
cancels out. However, due to the bijectivity of the FLOW map, such a method cannot bring the projected
distributions any closer than the discrepancy between the original distributions. At best, the method can
shift the discrepancy between the original distributions along different moments after projection. Due to
this issue, we found that F-DRE did not work well for the problems we considered (see experimental results
in Section 4). Recently, Liu et al. (2021) introduced an optimization-based solution to the density-chasm
problem in exponential family distributions by using (a) normalized gradient descent and (b) replacing the
logistic loss with an exponential loss. Finally, while BDRE remains the dominant method of density ratio
3Published in Transactions on Machine Learning Research (03/2023)
estimation in recent literature, prior works, such as Bickel et al. (2008) and Nock et al. (2016), have studied
multi-class classifier-based density ratio estimation for estimating ratios between a set of densities against a
common reference distribution and its applications in multi-task learning.
2.1 TRE’s performance can degrade due to training-evaluation distribution shifts
In supervised learning, distribution shift (Quiñonero-Candela et al., 2009) occurs when the training data
(x,y)∼ptrainand the test data (x,y)∼ptestcome from two different distributions, i.e. ptrain̸=ptest.
Common training methods, such as those used in BDRE, only guarantee that the model performs well on
unseen data that comes from the same distribution as ptrain. Thus, in the case of distribution shift at test
time, themodel’sperformance degrades proportionatelyto theshift. Wenowshow thatasimilardistribution
shift can occur in TRE when distributions pandqare sufficiently different. Recall that in TRE, we use
BDREs to estimate K+ 1density ratios p/m 1,m2/m1,...,mK/qthat are combined in a telescopic product
to form the overall ratio p/q. Let us denote the estimates of the K+ 1ratios by ˆη1,..., ˆηK+1.
Given the theoretical properties of BDRE, for any i∈{1,...,K +1},ˆηiestimatesrmi−1/miover the support
ofmi(Sugiyama et al., 2012; Gutmann & Hyvärinen, 2010; Menon & Ong, 2016). However, in TRE, when we
evaluate the target ratio p/qon the supports of pandq, we evaluate the individual ˆηion domains for which
we lack guarantees that they perform well. Since the overall estimator for p/q≈ˆη1∗···∗ ˆηK+1combines
multiple ratio estimators, it suffers from the distribution shift issue if anyof the individual estimators’
performance deteriorates. Thus, if the supports of {mi}K
i=1,p, andqare different, or when the samples from
{mi}K
i=1,p, andqdo not overlap well enough, the training and evaluation domains of the ˆηiare different
and we expect the ratio estimate ˆηiand, in turn, the overall estimator for p/qto be poor. We now illustrate
this with a toy example.
We consider estimating the density ratio between p=N(−1,0.1)andq=N(1,0.2). Since,pandqare well
separated, we introduce three auxiliary distributions m1,m2,m3to bridge them, providing the waymarks
that TRE needs. The auxiliary distributions m1,m2,m3are constructed with the linear-mixing strategy
that will be described in Section 3.2. This setup is shown in the top-left panel of Figure 2. We train 4
BDREs ˆη1,ˆη2,ˆη3,ˆη4to estimate ratios p/m 1,m1/m2,m2/m3andm3/qrespectively. We begin by showing
that each of the trained BDREs estimates their corresponding density ratio accurately on their corresponding
training distributions. To show this, in panels 2-5 in the first row of Figure 2, we evaluate ˆη1,ˆη2,ˆη3,ˆη4on
samples from their respective denominator densities m1,m2,m3,qand plot them via a scatter plot where
the x-axis is labeled with the distribution that we draw the samples from and the y-axis is the log-density
ratio (red). We plot the true density ratio in blue for comparison. As evident, red and blue scatter plots
overlap significantly, indicating the individual ratio estimators are accurate on their respective denominator
(training) distributions.
Next, we evaluate the BDREs ˆη1,ˆη2,ˆη3,ˆη4, on samples from pandqinstead of their corresponding training
distributions as before. Distributions pandqare shown in panel 1 of the second row in Figure 2. In the rest
of the panels (2-5) in the second row, estimators ˆη1,ˆη2,ˆη3,ˆη4are compared to the ground-truth log-density
ratios (blue) p/m 1,m1/m2,m2/m3andm3/qthat are also evaluated on samples from pandq. Unlike in
row 1, the estimated log-density ratios do not match the ground-truth. This reflects the training-evaluation
distribution-shift issues pointed out above. We show now that this deterioration in accuracy on the level
of the individual BDREs results in an deterioration of the overall performance of TRE. To this end, we
first recover the TRE estimator by chaining the individually trained BDREs via a telescoping product, i.e.
ˆη1∗ˆη2∗ˆη3∗ˆη4and then evaluate it on samples from all the 5 distributions p,m 1,m2,m3,q. The results are
shown in panels 1-5 of the third row. The estimated log-density ratios (red) do not match the corresponding
ground-truth log-density ratios (blue), which demonstrates that the distribution-shift in the training and
evaluation distributions of the individual BDREs significantly degrades the overall estimation accuracy of
TRE. Additional issues occur when both pandqdo not have full support, as discussed in Appendix G.
4Published in Transactions on Machine Learning Research (03/2023)
Figure 2: TRE for p=N(−1,0.1)andq=N(1,0.2)from Figure 1. In all scatter plots, x-axis denotes
the sampling distribution and y-axis denotes the log-density-ratio. The density plot in the first row shows
p,qand the 3 waymarks; the density plot in the second row shows pandqonly. The scatter plots in the
first row show individual density ratio estimators evaluated on samples from their corresponding training
data (denominator density), demonstrating accurate estimation on the training set. The scatter plots in the
second show individual density ratio estimators evaluated on samples from pandq. The estimation accuracy
has degraded notably due to the train-eval distribution shift. The last row shows the performance of the
overall density ratio estimator on samples from p,m 1,m2,m3,q. We see that the overall ratio estimate is
significantly affected by the deterioration of the individual ratio estimates, illustrating the sensitivity of TRE
to distribution shift problems in case of well-separated distributions.
3 Density Ratio Estimation using Multinominal Logistic Regression
We propose Multinomial Logistic Regression based Density Ratio Estimator (MDRE) to tackle
the density-chasm problem while avoiding the distribution shift issues of TRE. As in TRE, we introduce a
set ofK≥1auxiliary distributions {mk}K
k=1. But, in constrast to TRE, we then formulate the problem
of estimating logp/qas a multi-class classification problem rather than a sequence of binary classification
problems. We show that this change leads to an estimator that is accurate on the domain of all K+ 2
distributions and, therefore, does not suffer from distribution shift.
3.1 Loss function
Wehereestablishaformallinkbetweendensityratioestimationandmultinomiallogisticregression. Consider
a set ofCdistributions{pc}C
c=1and letpx(x) =/summationtextC
c=1πcpc(x)be their mixture distribution, with prior
class probabilities πc.1The multi-class classification problem then consists of predicting the correct class
Y∈{1,...,C}from a sample from the mixture px. For this purpose, we consider the model
P(Y=c|x;θ) =πcexp(hc
θ(x))/summationtextC
k=1πkexp(hk
θ(x)), (1)
1In our simulations, we will use a uniform prior over the classes.
5Published in Transactions on Machine Learning Research (03/2023)
where thehc
θ(x),c= 1,...,Care unnormalized log probabilities parametrized by θ. We estimate θby
minimizing the negative multinomial log-likelihood (i.e. the softmax cross-entropy loss) L(θ)
L(θ) =−C/summationdisplay
c=1πcEx∼pc[logP(Y=c|x;θ)] =C/summationdisplay
c=1πcEx∼pc/bracketleftbigg
−logπc−hc
θ+ logC/summationdisplay
k=1πkexp(hk
θ(x))/bracketrightbigg
,(2)
where, inpractice, theexpectationsarereplacedwithasampleaverage. Wedenotetheoptimalparametersby
θ∗= arg minθL(θ). To ease the theoretical derivation, we consider the case where the hc
θ(x)are parametrized
insuchaflexiblewaythatwecanconsidertheabovelossfunctiontobeafunctionalof Cfunctionsh1,...,hC,
L(h1,...,hC) =C/summationdisplay
c=1πcEx∼pc/bracketleftbigg
−logπc−hc+ logC/summationdisplay
k=1πkexp(hk(x))/bracketrightbigg
. (3)
The following propositions shows that minimizing L(h1,...,hC)allows us to estimate the log ratios between
any pair of the Cdistributions pc.
Proposition 3.1. Letˆh1,..., ˆhCbe the minimizers of L(h1,...,hC)in equation 3. Then the density ratio
betweenpi(x)andpj(x)for anyi,j≤Cis given by
logpi(x)
pj(x)=ˆhi(x)−ˆhj(x) (4)
for allxwherepx(x) =/summationtext
cπcpc(x)>0.
Proof.Wefirstnotethatthesumofexpectations/summationtextC
c=1πcEx∼pcinequation3isequivalenttotheexpectation
with respect to the mixture distribution px. Writing the expectation as an integral we obtain
L(h1,...,hC) =C/summationdisplay
c=1πcEx∼pc[−logπc−hc(x)] +/integraldisplay
px(x)[logC/summationdisplay
k=1πkexp(hk(x))]dx. (5)
The functional derivative of L(h1,...,hC)with respect to hi, i=1..., C, equals
δL
δhi=−πipi(x) +px(x)πiexp(hi(x))/summationtextC
k=1πkexp(hk(x))(6)
for allxwherepx(x)>0. Setting the derivative to zero gives the necessary condition for an optimum
πipi(x)
px(x)=πiexp(hi(x))/summationtextC
k=1πkexp(hk(x)), i = 1,...,C,and for allxwherepx(x)>0. (7)
The left-hand side of equation 7 equals the true conditional probability P∗(Y=i|x) =πipi(x)
px(x). Hence, at
the critical point, ˆh1,..., ˆhCare such that P∗(Y|X)is correctly estimated. From equation 7, it follows that
for two arbitrary iandj, we have (πipi)/(πjpj) = (πiexp(ˆhi))/(πjexp(ˆhj))i.e.
logpi(x)
pj(x)=ˆhi(x)−ˆhj(x) (8)
for allxwherepx(x)>0, which concludes the proof.
Remark 3.2 (Identifiability) .While we have Cunknownsh1,...,hCandCequations in equation 7, there is
a redundancy in the equations because
C/summationdisplay
i=1πipi(x)
px(x)=C/summationdisplay
i=1πiexp(hi(x))/summationtextC
k=1πkexp(hk(x))=/summationtextC
i=1πiexp(hi(x))
/summationtextC
k=1πkexp(hk(x))= 1
This means that we cannot uniquely identify all hiby minimising equation 3. However, the difference hi−hj,
fori̸=j, can be identified and is equal to the desired log ratio between piandpjper equation 8.
6Published in Transactions on Machine Learning Research (03/2023)
Remark 3.3 (Effect of parametrisation and finite sample size) .In practice, we only have a finite amount of
training data and the parametrisation introduces constraints on the flexibility of the model. With additional
assumptions, e.g. that the true density ratio logpi(x)−logpj(x)can be modeled by the difference of hi
θ(x)
andhj
θ(x), we show in Appendix A that our ratio estimator is consistent. We here do not dive further
into the asymptotic properties of the estimator but focus on the practical applications of the key result in
equation 8.
Importantly, equation 8 allows us to estimate rp/qby formulating our ratio estimation problem as a multi-
nomial nonlinear regression problem as summarized in the following corollary.
Corollary 3.4. Let the distributions of the first two classes be pandq, respectively, i.e. p1≡p,p2≡q,
and the remaining Kdistributions be equal to the auxiliary distributions mi, i.e.p3≡m1,...,pK+2≡mK.
Then
log ˆrp/q(x) =ˆh1(x)−ˆh2(x). (9)
Remark3.5 (Freefromdistributionshiftissues) .Sinceequation8holdsforall xwherethemixture px(x)>0,
the estimator ˆrp/q(x)in equation 9 is valid for all xin the union of the domain of p,q,m 1,...,mK. This
means that MDREdoes not suffer from the distribution shift problems that occur when solving a sequence
of binomial logistic regression problems as in TRE. We exemplify this in Section 3.3 after introducing three
schemes to construct the auxiliary distributions m1,...,mK.
3.2 Constructing the auxiliary distributions
InMDRE, auxiliary distributions need to be constructed such that they have overlapping support with the
empirical densities of pandq. This allows the multi-class classification probabilities to be better calibrated
and leads to an accurate density ratio estimation. We demonstrate this in panel (c) of Figure 1, where
p=N(−1,0.1)andq=N(1,0.2)and the single auxiliary distribution mis set to be Cauchy C(0,1)that
clearly overlaps with the other two distributions. The classification probabilities are shown as the scatter
plot that is overlayed on the empirical densities of these distributions. Compared to the BDRE case in panel
(a), which has high confidence in regions without any data, the multi-class classifier assigns, for pandq,
high class probabilities only over the support of the data and not where there are barely any data points
from these two distributions. Moreover, the auxiliary distribution well covers the space where pandqhave
low density, which provides the necessary training data to inform the values of ˆh1(x)andˆh2(x)in that area,
which leads to an accurate estimate of the log-density ratio shown in panel (d). This is contrast to BDRE
in panel (a) where the classifier, while constrained enough to get the classification right, is not learned well
enough to also get the density ratio right (panel b). This subtle, yet important distinction between the
usage of auxiliary distributions in MDREcompared to BDRE and TRE enables MDREto generalize on
out-of-domain samples, as we will demonstrate in Section 4.1.
Next, we briefly describe three schemes to construct auxiliary distributions for MDREand leave the details
to Appendix B: (1) Overlapping Distribution Unlike TRE, the formulation of MDREdoes not require
“gradually bridging” the two distributions pandq, hence, we introduce a novel approach to constructing
auxiliary distributions. We define mkas any distribution whose samples overlap with both pandq, and
p<<mk,q<<mk. This includes heavy-tailed distributions (e.g. Cauchy, Student-t), normal distributions,
uniform distributions, or their mixtures. We use this scheme in all low-dimensional simulations. (2) Linear
Mixing In this scheme, mkis defined as the distribution of the samples generated by linearly combining
samplesXp={xi
p}N
i=1andXq={xi
q}N
i=1from distributions pandq, respectively. The generative process
for a single sample xi
mkfrommkis given by xi
mk= (1−αk)xi
p+αkxi
q, withxp∈Xp,xi
q∈Xq. This
construction is similar to the linear combination scheme for auxiliary distributions introduced by Rhodes
et al. (2020) with a few key differences that we expand upon in Appendix B. One difference is that αkis
not limited t o 0< αk<1, which allows for non-convex mixtures that completely surround both pandq.
We use this construction scheme in higher-dimensional simulations. (3) Dimension-wise Mixing This
construction scheme was introduced in (Rhodes et al., 2020). Samples from the single auxiliary distribution
mare obtained by combining different subsets of dimensions from samples from pandq. We use this scheme
for experiments involving high-dimensional image data.
7Published in Transactions on Machine Learning Research (03/2023)
2
 1
 0 1 20.00.51.01.52.02.53.0
p
q
m
4
 2
 0 2
pTrue log p/q
MDRE log p/q
4
 2
 0 2
mTrue log p/q
MDRE log p/q
4
 2
 0 2
q300
200
100
0100200Log Ratio
True log p/q
MDRE log p/q
Figure 3: MDREwithm=Cauchy (0,1). The first density plots shows the target densities as well as the
auxiliary distribution (in green, hard to see due to heavy tail and the range of axes). The three scatter plots
show the estimated (red) and true (blue) density ratio p/qevaluated on samples from p,m, andq.MDRE
accurately estimates the ratio across the input domain. Contrast with Figure 2.
Figure 4: MDREusing TRE’s auxiliary distributions. Each scatter plot shows the overall log-density ratio
estimates on samples from the distribution on the x-axis ( MDREin red and true ratio in blue). MDREis
capable of accurately estimating ratios on all samples. Contrast with the bottom row of Figure 2.
3.3 Free from distribution-shift problems
We continue with the example task of estimating the density ratio between p = N (-1, 0.1) and q = N (1, 0.2)
and here illustrate Remark 3.5 that MDREdoes not suffer from the distribution shift problem identified in
Section 2.1. We test MDREwith two types auxiliary distributions. First using a heavy-tailed distribution
(m=Cauchy (0,1)), under the overlapping distributions scheme, and second, with waymark distributions
m1,m2,m3as used by TRE in Figure 2 using their linear-mixing construction scheme.
Figure 3 shows the result for the heavy tailed Cauchy (0,1)auxiliary distribution (green, shown in the left
most figure). We can see that the log-ratio learned by MDREis accurate even beyond the empirical support
ofpandq. This is because MDREis trained on samples from the mixture of p,qandmand hence, per
Remark 3.5, does not encounter distribution-shift, over the support of the mixture distribution. Figure 4
shows the result when using the auxiliary distributions of TRE that we used in Figure 2. We see that the
learnedlog-ratiowellmatchesthetruelog-ratioonsamplesfrom pandq, aswellastheauxiliarydistributions.
This can be directly compared to third row of Figure 2 where TRE suffers from distribution shift problems
and does not yield a well-estimated log-ratio. Note that we do not present results that correspond to the
second row of 2 since the estimation of the log-ratio in MDREdoes not depend on any intermediate density
ratios.
8Published in Transactions on Machine Learning Research (03/2023)
p q True KL BDRE TRE F-DRE MDRE(ours)
N(−1,0.08)N(2,0.15)200.27 21.74 ±4.10 136.05±5.91 14.87±1.72 203.32±2.01
N(−2,0.08)N(2,0.15)355.82 20.22 ±3.64 208.11±18.31 14.22±5.30 360.35±1.37
Table 1: 1D density ratio estimation task for pandqwith large first-order and higher-order differences. In all cases,
MDREoutperforms all the baselines.
(a)p=N(−1,0.08), q=N(2,0.15)
 (b)p=N(−2,0.08), q=N(2,0.15)
Figure 5: Log density-ratio estimates corresponding to the numbers reported in Table 1. Note that the ground truth
andMDREcurves are overlapping, while all the other estimators are significantly worse.
4 Experiments
We here provide an empirical analysis of MDREon both synthetic and real data, showing that it performs
better than previous methods— BDRE, TRE, and F-DRE—on three different density ratio estimation tasks.
We consider cases where numerator and denominator densities differ because their mean is different, i.e. p
andqexhibit first-order discrepancies (FOD), and cases where the difference stems from different higher-
order moments (higher-order discrepancies, HOD). Ratio estimation is closely related to KL divergence and
mutual information estimation since the KL divergence is the expectation of the log-ratios under p, and
mutual information can be expressed as a KL divergence between joint and product of marginals. Being
quantities of core interest in machine learning, we will use them to evaluate the ratio estimation methods.
4.1 1D Gaussian experiments with large KL divergence
In the following 1D experiments, we consider two scenarios, one where p=N(−1,0.08)andq=N(2,0.15),
and one where the mean of pis shifted to−2in order to increase the degree of separation between the two
distributions. In both cases, MDRE’s auxiliary distribution misCauchy(0,1), so that we have a three-class
classification problem ( p,q,m) and three functions hi
θthat parameterize the classifier of MDRE. The three
functions are quadratic polynomials of the form w1x2+w2x+b. For all the methods we set the total number
of samples to 100K.2We provide the exact hyperparameter settings for MDREand other baselines in Table
5 in Appendix C.
Table 1 shows the results. We can see that MDREyields more accurate estimates of the KL divergences
than the baselines, which are off by a significant margin.
We note that KL estimation only requires evaluating the log-ratios on samples from the numerator dis-
tributionp. In Figure 5, we thus show results for all methods where we evaluate the estimated log-
ratios on a wide interval (-12, 12). The figure shows that none of the baseline methods can accurately
estimate the ratio well on the whole interval while MDRE performs well overall. This is important be-
2We found that MDRE ’s results are unchanged even when using smaller sample sizes of 1K or 10K, see Table 4 in Appendix C.
9Published in Transactions on Machine Learning Research (03/2023)
Dimµ1,µ2True MI BDRE TRE F-DRE MDRE(ours)
400, 0 20 10.90 ±0.04 14.52±2.07 14.87±0.33 18.81±0.15
-1, 1 100 29.03 ±0.09 33.95±0.14 13.86±0.26 119.96±0.94
1600, 0 40 21.47 ±2.62 34.09±0.21 12.89±0.87 38.71±0.73
-0.5, 0.6 136 24.88 ±8.93 69.27±0.24 13.74±0.13 133.64±3.70
3200, 0 80 23.47 ±9.64 72.85±3.93 9.17±0.60 87.76±0.77
-0.5, 0.5 240 24.86 ±4.07 100.18±0.29 10.53±0.03 217.14±6.02
Table 2: High-dimensional mutual information estimation task. MDREis able to accurately estimate the MI often
by a very large margins.
cause it means that the ratio is well estimated in regions where pandqhave little probability mass.
These results demonstrate the effectiveness of MDRE with a single auxiliary distribution whose samples
overlaps with those from both pandq, in lieu of using a chain of BDREs with up to K= 28closely-
packed auxiliary distributions as used by TRE. Please see Appendix C for additional results and details.
8
 6
 4
 2
 0 2 4500
400
300
200
100
0100200
true
ours (mean)
ours (3)
Figure 6: Bayesian analysis of MDRE forp=
N(−1.0,0.1), q=N(1.0,0.2).To provide further clarity into MDRE’s density ratio es-
timation behavior, we analyze the uncertainty of its log
ratio estimates using Bayesian analysis. We use a stan-
dard normal prior on the classifier parameters and obtain
posterior samples with Hamiltonian Monte-Carlo. These
posterior samples then yield samples of the density ra-
tio estimates. Figure 6 shows that the high accuracy of
MDRE’s KL divergence estimates can be attributed to
MDRE being confidently accurate around the union of
the high density regions of both pandq. A more detailed
analysis is provided in Appendix D.
4.2 High dimensional experiments with large MI
Following Rhodes et al. (2020), we use the MI estimation benchmark from Belghazi et al. (2018); Poole et al.
(2019) to evaluate MDREon a more challenging, higher-dimensional problem. In this task, the goal is to
estimate the mutual information between a standard normal distribution and a Gaussian random variable
x∈R2dwith a block-diagonal covariance matrix where each block is 2×2with ones on the diagonal and
ρon the off-diagonal. The correlation coefficient ρis computed from the number of dimensions and the
target mutual information I=−d/2 log(1−ρ2). Since this problem construction only induces higher-order
discrepancies (HOD), we added an additional challenge by moving the means of the two distributions, thus
additionally inducing first-order discrepancies (FOD).
ForMDRE, we model the hi
θwith quadratic functions of the form xTW1x+W2x+b. We use linear-mixing
to construct each mk, whereK= 3orK= 5. In Appendix E, we provide the exact configurations for
MDREin Table 6 and explain how to choose mandKin practice.
Table 2 shows the results for each MI task averaged across 3 runs with different random seeds. MDRE
outperforms all baselines in the original MI task where the means of the distribution are the same. The
difference between the performance of MDREand the baselines is particularly stark when the means are
allowed to be nonzero. Only MDRE estimates the MI reasonably well while all baselines dramatically
underestimate it. We further note that MDRE only uses up to 5auxiliary distributions, lowering its
compute requirements compared to TRE, which is the next best performing method and uses up to 15
auxiliary distributions for its telescoping chain.
We found that the resolution proposed by Kato & Teshima (2021) to overcome the over-fitting issue in
Bregman Divergence minimization-based DREs, does not work well in practice. On the high-dimensional
10Published in Transactions on Machine Learning Research (03/2023)
setup of row 2 in Table 2, while the ground truth MI is 100, and MDREestimates it as 119±0,94, the best
model from Kato & Teshima (2021) yields 1.60, significantly underestimating the true value and being a
factor of ten smaller than the classifier-based DRE baselines. For further results, such as plots of estimated
log ratio vs. ground-truth log ratio, training curves, and more, please see Appendix E.
Above, following prior work, we evaluated the methods on problems where pandqare normal distributions.
To enhance this analysis, we further evaluate MDRE on the three new experimental setups below. The
results are summarized in Table 3.
Breaking Symmetry In our high-dimensional experiments reported in Table 2, the means of the Gaussian
distributions pandqwere symmetric around zero in the majority of cases. In order to ensure that
this symmetry did not provide an advantage to MDRE, we also evaluate it on Gaussians pandqwith
randomized means. The results are shown in rows 2 and 6 of Table 3. We see that MDRE continues to
estimate the ground truth KL divergence accurately, demonstrating that it did not benefit unfairly from
the symmetry of distributions around zero.
Model Mismatch In rows 4, 5, 7, and 8 of Table 3, we evaluate MDREby replacing one or both distri-
butionspandqwith a Student-t distribution of the same scale with randomized means. For the Student-t
distributions, we set the degrees of freedom as 5,10or20. These experiments test how well MDREperforms
when there is model mismatch, i.e. how MDREperforms using the same quadratic model that was used
whenpandqwere set to be Gaussian with lighter tails. We find that MDRE is still able to accurately
estimate the ground truth KL in these cases. We found the same to be true for other test distributions such
as a Mixture of Gaussians (shown in row 3 of Table 3).
Finite Support pandqFinally, we test MDRE on another problem where pandqare finite support
distributions that have both FOD and HOD. This is done by setting pandqto be truncated normal
distributions, as shown in row 1 of Table 3. We also set mto be a truncated normal distribution with its
scale set to 2 to allow it to have overlap with both pandq. This setting is similar to the 1D Gaussian example
illustrated in Section 3.3 and MDREmanages to estimate the ground-truth KL divergence accurately.
Dimp q m True KL Est. KL
1Truncated Normal
loc=-1, scale=0.1
support=(-1.1,-0.9)Truncated Normal
loc=1, scale=0.2
support=(-1.1,1.2)Truncated Normal
loc=-1, scale=2
support=(-1.1,1.2)50.65 52.35
160Normal
loc=R(-.5,.5), cov= 2×2BDNormal
loc=R(-.5,.5), cov= ILinear Mixing 54.29 54.10
160Normal
loc=-1, cov= 2×2BDMoG: 0.5*Normal(0.9, I)
+ 0.5*Normal(1.1, I)Linear Mixing 105.60 98.27
160Student T loc=R(-.5,.5),
scale= 2×2BD, df=5Student T
loc=R(-.5,.5), scale=I, df=5Linear Mixing 51.26 49.01
320Student T loc=R(-.5,.5),
scale= 2×2BD, df=10Student T
loc=R(-.5,.5), scale=I, df=10Linear Mixing 53.82 51.03
320Normal
loc=R(-1,1), cov= 2×2BDNormal
loc=R(-1,1), cov= ILinear Mixing 110.05 102.63
320Student T loc=R(-1,1),
scale= 2×2BD, df=10Student T
loc=R(-1,1), scale= I, df=10Linear Mixing 103.12 113.53
320Normal
loc=0, cov= 2×2BDStudent T
loc=0, scale= I, df=20Linear Mixing 82.02 83.63
Table 3: Robustness evaluation for MDRE. Here R(a,b) stands for randomized mean vector where each dimension
is sampled uniformly from the interval (a, b).MDREis able to consistently estimate the ground-truth KL with high
accuracy in all of the cases.
11Published in Transactions on Machine Learning Research (03/2023)
4 5 6 7 8 9
Number of characters5101520253035Mutual information
Ground Truth
Single Ratio
TRE
Ours
(a) MI estimation
4 5 6 7 8 9
Number of characters30405060708090100Mean label accuracy (test)
Single Ratio
TRE
Ours (b) Classification accuracy
1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00
K1112131415Mutual information
889092949698100
Mean label accuracy (test)
 (c) Varying K(# of auxiliary dist.)
Figure 7: SpatialMultiOmniglot representation learning results. Plot (a) shows the MI estimated by the
three methods, MDREis able to estimate the ground truth MI very accurately. Plot (b) shows the resulting
classification accuracy and plot (c) the impact of varying the number of auxiliary distributions on MI
estimation with MDRE.
4.3 Representation learning for SpatialMultiOmniglot
In order to benchmark MDREon large-scale real-world data, following the setup from Rhodes et al. (2020),
we apply MDREto the task of mutual information estimation and representation learning for the Spatial-
MultiOmniglot problem (Ozair et al., 2019). The goal is to estimate the mutual information between u
andvwhereuis an×ngrid of Omniglot characters from different Omniglot alphabets and vis an×n
grid containing (stochastic) realizations of the next characters of the corresponding characters in u. After
learning, we evaluate the representations from the encoder with a standard linear evaluation protocol (Oord
et al., 2018). For MDRE, similarly to TRE, we utilize a separable architecture commonly used in the MI-
based representation learning literature and model the unnormalized log-scores hi
θwith functions of the form
g(u)TWf(v)wheregandfare 14-layer convolutional ResNets (He et al., 2015). While this model amounts
to sharing of parameters across the hi
θ, we would like to emphasize that in all preceding examples, we did
not share parameters among the hi
θ. We construct the auxiliary distributions via dimension-wise mixing.
We here only compare MDREto the single ratio baseline and TRE because Rhodes et al. (2020, Figure 4)
already demonstrated that TRE significantly outperforms both Contrastive Predictive Coding (CPC) (Oord
et al., 2018) and Wasserstein Predictive Coding (WPC) (Ozair et al., 2019) on exactly the same task. Please
refer to Appendix F for the detailed experimental setup.
As can be seen in Figure 7a, MDRE performs better than TRE and the single ratio baseline, exactly
matching the ground truth MI. This improvement in MI estimation is reflected in the representations.
Figure 7b illustrates that MDRE’s encoder learns representations that achieve ∼100% Omniglot character
classification for both d=n2= 4,9. On the other hand, the performances of the single ratio estimator
and TRE (using the same exact dimension-wise mixing to construct auxiliary distributions) both degrade
as the complexity of the task increases, with TRE only reaching up to 91% and 85% for d= 4andd= 9,
respectively. All models were trained with the same encoder architecture to ensure fair comparison.
We further studied the effect of changing Kin thed= 4setup. ForK= 1, we aggregate all the dimension-
wise mixed samples into 1 class, whereas for K= 3, we separate them into their respective classes (corre-
sponding to the number of dimensions mixed). We illustrate this effect in Figure 7c. In line with the finding
of Ma & Collins (2018), increasing the number of K not only helps MDREto reach the ground truth MI,
but also the quality of representations improves from 86.7%to100%test classification accuracy.
5 Discussion
In this work, we presented the multinomial logistic regression based density ratio estimator (MDRE), a
new method for density ratio estimation that had better finite sample (non-asymptotic) performance in our
simulations than current state-of-the-art methods. We showed that it addresses the sensitivity to possible
distribution-shift issues of the recent method by Rhodes et al. (2020). MDREworks by introducing auxiliary
12Published in Transactions on Machine Learning Research (03/2023)
distributions that have overlapping support with the numerator and denominator distributions of the ratio.
It then trains a multinomial logistic regression model to estimate the density-ratio. We demonstrated that
MDRE is both theoretically grounded and empirically strong, and that it sets a new state of the art for
high-dimensional density ratio estimation problems.
However, there are some limitations. First, while the ratio was well estimated in our empirical studies, we do
not provide any bounds on the estimation, meaning that estimated KL divergences or mutual information
values may be over- or underestimated. Second, the choice of the auxiliary distribution, m, is an impor-
tant factor of consideration that significantly impacts the performance of MDRE. While in this work we
demonstrate the efficacy of three schemes for constructing the auxiliary distribution, empirically, it is, by no
means, an exhaustive study. We hope to address these issues in future work, including the development of
learning-based approaches to auxiliary distribution construction.
References
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville,
and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062 , 2018.
Steffen Bickel, Jasmina Bogojeska, Thomas Lengauer, and Tobias Scheffer. Multi-task learning for hiv
therapy screening. In Proceedings of the 25th international conference on Machine learning , pp. 56–63,
2008.
Kristy Choi, Madeline Liao, and Stefano Ermon. Featurized density ratio estimation. arXiv preprint
arXiv:2107.02212 , 2021a.
Kristy Choi, Chenlin Meng, Yang Song, and Stefano Ermon. Density ratio estimation via infinitesimal
classification. arXiv preprint arXiv:2111.11010 , 2021b.
Imre Csiszár. An information-theoretic inequality and its application to the evidence of the ergodicity of
markoff’s chains. Magyer Tud. Akad. Mat. Kutato Int. Koezl. , 8:85–108, 1964.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C.
Courville, and Yoshua Bengio. Generative adversarial nets. In Neural Information Processing Systems ,
2014.
M. U. Gutmann and J. Hirayama. Bregman divergence as general framework to estimate unnormalized
statistical models. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI) , 2011.
Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnor-
malizedstatisticalmodels. In Proceedings of the thirteenth international conference on artificial intelligence
and statistics , pp. 297–304. JMLR Workshop and Conference Proceedings, 2010.
M.U. Gutmann and A. Hyvärinen. Noise-contrastive estimation of unnormalized statistical models, with
applications to natural image statistics. Journal of Machine Learning Research , 13:307–361, 2012.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition,
2015.
Masahiro Kato and Takeshi Teshima. Non-negative bregman divergence minimization for deep direct density
ratio estimation. In International Conference on Machine Learning , pp. 5320–5333. PMLR, 2021.
Bingbin Liu, Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. Analyzing and improving the
optimization landscape of noise-contrastive estimation. arXiv preprint arXiv:2110.11271 , 2021.
Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models:
Consistency and statistical efficiency. arXiv preprint arXiv:1809.01812 , 2018.
Aditya Menon and Cheng Soon Ong. Linking losses for density ratio and class-probability estimation. In
International Conference on Machine Learning , pp. 304–313. PMLR, 2016.
13Published in Transactions on Machine Learning Research (03/2023)
Richard Nock, Aditya Menon, and Cheng Soon Ong. A scaled bregman theorem with applications. Advances
in Neural Information Processing Systems , 29:19–27, 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers us-
ing variational divergence minimization. In Proceedings of the 30th International Conference on Neural
Information Processing Systems , pp. 271–279, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding.arXiv preprint arXiv:1807.03748 , 2018.
Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, and Pierre Sermanet.
Wasserstein dependency measure for representation learning, 2019.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A. Alemi, and George Tucker. On variational
bounds of mutual information, 2019.
Joaquin Quiñonero-Candela, Masashi Sugiyama, Neil D Lawrence, and Anton Schwaighofer. Dataset shift
in machine learning . Mit Press, 2009.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolu-
tional generative adversarial networks. arXiv preprint arXiv:1511.06434 , 2015.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International con-
ference on machine learning , pp. 1530–1538. PMLR, 2015.
Benjamin Rhodes, Kai Xu, and Michael U Gutmann. Telescoping density-ratio estimation. arXiv preprint
arXiv:2006.12204 , 2020.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan: Reduc-
ing mode collapse in gans using implicit variational learning. In Proceedings of the 31st International
Conference on Neural Information Processing Systems , pp. 3310–3320, 2017.
Akash Srivastava, Kai Xu, Michael Gutmann, and Charles Sutton. Generative ratio matching networks. In
Eighth International Conference on Learning Representations , pp. 1–18, 2020.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine learning .
Cambridge University Press, 2012.
L. Wasserman. All of statistics . Springer, 2004.
A Consistency
In Section 3.1, we focused on properties of the loss function L(h1,...,hC)in equation 3. The arguments of
the loss functions were the functions hi, and the loss function was defined in terms of expectations over pc.
This simplified the analysis and provided important insights but does not correspond to practical settings.
Here, we relax the assumptions: We first consider the loss L(θ)in equation 2 where the functions hcare
parameterized by some parameters θ. Then we consider the case where the expectations are replaced by a
sample average based on nsamples. The corresponding loss function will be denote by Ln(θ). The main
point of this section is to derive conditions under which minimizing Ln(θ)leads to the results in Section 3.1,
obtained by minimizing L(h1,...,hC).
Lemma A.1. Denoting the true conditional distribution by P∗(Y|x)we have
ˆθ= arg min
θL(θ) = arg min
θEpx(x)KL(P∗(Y|x)||P(Y|x;θ)) (10)
whereP(Y|x;θ)is defined in equation 1.
14Published in Transactions on Machine Learning Research (03/2023)
Proof.We start with the definition of L(θ)in equation 2:
L(θ) =−C/summationdisplay
c=1πcEx∼pc[logP(Y=c|x;θ)] (11)
The sum of weighted expectations/summationtextC
c=1πcEx∼pccorresponds to a joint expectation over p(Y,x). Decom-
posing the joint as p(x)P∗(Y|x), we thus obtain
L(θ) =−Ep(x)EP∗(Y|x)[logP(Y|x;θ)] (12)
and
L(θ) +Ep(x)EP∗(Y|x)logP∗(Y|x) =Ep(x)EP∗(Y|x)logP∗(Y|x)
P(Y|x;θ)(13)
=Epx(x)KL(P∗(Y|x)||P(Y|x;θ)) (14)
The claim follows since the added term does not depend on θ.
If the true conditional P∗(Y|x)is part of the parametric family {P(Y|x;θ)}θ,ˆθis thus such that P(Y|x;ˆθ) =
P∗(Y|x)for allxwherepx(x)>0. Hence the same arguments after equation 7 in the main text lead the
parametric equivalent to equation 3.1, which we summarize in the following corollary.
Corollary A.2. If the true conditional P∗(Y|x)is part of the parametric family {P(Y|x;θ)}θ, then
hi
ˆθ(x)−hj
ˆθ(x) = logpi(x)
pj(x)(15)
for allxwherepx(x) =/summationtext
cπcpc(x)>0.
We next derive conditions under which ˆθis the unique minimum, which is needed to prove consistency. For
that purpose, we perform a second-order Taylor expansion of L(θ)around ˆθ.
Lemma A.3.
L(ˆθ+ϵϕ) =L(θ) +ϵ2
2ϕ⊤Iϕ (16)
whereϵ >0andI=−Epx(x)EP∗(Y|x)[H(Y,x)]. The matrix H(Y,x)contains the second derivatives of the
log-model, i.e. its (i,j)-th element is
[H(Y,x)]ij=∂2
∂θi∂θjlogP(Y|x;θ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
θ=ˆθ(17)
whereθiandθjare thei-th andj-th element of θ, respectively.
Proof.A second-order Taylor expansion around L(ˆθ)gives
L(ˆθ+ϵϕ) =−Epx(x)/summationdisplay
cP∗(Y=c|x) logP(Y=c|x,ˆθ+ϵϕ) (18)
=L(ˆθ)−∇θL(θ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
θ=ˆθ−Epx(x)/summationdisplay
cP∗(Y=c|x)ϵ2
2ϕ⊤H(Y=c,x)ϕ+O(ϵ2) (19)
=L(ˆθ)−ϵ2
2ϕ⊤/bracketleftigg
Epx(x)/summationdisplay
cP∗(Y=c|x)H(Y=c,x)/bracketrightigg
ϕ+O(ϵ2) (20)
where we have used that the gradient of L(θ)is zero at a minimizer ˆθ. Since/summationtext
cP∗(Y=c|x)H(Y=c,x) =
EP∗(Y|x)H(Y,x), the result follows.
15Published in Transactions on Machine Learning Research (03/2023)
Note thatI(x) =−EP∗(Y|x)[H(Y,x)]is the conditional Fisher information matrix, and I=Epx(x)I(x)is its
expected value taken with respect to px(x).
Corollary A.4. IfIis positive definite, then ˆθis the unique minimizer of L(θ).
Proof.IfIis positive definite, then ϕ⊤Iϕ > 0for all non-zero ϕand by Lemma A.3, L(ˆθ+ϵϕ)>L(ˆθ)
wheneverϕ̸= 0.
We now consider the objective function Ln(θ)where the expectations in L(θ)are replaced by a sample
average over nsamples. Let ˆθn= arg minθLn(θ).
Proposition A.5. If (i)Iis positive definite and (ii) supθ|Ln(θ)−L(θ)|p− →0, then ˆθnp− →ˆθ.
Proof.By Corollary A.4, condition (i) ensures that ˆθ= arg minθL(θ)is a unique minimizer, and hence that
changing ˆθby a small amount will increase the cost function L(θ). Together with the technical condition
(ii) on the uniform convergence of Ln(θ)toL(θ), this allows one to prove that ˆθnconverges in probability
toˆθas the sample size nincreases, following exactly the same reasoning as e.g. in proofs for consistency of
maximum likelihood estimation (Wasserman, 2004, Section 9.13) or noise-contrastive estimation (Gutmann
& Hyvärinen, 2012, Appendix A.3.2).
Corollary A.6. If (i)Iis positive definite, (ii) supθ|Ln(θ)−L(θ)|p− →0, and (iii) there is a parameter value
θ∗such thatP∗(Y|x) =p(Y|x;θ∗), then ˆθnp− →θ∗
Proof.With Proposition A.5, condition (i) and (ii) ensure that ˆθnconverges to ˆθ= arg minθL(θ). With
Lemma A.1, ˆθis also minimizing Epx(x)KL(P∗(Y|x)||P(Y|x;θ)). Hence, if condition (iii) holds, ˆθ=θ∗,
and the result follows.
Proposition A.7 (Consistency of the ratio estimator) .If (i)Iis positive definite, (ii) supθ|Ln(θ)−L(θ)|p− →
0, (iii)P∗(Y|x) =p(Y|x;θ∗)for some parameter value θ∗, and (iv) the mapping from θtohc
θis continuous,
then
hi
ˆθn(x)−hj
ˆθn(x)p− →logpi(x)
pj(x)(21)
for allxwherepx(x) =/summationtext
cπcpc(x)>0.
Proof.By Proposition A.5, condition (i) and (ii) ensure that ˆθnconverges to ˆθ= arg minθL(θ). By Corollary
A.2, condition (iii) ensures that hi
ˆθ(x)−hj
ˆθ(x) = logpi(x)
pj(x)for allxwherepx(x) =/summationtext
cπcpc(x)>0. Since
continuous functions are closed under addition, the mapping from θtohi
ˆθ(x)−hj
ˆθ(x)is continuous if condition
(iv) holds. We can then apply the continuous mapping theorem to conclude that hi
ˆθn(x)−hj
ˆθn(x)p− →
hi
ˆθ(x)−hj
ˆθ(x) = logpi(x)
pj(x), which establishes the result.
B Constructing M
We here elaborate on the three types of auxiliary distributions that we used in this work.
Overlapping Distribution: TheMDREestimator, logp
q= logp
m−logm
qis defined when p<<m and
q<<m. Therefore, mneeds to be such that its support contains the supports of pandq. Any distribution
with full support such as the normal distribution trivially satisfies this requirement. However, satisfying this
requirement does not guarantee empirical overlap of the distributions p,qwithmin finite sample setting.
In order to ensure overlap of samples between the two pairs of distributions we recommend the following:
16Published in Transactions on Machine Learning Research (03/2023)
•Heavy-tailed Distributions: Distributions such as, Cauchy and Student-t are better choice for M
compared to the normal distribution. This is because their heavier tails allow for easily connecting
pandqwith higher sample overlap when they are far apart (especially in the case of FOD).
•Mixtures: Another way to connect pandqusingmsuch that they have their samples overlap, is to
use the mixture distribution. Here, we first convolve pandqwith a standard normal and then take
equal mixtures of the two.
•Truncated Normal: If pandqhave finite support, one can also use a truncated normal distribution
or a uniform distribution that at least spans over the entire support of q. This is assuming that
p<<q.
Linear Mixing: In this construction scheme, distribution Mis defined as the empirical distribution of
the samples constructed by linearly combining samples Xp={xi
p}N
i=1andXq={xi
q}N
i=1from distributions p
andqrespectively. That is, mis the empirical distribution over the set Xm={xi
m|xi
m=αxi
p+(1−α)xi
q,xp∈
Xp,xi
q∈Xq}N
i=1, whereαis not constrained to create a convex mixture. This construction is related to the
linear combination auxiliary of Rhodes et al. (2020). In TRE, the auxiliary distribution is defined as the
empirical distribution of the set Xm={xi
m|xi
m=√
1−α2xi
p+αxi
q,xp∈Xp,xi
q∈Xq}N
i=1, where 0≤α≤1.
This weighting scheme skews the samples from the auxiliary distribution towards p. Therefore, care needs to
be taken when pandqare finite support distributions so that the samples from the auxiliary distributions
do not fall out of the support of q.
Using either of the weighting schemes, one can construct Kdifferent auxiliary distributions. MDRE can
either use these Kauxiliary distributions separately using a K+2-way classifier or define a single mixture
distribution using them as component distributions and train a 3-way classifier. We refer to this construction
as Mixture of Linear Mixing.
Dimension-wise Mixing: In this construction scheme, that is borrowed from TRE as it is, Mis defined
as the empirical distribution of the samples generated by combining different subsets of dimensions from
samples from pandq. We describe the exact construction scheme from TRE below for completeness:
Given ad-length vector xand thatdis divisible by l, we can write down x= (x[1],...x[l]), where each x[i]
has length d/l. Then, a sample from the kth auxiliary distribution is given by: xi
k= (xi
q[1],...xi
q[j],xi
p[j+
1],...,xi
p[l]), forj= 1,...,l), wherexi
p∼pandxi
q∼qare randomly paired.
C 1D density ratio estimation task
In Section 4.1, we studied three cases in which the two distributions pandqare separated by both FOD
and HOD. In all of these 1D experiments, all models were trained with 100,000 samples, and all results are
reported across 3 runs with different random seeds. Additionally, we found that MDREworked equally well
for 1K and 10K samples. The experimental configurations, including the auxiliary distributions for MDRE,
are detailed in Table 5.
p q True KL MDRE@ 1K MDRE@ 10K MDRE@ 100K
N(-1, 0.08)N(2, 0.15) 200.27 195.05 196.50 203.32
N(-2, 0.08)N(2, 0.15) 355.82 346.92 348.97 360.35
Table 4: MDREon 1D density ratio estimation for three settings of sample sizes. MDREestimates the density
ratio well for all the three settings.
17Published in Transactions on Machine Learning Research (03/2023)
p q TREpk MDREm
N(−1,0.08)N(2,0.15)Linear Mixing with
α= [0.053, 0.11,
0.16, 0.21, 0.26, 0.31,
0.37, 0.42, 0.47, 0.53,
0.58, 0.63, 0.68, 0.74,
0.79, 0.84, 0.89, 0.95]C(0,1)
N(−2,0.08)N(2,0.15)Linear Mixing with
α= [0.03, 0.07,
0.1, 0.14, 0.17, 0.21,
0.24, 0.28, 0.31, 0.34,
0.38, 0.41, 0.45, 0.48,
0.52, 0.55, 0.59, 0.62,
0.66, 0.69, 0.72, 0.76,
0.79, 0.83, 0.86, 0.9,
0.93, 0.97]C(0,1)
N(−10,1)N(10,1)Linear Mixing with
α= [0.11, 0.22,
0.33, 0.44, 0.55, 0.66,
0.77, 0.88]C(0,2)
Table 5: Experiment configurations for Table 1 of main text and Table 4 of Appendix C
(a) In-domain: p=N(−10,1)andq=N(10,1)
(b) Expanded domain: p=N(−10,1)andq=
N(10,1)
Figure 8: 1D density ratio estimation analysis. Figure 8a evaluates the log density ratios on uniform samples
inside the domain of the respective training distribution. Figure 8b evaluates the log density ratios on
uniform samples from an expanded domain of the training distribution. The shading represents 1 standard
deviation of the estimates.
D Uncertainty Quantification of MDRE Log-ratio Estimates with Hamiltonian
Monte Carlo
In the 1D experiments, MDRE consistently led to highly accurate KL divergence estimates even in chal-
lenging settings where state-of-the-art methods fail. To understand why MDRE gives such accurate KL
estimates, we conduct an analysis on the reliability of its log-ratio estimates by analyzing the distribution
of the estimates in a Bayesian setup, and study how it impacts the KL divergence estimation. For this
analysis, we use a classifier with the standard normal distribution as the prior on its parameters. The distri-
bution of the log-ratio estimates is simply the distribution of the estimates from the classifiers with different
18Published in Transactions on Machine Learning Research (03/2023)
8
 6
 4
 2
 0 2 4500
400
300
200
100
0100200
true
ours (mean)
ours (3)
(a)logp
qforp=N(−1.0,0.1), q=N(1.0,0.2)
4
 2
 0 2 4 6 8100
0100200300400
true
ours (mean)
ours (3)
 (b)logp
qforp=N(−1.0,0.2), q=N(1.0,0.1)
8
 6
 4
 2
 0 2 4
x02505007501000125015001750variancep
q
(c) Variance of logp
qas a function of distance from
the modes of pandqforp=N(−1.0,0.1), q=
N(1.0,0.2)
4
 2
 0 2 4 6 8
x0100200300400500600variancep
q
(d) Variance of logp
qas a function of distance from
the modes of pandqforp=N(−1.0,0.2), q=
N(1.0,0.1)
Figure 9: Uncertainty quantification for MDREestimator. We plot the 3x standard deviation around the mean in
light blue. In plots (c) and (d) the bars show the means of pandq.
posterior parameters, which are sampled. We consider two setups where first we set p=N(−1.0,0.1)and
q=N(1.0,0.2)and then swap their scales, i.e. p=N(−1.0,0.1)andq=N(1.0,0.2). In both the cases,
we draw samples from the posterior using an Hamiltonian Monte Carlo (HMC) sampler initialized by the
maximum likelihood estimate of the classifier parameter. We then compute a set of samples of the log-ratio
estimates from MDREand estimate the mean and standard deviation using these samples. Figure 9 (a) and
(b) shows these results. We find that MDREis accurate and manifests lowest uncertainty around the region
between the means of p(−1.0) andq(+1.0). The uncertainty increases as we move away from the modes
of distributions pandq. This is shown in plots (c) and (d), where we plot the variance of the estimates as
a function of the location of the sample.
Since KL divergence is the expectation of the log-ratio on samples from pand the high density region of p
exactly matches the high confidence region of MDRE, it is able to consistently estimate the KL divergence
accurately even when pandqare far apart.
E High Dimensional Experiment
In Section 4.2, we showed that MDRE performs better than all baseline models when pandqare high
dimensional Gaussian distributions. Prior work of Rhodes et al. (2020) has considered high dimensional cases
with HOD only, whereas we additionally consider cases with FOD and HOD to provide a more complete
picture. Our results show that MDRE outperforms all other methods on the task of MI estimation as
the function of the estimated density ratio. It is worth noting that MDRE uses only upto 5 auxiliary
distributions that are constructed using the linear mixing scheme and beats TRE substantially on cases
with both FOD and HOD, although TRE uses upto 15 auxiliary distributions also constructed using linear
mixing approach. This demonstrates that our proposal of using the multi-class logistic regression does, in
fact, prevent distribution shifts issues of TRE when both FOD and HOD are present and help estimate the
density ratio more accurately.
We now describe the MDREconfiguration and other setup related details.
19Published in Transactions on Machine Learning Research (03/2023)
Figure 10: Diagnostic plot for a high dimensional experiment.
Figure 11: Diagnostic plot for a high dimensional experiment with randomized means.
20Published in Transactions on Machine Learning Research (03/2023)
Dim MI musing LM
4020 [0.25,0.5,.75]
100 [0.35,0.5,.85]
16040 [0.25,0.5,.75]
136 [0.15,0.35,0.5,.75,.95]
32080 [0.25,0.5,.75]
240 [0.15,0.35,0.5,.75,.95]
Table 6: Configuration of MDREfor the high dimensional experiments. LM stands for Linear Mixing
Auxiliary Distributions: For all the high dimensional experiments throughout this work, we construct
musing the linear mixing scheme as described in Appendix B. Table 6 provides the number Kof auxiliary
distributions along with the exact mixing weights for each of the 6 settings.
As a general principle, we chose these three sets of mixing weights so that their cumulative samples overlap
with the samples of pandqsimilar to how the heavy tailed distribution worked in the 1D case. Please note
that while heavy tailed distributions can effectively bridge pandqwhen they have high FOD. However,
they do not work as well if the discrepancy is primarily HOD. For example consider p=N(0,1e−6)and
q=N(0,1). In this case, setting mto a heavier tailed distribution centered as zero will not be of help. We
needmthat is concentrated at zero but also maintains a decent overlap with q. Linear mixing pandqon
the other hand, mixes first and higher order statistics (second or higher) and therefore, populates samples
that overlap with both pandq. In some cases, we found that a mixture of linear mixing with K= 1can
also be used to estimate the density ratio. However, this requires using a neural network-based classifier and
requires much more tuning of the hyperparameters.
For choosing K, we use a grid search based approach. We monitor the classification accuracy across all the
K+ 2distribution. If this accuracy is very high ( >95% for all classes), this implies that the classification
task is easy and therefore the DRE may suffer from the density chasm issue. On the other hand, if the
classification accuracy is too low ( <50% for all classes), then again, the DRE does not estimate well. We
found that targeting an accuracy curve as shown in Figure 10 (last panel) empirically leads to accurate
density ratio estimation. This curve plots the test accuracy across all the classes and, empirically when it
stays between the low and the high bounds of (50%,95%), the DRE estimates the ratios fairly well. The
first panel shows that MDREestimates the ground truth ratio accurately across samples from all the K+ 2
distributions, the second panel shows that KL estimates of MDREis close to the ground truth KL and the
third panel shows that both test and training losses have converged. Figure 11 shows another example for
the case of randomized means. While MDREalso manages to get the ground truth KL correctly and most
of the ratio estimates are also accurate, it does, however, slightly overestimate the log ratio for some of the
samples from p.
F SpatialMultiOmniglot Experiment
SpatialMultiOmniglot is a dataset of paired images uandv, whereuis an×ngrid of Omniglot characters
from different Omniglot alphabets and vis an×ngrid containing the next characters of the corresponding
characters in u. In this setup, we treat each grid of n×nas a collection of n2categorical random variables,
not the individual pixels. The mutual information I(u,v)can be computed as: I(u,v) =/summationtextn2
i=1logli, where
liis the alphabet size for the ithcharacter in u. This problem allows us to easily control the complexity of
the task since increasing nincreases the mutual information.
For the model, as in TRE, we use a separable architecture commonly used in MI-based representation
learning literature and model the unnormalized log-scores with functions of the form g(u)TWf(u), where
gandfare 14-layer convolutional ResNets (He et al., 2015). We construct the auxiliary distributions via
dimension-wise mixing—exactly the way that TRE does.
21Published in Transactions on Machine Learning Research (03/2023)
(a) Mutual information estimation
 (b) Representation learning accuracy
Figure 12: SpatialMultiOmniglot representation learning results with same encoder for fandg.
To evaluate the representations after learning, we adopt a standard linear evaluation protocol to train a
linear classifier on the output of the frozen encoder g(u)to predict the alphabetic index of each character in
the gridu.
Additional Experiments
In addition to the experiments in the main text, we run an additional experiment with SpatialMultiOmniglot
to test the effect of using the same encoder for gandf(i.e, modeling the unnormalized log-scores with the
formg(u)TWg(v))instead of logp(u,v) =g(u)TWf(v)).
Single Encoder Design: We test the contribution of using two different encoders fandginstead of
one. As seen in Figure 12, in both cases of d= 4,9, the two models reach slightly different but similar MI
estimates, but, interestingly, do not differ at all in the test classification accuracy. Empirically, we also found
that using one encoder helps the model converge to much faster. Overall, this experiment demonstrates that
using two different encoders does not necessarily work to our advantage.
G TRE on Finite Support Distributions for K= 1
ForK= 1, TRE proposes the following telescoping: logp/q= logp/m + logm/q. As such, for TRE to be
well defined,dM
dQi.e. the Radon-Nikodym Derivatives (RND) needs to exist. The consequence of this is that
TRE is only defined when p<<m<<q . However this condition easily breaks if, for example, pandqare
mixtures of finite support distributions except for the trivial case when support of mis exactly equal to the
support ofq.
We now demonstrate this with a specific example in Figure 13. Here we set p= 0.5×TN (−1,0.1,low =
−1.1,high =−0.9) + 0.5×TN (1,0.1,low = 0.9,high = 1.1)andq= 0.5×TN (−1,0.2,low =−1.2,high =
0.8)+0.5×TN (1,0.2,low = 0.8,high = 1), as shown in Figure 13a where TNstands for Truncated Normal
distribution. We set the auxiliary distribution min TRE to m=TN(0,1,low =−1.2,high = 1.2)using
the proposed overlapping distribution construction. As such, p<<q<<m and therefore, TRE is undefined
for the second term asm
qis not defined for samples from mthat are outside the support of q. It can be
clearly seen in Figure 13b thatm
qblows up to very high values on samples from mwhereqdoes not have
any support. Similar examples can be constructed for all the auxiliary distribution construction schemes
proposed in Rhodes et al. (2020).
Please note, despite dM/dQ being undefined, when used with the proposed m, TRE estimates logrp/q
accurately on samples from p. We conjecture that this is because, the numerical estimation of the dM/dQ
is finite over the support of p.
22Published in Transactions on Machine Learning Research (03/2023)
(a) Setup
 (b)dM/dQ
Figure 13:dM/dQon mixtures of distributions with finite support
23