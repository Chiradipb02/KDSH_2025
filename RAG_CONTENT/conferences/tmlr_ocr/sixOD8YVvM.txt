Published in Transactions on Machine Learning Research (06/2023)
Bounded Space Diﬀerentially Private Quantiles
Daniel Alabi alabid@cs.columbia.edu
Columbia University
Omri Ben-Eliezer omrib@mit.edu
Massachusetts Institute of Technology
Anamay Chaturvedi chaturvedi.a@northeastern.edu
Northeastern University
Reviewed on OpenReview: https: // openreview. net/ forum? id= sixOD8YVvM
Abstract
Estimating the quantiles of a large dataset is a fundamental problem in both the streaming
algorithms literature and the diﬀerential privacy literature. However, all existing private
mechanisms for distribution-independent quantile computation require space at least lin-
ear in the input size n. In this work, we devise a diﬀerentially private algorithm for the
quantile estimation problem, with strongly sublinear space complexity, in the one-shot and
continual observation settings. Our basic mechanism estimates any α-approximate quantile
of a length- nstream over a data universe Xwith probability 1−βusingO/parenleftBig
log(|X|/β) logn
α/epsilon1/parenrightBig
space while satisfying /epsilon1-diﬀerential privacy at a single time point. Our approach builds
upon deterministic streaming algorithms for non-private quantile estimation instantiating
the exponential mechanism using a utility function deﬁned on sketch items, while (privately)
sampling from intervals deﬁned by the sketch. We also present another algorithm based on
histograms that is especially well-suited to the multiple quantiles case. We implement our
algorithms and experimentally evaluate them on synthetic and real-world datasets.
1 Introduction
Quantile estimation is a fundamental subroutine in data analysis and statistics. For q∈[0,1], theq-quantile
of a dataset of size nis the element ranked ⌈qn⌉when the elements are sorted from smallest to largest.
Computing a small number of quantiles in a massive data set can serve as a quick and eﬀective sketch of
the “shape” of the data. Quantile estimation also serves an essential role in robust statistics, where data is
generated from some distribution but is contaminated by a non-negligible fraction of outliers, i.e., “out of
distribution” elements that may sometimes even be adversarial. For example, the median (50th percentile) of
a dataset is used as a robust estimator of the mean in such situations where the data may be contaminated.
Locationparameterscanalsobe(robustly)estimatedviatruncationorwinsorization, anoperationthatrelies
on quantile estimation as a subroutine (Tukey, 1960; Huber, 1964). Rank-based nonparametric statistics
can be used for hypothesis testing (e.g., the Kruskal-Wallis test statistic (Kruskal & Wallis, 1952)). Thus,
designing quantile-based or rank-based estimators, whether distribution-dependent or distribution-agnostic,
is important in many scenarios.
Maintaining the privacy of individual users or data items, or even of groups, is an essential prerequisite
in many modern data analysis and management systems. Diﬀerential privacy (DP) is a rigorous and now
well-accepted deﬁnition of privacy for data analysis and machine learning. In particular, there is already a
substantial amount of literature on diﬀerentially private quantile estimation (e.g., see (Nissim et al., 2007;
Asi & Duchi, 2020; Gillenwater et al., 2021; Tzamos et al., 2020)).1
1Robust estimators are also known to be useful for accurate diﬀerentially private estimation; see, e.g., the work of Dwork
and Lei (Dwork & Lei, 2009) in the context of quantile estimation for the interquartile range and for medians.
1Published in Transactions on Machine Learning Research (06/2023)
All previous work either makes certain distributional assumptions about the input, or assumes the ability
to access all input elements (thus virtually requiring a linear or worse space complexity). Such assumptions
may be infeasible in many practical scenarios, where large scale databases have to quickly process streams
of millions or billions of data elements without clear a priori distributional characteristics. The ﬁeld of
streaming algorithms aims to provide space-eﬃcient algorithms for data analysis tasks such as these. These
algorithms typically maintain good accuracy and fast running time while having space requirements that
are substantially smaller than the size of the data. While distribution-agnostic quantile estimation is among
the most fundamental problems in the streaming literature (Agarwal et al., 2013; Felber & Ostrovsky, 2017;
Greenwald & Khanna, 2001; Hung & Ting, 2010; Karnin et al., 2016; Manku et al., 1999; Munro & Paterson,
1980; Shrivastava et al., 2004; Wang et al., 2013), no diﬀerentially private sublinear-space algorithms for the
same task are currently known. Thus, the following question, essentially posed by (Smith, 2011) and (Mir
et al., 2011), naturally arises:
Can we design diﬀerentially private quantile estimators that use space sublinear in the stream
length, have eﬃcient running time, provide high-enough utility, and do not rely on restrictive
distributional assumptions?
Itiswell-known(Munro&Paterson,1980)thatexactcomputationofquantilescannotbedonewithsublinear
space, even where there are no privacy considerations. Thus, one must resort to approximation. Speciﬁcally,
for a dataset of nelements, an α-approximate q-quantile is any element which has rank (q±α)nwhen sorting
the elements from smallest to largest, and it is known that the space complexity of α-approximating quantiles
is˜Ω(1/α)(Munro & Paterson, 1980). In our case, the general goal is to eﬃciently compute α-approximate
quantiles in a (pure or approximate) diﬀerentially private manner.
1.1 Our Contributions
We answer the above question aﬃrmatively by providing theoretically proven algorithms with accompanying
experimental validation for quantile estimation with DP guarantees. The algorithms are suitable for private
computation of either a single quantile or multiple quantiles. Concretely, the main contributions are:
1. Wedevise DPExpGK, adiﬀerentiallyprivatesublinear-spacealgorithmforquantileestimationbasedon
the exponential mechanism. In order to achieve sublinear space complexity, our algorithm carefully
instantiates the exponential mechanism with the basic blocks being intervals from the Greenwald-
Khanna (Greenwald & Khanna, 2001) data structure for non-private quantile estimation, rather
than single elements. We prove general distribution-agnostic utility bounds on our algorithm and
show that the space complexity is logarithmic in n.
2. Wepresent DPHistGK , anotherdiﬀerentiallyprivatemechanismforquantileestimation, whichapplies
the Laplace mechanism to a histogram, again using intervals of the GK-sketch as the basic building
block. We theoretically demonstrate that DPHistGK may be useful in cases where one has prior
knowledge on the input.
3. We extend our results to the continual release setting, wherein we must maintain and output an
estimate of the queried quantile in an online manner as the data is received and processed.
4. We empirically validate our results by evaluating DPExpGK, analyzing and comparing various aspects
of performance on real-world and synthetic datasets.
2 Related Work
2.1 Quantile Approximation of Streams and Sketches
Approximation of quantiles in large data streams (without privacy guarantees) is among the most well-
investigated problems in the streaming literature (Wang et al., 2013; Greenwald & Khanna, 2016; Xiang
et al., 2020). A classical result of Munro and Paterson from 1980 (Munro & Paterson, 1980) shows that
2Published in Transactions on Machine Learning Research (06/2023)
computing the median exactly with ppasses over a stream requires Ω(n1/p)space, thus implying the need
for approximation to obtain very eﬃcient (that is, at most polylogarithmic) space complexity. Manku,
Rajagopalan and Lindsay (Manku et al., 1999) built on ideas from (Munro & Paterson, 1980) to obtain a
randomized algorithm with only O((1/α) log2(nα))forα-approximating all quantiles; a deterministic variant
of their approach with the same space complexity exists as well (Agarwal et al., 2013). The best known
deterministic algorithm is that of Greenwald and Khanna (GK) (Greenwald & Khanna, 2001) on which we
build on in this paper, with a space complexity of O(α−1log(αn))to sketch all quantiles for nelements (up
to rank approximation error of ±αn). A recent deterministic lower bound of Cormode and Veselý (Cormode
& Veselý, 2020) (improving on (Hung & Ting, 2010)) shows that the GK algorithm is in fact optimal among
deterministic (comparison-based) sketches.
Randomization and sampling help for streaming quantiles, and the space complexity becomes independent of
n; an optimal space complexity of O((1/α) log log(1/β))was achieved by Karnin, Lang and Liberty (Karnin
et al., 2016) (for failure probability β), concluding a series of work on randomized algorithms (Agarwal et al.,
2013; Felber & Ostrovsky, 2017; Luo et al., 2016; Manku et al., 1999).
The problem of biased or relative error quantiles, where one is interested in increased approximation accuracy
forverysmallorverylargequantiles, hasalsobeeninvestigated(Cormodeetal.;2006); itwouldbeinteresting
to devise eﬃcient diﬀerentially private algorithms for this problem.
Recall that our approach is based on the Greenwald-Khanna deterministic all-quantiles sketch (Greenwald
& Khanna, 2001). While some of the aforementioned randomized algorithms have a slightly better space
complexity, diﬀerential privacy mechanisms are inherently randomized by themselves, and the analysis seems
somewhat simpler and more intuitive when combined with a deterministic sketch. This, of course, does not
rule out improved private algorithms based on modern eﬃcient randomized sketches (see Section 7).
2.2 Diﬀerential Privacy
Diﬀerentially private single quantile estimation: Works by Nissim, Raskhodnikova and Smith (Nis-
sim et al., 2007) and Asi and Duchi (Asi & Duchi, 2020) improve the trade-oﬀ between accuracy and privacy
byscalingthenoiseaddedforobfuscationinaninstance-speciﬁcmannerformedianestimationintheabsence
of distributional assumptions. Another work by Dwork and Lei (Dwork & Lei, 2009) uses a “propose-test-
release" paradigm to take advantage of local sensitivity; however, as observed in Gillenwater et al. (2021), in
practice the error incurred by this method is relatively large as compared to other works like Tzamos et al.
(2020). The work of Tzamos et al. (2020) achieves the optimal trade-oﬀ between privacy and utility in the
distributional setting, but again as observed by Gillenwater et al. (2021), with a time complexity of O(n4),
this method does not scale well to large data sets.
The work of Gillenwater et al. (2021) shows how to optimize the division of the privacy budget to estimate
mquantiles in a time-eﬃcient manner. For estimation of mquantiles, their time and space complexity are
O(mnlog(n) +m2n)andO(m2n), respectively. They do an extensive experimental analysis and ﬁnd lower
error compared to previous work. However, although they provide intuition for why their method should
incur relatively low error, they do not achieve formal theoretical accuracy guarantees. Kaplan et al. (2022)
improve upon the work of Gillenwater et al. (2021) for the multiple approximate quantiles problem by using
a tree-based (recursive) approach to CDF estimation. However, none of these works deal with quantile
estimation in the sublinear space setting. Böhler & Kerschbaum (2020) solve the problem of estimating the
joint median of two private data sets with time complexity sub-linear in the size of the data-universe and
provide privacy guarantees for small data sets as well as limited group privacy guarantees unconditionally
against polynomially time-bounded adversaries.
Inherent privacy: Another line of work (Blocki et al., 2012; Smith et al., 2020; Choi et al., 2020) demon-
strates that sketching algorithms for streaming problems might have inherent privacy guarantees under
minimal assumptions on the dataset in some cases. For such algorithms, relatively little noise needs to be
added to preserve privacy unconditionally.
3Published in Transactions on Machine Learning Research (06/2023)
3 Preliminaries and Notation
We now give standard diﬀerential privacy notation and formally describe the quantile estimation problem.
We also present the Greenwald-Khanna sketch guarantees in a suitable form.
3.1 Diﬀerential Privacy
Deﬁnition 3.1 (Diﬀerential Privacy (Dwork et al., 2006)) .LetQ:Xn→Rbe a (randomized) mechanism.
For any/epsilon1≥0,δ∈[0,1],Qsatisﬁes (/epsilon1,δ)-diﬀerential privacy if for any neighboring databases (that diﬀer
in one row) x∼x/prime∈Xnand any set S⊆R,
P[Q(x)∈S]≤e/epsilon1P[Q(x/prime)∈S] +δ.
The probability is taken over the coin tosses of Q. We say thatQsatisﬁes pure diﬀerential privacy ( /epsilon1-DP)
ifδ= 0and approximate diﬀerential privacy ( (/epsilon1,δ)-DP) ifδ>0. We can set /epsilon1to be a small constant (e.g.,
between 0.01 and 2) but will require that δ≤n−ω(1)be cryptographically small.
3.2 Quantile Approximation
2345
Totally ordered domain X1222
3 55
6Data set S/prime={1,2,2,3,5,2,6,5}
Figure 1: In the data set S/primeof8elements, the true
0.5quantile is the value 2, and the values 2,3,4and
5are all acceptable 0.25-approximate answers. Note
that although 1is adjacent to 2in the data universe
X, it is not an acceptable output.In this subsection we make some deﬁnitions to for-
malize our analysis.
Deﬁnition 3.2. (Quantiles) Let there be a totally
ordered data universe Xand an input data stream
X= ((x1,1),..., (xn,n))(sometimes implicitly re-
ferred to as X= (x1,...,xn)). For (xi,i)∈X,
letval((xi,i)) =xiandix((xi,i)) =/summationtext
j≤i|{(xj,j) :
xj< xiorxj=xi,j < i}|. We abuse notation to
say that for v1,v2∈X,v1≤v2ifix(v1)≤ix(v2).
Then, theq-quantile of Xisval(v)forv∈Xwith
ix(v) =⌈qn⌉.
We observe that in this setting a value from the data
universecanoccurmultipletimesinaset. Toaccount
for this we deﬁne a range of ranks that any value
can hold; this will be useful when reasoning about
quantile approximation.
Deﬁnition 3.3. (Rank and approximate quantiles)
Forx∈X, we deﬁne rmin(x) =|{v∈X: val(v)<
x}|,rmax(x) =|{v∈X: val(v)≤x}|andrank(X,x)
to be the interval [rmin(x),rmax(x)]. We say that x∈
Xis anα-approximate q-quantile for Xifrank(X,x)∩[⌈qn⌉−αn,⌈qn⌉+αn]/negationslash=∅.
Consider the following example to see how these deﬁnitions play out in practice.
Example 3.4. Given the data set {1,2,2,3,5,2,6,5}(refer to ﬁgure 1), the 0.5quantile is 2, which we
distinguish from the median, which would be the average of the elements ranked 4and5, i.e, 2.5for this
data set. A straightforward way to obtain quantiles is to sort the dataset and the pick the element at
the⌈q·n⌉position. This method only works in the oﬄine (non-streaming) setting. For α= 0.25, the
α-approximate 0.5quantiles are 2,3,4and5. Note that 4, which does not occur in the data set, is still a
valid response, but 1, which occurs in the data set and is even adjacent to the true 0.5quantile 2in the data
universeX, is not a valid response.
We can now formalize the two versions of the problems that are dealt with in the literature on streaming
quantile approximation. Let Dbe any distribution with random variable X∼Dand CDFFX:R→[0,1].
For anyq∈[0,1],Qq
Ddenotes the value xsuch thatFX(x) =q.
4Published in Transactions on Machine Learning Research (06/2023)
Deﬁnition 3.5 (Single Quantile) .Given sample S= (x1,...,xn)in a streaming fashion in arbitrary order,
construct a data structure for computing the quantile Qq
Dsuch that for any q∈[0,1], with probability at
least 1−β,|Qq
D−˜Qq
S|≤α.
Deﬁnition 3.6 (All Quantiles) .Given sample S= (x1,...,xn)in a streaming fashion in arbitrary order,
construct a data structure for computing the quantile Qq
Dsuch that with probability at least 1−β, for all
values ofq∈MwhereM⊂[0,1],|Qq
D−˜Qq
S|≤α.
3.3 Non-private Quantile Streaming
In this subsection, we provide formal guarantees needed for any sketch that our algorithms can build upon:
Lemma 3.7. Given a data stream x1,...,xnof elements drawn from X, there exists a sketching algorithm
that outputs a list S(X)ofs=O/parenleftbig1
αlogαn/parenrightbig
many tuples (vi,gi,∆i)∈(X×N)×N×Nfori= 1,...,ssuch
that ifXis the data multi-set then:
1.rank(X,val(vi))⊂[/summationtext
j≤igj,∆i+/summationtext
j≤igj].
2.gi+ ∆i≤2αn.
3. The ﬁrst tuple is (min{x∈X},1,0)and the last tuple is (max{x∈X},1,0).
4. Theviare sorted in ascending order. Without loss of generality, the lower interval bounds/summationtext
j≤igj
and upper interval bounds ∆i+/summationtext
j≤igjare also sorted in increasing order.
From conditions 1 and 2 above, it follows that the GK sketch can be used to compute α-approximate q-
quantiles by outputting the value vi∗for the unique index i∗such that/summationtext
j≤i∗gj≤qn≤∆i∗+/summationtext
j≤i∗gj.
More generally, any sketch which achieves the properties outlined in Lemma 3.7 can be used in place of GK.
4 Diﬀerentially Private Algorithms
In this section we present our two DP mechanisms for quantile estimation. Throughout, we assume that α
is a user-deﬁned approximation parameter. The goal is to obtain α-approximate q-quantiles.
The new algorithms we introduce are:
(1)DPExpGK (Algorithm 1): An exponential mechanism based (/epsilon1,0)-DP algorithm for computing a single
q-quantile. To solve the all-quantiles problem with approximation factor α, one can run this algorithm
iteratively with target quantile 0,α,2α,.... Doing so requires scaling the privacy parameter in each call by
an additional αfactor which increases the space complexity by a factor of 1/α. We also give an optimized
implementation (algorithm 8) of this algorithm in the appendix. We extend our results to the continual
observation setting (Dwork et al., 2010; Chan et al., 2011).
(2)DPHistGK (Algorithm 2): A histogram based (/epsilon1,0)-DP algorithm (Algorithm 2) for the α-approximate
all-quantiles problem. The privacy guarantee of this algorithm is unconditional, but there is no universal
theoretical utility bound as in the previous algorithm. However, in some cases the utility is provably better:
for example, we show that if the data set is drawn from a normal distribution (with unknown mean and
variance), we can avoid the quadractic 1/α2factor in the sample complexity that we incur when using
DPExpGKGumb for the same all-quantiles task.
The rest of this section contains the descriptions of the two algorithms; we relegate all proofs to the appendix.
4.1 DPExpGKGumb : Exponential Mechanism Based Approach
We ﬁrst establish how the exponential mechanism and the GK sketch may be used in conjunction to solve
the single quantile problem. Concretely, the high level idea is to call the privacy preserving exponential
mechanism with a score function derived from the GK sketch. The exponential mechanism is a fundamental
privacy primitive which when given a public set of choices and a private score for each choice outputs a choice
5Published in Transactions on Machine Learning Research (06/2023)
Algorithm 1: DPExpGK: Exponential Mechanism DP Quantiles : High Level Description
Data:X= (x1,x2,...,xn)
Input:/epsilon1,α(approximation parameter) ,q∈[0,1](quantile parameters), δu(sensitivity)
1S(X) ={(vi,gi,∆i) :i∈[s]}←GK(X,α)
2Deﬁne score function
u(S(X),x) =−min{|y−⌈qn⌉|:y∈[ˆrmin(x),ˆrmax(x)]}. (1)
where
ˆrmin(x) = max{/summationdisplay
j≤igj: val(vi)<x}
ˆrmax(x) = min{∆i+/summationdisplay
j≤igj: val(vi)>x}
3Execute the exponential mechanism with score function u(S(X),·), i.e. choose and output a single
e∈Xwith probability
∝exp/parenleftbigg/epsilon1
2δu·u(S(X),e)/parenrightbigg
.
that with high probability has a score close to optimal whilst preserving privacy. In the course of constructing
our algorithms, we have to resolve two problems; one, how to usefully construct a score function to pass to
the exponential mechanism so that the private value derived is a good approximation to the q-quantile, and
two, how to execute the exponential mechanism eﬃciently on the (possibly massive) data universe X. To
resolve the ﬁrst issue we devise the (not necessarily eﬃcient) routine Algorithm 1; and to resolve the second
one, we run an essentially equivalent but far more eﬃcient routine, Algorithm 8.
Constructing a score function: We recall that the GK sketch returns a short sequence of elements
from the data set with a deterministic interval for their ranks and the promise that for any target quantile
q∈[0,1], there is some sketch element that lies within αnunits in rank of⌈qn⌉. One technicality that we
run into when trying to construct a score function on the data universe Xis that when a single value occurs
with very high frequency in the data set, the ranks of the set of occurrences can span a large interval in
[0,n], and there is no one rank we can ascribe to it so as to compare it with the target rank ⌈qn⌉. This can
be resolved by deﬁning the score for any data domain value in terms of the distance of its respective rank
interval [rmin(x),rmax(x)](formalized in Deﬁnition B.1) from ⌈qn⌉; elements whose intervals lie closer to the
target have a higher score than further away ones.
Eﬃciently executing the exponential mechanism: The exponential mechanism samples one of the
public choices (in our case some element from the data universe X) with probability that increases with the
quality of the choice according to the score function. In general every element can have a possibly diﬀerent
score and the eﬃciency of the exponential mechanism can vary widely depending on the context. In our
setting, the succinctness of the GK sketch leads to a crucial observation: by deﬁning the score function via
the sketch, the data domain is partitioned into a relatively small number of sets such that the score function
is constant on each partition. Concretely, for any two successive elements in the GK sketch, the range of
values in the data universe that lie between them will have the same score according to our score function.
We can hence ﬁrst sample a partition from which to output a value, and then choose a value from within
that interval uniformly at random. To make our implementation even more eﬃcient and easy to use, we
also make use of the Gumbel-max trick that allows us to iterate through the set of choices instead of storing
them in memory - see appendix B.1 and the expanded pseudocode in algorithm 8 for more detail.
On formalizing this outline we get the following formal guarantees for algorithm 1 (please see appendix B.1
for a complete proof). d(·,·)denotes the /lscript1metric on R.
6Published in Transactions on Machine Learning Research (06/2023)
Theorem 4.1. Algorithm 1 is /epsilon1-diﬀerentially private. Let ˆxbe the value returned by Algorithm 1 when
initialized with target quantile q. The following statements hold:
1. Algorithm 1 can be implemented to run with space complexity O((1/α) logαn), such that with prob-
ability 1−β
d(⌈qn⌉,[ˆrmin(ˆx),ˆrmax(ˆx)])≤2αn+2(4αn+ 2) log(|X|/β)
/epsilon1(2)
2. Forn >24 log|X|/β
αmin{/epsilon1,1}, Algorithm 1 can be implemented to run with space complexity
O/parenleftbig
(α/epsilon1)−1log(|X|/β) logn/parenrightbig
such that with probability 1−β,ˆxis anαapproximate q-quantile.
4.2 DPHistGK : Histogram Based Approach
For methods in this section, we assume that we have K≥1disjointbins each of width w(e.g.,w=α/2)
partitioning the data universe. These bins are used to construct a histogram. Essentially, Algorithm 2
builds an empirical histogram based on the GK sketch, adds noise so that the bin values satisfy (/epsilon1,0)-DP,
and converts this empirical histogram to an approximate empirical CDF, from which the quantiles can be
approximately calculated. We demonstrate one use case of DPHistGK where the space complexity required
improves upon the worst-case bound for DPExpGK, Theorem 4.1. While the histogram based mechanism does
not have universal utility bounds in the spirit of the above theorem, the results in this section serve as one
simple example where it may yield desirable accuracy while using less space.
Algorithm 2: DPHistGK : Computing DP Quantiles in Bounded Space
Data:X= (x1,x2,...,xn)
Input:/epsilon1,α(approximation parameter) ,q∈[0,1](quantile parameters) ,w
1Build summary sketch S(X)where
2S(X) ={(vi,gi,∆i) :i∈[s]}←GK(X,α)
/* cell labels aiand counts ci= 0 */
3Initialize data-agnostic (empty) histogram Hist =/angbracketleft(ai,ci),.../angbracketrightwith cell widths w
4for(vi,gi,∆i)∈S(X)do
5Insertgicounts ofviinto histogram Hist
6c= 0
7H= [·]/* initialize empty list */
8for(ai,ci)∈Histdo
9 ˜ci= max(0,ci+ Lap(0,2//epsilon1))
10Append (ai,c+ ˜ci)toH
11c=c+ ˜ci
12r=⌈q·n⌉
13for(b,rank )∈Hdo
14ifr<rankthen
15 returnb
/* return last element of H */
16returnH[|H|−1]
Suppose that we are given an i.i.d. sample S= (X1,X2,...,Xn)such that for all i∈[n],Xi∼N(µ,σ2Id×d),
µ∈Rd. The goal is to estimate DP quantiles of the distribution N(µ,σ2Id×d)without knowledge of µor
σ2. We will show how to estimate the quantiles assuming that σ2is known. Note that it is easy to generalize
the work to the case where σ2is unknown as follows:
For any sample Sdrawn from i.i.d. from N(µ,σ2Id×d), the 1−βconﬁdence interval is
¯X±σ√n·z1−β/2,
7Published in Transactions on Machine Learning Research (06/2023)
wherez1−β/2is the 1−β/2quantile of the standard normal distribution and ¯Xis the empirical mean. The
length of this interval is ﬁxed and equal to2σz1−β/2√n= Θ/parenleftBig
σ√n/radicalBig
log1
β/parenrightBig
. In the case where σ2is unknown,
the conﬁdence interval becomes ¯X±s√n·tn−1,1−β/2, wheres2=1
n−1/summationtextn
i=1(Xi−¯X)2is the sample variance
(sample estimate of σ2) andtn−1,1−β/2is the 1−β/2quantile of the t-distribution with n−1degrees of
freedom. The length of the interval can be shown to be
2σ√n·kn·tn−1,1−β/2= Θ/parenleftbiggσ√n/radicalbigg
log1
β/parenrightbigg
,
wherekn= 1−O(1/n)is an appropriately chosen constant (see (Lehmann & Romano, 2005; Karwa &
Vadhan, 2018; Keener, 2010) for more details and discussion). We can hence assume that σ2is known and
proceed to show sample and space complexity bounds.2
For anyq∈(0,1), we denote the q-quantile of the sample S= (X1,X2,...,Xn)asQq
Sand theq-quantile of
the distribution as Qq
D. With this notation, for some β∈(0,1]andα>0, we wish to obtain a DP q-quantile
˜Qq
Ssuch that
P[/bardblQq
D−˜Qq
S/bardbl≥α]≤β.
We shall proceed in a three-step approach: (1) Estimate a DP range of the population in sub-linear space; (2)
Use this range of the population to construct a DP histogram using the stream S; (3) Use the DP histogram
to estimate one or more quantiles via the sub-linear data structure of Greenwald and Khanna. Our main
formal result for algorithm 2 can be summarized as follows (please see appendix B.2 for a complete proof).
Theorem 4.2. For the one-dimensional normal distribution N(µ,σ2), letS= (X1,X2,...,Xn)be a data
stream through which we wish to obtain ˜Qq
S, a DP estimate of the q-quantile of the distribution.
For anyq∈(0,1), there exists an (/epsilon1,δ)-DP algorithm such that, with probability at least 1−β, we obtain
|Qq
D−˜Qq
S|≤αfor anyα>0,β∈(0,1],/epsilon1,δ∈(0,1/n)and for stream length
n≥max{min{A,B},C},where
A=O/parenleftbiggR
/epsilon1σαlogR
σβ/parenrightbigg
,B=O/parenleftbiggR
/epsilon1σαlog1
βδ/parenrightbigg
,C=O/parenleftbiggR2
σ2α2log1
β/parenrightbigg
as long asµ∈(−R,R)and using space of O(max{R
σ,1
αlogαn}).
In the case where d= 1, by Theorem 4.2, there exists an (/epsilon1,δ)-DP algorithm ˜Qq
Ssuch that if µ∈(−R,R)
then using space of O(max{R
σ,1
αlogαn})(with probability 1) as long as the stream length nis at least
Ω/parenleftbigg
max/braceleftbigg
min/braceleftbigg
O/parenleftbiggR
/epsilon1σαlogR
σβ/parenrightbigg
,O/parenleftbiggR
/epsilon1σαlog1
βδ/parenrightbigg/bracerightbigg
,O/parenleftbiggR2
σ2α2log1
β/parenrightbigg/bracerightbigg/parenrightbigg
,
we get the guarantee that, for all β∈(0,1],α > 0,P[/bardblQq
D−˜Qq
S/bardbl ≥α]≤β. Intuitively, this means
that: (1)Space: We need less space to estimate any quantile with DP guarantees if the distribution is less
concentrated (i.e., σcan be large) or if we do not require a high degree of accuracy for our queries (i.e., α
can be large). (2) Stream Length : We need a large stream length to estimate quantiles if we require a
high degree of accuracy (i.e., smaller β,α), or do not have a good public estimate of µ(largeR), or have
small privacy parameters (small /epsilon1,δ), or have concentrated datasets (small σ).
5 Continual Observation
We now describe how our one-shot approach can be used as a black box to obtain a continual observation
solution (Dwork et al., 2010; Chan et al., 2011). In the absence of privacy, we recall that an α/2-approximate
solution for the q-quantile problem on a stream of length nallows for an additive error of αn/2in the rank of
any candidate q-quantile solution. It follows that if we append any arbitrary αn/2-many elements to the end
2One could also estimate the variance in a DP way and then prove the complexity bounds.
8Published in Transactions on Machine Learning Research (06/2023)
Algorithm 3: Continual Observation DP Quantiles
Data:Input stream X= (x1,...,xn)for somen>n min= Ω/parenleftBig
1
α2/epsilon1lognlog/parenleftBig
|X|logn
αβ/parenrightBig/parenrightBig
, privacy
parameter/epsilon1, approximation parameter α, target quantile q
1, failure probability βLets= 0
2/epsilon1∗←O(α/epsilon1/logn)
3α∗←α/2
4β∗←logn
αβ
5cp(s)←nmin/* stores stream checkpoints */
6Instantiate GK←GK(α∗)
7vs←⊥ /* holdsα-approximate q-quantile */
8forstream element xs∈Xdo
9s←s+ 1
10 GK.insert (xs)
11ifs=⌈cp(s−1)⌉then
12vs←DPExpGK (GK,/epsilon1∗,α∗,q,β∗)
13 cp(s)←cp(s−1)(1 +α/2)
14else
15vs←vs−1
16 cp(s)←cp(s−1)
of the stream, these new elements can lead to an additional additive error of at most αn/2. This suggests
that on releasing a DP quantile estimate at any point in the stream, we can avoid updating our q-quantile
estimate for a proportionate number of additional elements whilst maintaining a (1+α)-approximation. This
is essentially the idea behind the ﬂip number used in the study of adversarially robust streaming algorithms.
At a high level, the ﬂip number is a measure of the number of times the output of an online algorithm
changes by more than a (1±α)multiplicative factor (see (Ben-Eliezer et al., 2020) for more detail).
Building on this idea, we can show that we only need update our estimate after every additional α/2-fraction
as many elements are added as were present at the previous estimate. At the beginning of this process,
i.e. when no stream elements have been processed, for the ﬁrst few elements ( nmin-many, as described in
the pseudo-code) we will need to update our online estimate every time or omit producing any estimate.
However, after this short warm-up preﬁx of the stream, a relatively small number of estimates serve as
(1 +α)-approximate qquantiles for every point in the remainder of the stream.
Accounting for the additional requirement of privacy is now easy - we merely need to privatize the estimate
at each of the checkpoints , which are the points in the stream where the quantile estimate must be updated.
Fornelements in all there are O/parenleftbig
log1+αn
nmin/parenrightbig
many such elements, which for a suitable choice of nmin
simpliﬁes essentially to O/parenleftbiglogn
/epsilon1α/parenrightbig
. In other words we see that the privacy loss scales only with the logarithm
of the length of the stream because of the relatively few checkpoints that occur. The formal statement is as
below, and we present a complete proof in appendix B.3.
Theorem 5.1. Let/epsilon1,α > 0,n∈Z. For anyβ∈(0,1], with probability ≥1−β, Algorithm 3 maintains
anα-approximate q-quantile at every point sin the data stream for s= Ω/parenleftBig
lognlog|X|/β
α2/epsilon1/parenrightBig
. Furthermore,
Algorithm 3 satisﬁes /epsilon1-DP and has space complexity Ω/parenleftBig
1
α2/epsilon1log2nlog/parenleftBig
|X|logn
αβ/parenrightBig/parenrightBig
.
We conclude by mentioning that our continual observation solution incurs roughly a O(logn/α)overhead
in the space complexity, which is in line with classical works in diﬀerential privacy and adversarially robust
streaming (Ben-Eliezer et al., 2020; Dwork et al., 2010). Concurrently, Stemmer and Kaplan (Kaplan &
Stemmer, 2021) developed a notion of streaming sanitizers which yields a continual observation guarantee
“for free”, without incurring such an overhead over the one-shot case.
9Published in Transactions on Machine Learning Research (06/2023)
1234
·10510−610−4DPExpGKGumb
DPExpFull
(a) Error vs. stream
length for Uniform dis-
tribution.1234
·105105106107
DPExpGKGumb
DPExpFull
(b) Data structure sizes
vs. stream length for
Uniform distribution.1234
·10510−610−4DPExpGKGumb
DPExpFull
(c) Error vs. stream
length for Normal dis-
tribution.1234
·105105106107
DPExpGKGumb
DPExpFull
(d) Data structure sizes
vs. stream length for
Normal distribution.
Figure 2: DPExpGKGumb versus DPExpFull for uniform or normally distributed data, α= 10−4,/epsilon1= 1,q= 0.5
6 Experimental Evaluation
In this section, we experimentally evaluate our sublinear-space exponential-based mechanism, DPExpGKGumb .
We study how well our algorithm performs in terms of accuracy and space usage. This section validates
the theoretical results in the prequel - we ﬁnd that the space complexity of the algorithm is indeed very
small in practice, and that the accuracy is typically closely tied to the approximation parameter α. Our
main baselines will be DPExpFull , which applies the exponential mechanism on the full data set without
any sketching algorithms (see appendix C.1 for further implementation details), and the true quantile value.
The true quantile values are used to compute relative error , the absolute value of the diﬀerence between
the estimated quantile and the true quantile, divided by the standard deviation of the data set. We graph
the mean relative error over 100 trials of the exponential mechanism per experiment, as well as the 80%
conﬁdence interval computed by taking the 10th and 90th percentile. We ﬁx the target quantile to be
estimated to q= 0.5(the median) throughout as the relative error and the space usage generally do not
seem to vary much with choice of q.
6.1 Synthetic data sets
In this subsection, we compare our methods on synthetically generated datasets. We vary the parameters of
theStream Length n(the size of the input stream x1,...,xn), theApproximation Parameter α(the
approximation factor used by the internal GK sketch), and the Data Distribution . We generate data from
uniform and Gaussian distributions; we use a uniform distribution in range [0,1](i.e.,U(0,1)) or a normal
distribution with mean 0and variance 1(i.e.,N(0,1)), clipped to the interval [−10,10].3.
We show results on space usage and relative error (both plotted mostly on logarithmic scales) from non-DP
estimates, as we vary the parameters listed above.
In Figure 2, we vary the stream length nfor an approximation factor of α= 10−4. The streams are either
normally or uniformly distributed. In Figures 2a and 2b, we compare DPExpGKGumb (space strongly sublinear
inn) vs. DPExpFull (uses space of O(n)) in terms of space usage and accuracy. In general we ﬁnd that
although our method DPExpGKGumb incurs higher error, in absolute terms it remains quite small and the 95%
conﬁdence intervals tend to be adjacent for DPExpGKGumb andDPExpFull . However, there is a clear trend of
an exponential gap developing between their respective space usages which is a natural consequence of the
space complexity guarantee of the GK sketch. This holds for both distributions studied.
3Clipping is required for the exponential mechanism, as it must operate on some bounded interval of values. In any case,
we never expect to see samples from N(0,1)that lie outside [−10,10]for any practical purpose; the probability for any given
sample to satisfy this is minuscule, at about ≈10−21.
10Published in Transactions on Machine Learning Research (06/2023)
1234
·10510−610−410−2DPExpGKGumb
DPExpFull
(a) Uniform distribu-
tion: Error vs. stream
length1234
·105101103105DPExpGKGumb
DPExpFull
(b) Data structure sizes
vs. stream length1234
·10510−510−310−1DPExpGKGumb
DPExpFull
(c) Normal distribu-
tion: Error vs stream
length1234
·105101103105DPExpGKGumb
DPExpFull
(d) Data structure sizes
vs. stream length
Figure 3: DPExpGKGumb versus DPExpFull for uniform or normally distributed data, α= 10−1,/epsilon1= 1,q= 0.5
1234
·10502·10−44·10−4Uniform
Normal
(a) Relative error vs.
datastream size for α=
10−4,/epsilon1= 1,q= 0.51234
·10500.20.4Uniform
Normal
(b) Relative error vs.
datastream size for α=
10−1,/epsilon1= 1,q= 0.50.10.51500.00050.0010.0015Uniform
Normal
(c) Approx. factor
α= 10−40.10.51500.10.30.5Uniform
Normal
(d) Approx. factor
α= 10−1
Figure 4: Relative error versus approximation factor.
In Figure 3, we vary the stream length for a relatively large approximation factor of 0.1. Here we see that
compared to the non-approximate method we incur far higher error, although there is also a concomitant
increase in the space savings. This is not a typical use-case since the non-private error can itself be large,
but we get a complete picture of how space usage and performance vary with this user-deﬁned parameter.
In Figures 4c and 4d, we vary the approximation factor. In the small approximation setting we see the
inverse tendency of accuracy with privacy which is characteristic of most DP algorithms. However, in the
large approximation setting, there is no such clear drop in performance with more privacy. This motivates
the question of determining the true interplay between the approximation factor αand the private parameter
/epsilon1, as discussed further in Section 7.
Our algorithm performs well in practical settings where one wishes to estimate some quantity across all data
items privately and using small space. For example, our results indicate that choosing an approximation
factor ofα= 10−4induces an error which is also of order about 10−4for privately computing parameters
chosen according to a uniform or normal distribution, all while saving orders of magnitude in the space
complexity.
6.2 Real-World Datasets
We repeat our investigation of the utility and space complexity comparison between DPExpGKGumb and
DPExpFull with the following real-world data sets (Dua & Graﬀ, 2017): (1) Taxi Service Trajectory :
A dataset from the UCI machine learning repository describing trajectories performed by all 442 taxis (at
4The 80% CI for error incurred by DPExpFull was entirely supported on the point 0and drops oﬀ axis as we use the log
scale.
11Published in Transactions on Machine Learning Research (06/2023)
0.10.51510−410−21001E-2
1E-5
DPExpFull
(a)Taxidataset: Error
versus privacy parame-
ter/epsilon1forDPExpGKGumb
for approximation α=
10−2and 10−5, and
DPExpFull .exact 1E-21E-5103104105106
(b) Taxi data set:
Space used by
DPExpGKGumb forα
= 10−2and 10−5,
and DPExpFull (ab-
breviated “exact” for
brevity).0.10.51510−410−21001E-2
1E-5
DPExpFull
(c) Gas sensor data
set: Error versus
privacy parameter /epsilon1
for DPExpGKGumb for
approximation α =
10−2and 10−5, and
DPExpFull4.exact 1E-21E-5103105107
(d) Gas sensor data
set: Space used by
DPExpGKGumb forα
= 10−2and 10−5,
and DPExpFull (ab-
breviated “exact” for
brevity).
Figure 5: Taxi and Gas sensor data sets.
the time) in the city of Porto in Portugal (Moreira-Matias et al., 2013). This dataset contains real-valued
attributes with about 1.5 million instances. (2) Gas Sensor Dataset : A UCI repository dataset containing
recordings of 16 chemical sensors exposed to varying concentrations of two gas mixtures (Fonollosa et al.,
2015). The sensor measurements are acquired continuously during a 12-hour time range and contains about
4 millions instances. We pick a real-valued attribute from each dataset (the TIMESTAMP and the ﬁrst
ETHYLENE_CO gas sensor value, respectively) and calculate the median on these datasets. The results
are reported in Figures 5a and 5b, and Figures 5c and 5d respectively.
We see that the larger the approximation factor, the larger the space savings are with DPExpGK. Comparing
DPExpFull toDPExpGK, we see space savings of 2 times up to 1000 times as we vary the approximation factor.
These results are consistent with our expectations that the space savings are inversely proportional to the
allowed approximation factor.
7 Conclusion & Future Work
In this work, we presented sublinear-space and diﬀerentially private algorithms for approximately estimating
quantiles in a dataset. Our solutions are two-part: one based on the exponential mechanism and eﬃciently
implemented via the use of the Gumbel distribution; the other based on constructing histograms. Our
algorithms are supplemented with theoretical utility guarantees. Furthermore, we experimentally validate
our methods on both synthetic and real-world datasets. Our work leaves room for further exploration in
various directions.
Interplay between αand/epsilon1:The space complexity bounds we obtain are (up to lower order terms) in-
versely linear in αand in/epsilon1. While it is either known or easy to show that such linear dependence
in each of these parameters in itself is necessary, it is not clear whether the α−1/epsilon1−1term in Theo-
rem 4.1 can be replaced with, say, α−1+/epsilon1−1. Such an improvement, if possible, seems to require
substantially modifying the baseline Greenwald-Khanna sketch or adding randomness.
Alternative streaming baselines: We base our mechanisms upon the GK-sketch, which is known to
be space-optimal among deterministic streaming algorithms for quantile approximation. The use
of a deterministic baseline simpliﬁes the analysis and the overall solution, but better randomized
streaming algorithms for the same problem are known to exist. What would be the beneﬁt of
working, e.g., with the (optimal among randomized algorithms) KLL-sketch (Karnin et al., 2016)?
Dependence in universe size: Thedependenceofourspacecomplexityboundsinthesizeoftheuniverse,
X, is logarithmic. Recent work of Kaplan et al. (Kaplan et al., 2020) (see also (Bun et al., 2015)) on
12Published in Transactions on Machine Learning Research (06/2023)
the sample (not space) complexity of privately learning thresholds in one dimension, a fundamental
problem at the intersection of learning theory and privacy, demonstrate a bound polynomial in
log∗|X|on the sample complexity. As quantile estimation and threshold learning are closely related
problems, this raises the question of whether techniques developed in the aforementioned papers can
improve the dependence on |X|in our bounds.
Random order: The results presented here (except for those about normally distributed data) all assume
that the data stream is presented in worst case order, an assumption that may be too strong for
some scenarios. Can improved bounds be proved when the data elements are chosen in advance but
their order is chosen randomly? This can serve as a middle ground between the most general case
(which we address in this paper) and the case where data is assumed to be generated according to
a certain distribution.
8 Acknowledgements
D.A. was supported by a Junior Fellowship from the Simons Foundation Society of Fellows, Cooperative
Agreement CB20ADR0160001 with the U.S. Census Bureau, and a Fellowship from Meta AI. Most of this
work was done while he was a Ph.D. student at Harvard University. A.C. was supported in part by NSF
CAREER grant 1750716.
References
Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. Perturbation Techniques in Online Learning and Opti-
mization , pp. 233–264. 2017. 23
Pankaj K. Agarwal, Graham Cormode, Zengfeng Huang, Jeﬀ M. Phillips, Zhewei Wei, and Ke Yi. Mergeable
summaries. ACM Trans. Database Syst. , 38(4):26:1–26:28, 2013. 2, 3
Hilal Asi and John C. Duchi. Near instance-optimality in diﬀerential privacy. CoRR, abs/2005.10630, 2020.
1, 3
Victor Balcer and Salil P. Vadhan. Diﬀerential privacy on ﬁnite computers. In 9th Innovations in Theoretical
Computer Science Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA , volume 94 of
LIPIcs, pp. 43:1–43:21. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2018. 23
Omri Ben-Eliezer, Rajesh Jayaram, David P. Woodruﬀ, and Eylon Yogev. A framework for adversarially
robust streaming algorithms. In Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on
Principles of Database Systems, PODS 2020, Portland, OR, USA, June 14-19, 2020 , pp. 63–80, 2020. 9
Jeremiah Blocki, Avrim Blum, Anupam Datta, and Or Sheﬀet. The Johnson-Lindenstrauss transform itself
preserves diﬀerential privacy. In 53rd Annual IEEE Symposium on Foundations of Computer Science,
FOCS 2012, New Brunswick, NJ, USA, October 20-23, 2012 , pp. 410–419. IEEE Computer Society, 2012.
3
Jonas Böhler and Florian Kerschbaum. Secure sublinear time diﬀerentially private median computation. In
27th Annual Network and Distributed System Security Symposium, NDSS 2020, San Diego, California,
USA, February 23-26, 2020 , 2020. 3
Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Diﬀerentially private release and learning of
threshold functions. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015,
Berkeley, CA, USA, 17-20 October, 2015 , pp. 634–649, 2015. 12, 25
T.-H. Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Trans.
Inf. Syst. Secur. , 14(3):26:1–26:24, 2011. 5, 8
Seung Geol Choi, Dana Dachman-Soled, Mukul Kulkarni, and Arkady Yerukhimovich. Diﬀerentially-private
multi-party sketching for large-scale statistics. Proc. Priv. Enhancing Technol. , 2020(3):153–174, 2020. 3
13Published in Transactions on Machine Learning Research (06/2023)
Graham Cormode and Pavel Veselý. A tight lower bound for comparison-based quantile summaries. In
Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems ,
PODS’20, pp. 81–93. Association for Computing Machinery, 2020. 3
Graham Cormode, Zohar S. Karnin, Edo Liberty, Justin Thaler, and Pavel Veselý. Relative error stream-
ing quantiles. In Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of
Database Systems , PODS’21, pp. 96–108. Association for Computing Machinery. 3
Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. Space- and time-eﬃcient deter-
ministic algorithms for biased quantiles over data streams. In Proceedings of the Twenty-Fifth ACM
SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, June 26-28, 2006, Chicago,
Illinois, USA , pp. 263–272, 2006. 3
Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017. 11
A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of the sample distribution function
and of the classical multinomial estimator. Ann. Math. Statist. , 27(3):642–669, 09 1956. doi: 10.1214/
aoms/1177728174. 24
Cynthia Dwork and Jing Lei. Diﬀerential privacy and robust statistics. In Proceedings of the 41st Annual
ACM Symposium on Theory of Computing, STOC 2009, Bethesda, MD, USA, May 31 - June 2, 2009 ,
pp. 371–380, 2009. 1, 3
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of Cryptography, Third Theory of Cryptography Conference, TCC 2006,
New York, NY, USA, March 4-7, 2006, Proceedings , pp. 265–284, 2006. 4, 26
Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N. Rothblum. Diﬀerential privacy under continual ob-
servation. In Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge,
Massachusetts, USA, 5-8 June 2010 , pp. 715–724. ACM, 2010. 5, 8, 9
David Felber and Rafail Ostrovsky. A randomized online quantile summary in o((1/ /epsilon1) log(1//epsilon1)) words.
Theory Comput. , 13(1):1–17, 2017. 2, 3
Jordi Fonollosa, Sadique Sheik, Ramón Huerta, and Santiago Marco. Reservoir computing compensates
slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring.
Sensors and Actuators B: Chemical , 215:618–629, 2015. ISSN 0925-4005. 12
Jennifer Gillenwater, Matthew Joseph, and Alex Kulesza. Diﬀerentially private quantiles. In Proceedings of
the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning
Research , pp. 3713–3722. PMLR, 2021. 1, 3
Michael Greenwald and Sanjeev Khanna. Space-eﬃcient online computation of quantile summaries. In
Proceedings of the 2001 ACM SIGMOD international conference on Management of data, Santa Barbara,
CA, USA, May 21-24, 2001 , pp. 58–66, 2001. 2, 3, 17, 19
Michael Greenwald and Sanjeev Khanna. Power-conserving computation of order-statistics over sensor net-
works. In Proceedings of the Twenty-third ACM SIGACT-SIGMOD-SIGART Symposium on Principles of
Database Systems, June 14-16, 2004, Paris, France , pp. 275–285, 2004. 25
Michael B. Greenwald and Sanjeev Khanna. Quantiles and equi-depth histograms over streams. In Data
Stream Management - Processing High-Speed Data Streams , pp. 45–86. 2016. 2
Peter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics , 35(1):
73 – 101, 1964. doi: 10.1214/aoms/1177703732. 1
Regant Y. S. Hung and Hing-Fung Ting. An ω(1//epsilon1log(1//epsilon1))space lower bound for ﬁnding epsilon-
approximate quantiles in a data stream. In Frontiers in Algorithmics, 4th International Workshop, FAW
2010, Wuhan, China, August 11-13, 2010. Proceedings , volume 6213of Lecture Notes in Computer Science ,
pp. 89–100. Springer, 2010. 2, 3
14Published in Transactions on Machine Learning Research (06/2023)
Haim Kaplan and Uri Stemmer. A note on sanitizing streams with diﬀerential privacy, 2021. 9, 24
Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, and Uri Stemmer. Privately learning thresholds:
Closing the exponential gap. In Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual
Event [Graz, Austria] , volume 125 of Proceedings of Machine Learning Research , pp. 2263–2285. PMLR,
2020. 12
Haim Kaplan, Shachar Schnapp, and Uri Stemmer. Diﬀerentially private approximate quantiles. In Pro-
ceedings of the 39th International Conference on Machine Learning , Proceedings of Machine Learning
Research, pp. 10751–10761. PMLR, 2022. 3
Zohar S. Karnin, Kevin J. Lang, and Edo Liberty. Optimal quantile approximation in streams. In IEEE 57th
Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency,
New Brunswick, New Jersey, USA , pp. 71–78, 2016. 2, 3, 12
Vishesh Karwa and Salil Vadhan. Finite Sample Diﬀerentially Private Conﬁdence Intervals. In Anna R.
Karlin (ed.), 9th Innovations in Theoretical Computer Science Conference (ITCS 2018) , volume 94 of
Leibniz International Proceedings in Informatics (LIPIcs) , pp. 44:1–44:9, 2018. 8, 25
R.W. Keener. Theoretical Statistics: Topics for a Core Course . Springer Texts in Statistics. Springer New
York, 2010. 8
William H. Kruskal and W. Allen Wallis. Use of ranks in one-criterion variance analysis. Journal of the
American Statistical Association , 47(260):583–621, 1952. doi: 10.1080/01621459.1952.10483441. 1
Erich L. Lehmann and Joseph P. Romano. Testing Statistical Hypotheses . Springer Texts in Statistics.
Springer New York, New York, NY, 3. edition, 2005. ISBN 9780387276052. 8
Ge Luo, Lu Wang, Ke Yi, and Graham Cormode. Quantiles over data streams: experimental comparisons,
new analyses, and further improvements. VLDB J. , 25(4):449–472, 2016. 3
Gurmeet Singh Manku, Sridhar Rajagopalan, and Bruce G. Lindsay. Random sampling techniques for space
eﬃcient online computation of order statistics of large datasets. In SIGMOD 1999, Proceedings ACM
SIGMOD International Conference on Management of Data, June 1-3, 1999, Philadelphia, Pennsylvania,
USA, pp. 251–262, 1999. 2, 3
Frank McSherry and Kunal Talwar. Mechanism design via diﬀerential privacy. In 48th Annual IEEE
Symposium on Foundations of Computer Science (FOCS 2007), October 20-23, 2007, Providence, RI,
USA, Proceedings , pp. 94–103, 2007. 20, 21
Darakhshan J. Mir, S. Muthukrishnan, Aleksandar Nikolov, and Rebecca N. Wright. Pan-private algorithms
via statistics on sketches. In Proceedings of the 30th ACM SIGMOD-SIGACT-SIGART Symposium on
Principles of Database Systems, PODS 2011, June 12-16, 2011, Athens, Greece , pp. 37–48, 2011. 2
Ilya Mironov. On signiﬁcance of the least signiﬁcant bits for diﬀerential privacy. In the ACM Conference on
Computer and Communications Security, CCS’12, Raleigh, NC, USA, October 16-18, 2012 , pp. 650–661,
2012. 23
Luís Moreira-Matias, João Gama, Michel Ferreira, João Mendes-Moreira, and Luís Damas. Predicting taxi-
passenger demand using streaming data. IEEE Trans. Intell. Transp. Syst. , 14(3):1393–1402, 2013. 12
J.I. Munro and M.S. Paterson. Selection and sorting with limited storage. Theoretical Computer Science ,
12(3):315–323, 1980. ISSN 0304-3975. doi: https://doi.org/10.1016/0304-3975(80)90061-4. 2, 3
Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and sampling in private
data analysis. In Proceedings of the 39th Annual ACM Symposium on Theory of Computing, San Diego,
California, USA, June 11-13, 2007 , pp. 75–84, 2007. 1, 3
15Published in Transactions on Machine Learning Research (06/2023)
Nisheeth Shrivastava, Chiranjeeb Buragohain, Divyakant Agrawal, and Subhash Suri. Medians and beyond:
New aggregation techniques for sensor networks. In Proceedings of the 2nd International Conference on
Embedded Networked Sensor Systems , SenSys ’04, pp. 239–249. Association for Computing Machinery,
2004. ISBN 1581138792. 2
Adam D. Smith. Privacy-preserving statistical estimation with optimal convergence rates. In Proceedings
of the 43rd ACM Symposium on Theory of Computing, STOC 2011, San Jose, CA, USA, 6-8 June 2011 ,
pp. 813–822, 2011. 2, 21
Adam D. Smith, Shuang Song, and Abhradeep Thakurta. The Flajolet-Martin sketch itself preserves diﬀer-
ential privacy: Private counting with minimal space. In NeurIPS , 2020. 3
J. W. Tukey. A survey of sampling from contaminated distributions. Contributions to Probability and
Statistics , pp. 448–485, 1960. 1
Christos Tzamos, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Ilias Zadik. Optimal private median
estimation under minimal distributional assumptions. In NeurIPS , 2020. 1, 3
Salil P. Vadhan. The complexity of diﬀerential privacy. In Tutorials on the Foundations of Cryptography ,
pp. 347–450. 2017. 25
Lu Wang, Ge Luo, Ke Yi, and Graham Cormode. Quantiles over data streams: an experimental study.
InProceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2013,
New York, NY, USA, June 22-27, 2013 , pp. 737–748, 2013. 2
Zhuolun Xiang, Bolin Ding, Xi He, and Jingren Zhou. Linear and range counting under metric-based local
diﬀerential privacy. In IEEE International Symposium on Information Theory, ISIT 2020 , pp. 908–913,
2020. 2
A Greenwald-Khanna Sketch
For completeness of our algorithm’s description, we specify the operations in the Greenwald-Khanna (GK)
non-private sketch. Throughout, we will use n=n(t)to denote the number of elements encountered up to
timet∈Z+. Some of the operations outlined here will be used a subroutines for the DP procedures.
A.1 The Sketch
LetX= (x1,x2,...,xn)be a stream of items and S(X)be the resulting sketch with size sublinear in n.
The GK sketch stores
S(X) =/angbracketleftt0,t1,...,ts−1/angbracketright,∀i∈{0,...,s−1},ti= (vi,gi,∆i),
wheregi=rmin(vi)−rmin(vi−1)and∆i=rmax(vi)−rmin(vi). We reserve v0,vs−1be denote the smallest
and largest elements seem in the stream X, respectively. We use S(X)[i]to refer to the ith tuple in the
sketchS(X). i.e., for any i,S(X)[i] =ti= (vi,gi,∆i).
Implicitly, the goal is to (implicitly) maintain bounds rmin(v)andrmax(v)for everyvinS(X).rmin(v)
andrmax(v)are the lower and upper bounds on the rank of vamongst all items in X, respectively. We can
compute these bounds as follows:
rmin(vi) =/summationdisplay
j≤igj, rmin(vi) =/summationdisplay
j≤igj+ ∆i.
As a result, gi+ ∆i−1is an upper bound on the number of items between vi−1andvi. In addition,
n=/summationtext
igi.
16Published in Transactions on Machine Learning Research (06/2023)
The sketch is built in such a way to guarantee (maximum) error of maxs−1
i=0(gi+ ∆i)/2for approximately
computing any quantile using the sketch.
We will also impose a tree structure over tuples in S(X)(mostly because of the merge procedure) as follows:
the treeT(X)associated with S(X)has a node Vifor eachti. The parent of a node Viis the node Vjsuch
thatjis the smallest index greater than iwith band (tj)>band (ti).
band (ti)is the band of ∆iat timenand band τ(n)as all tuples that had band value of τ. All possible values
of∆are denoted as bands and it can take on values between
(0,1
22αn,3
42αn,...,2i−1
2i2αn,..., 2αn−1,2αn)corresponding to capacities of (2αn,αn,..., 8,4,2,1).
A.2 Quantile
Algorithm 4 computes the α-approximate q-quantile based on the sketch S(X)that has size that is sublinear
inn.
The algorithm goes through all tuples and checks if the condition max(r−rmin(vi),rmax(vi)−r)≤αn
is satisﬁed and return (i,vi)as the representative approximate quantile. This algorithm will be used a
subroutine for one or more of our diﬀerentially private algorithms.
Lemma A.1 (Proposition 1 & Corollary 1 (Greenwald & Khanna, 2001)) .If after receiving nitems in
the stream, the sketch S(X)satisﬁes the property maxi(gi+ ∆i)≤2αn, then Algorithm 4 returns an α-
approximate q-quantile.
Proof.The algorithm computes r=⌈qn⌉. Then the condition max(r−rmin(vi),rmax(vi)−r)≤αnclearly
is (by deﬁnition) an α-approximate q-quantile. We still need to show that such vialways exists. First set
e= maxi(gi+ ∆i)/2. Ifr>n−e, thenrmin(vs−1) =rmax(vs−1) =nso thati=s−1satisﬁes the property.
Whenr≤n−e, then the algorithm chooses the smallest index jsuch thatrmax(vj)>r+eso thatr−e≤
rmin(vj−1). This follows since if r−e>rmin(vj−1)thenrmax(vj) =rmin(vj−1) +gj+ ∆j>rmin(vj−1) + 2e
which contradicts the deﬁnition of e.
Algorithm 4: Quantile(S(X),q,n,α ): Computing α-Approximate Quantiles
Input:S(X),q,n,α(approximation parameter)
1Computer=⌈qn⌉
2fori= 0,...,s−1do
3 (vi,gi,∆i) =S(X)[i]
4ifmax(r−rmin(vi),rmax(vi)−r)≤αnthen
5return (i,vi)
6return⊥
A.3 Insert
Algorithm 5 goes through a stream of items and inserts into the sketch. The algorithm calls a compress
operator on the data structure every time that i≡0 mod1
2αfor anyi∈[m].
Algorithm 6 inserts a particular item xninto the data structure S(X). In the special case where xnis a
minimum or maximum, it inserts the tuple (xn,1,0)at the beginning or end of S(X). Otherwise, it ﬁnds
an indexisuch thatvi−1≤xn<viand then inserts the tuple (xn,1,⌊2αn⌋)intoS(X)at position i.
A.4 Compress
TheCompress operation is an internal operation used for compressing (contiguous) tuples in S(X). The goal
of this operation is to merge a node and its descendants into either its right sibling or parent node. After
merge, we have to maintain the property that the tuple is not full. A tuple is full when gi+ ∆i≥⌊2αn⌋.
17Published in Transactions on Machine Learning Research (06/2023)
Algorithm 5: Inserting a stream of items into Summary Sketch
Data:x1,x2,...,xm,...
Input:S(X),α(approximation parameter)
1n= 0
2fori= 1,...,m,... do
3ifi≡0 mod1
2αthen
4 Compress(S(X),α,n )
5 Insert(S(X),α,xi)
6n=n+ 1
7returnS(X),n
Algorithm 6: Insert(S(X),α,xn): Inserting into Summary Sketch
Data:xn
Input:S(X),α(approximation parameter)
1(v0,g0,∆0) =S(X)[0]
2(vs−1,gs−1,∆s−1) =S(X)[s−1]
3ifxn<v0then
4Shift all positions in S(X)[0...s−1]toS(X)[i...s ]
5S(X)[0] = (xn,1,0)
6else ifxn>vs−1then
7S(X)[s] = (xn,1,0)
8else
9fori= 0,...,s−1do
10 (vi,gi,∆i) =S(X)[i]
11 ifvi−1≤xn<vithen
12 Shift all positions in S(X)[i...s−1]toS(X)[i+ 1...s]
13 S(X)[i] = (xn,1,⌊2αn⌋)
14returnS(X),s+ 1
Algorithm 7: Compressing the Sketch
Input:S(X),α(approximation parameter) ,n
1ifn<1
2αthen
2return
3fori=s−2,..., 0do
4ti= (vi,gi,∆i) =S(X)[i]
5ti+1= (vi+1,gi+1,∆i+1) =S(X)[i+ 1]
6Computeg∗
i, the sum of g-values of tuple tiand its descendants
7ifband(ti)≤band(ti+1) & (g∗
i+gi+1+ ∆i+1<2αn)then
8 Delete all descendants of tiand the tuple tifrom sketch S(X)
9 Updateti+1inS(X)to(vi+1,g∗
i+gi+1,∆i+1)
18Published in Transactions on Machine Learning Research (06/2023)
By Proposition A.2, a node and its children will form a contiguous segment. Let g∗
ibe the sum of g-values
of tupletiand all of its descendants. Then merging tiand its descendants would update ti+1inS(X)to
(vi+1,g∗
i+gi+1,∆i+1)and deletetiand all of its descendants.
Proposition A.2 (Proposition 4 in (Greenwald & Khanna, 2001)) .For any node V, the set of all its
descendants in the tree forms a contiguous segment in S(X).
A.5 Formal guarantee
We brieﬂy recall the main guarantees of the GK sketch that we appeal to in the main body of this work and
provide a proof for the statements that are not reproduced from previous work.
Lemma 3.7. Given a data stream x1,...,xnof elements drawn from X, there exists a sketching algorithm
that outputs a list S(X)ofs=O/parenleftbig1
αlogαn/parenrightbig
many tuples (vi,gi,∆i)∈(X×N)×N×Nfori= 1,...,ssuch
that ifXis the data multi-set then:
1.rank(X,val(vi))⊂[/summationtext
j≤igj,∆i+/summationtext
j≤igj].
2.gi+ ∆i≤2αn.
3. The ﬁrst tuple is (min{x∈X},1,0)and the last tuple is (max{x∈X},1,0).
4. Theviare sorted in ascending order. Without loss of generality, the lower interval bounds/summationtext
j≤igj
and upper interval bounds ∆i+/summationtext
j≤igjare also sorted in increasing order.
Proof.The ﬁrst three statements are part of the GK sketch guarantee. For the third statement, i.e., to see
that theviare sorted in ascending order, we see that the GK sketch construction ensures that val(vi)≤
val(vi+1)for alli. Since an insertion operation always inserts a repeated value after all previous occurrences
and the tuple order is always preserved, it follows that ix(vi)≤ix(vi+1)as well, so in sum rank(X,vi)≤
rank(X,vi+1). In other words, the sort order in the GK sketch is stable.
The fact that the sequence/summationtext
j≤igjis sorted in increasing order follows from the non-negativity of the gi.
To ensure that ∆i+/summationtext
j≤igjare sorted in increasing order note that we always have that rank(X,vi)≤
rank(X,vi+1)so that we can decrement ∆iand ensure that ∆i+/summationtext
j≤igj≤∆i+1+/summationtext
j≤i+1gjwithout
violating the guarantees of the GK sketch.
B Omitted proofs
We ﬁrst recall and add some deﬁnitions that we will require in the sequel.
Deﬁnition B.1. LetX= ((x1,1),..., (xn,n))(sometimes implicitly referred to as X= (x1,...,xn)) be a
stream of elements drawn from some ﬁnite totally ordered data universe X, i.e.,xi∈Xfor alli∈[n].
1.Rank: Given a totally ordered ﬁnite data universe X, a data set Xand a value x∈ X, let
rankX(x) = rank(X,x) =/summationtext
y∈X1[y≤x].
2. For (xi,i)∈X, letval((xi,i)) =xiandix((xi,i)) =/summationtext
j≤i|{(xj,j) :xj<xiorxj=xi,j <i}|.
3. Forv1,v2∈X, we say that v1≤v2ifix(v1)≤ix(v2).
4. Theq-quantile of Xisval(v)forv∈Xwith ix(v) =⌈qn⌉.
5. Forx∈X, we deﬁne rmin(x) =|{v∈X: val(v)< x}|,rmax(x) =|{v∈X: val(v)≤x}|and
rank(X,x)to be the interval [rmin(x),rmax(x)].
6. We say that x∈Xis anα-approximate q-quantile for Xifrank(X,x)∩[⌈qn⌉−αn,⌈qn⌉+αn]/negationslash=∅.
With this notation, the data set Xis naturally identiﬁed as a multi-set of elements drawn from X.
Deﬁnition B.2. Letˆrmin(x) = max{/summationtext
j≤igj: val(vi)<x}andˆrmax(x) = min{∆i+/summationtext
j≤igj: val(vi)>x}.
Note that for every v∈Xsuch that val(v) =x,ix(v)∈[ˆrmin(x),ˆrmax(x)].
19Published in Transactions on Machine Learning Research (06/2023)
B.1 Proofs of privacy and utility for DPExpGK
Outline: From Deﬁnition B.2 to Lemma B.5, we formalize how the GK sketch may be used to construct
rank interval estimates for any data domain value. We then recall and apply the exponential mechanism with
a score function derived from the GK sketch (Deﬁnition B.6 to Lemma B.9 and Algorithm 1), and derive
the error guarantee Lemma B.11. We conclude this subsection with a detailed description of an eﬃcient
implementation of the exponential mechanism (Algorithm 8 and Lemma B.12), and summarize our ﬁnal
accuracy and space complexity guarantees in Theorem 4.1.
Remark B.3.We can add two additional tuples (−∞,0,0)and(∞,0,0)to the sketch, which corresponds to
respective rank intervals [0,0]and[n+ 1,n+ 1]. The bounds gi+ ∆i≤2αnare preserved. This will ensure
that the sets{i: val(vi)<x}and{i: val(vi)>x}for anyx∈Xare always non-empty.
We formalize the rank interval estimation in a partition-wise manner as below.
Lemma B.4. Given a GK sketch (v1,g1,∆1),..., (vs,gs,∆s), for every x∈Xone of the following two
cases holds:
1.x=vifor somei∈[s]andi= min{j:vj=x},
ˆrmin(x) =/summationdisplay
j≤igj
ˆrmax(x) = min{∆i∗+/summationdisplay
j≤i∗gj:∃i∗,val(vi∗)>val(vi)}
2.x∈(vi−1,vi), i.e.,x>vi−1andx<vifor somei∈[s],
ˆrmin(x) =/summationdisplay
j≤igj
ˆrmax(x) = ∆i+1+/summationdisplay
j≤i+1gj
Proof.Recall, by Remark B.3, that the ﬁrst tuple and the last tuple are formal elements at −∞and∞,
ensuring that every data universe element either explicitly occurs in the GK sketch or lies between two values
that occur in the GK sketch. Both statements now follow directly from Deﬁnition B.2 and the fact that the
valuesvioccur in increasing order in the sketch (Lemma 3.7).
We bound the quality of the rank interval estimate [ˆrmin(x),ˆrmax(x)]compared to the true rank interval
[rmin(x),rmax(x)]as follows.
Lemma B.5.|rmin(x)−ˆrmin(x)|≤2αnand|rmax(x)−ˆrmax(x)|≤2αn.
Proof.Leti∗= argmaxi:val(vi)<x/summationtext
j≤igj. Then by Deﬁnition of i∗, we have that val(vi∗)<x≤val(vi∗+1).
It follows that
[ix(vi∗),ix(vi∗+1)]⊂[/summationdisplay
j≤i∗gj,∆i∗+1+gi∗+1+/summationdisplay
j≤i∗gj]
⊂[ˆrmin(x),∆i∗+1+gi∗+1+ ˆrmin(x)]
Sincermin(x)∈[ix(vi∗),ix(vi∗+1)]andgi∗+1+ ∆i∗+1≤2αn, it follows that|rmin(x)−ˆrmin(x)|≤2αn. The
other inequality follows analogously.
Deﬁnition B.6 (Exponential Mechanism (McSherry & Talwar, 2007)) .Letu:SS×R→ Rbe an arbitrary
score function with global sensitivity δu. For any database summary d∈SSand privacy parameter /epsilon1>0,
the exponential mechanism E/epsilon1
u:SS→Routputsr∈Rwith probability∝exp(/epsilon1·u(S(X),r)
2δu)where
δu= max
X∼X/prime,r|u(S(X),r)−u(S(X/prime),r)|.
20Published in Transactions on Machine Learning Research (06/2023)
The following statement formalizes the trade-oﬀ between the privacy parameter /epsilon1and the tightness of the
tail bound on the score attained by the exponential mechanism.
Theorem B.7 ( (McSherry & Talwar, 2007; Smith, 2011)) .The exponential mechanism (Deﬁnition B.6)
satisﬁes/epsilon1-diﬀerential privacy. Further, the following tail bound on the utility (the score of the output element)
holds:
P/parenleftbigg
u(S(X),E/epsilon1
u(S(X)))<max
r∈Ru(S(X),r)−2δu(t+ lns)
/epsilon1/parenrightbigg
≤e−t,
wheresis the size of the universe from which we are sampling from.
To run the exponential mechanism using our approximate rank interval estimates, we deﬁne a score function
as follows.
Deﬁnition B.8. Letd(·,·)denote the /lscript1metric on R. Given a sketch S(X), we deﬁne a score function on
X:
u(S(X),x) =−min{|y−⌈qn⌉|:y∈[ˆrmin(x),ˆrmax(x)]}
=−d(⌈qn⌉,[ˆrmin(x),ˆrmax(x)])
The magnitude of the noise that is added in the course of the exponential mechanism depends on the
sensitivity of the score function, which we bound from above as follows.
Lemma B.9. For alln>1/α, the sensitivity of u(i.e.,δu) is at most 4αn+ 2units.
Proof.Fix any data set X/primeneighbouring Xunder swap DP and let [r/prime
min(·),r/prime
max(·)]be the rank ranges with
respect toX/primefor values inX. Let [ˆr/primemin(·),ˆr/primemax(·)]denote the conﬁdence interval derived from the GK
sketchS(X/prime)for values inX.
Claim B.10.|rmin(x)−r/prime
min(x)|≤2,|rmax(x)−r/prime
max(x)|≤2.
Proof.These bounds follow directly from the Deﬁnition of rminandrmax; under swap DP at most two
elements of the stream are changed which implies that the count of the sets deﬁning these terms changes by
at most 1 unit each for a total shift of 2units (in fact, this can be bounded by 1unit).
We now prove the sensitivity bound.
u(S(X),x) =−d(⌈qn⌉,[ˆrmin(x),ˆrmax(x)])
≤−d(⌈qn⌉,[rmin(x),rmax(x)]) + 2αn
≤−d(⌈qn⌉,[r/prime
min(x),r/prime
max(x)]) + 2αn+ 2
≤−d(⌈qn⌉,[ˆr/primemin(x),ˆr/primemax(x)]) + 4αn+ 2
≤u(S(X/prime),x) + 4αn+ 2.
Swapping the positions of XandX/prime, we get the reverse bound to complete the sensitivity analysis.
We can now derive a high probability bound on the utility that is achieved by Algorithm 1.
Lemma B.11. Ifˆxis the value returned DPExpGK then with probability 1−β,
d(⌈qn⌉,[ˆrmin(ˆx),ˆrmax(ˆx)])≤2αn+2(4αn+ 2) log(|X|/β)
/epsilon1.
Proof.By construction, Algorithm 1 is simply a call to the exponential mechanism with score function
u(S(X),·), Since for any target q-quantile,⌈qn⌉lies in [0,n]it follows that there is some i∗∈ssuch that
⌈qn⌉∈[/summationtext
j≤i∗gj,∆i∗+1+gi∗+1+/summationtext
j≤i∗gj]. It follows that d(⌈qn⌉,[rmin(val(vi∗),rmax(val(vi∗))])≤2αn
21Published in Transactions on Machine Learning Research (06/2023)
and that hence max(u(S(X),x))≥−2αn. Ifx∗is the output of the exponential mechanism, then applying
the utility tail bound we get that with probability 1−β,
u(S(X),x∗)≥−2αn−2(4αn+ 2) log(|X|/β)
/epsilon1.
By deﬁnition of u, the desired bound follows.
Algorithm 8: DPExpGKGumb : Implementing the Exponential Mechanism on S(X)using the Gumbel
Distribution (Optimized implementation of Algorithm 1)
Data:X= (x1,x2,...,xn)
Input:/epsilon1,α(approximation parameter) ,q∈[0,1](quantile parameters)
1Build summary sketch S(X)and lets=|S(X)|.
2Let(vi,gi,∆i) =S(X)[i]for alli∈[s]
3maxIndex =−1
4maxValue =−∞
/* Iterating over tuple values vi */
5Leti= 1
6whilei<=sdo
7 ˆrmin=/summationtext
j≤igj
8 ˆrmax= min{∆i∗+/summationtext
j≤i∗gj: val(vi∗)>val(vi)}
9ui=−min{|y−⌈qn⌉|:y∈[ˆrmin,ˆrmax]}.
10f=/epsilon1
2ui
11 ˜f=f+Gumb (0,1)
12if˜f >maxValue then
13 maxIndex = (i,tuple )
14 maxValue =˜f
15i←min{j:vj>vi}
/* Iterating over intervals between tuples X(vi−1,vi)⊂X */
16Leti= 1
17whilei<=sdo
18ifX(vi−1,vi)is not empty then
19 ˆrmin=/summationtext
j≤igj
20 ˆrmax= ∆i+1+/summationtext
j≤i+1gj
21ui−1,i=−min{|y−⌈qn⌉|:y∈[ˆrmin,ˆrmax]}.
22f= log(|X(vi−1,vi)|) +/epsilon1
2ui−1,i
23 ˜f=f+Gumb (0,1)
24 if˜f >maxValue then
25 maxIndex = (i,interval )
26 maxValue =˜f
27i←i+ 1
28ifmaxIndex = (i,tuple )for somei∈1,...,sthen
29returnvi
30else ifmaxIndex = (i,interval )for somei∈1,...,sthen
31Pickv∈X(vi−1,vi)uniformly at random
32returnv
As discussed before, in general a naive implementation of the exponential mechanism as in Algorithm 1 would
in general not be eﬃcient. To resolve this issue, in Algorithm 8 we take advantage of the partition of the data
domain by the score function and the Gumbel-max trick to implement the exponential mechanism without
22Published in Transactions on Machine Learning Research (06/2023)
any higher-order overhead and return an α-approximate qquantile. This trick has become a standard way
to implement the exponential mechanism over intervals/tuples.
Lemma B.12. Algorithm 8 implements Algorithm 1 on the data universe Xwith space complexity O(|S(X)|)
(whereS(X)is the GK sketch) and additional time complexity O(|S(X)|log|S(X)|).
Proof.We see that it will suﬃce to show that Algorithm 8 executes the exponential mechanism with the
same score function as Algorithm 1 to prove that it is a valid implementation of the latter.
As noted in previous work (Abernethy et al., 2017), if Z1,...,ZNare drawn i.i.d. from standard Gumbel
distribution, then
P/bracketleftbigg
fi+Zi= max
j∈[N]{fj+Zj}/bracketrightbigg
=exp(fi)/summationtext
j∈[N]exp(fj),∀i∈[N].
We recall that when running the exponential mechanism on X, we want to sample the element x∈Xwith
probability∝exp(/epsilon1u(S(X),x). To implement the exponential mechanism via the identiﬁcation with Gumbel
argmaxdistribution above, we will simply compute the scores u(S(X),x)and letfi=/epsilon1·u(S(X),x).
Forx∈ Xsuch thatx=vifor somei∈[s], Algorithm 8 directly computes the scores according to
Deﬁnition B.8 and Lemma B.4; this is formalized by lines 5 to 15 in the pseudo code.
Forx∈ Xwhich lie strictly between the tuple values {vi:i∈[s]}, we proceed as follows. Fixing i,
from Lemma B.4 we have that that for X(vi−1,vi) :={x∈X :x > vi−1,x < vi}, the rank conﬁdence
interval estimate is the same, i.e. [/summationtext
j≤i−1gj,∆i+/summationtext
j≤igj]. It follows from Deﬁnition B.8 that for all such
domain values the the score function value u(S(X),·)is equal; this is denoted ui−1,iin the pseudo code.
By summing the probabilities for sampling individual domain elements, it follows that the likelihood of the
exponential mechanism outputting some value from the set X(vi−1,vi)is∝|X (vi−1,vi)|exp(/epsilon1ui−1,i/2) =
exp(/epsilon1ui−1,i/2 + log(|X(vi−1,vi)|)). This is formalized by lines 16 to 27 in the pseudo code.
Finally, if some interval is selected, then by outputting elements chosen uniformly at random, we ensure
that the likelihood of x∈X(vi−1,vi)being output is∝1
|X(vi−1,vi)|·exp(/epsilon1ui−1,i/2 + log(|X(vi−1,vi)|) =
exp(/epsilon1ui−1,i). Note that we do not need to account for ties in the Gumbel scores as the event fi+Zi=fj+Zj
for anyj/negationslash=ihas measure 0.5
To bound the space and time complexity; we note that by the guarantees of the GK sketch, the size of the
sketchS(X)isO((1/α) logαn); we compute Gumbel scores by iterating over tuples and intervals of which
there are at most O(S(X))-many of each, each computation takes at most O(log|S(X)|)time, and only the
max score and index seen at any point is tracked in the course of the algorithm.
We can now state and prove our main theorem in this section, proving utility bounds for α-approximating
quantiles through DPExpGK with sublinear space.
Theorem 4.1. Algorithm 1 is /epsilon1-diﬀerentially private. Let ˆxbe the value returned by Algorithm 1 when
initialized with target quantile q. The following statements hold:
1. Algorithm 1 can be implemented to run with space complexity O((1/α) logαn), such that with prob-
ability 1−β
d(⌈qn⌉,[ˆrmin(ˆx),ˆrmax(ˆx)])≤2αn+2(4αn+ 2) log(|X|/β)
/epsilon1(2)
2. Forn >24 log|X|/β
αmin{/epsilon1,1}, Algorithm 1 can be implemented to run with space complexity
O/parenleftbig
(α/epsilon1)−1log(|X|/β) logn/parenrightbig
such that with probability 1−β,ˆxis anαapproximate q-quantile.
5As is usual in the privacy literature, we assume that the sampling of the Gumb (0,1)distribution can be done on ﬁnite-
precision computers (Balcer & Vadhan, 2018). While the problem of formally dealing with rounding has not been settled in the
privacy literature (Mironov, 2012), for any practical purpose it easily suﬃces to store the output of the Gumbel distribution
using a few computer words.
23Published in Transactions on Machine Learning Research (06/2023)
Our dependence in α, which for practical purposes is usually the most important term, is optimal. Recent
subsequent work by Kaplan and Stemmer (Kaplan & Stemmer, 2021) shows how to improve the dependence
in other parameters if approximate (rather than pure) diﬀerential privacy is allowed, or if the stream length
is large enough.
Proof.The privacy guarantee of Algorithm 1 follows from the privacy guarantee of the exponential mecha-
nism and Lemma B.12. The accuracy bound in equation 2 is simply a restatement of Lemma B.11, and the
space complexity bounds follow from the GK sketch space complexity bound O/parenleftbig1
αlogαn/parenrightbig
. To derive the
second statement, we substituteαmin{/epsilon1,1}
24 log(|X|/β)for the approximation parameter αin equation 2 and get
d(⌈qn⌉,[ˆrmin(ˆx),ˆrmax(ˆx)])
≤2·αmin{/epsilon1,1}n
24 log(|X|/β)+2(4(αmin{/epsilon1,1}
24 log(|X|/β))n+ 2) log(|X|/β)
/epsilon1
≤αn
12 log(|X|/β)+αn
3+4 log|X|/β
/epsilon1
≤αn
12+αn
3+αn
3
≤αn.
The space complexity bound now follows directly from the space complexity bound derived in Lemma B.12,
the space complexity bound O((1/α) logαn)for the GK sketch, and by substituting
αmin{/epsilon1,1}
24 log(|X|/β)forα. Sincve the approximation parameter must be greater than 1/n, we have that n≥
24 log(|X|/β)
αmin{/epsilon1,1}.
B.2 Proofs of privacy and utility for DPHistGK
In this section we prove Theorem 4.2, which we restate here for ease of reference.
Theorem 4.2. For the one-dimensional normal distribution N(µ,σ2), letS= (X1,X2,...,Xn)be a data
stream through which we wish to obtain ˜Qq
S, a DP estimate of the q-quantile of the distribution.
For anyq∈(0,1), there exists an (/epsilon1,δ)-DP algorithm such that, with probability at least 1−β, we obtain
|Qq
D−˜Qq
S|≤αfor anyα>0,β∈(0,1],/epsilon1,δ∈(0,1/n)and for stream length
n≥max{min{A,B},C},where
A=O/parenleftbiggR
/epsilon1σαlogR
σβ/parenrightbigg
,B=O/parenleftbiggR
/epsilon1σαlog1
βδ/parenrightbigg
,C=O/parenleftbiggR2
σ2α2log1
β/parenrightbigg
as long asµ∈(−R,R)and using space of O(max{R
σ,1
αlogαn}).
Proof.For any stream S= (X1,...,Xn), we use the triangle inequality so that
|Qq
D−˜Qq
S|≤|Qq
D−Qq
S|+|Qq
S−˜Qq
S| (3)
≤α/2 +α/2. (4)
|Qq
D−Qq
S|≤α/2follows with probability 1−β/2by Corollary B.14 and |Qq
S−˜Qq
S|≤α/2follows with
probability 1−β/2by Lemma B.15. The space complexity follows with probability 1 via the deterministic
nature of the Greenwald-Khanna sketch.
Lemma B.13 (Dvoretzky-Kiefer-Wolfowitz inequality (Dvoretzky et al., 1956)) .For anyn∈Z+, let
X1,...,Xnbe i.i.d. random variables with cumulative distribution function Fso thatF(x)is the prob-
ability that a single random variable Xis less than xfor anyx∈R. Let the corresponding empirical
distribution function be Fn(x) =1
n/summationtextn
i=11[Xi≤x]for anyx∈R. Then for any γ >0,
P/parenleftbigg
sup
x∈R|Fn(x)−F(x)|>γ/parenrightbigg
≤2 exp(−2nγ2).
24Published in Transactions on Machine Learning Research (06/2023)
Corollary B.14. For anyq∈(0,1), letQq
Dbe theq-quantile estimate for the distribution DandQq
Sbe the
q-quantile estimate for the sample. Then, |Qq
D−Qq
S|≤α/2with probability 1−β/2whenn≥2
α2log 4/β.
Proof.Follows by the DKW inequality (Lemma B.13) where n≥1
2γ2log 4/βandγ=α/2.
Lemma B.15. For anyq∈(0,1),α > 0,β∈(0,1],/epsilon1,δ∈(0,1/n), there exists an (/epsilon1,δ)-diﬀerentially
private algorithm ˜Qq
Sfor computing the q-quantile such that
|Qq
S−˜Qq
S|≤α/2,
with probability≥1−βfor stream length
n≥O/parenleftbigg
min{O/parenleftbiggR
/epsilon1σαlogR
σβ/parenrightbigg
,O/parenleftbiggR
/epsilon1σαlog1
βδ/parenrightbigg
}/parenrightbigg
.
Furthermore, with probability 1, ˜Qq
Suses space of O(max{R
σ,1
αlogαn}).
Proof.First, by the tail bounds of the Gaussian distribution (Claim B.17), we can obtain that for any i∈[n],
P[|Xi−µ|>c]≤2e−c2/2σ2,
so that by the union bound,
P[∃i,|Xi−µ|≥c]≤2ne−c2/2σ2,
which implies that for any β∈(0,1],
P[∀i,|Xi−µ|≤σ/radicalbig
2 log 4n/β]≥1−β/2,
which holds by our sample complexity (stream length) guarantees.
Next,letr=⌈R/σ⌉.6Divide [−R−σ/2,R+σ/2]into2r+1binsoflengthatmost σeach. Eachbin Bjshould
equal ((j−0.5)σ,(j+ 0.5)σ]for anyj∈{−r,...,r}. Next run the histogram learner of Lemma B.16 with
per-bin accuracy parameter of α/K, high-probability parameter of β/2, privacy parameters /epsilon1,δ∈(0,1/n),
and number of bins K= 2⌈R/σ⌉+ 1. We can do this because of our sample complexity (stream length)
bounds. Then we obtain noisy estimates ˜p−r,..., ˜prwith per-bin accuracy of α/K. Then any quantile
estimate would have accuracy of α(by summing noisy estimates for at most Kbins).
Next, we use these bins to construct a sketch (private by DP post-processing) based on the deterministic
algorithms of (Greenwald & Khanna, 2004) to, with probability 1, obtain space of O(max{R
σ,1
αlogαn}).
Lemma B.16 (Histogram Learner (Bun et al., 2015; Vadhan, 2017; Karwa & Vadhan, 2018)) .For every
K∈N∪{∞}and every collection of disjoint bins B1,...,BKdeﬁned on the domain X. For anyn∈N,
/epsilon1,δ∈(0,1/n),α> 0,andβ∈(0,1), there exists an (/epsilon1,δ)-DP algorithm M:Xn→RKsuch that for every
distribution Don the domainX, if
1.X1,...,Xn∼D,pk=P[Xi∈Bk]for anyk∈[K],
2.(˜p1,..., ˜pK)←M(X1,...,Xn),
3.n≥max/braceleftBig
min/braceleftBig
8
/epsilon1αlog2K
β,8
/epsilon1αlog4
βδ/bracerightBig
,1
2α2log4
β/bracerightBig
,
then (over the randomness of the data X1,...,Xnand ofM)
1.PX∼D,M[|˜pk−pk|≤α]≥1−β,
2.P[argmaxk˜pk=j]≤npjifK≥2/δ,
6Note that this argument is similar to the arguments for Algorithm 1 in (Karwa & Vadhan, 2018).
25Published in Transactions on Machine Learning Research (06/2023)
3.P[argmaxk˜pk=j]≤npj+ 2 exp(−(/epsilon1n/8)·(maxkpk))ifK < 2/δ.
Claim B.17 (Gaussian Tail Bound) .LetZbe a random variable distributed according to a standard normal
distribution (with mean 0 and variance 1). For every t>0,
P[|Z|>t]≤2 exp(−t2/2).
Lemma B.18. Algorithm 2 satisﬁes (/epsilon1,0)-DP.
Proof.For anyi∈[n], any itemxican belong in at most one bin. Plus, the global sensitivity of the function
that computes the empirical histogram is 2, since changing a single item can change the contents of at most
two bins.
As a result, adding noise of Lap(0,2//epsilon1)to each bin satisﬁes /epsilon1-DP by Theorem B.19.
Theorem B.19 (Laplace Mechanism (Dwork et al., 2006)) .Fix/epsilon1>0and any function f:Yn→RK. The
Laplace mechanism outputs
f(y) + (L1,...,LK),
L1,...,LK∼Lap(0,GSf//epsilon1)whereGSfis the global sensitivity of the function f. Furthermore, the mecha-
nism satisﬁes (/epsilon1,0)-DP.
B.3 Extension to the Continual Observation setting
In this section we prove Theorem 5.1, which formalizes the privacy and utility guarantees of Algorithm 3 in
the continual observation setting. We start by making some deﬁnitions that will aid us in our analysis.
Deﬁnition B.20. We make the following deﬁnitions:
1.Stream Preﬁx : Given an input data stream X= (x1,...,xn)we deﬁne the preﬁx up to the index
selement of this stream X[1 :s] := (x1,...,xs). Note that we can overload notation and treat
X[1 :s]as a data set by ignoring the order in which the elements arrive.
2.Checkpoint : A setS/negationslash=∅is a set of checkpoints for stream Xif∀j∈S,∃valuevjthat is a
(α/2)-approximate q-quantile for X[1 :j].
We ﬁrst observe that if we have an α/2approximate quantile for a given data set, then that estimate remains
at least an α-approximate quantile for a slightly larger set as well.
Lemma B.21. Ifx∈Xis anα/2-approximate q-quantile for a data set X(for someq∈[0,1]), then it is
anα-approximate q-quantile for any data set X/prime⊃Xsuch that|X/prime|≤(1 +α/2)|X|.
Proof.Sincexis anα/2-approximate q-quantile, we have that
(1−α/2)|X|≤rank
X(x)≤(1 +α/2)|X|.
We then have that
rank
X/prime(x) =/summationdisplay
y∈X/prime1[y≤x]
=/summationdisplay
y∈X1[y≤x] +/summationdisplay
y∈X/prime\X1[y≤x]
⇒/summationdisplay
y∈X1[y≤x]≤/summationdisplay
y∈X/prime1[y≤x]≤/summationdisplay
y∈X1[y≤x] +/summationdisplay
y∈X/prime\X1[y≤x]
⇒(1−α/2)|X|≤/summationdisplay
y∈X/prime1[y≤x]≤(1 +α/2)|X|+ (|X/prime|−|X|)
26Published in Transactions on Machine Learning Research (06/2023)
⇒(1−α/2)|X|≤/summationdisplay
y∈X/prime1[y≤x]≤(1 +α/2)|X|+α|X|/2
⇒(1−α)|X|≤/summationdisplay
y∈X/prime1[y≤x]≤(1 +α)|X|,
i.e.,xis also anα-approximate q-quantile for X/prime, as required.
We now show that the choice of checkpoints made in the pseudocode of algorithm 3 is valid as per our
deﬁnition.
Lemma B.22. In Algorithm 3, the set {cp(s/prime) :s/prime∈[n]}forms a valid set of checkpoints for the data stream
X. More concretely vsis anα/2-approximate q-quantile for X[1 :s].
Proof.First we bound the size of the set of checkpoints {cp(s/prime) :s/prime∈[n]}. Since a new checkpoint value is
generated only when s≥(1+α/2)cp(s−1), it follows that for any new checkpoint where cp(s/prime)/negationslash= cp(s/prime−1),
we have cp(s/prime)≥(1 +α/2)cp(s/prime−1). Using Theorem 4.1, we set the ﬁrst checkpoint value to24 log|X|/β∗
α∗/epsilon1∗,
whereα∗=α/2,β∗and/epsilon1∗aretheaccuracyparameter, thefailureprobability, andtheprivateparameterthat
are passed in the calls to DPExpGK, respectively.7It follows that there are at most k≤log1+α/2n=3
αlogn
checkpoints using the fact that for all x>−1,x
1+x≤log(1 +x)≤x.
Sincevcp(s)is the output of DPExpGK given a GK sketch with accuracy parameter α/2and privacy parameter
/epsilon1∗it follows that with probability 1−β∗whereβ∗=αβ
3 logn,rankX[1:s/prime]∈(1−α/2,1 +α/2)qn. We now
apply the union bound over all private approximate quantile computations at checkpoints and are done.
We can now prove our main technical result.
Theorem 5.1. Let/epsilon1,α > 0,n∈Z. For anyβ∈(0,1], with probability ≥1−β, Algorithm 3 maintains
anα-approximate q-quantile at every point sin the data stream for s= Ω/parenleftBig
lognlog|X|/β
α2/epsilon1/parenrightBig
. Furthermore,
Algorithm 3 satisﬁes /epsilon1-DP and has space complexity Ω/parenleftBig
1
α2/epsilon1log2nlog/parenleftBig
|X|logn
αβ/parenrightBig/parenrightBig
.
Proof.To see that Algorithm 3 is /epsilon1-DP, we observe that the output of this algorithm throughout the data
stream can be summarized by its outputs at the checkpoints {cp(s) :s∈[n]}(the points in the stream at
which a new checkpoint is reached and a new value released are known publicly, so this suﬃces for privacy
analysis). It follows that there is a choice of /epsilon1∗=α/epsilon1
3 lognthat gives us an /epsilon1-DP mechanism.
We now prove the accuracy guarantee. From the second statement of Theorem 4.1 we get that for points in
the stream ssuch that cp(s)≥48 log|X|/β
αmin{/epsilon1∗,1}, i.e., the ﬁrst checkpoint, the output vswill be anα/2-accurate
quantile for X[1 : cp(s)|. Then, since|X[1 :s]|=s≤(1 +α/2)cp(s)≤(1 +α/2)|X[1 : cp(s)]|, by
Lemma B.21 it follows that rankX[1:s](vs)∈(1−α,1 +α)qn, i.e.,vsis anα-approximate q-quantile for
X[1 :s].
C Additional Experiments
In this section, we include some additional experimental details and results.
Varying/epsilon1forDPExpGKGumb :In Table 1, we vary /epsilon1and compare DPExpFull toDPExpGKGumb in terms of
average absolute error and execution time (in seconds). We see that DPExpGKGumb is signiﬁcantly faster than
DPExpFull in terms of average execution time. However, DPExpGKGumb incurs larger error because of the
approximation factor of α= 0.0001.
7The last checkpoint might occur at (1 +α/2)n. We may ignore checkpoints past n.
27Published in Transactions on Machine Learning Research (06/2023)
/epsilon1 DPExpFull (Error) DPExpGKGumb (Error) DPExpFull (Time) DPExpGKGumb (Time)
0.1 0.0019796 0.002172 0.1330166 0.028163
0.5 0.0004581 0.0013467 0.13113595 0.02804
10.000249894 0.00127795 0.1335331 0.0284041
5 0.00007110 0.00151 0.1341164231 0.028285
Table 1:α= 0.0001,n= 105, U(0, 10). Time in Seconds.
Histogram-Based Algorithm: We also implemented the DPHistGK and tested by varying the bin width
of the histogram. In general, we observe that DPExpGK has superior performance in terms of minimizing the
error for any single quantile query. For example, we obtain averge absolute errors of at least 2.56 for 2 bins
and 3.001 for 10 bins for q= 0.3forDPHistGK . This suggests that we can rely on DPExpGK and its variants for
general-purpose implementations with minimal error. However, it is possible that certain parameter settings
(e.g., bin width) for DPHistGK might yield better performance.
Continual Observation Algorithm: We also implement the continual observation algorithm that builds
directly on DPExpGK. We ﬁxed the ﬁrst stream checkpoint to 10000. For α= 0.001and stream length of
n= 100000 fromU(0,10), we observe average absolute error of 0.00723 for /epsilon1∈{0.1,0.5,1,5}over 100 trials.
Forα= 0.001and stream length of n= 100000 fromN(5,1), we observe average absolute error of 0.00134.
However, it is possible that the ﬁrst few checkpoints might be an important parameter for the error of the
sketch in the continual observation setting. We leave the full exploration of this question to future work.
C.1 Full Space Quantile Computation
Without the bounded space requirement (i.e., space sublinear in the stream length), we can use the expo-
nential mechanism with a score function that uses the entire stream of values X. In that case, the sensitivity
of the score function is at most 1. We use this as one of the baselines for our experimental validation.
Lemma C.1. Given any insertion only stream
X= (x1,x2,...,xn−1,xn),
the sensitivity of the score function u(under swap diﬀerential privacy) is at most 1. i.e.,δu≤1. The
functionuis deﬁned as u(X,e) =−|rank(X,e)−r|whereris the approximate ⌊q·n⌋rank of the sketch
andrank(X,e)is the rank of eamongst all values in the stream X.
Proof.Let|X|=n. The score function becomes −|rank(X,e)−nq|wherenq=⌊q·n⌋. Consider two
streams with only one element changed: X,X/prime, denoting the element by xd. Then at time d≤n, in the
second stream x/prime
dis inserted instead of xd. In both cases, nqchanges by at most q(in the case of add-remove
DP) and for swap DP, nqremains the same. And for any e,rank(X/prime,e)would diﬀer from rank(X,e)by at
most 1 since the rank of any element can change by at most 1 after adding, deleting, or replacing an item
in the stream. Furthermore, for any n≥d, the rank of any ewill diﬀer in X,X/primeby at most 1 replacing xd
withx/prime
dcan displace the rank of any element by at most 1. Also, the term nqwill remain the same. (Note
that in the add-remove privacy deﬁnition nq=⌊q·n⌋would change to either ⌊q·(n+ 1)⌋or⌊q·(n−1)⌋.)
The “reverse triangle inequality” says that for any real numbers xandy,|x−y|≥||x|−|y||. As a result,
−|rank(X,e)−nq|+|rank(X/prime,e)−nq|≤|rank(X/prime,e)−rank(X,e)|≤1for anye.
28