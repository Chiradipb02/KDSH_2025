Under review as submission to TMLR
Increasing Both Batch Size and Learning Rate Accelerates
Stochastic Gradient Descent
Anonymous authors
Paper under double-blind review
Abstract
The performance of mini-batch stochastic gradient descent (SGD) strongly depends on set-
ting the batch size and learning rate to minimize the empirical loss in training the deep
neural network. In this paper, we present theoretical analyses of mini-batch SGD with four
schedulers: (i) constant batch size and decaying learning rate scheduler, (ii) increasing batch
size and decaying learning rate scheduler, (iii) increasing batch size and increasing learning
rate scheduler, and (iv) increasing batch size and warm-up decaying learning rate scheduler.
We show that mini-batch SGD using scheduler (i) does not always minimize the expectation
of the full gradient norm of the empirical loss, whereas it does using any of schedulers (ii),
(iii), and (iv). Furthermore, schedulers (iii) and (iv) accelerate mini-batch SGD. The paper
also provides numerical results of supporting analyses showing that using scheduler (iii) or
(iv) minimizes the full gradient norm of the empirical loss faster than using scheduler (i) or
(ii).
1 Introduction
Mini-batch stochastic gradient descent (SGD) (Robbins & Monro, 1951; Zinkevich, 2003; Nemirovski et al.,
2009; Ghadimi & Lan, 2012; 2013) is a simple and useful deep-learning optimizer for finding appropriate
parameters of a deep neural network (DNN) in the sense of minimizing the empirical loss defined by the
mean of nonconvex loss functions corresponding to the training set.
The performance of mini-batch SGD strongly depends on how the batch size and learning rate are set. In
particular, increasing batch size (Byrd et al., 2012; Balles et al., 2016; De et al., 2017; Smith et al., 2018;
Goyal et al., 2018; Shallue et al., 2019; Zhang et al., 2019) is useful for training DNNs with mini-batch SGD.
In (Smith et al., 2018), it was numerically shown that using an enormous batch size leads to a reduction in
the number of parameter updates.
Decaying a learning rate (Wu et al., 2014; Ioffe & Szegedy, 2015; Loshchilov & Hutter, 2017; Hundt et al.,
2019) is also useful for training DNNs with mini-batch SGD. In (Chen et al., 2020), theoretical results
indicatedthatrunningSGDwithadiminishinglearningrate ηt=O(1/t)andalargebatchsizeforsufficiently
many steps leads to convergence to a stationary point. A practical example of a decaying learning rate with
ηt+1≤ηtfor allt∈Nis a constant learning rate ηt=η>0for allt∈N. However, convergence of SGD with
a constant learning rate is not guaranteed (Scaman & Malherbe, 2020). Other practical learning rates have
been presented for training DNNs, including cosine annealing (Loshchilov & Hutter, 2017), cosine power
annealing (Hundt et al., 2019), step decay (Lu, 2024), exponential decay (Wu et al., 2014), polynomial decay
(Chen et al., 2018), and linear decay (Liu et al., 2020).
Contribution: The main contribution of the present paper is its theoretical analyses of mini-batch SGD
with batch size and learning rate schedulers used in practice satisfying the following inequality:
min
t∈[0:T−1]E[∥∇f(θt)∥]≤/braceleftbigg2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
BT+¯Lσ2
2−¯Lηmax1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
VT/bracerightbigg1
2
,
1Under review as submission to TMLR
wherefis the empirical loss for ntraining samples having ¯L-Lipschitz continuous gradient ∇fand lower
boundf⋆,σ2is an upper bound on the variance of the mini-batch stochastic gradient, and (θt)T−1
t=0is the
sequence generated by mini-batch SGD with batch size bt, learning rate ηt∈[ηmin,ηmax]⊂[0,2
¯L), and total
number of steps to train a DNN T.
Scheduler BT VT O(√BT+VT)
Case (i) (Theorem 3.1; Section 3.1) H1
TH2
b+H7
bTO/parenleftigg/radicalbigg
1
T+1
b/parenrightigg
bt:Constant;ηt:Decay
Case (ii) (Theorem 3.2; Section 3.2) H3
TH4
b0TO/parenleftbigg1√
T/parenrightbigg
, O/parenleftbigg1√
M/parenrightbigg
bt:Increase;ηt:Decay
Case (iii) (Theorem 3.3; Section 3.3) H5
γMH6
b0γMO/parenleftbigg1
γM
2/parenrightbigg
(*)∃¯m∀M≥¯m
bt:Increase;ηt:Increase1
γM
2<1√
M
Case (iv) (Theorem 3.4; Section 3.4) H5
γM→H3
TH6
b0γM→H4
b0TO/parenleftbigg1
γM
2/parenrightbigg
→O/parenleftbigg1√
T/parenrightbigg
bt:Increase;ηt:Increase→Decay
Hi(i∈[6]) (resp.H7) is a positive (resp. nonnegative) number depending on ηminandηmax.γandδare
such that 1<γ2<δ(e.g.,δ= 2when batch size is doubly increasing every Eepochs). The total number of
steps when batch size increases Mtimes isT(M) =/summationtextM
m=0⌈n
bm⌉E≥ME.O(1
γM/2)→O(1√
T)implies that
the convergence rate changes from O(1
γM/2)toO(1√
T)when the learning rate ηtchanges from an increasing
learning rate to a decaying learning rate ( ηt:Increase→Decay).
(i) Using constant batch size bt=band decaying learning rate ηt(Theorem 3.1; Section
3.1):Using a constant batch size and practical decaying learning rates, such as constant, cosine-annealing,
and polynomial decay learning rates, satisfies that, for a sufficiently large step T, the upper bound on
mint∈[0:T−1]E[∥∇f(θt)∥]becomes approximately O(1√
b)>0, which implies that mini-batch SGD does not
always converge to a stationary point. Meanwhile, the analysis indicates that using the cosine-annealing and
polynomial decay learning rates would decrease E[∥∇f(θt)∥]faster than using a constant learning rate (see
(8)), which is supported by the numerical results in Figure 1 .
(ii) Using increasing batch size btand decaying learning rate ηt(Theorem 3.2; Section 3.2):
Although convergence analyses of SGD were presented in (Vaswani et al., 2019; Fehrman et al., 2020; Scaman
&Malherbe,2020;Loizouetal.,2021;Wangetal.,2021;Khaled&Richtárik,2023), providingthetheoretical
performance of mini-batch SGD with increasing batch sizes that have been used in practice may not be
sufficient. The present paper shows that mini-batch SGD has an O(1√
T)rate of convergence. Increasing
batch size every Eepochs makes the polynomial decay and linear learning rates become small at an early
stage of training ( Figure 2 (a)). Meanwhile, the cosine-annealing and constant learning rates are robust
to increasing batch sizes ( Figure 2 (a)). Hence, it is desirable for mini-batch SGD using increasing batch
sizes to use the cosine-annealing and constant learning rates, which is supported by the numerical results in
Figure 2 .
(iii) Using increasing batch size btand increasing learning rate ηt(Theorem 3.3; Section 3.3):
From Case (ii), when batch sizes increase, keeping learning rates large is useful for training DNNs. Hence, we
are interested in verifying whether mini-batch SGD with both the batch sizes and learning rates increasing
can train DNNs. Let us consider a scheduler doubly increasing batch size (i.e., δ= 2). We setγ >1such
thatγ <√
δ=√
2and we set an increasing learning rate scheduler such that the learning rate is multiplied
byγeveryEepochs ( Figure 3 (a)). This paper shows that, when batch size increases Mtimes, mini-batch
SGD has an O(γ−M
2)convergence rate that is better than the O(1√
M)convergence rate in Case (ii). That is,
increasing both batch size and learning rate accelerates mini-batch SGD . We give practical results ( Figure
3(b);δ= 2andFigures 5 ,8,9(b);δ= 3,4) such that Case (iii) decreases ∥∇f(θt)∥faster than Case (ii)
2Under review as submission to TMLR
and tripling and quadrupling batch sizes ( δ= 3,4) decrease∥∇f(θt)∥faster than doubly increasing batch
sizes (δ= 2). The intuition for why increasing batch size and learning rate can provide fast convergence is
as follows:
(1) Increasing batch size decreases the variance of the stochastic gradient, since the upper bound on
the variance is inversely proportional to the batch size (see also Proposition A.1). Hence, increasing
batch size leads to finding stationary points of the empirical loss f. This fact is based on Case (i)
that the upper bound of mint∈[0:T−1]E[∥∇f(θt)∥]isO(/radicalig
1
T+1
b), wherebis the batch size.
(2) Mini-batch SGD does not work when learning rates are small at an early stage of training. This
fact is supported by the numerical results in Figure 1 (Case (i)) indicating that using the decaying
learning rate ηt=O(1√t+1)does not train a DNN. Hence, keeping learning rates large implies that
SGD works well.
(3) From (1) and (2), increasing batch size and learning rate can provide fast convergence of SGD.
Here, let us compare Case (ii) with Case (iii). For simplicity, we use a scheduler tripling batch size, i.e., bt
is multiplied by δ= 3at each step and consider Case (ii) with a constant learning rate ηt=ηsatisfying
ηt+1≤ηt(t∈{0}∪N) and Case (iii) with the learning rate ηtthat is multiplied by γ(<√
δ=√
3) at each
step.BTin Case (ii) is given by
BT=1/summationtextT−1
t=0ηt=1/summationtextT−1
t=0η=O/parenleftbigg1
T/parenrightbigg
,
whileBTin Case (iii) is given by
BT=1/summationtextT−1
t=0ηt=1/summationtextT−1
t=0γtη0=1
η0γT−1
γ−1=O/parenleftbigg1
γT/parenrightbigg
.
VTin Case (ii) is given by
VT=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt=1/summationtextT−1
t=0ηT−1/summationdisplay
t=0η2
bt=η
TT−1/summationdisplay
t=01
δtb0≤η
b0T1
1−1
δ=O/parenleftbigg1
T/parenrightbigg
.
Meanwhile, VTin Case (iii) is given by
VT=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0γ2tη2
0
δtb0=1
η0γT−1
γ−1η2
0
b0T−1/summationdisplay
t=0/parenleftbiggγ2
δ/parenrightbiggt
≤1
γT−1
γ−1η0
b01
1−γ2
δ=O/parenleftbigg1
γT/parenrightbigg
,
whereγ2<δis used to guarantee/summationtext+∞
t=0(γ2
δ)t<+∞. Therefore, Case (iii) has mint∈[0:T−1]E[∥∇f(θt)∥] =
O(1
γT/2), which is better than Case (ii) with mint∈[0:T−1]E[∥∇f(θt)∥] =O(1√
T).
(iv) Using increasing batch size btand warm-up decaying learning rate ηt(Theorem 3.4; Section
3.4):One way to guarantee fast convergence of mini-batch SGD with increasing batch sizes is to increase
learning rates (acceleration period; Case (iii)) during the first epochs and then decay the learning rates
(convergence period; Case (ii)), that is, to use a decaying learning rate with warm-up (He et al., 2016;
Vaswani et al., 2017; Goyal et al., 2018; Gotmare et al., 2019; He et al., 2019). We give numerical results
(Figure 4 ;δ= 2andFigure 10 ;δ= 3) indicating that using mini-batch SGD with increasing batch sizes
and decaying learning rates with a warm-up minimizes ∥∇f(θt)∥faster than using a constant learning rate
in Case (ii) or increasing learning rates in Case (iii). Our results are numerically supported by the previous
results reported in (He et al., 2016; Goyal et al., 2018; Gotmare et al., 2019; He et al., 2019) indicating
that a warm-up learning rate is useful for training deep neural networks, such as ResNets and Transformer
networks (Vaswani et al., 2017).
3Under review as submission to TMLR
2 Mini-batch SGD for empirical risk minimization
2.1 Empirical risk minimization
Letθ∈Rdbe a parameter of a deep neural network; let S={(x1,y1),..., (xn,yn)}be the training set,
where data point xiis associated with label yi; and letfi(·) :=f(·; (xi,yi)):Rd→R+be the loss function
corresponding to the i-th labeled training data (xi,yi). Empirical risk minimization (ERM) minimizes the
empirical loss defined for all θ∈Rdasf(θ) =1
n/summationtext
i∈[n]fi(θ). This paper considers the following stationary
point problem: Find θ⋆∈Rdsuch that∇f(θ⋆) =0.
Weassumethatthelossfunctions fi(i∈[n])satisfytheconditionsinthefollowingassumption(seeAppendix
A for definitions of functions, mappings, and notation used in this paper).
Assumption 2.1 Letnbe the number of training samples and let Li>0(i∈[n]).
(A1)fi:Rd→R(i∈[n]) is differentiable and Li-smooth, and f⋆
i:= inf{fi(θ):θ∈Rd}∈R.
(A2)Letξbe a random variable that is independent of θ∈Rd.∇fξ:Rd→Rdis the stochastic gradient of
∇fsuch that (i)for allθ∈Rd,Eξ[∇fξ(θ)] =∇f(θ)and(ii)there exists σ≥0such that, for all θ∈Rd,
Vξ[∇fξ(θ)] =Eξ[∥∇fξ(θ)−∇f(θ)∥2]≤σ2, where Eξ[·]denotes expectation with respect to ξ.
(A3)Letb∈Nsuch thatb≤n; and letξ= (ξ1,ξ2,···,ξb)⊤comprisebindependent and identically
distributed variables and be independent of θ∈Rd. The full gradient ∇f(θ)is estimated as the following
mini-batch gradient at θ:∇fB(θ) :=1
b/summationtextb
i=1∇fξi(θ).
TheLi-smoothness of fiin Assumption (A1) is used to analyze mini-batch SGD (Garrigos & Gower, 2024,
Assumption 4.3), since almost all of the analyses of mini-batch SGD have been based on the descent lemma
(Beck, 2017, Lemma 5.7) that is satisfied under smoothness of fi. Iff⋆
i:= inf{fi(θ):θ∈Rd}=−∞holds,
then the loss function ficorresponding to the i-th labeled training data (xi,yi)does not have any global
minimizer, which implies that the empirical loss fsatisfiesf⋆:= inf{f(θ):θ∈Rd}=−∞. Hence, the
interpolation property (Garrigos & Gower, 2024, Section 4.3.1) (i.e., there exists θ⋆∈Rdsuch that, for
alli∈[n],fi(θ⋆) =f⋆
i∈R) does not hold, whereas the interpolation property does hold for optimization
of a linear model with the squared hinge loss for binary classification on linearly separable data (Vaswani
et al., 2019, Section 2). Moreover, in the case where fis convex with f⋆=−∞, there are no stationary
points off, which implies that no algorithm ever finds stationary points of f. Accordingly, the condition
f⋆
i:= inf{fi(θ):θ∈Rd}∈Rin (A1) is natural under training DNNs including the case where the empirical
lossfis the cross-entropy with θ⋆∈Rdsuch thatf(θ⋆) = inf{f(θ):θ∈Rd}= 0. Assumption (A2) is
satisfied when (A1) holds and the random variable ξfollows the uniform distribution that is used to train
DNNs in practice (see Appendix A.1 for details). Assumption (A3) holds under sampling with replacement
(see Appendix A.2 for details).
2.2 Mini-batch SGD
Given the t-th approximated parameter θt∈Rdof the deep neural network, mini-batch SGD uses
btloss functions fξt,1,fξt,2,···,fξt,btrandomly chosen from {f1,f2,···,fn}at each step t, whereξt=
(ξt,1,ξt,2,···,ξt,bt)⊤is independent of θtandbtis a batch size satisfying bt≤n. The pseudo-code of the
algorithm is shown as Algorithm 1.
Algorithm 1 Mini-batch SGD algorithm
Require:θ0∈Rd(initial point), bt>0(batch size), ηt>0(learning rate), T≥1(steps)
Ensure: (θt)⊂Rd
1:fort= 0,1,...,T−1do
2:∇fBt(θt) :=1
bt/summationtextbt
i=1∇fξt,i(θt)
3:θt+1:=θt−ηt∇fBt(θt)
4:end for
4Under review as submission to TMLR
The following lemma can be proved using Proposition A.1, Assumption 2.1, and the descent lemma (Beck,
2017, Lemma 5.7): for all θ1,θ2∈Rd,f(θ2)≤f(θ1) +⟨∇f(θ1),θ2−θ1⟩+¯L
2∥θ2−θ1∥2, where Assumption
2.1(A1) ensures that fis¯L-smooth ( ¯L:=1
n/summationtext
i∈[n]Li).
Lemma 2.1 Suppose that Assumption 2.1 holds and consider the sequence (θt)generated by Algorithm 1
withηt∈[ηmin,ηmax]⊂[0,2
¯L)satisfying/summationtextT−1
t=0ηt̸= 0, where ¯L:=1
n/summationtext
i∈[n]Liandf⋆:=1
n/summationtext
i∈[n]f⋆
i. Then,
for allT∈N,
min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt+¯Lσ2
2−¯Lηmax/summationtextT−1
t=0η2
tb−1
t/summationtextT−1
t=0ηt,
where Edenotes the total expectation, defined by E:=Eξ0Eξ1···Eξt.
The proof of Lemma 2.1 depends on the following standard inequality (Garrigos & Gower, 2024, (27)) from
the literature on SGD analysis (Garrigos & Gower, 2024, Section 5.4, Theorem 5.12) using the descent
lemma:
min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤f(θ0)−f⋆
η/summationtextT−1
t=0αt+η¯LLmax∆∗
f, (1)
where (αt)is defined by αt(1 +η2¯LLmax) =αt−1,Lmax:= maxi∈[n]Li,f⋆is the optimal value of foverRd,
and∆∗
f:=f⋆−f⋆. While the existing approach (Garrigos & Gower, 2024) uses a sequence (αt)satisfying/summationtextT−1
t=0αt≥1
2η2¯LLmax, the present paper uses a learning rate ηtsatisfying/summationtextT−1
t=0ηt≥O(T)and an increasing
batch sizebtsatisfying/summationtextT−1
t=01
bt<+∞in Lemma 2.1. As a result, mini-batch SGD with η > 0and the
increasing batch size btsatisfy that mint∈[0:T−1]E[∥∇f(θt)∥] =O(1√
T)(Theorem 3.2), which is better than
the existing result that SGD with η=/radicalig
2
¯LLmaxTsatisfies mint∈[0:T−1]E[∥∇f(θt)∥] =O(1
T1/4)(Garrigos &
Gower, 2024, Theorem 5.12). Section 3.5 compares our results with the existing ones in detail.
The theorems in the present paper are based on Lemma 2.1. Here, we sketch a proof of Lemma 2.1. The
descent lemma under (A1) and the definition of θt+1imply the following inequality:
f(θt+1)≤f(θt)−ηt⟨∇f(θt),∇fBt(θt)⟩+¯Lη2
t
2∥∇fBt(θt)∥2.
Under (A2), the mini-batch gradient ∇fBt(θt)is an unbiased estimator of ∇f(θt), i.e., E[∇fBt(θt)] =
∇f(θt), and the upper bound on the variance of ∇fBt(θt)is inversely proportional to the batch size bt, i.e.,
V[∇fBt(θt)]≤σ2
bt(see Proposition A.1 for details). Using the properties of ∇fBt(θt), the above inequality
leads to the following:
E[f(θt+1)]≤E[f(θt)]−ηtE/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
+¯Lη2
t
2/parenleftbiggσ2
bt+E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig/parenrightbigg
.
Finally, summing the above inequality from t= 0tot=T−1, together with f⋆∈Runder (A1) and/summationtextT−1
t=0ηt̸= 0, leads to the assertion in Lemma 2.1. A detailed proof is given in Appendix A.2.
3 Convergence Analysis of Mini-batch SGD
3.1 Constant batch size and decaying learning rate scheduler
This section considers a constant batch size and a decaying learning rate:
bt=b(t∈N)andηt+1≤ηt(t∈N). (2)
Letp>0andT,E∈N; and letηminandηmaxsatisfy 0≤ηmin≤ηmax. Examples of decaying learning rates
are as follows: for all t∈[0 :T],
[Constant LR] ηt=ηmax, (3)
5Under review as submission to TMLR
[Diminishing LR] ηt=ηmax√t+ 1, (4)
[Cosine-annealing LR] ηt=ηmin+ηmax−ηmin
2/parenleftbigg
1 + cos/floorleftbiggt
K/floorrightbiggπ
E/parenrightbigg
, (5)
[Polynomial Decay LR] ηt= (ηmax−ηmin)/parenleftbigg
1−t
T/parenrightbiggp
+ηmin, (6)
whereK=⌈n
b⌉is the number of steps per epoch, Eis the total number of epochs, and the number of steps
Tin (5) is given by T=KE. A simple, practical decaying learning rate is the constant learning rate defined
by (3). A decaying learning rate used in theoretical analyses of deep-learning optimizers is the diminishing
learning rate defined by (4). The cosine-annealing learning rate defined by (5) and the linear learning rate
defined by (6) with p= 1(i.e., an example of a polynomial decay learning rate) are used in practice. Note
that the cosine-annealing learning rate is updated each epoch, whereas the polynomial decay learning rate
is updated each step.
Lemma 2.1 leads to the following (the proof of the theorem is given in Appendix A.3).
Theorem 3.1 (Upper bound on mintE∥∇f(θt)∥2for SGD using (2)) Under the assumptions in
Lemma 2.1, Algorithm 1 using (2) satisfies that, for all T∈N,
min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
BT+¯Lσ2
2−¯Lηmax/summationtextT−1
t=0η2
t
b/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
VT,
wherep,ηmin,ηmax,K, andEare the parameters used in (3)–(6), T=KE =⌈n
b⌉Efor Polynomial LR
(6),
BT≤

1
ηmaxT[Constant LR (3)]
1
2ηmax(√
T+ 1−1)[Diminishing LR (4)]
2
(ηmin+ηmax)T[Cosine LR (5)]
p+ 1
(pηmin+ηmax)T[Polynomial LR (6)] ,(7)
VT≤

ηmax
b[Constant LR (3)]
ηmax(1 + logT)
2b(√
T+ 1−1)[Diminishing LR (4)]
3η2
min+ 2ηminηmax+ 3η2
max
4(ηmin+ηmax)b+(ηmax−ηmin)K
bT[Cosine LR (5)]
2p2η2
min+ 2pηminηmax+ (p+ 1)η2
max
(2p+ 1)(pηmin+ηmax)b+(p+ 1)(η2
max−η2
min)
(pηmin+ηmax)bT[Polynomial LR (6)] .
Let us consider using Constant LR (3), Cosine LR (5), or Polynomial LR (6). Theorem 3.1 indicates that
the bias term including BTconverges to 0asO(1
T), whereas the variance term including VTdoes not always
convergeto 0. Hence, theupperboundon mint∈[0:T−1]E[∥∇f(θt)∥2]doesnotconvergeto 0. Infact, Theorem
3.1 withη=ηmaxandηmin= 0implies that
lim sup
T→+∞min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤¯Lσ2
(2−¯Lη)b×

η [Constant LR (3)]
3η
4[Cosine LR (5)]
(p+ 1)η
(2p+ 1)[Polynomial LR (6)] .(8)
6Under review as submission to TMLR
Since3η
4< ηand(p+1)η
(2p+1)< η(p > 0), using the cosine-annealing learning rate or the polynomial decay
learning rate is better than using the constant learning rate in the sense of minimizing the upper bound on
mint∈[0:T−1]E[∥∇f(θt)∥2]. Theorem 3.1 also indicates that Algorithm 1 using Diminishing LR (4) converges
to0with the convergence rate mint∈[0:T−1]E[∥∇f(θt)∥] =O(√
logT
T1
4). However, since Diminishing LR (4)
defined byηt=η√t+1decays rapidly (see Figure 1(a)), it would not be useful for training DNNs in practice.
3.2 Increasing batch size and decaying learning rate scheduler
An increasing batch size is used to train DNNs in practice (Byrd et al., 2012; Balles et al., 2016; De et al.,
2017; Smith et al., 2018; Goyal et al., 2018). This section considers an increasing batch size and a decaying
learning rate following one of (3)–(6):
bt≤bt+1(t∈N)andηt+1≤ηt(t∈N). (9)
Examples of btare, for example, for all m∈[0 :M]and allt∈Sm=N∩[/summationtextm−1
k=0KkEk,/summationtextm
k=0KkEk)
(S0:=N∩[0,K0E0)),
[Polynomial growth BS] bt=/parenleftbigg
am/ceilingleftbiggt/summationtextm
k=0KkEk/ceilingrightbigg
+b0/parenrightbiggc
, (10)
[Exponential growth BS] bt=δm/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
b0, (11)
wherea∈R++,c,δ > 1, andEmandKmare the numbers of, respectively, epochs and steps per epoch
when the batch size is (am+b0)corδmb0. For example, the exponential growth batch size defined by
(11) withδ= 2makes batch size double each Emepochs. We may modify the parameters aandδtoat
andδtmonotone increasing with t. The total number of steps for the batch size to increase Mtimes is
T=/summationtextM
m=0KmEm. An analysis of Algorithm 1 with a constant batch size bt=band decaying learning
rates satisfying (9) is given in Section 3.1.
Lemma 2.1 leads to the following them (the proof of the theorem and the result for Polynomial BS (10) are
given in Appendix A.3).
Theorem 3.2 (Convergence rate of SGD using (9)) Under the assumptions in Lemma 2.1, Algorithm
1 using (9) satisfies that, for all M∈N,
min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
BT+¯Lσ2
2−¯Lηmax1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
VT,
whereT=/summationtextM
m=0KmEm,Emax= supM∈Nsupm∈[0:M]Em<+∞,Kmax= supM∈Nsupm∈[0:M]Km<+∞,
BTis defined as in (7), and VTis bounded as
VT≤

δηmaxKmaxEmax
(δ−1)b0T[Constant LR (3)]
δηmaxKmaxEmax
2(δ−1)b0(√
T+ 1−1)[Diminishing LR (4)]
2δη2
maxKmaxEmax
(δ−1)(ηmin+ηmax)b0T[Cosine LR (5)]
(p+ 1)δη2
maxKmaxEmax
(δ−1)(ηmax+ηminp)b0T[Polynomial LR (6)] .([Exponential BS (11)])
That is, Algorithm 1 using Exponential BS (11) has the convergence rate
min
t∈[0:T−1]E[∥∇f(θt)∥] =

O/parenleftbigg1√
T/parenrightbigg
[Constant LR (3), Cosine LR (5), Polynomial LR (6)]
O/parenleftbigg1
T1
4/parenrightbigg
[Diminishing LR (4)] .
7Under review as submission to TMLR
Theorem 3.2 (Theorem A.1) indicates that, with increasing batch sizes such as Polynomial BS (10) and
Exponential BS (11), Algorithm 1 using each of Constant LR (3), Cosine LR (5), and Polynomial LR (6)
has the convergence rate O(1√
T), in contrast to Theorem 3.1.
3.3 Increasing batch size and increasing learning rate scheduler
This section considers an increasing batch size and an increasing learning rate:
bt≤bt+1(t∈N)andηt≤ηt+1(t∈N). (12)
Example of btandηtsatisfying (12) is as follows: for all m∈[0 :M]and allt∈Sm=N∩
[/summationtextm−1
k=0KkEk,/summationtextm
k=0KkEk)(S0=N∩[0,K0E0)),
[Exponential growth BS and LR] bt=δm/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
b0, ηt=γm/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
η0, (13)
whereδ,γ > 1such thatγ2< δ; andEmandKmare defined as in (11). We may modify the parameters
γandδto be monotone increasing parameters in t. The total number of steps when both batch size and
learning rate increase Mtimes isT=/summationtextM
m=0KmEm.
Lemma 2.1 leads to the following theorem (the proof of the theorem and the result for Polynomial growth
BS and LR (27) are given in Appendix A.3).
Theorem 3.3 (Convergence rate of SGD using (12)) Under the assumptions in Lemma 2.1, Algo-
rithm 1 using (12) satisfies that, for all M∈N,
min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
BT+¯Lσ2
2−¯Lηmax1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
VT,
whereT,Emax, andKmaxare defined as in Theorem 3.2, Emin= infM∈Ninfm∈[0:M]Em<+∞,Kmin=
infM∈Ninfm∈[0:M]Km<+∞,ˆγ=γ2
δ<1,
BT≤δ
η0KminEminγM, VT≤KmaxEmaxη0δ
KminEminb0(1−ˆγ)γM.
That is, Algorithm 1 has the convergence rate
min
t∈[0:T−1]E[∥∇f(θt)∥] =O/parenleftbigg1
γM
2/parenrightbigg
[Exponential growth BS and LR (13)] .
Under Exponential BS (11), using Exponential LR (13) improves the convergence rate from O(1√
M)with
Constant LR (3), Cosine LR (5), or Polynomial LR (6) (Theorem 3.2) to O(√γ−M)(γ >1).
3.4 Increasing batch size and warm-up decaying learning rate scheduler
This section considers an increasing batch size and a decaying learning rate with warm-up for a given
Tw=/summationtextMw
m=0KmEm>0(learning rate increases Mwtimes):
bt≤bt+1(t∈N)andηt≤ηt+1(t∈[Tw−1])∧ηt+1≤ηt(t≥Tw). (14)
Examples of btin (14) are Exponential BS (13) and Polynomial BS (27). Examples of ηtin (14) can be
obtained by combining (13) with (3)–(6). For example, for all m∈[0 :M]and allt∈Sm,
[Constant LR with warm-up] ηt=

γm/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
η0(m∈[Mw])
γMwη0 (m∈[Mw:M])(15)
8Under review as submission to TMLR
and [Cosine LR with warm-up]
ηt=

γm/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
η0 (m∈[Mw])
ηmin+ηmax−ηmin
2
×/braceleftigg
1 + cos/parenleftiggm−1/summationdisplay
k=0Ek+/floorleftigg
t−/summationtextm−1
k=0KkEk
Km/floorrightigg
−Ew/parenrightigg
π
EM−Ew/bracerightigg
(m∈[Mw:M]),(16)
whereEwis the number of warm-up epochs, ηmin≥0,ηmax=γMwη0, andγis defined as in (13).
Theorems 3.2 and 3.3 lead to the following theorem.
Theorem 3.4 (Convergence rate of SGD using (14)) Under the assumptions in Lemma 2.1, Algo-
rithm 1 using (14) satisfies that, for all M∈N,
min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
BT+¯Lσ2
2−¯Lηmax1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
VT,
wherebtis the exponential growth batch size defined by (13) with δ,γ > 1such thatγ2< δ;Kmin,Kmax,
Emin, andEmaxare defined as in Theorems 3.2 and 3.3;
BT≤

δ
η0KminEminγMw+1
ηmax(T−Tw)[Constant LR (15)]
δ
η0KminEminγMw+2
(ηmin+ηmax)(T−Tw)[Cosine LR (16)]
VT≤

KmaxEmaxη0δ
KminEminb0(1−ˆγ)γMw+δηmaxKmaxEmax
(δ−1)b0(T−Tw)[Constant LR (15)]
KmaxEmaxη0δ
KminEminb0(1−ˆγ)γMw+2δη2
maxKmaxEmax
(δ−1)(ηmin+ηmax)b0(T−Tw)[Cosine LR (16)] .
That is, Algorithm 1 has the convergence rate
min
t∈[Tw:T−1]E[∥∇f(θt)∥] =O/parenleftbigg1√T−Tw/parenrightbigg
[Constant LR (15), Cosine LR (16)] .
Since Algorithm 1 with (15) and (16) uses increasing batch sizes and decaying learning rates for t≥Tw, it
has the same convergence rate as using (9) in Theorem 3.2. Meanwhile, since Algorithm 1 with (15) and
(16) uses the warm-up learning rates for t∈[Tw], Algorithm 1 speeds up during the warm-up period, based
on Theorem 3.3. As a result, for increasing batch sizes, Algorithm 1 using decaying learning rates with
warm-up minimizes E[∥∇f(θt)∥]faster than using decaying learning rates in Theorem 3.2.
3.5 Comparisons of our convergence rate results with existing ones
This section compares our results with the existing analyses of SGD for nonconvex optimization. The
comparisons are summarized in Table 1. Let us consider the case where a learning rate ηtis constant,
i.e.,ηt=η > 0. Theorem 11 in (Scaman & Malherbe, 2020) indicated that SGD with η=O(1√¯LT)
satisfies mint∈[0:T−1]E[∥∇f(θt)∥] =O(1
T1/4). Corollary 1 in (Khaled & Richtárik, 2023) showed that, under
a weaker condition (the expected smoothness (Khaled & Richtárik, 2023, Assumption 2)) than (A2), SGD
withη=O(1√¯LT)satisfies mint∈[0:T−1]E[∥∇f(θt)∥] =O(1√
T). Meanwhile, Theorem 3.2 indicates that SGD
withη=O(1
¯L)and an increasing batch size btsatisfies mint∈[0:T−1]E[∥∇f(θt)∥] =O(1√
T). For example, let
us consider training a DNN on the CIFAR-100 dataset ( n= 50000) overE= 200epochs. When the batch
sizeb0is25, the number of steps per epoch is K=⌈n
b0⌉= 1563. Hence, we have T=KE= 312600 . Since
9Under review as submission to TMLR
the Lipschitz constant ¯Lof∇fwould be large, SGD with too small a learning rate η=O(1√¯LT)would not
work in practice. Meanwhile, since the learning rate η=O(1
¯L)is constant with respect to T, SGD with
η=O(1
¯L)will work well.
Table 1: Comparisons of convergence analyses of SGD for nonconvex optimization. “Noise" in the Gradient
column means that SGD uses noisy observation, i.e., g(θ) =∇f(θ) +(Noise), of the full gradient ∇f(θ),
whereσ2is the upper bound on (Noise), while “Mini-batch" in the Gradient column means that SGD uses
a mini-batch gradient ∇fBt(θ). “Strong Growth" in the Additional Assumption column means that there
existsκ >0such that, for all t∈Nand alli∈[n],∥∇fi(θt)∥≥κ∥∇f(θt)∥2. “Bounded Gradient" in the
Additional Assumption column means that there exists G> 0such that, for all t∈N,E[∥∇fBt(θt)∥]≤G.
“Polyak-Łojasiewicz" in the Additional Assumption column means that there exists ρ>0such that, for all
t∈N,∥∇f(θt)∥2≥2ρ(f(θt)−f⋆), wheref⋆is the optimal value of foverRd. “Armijo" in the Learning
Rate column means that ηtsatisfies the Armijo line search condition. “Step Decay" in the Learning Rate
column means that ηtis step decay. “Polyak" in the Learning Rate column means that ηtis a stochastic
Polyak learning rate. Here, we let E∥∇fT∥:= mint∈[0:T−1]E[∥∇f(θt)∥]andE[fT] :=E[f(θT)].ν∈(0,1)
andcis a positive constant.
Reference and Theorem Gradient Additional Assumption Learning Rate Convergence Analysis
(Scaman & Malherbe, 2020) Noise —— η=O/parenleftig
1√¯LT/parenrightig
E∥∇fT∥=O/parenleftbig1
T1/4/parenrightbig
(Vaswani et al., 2019) Noise Strong Growth Armijo E∥∇fT∥=O/parenleftig
1√
T/parenrightig
(Wang et al., 2021) Noise Bounded Gradient Step Decay E∥∇fT∥=O/parenleftbigg√
logT
T1/4/parenrightbigg
(Loizou et al., 2021) Noise Polyak-Łojasiewicz Polyak E[fT] =f⋆+O(νT+σ2)
(Khaled & Richtárik, 2023) Mini-batch —— η=O/parenleftig
1√¯LT/parenrightig
E∥∇fT∥=O/parenleftig
1√
T/parenrightig
Theorem 3.2 ( Case (ii) ) Mini-batch —— η=O/parenleftig
1
¯L/parenrightig
E∥∇fT∥=O/parenleftig
1√
T/parenrightig
Theorem 3.3 ( Case (iii) ) Mini-batch —— Increasing E∥∇fT∥=O/parenleftbigg
1
γM
2/parenrightbigg
Theorem 3.4 ( Case (iv) ) Mini-batch —— Warm-up E∥∇fT∥=O/parenleftig
1√
T/parenrightig
The previous results reported in (Vaswani et al., 2019; Wang et al., 2021; Loizou et al., 2021) showed
convergence of SGD with specialized learning rates, such as the Armijo line search learning rate, step decay
learning rate, and stochastic Polyak learning rate. Our results indicate that SGD using practical learning
rates, such as the cosine-annealing and polynomial decay learning rates, minimizes mint∈[0:T−1]E[∥∇f(θt)∥]
in the sense of a rate of convergence O(1√
T)(Theorem 3.2). In addition, we would like to emphasize that,
usinganincreasingbatchsize, SGDwithanincreasinglearningrateacceleratesSGDwithaconstantlearning
rate (Theorem 3.3). The acceleration of SGD is guaranteed during ηt<2
¯Land the convergence of SGD is
not guaranteed during ηt>2
¯L. Therefore, using a warm-up constant learning rate (Case (iv)) is appropriate
to guarantee fast convergence of SGD.
3.6 Comparisons of convergence rates under nonconvexity with ones under convexity
While Sections 3.1–3.5 consider the case where fis not always convex, this section considers the case
wherefis convex and compares convergence rates under nonconvexity with ones under convexity. Table 2
summarizes the convergence rates under nonconvexity and convexity. The left column in Table 2 is obtained
from the results in Sections 3.1–3.4 (see also the table in Section 1). A well-known performance measure of
Algorithm 1 when fis convex is mint∈[0:T−1]E[f(θt)−f⋆](Garrigos & Gower, 2024, Section 9.1), where f⋆
10Under review as submission to TMLR
is the optimal value of minimizing a convex function f. The right column in Table 2 gives the results under
convexity of f. An upper bound of mint∈[0:T−1]E[f(θt)−f⋆]for the sequence (θt)generated by Algorithm
1 can be obtained by using Lemma 2.1, that is, the result when fis not always convex (the proof of the
results for convexity is given in Appendix A.4). While the performance measure mint∈[0:T−1]E[∥∇f(θt)∥]
for nonconvexity of fdiffers from the measure mint∈[0:T−1]E[f(θt)−f⋆]for convexity of f, Algorithm 1
under, for example, Case (ii) with convexity of fsatisfies that mint∈[0:T−1]E[f(θt)−f⋆] =O(1
T).
Table 2: Comparisons of convergence rates under nonconvexity ( Left) with ones under convexity ( Right).
f⋆is the optimal value of the minimization problem for a convex function f, and Case (i) –Case (iv) are
the learning rates and batch size schedulers considered in Section 3.1–3.4 (see the table in Section 1 for the
definitions of b,γ,M, andT)
SchedularUpper bound of min
t∈[0:T−1]E[∥∇f(θt)∥]Upper bound of min
t∈[0:T−1]E[f(θt)−f⋆]
(fis nonconvex) ( fis convex)
Case (i) O/parenleftigg/radicalbigg
1
T+1
b/parenrightigg
O/parenleftbigg1
T+1
b/parenrightbigg
Case (ii) O/parenleftbigg1√
T/parenrightbigg
,O/parenleftbigg1√
M/parenrightbigg
O/parenleftbigg1
T/parenrightbigg
,O/parenleftbigg1
M/parenrightbigg
Case (iii) O/parenleftbigg1
γM
2/parenrightbigg
O/parenleftbigg1
γM/parenrightbigg
Case (iv) O/parenleftbigg1
γM
2/parenrightbigg
→O/parenleftbigg1√
T/parenrightbigg
O/parenleftbigg1
γM/parenrightbigg
→O/parenleftbigg1
T/parenrightbigg
4 Numerical results
We examined training ResNet-18 on the CIFAR100 dataset by using Algorithm 1 (see Appendices A.9 and
A.10 for training Wide-ResNet-28-10 on CIFAR100 and ResNet-18 on Tiny ImageNet). The experimental
environment was two NVIDIA GeForce RTX 4090 GPUs and Intel Core i9 13900KF CPU. The software
environment was Python 3.10.12, PyTorch 2.1.0, and CUDA 12.2. The code is available at https://
anonymous.4open.science/r/IncrBothBSLRAccelSGDarXiv .
We set the total number of epochs E= 300, the initial learning rate η0= 0.1, and the minimum learning
rateηmin= 0in (5) and (6). The solid line in the figure represents the mean value, and the shaded area in
the figure represents the maximum and minimum over three runs.
Let us first consider the case (Figure 1(a)) of a constant batch size ( b= 27) and decaying learning rates
ηtdefined by (3)–(6) discussed in Section 3.1, where “linear" in Figure 1 denotes Polynomial LR (6) with
p= 1. Figure 1(b)–(d) indicate that using Diminishing LR (4) did not work well, since it decayed rapidly
and was very small (Figure 1(a)). Figure 1(b)–(d) also indicate that Cosine LR (5) and Polynomial LR (6)
performed better than Constant LR (3), as promised in the theoretical results in Theorem 3.1 and (8).
Next, let us consider the case (Figure 2(a)) of doubly increasing batch size every 30epochs from an initial
batch sizeb0= 23and decaying learning rates ηtdefined by (3)–(6). Figure 2(a) indicates that the learning
rate of Polynomial LR (6) updated each step (“linear" and “polynomial ( p= 2.0)") becomes small at an early
stage of training. This is because the smaller the batch size btis, the larger the required number of steps
Kt=⌈n
bt⌉per epoch becomes and the smaller the decaying learning rate ηtbecomes. Hence, in practice,
increasing batch size is not compatible with Polynomial LR (6) updated each step. Meanwhile, Figure 2(a)
indicates Constant LR (3) (“constant") and Cosine LR (5) (“cosine") were compatible with increasing batch
size, since Constant LR (3) and Cosine LR (5) updated each epoch maintain large learning rates even for
small batch sizes. In particular, Figure 2(b)–(d) indicate that using Constant LR (3) performed well.
11Under review as submission to TMLR
0 50 100 150 200 250 300
Epochs0.000.020.040.060.080.10Learning RateLearning Rate and Batch Size Schedular
262728
Batch Sizeconstant
diminishing
cosine
linear
polynomial (p=2.0)
Batch Size
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs101
100Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
constant
diminishing
cosine
linear
polynomial (p=2.0) (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR100
constant
diminishing
cosine
linear
polynomial (p=2.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs203040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 300717273
constant
diminishing
cosine
linear
polynomial (p=2.0) (d) Test accuracy score versus epochs
Figure 1: (a) Decaying learning rates (constant, diminishing, cosine, linear, and polynomial) and constant
batch size, (b) full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing
for SGD to train ResNet-18 on CIFAR100 dataset.
0 50 100 150 200 250 300
Epochs0.000.020.040.060.080.10Learning RateLearning Rate and Batch Size Schedular
242628210212
Batch Sizeconstant
diminishing
cosine
linear
polynomial (p=2.0)
Batch Size
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
100Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
constant
diminishing
cosine
linear
polynomial (p=2.0) (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR100
constant
diminishing
cosine
linear
polynomial (p=2.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs203040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 300717273
constant
diminishing
cosine
linear
polynomial (p=2.0) (d) Test accuracy score versus epochs
Figure 2: (a) Decaying learning rates and doubly increasing batch size every 30 epochs, (b) full gradient
norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train ResNet-18
on CIFAR100 dataset.
Let us consider the case (Figure 3(a)) of doubly increasing batch size ( δ= 2) every 30epochs and increasing
learning rates defined by Exponential growth LR (13) with η0= 0.1. The parameters γin the increasing
learning rates considered here were (i) γ≈1.080whenηmax= 0.2, (ii)γ≈1.196whenηmax= 0.5, and
(iii)γ≈1.292whenηmax= 1.0, which satisfy the condition γ2< δ(= 2) to guarantee the convergence of
Algorithm 1 (see Theorem 3.3). Figure 3 compares the result for “constant" in Figure 2 with the ones for the
12Under review as submission to TMLR
increasing learning rates (i)–(iii). Figure 3(b) indicates that the larger the learning rate ηtwas, the smaller
the full gradient norm ∥∇f(θe)∥became and that Algorithm 1 with increasing learning rates minimized the
full gradient norm faster than Algorithm 1 with a constant learning rate (“constant" in Figures 2 and 3).
0 50 100 150 200 250 300
Epochs0.20.40.60.81.0Learning RateLearning Rate and Batch Size Schedular
242628210212
Batch Sizeconstant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
Batch Size (=2.0)
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR100
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs10203040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 30071.071.572.072.573.0
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
 (d) Test accuracy score versus epochs
Figure 3: (a) Increasing learning rates ( ηmax= 0.2,0.5,1.0) and doubly increasing batch size every 30 epochs,
(b) full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD
to train ResNet-18 on CIFAR100 dataset.
Let us consider the case (Figure 4(a)) of a doubly increasing batch size and decaying learning rates (Constant
LR (3) and Cosine LR (5)) with warm-up based on Figure 3(a). Figure 4(b) indicates that using decaying
learning rates with warm-up accelerated Algorithm 1 more than using only increasing learning rates in Figure
3(b) and only a constant learning rate in Figure 2(b).
From the sufficient condition γ2< δto guarantee convergence of Algorithm 1 with both batch size and
learning rate increasing (Theorem 3.3), we can set a larger γwhenδis large. Since Algorithm 1 has
anO(γ−M
2)convergence rate (Theorem 3.3), using triply ( γ= 1.5<√
δ=√
3) and quadruply ( γ=
1.9<√
δ=√
4) increasing batch sizes theoretically decreases ∥∇f(θe)∥faster than doubly increasing batch
sizes (γ= 1.080<√
δ=√
2whenηmax= 0.2; Figure 3). Finally, we would like to verify whether the
theoretical result holds in practice. The scheduler was as in Figure 5(a) with η0= 0.1andηmax= 0.2, where
schedulers were modified such that batch sizes belong to [23,212]and learning rates belong to [0.1,0.2](e.g.,
be=aδ⌊e
30⌋+bandηe=cγ⌊e
30⌋+d, wherea≈0.2077,b≈7.7923,c≈0.00267, andd≈0.09733when
δ= 3andγ= 1.50anda≈0.0155,b≈7.9844,c≈0.00031, andd≈0.09969whenδ= 4andγ= 1.90).
Figure 5(a) and (b) indicate that the larger the increasing rate of batch size was (the cases of δ= 3,4after
180epochs), the larger the increasing rate of the learning rate became ( γ= 1.5,1.9whenδ= 3,4) and the
smaller∥∇f(θe)∥became. That is, using increasing learning rates based on tripling and quadrupling batch
sizes minimizes∥∇f(θe)∥faster than using increasing learning rates based on doubly increasing batch sizes
(see also Appendix A.6). Figure 5(c) and (d) indicate that using δ= 3,4was better than using δ= 2in the
sense of minimizing f(θe)and achieving high test accuracy.
13Under review as submission to TMLR
0 50 100 150 200 250 300
Epochs0.00.20.40.60.81.0Learning RateLearning Rate and Batch Size Schedular
242628210212
Batch Sizeconstant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
Batch Size (=2.0)
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR100
constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs10203040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 300717273constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
 (d) Test accuracy score versus epochs
Figure 4: (a) Warm-up learning rates and doubly increasing batch size every 30 epochs, (b) full gradient
norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train ResNet-18
on CIFAR100 dataset.
0 50 100 150 200 250 300
Epochs0.100.120.140.160.180.20Learning RateLearning Rate and Batch Size Schedular
242628210212
Batch SizeBS: =2.0
BS: =3.0
BS: =4.0
LR: =1.08
LR: =1.50
LR: =1.90
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR100
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs10203040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 300717273
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
 (d) Test accuracy score versus epochs
Figure 5: (a) Increasing learning rates and increasing batch sizes based on δ= 2,3,4, (b) full gradient norm
of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train ResNet-18 on
CIFAR100 dataset.
To better understand the impact of increasing both batch size and learning rate, we compare the full gradient
norm across the four cases Case (i) –Case (iv) using ResNet-18 on CIFAR-100 in Figure 6. The results
highlight that Case (iv) , which employs both a larger batch size and a warm-up learning rate, exhibits
the most favorable convergence properties. Specifically, Case (iv) achieves a significantly reduced full
gradient norm throughout training compared to the other cases, confirming its effectiveness. Building on
14Under review as submission to TMLR
these observations, we validate the scalability of Case (iv) on the larger ImageNet dataset. The results,
presented in Appendix A.5, further highlight the advantages of this scaling strategy in improving training
efficiency and accelerating convergence on large-scale tasks.
0 50 100 150 200 250 300
Epochs102
101
100Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
Case(i) : Const-BS, Decay-LR
Case(ii) : Incr-BS, Decay-LR
Case(iii) : Incr-BS, Incr-LR
Case(iv) : Incr-BS, Warmup-LR
Figure 6: Performance comparison of best configurations for full gradient norm using Case (i) to Case (iv)
to train ResNet-18 on CIFAR100 dataset.
5 Conclusion
This paper presented theoretical analyses of mini-batch SGD under batch size and learning rate schedulers
used in practice. Our results indicated that using increasing batch sizes and decaying learning rates guaran-
tees convergence of mini-batch SGD and using both batch sizes and learning rates that increase accelerates
mini-batch SGD. That is, using increasing batch sizes and decaying learning rates with warm-up guarantees
fast convergence of mini-batch SGD in the sense of minimizing the expectation of the full gradient norm of
the empirical loss. This paper also provided numerical results to support the analysis results that increasing
both batch sizes and learning rates accelerates mini-batch SGD. One limitation of this study is that the num-
bers of models and datasets in the experiments were limited. Hence, we should conduct similar experiments
with larger numbers of models and datasets to support our theoretical results.
References
Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive batch sizes with learning rates, 2016.
Thirty-Third Conference on Uncertainty in Artificial Intelligence, 2017.
Amir Beck. First-Order Methods in Optimization . Society for Industrial and Applied Mathematics, Philadel-
phia, PA, 2017.
Richard H. Byrd, Gillian M. Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in optimization
methods for machine learning. Mathematical Programming , 134(1):127–155, 2012.
Hao Chen, Lili Zheng, Raed AL Kontar, and Garvesh Raskutti. Stochastic gradient descent in correlated set-
tings: A study on Gaussian processes. In Advances in Neural Information Processing Systems , volume 33,
2020.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
IEEE Transactions on Pattern Analysis and Machine Intelligence , 40(4):834–848, 2018.
15Under review as submission to TMLR
SohamDe, AbhayYadav, DavidJacobs, andTomGoldstein. AutomatedInferencewithAdaptiveBatches. In
AartiSinghandJerryZhu(eds.), Proceedings of the 20th International Conference on Artificial Intelligence
and Statistics , volume 54 of Proceedings of Machine Learning Research , pp. 1504–1513. PMLR, 2017.
Benjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradient
descent method for non-convex objective functions. Journal of Machine Learning Research , 21:1–48, 2020.
Guillaume Garrigos and Robert M. Gower. Handbook of convergence theorems for (stochastic) gradient
methods, 2024. URL https://arxiv.org/abs/2301.11235 .
SaeedGhadimiandGuanghuiLan. Optimalstochasticapproximationalgorithmsforstronglyconvexstochas-
tic composite optimization I: A generic algorithmic framework. SIAM Journal on Optimization , 22:1469–
1492, 2012.
SaeedGhadimiandGuanghuiLan. Optimalstochasticapproximationalgorithmsforstronglyconvexstochas-
tic composite optimization II: Shrinking procedures and optimal algorithms. SIAM Journal on Optimiza-
tion, 23:2061–2089, 2013.
Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep learning
heuristics: Learning rate restarts, warmup and distillation. In International Conference on Learning
Representations , 2019.
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour,
2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Computer Vision and Pattern Recognition , pp. 770–778, 2016.
T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li. Bag of tricks for image classification with convo-
lutional neural networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 558–567, 2019.
Andrew Hundt, Varun Jain, and Gregory D. Hager. sharpDARTS: Faster and more accurate differentiable
architecture search, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pp. 448–456,
2015.
AhmedKhaledandPeterRichtárik. BettertheoryforSGDinthenonconvexworld. Transactions on Machine
Learning Research , 2023.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.
On the variance of the adaptive learning rate and beyond. In International Conference on Learning
Representations , 2020.
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for
SGD: An adaptive learning rate for fast convergence. In Proceedings of the 24th International Conference
on Artificial Intelligence and Statistics , volume 130, 2021.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations , 2017.
Jun Lu. Gradient descent, stochastic optimization, and other tales, 2024.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approxima-
tion approach to stochastic programming. SIAM Journal on Optimization , 19:1574–1609, 2009.
16Under review as submission to TMLR
Herbert Robbins and Herbert Monro. A stochastic approximation method. The Annals of Mathematical
Statistics , 22:400–407, 1951.
Kevin Scaman and Cédric Malherbe. Robustness analysis of non-convex stochastic gradient descent using
biased expectations. In Advances in Neural Information Processing Systems , volume 33, 2020.
Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E.
Dahl. Measuring the effects of data parallelism on neural network training. Journal of Machine Learning
Research , 20:1–49, 2019.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase the batch
size. InInternational Conference on Learning Representations , 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems ,
volume 30, 2017.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien.
Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Advances in Neural
Information Processing Systems , volume 32, 2019.
Xiaoyu Wang, Sindri Magnússon, and Mikael Johansson. On the convergence of step decay step-size for
stochastic optimization. In Advances in Neural Information Processing Systems , 2021.
Yuting Wu, Daniel J. Holland, Mick D. Mantle, Andrew G. Wilson, Sebastian Nowozin, Andrew Blake, and
Lynn F. Gladden. A Bayesian method to quantifying chemical composition using NMR: Application to
porous media systems. In 2014 22nd European Signal Processing Conference , pp. 2515–2519, 2014.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl, Christopher J.
Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes? Insights from a noisy
quadratic model. In Advances in Neural Information Processing Systems , volume 32, 2019.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings
of the 20th International Conference on Machine Learning , pp. 928–936, 2003.
A Appendix
We here give the notation and state some definitions. Let Nbe the set of natural numbers. Define [n] :=
{1,2,···,n}and[0 :n] :={0,1,···,n}forn∈N. LetRdbe thed-dimensional Euclidean space with inner
product⟨θ1,θ2⟩=θ⊤
1θ2(θ1,θ2∈Rd)and its induced norm ∥θ∥:=/radicalbig
⟨θ,θ⟩(θ∈Rd). Let Rd
+:={θ=
(θ1,θ2,...,θd)⊤∈Rd:θi≥0 (i∈[d])}andRd
++:={θ= (θ1,θ2,...,θd)⊤∈Rd:θi>0 (i∈[d])}. The
gradient of a differentiable function f:Rd→Ratθ∈Rdis denoted by∇f(θ). LetL>0. A differentiable
functionf:Rd→Ris said to be L-smooth if the gradient ∇f:Rd→Rdis Lipschitz continuous, i.e., for
allθ1,θ2∈Rd,∥∇f(θ1)−∇f(θ2)∥≤L∥θ1−θ2∥. Let (xt),(yt)⊂R+be sequences. Let Obe Landau’s
symbol, i.e., yt=O(xt)if there exist c∈R+andt0∈Nsuch that, for all t≥t0,yt≤cxt.
A.1 Example of stochastic gradient satisfying (A2) under (A1)
Suppose that (A1) holds and a random variable ξfollows a uniform distribution. Then, let us show that
(A2) holds, that is, for all θ∈Rd, (i)Eξ[∇fξ(θ)] =∇f(θ)and (ii) Vξ[∇fξ(θ)]≤σ2.
(i) Sinceξis independent of θ, we have that
Eξ[∇fξ(θ)] =1
n/summationdisplay
i∈[n]∇fi(θ) =∇
1
n/summationdisplay
i∈[n]fi
(θ) =∇f(θ).
17Under review as submission to TMLR
(ii) Let ¯θ:=θ−1
Li∇fi(θ). The descent lemma and f⋆
i= inf{fi(θ):θ∈Rd}∈Rensure that
f⋆
i≤fi(¯θ)≤fi(θ) +⟨∇fi(θ),¯θ−θ⟩+Li
2∥¯θ−θ∥2
=fi(θ)−1
Li∥∇fi(θ)∥2+1
2Li∥∇fi(θ)∥2
=fi(θ)−1
2Li∥∇fi(θ)∥2,
which implies that, for all i∈[n],
∥∇fi(θ)∥2≤2Li(fi(θ)−f⋆
i)≤2LiMi,
whereMi:= sup{fi(θ)−f⋆
i:θ∈Rd}. Hence, from (i),
Vξ[∇fξ(θ)] =Eξ[∥∇fξ(θ)−∇f(θ)∥2]
=Eξ[∥∇fξ(θ)∥2]−2Eξ[⟨∇fξ(θ),∇f(θ)⟩] +Eξ[∥∇f(θ)∥2]
=Eξ[∥∇fξ(θ)∥2]−2∥∇f(θ)∥2+∥∇f(θ)∥2
=1
n/summationdisplay
i∈[n]∥∇fi(θ)∥2−∥∇f(θ)∥2
≤2
n/summationdisplay
i∈[n]LiMi.
A.2 Proofs of Proposition A.1 and Lemma 2.1
The following proposition holds for the mini-batch gradient.
Proposition A.1 Lett∈Nandξtbe a random variable that is independent of ξj(j∈[0 :t−1]); let
θt∈Rdbe independent of ξt; let∇fBt(θt)be the mini-batch gradient defined by Algorithm 1, where fξt,i
(i∈[bt]) is the stochastic gradient (see Assumption 2.1(A2)). Then, the following hold:
Eξt/bracketleftig
∇fBt(θt)/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
=∇f(θt)andVξt/bracketleftig
∇fBt(θt)/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
≤σ2
bt,
where Eξt[·|ˆξt−1]andVξt[·|ˆξt−1]are respectively the expectation and variance with respect to ξtconditioned
onξt−1=ˆξt−1.
ThefirstequationinPropositionA.1indicatesthatthemini-batchgradient ∇fBt(θt)isanunbiasedestimator
of the full gradient ∇f(θt). The second inequality in Proposition A.1 indicates that the upper bound on the
variance of the mini-batch gradient ∇fBt(θt)is inversely proportional to the batch size bt.
Assumption2.1(A3)holdsundersamplingwithreplacement, where ξ= (ξ1,ξ2,···,ξb)⊤areindependentand
identically distributed (i.i.d.). In practice, however, sampling without replacement is commonly used to im-
prove data efficiency and diversity. Under sampling without replacement, minor dependencies arise between
samples, particularly when b≈n. These dependencies reduce the conditional variance Vξt[∇fBt(θt)|ˆξt−1],
resulting in behavior that closely resembles full-batch gradient computation. On the other hand, when
b≪n, the dependencies introduced by sampling without replacement become negligible, and the behavior
of the mini-batch gradient closely approximates that of i.i.d. samples. Under such conditions, the theoretical
properties of Assumption 2.1(A3) are approximately satisfied. Therefore, in large-scale datasets, the approx-
imation of i.i.d. is sufficient to ensure that the theoretical predictions remain valid under sampling without
replacement. The experimental results (Appendix A.8) further confirm the validity of this approximation,
as the observed behavior aligns closely with the theoretical predictions.
The proof of Proposition A.1 is based on standard results from the literature on SGD analysis (Garrigos &
Gower, 2024, Section 6).
18Under review as submission to TMLR
Proof of Proposition A.1: Assumption 2.1(A3) and the independence of btandξtensure that
Eξt/bracketleftig
∇fBt(θt)/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
=Eξt/bracketleftigg
1
btbt/summationdisplay
i=1∇fξt,i(θt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightigg
=1
btbt/summationdisplay
i=1Eξt,i/bracketleftig
∇fξt,i(θt)/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
,
which, together with Assumption 2.1(A2)(i) and the independence of ξtandξt−1, implies that
Eξt/bracketleftig
∇fBt(θt)/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
=1
btbt/summationdisplay
i=1∇f(θt) =∇f(θt). (17)
Assumption 2.1(A3), the independence of btandξt, and (17) imply that
Vξt/bracketleftig
∇fBt(θt)/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
=Eξt/bracketleftig
∥∇fBt(θt)−∇f(θt)∥2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
=Eξt
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
btbt/summationdisplay
i=1∇fξt,i(θt)−∇f(θt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleˆξt−1

=1
b2
tEξt
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublebt/summationdisplay
i=1/parenleftbig
∇fξt,i(θt)−∇f(θt)/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleˆξt−1
.
From the independence of ξt,iandξt,j(i̸=j) and Assumption 2.1(A2)(i), for all i,j∈[bt]such thati̸=j,
Eξt,i[⟨∇fξt,i(θt)−∇f(θt),∇fξt,j(θt)−∇f(θt)⟩|ˆξt−1]
=⟨Eξt,i[∇fξt,i(θt)|ˆξt−1]−Eξt,i[∇f(θt)|ˆξt−1],∇fξt,j(θt)−∇f(θt)⟩
= 0.
Hence, Assumption 2.1(A2)(ii) guarantees that
Vξt/bracketleftig
∇fBt(θ)/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
=1
b2
tbt/summationdisplay
i=1Eξt,i/bracketleftig/vextenddouble/vextenddouble∇fξt,i(θt)−∇f(θt)/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
≤σ2bt
b2
t=σ2
bt,
which completes the proof. 2
Proof of Lemma 2.1: The ¯L-smoothness of fimplies that the descent lemma holds; i.e., for all t∈N,
f(θt+1)≤f(θt) +⟨∇f(θt),θt+1−θt⟩+¯L
2∥θt+1−θt∥2,
which, together with θt+1:=θt−ηt∇fBt(θt), implies that
f(θt+1)≤f(θt)−ηt⟨∇f(θt),∇fBt(θt)⟩+¯Lη2
t
2∥∇fBt(θt)∥2. (18)
Proposition A.1 guarantees that
Eξt/bracketleftig
∥∇fBt(θt)∥2|ˆξt−1/bracketrightig
=Eξt/bracketleftig
∥∇fBt(θt)−∇f(θt) +∇f(θt)∥2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
=Eξt/bracketleftig
∥∇fBt(θt)−∇f(θt)∥2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
+ 2Eξt/bracketleftig
⟨∇fBt(θt)−∇f(θt),∇f(θt)⟩/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
+Eξt/bracketleftig
∥∇f(θt)∥2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
≤σ2
bt+∥∇f(θt)∥2.(19)
19Under review as submission to TMLR
Taking the expectation conditioned on ξt−1=ˆξt−1on both sides of (18), together with Proposition A.1 and
(19), guarantees that, for all t∈N,
Eξt/bracketleftig
f(θt+1)/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
≤f(θt)−ηtEξt/bracketleftig
⟨∇f(θt),∇fBt(θt)⟩/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
+¯Lη2
t
2Eξt/bracketleftig
∥∇fBt(θt)∥2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
≤f(θt)−ηt∥∇f(θt)∥2+¯Lη2
t
2/parenleftbiggσ2
bt+∥∇f(θt)∥2/parenrightbigg
.
Hence, taking the total expectation on both sides of the above inequality ensures that, for all t∈N,
ηk/parenleftbigg
1−¯Lηt
2/parenrightbigg
E/bracketleftig
∥∇f(θt)∥2/bracketrightig
≤E[f(θt)−f(θt+1)] +¯Lσ2η2
t
2bt.
LetT∈N. Summing the above inequality from t= 0tot=T−1ensures that
T−1/summationdisplay
t=0ηt/parenleftbigg
1−¯Lηt
2/parenrightbigg
E/bracketleftig
∥∇f(θt)∥2/bracketrightig
≤E[f(θ0)−f(θT)] +¯Lσ2
2T−1/summationdisplay
t=0η2
t
bt,
which, together with Assumption 2.1(A1) (the lower bound f⋆:=1
n/summationtext
i∈[n]f⋆
ioff), implies that
T−1/summationdisplay
t=0ηt/parenleftbigg
1−¯Lηt
2/parenrightbigg
E/bracketleftig
∥∇f(θt)∥2/bracketrightig
≤f(θ0)−f⋆+¯Lσ2
2T−1/summationdisplay
t=0η2
t
bt.
Sinceηt∈[ηmin,ηmax], we have that
/parenleftbigg
1−¯Lηmax
2/parenrightbiggT−1/summationdisplay
t=0ηtE/bracketleftig
∥∇f(θt)∥2/bracketrightig
≤f(θ0)−f⋆+¯Lσ2
2T−1/summationdisplay
t=0η2
t
bt,
which, together with ηt∈[ηmin,ηmax]⊂[0,2
¯L), implies that
T−1/summationdisplay
t=0ηtE/bracketleftig
∥∇f(θt)∥2/bracketrightig
≤2(f(θ0)−f⋆)
2−¯Lηmax+¯Lσ2
2−¯LηmaxT−1/summationdisplay
t=0η2
t
bt. (20)
Therefore, from/summationtextT−1
t=0ηt̸= 0, we have
min
t∈[0:T−1]E[∥∇f(θt)∥2]≤2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt+¯Lσ2
2−¯Lηmax/summationtextT−1
t=0η2
tb−1
t/summationtextT−1
t=0ηt, (21)
which implies that the assertion in Lemma 2.1 holds. 2
A.3 Proofs of Theorems
We can also consider the case where batch sizes decay. For simplicity, let us set a constant learning rate ηt=
η>0andadecayingbatchsize bt=b
t+1, whereb>0. Then, wehavethat VT≤η
T/summationtextT−1
t=01
bt=η(T+1)
2b→+∞
(T→+∞), which implies that convergence of mini-batch SGD is not guaranteed. Accordingly, this paper
focuses on the four cases in the main text.
Proof of Theorem 3.1: Letηmax=η.
[Constant LR (3)] We have that
BT=1/summationtextT−1
t=0η=1
ηT, VT=/summationtextT−1
t=0η2
b/summationtextT−1
t=0η=η
b.
20Under review as submission to TMLR
[Diminishing LR (4)] We have that
T−1/summationdisplay
t=01√t+ 1≥/integraldisplayT
0dt√t+ 1= 2(√
T+ 1−1),
which implies that
BT=1/summationtextT−1
t=0η√t+1≤1
2η(√
T+ 1−1).
We also have that
T−1/summationdisplay
t=01
t+ 1≤1 +/integraldisplayT−1
0dt
t+ 1= 1 + logT,
which implies that
VT=η/summationtextT−1
t=01
t+1
b/summationtextT−1
t=01√t+1≤η(1 + logT)
2b(√
T+ 1−1).
[Cosine LR (5)] We have
KE−1/summationdisplay
t=0ηt=ηminKE+ηmax−ηmin
2KE+ηmax−ηmin
2KE−1/summationdisplay
t=0cos/floorleftbiggt
K/floorrightbiggπ
E.
From/summationtextKE
t=0cos⌊t
K⌋π
E=K−1, we have
KE−1/summationdisplay
t=0cos/floorleftbiggt
K/floorrightbiggπ
E=K−1−cosπ=K. (22)
We thus have
KE−1/summationdisplay
t=0ηt=ηminKE+ηmax−ηmin
2KE+ηmax−ηmin
2K
=1
2{(ηmin+ηmax)KE+ (ηmax−ηmin)K}
≥(ηmin+ηmax)KE
2.
Moreover, we have that
KE−1/summationdisplay
t=0η2
t=η2
minKE+ηmin(ηmax−ηmin)KE−1/summationdisplay
t=0/parenleftbigg
1 + cos/floorleftbiggt
K/floorrightbiggπ
E/parenrightbigg
+(ηmax−ηmin)2
4KE−1/summationdisplay
t=0/parenleftbigg
1 + cos/floorleftbiggt
K/floorrightbiggπ
E/parenrightbigg2
,
which implies that
KE−1/summationdisplay
t=0η2
t=ηminηmaxKE+(ηmax−ηmin)2
4KE+ηmin(ηmax−ηmin)KE−1/summationdisplay
t=0cos/floorleftbiggt
K/floorrightbiggπ
E
+(ηmax−ηmin)2
2KE−1/summationdisplay
t=0cos/floorleftbiggt
K/floorrightbiggπ
E+(ηmax−ηmin)2
4KE−1/summationdisplay
t=0cos2/floorleftbiggt
K/floorrightbiggπ
E.
21Under review as submission to TMLR
From
KE/summationdisplay
t=0cos2/floorleftbiggt
K/floorrightbiggπ
E=1
2KE/summationdisplay
t=0/parenleftbigg
1 + cos 2/floorleftbiggt
K/floorrightbiggπ
E/parenrightbigg
=1
2(KE+ 1) +1
2
=KE
2+ 1,
we have
KE−1/summationdisplay
t=0cos2/floorleftbiggt
K/floorrightbiggπ
E=KE
2+ 1−cos2π=KE
2.
From (22), we have
KE−1/summationdisplay
t=0η2
t=(ηmin+ηmax)2
4KE+ηmin(ηmax−ηmin)K+(ηmax−ηmin)2
2K+(ηmax−ηmin)2
4KE
2
=3η2
min+ 2ηminηmax+ 3η2
max
8KE+(ηmax−ηmin)(ηmax+ηmin)
2K.
Hence, we have
BT=1/summationtextKE−1
t=0ηt≤2
(ηmin+ηmax)KE
and
VT=/summationtextKE−1
t=0η2
t
b/summationtextKE−1
t=0ηt≤3η2
min+ 2ηminηmax+ 3η2
max
4(ηmin+ηmax)b+ηmax−ηmin
bE.
[Polynomial LR (6)] Since f(x) = (1−x)pis monotone decreasing for x∈[0,1), we have that
/integraldisplay1
0(1−x)pdx<1
TT−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbiggp
,
which implies that
T/integraldisplay1
0(1−x)pdx<T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbiggp
. (23)
Since/integraltext1
0(1−x)pdx=1
p+1, (23) implies that
T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbiggp
>T
p+ 1.
Accordingly,
T−1/summationdisplay
t=0ηt= (ηmax−ηmin)T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbiggp
+ηminT
>(ηmax−ηmin)T
p+ 1+ηminT
22Under review as submission to TMLR
=/parenleftbiggηmax−ηmin
p+ 1+ηmin/parenrightbigg
T
=ηmax+ηminp
p+ 1T.
Sincef(x) = (1−x)pandg(x) = (1−x)2pare monotone decreasing for x∈[0,1), we have that
1
TT−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbiggp
<1
T+/integraldisplay1
0(1−x)pdx,1
TT−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbigg2p
<1
T+/integraldisplay1
0(1−x)2pdx,
which imply that
T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbiggp
<1 +T/integraldisplay1
0(1−x)pdx,T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbigg2p
<1 +T/integraldisplay1
0(1−x)2pdx. (24)
Since we have that/integraltext1
0(1−x)pdx=1
p+1and/integraltext1
0(1−x)2pdx=1
2p+1, (24) ensures that
T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbiggp
<1 +T
p+ 1,T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbigg2p
<1 +T
2p+ 1.
Hence,
T−1/summationdisplay
t=0η2
t= (ηmax−ηmin)2T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbigg2p
+ 2(ηmax−ηmin)T−1/summationdisplay
t=0/parenleftbigg
1−t
T/parenrightbiggp
ηmin+η2
minT
<(ηmax−ηmin)2/parenleftbigg
1 +T
2p+ 1/parenrightbigg
+ 2(ηmax−ηmin)/parenleftbigg
1 +T
p+ 1/parenrightbigg
ηmin+η2
minT
=η2
max(p+ 1)(2p+T+ 1) + 2ηmaxηminpT+η2
min(2p2(T−1)−3p−1)
(p+ 1)(2p+ 1).
Therefore,
BT=1/summationtextT−1
t=0ηt≤p+ 1
(ηmax+ηminp)T
and
VT=/summationtextT−1
t=0η2
t
b/summationtextT−1
t=0ηt
=η2
max(p+ 1)(2p+T+ 1) + 2ηmaxηminpT+η2
min(2p2(T−1)−3p−1)
(2p+ 1)(ηmax+ηminp)bT
=2p2η2
min+ 2pηminηmax+ (p+ 1)η2
max
(2p+ 1)(pηmin+ηmax)b+(p+ 1)(2p+ 1)η2
max−(p+ 1)(2p+ 1)η2
min
(2p+ 1)(pηmin+ηmax)bT
=2p2η2
min+ 2pηminηmax+ (p+ 1)η2
max
(2p+ 1)(pηmin+ηmax)b+(p+ 1)(η2
max−η2
min)
(pηmin+ηmax)bT.
This completes the proof. 2
We will now show the following theorem, which includes Theorem 3.2.
Theorem A.1 (Convergence rate of SGD using (9)) Under the assumptions in Lemma 2.1, Algo-
rithm 1 using (9) satisfies that, for all M∈N,
min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
BT+¯Lσ2
2−¯Lηmax1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
VT,
23Under review as submission to TMLR
whereT=/summationtextM
m=0KmEm,Emax= supM∈Nsupm∈[0:M]Em<+∞,Kmax= supM∈Nsupm∈[0:M]Km<+∞,
a= min{a,b0},BTis defined as in (7), and VTis given by
VT≤

3ηmaxKmaxEmax
acT[Constant LR (3)]
3ηmaxKmaxEmax
2ac(√
T+ 1−1)[Diminishing LR (4)]
6η2
maxKmaxEmax
ac(ηmin+ηmax)T[Cosine LR (5)]
3(p+ 1)η2
maxKmaxEmax
ac(ηmax+ηminp)T[Polynomial LR (6)]([Polynomial BS (10)])
VT≤

δηmaxKmaxEmax
(δ−1)b0T[Constant LR (3)]
δηmaxKmaxEmax
2(δ−1)b0(√
T+ 1−1)[Diminishing LR (4)]
2δη2
maxKmaxEmax
(δ−1)(ηmin+ηmax)b0T[Cosine LR (5)]
(p+ 1)δη2
maxKmaxEmax
(δ−1)(ηmax+ηminp)b0T[Polynomial LR (6)] .([Exponential BS (11)])
That is, Algorithm 1 using each of Polynomial BS (10) and Exponential BS (11) has the convergence rate
min
t∈[0:T−1]E[∥∇f(θt)∥] =

O/parenleftbigg1√
T/parenrightbigg
[Constant LR (3), Cosine LR (5), Polynomial LR (6)]
O/parenleftbigg1
T1
4/parenrightbigg
[Diminishing LR (4)] .
Proof of Theorem A.1: LetM∈NandT=/summationtextM
m=0KmEm, whereEmax= supM∈Nsupm∈[0:M]Em<+∞,
Kmax= supM∈Nsupm∈[0:M]Km<+∞,S0:=N∩[0,K0E0), andSm=N∩[/summationtextm−1
k=0KkEk,/summationtextm
k=0KkEk)
(m∈[M]). Let us consider using (10). Let ηmax=ηanda= min{a,b0}.
[Constant LR (3)] Let m∈[M]. We have that
/summationdisplay
t∈Sm1
bt=/summationdisplay
t∈Sm1/parenleftbigg
am/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
+b0/parenrightbiggc≤/summationdisplay
t∈Sm1
acmc/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbiggc
≤/summationdisplay
t∈Sm1
acmc≤1
acmcKmEm≤KmaxEmax
ac1
mc≤KmaxEmax
ac1
mc
and
/summationdisplay
t∈S01
bt=/summationdisplay
t∈S01
bc
0≤KmaxEmax
ac.
Accordingly, we have that
M/summationdisplay
m=0/summationdisplay
t∈Sm1
bt≤KmaxEmax
ac/parenleftigg
1 +M/summationdisplay
m=11
mc/parenrightigg
≤KmaxEmax
ac/parenleftigg
1 ++∞/summationdisplay
m=11
mc/parenrightigg
≤3KmaxEmax
ac.(25)
Hence, we have that
VT=1/summationtextT−1
t=0ηT−1/summationdisplay
t=0η2
bt≤3ηKmaxEmax
acT.
24Under review as submission to TMLR
[Diminishing LR (4)] From (25), we have that
VT=1/summationtextT−1
t=0η√t+1T−1/summationdisplay
t=0η2
(t+ 1)bt
≤η
2(√
T+ 1−1)T−1/summationdisplay
t=01
bt≤3ηKmaxEmax
2ac(√
T+ 1−1).
[Cosine LR (5)] The cosine LR is defined for all m∈[0 :M]and allt∈Smby
ηt=ηmin+ηmax−ηmin
2/braceleftigg
1 + cos/parenleftiggm−1/summationdisplay
k=0Ek+/floorleftigg
t−/summationtextm−1
k=0KkEk
Km/floorrightigg/parenrightigg
π
EM/bracerightigg
.
We have that
T−1/summationdisplay
t=0η2
t
bt≤η2
maxT−1/summationdisplay
t=01
bt,
which, together with (25), implies that
T−1/summationdisplay
t=0η2
t
bt≤3η2
maxKmaxEmax
ac.
Hence, we have that
VT=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt≤6η2
maxKmaxEmax
ac(ηmin+ηmax)T.
[Polynomial LR (6)] We have that
T−1/summationdisplay
t=0η2
t
bt=T−1/summationdisplay
t=01
bt/braceleftbigg
(ηmax−ηmin)/parenleftbigg
1−t
T/parenrightbiggp
+ηmin/bracerightbigg2
≤η2
maxT−1/summationdisplay
t=01
bt,
which, together with (25), implies that
T−1/summationdisplay
t=0η2
t
bt≤3η2
maxKmaxEmax
ac.
Hence, we have that
VT=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt≤3(p+ 1)η2
maxKmaxEmax
ac(ηmax+ηminp)T.
Let us consider using (11). Let ηmax=η.
[Constant LR (3)] We have that
/summationdisplay
t∈Sm1
bt=/summationdisplay
t∈Sm1
δm/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
b0≤/summationdisplay
t∈Sm1
δmb0≤KmaxEmax
δmb0,
which implies that
M/summationdisplay
m=0/summationdisplay
t∈Sm1
bt≤KmaxEmax
b0M/summationdisplay
m=01
δm≤KmaxEmaxδ
b0(δ−1). (26)
25Under review as submission to TMLR
Hence, we have that
VT=1/summationtextT−1
t=0ηT−1/summationdisplay
t=0η2
bt≤ηKmaxEmaxδ
b0(δ−1)T.
[Diminishing LR (4)] From (26), we have that
VT=1/summationtextT−1
t=0η√t+1T−1/summationdisplay
t=0η2
(t+ 1)bt≤η
2(√
T+ 1−1)T−1/summationdisplay
t=01
bt≤ηKmaxEmaxδ
2(√
T+ 1−1)b0(δ−1).
[Cosine LR (5)] We have that
T−1/summationdisplay
t=0η2
t
bt≤η2
maxT−1/summationdisplay
t=01
bt,
which, together with (26), implies that
T−1/summationdisplay
t=0η2
t
bt≤η2
maxKmaxEmaxδ
b0(δ−1).
Hence, we have that
VT=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt≤2η2
maxKmaxEmaxδ
(δ−1)(ηmin+ηmax)b0T.
[Polynomial LR (6)] We have that
T−1/summationdisplay
t=0η2
t
bt=T−1/summationdisplay
t=01
bt/braceleftbigg
(ηmax−ηmin)/parenleftbigg
1−t
T/parenrightbiggp
+ηmin/bracerightbigg2
≤η2
maxT−1/summationdisplay
t=01
bt,
which, together with (26), implies that
T−1/summationdisplay
t=0η2
t
bt≤η2
maxKmaxEmaxδ
b0(δ−1).
Hence, we have that
VT=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt≤(p+ 1)η2
maxKmaxEmaxδ
(δ−1)(ηmax+ηminp)b0T.
2
Example of btandηtsatisfying (12) is as follows:
[Polynomial growth BS and LR]
bt=/parenleftbigg
a1m/ceilingleftbiggt/summationtextm
k=0KkEk/ceilingrightbigg
+b0/parenrightbiggc1
, ηt=/parenleftbigg
a2m/ceilingleftbiggt/summationtextm
k=0KkEk/ceilingrightbigg
+η0/parenrightbiggc2
,(27)
wherea1,a2>0;c1>1,c2>0such thatc1−2c2>1.
We next show the following theorem, which includes Theorem 3.3.
26Under review as submission to TMLR
Theorem A.2 (Convergence rate of SGD using (12)) Under the assumptions in Lemma 2.1, Algo-
rithm 1 using (12) satisfies that, for all M∈N,
min
t∈[0:T−1]E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤2(f(θ0)−f⋆)
2−¯Lηmax1/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
BT+¯Lσ2
2−¯Lηmax1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
VT,
whereT=/summationtextM
m=0KmEm,Emax= supM∈Nsupm∈[0:M]Em<+∞,Emin= infM∈Ninfm∈[0:M]Em<+∞,
Kmax= supM∈Nsupm∈[0:M]Km<+∞,Kmin= infM∈Ninfm∈[0:M]Km<+∞,η= min{a2,η0},η=
max{a2,η0},b= min{a1,b0},ˆγ=γ2
δ<1,
BT≤

1 +c2
ηc2KminEminM1+c2[Polynomial growth BS and LR (27)]
δ
η0KminEminγM[Exponential growth BS and LR (13)]
VT≤

2KmaxEmax(1 +c2)η2c2
KminEminηc2bc1M1+c2[Polynomial growth BS and LR (27)]
KmaxEmaxη0δ
KminEminb0(1−ˆγ)γM[Exponential growth BS and LR (13)] .
That is, Algorithm 1 has the convergence rate
min
t∈[0:T−1]E[∥∇f(θt)∥] =

O/parenleftbigg1
M1+c2
2/parenrightbigg
[Polynomial growth BS and LR (27)]
O/parenleftbigg1
γM
2/parenrightbigg
[Exponential growth BS and LR (13)] .
Proof of Theorem A.2: LetM∈NandT=/summationtextM
m=0KmEm, whereEmax= supM∈Nsupm∈[0:M]Em<+∞,
Kmax= supM∈Nsupm∈[0:M]Km<+∞,S0:=N∩[0,K0E0), andSm=N∩[/summationtextm−1
k=0KkEk,/summationtextm
k=0KkEk)
(m∈[M]).
[Polynomial growth BS and LR (27)] We have that
/summationdisplay
t∈Smηt=/summationdisplay
t∈Sm/parenleftbigg
a2m/ceilingleftbiggt/summationtextm
k=0KkEk/ceilingrightbigg
+η0/parenrightbiggc2
≥/summationdisplay
t∈Sm(a2m+η0)c2,
which, together with η= min{a2,η0}, implies that
/summationdisplay
t∈Smηt≥ηc2/summationdisplay
t∈Sm(m+ 1)c2≥ηc2KminEmin(m+ 1)c2.
Hence,
M/summationdisplay
m=0/summationdisplay
t∈Smηt≥ηc2KminEminM+1/summationdisplay
m=1mc2≥ηc2KminEmin
1 +c2M1+c2.
We also have that
/summationdisplay
t∈Smη2
t
bt=/summationdisplay
t∈Sm/parenleftbigg
a2m/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
+η0/parenrightbigg2c2
/parenleftbigg
a1m/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
+b0/parenrightbiggc1≤/summationdisplay
t∈Sm(a2m+η0)2c2
(a1m+b0)c1.
27Under review as submission to TMLR
Letη= max{a2,η0}andb= min{a1,b0}. Then,
M/summationdisplay
m=0/summationdisplay
t∈Smη2
t
bt≤KmaxEmaxη2c2
bc1M/summationdisplay
m=0(m+ 1)2c2
(m+ 1)c1≤KmaxEmaxη2c2
bc1M+1/summationdisplay
m=11
mc1−2c2
≤2KmaxEmaxη2c2
bc1.
Hence,
BT=1/summationtextT−1
t=0ηt≤1 +c2
ηc2KminEminM1+c2
and
VT=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt≤2KmaxEmax(1 +c2)η2c2
KminEminηc2bc1M1+c2.
[Exponential growth BS and LR (13)] We have that
M/summationdisplay
m=0/summationdisplay
t∈Smηt=M/summationdisplay
m=0/summationdisplay
t∈Smγm/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
η0≥η0KminEminM/summationdisplay
m=0γm
=η0KminEminγM−1
γ−1>η0KminEminγM
γ2>η0KminEminγM
δ
and
M/summationdisplay
m=0/summationdisplay
t∈Smη2
t
bt=M/summationdisplay
m=0/summationdisplay
t∈Smγ2m/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
η2
0
δm/ceilingleftbigg
t/summationtextm
k=0KkEk/ceilingrightbigg
b0≤KmaxEmaxη2
0
b0M/summationdisplay
m=0γ2m
δm
≤KmaxEmaxη2
0
b0M/summationdisplay
m=0/parenleftbiggγ2
δ/parenrightbiggm
≤KmaxEmaxη2
0
b01
1−ˆγ,
where ˆγ=γ2
δ<1. Hence,
BT=1/summationtextT−1
t=0ηt≤δ
η0KminEminγM
and
VT=1/summationtextT−1
t=0ηtT−1/summationdisplay
t=0η2
t
bt≤KmaxEmaxη0δ
KminEminb0(1−ˆγ)γM.
2
Proof of Theorem 3.4: Theorem 3.4 follows immediately from Theorems 3.2 and 3.3. 2
A.4 Proofs of Convergence Results under Convexity
Using Lemma 2.1, we have the following lemma.
28Under review as submission to TMLR
Lemma A.1 Suppose that Assumption 2.1 holds and fis convex and consider the sequence (θt)generated
by Algorithm 1 with ηt∈[ηmin,ηmax]⊂[0,2
Ln)satisfying/summationtextT−1
t=0ηt̸= 0, where ¯L:=1
n/summationtext
i∈[n]Li,f⋆:=
1
n/summationtext
i∈[n]f⋆
i,θ⋆∈Rdis a global minimizer of f, andf⋆:=f(θ⋆). Then, for all T∈N,
min
t∈[0:T−1]E[f(θt)−f⋆]
≤/parenleftbigg∥θ0−θ⋆∥2
2+ηmax(f(θ0)−f⋆)
2−¯Lηmax/parenrightbigg1/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
BT+σ2
2/parenleftbigg
1 +¯Lηmax
2−¯Lηmax/parenrightbigg/summationtextT−1
t=0η2
tb−1
t/summationtextT−1
t=0ηt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
VT,
where Edenotes the total expectation, defined by E:=Eξ0Eξ1···Eξt.
Proof:Since∥θ1−θ2∥2=∥θ1∥2−2⟨θ1,θ2⟩+∥θ2∥2holds for allθ1,θ2∈Rd, we have that, for all t∈N,
∥θt+1−θ⋆∥2=∥(θt−θ⋆)−ηt∇fBt(θt)∥2
=∥θt−θ⋆∥2−2ηt⟨θt−θ⋆,∇fBt(θt)⟩+η2
t∥∇fBt(θt)∥2.
Taking expectation conditioned on ξt−1=ˆξt−1on both sides of the above equation, Proposition A.1 and
(19) ensure that, for all t∈N,
Eξt/bracketleftig
∥θt+1−θ⋆∥2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
=∥θt−θ⋆∥2−2ηtEξt/bracketleftig
⟨θt−θ⋆,∇fBt(θt)⟩/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
+η2
tEξt/bracketleftig
∥∇fBt(θt)∥2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
≤∥θt−θ⋆∥2−2ηt⟨θt−θ⋆,∇f(θt)⟩+η2
t/parenleftbiggσ2
bt+∥∇f(θt)∥2/parenrightbigg
.
Since convexity of fimplies that, for all θ1,θ2∈Rd,f(θ1)≥f(θ2) +⟨θ1−θ2,∇f(θ2)⟩, we have that
Eξt/bracketleftig
∥θt+1−θ⋆∥2/vextendsingle/vextendsingle/vextendsingleˆξt−1/bracketrightig
≤∥θt−θ⋆∥2−2ηt(f(θt)−f⋆) +η2
t/parenleftbiggσ2
bt+∥∇f(θt)∥2/parenrightbigg
.
Taking the total expectation on both sides of the above inequality gives that, for all t∈N,
E/bracketleftbig
∥θt+1−θ⋆∥2/bracketrightbig
≤E/bracketleftbig
∥θt−θ⋆∥2/bracketrightbig
−2ηtE[f(θt)−f⋆] +η2
t/parenleftbiggσ2
bt+E/bracketleftbig
∥∇f(θt)∥2/bracketrightbig/parenrightbigg
.
LetT∈N. Summing the above inequality from t= 0tot=T−1gives that
2T−1/summationdisplay
t=0ηtE[f(θt)−f⋆]≤∥θ0−θ⋆∥2+σ2T−1/summationdisplay
t=0η2
t
bt+ηmaxT−1/summationdisplay
t=0ηtE/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
,
whereηt≤ηmaxis used. Since Lemma 2.1 (see (20)) guarantees that
T−1/summationdisplay
t=0ηtE/bracketleftbig
∥∇f(θt)∥2/bracketrightbig
≤2(f(θ0)−f⋆)
2−¯Lηmax+¯Lσ2
2−¯LηmaxT−1/summationdisplay
t=0η2
t
bt,
we have that
T−1/summationdisplay
t=0ηtE[f(θt)−f⋆]≤∥θ0−θ⋆∥2
2+ηmax(f(θ0)−f⋆)
2−¯Lηmax+σ2
2/parenleftbigg
1 +¯Lηmax
2−¯Lηmax/parenrightbiggT−1/summationdisplay
t=0η2
t
bt.
Therefore, from/summationtextT−1
t=0ηt̸= 0, we have
min
t∈[0:T−1]E[f(θt)−f⋆]
≤/parenleftbigg∥θ0−θ⋆∥2
2+ηmax(f(θ0)−f⋆)
2−¯Lηmax/parenrightbigg1/summationtextT−1
t=0ηt+σ2
2/parenleftbigg
1 +¯Lηmax
2−¯Lηmax/parenrightbigg/summationtextT−1
t=0η2
tb−1
t/summationtextT−1
t=0ηt,
which completes the proof. 2
The right column in Table 2 can be obtained by using Lemma A.1 and Theorems 3.1–3.4.
29Under review as submission to TMLR
A.5 Training ResNet-34 on ImageNet: Evaluating Case (iv)
0 50 100 150 200 250 300
Epochs0.00.20.40.60.81.0Learning RateLearning Rate and Batch Size Schedular
2526272829
Batch SizeBatch Size (=2.0)
warmup constant (max=1.0)
warmup cosine (max=1.0)
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
warmup constant (max=1.0)
warmup cosine (max=1.0)
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs100
6×101
2×1003×1004×100Empirical Loss Value for TrainingResNet-18 on CIFAR100
warmup constant (max=1.0)
warmup cosine (max=1.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs3040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 30069.469.669.870.0
warmup constant (max=1.0)
warmup cosine (max=1.0)
 (d) Test accuracy score versus epochs
Figure 7: (a) Warm-up decaying learning rates ( ηmax= 1.0) and increasing batch sizes based on δ= 2, (b)
full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to
train ResNet-34 on ImageNet dataset.
A.6 Training ResNet-18 on CIFAR10 and CIFAR100 using Doubling, Tripling, and Quadrupling Batch
Sizes
0 50 100 150 200 250 300
Epochs0.100.150.200.250.300.35Learning RateLearning Rate and Batch Size Schedular
272829210211
Batch SizeBS: constant
BS: =2.0
BS: =3.0
BS: =4.0
LR: constant
LR: =1.40
LR: =1.70
LR: =1.90
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs101
100Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR10
constant
=2.0,=1.40
=3.0,=1.70
=4.0,=1.90
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR10
constant
=2.0,=1.40
=3.0,=1.70
=4.0,=1.90
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs5060708090Accuracy Score for T estResNet-18 on CIFAR10
280 285 290 295 300929394
constant
=2.0,=1.40
=3.0,=1.70
=4.0,=1.90
 (d) Test accuracy score versus epochs
Figure 8: (a) Increasing learning rates and doubling, tripling, and quadrupling batch sizes ( (δ,γ) =
(2,1.4),(3,1.7),(4,1.9)satisfying√
δ > γ) every 100 epochs, (b) full gradient norm of empirical loss, (c)
empirical loss value, and (d) accuracy score in testing for SGD to train ResNet-18 on CIFAR10 dataset.
30Under review as submission to TMLR
0 50 100 150 200 250 300
Epochs0.100.150.200.250.300.35Learning RateLearning Rate and Batch Size Schedular
272829210211
Batch SizeBS: constant
BS: =2.0
BS: =3.0
BS: =4.0
LR: constant
LR: =1.40
LR: =1.70
LR: =1.90
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
100Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
constant
=2.0,=1.40
=3.0,=1.70
=4.0,=1.90
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR100
constant
=2.0,=1.40
=3.0,=1.70
=4.0,=1.90
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs203040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 3007071727374
constant
=2.0,=1.40
=3.0,=1.70
=4.0,=1.90
 (d) Test accuracy score versus epochs
Figure 9: (a) Increasing learning rates and doubling, tripling, and quadrupling batch sizes ( (δ,γ) =
(2,1.4),(3,1.7),(4,1.9)satisfying√
δ > γ) every 100 epochs, (b) full gradient norm of empirical loss, (c)
empirical loss value, and (d) accuracy score in testing for SGD to train ResNet-18 on CIFAR100 dataset.
A.7 Comparisons of Case (ii) with Cases (iii) and (iv) for Training ResNet-18 on CIFAR100 using
Increasing Batch Size based on δ= 3
0 50 100 150 200 250 300
Epochs0.00.20.40.60.81.01.2Learning RateLearning Rate and Batch Size Schedular
242628210212
Batch SizeBatch Size (=3.0)
constant
increasing (=1.70)
warmup constant (=1.70)
warmup cosine (=1.70)
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
100Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
constant
increasing
warmup constant
warmup cosine (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR100
constant
increasing
warmup constant
warmup cosine
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs203040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 30074.074.575.0
constant
increasing
warmup constant
warmup cosine (d) Test accuracy score versus epochs
Figure 10: (a) Increasing or warm-up decaying learning rates ( ηmin= 0.01) and increasing batch sizes based
onδ= 3, (b) full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing
for SGD to train ResNet-18 on CIFAR100 dataset.
Figures 2–4 compare Case (ii) with Cases (iii) and (iv) for training ResNet-18 on CIFAR100 using increasing
batch size based on δ= 2.
31Under review as submission to TMLR
A.8 Comparisons of With vs. Without Replacement for Training ResNet-18 on CIFAR100
0 50 100 150 200 250 300
Epochs0.00.20.40.60.81.0Learning RateLearning Rate and Batch Size Schedular
242628210212
Batch SizeBatch Size (=2.0)
constant
increasing (max=1.0)
warmup constant (max=1.0)
warmup cosine (max=1.0)
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
Full Gradient Norm of Empirical Loss for TrainingResNet-18 on CIFAR100
constant
increasing
warmup constant
warmup cosine
constant (with replacement)
increasing (with replacement)
warmup constant (with replacement)
warmup cosine (with replacement) (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on CIFAR100
constant
increasing
warmup constant
warmup cosine
constant (with replacement)
increasing (with replacement)
warmup constant (with replacement)
warmup cosine (with replacement)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs10203040506070Accuracy Score for T estResNet-18 on CIFAR100
280 285 290 295 300717273constant
increasing
warmup constant
warmup cosine
constant (with replacement)
increasing (with replacement)
warmup constant (with replacement)
warmup cosine (with replacement) (d) Test accuracy score versus epochs
Figure 11: (a) Increasing or warm-up decaying learning rates ( ηmax= 1.0) and increasing batch sizes based
onδ= 2, (b) full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing
for SGD to train ResNet-18 on CIFAR100 dataset.
A.9 Training Wide-ResNet-28-10 on CIFAR100
0 50 100 150 200 250 300
Epochs0.000.020.040.060.080.10Learning RateLearning Rate and Batch Size Schedular
262728
Batch Sizeconstant
diminishing
cosine
linear
polynomial (p=2.0)
Batch Size
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
100Full Gradient Norm of Empirical Loss for TrainingWide-ResNet-28-10 on CIFAR100
constant
diminishing
cosine
linear
polynomial (p=2.0) (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingWide-ResNet-28-10 on CIFAR100
constant
diminishing
cosine
linear
polynomial (p=2.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs10203040506070Accuracy Score for T estWide-ResNet-28-10 on CIFAR100
280 285 290 295 3007172737475
constant
diminishing
cosine
linear
polynomial (p=2.0) (d) Test accuracy score versus epochs
Figure 12: (a) Decaying learning rates (constant, diminishing, cosine, linear, and polynomial) and constant
batch size, (b) full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing
for SGD to train Wide-ResNet-28-10 on CIFAR100 dataset.
32Under review as submission to TMLR
0 50 100 150 200 250 300
Epochs0.000.020.040.060.080.10Learning RateLearning Rate and Batch Size Schedular
23242526272829210
Batch Sizeconstant
diminishing
cosine
linear
polynomial (p=2.0)
Batch Size
(a) Learning rate ηtand batch size btversus epochs
0 50 100 150 200 250 300
Epochs102
101
Full Gradient Norm of Empirical Loss for TrainingWide-ResNet-28-10 on CIFAR100
constant
diminishing
cosine
linear
polynomial (p=2.0) (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingWide-ResNet-28-10 on CIFAR100
constant
diminishing
cosine
linear
polynomial (p=2.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs01020304050607080Accuracy Score for T estWide-ResNet-28-10 on CIFAR100
280 285 290 295 30076.076.577.077.5
constant
diminishing
cosine
linear
polynomial (p=2.0) (d) Test accuracy score versus epochs
Figure 13: (a) Decaying learning rates and increasing batch size every 30 epochs, (b) full gradient norm of
empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train Wide-ResNet-28-10
on CIFAR100 dataset.
0 50 100 150 200 250 300
Epochs0.20.40.60.81.0Learning RateLearning Rate and Batch Size Schedular
23242526272829210
Batch Sizeconstant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
Batch Size (=2.0)
(a) Learning rate ηtand batch size btversus epochs
0 50 100 150 200 250 300
Epochs103
102
101
Full Gradient Norm of Empirical Loss for TrainingWide-ResNet-28-10 on CIFAR100
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingWide-ResNet-28-10 on CIFAR100
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs01020304050607080Accuracy Score for T estWide-ResNet-28-10 on CIFAR100
280 285 290 295 30076.076.577.077.5
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
 (d) Test accuracy score versus epochs
Figure 14: (a) Increasing learning rates ( ηmax= 0.2,0.5,1.0) and increasing batch size every 30 epochs, (b)
full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to
train Wide-ResNet-28-10 on CIFAR100 dataset.
33Under review as submission to TMLR
0 50 100 150 200 250 300
Epochs0.00.20.40.60.81.0Learning RateLearning Rate and Batch Size Schedular
23242526272829210
Batch Sizeconstant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
Batch Size (=2.0)
(a) Learning rate ηtand batch size btversus epochs
0 50 100 150 200 250 300
Epochs103
102
101
Full Gradient Norm of Empirical Loss for TrainingWide-ResNet-28-10 on CIFAR100
constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingWide-ResNet-28-10 on CIFAR100
constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs01020304050607080Accuracy Score for T estWide-ResNet-28-10 on CIFAR100
280 285 290 295 30076.577.077.578.0
constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
 (d) Test accuracy score versus epochs
Figure 15: (a) Warm-up learning rates and increasing batch size every 30 epochs, (b) full gradient norm of
empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train Wide-ResNet-28-10
on CIFAR100 dataset.
0 50 100 150 200 250 300
Epochs0.100.120.140.160.180.20Learning RateLearning Rate and Batch Size Schedular
23242526272829210
Batch SizeBS: =2.0
BS: =3.0
BS: =4.0
LR: =1.08
LR: =1.50
LR: =1.90
(a) Learning rate ηtand batch size btversus epochs
0 50 100 150 200 250 300
Epochs103
102
101
Full Gradient Norm of Empirical Loss for TrainingWide-ResNet-28-10 on CIFAR100
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingWide-ResNet-28-10 on CIFAR100
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs304050607080Accuracy Score for T estWide-ResNet-28-10 on CIFAR100
280 285 290 295 30076.2576.5076.7577.0077.25
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
 (d) Test accuracy score versus epochs
Figure 16: (a) Increasing learning rates and increasing batch sizes based on δ= 2,3,4, (b) full gradient norm
of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train Wide-ResNet-
28-10 on CIFAR100 dataset.
34Under review as submission to TMLR
A.10 Training ResNet-18 on Tiny ImageNet
0 50 100 150 200 250 300
Epochs0.000.020.040.060.080.10Learning RateLearning Rate and Batch Size Schedular
262728
Batch Sizeconstant
diminishing
cosine
linear
polynomial (p=2.0)
Batch Size
(a) Learning rate ηtand batch size bversus epochs
0 50 100 150 200 250 300
Epochs102
101
100Full Gradient Norm of Empirical Loss for TrainingResNet-18 on Tiny ImageNet
constant
diminishing
cosine
linear
polynomial (p=2.0) (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs102
101
100Empirical Loss Value for TrainingResNet-18 on Tiny ImageNet
constant
diminishing
cosine
linear
polynomial (p=2.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs102030405060Accuracy Score for T estResNet-18 on Tiny ImageNet
280 285 290 295 300565860
constant
diminishing
cosine
linear
polynomial (p=2.0) (d) Test accuracy score versus epochs
Figure 17: (a) Decaying learning rates (constant, diminishing, cosine, linear, and polynomial) and constant
batch size, (b) full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing
for SGD to train ResNet-18 on Tiny ImageNet dataset.
0 50 100 150 200 250 300
Epochs0.000.020.040.060.080.10Learning RateLearning Rate and Batch Size Schedular
23242526272829210
Batch Sizeconstant
diminishing
cosine
linear
polynomial (p=2.0)
Batch Size
(a) Learning rate ηtand batch size btversus epochs
0 50 100 150 200 250 300
Epochs102
101
100Full Gradient Norm of Empirical Loss for TrainingResNet-18 on Tiny ImageNet
constant
diminishing
cosine
linear
polynomial (p=2.0) (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on Tiny ImageNet
constant
diminishing
cosine
linear
polynomial (p=2.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs0102030405060Accuracy Score for T estResNet-18 on Tiny ImageNet
280 285 290 295 30059.059.560.060.5
constant
diminishing
cosine
linear
polynomial (p=2.0) (d) Test accuracy score versus epochs
Figure 18: (a) Decaying learning rates and increasing batch size every 30 epochs, (b) full gradient norm of
empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train ResNet-18 on Tiny
ImageNet dataset.
35Under review as submission to TMLR
0 50 100 150 200 250 300
Epochs0.20.40.60.81.0Learning RateLearning Rate and Batch Size Schedular
23242526272829210
Batch Sizeconstant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
Batch Size (=2.0)
(a) Learning rate ηtand batch size btversus epochs
0 50 100 150 200 250 300
Epochs102
101
Full Gradient Norm of Empirical Loss for TrainingResNet-18 on Tiny ImageNet
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on Tiny ImageNet
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs0102030405060Accuracy Score for T estResNet-18 on Tiny ImageNet
280 285 290 295 30059.059.560.060.5
constant
max=0.2(1.080)
max=0.5(1.196)
max=1.0(1.292)
 (d) Test accuracy score versus epochs
Figure 19: (a) Increasing learning rates ( ηmax= 0.2,0.5,1.0) and increasing batch size every 30 epochs, (b)
full gradient norm of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to
train ResNet-18 on Tiny ImageNet dataset.
0 50 100 150 200 250 300
Epochs0.00.20.40.60.81.0Learning RateLearning Rate and Batch Size Schedular
23242526272829210
Batch Sizeconstant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
Batch Size (=2.0)
(a) Learning rate ηtand batch size btversus epochs
0 50 100 150 200 250 300
Epochs103
102
101
Full Gradient Norm of Empirical Loss for TrainingResNet-18 on Tiny ImageNet
constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on Tiny ImageNet
constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs0102030405060Accuracy Score for T estResNet-18 on Tiny ImageNet
280 285 290 295 30059.059.560.060.5
constant
warmup constant (max=0.2)
warmup constant (max=0.5)
warmup constant (max=1.0)
warmup cosine (max=0.2)
warmup cosine (max=0.5)
warmup cosine (max=1.0)
 (d) Test accuracy score versus epochs
Figure 20: (a) Warm-up learning rates and increasing batch size every 30 epochs, (b) full gradient norm of
empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train ResNet-18 on Tiny
ImageNet dataset.
36Under review as submission to TMLR
0 50 100 150 200 250 300
Epochs0.100.120.140.160.180.20Learning RateLearning Rate and Batch Size Schedular
23242526272829210
Batch SizeBS: =2.0
BS: =3.0
BS: =4.0
LR: =1.08
LR: =1.50
LR: =1.90
(a) Learning rate ηtand batch size btversus epochs
0 50 100 150 200 250 300
Epochs102
101
Full Gradient Norm of Empirical Loss for TrainingResNet-18 on Tiny ImageNet
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
 (b) Full gradient norm ∥∇f(θe)∥versus epochs
0 50 100 150 200 250 300
Epochs103
102
101
100Empirical Loss Value for TrainingResNet-18 on Tiny ImageNet
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
(c) Empirical loss f(θe)versus epochs
0 50 100 150 200 250 300
Epochs30354045505560Accuracy Score for T estResNet-18 on Tiny ImageNet
280 285 290 295 30059.059.560.060.5
=2.0,=1.08
=3.0,=1.50
=4.0,=1.90
 (d) Test accuracy score versus epochs
Figure 21: (a) Increasing learning rates and increasing batch sizes based on δ= 2,3,4, (b) full gradient norm
of empirical loss, (c) empirical loss value, and (d) accuracy score in testing for SGD to train ResNet-18 on
Tiny ImageNet dataset.
37