Published in Transactions on Machine Learning Research (10/2024)
Let There be Direction in Hypergraph Neural Networks
Stefano Fiorini stefano.fiorini@iit.it
Pattern Analysis & Computer Vision (PAVIS)
Italian Institute of Technology (IIT)
Stefano Coniglio stefano.coniglio@unibg.it
Department of Economics
University of Bergamo
Michele Ciavotta michele.ciavotta@unimib.it
Department of Informatics, Systems and Communication
University of Milano-Bicocca
Alessio Del Bue alessio.delbue@iit.it
Pattern Analysis & Computer Vision (PAVIS)
Italian Institute of Technology (IIT)
Reviewed on OpenReview: https: // openreview. net/ forum? id= h48Ri6pmvi
Abstract
Hypergraphs are a powerful abstraction for modeling high-order interactions between a set
of entities of interest and have been attracting a growing interest in the graph-learning
literature. In particular, directed hypegraphs are crucial in their capability of representing
real-world phenomena involving group relations where two sets of elements affect one another
in an asymmetric way. Despite such a vast potential, an established, principled solution
to tackle graph-learning tasks on directed hypergraphs is still lacking. For this reason, in
this paper we introduce the Generalized Directed Hypergraph Neural Network (GeDi-HNN),
the first spectral-based Hypergraph Neural Network (HNN) capable of seamlessly handling
hypergraphs featuring both directed and undirected hyperedges. GeDi-HNN relies on a
graph-convolution operator which is built on top of a novel complex-valued Hermitian matrix
which we introduce in this paper: the Generalized Directed Laplacian ⃗LN. We prove that
⃗LNgeneralizes many previously-proposed Laplacian matrices to directed hypergraphs while
enjoying several desirable spectral properties. Extensive computational experiments against
state-of-the-art methods on real-world and synthetically-generated datasets demonstrate the
efficacy of our proposed HNN. Thanks to effectively leveraging the directional information
contained in these datasets, GeDi-HNN achieves a relative-percentage-difference improvement
of 7% on average (with a maximum improvement of 23.19%) on the real-world datasets and
of 65.3% on average on the synthetic ones.
1 Introduction
In recent years, ground-breaking research in the graph-learning literature has been prompted by seminal
works on Graph Neural Networks (GNNs) such as (Scarselli et al., 2009; Micheli, 2009; Li et al., 2016; Kipf
and Welling, 2017; Veličković et al., 2018). Since representing a set of complex relationships solely through
undirected or directed graphs can prove too restrictive in many real-world scenarios, generalizations to
graphs allowing for higher-order (group) relationships, i.e., hypergraphs, have been considered. Hypergraphs
generalize the notion of a graph to the case where an edge (a hyperedge) can connect an arbitrary number
of nodes, thus allowing to capture not just pairwise (dyadic) relationships but also group-wise (polyadic)
dynamics (Schaub et al., 2021). This has led to a new stream of research devoted to the investigation of
1Published in Transactions on Machine Learning Research (10/2024)
Hypergraph Neural Networks (HNNs) (Feng et al., 2019; Chien et al., 2021; Huang and Yang, 2021; Wang
et al., 2023a;b).
While most of the literature on HNNs has focused on undirected hypergraphs, many real-world phenomena
such as chemical reactions are naturally modeled on hypergraphs whose hyperedges have a notion of direction.
Despite this, the directed case has been addressed only sporadically, and often only in application-specific
scenarios such as traffic forecasting (Luo et al., 2022) and music recommendation (La Gatta et al., 2022). To
the best of our knowledge, a general solution based on a convolution operator which is solidly grounded in
spectral graph theory while not being problem-dependent is missing. We aim at bridging such a gap.
In this paper, we introduce the Generalized-Directed Hypergraph Neural Network (GeDi-HNN), the first
spectral-based HNN capable of seamlessly handling hypergraphs featuring both directed and undirected
hyperedges. GeDi-HNN relies on a graph-convolution operator which is built on top of a novel Hermitian
Laplacian matrix which we introduce in this paper: the Generalized Directed Laplacian ⃗LN.⃗LNgeneralizes
various Laplacian matrices: the one proposed in Zhou et al. (2006) and used in Feng et al. (2019) for
undirected hypergraphs, the Sign-Magnetic Laplacian proposed for directed graphs in (Fiorini et al., 2023),
and the classical Laplacian matrix (Chung and Graham, 1997) used for undirected graphs.
Main Contributions of the Work
•We extend the literature on spectral-based HNNs by introducing GeDi-HNN, the first spectral HNN
capable of handling hypergraphs with both directed and undirected hyperedges.
•We introduce the Generalized Directed Laplacian matrix⃗LN; we prove that it enjoys several desirable
properties, among which admitting an eigenvalue decomposition, and that it generalizes many existing
Laplacian matrices.
•Compared to state-of-the-art methods, GeDi-HNN achieves a relative-percentage-difference improve-
ment of 7% on average (with a maximum improvement of 23.19%) on the real-world datasets and of
65.3% on average on the synthetic ones. This demonstrates its efficacy in extracting and utilizing the
information encoded in the hyperedge directions.
The paper is organized as follows. Preliminaries and previous works are summarized in Section 2. ⃗LNis
introduced in Section 3, along with its properties. Section 4 provides an overview of GeDi-HNN’s architecture,
which is built upon ⃗LN. Experimental results are reported in Section 5. Conclusions are drawn in Section 6.
The proofs of our theorems and additional details are provided in the Appendix.
2 Preliminaries and Previous Work
Undirected and Directed Hypergraphs
A hypergraph is an ordered pair H= (V,E), withn:=|V|andm:=|E|, whereVis the set of vertices
(or nodes) and E⊆2V\{}is the (nonempty) set of hyperedges. The hyperedge weights are stored in the
diagonal matrix W∈Rm×m. The vertex and hyperedge degrees are defined as du=P
e∈E:u∈e|we|foru∈V
andδe=|e|fore∈Eand are stored in two diagonal matrices Dv∈Rn×nandDe∈Rm×m.Hypergraphs
whereδ(e) =kfor somek∈Nfor alle∈Eare calledk-uniform. Graphs are 2-uniform hypergraphs.
Following Gallo et al. (1993), we define a directed hypergraph as a hypergraph where each edge e∈Eis
partitioned in a head setH(e)and atail setT(e). IfT(e)is empty,eis an undirected edge.
Graph Fourier Transform and Graph Convolutions
LetLbe a generic Laplacian matrix of a given 2-uniform hypergraph Hwhich embeds its topology. We
assume thatLadmits an eigenvalue decomposition L=UΛU∗, withU∈Cn×n.U∗is the conjugate transpose
ofU, and Λ∈Rn×nis a diagonal matrix. Let x∈Cnbe agraph signal , i.e., a function x:V→Cwhose
domain coincides with the vertices of H. Following Shuman et al. (2013), we call ˆx=F(x) =U∗xthegraph
2Published in Transactions on Machine Learning Research (10/2024)
Fourier transform ofxandF−1(ˆx) =Uˆxits inverse transform. The convolution y⊛xbetweenxand another
graph signal y∈Cn(taking the role of a filter) has a natural construction in the frequency space, where it is
defined asy⊛x=Udiag(U∗y)U∗x. Letting ˆY:=UˆGU∗with ˆG:=diag(U∗y), we can write y⊛xin the
vertex space as the linear operator ˆYx.
In the context of a GNN, there are two drawbacks to learning yexplicitly as a non-parametric filter :
i)deriving the eigenvalue decomposition of Lcould be computationally too intensive (Kipf and Welling,
2017);ii)learningyexplicitly would require learning a number of parameters proportional to the input size,
which could be inefficient for tasks of high dimension (Defferrard et al., 2016).
For these reasons, it is customary in the GNN literature, see Kipf and Welling (2017) and Defferrard et al.
(2016), to work with filters whose graph Fourier transform is a degree- Kpolynomial function of Λwith a
smallK. This leads to a so-called localized filter thanks to which the output (i.e., filtered) signal at a vertex
u∈Vis a linear combination of the input signals within a K-hop neighborhood of u(Shuman et al., 2013).
Using either Chebyshev polynomials as done by Hammond et al. (2011) and Kipf and Welling (2017) or
power monomials as done by Singh and Chen (2022), with K= 1(as typical in the literature) one obtains a
parametric family of linear operators with two (learnable) parameters θ0andθ1:1
ˆY:=θ0I+θ1L. (1)
Discrete Laplacians for Undirected Hypergraphs
In a hypergraphH= (V,E), the relationship between vertices and hyperedges is classically represented via
an incidence matrix Bof size|V|×|E|. WhenHis undirected, Bis defined as:
Bve=(
1ifv∈e
0otherwisev∈V,e∈E. (2)
FromB, one can derive Q, theSignless Laplacian Matrix Chung and Graham (1997), as well as its normalized
counterpart QN:
Q:=BWB⊤QN:=D−1
2vBWD−1
eB⊤D−1
2v. (3)
When restricting to undirected graphs (i.e., 2-uniform undirected hypergraphs), an alternative Laplacian
matrix, the so-called Signed Laplacian Matrix , can be obtained with a similar construction to equation 3.
This involves applying an arbitrary orientation to the edges of the graph (i.e., arbitrarily multiplying by
−1exactly one entry per column of B). Calling such a matrix B′, theSigned Laplacian matrix Land its
normalized counterpart LNare defined as follows:
L:=B′WB′⊤LN:=D−1
2vB′WD−1
eB′⊤D−1
2v. (4)
By utilizing the standard definitions of weighted adjacency matrix A∈Rn×nwhereAuv=weife={u,v}∈E
andAuv= 0otherwise, for undirected graphs we have:
Q=Dv+A L =Dv−A Q N=I−LNLN=I−QN. (5)
While the definition of Lin equation 4 does not extend nicely to general (not 2-uniform) hypergraphs,
the definition of LNin equation 5 does.2A generalization of the Signed Laplacian to general undirected
hypergraphs which follows LN=I−QNfrom equation 5 is proposed by Zhou et al. (2006), and reads:3
∆ =I−QN. (6)
1Following w.l.o.g. Singh and Chen (2022), we employ the approximation ˆG=PK
k=0θkΛk, from which we deduce ˆYx=
UˆGU∗x=U(PK
k=0θkΛk)U∗x=PK
k=0θk(UΛkU∗)x=PK
k=0θkLkx.
2For instance, the choice of which entries of Bshould be multiplied by −1would drastically affect L(rendering the orientation
not arbitrary anymore) and Lmay feature both positive and negative off-diagonal entries, thereby violating LN=I−QN
(notice that QN≥0holds by construction).
3In Zhou et al. (2006), QNis called Θ.
3Published in Transactions on Machine Learning Research (10/2024)
Notably, all the Laplacian matrices we introduced satisfy some key properties: i)they are real and symmetric—
and thus diagonalizable with real-valued eigenvalues; ii)they are positive semi-definite; and iii)their
normalized versions possess a bounded spectrum.
Discrete Laplacian Matrices for Directed 2-Uniform Hypergraphs
In directed 2-uniform hypergraphs, the presence of edge directions renders the graph asymmetric and none
of the previous definitions of the graph Laplacian apply. Indeed, those in equation 3, equation 4, and
equation 6 would symmetrize the graph and destroy its directions, while the one in equation 5 would lead
to an asymmetric matrix which does not admit an eigenvalue decomposition and, thus, would prevent the
application of the graph Fourier transform.
TheMagnetic Laplacian L(q), proposed by Lieb and Loss (1993) in the context of electromagnetic fields
and adopted within a spectral GNN by Zhang et al. (2021b;a), is a complex-valued, Hermitian Laplacian
matrix. It encodes the directional information of the graph while enjoying an eigenvalue decomposition with a
nonnegative, real spectrum. This Laplacian matrix generalizes the Laplacian Ldefined in equation 5. Letting
As:=1
2 
A+A⊤
be the symmetrized version of Aand letting Ds:=diag(Ase), where eis the all-one
vector, the Magnetic Laplacian and its normalized version are defined as follows:
L(q):=Ds−H(q)andL(q)
N:=I−D−1
2sH(q)D−1
2s,withH(q):=As⊙exp 
i2πq 
A−A⊤
,
where i is the imaginary unit and q∈[0,1].
TheSign-Magnetic Laplacian Lσis a matrix proposed by Fiorini et al. (2023) which is well-defined also for
graphs with negative edge weights and enjoys some extra desirable properties. If q=1
4,LσandL(q)coincide
if the latter is first constructed for an unweighted version of the graph and then multiplied component-wise
byAs. Thus,Lσis invariant to a positive weight scaling which could otherwise alter the sign pattern of L(q)
and, thus, the edge direction. Letting ¯Ds:=diag(|As|e)andsign:R→{− 1,0,1}be the component-wise
signumfunction,Lσand its normalized version are defined as follows:
Lσ:=¯Ds−HσandLσ
N:=I−¯D−1
2sHσ¯D−1
2s,withHσ:=As⊙
e⊤−sign(|A−A⊤|) +isign 
|A|−|A⊤|
.
To the best of our knowledge, no extensions of the Laplacian matrix are known for the case of directed
hypergraphs which are not 2-uniform. Our paper aims to bridge this gap.
3 The Generalized Directed Laplacian
We now introduce our proposed complex-valued Hermitian Laplacian matrix, which is capable of handling
hypergraphs featuring both directed and undirected hyperedges. We also establish some of its key properties.
We refer to this matrix as the Generalized Directed Laplacian . We define it directly in normalized form as:
⃗LN:=I−⃗QN with⃗QN:=D−1
2v⃗BWD−1
e⃗B∗D−1
2v, (7)
where⃗Bis the following complex-valued incidence matrix:
⃗Bve=

1ifv∈H(e)
−i ifv∈T(e)
0otherwisev∈V,e∈E. (8)
To appreciate how ⃗LNencodes the directions of the hypergraph, we analyze its scalar form for a pair of
verticesu,v∈V:
(⃗LN)uv=

1−X
e∈E:u∈ewe
δe1
duu=v

−X
e∈E:
u,v∈H(e)∨u,v∈T(e)we
δe−i
X
e∈E:
u∈T(e)∧v∈H(e)we
δe−X
e∈E:
u∈H(e)∧v∈T(e)we
δe

1√du1√dvu̸=v(9)
4Published in Transactions on Machine Learning Research (10/2024)
The pairu,vaffects the value of (⃗LN)uvthrough each hyperedge e∈Ewhereu,v∈e. Considering the second
line of equation 9, each hyperedge where u,vtake both the role of head ( u,v∈H(e)) or tail (u,v∈T(e))
contributes negatively to the real part, ℜ((⃗LN)uv), by the opposite of its normalized weight ( −we/δe). For
undirected hypergraphs, this is the only contribution. Such a behavior is in line with the nature of LN
(equation 4) for undirected graphs and of ∆(equation 6) for undirected hypergraphs. Hyperedges where
u,vtake opposite roles contribute with their normalized weight negatively if u∈H(e)andv∈T(e)and
positively if u∈T(e)andv∈H(e). Due to this, the imaginary part, ℑ((⃗LN)uv), is affected by the net
contribution of uandvacross all the directed hyperedges that contain them. This is in line with the net flow
behavior observed by (Fiorini et al., 2023) for Lσfor the case of directed graphs. An example illustrating the
construction of ⃗LNfor a directed hypergraph is provided in Appendix F.
The relationship between ⃗LNand the previously-proposed Laplacian matrices that we introduced in Section 2
as well as its spectral properties are analyzed in more detail in what follows.
On the Relationship between ⃗LNand other Laplacian Matrices
The following theorem shows that ⃗LNand⃗QNgeneralize the Laplacian matrices proposed by Zhou et al.
(2006) for undirected hypergraphs which are defined in equation 3 and equation 6:
Theorem 1. IfHis an undirected hypergraph, ⃗LN= ∆and⃗QN=QN.
Focusing on 2-uniform hypergraphs, we show that ⃗LNand⃗QNgeneralize the Signed and Signless Laplacian
matricesLNandQNdefined in equation 3 and equation 4, which are classically used for undirected
graphs (Chung and Graham, 1997):
Corollary 1. IfHis an undirected 2-uniform hypergraph, ⃗LN=1
2LNand⃗QN=1
2QN.
Focusing on the case of directed graphs, we establish under which conditions ⃗LNgeneralizes the Signum-
Magnetic Laplacian proposed by Fiorini et al. (2023) and the Magnetic Laplacian introduced by Lieb and
Loss (1993):
Theorem 2. IfHis a directed 2-uniform hypergraph with no antiparallel edges, we have ⃗LN=1
2Lσ
Nwith
As=A+A⊤.
Corollary 2. IfHis a directed 2-uniform unweighted hypergraph with no antiparallel edges, we have
⃗LN=1
2L(q)
Nwithq=1
4andAs=A+A⊤.
Key Spectral Properties of ⃗LN
We start by showing that ⃗LNand⃗QNadmit an eigenvalue decomposition with real eigenvalues. The result is
structural and follows after showing that both matrices are Hermitian:
Theorem 3. ⃗LNand⃗QNare diagonalizable with real eigenvalues.
Next, we show that the spectrum of ⃗QNis nonnegative. This result is obtained by showing that ⃗QNcan be
decomposed into the product of the matrix D−1
2v⃗BW1
2D−1
2eand its conjugate transpose:
Theorem 4. ⃗QNis positive semidefinite.
To show that ⃗LNis positive semidefinite, we first derive the equation of ||x||2
⃗LN, i.e., thep-Dirichlet energy
function with p= 2induced by the Generalized Directed Laplacian for a signal x∈Cn. In line with the
2-uniform case (Shuman et al., 2013), such a function provides a measure of the global smoothness of xacross
the entire hypergraph.
5Published in Transactions on Machine Learning Research (10/2024)
Theorem 5. Letx=a+ib∈Cn, witha,b∈Rn. The 2-Dirichlet energy function ||x||2
⃗LN=x∗⃗LNxofx
induced by ⃗LNis the following quadratic form:
1
2X
e∈Ew(e)
δ(e)X
u,v∈E  au√du−av√dv2
+bu√du−bv√dv2!
1u,v∈H(e)∨u,v∈T(e)
+ au√du+bv√dv2
+av√dv−bu√du2!
1u∈H(e),v∈T(e)
+ au√du−bv√dv2
+av√dv+bu√du2!
1v∈H(e),u∈T(e)!
, (10)
where 1is the indicator function.
Corollary 3. ⃗LNis positive semidefinite.
Following the methods used in (Kipf and Welling, 2017; Zhang et al., 2021b; Fiorini et al., 2023), we show
that our Laplacian is positive semidefinite and thus can be adopted as a convolutional operator.
Lastly, we combine Corollary 3 and Theorem 4 to derive upper bounds on the spectra of ⃗LNand⃗QN:
Corollary 4. λmax(⃗LN)≤1andλmax(⃗QN)≤1.
While these spectral bounds are not required for the construction of the convolution operator defined
in equation 1, they are necessary to construct localized filters using Chebyshev’s polynomials of degree
K > 1(Kipf and Welling, 2017; Defferrard et al., 2016; He et al., 2022a), and could be of independent interest.
The proofs of the theorems and corollaries of this section can be found in Appendix B.
4 Generalized Directed Hypergraph Neural Network (GeDi-HNN)
We embed the Generalized Directed Laplacian ⃗LNin GeDi-HNN, the first HNN capable of handling
hypergraphs with both undirected and directed hyperedges via a spectral-based convolution operator. For this
purpose, we rely on the localized filter of Section 2 which led to equation 1. Letting L=⃗LN, our convolution
operator is ˆYx=θ0I+θ1⃗LN.
The adoption of a localized filter with two parameters θ0,θ1plays an important role towards the generality
and the flexibility of GeDi-HNN, which we highlight next:
Proposition 1. The convolution operator obtained from equation 1 by letting L=⃗LNwith parameters θ0,θ1
coincides with the one obtained by letting L=⃗QNwith parameters θ′
0=θ0+θ1,θ′
1=−θ1.
The proposition implies that GeDi-HNN generalizes previously-proposed GNNs and HNNs irrespective of
whether they are designed around a Signed or a Signless Laplacian matrix (provided that either ⃗LNor⃗QN
generalize the matrix such networks employ). This is because, as the proposition shows, GeDi-HNN can
implement the convolution of equation 1 built on top of either Laplacians by learning suitable values for
θ0,θ1.
In our implementation, GeDi-HNN features the following extension of the convolution operator of equation 1.
LetX∈Cn×c0be ac0-dimensional graph signal (a graph signal with c0input channels), which we compactly
represent as a matrix. We combine θ0andθ1with the mixingoperator that is commonly applied to X
to linearly combine the c0channels of the graph signal. In doing so, we introduce two linear operators
Θ0,Θ1∈Cc0×cwhich can either upscale (if c>c 0) or downscale (if c<c 0) the number of channels of X. A
similar choice is made in other GNN/HNNs such as MagNet (Zhang et al., 2021b).
Lettingϕbe an activation function applied component-wise to its input matrix, the output Z∈Cn×c′of any
of GeDi-HNN’s convolutional layers is:
Z(X) =ϕ
IXΘ0+⃗LNXΘ1
. (11)
6Published in Transactions on Machine Learning Research (10/2024)
As activation function, we adopt a complex extension of the ReLUfunction, in which, for a given z∈C,
ϕ(z) =zifℜ(z)≥0andϕ(z) = 0otherwise. A similar choice is followed in other GNNs/HNNs works such
as (Fiorini et al., 2023; 2024). We project the complex-valued output of the last convolutional layer into the
reals via an unwindoperation by which Z(X)∈Cn×cis transformed into (ℜ(Z(X))||ℑ(Z(X)))∈Rn×2c,
where||is the concatenation operator.
To obtain the final result, we add ℓlinear layers to GeDi-HNN’s architecture and a residual connection for
every convolutional layer except for the first one. These connections have been proven to aid in training
deeper models by allowing them to retain information from the input of the previous layers (He et al., 2016;
Kipf and Welling, 2017). GeDi-HNN’s architecture is depicted in Figure 1.
Figure 1: GeDi-HNN’s architecture: after two complex convolutional layers and a residual connection, we
unwind the real and imaginary parts of the feature matrix and apply a fully connected layer.
Complexity of GeDi-HNN. Let us assume w.l.o.g. (as it does not affect the asymptotic analysis of
complexity) that the input and output data of each convolutional layer except for the last one have cchannels
and that the output of the last convolutional layer and the number of channels of each linear layer are equal
toc′. Withℓconvolutional layers and Slinear layers, GeDi-HNN’s complexity is O(ℓ(n2c+nc2) +nc+ (S−
1)(nc′2) +nc′d+nd), wheredis the number of hidden node classes (the last channel of the last linear layer).
Letting ¯c:=max{c,c′,d}be the largest number of channels throughout the network, we have a complexity of
O(ℓ(n2¯c) + (ℓ+S)(n¯c2)). Such a quantity is quadratic w.r.t. the number of nodes nand the largest number
of channels ¯c, which is in line with previous GNNs and HNNs architectures. For more details see Appendix C.
5 Numerical Experiments
We now illustrate the results of an extensive set of experiments carried out to evaluate the performance of
GeDi-HNN on directed hypergraphs. We compare our proposal against 11 state-of-the-art methods from
the hypergraph-learning literature: HGNN (Feng et al., 2019), HCHA4(Bai et al., 2021), HCHA with
the attention mechanism (Bai et al., 2021), HNHN (Dong et al., 2020), HyperGCN (Yadati et al., 2019),
UniGCNII (Huang and Yang, 2021), HyperDN (Tudisco et al., 2021), AllDeepSets (Chien et al., 2021),
AllSetTransformer (Chien et al., 2021), LEGCN Yang et al. (2022), ED-HNN (Wang et al., 2023a), and
PhenomNN (Wang et al., 2023b). The hyperparameters of these baselines and of our proposed model are
selected via grid search (see Appendix E).
The experiments are carried out on the node classification task of predicting the class associated with each
node, which is the same task that was consistently used throughout the papers where the 11 baselines were
proposed. The comparison is carried out on real-world datasets (Subsection 5.1) and on synthetic hypergraphs
(Subsection 5.2).
Throughout the following tables, the best results are reported in boldface and the second-best are underlined.
The datasets and code we used are publicly available on GitHub (see Appendix A).
4Among the many versions of HCHA in Dong et al. (2020), we use the one implemented in https://github.com/Graph-COM/
ED-HNN, which coincides with HGNN+(Gao et al., 2022).
7Published in Transactions on Machine Learning Research (10/2024)
5.1 Node Classification Task on Real-World Datasets
We test GeDi-HNN on 10 real-world datasets from the literature: Cora,Citeseer , and PubMed(Zhang et al.,
2022); email-Enron andemail-Eu (Benson et al., 2018); Texas,Wisconsin , and Cornell (Pei et al., 2020);
WikiCS(Mernyei and Cangea, 2020); and Telegram (Bovet and Grindrod, 2020). We test the 11 baselines
(which, we recall, are not designed to handle directed hypergraphs) on an undirected version of the 10 datasets
which is compiled following the procedure of Feng et al. (2019). Differently, we test GeDi-HNN, which is the
only method designed to handle directed hypergraphs, on a directed version of these instances, which we
compile following a slight modification to the previous procedure. For every node psharing a relationship
with nodes a,b,c,d, we create the hyperedge ewithH(e) ={p}andT(e) ={a,b,c,d}. Considering, e.g., a
citation relationship in CiteSeer where paper pcites papers a,b,c,d, in the undirected case we follow Feng
et al. (2019) and create the hyperedge {a,b,c,d}to semantically represent the paper pwhereas, in the
directed case, we set {p}as head and{a,b,c,d}as tail. We adopt the split proposed by Zhang et al. (2021b)
forTelegram ,Texas,Wisconsin , and Cornell and the split of Chien et al. (2021) for the other ones. All
the experiments are conducted using 10-fold cross-validation. More details on the datasets can be found in
Appendix D.
Table 1: Mean accuracy and standard deviation obtained on the node classification task on the real-world
datasets. The results of Cora,Citeseer , and PubMedare taken from Table 3 of (Wang et al., 2023b).
Method Cora Citeseer Pubmed email-Eu email-Enron
HGNN 79.39 ±1.36 72.45±1.16 86.44±0.44 39.80±2.77 44.32±5.44
HCHA/HGNN+79.14±1.02 72.42±1.42 86.41±0.36 41.01±3.55 44.59±6.77
HCHA w/ Attention 58.20 ±2.53 68.44±1.27 79.90±1.70 28.75±2.82 35.68±6.96
HNHN 76.36 ±1.92 72.64±1.57 86.90±0.30 29.92±1.88 30.01±12.56
HyperGCN 78.45 ±1.26 71.28±0.82 82.84±8.67 30.81±2.80 36.76±5.87
UniGCNII 78.81 ±1.05 73.05±2.21 88.25±0.40 40.81±2.76 41.62±5.28
LEGCN 74.74 ±1.25 72.74±0.86 88.12±0.74 30.16±2.28 35.41±5.76
HyperND 79.20 ±1.14 72.62±1.49 86.68±0.43 29.23±1.80 35.41±5.62
AllDeepSets 76.88 ±1.80 70.83±1.63 88.75±0.33 29.92±1.88 36.76±7.01
AllSetTransformer 78.58 ±1.47 73.08±1.20 88.72±0.37 41.58±5.13 45.41±8.43
ED-HNN 80.31 ±1.35 73.70±1.38 89.03±0.53 30.85±2.87 42.97±7.37
PhenomNN 82.29±1.42 75.10±1.59 88.07±0.48 31.09±3.83 37.03±7.21
GeDi-HNN 84.04 ±1.15 75.68±1.04 89.80±0.51 49.27±3.17 52.43±5.28
GeDi-HNN w/o directions 78.85±1.75 74.08±1.15 88.67±0.58 46.88±3.04 48.38±6.55
Method Telegram Texas Wisconsin Cornell WikiCS
HGNN 59.42±6.04 71.08±7.32 75.69±4.64 70.81±4.73 77.95±5.69
HCHA/HGNN+52.12±3.32 71.35±6.77 73.53±5.41 70.81±5.06 76.50±5.07
HCHA w/ attention 57.69 ±2.86 72.97±6.50 70.59±6.40 73.51±4.73 11.67±4.79
HNHN 50.77 ±8.27 75.41±7.26 81.18±3.72 74.32±5.14 26.47±18.1
HyperGCN 55.77 ±3.95 65.95±9.03 70.98±5.05 68.39±6.87 75.80±6.16
UniGCNII 55.58 ±5.01 84.17±5.44 86.47±5.02 76.76±5.13 83.24±1.07
LEGCN 45.19 ±5.15 79.19±4.78 84.51±5.35 73.78±6.12 78.73±1.19
HyperND 43.65 ±4.35 81.62±6.60 85.10±4.45 74.87±4.60 72.28±3.14
AllDeepSets 38.46 ±6.08 82.97±5.85 84.51±5.43 78.11±3.70 83.00±1.10
AllSetTransformer 57.12 ±5.21 80.27±5.56 81.96±6.26 76.47±5.41 83.37±3.77
ED-HNN 54.42 ±6.01 83.78±7.64 86.27±2.45 77.84±5.67 82.12±1.57
PhenomNN 54.61 ±4.72 84.59±5.41 86.28±4.62 76.49±5.56 80.07±0.61
GeDi-HNN 75.01 ±4.96 84.59±4.78 88.43±3.31 80.54±2.79 82.23±1.47
GeDi-HNN w/o directions 64.80±6.60 83.51±4.51 86.66±4.96 77.83±4.65 82.52±1.19
The accuracy obtained across the different methods and datasets is reported in Table 1. The results show
that, across the whole dataset, GeDi-HNN achieves an average additive performance improvement over
the best-performing competitor of approximately 4.20 percentage points. In terms of Relative Percentage
Difference (RPD)5, we have an average RPD improvement of 7.06%. The most significant improvement is
observed on Telegram , where GeDi-HNN achieves an average RPD improvement of approximately 23.19%
and an average additive improvement of 15.59 percentage points w.r.t. the best competitor from the literature
(HGNN). Overall, GeDi-HNN ranks first on 9 out of 10 datasets and fourth on the 10th dataset, where it
5The RPD of two values P1,P2is the percentage ratio of their difference to their average, i.e., |P1−P2|/P1+P2
2%.
8Published in Transactions on Machine Learning Research (10/2024)
achieves an accuracy of 82.23, which is only 1.14 percentage points less than the best one recorded in the
experiment.
Table 1 also presents the results of an ablation study aimed at demonstrating that a significant portion of
the superior performance of GeDi-HNN is attributable to the Generalized Directed Laplacian ⃗LNrather
than to the network’s architecture. In this study, we compare GeDi-HNN to GeDi-HNN w/o directions ,
a version which employs the undirected hypergraph Laplacian ∆proposed in Zhou et al. (2006) (which
disregards hyperedge directions) instead of ⃗LN. As shown in Table 1, GeDi-HNN outperforms GeDi-HNN
w/o directions with an RPD improvement of 4.90% (an additive difference of 3.35 percentage points) on
average across 9 out of 10 datasets and achieves nearly identical performance on the 10th dataset ( WikiCS),
where the difference between the two versions is negligible (of, additively, only 0.29 percentage points). The
largest improvement, observed on Telegram , is of a RPD of 14.61% (an additive difference of 10.21 percentage
points).
These results underscore the importance of incorporating the directionality of the hyperedges and suggest that,
thanks to the Generalized Directed Laplacian, GeDi-HNN effectively captures and utilizes this information.
5.2 Node Classification Task on Synthetic Datasets
To emphasize the importance of leveraging the hypergraph’s directions in identifying the class of each node, we
conduct a set of experiments on synthetic datasets specifically designed to exhibit a high degree of correlation
between node classes and hyperedge directions.
Figure 2: Schematic representation of
the direction of the flow induced by the
hyperedges in a synthetic dataset with
5 classes (c= 5).Drawing inspiration from the methodology proposed in Zhang et al.
(2021b), we rely on a collection of datasets which are generated as
follows. First, the set Vof vertices is partitioned into cequally-sized
classesC1,...,Ccwith uniform probability. Subsequently, for each
classC,Iiintra-class undirected hyperedges are created, each with
a cardinality uniformly sampled from {hmin,...,h max}, containing
vertices of the same class also sampled with uniform probability.
Similarly, for each pair of classes CiandCjwithi < j,Iointer-
class directed hyperedges are created. The head and tail sets are
sampled from CiandCj, respectively, with uniform probability, and
both have a cardinality uniformly sampled from {hmin,...,h max}.
Figure 2 portrays the directional flow the hyperedges induce among
the classes of a synthetic dataset with 5 classes. Notice how the
flow is directed from a class Cito a classCjonly ifi<j.
Using this methodology, we generate three distinct datasets with
parameters n= 500,c= 5,hmin= 3,hmax= 10,Ii= 30, and
an increasing number of inter-class hyperedges Io= 10,30,50. For
these datasets, we implement a 50%/25%/25% split for training,
validation, and testing, respectively. The experiments are conducted
using 10-fold cross-validation.
The experimental results, summarized in Table 2, indicate a substantial performance difference between
GeDi-HNN and the other 11 baselines, which increases the higher the value of Iois. On average, GeDi-HNN
achieves an accuracy that surpasses the best competitor from the literature with an RPD improvement of
65.31% (an additive improvement of 35.47 percentage points). The additive difference compared to the
second-best performer is of up to 39.19 percentage points.
We conclude with the ablation study where GeDi-HNN is compared to GeDi-HNN w/o directions . The
results reveal a substantial accuracy difference of 40.48 percentage points (on average) between GeDi-HNN and
GeDi-HNN w/o directions Notably; as the number of inter-class hyperedges ( Io) increases, the performance
of GeDi-HNN improves, while that of GeDi-HNN w/o directions declines. This finding underscores the
significant contribution of our proposed Generalized Directed Laplacian to the superior performance of
GeDi-HNN.
9Published in Transactions on Machine Learning Research (10/2024)
Table 2: Mean accuracy and standard deviation obtained on the node classification tasks on the synthetic
datasets.
Method Io= 10 Io= 30 Io= 50
HGNN 30.02 ±5.99 31.52±4.20 32.40±3.36
HCHA/HGNN+33.60±4.76 36.96±4.60 39.04±2.66
HCHA w/ Attention 17.28 ±2.42 18.64±2.64 20.44±2.24
HNHN 19.28 ±4.16 20.16±3.88 19.28±2.86
HyperGCN 21.04 ±3.99 21.28±3.11 17.84±3.33
UniGCNII 20.80 ±3.94 21.52±3.72 20.40±4.67
LEGCN 17.84 ±1.31 19.76±5.27 19.84±4.04
HyperND 18.16 ±3.11 18.40±3.85 18.16±4.06
AllDeepSets 18.32 ±4.12 19.20±4.33 18.72±4.40
AllSetTransformer 19.44 ±4.42 18.96±4.30 22.72±5.06
ED-HNN 19.12 ±3.32 21.12±3.56 18.96±3.33
PhenomNN 20.88 ±4.24 21.20±4.30 20.32±4.62
GeDi-HNN 65.92 ±3.98 71.84±3.31 78.24±5.64
GeDi-HNN w/o directions 36.72±5.84 29.68±6.78 28.16±10.65
6 Conclusion
We introduced GeDi-HNN, the first spectral HNN capable of handling hypergraphs with both undirected
and directed edges. GeDi-HNN is built upon a novel complex-valued Laplacian matrix, the Generalized
Directed Laplacian , which is a Hermitian matrix that employs a complex-number representation of the
hyperedge directions. This approach naturally generalizes several previously proposed Laplacians for both
graphs and hypergraphs. Our proposal enables the seamless integration of directionality in HNNs, which is
crucial for accurately modeling various real-world phenomena involving asymmetric high-order interactions.
Our proposed GeDi-HNN model utilizes this new Laplacian matrix to perform spectral convolutions on
hypergraphs featuring both undirected and directed hyperedges.
Extensive computational experiments on both real-world and synthetic datasets have demonstrated the
superior performance of GeDi-HNN in 12 out of 13 experiments compared to a comprehensive representative
group of state-of-the-art methods for the node classification task. These findings underscore the importance
of incorporating directional information within GeDi-HNN’s convolution operator. Specifically, GeDi-HNN
consistently outperforms existing models across various datasets, achieving an average relative-percentage-
difference improvement of 7% on real-world dataset (with a maximum improvement of 23.19%) and of 65.3%
on synthetic datasets. The superiority of our method is particularly evident in the experiments on synthetic
hypergraphs. These results highlight the potential of adopting GeDi-HNN to significantly enhance the
modeling of complex, directed interactions within a hypergraph to the benefit of the hypergraph-learning
task at hand.
Broader Impact Statement
All the data we used are publicly available for research purposes and do not contain personally identifiable
information or offensive content (see Appendix A for more details). The methods presented here have an
impact on society comparable to other graph neural network algorithms.
Acknowledgments
This work is supported by PNRR MUR Project Cod. PE0000013 "Future Artificial Intelligence Research
(hereafter FAIR)" - CUP J53C22003010006.
10Published in Transactions on Machine Learning Research (10/2024)
References
Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. Pattern
Recognition , 110:107637, 2021.
Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial closure
and higher-order link prediction. Proceedings of the National Academy of Sciences , 2018. ISSN 0027-8424.
doi: 10.1073/pnas.1800683115.
Alexandre Bovet and Peter Grindrod. The activity of the far right on Telegram. ResearchGate preprint , DOI:
10.13140/RG.2.2.16700.05764:1–19, 2020.
Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework
for hypergraph neural networks. In International Conference on Learning Representations , 2021.
Fan RK Chung and Fan Chung Graham. Spectral graph theory . Number 92. American Mathematical Soc.,
1997.
Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs
with fast localized spectral filtering. Advances in neural information processing systems , 29, 2016.
Yihe Dong, Will Sawin, and Yoshua Bengio. Hnhn: Hypergraph networks with hyperedge neurons. arXiv
preprint arXiv:2006.12278 , 2020.
Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In
Proceedings of the AAAI conference on artificial intelligence , pages 3558–3565, 2019.
Stefano Fiorini, Stefano Coniglio, Michele Ciavotta, and Enza Messina. Sigmanet: One laplacian to rule them
all. InProceedings of the AAAI Conference on Artificial Intelligence , pages 7568–7576, 2023.
Stefano Fiorini, Stefano Coniglio, Michele Ciavotta, and Enza Messina. Graph learning in 4d: A quaternion-
valued laplacian to enhance spectral gcns. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 38, pages 12006–12015, 2024.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. Directed hypergraphs and applications.
Discrete applied mathematics , 42(2-3):177–201, 1993.
Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. Hgnn+: General hypergraph neural networks. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 45(3):3181–3199, 2022.
David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. Wavelets on graphs via spectral graph
theory.Applied and Computational Harmonic Analysis , 30(2):129–150, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.
Mingguo He, Zhewei Wei, and Ji-Rong Wen. Convolutional neural networks on graphs with chebyshev
approximation, revisited. Advances in neural information processing systems , 35:7264–7276, 2022a.
Yixuan He, Xitong Zhang, Junjie Huang, Benedek Rozemberczki, Mihai Cucuringu, and Gesine Reinert.
PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and
Directed Graphs. arXiv preprint arXiv:2202.10793 , 2022b.
Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks. In
Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21 , 2021.
Thomas. N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th
International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings , 2017.
Bryan Klimt and Yiming Yang. Introducing the enron corpus. In CEAS, volume 45, pages 92–96, 2004.
11Published in Transactions on Machine Learning Research (10/2024)
Valerio La Gatta, Vincenzo Moscato, Mirko Pennone, Marco Postiglione, and Giancarlo Sperlí. Music
recommendation via hypergraph embedding. IEEE transactions on neural networks and learning systems ,
2022.
Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow. Gated graph sequence neural networks. In
Proceedings of ICLR’16 , April 2016.
Elliott H Lieb and Michael Loss. Fluxes, Laplacians, and Kasteleyn’s theorem. In Statistical Mechanics ,
pages 457–483. Springer, 1993.
Xiaoyi Luo, Jiaheng Peng, and Jun Liang. Directed hypergraph attention network for traffic forecasting. IET
Intelligent Transport Systems , 16(1):85–98, 2022.
Péter Mernyei and Cătălina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural networks.
arXiv preprint arXiv:2007.02901 , 2020.
Alessio Micheli. Neural network for graphs: A contextual constructive approach. IEEE Transactions on
Neural Networks , 20(3):498–511, 2009.
Ashwin Paranjape, Austin R Benson, and Jure Leskovec. Motifs in temporal networks. In Proceedings of the
tenth ACM international conference on web search and data mining , pages 601–610, 2017.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph
convolutional networks. arXiv preprint arXiv:2002.05287 , 2020.
Jörg Reichardt and Stefan Bornholdt. Statistical mechanics of community detection. Physical review E , 74
(1):016110, 2006.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph
neural network model. IEEE Transactions on Neural Networks , 20(1):61–80, 2009.
Michael T Schaub, Yu Zhu, Jean-Baptiste Seby, T Mitchell Roddenberry, and Santiago Segarra. Signal
processing on higher-order networks: Livin’on the edge... and beyond. Signal Processing , 187:108149, 2021.
David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging
field of signal processing on graphs: Extending high-dimensional data analysis to networks and other
irregular domains. IEEE signal processing magazine , 30(3):83–98, 2013.
Rahul Singh and Yongxin Chen. Signed graph neural networks: A frequency perspective. Transactions on
Machine Learning Research , 2022.
Francesco Tudisco, Austin R Benson, and Konstantin Prokopchik. Nonlinear higher-order label spreading. In
Proceedings of the Web Conference 2021 , pages 2402–2413, 2021.
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph attention networks. In International Conference on Learning Representations , 2018.
Peihao Wang, Shenghao Yang, Yunyu Liu, Zhangyang Wang, and Pan Li. Equivariant hypergraph diffusion
neural operators. In International Conference on Learning Representations (ICLR) , 2023a.
Yuxin Wang, Quan Gan, Xipeng Qiu, Xuanjing Huang, and David Wipf. From hypergraph energy functions
to hypergraph neural networks. In Proceedings of the 40th International Conference on Machine Learning ,
pages 35605–35623, 2023b.
Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar.
Hypergcn: A new method for training graph convolutional networks on hypergraphs. Advances in neural
information processing systems , 32, 2019.
Chaoqi Yang, Ruijie Wang, Shuochao Yao, and Tarek Abdelzaher. Semi-supervised hypergraph node
classification on hypergraph line expansion. 2022.
12Published in Transactions on Machine Learning Research (10/2024)
JieZhang, BoHui, Po-WeiHarn, Min-TeSun, andWei-ShinnKu. smgc: Acomplex-valuedgraphconvolutional
network via magnetic laplacian for directed graphs, 2021a.
Jiying Zhang, Fuyang Li, Xi Xiao, Tingyang Xu, Yu Rong, Junzhou Huang, and Yatao Bian. Hypergraph
convolutional networks via equivalency between hypergraphs and undirected graphs. arXiv preprint
arXiv:2203.16939 , 2022.
Xitong Zhang, Yixuan He, Nathan Brugnone, Michael Perlmutter, and Matthew Hirn. Magnet: A neural
network for directed graphs, 2021b.
Dengyong Zhou, Jiayuan Huang, and Bernhard Schölkopf. Learning with hypergraphs: Clustering, classifica-
tion, and embedding. Advances in neural information processing systems , 19, 2006.
13Published in Transactions on Machine Learning Research (10/2024)
A Code Repository and Licensing
The code written for this research work is available at https://github.com/Stefa1994/GeDi-HNN and freely
distributed under the Apache 2.0 license.6
The Texas,Wisconsin ,Cornell,WikiCS, and Telegram datasets were obtained from the PyTorch Geometric
Signed Directed (He et al., 2022b) library (distributed under the MIT license). The Cora,Citeseer , and
PubMeddatasets are available at https://linqs.org/datasets/ . The email-Eu andemail-Enron datasets
are available at https://www.cs.cornell.edu/~arb/data/ .
The code for the baselines used in the experimental analysis is available at https://github.com/Graph-COM/
ED-HNNandhttps://github.com/yxzwang/PhenomNN under the MIT license.7
B Properties of Our Proposed Laplacian ⃗LN
This section contains the proofs of the theorems, corollaries, propositions, and lemma reported in the main
paper.
Theorem 1. IfHis an undirected hypergraph, ⃗LN= ∆and⃗QN=QN.
Proof.SinceH= (V,E)is an undirected hypergraph, ⃗Bis binary and only takes values 0 and 1 (rather than
being ternary and taking values 0,1,−i, which is the case in general). In particular, for each edge e∈Ewe
have⃗Bue= 1if eitheru∈H(e)oru∈T(e)and⃗Bue= 0otherwise. Consequently, the directed incidence
matrix⃗Bis identical to the non-directed incidence matrix B, i.e.,⃗B=B. Thus, by construction, ⃗LN= ∆
and⃗QN=QN.
Corollary 1. IfHis an undirected 2-uniform hypergraph, ⃗LN=1
2LNand⃗QN=1
2QN.
Proof.SinceHis an undirected 2-uniform hypergraph, it follows that:
(⃗BW⃗B∗=Dv+A
D−1
e =1
2I
Based on this, we can rewrite ⃗QNas follows:
⃗QN=D−1
2v⃗BWD−1
e⃗B∗D−1
2v
=D−1
2v⃗B1
2W
⃗B∗D−1
2v
=1
2
D−1
2v(Dv+A)D−1
2v
=1
2
I+D−1
2vAD−1
2v
=1
2(I+AN)
=1
2QN.
This proves the second part of the result. Since ⃗QN=1
2QNand, due to equation 5,1
2LN=I−1
2QN, it
follows that1
2LN=I−⃗QN=⃗LN.
Theorem 2. IfHis a directed 2-uniform hypergraph with no antiparallel edges, we have ⃗LN=1
2Lσ
Nwith
As=A+A⊤.
6https://www.apache.org/licenses/LICENSE-2.0
7https://choosealicense.com/licenses/mit/
14Published in Transactions on Machine Learning Research (10/2024)
Proof.SinceHis a directed 2-uniform hypergraph without antiparallel edges, it follows that:
(⃗BW⃗B∗=¯Ds+Hσ
D−1
e =1
2I.
SinceHhas no digons, the assumption As=A+A⊤implies ¯Ds=Dv. Thus, we can rewrite ⃗LNas follows:
⃗LN=I−D−1
2v⃗BWD−1
e⃗B∗D−1
2v
=I−D−1
2v⃗B1
2W
⃗B∗D−1
2v
=I−1
2
D−1
2v ¯Ds+Hσ
D−1
2v
=I−1
2
I+D−1
2vHσD−1
2v
=1
2Lσ
N.
Corollary 2. IfHis a directed 2-uniform unweighted hypergraph with no antiparallel edges, we have
⃗LN=1
2L(q)
Nwithq=1
4andAs=A+A⊤.
Proof.SinceHis a directed 2-uniform unweighted hypergraph, A∈{0,1}n×n. Thus, as shown by Fiorini
et al. (2023), with q=1
4we haveLσ=L(q). Since Theorem 2 states that ⃗LN=1
2Lσ
N, it follows that
⃗LN=1
2Lσ
N=1
2L(1
4)
N.
Theorem 3. ⃗LNand⃗QNare diagonalizable with real eigenvalues.
Proof.This follows from the fact that the two matrices are, by construction, Hermitian.
Theorem 4. ⃗QNis positive semidefinite.
Proof.
x∗⃗QNx:=x∗
D−1
2v⃗BWD−1
e⃗B∗D−1
2v
x

x∗D−1
2v⃗BW1
2D−1
2e
D−1
2eW1
2⃗B∗D−1
2vx

D−1
2eW1
2⃗B∗D−1
2vx∗
D−1
2eW1
2⃗B∗D−1
2vx
||
D−1
2eW1
2⃗B∗D−1
2vx∗
||2
2≥0.
Theorem 5. Letx=a+ib∈Cn, witha,b∈Rn. The 2-Dirichlet energy function ||x||2
⃗LN=x∗⃗LNxofx
induced by ⃗LNis the following quadratic form:
1
2X
e∈Ew(e)
δ(e)X
u,v∈E  au√du−av√dv2
+bu√du−bv√dv2!
1u,v∈H(e)∨u,v∈T(e)
+ au√du+bv√dv2
+av√dv−bu√du2!
1u∈H(e),v∈T(e)
+ au√du−bv√dv2
+av√dv+bu√du2!
1v∈H(e),u∈T(e)!
, (12)
15Published in Transactions on Machine Learning Research (10/2024)
where 1is the indicator function.
Proof.
x∗⃗LNx=X
u∈Vx∗
uxu−X
u,v∈VX
e∈Ew(e)
δ(e)¯B(u,e)¯B(v,e)∗
p
d(u)p
d(v)xux∗
v
=X
u∈Vx∗
uxu−X
e∈EX
u,v∈Vw(e)
δ(e)¯B(u,e)¯B(v,e)∗
p
d(u)p
d(v)xux∗
v
=X
u∈Vx∗
uxu−X
e∈Ew(e)
δ(e)X
u,v∈V:u≤v 
¯B(u,e)¯B(v,e)∗xux∗
vp
d(u)p
d(v)+¯B(v,e)¯B(u,e)∗xvx∗
up
d(v)p
d(u)!
=X
e∈Ew(e)
δ(e)X
u,v∈E:u≤vx∗
uxu
d(u)+x∗
vxv
d(v)
−X
e∈Ew(e)
δ(e)X
u,v∈V:u≤v 
¯B(u,e)¯B(v,e)∗xux∗
vp
d(u)p
d(v)+¯B(v,e)¯B(u,e)∗xvx∗
up
d(v)p
d(u)!
=X
e∈Ew(e)
δ(e)X
u,v∈V:u≤v 
x∗
uxu
d(u)+x∗
vxv
d(v)−¯B(u,e)¯B(v,e)∗xux∗
vp
d(u)p
d(v)−¯B(v,e)¯B(u,e)∗xvx∗
up
d(v)p
d(u)!
.
Let us analyze the three possible cases for the summand.
Case 1.a:u∈H(e)∧v∈H(e)⇔¯B(u,e) = 1,¯B(v,e) = 1. We have ¯B(u,e)¯B(v,e)∗=¯B(v,e)¯B(u,e)∗= 1.
Case 1.b:u∈T(e)∧v∈T(e)⇔¯B(u,e) =−i,¯B(v,e) =−i. We have ¯B(u,e)¯B(v,e)∗=¯B(v,e)¯B(u,e)∗=
(−i)(−i)∗= (−i)(i) = 1.
In both cases, we have:
x∗
uxu
d(u)+x∗
vxv
d(v)−xux∗
vp
d(u)p
d(v)−xvx∗
up
d(v)p
d(u)= 
xup
d(u)−xvp
d(v)!∗ 
xup
d(u)−xvp
d(v)!
.
Lettingxu=au+ibuandxv=av+ibv, we have:
au√du−av√dv2
+bu√du−bv√dv2
.
Case 2.a:u∈H(e)∧v∈T(e)⇔¯B(u,e) = 1,¯B(v,e) =−i. We have ¯B(u,e)¯B(v,e)∗= (1)(−i)∗=iand
¯B(v,e)¯B(u,e)∗= (−i)(1)∗=−i.
Thus:x∗
uxu
d(u)+x∗
vxv
d(v)−ixux∗
vp
d(u)p
d(v)+ixvx∗
up
d(v)p
d(u)
Letxu=au+ibuandxv=av+ibv, then we have:
au√du+bv√dv2
+av√dv−bu√du2
.
Case 2.b:u∈T(e)∧v∈H(e)⇔¯B(u,e) =−i,¯B(v,e) = 1. We have ¯B(u,e)¯B(v,e)∗= (−i)(1)∗=−iand
¯B(v,e)¯B(u,e)∗= (1)(−i)∗=i. We have:
x∗
uxu
d(u)+x∗
vxv
d(v)+ixux∗
vp
d(u)p
d(v)−ixvx∗
up
d(v)p
d(u)
16Published in Transactions on Machine Learning Research (10/2024)
Letxu=au+ibuandxv=av+ibv, then we have:
au√du−bv√dv2
+av√dv+bu√du2
.
The final equation reported in the statement of the theorem is obtained by combining the four cases we just
analyzed.
Corollary 3. ⃗LNis positive semidefinite.
Proof.Since ¯LNis Hermitian, it can be diagonalized as UΛU∗for someU∈Cn×nandΛ∈Rn×n, where
Λis diagonal and real. We have x∗⃗LNx=x∗UΛU∗x=y∗Λywithy=U∗x. Since Λis diagonal, we have
y∗Λy=P
u∈Vλuy2
u. Thanks to Theorem 5, the quadratic form x∗⃗LNxassociated with ⃗LNis a sum of
squares and, hence, nonnegative. Combined with x∗⃗LNx=P
u∈Vλuy2
u, we deduce λu≥0for allu∈V.
Corollary 4. λmax(⃗LN)≤1andλmax(⃗QN)≤1.
Proof.λmax(⃗LN)≤1holds if and only if ⃗LN−I⪯0. Since⃗LN=I−⃗QNholds by definition, we need to
prove−⃗QN⪯0, which holds true due to Theorem 4.
Similarly,λmax(⃗QN)≤1holds if and only if ⃗QN−I⪯0. Since⃗QN=I−⃗LNholds by definition, we need to
prove−⃗LN⪯0, which holds true due to Corollary 3.
Proposition 1. The convolution operator obtained from equation 1 by letting L=⃗LNwith parameters θ0,θ1
coincides with the one obtained by letting L=⃗QNwith parameters θ′
0=θ0+θ1,θ′
1=−θ1.
Proof.Consider the two operators θ0I+θ1⃗LNandθ′
0I+θ′
1⃗QN. Since⃗LN=I−⃗QN, the first operator reads:
θ0I+θ1(I−⃗QN). This is rewritten as (θ0+θ1)I−θ1⃗QN. By operating the choice θ′
0=θ0+θ1andθ1=−θ′
1,
the second operator is obtained.
C Complexity of GeDi-HNN
The detailed calculations for the (inference) complexity of GeDi-HNN are as follows.
1.The Generalized Directed Laplacian ⃗LNis constructed following equation 7 in time O(n2m), where
the factormis due to the need for computing the product between two rows of ⃗Bto calculate each
entry of⃗LN. After⃗LNhas been computed, the convolution matrix ˆY∈Cn×nis constructed in time
O(n2). Note that such a construction is carried out entirely in pre-processing and is not required at
inference time.
2.Each of the ℓconvolutional layers of GeDi-HNN requires O(n2c+nc2+nc) =O(n2c+nc2)elementary
operations across 3 steps. Let Xl−1be the input matrix to layer l= 1,...,ℓ. The operations that
are carried out are the following ones.
(a)⃗LNis multiplied by the node-feature matrix Xl−1∈Cn×c, obtaining Pl1∈Cn×cin timeO(n2c)
(we assume matrix multiplications takes cubic time);
(b)The matrices Pl0=IXl−1=Xl−1andPl1are multiplied by the weight matrices Θ0,Θ1∈Rc×c
(respectively), obtaining the intermediate matrices Pl01,Pl11∈Cn×cin timeO(nc2).
(c) The matrices Pl01andPl11are additioned in time O(nc)to obtainPl2.
(d)The activation function ϕis applied component-wise to Pl2in timeO(nc), resulting in the
output matrix Xl∈Cn×cof thel-th convolutional layer.
3.The unwind operator transforms Xℓ(the output of the last convolutional layer ℓ) into the matrix
U0∈Rn×2cin linear time O(nc).
17Published in Transactions on Machine Learning Research (10/2024)
4.CallUs−1the input matrix to each linear layer of index s= 1,...,S. The application of the s-th
linear layer to Us−1∈Cn×c′requires multiplying Us−1by a weight matrix Ms∈Cc′×c′(wherec′
is the number of channels from which and into which the feature vector of each node is projected).
This is done in time O(nc′2).
5.In the last linear layer of index S, the input matrix US−1∈Rn×c′is projected into the output matrix
O∈Rn×din timeO(nc′d).
6. The application of the Softmax activation function takes linear time O(nd).
We deduce an overall complexity of O(ℓ(n2c+nc2) +nc+ (S−1)(nc′2) +nc′d+nd)which, letting ¯c=
max{c,c′,d}, coincides with O(ℓ(n2¯c) + (ℓ+S)(n¯c2)).
D Further Details on the Datasets
We test GeDi-HNN on ten real-world dataset. Cora,Citeseer , and PubMed(Zhang et al., 2022); email-Eu ,
andemail-Enron (Benson et al., 2018); Texas,Wisconsin , and Cornell (Pei et al., 2020); WikiCS(Mernyei
and Cangea, 2020); and Telegram (Bovet and Grindrod, 2020).
Cora,Citeseer , and PubMedare citation networks with node labels based on paper topics. In these citation
networks, the nodes represent papers, their relationships denote citations of one paper by another, and the
node features are the bag-of-words representation of papers.
Email-Enron and email-Eu are two email datasets—one from communications exchanged between Enron
employees (Klimt and Yang, 2004) and the other from a European research institution (Paranjape et al., 2017).
The nodes are email addresses and their relationships are of sender-receiver type. Since no node labeling is
present in these two datasets, we define the node labels (node classes) using the Spinglass algorithm (Reichardt
and Bornholdt, 2006).
Texas,Wisconsin , and Cornell are WebKB data sets extracted from the CMU World Wide Knowledge
Base ( Web->KB) project.8WebKB is a webpage data set collected from computer science departments of
various universities by Carnegie Mellon University. In these networks, the nodes represent web pages, and
the relationship are hyperlinks between them. The node features are the bag-of-words representation of the
web pages. The web pages are manually classified into the five categories: student, project, course, staff, and
faculty.
WikiCSis a directed network whose nodes correspond to Computer Science articles, and the relationships are
on hyperlinks. This network has 10 classes representing different branches of the field.
Telegram models an influence network built on top of interactions among distinct users who propagate
ideologies of a political nature.
The statistic of these ten real-world datasets and of the synthetic datasets we generate are summarized in
Tables 3 and 4.
E Experiment Details
Hardware. The experiments were conducted on 2 different machines:
1.An Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz with 380 GB RAM, equipped with an NVIDIA
Ampere A100 40GB.
2.A 12th Gen Intel(R) Core(TM) i9-12900KF CPU @ 3.20GHz CPU with 64 GB RAM, equipped with
an NVIDIA RTX 4090 GPU.
8http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/
18Published in Transactions on Machine Learning Research (10/2024)
Table 3: Statistics of the real-world datasets
Data set # node # hyperedges # classes average |e|
Cora 2708 1579 7 3.03
Citeseer 3312 1079 6 3.20
Pubmed 19717 7963 3 4.35
email-Eu 986 873 10 38.01
email-Enron 143 128 7 20.03
Telegram 245 185 4 48.04
Texas 183 40 5 4.45
Wisconsin 251 65 5 4.77
Cornell 183 41 5 3.88
WikiCS 11701 6827 10 42.08
Table 4: Statistics of the synthetic datasets
Data set # node # hyperedges # classes average |e|
Io= 10 500 250 5 9.05
Io= 30 500 450 5 10.79
Io= 50 500 650 5 11.63
Model Settings. We trained every learning model considered in this paper for up to 500 epochs. We
adopted a learning rate of 5·10−3and employed the optimization algorithm Adam with weight decays equal
to5·10−4(in order to avoid overfitting). For all the models that adopt the classification layer, we set it to 2.
We adopted a hyperparameter optimization procedure to identify the best set of parameters for each model.
In particular, the hyperparameter values are:
•For AllDeepSets and ED-HNN, the number of basic block is chosen in {2,4,8}, the number of MLPs
per block in{1,2}, the dimension of the hidden MLP (i.e., the number of filters) in {64,128,256,512},
and the classifier hidden dimension in {64,128,256}.
•For AllSetTransformer the number of basic block is chosen in {2,4,8}, the number of MLPs per block
in{1,2}, the dimension of the hidden MLP in {64,128,256,512}, the classifier hidden dimension in
{64,128,256}, and the number of heads in {1,4,8}.
•For UniGCNII, HGNN, HNHN, HCHA/HGNN+, LEGCN, and HCHA with the attention mechanism,
the number of basic blocks is chosen in {2,4,8}and the hidden dimension of the MLP layer in
{64,128,256,512}.
•For HyperGCN, the number of basic blocks is chosen in {2,4,8}.
•For HyperND, the classifier hidden dimension is chosen in {64,128,256}.
•For PhenomNN, the number of basic blocks is chosen in {2,4,8}. We select four different settings:
1.λ0= 0.1,λ1= 0.1and prop step = 8,
2.λ0= 0,λ1= 50and prop step = 16,
3.λ0= 1,λ1= 1and prop step = 16,
4.λ0= 0,λ1= 20and prop step = 16.
•For GeDi-HNN and GeDi-HNN w/o directionality , the number of convolutional layers is chosen
in{1,2,3}, the number of filters in {64,128,256,512}, and the classifier hidden dimension in
{64,128,256}. We tested GeDi-HNN both with the input feature matrix X∈Cn×cwhereℜ(X) =
ℑ(X)̸= 0and withℑ(X) = 0.
19Published in Transactions on Machine Learning Research (10/2024)
Node Features. ForCora,Citeseer ,PubMed,Texas,Wisconsin ,Cornell,WikiCS, and Telegram , we
retain the datasets’ original features. For email-Eu ,email-Enron , and the synthetic datasets, the feature
vectors are generated using the vertex degree of each node.
F From a Directed Hypergraph to the Generalized Directed Laplacian
To illustrate the representation of a directed hypergraph in our Generalized Directed Laplacian, consider a
directed hypergraph H= (V,E)withV={v1,v2,v3,v4,v5}andE={e1,e2}. The incidence relationships
are defined as follows: v1,v2∈H(e1),v3∈T(e1),v4,v5∈H(e2), andv1,v2∈T(e2). The hyperedges have
unit weights (i.e., W=I). The hyperedge cardinalities are δe1= 3andδe2= 4.
For this hypergraph, we construct our Generalized Directed Laplacian using the following matrices: the
incidence matrix ⃗B, its conjugate transpose ⃗B∗, the vertex degree matrix Dv, and the hyperedge degree
matrixDe.
⃗B=
1−i
1−i
−i0
0 1
0 1
⃗B∗=1 1i0 0
i i 0 1 1
Dv=
2 0 0 0 0
0 2 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
De=3 0
0 4
.
Based on these matrices, we build ⃗QNas follows:
⃗QN=
0.29 0.29i0.24−i0.18−i0.18
0.29 0.29i0.24−i0.18−i0.18
−i0.24−i0.24 0.33 0 0
i0.18i0.18 0 0 .25 0.25
i0.18i0.18 0 0 .25 0.25

and then our Generalized Directed Laplacian:
⃗LN=
0.71−0.29−i0.24i0.18i0.18
−0.29 0.71−i0.24i0.18i0.18
i0.24i0.24 0.66 0 0
−i0.18−i0.18 0 0 .75−0.25
−i0.18−i0.18 0−0.25 0.75

By inspecting ⃗LN, one can observe that it encodes the elements of the hypergraph in the following way:
1.The presence of nodes belonging to the same head or tail set, i.e., v1,v2∈H(e1),v4,v5∈H(e2),
andv1,v2∈T(e2), is encoded in the real part. Specifically, (⃗LN)v1v2= (⃗LN)v2v1=−0.29and
(⃗LN)v4v5= (⃗LN)v5v4=−0.25.
2.The directed hyperedges are encoded via the imaginary part. For example, considering nodes v1and
v3, we have (⃗LN)v1v3=−(⃗LN)v3v1=−i0.24.
3.The absence of a relationship between a pair of nodes is encoded by 0. Specifically, (⃗LN)v3v4=
(⃗LN)v4v3= 0and(⃗LN)v3v5= (⃗LN)v5v3= 0.
4.The "self-loop information" (a measure of how strongly the feature of a node depends on its current
value within the convolution operator) is encoded by the diagonal of ⃗LN.
20