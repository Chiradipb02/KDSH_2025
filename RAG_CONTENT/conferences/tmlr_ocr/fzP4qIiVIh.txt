Published in Transactions on Machine Learning Research (10/2024)
Persona-aware Generative Model for Code-mixed Language
Ayan Sengupta ayan.sengupta@ee.iitd.ac.in
Department of Electrical Engineering
Indian Institute of Technology Delhi
Md. Shad Akhtar shad.akhtar@iiitd.ac.in
Department of Computer Science & Engineering
Indraprastha Institute of Information Technology Delhi
Tanmoy Chakraborty∗tanchak@iitd.ac.in
Department of Electrical Engineering
Yardi School of Artificial Intelligence
Indian Institute of Technology Delhi
Reviewed on OpenReview: https: // openreview. net/ forum? id= fzP4qIiVIh
Abstract
Code-mixing and script-mixing are prevalent across online social networks and multilingual
societies. However, a user’s preference toward code-mixing depends on the socioeconomic
status, demographics of the user, and the local context, which existing generative models
tend to ignore while generating code-mixed texts. In this work, we make a pioneering at-
tempt to develop a persona-aware generative model to generate texts resembling real-life
code-mixed texts of individuals. We propose PARADOX, a persona-aware generative model
for code-mixed text generation, which is a novel Transformer-based encoder-decoder model
that encodes an utterance conditioned on a user’s persona and generates code-mixed texts
withoutmonolingual reference data. We propose an alignment module that re-calibrates the
generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed
texts that are semantically more meaningful and linguistically more valid. To evaluate the
personification capabilities of PARADOX, we propose four new metrics – CM BLEU, CM
Rouge-1, CM Rouge-L and CM KS. On average, PARADOX achieves 1.6%better CM BLEU,
57%better perplexity and 32%better semantic coherence than the non-persona-based coun-
terparts. The source code is available at: https://github.com/victor7246/PARADOX .
1 Introduction
Code-mixing ( akacode-switching) appears when two or more languages are used interchangeably in a single
utterance. It is common in multilingual societies like India, where more than 24%of the population speaks
in more than one language (Sengupta et al., 2021). Code-mixing is even more prevalent on social media.
Informal usage of code-mixed languages on social media platforms like Twitter, Facebook, YouTube, and
other online social networks gives rise to script-mixing , in which a user can use a single script ( e.g.,Roman)
or multiple scripts ( e.g.,Devanagari for Hindi and Roman for English) within the same text (Srivastava
et al., 2020). Recent literature has made significant efforts to understand syntactic structure and semantics
from code-mixed texts (Singh et al., 2018a;b; Sengupta et al., 2022b). Similar attempts have been made for
pragmatic tasks – humour, sarcasm and hate detection in the code-mixed regime (Sengupta et al., 2022a;
Bansal et al., 2020).
∗Corresponding Author
1Published in Transactions on Machine Learning Research (10/2024)
Text generation models need to understand the syntax and semantics of texts and preserve semantic coher-
ence during generation. Previous studies utilized recurrent neural networks with generative models (Zhang
et al., 2017), as well as self-attention-based pre-trained language models (Zhang et al., 2020) for gener-
ating monolingual texts. However, such an effort is limited in case of code-mixing. Previously, linguistic
theories (Pratapa et al., 2018; Gupta et al., 2020), transfer learning (Gupta et al., 2020), and autoencod-
ing (Samanta et al., 2019) based approaches have been used to generate code-mixed texts from parallel
corpora or reference data. However, none of these methods incorporates user information while generating
code-mixed texts. Unlike traditional languages, code-mixing is a derived language whose adoption depends
on different socioeconomic, demographic, and linguistic factors (Rudra et al., 2016; Parshad et al., 2016).
Figure 1 demonstrates the code-mixing behaviour among Indian users on Twitter and YouTube in terms of
adoption and patterns in code-mixing. We visualize the mean and standard deviation of the Code-Mixing
Index (CMI) (Das & Gambäck, 2014) and the length of tweets/comments posted by different users. The
distributions show how different users conceive and prefer code-mixing.
User persona plays a vital role in generation models, particularly in personalized generation settings, such
as conversational agents and recommendation engines. Several studies have contributed towards persona-
based dialogue generation (Zheng et al., 2019; Wang et al., 2021), personalized story generation (Chandu
et al., 2019), and other sub-tasks in text generation. Being a conversational language, personification of
code-mixing could be deemed appropriate for conversational systems such as recommender engines, mental
health counselling bots, and event booking applications.
0.00 0.25 0.50 0.75 1.00
Average CMI104
103
102
101
100Probability
0 10 20 30 40
Average text length
0.00 0.15 0.30 0.45 0.60
Standard deviation of CMI104
103
102
101
100Probability (log scale)
0 10 20 30 40
Standard deviation of text length
T witter YouTube
Figure 1: User-specific distribution of Code-Mixing In-
dex (CMI) and text lengths across different platforms.
CMI is calculated as the fraction of minority language
words in a text. For instance, the CMI of the text “I
don’t want your nautanki” (“I don’t want your gim-
mick”) is1
5= 0.2, the fraction of Hindi (minority lan-
guage in this example) words in the text. Texts skewed
toward monolingualism, i.e.,having an unequal pro-
portion of words between different languages, tend to
have lower CMI than multilingual texts.This motivates us to develop PARADOX, a novel
persona-aware generative model for code-mixed text
generation. It aims to generate personalized code-
mixed texts by leveraging users’ historical utterances.
It uses a Transformer-based encoder-decoder archi-
tecture to learn the semantics of code-mixed genera-
tion. The model utilizes a novel persona encoder to
encode a user persona from their behavioural prefer-
ences. Instead of projecting the user’s persona onto
a static space, PARADOX projects it onto a proba-
bilistic latent space and captures the contextual per-
sona based on their historical persona. Additionally,
PARADOX uses an alignment module to re-align de-
coder outputs to generate coherent texts.
We evaluate PARADOX against the vanilla Transformer
in terms of both the quality and coherence of gener-
ated texts. To quantify the extent of personification
in the code-mixed generation, we propose four met-
rics – CM BLEU, CM Rouge-1, CM Rouge-L, and
CM KS (here CM stands for code-mixing). On aver-
age, PARADOX achieves 1.6%better CM BLEU than
the non-persona counterpart. We also conduct a de-
tailed human evaluation, concluding that PARADOX-
generated code-mixed texts are 32%more semanti-
cally coherent than that of the vanilla Transformer model. PARADOX can imitate a user’s linguistic preference
4%better than the non-persona-based Transformer model. Our empirical analyses also highlight the effec-
tiveness of PARADOX over pre-trained large language models. On average, PARADOX achieves 4.2%better CM
BLEU, 11%better CM Rouge-1, and 9.6%better CM Rouge-L than the pre-trained Llama 2 (Touvron et al.,
2023) and GPT-4 (Achiam et al., 2023) models.
Contributions. The major contributions of this paper are summarized below:
•We make a pioneering effort in utilizing user persona in code-mixed text generation. Compared to existing
approaches, PARADOX does not require parallel corpora or reference data for code-mixed text generation.
2Published in Transactions on Machine Learning Research (10/2024)
•We propose a probabilistic persona encoder module that learns the latent persona of a user from historical
contexts. PARADOX captures the user persona implicitly and does not require explicit persona features such
as user demographic information to encode user behaviours.
•Wedesignanalignmentmoduletoautomaticallyinducealignmentsbetweendifferentsubwords.Empirical
results show that it improves the coherence of generated texts.
•We propose three metrics influenced by supervised machine translation – CM BLEU, CM Rouge-1, and
CM Rouge-L for evaluating the personification of code-mixed generation models. We also propose CM
KS as a distance measure to evaluate code-mixing generation models.
•Finally, we collect two large-scale longitudinal datasets from Twitter and YouTube, primarily monolingual
Hindi and Hindi-English code-mixed texts. The datasets will be valuable for code-mixing research.
2 Related Works
Rule-based and Linguistic Approaches. Code-mixed text generation has garnered much interest in
recent times. Pratapa et al. (2018) explored equivalence constraint (EC) theory to generate Spanish-English
code-mixed texts from monolingual corpora. Their linguistic theory-based approach showed superiority over
recurrent neural networks in complex code-mixed text generation. Rizvi et al. (2021) developed GCM, a
toolkit that utilizes different linguistic theories to generate code-mixed texts. Motivated by embedding ma-
trix theory, Srivastava & Singh (2021) proposed rule-based methods to generate Hindi-English code-mixed
texts. Santy et al. (2021) utilized parse tree structures within the monolingual texts for generating code-
mixed texts. Alternative approaches use generative models – generative adversarial networks (Goodfellow
et al., 2020), or variational autoencoder (VAE) (Kingma & Welling, 2014). Towards this, Garg et al. (2018)
explored recurrent neural networks with SeqGAN pre-training for generating Mandarin-English code-mixed
data. Samanta et al. (2019) developed a VAE-based method to generate realistic and coherent Hindi-English
code-mixed texts. Other classes of code-mixed text generation models explore alignment within parallel
corpora for code-mixed generation. Notably, Winata et al. (2019); Tan & Joty (2021) explored word align-
ments and candidate selection from parallel corpora for generating synthetic code-mixed texts. Amin et al.
(2023) explored word alignments for generating Marathi-English code-mixed text generation. On a similar
attempt, Dowlagar & Mamidi (2021) explored gated convolutional encoder-decoder models to identify the
compositional structure and translate English texts to Hinglish.
Pre-trained Models. With the inception of self-attention (Vaswani et al., 2017), several attempts have
been made to develop large pre-trained models showing exceptional performances in semantic and gener-
ative tasks. Among these methods, multilingual models, such as XLM (Conneau & Lample, 2019), XLM-
RoBERTa (Conneau et al., 2020), and mBART (Liu et al., 2020) have shown noticeable performance even
on low-resource languages. Recently, MuRIL (Khanuja et al., 2021) was proposed, superseding the perfor-
mances of multilingual-BERT (Devlin et al., 2019) on different syntactic and semantic tasks on a diverse
set of low-resource languages. Gupta et al. (2020) devised a semi-supervised approach to transfer knowl-
edge from XLM to generate synthetic Hindi-English code-mixed texts. Gautam et al. (2021) explored a
pre-trained mBART model for generating Hindi-English code-mixed texts. Jawahar et al. (2021) explored
multilingual text-to-text models with curriculum learning for generating Hindi-English code-mixed texts.
They pre-trained an encoder-decoder model on synthetic code-mixed texts, which improved the generation
quality on the gold code-mixed dataset. In a recent study, Yong et al. (2023) explored multilingual large
language models (LLMs) for generating code-mixed texts in a zero-shot setting. They explored InstructGPT,
ChatGPT (Ouyang et al., 2022), BLOOMZ (Muennighoff et al., 2022) and Flan-T5-XXL (Chung et al., 2022)
for generating code-mixed texts in Indonesian, Malay, Chinese, Tagalog, Vietnamese, Tamil, and Singlish.
They further emphasized the importance of better prompt templates and language pairing for generating
more coherent and natural code-mixed texts.
Personification of Code-mixing Languages. Language, as a mode of communication, is often person-
alized for different users (King & Cook, 2020). The personification of language arises naturally, depending
on the context, socio-economic and demographic background of the user and their audience (Reichelt et al.,
2014). Particularly in multilingual societies, the complex dynamics between different languages play a cru-
cial role in the personification of language. Sengupta et al. (2024) highlighted the personalization aspects of
3Published in Transactions on Machine Learning Research (10/2024)
Figure 2: PARADOX: Transformer encoder-decoder architecture with persona encoder (multi-headed (M.H.)
fused attention (FAME)).
Hindi-English code-mixed language, highlighting the importance of different sociological aspects behind the
evolution of personalized code-mixed languages.
Major Limitations of Existing Studies. Despite their popularity in several generative applications,
personalization remains neglected in the code-mixed generation. The existing code-mixed generation models
utilize parallel corpora to understand the switching patterns and generate code-mixed texts synthetically.
These limitations motivate us to develop a code-mixed language generation model that can automatically
learn the language’s semantics and capture the linguistic preferences of users while generating texts. Our
proposed method, PARADOX, improves the quality of generated texts and preserves the real-life phenomenon
of code-mixing among users.
3 Proposed Methodology
Here, we explain PARADOX, and the personalized code-mixing (CM, henceforth) generation process utiliz-
ing user persona. Before describing the proposed generative model, we first elaborate on the definition of
personalization of code-mixed language.
3.1 Personalized Code-mixing
As highlighted in Section 2, the study of the personalization of a language is crucial to develop conversational
generative models. Typically, a personalized generative model captures the user’s preferential behaviors dur-
ing generation. Instead, our work delves into the personalization of user’s linguistic preferences in terms of
mixing multiple languages in an utterance. We define the personalization of code mixing as a behavioral
language that captures a user’s writing style given the historical trends, social context, targeted demograph-
ics, and topical relevance. The proposed method, PARADOX, aims to capture these behavioral and contextual
4Published in Transactions on Machine Learning Research (10/2024)
characteristics of user historical utterances and generate texts that are aligned with their linguistic prefer-
ences.
3.2 PARADOX Architecture
PARADOX, as shown in Figure 2, consists of four components – (a) an encoder, (b) a contextual persona
encoder, (c) a decoder, and (d) an alignment module. PARADOX utilizes a persona encoder to implicitly encode
a user’s persona based on his/her historical utterances. It further projects the persona onto a probabilistic
latent space to capture the user’s contextual persona. Finally, PARADOX employs an alignment module to
re-calibrate the output sequences that help our model understand the language of code-mixing and enable
the model to generate coherent texts.
3.2.1 The Encoder with FAME
The encoder is used to jointly learn the semantics of a code-mixed text and a user’s global persona based
on the previous comments/tweets. A comment/tweet Xuby useruis first tokenized using byte-pair encod-
ings (Sennrich et al., 2016) into ⟨x1,x2,...,xn⟩. The initial contextual embedding of a token xiis conditioned
with the user persona as
^Emb (xi,u)=Embxi+PEi+Embu (1)
whereEmbxiis the initial token embedding, PEiis the positional encoding at position i, andEmbuis the
user’s global persona embedding captured through an embedding layer. Embuis computed with a unique
identifierforeachuser.ThisuniqueuserIDisimportanttoencodethetemporalevolutionofauserpersona.In
order to calculate the global persona for cold-start users (users present in the development set but not during
model training), we use a learnable [UNK]token embedding as Embu. We use a stacked encoder, in which
each encoder consists of multi-headed fused attention (FAME), followed by a residual, layer-normalization,
andpointwisefeed-forwardlayers. Senguptaetal.(2021)introducedFAMEbycombiningscaleddot-product
attention (Vaswani et al., 2017) and outer-product attention (Le et al., 2020) and showed to be effective in
capturing both semantics and morphology of code-mixed texts.
3.2.2 Contextual Persona Encoder
We project the static persona embedding onto a probabilistic latent space for generating the contextual
persona embedding for each user in a given context. We hypothesize that each user has a static (global)
persona and a contextual (local) persona. The motivation behind projecting the persona embedding to a
latent space is to capture the contextual perturbations in the user persona. For instance, Table 1 highlights
a user who predominantly uses monolingual English for raising political opinions (CMI 0.0), however, at
occasions switches between English and Hindi to create stronger narrative.
User ID Generated Text CMI
158CM:This school student is a passionate supporter of clean politics. 0.0
Eng:this school student is a passionate supporter of clean politics.
CM:once again vendetta politics will fail. Satyamev jayate. 0.21
Eng:once again vendetta politics will fail. Truth will triumph.
CM:This so called party with difference has links with brastacharis. 0.12
Eng:This so called party with difference has links with corrupts.
Table 1: Examples of user linguistic behavioral changes in different contexts. Usage of Hindi words are
highlighted with blue. The user predominantly uses monolingual English for expressing political opinions.
However, at different instances the user uses Hindi interchangeably for referring to contextual popular nar-
ratives for strengthening their expression.
Formally, we generate a contextual persona embedding
^Embu∼qϕ(z|Embu) =N(µu, σ2
u). (2)
5Published in Transactions on Machine Learning Research (10/2024)
Towards this, we define two linear projection matrices to learn the distribution location and scale parameters
as
µu=Embu.Wµandσu=Embu.Wσ.
Following the reparameterization trick (Kingma & Welling, 2014), we define the final generated persona
encoding as:
^Embu=µu+ϵu⊙σu (3)
whereϵuis the random noise, independently drawn from N(0,1). We refer to this method randomized
persona encoder . In another ablation, we use only the linear projection ^Embu=µu=Embu.Wµfor
representing the contextual persona embedding, which we call linear persona encoder . We obtain the
hidden representation of token xiconditioned on the contextual persona encoding as
^h(xi,u)=h(xi,u)+^Embu (4)
whereh(xi,u)is the final hidden representation obtained from the final layer of the encoder.
3.2.3 The Decoder
We adopt the Transformer decoder conditioned on the contextual user persona. Similar to the original
Transformer decoder, we use a stacked decoder initialized with the encoded output sequence. Drawing the
motivation from autoregressive generative language models like GPT2 (Radford et al., 2019), the decoder’s
objective is to predict the next token, conditioned on all the previous tokens. The input to the decoder is the
encoded input sequence added with positional encoding. Each decoder block consists of masked multi-headed
FAME, a residual connection, and a normalization layer. We also deploy multi-headed FAME to attend to
each decoder token with the encoded input tokens ^h(xi,u). For each decoder input position j, we generate a
hidden representation h(dec)
(j,u)∈R|V|, representing the output token at (j+1)thposition;|V|is the vocabulary
size of the decoder.
3.2.4 The Alignment Module
The final layer of PARADOX is an alignment module that learns the latent alignment matrix and re-aligns the
outputs generated by the decoder. The primary objective behind using alignments in generative models is
explicitly learning the global semantic similarity between different tokens. We use two projection matrices,
WQandWK, to project the decoder token embedding matrix Emb(dec)into two different subspaces. The
alignment matrix is defined as,
A=softmax/parenleftbigg
Q·KT
√
d/parenrightbigg
(5)
whereQ=Emb(dec)·WQ,K=Emb(dec)·WK, anddis the hidden size of the decoder. This operation
resembles the scaled dot-product attention mechanism (Vaswani et al., 2017). However, as opposed to atten-
tion, we compute the global context by considering the original embedding space of all tokens. Finally, the
re-aligned hidden representation is derived as,
]h(j,u)(dec)
=h(dec)
(j,u)·A+h(dec)
(j,u)(6)
This hidden representation is finally fed to a softmax layer to convert the outputs into probabilities. It is
important to notice that the alignment matrix Ais not intended to capture the semantic similarity between
contextualtokensbutrathertolearntheirsimilaritiesatagloballevel.Bydoingso,thealignmentmodulecan
capture the associations between all the tokens and recalibrate their probabilities during decoding. Moreover,
as the dot-product is computed over the embedding matrix, the autoregressive rule of text generation is not
violated here.
For the sake of simplicity, we denote the combination of text encoder and persona encoder as ‘encoder’
and the combination of Transformer decoder and the alignment module as ‘decoder’ throughout this paper.
The generative model is trained w.r.t. the decoder reconstruction cross-entropy loss. The contextual persona
encoder gives rise to a variational KL-divergence loss. We use a variational hyperparameter λto assign its
weightage in the final computed loss.
6Published in Transactions on Machine Learning Research (10/2024)
Algorithm 1: Code-Mixed Text Generation with PARADOX
Require: Trained modelM= (enc,dec ), user idu, historical utterance xu, prompt word{w1}, decoder
vocabularyV
Require:max_length∈N
L←{[CLS],w1};
˜w←∅;
i=m;
while ˜w̸=[SEP]andi<max_lengthdo
h(xu,u)=enc(xu,u);
Pi+1=dec(L,h (xu,u));
˜w←arg maxVPi+1;
L←L∪{˜w};
end
Return L
3.3 Training Curricula
To learn the model parameters, we primarily minimize the reconstruction loss on output sequence
⟨y1,y2,...,ym⟩between defined as,
L1(x,u)=m/summationdisplay
j=1y(j,u)log(Pθdec(y(j,u)|Y(0:j−1,u),X1:n,u)) (7)
The output sequence is initialized with y0=[CLS]token. The contextual persona encoder module arises a
Kullback–Leibler divergence loss between the variational distribution and true posterior distribution, which
can be derived (Kingma & Welling, 2014) to
L2(u)=−1
2d/summationdisplay
k=1/parenleftbig
1 + 2·log(σk
u)−(µk
u)2−(σk
u)2/parenrightbig
(8)
During training, we minimize the task-specific loss
L(x,u)=L1(x,u)+λ·L2(u)(9)
foreachtextanduseridpair (x,u)∼Dontrainingdata.Thepersonaencodingweight λisahyperparameter
we set before running the experiments.
3.4 Code-Mixed Generation
We adopt an autoregressive generation technique to generate new code-mixed texts for different users. To
encode the user’s historical persona, we use the user’s last utterance (comment/tweet) in the encoder. As
PARADOX is trained autoregressively on all historical utterances for different users, it can capture the entire
conversationalhistoryofauserfromthelastutterancewithoutpassingtheentirehistoryduringgeneration.A
typical text generation model starts decoding with a placeholder [CLS]token. Instead, we pass an additional
seed word as input to the decoder for a more guided generation. We formally report the text generation
process in Algorithm 1.
4 Experimental Setup
This section elaborates on the experimental setup we adopt to evaluate our model and baselines on person-
alized code-mixed generation.
7Published in Transactions on Machine Learning Research (10/2024)
4.1 Datasets
To the best of our knowledge, no existing longitudinal dataset is available for Hindi-English code-mixed.
A longitudinal dataset is required to study the temporal evolution of a language. Although some datasets
in the literature consist of Hindi-English code-mixed texts collected from various online social networks,
none of them contain user-specific information, making them unsuitable for our study. To overcome this, we
collected code-mixed texts from the two most popular mediums where Indians are engaged – Twitter and
YouTube. From Twitter, we collected over 0.8million in tweets starting from the year 2011till date, from
which we filtered only tweets originating from Mumbai and Delhi metropolitan regions, two cities with the
largest Hindi population. We used Twitter API for academic research with full archival access1. Further, for
relevance, we restricted ourselves to tweets related to ‘Cricket’, ‘Bollywood’, ‘Politics’, and ‘Government’.
Starting at 2014, Twitter automatically tags the language of a tweet. We selected tweets with only non-empty
language tags. This gives us a total of 226,480tweets from 19,782users.
From YouTube, we chose two channels – NishaMadhulika2(a popular chef based out of India with more
than 12.7million followers), and T-Series3(a popular Hindi music record channel started in 1983having
more than 200million followers). We selected 42videos from the NishaMadhulika channel and 69from the
T-Series that were first posted in 2011. We scraped all comments corresponding to these videos, accounting
for144,822comments from 99,998users.
For both datasets, we use a pre-trained language model open-sourced with Huggingface4, that was fine-tuned
on Hindi-English parts-of-speech (PoS) and language identification (LID) tasks. Using this model, we label
each token in each text with the corresponding language (Hindi or English) and their associated PoS. We
tag a text as code-mixed only when the text contains at least one Hindi verb written in either Devanagari
or Roman script. We select users who have at least three utterances in their entire timeline. Finally, we are
left with 18,126tweets (from 2,241users) and 8,957YouTube comments (from 1,349users).
We remove all the HTML tags, URLs, emoticons, user mentions (starting with ‘@’), and hashtags (starting
with ‘#’). For simplicity, we remove all numeric values from texts, as well. Finally, we convert all texts to
lowercase. We highlight the key statistics of the datasets in Table 2. We use a 75-25split for training and
validation with stratified sampling. Therefore, we can ensure at least one training and validation sample for
each user. We choose the first word for each validation sample as the seed word for generating the code-
mixed text. Finally, the generated code-mixed text is evaluated against the original validation text using the
proposed personalization evaluation metrics.
4.2 Evaluation Metrics
Dataset #Texts #Users Mean text length Mean CMI
Twitter 18126 2241 21.77 0.41
YouTube 8957 1349 28.89 0.36
Table 2: Dataset statistics, with mean text CMI and
the average text lengths, demonstrating the extent of
code-mixing.We adopt intrinsic and extrinsic evaluation metrics
to evaluate our model in terms of semantic under-
standing of code-mixing language and the ability to
personify code-mixing for different users.
For the intrinsic evaluation, we use perplexity , a
metric that measures the predictive power of a lan-
guagemodel,comparedagainstgroundtruth.Wecal-
culate perplexity as eloss, withlossbeing the cross-entropy reconstruction loss on the validation data. A
lower perplexity score indicates better reconstructibility and ability to learn the semantics of a generative
model.
Unlike Gupta et al. (2020), we do not have any labelled gold data for evaluating our generative model.
Therefore, traditional supervised evaluation metrics – BLEU (Papineni et al., 2002), Rouge (Lin, 2004) can
not be used directly to evaluate the personification aspects of code-mixed generation models. Similarly, other
extrinsic evaluation measures such as Multilingual index (M Index) (Barnett et al., 2000), Burstiness and
1https://api.twitter.com/2/tweets/search/all
2https://www.youtube.com/c/nishamadhulika
3https://www.youtube.com/aashiqui2
4https://huggingface.co/sagorsarker/codeswitch-hineng-lid-lince
8Published in Transactions on Machine Learning Research (10/2024)
Span Entropy (Guzmán et al., 2017) can not be used, as these metrics are predominantly used to evaluate
the ability to capture corpus-level switching patterns of generative models. To overcome the limitations
of the existing evaluation metrics, we propose four metrics for benchmarking generated code-mixed texts
against the historical utterances by different users. These proposed metrics calculate the similarity between
the linguistic patterns of model-generated texts and the user’s historical utterances. We devise CM BLEU
by calculating the BLEU score between the candidate and reference language sequences. For example -
consider a candidate code-mixed text “ mujhe park janaa hai” (“I want to go to the park”) and a reference
text “mujhericeaurcurry khana hai” (“I want to eat rice and curry”). Using the LID model, we can extract
the corresponding language sequences {Hi, Hi, Hi, Hi} and {Hi, En, Hi, En, Hi, Hi, Hi, Hi, Hi, Hi, Hi } from
the candidate and reference texts, respectively (here, Hi and En stand for Hindi and English, respectively).
Therefore, considering only the unigram and bigram overlaps between the candidate and the reference, we
calculate the CM BLEU score5of0.606. If we use a different reference text “I don’t want your nautanki”
(Translation - “I don’t want your gimmick”) with the corresponding language sequence {En, En, En, En, Hi},
the CM BLEU reduces to 0.218. The proposed metric could calculate the similarity between the switching
patterns demonstrated in the candidate text and the historical references by calculating the overlap between
the language sequences. Similarly, we compute CM Rouge-1 andCM Rouge-L by computing Rouge-1
and Rouge-L scores between the candidate and reference language sequences.
Additionally, we leverage the user-level historical CMI to evaluate the linguistic patterns of the generated
texts. If a user historically prefers monolingualism over multilingualism, we want the generative model to
learnthepatternandgeneratetextswithalowerCMIvaluefortheuser.Towardsthis,wepropose CMKS,a
metricthatcomputestheKolmogorov-SmirnovdistancebetweenthegeneratedandoriginalCMIdistribution
of users. We highlight the relationships between these metrics by calculating the Pearson correlation between
these measures, reported in Figure 6 of Appendix A.1. Strong negative correlations between perplexity and
CM BLEU, CM Rouge-1, and CM Rouge-L indicate that understanding semantics is essential to personify
and replicate the switching patterns. Therefore, by learning semantics well, the generative models can learn
the code-mixing patterns for different users and generate texts that imitate users’ linguistic patterns. On
the other hand, the correlations between CM KS and other metrics are weak, indicating that the linguistic
preferences of users have no apparent linear relationships with switching patterns. These four metrics capture
different aspects of the personalized usage of code-mixing for different users, as introduced in Section 3.
4.3 Baseline Methods
We consider several code-mixed generation models for comparative evaluation.
VACS(Samanta et al., 2019) is a VAE-based encoder-decoder model, primarily developed for generating
Hindi-English synthetic code-mixed texts.
GCM(Rizvi et al., 2021) toolkit uses several linguistic theories and heuristics to generate code-mixed
texts.
CM-XLM (Gupta et al., 2020) is a generative model that utilizes pre-trained multilingual language model
XLM to generate Hindi-English code-mixed texts from parallel corpora.
These code-mixed generation models consider monolingual reference data for generating code-mixed texts
and generate code-mixed texts at a corpus level. Therefore, we compare these baselines only in intrinsic
evaluation. We also utilize several self-attention-based encoder-decoder and pre-trained language models to
evaluate the personification aspects of generative models.
Transformer (Vaswani et al., 2017) is an encoder-decoder architecture utilizing self-attention mechanism
that has shown superior performances in generation tasks like machine translation.
MuRIL (Khanuja et al., 2021) is an encoder-based language model pre-trained on 17 Indian languages
with a masked language modeling objective.
5Can be calculated and validated using https://www.nltk.org/api/nltk.translate.bleu_score.html
9Published in Transactions on Machine Learning Research (10/2024)
BLOOMZ (Muennighoff et al., 2022) is a family of large language model based on multilingual
BLOOM (Workshop et al., 2023) that was fine-tuned with multitask prompting. We use the 3B param-
eter BLOOMZ model as our baseline.
Llama 2 (Touvron et al., 2023) is a family of autoregressive large language models trained with reinforce-
ment learning with human feedback (RLHF). We adopt the 13B parameter instruction-tuned model as one
of our baselines.
GPT-4(Achiametal.,2023)isalargemultimodalmodel,acceptingimageandtextualdataandgenerating
text outputs through autoregressive generation.
These pre-trained language models are only utilized in the extrinsic evaluation. We provide the hyperparam-
eter settings for PARADOX and all the baseline methods in Appendix A.2.
5 Comparative Analysis
In this section, we report the performances of PARADOX and the non-persona-based code-mixed generation
models in terms of the intrinsic and extrinsic evaluation measures.
Model Perplexity ↓
Twitter YouTube
GCM* 4331.85 4323.15
CM-XLM* 5413.22 1603.59
VACS 361.05 552.35
Transformer 680.07 473.84
PARADOX 297.43 196.21
(-) Contextual Persona 320.79 295.26
(-) Speaker ID 582.24 371.70
(-) Alignment 377.63 294.83
(-) FAME 864.98 583.09
Table 3: Intrinsic evaluation of the competing models based on
perplexity(↓:lowervalueindicatesbetterperformance).Formod-
els highlighted with *, perplexity is calculated with word-level
generation. Boldindicates the best results among all the mod-
els.We report the intrinsic evaluation re-
sults in Table 3. PARADOX achieves 56%
better perplexity on the Twitter dataset
than the vanilla Transformer. On the
YouTube dataset, the margin is even
higher ( 59%). A lower validation perplex-
ity shows PARADOX’s strong ability to un-
derstand code-mixing semantics and gen-
erate texts of different linguistic varia-
tions. PARADOX achieves 18%better per-
plexity on the Twitter dataset than the
best non-transformer baseline VACS. On
the YouTube dataset, the margin is signif-
icantly higher ( 64%).
Table 4 highlights the extrinsic measures
across all the generative models. On the
Twitter dataset, PARADOX achieves 2.4%
better CM BLEU than the Transformer
model. Similarly, on the YouTube dataset,
PARADOX performs the best among all
the baselines and outperforms the Trans-
former model with a margin of 2.0%. In terms of the Rouge measures, PARADOX performs consistently better
than the non-persona counterpart with an average margin of 1.9%. In terms of distance-based measures,
PARADOX performs significantly better than the Transformer model on both datasets. Overall, PARADOX
achieves 11%lower CM KS distance than Transformer. Lower KS distance indicates the importance of
utilizing user persona in generating user-specific code-mixed texts.
Among the pre-trained language models, fine-tuned Llama 2 and GPT-4 are the most competitive. Interest-
ingly, even with a single example in the prompt (1-shot), CM BLEU increases by 9.8%for Llama 2. Similar
performance improvements are also observed with other extrinsic metrics. However, both Transformer and
PARADOX perform significantly better than the pre-trained language models in terms of personalized code-
mixed text generation. On average, PARADOX achieves 7.6%better CM BLEU and 12.5%better CM Rouge-1
than Llama 2. PARADOX even achieves 3.9%better CM BLEU than the fine-tuned Llama model. Similar
performance improvements are observed with CM Rouge-1 and CM Rouge-L metrics. PARADOX achieves
4.1%better CM Rouge-1 and 3.8%better CM Rouge-L than the GPT-4 model. Even with CM KS, our
model outperforms GPT-4 with a wide margin of 14%. Interestingly, with few-shot in-context learning,
GPT-4 model demonstrates stronger performance than the existing baselines. On Twitter dataset, 1-shot
10Published in Transactions on Machine Learning Research (10/2024)
Model CM BLEU ↑ CM Rouge-1↑CM Rouge-L↑ CM KS↓
Twitter YouTube Twitter YouTube Twitter YouTube Twitter YouTube
MuRIL 9.92 9.85 26.93 21.10 23.65 19.63 0.42 0.23
BLOOMZ 14.20 23.87 49.22 56.61 45.93 55.09 0.40 0.30
Llama 2 (zero-shot) 19.97 7.25 48.91 30.86 43.74 28.72 0.56 0.43
Llama 2 (1-shot) 26.69 20.03 55.17 46.08 49.57 43.17 0.50 0.39
Llama 2 (fine-tuned) 21.97 26.09 55.89 58.24 51.07 55.17 0.40 0.34
GPT-4 (zero-shot) 30.94 30.33 57.46 57.88 50.89 53.69 0.42 0.39
GPT-4 (1-shot) 31.90 30.57 60.67 61.46 54.55 57.34 0.42 0.36
Transformer 22.21 29.36 58.69 61.02 51.10 57.27 0.42 0.37
PARADOX 24.58 31.37 60.60 62.90 53.03 59.16 0.36 0.34
(-) Contextual Persona 24.06 30.67 60.03 62.44 52.52 58.71 0.35 0.34
(-) Speaker ID 20.10 31.01 56.46 62.72 49.71 58.96 0.35 0.34
(-) Alignment 24.37 31.08 60.79 62.46 53.18 59.15 0.32 0.37
(-) FAME 18.49 28.29 54.01 60.67 47.68 57.10 0.36 0.37
Table4:Extrinsicevaluationofpre-trainedlanguagemodels,Transformerand PARADOX intermsofpreserving
user-level switching patterns ( ↑(resp.↓): higher ( resp.lower) value indicates better performance). Bold
indicates the best results among all the models.
GPT-4 model achieves higher CM BLEU and CM Rouge scores than PARADOX. This superior performance
can be justified with the facts that tweets are generally shorter and more monolingual (lower CMI), there-
fore, capturing the corpus-level trends are easier for robust LLMs like GPT-4 with even a single example
in the prompt. However, it is worth noting that 1-shot GPT-4 still underperforms than PARADOX in terms
of the personalization metric – CM KS. On YouTube dataset however, PARADOX achieves 0.8%higher CM
BLEU and 1.5%higher CM Rouge scores than the 1-shot GPT-4 model demonstrating its superiority in
understanding and generating personalized code-mixed texts.
20.022.525.027.530.0CM BLEU 586062CM Rouge-1
Linear Randomized
Speaker Contextual Module50.052.555.057.5CM Rouge-L
Linear Randomized
Speaker Contextual Module300400500600700Perplexity
YouTube T witter
Figure 3: Performances of PARADOX under linear and randomized
persona encoder.Our ablation study in Table 3 shows the
effectiveness of fused attention, contextual
persona module, and the alignment mod-
ule in PARADOX. Additionally, we empir-
ically highlight the necessity of speaker
ID encoding for personalized code-mixed
text generation. Adding FAME improves
validation perplexity by 66%. Similarly,
the contextual user persona and align-
ment modules improve validation perplex-
ity by 21%and28%, respectively, justify-
ing their contributions to modelling low-
resource language. Modeling the user per-
sona with an additional identifier (speaker
ID) improves the validation perplexity by
48%, highlighting the importance of the
module for personalized text generation.
Ablation results in Table 4 also suggest
the importance of different modules of
PARADOX for personalized code-mixed text
generation. Removing the contextual per-
sona encoder, leads to an average performance drop by 0.52%in regards to CM BLEU and CM Rouge scores.
Moreover, an one-sided t-test concludes the statistical significance ( p-value<0.001) of the performance drop
across different extrinsic measures and datasets. Figure 3 highlights the distribution of intrinsic and extrinsic
metrics for linear and randomized speaker contextualization modules. Not only does having a randomized
speaker contextual module improve the performance, but it also improves the robustness of the generation
11Published in Transactions on Machine Learning Research (10/2024)
model. Therefore, we argue that a random exploration of user persona helps our model in capturing the vari-
ability in linguistic patterns demonstrated by users in different contexts, enabling it to imitate the linguistic
patterns during generation.
5.1 Human Evaluation
Model Semantic Coherence ↑Linguistic Quality ↑
Transformer 2.34 2.32
PARADOX 3.08 3.00
Table 5: Human evaluation of the models.We perform a human evaluation study to evalu-
ate the code-mixed texts generated by PARADOX
andthevanillaTransformer.Intheprevioussec-
tion, we discussed our methodology for evaluat-
ing the personification capabilities of PARADOX
using the intrinsic and proposed extrinsic met-
rics. With human evaluation, we aim to assess the generative models in terms of linguistic correctness and
contextual relevance. Towards that, we randomly sample 24examples from each of these models and ask
30human evaluators6to rate these examples based on Semantic coherence andLinguistic quality . Semantic
coherence measures the meaningfulness of the code-mixed texts, whereas, with linguistic quality, we measure
their structural validity. Both the scores ranged between 1-5,1being the lowest, and 5being the highest.
Table 5 presents the average semantic coherence and linguistic quality scores, along with Fleiss’s
Kappa (Fleiss, 1971) scores among the annotators. We observe that PARADOX displays a better semantic
coherence ( 32%better), as well as better linguistic quality ( 29%better) than the Transformer model. We
observe fair agreement (Kappa 0.13for semantic coherence and Kappa 0.14for linguistic quality) among the
annotators for both models.
6 Analysis of Code-Mixed Generation
0 10 20 30 40
Length of textsT witter YouTube
0.00 0.25 0.50 0.75 1.00
CMI
Real
GCMCM-XLM
VACSTransformer
PARADOX
Figure 4: Comparison of different text generation models.We further study the quality of code-
mixed generation and compare them
against other baselines. We analyze the
distribution of length and CMI of texts
generated by different generative mod-
els and report in Figure 4. Trained on
the Twitter dataset, PARADOX generates
texts with a median length of 16,25%
higher than the other generative baselines.
A similar trend can also be observed in
the YouTube dataset. Similarly, the me-
dian value of CMI on texts generated by
PARADOX is0.27and0.16, respectively, for
the Twitter and YouTube datasets, which
are significantly lower than the median
CMI achieved by other baselines ( 0.29and
0.28, respectively). A lower median CMI
indicates that the texts generated by PARADOX are more monolingual at a corpus level, acknowledging the
population level trend shown in Figure 1.
Figure5(a)showsthedistributionoftopHindiverbsandnounsfromtheTwitterdataset.Beingmoreinclined
towards monolingual, PARADOX assigns more probability to these Hindi words, irrespective of the parts of
speech. Figure 5(b) shows the distribution of top Hindi verbs and nouns from the Twitter dataset for different
ablations of PARADOX. Interestingly, PARADOX with the alignment module can replicate the word distribution
observed in the real dataset. On the other hand, without the alignment module, the generative model could
hallucinate and unrealistically use common phrases in incorrect contexts. This highlights the effectiveness of
the alignment module in recalibrating output tokens and generating semantically meaningful texts. Although
the dimension of the alignment matrix is |V|×|V|, with|V|being the decoder vocabulary size, the learnable
6Evaluators are proficient with Hindi-English code-mixed language and their age ranges in 21−35.
12Published in Transactions on Machine Learning Research (10/2024)
hai ke se nahi kar
T op Hindi verbs0.000.010.020.03Probability
salman bwood gov politics srk
T op Hindi nouns
Real
PARADOXVACS
GCMCM-XLM
(a)
hai the ki ko ka
T op Hindi verbs0.000.010.02Probability
bwood gov politics srk modi
T op Hindi nouns
Real
PARADOXTransformer
PARADOX w.o. Alignment (b)
Figure 5: Distribution of top Hindi verbs and nouns for Tweets. w.o. alignment is the ablation of PARADOX
without the alignment module.
parameters are of order O(d2), wheredis the hidden dimension. The total number of additional parameters
introduced by the alignment module is 0.025%of the entire network, which is insignificant compared to other
modules.
We further analyze a few sample texts generated by the generative models. We report examples of texts
generated by PARADOX and the Transformer, along with the average semantic coherence and linguistic
quality scores annotated by the annotators in Table 7 of Appendix B. Transformer usually picks top nouns
in the corpus and generates texts around them without considering the syntax of the code-mixed language,
resulting in incoherent texts in many cases. On the other hand, PARADOX preserves the grammar of code-
mixed texts with a more human-like switching pattern. It shows that PARADOX maintains the grammar of
the base language (Hindi in this case), attributing to a more coherent and reliable generation. The examples
highlight the key differences between the texts generated by persona-based PARADOX and non-persona-based
Transformer models regarding text quality. Table 6 further shows the personalized generation by our model.
With the same prompt ( e.g.,‘salman’ in the first example), PARADOX can understand different personas of
users and can generate texts suited for different users. The high similarity between the historical average of
the user CMIs and the generated CMIs indicates the model’s ability to understand the linguistic preferences
of users.
Table 8 of Appendix B highlights code-mixed texts generated by the Llama 2 model with zero-shot and 1-
shot. PARADOX exhibits better capability in mimicking the code-mixing linguistic traits than Llama. For user
ID2226, who has had more monolingual usage in the past (average CMI 0.04), the text generated by Llama
is more code-mixed than monolingual. Similarly, for user ID 3, the Llama model reverses the linguistic
preference of the user while generating the code-mixed texts. Albeit demonstrating superior performance
acrossvariousnaturallanguageunderstandingandreasoningtaskswithzeroandfew-shotin-contextlearning,
pre-trained large language models such as Llama fail to understand the linguistic complexities of informal
languages such as code-mixed Hinglish. It is imperative to notice that the Llama model not only fails to
capture the historical linguistic preferences of users but also fails to impersonate the semantic structure
of code-mixed texts. On the other hand, PARADOX demonstrates better code-mixed language understanding
capabilities, captures the linguistic preferences of the user from their historical utterances and preserves the
information for future generations. This highlights the effectiveness of personification in code-mixed text
generation and the importance of building more robust language understanding models for understanding
the linguistic nitty-gritty of low-resource languages.
13Published in Transactions on Machine Learning Research (10/2024)
User ID Generated Text Historical Avg. CMI Generated CMI
3CM:salman ji please aapse milna hai0.28 0.33Eng:Salman, I want to meet you, please
264CM:salman ka fan ho na.0.50 0.40Eng:You are Salman’s fan, right?
762CM:ye politics kr raha kya kr rahi h or truth
show0.15 0.30
Eng:Is this politics, or truth show
2226CM:ye fb ho jaye bhaijaan0.04 0.00Eng:This has became fb, brother
Table 6: Example of different prompted (prompts highlighted with blue) generation for different users.
Different CMI indicates the difference in prompted generation based on the user persona.
7 Conclusion
This paper described a personalized code-mixed generation model for generating human-alike code-mixed
texts.Wehighlightedtheneedforapersonalizedgenerationunderthepretextofcode-mixing.Towardthis,we
devised a novel persona-aware encoder-decoder model coupled with a novel alignment module for generating
more realistic and coherent Hindi-English code-mixed texts, the first attempt toward personalized code-
mixed generation. Empirical analyses would benefit the research community in developing robust and reliable
language models for low-resource languages. Although our current work explored PARADOX for Hindi-English
generation, the proposed generative framework can be further extended to generate code-mixed texts in other
personalized code-mixing and derived languages, such as Spanish-English and Chinese-English. Although our
empiricalstudyhasshowntheeffectivenessofpersona-attributedtextgeneration,currently PARADOX captures
only contextual persona, ignoring other explicit factors. Not only does this restrict our model in cold-start
generation (text generation for new users without any history), but it also fails to consider the co-association
among users. In conversational settings, particularly, this can be deemed essential. Another limitation of
PARADOX is the inability to determine the temporal evolution of a user’s persona driven by external factors.
PARADOX captures the user persona and its evolution solely from contextual information. At the same time,
a user’s linguistic preferences can also be driven by other external socio-demographic and economic factors
varying over time, which our model currently undermines.
Broader Impact Statement
Our work highlights the need for personalized generation models for conversational languages like code-
mixing. We release our curated datasets to encourage research on personalized code-mixed text generation.
Persona-aware code-mixed generation models can aid in building data-driven solutions in low-resource lan-
guagesandcanbeexpandedtobroaderdemographics.Wedonotcollectuser-specificfeaturesordeterminants
that could reveal user-specific sensitive information. We remove all the user IDs, mentions and delimiters
from the user texts to maintain user privacy. Moreover, no user-specific attributes are used to train the
generative model except the texts written by the users. Although we do not anticipate any immediate neg-
ative impact of our work, over-personalization can lead to targeted spamming and negative misuse of user
persona. We ask the researchers to be aware of the potential misuse and use the shared artefacts judiciously
to prevent unwarranted events.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774 , 2023.
14Published in Transactions on Machine Learning Research (10/2024)
Dhiraj Amin, Sharvari Govilkar, Sagar Kulkarni, Yash Shashikant Lalit, Arshi Ajaz Khwaja, Daries
Xavier, and Sahil Girijashankar Gupta. Marathi-english code-mixed text generation. arXiv preprint
arXiv:2309.16202 , 2023.
Srijan Bansal, Vishal Garimella, Ayush Suhane, Jasabanta Patro, and Animesh Mukherjee. Code-switching
patternscanbeaneffectiveroutetoimproveperformanceofdownstreamNLPapplications:Acasestudyof
humour, sarcasm and hate speech detection. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics , pp. 1018–1023, Online, July 2020. Association for Computational Linguis-
tics. doi:10.18653/v1/2020.acl-main.96. URL https://www.aclweb.org/anthology/2020.acl-main.96 .
Ruthanna Barnett, Eva Codó, Eva Eppler, Montse Forcadell, Penelope Gardner-Chloros, Roeland van
Hout, Melissa Moyer, Maria Carme Torras, Maria Teresa Turell, Mark Sebba, Marianne Starren, and
Sietse Wensing. The lides coding manual: A document for preparing and analyzing language inter-
action data version 1.1—july, 1999. International Journal of Bilingualism , 4(2):131–132, 2000. doi:
10.1177/13670069000040020101. URL https://doi.org/10.1177/13670069000040020101 .
Khyathi Chandu, Shrimai Prabhumoye, Ruslan Salakhutdinov, and Alan W Black. “My Way of Telling a
Story”: Persona based Grounded Story Generation. In Proceedings of the Second Workshop on Storytelling ,
pp. 11–21, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-3402.
URL https://www.aclweb.org/anthology/W19-3402 .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv
preprint arXiv:2210.11416 , 2022.
Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. Advances in neural
information processing systems , 32, 2019.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual
representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Com-
putational Linguistics , pp. 8440–8451, 2020.
Amitava Das and Björn Gambäck. Identifying languages at the word level in code-mixed Indian social media
text. In Proceedings of the 11th International Conference on Natural Language Processing , pp. 378–387,
Goa, India, December 2014. NLP Association of India. URL https://aclanthology.org/W14-5152 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirec-
tional Transformers for Language Understanding, May 2019. URL http://arxiv.org/abs/1810.04805 .
Number: arXiv:1810.04805 arXiv:1810.04805 [cs].
Suman Dowlagar and Radhika Mamidi. Gated convolutional sequence to sequence based learning for english-
hingilsh code-switched machine translation. In Proceedings of the Fifth Workshop on Computational Ap-
proaches to Linguistic Code-Switching , pp. 26–30, 2021.
Joseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin , 76(5):378,
1971.
Saurabh Garg, Tanmay Parekh, and Preethi Jyothi. Code-switched Language Models Using Dual RNNs
and Same-Source Pretraining. In Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing , pp. 3078–3083, Brussels, Belgium, 2018. Association for Computational Linguistics.
doi: 10.18653/v1/D18-1346. URL http://aclweb.org/anthology/D18-1346 .
Devansh Gautam, Prashant Kodali, Kshitij Gupta, Anmol Goel, Manish Shrivastava, and Ponnurangam
Kumaraguru. Comet:Towardscode-mixedtranslationusingparallelmonolingualsentences. In Proceedings
of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching , pp. 47–55, 2021.
15Published in Transactions on Machine Learning Research (10/2024)
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):
139–144, 2020.
DeepakGupta,AsifEkbal,andPushpakBhattacharyya. ASemi-supervisedApproachtoGeneratetheCode-
Mixed Text using Pre-trained Encoder and Transfer Learning. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pp. 2267–2280, Online, 2020. Association for Computational Lin-
guistics. doi: 10.18653/v1/2020.findings-emnlp.206. URL https://www.aclweb.org/anthology/2020.
findings-emnlp.206 .
Gualberto Guzmán, Joseph Ricard, Jacqueline Serigos, Barbara E. Bullock, and Almeida Jacqueline
Toribio. Metrics for Modeling Code-Switching Across Corpora. In Interspeech 2017 , pp. 67–71. ISCA,
August 2017. doi: 10.21437/Interspeech.2017-1429. URL https://www.isca-speech.org/archive/
interspeech_2017/guzman17_interspeech.html .
Ganesh Jawahar, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, and Laks VS Lakshmanan. Ex-
ploring text-to-text transformers for english to hinglish machine translation with synthetic code-mixing.
arXiv preprint arXiv:2105.08807 , 2021.
Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey, Balaji Gopalan, Dilip Kumar
Margam, Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, Shruti Gupta, Subhash Chandra Bose Gali,
Vish Subramanian, and Partha Talukdar. MuRIL: Multilingual Representations for Indian Languages,
April 2021. URL http://arxiv.org/abs/2103.10730 . Number: arXiv:2103.10730 arXiv:2103.10730 [cs].
Milton King and Paul Cook. Evaluating approaches to personalizing language models. In Nicoletta Calzo-
lari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi,
Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis(eds.), Proceedings of the Twelfth Language Resources and Evaluation Conference ,pp.2461–2469,
Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL
https://aclanthology.org/2020.lrec-1.299 .
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes, May 2014. URL http://arxiv.
org/abs/1312.6114 . Number: arXiv:1312.6114 arXiv:1312.6114 [cs, stat].
HungLe,TruyenTran,andSvethaVenkatesh. Self-attentiveassociativememory. In International Conference
on Machine Learning , pp. 5682–5691. PMLR, 2020.
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches
out, pp. 74–81, 2004.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the
Association for Computational Linguistics , 8:726–742, 2020.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through
multitask finetuning. arXiv preprint arXiv:2211.01786 , 2022.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational
Linguistics , pp. 311–318, 2002.
RanaD.Parshad,SumanBhowmick,VineetaChand,NituKumari,andNehaSinha. WhatisIndiaspeaking?
Exploring the “Hinglish” invasion. Physica A: Statistical Mechanics and its Applications , 449:375–389,
May 2016. ISSN 03784371. doi: 10.1016/j.physa.2016.01.015. URL https://linkinghub.elsevier.com/
retrieve/pii/S0378437116000236 .
16Published in Transactions on Machine Learning Research (10/2024)
Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram, Sandipan Dandapat, and Kalika
Bali. Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data. In
Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 1543–1553, Melbourne, Australia, 2018. Association for Computational Linguistics. doi:
10.18653/v1/P18-1143. URL http://aclweb.org/anthology/P18-1143 .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Maria Reichelt, Frauke Kämmerer, Helmut M Niegemann, and Steffi Zander. Talk to me personally: Per-
sonalization of language style in computer-based learning. Computers in Human behavior , 35:199–210,
2014.
Mohd Sanad Zaki Rizvi, Anirudh Srinivasan, Tanuja Ganu, Monojit Choudhury, and Sunayana Sitaram.
GCM: A Toolkit for Generating Synthetic Code-mixed Text. In Proceedings of the 16th Conference of
the European Chapter of the Association for Computational Linguistics: System Demonstrations , pp. 205–
211, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-demos.24. URL
https://aclanthology.org/2021.eacl-demos.24 .
Koustav Rudra, Shruti Rijhwani, Rafiya Begum, Kalika Bali, Monojit Choudhury, and Niloy Ganguly.
Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-English
Speakers do on Twitter? In Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-
guage Processing , pp. 1131–1141, Austin, Texas, 2016. Association for Computational Linguistics. doi:
10.18653/v1/D16-1121. URL http://aclweb.org/anthology/D16-1121 .
Bidisha Samanta, Sharmila Reddy, Hussain Jagirdar, Niloy Ganguly, and Soumen Chakrabarti. A Deep
Generative Model for Code Switched Text. In Proceedings of the Twenty-Eighth International Joint Con-
ference on Artificial Intelligence , pp. 5175–5181, Macao, China, August 2019. International Joint Confer-
ences on Artificial Intelligence Organization. ISBN 978-0-9992411-4-1. doi: 10.24963/ijcai.2019/719. URL
https://www.ijcai.org/proceedings/2019/719 .
SebastinSanty,AnirudhSrinivasan,andMonojitChoudhury. Bertologicomix:Howdoescode-mixinginteract
with multilingual bert? In Proceedings of the Second Workshop on Domain Adaptation for NLP , pp. 111–
121, 2021.
Ayan Sengupta, Sourabh Kumar Bhattacharjee, Tanmoy Chakraborty, and Md Shad Akhtar. Hit-a hierar-
chically fused deep attention network for robust code-mixed language representation. In Findings of the
Association for Computational Linguistics: ACL-IJCNLP 2021 , pp. 4625–4639, 2021.
Ayan Sengupta, Sourabh Kumar Bhattacharjee, Md. Shad Akhtar, and Tanmoy Chakraborty. Does ag-
gression lead to hate? Detecting and reasoning offensive traits in hinglish code-mixed texts. Neu-
rocomputing , 488:598–617, June 2022a. ISSN 0925-2312. doi: 10.1016/j.neucom.2021.11.053. URL
https://www.sciencedirect.com/science/article/pii/S0925231221017306 .
Ayan Sengupta, Tharun Suresh, Md Shad Akhtar, and Tanmoy Chakraborty. A Comprehensive Un-
derstanding of Code-mixed Language Semantics using Hierarchical Transformer, April 2022b. URL
http://arxiv.org/abs/2204.12753 . Number: arXiv:2204.12753 arXiv:2204.12753 [cs].
Ayan Sengupta, Soham Das, Md Shad Akhtar, and Tanmoy Chakraborty. Social, economic, and demographic
factors drive the emergence of hinglish code-mixing on social media. Humanities and Social Sciences
Communications , 11(1):1–12, 2024.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural Machine Translation of Rare Words with Sub-
word Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pp. 1715–1725, Berlin, Germany, 2016. Association for Computational Linguis-
tics. doi: 10.18653/v1/P16-1162. URL http://aclweb.org/anthology/P16-1162 .
17Published in Transactions on Machine Learning Research (10/2024)
Kushagra Singh, Indira Sen, and Ponnurangam Kumaraguru. Language Identification and Named Entity
Recognition in Hinglish Code Mixed Tweets. In Proceedings of ACL 2018, Student Research Workshop ,
pp. 52–58, Melbourne, Australia, 2018a. Association for Computational Linguistics. doi: 10.18653/v1/
P18-3008. URL http://aclweb.org/anthology/P18-3008 .
Kushagra Singh, Indira Sen, and Ponnurangam Kumaraguru. A Twitter Corpus for Hindi-English Code
Mixed POS Tagging. In Proceedings of the Sixth International Workshop on Natural Language Processing
for Social Media , pp. 12–17, Melbourne, Australia, 2018b. Association for Computational Linguistics. doi:
10.18653/v1/W18-3503. URL http://aclweb.org/anthology/W18-3503 .
Abhishek Srivastava, Kalika Bali, and Monojit Choudhury. Understanding script-mixing: A case study of
hindi-englishbilingualtwitterusers. In Proceedings of the The 4th Workshop on Computational Approaches
to Code Switching , pp. 36–44, 2020.
Vivek Srivastava and Mayank Singh. Hinge: A dataset for generation and evaluation of code-mixed hinglish
text. InProceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems , pp. 200–208,
2021.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning
in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,
pp. 3645–3650, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/
P19-1355. URL https://aclanthology.org/P19-1355 .
Samson Tan and Shafiq Joty. Code-mixing on sesame street: Dawn of the adversarial polyglots. arXiv
preprint arXiv:2103.09593 , 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-
rer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh
Koura,Marie-AnneLachaux,ThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,
Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,
Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert
Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
Zihao Wang, Ming Jiang, and Junli Wang. A Speaker-aware Parallel Hierarchical Attentive Encoder-Decoder
Model for Multi-turn Dialogue Generation, October 2021. URL http://arxiv.org/abs/2110.06823 .
Number: arXiv:2110.06823 arXiv:2110.06823 [cs].
Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Code-switched language models
using neural based synthetic data from parallel sentences. arXiv preprint arXiv:1909.08582 , 2019.
BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow,
Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang,
Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas
Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz
Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel,
Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien,
18Published in Transactions on Machine Learning Research (10/2024)
David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim,
Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar,
Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier
de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joy-
deep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long
Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María
Grandury,MarioŠaško,MaxHuang,MaximinCoavoux,MayankSingh,MikeTian-JianJiang,MinhChien
Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis,
Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla
Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey
Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma,
Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-
rent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea San-
tilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang,
Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful
Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen,
Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish
Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar
Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak
Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von
Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi,
Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anasta-
sia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette,
Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra
Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive,
Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton
Cheng,OlegSerikov,OmerAntverg,OskarvanderWal,RuiZhang,RuochenZhang,SebastianGehrmann,
Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena
Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bam-
berger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana
Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh
HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Daniel Mc-
Duff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi
Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani
Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis
Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akin-
lolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani,
Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo
Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, An-
tonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjava-
cas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec,
Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai
Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi,
Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cul-
lan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang,
Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick
Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su,
Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott,
SineeSang-aroonsiri,SrishtiKumar,StefanSchweter,SushilBharati,TanmayLaud,ThéoGigant,Tomoya
19Published in Transactions on Machine Learning Research (10/2024)
Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu,
Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A
176b-parameter open-access multilingual language model, 2023.
Zheng Xin Yong, Ruochen Zhang, Jessica Forde, Skyler Wang, Arjun Subramonian, Holy Lovenia, Samuel
Cahyawijaya, Genta Winata, Lintang Sutawika, Jan Christian Blaise Cruz, et al. Prompting multilingual
large language models to generate code-mixed texts: The case of south east asian languages. In Proceedings
of the 6th Workshop on Computational Approaches to Linguistic Code-Switching , pp. 43–63, 2023.
Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. Adversarial
feature matching for text generation. In International Conference on Machine Learning , pp. 4006–4015.
PMLR, 2017.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and William B Dolan. Dialogpt: Large-scale generative pre-training for conversational response
generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics:
System Demonstrations , pp. 270–278, 2020.
YinheZheng,RongshengZhang, XiaoxiMao,and Minlie Huang. APre-trainingBasedPersonalizedDialogue
Generation Model with Persona-sparse Data, November 2019. URL http://arxiv.org/abs/1911.04700 .
Number: arXiv:1911.04700 arXiv:1911.04700 [cs].
A Experimental Setup
A.1 Evaluation Metrics
We compute Pearson correlation (c.f. Figure 6) between the proposed metrics to understand their rela-
tionships. We observe strong positive correlations among the metrics – CM BLEU, CM Rouge-1 and CM
Rouge-L. Similarly, all of these metrics have a strong negative correlation with perplexity. This highlights
the strong linear associations between the evaluation metrics that measure the semantical behavior of the
code-switching patterns. On the other hand, the correlations between the CM KS measure and other metrics
are meagre, indicating that the linguistic preferences of users have very minimal impact on the switching
patterns. In other words, the switching patterns of a user who prefers multilingual could be more erratic
than those of a user who prefers monolinguals.
Perplexity CM BLEU CM Rouge-1 CM Rouge-L CM KS
Perplexity
CM BLEU
CM Rouge-1
CM Rouge-L
CM KS1 -0.58 -0.78 -0.62 0.15
-0.58 1 0.9 0.99 0.012
-0.78 0.9 1 0.9 -0.062
-0.62 0.99 0.9 1 0.019
0.15 0.012 -0.062 0.019 1
 0.6
0.4
0.2
0.00.20.40.60.81.0
Figure 6: Pearson correlation between different evaluation measures on the validation dataset.
20Published in Transactions on Machine Learning Research (10/2024)
[INST]
<<SYS>> 
You are a helpful assistant that generate a Hindi -English text based on a user’s previous message and a seed 
word. Make sure that you understand the user’s linguistic pattern from the previous message and generate a 
text that starts with the seed word.
<</SYS>>
User Previous Message: {user_message}
Seed Word: {seed_word}
[/INST]
Figure 7: Prompt used with the Llama 2 model for Hindi-English code-mixed text generation.
A.2 Training Details
For all the models across all the experiments, we use a maximum text length of 40.PARADOX consists of
six encoder and decoder layers, with hidden sizes of 768in all the layers. For multi-headed FAME and
masked multi-headed FAME blocks, we use a total of eight heads with Dropout probabilities set as 0.1. The
total number of parameters is 296M. We use six encoder and six decoder layers in the Transformer model,
with eight heads in each multi-headed attention block. For training PARADOX, We set the persona encoding
variational weight λ= 0.5. All the models are trained for 50epochs with an early stopping condition on
validation loss with the patience of 10. We setbatch_size = 4in all experiments during training and
validation. We use Adam optimizer with a learning rate of 4e−4andβ1= 0.9,β2= 0.98for both PARADOX
and Transformer. We fine-tune the MuRIL and BLOOMZ models on autoregressive language modeling tasks
for10epochs with learning rates 3e−5and3e−6, respectively. The Llama-2 baseline is used in zero-shot and
1-shot settings with the prompt shown in Figure 7. We use one Tesla P100 and one Tesla V100 GPU to run
all our experiments. For PARADOX, each training and validation iteration takes ∼0.18and∼0.12seconds,
respectively. Strubell et al. (2019) proposed estimation of power usage and carbon emission behind running
deep learning experiments. Following those guidelines, we estimate a total power usage of 23.56kWh and an
equivalent CO2 emission of 22.46pounds.
B Analysis of Code-Mixed Generation
We highlight few examples of texts generated by PARADOX and the vanilla Transformer model in Table 7,
along with their semantic coherence and linguistic quality scores annotated by the annotators. As highlighted
in Table 5, the semantic coherence of the texts generated by PARADOX is much higher than the non-persona-
based counterpart. Moreover, texts generated by PARADOX are grammatically more valid than those generated
by the Transformer model. This analysis highlights the importance of incorporating persona for an enhanced
understanding of conversational code-mixed languages.
Table 8 highlights the texts generated by the persona-based PARADOX and the Llama model for two different
users. The margin between the historical average CMI and the generated CMI highlights our method’s
superiority in capturing users’ linguistic patterns. A large pre-trained language model such as Llama, even
with few-shot in-context learning, may not be superior to a persona-based small language model fine-tuned
on low-resource code-mixed texts.
21Published in Transactions on Machine Learning Research (10/2024)
Model Generated Text Semantic Coherence Linguistic Quality
PARADOXCM:are bhai apne! 3.70 3.60
Eng:Hey my brother!
CM:bhai bahut thik ho gaya 4.27 4.27
Eng:brother is very well
TransformerCM:rockstar walo se samjha me-
dia........3.07 3.57
Eng:Media understood by rockstars
CM:boy likha hai thanks bhai 3.57 3.53
Eng:Boyhaswrittenthanksbrother
Table7:Examplesofcode-mixedtextsgeneratedby PARADOX andTransformerwithhumanannotatedaverage
Semantic coherence and linguistic quality scores.
User ID Model Generated Text Historical Avg. CMI Generated CMI
3PARADOXCM:salman ji please aapse milna hai0.28 0.33Eng:Salman, I want to meet you, please
Llama zero-shotCM:Salman ne kaha tha flash me jeetega apka
siddhant0.28 0.22
Eng:Salman had said flushme jeetega apna prin-
ciple
Llama 1-shotCM:Salman bhai ne kaha, Flush mein jitega apna
side, main toh bus se nikal raha hoon!0.28 0.11
Eng:Salman had said flushme jeetega apna side, I
am just leaving in a bus
2226PARADOXCM:ye fb ho jaye bhaijaan0.04 0.00Eng:This has became fb, brother
Llama zero-shotCM:ye superb jabardast bahut khub, bhadaai ho0.04 0.37Eng:This is superb, amazing, and congratulations
Llama 1-shotCM:ye superb jabardast bahut khub, bhadaai ho0.04 0.37Eng:This is superb, amazing, and congratulations
Table 8: Example of texts generated by PARADOX and Llama 2 models.
22