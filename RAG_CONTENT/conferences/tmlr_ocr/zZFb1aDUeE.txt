Published in Transactions on Machine Learning Research (12/2024)
Privacy Preserving Reinforcement Learning for Population
Processes
Samuel Yang-Zhao samuel.yang-zhao@anu.edu.au
Australian National University
Kee Siong Ng keesiong.ng@anu.edu.au
Australian National University
Reviewed on OpenReview: https: // openreview. net/ forum? id= zZFb1aDUeE
Abstract
We consider the problem of privacy protection in Reinforcement Learning (RL) algorithms
that operate over population processes, a practical but understudied setting that includes, for
example, the control of epidemics in large populations of dynamically interacting individuals.
In this setting, the RL algorithm interacts with the population over Ttime steps by
receiving population-level statistics as state and performing actions which can affect the
entire population at each time step. An individual’s data can be collected across multiple
interactions and their privacy must be protected at all times. We clarify the Bayesian
semantics of Differential Privacy (DP) in the presence of correlated data in population
processes through a Pufferfish Privacy analysis. We then give a meta algorithm that can
take any RL algorithm as input and make it differentially private. This is achieved by taking
an approach that uses DP mechanisms to privatize the state and reward signal at each time
step before the RL algorithm receives them as input. Our main theoretical result shows that
the value-function approximation error when applying standard RL algorithms directly to
the privatized states shrinks quickly as the population size and privacy budget increase. This
highlights that reasonable privacy-utility trade-offs are possible for differentially private RL
algorithms in population processes. Our theoretical findings are validated by experiments
performed on a simulated epidemic control problem over large population sizes.
1 Introduction
The increasing adoption of Reinforcement Learning (RL) algorithms in many practical applications such
as digital marketing, finance, and public health (Mao et al., 2020; Wang & Yu, 2021; Charpentier et al.,
2020) have led to new, challenging privacy considerations for the research community. This is a particularly
important issue in domains like healthcare where highly sensitive personal information is routinely collected
and the use of such data in training RL algorithms must be handled carefully, in light of successful privacy
attacks on RL algorithms (Pan et al., 2019). Privacy Preserving Reinforcement Learning is an active research
area looking to address these concerns, mostly via the now widely accepted concept of Differential Privacy
(DP) (Dwork et al., 2006), which confers formal ‘plausible deniability’ guarantees for users whose data are
used in training RL algorithms, thus offering them privacy protection.
In this paper, we consider the setting where an RL agent interacts with a population of individuals and
investigate how each individual’s differential privacy can be protected. We assume interactions between
individuals are not visible to the RL agent but the agent receives population-level statistics at every time
step and can perform actions that affect the entire population. This class of environments, which we call
population processes , models settings such as the control of epidemic spread by government interventions
(Kompella et al., 2020). As an individual’s data is collected across multiple interactions, our goal is to ensure
that an individual’s data contributions over all interactions are differentially private. To the best of our
1Published in Transactions on Machine Learning Research (12/2024)
knowledge, existing approaches to privacy-preserving reinforcement learning, which we survey next in § 1.1,
do not cater to this natural and important problem setting and are unable to exploit the structure of the
problem.
1.1 Related Work
The earliest works to consider differential privacy in a reinforcement learning context were focused on the
bandit or contextual bandit settings (Guha Thakurta & Smith, 2013; Mishra & Thakurta, 2015; Tossou
& Dimitrakakis, 2016; 2017; Shariff & Sheffet, 2018; Sajed & Sheffet, 2019; Zheng et al., 2020; Dubey &
Pentland, 2020; Ren et al., 2020; Chowdhury & Zhou, 2022b; Azize & Basu, 2022). A key negative result
is in Shariff & Sheffet (2018), who show that sublinear regret is not possible under differential privacy in
contextual bandits, a result which also holds in the reinforcement learning setting.
Differentially private reinforcement learning beyond the bandit setting has been primarily considered in a
personalization context. In Balle et al. (2016), the authors study policy evaluation under the setting where
the RL algorithm receives a set of trajectories, and a neighbouring dataset is one in which a single trajectory
differs. In a regret minimization context, there is a body of work on designing RL algorithms to satisfy
either joint differential privacy (Vietri et al., 2020; Luyo et al., 2021; Chowdhury & Zhou, 2022a; Ngo et al.,
2022; Zhou, 2022; Qiao & Wang, 2023) or local differential privacy (Garcelon et al., 2020; Luyo et al., 2021;
Chowdhury & Zhou, 2022a; Liao et al., 2023). In all of these works, the RL algorithm is framed as interacting
with users in trajectories or episodes, with each trajectory representing multiple interactions with a singleuser.
A neighbouring dataset is then defined with respect to a neighbouring trajectory. Such DP-RL approaches
cannot be easily adapted for privacy protection in population processes, where (i) each interaction is with
an entire population (rather than a single individual); (ii) a specific individual is typically not sampled in
consecutive time steps so there is not a corresponding notion of a trajectory; and (iii) an individual’s data
could actually be present across all time steps, a low-probability event that we nevertheless have to handle.
In Wang & Hegde (2019), the authors analyse the performance of deep Q-learning (Mnih et al., 2015) under
differential privacy guarantees specified with respect to neighbouring reward functions. This notion of privacy
makes natural sense when the reward function is viewed as an individual’s private preferences but is also
inapplicable to our setting as it does not consider the privacy of the state.
In relation to our application domain and experimental results, the control of epidemics is a topical subject
given the prevalence of COVID-19 in recent years and many different practical approaches have been developed
(Arango & Pelov, 2020; Charpentier et al., 2020; Colas et al., 2020; Berestizshevsky et al., 2021; Kompella
et al., 2020; Chen et al., 2022). Whilst the preservation of individual privacy has been considered in the
context of public release of population-level epidemic statistics (Dyda et al., 2021; Li et al., 2023), we are not
aware of other work that achieves individual privacy preservation in the modelling and control of epidemics
using reinforcement learning.
1.2 Contributions
The main questions we address in this paper are the following:
1.For an RL algorithm interacting with a population of individuals, what is the right notion of individual
privacy we should care about, and can that individual privacy be protected using differential privacy?
2.Assuming the answer to the first question is positive, are good privacy-utility trade-offs possible for
differentially private RL algorithms in population processes?
We answer the first question in two parts. We first show through a concrete example (see Example 4) that
highly correlated data is possible in population processes and that some sensitive information that one may
naturally wish to protect, like a person’s infection status during an epidemic, cannot always be protected.
We then give, using the general Pufferfish Privacy framework (Kifer & Machanavajjhala, 2014), a precise
semantic definition of the secrets around individual participation in samples that can actually be protected
and show its equivalence to k-fold adaptive composition of DP mechanisms (see Lemma 1). Building on that,
2Published in Transactions on Machine Learning Research (12/2024)
we then describe a family of DP-RL algorithms for population processes and show how differential privacy
and correlated data are somewhat orthogonal issues in our set up.
On the second question, we note our DP-RL solution is a meta algorithm that takes any RL algorithm
(whether online/offline or value-based/policy-based) as a black box and make it differentially private. Whilst
such a modular solution is desirable for its simplicity and generality, the trade-off is a more difficult control
problem because the underlying environment becomes partially observable to the RL agent. Standard methods
for dealing with partial observability typically require expensive state-estimation techniques or sampling based
approximations (Monahan, 1982; Shani et al., 2013; Kurniawati, 2022). Instead of using these methods, we
analyze the performance of standard RL algorithms as the sampled population size Nand privacy budget ϵ
increases. Under some assumptions, we obtain the following bound on the approximation error under privacy
/vextenddouble/vextenddoubleQ∗−˜Q∗/vextenddouble/vextenddouble
∞≤O/parenleftigg
√
Kexp/parenleftbigg
−ϵ
2√
2K/parenrightbigg
+Kexp/parenleftig
−√
Nϵ/parenrightig
+K3
2√
N/parenrightigg
, (1)
whereQ∗is the optimal value function for a given (arbitrary) population process M,˜Q∗is the optimal value
function for the privatized form of M, andKis the dimension of the state space. (The precise statement
is Theorem 3 in § 4.2.) Note that the RHS of (1) goes to 0 as Nandϵincreases, and that such a result
is not possible in the personalization setting (Shariff & Sheffet, 2018). Whilst this result does not imply a
finite-time sample complexity guarantee, we validate empirically on an epidemic control problem simulated
over large graphs that our DP-RL algorithm behaves well as the population size and privacy budget increases,
as suggested by (1). Our results demonstrate that reasonable privacy-utility trade-offs are certainly possible
for differentially private RL algorithms in population processes.
2 Preliminaries
2.1 Reinforcement Learning and Markov Decision Processes
We consider a time-homogeneous Markov Decision Process (MDP) M=(S,A,P,r,γ )with state space S,
action spaceA, transition function P:S×A→D (S), reward function r:S×A→ [0,rmax], discount factor
γ∈[0,1). The notationD(S)defines the set of distributions over S. The rewards are assumed to be bounded
between 0 and rmax∈R. A stationary policy is a function π:S→D (A)specifying a distribution over actions
based on a given state, i.e. at∼π(·|st). A stationary deterministic policy assigns probability 1 to a single
action in a given state. With a slight abuse of notation, we will define a stationary deterministic policy to have
the signature π:S→A. Let Πdenote the set of all stationary policies. The action-value function (Q-value)
of a policyπis the expected cumulative discounted reward Qπ(s,a) =r(s,a) +EP,π[/summationtext∞
t=1γtr(st,at)], where
the expectation is taken with respect to the transition function Pand policyπat each time step. The value
function is defined as Vπ(s) =Ea∼π(·|s)[Qπ(s,a)]. The Q-value satisfies the Bellman equation given by
Qπ(s,a) =r(s,a) +γEP[Vπ(s′)].
When considering the optimal policy, define V∗(s) =supπ∈ΠVπ(s)andQ∗(s,a) =supπ∈ΠQπ(s,a). The
optimal action-value function satisfies the bellman optimality equation given by Q∗(s,a) =r(s,a) +
γEP[maxa′Q∗(s′,a′)]. There exists an optimal stationary deterministic policy π∗∈Πsuch thatV∗(s) =
Vπ∗(s). The greedy policy π∗(s) = arg max aQ∗(s,a)is in fact an optimal policy.
We primarily consider the case when Sis finite. In this case, it is helpful to view our functions as vectors and
matrices. We use Pto refer to a matrix of size (|S|×|A| )×|S|wherePs′
sais equal toP(s′|s,a)andPsais a
length|S|vector denoting P(·|s,a). Similarly, we can view Vπas a vector of length |S|andQπandras
vectors of length |S|×|A| . The Bellman equation can now be expressed as
Qπ=r+γPVπ.
2.2 Stochastic Population Processes
A stochastic population process models a stochastic system evolving over a collection of individuals and
allows for heterogeneous interactions between them. Consider a population of N∗individuals indexed by
3Published in Transactions on Machine Learning Research (12/2024)
the set [N∗] ={1,...,N∗}. Individual i’s status at time tis given by the random variable Xt,i∈[K], which
takes one of Kvalues, and Xt= (Xt,1,...,Xt,N∗)denotes the random vector of all individuals’ status. A
graph at time tis denoted by Gt= (V,Et)where the nodes Vrepresent the individuals and Etrepresent
the interactions between individuals at time t. We assume that the total number of individuals is fixed
but the edges evolve over time. To denote sequences, let Y1:t= (Y1,...,Yt)andY<t= (Y1,...,Yt−1). The
graph at time tevolves according to a distribution Gt∼P(·|G<t). An individual’s status depends only
upon the previous graph and the previous status of its neighbours, thus satisfying the Markov property,
and can be expressed as P(Xt|G<t,X<t) =P(Xt|Gt−1,Xt−1). We will assume that individuals’ initial
status are drawn independently from a distribution P(X0|G<0,X<0) =P(X0)and are not dependent on
any interaction graph.
2.3 Differential Privacy
Differential privacy is now a commonly accepted definition of privacy with good guarantees even under
adversarial settings (Dwork et al., 2006). The differential privacy definition allows different notions of privacy
by defining the concept of a neighbouring dataset appropriately. The following definition is a common choice
(Dimitrakakis et al., 2017):
Definition 1 (Differential Privacy) .A randomized mechanism M:Xn→Uis(ϵ,δ)-differentially private if
for anyD∈Xnand for any measurable subset Ω⊆U
P(M(D)∈Ω)≤eϵP(M(D′)∈Ω) +δ,
for allD′in the Hamming-1 neighbourhood of D. That is,D′may differ in at most one entry from D: there
exists at most one i∈[n]such thatDi̸=D′
i.
A standard approach to privatising a query over an input dataset is to design a mechanism Mthat samples
noise from a carefully scaled distribution and add it to the true output of the query. To scale the noise level
appropriately, the sensitivity of a query is an important parameter.
Definition 2 (ℓ1sensitivity) .Letfbe a function f:Xn→ U. Letd:Xn×Xn→ { 0,1}be
a function indicating whether two inputs are neighbours. The sensitivity of fis defined as ∆f=
supx,x′∈Xn:d(x,x′)=1∥f(x)−f(x′)∥1.
Differential privacy has several well known properties, such as composing adaptively and being preserved
under post-processing, that we will make use of.
2.4 Pufferfish Privacy.
Pufferfish privacy was introduced in Kifer & Machanavajjhala (2014) and proposes a generalization of
differential privacy from a Bayesian perspective. In the Pufferfish framework, privacy requirements are
instantiated through three components: (i) S, the set of secrets representing functions of the data that we
wish to protect; (ii) Q⊆S×S, a set of secret pairs that need to be indistinguishable to an adversary; and
(iii)Θ, a class of distributions that can plausibly generate the data. Typically Θis viewed as the beliefs that
an adversary holds over how the data was generated. Pufferfish privacy is defined as follows:
Definition 3 (Pufferfish Privacy) .Let(S,Q,Θ)denote the set of secrets, secret pairs, and data generating
distributions and let Dbe a random variable representing the dataset. A privacy mechanism Mis said to
be(ϵ,δ)-Pufferfish private with respect to (S,Q,Θ)if for allθ∈Θ,D∼θ, for all (si,sj)∈Q, and for all
w∈Range (M), we have
e−ϵ≤P(M(D) =ω|si,θ)−δ
P(M(D) =ω|sj,θ)≤eϵ, (2)
wheresi,sjare such that P(si|θ)̸= 0,P(sj|θ)̸= 0.
Forδ= 0, applying Bayes Theorem to Equation 2 shows that Pufferfish privacy can be interpreted as
bounding the odds ratio of sitosj; an attacker’s belief in sibeing true over sjcan only increase by a factor
4Published in Transactions on Machine Learning Research (12/2024)
ofeϵafter seeing the mechanism’s output. The main advantages of Pufferfish privacy are that it provides a
formal way to explicitly codify what privacy means and what impact the data generating process has. This
has been used to clarify, among other things, the semantics of differential privacy protection in the presence
of possibly correlated data in Kifer & Machanavajjhala (2014), a topic that motivated multiple attacks and
possible solutions (Kifer & Machanavajjhala, 2011; Liu et al., 2016; Srivatsa & Hicks, 2012; Zhao et al., 2017;
Yang et al., 2015; Almadhoun et al., 2020). We will similarly use the Pufferfish framework to make the
semantics of our privacy guarantees in population processes, which can have correlated data, explicit.
3 Problem Setting
3.1 Model
We now describe how the problem of controlling population processes is modelled as an MDP and also the
underlying data-generation process.
The environment Eis modelled as a stochastic population process evolving over N∗individuals. The RL agent
is denoted by A. We assume there is a trusted data curator Dthat collects the data from the environment.
At each time step, Nindividuals are randomly sampled (not necessarily uniformly) and their potentially
sensitive data is collected by the data curator. We consider the case where the interactions between individuals
are unknown, i.e. Dhas no access to the graph sequence G1:T. This models many problem domains such as
the control of epidemics where the underlying interactions between people in a population are not visible and
decisions can only be made based upon population statistics. We consider the case of histogram queries. The
agentApicks actions at each time step depending upon the received state and also computes its reward
rt=r(st,at−1)as a function of the current state and previous action. In summary, the environment, agent,
and data curator interact in the following manner, given an initial graph G0and status X0generated from
some distribution.
For timet= 1,...,T:
1.Egenerates the status for all individuals Xt∼P(·|Gt−1,Xt−1), withXt= (Xt,1,...,Xt,N∗).
2.Dsamples a subset Lt⊆[N∗]of sizeNand produces the dataset Dt= (Xt,i)i∈Lt.
3.Danswers histogram query st=q(Dt) =1
N/summationtext
i∈Lt(I(Xt,i=α))α∈[K]and computes reward
rt=r(st,at−1).
4.Areceives state stand reward rtand forms the transition sample (st−1,at−1,st,rt)to learn from.
5.Apicks the next action at∼πt(·|st).
6. The graph Gt∼P(·|Gt−1,at)is sampled depending on the last graph and action selected.
The state spaceSthus consists of any vector z∈[0,1]Kthat satisfies/summationtext
i∈[K]zi= 1andzi=ci/Nfor some
ci∈{0,1,...,N}. The action space Ais application-dependent. In general, we consider the setting where a
selected action at∈Amodifies the edges in the graph. For example an action could correspond to cutting all
edges for a subset of nodes, emulating the effect of ‘quarantining’ individuals in an epidemic control context.
Thus the graph at time tdepends upon the action chosen and is distributed according to P(Gt|Gt−1,at).
These underlying transitions on individuals’ states and the graph structure will induce a transition function
P:S×A→D (S)over the state space that implicitly captures the impact of the agent’s actions.
An individual’s data is exposed via the state histogram query st=q(Dt). The state query has sensitivity
∆q= 2/Nas using individual j’s data instead of individual i’s data can change the counts in at most two
bins of the histogram. Additionally an individual could be sampled multiple times by DoverTinteractions
so we need to ensure that their combined data over Tsteps are treated in a differentially private way.
Example 1 (Epidemic Control) .Throughout this paper we consider the Epidemic Control problem as a
concrete instantiation of our problem setting. One particular example is the Susceptible-Exposed-Infected-
Recovered-Susceptible (SEIRS) process on contact networks (Pastor-Satorras et al., 2015; Nowzari et al.,
5Published in Transactions on Machine Learning Research (12/2024)
2016; Newman, 2018). An SEIRS process on contact networks is parametrized by a graph G= (V,E),
representing interactions, and four transition rates β,σ,γ,ρ> 0. At any point in time, each individual is in
one of four states: Susceptible ( S), Exposed ( E), Infected ( I), or Recovered ( R). If individual iis Susceptible
at timetand has interacted with dt,iindividuals who are Infected, then individual ibecomes Exposed with
probability 1−(1−β)dt,i. Once Exposed, an individual becomes Infected after Geometric( σ) amount of time.
Similarly, an Infected individual becomes Recovered with Geometric( γ) time and a Recovered individual
becomes Susceptible again with Geometric( ρ) time. At each time t, the state is a histogram representing the
proportion of individuals that are of each status. Instead of allowing all interactions, the agent’s actions allow
for a subset of nodes to be quarantined for one time step. This has the effect of modifying the graph’s edges
such that quarantined individuals have no edges for a single time step. A typical reward function provides a
cost proportional to the number of individuals quarantined and the number of infected individuals.
S E I Rβ σ γρ
Figure 1: Visualisation of the parameters that govern the transitions between states for individuals in the
SEIRS process over contact networks.
Example 2 (Countering Misinformation) .The spread of misinformation in online social media is one of
the key threats to society. There is good literature on different (mis)information percolation and diffusion
models (Del Vicario et al., 2016; Van Der Linden, 2022) that take factors like homogeneity and polarisation
into account, as well as containment strategies like running counter-information campaigns to debunk or
‘prebunk’ misinformation, possibly through targeting of influencers in social networks (Budak et al., 2011;
Nguyen et al., 2012; Acemoglu et al., 2023; Ariely, 2023). Designing reinforcement learning algorithms that
can detect and control spread of misinformation in a differentially private manner is another example of our
general problem setting.
Example 3 (Malware Detection and Control) .Malware propagation models on large-scale networks (Yu
et al., 2014) and smartphones (Peng et al., 2013) have long been a subject of interest in cyber security, with
more recent work focussing on malware propagation in internet-of-things (Li et al., 2020; Yu et al., 2022;
Muthukrishnan et al., 2021). Designing reinforcement learning algorithms to detect and control the spread
of malware, especially unknown malwares whose signatures can only be discerned from collecting data on
potentially sensitive device-usage behaviour, is also an example of our general problem setting.
3.2 Formalizing Privacy in the Presence of Correlated Data
In this section we clarify the precise privacy protections we will provide under differential privacy in our
problem setting. Protecting privacy in population processes presents some unique issues that are not typically
encountered in the standard application of differential privacy. In particular, correlations between different
individuals’ data can easily arise due to the fact that individuals interact with each other at each time step
and also over multiple time steps. Whilst differential privacy’s guarantees are a mathematical statement
that hold regardless of the process that generated the data, the semantics of these guarantees are often
misinterpreted when it is applied. This leads to misalignment between what one hopes to keep private and
what is actually kept private. We illustrate this with an example.
Example 4. Consider a highly contagious but long recovery flu spreading in a tight-knit community of
N∗individuals who live in the same location. The dataset is Dt= (Xt,i)i∈LtwhereLt⊆[N∗]denotes a
subset of individuals sampled for their flu status Xt,i∈{0,1}. As the individuals live in the same location,
their interaction graph is a fully connected graph. The goal is to release the number of infected individuals
qt=/summationtext
i∈LtXt,iat timetwhilst still preventing an adversary from detecting whether a particular individual,
say Alice, has the flu at that point in time. If the underlying data generating process is ignored, the statistic qt
has sensitivity 1 and ˜qt=qt+Lap(1/ϵ), whereLap(1/ϵ)is the noise generated from the Laplace mechanism, is
anϵ-differentially private statistic. However, given the flu’s characteristics and the fully connected interaction
graph, we can say with high probability either all individuals or no individuals are infected at any time t.
6Published in Transactions on Machine Learning Research (12/2024)
Thus, an adversary could guess from the value of ˜qtwhich of these two scenarios is the case, thereby also
recovering Alice’s flu status with high probability.
It would be natural to hope that an individual’s flu status could be protected under differential privacy but
Example 4 illustrates that this may not be possible if the data is correlated and the adversary has additional
information about the problem. Solutions like Group Differential Privacy (Dwork & Roth, 2014), which adds
noise proportional to the largest connected component in a graph, and Dependent Differential Privacy (Liu
et al., 2016), which adds noise proportional to the amount of correlation in the data, exist but the amount
of additional noise that is required typically destroys utility. So what can differential privacy protect in
population processes in general and in special scenarios like Example 4? The answer, as we shall see, is that
differential privacy confers plausible deniability on an individual’s participation or presence in a dataset
sampled from the underlying population, but not the individual’s actual status, which can sometimes be
inferred. Example 4 highlights the need for privacy researchers to be explicit about the data generating
process, the adversary’s assumptions, and what information one wishes to protect. This can be done using
Pufferfish privacy and this is how we will make explicit the guarantees provided by differential privacy in our
problem setting.
Given the data-generation model described in § 3.1, let D1:Tbe the sequence of sampled datasets, where
Dt= (Xt,i)i∈Ltdenote the dataset produced at time t. Given a subset Sof[T], we first define the boolean
variableσ(i,S)that indicates whether an individual i’s data is present in Dtfor eacht∈Sand not anywhere
else:
σ(i,S):=/parenleftbigg/logicalanddisplay
t∈Si∈Lt/parenrightbigg
∧/parenleftbigg/logicalanddisplay
t∈[T]\Si /∈Lt/parenrightbigg
.
We then define the secret pairs Qas follows, which state, in extensional form, that for each individual i, it is
indistinguishable whether the person’s status exists in a subset of the sampled datasets.
S:=/uniondisplay
i∈[N∗]/uniondisplay
S⊆[T]:|S|≥1{σ(i,S)}
Q:=/uniondisplay
i∈[N∗]/uniondisplay
S⊆[T]:|S|≥1/uniondisplay
R⊆S:|R|≥1{(σ(i,S),σ(i,S\R)}
The additional element to specify in the Pufferfish framework is the data generating processes Θ, representing
the possible ways an attacker believes the data could have been generated. We define each θ∈Θto be a
parameterization of the form θ:={E,µt,1,...,µt,N∗}t=1...T, where Erepresents the underlying stochastic
population process and is a distribution over the graph and status sequences, and µt,iis the probability that
i∈Lt. We assume the attacker also has a distribution over the agent’s policy that is integrated out in E.
Thus, we have
P(D1:T|θ) =/summationdisplay
G1:T/summationdisplay
X1:TE(G1:T,X1:T)T/productdisplay
t=1P(Dt|Xt,θ), (3)
where
P(Dt|Xt,θ) =/productdisplay
i∈Ltµt,i/productdisplay
j̸=Lt(1−µt,j). (4)
We do not place any restriction on E, which models the attacker’s prior belief over the likely sequence of
interactions between individuals in the population and the attacker’s knowledge about the dynamics model of
the underlying population process.
The following result states the equivalence between (ϵ,δ)-Pufferfish privacy with parameters (S,Q,Θ)and
(ϵ,δ)-differential privacy under T-fold adaptive composition (Dwork et al., 2010; Dwork & Roth, 2014). The
full proof is given in Appendix A.
Lemma 1. A family of mechanisms Fsatisfies (ϵ,δ)-differential privacy under T-fold adaptive composition
iff every sequence of mechanisms M= (M1,...,MT), withMi∈F, satisfies (ϵ,δ)-Pufferfish privacy with
parameters (S,Q,Θ).
7Published in Transactions on Machine Learning Research (12/2024)
Algorithm 1 Differentially Private Reinforcement Learning
1:Input:Environment M= (S,A,r,P,γ ), RL algorithm RL:S×A×R×S× Π→Π, Privacy Mechanism
M:S×R→S, initial state s0∈S.
2:Parameters Privacy parameter ϵ′, number of interactions T.
3:Randomly initialize policy π0.
4:˜s0=Mϵ′(s0).
5:fort= 0,1,2,...,T−1do
6: ˜at∼πt(·|˜st).
7:Receivest+1∼P(·|st,˜at).
8: ˜st+1=Mϵ′(st+1).
9: ˜rt=r(˜st+1,˜at).
10:πt+1←RL((˜st,˜at,˜rt,˜st+1),πt).
11:end for
4 Differentially Private Reinforcement Learning
Our solution for differentially private reinforcement learning is presented in Algorithm 1. It is a meta algorithm
that takes as input an MDP environment M, a reinforcement learning algorithm RL, a privacy mechanism M,
and parameters (ϵ,δ)andTthat specify the level of privacy that should hold over Tinteractions between the
agent and the environment. The RL algorithm can be any method that takes a transition sample (s,a,r′,s′)
and a policy πoldand outputs a new policy πnew=RL((s,a,r′,s′),πold). (Appendix C describes a concrete
instantiation of Algorithm 1 using DQN as the RL algorithm.)
Our approach is to privatise the inputs before the RL algorithm receives them. We begin by first showing
that Algorithm 1 satisfies the privacy guarantees specified in § 3.2. We then characterize the resulting control
problem under our privacy approach before providing a utility bound.
4.1 Privacy Analysis
Algorithm 1 is constructed using differential privacy tools to satisfy (ϵ,δ)-differential privacy under T-fold
adaptive composition. Since an individual’s data is used at each time tto output a state st, we need to
privatisestand ensure that all functions that take stas input are also differentially private. Algorithm 1
does this by privatising every state using an ϵ′-differentially private mechanism (lines 4 and 8). For instance,
the Laplace mechanism with scale parameter 2/Nϵ′(since ∆q= 2/N) would satisfy ϵ′-differential privacy. As
the state space is discrete and bounded, we also need to project the output from the Laplace mechanism back
to the closest element in the state space. (See Algorithm 2 for more details.) The projection operation is
guaranteed to be differentially private by Lemma 2.
Lemma 2 (Post-processing (Dwork & Roth, 2014)) .LetM:Xn→Zbe an (ϵ,δ)-differentially private
mechanism and f:Z→Yan arbitrary function. Then the composition f◦Mis(ϵ,δ)-differentially private.
In Algorithm 1, the action ˜atis selected using the current policy πtwith the privatized state ˜stas input at
every time step (line 6). Once the next state st+1is sampled, it is immediately privatized to ˜st+1(lines 7 and
8). The agent then receives reward ˜rt=r(˜st+1,˜at). Since the action ˜atand received reward ˜rtare functions
of the privatized state, they are also guaranteed private by Lemma 2. Thus, the entire transition sample
received by the RL algorithm is ϵ′-differentially private.
Lemma 3now states the cumulative privacy guarantee over Tinteractions.
Lemma 3 (T-fold adaptive composition (Dwork et al., 2010; Dwork & Roth, 2014)) .For allϵ′,δ′,δ≥0, the
class of (ϵ′,δ′)-differentially private mechanisms satisfies (ϵ,Tδ′+δ)-differential privacy under T-fold adaptive
composition for:
ϵ=/radicalbig
2Tlog(1/δ)ϵ′+Tϵ′(eϵ′−1). (5)
Combining Lemma 1, Lemma 2 and Lemma 3 then yields the following result.
8Published in Transactions on Machine Learning Research (12/2024)
Theorem 1. Algorithm 1 satisfies (ϵ,δ)-differential privacy under T-fold adaptive composition, and equiva-
lently (ϵ,δ)-Pufferfish privacy with parameters (S,Q,Θ)as defined in § 3.2.
Truthfulness in Data Collection A related question to privacy protection is an individual’s willingness
to honestly disclose her data to the data curator. For example, in the context of epidemic control, each
individualimay have a utility function ui:S→[0,1]mapping states to a positive real number, with states
that represent high infection rates assigned lower values because they are, perhaps, more likely to attract
a mandated lock-down of the community. Would an individual who is sampled for data collection gain an
advantage by misreporting her true infection status?
Definition 4. Given a (randomized) mechanism M:S→S, truthful reporting is an ϵ-approximate dominant
strategy for individual iwith utility uiand status xiif, for every dataset Dand everyyi̸=xi,
Eo∼M(q(D∪{xi}))ui(o)≥Eo∼M(q(D∪{yi}))ui(o)−ϵ.
If truth reporting is an ϵ-dominant strategy for every individual in the population, we say Misϵ-approximately
dominant strategy truthful.
In McSherry & Talwar (2007), the authors show that any (ϵ,0)-differentially private mechanism is ϵ-
approximately dominant strategy truthful, which we can see by noting that
Eo∼M(q(D∪{xi}))ui(o) =/summationdisplay
oui(o)P(M(q(D∪{xi})) = 0)≥/summationdisplay
oui(o)e−ϵP(M(q(D∪{yi})) = 0)
=e−ϵEo∼M(q(D∪{yi}))ui(o)≥Eo∼M(q(D∪{yi}))ui(o)−ϵ.
The first inequality follows from Definition 1. The second inequality follows by noting that for ϵ<1,e−ϵ≥1−ϵ.
Note that the argument does not work for (ϵ,δ)-differentially private mechanisms where δ>0. Algorithm 1
uses only an (ϵ,0)differentially private mechanism in lines 4 and 8.
4.2 Utility Analysis
We now analyze the utility of our DPRL approach and present a theoretical result that bounds the approxi-
mation error of the optimal value function under privacy from the true optimal value function. The analysis
we provide is asymptotic in nature and serves as an important step in establishing that good solutions are
possible in our problem setting.
Whilst our approach to privacy in Algorithm 1 makes it easy to guarantee the differential privacy of any
downstream RL algorithm, the learning and control problems are made more difficult as the true underlying
process is now unobservable to the agent. Figure 2 visualizes the graphical model under our approach and
highlights that the state, privatized states and actions evolve according to a partially-observable markov
decision process (POMDP).
Unobservable :s0 s1 ··· sT
Observable : ˜a0 ˜a1··· ˜aT−1
˜s0 ˜s1 ··· ˜sT
Figure 2: A graphical model of the underlying state and action sequence under our differentially private
reinforcement learning approach. The true states are unobservable.
9Published in Transactions on Machine Learning Research (12/2024)
One subtle difference between a POMDP and our privatized system however is that the observed reward is
not a direct function of the underlying state as that would constitute a privacy leak. The typical methods for
solving POMDP problems resort to computationally expensive state-estimation techniques or sampling based
approximations (Monahan, 1982; Shani et al., 2013; Kurniawati, 2022). Instead, we analyse the approximation
error when standard MDP RL algorithms are applied directly on the observed privatized states without
resorting to state-estimation. The analysis is done under several assumptions.
Assumption 1 (Ergodicity) .The underlying MDP environment is ergodic (Puterman, 2014). An ergodic
MDP ensures that a stationary distribution over states is well defined under any stationary policy.
Assumption 1 is a necessary tool when analyzing the asymptotic performance of RL algorithms and it is
commonly used to ensure that every state-action pair is visited infinitely often (see e.g. Singh et al. (1994)).
Assumption 2 (Lipschitz dynamics) .For alls,s′∈S, a∈A, there exists L>0such that
∥P(·|s,a)−P(·|s′,a)∥1≤L∥s−s′∥1.
Since the privatized environment evolves as a POMDP, the distribution for the privatized state ˜stwill in
general depend on the entire history of observed states, actions and rewards. However, when an MDP RL
algorithm is directly applied on top of privatized states, the transitions between privatized states are assumed
to be Markovian. The induced transition model will however depend on the asymptotic state distribution
under a behaviour policy generating interactions. For our analysis, we consider an off-policy setting where a
stationary behaviour policy πgenerates a sequence of privatized states, actions, and rewards. The induced
Markovian transition model ˜Pπ:S×A→D (S)describes the asymptotic transition probabilities and is
given by
˜Pπ(˜st+1|˜st,˜at) =/summationdisplay
st∈Sνπ(st|˜st,˜at)/summationdisplay
st+1∈SP(st+1|st,˜at)PM(˜st+1|st+1). (6)
HerePM(˜s|s) =P(M(s) =˜s)denotesthedistributionofthestateprivatizationmechanismand P(st+1|st,at)
denotes the transition matrix of the underlying MDP. The transition model ˜Pπdepends upon the behaviour
policyπthrough the distribution νπ(st|˜st,˜at), which is the asymptotic probability of the underlying state
beingstunderπwhen ˜stis observed and ˜atis performed. Using Bayes theorem, it can be expressed as
νπ(s|˜s,˜a) =PM(˜s|s)νπ(s|˜a)/summationtext
s′∈SPM(˜s|s′)νπ(s′|˜a)
=PM(˜s|s)νπ(s|˜a)
˜νπ(˜s|˜a), (7)
whereνπ(s|˜a)denotes the asymptotic probability of the underlying state being swhen ˜ais performed, and
˜νπ(˜s|˜a) =/summationtext
s′∈SPM(˜s|s′)νπ(s′|˜a). To avoid degenerate cases, we will assume that the behaviour policy is
stochastic and assigns non-zero probability to every action in all states.
Assumption 3. The behaviour policy πis a stochastic policy where ∀s∈S,∀a∈A,π(a|s)>0.
Under Assumptions 1 and 3, the distributions νπ(s|˜a)and (7) are well defined.
Under our privatisation scheme, the induced MDP on top of privatized states is given by ˜M= (S,A,˜Pπ,r,γ).
Note that the reward function does not change as the received rewards are functions of the privatized
states. For any policy ¯π∈Π, the Q-value ˜Q¯πin˜Msatisfies the Bellman equation ˜Q¯π=r+γ˜Pπ˜V¯πwhere
˜V¯π(s) =Ea∼¯π(·|s)[˜Q¯π(s,a)].
Our analysis is performed using the projected laplace mechanism detailed in Algorithm 2. The projected
laplace mechanism first applies additive noise to the input state as per the standard laplace mechanism. We
require that the privacy mechanism we use has the state space as its output domain. Adding laplace noise no
longer guarantees that the output will be in the state space, and so we take the orthogonal projection back
onto the state space. This operation is guaranteed to be differentially private by Lemma 2.
The projected laplace mechanism satisfies the following tail bound. The proof is provided in Appendix B.
10Published in Transactions on Machine Learning Research (12/2024)
Algorithm 2 Projected Laplace Mechanism
1:Input:states∈S
2:Parameters: Privacy parameter ϵ, sensitivity parameter ∆q.
3:s′=s+η, whereη∈RKand fori∈[K], ηi∼Lap/parenleftig
∆q
ϵ/parenrightig
.
4:˜s= arg min ¯s∈S∥s′−¯s∥2.
5:Return ˜s.
Theorem 2. LetXt= (Xt,i)i∈Ltdenote a dataset and let s=q(Xt)and˜s=M(s), whereMis the Projected
Laplace mechanism. Then for all ϵ>0andα>0,
P/parenleftbigg
∥s−˜s∥∞≥α+1√
2N/parenrightbigg
≤Kexp/parenleftbigg
−Nαϵ
2√
K/parenrightbigg
.
Other privacy mechanisms were also considered but each have their own shortcomings. The exponential
mechanism is a natural choice as it can be configured to output elements of the state space directly but the
exponential mechanism is impractical to sample from. The output domains of additive noise mechanisms
are typically unbounded and need to be modified for our problem setting. Whilst other approaches exist
to bound the output to a particular domain, such as truncating the output (Holohan et al., 2020), we find
working with the orthogonal projection simplifies the analysis. Another reason for working with the projected
laplace mechanism is the fact that it can satisfy ‘pure’ differential privacy (i.e. δ= 0). This is in contrast to
mechanisms like the discrete or continuous gaussian mechanism (Dwork & Roth, 2014; Canonne et al., 2020)
which only satisfy approximate differential privacy (i.e. δ>0). Since we use adaptive composition, satisfying
approximate differential privacy at each time step would lead to the privacy budget increasing linearly with
Twhich is undesirable.
Our main utility result is the following bound on the approximation error between the optimal Q value in the
underlying MDP Mand the privatized MDP ˜M.
Theorem 3. LetMbe the MDP environment and ˜Mdenote the privatised MDP under an ϵ-differentially
private projected laplace mechanism. Let Q∗be the optimal value function in Mand ˜Q∗be the optimal value
function in ˜M. Then,
/vextenddouble/vextenddoubleQ∗−˜Q∗/vextenddouble/vextenddouble
∞≤O/parenleftigg
√
Kexp/parenleftbigg
−ϵ
2√
2K/parenrightbigg
+Kexp/parenleftig
−√
Nϵ/parenrightig
+K3
2√
N/parenrightigg
.
Proof.(Sketch) The full proof is given in Appendix B but we provide a sketch of the main ideas here.
The Simulation Lemma (Kearns & Singh, 2002) can be used to reduce the problem of bounding the Q-value
error to the L1error∥Psa−˜Pπ
sa∥1between the transition models. Expanding the definition of ˜Pπ
sathen allows
us to split into two terms:
∥Psa−˜Pπ
sa∥1≤/summationdisplay
s1∈Sνπ(s1|s,a)
/vextenddouble/vextenddoublePsa−¯Psa/vextenddouble/vextenddouble
1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(one)+/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(two)
,
where ¯P(s′|s1,a) =/summationtext
s2∈SP(s2|s1,a)PM(s′|s2). The first term can be viewed as the error due to privatising
the output state from the transition model and the second term can be viewed as the error due to privatising
the input state to the transition model.
The first term is bound by first applying the Bretagnolle-Huber inequality. The KL divergence between Psa
and ¯Psacan be bound by noting that ¯Psais a convolution between the privacy mechanism and the true
transition model. The sum in the convolution can be reduced to a single element, leading to the bound:
∥Psa−¯Pπ
sa∥1≤2/radicalig
1−min
s′PM(s′|s′),
11Published in Transactions on Machine Learning Research (12/2024)
wherePM(s′|s′)denotes the probability of the projected laplace mechanism outputting s′when the underlying
state iss′. We can show PM(s′|s′)≥1−Kexp/parenleftig
−ϵ
2√
2K/parenrightig
, thus yielding a bound on term (one).
The second term is first bound by ˜C/summationtext
s1∈SPM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1, where ˜Cis a constant. For α>0, let
Bα(s):={s′∈S:∥s−s′∥∞<α+1/√
2N}betheℓ∞ballcentredonstate s. Thesumover s1isthensplitinto
Bα(s)anditscomplement Bc
α(s). Thelipschitzpropertyofourtransitionmodelandtheconcentrationproperty
of the projected laplace mechanism then allow us to bound the sum over Bα(s)andBc
α(s)respectively. Term
(two) is then bound as/summationtext
s1∈Sνπ(s1|s,a)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1byO/parenleftig
2Kexp/parenleftig
−Nαϵ
2√
K/parenrightig
+LKα +LK√
2N/parenrightig
. Choosing
α=2√
K√
Nand combining terms then gives the final result.
Theorem 3 highlights how the approximation error scales as the population sample size Nand privacy
parameterϵincrease. For a given K, the approximation error decreases exponentially quickly as ϵincreases
and at a rate of N−1
2asNincreases. Thus, increasing the sampled population size is an important factor in
attaining good quality solutions for RL in population processes. Importantly, there are components in the
upper bound that depend on only one of Norϵ. This implies that both quantities must be increased to drive
the error completely to zero. The bound also highlights that performance will degrade when the number of
statuses of interest, K, increases. This is likely a function of the fact that the state space is discrete and
scales exponentially with the dimension; its possible that formulating the problem with a query whose range
is continuous and as a continuous reinforcement learning problem could avoid this issue. Nevertheless, our
theoretical result shows the scaling behaviour of the approximation error and we corroborate this behaviour
in our experiments.
5 Experiments
We present empirical results that corroborate our theoretical findings on the SEIRS Epidemic Control problem
detailed in § 3. Our experiments on the Epidemic Control problem are representative of the class of population
process environments as alternate problems will primarily differ in their transition dynamics. We encapsulate
this by running experiments that vary the graph structure, population size, and transition parameters.
Table 1: Transition parameters for each experiment.
Experiment β σ γ ρ
1 0.3 0.5 0.143 0.015
2 0.5 0.1 0.15 0.01
3 0.2 0.3 0.1 0.01
We conduct three experiments. Each experiment corresponds to a different set of transition parameters for
the SEIRS epidemic model (see Example 1) and the transition parameters used in each experiment are listed
in Table 1. A graphical representation of the transition parameters and how they govern state transitions is
shown in Figure 1. Every run of each experiment is conducted over T= 2e5steps and the sample size taken
to be 90% of the population size.
In each experiment the Epidemic Control problem is simulated over four large social networks from the
Stanford Large Graph Network Dataset (Leskovec & Krevl, 2014). Table 2 lists the details of the datasets
used, including the number of nodes and edges in each graph. These social networks represent reasonable
models of social interactions in a population. The state space for each of these models is O(NK), and that is
computationally challenging for RL algorithms.
On each graph, the agent has access to 5 actions {Quarantine (i) :i∈ {0,0.25,0.5,0.75,1.0}}, where
Quarantine (i)quarantinesthetop ithpercentofnodesrankedbydegreecentralitybymodifyingtheinteraction
matrix in the way described in § 3. The reward function is given by r(st,at) =−(αI(st) + (1−α)C(at))
12Published in Transactions on Machine Learning Research (12/2024)
Table 2: Summary of network datasets used in experiments.
Dataset Name Nodes Edges
Slashdot (Leskovec et al., 2009) 82K 82,168 948,464
Twitch(Rozemberczki & Sarkar, 2021) 168k 168,114 6,797,557
Gowalla (Cho et al., 2011) 196K 196,591 950,327
Youtube (Yang & Leskovec, 2012) 1.1M 1,134,890 2,987,624
and is taken as a convex combination between two functions I(st)andC(at). The function I(st)returns the
proportion of Exposed and Infected individuals at time tandC(at)returns the proportion of individuals
quarantined by action at. We setα= 0.8in all experiments.
For each graph, we run simulations that vary the population size and the target cumulative privacy budget ϵ
to see how the RL algorithm’s performance changes as these key variables change. For given target cumulative
privacy parameters (ϵ,δ), we set the per-step privacy budget as
ϵ′=f(ϵ,δ) =ϵ
2/radicalbig
2Tlog(1/δ), (8)
andδ′= 0. Under Lemma 3, setting the per-step privacy budget in this manner only satisfies the target value
ofϵfor a certain range of values for δ. As the value of δdecreases, the gap between the privacy achieved and
the target privacy widens. In practice, ϵvalues under 10 are of interest and we find that this can be achieved
whenT= 2e5ifδ≤10−2. For more details see Figure 5, Appendix C.
The RL algorithm we use is the DQN algorithm (Mnih et al., 2015) initialized with an experience replay buffer
(Lin, 1992). We refer to its differentially private version as DP-DQN. DP-DQN can only store privatized
transitions in its replay buffer. The DQN algorithm and the environment interact in an online fashion
and exploration is performed using epsilon-greedy. The projected laplace mechanism was used as the state
privatisation mechanism. A full description of the DP-DQN algorithm, a concrete algorithm for computing
the projected laplace mechanism, and hyperparameters used is provided in Appendix C.
5.1 Results
Figure 3: Private reward received by DP-DQN in Experiment 1 (top row), Experiment 2 (middle row), and
Experiment 3 (bottom row).
13Published in Transactions on Machine Learning Research (12/2024)
Figures 3 and 4 plot the private reward and true reward performance of DP-DQN across all experiments.
In each plot, we compare the performance of DP-DQN under differential privacy as ϵis varied against the
default performance of DP-DQN under no privacy. Each curve displays the mean and standard deviation
over five random seeds. The privacy parameter δwas kept fixed across all runs at δ= 10−5. In all plots, the
blue curve indicates the default performance of DP-DQN without differential privacy. Note that the blue
curve does not change between the corresponding true reward and private reward plots.
The plots in Figure 3 show the performance of DP-DQN increases as ϵincreases. Notably, the performance
clusters close to the default performance under low privacy (i.e. ϵ≥5), and is indistinguishable from the
default performance in the 1.1M graph. Additionally as the population size increases, the overall performance
also improves. This falls in line with intuition as the noise added under a given privacy parameter has
absolute scale and as the population size increases, the relative error due to privacy is much smaller. On the
1.1M graph we additionally plot the performance of DP-DQN under ϵ= 0.1. Thus, the empirical results
demonstrate in a finite sample setting the relationship between performance and the population size and
privacy parameters highlighted by Theorem 3.
Figure 4: True reward received by DP-DQN in Experiment 1 (top row), Experiment 2 (middle row), and
Experiment 3 (bottom row).
Whilst the results in Figure 3 are useful for corroborating our theoretical experiments, it is also important to
investigate how the policy learnt under our privacy setup performs. Figure 4 plots the true reward received by
DP-DQN across different graphs as ϵvaries. Overall, DP-DQN’s performance increases as ϵincreases across
all networks. Additionally, performance under privacy is clustered much closer to the default performance
across allϵvalues in the 82k and 196k graphs than in the private reward case. The plots for the 1.1M
graph show that performance under high privacy is indistinguishable from default performance. The gap in
performance between the high privacy setting (i.e. ϵ∈{0.1,0.5}) and the low privacy setting on the 1.1M
graph suggest that in some cases there may be a threshold value for ϵthat needs to be crossed to obtain
almost optimal performance. Thus, we find that the policy learnt under our privacy setup can perform well
and improves, drastically in some cases, as the privacy budget increases.
6 Limitations and Future Work
One limitation of our theoretical results is that they are asymptotic in nature. This analysis provides
guarantees on the error between the solution under privacy and the true solution without privacy, but does
not provide any guidance on whether such a solution can be learned. Nevertheless, our empirical results
14Published in Transactions on Machine Learning Research (12/2024)
provide some confidence the solutions found by learning algorithms will scale the way our results predict.
Also, understanding the asymptotic properties of a problem setting is an important first step. Constructing
and proving sublinear-regret algorithms in our problem setting is important future work. Additionally, whilst
we have shown experimentally that the policy learnt under privacy can perform well, it would be useful to
bound this performance theoretically and is the subject of further investigation.
Whilst our approach to achieving differential privacy is desirable as it is agnostic to the RL algorithm itself,
it leaves open the possibility that the same privacy guarantees could be achieved by directly modifying an RL
algorithm. Such an approach may be able to better deal with higher dimensional state spaces, which can
adversely impact the approximation error, and is one of the limitations of our approach. Our analysis is also
performed under some regularity assumptions; removing these assumptions would provide more generally
applicable theoretical guarantees and is an interesting topic for future research.
Impact Statement
As reinforcement learning algorithms see wider adoption and begin interacting with humans and their data,
issues around protecting privacy become more pertinent. Our work can be seen as providing a promising
start and solid theoretical foundation to providing privacy protections in an important problem class where
reinforcement learning may be applied in the future.
Acknowledgements
The authors would like to thank Mike Purcell for the many helpful discussions and whiteboard sessions during
the early stages of the project.
References
Daron Acemoglu, Asuman Ozdaglar, and James Siderius. A model of online misinformation. Review of
Economic Studies , pp. rdad111, 2023.
Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms.
URL https://rltheorybook.github.io , 2022.
Nour Almadhoun, Erman Ayday, and Özgür Ulusoy. Differential privacy under dependent tuples—the case of
genomic privacy. Bioinformatics , 36(6):1696–1703, 2020.
Mauricio Arango and Lyudmil Pelov. COVID-19 pandemic cyclic lockdown optimization using reinforcement
learning. CoRR, abs/2009.04647, 2020. URL https://arxiv.org/abs/2009.04647 .
Dan Ariely. Misbelief: What Makes Rational People Believe Irrational Things . Heligo Books, 2023.
Achraf Azize and Debabrota Basu. When privacy meets partial information: A refined analysis of differentially
private bandits. Advances in Neural Information Processing Systems , 35:32199–32210, 2022.
Borja Balle, Maziar Gomrokchi, and Doina Precup. Differentially private policy evaluation, 2016. URL
https://arxiv.org/abs/1603.02010 .
Konstantin Berestizshevsky, Koffi-Emmanuel Sadzi, Guy Even, and Moni Shahar. Optimization of resource-
constrained policies for covid-19 testing and quarantining. Journal of Communications and Networks , 23
(5):326–339, 2021. doi: 10.23919/JCN.2021.000029.
Jean Bretagnolle and Catherine Huber. Estimation des densités: risque minimax. Zeitschrift für Wahrschein-
lichkeitstheorie und verwandte Gebiete , 47:119–137, 1979.
Ceren Budak, Divyakant Agrawal, and Amr El Abbadi. Limiting the spread of misinformation in social
networks. In Proceedings of the 20th international conference on World wide web , pp. 665–674, 2011.
15Published in Transactions on Machine Learning Research (12/2024)
Clément L Canonne. A short note on an inequality between KL and TV. arXiv preprint arXiv:2202.07198 ,
2023.
Clément L Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential privacy.
Advances in Neural Information Processing Systems , 33:15676–15688, 2020.
Arthur Charpentier, Romuald Elie, Mathieu Laurière, and Viet Chi Tran. Covid-19 pandemic control:
balancing detection policy and lockdown intervention under ICU sustainability. Mathematical Modelling of
Natural Phenomena , 15:57, 2020.
Dawei Chen, Samuel Yang-Zhao, John Lloyd, and Kee Siong Ng. Factored conditional filtering: Tracking
states and estimating parameters in high-dimensional spaces. arXiv:2206.02178 , 2022.
Eunjoon Cho, Seth A Myers, and Jure Leskovec. Friendship and mobility: user movement in location-based
social networks. In Proceedings of the 17th ACM SIGKDD , pp. 1082–1090, 2011.
Sayak Ray Chowdhury and Xingyu Zhou. Differentially private regret minimization in episodic markov
decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pp.
6375–6383, 2022a.
Sayak Ray Chowdhury and Xingyu Zhou. Distributed differential privacy in multi-armed bandits. arXiv
preprint arXiv:2206.05772 , 2022b.
Cédric Colas, Boris Hejblum, Sébastien Rouillon, Rodolphe Thiébaut, Pierre-Yves Oudeyer, Clément Moulin-
Frier, and Mélanie Prague. EpidemiOptim: A toolbox for the optimization of control policies in epidemio-
logical models, 2020. URL https://arxiv.org/abs/2010.04452 .
Michela Del Vicario, Alessandro Bessi, Fabiana Zollo, Fabio Petroni, Antonio Scala, Guido Caldarelli,
H Eugene Stanley, and Walter Quattrociocchi. The spreading of misinformation online. Proceedings of the
National Academy of Sciences , 113(3):554–559, 2016.
Christos Dimitrakakis, Blaine Nelson, Zuhe Zhang, Aikaterini Mitrokotsa, and Benjamin IP Rubinstein.
Differential privacy for Bayesian inference through posterior sampling. Journal of Machine Learning
Research , 18(11):1–39, 2017.
Abhimanyu Dubey and Alex Pentland. Differentially-private federated linear bandits. Advances in Neural
Information Processing Systems , 33:6003–6014, 2020.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and
Trends ®in Theoretical Computer Science , 9(3–4):211–407, 2014.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In 3rd Theory of Cryptography Conference , pp. 265–284. Springer, 2006.
Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In 51st Annual
Symposium on Foundations of Computer Science , pp. 51–60. IEEE, 2010.
Amalie Dyda, Michael Purcell, Stephanie Curtis, Emma Field, Priyanka Pillai, Kieran Ricardo, Haotian
Weng, Jessica C Moore, Michael Hewett, Graham Williams, and Colleen Lau. Differential privacy for
public health data: An innovative tool to optimize information sharing while protecting data confidentiality.
Patterns, 2(12), 2021.
Evrard Garcelon, Vianney Perchet, Ciara Pike-Burke, and Matteo Pirotta. Local differential privacy for
regret minimization in reinforcement learning, 2020. URL https://arxiv.org/abs/2010.07778 .
Abhradeep Guha Thakurta and Adam Smith. (Nearly) optimal algorithms for private online learning in
full-information and bandit settings. Advances in Neural Information Processing Systems , 26, 2013.
Naoise Holohan, Spiros Antonatos, Stefano Braghin, and Pól Mac Aonghusa. The bounded laplace mechanism
in differential privacy. Journal of Privacy and Confidentiality , 10:1, 2020.
16Published in Transactions on Machine Learning Research (12/2024)
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine
learning, 49:209–232, 2002.
Daniel Kifer and Ashwin Machanavajjhala. No free lunch in data privacy. In ACM International Conference
on Management of Data (SIGMOD) , pp. 193–204. ACM, 2011.
Daniel Kifer and Ashwin Machanavajjhala. Pufferfish: A framework for mathematical privacy definitions.
ACM Transactions on Database Systems (TODS) , 39(1):1–36, 2014.
Varun Kompella, Roberto Capobianco, Stacy Jong, Jonathan Browne, Spencer Fox, Lauren Meyers, Peter
Wurman, and Peter Stone. Reinforcement learning for optimization of covid-19 mitigation policies, 2020.
URL https://arxiv.org/abs/2010.10560 .
Hanna Kurniawati. Partially observable markov decision processes and robotics. Annual Review of Control,
Robotics, and Autonomous Systems , 5:253–277, 2022.
Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:
//snap.stanford.edu/data , June 2014.
Jure Leskovec, Kevin J Lang, Anirban Dasgupta, and Michael W Mahoney. Community structure in large
networks: Natural cluster sizes and the absence of large well-defined clusters. Internet Mathematics , 6(1):
29–123, 2009.
Li Li, Jufu Cui, Rui Zhang, Hui Xia, and Xiangguo Cheng. Dynamics of complex networks: Malware
propagation modeling and analysis in industrial internet of things. IEEE Access , 8:64184–64192, 2020.
Yang Li, Michael Purcell, Thierry Rakotoarivelo, David Smith, Thilina Ranbaduge, and Kee Siong Ng.
Private graph data release: A survey. ACM Computing Surveys , 55(11):1–39, 2023.
Chonghua Liao, Jiafan He, and Quanquan Gu. Locally differentially private reinforcement learning for linear
mixture markov decision processes. In Asian Conference on Machine Learning , pp. 627–642. PMLR, 2023.
Long-Ji Lin. Reinforcement learning for robots using neural networks . Carnegie Mellon University, 1992.
Changchang Liu, Supriyo Chakraborty, and Prateek Mittal. Dependence makes you vulnerable: Differential
privacy under dependent tuples. In Network and Distributed System Security Symposium , pp. 21–24. The
Internet Society, 2016.
Paul Luyo, Evrard Garcelon, Alessandro Lazaric, and Matteo Pirotta. Differentially private exploration in
reinforcement learning with linear representation. arXiv preprint arXiv:2112.01585 , 2021.
Hongzi Mao, Shannon Chen, Drew Dimmery, Shaun Singh, Drew Blaisdell, Yuandong Tian, Mohammad
Alizadeh, and Eytan Bakshy. Real-world video adaptation with reinforcement learning. arXiv preprint
arXiv:2008.12858 , 2020.
Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual IEEE
Symposium on Foundations of Computer Science , pp. 94–103. IEEE, 2007.
Nikita Mishra and Abhradeep Thakurta. (Nearly) optimal differentially private stochastic multi-arm bandits.
InProceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence , pp. 592–601, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–533, 2015.
George E Monahan. State of the art – A survey of partially observable markov decision processes: theory,
models, and algorithms. Management science , 28(1):1–16, 1982.
Senthilkumar Muthukrishnan, Sumathi Muthukumar, and Veeramani Chinnadurai. Optimal control of
malware spreading model with tracing and patching in wireless sensor networks. Wireless Personal
Communications , 117:2061–2083, 2021.
17Published in Transactions on Machine Learning Research (12/2024)
Mark Newman. Networks . Oxford University Press, 2018.
Dung Daniel T Ngo, Giuseppe Vietri, and Steven Wu. Improved regret for differentially private exploration
in linear mdp. In International Conference on Machine Learning , pp. 16529–16552. PMLR, 2022.
Nam P Nguyen, Guanhua Yan, My T Thai, and Stephan Eidenbenz. Containment of misinformation spread
in online social networks. In Proceedings of the 4th Annual ACM Web Science Conference , pp. 213–222,
2012.
Cameron Nowzari, Victor M Preciado, and George J Pappas. Analysis and control of epidemics: A survey of
spreading processes on complex networks. IEEE Control Systems Magazine , 36(1):26–46, 2016.
Xinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, and Dawn Song. How you act tells a lot:
Privacy-leaking attack on deep reinforcement learning. In Proceedings of the 18th International Conference
on Autonomous Agents and MultiAgent Systems , pp. 368–376. IFAAMAS, 2019.
Romualdo Pastor-Satorras, Claudio Castellano, Piet Van Mieghem, and Alessandro Vespignani. Epidemic
processes in complex networks. Reviews of Modern Physics , 87(3), 2015.
Sancheng Peng, Shui Yu, and Aimin Yang. Smartphone malware and its propagation modeling: A survey.
IEEE Communications Surveys & Tutorials , 16(2):925–941, 2013.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Dan Qiao and Yu-Xiang Wang. Near-optimal differentially private reinforcement learning. In Francisco Ruiz,
Jennifer Dy, and Jan-Willem van de Meent (eds.), Proceedings of The 26th International Conference on
Artificial Intelligence and Statistics , volume 206, pp. 9914–9940. PMLR, 25–27 Apr 2023.
Wenbo Ren, Xingyu Zhou, Jia Liu, and Ness B Shroff. Multi-armed bandits with local differential privacy.
arXiv preprint arXiv:2007.03121 , 2020.
Benedek Rozemberczki and Rik Sarkar. Twitch gamers: a dataset for evaluating proximity preserving and
structural role-based node embeddings, 2021.
Touqir Sajed and Or Sheffet. An optimal private stochastic-mab algorithm based on optimal private stopping
rule. InInternational Conference on Machine Learning , pp. 5579–5588. PMLR, 2019.
Shai Shalev-Shwartz, Yoram Singer, Kristin P Bennett, and Emilio Parrado-Hernández. Efficient learning of
label ranking by soft projections onto polyhedra. Journal of Machine Learning Research , 7(7), 2006.
Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based POMDP solvers. Autonomous Agents
and Multi-Agent Systems , 27:1–51, 2013.
Roshan Shariff and Or Sheffet. Differentially private contextual linear bandits, 2018. URL https://arxiv.
org/abs/1810.00068 .
Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Learning without state-estimation in partially
observable markovian decision processes. In Machine Learning Proceedings , pp. 284–292. Elsevier, 1994.
Mudhakar Srivatsa and Mike Hicks. Deanonymizing mobility traces: Using social network as a side-channel.
InACM conference on Computer and Communications Security (CCS) , pp. 628–637. ACM, 2012.
Aristide Tossou and Christos Dimitrakakis. Algorithms for differentially private multi-armed bandits. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 30, 2016.
Aristide Tossou and Christos Dimitrakakis. Achieving privacy in the adversarial multi-armed bandit. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 31, 2017.
Sander Van Der Linden. Misinformation: Susceptibility, spread, and interventions to immunize the public.
Nature Medicine , 28(3):460–467, 2022.
18Published in Transactions on Machine Learning Research (12/2024)
Giuseppe Vietri, Borja Balle, Akshay Krishnamurthy, and Zhiwei Steven Wu. Private reinforcement learning
with PAC and regret guarantees, 2020. URL https://arxiv.org/abs/2009.09052 .
Baoxiang Wang and Nidhi Hegde. Privacy-preserving q-learning with functional noise in continuous spaces.
Advances in Neural Information Processing Systems , 32, 2019.
Haoran Wang and Shi Yu. Robo-advising: Enhancing investment with inverse optimization and deep
reinforcement learning. In 20th IEEE International Conference on Machine Learning and Applications
(ICMLA) , pp. 365–372. IEEE, 2021.
Bin Yang, Issei Sato, and Hiroshi Nakagawa. Bayesian differential privacy on correlated data. In ACM
International Conference on Management of Data (SIGMOD) , pp. 747–762. ACM, 2015.
Jaewon Yang and Jure Leskovec. Defining and evaluating network communities based on ground-truth. In
Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics , pp. 1–8, 2012.
Shui Yu, Guofei Gu, Ahmed Barnawi, Song Guo, and Ivan Stojmenovic. Malware propagation in large-scale
networks. IEEE Transactions on Knowledge and data engineering , 27(1):170–179, 2014.
Zhenhua Yu, Hongxia Gao, Dan Wang, Abeer Ali Alnuaim, Muhammad Firdausi, and Almetwally M Mostafa.
SEI2RS malware propagation model considering two infection rates in cyber–physical systems. Physica A:
Statistical Mechanics and its Applications , 597:127207, 2022.
Jun Zhao, Junshan Zhang, and H Vincent Poor. Dependent differential privacy for correlated data. In IEEE
Globecom Workshops , pp. 1–7. IEEE, 2017.
Kai Zheng, Tianle Cai, Weiran Huang, Zhenguo Li, and Liwei Wang. Locally differentially private (contextual)
bandits learning. Advances in Neural Information Processing Systems , 33:12300–12310, 2020.
Xingyu Zhou. Differentially private reinforcement learning with linear function approximation. Proceedings
of the ACM on Measurement and Analysis of Computing Systems , 6(1):1–27, 2022.
19Published in Transactions on Machine Learning Research (12/2024)
Appendix
A Proof of Lemma 1
Lemma 1. A family of mechanisms Fsatisfies (ϵ,δ)-differential privacy under T-fold adaptive composition
iff every sequence of mechanisms M= (M1,...,MT), withMi∈F, satisfies (ϵ,δ)-Pufferfish privacy with
parameters (S,Q,Θ)as defined in Section 3.2.
Proof.We first show T-fold adaptive composition implies (ϵ,δ)-Pufferfish privacy.
SupposeM= (M1,...,MT)satisfies (ϵ,δ)differential privacy under T-fold adaptive composition. Let D1:T
be a random variable denoting the sequence of databases. Given an arbitrary θ, secretσi,S, we have for each
non-empty R⊆S,
P(M(D1:T) =y1:T|σ(i,S),θ)
=/summationdisplay
G1:T/summationdisplay
X1:T/summationdisplay
D1:T|σ(i,S)P(G1:T,X1:T|θ)P(D1:T=D1:T|G1:T,X1:T,σ(i,S),θ)P(M(D1:T) =y1:T)
≤δ+eϵ/summationdisplay
G1:T/summationdisplay
X1:T/summationdisplay
D1:T|σ(i,S)P(G1:T,X1:T|θ)P(D1:T=D1:T|G1:T,X1:T,σ(i,S),θ)P(M(D′
1:T) =y1:T),(9)
whereD′
1:Tis obtained from D1:Tby removing individual i’s data from Dt,t∈R. The inequality (9) follows
from the property of T-fold adaptive composition:
max
ω:P(M(D1:T)=ω)≥δlnP(M(D1:T) =ω)−δ
P(M(D′
1:T) =ω)≤ϵ. (10)
Given aD1:Tsatisfyingσ(i,S), we have
P(D1:T=D1:T|G1:T,X1:T,σ(i,S),θ) =/bracketleftbigg/productdisplay
t∈SP(Dt=Dt|Xt,i∈Lt,θ)/bracketrightbigg/bracketleftbigg/productdisplay
t∈T\SP(Dt=Dt|Xt,i /∈Lt,θ)/bracketrightbigg
.
GivenD′
1:Tis obtained from D1:Tas described above, it satisfies σ(i,S\R)and we have
P(D1:T=D′
1:T|G1:T,X1:T,σ(i,S\R),θ) =/bracketleftbigg/productdisplay
t∈RP(Dt=D′
t|Xt,i̸=Lt,θ)/bracketrightbigg/bracketleftbigg/productdisplay
t∈S\RP(Dt=Dt|Xt,i∈Lt,θ)/bracketrightbigg/bracketleftbigg/productdisplay
t∈T\SP(Dt=Dt|Xt,i /∈Lt,θ)/bracketrightbigg
.
For eacht∈R, we have
P(Dt=Dt|Xt,i∈Lt,θ) =P(Dt=D′
t|Xt,i /∈Lt,θ) (11)
since, by Equation (4), both the LHS and RHS of (11) are equal to
/productdisplay
j∈Lt\{i}µt,j/productdisplay
j∈[N∗]\Lt}(1−µt,j).
We have thus established that
P(D1:T=D1:T|G1:T,X1:T,σ(i,S),θ) =P(D1:T=D′
1:T|G1:T,X1:T,σ(i,S\R),θ). (12)
Substituting Equation (12) into (9), and noting that the innermost summation/summationtext
D1:T|σ(i,S)in (9) can be
rewritten in the equivalent form/summationtext
D′
1:T|σ(i,S\R), allows us to claim
P(M(D1:T) =y1:T|σ(i,S),θ)≤δ+eϵP(M(D1:T) =y1:T|σ(i,S\R),θ).
20Published in Transactions on Machine Learning Research (12/2024)
Givenθ∈Θ,i∈[N∗],S⊆[T]andR⊆Sare all arbitrary, we have shown Msatisfies (ϵ,δ)-Pufferfish privacy
with parameters (S,Q,Θ).
We next show (ϵ,δ)-Pufferfish privacy implies T-fold adaptive composition.
SupposeM= (M1,...,MT)satisfies (ϵ,δ)Pufferfish privacy with parameters (S,Q,Θ). We show for any
pair of neighbouring databases there is a θ∈Θthat preserves differential privacy.
LetD1:Tbe an arbitrary sequence of datasets. For each individual iinD1:T, letD′
1:Tbe obtained from D1:T
by removing individual i’s data from one or more of the datasets. Thus, there exists SandR⊆Ssuch that
D1:Tsatisfiesσ(i,S)andD′
1:Tsatisfiesσ(i,S\R).
We choose θ={(E,µt,1,...,µt,N∗}t=1..Tto be the following. Ecan be any stochastic population process.
For eacht∈S, defineµt,i= 1/2, andµt,j= 1for eachj∈Lt\Sandµt,k= 0for eachk /∈Lt. And for each
t /∈S, defineµt,j= 1for eachj∈Ltandµt,k= 0for eachk /∈Lt. GivenMsatisfies (ϵ,δ)Pufferfish privacy,
we have for any y1:T:
P(M(D1:T) =y1:T|σ(i,S),θ)≤eϵP(M(D1:T) =y1:T|σ(i,S\R),θ) +δ (13)
⇔P(M(D1:T) =y1:T|σ(i,S),θ)≤eϵP(M(D′
1:T) =y1:T|σ(i,S\R),θ) +δ (14)
⇔P(M(D1:T) =y1:T|θ)≤eϵP(M(D′
1:T) =y1:T|θ) +δ (15)
Step (14) follows because of all the dataset sequences that can be generated using θ, onlyD1:Tsatisfiesσ(i,S)
and onlyD′
1:Tsatisfiesσ(i,S\R). Step (15) follows because P(σ(i,S)|θ) =P(σ(i,S\R)|θ) = (1/2)|S|.
Choosingθ∈Θin this manner for all neighbouring database sequences and repeating the calculations proves
the final result.
In the proof of Lemma 1, we have only considered neighbouring datasets where an individual’s data is dropped.
The case when a neighbouring dataset D′is obtained from a dataset Dby replacing an individual i’s data
with another individual j’s data, where jis not inD, can be formulated using the following secrets and secret
pairs:
σ(i,S):=/parenleftbigg/logicalanddisplay
t∈Si∈Lt/parenrightbigg
∧/parenleftbigg/logicalanddisplay
t∈[T]\Si /∈Lt/parenrightbigg
σ(i,S(W)):=/parenleftbigg/logicalanddisplay
t∈S\W|1i∈Lt/parenrightbigg
∧/parenleftbigg/logicalanddisplay
t∈[T]\(S\W|1)i /∈Lt/parenrightbigg
∧/parenleftbigg/logicalanddisplay
(t,j)∈Wj∈Lt/parenrightbigg
S′:=/uniondisplay
i∈[N∗]/uniondisplay
S⊆[T]:|S|≥1{σ(i,S)}
Q′:=/uniondisplay
i∈[N∗]/uniondisplay
S⊆[T]:|S|≥1/uniondisplay
W={(t,j) :t∈R,j∈[N∗]\S}
R⊆S∧|R|≥1{(σ(i,S),σ(i,S(W))}
In the above, given W={(t,x)}, we define W|1={t:∃x.(t,x)∈W}. Using very similar arguments, one
can show that (ϵ,δ)differential privacy under T-fold adaptive composition is equivalent to (ϵ,δ)Pufferfish
privacy with parameters (S′,Q′,Θ).
For each of the secret pairs (s1,s2)∈Q′, the Pufferfish privacy guarantees that
e−ϵ≤P(s1|M(D1:T) =ω,θ)
P(s2|M(D1:T) =ω,θ)/slashbiggP(s1|θ)
P(s2|θ)≤eϵ. (16)
By inspecting the proof of Lemma 1, it is clear that secret pairs that represent an individual having two
different status at a given time cannot be accommodated, and this is consistent with Example 4.
21Published in Transactions on Machine Learning Research (12/2024)
B Utility Analysis Proofs
B.1 Laplace and Projected Laplace Mechanism Properties
The mechanism we consider using is the projected laplace mechanism. Denote by MLthe laplace mechanism
which outputs s′=ML(s) =s+ (Y1,...,YK),Yi∼Lap(∆q/ϵ). The projected laplace mechanism outputs
˜s=M(s)by first applying the laplace mechanism s′=ML(s)and then takes the ℓ2projection back onto
the state space, i.e. ˜s= arg min ˜s∈S∥˜s−s′∥2.
A concentration bound for the Laplace mechanism is given as follows.
Theorem 4 (Dwork & Roth (2014)) .LetXt= (Xt,i)i∈Ltdenote a dataset, s=q(Xt)ands′=ML(s). For
ϵ>0andδ∈(0,1],
P/bracketleftbigg
∥s−s′∥∞≥ln/parenleftbiggK
δ/parenrightbigg
·/parenleftbigg∆q
ϵ/parenrightbigg/bracketrightbigg
≤δ.
The following is a simple corollary of Theorem 4.
Corollary 1. LetXt= (Xt,i)i∈Ltdenote a dataset and let s=q(Xt)ands′=ML(s). Then forϵ>0and
α>0,
P(∥s−s′∥∞≥α)≤Kexp/parenleftbigg
−Nαϵ
2/parenrightbigg
.
We now prove Theorem 2, a concentration bound for the projected laplace mechanism.
Theorem 2. LetXt= (Xt,i)i∈Ltdenote a dataset and let s=q(Xt)and ˜s=M(s), whereMis the
Projected Laplace mechanism. Then for all ϵ>0andα>0,
P/parenleftbigg
∥s−˜s∥∞≥α+1√
2N/parenrightbigg
≤Kexp/parenleftbigg
−Nαϵ
2√
K/parenrightbigg
.
Proof.First, define P∆K(s) =arg mins′∈∆K∥s′−s∥2andPS(s) =arg mins′∈S∥s′−s∥2as theℓ2projections
onto theK−1probability simplex ∆Kand the state space S, noting thatS⊂∆K. Lettings′=ML(s), we
can express ˜sas˜s=M(s) =PS(s′).
Rather than finding an upper bound on P(∥s−˜s∥∞≥α), we instead find a lower bound on P(∥s−˜s∥∞<α).
Note that the ℓ∞ball of radius α(actually a hypercube) has greater volume than the ℓ2ball of radius α.
Thus,
P(∥s−˜s∥∞<α)≥P(∥s−˜s∥2<α). (17)
Now define the following sets:
A:=/braceleftbigg
s′∈RK:∥s−PS(s′)∥2<α+1√
2N/bracerightbigg
B:=/braceleftbig
s′∈RK:∥s−P∆K(s′)∥2<α/bracerightbig
Note thatPS(s′)must be a point that minimizes the ℓ2distance to P∆K(s′), asP∆K(s′)is the minimum
distance to the simplex ∆Kand minimizing the distance to Sthus involves minimizing the distance along
∆KfromP∆K(s′). Also for two neighbouring points s1,s2∈S(i.e.s1̸=s2and closest in ℓ2distance), we
must have two dimensions i̸=jwheres1,i=s2,i−1
Nands1,j=s2,i+1
Nands1,k=s2,kfork̸=i,j. Thus
the minimum distance between two neighbouring points is ∥s1−s2∥2=√
2
NandPS(s′)can be at most1√
2N
away fromP∆K(s′). Then fors′∈B, we have:
∥s−PS(s′)∥2≤∥s−P∆K(s′)∥2+∥P∆K(s′)−PS(s′)∥2
≤α+1√
2N.
22Published in Transactions on Machine Learning Research (12/2024)
Thuss′∈B=⇒s′∈A, implyingB⊆Aand giving us P(A)≥P(B)or equivalently:
P/parenleftbigg
∥s−PS(s′)∥2<α+1√
2N/parenrightbigg
≥P(∥s−P∆K(s′)∥2<α). (18)
Letθ∈[0,π
2]denote the angle between s−s′and the plane ∆K. SinceP∆K(s′)is the orthogonal projection,
we have∥s−s′∥2=∥s−P∆K(s′)∥2cos−1(θ). Then we have:
P(∥s−P∆K(s′)∥2<α) =P/parenleftbig
∥s−s′∥2<αcos−1(θ)/parenrightbig
(a)
≥P/parenleftbigg
∥s−s′∥∞<αcos−1(θ)√
K/parenrightbigg
= 1−P/parenleftbigg
∥s−s′∥∞≥αcos−1(θ)√
K/parenrightbigg
(b)
≥1−Kexp/parenleftbigg
−Nαϵ cos−1(θ)
2√
K/parenrightbigg
, (19)
where (a) follows as {s′∈RK:∥s−s′∥∞≤α/√
K}⊆{s′∈RK:∥s−s′∥2≤α}– the former is a hypercube
with side length 2α/√
Ksitting completely inside the ℓ2ball of radius α– and (b) follows by Corollary 1.
Combining (17), (18) and (19) then gives us:
P/parenleftbigg
∥s−˜s∥∞<α+1√
2N/parenrightbigg
≥1−Kexp/parenleftbigg
−Nαϵ
2√
K/parenrightbigg
,
where we drop cos−1(θ)ascos−1(θ)∈[1,∞]forθ∈[0,π
2]. Thus, we have:
P/parenleftbigg
∥s−˜s∥∞≥α+1√
2N/parenrightbigg
= 1−P/parenleftbigg
∥s−˜s∥∞<α+1√
2N/parenrightbigg
≤Kexp/parenleftbigg
−Nαϵ
2√
K/parenrightbigg
.
B.2 Additional Lemmas
The Simulation Lemma (Kearns & Singh, 2002; Agarwal et al., 2022) lets us bound the value function error
in terms of the error in the transition functions.
Lemma 4 (Simulation Lemma) .LetM= (S,A,r,P,γ )and ˜M= (S,A,r,˜P,γ)be two MDPs that differ
only in the transition model. Given a policy π, letQπbe the value function under πinMand ˜Qπbe the
value function under πin˜M. Then for all π
/vextenddouble/vextenddoubleQπ−˜Qπ/vextenddouble/vextenddouble
∞≤γ
1−γ/vextenddouble/vextenddouble(P−˜P)Vπ/vextenddouble/vextenddouble
∞.
Lemma 5 (Lipschitz preservation under convolution) .Supposefis anL-Lipschitz function and ϕis a
function such that/integraltext
ϕ(x)dx= 1andϕ(x)≥0for allx. Theng=f∗ϕis also anL-Lipschitz function.
Proof.
∥g(z)−g(z′)∥≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
(f(z+x)−f(z′+x))ϕ(x)dx/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤/integraldisplay
∥f(z+x)−f(z′+x)∥ϕ(x)dx
≤L∥z−z′∥/integraldisplay
ϕ(x)dx
=L∥z−z′∥.
23Published in Transactions on Machine Learning Research (12/2024)
B.3 Proof of Theorem 3
Theorem 3. LetMbe the MDP environment and ˜Mdenote the privatised MDP under an ϵ-differentially
private projected laplace mechanism. Let Q∗be the optimal value function in Mand ˜Q∗be the optimal
value function in ˜M. Then,
/vextenddouble/vextenddoubleQ∗−˜Q∗/vextenddouble/vextenddouble
∞≤O/parenleftigg
√
Kexp/parenleftbigg
−ϵ
2√
2K/parenrightbigg
+Kexp/parenleftig
−√
Nϵ/parenrightig
+K3
2√
N/parenrightigg
.
Proof.We have
/vextendsingle/vextendsingleQ∗(s,a)−˜Q∗(s,a)/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsinglesup
πQπ(s,a)−sup
π˜Qπ(s,a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤sup
π/vextendsingle/vextendsingleQπ(s,a)−˜Qπ(s,a)/vextendsingle/vextendsingle
≤sup
π/vextenddouble/vextenddoubleQπ−˜Qπ/vextenddouble/vextenddouble
∞. (20)
Then (20) can be bound with Lemma 4. For any policy ¯πand any behaviour policy πsatisfying Assumption 3,
/vextenddouble/vextenddoubleQ¯π−˜Q¯π/vextenddouble/vextenddouble
∞≤γ
1−γ/vextenddouble/vextenddouble(P−˜Pπ)V¯π/vextenddouble/vextenddouble
∞
≤γ
1−γmax
s,a/vextenddouble/vextenddoublePsa−˜Pπ
sa/vextenddouble/vextenddouble
1/vextenddouble/vextenddoubleV¯π/vextenddouble/vextenddouble
∞
≤γrmax
(1−γ)2max
s,a/vextenddouble/vextenddoublePsa−˜Pπ
sa/vextenddouble/vextenddouble
1. (21)
In the above, ˜Pπis as defined in (6) and denotes the transition model in ˜Munder behaviour policy π. Writing
¯P(s′|s1,a) =/summationtext
s2∈SP(s2|s1,a)PM(s′|s2), we have, for any s,a,
/vextenddouble/vextenddoublePsa−˜Pπ
sa/vextenddouble/vextenddouble
1=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublePsa−/summationdisplay
s1∈Sνπ(s1|s,a)¯Ps1a/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
1
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
s1∈Sνπ(s1|s,a)Psa−/summationdisplay
s1∈Sνπ(s1|s,a)¯Ps1a/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
1
≤/summationdisplay
s1∈Sνπ(s1|s,a)/vextenddouble/vextenddoublePsa−¯Ps1a/vextenddouble/vextenddouble
1
≤/summationdisplay
s1∈Sνπ(s1|s,a)
/vextenddouble/vextenddoublePsa−¯Psa/vextenddouble/vextenddouble
1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(one)+/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(two)
,
where the last two steps follows from the triangle inequality and splitting/vextenddouble/vextenddoublePsa−¯Ps1a/vextenddouble/vextenddouble
1. The first term can
be viewed as the error due to output privatisation and the second term can be viewed as the error due to
input privatisation. We will look to bound the two terms separately.
Term (one)
By the Bretagnolle-Huber inequality (Bretagnolle & Huber, 1979; Canonne, 2023), term (one) can be bound
as:
/vextenddouble/vextenddoublePsa−¯Psa/vextenddouble/vextenddouble
1≤2/radicalig
1−exp(−DKL(Psa||¯Psa).
24Published in Transactions on Machine Learning Research (12/2024)
The KL divergence term can be lower bound as follows:
−DKL(Psa||¯Psa) =/summationdisplay
s′∈SPs′
salog¯Ps′
sa
Ps′
sa
=/summationdisplay
s′∈SPs′
salog/summationtext
s2∈SPM(s′|s2)Ps2sa
Ps′
sa
≥/summationdisplay
s′∈SPs′
salogPM(s′|s′)Ps′
sa
Ps′
sa(22)
=/summationdisplay
s′∈SPs′
salogPM(s′|s′)
≥min
s′∈SlogPM(s′|s′).
Here (22) follows as we shrink the sum to just when s2=s′.
Note thatPM(s′|s′)is smallest when s′is an interior point as it has the smallest subspace of RKthat
projects back onto itself. So let s′∈S\∂Sdenote an interior point going forward. Thus term (one) can be
bound as:
/vextenddouble/vextenddoublePsa−¯Psa/vextenddouble/vextenddouble
1≤2/radicalbig
1−PM(s′|s′). (23)
We now look to lower bound PM(s′|s′). LettingR(s′)denote the subspace that projects onto s′,PM(s′|s′)
can be expressed as
PM(s′|s′) =PML(s∈R(s′)|s′)
=/integraldisplay
s∈R(s′)/parenleftbiggNϵ
4/parenrightbiggK
exp/parenleftbigg
−Nϵ
2∥s−s′∥1/parenrightbigg
ds.
Letδ=1√
2NandB2
δ(s′):={s∈RK:∥s−s′∥2≤δ}denote the ℓ2ball of radius δ. Clearly, all points inside
B2
δ(s′)are closest to s′compared to any other point in the state space. Also, we have that B2
δ(s′)⊆R(s′).
Integrating, we get:
PML(s∈R(s′)|s′) =/integraldisplay
s∈R(s′)/parenleftbiggNϵ
4/parenrightbiggK
exp/parenleftbigg
−Nϵ
2∥s−s′∥1/parenrightbigg
ds
≥/integraldisplay
s∈B2
δ(s′)K/productdisplay
i=1/parenleftbiggNϵ
4/parenrightbigg
exp/parenleftbigg
−Nϵ
2|si−s′
i|/parenrightbigg
ds
LetδK=δ/√
K. We now integrate over CδK(x′):=×K
i=1[s′
i−δK,s′
i+δK]to get a lower bound as
CδK(x′)⊆B2
δ(x′). The integral is then:
PML(s∈R(s′)|s′)≥/integraldisplay
s∈B2
δ(s′)K/productdisplay
i=1/parenleftbiggNϵ
4/parenrightbigg
exp/parenleftbigg
−Nϵ
2|si−s′
i|/parenrightbigg
ds
≥/integraldisplay
s∈CδK(s′)K/productdisplay
i=1/parenleftbiggNϵ
4/parenrightbigg
exp/parenleftbigg
−Nϵ
2|si−s′
i|/parenrightbigg
ds
=K/productdisplay
i=1/integraldisplays′
i+δK
s′
i−δKNϵ
4exp/parenleftbigg
−Nϵ
2|si−s′
i|/parenrightbigg
dsi
=K/productdisplay
i=1/integraldisplayδK
−δKNϵ
4exp/parenleftbigg
−Nϵ
2|si|/parenrightbigg
dsi
25Published in Transactions on Machine Learning Research (12/2024)
=K/productdisplay
i=1(1−exp(−αδK)) (24)
=/parenleftbigg
1−exp/parenleftbigg
−ϵ
2√
2K/parenrightbigg/parenrightbiggK
≥1−Kexp/parenleftbigg
−ϵ
2√
2K/parenrightbigg
, (25)
where step (24) follows by noting that for, all α>0
α
2/integraldisplayδK
−δKexp (−α|x|)dx=α/parenleftigg/integraldisplayδK
0exp (−αx)dx/parenrightigg
=α/bracketleftbigg1−exp(−αδK)
α/bracketrightbigg
= 1−exp(−αδK)
and setting α=Nϵ/2, and step (25) follows by noting that for all p∈[0,1],(1−p)k≥1−kp.
Substituting into equation (23) gives the following bound on term (one):
/summationdisplay
s1∈Sνπ(s1|s,a)/vextenddouble/vextenddoublePsa−¯Psa/vextenddouble/vextenddouble
1≤2/radicalbig
1−PM(s′|s′)
= 2/radicaligg
1−/parenleftbigg
1−Kexp/parenleftbigg
−ϵ
2√
2K/parenrightbigg/parenrightbigg
= 2√
Kexp/parenleftbigg
−ϵ
2√
2K/parenrightbigg
. (26)
Term (two)
Using equation 7, we have:
/summationdisplay
s1∈Sνπ(s1|s,a)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1=/summationdisplay
s1∈SPM(s|s1)νπ(s1|a)
˜νπ(s|a)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1
≤H/summationdisplay
s1∈SPM(s|s1)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1
≤H/summationdisplay
s1∈SPM(s|s1)
PM(s1|s)PM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1
≤HCmax/summationdisplay
s1∈SPM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1,
whereH= maxs,s1νπ(s1|a)
˜νπ(s|a), andCmax= maxs,s1PM(s|s1)
PM(s1|s).
We now look to bound/summationtext
s1∈SPM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1. Forα>0, define the ℓ∞ball of radius β=α+1√
2N
around a state sand its complement as:
Bα(s):={s′∈S:∥s−s′∥∞<β}
Bc
α(s):={s′∈S:∥s−s′∥∞≥β}.
26Published in Transactions on Machine Learning Research (12/2024)
Splitting/summationtext
s1∈SPM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1overBβ(s)andBc
β(s)gives us the following bound:
/summationdisplay
s1∈SPM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1
=/summationdisplay
s1∈Bβ(s)PM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1+/summationdisplay
s1∈Bc
β(s)PM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1
≤LKβ + 2Kexp/parenleftbigg
−Nαϵ
2√
K/parenrightbigg
(27)
= 2Kexp/parenleftbigg
−Nαϵ
2√
K/parenrightbigg
+LKα +LK√
2N. (28)
The first term in inequality (27) follows by noting that ¯PisL-Lipschitz by Lemma 5 and Assumption 2, and
that, for all x∈RK,∥x∥1≤K∥x∥∞. The second term in inequality (27) follows by noting that the L1norm
between distributions is bound by 2 and applying Proposition 2. Picking α=2√
K√
Ngives us
/summationdisplay
s1∈SPM(s1|s)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1≤O/parenleftbigg
Kexp/parenleftig
−√
Nϵ/parenrightig
+K3/2
√
N/parenrightbigg
.
Thus the final bound on term (two) is:
/summationdisplay
s1∈Sνπ(s1|s,a)/vextenddouble/vextenddouble¯Psa−¯Ps1a/vextenddouble/vextenddouble
1≤O/parenleftbigg
Kexp/parenleftig
−√
Nϵ/parenrightig
+K3/2
√
N/parenrightbigg
. (29)
Combining (26) and (29) and picking the higher growth rate terms gives the final result.
C Implementation Details
Figure 5: Target privacy vs privacy achieved under equations (5) and (8) as δis varied.T= 2e5.
Figure 5 plots the curves for Equation 8 as a function of ϵfor different δand T = 2e5 and shows that
decreasing the value of δincreases the gap between the privacy achieved and the target privacy. Target ϵ
values under 10 can be achieved for δ= 10−2.
Our experimental results use a concrete instantiation of Algorithm 1 with DQN (Mnih et al., 2015) and
epsilon-greedy for exploration. We display the full algorithm details in Algorithm 4.
The state privatisation mechanism used is the projected laplace mechanism. We provide a particular
instantiation of Algorithm 2 in Algorithm 3. Algorithm 3 works by first performing a projection onto the
27Published in Transactions on Machine Learning Research (12/2024)
simplex using the sorting method from Shalev-Shwartz et al. (2006) before finding the nearest point in the
state space. The sorting method has complexity O(KlogK)and finding the nearest point in the state space
can be done inO(K).
Algorithm 4 has a few minor differences to Algorithm 1. Firstly, the algorithm takes the target cumulative
privacy parameters as input instead of the per-step privacy parameters. The per-step privacy budget is
calculated in line 5. Lines 10-21 of can be considered the RL algorithm in Algorithm 1 expanded. The
replay buffer and target network are written such that they are not a part of the RL algorithm itself. This is
done to make clear that the buffer and target network maintain state across all iterations. Finally, DQN
returns a value function instead of a policy. This is a minor change however as the policy used is simply the
epsilon-greedy policy with respect to the returned value function.
The parameters used in each experiment are listed in Table 3. The neural network used in all experiments
was a 6 layer, fully connected MLP. The learning rate ( α) is not listed as we use the default settings of the
RMSProp optimizer in PyTorch to optimize the neural network.
All experiments were performed on a shared server with a 32-Core Intel(R) Xeon(R) Gold 5218 CPU, 192
gigabytes of RAM. A single NVIDIA GeForce RTX 3090 GPU was also used.
Table 3: DP-DQN parameters used for every experiment.
γ T B D ϵ startκ
0.999 2e5 128 800 0.9999 10−5
Algorithm 3 ℓ2-projected laplace mechanism
1:Input:states∈S⊆ RK.
2:Parameters: Privacy parameter ϵ, population size N.
3:s′=s+η, whereη∈RKand fori∈[K], ηi∼Lap/parenleftbig2
Nϵ/parenrightbig
.
4:Sorts′= (s′
1,...,s′
K)in descending order: s′
(1)≤...≤s′
(n).
5:Defineπ(m) =1
m/parenleftig/summationtextm
r=1s′
(r)−1/parenrightig
6:Computeρ= max/braceleftig
m∈[K] :s′
(m)−π(m)>0/bracerightig
7:Compute ¯swhere ¯si= max(0,s′
i−π(ρ))
8:Computee∈RKwhereei=xi−¯siandxi= arg min x∈S1|x−¯si|forS1=/braceleftbig
0,1
N,...,N−1
N,1/bracerightbig
.
9:FindJ⊂[K]with|J|=K−1that minimizes/radicalig/summationtext
i∈Je2
i. Letj= [K]\Jand setej=−/summationtext
i∈Jei.
10:return ¯s+e.
28Published in Transactions on Machine Learning Research (12/2024)
Algorithm 4 Differentially Private DQN (DP-DQN)
1:Input:Environment M= (S,A,r,P,γ ), initial state s0∈S, privacy mechanism M:S×R→S.
2:Parameters: cumulative privacy parameters (ϵ,δ), time horizon T, batch size B, target update step D,
population size N, learning rate α, initial exploration rate ϵstart<1, decay rate κ<1.
3:Initialize: network parameters θrandomly, target network parameters ¯θ=θ,ϵexplore =ϵstart.
4:Buffer←{}.
5:ϵ′=ϵ
2√
2Tlog(1/δ).
6:˜s0=Mϵ′(s0).
7:fort= 0,1,2,...,Tdo
8:p∼Uniform ([0,1]).
9:ifp>ϵexplorethen ˜at= arg max aQθ(˜st,a)else ˜at∼Uniform (A).
10:Receivest+1∼P(·|st,˜at).
11: ˜st+1=Mϵ′(st+1).
12: ˜rt=r(˜st,at),
13:Append (˜st,˜at,˜rt,˜st+1)toBuffer.
14:ift>Bthen
15:fori= 1,...,Bdo
16: (s,a,r,s′)∼Uniform (Buffer ).
17:yi←r+γmaxaQ¯θ(s′,a).
18:ℓi←1
2(Qθ(s,a)−yi)2.
19:end for
20:Run one step SGD θ←θ+α1
B∇θ/summationtextB
j=1ℓj.
21:end if
22:iftmodD= 0then
23:Update target network parameters ¯θ←θ.
24:end if
25:ϵexplore←0.03 + (ϵstart−0.03)·e−κt.
26:end for
29