Under review as submission to TMLR
Systematic Exploration and Exploitation
via a Markov Game with Impulse Control
Anonymous authors
Paper under double-blind review
Abstract
Efficient reinforcement learning (RL) involves a trade-off between “exploitative” actions that
maximise expected reward and “explorative” actions that lead to the visitation of “novel”
states. To encourage exploration, existing methods proposed methods such as injecting
stochasticity into action selection, implicit regularisation, and synthetic heuristic rewards.
However, these techniques do not necessarily offer systematic approach for making this
trade-off. Here we introduce SElectiveReinforcement Exploration Network (SEREN), a
plug-and-play framework that casts the exploration-exploitation trade-off as a Markov game
between an RL agent – Exploiter, which purely exploits task-dependent rewards, and another
RL agent – Switcher, which chooses at which states to activate a pure exploration policy that
is trained to minimise system uncertainty and override Exploiter. Using a form of policies
known as impulse control ,Switcher is able to determine the best set of states to switch to the
exploration policy while Exploiter is free to execute its actions everywhere else. We prove
that the convergence of SEREN under linear regime, and show that it induces a natural
schedule towards pure exploitation. Through extensive empirical studies in both discrete
and continuous control benchmarks, we show that with minimal modification, SEREN can
be readily combined with existing RL algorithms and yield performance improvement.
1 Introduction
Reinforcement learning (RL) is a framework that enables autonomous agents to learn complex behaviours
through trial and error (Sutton & Barto, 2018). With the combination of neural-network based func-
tion approximations, RL has had notable successes in a number of practical domains such as robotics and
games (Silver et al., 2016; Reed et al., 2022). In order to find the global optimal policy, the RL agent needs to
trade off visiting states with known high rewards (exploitation) against going to (current) suboptimal states
(exploration) during training, such that sufficient coverage of the state space is achieved. However, randomly
perturbing actions for exploration is sample inefficient since it does not take into account the information
acquired from previous experiences. In practice, this procedure exacerbates the sample complexity of the
agent’s learning of the optimal policy, despite theoretically grounded asymptotic convergence (Dabney et al.,
2020).
HerewetacklethechallengeofperformingsystematicandefficientexplorationinRL.Weproposeanoveltwo-
agent framework that disentangles exploration and exploitation into independent RL problems for more effi-
cientlearningoftherespectiveoptimalbehaviours. SElectiveReinforcement Exploration Network(SEREN)
entails an interdependent interaction between an RL agent, Exploiter, whose goal is to maximise the current
estimate of future task-dependent (extrinsic) rewards, and an additional RL agent, Switcher, whose goal
is to explore so as to reduce system uncertainty across the state space. Furthermore, at any given state,
Switcher has the power to override Exploiter and assume control of the system (at that state) to apply ex-
ploratory actions. Therefore, Switcher acts to reduce system uncertainty in subregions of the state space
in which (high) system uncertainty exists. A key ingredient of the SEREN framework is the use of a form
of policy known as impulse control (Øksendal & Sulem, 2007; Mguni et al., 2022) used by Switcher. This
enables Switcher to quickly determine the appropriate points to activate its exploration policy to minimise
system uncertainty.
1Under review as submission to TMLR
By using a two-agent framework for independent learning of the exploitation and exploration policies, the
competing individual goals of completing the task set by the environment versus exploration over the state
space are decoupled and each delegated to an independent agent. This means that Exploiter pursues its
task of maximising its objective by purely exploiting without trading-off rewards from the environment
for exploration. Moreover, as Switcher itself is an RL agent, it learns to perform systematic and targeted
arbitration between exploitative and exploratory actions, switching to exploration only where such actions
produce a reduction (surpassing certain threshold, see Section 3) in the cummulative system uncertainty. We
formally prove in Section 4 that an optimal schedule of exploration naturally emerges from SEREN without
theneedforheuristicexplorationscheduling. SERENisaflexibleplug-and-playframeworkthatcanbeeasily
integrated with existing RL algorithms. We instantiate SEREN on both value-based and policy-gradient RL
agents and empirically evaluate on both discrete and continuous control benchmarks. We show that SEREN
leads to systematic improvement over existing baselines on all presented tasks.
The intuition behind our framework is inspired by naturally occurring learning phenomena. A well-
established hypothesis of animal decision-making is that animals exhibit information-seeking behaviour to
reduce the internal estimates of the uncertainty of the environment (Gottlieb et al., 2013). Experimental
evidence indicates the orthogonal encoding of information value and primary reward value in primate or-
bitofrontal cortex (OFC) for curiosity-based decision making (Blanchard et al., 2015), which coheres nicely
withSEREN’sframeworkofdualsystemsforindependentlearningofexploitativeandexploratorybehaviours
(see further discussion in Section 7).
2 Preliminaries
In RL problems, an agent gradually learns to select actions for maximisation of its expected returns through
interactions with the environment. The underlying maximisation problem is typically formalised as a Markov
Decision Process (MDP): ⟨S,A,P,R,γ⟩, whereS⊂Rpis the set of states, A⊂Rkis the action space,
P:S×A×S→ [0,1]is the transition function describing the environment dynamics, R:S×A→ Ris the
environmental reward function and the discounting factor γ∈[0,1]specifies the degree to which the agent’s
future rewards are discounted over time (Sutton & Barto, 2018). The goal of an RL agent is to learn an
optimal policy, π∗:S→µ(A), whereµ(A)is a probability measure over A, such that the expected value
function is maximised for all s∈S:
π∗= arg max
π∈Ψvπ(s),vπ(s) =Eπ/bracketleftigg∞/summationdisplay
τ=0γkRt+τ(st+τ,at+τ)|St=s/bracketrightigg
, (1)
whereRt:=R(st,at)for allt= 0,1,...,∀st∈S,∀at∈A,Ψ ={π:S→µ(A)}is the policy space1, andSt
is the random variable of the state value at time t, given the policy πand the transition dynamics P.
3 SEREN: A Dual System for Exploitation and Exploration
SEREN consists of an RL agent, Exploiter and animpulse control (Mguni et al., 2022; 2021b;a) agent,
Switcher.Switcher has the ability to transfer control of the system to the exploration policy and does so
at a set of states it chooses. Switcher is trained to minimise the system uncertainty across the state space,
hence determining the best set of states to transfer control to the exploration policy, whilst Exploiter is free
to exploit everywhere else. The schematic illustration of the SEREN can be found in Figure 1.
As opposed to standard methods for exploration, we separate the exploration and exploitation into two
separate agents. We do so by introducing another learned decision of when to switch from exploitative to
explorative beahviour, and vice versa. To simplify learning, we consider two agents: Exploiter, which learns
a standard exploitation policy by only maximising the extrinsic reward ; and Switcher, which is trained to
decide when to switch to explorative behaviour and learns a purely exploration policy by only maximising an
uncertainty-based intrinsic reward (see equation 4). Thus, each agent operates on respective action spaces,
1Unless stated otherwise, we use µ(·)to denote some probability measure over the specified space.
2Under review as submission to TMLR
Switcher   sample action of switching controlg∼𝔤(⋅|st)
Exploiteragents’ joint action atEnvironmentstate st
Exploiterg=0g=1
Figure 1: Schematic illustration of SEREN components and how SEREN interacts with the environment for
action selection.
Ait≡AandAre≡A×{ 0,1}(whereAis the environment action space), with respective reward functions,
Rit,Rre:S×Ait×Are→R(see below for definition of Rre)2.
Formally, our framework is defined by a tuple G=⟨N,S,Ait,Are,P,Rit,Rre,γit,⃗Γre⟩whereN=
{Exploiter, Switcher }are the sets of agents, the transition probability, P:S×Ait×Are×S→ [0,1],
which takes the current state and the actions of both agents as inputs, and the discounting factors, γit,
⃗Γre:= (γre,γg)∈(0,1)×(0,1), forπit,πreandg, respectively (see below). Exploiter defines a Markov
policyπit:S→Ait, which is contained in the set Ψit⊆Ψ.Switcher has two components: a Markov policy
πre:S→Afrom Ψre⊆Ψ, which determines the exploration action based on the measure of cumulative
future uncertainty, and a discrete switching policy g:S→{ 0,1}. By default, the joint system relies on
the exploitation policy, πitto take actions. At each state Switcher makes a binary decision based on gto
decide whether to transfer control of the system to the exploration policy, πre. We denote by{τk}k≥0the
timepoints at which the Switcher decides to activate the exploration policy, or simply the intervention times .
The intervention times obey the expression τk= inf{t > τk−1|st∈S,s.t.,g(st) = 1}and are therefore
rulesthat depend on the state . Hence, by learning an optimal g,Switcher learns the best states to activate
exploration. As we later explain, these intervention times are determined by a condition on the state which
is easy to evaluate (see Prop. 2). For notational simplicity, we denote Rz
t=Rz(st,ait
t,are
t,gt)forz∈{it,re},
wheregt=g(st),ait/re∼πit/re(·|st).
3.1 The Exploiter Objective
The goal of Exploiter is to (greedily) maximise its expected cumulative reward set by the environment. The
objective that Exploiter seeks to maximise is:
vit(s|πit,{πre,g}) =E/bracketleftigg∞/summationdisplay
τ=0γτ
itRit(st+τ,ait
t+τ,are
t+τ,gt+τ)|St=s/bracketrightigg
,∀s∈S
whereRit(st,ait
t,are
t,gt) =R(st,ait
t)(1−gt) +R(s,are
t)gt(2)
whereait
t∼πit(·|st)is the exploitative action, are
t∼πre(·|st)is the exploratory action at time t,gtis
the binary output of the switcher (Section 3.3) that controls the execution of actions of ExploiterorEx-
plorer. Therefore, the reward received by Exploiter isR(st,are
t)whent=τk,k= 1,2,...i.e. whenever the
Switcher activates the exploration policy and R(st,ait
t)otherwise.
2Note that we use ·itand·reto refer to the quantities associated with the Exploiter andSwitcher, respectively.
3Under review as submission to TMLR
Whenever Switcher decides to transfer control to the exploration policy, the exploration policy overrides the
Exploiter and the integrated transition dynamics (given both the environment transition and the policy) are
affectedbyonlytheexplorationpolicy(while Exploiter influencesthedynamicsatallothertimes). Theoverall
transitiondynamicsarethereforegivenby P(st+1,are
t,ait
t,gt,st) :=P(st+1,ait
t,st) (1−gt)+P(st+1,are
t,st)gt.
3.2 The Exploration Policy
The actions selected by the exploration policy πreare chosen so as to maximise the following objective
function:
vre(s|πit,{πre,g}) =E/bracketleftigg∞/summationdisplay
τ=0γτ
reRre(st+τ,ait
t+τ,are
t+τ,gt+τ)|St=s/bracketrightigg
,∀s∈S
whereRre(st,ait
t,are
t,gt) :=L(st,are
t)gt+L(st,ait
t)(1−gt)(3)
whereL(s,a)is some measure of the system uncertainty about the state space which we specify in detail
shortly, and is chosen to satisfy the property that L→0as the system uncertainty decreases (see, e.g.,
Equation 4). Analogous to the reward function for Exploiter, the functionRreis defined so that the received
reward isL(st,are
t)whent=τk,k= 0,1,...i.e. whenever the Switcher activates the exploration policy, and
L(st,ait
t)otherwise.
Wenote thatboth Rit
tandRre
tare evaluatedat alltimesteps, instead ofindependentlywhen exploitationand
exploration actions are respectively chosen, since extrinsic reward is received during exploratory behaviour
and system uncertainty is also received during exploitative behaviour. Hence the joint system (SEREN) can
maximally utilise the rewardsignals available (regardless whether exploitative or explorative behaviour is
activated at each timestep) and learns to choose the optimal combination of exploitative and exploratory
actions to maximise the respective objectives across all timepoints.
3.3 The Switching Mechanism
SEREN then utilises intrinsic rewards for choosing when to switch between exploitative and exploratory
behaviour Schmidhuber (1991).
In principle, SEREN could accommodate various intrinsically constructed measures of uncertainty or novelty,
possible choices include the model-based ensemble epistemic uncertainty in dynamics prediction (Chua et al.,
2018), action-prediction errors (Pathak et al., 2017), and causally instructed novelty based on retrospective
information (Yu et al., 2024), etc. Here we focus on the model-free instantiation of the proposed SEREN
framework, hence we choose to employ a model-free estimate of uncertainty across the state space. In
this case, we assume that the Exploiter employs an ensemble of neural networks as its value function. We
quantify uncertainty over the state space using the non-parametric ensemble disagreement based on ensemble
modelling of the value function of the Exploiter (Osband et al., 2016). In particular, for an ensemble of
Mcritic estimates of {Q1,...,QM}, we use the ensemble-based empirical estimate of uncertainty for any
(s,a)∈S×A:
L(s,a) =1
M−1M/summationdisplay
m=1(Qm(s,a)−EQ(s,a))2, (4)
whereEQ(s,a) :=1
M/summationtextM
m=1Qm(s,a)is the empirical mean of the ensemble predictions.
The goal of Switcher is to take actions that lead to states with high uncertainty, such that the system
uncertainty over the state (-action) space is minimised. To induce Switcher toselectively choose when
to switch to exploration, each switching activation incurs a fixed cost. These costs are quantified by the
indicator function which is βwhenever an exploratory action is performed and 0otherwise, where βis a
fixed positive constant. The presence of the switching cost ensures that the gain in measured uncertainty
for performing exploratory actions to arrive at a given set of states is sufficiently high to merit forgoing
rewards from taking exploitative actions in the corresponding states. Therefore to maximise this objective,
Switcher must determine the sequence of points {τk}at which the benefit of performing an exploratory action
4Under review as submission to TMLR
overcomes the cost of doing so. Accordingly at time t∈0,1,...,Switcher seeks to maximise the following
objective:
vg(s|πit,{πre,g}) =E/bracketleftigg∞/summationdisplay
τ=0Rg(st+τ,ait
t+τ,ait
t+τ,gt+τ;β)|St=s/bracketrightigg
,∀s∈S
whereRg(st+τ,ait
t+τ,ait
t+τ,gt+τ;β) =γτ
gL(st+τ,ait
t+τ)−gt+τ(L(st+τ,ait
t+τ)−L(st+τ,are
t+τ) +β)(5)
Therefore to maximise its objective, Switcher must determine the best set of states to reduce system uncer-
tainty. Note that since Rredepends on the uncertainty measure Lequation 4, which is in turn dependent on
the rapidly changing value network, hence it has the non-stationary property that E[Rre(s)]→0as system
uncertainty decreases across the state space. With low levels of uncertainty, the switching cost dominates
so the Switcher does not intervene, leaving the Exploiter to take actions that deliver high rewards. This
effectively pushes the Switcher out of the game as systemic uncertainty is reduced. This is precisely the
behaviour that we seek as more information about the environment becomes known. We formally prove this
property in Sec. 4 (see Prop. 2).
The pseudocode for off-policy training of SEREN is shown in Algorithm 1.
3.4 Relation to Other Exploratory Mechanisms
SEREN entails an exploration framework of great generality, such that many existing exploration models can
be viewed as some degenerate form of SEREN. For instance, the classical ϵ-greedy exploration can be inter-
pretedasSERENwitharandomswitchingmechanismanduniformlyrandomexplorationpolicy. Moreover,if
we consider the case in which Switcher has the identical objective to Exploiter, consisting of task rewards with
additive intrinsic rewards, then the model is equivalent to curiosity-based exploration (Schmidhuber, 1991;
Pathak et al., 2017; Burda et al., 2018). We hope the framework of a dual system for exploration-exploitation
tradeoff proposed in the current paper could inspire the development of more systematic exploration methods
currently unthought of.
3.5 SEREN Training
As we show in Sec. 4, the learning processes for both agents converge to a stable solution. Note that since πit
andπreshare the same action space ( A), the Exploiter is trained off-policy using the data generated by both
exploration and exploitation policies. The three policies, πit,πre, and g, maintain their independent replay
buffers with respective actions and reward functions. We note that as the training progresses, the uncertainty
inevitably decreases, yielding the reward function for the exploration policy training non-stationary. To
counteract the non-stationarity of the reward structure in the learning of πre, the discounting factor γreis
set to be appropriately lower (comparing to standard values) such that the agent still learns a policy that
maximises future returns (instead of only relying on immediate uncertainty as in existing works in using
intrinsic rewards), but at the same time reduces the negative effects caused by the distributional shift in the
reward distribution.
4 Convergence & Optimality of SEREN
A key aspect of SEREN is the presence of two RL agents that each adapt their play according to the other’s
behaviour. This produces two concurrent learning processes each designed to fulfill distinct objectives. At
a stable point of the learning processes Switcher minimises uncertainty about less explored states while
Exploiter maximises the environment reward. Introducing simultaneous learners can occasionally lead to
issues that prevent convergence to the stable point (Zinkevich et al., 2006).
We now show that Gadmits a stable point and that our method converges to it. In particular, we show that
the joint system converges in its value functions for each agent. Additionally, we show that SEREN induces
a natural schedule in which as the environment is explored, the number of switching operations (induced by
Switcher) tends to 0. All proofs can be found in Appendix B.
We begin by stating a key result:
5Under review as submission to TMLR
Algorithm 1 SE lectiveReinforcement Exploration Network (SEREN)
1:Given reward objective function for Switcher, uncertainty objective function L(·,·), initialise independent
Replay BuffersBzforz∈{it,re,g}, switching cost β, episode length Teps.
2:forNepisodesdo
3:Reset state s0
4:fort= 0,1,...,T epsdo
5:Sample Exploiter action,ait
t∼πit(·|st);Exploreraction,are
t∼πre(·|st);Switcher action,gt∼g(st),
6:ifgt= 0then
7: Applyait
tsost+1∼P(·|ait
t,st),
8:else
9: Applyare
tsost+1∼P(·|are
t,st),
10:end if
11:Compute rewards: rit
t(Equation 2), rre
t(Equation 3), and switcher reward rg
t(Equation 5).
12:Store (st,az
t,st+1,rz
t})inBzforz∈{it,re,g};
13:end for
14:// Independent training of individual policies
15:Sample batches of |B|transitions, Bz={(sz
b,az
b,sz
b+1,rz
b)}|B|
b=1fromBzforz∈{it,re,g}
16:UpdateπitwithBit, updateπrewithBre, update gwithBg.
17:end for
Theorem 1 SEREN converges to a stable solution in the agents’ value functions.
Theorem 1 is established by proving a series of results. Firstly, for a given πit, we prove that the training
process of Switcher converges (to its optimal value function). Secondly, we show that the system of the two
learners Exploiter andSwitcher jointly converges to their optimal value functions.
Our first result proves that the optimal value function of Switcher can be obtained as the limit point of
a sequence of Bellman operations. We subsequently prove that the convergence result extends to linear
function approximation. To begin, we firstly define the optimal linear-projection ,Π, as:
ΠΛ := Φr∗,wherer∗∈arg min
r∈Rp∥Φr−Λ∥ (6)
for any function Λ, where Φis a given matrix of p-dimensional linear features and ris the corresponding
weight vector.
Given a function Q:S×A→ R,∀πre,π∈Ψand∀g,∀sτk∈S, we define the intervention operator M
byMπre[Qπ′,g(sτk,a)] := Eaτk∼πre(·|sτk)[R(sτk,aτk)−β+γ/summationtext
s′∈SP(s′;aτk,s)Qπ′,g(s′,aτk)].3We denote
byMQπ,gthe intervention operator acting on Qπ,gwhen the immediate action is chosen according to an
epsilon-greedy policy. Note that Mapplies similarly to the value functions V:S→R.
Proposition 1 For a given Exploiter policyπit∈Ψ, the training process of Switcher converges (i.e., Qg→
Q⋆
g, see further details in Prop. 3). Moreover, given a set of linearly independent basis functions Φ =
{ϕ1,...,ϕp}whereϕ1≤k≤p∈L2, the value function of the Switcher converges to a limit point, r⋆∈Rp.r⋆
is the unique solution to ΠB(Φr⋆) = Φr⋆, whereBis defined by:BΛ :=R+γPmax{MΛ,Λ}. Moreover,
r⋆satisfies:∥Φr⋆−Qg,⋆∥≤(1−γ2
g)−1/2∥ΠQg,⋆−Qg,⋆∥, where for any function J:S→Rthe functional
Pis defined through PJ:=/summationtext
s′∈SJ(s′)P(s,a,s′).
Prop. 1 establishes the convergence guarantee of the training of Switcher (and additionally under the context
of linear function approximation). The second statement bounds the proximity of the convergence point by
the smallest approximation error that can be achieved given the choice of basis functions.
Having constructed a procedure to find the optimal Switcher policy, our next result characterises the
Switcher policy gand the times that Switcher must perform an intervention.
3Note for the term MQ, the action input of the Qfunction is a dummy variable decided by the operator M.
6Under review as submission to TMLR
Proposition 2 i) The Switcher intervention times are given by the following:
τk= inf{τ >τk−1|Mvg=vg} (7)
ii) Denote by µl(g)the number of switch activations performed by the Switcher when max (s,a)∈S×AL(s,a) =l
under g, then liml→0µl(g) = 0.
Part i) of Prop. 2 characterises the distribution g. Moreover, given the corresponding value function, vg, the
switching times{τk}can be determined by evaluating if Mvg=vgholds. Part ii) of Prop. 2 establishes
that the number of switches performed by Switcher tends to 0as the system uncertainty is reduced through
the systematic exploration induced by Switcher, which consequently induces a natural exploration schedule
based on the current system uncertainty.
5 Related Work
Exploration-Exploitation Tradeoff is a fundamental question in RL research, i.e. trading off finding
higher reward states and exploiting known rewards. Existing approaches include directly injecting pure
noise or certain parametric stochasticity into action choices during learning (Sutton & Barto, 2018; Lillicrap
et al., 2015); using stochastic controllers regularised by the maximum entropy principle (Haarnoja et al.,
2018); augmenting task rewards with synthetic exploration bonus / intrinsic reward (Stadie et al., 2015;
Pathak et al., 2017; Sekar et al., 2020; Yu et al., 2024). Despite the simplicity, no existing methods explicitly
learn an exploration policy for performing targeted exploration that exclusively maximise the expected
uncertainty over the future trajectory. Moreover, most existing methods utilise one policy for capturing
both the task-dependent optimal behaviour and the exploratory behaviour for efficient covering of the state
space, yielding suboptimal learning in both aspects, whereas SEREN is able to disentangle the learning of
the two policies with independently trained RL agents, leading to improved training for both the optimal
policy and the exploration policy. Moreover, in contrast to Reward free exploration (also known as the task-
or reward-agnostic exploration) (Zhang et al., 2020; Jin et al., 2020; Sekar et al., 2020), where the agent goes
through independent exploration and RL stages (with respect to the extrinsic reward), SEREN performs
exploration and learning simultaneously, eliminating the additional complexity involved with a pre-training
(exploration) phase.
One prominent prior work on disentangling exploitation and exploration objectives is MULEX (Beyer et al.,
2019), where separate value functions are learned for the environmental rewards and intrinsic exploratory
rewards independently (both dependent with respect to environmental dynamics and behavioural policy).
Similar to MULEX, SEREN is partially based on the motivation that learning separate value functions
for environmental and intrinsic rewards would reduce interference of value learning, and also exclude the
non-stationarity in the exploitation value function (based solely on environmental rewards). However, a key
novelty of SEREN is the introduction of the impulse control mechanism for switching between exploitative
and exploratory behaviour during the course of learning (Øksendal & Sulem, 2007; Mguni et al., 2022).
Hence the SEREN agent is able to learn the optimal behaviour specified by the environmental reward
function without human intervention that selects the suitable value function to act greedily with respect to.
Uncertainty quantification in exploration is an active field of research in RL. It is common practice to
use the disagreement of the predictions over an ensemble of neural networks as the epistemic uncertainty to
guideexplorationOsbandetal.(2016);Janneretal.(2019);Sekaretal.(2020);Leeetal.(2021). Connections
between ensemble disagreement and information theory have been drawn showing that choosing actions that
maximises the expected ensemble disagreement would maximally increase the information gain, improving
the efficiency of exploration Sekar et al. (2020); O’Donoghue (2021). Other popular alternatives involve the
prediction error of a discriminative dynamics model Schmidhuber et al. (1997); Pathak et al. (2017) and the
predictive uncertainty given a generative dynamics model Ratzlaff et al. (2020); Jiang & Lu (2020). Future
directions involve investigating instantiations of SEREN under different exploration bonuses.
Relation to Markov games. Our framework involves a system of two agents each with their individual
objectives. Settings of this kind are formalised by Markov games (MG), a framework for studying self-
interested agents that simultaneously act over time (Littman, 1994). In the standard MG setup, the actions
7Under review as submission to TMLR
ofbothagents influence both each agent’s rewards and the system dynamics. Therefore, each agent i∈{1,2}
has its own reward function Ri:S×(×2
i=1Ai)→Rand action setAiand its goal is to maximise its own
expected returns. The system dynamics, now influenced by both agents, are described by a transition
probabilityP:S×(×2
i=1Ai)×S→ [0,1]. Unlike classical MGs, in our MG, Switcher does not intervene at
each state but is allowed to assume control of the system at certain states which it decides using impulse
controls. Our setup is related to stochastic differential games with impulse control (Mguni, 2018). However,
our Markov Game differs markedly since it is nonzero-sum in which only one agent assumes control and is
a discrete-time treatment.
Policy Switching Policy switching with respect to different reward function is an active field of research in
reinforcement learning community. Its core concept could date back to the original optionsframework that
partition the behavioural sequence in an online fashion for learning option-dependent optimal policy (Sut-
ton et al., 1999). Some recent works focus on explicit construction of switching mechanism, similar to
SEREN (Zhou et al., 2022; Jacq et al., 2022). In particular, Jacq et al. (2022) proposed Lazy-MDPs, which
switch to random exploration policy during non-critical states for encouraging exploration. From an algo-
rithmic perspective, Lazy-MDP can be interpreted as a degenerate form of SEREN such that the switching
mechanism is manually constructed and the exploration policy is purely random. From a theoretical per-
spective, despite the switching mechanisms in both SEREN and Lazy-MDP are based on cost-minimisation,
SEREN is developed based on the interplay of agents maximising different objective under a Markov game
setting.
6 Experiments
We performed a series of experiments to demonstrate that SEREN’s multi-player framework is able to
improve the tradeoff between exploration and exploitation leading to marked improvement of the underling
RL methods (all experimental details can be found in Appendix D).
Specifically, we wish to address the question of whether SEREN learns to improve performance of an un-
derlying base RL learner by more efficiently locating higher reward states in MDPs with a) discrete b)
continuous action spaces with different base learners (e.g., value-based and actor-critic). Moreover, we wish
to empirically investigate if the non-stationary exploration reward structure negatively impacts the overall
learning (hence justifying our choice of lower discounting value for the training of πre).
6.1 MiniGrid Experiments
We firstly demonstrate SEREN in combination with a standard DQN (Mnih et al., 2013). It is well known
that DQN usually performs poorly in sparse-reward settings (Osband et al., 2016; Pathak et al., 2017).
To this end, we choose the MiniGrid environments (Chevalier-Boisvert et al., 2018), where all transitions to
non-goal states leads to zero reward. As we observe in Figure 2b, SEREN-DQN quickly learns to consistently
navigate towards the goal state within 100training episodes, whereas the standard DQN with ϵ-greedy has
failed to acquire a sensible policy over the 150episodes. Hence we conclude that SEREN can be readily
plugged into DQNs to deal with sparse-reward and/or goal-directed tasks (albeit not being our primary focus
in the current paper).
6.2 MuJoCo Experiments
We now evaluate SEREN based on a stochastic policy algorithm, the Soft Actor-Critic (SAC Haarnoja et al.
(2018)). SAC is an off-policy policy gradient algorithm, which augments the standard maximising return ob-
jectivewithanentropymaximisationterm(Haarnojaetal.,2017),leadingtoanimplicitlydefinedexploratory
component. Due to the exploratory component and the off-policy nature, SAC is arguably one of the most
sample-efficient algorithms, which makes any exploration improvements rather challenging. We evaluate
our model, SEREN-SAC, on continuous control benchmarks from the MuJoCo suite (Figure 3a; Todorov
et al. (2012)) to show that SEREN-SAC yields a more sample-efficient exploratory strategy than the im-
plicit exploratory behaviour given by the maximum entropy regularisation in standard SAC. We further
aim to disentangle the contribution of the improved performance by comparing the full SEREN-SAC with
8Under review as submission to TMLR
0 50 100 150 200 2500
50
100
150
200
250
(a) (b)0 20 40 60 80 100 120 1400.00.20.40.60.8SEREN-DQN
DQN
a
0 20 40 60 80 100 120 1400.2
0.00.20.40.60.81.0SEREN-DQN
DQN b
Figure 2: DQN and SEREN-DQN in the MiniGrid-World Chevalier-Boisvert et al. (2018). (a) Graphical
illustration of the “8x8" minigrid-world environment, only transitions into goal states (green block) lead to
non-zero rewards; (b) SEREN-DQN quickly learns the optimal policy to the goal state while the standard
DQN has not learned good policy over 150episodes of training.
a
0.0 0.2 0.4 0.6 0.8 1.0
1e60500100015002000250030003500Ant
0.0 0.2 0.4 0.6 0.8 1.0
1e61000
0100020003000HalfCheetah
0.0 0.2 0.4 0.6 0.8 1.0
1e640
30
20
10
0102030Reacher
0.0 0.2 0.4 0.6 0.8 1.0
1e60500100015002000Walker2D
SEREN-SAC SAC SEREN-SAC (random switching) SAC-ensemble
b
Figure 3: Evaluation of SEREN with the baseline SAC algorithms on the MuJoCo tasks (Todorov et al.,
2012). Average evaluation returns (over 5random seeds) of SEREN-SAC, SAC, and SAC-ensemble (see text)
over 1×106training steps.
associated degenerative alternatives: SAC, SEREN-SAC with random switching, and SAC-ensemble, where
SAC-ensemble augments standard SAC with an ensemble-based value network, and utilises the ensemble
epistemic uncertainty for computing the intrinsic reward for guiding switching (Equation 4).
From Figure 3, we observe that in 3out of the 4selected tasks, SEREN-SAC outperforms the baseline
SAC agent in terms of both sample efficiency and the asymptotic performance, over the 106training steps.
SEREN-SAC performs slightly worse than the baseline SAC on the HalfCheetah task in terms of sample
efficiency of learning, but reaches similar asymptotic performance. Moreover, we note that across all 4tasks,
SEREN-SAC leads to more robust training, as indicated by the standard deviation of the evaluation scores
over 5random seeds throughout training. The gain may be attributed to the effective exploration by the
Switcher, especially during the early phase of training, which facilitates the diversity of the off-policy replay
buffer, hence enabling the identification of better solutions.
9Under review as submission to TMLR
Finally, we evaluate the importance of the switching control mechanism for guiding exploration by comparing
SEREN-SAC to SAC-intrinsic. In Figure 3 we observe that merely including the additive uncertainty-based
intrinsic reward leads to worse performance across all tasks, and SEREN-SAC outperforms SAC-intrinsic
on all 4selected mujoco tasks. Hence, we empirically justify that the impulse switching control arbitration
enables the learning of more targeted exploration policies comparing to the naive additive combination of
the extrinsic reward and the uncertainty-based exploration bonus.
In order to further illustrate the importance of the impulse control switching mechanism, we implement
an alternative version of SEREN-SAC, this time replacing the learned switching policy, g, with a random
switching policy with manually defined decreasing switching probabilities. From Figure 3 we again observe
that the full SEREN-SAC outperforms the alternative with random switching in all 4tasks, demonstrating
the effectiveness of the learned impulse control switching mechanism.
6.2.1 Ablation Studies on γre
As discussed in Sec. 3, the discounting factor for the πre,γre, needs to be set small to mitigate the negative
effectsofthenon-stationarityrewardstructureinthetrainingof πre. However, naivelysettingtheexplorative
discounting factor too small would not yield good performance either, since the resulting πrewould choose
actions primarily dependent on the immediate epistemic uncertainty, hence failing to generate targeted
exploratory behaviour towards areas of high uncertainties. Here we empirically justify our hypothesis by
performing an ablation study on the effect of the value of the discounting factor for the training of πre.
By examining the asymptotic performance given 1×106training steps (Table 1), we see that setting the
discounting factor too large or too small both induce worse performance, whereas an intermediate values of
γre(0.6) yields the best performance in terms of averaged evaluation return. Noticeably, we also observe that
settingγretoo large or too small makes the training less robust with respect to the random seed, leading to
increased variability in the evaluation performance.
Table 1: Ablation studies on the effects of the discounting factor values of πre(γre).
Ant HalfCheetah Reacher Walker2D
SEREN-SAC ( γre= 0.6)2607.6±99.02327.8±102.521.8±2.01996.1±69.9
SEREN-SAC ( γre= 0.1)1853.7±1362.72205.4±846.2 20.2±4.01169.7±762.9
SEREN-SAC ( γre= 0.98)2477.9±128.9 1944.9±587.0 18.6±0.81720.7±177.6
6.2.2 Asymptotic Bias Towards Full Exploitation
We prove in Proposition 2 that as the agent’s quantification of uncertainty over the state space (indicated by
theE(s,a)∈S×A [L(s,a)], equation 4) decreases given more exploration, the number of switching activations
performed by Switcher converges to 0asymptotically, hence the overall behaviour gradually shifts towards
pure exploitation given more training. We observe this behaviour empirically with SEREN-SAC on the
mujoco tasks (Figure 4).
6.3 Atari Experiments
We further evaluate SEREN on a set of sample-constrained discrete control tasks, the Atari 100K bench-
mark (Kaiser et al., 2019). Here we instantiate SEREN based on DQN (Mnih et al., 2013), with addi-
tional components, including double Q-learning (Van Hasselt et al., 2016), dueling network for value esti-
mation (Wang et al., 2016), and multi-step TD-target (Mnih et al., 2016). The resulting DQN instantiated
with these additional features are referred to as the Efficient-DQN (Kostrikov et al., 2020). We additionally
utilise the image augmentation techniques for training the DQN proposed in DrQ (Kostrikov et al., 2020) (we
only use the “intensity" augmentation instead of all augmentation types as in Kostrikov et al. (2020)). We
apply the resulting model, SEREN-Eff-DQN on all games in the Atari 100K benchmark, and we evaluate the
performance given 100K training steps. We follow the evaluation procedures outlined in Kaiser et al. (2019).
In Figure 5, we show that SEREN-Eff-DQN outperforms all selected baselines (see Appendix D) in terms of
10Under review as submission to TMLR
0 200 400 600 800 1000
Episodes0100200300400500600700800Number of Switching / EpisodeAnt
0 200 400 600 800 1000
Episodes20406080100Reacher
Figure 4: Asymptotic decrease of switching actions given more training.
Rainbow
 Efficient-
Rainbow
Efficient-
DQN
DrQ
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Mean Human Normalised Scores
Median Human Normalised Scores
SEREN-Eff-DQN
Figure 5: Mean (normal bars) and median (shaded bars) human normalised scores of evaluations of SEREN-
Eff-DQN and selected baselines (see Appendix D) on Atari 100K benchmarks.
both the median and mean human normalized returns for all SEREN-Eff-DQN and selected baselines, hence
again indicating the improvement brought by the SEREN framework. While SEREN-Eff-DQN is performing
fairly similarly to DrQ, the plug & play nature of our approach means that using other base algorithms can
further improve the performance without significant redesign. Full experimental setup and results (for all
games) can be found in Appendix D and E.
7 Discussion
We introduced SEREN, a plug-and-play framework that seeks to learn the optimal arbitration between ex-
ploitative and exploratory behaviours using an impulse control mechanism. SEREN can be readily combined
withexistingvalue-basedandactor-criticalgorithms, herewedemonstratetheinstantiationsofSERENgiven
DQN and SAC, but more combinations can be considered for future works. We formulate the problem of the
arbitration between the exploration and the exploitation policies under a Markov game framework, where
Exploiter seeks to only maximise the cumulative return and Switcher, consisting of an exploration policy, πre,
and an impulse control switching policy, g, seeks to learn the optimal exploration timing and exploration
behaviour, in order to minimise the epistemic uncertainty of the value estimates of Exploiter (reflecting the
level of information about the environment acquired by the joint system) over the state space. We provide
theoretical justification for the convergence of SEREN to the optimal achievable value estimates with linear
function approximation. We demonstrate the utility of SEREN through extensive experimental studies on
continuous control benchmarks. When implemented with state-of-the-art policy gradient algorithms (SAC),
11Under review as submission to TMLR
we show that the SEREN-augmented agents consistently yield improvement in terms of sample efficiency
and asymptotic performance with respect to the baseline agents. We also showed that SEREN can be com-
bined with value-based algorithms such as DQN, to yield improvement on the Atari 100K benchmarks over
competitive baseline algorithms.
Behaviourally, animals tend to sacrifice short-term rewards to obtain information gain in uncertain environ-
ments (Bromberg-Martin & Hikosaka, 2009; Gottlieb et al., 2013). Blanchard et al. (2015) demonstrated
that OFC neurons have firing correlated with both information value and primary value signals. Instead of
integrating these variables to code subjective value, they found that OFC neurons tend to encode the two
signals in an orthogonal manner. Hence providing stronger biological plausibility, supporting the indepen-
dent representation and learning paradigm in SEREN over the joint encoding scheme in standard intrinsic
reward models Pathak et al. (2017).
References
Albert Benveniste, Michel Métivier, and Pierre Priouret. Adaptive algorithms and stochastic approximations ,
volume 22. Springer Science & Business Media, 2012.
Lucas Beyer, Damien Vincent, Olivier Teboul, Sylvain Gelly, Matthieu Geist, and Olivier Pietquin. Mulex:
Disentangling exploitation from exploration in deep rl. arXiv preprint arXiv:1907.00868 , 2019.
TommyCBlanchard, BenjaminYHayden, andEthanSBromberg-Martin. Orbitofrontalcortexusesdistinct
codes for different choice attributes in decisions motivated by curiosity. Neuron, 85(3):602–614, 2015.
Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters , 29(5):291–294,
1997.
Ethan S Bromberg-Martin and Okihide Hikosaka. Midbrain dopamine neurons signal preference for advance
information about upcoming rewards. Neuron, 63(1):119–126, 2009.
YuriBurda, HarrisonEdwards, AmosStorkey, andOlegKlimov. Explorationbyrandomnetworkdistillation.
arXiv preprint arXiv:1810.12894 , 2018.
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai
gym. https://github.com/maximecb/gym-minigrid , 2018.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a
handful of trials using probabilistic dynamics models. Advances in Neural Information Processing Systems ,
31, 2018.
Will Dabney, Georg Ostrovski, and André Barreto. Temporally-extended {\epsilon}-greedy exploration.
arXiv preprint arXiv:2006.01782 , 2020.
JacquelineGottlieb,Pierre-YvesOudeyer,ManuelLopes,andAdrienBaranes. Information-seeking,curiosity,
and attention: computational and neural mechanisms. Trends in cognitive sciences , 17(11):585–593, 2013.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep
energy-based policies. In International conference on machine learning , pp. 1352–1361. PMLR, 2017.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,
Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv
preprint arXiv:1812.05905 , 2018.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,
Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement
learning. In Thirty-second AAAI conference on artificial intelligence , 2018.
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. Convergence of stochastic iterative dynamic
programming algorithms. In Advances in neural information processing systems , pp. 703–710, 1994.
12Under review as submission to TMLR
Alexis Jacq, Johan Ferret, Olivier Pietquin, and Matthieu Geist. Lazy-mdps: Towards interpretable rein-
forcement learning by learning when to act. arXiv preprint arXiv:2203.08542 , 2022.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. In Advances in Neural Information Processing Systems , volume 32, pp. 12519–12530,
2019.
Jiechuan Jiang and Zongqing Lu. Generative exploration and exploitation. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 34, pp. 4337–4344, 2020.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforce-
ment learning. In International Conference on Machine Learning , pp. 4870–4879. PMLR, 2020.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,
DumitruErhan, Chelsea Finn, Piotr Kozakowski, SergeyLevine, et al. Model-based reinforcementlearning
for atari. arXiv preprint arXiv:1903.00374 , 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep
reinforcement learning from pixels. arXiv preprint arXiv:2004.13649 , 2020.
Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified framework for
ensemble learning in deep reinforcement learning. In International Conference on Machine Learning , pp.
6131–6141. PMLR, 2021.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971 , 2015.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine
learning proceedings 1994 , pp. 157–163. Elsevier, 1994.
David Mguni. A viscosity approach to stochastic differential games of control and stopping involving impul-
sive control. arXiv preprint arXiv:1803.11432 , 2018.
David Mguni. Cutting your losses: Learning fault-tolerant control and optimal stopping under adverse risk.
arXiv preprint arXiv:1902.05045 , 2019.
David Mguni, Aivar Sootla, Juliusz Ziomek, Oliver Slumbers, Zipeng Dai, Kun Shao, and Jun Wang. Timing
is everything: Learning to act selectively with costly actions and budgetary constraints. arXiv preprint
arXiv:2205.15953 , 2022.
David Henry Mguni, Taher Jafferjee, Jianhong Wang, Nicolas Perez-Nieves, Oliver Slumbers, Feifei Tong,
Yang Li, Jiangcheng Zhu, Yaodong Yang, and Jun Wang. Ligs: Learnable intrinsic-reward generation
selection for multi-agent learning. arXiv preprint arXiv:2112.02618 , 2021a.
David Henry Mguni, Jianhong Wang, Taher Jafferjee, Nicolas Perez-Nieves, Wenbin Song, Feifei Tong, Hui
Chen, Jiangcheng Zhu, Yaodong Yang, and Jun Wang. Learning to shape rewards using a game of two
partners. 2021b.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 , 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Inter-
national conference on machine learning , pp. 1928–1937. PMLR, 2016.
13Under review as submission to TMLR
Brendan O’Donoghue. Variational bayesian reinforcement learning with regret bounds. Advances in Neural
Information Processing Systems , 34, 2021.
Bernt Karsten Øksendal and Agnes Sulem. Applied stochastic control of jump diffusions , volume 498.
Springer, 2007.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped
dqn.Advances in neural information processing systems , 29:4026–4034, 2016.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems
32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf .
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-
supervised prediction. In International Conference on Machine Learning (ICML) , pp. 2778–2787, 2017.
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.
Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Re-
search, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/20-1364.html .
Neale Ratzlaff, Qinxun Bai, Li Fuxin, and Wei Xu. Implicit generative modeling for efficient exploration. In
International Conference on Machine Learning , pp. 7985–7995. PMLR, 2020.
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-
Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv
preprint arXiv:2205.06175 , 2022.
Jürgen Schmidhuber. Curious model-building control systems. In Proc. international joint conference on
neural networks , pp. 1458–1463, 1991.
Jürgen Schmidhuber, Jieyu Zhao, and Marco Wiering. Shifting inductive bias with success-story algorithm,
adaptive levin search, and incremental self-improvement. Machine Learning , 28(1):105–130, 1997.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Plan-
ning to explore via self-supervised world models. In International Conference on Machine Learning , pp.
8583–8592. PMLR, 2020.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. nature, 529(7587):484–489, 2016.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with
deep predictive models. arXiv preprint arXiv:1507.00814 , 2015.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for
temporal abstraction in reinforcement learning. Artificial intelligence , 112(1-2):181–211, 1999.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems , pp. 5026–5033. IEEE, 2012.
John N Tsitsiklis and Benjamin Van Roy. Optimal stopping of Markov processes: Hilbert space theory,
approximation algorithms, and an application to pricing high-dimensional financial derivatives. IEEE
Transactions on Automatic Control , 44(10):1840–1851, 1999.
14Under review as submission to TMLR
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In
Proceedings of the AAAI conference on artificial intelligence , volume 30, 2016.
Hado P Van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement
learning? Advances in Neural Information Processing Systems , 32, 2019.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network
architectures for deep reinforcement learning. In International conference on machine learning , pp. 1995–
2003. PMLR, 2016.
Changmin Yu, Neil Burgess, Maneesh Sahani, and Samuel J Gershman. Successor-predecessor intrinsic
exploration. Advances in Neural Information Processing Systems , 36, 2024.
Xuezhou Zhang, Adish Singla, et al. Task-agnostic exploration in reinforcement learning. arXiv preprint
arXiv:2006.09497 , 2020.
Zihan Zhou, Wei Fu, Bingliang Zhang, and Yi Wu. Continuously discovering novel strategies via reward-
switching policy optimization. arXiv preprint arXiv:2204.02246 , 2022.
Martin Zinkevich, Amy Greenwald, and Michael Littman. Cyclic equilibria in Markov games. Advances in
Neural Information Processing Systems , 18:1641, 2006.
15Under review as submission to TMLR
Appendix for Systematic Exploration and Exploitation via a Markov Game with
Impulse Control
Appendix Table of Contents
A Notation & Assumptions 16
B Proof of Technical Results 17
C Alternative uncertainty measures 28
D Implementation Details 29
D.1 MuJoCo Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
D.2 Atari 100K Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E Further Experimental Results 29
E.1 Full Evaluation Results on Atari 100K Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . 29
A Notation & Assumptions
We assume thatSis defined on a probability space (Ω,F,P)and anys∈Sis measurable with respect to
the Borelσ-algebra associated with Rp. We denote the σ-algebra of events generated by {st}t≥0byFt⊂F.
In what follows, we denote by (V,∥∥)any finite normed vector space and by Hthe set of all measurable
functions.
For notational simplicity, we denote the value/action functions for Exploiter andExplorerasv1/Q1andv2/Q2,
respectively.
The results of the paper are built under the following assumptions which are standard within RL and
stochastic approximation methods:
Assumption 1 The stochastic process governing the system dynamics is ergodic, that is the process is
stationary and every invariant random variable of {st}t≥0is equal to a constant with probability 1.
Assumption 2 The constituent functions of the players’ objectives RandLare inL2(square-integrable
functions).
Assumption 3 For any positive scalar c, there exists a scalar µcsuch that for all s∈Sand for any t∈N
we have: E[1 +∥st∥c|s0=s]≤µc(1 +∥s∥c).
Assumption 4 There exists scalars C1andc1such that for any function Jsatisfying|J(s)|≤C2(1 +∥s∥c2)
for some scalars c2andC2we have that:/summationtext∞
t=0|E[J(st)|s0=s]−E[J(s0)]|≤C1C2(1 +∥st∥c1c2).
Assumption 5 There exists scalars candCsuch that for any s∈Swe have that:|J(z,·)|≤C(1 +∥z∥c)
forJ∈{R,L}.
We also make the following finiteness assumption on set of switching control policies for Switcher:
Assumption 6 For any policy gc, the total number of interventions is given by K <∞.
In what follows, we denote by A:=Ait×Areand by Ψ:= Ψit×Ψre×GwhereGis the policy set of the
Switcher’s policy g.
16Under review as submission to TMLR
B Proof of Technical Results
We begin the analysis with some preliminary lemmata and definitions which are useful for proving the main
results.
Definition 1 An operator T:V→Vis said to be a contraction w.r.t a norm∥·∥if there exists a constant
c∈[0,1[such that for any V1,V2∈Vwe have that:
∥TV1−TV2∥≤c∥V1−V2∥. (8)
Definition 2 An operator T:V→Visnon-expansive if∀V1,V2∈Vwe have:
∥TV1−TV2∥≤∥V1−V2∥. (9)
Lemma 1 For anyf:V→R:V→R, we have that:
/vextenddouble/vextenddouble/vextenddouble/vextenddoublemax
a∈Vf(a)−max
a∈Vg(a)/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤max
a∈V∥f(a)−g(a)∥. (10)
Proof 1 We restate the proof given in Mguni (2019):
f(a)−g(a)≤∥f(a)−g(a)∥ (11)
=⇒f(a)≤∥f(a)−g(a)∥+g(a) (12)
=⇒max
a∈Vf(a)≤max
a∈V{∥f(a)−g(a)∥+g(a)}≤max
a∈V∥f(a)−g(a)∥+ max
a∈Vg(a). (13)
Deducting max
a∈Vg(a)from both sides of (13) yields:
max
a∈Vf(a)−max
a∈Vg(a)≤max
a∈V∥f(a)−g(a)∥. (14)
After reversing the roles of fandgand redoing steps (12) - (13), we deduce the desired result since the RHS
of (14) is unchanged.
Lemma 2 The probability transition kernel Pis non-expansive, that is:
∥PV1−PV2∥≤∥V1−V2∥. (15)
Proof 2 The result is well-known e.g. Tsitsiklis & Van Roy (1999). We give a proof using the Tonelli-Fubini
theorem and the iterated law of expectations, we have that:
∥PJ∥2=E/bracketleftbig
(PJ)2[s0]/bracketrightbig
=E/bracketleftig
[E[J[s1]|s0]]2/bracketrightig
≤E/bracketleftbig
E/bracketleftbig
J2[s1]|s0/bracketrightbig/bracketrightbig
=E/bracketleftbig
J2[s1]/bracketrightbig
=∥J∥2,
where we have used Jensen’s inequality to generate the inequality. This completes the proof.
Proof of Theorem 1
We begin by proving the following result:
Proposition 3 Define byRre(st,ait
t,are
t,gt) :=Rre/parenleftbig
st,ait
t,are
t/parenrightbig
−β·gtand consider the following Q learning
variant:
Q2,t+1(st,ait
t,are
t) =Q2,t(st,ait
t,are
t)
+αt(st,are
t)/bracketleftbigg
max/braceleftbigg
Mπ,gQ2,t(st,ait
t,are
t),Rre(st,ait
t,are
t) +γmax
a′∈AQ2,t(st+1,ait
t,a′)/bracerightbigg
−Q2,t(st,ait
t,are
t)/bracketrightbigg
,
then for a fixed Exploiter policyπitand for a fixed L,Q2,t(s)converges to Q∗
2with probability 1, where
st,st+1∈Sandait
t∼πit(·|st)isExploiter’s action.
17Under review as submission to TMLR
Proof 3 Recall that for a function Q:S×A→ R,∀πre,π∈Ψand∀g,∀sτk∈S, we define the intervention
operatorMbyMπre[Qπ′,g(sτk,a)] :=Eaτk∼πre(·|sτk)[R(sτk,aτk)−β+γ/summationtext
s′∈SP(s′;aτk,s)Qπ′,g(s′,aτk)]. We
denote byMQπ,gthe intervention operator acting on Qπ,gwhen the immediate action is chosen according
to an epsilon-greedy policy. Note that Mapplies similarly to the value functions by defining the Bellman
operatorTof acting on the value function vπit,πre
2 :S→Rby
Tv(sτk) := max/braceleftigg
Mv(sτk),/bracketleftigg
Rre(sτk,a) +γmax
a∈A/summationdisplay
s′∈SP(s′;a,sτk)v(s′)/bracketrightigg/bracerightigg
(16)
Our first result proves that the operator Tis a contraction operator. First let us recall that the switching
timeτkis defined recursively τk= inf{t>τk−1|st∈A,τk∈Ft}whereA={s∈S|g(st) = 1}. To this end,
we show that the following bounds holds:
Lemma 3 The Bellman operator Tis a contraction, that is the following bound holds:
∥Tψ−Tψ′∥≤γ∥ψ−ψ′∥.
In what follows and for the remainder of the script, we employ the following shorthands:
Pa
ss′=:/summationdisplay
s′∈SP(s′;a,s),Pπ
ss′=:/summationdisplay
a∈Aπ(a|s)Pa
ss′
To prove that Tis a contraction, we consider the three cases produced by equation 16, that is to say we prove
the following statements:
i)/vextendsingle/vextendsingle/vextendsingle/vextendsingleRre(st,a) +γmax
a∈APa
s′stv(s′)−/parenleftbigg
Rre(st,a) +γmax
a∈APa
s′stv′(s′)/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤γ∥v−v′∥
ii)∥Mv−Mv′∥≤γ∥v−v′∥, (and henceMis a contraction).
iii)/vextenddouble/vextenddouble/vextenddouble/vextenddoubleMv−/bracketleftbigg
Rre(·,a) +γmax
a∈APav′/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤γ∥v−v′∥.
We begin by proving i).
Indeed, for any ait∈Aitand∀st∈S,∀s′∈Swe have that
/vextendsingle/vextendsingle/vextendsingle/vextendsingleRre(sτk,a) +γPπ
s′stv(s′)−/bracketleftbigg
Rre(sτk,a) +γmax
a∈APa
s′stv′(s′)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤max
a∈A/vextendsingle/vextendsingleγPa
s′stv(s′)−γPa
s′stv′(s′,·)/vextendsingle/vextendsingle
≤γ∥Pv−Pv′∥
≤γ∥v−v′∥,
again using the fact that Pis non-expansive and Lemma 1.
We now prove ii).
For anyτ∈F, define by τ′= inf{t > τ|st∈A,τ∈Ft}. Now using the definition of Mwe have that for
anysτ∈S
|(Mv−Mv′)(sτ)|
≤max
aτ∈A/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleRre(sτ,aτ)−β1Are(are
t) +γPπ
s′sτPav(sτ)−/parenleftbig
Rre(sτ,aτ)−β1Are(are
t) +γPπ
s′sτPav′(sτ)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=γ/vextendsingle/vextendsinglePπ
s′sτPav(sτ)−Pπ
s′sτPav′(sτ)/vextendsingle/vextendsingle
≤γ∥Pv−Pv′∥
≤γ∥v−v′∥,
18Under review as submission to TMLR
using the fact that Pis non-expansive. The result can then be deduced easily by applying max on both sides.
We now prove iii). We split the proof of the statement into two cases:
Case 1:
Mv(sτ)−/parenleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)/parenrightbigg
<0. (17)
We now observe the following:
Mv(sτ)−Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)
≤max/braceleftbig
Rre(sτ,aτ) +γPπ
s′sτPav(s′),Mv(sτ)/bracerightbig
−Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemax/braceleftbig
Rre(sτ,aτ) +γPπ
s′sτPav(s′),Mv(sτ)/bracerightbig
−max/braceleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′),Mv(sτ)/bracerightbigg
+ max/braceleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′),Mv(sτ)/bracerightbigg
−Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemax/braceleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv(s′),Mv(sτ)/bracerightbigg
−max/braceleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′),Mv(sτ)/bracerightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemax/braceleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′),Mv(sτ)/bracerightbigg
−Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤γmax
a∈A/vextendsingle/vextendsinglePπ
s′sτPav(s′)−Pπ
s′sτPav′(s′)/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingle/vextendsinglemax/braceleftbigg
0,Mv(sτ)−/parenleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)/parenrightbigg/bracerightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤γ∥Pv−Pv′∥
≤γ∥v−v′∥,
where we have used the fact that for any scalars a,b,cwe have that|max{a,b}−max{b,c}|≤|a−c|and
the non-expansiveness of P.
Case 2:
Mv(sτ)−/parenleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)/parenrightbigg
≥0.
For this case, we have that
Mv(sτ)−/parenleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)/parenrightbigg
≤Mv(sτ)−/parenleftbigg
Rre(sτ,aτ) +γmax
a∈APa
s′sτv′(s′)/parenrightbigg
+β1Are(are
t)
≤Rre(sτ,aτ)−β1Are(are
t) +γPπ
s′sτPav(s′)
−/parenleftbigg
Rre(sτ,aτ)−β1Are(are
t) +γmax
a∈APa
s′sτv′(s′)/parenrightbigg
≤γmax
a∈A/vextendsingle/vextendsinglePπ
s′sτPa(v(s′)−v′(s′))/vextendsingle/vextendsingle
≤γ|v(s′)−v′(s′)|
≤γ∥v−v′∥,
19Under review as submission to TMLR
again using the fact that Pis non-expansive. Hence we have succeeded in showing that for any v,v′∈L2we
have that
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleMv−max
a∈A[R(·,a) +γPav′]/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤γ∥v−v′∥. (18)
Gathering the results of the three cases gives the desired result.
We now make use of the following result:
Theorem 2 (Theorem 1, pg 4 in Jaakkola et al. (1994)) LetΞt(s)be a random process that takes
values in Rnand given by the following:
Ξ2,t+1(s) = (1−αt(s)) Ξ2,t(s)αt(s)Lt(s), (19)
then Ξt(s)converges to 0with probability 1under the following conditions:
i)0≤αt≤1,/summationtext
tαt=∞and/summationtext
tαt<∞
ii)∥E[Lt|Ft]∥≤γ∥Ξt∥, withγ <1;
iii)Var [Lt|Ft]≤c(1 +∥Ξt∥2)for somec>0.
Proof 4 To prove the convergence in Theorem 1, we show (i) - (iii) hold. Condition (i) holds by choice of
learning rate. It therefore remains to prove (ii) - (iii). We first prove (ii). For this, we consider our variant
of the Q-learning update rule:
Q2,t+1(st,at) =Q2,t(st,at) +αt(st,at)/bracketleftbigg
max/braceleftbigg
MπQ2,t(sτk,a),Rre(sτk,a) +γmax
a′∈AQ2,t(s′,a′)/bracerightbigg
−Q2,t(st,at)/bracketrightbigg
.
After subtracting Q⋆
2(st,at)from both sides and some manipulation we obtain that:
Ξ2,t+1(st,at) = (1−αt(st,at))Ξ2,t(st,at)
+αt(st,at)/bracketleftbigg
max/braceleftbigg
MπQ2,t(sτk,a),Rre(sτk,a) +γmax
a′∈AQ2,t(s′,a′)/bracerightbigg
−Q⋆
2(st,at)/bracketrightbigg
,
where Ξ2,t(st,at) :=Q2,t(st,at)−Q⋆
2,t(st,at).
Let us now define by
Lt(sτk,a) := max/braceleftbigg
MπQ2,t(sτk,a),Rre(sτk,a) +γmax
a′∈AQ2,t(s′,a′)/bracerightbigg
−Q⋆
2(st,a).
Then
Ξ2,t+1(st,at) = (1−αt(st,at))Ξ2,t(st,at) +αt(st,at)) [Lt(sτk,a)]. (20)
We now observe that
E[Lt(sτk,a)|Ft] =/summationdisplay
s′∈SP(s′;a,sτk) max/braceleftbigg
MπQ2,t(sτk,a),Rre(sτk,a) +γmax
a′∈AQ2,t(s′,a′)/bracerightbigg
−Q⋆
2(sτk,a)
=TQ2,t(s,a)−Q⋆
2(s,a). (21)
Now, using the fixed point property that implies Q⋆
2=TQ⋆
2, we find that
E[Lt(sτk,a)|Ft] =TQ2,t(s,a)−TQ⋆
2(s,a)
≤∥TQ2,t−TQ⋆
2∥
≤γ∥Q2,t−Q⋆
2∥∞=γ∥Ξt∥∞. (22)
20Under review as submission to TMLR
using the contraction property of Testablished in Lemma 3. This proves (ii).
We now prove iii), that is
Var [Lt|Ft]≤c(1 +∥Ξt∥2). (23)
Now by equation 21 we have that
Var [Lt|Ft] = Var/bracketleftbigg
max/braceleftbigg
MπQ2(sτk,a),Rre(sτk,a) +γmax
a′∈AQ2,t(s′,a′)/bracerightbigg
−Q⋆
2(st,a)/bracketrightbigg
=E/bracketleftigg/parenleftigg
max/braceleftbigg
MπQ2(sτk,a),Rre(sτk,a) +γmax
a′∈AQ2,t(s′,a′)/bracerightbigg
−Q⋆
2(st,a)−(TQ2,t(s,a)−Q⋆
2(s,a))/parenrightigg2/bracketrightigg
=E/bracketleftigg/parenleftbigg
max/braceleftbigg
MπQ2(sτk,a),Rre(sτk,a) +γmax
a′∈AQ2,t(s′,a′)/bracerightbigg
−TQ2,t(s,a)/parenrightbigg2/bracketrightigg
= Var/bracketleftbigg
max/braceleftbigg
MπQ2(sτk,a),Rre(sτk,a) +γmax
a′∈AQ2,t(s′,a′)/bracerightbigg
−TQ2,t(s,a))2/bracketrightbigg
≤c(1 +∥Ξt∥2),
for somec>0where the last line follows due to the boundedness of Q2(which follows from Assumptions 2
and 4).
This concludes the proof of Prop. 3.
Proof of Proposition 2
Proof 5 (Proof of Prop. 2) The proof is given by establishing a contradiction. Therefore suppose that
Mπ,πrevπit,π′re
2 (sτk)≤vπit,π′re
2 (sτk)and suppose that the intervention time τ′
1>τ1is an optimal intervention
time. Construct the policy π′re∈Ψreand˜πrepolicy switching times by (τ′
0,τ′
1,..., )andπ′re∈Ψrepolicy by
(τ′
0,τ1,...)respectively. Define by l= inf{t >0;Mπ,πrevπit,π′re
2 (st) =vπit,π′re
2 (st)}andm= sup{t;t < τ′
1}.
By construction we have that
vπit,π′re
2 (s)
=E/bracketleftig
L(s0,a0) +E/bracketleftig
...+γl−1E/bracketleftig
L(sτ1−1,aτ1−1) +...+γm−l−1E/bracketleftig
L(sτ′
1−1,aτ′
1−1) +γMπit,π′revπit,π′re
2 (s′)/bracketrightig/bracketrightig/bracketrightig/bracketrightig
<E/bracketleftig
L(s0,a0) +E/bracketleftig
...+γl−1E/bracketleftig
L(sτ1−1,aτ1−1) +γMπit,˜πrevπit,π′re
2 (sτ1)/bracketrightig/bracketrightig/bracketrightig
We now use the following observation E/bracketleftig
L(sτ1−1,aτ1−1) +γMπit,˜πrevπit,π′re
2 (sτ1)/bracketrightig
≤max/braceleftbigg
Mπit,˜πrevπit,π′re
2 (sτ1),max
aτ1∈A/bracketleftig
L(sτ1,aτ1) +γ/summationtext
s′∈SP(s′;aτ1,sτ1)vπit,πre
2 (s′)/bracketrightig/bracerightbigg
.
Using this we deduce that
vπit,π′re
2 (s)≤E/bracketleftigg
L(s0,a0) +E/bracketleftigg
...
+γl−1E/bracketleftigg
−L(sτ1−1,aτ1−1) +γmax/braceleftigg
Mπit,˜πrevπit,π′re
2 (sτ1),max
aτ1∈A/bracketleftigg
L(sτ1,aτ1) +γ/summationdisplay
s′∈SP(s′;aτ1,sτ1)vπit,πre
2 (s′)/bracketrightigg/bracerightigg/bracketrightigg/bracketrightigg/bracketrightigg
=E/bracketleftig
L(s0,a0) +E/bracketleftig
...+γl−1E/bracketleftig
L(sτ1−1,aτ1−1) +γ/bracketleftig
Tvπit,˜πre
2/bracketrightig
(sτ1)/bracketrightig/bracketrightig/bracketrightig
=vπit,˜πre
2 (s)),
21Under review as submission to TMLR
where the first inequality is true by assumption on M. This is a contradiction since π′reis an optimal
policy for Player 2. Using analogous reasoning, we deduce the same result for τ′
k< τkafter which de-
duce the result. Moreover, by invoking the same reasoning, we can conclude that it must be the case that
(τ0,τ1,...,τk−1,τk,τk+1,..., )are the optimal switching times, this completes the proof of part (i).
We now prove part (ii). First, we note that it is easy to see that vπit,(πre,g)
2 is bounded above, indeed using
the above we have that
vπit,(πre,g)
2 (s) =E
/summationdisplay
t≥0γt/parenleftbig
Rre/parenleftbig
st,ait
t,are
t/parenrightbig
−βgt/parenrightbig
 (24)
=E/bracketleftigg∞/summationdisplay
t=0γt/parenleftbig
(L(s,are
t)1Are(are
t) +L(st,ait)(1−gt−βgt/parenrightbig/bracketrightigg
(25)
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleEπ,πre/bracketleftigg∞/summationdisplay
t=0γt(2L−βgt)/bracketrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(26)
≤∞/summationdisplay
t=0γt(2∥L∥+K) (27)
=1
1−γ(2∥L∥+K), (28)
using the triangle inequality, the (upper-)boundedness of L(Assumption 5). We now note that by the domi-
nated convergence theorem we have that ∀(s0)∈S×{ 0,1}
lim
l→0vπit,(πre,g)
2 (s) = lim
l→0E
/summationdisplay
t≥0γt/parenleftbig
Rre/parenleftbig
st,ait
t,are
t/parenrightbig
−βgt/parenrightbig
 (29)
= lim
l→0E/bracketleftigg∞/summationdisplay
t=0γt/parenleftbig
L(s,are
t)gt+L(st,ait)(1−gt)−βgt/parenrightbig/bracketrightigg
(30)
=E/bracketleftigg
lim
l→0∞/summationdisplay
t=0γt/parenleftbig
L(s,are
t)gt+L(st,ait)(1−gt)−βgt/parenrightbig/bracketrightigg
(31)
=−βE/bracketleftigg∞/summationdisplay
t=0γtgt/bracketrightigg
, (32)
using Assumption 6 in the last step, after which we deduce (ii) since equation 32 is maximised when gt= 0
for allt= 0,1,...which is achieved only when µl(g) = 0. Additionally, by part (i) we have that
τk= inf/braceleftig
τ >τk−1|MΨrevπit,Ψre
2 =vπit,Ψre
2/bracerightig
. (33)
It is easy to see that given equation 32 and the definition of M(c.f. Proposition 3), condition equation 33
can never be satisfied which implies that Switcher performs no interventions.
This completes the proof of Prop. 2.
To complete the proof of Theorem 1, we prove the following result:
Lemma 4 The Explorerlearns to solve the MDP ⟨S,A,P,R,γ⟩and its value function converges.
22Under review as submission to TMLR
Proof 6 We first deduce the boundedness of the Exploiter objectivevπit,(πre,g)
1:
vπit,(πre,g)
1 (s) =E
/summationdisplay
t≥0γt
1Rit/parenleftbig
st,ait
t,are
t/parenrightbig

=E
/summationdisplay
t≥0γt
1/parenleftbig
R(st,ait
t)(1−gt) +R(s,are
t)gt/parenrightbig

≤E
/summationdisplay
t≥0γt
1/parenleftbig
R(st,ait
t) +R(s,are
t)/parenrightbig

≤2
1−γ1∥R∥,
therefore the Explorer’s objective is bounded above by some finite quantity.
Using the kronecker-delta function, the Exploiter objective as:
vπit,(πre,g)
1 (s) =E
/summationdisplay
t≥0µl(g)/summationdisplay
k=0γt
1R(st,ait
t)(1−δt
τk) +R(s,are
t)δt
τk
. (34)
Recall that µl(g)denotes the number of switch activations performed by the Switcher. Denote by g0the
Explorerintervention policy that performs no interventions. By Prop. 2 and by the dominated convergence
theorem, we have that ∀s∈S
lim
l→0vπit,(πre,g)
1 (s) = lim
l→0EP,πit,(πre,g)
/summationdisplay
t≥0µl(g)/summationdisplay
k=0γt
1R(st,ait
t)(1−δt
τk) +R(s,are
t)δt
τk

=EP,πit,(πre,g)lim
l→0
/summationdisplay
t≥0µl(g)/summationdisplay
k=0γt
1R(st,ait
t)(1−δt
τk) +R(s,are
t)δt
τk

=EP,πit,(πre,g0)
/summationdisplay
t≥0γt
1R(st,ait
t)

=EP,πit
/summationdisplay
t≥0γt
1R(st,ait
t)
=vπit
1(s),
using the fact that lim
l→0µl= 0and by Fubini’s theorem in the penultimate step. Therefore, in the limit l→0,
theExploiter solves the MDP⟨S,A,P,R,γ⟩which converges to a stable point.
Corollary 1 After combining Lemma 4, Prop. 2 and Prop. 3 we deduce the result of Theorem 1.
Proof of Convergence with Function Approximation
First let us recall the statement of the theorem:
Theorem 3 SEREN converges to a limit point r⋆which is the unique solution to the equation:
ΠF(Φr⋆) = Φr⋆,a.e. (35)
where we recall that for any test function Λ∈V, the operator Fis defined by FΛ := Θ +γPmax{MΛ,Λ}.
Moreover,r⋆satisfies the following:
∥Φr⋆−Q⋆
2∥≤c∥ΠQ⋆
2−Q⋆
2∥. (36)
23Under review as submission to TMLR
The theorem is proven using a set of results that we now establish. To this end, we first wish to prove the
following bound:
Lemma 5 For anyQ∈Vwe have that
∥FQ2−Q′
2∥≤γ∥Q2−Q′
2∥, (37)
so that the operator Fis a contraction.
Proof 7 Recall, for any test function ψ, a projection operator Πacting Λis defined by the following
ΠΛ := arg min
¯Λ∈{Φr|r∈Rp}/vextenddouble/vextenddouble¯Λ−Λ/vextenddouble/vextenddouble.
Now, we first note that in the proof of Lemma 3, we deduced that for any Λ∈L2we have that
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleMπΛ−/bracketleftbigg
R(·,a) +γmax
a∈APaΛ′/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤γ∥Λ−Λ′∥,
(c.f. Lemma 3).
Setting Λ =Q2andψ=R, it can be straightforwardly deduced that for any Q2,ˆQ2∈L2:/vextenddouble/vextenddouble/vextenddoubleMπQ2−ˆQ2/vextenddouble/vextenddouble/vextenddouble≤
γ/vextenddouble/vextenddouble/vextenddoubleQ2−ˆQ2/vextenddouble/vextenddouble/vextenddouble. Hence, using the contraction property of M, we readily deduce the following bound:
max/braceleftig/vextenddouble/vextenddouble/vextenddoubleMπQ2−ˆQ2/vextenddouble/vextenddouble/vextenddouble,/vextenddouble/vextenddouble/vextenddoubleMπQ2−M ˆQ2/vextenddouble/vextenddouble/vextenddouble/bracerightig
≤γ/vextenddouble/vextenddouble/vextenddoubleQ2−ˆQ2/vextenddouble/vextenddouble/vextenddouble, (38)
We now observe that Fis a contraction. Indeed, since for any Q2,Q′
2∈L2we have that:
∥FQ2−FQ′
2∥=∥Θ +γPmax{MπQ2,Q2}−(Θ +γPmax{MπQ′
2,Q′
2})∥
=γ∥Pmax{MπQ2,Q2}−Pmax{MπQ′
2,Q′
2}∥
≤γ∥max{MπQ2,Q2}−max{MπQ′
2,Q′
2}∥
≤γ∥max{MπQ2−MπQ′
2,Q2−MπQ′
2,MπQ2−Q′
2,Q2−Q′
2}∥
≤γmax{∥MπQ2−MπQ′
2∥,∥Q2−MπQ′
2∥,∥MπQ2−Q′
2∥,∥Q2−Q′
2∥}
=γ∥Q2−Q′
2∥,
using equation 38 and again using the non-expansiveness of P.
We next show that the following two bounds hold:
Lemma 6 For anyQ2∈Vwe have that
i)/vextenddouble/vextenddoubleΠFQ2−ΠF¯Q2/vextenddouble/vextenddouble≤γ/vextenddouble/vextenddoubleQ2−¯Q2/vextenddouble/vextenddouble,
ii)∥Φr⋆−Q⋆
2∥≤1√
1−γ2∥ΠQ⋆
2−Q⋆
2∥.
Proof 8 The first result is straightforward since as Πis a projection it is non-expansive and hence:
/vextenddouble/vextenddoubleΠFQ2−ΠF¯Q2/vextenddouble/vextenddouble≤/vextenddouble/vextenddoubleFQ2−F¯Q2/vextenddouble/vextenddouble≤γ/vextenddouble/vextenddoubleQ2−¯Q2/vextenddouble/vextenddouble,
using the contraction property of F. This proves i). For ii), we note that by the orthogonality property of
projections we have that ⟨Φr⋆−ΠQ⋆
2,Φr⋆−ΠQ⋆
2⟩, hence we observe that:
∥Φr⋆−Q⋆
2∥2=∥Φr⋆−ΠQ⋆
2∥2+∥Φr⋆−ΠQ⋆
2∥2
=∥ΠFΦr⋆−ΠQ⋆
2∥2+∥Φr⋆−ΠQ⋆
2∥2
≤∥FΦr⋆−Q⋆
2∥2+∥Φr⋆−ΠQ⋆
2∥2
=∥FΦr⋆−FQ⋆
2∥2+∥Φr⋆−ΠQ⋆
2∥2
≤γ2∥Φr⋆−Q⋆
2∥2+∥Φr⋆−ΠQ⋆
2∥2,
after which we readily deduce the desired result.
24Under review as submission to TMLR
Lemma 7 Define the operator Hby the following: HQ 2(z) =/braceleftigg
MπQ2(z),ifMπQ2(s)>Φr⋆,
Q2(z),otherwise,
and˜Fby:˜FQ2:=R+γPHQ 2.
For anyQ2,¯Q2∈L2we have that
/vextenddouble/vextenddouble˜FQ2−˜F¯Q2/vextenddouble/vextenddouble≤γ/vextenddouble/vextenddoubleQ2−¯Q2/vextenddouble/vextenddouble (39)
and hence ˜Fis a contraction mapping.
Proof 9 Using equation 38, we now observe that
/vextenddouble/vextenddouble˜FQ2−˜F¯Q2/vextenddouble/vextenddouble=/vextenddouble/vextenddoubleR+γPHQ 2−/parenleftbig
R+γPH ¯Q2/parenrightbig/vextenddouble/vextenddouble
≤γ/vextenddouble/vextenddoubleHQ 2−H¯Q2/vextenddouble/vextenddouble
≤γ/vextenddouble/vextenddoublemax/braceleftbig
MπQ2−M ¯Q2,Q2−¯Q2,MπQ2−¯Q2,M¯Q2−Q2/bracerightbig/vextenddouble/vextenddouble
≤γmax/braceleftbig/vextenddouble/vextenddoubleMπQ2−M ¯Q2/vextenddouble/vextenddouble,/vextenddouble/vextenddoubleQ2−¯Q2/vextenddouble/vextenddouble,/vextenddouble/vextenddoubleMπQ2−¯Q2/vextenddouble/vextenddouble,/vextenddouble/vextenddoubleM¯Q2−Q2/vextenddouble/vextenddouble/bracerightbig
≤γmax/braceleftbig
γ/vextenddouble/vextenddoubleQ2−¯Q2/vextenddouble/vextenddouble,/vextenddouble/vextenddoubleQ2−¯Q2/vextenddouble/vextenddouble,/vextenddouble/vextenddoubleMπQ2−¯Q2/vextenddouble/vextenddouble,/vextenddouble/vextenddoubleM¯Q2−Q2/vextenddouble/vextenddouble/bracerightbig
=γ/vextenddouble/vextenddoubleQ2−¯Q2/vextenddouble/vextenddouble,
again using the non-expansive property of P.
Lemma 8 Define by ˜Q2:=Rre+γPv˜π
2where
v˜π
2(s) :=R2(sτk,a) +γmax
a∈A/summationdisplay
s′∈SP(s′;a,sτk)Φr⋆(s′), (40)
then ˜Q2is a fixed point of ˜F˜Q2, that is ˜F˜Q2=˜Q2.
Proof 10 We begin by observing that
H˜Q2(z) =H/parenleftbig
L(z) +γPv˜π
2/parenrightbig
=/braceleftigg
MπQ2(z),ifMπQ2(z)>Φr⋆,
Q2(z),otherwise,
=/braceleftigg
MπQ2(z),ifMπQ2(z)>Φr⋆,
L(z) +γPv˜π
2,otherwise,
=v˜π
2(s).
Hence,
˜F˜Q2=R+γPH ˜Q2=R+γPv˜π
2=˜Q2. (41)
which proves the result.
Lemma 9 The following bound holds:
E/bracketleftbig
vˆπ
2(s0)/bracketrightbig
−E/bracketleftbig
v˜π
2(s0)/bracketrightbig
≤2/bracketleftig
(1−γ)/radicalbig
(1−γ2)/bracketrightig−1
∥ΠQ⋆
2−Q⋆
2∥. (42)
Proof 11 By definitions of vˆπ
2andv˜π
2(c.f equation 40) and using Jensen’s inequality and the stationarity
property we have that,
E/bracketleftbig
vˆπ
2(s0)/bracketrightbig
−E/bracketleftbig
v˜π
2(s0)/bracketrightbig
=E/bracketleftbig
Pvˆπ
2(z0)/bracketrightbig
−E/bracketleftbig
Pv˜π
2(z0)/bracketrightbig
≤/vextendsingle/vextendsingleE/bracketleftbig
Pvˆπ
2(z0)/bracketrightbig
−E/bracketleftbig
Pv˜π
2(z0)/bracketrightbig/vextendsingle/vextendsingle
≤/vextenddouble/vextenddoublePvˆπ
2−Pv˜π
2/vextenddouble/vextenddouble. (43)
25Under review as submission to TMLR
Now recall that ˜Q2:=R+γPv˜π
2andQ⋆
2:=R+γPvπ⋆
2, using these expressions in equation 43 we find that
E/bracketleftbig
vˆπ
2(z0)/bracketrightbig
−E/bracketleftbig
v˜π
2(z0)/bracketrightbig
≤1
γ/vextenddouble/vextenddouble˜Q2−Q⋆
2/vextenddouble/vextenddouble.
Moreover, by the triangle inequality and using the fact that F(Φr⋆) = ˜F(Φr⋆)and that FQ⋆
2=Q⋆
2and
F˜Q2=˜Q2(c.f. equation 42) we have that
/vextenddouble/vextenddouble˜Q2−Q⋆
2/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble˜Q2−F(Φr⋆)/vextenddouble/vextenddouble+/vextenddouble/vextenddoubleQ⋆
2−˜F(Φr⋆)/vextenddouble/vextenddouble
≤γ/vextenddouble/vextenddouble˜Q2−Φr⋆/vextenddouble/vextenddouble+γ∥Q⋆
2−Φr⋆∥
≤2γ/vextenddouble/vextenddouble˜Q2−Φr⋆/vextenddouble/vextenddouble+γ/vextenddouble/vextenddoubleQ⋆
2−˜Q2/vextenddouble/vextenddouble,
which gives the following bound:
/vextenddouble/vextenddouble˜Q2−Q⋆
2/vextenddouble/vextenddouble≤2 (1−γ)−1/vextenddouble/vextenddouble˜Q2−Φr⋆/vextenddouble/vextenddouble,
from which, using Lemma 6, we deduce that/vextenddouble/vextenddouble˜Q2−Q⋆
2/vextenddouble/vextenddouble≤2/bracketleftig
(1−γ)/radicalbig
(1−γ2)/bracketrightig−1/vextenddouble/vextenddouble˜Q2−Φr⋆/vextenddouble/vextenddouble, after which
by equation 44, we finally obtain
E/bracketleftbig
vˆπ
2(s0)/bracketrightbig
−E/bracketleftbig
v˜π
2(s0)/bracketrightbig
≤2/bracketleftig
(1−γ)/radicalbig
(1−γ2)/bracketrightig−1/vextenddouble/vextenddouble˜Q2−Φr⋆/vextenddouble/vextenddouble,
as required.
Let us rewrite the update in the following way:
rt+1=rt+γtΞ2(wt,rt),
where the function Ξ2:R2d×Rp→Rpis given by:
Ξ2(w,r) :=ϕ(z) (L(z) +γmax{(Φr)(z′),M(Φr)(z′)}−(Φr)(s)),
for anyw≡(z,z′)∈(N×S)2wherez= (t,s)∈N×Sandz′= (t,s′)∈N×Sand for any r∈Rp. Let us
also define the function Ξ2:Rp→Rpby the following:
Ξ2(r) :=Ew0∼(P,P)[Ξ2(w0,r)] ;w0:= (z0,z1).
Lemma 10 The following statements hold for all z∈{0,1}×S:
i)(r−r⋆)Ξ2,k(r)<0,∀r̸=r⋆,
ii)Ξ2,k(r⋆) = 0.
Proof 12 To prove the statement, we first note that each component of Ξ2,k(r)admits a representation as
an inner product, indeed:
Ξ2,k(r) =E[ϕk(z0)(L(z0) +γmax{Φr(z1),MπΦ(z1)}−(Φr)(z0)]
=E[ϕk(z0)(L(z0) +γE[max{Φr(z1),MπΦ(z1)}|z0]−(Φr)(z0)]
=E[ϕk(z0)(L(z0) +γPmax{(Φr,MπΦ)}(z0)−(Φr)(z0)]
=⟨ϕk,FΦr−Φr⟩,
using the iterated law of expectations and the definitions of PandF.
We now are in position to prove i). Indeed, we now observe the following:
(r−r⋆)Ξ2,k(r) =/summationdisplay
l=1(r(l)−r⋆(l))⟨ϕl,FΦr−Φr⟩
=⟨Φr−Φr⋆,FΦr−Φr⟩
=⟨Φr−Φr⋆,(1−Π)FΦr+ ΠFΦr−Φr⟩
=⟨Φr−Φr⋆,ΠFΦr−Φr⟩,
26Under review as submission to TMLR
where in the last step we used the orthogonality of (1−Π). We now recall that ΠFΦr⋆= Φr⋆since Φr⋆is a
fixed point of ΠF. Additionally, using Lemma 6 we observe that ∥ΠFΦr−Φr⋆∥≤γ∥Φr−Φr⋆∥. With this
we now find that
⟨Φr−Φr⋆,ΠFΦr−Φr⟩
=⟨Φr−Φr⋆,(ΠFΦr−Φr⋆) + Φr⋆−Φr⟩
≤∥Φr−Φr⋆∥∥ΠFΦr−Φr⋆∥−∥ Φr⋆−Φr∥2
≤(γ−1)∥Φr⋆−Φr∥2,
which is negative since γ <1which completes the proof of part i).
The proof of part ii) is straightforward since we readily observe that
Ξ2,k(r⋆) =⟨ϕl,FΦr⋆−Φr⟩=⟨ϕl,ΠFΦr⋆−Φr⟩= 0,
as required and from which we deduce the result.
To prove the theorem, we make use of a special case of the following result:
Theorem 3 (Th. 17, p. 239 in Benveniste et al. (2012)) Consider a stochastic process rt:R×
{∞}× Ω→Rkwhich takes an initial value r0and evolves according to the following:
rt+1=rt+αΞ2(st,rt), (44)
for some function s:R2d×Rk→Rkand where the following statements hold:
1.{st|t= 0,1,...}is a stationary, ergodic Markov process taking values in R2d
2. For any positive scalar q, there exists a scalar µqsuch that E[1 +∥st∥q|s≡s0]≤µq(1 +∥s∥q)
3. The step size sequence satisfies the Robbins-Monro conditions, that is/summationtext∞
t=0αt=∞and/summationtext∞
t=0α2
t<
∞
4. There exists scalars candqsuch that∥Ξ2(w,r)∥≤c(1 +∥w∥q) (1 +∥r∥)
5. There exists scalars candqsuch that/summationtext∞
t=0∥E[Ξ2(wt,r)|z0≡z]−E[Ξ2(w0,r)]∥≤c(1 +∥w∥q) (1+
∥r∥)
6. There exists a scalar c>0such that∥E[Ξ2(w0,r)]−E[Ξ2(w0,¯r)]∥≤c∥r−¯r∥
7. There exists scalars c>0andq >0such that/summationtext∞
t=0∥E[Ξ2(wt,r)|w0≡w]−E[Ξ2(w0,¯r)]∥≤c∥r−
¯r∥(1 +∥w∥q)
8. There exists some r⋆∈Rksuch that Ξ2(r)(r−r⋆)<0for allr̸=r⋆and¯s(r⋆) = 0.
Thenrtconverges to r⋆almost surely.
In order to apply the Theorem 3, we show that conditions 1 - 7 are satisfied.
Conditions 1-2 are true by assumption while condition 3 can be made true by choice of the learning rates.
Therefore it remains to verify conditions 4-7 are met.
To prove 4, we observe that
∥Ξ2(w,r)∥=∥ϕ(s) (L(z) +γmax{(Φr)(z′),MπΦ(z′)}−(Φr)(z))∥
≤∥ϕ(z)∥∥L(z) +γ(∥ϕ(z′)∥∥r∥+MπΦ(z′))∥+∥ϕ(z)∥∥r∥
≤∥ϕ(z)∥(∥L(z)∥+γ∥MπΦ(z′)∥) +∥ϕ(z)∥(γ∥ϕ(z′)∥+∥ϕ(z)∥)∥r∥.
27Under review as submission to TMLR
Now using the definition of M, we readily observe that ∥MπΦ(z′)∥≤∥R∥ +γ∥Pπ
s′stΦ∥≤∥R∥ +γ∥Φ∥using
the non-expansiveness of P.
Hence, we lastly deduce that
∥Ξ2(w,r)∥≤∥ϕ(z)∥(∥L(z)∥+γ∥MπΦ(z′)∥) +∥ϕ(z)∥(γ∥ϕ(z′)∥+∥ϕ(z)∥)∥r∥
≤∥ϕ(z)∥(∥L(z)∥+γ∥R∥+γ∥ψ∥) +∥ϕ(z)∥(γ∥ϕ(z′)∥+∥ϕ(z)∥)∥r∥,
we then easily deduce the result using the boundedness of ϕ,Randψ.
Now we observe the following Lipschitz condition on Ξ2:
∥Ξ2(w,r)−Ξ2(w,¯r)∥
=∥ϕ(z) (γmax{(Φr)(z′),MπΦ(z′)}−γmax{(Φ¯r)(z′),MπΦ(z′)})−((Φr)(z)−Φ¯r(z))∥
≤γ∥ϕ(z)∥∥max{ϕ′(z′)r,MπΦ′(z′)}−max{(ϕ′(z′)¯r),MπΦ′(z′)}∥+∥ϕ(z)∥∥ϕ′(z)r−ϕ(z)¯r∥
≤γ∥ϕ(z)∥∥ϕ′(z′)r−ϕ′(z′)¯r∥+∥ϕ(z)∥∥ϕ′(z)r−ϕ′(z)¯r∥
≤∥ϕ(z)∥(∥ϕ(z)∥+γ∥ϕ(z)∥∥ϕ′(z′)−ϕ′(z′)∥)∥r−¯r∥
≤c∥r−¯r∥,
using Cauchy-Schwarz inequality and that for any scalars a,b,cwe have that|max{a,b}−max{b,c}| ≤
|a−c|.
Using Assumptions 3 and 4, we therefore deduce that
∞/summationdisplay
t=0∥E[Ξ2(w,r)−Ξ2(w,¯r)|w0=w]−E[Ξ2(w0,r)−Ξ2(w0,¯r)∥]≤c∥r−¯r∥(1 +∥w∥l).(45)
Part 2 is assured by Lemma 6 while Part 4 is assured by Lemma 9 and lastly Part 8 is assured by Lemma
10.
To complete the proof of Theorem 1, we make use of Theorem 1.1. in Borkar (1997) in which case we readily
verify that with the appropriate choices of timesteps the Theorem is readily satisfied.
C Alternative uncertainty measures
•Model-Based Ensemble Disagreement. By employing an ensemble of dynamics models,
{M 1,...,MM}, where eachMm∈ Fform= 1,...,M. Model training entails independent training
of each of the model in the ensemble with the identical objectives (e.g., minimising the L2 distance between
the predicted and the ground-truth next states). The uncertainty about a state-action pair (s,a), can be
quantified as the predictive ensemble disagreement:
L(s,a) =1
M−1M/summationdisplay
m=1(Mm(s,a)−EM(s,a)) (46)
whereEM(s,a) =1
M/summationtextM
m=1Me(s,a)is the empirical mean of the ensemble predictions. This approach
has a information-theoretic interpretation such that through training, the mutual information between the
dynamics model parameters and next-state is maximised, hence relating the epistemic uncertainty with the
information-theoretic framework.
•Integrating control into dynamics modelling with LSSM. Consider we embed the dynamics mod-
elling problem into a sequential modelling problem using latent state-space models (LSSM), using amortised
inference, we are able to achieve fast inference and learning of the probabilistic graphical model. We could
additionally incorporate action into the LSSM as a global factor that (potentially) influences both the latent
28Under review as submission to TMLR
and observable codes. For instance, the generative process could be modelled as:
p(x1:T,z1:T,a1:T−1) =p(z1)p(x1|z1)·
·T/productdisplay
t=2p(zt|zt−1,at−1)p(xt|zt)p(at−1|zt−1,xt−1).
We could easily train an LSSM by maximising the variational lower bound utilising amortised inference. In
the meantime, we could quantify the model uncertainty about the state-action pair (zt,at)in terms of the
variance of the latent predictive distributions. By random trajectory-sampling (multiple particles), we target
regions of the action space that maximises the predictive variance (assuming Gaussian for now). This could
be achieved by importance-weighting on the computation of the marginal variance. Hence in this case we
use the following uncertainty instantiation:
L(s,a) =V(s′|s,a). (47)
We could consider equation 47 as a parametric generalisation of equation 46 (despite the fact that in the
model-ensemble method, the action is taken as an external input instead of a random variable as in the
LSSM method).
D Implementation Details
The reinforcement learning agents (both SEREN and related baselines) are implemented in PyTorch (Paszke
et al., 2019), and our implementations of baseline agents (SAC) are based on Stable-Baselines 3 (Raffin et al.,
2021).
D.1 MuJoCo Tasks
For SEREN-SAC, we use MLPs as the function approximator, with Adam optimiser (Kingma & Ba, 2014).
WeshowtheimplementationdetailsoftheSEREN-SACagentinTable2thatareusedinallstudiedMuJoCo
environments. We note that the baseline SAC agent is implemented using exactly the same hyperparameters
asExploiter agent in SEREN-SAC.
D.2 Atari 100K Benchmarks
For the Atari 100K Benchmarks, we use the Efficient-DQN structure as the baseline algorithm (Kostrikov
et al., 2020), which augments a standard DQN with double Q-learning (Van Hasselt et al., 2016), dueling
network for value estimation (Wang et al., 2016), and multi-step return as the TD target (Mnih et al., 2016).
We compare our model, SEREN-Eff-DQN, against the following baseline algorithms: Rainbow (Hessel et al.,
2018), Data-EfficientRainbow(Efficient-Rainbow; VanHasseltetal.(2019)), Efficient-DQN,DrQ(Kostrikov
et al., 2020). The specific implementation details of SEREN-Eff-DQN can be found in Table 3.
We compute the human normalised score as human_normaliased_score =agent_score−random_score
human_score−random_score(Mnih
et al., 2013). The evaluation is based on 1.25×105steps at the end of 1×105training steps and is averaged
over 5random seeds (Kaiser et al., 2019).
E Further Experimental Results
E.1 Full Evaluation Results on Atari 100K Benchmarks
29Under review as submission to TMLR
Component Attribute Value
Exploiter critic MLP hidden layer dimensions [256,256]
critic MLP activation function ReLU
actor MLP hidden layer dimensions [256,256]
actor MLP activation function ReLU
learning rate 7.3×10−4
replay buffer size 3×105
batch size 256
number of critic ensemble 3
discounting factor 0.98
Explorer critic MLP hidden layer dimensions [256,256]
critic MLP activation function ReLU
actor MLP hidden layer dimensions [256,256]
actor MLP activation function ReLU
learning rate 1×10−4
replay buffer size 3×105
discounting factor 0.60
Switcher critic MLP hidden layer dimensions [64,64]
critic MLP activation function ReLU
actor MLP hidden layer dimensions [64,64]
actor MLP activation function ReLU
learning rate 1×10−4
replay buffer size 3×105
SEREN Switcher intervention cost ( β; Equation 5) 0.01
number of initial exploration steps 10000
frequency of training Exploiter 8
frequency of training Exploiter andSwitcher 4
mask probability for ensemble training 0.2
Table 2: Implementation specifications for SEREN-SAC for all MuJoCo environments considered.
30Under review as submission to TMLR
Component Attribute Value
Exploiter DQN ConvNet channels [32,64,64]
DQN ConvNet filter size [8×8,4×4,3×3]
DQN ConvNet stride [4,2,1]
DQN MLP hidden unit [256]
critic MLP activation function ReLU
learning rate 1×10−4
replay buffer size 1×105
batch size 256
multi-step return 10
number of critic ensemble 3
mask probability for ensemble training 0.2
dueling True
double Q-learning True
discounting factor ( γ1) 0.99
Explorer DQN ConvNet channels [32,64,64]
DQN ConvNet filter size [8×8,4×4,3×3]
DQN ConvNet stride [4,2,1]
DQN MLP hidden unit [256]
critic MLP activation function ReLU
learning rate 1×10−4
replay buffer size 1×105
batch size 256
multi-step return 10
number of critic ensemble 1
dueling True
double Q-learning True
discounting factor ( γ2) 0.80
Switcher DQN ConvNet channels [32,64,64]
DQN ConvNet filter size [8×8,4×4,3×3]
DQN ConvNet stride [4,2,1]
DQN MLP hidden unit [256]
critic MLP activation function ReLU
learning rate 1×10−4
replay buffer size 1×105
batch size 256
multi-step return 10
number of critic ensemble 3
mask probability for ensemble training 0.2
dueling True
double Q-learning True
discounting factor ( γ3) 0.99
SEREN Switcher intervention cost ( β; Equation 5) 3×10−5
number of initial exploration steps 1600
action repetitions 4
frames stacked 4
terminal on loss of life True
data augmentation intensity
Table 3: Implementation details for SEREN-Eff-DQN for all games in the Atari 100K benchmarks.
31Under review as submission to TMLR
Games Rainbow Efficient Rainbow Efficient DQN DrQ SEREN-DrQ
Alien 318.7 739.9 558.1 702.5 721.8
Amidar 32.5 188.6 63.7 100.2 173.4
Assault 231.0 431.2 589.5 490.3 596.8
Asterix 243.6 470.8 341.9 577.9 436.1
BankHeist 15.6 51.0 74.0 205.3 145.8
BattleZone 2360.0 10124.6 4760.8 6240.0 5287.3
Boxing -24.8 0.2 -1.8 5.1 5.8
Breakout 1.2 1.9 7.3 14.3 14.6
ChopperCommand 120.0 861.8 624.4 870.1 1474.5
CrazyClimber 2254.5 16195.3 5430.6 20072.216679.1
DemonAttack 163.6 508.0 403.5 1086.0 907.2
Freeway 0.0 27.9 3.7 20.0 26.9
FrostBite 60.2 866.8 202.9 889.9 761.5
Gopher 431.2 349.5 320.8 678.0 707.9
Hero 487.0 6857.0 2200.1 4083.7 4495.5
JamesBond 47.4 301.6 133.2 330.3 284.5
Kangaroo 0.0 779.3 448.6 1282.6 921.0
Krull 1468.0 2851.5 2999.0 4163.0 4443.1
KungFuMaster 0.0 14346.1 2020.9 7649.0 8937.5
MsPacman 67.0 1204.1 872.0 1015.9 1396.9
Pong -20.6 -19.3 -19.4 -17.1 −16.3
PrivateEye 0.0 97.8 351.3 -50.4 100.0
Qbert 123.5 1152.9 627.5 769.1 1043.3
RoadRunner 1588.5 9600.0 1491.9 8296.3 7835.0
Seaquest 131.7 354.1 240.1 299.4 395.9
UpNDown 504.6 2877.4 2901.7 2901.7 4534.2
Median Human Normalised Score 0.020 0.194 0.064 0.247 0.253
Mean Human Normalised Score 0.045 0.290 0.144 0.377 0.396
Table 4: Evaluation on Atari 100K Benchmarks.
32