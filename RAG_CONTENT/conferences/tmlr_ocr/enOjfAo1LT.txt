Under review as submission to TMLR
Norm-count Hypothesis: On the Relationship Between
Norm and Object Count in Visual Representations
Anonymous authors
Paper under double-blind review
Abstract
We present a novel hypothesis on norms of representations produced by convolutional neural
networks(CNNs). Inparticular, weproposethenorm-counthypothesis(NCH),whichstates
that there is a monotonically increasing relationship between the number of certain objects
in the image, and the norm of the corresponding representation. We formalize and prove
our hypothesis in a controlled setting, showing that the NCH is true for linear and batch
normalized CNNs followed by global average pooling, when they are applied to a certain
class of images. Further, we present experimental evidence that corroborates our hypothesis
for CNN-based representations. Our experiments are conducted with several real-world
image datasets, in both supervised and self-supervised learning – providing new insight on
the relationship between object counts and representation norms.
1 Introduction
The ability to learn high-quality representations from a wide range of complex data types lies at the heart of
the success of deep learning. Recently, several works have studied how deep learning-based representations
can be embedded in non-Euclidean spaces, to further improve representation quality (Bronstein et al., 2017).
In particular, embedding representations on the hypersphere using L2normalization has proven to be a par-
ticularly promising direction for several downstream applications, such classification and regression (Mettes
et al., 2019; Scott et al., 2021; Tan et al., 2022), and self-supervised learning (SSL) (Chen et al., 2020; Caron
et al., 2021).
However, despite the widespread use of L2normalization in several aspects of deep learning, little work exists
on understanding exactly what type of information the norm contains, and why discarding this information
can improve representation quality. Hence, we still lack critical understanding of the role of representation
norms, and what types information they encode. In this work, we aim to improve the understanding of
norms of image representations produced by convolutional neural network (CNN). Our work is built on a
novel hypothesis on the relationship between CNN-based representations and the number of objects in the
input image:
Informal Definition 1 (Norm-count hypothesis) .There is a monotonically increasing relationship between
the norm of a representation produced by a CNN, and the number of objects in the input image, for which
the CNN is trained to recognize.
Thenorm-counthypothesis(NCH)thusproposesatheoryontherelationshipbetweennorm, andthenumber
of detections produced by the CNN. This relationship is monotonically increasing , meaning that increasing
the number of objects in an image, will increase the norm of the image representation. In addition, we will
see later that the NCH further implies that angles encode information about the types of objects detected
by the CNN in the given image.
Presuming that angles encode semantic information and norms encode count information, it is not surprising
that models trained to perform inherently count-invariant tasks will perform better when count information
is discarded, for instance by L2normalization. Interestingly, count-invariant tasks are widely studied in
1Under review as submission to TMLR
computer vision. Single-label supervised classification ( e.g. ImageNet (Deng et al., 2009)), whole-image
clustering, linear evaluation in self-supervised learning, and standard evaluation setups in few-shot learning
all belong to the category of count-invariant tasks.
The objective of our work is to formalize and experimentally evaluate the NCH for CNNs in supervised and
self-supervised learning. Our main contributions are:
1. We propose the NCH – stating that there is a monotonically increasing relationship between the
norm of a representation, and the number of objects in the given image.
2. We prove that the NCH is true in a controlled setting, assuming that input images are composed
of several object images , for which the feature extractor provides a delta-like response in a single
channel.
3. We conduct an extensive experimental evaluation with synthetic and real-world datasets, with both
supervised and self-supervised models. Our results show monotonically increasing relationships
between norm and count for the majority of models and datasets – corroborating the NCH.
The rest of the paper is structured as follows: Section 2 gives an overview of work related to ours. In
Section 3, we theoretically analyze the NCH, and prove that it holds under certain assumptions. Section 4
includes the results of our experiments. We finish the paper with Section 5, presenting some concluding
remarks and directions for future work.
2 Related work
Inthissection, wesummarizeotherworkrelatedtothispaper. Weemphasizethatourworkiscomplementary
to these, as none of the works below provide an accurate and rigorous understanding of the information
contained in norms of CNN-based representations.
2.1 Embedding representations on the hypersphere
Embedding representations on the hypersphere instead of in Euclidean space has shown to be beneficial for
both supervised classification and regression (Mettes et al., 2019; Scott et al., 2021; Tan et al., 2022). Mettes
et al. (2019) develop classification and regression losses on the hypersphere, illustrating that L2normalized
representations and prototypes are beneficial for both classification and regression. The more recent work
by Tan et al. (2022) shows that a supervised classification model can be regularized with a self-supervised
contrastive loss on the unit hypersphere.
L2normalization is also common in models for self-supervised learning of image representations (Chen et al.,
2020; He et al., 2020; Grill et al., 2020; Caron et al., 2020; 2021; Goyal et al., 2022; Li et al., 2023). The
benefit ofL2normalization appears to stem from similarity measures and contrastive losses being more
well-behaved after discarding the norm – resulting in compact and well-separated classes (Wang & Isola,
2020).
2.2 Hyperspherical regularization
Hyperspherical embedding of network weights have also shown to be beneficial to regularize training of deep
neural networks (DNNs) (Salimans & Kingma, 2016; Liu et al., 2017; 2018; 2021). These methods constrain
the weight vectors in DNNs to lie on the unit hypersphere. Liu et al. (2017) show that hyperspherical
weights improve the conditioning of the optimization problem, helping the optimizer converge faster to
potentially better solutions. However, our work is orthogonal to this, since we aim to understand norms of
representations , and not norms of weights.
2Under review as submission to TMLR
3 Norm-count hypothesis
The purpose of this section is to formalize the NCH, and to analyze it in a rigorous theoretical setting. To
do so, we assume certain properties of the feature extractor ( e.g. CNN). These feature extractors admit a
certain class of images, referred to as object images , which can be seen as “prototypes” of the objects the
feature extractor is trained to detect.
Having established properties of the feature extractor and corresponding object images, we prove that the
norm of the representation produced by the feature extractor is proportional to the number of object images
present in the given image. Thereby corroborating the NCH. Proofs for the results presented in this section
are given in Appendix A.
We start by providing exact definitions of images and image translation.
Definition 1 (Images).The set of images with Cchannels and size W×His defined as
IC,W,H ={I:N0
<C×Z×Z→R|I(c,x,y ) = 0if(x,y)/∈N0
<W×N0
<H} (1)
where N0
<a={0,1,...,a−1}.
Definition 2 (Translation operator) .A translation operator Trx′,y′:IC,W,H→IC,W,Hshifts the given
image byx′,y′pixels
Trx′,y′(I)(c,x,y ) =I(c,x−x′,y−y′) (2)
Definition 3 (Translation equivariance) .A mapping f:IC,W,H→IC′,W′,H′is translation equivariant iff
for a translation operator Trx′,y′, we have
f◦Trx′,y′= Trx′,y′◦f (3)
where◦denotes the composition of functions.
We will now define two types of functionals acting on images, namely strict detectors andrelaxed detectors .
Detectors are generic functionals with certain constraints on how they act on element-wise sums of images.
As we will show later, detectors are closely related to CNNs, and when combined with a pooling operator
(defined later), detectors are responsible for producing vectorial representations for a given image.
Definition 4 (Strict detector) .A strict detector is a translation invariant mapping f:IC,W,H→IC′,W′,H′
that satisfies
f(I1+I2) =f(I1) +f(I2) +Af (4)
whereAf∈IC′,W′,H′is a constant image independent of I1andI2, satisfying Af(k,x,y ) =af,kfor all
(k,x,y )∈N0
<C′×N0
<W′×N0
<H′. Addition is defined element-wise.
Definition 5 (Relaxed detector) .A relaxed detector is a translation invariant mapping f:IC,W,H→
IC′,W′,H′that satisfies
|f(I1+I2)|≼|f(I1)|+|f(I2)|+|Af| (5)
whereAf∈IC′,W′,H′is a constant image independent of I1andI2, satisfying Af(k,x,y ) =af,kfor all
(k,x,y )∈N0
<C′×N0
<W′×N0
<H′. Addition and absolute value are defined element-wise, and I1≼I2implies
thatI1(k,x,y )≤I2(k,x,y )for allk,x,y.
The following propositions show properties of detectors that are relevant for CNNs.
3Under review as submission to TMLR
Proposition 1 (Composition of detectors) .For detectors fandg, the following holds:
1. Iffandgare strict detectors, then g◦fis a strict detector with Ag◦f=g(Af) + 2Ag
2. Iffis a strict detector and gis a relaxed detector, then g◦fis a relaxed detector with Ag◦f=
|g(Af)|+ 2|Ag|.
Proposition 2 (Convolutions are strict detectors.) .LetConvK:IC,W,H→I1,W+w−1,H+h−1be the convo-
lution operator convolving the given image, I∈IC,W,H, with a filter, K∈IC,h,w
ConvK(I)(0,x,y) =C−1/summationdisplay
c=0∞/summationdisplay
x′=−∞∞/summationdisplay
y′=−∞K(c,x′,y′)I(c,x−x′,y−y′). (6)
Then ConvKis a strict detector with (C′,H′,W′) = (1,W+w−1,H+h−1)andAConvK= 0.
Since images are infinitely zero-padded (Definition 1), Proposition 2 assumes that “full” padding is used in
the convolution operator. However, we note that the proposition also holds if “valid” or “same” padding is
used.
Proposition 3 (Batch-normalization in inference mode is a strict detector) .LetBNb,g,µ,σ:IC,W,H→
IC,W,Hrepresent batch normalization in inference mode, with parameters b= [b0,...,bC−1]⊤,g=
[g0,...,gC′−1]⊤and running moment estimates µ= [µ0,...,µC−1]⊤,σ= [σ0,...,σC′−1]⊤, defined as
BNb,g,µ,σ(I)(k,x,y ) =I(k,x,y )−µk
σkgk+bk. (7)
Then BNb,g,µ,σis a strict detector with (C′,W′,H′) = (C,W,H )andaBNb,g,µ,σ,k=µk
σkgk−bk.
Proposition 4 (LeakyReLU is a relaxed detector.) .LetLeakyReLUα:IC,W,H→IC,W,Hbe defined
element-wise as
LeakyReLUα(I(k,x,y )) =/braceleftigg
I(k,x,y ),ifI(k,x,y )>0
α·I(k,x,y ),otherwise(8)
for allk,x,y, andα∈[0,1). Then LeakyReLUαis a relaxed detector with (C′,W′,H′) = (C,W,H )and
ALeakyReLUα= 0.
This also holds for the standard ReLU(x) = max{0,x}activation, since ReLU = LeakyReLU0.
From Propositions 1, 2 and 3, we see that linear and batch normalized CNNs – i.e. networks consisting
only of compositions of convolutions and batch normalization – are strict detectors. Furthermore, combining
Propositions 1, 2 and 4 shows that a CNN consisting of convolutions and LeakyReLU (orReLU) activations
are compositions of relaxed detectors. These propositions thus cover the most important building blocks of
CNNs, along with the most common activation functions.
CNNs for classification and representation learning are often followed by a global pooling operator that
aggregates information from all spatial locations. The following definition considers a general pooling oper-
ation, which we use in our theoretical analysis. We then prove that global average pooling (GAP) – one of
the most frequently used pooling operations – is a special case of the general pooling operation.
Definition 6 (Global pooling operator) .LetI∈IC,W,Hbe an image. The mapping Pool :IC,W,H→RC
is called a global pooling operator if there exists non-negative real numbers γ0,...,γC−1independent of I,
such that
|Pool(I)k|≤γk/vextendsingle/vextendsingle/vextendsingle/vextendsingleW−1/summationdisplay
x=0H−1/summationdisplay
y=0I(k,x,y )/vextendsingle/vextendsingle/vextendsingle/vextendsingle, k∈N0
<C. (9)
4Under review as submission to TMLR
Proposition 5 (GAP is a global pooling operator) .For an image I∈IC,W,H, let GAP be defined as
GAP(I)k=1
WHW−1/summationdisplay
x=0H−1/summationdisplay
y=0I(k,x,y ), k∈N0
<C. (10)
Then GAPis a global pooling operator with γk=1
WH,∀k∈N0
<C.
Our objective is now to understand norms of representations computed by a detector followed by a global
pooling operator. Hence, we define object images as “prototypical” images that give a delta-like response
when processed by the given detector.
Definition 7 (Object images) .An imageOj∈IC,h,wis said to be an object image of type jw.r.t. the
detectorf, iff
f(Oj) =δj,0,0 (11)
whereδj,x′,y′is the Kronecker delta function
δj,x′,y′(k,x,y ) =/braceleftigg
1,(k,x,y ) = (j,x′,y′)
0,otherwise. (12)
The set of all object images of type jis denoted Ωj={O|f(O) =δj,0,0}.
In a highly accurate supervised model the object image types will tend to coincide with the classes the model
is trained to detect. This is because supervised models are trained to output a one-hot prediction vector,
resembling the delta-response assumed in Definition 7, after global pooling. However, we emphasize that
by definition, object images and types are entirely determined by the detector, as images that give a delta
response in a single output feature map. All images that result in a delta response in an output feature
map,jare said to be object images of type j, regardless of their class affiliation. Hence, it is only for trained
models that the object image types will tend to coincide with semantically meaningful ground-truth classes.
Another interpretation of object images is as “parts of a whole”, where it is assumed that image motifs
consist of a collection of object images. An image of a car, for instance, will be composed of object images
with type “wheel”, “car body”, etc.
We note that for self-supervised models and for models where we remove layers from the network before
computing representations, the correspondence between ground-truth classes and object images becomes less
straightforward. However, we can still assume that the model is trained to recognize something . The outputs
of intermediate layers will therefore still correspond to objects or features that are related to what the model
is trained to recognize.
We will now define multiple objects images (MO-images) as images composed of one or more object images.
This definition gives rise to a natural notion of object “count” in the image, which is necessary to formalize
the NCH.
Definition 8 (Multiple objects image) .An MO-image, I∈IC,W,H, constructed from object images in
Ω0,..., ΩC′−1, w.r.t. the detector f, is defined as
I=C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈PjTrx′,y′(O). (13)
whereC′is the number of object types. Pjis a set of 3-tuples, where the first element is an object image
from Ωj, and the second third elements are the positions of that object image in I.Pjcan also be empty,
indicating that Icontains no object image of type j.
5Under review as submission to TMLR
We now have the following theorem stating that the NCH is true for detectors and global pooling operators
applied to MO-images.
Theorem 1 (Norm-count hypothesis – simplified setting) .Letf:IC,W,H→IC′,W′,H′be a relaxed detector
with object image types Ω0,..., ΩC′−1, and letI∈IC,W,Hbe a MO-image constructed from the same object
images. Then, if z= [z0,...,zC′−1]⊤∈RC′is the output of a global pooling operator applied to the feature
mapsf(I), we have
|zk|=|Pool(f(I))k|≤γk(|Pk|+WHn P|af,k|), k∈N0
<C′ (14)
for non-negative numbers γ0,...,γC−1independent of I.
Theorem 1 shows that the k-th component is an increasing affine transformation of the number of object
images of type kinI. Furthermore, if the detector fhasAf= 0we get exact proportionality between |zk|
and|Pk|.
Taking the norm of zwith elements bounded as in Equation (14) gives the following corollary.
Corollary 1.1 (Lpnorm of z).Forp>0, theLpnorm of zfrom Theorem 1 is
||z||p≤
C′−1/summationdisplay
k=0γp
k(|Pk|+WHn P|af,k|)p
1
p
(15)
Corollary 1.1 shows that the Lpnorm of representations is upper bounded by a monotonically increasing
function of the count, corroborating the NCH.
Corollary 1.2 (Strict detector and GAP) .Iffis a strict detector, and GAPis used in place of Pool,
Theorem 1 simplifies to
zk= GAP(f(I))k=|Pk|
W′H′+nPak (16)
wherenP=/summationtextC′−1
j=0|Pj|−1.
Corollary 1.2 shows that for strict detectors followed by GAP, there is an affine relationship between each
component of z, and the number of objects of the corresponding type present in the image. Furthermore, if
we haveak= 0, we have exact proportionality between zkand|Pk|. Since linear CNNs are strict detectors
withak= 0, this corollary proves that each dimension, in a representation produced by linear CNNs, is
proportional to the number of object images in the given MO-image.
Furthermore, Theorem 1 states that the norm of zis directly related to the absolute (total) count of objects
in the image, regardless of the type of the object images. This is expected, since the perfect detector produces
a set of delta-responses, and the global pooling operator aggregates these over the spatial dimensions.
3.1 Semantic information in angles
In contrast to the norm, the angle of zdepends on the count of one object type relative to the count of
another object type. This is demonstrated by the following result.
6Under review as submission to TMLR
Result 1 (Semantic information in angles) .SupposeI1,I2∈IC,W,Hare MO-images processed by a strict
detectorfwithak= 0, followed by GAP. Furthermore, assume that I1only consists of objects of type j, and
thatI2only consists of objects of type k. This gives
z1= GAP(f(I1)) =|P(1)
j|
W′H′ejand z2= GAP(f(I2)) =|P(2)
k|
W′H′ek (17)
where ej(ek) denotes the vector where element j(k) is1, and all other elements are 0.
Then, if (||z||,θ(z))denotes the transformation of zto hyperspherical coordinates, we can consider the
following two cases:
1.Different class, same count :j̸=kand|P(1)
j|=|P(2)
k|, which gives
||z1||=||z2||and θ(z1)̸=θ(z2) (18)
2.Same class, different count: j=kand|P(1)
j|̸=|P(2)
k|, which gives
||z1||̸=||z2||and θ(z1) =θ(z2) (19)
In both cases in Result 1, the angles θ(z1)andθ(z2)are most informative of the image classes (object types).
When the images belong to different classes (case 1), the discriminative power lies in the angles and not
in the norms. Conversely, when I1andI2belong to the same class (case 2), the within class distance is 0
for the angles, but non-zero for the norms. The angle thus encodes information about which classes (object
types) that were detected in the image – i.e. the semantic information.
3.2 Feature norm vs. object size
It might be natural to think that the norm of a representation is positively correlated with the size of the
object in the image, since larger object should lead to more, and possibly stronger activations. However,
CNNs are not size equivariant, meaning that this is not necessarily the case. This is because convolutions –
the basic building blocks of CNNs – detect patterns with a certain size. Resizing the patterns by contracting
or dilating spatial dimensions can therefore completely change the response, both reducing og increasing
its strength. In the context of our work, this means that object images are not resizeable: If one resizes
an object image, it may no longer be an object image for the same detector, as it might no longer give a
delta response. This, however depends on the properties of the detector, and whether it has been trained to
produce similar responses for objects with different sizes. Section 4.2.2 includes an experimental analysis of
the relationship between object size and feature norm.
4 Experiments
The purpose of these experiments is to experimentally investigate the NCH in a controlled setting. We
design the experiments to have fine-grained control of the “count” in each image. This allows us to properly
examine the relationship between norm and count – both quantitatively and qualitatively. Our experiments
are conducted with both supervised and self-supervised models on several datasets.
Although our theoretical results hold for arbitrary Lpnorms, we focus on L2norms in the experimental
evaluation. Thisisbecausethe L2normistheonemostfrequentlyencounteredinotherworks(seeSection2),
and has known benefits related to optimization (Liu et al., 2021), as well as alignment, uniformity, and class
separability (Wang & Isola, 2020).
7Under review as submission to TMLR
(a) MNIST
 (b) STL-10 (Zeros)
 (c) STL-10 (Random)
 (d) STL-10 (Blur)
Figure 1: Example synthetic images generated from MNIST and STL-10, with different filling approaches
for the latter.
(a) Count = 0
(b) Count = 1
(c) Count = 2
(d) Count = 3
 (e) Count = 4
Figure 2: Example images from the MSO dataset.
4.1 Setup
4.1.1 Datasets
Synthetic datasets. In order to mimic the properties of MO-images in evaluation, we start with datasets
consisting of natural images (MNIST (Lecun et al., 1998) and STL-10 (Coates et al., 2011)). Then, to
generate a single evaluation image, we sample a random number of images, and place them at random
positions in a 4×4grid. This gives us an image that resembles an MO-image, where we know the true count
–i.e. the number of object images.
For MNIST, we fill the empty grid positions with 0-values, as indicated by Definition 8. For STL-10, we
generate datasets with 3different approaches to filling the empty grid slots:
•Zeros: empty grid positions are filled with 0-values.
•Random: empty grid positions are filled with random Gaussian noise with the same mean and
standard deviation as the object images.
•Blur: the background for the whole grid is a random blurred image, and the object images are placed
on top of this image.
We experiment with different fill modes to ensure that the results are not skewed by changes in global image
statistics, such as mean and variance. Figure 1 shows examples of the generated images.
Real-world counting and object detection datasets. In addition to the syntetic datasets, we include
two real-world datasets in our experimental evaluation:
8Under review as submission to TMLR
(a) Count = 2
(b) Count = 4
 (c) Count = 6
(d) Count = 8
(e) Count = 10
Figure 3: COCO example images.
1. The multi salient objects (MSO) dataset1(Figure 2) is derived from the Salient Object Subitizing
dataset(Zhangetal.,2015), andincludesimageswithavaryingnumberofsalientforegroundobjects.
Each image in the dataset is annotated with a count between 0and4objects.
2. The common objects in context (COCO) dataset (Lin et al., 2014) (Figure 3) is a large-scale object
detection dataset with images containing multiple objects from several classes. We use the number
of ground-truth bounding boxes for an image as the object count. In order to ensure that each count
has a representative number of images, we select images with 2≤count≤9.
As can be seen in Figures 2 and 3, MSO contains objects that are more prominent in the image, while COCO
contains objects that might be more difficult to detect, with greater variation in size and clarity.
4.1.2 Models and architectures
Architectures. Our evaluation is performed with models using the following two CNN architectures:
•Simple-6 : A simple 6-layer CNN followed by GAP. The model has ReLUactivations, max pooling
after every second convolutional layer. No batch-normalization or other forms of normalization is
applied anywhere in the architecture.
•ResNet-50 : The standard 50-layer residual network architecture by He et al. (2016), with batch
normalization.
For both models we add a final convolutional layer, followed by GAP to produce the final representation.
Supervised models. The supervised models are trained using the standard cross-entropy loss, with the
softmaxed final representation (GAP output) as the model’s prediction. We thus avoid any additional fully-
connected classification layers, making the model more likely to satisfy the detector assumptions given in
Definitions 4 and 5.
The Simple-6 architecture is trained on MNIST, starting from randomly initialized weights. The ResNet-50
architecture is initialized with pre-trained weights from ImageNet training2, and fine-tuned on single-object
images from COCO.
Self-supervised models. We include two different self-supervised learning models in our evaluation:
1https://www.kaggle.com/datasets/jessicali9530/mso-dataset
2Weights: IMAGENET1K_V2 , documentation: https://pytorch.org/vision/main/models/generated/torchvision.models.
resnet50.html#torchvision.models.resnet50
9Under review as submission to TMLR
•The standard SimCLR model (Chen et al., 2020), with a two-layer projection head, and a normalized
temperature-scaled cross entropy loss. The input to the projection head is the final representation
(GAP output). Similar to the supervised setup, we train the Simple-6 architecture on MNIST
from randomly initialized weights. The ResNet-50 architecture is initialized with self-supervised
pre-trained ImageNet weights3, and fine-tuned on single-object images from COCO.
•The Dense Contrastive Learning (DenseCL) model (Wang et al., 2021) with a ResNet-50 encoder.
This is a self-supervised model aimed to provide better representations for pixel-level predictions
and multi-label data. For this model we use the pre-trained weights directly without fine-tuning.
We do this both to assess our hypothesis with a model that has not been fine-tuned on single-object
images, and to avoid the large computational demands required to train DenseCL.
4.1.3 Implementation
The experiments are implemented in Python with the PyTorch framework (Paszke et al., 2019). We will
make the code for our experiments publicly available upon publication of the paper.
4.1.4 Quantitative evaluation of monotonic increase
We use a weighted linear regression model to quantitatively assess whether there is an increasing relationship
between Z-score normalized feature norms, νi, and the count, c(xi)
νi=β0+β1c(xi) +ϵi (20)
where
νi=||zi||−µ||z||
σ||z||, µ||z||=1
nn/summationdisplay
i=1||zi||, σ||z||=/radicaltp/radicalvertex/radicalvertex/radicalbt1
n−1n/summationdisplay
i=1/parenleftbig
||zi||−µ||z||/parenrightbig2, (21)
and the residual, ϵi, is assumed to be Gaussian with zero mean and standard deviation σc(xi). We allow the
standard deviation to be count-dependent to account for heteroskedasticity in the data.
The parameter estimates (ˆβ0,ˆβ1)are computed using weighted least squares. Based on these estimates, we
can test for monotonic increase by checking whether the slope β1is positive:
H0:β1≤0vs.H1:β1>0. (22)
We report the estimated slope ˆβ1and thep-value for the above hypothesis test.
4.2 Results
4.2.1 Relationship between norm and count
The results in Table 1 show a monotonic increase between norm and count for almost all experimental
configurations. According to these results, the expected value of a representation norm increases by roughly
0.1standard deviations per additional object in the image.
Our results show that the relationship between norm and count is more prominent for the synthetic grid-
datasets, compared to the datasets composed of natural, real-world images (Figure 4). This is expected,
since the synthetic datasets include clear, single-object images on top of a non-informative background with
little distraction. We also observe a difference between the real-world datasets, with a stronger relationship
between norm and count on MSO compared to COCO. This is likely caused by the more prominent objects in
MSO, compared to the more complex scenes in COCO, with the latter including partially occluded, smaller,
and more diverse objects.
10Under review as submission to TMLR
Table 1: Estimated slopes ( ˆβ1) andp-values for the linear regression model for norm vs. count. A positive
slope with a low p-value indicates a statistically significant, monotonically increasing relationship between
norm and count.
Dataset Model Slope ( ˆβ1)p-value
MNIST (Zeros)Supervised (Simple-6 ) 0.209 0.000
SimCLR (Simple-6 ) 0.126 0.000
STL-10 (Zeros)Supervised (ResNet-50 ) 0.087 0.000
SimCLR (ResNet-50 ) 0.134 0.000
STL-10 (Random)Supervised (ResNet-50 )-0.101 1.000
SimCLR (ResNet-50 ) 0.106 0.000
STL-10 (Blur)Supervised (ResNet-50 ) 0.133 0.000
SimCLR (ResNet-50 ) 0.075 0.000
MSOSupervised (ResNet-50 ) 0.040 0.032
SimCLR (ResNet-50 ) 0.175 0.000
DenseCL (ResNet-50 ) 0.474 0.000
COCOSupervised (ResNet-50 ) 0.004 0.000
SimCLR (ResNet-50 ) 0.009 0.000
DenseCL (ResNet-50 ) -0.028 1.000
246810121416
Count−2−10123Z-score normalized representation norm
(a) MNIST (Zeros)
246810121416
Count−3−2−1012Z-score normalized representation norm (b) STL-10 (Blur)
0 1 2 3 4
Count−2−1012Z-score normalized representation norm (c) MSO
23456789
Count−2−1012Z-score normalized representation norm (d) COCO
Figure 4: Boxplots illustrating the relationship between norm and count for supervised models.
11Under review as submission to TMLR
0 200000 400000 600000
Total object size [pixels]−2.50.02.55.07.510.012.515.0Z-score normalized representation norm
(a) Supervised (slope =−1.24·10−7,
p-value = 1.0)
0 200000 400000 600000
Total object size [pixels]−4−20246Z-score normalized representation norm(b) SimCLR (slope = 8.74·10−7,p-
value = 0.0)
0 200000 400000 600000
Total object size [pixels]−20246Z-score normalized representation norm(c) DenseCL (slope = 8.81·10−7,p-
value = 0.0)
Figure 5: Relationship between norm and object size on COCO. Slopes and p-values are from a linear
regression model for norm vs. object size, with a one-sided test for positive slopes.
4.2.2 Relationship between norm and object size.
In order to experimentally investigate our arguments about norm object size from Section 3.2, we evaluate
the relationship between norm and object size on COCO. We chose the COCO dataset for these experiments
since it is a real-world dataset with bounding box annotations, allowing us to infer the size of each object
as the area of the bounding box. We repeat the norm-size experiments for both the supervised and self-
supervised models.
The plots in Figure 5 show no clear relationship between norm and object size. Regressing norm against
count shows a small but negative correspondence between the two for the supervised model, and very small
positive slopes ( <10−6) for the self-supervised models. These negative and almost-zero slopes show that
the correspondence between norm and size is significantly weaker compared to the correspondence between
norm and count, thus corroborating our arguments in Section 3.2.
5 Conclusion and future work
Wehavepresentedthenorm-counthypothesis(NCH)–providingnovelinsightinthenormsofrepresentations
produced by CNNs. Under certain assumptions on the model and input images, we proved the NCH, showing
that each component in a given representation is upper-bounded by the number of object images present
in the input image. Moreover, from our theoretical analysis, it follows that representation norms carry
information related to count, whereas angles represent semantic information. Our experiments with both
supervised and self-supervised models, applied on synthetic and real-world datasets, show increasing trends
between norm and count on the majority of experimental configurations - corroborating the NCH.
We believe that understanding representation norms through object counts is a promising direction of re-
search. Although we focus on CNNs in this work, our results might generalize to other architectures and
learning regimes. With vision transformers (ViTs) recently showing remarkable performance in computer
vision, it should be investigated whether there is a relationship between norm and count for ViT architectures
as well. It is however not trivial to prove that ViT components meet the detector conditions. We therefore
leave further analyses of ViTs and the NCH to future work.
3https://pytorch-lightning-bolts.readthedocs.io/en/latest/models/self_supervised.html#
imagenet-baseline-for-simclr
12Under review as submission to TMLR
References
Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric Deep
Learning: Going beyond Euclidean Data. IEEE Signal Processing Magazine , 34(4):18–42, July 2017. ISSN
1053-5888. doi: 10.1109/MSP.2017.2693418.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsuper-
vised Learning of Visual Features by Contrasting Cluster Assignments. In NeurIPS , 2020.
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging Properties in Self-Supervised Vision Transformers. arXiv:2104.14294 [cs] , May 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Con-
trastive Learning of Visual Representations. In ICML, pp. 1597–1607. PMLR, November 2020.
Adam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single-Layer Networks in Unsupervised Feature
Learning. In AISTATS , pp. 215–223. JMLR Workshop and Conference Proceedings, June 2011.
J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image
Database. In IEEE Conference on Computer Vision and Pattern Recognition , pp. 248–255, June 2009.
doi: 10.1109/CVPR.2009.5206848.
Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent Sagun, Armand Joulin,
and Piotr Bojanowski. Vision Models Are More Robust And Fair When Pretrained On Uncurated Images
Without Supervision, February 2022.
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray
Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-
supervised Learning. In NeurIPS , September 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition.
InCVPR, pp. 770–778, Las Vegas, NV, USA, June 2016. IEEE. ISBN 978-1-4673-8851-1. doi: 10.1109/
CVPR.2016.90.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised
Visual Representation Learning. In CVPR, pp. 9726–9735. IEEE, June 2020. ISBN 978-1-72817-168-5.
doi: 10.1109/CVPR42600.2020.00975.
Bernd Jähne. Digital image processing . Engineering online library. Springer, Berlin Heidelberg, 5., rev. and
extended edition, 2002. ISBN 978-3-540-67754-3.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE , 86(11):2278–2324, November 1998. ISSN 0018-9219. doi: 10.1109/5.726791.
Alexander C. Li, Ellis Brown, Alexei A. Efros, and Deepak Pathak. Internet Explorer: Targeted Represen-
tation Learning on the Open Web, February 2023.
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona,
Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO: Common Objects in Context.
InECCV, 2014.
Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep Hyperspher-
ical Learning. In NeurIPS , 2017.
Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning towards
Minimum Hyperspherical Energy. In NeurIPS , 2018.
Weiyang Liu, Rongmei Lin, Zhen Liu, Li Xiong, Bernhard Scholkopf, and Adrian Weller. Learning with
Hyperspherical Uniformity. In AISTATS , pp. 23, 2021.
13Under review as submission to TMLR
Pascal Mettes, Elise van der Pol, and Cees G. M. Snoek. Hyperspherical Prototype Networks. In NeurIPS ,
pp. 11, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett (eds.), NeurIPS , pp. 8024–8035.
Curran Associates, Inc., 2019.
Tim Salimans and Durk P Kingma. Weight Normalization: A Simple Reparameterization to Accelerate
Training of Deep Neural Networks. In NeurIPS , volume 29. Curran Associates, Inc., 2016.
Tyler R. Scott, Andrew C. Gallagher, and Michael C. Mozer. Von Mises-Fisher Loss: An Exploration of
Embedding Geometries for Supervised Learning. In ICCV, pp. 10612–10622, 2021.
Cheng Tan, Zhangyang Gao, Lirong Wu, Siyuan Li, and Stan Z Li. Hyperspherical Consistency Regulariza-
tion. InCVPR, pp. 12, 2022.
Tongzhou Wang and Phillip Isola. Understanding Contrastive Representation Learning through Alignment
and Uniformity on the Hypersphere. In ICML, 2020.
Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense Contrastive Learning for
Self-Supervised Visual Pre-Training. In CVPR, pp. 3024–3033, 2021.
Jianming Zhang, Shugao Ma, Mehrnoosh Sameki, Stan Sclaroff, Margrit Betke, Zhe Lin, Xiaohui Shen, Brian
Price, and Radomir Mech. Salient Object Subitizing. In CVPR, 2015.
A Proofs
A.1 Proposition 1
Proof.
1. Invoking condition (4) of strict detectors gives
(g◦f)(I1+I2) =g(f(I1+I2)) =g(f(I1) +f(I2) +Af) (23)
=g(f(I1)) +g(f(I2)) +g(Af) + 2Ag (24)
= (g◦f)(I1) + (g◦f)(I2) +g(Af) + 2Ag (25)
2. By conditions (4) and (5), we have
|(g◦f)(I1+I2)|=|g(f(I1+I2))|=|g(f(I1) +f(I2) +Af)| (26)
≤|g(f(I1))|+|g(f(I2))|+|g(Af)|+ 2|Ag| (27)
=|(g◦f)(I1)|+|(g◦f)(I2)|+|g(Af)|+ 2|Ag| (28)
A.2 Proposition 2
Proof.The proposition follows directly from convolutions being linear and translation equivariant. See
e.g. (Jähne, 2002, Ch. 4).
14Under review as submission to TMLR
A.3 Proposition 3
Proof.For images I1andI2we have
BNb,g,µ,σ(I1+I2)(k,x,y ) =I1(k,x,y ) +I2(k,x,y )−µk
σkgk+bk (29)
=gk
σk(I1(k,x,y ) +I2(k,x,y )) +bk−µggk
σk(30)
=/parenleftbigggk
σkI1(k,x,y ) +bk−µggk
σk/parenrightbigg
+/parenleftbigggk
σkI2(k,x,y ) +bk−µggk
σk/parenrightbigg
−/parenleftbigg
bk−µkgk
σk/parenrightbigg
(31)
= BN b,g,µ,σ(I1)(k,x,y ) + BN b,g,µ,σ(I2)(k,x,y ) +/parenleftbiggµkgk
σk−bk/parenrightbigg
(32)
A.4 Proposition 4
Proof.Leta1=I1(k,x,y )anda2=I2(k,x,y ). Since addition is commutative, we can assume a1≥a2
without loss of generality.
Observe that, if a1anda2is positive (negative), then a1+a2will be positive (negative). This means that
LeakyReLUα(a1+a2) = LeakyReLUα(a1) + LeakyReLUα(a2)in this case.
On the other hand, if a2<0<a1, we have|LeakyReLUα(a1)|+|LeakyReLUα(a2)|=|a1|+α|a2|. Now we
consider the following two cases:
1. If|a1|≥|a2|, then
|LeakyReLUα(a1+a2)|=|a1+a2| (33)
≤|a1| (34)
≤|a1|+α|a2| (35)
=|LeakyReLUα(a1)|+|LeakyReLUα(a2)| (36)
2. If|a1|≤|a2|, then
|LeakyReLUα(a1+a2)|=α|a1+a2| (37)
≤α|a2| (38)
≤|a1|+α|a2| (39)
=|LeakyReLUα(a1)|+|LeakyReLUα(a2)| (40)
A.5 Proposition 5
Proof.Setting1
WH=γkgives
GAP (I)k=γkW−1/summationdisplay
x=0H−1/summationdisplay
y=0I(k,x,y ), k∈N0
<C (41)
from which it follows that
|GAP (I)k|≤γk/vextendsingle/vextendsingle/vextendsingle/vextendsingleW−1/summationdisplay
x=0H−1/summationdisplay
y=0I(k,x,y )/vextendsingle/vextendsingle/vextendsingle/vextendsingle, k∈N0
<C (42)
15Under review as submission to TMLR
A.6 Theorem 1
Proof.Sincefis a relaxed detector, we have
|f(I)|=/vextendsingle/vextendsingle/vextendsingle/vextendsinglef
C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈PjTrx′,y′(O)
/vextendsingle/vextendsingle/vextendsingle/vextendsingle(43)
≼C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pj|f(Trx′,y′(O))|+nP|Af| (44)
wherenP=/summationtextC′−1
j=0|Pj|−1.
Then, since fis translation equivariant, and provides delta detections
|f(I)|≼C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pj|f(Trx′,y′(O))|+nP|Af| (45)
=C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pjδ(j,x′,y′)+nP|Af|. (46)
Applying a global pooling operator to f(I)then gives
|zk|=|Pool(f(I))k|≤γk/vextendsingle/vextendsingle/vextendsingle/vextendsingleW′−1/summationdisplay
x=0H′−1/summationdisplay
y=0f(I)(k,x,y )/vextendsingle/vextendsingle/vextendsingle/vextendsingle(47)
=γkW′−1/summationdisplay
x=0H′−1/summationdisplay
y=0
C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pjδ(j,x′,y′)(k,x,y ) +nP|af,k|
 (48)
=γkW′−1/summationdisplay
x=0H′−1/summationdisplay
y=0
/summationdisplay
(O,x′,y′)∈Pkδ(k,x′,y′)(k,x,y ) +nP|af,k|
 (49)
=γk
/summationdisplay
(O,x′,y′)∈Pkδ(k,x′,y′)(k,x′,y′) +WHn P|af,k|
 (50)
=γk(|Pk|+WHn P|af,k|) (51)
A.7 Corollary 1.1
Proof.TheLpnorm of zis defined as
||z||p=
C′−1/summationdisplay
k=0|zk|p
1
p
(52)
forp>0. Since each|zk|is positive and upper bounded by γk|Pk|(by Theorem 1), we have
C′−1/summationdisplay
k=0γk|Pk|p≥C′−1/summationdisplay
k=0|zk|p(53)
which gives

C′−1/summationdisplay
k=0γk|Pk|p
1
p
≥
C′−1/summationdisplay
k=0|zk|p
1
p
=||z||p (54)
16Under review as submission to TMLR
A.8 Corollary 1.2
Proof.This proof follows the same steps as the proof of Theorem 1, but without absolute values, and with
equality instead of inequality.
Sincefis a strict detector, we have
f(I) =f
C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈PjTrx′,y′(O)
 (55)
=nPAf+C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pj(f(Trx′,y′(O))) (56)
=nPAf+C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pjf(Trx′,y′(O)) (57)
wherenP=/summationtextC′−1
j=0|Pj|−1. Then, since fis translation equivariant, and provides delta detections
f(I) =nPAf+C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pjf(Trx′,y′(O)) =nPAf+C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pjδ(j,x′,y′). (58)
Applying a global pooling operator to f(I)then gives
zk= GAP(f(I))k=1
W′H′W′−1/summationdisplay
x=0H′−1/summationdisplay
y=0f(I)(k,x,y ) (59)
=1
W′H′W′−1/summationdisplay
x=0H′−1/summationdisplay
y=0
nPAf(k,x,y ) +C′−1/summationdisplay
j=0/summationdisplay
(O,x′,y′)∈Pjδ(j,x′,y′)(k,x,y )
(60)
=1
W′H′W′−1/summationdisplay
x=0H′−1/summationdisplay
y=0
nPaf,k+/summationdisplay
(O,x′,y′)∈Pkδ(k,x′,y′)(k,x,y )
 (61)
=1
W′H′/summationdisplay
(O,x′,y′)∈Pkδ(k,x′,y′)(k,x′,y′) +nPaf,k (62)
=1
W′H′|Pk|+nPaf,k (63)
B Experiments with overlapping object images
Table 2: Quantitative regression results for MNIST when digits are randomly placed and allowed to overlap.
Dataset Model Slope ( ˆβ1)p-value
MNIST (overlapping digits)Supervised (Simple-6 ) 0.302 0.000
SimCLR (Simple-6 ) 0.018 0.000
Table 2 and Figure 6 show the norm-count correspondence on a variant of MNIST where a number of digits
are placed randomly in a 128×128image. The results show that there is still a strong trend between norm
and count, event though the digit images are allowed to overlap with each other.
17Under review as submission to TMLR
12345678910
Count−2−10123Z-score normalized representation norm
Figure 6: Left: Correspondence between norm and count when objects are allowed to overlap. Right:
Example image with overlapping digits.
C Stepwise evaluation of monotonicity
In order to have a more fine-grained evaluation of the monotonicity in the relationship between norm and
count, we measure the mean change in norm when increasing the count by 1. The results are reported in
Table 3, and shows that the norm increases in the majority of cases ( >70%). We observe the same trend as
in Table 1 where the increase is stronger for the synthetic dataset, and is weakest for COCO, which is the
most difficult dataset.
We emphasize that for almost all experimental configurations, the negative changes are smaller, on average,
compared to the positive changes . This means that we have an overall positive trend between norm and
count, as found in Section 4.2.
18Under review as submission to TMLR
Table 3: Change in mean Z-score-normalized norm when increasing the count by 1.
Dataset Model 0→1 1→2 2→3 3→4 4→5 5→6 6→7 7→8
MNIST (Zeros)Supervised (Simple-6 )– 1.040 1.001 0.830 0.697 0.630 0.686 0.499
SimCLR (Simple-6 )– -0.828 -0.219 0.110 0.173 0.527 0.548 0.576
STL10 (Zeros)Supervised (ResNet-50 )– 0.201 0.281 0.255 0.226 0.205 0.197 0.056
SimCLR (ResNet-50 )– 1.111 0.524 0.330 0.143 0.048 0.005 0.009
STL10 (Random)Supervised (ResNet-50 )– 0.029 -0.070 -0.086 -0.010 0.006 -0.222 -0.018
SimCLR (ResNet-50 )– 0.990 0.477 0.352 0.184 0.080 0.137 0.025
STL10 (Blur)Supervised (ResNet-50 )– 0.352 0.445 0.234 0.364 0.247 0.304 0.191
SimCLR (ResNet-50 )– 0.639 0.341 0.213 0.112 0.119 0.041 -0.065
MSOSupervised (ResNet-50 )0.030 0.025 0.047 0.166 – – – –
SimCLR (ResNet-50 )0.461 -0.075 0.317 -0.415 – – – –
DenseCL (ResNet-50 )1.026 -0.004 0.297 0.003 – – – –
COCOSupervised (ResNet-50 )– – -0.006 0.011 -0.025 0.035 -0.007 0.089
SimCLR (ResNet-50 )– – 0.065 0.028 0.023 -0.000 -0.029 -0.013
DenseCL (ResNet-50 )– – -0.035 0.003 -0.055 -0.028 -0.059 0.055
Dataset Model 8→9 9→10 10→11 11→12 12→13 13→14 14→15 15→16
MNIST (Zeros)Supervised (Simple-6 )0.520 0.550 0.485 0.406 0.383 0.347 0.453 0.338
SimCLR (Simple-6 )0.606 0.575 0.574 0.683 0.619 0.632 0.635 0.573
STL10 (Zeros)Supervised (ResNet-50 )0.159 -0.062 0.095 -0.007 0.037 0.029 -0.003 0.008
SimCLR (ResNet-50 )-0.047 -0.016 -0.004 0.113 0.113 0.165 0.453 0.561
STL10 (Random)Supervised (ResNet-50 )-0.066 -0.033 -0.119 -0.167 -0.167 -0.318 -0.300 -0.589
SimCLR (ResNet-50 )-0.008 -0.050 -0.100 -0.031 -0.050 0.087 0.136 0.559
STL10 (Blur)Supervised (ResNet-50 )0.212 0.017 0.134 0.126 0.105 0.151 0.076 0.057
SimCLR (ResNet-50 )0.008 0.058 -0.072 0.071 -0.049 0.163 0.169 0.128
MSOSupervised (ResNet-50 )– – – – – – – –
SimCLR (ResNet-50 )– – – – – – – –
DenseCL (ResNet-50 )– – – – – – – –
COCOSupervised (ResNet-50 )-0.058 – – – – – – –
SimCLR (ResNet-50 )-0.020 – – – – – – –
DenseCL (ResNet-50 )-0.059 – – – – – – –
19