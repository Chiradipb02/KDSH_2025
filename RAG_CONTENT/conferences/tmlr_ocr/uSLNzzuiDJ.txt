Published in Transactions on Machine Learning Research (04/2024)
Enhancing Low-Precision Sampling via Stochastic Gradient
Hamiltonian Monte Carlo
Ziyi Wang wang4538@purdue.edu
Department of Statistics
Purdue University
Yujie Chen chen1866@purdue.edu
Department of Statistics
Purdue University
Qifan Song∗qfsong@purdue.edu
Department of Statistics
Purdue University
Ruqi Zhang∗ruqiz@purdue.edu
Department of Computer Science
Purdue University
Reviewed on OpenReview: https: // openreview. net/ forum? id= uSLNzzuiDJ
Abstract
Low-precision training has emerged as a promising low-cost technique to enhance the train-
ing efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian
counterpart can further provide uncertainty quantification and improved generalization ac-
curacy. This paper investigates low-precision sampling via Stochastic Gradient Hamilto-
nian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for
both strongly log-concave and non-log-concave distributions. Theoretically, our results show
that to achieve ϵ-error in the 2-Wasserstein distance for non-log-concave distributions, low-
precision SGHMC achieves quadratic improvement ( ˜O/parenleftbig
ϵ−2µ∗−2log2/parenleftbig
ϵ−1/parenrightbig/parenrightbig
) compared to
the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD)
(˜O/parenleftbig
ϵ−4λ∗−1log5/parenleftbig
ϵ−1/parenrightbig/parenrightbig
). Moreover, we prove that low-precision SGHMC is more robust
to the quantization error compared to low-precision SGLD due to the robustness of the
momentum-based update w.r.t. gradient noise. Empirically, we conduct experiments on
synthetic data, and MNIST, CIFAR-10 & CIFAR-100 datasets, which validate our theoreti-
cal findings. Our study highlights the potential of low-precision SGHMC as an efficient and
accurate sampling method for large-scale and resource-limited machine learning.
1 Introduction
Inrecentyears, whiledeepneuralnetworks(DNNs)standoutfortheirstate-of-artperformanceacrossvarious
AI tasks, accompanied by an increase in model and computation complexity (Simonyan & Zisserman, 2014;
He et al., 2016; Vaswani et al., 2017; Radford et al., 2018; Chen et al., 2023). Addressing this challenge,
techniques that enable efficient DNN processing become imperative to enhance efficiency and facilitate the
widespread deployment of DNNs in AI systems (Courbariaux et al., 2015; Sze et al., 2017). Consequently,
thereisagrowinginterestinutilizinglow-precisionoptimizationtechniquestoaddressthecomputationaland
memory costs associated with these complex models (Sze et al., 2017). By employing reduced precision for
∗Equal advising
1Published in Transactions on Machine Learning Research (04/2024)
Table 1: Theoretical results of the achieved 2-Wasserstein distance and the required gradient complexity
for both log-concave ( italic) and non-log-concave ( bold) target distributions, where ϵis any sufficiently
small constant, ∆is the quantization error, and µ∗andλ∗denote the contraction rate of underdamped
and overdamped Langevin dynamics respectively (Definition 1). Under non-log-concave target distributions,
low-precision SGHMC achieves a better upper bound within shorter iterations compared with low-precision
SGLD.
Condition Gradient Complexity Achieved 2-Wasserstein
Full-precision gradient accumulators
SGLD/SGHMC (Theorem 4) Strongly log-concave ˜O/parenleftbig
log/parenleftbig
ϵ−1/parenrightbig
ϵ−2/parenrightbig˜O(ϵ+ ∆)
SGLD(Theorem 7) Non-log-concave ˜O/parenleftbig
ϵ−4λ∗−1log5/parenleftbig
ϵ−1/parenrightbig/parenrightbig˜O/parenleftig
ϵ+ log/parenleftbig
ϵ−1/parenrightbig√
∆/parenrightig
SGHMC (Theorem 1) Non-log-concave ˜O/parenleftbig
ϵ−2µ∗−2log2/parenleftbig
ϵ−1/parenrightbig/parenrightbig˜O/parenleftig
ϵ+/radicalbig
log (ϵ−1) ∆/parenrightig
Low-precision gradient accumulators
SGLD/SGHMC (Theorem 5) Strongly log-concave ˜O/parenleftbig
log/parenleftbig
ϵ−1/parenrightbig
ϵ−2/parenrightbig˜O/parenleftbig
ϵ+ϵ−1∆/parenrightbig
VC SGLD/VC SGHMC (Theorem 6) Strongly log-concave ˜O/parenleftbig
log/parenleftbig
ϵ−1/parenrightbig
ϵ−2/parenrightbig˜O/parenleftig
ϵ+√
∆/parenrightig
SGLD(Theorem 8) Non-log-concave ˜O/parenleftbig
ϵ−4λ∗−1log5/parenleftbig
ϵ−1/parenrightbig/parenrightbig˜O/parenleftig
ϵ+ log5/parenleftbig
ϵ−1/parenrightbig
ϵ−4√
∆/parenrightig
VC SGLD (Theorem 9) Non-log-concave ˜O/parenleftbig
ϵ−4λ∗−1log3/parenleftbig
ϵ−1/parenrightbig/parenrightbig˜O/parenleftig
ϵ+ log3/parenleftbig
ϵ−1/parenrightbig
ϵ−2√
∆/parenrightig
SGHMC (Theorem 2) Non-log-concave ˜O/parenleftbig
ϵ−2µ∗−2log2/parenleftbig
ϵ−1/parenrightbig/parenrightbig˜O/parenleftig
ϵ+ log3/2/parenleftbig
ϵ−1/parenrightbig
ϵ−2√
∆/parenrightig
VC SGHMC (Theorem 3) Non-log-concave ˜O/parenleftbig
ϵ−2µ∗−2log2/parenleftbig
ϵ−1/parenrightbig/parenrightbig˜O/parenleftig
ϵ+ log/parenleftbig
ϵ−1/parenrightbig
ϵ−1√
∆/parenrightig
bothmodelanddatarepresentations(e.g. mixed-precision, low-bitsfixed-point, low-bitblockfloatingpoint),
significantimprovementscanbeachievedintermsofDNNtrainingspeedandresourceefficiency(Micikevicius
et al., 2017; Gupta et al., 2015; Li et al., 2017; De Sa et al., 2017; Zhou et al., 2016). Notably, several recent
studies(Wangetal.,2018;Banneretal.,2018;Wuetal.,2018;Linetal.,2019;Sunetal.,2019)demonstrated
the successful application of 8-bit training techniques in accelerating the training of different models, such
as VGG (Wu et al., 2018), ResNet (Banner et al., 2018), LSTMs, Transformers (Sun et al., 2019), and
vision-language models (Wortsman et al., 2023).
With the increasing demand and huge success of complicated architecture such as Large Languages Mod-
els (LLMs) and Vision-transformers, a wide range of quantization methods are adopted to reduce the com-
puting and memory consumption while retaining acceptable performances(Liu et al., 2023; Zhao et al., 2023;
Li et al., 2023; Xu et al., 2023; Xiao et al., 2023). Readers can find more information on low-precision
optimization and model compression in the survey papers (Sze et al., 2017; Deng et al., 2020; Liang et al.,
2021).
As a counterpart of low-precision optimization, low-precision sampling is relatively unexplored but has
shown promising preliminary results. Zhang et al. (2022) studied the effectiveness of Stochastic Gradient
Langevin Dynamics (SGLD) (Welling & Teh, 2011) in the context of low-precision arithmetic, highlighting
its superiority over the optimization counterpart, Stochastic Gradient Descent (SGD). This superiority stems
from SGLD’s inherent robustness to system noise compared with SGD.
Other than SGLD, Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014) is another
popular gradient-based sampling method closely related to the underdamped Langevin dynamics. Recently,
Cheng et al. (2018); Gao et al. (2022) showed that SGHMC converges to its target distribution faster than
the best-known convergence rate of SGLD in the 2-Wasserstein distance under both strongly log-concave and
non-log-concave assumptions. Beyond this, SGHMC is analogous to stochastic gradient methods augmented
with momentum, which is shown to have more robust updates w.r.t. gradient estimation noise (Liu et al.,
2020). Since the quantization-induced stochastic error in low-precision updates acts as extra gradient noise,
we believe SGHMC is particularly suited for low-precision arithmetic.
Our main contributions in this paper are threefold:
2Published in Transactions on Machine Learning Research (04/2024)
•We conduct the first study of low-precision SGHMC, adopting the low-precision arithmetic (in-
cluding full- and low-precision gradient accumulators and the variance correction (VC) version of
low-precision gradient accumulators) to SGHMC.
•We present a thorough theoretical analysis of low-precision SGHMC for both strongly log-concave
and non-log-concave target distributions. Beyond Zhang et al. (2022)’s analysis for strongly log-
concave distributions, we introduce an intermediate process for quantization noise management
to facilitate the analysis of non-log-concave target distributions. All our theoretical results are
summarized in Table 1, where we compare the 2-Wasserstein convergence limit and the required
gradient complexity. The table highlights the superiority of HMC-based low-precision algorithms
over SGLD counterpart w.r.t. convergence speed and robustness to quantization error, especially
under the non-log concave distributions.
•We provide promising empirical results across various datasets and models. We show the sampling
capabilities of HMC-based low-precision algorithms and the effectiveness of the VC function in both
strongly log-concave and non-log-concave target distributions. We also demonstrate the superior
performance of HMC-based low-precision algorithms compared to SGLD in deep learning tasks.
Our code is available here.
In summary, low-precision SGHMC emerges as a compelling alternative to standard SGHMC due to its
ability to enhance speed and memory efficiency without sacrificing accuracy. These advantages position
low-precision SGHMC as an attractive option for efficient and accurate sampling in scenarios where reduced
precision representations are employed.
It is worth mentioning that low-precision gradient representations are also used in Federated Learning (FL)
for either optimization (Gorbunov et al., 2021; Tyurin & Richtárik, 2022) or sampling tasks (Vono et al.,
2022; Sun et al., 2022; Karagulyan & Richtárik, 2023). These methods use low-precision representations for
between-node communication, aiming to mitigate communication bottlenecks, but still utilize full-precision
arithmetic for local training. Thus these methods do not apply to the low-precision sampling challenge
studied in this paper.
2 Preliminaries
2.1 Low-Precision Quantization
Twopopularlow-precisionnumberrepresentationformatsareknownasthe fixed point (FP)and block floating
point(BFP) (Song et al., 2018). Theoretical investigation of this paper only considers the fixed point case,
where the quantization error (i.e., the gap between two adjacent representable numbers) is denoted as ∆.
For example, if we use 8 bits to represent a number where 1 bit is assigned for the sign, 2 bits for the integer
part, and 5 bits for the fractional part, then the gap between two consecutive low-precision numbers is 2−5,
i.e.,∆ = 2−5. Furthermore, all representable numbers are truncated to an upper limit ¯Uand a lower limit
¯L.
Given the low-precision number representation, a quantization function is desired to round real-valued num-
bers to their low-precision counterparts. Two common quantization functions are deterministic rounding and
stochastic rounding . The deterministic rounding function, denoted as Qd, quantizes a number to its nearest
representable neighbor. The stochastic rounding, denoted as Qs(refer to (19) in Appendix D), randomly
quantizes a number to its close representable neighbor satisfying the unbiased condition, i.e. E[Qs(θ)] =θ. In
what follows, QWandQGdenote stochastic rounding quantizers for the weights and gradients respectively,
allowing different quantization errors (i.e., different ∆’s forQWandQG). For simplicity in the analysis and
experiments, we use the same number of bits to represent the weights and gradients.
3Published in Transactions on Machine Learning Research (04/2024)
2.2 Low-precision Stochastic Gradient Langevin Dynamics
When performing gradient updates in low-precision training, there are two common choices, full-precision
and low-precision gradient accumulators depending on whether we store an additional copy of full-precision
weights. Low-precision SGLD (Zhang et al., 2022) considers both choices.
Low-precision SGLD with full-precision gradient accumulators (SGLDLP-F) only quantizes weights before
computing the gradient. The update rule can be defined as:
xk+1=xk−ηQG/parenleftig
/tildewidest∇U(QW(xk))/parenrightig
+/radicalbig
2ηξk+1, (1)
/tildewidest∇Uis the unbiased gradient estimation of U. Zhang et al. (2022) showed that the SGLDLP-F outperforms
its counterpart low-precision SGD with full-gradient accumulators (SGDLP-F). The computation costs can
be further reduced using low-precision gradient accumulators by only keeping low-precision weights. Low-
precision SGLD with low-precision gradient accumulators (SGLDLP-L) can be defined as the following:
xk+1=QW/parenleftig
xk−ηQG(/tildewidest∇U(xk)) +/radicalbig
2ηξk+1/parenrightig
. (2)
Zhang et al. (2022) studied the convergence property of both SGLDLP-F and SGLDLP-L under strongly-
log-concave distributions and showed that a small step size deteriorates the performance of SGLDLP-L. To
mitigate this problem, Zhang et al. (2022) proposed a variance-corrected quantization function (Algorithm
2 in Appendix D).
2.3 Stochastic Gradient Hamiltonian Monte Carlo
Given a dataset D, a model with weights (i.e., model parameters) x∈Rd, and a prior p(x), we are inter-
ested in sampling from the posterior p(x|D)∝exp(−U(x)), whereU(x) =−logp(D|x)−logp(x)is the
energy function. In order to sample from the target distribution, SGHMC (Chen et al., 2014) is proposed
and strongly related to the underdamped Langevin dynamics. Cheng et al. (2018) proposed the following
discretization of underdamped Langevin dynamics (10) with stochastic gradient:
vk+1=vke−γη−uγ−1(1−e−γη)/tildewidest∇U(xk) +ξv
k (3)
xk+1=xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)/tildewidest∇U(xk) +ξx
k,
whereu,γdenote the hyperparameters of the inverse mass and friction respectively, and ξv
k,ξx
kare normal
distributed in Rdsatisfying that :
Eξv
k(ξv
k)⊤=u(1−e−2γη)·I,
Eξx
k(ξx
k)⊤=uγ−2(2γη+ 4e−γη−e−2γη−3)·I, (4)
Eξx
k(ξv
k)⊤=uγ−1(1−2e−γη+e−2γη)·I.
3 Low-Precision Stochastic Gradient Hamiltonian Monte Carlo
In this section, we investigate the convergence property of low-precision SGHMC under non-log-concave
target distributions. We defer the convergence analysis of low-precision SGHMC under strongly log-concave
target distributions, as well as the extension analysis under non-log-concave target distributions of low-
precision SGLD (Zhang et al., 2022) to Appendix A and B respectively. All of our theorems are based on
the fixed point representation and omit the clipping effect. We show that low-precision SGHMC exhibits
superior convergence rates and mitigates the performance degradation caused by the quantization error than
low-precision SGLD, especially for non-log-concave target distributions. Similar to Zhang et al. (2022),
we also observe an overdispersion phenomenon in sampling distributions obtained by SGHMC with low-
precisiongradientaccumulators, andweexaminetheeffectivenessofvariance-correctedquantizationfunction
in resolving this overdispersion problem.
4Published in Transactions on Machine Learning Research (04/2024)
In the statement of theorems, the big-O notation ˜Ogives explicit dependence on the quantization error ∆
and concentration parameters ( λ∗,µ∗) but hides multiplicative terms that polynomially depend on the other
parameters (e.g., dimension d, frictionγ, inverse mass uand gradients variance σ2). We refer readers to the
appendix for all the theorems’ proof. Before diving into theorems, we first introduce necessary assumptions
for the convergence analysis as follows:
Assumption 1 (Smoothness) .The energy function UisM-smooth, i.e., there exists a positive constant M
such that
∥∇U(x)−∇U(y)∥2≤M2∥x−y∥2,for any x,y∈Rd.
Assumption 2 (Dissaptiveness) .There exist constants m2,b> 0, such that the following holds
⟨∇U(x),x⟩≥m2∥x∥2−b,for any x∈Rd.
Assumption 3 (Bounded Variance) .There exists a constant σ2>0, such that the following holds
E/vextenddouble/vextenddouble/vextenddouble/tildewidest∇U(x)−∇U(x)/vextenddouble/vextenddouble/vextenddouble2
≤σ2,for any x∈Rd.
Beyond the above assumptions, we further define κ1=M/m 1(refer to Assumption 4 in Appendix A) and
κ2=M/m 2as the condition numbers for strongly log-concave and non-log-concave target distribution,
respectively, and denote the global minimum of U(x)asx∗. All of our assumptions are standard and
commonly used in the sampling literature. In particular, Assumption 2 is a standard assumption (Raginsky
et al., 2017; Zou et al., 2019; Gao et al., 2022) in the analysis of sampling from non-log-concave distributions
and is essential to guarantee the convergence of underdamped Langevin dynamics. Assumption 3 can be
further relaxed, allowing the variance of /tildewidest∇U(x)scale up w.r.t x. Please refer to the appendix H-P.
Definition 1. Letλ∗andµ∗denote the contraction rates for continuous-time overdamped Langevin dy-
namics and underdamped Langevin dynamics respectively. In other words, let xtfollow the overdamped (or
underdamped) Langevin dynamics initialized at x0= 0,πzbe the invariant distribution, ptbe the marginal
distribution xt, thenλ∗andµ∗satisfy
W2
2(pt,πz)≤Ce−λ∗t/d,orW2
2(pt,πz)≤Ce−µ∗t/d,
for some constant C.
The contraction rates µ∗andλ∗are related to the nature of the Langevin dynamics. In general, the
contraction rates exponentially depend on the dimension d. For example, two popular approaches to analyze
the Wasserstein distance convergence property are the couplings method (Dalalyan & Riou-Durand, 2020;
Eberle et al., 2019) and Bakry–Émery method based on which the exponential convergence of the kinetic
Fokker–Planck equation is proved (Bakry & Émery, 2006; Baudoin, 2016; 2017). Unfortunately, both rates
leadtoexponentialdependencyondimensioningeneral. Itraisesacrucialopenquestionofwhetherrestricted
models can exhibit improved dimensional dependence. While overdamped Langevin diffusions have seen
corresponding advancements, as evidenced in (Eberle et al., 2019; Zimmer, 2017), analogous progress for
underdamped Langevin diffusions remains underexplored. More detailed discussions of µ∗andλ∗(and their
uniform bounds) can be found in the Appendix C.
3.1 Full-Precision Gradient Accumulators
Adopting the update rule in equations (3), we propose low-precision SGHMC with full gradient accumulators
(SGHMCLP-F) as the following:
vk+1=vke−γη−uγ−1(1−e−γη)QG(/tildewidest∇U(QW(xk))) +ξv
k (5)
xk+1=xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(QW(xk))) +ξx
k,
which keeps full-precision parameters vk,xkat each iteration and quantizes them to low-precision represen-
tations before taking the gradient. Our analysis for non-log-concave distributions utilizes similar techniques
in Raginsky et al. (2017). We are now ready to present our first theorem:
5Published in Transactions on Machine Learning Research (04/2024)
Theorem 1. Assuming 1, 2 and 3 hold. Let p∗denote the target distribution of (x,v). Ifγ2≤4Muand
setting the step size η=˜O/parenleftig
µ∗ϵ2
log(1/ϵ)/parenrightig
satisfying
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d,1
8γ,γm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ/bracerightigg
,
then afterKsteps starting at the initial point x0=v0= 0, the output (xK,vK)of SGHMCLP-F in (5)
satisfies
W2(p(xK,vK),p∗)≤˜O/parenleftigg
ϵ+/tildewideA/radicaligg
log/parenleftbigg1
ϵ/parenrightbigg/parenrightigg
,
for someKsatisfying
K=˜O/parenleftbigg1
ϵ2µ∗2log2/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
,
where constants are defined as: /tildewideA= max/braceleftbig√
∆2d+σ2,4√
∆2d+σ2/bracerightbig
.
Similar to the convergence result of full-precision SGHMC or SGLD (Raginsky et al., 2017; Gao et al.,
2022), the above upper bound of the 2-Wasserstein distance contains an ϵterm and a log(ϵ−1)term. The
difference is that for the SGHMCLP-F algorithm, the quantization error ∆affects the multiplicative constant
of the log(ϵ−1)term. Focusing on the effect of quantization error ∆, due to the fact that log(x)≤x1/e,
one can tune the choice of ϵandηand obtain a ˜O/parenleftbig
∆e/(1+2e)/parenrightbig
2-Wasserstein bound. As for the non-
convergence of our result (i.e, log(ϵ−1)term), we note that even for full-precision sample algorithms, the
best non-asymptotic convergence result in the 2-Wasserstein distance (Zou et al., 2019; Raginsky et al., 2017;
Gao et al., 2022) also contain a log(ϵ−1)factor which is brought by stochastic gradient noise, and diverge
asϵ→0. The non-convergence of our Wasserstein upper bound is due to the accumulation of stochastic
gradient noise and stochastic discretion error. Conceptually, these random errors may average out over
iterations when the iteration number increases to infinity (i.e., the law of large numbers), as in the classical
ergodic theory of Markov chain (Theorem 17.25, 17.28 of Kallenberg & Kallenberg (1997)). However, our
mathematical tools lead to an upper bound that involves some weighted summation of the norm of these
random errors over iterations rather than the summation of these random errors. Under strongly log-concave
target distributions with no discretion error, this sum is bounded as t→∞and proportional to the stepsize,
allowing for a sufficiently small step size to zero the bound Dalalyan & Karagulyan (2019); Cheng et al.
(2018). However, for general cases, this sum grows to infinity. It is yet an open question to sharpen this
type of analysis.
With the same technical tools, we conduct a similar convergence analysis of SGLDLF-P for non-
log-concave target distributions. The details are deferred in Theorem 7 of Appendix B. Compar-
ing Theorems 1 and 7, we show that SGHMCLP-F can achieve lower 2-Wasserstein distance (i.e.,
˜O/parenleftig
log1/2/parenleftbig
ϵ−1/parenrightbig
∆1/2/parenrightig
versus ˜O/parenleftbig
log/parenleftbig
ϵ−1/parenrightbig
∆1/2/parenrightbig
) for non-log-concave target distribution within fewer itera-
tions (i.e., ˜O/parenleftbig
ϵ−2µ∗−2log2/parenleftbig
ϵ−1/parenrightbig/parenrightbig
versus ˜O/parenleftbig
ϵ−4λ∗−1log5/parenleftbig
ϵ−1/parenrightbig/parenrightbig
). Furthermore, by the same argument in
the previous paragraph, after carefully choosing the stepsize η, the 2-Wasserstein distance of the SGLDLF-P
algorithm can be further bounded by ˜O/parenleftbig
∆e/(2+2e)/parenrightbig
which is worse than the bound ˜O/parenleftbig
∆e/(1+2e)/parenrightbig
obtained
by SGHMC. We verify the advantage of SGHMCLF-P over SGLDLF-P by our simulations in section 4.
3.2 Low-Precision Gradient Accumulators
The storage and computation costs of low-precision algorithms can be further reduced by low-precision
gradient accumulators. We can adopt low-precision SGHMC with low-precision gradient accumulators
(SGHMCLP-L) as
vk+1=QW/parenleftig
vke−γη−uγ−1(1−eγη)QG(/tildewidest∇U(xk)) +ξv
k/parenrightig
, (6)
xk+1=QW/parenleftig
xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xk)) +ξx
k/parenrightig
.
6Published in Transactions on Machine Learning Research (04/2024)
Similar to the observation of Zhang et al. (2022), we also empirically find that the output xK’s distribution
has a larger variance than the target distribution (see Figures 1 (a) and 2 (a)), as the update rule (6)
introduces extra rounding noise. Our theorem in the section aims to support this argument. We present the
convergence theorem of SGHMCLP-L under non-log-concave target distributions.
Theorem 2. Assuming 1, 2 and 3 hold. Let p∗denote the target distribution of (x,v). Ifγ2≤4Muand
setting the step size η=˜O/parenleftig
µ∗ϵ2
log(1/ϵ)/parenrightig
satisfying
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d,1
8γ,γm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ/bracerightigg
,
then afterKsteps starting at the initial point x0=v0= 0, the output (xK,vK)of SGHMCLP-L in (6)
satisfies
W2(p(xK,vK),p∗) =˜O/parenleftigg
ϵ+/radicaligg
max{σ2,σ}log/parenleftbigg1
ϵ/parenrightbigg
+log3/2/parenleftbig1
ϵ/parenrightbig
ϵ2√
∆/parenrightigg
, (7)
for someKsatisfying
K=˜O/parenleftbigg1
ϵ2µ∗2log2/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
For non-log-concave target distribution, the output of the naïve SGHMCLP-L has a worse convergence upper
bound than Theorem 1. The source of the observed problem is the variance introduced by the quantization
QW, causing actual variances of (xk,vk)to be larger than the variances needed. In Theorem 8, we generalize
theresultofthenaïveSGLDLP-Lin(Zhangetal.,2022)tonon-log-concavetargetdistributions, andwedefer
this theorem to appendix B. Similarly, we observe that SGHMCLP-L needs fewer iterations than SGLDLP-
L in terms of the order w.r.t. ϵandlog(ϵ−1)(˜O/parenleftbig
ϵ−2µ∗−2log2/parenleftbig
ϵ−1/parenrightbig/parenrightbig
versus ˜O/parenleftbig
ϵ−4λ∗−1log5/parenleftbig
ϵ−1/parenrightbig/parenrightbig
) and
achieves better upper bound ˜O/parenleftig
ϵ−2log3/2/parenleftbig
ϵ−1/parenrightbig√
∆/parenrightig
versus ˜O/parenleftig
ϵ−4log5/parenleftbig
ϵ−1/parenrightbig√
∆/parenrightig
.
By the same argument in Theorem 1’s discussion, after carefully choosing the stepsize η, the 2-Wasserstein
distance between samples obtained by SGHMCLP-L and non-log-concave target distributions can be further
bounded as ˜O/parenleftbig
∆e/(3+6e)/parenrightbig
, whilst the distance between the samples obtained by SGLDLP-L to the target
can be bounded as ˜O/parenleftbig
∆e/10(1+e)/parenrightbig
. Thus, low-precision SGHMC is more robust to the quantization error
than SGLD.
3.3 Variance Correction
To resolve the overdispersion caused by low-precision gradient accumulators, Zhang et al. (2022) proposed
a quantization function Qvc(refer to Algorithm 2 in Appendix D) that directly samples from the discrete
weight space instead of quantizing a real-valued Gaussian sample. This quantization function aims to reduce
the discrepancy between the ideal sampling variance (i.e., the required variance of full-precision counterpart
algorithms) and the actual sampling variance in our low-precision algorithms. We adopt the variance-
corrected quantization function to low-precision SGHMC (VC SGHMCLP-L) and study its convergence
property for non-log-concave target distributions. We extend the convergence analysis of VC SGLDLP-L
in Zhang et al. (2022) to the case of the non-log-concave distributions as well. The details are deferred to
AppendixBforcomparisonpurposes. Let Varhmc
v=u(1−e−2γη)andVarhmc
x=uγ−2(2γη+4e−γη−e−2γη−3),
which are the variances added by the underdamped Langevin dynamics in (3). The VC SGHMCLP-L can
be done as follows:
vk+1=Qvc/parenleftig
vke−γη−uγ−1(1−e−γη)QG(/tildewidest∇U(xk)),Varhmc
v,∆/parenrightig
, (8)
xk+1=Qvc/parenleftig
xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xk)),Varhmc
x,∆/parenrightig
.
The variance corrected quantization function Qvcaims to output a low-precision random variable with the
desired mean and variance. When the desired variance vis larger than ∆2/4, which is the largest possible
7Published in Transactions on Machine Learning Research (04/2024)
variance introduced by the quantization Qs, the variance-corrected quantization first adds a small Gaussian
noise to compensate for the variance and then adds a categorical random variable with a desired variance.
Whenvis less than ∆2/4the variance-corrected quantization computes the actual variance introduced by
Qs. If it is larger than v, a categorical random variable is added to the weights to match the desired variance
v. If it is less than v, we will not be able to match the variance after quantization. However, this case arises
only with exceptionally small step sizes. With the variance-corrected quantization Qvcin hand, we now
present the convergence analysis of the VC SGHMCLP-L for non-log-concave distributions.
Theorem 3. Assuming 1, 2 and 3 hold and E/vextenddouble/vextenddouble/vextenddoubleQG(/tildewidest∇U(x))/vextenddouble/vextenddouble/vextenddouble2
2≤G2. Letp∗be the target distribution of x.
Ifγ2≤4Muand setting the step size η=˜O/parenleftig
µ∗ϵ2
log(1/ϵ)/parenrightig
satisfying
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d,1
8γ,γm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ/bracerightigg
,
then afterKsteps starting at the initial point x0=v0= 0the output (xK)of the VC SGHMCLP-L in (9)
satisfies
W2(p(xK),p∗) =˜O/parenleftigg
ϵ+/radicaligg
max{σ2,σ}log/parenleftbigg1
ϵ/parenrightbigg
+log/parenleftbig1
ϵ/parenrightbig
ϵ√
∆/parenrightigg
, (9)
for someKsatisfying
K=˜O/parenleftbigg1
ϵ2µ∗2log2/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
Compared with Theorem 1, we cannot show that the variance corrected quantization fully resolves the
overdispersion problem observed for non-log-concave target distributions. However comparing with Theo-
rem 2, we show in Theorem 3 that the variance-corrected quantization can improve the upper bound w.r.t.
ϵfrom ˜O/parenleftig
ϵ−2log3/2/parenleftbig
ϵ−1/parenrightbig√
∆/parenrightig
to˜O/parenleftig
ϵ−1log/parenleftbig
ϵ−1/parenrightbig√
∆/parenrightig
. In Theorem 9, we generalize the result of the
VC SGLDLP-L in (Zhang et al., 2022) to non-log-concave target distributions, and we defer this theorem
to appendix B. Similarly, we observe that VC SGHMCLP-L needs fewer iterations than VC SGLDLP-L in
terms of the order w.r.t. ϵandlog(ϵ−1)(˜O/parenleftbig
ϵ−2µ∗−2log2/parenleftbig
ϵ−1/parenrightbig/parenrightbig
versus ˜O/parenleftbig
ϵ−4λ∗−1log5/parenleftbig
ϵ−1/parenrightbig/parenrightbig
).
Beyond the above analysis, we apply similar mathematical tools and study the convergence property of VC
SGHMCLP-L and VC SGLDLP-L in terms of ∆for non-log-concave target distributions. Based on the
Theorem 2 and 3, the variance-corrected quantization can improve the upper bound from ˜O/parenleftbig
∆e/(3+6e)/parenrightbig
to˜O/parenleftbig
∆e/(2+4e)/parenrightbig
. Compared with VC SGLDLP-L, the VC SGHMCLP-L has a better upper bound (i.e.
˜O/parenleftbig
∆e/(2+4e)/parenrightbig
versus ˜O/parenleftbig
∆e/6(1+e)/parenrightbig
). Interestingly, the naïve SGHMCLP-L has similar dependence on the
quantization error ∆with VC SGLDLP-L but saves more computation resources since the variance corrected
quantization requires sampling discrete random variables. We verify our findings in Table 4.
4 Experiments
We evaluate the performance of the proposed low-precision SGHMC algorithms across various experiments:
Gaussian and Gaussian mixture distributions (Section 4.1), Logistic Regression and Multi-Layer Perceptron
(MLP) applied to the MNIST dataset (Section 4.2), and ResNet-18 on both CIFAR-10 and CIFAR-100
datasets (Section 4.3). Additionally, we compare the accuracy of our proposed algorithms with their SGLD
counterparts. Throughout all experiments, low-precision arithmetic is implemented using qtorch(Zhang
et al., 2019). Beyond our theoretical settings, our experiments encompass a range of low-precision setups, in-
cluding fixed point, block floating point, as well as quantization of weights, gradients, errors, and activations.
For more details of our low-precision settings used in experiments, please refer to Appendix D
4.1 Sampling from standard Gaussian and Gaussian mixture distributions
We first demonstrate the performance of low-precision SGHMC for fitting synthetic distributions. We use
the standard Gaussian distribution and Gaussian mixture distribution to represent strongly log-concave and
8Published in Transactions on Machine Learning Research (04/2024)
−6−4−2 0 2 40.00.10.20.30.4DensityTrue
SGHMCLP-L
−6−4−2 0 2 40.00.10.20.30.4DensityTrue
VC SGHMCLP-L
−6−4−2 0 2 40.00.10.20.30.4DensityTrue
SGHMCLP-F
(a) (b) (c)
Figure 1: Low-precision SGHMC on a Gaussian distribution. (a): SGHMCLP-L. (b): VC SGHMCLP-L.
(c): SGHMCLP-F. VC SGHMCLP-L and SGHMCLP-F converge to the true distribution, whereas naïve
SGHMCLP-L suffers a larger variance.
−6−4−2 0 2 40.00.10.20.30.4DensityTrue
SGHMCLP-L
−6−4−2 0 2 40.00.10.20.30.4DensityTrue
VC SGHMCLP-L
−6−4−2 0 2 40.00.10.20.30.4DensityTrue
SGHMCLP-F
(a) (b) (c)
Figure 2: Low-precision SGHMC with on a Gaussian mixture distribution. (a): SGHMCLP-L. (b): VC
SGHMCLP-L. (c): SGHMCLP-F. VC SGHMCLP-L and SGHMCLP-F converge to the true distribution,
whereas naïve SGHMCLP-L suffers a larger variance.
0.0 0.2 0.4 0.6 0.8 1.0
Iteration ×1060.00.20.40.60.8Log L2 Distance of Density
SGHMCLP-L
VC SGHMCLP-L
SGLDLP-L
VC SGLDLP-L
0.0 0.2 0.4 0.6 0.8 1.0
Iteration ×1060.00.20.40.60.8Log L2 Distance of DensitySGHMCLP-F
SGLDLP-F
(a) (b)
Figure 3: Log L2distance from sample density estimation obtained by low-precision SGHMC and SGLD
to the Gaussian mixture distribution. (a) Low-precision gradient accumulators. (b): Full-precision gradient
accumulators. Overall, SGHMC methods enjoy a faster convergence speed. In particular, SGHMCLP-L
achieves a lower distance compared to SGLDLP-L and VC SGLDLP-L.
non-log-concave distribution, respectively. The density of the Gaussian mixture example is defined as
e−U(x)=e2∥x−1∥2+e2∥x+1∥2.
We use 8-bit fixed point representation with 4 of them representing the fractional part. For hyper-parameters
please the Appendix D. The simulation results are shown in Figure 1 and 2. From Figure 1(a) and 2(a),
9Published in Transactions on Machine Learning Research (04/2024)
we see that the sample from naïve SGHMCLP-L has a larger variance than the target distribution. This
verifies the results we prove in Theorem 2. In Figure 1(b) and 2(b), we verify that the variance-corrected
quantizer mitigates this problem by matching variance of the quantizer to the variance Varhmc
xdefined
by the underdamped Langevin dynamics (10). In Figure 3, we compare the performance of low-precision
SGHMC with low-precision SGLD for sampling from Gaussian mixture distribution. Since calculating the 2-
Wasserstein distance over long iterations is time-consuming, instead of computing the Wasserstein distance,
we resort to L2distance of the sample density estimation to the true density function. It shows that low-
precision SGHMC enjoys faster convergence speed and smaller distance, especially SGHMCLP-L compared
to SGLDLP-L and VC SGLDLP-L.
0 20 40 60 80
Variance Ratio0.60.70.80.91.0Distance Ratio
Figure 4: Mean (dotted line) and 95% confidence inter-
val (shaded area) of 2-Wasserstein error ratio between VC
SGHMCLP-L&SGHMCLP-L(Smallermeansthevariance
correction is more effective), computed over 5 experimental
runs. The x-axis represents the ratio between Varhmc
xand
∆2/4.We also study in which case the variance-
corrected quantizer is advantageous over the
naïve stochastic quantization function. We
test the 2-Wasserstein sampling error of VC
SGHMCLP-L and SGHMCLP-L over differ-
ent variances. The result is shown in Figure
4. We find that when the variance Varhmc
x
is close to the largest quantization variance
∆2/4, thevariancecorrectedquantizationfunc-
tion shows the largest advantage over the naïve
quantization. When the variance Varhmc
xis
less than ∆2/4, the correction has a chance to
fail. When the variance Varhmc
xis 100 times
the quantization variance, the advantage of
variance-corrected quantizer shows less advan-
tage. One possible reason is that the quan-
tization variance eliminated by the variance-
corrected quantizer is not critical compared
toVarhmc
xwhich is the intrinsic variance for
SGHMC. We advocate for the adoption of
variance-corrected quantization under the specific condition where the ideal variance approximates ∆2/4.
Our observations indicate that this scenario yields the most significant performance gains. Conversely, in
other situations, we suggest employing naïve low-precision gradient accumulators, as they offer comparable
performance while conserving computational resources.
4.2 MNIST
Inthissection, wefurtherexaminethesamplingperformanceoflow-precisionSGHMCandSGLDonstrongly
log-concavedistributionsandnon-log-concavedistributionsonreal-worlddata. Weuselogisticandmultilayer
perceptron (MLP) models to represent the class of strongly log-concave and non-log-concave distributions,
respectively. The results are shown in Figure 5 and 6. We use N/parenleftbig
0,10−2/parenrightbig
as the prior distribution and fixed
point number representation, where we set 2integer bits and various fractional bits. A smaller number of
fractional bits corresponds to a larger quantization gap ∆. For MLP model, we use two-layer MLP with 100
hidden units and ReLu nonlinearities. We report the training negative log-likelihood (NLL) with different
numbers of fractional bits in Figure 5 and 6. For detailed hyperparameters and experiment setup, please see
Appendix D.
From the results on MNIST, we can see that when using full-precision gradient accumulators, low-precision
SGHMC are robust to the quantization error. Even when we use only 2fractional bits, SGHMCLP-F can
still converge to a distribution with a small and stable NLL but with more iterations. However, regarding
low-precision gradient accumulators, SGHMCLP-L and SGLDLP-L are less robust to the quantization error.
As the precision error increases, both SGHMCLP-L and SGLDLP-L have a worse convergence pattern com-
pared to SGHMCLP-F and SGLDLP-F. We showed empirically that SGHMCLP-L and VC SGHMCLP-L
outperform SGLDLP-L and VC SGLDLP-L. As shown in Figure 5 and 6, when we increase the quanti-
10Published in Transactions on Machine Learning Research (04/2024)
0 500 1000 1500
Iteration0.00.51.01.52.02.53.03.5Log Train NLL
SGHMCLP-F 4
SGHMCLP-F 2
SGLDLP-F 4
SGLDLP-F 2
0 500 1000 1500
Iteration0246810Log Train NLL
SGHMCLP-L 4
SGHMCLP-L 2
SGLDLP-L 4
SGLDLP-L 2
0 500 1000 1500
Iteration012345Log Train NLL
VC SGHMCLP-L 4
VC SGHMCLP-L 2
VC SGLDLP-L 4
VC SGLDLP-L 2
(a) (b) (c)
Figure 5: Training NLL of low-precision SGHMC and SGLD on logistic model with MNIST in terms of
different numbers of fractional bits. (a): Full-precision gradient accumulators. (b): Low-precision gradient
accumulators. (c): Variance-corrected quantizer. SGHMCLP-F achieves comparable results with SGLDLP-
F.However, bothSGHMCLP-LandVCSGHMCLP-Lshowmorerobustnesstoquantizationerror, especially
when the number of representable bits is low. Please be aware of the different scales of y-axis across three
figures.
0 500 1000 1500
Iteration0.00.51.01.52.02.53.03.5Log Train NLL
SGHMCLP-F 4
SGHMCLP-F 2
SGLDLP-F 4
SGLDLP-F 2
0 500 1000 1500
Iteration0246810Log Train NLL
SGHMCLP-L 4
SGHMCLP-L 2
SGLDLP-L 4
SGLDLP-L 2
0 500 1000 1500
Iteration01234Log Train NLL
VC SGHMCLP-L 4
VC SGHMCLP-L 2
VC SGLDLP-L 4
VC SGLDLP-L 2
(a) (b) (c)
Figure 6: Training NLL of low-precision SGHMC and SGLD on MLP with MNIST in terms of different num-
bers of fractional bits. (a): Full-precision gradient accumulators. (b): Low-precision gradient accumulators.
(c): Variance-corrected quantizer. SGHMCLP-F achieves comparable results with SGLDLP-F. However,
both SGHMCLP-L and VC SGHMCLP-L show more robustness to quantization error, especially when the
number of representable bits is low. Please be aware of the different scales of y-axis across three figures.
zation error, SGHMCLP-L and VC SGHMCLP-L are more robust than SGLDLP-L and VC SGLDLP-L,
respectively.
4.3 CIFAR-10 & CIFAR-100
We consider image tasks CIFAR-10 and CIFAR-100 on the ResNet-18. We use 8-bit number representation
following Zhang et al. (2022). We report the test errors averaging over 3runs in Tables 2 and 4. For detailed
hyperparameters and experiment setup, please see Appendix D.
Fixed Point We employ fixed point representations for both weights and gradients while retaining full
precision for activations and errors following previous work (Zhang et al., 2022). From the figure, unsur-
prisingly the full-precision algorithms outperform their low-precision counterparts. But with long enough
iterations the performance gap between SGHMC/SGLD and SGHMCLP-F/SGLDLP-F converges toward
zero. Similar to the results in previous sections, SGHMCLP-F is comparable with SGDLP-F, and the naïve
SGHMCLP-L significantly outperforms naïve SGLDLP-L and SGDLP-L across datasets and architectures.
For example, SGHMCLP-L outperforms SGLDLP-L by 1.19% on CIFAR-10, and SGHMCLP-L outperforms
SGLDLP-L by 0.58% on CIFAR-100. Furthermore, from the result in Figure 7, we empirically show that
11Published in Transactions on Machine Learning Research (04/2024)
0 20000 40000 60000 80000
Iteration−5−4−3−2−1012Log Train NLL
SGLD
SGLDLP-F
SGLDLP-L
VC SGLDLP-L
SGHMC
SGHMCLP-F
SGHMCLP-L
VC SGHMCLP-L
0 20000 40000 60000 80000
Iteration−5−4−3−2−1012Log Train NLL
SGLD
SGLDLP-F
SGLDLP-L
VC SGLDLP-L
SGHMC
SGHMCLP-F
SGHMCLP-L
VC SGHMCLP-L
(a) (b)
Figure 7: Log of training NLL of low-precision SGHMC and SGLD on ResNet-18 with CIFAR-100. (a):
8-bits Fixed Point. (b): 8-bits Block Floating Point. For fixed point representations, low-precision SGHMC
shows faster convergence and SGHMCLP-L outperforms SGLDLP-L and VC SGLDLP-L.
the convergence speed of SGHMC is way better than the convergence speed of SGLD. SGHMCLP-L even
achieves faster convergence than SGLDLP-F. When the variance Varhmc
xis comparable with or less than
∆2/4, we recommend implementing SGHMCLP-L rather than VC SGHMCLP-L. This is the case when we
assess the performance of low-precision SGHMC on CIFAR-10 and CIFAR-100. Notably, even in the absence
of the performance enhancement provided by the variance-corrected quantization function, the test results
indicate that SGHMCLP-L’s performance is on par with its SGLD counterpart with variance correction.
This result verifies our findings in Theorems 2 and 9.
Block Floating Point We also consider the block floating point (BFP) representation adopted with deep
models, which causes less quantization error and thus performs better compared with fixed point represen-
tation (Song et al., 2018). Given sufficient iterations, the performance differences between SGHMC/SGLD
and SGHMCLP-F/SGLDLP-F almost disappear. As illustrated in plot b) of Figure 7, SGHMCLP-F out-
performs full-precision SGLD in block floating point low-precision format. By using BFP, the performance
of all low-precision methods improves over fixed point representation. The naïve SGHMCLP-L outperforms
the naïve SGLDLP-L 0.82%. Moreover, the naïve SGHMCLP-L achieves comparable results with the VC
SGLDLP-L method, and SGHMCLP-L can save more computation resources since the variance-corrected
quantization function would need to sample an additional categorical random vector c∈Rdat each iter-
ation. Let Varsgld
x= 2ηdenote the variance added by overdamped Langevin dynamics in (14). For most
deep learning tasks, a small step size is preferred, and thus there is a large chance that Varsgld
x≤∆2/4in
which case we recommend running the naïve SGHMCLP-L to achieve comparable accuracy and save more
computation resources.
Expected Calibration Error To study the model calibration of low-precision SGHMC, we further report
the results of expected calibration error (ECE) (Guo et al., 2017) in Table 3 and 5. We observe that
sometimes SGLDLP-L and SGLDLP-F achieve a lower ECE than the full-precision SGLD counterpart,
implying that the corresponding sample distributions deviate from the true target posterior. We conjecture
that it is caused by the implicit regularization effect of the operator QW. On the other hand, we observe
that SGHMCLP-F and SGHMCLP-L have almost the same ECE as full-precision SGHMC in CIFAR-10,
showing that low-precision arithmetic does not degrade the calibration ability of SGHMC. In the CIFAR-100
dataset, HMC-based low-precision algorithms outperform their SGLD counterparts, especially SGHMCLP-
F, which outperforms SGLDLP-F around 1.4%in fixed point representation for the CIFAR-100 task. For
the low-precision gradient a ccumulators method, SGHMCLP-L and VC SGHMCLP-L achieve comparable
or better ECE with SGLDLP-L and VC SGLDLP-L and dramatically outperform low-precision SGD.
12Published in Transactions on Machine Learning Research (04/2024)
Table 2: Test errors (%) of full-precision gradient ac-
cumulators on CIFAR with ResNet-18. SGHMCLP-F
achieves comparable results with SGLDLP-F.
CIFAR-10 CIFAR-100
32-bit Float
SGD 4.73 ±0.1022.34±0.22
SGLD 4.52±0.0722.40±0.04
SGHMC 4.78 ±0.0822.37±0.04
8-bit Fixed Point
SGD 5.19 ±0.0923.71±0.18
SGLD 5.07±0.0423.36±0.10
SGHMC 5.08 ±0.0823.54±0.10
8-bit Block Floating Point
SGD 4.75 ±0.2122.86±0.14
SGLD 4.58±0.0722.70±0.22
SGHMC 4.93 ±0.0922.39±0.11Table 3: ECE (%) of full-precision gradient accu-
mulators on CIFAR with ResNet-18. SGHMCLP-F
achieves comparable ECE with SGLDLP-F.
CIFAR-10 CIFAR-100
32-bit Float
SGD 2.50 4.97
SGLD 1.12 3.71
SGHMC 0.72 1.52
8-bit Fixed Point
SGD 2.79 7.11
SGLD 0.86 3.57
SGHMC 1.11 1.92
8-bit Block Floating Point
SGD 2.43 5.97
SGLD 1.01 3.87
SGHMC 1.12 3.65
Table4: Testerrors(%)oflow-precisiongradientaccumu-
lators on CIFAR with ResNet-18. SGHMCLP-L and VC
SGHMCLP-L outperform SGLDLP-L and VC SGLDLP-
L,respectively. SGHMCLP-Lachievescomparableresults
with VC SGLDLP-L.
CIFAR-10 CIFAR-100
32-bit Float
SGD 4.73 ±0.1022.34±0.22
SGLD 4.52±0.0722.40±0.04
SGHMC 4.78 ±0.0822.37±0.04
8-bit Fixed Point
SGD 8.50 ±0.2228.42±0.35
SGLD 7.81 ±0.0727.15±0.35
VC SGLD 7.03 ±0.2326.73±0.12
SGHMC 6.63 ±0.1026.57±0.10
VC SGHMC 6.60 ±0.0626.43±0.19
8-bit Block Floating Point
SGD 5.86 ±0.1826.75±0.11
SGLD 5.75 ±0.0526.11±0.38
VC SGLD 5.51 ±0.0125.14±0.11
SGHMC 5.38 ±0.0625.29±0.03
VC SGHMC 5.15 ±0.0824.45±0.16Table 5: ECE (%) of low-precision gradient ac-
cumulators on CIFAR with ResNet-18. Low-
precision SGHMC are less affected by the quan-
tization error.
CIFAR-10 CIFAR-100
32-bit Float
SGD 2.50 4.97
SGLD 1.12 3.71
SGHMC 0.72 1.52
8-bit Fixed Point
SGD 5.12 12.92
SGLD 1.67 1.11
VC SGLD 0.60 2.89
SGHMC 0.72 2.46
VC SGHMC 0.70 2.44
8-bit Block Floating Point
SGD 4.62 13.93
SGLD 0.67 5.63
VC SGLD 0.60 5.09
SGHMC 0.78 4.94
VC SGHMC 0.67 5.02
5 Conclusion
We provide the first comprehensive investigation for low-precision SGHMC in both strongly log-concave and
non-log-concave target distributions with several variants of low-precision training. In particular, we prove
that for non-log-concave distributions, low-precision SGHMC with full-precision, low-precision, and variance-
corrected gradient accumulators all achieve an acceleration in iterations and have a better convergence upper
bound w.r.t the quantization error compared to low-precision SGLD counterparts. Moreover, we study
the improvement of variance-corrected quantization applied to low-precision SGHMC under different cases.
Under certain conditions, the naïve SGHMCLP-L can replace the VC SGLDLP-L to get comparable results,
13Published in Transactions on Machine Learning Research (04/2024)
saving more computation resources. We conduct empirical experiments on Gaussian, Gaussian mixture
distribution, logistic regression, and Bayesian deep learning tasks to justify our theoretical findings.
References
Dominique Bakry and Michel Émery. Diffusions hypercontractives. In Séminaire de Probabilités XIX
1983/84: Proceedings , pp. 177–206. Springer, 2006.
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural
networks. Advances in neural information processing systems , 31, 2018.
Fabrice Baudoin. Wasserstein contraction properties for hypoelliptic diffusions. arXiv preprint
arXiv:1602.04177 , 2016.
Fabrice Baudoin. Bakry–émery meet villani. Journal of functional analysis , 273(7):2275–2291, 2017.
François Bolley and Cédric Villani. Weighted csiszár-kullback-pinsker inequalities and applications to trans-
portation inequalities. In Annales de la Faculté des sciences de Toulouse: Mathématiques , volume 14, pp.
331–352, 2005.
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In International
conference on machine learning , pp. 1683–1691. PMLR, 2014.
Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi
Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint
arXiv:2302.06675 , 2023.
Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin mcmc:
A non-asymptotic analysis. In Conference on learning theory , pp. 300–323. PMLR, 2018.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural net-
works with binary weights during propagations. Advances in neural information processing systems , 28,
2015.
Matteo Croci, Massimiliano Fasi, Nicholas J Higham, Theo Mary, and Mantas Mikaitis. Stochastic rounding:
implementation, error analysis and applications. Royal Society Open Science , 9(3):211631, 2022.
Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte carlo with
inaccurate gradient. Stochastic Processes and their Applications , 129(12):5278–5311, 2019.
Arnak S. Dalalyan and Lionel Riou-Durand. On sampling from a log-concave density using kinetic Langevin
diffusions. Bernoulli , 26(3):1956 – 1988, 2020. doi: 10.3150/19-BEJ1178. URL https://doi.org/10.
3150/19-BEJ1178 .
Christopher De Sa, Matthew Feldman, Christopher Ré, and Kunle Olukotun. Understanding and optimizing
asynchronous low-precision stochastic gradient descent. In Proceedings of the 44th Annual International
Symposium on Computer Architecture , pp. 561–574, 2017.
Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration
for neural networks: A comprehensive survey. Proceedings of the IEEE , 108(4):485–532, 2020.
Andreas Eberle, Arnaud Guillin, and Raphael Zimmer. Couplings and quantitative contraction rates for
Langevin dynamics. The Annals of Probability , 47(4):1982 – 2010, 2019.
Xuefeng Gao, Mert Gürbüzbalaban, and Lingjiong Zhu. Global convergence of stochastic gradient hamil-
tonian monte carlo for nonconvex stochastic optimization: Nonasymptotic performance bounds and
momentum-based acceleration. Operations Research , 70(5):2931–2947, 2022.
Eduard Gorbunov, Konstantin P Burlachenko, Zhize Li, and Peter Richtárik. Marina: Faster non-convex
distributed learning with compression. In International Conference on Machine Learning , pp. 3788–3798.
PMLR, 2021.
14Published in Transactions on Machine Learning Research (04/2024)
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
International conference on machine learning , pp. 1321–1330. PMLR, 2017.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited
numerical precision. In International conference on machine learning , pp. 1737–1746. PMLR, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Olav Kallenberg and Olav Kallenberg. Foundations of modern probability , volume 2. Springer, 1997.
Avetik Karagulyan and Peter Richtárik. Elf: Federated langevin algorithms with primal, dual and bidirec-
tional compression. arXiv preprint arXiv:2303.04622 , 2023.
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized
nets: A deeper understanding. Advances in neural information processing systems , 2017.
Zheng Li and Christopher M De Sa. Dimension-free bounds for low-precision training. Advances in Neural
Information Processing Systems , 32, 2019.
Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu. Repq-vit: Scale reparameterization for post-training
quantization of vision transformers. In Proceedings of the IEEE/CVF International Conference on Com-
puter Vision , pp. 17227–17236, 2023.
Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization for
deep neural network acceleration: A survey. Neurocomputing , 461:370–403, 2021.
Po-Chen Lin, Mu-Kai Sun, Chuking Kung, and Tzi-Dar Chiueh. Floatsd: A new weight representation and
associated update method for efficient convolutional neural network training. IEEE Journal on Emerging
and Selected Topics in Circuits and Systems , 9(2):267–279, 2019.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with momentum.
Advances in Neural Information Processing Systems , 33:18261–18271, 2020.
YijiangLiu, HuanruiYang, ZhenDong, KurtKeutzer, LiDu, andShanghangZhang. Noisyquant: Noisybias-
enhanced post-training activation quantization for vision transformers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 20321–20330, 2023.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv
preprint arXiv:1710.03740 , 2017.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding
by generative pre-training. 2018.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient
langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory , pp. 1674–1703. PMLR,
2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
Zhourui Song, Zhenyu Liu, and Dongsheng Wang. Computation error analysis of block floating point
arithmetic oriented convolution neural network accelerator design. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 32, 2018.
Lukang Sun, Adil Salim, and Peter Richtárik. Federated learning with a sampling algorithm under isoperime-
try.arXiv preprint arXiv:2206.00920 , 2022.
15Published in Transactions on Machine Learning Research (04/2024)
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Viji
Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (hfp8)
training and inference for deep neural networks. Advances in neural information processing systems , 32,
2019.
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural networks:
A tutorial and survey. Proceedings of the IEEE , 105(12):2295–2329, 2017.
Alexander Tyurin and Peter Richtárik. Dasha: Distributed nonconvex optimization with communication
compression, optimal oracle complexity, and no client synchronization. arXiv preprint arXiv:2202.01268 ,
2022.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
Maxime Vono, Vincent Plassier, Alain Durmus, Aymeric Dieuleveut, and Eric Moulines. Qlsd: Quantised
langevin stochastic dynamics for bayesian federated learning. In International Conference on Artificial
Intelligence and Statistics , pp. 6459–6500. PMLR, 2022.
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep
neural networks with 8-bit floating point numbers. Advances in neural information processing systems ,
31, 2018.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings
of the 28th international conference on machine learning (ICML-11) , pp. 681–688, 2011.
Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. Stable
and low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 , 2023.
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural
networks. arXiv preprint arXiv:1802.04680 , 2018.
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate
and efficient post-training quantization for large language models. In International Conference on Machine
Learning , pp. 38087–38099. PMLR, 2023.
Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng
Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. arXiv
preprint arXiv:2309.14717 , 2023.
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, and Chris De Sa.
Swalp: Stochastic weight averaging in low precision training. In International Conference on Machine
Learning , pp. 7015–7024. PMLR, 2019.
Ruqi Zhang, Andrew Gordon Wilson, and Christopher De Sa. Low-precision stochastic gradient langevin
dynamics. In International Conference on Machine Learning , pp. 26624–26644. PMLR, 2022.
Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. Qpytorch: A low-precision arithmetic
simulation framework. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive
Computing-NeurIPS Edition (EMC2-NIPS) , pp. 10–13. IEEE, 2019.
Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,
Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv
preprint arXiv:2310.19102 , 2023.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 ,
2016.
16Published in Transactions on Machine Learning Research (04/2024)
Raphael Zimmer. Explicit contraction rates for a class of degenerate and infinite-dimensional diffusions.
Stochastics and Partial Differential Equations: Analysis and Computations , 5:368–399, 2017.
Difan Zou, Pan Xu, and Quanquan Gu. Stochastic gradient hamiltonian monte carlo methods with recursive
variance reduction. Advances in Neural Information Processing Systems , 32, 2019.
17Published in Transactions on Machine Learning Research (04/2024)
A Additional Results for Low-precision Stochastic Gradients Hamiltonian Monte
Carlo
In this section, we mainly summarize the theoretical results of Low-precision SGHMC under strongly log-
concave target distribution. The underdamped Langevin dynamics can be defined as:
dvt=−γvtdt−u∇U(xt)dt+/radicalbig
2γudBt
dxt=vtdt,(10)
where (xt,vt)∈R2d, andu,γdenote the hyperparameters of inverse mass and friction respectively. We
introduce the the strongly-log-concave assumption as:
Assumption 4 (Strongly Log-Convex) .The energy function Uism-strongly log-convex, i.e., there exists a
positive constant msuch that,
U(y)≥U(x) +⟨∇U(x),y−x⟩+m1
2∥y−x∥2,for any x,y∈Rd.
Once we introduce the continuous underdamped Langevin dynamics (10), we are ready to find a contraction
rate for (10). According to
Theorem 4. Suppose Assumptions 1, 3, and 4 hold and the minimum satisfies ∥x∗∥2<D2. Furthermore,
letp∗denote the target distribution of xandv. Given any sufficiently small ϵ, if we set the step size to be
η= min/braceleftigg
ϵκ−1
1/radicalbig
479232/5(d/m 1+D2),ϵ2
1440κ1u2/bracketleftbig
(M2+ 1)∆2d
4+σ2/bracketrightbig/bracerightigg
,
then afterKsteps starting with initial points x0=v0= 0, the output (xK,vK)of the SGHMCLP-F in (5)
satisfies
W2(p(xK,vK),p∗)≤˜O(ϵ+ ∆),
for someKsatisfying
K≤κ1
ηlog
36/parenleftig
d
m1+D2/parenrightig
ϵ
=˜O/parenleftbig
ϵ−2log/parenleftbig
ϵ−1/parenrightbig
∆2/parenrightbig
.
Theorem 1 in Zhang et al. (2022) implies that for strongly log-concave target distribution, the low-precision
SGLD with full-precision gradient accumulators can achieve ϵaccuracy within ˜O/parenleftbig
ϵ−2log/parenleftbig
ϵ−1/parenrightbig
∆2/parenrightbig
itera-
tions. Thus, the theorem of SGHMCLP-F does not showcase any advantage over SGLDLP-F. This is not
surprising, since the quantization applied to the gradients in the full-precision gradient accumulator algo-
rithm is equivalent to adding extra noise to the stochastic gradients. As theoretically shown by Cheng et al.
(2018) for strongly-log-concave target distribution, SGHMC doesn’t exhibit any advantage over the over-
damped Langevin algorithm when stochastic gradients are used. Now we present the convergence analysis
of SGHMCLP-L under strongly log-concave target distributions.
Theorem 5. Let Assumption 1, 4 and 3 hold and the minimum satisfies ∥x∗∥2<D2. Furthermore, let p∗
denote the target distribution of vandx. Given any sufficiently small ϵ, if we set the step size ηto be
η= min

ϵκ−1
1/radicalbigg
663552/5/parenleftig
d
m1+D2/parenrightig,ϵ2
2880κ1u/parenleftbig∆2d
4+σ2/parenrightbig

,
then afterKsteps starting with initial points x0=v0= 0, the output (xK,vK)of the SGHMCLP-L in (6)
satisfies
W2(p(xK,vK),p∗) =˜O/parenleftbigg
ϵ+∆
ϵ/parenrightbigg
, (11)
18Published in Transactions on Machine Learning Research (04/2024)
for someKsuch that
K≤κ1
ηlog
36/parenleftig
d
m1+D2/parenrightig
ϵ
=˜O/parenleftbig
ϵ−2log/parenleftbig
ϵ−1/parenrightbig
∆2/parenrightbig
.
Comparing Theorem 4 and Theorem 5, we show that for strongly log-concave target distribution the naïve
SGHMCLP-LhasworseconvergenceupperboundthanSGHMCLP-F.SinceSGHMCLP-Ldirectlyquantizes
the weights after each update, a small stepsize update is often quantized to zero, resulting in the sample
distribution converging to a Dirac distribution at the initial point. In such cases, ensuring convergence
becomes challenging. Compared with Theorem 2 in Zhang et al. (2022), We cannot show the advantages
of low-precision SGHMC over SGLD. Next, we present the theorem for VC SGHMCLP-L under strongly
log-concave target distribution.
Theorem 6. Let Assumption 1, 4 and 3 hold and the minimum satisfies ∥x∗∥2<D2. Furthermore, let p∗
denote the target distribution of xandv. Given any sufficiently small ϵ, if we set the stepsize to be
η= min

ϵ
479232/5/parenleftig
d
m1+D2/parenrightig
κ1,ϵ2
90u2∆2dκ1+ 360u2σ2κ1


afterKsteps starting from the initial point x0=v0= 0the output (xK,vK)of the VC SGHMCLP-L in
(9)satisfies
W2(p(xK,vK),p∗) =˜O/parenleftig
ϵ+√
∆/parenrightig
, (12)
for someKsatisfied
K≤κ1
ηlog
36/parenleftig
d
m1+D2/parenrightig
ϵ
=˜O/parenleftbig
ϵ−2log/parenleftbig
ϵ−1/parenrightbig
∆2/parenrightbig
.
Theorem 6 shows that the variance corrected quantization function can solve the overdispersion problem
we observe for the naïve SGHMCLP-L algorithm for strongly log-concave distribution. The W2distance
between the sample distribution and target distribution can be arbitrarily close to ˜O(√
∆). Compared
to the Theorem 3 in Zhang et al. (2022), the VC SGHMCLP-L doesn’t showcase its advantage over VC
SGLDLP-L for strongly log-concave distribution.
B Stochastic Gradient Langevin Dynamics Result
In order to sample from the target distribution, Langevin dynamics-based samplers, such as overdamped
Langevin MCMC and underdamped Langevin MCMC methods, are widely used when the evaluation of
U(x)is expansive due to a large sample size. The continuous-time overdamped Langevin MCMC can be
represented by the following stochastic differential equation(SDE):
dxt=−∇U(xt) +√
2dBt, (13)
where Btrepresents the standard Brownian motion in Rd. Under some mild conditions, it can be proved
that the invariant distribution of (13) converges the target distribution exp(−U(x)). To reduce the com-
putational cost of evaluating ∇U(x), Welling & Teh (2011) proposed the Stochastic Gradient Langevin
Dynamics (SGLD) and updates the weights using stochastic gradients:
xk+1=xk−η∇˜U(xk) +/radicalbig
2ηξk+1, (14)
whereηis the stepsize, the ξk+1is a standard Gaussian noise, and ∇˜U(xk)is an unbiased estimation of
∇U(xk). Despite the additional noise induced by stochastic gradient estimations, SGLD can still converge
to the target distribution.
In this section, we present the theoretical result for SGLD. We start from the SGLDLP-F’s result.
19Published in Transactions on Machine Learning Research (04/2024)
Theorem 7. Suppose Assumptions 1, 2 and 3 hold. Let p∗denote the target distribution of x,/tildewideAhave the
same definition in Theorem 1. After Ksteps starting with initial point x0= 0, if we set the stepsize to be
η=˜O/parenleftbigg/parenleftig
ϵ
log(1/ϵ)/parenrightig4/parenrightbigg
. The output xKof SGLDLP-F in (1)satisfies
W2(p(xK),p∗)≤˜O/parenleftbigg
ϵ+/tildewideAlog/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
, (15)
for someKsatisfied
K=˜O/parenleftbigg1
ϵ4λ∗log5/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
Theorem 7 shows that the low-precision SGLD with full-precision gradient accumulators can converge to the
non-log-concave target distribution if provided a small gradient variance and quantization error. Next, we
present the SGLDLP-L’s result.
Theorem 8. Let Assumptions 1, 2 and 3 hold. Let p∗denote the target distribution of x. If we set the
step size to be η=˜O/parenleftbigg/parenleftig
ϵ
log(1/ϵ)/parenrightig4/parenrightbigg
, afterKsteps starting at the initial point x0= 0the output xKof the
SGLDLP-L in (2)satisfies
W2(p(xK),p∗) =˜O/parenleftigg
ϵ+/radicalbig
max{σ2,σ}log/parenleftbigg1
ϵ/parenrightbigg
+log5/parenleftbig1
ϵ/parenrightbig
ϵ4√
∆/parenrightigg
, (16)
for someKsatisfied
K=˜O/parenleftbigg1
ϵ4λ∗log5/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
The VC SGLDLP-L can be done as:
xk+1=Qvc/parenleftbig
xk−ηQG(∇˜U(xk)),2η,∆/parenrightbig
(17)
We present the convergence analysis of VC SGLDLP-L in the following theorem:
Theorem 9. Let Assumption 1, 2 and 3 hold. Let p∗denote the target distribution of x. If we set the
stepsize to be η=˜O/parenleftbigg
ϵ4
log4(1
ϵ)/parenrightbigg
, afterKsteps from the initial point x0= 0the output xKof VC SGLDLP-L
in(17)satisfies
W2(p(xK),p∗) =˜O/parenleftigg
ϵ+/radicaligg
max{σ2,σ}log/parenleftbigg1
ϵ/parenrightbigg
+log3/parenleftbig1
ϵ/parenrightbig
ϵ2√
∆/parenrightigg
, (18)
for someKsatisfied
K=˜O/parenleftbigg1
ϵ4λ∗log5/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
C Uniform Bound of Contraction Rate
According to reference (Raginsky et al., 2017), under Assumptions 1 and 2, one can choose λ∗to be the
uniform lower bound of the contraction rate, i.e.,
λ∗:= inf/braceleftigg/integraltext
Rd∥∇g∥2dp∗
/integraltext
Rdg2dp∗:g∈C1/parenleftbig
Rd/parenrightbig
∩L2(p∗),g= 0,/integraldisplay
Rdgdp∗= 0/bracerightigg
,
which satisfied
1
λ∗≤2
m2(d+b)+4C(d+b)
m2exp/parenleftbigg2
m2(M+B)(b+d) + (A+B)/parenrightbigg
,
20Published in Transactions on Machine Learning Research (04/2024)
whereA,Bdenote bounds such that |U(0)|≤A,∥∇/tildewideU(0)∥≤B. In other words, asymptotically w.r.t. the
dimension, we have λ∗−1= exp(O(d)).
Similarly, (Zou et al., 2019) derives a contraction rate as
µ∗=2d
768γeΛmin/braceleftig
λMueΛ,Λ1/2Mu,γ Λ1/2/bracerightig
,
where the constants are defined as:
λ=2m2
4M+u−1γ2
Λ =12(1 + 2α+ 2α2)(d+A)Mu
5γ2λ(1−2λ)
A=2m2(U(x∗) +M∥x∗∥2)
4M+u−1γ2+b
2.
Note that the above rate also satisfies µ∗−1= exp(O(d)).
D Technical Detail
In this section, we disclose more details of empirical experiments. We can define the stochastic quantization
functionQsas:
Qs(θ) =/braceleftigg
∆/floorleftbigθ
∆/floorrightbig
,w.p./ceilingleftbigθ
∆/ceilingrightbig
−θ
∆
∆/ceilingleftbigθ
∆/ceilingrightbig
,w.p. 1−/parenleftbig/ceilingleftbigθ
∆/ceilingrightbig
−θ
∆/parenrightbig
.(19)
In practice, to implement stochastic rounding based on the rule (19), the computer still needs a full-precision
Unif(0,1)random number generator (note that we ignore the discretization gap between full precision values
and real values), then compares this random number with the residual and rounds up if it is smaller otherwise
rounds down. This full precision random number generator can be shared for all rounding steps, hence won’t
affect memory usage too much. For more details about the implementation of stochastic rounding, please
refer to (Gupta et al., 2015; Croci et al., 2022).
Now, we show the details of the experiment setup. For the standard normal distribution experiment, we use
8-bit fixed point low-precision representation with 4 of them representing fractional parts. Moreover, we set
the step size η= 0.09, inverse mass u= 2, and friction γ= 3. Similarly, for Gaussian mixture distribution,
we also use 8-bit fixed point low-precision representation with 4 of them representing fractional parts for
both low-precision SGHMC and SGLD, but we set the step size η= 0.1, inverse mass u= 1, and friction
γ= 3.
Next, for both logistic, MLP models, low-precision SGLD and SGHMC in MNIST task, we set N/parenleftbig
0,10−2/parenrightbig
as the prior distribution, and step size η= 0.01. Moreover, for SGHMC, we set the inverse mass u= 2, and
frictionγ= 2.
Then we introduce the training detail of low-precision SGHMC for CIFAR-10 & CIFAR-100. We adopt the
quantization framework from previous research Wu et al. (2018); Wang et al. (2018); Yang et al. (2019) to
applyquantizationtoweights,activations,backpropagationerrors,andgradients. PleaseseetheAlgorithm1.
We useN/parenleftbig
0,10−4/parenrightbig
as the prior distribution. Furthermore, we set the set the step size η= 0.1, and
u= 2,γ= 2for low-precision SGHMC.
Algorithm 1 is a practical version of the three different types of low-precision SGHMC updates proposed
in our main text, i.e. equations (5), (6), and (9). Additional components in the algorithm include (i) How
to compute the stochastic gradient via forward/back-propagation (steps colored by red in the Algorithm
box). Due to the low-precision nature of the algorithm, we also quantize all intermediate results along the
propagation process by proper quantizers QAandQE. (ii) Additional optional scale/re-scale step (colored
21Published in Transactions on Machine Learning Research (04/2024)
by blue in the Algorithm box). The reason for adding this step is that: in practice, we found that the
momentum term vktends to be close to 0. When vkis represented in the low-precision fixed-point format,
the information carried by vkis lost since the low-precision fixed-point is loose around 0, and only 2 or 3
bits are used representing the momentum (i.e., the other bits are wasted). With this observation, we store
a scaled-up momentum to fully utilize all bits, thus the information carried will be kept in an optimal way.
Algorithm 2 is proposed by (Zhang et al., 2022). The rationale behind Algorithm 2 is that: if we directly
quantize the SGLD update result, i.e. the mean shift plus a Gaussian noise variable, it essentially introduces
an additional quantization noise to the sample, leading to a larger sampling variance. Instead of quantizing
the mean shift plus Gaussian noise, we can first quantize the mean shift, then plus a low-precision discrete
random variable. In this way, we guarantee the sampler yields low-precision values and have the freedom to
design the variance of the low-precision discrete random variable, such that overall sampling variance (i.e.,
variance due to stochastic round and low-precision discrete random variable) matches the idea sampling
variance of full-precision Langevin update.
When implementing low-precision SGHMC on classification tasks in the MNIST, CIFAR-10 and CIFAR-100
dataset, we observed that the momentum term vtend to gather in a small range around zero in which
case the low-precision representations of vend up in using few bits, thus the momentum information is
seriously lost and cause in performance degradation. In order to tackle this problem and fully utilize all the
low-precision representations, we borrowed the idea of rescaling from the bit-centering trick and adopted the
low-precision SGHMC method. The detailed algorithm is listed in Algorithms 1.
Now, we give a brief introduction of the variance-corrected quantization function Qvc. Instead of adding real
value Gaussian noise and quantizing the weights, we can design a categorical sampler that samples from the
space{∆,−∆,0}with the desired expectation µand variance vas
Cat(µ,v) =

∆, w.p.v+µ2+µ∆
2∆2
−∆, w.p.v+µ2−µ∆
2∆2
0,otherwise.(20)
Based on the sampler (20), one can design the variance-corrected quantization function Qvcin the Algo-
rithm 2.
E Proof of Main Theorems
E.1 Proof of Theorem 1
In this section we analyze the Wasserstein distance between the sample (xk,vK)in (5) and the target
distribution, given the target distribution satisfies the assumption 1 and 2. We follow the proof in Raginsky
et al. (2017). To analyze the Wasserstein distance, we first calculate the distance between solutions of
low-precision discrete underdamped Langevin dynamics and solutions of the ideal continuous underdamped
Langevin dynamics, also the distance between solutions of the ideal continuous underdamped Langevin
dynamics and the target distribution.
Again letpk= (xk,vk)denote the low-precision sample from (5) at k-th iteration, let ˆpt= (ˆxt,ˆvt)denote the
sample from the ideal continuous underdamped Langevin dynamics in (41) at time t. Then the Wasserstein
distance between the pkand the target distribution p∗can be bounded as:
W2(pK,p∗)≤W 2(pK,ˆpKη) +W2(ˆpKη,p∗).
Then we bound the first term W2(pK,ˆpKη)by invoking the weighted CKP inequality Bolley & Villani (2005),
W2
2(pK,ˆpKη)≤Λ/parenleftbigg/radicalig
DKL(pK||ˆpKη) +4/radicalig
DKL(pK||ˆpKη)/parenrightbigg
,
22Published in Transactions on Machine Learning Research (04/2024)
Algorithm 1 Low-Precision Training for SGHMC.
given:Llayers DNN{f1...,fL}. Weight, gradient, activation, and error quantizers QW,QG,QA,QE.
Variance-corrected quantization Qvc, and quantization gap of weights ∆. Data batch sequence
{(θk,hk)}K
k=1, where the θkis the input, and hkis the target. The loss function L(a,h)measures the
loss between the prediction aand target h. And xfp
kdenotes the full-precision buffer of the weight. Let
Varhmc
v=u(1−e−2γη)andVarhmc
x=uγ−2(2γη+ 4e−γη−e−2γη−3)andSv= 1. {Initialize the scaling
parameter}
fork= 1 :Kdo
1. Forward Propagation:
a(0)
k=θk
a(l)
k=QA(fl(a(l−1)
k,xl
k)),∀l∈[1,L]
2. Backward Propagation:
e(L)=∇a(L)
kL(a(L)
k,hk)
e(l−1)=QE/parenleftbigg
∂fl(a(l)
k)
∂a(l−1)
ke(l)
k/parenrightbigg
,∀l∈[1,L]
g(l)
k=QG/parenleftbigg
∂fl
∂θ(l)
ke(l)
k/parenrightbigg
,∀l∈[1,L]
3. SGHMC Update:
full-precision gradient accumulators:
v(l)
k+1←v(l)
k−uγ−1(1−e−γη)g(l)
k+ξv
k,∀l∈[1,L],
x(l),fp
k+1←x(l),fp
k+γ−1(1−e−γη)v(l)
k+uγ−2(γη+e−γη−1)g(l)
k+ξx
k,x(l)
k+1←QW/parenleftig
x(l),fp
k+1/parenrightig
,∀l∈
[1,L]
low-precision gradient accumulators:
v(l)
k=v(l)
k∗S(l)
v,∀l∈[1,L]{Restore the velocity before update}
µ(v(l)
k+1)←v(l)
ke−γη−uγ−1(1−e−γη)g(l)
k,∀l∈[1,L]
S(l)
v=/vextenddouble/vextenddoubleµ(v(l)
k+1)/vextenddouble/vextenddouble
∞¯U,∀l∈[1,L]{Update the Scaling}
v(l)
k+1←QW(/parenleftig
µ(v(l)
k+1) +ξv
k/parenrightig
/S(l)
v),∀l∈[1,L]
x(l)
k+1←QW/parenleftig
x(l)
k+γ−1(1−e−γη)v(l)
k+uγ−2(γη+e−γη−1)g(l)
k+ξx
k/parenrightig
,∀l∈[1,L]
Variance-corrected low-precision gradient accumulators:
v(l)
k=v(l)
k∗S(l)
v,∀l∈[1,L]{Restore the velocity before update}
µ(v(l)
k+1) =v(l)
ke−γη−uγ−1(1−e−γη)g(l)
k,∀l∈[1,L]
µ(x(l)
k+1) =x(l)
k+γ−1(1−e−γη)v(l)
k+uγ−2(γη+e−γη−1)g(l)
k,∀l∈[1,L]
S(l)
v=/vextenddouble/vextenddoubleµ(v(l)
k+1)/vextenddouble/vextenddouble
∞¯U,∀l∈[1,L]{Update the Scaling}
v(l)
k+1←Qvc/parenleftig
µ(v(l)
k+1)/S(l)
v,Varhmc
v/(S(l)
v)2,∆/parenrightig
,∀l∈[1,L]
x(l)
k+1←Qvc/parenleftig
µ(x(l)
k+1),Varhmc
x,∆/parenrightig
,∀l∈[1,L]
end for
output: samples{(v(l)
k,x(l)
k)}
where Λ = 2 inf θ>0/radicalig
1/θ/parenleftbig
3/2 +logEˆpKη[exp(θ(∥ˆxKη∥2+∥ˆvKη∥2))]/parenrightbig
. We define a Lyapunov function for
every (x,v)∈Rd×Rd
E(x,v) =∥x∥2+∥x+ 2v/γ∥2+ 8u(U(x)−U(x∗))/γ2.
Note that∥a∥2+∥b∥2≥∥a−b∥2/2andU(x)≥U(x∗), we can have:
E(x,v)≥∥x∥2+∥x+ 2v/γ∥2≥max{∥x∥2,2∥v/γ∥2}.
23Published in Transactions on Machine Learning Research (04/2024)
Algorithm 2 Variance-Corrected Quantization Function Qvc. (Zhang et al., 2022)
input: (µ,v,∆) {Qvcreturns a variable with mean µand variance v}
v0←∆2/4{∆2/4is the largest possible variance that stochastic rounding can cause}
ifv>v 0then{add a small Gaussian noise and sample from the discrete grid to make up the remaining
variance}
x←µ+√v−v0ξ, whereξ∼N(0,Id)
r←x−Qd(x)
for allido
samplecifrom Cat (|ri|,v0)as in (20)
end for
θ←Qd(x) +sign(r)⊙c
else{sample from the discrete grid to achieve the target variance}
r←µ−Qs(µ)
for allido
vs←/parenleftig
1−|ri|
∆/parenrightig
·r2
i+|ri|
∆·(−ri+sign(ri)∆)2
ifv>vsthen
samplecifrom Cat (0,v−vs)as in (20)
θi←Qs(µ)i+ci
else
θi←Qs(µ)i
end if
end for
end if
clipθif outside representable range
returnθ
Given assumptions 4 and 2 hold and apply Lemma B.4 in Zou et al. (2019), we can get
Λ≤2 inf
0<θ≤min{γ
128u,m2
32}/radicaligg
1
θ/parenleftbigg3
2+ 2θE(X0,V0) +32Mθu (4d+ 2b+m2∥x∗∥2)
γ2m2/parenrightbigg
≤2/radicaligg
2E(X0,V0) +32Mθu (4d+ 2b+m2∥x∗∥2) + 16(12um2+ 3γ2)
γ2m2:=¯Λ.
It remains to bound the divergence between the distribution pKand ˆpKη. We first define a continuous
interpolation of the low-precision sample (xk,vk),
dvt=−γvtdt−uGtdt+/radicalbig
2γudBt (21)
dxt=vtdt, (22)
whereGt=K/summationtext
k=0˜g(xk)1t∈[kη,(k+1)η). Integrating this equation from time 0tot, we can get
vt=v0−/integraldisplayt
0γvsds−/integraldisplayt
0uGsdt+/integraldisplayt
0/radicalbig
2γudBs
xt=x0+/integraldisplayt
0vsds.
Notice that when t=kη, the solution of (21) has the same distribution with the low-precision sample
(xk,vk). Now by Girsanov formula, we can compute the Radon-Nikodym derivative of ˆpKηwith respect to
24Published in Transactions on Machine Learning Research (04/2024)
pKas follows:
dˆpKη
dpK=exp/braceleftigg/radicalbiggγu
2/integraldisplayt
0(∇U(xs)−Gs)dBs−γu
4/integraldisplayT
0∥∇U(xs)−Gs∥ds/bracerightigg
.
It follows that
DKL(pK||ˆpKη) =EpK/bracketleftbigg
log/parenleftbiggdˆpKη
dpK/parenrightbigg/bracketrightbigg
(23)
=γu
4E/integraldisplayKη
0∥∇U(xs)−Gs∥2ds
=γu
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xs)−Gs∥2/bracketrightig
ds
=γu
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xs)−˜g(xk)∥2/bracketrightig
ds.
Furthermore, in the k-th interval, we have
E/bracketleftig
∥∇U(xs)−˜g(xk)∥2/bracketrightig
≤2E/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
+ 2E/bracketleftig
∥∇U(xk)−˜g(xk)∥2/bracketrightig
. (24)
We now bound the first term in the RHS of the (24). By the smooth Assumption1, we have
E/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
≤M2E/bracketleftig
∥xs−xk∥2/bracketrightig
.
Notice that
xs=xk+/integraldisplays
kηvrdr
=xk+/integraldisplays
kη/parenleftbigg
vkηe−γ(r−kη)−u/parenleftbigg/integraldisplayr
kηe−γ(r−z)˜g(xk)dz/parenrightbigg
+/radicalbig
2γu/integraldisplayr
kηe−γ(r−z)dBz/parenrightbigg
dr.
This further implies that:
∥xs−xk∥2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplays
kη/parenleftbigg
vkηe−γ(r−kη)−u/parenleftbigg/integraldisplayr
kηe−γ(r−z)˜g(xk)dz/parenrightbigg
+/radicalbig
2γu/integraldisplayr
kηe−γ(r−z)dBz/parenrightbigg
dr/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤3/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplays
kηvkηeγ(kη−r)dr/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 3/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplays
kη/integraldisplayr
kηu˜g(xk)eγ(z−r)dzdr/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 6ru/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplays
kη/integraldisplays
0e−γ(r−z)dBzdr/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤3η2∥vk∥2+ 3u2η4∥˜g(xk)∥2+ 3/bracketleftbiggu
γ2/parenleftig
2γ(s−kη) + 4e−γ(s−kη)−e−2γ(s−kη)−3/parenrightig
d/bracketrightbigg
≤3η2/parenleftig
∥vk∥2+u2η2∥˜g(xk)∥2+ 2du/parenrightig
, (25)
where we use inequality 1−x≤e−x≤1−x+x2/2forx>0andkη≤s≤(k+1)ηto get the last inequality.
Given this analysis we can bound the first term in the RHS of (24)
E/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
≤3M2η2/parenleftig
E∥vk∥2+u2η2E∥˜g(xk)∥2+ 2du/parenrightig
.
By lemma 12, the second term in the RHS of (24) can be bounded as:
E/bracketleftig
∥∇U(xk)−˜g(xk)∥2/bracketrightig
≤(M2+ 1)∆2d
4+σ2.
We need to introduce a lemma to bound the sup
k∥xk∥2,sup
k∥vk∥2andsup
k∥˜g(xk)∥2.
25Published in Transactions on Machine Learning Research (04/2024)
Lemma 10. Under Assumptions 1 and 2, if we set the step size statisfied the following condition:
η≤min/braceleftbiggγ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d,
1
8γ,γm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ/bracerightbigg
,
then for all k≥0theE/bracketleftig
∥xk∥2/bracketrightig
,E/bracketleftig
∥vk∥2/bracketrightig
andE/bracketleftig
∥˜g(xk)∥2/bracketrightig
can be bounded as
E/bracketleftig
∥xk∥2/bracketrightig
≤E+C0/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
E/bracketleftig
∥vk∥2/bracketrightig
≤γ2E/2 +γ2C0/2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
E/bracketleftig
∥˜g(xk)∥2/bracketrightig
≤2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+ 4M2E+ 4G2
whereEandC0are defined as:
E=E[E(x0,v0)] +24(21u+γ)uM
m2γ3G2+96(d+b)uM
m2γ2, G =∥∇U(0)∥
C0=96u/parenleftbig
γ2+ 2u/parenrightbig
m2γ4.
The proof of Lemma 10 can be found in Appendix F.3. We now ready to bound E/bracketleftig
∥∇U(xs−˜g(xk))∥2/bracketrightig
as:
E/bracketleftig
∥∇U(xs)−˜g(xk)∥2/bracketrightig
≤2E/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
+ 2E/bracketleftig
∥∇U(xk)−˜g(xk)∥2/bracketrightig
≤6M2η2/parenleftig
E∥vk∥2+u2η2E∥˜g(xk)∥2+ 2du/parenrightig
+ 2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
≤6M2η2/parenleftbigg
(γ2/2 + 4M2u2η2)E+ (γ2C0/2 + 2u2η2)/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+ 4u2η2G2+ 2du/parenrightbigg
+ 2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
≤6M2η2/bracketleftbig
(γ2/2 + 4M2u2η2)E+ 4u2η2G2+ 2du/bracketrightbig
+/parenleftbig
6M2η2(γ2C0/2 + 2u2η2) + 2/parenrightbig/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
.
Thus the divergence can be bounded as:
DKL(pK||ˆpKη)≤3γu
2M2Kη3/bracketleftbig
(γ2/2 + 4M2u2η2)E+ 4u2η2G2+ 2du/bracketrightbig
+γu
4Kη/parenleftbig
6M2η2(γ2C0/2 + 2u2η2) + 2/parenrightbig/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
.
By the weighted CKP inequality and given Kη≥1,
W2(pK,ˆpKη)≤Λ/parenleftbigg/radicalig
DKL(pK||ˆpKη) +4/radicalig
DKL(pK||ˆpKη)/parenrightbigg
≤Λ/parenleftig
/tildewiderC0√η+/tildewiderC1/tildewideA/parenrightig/radicalbig
Kη,
26Published in Transactions on Machine Learning Research (04/2024)
where the constants /tildewiderC0,/tildewideC1and/tildewideAare defined as:
/tildewiderC0=/radicalbigg
3γu
2M2/bracketleftbig
(γ2/2 + 4M2u2η2)E+ 4u2η2G2+ 2du/bracketrightbig
+4/radicalbigg
3γu
2M2/bracketleftbig
(γ2/2 + 4M2u2η2)E+ 4u2η2G2+ 2du/bracketrightbig
/tildewiderC1=/radicalbiggγu
4(6M2η2(γ2C0/2 + 2u2η2) + 2) +4/radicalbiggγu
4(6M2η2(γ2C0/2 + 2u2η2) + 2)
/tildewideA=max/braceleftigg/radicaligg/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
,4/radicaligg/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg/bracerightigg
.
Finally by the Lemma A.2 in Zou et al. (2019), we can have
W2(ˆpKη,p∗)≤Γ0e−µ∗Kη,
whereµ∗=e−/tildewideO(d)denotes the concentration rate of the underdamped Langevin dynamics and Γ0is a
constant of order O(1/µ∗). Combining this inequality with the previous analysis we can prove:
W2(pK,p∗)≤Λ/parenleftig
/tildewiderC0√η+/tildewiderC1/tildewideA/parenrightig/radicalbig
Kη+ Γ0e−µ∗Kη. (26)
To bound the Wasserstein distance, we need to set
Λ/tildewiderC0/radicalbig
Kη2=ϵ
2and Γ0e−µ∗Kη=ϵ
2. (27)
Solving the equation (27), we can have
Kη=log/parenleftbig2Γ0
ϵ/parenrightbig
µ∗andη=ϵ2
4Λ2/tildewiderC02Kη.
Combining these two we can have
η=ϵ2µ∗
4Λ2/tildewiderC02log/parenleftbig2Γ0
ϵ/parenrightbigandK=4Λ2/tildewiderC02log2/parenleftbig2Γ0
ϵ/parenrightbig
ϵ2(µ∗)2.
Plugging in (26) completes the proof.
E.2 Proof of Theorem 2
In this section, we analyze the convergence of SGHMCLP-L when the target distribution is non-log-concave.
In this proof, unlike in the SGHMCLP-F algorithm where gradients are unbiased, additional noise applied
to the state xcauses deviation from the underdamped Langevin dynamics, leading us to establish an inter-
mediate process to address this noise.
Recall the continuous interpolation of the SGHMCLP-L,
vt=v0−/integraldisplayt
0γvsds−u/integraldisplayt
0Gsds+/radicalbig
2γu/integraldisplayt
0e−γ(t−s)dBs+/integraldisplayt
0αv(s)ds
xt=x0+/integraldisplayt
0vsds+/integraldisplayt
0αx(s)ds,
whereGs=∞/summationtext
k=0QG(∇U(x′
k))1s∈(kη,(k+1)η).And we define an intermediate process by let v′
t=vt+αx(t):
v′
t=v′
0−/integraldisplayt
0γ(v′
s−αx(s))ds−u/integraldisplayt
0Gsds+/radicalbig
2γu/integraldisplayt
0e−γ(t−s)dBs+/integraldisplayt
0/parenleftbigg
αv(s) +1
tαx(t)/parenrightbigg
ds
x′
t=x′
0+/integraldisplayt
0v′
sds. (28)
27Published in Transactions on Machine Learning Research (04/2024)
By integrating the underdamped Langevin dynamic (10), we can have:
vt=v0−/integraldisplayt
0γ(vs−αx(s))ds−u/integraldisplayt
0∇U(xs)ds+/radicalbig
2γu/integraldisplayt
0e−γ(t−s)dBs
xt=x0+/integraldisplayt
0vsds. (29)
Notice that the process x′
thas the same distribution with xt, thus in the following analysis we study the
convergence of the intermediate process p′
k= (x′
kη,v′
kη). By taking the difference of equation (28) with (29)
and the Girsanov formula, we can derive the Radon-Nikodym derivative of ˆPKηw.r.tp′
K:
dˆpKη
dp′
K=exp/braceleftigg/radicalbiggu
2γ/integraldisplayT
0(γαx(s) +αv(s) +1
Tαx(T) +∇U(xs)−Gs)dBs
−u
4γ/integraldisplayT
0∥γαx(s) +αv(s) +1
Tαx(T) +∇U(xs)−Gs∥2ds/bracerightigg
.
Thus the divergence can be bouned as:
DKL(pK||ˆpKη) =EpK/bracketleftbigg
log/parenleftbiggdˆpKη
dpK/parenrightbigg/bracketrightbigg
=u
4γ/integraldisplayT
0E/vextenddouble/vextenddouble/vextenddouble/vextenddoubleγαx(s) +αv(s) +1
Tαx(T) +∇U(xs)−Gs/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
ds
=u
4γTE/bracketleftig
∥αx(T)∥2/bracketrightig
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥γαv(s) +αx(s) +∇U(xs)−Gs∥2/bracketrightig
ds
≤u
4γTη2E/bracketleftig
∥αx
k∥2/bracketrightig
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥γαv(s)∥2/bracketrightig
ds+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥αx(s)∥2/bracketrightig
ds
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xs)−Gs∥2/bracketrightig
ds
≤u
4γTη2E/bracketleftig
∥αx
k∥2/bracketrightig
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥γαv
k/η∥2/bracketrightig
ds+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥αx
k/η∥2/bracketrightig
ds
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xs)−QG(∇U(xk))∥2/bracketrightig
ds
≤u
4γTη2E/bracketleftig
∥αx
k∥2/bracketrightig
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥γαv
k/η∥2/bracketrightig
ds+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥αx
k/η∥2/bracketrightig
ds (30)
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
ds+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xk)−QG(∇U(xk))∥2/bracketrightig
ds.
By assumption 1, we know that:
E/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
≤M2E/bracketleftig
∥xs−xk∥2/bracketrightig
.
From the same analysis in (25), we can derive:
E/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
≤3M2η2/parenleftig
E/bracketleftig
∥v′
k∥2/bracketrightig
+u2η2E/bracketleftig
∥QG(∇U(xk))∥2/bracketrightig
+ 2du/parenrightig
.
Now we need to derive a uniform bound of E/bracketleftig
∥xk∥2/bracketrightig
andE/bracketleftig
∥v′
k∥2/bracketrightig
.
28Published in Transactions on Machine Learning Research (04/2024)
Lemma 11. Let Assumptions 2 and 1 hold. If we set the step size to the following condition
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d,γm2
6 (22u+γ)M2/bracerightigg
,
then for all k>0E/bracketleftig
∥xk∥2/bracketrightig
andE/bracketleftig
∥vk∥2/bracketrightig
can be bouned as follow:
E/bracketleftig
∥xk∥2/bracketrightig
≤E+C∆2d,E/bracketleftig
∥v′
k∥2/bracketrightig
≤γ2E/2 +γ2C∆2d/2,
where constantsEandCare defined as:
E=E[E(x0,v0)] +54/parenleftbig
4u+γ2/parenrightbig
u
m2γ4σ2+12(22u+γ)uM3
m2γ3G2+96 (d+b)uM
m2γ2
C=27/parenleftbig
4u+γ2/parenrightbig
u
2m2γ4.
The proof of Lemma 11 can be found in Appendix F.5. Thus,
E/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
≤3M2η2/parenleftbigg
E/bracketleftig
∥vk∥2/bracketrightig
+u2η2/parenleftbigg∆2d
4+σ2+ 2M2E/bracketleftig
∥xk∥2/bracketrightig
+ 2G2/parenrightbigg
+ 2du/parenrightbigg
≤3M2η2/parenleftbigg
γ2E/2 +γ2C∆2d/2 +u2η2/parenleftbigg∆2d
4+σ2+ 2M2E+ 2M2C∆2d+ 2G2/parenrightbigg
+ 2du/parenrightbigg
≤3M2η2/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+/parenleftbig
γ2+ 2u2M2/parenrightbig
C∆2d+u2σ2+ 2u2G2+ 2du/parenrightbig
.
Now we can go back to the divergence of pKandˆpKη,
DKL(pK||ˆpKη)
≤u
4γTη2E/bracketleftig
∥αx
k∥2/bracketrightig
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥γαv
k/η∥2/bracketrightig
ds+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥αx
k/η∥2/bracketrightig
ds
+u
4γ3M2Kη3/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+/parenleftbig
γ2+ 2u2M2/parenrightbig
C∆2d+u2σ2+ 2u2G2+ 2du/parenrightbig
+u
4γKη/parenleftbigg∆2d
4+σ2/parenrightbigg
≤u
4γ3M2Kη3/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+/parenleftbig
γ2+ 2u2M2/parenrightbig
C∆2d+u2σ2+ 2u2G2+ 2du/parenrightbig
+u
4γKη/parenleftbigg∆2d
4+σ2/parenrightbigg
+u∆2d
16γTη2+uK∆2d
8γη
≤u
4γ3M2Kη3/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+u2σ2+ 2u2G2+ 2du/parenrightbig
+u
4γKησ2
+/parenleftbiggu
4γ3M2Kη3C/parenleftbig
γ2+ 2u2M2/parenrightbig
+uKη
16γ+u
16γTη2+uK
8γη/parenrightbigg
∆2d
=:C0Kη3+C1Kησ2+C2K∆2,
where the constants C0,C1andC2are defined as:
C0=u
4γ3M2/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+u2σ2+ 2u2G2+ 2du/parenrightbig
C1=u
4γ
C2=/parenleftbiggu
4γ3M2η3C/parenleftbig
γ2+ 2u2M2/parenrightbig
+u
16γ+u
16γT2η+u
8γη/parenrightbigg
d.
By the weighted CKP inequality and given Kη≥1,
29Published in Transactions on Machine Learning Research (04/2024)
W2(pK,ˆpKη)≤Λ/parenleftbigg/radicalig
DKL(pK||ˆpKη) +4/radicalig
DKL(pK||ˆpKη)/parenrightbigg
≤/parenleftig
/tildewiderC0√η+/tildewiderC1/tildewideA/parenrightig/radicalbig
Kη+/tildewiderC2√
K∆, (31)
where the constants are defined as:
/tildewiderC0=/parenleftig/radicalbig
C0+4/radicalbig
C0/parenrightig
/tildewiderC1=/parenleftig/radicalbig
C1+4/radicalbig
C1/parenrightig
/tildewiderC2=/parenleftig/radicalbig
C2+4/radicalbig
C2/parenrightig
/tildewideA= max/braceleftbig
σ,√σ/bracerightbig
.
From the same analysis in (26), we can have:
W2(pK,p∗)≤Λ/parenleftig
/tildewiderC0√η+/tildewiderC1/tildewideA/parenrightig/radicalbig
Kη+/tildewiderC2/radicalbig
Kη+ Γ0e−µ∗Kη. (32)
In order to bound the Wasserstein distance, we need to set
Λ/tildewiderC0/radicalbig
Kη2=ϵ
2and Γ0e−µ∗Kη=ϵ
2. (33)
Solving the equation (33), we can have
Kη=log/parenleftbig2Γ0
ϵ/parenrightbig
µ∗andη=ϵ2
4Λ2/tildewiderC02Kη.
Combining these two we can have
η=ϵ2µ∗
4Λ2/tildewiderC02log/parenleftbig2Γ0
ϵ/parenrightbigandK=4Λ2/tildewiderC02log2/parenleftbig2Γ0
ϵ/parenrightbig
ϵ2(µ∗)2.
Plugging in (32) completes the proof.
E.3 Proof of Theorem 3
In this section, we analyze the convergence of VC SGHMCLP-L when the target distribution is non-log-
concave. Similarly, the gradients are unbiased, additional noise applied to the state xcauses deviation
from the underdamped Langevin dynamics. This proof is similar to the proof of Theorem 2, however, the
variance-corrected quantization function gives us a bound for the difference between the quantized value and
the full-precision value. This bound can scale with the learning rate. This fact leads to the advantage of
variance-corrected quantization over naive stochastic rounding.
Similarily, from the analysis in (57), we know that
E/bracketleftig
∥αv
k∥2/bracketrightig
≤γηA, (34)
whereA= max/braceleftig
∆√
d(A′+G),4ud/bracerightig
. By the analysis in (55), we know that if Varhmc
x≥∆2
4, we can have
E/bracketleftig
∥αx
k∥2/bracketrightig
≤4udη2(35)
by (58), if Varhmc
x<∆2
4,
E/bracketleftig
∥αx
k∥2/bracketrightig
≤ηB, (36)
30Published in Transactions on Machine Learning Research (04/2024)
whereB= max/braceleftig
2∆√
dA′+uη√
dG,4udη/bracerightig
. Thus, we can define the following:
E/bracketleftig
∥αx
k∥2/bracketrightig
=ηB, (37)
whereBis defined as:
B=/braceleftigg
4udη,ifVarhmc
x≥∆2
4
B,else.
Combining the bound of E/bracketleftig
∥αx
k∥2/bracketrightig
,E/bracketleftig
∥αv
k∥2/bracketrightig
with (30), we can show,
DKL(pK||ˆpKη)
≤u
4γTη2E/bracketleftig
∥αx
k∥2/bracketrightig
+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥γαv
k/η∥2/bracketrightig
ds+u
4γK/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥αx
k/η∥2/bracketrightig
ds
+u
4γ3M2Kη3/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+/parenleftbig
γ2+ 2u2M2/parenrightbig
C∆2d+u2σ2+ 2u2G2+ 2du/parenrightbig
+u
4γKη/parenleftbigg∆2d
4+σ2/parenrightbigg
≤u
4γ3M2Kη3/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+/parenleftbig
γ2+ 2u2M2/parenrightbig
C∆2d+u2σ2+ 2u2G2+ 2du/parenrightbig
+u
4γKη/parenleftbigg∆2d
4+σ2/parenrightbigg
+uB
4γT+uKA
4+uKB
4γ
≤u
4γ3M2Kη3/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+/parenleftbig
γ2+ 2u2M2/parenrightbig
C∆2d+u2σ2+ 2u2G2+ 2du/parenrightbig
+u
4γKη/parenleftbigg∆2d
4+σ2/parenrightbigg
+uKA
4+uKB
2γ
≤u
4γ3M2Kη3/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+u2σ2+ 2u2G2+ 2du/parenrightbig
+u
4γKησ2+u
16γKη∆2d+uKA
4+uKB
2γ
=:C0Kη3+C1Kησ2+C2Kη∆2+C3KA+C4KB,
where the constants are defined as
C0=u
4γ3M2/parenleftbig/parenleftbig
γ2+ 2u2M2/parenrightbig
E+u2σ2+ 2u2G2+ 2du/parenrightbig
C1=u
4γ
C2=u
16γd
C3=u
4
C4=u
2γ.
By the weighted CKP inequality and given Kη≥1,
W2(pK,ˆpKη)≤Λ/parenleftbigg/radicalig
DKL(pK||ˆpKη) +4/radicalig
DKL(pK||ˆpKη)/parenrightbigg
≤/parenleftig
/tildewiderC0√η+/tildewiderC1/tildewideA+/tildewiderC2√
∆/parenrightig/radicalbig
Kη+/tildewiderC3√
KA+/tildewiderC4√
KB,
31Published in Transactions on Machine Learning Research (04/2024)
where the constants are defined as:
/tildewiderC0=Λ/parenleftig/radicalbig
C0+4/radicalbig
C0/parenrightig
/tildewiderC1=Λ/parenleftig/radicalbig
C1+4/radicalbig
C1/parenrightig
/tildewiderC2=Λ/parenleftig/radicalbig
C2+4/radicalbig
C2/parenrightig
/tildewiderC3=Λ/parenleftig/radicalbig
C3+4/radicalbig
C3/parenrightig
/tildewiderC4=Λ/parenleftig/radicalbig
C4+4/radicalbig
C4/parenrightig
/tildewideA2=Λ max/braceleftig
σ2,√
σ2/bracerightig
.
From the same analysis of (26), we can have:
W2(pK,p∗)≤/parenleftig
/tildewiderC0√η+/tildewiderC1/tildewideA/parenrightig/radicalbig
Kη+/tildewiderC2/radicalbig
Kη∆ +/tildewiderC3√
KA+/tildewiderC4√
KB+ Γ0e−µ∗Kη.(38)
To bound the Wasserstein distance, we need to set
Λ/tildewiderC0/radicalbig
Kη2=ϵ
2and Γ0e−µ∗Kη=ϵ
2. (39)
Solving the equation (39), we can have
Kη=log/parenleftbig2Γ0
ϵ/parenrightbig
µ∗andη=ϵ2
4Λ2/tildewiderC02Kη.
Combining these two we can have
η=ϵ2µ∗
4Λ2/tildewiderC02log/parenleftbig2Γ0
ϵ/parenrightbigandK=4Λ2/tildewiderC02log2/parenleftbig2Γ0
ϵ/parenrightbig
ϵ2(µ∗)2.
Plugging in (38) completes the proof.
E.4 Proof of Theorem 4
Inthissection, weanalyzetheconvergenceoffull-precisiongradientaccumulators(SGHMCLP-F)introduced
in Section 3.1 when the target distribution is strongly log-concave. Again SGHMCLP-F uses biased gradients
because we quantize the parameter before taking the gradients. We need to derive the upper bound given
biased gradients.
The SGHMCLP-F update follows
vvk+1=vke−γη−uγ−1(1−e−γη)QG(/tildewidest∇U(QW(xk))) +ξv
k
vxk+1=xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(QW(xk))) +ξx
k,
In this section, we prove the convergence of SGHMCLP-F in terms of 2-Wasserstein distance for strongly-
log-concave target distribution via coupling argument. To simplify the notation we define the quantized
stochastic gradients at xas:
˜g(x) :=QG(/tildewidest∇U(QW(x)))
=:∇U(x) +ξ. (40)
32Published in Transactions on Machine Learning Research (04/2024)
Lemma 12. For any x∈Rd, the random noise ξof the low-precision gradients defined in (40)satisfies:
∥Eξ∥2≤M2∆2d
4
E[∥ξ∥2]≤(M2+ 1)∆2d
4+σ2.
The proof of Lemma 12 can be found in Appendix F.1. We follow the proof in Cheng et al. (2018). Denote by
B(Rd)the Borelσ-field of Rd. Given probability measures µandνon(Rd,B(Rd)), we define a transference
planζbetweenµandνas a probability measure on (Rd×Rd,B(Rd×Rd))such that for all sets A∈Rd,
ζ(A×Rd) =µ(A)andζ(Rd×A) =ν(A). We denote Γ(µ,ν)as the set of all transference plans. A pair
of random variables (x,y)is called a coupling if there exists a ζ∈Γ(µ,ν)such that (x,y)is distributed
according to ζ. (With some abuse of notation, we will also refer to ζas the coupling.)
To calculate the Wasserstein distance from the proposed sample (xK,vK)and the target distribution sample
(x∗,v∗), we define sample qk= (xk,xk+vk)and the target distribution sample q∗= (x∗,x∗+v∗). Let
pk= (xk,vk)and/hatwideΦηbe the operator that maps from pktopk+1i.e.
pk+1=/hatwideΦηpk.
The solution (xt,vt)of the continuous underdamped Langevin dynamics with exact gradient satisfies the
following equations:
vt=v0e−γt−u/parenleftbigg/integraldisplayt
0e−γ(t−s)∇U(xs)ds/parenrightbigg
+/radicalbig
2γu/integraldisplayt
0e−γ(t−s)dBs, (41)
xt=x0+/integraldisplayt
0˜vsds.
LetΦηdenote the operator that maps p0to the solution of continuous underdamped Langevin dynamics in
(41) after time step η. Notice the solution (˜vt,˜xt)of the discrete underdamped Langevin dynamics as in
(10) with an exact gradient can be written as
˜vt=˜v0e−γt−u/parenleftbigg/integraldisplayt
0e−γ(t−s)∇U(˜x0)ds/parenrightbigg
+/radicalbig
2γu/integraldisplayt
0e−γ(t−s)dBs, (42)
˜xt=˜x0+/integraldisplayt
0˜vsds.
We can also define a similar operator for the discrete underdamped Langevin dynamics solution ˜pt= (˜xt,˜vt),
let/tildewideΦtbe the operator that maps ˜p0to˜pt. Furthermore the SGHMCLP-F can be written as:
vt=v0e−γt−u/parenleftbigg/integraldisplayt
0e−γ(t−s)˜g(x0)ds/parenrightbigg
+/radicalbig
2γu/integraldisplayt
0e−γ(t−s)dBs, (43)
xt=˜x0+/integraldisplayt
0vsds.
Given ˜g(x0) =∇U(x0) +ξ0andx0=˜x0, we know:
vt=˜vt−u/parenleftbigg/integraldisplayt
0e−γ(t−s)ds/parenrightbigg
ξ (44)
xt=˜xt−u/parenleftbigg/integraldisplayt
0/parenleftbigg/integraldisplayr
0e−γ(t−s)ds/parenrightbigg
dr/parenrightbigg
ξ.
Lemma 13. Letq0be some initial distribution and /tildewideΦηandΦηbe the operator we defined above for discrete
Langevin dynamics with exact full-precision gradients and low-precision gradients respectively. If the stepszie
1>η> 0, then the Wasserstein distance satisfies
W2
2(Φηq0,q∗)≤/parenleftig
W2(/tildewideΦηq0,q∗) +√
5/2uη√
dM∆/parenrightig2
+ 5u2η2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
.
33Published in Transactions on Machine Learning Research (04/2024)
The proof of Lemma 13 can be found in Appendix F.2. The lemma 13 says that if starting from the same
distribution after one step of low-precision update the Wasserstein distance from the target distribution is
bounded by the distance after one step of exact gradients plus O(η2∆2). Furthermore from the corollary 7
in Cheng et al. (2018) we know that for any i∈{1,···,K}:
W2
2(Φηqi,q∗)≤e−η/2κ1W2
2(qi,q∗), (45)
whereκ1=M/m 1is the condtion number. Let EKdenote the 26/parenleftbig
d/m 1+D2/parenrightbig
, and from the discretization
error bound from Theorem 9 and Lemma 8 (sandwich inequality) in Cheng et al. (2018), we get
W2(Φηqi,/tildewideΦηqi)≤2W2(Φηpi,/tildewideΦηpi)≤η2/radicalbigg
8EK
5.
By triangle inequality:
W2(/tildewideΦηqi,q∗)≤W 2(Φηqi,/tildewideΦηqi) +W2(Φηqi,q∗)
≤η2/radicalbigg
8EK
5+e−η/2κ1W2(qi,q∗).
Combine this with the result in Lemma 13 we have,
W2
2(/hatwideΦηqi,q∗)≤/parenleftigg
e−η/2κ1W2(qi,q∗) +η2/radicalbigg
8EK
5+√
5/2uη√
dM∆/parenrightigg2
+ 5u2η2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
.(46)
By invoking the Lemma 7 in Dalalyan & Karagulyan (2019) we can bound the 2-Wasserstein distance by:
W2(qK,q∗)≤e−Kη/2κ1W2(q0,q∗) +η2/radicalig
8EK
5+uηM ∆√
5d
2
1−e−η/2κ1
+5u2η2/parenleftig
(M2+ 1)∆2d
4+σ2/parenrightig
η2/radicalig
8EK
5+uηM ∆√
5d
2+√
1−e−η/κ 1/radicalig
5u2η2/parenleftbig
(M2+ 1)∆2d
4+σ2/parenrightbig.
Finally, by sandwich inequality we have:
W2(pK,p∗)≤4e−Kη/2κW2(p0,p∗) + 4η2/radicalig
8EK
5+uηM ∆√
5d
2
1−e−η/2κ
+20u2η2/parenleftig
(M2+ 1)∆2d
4+σ2/parenrightig
η2/radicalig
8EK
5+uηM ∆√
5d
2+√
1−e−η/κ/radicalig
5u2η2/parenleftbig
(M2+ 1)∆2d
4+σ2/parenrightbig.
Now we let the first term less than ϵ/3, from the lemma 13 in (Cheng et al., 2018) we know that W2(pK,p∗)≤
3/parenleftig
d
m1+D2/parenrightig
. So we can choose Kas the following,
K≤2κ1
ηlog/parenleftbigg
36/parenleftbiggd
m1+D2/parenrightbigg/parenrightbigg
.
Next, we choose a step size η≤ϵκ−1
1√
479232/5(d/m 1+D2)to ensure the second term is controlled below ϵ/3 +
16κ1uM∆√
5d
2. Since 1−e−η/2κ1≥η/4κ1and definition of EK,
4η2/radicalig
8EK
5+uηM ∆√
5d
2
1−e−η/2κ≤4η2/radicalig
8EK
5+uηM ∆√
5d
2
η/4κ1≤16κ1/parenleftigg
η/radicalbigg
8EK
5+uM∆√
5d
2/parenrightigg
≤ϵ/3 +16κ1uM∆√
5d
2.
34Published in Transactions on Machine Learning Research (04/2024)
Finally by choosing the step size satisfied that,
η≤ϵM∆√
5d
120u/bracketleftbig
(M2+ 1)∆2d
4+σ2/bracketrightbig,
the third term can be bounded as:
20u2η2/parenleftig
(M2+ 1)∆2d
4+σ2/parenrightig
η2/radicalig
8EK
5+uηM ∆√
5d
2+√
1−e−η/κ/radicalig
5u2η2/parenleftbig
(M2+ 1)∆2d
4+σ2/parenrightbig
≤20u2η2/parenleftig
(M2+ 1)∆2d
4+σ2/parenrightig
uηM ∆√
5d
2= 40uη/parenleftig
(M2+ 1)∆2d
4+σ2/parenrightig
M∆√
5d≤ϵ/3.
This complete the proof.
E.5 Proof of Theorem 5
In this section, we analyze the convergence of SGHMCLP-L when the target distribution is strongly log-
concave. We mainly follow the proof in Cheng et al. (2018), the difference is we need to handle the noise.
Recall the SGHMCLP-L update rule:
vk+1=QW/parenleftig
vvke−γη−uγ−1(1−eγη)QG(/tildewidest∇U(xk)) +ξv
k/parenrightig
xk+1=QW/parenleftig
xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xk)) +ξx
k/parenrightig
.
If we letαx
kandαv
kdenote the quantization error,
αx
k=QW/parenleftig
vke−γη−uγ−1(1−eγη)QG(/tildewidest∇U(xs)) +ξv
k/parenrightig
−/parenleftig
vke−γη−uγ−1(1−eγη)QG(/tildewidest∇U(xs)) +ξv
k/parenrightig
αv
k=QW/parenleftig
xs+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xs)) +ξx
k/parenrightig
−/parenleftig
xs+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xs)) +ξx
k/parenrightig
,
we can rewrite the update rule as:
vk+1=vke−γη−uγ−1(1−eγη)QG(/tildewidest∇U(xs)) +ξv
k+αv
k
xk+1=xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xk)) +ξx
k+αx
k. (47)
Similarly, we can define a continuous interpolation of (47) for t∈(0,η].
vt=v0e−γt−u/parenleftbigg/integraldisplayt
0e−γ(t−s)(∇U(x0) +ζ)ds/parenrightbigg
+/radicalbig
2γu/integraldisplayt
0e−γ(t−s)dBs+/integraldisplayt
0αv(s)ds
xt=x0+/integraldisplayt
0vsds+/integraldisplayt
0αx(s)ds, (48)
where theζ=QG/parenleftig
/tildewidest∇U(ˆx0)/parenrightig
−/tildewidest∇U(ˆx0)the function αv(s),αx(s)are defined as:
αv(s) =∞/summationdisplay
k=0αv
k/η1s∈(kη,(k+1)η)
αx(s) =∞/summationdisplay
k=0αx
k/η1s∈(kη,(k+1)η).
35Published in Transactions on Machine Learning Research (04/2024)
If we let ˆp0= (ˆx0,ˆv0)be the initial sample and ˆpt= (ˆxt,ˆvt)be the sample that satisfies the previous
equations, we can define an operator ˆΦtthat maps ˆp0toˆpti.e., ˆpt=ˆΦtˆp0. Notice that since ˆptis the
continuous interpolation of (6), thus ˆpkη=pk= (xk,vk). Similarly, we define qk= (xk,vk+xk) =: (xk,ωk)
as a tool to analyze the convergence of pk.
We are now ready to compute the Wasserstein distance between ˆΦηq0andq∗. Let Γ1be all of the couplings
between/tildewideΦηq0andq∗, and Γ2be all of the couplings between /hatwideΦηq0andq∗. Letr1be the optimal coupling
between/tildewideΦηq0andq∗. By taking the difference between (48) and (42),
/bracketleftbiggx
ω/bracketrightbigg
=/bracketleftbigg/tildewidex
/tildewideω/bracketrightbigg
+u/bracketleftbigg/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr/parenrightbig
ζ+/integraltextη
0αx(s)ds/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr+/integraltextη
0e−γ(s−η)ds/parenrightbig
ζ+/integraltextη
0αx(s) +αv(s)ds/bracketrightbigg
.
Let us now analyze the Wasserstein distance between ˆΦηq0andq∗,
W2
2/parenleftig
ˆΦηq0,q∗/parenrightig
(49)
≤Er1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbigg/tildewidex
/tildewideω/bracketrightbigg
+u/bracketleftbigg /parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr/parenrightbig
ζ+/integraltextη
0αx(s)ds/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr+/integraltextη
0e−γ(s−η)ds/parenrightbig
ζ+/integraltextη
0(αx(s) +αv(s))ds/bracketrightbigg
−/bracketleftbiggx∗
ω∗/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤Er1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbigg/tildewidex
/tildewideω/bracketrightbigg
−/bracketleftbiggx∗
ω∗/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+u2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbigg /parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr/parenrightbig
ζ+/integraltextη
0αx(s)ds/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr+/integraltextη
0e−γ(s−η)ds/parenrightbig
ζ+/integraltextη
0(αx(s) +αv(s))ds/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤W2
2/parenleftig
/tildewideΦηq0,q∗/parenrightig
+ 4u2
/parenleftigg/integraldisplayδ
0/parenleftbigg/integraldisplayr
0e−γ(s−r)ds/parenrightbigg
dr/parenrightigg2
+/parenleftigg/integraldisplayδ
0e−γ(s−δ)ds/parenrightigg2
/parenleftbigg∆2d
4+σ2/parenrightbigg
+u2E/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplayη
0(αx(s))ds/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
+u2E/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplayη
0(αx(s) +αv(s))ds/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
≤W2
2/parenleftig
/tildewideΦηq0,q∗/parenrightig
+ 4u2/parenleftbiggη4
4+η2/parenrightbigg/parenleftbigg∆2d
4+σ2/parenrightbigg
+u2E/bracketleftig
∥αx
k∥2/bracketrightig
+u2E/bracketleftig
∥αx
k+αv
k∥2/bracketrightig
≤W2
2/parenleftig
/tildewideΦηq0,q∗/parenrightig
+ 5u2η2/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 2u2/parenleftig
E∥αx
k∥2+E∥αv
k∥2/parenrightig
≤W2
2/parenleftig
/tildewideΦηq0,q∗/parenrightig
+ 5u2η2/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 2u2(A+B), (50)
where the constant A,Bare the uniform bounds of E[∥αx
k∥]andE[∥αv
k∥]respectively. Furthermore from
the corollary 7 in Cheng et al. (2018) we know that for any i∈{1,···,K}:
W2
2(Φηqi,q∗)≤e−η/2κ1W2
2(qi,q∗), (51)
whereκ1=M/m 1is the condtion number. From the discretization error bound from theorem 9 and lemma
8(sandwich inequality) in Cheng et al. (2018), we get
W2(Φηqi,/tildewideΦηqi)≤2W2(Φηpi,/tildewideΦηpi)≤η2/radicalbigg
8EK
5.
By triangle inequality:
W2(/tildewideΦηqi,q∗)≤W 2(Φηqi,/tildewideΦηqi) +W2(Φηqi,q∗)
≤η2/radicalbigg
8EK
5+e−η/2κ1W2(qi,q∗),
further implies the following inequality:
W2
2/parenleftig
ˆΦηqi,q∗/parenrightig
≤/parenleftigg
e−η/2κ1W2(qi,q∗) +η2/radicalbigg
8EK
5/parenrightigg2
+ 5u2η2/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 2u2(A+B).(52)
36Published in Transactions on Machine Learning Research (04/2024)
By invoking the Lemma 7 in Dalalyan & Karagulyan (2019) we can bound the Wasserstein distance by:
W2(qK,q∗)≤e−Kη/2κ1W2(q0,q∗) +η2/radicalig
8EK
5
1−e−η/2κ1
+5u2η2/parenleftig
∆2d
4+σ2/parenrightig
+ 2u2(A+B)
η2/radicalig
8EK
5+√
1−e−η/2κ1/radicalig
5u2η2/parenleftbig∆2d
4+σ2/parenrightbig
+ 2u2(A+B).
Finally, by sandwich inequality we have:
W2(pK,p∗)≤4e−Kη/2κ1W2(q0,q∗) +4η2/radicalig
8EK
5
1−e−η/2κ1(53)
+20u2η2/parenleftig
∆2d
4+σ2/parenrightig
+ 8u2(A+B)
η2/radicalig
8EK
5+√
1−e−η/2κ1/radicalig
5u2η2/parenleftbig∆2d
4+σ2/parenrightbig
+ 2u2(A+B).
And in this case, we know that E[∥αx
k∥]andE[∥αv
k∥]can be bouned by∆2d
4. Finally, we can have:
W2(pK,p∗)≤4e−Kη/2κ1W2(q0,q∗) +4η2/radicalig
8EK
5
1−e−η/2κ1
+20u2η2/parenleftig
∆2d
4+σ2/parenrightig
+ 4u2∆2d
η2/radicalig
8EK
5+√
1−e−η/2κ1/radicalig
5u2η2/parenleftbig∆2d
4+σ2/parenrightbig
+u2∆2d.
Now we let the first term less than ϵ/3, from the lemma 13 in (Cheng et al., 2018) we know that W2(q0,q∗)≤
3/parenleftig
d
m1+D2/parenrightig
. So we can choose Kas the following,
K≤2κ1
ηlog/parenleftbigg
36/parenleftbiggd
m1+D2/parenrightbigg/parenrightbigg
.
Next, we choose a step size η≤ϵκ−1
1√
479232/5(d/m 1+D2)to ensure the second term is controlled below ϵ/3. Since
1−e−η/2κ1≥η/4κ1and definition of EK,
4η2/radicalig
8EK
5
1−e−η/2κ≤4η2/radicalig
8EK
5
η/4κ1≤16κ1/parenleftigg
η/radicalbigg
8EK
5/parenrightigg
≤ϵ/3.
Finally by choosing the step size satisfied that,
η≤ϵ2
2880κ1u/parenleftbig∆2d
4+σ2/parenrightbig,
37Published in Transactions on Machine Learning Research (04/2024)
the third term can be bounded as:
20u2η2/parenleftig
(M2+ 1)∆2d
4+σ2/parenrightig
+ 4u2∆2d
η2/radicalig
8EK
5+√
1−e−η/2κ1/radicalig
5u2η2/parenleftbig
(M2+ 1)∆2d
4+σ2/parenrightbig
≤20u2η2/parenleftig
(M2+ 1)∆2d
4+σ2/parenrightig
+ 4u2∆2d
√
1−e−η/2κ1/radicalig
5u2η2/parenleftbig
(M2+ 1)∆2d
4+σ2/parenrightbig≤20u2η2/parenleftig
(M2+ 1)∆2d
4+σ2/parenrightig
+ 4u2∆2d
/radicalbig
η/4κ1/radicalig
5u2η2/parenleftbig
(M2+ 1)∆2d
4+σ2/parenrightbig
≤4/radicaligg
20κ1u2η/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+8u2∆2d√κ1
η3/2/radicalig
5u2η2/parenleftbig
(M2+ 1)∆2d
4+σ2/parenrightbig
≤ϵ/3 +8u2∆2d√κ1
η3/2/radicalig
5u2η2/parenleftbig
(M2+ 1)∆2d
4+σ2/parenrightbig.
This completes the proof.
E.6 Proof of Theorem 6
In this section, we analyze the convergence of VC SGHMCLP-L when the target distribution is strongly
log-concave. This proof is similar to the proof of Theorem 5, however, the variance-corrected quantization
function gives us a bound for the difference between the quantized value and the full-precision value. This
bound can scale with the learning rate. This fact leads to the advantage of variance-corrected quantization
over naive stochastic rounding. Recall the VC SGHMCLP-L update rule is the following,
vk+1=Qvc/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG/parenleftig
/tildewidest∇U(xk)/parenrightig
,Varv,∆/parenrightig
xk+1=Qvc/parenleftig
xk+γ−1/parenleftbig
1−e−γη/parenrightbig
vk+uγ−2/parenleftbig
γη+e−γη−1/parenrightbig
QG(/tildewidest∇U(xk)),Varx,∆/parenrightig
.(54)
If we letαx
kandαv
kdenote the quantization error,
αv
k=Qvc/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG/parenleftig
/tildewidest∇U(xk)/parenrightig
,Varv,∆/parenrightig
−/parenleftig
vke−γη−uγ−1(1−eγη)QG(/tildewidest∇U(xk)) +ξv
k/parenrightig
αx
k=Qvc/parenleftig
xk+γ−1/parenleftbig
1−e−γη/parenrightbig
vk+uγ−2/parenleftbig
γη+e−γη−1/parenrightbig
QG(/tildewidest∇U(xk)),Varx,∆/parenrightig
−/parenleftig
xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xk)) +ξx
k/parenrightig
,
we can rewrite the update rule as:
vk+1=vke−γη−uγ−1(1−eγη)QG(/tildewidest∇U(xk)) +ξv
k+αv
k
xk+1=xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xk)) +ξx
k+αx
k.
Next, we first derive a uniform bound of E/bracketleftig
∥αv
k∥2/bracketrightig
. In this section and the following section, we further
assume the norm of quantized stochastic gradients are bounded.
Assumption 5. For anyx∈Rd, there exists a constant Gand the quantized stochastic gradients at x
satisfies the following
E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoubleQG(/tildewidest∇U(x))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
≤G2.
38Published in Transactions on Machine Learning Research (04/2024)
By the definition of the variance corrected quantization function Qvc, whenVarv> ρ0=∆2
4, if we letψk
denotevke−γη−uγ−1(1−e−γη)QG/parenleftig
/tildewidest∇U(xk)/parenrightig
,
E/bracketleftig
∥αv
k∥2/vextendsingle/vextendsingle/vextendsingleψk/bracketrightig
=E/bracketleftig/vextenddouble/vextenddouble/vextenddouble/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig
+/radicalbig
Varvξk
−Qd/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk)) +/radicalbig
Varv−ρ0ξk/parenrightig
−sign(r)c/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingle/vextendsingleψk/bracketrightbigg
Let
b=Qd/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk)) +/radicalbig
Varv−ρ0ξk/parenrightig
−/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk)) +/radicalbig
Varv−ρ0ξk/parenrightig
,
then
E/bracketleftig
∥αv
k∥2/vextendsingle/vextendsingle/vextendsingleψk/bracketrightig
=E/bracketleftig/vextenddouble/vextenddouble/vextenddouble/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig
+/radicalbig
Varvξk
−/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk)) +/radicalbig
Varv−ρ0ξk/parenrightig
−b−sign(r)c/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingle/vextendsingleψk/bracketrightbigg
=E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/radicalbig
Varvξk−/radicalbig
Varv−ρ0ξk−b−sign(r)c/vextenddouble/vextenddouble/vextenddouble2/vextendsingle/vextendsingle/vextendsingle/vextendsingleψk/bracketrightbigg
≤E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/radicalbig
Varvξk−/radicalbig
Varv−ρ0ξk/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+E/bracketleftig
∥b+sign(r)c∥2/vextendsingle/vextendsingle/vextendsingleψk/bracketrightig
≤2Varvd−ρ0d+ρ0d
≤4γudη. (55)
WhenVarv<∆2
W
4,
E[∥αv
k∥2]
=E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig
−vk+1+/radicalbig
Varvξk/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
=E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig
−vk+1/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/radicalbig
Varvξk/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
≤max/parenleftbigg
2E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig
−Qs/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
,2Varvd/parenrightbigg
.
(56)
Using the bound equation (6) in Li & De Sa (2019) gives us,
E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig
−Qs/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
≤∆/parenleftbig
1−e−γη/parenrightbig
E/bracketleftig/vextenddouble/vextenddouble/vextenddoublevk−uγ−1QG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble
1/bracketrightig
≤∆/parenleftbig
1−e−γη/parenrightbig√
d/parenleftig
E[∥vk∥] +E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleQG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble/bracketrightig/parenrightig
.
39Published in Transactions on Machine Learning Research (04/2024)
Now we need to derive a uniform bound of E[∥vk∥], by the update rule, we know that,
E/bracketleftig
∥vk+1∥2/bracketrightig
=E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoublevke−γη−uγ−1(1−eγη)QG(/tildewidest∇U(xk)) +ξv
k+αv
k/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
≤(1 +γη/2) (1−γη/2)2E/bracketleftig
∥vk∥2/bracketrightig
+/parenleftbigg2
γη+ 1/parenrightbigg
u2η2E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoubleQG(/tildewidest∇U)/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+ 2γudη +E/bracketleftig
∥αv
k∥2/bracketrightig
≤(1−γη/2)E/bracketleftig
∥vk∥2/bracketrightig
+ 3u2η/γG2+ 2γudη +E/bracketleftig
∥αv
k∥2/bracketrightig
.
When E/bracketleftig
∥αv
k∥2/bracketrightig
≤2Varvd<4γudη, the inequality can be further written as:
E/bracketleftig
∥vk+1∥2/bracketrightig
≤(1−γη/2)E/bracketleftig
∥vk∥2/bracketrightig
+ 3u2η/γG2+ 6γudη
≤E/bracketleftig
∥v0∥2/bracketrightig
+6u2ηG2
γ2η+12γudη
γη
≤E/bracketleftig
∥v0∥2/bracketrightig
+6u2ηG2
γ2+ 12ud.
IfE/bracketleftig
∥αv
k∥2/bracketrightig
≤2E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/parenleftig
vke−γη−uγ−1(1−e−γη)QG(/tildewidest∇U(xk))/parenrightig
−Qs/parenleftig
vke−γη−uγ−1(1−e−γη)QG(/tildewidest∇U(xk))/parenrightig/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
,
the ineuqality can be wirtten as:
E/bracketleftig
∥vk+1∥2/bracketrightig
≤(1−γη/2)E/bracketleftig
∥vk∥2/bracketrightig
+ 3u2η/γG2+ 2γudη + 2∆/parenleftbig
1−e−γη/parenrightbig√
d/parenleftig
E[∥vk∥] +E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleQG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble/bracketrightig/parenrightig
≤(1−γη/2)E/bracketleftig
∥vk∥2/bracketrightig
+ 3u2η/γG2+ 2γudη + 2∆γη√
d/parenleftigg/radicalbigg
E/bracketleftig
∥vk∥2/bracketrightig
+G/parenrightigg
≤/parenleftigg
/radicalbig
1−γη/2/radicalbigg
E/bracketleftig
∥vk∥2/bracketrightig
+∆γη√
d/radicalbig
1−γη/2/parenrightigg2
+ 3u2η/γG2+ 2γudη + 2∆γη√
dG.
Thus,
E[∥vk∥]≤/radicalbigg
E/bracketleftig
∥v0∥2/bracketrightig
+∆γη√
d/parenleftig
1−/radicalbig
1−γη/2/parenrightig/radicalbig
1−γη/2+3u2η/γG2+ 2γudη + 2∆γη√
dG
∆γη√
d√
1−γη/2+/radicalbigg
γη/2/parenleftig
3u2η/γG2+ 2γudη + 2∆γη√
dG/parenrightig
≤/radicalbigg
E/bracketleftig
∥v0∥2/bracketrightig
+∆γη√
d
1−γη/2+/radicalig
6u2/γ2G2+ 4ud+ 4∆√
dG
≤/radicalbigg
E/bracketleftig
∥v0∥2/bracketrightig
+ ∆√
d+/radicalig
6u2/γ2G2+ 4ud+ 4∆√
dG.
Finally, we can have:
E[∥vk∥]≤max/braceleftigg/radicalbigg
E/bracketleftig
∥v0∥2/bracketrightig
+ ∆√
d+/radicalig
6u2/γ2G2+ 4ud+ 4∆√
dG,
/radicalbigg
E/bracketleftig
∥v0∥2/bracketrightig
+/radicaligg
6u2ηG2
γ2+√
12ud/bracerightigg
=:A′.
Thus, we can have,
E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig
−Qs/parenleftig
vke−γη−uγ−1/parenleftbig
1−e−γη/parenrightbig
QG(/tildewidest∇U(xk))/parenrightig/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
≤∆γη√
d(A′+G),
40Published in Transactions on Machine Learning Research (04/2024)
and we can bound the E/bracketleftig
∥αv
k∥2/bracketrightig
as,
E/bracketleftig
∥αv
k∥2/bracketrightig
≤max/braceleftig
∆γη√
d(A′+G),4γudη/bracerightig
=γηmax/braceleftig
∆√
d(A′+G),4ud/bracerightig
=:γηA. (57)
Now we bound the E/bracketleftig
∥αx
k∥2/bracketrightig
. WhenVarx≥ρ0, as the same analysis in (55) we can show,
E/bracketleftig
∥αx
k∥2/bracketrightig
≤2Varxd≤4udη2.
IfVarx<ρ0, and letµx=xk+γ−1(1−e−γη)vk+uγ−2(γη+e−γη−1)QG(/tildewidest∇U(xk)), by the same analysis
in (56) we can have:
E/bracketleftig
∥αx
k∥2/bracketrightig
≤max/braceleftig
2E/bracketleftig
∥µx−Qs(µx)∥2/bracketrightig
,2Varxd/bracerightig
.
Again using the bound equation (6) in Li & De Sa (2019) gives us,
E/bracketleftig
∥µx−Qs(µx)∥2/bracketrightig
≤∆E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleγ−1/parenleftbig
1−e−γη/parenrightbig
vk+uγ−2/parenleftbig
γη+e−γη−1/parenrightbig
QG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble
1/bracketrightig
≤∆ηE[∥vk∥1] +uη2
2E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleQG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble
1/bracketrightig
≤∆η√
dE[∥vk∥] +uη2
2√
dE/bracketleftig/vextenddouble/vextenddouble/vextenddoubleQG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble/bracketrightig
≤∆η√
dA′+uη2
2√
dG.
Thus, we can have,
E/bracketleftig
∥αx
k∥2/bracketrightig
≤max/braceleftig
2∆η√
dA′+uη2√
dG,4udη2/bracerightig
≤ηmax/braceleftig
2∆√
dA′+uη√
dG,4udη/bracerightig
=:ηB. (58)
Then follow the same analysis of (53), we can show
W2(pK,p∗)≤4e−Kη/2κ1W2(q0,q∗) +4η2/radicalig
8EK
5
1−e−η/2κ1(59)
+20u2η2/parenleftig
∆2d
4+σ2/parenrightig
+ 8u2η(γA+B)
η2/radicalig
8EK
5+√
1−e−η/κ 1/radicalig
5u2η2/parenleftbig∆2d
4+σ2/parenrightbig
+ 2u2η(γA+B). (60)
Now we let the first term less than ϵ/3, from the Lemma 13 in (Cheng et al., 2018) we know that W2(q0,q∗)≤
3/parenleftig
d
m1+D2/parenrightig
. So we can choose Kas the following,
K≤2κ1
ηlog/parenleftbigg
36/parenleftbiggd
m1+D2/parenrightbigg/parenrightbigg
.
41Published in Transactions on Machine Learning Research (04/2024)
Next, we choose a step size η≤ϵκ−1
1√
479232/5(d/m 1+D2)to ensure the second term is controlled below ϵ/3. Since
1−e−η/2κ1≥η/4κ1and definition of EK,
4η2/radicalig
8EK
5
1−e−η/2κ1≤4η2/radicalig
8EK
5
η/4κ1≤16κ1/parenleftigg
η/radicalbigg
8EK
5/parenrightigg
≤ϵ/3.
Finally choosing the step size satisfied that,
η≤ϵ2
2880κ1u/parenleftbig∆2d
4+σ2/parenrightbig,
the third term can be bounded as:
20u2η2/parenleftig
∆2d
4+σ2/parenrightig
+ 8u2η(γA+B)
η2/radicalig
8EK
5+√
1−e−η/κ 1/radicalig
5u2η2/parenleftbig∆2d
4+σ2/parenrightbig
+ 2u2η(γA+B)
≤20u2η2/parenleftig
∆2d
4+σ2/parenrightig
+ 8u2η(γA+B)
√
1−e−η/κ 1/radicalig
5u2η2/parenleftbig∆2d
4+σ2/parenrightbig
+ 2u2η(γA+B)≤20u2η2/parenleftig
∆2d
4+σ2/parenrightig
+ 8u2η(γA+B)
/radicalbig
η/4κ1/radicalig
5u2η2/parenleftbig∆2d
4+σ2/parenrightbig
+ 2u2η(γA+B)
≤4/radicaligg
20u2κ1η/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 8κ1u2(γA+B)
≤ϵ/3 + 8/radicalbig
2κ1u2(γA+B).
This completes the proof.
E.7 Proof of Thoerem 7
In this section we generalize the convergence analysis of LPSGLDLP-F in Zhang et al. (2022) to non-log-
concave target distribution. We prove a more general version of theorem 7 following the same proof outlines
in Raginsky et al. (2017). We further introduce an assumption about the initial distribution p0.
Assumption 6. The probability p0of the initial hypothesis x0has a bounded and strictly positive density
and satisfies the following:
κ0:= log/integraldisplay
Rde∥x∥2p0(x)dx<∞.
Note that the for initial distribution x0= 0, the value κ0= 0is bounded and the assumption is satisfied.
Recall the Overdamped Langevin dynamics is
dxt=−∇U(xt)dt+√
2dBt. (61)
We further define the value of the energy function and the gradient at point 0at the following:
|U(0)|=G0,∥∇U(0)∥=G1.
In order to analyze the convergence of SGLD for non-log-concave distribution, we need to introduce extra
assumptions.
Then the solution of the Langevin dynamics should satisfies
xt=x0−/integraldisplayt
0∇U(xs)ds+√
2/integraldisplayt
0dBs. (62)
To analyze the LPSGLDLP-F in (1), we define a continuous interpolation of the low-precision sample as:
ˆxt= ˆx0−/integraldisplayt
0Gsds+√
2/integraldisplayt
0dBs. (63)
42Published in Transactions on Machine Learning Research (04/2024)
whereGs=K/summationtext
k=0˜g(ˆxk)1s∈[kη,(k+1)η). The Wasserstein distance can be bounded as
W2(pK,p∗)≤W 2(pK,ˆpKη) +W2(ˆpKη,p∗). (64)
Now, we are ready to introduce the contraction rate λ∗of the overdamped Langevin dynamics (13). By
borrowing the proposition 9 of (Raginsky et al., 2017), we can have:
Lemma 14 (Proposittion 9 in Raginsky et al. (2017)) .Suppose Assumptions 1 and 2 hold. Then
W2(ˆpKη,p∗)≤/radicaligg
2CLS/parenleftbigg
log∥p0∥∞+d
2log3π
m+/parenleftbiggMκ0
3+B√κ0+G0+b
2log 3/parenrightbigg/parenrightbigg
e−Kη/CLS
≤/tildewiderC3e−Kη/CLS, (65)
with constant
CLS≤2m2
2+ 8M2
m2
2M+1
λ∗/parenleftbigg6Md
m2+ 2/parenrightbigg
.
where theλ∗=e−˜O(d)denotes the contraction rate of overdamped Langevin dynamics.
Here,λ∗acts as a contraction rate of the Markov process initiated by (13), with an exponential dependency
on the dimension dbeing inescapable in the worst-case scenario proved in Appendix B in (Raginsky et al.,
2017).
The first term of equation 64 can be bounded via the weighted CKP inequality
W2(pK,ˆpKη)≤CˆpKη/bracketleftigg/radicalig
DKL(pK||ˆpKη) +/parenleftbiggDKL(pK||ˆpKη)
2/parenrightbigg1/4/bracketrightigg
,
where the constant CˆpKη= 2 inf
λ>0/parenleftigg
1
λ/parenleftigg
3
2+log/integraltext
Rdeλ∥ω∥2ˆPKη(dω)/parenrightigg/parenrightigg
. By Lemma 4 in Raginsky et al. (2017)
and assuming Kη> 1, we can wrtie:
W2
2(pK,ˆpKη)≤(12 + 8 (κ0+ 2b+ 2d)Kη)/parenleftbigg
DKL(pK||ˆpKη) +/radicalig
DKL(pK||ˆpKη)/parenrightbigg
.
Now we bound the term DKL(pK||ˆpKη). The Radon-Nikodym derivative of the ˆPKηw.r.tpKis the following
dˆpKη
dpK=exp/braceleftigg
1
2/integraldisplayt
0(∇U(xs)−Gs)dBs−1
4/integraldisplayT
0∥∇U(xs)−Gs∥ds/bracerightigg
.
43Published in Transactions on Machine Learning Research (04/2024)
Thus, we have:
DKL(pK||ˆpKη) =EpK/bracketleftbigg
log/parenleftbiggdˆpKη
dpK/parenrightbigg/bracketrightbigg
=1
4/integraldisplayKη
0E/bracketleftig
∥∇U(xs)−Gs∥2/bracketrightig
ds
=1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xs)−˜g(xk)∥2/bracketrightig
ds
≤1
2K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
+1
2K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xk)−˜g(xk)∥2/bracketrightig
≤M2
2K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥xs−xk∥2/bracketrightig
+1
2K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xk)−˜g(xk)∥2/bracketrightig
. (66)
We now bound the first term in the RHS of the equation (66), from the update rule in (63) we know:
xs−xk=−(s−kη)˜g(xk) +√
2 (Bs−Bkη)
=−(s−kη)∇U(xk) + (s−kη) (∇U(xk)−˜g(xk)) +√
2 (Bs−Bkη),
thus,
E/bracketleftig
∥xs−xk∥2/bracketrightig
≤3η2E/bracketleftig
∥∇U(xk)∥2/bracketrightig
+ 3η2E/bracketleftig
∥∇U(xk)−˜g(xk)∥2/bracketrightig
+ 6ηd
≤3η2(ME[∥xk∥] +G)2+ 3η2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+ 6ηd. (67)
Similarly, we need a uniform bound of E/bracketleftig
∥xk∥2/bracketrightig
.
Lemma 15. Under assumptions 1, 2 and 3, if we set the step size η∈/parenleftbig
0,1∧m2
2M2/parenrightbig
, then for all k≥0, the
E/bracketleftig
∥vxk∥2/bracketrightig
can be bounded as
E/bracketleftig
∥xk∥2/bracketrightig
≤E+2/parenleftbig
M2+ 1/parenrightbig
∆2d
4m2,
providedE=E/bracketleftig
∥x0∥2/bracketrightig
+M
m2/parenleftbig
2b+ 2ηG2+ 2d/parenrightbig
.
The proof of Lemma 15 can be found in Appendix F.4. Using this bound, we can further bound
E/bracketleftig
∥xs−xs∥2/bracketrightig
as:
E/bracketleftig
∥xs−xs∥2/bracketrightig
≤6η2M2/parenleftigg
E+2/parenleftbig
M2+ 1/parenrightbig
m2∆2d
4/parenrightigg
+ 6η2G2+ 3η2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+ 6ηd
≤6η2M2E+ 6η2G2+ 6ηd+/parenleftigg
12η2M2/parenleftbig
M2+ 1/parenrightbig
m2+ 3(M2+ 1)/parenrightigg
η2∆2d
4+ 3η2σ2
=:Eη+Cη2∆2d
4+ 3η2σ2,
44Published in Transactions on Machine Learning Research (04/2024)
where the costant EandCare defined as:
E= 6M2E+ 6G2+ 6d
C=12η2M2/parenleftbig
M2+ 1/parenrightbig
m2+ 3(M2+ 1).
Thus the divergence can be bounded as:
DKL(pK||ˆpKη)≤M2
2/parenleftbigg
E+Cη∆2d
4+ 3ησ2/parenrightbigg
Kη2+1
2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
Kη
=M2
2EKη2+/parenleftbiggM2
2Cη2+1
2(M2+ 1)/parenrightbigg∆2d
4Kη+3M2η2+ 1
2σ2Kη
=M2
2EKη2+/parenleftbiggM2
2C+1
2(M2+ 1)/parenrightbigg∆2d
4Kη+3M2+ 1
2σ2Kη
=:C0Kη2+C1∆2d
4Kη+C2σ2Kη.
We are ready to bound the Wasserstein distance,
W2
2(pK,ˆpKη)≤(12 + 8 (κ0+ 2b+ 2d))/parenleftig
(C0+/radicalbig
C0)√η+/parenleftig
C1+/radicalbig
C1/parenrightig
A+/parenleftig
C2+/radicalbig
C2/parenrightig
B/parenrightig
(Kη)2
=:/parenleftig
/tildewiderC02√η+/tildewiderC12A+/tildewiderC22B/parenrightig
(Kη)2,
where the constants are defined as:
A=max/braceleftigg
∆2d
4,/radicalbigg
∆2d
4/bracerightigg
B=max/braceleftig
σ2,√
σ2/bracerightig
/tildewiderC02= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C0+/radicalbig
C0/parenrightig
/tildewiderC12= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C1+/radicalbig
C1/parenrightig
/tildewiderC22= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C2+/radicalbig
C2/parenrightig
.
From Proposition 9 in the paper Raginsky et al. (2017), we know that
W2(ˆpKη,p∗)≤/radicaligg
2CLS/parenleftbigg
log∥p0∥∞+d
2log3π
m+/parenleftbiggMκ0
3+B√κ0+G0+b
2log 3/parenrightbigg/parenrightbigg
e−Kη/CLS
=:/tildewiderC3e−Kη/CLS
Finally, we can have
W2(pK,p∗)≤/parenleftig
/tildewiderC0η1/4+/tildewiderC1√
A+/tildewiderC2√
B/parenrightig
Kη+/tildewiderC3e−Kη/CLS. (68)
To bound the Wasserstein distance, we need to set
/tildewiderC0Kη5/4=ϵ
2and/tildewiderC3e−Kη/CLS=ϵ
2. (69)
Solving the (69), we can have
Kη=CLSlog/parenleftigg
2/tildewiderC3
ϵ/parenrightigg
andη=ϵ4
16/tildewiderC04(Kη)4.
45Published in Transactions on Machine Learning Research (04/2024)
Combining these two we can have
η=ϵ4
16/tildewiderC04C4
LSlog4/parenleftig
2/tildewideC3
ϵ/parenrightigandK=16/tildewiderC04C5
LSlog5/parenleftig
2/tildewideC3
ϵ/parenrightig
ϵ4.
PluggingKandηinto (68) completes the proof.
E.8 Proof of Theorem 8
In this section we generalize the convergence analysis of SGLDLP-L in Zhang et al. (2022) to non-log-concave
target distribution. Following the same proof outlines in Raginsky et al. (2017). Recall the SGLDLP-L
update rule (2) is the following,
xk+1=QW(xk−η/tildewidest∇U(xk) +/radicalbig
2ηξk+1)
=:xk−η/tildewidest∇U(xk) +/radicalbig
2ηξk+1+αk,
whereαkis defined as:
αk=QW(xk−η/tildewidest∇U(xk) +/radicalbig
2ηξk+1)−xk−η/tildewidest∇U(xk) +/radicalbig
2ηξk+1.
Thus, we can define a continuous interpolation of the SGLDLP-L as:
xt=x0−/integraldisplayt
0Gsds+√
2/integraldisplayt
0dB(s) +/integraldisplayt
0α(s)ds,
whereGs=∞/summationtext
k=0QG(/tildewidest∇U(xk))1s∈(kη,(k+1)η)andα(s) =∞/summationtext
k=0αk/η1s∈(kη,(k+1)η). By taking the difference
of the interpolation with the discrete estimation of Langevin process in equation (62), we can derive the
Radon-Nikodym derivative of the ˆpKηw.r.tpKas:
dˆpKη
dpK=exp/braceleftigg
1
2/integraldisplayt
0(∇U(xs)−Gs−α(s))dBs−1
4/integraldisplayT
0∥∇U(xs)−Gs−α(s)∥2ds/bracerightigg
.
Thus, the divergence can be computed as:
DKL(pK||ˆpKη) =1
4/integraldisplayKη
0E/bracketleftig
∥∇U(xs)−Gs−α(s)∥2/bracketrightig
ds
=1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble∇U(xs)−QG(/tildewidest∇U(xk))−αk/η/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
ds
=1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble∇U(xs)−QG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
ds+1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥αk/η∥2/bracketrightig
ds
=1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥∇U(xs)−∇U(xk)∥2/bracketrightig
ds+1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble∇U(xk)−QG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
ds
+1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥αk/η∥2/bracketrightig
ds
≤M2
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥xs−xk∥2/bracketrightig
ds+1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble∇U(xk)−QG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
ds
+1
4K−1/summationdisplay
k=0/integraldisplay(k+1)η
kηE/bracketleftig
∥αk/η∥2/bracketrightig
ds. (70)
46Published in Transactions on Machine Learning Research (04/2024)
From the same analysis in (25), we know that
E/bracketleftig
∥xs−xk∥2/bracketrightig
≤3η2E/bracketleftig
∥∇U(xk)∥2/bracketrightig
+ 3η2E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble∇U(xk)−QG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+ 6ηd
≤3η2/parenleftig
ME/bracketleftig
∥xk∥2/bracketrightig
+G/parenrightig2
+ 3η2/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 6ηd.
Again, we need to derive a uniform bound of E/bracketleftig
∥xk∥2/bracketrightig
,
E/bracketleftig
∥xk+1∥2/bracketrightig
=E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoublexk−ηQG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+ 2E/bracketleftig
∥ξk+1∥2/bracketrightig
+E/bracketleftig
∥αk∥2/bracketrightig
=E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoublexk−η∇U(xk) +η∇U(xk)−ηQG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+ 2ηd+E/bracketleftig
∥αk∥2/bracketrightig
=E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoublexk−η∇U(xk) +η∇U(xk)−ηQG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+E/bracketleftig
∥αk∥2/bracketrightig
+ 2ηd
=E/bracketleftig
∥xk−η∇U(xk)∥2/bracketrightig
+η2E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble∇U(xk)−QG(/tildewidest∇U(xk))/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+E/bracketleftig
∥αk∥2/bracketrightig
+ 2ηd.
By plugging in the inequality we derived before:
E/bracketleftig
∥xk−η∇U(xk)∥2/bracketrightig
≤/parenleftbig
1−2ηm2+ 2η2M2/parenrightbig
E/bracketleftig
∥xk∥2/bracketrightig
+ 2ηb+ 2η2G2.
we can have:
E/bracketleftig
∥xk+1∥2/bracketrightig
≤/parenleftbig
1−2ηm2+ 2η2M2/parenrightbig
E/bracketleftig
∥xk∥2/bracketrightig
+ 2ηb+ 2η2G2+η2∆2d
4+η2σ2+E/bracketleftig
∥αk∥2/bracketrightig
+ 2ηd.(71)
Thus for any η∈(0,1∧m2
2M2)and1−2ηm2+ 2η2M2>0, we can bound E/bracketleftig
∥xk∥2/bracketrightig
for anyk>0as:
E/bracketleftig
∥xk∥2/bracketrightig
≤E/bracketleftig
∥x0∥2/bracketrightig
+1
2 (m2−ηM2)/parenleftbigg
2b+ 2G2+∆2d
4+σ2+ 2d/parenrightbigg
+E/bracketleftig
∥αk∥2/bracketrightig
2η(m2−ηM2)
≤E/bracketleftig
∥x0∥2/bracketrightig
+1
m2/parenleftbigg
2b+ 2G2+∆2d
4+σ2+ 2d/parenrightbigg
+E/bracketleftig
∥αk∥2/bracketrightig
ηm2
≤E+∆2d
4m2+E/bracketleftig
∥αk∥2/bracketrightig
ηm2,
where the constant Eis defined as:
E=E/bracketleftig
∥x0∥2/bracketrightig
+1
m2/parenleftbig
2b+ 2G2+σ2+ 2d/parenrightbig
.
Thus, we can have,
E/bracketleftig
∥xs−xk∥2/bracketrightig
≤6η2
E+∆2d
4m2+E/bracketleftig
∥αk∥2/bracketrightig
ηm2
+ 6η2G2+ 3η2/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 6ηd
≤Eη+ 3η2σ2+6 + 3m2
4m2η2∆2d+6ηE/bracketleftig
∥αk∥2/bracketrightig
m2.
47Published in Transactions on Machine Learning Research (04/2024)
Plugging this into the equation (70), we can have,
DKL(pK||ˆpKη)≤ME
4Kη2+3Mσ2Kη3
4+(6 + 3m2)M∆2d
16m2Kη3+6ME/bracketleftig
∥αk∥2/bracketrightig
Kη2
4m2+1
4/parenleftbigg∆2d
4+σ2/parenrightbigg
Kη+KE/bracketleftig
∥αk∥2/bracketrightig
4η
≤ME
4Kη2+3M+ 1
4σ2Kη+((6 + 3m2)M+m2)d
16m2∆2Kη+/parenleftbigg6Mη
4m2+1
4η/parenrightbigg
KE/bracketleftig
∥αk∥2/bracketrightig
.
By the fact that E/bracketleftig
∥αk∥2/bracketrightig
≤∆2d
4, we can further bound the divergence as:
DKL(pK||ˆpKη)≤ME
4Kη2+3M+ 1
4σ2Kη+/parenleftbigg((12 + 3m2)M+m2)d
16m2+d
16η/parenrightbigg
∆2K
=:C0Kη2+C1σ2Kη+C2∆2K,
where the constants are defined as:
C0=ME
4
C1=3M+ 1
4
C2=/parenleftbigg((12 + 3m2)M+m2)d
16m2+d
16η/parenrightbigg
.
We are ready to bound the Wasserstein distance,
W2
2(pK,ˆpKη)≤(12 + 8 (κ0+ 2b+ 2d))/bracketleftig/parenleftig
C0+/radicalbig
C0+/parenleftig
C1+/radicalbig
C1/parenrightig
A/parenrightig
(Kη)2+/parenleftig
C2+/radicalbig
C2/parenrightig
∆K2η/bracketrightig
=:/parenleftig
/tildewiderC02√η+/tildewiderC12A/parenrightig
(Kη)2+/tildewiderC22∆K2η,
where the constants are defined as:
A= max/braceleftig
σ2,√
σ2/bracerightig
/tildewiderC02= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C0+/radicalbig
C0/parenrightig
/tildewiderC12= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C1+/radicalbig
C1/parenrightig
/tildewiderC22= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C2+/radicalbig
C2/parenrightig
.
From Proposition 9 in the paper Raginsky et al. (2017), we know that
W2(ˆpKη,p∗)≤/radicaligg
2CLS/parenleftbigg
log∥p0∥∞+d
2log3π
m+/parenleftbiggMκ0
3+B√κ0+G0+b
2log 3/parenrightbigg/parenrightbigg
e−Kη/CLS
=:/tildewiderC3e−Kη/CLS
Finally, we can have
W2(pK,p∗)≤/parenleftig
/tildewiderC0η1/4+/tildewiderC1√
A/parenrightig
Kη+/tildewiderC2√
∆/radicalbig
K2η+/tildewiderC3e−Kη/CLS. (72)
To bound the 2-Wasserstein distance, we need to set
/tildewiderC0Kη5/4≤ϵ
2and/tildewiderC3e−Kη/CLS=ϵ
2. (73)
48Published in Transactions on Machine Learning Research (04/2024)
Solving the (73), we can have
Kη=CLSlog/parenleftigg
2/tildewiderC3
ϵ/parenrightigg
andη≤ϵ4
16/tildewiderC04(Kη)4.
Combining these two we can have
η≤ϵ4
16/tildewiderC04C4
LSlog4/parenleftig
2/tildewideC3
ϵ/parenrightigandK≥16/tildewiderC04C5
LSlog5/parenleftig
2/tildewideC3
ϵ/parenrightig
ϵ4.
PluggingKandηinto (72) completes the proof.
E.9 Proof of Theorem 9
In this section we generalize the convergence analysis of VC SGLDLP-F in Zhang et al. (2022) to non-log-
concave target distribution. The proof is similar to the proof of Theorem 8, but the variance corrected-
quantization function The variance-corrected quantization technique establishes a scalable bound for the
discrepancy between quantized and full-precision values, contingent on the learning rate. This enables
variance-corrected quantization advantage over simple stochastic rounding.
Recall that the update of VC SGLDLP-L is
xk+1=Qvc/parenleftig
xk−ηQG(/tildewidest∇U(xk)),2η,∆/parenrightig
=xk−ηQG(/tildewidest∇U(xk)) +/radicalbig
2ηξk+αk,
whereαkis defined as
αk=Qvc/parenleftig
xk−ηQG(/tildewidest∇U(xk)),2η,∆/parenrightig
−xk−ηQG(/tildewidest∇U(xk)) +/radicalbig
2ηξk.
From analysis in Zhang et al. (2022), we know that
E/bracketleftig
∥αk∥2/bracketrightig
≤max (2∆ηG,5ηd)
=:ηA.
Combining the analysis in section E.8, we can show,
DKL(pK||ˆpKη)≤ME
4Kη2+3M+ 1
4σ2Kη+((6 + 3m2)M+m2)d
16m2∆2Kη+/parenleftbigg6Mη
4m2+1
4η/parenrightbigg
KE/bracketleftig
∥αk∥2/bracketrightig
≤ME
4Kη2+3M+ 1
4σ2Kη+((6 + 3m2)M+m2)d
16m2∆2Kη+/parenleftbigg6Mη
4m2+1
4η/parenrightbigg
KηA
≤ME
4Kη2+3M+ 1
4σ2Kη+((6 + 3m2)M+m2)d
16m2∆2Kη+6M+m2
m2KA
=:C0Kη2+C1Kησ2+C2Kη∆2+C3KA,
where the constant C0,C1,C2andC3are defined as:
C0=ME
4
C1=3M+ 1
4
C2=((6 + 3m2)M+m2)d
16m2
C3=6M+m2
m2
49Published in Transactions on Machine Learning Research (04/2024)
We are ready to bound the Wasserstein distance,
W2
2(pK,ˆpKη)≤(12 + 8 (κ0+ 2b+ 2d))/bracketleftig/parenleftig/parenleftig
C0+/radicalbig
C0/parenrightig
η+/parenleftig
C1+/radicalbig
C1/parenrightig
/tildewideA/parenrightig
(Kη)2+/parenleftig
C2+/radicalbig
C2/parenrightig
∆(Kη)2
+/parenleftig
C3+/radicalbig
C3/parenrightig
AK2η/bracketrightig
=:/parenleftig
/tildewiderC02η+/tildewiderC12/tildewideA+/tildewiderC22∆/parenrightig
(Kη)2+/tildewiderC32AK2η,
where the constants are defined as:
/tildewideA= max/braceleftig
σ2,√
σ2/bracerightig
A= max/braceleftig
A,√
A/bracerightig
/tildewiderC02= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C0+/radicalbig
C0/parenrightig
/tildewiderC12= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C1+/radicalbig
C1/parenrightig
/tildewiderC22= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C2+/radicalbig
C2/parenrightig
/tildewiderC32= (12 + 8 (κ0+ 2b+ 2d))/parenleftig
C3+/radicalbig
C3/parenrightig
.
From Proposition 9 in the paper Raginsky et al. (2017), we know that
W2(ˆpKη,p∗)≤/radicaligg
2CLS/parenleftbigg
log∥p0∥∞+d
2log3π
m+/parenleftbiggMκ0
3+B√κ0+G0+b
2log 3/parenrightbigg/parenrightbigg
e−Kη/CLS
=:/tildewiderC4e−Kη/CLS
Finally, we can have
W2(pK,p∗)≤/parenleftig
/tildewiderC0√η+/tildewiderC1√
A+/tildewiderC2√
∆/parenrightig
Kη+/tildewiderC3√
A/radicalbig
K2η+/tildewiderC4e−Kη/CLS. (74)
Too bound the 2-Wasserstein distance, we need to set
/tildewiderC0Kη5/4=ϵ
2and/tildewiderC3e−Kη/CLS=ϵ
2. (75)
Solving the (75), we can have
Kη=CLSlog/parenleftigg
2/tildewiderC3
ϵ/parenrightigg
andη=ϵ4
16/tildewiderC04(Kη)4.
Combining these two we can have
η=ϵ4
16/tildewiderC04C4
LSlog4/parenleftig
2/tildewideC3
ϵ/parenrightigandK=16/tildewiderC04C5
LSlog5/parenleftig
2/tildewideC3
ϵ/parenrightig
ϵ4.
PluggingKandηinto (74) completes the proof.
50Published in Transactions on Machine Learning Research (04/2024)
F Techinical Proofs
F.1 Proof of Lemma 12
Proof.By the definition of ξin (E.4)
∥Eξ∥2=∥E˜g(x)−E∇U(x)∥2
=∥E∇U(Qw(x))−E∇U(x)∥2
≤E/bracketleftig
∥∇U(Qw(x))−∇U(x)∥2/bracketrightig
≤M2E/bracketleftig
∥Qw(x)−∇U(x)∥2/bracketrightig
≤M∆2d
4.
We also know that from the definition that
E∥ξ∥2=E∥˜g(x)−∇U(x)∥2
=E/vextenddouble/vextenddoubleQG(∇˜U(QW(x)))−∇˜U(QW(x)) +∇˜U(QW(x))−∇U(QW(x)) +∇U(QW(x))−∇U(x)/vextenddouble/vextenddouble2
=E/vextenddouble/vextenddoubleQG(∇˜U(QW(x)))−∇˜U(QW(x))/vextenddouble/vextenddouble2+E/vextenddouble/vextenddouble∇˜U(QW(x))−∇U(QW(x))/vextenddouble/vextenddouble2+E∥∇U(QW(x))−∇U(x)∥2
≤∆2d
4+σ2+M2E∥QW(x)−x∥2
≤(M2+ 1)∆2d
4+σ2,
where in the first inequality, we apply Assumptions 1 and 3.
F.2 Proof of Lemma 13
Proof.LetΓ1be the set of all couplings between /tildewideΦηq0andq∗andΓ2be the set of all couplings between
/hatwideΦηq0adnq∗. Letr1be the optimal coupling between /tildewideΦηq0andq∗, i.e.
E(θ,ϕ)∼r1[∥θ−ϕ∥2] =W2
2(/tildewideΦηq0,q∗).
Let/parenleftbigg/bracketleftbigg
˜x
˜ω/bracketrightbigg
,/bracketleftbigg
x∗
ω∗/bracketrightbigg/parenrightbigg
∼r1. We define the random variable/bracketleftbigg
x
ω/bracketrightbigg
as
/bracketleftbiggx
ω/bracketrightbigg
=/bracketleftbigg˜x
˜ω/bracketrightbigg
+u/bracketleftbigg/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr/parenrightbig
ξ/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr+/integraltextη
0e−γ(s−η)ds/parenrightbig
ξ/bracketrightbigg
.
By equation (44),/parenleftbigg/bracketleftbiggx
ω/bracketrightbigg
,/bracketleftbiggx∗
ω∗/bracketrightbigg/parenrightbigg
define a valid coupling between Φηq0andq∗. Now we can analyze the
Wasserstein distance between Φηq0andq∗.
51Published in Transactions on Machine Learning Research (04/2024)
W2
2(/hatwideΦηq0,q∗)≤Er1
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbigg˜x
˜ω/bracketrightbigg
+u/bracketleftigg/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr/parenrightbig
ξ/parenleftig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr+/integraltextδ
0e−γ(s−η)ds/parenrightig
ξ/bracketrightigg
−/bracketleftbiggx∗
ω∗/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(76)
≤Er1
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbigg˜x−x∗
˜ω−ω∗/bracketrightbigg
+u/bracketleftigg/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr/parenrightbig
Eξ/parenleftig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr+/integraltextδ
0e−γ(s−η)ds/parenrightig
Eξ/bracketrightigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

+Er1/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleu/bracketleftbigg/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr/parenrightbig
(ξ−Eξ)/parenleftbig/integraltextη
0/parenleftbig/integraltextr
0e−γ(s−r)ds/parenrightbig
dr+/integraltextη
0e−γ(s−η)ds/parenrightbig
(ξ−Eξ)/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
≤/parenleftig
W2(/tildewideΦηq0,q∗) + 2u/radicalbig
η4/4 +η2∥Eξ∥/parenrightig2
+ 4u2(η4/4 +η2)Er1/bracketleftig
∥ξ−Eξ∥2/bracketrightig
≤/parenleftig
W2(/tildewideΦηq0,q∗) +√
5/2uη√
dM∆/parenrightig2
+ 5u2η2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
. (77)
F.3 Proof of Lemma 10
Proof.In order to get the upper bound of ∥xk∥and∥vk∥, we bound the Lyapunov function E(xk,vk). By
the smooth Assumption 1, we know
U(xk+1)−U(x∗)≤U(xk) +⟨∇U(xk),xk+1−xk⟩+M2/2∥xk+1−xk∥2−U(x∗).
Recall the definition of the Lyapunov function
E(xk+1,vk+1) =∥xk+1∥2+∥xk+1+ 2vk+1/γ∥2+ 8u(U(xk+1)−U(x∗))/γ2.
For the first two terms we have
∥xk+1∥2=∥xk∥2+ 2⟨xk,xk+1−xk⟩+∥xk+1−xk∥2
∥xk+1+ 2vk+1/γ∥2=∥xk+ 2vk/γ∥2+ 2⟨xk+ 2vk/γ,xk+1−xk+ 2(vk+1−vk)/γ⟩
+∥xk+1−xk+ 2(vk+1−vk)/γ∥2.
This implies the following:
E[E(xk+1,vk+1)]≤E[E(xk,vk)] + 4E[⟨xk,xk+1−xk⟩] +4
γE[⟨xk,vk+1−vk⟩] +4
γE(⟨vk,xk+1−xk⟩)
(78)
+8
γ2E[⟨vk,vk+1−vk⟩] +8u
γ2E/bracketleftig
⟨∇U(xk),xk+1−xk⟩+M/2∥xk+1−xk∥2/bracketrightig
+E/bracketleftig
∥xk+1−xk∥2/bracketrightig
+E/bracketleftig
∥xk+1−xk+ 2(vk+1−vk)/γ∥2/bracketrightig
.
By the update rule in (5), we know that
E[⟨xk,xk+1−xk⟩] =1−e−γη
γE[⟨xk,vk⟩] +u(γη+e−γη−1)
γ2E[⟨xk,˜g(xk)⟩],
E[⟨xk,vk+1−vk⟩] =−(1−e−γη)E[⟨xk,vk⟩]−u(1−e−γη)
γE[⟨xk,˜g(xk)⟩],
E[⟨vk,xk+1−xk⟩] =1−e−γη
γE/bracketleftig
∥vk∥2/bracketrightig
+u(γη+e−γη−1)
γ2E[⟨vk,˜g(xk)⟩],
E[⟨vk,vk+1−vk⟩] =−(1−e−γη)E/bracketleftig
∥vk∥2/bracketrightig
−u(1−e−γη)
γE[⟨vk,˜g(xk)⟩].
52Published in Transactions on Machine Learning Research (04/2024)
Plug into the (78) yields:
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−4u(2−γη−2e−γη)
γ2E[⟨xk,˜g(xk)⟩]−4(1−e−γη)
γ2E/bracketleftig
∥vk∥2/bracketrightig
+4u(γη+e−γη−1)
γ3E[⟨vk,˜g(xk)⟩] +8u(1−e−γη)
γ3E[⟨vk,∇U(xk)−˜g(xk)⟩]
+8u2(γη+e−γη−1)
γ4E[⟨∇U(xk),˜g(xk)⟩] +/parenleftbigg4Mu
γ2+ 3/parenrightbigg
E/bracketleftig
∥xk+1−xk∥2/bracketrightig
+8
γ2E/bracketleftig
∥vk+1−vk∥2/bracketrightig
. (79)
By Assumption 2, we know that ⟨xk,∇U(xk)⟩≥m2∥xk∥2−b. We then assume η≤1/(8γ)and use the
inequality−x≤e−x−1≤x2/2−xfor anyx≥0, it follows that
−4u(2−γη−2e−γη)
γ2E[⟨xk,˜g(xk)⟩]
=−4u(2−γη−2e−γη)
γ2(E[⟨xk,∇U(xk)⟩] +E[⟨xk,˜g(xk)−∇U(xk)⟩])
≤−4u(2−γη−2e−γη)
γ2/parenleftig
m2E/bracketleftig
∥xk∥2/bracketrightig
−b/parenrightig
+4u(2−γη−2e−γη)
γ2/parenleftbigg1
8E/bracketleftig
∥xk∥2/bracketrightig
+ 2E/bracketleftig
∥˜g(xk)−∇U(xk)∥2/bracketrightig/parenrightbigg
≤−3m2uη
γE/bracketleftig
∥xk∥2/bracketrightig
+4uηb
γ+8uη
γE/bracketleftig
∥˜g(xk)−∇U(xk)∥2/bracketrightig
,
where the first inequality is because of the Young’s inequaltiy and Assumption 1 and the last inequality is
based on the inequality that γη−(γη)2≤2−γη−2e−γη≤γη. Again by Young’s inequality and the update
rule in (5) we have:
E/bracketleftig
∥xk+1−xk∥2/bracketrightig
≤2η2E/bracketleftig
∥vk∥2/bracketrightig
+u2η4/2E/bracketleftig
∥˜g(xk)∥2/bracketrightig
+E/bracketleftig
∥ξx
k∥2/bracketrightig
E/bracketleftig
∥vk+1−vk∥2/bracketrightig
≤2γ2η2E/bracketleftig
∥vk∥2/bracketrightig
+ 2u2η2E/bracketleftig
∥˜g(xk)∥2/bracketrightig
+E/bracketleftig
∥ξv
k∥2/bracketrightig
.
It is easy to verify the fact that E/bracketleftig
∥ξv
k∥2/bracketrightig
≤2γudηandE/bracketleftig
∥ξx
k∥2/bracketrightig
≤2udη2. Thus,
E[E(xk+1,vk+1)]
≤E[E(xk,vk)]−3umη2
γE/bracketleftig
∥xk∥2/bracketrightig
−3(1−e−γη)−η2(8Mu+uγ+ 22γ2)
γ2E/bracketleftig
∥vk∥2/bracketrightig
+36u2η2+ 2γuη2+/parenleftbig
4Mu+ 3γ2/parenrightbig
η4
2γ2E/bracketleftig
∥˜g(xk)∥2/bracketrightig
+2u2η2
γ2E/bracketleftig
∥∇U(xk)∥2/bracketrightig
+8uη(γ2+ 2u)
γ3E/bracketleftig
∥∇U(xk)−˜g(xk)∥2/bracketrightig
+(8Mu+ 6γ2)udη2+ 4(4d+b)uγη
η2.
If we set
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d/bracerightigg
,
we can obtain the following,
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−3um2η
γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
+(20u+γ)uη2
γ2E/bracketleftig
∥˜g(xk)∥2/bracketrightig
+2u2η2
γ2E/bracketleftig
∥∇U(xk)∥2/bracketrightig
+8uη/parenleftbig
γ2+ 2u/parenrightbig
γ3E/bracketleftig
∥∇U(xk)−˜g(xk)∥2/bracketrightig
+16(d+b)uη
γ.(80)
53Published in Transactions on Machine Learning Research (04/2024)
Furthermore we can bound E/bracketleftig
∥˜g(xk)∥2/bracketrightig
by the following analysis:
E/bracketleftig
∥˜g(xk)∥2/bracketrightig
≤2E/bracketleftig
∥˜g(xk)−∇U(xk)∥2/bracketrightig
+ 2E/bracketleftig
∥∇U(xk)∥2/bracketrightig
≤2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+ 4M2E/bracketleftig
∥xk∥2/bracketrightig
+ 4G2,(81)
whereG2is the bound of the gradient at 0, i.e.∥∇U(0)∥2≤G2. Thus we can have:
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−3um2η
γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
+(21u+γ)4M2uη2
γ2E/bracketleftig
∥xk∥2/bracketrightig
(82)
+/parenleftigg
2(20u+γ)uη2
γ2+8uη/parenleftbig
γ2+ 2u/parenrightbig
γ3/parenrightigg/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
(83)
+(21u+γ)4uη2
γ2G2+16(d+b)uη
γ. (84)
If we set the stepsize
η≤min/braceleftbiggγm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ/bracerightbigg
,
then we have:
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−8um2η
3γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
+/parenleftigg
16uη/parenleftbig
γ2+ 2u/parenrightbig
γ3/parenrightigg/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+(21u+γ)4uη2
γ2G2+16(d+b)uη
γ.
Furthermore by Young’s inequality and Assumption 1, we can bound the Lyapunov function by the following:
E(x,v)≤5/2∥x∥2+12
γ2+2uM
γ2/parenleftig
3∥x∥2+ 6∥x∗∥2/parenrightig
.
Then ifγ2≤4Mu, we have
E(x,v)≤16uM
γ2∥x∥2+12
γ2∥v∥2+12uM
γ2∥x∗∥2. (85)
Thus,
E[E(xk+1,vk+1)]≤/parenleftig
1−γm2η
6M/parenrightig
E[E(xk,vk)] +/parenleftigg
16uη/parenleftbig
γ2+ 2u/parenrightbig
γ3/parenrightigg/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+(21u+γ)4uη2
γ2G2+16(d+b)uη
γ.
Finally we show that
sup
k≥0E[E(xk,vk)]≤E[E(x0,v0)] +6M
γm2η/parenleftigg
16uη/parenleftbig
γ2+ 2u/parenrightbig
γ3/parenrightigg/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+6M
γm2η(21u+γ)4uη2
γ2G2+6M
γm2η16(d+b)uη
γ
≤E[E(x0,v0)] +96u/parenleftbig
γ2+ 2u/parenrightbig
m2γ4/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+24(21u+γ)uM
m2γ3G2+96(d+b)uM
m2γ2
≤E+C0/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
, (86)
54Published in Transactions on Machine Learning Research (04/2024)
whereE=E[E(x0,v0)] +24(21u+γ)uM
m2γ3G2+96(d+b)uM
m2γ2andC0=96u(γ2+2u)
m2γ4. Moreover by the definition of
Laypunov function, we know E(x,v)≥max{∥x∥2,2∥v/γ∥2}. This further implies that
E/bracketleftig
∥xk∥2/bracketrightig
≤E+C0/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
E/bracketleftig
∥vk∥2/bracketrightig
≤γ2E/2 +γ2C0/2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
.
Combining with equation (81) we can bound E/bracketleftig
∥˜g(xk)∥2/bracketrightig
as:
E/bracketleftig
∥˜g(xk)∥2/bracketrightig
≤2/parenleftbigg
(M2+ 1)∆2d
4+σ2/parenrightbigg
+ 4M2E+ 4G2. (87)
F.4 Proof of Lemma 15
Proof.By the update rule in (1), we have:
E/bracketleftig
∥xk+1∥2/bracketrightig
=E/bracketleftig
∥xk−η˜g(xk)∥2/bracketrightig
+/radicalbig
8ηE[⟨xk−η˜g(xk),ξk+1⟩] + 2ηE/bracketleftig
∥ξk+1∥2/bracketrightig
=E/bracketleftig
∥xk−η˜g(xk)∥2/bracketrightig
+ 2ηd
=E/bracketleftig
∥xk−η∇U(xk)−η(˜g(xk)−∇U(QW(xk)))−η(∇U(QW(xk))−∇U(xk))∥2/bracketrightig
+ 2ηd
=E/bracketleftig
∥xk−η∇U(xk)−η(∇U(QW(xk))−∇U(xk))∥2/bracketrightig
+η2E/bracketleftig
∥˜g(xk)−∇U(QW(xk))∥2/bracketrightig
+ 2ηd
= (E[∥xk−η∇U(xk)∥] +ηE[∥∇U(QW(xk))−∇U(xk)∥])2+η2/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 2ηd.
We know the fact that:
E/bracketleftig
∥xk−η∇U(xk)∥2/bracketrightig
=E/bracketleftig
∥xk∥2/bracketrightig
−2ηE[⟨xk,∇U(xk)⟩] +η2E/bracketleftig
∥∇U(xk)∥2/bracketrightig
=E/bracketleftig
∥xk∥2/bracketrightig
+ 2η/parenleftig
b−m2E/bracketleftig
∥xk∥2/bracketrightig/parenrightig
+ 2η2/parenleftig
M2E/bracketleftig
∥xk∥2/bracketrightig
+G2/parenrightig
=/parenleftbig
1−2ηm2+ 2η2M2/parenrightbig
E/bracketleftig
∥xk∥2/bracketrightig
+ 2ηb+ 2η2G2.
For anyη∈/parenleftbig
0,1∧m2
2M2/parenrightbig
, if0<1−2ηm2+ 2η2M2<1and setc=ηm2−η2M2
1−2ηm+2η2M2, then we have:
E/bracketleftig
∥xk+1∥2/bracketrightig
≤(1 +c)E/bracketleftig
∥xk−η∇U(xk)∥2/bracketrightig
+/parenleftbigg
1 +1
c/parenrightbigg
η2E/bracketleftig
∥∇U(QW(xk))−∇U(xk)∥2/bracketrightig
+η2/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 2ηd
(88)
≤/parenleftbig
1−ηm2+η2M2/parenrightbig
E/bracketleftig
∥xk∥2/bracketrightig
+1−ηm2+η2M
ηm2−η2MM2η2∆2d
4+1−ηm2+η2M
1−2ηm2+ 2η2M2/parenleftbig
2ηb+ 2η2G2/parenrightbig
(89)
+η2/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 2ηd. (90)
55Published in Transactions on Machine Learning Research (04/2024)
For anyk>0we can bound the recursive equations as:
E/bracketleftig
∥xk∥2/bracketrightig
≤E/bracketleftig
∥x0∥2/bracketrightig
+1−ηm2+η2M2
η2(m2−ηM2)2M2η2∆2d
4+1−ηm2+η2M2
η(1−2ηm2+ 2η2M2)(m2−ηM2)/parenleftbig
2ηb+ 2η2G2/parenrightbig
+1
η(m2−ηM)/parenleftbigg
η2∆2d
4+ 2ηd/parenrightbigg
=E/bracketleftig
∥x0∥2/bracketrightig
+1−ηm2+η2M2
(m2−ηM2)2M2∆2d
4+1−ηm2+η2M2
(1−2ηm2+ 2η2M2) (m2−ηM2)/parenleftbig
2b+ 2ηG2/parenrightbig
+1
m2−ηM2/parenleftbigg
η∆2d
4+ησ2+ 2d/parenrightbigg
≤E/bracketleftig
∥x0∥2/bracketrightig
+2M2
m2∆2d
4+2
m2/parenleftbig
2b+ 2ηG2/parenrightbig
+2
m2/parenleftbigg
η∆2d
4+ησ2+ 2d/parenrightbigg
.
Now if we letE=E/bracketleftig
∥x0∥2/bracketrightig
+M
m2/parenleftbig
2b+ 2ηG2+ 2d/parenrightbig
, then we can write:
E/bracketleftig
∥xk∥2/bracketrightig
≤E+2/parenleftbig
M2+ 1/parenrightbig
m2∆2d
4+2σ2
m2.
F.5 Proof of Lemma 11
Proof.From the same analysis in (80), if we set
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d/bracerightigg
,
we can obtain the following,
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−3um2η
γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
+(20u+γ)uη2
γ2E/bracketleftig/vextenddouble/vextenddoubleQG(∇˜U(xk))/vextenddouble/vextenddouble2/bracketrightig
+2u2η2
γ2E/bracketleftig
∥∇U(xk)∥2/bracketrightig
+8uη/parenleftbig
γ2+ 2u/parenrightbig
γ3E/bracketleftig/vextenddouble/vextenddouble∇U(xk)−QG(∇˜U(xk))/vextenddouble/vextenddouble2/bracketrightig
+16(d+b)uη
γ.
(91)
By assumption 1, we can bound E/bracketleftig/vextenddouble/vextenddoubleQG(∇˜U(xk))/vextenddouble/vextenddouble2/bracketrightig
by the following,
E/bracketleftig
∥QG(∇U(xk))∥2/bracketrightig
=E/bracketleftig/vextenddouble/vextenddoubleQG(∇˜U(xk))−∇U(xk) +∇U(xk)−∇U(0) +∇U(0)/vextenddouble/vextenddouble2/bracketrightig
≤E/bracketleftig/vextenddouble/vextenddoubleQG(∇˜U(xk))−∇U(xk)/vextenddouble/vextenddouble2/bracketrightig
+ 2E/bracketleftig
∥∇U(xk)−∇U(0)∥2/bracketrightig
+ 2E/bracketleftig
∥∇U(0)∥2/bracketrightig
≤/parenleftbigg∆2d
4+σ2/parenrightbigg
+ 2M2E/bracketleftig
∥xk∥2/bracketrightig
+ 2G2.
56Published in Transactions on Machine Learning Research (04/2024)
Plugging this bound into equation (91), we can have:
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−3um2η
γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
+2(20u+γ)uη2M2
γ2E/bracketleftig
∥xk∥2/bracketrightig
+(20u+γ)uη2
γ2/parenleftbigg∆2d
4+σ2+ 2G2/parenrightbigg
+2u2η2
γ2/parenleftig
2M2E/bracketleftig
∥xk∥2/bracketrightig
+ 2G2/parenrightig
+8uη/parenleftbig
γ2+ 2u/parenrightbig
γ3/parenleftbigg∆2d
4+σ2/parenrightbigg
+16 (d+b)uη
γ
≤E[E(xk,vk)]−3um2η
γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
+2(22u+γ)uη2M2
γ2E/bracketleftig
∥xk∥2/bracketrightig
+(20u+γ)γuη2+ 8/parenleftbig
γ2+ 2u/parenrightbig
uη
γ3/parenleftbigg∆2d
4+σ2/parenrightbigg
+2(22u+γ)uη2M2
γ2G2+16 (d+b)uη
γ
≤E[E(xk,vk)]−3um2η
γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
+2(22u+γ)uη2M2
γ2E/bracketleftig
∥xk∥2/bracketrightig
+/parenleftbig
36u+ 9γ2/parenrightbig
uη
γ3/parenleftbigg∆2d
4+σ2/parenrightbigg
+2(22u+γ)uη2M2
γ2G2+16 (d+b)uη
γ.
If we set the step size η≤γm2
6(22u+γ)M2, we can have:
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−8um2η
3γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
(92)
+/parenleftbig
36u+ 9γ2/parenrightbig
uη
γ3/parenleftbigg∆2d
4+σ2/parenrightbigg
+2(22u+γ)uη2M2
γ2G2+16 (d+b)uη
γ.(93)
Again from the same analysis in (85), if γ2≤4Mu, we have
E(x,v)≤16uM
γ2∥x∥2+12
γ2∥v∥2+12uM
γ2∥x∗∥2.
Thus,
E[E(xk+1,vk+1)]≤/parenleftig
1−γm2η
6M/parenrightig
E[E(xk,vk)] +/parenleftbig
36u+ 9γ2/parenrightbig
uη
γ3/parenleftbigg∆2d
4+σ2/parenrightbigg
+2(22u+γ)uη2M2
γ2G2+16 (d+b)uη
γ.
Finally, we show that for any k>0,
E[E(xk,vk)]≤E[E(x0,v0)] +6M
γm2η/parenleftbig
36u+ 9γ2/parenrightbig
uη
γ3/parenleftbigg∆2d
4+σ2/parenrightbigg
+6M
γm2η2(22u+γ)uη2M2
γ2G2+6M
γm2η16 (d+b)uη
γ
≤E[E(x0,v0)] +54/parenleftbig
4u+γ2/parenrightbig
u
m2γ4/parenleftbigg∆2d
4+σ2/parenrightbigg
+12(22u+γ)uM3
m2γ3G2+96 (d+b)uM
m2γ2
=:E+C∆2d.
Finally by the fact that E/bracketleftig
∥xk∥2/bracketrightig
≤E[E(xk,vk)]andE/bracketleftig
∥vk∥2/bracketrightig
≤γ2E[E(xk,vk)]/2we can get our claim
in Lemma 11.
57Published in Transactions on Machine Learning Research (04/2024)
(a) (b) (c)
Figure 8: Train NLL of low-precision SGHMC on logistic model with MNIST in terms of different numbers
of fractional bits. (a): Methods with Full-precision gradient accumulators. (b): Methods with Low-precision
gradients accumulators. (c): Variance corrected quantization.
(a) (b) (c)
Figure 9: Train NLL of low-precision SGHMC on MLP with MNIST in terms of different numbers of
fractional bits. (a): Methods with full-precision gradient accumulators. (b): Methods with low-precision
gradient accumulators. (c): Variance corrected quantization.
G Additional experiment results
In this section, we provide additional experiment results.
G.1 Logistic model
In this section, we present the low-precision SGHMC with logistic models on the MNIST dataset. The results
are shown in Figure 8. We can see that SGHMCLP-F is robust to the quantization error, even though only
2 bits are used to represent the fractional part the SGHMCLP-F can converge to a good point.
G.2 Multi-layer perception
We present the low-precision SGHMC with MLP on MNIST dataset in Figure 9. We observe similar results
as the low-precision SGHMC with the logistic model.
H Generalization of Theorem 4 under Relaxed Variance Assumption 7
All theorems presented in the paper assume bounded variance for the stochastic variance (i.e., Assumption
3). Another more flexible yet commonly used variance assumption (e.g., Raginsky et al., 2017; Gao et al.,
58Published in Transactions on Machine Learning Research (04/2024)
2022) allows the variance scales with ∥x∥2. In the rest of this supplementary material (Sections G-O), we
generalizeallthetheoremsinthemaintexttosuchavarianceassumption, i.e. Assumption7. Notethatmost
of the arguments used in the proofs under Assumption 3 still hold, necessitating only slight modifications.
Hence for the sake of readability, we only present the key changes due to the differences in the variance
assumptions.
Assumption 7 (Bouned Variance) .There exists a constant ˜B≥0, such that/vextenddouble/vextenddouble/vextenddouble/tildewidest∇U(0)/vextenddouble/vextenddouble/vextenddouble2
≤˜B. And there
exists a constant δ∈[0,1)such that:
E/vextenddouble/vextenddouble/vextenddouble/tildewidest∇U(x)−∇U(x)/vextenddouble/vextenddouble/vextenddouble2
≤2δ/parenleftig
M2∥x∥2+˜B2/parenrightig
,for any x∈Rd.
Let Assumption 7 be true, we can then derive the following lemma.
Lemma 16. For any x∈Rd, the random noise ξof the low-precision gradients defined in (40)satisfies:
∥Eξ∥2≤M2∆2d
4, (94)
E[∥ξ∥2]≤/parenleftbig
2M2+ 1/parenrightbig∆2d
4+ 2δM2E∥x∥2+ 2δ˜B2. (95)
Now, we are ready to present the Theorem 4 after revision.
Theorem 17. Suppose Assumptions 1, 4, and 7 hold and the minimum satisfies ∥x∗∥2<D2. Furthermore,
letp∗denote the target distribution of xandv. Given any sufficiently small ϵ, if we set the step size to be
η= min/braceleftigg
ϵκ−1
1/radicalbig
479232/5(d/m 1+D2),ϵ2
72/parenleftbig
20u2δM2κ1(d/m 1+D2) + 5u2/parenleftbig
(2M2+ 1)∆2d
4/parenrightbig
+δ˜B2/parenrightbig,/radicalbigg
e1/(4κ1)−1
10uδM2/bracerightigg
,
then afterKsteps starting with initial points x0=v0= 0, the output (xK,vK)of the SGHMCLP-F in (5)
satisfies
W2(p(xK,vK),p∗)≤˜O(ϵ+ ∆),
for someKsatisfying
K≤κ1
ηlog
36/parenleftig
d
m1+D2/parenrightig
ϵ
=˜O/parenleftig√
δϵ−2log/parenleftbig
ϵ−1/parenrightbig
∆2/parenrightig
.
Proof.For strongly log-concave target distributions, the equation (77) in Lemma 10 needs to be updated as
the following,
59Published in Transactions on Machine Learning Research (04/2024)
W2
2(/hatwideΦηq0,q∗)≤/parenleftig
W2(/tildewideΦηq0,q∗) +√
5/2uη√
dM∆/parenrightig2
+ 5u2η2/parenleftbigg
(2M2+ 1)∆2d
4+δM2Eq0∥x∥2+δδ˜B2/parenrightbigg
≤/parenleftig
W2(/tildewideΦηq0,q∗) +√
5/2uη√
dM∆/parenrightig2
+ 10u2η2δM2Eζ∥x−x′∥2+ 10u2η2δM2Ep∗∥x∥2
+ 5u2η2/parenleftbigg
(2M2+ 1)∆2d
4+δδ˜B2/parenrightbigg
(96)
=/parenleftig
W2(/tildewideΦηq0,q∗) +√
5/2uη√
dM∆/parenrightig2
+ 10u2η2δM2W2
2(/tildewideΦηq0,q∗) + 10u2η2δM2Ep∗∥x∥2
+ 5u2η2/parenleftbigg
(2M2+ 1)∆2d
4+δδ˜B2/parenrightbigg
(97)
≤/parenleftig/parenleftbig
1 + 10u2η2δM2/parenrightbig
W2(/tildewideΦηq0,q∗) +√
5/2uη√
dM∆/parenrightig2
+ 10u2η2δM2Ep∗∥x∥2
+ 5u2η2/parenleftbigg
(2M2+ 1)∆2d
4+δ˜B2/parenrightbigg
.
(98)
In the second inequality, ζis an optimal coupling between q0andq∗.x/x′are from the distributions q0/q∗.
Then we choose η≤/radicalig
e1/(4κ1)−1
10uδM2, and following the same argument for deriving equation (46), we can have:
W2
2(/hatwideΦηqi,q∗)≤/parenleftigg
e−η/4κ1W2(qi,q∗) +η2/radicalbigg
8EK
5+√
5/2uη√
dM∆/parenrightigg2
+ 10u2η2δM2Ep∗∥x∥2(99)
+ 5u2η2/parenleftbigg
(2M2+ 1)∆2d
4+δM2B2/parenrightbigg
. (100)
Moreover, we know the fact that
Ep∗∥x∥2=Ep∗∥x−x0∥2(101)
≤2E∥x−x∗∥2+ 2∥x∗−x0∥2(102)
≤2d
m+ 2D2(103)
Thus plug it into the equation (99), we can have
W2(/hatwideΦηqi,q∗)≤/parenleftigg
e−η/4κ1W2(qi,q∗) +η2/radicalbigg
8EK
5+√
5/2uη√
dM∆/parenrightigg2
+ 10u2η2δM2/parenleftbigg2d
m+ 2D2/parenrightbigg
(104)
+ 5u2η2/parenleftbigg
(2M2+ 1)∆2d
4+δ˜B2/parenrightbigg
. (105)
Finally, by invoking Lemma 7 Dalalyan & Karagulyan (2019), we can have
W2(qK,q∗)≤e−Kη/4κ1W2
2(q0,q∗) +η2/radicalig
8EK
5+uηM ∆√
5d
2
1−e−η/2κ1(106)
+10u2η2δM2/parenleftbig2d
m+ 2D2/parenrightbig
+ 5u2η2/parenleftig
(2M2+ 1)∆2d
4+δ˜B2/parenrightig
η2/radicalig
8EK
5+uηM ∆√
5d
2+√
1−e−η/2κ1/radicalig
10u2η2δM2/parenleftbig2d
m+ 2D2/parenrightbig
+ 5u2η2/parenleftbig
(2M2+ 1)∆2d
4+δ˜B2/parenrightbig.
(107)
60Published in Transactions on Machine Learning Research (04/2024)
Thus for some Ksatisfied
K≤2κ1
ηlog/parenleftbigg
36/parenleftbiggd
m1+D2/parenrightbigg/parenrightbigg
,
we can get the following
W2(pK,p∗)≤ϵ+16κ1uM∆√
5d
2. (108)
I Generalization of Theorem 5 under Relaxed Variance Assumption 7
Now, we are ready to present the Theorem 5 after revision.
Theorem 18. Suppose Assumptions 1, 4, and 7 hold and the minimum satisfies ∥x∗∥2<D2. Furthermore,
letp∗denote the target distribution of xandv. Given any sufficiently small ϵ, if we set the step size to be
η= min/braceleftigg
ϵκ−1
1/radicalbig
663552/5(d/m 1+D2),ϵ2
144/parenleftbig
20u2δM2κ1(d/m 1+D2) + 5u2/parenleftbig
(2M2+ 1)∆2d
4/parenrightbig
+δ˜B2/parenrightbig,/radicalbigg
e1/(4κ1)−1
10uδM2/bracerightigg
,
then afterKsteps starting with initial points x0=v0= 0, the output (xK,vK)of the SGHMCLP-L in (5)
satisfies
W2(p(xK,vK),p∗)≤˜O/parenleftbigg
ϵ+∆
ϵ/parenrightbigg
,
for someKsatisfying
K≤κ1
ηlog
36/parenleftig
d
m1+D2/parenrightig
ϵ
=˜O/parenleftig√
δϵ−2log/parenleftbig
ϵ−1/parenrightbig
∆2/parenrightig
.
Proof.Now we revise the proof of Theorem 5. The revision is similar to the revision for Theorem 5.
By similar argument in (98), equation (52) need to be changed as the following:
W2
2/parenleftig
ˆΦηq0,q∗/parenrightig
≤W2
2(/tildewideΦηq0,q∗) + 5u2η2/parenleftbigg∆2d
4+δM2Eqi∥x∥2+˜B2/parenrightbigg
+ 2u2(A+B) (109)
≤W2
2(/tildewideΦηq0,q∗) + 10u2η2δM2Eζ∥x−x′∥2+ 10u2η2δM2Ep∗∥x∥2(110)
+ 5u2η2˜B2+ 5u2η2∆2d
4+ 2u2(A+B) (111)
≤/parenleftbig
1 + 10u2η2δM2/parenrightbig
W2
2(/tildewideΦηq0,q∗) + 10u2η2δM2/parenleftbigg2d
m+D2/parenrightbigg
+ 5u2η2˜B2+ 5u2η2∆2d
4+ 2u2(A+B).
(112)
Then we choose η≤/radicalig
e1/(4κ1)−1
10uδM2, and following the same argument for (52), we can have
W2
2/parenleftig
ˆΦηqi,q∗/parenrightig
≤/parenleftigg
e−η/4κ1W2(qi,q∗) +η2/radicalbigg
8EK
5/parenrightigg2
+ 10u2η2δM2/parenleftbigg2d
m+D2/parenrightbigg
(113)
+ 5u2η2˜B2+ 5u2η2∆2d
4+ 2u2(A+B). (114)
Then the remaining analysis follows the proof in the original Theorem 5.
61Published in Transactions on Machine Learning Research (04/2024)
J Generalization of Theorem 6 under Relaxed Variance Assumption 7
Theorem 19. Let Assumption 1, 4 and 7 hold and the minimum satisfies ∥x∗∥2<D2. Furthermore, let p∗
denote the target distribution of vandx. Given any sufficiently small ϵ, if we set the step size ηto be
η= min

ϵκ−1
1/radicalbigg
663552/5/parenleftig
d
m1+D2/parenrightig,ϵ2
144/parenleftbig
20u2δM2κ1(d/m 1+D2) + 5u2/parenleftbig
(2M2+ 1)∆2d
4/parenrightbig
+δ˜B2/parenrightbig,/radicalbigg
e1/(4κ1)−1
10uδM2

,
then afterKsteps starting with initial points x0=v0= 0, the output (xK,vK)of the SGHMCLP-L in (6)
satisfies
W2(p(xK,vK),p∗) =˜O/parenleftig
ϵ+√
∆/parenrightig
, (115)
for someKsuch that
K≤κ1
ηlog
36/parenleftig
d
m1+D2/parenrightig
ϵ
=˜O/parenleftig
ϵ−2log/parenleftig√
δϵ−1/parenrightig
∆2/parenrightig
.
The revision closely mirrors the revised proof of Theorem 5, based on the assumption that Assumption 3
holds.
The only equation we need to revise is (60). Given the similar argument in (98), we can have
W2(pK,p∗)≤4e−Kη/4κ1W2(q0,q∗) +4η2/radicalig
8EK
5
1−e−η/4κ1(116)
+40u2η2δM2/parenleftig
2d
m1+ 2D2/parenrightig
+ 20u2η2/parenleftig
∆2d
4+δ˜B2/parenrightig
+ 8u2η(γA+B)
η2/radicalig
8EK
5+√
1−e−η/κ 1/radicalbigg
10u2η2δM2/parenleftig
2d
m1+ 2D2/parenrightig
+ 5u2η2/parenleftbig∆2d
4+δ˜B2/parenrightbig
+ 2u2η(γA+B).
(117)
K Generalization of Theorem 1 under Relaxed Variance Assumption 7
This section updates Theorem 1’s proof, replacing Assumption 3 with 7, and introduces the revised theorem.
Theorem 20. Assuming 1, 2, 7, and 8 hold. Let p∗denote the target distribution of (x,v). Ifγ2≤4Mu
and setting the step size η=˜O/parenleftig
µ∗ϵ2
log(1/ϵ)/parenrightig
satisfying
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d,1
8γ,γm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ,m2γ2
12(γ2+ 2u)M2/bracerightigg
,
then afterKsteps starting at the initial point x0=v0= 0, the output (xK,vK)of SGHMCLP-F in (5)
satisfies
W2(p(xK,vK),p∗)≤˜O/parenleftigg
ϵ+/tildewideA/radicaligg
log/parenleftbigg1
ϵ/parenrightbigg/parenrightigg
,
for someKsatisfying
K=˜O/parenleftbigg1
ϵ2µ∗2log2/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
,
where constants are defined as: /tildewideA= max/braceleftig√
∆2d+δ1/4,4√
∆2d+δ1/4/bracerightig
, and constant 1/µ∗=exp(O(d))
denotes the contraction rate of underdamped Langevin dynamics (10).
62Published in Transactions on Machine Learning Research (04/2024)
Proof.We first need to introduce a further assumption on the variance parameter δas the following assump-
tion.
Assumption 8. Given Assumption 7, we further assume the δsatisfies the following condition:
δ≤min/braceleftbiggγm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ,m2γ
3(20u+γ),m2γ2
12(γ2+ 2u)M2/bracerightbigg
.
We need to revise the (84) as the following:
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−3um2η
γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
+(21u+γ)4M2uη2
γ2E/bracketleftig
∥xk∥2/bracketrightig
(118)
+/parenleftigg
2(20u+γ)uη2
γ2+8uη/parenleftbig
γ2+ 2u/parenrightbig
γ3/parenrightigg/parenleftbigg
(M2+ 1)∆2d
4+δM2E∥xk∥2+δ˜B2/parenrightbigg
(119)
+(21u+γ)4uη2
γ2G2+16(d+b)uη
γ. (120)
If we choose ηsatisfy the following condition
δ≤η≤min/braceleftbiggγm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ,m2γ
3(20u+γ),m2γ2
12(γ2+ 2u)M2/bracerightbigg
, (121)
then we can have
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−4um2η
3γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
(122)
+/parenleftigg
16uη/parenleftbig
γ2+ 2u/parenrightbig
γ3/parenrightigg/parenleftbigg
(M2+ 1)∆2d
4+δ˜B2/parenrightbigg
(123)
+(21u+γ)4uη2
γ2G2+16(d+b)uη
γ. (124)
The remainder of the analysis is unchanged.
L Generalization of Theorem 2 under Relaxed Variance Assumption 7
This section updates Theorem 2’s proof, replacing Assumption 3 with 7, and introduces the revised theorem.
Theorem 21. Assuming 1, 2, 7 and 8 hold. Let p∗denote the target distribution of (x,v). Ifγ2≤4Mu
and setting the step size η=˜O/parenleftig
µ∗ϵ2
log(1/ϵ)/parenrightig
satisfying
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d,1
8γ,γm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ,m2γ2
12(γ2+ 2u)M2/bracerightigg
,
then afterKsteps starting at the initial point x0=v0= 0, the output (xK,vK)of SGHMCLP-L in (6)
satisfies
W2(p(xK,vK),p∗) =˜O/parenleftigg
ϵ+δ1/4/radicaligg
log/parenleftbigg1
ϵ/parenrightbigg
+log3/2/parenleftbig1
ϵ/parenrightbig
ϵ2√
∆/parenrightigg
, (125)
for someKsatisfying
K=˜O/parenleftbigg1
ϵ2µ∗2log2/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
63Published in Transactions on Machine Learning Research (04/2024)
Proof.We need to revise the equation (93) as the following:
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−8um2η
3γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
(126)
+/parenleftbig
36u+ 9γ2/parenrightbig
uη
γ3/parenleftbigg∆2d
4+δM2E∥xk∥2+δ˜B2/parenrightbigg
+2(22u+γ)uη2M2
γ2G2+16 (d+b)uη
γ.
(127)
If we further choose η≤4γ2m2
3(36u+9γ2)M2and assume δ≤η, then we can have
E[E(xk+1,vk+1)]≤E[E(xk,vk)]−4um2η
3γE/bracketleftig
∥xk∥2/bracketrightig
−2η
γE/bracketleftig
∥vk∥2/bracketrightig
(128)
+/parenleftbig
36u+ 9γ2/parenrightbig
uη
γ3/parenleftbigg∆2d
4+δ˜B2/parenrightbigg
+2(22u+γ)uη2M2
γ2G2+16 (d+b)uη
γ.(129)
M Generalization of Theorem 3 under Relaxed Variance Assumption 7
In this section, we present Theorem 3 after revision.
Theorem 22. Assuming 1, 2, 7 and 8 hold. Let p∗denote the target distribution of (x,v). Ifγ2≤4Mu
and setting the step size η=˜O/parenleftig
µ∗ϵ2
log(1/ϵ)/parenrightig
satisfying
η≤min/braceleftigg
γ
4 (8Mu+uγ+ 22γ2),/radicaligg
4u2
4Mu+ 3γ2,6γbu
(4Mu+ 3γ2)d,1
8γ,γm2
12(21u+γ)M2,8(γ2+ 2u)
(20u+γ)γ,m2γ2
12(γ2+ 2u)M2/bracerightigg
,
then afterKsteps starting at the initial point x0=v0= 0, the output (xK,vK)of SGHMCLP-L in (6)
satisfies
W2(p(xK,vK),p∗) =˜O/parenleftigg
ϵ+δ1/4/radicaligg
log/parenleftbigg1
ϵ/parenrightbigg
+log/parenleftbig1
ϵ/parenrightbig
ϵ√
∆/parenrightigg
, (130)
for someKsatisfying
K=˜O/parenleftbigg1
ϵ2µ∗2log2/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
The revision of Theorem 3 is the same as the revision for Theorem 2.
N Generalization of Theorem 7 under Relaxed Variance Assumption 7
In this proof, we revise the proof of Theorem 7 if the Assumption 3 is replaced by Assumption 7.
Theorem 23. Suppose Assumptions 1, 2, and 7 hold. Let p∗denote the target distribution of x,/tildewideAhave
the same definition in Theorem 1, and 1/λ∗=exp(O(d))be the concentration number of (13). AfterK
steps starting with initial point x0= 0, if we set the stepsize to be η=˜O/parenleftbigg/parenleftig
ϵ
log(1/ϵ)/parenrightig4/parenrightbigg
. The output xKof
SGLDLP-F in (1)satisfies
W2(p(xK),p∗)≤˜O/parenleftbigg
ϵ+/parenleftig√
∆ +δ1/4/parenrightig
log/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
, (131)
for someKsatisfied
K=˜O/parenleftbigg1
ϵ4λ∗log5/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
64Published in Transactions on Machine Learning Research (04/2024)
Proof.We need to revise the equation (90) as the following:
E/bracketleftig
∥xk+1∥2/bracketrightig
≤/parenleftbig
1−ηm2+ 2η2M2/parenrightbig
E/bracketleftig
∥xk∥2/bracketrightig
+1−ηm2+η2M
ηm2−η2MM2η2∆2d
4+1−ηm2+η2M
1−2ηm2+ 2η2M2/parenleftbig
2ηb+ 2η2G2/parenrightbig
(132)
+η2/parenleftbigg∆2d
4+δ˜B2/parenrightbigg
+ 2ηd. (133)
Then we can have the following:
E/bracketleftig
∥xk∥2/bracketrightig
≤E+2/parenleftbig
M2+ 1/parenrightbig
m2∆2d
4+2δ˜B2
m2.
Moreover, the equation (67) is also needed to revised as the accordingly:
E/bracketleftig
∥xs−xk∥2/bracketrightig
≤3η2(ME[∥xk∥] +G)2+ 3η2/parenleftbigg
(M2+ 1)∆2d
4+δM2E∥xk∥2+δ˜B2/parenrightbigg
+ 6ηd
≤3η2(2ME[∥xk∥] +G)2+ 3η2/parenleftbigg
(M2+ 1)∆2d
4+δ˜B2/parenrightbigg
+ 6ηd. (134)
The remaining analysis should be the same as the proof of Theorem 7.
O Generalization of Theorem 8 under Relaxed Variance Assumption 7
In this proof, we revise the proof of Theorem 8 if the Assumption 3 is replaced by Assumption 7.
Theorem 24. Let Assumptions 1, 2 and 7 hold. Let p∗denote the target distribution of xand1/λ∗=
exp(O(d))be the concentration number of (13). If we set the step size to be η=˜O/parenleftbigg/parenleftig
ϵ
log(1/ϵ)/parenrightig4/parenrightbigg
, afterK
steps starting at the initial point x0= 0the output xKof the SGLDLP-L in (2)satisfies
W2(p(xK),p∗) =˜O/parenleftigg
ϵ+δ1/4log/parenleftbigg1
ϵ/parenrightbigg
+log5/parenleftbig1
ϵ/parenrightbig
ϵ4√
∆/parenrightigg
, (135)
for someKsatisfied
K=˜O/parenleftbigg1
ϵ4λ∗log5/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
We need to revise the (71)
E/bracketleftig
∥xk+1∥2/bracketrightig
≤/parenleftbig
1−2ηm2+ 2η2M2/parenrightbig
E/bracketleftig
∥xk∥2/bracketrightig
+ 2ηb+ 2η2G2+η2∆2d
4+η2/parenleftig
2δM2E∥xk∥2+ 2δ˜B2/parenrightig
(136)
+E/bracketleftig
∥αk∥2/bracketrightig
+ 2ηd. (137)
≤/parenleftbig
1−2ηm2+ 4η2M2/parenrightbig
E/bracketleftig
∥xk∥2/bracketrightig
+ 2ηb+ 2η2G2+η2∆2d
4+η2δ˜B2+E/bracketleftig
∥αk∥2/bracketrightig
+ 2ηd.
(138)
65Published in Transactions on Machine Learning Research (04/2024)
P Generalization of Theorem 9 under Relaxed Variance Assumption 7
In this proof, we update Theorem 9 if the Assumption 3 is replaced by Assumption 7.
Theorem 25. Let Assumptions 1, 2 and 7 hold. Let p∗denote the target distribution of xand1/λ∗=
exp(O(d))be the concentration number of (13). If we set the step size to be η=˜O/parenleftbigg/parenleftig
ϵ
log(1/ϵ)/parenrightig4/parenrightbigg
, afterK
steps starting at the initial point x0= 0the output xKof the SGLDLP-L in (2)satisfies
W2(p(xK),p∗) =˜O/parenleftigg
ϵ+δ1/4log/parenleftbigg1
ϵ/parenrightbigg
+log3/parenleftbig1
ϵ/parenrightbig
ϵ2√
∆/parenrightigg
, (139)
for someKsatisfied
K=˜O/parenleftbigg1
ϵ4λ∗log5/parenleftbigg1
ϵ/parenrightbigg/parenrightbigg
.
The key revision is the same as the Theorem 8.
66