Under review as submission to TMLR
Accelerating Fair Federated Learning:
Adaptive Federated Adam
Anonymous authors
Paper under double-blind review
Abstract
Federated learning is a distributed and privacy-preserving approach to train a statistical
model collaboratively from decentralized data of different parties. However, when datasets
of participants are not independent and identically distributed, models trained by naive fed-
eratedalgorithmsmaybebiasedtowardscertainparticipants, andmodelperformanceacross
participants is non-uniform. This is known as the fairness problem in federated learning.
In this paper, we formulate fairness-controlled federated learning as a dynamical multi-
objective optimization problem to ensure fair performance across all participants. To solve
the problem efficiently, we study the convergence and bias of Adamas the server optimizer
in federated learning, and propose Adaptive Federated Adam ( AdaFedAdam ) to accelerate
fair federated learning with alleviated bias. We validated the effectiveness, Pareto optimal-
ity and robustness of AdaFedAdam with numerical experiments and show that AdaFedAdam
outperforms existing algorithms, providing better convergence and fairness properties of the
federated scheme.
1 Introduction
Federated Learning (FL), first proposed by McMahan et al. (2017), is an emerging collaborative learning
technique enabling multiple parties to train a joint machine learning model with input privacy being pre-
served. By iteratively aggregating local model updates done by participating clients using their local, private
data, a joint global model is obtained. The promise of federated learning is to achieve superior performance
compared to models trained in isolation by each participant. Compared with traditional distributed machine
learning, FL works with larger local updates and seeks to minimize communication cost while keeping the
data of participants local and private. With increasing concerns about data security and privacy protection,
federated learning has attracted much research interest (Li et al., 2020b; Kairouz et al., 2021) and has been
proven to work effectively in various application domains (Li et al., 2020a; Xu et al., 2021).
When datasets at the client sites are not independent and identically distributed (IID), the standard al-
gorithm for federated learning, FedAvg, can struggle to achieve good model performance, with an increase
of communication rounds needed for convergence (Li et al., 2019b; Zhu et al., 2021). Moreover, the global
model trained with heterogeneous data can be biased towards some of the participants, while performing
poorly for others (Mohri et al., 2019). This is known as fairness problem in federated learning. The term
fairness encompasses various interpretations such as group fairness and performance fairness in the field of
machine learning(Mehrabi et al., 2021). In this paper, we specifically employ the term fairness to describe
disparities in model performance observed among participants in a federated learning process. There are
ways to improve fairness in federated learning, at the cost of model convergence (Mohri et al., 2019; Li et al.,
2019a; 2021; Hu et al., 2022). This study aims to contribute to enabling fair federated learning without
negatively impacting the convergence rate.
Acceleration techniques for federated learning aim at reducing the communication cost and improving con-
vergence. For instance, momentum-based and adaptive optimization methods such as AdaGrad,Adam, Mo-
mentum SGDhavebeenappliedtoacceleratethetrainingprocess(Wangetal.,2019;Karimireddyetal.,2020;
Reddi et al., 2020). However, default hyperparameters of adaptive optimizers tuned for centralized training
1Under review as submission to TMLR
do not tend to perform well in federated settings (Reddi et al., 2020). Furthermore, optimal hyperparameters
are not generalizable for federated learning, and hyperparameter optimization with e.g. grid search is needed
for each specific federated task, which is infeasible due to the expensive (sometimes unbounded) nature for
federated learning. Further research is needed to understand how to adapt optimizers for federated learning
with minimal hyperparameter selection.
In this study, to accelerate the training of fair federated learning, we formulate fairness-aware federated learn-
ingasadynamical multi-objective optimization problem (DMOO)problem. Byanalyzingtheconvergenceand
bias of federated Adam, we propose Adaptive Federated Adam ( AdaFedAdam ) to solve the formulated DMOO
problem efficiently. With experiments on standard benchmark datasets, we illustrate that AdaFedAdam al-
leviates model unfairness and accelerates federated training. In additional, AdaFedAdam is proved to be
robust against different levels of data and resource heterogeneity, which suggests that its performance on
fair federated learning can be expected in real-life use cases.
The remainder of the paper is structured as follows. Section 2 summarizes related work including different
acceleration techniques for federated training and the fairness problem in federated learning. Then fair
federated learning problem is formulated in Section 3 and Federated Adamis analyzed in Section 4. Section
5 introduces the design of AdaFedAdam and shows the convergence guarantee for AdaFedAdam . Experimental
setups and results are presented in Section 6. Finally, Section 7 concludes the paper and suggests future
research directions.
2 Related Work
In this section, we review recent techniques to accelerate federated training as well as studies of model
fairness in FL.
2.1 Acceleration Techniques for Federated Learning
Adaptive methods and momentum-based methods accelerate centralized training of neural networks over
vanilla SGD(Kingma & Ba, 2014; Zeiler, 2012). In the context of federated learning, Hsu et al. (2019) and
Wang et al. (2019) introduced first-order momentum to update the global model by treating local updates as
pseudo-gradients, showing the effectiveness of adaptive methods for federated learning. Further, Reddi et al.
(2020) demonstrated a two-level optimization framework FedOptfor federated optimization. On the local
level, clients optimize the local objective functions while local updates are aggregated as "pseudo-gradients"
to update the global model on the server level. By applying adaptive optimizers (e.g. Adam) as the server
optimizer, we obtain adaptive federated optimizers (e.g. FedAdam). It has been empirically validated that
adaptive federated optimizers are able to accelerate training with careful fine-tuning (Reddi et al., 2020).
Fine-tuning for server optimizers is challenging for the following reasons:
•Due to the inherent differences of federated and centralized training, default hyperparameters of
optimizerswhichworkwelloncentralizedtrainingdoesnotnecessarilyhavesatisfactoryperformance
in federated training.
•For adaptive optimizers, grid search needs to be performed to get multiple hyperparameters opti-
mized (Reddi et al. (2020)), which is prohibitively expensive considering the orchestration cost for
the entire federation.
•The optimal hyperparameters for server-side optimizers are not generalizable across different feder-
ated tasks, and fine-tuning must be done for each individual task.
It would greatly ease the use of server-side optimizers if the selection of hyperparameters were automatic.
The proposed method AdaFedAdam minimizes the efforts of fine-tuning by adapting default hyperparameters
ofAdamin centralized settings to federated training.
2Under review as submission to TMLR
2.2 Model Fairness
The concept of model unfairness describes the differences of model performance across participants (Mohri
et al., 2019). In a federated training process, model performances of clients may be not uniform when data
across participants are heterogeneous, and the global model can be biased towards some participants. To re-
duce the unfairness, Mohri et al. (2019) proposed the algorithm Agnostic Federated Learning , a minimax
optimization approach that only optimizes the single device with the worst performance. Inspired by fair
resource allocation, Li et al. (2019a) formulated fair federated learning as a fairness-controlled optimization
problem with α-fairness function (Mo & Walrand, 2000). An algorithm q-FedAvg was proposed to solve the
optimization problem, which dynamically adjust step sizes of local SGDby iteratively estimating Lipschitz
constants. More recently, Hu et al. (2022) interpreted federated learning as a multi-objective optimization
problem, and adapted Multi-Gradient Descent Algorithm to federated settings as FedMGDA+ to reduce the un-
fairness. Alternatively, Li et al. (2021) proposed Dittoto improve the performance fairness by personalizing
global models on client sites.
Unlike previous work that are based on FedAvgwith improved fairness at a cost of model convergence,
the here proposed approach formulates fair federated learning as a dynamic multi-objective function and
proposes AdaFedAdam to solve the formulated problem. Compared with other FedAvg-based algorithms for
fairness control, AdaFedAdam offer equivalent fairness guarantee with improved convergence properties.
3 Preliminaries & Problem Formulation
3.1 Standard Federated Learning
Considering the distributed optimization problem to minimize the global loss function F(x)acrossKclients
as follows:
min
x[F(x) :=K/summationdisplay
k=1pkFk(x)] (1)
where xdenotes the parameter set of function F,Fk(x)is the local objective function of client kw.r.t local
datasetDk, andpk:=|Dk|/summationtext
|D|denotes the relative sample size of Dkwith number of samples |Dk|inDk.
The data distribution on client kis denoted byDk.
3.2 Fair Federated Learning
WhenDkacrossclientsarenon-IID,thestandardformulationoffederatedlearningcansufferfromsignificant
fairness problem (Mohri et al., 2019) in addition to a potential loss of convergence. To improve the model
fairness, federated learning can be formulated with α-fairness function as α-fair Federated Learning (also
known asq-fair Federated Learning in Li et al. (2019a)) as follows:
min
x[F(x) :=K/summationdisplay
k=1pk
α+ 1Fα+1
k(x)] (2)
where notations is as in equation 1 with additional α≥0.
However, it is challenging to solve the problem with distributed first-order optimization. With only access to
gradients of local objective functions ∇tFk(x), the gradient of F(x)atxtand the update rule of distributed
SGDare given as follows:
∇F(xt) =K/summationdisplay
k=1pkFα
k(xt)∇Fk(xt) (3)
xt+1:=xt−η∇F(xt) (4)
3Under review as submission to TMLR
The gradient∇F(xt)has decreasing scales due to the factor Fα(xt). With the number of iterations t
increases, decreasing Fα(xt)scales∇F(xt)down drastically. With a fixed learning rate η, the update of SGD
−η∇F(xt)scales down correspondingly and thus, the convergence deteriorates. To improve the convergence,
Li et al. (2019a) proposes q-FedAvg to adjust learning rates adaptively to alleviate the problem. However,
the convergence of q-FedAvg is still slow since the scaling challenge is introduced by the problem formulation
intrinsically. To achieve a better convergence for first-order federated optimization methods with fairness
control, reformulating the problem is required.
3.3 Problem Formulation
In the field of multi-task learning, neural networks are designed to achieve multiple tasks at the same time
by summing multiple component objective functions up as a joint loss function. In a similar spirit to fair
federated learning, training multitask deep neural networks also requires keeping similar progress for all
component objectives. Inspired by Chen et al. (2018), we formulate fair federated learning as a dynamic
multi-objective optimization problem in the following form:
min
x[F(x,t) :=/summationtextK
k=1pkIα
k(t)Fk(x)
/summationtextK
k=1pkIα
k(t)] (5)
Here, the notations are the same as in Equation equation 1. Additionally, inverse training rate is defined as
Ik(t) :=Fk(xt)/Fk(x0)for clientkat roundt, to quantify its training progress. α≥0is a hyperparameter
to adjust the model fairness similar to αinα-fairness function. The problem reduces to the federated
optimization without fairness control if setting α= 0, and it restores the minimax approach for multi-
objective optimization (Mohri et al., 2019) if setting αa sufficiently large value.
Compared with α-Fair Federated Learning, the proposed formulation has equivalent fairness guarantee with-
out the problem of decreasing scales of gradients. With a global model x0initialized with random weights,
we assume that Fi(x0)≃Fj(x0)for∀i,j∈[K]. Then the gradient of F(x,t)atxtis given by:
∇F(xt,t)≃/summationtextK
k=1pkFα
k(xt)∇Fk(xt)
/summationtextK
k=1pkFα
k(xt)(6)
∝K/summationdisplay
k=1pkFα
k(xt)∇Fk(xt) (7)
The gradient F(xt,t)of the DMOP formulation is proportional to the gradient of the α-Fair Federated
Learning equation 2. Thus, with first-order optimization methods, the solution of the DMOP formulation is
also the solution of the α-fairness function, which has been proved to enjoy (p,α)-Proportional Fairness (Mo
& Walrand, 2000). Moreover, the DMOP formulation of Fair Federated Learning does not have the problem
of decreasing gradient scales in the α-fairness function, so that distributed first-order optimization methods
can be applied to solve the problem more efficiently.
4 Analysis of FedAdam
In this section, we analyze the performance of Adamas the server optimizer in federated learning. We first
study the effect of using accumulated updates as pseudo-gradients for Adamin centralized training. The bias
introduced by averaging accumulated local updates without normalization in FedAdam is then discussed.
4.1 From Adam toFedAdam
As the de facto optimizer for centralized deep learning, Adamprovides stable performance with little need
of fine-tuning. Adamprovides adaptive stepsize selection based on the initial stepsize ηfor each individual
4Under review as submission to TMLR
coordinate of model weights. The adaptivity of stepsizes can be understood as continuously establishing
trust regions based on estimations of the first- and second-order momentum(Kingma & Ba, 2014), which are
updated by exponential moving averages of gradient estimations and their squares with hyperparameters β1
andβ2respectively in each step.
The choice of hyperparameters in Adamcan be explained by the certainty of directions for model updates.
In centralized Adam, directions for updates are from gradient estimations ∇ζ∼DF(x)obtained from a batch
of dataζ. Generally the size of the batch |ζ|is relatively small, and the variance of ∇ζ∼DF(x)is large,
indicating low certainty of update directions. Thus, large β1andβ2(0.9and0.999by default) are set
to assign less weight for each gradient estimation when updating first- and second-order momentum. Low
certainty of update directions also only allows small trust regions to be constructed from small initial stepsize
η(default value: 0.001).
In federated learning, FedAdam is obtained if we apply Adamas the server optimizer. The size-weighted
average of clients’ local updates at round t,∆t, acts as the pseudo-gradient. Although empirical results have
shown that FedAdam outperforms the standard FedAvgwith careful fine-tuning in terms of average error
(Reddi et al., 2020), several problems exist in FedAdam. In the following subsections, we analyze the problem
of convergence loss of FedAdam and bias of the pseudo-gradients used for FedAdam.
4.2 Adam with Accumulated Updates
When data between clients are statistically homogeneous ( ∀k,i∈[K],Dk=Di), it has been proven in prior
work(Khaled et al., 2020; Stich, 2018) FedAvgconverges to the same optima as mini-batch SGD, from which
within a squared distance of O(N−1), whereNdenotes the number of local steps. Thus, the average of local
updates is an unbiased estimator of accumulated updates of multiple centralized SGDsteps. By applying Adam
as the server optimizer in IID cases, FedAdam shrinks as Adamwith gradient estimation given by accumulated
updates of NSGDsteps (N-AccAdam ). The Pseudo-codes of N-AccAdam is given in the Appendix B. We
prove that even in centralized settings, N-AccAdam has less convergence guarantee than standard Adamwith
same hyperparameters.
Theorem 1 (Convergence of N-AccAdam)Assume the L-smooth convex loss function F(x)has bounded
gradients∥∇∥∞≤Gfor all x∈Rd. Hyperparameters ϵ,β2andηinN-AccAdam are chosen with the
following conditions: η≤ϵ/2Land1−β2≤ϵ2/16G2. The accumulated update of NSGD updates at step
t∆xt:=−/summationtextN
n=1∆nxtis applied to Adam, where ∆nxtdenotes the local update after nSGD steps with
a fixed learning rate on model xt.SGDexhibits linear convergence to the neighbourhood of the optima with
constants (A,c). In the worst case, the algorithm has no convergence guarantee. In the best cases where
Rt=Nfor allt∈[T], the converge guarantee is given by:
1
TT/summationdisplay
t=1∥∇F(xt)∥2≤F(x1)−F(x∗)(√β2G+ϵ)
(Nc
1−(1−c)N−1
2)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
SηT(8)
whereRt:= min|∆t,i
∇t,i|fori∈[d]
The proof for Theorem 1 is deferred to Appendix A.1. In the best case where Rt=N(which is almost not
feasible),N-AccAdam gainsSa speedup compared with Adam. However, the computation cost of N-AccAdam
is linear to Nbut the speedup Sis sublinear with respect to N. Thus, with a fixed computation budget,
the convergence rate of N-AccAdam is slower than Adamwith the same hyperparameters. Compared with
gradient estimation by a small batch of data, accumulated updates of multiple SGDsteps have larger certainty
about directions of updates for the global model. To improve the convergence of N-AccAdam , it is possible
to construct larger trust regions with larger stepsize ηand smaller βvalues with accumulated updates.
5Under review as submission to TMLR
4.3 Bias of Pseudo-gradients for FedAdam
In federated settings, when data among clients are heterogeneous, the average of local updates weighted by
sizes of client datasets introduces bias toward a portion of clients. Moreover, the biased pseudo-gradients
lead to even lower convergence and increase the unfairness of FedAdam.
Wang et al. (2020) has proved that there exists objective inconsistency between the stationary point and
the global objective function, and biases are caused by different local SGDsteps taken by clients. They
propose FedNova to reduce the inconsistency by normalizing local updates with the number of local steps.
The convergence analysis of FedNova assumes that all local objective functions have the same L-smoothness,
which is also identical to the smoothness constant of the global objective function. However, in federated
learning with highly heterogeneous datasets, smoothness constants Lkof local objective functions Fk(x)are
very different across clients and from Lgof the global objective function. Although the assumption and proof
still holds if taking Lg:= max(Lk)for allk∈[K], we argue that the inconsistency still exists in FedNova if
only normalizing local updates with number of steps regardless of differences of Lk-smoothness constant of
local objectives.
In one communication round, with the same numbers of local SGDsteps and a fixed learning rate η, it is
likely to happen that while objectives with small L-constant are still slowly converging, local objectives with
largeL-constants have converged in a few steps and extra steps are ineffective. In such cases, normalizing
local updates with number of local SGDsteps implicitly over-weights updates from objectives with smaller
L-constants when computing pseudo-gradients. Further improvements are needed in the normalization of
local updates to de-bias the pseudo-gradients, accounting for both the different numbers of local steps and
theL-smoothness constants of local objectives.
5AdaFedAdam
To address the drawbacks mentioned above, we propose Adaptive FedAdam (AdaFedAdam ) to make better use
of accumulated local updates for fair federated learning (equation 5) with little efforts on fine-tuning.
5.1 Algorithm
Figure 1 is an illustration of AdaFedAdam . Intuitively, the proposed method AdaFedAdam operates as follows:
it initially estimates the certainty values of local updates obtained after local training, indicating the confi-
dence of each local update. Subsequently, using the normalized local updates and their associated certainty
values, it computes the pseudo-gradient along with its certainty. Adaptive adjustment of hyperparameters
is then performed based on the certainty of the pseudo-gradient. Finally, the pseudo-gradient is utilized to
update the global model, incorporating the locally trained information of both the update directions and
their associated certainties. The pseudo-code of the algorithm is presented as Algorithm 1 and Algorithm 2.
Compared with standard FedAdam, three extra steps are taken, as explained in details below:
Normalization of local updates: Due to different L-smoothness constants of local objectives and local
steps across participants, lengths of accumulated updates ∆kare not at uniform scales and normalization of
local updates is necessary as discussed in Section 4. Natural scales for local updates are the ℓ2-norms of local
gradients∥∇Fk(xt)∥2on clientk. By normalizing ∆kto the same ℓ2-norm of∥∇Fk(xt)∥2, a normalized
update Ukand a supportive factor η′
kare obtained. Intuitively, ∆kcan be interpreted as a single update
step following a "confident" update direction −Ukwith a large learning rate η′
kon the model xtprovided
by clientk. The certainty of the direction Ukis defined as Ck:= log(η′
k/ηk) + 1(ηkas the learning rate of
the local solver), and the larger value of Ckis, the greater update can be made following Uk.
Fairness control: Recall that Ikrepresents the inverse training rate and αis a predefined hyperparameter
for fairness control. Following the formulation of the loss function in fair federated learning in Section 3, the
pseudo-gradient gtof the global model xtis correspondingly the average of the normalized local updates
with adaptive weights Iα
k. The certainty of gtis given by the weighted average of local certainties Ckfor all
k∈[K].
6Under review as submission to TMLR
Algorithm 1 AdaFedAdam : Adaptive Federated Adam
Require: initial model x0,η,β1,β2,ϵ
Initializemandv:m0←0,v0←0
Initialize correction factors for mandv:c0,m←1,c0,v←1
forroundtin{0,1,...T−1}do
gt,Ct=GetPseudoGradient (xt) ▷Compute pseudo-gradient and its certainty
βt,1←βCt
1,βt,2←βCt
2 ▷Adaptβ1andβ2
ηt←Ctη ▷ Adapt step size η
ct+1,m←ct,mβt,1,ct+1,v←ct,vβt,2 ▷Update correction factors
mt+1←(1−βt,1)gt+βt,1mt
vt+1←(1−βt,2)gt⊙gt+βt,2vt ▷Updatemandv
ˆmt+1←mt+1/(1−ct+1,m)
ˆvt+1←vt+1/(1−ct+1,v) ▷Correctmandv
xt+1←xt−ηˆmt+1/(/radicalbig
ˆvt+1+ϵ) ▷Update model weights
end for
Algorithm 2 Pseudo-gradient calculation
Require:α
function GetPseudoGradient (x))
Broadcast xto all clients
forclientkin{0,1,...K−1}parallel do
CalculateFk(x)and∇Fk(x)
xk=LocalSolver (x,ηk) ▷Local training on client datasets
∆k←xk−x
η′
k←∥∆k∥2
∥∇Fk(xt)∥2
Uk←−∆k
η′
k▷Normalize local updates
Ck←log(η′
k/ηk) + 1 ▷Estimate certainty of local updates
Ik=Fk(x)/Fk(x0)
Report (Uk,Ck,Ik)to the server
end for
g←/summationtext
pkIα
kUk/summationtext
pkIα
k,C←/summationtext
pkIα
kCk/summationtext
pkIα
k▷Aggregate local updates
Return g,C
end function
Adaptive hyperparameters for federated Adam:Hyperparameters of FedAdam are adapted as follows
to make better use of pseudo-gradients gfrom accumulated updates. Here, Crepresents the certainty of the
pseudo-gradient and is determined based on the weighted average of local certainties Ckas discussed earlier.
•βt,1←βC
1,βt,2←βC
2: Adaptive βt,1andβt,2dynamically control the weight of the current update
for the momentum estimation. AdaFedAdam assigns more weight to more "certain" pseudo-gradients
to update the average, and thus β1andβ2are adapted exponentially following the form of expo-
nentially weighted moving average.
•ηt←Cη: The base stepsize ηtis adjusted based on the certainty of the pseudo-gradient Cas well.
Greater certainty enables larger ηtto construct larger trust regions and vice versa.
Theoretically, AdaFedAdam ensures the following features:
•Fairness Guarantee : The fairness of the model has been incorporated into the objective function,
optimizing both the error and the fairness with theoretical (p,α)-Proportional Fairness (Mo &
Walrand,2000)guarantee. Also, thealgorithmcanbeadaptedtodifferentfairnesslevelsbyadjusting
αin the problem formulation.
7Under review as submission to TMLR
Pseudo gradientIII
Model updateIII
INormalising local updates, calculating update certainty
II Adaptively rebalance local updates for fairness control
III Accelerating the training with Adam taking advantages of update certaintyI
Local updates
Normalized
updates
Figure 1: Illustration of AdaFedAdam :xt,x∗
kandx∗denote the global model at round t, the optima of local
objective functions of client kand the optima of the global objective function, respectively.
•Fine-tuning Free: The adaptivity of AdaFedAdam derives from dynamic adjustment of hyperpa-
rameters for Adam. All initial hyperparameters of AdaFedAdam can be chosen as the default values in
the standard Adamfor the centralized setting, and they are adaptively adjusted during the federated
training process.
•Allowance for Resource Heterogeneity: Thanks to the normalization of local updates,
AdaFedAdam allows arbitrary numbers of local steps, which could be caused by resource limitation
of clients (also known as resource heterogeneity).
•Compatibility with Arbitrary Local Solvers: The normalization of local updates only relies on
theℓ2-norm of the local gradient estimation. Thus, any first-order optimizers are compatible with
AdaFedAdam .
These features of AdaFedAdam are empirically validated and discussed in Section 6.
5.2 Convergence analysis for AdaFedAdam
The convergence guarantee of AdaFedAdam for convex functions is proved as follows.
Theorem 2 (Convergence of AdaFedAdam )Assume the L-smooth convex loss function F(x)has bounded
gradients∥∇∥∞≤Gfor all x∈Rd, and hyperparameters ϵ,β2,0andη0are chosen according to the following
conditions: η0≤ϵ/2Land1−β2,0≤ϵ2/16G2.The pseudo-gradient gtat steptis given by Algorithm 1 with
its certainty Ct. The convergence guarantee of AdaFedAdam is given by:
1
TT/summationdisplay
t=1∥∇F(xt)∥2≤2(F(x1)−F(x∗))(/radicalbig
β2,0G+ϵ)
RCη 0T(9)
whereR:= mint(mini|gt,i
∇t,i|)for alli∈[d],t∈[T]andC:= minCtfort∈[T].
The proof of Theorem 2 is deferred to Appendix A.2. By normalizing local updates to the same ℓ2-norm
of local gradients, the convergence of AdaFedAdam can be guaranteed. When the client optimizers are fixed
8Under review as submission to TMLR
asSGDand perform one step locally, the federated training is identical to minibatch Adam. Theorem 2 then
provides the same convergence guarantee of Adam(Reddi et al., 2019). It is important to note that Theorem
2 focuses on the convergence guarantee, rather than providing a tight bound for the convergence rate. Better
empirical performance of AdaFedAdam can be anticipated.
6 EXPERIMENTAL RESULTS
Experimental Setups To validates the effectiveness and robustness of AdaFedAdam , we conducted ex-
periments with four federated setups: 1). Femnist setup: A multi-layer perceptron (MLP) network (Pal
& Mitra, 1992) for image classification on Federated EMNIST dataset (Deng, 2012), proposed by Caldas
et al. (2018) as a benchmark task for federated learning; 2). Cifar10 setup: VGG11 (Simonyan & Zisserman,
2014) for image classification on Cifar10 dataset (Krizhevsky et al., 2009) partitioned by Dirichlet distribu-
tionDir(0.1)for 16 clients; 3). Sent140 setup: A stacked-LSTM model (Gers et al., 2000) for sentiment
analysis on the Text Dataset of Tweets (Go et al., 2009); 4). Synthetic setup: A linear regression classifier for
multi-class classification on a synthetic dataset (Synthetic), proposed by Caldas et al. (2018) as a challenging
task for benchmarking federated algorithms. Details of the model architectures, experimental settings and
fine-tuning methods are available in Appendix C.1. All code, data, and experiments are publicly available
at GitHub (the link to be provided in the final manuscript).
Convergence & Fairness We benchmark AdaFedAdam against FedAvg,FedAdam,FedNova,FedProx with
optimalµand q-FedAvg withq= 1. To evaluate the fairness of the models, we measured the standard
deviation (STD) of local accuracy on the clients and the average accuracy of the worst 30% clients. The
training curves are presented in Figure 2 with numerical details in Appendix C.2. Figure 2 shows that
AdaFedAdam consistently converges faster than other algorithms, with better worst 30% client performance
for all setups. Probability density of distributions of local accuracy indicates that federated models trained
with AdaFedAdam provide local accuracy with the least standard deviation for the participants. In contrast,
it is observed that other federated algorithms do not provide consistent performance in different setups with
fine-tuning. To summarize, even without fine-tuning, AdaFedAdam is able to train fair federated models
among participants with better convergence.
Choice of αHyperparameter αis to control the level of desired model fairness. By increasing α, models
become more fair between clients at a cost of convergence. Thus, for each federated learning process, there
exists a Pareto Front Ngatchou et al. (2005) for the trade-off. Taken the Synthetic setup as an example, the
average and relative standard deviation (RSD, definted asσ
µ) of local validation error during the training
process and the formed Pareto Front is shown as Figure 3. It is observed that with increase of αfrom
1 to 4, the RSD of the local error decreases significantly with a slight decrease of the convergence. With
α > 4, the RSD of the local error does not reduce significantly but the convergence continues to decrease.
By plotting the average and RDS of local error of models trained with AdaFedAdam for different αtogether
with other federated algorithms, it can be observed that FedAvg,FedAdam,FedNova,FedProx andq-FedAvg
(withq∈{1,2,4}) are sub-optimal in the blue area in Figure 3. By default, 1≤α≤4is enough to provide
proper model fairness without sacrificing much convergence speed.
Robustness Experiments to validate the robustness of AdaFedAdam against resource heterogeneity and
different levels of data heterogeneity are conducted with the Cifar10 setup.
Robustness against resource heterogeneity is important for algorithms to be applied in real life. Due to the
heterogeneity of clients’ computing resources, the server cannot expect all participants perform requested
number of local steps / epochs in each global round and thus, clients may perform arbitrary numbers of
local steps on the global model in each communication round. To simulate settings of resource heterogeneity,
time-varying numbers of local epochs are randomly sampled from a uniform distribution U(1,3)in each
communication round for each participant. The results are shonw in Table 1. With resource heterogeneity,
AdaFedAdam outperform other federated algorithms with higher average accuracy and more fairness.
Robustness against different non-IID levels ensures the performance of an algorithm in various application
cases. To simulate different non-IID levels, the Cifar10 dataset is partitioned by the Dirichlet distribution
9Under review as submission to TMLR
0 100 200 300 400 500
Num. of rounds5060708090Accuracy / %FEMNIST: Test accuracy
FedAvg
FedAdam
q-FedAvgFedNova
FedProx
AdaFedAdam
0 50 100 150 200
Num. of rounds020406080Accuracy / %CIFAR10: Test accuracy
0 500 1000 1500 2000
Num. of rounds4050607080Accuracy / %SENT140: Test accuracy
0 200 400 600 800 1000
Num. of rounds707580859095100Accuracy / %SYNTHETIC: Test accuracy
0 100 200 300 400 500
Num. of rounds020406080Accuracy of worst 30% / %FEMNIST: Test accuracy of worst 30%
FedAvg
FedAdam
q-FedAvgFedNova
FedProx
AdaFedAdam
0 50 100 150 200
Num. of rounds0204060Accuracy of worst 30% / %CIFAR10: Test accuracy of worst 30%
0 500 1000 1500 2000
Num. of rounds203040506070Accuracy of worst 30% / %SENT140: Test accuracy of worst 30%
0 200 400 600 800 1000
Num. of rounds020406080100Accuracy of worst 30% / %SYNTHETIC: Test accuracy of worst 30%
0 20 40 60 80 100
Accuracy / %02468Density1e3
FEMNIST: Local accuracy distribution
FedAvg
FedAdam
q-FedAvg
FedNova
FedProx
AdaFedAdam
0 20 40 60 80 100
Accuracy / %0246Density1e3
CIFAR10: Local accuracy distribution
0 20 40 60 80 100
Accuracy / %01234Density1e3
SENT140: Local accuracy distribution
0 20 40 60 80 100
Accuracy / %0.000.250.500.751.001.25Density1e2
SYNTHETIC: Local accuracy distribution
Figure 2: Metrics of local test accuracy during the training process: FedAvg,FedAdam,FedNova,FedProx,
q-FedAvg andAdaFedAdam on different setups. Top:Average of local test accuracy of participants. Middle:
Average of local test accuracy of the worst 30% participants. Bottom: Probability density of distributions of
local test accuracy. Figures in each row share the same legend in the first figure. For all setups, AdaFedAdam
consistently outperforms baseline algorithms, with better model convergence and fairness among the partic-
ipants.
0 200 400 600 800 1000
Num. of rounds0.10.20.30.40.50.60.7Test error
AdaFedAdam: Average of test error
=1
=2
=4
=8
=16
=32
=64
=128
0 200 400 600 800 1000
Num. of rounds0.40.60.81.01.2RSD of test error
AdaFedAdam: RSD of test error
0.15 0.20 0.25 0.30
Error1234RSD of Error
=1
=2
=4
=8
 =16
 =32
 =64
 =128
q=1q=2 q=4Pareto Front of the Synthetic Setup
AdaFedAdam
FedAvg
FedAdam
FedNova
FedProx
q-FedAvg
Figure 3: Left:Training curves of AdaFedAdam with different αon the Synthetic setup. Middle: RSD of
local error of AdaFedAdam with different αon the Synthetic setup. Right:Pareto Front of the Synthetic
setupformedby AdaFedAdam withdifferent α. Byadjustingvaluesof α, thetrade-offbetweentheconvergence
and the fairness can be observed together with the suboptimality of other federated algorithms.
over labels with different concentration parameters β∈[0,1], denoted as Dir(β). A smaller value of β
indicates larger level of data heterogeneity. The results are shown in Table 2. With different levels of data
heterogeneity, AdaFedAdam is able to converge, and better performance and fairness are obtained in settings
with less data heterogeneity, as expected.
Compatibility with Momentum-based Local Solvers We also show that AdaFedAdam is compatible
with momentum-based local optimizers in Table 3, which can further improve the model performance.
Adaptiveoptimizersasclientsolvers(e.g. Adam)donotguaranteebetterperformanceovervanilla SGDwithout
10Under review as submission to TMLR
Table 1: Experimental result of federated algorithms against resource heterogeneity on the Cifar10 setup.
Time-varyingnumbersoflocalepochsaresampledfromauniformdistribution U(1,3)ineachcommunication
round. Test accuracy of models is reported.
Algorithm Avg.(%) STD.(%) Worst 30. (%)
FedAvg 49.31±1.37 12.28 ±1.81 33.55 ±2.29
FedAdam 62.44±2.06 16.11 ±2.53 42.25 ±2.11
q-FedAvg 30.96±1.67 15.69 ±0.95 11.58 ±1.16
FedNova 45.77±3.27 20.1 ±2.15 25.24 ±2.71
FedProx 58.25±1.1 12.76 ±1.23 39.95 ±2.78
AdaFedAdam 67.59±1.61 8.5 ±2.2 58.01 ±2.38
Table 2: Experimental results of AdaFedAdam in different levels of non-iid settings on the Cifar10 setup. Test
accuracy of models is reported.
Distribution Avg.(%) STD.(%) Worst 30. (%)
Dir(0.05) 62.81±1.02 8.18 ±1.33 46.01 ±2.23
Dir(0.1) 66.16±1.13 6.58 ±0.77 55.26 ±2.83
Dir(0.5) 71.43±0.81 5.4 ±0.22 64.93 ±1.04
Dir(1) 72.77±0.44 3.05 ±0.11 69.57 ±0.54
synchronizing states of local optimizers, as discussed in Yu et al. (2019) and Yuan & Ma (2020). There are
reported algorithms to synchronize states of local optimizers and AdaFedAdam is orthogonal and compatible
with these algorithms. Full experimental results for different local solvers are deferred to Appendix C.2.
Table 3: Experimental results of the AdaFedAdam with different local optimizers on the Synthetic setup,
including vanilla SGD,SGDwith momentum and SGDwith Nesterov momentum. Test accuracy of models is
reported.
Local Optimizer Avg.(%) STD.(%) Worst 30. (%)
Vanilla SGD 94.18±0.45 8.52 ±0.37 87.07 ±2.28
SGDw. Momen. 97.19±0.11 3.32 ±0.02 93.41 ±0.11
SGDw. Neste. Momen. 97.27±0.16 3.19 ±0.21 94.19 ±0.24
7 Conclusion
In this work, we address the challenge of fair federated learning by formulating it as a dynamic multi-
objective optimization problem. To solve the problem efficiently, we presented AdaFedAdam to achieve fair
model performance among participants. We demonstrate that AdaFedAdam reduces biases in FedAdam and
accelerates the training process of fair federated learning with minimal fine-tuning efforts. Empirically we
validated the efficiency and fairness of AdaFedAdam and demonstrated its Pareto optimality compared with
other alternative federated algorithms. Further, we showed the robustness of AdaFedAdam against resource
heterogeneity and different levels of data heterogeneity. We have also demonstrated the compatibility of
AdaFedAdam with other local optimizers. All code, data, and experiments are publicly available at GitHub
(the link will be provided in the final manuscript). In future work, we plan to test AdaFedAdam in real-world
geographically distributed settings, including cross-silo and cross-device scenarios, using production-grade
open-source frameworks such as Yang et al. (2019); Ekmefjord et al. (2022).
References
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečn` y, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint
arXiv:1812.01097 , 2018.
11Under review as submission to TMLR
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normal-
ization for adaptive loss balancing in deep multitask networks. In International conference on machine
learning, pp. 794–803. PMLR, 2018.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine , 29(6):141–142, 2012.
Morgan Ekmefjord, Addi Ait-Mlouk, Sadi Alawadi, Mattias Åkesson, Prashant Singh, Ola Spjuth, Salman
Toor, and Andreas Hellander. Scalable federated machine learning with fedn. In 2022 22nd IEEE Inter-
national Symposium on Cluster, Cloud and Internet Computing (CCGrid) , pp. 555–564. IEEE, 2022.
Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm.
Neural computation , 12(10):2451–2471, 2000.
Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. CS224N
project report, Stanford , 1(12):2009, 2009.
Chirag Gupta, Sivaraman Balakrishnan, and Aaditya Ramdas. Path length bounds for gradient descent and
flow.J. Mach. Learn. Res. , 22(68):1–63, 2021.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution
for federated visual classification. arXiv preprint arXiv:1909.06335 , 2019.
Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu. Federated learning meets multi-objective
optimization. IEEE Transactions on Network Science and Engineering , 2022.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. Foundations and Trends ®in Machine Learning , 14(1–2):1–210, 2021.
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich,
and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning.
arXiv preprint arXiv:2008.03606 , 2020.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on identical and
heterogeneous data. In International Conference on Artificial Intelligence and Statistics , pp. 4519–4529.
PMLR, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Li Li, Yuxi Fan, Mike Tse, and Kuo-Yi Lin. A review of applications in federated learning. Computers &
Industrial Engineering , 149:106854, 2020a.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning.
InInternational Conference on Learning Representations , 2019a.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods,
and future directions. IEEE Signal Processing Magazine , 37(3):50–60, 2020b.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In International Conference on Machine Learning , pp. 6357–6368. PMLR, 2021.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg
on non-iid data. In International Conference on Learning Representations , 2019b.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics , pp. 1273–1282. PMLR, 2017.
12Under review as submission to TMLR
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on
bias and fairness in machine learning. ACM Computing Surveys (CSUR) , 54(6):1–35, 2021.
Jeonghoon Mo and Jean Walrand. Fair end-to-end window-based congestion control. IEEE/ACM Transac-
tions on networking , 8(5):556–567, 2000.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International
Conference on Machine Learning , pp. 4615–4625. PMLR, 2019.
Patrick Ngatchou, Anahita Zarei, and A El-Sharkawi. Pareto multi objective optimization. In Proceedings of
the 13th international conference on, intelligent systems application to power systems , pp. 84–91. IEEE,
2005.
Sankar K Pal and Sushmita Mitra. Multilayer perceptron, fuzzy sets, classifiaction. 1992.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representa-
tion. InProceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) ,
pp. 1532–1543, 2014.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv preprint
arXiv:1904.09237 , 2019.
Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečn` y, Sanjiv
Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on
Learning Representations , 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767 , 2018.
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. Slowmo: Improving communication-
efficient distributed sgd with slow momentum. In International Conference on Learning Representations ,
2019.
JianyuWang,QinghuaLiu,HaoLiang,GauriJoshi,andHVincentPoor. Tacklingtheobjectiveinconsistency
problem in heterogeneous federated optimization. Advances in neural information processing systems , 33:
7611–7623, 2020.
Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei Wang. Federated learning for
healthcare informatics. Journal of Healthcare Informatics Research , 5(1):1–19, 2021.
Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, and Han Yu. Federated learning. Synthesis
Lectures on Artificial Intelligence and Machine Learning , 13(3):1–207, 2019.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd
fordistributednon-convexoptimization. In International Conference on Machine Learning , pp.7184–7193.
PMLR, 2019.
Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. Advances in Neural
Information Processing Systems , 33:5332–5344, 2020.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for
nonconvex optimization. Advances in neural information processing systems , 31, 2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 , 2012.
Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning on non-iid data: A survey.
Neurocomputing , 465:371–390, 2021.
13Under review as submission to TMLR
A Proof for Theorems
A.1 Proof for Theorem 1
In this section we provide the proof for Theorem 1.
Lemma 3 (Path length bound for Stochastic Gradient Descent) With same assumptions for func-
tionF(x)in Theorem 1, if the SGD iterates with learning rate ηexhibit approximately linear convergence
with constants (A,c)forNsteps, then the path length LN:=/summationtextN
0∥xn+1−xn∥2is bounded as:
LN≤∥x0−x∗∥2N/summationdisplay
0(1−c)nηAL
≤∥x0−x∗∥21−(1−c)N
cAL
The proof of the lemma can be referred to Gupta et al. (2021).
Here we analyze the convergence with no momentum ( β1= 0), and the result can be extended to general
cases (Zaheer et al., 2018).
To simplify the notation, we denote ∇t,ias theith element of the gradient of model ∇F(xt)at roundt, and
∆t,ifor theith element of ∆t. The path length of SGDupdates for at step tis denoted asLt
N
Recall that the update rule of N-AccAdam is given by
xt+1=xt−η∆t√vt+ϵ
for alli∈[d]. LetRt:= min|∆t,i
∇t,i|fori∈[d]andPt
N:=∥∆t∥2
∥∇F(xt)∥2. L-smoothness of function F(x)
guarantees that
F(xt+1)≤F(xt) +⟨∇t,xt+1−xt⟩+L
2∥xt+1−xt∥2
2
=F(xt)−ηd/summationdisplay
i=1(∇t,i·∆t,i√vt,i+ϵ) +Lη2
2d/summationdisplay
i=1∆2
t,i
(√vt,i+ϵ)2
=F(xt)−ηd/summationdisplay
i=1(∇t,i·(∆t,i√vt,i+ϵ−Rt∇t,i/radicalbig
β2vt−1,i+ϵ
+Rt∇t,i/radicalbig
β2vt−1,i+ϵ)) +Lη2
2d/summationdisplay
i=1∆2
t,i
(√vt,i+ϵ)2
≤F(xt)−Rtηd/summationdisplay
i=1∇2
t,i/radicalbig
β2vt−1,i+ϵ
+ηd/summationdisplay
i=1∇t,i|∆t,i√vt,i+ϵ−Rt∇t,i/radicalbig
β2vt−1,i+ϵ|
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T
+Lη2
2d/summationdisplay
i=1∆2
t,i
(√vt,i+ϵ)2
14Under review as submission to TMLR
Tis bounded by
T=|∆t,i√vt,i+ϵ−Rt∇t,i/radicalbig
β2vt−1,i+ϵ|
≤|∆t,i√vt,i+ϵ−∆t,i/radicalbig
β2vt−1,i+ϵ|
≤|∆t,i|·|1√vt,i+ϵ−1/radicalbig
β2vt−1,i+ϵ|
=|∆t,i|
(√vt,i+ϵ)(/radicalbig
β2vt−1,i+ϵ)·(1−β2)∆2
t,i√vt,i+/radicalbig
β2vt−1,i
≤1
(√vt,i+ϵ)(/radicalbig
β2vt−1,i+ϵ)·/radicalbig
1−β2∆2
t,i
≤√1−β2∆2
t,i
(/radicalbig
β2vt−1,i+ϵ)ϵ
With the bound above and ∥∇F(xt)∥∞≤Gfor alli∈[d], we have following
F(xt+1)≤F(xt)−Rtηd/summationdisplay
i=1∇2
t,i/radicalbig
β2vt−1,i+ϵ
+ηG√1−β2
ϵd/summationdisplay
i=1∆2
t,i/radicalbig
β2vt−1,i+ϵ+Lη2
2ϵd/summationdisplay
i=1∆2
t,i√vt,i+ϵ
≤F(xt)−ηRtd/summationdisplay
i=1∇2
t,i/radicalbig
β2vt−1,i+ϵ
+Pt
NηG√1−β2
ϵd/summationdisplay
i=1∇2
t,i/radicalbig
β2vt−1,i+ϵ+Pt
NLη2
2ϵd/summationdisplay
i=1∇2
t,i/radicalbig
β2vt−1,i+ϵ
From the parameters η,ϵandβstated in Adam,Lη/2ϵ≤1/4andG√1−β2/ϵ≤1/4hold. Using the
inequality conditions and let Vt:=∥∆t∥2
∥∇F(xt)∥2, we have
F(xt+1)≤F(xt)−(Rt−Pt
N
2)ηd/summationdisplay
i=1∇2
t,i/radicalbig
β2vt−1,i+ϵ
≤F(xt)−(Rt
Pt
N−1
2)η√β2G+ϵ∥∇F(xt)∥2
Using a telescope sum and rearranging the inequality, we have
η√β2G+ϵT/summationdisplay
t=1(Rt
Pt
N−1
2)∥∇F(xt)∥2)≤F(x1)−F(xt+1)
Due to the fact that 0≤Rt≤Nfor alltandF(x∗)≤F(xt+1), in the case where Rt≤Pt
N/2, the algorithm
does not converge.
With∥∆t∥2≤ηsLt
Nand∇F(xt) =ηsLt
1, we havePt
N=∥∆t∥2
ηsLt
1≤LN
L1≤1−(1−c)N
cfor allt<T. In the best
case where Rt=N, the convergence rate can be derived as follows:
15Under review as submission to TMLR
1
TT/summationdisplay
t=1∥∇F(xt)∥2≤F(x1)−F(x∗)(√β2G+ϵ)
(Nc
1−(1−c)N−1
2)ηT
WhenN= 1, the convergence rate of N-AccAdam is the same as Adam(Zaheer et al., 2018).
A.2 Proof for Theorem 2
In this section we provide the proof for Theorem 2. We analyze the convergence with no momentum ( β1= 0)
andα= 0here. Similar to the proof for Theorem 1, the convergence analysis can be extended to general
cases. The notation in the proof follows A.1. In AdaFedAdam ,gtis given by gt:=/summationtext
pkUk/summationtext
pkwhere Ukis
the normalized local update given by client kin roundtwith its certainty Ck(i.e.∥Uk∥2=∥∇k∥2and
∆k=ηkCkUk). The certainty of gtis given by Ct:=/radicalbig/summationtextpkCk.
Recall that the update rule of AdaFedAdam is given by
xt+1=xt−(logCt+ 1)η0gt√vt+ϵ
for alli∈[d]. LetRt:= min∥gt,i
∇F(xt)i∥fori∈[d]. L-smoothness of the function F(x)guarantees that
F(xt+1)≤F(xt) +⟨∇t,xt+1−xt⟩+L
2∥xt+1−xt∥2
2
=F(xt)−(logCt+ 1)η0d/summationdisplay
i=1(∇t,i·gt,i√vt,i+ϵ) +L((logCt+ 1)η0)2
2d/summationdisplay
i=1g2
t,i
(√vt,i+ϵ)2
=F(xt)−(logCt+ 1)η0d/summationdisplay
i=1(∇t,i·(gt,i√vt,i+ϵ−Rt∇t,i/radicalbig
β2,tvt−1,i+ϵ+Rt∇t,i/radicalbig
β2,tvt−1,i+ϵ))
+L((logCt+ 1)η0)2
2d/summationdisplay
i=1g2
t,i
(√vt,i+ϵ)2
≤F(xt)−Rt(logCt+ 1)η0d/summationdisplay
i=1∇2
t,i/radicalbig
β2,tvt−1,i+ϵ
+ (logCt+ 1)η0d/summationdisplay
i=1∇t,i|gt,i√vt,i+ϵ−Rt∇t,i/radicalbig
β2,tvt−1,i+ϵ|
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T
+L((logCt+ 1)η0)2
2d/summationdisplay
i=1g2
t,i
(√vt,i+ϵ)2
16Under review as submission to TMLR
Tis bounded by
T=|gt,i√vt,i+ϵ−Rt∇t,i/radicalbig
β2,tvt−1,i+ϵ|
≤|gt,i√vt,i+ϵ−gt,i/radicalbig
β2,tvt−1,i+ϵ|
≤|gt,i|·|1√vt,i+ϵ−1/radicalbig
β2,tvt−1,i+ϵ|
=|gt,i|
(√vt,i+ϵ)(/radicalbig
β2,tvt−1,i+ϵ)·(1−β2,t)g2
t,i√vt,i+/radicalbig
β2,tvt−1,i
≤1
(√vt,i+ϵ)(/radicalbig
β2,tvt−1,i+ϵ)·/radicalbig
1−β2,tg2
t,i
≤/radicalbig
1−β2,tg2
t,i
(/radicalbig
β2,tvt−1,i+ϵ)ϵ
With the bound above and ∥∇F(xt)∥∞≤G, we have following
F(xt+1)≤F(xt)−Rt(logCt+ 1)η0d/summationdisplay
i=1∇2
t,i/radicalbig
β2,tvt−1,i+ϵ
+(logCt+ 1)η0G/radicalbig
1−β2,t
ϵd/summationdisplay
i=1g2
t,i/radicalbig
β2,tvt−1,i+ϵ
+L((logCt+ 1)η0)2
2ϵd/summationdisplay
i=1g2
t,i√vt,i+ϵ
≤F(xt)−(logCt+ 1)η0Rtd/summationdisplay
i=1∇2
t,i/radicalbig
β2,tvt−1,i+ϵ
+(logCt+ 1)η0G/radicalbig
1−β2,t
ϵd/summationdisplay
i=1∇2
t,i/radicalbig
β2,tvt−1,i+ϵ
+L((logCt+ 1)η0)2
2ϵd/summationdisplay
i=1∇2
t,i/radicalbig
β2,tvt−1,i+ϵ
From the parameters η,ϵandβstated in Adam,Lη0/2ϵ≤1/4andG/radicalbig
1−β2,0/ϵ≤1/4hold. The inequality
G/radicalig
1−βCt
2,0/η≤(logCt+ 1)/4holds ifβ2,0≥log22≈0.520andCt≥1, which is true since β2,0is close to
1 with the default value β2,0= 0.999andCt≥1. Using the inequality conditions, we have
F(xt+1)≤F(xt)−(Rt−(logCt+ 1)
2)(logCt+ 1)η0d/summationdisplay
i=1∇2
t,i/radicalbig
β2,tvt−1,i+ϵ
≤F(xt)−Rt
2(logCt+ 1)η0/radicalbig
β2,0G+ϵ∥∇F(xt)∥2
The second inequality is due to the fact that Rt≥Ct>(logCt+ 1)andβ2,t≤β2,0ifCt≥1. Using a
telescope sum and rearranging the inequality, we have
η0/radicalbig
β2,0G+ϵT/summationdisplay
t=1Rt(logCt+ 1)∥∇F(xt)∥2≤F(x1)−F(xt+1)
17Under review as submission to TMLR
LetR:= minRtandC:= minCtfor allt∈[T], by rearranging the inequality, we obtain
1
TT/summationdisplay
t=1∥∇F(xt)∥2≤2(F(x1)−F(x∗))(/radicalbig
β2,0G+ϵ)
R(logC+ 1)η0T
18Under review as submission to TMLR
B PSEUDO CODES FOR ALGORITHMS
Pseudo codes for Adam,N-AccAdam and minibatch SGDare given as Algorithm 3 and Algorithm 4.
Algorithm 3 AdamandN-AccAdam
Require: model weights x0, stepsizeη,β1,β2,ϵfor both AdamandN-AccAdam ,ηsandNforN-AccAdam
m0←0
v0←0
forsteptin{0,1,...T−1}do
Adam:∆t=∇ζ∼DF(xt)
N-AccAdam :∆t= (xt−SGD (xt,∇ζ∼D,ηs,N))/ηs
mt+1←(1−β1)∆t+β1mt
vt+1←(1−β2)∆2
t+β2vt
ˆmt+1←mt+1/(1−βt+1
1)
ˆvt+1←vt+1/(1−βt+1
2)
xt+1←xt−ηˆmt+1/(/radicalbig
ˆvt+1+ϵ)
end for
Algorithm 4 Minibatch Stochastic Gradient Descent ( SGD)
Require: model weights x0, learning rate ηs, batch size ζand number of steps N
forstepnin{0,1,...N−1}do
Sample a batch of data with size of ζfrom the training dataset
Calculate gradient estimation ∇ζ∼DF(xn)
Update model xn+1=xn−ηs∇ζ∼DF(xn)
end for
Pseudo codes for FedOpt(Reddi et al. (2020)) is given as Algorithm 5.
Algorithm 5 Adaptive federated optimization ( FedOpt)
Require: Seed model x0
forroundtin{0,1,...T−1}do
forclientkin{0,1,...K−1}parallel do
xt
k:=ClientOpt (xt) ▷Client-side
∆t
k:=xt
k−xt
end for
∆t:=Aggre ({∆t
k,0≤k<K}) ▷Server-side
xt+1:=ServerOpt (∆t)
end for
19Under review as submission to TMLR
C Experiments
C.1 Experimental details
Platform All experiments in the paper are conducted on a server with Intel(R) Xeon(R) Gold 6230R CPU
and and 2x NVidia RTX A5000 GPUs. All codes are implemented in PyTorch.
Setups Details of all federated setups are shown as follows:
•Femnist: A multi-layer perceptron network (MLP) for the classification of the EMNIST dataset.
The MLP used for the setup consisted 128 hidden nodes activated by ReLu functions with a loss
function of cross-entropy. The EMNIST dataset is partitioned according to the writer of images and
each partition acts as a local dataset for each client. Local datasets are thus intrinsically non-IID
due to different writing characteristics from different writers.
•Cifar10: A VGG11 (Simonyan & Zisserman (2014)) model for Cifar10 dataset. The model used
for the setup is VGG11 with slight modifications to be compatible with Cifar10 dataset. The
architecture of the model is shown as Figure 4 with a loss function of cross-entropy. The Cifar10
dataset is partitioned into 16 subsets by the Dirichlet distribution Dir16(0.1)over labels.
•Sent140: An LSTM model (Gers et al. (2000)) for the sentiment analysis for the Sent140 dataset
(Go et al. (2009)). Input words are embedded with pretrained Glove (Pennington et al. (2014))
and logits are output after two LSTM layers with 100 hidden units and one dense layer, with
architecture shown in Figure 5. The partitioning of the Sent140 dataset follows Caldas et al. (2018)
and a collection of tweets from each twitter account acts as the local dataset of one client.
•Synthetic: Alinearregressionclassifierformulti-classclassificationonasyntheticdataset,proposed
by Caldas et al. (2018) as a challenging task for the benchmark of federated learning algorithms.
The model is y=argmax (softmax (Wx+b)), wherex∈R60,W∈R10×60andb∈R10with a loss
function of cross-entropy. In the Synthetic dataset, there are 100 partitions, the sizes of which follow
a power law.
For all setups, each client is associated with a partition and randomly split the local partition with a ratio
of8 : 2acting as its local training and testing set before federated training starts. A summary of four setups
are shown in Table 4.
Table 4: A summary of setups: the four setups cover different Federated Learning scenarios, non-IID types
and task types.
Setup # Clients Model Scenario Non-IID Type Task Type
Femnist 3500 MLP Cross Device Intrinsic Computer Vision
Cifar10 16 CNN Cross Silo Dirichlet Computer Vision
Sent140 697 LSTM Cross Device Intrinsic Natural Language Process
Synthetic 100 Linear Model Cross Device/Silo Synthetic Classification
Hyperparameter settings Hyperparemters of federated optimization methods can be categorized as a).
method-independent hyperparameters (including local learning rate, batch size, number of local epochs and
communication rounds), and b). method-specific hyperparameters (e.g. qin q-FedAvg, µin FedProx).
It is prohibitively infeasible to find the optimal combinations of hyperparameter of all optimization with
grid-search. In this work, we first tune the local learning rate, batch size and number of local epochs on
FedAvg, and fix them for other algorithms first. Then the method-specific hyperparameters for individual
federated method are tuned to obtain their optimal performance. For all experiments without specifications,
local optimizers for clients are fixed as SGDand number of local epoch is fixed as 1. Local learning rate
for Femnist, Cifar10, Sent140 and Synthetic setup are 0.01,0.02,0.3and0.01, respectively. Batch sizes for
20Under review as submission to TMLR
Figure 4: Architecture of VGG11 for the CIFAR10 setup.
Figure 5: Architecture of the two-layer LSTM for the Sent140 setup.
21Under review as submission to TMLR
five setups are 10,32,10and10respectively. If local optimizers are set as SGDwith (Nesterov) momentum,
the momentum factor is fixed as 0.9by default. For server optimizers, FedAvghas the default learning rate
η= 1,FedAdam has the default hyperparameter set ( η= 0.001,β1= 0.9andβ2= 0.999), and q-FedAvg has
learning rate η= 1. For all experiments with q-FedAvg ,q= 1is fixed to compare with AdaFedAdam with
α= 1. For all experiments with FedProx,µis tuned with grid search from {0.001,0.005,0.01,0.1,1}. Total
communication rounds are 500, 200, 1000 and 1000 for the Femnist, Cifar10, Sent140 and Synthetic setup,
respectively. For each experiment, global models are initialized with 3 different random seeds and trained
independently, and averaged metrics are reported.
C.2 Full experimental results
Convergence & Fairness Table 5 shows the full results of the experiment of fairness and convergence.
Table 5: Full experimental results of convergence and fairness: Statistics of test accuracy on clients for
AdaFedAdam compared to FedAvg,FedAdam,FedNova andq-FedAvg withq= 1, for Femnist, Cifar10, Sent140
and Synthetic setups.
Settings Algorithms Avg.(%) STD.(%) Worst 30%(%)
Femnist FedAvg 77.77±0.64 13.2 ±0.91 60.11 ±2.19
FedAdam 82.97±0.26 11.44 ±0.76 67.65 ±1.8
q-FedAvg 76.91±0.21 10.94 ±0.26 64.06 ±0.37
FedNova 78.31±0.53 10.77 ±0.36 64.97 ±1.01
FedProx 80.16±0.16 10.02 ±0.19 66.3 ±0.38
AdaFedAdam 84.48±0.5 8.62 ±0.25 74.16 ±0.3
Cifar10 FedAvg 48.3±0.37 14.33 ±1.3 26.43 ±1.09
FedAdam 59.35±1.43 7.74 ±0.59 49.9 ±0.7
q-FedAvg 31.94±1.09 13.49 ±0.05 13.18 ±1.37
FedNova 40.32±2.01 18.99 ±1.65 20.06 ±3.31
FedProx 55.77±0.28 13.73 ±0.94 39.11 ±2.66
AdaFedAdam 65.27±1.32 7.65 ±1.62 57.12 ±1.61
Sent140 FedAvg 73.4±1.02 18.76 ±1.08 49.82 ±2.75
FedAdam 73.82±0.91 15.47 ±0.37 55.54 ±0.23
q-FedAvg 73.38±0.61 16.99 ±1.02 52.88 ±2.08
FedNova 74.64±0.51 17.6 ±0.95 53.03 ±1.86
FedProx 74.72±0.42 16.74 ±1.29 54.65 ±2.32
AdaFedAdam 74.62±0.07 10.24 ±0.35 58.8 ±0.33
Synthetic FedAvg 90.08±0.12 14.23 ±0.28 39.51 ±1.9
FedAdam 89.97±0.16 13.52 ±0.29 53.08 ±2.95
q-FedAvg 90.64±0.1 10.36 ±0.16 78.65 ±0.23
FedNova 92.97±0.11 8.06 ±0.11 83.16 ±0.3
FedProx 91.74±0.06 15.53 ±0.07 28.62 ±0.3
AdaFedAdam 95.07±0.13 5.5 ±0.2 88.64 ±0.42
Robustness against different levels of data heterogeneity Figure 6 shows label distributions of dif-
ferent non-IID levels of the Cifar10 setup. Table 6 shows the full results of comparison between different
algorithms on the Cifar10 setup. Different levels of data heterogeneity are generated with Dirichlet distribu-
tion of different concentration parameter βranging from 0.05to1and together with an IID partitioning. It
can be observed that AdaFedAdam consistently outperforms other algorithms with the highest test accuracy
and lowest STD of test accuracy in all different settings.
Compatibility with local momentum Table 7 shows the full results of different federated algorithms
with different local solvers. It is observed that AdaFedAdam is not only compatible with momentum-based
local solvers, it also provides better results compared to other federated algorithms.
22Under review as submission to TMLR
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Client Idx0.00.20.40.60.81.0DensityLabel Distribution of Non-IID Cifar10: Dir(0.05)
Class
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Client Idx0.00.20.40.60.81.0DensityLabel Distribution of Non-IID Cifar10: Dir(0.1)
Class
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Client idx0.00.20.40.60.81.0DensityLabel Distribution of Non-IID Cifar10: Dir(0.5)
Class
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Client Idx0.00.20.40.60.81.0DensityLabel Distribution of Non-IID Cifar10: Dir(1)
Class
0
1
2
3
4
5
6
7
8
9
Figure 6: Label distributions of local datasets of the Cifar10 setup for different non-IID levels.
Table 6: Full experimental results of federated algorithms against different levels of data heterogeneity on
the Cifar10 setup.
Data Distribution Algorithm Avg.(%) STD.(%) Worst 30%(%)
Dir(0.05) FedAvg 36.47±0.7520.28±0.90 9.45 ±3.51
FedAdam 56.33±0.9611.77±1.99 40.4 ±5.13
q-FedAvg 28.01±0.8121.92±0.47 3.15 ±3.25
FedNova 36.25±0.8824.34±1.34 5.00 ±3.24
AdaFedAdam 62.81±1.028.18±1.33 46.01 ±2.23
Dir(0.1) FedAvg 50.41±0.4613.21±0.36 33.20 ±3.98
FedAdam 65.79±0.918.61±0.51 55.92 ±2.36
q-FedAvg 38.95±0.7312.46±0.20 24.59 ±2.11
FedNova 48.09±1.8214.29±0.58 33.34 ±3.28
AdaFedAdam 66.16±1.138.59±0.39 56.48 ±1.45
Dir(0.5) FedAvg 49.38±0.927.29±2.60 41.22 ±3.25
FedAdam 70.49±0.783.97±0.49 65.83 ±0.84
q-FedAvg 44.95±0.414.60±0.51 39.73 ±0.61
FedNova 49.47±1.066.09±2.19 43.00 ±3.26
AdaFedAdam 71.43±0.815.40±0.22 64.93 ±1.04
Dir(1): FedAvg 40.97±0.664.93±0.47 35.53 ±1.12
FedAdam 71.22±0.172.95±0.27 68.01 ±0.02
q-FedAvg 36.27±0.955.40±0.75 30.63 ±1.56
FedNova 40.28±0.104.70±0.49 35.30 ±0.40
AdaFedAdam 72.77±0.443.05±0.11 69.57 ±0.54
23Under review as submission to TMLR
Table 7: Full experimental results of federated algorithms in cooperation with local momentum on the
Synthetic setup.
Local Solver Algorithm Avg.(%) STD.(%) Worst 30%(%)
Vanilla SGD FedAvg 90.08±0.1214.23±0.28 39.51 ±1.9
FedAdam 89.97±0.1613.52±0.29 53.08 ±2.95
q-FedAvg 90.64±0.110.36±0.16 78.65 ±0.23
FedNova 92.97±0.118.06±0.11 83.16 ±0.3
FedProx 91.74±0.0615.53±0.07 28.62 ±0.3
AdaFedAdam 95.07±0.135.5±0.2 88.64 ±0.42
SGDwith Momen. FedAvg 95.26±0.228.42±0.24 68.65 ±0.19
FedAdam 91.60±0.3212.32±0.66 59.52 ±4.79
q-FedAvg 94.64±0.225.73±0.03 88.04 ±0.02
FedNova 96.12±0.093.82±0.05 93.07 ±0.15
FedProx 93.12±0.067.21±0.05 84.53 ±0.09
AdaFedAdam 97.19±0.113.32±0.02 93.41 ±0.11
SGDwith Neste. Momen. FedAvg 95.24±0.148.34±0.05 68.80 ±0.36
FedAdam 91.79±0.1912.07±0.43 61.51 ±3.52
q-FedAvg 94.56±0.025.80±0.12 87.85 ±0.02
FedNova 96.85±0.043.80±0.30 94.02 ±0.11
FedProx 95.46±0.155.37±0.12 87.41 ±0.93
AdaFedAdam 97.27±0.163.19±0.21 94.19 ±0.24
24