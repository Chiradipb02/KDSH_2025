Published in Transactions on Machine Learning Research (10/2023)
Bridging Imitation and Online Reinforcement Learning: An
Optimistic Tale
Botao Hao bhao@google.com
Google DeepMind
Rahul Jain rahul.jain@usc.edu
University of Southern California
Google DeepMind∗
Dengwang Tang dengwang@usc.edu
University of Southern California
Zheng Wen zhengwen@google.com
Google DeepMind
Reviewed on OpenReview: https: // openreview. net/ forum? id= lanGfX0M6C
Abstract
In this paper, we address the following problem: Given an offline demonstration dataset
from an imperfect expert, what is the best way to leverage it to bootstrap online learning
performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL)
algorithm that uses the offline dataset, and information about the expert’s behavioral policy
used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero
exponentially fast in N, the offline dataset size if the expert is competent enough. Since
this algorithm is computationally impractical, we then propose the iRLSVI algorithm that
can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning.
Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant
reduction in regret as compared to two baselines: no offline data, and offline dataset but
used without suitably modeling the generative policy. Our algorithm can be seen as bridging
online RL and imitation learning.
1 Introduction
An early vision of the Reinforcement Learning (RL) field is to design a learning agent that when let loose
in an unknown environment, learns by interacting with it. Such an agent starts with a blank slate (with
possibly, arbitrary initialization), takes actions, receives state and reward observations, and thus learns by
“reinforcement”. This remains a goal but at the same time, it is recognized that in this paradigm learning is
too slow, inefficient and often impractical. Such a learning agent takes too long to learn near-optimal policies
way beyond practical time horizons of interest. Furthermore, deploying an agent that learns by exploration
over long time periods may simply be impractical.
In fact, reinforcement learning is often deployed to solve complicated engineering problems by first collecting
offline data using a behavioral policy, and then using off-policy reinforcement learning, or imitation learning
methods (if the goal is to imitate the policy that generated the offline dataset) on such datasets to learn
a policy. This often suffers from the distribution-shift problem, i.e., the learnt policy upon deployment
often performs poorly on out-of-distribution state-action space. Thus, there is a need for adaptation and
fine-tuning upon deployment.
∗Work done while at Google DeepMind.
1Published in Transactions on Machine Learning Research (10/2023)
In this paper, we propose a systematic way to use offline datasets to bootstrap online RL algorithms.
Performance of online learning agents is often measured in terms of cumulative (expected) regret. We
show that, as expected, there is a gain in performance (reflected in reduction in cumulative regret) of the
learning agent as compared to when it did not use such an offline dataset. We call such an online learning
agent as being partially informed . However, somewhat surprisingly, if the agent is further informed about
the behavioral policy that generated the offline dataset, such an informed (online learning) agent can do
substantially better, reducing cumulative regret significantly. In fact, we also show that if the behavioral
policy is suitably parameterized by a competence parameter , wherein the behavioral policy is asymptotically
the optimal policy, then the higher the “competence” level, the better the performance in terms of regret
reduction over the baseline case of no offline dataset.
We first propose an ideal (informed) iPSRL(posterior sampling-based RL) algorithm and show via theoretical
analysis that under some mild assumptions, its expected cumulative regret is bounded as ˜O(√
T)whereT
is the number of episodes. In fact, we show that if the competence of the expert is high enough (quantified
in terms of a parameter we introduce), the regret goes to zero exponentially fast as N, the offline dataset
size grows. This is accomplished through a novel prior-dependent regret analysis of the PSRL algorithm,
the first such result to the best of our knowledge. Unfortunately, posterior updates in this algorithm can
be computationally impractical. Thus, we introduce a Bayesian-bootstrapped algorithm for approximate
posterior sampling, called the (informed) iRLSVIalgorithm (due to its commonality with the RLSVI algo-
rithm introduced in Osband et al. (2019)). The iRLSVIalgorithm involves optimizing a loss function that
is anoptimistic upper bound on the loss function for MAP estimates for the unknown parameters. Thus,
while inspired by the posterior sampling principle, it also has an optimism flavor to it. Through, numeri-
cal experiments, we show that the iRLSVIalgorithm performs substantially better than both the partially
informed-RLSVI (which uses the offline dataset naively) as well as the uninformed-RLSVI algorithm (which
doesn’t use it at all).
We also show that the iRLSVIalgorithm can be seen as bridging online reinforcement learning with imitation
learning since its loss function can be seen as a combination of an online learning term as well as an imitation
learning term. And if there is no offline dataset, it essentially behaves like an online RL algorithm. Of course,
in various regimes in the middle it is able to interpolate seamlessly.
Related Work. Because of the surging use of offline datasets for pre-training (e.g., in Large Language
models (LLMs), e.g., see Brown et al. (2020); Thoppilan et al. (2022); Hoffmann et al. (2022)), there has
been a lot of interest in Offline RL, i.e., RL using offline datasets (Levine et al., 2020). A fundamental issue
this literature addresses is RL algorithm design (Nair et al., 2020; Kostrikov et al., 2021; Kumar et al., 2020;
Nguyen-Tang & Arora, 2023; Fujimoto et al., 2019; Fujimoto & Gu, 2021; Ghosh et al., 2022) and analysis
to best address the “out-of-distribution” (OOD) problem, i.e., policies learnt from offline datasets may not
perform so well upon deployment. The dominant design approach is based on ‘pessimism’ (Jin et al., 2021;
Xie et al., 2021a; Rashidinejad et al., 2021) which often results in conservative performance in practice. Some
of the theoretical literature (Xie et al., 2021a; Rashidinejad et al., 2021; Uehara & Sun, 2021; Agarwal &
Zhang, 2022) has focused on investigation of sufficient conditions such as “concentrability measures” under
which such offline RL algorithms can have guaranteed performance. Unfortunately, such measures of offline
dataset quality are hard to compute, and of limited practical relevance (Argenson & Dulac-Arnold, 2020;
Nair et al., 2020; Kumar et al., 2020; Levine et al., 2020; Kostrikov et al., 2021; Wagenmaker & Pacchiano,
2022).
There is of course, a large body of literature on online RL (Dann et al., 2021; Tiapkin et al., 2022; Ecoffet
et al., 2021; Guo et al., 2022; Ecoffet et al., 2019; Osband et al., 2019) with two dominant design philosophies:
Optimism-based algorithms such as UCRL2 in Auer et al. (2008), and Posterior Sampling (PS)-type algo-
rithms such as PSRL (Osband et al., 2013; Ouyang et al., 2017), etc. (Osband et al., 2016a;b; 2019; Russo &
Van Roy, 2018; Zanette & Sarkar, 2017; Wen et al., 2020; Hao & Lattimore, 2022). However, none of these
algorithms consider starting the learning agent with an offline dataset. Of course, imitation learning (Hester
et al., 2018; Beliaev et al., 2022; Schaal, 1996) is exactly concerned with learning the expert’s behavioral
policy (which may not be optimal) from the offline datasets but with no online finetuning of the policy learnt.
Several papers have actually studied bridging offline RL and imitation learning (Ernst et al., 2005; Kumar
et al., 2022; Rashidinejad et al., 2021; Hansen et al., 2022; Vecerik et al., 2017; Lee et al., 2022). Some have
2Published in Transactions on Machine Learning Research (10/2023)
also studied offline RL followed by a small amount of policy fine-tuning (Song et al., 2022; Fang et al., 2022;
Xie et al., 2021b; Wan et al., 2022; Schrittwieser et al., 2021; Ball et al., 2023; Uehara & Sun, 2021; Xie
et al., 2021b; Agarwal & Zhang, 2022) with the goal of finding policies that optimize simple regret.
Theproblemwestudyinthispaperismotivatedbyasimilarquestion: Namely,givenanofflinedemonstration
datasetfromanimperfectexpert, whatisthebestwaytoleverageittobootstraponlinelearningperformance
inMDPs? However,ourworkisdifferentinthatitfocusesoncumulativeregretasameasureofonlinelearning
performance. This requires smart exploration strategies while making maximal use of the offline dataset to
achievethebestregretreductionpossibleoverthecasewhenanofflinedatasetisnotavailable. Ofcourse, this
will depend on the quality and quantity of the demonstrations. And the question is what kind of algorithms
can one devise to achieve this objective, and what information about the offline dataset-generation process
is helpful? What is the best regret reduction that is achievable by use of offline datasets? How it depends
on the quality and quantity of demonstrations, and what algorithms can one devise to achieve them? And
does any information about the offline-dataset generation process help in regret reduction? We answer some
of these questions in this paper.
2 Preliminaries
Episodic Reinforcement Learning. Consider a scenario where an agent repeatedly interacts with an
environmentmodelledasafinite-horizonMDP,andrefertoeachinteractionasanepisode. Thefinite-horizon
MDP is represented by a tuple M= (S,A,P,r,H,ν ), whereSis a finite state space (of size S),Ais a
finite action space (of size A),Pencodes the transition probabilities, ris the reward function, His the time
horizon length, and νis the initial state distribution. The interaction protocol is as follows: at the beginning
of each episode t, the initial state st
0is independently drawn from ν. Then, at each period h= 0,1,...,H−1
in episodet, if the agent takes action at
h∈Aat the current state st
h∈S, then it will receive a reward
rh(st
h,at
h)and transit to the next state st
h+1∈Ph(·|st
h,at
h). An episode terminates once the agent arrives at
statest
Hin periodHand receives a reward rH(st
H). We abuse notation for the sake of simplicity, and just
userH(st
H,at
H)instead ofrH(st
H), though no action is taken at period H. The objective is to maximize its
expected total reward over Tepisodes.
LetQ∗
handV∗
hrespectively denote the optimal state-action value and state value functions at period h.
Then, the Bellman equation for MDP Mis
Q∗
h(s,a) =rh(s,a) +/summationdisplay
s′Ph(s′|s,a)V∗
h+1(s′), (1)
whereV∗
h+1(s′) := max bQ∗
h+1(s′,b), ifh < H−1andV∗
h+1(s′) = 0, ifh=H−1. We define a policy π
as a mapping from a state-period pair to a probability distribution over the action space A. A policyπ∗is
optimal ifπ∗
h(·|s)∈arg maxπh/summationtext
aQ∗
h(s,a)πh(a|s)for alls∈Sand allh.
Agent’s Prior Knowledge about M.We assume that the agent does not fully know the environment
M; otherwise, there is no need for learning and this problem reduces to an optimization problem. However,
the agent usually has some prior knowledge about the unknown part of M. For instance, the agent might
know thatMlies in a low-dimensional subspace, or may have a prior distribution over M. We use the
notationM(θ)whereθparameterizes the unknown part of the MDP. When we want to emphasize it as a
random quantity, we will denote it by θ∗with a prior distribution µ0. Of course, different assumptions about
the agent’s prior knowledge lead to different problem formulations and algorithm designs. As a first step,
we consider two canonical settings:
•Tabular RL: The agent knows S,A,r,Handν, but does not know P. That is,θ∗=Pin this
setting. We also assume that the agent has a prior over P, and this prior is independent across
state-period-action triples.
•Linear value function generalization: The agent knows S,A,Handν, but does not know P
andr. Moreover, the agent knows that for all h,Q∗
hlies in a low-dimensional subspace span(Φh),
3Published in Transactions on Machine Learning Research (10/2023)
where Φh∈ℜ|S||A|×dis a known matrix. In other words, Q∗
h= Φhθ∗
hfor someθ∗
h∈ℜd. Thus, in
this setting θ∗=/bracketleftbig
θ∗⊤
0,...,θ∗⊤
H−1/bracketrightbig⊤. We also assume that the agent has a Gaussian prior over θ∗.
As we will discuss later, the insights developed in this paper could potentially be extended to more general
cases.
Offline Datasets. We denote an offline dataset withLepisodes asD0={(¯sl
0,¯al
0,···,¯sl
H)L
l=1}, where
N=HLdenotes the dataset size in terms of number of observed transitions. For the sake of simplicity, we
assume we have complete trajectories in the dataset but it can easily be generalized if not. We denote an
online dataset withtepisodes asHt={(sl
0,al
0,···,sl
H)t
l=1}andDt=D0⊕Ht.
The Notion of Regret. A online learning algorithm ϕis a map for each episode t, and timeh,ϕt,h:Dt→
∆A, the probability simplex over actions. We define the Bayesian regret of an online learning algorithm ϕ
overTepisodes as
BRT(ϕ) :=E/bracketleftiggT/summationdisplay
t=1/parenleftigg
V∗
0(st
0;θ∗)−H/summationdisplay
h=0rh(st
h,at
h)/parenrightigg/bracketrightigg
,
where the (st
h,at
h)’s are the state-action tuples from using the learning algorithm ϕ, and the expectation
is over the sequence induced by the interaction of the learning algorithm and the environment, the prior
distributions over the unknown parameters θ∗and the offline dataset D0.
Expert’s behavioral policy and competence. We assume that the expert that generated the offline
demonstrations may not be perfect, i.e., the actions it takes are only approximately optimal with respect to
the optimal Q-value function. To that end, we model the expert’s policy by use of the following generative
model,
πβ
h(a|s) =exp(β(s)Q∗
h(s,a))/summationtext
aexp(β(s)Q∗
h(s,a)), (2)
whereβ(s)≥0is called the state-dependent deliberateness parameter, e.g., when β(s) = 0, the expert
behaves naively in state s, and takes actions uniformly randomly. When β(s)→∞, the expert uses the
optimal policy when in state s. Whenβ(·)is unknown, we will assume an independent exponential prior for
the sake of analytical simplicity, f2(β(s)) =λ2exp(−λ2β(s))overβ(s)whereλ2>0is the same for all s.
In our experiments, we will regard β(s)as being the same for all states, and hence a single parameter.
The above assumes the expert is knowledgeable about Q∗. However, it may know it only approximately. To
model that, we introduce a knowledgeability parameterλ≥0. The expert then knows ˜Qwhich is distributed
asN(Q∗,I/λ2)conditioned on θ, and selects actions according to the softmax policy Eq. (2), with the Q∗
replaced by ˜Q. The two parameters (β,λ)together will be referred to as the competence of the expert. In
this case, we denote the expert’s policy as πβ,λ
h.
Remark 2.1.While the form of the generative policy in Eq. (2) seems specific, πβ
h(·|s)is a random vector
with support over the entire probability simplex. In particular, if one regards β(s)and ˜Qh(s,·)as parameters
that parameterize the policy, the softmax policy structure as in Eq. (2) is enough to realize any stationary
policy.
Furthermore, we note that our main objective here is to yield clear and useful insights when information is
available to be able to model the expert’s behavioral policy with varying competence levels. Other forms of
generative policies can also be used including ϵ-optimal policies introduced in (Beliaev et al., 2022), and the
framework extended.
3 The Informed PSRL Algorithm
We now introduce a simple Informed Posterior Sampling-based Reinforcement Learning (iPSRL) algorithm
that naturally uses the offline dataset D0and action generation information to construct an informed prior
distribution over θ∗. The realization of θ∗is assumed known to the expert (but not the learning agent) with
4Published in Transactions on Machine Learning Research (10/2023)
Algorithm 1 iPSRL
Input:Priorµ0, Initial state distribution ν
fort= 1,···,Tdo
(A1) Update posterior, µt(θ|Ht−1,D0)using Bayes’ rule
(A2) Sample ˜θt∼µt
Compute optimal policy ˜πt(·|s) :=π∗
h(·|s;˜θt)(for allh) by using any DP or other algorithm
Sample initial state sl
0∼ν
forh= 0,···,H−1do
Take action at
h∼˜πt(|st
h)
Observe (st
h+1,rt
h)
end for
end for
˜Q(·,·;θ∗) =Q(·,·;θ∗), andβ(s) :=β≥0(i.e., it is state-invariant) is also known to the expert. Thus, the
learning agent’s posterior distribution over θ∗given the offline dataset is,
µ1(θ∗∈·) :=P(θ∗∈·|D 0)∝P(D0|θ∗∈·)P(θ∗∈·)
=P(θ∗∈·)×/integraldisplay
θ∈·L/productdisplay
lH−1/productdisplay
h=0θ(¯sl
h+1|¯sl
h,¯al
h)πβ
h(¯al
h|¯sl
h,θ)ν(¯sl
0)dθ.(3)
A PSRL agent (Osband et al., 2013; Ouyang et al., 2017) takes this as the prior, and then updates the
posterior distribution over θ∗as online observation tuples, {(st
h,at
h,st′
h,rt
h)H−1
h=0become available. Such an
agent is really an ideal agent with assumed posterior distribution updates being exact. In practice, this is
computationally intractable and we will need to get samples from an approximate posterior distribution, an
issue which we will address in the next section. We will denote the posterior distribution over θ∗given the
online observations by episodes tand the offline dataset by µt.
We note the key steps (A1)-(A2) in the algorithm above where we use both offline dataset D0and online
observationsHtto compute the posterior distribution over θ∗by use of the prior ν.
3.1 Prior-dependent Regret Bound
It is natural to expect some regret reduction if an offline demonstration dataset is available to warm-start
the online learning. However, the degree of improvement must depend on the “quality” of demonstrations,
for example through the competence parameter β. Further note that the role of the offline dataset is via the
prior distribution the PSRL algorithm uses. Thus, theoretical analysis involves obtaining a prior-dependent
regret bound, which we obtain next.
Lemma 3.1. Letε=P(˜π1̸=π∗), i.e. the probability that the strategy used for the first episode, ˜π1is not
optimal. Then,
BRT(ϕiPSRL) =O(√
εH4S2AT(1 + log(T))). (4)
The proof can be found in the Appendix. Note that this Lemma provides a prior dependent-bound in terms
ofε.
In the rest of the section, we provide an upper bound on ε=P(˜π1̸=π∗)forAlgorithm 1: iPSRL. We
first show that εcan be bounded in terms of estimation error of the optimal strategy given the offline data.
Lemma 3.2. Letˆπ∗be any estimator of π∗constructed from D0, then P(˜π1̸=π∗)≤2P(ˆπ∗̸=π∗).
Proof.If˜π1̸=π∗, then either ˆπ∗̸= ˜π1orˆπ∗̸=π∗must be true. Conditioning on D0,ˆπ∗is identically
distributed as π∗while ˆπ∗is deterministic, therefore
P(˜π1̸=π∗)≤P(ˆπ∗̸= ˜π1) +P(ˆπ∗̸=π∗) = 2P(ˆπ∗̸=π∗)
5Published in Transactions on Machine Learning Research (10/2023)
We now bound the estimation error of π∗. We assume the following about the prior distribution of θ∗.
Assumption 3.3. There exists a ∆>0such that for all θ∈Θ,h= 0,1,···,H−1, ands∈S, there exists
ana∗∈Asuch thatQh(s,a∗;θ)≥Qh(s,a′;θ) + ∆,∀a′∈A\{a∗}.
Defineph(s;θ) :=Pθ,π∗(θ)(sh=s), whereπ∗(θ)denotes the optimal policy under model θ.
Assumption 3.4. The infimum probability of any reachable non-final state , defined as
p:= inf{ph(s;θ) : 0≤h<H,s∈S,θ∈Θ,ph(s;θ)>0}
satisfiesp>0.
We now describe a procedure to construct an estimator of the optimal policy, ˆπ∗fromD0so that P(π∗̸= ˆπ∗)
is small. Fix an integer L, and choose a δ∈(0,1). For each θ∈Θ, define a deterministic Markov policy
π∗(θ) = (π∗
h(·;θ))H−1
h=0sequentially through
π∗
h(s;θ) =/braceleftigg
arg maxaQh(s,a;θ),ifPθ,π∗
0:h−1(θ)(sh=s)>0
¯a0, ifPθ,π∗
0:h−1(θ)(sh=s) = 0,(5)
where the tiebreaker for the argmax operation is based on a fixed order on actions, and ¯a0∈Ais a fixed
action inA. It is clear that π∗(θ)is an optimal policy for the MDP θ. Furthermore, for those states that are
impossible to be visited, we choose to take a fixed action ¯a0. Although the choice of action at those states
doesn’t matter, our construction will be helpful for the proofs.
Construction of ˆπ∗:LetNh(s)(resp.Nh(s,a)) be the number of times state s(resp. state-action pair
(s,a)) appears at time hin datasetD0. Define ˆπ∗to be such that:
•ˆπ∗
h(s) = arg max a∈ANh(s,a)(ties are broken through some fixed ordering of actions) whenever
Nh(s)≥δL;
•ˆπ∗
h(s) = ¯a0wheneverNh(s)<δL.¯a0is a fixed action in Athat was used in the definition of π∗(θ).
The idea of the proof is that for sufficiently large βandL, we can choose a δ∈(0,1)such that
•Claim 1: Ifs∈Sis probable at time hunderπ∗(θ), thenNh(s)≥δLwith large probability.
Furthermore, π∗
h(s) = arg max a∈ANh(s,a)with large probability as well.
•Claim 2: Ifs∈Sis improbable at time hunderπ∗(θ), thenNh(s)<δLwith large probability;
Given the two claims, we can then conclude that the probability of π∗̸= ˆπ∗is small via a standard union
bound argument. The arguments are formalized in Lemma A.2. We now present the upper bound on
Bayesian regret for Algorithm 1: iPSRL .
Theorem 3.5. For sufficiently large βindependent of L, we have
BRT(ϕiPSRL) =O(/radicalbig
εLH4S2AT(1 + log(T))). (6)
where
εL= min/braceleftigg
1,2SH/bracketleftigg
exp/parenleftigg
−Lp2
18/parenrightigg
+ exp/parenleftbigg
−Lp
36/parenrightbigg/bracketrightigg/bracerightigg
.
Proof.The result follows from Lemma 3.1, Lemma 3.2, and Lemma A.2.
We note that the right-hand side of Eq. (6) converges to zero exponentially fast as N=LH→∞.
Remark3.6.(a) For fixed N, and large SandA, the regret bound is ˜O(H2S√
AT), which possibly could be
improved in H. (b) For a suitably large β, the regret bound obtained goes to zero exponentially fast as L,
the number of episodes in the offline dataset, goes to infinity thus indicating the online learning algorithm’s
ability to learn via imitation of the expert.
6Published in Transactions on Machine Learning Research (10/2023)
4 Approximating iPSRL
4.1 The Informed RLSVI Algorithm
The iPSRLalgorithm introduced in the previous section assumes that posterior updates can be done exactly.
In practice, the posterior update in Eq. (3) is challenging due to the loss of conjugacy while using the Bayes
rule. Thus, we must find a computationally efficient way to do approximate posterior updates (and obtain
samples from it) to enable practical implementation. Hence, we propose a novel approach based on Bayesian
bootstrapping to obtain approximate posterior samples. The key idea is to perturb the loss function for
the maximum a posterior (MAP) estimate and use the point estimate as a surrogate for the exact posterior
sample.
Note that in the ensuing, we regard βas also unknown to the learning agent (and λ=∞for simplicity).
Thus, the learning agent must form a belief over both θandβvia a joint posterior distribution conditioned
on the offline dataset D0and the online data at time t,Ht. We denote the prior pdf over θbyf(·)and prior
pdf overβbyf2(·).
For the sake of compact notation, we denote Q∗
h(s,a;θ)asQθ
h(s,a)in this section. Now, consider the offline
dataset,
D0={((sl
h,al
h,ˇsl
h,rl
h)H−1
h=0)L
l=1}
and denote θ= (θh)H−1
h=0. We introduce the temporal difference error El
h(parameterized by a given Qθ),
El
h(Qθ) :=/parenleftbigg
rl
h+ max
bQθ
h+1(ˇsl
h,b)−Qθ
h(sl
h,al
h)/parenrightbigg
.
We will regard Qθ
hto only be parameterized by θh, i.e.,Qθh
hbut abuse notation for the sake of simplicity.
We use this to construct a parameterized offline dataset ,
D0(Qθ) ={((sl
h,al
h,ˇsl
h,El
h(Qθ))h=0:H−1)l=1:L}.
A parametrized online dataset Ht(Qθ)after episode tcan be similarly defined. To ease notation, we will
regard thejth episode during the online phase as the (L+j)th observed episode. Thus,
Ht(Qθ) ={((sk
h,ak
h,ˇsk
h,Ek
h(Qθ))h=0:H−1)k=L+1:L+t},
the dataset observed during the online phase by episode t.
Note thatQθis to be regarded as a parameter. Now, at time t, we would like to obtain a MAP estimate
for(θ,β)by solving the following:
MAP: arg max
θ,βlogP(Ht(Qθ)|D0(Qθ),θ,β) + logP(D0(Qθ)|θ,β) + logf(θ) + logf2(β).(7)
Denote a perturbed version of the Qθ-parameterized offline dataset by
˜D0(Qθ) ={((sl
h,˜al
h,ˇsl
h,˜El
h)h=0:H−1)l=1:L}
where random perturbations are added: (i) actions have perturbation wh
l∼exp(1), (ii) rewards have per-
turbationszl
h∼N(0,σ2), and (iii) the prior ˜θ∼N(0,Σ0).
Note that the first and second terms involving HtandD0in Eq. (7) are independent of βwhen conditioned
on the actions. Thus, we have a sum of log-likelihood of TD error, transition and action as follows:
logP(˜D0(Qθ)|Qθ
0:H) =L/summationdisplay
l=1/summationdisplay
h/parenleftig
logP(˜El
h|ˇsl
h,al
h,sl
h,Qθ
0:H)+ logP(ˇsl
h|al
h,sl
h,Qθ
0:H) + logP(al
h|sl
h,Qθ
0:H)/parenrightig
≤L/summationdisplay
l=1/summationdisplay
h/parenleftig
logP(˜El
h|ˇsl
h,al
h,sl
h,Qθ
h:h+1) + logπβ
h(al
h|sl
h,Qθ
h)/parenrightig
.
7Published in Transactions on Machine Learning Research (10/2023)
By ignoring the log-likelihood of the transition term (akin to optimizing an upper bound on the negative
loss function), we are actually being optimistic .
For the terms in the upper bound above, under the random perturbations assumed above, we have
logP(˜El
h|ˇsl
h,al
h,sl
h,Qθ
h:h+1) =−1
2/parenleftbigg
rl
h+zl
h+ max
bQθ
h+1(ˇsl
h,b)−Qθ
h(sl
h,al
h)/parenrightbigg2
+constant
and
logπβ
h(al
h|sl
h,Qθ
h) =wl
h/parenleftigg
βQθ
h(sl
h,al
h)−log/summationdisplay
bexp/parenleftbig
βQθ
h(sl
h,b)/parenrightbig/parenrightigg
.
Now, denote a perturbed version of the Qθ-parametrized online dataset,
˜Ht(Qθ) ={((sk
h,ak
h,ˇsk
h,˜Ek
h)h=0:H−1)k=L+1:L+t},
and thus similar to before, we have
logP(˜Ht(Qθ)|˜D0(Qθ),Qθ
0:H) =L+t/summationdisplay
k=L+1/summationdisplay
h/parenleftig
logP(˜Ek
h(Qθ)|ˇsk
h,ak
h,sk
h,Qθ
0:H)+ logP(ˇsk
h|ak
h,sk
h,Qθ)/parenrightig
,
≤L+t/summationdisplay
k=L+1/summationdisplay
h/parenleftbig
logP(˜Ek
h|ˇsk
h,ak
h,sk
h,Qθ
h:h+1)/parenrightbig
,
where we again ignored the transition term to obtain an optimistic upper bound.
Given the random perturbations above, we have
logP(˜Ek
h(Qθ)|ˇsk
h,ak
h,sk
h,Qθ
h:h+1) =−1
2/parenleftbigg
rk
h+zk
h+ max
bQθ
h+1(ˇsk
h,b)−Qθ
h(sk
h,ak
h)/parenrightbigg2
+constant.
Thepriorover β,f2(β)isassumedtobeanexponentialpdf λ2exp(−λ2β),β≥0, whilethatover θisassumed
Gaussian. Thus, putting it all together, we get the following optimistic loss function (to minimize over
θandβ),
˜L(θ,β) =1
2σ2L+t/summationdisplay
k=1H−1/summationdisplay
h=0/parenleftbigg
rk
h+zk
h+ max
bQθ
h+1(ˇsk
h,b)−Qθ
h(sk
h,ak
h)/parenrightbigg2
−L/summationdisplay
l=1H−1/summationdisplay
h=0wl
h/parenleftigg
βQθ
h(sl
h,al
h)−log/summationdisplay
bexp/parenleftbig
βQθ
h(sl
h,b)/parenrightbig/parenrightigg
+1
2(θ−˜θ)⊤Σ0(θ−˜θ) +λ2β.(8)
The above loss function is difficult to optimize in general due to the maxoperation, and the Q-value function
in general having a nonlinear form.
NowAlgorithm 2: iRLSVI can be summarized by replacing steps (A1)-(A2) in Algorithm 1: iPSRL
by (B1)-(B2), with the other steps being the same:
(B1) Solve the MAP Problem (7) by minimizing loss function (8).
(B2) Get solutions (˜θl,˜βl).
Remark4.1.Note that the loss function in Eq. (8) can be hard to jointly optimize over θandβ. In particular,
estimates of βcan be quite noisy when βis large, and the near-optimal expert policy only covers the state-
action space partially. Thus, we consider other methods of estimating βthat are more robust, which can then
be plugged into the loss function in Eq. (8). Specifically, we could simply look at the entropy of the empirical
8Published in Transactions on Machine Learning Research (10/2023)
distribution of the action in the offline dataset. Suppose the empirical distribution of {¯al
0,...¯al
H}L
l=1isµA.
Then we use c0/H(µA)as an estimation for β, wherec0>0is a hyperparameter. The intuition is that
for smaller β, the offline actions tend to be more uniform and thus the entropy will be large. This is an
unsupervised approach and agnostic to specific offline data generation process.
Remark4.2.In the loss function in Eq. (8), the parameter θappears inside the maxoperation. Thus, it can
be quite difficult to optimize over β. Since the loss function is typically optimized via an iterative algorithm
such as a gradient descent method, a simple and scalable solution that works well in practice is to use the
parameter estimate θfrom the previous iteration inside the maxoperation, and thus optimize over θonly
in the other terms.
4.2 iRLSVI bridges Online RL and Imitation Learning
In the previous subsection, we derived iRLSVI, a Bayesian-bootstrapped algorithm. We now present inter-
pretation of the algorithm as bridging online RL (via commonality with the RLSVI algorithm (Osband et al.,
2016a) and imitation learning, and hence a way for its generalization.
Consider the RLSVI algorithm for online reinforcement learning as introduced in (Osband et al., 2019). It
draws its inspiration from the posterior sampling principle for online learning, and has excellent cumulative
regret performance. RLSVI, that uses all of the data available at the end of episode t, including any offline
dataset involves minimizing the corresponding loss function at each time step:
˜LRLSVI (θ) =1
2σ2L+t/summationdisplay
k=1H−1/summationdisplay
h=0/parenleftbigg
rk
h+ max
bQθ
h+1(ˇsk
h,b)−Qθ
h(sk
h,ak
h)/parenrightbigg2
+1
2(θ0:H−˜θ0:H)⊤Σ0(θ0:H−˜θ0:H).
Now, let us consider an imitation learning setting. Let τl= (sl
h,al
h,ˇsl
h)H−1
h=0be the trajectory of the lth
episode. Let ˆπh(a|s)denote the empirical estimate of probability of taking action ain statesat timeh, i.e.,
an empirical estimate of the expert’s randomized policy. Let p(τ)denote the probability of observing the
trajectory under the policy ˆπ.
Letπβ,θ
h(·|s)denote the parametric representation of the policy used by the expert. And let pβ,θ(τ)denote
the probability of observing the trajectory τunder the policy πβ,θ. Then, the loss function corresponding to
the KL divergence between ΠL
l=1p(τl)andΠL
l=1pβ,θ(τl)is given by
˜LIL(β,θ) =DKL/parenleftbig
ΠL
l=1p(τl)||ΠL
l=1pβ,θ(τl)/parenrightbig
=/integraldisplay
ΠL
l=1p(τl) logΠL
l=1p(τl)
ΠL
l=1pβ(τl)=L/summationdisplay
l=1/integraldisplay
p(τl) logp(τl)
pβ,θ(τl),
=L/summationdisplay
l=1H−1/summationdisplay
h=0logˆπh(al
h|sl
h)
πβ,θ
h(al
h|sl
h)
=L/summationdisplay
l=1H−1/summationdisplay
h=0[log ˆπh(al
h|sl
h)−logπβ,θ
h(al
h|sl
h)]
=−L/summationdisplay
l=1H−1/summationdisplay
h=0/parenleftigg
βQθ
h(sl
h,al
h)−log/summationdisplay
bexp/parenleftbig
βQθ
h(sl
h,b)/parenrightbig/parenrightigg
+constant.
Remark 4.3.(i) The loss function ˜LIL(β,θ)is the same as the second (action-likelihood) term in Eq. (8)
while the loss function ˜LRLSVI (θ)is the same as the first and third terms there (except for perturbation) and
minus theλ2βterm that corresponds to the prior over β. (ii) Note that while we used the more common
KL divergence for the imitation learning loss function, use of log loss would yield the same outcome.
Thus, the iRLSVI loss function can be viewed as
˜L(β,θ) =˜LRLSVI (θ) +˜LIL(β,θ) +λ2β, (9)
thus establishing that the proposed algorithm may be viewed as bridging Online RL with Imitation Learning.
Note that the last term corresponds to the prior over β. Ifβis known (or uniform), it will not show up in
the loss function above.
9Published in Transactions on Machine Learning Research (10/2023)
The above also suggests a possible way to generalize and obtain other online learning algorithms that can
bootstrap by use of offline datasets. Namely, at each step, they can optimize a general loss function of the
following kind:
˜Lα(β,θ) =α˜LORL(θ) + (1−α)˜LIL(β,θ) +λ2β, (10)
where ˜LORLis a loss function for an Online RL algorithm, ˜LILis a loss function for some Imitation Learning
algorithm, and factor α∈[0,1]provides a way to tune between emphasizing the offline imitation learning
and the online reinforcement learning.
5 Empirical Results
Experimentalsetup. Wenowpresentsomeempiricalresultsintwoprototypicalenvironments: “DeepSea”
and “Maze”. Specifically, we compare three variants of the RLSVI agents, which are respectively referred to
asinformed RLSVI ( iRLSVI),partially informed RLSVI ( piRLSVI), anduninformed RLSVI ( uRLSVI). All
three agents are tabular RLSVI agents with similar posterior sampling-type exploration schemes. However,
they differ in whether or not and how to exploit the offline dataset. In particular, uRLSVIignores the offline
dataset; piRLSVI exploits the offline dataset but does not utilize the information about the generative policy;
while iRLSVIfully exploits the information in the offline dataset, about both the generative policy and the
reward feedback. Please refer to Appendix D.1 for the pseudo-codes of these agents. We note no other
algorithms are known for the problem as posed.
Deep Sea (Figure 1 (i))(Osband et al., 2019) is an episodic reinforcement learning problem with state space
S={0,1,...,M}2and , where Mis its size. The state at period hin episodetisst
h= (xt
h,dt
h)∈S, where
xt
h= 0,1,...,Mis the horizontal position while dt
h= 0,1,...,Mis the depth (vertical position). Its action
space isA={left,right}and time horizon length is H=M. Its reward function is as follows: If the
agent chooses an action rightin periodh < H, then it will receive a reward −0.1/M, which corresponds
to a “small cost”; If the agent successfully arrives at state (M,M )in periodH=M, then it will receive
a reward 1, which corresponds to a “big bonus”; otherwise, the agent will receive reward 0. The system
dynamics are as follows: for period h < H, the agent’s depth in the next period is always increased by 1,
i.e.,dt
h+1=dt
h+ 1. For the agent’s horizontal position, if at
h=left, thenxt
h+1= max{xt
h−1,0}, i.e., the
agent will move left if possible. On the other hand, if at
h=right, then we have xt
h+1= min{xt
h+ 1,M}
with prob. 1−1/Mandxt
h+1=xt
hwith prob. 1/M. The initial state of this environment is fixed at state
(0,0). In this section, we fix the size of Deep Sea as M= 10.
Figure 1: (i) DeepSea Environment with start state in top left corner, and goal state in bottom right corner.
(ii) The map for the Maze environment.
Maze(Figure 1 (ii)) is also an episodic reinforcement learning problem, which is a variant of a maze problem
proposed in D4RL (Fu et al., 2020). To fit this problem into the finite-state framework of this paper, we
discretize the locations in Figure 1 (ii) into a 6×6grid. Note that excluding the walls, there are 26valid
locations in the maze. We set the time horizon of this problem as H= 7. Since each state is a location-period
10Published in Transactions on Machine Learning Research (10/2023)
pair, there are 26×7 = 182states in this problem. Moreover, we assume that this problem has a fixed initial
state: in each episode, the agent starts at the location denoted by the green dot in Figure 1 (ii).
We assume that the action space for Maze is A={left,right,up,down,stay}. At each period h<H−1,
if the agent takes action left, and it is also feasible to go left (i.e. the agent does not hit the wall if going
left), then the agent will successfully go left with probability 0.95, and fail to go left and stay at the same
location with probability 0.05. On the other hand, if it is infeasible to go left, then the agent will stay at
the same location with probability 1. The state transitions under action right,up, and downare defined
similarly, with the same success/failure probabilities. Finally, if the agent takes action stay, then it will stay
at the same location with probability 1. The reward function of Maze is as follows: if the agent is at the
target location, which is denoted by the red dot in Figure 1 (ii), then the agent will receive a “big bonus" of
reward 1; otherwise, the agent will incur a “small cost", which is reward −0.01.
In both environments, the offline dataset is generated based on the expert’s policy specified in Eq. (2), and
we assumeβ(s) =β(a constant) across all states. We set the size of the offline dataset D0as|D0|=κ|A||S|,
whereκ≥0is referred to as data ratio .
Experimentalresults. WeruntheexperimentsonDeepSeafor T= 300episodes, andruntheexperiments
on Maze for T= 200episodes. For both environments, the empirical cumulative regrets are averaged over 50
simulations. The experimental results are illustrated in Figure 2 and 3, as well as Figure 5 in Appendix D.2
and Figure 6 in Appendix D.3.
0.1 1 10 1006080100Regret(T)data_ratio: 1.0
agent
iRLSVI
piRLSVI
uRLSVI
0.1 1 10 100
6080100data_ratio: 5.0
Figure 2: Cumulative regret vs. βin Deep Sea.
0.1 1 10 100100200300data_ratio: 1.0
0.1 1 10 100100200300data_ratio: 5.0
Regret(T)agent
iRLSVI
piRLSVI
uRLSVI
Figure 3: Cumulative regret vs. βin Maze.
Specifically, Figure 2 plots the cumulative regret on Deep Sea in the first T= 300episodes as a function
of the expert’s deliberateness β, for two different data ratios, κ= 1,and 5. There are several interesting
observations based on Figure 2: (i) Figure 2 shows that iRLSVIandpiRLSVI tend to perform much better
than uRLSVI, which demonstrates the advantages of exploiting the offline dataset, and this improvement
tends to be more dramatic with a larger offline dataset. (ii) When we compare iRLSVIandpiRLSVI, we note
11Published in Transactions on Machine Learning Research (10/2023)
0.1 1 10 10080859095100Regret(T)data_ratio: 1.0
agent
iRLSVI
piRLSVI
uRLSVI
0.1 1 10 100
708090100data_ratio: 5.0
Figure 4: Robustness of iRLSVIto misspecification.
that their performance is similar when βis small, but iRLSVIperforms much better than piRLSVI whenβ
is large. This is because when βis small, the expert’s generative policy does not contain much information;
and asβgets larger, it contains more information and eventually it behaves like imitation learning and
learns the optimal policy as β→∞. Note that the error bars denote the standard errors of the empirical
cumulative regrets, hence the improvements are statistically significant.
Similarly, Figure 3 plots the cumulative regret on Maze in the first T= 200episodes as a function of β,
also for data ratios κ= 1and5. Note that we have obtained similar results in this experiment, and the
improvements are also statistically significant.
Robustness to misspecification of β.We also investigate the robustness of various RLSVI agents with
respect to the possible misspecification of β. In particular, we demonstrate empirically that in the Deep Sea
environment with M= 10, with offline dataset is generated by an expert with deliberateness β= 5, the
iRLSVIagent is quite robust to moderate misspecification. Here, the misspecified deliberateness parameter
is denoted ˜β. The empirical results are illustrated in Figure 4, where the experiment is run for T= 300
episodes and the empirical cumulative regrets are averaged over 50simulations.
Since uRLSVIand piRLSVI do not use parameter ˜β, thus, as expected, their performance is constant over
˜β. On the other hand, iRLSVIexplicitly uses parameter ˜β. As Figure 4 shows, the performance of iRLSVI
does not vary much as long as ˜βhas the same order of magnitude as β. However, there will be significant
performance loss when ˜βis too small, especially when the data ratio is also small. This makes sense since
when ˜βis too small, iRLSVIwill choose to ignore all the information about the generative policy and
eventually reduces to piRLSVI. Similar results can be anticipated for the Maze environment and are omitted
to save space.
6 Conclusions
In this paper, we have introduced and studied the following problem: Given an offline demonstration dataset
from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in
MDPs. We have followed a principled approach and introduced two algorithms: the ideal iPSRLalgorithm,
and the iRLSVIalgorithm that is computationally practical and seamlessly bridges online RL and imitation
learning in a very natural way. We have shown significant reduction in regret both empirically, and theoret-
ically as compared to two natural baselines. The dependence of the regret bound on some of the parameters
(e.g.,H) could be improved upon, and is a good direction for future work. In future work, we will also
combine the iRLSVIalgorithm with deep learning to leverage offline datasets effectively for continuous state
and action spaces as well.
12Published in Transactions on Machine Learning Research (10/2023)
References
Alekh Agarwal and Tong Zhang. Model-based RL with optimistic posterior sampling: Structural conditions
and sample complexity. arXiv preprint arXiv:2206.07659 , 2022.
Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. arXiv preprint arXiv:2008.05556 ,
2020.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning.
Advances in neural information processing systems , 21, 2008.
Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with
offline data. arXiv preprint arXiv:2302.02948 , 2023.
Mark Beliaev, Andy Shih, Stefano Ermon, Dorsa Sadigh, and Ramtin Pedarsani. Imitation learning by
estimating expertise of demonstrators. Proceedings of the 39th International Conference on Machine
Learning , 162:1732–1748, 2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
Christoph Dann, Mehryar Mohri, Tong Zhang, and Julian Zimmert. A provably efficient model-free posterior
sampling method for episodic reinforcement learning. Advances in Neural Information Processing Systems ,
34:12040–12051, 2021.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: A new
approach for hard-exploration problems. arXiv preprint arXiv:1901.10995 , 2019.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then explore.
Nature, 590(7847):580–586, 2021.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal
of Machine Learning Research , 6, 2005.
Kuan Fang, Patrick Yin, Ashvin Nair, and Sergey Levine. Planning to practice: Efficient online fine-tuning
by composing goals in latent space. arXiv preprint arXiv:2205.08129 , 2022.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-
driven reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances
in neural information processing systems , 34:20132–20145, 2021.
ScottFujimoto, DavidMeger, andDoinaPrecup. Off-policydeepreinforcementlearningwithoutexploration.
InInternational conference on machine learning , pp. 2052–2062. PMLR, 2019.
Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline RL policies should be trained to be
adaptive. In International Conference on Machine Learning , pp. 7513–7530. PMLR, 2022.
Zhaohan Daniel Guo, Shantanu Thakoor, Miruna Pîslar, Bernardo Avila Pires, Florent Altché, Corentin Tal-
lec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byol-explore: Exploration
by bootstrapped prediction. arXiv preprint arXiv:2206.08332 , 2022.
Nicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang, Vikash Kumar, and Aravind Rajeswaran. Mo-
Dem: Accelerating visual model-based reinforcement learning with demonstrations. arXiv preprint
arXiv:2212.05698 , 2022.
BotaoHaoandTorLattimore. Regretboundsforinformation-directedreinforcementlearning. arXiv preprint
arXiv:2206.04640 , 2022.
13Published in Transactions on Machine Learning Research (10/2023)
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John
Quan, Andrew Sendonaris, Ian Osband, et al. Deep Q-learning from demonstrations. In Proceedings of
the AAAI Conference on Artificial Intelligence , volume 32, 2018.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal
large language models. arXiv preprint arXiv:2203.15556 , 2022.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline RL? In International
Conference on Machine Learning , pp. 5084–5096. PMLR, 2021.
Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning.
arXiv preprint arXiv:2110.06169 , 2021.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline rein-
forcement learning. Advances in Neural Information Processing Systems , 33:1179–1191, 2020.
Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. When should we prefer offline reinforcement
learning over behavioral cloning? arXiv preprint arXiv:2204.05618 , 2022.
Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online reinforcement
learning via balanced replay and pessimistic q-ensemble. In Conference on Robot Learning , pp. 1702–1712.
PMLR, 2022.
SergeyLevine, AviralKumar, GeorgeTucker, andJustinFu. Offlinereinforcementlearning: Tutorial, review,
and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. AWAC: Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359 , 2020.
ThanhNguyen-TangandRamanArora. Provablyefficientneuralofflinereinforcementlearningviaperturbed
rewards. arXiv preprint arXiv:2302.12780 , 2023.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (More) efficient reinforcement learning via posterior
sampling. Advances in Neural Information Processing Systems , 26, 2013.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped
DQN.Advances in neural information processing systems , 29, 2016a.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value
functions. In International Conference on Machine Learning , pp. 2377–2386. PMLR, 2016b.
Ian Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, et al. Deep exploration via randomized value
functions. J. Mach. Learn. Res. , 20(124):1–62, 2019.
YiOuyang,MukulGagrani,AshutoshNayyar,andRahulJain. LearningunknownMarkovdecisionprocesses:
A thompson sampling approach. Advances in neural information processing systems , 30, 2017.
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement
learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems ,
34:11702–11716, 2021.
Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. Operations
Research , 66(1):230–252, 2018.
Stefan Schaal. Learning from demonstration. Advances in neural information processing systems , 9, 1996.
Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou,
and David Silver. Online and offline reinforcement learning by planning with a learned model. Advances
in Neural Information Processing Systems , 34:27580–27591, 2021.
14Published in Transactions on Machine Learning Research (10/2023)
Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid
RL: Using both offline and online data can make rl efficient. arXiv preprint arXiv:2210.06718 , 2022.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv
preprint arXiv:2201.08239 , 2022.
Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Éric Moulines, Remi Munos, Alexey Naumov, Mark
Rowland, Michal Valko, and Pierre Ménard. Optimistic posterior sampling for reinforcement learning with
few samples and tight guarantees. arXiv preprint arXiv:2209.14414 , 2022.
Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial cov-
erage.arXiv preprint arXiv:2107.06226 , 2021.
MelVecerik,ToddHester,JonathanScholz,FuminWang,OlivierPietquin,BilalPiot, NicolasHeess,Thomas
Rothörl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learn-
ing on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817 , 2017.
Andrew Wagenmaker and Aldo Pacchiano. Leveraging offline data in online reinforcement learning. arXiv
preprint arXiv:2211.04974 , 2022.
RunzheWan,BranislavKveton,andRuiSong. Safeexplorationforefficientpolicyevaluationandcomparison.
arXiv preprint arXiv:2202.13234 , 2022.
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. Inequalities
for the L1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep , 2003.
Zheng Wen, Doina Precup, Morteza Ibrahimi, Andre Barreto, Benjamin Van Roy, and Satinder Singh. On
efficiency in hierarchical reinforcement learning. Advances in Neural Information Processing Systems , 33:
6708–6718, 2020.
TengyangXie, Ching-AnCheng, NanJiang, PaulMineiro, andAlekhAgarwal. Bellman-consistentpessimism
forofflinereinforcementlearning. Advances in neural information processing systems , 34:6683–6694, 2021a.
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-
efficient offline and online reinforcement learning. Advances in neural information processing systems , 34:
27395–27407, 2021b.
AndreaZanetteandRahulSarkar. Informationdirectedreinforcementlearning. Tech. Rep., Technical report,
Technical report , 2017.
15Published in Transactions on Machine Learning Research (10/2023)
A Auxiliary Lemmas
Lemma A.1. LetXbe the sum of Li.i.d. Bernoulli random variables with mean p∈(0,1). Letq∈(0,1),
then
P(X≤qL)≤exp/parenleftbig
−2L(q−p)2/parenrightbig
,ifq<p,
P(X≥qL)≤exp/parenleftbig
−2L(q−p)2/parenrightbig
,ifq>p.
Proof.Both inequalities can be obtained by applying Hoeffding’s Inequality.
Lemma A.2. Let∆andpbe as in Assumptions 3.3 and 3.4 respectively and let
β:= [log 3−logp+ log(H−1) + log(A−1)]/∆.
For anyβ≥βandN∈N, there exists an estimator ˆπ∗constructed from D0that satisfies
P(π∗̸= ˆπ∗)≤SH/bracketleftigg
exp/parenleftigg
−Lp2
18/parenrightigg
+ exp/parenleftbigg
−Lp
36/parenrightbigg/bracketrightigg
.
The proof is available in Appendix C.
B Proof of Lemma 3.1
Let˜θkthe environment sampled for the kth episode, and ˜πkbe the optimal strategy under ˜θk. Recall
thatHk−1is the data available to the iPSRL agent before the start of learning episode k. For notational
simplicity, let Pk(·) =P(·|Hk−1)andEk[·] =E[·|Hk−1]. We first prove the following Lemma.
Lemma B.1. P(˜πk̸=π∗)≤P(˜π1̸=π∗)for allk≥1.
Proof.Definef(x) =x(1−x).fis a concave function. We have
P(˜πk̸=π∗) =E[Pk[˜πk̸=π∗]] =E/bracketleftigg/summationdisplay
π∈ΠPk(˜πk=π,π∗̸=π)/bracketrightigg
(11)
=E/bracketleftigg/summationdisplay
π∈Πf(Pk(π∗=π))/bracketrightigg
(12)
=E/bracketleftigg/summationdisplay
π∈ΠE1[f(Pk(π∗=π))]/bracketrightigg
≤E/bracketleftigg/summationdisplay
π∈Πf(E1[Pk(π∗=π)])/bracketrightigg
(13)
=E/bracketleftigg/summationdisplay
π∈Πf(P1(π∗=π))/bracketrightigg
=E/bracketleftigg/summationdisplay
π∈ΠP1(˜π1=π,π∗̸=π)/bracketrightigg
(14)
=P(˜π1̸=π∗) (15)
Now we proceed to prove Lemma 3.1.
LetJθ
π:=Eθ,π[/summationtextH−1
h=0rh(sh,ah) +rH(sH)]denote the expected total reward under the environment θand
the Markov strategy π. Define
Zk:=Jθ∗
π∗−Jθ∗
˜πk (16)
˜Zk:=J˜θk
˜πk−Jθ∗
˜πk (17)
Ik:=1{˜πk̸=π∗} (18)
16Published in Transactions on Machine Learning Research (10/2023)
First, note that Zk=ZkIkwith probability 1, hence
Ek[Zk−˜ZkIk] =Ek[(Zk−˜Zk)Ik] =Ek[(Jθ∗
π∗−J˜θk
˜πk)Ik] (19)
=Ek[Jθ∗
π∗1{˜πk̸=π∗}]−Ek[J˜θk
˜πk1{π∗̸=˜πk}] = 0, (20)
where the last equality is true since θ∗and˜θkare independently identically distributed given Dk. Therefore,
we can write the Bayesian regret as E[/summationtextT
k=1˜ZkIk]. By Cauchy-Schwartz inequality, we have
E/bracketleftiggT/summationdisplay
k=1˜ZkIk/bracketrightigg
≤/radicaltp/radicalvertex/radicalvertex/radicalbt/parenleftiggT/summationdisplay
k=1E[I2
k]/parenrightigg/parenleftiggT/summationdisplay
k=1E[˜Z2
k]/parenrightigg
(21)
Using Lemma B.1, the first part can be bounded by
T/summationdisplay
k=1E[I2
k] =T/summationdisplay
k=1P(˜πk̸=π∗)≤TP(˜π1̸=π∗) =εT (22)
The rest of the proof provides a bound on/summationtextT
k=1E[˜Z2
k].
LetTθ
πhbe the Bellman operator at time hdefined byTθ
πhVh+1(s) :=rh(s,a)+/summationtext
s′∈SVh+1(s′)Pθ
h(s′|s,πh(s)).
Using Equation (6) of Osband et al. (2013), we have
˜Zk=Eθ∗,˜θk/bracketleftiggH−1/summationdisplay
h=0[T˜θk
˜πk
hV˜θk
h+1(s(k−1)H+h)−Tθ∗
˜πk
hV˜θk
h+1(s(k−1)H+h)]/bracketrightigg
(23)
For convenience, we write ˜Pk
h=P˜θk
h,P∗
h=Pθ∗
h,sk,h=s(k−1)H+h, andak,h=a(k−1)H+h= ˜πk
h(s(k−1)H+h).
Recall that the instantaneous reward satisfies rh(s,a)∈[0,1]. We have
|T˜θk
˜πk
hV˜θk
h+1(sk,h)−Tθ∗
˜πk
hV˜θk
h+1(sk,h)|≤H∥˜Pk
h(·|sk,h,ak,h)−P∗
h(·|sk,h,ak,h)∥1
Therefore,
E[˜Z2
k]≤HE/bracketleftiggH−1/summationdisplay
h=0[T˜θk
˜πk
hV˜θk
h+1(sk,h)−Tθ∗
˜πk
hV˜θk
h+1(sk,h)]2/bracketrightigg
(24)
≤H3E/bracketleftiggH−1/summationdisplay
h=0∥˜Pk
h(·|sk,h,ak,h)−P∗
h(·|sk,h,ak,h)∥2
1/bracketrightigg
(25)
where we used Cauchy-Schwartz inequality for the first inequality.
Following Osband et al. (2013), define Nk(s,a,h ) =/summationtextk−1
l=11{(sl,h,al,h)=(s,a)}to be the number of times (s,a)
was sampled at step hin the first (k−1)episodes.
Claim: For anyk,hand anyδ>0,
E[∥˜Pk
h(·|sk,h,ak,h)−P∗
h(·|sk,h,ak,h)∥2
1]≤4E/bracketleftbigg(2 log 2)S+ 2 log(1/δ)
max{Nk(sk,h,ak,h,h),1}/bracketrightbigg
+ 8(k−1)SAδ (26)
17Published in Transactions on Machine Learning Research (10/2023)
Given the claim, we have
T/summationdisplay
k=1E[˜Z2
k] (27)
≤H3/bracketleftigg
4E/bracketleftiggT/summationdisplay
k=1H−1/summationdisplay
h=0(2 log 2)S+ 2 log(1/δ)
max{Nk(sk,h,ak,h,h),1}/bracketrightigg
+T/summationdisplay
k=18(k−1)SAδ/bracketrightigg
(28)
≤4H3E/bracketleftiggT/summationdisplay
k=1H−1/summationdisplay
h=0(2 log 2)S+ 2 log(1/δ)
max{Nk(sk,h,ak,h,h),1}/bracketrightigg
+H3(4T2SAδ) (29)
= 8H3[(log 2)S+ log(1/δ)]E/bracketleftiggT/summationdisplay
k=1H−1/summationdisplay
h=0(max{Nk(sk,h,ak,h,h),1})−1/bracketrightigg
+ 4H3SAT2δ (30)
It remains to bound the expectation term in Eq. (30). We have
T/summationdisplay
k=1H−1/summationdisplay
h=0(max{Nk(sk,h,ak,h,h),1})−1=H−1/summationdisplay
h=0/summationdisplay
s∈S/summationdisplay
a∈ANT(s,a,h )−1/summationdisplay
j=01
max{j,1}(31)
≤H−1/summationdisplay
h=0/summationdisplay
(s,a):NT(s,a,h )>0(2 + log(NT(s,a,h )))≤HSA (2 + log(T)) (32)
Combining Eq. (30) and Eq. (32), setting δ=1
T2, we have
T/summationdisplay
k=1E[˜Z2
k] (33)
≤8H4SA[(log 2)S+ log(1/δ)] [2 + log(L)] + 4H3SAT2δ (34)
= 8H4SA[(log 2)S+ 2 log(T)] [2 + log(T)] + 4H3SA (35)
=O(H4S2A[1 + log(T)]2) (36)
Combining Eq. (21)Eq. (22)Eq. (36), we conclude that
BRT=O(√
εH4S2ATlog(1 +T)) (37)
Proof of Claim. Let ˇPn
h(·|s,a)be the empirical distribution of transitions after sampling (s,a)at steph
exactlyntimes. By the L1concentration inequality for empirical distributions Weissman et al. (2003), we
have
Pθ∗(∥ˇPn
h(·|s,a)−Pθ∗
h(·|s,a)∥1>ϵ)≤2Sexp/parenleftbigg
−1
2nϵ2/parenrightbigg
(38)
for anyϵ>0. Forδ>0, defineξ(n,δ) :=(2 log 2)S+2 log(1/δ)
max{n,1}, we have
Pθ∗/parenleftig
∥ˇPn
h(·|s,a)−Pθ∗
h(·|s,a)∥2
1>ξ(n,δ)/parenrightig
≤δ. (39)
Define ˆPk
h(·|s,a)to be the empirical distribution of transitions after sampling (s,a)at stephin the first
k−1episodes. Define
ˆΘk
h=/braceleftig
θ:∥ˆPk
h(·|s,a)−Pθ
h(·|s,a)∥2
1≤ξ(Nk(s,a,h ),δ)∀s∈S,a∈A/bracerightig
18Published in Transactions on Machine Learning Research (10/2023)
Then, we have
P(θ∗̸∈ˆΘk
h) (40)
≤P/parenleftig
∃n∈[k−1],s∈S,a∈A ∥ ˇPn
h(·|s,a)−Pθ∗
h(·|s,a)∥2
1>ξ(n,δ)/parenrightig
(41)
≤(k−1)SAδ (42)
Sinceθ∗and ˜θkare i.i.d. givenDk, and the random set ˆΘk
his measurable to Dk, we have P(θ∗̸∈ˆΘk
h) =
P(˜θk̸∈ˆΘk
h). Therefore we have
E/bracketleftbig
∥˜Pk
h(·|sk,h,ak,h)−P∗
h(·|sk,h,ak,h)∥2
1/bracketrightbig
(43)
≤E/bracketleftig
∥˜Pk
h(·|sk,h,ak,h)−P∗
h(·|sk,h,ak,h)∥2
11{θ∗,˜θk∈ˆΘk
h}/bracketrightig
+ (44)
+E/bracketleftig
∥˜Pk
h(·|sk,h,ak,h)−P∗
h(·|sk,h,ak,h)∥2
1/parenleftig
1{θ∗̸∈ˆΘk
h}+1{˜θk̸∈ˆΘk
h}/parenrightig/bracketrightig
(45)
≤E[4ξ(Nk(sk,h,ak,h,h),δ)] + 4/parenleftig
P(θ∗̸∈ˆΘk
h) +P(˜θk̸∈ˆΘk
h)/parenrightig
(46)
≤4E[ξ(Nk(sk,h,ak,h,h),δ)] + 8(k−1)SAδ (47)
C Proof of Lemma A.2
Proof.For convenience, write Pθ(·) =P(·|θ).
Define the event En,h={¯an,h̸=a∗
h(¯sn,h;θ)}, i.e. in the n-th round of demonstration, the expert did not
take the optimal action at time h. Given the expert’s randomized policy ϕ, we have
Pθ(En,h|¯sn,0:h,¯an,0:h−1) (48)
= 1−1
1 +/summationtext
a̸=a∗
h(¯sn,h;θ)exp(−β∆h(¯sn,h,a;θ))(49)
≤/summationdisplay
a̸=a∗
h(¯sn,h;θ)exp(−β∆h(¯sn,h,a;θ)) (50)
≤(A−1) exp(−β∆) =: ˜κβ. (51)
Defineκβ= (H−1)˜κβ. Thenβ≥βmeans that κβ≤p/3.
Consider each (h,s)∈{0,1,···,H−1}×S, conditioning on θthere are two cases:
•Ifph(s;θ)>0then
Pθ(¯sn,h=s)≥(1−˜κβ)hph(s;θ)≥(1−κβ)p≥2
3p. (52)
The first inequality in Eq. (52) can be established via induction on h: First observe that Pθ(¯sn,0=
s) =p0(s;θ)for alls∈Sby definition. Suppose that we have proved the statement for time h, i.e.
Pθ(¯sn,h=s)≥(1−˜κβ)hph(s;θ)for alls∈S. Then we have
ph+1(s′;θ) =/summationdisplay
s∈SPθ(s′|s,a∗
h(s;θ))ph(s;θ) (53)
Pθ(¯sn,h+1=s′)≥/summationdisplay
s∈Sϕβ
h(a∗
h(s;θ)|s;θ)Pθ(s′|s,a∗
h(s;θ))Pθ(¯sn,h=s) (54)
≥/summationdisplay
s∈S(1−˜κβ)Pθ(s′|s,a∗
h(s;θ))Pθ(¯sn,h=s) (55)
≥/summationdisplay
s∈S(1−˜κβ)h+1Pθ(s′|s,a∗
h(s;θ))ph(s;θ). (56)
19Published in Transactions on Machine Learning Research (10/2023)
The statement for h+ 1then follows by comparing Eq. (53) and Eq. (56), establishing the induction
step.
•Ifph(s;θ) = 0, then if ¯sn,h=s, the expert must have chosen some action that was not optimal
before time hin then-th round of demonstration. We conclude that
Pθ(¯sn,h=s)≤Pθ
h−1/uniondisplay
˜h=1En,˜h
≤h−1/summationdisplay
˜h=1Pθ(En,˜h)≤κβ≤1
3p. (57)
The above argument shows that there’s a separation of probability between two types of state and time index
pairs under the expert’s policy ϕβ(θ): the ones that are probable under the optimal policy π∗(θ)and the
ones that are not. Using this separation, we will proceed to show that when Lis large, we can distinguish
the two types of state and time index pairs through their appearance counts in D0. This will allow us to
construct a good estimator of π∗.
Define ˆπ∗to be the estimator of π∗constructed with δ=p/2. Ifˆπ∗̸=π, then either one of the following
cases happens
•There exists an (s,h)∈S×{ 0,1,···,H−1}pair such that ph(s;θ)>0butNh(s)<δL;
•There exists an (s,h)∈S×{ 0,1,···,H−1}pair such that ph(s;θ) = 0butNh(s)≥δL;
•There exists an (s,h)∈S×{ 0,1,···,H−1}pair such that ph(s;θ)>0andNh(s)≥δL, but
π∗
h(s;θ) =a∗
h(s;θ)̸= arg max aNh(s,a) = ˆπ∗
h(s);
Using union bound, we have
Pθ(π∗̸= ˆπ∗) (58)
≤/summationdisplay
(s,h):ph(s;θ)>0Pθ(Nh(s)<δL ) +/summationdisplay
(s,h):ph(s;θ)=0Pθ(Nh(s)≥δL) (59)
+/summationdisplay
(s,h):ph(s;θ)>0Pθ(Nh(s)≥δL,a∗
h(s;θ)̸= arg max
aNh(s,a)). (60)
LetBin(M,q)denote a binomial random variable with parameters M∈Nandq∈[0,1]. Notice that
conditioning on θ, eachNh(s)is a binomial random variable with parameters Land˜pθ,h(s) :=Pθ(¯s1,h=s).
Using equation 52 and Lemma A.1, we conclude that each term in the first summation of equation 60 satisfies
Pθ(Nh(s)<δL )≤P(Bin(L,2p/3)<(p/2)L) (61)
≤exp(−2L(p/6)2) = exp/parenleftigg
−Lp2
18/parenrightigg
. (62)
Using equation 57 and Lemma A.1, we conclude that each term in the second summation of equation 60
satisfies
Pθ(Nh(s)≥δL)≤P(Bin(L,κβ)≤(p/2)L) (63)
≤exp/parenleftbigg
−2L/parenleftigp
2−κβ/parenrightig2/parenrightbigg
≤exp/parenleftigg
−Lp2
18/parenrightigg
. (64)
20Published in Transactions on Machine Learning Research (10/2023)
Again, using Lemma A.1, each term in the third summation of equation 60 satisfies
Pθ(Nh(s)≥δL,a∗
h(s;θ)̸= arg max
aNh(s,a)) (65)
≤Pθ(a∗
h(s;θ)̸= arg max
aNh(s,a)|Nh(s)≥δL) (66)
≤Pθ(Nh(s)−Nh(s,a∗
h(s;θ))≥Nh(s)/2|Nh(s)≥δL) (67)
≤Pθ(Bin(Nh(s),˜κβ)≥Nh(s)/2|Nh(s)≥δL) (68)
≤Pθ(Bin(Nh(s),1/3)≥Nh(s)/2|Nh(s)≥δL) (69)
≤Eθ/bracketleftbigg
exp/parenleftbigg
−Nh(s)
18/parenrightbigg/vextendsingle/vextendsingle/vextendsingleNh(s)≥δL/bracketrightbigg
(70)
≤exp/parenleftbigg
−δL
18/parenrightbigg
= exp/parenleftbigg
−Lp
36/parenrightbigg
. (71)
Combining the above we obtain
Pθ(π∗̸∈ˆπ∗)≤SH/bracketleftigg
exp/parenleftigg
−Lp2
18/parenrightigg
+ exp/parenleftbigg
−Lp
36/parenrightbigg/bracketrightigg
. (72)
D Empirical Results
D.1 Pseudo-codes for RLSVI agents
In this appendix, we provide the pseudo-codes for iRLSVI,piRLSVI, and uRLSVIused in experiments in
Section 5, which are illustrated in Algorithm 2 and 3. Note that all three agents are tabular RLSVI agents
with similar posterior sampling-type exploration schemes. However, they differ in whether or not and how
to exploit the offline dataset. More precisely, they differ in use of the offline dataset D0when computing
the RLSVI loss ˜LRLSVI(see Algorithm 2 and 3), and if the total loss includes the loss associated with the
expert actions ˜LIR(see Algorithm 3). Table 1 summarizes the differences. In all experiments in this paper,
we choose the algorithm parameters σ2
0= 1,σ2= 0.1, andB= 20.
agent useD0in˜LRLSVI?include ˜LIR?
uRLSVI No No
piRLSVI Yes No
iRLSVI Yes Yes
Table 1: Differences between iRLSVI,piRLSVI, and uRLSVI.
D.2 More empirical results on Deep Sea
In this appendix, we provide more empirical results for the Deep Sea experiment described in Section 5.
Specifically, for Deep Sea with size M= 10, data ratio κ= 1,5, and expert’s deliberateness β= 1,10, we
plot the cumulative regret of iRLSVI,piRLSVI, and uRLSVIas a function of the number of episodes tfor
the firstT= 300episodes. The experiment results are averaged over 50simulations and are illustrated in
Figure 5.
As we have discussed in the main body of the paper, when both data ratio κand the expert’s deliberateness β
are small, then there are not many offline data and the expert’s generative policy is also not very informative.
In this case, iRLSVI,piRLSVI, and uRLSVIperform similarly. On the other hand, when the data ratio κis
large, iRLSVIandpiRLSVI tend to perform much better than uRLSVI, which does not use the offline dataset.
Similarly, when the expert’s deliberateness βis large, then the expert’s generative policy is informative. In
this case, iRLSVIperforms much better than piRLSVI anduRLSVI.
21Published in Transactions on Machine Learning Research (10/2023)
Algorithm 2 RLSVI agents for numerical experiments
Input:algorithm parameter σ2
0,σ2>0, deliberateness parameter β >0, offline datasetD0, offline buffer
sizeB, agent type agent
ifagent =uRLSVIthen
initialize data buffer Das an empty set
else
initialize data buffer D←D 0
end if
fort= 1,···,Tdo
sample state-action value function ˆQtbackwardly based on Algorithm 3
sample initial state st
0∼ν
forh= 0,···,H−1do
take action at
h∼Unif/parenleftig
arg maxaˆQt
h(st
h,a)/parenrightig
observe (st
h+1,rt
h)
append (st
h,at
h,h,st
h+1,rt
h)to the data buffer D
end for
end for
Algorithm 3 sample ˆQt
Input:algorithm parameter σ2
0,σ2>0, deliberateness parameter β >0, offline datasetD0, data buffer
D, offline buffer size B, agent type agent
setˆQt
H+1←0
forh=H,H−1,···,0do
LetQ∈ℜ|S|×|A|denote the decision variable, then we define the RLSVI loss function
˜LRLSVI (Q) =1
2/summationdisplay
d=(s,a,h′,s′,r)∈D/parenleftbigg
Q(s,a)−(r+ωd)−max
bˆQt
h+1(s′,b)/parenrightbigg2
+1
2σ2
0∥Q−Qprior∥2
F,
where for each data entry d= (s,a,s′,r)∈D,ωdis i.i.d. sampled from N(0,σ2). Each element in
Qprior∈ℜ|S|×|A|is i.i.d. sampled from N(0,σ2
0)and∥·∥Fdenotes the Frobenius norm.
ifagent =iRLSVIthen
sample a buffer of Boffline data tuple, B, without replacement from the offline dataset D0
defineBh={(s,a,h′,s,,r )∈B:h′=h}
define the loss function associated with expert actions as
˜LIL(Q) =/summationdisplay
(s,a,h′,s′,r)∈Bh/bracketleftigg
log/summationdisplay
bexp (βQ(s,b))−βQ(s,a)/bracketrightigg
choose ˆQt
h∈arg minQ/bracketleftbig˜LRLSVI (Q) +˜LIL(Q)/bracketrightbig
else
choose ˆQt
h∈arg minQ˜LRLSVI (Q)
end if
end for
Return ˆQt=/parenleftig
ˆQt
0,ˆQt
1,..., ˆQt
H/parenrightig
22Published in Transactions on Machine Learning Research (10/2023)
0 100 200 3000255075100cumulative regretbeta: 1.0
data_ratio: 1.0
agent
iRLSVI
piRLSVI
uRLSVI0 100 200 3000255075100beta: 1.0
data_ratio: 5.0
0 100 200 3000255075100beta: 10.0
data_ratio: 1.0
0 100 200 300
t0255075100beta: 10.0
data_ratio: 5.0
Figure 5: Cumulative regret vs. number of episodes in Deep Sea.
D.3 More empirical results on Maze
In this appendix, we provide more empirical results for the Maze experiment described in Section 5. Specif-
ically, for the Maze environment, data ratio κ= 1,5, and expert’s deliberateness β= 1,10, we plot the
cumulative regret of iRLSVI,piRLSVI, and uRLSVIas a function of the number of episodes tfor the first
T= 200episodes. The experiment results are averaged over 50simulations and are illustrated in Figure 6.
We have observed similar experiment results as the Deep Sea experiment.
23Published in Transactions on Machine Learning Research (10/2023)
0 50 100 150 2000100200300beta: 1.0
data_ratio: 1.0
0 50 100 150 2000100200300beta: 1.0
data_ratio: 5.0
0 50 100 150 2000100200300beta: 10.0
data_ratio: 1.0
0 50 100 150 2000100200300beta: 10.0
data_ratio: 5.0
tcumulative regretagent
iRLSVI
piRLSVI
uRLSVI
Figure 6: Cumulative regret vs. number of episodes in Maze.
24