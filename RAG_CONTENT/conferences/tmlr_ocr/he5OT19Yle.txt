Under review as submission to TMLR
Regret Bounds for Satisficing in Multi-Armed Bandit Prob-
lems
Anonymous authors
Paper under double-blind review
Abstract
This paper considers the objective of satisficing in multi-armed bandit problems. Instead of
aiming to find an optimal arm, the learner is content with an arm whose reward is above
a given satisfaction level. We provide algorithms and analysis for the realizable case when
such a satisficing arm exists as well as for the general case when this may not be the case.
Introducing the notion of satisficing regret , our main result shows that in the general case
it is possible to obtain constant satisficing regret when there is a satisficing arm (thereby
correcting a contrary claim in the literature), while standard logarithmic regret bounds can
be re-established otherwise. Experiments illustrate that our algorithm is not only superior
to standard algorithms in the satisficing setting, but also works well in the classic bandit
setting.
1 Introduction
One of the reasons why reinforcement learning (RL) is in general difficult is that finding an optimalpolicy in
general requires a lot of exploration. In practice however, we are often happy to perform a task just good
enough. For example, when driving to work we will be content with a strategy that will let us arrive just in
time, while the computation of a policy that is ‘optimal’ in some sense (e.g., along the shortest route, or as
fast as possible) may be prohibitive. Accordingly, it is to be expected that when considering a satisficing
objective aiming to find a solution that is above a certain satisfaction level it is possible to learn a respective
policy much faster.
Investigating the multi-armed bandit (MAB) setting, in this paper we introduce the notion of satisficing
regretthat measures the loss with respect to a given satisfaction level S. We first consider the realizable
case, where this level can be satisfied, that is, there is at least one arm whose expected reward is above the
satisfaction level. In this setting, quite a simple algorithm can be shown to have constant satisficing regret
(i.e., no dependence on the horizon T). For the general setting we provide an algorithm that is able to extend
this result, giving constant satisficing regret in the realizable case, while obtaining logarithmic bounds on the
ordinary regret with respect to the optimal arm as for classic MAB algorithms such as UCB1 (Auer et al.,
2002). Experiments not only confirm our theoretical findings but also show that our algorithm is competitive
even in the standard setting.
1.1 Setting
We consider the standard multi-armed bandit (MAB) setting with a set of Karms, in the following denoted
asJ1,KK:={1,2,...,K}. In discrete time steps t= 1,2,...the learner picks an arm At=ifrom J1,KKand
observes a random reward rtdrawn from a fixed reward distribution specific to the chosen arm iwith mean
µi. In the following we assume that the reward distributions for each arm are sub-Gaussian. This is e.g.
guaranteed when the reward distributions are bounded, which is a common assumption in the bandit setting.
1Under review as submission to TMLR
The usual performance measure for a learning algorithm in the MAB setting is the (pseudo-)regret afterT
steps, defined as
RT:=T/summationdisplay
t=1/parenleftbig
µ∗−E[µAt]/parenrightbig
,
whereµ∗:= maxiµiis the maximal mean reward over all arms.
In the satisficing setting however, we only care about whether an arm with mean reward ⩾Sis chosen, where
Sis the level of satisfaction we aim at. Accordingly, we modify the classic notion of regret and consider what
we call the satisficing (pseudo-)regret with respect to S(shortS-regret) defined as
RS
T:=T/summationdisplay
t=1max/braceleftbig
S−E[µAt],0/bracerightbig
.
This definition reflects that we are happy with any arm having mean reward ⩾Sand that there is no benefit
in overfulfilling the given satisfaction level S. Note that the S-regret will be linear in Twhenever there is no
satisficing arm with mean reward ⩾S, that is, if µ∗<S. As will be discussed below, S-regret is a special
case of the notion of expected satisficing regret as considered in a more general Bayesian setting introduced by
Reverdy et al. (2017).
1.2 Related Work
While there are some connections to multi-criterion RL (Roijers et al., 2013), there is hardly any literature on
satisficing in RL, with a few exceptions for the MAB setting that we also consider. Kohno and Takahashi (2017)
and Tamatsukuri and Takahashi (2019) propose simple index policies, which are experimentally evaluated.
Tamatsukuri and Takahashi (2019) also show that the suggested algorithm converges to a satisficing arm
and that the regret is finite if the satisfaction level is chosen to be between the reward of the best and the
second-best arm.
Reverdy et al. (2017) consider a more general Bayesian setting, which also considers the learner’s belief that
some arm is satisficing. The notion of expected satisficing regret is introduced that measures the loss over all
steps where a non-satisficing arm is chosen and the learner’s degree of belief in the chosen arm was below
some levelδ∈[0,1]. Forδ= 0this coincides with our notion of satisficing regret as defined above. Reverdy
et al. (2017) present various bounds on the expected satisficing regret, including lower bounds as well as
upper bounds for problems with Gaussian reward distributions when using adaptations of the UCL algorithm
(Reverdy et al., 2014). The given bounds for the case δ= 0that correspond to our setting will be discussed
in Section 2 below.
A line of reseach that pursues similar ideas as our setting of satisficing is that of conservative bandits . Here
the learner has an arm at her disposal that provides a baseline level (similar to our satisfaction level) one
would not like to fall below, while trying to converge to an optimal arm. Thus Wu et al. (2016) present an
algorithm that on the one hand with high probability stays above the baseline level at all time steps (with a
certain amount of allowed error α) and on the other hand has regret bounded similar to standard bandit
algorithms (but with an additional dependence on α).
Merlis and Mannor (2021) consider a related notion of so-called lenient regret that considers the loss with
respect toµ∗−εfor a parameter ε>0that specifies the allowed deviation from the optimal mean reward µ∗.
The definition of lenient regret formally depends on a so-called ε-gap function. When choosing this function
to be the hinge loss, lenient regret corresponds to S-regret when choosing S:=µ∗−ε. Merlis and Mannor
(2021) show asymptotic upper bounds on the lenient regret for a version of Thompson sampling (Thompson,
1933) that match a given lower bound. When µ∗>1−εthe lenient regret turns out to be constant. This
resembles the results we have for S-regret, which in Theorems 1 and 2 below is shown to be constant when
there is a satisficing arm. However, the results are not equivalent: The lenient regret is with respect to the
valueµ∗−εand constant regret is only obtained when µ∗itself isε-close to the theoretical maximum reward 1.
On the other hand, considering an absolute satisfaction level Swe obtain constant S-regret whenever µ∗>S.
This holds in particular when Sis chosen to be µ∗−εand without further assumptions on µ∗.
2Under review as submission to TMLR
Russo and Roy (2018) consider satisficing in a setting with discounted rewards and provide respective bounds
on the expected discounted regret for a satisficing variant of Thompson sampling.
Also related to our paper, Kano et al. (2019) consider the problem of identifying allarms above a given
satisfaction level and derive sample complexity bounds for the pure-exploration setting with fixed confidence.
Related sample complexity bounds can be found in (Mason et al., 2020) for identification of all ε-good arms.
Closer to our setting is the problem of identifying an arbitrary arm among the top marms, for which sample
complexity bounds are derived by Chaudhuri and Kalyanakrishnan (2017). A follow-up paper (Chaudhuri
and Kalyanakrishnan, 2019) considers the sample complexity of the more general problem of identification of
anykof the best marms. None of these latter investigations however considers the online learning setting
with regret as performance measure as we do. Note that an algorithm for pure exploration (Audibert et al.,
2010) after any number of steps with high probability will identify an optimal or at least a satisficing arm.
However, subsequent exploitation will always give linear regret due to the small but positive error probability
so that a simple approach of first exploring and then exploiting does not work well in general.
2 The Realizable Case
We start with the realizable case whenµ∗> S. The main goal of this section is to show that suitable
algorithms will have just constant S-regret in this case. Note that this does not hold for standard algorithms
like UCB1 (Auer et al., 2002). Lower bounds show that these algorithms will choose a suboptimal arm ifor
Ω/parenleftig
logT
(µ∗−µi)2/parenrightig
times. This of course also holds for any arm below the satisfaction level Sgiving a contribution
to the overall S-regret of Ω/parenleftig
(S−µi) logT
(µ∗−µi)2/parenrightig
.
2.1 Simple Algorithm
We start with the simple algorithm Simple-Sat shown as Algorithm 1. It plays the empirical best arm so
far if its empirical mean reward is ⩾Sand explores uniformly at random otherwise. In the following, the
empirical reward for arm iavailable at step t(i.e.,beforechoosing the arm At) is denoted by ˆµi(t).
Algorithm 1 :Simple-Sat (Simple Algorithm for Satisficing in the Realizable Case)
Require:K,S
1:Play each arm once, i.e., for time steps t= 1,...,Kplay armAt=t.
2:fortime stepst=K+ 1,...do
3: if∃iˆµi(t)⩾Sthen
4: PlayAt←arg maxi∈J1,KKˆµi(t).
5: else
6: ChooseAtuniformly at random from J1,KK.
7: end if
8:end for
Analogously to the ordinary MAB setting where the gaps ∆i:=µ∗−µito the optimal arm appear in bounds
on the (classic) regret, when satisficing the gaps ∆S
i=S−µifor non-satisficing arms as well as |∆S
∗|=µ∗−S
are important parameters describing the difficulty of the problem. Indeed, one can show the following bound
on theS-regret.
Theorem 1. IfS <µ∗thenSimple-Sat satisfies for all T⩾1,
RS
T⩽/summationdisplay
i:∆S
i>0/parenleftig
∆S
i+2
∆S
i+2∆S
i
|∆S∗|2/parenrightig
.
For the proof we shall need the following result that follows by our assumption of sub-Gaussianity and a
Chernoff bound.
3Under review as submission to TMLR
Lemma 1. Letˆµi,nbe an empirical estimate for µicomputed from nsamples. Then for all ε>0and each
i∈J1,KK,
P(ˆµi,n⩾µi+ε)⩽exp(−nε2
2),
P(ˆµi,n⩽µi−ε)⩽exp(−nε2
2).
Proof of Theorem 1. Letibe the index of a non-satisficing arm. In the following we decompose the event
that armiis chosen at some step t. To do that we introduce the event Zt:={∀j∈J1,KK,ˆµj(t)<S}that
all arms have empirical estimates below S, when the algorithm chooses an arm randomly according to line 6
of the algorithm. Then we have
{At=i}⊂{t=i}∪{At=i,Zc
t}∪{At=i,Zt}. (1)
For the first two events we have
T/summationdisplay
t=1P(t=i)⩽1 (2)
and
T/summationdisplay
t=1P(At=i,Zc
t)⩽T/summationdisplay
t=1P/parenleftbig
At=i,ˆµi(t)⩾S/parenrightbig
=T/summationdisplay
t=1P/parenleftbig
At=i,ˆµi(t)⩾µi+ ∆S
i/parenrightbig
⩽T/summationdisplay
n=1P(ˆµi,n⩾µi+ ∆S
i)
⩽T/summationdisplay
n=1exp/parenleftbig
−n(∆S
i)2
2/parenrightbig
⩽e−(∆S
i)2
2
1−e−(∆S
i)2
2
⩽2
(∆S
i)2. (3)
Rewriting the probability of the third event in eq. 1, using ∗to refer to an arbitrary optimal arm, we obtain
P(At=i,Zt) =P(At=i|Zt)P(Zt) =1
K·P(Zt)
=P(At=∗|Zt)P(Zt) =P(At=∗,Zt).
Now summing over the time steps up to Tyields
T/summationdisplay
t=1P(At=i,Zt) =T/summationdisplay
t=1P(At=∗,Zt)
⩽T/summationdisplay
t=1P/parenleftbig
At=∗,ˆµ∗(t)⩽S/parenrightbig
=E/parenleftigT/summationdisplay
t=11/braceleftbig
At=∗,ˆµ∗(t)⩽S/bracerightbig/parenrightig
⩽E/parenleftigT/summationdisplay
n=11{ˆµ∗,n⩽S}/parenrightig
=T/summationdisplay
n=1P(ˆµ∗,n⩽S)
=T/summationdisplay
n=1P/parenleftbig
ˆµ∗,n⩽µ∗−|∆S
∗|/parenrightbig
⩽T/summationdisplay
n=1exp/parenleftbig
−n|∆S
∗|2
2/parenrightbig
⩽2
|∆S∗|2. (4)
4Under review as submission to TMLR
Finally writing
ni(T) =T/summationdisplay
t=11{At=i}
for the number of times arm iwas pulled up to step T, we can combine eqs. 1–4 to obtain
RS
T=/summationdisplay
i:∆S
i>0∆S
iE(ni(T))
=/summationdisplay
i:∆S
i>0∆S
iT/summationdisplay
t=1P(At=i)
⩽/summationdisplay
i:∆S
i>0∆S
i/parenleftig
1 +2
(∆S
i)2+2
|∆S∗|2/parenrightig
=/summationdisplay
i:∆S
i>0/parenleftig
∆S
i+2
∆S
i+2∆S
i
|∆S∗|2/parenrightig
.
The algorithm as well as the analysis are adaptations from (Bubeck et al., 2013) where ordinary regret bounds
for the MAB setting are considered under the assumption that the learner knows the value of µ∗as well as (a
bound on) the gap ∆between the optimal and the best suboptimal arm.1The crucial insight is that what is
actually needed in order to apply algorithm and analysis of Bubeck et al. (2013) is to have a reference value
µthat separates the optimal from suboptimal arms, that is, µ∗>µ>µifor all suboptimal arms i. In our
case this reference value is given by the satisfaction level S, which in the realizable case separates the good
arms from the bad ones. Note that for this we need to have S <µ∗, so that we do not get constant regret
whenS=µ∗.
Remarks. (i) Concerning the quality of the upper bound of Theorem 1, standard lower bounds imply that for
an armiwith unknown mean reward µi<Sone needs∼(∆S
i)−2samples to determine that arm iis not
satisficing. As the respective S-regret for choosing arm iis∆S
i, this gives a lower bound on the regret of
order (∆S
i)−1. Similarly, in order to be sure that the optimal arm with mean reward µ∗>Sis satisficing it
takes∼(∆S
∗)−2samples. Although sampling µ∗does not incur any regret it is intuitive that the problem
becomes more difficult for smaller ∆S
∗so that it is intuitive that this term appears in the bound of Theorem
1. However, a matching lower bound is only available for special cases such as when ∆S
i= ∆S
∗for all not
satisficing arms i. As neither ∆S
inor∆S
∗is known to the learner, it is not obvious how our simple algorithm
and the respective upper bound could be improved either. We conjecture that the upper bound of Theorem 1
is basically optimal.
(ii) The constant regret bound of Theorem 1 not only improves over the logarithmic bounds given by Reverdy
et al. (2017) for a variant of the UCL algorithm (Reverdy et al., 2014) that picks an arbitrary arm with
UCL-index above S(instead of an arm with maximal index). Our bound also is not consistent with a claimed
lower bound that is also logarithmic in the horizon (not mentioned in the corrections of Reverdy et al., 2021).
This bound is obtained by application of a lower bound for the multiple play setting (Anantharam et al.,
1987), where at each step marms are chosen by the learner, who hence has to identify the mbest arms.
The given proof chooses mto be all arms above the given satisfaction level S. However, the lower bound is
obviously not directly applicable to the satisficing setting: not allarms above the satisfaction level have to
be found, but a single one is sufficient.
Bubeck et al. (2013) also provide another algorithm with a more refined approach for exploration, using a
potential function instead of a uniform probability distribution over the arms. We note that a respective
adaptation of algorithm and analysis to the satisficing setting can be done in a quite straightforward way.
1As has been shown in the meantime, knowledge of µ∗is sufficient for obtaining constant bounds on the regret (Garivier
et al., 2019).
5Under review as submission to TMLR
3 The General Case
Now let us consider the general case where it is not guaranteed that the chosen satisfaction level Sis realizable,
that is, it may happen that S >µ∗. Then unlike in the realizable case the satisfaction level Sdoes not give
the learner any useful information so that we cannot hope to perform better than in an ordinary MAB setting.
Obviously the S-regret will be linear, but we can still aim at getting bounds on the (classic) regret. On the
other hand, if there is at least one arm above the satisfaction level S, we would like to re-establish constant
bounds on the S-regret as in the realizable case.
For the general setting we propose the Sat-UCB scheme shown as Algorithm 3. Sat-UCB exploits when
there is an arm with empirical mean above S(cf. line 4 of the algorithm) and explores otherwise. The
exploration takes into account a UCB value similar to the classical index suggested for the UCB1 algorithm
of (Auer et al., 2002), that is,
UCBi(t) := ˆµi(t) +βi(t),, (5)
where
βi(t) =/radicalig
2 log(f(t))
ni(t−1)
withf(t) = 1 +tlog2(t). If there is at least one arm with UCB-value above SthenSat-UCB chooses such
an arm uniformly at random, which makes sure that all promising arms are explored sufficiently to decide
whether they are satisficing. Otherwise, if all arms have UCB-value below S, the algorithm chooses an arm
according to UCB1, that is, an arm imaximizing UCBi. This guarantees that the algorithm performs similar
to UCB1 when there is no satisficing arm.
Algorithm 3 :Sat-UCB Scheme for Satisficing in the General Case
Require:K,S
1:Play each arm once, i.e., for time steps t= 1,...,Kplay armAt=t.
2:fortime stepst=K+ 1,...do
3: if∃iˆµi(t)⩾Sthen
4: Choose an arbitrary Atfrom{i|ˆµi(t)⩾S}.
5: else if∃iUCBi(t)⩾Sthen
6: ChooseAtuniformly at random from {i|UCBi(t)⩾S}.
7: else
8: Play armAt∈argmax
i∈J1,KKUCBi(t).
9: end if
10:end for
For step 4 in Sat-UCB different concrete instantiations of the exploitation step are possible. In Section 4 we
will consider different sub-algorithms for choosing an arm from {i|ˆµi(t)⩾S}. The following two theorems are
independent of the selected exploitation sub-algorithm and show that Sat-UCB achieves constant S-regret if
µ∗>S, while the regret is bounded as for UCB1 (Auer et al., 2002) otherwise.
Theorem 2. Ifµ∗>SthenSat-UCB satisfies for all T⩾1,
RS
T⩽/summationdisplay
i:∆S
i>0/parenleftig
∆S
i+2
∆S
i+7∆S
i
|∆S∗|2/parenrightig
.
Proof.As before we write the S-regret as
RS
T=k/summationdisplay
i:∆S
i>0E(ni(T)) ∆S
i
6Under review as submission to TMLR
and proceed bounding E(ni(T)) =/summationtextT
t=1P(At=i)for all non-satisficing arms i. Thus letibe the index of
a non-satisficing arm. Let Zt:={∀j∈J1,KK,ˆµj(t)<S}be again the event that all arms have empirical
values below the satisfaction level. Then we can decompose the event {At=i}as
{At=i}⊂{t=i}∪/braceleftbig
At=i,ˆµi(t)⩾S,t>K/bracerightbig
∪/braceleftbig
At=i,UCBi(t)⩾S,UCB∗(t)⩾S,t>K,Z t/bracerightbig
∪/braceleftbig
At=i,UCB∗(t)<S,t>K,Z t/bracerightbig
. (6)
For the first two events we have
T/summationdisplay
t=1P(t=i)⩽1 (7)
and analogously to eq. 3
T/summationdisplay
t=1P/parenleftbig
At=i,ˆµi(t)⩾S,t>K/parenrightbig
⩽T/summationdisplay
t=1P/parenleftbig
At=i,ˆµi(t)⩾µi+ ∆S
i/parenrightbig
⩽T/summationdisplay
n=1P(ˆµi,n⩾µi+ ∆S
i)
⩽2
(∆S
i)2. (8)
For the probability of the third event we have
T/summationdisplay
t=1P/parenleftbig
At=i,UCBi(t)⩾S,UCB∗(t)⩾S,t>K,Z t/parenrightbig
=T/summationdisplay
t=1P/parenleftbig
At=∗,UCBi(t)⩾S,UCB∗(t)⩾S,t>K,Z t/parenrightbig
⩽T/summationdisplay
t=1P(At=∗,Zt)⩽T/summationdisplay
t=1P/parenleftbig
At=∗,ˆµ∗(t)⩽S/parenrightbig
=E/parenleftigT/summationdisplay
t=11/braceleftbig
At=∗,ˆµ∗(t)⩽S/bracerightbig/parenrightig
⩽E/parenleftigT/summationdisplay
n=11{ˆµ∗,n⩽S}/parenrightig
=T/summationdisplay
n=1P(ˆµ∗,n⩽S) =T/summationdisplay
n=1P/parenleftbig
ˆµ∗,n⩽µ∗−|∆S
∗|/parenrightbig
⩽T/summationdisplay
n=1exp/parenleftbig
−n|∆S
∗|2
2/parenrightbig
⩽2
|∆S∗|2. (9)
Finally, the probability of the last event of eq. 6 is upper bounded by
T/summationdisplay
t=1P/parenleftbig
UCB∗(t)<S/parenrightbig
=T/summationdisplay
t=1P/parenleftbig
ˆµ∗(t)<µ∗−(|∆S
∗|+β∗(t))/parenrightbig
⩽T/summationdisplay
t=1t/summationdisplay
n=1P/parenleftbigg
ˆµ∗,n<µ∗−/parenleftig
|∆S
∗|+/radicalig
2 log(f(t))
n/parenrightig/parenrightbigg
⩽T/summationdisplay
t=1t/summationdisplay
n=11
f(t)exp/parenleftbig
−n|∆S
∗|2
2/parenrightbig
⩽2
|∆S∗|2T/summationdisplay
t=11
f(t)
⩽5
|∆S∗|2, (10)
7Under review as submission to TMLR
where the last inequality is obtained by observing that/summationtextT
t=11
f(t)⩽1 +/summationtextT
t=21
tlog2(t)and then bounding the
sum with an integral.
Finally, by putting everything together, we obtain from equations eqs. 6– 10 the claimed result
RS
T=/summationdisplay
i:∆S
i>0∆S
iE(ni(T))⩽/summationdisplay
i:∆S
i>0/parenleftig
∆S
i+2
∆S
i+7∆S
i
|∆S∗|2/parenrightig
.
Theorem 3. Ifµ∗⩽SthenSat-UCB satisfies for all T⩾1
RT⩽/summationdisplay
i:∆i>0inf
ε∈(0,∆i)∆i/parenleftigg
1 +5
ε2+2(logf(T) +/radicalbig
πlogf(T) + 1)
(∆i−ε)2/parenrightigg
. (11)
Furthermore,
lim sup
T→∞RT
log(T)⩽/summationdisplay
i:∆i>02
∆i. (12)
Thus, for a constant C > 0it holds that
RT⩽C/summationdisplay
i:∆i>0/parenleftbigg
∆i+log(T)
∆i/parenrightbigg
.
Proof.The proof can be reduced to the derivation of the regret bounds for UCB1 as given in Theorem 8.1 of
Lattimore and Szepesvári (2020). We start with the standard regret decomposition
RT=/summationdisplay
i:∆i>0E(ni(T)) ∆i.
In the following, we bound for each suboptimal arm ithe number of times ni(T)it is played. By definition of
the algorithm, arm iis chosen after step Konly if either
ˆµi(t) +βi(t)⩾ˆµ∗(t) +β∗(t)or ˆµi(t) +βi(t)⩾S.
(Note that the case ˆµi(t)⩾Sis subsumed by the second event.) Accordingly, we can decompose the event
At=iusing some arbitrary but fixed ε∈(0,∆i)as
{At=i}⊆/braceleftbig
At=iandˆµ∗(t) +β∗(t)⩽µ∗−ε/bracerightbig
∪/braceleftbig
At=iandˆµ∗(t) +β∗(t)⩾µ∗−ε/bracerightbig
⊆/braceleftbig
ˆµ∗(t) +β∗(t)⩽µ∗−ε/bracerightbig
∪/braceleftbig
At=iandˆµi(t) +βi(t)⩾ˆµ∗(t) +β∗(t)⩾µ∗−ε/bracerightbig
∪/braceleftbig
At=iandˆµ∗(t) +β∗(t)⩾µ∗−εandˆµi(t) +βi(t)⩾S/bracerightbig
⊆/braceleftbig
ˆµ∗(t) +β∗(t)⩽µ∗−ε/bracerightbig
∪/braceleftbig
At=iandˆµi(t) +βi(t)⩾µ∗−ε/bracerightbig
,
where the last inclusion is due to the assumption that µ∗⩽S. It follows that
ni(T)⩽T/summationdisplay
t=11/braceleftbig
ˆµ∗(t) +β∗(t)⩽µ1−ε/bracerightbig
+T/summationdisplay
t=11/braceleftbig
At=iandˆµi(t) +βi(t)⩾µ∗−ε/bracerightbig
.
The obtained decomposition is the same as the one in the proof of Theorem 8.1 from (Lattimore and
Szepesvári, 2020) and the very same arguments can be used to finish the proof of eq. 11. The second part of
the theorem, that is eq. 12, follows by choosing ε=log−1/4(T)and taking the limit as Ttends to infinity.
8Under review as submission to TMLR
4 Experiments
We compared Sat-UCB to other bandit algorithms in order to show that the latter keep accumulating
S-regret, while Sat-UCB sticks to a satisficing arm after finite time, thus confirming the results of Theorem 2.
We also investigated the behavior of Sat-UCB in the not realizable case with different values for the chosen
satisfaction level Sand did experiments with a slightly modified version Sat-UCB+introduced below in
Section 4.2.
4.1 Exploitation in Sat-UCB
As already mentioned, we investigated different sub-algorithms for exploitation in step 4 of Sat-UCB .
Obvious choices for this exploitation step are e.g. selecting the arm with maximal empirical mean reward or
using UCB1 to choose among the arms with empirical mean reward above S.
However, the following more refined approach for exploitation empirically worked best. In addition to the
UCB value for each arm we define an analogous lower confidence bound value
LCBi(t) := ˆµi(t)−βi(t). (13)
Then for any arm iwith empirical mean above Swe consider the confidence interval [LCBi,UCBi]and then
choose the arm for which the largest share of this confidence interval is above the satisfaction level S. That
is, step 4 of Sat-UCB chooses an arm from
argmax
i∈J1,KK/braceleftbiggUCBi(t)−max{S,LCBi(t)}
βi(t)/bracerightbigg
. (14)
The intuition behind this choice is that an arm whose confidence interval ist mostly above Swill most likely
have actual mean above S. In the following experiments, Sat-UCB always refers to Sat-UCB employing
thisconfidence fraction index as exploitation sub-algorithm.
4.2Sat-UCB+: Modified Exploration in Sat-UCB
Concerning exploration we also considered a simplified version of Sat-UCB which does not use random
exploration and instead always plays UCB1 when there is no empirically satisficing arm. For the sake of
completeness, this modification Sat-UCB+is shown as Algorithm 4.
Algorithm 4 :Sat-UCB+(Experimental Simplification of Sat-UCB )
Require:K,S
1:Play each arm once, i.e., for time steps t= 1,...,Kplay armAt=t.
2:fortime stepst=K+ 1,...do
3: if∃iˆµi(t)⩾Sthen
4: ChooseAtfrom argmax
i∈J1,KK/braceleftig
UCB i(t)−max{S,LCB i(t)}
βi(t)/bracerightig
.
5: else
6: Play armAt∈argmax
i∈J1,KKUCBi(t).
7: end if
8:end for
While we were not able to provide a constant bound on the S-regret as for the original Sat-UCB algorithm,
in the experiments Sat-UCB+performed better than Sat-UCB .
9Under review as submission to TMLR
4.3 Setup
4.3.1 Settings
To illustrate the influence of the structure of the underlying bandit problem we performed experiments in the
following two settings each with 20 arms and normally distributed rewards2with standard deviation 1:
InSetting 1 the mean reward of each arm i= 1,2,..., 20is set toi−1
20. For the satisfaction level we chose 0.8
in the realizable case resulting in four satisfying arms. For experiments in the not realizable case S= 1was
chosen.
InSetting 2 the mean reward of each arm iis set to/radicalig
i
20. For the satisfaction level we chose S= 0.92so
that there are three satisfying arms in the realizable case. This setting is more difficult than Setting 1, as
arms are closer to Sas well as to each other. The not realizable satisfaction level was set to 1.1.
Further experiments for a complementary very simple setting are reported in the appendix.
4.3.2 Algorithms
We compared different instantiations of Sat-UCB . Beside the main variant that uses the confidence fraction
index in eq. 14 for exploitation in line 4 of Sat-UCB , we also consider using UCB1 or the maximal empirical
mean for choosing an arm, respectively. We also performed experiments with the modified Sat-UCB+
algorithm.
For comparison, beside UCB1 (Auer et al., 2002) (choosing the same confidence intervals as for Sat-UCB )
we used UCB α(Degenne et al., 2019) as well as the Satisfaction in Mean Reward UCL algorithm (Reverdy
et al., 2017) and the (deterministic) UCL algorithm (Reverdy et al., 2014) it is based on.
For UCBαthat aims to combine regret minimization and best arm identification we chose confidence δ= 0.001
andα= 1to focus on exploitation. We also performed a few experiments with α>1that confirmed that
in this case the algorithm explores more and accumulates more S-regret with increasing α. The original
UCBαis given for just two arms and stops when the optimal arm has been identifed. In our case with an
arbitrary number of arms we eliminate arms that are identified as suboptimal with respect to same criterion
as suggested by Degenne et al. (2019).
For Satisfaction in Mean Reward UCL we considered various ways of how to choose arms from the eligible
set(cf. eq. 28 of Reverdy et al., 2017), as this step is not specified in the original paper. Experimentally it
worked best to select at any step teacharm from the eligible set once but increase the time step counter just
by 1, independent of how many arms have been chosen at t. For the parameter awe chosea= 1, which was
suggested in the original paper and worked best experimentally, although Reverdy et al. (2021) state that for
the theoretical results to hold one should have a>4
3.
4.4 Results
4.4.1 Realizable Case
We started with comparing Sat-UCB to UCB1 in Setting 1. Figure 1a depicts a showcase run in Setting 1
illustrating that Sat-UCB soon focuses on a satisficing arm, while UCB1 keeps exploring. Accordingly, UCB1
suffers growing S-regret due to ongoing exploration of arms below the satisfaction level, cf. Figure 2 below.
Figure 1b shows a comparison of the S-regret of the different versions of Sat-UCB as well as the experimental
modification Sat-UCB+in Setting 1. Here and in the following plots, we show the averaged values for
(S-)regret over 500 runs and error bars indicate a 90% confidence interval (between the 5%- and the 95%-
percentile). We see that the experimental modification Sat-UCB+not only achieves the smallest S-regret,
it also displays much smaller variance than the Sat-UCB variants. Experiments for Setting 2 give similar
2We repeated all experiments also with Bernoulli rewards, which gave similar results, which are consequently not reported
here.
10Under review as submission to TMLR
(a) Arm pulls for UCB1 and Sat-UCB in exemplary run.
 (b) Comparing the S-regret for variants of Sat-UCB .
Figure 1: Experiments in the realizable case of Setting 1.
results (cf. Fig. 2b below). We note that for Sat-UCB+the confidence fraction index of eq. 14 also gives
better results than using UCB1 or choosing the empirically best arm instead.
Figure 2 shows a comparison of the S-regret of Sat-UCB andSat-UCB+to other algorithms. Although
S-regret is smaller than classic regret, all algorithms except our two algorithms suffer growing regret due
to ongoing exploration of arms below the satisfaction level. As expected, Sat-UCB gives constant regret,
while surprisingly Deterministic UCL is superior to its Satisfaction in Mean Reward counterpart. Figure 2b
illustrates that the regret for Sat-UCB is larger in Setting 2 in which the gaps of the relevant arms to the
satisfaction level are smaller. In both cases Sat-UCB+performs best however.
(a) Setting 1 with S= 0.8.
 (b) Setting 2 with S= 0.92.
Figure 2: Comparison of S-regret of different algorithms in the realizable case.
4.4.2 Not Realizable Case
In the not realizable case Sat-UCB usually performed a bit worse than UCB1, cf. Figure 3a. (The respective
plot for Setting 2 can be found in Figure 4a of the appendix.) In particular, in the beginning Sat-UCB does
more (random) exploration and catches up only for larger horizon. The experimental modification Sat-UCB+
shows the opposite behavior performing much better for small horizon before performance coincides with
UCB1 and Sat-UCB after a higher number of steps.
11Under review as submission to TMLR
(a) Comparison of classic regret of different algorithms
in the not realizable case of Setting 1 with S= 1.
(b) Comparison of different choices of S for Sat-UCB+
in Setting 1.
Figure 3: Classic regret in the not realizable case.
Interestingly, while Sat-UCB is quite insensitive to the choice of S, in the not realizable case Sat-UCB+
works better the closer Sis chosen to µ∗. This is illustrated in Figure 3b. When Sis chosen close to µ∗= 1
the regret becomes nearly constant. This behavior of Sat-UCB+can be explained as follows: When Sis
close toµ∗this increases exploitation in case there are arms with empirical mean above S. WithSbeing
close toµ∗it is also more likely that such arms exist. On the other hand, if there are no such arms the
increased exploitation when using UCB1 (instead of random exploration as in Sat-UCB ) leads to improved
performance of Sat-UCB+.
5 Conclusion
Our results for the multi-armed bandit case are just a first step in an ongoing project on satisficing in
reinforcement learning. While some ideas may be used also in the general standard Markov decision process
setting, it seems already not quite simple to obtain reasonable constant regret bounds in the realizable case.
While it might be possible to consider each policy as an arm in an MAB setting, the resulting bounds would
be linear in the number of policies and hence exponential in the number of states.
Another interesting direction of further research is satisficing with adaptive satisfaction level. While overall
such an approach obviously will not be possible to obtain constant regret in any case, it is an interesting
question what could be gained by trying to adapt the satisfaction level towards the optimal mean reward.
This is in particular interesting in view of the results for Sat-UCB+, although currently provable regret
bounds for this version of the algorithm are still missing.
A lesson to take from the MAB setting is that the savings from considering a satisficing instead of an
optimizing objective –at least with respect to regret– is not that there are arms that need no exploration at
all. Rather in the worst case (as always considered by notions of regret) one still has to explore all arms,
however the amount of necessary exploration is now constant and independent of the horizon.
References
Venkatachalam Anantharam, Pravin Varaiya, and Jean Walrand. Asymptotically efficient allocation rules for
the multiarmed bandit problem with multiple plays—part I: I.i.d. rewards. IEEE Trans. Autom. Control ,
32:968–976, 12 1987.
Jean-Yves Audibert, Sébastien Bubeck, and Rémi Munos. Best arm identification in multi-armed bandits. In
COLT 2010, 23rd Conference on Learning Theory , pages 41–53, 2010.
12Under review as submission to TMLR
Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Mach. Learn. , 47(2-3):235–256, 2002.
Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet. Bounded regret in stochastic multi-armed bandits.
InCOLT 2013 – The 26th Annual Conference on Learning Theory , volume 30 of Proceedings of Machine
Learning Research , pages 122–134, 2013.
Arghya Roy Chaudhuri and Shivaram Kalyanakrishnan. PAC identification of a bandit arm relative to a
reward quantile. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI 2017 ,
pages 1777–1783, 2017.
Arghya Roy Chaudhuri and Shivaram Kalyanakrishnan. PAC identification of many good arms in stochastic
multi-armed bandits. In Proceedings of the 36th International Conference on Machine Learning, ICML
2019, volume 97 of Proceedings of Machine Learning Research , pages 991–1000, 2019.
Rémy Degenne, Thomas Nedelec, Clément Calauzènes, and Vianney Perchet. Bridging the gap between
regret minimization and best arm identification, with application to A/B tests. In The 22nd International
Conference on Artificial Intelligence and Statistics, AISTATS 2019 , volume 89 of Proceedings of Machine
Learning Research , pages 1988–1996. PMLR, 2019.
Aurélien Garivier, Pierre Ménard, and Gilles Stoltz. Explore first, exploit next: The true shape of regret in
bandit problems. Math. Oper. Res. , 44(2):377–399, 2019.
Hideaki Kano, Junya Honda, Kentaro Sakamaki, Kentaro Matsuura, Atsuyoshi Nakamura, and Masashi
Sugiyama. Good arm identification via bandit feedback. Mach. Learn. , 108(5):721–745, 2019.
Yu Kohno and Tatsuji Takahashi. A cognitive satisficing strategy for bandit problems. Int. J. Parallel
Emergent Distrib. Syst. , 32(2):232–242, 2017.
Tor Lattimore and Csaba Szepesvári. Bandit algorithms . Cambridge University Press, 2020.
Blake Mason, Lalit Jain, Ardhendu Tripathy, and Robert Nowak. Finding all ε-good arms in stochastic
bandits. In Advances in Neural Information Processing Systems 33, NeurIPS 2020 , 2020.
Nadav Merlis and Shie Mannor. Lenient regret for multi-armed bandits. In Thirty-Fifth AAAI Conference
on Artificial Intelligence, AAAI 2021 , pages 8950–8957, 2021.
Paul Reverdy, Vaibhav Srivastava, and Naomi Ehrich Leonard. Modeling human decision making in generalized
Gaussian multiarmed bandits. Proc. IEEE , 102(4):544–571, 2014.
Paul Reverdy, Vaibhav Srivastava, and Naomi Ehrich Leonard. Satisficing in multi-armed bandit problems.
IEEE Trans. Autom. Control. , 62(8):3788–3803, 2017.
Paul Reverdy, Vaibhav Srivastava, and Naomi Ehrich Leonard. Corrections to “Satisficing in multiarmed
bandit problems”. IEEE Trans. Autom. Control , 66(1):476–478, 2021.
Diederik M. Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-objective
sequential decision-making. J. Artif. Intell. Res. , 48:67–113, 2013.
Daniel Russo and Benjamin Van Roy. Satisficing in time-sensitive bandit learning. CoRR, abs/1803.02855,
2018.
Akihiro Tamatsukuri and Tatsuji Takahashi. Guaranteed satisficing and finite regret: Analysis of a cognitive
satisficing value function. Biosystems , 180:46–53, 2019.
William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika , 25(3–4):285–294, 1933.
Yifan Wu, Roshan Shariff, Tor Lattimore, and Csaba Szepesvári. Conservative bandits. In Proceedings of the
33nd International Conference on Machine Learning, ICML 2016 , volume 48 of Proceedings of Machine
Learning Research , pages 1254–1262. JMLR.org, 2016.
13Under review as submission to TMLR
A Complementary Experiments
In this section we report additional experiments that we performed in the following very easy Setting 3 :
There are 19 arms with mean reward 0 and one with mean reward 1. Here for any satisfaction level between
0 and 1 the task of satisficing is equivalent to learning the optimal arm. However knowing the satisfaction
level gives some additional information that allows to learn faster. For the experiments S= 0.5was chosen in
the realizable case and S= 1.1in the not realizable case.
As shown in Fig. 4b, in the realizable case Sat-UCB+andSat-UCB work equally well in this very simple
setting. Also UCB αexhibits at least close to constant regret in Setting 3. In the not realizable case the two
UCL variants perform best, cf. Figure 5a. As Figure 5b demonstrates, Sat-UCB+performs better the closer
Sis chosen to µ∗.
(a) Classic regret in the not realizable Setting 2 with
S= 1.1.
(b)S-regret for the realizable case in Setting 3 with
S=0.5.
Figure 4
(a) Comparison of algorithms in Setting 3 with S=1.1.
 (b) Comparison of different choices of SforSat-UCB+.
Figure 5: Classic regret in the not realizable Setting 3.
14