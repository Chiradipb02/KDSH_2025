Published in Transactions on Machine Learning Research (8/2024)
ClosingthegapbetweenSVRGandTD-SVRGwithGradient
Splitting
Arsenii Mustafin aam@bu.edu
Department of Computer Science
Boston University
Alex Olshevsky alexols@bu.edu
Department of Electrical and Computer Engineering
Boston University
Ioannis Ch. Paschalidis yannisp@bu.edu
Department of Electrical and Computer Engineering
Boston University
Reviewed on OpenReview: https: // openreview. net/ forum? id= dixU4fozPQ
Abstract
Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose
performance can be enhanced by variance reduction methods. Recently, multiple works have
sought to fuse TD learning with Stochastic Variance Reduced Gradient (SVRG) method
to achieve a geometric rate of convergence. However, the resulting convergence rate is
significantly weaker than what is achieved by SVRG in the setting of convex optimization.
In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient
of an appropriately chosen function, thus simplifying the algorithm and fusing TD with
SVRG. Our main result is a geometric convergence bound with predetermined learning rate
of1/8, which is identical to the convergence bound available for SVRG in the convex setting.
Our theoretical findings are supported by a set of experiments.
1 Introduction
Reinforcement learning (RL) is a framework for solving sequential decision making environments. Policy
evaluation is one of those problems, which seeks to determine the expected return an agent achieves if it
chooses actions according to a specific stationary policy. Temporal Difference (TD) learning Sutton (1988)
is a popular algorithm with a particularly simple form which can be performed in an online setting. TD
learning uses the Bellman equation to bootstrap the estimation process and update the value function from
each incoming sample or mini-batch. As all RL methods, tabular TD learning suffers from the “curse
of dimensionality" when the number of states is large, motivating parametric approximations of the value
function.
Despite its simple formulation, theoretical analysis of approximate TD learning is subtle. There are a
few important milestones in this process, one of which is the work in Tsitsiklis & Van Roy (1997), where
asymptotic convergence guarantees were established. More recent advances include Bhandari et al. (2018),
Srikant & Ying (2019) and Liu & Olshevsky (2021). In particular, Liu & Olshevsky (2021) shows that TD
learning might be viewed as an example of gradient splitting, a process analogous to gradient descent.
TD-leaning has an inherent variance problem: the variance of the update does not go to zero as the method
converges. This problem is also present in a class of convex optimization problems where the objective
function is a sum of functions and Stochastic Gradient Descent (SGD) -type methods are applied Robbins
& Monro (1951). Such methods proceed incrementally by sampling a single function, or a mini-batch
1Published in Transactions on Machine Learning Research (8/2024)
of functions, to use for stochastic gradient evaluations. Variance reduction techniques were developed to
address this problem and yield faster convergence, including Stochastic Average Gradient (SAG) (Schmidt
et al., 2013), SVRG (Johnson & Zhang, 2013) and SAGA (Defazio et al., 2014). Their distinguishing feature
is that they converge geometrically.
Previous research has analysed the application of variance reduction technique to TD updates in two problem
settings: (i)a pre-sampled trajectory of the Markov Decision Process (MDP) (finite sample), and (ii)when
states are sampled directly from the MDP (online sampling). We briefly mention the most relevant works in
both veins. In the online sampling setting, the first attempt to adapt variance reduction to TD learning was
made in Korda & La (2015). Their results were discussed by Dalal et al. (2018) and Narayanan & Szepesvári
(2017); Xu et al. (2020) provided further analysis of such approaches and showed geometric convergence
for the so-called Variance Reduction Temporal Difference learning (VRTD) algorithm for both Markovian
and i.i.d. sampling; Ma et al. (2020) applies the variance reduction technique to Temporal Difference with
Correction. However, both Xu et al. (2020) and Ma et al. (2020) achieve total complexity better than 1/ϵ
for the policy evaluation problem, which is not possible in the setting of this paper (see Appendix A for
details).
The finite sample setting was analysed in Du et al. (2017), where authors directly applied SVRG and SAGA
to a version of policy evaluation by transforming it into an equivalent convex-concave saddle-point problem.
Since their algorithm uses two sets of parameters, in this paper we call it Primal-Dual SVRG or PD-SVRG.
Their results were improved in Peng et al. (2020) by introducing inexact mean path update calculation
(Batched SVRG algorithm).
1.1 Motivation and Contribution
The previous analysis of finite sample settings and both cases of online sampling has demonstrated the
geometric convergence of the algorithm. However, this convergence has been established separately with
different proof strategies, and several unsatisfying aspects persist. A prominent concern is the high com-
plexity in all scenarios: convergence times derived for variance reduction in temporal difference learning not
only show a quadratic relationship with the condition number (ratio of largest to smallest eigenvalues of a
matrix) but also include additional factors related to the condition number of certain diagonalizing matrices.
Such complexities, especially in the context of ill-conditioned matrices typical in reinforcement learning,
lead to prohibitive sample complexities even for straightforward problems. For instance, in a simple Markov
Decision Process (MDP) with 400 states and 10 actions, the batch size required to ensure convergence is
impractically large using the bounds from previous work, as illustrated in Table 2 (second and third rows of
the table) and further discussed in Appendix J.1.
Additionally, there is a qualitative discrepancy in the current results: the current analysis of the SVRG-
enhanced TD algorithm requires complexity that is quadratic in terms of the condition number, which does
not align with the complexity of classical SVRG in the convex setting with its lineardependency on the
condition number. This gap, not addressed by the previous literature, remains an unresolved question.
In this paper we analyze the convergence of SVRG applied to TD (for convenience we call it TD-SVRG) in
both finite sample and online sampling cases. Our theoretical results are summarized in Table 1. Our key
contributions are:
•For the finite sample case, we show that TD-SVRG has the same convergence rate as SVRG in
the convex optimization setting. In particular, we replace the quadratic scaling with the condition
number by linear scaling and remove extraneous factors depending on the diagonalizing matrix.
Notably, we use a simple, pre-determined learning rate of 1/8to do this.
•For i.i.d. online sampling, we similarly achieve better rates with simpler analysis. Again, our
analysis is the first to show that TD-SVRG has the same convergence rate as SVRG in the convex
optimization setting with a predetermined learning rate of 1/8, and a linear rather than quadratic
scaling with the condition number. Similar improvement is obtained for Markovian sampling.
•Our theoretical findings have significant practical implications: Previous analyses for both finite
sample and online sampling scenarios require batch sizes so large as to be impractical. In contrast,
2Published in Transactions on Machine Learning Research (8/2024)
Table 1: Comparison of algorithmic complexities, where ϵis a desired expected accuracy, λAis a minimum
eigenvalue of the matrix A,πminis a minimum state probability of the MDP stationary distribution, γis
a discount factor and Nis a dataset size. The complexity is reported as the number of samples required
to shrink a distance function on average by a factor of ϵ, where the distance function is a quadratic of the
quantityθ−θ∗, similar to previous works (Du et al., 2017; Xu et al., 2020). The definitions of other quantities
and a table with additional algorithms and details of the comparison might be found in Appendix K
.Complexity
Type Algorithm Feature case Tabular case
Finite PD-SVRG O/parenleftig/parenleftig
N+κ2(C)L2
G
λmin(ATC−1A)2/parenrightig
log(1
ϵ)/parenrightig
O/parenleftig/parenleftig
N+1
(1−γ)2π4
min/parenrightig
log(1
ϵ)/parenrightig
Finite Our O/parenleftig/parenleftig
N+1
λA/parenrightig
log(1
ϵ)/parenrightig
O/parenleftig/parenleftig
N+1
(1−γ)πmin/parenrightig
log(1
ϵ)/parenrightig
i.i.d. TDO/parenleftig
1
λ2
Aϵlog(1
ϵ)/parenrightig
O/parenleftig
1
(1−γ)2π2
minϵlog(1
ϵ)/parenrightig
i.i.d. OurO/parenleftig
1
λAϵlog(1
ϵ)/parenrightig
O/parenleftig
1
(1−γ)πminϵlog(1
ϵ)/parenrightig
Markovian VRTD O/parenleftig
1
ϵλ2
Alog(1
ϵ)/parenrightig
O/parenleftig
1
(1−γ)2π2
minϵlog(1
ϵ)/parenrightig
Markovian Our O/parenleftig
1
ϵλAlog2(1
ϵ)/parenrightig
O/parenleftig
1
(1−γ)πminϵlog2(1
ϵ)/parenrightig
Table 2: This table gives the output of formulas from Table 1 on the simplest possible MDP (a random MDP)
to show the magnitude of the improvement. Specifically, we compare theoretically suggested batch sizes for
a random MDP with 400 states, 10 actions and γ= 0.95. Values in the first row indicate the dimensionality
of the feature vectors. Values in the other rows show the batch size required by the corresponding method.
Values are averaged over 10 generated datasets and environments.
Method/Features 6 11 21 41
TD-SVRG (ours) 3176 6942 18100 54688
PD-SVRG 1.72·10163.83·10183.06·10215.77·1024
VRTD 5.41·1062.53·1071.63·1081.58·109
our analysis leads to batch-sizes that are implementable in practice. A simple example of random
MDPs illustrating this is given in Table 2.
•We conducted experimental studies demonstrating that our theoretically derived batch size and
learningrateachievegeometricconvergenceandoutperformotheralgorithmsthatrelyonparameters
selected via grid search, as detailed in Section 6 and Appendix J. Specifically, we have re-done earlier
experiments from Du et al. (2017) and found that our TD-SVRG method with parameters coming
from our theoretical analysis converges much faster than previous best SVRG based algorithm (PD
SVRG): on average, it requires 132 times fewer iterations to contract by a factor of 0.5. Note
that this comparison favors previous work due; specifically we use parameters from our theorems
whereas previous work uses parameters selected by grid search. When we also run our algorithm
with parameters chosen via grid search this disparity increases to 180 times.
To summarize, in every setting our key contribution is the reduce the scaling with a condition number from
quadratic to linear, as well as to remove extraneous factors that do not appear in the analysis of SVRG in
the convex setting. As described below, the final result matches the bounds that are known for the SVRG in
the separable convex optimization setting. These theoretical results also lead to large gains in convergence
speed.
3Published in Transactions on Machine Learning Research (8/2024)
2 Problem formulation
We consider a discounted reward Markov Decision Process (MDP) defined by the tuple (S,A,P,r,γ), where
Sis the state space, Athe action space, P=P(s′|s,a)s,s′∈S,a∈Athe transition probabilities, r=r(s,s′)the
reward function, and γ∈[0,1)is a discount rate. The agent follows a policy π:S→ ∆A– a mapping from
states to the probability simplex over actions. A policy πinduces a joint probability distribution π(s,a),
defined as the probability of choosing action awhile being in state s. Given that the policy is fixed and we are
interested only in policy evaluation, for the remainder of the paper we will consider the transition probability
matrixP, such that: P(s,s′) =/summationtext
aπ(s,a)P(s′|s,a).We assume, that the Markov process produced by the
transition probability matrix is irreducible and aperiodic with stationary distribution µπ.
Thepolicyevaluationproblemistocompute Vπ, definedas: Vπ(s) :=E[/summationtext∞
t=0γtrt+1],whichistheexpected
sum of discounted rewards, where the expectation is taken with respect to the sampled trajectory of states.
Herertis the reward at time tandVπis the value function, formally defined to be the unique vector which
satisfies the Bellman equation TπVπ=Vπ, whereTπis the Bellman operator, defined as: TπVπ(s) =/summationtext
s′P(s,s′) (r(s,s′) +γVπ(s′)).The TD(0) method is defined as follows: one iteration performs a fixed
point update on a randomly sampled pair of states s,s′with learning rate α:V(s)←V(s) +α(r(s,s′) +
γV(s′)−V(s)).When the state space size |S|is large, tabular methods which update the value function
for every state become impractical. For this reason, a linear approximation of the value function is often
used. Each state is represented by a feature vector ϕ(s)∈Rdand the state value Vπ(s)is approximated
byVπ(s)≈ϕ(s)Tθ, whereθis a tunable parameter vector. A single TD update on a randomly sampled
transitions,s′becomes:
θ←θ+αgs,s′(θ) =θ+α((r(s,s′) +γϕ(s′)Tθ−ϕ(s)Tθ)ϕ(s)),
where the second equation should be viewed as a definition of gs,s′(θ).
Our goal is to find a parameter vector θ∗such that the average update vector is zero
Es,s′[gs,s′(θ∗)] = 0,
where the expectation is taken with respect to sampled pair of states s,s′. This expectation is also called
mean-path update ¯g(θ)and can be written as:
¯g(θ) =Es,s′[gs,s′(θ)] =Es,s′[(γϕ(s′)Tθ−ϕ(s)Tθ)ϕ(s)] +Es,s′[r(s,s′)ϕ(s)]
:=−Aθ+b,(1)
where the last line should be taken as the definition of the matrix Aand vector b. Finally, the minimum
eigenvalue of the matrix (A+AT)/2plays an important role in our analysis and will be denoted as λA.
There are a few possible settings of the problem: the samples s,s′might be drawn from the MDP on-line
(Markovian sampling) or independently ( i.i.d.sampling): the first state sis drawn from µπ, thens′is
drawn as the next state under the policy π. Another possible setting for analysis is the “finite sample set":
first, a trajectory of length Nis drawn from an MDP following Markovian sampling and forms dataset
D={(st,at,rt,st+1)}N
t=1. Then TD(0) proceeds by drawing samples from this dataset. Note that the
definition of the expectation Es,s′and, consequently, of matrix Awill be slightly different in these two
settings: in the on-line sampling case probability of a pair of states s,s′is determined by the stationary
distribution µπ, and the transition matrix P; we define
Ae=/summationdisplay
s∈S/summationdisplay
s′∈Sµπ(s)P(s,s′)ϕ(s)(ϕ(s)T−γϕ(s′)T).
In the “finite sample" case, the probability of s,s′refers to the probability of getting a pair of states from
one particular data point t:s=st,s′=st+1, and the matrix Ais defined as:
Ad=1
NN/summationdisplay
t=1ϕ(st)(ϕ(st)T−γϕ(st+1)T).
4Published in Transactions on Machine Learning Research (8/2024)
Likewise, the definition of ¯g(θ)differs between the MDP and dataset settings, since that definition involves
Es,s′which, as discussed above, means slightly different things in both settings.
In the sequel, we will occasionally refer to the matrix A. Whenever we make such a statement, we are in fact
making two statements: one for the dataset case when Ashould be taken to be Ad, and one in the on-line
case whenAshould be taken to be Ae.
We make the following standard assumptions:
Assumption 2.1. (Problem solvability) The matrix Ais non-singular.
Assumption 2.2. (Bounded features) ||ϕ(s)||2≤1for alls∈S.
These assumptions are widely accepted and have been utilized in previous research within the field Bhandari
et al. (2018), Du et al. (2017), Korda & La (2015), Liu & Olshevsky (2021), Xu et al. (2020). Assumption
2.1 ensures that A−1bexists and the problem is solvable. At the risk of being repetitive, we note that this
is really two assumptions, one that Aeis non-singular in the on-line case, and one that Adis non-singular
in the dataset case, which are stated together. Assumption 2.2 is made for simplicity and it can be satisfied
by feature vector rescaling.
In our analysis we often use the function f(θ), defined as:
f(θ) = (θ−θ∗)TA(θ−θ∗). (2)
We will use fdandfenotation for the dataset ( A=Ad) and environment ( A=Ae) cases respectively.
3 The TD-SVRG algorithm
Let us consider an optimization problem where the target function f(θ)is the sum of convex functions
f(θ) = (1/N)/summationtextN
i=1fi(θ), and the total number of functions Nis very large. This makes computing the full
gradient (1/N)/summationtextN
i=1∇fi(θ)too costly for every update. Instead, during iteration t, we want to apply an
updategtthat is inexpensive to compute:
θt=θt−1+αgt,
whereαis the learning rate. If we use gt=∇fi(θ)with a randomly chosen i, we obtain a standard SGD
(stochastic gradient descent) algorithm. The challenge with SGD lies in its high variance. One common
approach to mitigate this is by applying so-called variance reduction techniques: we instead update
θt=θt−1+αvt,
where
vt=gt−g′
t+E[g′
t],
for some appropriately defined g′
t. The key idea here is that regardless of how we choose g′
t, we will have
E[vt] =E[gt],
so in expectation the update is the same. On the other hand, if we can choose g′
tto be highly correlated
withgt, then the variance of vtwill be substantially smaller than the variance of gt.
There are several algorithms based on this idea, the most prominent of which are SAG (Schmidt et al.,
2013), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013). These algorithms propose different
methods of constructing g′
t. In this work, we take our inspiration from the SVRG algorithm which suggests to
choosegt’ to be the gradient of the function fichosen at time step t, but estimated on a previous parameter
vector ˜θ:g′
t=∇fi(˜θ),˜θ=θt′,t′<t.
The major drawback of this idea is that E[g′
t] = (1/N)/summationtextN
i=1∇fi(˜θ)– while being a full update for the
parameter vector – is costly to compute, because the motivating scenario here involved large N. However,
5Published in Transactions on Machine Learning Research (8/2024)
Algorithm 1 TD-SVRG for the finite sample case
Parameters update batch size Mand learning rate α.
Initialize ˜θ0.
form′= 1,2,...,mdo
˜θ=˜θm′−1,
¯gm′(˜θ) =1
N/summationtext
s,s′∈Dgs,s′(˜θ),
wheregs,s′(˜θ) = (r(s,s′) +γϕ(s′)T˜θ−ϕ(s)T˜θ)ϕ(st).
θ0=˜θ.
fort= 1toMdo
Samples,s′fromD.
Computevt=gs,s′(θt−1)−gs,s′(˜θ) + ¯gm′(˜θ).
Update parameters θt=θt−1+αvt.
end for
Set˜θm′=θt′for randomly chosen t′∈(0,...,M−1).
end for
it turns out that for SVRG to work well, we don’t need to compute this expectation at every time step, but
rather can re-use the computation from a previous iteration. It is proved in Johnson & Zhang (2013) that
an optimal frequency of updates between computations of the full update E[g′
t(˜θ)]allows the algorithm to
achieve a geometric convergence rate (in contrast to SGD, which does not attain a geometric convergence
rate).
In this paper we propose a modification of the TD(0) method with SVRG technique (TD-SVRG) which can
attain a geometric convergence rate. This algorithm is given above as Algorithm 1. The algorithm works
under the “finite sample set” setting which assumes there already exists a sampled data set D. This is the
same setting as in Du et al. (2017). However, the method we propose does not add regularization and does
not use dual parameters, which makes it considerably simpler.
Like the classic SVRG algorithm, our proposed TD-SVRG has two nested loops. We refer to one step of the
outer loop as an epochand to one step of the inner loop as an iteration. TD-SVRG keeps two parameter
vectors: the current parameter vector θt, which is being updated at every iteration, and the vector ˜θt, which
is updated at the end of each epoch.
Each epoch contains Miterations, which we call update batch size (not to be confused with the estimation
batch size, which will be used in the algorithms below to compute an estimate of the mean-path update).
4 Outline of the Analysis
In this section we briefly discuss a perspective on TD learning which represents the key difference between
our analysis and the previous works. In Xu et al. (2020) the authors note: “In Johnson & Zhang (2013) ,
the convergence proof relies on the relationship between the gradient and the value of the objective function,
but there is not such an objective function in the TD learning problem.” We show, that viewing TD learning
as gradient splitting allows us to find such a function and establish a relationship between the gradient and
the value function.
The concept of viewing TD-learning as gradient splitting comes from Liu & Olshevsky (2021), in which
the authors define the linear function h(θ) =B(θ−θ∗)asgradient splitting of a quadratic function
f(θ) = (θ−θ∗)TA(θ−θ∗)ifB+BT= 2A. Liu & Olshevsky define the function:
f(θ) = (1−γ)||Vθ−Vθ∗||2
D+γ||Vθ−Vθ∗||2
Dir,
whereVθis a vector of state values induced by θ,
||V||2
D=/summationdisplay
sµπ(s)V(s)2
6Published in Transactions on Machine Learning Research (8/2024)
Figure 1: Illustration of gradient splitting. All gradient splittings of the function f(θ)will lie on line l. In
addition, if we have a constraint on the 2-norm of the matrix A, all gradient splittings will lie on an interval
I, thus suggesting that an update in the direction of gradient splitting is almost as good, is an update in the
direction of the true gradient.
is a weighted norm, and
||V||2
Dir=1
2/summationdisplay
s,s′µπ(s)P(s,s′)(V(s)−V(s′))2
is a Dirichlet seminorm. They show that mean-path update −¯g(θ)is a gradient splitting of this function
f(θ), which is how this function naturally appears in the analysis of TD-learning.
Our arguments build on the gradient splitting interpretation of TD, and it is this approach that differentiates
our paper from previous works on variance-reduced policy evaluation. This interpretation provides a tool for
its convergence analysis, since it leads to bounds on a key quantity: the inner product of a gradient splitting
h(θ)and the direction θ∗−θto the minimizer is the same as the inner product of −∇f(θ)andθ∗−θ(see
Figure 1). At the same time, gradient splitting is not the gradient itself, and many properties that hold for
the gradient do not hold for gradient splitting. Please see Appendix E for additional discussion.
A key difficulty to overcome is that, in the “finite sample” case discussed earlier, the two definitions of the
functionf(θ)are no longer equivalent and, as a result, the TD(0) update is no longer a gradient splitting.
This complicates things considerably and our key idea is to view TD updates in this case as a form of an
approximate gradient splitting.
In addition to f(θ), we define the expected square norm of the difference between the current and optimal
parameters as w(θ) :
w(θ) = E[||gs,s′(θ)−gs,s′(θ∗)||2], (3)
where expectation is taken with respect to sampled pair of states s,s′. With this notation we provide a
technical lemma. The next proofs are based on variations of this lemma.
Lemma 4.1. If Assumptions 2.1, 2.2 hold, the epoch parameters of two consecutive epochs m′−1andm′
are related by the following inequality:
2αME[fd(˜θm′)]−2Mα2E[w(˜θm′)]≤E[||˜θm′−1−θ∗||2] + 2α2ME[w(˜θm′−1)], (4)
where the expectation is taken with respect to all previous epochs and choices of states s,s′during the epoch
m.
7Published in Transactions on Machine Learning Research (8/2024)
Proof.The proof of the lemma generally follows the analysis in Johnson & Zhang (2013) and can be found
in Appendix B.
Lemma 4.1 plays an auxiliary role in our analysis and significantly simplifies it. It introduces a new approach
to the convergence proof by carrying iteration to iteration and epoch to epoch bounds to the earlier part
of the analysis. In particular, deriving bounds in terms of some arbitrary function u(θ)is now reduced to
deriving upper bounds on ||˜θm′−1−θ∗||2andw(θ), and a lower bound on f(θ)in terms of the function u.
Three mentioned quantities are natural choices for the function u. In Appendix C we show Lemma 4.1 might
be used to derive convergence in terms of ||˜θm′−1−θ∗||2with similar bounds as in Du et al. (2017). In this
paper we use f(θ)asuto improve on previous results.
5 Main results
Our main results contain 4 theorems which establish convergence for 4 different settings: TD-SVRG for
the finite samples setting (with one extra subseciton which outlines the similarity between the achieved
complexity of TD-SVRG and classical SVRG), batched TD-SVRG for the finite sample setting, TD-SVRG
for i.i.d. online sampling and TD-SVRG for Markovian online sampling.
5.1 Convergence of TD-SVRG for finite sample setting
In this section, we show that Algorithm 1 attains geometric convergence in terms of a specially chosen
functionfd(θ)withαbeingO(1)andMbeingO(1/λA). Before we start note that in general a first state
of the first pair and a second state of the last state pair in the randomly sampled dataset would not be the
same state. That leads to the effect which we call unbalanced dataset : unlike the MDP, the first and second
states distributions in such a dataset are different. In the unbalanced dataset case, mean path update is not
exactly a gradient splitting of the target function f(θ)and we need to introduce a correction term in our
analysis. The following theorem covers the unbalanced dataset case and the balanced dataset case is covered
in the corollary.
Theorem 5.1. Suppose Assumptions 2.1, 2.2 hold and the dataset Dmay be unbalanced. Define the error
termJ=4γ2
NλA. Then, if we choose learning rate α= 1/(8 +J)and update batch size M= 2/(λAα),
Algorithm 1 will have a convergence rate of:
E[fd(˜θm)]≤/parenleftbigg2
3/parenrightbiggm
fd(˜θ0).
Corollary 5.2. If the datasetDis balanced, then we may take the error term is J= 0and consequently
the same convergence rate might be obtained with choices of learning rate α= 1/8and update batch size
M= 16/λA
Proof of Theorem 5.1. The proof is given in Appendix D.
Note that ˜θm′refers to the iterate after miterations of the outer loop. Thus, the total number of samples
guaranteed by this theorem until E[fd(˜θm)]≤ϵis actuallyO((N+ 16/λA) log(1/ϵ))in the balanced case
andO((N+16+2/(NλA)
λA) log(1/ϵ))in the unbalanced case, which means that two complexities are identical
if the dataset size Nis large enough so that N≥λ−1
A.
Even an error term introduced by an unbalanced dataset is negligible in the randomly sampled dataset cases;
it might not be lower bounded by a value less than J. In practice the issue might be tackled by sampling
from a modified dataset this issue is discussed in Appendix F.
5.2 Similarity of SVRG and TD-SVRG
NotethatthedatasetcaseissimilartoSVRGintheconvexsettinginthesensethat: 1)theupdateperformed
at each step is selected uniformly at random, and 2) the exact mean-path update can be computed at every
8Published in Transactions on Machine Learning Research (8/2024)
epoch. If the dataset is balanced, a negative mean-path update −¯g(θ)is a gradient splitting of the function
f(θ). These allow us to further demonstrate the significance of the function f(θ)for the TD learning process
and the greater similarity between TD-learning and convex optimization. We recall the convergence rate
obtained in Johnson & Zhang (2013) for a sum of convex functions:
1
γ′α′(1−2Lα′)M′+2Lα′
1−2Lα′,
whereγ′is a strong convexity parameter and Lis a Lipschitz smoothness parameter (we employ the notation
from the original paper and introduce the symbol′to avoid duplicates). The function f(θ) =1
2(θ−θ∗)TA(θ−
θ∗)isλAstrongly convex and 1-Lipschitz smooth, which means that the convergence rate obtained in this
paper is identical to the convergence rate of SVRG in the convex setting. We provide an intuition that
supports this similarity in Appendix D. This fact further extends the analogy between TD learning and
convex optimization earlier explored by Bhandari et al. (2018) and Liu & Olshevsky (2021).
5.3 TD-SVRG with batching
In this section, we extend our results to an inexact mean-path update computation, applying the results of
Babanezhad Harikandeh et al. (2015) to the TD SVRG algorithm. We show that the geometric convergence
rate might be achieved with a smaller number of computations by estimating the mean-path TD-update
instead of performing full computation. This approach is similar to Peng et al. (2020), but does not require
dual variables and achieves better results.
Since the computation of the mean-path error is not related to the dataset balance, in this section we assume
that the dataset is balanced for simplicity.
Theorem 5.3. Suppose Assumptions 2.1, 2.2 hold and the algorithm runs for a total of mepochs. Then,
if the learning rate is chosen as α= 1/8, the update batch size is M= 16/λA, and the estimation batch
size during epoch m′isnm′= min/parenleftig
N,N
N−11
cλA(2/3)m(4f(˜θm′) +σ2))/parenrightig
, wherecis a parameter and σ2=
E[gs,s′(θ∗)]is an optimal point update variance, Algorithm 2 will converge to the optimum with a convergence
rate of:
E[fd(˜θm)]≤/parenleftbigg2
3/parenrightbiggm
(fd(˜θ0) +C),
whereCis a constant dependent on the parameter c.
Proof.The proof is given in Appendix G.1.
This result is an improvement on Peng et al. (2020), compared to which it improves both the estimation
and update batch sizes. In terms of the update batch size, our result is better by at least a factor of
1/((1−γ)π3
min), whereπminrepresents the minimum probability within the stationary distribution of the
transition matrix, see Table 1 for theoretical results and Section J.4 for experimental comparison. In terms
estimation batch size, we have given the result explicitly in terms of the iterate norm, while Peng et al.
(2020) has a bound in terms of the variance of both primal and dual update vectors ( Ξ2in their notation).
Note, that both quantities f(˜θm′)andσ2required to compute the estimation batch size nm′are not known
duringtherunofthealgorithm. However,weprovideanalternativequantity,whichmightbeusedinpractice:
nm′= min(N,N
N−11
cλA(2/3)m(2|rmax|2+ 8||˜θm′−1||2), where|rmax|is the maximum absolute reward.
5.4 Online i.i.d. sampling from the MDP
We now apply a gradient splitting analysis to TD learning in the case of online i.i.d. sampling from the
MDP each time we need to generate a new state s. We show that our methods can be applied in this case
to derive tighter convergence bounds. One issue of TD-SVRG in the i.i.d. setting is that the mean-path
update may not be computed directly. Indeed, once we have a dataset of size N, we can simply make a
pass through it; but in an MDP setting, it is typical to assume that making a pass through all the states
9Published in Transactions on Machine Learning Research (8/2024)
of the MDP is impossible. The inexactness of mean-path update is addressed with the sampling technique
introduced previously in Subsection 5.3, which makes the i.i.d. case very similar to TD-SVRG with non-exact
mean-path computation in the finite sample case. Thus, the TD-SVRG algorithm for the i.i.d. sampling
case is very similar to Algorithm 2, with the only difference being that states s,s′are being sampled from
the MDP instead of the dataset D. Formal description of the algorithm is provided in Appendix H.
In this setting, geometric convergence is not attainable with variance reduction, which always relies on a
pass through the dataset. Since here one sample is obtained from the MDP at every step, one needs to use
increasing batch sizes. Our algorithm does so, and the next theorem once again improves the scaling with
the condition number from quadratic to linear compared to the previous literature.
Theorem 5.4. Suppose Assumptions 2.1, 2.2 hold. Then if the learning rate is chosen as α= 1/16, the
update batch size as M= 32/λAand the estimation batch size as nm′=1
cλA(2/3)m(4f(θm′) + 2σ2), wherec
is some arbitrary chosen constant, Algorithm 3 will have a convergence rate of:
E[fe(˜θm)]≤/parenleftbigg2
3/parenrightbiggm
(fe(˜θ0) +C1),
whereC1is a constant.
Proof.The proof is given in Appendix H.
This convergence rate will lead to total computational complexity of O(1
λAϵlog(ϵ−1))to achieve accuracy ϵ.
Similarly to the previous section, a quantity1
cλA(2/3)m(2|rmax|2+ 8||˜θm′−1||2)might be used for estimation
batch sizes nm′during practical implementation of the algorithm. Note that the expression |rmax|2+
4||˜θm′−1||2is common in the literature, e.g., it is denoted as D2in Xu et al. (2020).
5.5 Online Markovian sampling from the MDP
The Markovian sampling case is the hardest to analyse due to its dependence on the MDP properties, which
makes establishing bounds on various quantities used during the proof much harder. Leveraging the gradient
splitting view still helps us improve over existing bounds, but the derived algorithm does not have the nice
property of a constant learning rate. To deal with sample-to-sample dependencies we introduce one more
assumption often used in the literature:
Assumption 5.5. For the MDP there exist constants ¯m> 0andρ∈(0,1)such that
sup
s∈SdTV(P(st∈·|s0=s),π)≤¯mρt,∀t≥0,
wheredTV(P,Q)denotes the total-variation distance between the probability measures P and Q.
In the Markovian setting, we also need to employ a projection, which helps to set a bound on the update
vectorv. Following Xu et al. (2020), after each iteration we project the parameter vector on a ball of radius
R(denoted as ΠR(θ) = arg minθ′:|θ′|≤R|θ−θ′|2). We assume that |θ∗|≤R, where the choice of Rthat
satisfies this bound can be found in Section 8.2 at Bhandari et al. (2018). The detailed description of the
algorithm is in Appendix I.
Theorem 5.6. Suppose Assumptions 2.1, 2.2, 5.5 hold. Then, the output of Algorithm 4 satisfies:
E[fe(˜θm)]≤/parenleftbigg3
4/parenrightbiggm
fe(θ0) +8C2
λAnm+ 4α(2G2(4 + 6τmix(α)) + 9R2),
whereC2=4(1+(m−1)ρ)
(1−ρ)[4R2+r2
max].
Proof.The proof is given in Appendix I.
10Published in Transactions on Machine Learning Research (8/2024)
Figure 2: Geometric average performance of different algorithms in the finite sample case. Columns - dataset
source environments: MDP, Acrobot, CartPole and Mountain Car. Rows - performance measurements:
log(f(θ))andlog(|θ−θ∗|).
Theorem 5.6 implies that if we choose s=O(log(1/ϵ)),nm′=O(1/(λAϵ)),α=O(ϵ/log(1/ϵ)andM=
O/parenleftig
log(1/ϵ)
ϵλA/parenrightig
, the total sample complexity is:
O/parenleftbigglog2(1/ϵ)
ϵλA/parenrightbigg
.
This has improved scaling with the condition number λ−1
Acompared toO/parenleftig
1
ϵλ2
Alog(1/ϵ)/parenrightig
in Xu et al. (2020).
6 Experimental results
Figure 2 shows the relative performance of TD-SVRG, GTD2 (Sutton et al., 2009), “vanilla" TD learning
(Sutton, 1988), and PD-SVRG (Du et al., 2017) in the finite sample setting. We used theory-suggested
parametersforTD-SVRG,whereasparametersforPD-SVRGandGTD2areselectedbygridsearch. Datasets
of size 5,000 are generated from 4 environments: Random MDP (Dann et al., 2014), and the Acrobot,
CartPole and Mountain car OpenAI Gym environments (Brockman et al., 2016). The complexity (x-axis on
the graph) is measured in the number of basic updates computations, which is computing an update gs,s′(θ)
for a sampled pair of states s,s′and parameter vector θ. Note that this complexity accounts for both basic
updates required to perform inner loop iterations of the algorithms and updates required to compute or
estimate the mean-path update. As the theory predicts, TD-SVRG and PD-SVRG converge geometrically,
while GTD and vanilla TD converge sub-linearly.
Details on the experiments and grid search can be found in Appendix J. In addition, Appendix J has more
experimental results: comparison of theoretical batch sizes (Appendix J.1), results on a datasets with DQN
produced features (Appendix J.3), results for the dataset case with batched estimation of mean-path update
(Appendix J.4), parameter search results for TD-SVRG algorithm (Appendix J.2), results of experiments
for the online case with i.i.d sampling (Appendix J.5) and Markovian sampling (Appendix J.6). Instructions
and code for reproducing the experiments can be found in our github repository.
11Published in Transactions on Machine Learning Research (8/2024)
7 Conclusions
In the paper we provide improved sample complexity results for variance-reduced policy evaluation. Our
key theoretical finding is that it is possible to reduce the scaling with the condition number of the problem
from quadratic to linear, matching what is known for SVRG in the convex optimization setting, while
simultaneously removing a number of extraneous factors. This results in a many orders of magnitude
improvements for batch size and sample complexity for even simple problems such as random MDPs or
OpenAI Gym problems. Results of this type are attained in several settings, e.g., when a dataset of size N
is sampled from the MDP, and when states of the MDP are sampled online either in an i.i.d. or Markovian
fashion. In simulations we find that our method with step-sizes and batch-sizes coming from our theorems
outperforms algorithms from the previous literature with the same parameters selected by grid search. The
main innovation in the proofs of our results is to draw on a view of TD learning as an approximate splitting
of gradient descent.
References
Reza Babanezhad Harikandeh, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Konečný, and
ScottSallinen. Stopwastingmygradients: Practicalsvrg. InC.Cortes, N.Lawrence, D.Lee, M.Sugiyama,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 28. Curran Associates,
Inc., 2015.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning
with linear function approximation. In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.),
Proceedings of the 31st Conference On Learning Theory , volume 75 of Proceedings of Machine Learning
Research , pp. 1691–1692. PMLR, 06–09 Jul 2018.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym. arXiv preprint arXiv:1606.01540 , 2016.
Gal Dalal, Balázs Szörényi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td(0) with function
approximation. Proceedings of the AAAI Conference on Artificial Intelligence , 32(1), Apr. 2018.
Christoph Dann, Gerhard Neumann, Jan Peters, et al. Policy evaluation with temporal differences: A survey
and comparison. Journal of Machine Learning Research , 15:809–883, 2014.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with sup-
port for non-strongly convex composite objectives. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence,
and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems , volume 27, 2014.
SimonS.Du, JianshuChen, LihongLi, LinXiao, andDengyongZhou. Stochasticvariancereductionmethods
for policy evaluation. In Proceedings of the 34th International Conference on Machine Learning , volume 70
ofProceedings of Machine Learning Research , pp. 1049–1058, 06–11 Aug 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger (eds.), Advances in Neural
Information Processing Systems , volume 26. Curran Associates, Inc., 2013.
Nathaniel Korda and Prashanth La. On td(0) with function approximation: Concentration bounds and a
centered variant with exponential convergence. In Proceedings of the 32nd International Conference on
Machine Learning (ICML) , pp. 626–634, 2015.
Lucien Le Cam. Asymptotic methods in statistical decision theory . Springer Science & Business Media, 2012.
Rui Liu and Alex Olshevsky. Temporal difference learning as gradient splitting. In Proceedings of the 38th
International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research ,
pp. 6905–6913, 18–24 Jul 2021.
Shaocong Ma, Yi Zhou, and Shaofeng Zou. Variance-reduced off-policy tdc learning: Non-asymptotic con-
vergence analysis. Advances in Neural Information Processing Systems , 33:14796–14806, 2020.
12Published in Transactions on Machine Learning Research (8/2024)
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir
Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis
Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, February
2015.
C Narayanan and Csaba Szepesvári. Finite time bounds for temporal difference learning with function
approximation: Problems with some “state-of-the-art” results. Technical report, Technical report, 2017.
Zilun Peng, Ahmed Touati, Pascal Vincent, and Doina Precup. Svrg for policy evaluation with fewer gradient
evaluations. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,
IJCAI-20 , pp. 2697–2703, 7 2020. doi: 10.24963/ijcai.2020/374. Main track.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics , pp. 400–407, 1951.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average
gradient, 2013.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd
learning. In Conference on Learning Theory , pp. 2803–2830. PMLR, 2019.
Richard Sutton. Learning to predict by the methods of temporal differences. Mach Learn, 3 , 1988.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári,
and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function
approximation. In Proceedings of the 26th annual international conference on machine learning , pp. 993–
1000, 2009.
Ahmed Touati, Pierre-Luc Bacon, Doina Precup, and Pascal Vincent. Convergent tree backup and retrace
with function approximation. In International Conference on Machine Learning , pp. 4955–4964. PMLR,
2018.
J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation.
IEEE Transactions on Automatic Control , 42(5):674–690, 1997. doi: 10.1109/9.580874.
Tengyu Xu, Zhe Wang, Yi Zhou, and Yingbin Liang. Reanalysis of variance reduced temporal difference
learning. In International Conference on Learning Representations , 2020.
13Published in Transactions on Machine Learning Research (8/2024)
A Discussion on TD-learning lower bound
In this paper, we want to show that the sample efficiency of TD-learning cannot be lower than O(ϵ−1). This
is easy to demonstrate by utilizing the fact that the policy evaluation problem easily reduces to a mean
estimation problem, for which we have an established bound of O(ϵ−1). Here, we give a simple example of
how to reduce the policy evaluation problem to the problem of estimating the probability pof a Bernoulli
random variable.
Consider a 3 state MDP and a policy which has the following transition matrix between states:
P=
1/2p1/2−p
1/2 1/2 0
1/2 0 1 /2
.
As is standard in policy evaluation, we do not assume the MDP is known; in particular, the parameter pis
unknown. The agent receives reward of 1whenever it moves from state 2and reward of 0otherwise. Thus,
the reward here depends only on the state. The MDP with this policy is geometrically ergodic since P2is a
strictly positive matrix.
Let us compute the value function assuming we start from state 1. This can be done by observing that the
stationary distribution of this probability transition matrix is (1/2,p,1/2−p), and that if we start at node
1, it reaches the stationary distribution after a single step. Thus
V(1) = 0 +γp·1 +γ2p·1 +···=γp
1−γ.
Thus if we take γ= 1/2,V(1) =pand thus estimating the value function with a certain expected square
error will translate to a similar expected square error on estimating p, up to constants. Therefore, it is not
possible to get the error ϵwith better complexity than O(ϵ−1), since it would violate lower bound provided
by LeCam’s method (Le Cam, 2012).
This simple argument states that the results of i.i.d. case analysis reported directly in Ma et al. (2020) and
indirectly implied by Xu et al. (2020) (where complexity better than ϵ−1might be achieved under certain
choices of batch size and learning rate) are not possible to achieve.
14Published in Transactions on Machine Learning Research (8/2024)
B Proof of Lemma 4.1
The proof follows the same logic as in Johnson & Zhang (2013) and is organized in four steps.
StepB.1.In the original paper, the proof starts with deriving a bound on the squared norm of the difference
between the current and optimal parameter vectors. With the introduction of w(θ)this step in our proof is
trivial. We have
Es,s′||gs,s′(θ)−gs,s′(θ∗)||2=w(θ),
where Es,s′denotes the expectation taken with respect to the choice of a random pair of states s,s′. In other
words, Es,s′[·]denotes the conditional expectation with respect to all variables that are not s,s′, which,
recall, are generated at time tby sampling sfrom the stationary distribution and letting s′be the next state.
We will slightly abuse notation to write Es,s′[·]instead of the more rigorous Est,st+1, since what time index
the states are generated at random is usually clear from the context.
StepB.2.During Step 2 we derive a bound on the norm of a single iteration tupdatevt=gs,s′(θt−1)−
gs,s′(˜θ) + ¯g(˜θ), where ¯g(˜θ)is defined in 1 assuming that states s,s′were sampled randomly during step t:
Es,s′[||vt||2] =Es,s′||gs,s′(θt−1)−gs,s′(˜θ) + ¯g(˜θ)||2
=Es,s′||(gs,s′(θt−1)−gs,s′(θ∗)) + (gs,s′(θ∗)−gs,s′(˜θ) + ¯g(˜θ)||2
≤2Es,s′||(gs,s′(θt−1)−gs,s′(θ∗))||2
+ 2Es,s′||gs,s′(˜θ)−gs,s′(θ∗)−(¯g(˜θ)−¯g(θ∗))||2
= 2Es,s′||(gs,s′(θt−1)−gs,s′(θ∗))||2+ 2Es,s′||gs,s′(˜θ)−gs,s′(θ∗)
−Es,s′[gs,s′(˜θ)−gs,s′(θ∗)]||2
≤2Es,s′||(gs,s′(θt−1)−gs,s′(θ∗))||2+ 2Es,s′||gs,s′(˜θ)−gs,s′(θ∗)||2
= 2w(θt−1) + 2w(˜θ).
The first inequality uses E||a+b||2≤2E||a||2+ 2E||b||2. The second inequality uses the fact that the second
central moment is smaller than the second moment. The last equality uses the equality from Step 1.
StepB.3.During this step we derive a bound on the expected squared norm of a distance to the optimal
parameter vector after a single update t:
Es,s′||θt−θ∗||2=Es,s′||θt−1−θ∗+αvt||2
=||θt−1−θ∗||2+ 2α(θt−1−θ∗)TEs,s′vt+α2Es,s′||vt||2
≤||θt−1−θ∗||2+ 2α(θt−1−θ∗)T¯g(θt−1) + 2α2w(θt−1) + 2α2w(˜θ)
=||θt−1−θ∗||2−2αfd(θt−1) + 2α2w(θt−1) + 2α2w(˜θ).
The inequality uses the bound obtained in Step 2 and equality uses gradient splitting properties of ¯g(θt−1) :
(θt−1−θ∗)T¯g(θt−1) = (θt−1−θ∗)T(¯g(θt−1)−¯g(θ∗))
= (θt−1−θ∗)T(−Adθt−1+b+Adθ∗−b)
=−(θt−1−θ∗)TAd(θt−1−θ∗) =−fd(θt−1).(5)
After rearranging terms it becomes:
Es,s′||θt−θ∗||2+ 2αfd(θt−1)−2α2w(θt−1)≤||θt−1−θ∗||2+ 2α2w(˜θ).
StepB.4.During this step we sum the inequality obtained in Step 3 over the epoch and take another
expectation to obtain:
E[M/summationdisplay
t=1||θt−θ∗||2+M/summationdisplay
t=12αEfd(θt−1)−M/summationdisplay
t=12α2w(θt−1)|Fm′−1]≤EM/summationdisplay
t=1||θt−1−θ∗||2+M/summationdisplay
t=12α2w(˜θ)|Fm′−1],(6)
whereFm′−1is the information available in the beginning of epoch m′. We analyze this expression term-wise.
15Published in Transactions on Machine Learning Research (8/2024)
Notice that/summationtextM
t=1||θt−1−θ∗||2and/summationtextM
t=1||θt−θ∗||2consist of the same terms, except the first term in
the first sum and the last term in the last sum, which are ||θ0−θ∗||2and||θM−θ∗||2respectively. Since
||θM−θ∗||2is always positive and it is on the left hand side of the inequality, we could drop it.
We denote the parameter vector θchosen for epoch parameters at the end of the epoch ˜θm′. Since this vector
is chosen uniformly at random among all iteration vectors θt,t∈(0,M−1), we have that/summationtextM
t=1Efd(θt−1) =
MEfd(˜θm′)and/summationtextM
t=1Ew(θt−1) =MEw(˜θm′).
At the same time, ˜θ, which was chosen at the end of the previous epoch remains the same throughout the
epoch, therefore,/summationtextM
t=1Ew(˜θ) =MEw(˜θ). Note, that the current epoch starts with setting θ0=˜θ. Also,
to underline that ˜θduring the current epoch refers to the previous epoch, we denote it as ˜θm′−1. Plugging
these values in (4) we have :
2αMEfd(˜θm′)−2Mα2Ew(˜θm′)≤E||˜θm′−1−θ∗||2+ 2α2MEw(˜θm′−1).
16Published in Transactions on Machine Learning Research (8/2024)
C Convergence in terms of squared norm
At this point, we go on an aside to prove a result that is not in the main body of the paper. We observe it
is possible to derive a bound on Algorithm 1 in the squared norm. This bound is generally worse than the
results we report in the main body of the paper since it scales with the square of the condition number.
Proposition C.1. Suppose Assumptions 2.1, 2.2 hold. If we chose the learning rate as α=λA/32and
update batch size as M= 32/λ2
A, then Algorithm 1 has a convergence rate of:
E[||˜θm′−θ∗||2]≤/parenleftbigg5
7/parenrightbiggm
||˜θ0−θ∗||2.
This leads to batch size MbeingO(1/λ2
A), which is better than the results in Du et al. (2017), since their
results have complexity O(κ2(C)κ2
G), whereκ(C)is the condition number of matrix C=Es∈D[ϕ(s)ϕ(s)T]
andκG∝1/λmin(ATC−1A).
Proof.To transform inequality (4) from Lemma 4.1 into a convergence rate guarantee, we need to bound
w(θ)andfd(θ)in terms of||θ−θ∗||2. Both bounds are easy to show:
w(θ) =Es,s′||gs,s′(θ)−gs,s′(θ∗)||2
= (θ−θ∗)TEs,s′[(γϕ(s′)−ϕ(s))ϕ(s)Tϕ(s)(γϕ(s′)−ϕ(s))T](θ−θ∗)
≤(θ−θ∗)TEs,s′[||(γϕ(s′)−ϕ(s))||·||ϕ(s)||·||ϕ(s)||·||(γϕ(s′)−ϕ(s))||](θ−θ∗)
≤4||θ−θ∗||2,
fd(θ) = (θ−θ∗)TEs,s′[ϕ(s)(ϕ(s)−γϕ(s′))T](θ−θ∗)≥λA||θ−θ∗||2,
where Es,s′denotes the expectation taken with respect to a choice of pair of states s,s′. Plugging these
bounds into Equation (4) we have:
(2αMλA−8Mα2)||˜θm′−θ∗||2≤(1 + 8Mα2)||˜θm′−1−θ∗||2,
which yields an epoch to epoch convergence rate of:
1 + 8Mα2
2αMλA−8Mα2.
For this expression to be <1, we need that αMis set toO(1/λA), which means that αneeds to beO(λA)
forMα2to beO(1). Therefore, Mneeds to beO(1/λ2
A). Settingα=λA/32andM= 32/λ2
Ayields a
convergence rate of 5/7.
17Published in Transactions on Machine Learning Research (8/2024)
D Proof of Theorem 5.1
An analysis of the balanced dataset case follows from unbalanced dataset but for clarity of presentation we
provide a proof for balanced dataset separately, but before diving into it, let us provide an intuition as to
why TD-SVRG in this case exhibits the same convergence as in the convex optimization case.
Let’s assume we are solving a convex optimization problem for the function fd(θ),i.e., we have access to the
true gradients of the functions fs,s′. In this case the update at time tis
g′
t=1
2∇ft(θt) =1
2(At+AT
t)(θ−θ∗).
In this case, the results of the SVRG paper are directly applicable. Instead, in the TD setting, we have
updates of the form:
gt=Atθt+bt.
We can see that the TD update is quite different from the convex update, as it has a different linear function
and an extra term bt, which would affect the convergence as extra noise. However, once we apply the SVRG
technique to these updates, as described in Section 3, the new updates become
v′
t=1
2∇ft(θt)−1
2∇ft(˜θ) +Es,s′[1
2∇fs,s′(˜θ)]
=1
2(At+AT
t)(θt−˜θ) +1
2n/summationdisplay
s,s′(As,s′+AT
s,s′)(˜θ−θ∗)
in the convex case and
vt= (Atθt+bt)−(At˜θ+bt) +Es,s′[As,s′˜θ+bs,s′]
=At(θt−˜θ) +1
n/summationdisplay
s,s′As,s′(˜θ−θ∗),
in the TD case, where we again use the fact Es,s′[bs,s′] =Es,s′[−As,s′θ∗]to establish the equality.
The two updates look much more similar after applying the SVRG technique to them since the extra "noise"
termbtgets canceled with probability 1. Also,vtis a splitting of the true gradient v′
t, which suggests that
the application of vtupdates instead of v′
tupdates results in the same convergence rate. The formal proof
of this fact is given below.
D.1 Balanced dataset case
Similar to the previous section, we start with deriving bounds, but this time we bound ||θ−θ∗||2andw(θ)
in terms of fd(θ). The first bound is straightforward:
fd(θ) = (θ−θ∗)TEs,s′[ϕ(s)(ϕ(s)−γϕ(s′)T](θ−θ∗) =⇒ ||θ−θ∗||2≤1
λAfd(θ),
18Published in Transactions on Machine Learning Research (8/2024)
where Es,s′denotes the expectation taken with respect to a choice of pair of states s,s′. Forw(θ)we have:
w(θ) = (θ−θ∗)TEs,s′[(γϕ(s′)−ϕ(s))ϕ(s)Tϕ(s)(γϕ(s′)−ϕ(s))T](θ−θ∗)
= (θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈D(γϕ(s′)−ϕ(s))ϕT(s)ϕ(s)(γϕ(s′)−ϕ(s))T/bracketrightbig
(θ−θ∗)
≤(θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈D(γϕ(s′)−ϕ(s))(γϕ(s′)−ϕ(s))T/bracketrightbig
(θ−θ∗)
= (θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈Dγ2ϕ(s′)ϕ(s′)T−γϕ(s′)ϕ(s)T/bracketrightbig
(θ−θ∗) +fd(θ)
= (θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈Dγ2ϕ(s)ϕ(s)T−γϕ(s)ϕ(s′)T/bracketrightbig
(θ−θ∗) +fd(θ)
≤2fd(θ),(7)
where the first inequality uses Assumption 2.2, the third equality uses the dataset balance property, and/summationtext
s′γ2ϕ(s′)ϕ(s′)T=/summationtext
sγ2ϕ(s)ϕ(s)T, sincesands′are the same set of states. The last inequality uses the
fact thatγ <1.
Plugging these bounds into Equation (4), we have:
2αMEfd(˜θm′)−4Mα2Efd(˜θm′)≤1
λAEfd(˜θm′−1) + 4α2MEfd(˜θm′−1),
which yields an epoch to epoch convergence rate of:
Efd(˜θm′)≤/bracketleftig1
2λAαM(1−2α)+2α
1−2α/bracketrightig
Efd(˜θm′−1).
Settingα=1
8andM=16
λAwe have the desired inequality.
D.2 Unbalanced dataset case
To prove the theorem we follow the same strategy as in D. For the fd(θ)we can use the same bound:
fd(θ) = (θ−θ∗)TEs,s′[ϕ(s)(ϕ(s)−γϕ(s′)T](θ−θ∗) =⇒ ||θ−θ∗||2≤1
λAfd(θ).
The bound for w(θ)is a little bit more difficult:
w(θ) = (θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈D(γϕ(s′)−ϕ(s))ϕT(s)ϕ(s)(γϕ(s′)−ϕ(s))T/bracketrightbig
(θ−θ∗)
≤(θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈D(γϕ(s′)−ϕ(s))(γϕ(s′)−ϕ(s))T/bracketrightbig
(θ−θ∗)
= (θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈Dγϕ(s′)(γϕ(s′)−ϕ(s))T−ϕ(s)(γϕ(s′)−ϕ(s))T/bracketrightbig
(θ−θ∗)
= (θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈Dγ2ϕ(s′)ϕ(s′)T−γϕ(s′)ϕ(s)T/bracketrightbig
(θ−θ∗) +fd(θ)
= (θ−θ∗)T/bracketleftbig1
N/summationdisplay
s,s′∈Dγ2ϕ(s)ϕ(s)T−γϕ(s)ϕ(s′)T/bracketrightbig
(θ−θ∗) +fd(θ)
+γ2
N(θ−θ∗)T(ϕ(sN+1)ϕ(sN+1)T−ϕ(s1)ϕ(s1)T)(θ−θ∗)T
≤2fd(θ) +γ2
N(θ−θ∗)T(ϕ(sN+1)ϕ(sN+1)T−ϕ(s1)ϕ(s1)T)(θ−θ∗)T.
19Published in Transactions on Machine Learning Research (8/2024)
The first inequality follows from Assumption 2.2. The third equality is obtained by adding and subtracting
γ2
N(θ−θ∗)Tϕ(s1)ϕ(s1)T(θ−θ∗). The second inequality uses the fact that γ2<1. We denote the maximum
eigenvalue of the matrix ϕ(sN+1)ϕ(sN+1)T−ϕ(s1)ϕ(s1)TbyK(note thatK≤ 1). Thus,
w(θ)≤2fd(θ) +γ2K
N||θ−θ∗||2≤fd(θ)(2 +γ2K
NλA)≤fd(θ)(2 +γ2
NλA).
Plugging these bounds into Equation (4) we have:
(2αM−2Mα2(2 +γ2
NλA))Efd(˜θm′)≤(1
λA+ 2α2M(2 +γ2
NλA))fd(˜θm′−1),
which yields a convergence rate of:
1
λA2αM(1−α(2 +γ2
NλA))+α(2 +γ2
NλA)
1−α(2 +γ2
NλA).
To achieve constant convergence rate, for example2
3, we set up αsuch thatα(2 +γ2
NλA) = 0.25, thus the
second term is equal to 1/3 and α=1
8+4γ2
NλA. Then, to make the first term equal to 1/3, we need to set
M=2
λAα=2
λA1
8+4γ2
NλA.
Thus,αis on the order of1
max(1,1/(NλA)))andMis on the order of1
λAmin(1,NλA).
20Published in Transactions on Machine Learning Research (8/2024)
E Properties of gradient splitting
While gradient splitting is one of our main tools, it is not true that this interpretation can be simply used
to carry over results from convex optimization to policy evaluation. To illustrate this point, consider the
following properties of convex functions with L-smooth gradients:
1.||∇f(x)||≤L||x−x∗||2,
2.f(x)−f(x∗)≤∇f(x)T(x−x∗),
3.f(y)≥f(x) +∇f(x)T(y−x),
4.||∇f(x)−∇f(y)||2≤L(∇f(x)−∇f(y))T(x−y),
5.f(y)≤f(x) +∇f(x)T(y−x) +L
2||y−x||2,
6.αf(x) + (1−α)f(y)≤αf(x) + (1−α)f(y)−(α(1−α)/(2L))||∇f(x)−∇f(y)||2,
7.f(y)≥f(x) +∇f(x)T(y−x) +1
2L||∇f(x)−∇f(y)||2.
Now consider the following question: suppose we replace each instance of a gradient by gradient splitting;
which of the above inequalities still hold? It turns out that (2), (4), (6) still work with gradient splittings,
but (1), (3), (5), (7) do not.
Proofs in the convex optimization literature will typically use some subset of the inequalities (1)-(7), and
when porting these arguments to the convex optimization literature, they must be reworked to use only (2),
(4), (6). Sometimes this will be trivial, but sometimes this may require a lot of creativity. Adopting the
proofs to use gradient splitting instead of the gradient is one of the technical contributions of this paper.
21Published in Transactions on Machine Learning Research (8/2024)
F Discussion on Unbalanced dataset
If the dataset balance assumption is not satisfied, it is always possible to modify the MDP slightly and make
it satisfied. Indeed, suppose we are given an MDP Mwith initial state (or distribution) s0and discount
factorγ. We can then modify the transition probabilities by always transitioning to s0with probability p
regardless of state and action chosen (and doing the normal transition from the MDP Mwith probability
1−p), and changing the discount factor to a new γ′. Calling the new MDP M′, we have that:
•It is very easy to draw a dataset from M′such that the last state is the same as the first one (just
make sure to end on a transition to s0!) and the collected dataset will have the dataset balance
property.
•Under appropriate choice of pandγ′, the value function VMin the original MDP can be easily
recovered from the value function of the new MDP VM′.
A formal statement of this is in the comment below. Note that all we need to be able to do is change
the discount factor (which we usually set) as well as be able to restart the MDP (which we can do in any
computer simulation).
The only caveat that the size of the dataset one can draw this way will have to be at least (1−γ)−1in
expectation because to make the above sketch work will require a choice of pthat is essentially proportional
to(1−γ)(see Theorem statement in the next comment for a formal statement). This is not a problem
in practice, as typical discount factors are usually ≈0.99, whereas datasets tend to be many orders of
magnitude bigger than ≈100 = (1−γ)−1. Even a discount factor of ≈0.999, much closer to one than is
used in practice, only forces us to draw a dataset of size 1000in expectation.
Theorem F.1. Choose
γ′=1 +γ
2, p =1−γ
1 +γ,
and consider the pair of MDPs MandM′which are defined in our previous comment. Then the quantities
VM(s)andVM′(s)satisfy the following recursion:
VM(s) =VM′(s) +γ(1−γ)
1 +γ−2γ2VM′(s0)
Proof.LetTdenote be a time step when the first reset appears. We can condition on Tto represent VM′(s)
as:
VM′(s) =∞/summationdisplay
t′=1P(t′=T)E[VM′(s)|t′=T]
=∞/summationdisplay
t′=1(1−p)t′−1p((t′/summationdisplay
t=1γ′t−1E[rt]) +γ′t′VM′(s0)),
where the expected rewards E[rt]are the same as in the original MDP. We next change the order of sum-
mations:
VM′(s) =∞/summationdisplay
t=1(γ′t−1E[r(t)]∞/summationdisplay
t′=t(1−p)t′−1p) +∞/summationdisplay
t′=1(1−p)t′−1pγ′t′VM′(s0)
=∞/summationdisplay
t′=1γ′t′−1(1−p)t′−1E[r(t)] + (1−p)t′−1pγ′t′VM′(s0).
22Published in Transactions on Machine Learning Research (8/2024)
Now we use the fact that the chosen γ′=γ/(1−p)and perform some algebraic manipulations:
VM′(s) =∞/summationdisplay
t′=1γt′−1E[r(t)] + (1−p)t′−1pγ′t′VM′(s0)
=VM(s) +∞/summationdisplay
t′=1(1−p)t′−1pγ′t′VM′(s0)
=VM(s) +γp
1−γpVM′(s0),
which implies the claimed equality:
VM(s) =VM′(s)−γ(1−γ)
1 +γ−2γ2VM′(s0)
As claimed above, this theorem can be used to recover VMfromVM′. Consequently, the artificial addition of
a reset button as above makes it possible to generate a dataset which satisfies our dataset balance assumption
from any MDP.
23Published in Transactions on Machine Learning Research (8/2024)
G TD-SVRG with batching exact algorithm and proof
The algorithm for the batching case is given as follows:
Algorithm 2 TD-SVRG with batching for the finite sample case
Parameters update batch size Mand learning rate α.
Initialize ˜θ0.
form′= 1,2,...,mdo
˜θ=˜θm′−1,
choose estimation batch size nm′,
sample batchDm′of sizenm′fromDw/o replacement,
computegm′(˜θ) =1
nm′/summationtext
s,s′∈Dm′gs,s′(˜θ),
wheregs,s′(˜θ) = (r(s,s′) +γϕ(s′)T˜θ−ϕ(s)T˜θ)ϕ(st).
θ0=˜θ.
fort= 1toMdo
Samples,s′fromD.
Computevt=gs,s′(θt−1)−gs,s′(˜θ) +gm′(˜θ).
Update parameters θt=θt−1+αvt.
end for
Set˜θm′=θt′for randomly chosen t′∈(0,...,M−1).
end for
G.1 Proof of Theorem 5.3
In the first part of the proof we derive an inequality which relates model parameters of two consecutive
epochs similar to what we achieved in previous proofs, but now we introduce error vector to show that the
mean path update is estimated instead of being computed exactly. In this proof, we follow the same 4 steps
we introduced in the proof of Lemma 4.1. In the second part of the proof we show that there are conditions
under which the error term converges to 0.
StepG.1.During the first step we use the bound obtained in inequality (7):
w(θ)≤2fd(θ).
StepG.2.During this step we derive a bound on the squared norm of a single update E[||vt||2]. But now,
compared to previous case, we do not compute the exact mean-path updated ¯g(θ), but its estimate, and
assume our computation has error gm′(θ) = ¯g(θ)+ηm′. Thus, during iteration tof epochmthe single update
vector is
vt=gt(θt−1)−gt(˜θ) + ¯g(˜θ) +ηm′.
24Published in Transactions on Machine Learning Research (8/2024)
Taking expectation conditioned on all history previous to epoch m, which we denote as Fm′−1, the bound
on the single update can be derived as:
E[||vt||2|Fm′−1] =E[||gt(θt−1)−gt(˜θ) + ¯g(˜θ) +ηm′||2|Fm′−1]
=E[||(gt(θt−1)−¯g(θ∗)) + (¯g(θ∗)−gt(˜θ) + ¯g(˜θ) +ηm′)||2|Fm′−1]
≤2E[||(gt(θt−1)−gt(θ∗))||2|Fm′−1]+
2E[||gt(˜θ)−gt(θ∗)−(¯g(˜θ)−¯g(θ∗))−ηm′||2|Fm′−1]
= 2E[||(gt(θt−1)−gt(θ∗))||2|Fm′−1]+
2E[||gt(˜θ)−gt(θ∗)−E[gt(˜θ)−gt(θ∗)]−ηm′||2|Fm′−1]
= 2E[||(gt(θt−1)−gt(θ∗))||2|Fm′−1]+
2E[||gt(˜θ)−gt(θ∗)−E[gt(˜θ)−gt(θ∗)]||2|Fm′−1]
−4E[⟨gt(˜θ)−gt(θ∗)−E[gt(˜θ)−gt(θ∗)],ηm′⟩|Fm′−1] + 2E[||ηm′||2|Fm′−1]
≤2E[||(gt(θt−1)−gt(θ∗))||2|Fm′−1]+
2E[||gt(˜θ)−gt(θ∗)||2|Fm′−1] + 2E[||ηm′||2|Fm′−1]
= 2E[w(θt−1) + 2w(˜θ) + 2||ηm′||2|Fm′−1]
≤E[4fd(θt−1) + 4fd(˜θ) + 2E||ηm′||2|Fm′−1],
where the first inequality uses E||A+B||2≤2E||A||2+ 2E||B||2, the second inequality uses E||A−E[A]||2≤
E||A||2and the fact E[ηm′|g(˜θ)−g(θ∗)−Es,s′[g(˜θ)−g(θ∗)],Fm′−1] = 0; and the third inequality uses the
result of Step G.1.
StepG.3.During this step, we derive a bound on a vector norm after a single update:
E[||θt−θ∗||2|Fm′−1] =E[||θt−1−θ∗+αvt||2|Fm′−1]
=E[||θt−1−θ∗||2+ 2α(θt−1−θ∗)Tvt+α2||vt||2|Fm′−1]
≤E[||θt−1−θ∗||2+ 2α(θt−1−θ∗)T¯g(θt−1) + 2α(θt−1−θ∗)Tηm′
4α2fd(θt−1) + 4α2fd(˜θ) + 2α2||ηm′||2|Fm′−1]
=E[||θt−1−θ∗||2−2αfd(θt−1) + 2α(θt−1−θ∗)Tηm′
+ 4α2fd(θt−1) + 4α2fd(˜θ) + 2α2||ηm′||2|Fm′−1],
where the first inequality uses
E[2α(θt−1−θ∗)Tvt|Fm′−1] =E[2α(θt−1−θ∗)T(gt(θt−1)−gt(˜θ) + ¯g(˜θ) +ηm′)|Fm′−1]
=E[2α(θt−1−θ∗)T(¯g(θt−1)−¯g(˜θ) + ¯g(˜θ) +ηm′)|Fm′−1],
and the last equality uses (5). Rearranging terms we obtain:
E[||θt−θ∗||2+ 2αfd(θt−1)−4α2fd(θt−1)|Fm′−1]
≤E[||θt−1−θ∗||2+ 4α2fd(˜θ) + 2α(θt−1−θ∗)Tηm′+ 2α2||ηm′||2|Fm′−1]
≤E[||θt−1−θ∗||2+ 4α2fd(˜θ) + 2α||θt−1−θ∗||·||ηm′||+ 2α2||ηm′||2|Fm′−1].
≤E[||θt−1−θ∗||2+ 4α2fd(˜θ) + 2α(λA
2||θt−1−θ∗||2+1
2λA||ηm′||2) + 2α2||ηm′||2|Fm′−1].
StepG.4.Now derive a bound on epoch update. We use similar logic as during the proof of Theorem 5.1.
Since the error term doesn’t change over the epoch, summing over the epoch we have:
25Published in Transactions on Machine Learning Research (8/2024)
E[||θm′−θ∗||2+ 2αMfd(˜θm′)−4α2Mfd(˜θm′)|Fm′−1]≤
||θ0−θ∗||2+ 4α2Mfd(˜θm′−1) +E[2αM/summationdisplay
t=1(λA
2||θt−1−θ∗||2+1
2λA||ηm′||2) + 2α2M||ηm′||2|Fm′−1] =
||˜θm′−1−θ∗||2+ 4α2Mfd(˜θm′−1) +E[2αM(λA
2||˜θm′−θ∗||2+1
2λA||ηm′||2) + 2α2M||ηm′||2|Fm′−1]≤
1
λAfd(˜θm′−1) + 4α2Mfd(˜θm′−1) +E[2αM/parenleftbigg1
2fd(˜θm′) +1
2λA||ηm′||2/parenrightbigg
+ 2α2M||ηm′||2|Fm′−1].
Rearranging terms, dropping E||θm′−θ∗||2and dividing by 2αMwe further obtain:
/parenleftbigg1
2−2α/parenrightbigg
E[fd(˜θm′)|Fm′−1]≤
/parenleftbigg1
2αMλA+ 2α/parenrightbigg
fd(˜θm′−1) +/parenleftbigg1
2λa+α/parenrightbigg
E[||ηm′||2|Fm′−1]
Dividing both sides of this equation to 0.5−2αwe have the epoch convergence:
E[fd(˜θm′)|Fm′−1]≤/parenleftbigg1
λA2αM(0.5−2α)+2α
0.5−2α/parenrightbigg
fd(˜θm′−1)+
/parenleftbigg1
2λa(0.5−2α)+α
0.5−2α/parenrightbigg
E[||ηm′||2|Fm′−1].(8)
To achieve convergence, we need to guarantee the linear convergence of the first and second terms in the
sum separately. The first term is dependent on inner loop updates; its convergence is analyzed in Theorem
5.1. Here we show how to achieve a similar geometric convergence rate of the second term. Since the error
term has 0 mean and we are in a finite sample case with replacement, the expected squared norm can be
bounded by:
E||ηm′||2≤N−nm′
Nnm′S2≤/parenleftig
1−nm′
N/parenrightigS2
nm′≤S2
nm′,
whereS2is a bound on the update vector norm variance. If we want the error to be bounded by cρm, we
need the estimation batch size nm′to satisfy the condition:
nm′≥S2
cρm′.
until growing batch size reaches sample size. Satisfying this condition, guarantees that the second term has
geometric convergence:
/parenleftbigg1
2λa(0.5−2α)+α
0.5−2α/parenrightbigg
E||ηm′||2≤/parenleftbigg1
2λa(0.5−2α)+α
0.5−2α/parenrightbigg
cρm.
26Published in Transactions on Machine Learning Research (8/2024)
It remains to derive a bound S2for the update vector norm sample variance:
1
N−1/summationdisplay
s,s′||gs,s′(θ)||2−||¯g(θ)||2≤
N
N−11
N/summationdisplay
s,s′||gs,s′(θ)||2=N
N−11
N/summationdisplay
s,s′||gs,s′(θ)−gs,s′(θ∗) +gs,s′(θ∗)||2≤
N
N−11
N/summationdisplay
s,s′2||gs,s′(θ)−gs,s′(θ∗)||2+ 2||gs,s′(θ∗)||2=
N
N−1(2w(θ) + 2σ2)≤N
N−1(4f(θ) + 2σ2) =S2,
whereσ2is the variance of the updates in the optimal point similar to Bhandari et al. (2018).
Alternatively, we might derive a bound S2in terms of quantities known during the algorithm execution:
1
N−1/summationdisplay
s,s′||gs,s′(θ)||2−||¯g(θ)||2≤
N
N−11
N/summationdisplay
s,s′||gs,s′(θ)||2=N
N−11
N/summationdisplay
s,s′||(r(s,s′) +γϕ(s′)Tθ−ϕ(s)Tθ)ϕ(s)||2≤
N
N−11
N/summationdisplay
s,s′2||rϕ(s)||2+ 4||γϕ(s′)Tθϕ(s)||2+ 4||ϕ(s)Tθϕ(s)||2≤
N
N−1(2|rmax|2+ 4γ2||θ||2+ 4||θ||2) =N
N−1(2|rmax|2+ 8||θ||2) =S2.
Having the convergence of the both terms of 8, we proceed by expanding the equation for earlier epochs
(denoting bracket terms as ρandρ′):
E[fd(˜θm′)|Fm′−1]≤ρfd(˜θm′−1) +ρ′E[||ηm′||2|Fm′−1] =⇒
E[fd(˜θm′)|Fm−2]≤ρ2fd(˜θm−2) +ρ′(E[||ηm′||2|Fm−2] +ρE[||ηm′||2|Fm′−1]) =⇒
E[fd(˜θm′)]≤ρmfd(˜θ0) +ρ′(m/summationdisplay
i=1ρiE[||ηi||2|Fi])
Now, assuming that estimation batch sizes are large enough that all error terms are bounded by cρm:
E[fd(˜θm′)|Fm′−1]≤ρmfd(˜θ0) +ρ′(m/summationdisplay
i=1ρicρm)≤ρmfd(˜θ0) +ρmcρ′
1−ρ.
Denotingcρ′
1−ρasCwe have the claimed result.
27Published in Transactions on Machine Learning Research (8/2024)
H Proof of Theorem 5.4
Algorithm 3 TD-SVRG for the i.i.d. sampling case
Parameters update batch size Mand learning rate α.
Initialize ˜θ0.
form′= 1,2,...,mdo
˜θ=˜θm′−1,
choose estimation batch size nm′,
sample batchDm′of sizenm′,
computegm′(˜θ) =1
nm′/summationtext
s,s′∈Dm′gs,s′(˜θ),
wheregs,s′(˜θ) = (r(s,s′) +γϕ(s′)T˜θ−ϕ(s)T˜θ)ϕ(st).
θ0=˜θ.
fort= 1toMdo
Samples,s′fromD.
Computevt=gs,s′(θt−1)−gs,s′(˜θ) +gm′(˜θ).
Update parameters θt=θt−1+αvt.
end for
Set˜θm′=θt′for randomly chosen t′∈(0,...,M−1).
end for
The proof is very similar to 8, the only difference is that now we derive an expectation with respect to an
MDP instead of a finite sample dataset.
StepH.1.During the first step we use the bound obtained during the proof of Theorem 5.1:
w(θ) = (θ−θ∗)TE[(γϕ(s′)−ϕ(s))ϕ(s)Tϕ(s)(γϕ(s′)−ϕ(s))T](θ−θ∗)
= (θ−θ∗)T/bracketleftbig/summationdisplay
s,s′µπ(s)P(s,s′)(γϕ(s′)−ϕ(s))ϕT(s)ϕ(s)(γϕ(s′)−ϕ(s))T/bracketrightbig
(θ−θ∗)
≤(θ−θ∗)T/bracketleftbig/summationdisplay
s,s′µπ(s)P(s,s′)(γϕ(s′)−ϕ(s))(γϕ(s′)−ϕ(s))T/bracketrightbig
(θ−θ∗)
= (θ−θ∗)T/bracketleftbig/summationdisplay
s,s′µπ(s)P(s,s′)(γ2ϕ(s′)ϕ(s′)T−γϕ(s′)ϕ(s)T)/bracketrightbig
(θ−θ∗) +fe(θ)
= (θ−θ∗)T/summationdisplay
s,s′µπ(s)P(s,s′)(γ2ϕ(s)ϕ(s)T−γϕ(s)ϕ(s′)T)/bracketrightbig
(θ−θ∗) +fe(θ)
≤2fe(θ),(9)
where the first inequality uses Assumption 2.2, the third equality uses the fact that µπis a stationary
distribution of P(/summationtext
s′γ2µπ(s)P(s,s′)ϕ(s′)ϕ(s′)T=/summationtext
s′γ2µπ(s′)ϕ(s′)ϕ(s′)T=/summationtext
sµπ(s)γ2ϕ(s)ϕ(s)T). The
last inequality uses the fact that γ <1.
StepH.2.During this step we derive a bound on the squared norm of a single update E[||vt||2], which
is performed during time step tof epochm. Since we are aiming to derive epoch to epoch convergence
bound, we will be taking expectation conditioned on all history previous to epoch m, which we denote as
Fm′−1. Similarly with Appendix G.1 we assume that mean path update in the end of the previous epoch
was computed inexactly and has estimation error: ¯g(˜θm′−1) +ηm′. Thus the single update vector becomes
(for simplicity we denote ˜θm′−1as˜θ):
vt=gt(θt−1)−gt(˜θ) + ¯g(˜θ) +ηm′.
28Published in Transactions on Machine Learning Research (8/2024)
The norm of this vector is bounded by:
E[||vt||2|Fm′−1] =E[||gt(θt−1)−gt(˜θ) + ¯g(˜θ) +ηm′||2|Fm′−1]
=E[||(gt(θt−1)−¯g(θ∗)) + (¯g(θ∗)−gt(˜θ) + ¯g(˜θ) +ηm′)||2|Fm′−1]
≤2E[||(gt(θt−1)−gt(θ∗))||2|Fm′−1]+
2E[||gt(˜θ)−gt(θ∗)−(¯g(˜θ)−¯g(θ∗))−ηm′||2|Fm′−1]
= 2E[||(gt(θt−1)−gt(θ∗))||2|Fm′−1]+
2E[||gt(˜θ)−gt(θ∗)−E[gt(˜θ)−gt(θ∗)]−ηm′||2|Fm′−1]
= 2E[||(gt(θt−1)−gt(θ∗))||2|Fm′−1]+
2E[||gt(˜θ)−gt(θ∗)−E[gt(˜θ)−gt(θ∗)]||2|Fm′−1]
−E[⟨gt(˜θ)−gt(θ∗)−E[gt(˜θ)−gt(θ∗)],ηm′⟩|Fm′−1] + 2E[||ηm′||2|Fm′−1]
≤2E[||(gt(θt−1)−gt(θ∗))||2|Fm′−1]+
2E[||gt(˜θ)−gt(θ∗)||2|Fm′−1] + 2E[||ηm′||2|Fm′−1]
= 2E[w(θt−1) + 2w(˜θ) + 2||ηm′||2|Fm′−1]
≤E[4fe(θt−1) + 4fe(˜θ) + 2||ηm′||2|Fm′−1],
where the first inequality uses E||A+B||2≤2E||A||2+ 2E||B||2, the second inequality uses E||A−E[A]||2≤
E||A||2and the fact E[ηm′|g(˜θ)−g(θ∗)−Es,s′[g(˜θ)−g(θ∗)],Fm′−1] = 0; and the third inequality uses the
result of Step H.1.
StepH.3.We obtain a bound on a vector norm after a single update during iteration tof epochm:
E[||θt−θ∗||2|Fm′−1] =E[||θt−1−θ∗+αvt||2|Fm′−1]
=E[||θt−1−θ∗||2+ 2α(θt−1−θ∗)Tvt+α2||vt||2|Fm′−1]
=E[||θt−1−θ∗||2+ 2α(θt−1−θ∗)T¯g(θt−1) + 2α(θt−1−θ∗)Tηm′
+ 4α2fe(θt−1) + 4α2fe(˜θ) + 2α2||ηm′||2|Fm′−1].
Applying an argument similar to 5 and rearranging terms we obtain:
E[||θt−θ∗||2+ 2αfe(θt−1)−4α2fe(θt−1)|Fm′−1]
≤E[||θt−1−θ∗||2+ 4α2fe(˜θ)−2α(θt−1−θ∗)Tη+ 2α2||ηm′||2|Fm′−1]
≤E[||θt−1−θ∗||2+ 4α2fe(˜θ) + 2α||θt−1−θ∗||·||η||+ 2α2||ηm′||2|Fm′−1].
≤E[||θt−1−θ∗||2+ 4α2fe(˜θ) + 2α(λA
2||θt−1−θ∗||2+1
2λA||ηm′||2) + 2α2||ηm′||2|Fm′−1].
StepH.4.Now derive a bound on epoch update. We use the similar logic as during the proof of Theorem
5.1. Since the error term doesn’t change over the epoch, summing over the epoch we have:
E[||θm′−θ∗||2+ 2αMfe(˜θm′)−4α2Mfe(˜θm′)|Fm′−1]≤
||θ0−θ∗||2+ 4α2Mfe(˜θm′−1) +E[2αM/summationdisplay
t=1(λA
2||θt−1−θ∗||2+1
2λA||ηm′||2) + 2α2M||ηm′||2|Fm′−1] =
||˜θm′−1−θ∗||2+ 4α2Mfe(˜θm′−1) +E[2αM(λA
2||˜θm′−θ∗||2+1
2λA||ηm′||2) + 2α2M||ηm′||2|Fm′−1]≤
1
λafe(˜θm′−1) + 4α2Mfe(˜θm′−1) +E[2αM/parenleftbigg1
2fe(˜θm′) +1
2λA||ηm′||2/parenrightbigg
+ 2α2M||ηm′||2|Fm′−1]
29Published in Transactions on Machine Learning Research (8/2024)
Rearranging terms, dropping E||θm′−θ∗||2and dividing by 2αMwe further obtain:
/parenleftbigg1
2−2α/parenrightbigg
E[fe(˜θm′)|Fm′−1]≤
/parenleftbigg1
2αMλA+ 2α/parenrightbigg
fe(˜θm′−1) +/parenleftbigg1
2λa+α/parenrightbigg
E[||ηm′||2|Fm′−1]
Dividing both sides of this equation to 0.5−2αwe have the epoch convergence:
E[fe(˜θm′)|Fm′−1]≤/parenleftbigg1
λA2αM(0.5−2α)+2α
0.5−2α/parenrightbigg
fe(˜θm′−1)+
/parenleftbigg1
2λa(0.5−2α)+α
0.5−2α/parenrightbigg
E[||ηm′||2|Fm′−1].
Similarly to Appendix D, convergence for the first term might be obtained by setting the learning rate to
α= 1/16and the update batch size to M= 32/λA. To guarantee convergence of the second term, we need
to bound E||ηm′||2. In the infinite population with replacement case, the norm of the error vector is bounded
by:
E||ηm′||2≤S2
nm′,
whereS2is a bound update vector norm variance. If we want the error to be bounded by cρm, we need the
estimation batch size nm′to satisfy the condition:
nm′≥S2
cρm.
Satisfying this condition guarantees that the second term has geometric convergence:
/parenleftbigg1
2λa(0.5−2α)+α
0.5−2α/parenrightbigg
E||ηm′||2≤/parenleftbigg1
2λa(0.5−2α)+α
0.5−2α/parenrightbigg
cρm.
Similarly to Appendix G.1, the bound on sample variance S2can be derived as follows:
/summationdisplay
s,s′µπ(s)P(s,s′)||gs,s′(θ)||2−||¯g(θ)||2≤
/summationdisplay
s,s′µπ(s)P(s,s′)||gs,s′(θ)−gs,s′(θ∗) +gs,s′(θ∗)||2≤
/summationdisplay
s,s′µπ(s)P(s,s′)2||gs,s′(θ)−gs,s′(θ∗)||2+ 2||gs,s′(θ∗)||2=
2w(θ) + 2σ2≤4f(θ) + 2σ2=S2,
whereσ2is the variance of the updates in the optimal point similar to Bhandari et al. (2018).
An alternative bound on S2with known quantities for practical implementation:
/summationdisplay
s,s′µπ(s)P(s,s′)||gs,s′(θ)||2−||¯g(θ)||2≤
/summationdisplay
s,s′µπ(s)P(s,s′)||gs,s′(θ)||2=/summationdisplay
s,s′µπ(s)P(s,s′)(||(r(s,s′) +γϕ(s′)Tθ−ϕ(s)Tθ)ϕ(s)||2)≤
/summationdisplay
s,s′µπ(s)P(s,s′)(2||rϕ(s)||2+ 4||γϕ(s′)Tθϕ(s)||2+ 4||ϕ(s)Tθϕ(s)||2)≤
(2|rmax|2+ 4γ2||θ||2+ 4||θ||2) = (2|rmax|2+ 8||θ||2) =S2.
30Published in Transactions on Machine Learning Research (8/2024)
Setting hyperparameters to obtained values will results in final computational complexity of O(1
ϵλAlog(ϵ−1)).
31Published in Transactions on Machine Learning Research (8/2024)
I Proof of Theorem 5.6
Algorithm 4 TD-SVRG for the Markovian sampling case
Parameters update batch size Mand learning rate αand projection radius R.
Initialize ˜θ0.
form= 1,2,...,mdo
˜θ=˜θm′−1,
choose estimation batch size nm′,
sample trajectory Dm′of lengthnm′,
computegm′(˜θ) =1
nm′/summationtext
s,s′∈Dm′gs,s′(˜θ),
wheregs,s′(˜θ) = (r(s,s′) +γϕ(s′)T˜θ−ϕ(s)T˜θ)ϕ(st).
θ0=˜θ.
fort= 1toMdo
Samples,s′fromD.
Computevt=gs,s′(θt−1)−gs,s′(˜θ) +gm′(˜θ).
Update parameters θt= ΠR(θt−1+αvt).
end for
Set˜θm′=θt′for randomly chosen t′∈(0,...,M−1).
end for
In this section we show the convergence of the Algorithm 4, which might be applied in the Markovian
sampling case. In this case, we cannot simply apply Lemma 4.1; due to high estimation bias the bounds on
fe(θ)andw(θ)will not be derived based on the current value of θ, but based on global constraints on the
updates guaranteed by applying projection.
First, we analyse a single iteration on step tof epochm, during which we apply the update vector vt=
gt(θ)−gt(˜θ) +gm′(˜θ). The update takes the form:
E||θt−θ∗||2
2=E||ΠR(θt−1+αvt)−ΠR(θ∗)||2
2≤E||θt−1−θ∗+ (−αvt)||2
2=
||θt−1−θ∗||2
2+ 2α(θt−1−θ∗)TE[vt] +α2E||vt||2
2=
||θt−1−θ∗||2
2+ 2α(θt−1−θ∗)T(E[gt(θt−1)]−E[gt(˜θ)] +gm′(˜θ))+
α2E||vt||2
2,(10)
where the expectation is taken with respect to s,s′sampled during iteration t. Recall that under Markovian
sampling, E[gt(θt−1)]̸= ¯g(θt−1)and that for the expectation of the estimated mean-path update we have
E[gm′(˜θ)|sm′−1]̸= ¯g(˜θ), wheresm′−1is the last state of epoch m−1. To tackle this issue, we follow the
approach introduced in Bhandari et al. (2018) and Xu et al. (2020), and rewrite the expectation as a sum of
mean-path updates and error terms. Similar to Bhandari et al. (2018), we denote the error term on a single
update asζ:
ζt(θ) = (θ−θ∗)T(gt(θ)−¯g(θ)).
For an error term on the trajectory, we follow Xu et al. (2020) and denote it as ξ:
ξm′(θ,˜θ) = (θ−θ∗)T(gm′(˜θ)−¯g(θ)).
Applying this notation, (10) can be rewritten as:
E||θt−θ∗||2
2≤||θt−1−θ∗||2
2+
2α(θt−1−θ∗)T(E[gt−1(θt−1)]−E[gt(˜θ)] +gm′(˜θ)) +α2E||vt||2
2=
||θt−1−θ∗||2
2+ 2α/bracketleftbig
(E[ζt(θt−1)] + (θt−1−θ∗)T¯g(θt−1))−
(E[ζt(˜θ)]−(θt−1−θ∗)T¯g(˜θ))+
(E[ξ(θt−1,˜θ)]−(θt−1−θ∗)T¯g(˜θ))/bracketrightbig
+α2E||vt||2
2.(11)
32Published in Transactions on Machine Learning Research (8/2024)
The error terms can be bounded by slightly modified lemmas from the original papers. For ζ(θ), we apply
a bound from Bhandari et al. (2018), Lemma 11:
|E[ζt(θ)]|≤G2(4 + 6τmix(α))α. (12)
In the original lemma, a bound on E[ζt(θ)]is stated, however, in the proof a bound on absolute value of the
expectation is also derived.
For the mean-path estimation error term, we use a modified version of Lemma 1 Xu et al. (2020). The proof
of this lemma in the original paper starts by applying the inequality
aTb≤k
2||a||2+1
2k||b||2
to the expression (θt−1−θ∗)T(gm′(˜θ)−¯g(θ)), withk=λA/2(using the notation in Xu et al. (2020)). For
the purposes of our proof we use k=λA. Thus, we will have the expression:
E[ξm′(θt−1,˜θ)]≤λA
2E[||θt−1−θ∗||2
2|sm′−1] +4(1 + (m−1)ρ)
λA(1−ρ)nm′[4R2+r2
max] =
λA
2E[||θt−1−θ∗||2
2|sm′−1] +C2
λAnm′.(13)
Also, note, that the term E||vt||2
2might be bounded as E||vt||2
2≤18R2. Plugging these bounds into (11) we
obtain:
E||θt−θ∗||2
2≤||θt−1−θ∗||2
2−2αfe(θt−1) + 4α2G2(4 + 6τmix(α))+
2α/parenleftbiggλA
2||θt−1−θ∗||2
2+C2
λAnm′/parenrightbigg
+ 18α2R2.
Summing the inequality over the epoch and taking expectation with respect to all previous history, we have:
2αME[fe(˜θs)]≤||˜θs−1−θ∗||2
2+ 2αM/parenleftbiggλA
2||˜θs−1−θ∗||2
2+C2
λAnm′/parenrightbigg
+
α2M(4G2(4 + 6τmix(α)) + 18R2).
Then we divide both sides by 2αMand use||˜θs−1−θ∗||2
2≤fe(˜θs−1)/λAto obtain:
E[fe(˜θs)]≤/parenleftbigg1
2λAαM+1
2/parenrightbigg
fe(˜θs−1) +C2
λAnm′+
α(2G2(4 + 6τmix(α)) + 9R2).
We choose αandMsuch thatαMλA= 2. We then apply this inequality to the value of the function fin
the first term of the right-hand side recursively, which yields the desired result:
E[fe(˜θs)]≤/parenleftbigg3
4/parenrightbiggs
fe(θ0) +8C2
λAnm′+ 4α(2G2(4 + 6τmix(α)) + 9R2).
33Published in Transactions on Machine Learning Research (8/2024)
J Additional experiments
J.1 Comparison of theoretic batchsizes
In this subsection, we compare the values of update batch sizes which are theoretically required to guarantee
convergence. We compare batch sizes of three algorithms: TD-SVRG, PDSVRG (Du et al. (2017)) and
VRTD (Xu et al. (2020)). Note that PDSVRG and VRTD are algorithms for different settings, but for
TD-SVRG the batch size value is the same: 16/λA, thus, we compare two algorithms in the same table. We
compare the batch sizes required by the algorithm for three MDPs: a first MDP with 50 state, 20 actions
andγ= 0.8, a second MDP with 400 states, 10 actions and γ= 0.95, and a third MDP with 1000 states, 20
actions and γ= 0.99, with actions selection probabilities generated from U[0,1)(similar to the settings used
for the experiments in Sections 6 and J.5). Since the batch size is dependent on the smallest eigenvalue of
the matrixA, which, in turn, is dependent on the dimensionality of the feature vector, we do the comparison
for different feature vector sizes: 5, 10, 20 and 40 randomly generated features and 1 constant feature for
each state. We generate 10 datasets and environments for each feature size. Our results are summarized in
Figure 3 and Tables 3, 2 and 5.
Table 3: Comparison of theoretically suggested batch sizes for an MDP with 50 states, 20 actions and
γ= 0.8. Values in the first row indicate the demensionality of the feature vectors. Values in the other rows:
batch size of the corresponding method. Values are averaged over 10 generated datasets and environments.
Method/Features 6 11 21 41
TD-SVRG 2339 6808 21553 4.51·105
PD-SVRG 1.52·10163.09·10191.85·10231.41·1036
VRTD 3.07·1062.13·1073.79·108165·1011
Table 4: Comparison of theoretically suggested batch sizes for an MDP with 400 states, 10 actions and
γ= 0.95. Values in the first row indicate the demensionality of the feature vectors. Values in the other rows:
batch size of the corresponding method. Values are averaged over 10 generated datasets and environments.
Method/Features 6 11 21 41
TD-SVRG 3176 6942 18100 54688
PD-SVRG 1.72·10163.83·10183.06·10215.77·1024
VRTD 5.41·1062.53·1071.63·1081.58·109
Table 5: Comparison of theoretically suggested batch sizes for an MDP with 1000 states, 20 actions and
γ= 0.99. Values in the first row indicate the demensionality of the feature vectors. Values in the other rows:
batch size of the corresponding method. Values are averaged over 10 generated datasets and environments.
Method/Features 6 11 21 41
TD-SVRG 9206 16096 32723 79401
PD-SVRG 7.38·10189.64·10205.14·10234.97·1026
VRTD 4.35·1071.34·1085.44·1081.45·109
34Published in Transactions on Machine Learning Research (8/2024)
Figure 3: Theoretical batch sizes of different algorithms in log-scale , geometrical average over 10 samples.
Thex-axis plots the dimension of the feature vector. First row: Batch sizes for random MDP environment
(see Sec. 6). Left to right: Figure 1 - 50 states, 20 actions and γ= 0.8; Figure 2: 400 states, 10 actions and
γ= 0.95, Figure 3: 1000 states, 20 actions and γ= 0.99; Figure 4: 2000 states, 50 actions and γ= 0.75.
Second row: batch sizes for dataset generated from OpenAI gym classic control environments Brockman
et al. (2016). Features generated by applying RBF kernels and then removing highly correlated feature
vectors one by one (see Sec. 6).
Figure 4: Average performance of TD-SVRG and PD-SVRG algorithms with different parame-
ters: “SVRG_theory” is TD-SVRG algorithm with parameters suggested by theoretical analysis;
“SVRG_lr_1/4_batch_scale_8” is a best-performing algorithm from TD-SVRG search grid ( α= 1/4,
M= 8/λA); “PD-SVRG_0.01_1e-6” is a the best perforing algorithm from the first PD-SVRG search
grid (σθ= 10−6 1
Lρκ(ˆC),σw= 10−2 1
λmax(C)); “PD-SCRG_0.01_0.125_8” is the best performing PS-SVRG
algorithms from the second grid search ( α= 1/8,M= 8/λA,σw= 10−2 1
λmax(C)). Rows - performance
measurements: log(f(θ))andlog(|θ−θ∗|).
J.2 Additional parameter grid search in dataset case
In this set of experiments, we conducted additional grid searches for the TD-SVRG and PD-SVRG algo-
rithms. For TD-SVRG, we executed a grid search on the set of parameters near the theoretically predicted
parameters (update batch size M= 16/λA, learning rate α= 1/8). For PD-SVRG, we ran searches
over two grids: parameters suggested by the authors of the original paper and parameters close to those
suggested by our theory. All experiments were conducted on an MDP environment with 400 states, 21
35Published in Transactions on Machine Learning Research (8/2024)
features, 10 actions, and γ= 0.95, identical to the one described in Section 6 of this paper. For TD-
SVRG, we ran a grid search over the parameter batch size M∈{8,12,16,24,32}/λAand learning rate
α∈{1/4,1/6,1/8,1/12,1/16}. For the PD-SVRG algorithm, the first grid was formed near the exact values
suggested in Du et al. (2017), i.e., primal variables learning rate σθ∈{10−1,..., 10−6}/(Lρκ(ˆC)), dual pa-
rameters learning rate σw∈{1,10−1,10−2}/λmax(C), and the batch size is twice the dataset size ( M= 2N).
The second grid uses the same learning rate and batch sizes as TD-SVRG, with the dual parameters learning
rateσwbeing the same as in the previous grid.
The results are illustrated in Figure 4. This figure demonstrates that TD-SVRG converges faster than the
PD-SVRG algorithm, which utilizes dual variables.
J.3 Datasets with DQN features
Figure 5: Geometric average performance of different algorithms in the finite sample case with DQN fea-
tures. Columns - dataset source environments: Acrobot, CartPole and Mountain Car. Rows - performance
measurements: log(f(θ))andlog(|θ−θ∗|).
In this set of experiments, we compare the performance of the same algorithms as in Section 6 on datasets
collected from OpenAI Acrobot, CartPOle and MountainCar environments Brockman et al. (2016) using
DQN features. To collect these features, we trained 1 hidden layer neural network with DQN algorithm
Mnih et al. (2015) for 1000 plays. Then, the trained agent played 5000 episodes following greedy policy,
while neural network hidden states were recorded as feature representation of the visited states. Features
collected this way tend to be highly correlated, therefore we applied PCA clearing, keeping minimum set of
principal components, corresponding to 90 % of the variance.
For the TD-SVRG algorithm we used theoretically justified parameters, for the other algorithms parameters
selected with grid search (Sec. J.2), the results are presented in Figure 5. In all environments TD-SVRG
exhibits stable linear convergence, GTD2 and vanilla TD algorithms converge sublinearly, while PD-SVRG
performance is unstable due to high range of condition numbers of dataset’s characteristic matrices Aand
C(large values of κ(C)caused PD-SVRG divergence in the Acrobot dataset).
J.4 Batched SVRG performance
In this set of experiments we compare the performance of TD-SVRG and batched TD-SVRG in the finite-
sample case. We generate 10 datasets of size 50000 from a similar MDP as in Section 6. Algorithms run with
36Published in Transactions on Machine Learning Research (8/2024)
the same hyperparameters. Average results over 10 runs are presented in Figure 6 and show that batched
TD-SVRG saves a lot of computations during the earlier epochs, which provides faster convergence.
Figure 6: Average performance of TD-SVRG and batching TD-SVRG in the finite sample case. Datasets
sampledfromMDPenvironments. Leftfigure–performanceintermsof log(f(θ)). Rightfigure–performance
in terms of log(|θ−θ∗|).
J.5 Online i.i.d. sampling from the MDP
Figure 7: Online iid sampling: Average performance of TD-SVRG with theoretical parameters, VRTD
with different batch sizes and vanilla TD with learning rate equal to 1/√
tin the i.i.d. sampling case. Left
figure – performance in terms of log(f(θ)), right figure in terms of log(|θ−θ∗|).
In this set of experiments we compare the performance of TD-SVRG, VRTD and Vanilla TD with decreasing
learning rates in the i.i.d. sampling case. States and rewards are sampled from the same MDP as in Section
6 under iid sampling strategy - next transition is being sampled independently from previous transition.
Hyperparametersarechosenasfollows: forTD-SVRG–learningrate α= 1/8,updatebatchsize M= 16/λA,
estimation batch size epoch expansion factor is ρ2= 1.2. VRTD – learning rate α= 0.1and batch sizes
M∈5,10,20∗103. For vanilla TD decreasing learning rate is set to 1/√
t, wheretis a number of the
performed update. Average results over 10 runs are shown in Figure 7. The figure shows that TD-SVRG
converges even if its performance suffers from high variance, VRTD algorithms oscillate after reaching a
certain level (due to bias). Vanilla TD with decreasing learning rate converges slowly then SVRG.
37Published in Transactions on Machine Learning Research (8/2024)
J.6 Online Markovian sampling from an MDP
Figure 8: Online Markovian sampling: Average performance of TD-SVRG with theoretical parameters,
VRTD with different batch sizes and vanilla TD with learning rate equal to 1/√
tin the i.i.d. sampling case.
Left figure – performance in terms of log(f(θ)), right figure in terms of log(|θ−θ∗|).
In this set of experiments we compare the performance of TD-SVRG, VRTD and Vanilla TD with decreasing
learning rates in the Markovian sampling case. States and rewards are sampled from the same MDP as in
Section 6 under Markovian sampling strategy - next transition is being sampled dependent on the previous
transition. Hyperparameters are chosen as follows: for TD-SVRG – learning rate α= 1/8, update batch size
M= 16/λA, estimation batch size epoch expansion factor is ρ2= 1.2. VRTD – learning rate α= 0.1and
batch sizes M∈5,10,20∗103. For vanilla TD decreasing learning rate is set to 1/√
t, wheretis a number
of the performed update. Average results over 10 runs are shown in Figure 8. Because this MDP mixes very
fast even under Markovian sampling, the results are very similar to iid sampling case. The figure shows that
TD-SVRG converges with decreasing rate, VRTD algorithms reach certain level and then oscillate, vanilla
TD converges with decreasing rate and slower then TD-SVRG.
J.7 Comparison of update batch sizes
In this set of experiments, we assume that λAand, consequently, the theory-predicted batch size 16/λA
are not known. We investigate the effect of approximate update batch size on the algorithm’s performance,
checking the performance of batch sizes {8,12,24,32}/λAagainst the theory-predicted value. We run this
experiment for all three cases: dataset (Figure 9), online iid sampling (Figure 10), and online Markovian
sampling (Figure 11). In all three cases, the algorithms demonstrate comparable performance, while in the
online sampling cases, both iid and Markovian, the difference is negligible. This is caused by the fact that
mean-path update estimation dominates the complexity.
38Published in Transactions on Machine Learning Research (8/2024)
Figure 9: Dataset sampling case: Average performance of TD-SVRG with theoretical parameters and
with different update batch size scales of 1/λA.
Figure 10: IID sampling case: Average performance of TD-SVRG with theoretical parameters and with
different update batch size scales of 1/λA.
Figure 11: Markovian sampling case: Average performance of TD-SVRG with theoretical parameters
and with different update batch size scales of 1/λA.
39Published in Transactions on Machine Learning Research (8/2024)
J.8 Experiment details
In our experiments (Section 6) we compare the performance of TD-SVRG with GTD2 Sutton et al. (2009),
“vanilla” TD learning Sutton (1988), and PD-SVRG Du et al. (2017) in the finite sample setting. Gener-
ally, our experimental set-up is similar to Peng et al. (2020). Datasets of size 5,000 are generated from 4
environments: Random MDP Dann et al. (2014), and the Acrobot, CartPole and Mountain car OpenAI
Gym environments Brockman et al. (2016). For the Random MDP, we construct an MDP environment with
|S|= 400, 21 features (20 random and 1 constant) and 10 actions, with action selection probabilities gener-
ated fromU[0,1). For OpenAI gym environments, the agent selects states uniformly at random. Features
are constructed by applying RBF kernels to discretize the original states and then removing highly correlated
features. The decay rate γis set to 0.95.
We compare the performance of TD-SVRG against the performance of other algorithms with parameters
selected by grid search. Details on the grid search might be found in Appendix J.2. Hyperparamters for
the algorithms are selected as follows: for TD-SVRG our theoretically justified parameters are selected, the
learning rate is set to α= 1/8and the update batch size to M= 16/λA; for GTD2 the best performing
parameters were: α= 0.125andβ= 0.25; for vanilla TD a decreasing learning rate is set to α= 1/√
t;
for PD-SVRG the parameters are set to σθ= 0.1/(Lρκ(ˆC)),σw= 0.1/λmax(C)and the batch size is twice
the size of the dataset, i.e., M= 2N. Each algorithm for each setting was run 10 times and the geometric
average performance is presented.
40Published in Transactions on Machine Learning Research (8/2024)
K Algorithms comparison
In this section, we present a more detailed comparison of TD algorithms. Our results are summarized in
Table 6, and a detailed explanation of the quantities in the table is provided below.
Please note that while other algorithms derive convergence in terms of ||θ−θ||2, our convergence is expressed
intermsofthefunction f(θ). Theresultscanbecomparedusingtheinequality λA||θ−θ||2≤f(θ)≤||θ−θ||2.
This implies that achieving an accuracy of ϵin terms of one quantity can be accomplished by achieving an
accuracy of λA/ϵin terms of the other quantity. Consequently, our results for the finite sample case are
strictly superior. For environment sampling cases, our results imply previous findings, whereas our results
are not implied by previous ones. Furthermore, it is worth noting that the inequality λA||θ−θ||2≤f(θ)is
rarely strict, which means that in most cases, the convergence implied by our results would be superior.
Table 6: Comparison of algorithmic parameters. PD-SVRG and PD SAGA results reported from Du et al.
(2017), VRTD and TD results from Xu et al. (2020), GTD2 from Touati et al. (2018). λmin(Q)andκ(Q)
are used to define, respectively, minimum eigenvalue and condition number of a matrix Q.λAin this table
denotes minimum eigenvalue of the matrix 1/2(A+AT), which is defined in Equation (1). Finite sample
results useNfor the size of the dataset sampled from the MDP. Other notation is taken from original papers,
and Section 1 in the supplementary information gives self-contained definitions of all the symbols appearing
in this table. For simplicity 1 +γis upper bounded by 2throughout, where γis the discount factor.
Method Learning rate Batch size Total complexity
Finite sample case
GTD292×2σ
8σ2(k+2)+92ζ1 O/parenleftig
κ(Q)2H
λmin(G)ϵ/parenrightig
PD-SVRGλmin(ATC−1A)
48κ(C)L2
G51κ2(C)L2
G
λmin(ATC−1A)2O/parenleftig/parenleftig
N+ (κ2(C)L2
G
λmin(ATC−1A)2/parenrightig
log(1
ϵ)/parenrightig
PD SAGAλmin(ATC−1A)
3(8κ2(C)L2
G+nµρ)1O/parenleftig/parenleftig
N+κ2(C)L2
G
λmin(ATC−1A)2/parenrightig
log(1
ϵ)/parenrightig
This paper1/8 16/λAO/parenleftig/parenleftig
N+1
λA/parenrightig
log(1
ϵ)/parenrightig
i.i.d. sampling
TD min(λA
4,1
2λA) 1 O/parenleftig
1
ϵλ2
Alog(1
ϵ)/parenrightig
This paper1/8 16/λA O/parenleftig
1
ϵλAlog(1
ϵ)/parenrightig
Markovian sampling
TDO(ϵ/log(1
ϵ)) 1 O/parenleftig
1
ϵλ2
Alog2(1
ϵ)/parenrightig
VRDTO(λA)O/parenleftig
1
ϵλ2
A/parenrightig
O/parenleftig
1
ϵλ2
Alog(1
ϵ)/parenrightig
This paperO(ϵ/log(1
ϵ))O/parenleftiglog(1
ϵ)
ϵλA/parenrightig
O/parenleftig
1
ϵλAlog2(1
ϵ)/parenrightig
Definitions of quantities in Table 6:
GTD2convergence analysis resutls are taken from Touati et al. (2018). The learning rate required for their
guarantee to work is set to92×2σ
8σ2(k+2)+92ζand the complexity to obtain accuracy ϵisO(κ(Q)2Hd
λmin(G)ϵ). In this
notation:
•σis the minimum eigenvalue of the matrix A′TM−1A′, where the matrix M=E[ϕ(sk,ak)ϕ(sk,ak)T]
andA′=E[ek(γEπ[ϕ(sk+1,.]−ϕ(sk,ak))T], whereekis the eligibility trace vector ek=
λγκ(sk,ak)ek−1+ϕ(sk,ak).
41Published in Transactions on Machine Learning Research (8/2024)
•kis an iteration number.
•The matrix Gplays key role in the analysis, it is a block matrix of the form
G=/parenleftbigg
0√βA′T
−√βA′βMk/parenrightbigg
,
andGkis a matrix of similar form generated from quantities estimated at time point k.
•ζis2×92c(M)2ρ2+ 32c(M)LG, wherec(M)is the condition number of the matrix M,ρis the
maximum eigenvalue of the matrix A′TM−1A′andLGis theLG=||E[GT
KGK|Fk−1]||.Fk−1in this
analysis is the σ-algebra generated by all previous history up to moment k−1.
•The quantityHis equal to E||GKz∗−gk||, wherez∗= (θ∗,1√
βw∗)is the optimal solution and
gk= (0,1√
βb).
•The last quantity left undefined is κ(Q), which is the condition number of the matrix Q, obtained
by diagonalization of the matrix G=QTΛQ.
PD-SVRG andPD SAGA use the same quantities as GTD2, except that matrices AandCare defined
the same way as in this paper: A=E[(ϕ(s)T−γϕ(s′)T)ϕ(s)],C=E[ϕ(s)ϕT(s)].
•nin this notation is the size of the dataset.
•µρis the minimum eigenvalue of matrix ATC−1A.
All other quantities are defined in the paper.
42