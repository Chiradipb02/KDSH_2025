Under review as submission to TMLR
Cooperative Minibatching in Graph Neural Networks
Anonymous authors
Paper under double-blind review
Abstract
Training large scale Graph Neural Networks (GNNs) requires significant computational
resources, and the process is highly data-intensive. One of the most effective ways to reduce
resource requirements is minibatch training coupled with graph sampling. GNNs have the
unique property that items in a minibatch have overlapping data. However, the commonly
implemented Independent Minibatching approach assigns each Processing Element (PE, i.e.,
cores and/or GPUs) its own minibatch to process, leading to duplicated computations and
input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon
(NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the
multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach
capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch
size, leading to significant reductions in the amount of work as batch sizes increase. Hence,
it is favorable for processors equipped with a fast interconnect to work on a large minibatch
together as a single larger processor, instead of working on separate smaller minibatches,
even though global batch size is identical. We also show how to take advantage of the same
phenomenon in serial execution by generating dependent consecutive minibatches. Our
experimental evaluations show up to 4x bandwidth savings for fetching vertex embeddings,
by simply increasing this dependency without harming model convergence. Combining our
proposed approaches, we achieve up to 64% speedup over Independent Minibatching on
single-node multi-GPU systems, using same resources.
1 Introduction
Graph Neural Networks (GNNs) have become de facto deep learning models for unstructured data, achiev-
ing state-of-the-art results on various application domains involving graph data such as recommendation
systems (Wu et al., 2020; Ying et al., 2018), fraud detection (Liu et al., 2022; Patel et al., 2022), identity
resolution (Xu et al., 2019), and traffic forecasting (Jiang & Luo, 2022). However, as the usage of technology
continues to increase, the amount of data generated by these applications is growing exponentially, resulting
in large and complex graphs that are infeasible or too time-consuming to train on a single processing
element (Ying et al., 2018; Zhu et al., 2019). For example, some social media graphs are reaching billions of
vertices and trillions of interactions (Ching et al., 2015). Efficient distributed training of GNNs is essential for
extracting value from large-scale unstructured data that exceeds the cost of storing and processing such data.
Due to the popularity of Deep Neural Networks (DNNs) and the need to support larger models and datasets,
a great deal of research has focused on increasing the scale and efficiency of distributed DNN training.
Techniques such as data parallelism (Ginsburg et al., 2017; Goyal et al., 2018), pipelining (Narayanan et al.,
2019), and intra-layer parallelism (Dean et al., 2012) have been employed. Following the success of traditional
distributed DNN training, the same techniques have also been adapted to GNN training, such as data
parallelism (Gandhi & Iyer, 2021; Lin et al., 2020; Zheng et al., 2021; Zhu et al., 2019) and intra-layer
parallelism (Tripathy et al., 2020).
The parallelization techniques mentioned earlier are used to scale both full-batch and minibatch training in a
distributed setting. Minibatch training (Bertsekas, 1994) is the go-to method to train DNN models as it
outperforms full-batch training in terms of convergence (Allen-Zhu & Hazan, 2016; Li et al., 2014; Keskar
et al., 2016; Wilson & Martinez, 2003), and more recently has been shown to also offer the same benefit for
1Under review as submission to TMLR
GNNs (Zheng et al., 2021). In the distributed setting, minibatch training for DNNs using data parallelism is
straightforward. The training samples are partitioned across the Processing Elements (PE) and they compute
the forward/backward operations on their minibatches. The only communication required is an all-reduce
operation for the gradients.
Unfortunately, minibatch training a GNN model is more challenging than a usual DNN model. GNNs turn
a given graph encoding relationships into computational dependencies. Thus in an L-layer GNN model,
each minibatch computation has a different structure as it is performed on the L-hop neighborhood of the
minibatch vertices. Real-world graphs usually are power law graphs (Artico et al., 2020) with small diameters,
thus, it is a challenge to train deep GNN models as the L-hop neighborhood grows exponentially w.r.t. L,
reaching almost the whole graph within a few hops. This is especially the case for the node classification and
link prediction scenarios where the graph can be large and a single connected component contains almost all
the data.
Very large GNN datasets necessitate storing the graph and node embeddings on slower storage mediums. To
enable GNN training efficiently in such a setting, several techniques have been proposed (Park et al., 2022;
Waleffe et al., 2022). These studies assume that the graph and its features are stored on disks or SSDs and
design their systems to reduce data transfers. The methods proposed in this paper directly apply to these
settings by reducing bandwidth requirements, as seen in Section 4.
A single epoch of full-batch GNN training requires computation proportional to the number of layers Land
the size of the graph. However, minibatch training requires more operations to process a single epoch due to
repeating calculations in the 2nd through L-th layers. As the batch size decreases, the number of repeated
calculations increases. This is because the vertices and edges have to be processed each time they appear in
theL-hop neighborhood. Thus, it is natural to conclude that using effectively larger batch sizes in GNNs
reduces the number of computations and data accesses of an epoch in contrast to regular DNN models. Our
contributions in this work utilizing this important observation can be listed as follows:
•Investigate work vs. batch size relationship and present theorems stating the cost of processing a
minibatch is a concave function of the batch size (Theorems 3.1 and 3.2).
•Utilize this relationship by combining data and intra-layer parallelism to process a minibatch across
multiple PEs for reduced work (Section 3.1), with identical global batch size. We call this new
approach Cooperative Minibatching .
•Use the same idea to generate consecutive dependent minibatches to increase temporal vertex
embedding access locality (Section 3.2). Dependent Minibatching can reduce the transfer amount of
vertex embeddings up to 4×, without harming model convergence.
•Show that the two approaches are orthogonal. When combined, they result in up to 1.64×speedup
over Independent Minibatching with identical global batch size.
•Further show the applicability of Cooperative Minibatching to the subgraph sampling methods and
prove that sampled subgraph density is a nondecreasing function of the batch size (Theorem 3.3).
2 Background
A graphG= (V,E)consists of vertices Vand edgesE⊂V×Valong with optional edge weights Ats>
0,∀(t→s)∈E. Given a vertex s, the 1-hop neighborhood N(s)is defined as N(s) ={t|(t→s)∈E}, and it
can be naturally expanded to a set of vertices SasN(S) =∪s∈SN(s).
GNN models work by passing previous layer embeddings ( H) fromN(s)tos, and then combining them using
a nonlinear function f(l)at layerl, given initial vertex features H(0):
H(l+1)
s =f(l)(H(l)
s,{H(l)
t|t∈N(s)}) (1)
If the GNN model has Llayers, then the loss is computed by taking the final layer L’s embeddings and
averaging their losses over the set of training vertices Vt⊂Vforfull-batch training. In L-layer full-batch
training, the total number of vertices that needs to be processed is L|V|.
2Under review as submission to TMLR
A
B
C
D
E
F
G
H
ଶ ଵ ଴
PE 1
PE 0
Figure 1:Minibatchesoftwoprocessingelementsmay
share edges in the second layer and vertices starting
in the first layer. For independent minibatching, the
solid green edges shared by both processing elements
represent duplicate work, and input nodes B through
G are duplicated along with the directed endpoints of
green edges. However for cooperative minibatching,
the vertices and edges are partitioned across the PEs
with no duplication, and the edges crossing the line
between the two PEs necessitate communication.
2.1 Minibatching in GNNs
In minibatch training, a random subset of training vertices, called Seed Vertices , is selected, and training is
done over the (sampled) subgraph composed of L-hop neighborhood of the seed vertices. On each iteration,
minibatch training computes the loss on seed vertices, which are random subsets of the training set Vt.
Given a set of vertices S, we define l-th layer expansion set, or the l-hop neighborhood Slas:
S0=S, S(l+1)=Sl∪N(Sl) (2)
For GNN computations, Slwould also denote the set of the required vertices to compute (1) at each layer l.
Using the same notation, {s}ldenotesl-layer expansion set starting from single vertex s∈V.
For a single minibatch iteration, the total number of vertices that need to be processed is/summationtextL
l=1|Sl|. There
are|V|
|S0|minibatches assuming Vt=V. Since each|Sl|≥|S0|, and a single epoch of minibatch training needs
to go over the whole dataset, the work W(|S0|)for a single epoch is:
W(|S0|) =|V|
|S0|L/summationdisplay
l=1E[|Sl|]≥|V|
|S0|L/summationdisplay
l=1|S0|=L|V| (3)
whereE[|Sl|]is the expected number of sampled vertices in layer land|S0|is the batch size. That is, the
total amount of work to process a single epoch increases over full-batch training. The increase in work due to
minibatch training is thus encoded in the ratiosE[|Sl|]
|S0|,1≤l≤L.
When sampling is used with minibatching, the minibatch subgraph may potentially become random. However,
the same argument for the increasing total amount of work holds for them too, as seen in Figure 3.
2.2 Graph Sampling
We focus on samplers whose expected number of sampled vertices is a function of the batch size. Neighbor
Sampling (NS) (Hamilton et al., 2017), LABOR (Balin & Çatalyürek, 2023) and RandomWalk (RW)
Sampling (Ying et al., 2018) all have this property and they are all applied recursively for GNN models with
multiple layers. Appendix A.1 describes the details of these sampling methods.
2.3 Independent Minibatching
Independent minibatching is commonly used in multi-GPU, and distributed GNN training frameworks (Cai
et al., 2023; Gandhi & Iyer, 2021; Lin et al., 2020; Zheng et al., 2021; Zhu et al., 2019) to parallelize the
training and allows scaling to larger problems. Each Processing Element (PE, e.g., GPUs, CPUs, or cores
of multi-core CPU), starts with their own S0of sizebas the seed vertices, and compute S1,...,SLalong
with the sampled edges to generate minibatches (see Figure 1). Computing S1,...,SLdepends on the
chosen sampling algorithm, such as the ones explained in Appendix A.1. It has the advantage that doing
a forward/backward pass does not involve any communication with other PEs after the initial minibatch
preparation stage at the expense of duplicate work.
3Under review as submission to TMLR
3 Cooperative Minibatching
In this section, we present two theorems that show the work of an epoch will be monotonically nonincreasing
with increasing batch sizes. After that, we propose two algorithms that can take advantage of this monotonicity.
Due to lack of space, we provide their full proofs in Appendices A.2 and A.3.
Theorem 3.1. The work per epochE[|Sl|]
|S0|required to train a GNN model using minibatch training is
monotonically nonincreasing as the batch size |S0|increases.
Proof.(Idea) Given any random S0andl≥1, we have the corresponding set Sl. We can shrink S0by
removing random elements s∈S0from it to get S0−{s}. If we compare (S0−{s})ltoSl, we will see that
any difference will come from elements that are contained only in {s}l. If we count such elements for each
s∈S0and sum them, it will be bounded by |Sl|. More concretely if we let S′0=S0−{s}be a random
subset ofS0with one element missing, we have:
/summationdisplay
s∈S0|Sl|−|(S0−{s})l|≤|Sl| ⇐⇒ |S0||Sl|−|Sl|≤/summationdisplay
s∈S0|(S0−{s})l| ⇐⇒ |Sl||S′0|≤|S0|E[|S′l|](4)
Empirical evidence can be seen in Figures 3 and 6. In fact, the decrease is related to the cardinality of the
following set:
Tl(S) ={w∈Sl|w∈{s}l,∃!s∈S0} (5)
WhenT(S0)is equal toSl, the work is equal as well. In the next section, we further investigate the effect of
|T(S0)|onE[|Sl|].
In addition to the definition of T(S)above, if we define the following set T2(S):
Tl
2(S) ={w∈Sl|w∈{s}l∩{s′}l,∃!{s,s′}⊂S0} (6)
Theorem 3.2. The expected subgraph size E[|Sl|]required to train a GNN model using minibatch training is
a concave function of batch size, |S0|.
Proof.(Idea) Note that we have:
|Tl(S0)|−2|Tl
2(S0)|=/summationdisplay
S′0⊂S0
|S′0|+1=|S0||Tl(S0)|−|Tl(S′0)|=|S0||Tl(S0)|−|S0|E[|Tl(S′0)|]
=⇒(|S0|−1)|Tl(S0)|≤|S0|E[|Tl(S′0)|] =⇒|Tl(S0)|
|S0|≤E[|Tl(S′0)|]
|S′0|
We have just shown thatE[|Tl(S0
i)|]
iis monotonically nonincreasing as iincreases. If we let S0
idenote a
random subset of S0of sizei, we have:
E[|Sl|] =|S0|/summationdisplay
i=1E[|Tl(S0
i)|]
i(7)
AsE[|Sl|]is the sum of monotonically nonincreasing parts, we conclude that it is a concave function of the
batch size,|S0|.
So, the slope of the expected number of sampled vertices flattens as batch size increases, see the last row
in Figure 3 and the first row in Figure 6. Note that this implies work monotonicity as well.
4Under review as submission to TMLR
3.1 Cooperative Minibatching
As explained in Section 2, Independent Minibatching can not take advantage of the reduction in work with
increasing global batch sizes and number of PEs, because it uses separate small, local, batches of sizes b
on each PE for each step of training. On the other hand, one can also keep the globalbatch size constant,
bP=|S0|, and vary the number of processors P. AsPincreases, Independent Minibatching will perform
more and more duplicate work because the local batch size is a decreasing function, b=|S0|
P, ofP.
Algorithm 1 Cooperative minibatching
Input:seed vertices S0
pfor each PE p∈P, # layersL
for alll∈{0,...,L−1}do{Sampling}
for allp∈Pdo in parallel
Sample next layer vertices ˜Sl+1
pand edgesEl
pforSl
p
all-to-all to redistribute vertex ids for ˜Sl+1
pto getSl+1
p
for allp∈Pdo in parallel {Feature Loading}
Load input features HL
pfrom Storage for vertices SL
p
all-to-all to redistribute HL
pto get ˜HL
p
for alll∈{L−1,..., 0}do{Forward Pass}
for allp∈Pdo in parallel
ifl+ 1<Lthen
all-to-all to redistribute Hl+1
pto get ˜Hl+1
p
Forward pass on bipartite graph ˜Sl+1
p→Sl
pwith edges El
p, input ˜Hl+1
pand output Hl
p
for allp∈Pdo in parallel
Compute the loss and initialize gradients G0
p
for alll∈{0,...,L−1}do{Backward Pass}
for allp∈Pdo in parallel
Backward pass on bipartite graph Sl
p→˜S(l+1)
pwith edges El
p, inputGl
pand output ˜Gl+1
p
ifl+ 1<Lthen
all-to-all to redistribute ˜Gl+1
pto getGl+1
p
Here, we propose the Cooperative Minibatching method that will take advantage of the work reduction with
increasing batch sizes in multi-PE settings. In Cooperative Minibatching, a single global batch of size bPwill
be processed by all the PPEs in parallel, eliminating any redundant work across PEs.
We achieve this as follows: we first partition the graph in 1D fashion by logically assigning each vertex
and its incoming edges to PEs as VpandEpfor each PE p. Next, PE psamples its batch seed vertices
Sl
pfrom the training vertices in Vpforl= 0of sizeb. Then using any sampling algorithm, PE psamples
the incoming edges El
pfromEpfor its seed vertices. Each PE then computes the set of vertices sampled
˜Sl+1
p={t|(t→s)∈El
p}. Note that, ˜Sl+1
phas elements residing on different PEs. The PEs exchange the
vertex ids ˜Sl+1
pso that each PE receives the set Sl+1
p∈Vp. This process is repeated recursively for GNN
models with multiple layers by using Sl+1
pas the seed vertices for the next layer. The exchanged information
is cached to be used during the forward/backward passes.
For the forward/backward passes, the same communication pattern used during cooperative sampling is used
to send and receive input and intermediate layer embeddings before each GNN layer invocation. Algorithm 1
details cooperative sampling and cooperative forward/backward passes for a single GNN training iteration.
Independent minibatching works the same except that it lacks the all-to-all operations and has ˜Al+1
p=Al+1
p
for any given variable Ainstead. The redistribution of vertices during sampling happens according to the
initial graph partitioning and the rest of the redistribution operations follow the same communication pattern,
always converting a variable ˜Al+1
pintoAl+1
pduring the forward pass and Al+1
pinto ˜Al+1
pduring sampling
and the backward passes for any variable A. Note that a similar training approach is explored concurrently
with our work in Polisetty et al. (2023). We refer the reader to Appendix A.10 to see the relation between
the approach proposed here and the work of Jia et al. (2020) on redundancy-free GCN aggregation.
5Under review as submission to TMLR
Table 1: Algorithmic complexities of different stages of GNN training at layer 0≤l<LwithLtotal layers
and batch size B=|S0|withPPEs. Note that|Sl
p(B)|=|Sl(B)|1
P,|El
p(B)|=|El(B)|1
Psince the PEs
process the partitioned subgraphs. Feature loading happens only at layer L.
Stage Independent Cooperative
Sampling O(|Sl(B
P)|1
β) O(|Sl
p(B)|1
β+|˜Sl+1
p(B)|c
α)
Feature loading O(|SL(B
P)|dρ
β) O(|SL
p(B)|dρ
β+|˜SL
p(B)|dc
α)
Forward/Backward O(M(Sl(B
P),El(B
P),Sl+1(B
P))d
γ)O(M(Sl
p(B),El
p(B),˜Sl+1
p(B))d
γ+|˜Sl+1
p(B)|dc
α)
Complexity Analysis: LetM(V1,E,V 2)denote the work to process a bipartite graph V2→V1with edges
Efor a given GNN model M. Assuming cross PE communication bandwidth α, Storage (e.g., disk, network,
or DRAM) to PE bandwidth as βand PE memory bandwidth γ, and cache miss rate ρ, we have the time
complexities given in Table 1 to perform different stages of GNN training per PE. We also use dfor embedding
dimension and c<1for the cross edge ratio, note that c≈P−1
Pfor random partitioning, and smaller for
smarter graph partitioning with Pstanding for the number of PEs. Also the sizes of ˜Slbecome smaller when
graph partitioning is used due to increased overlap.
We see that γ≈2TB/s,α≈300GB/sandβ≈30GB/sin today’s modern multi-GPU systems (NVIDIA,
2020a). Due to αbeing relatively fast compared toγ
Mandβ, our approach brings performance benefits.
On newer systems, the all-to-all bandwidth continues to increase (NVIDIA, 2023), decreasing the cost of
cooperation on a global mini-batch. However, on systems where the interconnect does not provide full
bandwidth for all-to-all operations, our approach is limited in the speedup it can provide. Our approach
is most applicable for systems with relatively fast all-to-all bandwidthα
ccompared toγ
Mandβand large
P. In particular, starting from P= 2, cooperative starts to outperform independent even on F/B with the
mag240M dataset and the R-GCN model in Section 4.3 and Tables 4 and 5. More discussion on this topic
can be found in Appendix A.6.
3.2 Cooperative Dependent Minibatching
Just as any parallel algorithm can be executed sequentially, we can reduce the number of distinct data
accesses by having a single PE process b-sized parts of a single κb-sized minibatch for κiterations. In light
of Theorems 3.1 and 3.2, consider doing the following: choose κ∈Z+, then sample a batch S0of sizeκb, i.e.,
κb=|S0|to getS0,...,SL. Then sample κminibatches S0
i, of sizeb=|S0
i|from this batch of size κbto get
S0
i,...,SL
i,∀i∈{0,...,κ−1}. In the end, all of the input features required for these minibatches will be
a subset of the input features of the large batch, i.e. Sj
i⊂Sj,∀i,j. This means that the collective input
feature requirement of these κbatches will be|SL|, the same as our batch of size κb. Hence, we can now take
advantage of the concave growth of the work in Theorem 3.2 and Figure 3.
Note that, if one does not use any sampling algorithms and proceeds to use the full neighborhoods, this
technique will not give any benefits, as by definition, the l-hop neighborhood of a batch of size κbwill always
be equal to the union of the l-hop neighborhoods of batches of sizes b. However for sampling algorithms, any
overlapping vertex sampled by any two batches of sizes bmight end up with different random neighborhoods
resulting in a larger number of sampled vertices. Thus, having a single large batch ensures that only a single
random set of neighbors is used for any vertex processed over a period of κbatches.
The approach described above has a nested iteration structure and the minibatches part of one κgroup will
be significantly different than another group and this might slightly affect convergence. In Appendix A.7, we
propose an alternative smoothing approach that does not require two-level nesting and still takes advantage
of the same phenomenon for the NS and LABOR sampling algorithms.
The main idea of our smoothing approach is as follows: each time one samples the neighborhood of a vertex,
normally it is done independently of any previous sampling attempts. If one were to do it fully dependently,
then one would end up with an identical sampled neighborhood at each sampling attempt. What we propose
is to do something inbetween, so that the sampled neighborhood of a vertex changes slowly over time. The
6Under review as submission to TMLR
d 1 =3k=2d 2 =422324146573r 4 =0.3r 6 =0.6r 7 =0.9r 3 =0.7r 5 =0.4
d 1 =3k=2d 2 =422324146573r 4 =0.55r 6 =0.65r 7 =0.6r 3 =0.6r 5 =0.3
d 1 =3k=2d 2 =422324146573r 4 =0.8r 6 =0.7r 7 =0.3r 3 =0.5r 5 =0.2
Figure 2: A smoothed dependent minibatching example for κ= 2. The middle minibatch is interpolated
between the two independent minibatches on the left and the right by interpolating the random numbers
used during sampling.
speed of change in the sampled neighborhoods is1
κ, and after every κiterations, one gets fully independent
new random neighborhoods for all vertices, see Figure 2. We will experimentally evaluate the locality benefits
and the overall effect of this algorithm on convergence in Sections 4.2 and 4.3.1, and more details on our
smoothing approach are discussed in Appendix A.7.
3.3 Relation to subgraph sampling methods
Existing subgraph sampling methods Chiang et al. (2019); Zeng et al. (2020); Hu et al. (2020b); Zeng et al.
(2021); Fey et al. (2021); Shi et al. (2023) randomly sample a subset Sof the vertices and use the same
subset for all layers of GNN training, S=S0=···=SL. The edges SEused for training are obtained by
taking the vertex induced subgraph SE={t→s|(t→s)∈E∧t,s∈S}. We utilize the same observation
as Theorems 3.1 and 3.2 and state:
Theorem 3.3. The expected sampled subgraph densityE[|SE|]
|S|is nondecreasing as the batch size |S|increases.
Thus, havingmultiplePEscooperateandworkonalargerminibatchtogetherinsteadofprocessingindependent
smaller minibatches is theoretically expected to converge faster, as the higher density leads to better
approximations. The proof of Theorem 3.3 is provided in Appendix A.4.
4 Experimental Evaluation
We first compare how the work to process an epoch changes w.r.t. to the batch size to empirically validate The-
orems 3.1 and 3.2 for different graph sampling algorithms. Next, we show how dependent batches introduced
in Section 3.2 benefits GNN training. We also show the runtime benefits of cooperative minibatching compared
to independent minibatching in the multi-GPU setting. Finally, we show that these two techniques are
orthogonal, can be combined to get multiplicative savings. Table 2 lists details of the datasets we used in
experiments. Details on our experimental setup can be found in Appendix A.5.
4.1 Demonstrating monotonicity of work
We use three sampling approaches, NS, LABOR, and RW, to demonstrate that the work to process an
epoch decreases as the batch size increases for the L= 3layer case across these three different classes of
sampling algorithms. We carried out our evaluation in two problem settings: node and edge prediction. For
node prediction, a batch of training vertices is sampled with a given batch size. Then, the graph sampling
algorithms described in Appendix A.1 are applied to sample the neighborhood of this batch. The top row
of Figure 3 shows how many input vertices is required on average to process an epoch, specificallyE[|S3|]
|S0|.
For edge prediction, we add reverse edges to the graph making it undirected and sample a batch of edges.
7Under review as submission to TMLR
Table 2: Traits of datasets used in experiments: numbers of vertices, edges, avg. degree, features, cached
vertex embeddings, and training, validation and test vertex splits. Last column has # minibatches in an
epoch during model training with 1024 batch size including validation.
Dataset |V| |E||E|
|V|# feats. cache size train - val - test (%) # minibatches
flickr 89.2K 900K 10.09 500 70k 50.00 - 25.00 - 25.00 65
yelp 717K14.0M 19.52 300 200k 75.00 - 10.00 - 15.00 595
reddit 233K 115M493.56 602 60k 66.00 - 10.00 - 24.00 172
papers100M 111M 3.2B29.10 128 2M 1.09 - 0.11 - 0.19 1300
mag240M 244M 3.44B 14.16 768 2M 0.45 - 0.06 - 0.04 1215
Figure 3: Monotonicity of the work. x-axis shows the batch size, y-axis showsE[|S3|]
|S0|(see Theorem 3.1) for
node prediction (top row) and E[|S3|](see Theorem 3.2) for edge prediction (bottom row), where E[|S3|]
denotes the expected number of sampled vertices in the 3rd layer and |S0|denotes the batch size. RW stands
for Random Walks, NS for Neighbor Sampling, and LABOR-0/* for the two different variants of the LABOR
sampling algorithm described in Section 2.2.
For each of these edges a random negative edge (an edge that is not part of E) with one endpoint coinciding
with the positive edge is sampled. Then, all of the endpoints of these positive and negative edges are used as
seed vertices to sample their neighborhoods. The bottom row of Figure 3 shows E[|S3|].
We can see that in all use cases, datasets and sampling algorithms, the work to process an epoch is
monotonically decreasing as we proved in Theorem 3.1. We also see the plot of the expected number of
vertices sampled, E[|S3|], is concave with respect to batch size as we already know from Theorem 3.2.
Another observation is that the concavity characteristic of E[|S3|]seems to differ for different sampling
algorithms. In increasing order of concavity we have RW, NS, LABOR-0 and LABOR-*. The more concave a
sampling algorithm’s E[|SL|]curve is, the less it is affected from the NEP and more savings are available
through the use of the proposed methods in Sections 3.1 and 3.2. Note that the differences would grow with
a larger choice of layer count L.
8Under review as submission to TMLR
Figure 4: The validation F1-score with NS sampled neighborhoods trained with the LABOR-0 sampling
algorithm with 1024batch size and varying κdependent minibatches, κ=∞denotes infinite dependency,
meaning the neighborhood sampled for a vertex stays static during training. See Figure 5a for cache miss
rates. See Figure 8 for the training loss and F1-score with the dependent sampler.
Table 3: Test F1-scores at the highest validation F1-score corresponding to Figure 4, using early stopping.
Averages of 40 runs are presented.
κreddit papers100M yelp flickr
196.72±0.06 66.58±0.17 63.40±0.18 53.82±0.21
496.71±0.05 66.60±0.13 63.39±0.19 53.87±0.18
1696.70±0.05 66.58±0.20 63.42±0.18 53.90±0.17
6496.67±0.05 66.55±0.20 63.35±0.18 53.88±0.20
25696.65±0.06 66.56±0.14 63.35±0.19 53.86±0.24
∞95.01±0.30 66.46±0.15 62.88±0.18 53.84±0.19
4.2 Dependent Minibatches
We vary the batch dependency parameter κintroduced in Section 3.2 for the LABOR-0 sampler with a
batch size of 1024. Our expectation is that as consecutive batches become more dependent on each other,
the subgraphs used during consecutive steps of training would start overlapping with each other, in which
case, the vertex embedding accesses would become more localized. We attempted to capture this increase
in temporal locality in vertex embedding accesses by implementing an LRU cache to fetch them, the cache
sizes used for different datasets is given in Table 2. Note that the cache miss rate is proportional to the
amount of data that needs to be copied from the vertex embedding storage. The Figure 5a shows that as
κincreases, the cache miss rate across all datasets drops. On reddit, this is a drop from 64% to 16% on,
a 4x improvement. We also observe that the improvement is monotonically increasing as a function of|E|
|V|
given in Table 2. Table 3 and Figure 4 show that training is not negatively affected across all datasets up to
κ= 256with less than 0.1%F1-score difference, after which point the validation F1-score with w/o sampling
starts to diverge from the κ= 1case. Runtime benefits of this approach can be observed by comparing the
CacheandCache,κcolumns in Table 4. Appendix A.8 has additional discussion about the effect of varying
κand the last column of Table 2 shows the number of minibatches in an epoch during training.
4.3 Cooperative Minibatching
We use our largest datasets, mag240M and papers100M, as distributed training is motivated by large-scale
datasets. We present our runtime results on systems equipped with NVIDIA GPUs, with 4 and 8 A100
80 GB (NVIDIA, 2021) and 16 V100 32GB (NVIDIA, 2020b), all with NVLink interconnect between the
GPUs (600 GB/s for A100 and 300 GB/s for V100). The GPUs perform all stages of GNN training and
the CPUs are only used to launch kernels for the GPUs. Feature copies are performed by GPUs as well,
accessing pinned feature tensors over the PCI-e using zero-copy access. In cooperative minibatching, both
data size and computational cost are shrinking with increasing numbers of GPUs, relative to independent
minibatching. We use the GCN model for papers100M and the R-GCN model (Schlichtkrull et al., 2017) for
mag240M. As seen in Table 4, cooperative minibatching reduces all the runtimes for different stages of GNN
9Under review as submission to TMLR
(a) 1 GPU, cache sizes are listed in Table 2.
 (b) 4 cooperating GPUs, each having a cache of size 1M.
Figure 5: LRU-cache miss rates for LABOR-0 sampling algorithm with 1024batch size per GPU and varying
κdependent minibatches, κ=∞denotes infinite dependency.
Table 4: Cooperative vs independent minibatching runtimes per minibatch (ms) on three different systems
with 4 and 8 NVIDIA A100 80 GB GPUs, and 16 NVIDIA V100 32GB GPUs. I/C denotes whether
independent or cooperative minibatching is used. Samp. is short for Graph Sampling, Feature Copy stands
for vertex embedding copies over PCI-e and Cache denotes the runtime of copies performed with a cache that
can hold 106vertex embeddings per A100 and 5×105per V100. κdenotes the use of batch dependency
κ= 64. F/B means forward/backward. Total time is computed by the fastest available Feature Copy time,
the sampling time, and the F/B time. |S0|is the global batch size and bis the the batch size per GPU. α
stands for cross GPU communication bandwidth (NVLink), βfor PCI-e bandwidth and γfor GPU global
memory bandwidth. Green was used to indicate the better result between independent and cooperative
minibatching, while Boldwas used to highlight the feature copy time included in the Totalcolumn.
# GPUs,γ
α,β,|S0|Dataset
& ModelSampler I/C Samp.Feature CopyF/B Total-Cache Cache,κ
4 A100
γ=2TB/s
α=600GB/s
β=64GB/s
|S0|= 212
b= 1024papers100M
GCNLABOR-0Indep 21.718.4 16.8 11.2 8.941.8
Coop 17.714.0 10.1 5.813.0 36.5
NSIndep 16.126.5 22.1 -10.1 48.3
Coop 11.921.3 12.9 -15.0 39.8
mag240M
R-GCNLABOR-0Indep 26.057.9 56.0 41.0199.9 266.9
Coop 20.051.1 36.9 23.4183.3 226.7
NSIndep 14.478.0 71.2 -223.0 308.6
Coop 12.373.9 47.5 -215.6 275.4
8 A100
γ=2TB/s
α=600GB/s
β=64GB/s
|S0|= 213
b= 1024papers100M
GCNLABOR-0Indep 21.321.1 18.7 12.0 9.342.6
Coop 16.512.4 7.1 4.013.5 34.0
NSIndep 15.831.0 24.5 -10.3 50.6
Coop 12.519.4 9.0 -15.6 37.1
mag240M
R-GCNLABOR-0Indep 30.670.1 66.2 46.8202.1 279.5
Coop 21.650.6 29.0 19.3172.2 213.1
NSIndep 15.094.9 80.9 -224.8 320.7
Coop 14.971.6 39.6 -209.0 263.5
16 V100
γ=0.9TB/s
α=300GB/s
β=32GB/s
|S0|= 213
b= 512papers100M
GCNLABOR-0Indep 39.144.5 40.2 29.4 15.1 83.6
Coop 26.922.7 10.4 4.919.1 50.9
NSIndep 18.061.3 52.0 -16.2 86.2
Coop 19.234.9 13.0 -21.3 53.5
mag240M
R-GCNLABOR-0Indep 50.8128.8 121.3 96.2156.1 303.1
Coop 29.278.1 42.8 23.5133.3 186.0
NSIndep 19.3167.3 152.6 -170.9 342.8
Coop 19.3116.1 53.1 -160.4 232.8
10Under review as submission to TMLR
training, except for the F/B (forward/backward) times on papers100M where the computational cost is not
high enough to hide the overhead of communication.
Table 5: Runtime improvements of Cooperative Minibatching over Independent Minibatching compiled
from the Totalcolumn of Table 4. This is a further improvement on top of the speedup of independent
minibatching already achieves using the same number GPUs.
Dataset & Model Sampler 4 GPUs 8 GPUs 16 GPUs
papers100M & GCNLABOR-0 15% 25% 64%
NS 21% 36% 61%
mag240M & R-GCNLABOR-0 18% 31% 63%
NS 12% 22% 47%
If we take the numbers in the Totalcolumns from Table 4, divide independent runtimes by the corresponding
cooperative ones, then we get Table 5. We can see that the theoretical decrease in work results in increasing
speedup numbers with the increasing number of PEs, due to Theorem 3.1. We would like to point out
thatE[|S3|]
|S0|curves in Figure 3 are responsible for these results. With PPEs and|S0|global batch size, the
work performed by independent minibatching vs cooperative minibatching can be compared by looking at
x=1
P|S0|vsx=|S0|respectively.
We also ran experiments that show that graph partitioning using METIS (Karypis & Kumar, 1998) prior to the
start of training can help the scenarios where communication overhead is significant. The forward-backward
time decreases from 13.0ms to 12.0ms on papers100M with LABOR-0 on 4 NVIDIA A100 GPUs with
partitioning due to reduced communication overhead using the same setup as Table 4.
Increasing the number of GPUs increases the advantage of cooperative minibatching compared to independent
minibatching. The forward-backward time on mag240M with LABOR-0 is 200 (same as independent baseline),
194, 187 and 183 ms with 1, 2, 3 and 4 cooperating GPUs, respectively measured on the NVIDIA DGX
Station A100 machine with R-GCN. When using a different GNN model such as GATs (Veličković et al.,
2018), the forward-backward runtime is 190 ms on 1 GPU vs 172 ms on 4 GPUs on mag240M. The decrease in
runtime with increasingly cooperating GPUs is due to the decrease in redundant work they have to perform.
Even though the batch size per GPU is constant, the runtime goes down similar to the plots in the top row
of Figure 3, except that it follows,kE[|S2|]
|S0|, the average number of edges in the 3rd layer when a sampler
with fanout kis used.
Additionally, we demonstrate that there is no significant model convergence difference between independent
vs cooperative minibatching in Appendix A.9.
4.3.1 Cooperative-Dependent Minibatching
Table 6: Runtime improvements of Dependent Minibatching, for Independent and Cooperative Minibatching
methods, compiled from the Cache,κandCachecolumns of Table 4 with LABOR-0. Making consecutive
minibatches dependent increases temporal locality, hence reducing cache misses.
Dataset & Model I/C 4 GPUs 8 GPUs 16 GPUs
papers100M & GCNIndep + Depend 50% 57% 37%
Coop + Depend 74% 78% 112%
mag240M & R-GCNIndep + Depend 37% 41% 26%
Coop + Depend 58% 50% 82%
We use the same experimental setup as Section 4.3 but vary the κparameter to show that cooperative
minibatching can be used with dependent batches (Figure 5b). We use a cache size of 1M vertex embeddings
per GPU. Cooperative feature loading effectively increases the global cache size since each PE (GPU) caches
only the vertices assigned to them while independent feature loading can have duplicate entries across caches.
11Under review as submission to TMLR
For our largest dataset mag240M, on top of 1.4×reduced work due to cooperative minibatching alone, the
cache miss rates were reduced by more than 2×, making the total improvement 3×. Runtime results for
κ∈{1,256}are presented in Table 4, the Feature Copy CacheandCache,κcolumns. Table 6 summarizes
these results by dividing the runtimes in CachebyCache,κand reporting percentage improvements.
5 Key insights of Cooperative Minibatching
In Section 2, the work W(|S0|)to process an epoch (full pass over the dataset) for a given minibatch size |S0|
is characterized by the number of minibatches in an epoch (|V|
|S0|)×the amount of work to process a single
minibatch, which is approximated by the sum of the number of sampled vertices in each layer (/summationtextL
l=1|Sl|).
This can be seen in Equation (3).
Equation (3) only considers the number of processed vertices and it is good enough for our purposes. Since all
the sampling algorithms we consider in Section 2.2 have fanout parameters k, the number of edges sampled
for each seed vertex has an upper bound k. So, given vertices Slfor thelth layer, the number of sampled
edges in that same layer will be ≤k|Sl|. Clearly for almost any GNN model, the runtime complexity to
process layer lis linearly increasing w.r.t. the number of vertices ( |Sl|) and edges (≤k|Sl|) processed, so the
runtime complexity is O(|Sl|+k|Sl|) =O(|Sl|).
A more comprehensive analysis of the runtime complexities of Cooperative and Independent Minibatching
approaches is provided in Appendix A.6, taking into account the exact numbers of sampled vertices ( |Sl|),
edges (|El|), and various communication bandwidths ( α,γ,β) and even graph partition quality cand cache
missesρ.
As Cooperative Minibatching considers a single minibatch of size Bfor allPPEs, the growth of the number of
sampled vertices and edges is characterized by Bas the minibatch size. In contrast, Independent Minibatching
assigns different minibatches of sizesB
Pto each PE, so the growth of the sampled vertices and edges is
characterized byB
Pas the minibatch size. Due to Theorems 3.1 and 3.2, the work W(B)with respect to a
minibatch size Bis a concave function, we have W(B)≤PW(B
P), meaning that Cooperative Minibatching is
theoretically faster than Independent Minibatching, if communication between PEs is infinitely fast. For
empirical data, one can look at the first-row of Figure 3 at the x-axisBfor Cooperative andB
Pfor Independent,
due to:
W(B)≤PW(B
P)⇐⇒W(B)
B≤W(B
P)
B
P
as proven by Theorem 3.1. AsE[|S3|]
|S0|curves in Figure 3 are decreasing, the work of Cooperative Minibatching
is significantly less than Independent Minibatching. Finally, modern multi-GPU computer systems have very
fast inter-GPU communication bandwidths, e.g. NVLink, which is why we are able to show favorable results
compared to Independent Minibatching despite performing seemingly unnecessary communication.
6 Conclusions
In this paper, we investigated the difference between DNN and GNN minibatch training. We observed that
the cost of processing a minibatch is a concave function of batch size in GNNs, unlike DNNs where the cost
scales linearly. We then presented theorems that this is indeed the case for every graph and then proceeded
to propose two approaches to take advantage of cost concavity. The first approach, which we call cooperative
minibatching proposes to partition a minibatch between multiple PEs and process it cooperatively. This is
in contrast to existing practice of having independent minibatches per PE, and avoids duplicate work that
is a result of vertex and edge repetition across PEs. The second approach proposes the use of consecutive
dependent minibatches, through which the temporal locality of vertex and edge accesses is manipulated. As
batches get more dependent, the locality increases. We demonstrated this increase in locality by employing
an LRU-cache for vertex embeddings on GPUs. Finally, we showed that these approaches can be combined
without affecting convergence, and speed up multi-GPU GNN training by up to 64%.
12Under review as submission to TMLR
References
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In Proceedings
of The 33rd International Conference on Machine Learning , pp. 699–707. PMLR, 20–22 Jun 2016. URL
https://proceedings.mlr.press/v48/allen-zhua16.html .
I. Artico, I. Smolyarenko, V. Vinciotti, and E. C. Wit. How rare are power-law networks really? In Royal
Society, volume 476, 2020. URL http://doi.org/10.1098/rspa.2019.0742 .
Muhammed Fatih Balin and Ümit Çatalyürek. Layer-neighbor sampling — defusing neigh-
borhood explosion in gnns. In Advances in Neural Information Processing Systems , vol-
ume 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
51f9036d5e7ae822da8f6d4adda1fb39-Paper-Conference.pdf . Also available as a Tech Rep on
ArXiv.
D.P. Bertsekas. Incremental least squares methods and the extended kalman filter. In Proceedings of 1994
33rd IEEE Conference on Decision and Control , volume 2, pp. 1211–1214 vol.2, 1994. doi: 10.1109/CDC.
1994.411166.
Zhenkun Cai, Qihui Zhou, Xiao Yan, Da Zheng, Xiang Song, Chenguang Zheng, James Cheng, and George
Karypis. Dsp: Efficient gnn training with multiple gpus. In Proceedings of the 28th ACM SIGPLAN
Annual Symposium on Principles and Practice of Parallel Programming , PPoPP ’23, pp. 392–404, 2023.
doi: 10.1145/3572848.3577528.
Wei Lin Chiang, Yang Li, Xuanqing Liu, Samy Bengio, Si Si, and Cho Jui Hsieh. Cluster-GCN: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 257–266. Association for
Computing Machinery, jul 2019. doi: 10.1145/3292500.3330925.
Avery Ching, Sergey Edunov, Maja Kabiljo, Dionysios Logothetis, and Sambavi Muthukrishnan. One
trillion edges: Graph processing at facebook-scale. Proc. VLDB Endow. , 8(12):1804–1815, aug 2015. doi:
10.14778/2824032.2824077.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc 'aurelio Ranzato,
Andrew Senior, Paul Tucker, Ke Yang, Quoc Le, and Andrew Ng. Large scale distributed deep networks.
In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (eds.), Advances in Neural Information
Processing Systems , volume 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips.cc/
paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf .
MatthiasFey, JanE.Lenssen, FrankWeichert, andJureLeskovec. Gnnautoscale: Scalableandexpressivegraph
neural networks via historical embeddings. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th
International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research ,
pp. 3294–3304. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/fey21a.html .
Swapnil Gandhi and Anand Padmanabha Iyer. P3: Distributed deep graph learning at scale. In 15th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 21) , pp. 551–568, 2021.
Boris Ginsburg, Igor Gitman, and Yang You. Large batch training of convolutional networks with layer-
wise adaptive rate scaling. Technical Report arXiv:1708.03888, ArXiv, September 2017. URL http:
//arxiv.org/abs/1708.03888 .
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour.
Technical Report arXiv:1706.02677, ArXiv, April 2018. URL http://arxiv.org/abs/1706.02677 .
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Proceedings of the 31st International Conference on Neural Information Processing Systems , NIPS’17, pp.
1025–1035, 2017.
13Under review as submission to TMLR
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and
Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in Neural
Information Processing Systems , 2020-Decem(NeurIPS):1–34, 2020a.
Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A
large-scale challenge for machine learning on graphs, 2021. URL https://arxiv.org/abs/2103.09430 .
Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous Graph Transformer. The Web
Conference 2020 - Proceedings of the World Wide Web Conference, WWW 2020 , pp. 2704–2710, 2020b.
doi: 10.1145/3366423.3380027.
Zhihao Jia, Sina Lin, Rex Ying, Jiaxuan You, Jure Leskovec, and Alex Aiken. Redundancy-free computation
for graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining , KDD ’20, pp. 997–1005. Association for Computing Machinery, 2020. URL
https://doi.org/10.1145/3394486.3403142 .
Weiwei Jiang and Jiayun Luo. Graph neural network for traffic forecasting: A survey. Expert Systems with
Applications , 207:117921, nov 2022. doi: 10.1016/j.eswa.2022.117921.
George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs.
SIAM Journal on Scientific Computing , 20(1):359–392, 1998. doi: 10.1137/S1064827595287997.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint
arXiv:1609.04836 , 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL https:
//arxiv.org/abs/1412.6980 .
Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J. Smola. Efficient mini-batch training for stochastic
optimization. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining , KDD ’14, pp. 661–670, 2014. doi: 10.1145/2623330.2623612.
Zhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, and Yinlong Xu. Pagraph: Scaling gnn training on large
graphs via computation-aware caching. In Proceedings of the 11th ACM Symposium on Cloud Computing ,
SoCC ’20, pp. 401–415, 2020. doi: 10.1145/3419111.3421281.
Yajing Liu, Zhengya Sun, and Wensheng Zhang. Improving fraud detection via hierarchical attention-based
graph neural network, 2022.
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger,
Phillip B. Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In
Proceedings of the 27th ACM Symposium on Operating Systems Principles , SOSP ’19, pp. 1–15, 2019. doi:
10.1145/3341301.3359646.
NVIDIA. NVIDIA DGX A100 system architecture: The universal system for AI infrastructure. Tech-
nical report, NVIDIA Corporation, July 2020a. URL https://www.nvidia.com/content/dam/en-zz/
Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf .
NVIDIA. NVIDIA V100 | datasheet. Technical report, NVIDIA Corporation, Jan-
uary 2020b. URL https://images.nvidia.com/content/technologies/volta/pdf/
volta-v100-datasheet-update-us-1165301-r5.pdf .
NVIDIA. NVIDIA A100 tensor core GPU | datasheet. Technical report, NVIDIA Corporation,
June 2021. URL https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/
nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf .
NVIDIA. NVIDIA DGX H100: The gold standard for AI infrastructure. Technical report, NVIDIA Corpora-
tion, March 2023. URL https://resources.nvidia.com/en-us-dgx-systems/ai-enterprise-dgx .
14Under review as submission to TMLR
NVIDIA. NVIDIAGB200NVL72: Poweringtheneweraofcomputing. Technicalreport, NVIDIACorporation,
March 2024. URL https://www.nvidia.com/en-us/data-center/gb200-nvl72/ .
Yeonhong Park, Sunhong Min, and Jae W. Lee. Ginex: Ssd-enabled billion-scale graph neural network training
on a single machine via provably optimal in-memory caching. Proc. VLDB Endow. , 15(11):2626–2639, jul
2022. ISSN 2150-8097. doi: 10.14778/3551793.3551819.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS ,
2019.
Vatsal Patel, Sutharshan Rajasegarar, Lei Pan, Jiajun Liu, and Liming Zhu. Evangcn: Evolving graph deep
neural network based anomaly detection in blockchain. In Weitong Chen, Lina Yao, Taotao Cai, Shirui
Pan, Tao Shen, and Xue Li (eds.), Advanced Data Mining and Applications , pp. 444–456, 2022.
Sandeep Polisetty, Juelin Liu, Kobi Falus, Yi Ren Fung, Seung-Hwan Lim, Hui Guan, and Marco Serafini.
Gsplit: Scaling graph neural network training on large graphs via split-parallelism, 2023.
Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling.
Modeling relational data with graph convolutional networks, 2017.
Zhihao Shi, Xize Liang, and Jie Wang. LMC: Fast training of GNNs via subgraph sampling with provable
convergence. In The Eleventh International Conference on Learning Representations , 2023. URL https:
//openreview.net/forum?id=5VBBA91N6n .
Alok Tripathy, Katherine Yelick, and Aydın Buluç. Reducing communication in graph neural network training.
InProceedings of the International Conference for High Performance Computing, Networking, Storage and
Analysis, 2020.
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph attention networks. In International Conference on Learning Representations , 2018. URL https:
//openreview.net/forum?id=rJXMpikCZ .
Roger Waleffe, Jason Mohoney, Theodoros Rekatsinas, and Shivaram Venkataraman. Mariusgnn: Resource-
efficient out-of-core training of graph neural networks, 2022.
Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan
Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang. Deep graph
library: A graph-centric, highly-performant package for graph neural networks, 2019. URL https:
//arxiv.org/abs/1909.01315 .
D Randall Wilson and Tony R Martinez. The general inefficiency of batch training for gradient descent
learning. Neural networks , 16(10):1429–1451, 2003.
Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recommender systems:
A survey, 2020.
Nuo Xu, Pinghui Wang, Long Chen, Jing Tao, and Junzhou Zhao. MR-GNN: Multi-resolution and dual
graph neural network for predicting structured entity interactions. In Proceedings of the Twenty-Eighth
International Joint Conference on Artificial Intelligence , aug 2019. doi: 10.24963/ijcai.2019/551.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph
convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’18, pp. 974–983, 2018.
doi: 10.1145/3219819.3219890.
15Under review as submission to TMLR
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. GraphSAINT:
Graph sampling based inductive learning method. In International Conference on Learning Representations ,
2020. URL https://openreview.net/forum?id=BJe8pkHFwS .
Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich, Rajgopal Kannan, Viktor
Prasanna, Long Jin, and Ren Chen. Decoupling the depth and scope of graph neural networks. In
A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information
Processing Systems , 2021. URL https://openreview.net/forum?id=d0MtHWY0NZ .
Da Zheng, Xiang Song, Chengru Yang, Dominique LaSalle, Qidong Su, Minjie Wang, Chao Ma, and George
Karypis. Distributed hybrid cpu and gpu training for graph neural networks on billion-scale graphs. arXiv
preprint arXiv:2112.15345 , 2021.
Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong Li, and Jingren Zhou. Aligraph:
A comprehensive graph neural network platform. Proc. VLDB Endow. , 12(12):2094–2105, aug 2019. doi:
10.14778/3352063.3352127.
16Under review as submission to TMLR
A Appendix
A.1 Graph Sampling
Below, we review three different sampling algorithms for minibatch training of GNNs. Our focus in this work
is samplers whose expected number of sampled vertices is a function of the batch size. All these methods are
applied recursively for GNN models with multiple layers.
A.1.1 Neighbor Sampling (NS)
Given a fanout parameter kand a batch of seed vertices S0, NS by (Hamilton et al., 2017) samples the
neighborhoods of vertices randomly. Given a batch of vertices S0, a vertexs∈S0with degree ds=|N(s)|, if
ds≤k, NS uses the full neighborhood N(s), otherwise it samples krandom neighbors for the vertex s.
A.1.2 LABOR Sampling
Given a fanout parameter kand a batch of seed vertices S0, LABOR-0 (Balin & Çatalyürek, 2023) samples
the neighborhoods of vertices as follows. First, each vertex rolls a uniform random number 0≤rt≤1. Given
batch of vertices S0, a vertexs∈S0with degree ds=|N(s)|, the edge (t→s)is sampled if rt≤k
ds. Since
different seed vertices ∈S0end up using the same random variate rtfor the same source vertex t, LABOR-0
samples fewer vertices than NS in expectation.
The LABOR-* algorithm is the importance sampling variant of LABOR-0 and samples an edge (t→s)
ifrt≤csπt, whereπis importance sampling probabilities optimized to minimize the expected number
of sampled vertices and csis a normalization factor. LABOR-* samples fewer vertices than LABOR-0 in
expectation.
Note that, choosing k≥maxs∈Vdscorresponds to training with full neighborhoods for both NS and LABOR
methods.
A.1.3 RandomWalk (RW) Sampling
Given a walk length o, a restart probability p, number of random walks a, a fanoutk, and a batch of vertices
S0, a vertexs∈S0, aRandom Walk (Ying et al., 2018) starts from sand each step picks a random neighbor
s′fromN(s). For the remaining o−1steps, the next neighbor is picked from N(s′)with probability 1−p,
otherwise it is picked from N(s). This process is repeated atimes for each seed vertex and lastly, the top k
visited vertices become the neighbors ofsfor the current layer.
Notice that random walks correspond to weighted neighbor sampling from a graph with adjacency matrix
˜A=/summationtexto
i=1Ai, where the weights of ˜Adepend on the parameters a,pandk. Random walks give us the ability
to sample from ˜Awithout actually forming ˜A.
A.2 Work Monotonicity Theorem
Theorem A.1. The work per epoch required to train a GNN model using minibatch training is monotonically
nonincreasing as the batch size increases.
Proof.Given anyn≥2, let’s say we uniform randomly sample without replacement S0⊂V, wheren=|S0|.
Now note that for any S′0⊂S0, using the definition in (2), we have S′l⊂Sl,∀l. We will take advantage of
that and define S′0=S0\{s}in following expression.
17Under review as submission to TMLR
/summationdisplay
s∈S0
S′0=S0\{s}|Sl|−|S′l|=/summationdisplay
s∈S0
S′0=S0\{s}/summationdisplay
w∈Sl1[w /∈S′l]
=/summationdisplay
s∈S0
S′0=S0\{s}/summationdisplay
w∈{s}l1[w /∈S′l]
=/summationdisplay
s∈S0
S′0=S0\{s}/summationdisplay
w∈{s}l1[w /∈{s′}l,∀s′∈S′0]
=/summationdisplay
w∈Sl/summationdisplay
s∈S0
w∈{s}l1[w /∈{s′}l,∀s′∈S0\{s}](8)
In the last expression, for a given w∈Sl, if there are two different elements s,s′∈S0such thatw∈{s}l
andw∈{s′}l, then the indicator expression will be 0. It will be 1only ifw∈{s}lfor a unique s∈S0. So:
/summationdisplay
w∈Sl/summationdisplay
s∈S0
w∈{s}l1[w /∈{s′}l,∀s′∈S0\{s}] =/summationdisplay
w∈Sl
∃!s∈S0,w∈{s}l1
=|{w∈Sl|w∈{s}l,∃!s∈S0}|≤|Sl|(9)
Using this, we can get:
/summationdisplay
S′0⊂S0
|S′0|+1=|S0||Sl|−|S′l|≤|Sl|
⇐⇒ |S0||Sl|−/summationdisplay
S′0⊂S0
|S′0|+1=|S0||S′l|≤|Sl|
⇐⇒ |Sl|(|S0|−1)≤/summationdisplay
S′0⊂S0
|S′0|+1=|S0||S′l|
⇐⇒ |Sl|(|S0|−1)≤|S0|E[|S′l|]
⇐⇒|Sl|
|S0|≤E[S′l]
|S′0|
SinceS0was uniformly randomly sampled from V, its potential subsets S′0are also uniformly randomly
picked from Vas a result. Then, taking an expectation for the random sampling of S0⊂V, we conclude
thatE[|Sl|]
|S0|≤E[|S′l|]
|S′0|, i.e., the expected work of batch size nis not greater than the work of batch of size
n−1. This implies that the work with respect to batch size is a monotonically nonincreasing function.
A.3 Overlap monotonicity
Theorem A.2. The expected subgraph size E[|Sl|]required to train a GNN model using minibatch training is
a concave function of batch size, |S0|.
Proof.Given anyn≥2, let’s say we uniformly randomly sample without replacement S0⊂Vof sizen. Note
that we use the TlandTl
2as defined in Equations (5) and (6).
18Under review as submission to TMLR
|Tl(S0)|−2|Tl
2(S0)|=/summationdisplay
S′0⊂S0
|S′0|+1=|S0||Tl(S0)|−|Tl(S′0)|
=|S0||Tl(S0)|−|S0|E[|Tl(S′0)|]
⇐⇒ (|S0|−1)|Tl(S0)|=|S0|E[|Tl(S′0)|]−2|Tl
2(S0)|
⇐⇒|Tl(S0)|
|S0|=E[|Tl(S′0)|]
|S′0|−2|Tl
2(S0)|
|S′0||S0|
=⇒|Tl(S0)|
|S0|≤E[|Tl(S′0)|]
|S′0|(10)
where the first equality above is derived similar to Equations (8) and (9). Overall, this means that the overlap
between vertices increases as the batch size increases. Utilizing our finding from Equations (8) and (9), we
have:
/summationdisplay
S′0⊂S0
|S′0|+1=|S0||Sl|−|S′l|=|Tl(S0)|
=⇒ |S0||Sl|−|S0|E[|S′l|] =|Tl(S0)|
=⇒ |Sl|=E[|S′l|] +|Tl(S0)|
|S0|
=⇒E[|Sl|] =E[|S′l|] +E[|Tl(S0)|]
|S0|(11)
Note that the last step involved taking expectations for the random sampling of S0. See the recursion
embedded in the equation above, the expected size of the subgraph Slwith batch size|S0|depends on the
expected size of the subgraph S′lwith batch size|S0|−1. Expanding the recursion, we get:
E[|Sl|] =|S0|/summationdisplay
i=1E[|Tl(S0
i)|]
i(12)
whereS0
iis a random subset of S0of sizei. SinceE[|Tl(S0
i)|]
iis monotonically nonincreasing as iincreases as
we showed in (10), we conclude that E[|Sl|]is a concave function of the batch size, |S0|.
A.4 Subgraph density monotonicity
Theorem A.3. The expected sampled subgraph densityE[|SE|]
|S|is nondecreasing as the batch size |S|increases.
Proof.Givenn≥1and any probability distribution ponVsuch thatpsdenotes the probability of including
sin our minibatch Sof size|S|=n, the probability of including the edge t→sbecomesptps. Sinceps(n)is
linear as a function of n,∀s∈V, it can be decomposed as ps(n) =p′
sn.
E[|SE|]
|S|=1
|S|/summationdisplay
(t→s)∈EP(t∈S∧s∈S) =1
|S|/summationdisplay
(t→s)∈Eptps=1
n/summationdisplay
(t→s)∈Ep′
tnp′
sn=n/summationdisplay
(t→s)∈Ep′
tp′
s(13)
We can see that the expected sampled subgraph density is a linear function of the batch size |S|=nand this
implies that it is nondecreasing as the batch size |S|increases.
19Under review as submission to TMLR
Figure 6: Monotonicity of the work. x axis shows the batch size, y axis shows E[|S3|]for node prediction (top
row) andE[|S3|]
|S0|for edge prediction (bottom row), where E[|S3|]denotes the expected number of vertices
sampled in the 3rd layer and |S0|denotes the batch size. RW stands for Random Walks, NS stands for
Neighbor Sampling, and LABOR-0/* stand for the two different variants of the LABOR sampling algorithm
described in Section 2.2. Completes Figure 3.
A.5 Experimental Setup
Setup:In our experiments, we use the following datasets: reddit (Hamilton et al., 2017), papers100M (Hu
et al., 2020a), mag240M (Hu et al., 2021), yelp and flickr (Zeng et al., 2020), and their details are given
in Table 2. We use Neighbor Sampling (NS) (Hamilton et al., 2017), LABOR Sampling (Balin & Çatalyürek,
2023) and Random Walks (RW) (Ying et al., 2018) to form minibatches. We used a fanout of k= 10for
the samplers. In addition, Random Walks used length of o= 3, restart probability p= 0.5and number of
random walks from each seed a= 100. All our experiments involve a GCN model with L= 3layers (Hamilton
et al., 2017), with 1024 hidden dimension for mag240M and papers100M and 256 for the rest. Additionally,
papers100M and mag240M datasets were made undirected graphs for all experiments and this is reflected in
the reported edge counts in Table 2. Input features of mag240M are stored with the 16-bit floating point
type. We use the Adam optimizer (Kingma & Ba, 2014) with 10−3learning rate in all the experiments.
Implementation: We implemented1our experimental code using C++ and Python in the DGL frame-
work (Wang et al., 2019) with the Pytorch backend (Paszke et al., 2019). All our experiments were repeated
50 times and averages are presented. Early stopping was used during model training runs. So as we go to the
right along the x-axis, the variance of our convergence plots increases because the number of runs that were
ongoing is decreasing.
A.6 Complexity Analysis (cont.)
Our goal in this section is to empirically show the work reduction enjoyed by cooperative minibatching
over independent minibatching by reporting the number of vertices and edges processed per PE. We also
report the number of vertices that are communicated for cooperative minibatching during its all-to-all calls
in Algorithm 1 and Figure 7b. The results are given in Table 7.
Looking at Tables 1 and 7, we make the following observations:
1Source code is available in the supplementary material.
20Under review as submission to TMLR
Forward 
L2Backward 
L1Update
PE 0
PE 1Forward 
L1Backward 
L2
Forward 
L2Backward 
L1UpdateForward 
L1Backward 
L2
Storage to Compute Cluster Connection
Compute Cluster InterconnectCommunication 
Volume Relative 
to L2 input nodes, 
𝑆2
(a) Independent Minibatching
Forward 
L2Backward 
L1Update
PE 0
PE 1Forward 
L1Backward 
L2
Forward 
L2Backward 
L1UpdateForward 
L1Backward 
L2
Storage to Compute Cluster ConnectionCommunication 
Volume Relative 
to L2 input nodes, 
𝑆2
Compute Cluster Interconnect
Communication 
Volume Relative to 
L1 input nodes, 𝑆1Communication 
Volume Relative to 
L2 input nodes, 𝑆2 (b) Cooperative Minibatching
Figure 7: A comparison of Independent and Cooperative Minibatching approaches in the feature loading and
forward-backward GNN stages. The thickness of the red arrows indicates the data volume. Due to redundant
vertices across PEs, Independent Minibatching wastes (PCI-e) bandwidth for vertex embedding copies from
Storage. Moreover, the PEs perform identical computations for redundant edges across PEs in Independent
Minibatching.
Table 7: Average number of vertices and edges sampled in different layers with LABOR-0 per PE, reduced
by taking the maximum over 4 PEs (All the numbers are in thousands, lower is better) with batch size
|S0|= 1024.c|˜Sl|shows the number of vertices communicated at layer l. Papers and mag were used as short
versions of papers100M/GCN and mag240M/R-GCN dataset model pairs respectively. Last column shows
forward-backward (F/B) runtime in ms.
Dataset Part. I/C |S3|c|˜S3| |˜S3| |E2| |S2|c|˜S2| |˜S2| |E1| |S1|F/B
papersrandom Indep 463 046373074.8 074.893.69.638.9
random Coop 31831146360862.456.882.889.99.2813.0
metis Coop 32817940261563.134.073.890.89.3512.0
magrandom Indep 443 044364767.9 067.982.08.78199.9
random Coop 32431045956659.853.177.380.48.62183.3
metis Coop 33417841957660.631.071.381.88.80185.1
1.All runtime complexities for cooperative minibatching scales with |Sl
p(B)|=|Sl(B)|1
Pand|El
p(B)|=
|El(B)|1
P≤|Sl(B)|k
Pand for independent minibatching with |Sl(B
P)|and|El(B
P)|≤|Sl(B
P)|k, for a
sampler with fanout k. SinceE[|Sl(B)|]is a concave function, E[|Sl(B)|]1
P≤E[|Sl(B
P)|], and this
corresponds to looking at Figure 3 first row with x=Bfor coop and x=B
Pfor independent if one
wanted to guess how their runtime would change with changing BandP. For an example, all the
runtime numbers we have provided in the Table 4 are for 4 GPUs. Going from 4 to 8 would increase
the edge of cooperative over independent even more, see Table 5.
2.Sampling and Feature loading over PCI-e requires α≫βfor cooperative to get a speedup over
independent.
3. In order for cooperative F/B to improve against independent, we need thatα
c>γ
M.
4.Cross edge ratio creduces all communication between PEs. In particular, graph partitioning will
lower both cand|˜Sl
p(B)|, lowering the communication overhead, see c|˜Sl|columns in Table 7.
5.The model complexity Mis small for the GCN model (papers100M) but large for the R-GCN model
(mag240M), as shown by the F/B runtime numbers in Table 2. Also, the communication overhead
between the two scenarios is similar, meaning communication can take from upto 30% to less than a
few percent depending on M. For the papers100M dataset, communication makes up more of the
21Under review as submission to TMLR
runtime, so graph partitioning helps bring the F/B runtime down. However, the load imbalance
caused by graph partitioning slows down the F/B runtime despite lowered communication costs for
the mag240M dataset.
A.7 Smoothed Dependent Minibatching
As described in Section 2, NS algorithm works by using the random variate rtsfor each edge (t→s). Being
part of the same minibatch means that a single random variate rtswill be used for each edge. To generate
these random variates, we initialize a Pseudo Random Number Generator (PRNG) with a random seed z
along with tandsto ensure that the first rolled random number rtsfrom the PRNG stays fixed when the
random seed zis fixed. Given random seeds z1andz2, let’s say we wanted to use z1for the first κiterations
and would later switch to the seed z2. This switch can be made smoothly by interpolating between the
random seeds z1andz2while ensuring that the resulting sampled random numbers are uniformly distributed.
If we are processing the batch number i<κin the group of κbatches, then we want the contribution of z2to
bec=i
κ, while the contribution of z1is1−c. We can sample n1
ts∼N(0,1)with seedz1andn2
ts∼N(0,1)
with seedz2. Then we combine them as
nts(c) = cos(cπ
2)n1
ts+ sin(cπ
2)n2
ts
note thatnts(0) =n1
ts,nts(1) =n2
tsandnts(c)∼N(0,1),∀c∈Ralso, then we can set rts= Φ(nts(c)), where
Φ(x) =P(Z≤x)forZ∼N(0,1), to getrts∼U(0,1)that the NS algorithm can use. Dropping sfrom all
the notation above gives the version for LABOR. In this way, the random variates change slowly as we are
switching from one group of κbatches to another. When i=κ, we letz1←z2and initialize z2with another
random seed. To use this approach, only the random variate generation logic needs modification, making
its implementation for any sampling algorithm straightforward compared to the nested approach initially
described. Figure 2 shows an example of this approach for κ= 2.
A.8 Dependent batches (cont.)
Looking at the training loss and validation F1-score with sampling curves in Figure 8, we notice that the
performance gets better as κis increased. This is due to the fact that a vertex’s neighborhood changes slower
and slower as κis increased, the limiting case being κ=∞, in which case the sampled neighborhoods are
unchanging. This makes training easier so κ=∞case leads the pack in the training loss and validation F1-
score with sampling curves.
A.9 Comparing a single batch vs Pindependent batches convergence
We investigate whether training with a single large batch in P-GPU training shows any convergence differences
to the current approach of using Pseparate batches for each of the GPUs. We use a global batch size of
4096and divide a batch into P≤8independent batches, with each batch having a size of4096
P. We use NS
and LABOR-0 samplers with fanouts of k= 10for each of the 3layers. Figure 9 shows that there are no
significant differences between the two approaches, we present the results averaged over the samplers to save
space.
A.10 Redundancy Free GCN aggregation
Jia et al. (2020) proposes a method of reducing processing during neighborhood aggregation by finding
common sub-neighborhoods among aggregated vertices, whether using full-batch or minibatch training. That
is, if two vertices have the neighborhoods {A,B,C}and{B,C,D}, and a summation operator is used for
aggregation, then instead of computing four additions: A+B+CandB+C+Dconcurrently, the three
additionsBC=B+C,A+BC, andBC+Dcan be computed. This approach is orthogonal to the
approaches proposed in Section 3 in that it reduces redundant aggregation steps, whereas our approach
reduces redundant input nodes and edges in parallel computations. As such, the two approaches could be
employed together.
22Under review as submission to TMLR
Figure 8: LABOR-0 sampling algorithm with 1024batch size and varying κdependent minibatches, κ=∞
denotes infinite dependency, meaning the neighborhood sampled for a vertex stays static during training.
The first row shows the training F1-score with the dependent sampler. The second row shows the training
loss curve. Completes Figure 4.
Figure 9: Convergence difference between cooperative vs independent minibatching with a global batch size
of 4096 averaged over Neighbor and LABOR-0 samplers.
A.11 Limitations
OurproposedCooperativeMinibatchingapproachrequiresarelativelyfastinterconnectbetweentheProcessing
Elements. Modern multi-GPU systems usually have such interconnect between the GPUs. Nowadays, such fast
interconnects are being extended to the multi-node setting, for up to 72 GPUs connected via NVLink (NVIDIA,
2024).
23Under review as submission to TMLR
On distributed training, each computing node equipped with multiple cores and/or GPUs, the interconnect
between cores and/or GPUs of the same node, are also relatively much faster than the interconnect among
nodes.
The proposed dependent minibatching approach does not have any such limitation.
24