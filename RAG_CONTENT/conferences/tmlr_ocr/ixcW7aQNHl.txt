Under review as submission to TMLR
Exact and Approximate Conformal Inference for Multi-task
Learning
Anonymous authors
Paper under double-blind review
Abstract
It is common in machine learning to estimate a response ygiven covariate information
x. However, these predictions alone do not quantify any uncertainty associated with said
predictions. One way to overcome this deficiency is with conformal inference methods,
which construct a set containing the unobserved response ywith a prescribed probability.
Unfortunately, even with a one-dimensional response, conformal inference is computationally
expensive despite recent encouraging advances. In this paper, we explore multi-task learning
within a regression setting, delivering exact derivations of conformal inference p-values
when the predictive model can be described as a linear function of y. Additionally, we
propose unionCP and a multivariate extension of rootCPas efficient ways of approximating
the conformal prediction region for a wide array of multi-task predictors while preserving
computational advantages. We also provide both theoretical and empirical evidence of the
effectiveness of these methods.
1 Introduction
In regression, we aim to predict (or estimate) some response ygiven covariate information x. These predictions
alone deliver no information related to the uncertainty associated with the unobserved response, and thus,
would benefit from the inclusion of a set Γ(α)(x)such that, for any significance level α∈(0,1),
P/parenleftbig
y∈Γ(α)(x)/parenrightbig
= 1−α. (1)
One method to generate Γ(α)is through conformal inference (used interchangeably with “conformal
prediction” in this work) (Gammerman et al., 1998; Lei et al., 2018), which generates conservative prediction
sets for some unobserved response yunder only the assumption of exchangeability. Given a finite number
of observationsDn={(xi,yi)}n
i=1and a new unlabelled example xn+1, conformal prediction regions are
generated through the repeated inversion of the test,
H0:yn+1=zvs.Ha:yn+1̸=z, (2)
wherezis a potential candidate response value, i.e.,the null hypothesis (Lei et al., 2018). A p-value for
this test is constructed by learning a predictive model ˆy(z)on the augmented dataset Dn∪(xn+1,z)and
comparing one’s ability to predict the new candidate zusing ˆyn+1(z)to the already observed responses using,
say,ˆyi(z), the predicted value for the i-th response as a function of z. We note that while ˆyi(z)depends
onDn,xi,xn+1, andz, we only explicitly highlight dependence on z. The so-called conformal prediction
set is the collection of candidates zfor which the null hypothesis is not rejected, i.e.,when the error in
predictingzis not too high compared to others.
The inversion of the test is Equation (2) is traditionally called “full” conformal prediction since it uses
the entire dataset to learn a predictive model. Unfortunately, full conformal prediction is computationally
demanding in most cases, with each new candidate point zrequiring a new model to be fit. To avoid this
complexity, more efficient methods, e.g.,split conformal inference (Vovk et al., 2005; Lei et al., 2018) and
trimmed conformal inference (Chen et al., 2016), have been introduced with trade-offs between computational
efficiency and performance.
1Under review as submission to TMLR
Of interest to our work in this paper are exactandapproximate conformal inference methods, which aim to
reduce computational complexity without sacrificing performance. Nouretdinov et al. (2001) showed that
with certain models, ridge regressors in particular, conformity measures for every observation in a dataset
can be constructed as an affine function of the candidate value zand only require training the model once.
In our work, we extend the result of Nouretdinov et al. (2001) to predictors of the form
ˆy=Hy, (3)
whereyisann×1vectorofresponses, Hisann×nmatrix, and ˆyisann×1vectorofpredictions. Wenotethat
Hcan also be a function of a set of covariates, e.g.,as with ridge regression where H=X(X⊤X+λI)−1X⊤.
In reality, the restriction shown in Equation (3) is more general than ridge regression; we only require the
predictions be linear functions of the input. In this paper, we refer to models that follow Equation (3) as
linearmodels; this is in contrast to the traditional usage of the term to reflect models that are linear with
respect to their parameters.
With some models, exact conformal inference is difficult. However, Ndiaye (2022) showed that under certain
regularity conditions on the model of interest, we can generate upper and lower bounds on the conformal
prediction set, i.e.,interval, with only a single model fit, allowing for conservative approximations of the
true conformal prediction set. In more complex settings it might be of interest to construct a model for
multiple responses, i.e.,for some response y∈Rq, also known as multi-task (or multi-output) regression
(Zhang & Yang, 2018; Borchani et al., 2015; Xu et al., 2019). Thus, we might wish to construct a prediction
set such that some some q-dimension version of y, sayy= (y(1),...,y(q))⊤, is contained with some specified
probability.
Contributions With these potential scenarios is mind, we aim to extend exact and approximate conformal
inference to the multi-task setting. Specifically, we contribute:
•extensions of exact conformal inference to multiple dimensions with various predictors and conformity
measures
•unionCP to approximate conformal prediction sets without model retraining
•a multivariate extension of rootCP(Ndiaye & Takeuchi, 2021) which utilizes numerical root-based
methods to find points on the boundary of a conformal prediction set
The introduction of unionCP and extension of rootCPprovide a trade-off between various conformal inference
methods, balancing the computational efficiency of split conformal prediction ( splitCP) with the performance
of full conformal prediction ( fullCP). Table 1 summarizes the overall computational costs for each of these
methods in terms of the number of model retraining iterations required to generate the conformal prediction
region. We also include the computation complexity of the CP approximation provided by a grid-based
approach ( gridCP). In contrast, fullCPcomprises approaches where the exact conformal prediction set can
be constructed in a closed-form.
Table 1: Computational complexity of methods where qis the response dimension, mis the cardinality of the
candidate value set, dis the number of search directions, and ϵis the tolerance.
Method Linear Nonlinear
splitCPO(q)O(q)
gridCPO(q)O(mq)
fullCPO(q) -
unionCPO(q)O/parenleftbig
ndqlog2(1/ϵ)/parenrightbig
rootCPO(q)O/parenleftbig
dqlog2(1/ϵ)/parenrightbig
2Under review as submission to TMLR
From Table 1, we can see that in the linear case, each of the methods for prediction set generation require the
same number of model refits as splitCP. We note that this does not account for the complexity of interval
construction in each case.
The rest of the paper is laid out as follows. Section 2 provides requisite background for the paper. Section
3 extends exact conformal inference to multiple dimensions, while Section 4 introduces various conformal
prediction set approximation methods in multiple dimensions. Section 5 provides empirical evaluation of our
proposed approaches. Section 6 concludes the paper.
Notation We denote the design matrix X= (x1,...,xn,xn+1)⊤. Givenj∈[n], the rank of an element uj
among a sequence {u1,...,un}is defined as
Rank(uj) =n/summationdisplay
i=11ui≤uj.
2 Conformal Inference
In this section we provide background on relevant topics for this paper. Our applications within with paper
are focused on regression, so we focus our background discussion on regression as well.
Originally introduced in Gammerman et al. (1998) as “transductive inference”, conformal inference (CI)
was originally focused on providing inference with classification approaches. Vovk et al. (2005) provides a
formalized introduction to conformal inference within regression. With the express purpose of inference, the
goal of CI is to attach, in some fashion, a measure of uncertainty to a predictor, specifically through the
construction of a conservative prediction set, i.e.,one such that
P/parenleftbig
yn+1∈Γ(α)(xn+1)/parenrightbig
≥1−α. (4)
We defineDn={(xi,yi)}n
i=1as a collection of nobservations, where the i-th data tuple (xi,yi)is made
up of a covariate vector xiand a response yi. We wish to construct a validprediction set for a new
observation (xn+1,yn+1), wherexn+1is some known covariate vector and yn+1is some, yet-to-be-observed
response. Assuming each data pair (xi,yi)and(xn+1,yn+1)are drawn exchangeably from some distribution
P, conformal inference generates conservative, finite-sample valid prediction sets in a distribution-free manner.
The main approach to perform the test inversion associated with Equation (2) relies on Lemma 1.
Lemma 1. LetU1,...,Un,Un+1be an exchangeable sequence of random variables. Then, for any α∈(0,1),
P/parenleftbig
Rank (Un+1)≤⌈(1−α)(n+ 1)⌉/parenrightbig
≥1−α.
In a prediction setting, test inversion for a particular candidate value zis achieved by training the model of
interest on an augmented data set Dn+1(z) =Dn∪(xn+1,z). At this point, we leave our model of interest
general, denoting the prediction of the i-th observation based on a model trained with Dn+1(z)asˆyi(z).
Following the refitting, each observation in the augmented data set receives a (non)conformity measure, which
determines the level of (non)conformity between itself and other observations. One popular, and particularly
effective, conformity measure is the absolute residual
Si(z) =|yi−ˆyi(z)|. (5)
We can construct the conformity scoreassociated with a particular candidate point zwith
π(z) =1
n+ 1+1
n+ 1n/summationdisplay
i=11Si(z)≤Sn+1(z), (6)
whereSi(z)is the conformity measure for the data pair (xi,yi)as a function of zandSn+1(z)is the conformity
measure associated with (xn+1,z). Then, a valid p-value for the test shown in Equation (2) can be found with
p-value (z) = 1−π(z).
3Under review as submission to TMLR
A prediction set for an unknown response yn+1associated with some covariate vector xn+1is
Γ(α)(xn+1) ={z: (n+ 1)π(z)≤⌈(1−α)(n+ 1)⌉}. (7)
Then, by Lemma 1, with
(n+ 1)π(yn+1)≡Rank(Sn+1(yn+1)),
Equation (4) holds for Γ(α)(xn+1). By the previous results, CI can also be utilized in the multivariate
response case, where one is interested in quantifying uncertainty with respect to the joint behavior of a
collection of responses, given a set of covariates. Thus, we can construct a multidimensional prediction set
Γ(α)(xn+1)⊂Rqsuch that Equation (4) holds when yn+1is someq-dimensional random vector.
The first result extending conformal inference to the multivariate setting comes from Lei et al. (2015),
which applies conformal inference to functional data, providing bounds associated with prediction “bands”.
Diquigiovanni et al. (2022) extends and generalizes additional results for conformal inference on functional
data. Joint conformal prediction sets outside the functional data setting are explored in Kuleshov et al.
(2018) and Neeven & Smirnov (2018). Messoudi et al. (2020; 2021) extend these works through the use of
Bonferroni- and copula-based conformal inference, respectively. Cella & Martin (2020), Kuchibhotla (2020)
and Johnstone & Cox (2021) construct joint conformal sets through the use of depth measures, e.g.,half-space
and Mahalanobis depth, as the overall conformity measure. Applications of conformal inference have been
seen in healthcare (Olsson et al., 2022), drug discovery (Cortés-Ciriano & Bender, 2019; Eklund et al., 2015;
Alvarsson et al., 2021), and decision support (Wasilefsky et al., 2023), to name a few. For a thorough
treatment on conformal inference in general, we point the interested reader to Fontana et al. (2023) and
Angelopoulos et al. (2023).
2.1 Computationally Efficient Conformal Inference
Due to the inherent model refitting required to generate prediction sets through full conformal inference,
i.e.,the testing of an infinite amount candidate points, more computationally efficient methods have
been explored. We describe a subset of these methods in the following sections. Specifically, we focus on
resampling-based and exact conformal inference.
2.1.1 Resampling Methods
Split conformal inference (Vovk et al., 2005; Lei et al., 2018) generates conservative prediction intervals under
the same assumptions of exchangeability as fullconformal inference . However, instead of refitting a model
for each new candidate value, split conformal inference utilizes a randomly selected partition of Dn, which
includes a training set I1and a calibration set I2. First, a prediction model fit using I1. Then, conformity
measures are generated using out-of-sample predictions for observations in I2. The split conformal prediction
interval for an incoming (xn+1,yn+1), when using the absolute residual as our comformity measure, is
Γ(α)
split(xn+1) = [ˆyn+1−s,ˆyn+1+s], (8)
where ˆyn+1is the prediction for yn+1generated using the observations in I1, andsis the⌈(|I2|+1)(1−α)⌉-th
largest conformity measure for observations in I2. In order to combat the larger widths and high variance
associated with split conformal intervals, cross-validation (CV) approaches to conformal inference have also
been implemented. The first CV approach was introduced in Vovk (2015) as cross-conformal inference with
the goal to “smooth” inductive conformity scores across multiple folds. Aggregated conformal predictors
Carlsson et al. (2014) generalize cross-conformal predictors, constructing prediction intervals through any
exhangeable resampling method, e.g.,bootstrap resampling. Other resampling-based conformal predictors
also include CV+ and jackknife+ (Barber et al., 2021). For a more detailed review and empirical comparison
of resampling-based conformal inference methods, we point the interested reader to Contarino et al. (2022).
2.1.2 Exact Conformal Inference for Piecewise Linear Estimators
In order to test a particular set of candidate values for inclusion in Γ(α)(xn+1), we must compare the
conformity measure associated with our candidate data point to the conformity measures of our training
4Under review as submission to TMLR
data. Naively, this requires the refitting of our model for each new candidate value. However, Nouretdinov
et al. (2001) showed that Si(z), constructed using Equation (5) in conjunction with a ridge regressor, varies
piecewise-linearly as a function of the candidate value z, eliminating the need to test a dense set of candidate
points through model refitting. Other exact conformal inference methods include conformal inference through
homotopy (Lei, 2019; Ndiaye & Takeuchi, 2019), influence functions (Bhatt et al., 2021; Cherubin et al.,
2021), and root-finding approaches (Ndiaye & Takeuchi, 2021). While not exact, Ndiaye (2022) provide
approximations to the full conformal prediction region through stability-based approaches.
3 Exact Conformal Inference for Multi-task Learning
In the following sections, we extend the results in Nouretdinov et al. (2001) to multiple dimensions. We also
discuss closed-form solutions for more general predictors as well as higher dimension prediction sets with
other conformity measures. While CI can be applied to any prediction or classification task, in this section
we restrict each of our predictors, given an incoming observation (xn+1,z), to the form
ˆy(k)(zk) =Hk(xn+1,xi)y(k)(zk), (9)
where ˆy(k)(zk)is the vector of predictions for the k-th response as a function of the candidate value zk, and
the candidate value vectoris defined as z= (z1,...,zq)⊤. We note that the restriction shown in Equation (9)
is analogous to the restriction identifed in Equation (3). Additionally, we require that Hkbe constructed
independently of y(k),i.e.,not as a function of y(k). Even with this restriction, Hkis general enough so as
to include many classes of predictors with examples described below. We specifically discuss how to construct
exactp-values for a given zwithout retraining our model. We also identify how we construct explicit p-value
change-point sets , denoted asEifor thei-th observation, where
Ei≡{z∈Rq:Sn+1(z)≤Si(z)}, (10)
with the end goal of generating exact conformal prediction sets. Note that En+1≡Rq. Then, the p-value
associated with the hypothesis test shown in Equation (2) for any candidate point zis
p-value (z) =|{i∈[n+ 1] :z∈Ei}|
n+ 1. (11)
In the following sections, we describe several predictors with which we can perform exact conformal inference.
We also describe exact conformal inference results for two conformity measure constructions, ℓ1and||·||2
W, as
well as results for finding points on the boundary of a conformal prediction set for any conformity measure.
3.1 Predictors For Exact Conformal Inference
Many regression methods generate predictions that follow Equation (9), including: ridge regression, kernel
regression, and k-nearest neighbors, among others. In the sequel, we describe the predictions resulting from
these approaches in the form laid out prior in Equation (3).
Ridge Regression While we have already described Hwith respect to a ridge regressor using regularization
parameterλin Section 1, we explicitly describe it for the k-th response dimension as
Hk(xn+1) =X(X⊤X+λkI)−1X⊤. (12)
Local Constant (Nadaraya-Watson) Regression Kernel regression (Nadaraya, 1964; Watson, 1964) is
a nonparametric regression technique that utilizes kernel density estimators (Parzen, 1962). Traditionally, a
“kernel” is of the form
Kh(u) =1
hK/parenleftigu
h/parenrightig
, (13)
5Under review as submission to TMLR
whereK(·)is a (symmetric) probability density, and his a bandwidth parameter. Often a Guassian kernel is
used,i.e.,K(u)≡Φ(u), but other kernels are also popular. Using a kernel, we can perform nonparametric
regression.
For somexiinDn+1(z), the Nadaraya-Watson regression estimator generates a prediction
ˆyi(z) =n+1/summationdisplay
j=1Kh(xi−xj)/summationtextn+1
t=1Kh(xi−xt)yj(z)
whereyj(z)is thej-th element of y(z). Thus, we can perform “local-constant” regression by using
Hk(xn+1,xi)≡Hk(xn+1) = (w1,...,wn+1)⊤where each wiis a vector of the normalized kernel values
Kh(·)for each observation xjcentered on xi,i.e.,
wi=/parenleftbig
Kh(xi−x1),...,Kh(xi−xn+1)/parenrightbig⊤/parenleftbig
In+1n+1/summationdisplay
j=1Kh(xi−xj)/parenrightbig−1.
The current form for our kernel is general. However, we have not specified how it can be extended to a
multivariate scenario; this is especially important for applications where we consider multiple covariates.
While there do exist multivariate kernels that take vector arguments and utilize a bandwidth matrix, simpler
approaches provide a more attractive and computationally efficient approach to generating multivariate
kernels. As an example, a productkernel (Scott, 1992) generates a multivariate kernel by multiplying marginal
kernel functions for each covariate. Namely, for an argument u= (u1,...,up)⊤and bandwidth vector
h= (h1,...,hp)⊤,
Kh(u) =p/productdisplay
k=1Khp(up). (14)
Another popular method for extension to multiple covariates are radial basis functions (Broomhead & Lowe,
1988), which utilize a norm as an argument to a univariate kernel.
Local Linear Regression In contrast to the local-constant regression with the Nadaraya-Watson estimator,
local-linear regression (Fan, 1992) utilizes a weighted version of the covariate matrix. Using the kernel
introduced in Equation (13), we can construct a vector wifor thei-th observation. Local-linear regression
then constructs an estimate for yi, as a function of the candidate value pair (xn+1,z), by using an adjusted
covariate matrix,
˜Xi=
1 (x1−xi)⊤
......
1 (xn+1−xi)⊤
,
and a diagonalized version of wi, which we identify as G(xi), resulting in predictions for the i-th observation
of thek-th response of the form
ˆy(k)
i(zk) =/parenleftbig
Hk(xn+1,xi)y(k)(zk)/parenrightbig
i,
where
Hk(xn+1,xi) =˜Xi/parenleftbig˜X⊤
iG(xi)˜Xi/parenrightbig−1˜X⊤
iG(xi),
and(·)iis thei-th element of the vector argument.
6Under review as submission to TMLR
k-nearest Neighbors k-Nearest neighbors (Cover & Hart, 1967; Fix, 1985) is a nonparametric regression
technique that generates predictions based on neighboring values of an incoming observation. Traditionally,
kis used to describe the number of neighbors utilized to construct a prediction. In this work, we use m.
Specifically, given a fixed m, with an observation for some xwe can construct a set of mneighbors of x,
made up of the training data observations. We define the set of neighbors for xasN(x). Then, a matrix
Hk(xn+1,xi)≡Hk(xn+1)can be constructed such that for each row-column position (i,j),
Hk(xn+1)ij=/braceleftbigg1/m x j∈N(xi)
0 otherwise. (15)
The result of Nouretdinov et al. (2001) was extended to include both lasso and elastic net regressors in Lei
(2019). For this paper, we utilize a generalized version, shown in Proposition 1.
Proposition 1. Assume the fitted model as in Equation (3), whereH(xn+1,xi)≡H. Then, if we define
y(z) = (y⊤,z)⊤, we can describe the vector residuals associated with the augmented dataset and some candidate
valuezas
ˆy(z)−Hy(z) =A−Bz
whereAandBare of the form
A=/parenleftbig
I−H/parenrightbig
y(0)
B=/parenleftbig
I−H/parenrightbig
(0,..., 0,1)⊤.
Proof.By assumption, we can describe our vector of predictions ˆy(z) =Hy(z). Thus,
y(z)−ˆy(z) =y(z)−Hy(z)
= (In+1−H)y(z)
= (In+1−H)y(0) + (In+1−H)(0,..., 0,1)⊤z
=A+Bz
With Proposition 1, we can then describe the conformity measure for the i-th observation, when using
Equation (5), as Si(z) =|ai+biz|.
3.2 Exact p-values with ℓ1
We formalize our extension of Nouretdinov et al. (2001) to multiple dimensions, specifically utilizing
Si(z) =||yi−ˆyi(z)||1, (16)
as our conformity measure, in Proposition 2.
Proposition 2. Assume the fitted model, ˆy(k)(zk) =Hk(xn+1,xi)y(k)(zk). Then, using Equation (16),
Si(z) =||ai+biz||1,
whereai= (a1i,...,aqi)⊤,bi= (b1i,...,bqi)⊤, andakiandbkiare thei-th elements of the vectors Akand
Bk, respectively, defined as
Ak=/parenleftbig
I−Hk(xn+1,xi)/parenrightbig
y(k)(0), (17)
and,
Bk=/parenleftbig
I−Hk(xn+1,xi)/parenrightbig
(0,..., 0,1)⊤. (18)
7Under review as submission to TMLR
Algorithm 1 exact conformal prediction with ℓ1
Input:dataDn={(x1,y1),..., (xn,yn)}, andxn+1
Coverage level α∈(0,1)
Dimension q
#Initialization
ConstructHk(xn+1)for eachk= 1,...,d.
Constructaki,bkifor alli= 1,...,n + 1.
Construct ˜zas in Equation (19).
#Constructp-value change-point sets
We define ˜z(j)as˜zwithout the j-th component.
fori∈1,...,ndo
Initialize set of corner points V=∅.
forj∈1,...,qdo
Fixzk= ˜zkfork̸=j. Then, find the set
z∗
j={zj:ci+|aji+bjizj|=|ajn+1+bjn+1zj|}
SetV=V∪{ (˜z(j),z∗
j)}
end for
SetEi=˜chull/braceleftbig
V}where ˜chull/braceleftbig
S/bracerightbig
is the convex hull of the set S.
end for
Return:E={Ei}n
i=1
Proof.The proof follows directly from applying Proposition 1 to each element of the vector.
Proposition 2 allows us to construct conformity measures associated with a multidimensional response without
retraining the model for each new z. Additionally, using Proposition 2 for each observation (xi,yi), we can
generate a region Ei, as defined in Equation (10). Additionally, we can construct a fixed-point solution for
ˆyn+1(z),i.e.,a point where ˆyn+1(z) =z, as
˜z=/parenleftbigg
−a1n+1
b1n+1,...,−aqn+1
bqn+1/parenrightbigg
. (19)
Equation (19) can be derived by setting each component of Sn+1(z)equal to zero; the fixed point for a given
observation is where the probability of a more extreme response, i.e.,p-value (z), is maximized.
It is initially unclear howthe construction of an individual region Eioccurs. As it stands, finding all zsuch
thatSi(z) =Sn+1(z)is a multidimensional root-finding problem with infinite solutions, which has exponential
complexity as qincreases. However, utilizing the inherent structure of each Ei, we can simplify the problem.
In order to provide clarity, we include Algorithm 1 to construct Eiin practice when using ℓ1.
Algorithm 1 leverages the fact that when using ℓ1, eachEican be defined by the convex hull of a collection of
points, specifically points axis-aligned with the fixed-point solution ˜z. These points, referred to as “corners”
within this paper, differ from ˜zin only the j-th element. The j-th element of a corner point, defined as z∗
j, is
such that
z∗
j={zj:ci+|aji+bjizj|=|ajn+1+bjn+1zj|}, (20)
8Under review as submission to TMLR
whereci=/summationtext
k̸=j|aki+bki˜zk|. We emphasize that all other components for the n+ 1-th conformity measure
are zero by definition of ˜z. Then,z∗
jis either one of two solutions, zl
jorzu
j. With the decomposition shown
in Equation (20), finding Eiis reduced to a series of qone-dimensional root-finding problems. We include
a two-dimensional visual of the solutions generated using Algorithm 1 for an observation from the cement
dataset in Figure 1. We also include further discussion on Algorithm 1 in Supplementary Materials
−10 0 10 20 30 4020 30 40 50 60 70 80
slumpflow
Figure 1: Example of Algorithm 1 for constructing Eifor an observation from cementdataset. The “•”
identifies ˜z, while the black line represents the border of the p-value change-point set The points (˜z1,zl
2),
(˜z1,zu
2),(zl
1,˜z2)and(zu
1,˜z2)are identified with “ •”. The axes generated with ˜zare shown with the dotted
black lines.
3.3 Exact p-values with||·||2
W
In order to generalize our exact p-value construction further than for use solely with ℓ1, we now consider
conformity measures of the form
Si(z) =ri(z)⊤Wri(z)≡||ri(z)||2
W, (21)
whereri(z) =yi−ˆyi(z), andWis someq×qmatrix. Proposition 3 provides a similar result to Proposition 2,
but instead utilizes Equation (21). Namely, Sibecomes quadratic with respect to z, instead of piecewise-linear.
Proposition 3. Assume the fitted model ˆy(k)(zk) =Hk(xn+1,xi)y(k)(zk)for each response dimension k∈[q].
Then, using Equation (21),
Si(z) =
a1i+b1iz1
...
aqi+bqizq
⊤
W
a1i+b1iz1
...
aqi+bqizq

whereakiandbkiare thei-th elements of the vectors AkandBk, respectively, as defined in Equation (17)
and Equation (18).
Proof.LetSi(z)be constructed as in Equation (21). Then, by Proposition 1, each element of the vector of
residuals can be described in the form of a1k+b1kzk, which gives us the desired result.
With Proposition 3, the difference between Sn+1(z)andSi(z)is the difference between two quadratic forms.
Thus, we can describe the boundary Eifor everyi∈[n]as aconic section . Specifically, we can describe the
difference between the candidate conformity measure and the conformity measure for observation ias,
9Under review as submission to TMLR
Sn+1(z)−Si(z) =
a1n+1+b1n+1z1
...
aqn+1+bqn+1zq
⊤
W
a1n+1+b1n+1z1
...
aqn+1+bqn+1zq
−
a1i+b1iz1
...
aqi+bqizq
⊤
W
a1i+b1iz1
...
aqi+bqizq
.(22)
Knowing we aim to find the boundary of each Ei,i.e.,the roots of Equation (22), we can expand the statement
into the form of a conic section such that
(1,z1,...,zq)⊤[Mn+1−Mi](1,z1,...,zq) = 0, (23)
whereMiis
Mi=
βi0βi1/2... βiq/2
βi1/2βi11... βiq/2
............
βiq/2βqi/2... βiqq
, (24)
with the construction of each element of Mishown in Table 2.
Table 2:βparameter formulas.
Parameter Formula
βi0/summationtextq
k=1/summationtextq
j=1aikaijwkj
βik 2/summationtextq
j=1aijbikwkj
βikj bikbijwkj
We can then translate a point son the unit-ball to the boundary of Eiwith
z=/radicalbig
KiLis+zc
i
whereLiis the upper-triangular Cholesky matrix of M∗
i≡Mn+1−Mi. We define (·)qqas the lower q×q
submatrix of the argument and (·)qqias thei-th row of the lower q×qsubmatrix. zc
iis the center ofEi,i.e.,
zc
i= (M∗
i)−1
qq(−M∗
i)qq1 (25)
andKiis
Ki=−det(M∗
i)
det/parenleftbig
(M∗
i)qq/parenrightbig.
In order to maintain the probabalistic guarantees inherent to conformal inference, we require Wto be
constructed exchangeably. Two constructions that satisfy exchangeability are: 1) Wconstructed independently
ofDn+1(z), or 2)Wconstructed using all observations within Dn+1(z). However, we show in Section 5 that,
in practice, setting W≡ˆΣ−1, the observed inverse-covariance matrix associated with the residuals from our
qresponses using a model constructed using only Dn, performs well. The p-value associated with some z
using sets constructed using Equation (21) is the same as in Equation (11).
While Proposition 3 does not restrict the structure of W, limitingWto be a symmetric, positive semi-definite
matrix ensures that the set Eiis not only convex, but ellipsoidal. Without this additional restriction on the
matrixW, thep-value change-point sets could be ill-formed, i.e.,non-convex. An example of an ill-formed
p-value change-point set is shown in Figure 2.
For clarity, we include Algorithm 2 to describe how each Eican be constructed in practice when using ||·||2
W.
We also compare the exact p-value change-point sets constructed using ||·||2
WwithW=ˆΣ−1to the
conformal prediction p-value contours constructed using gridCPin Figure 3. Exact p-value change-points
sets constructed using ℓ1and||·||2
WwithW=Iqare shown in Figure 4.
10Under review as submission to TMLR
x−5
0
5y
−505z
050100150200
(a) Plot of ψ1andψ2
x−5 0 5y−505z−200
−100
0
100 (b) Plot of the difference ψ1−ψ2
Figure 2:ψ1:z∝⇕⊣√∫⊔≀→∥y0−T1z∥andψ2:z∝⇕⊣√∫⊔≀→∥T2z∥wherey0= (1,0)⊤,T1= (−1−1
−1 0)andT2= (0−1
0 1).
0 5 10 15 20 2520 30 40 50 60 70 80
slumpflow
0 5 10 15 20 2520 30 40 50 60 70 80
slumpflow
Figure 3: Comparing gridCPsets (left) to closed-form p-value change-point sets (right) constructed using
||·||2
WwithW=ˆΣ−1
.
11Under review as submission to TMLR
Algorithm 2 exact conformal prediction with ||·||2
w
Input:dataDn={(x1,y1),..., (xn,yn)}, andxn+1
Coverage level α∈(0,1)
Dimension q
MatrixWof dimension q×q
#Initialization
ConstructHk(xn+1)for eachk= 1,...,d.
Constructaki,bkifor alli= 1,...,n + 1.
#Constructp-value change-point sets
fori∈1,...,ndo
Generate matrix M∗
i≡Mn+1−MiwithMn+1andMiconstructed as in Equation (24).
GenerateEi={z: (1,z)∗
i(1,z)≤0}
end for
Return:E={Ei}n
i=1
0 5 10 15 20 2520 30 40 50 60 70 80
slumpflow
0 5 10 15 20 2520 30 40 50 60 70 80
slumpflow
Figure 4:p-value change-point sets in two-dimensions for observations from single cementtest data point
withℓ1conformity measure constructed using Algorithm 1 (left), and ||·||2
Wconformity measure with W=I2
(middle) constructed using Algorithm 2.
3.4 Sampling points in the boundary
To cope with higher-dimensional complexity, we now aim to sample points on the boundary of the conformal
prediction set. We can describe our results more generally by finding roots to Sn+1(z)−Si(z)where
z(t,d) =z0+td (26)
forsomedirectionvector d∈Rqandsomeinteriorpoint z0. Then, findingrootsislimitedtofinding t∗suchthat
t∗={t>0 :Sn+1(z(t,d))−Si(z(t,d)) = 0}.
12Under review as submission to TMLR
Restricting our models to linear predictors that follow Equation (3) in one direction of the space is equivalent
to restricting the observed output. As such, we have
ˆy(z(t,d) =Hy(z(t,d)) =H(y1,...,yn,z(t,d))⊤
=Hy(0) +H(0,..., 0,z0+td)⊤
=Hy(z0) +tH(0,..., 0,d)⊤
The conformity measures along the direction dare then given by
Si(z(t,d)) =∥yi−ˆyi(z(t,d))∥=∥ai−tbi∥
Sn+1(z(t,d)) =∥z(t,d)−ˆyn+1(z(t,d))∥=∥an+1−tbn+1∥,
where we define
ai=yi−(Hy(z0))i, a n+1=z0−(Hy(z0))n+1
bi=Hn+1d, b n+1=d−Hn+1d
The goal is now to solve the one dimension problem ψ(t) =Sn+1(z(t,d))−Si(z(t,d))≥0. Without this one
dimensional restriction, the computations are significantly more difficult and impossible to track without
stronger data assumptions. This is illustrated in Figure 2 where we provide simple examples that lead to a
non-convex set of solutions. For completeness, we describe below the solution for different norms to be used
as score functions and explicit form of the conformal set for a given direction.
Solving for ℓ1normWe have
ψ(t) =∥an+1−tbn+1∥1−∥ai−tbi∥1
=⟨sign(an+1−tbn+1),an+1−tbn+1⟩−⟨sign(ai−tbi),ai−tbi⟩
=c(t) +ts(t)
where we can easily see that ψis piecewise linear with slopes s(t)and baisc(t)defined as
s(t) =⟨sign(ai−tbi),bi⟩−⟨sign(an+1−tbn+1),bn+1⟩
c(t) =⟨sign(an+1−tbn+1),an+1⟩−⟨sign(ai−tbi),ai⟩
Every piece is characterized by the moment where the sign terms change i.e.,when for a coordinate j∈[q],
it holds
ai,j−tbi,j= 0oran+1,j−tbn+1,j= 0 (27)
Without loss of generality, let us assume that bi,jandbn+1,jare non zero; otherwise, the equation does not
have a solution and we can skip them. Also let us assume that ai,jandan+1,jare non zero, otherwise the
solution is trivial equal to zero. As such, we have 2qsolutions of Equation (27) that we denote
t⋆
1,...,t⋆
2q
By the Intermediate Value Theorem, the function t∝⇕⊣√∫⊔≀→ψ(t)is equal to zero if and only if it exists consecutive
rootst⋆
k,t⋆
k+1for whichψtakes opposite sign i.e.,ψ(t⋆
k)ψ(t⋆
k+1)≤0(note that the product is equal to zero
only at the roots). Then, we deduce that Ei≡{z∈Rq:Sn+1(z)≤Si(z)}restricted on the line z0+tdis a
union of intervals whose boundaries are delimited by the roots of ψthat are easily obtained as
ˆtk=−c(t⋆
k+1)
s(t⋆
k+1)orˆtk=−c(t⋆
k)
s(t⋆
k).
Solving for general Mahalanobis score function By monotonicity, squaring the norm in the definition
ofψdoes not change its level set. So, let’s consider the function
ψ(t) =∥an+1−tbn+1∥2
M−∥ai−tbi∥2
M
=t2(∥bn+1∥2
M−∥bi∥2
M)−2t(⟨an+1,bn+1⟩M−⟨ai,bi⟩M) + (∥an+1∥2
M−∥ai∥2
M)
where we denote ∥α∥2
M=⟨α,α⟩Mand⟨α,β⟩M=⟨α,Mβ⟩. Hence the root of ψare obtained by merely
solving a quadratic equation.
13Under review as submission to TMLR
Explicit description of the conformal set For anyiin[n+ 1], we denote the intersection points of the
functionsSi(zt)andSn+1(zt)bytis and then we have Eican be an interval (possibly a point), a union of
interval or even empty. In all cases, it is characterized by the intersection points obtained explicitly. Since
π(z(t,d))is piecewise constant, it changes only at those points. We denote the set of solutions t1,···,tK
in increasing order as t0< t1<···< tK. Whence for any t, it exists a unique index j=J(t)such that
t∈(tj,tj+1)ort∈{tj,tj+1}and for any t, we have
(n+ 1)π(zt) =n+1/summationdisplay
i=11t∈Si=N(J(t)) +M(J(t))
where the functions
N(j) =n+1/summationdisplay
i=11(tj,tj+1)⊂SiandM(j) =n+1/summationdisplay
i=11tj∈Si
Note thatJ−1(j) = (tj,tj+1)orJ−1(j) ={tj,tj+1}. Finally, we have the restriction of the conformal set to
the direction dis given by
Γ(α)(xn+1,d) =/uniondisplay
j∈[K]
N(j)>(n+1)α(tj,tj+1)∪/uniondisplay
j∈[K]
M(j)>(n+1)α{tj}. (28)
4 Approximate Conformal Inference for Multi-task Learning
While the results in Section 3 allows for the construction of exact p-values with no additional model refitting
(for multiple responses), we still cannot describe exactly the conformal prediction sets in closed-form. Thus,
we aim to construct approximations of the conformal prediction set for a given xn+1. In this section we
specifically introduce a union-based approximation for a conformal prediction set generated using the results
from Section 2.1.2. Additionally, we extend the root-based approximation procedures introduced in Ndiaye &
Takeuchi (2021) to the multi-task setting.
4.1 unionCPApproximation Method
After constructing the set Efor an incoming point xn+1, it is initially unclear which regions Eimake up
various conformal prediction sets, let alone how we need to combine these regions to get the exact conformal
prediction sets. Thus, we aim to provide an approximation of conformal prediction sets using the regions
generated with the approaches introduced in Section 3. We provide Proposition 4 to bound error probabilities
associated with potential combinations of these regions.
Proposition 4. Under uniqueness of conformity measures, for some y∈Rqsuch that
(x1,y1),..., (xn+1,yn+1)are drawn exchangeably from P, for anyS⊂[n]
P/parenleftig
y∈/uniondisplay
i∈SEi/parenrightig
≥|S|
n+ 1.
Proof.Assume we have the data pair (x,y)drawn exchangeably with (x1,y1),..., (xn,yn). Also assume that
we have constructed the set E. In Section 3, we show construction of Ewithℓ1and||·||2
Was conformity
measures, but the following proof holds for any conformity measure. First, we select and fix any S⊆[n]. We
then fix azsuch thatz /∈/uniontext
i∈SEi. Then,
z /∈/uniondisplay
i∈SEi⇐⇒Si(z)≤Sn+1(z)∀i∈S=⇒n+1/summationdisplay
i=11{Si(z)≤Sn+1(z)}≥|S| + 1
14Under review as submission to TMLR
Then, fory
P/parenleftig
y /∈/uniondisplay
i∈SEi/parenrightig
≤P/parenleftign+1/summationdisplay
i=11{Si(y)≤Sn+1(y)}≥|S| + 1/parenrightig
⇒P/parenleftig
y∈/uniondisplay
i∈SEi/parenrightig
≥1−P/parenleftign+1/summationdisplay
i=11{Si(y)≤Sn+1(y)}≥|S| + 1/parenrightig
≥P/parenleftign+1/summationdisplay
i=11{Si(y)≤Sn+1(y)}≤|S|/parenrightig
.
By Lemma 1, with the selection of α= 1−|S|
n+1,
P/parenleftign+1/summationdisplay
i=11{Si(y)≤Sn+1(y)}≤|S|/parenrightig
≥|S|
n+ 1.
Proposition 4 states that with the selection of anysubset ofE, the probability of the response yn+1being
contained in the union of that subset is bounded-below by a function of cardinality. For example, if we wish
to construct, say, a conservative 50% prediction set, we could select (at random) a set S⊆Esuch that
|S|≥|E|/2; the union of all sets within Swould provide a conservative prediction set. We again note that
while our work emphasizes ℓ1and||·||2
W, Proposition 4 holds for any conformity measure.
Now, the random set constructed might not provide tight coverage as there exist some Eisuch that
/uniondisplay
i′∈S(i)Ei′⊆Ei,
whereS(i)is some subset of [n]that does not contain i; somep-value change-point sets are contained in
others and, thus, choosing the larger set could result in extremely conservative coverage. We include results
related to the theoretical coverage associated with the randomized approach in Supplemental Materials.
While the union of a random selection of regions forms a conservative 1−|S|/(n+ 1)prediction set, we can
provide more intelligently constructed sets that are empirically less conservative (but still valid). Suppose we
provide an ordering of our regions, where E(k)is defined as the k-th smallest region by volume.
Definition 1 (unionCP).A more efficient (1−α)prediction set approximation can then be constructed as
ˆΓ(α)(xn+1) =/uniondisplay
i∈S1−αE(i), (29)
whereS1−α= [⌈(1−α)(n+ 1)⌉]. We dub the approximation shown in Equation (29)asunionCP.
By Proposition 4, unionCP generates an approximation that, at minimum, provides a region that is at least
valid. We compare prediction sets constructed using unionCP to a random selection of regions for multiple
predictors in Section 5. We find empirically that sets constructed using unionCP are less conservative than a
random collection of p-value change-point sets.
While Proposition 4 and the adjustment described in Equation (29) allow for conservative prediction sets, at
times, the union of various Eidoes not explicitly describe a conformal prediction set exactly. Thus, unionCP
provides (at worse) a conservative approximation of the true conformal prediction set. Figure 5 provides an
example where the region constructed with unionCP differs from the true conformal prediction set.
With full comformal prediction, the computational complexity depends heavily on the number of candidate
values chosen, while the computational burden of unionCP depends on the number of observations n. To
reduce the computation required to generate the approximation, we can utilize the result shown in Lemma 2.
15Under review as submission to TMLR
0 5 10 15 20 2520 30 40 50 60 70 80
slumpflow
Figure 5: Comparison of full conformal prediction set for α=.25(red line) and regions included in unionCP
approximation (black line(s)).
Algorithm 3 unionCP
Input:dataDn={(x1,y1),..., (xn,yn)}, andxn+1
Coverage level α∈(0,1)
Subset size m≤n
#Initialization
Generate a random subset M⊆[n]of sizem.
ConstructEm={Ei}i∈Mwith Algorithm 1 or Algorithm 2.
#Construct conformal prediction region approximation
Order each element of Emby volume where Em
(k)is defined as the k-th smallest region by volume of Em.
GenerateS1−α={1,...,⌈(1−α)(m+ 1)⌉}.
SetˆΓ(α)(xn+1) =/uniontext
i∈S1−αEm
(i).
Return: ˆΓ(α)(xn+1)
Lemma 2. LetU1,...,Un,Un+1be an exchangeable sequence of random variables. Then, any subsample
U1,...,Umis also exchangeable.
By Lemma 2, we can randomly select any mobservations, where 1<m≤n, and the conformity measures of
this subset, along with Sn+1(z), will also be exchangeable. Thus, we can randomly select a subset of Eof size
m, defined asEm, and then order this subset by volume, where Em
(k)is defined as the k-th smallest region
by volume of the set Em. Then, by Proposition 4, unionCP constructed with this subset also provides valid
prediction regions, at a potentially much lower computational cost.
If we wish to avoid the unionCP approximation, we can generate exact p-values using Equation (11) in
conjunction with a grid-based approach with much computational gain over that of full conformal prediction.
We include Algorithm 3 to construct a generalized version of the unionCP approximation of the conformal
prediction set for a given test observation.
16Under review as submission to TMLR
4.2 Connection between unionCPand splitCP
In this section, we provide further theoretical backing for unionCP by connecting it explicitly to splitCP.
First, we can further generalize the conformal prediction set generated when using splitCP than just with
the use of the absolute residual. In general, for an incoming xn+1, the split conformal prediction set is
Γ(α)
split(xn+1) ={z:Sn+1(z)≤s}, (30)
whereSn+1(z)is constructed as a function of zandˆyn+1, generated using observations in I1, andsis the
⌈(|I2|+ 1)(1−α)⌉-th largest conformity measure for observations in I2. In one dimension, it is easy to show
that with the absolute residual |z−ˆyn+1|, Equation (30) reduces to the region shown in Equation (8). For
y∈Rq, when using||·||2
Was the conformity measure,
Sn+1(z)≤s⇒||z−ˆyn+1||2
W≤s
We also note that for splitCP, we can construct the p-value change-point set for observation iwhen using
||·||2
Was
Ei≡{z:Sn+1(z)≤Si(z)}={z:||z−ˆyn+1||2
W≤||yi−ˆyi||2
W}.
Now, if we select two observations iandjsuch thatSi(z)≤Sj(z)then the result in Proposition 5 holds.
Proposition 5. For two observations iandjsuch thatSi(z)≤Sj(z)∀z,
Si(z)≤Sj(z)⇒Ei⊆Ej
Proof.We assume Si(z)≤Sj(z)∀z. We note this this assumption is valid for splitCP as the conformity
measure for each observation is constant with respect to z. Thus, we can provide an ordering of the conformity
measures. Then,
Ej={z:Sn+1(z)≤Sj(z)}
={z:Sn+1(z)≤Sj(z) +Si(z)−Si(z)}
={z:Sn+1(z)≤Si(z) +Sj(z)−Si(z)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≥0by assumption}
=Ei∪{z:Si(z)≤z≤Sj(z)}
Thus, with the unionCP approach, we can match exactly the conformal prediction sets constructed using
splitCP. We note that this result is related to the nestedconformal prediction sets discussed in Gupta et al.
(2022).
4.3 Root-based Approximation Methods
As noted earlier, computation of the conformal prediction sets requires model readjustment for any candidate
value to replace the true yn+1value. Current efficient approaches to exact computation, limited to dimension
one, are restricted to models that are piecewise-linear; this structure allows to track changes in the conformity
function. We have extended these approaches to higher dimensions in the previous section.
17Under review as submission to TMLR
−30−20−10 0−70−60−50−40−30−20 Convex Hull
Fitted Ellipse
Targetyn+1
(a)3search directions
−30−25−20−15−10−5 0−60−50−40−30−20 (b)5search directions
−30−25−20−15−10−5 0−60−50−40−30−20
(c)10search directions
−30−20−10 0−60−50−40−30−20 (d)30search directions
Figure 6: Illustration of the approximated conformal prediction set obtained fitting ellipse and convex hull
given boundary points obtained by rootCP. We use scikit-learn make_regression to generate synthetic
dataset with the parameters n_samples = 15, n_features = 5, n_targets = 2is the dimension of in output
yn+1. We selected 80%of informative features and 60%for effective rank (described as the approximate
number of singular vectors required to explain most of the input data by linear combinations) and the
standard deviation of the random noise is set to 5.
To go beyond linear structures, we can use approximate homotopy approaches which, given an optimization
tolerance, provide a discretization of all the values that yn+1can take. However, these approaches are
also limited in dimension one and have an exponential complexity in the dimension of yn+1. Convexity
assumptions are also required, which, unfortunately, are not verified for more complex prediction models.
In this section, we extend the approximations of conformal prediction in multiple dimensions by computing
conformal prediction set boundaries directly. Unlike the one-dimensional case where the boundary is often two
points, in multiple dimensions the boundary is continuous and, thus, uncountable, which makes finite-time
computation impossible.
To get around this difficulty, the main idea here is very simple. We will first fix a finite set of search directions;
we will estimate the intersection points between the boundary of the conformal prediction set and the chosen
direction. Then, we use the points on the boundary as a data base to fit a convex approximation, e.g.,an
18Under review as submission to TMLR
−1.0−0.5 0.0 0.5 1.0 1.5−1.5−1.0−0.50.00.51.01.5p= 1
−1.0−0.5 0.0 0.5 1.0 1.5−1.5−1.0−0.50.00.51.01.5p= 1.3
−1.0−0.5 0.0 0.5 1.0 1.5−1.5−1.0−0.50.00.51.0p= 2
−0.5 0.0 0.5 1.0−1.0−0.50.00.5p=∞
Figure 7: Illustration of the approximated Eiwith 30search directions with conformity measures defined
withℓpnorms. Note that the different level sets can actually overlap. Solid black lines denote ellipsoid
approximations of Eiusing calculated boundary points.
ellipse or the convex hull, passing through these points. More formally, we want to estimate (efficiently) the
set described in Equation (7).
Assumptions. We suppose that the conformal prediction set is star-shaped i.e., there exists a point z0
such that any other point zwithin Γ(α)(xn+1)can be connected to z0with a line segment.
Note that a star-shaped set can be non-convex; we can still approximate complex, e.g., non-convex, conformal
sets. We provide some illustration in Figure 6. We also note that ellipsoidal sets (or any convex set) are
inherently star-shaped.
Outline of rootCP
Givenanydirection d∈Rq, theintersectionpointsbetweentheboundaryof Γ(α)(xn+1), definedas ∂Γ(α)(xn+1),
and the line passing through z0and directed by dare obtained by solving the one dimensional equation
π(z(t,d)/parenrightbig
= 1−α, (31)
wherez(t,d)is as in Equation (26). We briefly described the main steps and display the detail in Algorithm 4.
19Under review as submission to TMLR
1. Fit a model µ0on the observed training set Dnand predict a feasible point z0=µ0(xn+1).
2.For a collection of search directions {d1,...,dK}, perform a bisection search in [tmin,0]and[0,tmax]
to output solutions ˆℓ(dk)andˆu(dk)of Equation (31) at direction dk, after at most log2(tmax−tmin
ϵr)
iterations for an optimization tolerance ϵr>0. Notice that the star-shape assumption implies that
we will have only two roots on the selected directions.
3.Fit a convex set on the roots obtained at the previous step {ˆℓ(dk),ˆu(dk)}k∈[K]. In practice, when one
uses a least-squares ellipse as the convex approximation, a number of search directions Kproportional
to the dimension qof the target yn+1is sufficient. This is not necessarily the case for the convex hull.
We refer to Figure 6 where we observe that many more search directions are needed to cover the
conformal set when using the convex hull approximation.
Algorithm 4 rootCPin multiple dimensions
Input:dataDn={(x1,y1),..., (xn,yn)}, andxn+1
Coverage level α∈(0,1), accuracyϵr>0, list of search directions d1,...,dK
#Initialization
Setz0=µ0(xn+1)where we fitted a model µ0on the initial training dataset Dn
#Approximation of boundary points
fork∈{1,...,K}do
We define the direction-wise conformity function as
πk(t) =π(z(t))−αwherez(t) =z0+tdk
1.t−=bisection_search (πk,tmin,0,ϵr)
2.t+=bisection_search (πk,0,tmax,ϵr)
Setˆℓdk=z0+t−dkandˆudk=z0+t+dk
end for
#Convex approximation
ˆΓ(xn+1) = ˜ co/braceleftig
ˆℓd1,ˆud1,..., ˆℓdK,ˆudK/bracerightig
where ˜ co/braceleftbig
S/bracerightbig
is a convex set built on S,e.g.,the convex hull or fit an ellipse using the points in S
Return: ˆΓ(xn+1)
The root-finding approach can also be adapted to unionCP by approximating the level-line boundary of the
Eiscore difference introduced in Equation (10). In so doing, the previous restriction to quadratic functions
that enabled an explicit construction is no longer necessary, at the cost of an approximation. We illustrate
this generalization to different score functions in Figure 7.
5 Empirical Results and Application
To provide empirical support for our theoretical results, we consider a small data example using the multi-task
cementdata set (Yeh, 2007). For the sake of simplicity, we limit our exploration to two dimensions, focusing
on the construction of prediction sets for slump and flow, given information on other elements in the mixture,
e.g.,amount of cement, fly ash and superplasticizer.
We include empirical coverage results for the approximation approaches described in Section 4, specifically
with regions constructed using ||·||2
W. We also include results for the root-based approximation results
20Under review as submission to TMLR
0 10 20 30 400.0 0.2 0.4 0.6 0.8 1.0
number of regions selectedempirical coverage
random theoretical
random−LR
unionCP−LR
unionCP − LC
unionCP−LL
calibrated
Figure 8: Comparison of empirical coverage with random selection of kregions and unionCP for various
predictors, including: linear regression (LR), Nadaraya-Watson (LC), and local-linear (LL) across 100
repetitions. The calibrated curve is constructed using number of regions selected /(m+ 1), wherem=n= 40.
described in Section 4.3 for a wide-array of predictors that include those more general than the restrictions
we outline in our paper.
As a reference benchmark, from Lemma 1, we have π(yn+1)≥αwith probability larger than 1−α. Hence,
we can define the oracleCP asπ−1([α,+∞))whereπis obtained with a model fit optimized on the oracle
dataDn+1(yn+1)on top of the root-based approach to find boundary points. We remind the reader that the
target variable yn+1is not available in practice.
5.1 unionCPApproximation Application
While our focus for the construction of H(xn+1,xi)has been general, for much of our discussion we often
referenceH(·)constructions associated with a ridge-regressor. Thus, in this section, we wish to demonstrate
the performance of unionCP with other methods that fall under our the general model restriction; we
specifically explore the prediction methods discussed in Section 3.1. We also note that there are additional
methods other than these which also adhere to our model restrictions.
Figure 3 includes a comparison of the p-value change-point regions constructed with Equation (21) to the
conformal prediction sets constructed using the grid-based approach, also with linear regression for each
predictor as well as coverage results for various predictors on the cement data set. From Figure 8, we can see
that the unionCP approximation, with each of the models, is empirically calibrated.
5.2 rootCPApproximation Application
We numerically examine the performance of rootCPon multi-task regression problems using both synthetic
and real databases. The experiments were conducted with a coverage level of 0.9,i.e.,α= 0.1.
For comparisons, we run the evaluations on 100repetitions of examples, and display the average of the
following performance statistics for different methods: 1) the empirical coverage, i.e.,the percentage of times
21Under review as submission to TMLR
oracleCP
cov= 0.83
T= 1.0splitCP
cov= 0.7
T= 0.76rootCP
cov= 0.8
T= 2.621.52.02.53.0Volume
(a) Multitask Enet
oracleCP
cov= 0.53
T= 1.0splitCP
cov= 0.53
T= 0.72rootCP
cov= 0.53
T= 49.116789Volume
 (b) MultiLayers Perceptron
oracleCP
cov= 0.93
T= 1.0splitCP
cov= 0.77
T= 0.74rootCP
cov= 0.93
T= 6.612.02.53.03.54.04.5Volume
 (c) Orthogonal Matching Pursuit
oracleCP
cov= 0.97
T= 1.0splitCP
cov= 0.8
T= 0.73rootCP
cov= 0.83
T= 7.376.06.57.07.58.0Volume
(d) (Linear) Kernel Ridge
oracleCP
cov= 0.76
T= 1.0splitCP
cov= 0.74
T= 0.65rootCP
cov= 0.76
T= 7.426.06.57.07.58.08.59.0Volume
 (e) (RBF) Kernel Ridge
oracleCP
cov= 0.79
T= 1.0splitCP
cov= 0.82
T= 0.67rootCP
cov= 0.79
T= 12.24234567Volume
 (f) Support Vector Regression
oracleCP
cov= 0.89
T= 1.0splitCP
cov= 0.87
T= 0.66rootCP
cov= 0.87
T= 8.0945678Volume
(g)K-NN
oracleCP
cov= 0.75
T= 1.0splitCP
cov= 0.74
T= 0.4rootCP
cov= 0.75
T= 60.157.07.58.08.59.09.5Volume
 (h) Quantile regression
Figure 9: Ellipse Benchmarking conformal sets for several regression models on cementdataset. We display
the volumes of the confidence sets over 100random permutations of the data. We denoted covthe average
coverage, and Tthe average computational time normalized with the average time for computing oracleCP
which requires a single model fit on the whole data.
the prediction set contains the held-out target yn+1, 2) the volume of the confidence intervals, and 3) the
execution time. For each run, we randomly select a data tuple (xi,yi)to constitute the targeted variables
for which we will compute the conformal prediction set. The rest is considered as observed data Dn. Similar
experimental settings are considered in Lei (2019).
We run experiments on a suite of complex regression models, including: multi-task elastic net, multi-layer
perceptron, orthogonal matching pursuit, kernel ridge regression with both linear and Gaussian kernels,
support vector regression, k-nearest neighbors and quantile regression. The results are shown in Figure 9.
We include additional results in Supplementary Materials.
6 Conclusion
Inthispaper, weintroducedexact p-valuesinmultipledimensionsforpredictorsthatarealinearfunctionofthe
candidatevalue. Specifically, wediscussedtheexactconstructionof p-valuesusingvariousconformitymeasures,
includingℓ1and||·||2
W. Additionally, we introduced methods for various approximations of multidimensional
22Under review as submission to TMLR
1−αconformal prediction sets through union-based and root-based prediction set construction, unionCP and
and a multi-task extension to rootCP, respectively. We also also deliver probabilistic bounds and convergence
results for these approximations. We then showed empirically with multiple predictors, including a subset
of both linear and nonlinear predictors, that these approximations were comparable to t gridCPsets, while
drastically reducing the computational requirements.
One drawback of the methods described in this work is their lack of adaptability. We hope to include region
adaptability in future work, e.g., with methods similar to those introduced in Messoudi et al. (2022).
Other questions about the theoretical guarantees of our approach have yet to be answered. For example,
we lack precise characterizations on the number of points to be sampled on the conformal set boundary, as
well as implications our convex approximations, e.g.,ellipse, convex hull, related to expected volume and
potential coverage loss in the worst case. Besides the conformal sets presented in this paper, these questions
are equally relevant to the construction of any high-dimensional confidence sets.
References
Jonathan Alvarsson, Staffan Arvidsson McShane, Ulf Norinder, and Ola Spjuth. Predicting with confidence:
using conformal prediction in drug discovery. Journal of Pharmaceutical Sciences , 110(1):42–49, 2021.
Anastasios N Angelopoulos, Stephen Bates, et al. Conformal prediction: A gentle introduction. Foundations
and Trends ®in Machine Learning , 16(4):494–591, 2023.
Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Predictive inference
with the jackknife+. The Annals of Statistics , 49(1):486–507, 2021.
Umang Bhatt, Adrian Weller, and Giovanni Cherubin. Fast conformal classification using influence functions.
InConformal and Probabilistic Prediction and Applications , pp. 303–305. PMLR, 2021.
Hanen Borchani, Gherardo Varando, Concha Bielza, and Pedro Larranaga. A survey on multi-output
regression. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery , 5(5):216–233, 2015.
David S Broomhead and David Lowe. Multivariable functional interpolation and adaptive networks, 1988.
Lars Carlsson, Martin Eklund, and Ulf Norinder. Aggregated conformal prediction. In IFIP International
Conference on Artificial Intelligence Applications and Innovations , pp. 231–240. Springer, 2014.
Leonardo Cella and Ryan Martin. Valid distribution-free inferential models for prediction. arXiv preprint
arXiv:2001.09225 , 2020.
Wenyu Chen, Zhaokai Wang, Wooseok Ha, and Rina Foygel Barber. Trimmed conformal prediction for
high-dimensional models. arXiv preprint arXiv:1611.09933 , 2016.
Giovanni Cherubin, Konstantinos Chatzikokolakis, and Martin Jaggi. Exact optimization of conformal
predictors via incremental and decremental learning. In International Conference on Machine Learning ,
pp. 1836–1845. PMLR, 2021.
Alex Contarino, Christine Shubert Kabban, Chancellor Johnstone, and Fairul Mohd-Zaid. Constructing
prediction intervals with neural networks: an empirical evaluation of bootstrapping and conformal inference
methods. arXiv preprint arXiv:2210.05354 , 2022.
IsidroCortés-CirianoandAndreasBender. Conceptsandapplicationsofconformalpredictionincomputational
drug discovery. arXiv preprint arXiv:1908.03569 , 2019.
Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on information
theory, 13(1):21–27, 1967.
Jacopo Diquigiovanni, Matteo Fontana, and Simone Vantini. Conformal prediction bands for multivariate
functional data. Journal of Multivariate Analysis , 189:104879, 2022.
23Under review as submission to TMLR
Martin Eklund, Ulf Norinder, Scott Boyer, and Lars Carlsson. The application of conformal prediction to the
drug discovery process. Annals of Mathematics and Artificial Intelligence , 74:117–132, 2015.
Jianqing Fan. Design-adaptive nonparametric regression. Journal of the American statistical Association , 87
(420):998–1004, 1992.
Evelyn Fix. Discriminatory analysis: nonparametric discrimination, consistency properties , volume 1. USAF
school of Aviation Medicine, 1985.
Matteo Fontana, Gianluca Zeni, and Simone Vantini. Conformal prediction: a unified review of theory and
new challenges. Bernoulli , 29(1):1–23, 2023.
Alexander Gammerman, Vladimir Vovk, and Vladimir Vapnik. Learning by transduction. In Proceedings of
the Fourteenth conference on Uncertainty in artificial intelligence , pp. 148–155, 1998.
Chirag Gupta, Arun K Kuchibhotla, and Aaditya Ramdas. Nested conformal prediction and quantile
out-of-bag ensemble methods. Pattern Recognition , 127:108496, 2022.
Chancellor Johnstone and Bruce Cox. Conformal uncertainty sets for robust optimization. In Conformal and
Probabilistic Prediction and Applications , pp. 72–90. PMLR, 2021.
Arun Kumar Kuchibhotla. Exchangeability, conformal prediction, and rank tests. arXiv preprint
arXiv:2005.06095 , 2020.
Alexander Kuleshov, Alexander Bernstein, and Evgeny Burnaev. Conformal prediction in manifold learning.
InConformal and Probabilistic Prediction and Applications , pp. 234–253, 2018.
Jing Lei. Fast exact conformalization of the lasso using piecewise linear homotopy. Biometrika , 106(4):
749–764, 2019.
Jing Lei, Alessandro Rinaldo, and Larry Wasserman. A conformal prediction approach to explore functional
data.Annals of Mathematics and Artificial Intelligence , 74(1):29–43, 2015.
Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free
predictive inference for regression. Journal of the American Statistical Association , 113(523):1094–1111,
2018.
Soundouss Messoudi, Sébastien Destercke, and Sylvain Rousseau. Conformal multi-target regression using
neural networks. In Conformal and Probabilistic Prediction and Applications , pp. 65–83. PMLR, 2020.
Soundouss Messoudi, Sébastien Destercke, and Sylvain Rousseau. Copula-based conformal prediction for
multi-target regression. arXiv preprint arXiv:2101.12002 , 2021.
Soundouss Messoudi, Sébastien Destercke, and Sylvain Rousseau. Ellipsoidal conformal inference for multi-
target regression. Proceedings of Machine Learning Research , 179:1–13, 2022.
Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications , 9(1):141–142, 1964.
Eugene Ndiaye. Stable conformal prediction sets. In International Conference on Machine Learning , pp.
16462–16479. PMLR, 2022.
Eugene Ndiaye and Ichiro Takeuchi. Computing full conformal prediction set with approximate homotopy.
arXiv preprint arXiv:1909.09365 , 2019.
Eugene Ndiaye and Ichiro Takeuchi. Root-finding approaches for computing conformal prediction set. arXiv
preprint arXiv:2104.06648 , 2021.
Jelmer Neeven and Evgueni Smirnov. Conformal stacked weather forecasting. In Conformal and Probabilistic
Prediction and Applications , pp. 220–233, 2018.
Ilia Nouretdinov, Thomas Melluish, and Volodya Vovk. Ridge regression confidence machine. In ICML, pp.
385–392. Citeseer, 2001.
24Under review as submission to TMLR
Henrik Olsson, Kimmo Kartasalo, Nita Mulliqi, Marco Capuccini, Pekka Ruusuvuori, Hemamali Samaratunga,
Brett Delahunt, Cecilia Lindskog, Emiel AM Janssen, Anders Blilie, et al. Estimating diagnostic uncertainty
in artificial intelligence assisted pathology using conformal prediction. Nature communications , 13(1):7761,
2022.
Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathematical
statistics , 33(3):1065–1076, 1962.
David W Scott. Multivariate density estimation: theory, practice, and visualization . John Wiley & Sons,
1992.
Vladimir Vovk. Cross-conformal predictors. Annals of Mathematics and Artificial Intelligence , 74(1):9–28,
2015.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic learning in a random world . Springer
Science & Business Media, 2005.
Devin Wasilefsky, William Caballero, Chancellor Johnstone, Nathan Gaw, and Phillip Jenkins. Responsible
machine learning for united states air force pilot candidate selection, 2023.
Geoffrey S Watson. Smooth regression analysis. Sankhy¯ a: The Indian Journal of Statistics, Series A , pp.
359–372, 1964.
Donna Xu, Yaxin Shi, Ivor W Tsang, Yew-Soon Ong, Chen Gong, and Xiaobo Shen. Survey on multi-output
learning. IEEE transactions on neural networks and learning systems , 31(7):2409–2429, 2019.
I-Cheng Yeh. Modeling slump flow of concrete using second-order regressions and artificial neural networks.
Cement and concrete composites , 29(6):474–480, 2007.
Yu Zhang and Qiang Yang. An overview of multi-task learning. National Science Review , 5(1):30–43, 2018.
25Under review as submission to TMLR
Supplementary Materials
In the following sections, we include additional content to further support the contributions of the paper.
S.1 Additional Experiments
oracleCP
cov= 0.77
T= 1.0splitCP
cov= 0.73
T= 0.76rootCP
cov= 0.77
T= 2.621.01.52.02.5Volume
(a) Multitask Enet
oracleCP
cov= 0.7
T= 1.0splitCP
cov= 0.53
T= 0.72rootCP
cov= 0.7
T= 49.114.55.05.56.06.57.07.5Volume
 (b) MultiLayers Perceptron
oracleCP
cov= 0.8
T= 1.0splitCP
cov= 0.7
T= 0.74rootCP
cov= 0.8
T= 6.611.52.02.53.03.5Volume
 (c) Orthogonal Matching Pursuit
oracleCP
cov= 0.93
T= 1.0splitCP
cov= 0.87
T= 0.73rootCP
cov= 0.93
T= 7.375.05.56.06.5Volume
(d) (Linear) Kernel Ridge
oracleCP
cov= 0.76
T= 1.0splitCP
cov= 0.73
T= 0.65rootCP
cov= 0.76
T= 7.425.05.56.06.57.0Volume
 (e) (RBF) Kernel Ridge
oracleCP
cov= 0.78
T= 1.0splitCP
cov= 0.81
T= 0.67rootCP
cov= 0.76
T= 12.24123456Volume
 (f) Support Vector Regression
oracleCP
cov= 0.85
T= 1.0splitCP
cov= 0.88
T= 0.66rootCP
cov= 0.87
T= 8.0934567Volume
(g)K-NN
oracleCP
cov= 0.67
T= 1.0splitCP
cov= 0.66
T= 0.4rootCP
cov= 0.67
T= 60.155.56.06.57.07.5Volume
 (h) Quantile regression
Figure S1: Benchmarking the convex Hull based conformal sets for several regression models on cement
dataset. We display the lengths of the confidence sets over 100random permutation of the data. We denoted
covthe average coverage, and Tthe average computational time normalized with the average time for
computing oracleCP which requires a single model fit on the whole data.
26Under review as submission to TMLR
oracleCP
cov= 0.89
T= 1.0splitCP
cov= 0.92
T= 0.74rootCP
cov= 0.89
T= 2.470.51.01.52.02.5Volume
(a) Multitask Enet
oracleCP
cov= 0.75
T= 1.0splitCP
cov= 0.78
T= 0.35rootCP
cov= 0.75
T= 82.779101112Volume
 (b) MultiLayers Perceptron
oracleCP
cov= 0.9
T= 1.0splitCP
cov= 0.87
T= 0.69rootCP
cov= 0.9
T= 15.620.51.01.52.02.5Volume
 (c) Orthogonal Matching Pursuit
oracleCP
cov= 0.71
T= 1.0splitCP
cov= 0.72
T= 0.65rootCP
cov= 0.73
T= 5.3434567Volume
(d) (Linear) Kernel Ridge
oracleCP
cov= 0.79
T= 1.0splitCP
cov= 0.76
T= 0.66rootCP
cov= 0.79
T= 6.63910111213Volume
 (e) (RBF) Kernel Ridge
oracleCP
cov= 0.83
T= 1.0splitCP
cov= 0.75
T= 0.5rootCP
cov= 0.81
T= 26.912468Volume
 (f) Support Vector Regression
oracleCP
cov= 0.67
T= 1.0splitCP
cov= 0.69
T= 0.68rootCP
cov= 0.7
T= 7.78678910Volume
(g)K-NN
Figure S2: Benchmarking the ellipse based conformal sets for several regression models on synthetic dataset.
We display the lengths of the confidence sets over 100random permutation of the data. We denoted cov
the average coverage, and Tthe average computational time normalized with the average time for computing
oracleCP which requires a single model fit on the whole data.
27Under review as submission to TMLR
oracleCP
cov= 0.84
T= 1.0splitCP
cov= 0.86
T= 0.74rootCP
cov= 0.84
T= 2.470.51.01.52.0Volume
(a) Multitask Enet
oracleCP
cov= 0.82
T= 1.0splitCP
cov= 0.82
T= 0.35rootCP
cov= 0.82
T= 82.776.06.57.07.58.08.5Volume
 (b) MultiLayers Perceptron
oracleCP
cov= 0.85
T= 1.0splitCP
cov= 0.79
T= 0.69rootCP
cov= 0.85
T= 15.620.51.01.52.0Volume
 (c) Orthogonal Matching Pursuit
oracleCP
cov= 0.84
T= 1.0splitCP
cov= 0.8
T= 0.65rootCP
cov= 0.84
T= 5.342345Volume
(d) (Linear) Kernel Ridge
oracleCP
cov= 0.82
T= 1.0splitCP
cov= 0.81
T= 0.66rootCP
cov= 0.82
T= 6.636789Volume
 (e) (RBF) Kernel Ridge
oracleCP
cov= 0.86
T= 1.0splitCP
cov= 0.75
T= 0.5rootCP
cov= 0.8
T= 26.91123456Volume
 (f) Support Vector Regression
oracleCP
cov= 0.71
T= 1.0splitCP
cov= 0.72
T= 0.68rootCP
cov= 0.68
T= 7.784567Volume
(g)K-NN
Figure S3: Benchmarking the convex Hull based conformal sets for several regression models on synthetic
dataset. We display the lengths of the confidence sets over 100random permutation of the data. We denoted
covthe average coverage, and Tthe average computational time normalized with the average time for
computing oracleCP which requires a single model fit on the whole data.
28Under review as submission to TMLR
S.2 Sketch-Proof of Random Region Selection Coverage Probability
We now include a sketch-proof for the probability that a randomly selected subset of E, of cardinality k,
constructed with Dn+1(z)will contain yn+1.
Proof.First, we define Γkas a random subset of Esuch that|Γk|=kwhere each region i, constructed
as a function of Dn+1(z), is selected with probability 1/n. We assume, without loss of generality, that
E(i)⊂E(i+1)∀i= 1,...,n. While this assumption does not always hold, it results in a larger upper bound.
Then, foryn+1,
P/parenleftbig
yn+1∈E(k)/parenrightbig
=k
n+ 1(32)
Furthermore, let the random variable ei= 1if regionE(i)is selected (with probability 1/n). Then, for k= 1,
P(yn+1∈Γ1) =n/summationdisplay
i=1P(yn+1∈Γ1,ei= 1)
=n/summationdisplay
i=1P(yn+1∈Γ1|ei= 1)P(ei= 1)
=1
nn/summationdisplay
i=1P(yn+1∈Γ1|ei= 1)
=1
nn/summationdisplay
i=1i
n+ 1
/parenleftbig
by Equation (32)/parenrightbig
=1
n(n+ 1)n/summationdisplay
i=1i
=1
n(n+ 1)n(n+ 1)
2
=1
2
Now consider Γkfork>1.
We define the random variable πkas a joint random variable for (e1,...,en)where the support of πkisΠk,
the set of all 2ncombinations of ei∈{0,1}. Because we limit our regions to be nested, the k-th region is
contained in (k+ 1)-th region. Thus, we can describe coverage probabilities with various Γkandπkby just
29Under review as submission to TMLR
examining the highest ksuch thatek= 1, defined as max
k(πk). Then,
P(yn+1∈Γk) =/summationdisplay
πk∈ΠkP(yn+1∈Γk,πk)
=/summationdisplay
πk∈ΠkP(yn+1∈Γk|πk)P(πk)
=n/summationdisplay
i=kP(yn+1∈Γk|max
k(πk) =i)P(max
k(πk) =i)
/parenleftbig
P(max
k(πk) =i) = 0∀i<k/parenrightbig
≥n/summationdisplay
i=ki
n+ 1P(max
k(πk) =i)
(by Lemma 1)
=1
n+ 1n/summationdisplay
i=kiP(max
k(πk) =i)
where
P/parenleftbig
max
k(πk) =i/parenrightbig
=/parenleftbigi−1
k−1/parenrightbig
/parenleftbign
k/parenrightbig.
S.3 Reproducibility
The source code utilized for our experimentation will be available in open-source later.
S.4 Discussion on Algorithm 1
There are several potential solutions to Equation (20); some of these solutions are invalid. In order to solve
for valid solutions, we can find intervals where the argument of each absolute value component is less than
zero. Without loss of generality, with two-dimensions,
a2i+b2iz2<0⇒z2<−a2i
b2ianda2n+1+b2n+1z2<0⇒z2<−a2n+1
b2n+1. (33)
Then, we can construct a set of intervals
(−∞,li),(li,ui),(ui,∞)
to check for solutions, where li=min(−a2i
b2i,−a2n+1
b2n+1)andui=max(−a2i
b2i,−a2n+1
b2n+1). The left-most interval
corresponds to both the arguments within Equation (20) being negative; the right-most interval corresponds
to both the arguments within Equation (20) being positive, resulting in
z2(i) =c1+ (−a2i+a2n+1)
b2i−b2n+1andz2(i) =c1+ (a2i−a2n+1)
b2n+1−b2i,
respectively. We explicitly denote z2(i)as a function of the index ibecause we must repeat this process for
each observation. For simplicity, we drop the iindex. The sign of the components when z2is contained
within the inner interval depends on the value of lianduiwhere
30Under review as submission to TMLR
li=−a2i
b2i⇒left component is positive, right component is negative.
otherwise⇒right component is positive, left component is negative.
Regardless, we can find the two valid solutions by checking all four potential solutions to see if they fall
within their respective intervals. We denote these two valid solutions zl
2andzu
2, corresponding to the smallest
and largest solution value, respectively. We can repeat the above process, instead fixing z2=˜z2to find
equivalent solutions for z1, denotedzl
2andzu
2. The points (˜z1,zl
2),(˜z1,zu
2),(zl
1,˜z2)and(zu
1,˜z2), all generated
as a function of i, make up the “corners” of each region Ei.
31