Under review as submission to TMLR
When Less is More: Simplifying Inputs Aids Neural Network
Understanding
Anonymous authors
Paper under double-blind review
Abstract
How do neural network image classiﬁers respond to simpler and simpler inputs? And what
do such responses reveal about the characteristics of the data and their interaction with
the learning process? To answer these questions, we need measures of input simplicity (or
inversely, complexity) and class-discriminative input information as well as a framework
to optimize these during training. Lastly, we need experiments that evaluate whether this
framework can simplify data to remove injected distractors and evaluate the impact of such
simpliﬁcation on real-world data. In this work, we measure simplicity with the encoding bit
size given by a pretrained generative model, and minimize the bit size to simplify inputs
during training. At the same time, we minimize a gradient- and activation-based distance
metricbetweenoriginalandsimpliﬁedinputstoretaindiscriminativeinformation. Weinves-
tigate the trade-oﬀ between input simplicity and task performance. For images with injected
distractors, such simpliﬁcation naturally removes superﬂuous information. For real-world
datasets, qualitative analysis suggests the simpliﬁed images retain visually discriminative
features and quantitative analysis show they retain features that may be more robust in
some settings.
1 Introduction
A better understanding of the information deep neural networks use to learn can lead to new scientiﬁc
discoveries (Raghu & Schmidt, 2020), highlight diﬀerences between human and model behaviors (Makino
et al., 2022) and serve as powerful auditing tools (Geirhos et al., 2020; D’souza et al., 2021; Bastings et al.,
2021; Agarwal et al., 2021).
Removing information from the input deliberately is one way to illuminate what information content is
relevant for learning. For example, occluding speciﬁc regions, or removing certain frequency ranges from the
input gives insight into which input regions and frequency ranges are relevant for the network’s prediction
(Zintgraf et al., 2017; Makino et al., 2022; Banerjee et al., 2021a). These ablation techniques use simple
heuristics such as random removal (Hooker et al., 2019; Madsen et al., 2021), or exploit domain knowledge
about interpretable aspects of the input to create simpler versions of the input on which the network’s
prediction is analyzed (Banerjee et al., 2021b).
What if, instead of using heuristics, one learnsto synthesize simpler inputs that retain task-relevant in-
formation? This way, we could gain intuition into the model behavior without relying on prior or domain
knowledge about what input content may be relevant for the network’s learning target. To achieve this,
one needs to deﬁne the precise meaning of “simplifying an input” and "retaining task-relevant information",
including metrics for simplicity and retention of task-relevant information.
In this work, we propose SimpleBits , an information-reduction method that learns to synthesize simpliﬁed
inputs which contain less information while remaining informative for the task. To measure simplicity, we use
a ﬁnding initially reported as a problem for density-based anomaly detection—that generative image models
tend to assign higher probability densities and hence lower bits to visually simpler inputs (Kirichenko et al.,
2020; Schirrmeister et al., 2020). Here, we use this to our advantage and minimize the encoding bit size
given by a generative network trained on a general image distribution to simplify inputs. To measure the
1Under review as submission to TMLR
retention of task-relevant information, we analyze several activation- and gradient-based metrics for input
similarity. Our analysis shows that gradient-weighted activation diﬀerences between the network activations
of the simpliﬁed and the original input are a suitable metric.
Sample
batchesSimpleBits
Train classifier
Original
dataset
Simplify by
removing
information
Retain task-
discrimative
information
= SimpleBits+
Figure 1: SimpleBits overview. Top: SimpleBits simpliﬁes images by removing information as measured by a
generative model while also retaining task-discriminative information, see Section 2 and 3. Wheel of the car
remains visible after SimpleBits simpliﬁcation. Bottom: We apply SimpleBits as a per-instance simpliﬁer
during training, where each image is simpliﬁed but the total number of images remain the same.
1.6 1.8 2.0 2.2 2.4 2.6 2.8
Bits per dimension7080Accuracy [%]
Results for Instance Simplification on CIFAR10
Figure 2: Examples of SimpleBits applied to CIFAR10 training images. Simpliﬁed images with varying
degree of simplication by varying upper bound on Ltask, see Section 4. We observe that at lower bits per
dimension, only few discriminative features remain in the image.
We apply SimpleBits in aper-instance setting, where each image is processed to be a simpliﬁed version of
itself during training, see Figure 1. We use a variety of synthetic and real-world image datasets to evaluate
2Under review as submission to TMLR
the eﬀect of SimpleBits on the network behavior and on the simpliﬁed input data. Our synthetic datasets
contain injected noise distractors that SimpleBits is supposed to remove, allowing a controlled evaluation of
theSimpleBits framework. For real-world datasets, we analyze the tradeoﬀ between simpliﬁcation and task
performance and the characteristics of the resulting simpliﬁed images, see Figure 2 for CIFAR10 examples.
Our evaluation provides the following insights:
1.Successful distractor removal. SimpleBits successfully removes superﬂuous information for tasks
with injected distractors.
2.Visually task-speciﬁc simpliﬁed images. On natural image datasets, qualitative analysis sug-
gestsSimpleBits retains plausible task-relevant information across all datasets we tried.
3.Dataset-dependent simpliﬁcation eﬀects on accuracy. Increasing simpliﬁcation leads to ac-
curacy decreases, as expected, and we report the trade-oﬀ between input simpliﬁcation and task
performance for diﬀerent datasets and problems.
4.SimpleBits learns to represent robust features in the simpliﬁed images. Evaluations
on CIFAR10-C, which consists of the CIFAR10 evaluation set with various corruptions applied
(Hendrycks & Dietterich, 2019), suggest that SimpleBits retains robust features.
Figure 3: Visualization of the bits-per-dimension ( bpd) measure for image complexity, sorted from low to
high. Image samples are taken from MNIST, Fashion-MNIST, CIFAR10 and CIFAR100, in addition to a
completely black image sample. bpdis calculated from the density produced by a Glow (Kingma & Dhariwal,
2018) model pretrained on 80 Million Tiny Images. Switching to other types of generative models including
PixelCNN and diﬀusion models trained on other datasets produces consistent observations; see Figure S1 in
Supplementary Information for more details.
2 Measuring and Reducing Instance Complexity
Howtodeﬁnesimplicity? Weusethefactthatgenerativeimagemodelstendtoassignlowerencodingbitsizes
to visually simpler inputs (Kirichenko et al., 2020; Schirrmeister et al., 2020). Concretely, the complexity of
an image xcan be quantiﬁed as the negative log probability mass given by a pretrained generative model
with tractable likelihood, G, i.e.−logpG(x).−logpG(x)can be interpreted as the image encoding size in
bits per dimension (bpd) via Shannon’s theorem (Shannon, 1948): bpd (x) =−log2pG(x)/dwheredis the
dimension of the ﬂattened x.
The simpliﬁcation loss for an input x, given a pre-trained generative model G, is as follows:
Lsim(x) =−logpG(x) (1)
Figure 3 visualizes images and their corresponding bits-per-dimension ( bpd) values given by a Glow network
(Kingma & Dhariwal, 2018) trained on 80 Million Tiny Images (Torralba et al., 2008) (see Supplementary
Section S1 for other models). This is the generative network used across all our experiments. A visual
inspection of Figure 3 suggests that lower bpdcorresponds with simpler inputs, as also noted in prior
work (Serrà et al., 2020). The goal of our approach, SimpleBits , is to minimize bpdof input images while
preserving task-relevant information. In the following section, we will explore how to ensure the preservation
of task-relevant information.
3Under review as submission to TMLR
3 Measuring And Preserving Task-Relevant Instance Information
Figure 4: Result of matching diﬀerent values on 2D binary classiﬁcation problem with one discriminative and
one non-discriminative dimension. Matching the examplewise gradients leads to retaining nondiscriminative
information, matching the average gradient leads to losing the correspondence between individual simpliﬁed-
original example pairs, whereas matching weights times input leads to the desired behavior.
How to measure the preservation of task-relevant information? The task-relevant information learned by a
network must be reﬂected in the network activations and hence minimizing the eﬀect of the simpliﬁcation
on them may be a viable approach. However, diﬀerent activations vary in their contribution to the ﬁnal
prediction, due to the later processing in subsequent layers. Therefore, we will weight activation changes by
the gradient of the classiﬁcation loss function in our information-preservation measure.
We propose to use the distance of the gradient-weighted network activations of the original and the simpliﬁed
input as our information-preservation measure. The gradient of the loss on the original input with regard
to the network activations∂L(f(xorig),y))
∂h(xorig)is used as a proxy for the contribution of that activation to the
predictionf(xorig). Therefore, we try to reduce the distance of the activations weighted by that gradient,
i.e., reducing the distances of vorig=∂L(f(xorig),y))
∂h(xorig)◦h(xorig)andvsimple =∂L(f(xorig),y))
∂h(xorig)◦h(xsimple ).
A variety of distance functions have recently been used to match gradients or weight changes, e.g., layerwise
cosine similarity (Zhao et al., 2021) or l2-normalized squared distance (Cazenavette et al., 2022). Here, we
use layerwise l2-normalized squared distance, concretely||vl
simple−vl
orig||2
2
||vl
orig||2
2for original and simpliﬁed values
for layerl, so forvl
orig=∂L(f(xorig),y))
∂hl(xorig)◦hl(xorig)andvl
simple =∂L(f(xorig),y))
∂hl(xorig)◦hl(xsimple ). Using the
layerwise l2-normalized squared distance instead of the cosine distance takes into account the magnitude of
the activations, which preliminary experiments showed to better ensure that the simpliﬁed images remain
visually interpretable. Overall, this results in the loss function:
Ltask(xsimple ) =/summationdisplay
l||∂L(f(xorig),y))
∂hl(xorig)◦(hl(xorig)−hl(xsimple ))||2
2
||◦∂L(f(xorig),y))
∂hl(xorig)◦hl(xorig)||2
2(2)
We will further motivate our measure with a 2D toy example. Our 2D binary classiﬁcation problem has
one discriminative dimension (both classes have diﬀerent distributions) and one nondiscriminative dimension
(both classes have the same distribution), see Fig. 4. Here, we propose the simpliﬁcation should only retain
the input values of the discriminative dimension and map the nondiscriminative dimension to zero.
Commonly used methods to preserve information by matching gradients (as used in dataset condensation
methods that condense entire training datasets into smaller synthetic datasets) will fail to achieve the desired
result. Matching the per-example loss gradients undesirably retains the values of the nondiscriminative
dimension in the simpliﬁed examples. The loss gradient for example xand target ywrt. to weights wis
∂L(f(x),y)
∂w=∂L(f(x),y)
∂f(x)·∂f(x)
∂w=∂L(f(x),y)
∂f(x)·x, so both dimensions x1andx2will be optimized to be the
same for the original and simpliﬁed examples. Instead matching the gradient averaged over all examples
maps all simpliﬁed examples of one class to the same point, losing the correspondence between simpliﬁed
and original examples. What does work as desired is matching w◦xas seen in Fig. 4, where only the
4Under review as submission to TMLR
Algorithm 1 One SimpleBits Joint Training Step
1:givengenerative network G, input and target batch Xandy, the classiﬁer clf and the image-to-image simpliﬁer network.
2:Create a copy of the classiﬁer and train it one step on real data, e.g. using SGD/Adam
3: copied_clf =train_one_step (copy(clf) ,X,y)
4:Simplify original data batch
5:Xsimple =simpliﬁer (X)
6:Compute per-example simpliﬁcation loss as in equation 1
7:sim_loss = log pG(Xsimple )
8:Compute per-example task loss as in equation 2
9:task_loss =Ltask(copied_clf ,Xsimple ,X,y)
10:Predict original data to know which examples are predicted correctly.
11:predicted _y=copied_clf (X)
12:Mask out simpliﬁcation loss for examples with too high task loss or incorrectly predicted class.
13:sim_loss =sim_loss ◦(task_loss<threshold )◦(predicted _y==y)
14:Compute average total loss over examples.
15: total_loss =average (sim_loss+task_loss)
16:Update simpliﬁer using the total loss, e.g. with SGD/Adam
17: simpliﬁer =update_simpliﬁer (simpliﬁer ,total_loss )
18:Simplify original data batch with updated simpliﬁer
19:Xsimple =simpliﬁer (X)
20:Train one step with classiﬁcation loss on simpliﬁed data, e.g. using SGD/Adam.
21: clf =train_one_step (clf,Xsimple ,y)
22:Return updated simpliﬁer and classiﬁer
23:returnsimpliﬁer, clf
discriminative dimension x1is optimized to be the same for the simpliﬁed and the original examples. Noting
that the gradient of loss of the with regard to the inputs is∂L(f(x),y)
∂x=∂L(f(x),y)
∂f(x)◦∂f(x)
∂x=∂L(f(x),y)
∂f(x)◦w, our
gradient-weighed activation matching can be seen as a generalization of the x◦wmatching to deep networks.
In preliminary work, we also considered w◦∂L(f(x),y)
∂w(weights times gradients wrt. to weights). We mainly
opted for the activation-based matching due to the faster computation as no examplewise weight gradients
need to be computed. Note that for the simple linear model derived above, both of these metrics are
equivalent∂L(f(x),y)
∂w◦w=∂L(f(x),y)
∂f(x)·∂f(x)
∂w◦w=∂L(f(x),y)
∂f(x)·x◦w=∂L(f(x),y)
∂f(x)·x◦∂f(x)
∂x=∂L(f(x),y)
∂x·x.
We note that both of these metrics have been used elsewhere, e.g. the w◦∂L(f(x),y)
∂wmetric has been used
to identify important connections for pruning (Lee et al., 2019) and h(x)◦∂L(f(x),y))
∂h(x)has been used in
interpretability work where mostly the original input activation xwas used as h(x)(Shrikumar et al., 2017).
4 Per-Instance Simpliﬁcation During Training
When plugged into the training of a classiﬁer f,SimpleBits aims to simplify each image such that fcan
still learn the original classiﬁcation task from the simpliﬁed images. For that, we train an image-to-image
network simpliﬁer to simultaneously optimize LsimandLtaskwhile we train the classiﬁer on the simpliﬁed
images. As the classiﬁer guides the simpliﬁer in learning what information is task-relevant, we also need to
enable the classiﬁer to learn task-relevant information that the simpliﬁer network has not included in the
simpliﬁed images at that point. For that, before computing Ltaskon a paired batch of real and simpliﬁed
images, we ﬁrst copy the current classiﬁer state and train that copied classiﬁer for one step on the real images
of that batch. Since only a copy of the classiﬁer is trained in that step, the actual classiﬁer still only learns
from the simpliﬁed images.
To achieve a tradeoﬀ between simpliﬁcation and preservation of task-relevant information, we set an upper
bound on the allowed Ltask. This intuitively deﬁnes how much task-relevant information may be lost due to
the simpliﬁcation on each example and we vary it to investigate diﬀerent tradeoﬀs. We only optimize the
simpliﬁcation loss on an example as long as (a) the Ltaskstays below the threshold on that same example
(b) the classiﬁer predicts the correct class on that example. This was empirically more stable than using a
linear combination of the losses and also had the desired result of keeping task-relevant information in each
image rather than just in the dataset. Pseudocode for one training step can be found in Algorithm 1.
5Under review as submission to TMLR
5 Experiments
Our classiﬁer architecture is a normalizer-free architecture to avoid interpretation diﬃculties that may arise
from normalization layers, such as image values being downscaled by the simpliﬁer and then renormalized
again. We use Wide Residual Networks as described by Brock et al. (2021). The normalizer-free architecture
reaches 94.0% on CIFAR10 in our experiments, however we opt for a smaller variant for faster experiments
that reaches 91.7%; additional details are included in the Supplementary Section S2.1. For the simpliﬁer
network, we extend the Wide ResNet architecture with 3 output heads at diﬀerent stages of the network
that outputs encodings to be inverted by our generative model, a three-scale Glow architecture as mentioned
in Section 2. This leverages the learnt decoder of the Glow architecture for our simpliﬁer , more details are
included in Supplementary Section S2.2.
5.1 SimpleBits Removes Injected Distractors
Original
SimpleBits
output
OriginalSide-by-Side MNIST s (ground truth: MNIST) MNIST with Uniform Noise (ground truth: MNIST)
Interpolated MNIST and CIF AR10 (ground truth: MNIST) CIFAR10 with Stripes (ground truth: Stripes)
SimpleBits
output
Figure 5: Evaluation of SimpleBits as a distractor removal on four composite datasets. Shown are the
composite original images and the corresponding simpliﬁed images produced by SimpleBits trained alongside
theclassiﬁer. SimpleBits isabletoalmostentirelyremovetask-irrelevantimageparts, namelyFashionMNIST
(top left), random noise ( top right ), CIFAR10 ( bottom left as well as bottom right ).
We ﬁrst evaluate whether our per-instance simpliﬁcation during training successfully removes superﬂuous
information for tasks with injected distractors. To that end, we construct datasets to contain both useful
(ground truth) and redundant (distractor) information for task learning. We create four composite datasets
derived from three conventional datasets: MNIST (LeCun & Cortes, 2010), FashionMNIST (Xiao et al.,
2017) and CIFAR10 (Krizhevsky, 2009). Sample images, both constructed (input to the whole model) and
simpliﬁed (output of simpliﬁer and input to classiﬁer), are shown in Figure 5.
Side-by-SideMNIST constructseachimagebyhorizontallyconcatenatingoneimagefromFashion-MNIST
and one from MNIST. Each image is rescaled to 16x32, so the concatenated image size remains 32x32; the
order of concatenation is random. The ground truth target is MNIST labels, and therefore FashionMNIST
is an irrelevant distractor for the classiﬁcation task. As seen in Figure 5, the simpliﬁer eﬀectively removes
the clothes side of the image.
MNIST with Uniform Noise adds uniform noise to the MNIST digits, preserving the MNIST digit as
the classiﬁcation target. Hence the noise is the distractor and is expected to be removed. And indeed the
noise is no longer visible in the simpliﬁed outputs shown in Figure 5.
Interpolated MNIST and CIFAR10 is constructed by interpolating between MNIST and CIFAR10
images. MNIST digits are the classiﬁcation target. The expectation is that the simpliﬁed images should
no longer contain any of the CIFAR10 image information. The result shows that most of the CIFAR10
information is removed, leaving only some information that is visually similar to parts of digits.
6Under review as submission to TMLR
2 3
Bits per Dim.6080Accuracy [%]CIFAR10
1.5 2.0 2.5
Bits per Dim.8090SVHN
1.25 1.50
Bits per Dim.8090FASHIONMNIST
1.0 1.2
Bits per Dim.8090100MNIST
Original Data End of Joint Training Retrained on Simplified
Figure 6: Results for training image simpliﬁcations on real datasets. Dots show results for training with
varying upper bound on Ltask. Images with less bits per dimension lead to reduced accuracies, and such
reduction is more pronounced for complex datasets like CIFAR10.
Figure 7: SimpleBits examples of training for increased simpliﬁcation by increasing loss bound for Ltaskon
CIFAR10, SVHN, MNIST and Fashion-MNIST. Images produced by the simpliﬁer at end of joint training,
shown are the ﬁrst example of each class in the test set. Percentages left of each row indicate accuracies
after retraining on images produced by the same simpliﬁer.
CIFAR10 with Stripes overlays either horizontal or vertical stripes onto CIFAR10 images, with the binary
classiﬁcation label 0 for horizontal and 1 for vertical stripes. With this dataset the simpliﬁcation mostly
only retains some horizontal and vertical stripes, which is enough to solve this task.
7Under review as submission to TMLR
8688
Acc. [%]1.01.11.21.31.41.5Bits per dimension
Healthy Pleural Effusion
Figure 8: Results on MIMIC-CXR Pleural Eﬀusion vs. Healthy Diagnosis from Chest X-Rays.
5.2 Trade-oﬀ Curves on Conventional Datasets
We veriﬁed that SimpleBits is able to discern information relevance in inputs, and eﬀectively removes re-
dundant content to better serve the classiﬁcation task. In real-world datasets, however, the type, as well as
the amount of information redundancy in inputs is often unclear.
With the same framework, by varying the upper bound on the allowed Ltaskwe can study the trade-oﬀ
between task performance and level of simpliﬁcation to better understand a given dataset. With lower upper
bounds, the training should resemble conventional training as Lsimwill be mostly turned oﬀ. On the other
end, with higher upper bounds, the inputs should be more drastically simpliﬁed at the expense of accuracy
due to the loss of task-relevant information.
We experimented with MNIST, Fashion-MNIST, SVHN (Netzer et al., 2011) and CIFAR10, producing a
trade-oﬀ curve for each by setting the upper bound on the allowed Ltaskvarious values for diﬀerent training
runs. For each setting, we report the classiﬁcation accuracy as a result of joint training of simpliﬁcation and
classiﬁcation and after retraining from scratch with only simpliﬁed images, as shown in Figure 6. We also
show the ﬁrst simpliﬁed test image of each class, see Figure 7.
As expected, higher strengths of simpliﬁcation lead to decreased task performance. Interestingly, such decay
is observed to be more pronounced for more complex datasets such as CIFAR10. This suggests either the
presence of a relatively small amount of information redundancy, or that naturally occurring noise in data
help with generalization, analogous to how injected noise from data augmentation helps learning features
invariant to translations and viewpoints, even though the augmentation itself does not contain additional
discriminative information.
Qualitative analysis of the simpliﬁed images in Figure 7 suggests that even with strong simpliﬁcation, dis-
criminative information is retained in the images. At the strongest simpliﬁcation level, with many details
removed, most classes are still visually identiﬁable.
We run three baselines to validate the eﬃcacy of our simpliﬁcation framework, as described in Supplementary
Section S5 and shown in Figure S4. Section S3 shows that our simpliﬁcation has practical savings in image
storage. Training curves can be seen in Section S4. Additionally, we show that SimpleBits can also simplify
images in a dataset condensation setting, see Section S8 .
8Under review as submission to TMLR
5.3 SimpleBits is Able to Retain Good Accuracies on Corrupted CIFAR10
2 3
Bits per dimension of
training images8090Accuracy  [%]
Evaluated on CIFAR10
2 3
Bits per dimension of
training images6870
Evaluated on CIFAR10-C
SimpleBits with
simplifier during
evaluation
Regular
CIFAR10 
classifierSimpleBits trained on CIFAR10
gaussian_noiseimpulse_noise gaussian_blurshot_noise zoom_blur
speckle_noiseglass_blur
defocus_blur motion_blurcontrast
jpeg_compressionfrost
pixelate
elastic_transformfog
saturatespattersnow
brightness
Corruption050Accuracy [%]Results per Subset SimpleBits with Strongest Simplification
Regular Classifier SimpleBits Ensemble of both
Figure 9: Results of SimpleBits on CIFAR10-C. Top:Accuracies of classiﬁers trained with varying upper
bound onLtaskat the end of joint training. Blue lines indicate accuracies after SimpleBits training, with
the frozen simpliﬁer network applied to the test data during evaluation. Black lines indicate baseline accu-
racy of a network trained without any simpliﬁcation. Accuracies decrease with increasing simpliﬁcation on
the CIFAR10 dataset, but accuracies do not decrease and even slightly increase for the simpliﬁed images
on CIFAR10-C. This may indicate the combination of the simpliﬁer and the classiﬁer have learned more
robust features. Bottom: Results per CIFAR10-C subset of the SimpleBits -trained model with strongest
simpliﬁcation. SimpleBits has especially high accuracies on high-frequency noise and blurring corruptions.
We evaluated the performance of our SimpleBits -trained CIFAR10 models on the CIFAR10-C dataset, which
consists of the CIFAR10 evaluation set with various corruptions applied (Hendrycks & Dietterich, 2019). We
found that SimpleBits evaluated on the simpliﬁed images actually increased accuracy on CIFAR10-C with
increasing simpliﬁcation, even slightly outperforming a regularly trained CIFAR10 classiﬁer (see Figure 9).
SimpleBits has especially high accuracies on high-frequency noise and blurring corruptions. This suggests
that the combination of the simpliﬁer and SimpleBits -trained classiﬁer is able to retain features robust to
some corruptions on CIFAR10.
5.4 SimpleBits Retains Plausible Features on Chest X-Rays
We applied SimpleBits to the MIMIC-CXR-JPG dataset to classify Chest X-Rays of healthy patients and
thosewithpleuraleﬀusion. Theclassiﬁerwasabletomaintainhighaccuracyacrossalllevelsofsimpliﬁcation,
and the simpliﬁed images still displayed plausible Chest-X-Ray features for pleural eﬀusion, see Figure 8.
9Under review as submission to TMLR
6 Related Work
Our approach simplifying individual training images builds on Raghu et al. (2021), where they learn to
inject information into the classiﬁer training. Per-instance simpliﬁcation during training can be seen as a
instance of their framework combined with the idea of input simpliﬁcation. In diﬀerence to their methods,
SimpleBits explicitly aims for interpretability through input simpliﬁcation.
Other interpretability approaches that synthesize inputs include generating counterfactual inputs (Hvilshøj
et al., 2021; Dombrowski et al., 2021; Goyal et al., 2019) or inputs with exaggerated features (Singla et al.,
2020).SimpleBits diﬀers in explicitly optimizing the inputs to be simpler.
Generative models have often been used in various ways for interpretability such as generating realistic-
looking inputs (Montavon et al., 2018) and by directly training generative classiﬁers (Hvilshøj et al., 2021;
Dombrowski et al., 2021), but we are not aware of any work except (Dubois et al., 2021) (discussed above) to
explicitly generate simpler inputs. A diﬀerent approach to reduce input bits while retaining classiﬁcation per-
formance is to train a compressor that only keeps information that is invariant to predeﬁned label-preserving
augmentations. Dubois et al. (2021) implement this elegant approach in two ways. In their ﬁrst variant, by
training a VAE to reconstruct an unaugmented input from augmented (e.g. rotated, translated, sheared)
versions. In their second variant, building on the CLIP (Radford et al., 2021) model, they view images with
the same associated text captions as augmented versions of each other. This allows the use of compressed
CLIP encodings for classiﬁcation and achieves up to 1000x compression on Imagenet without decreasing
classiﬁcation accuracy. Their approach focuses on achieving maximum compression while our approach is
focused on interpretability. Their approach requires access to predeﬁned label-preserving augmentations and
has reduced classiﬁcation performance in input space compared to latent/encoding space.
7 Conclusion
We propose SimpleBits , an information-based method to synthesize simpliﬁed inputs. Crucially, Sim-
pleBitsdoes not require any domain-speciﬁc knowledge to constrain or dictate which input components
should be removed; instead SimpleBits itself learns to remove the components of inputs which are least
relevant for a given task.
Our simpliﬁcation approach sheds light on the information required for a deep network classiﬁer to learn
its task. It can remove injected distractors and reveal discriminative features. Further, we ﬁnd that for our
approach the tradeoﬀ between task performance and input simpliﬁcation varies by dataset and setting — it
is more pronounced for more complex datasets.
10Under review as submission to TMLR
References
Agarwal, C., D’souza, D., and Hooker, S. Estimating example diﬃculty using variance of gradients, 2021.
Banerjee, I., Bhimireddy, A. R., Burns, J. L., Celi, L. A., Chen, L., Correa, R., Dullerud, N., Ghas-
semi, M., Huang, S., Kuo, P., Lungren, M. P., Palmer, L. J., Price, B. J., Purkayastha, S., Pyrros, A.,
Oakden-Rayner, L., Okechukwu, C., Seyyed-Kalantari, L., Trivedi, H., Wang, R., Zaiman, Z., Zhang,
H., and Gichoya, J. W. Reading race: AI recognises patient’s racial identity in medical images. CoRR,
abs/2107.10356, 2021a. URL https://arxiv.org/abs/2107.10356 .
Banerjee, I., Bhimireddy, A. R., Burns, J. L., Celi, L. A., Chen, L.-C., Correa, R., Dullerud, N., Ghassemi,
M., Huang, S.-C., Kuo, P.-C., Lungren, M. P., Palmer, L., Price, B. J., Purkayastha, S., Pyrros, A.,
Oakden-Rayner, L., Okechukwu, C., Seyyed-Kalantari, L., Trivedi, H., Wang, R., Zaiman, Z., Zhang, H.,
and Gichoya, J. W. Reading race: Ai recognises patient’s racial identity in medical images, 2021b.
Bastings, J., Ebert, S., Zablotskaia, P., Sandholm, A., and Filippova, K. "will you ﬁnd these shortcuts?" a
protocol for evaluating the faithfulness of input salience methods for text classiﬁcation, 2021.
Bellemare, F., Jeanneret, A., and Couture, J. Sex diﬀerences in thoracic dimensions and conﬁguration.
American journal of respiratory and critical care medicine , 168(3):305–312, 2003.
Brock, A., De, S., Smith, S. L., and Simonyan, K. High-performance large-scale image recognition without
normalization. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine
Learning Research , pp.1059–1071.PMLR,2021. URL http://proceedings.mlr.press/v139/brock21a.
html.
Cazenavette, G., Wang, T., Torralba, A., Efros, A. A., and Zhu, J. Dataset distillation by matching training
trajectories. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022 , pp.10708–10717.IEEE,2022. doi: 10.1109/CVPR52688.2022.01045.
URL https://doi.org/10.1109/CVPR52688.2022.01045 .
Dombrowski, A.-K., Gerken, J. E., and Kessel, P. Diﬀeomorphic explanations with normalizing ﬂows. In
ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models , 2021.
URL https://openreview.net/forum?id=ZBR9EpEl6G4 .
D’souza, D., Nussbaum, Z., Agarwal, C., and Hooker, S. A tale of two long tails, 2021.
Dubois, Y., Bloem-Reddy, B., Ullrich, K., and Maddison, C. J. Lossy compression for lossless prediction.
CoRR, abs/2106.10800, 2021. URL https://arxiv.org/abs/2106.10800 .
Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A.
Shortcut learning in deep neural networks. Nature Machine Intelligence , 2(11):665–673, 2020.
Goyal, Y., Wu, Z., Ernst, J., Batra, D., Parikh, D., and Lee, S. Counterfactual visual explanations. In
Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA ,volume97of Proceedings of Machine
Learning Research , pp. 2376–2384. PMLR, 2019. URL http://proceedings.mlr.press/v97/goyal19a.
html.
Havtorn, J. D., Frellsen, J., Hauberg, S., and Maaløe, L. Hierarchical vaes know what they don’t know. In
Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research ,
pp. 4117–4128. PMLR, 2021. URL http://proceedings.mlr.press/v139/havtorn21a.html .
Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and
perturbations. Proceedings of the International Conference on Learning Representations , 2019.
11Under review as submission to TMLR
Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B. A benchmark for interpretability meth-
ods in deep neural networks. In Wallach, H., Larochelle, H., Beygelzimer, A., dÁlché-Buc,
F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems , vol-
ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf .
Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In
Leibe, B., Matas, J., Sebe, N., and Welling, M. (eds.), Computer Vision - ECCV 2016 - 14th European
Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV , volume 9908 of
Lecture Notes in Computer Science , pp. 646–661. Springer, 2016. doi: 10.1007/978-3-319-46493-0\_39.
URL https://doi.org/10.1007/978-3-319-46493-0_39 .
Hull, J. J. A database for handwritten text recognition research. IEEE Trans. Pattern Anal. Mach. Intell. ,
16(5):550–554, 1994. doi: 10.1109/34.291440. URL https://doi.org/10.1109/34.291440 .
Hvilshøj, F., Iosiﬁdis, A., and Assent, I. ECINN: eﬃcient counterfactuals from invertible neural networks.
CoRR, abs/2103.13701, 2021. URL https://arxiv.org/abs/2103.13701 .
Jany, B. and Welte, T. Pleural eﬀusion in adults—etiology, diagnosis, and treatment. Deutsches Ärzteblatt
International , 116(21):377, 2019.
Johnson, A. E., Pollard, T. J., Berkowitz, S. J., Greenbaum, N. R., Lungren, M. P., Deng, C.-y., Mark,
R. G., and Horng, S. Mimic-cxr, a de-identiﬁed publicly available database of chest radiographs with
free-text reports. Scientiﬁc data , 6(1):1–8, 2019a.
Johnson, A. E., Pollard, T. J., Greenbaum, N. R., Lungren, M. P., Deng, C.-y., Peng, Y., Lu, Z., Mark,
R. G., Berkowitz, S. J., and Horng, S. Mimic-cxr-jpg, a large publicly available database of labeled chest
radiographs. arXiv preprint arXiv:1901.07042 , 2019b.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.),
3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings , 2015. URL http://arxiv.org/abs/1412.6980 .
Kingma, D. P. and Dhariwal, P. Glow: Generative ﬂow with invertible 1x1 convolutions. In Bengio, S.,
Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems (NeuRIPs) , pp. 10215–10224, 2018.
Kirichenko, P., Izmailov, P., and Wilson, A. G. Why normalizing ﬂows fail to detect out-of-distribution
data. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/ecb9fe2fbb99c31f567e9823e884dbec-Abstract.html .
Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009.
LeCun, Y. and Cortes, C. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/
mnist/.
Lee, N., Ajanthan, T., and Torr, P. H. S. Snip: single-shot network pruning based on connection sensitivity.
In7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 . OpenReview.net, 2019. URL https://openreview.net/forum?id=B1VZqjAcYX .
Loshchilov, I. and Hutter, F. SGDR: stochastic gradient descent with warm restarts. In 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
Track Proceedings . OpenReview.net, 2017. URL https://openreview.net/forum?id=Skq89Scxx .
Maclaurin, D., Duvenaud, D., and Adams, R. Gradient-based hyperparameter optimization through re-
versible learning. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on
Machine Learning , volume 37 of Proceedings of Machine Learning Research , pp. 2113–2122, Lille, France,
07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/maclaurin15.html .
12Under review as submission to TMLR
Madsen, A., Meade, N., Adlakha, V., and Reddy, S. Evaluating the faithfulness of importance measures in
nlp by recursively masking allegedly important tokens and retraining, 2021.
Makino, T., Jastrzębski, S., Oleszkiewicz, W., Chacko, C., Ehrenpreis, R., Samreen, N., Chhor, C., Kim, E.,
Lee, J., Pysarenko, K., et al. Diﬀerences between human and machine perception in medical diagnosis.
Scientiﬁc reports , 12(1):1–13, 2022.
Montavon, G., Samek, W., and Müller, K.-R. Methods for interpreting and understanding deep neural
networks. Digital Signal Processing , 73:1–15, 2018. ISSN 1051-2004. doi: https://doi.org/10.1016/j.dsp.
2017.10.011. URL https://www.sciencedirect.com/science/article/pii/S1051200417302385 .
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with
unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning ,
2011.
Nguyen, T., Novak, R., Xiao, L., and Lee, J. Dataset distillation with inﬁnitely wide convolutional networks.
CoRR, abs/2107.13034, 2021. URL https://arxiv.org/abs/2107.13034 .
Raasch, B., Carsky, E., Lane, E., O’Callaghan, J., and Heitzman, E. Pleural eﬀusion: explanation of some
typical appearances. American Journal of Roentgenology , 139(5):899–904, 1982.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin,
P., Clark, J., et al. Learning transferable visual models from natural language supervision. arXiv preprint
arXiv:2103.00020 , 2021.
Raghu, A., Raghu, M., Kornblith, S., Duvenaud, D., and Hinton, G. E. Teaching with commentaries. In
9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,
2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=4RbdgBh9gE .
Raghu, M. and Schmidt, E. A survey of deep learning for scientiﬁc discovery. arXiv preprint
arXiv:2003.11755 , 2020.
Schirrmeister, R., Zhou, Y., Ball, T., and Zhang, D. Understanding anomaly detection with deep
invertible networks through hierarchies of distributions and features. In Larochelle, H., Ran-
zato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Process-
ing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
f106b7f99d2cb30c3db1c3cc0fde9ccb-Abstract.html .
Serrà, J., Álvarez, D., Gómez, V., Slizovskaia, O., Núñez, J. F., and Luque, J. Input complexity and
out-of-distribution detection with likelihood-based generative models. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.
URL https://openreview.net/forum?id=SyxIWpVYvr .
Shannon, C. E. A mathematical theory of communication. The Bell System Technical Journal , 27(3):
379–423, July 1948.
Shrikumar, A., Greenside, P., and Kundaje, A. Learning important features through propagating activation
diﬀerences. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on
Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70 of Proceedings of
Machine Learning Research , pp. 3145–3153. PMLR, 2017. URL http://proceedings.mlr.press/v70/
shrikumar17a.html .
Singla, S., Pollack, B., Chen, J., and Batmanghelich, K. Explanation by progressive exaggeration. In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=H1xFWgrFPS .
Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny images: A large data set for nonparametric
object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence , 30(11):
1958–1970, 2008.
13Under review as submission to TMLR
Wang, T., Zhu, J., Torralba, A., and Efros, A. A. Dataset distillation. CoRR, abs/1811.10959, 2018. URL
http://arxiv.org/abs/1811.10959 .
Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms, 2017.
Zagoruyko, S. and Komodakis, N. Wide residual networks. In Wilson, R. C., Hancock, E. R., and Smith,
W. A. P. (eds.), Proceedings of the British Machine Vision Conference 2016, BMVC 2016, York, UK,
September 19-22, 2016 . BMVA Press, 2016. URL http://www.bmva.org/bmvc/2016/papers/paper087/
index.html .
Zhao, B. and Bilen, H. Dataset condensation with diﬀerentiable siamese augmentation. In Meila, M. and
Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-
24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pp. 12674–12685.
PMLR, 2021. URL http://proceedings.mlr.press/v139/zhao21a.html .
Zhao, B., Mopuri, K. R., and Bilen, H. Dataset condensation with gradient matching. In 9th International
Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenRe-
view.net, 2021. URL https://openreview.net/forum?id=mSAKhLYLSsl .
Zintgraf, L. M., Cohen, T. S., Adel, T., and Welling, M. Visualizing deep neural network decisions:
Prediction diﬀerence analysis. In 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL
https://openreview.net/forum?id=BJ5UeU9xx .
14Under review as submission to TMLR
Supplementary Information for:
When Less is More: Simplifying Inputs Aids Neural
Network Understanding
Supplementary Outline
This document completes the presentation of the main paper with the following:
Supplementary
SectionType of
ContentRelevant Section
in Main TextTL;DR
S1 Additional experiments Section 2 Example images with bpdvalues produced
by other generative models
S2 Implementation details Section 4 Architecture details for simpliﬁcation dur-
ing training
S3, S4, S5 Additional experiments Section 5.2 Baselines, training curves, ﬁle size analysis
S7 Additional experiments Section 5.2 SimpleBits applied to Vision Transformers
S6 More ﬁgures Figure 2 Uncurated sets of simpliﬁed images at end
of joint training
S8 Additional experiments - SimpleBits applied to Dataset Condensa-
tion
S1 BPDs of Other Generative Models
Figure S1 shows that the bits per dimension produced by other generative models than Glow also correlate
well with visual complexity, validating our measure. This is consistent with prior work that found bpds
of generative models trained on natural image datasets are strongly inﬂuenced by general natural image
characteristicsindependentofanyspeciﬁcdataset(Kirichenkoetal.,2020;Schirrmeisteretal.,2020;Havtorn
et al., 2021).
S2 Architecture Details for Per-Instance Simpliﬁcation During Training
S2.1 Classiﬁer Network
Our classiﬁcation network is based on the Wide ResNet architecture (Zagoruyko & Komodakis, 2016). We
used a version with relatively few parameters with depth = 16and widen_factor = 2to allow for fast
iteration on experimentation. We used ELU instead of ReLU nonlinearities.
Additionally, we removed batch normalization to avoid interference of normalization layers with the simpli-
ﬁcation process. We followed the method from Brock et al. (2021) to create a normalizer-free Wide ResNet.
We reparameterize the convolutional layers using Scaled Weight Standardization:
ˆWij=Wij−µi√
Nσi, (S1)
whereµi= (1/N)/summationtext
jWij,σ2
i= (1/N)/summationtext
j(Wij−µi)2, andNdenotes the fan-in. Further as in Brock
et al. (2021), "activation functions are also scaled by a non-linearity speciﬁc scalar gain γ, which ensures
S15Under review as submission to TMLR
Figure S1: Visualization of the bits-per-dimension ( bpd) measure for image complexity, sorted from low to
high. Image samples are taken from MNIST, Fashion-MNIST, CIFAR10 and CIFAR100, in addition to a
completely black image sample. bpdis calculated from the density produced by a Glow (Kingma & Dhariwal,
2018) model pretrained on 80 Million Tiny Images, a PixelCNN model trained on CIFAR10, and a diﬀusion
model trained on CIFAR10.
that the combination of the γ-scaled activation function and a Scaled Weight Standardized layer is variance
preserving." Finally, the output of the residual branch is downscaled by 0.2, so the function to compute the
output becomes hi+1=hi+ 0.2·fi(hi), wherehidenotes the inputs to the ithresidual block, and fidenotes
the function computed by the ithresidual branch. Unlike Brock et al. (2021), we did not multiply scalars βi
with the input of the residual branch or learned zero-initialized scalars to multiply with the output of the
residual branch, as we did not ﬁnd these two parts helpful in our setting. We also did not attempt to use
Stochastic Depth (Huang et al., 2016), which may further improve upon the accuracies reported here. Due
to our small batch sizes (32), we also did not use adaptive gradient clipping.
With this setup, the normalizer-free Wide ResNet with depth = 28and widen_factor = 10reached 94.0%
on CIFAR10. To enable faster experiments, we instead use a smaller architecture with depth = 16and
widen_factor = 2which reached 91.2% on CIFAR10.
S2.2 Simpliﬁer Network
For the simpliﬁer network, we extend the Wide ResNet architecture with 3 output heads at diﬀerent stages of
the network that output encodings to be inverted by our generative model, a three-scale Glow architecture
as mentioned in Section 2. This leverages the learnt decoder of the Glow architecture for our simpliﬁer .
Concretely, our simpliﬁer network predicts scaling and translation coeﬃcients for the Glow encodings of the
original image. We invert the scaled and translated encodings using the same Glow network to obtain our
simpliﬁed images.
S3 PNG-compressed File Sizes of Simpliﬁed Images
We validated that simpliﬁed images actually occupy less storage space. We saved images in the PNG ﬁle
format and calculated the ﬁle size. Figure S2 shows the average PNG ﬁle size of the simpliﬁed images
obtained from our simpliﬁcation framework. Concretely, we PNG-compressed the images at the end of the
joint simpliﬁcation and classiﬁcation training and then computed the average ﬁle size. Varying the upper
S16Under review as submission to TMLR
1.50 1.75 2.00 2.25 2.50 2.75 3.00 3.25 3.50
Bits per dimension (produced by Glow)100120140PNG-compressed
File Size
in Kilobytes
Simplified CIFAR10 Compression Sizes
FigureS2: SimpliﬁedCIFAR10imagesresultinsmallerstoragespacewhenconvertedintoPNGﬁles. Plotted
are the average PNG ﬁle sizes of the simpliﬁed images after joint simpliﬁcation and classiﬁcation training (see
Section 4), against bpdvalues produced by Glow, the generative model. Runs with diﬀerent simpliﬁcation
loss weight λsimlead to diﬀerent average ﬁle sizes.
0 20 40 60 80 100
Training Epoch255075T est Accuracy [%]Retraining Learning Curves CIFAR10
2.82 BPD
2.47 BPD
1.96 BPD
1.74 BPD
1.68 BPD
1.55 BPD
Figure S3: Learning curves for retraining on simpliﬁed images on CIFAR10.
bound onLtaskleads to diﬀerent average bpdvalues and also diﬀerent ﬁle sizes, with larger bounds resulting
in smaller ﬁle sizes.
S4 Learning Curves for Retraining
Figure S3 shows learning curves during retraining on the simpliﬁed images on CIFAR10. There are no
noticeable diﬀerences in training speed for more or less simpliﬁed images.
S5 Simpliﬁer Baselines
We implemented three simpler baselines to check whether the losses used in SimpleBits during training
help retain task-relevant information. In the ﬁrst baseline, we train the simpliﬁer to simultaneously reduce
bpdof the simpliﬁed image and the mean squared error between the simpliﬁed and the original image.
Afterwards we train the classiﬁer on the simpliﬁed images and evaluate on the original images the same
way as during retraining of SimpleBits . In the second baseline, we blur the original images with a gaussian
kernel, which also reduces their bpd. We vary the sigma/standard deviation for the gaussian kernel to
trade oﬀ smoothness and task-informativeness. In the third baseline, we use lossy JPEG compression with
varying quality levels. As in SimpleBits and the other baselines, we estimate the bits per dimension of the
lossy-JPEG-compressed images through our pretrained Glow network for a fair comparison. The gaussian
blurring and JPEG compression each replace the simpliﬁer, so these are ﬁxed simpliﬁer baselines without
S17Under review as submission to TMLR
1 2 3
Bits per dimension255075Accuracy [%]
CIFAR10
1 2
Bits per dimension5075
SVHN
1 2
Bits per dimension5075
FASHIONMNIST
0.5 1.0 1.5
Bits per dimension50100
MNISTPer-Instance Simplification During Training, Retraining Results
SimpleBits
Minimize MSE and BPD
Gaussian Blurring
JPEG Compress
Figure S4: Comparison between SimpleBits and two simpler baselines: In the ﬁrst one, the simpliﬁer network
is trained to simultaneously reduce bpdof the simpliﬁed image and the mean squared error between the
simpliﬁed and the original image. In the second one, gaussian blurring is applied to the input images,
diﬀerent runs vary in the standard deviation used to create the gaussian blurring kernel. In the third one,
we use JPEG compression with varying quality levels. Tradeoﬀ curves are mostly worse for the baselines
than for SimpleBits .
training a simpliﬁer. While these three baselines also retain some task-relevant information allowing the
classiﬁer to retain above-chance accuracies (see Figure S4), the tradeoﬀ between bpdand accuracy is mostly
worse than for SimpleBits , especially on CIFAR10 and SVHN. This shows the losses used in SimpleBits help
retain more task-relevant information compared to these baselines.
S6 More Images Simpliﬁed During Training
We show a larger number of images that were simpliﬁed on CIFAR10 during training with the largest
simpliﬁcation loss weight λsim= 2.0in Figures S5 and S6.
S7 SimpleBits with Vision Transformers
To validate that SimpleBits also works with non-convolutional architectures, we trained a Vision Transformer
(ViT) with SimpleBits on CIFAR101. As Figure S7 shows, while accuracies are lower on CIFAR10 as
expected for training a ViT from scratch, SimpleBits still shows a accuracy - bits per dimension tradeoﬀ
and the simpliﬁed images qualitatively contain discriminative information. Interestingly, they look similar,
yet diﬀerent from the simpliﬁed images resulting when applying SimpleBits to convolutional networks. This
suggests the simpliﬁed images may reveal what information diﬀerent architectures are more likely to learn.
Additionally, we also evaluated how the bits-per-dimension/accuracy tradeoﬀ changes when we retrain one
type of architecture (Vision Transformers or residual convolutional networks) on simpliﬁed images obtained
from applying SimpleBits to the other type of architecture. Figure S8 show that in all variants on CIFAR10,
the networks learn above-chance accuracies, indicating that the simpliﬁed images always contain discrimi-
native information that both architectures can exploit. When retraining the convolutional networks, there
is a larger decrease in accuracy when using the simpliﬁed images obtained via vision transformers. This
may indicate that the simpliﬁed images of vision transformers are more architecture-speciﬁc. An interesting
future work can be to more deeply investigate what the simpliﬁed images reveal about the inductive biases
of diﬀerent types of network architectures.
1Implementationtakenfrom https://github.com/omihub777/ViT-CIFAR/blob/f5c8f122b4a825bf284bc9b471ec895cc9f847ae/
vit.py
S18Under review as submission to TMLR
Figure S5: Uncurated set of simpliﬁed images with λsim= 2.0, 6 per class. Rows alternate between original
and simpliﬁed images.
S8 SimpleBits applied to Dataset Condensation
Nowweinvestigatehow SimpleBits aﬀectstrainingonasmallsyntheticcondenseddataset. Multiplemethods
have been developed for dataset condensation (Zhao et al., 2021; Zhao & Bilen, 2021; Wang et al., 2018;
Maclaurin et al., 2015), via backpropagation through training (Wang et al., 2018; Maclaurin et al., 2015),
gradient matching (Zhao & Bilen, 2021), or kernel based meta-learning (Nguyen et al., 2021). Due to its
small size, one can visualize the full condensed dataset to understand what information is preserved for
learning. Our aim here is to combine SimpleBits with dataset condensation to see if we could obtain a both
smaller and simpler training dataset than the original.
In this setting, we jointly condense our training dataset to a smaller number of synthetic training inputs
and simplify the synthetic inputs according to our simpliﬁcation loss (Eq. 1). Concretely, we add the
simpliﬁcation loss Lsimto the gradient matching loss proposed by Zhao & Bilen (2021). The gradient
matching loss computes the layerwise cosine distance between the gradient of the classiﬁcation loss wrt. to
the classiﬁer parameters θproduced by a batch of original images Xorigand a batch of synthetic images
Xsyn:
Lmatch (Xorig,Xsyn) =
D(∇θl(f(Xorig),y),∇θl(f(Xsyn),y)).(S2)
whereDis the layerwise cosine distance. The matching loss is computed separately per class.
Overall, with our simpliﬁcation loss, we get:
S19Under review as submission to TMLR
Figure S6: Uncurated set of correctly predicted simpliﬁed images with λsim= 2.0, 6 per class. Rows alternate
between original and simpliﬁed images.
Lsyn(Xorig,Xsyn) =Lmatch (Xorig,Xsyn)
+/summationdisplay
xsyn∈Xsyn−logpG(xsyn)(S3)
We perform dataset condensation on MNIST, Fashion-MNIST, SVHN and CIFAR10 with varying λsimfor
the simpliﬁcation loss. We also apply dataset condensation to the chest radiograph dataset MIMIC-CXR-
JPG (Johnson et al., 2019a;b) for predicting pleural eﬀusion and gender. We use the networks from (Zhao
et al., 2021), but use Adam (Kingma & Ba, 2015) for optimization.
S8.1 SimpleBits Retains Condensation Performance While Greatly Simplifying Data
In Figure S9, we examine the accuracy for each condensed-and-simpliﬁed dataset. We observe that for the
natural image datasets, accuracies are mostly retained when decreasing the number of bits per image. Note
that the setting with highest bpd is a reimplementation of Zhao et al. (2021) and therefore a baseline without
simpliﬁcation loss. We visualize examples in Figure S9 and observe that the jointly condensed and simpliﬁed
images look visually smoother, indicating that higher frequency patterns visible in the original images are
not needed to reach the same accuracy. These visualizations are also noticeably more smooth than the results
for per-instance simpliﬁcation Figure 5, which suggests that data condensation may already favor features
that are less complex. Further condensed sets are in Section S8.2 and a continual learning evaluation in S8.3.
Evaluation of a Medical Chest Radiograph Dataset We also evaluate jointly condensing and simpli-
fying for a dataset of chest radiograph images (Johnson et al., 2019a;b). This dataset has known radiologic
features for the presence of pleural eﬀusion (Jany & Welte, 2019; Raasch et al., 1982) and diﬀerence in
gender (Bellemare et al., 2003). In Figure S10, we visualize both the condensed (top row) and the jointly
condensed and simpliﬁed dataset (bottom row) . The overlayed shows that a visible diﬀerence between
S20Under review as submission to TMLR
1.6 1.8 2.0 2.2 2.4
Bits per dimension5075Accuracy [%]
Results for Instance Simplification on CIFAR10
Figure S7: Examples of SimpleBits applied to CIFAR10 training images with a Vision Transformer. Conven-
tions as in Figure 2. We observe that simpliﬁed images seem to retain discriminative information, yet look
slightly diﬀerent from the ones in Figure 2, suggesting SimpleBits SimpleBits reveals architecture-speciﬁc
discriminative information.
1.50 1.75 2.00 2.25 2.50 2.75
Bits per dimension20406080T est Accuracy [%]
Retraining Results for Vision Transformer
1.50 1.75 2.00 2.25 2.50 2.75
Bits per dimension
Retraining Results for Residual ConvNet
SimpleBits Images
From Residual ConvNet
SimpleBits Images
From Vision Transformer
Figure S8: Cross-architecture results retraining an architecture (Vision Transformers or residual convolu-
tional networks) on simpliﬁed images resulting SimpleBits applied to another architecture.
presence of feature. For pleural eﬀusion, a larger white region on the bottom of the lung occurs in the
simpliﬁed pathological image, while for gender, lungs appear slightly smaller for the simpliﬁed female image.
S8.2 More Condensed Datasets
We also show condensed datasets for MNIST and SVHN (Figure S12), some interesting condensed datasets
that resulted when we varied the architecture (Figure S13) or the condensation loss (from gradient matching
to either negative gradient product or single-training-step unrolling, Figure S14).
S8.3 Evaluation of Condensed Datasets for Continual Learning
We also evaluate the simpliﬁed condensed datasets in a continual learning setting, following (Zhao & Bilen,
2021). In this task-incremental continual learning setting, the model is trained on diﬀerent classiﬁcation
S21Under review as submission to TMLR
4 6 8
Bits per dimension5075Accuracy [%]
1 Images per Class
2 4 6 8 10
Bits per dimension5075100
10 Images per Class
FashionMNIST
CIFAR10
SVHN
MNIST
Figure S9: Dataset condensation accuracies (when retraining with the condensed dataset) vs. data sim-
plicity.Top:Each dot represents a data condensation experiment run with a particular weight for the
simpliﬁcation loss, which results in more or less complex datasets. Accuracies can be retained even with sub-
stantially reduced bits per dimension. In the 1-image-per-class case ( top left ﬁgure ), arrows highlight the
settings that are visualized in the bottom ﬁgure. Bottom: Condensed datasets with varying simpliﬁcation
loss weight. Each row represents the whole condensed dataset (1 image per class), with high (top row) or
low (bottom row) bits per dimension. Lower bits per dimension datasets are visually simpler and smoother
while retaining the accuracy.
Figure S10: Condensed dataset for pleural eﬀusion and gender prediction from chest radiographs in MIMIC-
CXR. Condensed images for the classes look very similar. Color-coded mixed rightmost images reveal the
diﬀerences between the classes. Green highlighted region at the lower end of the lung consistent with typical
radiologic features for pleural eﬀusion (white region indicating ﬂuid on lungs), red highlighted around lung
for gender indicate smaller lung volume for the female class.
S22Under review as submission to TMLR
SVHN SVHN+MNIST SVHN+MNIST+USPS
Training Stage7580859095Accuracy [%]
 (average across datasets
 seen during training)
Continual Learning Results
ours w. KD SVHN bpd: 11.9, MNIST BPD: 4.3
ours w.o. KD
ours w. KD SVHN bpd: 3.9, MNIST BPD: 2.5
ours w.o. KD
ours w. KD SVHN bpd: 1.0, MNIST BPD: 0.8
ours w.o. KD
no condensed dataset
Orig w. KD
Orig w.o. KD
Figure S11: Continual Learning Results without Condensed Dataset (regular sequential training). Conven-
tions as in Figure S15. Accuracies substantially worse without any condensed dataset.
datasets sequentially. When training on a new dataset, the model is additionally trained on the condensed
versions of the previous datasets.
The continual learning experiment reproduces the setting from (Zhao & Bilen, 2021) to ﬁrst train on SVHN,
then on MNIST and ﬁnally on USPS (Hull, 1994), using the average accuracy across all three datasets of
the classiﬁer at the end of training as the ﬁnal accuracy (see (Zhao & Bilen, 2021) for details).
We created a simpler and faster continual training pipeline that achieves comparable results to Zhao &
Bilen (2021). First, we train 3 times for 50 epochs on SVHN, with a cosine annealing learning rate schedule
(Loshchilov & Hutter, 2017) that is restarted at each time with lr= 0.1. Then for each MNIST and USPS,
we train one cosine annealing cycle of 50 epochs for lr= 0.1.
We ﬁrst veriﬁed that we can reproduce the prior continual learning results with our simpler training pipeline
and ﬁnd that our training pipeline indeed even slightly outperforms the reported ﬁnal results (96.0% vs.
95.2% with, and 95.4% vs 93.0% without knowledge distillation) despite slightly inferior performance in the
ﬁrst training stage (before any continual learning, 93.6% vs 94.1%), see following subsection. When using
diﬀerent SVHN and MNIST condensed datasets, we ﬁnd that we can retain the original continual learning
accuracies even with condensed datasets with substantially less ( ∼9x less) bits per dimension (see Fig.S15).
Without Condensed Dataset Our training pipeline still exhibits forgetting when not using any condensed
datasets of previously trained-on datasets. As Figure S11 shows, the accuracies are far lower than with just
regular sequential training. We performed this ablation to ensure forgetting still occurs in our training
pipeline.
S23Under review as submission to TMLR
Figure S12: Dataset condensation results with varying simpliﬁcation loss weight. Top:Individual dots
represent accuracies for setting with diﬀerent simpliﬁcation loss weights. Accuracies can be retained even
with substantially reduced bits per dimension. For 1 image per class, arrows highlight the settings that are
visualized below. Below: Condensed datasets with varying simpliﬁcation loss weight. Per dataset, showing
condensed datasets with high (top row) and low (bottom row) bits per dimension. Lower bits per dimension
datasets are visually simpler and smoother while mostly retaining accuracies.
S24Under review as submission to TMLR
Instance
norm
ReLU  
Instance
norm
ELU  
No
norm
ReLU  
No
norm
ELU  22.5% Acc.
4.35 BPD  
22.7% Acc.
4.31 BPD  27.3% Acc.
4.35 BPD  28.7% Acc.
4.35 BPD  
Figure S13: Dataset condensation on CIFAR10 with varying architecture.
Grad
Distance,
1 Outer
step 
Grad
Product
1-step
Unrolling
Grad
Product,
ReLU,
4 Outer
Steps26.4% Acc.
5.24 BPD  
26.9% Acc.
0.37 BPD  29.1% Acc.
6.47 BPD  28.7% Acc.
4.35 BPD  
Grad
Product,  
4 Outer
Steps27.6% Acc.
1.6 BPD  
Figure S14: Dataset condensation on CIFAR10 with varying condensation loss and varying outer loop steps,
i.e. how many steps the classiﬁer is trained at each training epoch (default 1 in the 1 image per class setting),
after each step the condensation loss is again optimized.
S25Under review as submission to TMLR
SVHN SVHN+
MNISTSVHN+
MNIST+
USPS
Training Stage9293949596Accuracy [%]
 (average across datasets
 seen during training)
Continual Learning Results
ours w. KD SVHN bpd: 11.9, MNIST BPD: 4.3
ours w.o. KD
ours w. KD SVHN bpd: 3.9, MNIST BPD: 2.5
ours w.o. KD
ours w. KD SVHN bpd: 1.0, MNIST BPD: 0.8
ours w.o. KD
Orig w. KD
Orig w.o. KD
Figure S15: Continual Learning Results. Results for ﬁrst training on SVHN, then MNIST and then USPS for
condensed datasets with varying bits per dimension. Solid lines are with and dashed lines without knowledge
distillation. Note that continual learning accuracies remain similar also for substantially reduced bits per
dimension. Ablations show that accuracies degrade without any condensed dataset, see supplementary.
S26