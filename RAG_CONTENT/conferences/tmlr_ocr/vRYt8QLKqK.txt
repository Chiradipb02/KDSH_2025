Under review as submission to TMLR
Building Blocks for Robust and Effective Semi-Supervised
Real-World Object Detection
Anonymous authors
Paper under double-blind review
Abstract
Semi-supervised object detection (SSOD) based on pseudo-labeling significantly reduces
dependence on large labeled datasets by effectively leveraging both labeled and unlabeled
data. However, real-world applications of SSOD often face critical challenges, including
class imbalance, label noise, and labeling errors. We present an in-depth analysis of SSOD
under real-world conditions, uncovering causes of suboptimal pseudo-labeling and key trade-
offs between label quality and quantity. Based on our findings, we propose four building
blocks that can be seamlessly integrated into an SSOD framework. Rare Class Collage
(RCC): a data augmentation method that enhances the representation of rare classes by
creating collages of rare objects. Rare Class Focus (RCF): a stratified batch sampling
strategy that ensures a more balanced representation of all classes during training. Ground
Truth Label Correction (GLC): a label refinement method that identifies and corrects
false, missing, and noisy ground truth labels by leveraging the consistency of teacher model
predictions. Pseudo-Label Selection (PLS): a selection method for removing low-quality
pseudo-labeled images, guided by a novel metric estimating the missing detection rate while
accounting for class rarity. We validate our methods through comprehensive experiments on
autonomous driving datasets, resulting in up to 6% increase in SSOD performance. Overall,
our investigation and novel, data-centric, and broadly applicable building blocks enable
robust and effective SSOD in complex, real-world scenarios. Code will be released upon
publication.
1 Introduction
Training robust object detectors for real-world applications is challenging due to the high cost of obtaining
large labeled datasets with accurate labels. Semi-supervised learning (SSL) offers a promising solution by
leveraging both labeled and unlabeled data. Existing SSL methods often utilize a teacher-student framework,
where a teacher model trained on labeled data generates pseudo-labels for unlabeled data, which are then
used together to train a student model (Li et al., 2020b; Sohn et al., 2020; Yang et al., 2021; Xu et al., 2021;
Liu et al., 2021; Zhang et al., 2022; Liu et al., 2022b; Mi et al., 2022; Li et al., 2022c). While effective, the
potential of SSL is often hindered by the quality of both labeled and pseudo-labeled data.
We identify three primary challenges that affect label quality and, consequently, the effectiveness of pseudo-
labeling-based SSL frameworks. These are: (1) class imbalance in both labeled and pseudo-labeled data,
which leads to poor generalization on underrepresented classes; (2) noise in ground truth labels, including
incorrect or missing labels, which compromises training; and (3) incomplete or inaccurate pseudo-labels
generated by the teacher, particularly for underrepresented classes, which propagate errors to the student.
To address these challenges, we propose the following four novel building blocks aimed at increasing the
effectiveness and robustness of SSOD by enhancing label quality, utilization, and class balance.
First, the natural occurrence of objects in the real-world leads to an imbalanced class distribution within
each image. Re-sampling entire images only reinforces this imbalance, as each image retains its inherently
unbalanced class distribution (Chang et al., 2021). Therefore, we introduce Rare Class Collage (RCC), which
enhances the representation of underrepresented classes by creating a collage of cropped rare object images.
1Under review as submission to TMLR
Second, while upsampling rare classes helps mitigate the imbalance, their infrequent occurrence in random
training batches prevents the model from retaining the learned features. We propose Rare Class Focus
(RCF), which ensures consistent exposure to rare classes by including at least one rare example in each
training batch. Our method departs from the standard random sampling strategy, enhancing the ability of
the model to handle challenging rare cases.
Third, labeled data is often implicitly assumed to be flawless, although ground truth label errors can range
from 10% to 40% (Northcutt et al., 2021; Seedat et al., 2024), significantly hindering performance (Kuan &
Mueller, 2022). Manual inspection to identify mislabeled data is time-consuming, error-prone, and frequently
impractical due to the large dataset sizes (Tkachenko et al., 2023). We investigate the effect of noisy, false,
and missing ground truth labels on performance by simulating two levels of errors. Our finding motivate
the introduction of our Ground Truth Label Correction (GLC) method to refine noisy or incomplete ground
truth labels by leveraging inference-time augmentation and teacher model predictions.
Finally, detections are usually filtered based on a score (Li et al., 2022b; Xu et al., 2021), with a trade-off
between recall and precision (Zhang et al., 2022). For example, Xu et al. (2021) observe that setting a
high threshold, such as 0.9 (Sohn et al., 2020), results in high precision. However, it also results in low
recall, leading to many missed detections. In contrast, setting a lower threshold improves recall but retains
many false detections. Although two-stage filtering, dynamic thresholding (Cai et al., 2021; Li et al., 2022c;
Chen et al., 2023a), or using uncertainty as a score (Munir et al., 2021; Cai et al., 2021), can help reduce
this trade-off, the resulting missing detections are still not addressed. Typically, all pseudo-labeled images
are used to train the student model after the score-based filtering despite missing or false detections. This
introduces noise and hinders effective learning (Xu et al., 2019; Yang et al., 2020). We therefore introduce
Pseudo-Label Selection (PLS), a method that estimates the proportion of missing detections in an image
using a novel metric. The metric also accounts for class distribution, ensuring that images with high missing
detections are retained if they contain valuable learning signals from rare classes. This approach enables the
filtering of low-quality pseudo-labeled images that could otherwise negatively impact model performance.
Our building blocks are designed to be as model- and framework-agnostic as possible, ensuring broad applica-
bility to many SSOD frameworks and object detection tasks. Additionally, they add minimal computational
overhead, ensuring feasibility for large-scale, real-world applications. Due to high labeling costs and abun-
dant unlabeled data, SSL is crucial in real-world applications. Although some works (Cai et al., 2021; Han
et al., 2021; Munir et al., 2021) explore the use of pseudo-labels for domain adaptation in autonomous driv-
ing, comprehensive analyses of the efficacy of pseudo-labeling remain limited. Our work addresses this gap
by analyzing the key limitations of SSOD and proposing improvements that result in more effective and
robust real-world SSOD. Our contributions are summarized as follows:
•We investigate the limitations of pseudo-labels in SSOD on real-world datasets, identifying factors
hindering its effectiveness: (1) error propagation due to class imbalance, (2) noisy and erroneous
ground truth labels, and (3) missing and false pseudo-labels.
•Based on the resulting insights, we develop novel and computationally efficient building blocks, each
of which can be independently integrated into an SSOD framework to:
–Balance the class distribution and mitigate error propagation on underrepresented classes.
–Assess the impact on performance by varying levels of noisy, false, and missing detections in
ground truth labels and correcting them.
–Remove low-quality pseudo-labeled images post-filtering while considering class distribution.
2 Related Work
Class Amplifiers: Class imbalance often leads to rare classes being filtered out from pseudo-labels due to
them receiving low confidence scores. The effectiveness of re-sampling, i.e., adjusting frequency (Yu et al.,
2022; Chang et al., 2021), and re-weighting, i.e., adjusting sample weights in the loss function (Tantithamtha-
vorn et al., 2018; Cui et al., 2019; Li et al., 2020a; Yu et al., 2022), strongly varies depending on detector
type (Crasto, 2024). Re-weighting by inverse class frequency often leads to poor performance, particularly
2Under review as submission to TMLR
in highly imbalanced datasets where noisy rare samples receive disproportionately high weights (Phan &
Yamamoto, 2020). On the other hand, synthetic re-sampling (Chawla et al., 2002; Chen et al., 2023b) and
class-aware sampling (Shen et al., 2016) increase the frequency of rare classes artificially, reducing perfor-
mance on common classes. Furthermore, entire image collages (Chen et al., 2020; Ly et al., 2023; Chen et al.,
2023b) downscale images to increase object density, while copy-pasting approaches (Wang et al., 2018; Hong
et al., 2019; Yan et al., 2022) can compromise spatial and contextual consistency. Ghiasi et al. (2021) demon-
strate the effectiveness of copy-pasting for segmentation. However, object detection labels are rectangles that
include background along with the object. This contrasts with instance segmentation, where objects can be
seamlessly integrated into new images. Shen et al. (2016) ensure uniform class representation in the training
batches for classification, but such a representation is challenging due to strongly imbalanced real-world
data. Other class re-balancing techniques improve recall but may decrease precision due to overfitting on
rare classes (Tantithamthavorn et al., 2018). To address these challenges, our Rare Class Collage (RCC)
and Rare Class Focus (RCF) approaches increase the representation of rare classes while preserving spatial
context. They ensure consistent and balanced exposure to rare classes during training without sacrificing
performance on common classes.
Label Filters: Confirmation bias, where models overfit to incorrect labels, remains a critical challenge in
SSOD (Arazo et al., 2020; Wang et al., 2021). Noisy and incomplete labels hinder effective learning. While
several works attempt to mitigate pseudo-label noise using dynamic thresholding (Munir et al., 2021; Chen
et al., 2022; Li et al., 2023; Chen et al., 2023a; Kimhi et al., 2024), consistency-based filtering (Li et al.,
2022b; Yang et al., 2021; Yan et al., 2022), uncertainty-weighted filtering (Cai et al., 2021) or iterative
refinement through multi-iteration predictions (Wang et al., 2018), these approaches often overlook noise
in the ground truth labels, assuming perfectly labeled data. Seedat et al. (2024) use aleatoric uncertainty
to identify noisy ground truth labels. Zhou et al. (2023) refine ground truth boxes by aggregating region
proposals in two-stage detectors. These methods generally focus on box refinement and do not address
missing or false labels. In contrast, inspired by consistency-based pseudo-label refinement (Li et al., 2022b)
and consistency regularization (Li et al., 2022a), our Ground Truth Label Correction (GLC) mechanism
simultaneously addresses noisy, missing, and false detections by evaluating ground truth labels through
teacher predictions under inference-time augmentation.
As for pseudo-label quality, confidence-based filtering is typically used to retain high quality pseudo-labels
(Liuetal.,2021), buthigh-confidencepredictionsdonotalwaysindicateaccuratedetections(Lietal.,2020b),
and low-confidence scores may not always imply incorrect detections. The filtering process often leads to
missing detections and noisy pseudo-labels remaining in the dataset, which amplifies errors over successive
training iterations (Wang et al., 2021). The balance between the quality and quantity of pseudo-labels
remains an unresolved challenge, as highlighted by conflicting works regarding the benefits of large volumes
of pseudo-labeled data (Wang et al., 2021; Sohn et al., 2020). Wang et al. (2021) and Yang et al. (2021)
argue that the volume of unlabeled data needs to be considered carefully, as more data does not necessarily
lead to better performance. In contrast, Sohn et al. (2020) highlight the importance of large-scale unlabeled
data in the context of SSL and do not observe a clear correlation between the accuracy of the pseudo-labels
and the performance of their SSL approach. Our work investigates the impact of quality vs. quantity on
performance. Furthermore, the impact of missing detections remains unclear. Xu et al. (2019) show that the
performance of detectors declines significantly as the rate of missing detections increases. In contrast, Wu
et al. (2018) and Chadwick & Newman (2019) argue for the robustness of detectors to it. We show that the
impact of missing detections on performance depends on the class of the missing objects. Building on this
finding, we propose Pseudo-Label Selection (PLS). PLS estimates the missing detection rate per image while
accounting for class distribution, allowing for targeted selection of pseudo-labeled images post-filtering.
3 SSOD Building Blocks
In this section, we begin by outlining the student-teacher pseudo-labeling framework and then introduce our
proposed building blocks. An overview of their integration into an SSOD framework is illustrated in Fig. 1,
using the STAC framework (Sohn et al., 2020) as a representative example.
3Under review as submission to TMLR
RCC RCF
PLS GLCLabeled DataTeacher
StudentClass Amplifiers
Label Filters
RCC: Rare Class Collage
RCF: Rare Class Focus
PLS: Pseudo-Label Selection
GLC: Ground Truth Label Correction
Unlabeled Data
Batch
0.1
0.4
0.7
0.911
4
3
2
Ground Truth Flip
 Noise
 Blur
Pseudo-Labeled Data
Collage
Crop
Common
Original
Rare
Original + Augmented
Noisy
Label
Missing
Label
False
Label
Consistency
Prediction
Figure 1: Our building blocks integrated into an exemplary SSOD framework. The teacher model MT,
trained on labeled data, generates pseudo-labels for unlabeled data, which are then filtered by a confidence
thresholdδs. To address class imbalance, Rare Class Collage (RCC) (Section 3.2.1) crops instances of
rare classes and combines them into collages, increasing their representation. Rare Class Focus (RCF)
(Section 3.2.2) ensures each training batch contains common and rare classes, with augmented rare class
images to boost their impact. Ground Truth Label Correction (GLC) (Section 3.3.1) corrects false,
missing, and noisy labels by utilizing teacher prediction consistency across augmentations. Pseudo-Label
Selection (PLS) (Section 3.3.2) removes pseudo-labeled images with many missing detections, estimated
using our metric Di(δs,β), which incorporates detection confidence and class rarity. Together, our methods
enhance the ability of the student model MS(δs)to learn effectively from both labeled and pseudo-labeled
data, minimizing the propagation of errors from the teacher model.
3.1 Pseudo-Labeling
In SSOD, we are given a labeled dataset Dlabeled ={(xi,bi,ci)}m
i=1of imagesxi, bounding boxes biand class
labelsci, fori= 1,...m; and a larger unlabeled dataset Dunlabeled ={xi}l
i=1, wherel≫m. The objective
is to train a student model MSusing both labeled data and pseudo-labels, with the latter generated by a
teacher model MTfor the unlabeled data. Formally, the teacher model MT, trained onDlabeled, generates
pseudo-labels for Dunlabeled, resulting in a pseudo-labeled dataset Dpseudo ={(xi,ˆbi,ˆci)}l
i=1, where ˆbiandˆci
represent the predicted bounding boxes and class labels. Following Sohn et al. (2020), pseudo-labeled images
are augmented (A), and the student model MS(δs)is trained by minimizing the total loss given by
ℓtotal=1
mm/summationdisplay
i=1ℓlabeled (xi,bi,ci) +λ·1
ll/summationdisplay
i=1I(s≥δs)ℓpseudo (A(xi),ˆbi,ˆci), (1)
whereℓlabeledandℓpseudoare the default loss terms of the detector calculated on DlabeledandDpseudo,
respectively, and Iis an indicator function that selects pseudo-labels based on their predicted confidence
4Under review as submission to TMLR
scores. Formally,I(s) = 1if the confidence score s≥δsand zero otherwise. The hyperparameter λ,
controlling the balance between the loss terms, is best set as λ∈[1,2](Sohn et al., 2020).
3.2 Class Amplifiers
To alleviate class imbalance, we introduce two methods that boost the representation of rare classes in
Dlabeledand ensure their consistent inclusion during training for a balanced learning across all object classes.
3.2.1 Rare Class Collage (RCC)
RCC addresses class imbalance by increasing the representation of rare classes through collage generation.
Rare objects identified in the ground truth labels are cropped and upscaled to fit the height of a new collage
image, while preserving their original aspect ratios. These resized objects are arranged side by side on a
blank canvas to create a collage primarily composed of rare class objects.
LetCrdenote the manually defined set of rare object classes. For each object of a class ∈Crin an image
xi∈ D labeled, we crop an area around its bounding box br= [bx
r,by
r,bw
r,bh
r], wherebx
randby
rare the
top-left coordinates, and bw
randbh
rrepresent the width and height. To determine the area, we sample a
random scale factor prfrom a uniform distribution U(γr,min,γr,max), resulting in an expanded bounding box
b′
r= [max(0,bx
r−prbw
r),max(0,by
r−prbh
r),min(w,bx
r+ (1 +pr)bw
r),min(h,by
r+ (1 +pr)bh
r)], wherewandh
are the width and height of the original image. This ensures that the cropping area based on the resized
bounding box remains within the image boundaries.
After cropping, the rare objects are shuffled, resized, and sequentially pasted into a new collage image x′
i.
The newly generated collages are added to Dlabeled, thereby increasing the representation of rare classes.
Unlike previous works, RCC ensures improved generalization across specifically targeted rare classes without
compromising performance on more common classes. It is also computationally efficient, as it processes only
the images and their corresponding labels in Dlabeled.
3.2.2 Rare Class Focus (RCF)
Class imbalance in real-world data leads to a non-uniform distribution of classes across training batches,
resulting in uneven gradient updates and limiting the ability of the detector to learn from rare class examples
effectively. RCC increases the overall representation of rare classes, but it does not guarantee their balanced
presence within each training batch. RCF further mitigates class imbalance by ensuring each training batch
contains at least one example of a rare class. Unlike RCC, which explicitly defines rare classes, RCF stratifies
Dlabeledinto common and rare examples based on class frequency, adjusting batch composition to include
rare examples consistently. For that, it assigns a score Fito each image xibased on its class distribution.
For an image xi, containing jclasses, the score is computed as
Fi=1
jj/summationdisplay
k=11
log(fk), (2)
wherefkisthefrequencyof class kinDlabeled. This inverse-logarithmicformulationensuresthatrarer classes
receive higher scores, emphasizing their importance in the learning process (Phan & Yamamoto, 2020). The
class scores are then linearly scaled within a predefined range [1,γf], whereγfis the maximum score that
controls the scaling strength.
To construct training batches, the score Fiis computed for all images in Dlabeled. Images are ranked based
on their scores, and the top kimages are classified as rare, while all others are considered as common. The
value ofkis selected such that each batch contains at least one rare image, satisfying the condition k≥m
B,
wheremis the total number of images in DlabeledandBis the batch size. Additionally, the rare set of
images is augmented and shuffled twice to introduce diversity by adding a pair of rare samples to each batch.
This ensures the batch size remains a multiple of two, ensuring efficient GPU utilization and stable batch
normalization. Our RCF approach promotes balanced learning, providing adequate focus on rare classes,
while preserving the ability of the model to generalize across all classes. Similar to RCC, the steps in RCF
incur minimal computational overhead, as they operate solely on Dlabeled.
5Under review as submission to TMLR
3.3 Label Filters
The quality of labels, both ground truth and pseudo-labels, is crucial to model performance. Leveraging
the knowledge of the teacher model before training the student model helps prevent error propagation from
label errors.
3.3.1 Ground Truth Label Correction (GLC)
GLC is designed to refine noisy labels, remove false labels, and add missing ground truth (GT) labels by
leveraging a trained teacher model and inference-time augmentations on the training set. This approach
acknowledges that GT labels are not always reliable and aims to prevent erroneous GT labels from propa-
gating through the training process to the student model. Specifically, the teacher model generates bounding
box predictions on the original training images and their augmented versions post-training. They are then
compared against GT labels to identify and correct errors before training the student.
Typical augmentations such as Gaussian blur, horizontal flipping, and Gaussian noise can be employed. The
computational cost of GLC scales linearly with the number of augmentations, starting at a minimum of twice
the base inference time. Corrections are determined based on the Intersection over Union (IoU) between
predicted boxes ˆband GT boxes b. The core intuition is that consistent predictions from the teacher model
across augmented versions of an image – despite the teacher not being trained on these augmentations –
indicate the reliability of its detections. This consistency serves as an indicator of whether the detections
may be used to correct GT errors. We define three error cases:
•False GT: A GT label bthat does not intersect with any prediction, even at a low confidence
thresholdδs= 0.1, likely indicates a false label. In one-stage object detectors, anchors are defined
densely across the grid; thus, any valid GT label should intersect with at least one anchor if there
is a recognizable feature. Similarly, in two-stage detectors, the region proposal network generates
region proposals around potential object locations, so any valid GT label should intersect with at
least one proposal region if a relevant feature is present.
•Missing GT: If a predicted bounding box ˆbappears consistently despite augmentations
(µIoU(ˆb,A(ˆb))> γc) but has no corresponding GT label, a GT label is likely missing for that ob-
ject. Here, µis the mean IoU between detections on the original and augmented images, and γcis
a predefined consistency threshold.
•Noisy GT: Typically, each GT box bis paired with a predicted box ˆbbased on the highest IoU,
provided the IoU exceeds the default threshold of 0.5. However, if the IoU between the predicted
and GT box falls below an upper threshold (IoU (ˆb,b)<γo), but the predicted box shows consistency
despite augmentations ( µIoU(ˆb,A(ˆb))> γc), then the predicted box is considered more accurate and
replaces the GT box.
For each of the three categories above, we simulate real-world labeling inaccuracies and evaluate the robust-
ness of the model and the effectiveness of our GLC approach under synthetic error conditions as follows.
For false GT, random bounding boxes are added to simulate false detections. For each image, random boxes
˜bare generated, with width and height sampled from uniform distributions, ˜bw∼U(γ˜bw,min,γ˜bw,max)and
˜bh∼U(γ˜bh,min,γ˜bh,max), placed in non-overlapping positions relative to the existing GT boxes.
For missing GT, a percentage ρMGTof GT bounding boxes are randomly removed to simulate missing labels.
The removed boxes are selected uniformly across Dlabeled.
Finally, for noisy GT, bounding boxes are randomly perturbed in their dimensions and position. For a GT
boxb= [bx,by,bw,bh], the width and height are altered by bw′=bw+ ∆bwandbh′=bh+ ∆bh, where ∆bw
and∆bhare sampled from {−ϵb,ϵb}. The center of the box is also shifted: bx′=bx+∆bxandby′=by+∆by,
where ∆bx=±∆w
2and∆by=±∆h
2.
6Under review as submission to TMLR
3.3.2 Pseudo-Label Selection (PLS)
As outlined in Section 1, regardless of the choice or sophistication of the threshold δswhich regulates the
precision-recall trade-off in pseudo-labeling, pseudo-labeled images inherently contain missing detections. By
estimating the missing detection rate (MDR) for each image, PLS identifies which pseudo-labeled images to
remove after initial filtering with δs.
Wehypothesizethatthedistributionofconfidencescoreswithinanimageisanindicatorofmissingdetections.
Specifically, the number of detections per image at different thresholds δs∈[0,1]reflects the reliability of
the detector on that image, correlating with the potential of missing detections. Given a confidence score
thresholdδs, the number of detections at that threshold is denoted ni(δs). We define the score metric for an
imagexias
Si(δs) =ni(δs)
ni(α),
whereni(α)represents the number of detections at a low reference threshold (e.g., α= 0.1), providing a
baseline for the total number of detections in an image. The value of Siincreases as the potential of missing
detections decreases.
To accommodate class imbalance, we recognize that pseudo-labeled images with a high MDR may still
contain valuable learning signals if they include rare objects. To address this, we introduce a regularizing
class metric Ciwhich considers class rarity. To keep Ciin the range [0,1], it is calculated as
Ci=1
jj/summationdisplay
k=1/parenleftbigg
1−fk/summationtextm
i=1ni/parenrightbigg
,
wherejandniare the number of classes and predictions in an image xi, respectively, and fkis the frequency
of classkinDpseudo. The value of Ciincreases as the rarity of the predicted objects increases.
The overall PLS metric Difor an image xiis then given by
Di(δs,β) = (1−β)·Si(δs) +β·Ci, (3)
whereβis a weighting factor balancing the contributions of missing detections and class rarity. The value
ofDiincreases as the potential of missing detections decreases and class rarity increases. Therefore, images
with lowDiscores are filtered out to improve the quality of pseudo-labels used during student training.
Eq. (3) offers a computationally efficient solution for pseudo-labeled images selection as it relies solely on
the pseudo-labels.
4 Experiments
4.1 Implementation Details
To demonstrate the effectiveness of our data-centric methods, we select EfficientDet-D0 (Tan et al., 2020;
Google, 2020) pre-trained on COCO (Lin et al., 2014) as our baseline detector. Experiments are conducted
on two autonomous driving datasets: KITTI (Geiger et al., 2012) (7 classes, 20% random split for validation)
and BDD100K (Yu et al., 2020) (10 classes, 12.5% official split). Each training comprises 200 epochs with
8 batches and an input image resolution of 1024 ×512 pixels. All other hyperparameters of EfficientDet are
set to their default values (Tan et al., 2020).
Our methods are also framework-agnostic. As an exemplary student-teacher framework, this work adopts
STAC (Sohn et al., 2020) with λ= 1in Eq. (1) due to its simplicity. For data augmentations ( A), we
use RandAugment (Cubuk et al., 2020). Using AutoAugment (Cubuk et al., 2019) instead yields identical
performance. We continue to denote the teacher model as MTand the student model as MS(δs), which
depends on the score threshold δs. We observe a minimal inter-run variance and therefore average over a
total of three training runs. We evaluate each proposed method individually to isolate its specific impact on
SSOD performance.
7Under review as submission to TMLR
4.2 SSOD Framework
Impact of δsand Amount of Labeled Data: We begin our experiments by examining key fac-
tors that influence the effectiveness of SSOD frameworks. These include the choice of confidence score
threshold ( δs) and the ratio of labeled to unlabeled data. Fig. 2 shows a strong correlation between
the choice of δsand the amount of labeled data used during training. As the quantity of labeled
data increases, the pseudo-labels generated by the teacher model become more reliable, enabling the use
of a higher δs. While a lower threshold ( δs= 0.4) is optimal for setups with 15% labeled data in
KITTI and 10% in BDD, a higher threshold ( δs= 0.7) outperforms the latter at 25% labeled data.
Figure 2: KITTI (left), BDD (right). Impact of the re-
lationship between the confidence score threshold ( δs)
and the proportion of labeled data on performance.
A higher proportion of labeled data allows an effec-
tive increase in δs. A misconfigured δsrelative to the
available labeled data results in a student ( MS) that
underperforms its teacher ( MT).However, misconfiguring δs, such as selecting 0.9 for
10% of BDD labeled, results in a high MDR since
many pseudo-labels get filtered out. This leads to a
significant performance drop of 3%, suggesting that
both the amount of labeled data and the choice
ofδsmust be carefully calibrated (also relative to
each other) to achieve optimal performance. While
SSLimprovesthemeanAveragePrecision(mAP)on
both datasets by up to a 2%, ineffective threshold-
ing can lead to its degradation for MSby up to 3%
compared to MT. The latter is most pronounced
when labeled data is significantly limited, and both
labeled and pseudo-labeled data are noisy and un-
balanced, as demonstrated by the consistent under-
performance of MSat 1% of BDD labeled. This can
be attributed to poor-quality pseudo-labels gener-
ated in such sparse settings, as shown in Table 1.
Toavoidmisconfiguring δs, wegenerallyrecommend
based on our experiments to calculate the average
score across matched detections with GT labels in
the validation set to effectively select the minimum δsfor pseudo-labels. We select δs= 0.4as the default
value since it aligns with the default setting of EfficientDet (Tan et al., 2020) and matches the average score
(0.36±0.1) across correctly matched detections on the validation sets of both KITTI and BDD.
Impact of Pseudo-Label Quality: We continue our experiments on the SSOD framework by investi-
gating the quality of pseudo-labels and its impact on performance. We identify three primary factors that
affect it: noisy ( ND), false ( FD), and missing detections ( MD). As shown in Table 1, MTachieves an mAP
of 47.16% on KITTI and 14.43% on BDD. The default MS(0.4)shows an increase of 1.29% on KITTI but
a decrease of 0.97% on BDD. This poor performance on BDD is a result of poorer pseudo-label quality,
as demonstrated by metrics such as classification accuracy (mACC), mean Intersection over Union (mIoU),
MDR, and uncategorized detection rate (UDR). MDR represents instances that are not detected, while UDR
refers to detections that may be correct but lack a corresponding GT label or are entirely false.
The quality of pseudo-labels has a more significant impact on student model performance than their quantity.
Despite the number of detections per image ( ni) being approximately 23 times higher in BDD compared to
KITTI, the quality of pseudo-labels remains crucial. The 12 times larger number of unlabeled images ( l) in
BDD at a comparable number of labeled images ( m, 598 for KITTI vs. 698 for BDD) could not compensate
for lower pseudo-label quality.
Further analysis shows that the influence of ND,FD, and MDin pseudo-labeled data is closely tied to the
quality of the GT labels. On KITTI, which features higher-quality images and labels, removing NDs leads to
an approximate 4% increase in mAP. However, excluding images with MDs lowers performance by reducing
the total number of images linDunlabeled and detections ni. This reduction compromises the reliability of
ℓpseudoand disrupts the balance between ℓlabeledandℓpseudo. Since both losses are weighted equally in the
total loss (Eq. (1)), the negative impact of the low quality of the pseudo-labels is further amplified by their
limited quantity. Pseudo-label quality generally has a more significant impact than quantity, but quantity
8Under review as submission to TMLR
Table 1: KITTI (top, 10% labeled), BDD (bottom, 1% labeled). Impact of pseudo-label quality on perfor-
mance, with NDs,FDs, and MDs removed (×) from the pseudo-labels. A dash (-) indicates non-applicability.
NDFDMDmAP MDR UDR mACC mIoU nim+l
(%) (%) (%) (%) (%)
MT- - - 47.16 - - - - 3069 598
MS(0.4)- - - 48.45 23.26 7.10 97.24 85.92 23453 5866
MS(0.4)×- - 52.21 29.54 6.66 99.30 95.97 23453 5866
MS(0.4)-×- 48.95 23.26 0.00 97.24 85.92 21789 5861
MS(0.4)- -×47.47 00.00 4.92 97.83 87.40 6540 2668
MT- - - 14.43 - - - - 12365 698
MS(0.4)- - - 13.46 64.03 14.63 96.36 81.15 528818 69021
MS(0.4)×- - 13.59 64.03 14.63 98.83 90.30 528818 69021
MS(0.4)-×-13.87 64.03 00.00 96.36 81.23 450202 69021
becomes crucial when GT label quality is high. Furthermore, some pseudo-labeled images remain valuable
even if they contain MDs, particularly when they contain rare objects as demonstrated in Section 4.4. Mean-
while, removing FDs from BDD boosts performance more than removing NDs, underscoring the challenges
of noisy datasets. In BDD, no images exist without MDs, thereby preventing an analysis of the impact of
their complete absence.
These observations emphasize the need to maintain a careful balance between noise, precision, and recall in
pseudo-labels based on the characteristics of Dlabeled. We also test advanced pseudo-label selection strategies,
such as uncertainty-based thresholding with class-specific weighting, but observe no direct gains in mAP over
usingδsfor filtering. This is primarily due to the rarity of some classes in the labeled set, which adversely
impacts the performance of MTon them, hence propagating errors to MS. As a result, for SSL to be
effective in object detection, a class-specific approach is necessary, motivating the development of our Rare
Class Collage (RCC) and Rare Class Focus (RCF) methods.
Summary: our experiments reveal that optimal SSOD performance depends on careful calibration of the
confidence threshold δswith the amount of labeled data. Furthermore, pseudo-label quality is generally more
influential than quantity, unless Dlabeledis of high quality and MTis well-trained. KITTI benefits from
noise reduction in pseudo-labels, while BDD sees improvement with reduced false detections, underscoring
the importance of dataset-specific adjustments.
4.3 Class Amplifiers
Due to limited labeled data in SSL, class imbalance significantly impacts model performance. In addition
to our proposed RCC and RCF methods, we evaluate two baselines that address this: classical image-level
re-sampling and re-weighting, as detailed in Section 2.
Figure 3: KITTI (left, 10% labeled), BDD (right, 1% labeled). Class frequency fkfor each class inDlabeled.
Imbalance Strength: Class imbalance is evident in Fig. 3, showing the frequency fkof classes in KITTI
and BDD. The imbalance spans a 260-fold difference between the most and least frequent classes. In RCC,
we target the rare classes “person sitting” and “tram” for KITTI, and “rider”, “motorcycle”, and “bicycle”
9Under review as submission to TMLR
for BDD to boost their representation. An example collage is visualized in Fig. 4 for each dataset. Unlike
RCC, RCF does not explicitly target specific classes but instead stratifies Dlabeledby using a class score that
accounts for the frequency of all classes, as defined in Eq. (2).
Figure 4: KITTI (left), BDD (right). Example collages with scaling factors γr,min= 0.25andγr,max= 0.75.
Overall Performance of RCC and RCF: Table 2 highlights the importance of a per-class perspective
when analyzing SSOD performance. While both RCC and RCF improve MTperformance, only RCF
consistently benefits MS. RCC targets specific classes, resulting in a limited number of collage images
(18 for KITTI and 43 for BDD) based on our choice of classes. The impact of the additional images
becomes negligible during the training of MSdue to the thousands of additional pseudo-labeled images.
Table 2: KITTI (left, 10% labeled), BDD (right, 1%
labeled). Impact of RCF and RCC on mAP.
RCF RCC mAP
MT- - 47.16±0.43 14.43±0.18
MT✓-48.11±0.1514.65±0.16
MT-✓47.49±0.2915.00±0.06
MS(0.4)- - 48.45±0.18 13.76±0.04
MS(0.4)✓-49.56±0.13 14.27±0.05
MS(0.4)-✓48.41±0.11 14.26±0.15Furthermore, the class-specific improvements from
RCC have a less significant impact on mAP, as mAP
averages performance across all classes by default.
Nevertheless, the impact of class balancing through
our methods is clear: using RCC for targeted collag-
ingandRCFforstructuredbatchtraining, themAP
ofMTis improved by up to 1% from each approach
with no additional manual labeling or changes to
the core training process. Fig. 5 demonstrates that
RCC effectively increases the frequency of targeted
rare classes by approximately 300% while minimally
affecting the distribution of common classes (up to
3% increase). In contrast, re-sampling introduces a
greater bias toward common classes (up to 10% increase) while achieving a comparatively smaller boost in
the representation of rare classes (only 200%).
Re-sampleOriginal
Figure 5: KITTI. Class frequency fkafter applying
RCC vs. re-sampling of entire images. RCC increases
fkfor targeted classes (“person sitting” and “tram”)
without over-representing common classes.Per-Class Performance of RCC and RCF: We
investigate both methods from a per-class perspec-
tive. Table 3 compares the performance of RCC and
RCF against re-weighting (RW) and re-sampling en-
tire images (RS). For RW, we leverage Fiin Eq. (2)
to weight images in Dlabeledas described in Sec-
tion 2. We use the mean weight per image since al-
ternative approaches, such as per-detection weight-
ingandusingsumormaxvaluesperimage, wereless
effective. The higher mAP of RCF observed in Ta-
ble 2 is further supported by consistent performance
gains across individual classes. However, while RCF
requires changes to the data loader, RCC remains a
more straightforward, data-centric approach that consistently outperforms RS and RW on targeted rare
classes.
10Under review as submission to TMLR
Table 3: KITTI (top, 10% labeled), BDD (bottom, 1% labeled). Impact of RS, RW, RCF ( γf= 20), and
RCC (γr,min= 0.25,γr,max= 0.75) on per-class AP of MTand the total sum across all classes (/summationtext). Green
arrows ( ) indicate a performance increase, while red arrows ( ) denote a decrease compared to the mean
AP of the original MTacross the multiple runs (shown in the first row of each dataset). The values adjacent
to the arrows are the absolute differences to the mean APs. Colored columns highlight the classes targeted
by RS and RCC. Classes are ordered left to right from rarest to most common.
RS RW RCF RCCAP
/summationtext Person
SittingTram Truck Cyclist VanPedest-
rianCar - -
- - - - 22.89 43.64 60.14 45.03 51.70 36.80 65.99 - -
✓- - - 1.30 0.95 0.880.420.00 0.520.10 - - 0.33
-✓- - 2.72 2.64 1.080.260.76 0.320.31 - - 10.53
- - ✓-3.184.32 1.03 0.170.250.81 0.11 - - 9.15
- - - ✓4.92 1.59 1.050.440.99 0.230.30 - - 3.50
Motor-
cycleRider Bicycle Bus TruckPedest-
rianTraffic
LightTraffic
SignCar
- - - - 4.59 6.66 9.59 24.72 23.02 17.07 7.83 14.71 35.62
✓- - - 0.962.02 0.810.050.41 0.040.000.210.21 2.49
-✓- - 0.78 0.91 1.272.180.54 0.880.060.280.09 4.49
- - ✓-1.00 1.60 0.05 0.450.52 0.260.190.29 0.20 3.16
- - - ✓1.90 1.88 0.290.71 0.24 0.010.090.020.02 4.54
The impact on other classes varies across datasets. On KITTI, RCC reduces the AP on common classes by
up to 1%, while on BDD it consistently improves it by up to 1% across all classes (see Table 3). This variation
can be attributed to the strong model performance on KITTI, where additional collage-based augmentation
may introduce unnecessary variability. This is also evident in the total sum (/summationtext) across all classes, where
RCC achieves an additional 1% improvement in total AP on BDD compared to KITTI. On the other hand,
RCF enhances performance across all classes as it maintains the original object resolution. RW performs
best on KITTI (total increase of 10.53% in AP across all classes) but performs worse on BDD (decrease
of 4.49%), particularly struggling with common classes. This is due to its disregard for label noise when
weighting samples, and BDD containing higher noise levels compared to KITTI. Challenging samples are
assigned high weights, hindering learning on easier and more common samples. Meanwhile, RS marginally
improves the AP on rare classes by up to 1% at the cost of degrading it on common classes by up to
1%. This decline occurs because RS introduces challenging images that hinder the effective optimization
of Eq. (1). It is worth mentioning that we test re-sampling both with and without augmenting the images
using RandAugment (Cubuk et al., 2020). Given the consistent underperformance of RS compared to other
methods in Table 3, we exclude it from the analysis in Table 4. Furthermore, the “train” class is excluded
from the analysis on BDD because there is only one instance of a train in the labeled set at 1% and it is
occluded, resulting in an AP of 0% for that class irrespective of the class-balancing approach used.
Both tables (Table 3 for MTand Table 4 for MS(0.4)) show similar trends with RCF yielding the highest
improvements for rare classes by up to 5% AP without significantly reducing performance on common
classes (less than 1% AP). This indicates that addressing class imbalance in Dlabeledis beneficial, regardless
of whether the training includes Dpseudo. However, the impact is diminished for MS(0.4)(9.15% vs 3.16%
total increase in AP across all classes), as the contribution of ℓlabeledto the overall training is reduced.
The limited improvement with RCC on the AP of the student (up to 2% vs. 5% for the teacher) can be
attributed to the small number (18 for KITTI) of generated collage images relative to the large pool of
added pseudo-labeled images (5866 for KITTI). Nonetheless, RCC still improves performance on targeted
rare classes. Meanwhile, RW significantly reduces performance, particularly for rare classes, with drops of
up to 3% in AP. Our findings support our hypothesis regarding error propagation: MS(0.4)underperforms
MTon certain inadequately-learned rare classes such as “motorcycle” in BDD, yet improves performance
on well-learned classes such as “car” and “traffic light”. Both RCF and RCC effectively mitigate this issue,
consistently improving performance on rare classes across both datasets while minimizing the impact on
common classes.
11Under review as submission to TMLR
Table 4: KITTI (top, 10% labeled), BDD (bottom, 1% labeled). Impact of RW, RCF ( γf= 20), and RCC
(γr,min= 0.25,γr,max= 0.75) on per-class AP of MS(0.4).
RW RCF RCCAP
/summationtext Person
SittingTram Truck Cyclist VanPedest-
rianCar - -
- - - 25.05 47.60 61.21 47.36 51.60 37.45 67.60 - -
✓- - 2.502.54 0.691.210.52 0.931.45 - - 3.72
-✓-3.265.03 2.130.34 0.011.310.09 - - 11.29
- - ✓ 1.050.05 1.210.420.62 0.370.34 - - 1.20
Motor-
cycleRider Bicycle Bus TruckPedest-
rianTraffic
LightTraffic
SignCar
- - - 4.13 5.62 7.48 23.89 22.78 13.63 9.14 15.37 36.55
✓- - 0.750.32 1.642.401.90 0.890.220.200.56 8.88
-✓-1.43 0.48 0.730.020.87 2.17 0.620.020.24 4.82
- - ✓ 0.911.24 0.400.250.36 0.710.100.250.07 3.59
Summary: class imbalance strongly impacts SSOD performance, particularly with limited labeled data, and is
effectively addressed by our RCC and RCF methods. RCC targets specific rare classes in KITTI and BDD,
while RCF achieves balanced representation across all classes during training. Therefore, if performance on
common classes is less critical, RCC is more beneficial. However, if maintaining performance on common
classes is a priority, RCF is the better choice. Both methods consistently outperform traditional re-sampling
and re-weighting, underscoring the critical role of per-class analysis when evaluating SSOD.
4.4 Label Filters
First, we investigate the impact of the quality of labeled data on performance and the effectiveness of our
GLC method. Second, we evaluate our proposed PLS metric and the advantages of pseudo-labeled images
selection post-filtering via δs.
Table 5: KITTI (left, 10% and 15% labeled), BDD
(right, 1% and 10% labeled). Total number of detec-
tions/summationtextm
i=1niinDlabeledand identified GT errors.
%/summationtextm
i=1niFGT MGT%/summationtextm
i=1niFGT MGT
103069 14 20 1 4661 20 20
1512365 380 123 10 126726 2772 1608GLC.We first evaluate the impact of missing
(MGT), false ( FGT) and noisy ( NGT) GT labels on
the performance of MT. Table 5 presents the num-
ber of discovered errors in KITTI and BDD using
GLC. Given their limited number in both datasets
(up to 3% of all detections), GLC only slightly im-
proves the mAP by around 0.5%. Examples of de-
tected GT errors and their correction are visualized
in Fig. 6 for both datasets.
Figure 6: KITTI (left), BDD (right). Examples of GT label errors identified and corrected via our GLC
approach. Both human annotators and model-assisted labeling tools often fail to label distant or occluded
objects. Moreover, they frequently make errors, such as drawing tiny, incorrect bounding boxes scattered
across the images or misplacing boxes near the image edges.
12Under review as submission to TMLR
Impact of GT Quality: To further understand the impact of GT quality on model performance, we
introduce two levels of synthetic GT errors as depicted in Fig. 7. This allows for a controlled analysis of
label errors, highlighting the sensitivity of the model to the quality of the labeled data and emphasizing the
advantages of our proposed GLC method. The two levels include:
Level 1 Level 2
Original
Modified
Figure 7: KITTI. Visualization of two levels of synthetic GT errors: Level 1 ( +) and Level 2 ( ++).
•Level 1 ( +):ρMGT= 20%dropped GT boxes, added one mistake per image ( γ˜bw,min= 10pixels
andγ˜bw,max= 100pixels), and noise applied to 20% of images with a perturbation factor ϵb= 0.1.
•Level 2 ( ++):ρMGT= 50%dropped GT boxes, added five mistakes per image ( γ˜bw,min= 10pixels
andγ˜bw,max= 100pixels), and noise applied to 20% of images with a perturbation factor ϵb= 0.2.
Table 6: KITTI (top, left 10%, right 15% labeled), BDD (bottom, left 1%, right 10% labeled). Impact of
synthetic errors on mAP of MTwithout (×) and with GLC ( ✓). The errors include noisy ( NGT), false
(FGT), and missing ( MGT) GT labels, evaluated at two intensity levels: Level 1 ( +) and Level 2 ( ++).
NGT FGT MGTmAP
× ✓× ✓
- - - 47.16±0.43 14.43±0.18
+- - 39.38±0.21 45.65±0.3211.78±0.61 13.57±0.01
++ - - 34.99±0.73 43.08±0.3711.32±0.18 13.54±0.02
- +-41.64±0.44 46.76±0.5814.17±0.07 14.12±0.24
- ++ -33.32±0.77 46.56±0.3313.02±0.08 14.17±0.02
- - +41.91±1.02 43.88±0.0212.68±0.22 13.39±0.21
- - ++29.43±0.18 39.88±0.8309.50±0.23 12.02±0.03
- - - 51.61±0.21 18.73±0.17
+- - 45.86±0.13 49.81±0.2015.85±0.02 17.05±0.83
++ - - 37.62±0.40 46.77±0.8915.06±0.34 16.99±0.71
- +-47.28±0.96 51.54±0.5618.25±0.01 18.88±0.04
- ++ -37.54±0.48 51.21±0.2617.25±0.13 18.81±0.17
- - +46.19±0.42 48.63±0.1516.65±0.20 17.21±0.11
- - ++32.75±0.93 42.89±0.3913.93±0.35 15.54±0.02
The performance impact of the selected error levels on MTis presented in Table 6. All error types substan-
tially reduce the mAP, with a reduction of up to 18%. MGTs have the most detrimental effect across all
tested data regimes and datasets. As for FGTs, even introducing a single error per image results in up to a
6% drop in mAP, demonstrating the high sensitivity of the model to GT errors. This model vulnerability
remains consistent irrespective of the data regime or dataset characteristics. For instance, despite KITTI
having four times fewer detections (3069 in 598 images) compared to BDD (12365 in 698 images, see Table 1),
the performance reduction is comparable at both levels. The mAP drops relatively to its original value by
17% and 38% on KITTI and 18% and 34% on BDD at Levels 1 and 2, respectively.
13Under review as submission to TMLR
Effectiveness of GLC: GLC mitigates the impact of GT label errors by correcting them before training
MSor for retraining MT. Correcting FGTs is found to be more straightforward than recovering MGTs due
to overlapping GT labels, as GLC assumes for the latter no overlap between consistent detections and GT
labels. As for NGTs, GLC restores significant performance, raising mIoU from 83.98% to 90.97% and mAP
from 39.38% to 45.65% on KITTI (10% labeled) after correction at Level 1.
Figure 8: KITTI. ROC curve for identifying images
with more than 50% MDs (δs= 0.9). Our metrics Si
andDioutperform baseline metrics such as the aver-
age score per image ( µs) and the number of detections
per image ( ni) (see AUC values).PLS.Given the importance of addressing MGTs
and the abundance of MDs inDpseudoof real-world
datasets such as BDD, we introduce PLS. The mIoU
ofMTtrained on 10% of KITTI is around 88% for
common classes such as “car”, but drops to 76% for
rare classes such as “person sitting”. Similarly, the
mACC is 100% for common classes vs. 96% for rare
ones. This disparity motivates the consideration of
the class distribution and the estimated MDR when
selecting pseudo-labeled images with PLS.
EvaluatingourPLSMetric: Fig.8demonstrates
the effectiveness of our proposed metric Siin iden-
tifying images with high MDs, achieving an AUC
of 90% on KITTI and 91% on BDD, significantly
outperforming the average score per image ( µs) at
74%, and the number of detections per image ( ni)
at 66%. As expected, class distribution does not correlate with the MDR (50% AUC). Still, it is benefi-
cial when incorporated into our PLS metric, as it allows for the inclusion of images containing rare classes
without compromising effectiveness in recognizing MDs. This is evidenced by Di(0.10)andDi(0.25)still
outperforming µsandnidespite the increase of β. The results are consistent across both KITTI and BDD,
underscoring the robustness of our approach.
Figure 9: BDD (1% labeled). PLS results for δs= 0.4
andβ= 0.1and0.2. We compare the original student
model to a student trained: on the remaining data
post-selection using Di, on randomly selected data of
equal size as the remaining data, and on the removed
data.Effectiveness of PLS: Fig. 9 summarizes the PLS
results. At 1% of BDD labeled and δs= 0.4, we
comparemAP,MDR,andUDRacrossdifferentcon-
figurations ( β= 0.1and0.2). We demonstrate that
PLS consistently outperforms random selection in
mAP and MDR/UDR, confirming that PLS effec-
tively identifies high-quality pseudo-labeled images.
PLS improves the performance of MS(0.4)to match
orevensurpasstheperformanceof MT(seeTable1)
by up to 0.5% mAP. Our method effectively filters
out images with high MDR and UDR (around 80%
and 16% in the removed set, compared to 60% and
14.5% in the remaining pseudo-labeled images), in-
dicating a correlation between our metric and both
the MDR and UDR.
Moreover, the model trained on the remaining 50%
ofDpseudopost-selectionusingour Dimetricoutper-
formstheoriginalstudenttrainedonthefull Dpseudo
and the students trained on a random 50% subset
or the removed 50%. Increasing βremoves fewer rare classes, further underscoring the importance of a
class-balanced perspective in SSL.
Fig. 10 qualitatively compares the lowest and highest scoring images based on Si. The contrast between
these examples provides insights into the effectiveness of PLS in discriminating between high-quality and
low-quality pseudo-labeled images, reinforcing its reliability in pseudo-label selection.
14Under review as submission to TMLR
D 
Figure 10: KITTI (left), BDD (right). For each dataset, the left image represents the highest Si, while the
right image represents the lowest Si(poor detection quality). Predictions (in green) are filtered at δs= 0.9.
Summary: GT and pseudo-label quality significantly impacts model performance. Synthetic errors reveal high
model sensitivity to GT label quality, with even minor errors causing notable performance declines. Our GLC
method effectively identifies and corrects GT label errors, while PLS filters low-quality pseudo-labeled images.
By preventing error propagation and leveraging the learned knowledge of MT, GLC and PLS improve the
effectiveness of SSOD and therefore MSperformance.
4.5 Ablation Studies
The following ablation studies present empirical validations for our parameter selection and the robustness
of our methods, alongside a comparison between the selected SSOD framework STAC and other SSL and
active learning (AL) frameworks.
SSOD Framework We compare the basic student-teacher SSL framework to consistency-based SSL,
active learning (AL), and uncertainty-based pseudo-label filtering methods. For consistency-based SSL, we
re-implement CSD (Jeong et al., 2019) for EfficientDet (Tan et al., 2020). For uncertainty-based methods,
we employ Loss Attenuation (Kendall & Gal, 2017; Kassem Sbeyti et al., 2023) and 2D spatial Monte Carlo
(MC) dropout (Tompson et al., 2015) with a dropout rate of 0.05 selected based on best performance and
10 MC samples.
ALinherentlyaddressesclassimbalancebyselectinguncertain,oftenunderrepresentedsamples. Forinstance,
it increases “person sitting” from 51 to 164 samples in Dlabeledat the first iteration (5% start and increase to
10%) compared to random 10% sampling. The increased representation results in an mAP boost of 5% for
MTand 6% for MS(0.4)over random sampling on KITTI, despite similar pseudo-label quality ( 80% MDR,
4% UDR). This underscores the potential of combining AL with SSL and the importance of class-aware
analyses and methods.
We observe a consistent increase in mAP by 2% via SSL regardless of the filtering threshold and uncertainty
metric used, including entropy (Roy et al., 2018), combined uncertainty (Kassem Sbeyti et al., 2024), and
epistemic uncertainty. This validates our focus on pseudo-label quality post-filtering and class-balancing
over uncertainty-based filtering alone.
Additionally, CSD (Jeong et al., 2019) shows no improvement over MTat 10% on KITTI or BDD, suggesting
that augmentations and consistency strategies that work on MS COCO (Lin et al., 2014) and PASCAL VOC
(Everingham et al., 2010) do not necessarily generalize to real-world datasets with different challenges and
characteristics.
RCC. Our ablation studies on RCC focus on the configuration of the collage (horizontal allocation setup
vs. 4×4 grid setup), scale variation (SV), augmentation using RandAugment (Cubuk et al., 2020), and the
croppingparameters γr,minandγr,max. AhorizontalallocationasvisualizedinFig.4outperformsagridsetup
asillustratedinFig.11(left). ThegridsetuponlyimprovestheAPby1.5%vs.1.9%for“motorcycle”, though
it decreases it by 1.3% for “person sitting” and 0.2% for “rider” and “bicycle”. Furthermore, incorporating
different scales via SV (see Fig. 11 (right)) confuses the model, as the upscaling from cropping already
introduces sufficient variation. SV leads to a smaller increase in AP for rare classes, e.g., “person sitting”
(only 1.6% vs. 4.9%) and “motorcycle” (0.9% vs. 1.9%). Similarly, adding augmentations to the collages
via RandAugment (Cubuk et al., 2020) also results in a smaller increase in AP, e.g., only 2% vs. 4.9% for
“person sitting”.
15Under review as submission to TMLR
Figure 11: KITTI (left, 4 ×4 grid setup with γr,min= 0.5andγr,max= 1.0), BDD (right, scale variation with
γr,min= 0.25andγr,max= 0.75). Example collages.
We demonstrate in Table 7 the robustness of our RCC method under different cropping parameters. RCC
remains effective across different configurations of γr,minandγr,max, with the combination of γr,min= 0.25
andγr,max= 0.75yielding the highest performance on rare classes in both KITTI and BDD (up to 5%).
Importantly, RCC consistently increases the performance of targeted classes without significantly impacting
other classes (total increase of up to 7% across all classes (/summationtext)). Whileγr,max= 1.0results in the highest
total improvements on KITTI due to a smaller drop in the AP of common classes, this comes at the cost
of a smaller gain in AP for rare classes. A higher γr,maxavoids truncating surrounding objects, maintaining
performance on the common classes of the cleaner KITTI dataset, in contrst to BDD. A limitation of RCC
is visible on the class “bicycle”, as it does not increase the performance due to labeling inconsistencies in
BDD. As shown in Table 3 and discussed in Section 4.4, the labeling of “bicycle” often includes the rider,
which limits the improvement potential of the model despite the use of RCC due to inter-class confusion.
This underscores the limitations of post-labeling balancing strategies.
Table 7: KITTI (top, 10% labeled), BDD (bottom, 1% labeled). Impact of γr,minandγr,maxon AP ofMT.
γr,minγr,maxAP/summationtextPerson
SittingTram Truck Cyclist VanPedest-
rianCar - -
- - 22.89 43.64 60.14 45.03 51.70 36.80 65.99 - -
0.1 0.5 0.912.61 2.070.420.120.19 0.07 - - 1.03
0.25 0.75 4.92 1.59 1.050.440.99 0.230.30 - - 3.50
0.5 1.0 3.942.05 1.130.020.13 0.400.00 - - 6.83
1.0 1.5 3.463.63 0.160.420.74 0.500.14 - - 5.41
Motor-
cycleRider Bicycle Bus TruckPedest-
rianTraffic
LightTraffic
SignCar
- - 4.59 6.66 9.59 24.72 23.02 17.07 7.83 14.71 35.62
0.25 0.75 1.90 1.88 0.290.710.24 0.010.090.020.024.54
0.5 1.0 1.302.13 0.01 0.170.15 0.060.080.270.323.01
RFCOur ablation studies on RFC examine the effect of different scaling strengths via γffor re-weighting
(RW) and RCF, both with and without augmentation A, as well as their combined use. Depending on the
selectedγf, RW improves the AP by up to 2%, but also decreases it by up to 3% on certain classes. In
contrast, RCF is less dependent on γfas shown in Tables 8 and 9, with a consistent increase in performance
almost across all classes, and particularly on rare classes (up to 6% in AP). Evaluating γf= 5,10,20,100
reveals that γf= 20yields the highest improvement on both datasets for MT.γf= 10is however sufficient
forMS(0.4), asDpseudocompensates for weaker scaling by inherently enhancing the representation of rare
classes. Values of γfgreater than 20 only increase the mAP by up to 0.5% ( γf= 100on KITTI with 15%
labeled). Moreover, using logarithmic smoothing in Eq. (2) further stabilizes the improvements, with an
16Under review as submission to TMLR
additional 1% increase in mAP compared to linear weighting at γf= 10. The combined use of RW and RCF
proves beneficial for MS(0.4)but notMT, as the additional loss term ℓpseudoin Eq. (1) reduces sensitivity
to weight fluctuations and improves overall training stability. RandAugment (Cubuk et al., 2020) in RCF
improves the AP by up to 3% due to increasing the insufficient variability in the dataset for rare classes.
Table 8: KITTI (top, 10% labeled), BDD (bottom, 1% labeled). Impact of γfandAon AP ofMT.
RW RCF γfAAP
/summationtext Person
SittingTram Truck Cyclist VanPedest-
rianCar - -
- - - - 22.89 43.64 60.14 45.03 51.70 36.80 65.99 - -
✓- 10 - 2.901.61 0.810.011.30 0.450.48 - - 0.14
✓- 20 - 2.722.64 1.080.260.76 0.320.31 - - 5.93
✓ ✓ 10 - 2.811.29 2.350.220.12 0.290.05 - - 1.41
✓ ✓ 20 - 1.334.14 0.380.160.15 0.270.14 - - 3.15
-✓10 - 0.941.58 0.440.630.30 0.400.02 - - 2.41
-✓20 - 2.560.38 0.891.320.73 0.350.04 - - 0.31
-✓10✓4.472.58 0.791.140.61 0.370.36 - - 5.00
-✓20✓3.184.32 1.030.170.25 0.810.11 - - 9.15
-✓30✓6.14 1.27 0.531.020.631.18 0.17 - - 8.28
✓ ✓ 10✓1.584.56 1.40 0.650.43 0.220.00 - - 8.84
✓ ✓ 20✓2.633.14 0.071.18 0.94 0.250.13 - - 7.70
Motor-
cycleRider Bicycle Bus TruckPedest-
rianTraffic
LightTraffic
SignCar
- - - - 4.59 6.66 9.59 24.72 23.02 17.07 7.83 14.71 35.62
✓- 10 - 1.130.73 1.421.680.41 0.520.040.230.074.31
✓- 20 - 0.780.91 1.272.180.54 0.880.060.280.094.49
✓ ✓ 10 - 1.870.16 0.330.590.32 0.310.030.230.133.51
✓ ✓ 20 - 0.771.24 1.181.590.69 0.650.020.420.02 3.26
-✓10 - 0.951.20 0.110.31 0.370.28 0.120.090.190.38
-✓20 - 2.400.44 0.670.380.28 0.120.090.070.112.56
-✓10✓0.592.21 1.280.030.71 0.190.010.310.251.26
-✓20✓1.00 1.600.05 0.450.52 0.260.190.290.203.16
✓ ✓ 10✓1.00 1.40 0.850.720.11 0.460.090.320.290.38
✓ ✓ 20✓0.780.83 1.821.770.04 1.190.250.49 0.234.18
Table 9: KITTI (top, 10% labeled), BDD (bottom, 1% labeled). Impact of γfandAon AP ofMS(0.4).
RW RCF γfAAP
/summationtext Person
SittingTram Truck Cyclist VanPedest-
rianCar - -
- - - - 25.05 47.60 61.21 47.36 51.60 37.45 67.6 - -
✓- 10 - 0.652.33 0.741.131.19 0.790.96 - - 2.03
✓- 20 - 2.502.54 0.691.210.52 0.931.45 - - 3.72
✓ ✓ 10 - 1.153.71 0.730.650.26 0.630.13 - - 3.98
✓ ✓ 20 - 0.583.35 0.031.032.57 0.490.10 - - 6.93
-✓10 - 1.983.24 0.710.060.361.87 0.20 - - 3.94
-✓20 - 1.392.47 0.090.391.44 1.300.13 - - 6.25
-✓10✓5.156.12 3.39 0.021.70 1.340.10 - - 17.58
-✓20✓3.265.03 2.130.340.01 1.310.09 - - 11.29
-✓30✓3.072.96 0.470.841.09 1.690.04 - - 8.40
✓ ✓ 10✓1.564.16 1.790.632.42 1.060.32 - - 10.04
✓ ✓ 20✓0.024.53 0.750.322.30 0.120.21 - - 7.79
Motor-
cycleRider Bicycle Bus TruckPedest-
rianTraffic
LightTraffic
SignCar
- - - - 4.13 5.62 7.48 23.89 22.78 13.63 9.14 15.37 36.55
✓- 10 - 0.611.01 0.370.800.19 0.420.190.220.38 1.79
✓- 20 - 0.750.32 1.642.401.90 0.890.220.200.56 8.88
✓ ✓ 10 - 0.310.63 0.310.570.11 0.740.060.330.03 2.19
✓ ✓ 20 - 0.100.19 1.53 0.91.87 0.130.10 0.110.21 4.36
-✓10 - 0.791.561.13 0.310.22 1.500.380.400.19 5.72
-✓20 - 1.341.14 0.750.51 0.63 1.420.460.320.06 3.03
-✓10✓0.731.86 0.200.500.51 1.890.110.130.05 4.20
-✓20✓1.430.48 0.730.020.87 2.17 0.620.020.24 4.82
✓ ✓ 10✓1.78 1.76 1.041.010.25 1.760.020.140.19 5.51
✓ ✓ 20✓0.792.36 0.380.800.20 1.030.060.050.29 2.90
17Under review as submission to TMLR
GLC.Our ablation studies on GLC explore the impact of IoU threshold γofor GT and prediction overlap,
γcfor consistency across augmented predictions, and choice of augmentation A(see Fig. 12). Gaussian blur
is applied using a 9 ×9 kernel size, and Gaussian noise is drawn with a mean of 0 and variance of 0.5.
Original
Horizontal Flipping (HF)
Gaussian Blur (GB)
Gaussian Noise (GN)
Figure 12: Augmentations applied during inference on the training set of KITTI via GLC.
Table 10: Impact of A– Gaussian Noise
(GN), horizontal flipping (HF), Gaussian
Blur (GB) – and IoU thresholds γoandγc
on the efficacy of GLC in correcting added
NGTs at Level 1 to KITTI (10% labeled).
AγoγcmAPGN HF GB
✓- - 90 90 46.47±0.56
-✓- 90 90 45.99±0.72
- - ✓90 90 45.11±0.27
✓ ✓ ✓ 90 90 45.65±0.32
✓ ✓ ✓ 80 90 45.41±0.53
✓ ✓ ✓ 70 90 45.40±0.04
✓ ✓ ✓ 90 80 45.85±0.03
✓ ✓ ✓ 90 70 44.97±0.21Table 10 presents the impact of each parameter on the effec-
tiveness of our GLC method in correcting synthetically intro-
duced NGTs at Level 1 to KITTI (10% labeled). The results
indicate robustness to different choices of Aand IoU thresh-
olds, with consistent mAP recovery from added NGTs by up
to 7%. This robustness can be attributed to the use of Gaus-
sian Noise (GN), Horizontal Flipping (HF), or Gaussian Blur
(GB) solely during inference, introducing sufficient variability
to evaluate model consistency effectively. GLC achieves strong
performance (47.16 ±0.43 on originalDlabeled, 39.38±0.21
post-corruption, and up to 46.47 ±0.56 post-GLC) even with
a single augmentation, thereby avoiding the additional compu-
tational cost incurred by multiple augmentations.
A similar analysis for correcting MGTs demonstrates consistent
robustness across different γcvalues, yielding mAPs of 43.58 ±
0.01 forγc= 70, 44.08±0.06 forγc= 80, and 43.88±0.02
forγc= 90(refer to Table 6).
Furthermore, we also evaluate the importance of evaluating
model consistency, compared to simply using all detections
with a confidence score above the default δs= 0.4. Relying solely on δsresults in a reduction of up to
4% in mAP, achieving only an mAP of 41.32 ±0.17 post-correction of NGTs at Level 1 on 10% of KITTI.
Figure13: ExamplesofnoisyboxlabelsinKITTI.Hu-
man annotators often fail to accurately label occluded
and truncated objects, especially in crowded scenes.Our experiments on NGTs focus on box perturba-
tions, as we assume that noisy ground truth labels
primarily arise from inaccuracies in box locations
rather than misclassification of objects. Manual la-
beling often struggles with precise box placement
but rarely misidentifies object categories. For in-
stance, confusing a car with a pedestrian is less
likely than an imprecise bounding box placement.
This assumption is supported by the high mACC
and lower mIoU of the detector on both datasets
(see Table 1), as well as by previous work demon-
strating that spatial inaccuracies in labeling can sig-
nificantly degrade model performance (Grad et al.,
2024). On KITTI, the detector encounters difficul-
tiesincrowdedscenes. Whiletheoverlappingnature
18Under review as submission to TMLR
of objects in such scenes complicates boundary identification, the primary issue arises from low-quality box
labels, as visualized in Fig. 13.
Nevertheless, to investigate the effect of class label noise, we randomly mislabel 20% of the objects in KITTI
(10% labeled), corresponding to level 1 NGT. The 20% noise result in a reduced label accuracy of 86.83%.
Applyingourconsistency-basedcorrectionmethod, wheretheclasslabelsarecorrectedusingpredictionsthat
remain consistent across multiple inferences under data augmentations, increases label accuracy from 86.83%
to 95.70%. This results in an mAP improvement from 35.91±0.22under noise to 42.55±0.14. Despite this
correction, the approximately 4% remaining loss in class label accuracy demonstrates the sensitivity of the
model to it, as the original labels yield a substantially higher mAP of 47.16% (see Table 6).
Label noise, whether in class or box labels, appears to have a comparable impact on model performance.
In the absence of significant label errors (see Section 4.4 and Table 5), the primary bottleneck in SSOD
lies in class distribution and object availability. This highlights the critical importance of addressing class
imbalance to achieve substantial improvements in object detectors.
PLS.OurfirstablationstudyonPLSexploresthecorrelationbetween MDsandourSimetricatvarying δs.
Increasingδsresults in a higher proportion of MDs, withSishowing a proportional increase (see Fig. 14),
thus validating the effectiveness of our metric. Next, we assess the impact of βin Eq. (3) on the class
distribution of removed pseudo-labeled images at 30% of KITTI labeled. Fig. 15 illustrates the shift in class
distribution for different βvalues via the relative class count calculated as/parenleftbigg
fkDi(β)−fkSi
fkSi/parenrightbigg
·100. A higher
βretains more rare classes, with the trade-off of a weaker correlation with MDs. Compared to β= 0.1,
β= 0.25retains 95% more of the rare class “person sitting”, while removing 70% more of the most common
class “car”. Despite these adjustments, the AUC only decreases by 10%, as shown in Fig. 8, highlighting the
robustness of Eq. (3). Identical behavior is observed on BDD.
DG
Figure 14: Correlation between our
PLS metric Siand the proportion of
missing detections MDs per image for
varying values of δson KITTI.
Relative 
Class Count (%)Figure 15: Impact of βon the relative class
countintheremovedpseudo-labeledimages
at 30% of KITTI labeled with δs= 0.4.
5 Conclusion
We conduct extensive analyses on label quality and its impact on SSOD performance. Our findings indicate
that effective pseudo-labels-based SSOD requires a fundamental understanding of the challenging factors
affecting label quality, such as class distribution, noise, and the precision-recall trade-off. By addressing
dataset imbalances and implementing data-centric quality measures, we demonstrate that even basic SSOD
frameworks can perform well under real-world conditions. We propose four novel, computationally efficient,
and broadly applicable building blocks to improve the effectiveness SSOD. Our building blocks consist of two
methods for balancing training data to increase rare class representation (RCC, RCF), a method to enhance
labeled data quality (GLC), and a method to refine the selection of pseudo-labeled images (PLS). Through
comprehensive experiments across different data configurations on the KITTI and BDD100K autonomous
driving datasets, we show that our methods significantly improve the effectiveness of student-teacher SSOD
19Under review as submission to TMLR
frameworks when integrated individually. Our results highlight the significant potential of our methods and
pave the way for future research in SSL.
6 Discussion and Future Work
We evaluate each proposed method individually to isolate its specific impact on SSOD, as joint evaluation
risks conflating their contributions. Therefore, we focus on a detailed analysis within a controlled setting,
providing a clear foundation for future work to build upon and explore these methods in combination and
across other application domains. For instance, in the context of filtering in the SSL framework, combining
more advanced filtering methods, such as those proposed by (Chen et al., 2022; Li et al., 2023; Kimhi
et al., 2024), and training strategies, including those by (Li et al., 2022a; 2023), with our building blocks
holds potential for more significant improvements. Our experiments demonstrate that combining different
uncertainties (epistemic and aleatoric) achieves a 41% MDR at a 1% UDR, compared to 46% MDR at the
same UDR with δs.
Furthermore, understanding the influence of training parameters on detector performance and robustness,
particularly in the presence of noise, is a promising direction for future work. Additionally, examining
how robustness evolves across different stages of training could offer valuable insights into optimizing SSL
frameworks.
Our building blocks are designed to be as model- and framework-agnostic as possible, avoiding reliance on
domain-specific assumptions. They utilize available labels and predictions without requiring modifications
to the underlying detector architecture or SSOD framework. RCC generates collages based on ground truth
labels. GLCrefinesgroundtruthlabelsusingteacherpredictionsandaugmentationconsistency,assumingthe
availability of a trained teacher detector. RCF only requires changes to the dataloader for the stratification
process. PLS assumes a trained teacher detector that outputs confidence scores and incorporates filtering as
part of its post-processing, a common feature in most object detectors but not universal. This highlights the
advantagesofevaluatingwith additional detectors, such asTransformer-basedmodels, aspartoffuturework.
Our results demonstrate that RCC, RCF, and GLC improve teacher performance, with benefits extending
to the student, particularly through PLS. However, since our empirical validation involves a single model
(EfficientDet-D0) and a single SSOD framework (STAC), additional evaluation across diverse detectors and
SSOD frameworks would strengthen the potential for widespread adoption of our methods. KITTI and
BDD provide diverse and challenging conditions reflective of real-world datasets, including class imbalance,
label noise, and varying dataset sizes. Extending the evaluation to other domains, such as medical imaging,
satellite imagery, and industrial quality control, would further demonstrate the broad applicability and
robustness of our methods.
PLS effectively identifies low-quality pseudo-labeled images post-filtering. However, these are currently
discarded. Integrating AL with SSL could enhance MSperformance by utilizing those images. Low-quality
pseudo-labeled images could be directed either to an oracle or included in a consistency-based unsupervised
loss, enabling further feature extraction in a self-supervised manner. Moreover, adding depth estimation
to our PLS metric Dicould improve its correlation with missing detections and reduce reliance on teacher
model score quality, as distant objects often have higher miss rates than nearby ones.
Curriculum learning approaches could also be explored to complement SSL and AL. Specifically, we propose
defining a curriculum for MSbased on the learning progression of MTto mitigate error propagation. For
example, adopting a ramp-up strategy by starting with labeled data before gradually incorporating pseudo-
labeled data might outperform using both from the start. Moreover, alternative weighting strategies for
ranking images in RCF, considering various forms of prior knowledge beyond class frequency, could be
further investigated.
Constructing collages of rare pseudo-labeled objects could also further increase performance via RCC, espe-
cially under low data regimes. However, collages introduce a trade-off between addressing class imbalance
and potential distribution shifts, highlighting the possible limitation of this approach. Using the same train-
ing dataset to generate the collages ensures consistency with the original data distribution but also limits the
diversity of objects and contexts available for training. Gathering collages from other datasets containing
20Under review as submission to TMLR
similar classes is promising despite significant challenges such as differences in labeling conventions and the
semantic understanding of class definitions across datasets, as also highlighted by (Liu et al., 2022a). For
example, distinctions between categories such as van, truck, and car often vary based on annotator inter-
pretation, potentially introducing additional label noise. Moreover, matching rare classes across datasets
poses significant challenges. For instance, BDD contains rare classes such as "rider," which are absent in
KITTI, making cross-dataset collage generation infeasible in specific scenarios. Future work could explore
methods to enable more effective collage generation in RCC by integrating domain adaptation techniques
and utilizing large-scale datasets with harmonized class definitions.
For GLC, our work could be extended to analyze class-based errors. Given the excellent performance of the
classification head across datasets, optimizing the combination of class and box losses could further improve
the quality of pseudo-labeled samples.
In summary, basic SSOD frameworks become more nuanced when confronted with real-world data. We
hope this work encourages deeper investigation into SSOD, moving beyond incremental gains toward a more
comprehensive understanding of the core challenges and mechanisms underlying SSL.
References
Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and confirmation
bias in deep semi-supervised learning. In 2020 International joint conference on neural networks (IJCNN) , pp.
1–8. IEEE, 2020.
Minjie Cai, Minyi Luo, Xionghu Zhong, and Hao Chen. Uncertainty-aware model adaptation for unsupervised cross-
domain object detection. arXiv preprint arXiv:2108.12612 , 2021.
Simon Chadwick and Paul Newman. Training object detectors with noisy data. In 2019 IEEE Intelligent Vehicles
Symposium (IV) , pp. 1319–1325. IEEE, 2019.
Nadine Chang, Zhiding Yu, Yu-Xiong Wang, Animashree Anandkumar, Sanja Fidler, and Jose M Alvarez. Image-
level or object-level? a tale of two resampling strategies for long-tailed detection. In International conference on
machine learning , pp. 1463–1472. PMLR, 2021.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-
sampling technique. Journal of artificial intelligence research , 16:321–357, 2002.
BinbinChen, WeijieChen, ShicaiYang, YunyiXuan, JieSong, DiXie, ShiliangPu, MingliSong, andYuetingZhuang.
Labelmatchingsemi-supervisedobjectdetection. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 14381–14390, 2022.
Sijin Chen, Yingyun Yang, and Yan Hua. Semi-supervised active learning for object detection. Electronics , 12(2):
375, 2023a.
Yukang Chen, Peizhen Zhang, Zeming Li, Yanwei Li, Xiangyu Zhang, Lu Qi, Jian Sun, and Jiaya Jia. Dynamic scale
training for object detection. arXiv preprint arXiv:2004.12432 , 2020.
Zeming Chen, Wenwei Zhang, Xinjiang Wang, Kai Chen, and Zhi Wang. Mixed pseudo labels for semi-supervised
object detection. arXiv preprint arXiv:2312.07006 , 2023b.
Nieves Crasto. Class imbalance in object detection: An experimental diagnosis and study of mitigation strategies.
arXiv preprint arXiv:2403.07113 , 2024.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmenta-
tion strategies from data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,
pp. 113–123, 2019.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmen-
tation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition workshops , pp. 702–703, 2020.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of
samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 9268–9277,
2019.
21Under review as submission to TMLR
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual
object classes (voc) challenge. International Journal of Computer Vision , 88:303–338, 2010.
Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark
suite. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 3354–3361, 2012.
Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Sim-
ple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pp. 2918–2928, 2021.
Google. GitHub - google/automl: Google Brain AutoML. https://github.com/google/automl , 2020. Accessed:
2024-06-10.
Eden Grad, Moshe Kimhi, Lion Halika, and Chaim Baskin. Benchmarking label noise in instance segmentation:
Spatial noise matters. arXiv preprint arXiv:2406.10891 , 2024.
Jianhua Han, Xiwen Liang, Hang Xu, Kai Chen, Lanqing Hong, Jiageng Mao, Chaoqiang Ye, Wei Zhang, Zhenguo
Li, Xiaodan Liang, and Chunjing Xu. Soda10m: A large-scale 2d self/semi-supervised object detection dataset for
autonomous driving, 2021.
Sungeun Hong, Sungil Kang, and Donghyeon Cho. Patch-level augmentation for object detection in aerial images.
InProceedings of the IEEE/CVF international conference on computer vision workshops , pp. 0–0, 2019.
Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object
detection. Advances in neural information processing systems , 32, 2019.
Moussa Kassem Sbeyti, Michelle Karg, Christian Wirth, Azarm Nowzad, and Sahin Albayrak. Overcoming the lim-
itations of localization uncertainty: Efficient and exact non-linear post-processing and calibration. In Machine
Learning and Knowledge Discovery in Databases: Research Track. ECML-PKDD. , pp. 52–68, 2023.
Moussa Kassem Sbeyti, Michelle E Karg, Christian Wirth, Nadja Klein, and Sahin Albayrak. Cost-sensitive
uncertainty-based failure recognition for object detection. In The 40th Conference on Uncertainty in Artificial
Intelligence , 2024.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances
in neural information processing systems , 30, 2017.
Moshe Kimhi, David Vainshtein, Chaim Baskin, and Dotan Di Castro. Robot instance segmentation with few
annotations for grasping. arXiv preprint arXiv:2407.01302 , 2024.
Johnson Kuan and Jonas Mueller. Model-agnostic label quality scoring to detect real-world label errors. In ICML
DataPerf Workshop , 2022.
Aoxue Li, Peng Yuan, and Zhenguo Li. Semi-supervised object detection via multi-instance alignment with global
class prototypes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp.
9809–9818, 2022a.
GangLi, XiangLi, YujieWang, YichaoWu, DingLiang, andShanshanZhang. Pseco: Pseudolabelingandconsistency
training for semi-supervised object detection. In European Conference on Computer Vision , pp. 457–472. Springer,
2022b.
Hengduo Li, Zuxuan Wu, Chen Zhu, Caiming Xiong, Richard Socher, and Larry S Davis. Learning from noisy
anchors for one-stage object detection. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition , pp. 10588–10597, 2020a.
Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, and Larry S Davis. Rethinking pseudo labels for semi-supervised
object detection. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pp. 1314–1322,
2022c.
JiamingLi, XiangruLin, WeiZhang, XiaoTan, YingyingLi, JunyuHan, ErruiDing, JingdongWang, andGuanbinLi.
Gradient-based sampling for class imbalanced semi-supervised object detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 16390–16400, 2023.
Yandong Li, Di Huang, Danfeng Qin, Liqiang Wang, and Boqing Gong. Improving object detection with selective
self-supervised self-training. In European Conference on Computer Vision , pp. 589–607. Springer, 2020b.
22Under review as submission to TMLR
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vi-
sion (ECCV) , pp. 740–755, 2014.
Xinyu Liu, Wuyang Li, Qiushi Yang, Baopu Li, and Yixuan Yuan. Towards robust adaptive object detection under
noisy annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp.
14207–14216, 2022a.
Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and
Peter Vajda. Unbiased teacher for semi-supervised object detection. arXiv preprint arXiv:2102.09480 , 2021.
Yen-Cheng Liu, Chih-Yao Ma, and Zsolt Kira. Unbiased teacher v2: Semi-supervised object detection for anchor-
free and anchor-based detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 9819–9828, 2022b.
Cuong Ly, Grayson Jorgenson, Dan Rosa de Jesus, Henry Kvinge, Adam Attarian, and Yijing Watkins. Colmix–a
simple data augmentation framework to improve object detector performance and robustness in aerial images.
arXiv preprint arXiv:2305.13509 , 2023.
Peng Mi, Jianghang Lin, Yiyi Zhou, Yunhang Shen, Gen Luo, Xiaoshuai Sun, Liujuan Cao, Rongrong Fu, Qiang Xu,
and Rongrong Ji. Active teacher for semi-supervised object detection. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pp. 14482–14491, 2022.
Muhammad Akhtar Munir, Muhammad Haris Khan, M Sarfraz, and Mohsen Ali. Ssal: Synergizing between self-
training and adversarial learning for domain adaptive object detection. Advances in Neural Information Processing
Systems, 34:22770–22782, 2021.
Curtis G Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine learning
benchmarks. arXiv preprint arXiv:2103.14749 , 2021.
Trong Huy Phan and Kazuma Yamamoto. Resolving class imbalance in object detection with weighted cross entropy
losses.arXiv preprint arXiv:2006.01413 , 2020.
Soumya Roy, Asim Unmesh, and Vinay P Namboodiri. Deep active learning for object detection. In British Machine
Vision Conference (BMVC) , 2018.
Nabeel Seedat, Nicolas Huynh, Fergus Imrie, and Mihaela van der Schaar. v (dirty) truth: Data-centric insights
improve pseudo-labeling. arXiv preprint arXiv:2406.13733 , 2024.
Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional
neural networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
October 11–14, 2016, Proceedings, Part VII 14 , pp. 467–482. Springer, 2016.
Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised
learning framework for object detection. arXiv preprint arXiv:2005.04757 , 2020.
Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 10778–10787, 2020.
Chakkrit Tantithamthavorn, Ahmed E Hassan, and Kenichi Matsumoto. The impact of class rebalancing techniques
on the performance and interpretation of defect prediction models. IEEE Transactions on Software Engineering ,
46(11):1200–1219, 2018.
Ulyana Tkachenko, Aditya Thyagarajan, and Jonas Mueller. Objectlab: Automated diagnosis of mislabeled images
in object detection data. arXiv preprint arXiv:2309.00832 , 2023.
Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. Efficient object localization
using convolutional networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,
pp. 648–656, 2015.
Keze Wang, Xiaopeng Yan, Dongyu Zhang, Lei Zhang, and Liang Lin. Towards human-machine cooperation: Self-
supervised sample mining for object detection. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pp. 1605–1613, 2018.
23Under review as submission to TMLR
Zhenyu Wang, Yali Li, Ye Guo, Lu Fang, and Shengjin Wang. Data-uncertainty guided multi-phase learning for
semi-supervised object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 4568–4577, 2021.
Zhe Wu, Navaneeth Bodla, Bharat Singh, Mahyar Najibi, Rama Chellappa, and Larry S Davis. Soft sampling for
robust object detection. arXiv preprint arXiv:1806.06986 , 2018.
Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-
end semi-supervised object detection with soft teacher. In Proceedings of the IEEE/CVF international conference
on computer vision , pp. 3060–3069, 2021.
Mengmeng Xu, Yancheng Bai, Bernard Ghanem, Boxiao Liu, Yan Gao, Nan Guo, Xiaochun Ye, Fang Wan, H You,
D Fan, et al. Missing labels in object detection. In CVPR workshops , volume 3, 2019.
Pengxiang Yan, Ziyi Wu, Mengmeng Liu, Kun Zeng, Liang Lin, and Guanbin Li. Unsupervised domain adaptive
salient object detection through uncertainty-aware pseudo-label learning. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 36, pp. 3000–3008, 2022.
Qize Yang, Xihan Wei, Biao Wang, Xian-Sheng Hua, and Lei Zhang. Interactive self-training with mean teachers
for semi-supervised object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pp. 5941–5950, 2021.
Yuewei Yang, Kevin J Liang, and Lawrence Carin. Object detection as a positive-unlabeled problem. arXiv preprint
arXiv:2002.04672 , 2020.
Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor
Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pp. 2636–2645, 2020.
Sihao Yu, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Zizhen Wang, and Xueqi Cheng. A re-balancing strategy for class-
imbalanced classification based on instance difficulty. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 70–79, 2022.
Fangyuan Zhang, Tianxiang Pan, and Bin Wang. Semi-supervised object detection with adaptive class-rebalancing
self-training. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pp. 3252–3261, 2022.
Donghao Zhou, Jialin Li, Jinpeng Li, Jiancheng Huang, Qiang Nie, Yong Liu, Bin-Bin Gao, Qiong Wang, Pheng-Ann
Heng, and Guangyong Chen. Distribution-aware calibration for object detection with noisy bounding boxes. arXiv
preprint arXiv:2308.12017 , 2023.
24