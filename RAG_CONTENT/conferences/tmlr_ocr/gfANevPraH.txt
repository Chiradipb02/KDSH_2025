Demonstrating and Reducing Shortcuts in Vision-Language
Representation Learning
Maurits Bleeker∗m.j.r.bleeker@uva.nl
University of Amsterdam, Amsterdam, The Netherlands
Mariya Hendriksen∗m.hendriksen@uva.nl
AIRLab, University of Amsterdam, Amsterdam, The Netherlands
Andrew Yates a.c.yates@uva.nl
University of Amsterdam, Amsterdam, The Netherlands
Maarten de Rijke m.derijke@uva.nl
University of Amsterdam, Amsterdam, The Netherlands
Abstract
Vision-language models ( VLMs) mainly rely on contrastive training to learn general-purpose
representationsofimagesandcaptions. Wefocusonthesituationwhenoneimageisassociated
with several captions, each caption containing both information shared among all captions
and unique information per caption about the scene depicted in the image. In such cases, it is
unclear whether contrastive losses are sufficient for learning task-optimal representations that
contain all the information provided by the captions or whether the contrastive learning setup
encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce
synthetic shortcuts for vision-language : a training and evaluation framework where we inject
synthetic shortcuts into image-text data. We show that contrastive VLMs trained from
scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features
that represent the shortcut. Hence, contrastive losses are not sufficient to learn task-optimal
representations, i.e., representations that contain all task-relevant information shared between
the image and associated captions. We examine two methods to reduce shortcut learning in
our training and evaluation framework: (i) latent target decoding and (ii) implicit feature
modification. We show empirically that both methods improve performance on the evaluation
task, but only partially reduce shortcut learning when training and evaluating with our
shortcut learning framework. Hence, we show the difficulty and challenge of our shortcut
learning framework for contrastive vision-language representation learning.
1 Introduction
Recent work on understanding the internal mechanisms of representation learning has brought to attention
the problem of shortcut learning (Robinson et al., 2021; Chen et al., 2021; Scimeca et al., 2022). While there
are multiple definitions of shortcut learning (e.g., Geirhos et al., 2020; Wiles et al., 2022), in this work we
defineshortcuts aseasy-to-learn discriminatory features that minimize the (contrastive) optimization objective
but are not necessarily sufficient for solving the evaluation task . More specifically, we focus on the problem of
shortcut learning in the relatively unexplored context of vision-language ( VL) representation learning with
multiple matching captions per image.
Contrastive learning ( CL) plays a crucial role in VLrepresentation learning. Despite the success of non-
contrastive approaches, e.g., (Bardes et al., 2022), the dominant paradigm in VLrepresentation learning
revolves around either fully contrastive strategies (Faghri et al., 2018; Li et al., 2019a; Jia et al., 2021;
∗Co-first author.
1Radford et al., 2021) or a combination of contrastive methods with additional objectives (Li et al., 2021; Zeng
et al., 2022; Li et al., 2022a; Zeng et al., 2022; Li et al., 2023a). It is standard practice in contrastive VL
representation learning to sample batches of image-caption pairs and maximize the alignment between the
representations of the matching images and captions (Radford et al., 2019; Jia et al., 2021). Given that the
typicalVLbenchmarks, e.g., Flickr30k (Young et al., 2014) and MS-COCO Captions (Lin et al., 2014; Chen
et al., 2015), are constructed in such a way that each image is associated with multiple captions, each caption
can be seen as a different viewof the image it describes. Therefore, CLwith multiple captions per image can
be seen as CLwith multiple views, where each caption provides a different view of the scene depicted in the
image.
CLwith multiple views, where each view represents a different observation of the same datapoint, has proven
to be effective for general-purpose representation learning (Hjelm et al., 2019; Chen et al., 2020a; Tian et al.,
2020a). The goal of multi-view (contrastive) representation learning methods is to learn representations
that remain invariant to a shift of view, which is achieved by maximizing alignment between embeddings of
similar views. A core assumption within the multi-view representation learning literature is that task-relevant
information is shared across views whereas task-irrelevant information is not shared, given a downstream
evaluation task (Zhao et al., 2017; Federici et al., 2020; Tian et al., 2020a; Shwartz-Ziv & LeCun, 2023).
: a couple of boats and a red car: a couple of boats and car on a street
<latexit sha1_base64="xln17fZi6/tqBxHk0twA8FS04OU=">AAACCHicdVC7TsMwFHXKq5RXgJEBiwqJKUrS0nYsdGEsEn1Ibagc12mtOg/ZDqKKMrLwKywMIMTKJ7DxNzhtkQDBkSwdn3Ov7r3HjRgV0jQ/tNzS8srqWn69sLG5tb2j7+61RRhzTFo4ZCHvukgQRgPSklQy0o04Qb7LSMedNDK/c0O4oGFwJacRcXw0CqhHMZJKGuiHfR/Jseslt+kgmXGMWNJQn7M0vU7SgV40DbNSKddOoWmUrWq1ZCti2XbJsqFlmDMUwQLNgf7eH4Y49kkgMUNC9Cwzkk6CuKSYkbTQjwWJEJ6gEekpGiCfCCeZHZLCY6UMoRdy9QIJZ+r3jgT5Qkx9V1Vmq4rfXib+5fVi6dWchAZRLEmA54O8mEEZwiwVOKScYMmmiiDMqdoV4jHiCEuVXUGF8HUp/J+0bcOqGJXLcrF+vogjDw7AETgBFqiCOrgATdACGNyBB/AEnrV77VF70V7npTlt0bMPfkB7+wTE6psg</latexit>xCA
<latexit sha1_base64="eV1bBAR0/cgyNV7BexHH5SBgq5A=">AAACCHicdVC7TsMwFHXKq5RXgJEBiwqJKUpaUspWtQtjkehDakvluE5r1XEi20FUUUYWfoWFAYRY+QQ2/gb3gQQIjmTp+Jx7de89XsSoVLb9YWSWlldW17LruY3Nre0dc3evKcNYYNLAIQtF20OSMMpJQ1HFSDsSBAUeIy1vXJv6rRsiJA35lZpEpBegIac+xUhpqW8edgOkRp6f3Kb9ZMYxYklNf6ppep2kfTNvW07RLroutK1C2Sk455qUXFdr0LHsGfJggXrffO8OQhwHhCvMkJQdx45UL0FCUcxImuvGkkQIj9GQdDTlKCCyl8wOSeGxVgbQD4V+XMGZ+r0jQYGUk8DTldNV5W9vKv7ldWLll3sJ5VGsCMfzQX7MoArhNBU4oIJgxSaaICyo3hXiERIIK51dTofwdSn8nzQLllOySpen+Up1EUcWHIAjcAIccAYq4ALUQQNgcAcewBN4Nu6NR+PFeJ2XZoxFzz74AePtE8LHmx8=</latexit>xCB: a couple of boats and car on a street: a couple of boats and car on a street
Figure 1: Shared vs. caption-specific infor-
mation given an example of one image and
two associated captions xCAandxCB. The
purple color indicates information shared
between the image and both captions. The
green color indicates task-relevant informa-
tion specific for xCA. The blue color indi-
cates task-relevant information specific for
xCB.An open challenge in the multi-view representation learning
domain concerns learning representations that contain task-
relevant information that is not shared among different views,
i.e., that may be unique for some views (Shwartz-Ziv & LeCun,
2023; Zong et al., 2023). In the case of image-caption datasets
where each image is paired with at least one corresponding
caption, the captions matching the same image do not necessar-
ily share the same information as each caption is distinct and
may describe different aspects of the image (Biten et al., 2022).
Figure 1 illustrates the concept of shared vs. caption-specific
task-relevant information. The image is accompanied by two
captions: ‘a couple of boats and a red car’ ( xCA) and ‘a couple
of boats and a car on a street’ ( xCB). The shared information
between the captions includes ‘couple of boats’ and ‘car’. Cap-
tionxCAprovides unique information by describing the car as
‘red’. Caption xCBadds unique contextual details about the
location with the phrase ‘on a street’. To learn task-optimal
representations, it is essential to integrate both the shared and
unique information from these captions. Furthermore, given
the typical quality of captions of image-caption datasets (Chen
et al., 2015), we assume that all information present in the cap-
tions is relevant. Hence, each image-caption pair may contain
bothsharedtask-relevant information, i.e., information shared
across all the captions in the tuple, and uniquetask-relevant
information, i.e., information not shared with other captions.
Therefore, learning task-optimal representations for the image
implies learning all task-relevant information that comprises
both shared and caption-specific information.
Another problem of CLapproaches is related to feature suppression . Shwartz-Ziv & LeCun (2023) argue
that although contrastive loss functions lack explicit information-theoretical constraints aimed at suppressing
non-shared information among views, the learning algorithm benefits from simplifying representations by
suppressing features from the input data that are not relevant for minimizing the contrastive loss. Furthermore,
Robinson et al. (2021) demonstrate that contrastive loss functions are susceptible to solutions that suppress
features from the input data. In the case of VL,CLwith multiple captions per image where at least one caption
contains caption-specific information, the image representation can never have a perfect alignment with all
matching captions. This is due to the misalignment that happens when encoding unique information for the
2ImageLatentVariables Captions
<latexit sha1_base64="wsbG1Ff9kiUnCTW3HHeuQ+bjeEk=">AAAB83icbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsK9gGdoWTSTBuaSYYkI9Shv+HGhSJu/Rl3/o2ZdhbaeiBwOOde7skJE860cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpuiDXlTNCWYYbTbqIojkNOO+H4Nvc7j1RpJsWDmSQ0iPFQsIgRbKzk+zE2ozDKnqZ9r1+tuXV3BrRMvILUoECzX/3yB5KkMRWGcKx1z3MTE2RYGUY4nVb8VNMEkzEe0p6lAsdUB9ks8xSdWGWAIqnsEwbN1N8bGY61nsShncwz6kUvF//zeqmJroOMiSQ1VJD5oSjlyEiUF4AGTFFi+MQSTBSzWREZYYWJsTVVbAne4peXSfus7l3WL+7Pa42boo4yHMExnIIHV9CAO2hCCwgk8Ayv8Oakzovz7nzMR0tOsXMIf+B8/gAwrpHM</latexit>z1
<latexit sha1_base64="3Fz/iNFZvpqPfdeLs7nUGPShfnw=">AAAB9XicbVC7TsMwFL0pr1JeBUYWiwqJqUoqCowVLIxFog+pDZXjOq1Vx4lsB1Si/AcLAwix8i9s/A1OmwFajmTp6Jx7dY+PF3GmtG1/W4WV1bX1jeJmaWt7Z3evvH/QVmEsCW2RkIey62FFORO0pZnmtBtJigOP0443uc78zgOVioXiTk8j6gZ4JJjPCNZGuu8HWI89P3lKB0ktHZQrdtWeAS0TJycVyNEclL/6w5DEARWacKxUz7Ej7SZYakY4TUv9WNEIkwke0Z6hAgdUucksdYpOjDJEfijNExrN1N8bCQ6UmgaemcxSqkUvE//zerH2L92EiSjWVJD5IT/mSIcoqwANmaRE86khmEhmsiIyxhITbYoqmRKcxS8vk3at6pxX67dnlcZVXkcRjuAYTsGBC2jADTShBQQkPMMrvFmP1ov1bn3MRwtWvnMIf2B9/gD+fZLZ</latexit>z2
<latexit sha1_base64="9ICmawpORVKL4gfMkjXAa5F7+Es=">AAAB9XicbVC7TsMwFL0pr1JeAUYWiwqJqUoQr7GChbFI9CG1oXJcp7XqOJHtgEqU/2BhACFW/oWNv8FpM0DLkSwdnXOv7vHxY86Udpxvq7S0vLK6Vl6vbGxube/Yu3stFSWS0CaJeCQ7PlaUM0GbmmlOO7GkOPQ5bfvj69xvP1CpWCTu9CSmXoiHggWMYG2k+16I9cgP0qesn4qsb1edmjMFWiRuQapQoNG3v3qDiCQhFZpwrFTXdWLtpVhqRjjNKr1E0RiTMR7SrqECh1R56TR1ho6MMkBBJM0TGk3V3xspDpWahL6ZzFOqeS8X//O6iQ4uvZSJONFUkNmhIOFIRyivAA2YpETziSGYSGayIjLCEhNtiqqYEtz5Ly+S1knNPa+d3Z5W61dFHWU4gEM4BhcuoA430IAmEJDwDK/wZj1aL9a79TEbLVnFzj78gfX5A1m4kxU=</latexit>zn
<latexit sha1_base64="bxOAupAmAotS45D8wTdXtzQQusE=">AAACAXicbZDLSsNAFIZP6q3WW9SN4CZYBFclEW/LohvdVbAXaEOYTCft0MkkzEzEEuLGV3HjQhG3voU738ZpmoW2/jDw8Z9zmHN+P2ZUKtv+NkoLi0vLK+XVytr6xuaWub3TklEiMGniiEWi4yNJGOWkqahipBMLgkKfkbY/uprU2/dESBrxOzWOiRuiAacBxUhpyzP3eiFSQz9IHzIvzRkjlt5kmWdW7Zqdy5oHp4AqFGp45levH+EkJFxhhqTsOnas3BQJRTEjWaWXSBIjPEID0tXIUUikm+YXZNahdvpWEAn9uLJy9/dEikIpx6GvOyc7ytnaxPyv1k1UcOGmlMeJIhxPPwoSZqnImsRh9akgWLGxBoQF1btaeIgEwkqHVtEhOLMnz0PruOac1U5vT6r1yyKOMuzDARyBA+dQh2toQBMwPMIzvMKb8WS8GO/Gx7S1ZBQzu/BHxucPrNSXsQ==</latexit>xI<latexit sha1_base64="XQyyy4bbrLmB3rDTmHKGMZw+96o=">AAAB83icbVDLSgMxFL3js9ZX1aWbYBHERZkRX8uiG5cV7AM6Q8mkmTY0kwxJRqhDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJpxp47rfztLyyuraemmjvLm1vbNb2dtvaZkqQptEcqk6IdaUM0GbhhlOO4miOA45bYej29xvP1KlmRQPZpzQIMYDwSJGsLGS78fYDMMoe5r0TnuVqltzp0CLxCtIFQo0epUvvy9JGlNhCMdadz03MUGGlWGE00nZTzVNMBnhAe1aKnBMdZBNM0/QsVX6KJLKPmHQVP29keFY63Ec2sk8o573cvE/r5ua6DrImEhSQwWZHYpSjoxEeQGozxQlho8twUQxmxWRIVaYGFtT2ZbgzX95kbTOat5l7eL+vFq/KeoowSEcwQl4cAV1uIMGNIFAAs/wCm9O6rw4787HbHTJKXYO4A+czx8mEpHF</latexit>z⇤
<latexit sha1_base64="LdDk+KGYhCJrr0vnq3DjpWl1YlM=">AAAB9XicbVC7TsMwFL0pr1JeBUYWiwqJqUqgPMYKFsYi0YfUhspxndaq40S2AypR/oOFAYRY+Rc2/ganzQAtR7J0dM69usfHizhT2ra/rcLS8srqWnG9tLG5tb1T3t1rqTCWhDZJyEPZ8bCinAna1Exz2okkxYHHadsbX2d++4FKxUJxpycRdQM8FMxnBGsj3fcCrEeenzyl/eQ07ZcrdtWeAi0SJycVyNHol796g5DEARWacKxU17Ej7SZYakY4TUu9WNEIkzEe0q6hAgdUuck0dYqOjDJAfijNExpN1d8bCQ6UmgSemcxSqnkvE//zurH2L92EiSjWVJDZIT/mSIcoqwANmKRE84khmEhmsiIywhITbYoqmRKc+S8vktZJ1Tmvnt3WKvWrvI4iHMAhHIMDF1CHG2hAEwhIeIZXeLMerRfr3fqYjRasfGcf/sD6/AEAEZLa</latexit>z3<latexit sha1_base64="HIu5AI4bpifJ1JaIEtkS6BPUqiI=">AAACBXicbVC7TsMwFL3hWcorwAiDRYXEVCUIFcZCF8Yi0YfURpXjOq1V5yHbQVRRFhZ+hYUBhFj5Bzb+BifNAC1HsnR8zr269x434kwqy/o2lpZXVtfWSxvlza3tnV1zb78tw1gQ2iIhD0XXxZJyFtCWYorTbiQo9l1OO+6kkfmdeyokC4M7NY2o4+NRwDxGsNLSwDzq+1iNXS95SAdJzgnmSUN/rtJ0YFasqpUDLRK7IBUo0ByYX/1hSGKfBopwLGXPtiLlJFgoRjhNy/1Y0giTCR7RnqYB9ql0kvyKFJ1oZYi8UOgXKJSrvzsS7Es59V1dme0p571M/M/rxcq7dBIWRLGiAZkN8mKOVIiySNCQCUoUn2qCiWB6V0TGWGCidHBlHYI9f/IiaZ9V7Vq1dnteqV8XcZTgEI7hFGy4gDrcQBNaQOARnuEV3own48V4Nz5mpUtG0XMAf2B8/gDNb5ls</latexit>xCA
<latexit sha1_base64="U+ijf6xPK+q+DqbyjFmUzJ+iXZI=">AAACBXicbVDLSsNAFL3xWesr6lIXwSK4KolIdVnajcsK9gFtCJPppB06mYSZiVhCNm78FTcuFHHrP7jzb5ymWWjrgYEz59zLvff4MaNS2fa3sbK6tr6xWdoqb+/s7u2bB4cdGSUCkzaOWCR6PpKEUU7aiipGerEgKPQZ6fqT5szv3hMhacTv1DQmbohGnAYUI6UlzzwZhEiN/SB9yLw05xixtKk/jSzzzIpdtXNYy8QpSAUKtDzzazCMcBISrjBDUvYdO1ZuioSimJGsPEgkiRGeoBHpa8pRSKSb5ldk1plWhlYQCf24snL1d0eKQimnoa8rZ3vKRW8m/uf1ExVcuynlcaIIx/NBQcIsFVmzSKwhFQQrNtUEYUH1rhYeI4Gw0sGVdQjO4snLpHNRdWrV2u1lpd4o4ijBMZzCOThwBXW4gRa0AcMjPMMrvBlPxovxbnzMS1eMoucI/sD4/AHO9Zlt</latexit>xCB(a) Minimal shared information.
ImageLatentVariables Captions
<latexit sha1_base64="wsbG1Ff9kiUnCTW3HHeuQ+bjeEk=">AAAB83icbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsK9gGdoWTSTBuaSYYkI9Shv+HGhSJu/Rl3/o2ZdhbaeiBwOOde7skJE860cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpuiDXlTNCWYYbTbqIojkNOO+H4Nvc7j1RpJsWDmSQ0iPFQsIgRbKzk+zE2ozDKnqZ9r1+tuXV3BrRMvILUoECzX/3yB5KkMRWGcKx1z3MTE2RYGUY4nVb8VNMEkzEe0p6lAsdUB9ks8xSdWGWAIqnsEwbN1N8bGY61nsShncwz6kUvF//zeqmJroOMiSQ1VJD5oSjlyEiUF4AGTFFi+MQSTBSzWREZYYWJsTVVbAne4peXSfus7l3WL+7Pa42boo4yHMExnIIHV9CAO2hCCwgk8Ayv8Oakzovz7nzMR0tOsXMIf+B8/gAwrpHM</latexit>z1
<latexit sha1_base64="3Fz/iNFZvpqPfdeLs7nUGPShfnw=">AAAB9XicbVC7TsMwFL0pr1JeBUYWiwqJqUoqCowVLIxFog+pDZXjOq1Vx4lsB1Si/AcLAwix8i9s/A1OmwFajmTp6Jx7dY+PF3GmtG1/W4WV1bX1jeJmaWt7Z3evvH/QVmEsCW2RkIey62FFORO0pZnmtBtJigOP0443uc78zgOVioXiTk8j6gZ4JJjPCNZGuu8HWI89P3lKB0ktHZQrdtWeAS0TJycVyNEclL/6w5DEARWacKxUz7Ej7SZYakY4TUv9WNEIkwke0Z6hAgdUucksdYpOjDJEfijNExrN1N8bCQ6UmgaemcxSqkUvE//zerH2L92EiSjWVJD5IT/mSIcoqwANmaRE86khmEhmsiIyxhITbYoqmRKcxS8vk3at6pxX67dnlcZVXkcRjuAYTsGBC2jADTShBQQkPMMrvFmP1ov1bn3MRwtWvnMIf2B9/gD+fZLZ</latexit>z2
<latexit sha1_base64="9ICmawpORVKL4gfMkjXAa5F7+Es=">AAAB9XicbVC7TsMwFL0pr1JeAUYWiwqJqUoQr7GChbFI9CG1oXJcp7XqOJHtgEqU/2BhACFW/oWNv8FpM0DLkSwdnXOv7vHxY86Udpxvq7S0vLK6Vl6vbGxube/Yu3stFSWS0CaJeCQ7PlaUM0GbmmlOO7GkOPQ5bfvj69xvP1CpWCTu9CSmXoiHggWMYG2k+16I9cgP0qesn4qsb1edmjMFWiRuQapQoNG3v3qDiCQhFZpwrFTXdWLtpVhqRjjNKr1E0RiTMR7SrqECh1R56TR1ho6MMkBBJM0TGk3V3xspDpWahL6ZzFOqeS8X//O6iQ4uvZSJONFUkNmhIOFIRyivAA2YpETziSGYSGayIjLCEhNtiqqYEtz5Ly+S1knNPa+d3Z5W61dFHWU4gEM4BhcuoA430IAmEJDwDK/wZj1aL9a79TEbLVnFzj78gfX5A1m4kxU=</latexit>zn
<latexit sha1_base64="bxOAupAmAotS45D8wTdXtzQQusE=">AAACAXicbZDLSsNAFIZP6q3WW9SN4CZYBFclEW/LohvdVbAXaEOYTCft0MkkzEzEEuLGV3HjQhG3voU738ZpmoW2/jDw8Z9zmHN+P2ZUKtv+NkoLi0vLK+XVytr6xuaWub3TklEiMGniiEWi4yNJGOWkqahipBMLgkKfkbY/uprU2/dESBrxOzWOiRuiAacBxUhpyzP3eiFSQz9IHzIvzRkjlt5kmWdW7Zqdy5oHp4AqFGp45levH+EkJFxhhqTsOnas3BQJRTEjWaWXSBIjPEID0tXIUUikm+YXZNahdvpWEAn9uLJy9/dEikIpx6GvOyc7ytnaxPyv1k1UcOGmlMeJIhxPPwoSZqnImsRh9akgWLGxBoQF1btaeIgEwkqHVtEhOLMnz0PruOac1U5vT6r1yyKOMuzDARyBA+dQh2toQBMwPMIzvMKb8WS8GO/Gx7S1ZBQzu/BHxucPrNSXsQ==</latexit>xI<latexit sha1_base64="XQyyy4bbrLmB3rDTmHKGMZw+96o=">AAAB83icbVDLSgMxFL3js9ZX1aWbYBHERZkRX8uiG5cV7AM6Q8mkmTY0kwxJRqhDf8ONC0Xc+jPu/Bsz7Sy09UDgcM693JMTJpxp47rfztLyyuraemmjvLm1vbNb2dtvaZkqQptEcqk6IdaUM0GbhhlOO4miOA45bYej29xvP1KlmRQPZpzQIMYDwSJGsLGS78fYDMMoe5r0TnuVqltzp0CLxCtIFQo0epUvvy9JGlNhCMdadz03MUGGlWGE00nZTzVNMBnhAe1aKnBMdZBNM0/QsVX6KJLKPmHQVP29keFY63Ec2sk8o573cvE/r5ua6DrImEhSQwWZHYpSjoxEeQGozxQlho8twUQxmxWRIVaYGFtT2ZbgzX95kbTOat5l7eL+vFq/KeoowSEcwQl4cAV1uIMGNIFAAs/wCm9O6rw4787HbHTJKXYO4A+czx8mEpHF</latexit>z⇤
<latexit sha1_base64="LdDk+KGYhCJrr0vnq3DjpWl1YlM=">AAAB9XicbVC7TsMwFL0pr1JeBUYWiwqJqUqgPMYKFsYi0YfUhspxndaq40S2AypR/oOFAYRY+Rc2/ganzQAtR7J0dM69usfHizhT2ra/rcLS8srqWnG9tLG5tb1T3t1rqTCWhDZJyEPZ8bCinAna1Exz2okkxYHHadsbX2d++4FKxUJxpycRdQM8FMxnBGsj3fcCrEeenzyl/eQ07ZcrdtWeAi0SJycVyNHol796g5DEARWacKxU17Ej7SZYakY4TUu9WNEIkzEe0q6hAgdUuck0dYqOjDJAfijNExpN1d8bCQ6UmgSemcxSqnkvE//zurH2L92EiSjWVJDZIT/mSIcoqwANmKRE84khmEhmsiIywhITbYoqmRKc+S8vktZJ1Tmvnt3WKvWrvI4iHMAhHIMDF1CHG2hAEwhIeIZXeLMerRfr3fqYjRasfGcf/sD6/AEAEZLa</latexit>z3<latexit sha1_base64="HIu5AI4bpifJ1JaIEtkS6BPUqiI=">AAACBXicbVC7TsMwFL3hWcorwAiDRYXEVCUIFcZCF8Yi0YfURpXjOq1V5yHbQVRRFhZ+hYUBhFj5Bzb+BifNAC1HsnR8zr269x434kwqy/o2lpZXVtfWSxvlza3tnV1zb78tw1gQ2iIhD0XXxZJyFtCWYorTbiQo9l1OO+6kkfmdeyokC4M7NY2o4+NRwDxGsNLSwDzq+1iNXS95SAdJzgnmSUN/rtJ0YFasqpUDLRK7IBUo0ByYX/1hSGKfBopwLGXPtiLlJFgoRjhNy/1Y0giTCR7RnqYB9ql0kvyKFJ1oZYi8UOgXKJSrvzsS7Es59V1dme0p571M/M/rxcq7dBIWRLGiAZkN8mKOVIiySNCQCUoUn2qCiWB6V0TGWGCidHBlHYI9f/IiaZ9V7Vq1dnteqV8XcZTgEI7hFGy4gDrcQBNaQOARnuEV3own48V4Nz5mpUtG0XMAf2B8/gDNb5ls</latexit>xCA
<latexit sha1_base64="U+ijf6xPK+q+DqbyjFmUzJ+iXZI=">AAACBXicbVDLSsNAFL3xWesr6lIXwSK4KolIdVnajcsK9gFtCJPppB06mYSZiVhCNm78FTcuFHHrP7jzb5ymWWjrgYEz59zLvff4MaNS2fa3sbK6tr6xWdoqb+/s7u2bB4cdGSUCkzaOWCR6PpKEUU7aiipGerEgKPQZ6fqT5szv3hMhacTv1DQmbohGnAYUI6UlzzwZhEiN/SB9yLw05xixtKk/jSzzzIpdtXNYy8QpSAUKtDzzazCMcBISrjBDUvYdO1ZuioSimJGsPEgkiRGeoBHpa8pRSKSb5ldk1plWhlYQCf24snL1d0eKQimnoa8rZ3vKRW8m/uf1ExVcuynlcaIIx/NBQcIsFVmzSKwhFQQrNtUEYUH1rhYeI4Gw0sGVdQjO4snLpHNRdWrV2u1lpd4o4ijBMZzCOThwBXW4gRa0AcMjPMMrvBlPxovxbnzMS1eMoucI/sD4/AHO9Zlt</latexit>xCB (b) Task-optimal information.
Figure 2: Synthetic shortcuts in the context of minimal shared and task-optimal information for vision-
language representation learning with multiple captions per image. The purple color represents features shared
among the image and all captions (minimal shared information). The yellow color represents caption-specific
features (unique information). The grey color indicates features that are not present in both the image and
any of the captions (task-irrelevant information). The red color indicates synthetic shortcuts. We demonstrate
that while shortcuts exist in both scenarios, minimal shared information also includes information shared
among the image and all associated captions, whereas task-optimal information combines both minimal
shared information and caption-specific information.
other captions. Therefore, it is unclear whether contrastive methods can learn task-optimal representations,
i.e., representations that contain all information present in the captions associated with the image, or if
they learn only the minimal shared information, i.e., information shared between the image and all captions
that are sufficient to minimize the contrastive discrimination objective. An illustration of minimal shared
information and a task-optimal representation is given in Figure 2.
Motivated by the abovementioned problems, we address the following question:
In the context of VLrepresentation learning with multiple captions per image, to what extent
does the presence of a shortcut hinder learning task-optimal representations?
To answer this question, we investigate the problem of shortcut learning for VLrepresentation learning with
multiple captions per image. We do this by introducing the synthetic shortcuts for vision-language (SVL)
framework for adding additional, easily identifiable information to image-caption tuples. The information
that we add is represented as identifiers that are applied to both image and caption; these identifiers do not
bear any semantic meaning. The identifiers provide additional shared information between the image and
captions, which is a subset of the total shared information between the image and the caption. For details
and examples of shortcuts, refer to Section 3, where Figure 4 illustrates an example of an image-caption pair
with a shortcut added. The synthetic shortcuts framework allows us to investigate how much the encoder
model relies on the added shortcut during training and evaluation, and hence how much of the relevant
information is still captured if a shortcut solution is available. Overall, our SVLframework allows us to
investigate the shortcut learning problem in a controlled way. We focus on image-caption retrieval ( ICR) as
an evaluation task because contrastive losses directly optimize for the ICRevaluation task, which assesses the
quality of the learned representations by computing a similarity score between images and captions (Radford
et al., 2021; Yuksekgonul et al., 2023). To investigate the problem, we run experiments on two distinct
models: (i) CLIP (Radford et al., 2019), a large-scale model that we fine-tune; and (ii) VSE++ (Faghri et al.,
2018), a relatively small model that we train from scratch. We evaluate the models’ performance on the
Flickr30k (Young et al., 2014) and MS-COCO (Lin et al., 2014; Chen et al., 2015) and benchmarks. The
benchmarks are constructed in such a way that each image is associated with five captions and each caption
represents a concise summary of the corresponding image.
3Therefore, the contributions of this work are two-fold:
IA framework for investigating the problem of shortcut learning for contrastive vision-
language representation learning in a controlled way : We introduce the synthetic shortcuts
for vision-language framework. The framework enables the injection of synthetic shortcuts into
image-caption tuples in the training dataset. We use the framework to investigate and understand the
extent to which contrastive VLmodels rely on shortcuts when a shortcut solution is available. We run
our experiments using CLIP and VSE++, two distinct vision-language models ( VLMs). We evaluate
the models’ performance on the Flickr30k andMS-COCO benchmarks. We evaluate the effectiveness
of contrastive VLmodels by comparing their performance with and without synthetic shortcuts. We
demonstrate that both models trained from scratch and fine-tuned, large-scale pre-trained foundation
models mainly rely on shortcut features and do not learn task-optimal representations. Consequently,
we show that contrastive losses mainly capture the easy-to-learn discriminatory features that are
shared among the image and all matching captions, while suppressing other task-relevant information.
Hence, we argue that contrastive losses are not sufficient to learn task-optimal representations for
VL representation learning.
IIWe present two shortcut learning reduction methods on our proposed training and
evaluation framework: We investigate latent target decoding ( LTD) and implicit feature mod-
ification ( IFM) using our SVLtraining and evaluation framework. While both methods improve
performance on the evaluation task, our framework poses challenges that existing shortcut reduction
techniques can only partially address, as the performance is not on par with models trained without
synthetic shortcuts. These findings underline the importance and complexity of our framework
in studying and evaluating shortcut learning within the context of contrastive VL representation
learning.
2 Background and Analysis
In this section, we present the notation, setup, and assumptions on which we base the work. Additionally, we
conduct an analysis of contrastive VL representation learning with multiple captions per image.
2.1 Preliminaries
Notation. We closely follow the notation from (Bleeker et al., 2023). See Table 3 for an overview. Let Dbe
a dataset of Nimage-caption tuples: D=/braceleftig/parenleftig
xi
I,{xi
Cj}k
j=1/parenrightig/bracerightigN
i=1. Each tuple i∈Ncontains one image xi
I
andkcaptions xi
Cj, where 1≤j≤k. All captions in tuple i∈Nare considered as matching captions w.r.t.
image xIin the tuple i. The latent representation of an image-caption pair from a tuple iis denoted as zi
I
andzi
Cjrespectively. During training, we sample image-caption pairs from the dataset Dand optimize for
the evaluation task T. We include all captions in the dataset once per training epoch, hence, each image is
sampledktimes.
Given an image xI, a set ofkassociated captions K={xCj}k
j=1, and one caption randomly sampled from
the set xC∈K, we define the following representations: (i) zSUF
C→Iassufficient representation of the caption
xCthat describes the image xI; (ii) zSUF
I→Cas representation of the image xIsufficient for the caption xC;
(iii)zMIN
I→Cas representation of the image xIthat isminimally sufficient for the caption xC; and (iv) zOPT
I→Kas
representation of the image xIthat isoptimal for the set of captions Kgiven the task T.
In addition, we write SSynSCfor a synthetic shortcut, Sfor the original shared information, i.e., information
that does not contain synthetic shortcuts, S+for the shared information that includes a synthetic shortcut,
andR+for task-relevant information that contains a synthetic shortcut.
In the context of task relevance, we define Rand¬Ras task-relevant and task-irrelevant information,
respectively, and Cas task-relevant information specific for caption xC.
4Setup. We work with a dual-encoder setup, with an image encoder and a caption encoder that do not
share parameters. The image encoder fθ(·)takes an image xIas input and returns its latent representation:
zI:=fθ(xI). Similarly, the caption encoder gϕ(·)takes a caption xCas input, and encodes the caption
into a latent representation: zC:=gϕ(zC). Both zCandzIare unit vectors projected into d-dimensional
multi-modal space: zC∈Rd,zI∈Rd. For an overview of notation, we refer to Appendix A, Table 3.
Assumptions. Given an image-caption tuple, we assume that each caption in the tuple is distinct from the
other captions in the tuple. We also assume that each caption in the tuple contains two types of task-relevant
information: (i) shared information, i.e., information shared with other captions in the same tuple, and
(ii) caption-specific information, i.e., information that is not shared with the other captions. For simplicity,
we base our subsequent analysis on tuples where one image xIis associated with two captions xCAandxCB:/parenleftbig
xI,{xCA,xCB}/parenrightbig
. However, the analysis described in this section can be extended to a case with more than
two captions. We treat images and captions as views and define xI,xCA, and xCBto be random variables
of an image and two matching captions, with the joint distribution p(xI,xCA,xCB). For more details on
assumptions and problem definition, we refer to Appendix B.
2.2Analysis of Contrastive Vision-Language Representation Learning for Multiple Captions per Image
InfoMax. We start our analysis of contrastive VLrepresentation learning by introducing the InfoMax
optimization objective, a typical loss for VLrepresentation learning. The goal of an InfoMax optimization
objective, e.g., InfoNCE (van den Oord et al., 2018), is to maximize the mutual information ( MI) between the
latent representations of two views of the same data (Tschannen et al., 2020). Therefore, the optimization
objective is equivalent to: maxfθ,gϕI(zI;zC)where zI:=fθ(xI)andzC:=gϕ(xC).
Minimally Sufficient Image Representation. During training, batches of image-caption pairs are sampled.
The optimization involves maximizing the MIbetween the image representation zIand the matching caption
representation zC. Wang et al. (2022) argue that, since all supervision information for one view (i.e., the image)
comes from the other view (i.e., the caption), the representations learned contrastively are approximately
minimally sufficient. Following (Tian et al., 2020b; Wang et al., 2022), we extend the definition of sufficient
representation to VLcontext and define sufficient caption representations, sufficient image representations,
and minimally sufficient image representation.
Definition 2.1 (Sufficient caption representation) .Given an image xI, and a set of matching captions
C={xCA,xCB}, the representation zSUF
C→Iof caption xC∈Cis sufficient for image xIif, and only if,
I(zSUF
C→I;xI) =I(xC;xI).
The sufficient caption representation zSUF
C→Icontains all the information about image xIin caption xC.
Definition 2.2 (Sufficient image representation) .Given an image xI, and a set of matching captions
C={xCA,xCB}, the representation zSUF
I→Cof image xIis sufficient for caption xC∈Cif, and only if,
I(zSUF
I→C;xC) =I(xI;xC).
Similarly, the sufficient image representation zSUF
I→Ccontains all the shared information between an image xI
and a caption xC. Note that a sufficient image representation can be sufficient w.r.t. multiple captions.
Definition 2.3 (Minimally sufficient image representation) .Given an image xI, and a set of matching
captionsC={xCA,xCB}, the sufficient image representation zMIN
I→Cof image xIis minimally sufficient for
caption xC∈Cif, and only if, I(zMIN
I→C;xI)≤I(zSUF
I→C;xI), for all zSUF
I→Cthat are sufficient.
Intuitively, zMIN
I→Ccomprises the smallest amount of information about xI(while still being sufficient) and,
therefore, only contains the information that is shared with caption xC, i.e., the non-shared information is
suppressed.
Task-Optimal Image Representation. The definition of task-optimal image representation is based on
the notion of task-relevant information. In the context of VLrepresentation learning with multiple captions
per image, we define task-relevant information as all information described by the matching captions. That
5Shared informationCaption A task-relevant informationTask-relevant informationCaption B task-relevant information
<latexit sha1_base64="BilE5e2YIRPHTuYMzEUDrwEmhCM=">AAAB+nicdVDLSgMxFM34rPU11aWbYBEEYZiR1lZBqHbjsj76gHYYMmnahmYyQ5JRythPceNCEbd+iTv/xkxbQUUP5HI4517uzfEjRqWy7Q9jbn5hcWk5s5JdXVvf2DRzWw0ZxgKTOg5ZKFo+koRRTuqKKkZakSAo8Blp+sNq6jdviZA05DdqFBE3QH1OexQjpSXPzF3BU1j1zuCBrue6Xntm3rbsUqlQLMKUHDuF8pTYZRs6lj1BHsxQ88z3TjfEcUC4wgxJ2XbsSLkJEopiRsbZTixJhPAQ9UlbU44CIt1kcvoY7mmlC3uh0I8rOFG/TyQokHIU+LozQGogf3up+JfXjlWv7CaUR7EiHE8X9WIGVQjTHGCXCoIVG2mCsKD6VogHSCCsdFpZHcLXT+H/pHFoOUdW8bKQr5zM4siAHbAL9oEDSqACLkAN1AEGd+ABPIFn4954NF6M12nrnDGb2QY/YLx9AjoBkWc=</latexit>R=CA+CB+S
<latexit sha1_base64="e32zs8bADxIwlS9rmaYLX2i3TdE=">AAACVXicdVFbS8MwGE2rzlkvm/roS3AI86W0011UBHUv+qbgLrDNkmbpFkwvJKk4av+kL+I/8UUw3SaozAOBk3POR5ITN2JUSMt61/Sl5ZXcan7NWN/Y3CoUt3faIow5Ji0cspB3XSQIowFpSSoZ6UacIN9lpOM+NjO/80S4oGFwLycRGfhoFFCPYiSV5BRZ07mE59C4KRt9H8mx6yXPqZNMOUYsuUnThyQ1zha7TbW5nCVe/k9czRKHhlMsWaZdr1SPT6Bl1jLUFbEbjcpRFdqmNUUJzHHrFF/7wxDHPgkkZkiInm1FcpAgLilmJDX6sSARwo9oRHqKBsgnYpBMW0nhgVKG0Au5WoGEU/XnRIJ8ISa+q5LZbcVfLxMXeb1Yeo1BQoMoliTAs4O8mEEZwqxiOKScYMkmiiDMqborxGPEEZbqI7ISvl8K/yftimnXzOrdcenidF5HHuyBfVAGNqiDC3ANbkELYPAKPjRN07U37VNf1nOzqK7NZ3bBL+iFL3/MtJI=</latexit>CA=I(xI;xCA|xCB)<latexit sha1_base64="fVzNmFhrWBe/Ey8xO6yUsyu4i60=">AAACVHicdVFbS8MwGE07p7Pepj76EhyCvpR2q84LgroXfVNwKmyzpFm6haUXklQctT9SHwR/iS8+mG4TVLYPEk7OOR/5cuLFjAppWR+aXpgrzi+UFo2l5ZXVtfL6xp2IEo5JE0cs4g8eEoTRkDQllYw8xJygwGPk3hs0cv3+iXBBo/BWDmPSCVAvpD7FSCrKLQ8a7gU8hVe7RjtAsu/56XPmpiOMEUuvsuwxzYyT6WpDHS7GjpfZjvOxY89wyxXLtGpH9aoNLdN2nJptKaB2p16DtmmNqgImde2W39rdCCcBCSVmSIiWbcWykyIuKWYkM9qJIDHCA9QjLQVDFBDRSUehZHBHMV3oR1ytUMIR+7sjRYEQw8BTznxa8V/LyWlaK5H+YSelYZxIEuLxRX7CoIxgnjDsUk6wZEMFEOZUzQpxH3GEpfqHPISfl8LZ4K5q2gfm/o1TOTuexFECW2Ab7AIb1MEZuATXoAkweAWfGtA07V370gt6cWzVtUnPJvhT+uo3D0K0WA==</latexit>CB=I(xI;xCB|xCA)
<latexit sha1_base64="8sSucfjJpEo7qrBos8GRkhNsFiA=">AAACU3ichVFdS8MwFE3rd3U69dGX4BDmS2nrNhURdHtxbxOdG2xzpFk6o+kHSSqO0v8ogg/+EV980HSboKJ4IXByzrncmxM3YlRIy3rR9JnZufmFxSVjeSW3upZf37gSYcwxaeKQhbztIkEYDUhTUslIO+IE+S4jLfeulumte8IFDYNLOYpIz0fDgHoUI6mofv72Ah5Do140uj6SN66XPKT9ZIwxYkk9Ta+T1Dj6Xa2py+m/jurEsWv08wXL3HPscuUQWqZVcfZLZQXKzmHJdqBtWuMqgGk1+vmn7iDEsU8CiRkSomNbkewliEuKGUmNbixIhPAdGpKOggHyiegl40xSuKOYAfRCrk4g4Zj92pEgX4iR7ypntq34qWXkb1onlt5BL6FBFEsS4MkgL2ZQhjALGA4oJ1iykQIIc6p2hfgGcYSl+oYshM+Xwr/BlWPaFbN8XiqcVKdxLIItsA2KwAb74AScgQZoAgwewSt414D2rL3puj47seratGcTfCs99wHEMLSh</latexit>S=I(xI;xCA;xCB)
<latexit sha1_base64="77X78/eNhhjULqFuR0dVHf37Lrk=">AAAB9XicdVDLSsNAFJ34rPVVdelmsAh1ExKb1rgruqm7CvYBbSyT6aQdOpmEmYlSQv/DjQtF3Pov7vwbJ20FFT0wcDjnXu6Z48eMSmVZH8bS8srq2npuI7+5tb2zW9jbb8koEZg0ccQi0fGRJIxy0lRUMdKJBUGhz0jbH19mfvuOCEkjfqMmMfFCNOQ0oBgpLd3WS70QqRFGLL2anvQLRcusurZdrkDLdCzHKduaWOdl13WgbVozFMECjX7hvTeIcBISrjBDUnZtK1ZeioSimJFpvpdIEiM8RkPS1ZSjkEgvnaWewmOtDGAQCf24gjP1+0aKQiknoa8ns4zyt5eJf3ndRAWul1IeJ4pwPD8UJAyqCGYVwAEVBCs20QRhQXVWiEdIIKx0UXldwtdP4f+kdWraVbNy7RRrF4s6cuAQHIESsMEZqIE6aIAmwECAB/AEno1749F4MV7no0vGYucA/IDx9glIk5Jo</latexit>H(I)<latexit sha1_base64="oKEumTlgUrKGK9In2acMNB9QGIs=">AAAB7XicdVDLSgMxFM34rPVVdekmWIS6GSZtR+uutJsuK9gHtEPJpJk2NjMZkoxQhv6DGxeKuPV/3Pk3pg9BRQ9cOJxzL/fe48ecKe04H9ba+sbm1nZmJ7u7t39wmDs6biuRSEJbRHAhuz5WlLOItjTTnHZjSXHoc9rxJ/W537mnUjER3eppTL0QjyIWMIK1kdqNQn1Quxjk8o6NSk7JdaFjFyuoiK4NKbvFsosgsp0F8mCF5iD33h8KkoQ00oRjpXrIibWXYqkZ4XSW7SeKxphM8Ij2DI1wSJWXLq6dwXOjDGEgpKlIw4X6fSLFoVLT0DedIdZj9dubi395vUQHFS9lUZxoGpHloiDhUAs4fx0OmaRE86khmEhmboVkjCUm2gSUNSF8fQr/J+2ijS5t96acr9ZWcWTAKTgDBYDAFaiCBmiCFiDgDjyAJ/BsCevRerFel61r1mrmBPyA9fYJiaiOeg==</latexit>H(CB)
<latexit sha1_base64="Yzt6GH2qkskJ4W0ZbwGNsakgYlk=">AAAB7XicdVDLSgMxFM3UV62vqks3wSLUzTAzfS6r3XRZwT6gHUomTdvYTDIkGaEM/Qc3LhRx6/+4829MH4KKHrhwOOde7r0niBhV2nE+rNTG5tb2Tno3s7d/cHiUPT5pKxFLTFpYMCG7AVKEUU5ammpGupEkKAwY6QTT+sLv3BOpqOC3ehYRP0RjTkcUI22kdiNfH1xdDrI5x3bK5WK1BB276FYqBc8Q1/MKrgdd21kiB9ZoDrLv/aHAcUi4xgwp1XOdSPsJkppiRuaZfqxIhPAUjUnPUI5Covxkee0cXhhlCEdCmuIaLtXvEwkKlZqFgekMkZ6o395C/MvrxXpU9RPKo1gTjleLRjGDWsDF63BIJcGazQxBWFJzK8QTJBHWJqCMCeHrU/g/aXu2W7ZLN8Vc7XodRxqcgXOQBy6ogBpogCZoAQzuwAN4As+WsB6tF+t11Zqy1jOn4Aest0+NQo58</latexit>H(CA)Figure 3: We define H(xI)as image information, H(xCA)andH(xCB)as caption information; both
captions only describe the information depicted in the image and contain shared and caption-specific
information. We further define CA=I(xI;xCA|xCB)andCB=I(xI;xCB|xCA)as caption-specific
information; S=I(xI;xCA;xCB)assharedinformation; ¬R=H(xI|xCA,xCB)astask-irrelevantinformation;
R=CA+CB+Sas task-relevant information.
includes both caption-specific and shared information. Consequently, task-optimal image representation is
image representation that is sufficient w.r.t. all matching captions.
Formally, following assumptions from Appendix B.2, we define task-relevant information Ras all the
information described by the matching captions. The task-relevant information can be expressed as follows:
R/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Task-relevant
information=H(xI)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Image
information−H(xI|xCA,xCB)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Task-irrelevant
information
=I(xI;xCA|xCB)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
CA-specific
task-relevant information+I(xI;xCB|xCA)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
CB-specific
task-relevant information+I(xI;xCA;xCB)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Shared
information.(1)
Similarly, task-irrelevant information ¬Ris the image information not described by the captions. Figure 3
illustrates both definitions.
The multi-view assumption states that task-relevant information for downstream tasks comes from the
information shared between views (Shwartz-Ziv & LeCun, 2023). However, in the case of VLrepresentation
learning with multiple captions per image, task-relevant information Rincludes both shared information S,
and caption-specific information CAandCB(Eq. 1).
Definition 2.4 (Task-optimal image representation) .Given an image xI, and a set of matching captions
C={xCA,xCB}, the representation zOPT
I→Cis task-optimal image representation for all matching captions if,
and only if, I(zOPT
I→C;xC) =I(xI;xC), for all xC∈C.
In other words, task-optimal image representations contain all the information that the image shares with the
matching captions. Hence, a task-optimal image representation is sufficient w.r.t. all matching captions. The
information contained in the task-optimal image representation includes both shared and caption-specific
information. Therefore, a task-optimal image representation can never be a minimally sufficient image
representation w.r.t. to a specific caption.
Theorem 1 (Suboptimality of contrastive learning with multiple captions per image) .Given an image xI, a
set of matching captions C={xCA,xCB}, and a contrastive learning loss function LInfoNCEthat optimizes for
taskT, image representations learned during contrastive learning will be minimally sufficient and will never
be task-optimal image representations.
The proof is provided in Appendix C. Rephrasing Theorem 1, given an image and two captions that form
two image-caption pairs, (xI,xCA)and(xI,xCB), and assuming that contrastive loss optimizes the image
encoder to be minimally sufficient w.r.t. to caption xCAduring a training step, all task-relevant information
CBspecific to caption xCBwill be suppressed in zI. Hence, the resulting image representation will not be
optimal for the task T.
6Theorem 1 indicates a discrepancy between minimally sufficient representations learned during contrastive
training with the InfoNCE loss and the task-optimal image representations in the context of learning VL
representations with multiple captions per image. Although the InfoMax loss does not have an explicit
constraint to compress information, prior work indicates that feature suppression is happening (Shwartz-Ziv &
LeCun, 2023; Robinson et al., 2021). Hence, we question if contrastive loss can be used to learn task-optimal
image representations in the context of multiple captions per image.
Furthermore, Theorem 1 implies that in the context of contrastive VLrepresentation learning with multiple
captions per image, the minimally sufficient representation, which discards non-shared information, is not
the same as the task-optimal representation that comprises both caption-specific and shared information.
This suggests that the features learned during contrastive learning might be shortcuts, i.e., easy-to-detect
discriminatory features that minimize the contrastive optimization objective but are not necessarily sufficient
for solving the evaluation task. To examine this problem, we introduce a synthetic shortcuts framework that
allows us to investigate the problem of suboptimality of contrastive learning with multiple captions per image
in a controlled way.
3 Synthetic Shortcuts to Control Shared Information
In Section 2 we show the suboptimality of the contrastive InfoNCE loss with multiple captions per image. In
the case of real-world VLdatasets with multiple captions per image, there are no annotations that indicate the
information shared between the image and captions and the information specific to each caption. Hence, we
cannot directly measure how much of the shared and unique information is captured by the representations.
A player up to bat in a baseball game. 1 0 1 9 9 2
Figure 4: An image-caption pair from the
MS-COCO dataset with a shortcut added
to both the image and the caption.Synthetic Shortcuts. In this section, we introduce the syn-
thetic shortcuts for vision-language ( SVL)training and evalu-
ation framework. We denote the synthetic shortcuts for image-
caption data asSSynSC. The purpose of the framework is to
introduce additional and easily identifiable information shared
between an image and the matching captions that lacks any
semantic meaning. The shortcuts we use in this work are rep-
resented as numbers that we add to images and captions. For
images, we add the shortcut number by adding MNIST images
as an overlay to the original images. For captions, we append
the numbers of the shortcut as extra tokens at the end of the
caption.
Figure 4 illustrates an example of an image-caption pair with
an added shortcut. The example contains an image with the
caption: ‘A player up to bat in a baseball game. 1 0 1 9 9 2.’
Here, ‘1 0 1 9 9 2’ is a shortcut added to both the image and
the caption. For the image modality, we add the shortcut by
overlaying MNIST images at the top of the original image. For the text modality, we append the shortcut as
additional tokens at the end of the caption. This identifier provides an additional link between the image and
the caption without carrying any semantic meaning related to their content. Additional examples are shown
in Figure 6.
If contrastive losses learn task-optimal representations, then the presence of synthetic shortcuts should not
negatively impact the evaluation performance, since synthetic shortcuts represent additional information and
the remaining task-relevant information is intact. By incorporating synthetic shortcuts into the image-caption
dataset, the shared information would include the information that was originally shared and the synthetic
shortcut:S+=S+SSynSC. Hence, the task-relevant information would comprise caption-specific information
that was originally shared and a synthetic shortcut: R+=CA+CB+S+SSynSC. If injecting a synthetic
shortcut influences the performance negatively, we can conclude that by learning to represent a synthetic
shortcut the model suppresses other task-relevant information in favor of the shortcut, hence the representation
7is not task-optimal. The setup is inspired by the “datasets with explicit and controllable competing features,”
introduced by Chen et al. (2021), but we adapt this setup to the VL scenario.
For experiments, we use the Flickr30k andMS-COCO image-caption datasets, that consist of image-caption
tuples, each image is associated with five captions. During training, we sample a batch of image-caption
pairsB={(xi
I,xi
Cj),...}|B|
i=1, from datasetD, and apply shortcut sampling. We inject the shortcuts in a
manner that preserves the original information of the images and captions. Furthermore, we append the
shortcut after applying data augmentations to ensure that the shortcut is present in both the images and
captions (i.e., the shortcut is not augmented away). We refer to Figure 6 for some examples. The training,
evaluation, and implementation details of the shortcut sampling are provided in Appendix D.4.
We define the following experimental setups:
INo shortcuts : As a baseline, we fine-tune a pre-trained CLIP (Radford et al., 2021) and train
VSE++ (Faghri et al., 2018) from scratch on Flickr30k andMS-COCO , without using any shortcuts.
The experimental setup for training both models is provided in Appendix D.2 and D.3. The goal of
this setup is to show the retrieval evaluation performance without adding any shortcuts for both a
large-scale pre-trained foundation model and a small-scale model trained from scratch.
IIUnique shortcuts : We add a unique shortcut to each image-caption tuple i∈Din the dataset. In
this setup, each image caption pair can be uniquely matched during training by only detecting the
shortcut. For each tuple i∈D, we use the number ias the number of the shortcut we inject to
the image and captions in the tuple. If the contrastive loss learns task-optimal representations, the
downstream evaluation performance should not decrease when training with unique shortcuts.
IIIUnique shortcuts on only one modality : To show that the shortcuts do not interfere with the original
task-relevant information ( S,CA, andCB) of the images and captions, we create a dataset with only
shortcuts on either the image or caption modality. Therefore, the shortcut cannot be used by the
encoders to match an image-caption pair. Hence, we expect the encoders to ignore the shortcuts and
extract the features from the original data similar to the features learned by the baseline models in
experimental setup I.
IVN bits of shortcuts : In this setup, for each image-caption pair in the training batch B, we randomly
sample a shortcut number from the range [0,2n], wherenis the number of bits. The higher the value
ofn, the more image-caption pairs in the training batch will have by expectation a unique shortcut,
and, the less the model has to rely on Sand the remaining task-relevant information to solve the
contrastive objective. The goal of this setup is to show that, the more unique (shortcut) information
is present per sample in the batch, the less contrastive models rely on the remaining task-relevant
information.
It should be noted that the shortcuts we add are independent of the image-caption pairs. However, the goal
of theSVLframework is to measure the effect of the presence of additional easy-to-detect shared information
on the learned representations.
Evaluation Method. To show the effect of the injected shortcuts on retrieval evaluation performance, we
evaluate both with and without adding the shortcuts during evaluation. When training with unique shortcuts,
we add a unique shortcut to each tuple in the test set as well. When training with shortcuts on either one of
the two modalities, we only evaluate without shortcuts to show that training with shortcuts on one modality
does not influence performance. When training with nbits of shortcuts, we add the shortcut mod (i,n)
(modulo) to each tuple iin the evaluation set, to make sure we use the same number of shortcuts during
evaluation as during training. To facilitate the reproducibility and support further research, we provide the
code with our paper.1
1https://github.com/MauritsBleeker/svl-framework
8012345678910111213
Flickr30k0100200300400500600Recall sum
Number of bitsMax eval score
CLIP zero-shot
Baseline -  no shortcutsOnly shortcuts on the captions
Only shortcuts on the images
Unique shortcuts - eval w/ shortcutsUnique shortcuts - eval w/o shortcuts
N bits of shortcuts - eval w/ shortcuts
N bits of shortcuts - eval w/o shortcuts
012345678910111213
MS-COCO0100200300400500600
Number of bits(a) Evaluation results for the CLIP model when using different shortcut sampling setups.
012345678910111213
Flickr30k0100200300400500600Recall sum
Number of bits012345678910111213
MS-COCO0100200300400500600
Number of bits
(b) Evaluation results for the VSE++ model when using different shortcut sampling setups.
Figure 5: Effect of synthetic shortcuts on CLIP and VSE++ performance on ICRtask. The dotted line
represents the maximum achievable recall sum, while the dashed line for CLIP indicates its zero-shot evaluation
performance (Best viewed in color.)
4 Synthetic Shortcuts and their Impact on Learned Representations and Evaluation
Performance
4.1 Findings
First, we train and evaluate both a CLIP and VSE++ without shortcuts on the Flickr30k andMS-COCO
dataset for the image-caption retrieval task as a baseline. We use the recall sum (i.e., the sum of R@1,R@5,
andR@10for both image-to-text ( i2t) and text-to-image ( t2i) retrieval) as evaluation metric (see Appendix B.1
for the evaluation task description). We visualize the results in Figure 5. The dotted line (in Figure 5a and 5b)
indicates the maximum evaluation score (i.e., 600). For CLIP, we also provide the zero-shot performance of
the model, indicated by the dashed line in Figure 5a. When referring to specific results in Figure 5, we use
the color of the corresponding bar and legend key in brackets in the text.
Based on Figure 5, we draw the following conclusions:
IWhen training CLIP and VSE++ with only shortcuts on either the caption modality (in Figure 5, the
corresponding bar/legend box is colored ) or on the image modality ( , in Figure 5), we do
not observe a drop in evaluation scores for CLIP compared to the baseline model ( , in Figure 5a).
For VSE++ we only observe a slight drop in evaluation score when training with shortcuts on the
caption modality (again , mainly for MS-COCO , in Figure 5b). Therefore, we conclude that the
9synthetic shortcuts do not interfere with the original shared information Sor other task-relevant
information.
IIWhen training the models with unique shortcuts , we observe for both CLIP and VSE++ that when
evaluating with shortcuts ( , in Figure 5), the models obtain a perfect evaluation score. When
evaluating without shortcuts ( , in Figure 5) the evaluation score for VSE++ drops to zero and
for CLIP below the zero-shot performance. We conclude that with unique shortcuts: (i) both CLIP
and VSE++ fully rely on the shortcuts to solve the evaluation task, (ii) VSE++ has not learned any
other shared or task-relevant information other than the shortcuts (since it is trained from scratch,
only detecting the shortcuts is sufficient to minimize the contrastive loss), and (iii) fine-tuned CLIP
has suppressed original features from the zero-shot model in favor of the shortcuts.
IIIWhen training the models with N bits of shortcuts , we observe for both CLIP and VSE++ that the
larger the number of bits we use during training and when evaluating without shortcuts ( , in
Figure 5), the bigger the drop in evaluation performance. When we evaluate with shortcuts ( ,
in Figure 5), the evaluation performance improves as we use more bits compared to the baseline
without shortcuts , in Figure 5). For VSE++, evaluating without shortcuts ( , in Figure 5b)
results in a drop to zero when having a large number of bits. For CLIP, the evaluation performance
drops below the zero-shot performance. If we train with 0 bits of shortcuts (i.e., the shortcut is a
constant) we do not observe any drop or increase in evaluation scores for CLIP.
4.2 Upshot
Given the findings based on Figure 5 we conclude that a contrastive loss (i.e., InfoNCE) mainly learns
the easy-to-detect minimal shared features among image-caption pairs that are sufficient to minimize the
contrastive objective while suppressing the remaining shared and/or task-relevant information. If contrastive
losses are sufficient to learn task-optimal representations for image-caption matching, these shortcuts should
not adversely impact the evaluation performance. Moreover, if the contrastive loss would only learn features
that are shared among the image and all captions (i.e, S), we should not observe a drop in performance
to 0 for the VSE++ model when training with unique shortcuts, since there is still a lot of task-relevant
information present in S. Especially in a training setup where a model is trained from scratch or fine-tuned
on small datasets, the easy-to-detect features are likely not equivalent to all task-relevant information in the
images and captions. Hence, we conclude that contrastive loss itself is not sufficient to learn task-optimal
representations of the images (and sufficient representations of captions) and that it only learns the minimal
easy-to-detect features that are needed to minimize the contrastive objective.
5 Reducing Shortcut Learning
In the earlier section, we have demonstrated that contrastive loss mainly relies on the minimal, easy-to-detect
features shared among image-caption pairs while suppressing remaining task-relevant information. In this
section, we describe two methods that help to reduce shortcut learning for contrastive learning on our SVL
framework: Latent target decoding (Bleeker et al., 2023) and implicit feature modification (Robinson et al.,
2021).
5.1 Latent Target Decoding
Latent target decoding ( LTD) (Bleeker et al., 2023) is a method to reduce predictive feature suppression (i.e.,
shortcut learning) for resource-constrained contrastive image-caption matching. The contrastive objective
(i.e., InfoNCE) is combined with an additional reconstruction loss, which reconstructs the input caption from
the latent representation of the caption zi
Cj. We refer to Appendix E.2 for the mathematical definition of LTD.
Instead of reconstructing the tokens of the input caption in an auto-regressive manner (i.e., auto-encoding),
the caption is reconstructed non-auto-regressively, by mapping the caption representation into the latent
space of a Sentence-BERT (Reimers & Gurevych, 2019; Song et al., 2020) and minimizing the distance (i.e.,
reconstructing) between the reconstruction and the Sentence-BERT representation of the caption xi
Cj. The
assumption is that the targetgenerated by the Sentence-BERT model contains all task-relevant information
10in the caption. Hence, by correctly mapping the latent caption representation zi
Cjinto the latent space
of Sentence-BERT, the caption encoder cannot suppress any task-relevant information or rely on shortcut
solutions. LTDis implemented both as a dual-loss objective (i.e., the contrastive loss and LTDare added
up) and as an optimization constraint while minimizing the InfoNCE loss, by implementing the loss as a
Lagrange multiplier. For the mathematical definition of LTD, we refer to Appendix E.2.
Experimental Setup. We use the LTDimplementation and set-up similar to Bleeker et al. (2023). We train
both CLIP and VSE++ with LTD, implemented as either dual loss or an optimization constraint. When
implementing LTDas a constraint, we try η∈{0.01,0.05,0.1,0.15,0.2,0.25,0.3}as bound values. Similar to
Bleeker et al. (2023), when implementing LTDas a dual loss, we use β= 1as balancing parameters. We
train both with and without unique shortcuts. We do this to show (i) what the performance improvement is
compared to using only InfoNCE, and (ii) to what degree LTDprevents full collapse to shortcut features. For
each model and dataset, we take the training setup that results in the highest performance on the validation
set.
5.2 Implicit Feature Modification
Implicit feature modification ( IFM) (Robinson et al., 2021) is a method, originally introduced in the context
of representation learning for images, that applies perturbations to logits used for guiding contrastive models.
IFMperpetuates features that the encoders use during a training step to discriminate between positive and
negative samples. By doing so, IFMalters the features that are currently used to solve the discrimination
task, to avoid the InfoNCE loss to learn shortcut solutions. How much of the features are removed, is defined
by a perturbation budget ϵ.IFMis implemented as a dual loss in combination with the InfoNCE loss. For
the mathematical definition of IFM, we refer to Appendix E.3.
Experimental Setup. We apply a similar experimental set-up for IFMas forLTD. We apply IFMboth to
CLIP and to VSE++, both with and without unique shortcuts. Similar to (Robinson et al., 2021), we try
different perturbation budgets ϵ, we tryϵ∈{0.05,0.1,0.2,0.5,1}. In line with the LTDsetup, we take the
training setup that results in the highest performance on the validation set.
5.3 Method Comparison
BothLTDandIFMaim to mitigate shortcut learning through different approaches. LTDaims to learn all
task-relevant information by reconstructing the input captions. In contrast, IFMperturbs the discriminative
features in the latent space of the encoder and does not rely on a reconstruction objective. Overall, both
methods represent distinct strategies for improving the robustness and generalization capabilities of VL
representation learning.
In the following section, we present experimental results with LTDandIFM, providing insight into their
effectiveness in mitigating shortcut learning.
6 Experimental Results
6.1 Does Latent Target Decoding Reduce Shortcut Learning?
In Table 1 we summarize the effect of LTD on reducing shortcut learning.
For CLIP, for both the Flickr30k andMS-COCO dataset, we do not observe an increase in recall scores
when fine-tuning with LInfoNCE+LTD compared to models that are only fine-tuned with LInfoNCE. LTD has
originally been proposed for resource-constrained VLmodels. We argue that the additional features that
LTD can extract are either already present in the pre-trained CLIP model, or not relevant for the evaluation
task. However, when fine-tuning with LInfoNCE+LTD and in the presence of shortcuts in the training data,
degradation in recall scores is significantly lower than when fine-tuned only with the LInfoNCE. This shows
that LTD can reduce the suppression of features in favor of the shortcut features when fine-tuning large-scale
VL models.
11Table 1: Mean and variance (over three training runs) recall@ kevaluation scores for the Flickr30k and
MS-COCO datasets for image-to-text and text-to-image retrieval. We train with two loss functions: LInfoNCE
andLInfoNCE+LTD . We train either with ( ✓) or without ( ✗) shortcuts. For the model trained with
LInfoNCE+LTD , we provide the hyper-parameters of the best-performing model. ηindicates that the best-
performing model uses LTD implemented as an optimization constraint with bound η.βindicates that the
best-performing model uses LTD implemented as a dual-loss with β= 1.
i2t t2i
Loss SSynSC R@1 R@5 R@10 R@1 R@5 R@10 rsum
Flickr30k
CLIP
LInfoNCE ✗ 86.9±0.197.4±0.199.0±0.072.4±0.192.1±0.095.8±0.0543.5±1.1
LInfoNCE+LTD ,β= 1 ✗ 86.5±0.6–97.1±0.0↓98.5±0.0↓72.4±0.0–92.3±0.0↓95.9±0.0↓542.8±0.8–
LInfoNCE ✓ 57.2±8.384.0±4.891.0±1.944.9±4.574.9±6.084.2±2.5436.2±145.0
LInfoNCE+LTD ,β= 1 ✓64.0±1.3↑87.8±0.9↑93.2±0.8↑50.7±0.6↑79.8±0.7↑88.1±0.5↑463.6±17.3↑
VSE++
LInfoNCE ✗ 52.6±1.179.8±0.187.8±0.139.5±0.369.8±0.079.4±0.1409.0±4.0
LInfoNCE+LTD ,η= 0.2 ✗54.1±0.1↑81.1±0.8↑88.6±0.1↑42.5±0.0↑71.9±0.1↑81.3±0.0↑419.6±0.1↑
LInfoNCE ✓ 0.1±0.0 0.6±0.1 1.1±0.1 0.1±0.0 0.5±0.0 1.0±0.0 3.4±0.6
LInfoNCE+LTD ,η= 0.05✓24.7±0.5↑51.8±0.7↑65.6±1.4↑20.7±1.0↑49.2±0.6↑62.6±1.2↑274.6±4.6↑
MS-COCO
CLIP
LInfoNCE ✗ 63.8±0.386.1±0.292.3±0.046.3±0.374.8±0.184.1±0.2447.5±0.5
LInfoNCE+LTD ,β= 1 ✗ 63.8±0.0–86.1±0.0–92.3±0.0–46.3±0.0–74.7±0.0–84.1±0.0–447.4±0.0–
LInfoNCE ✓ 13.6±0.931.5±2.442.2±3.7 7.3±0.622.1±1.032.7±1.7149.4±32.7
LInfoNCE+LTD ,β= 1 ✓18.9±0.1↑41.8±0.1↑54.1±0.1↑16.5±0.0↑39.4±0.0↑52.6±0.1↑223.4±0.2↑
VSE++
LInfoNCE ✗ 42.2±0.172.7±0.183.2±0.130.9±0.061.2±0.173.5±0.1363.8±2.3
LInfoNCE+LTD ,η= 0.1 ✗43.6±0.1↑73.5±0.0↑83.7±0.0↑32.4±0.1↑62.5±0.0↑74.7±0.0370.5±0.1↑
LInfoNCE ✓ 0.0±0.0 0.1±0.0 0.2±0.0 0.0±0.0 0.1±0.0 0.2±0.0 0.7±0.0
LInfoNCE+LTD ,η= 0.01✓3.9±0.0↑13.7±0.6↑21.6±0.9↑3.1±0.2↑11.0±1.6↑18.1±3.0↑71.3±3.6↑
Across the board, VSE++ models trained with the LInfoNCE+LTD loss consistently outperform the LInfoNCE
loss, both for i2tandt2iretrieval and both when trained either with or without shortcuts, as indicated by
higher recall@ kscores; this is consistent with the findings presented in (Bleeker et al., 2023)). For both the
Flickr30k andMS-COCO dataset, when trained with the LInfoNCEand with shortcuts present in the training
data, the model performance collapses to around 0 in the absence of shortcuts (as we have seen in Section 4).
However, when we train with shortcuts in the training data and with LInfoNCE+LTD , we observe, for both
Flickr30k andMS-COCO , a significant gain in performance. The performance improvement is bigger for
Flickr30k than for MS-COCO . In general, the recall scores are still significantly lower than training without
shortcuts, however, the models do not solely rely on the shortcuts anymore to minimize the contrastive loss
and are able during evaluation (in the absence of shortcuts) to still correctly match image-caption pairs with
each other. The results in Table 1 show that LTD is able, in the presence of shortcuts in the training data,
to guide (small-scale) VLmodels that are trained from scratch to not only learn the shortcut features that
12Table 2: Mean and variance (over three training runs) recall@ kevaluation scores for the Flickr30k and
MS-COCO datasets for image-to-text and text-to-image retrieval. We train with two loss functions: LInfoNCE
andLInfoNCE+IFM . We train either with ( ✓) or without ( ✗) shortcuts. For the model trained with
LInfoNCE+IFM , we provide the hyper-parameters of the best-performing model.
i2t t2i
Loss SSynSC R@1 R@5 R@10 R@1 R@5 R@10 rsum
Flickr30k
CLIP
LInfoNCE ✗ 86.9±0.197.4±0.098.8±0.072.8±0.292.1±0.095.6±0.0543.5±1.3
LInfoNCE+IFM ,ϵ= 0.05 ✗87.4±0.1↑97.4±0.2–99.1±0.0–73.2±0.0–92.2±0.0–95.6±0.0–544.9±0.2–
LInfoNCE ✓ 57.9±0.384.6±0.891.3±0.043.9±2.274.6±0.884.4±0.4436.7±18.8
LInfoNCE+IFM ,ϵ= 0.1✓73.8±0.8↑91.5±0.5↑95.6±0.0↑58.9±0.1↑84.4±0.1↑91.1±0.2↑495.2±5.7↑
VSE++
LInfoNCE ✗52.9±0.280.5±0.187.6±0.440.5±0.168.8±0.478.9±0.3409.3±2.6
LInfoNCE+IFM ,ϵ= 0.05 ✗ 52.4±0.2↓76.9±0.1↓85.3±0.0↓39.1±0.0↓68.8–±0.178.2±0.1↓400.7±0.0↓
LInfoNCE ✓ 0.1±0.0 0.4±0.0 0.8±0.0 0.1±0.0 0.4±0.0 1.0±0.0 2.9±0.0
LInfoNCE+IFM ,ϵ= 0.05✓ 0.0±0.0–0.6±0.1–0.9±0.2–0.1±0.0–0.5±0.0–1.0±0.0– 3.2±0.8–
MS-COCO
CLIP
LInfoNCE ✗63.5±0.186.0±0.392.2±0.046.3±0.074.7±0.084.2±0.0446.9±0.9
LInfoNCE+IFM ,ϵ= 0.05 ✗ 63.0±0.1↓86.6±0.1↓92.6±0.2↓47.2±0.0↑75.6±0.0↑84.5±0.0↑449.5±1.7↑
LInfoNCE ✓ 13.9±0.032.7±0.143.8±0.0 8.8±0.024.7±0.235.5±0.5159.4±3.4
LInfoNCE+IFM ,ϵ= 0.05✓23.4±1.5↑46.5±2.7↑58.2±2.5↑17.1±0.3↑38.9±0.9↑51.3±1.0↑235.5±43.8↑
VSE++
LInfoNCE ✗41.7±0.372.5±0.183.1±0.131.3±0.061.1±0.073.6±0.0363.4±0.4
LInfoNCE+IFM ,ϵ= 0.05 ✗ 40.2±0.0↓70.8±0.1↓81.6±0.1↓30.8±0.0↓61.5±0.0↑74.3±0.0↑359.3±1.1↓
LInfoNCE ✓ 0.0±0.0 0.1±0.0 0.2±0.0 0.0±0.0 0.1±0.0 0.2±0.0 0.6±0.0
LInfoNCE+IFM ,ϵ= 0.05✓ 0.0±0.0–0.1±0.0–0.2±0.0–0.0±0.0–0.1±0.0–0.2±0.0– 0.7±0.0–
minimize the contrastive training objective but also represent other remaining task-relevant features in the
data that are not extracted by LInfoNCE.
6.2 Does Implicit Feature Modification Reduce Shortcut Learning?
In Table 2 we summarize the effect of IFM on reducing shortcut solutions.
For CLIP, we observe that LInfoNCE+IFM , when training without shortcuts in the training data, only improves
performance for the MS-COCO dataset for the t2itask. However, for both Flickr30k andMS-COCO we
observe that, when training with unique shortcuts in the training data, fine-tuning with LInfoNCE+IFM results
in a significantly lower performance drop in recall score than when fine-tuning with the LInfoNCE. Similar
to LTD, the recall@ kscores are still lower than when trained without shortcuts in the training data. We
conclude that IFM is sufficient to reduce the suppression of features in favor of the shortcut features when
fine-tuning a large-scale VLmodel, as indicated by higher recall@ kscores when evaluating without shortcuts.
13For VSE++, both for the Flickr30k andMS-COCO dataset, we do not observe that LInfoNCE+IFM outper-
forms theLInfoNCE, both with and without shortcuts present in the training data. We even observe that
LInfoNCE+IFM , when training without shortcuts, results in a decrease in performance across all recall@ k
metrics. When training with LInfoNCE+IFM and with unique shortcuts in the training data, the evaluation
performance still collapses to around 0. The results in Table 2 show that IFM is not sufficient to prevent
models trained from scratch from fully collapsing to the artificial shortcut solutions we introduce in this work
(as opposed to LTD).
6.3 Upshot
In this section, we have evaluated two methods for reducing shortcut learning on our SVLframework: LTD
andIFM.LTDproves effective in reducing shortcut learning for both CLIP and VSE++. IFMdemonstrates
its efficacy solely during the fine-tuning of CLIP. These findings indicate that our SVLframework is a
challenging and interesting framework to study and evaluate shortcut learning for contrastive VLmodels.
Moreover, our results show that shortcut learning is only partially addressed by the evaluated methods since
the evaluation results are not on par with the results on data lacking synthetic shortcuts.
7 Related work
We discuss related work on multi-view representation learning, vision-language learning, and shortcut learning.
Multi-view Representation Learning. To learn the underlying semantics of the training data, a subgroup
of representation learning methods involves training neural encoders that maximize the agreement between
representations of the similar views(van den Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020a; Radford
et al., 2021; Bardes et al., 2022). In general, for uni-modal representation learning, data augmentations
are used to generate different views of the same data point. One of the core assumptions in multi-view
representation learning is that each view shares the same task-relevant information (Sridharan & Kakade, 2008;
Zhao et al., 2017; Federici et al., 2020; Tian et al., 2020a; Shwartz-Ziv & LeCun, 2023). However, the optimal
view for contrastive self-supervised learning ( SSL) (i.e., which information is shared among views/which data
augmentation is used) is task-dependent (Tian et al., 2020b; Xiao et al., 2021). Therefore, maximizing the
mutual information ( MI) between representations of views (i.e., shared information) does not necessarily
result in representations that generalize better to down-stream evaluation tasks, since the representations may
contain too much additional noise that is irrelevant for the downstream task (Tian et al., 2020b; Tschannen
et al., 2020). An open problem in multi-view SSLis to learn representations that contain all task-relevant
information from views where each view contains distinct, task-relevant information (Shwartz-Ziv & LeCun,
2023), this is especially a problem in the multi-modal learning domain (Zong et al., 2023).
Chen et al. (2021) investigate multi-view representation learning for images using contrastive losses. They
demonstrate that when multiple competing features exist that redundantly predict the match between two
views, contrastive models tend to focus on learning the easy-to-represent features while suppressing other
task-relevant information. This results in contrastive losses mainly capturing the easy features, even if all
task-relevant information is shared between the two views, suppressing the remaining relevant information.
Several optimization objectives have been introduced to either maximize the lower bound on the MIbetween
views and their latent representations (van den Oord et al., 2018; Bachman et al., 2019; Hjelm et al., 2019;
Tian et al., 2020a) or minimize the MIbetween representations of views while keeping the task-relevant
information (Federici et al., 2020; Lee et al., 2021). To learn more task-relevant information that either might
not be shared between views or that is compressed by a contrastive loss, several works proposed additional
reconstruction objectives to maximize the MI between the latent representation and input data (Tsai et al.,
2021; Wang et al., 2022; Li et al., 2023b; Bleeker et al., 2023). Liang et al. (2023) introduce a multimodal
contrastive objective that factorizes the representations into shared and unique information, while also
removing task-irrelevant information by minimizing the upper bound on MI between similar views.
Vision-language Representation Learning. The goal of VLrepresentation learning is to combine
information from the visual and textual modalities into a joint representation or learn coordinated represen-
14tations (Baltrusaitis et al., 2019; Guo et al., 2019). The representation learning approaches can be separated
into several groups.
Contrastive methods represent one prominent category of VLrepresentation methods. The approaches in
this group are typically dual encoders. Early methods in this category are trained from scratch; for instance,
(Frome et al., 2013) proposed a VLrepresentation learning model that features a skip-gram language model
and a visual object categorization component trained with hinge rank loss. Another subgroup of methods
uses adual-encoder with a hinge-based triplet loss (Kiros et al., 2014; Li et al., 2019a; Lee et al., 2018).
Kiros et al. (2014) use the loss for training a CNN-RNN dual encoder. Li et al. (2019a) leverage bottom-up
attention and graph convolutional networks (Kipf & Welling, 2017) to learn the relationship between image
regions. Lee et al. (2018) add stacked cross-attention to use both image regions and words as context.
More recently, contrastive approaches involve transformer-based dual-encoders trained with more data than
the training data from the evaluation set(s). ALBEF (Li et al., 2021) propose to contrastively align unimodal
representations before fusion, while X-VLM (Zeng et al., 2022) employs an additional cross-modal encoder
to learn fine-grained VLrepresentations. Florence (Yuan et al., 2021) leverages various adaptation models
for learning fine-grained object-level representations. CLIP (Radford et al., 2021), a scaled-up dual-encoder,
is pre-trained on the task of predicting which caption goes with which image. ALIGN (Jia et al., 2021)
uses a simple dual-encoder trained on over a billion image alt-text pairs. FILIP (Yao et al., 2022) is
a transformer-based bi-encoder that features late multimodal interaction meant to capture fine-grained
representations. SLIP (Mu et al., 2022) combines language supervision and image self-supervision to learn
visual representations without labels. DeCLIP (Li et al., 2022b) proposes to improve the efficiency of CLIP
pretraining using intra-modality self-supervision, cross-modal multi-view supervision, and nearest neighbor
supervision.
Another line of work includes learning VLrepresentations using models that are inspired by BERT (Devlin
et al., 2019). ViLBERT (Lu et al., 2019) and LXMERT (Tan & Bansal, 2019) expand upon BERT by
introducing a two-stream architecture, where two transformers are applied to images and text independently,
which is fused by a third transformer in a later stage. B2T2 (Alberti et al., 2019), VisualBERT (Li et al.,
2019b), Unicoder-VL (Li et al., 2020a), VL-BERT (Su et al., 2020), and UNITER (Chen et al., 2020b) propose
a single-stream architecture, where a single transformer is applied to both images and text. Oscar (Li et al.,
2020b) uses caption object tags as anchor points that are fed to the transformer alongside region features.
BEIT-3 (Wang et al., 2023) adapt multiway transformers trained using cross-entropy loss (Bao et al., 2022).
Another category of methods for learning VLrepresentations are generative methods, that imply learning VL
representation by generating new instances of one modality conditioned on the other modality. For instance,
BLIP (Li et al., 2022a) bootstraps captions by generating synthetic captions and filtering out the noisy
ones; BLIP-2 (Li et al., 2023a) bootstraps VLrepresentation learning and, subsequently, vision-to-language
generative learning. On the other hand, Tschannen et al. (2023) propose to pretrain a encoder-decoder
architecture via the image captioning task.
Shortcut Learning. Geirhos et al. (2020) define shortcuts in deep neural networks as “decision rules that
perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as
real-world scenarios.” In the context of deep learning, a shortcut solution can also be seen as a discrepancy
between the features that a model has learned during training and the intended features that a model should
learn to perform well during evaluation. For example, shortcuts might be features that minimize the training
objective but are much easier to detect than the intended features that are relevant to the evaluation task.
Shortcut learning can be caused by biases in the dataset or inductive biases in either the network architecture
or training objective.
Hermann & Lampinen (2020) design a dataset with multiple predictive features, where each feature can be
used as a label for an image classification task. The authors show that in the presence of multiple features
that each redundantly predicts the target label, the deep neural model chooses to represent only one of
the predictive features that are the easiest to detect, i.e., the model favors features that are easy to detect
over features that are harder to discriminate. Next to that, they show that features that are not needed
for a classification task, are in general suppressed by the model instead of captured in the learned latent
representations.
15Robinson et al. (2021) show that contrastive losses can have multiple local minima, where different local
minima can be achieved by suppressing features from the input data (i.e., the model learns a shortcut by not
learning all task-relevant features). To mitigate the shortcut learning problem, Robinson et al. (2021) propose
implicit feature modification, a method that perpetuates the features of positive and negative samples during
training to encourage the model to capture different features than the model currently relies on.
Scimeca et al. (2022) design an experimental set-up with multiple shortcut cues in the training data, where
each shortcut is equally valid w.r.t. predicting the correct target label. The goal of the experimental setup is
to investigate which cues are preferred to others when learning a classification task.
Latent target decoding ( LTD) is a method to reduce predictive feature suppression (i.e., shortcuts) for
resource-constrained contrastive ICRby reconstructing the input caption in a non-auto-regressive manner.
Bleeker et al. (2023) argue that most of the task-relevant information for the ICRtask is captured by the
text modality. Hence, the focus is on the reconstruction of the text modality instead of the image modality.
Bleeker et al. (2023) add a decoder to the learning algorithm, to reconstruct the input caption. Instead
of reconstructing the input tokens, the input caption is reconstructed in a non-autoregressive manner in
the latent space of a Sentence-BERT (Reimers & Gurevych, 2019; Song et al., 2020) model. LTDcan be
implemented as an optimization constraint and as a dual-loss. Li et al. (2023b) show that contrastive losses
are prone to feature suppression. They introduce predictive contrastive learning (PCL), which combines
contrastive learning with a decoder to reconstruct the input data from the latent representations to prevent
shortcut learning.
Adnan et al. (2022) measure the MIbetween the latent representation and the input as a domain agnostic
metric to find where (and when) in training a network relies on shortcuts in the input data. Their main
finding is that, in the presence of shortcuts, the MIbetween the input data and the latent representation of
the data is lower than without shortcuts in the input data. Hence, the latent representation captures less
information of the input data in the presence of shortcuts and mainly relies on shortcuts to predict the target.
Our Focus. In this work, we focus on the problem of shortcut learning for VLin the context of multi-view
VLrepresentation learning with multiple captions per image. In contrast with previous (uni-modal) work
on multi-view learning, we consider different captions matching to the same image as different views. We
examine the problem by introducing a framework of synthetic shortcuts designed for VLrepresentation
learning, which allows us to investigate the problem in a controlled way. For our experiments, we select two
prevalent VLmodels that are solely optimized with the InfoNCE loss: CLIP, a large-scale pre-trained model,
and VSE++, a model trained from scratch. We select models that are solely optimized with a contrastive
loss, to prevent measuring the effect of other optimization objectives on the shortcut learning problem.
8 Conclusion
In this work, we focus on the shortcut learning problem of contrastive learning in the context of vision-
language ( VL) representation learning with multiple captions per image. We have proposed synthetic shortcuts
for vision-language ( SVL): a training and evaluation framework to examine the problem of shortcut learning
in a controlled way. The key component of this framework is synthetic shortcuts that we add to image-text
data. Synthetic shortcuts represent additional, easily identifiable information that is shared between images
and captions. We fine-tune CLIP and train a VSE++ model from scratch using our training framework to
evaluate how prone contrastive VLmodels are to shortcut learning. Next, we have evaluated how shortcut
learning can be partially mitigated using latent target decoding and implicit feature modification.
Main Findings. We have conducted experiments on two distinct VLmodels, CLIP and VSE++, and
have evaluated the performance on Flickr30k andMS-COCO . We have found that when training with
unique shortcuts, CLIP suppresses pre-trained features in favor of the shortcuts. VSE++ only learns to
represent the shortcuts, when using unique shortcuts, showing that none of the remaining task-relevant (both
shared and unique) information is captured by the encoders when training a model from scratch. When
usingnbits of shortcuts , we have shown that the more bits we use, the more the contrastive VLmodels
rely on the synthetic shortcuts. Our results demonstrate that contrastive VLmethods tend to depend on
easy-to-learn discriminatory features shared among images and all matching captions while suppressing the
16remaining task-relevant information. Next, we have evaluated two methods for reducing shortcut learning on
our framework of synthetic shortcuts for image-caption datasets. Both methods partially mitigate shortcut
learning when training and evaluating with our shortcut learning framework. These findings show that
our framework is a challenging framework to study and evaluate shortcut learning for contrastive VLand
underline the complexity of our framework in studying and evaluating shortcut learning within the context of
contrastive VL representation learning.
Implications. The implications of our findings are twofold. First, we examine the limitations of contrastive
optimization objectives for VLrepresentation learning, demonstrating that they predominantly capture
features that are easily discriminable but may not necessarily constitute task-optimal representations. Second,
our work contributes a novel framework for investigating shortcut learning problem in the context of VL
representation learning with multiple captions per image, providing insights into the extent to which models
rely on shortcuts when they are available and how existing shortcut reduction methods are capable of reducing
shortcut learning when training with our framework.
Limitations. Some of the limitations of our work are related to the fact that we focused on two specific
models, one optimization objective (InfoNCE), and two datasets, and the generalizability of our findings
to other VLmodels, optimization objectives, and datasets warrants further exploration. Additionally, the
synthetic shortcuts introduced in this work are not dependent on image-caption pairs. Our training and
evaluation setup shows that, in the presence of shortcuts in the training data, contrastive VLmodels mainly
rely on the easy-to-detect shortcut features, which indicates that the InfoNCE loss cannot learn tasks-optimal
representations for VLtasks when multiple captions are used for training. However, it remains unclear to
what degree the unique information of the captions is captured by the contrastive loss VL models.
Future Work. We suggest working on the development of optimization objectives that specifically address
the shortcut learning problem for VLtraining with multiple captions per image. We also suggest extending
our synthetic shortcuts for image-caption datasets to a framework with unique shortcut information per
caption. By having unique shortcut information per caption, it becomes possible to measure how much of
the shared/caption-specific shortcut information is captured by encoder models. Another future direction
includes investigating alternative training strategies or loss functions to further mitigate shortcut learning
problems. Another promising direction for future work includes the improvement of existing methods or the
exploration of novel techniques that address the limitations of existing shortcut reduction methods, potentially
through the combination of multiple approaches. Extending the SVL framework to better capture nuances
and complexities of natural data is another important direction that would facilitate a more comprehensive
understanding of the implications of shortcut learning in real-world scenarios and datasets.
9 Broader Impact
This paper motivates and introduces a framework for investigating the problem of shortcut learning for
contrastive VLrepresentation learning with multiple captions per image in a controlled way. It also examines
how two shortcut learning reduction methods perform on the proposed framework. Overall, the framework
provides a tool for analyzing and understanding the problem of shortcut learning in the context of contrastive
VL representation learning; it can be used in various settings that require deeper insight into the quality of
learned VL representations.
We should be aware that the reliance on shortcuts in VLMs poses ethical concerns with potential real-world
implications. Models that learn shortcuts may overlook nuanced details in images and text, leading to biased
or inaccurate outcomes. Furthermore, the transparency and explainability of VLMs are crucial considerations.
Models that rely on shortcuts may make decisions based on features that are not easily interpretable or
explainable to users. This lack of transparency can diminish trust in AI systems.
Acknowledgements
We thank Marco Federici and Mathijs Henquet for the discussions on mutual information and feedback on
the draft. Additionally, we thank Shashank Gupta and Panagiotis Efstratiadis for helpful feedback.
17This research was supported by the Nationale Politie, Ahold Delhaize, project IDEAS with project
number VI.Vidi.223.166 of the NWO Talent Programme, which is (partly) financed by the Dutch Re-
search Council (NWO), the Hybrid Intelligence Center, a 10-year program funded by the Dutch Min-
istry of Education, Culture and Science through the Netherlands Organisation for Scientific Research,
https://hybrid-intelligence-centre.nl , project LESSEN with project number NWA.1389.20.183 of the
research program NWA ORC 2020/21, which is (partly) financed by the Dutch Research Council (NWO),
project ROBUST with project number KICH3.LTP.20.006, which is (partly) financed by the Dutch Research
Council (NWO), DPG Media, RTL, and the Dutch Ministry of Economic Affairs and Climate Policy (EZK)
under the program LTP KIC 2020-2023, and the FINDHR (Fairness and Intersectional Non-Discrimination in
Human Recommendation) project that received funding from the European Union’s Horizon Europe research
and innovation program under grant agreement No 101070212. All content represents the opinion of the
authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
References
Mohammed Adnan, Yani Ioannou, Chuan-Yung Tsai, Angus Galloway, H.R. Tizhoosh, and Graham W.
Taylor. Monitoring shortcut learning using mutual information. arXiv preprint arXiv:2206.13034 , 2022.
Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in text for visual
question answering. In EMNLP, pp. 2131–2140, 2019.
Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual
information across views. In NeurIPS , pp. 15509–15519, 2019.
Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey
and taxonomy. IEEE Trans. Pattern Anal. Mach. Intell. , 41:423–443, 2019.
Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som,
Songhao Piao, and Furu Wei. VLMo: Unified vision-language pre-training with mixture-of-modality-experts.
NeurIPS , pp. 32897–32912, 2022.
Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for
self-supervised learning. In ICLR, 2022.
Ali Furkan Biten, Andrés Mafla, Lluís Gómez, and Dimosthenis Karatzas. Is an image worth five sentences?
A new look into semantics for image-text matching. In WACV, pp. 2483–2492. IEEE, 2022.
Maurits Bleeker, Andrew Yates, and Maarten de Rijke. Reducing predictive feature suppression in resource-
constrained contrastive image-caption retrieval. Transactions on Machine Learning Research , 2023.
Ting Chen, Simon Kornblith, MohammadNorouzi, and Geoffrey E. Hinton. Asimple framework for contrastive
learning of visual representations. In ICML, pp. 1597–1607, 2020a.
Ting Chen, Calvin Luo, and Lala Li. Intriguing properties of contrastive losses. In NeurIPS, pp. 11834–11845,
2021.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C. Lawrence
Zitnick. Microsoft COCO captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 ,
2015.
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing
Liu. UNITER: Universal image-text representation learning. In ECCV, pp. 104–120, 2020b.
Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical
machine translation. In ACL, pp. 1724–1734, 2014.
JacobDevlin, Ming-WeiChang, KentonLee, andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. In NAACL-HLT , 2019.
18AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.
Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja Fidler. VSE++: Improving visual-semantic
embeddings with hard negatives. In BVCM, pp. 12, 2018.
Marco Federici, Anjan Dutta, Patrick Forré, Nate Kushman, and Zeynep Akata. Learning robust representa-
tions via multi-view information bottleneck. In ICLR, 2020.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas
Mikolov. DeViSE: A deep visual-semantic embedding model. NeurIPS , pp. 2121–2129, 2013.
Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge,
and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence , pp.
665–673, 2020.
Wenzhong Guo, Jianwen Wang, and Shiping Wang. Deep multimodal representation learning: A survey.
IEEE Access , 7:63373–63394, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, pp. 770–778, 2016.
Katherine L. Hermann and Andrew K. Lampinen. What shapes feature representations? Exploring datasets,
architectures, and training. In NeurIPS , pp. 9995–10006, 2020.
R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler,
and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization.
InICLR, 2019.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,
Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text
supervision. In ICML, pp. 4904–4916, 2021.
Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descriptions. In
CVPR, pp. 3128–3137, 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
ICLR, 2017.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with
multimodal neural language models. arXiv preprint arXiv:1411.2539 , 2014.
Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text
matching. In ECCV, pp. 201–216, 2018.
Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John F. Canny, and Ian Fischer. Compressive visual
representations. In NeurIPS , pp. 19538–19552, 2021.
Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-VL: A universal encoder for vision
and language by cross-modal pre-training. In AAAI, 2020a.
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong
Hoi. Align before fuse: Vision and language representation learning with momentum distillation. NeurIPS ,
pp. 9694–9705, 2021.
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: Bootstrapping language-image
pre-training for unified vision-language understanding and generation. In ICML, pp. 12888–12900, 2022a.
19Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image
pre-training with frozen image encoders and large language models. In ICML, pp. 19730–19742, 2023a.
Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. Visual semantic reasoning for image-text
matching. In ICCV, pp. 4654–4662, 2019a.
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A simple and
performant baseline for vision and language. arXiv preprint arXiv:1908.03557 , 2019b.
Tianhong Li, Lijie Fan, Yuan Yuan, Hao He, Yonglong Tian, Rogério Feris, Piotr Indyk, and Dina Katabi.
Addressing feature suppression in unsupervised visual representations. In WACV, pp. 1411–1420, 2023b.
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,
Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV,
pp. 121–137, 2020b.
Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie
Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. In
ICLR, 2022b.
Paul Pu Liang, Zihao Deng, Martin Q. Ma, James Zou, Louis-Philippe Morency, and Russ Salakhutdinov.
Factorized contrastive learning: Going beyond multi-view redundancy. In NeurIPS , 2023.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, pp. 740–755, 2014.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic visiolinguistic
representations for vision-and-language tasks. In NeurIPS , pp. 13–23, 2019.
Norman Mu, Alexander Kirillov, David A. Wagner, and Saining Xie. SLIP: Self-supervision meets language-
image pre-training. In ECCV, pp. 529–544, 2022.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , pp. 9, 2019.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable
visual models from natural language supervision. In ICML, pp. 8748–8763, 2021.
Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese BERT-Networks. In
EMNLP-IJCNLP , pp. 3980–3990, 2019.
Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive
learning avoid shortcut solutions? In NeurIPS , pp. 4974–4986, 2021.
Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, Michael Poli, and Sangdoo Yun. Which shortcut cues will
DNNs choose? A study from the parameter-space perspective. In ICLR, 2022.
Ravid Shwartz-Ziv and Yann LeCun. To compress or not to compress–self-supervised learning and information
theory: A review. arXiv preprint arXiv:2304.09355 , 2023.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
InICLR, 2015.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MPNet: Masked and permuted pre-training
for language understanding. In NeurIPS , pp. 16857–16867, 2020.
Karthik Sridharan and Sham M. Kakade. An information theoretic framework for multi-view learning. In
COLT, pp. 403–414, 2008.
20Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-training of
generic visual-linguistic representations. In ICLR, 2020.
Hao Tan and Mohit Bansal. LXMERT: learning cross-modality encoder representations from transformers.
InEMNLP-IJCNLP , pp. 5099–5110, 2019.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, pp. 776–794,
2020a.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for
good views for contrastive learning? In NeurIPS , pp. 6827–6839, 2020b.
Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-supervised learning
from a multi-view perspective. In ICLR, 2021.
Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. In ICLR, 2020.
Michael Tschannen, Manoj Kumar, Andreas Peter Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer.
Image captioners are scalable vision learners too. In NeurIPS , 2023.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.
arXiv preprint arXiv:1807.03748 , 2018.
Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking minimal sufficient representation in
contrastive learning. In CVPR, pp. 16020–16029, 2022.
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan
Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: BEIT pretraining
for vision and vision-language tasks. In CVPR, pp. 19175–19186, 2023.
Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dvijotham,
and Ali Taylan Cemgil. A fine-grained analysis on distribution shift. In ICLR, 2022.
Tete Xiao, Xiaolong Wang, Alexei A. Efros, and Trevor Darrell. What should not be contrastive in contrastive
learning. In ICLR, 2021.
Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin
Jiang, and Chunjing Xu. FILIP: Fine-grained interactive language-image pre-training. In ICLR, 2022.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations:
New similarity metrics for semantic inference over event descriptions. Transactions of the Association for
Computational Linguistics , 2:67–78, 2014.
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong
Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv
preprint arXiv:2111.11432 , 2021.
Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why
vision-language models behave like bags-of-words, and what to do about it? In ICLR, 2023.
Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with
visual concepts. In ICML, pp. 25994–26009, 2022.
Jing Zhao, Xijiong Xie, Xin Xu, and Shiliang Sun. Multi-view learning overview: Recent progress and new
challenges. Inf. Fusion , 38:43–54, 2017.
Yongshuo Zong, Oisin Mac Aodha, and Timothy Hospedales. Self-supervised multimodal learning: A survey.
arXiv preprint arXiv:2304.01008 , 2023.
21A Notation
Table 3: Notation used in the paper.
Symbol Description
LInfoNCE InfoNCE loss
LInfoNCE+LTD Loss that combines InfoNCE and latent target decoding (LTD)
LInfoNCE+IFM Loss that combines InfoNCE and implicit feature modification (IFM)
Lrecon Reconstruction loss
D DatasetDthat comprises Nimage-caption tuples: D=/braceleftig/parenleftig
xi
I,{xi
Cj}k
j=1/parenrightig/bracerightigN
i=1;i-th image-
caption tuple in the dataset Dconsist out of an image xi
Iandkassociated captions {xi
Cj}k
j=1
B Batch of image-caption pairs
xI Image
xC Caption
zI Latent representation of image xI
zC Latent representation of caption xC
zSUF
C→I Latent representation of caption xCthat is sufficient for image xI
zSUF
I→C Latent representation of image xIsufficient for caption xC
zMIN
I→C Latent representation of image xIthat is minimal sufficient for caption xC
zOPT
I→K Latent representation of image xIthat is optimal for set of captions Kgiven taskT
R Task-relevant information
¬R Task-irrelevant information
C Task-relevant information specific for a caption xC
SSynSC Synthetic shortcut
S Original shared information
S+Shared information that includes synthetic shortcut
R+Task-relevant information that contains synthetic shortcut
fθ(·) Image encoder parametrised by θ; takes image xIas input and returns its latent representation
zI:zI:=fθ(xI)
gϕ(·) Caption encoder parametrised by ϕ; takes caption xCas input and returns its latent represen-
tation zC:zC:=gϕ(zC)
τ Temperature paramater of LInfoNCE
ϵ Perturbation budget for LIFM
η Reconstruction bound for LLTD
B Problem Definition and Assumptions
In this work, we solely focus on contrastive VLrepresentation learning. We work in a setting where we
investigate the problem by fine-tuning a large pre-trained foundation model (CLIP, Radford et al., 2021) and
training a resource-constrained image-text method from scratch (VSE++, Faghri et al., 2018). We train and
22evaluate using two benchmark datasets where multiple captions per image are available: Flickr30k (Young
et al., 2014) and MS-COCO Captions (Lin et al., 2014). Both datasets come with 5 captions per image.
We work in a dual-encoder setup, i.e., we have a separate image and caption encoder, which do not share
parameters.
B.1 Evaluation Task
The image-caption retrieval ( ICR) evaluation task, consists of two sub-tasks: image-to-text ( i2t) and text-to-
image (t2i) retrieval. In ICR, either an image or a caption is used as a query and the goal is to rank a set of
candidates in the other modality. In this work, we follow the standard ICRevaluation procedure (see, e.g.,
Faghri et al., 2018; Lee et al., 2018; Li et al., 2019a). The evaluation metric for the ICRtask is Recall @k,
withk={1,5,10}. Fort2iretrieval, there is one matching/positive image per query caption (when using
theFlickr30k orMS-COCO or dataset). Hence, the Recall @kmetric represents how often the correct image
is present in the top- kof the ranking. For i2tretrieval, however, there are 5 matching captions per image.
Therefore, only the highest-ranked correct caption is taken into account when measuring the Recall @k(i.e.,
in the highest-ranked caption present in the top k). Standard practice to select the best model checkpoint
during training is to use the recall sum (rsum) as a validation metric. The recall sum is the sum of recall at
1, 5, and 10, for both i2t and t2i. Therefore, the maximum value of the recall sum is 600.
B.2 Assumptions
Throughout this work, we rely on several assumptions about the problem definition. Our assumptions are
defined at the level of an image-text tuple. Following Section 2, we formalize the assumptions on the case
where one image is associated with two captions:/parenleftbig
xI,{xCA,xCB}/parenrightbig
.
Assumption 1. Each caption in the tuple contain information that is distinct from the other captions in the
tuple and all captions and image in the tuple contain shared and unique information:
I(xI;xCA;xCB)>0
I(xI;xCA|xCB)>0, I(xI;xCB|xCA)>0andI(xCA;xCB|xI)>0
H(xI|xCA,xCB)>0, H(xCA|xI,xCB)>0andH(xCB|xI,xCA)>0.
Assumption 2. Task-relevant information Ris the combination of all the information shared between an
image and each caption in the tuple:
R=I(xI;xCA|xCB) +I(xI;xCB|xCA) +I(xI;xCA;xCB).
C Analysis of Contrastive Learning for Multiple Captions per Image
Theorem 1 (Suboptimality of contrastive learning with multiple captions per image) .Given an image xI, a
set of matching captions C={xCA,xCB}, and a contrastive learning loss function LInfoNCEthat optimizes for
taskT, image representations learned during contrastive learning will be minimal sufficient and will never be
task-optimal image representations. More formally, assume that:
(H1)∀i,j∈{A,B}such thati̸=j, I(zMIN
I→Ci;xCi) =I(xI;xCi|xCj) +I(xI;xCi;xCj).
(H2)∃i,j∈{A,B}withi̸=jsuch thatI(xI;xCi|xCj)>0.
Then the following holds:
(T2)∃i∈{A,B}such thatI(zOPT
I→C;xCAxCB)>I(zMIN
I→Ci;xCi).
Proof.Following Eq. 1 we define a task-optimal representation of an image xIw.r.t. all matching captions in
Cas:
I(zOPT
I→C;xCAxCB) =I(xI;xCA|xCB)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
CA+I(xI;xCB|xCA)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
CB+I(xI;xCA;xCB)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
S.
23Furthermore, following Definition 2.3, we define minimal sufficient representations of image xIw.r.t. each
matching caption in Cas a combination of caption-specific and shared information:
I(zMIN
I→CA;xCA) =I(xI;xCA|xCB)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
CA+I(xI;xCA;xCB)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
S
I(zMIN
I→CB;xCB) =I(xI;xCB|xCA)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
CB+I(xI;xCA;xCB)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
S.
Following assumption H2, for at least one caption xC∈Cassociated with the image xI, caption-specific
information is positive. Therefore, we consider two cases:
•If caption-specific information of xCAis positive, that is, if I(xI;xCA|xCB)>0:
I(xI;xCA|xCB) +I(xI;xCB|xCA) +I(xI;xCA;xCB)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(zOPT
I→C ;xCAxCB)>I(xI;xCB|xCA) +I(xI;xCA;xCB)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
I(zMIN
I→CB;xCB)⇒
⇒I(zOPT
I→C;xCAxCB)>I(zMIN
I→CB;xCB).
•Similarly, if caption-specific information of xCBis positive, that is, if I(xI;xCB|xCA)>0:
I(xI;xCA|xCB) +I(xI;xCB|xCA) +I(xI;xCA;xCB)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(zOPT
I→C ;xCAxCB)>I(xI;xCA|xCB) +I(xI;xCA;xCB)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
I(zMIN
I→CA;xCA)⇒
⇒I(zOPT
I→C;xCAxCB)>I(zMIN
I→CA;xCA).
Therefore, we show that in a setup where a single image is associated with multiple captions, and given
at least one caption contains caption-specific information, image representations learned contrastively
w.r.t. associated captions would contain less information than task-optimal image representation: ∃i∈
{A,B}such thatI(zOPT
I→C;xCAxCB)>I(zMIN
I→Ci;xCi).
D Experimental Setup
D.1 Datasets
Flickr30k consists of 31,000 images annotated with 5 matching captions (Young et al., 2014).
MS-COCO consists of 123,287 images, each image annotated with 5 matching captions (Lin et al., 2014).
The original dataset was introduced for large-scale object recognition.
For both datasets, we use the training, validation, and test splits from (Karpathy & Li, 2015).
D.2 Models
We use CLIP and VSE++. Both consist of an image and a text encoder that do not share parameters.
CLIPis a large-scale image-text foundation model (Radford et al., 2021). The model is pre-trained on
a collection of 400 million image-text pairs collected from the Web. The encoders are pre-trained using a
contrastive loss (InfoNCE) on image-text pairs. The text encoder of consists of a 12-layer transformer model,
described in (Radford et al., 2019). As for the image encoder, CLIP utilizes various model backbones, such as
ResNet (He et al., 2016) and Vision Transformer (Dosovitskiy et al., 2021). In this work, we use the ResNet-50
(‘RN50’) variant of the CLIP image encoder.2The CLIP encoders are trained to jointly understand images
2https://github.com/openai/CLIP/
24and text. Therefore, the learned representations generalize to a wide range of different zero-shot (visual)
evaluation tasks, such as classification, without task-specific fine-tuning, by using textual prompts.
VSE++ is an image-caption encoder trained from scratch (Faghri et al., 2018). The model features a
triplet loss function with a margin parameter α= 0.2. The text encoder is a one-layer gated recurrent unit
(GRU) (Cho et al., 2014). The available image encoder configurations are ResNet-152 (He et al., 2016) and
VGG19 (Simonyan & Zisserman, 2015). In this work, we use ResNet-152.
D.3 Training
CLIP.To fine-tune CLIP, we follow (Yuksekgonul et al., 2023). All models are fine-tuned for 5 epochs.
We employ a cosine-annealing learning rate schedule, with a base learning rate of 2e−5, and 100 steps of
warm-up. As an optimizer, we use AdamW (Loshchilov & Hutter, 2019) with a gradient clipping value of
2. For the InfoNCE loss, we use the logit-scale (i.e., temperature τ) from the pre-trained CLIP model and
fine-tune the logit-scale end-to-end along with the rest of the model parameters.
VSE++. The model is trained for 30 epochs using a linear learning rate schedule with a base learning rate
of2e−4. We use the Adam optimizer (Kingma & Ba, 2015) with a gradient clipping value of 2. Instead of
the triplet loss, we use the InfoNCE loss similar to Radford et al. (2021),
For both models, instead of selecting the best-performing model based on the validation set scores, we use
the final checkpoint at the end of training.
D.4 Shortcut Sampling
Our goal is to add the shortcuts in a manner that preserves the original information of the images and
captions. For the captions, we append the shortcut at the end of the captions. In order to prevent a tokenizer
from tokenizing the shortcut into a single token, we insert spaces between each number of the shortcut. For
the images, we place the numbers of the shortcuts at the top of the images, evenly spaced across the entire
width of the images (to make sure the shortcut is evenly spaced across the feature map of the image). We
always use 6 digits to represent a shortcut. If a shortcut number contains fewer than 6 digits, we fill the
remaining positions with zeros for padding. For the MNIST images, we always sample a random image from
the set of images representing the number that belongs to (also during evaluation), to prevent overfitting on
specific MNIST images. In Figure 6, we provide four examples of image-caption pairs with randomly added
shortcuts. The examples in Figure 6 show (i) how synthetic shortcuts are added to the image and the caption,
and (ii) that the shortcuts preserve the original (task-relevant) information of the images and captions.
E Optimization Objectives
E.1 InfoNCE
In this work, we use InfoNCE loss, LInfoNCE(van den Oord et al., 2018). Given a dual-encoder setup, we
optimize a model in two directions: image-to-text ( i2t) and text-to-image ( t2i). The loss is defined as follows:
Li2t
InfoNCE =1
|B|/summationdisplay
i∈Blogexp(zi
Izi
C/τ)
exp(zi
Izi
C/τ) +/summationtext
j̸=iexp(zi
Izj
C/τ),
Li2t
InfoNCE =1
|B|/summationdisplay
i∈Blogexp(zi
Izi
C/τ)
exp(zi
Izi
C/τ) +/summationtext
j̸=iexp(zj
Izi
C/τ),
LInfoNCE =1
2Li2t
InfoNCE +1
2Lt2i
InfoNCE.
25(a)Caption : “A bathroom sink with wood finish cabi-
nets. 0 3 9 9 6 5.”
(b)Caption : “A guy in a brown shirt has just hit a
tennis ball. 0 7 7 1 1 4.”
(c)Caption : “A man in shorts is lying on the beach. 0
0 6 9 9 3.”
(d)Caption : “A player up to bat in a baseball game. 1
0 1 9 9 2.”
Figure 6: Four random samples from the MS-COCO dataset including shortcuts added on both the image
and caption.
E.2 Latent Target Decoding
Latent target decoding ( LTD) (Bleeker et al., 2023) is an optimization objective that reduces predictive
feature suppression for resource-constrained VLmethods. LTDconsists ofLInfoNCEand a reconstruction loss
Lrecon, which reconstructs the input caption from the latent representation zC.
26In (Bleeker et al., 2023), LTD is implemented in two ways. Firstly, as a dual optimization objective:
LInfoNCE+LTD =LInfoNCE +βLrecon.
Secondly, as an optimization constraint in combination with gradient descent by using the method of Lagrange
multipliers:
max
λminLInfoNCE+LTD =LInfoNCE +λ/parenleftbiggLrecon
η−1/parenrightbigg
.
This optimization objective is minimized w.r.t. model parameter, while also being maximized w.r.t. λ. The
value ofλis automatically tuned by gradient ascent, such that the reconstruction bound ηis met. In this
work, we use both LTDas a dual optimization objective and an optimization constraint. We select the loss
with the highest evaluation scores on the validation set for evaluation.
E.3 Implicit Feature Modification
Implicitfeaturemodification( IFM)(Robinsonetal.,2021)isacontrastiveloss, withanadditionalperturbation
budgetϵ.IFMperturbs the logits value of the similarity scores between the images and captions, such that
the model avoids using shortcut solutions for a correct similarity score. IFMsubtractsϵ/τfrom the positive
logit values and adds ϵ/τto the negative logits values.
Lt2i
IFM=1
|B|/summationdisplay
i∈Blogexp(( zi
Izi
C−ϵ)/τ)
exp(( zi
Izi
C)−ϵ)/τ) +/summationtext
j̸=iexp(( zi
Izj
C+ϵ)/τ),
Li2t
IFM=1
|B|/summationdisplay
i∈Blogexp(( zi
Izi
C−ϵ)/τ)
exp(( zi
Izi
C)−ϵ)/τ) +/summationtext
j̸=iexp(( zj
Izi
C+ϵ)/τ),
LIFM=1
2Lt2i
IFM+1
2Li2t
IFM,
LInfoNCE+IFM =1
2LIFM+1
2LInfoNCE.
Similar to Robinson et al. (2021), we combine IFM and the InfoNCE in a dual optimization objective.
27