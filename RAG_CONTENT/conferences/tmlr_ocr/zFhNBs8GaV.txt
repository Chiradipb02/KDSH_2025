Published in Transactions on Machine Learning Research (12/2022)
Calibrated Selective Classification
Adam Fisch fisch@csail.mit.edu
Tommi Jaakkola tommi@csail.mit.edu
Regina Barzilay regina@csail.mit.edu
Computer Science and Artificial Intelligence Laboratory (CSAIL)
Massachusetts Institute of Technology, Cambridge, MA, 02142, USA.
Reviewed on OpenReview: https: // openreview. net/ forum? id= zFhNBs8GaV
Abstract
Selective classification allows models to abstain from making predictions (e.g., say “I don’t
know”) when in doubt in order to obtain better effective accuracy. While typical selective
models can succeed at producing more accurate predictions on average, they may still allow
for wrong predictions that have high confidence, or skip correct predictions that have low con-
fidence. Providing calibrated uncertainty estimates alongside predictions—probabilities that
correspond to true frequencies—can be as important as having predictions that are simply ac-
curateonaverage. Uncertaintyestimates, however, cansometimesbeunreliable. Inthispaper,
we develop a new approach to selective classification in which we propose a method for reject-
ing examples with “uncertain” uncertainties. By doing so, we aim to make predictions with
well-calibrated uncertainty estimates over the distribution of accepted examples, a property
we call selective calibration. We present a framework for learning selectively calibrated models,
whereaseparateselectornetworkistrainedtoimprovetheselectivecalibrationerrorofagiven
basemodel. Inparticular, ourworkfocusesonachievingrobustcalibration, wherethemodelis
intentionally designed to be tested on out-of-domain data. We achieve this through a training
strategy inspired by distributionally robust optimization, in which we apply simulated input
perturbations to the known, in-domain training data. We demonstrate the empirical effective-
ness of our approach on multiple image classification and lung cancer risk assessment tasks.1
1 Introduction
Even the best machine learning models can make errors during deployment. This is especially true when
there are shifts in training and testing distributions, which we can expect to happen in nearly any practical
scenario (Quionero-Candela et al., 2009; Rabanser et al., 2019; Koh et al., 2021). Selective classification is an
approach to mitigating the negative consequences of potentially wrong predictions, by allowing models to ab-
stainonuncertaininputsinordertoachievebetteraccuracy(Chow,1957;El-Yaniv&Wiener,2010;Geifman&
El-Yaniv, 2017). While such systems may indeed be more accurate on average, they still fail to answer a critical
question: how reliable are the uncertainties that they use to base their predictions? Often it can be important
to precisely quantify the uncertainty in each prediction via calibrated confidence metrics (that reflect the true
probabilityofaneventofinterest, suchasourmodelbeingcorrect). Thisisparticularlyrelevantinsituationsin
which a user cannot easily reason about themselves in order to verify predictions; in which there is inherent ran-
domness, and accurate predictions may be impossible; or in which high-stakes decisions are made, that require
weighing risks of alternative possible outcomes (Amodei et al., 2016; Jiang et al., 2018; Ovadia et al., 2019).
Consider the task of lung cancer risk assessment, where a model is asked if a patient will develop cancer within
the next few years (Aberle et al., 2011). A standard selective classifier that aims to maximize accuracy may
1Our code is available at https://github.com/ajfisch/calibrated-selective-classification .
1Published in Transactions on Machine Learning Research (12/2022)
Figure 1: A demonstration of calibrated selective classification for lung cancer risk assessment. The input
Xis a CT scan; the output Yindicates if the patient develops cancer within 6 years of the scan. f(yellow)
provides a confidence estimate for the patient being at risk, while g(red) acts as a gatekeeper to decide when
to trustf’s outputs. Our goal is for the accepted predictions to be calibrated. For example, of all patients with
predicted (and non-rejected) risks of 0.3, 30% should be expected to develop cancer within 6 years of the scan.
only make predictions on examples with clear outcomes; predictions that are less useful to the radiologists who
these models are meant to help (Dromain et al., 2013; Choy et al., 2018). Furthermore, even if the selective
classifier is accurate, it may fail to reliably reflect the risk variations of different patients. For example,
suppose that a patient has a 30% chance of developing cancer in reality. The prediction that the patient will
notdevelop cancer will most likely be correct. However, knowing the degree of uncertainty in this outcome—
namely that the patient’s true risk of developing cancer is 30%—might dramatically change the patient’s care
plan. For these reasons, having a calibrated confidence estimate (in this case, that reflects the true likelihood
of developing cancer) can be key to guiding clinical decision making (Jiang et al., 2012; Tsoukalas et al., 2015).
We develop a new approach to selective classification in which we aim to only make predictions with
well-calibrated uncertainties. In our lung cancer setting, this means that if the model predicts that cancer
will develop with confidence αfor a group of patients, then a fraction αof them should indeed develop
cancer. Under our framework, a model should abstain if it is uncertain about its confidence, rather than risk
misleading a user.2In other words, we shift the focus from abstaining on uncertain examples, to abstaining
on examples with “uncertain” uncertainties. Figure 1 illustrates our approach.
Concretely, consider a binary classification task (we consider multi-class tasks later) with input space X
and label spaceY={0,1}, where inputs X∈Xand labelsY∈Yare random variables drawn from a joint
distribution P. Suppose we have a model f:X → [0,1], wheref(X)represents the model’s confidence
thatY= 1, while 1−f(X)represents the confidence that Y= 0. Following the selective classification
setting of Geifman & El-Yaniv (2017), we assume that fisfixed(e.g., it may be a black-box or expensive
to re-train)—and we are now simply seeking a safe gate-keeping mechanism for its predictions. Typical
calibration stipulates that ∀αin the range of fwe have E[Y|f(X) =α] =α, but this may not be satisfied
by the provided f. Instead, here we let g:X→{ 0,1}be our selection model, where we make a prediction if
g(X) = 1, and abstain otherwise. Our goal is to then make predictions that are conditionally calibrated, i.e.,
E/bracketleftbig
Y|f(X) =α, g(X) = 1/bracketrightbig
=α,∀α∈[0,1]in the range of frestricted to{x:g(x) = 1}.(1)
We refer to this property as selective calibration. It is also important to preserve some minimum amount of
prediction coverage for fto still be useful (e.g., we may want to make predictions on at least 80% of inputs). As
a result, we seek to become as relatively calibrated as possible by minimizing the selective calibration error of
(f,g), i.e., deviation from Eq. (1), subject to a user-specified coverage constraint on g. Inspired by the trainable
Maximum Mean Calibration Error (MMCE) objective of Kumar et al. (2018), we propose the Selective
Maximum Mean Calibration Error (S-MMCE)—along with an efficient method for training gto minimize a
(easily estimated) upper bound of it that we compute in practice, in order to improve the selective calibration
error. This loss can be effectively trained over a relatively small subset of held-out examples (e.g., ≈O(103)).
2Alternatively, the model may make a prediction , but should abstain from providing (or raise a red flag about) its confidence.
2Published in Transactions on Machine Learning Research (12/2022)
Still, the key empirical question that remains, is on what subset of held-out examples can we train g? We typ-
ically do not know the test distribution, P=Ptest, that we may encounter, and our goal is for gto generalize
beyond simple in-domain examples (for which standard, non-selective, calibration is more readily obtained). To
address this, we formulate a robust calibration objective using synthetically constructed dataset shifts. Specifi-
cally, given any original training data Dtrain∼Ptrainthat wedohave, and some family Tof perturbations that
we can define (e.g., via common data augmentation techniques), we minimize the S-MMCE over the uncali-
brateddatasets that we create by applying perturbations in Tto the data inDtrain. Of course, we do not expect
these augmentations to represent all possible test distributions Ptest. Rather, by constructing a broad, albeit
still limited, set of example shifts, our intention is for gto learn to generalize well to new perturbation types, be-
yondthoseseenduringtraining. (naturally, g’sgeneralizationabilitieswilldependonhowfeasibleitistocreate
a meaningful perturbation family T). Empirically, when training and testing on disjointsets of both simulated
andnaturallyoccurringperturbationsondiversedatasets, wedemonstrateconsistentempiricalreductionsinse-
lective calibration error metrics relative to typical confidence-based baselines across multiple tasks and datasets.
Contributions. To summarize, the main contributions of this work are as follows:
1. We introduce the concept of selective calibration as a selective classification objective;
2. We propose a coverage-constrained loss for training selectors to identify well-calibrated predictions;
3. We provide a robust framework for training selectors that generalize to out-of-distribution data;
4.We demonstrate consistent empirical reductions in selective calibration error metrics (e.g., 18-25% reduction
inℓ2calibration error AUC) relative to standard heuristic baselines across multiple tasks and datasets.
2 Related work
Selective classification. Selective classification, or classification with a reject option, attempts to abstain on
examples that the model is likely to get wrong (El-Yaniv & Wiener, 2010; Geifman & El-Yaniv, 2017). The lit-
erature on selective classification is extensive (Hellman, 1970; De Stefano et al., 2000; Herbei & Wegkamp, 2006;
Cortesetal.,2016a;b;Nietal.,2019;Geifman&El-Yaniv,2019). Astraightforwardandpopulartechniqueisto
usesomeconfidencemeasuretoselectthemostcertainexamples(Cordellaetal.,1995;El-Yaniv&Wiener,2010;
Geifman & El-Yaniv, 2017). If the underlying confidence measure is unreliable, however, this approach can per-
form poorly (Jones et al., 2021). As in our setting, Kamath et al. (2020) consider selective classification in the
context of domain shift, though they focus on accuracy, rather than calibration. Approaches to selective classi-
fication that build beyond simple accuracy include cost-sensitive selective classification (Bartlett & Wegkamp,
2008; Charoenphakdee et al., 2021), selective classification optimized for experts in-the-loop (Mozannar & Son-
tag, 2020), and selective classification with fairness constraints (Shah et al., 2021); all of which represent com-
plementary directions to our work. Lin et al. (2022) also considers the possibility of a non-uniform significance
over different classes, and provide a set-based selective classifier with controlled class-specific miscoverage rates.
Here, we extend selective classification to focus on improving model calibration over non-rejected instances.
Model calibration. Calibration has a rich history in machine learning (Brier, 1950; Murphy & Epstein, 1967;
Dawid, 1982; Foster & Vohra, 1998; Gneiting et al., 2007). Recently, it has begun to experience a resurgence in
the deep learning literature (Kuleshov & Liang, 2015; Kuleshov et al., 2018; Kumar et al., 2019; van Amersfoort
et al., 2020; Gupta et al., 2020), partly motivated by observations that modern neural networks can be
significantly miscalibrated out-of-the-box (Guo et al., 2017; Ashukha et al., 2020). To start, a number of efforts
have focused on how to best define and measure calibration, especially in multi-class settings. A common
approach (e.g., as taken by Guo et al. (2017)) measures the top-label calibration error, or the probability that
the model’s top prediction is correct (a definition we adopt). Other methods propose more precise notions of
calibration, including calibration that is conditioned on the predicted class (Gupta & Ramdas, 2022), across
classes (Kull et al., 2019), for sub-populations or individuals (Hebert-Johnson et al., 2018; Zhao et al., 2020),
or with respect to decision making (Zhao et al., 2021). Additionally, conformal prediction provides tools for
calibrating set-based predictions such that they provably satisfy fixed risk limits (e.g., cover the correct label)
with some specified probability (Vovk et al., 2005; Bates et al., 2020; Angelopoulos et al., 2022). In the case
of providing calibrated probability estimates, conformal predictors can also be modified to output (sets of)
probability predictions with the same coverage properties (Vovk & Petej, 2014; Vovk et al., 2018). We note
that aspects of these methods are complementary to our work, and could be incorporated in a selective setting
3Published in Transactions on Machine Learning Research (12/2022)
following, or similar to, our framework. Finally, approaches to achieving better calibration over single point
estimates include both parameteric and non-parametric post-processing techniques (Platt, 1999; Zadrozny
& Elkan, 2001; Niculescu-Mizil & Caruana, 2005; Guo et al., 2017; Kumar et al., 2019), as well as modified
losses introduced during model training (Kumar et al., 2018; Mukhoti et al., 2020; Karandikar et al., 2021).
Here, we attempt to achieve better conditional calibration across domains through selective classification.
Robustness to domain shift. It is well-known that model performance can often degrade when the
testing and training distributions are different—and many methods have been proposed to help combat this
poor behavior (Sugiyama & Müller, 2005; Wen et al., 2014; Reddi et al., 2015; Lipton et al., 2018; Tran et al.,
2022). With respect to model calibration, even models that are well-calibrated in-domain can still suffer
from considerable miscalibration under domain shift (Ovadia et al., 2019; Minderer et al., 2021). Given some
knowledge or assumptions about the possible new target distribution (e.g., such as an estimated likelihood
ratios or maximum divergence) several works have considered corrections for calibration under distribution
shift (Tibshirani et al., 2019; Cauchois et al., 2020; Gupta et al., 2020; Park et al., 2022). Similarly, given
samples from two distributions (e.g., the training and testing distributions), many tests exist for detecting if
they are different (Vovk et al., 2003; Gretton et al., 2012; Chwialkowski et al., 2015). A shift in input features,
however, is neither necessary nor sufficient as a predictor of accuracy; methods such as Ginart et al. (2022)
attempt to monitor when distribution drifts are severe enough to indeed cause a performance drop, in order
to trigger a decision to collect more supervised data. Closer to our work, a number of approaches have also
attempted robust training (e.g., for temperature scaling) over multiple environments or domains (Wald et al.,
2021; Yu et al., 2022), rather than relying on test-time adjustments. Our work adds to the broad array of tools
in this area by providing a robust selective classification mechanism with few assumptions or requirements.
Synthetic perturbations. Data augmentation is commonly used to improve model performance and
generalization (Hendrycks et al., 2020; Rusak et al., 2020; Cubuk et al., 2020; Buslaev et al., 2020). Similarly,
automated methods have been developed for identifying challenging sub-populations of a larger training
set to target bias (Liu et al., 2021; Bao & Barzilay, 2022). Meanwhile, we create perturbed datasets by
sampling different augmentation types, in order to uncover relationships between types of perturbations
and model calibration. Our line of work is a consumer of the other: as new approaches are developed to
expand the space of possible perturbations, the more tools our framework has to use for robust training.
3 Background
We briefly review selective classification and calibration. We use upper-case letters ( X) to denote random vari-
ables; lower-case letters ( x) to denote scalars; and script letters ( X) to denote sets, unless otherwise specified.
3.1 Selective classification
Returning to the setting in §1, let f:X→Ybe our prediction model with input space Xand label spaceY,
and letg:X→{ 0,1}be a binary selection function over the input space X. LetPbe a joint distribution
overX×Y. For now, we make no distinction between PtrainandPtest, and simply assume a general
P=Ptrain=Ptest. A selective classification system (f,g)(x)at an input x∈Xcan then be described by
(f,g)(x) :=/braceleftigg
f(x) ifg(x) = 1,
“abstain” otherwise.(2)
Selective classifier performance is commonly evaluated in terms of risk versus coverage (El-Yaniv & Wiener,
2010). Coverage is defined as the probability mass of the region of Xthat is not rejected, E[g(X)]. In practice,
coverageisusuallytunableviaathreshold τ∈R, wheregivensomesoftscoringfunction ˜g:X→R,gisdefined
asg(x) :=1{˜g(x)≥τ}. Given some loss function L, the selective risk with respect to Pis then defined as
R(f,g;P) :=E[L(f(X),Y)|g(X) = 1] =E[L(f(X),Y)g(X)]
E[g(X)]. (3)
Lis typically the 0/1loss, makingR(f,g;P)the selective error. As evident from Eq. (3), there is a strong
dependency between risk and coverage. Rejecting more examples can result in lower selective risk, but also
lowercoverage. Therisk-coveragecurve, R(f,g;P)atdifferent E[g(X)], anditsAUCprovideastandardwayto
evaluate models. We will also use the AUC to evaluate the calibration -coverage curve of our proposed method.
4Published in Transactions on Machine Learning Research (12/2022)
3.2 Model calibration
We consider two simple marginal measures of binary and multi-class calibration error. Starting with the binary
setting, letf:X→ [0,1]be our model, where f(X)is the confidence that Y= 1. The binary calibration error
measures the expected difference between the estimate f(X), and the true probability of Ygiven that estimate.
Definition 3.1 (Binary calibration error) .Theℓqbinary calibration error of fw.r.t.Pis given by
BCE(f;q,P) :=/parenleftbig
E/bracketleftbig
|E[Y|f(X)]−f(X)|q/bracketrightbig/parenrightbig1
q(4)
Typical choices for the parameter q≥1are1,2, and∞(we will use both q= 2andq=∞). We now turn
to the multi-class case, where Y={1,...,K}is a set ofKclasses, and the model f:X→ [0,1]Koutputs
a confidence score for each of the Kclasses. A common measure of calibration is the difference between
the model’s highest confidence in any class, maxy∈[K]f(X)y, and the probability that the top predicted
class, arg maxy∈[K]f(X)y, is correct given that estimate (Guo et al., 2017; Kumar et al., 2019):3
Definition 3.2 (Top-label calibration error) .Theℓqtop-label calibration error of fw.r.t.Pis given by
TCE(f;q,P) :=/parenleftbig
E/bracketleftbig
|E[1{Y= arg max
y∈[K]f(X)y}|max
y∈[K]f(X)y]−max
y∈[K]f(X)y|q/bracketrightbig/parenrightbig1
q(5)
In practice, we measure the empirical calibration error, where BCEand TCEare approximated (see
Appendix B). Having zero calibration error, however, does not imply that fis a useful model. It is still
important for fto be a good predictor of Y. For example, the constant f(X) :=E[Y]is calibrated, but is not
necessarily accurate. As another measure of prediction quality, we also report the Brier score (Brier, 1950):
Definition 3.3 (Brier score) .The mean-squared error (a.k.a., Brier score) of fw.r.t.Pis given by
Brier(f;P) :=E[(f(X)−Y)2] (6)
In multi-class scenarios, we set f(X)tomaxy∈[K]f(X)yandYto1{Y= arg maxy∈[K]f(X)y}.
4 Calibrated selective classification
We now propose an approach to defining—and optimizing—a robust selective calibration objective. We begin
with a definition of selective calibration for selective classification (§4.1). We then present a trainable objective
for selective calibration (§4.2), together with a coverage constraint (§4.3). Finally, we construct a framework
for training robust selective classifiers that encounter domain shift at test time (§4.4). For notational
convenience, we focus our discussion on binary classification, but evaluate both binary and multi-class tasks
in §6. As previously stated, we assume that fis a pre-trained and fixedblack-box, and only train gfor now.
4.1 Selective calibration
Once more, letXbe our input space and Y={0,1}be our binary label space with joint distribution P.
Again, we assume for now that Pis accessible (i.e., we can sample from it) and fixed. When classification
modelf:X→ [0,1]is paired with a selection model g:X→{ 0,1}, we define selective calibration as follows:
Definition 4.1 (Selective calibration) .A binary selective classifier (f,g)is selectively calibrated w.r.t. Pif
E/bracketleftbig
Y|f(X) =α, g(X) = 1/bracketrightbig
=α,∀α∈[0,1]in the range of frestricted to{x:g(x) = 1}.(7)
Similarly, we also define the selective calibration error, which measures deviation from selective calibration.
Definition 4.2 (Selective calibration error) .Theℓqbinary selective calibration error of (f,g)w.r.tPis
S-BCE(f,g;q,P) :=/parenleftig
E/bracketleftig/vextendsingle/vextendsingleE[Y|f(X), g(X) = 1]−f(X)/vextendsingle/vextendsingleq|g(X) = 1/bracketrightig/parenrightig1
q. (8)
3As discussed in §2, many different definitions for multi-class calibration have been proposed in the literature. For simplicity,
we focus only on the straightforward TCE metric, and leave extensions for more precise definitions to future work.
5Published in Transactions on Machine Learning Research (12/2022)
The top-label selective calibration error for multi-class classification is defined analogously, as is the selective
Brier score. What might we gain by selection? As a basic observation, we show that the coverage-constrained
selective calibration error of (f,g)can, at least in theory, always improve over its non-selective counterpart.
Claim 4.3 (Existence of a good selector) .For any fixed f:X→ [0,1], distribution P, and coverage ξ∈(0,1],
there exists g:X→{ 0,1}such that (i) E[g(X)]≥ξ, and (ii) S-BCE(f,g;q,P)≤BCE(f;q,P).
See Appendix A for a proof. Note that in some (e.g., adversarially constructed) cases we may only be able to
find trivial g(i.e., the constant g(X) = 1which recovers BCE (f;q,P)exactly), though this is typically not
the case. For intuition on how an effective selection rule may be applied, consider the following toy example.
Example 4.4 (Toy setting) .Assume that input X= (X1,X2)∈[0,1]×{0,1}follows the distribution
PX:=Unif(0,1)×Bern (p)for somep≥ξ∈(0,1), and output Y∈{0,1}follows the conditional distribution
PY|X:=/braceleftigg
Bern(X1) ifX2= 1,
Bern(min(X1+ ∆,1))ifX2= 0,(9)
for some ∆>0. Further assume that the given confidence predictor is simply f(X) =X1. That is, by design,
f(X)|X2= 1is calibrated, while f(X)|X2= 0has an expected calibration error of ≈∆. Marginally, f(X)
will have an expected calibration error of ≈∆(1−p)>0. It is then easy to see that defining the simple selector
g(X) =X2will result in a selectively calibrated model (f,g)with an expected selective calibration error of 0.
Extending this intuition to more realistic settings, our key hypothesis is that there often exist some predictive
features of inputs that are likely to be uncalibrated (e.g., when X2= 1from Example 4.4) that can be identified
and exploited via a selector g. In the next sections, we investigate objectives for learning empirically effective g.
4.2 A trainable objective for selective calibration
Our calibration objective is based on the Maximum Mean Calibration Error (MMCE) of Kumar et al. (2018).
LetHdenote a reproducing kernel Hilbert space (RKHS) induced by a universal kernel k(·,·)with feature
mapϕ: [0,1]→H. LetR:=f(X)∈[0,1]. The MMCE of fw.r.t.Pand kernel kis then
MMCE(f;k,P) :=/vextenddouble/vextenddouble/vextenddoubleE/bracketleftbig
(Y−R)ϕ(R)/bracketrightbig/vextenddouble/vextenddouble/vextenddouble
H, (10)
where∥·∥HdenotesnormintheHilbertspace H. Kumaretal.(2018)showedthattheMMCEis 0ifffisalmost
surely calibrated, a fundamental property we call faithfulness. Transitioning to selective classification, we now
propose the Selective Maximum Mean Calibration Error (S-MMCE). For notational convenience, define the
conditionalrandomvariable V:=f(X)|g(X) = 1∈[0,1]. TheS-MMCEof (f,g)w.r.t.Pandkernelkisthen
S-MMCE(f,g;q,k,P ) :=/vextenddouble/vextenddouble/vextenddoubleE/bracketleftbig
|E[Y|V]−V|qϕ(V)/bracketrightbig/vextenddouble/vextenddouble/vextenddouble1
q
H. (11)
S-MMCE involves only a few modifications to MMCE. The iterated expectation in Eq. (11), however, makes
the objective difficult to estimate in a differentiable way. To overcome this, instead of directly trying to
calculate the S-MMCE, we formulate and optimize a more practical upper bound to S-MMCE,
S-MMCE u(f,g;q,k,P ) :=/vextenddouble/vextenddouble/vextenddoubleE/bracketleftbig
|Y−V|qϕ(V)/bracketrightbig/vextenddouble/vextenddouble/vextenddouble1
q
H=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE[|Y−f(X)|qg(X)ϕ(f(X))]
E[g(X)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
q
H.(12)
As with MMCE and other kernel-based IPMs (such as MMD, see Gretton et al. (2012)), we can take
advantage of the kernel trick when computing a finite sample estimate of S-MMCE2
uover a mini-batch
D:={(xi,yi)}n
i=1, where (xi,yi)are drawn i.i.d. from P. Denotef(x)∈[0,1]asr. The empirical estimate is
\S-MMCE2
u(f,g;q,k,D) :=/parenleftigg/summationtext
i,j∈D|yi−ri|q|yj−rj|qg(xi)g(xj)k(ri,rj)/summationtext
i,j∈Dg(xi)g(xj)/parenrightigg1
q
. (13)
Intuitively, we can see that a penalty is incurred when xiandxjwith similar confidence values ri,rjare
both selected, but have ri,rjthat are misaligned with the true labels yi,yj. In the remainder of the paper we
will informally collectively refer to S-MMCE, S-MMCE u, and the empirical estimate as simply the S-MMCE.
6Published in Transactions on Machine Learning Research (12/2022)
4.2.1 Theoretical analysis
We briefly highlight a few theoretical properties of S-MMCE, which are mainly analogous to those of MMCE.
SeeAppendixAforproofs. Theseresultsalsoholdinthemulti-classcasewhenfollowingamulti-class-to-binary
reduction (as in the TCE). First, we show that S-MMCE is a faithful measure of selective calibration.
Theorem 4.5 (Faithfulness) .Letkbe a universal kernel, and let q≥1. The S-MMCE is then 0 if and only
if(f,g)is almost surely selectively calibrated, i.e., P(E[Y|V] =V) = 1, whereV:=f(X)|g(X) = 1.
Whilefaithfulnessisappealing, wenotethat S-MMCE = 0isnotalwaysachievableforafixed f(i.e., noselector
gmay exist). Next, we show a connection between S-MMCE and the expected selective calibration error.
Proposition 4.6 (Relationship to S-BCE) .S-MMCE≤K1
2qS-BCE, whereK= maxα∈[0,1]k(α,α).
As can be expected, lower S-BCE leads to a lower upper bound on S-MMCE (both are 0 when (f,g)is
calibrated). Lastly, we show that S-MMCE u, which we always minimize in practice, upper bounds S-MMCE.
Proposition 4.7 (S-MMCE upper bound) .For any (f,g),P, andq≥1we have S-MMCE≤S-MMCEu.
4.3 Incorporating coverage constraints
S-MMCE depends heavily on the rejection rate of g(X). As discussed in §1, we desire gto allow predictions
on at least some ξ-fraction of inputs. This leads us to the following constrained optimization problem:
minimize
gS-MMCE(f,g;k,P)s.t.E/bracketleftbig
g(X)/bracketrightbig
≥ξ. (14)
A typical approach to solving Eq. (14)might be to convert it into an unconstrained problem with, e.g., a
quadratic penalty (Bertsekas, 1996), and then to train a soft version of g,˜g:X→ [0,1], together with some
form of simulated annealing to make it asymptotically discrete (as desired). However, we find that we can
effectively change the problem slightly (and simplify it) by training the soft selector ˜gon the S-MMCE loss
withoutthe denominator that depends on g(X). Instead of explicitly training for a specific ξ, we simply use
a logarithmic regularization term that prevents ˜gfrom collapsing to 0(we will later re-calibrate it for ξ), i.e.,
Lreg(f,˜g;q,k,D) :=λ1/parenleftbigg/summationdisplay
i,j∈D|yi−ri|q|yj−rj|q˜g(xi)˜g(xj)k(ri,rj)/parenrightbigg1
q
−λ2/summationdisplay
i∈Dlog ˜g(xi)(15)
Regularized S-MMCE uloss for training soft ˜g.
whereλ1,λ2≥0are hyper-parameters. This also has the advantage of avoiding some of the numerical
instability present in Eq. (13) for small effective batch sizes (i.e., when either the batch size is small, or the
rejection rate is relatively large). The continuous ˜gis then discretized as g(x) :=1{˜g(x)≥ˆτ}, similar to the
selective models in §3.1.ˆτcan conveniently be tuned to satisfy multiple ξ(versus being fixed at training time),
as follows. LetSbe a set of ˜g(x)scores over an additional split of i.i.d. unlabeled data (ideally, test data). We
setˆτ:=Threshold (ξ,S), where Threshold (ξ,S)isthelargestvaluethatatleast ξ-fractionofSisgreaterthan:
ˆτ:= Threshold( ξ,S) := sup/braceleftig
τ∈R:1
|S|/summationdisplay
s∈S1{s≥τ}≥ξ/bracerightig
. (16)
Whiletargetingthelowestviablecoverage(i.e., =ξvs>ξ)isn’tnecessarily optimal, weempiricallyfindittobe
a good strategy. For large enough |S|, we can further show that gwill have coverage ≈ξwith high probability.
Proposition 4.8 (Coverage tuning) .LetDtune:= (Xi,...,Xη}be a set of i.i.d. unlabeled random variables
drawn from a distribution P. Let random variable ˆτ:=Threshold (ξ,{˜g(Xi):Xi∈D tune}). Note that ˆτis a
constant givenDtune. Further assume that random variable ˜g(X)is continuous for X∼P. Then∀ϵ>0,
P/parenleftbig
E[1{˜g(X)≥ˆτ}]≤ξ−ϵ|Dtune/parenrightbig
≤e−2ηϵ2, (17)
where the probability is over the draw of unlabeled threshold tuning data, Dtune.
7Published in Transactions on Machine Learning Research (12/2022)
Algorithm 1 Robust training for calibrated selective classification.
Definitions fis the confidence model. Dtrainis a sample of available training data. Tis a (task-specific) family of
perturbationsthatcanbeappliedto Dtraintosimulatepotentialtest-timedomainshifts. Dtuneisanadditionalsampleof
unlabeled dataavailabletous. Ifpossible, Dtuneshouldbedrawnfromthetest-timedistribution. ξisthetargetcoverage.
1:function train(f,Dtrain,Dtune,T,ξ)
2:▷Initialize a soft selector model ˜g:X→ [0,1]. (§4.3)
3:▷Define the binarized selector model, g(x) :=1{˜g(x)≥τ}, for someτ∈Rthat we will choose.
4:foriter = 1,2,...do (§4.4)
5:▷Randomly sample a batch of mperturbation functions from T,Bfunc:={ti∼T}m
i=1.
6:▷Apply each perturbation function titoDtrain, obtainingBshifted :={ti◦Dtrain:ti∈Bfunc}.
7:▷Calculate the selective calibration error of (f,g)forQi∈Bshiftedusingτ= Threshold( ξ,{˜g(x):x∈Qi}).
8:▷(Optional) Identify the κ-worst datasets,Qπ(i)∈Bshifted,i= 1,...,κ, sorted by selective calibration error.
9:▷Take a gradient step for ˜gusing theκ-worstQi(or all, if not L8 ),1
κ/summationtextκ
i=1Lreg(f,˜g;q,k,Qπ(i)), see Eq. (15).
10:▷Stop if ˜ghas converged, or if a maximum number of iterations has been reached.
11:▷Set final ˆτonDtune, where ˆτ:= Threshold( ξ,{˜g(x):x∈D tune}), see Eq. (16). (§4.3)
12:returng(x) :=1{˜g(x)≥ˆτ}
Tosummarize, thebasicstepsforobtaining gareto(1)trainasoft ˜gviaEq.(15), and(2)tuneathresholdtoob-
tainadiscrete gviaEq.(16). Proposition4.8showsthatthisgivesusamodelwithclosetoourdesiredcoverage
with high probability. Still, performance may start to degrade when distributions shift, which we address next.
4.4 Learning robust selectors via simulated domain shifts
Until now, we have treated the distribution Pas constant, and have not distinguished between a Ptrainand a
Ptest. A main motivation of our work is to create selective classifiers (f,g)that can effectively generalize to new
domains notseen during training, i.e., Ptrain̸=Ptest, so that our models are more reliable when deployed in the
wild. We adopt a simple recipe for learning a robust gfor a fixedfthrough data augmentation. Let Tbe a fam-
ily of perturbations that can be applied to a set of input examples. For example, in image classification, Tcan
include label-preserving geometric transformations or color adjustments. Tneed not solely include functions
ofindividual examples. For example, in lung cancer risk assessment, we randomly resample Dtrainaccording to
the hospital each scan was taken in, to simulate covariate shift (without directly modifying the input CT scans).
LetDtrain:={(xi,yi)}n
i=1∼Ptrainbe a split of available training data (ideally distinct from whatever data was
used to train f). For a given perturbation t∈T, letQt:=t◦Dtraindenote the result of applying ttoDtrain.
Depending on the severity of the perturbation that is used (as measured relative to f), predictions onQtmay
suffer from low, moderate, or high calibration error. Motivated by distributionally robust optimization (DRO)
methods(Levyetal.,2020), weoptimizeselectivecalibrationerroracrossperturbationsby(1)samplingabatch
of perturbations from T, (2) applying them to Dtrainand making predictions using f, (3) optionally identifying
theκ-worst selectively calibrated perturbed datasets in the batch when taking ˜gat coverage ξ,4and then (4)
optimizing S-MMCE only over these κ-worst examples (or all examples otherwise).5See Algorithm 1. Our
method can be viewed as a form of group DRO (Sagawa et al., 2020), combined with ordered SGD (Kawaguchi
& Lu, 2020). A unique aspect of our setup, however, is that our groups are defined over general functions of the
data (i.e., requiring robustness over a broad set of derived distributions), rather than a fixed set of identified
sub-populations of the data. Furthermore, the number of functions applied to the data can be combinatorially
large—for example, when defining Tto be comprised of compositions of a set of base transformations.
4.5 Selector implementation
We implement ˜gas a binary MLP that takes high-level meta features of the example xas input. Specifically, we
define ˜g:=σ(FF(ϕmeta(x))), whereσ(·)isasigmoidoutput, FFisa3-layerReLUactivatedfeed-forwardneural
4When not optimizing for one ξ(e.g., when sharing ˜gfor multiple ξ), we measure the selective calibration error AUC.
5Not all perturbations cause performance degradation, hence the motivation to only optimize over the top- κworst per batch.
8Published in Transactions on Machine Learning Research (12/2022)
network with 64 dimensional hidden states, and ϕmeta(x)extracts the following assortment of fixed features,6
many of which are derived from typical out-of-distribution and confidence estimation method outputs:
•Confidence score. maxpθ(y|x), wherepθ(y|x)is the model estimate of p(y|x)(i.e., equal to f(X)).
•Predicted class index. One-hot encoding of arg maxpθ(y|x)∈{0,1}K, whereK≥2.
•Prediction entropy. The entropy,/summationtextK
i=1pθ(yi|x) logpθ(yi|x), of the model prediction, where K≥2.
•Full confidence distribution. Raw model estimate pθ(y|x)∈[0,1]K, whereK≥2.
•Kernel density estimate. 1D estimate of p(x)derived from a KDE with a Gaussian kernel.
•Isolation forest score. 1D outlier score for xderived from an Isolation Forest (Liu et al., 2008).
•One-class SVM score. 1D novelty score for xderived from a One-Class SVM (Schölkopf et al., 2001).
•Outlier score. 1D novelty score for xderived from a Local Outlier Factor model (Breunig et al., 2000).
•kNNdistance. 1D outlierscore for xderived from the meandistance to its knearest training set neighbors.
We also use a subset of the derived features individually as baselines in §5.3. For all derived scores, we use the
base network’s last hidden layer representation of xas features. If the number of classes Kis large (e.g., Ima-
geNet where K= 1000), we omit the full confidence distribution and the one-hot predicted class index to avoid
over-fitting. Allmodelsaretrainedwith q= 2forS-MMCE-basedlosses. ForadditionaldetailsseeAppendixB.
5 Experimental setup
5.1 Tasks
CIFAR-10-C. The CIFAR-10 dataset (Krizhevsky, 2012) contains 32×32×3color images spanning 10
image categories. The dataset has 50k images for training and 10k for testing. We remove 5k images each from
the training set for validation and perturbation datasets for training ˜g. Our perturbation family Tis based on
the AugMix framework of Hendrycks et al. (2020), where various data augmentation operations (e.g., contrast,
rotate, shear, etc) are randomly chained together with different mixture weights and intensities. Our base
modelfis a WideResNet-40-2 (Zagoruyko & Komodakis, 2016). The last layer features for deriving ϕmeta(x)
are inR128. We temperature scale fafter training on clean validation data (we use the same underlying
set of validation images for training both fand˜g).We evaluate on the CIFAR-10-C dataset (Hendrycks
& Dietterich, 2019), where CIFAR-10-C is a manually corrupted version of the original CIFAR-10 test set.
CIFAR-10-C includes a total of 15 noise, blur, weather, and digital corruption types that are applied with 5
different intensity levels (to yield 50k images per corrupted test set split). Note that these types of corruptions
are not part of the AugMix perturbation family, and therefore not seen during training, in any form.
ImageNet-C. The ImageNet dataset (Deng et al., 2009) contains ≈1.2million color images (scaled to
224×224×3) spanning 1k image categories. We use a standard off-the-shelf ResNet-50 (He et al., 2016) for
fthat was trained on the ImageNet training set. We then simply reuse images from the training set to create
separateperturbedvalidationandtrainingdatasetsfortraining ˜g, andusethesameperturbationfamilyasused
for CIFAR-10 above (i.e., based on AugMix). We also use the clean validation data for temperature scaleing.
The last layer features for deriving ϕmeta(x)are projected down from R2048toR128using SVD. Since the
number of classes is large, we omit the predicted class index and full confidence distribution from our selector
input features. We evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019), which applies many of
the same corruptions as done in CIFAR-10-C. Again, none of these types of corruptions are seen during training.
Lung cancer (NLST-MGH). As described in §1, in lung cancer risk assessment, lung CT scans are used to
predict whether or not the patient will develop a biopsy-confirmed lung cancer within the 6 years following the
scan. In this work we utilize, with permission, data and models from Mikhael et al. (2022), all of the which was
subject to IRB approval (including usage for this study). The base model for fis a 3D CNN trained on scans
from the National Lung Screening Trial (NLST) data (Aberle et al., 2011). We then use a separate split of 6,282
scans from the NLST data to train ˜g. The last layer features for deriving ϕmeta(x)are inR256. As the NLST
6We opt to use fixed, simple, meta features as most of our training sets are small.
9Published in Transactions on Machine Learning Research (12/2022)
datacontainsscansfrommultiplehospitals(33hospitalsintotal), weareabletoconstructaperturbationfamily
Tusing randomly weighted resamplings according to the hospital each scan is taken from. We evaluate our
selective classifier on new set of 1,337 scans obtained from Massachusetts General Hospital (MGH), taken from
patients with different demographics and varied types of CT scans. See Appendix B.3 for additional details.
5.2 Evaluation
For each task, we randomly reinitialize and retrain ˜gfive times, resample the test data with replacement to cre-
atefivesplits, andreportthemeanandstandarddeviationacrossall25trials(#models ×#splits). Inorderto
compareperformanceacrosscoveragelevels, weploteachmetricasafunctionof ξ, andreporttheAUC(starting
from a coverage of 5% to maintain a minimum sample size). Our primary metric is the Selective Calibration
error AUC (lower is better). We report both the ℓ2andℓ∞error, written as S-BCE 2and S-BCE∞, respec-
tively (and, accordingly, S-TCE 2and S-TCE∞). Our secondary metric is the Selective Brier Score AUC
(lower is better). Thresholds for establishing the desired coverage are obtained directly from the test data, with
thelabelsremoved. Hyper-parametersaretunedondevelopmentdataheldoutfromthetrainingsets(bothdata
and perturbation type), including temperature scaling. Shaded areas in plots show ±standard deviation across
trials, while the solid line is the mean. AUCs are computed using the mean across trials at each coverage level.
5.3 Baselines
Full model. We use our full model with no selection applied (i.e., g(x) = 1always). All examples are retained,
and the selective calibration error of the model is therefore unchanged from its original calibration error.
Confidence. We use the model’s raw confidence score to select the most confident examples. For binary
models, we define this as max(f(x),1−f(x)), and maxy∈[K]f(x)yfor multi-class models. Often, but not
always, more confident predictions can also be more calibrated, making this a strong baseline.
Outlierandnoveltydetectionmodels. Totestifourlearnedmethodimprovesovercommonheuristics, we
directly use the features from §4.5 to select the examples that are deemed to be the least anomalous or outlying.
Specifically, we compare to isolation forest scores, one-class SVM scores, and training set average kNN distance
by simply replacing ˜gwith these outputs.7We then apply the same thresholding procedure as in Eq. (16).
6 Experimental results
Selective calibration error. Figure 2 reports our main calibration results. In all cases, we observe that
optimizing for our S-MMCE objective leads to significant reductions in measured calibration error over the
selected examples. In particular, S-MMCE-derived scores outperform all baseline scoring mechanisms in both
ℓ2andℓ∞Selective Calibration Error AUC, as well as in Selective Brier Score AUC. This is most pronounced
on the image classification tasks—which we conjecture is due to the effectiveness of AugMix perturbations at
simulating the test-time shifts. Note that the lung cancer task has far less data (both in training and testing)
than CIFAR-10-C or ImageNet-C, hence its higher variance. Its perturbation class (reweighted resampling
based on the source hospital) is also relatively weak, and limited by the diversity of hospitals in the NLST
training set. Despite this, S-MMCE still improves over all baselines. Interestingly, while some baseline
approaches—such as rejecting based on one-class SVM scores or isolation forest scores—can lead to worse
calibration error than the full model without abstention, predictors produced using S-MMCE have reliably
lower selectivecalibration error (per our goal) across allcoverage rates ξ. The fact that our method significantly
outperforms distance-based baselines, such as kNN distance or Isolation Forests, is also encouraging in that
it suggests that ghas learned behaviors beyond identifying simple similarity to the training set.
Robustness to perturbation type. Figure 3 shows a breakdown of Selective Calibration Error AUC across
all 15 test time perturbations for CIFAR-10-C (note that Figure 2 shows only averages across all perturbations).
Naturally, the reduction in calibration error is more pronounced when there is significant calibration error to be-
gin with (e.g., see Gaussian noise). That said, across all perturbation types, we observe substantial decreases in
Selective Error AUC, both relative to the full model without abstentions, and relative to a standard confidence-
based selection model. Similar results are also seen for ImageNet-C, which we provide in Appendix C.
7We omit entropy, KDE, and LOF scores for brevity as they perform similarly or worse compared to the other heuristics.
10Published in Transactions on Machine Learning Research (12/2022)
(a) CIFAR-10-C
 (b) ImageNet-C
 (c) Lung cancer (NLST-MGH)
Figure 2: Coverage vs. selective calibration error and Brier scores on CIFAR-10-C, ImageNet-C, and lung
cancer data. For CIFAR-10-C and ImageNet-C, we report average results across all perturbation types. For
lung cancer, we report results on the diverse MGH test population. Empirically, across all coverage levels and
metrics, rejections based on goptimized for S-MMCE perform the best (or on par with the best in some cases).
Analysisofrejectionbehavior. Weempiricallyinvestigatethetypesofselectionsthat gtrainedtooptimize
S-MMCE make, as compared to typical confidence-based selections. For the lung cancer risk assessment task
on MGH data, Figure 4 shows the empirical distribution of confidence scores at various coverage levels ξ,
conditioned on the example Xbeing selected ( g(X) = 1). As expected, the confidence-based method selects
only the most confident examples, which in this case, is primarily examples where the patient is deemed least
likely to be at risk ( f(X)≈0). Figure 4 (b) then demonstrates that an undesirable outcome of this approach
is that a disproportionate number of patients that dodevelop cancer ( Y= 1) are not serviced by the model
(g(X) = 0). In other words, the model avoids making wrong predictions on these patients, but as a result,
doesn’t make predictions on the very examples it is needed most. On the other hand, our S-MMCE-based
model clearly does not solely reject examples based on raw confidence—the distribution of confidence scores
of accepted examples matches the marginal distribution (i.e., without rejection) much more closely. Similarly,
as demonstrated in Figure 5, this selector rejects relatively fewer examples where Y= 1as compared to the
confidence-based method. Together with the low calibration error results from Figure 2, this suggests that
it is more capable of giving calibrated predictions both for non-cancerous and cancerous patients alike.
11Published in Transactions on Machine Learning Research (12/2022)
Figure 3:ℓ2selective top-label calibration error AUC reported for each of the 15 test perturbations in CIFAR-
10-C(inadditiontotheaverage). OptimizingS-MMCEleadstosignificanterrorreductionsacrossperturbation
types, relativetoamodelwithoutabstentions("Full"), aswellasthestandardselectiveclassifier("Confidence").
(a) Coverage ξ= 0.90.
 (b) Coverage ξ= 0.70.
 (c) Coverage ξ= 0.50.
Figure 4: Empirical distribution of f(X)|g(X) = 1for different coverage rates on MGH data (for f(X)≤0.4
for visualization). Empirically, by selecting the most confident predictions, confidence-based predictions mainly
take examples that are thought to be notbe cancerous (i.e., where f(x)≈P(Y= 1|X=x)is low). The
behavioroftheS-MMCE-basedclassifier, however, islessskewedtowardsonlyselectingexamplesofaparticular
confidencevalue, and f(X)|g(X) = 1morecloselyfollowsthemarginaldistributionof f(X)withoutselection.
Figure 5: Rejection ratios by label type at
ξ= 0.90on MGH data. Blue denotes the ratio of
each class in the full data. Proportionally more
of the confidence-based rejections are cancerous
(presumably as they are “harder” to classify). S-
MMCE rejections are relatively less imbalanced.Additional results. We briefly highlight a few addi-
tional results that are included in Appendix C. Figure C.3
presents an ablation on CIFAR-10-C that explores the
relative importance of different components of our training
procedure. Interestingly, even gtrained to minimize
the S-MMCE on in-domain data yield improvements in
selective calibration error on out-of-domain data, though
the results are significantly improved when including the
perturbation family T. Figure C.2 shows the effect of first
trainingfon perturbations from T(though our focus
is on black-box f). We use a model that is both trained
and temperature scaled with AugMix perturbations on
CIFAR-10. In line with Hendrycks et al. (2020), this leads
tof(X)with lower calibration error on CIFAR-10-C.
Still, our framework leads to substantially lower selective
calibration error AUC, relative to this new baseline.
We also report standard selective accuracy scores in
Figure C.5. Note that more calibrated classifiers are
not necessarily more accurate. Indeed, on ImageNet-C,
where S-MMCE yields the most selectively calibrated
12Published in Transactions on Machine Learning Research (12/2022)
predictions, it does not yield the best selectively accurate predictions. Still, accuracy is not always at odds
with calibration (especially if the higher confidence scores are also calibrated): in CIFAR-10-C and the lung
data, S-MMCE also achieves close to the best selective accuracy.
Limitations and future work. We note a few considerations that are unaddressed by this work. Marginal
calibrationdoesnotaccountfordifferencesbetweensub-populationsorindividuals. Thisisbothrelevanttofair-
ness and personalized decision making (Zhao et al., 2020; Shah et al., 2021; Jones et al., 2021). While we do not
study them here, algorithms for more complete notions of calibration (Hebert-Johnson et al., 2018; Vaicenavi-
cius et al., 2019; Shah et al., 2021), can be incorporated into our framework. We also note that the performance
of our method is strongly coupled with the perturbation family T, which, here, is manually defined per task.
Thiscanfailwhenthefamily Tisnotstraightforwardtoconstruct. Forexample, theimageaugmentationssuch
as those in AugMix (Hendrycks et al., 2020) do not generalize across modalities (or even other image tasks, nec-
essarily). Still, as tools for creating synthetic augmentations improve, they can directly benefit our algorithm.
7 Conclusion
Reliable quantifications of model uncertainty can be critical to knowing when to trust deployed models, and
how to base important decisions on the predictions that they make. Inevitably, deployed models will make
mistakes. More fundamentally, not every query posed to even the best models can always be answered with
complete certainty. In this paper, we introduced the concept of selective calibration , and proposed a framework
for identifying when to abstain from making predictions that are potentially uncalibrated —i.e., when the
model’s reported confidence levels may not accurately reflect the true probabilities of the predicted events.
We showed that our method can be used to selectively make predictions that are better calibrated as a whole,
while still being subject to basic coverage constraints. Finally, our experimental results demonstrated that
(1) our calibrated selective classifiers make more complex abstention decisions than simple confidence-based
selective classifiers, and (2) are able to generalize well under distribution shift.
Acknowledgements
We thank Dmitry Smirnov, Anastasios Angelopoulos, Tal Schuster, Hannes Stärk, Bracha Laufer-Goldshtein,
members of the Regina Barzilay and Tommi Jaakkola research groups, and the anonymous reviewers for
helpful discussions and feedback. We also thank Peter Mikhael for invaluable support on the lung cancer risk
assessment experiments. AF is supported in part by the NSF GRFP and MIT MLPDS.
References
Denise R Aberle, Amanda M Adams, Christine D Berg, William C Black, Jonathan D Clapp, Richard M
Fagerstrom, Ilana F Gareen, Constantine Gatsonis, Pamela M Marcus, JoRean D Sicks, National Lung
Screening Trial Research Team, and Reginald F. Munden. Reduced lung-cancer mortality with low-dose
computed tomographic screening. New England Journal of Medicine , 365(5):395–409, 2011.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete
problems in ai safety. ArXiv preprint: 1606.06565 , 2016.
Anastasios N. Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control.
ArXiv preprint: 2208.02814 , 2022.
Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty
estimation and ensembling in deep learning. In International Conference on Learning Representations
(ICLR), 2020.
Yujia Bao and Regina Barzilay. Learning to split for automatic bias detection. ArXiv preprint: 2204.13749 ,
2022.
Peter L. Bartlett and Marten H. Wegkamp. Classification with a reject option using a hinge loss. Journal of
Machine Learning Research , 9(59):1823–1840, 2008.
13Published in Transactions on Machine Learning Research (12/2022)
Stephen Bates, Anastasios Nikolas Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I. Jordan. Distribu-
tion free, risk controlling prediction sets. ArXiv preprint: 2101.02703 , 2020.
Dimitri P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural
Computation Series) . Athena Scientific, 1 edition, 1996. ISBN 1886529043.
Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. Lof: identifying density-based
local outliers. In ACM sigmod record , volume 29, pp. 93–104. ACM, 2000.
Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review , 78:1–3,
1950.
Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and
Alexandr A. Kalinin. Albumentations: Fast and flexible image augmentations. Information , 11(2), 2020.
Maxime Cauchois, Suyash Gupta, Alnur Ali, and John C. Duchi. Robust validation: Confident predictions
even when distributions shift. ArXiv preprint: 2008.04267 , 2020.
Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. Classification with
rejection based on cost-sensitive classification. In International Conference on Machine Learning (ICML) ,
2021.
C. K. Chow. An optimum character recognition system using decision functions. IRE Transactions on
Electronic Computers , 6:247–254, 1957.
Garry Choy, Omid Khalilzadeh, Mark Michalski, Synho Do, Anthony E. Samir, Oleg S. Pianykh, J. Raymond
Geis, Pari V. Pandharipande, James A. Brink, and Keith J. Dreyer. Current applications and future impact
of machine learning in radiology. Radiology , 288(2):318–328, 2018.
Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample testing with
analytic representations of probability measures. In Advances in Neural Information Processing Systems
(NeurIPS) , 2015.
L.P. Cordella, C. De Stefano, F. Tortorella, and M. Vento. A method for improving classification reliability
of multilayer perceptrons. IEEE Transactions on Neural Networks , 6(5):1140–1147, 1995. doi: 10.1109/72.
410358.
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. In Advances in Neural
Information Processing Systems (NeurIPS) , 2016a.
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In International Conference
on Algorithmic Learning Theory (ALT) , 2016b.
Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data
augmentation with a reduced search space. In Advances in Neural Information Processing Systems
(NeurIPS) , 2020.
A. P. Dawid. The well-calibrated bayesian. Journal of the American Statistical Association , 77(379):605–610,
1982.
Claudio De Stefano, Carlo Sansone, and Mario Vento. To reject or not to reject: That is the question -
an answer in case of neural classifiers. IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews) , 30:84 – 94, 03 2000.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2009.
C. Dromain, B. Boyer, R. Ferré, S. Canale, S. Delaloge, and C. Balleyguier. Computed-aided diagnosis (cad)
in the detection of breast cancer. European Journal of Radiology , 82(3):417–423, 2013.
14Published in Transactions on Machine Learning Research (12/2022)
Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of Machine
Learning Research (JMLR) , 11, 2010.
Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration. Biometrika , 85(2):379–390, 1998.
Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Advances in Neural
Information Processing Systems (NeurIPS) , 2017.
Yonatan Geifman and Ran El-Yaniv. SelectiveNet: A deep neural network with an integrated reject option.
InInternational Conference on Machine Learning (ICML) , 2019.
Tony Ginart, Martin Jinye Zhang, and James Zou. MLDemon: Deployment monitoring for machine learning
systems. In International Conference on Artificial Intelligence and Statistics (AISTATS) , 2022.
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E. Raftery. Probabilistic forecasts, calibration and
sharpness. Journal of the Royal Statistical Society Series B , 69(2):243–268, 2007.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel
two-sample test. Journal of Machine Learning Research (JMLR) , 13(25):723–773, 2012.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In
International Conference on Machine Learning (ICML) , 2017.
ChiragGuptaandAadityaRamdas. Top-labelcalibrationandmulticlass-to-binaryreductions. In International
Conference on Learning Representations (ICLR) , 2022.
Chirag Gupta, Aleksandr Podkopaev, and Aaditya Ramdas. Distribution-free binary classification: prediction
sets, confidence intervals and calibration. In Advances in Neural Information Processing Systems (NeurIPS) ,
2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.
Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration
for the (Computationally-identifiable) masses. In International Conference on Machine Learning (ICML) ,
2018.
Martin E. Hellman. The nearest neighbor classification rule with a reject option. IEEE Transactions on
Systems Science and Cybernetics , 6:179–185, 1970.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. International Conference on Learning Representations (ICLR) , 2019.
Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan.
AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the
International Conference on Learning Representations (ICLR) , 2020.
Radu Herbei and Marten H. Wegkamp. Classification with reject option. Canadian Journal of Statistics , 34
(4):709–721, 2006.
Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or not to trust a classifier. In Advances
in Neural Information Processing Systems (NeurIPS) , 2018.
Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates
to support personalized medicine. Journal of the American Medical Informatics Association , 19(2):263–274,
2012.
Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classification can
magnify disparities across groups. In International Conference on Learning Representations (ICLR) , 2021.
15Published in Transactions on Machine Learning Research (12/2022)
Amita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift. In Annual
Meeting of the Association for Computational Linguistics (ACL) , 2020.
Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael Curtis
Mozer, and Rebecca Roelofs. Soft calibration objectives for neural networks. In Advances in Neural
Information Processing Systems (NeurIPS) , 2021.
Kenji Kawaguchi and Haihao Lu. Ordered sgd: A new stochastic optimization framework for empirical risk
minimization. In International Conference on Artificial Intelligence and Statistics (AISTATS) , 2020.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,
Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness,
Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson,
Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In
International Conference on Machine Learning (ICML) , 2021.
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto , 2012.
Volodymyr Kuleshov and Percy Liang. Calibrated structured prediction. In Advances in Neural Information
Processing Systems (NeurIPS) , 2015.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using
calibrated regression. In International Conference on Machine Learning (ICML) , 2018.
Meelis Kull, Miquel Perelló-Nieto, Markus Kängsepp, Telmo de Menezes e Silva Filho, Hao Song, and Peter A.
Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet
calibration. In Advances in Neural Information Processing (NeurIPS) , 2019.
Ananya Kumar, Percy Liang, and Tengyu Ma. Verified uncertainty calibration. In Advances in Neural
Information Processing Systems (NeurIPS) , 2019.
Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks from
kernel mean embeddings. In International Conference on Machine Learning (ICML) , 2018.
Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally robust
optimization. In Advances in Neural Information Processing Systems (NeurIPS) , 2020.
Zhen Lin, Lucas Glass, M. Brandon Westover, Cao Xiao, and Jimeng Sun. SCRIB: Set-classifier with
class-specific risk bounds for blackbox models. In AAAI Conference on Artificial Intelligence (AAAI) ,
2022.
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black
box predictors. In International Conference on Machine Learning (ICML) , 2018.
Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy
Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information.
InProceedings of the 38th International Conference on Machine Learning (ICML) , 2021.
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In IEEE International Conference on
Data Mining , 2008.
Peter G. Mikhael, Jeremy Wohlwend, Ludvig Karstens Adam Yala, Justin Xiang, Angelo K. Takigami,
Patrick P. Bourgouin, PuiYee Chan, Sofiane Mrah, Wael Amayri, Yu-Hsiang Juan, Cheng-Ta Yang, Yung-
Liang Wan, Gigin Lin, Lecia V. Sequist, Florian J. Fintelmann, and Regina Barzilay. Sybil: A validated
deep learning model to predict future lung cancer risk from a single low-dose chest computed tomography,
2022.
Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Ann Hubis, Xiaohua Zhai, Neil Houlsby, Dustin
Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. In Advances in Neural
Information Processing Systems (NeurIPS) , 2021.
16Published in Transactions on Machine Learning Research (12/2022)
Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In International
Conference on Machine Learning (ICML) , 2020.
Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania.
Calibrating deep neural networks using focal loss. In Advances in Neural Information Processing Systems
(NeurIPS) , 2020.
Allan H. Murphy and Edward S. Epstein. Verification of probabilistic predictions: A brief review. Journal of
Applied Meteorology and Climatology , 6(5):748 – 755, 1967.
Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama. On the calibration of multiclass
classification with rejection. In Advances in Neural Information Processing Systems (NeurIPS) , 2019.
Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In
International Conference on Machine Learning (ICML) , 2005.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji
Lakshminarayanan, and Jasper Snoek. Can you trust your model 's uncertainty? evaluating predictive
uncertainty under dataset shift. In Advances in Neural Information Processing Systems (NeurIPS) , 2019.
Sangdon Park, Edgar Dobriban, Insup Lee, and Osbert Bastani. PAC prediction sets under covariate shift.
InInternational Conference on Learning Representations (ICLR) , 2022.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems (NeurIPS) , 2019.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research (JMLR) , 12:2825–2830,
2011.
John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood
methods. In Advances in Large Margin Classifiers , pp. 61–74. MIT Press, 1999.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in
Machine Learning . The MIT Press, 2009. ISBN 0262170051.
Stephan Rabanser, Stephan Günnemann, and Zachary Lipton. Failing loudly: An empirical study of methods
for detecting dataset shift. In Advances in Neural Information Processing Systems (NeurIPS) , 2019.
Sashank Reddi, Barnabas Poczos, and Alex Smola. Doubly robust covariate shift correction. In AAAI
Conference on Artificial Intelligence , 2015.
Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, and Michael C. Mozer. Mitigating bias in calibration error
estimation. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics
(AISTATS) , 2022.
Evgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge,
and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In
European Conference on Computer Vision (ECCV) , 2020.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural
networks. In International Conference on Learning Representations (ICLR) , 2020.
Bernhard Schölkopf, John C. Platt, John C. Shawe-Taylor, Alex J. Smola, and Robert C. Williamson.
Estimating the support of a high-dimensional distribution. Neural Computation , 13(7):1443–1471, 2001.
17Published in Transactions on Machine Learning Research (12/2022)
Abhin Shah, Yuheng Bu, Joshua Ka-Wing Lee, Subhro Das, Rameswar Panda, Prasanna Sattigeri, and
Gregory W Wornell. Selective regression under fairness criteria. arXiv preprint: 2110.15403 , 2021.
M. Shaked and J.G. Shanthikumar. Stochastic Orders . Springer Series in Statistics. Springer New York, 2007.
ISBN 9780387346755.
Masashi Sugiyama and Klaus-Robert Müller. Input-dependent estimation of generalization error under
covariate shift. Statistics and Decisions , 23(4):249–279, 2005.
Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candès, and Aaditya Ramdas. Conformal prediction
under covariate shift. In Advances in Neural Information Processing Systems (NeurIPS) , 2019.
Dustin Tran, Jeremiah Liu, Michael W. Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang,
Zelda Mariet, Huiyi Hu, Neil Band, Tim G. J. Rudner, Karan Singhal, Zachary Nado, Joost van Amersfoort,
Andreas Kirsch, Rodolphe Jenatton, Nithum Thain, Honglin Yuan, Kelly Buchanan, Kevin Murphy,
D. Sculley, Yarin Gal, Zoubin Ghahramani, Jasper Snoek, and Balaji Lakshminarayanan. Plex: Towards
reliability using pretrained large model extensions. ArXiv preprint: 2207.07411 , 2022.
Athanasios Tsoukalas, Timothy Albertson, and Ilias Tagkopoulos. From data to optimal decision making: A
data-driven, probabilistic machine learning approach to decision support for patients with sepsis. JMIR
Medical Informatics , 3(1):e11, 2015.
Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas Schön.
Evaluating model calibration in classification. In International Conference on Artificial Intelligence and
Statistics (AISTATS) , 2019.
Joost van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single
deep deterministic neural network. In International Conference on Machine Learning (ICML) , 2020.
Vladimir Vovk and Ivan Petej. Venn-abers predictors. In Proceedings of the Thirtieth Conference on
Uncertainty in Artificial Intelligence , 2014.
Vladimir Vovk, Ilia Nouretdinov, and Alexander Gammerman. Testing exchangeability on-line. In ICML,
2003.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World . Springer-
Verlag, Berlin, Heidelberg, 2005.
Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alexander Gammerman. Cross-conformal predictive
distributions. In Proceedings of the Seventh Workshop on Conformal and Probabilistic Prediction and
Applications , 2018.
Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain generalization.
InAdvances in Neural Information Processing Systems (NeurIPS) , 2021.
Junfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distributions:
Relating covariate shift to model misspecification. In International Conference on Machine Learning
(ICML), 2014.
Yaodong Yu, Stephen Bates, Yi Ma, and Michael I Jordan. Robust calibration with multi-domain temperature
scaling.arXiv preprint: 2206.02757 , 2022.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive
bayesian classifiers. In International Conference on Machine Learning (ICML) , 2001.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference
(BMVC) , 2016.
Shengjia Zhao, Tengyu Ma, and Stefano Ermon. Individual calibration with randomized forecasting. In
International Conference on Machine Learning (ICML) , 2020.
18Published in Transactions on Machine Learning Research (12/2022)
Shengjia Zhao, Michael Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating predictions to
decisions: A novel approach to multi-class calibration. In Advances in Neural Information Processing
Systems (NeurIPS) , 2021.
A Proofs
A.1 Proof of Claim 4.3
Proof.We prove existence in Claim 4.3 by example. Let random variable R:=f(X). Defineh(r) =|E[Y|
R=r]−r|to be the true calibration error for confidence level r∈[0,1]. Then take g(x)as
g(x) :=/braceleftigg
1ifh(f(x))≤ˆλ,
0 otherwise.(18)
where we define ˆλas
ˆλ:= inf/braceleftig
λ∈R:P(h(R)≤λ)≥ξ)/bracerightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
ξ-th quantile of the calibration error of f. (19)
We show that this choice of gsatisfies parts (i) and (ii) of Claim 4.3.
(i) Asg(X)is binary, we have E[g(X)] =P(h(R)≤ˆλ). The CDF P(h(R)≤λ)per Eq.(19)is right-continuous
with supλP(h(R)≤λ) = 1, so that ˆλis well-defined and satisfies P(h(R)≤ˆλ)≥ξ.
(ii) Let random variable V:=R|g(X) = 1. Then let random variables E,˜Ebe defined as
E:=|E[Y|R]−R|, (20)
˜E:=|E[Y|V]−V|. (21)
For our choice of g, we have for all λ∈[0,1]
P(˜E≤λ) =P(E≤λ|E≤ˆλ)≤P(E≤λ), (22)
givingE[ϕ(˜E)]≤E[ϕ(E)]for all increasing functions ϕby stochastic dominance (Shaked & Shanthikumar,
2007). For x≥0,ϕ(x) :=xqis increasing∀q≥1, hence E[˜Eq]≤E[Eq], which implies (E[˜Eq])1
q≤(E[˜Eq])1
q.
Comparing definitions, this is equivalent to S-BCE ≤BCE.
A.2 Proof of Theorem 4.5
Our proof follows that of Kumar et al. (2018), differing its treatment of (1) random variables conditioned on
g(X) = 1, and (2) non-negative ℓqcalibration error terms. We begin with the following lemma.
Lemma A.1. LetX,Y∈[0,1]be non-negative random variables with some joint distribution P. DefineM
as the integral probability metric,
M(X,Y ;C) := sup
h∈CE[Y·h(X)], (23)
whereCis the space of all continuous, bounded functions h(x)defined over x∈[0,1]. Then,
M(X,Y ;C) = 0⇐⇒Y= 0almost surely. (24)
Proof.SinceYis non-negative and bounded, and all h∈Care bounded, we can write
0≤M (X,Y ;C)≤Bhi·E[Y]<∞ (25)
forBup:= sup
h∈Csup
x∈[0,1]h(x), where 0<B up<∞, asCincludes functions that are not strictly negative.
19Published in Transactions on Machine Learning Research (12/2022)
Similarly,
∞>M(X,Y ;C)≥Blo·E[Y]≥0 (26)
forBlo:= sup
h∈Cinf
x∈[0,1]h(x), where 0<B lo<∞asCincludes functions that are strictly positive.
Combining bounds from Eqs. (25) and (25), we have
(i)P(Y= 0) = 1 =⇒E[Y] = 0 =⇒M (X,Y ;C) = 0, and
(ii)P(Y= 0)<1 =⇒E[Y]>0 =⇒M (X,Y ;C)>0.
We now proceed to prove Theorem 4.5.
Proof.For ease of notation, we define random variables V:=f(X)|g(X) = 1andE:=|E[Y|V]−V|q,
whereVis the selective confidence and Eis the selective calibration error at V. First, we have that
S-MMCE = 0⇐⇒S-MMCEq=/vextenddouble/vextenddouble/vextenddoubleE/bracketleftbig
E·ϕ(V)/bracketrightbig/vextenddouble/vextenddouble/vextenddouble
H= 0,
and second that
E= 0⇐⇒E[Y|V]−V= 0. (27)
Therefore, it will suffice to show that
S-MMCEq= 0⇐⇒E= 0almost surely .
We start by defining an alternative (intractable) integral probability metric:
M(E,V;C) := sup
h∈CE[E·c(V)], (28)
whereCdenotes the space of all continuous, bounded functions h(v)overv∈[0,1].
Applying Lemma A.1 gives M(E,V;C) = 0⇐⇒E= 0almost surely.
We now replaceCwith the set of functions F:={h∈H:∥h∥H≤1}in the RKHS with universal kernel k(·,·).
Sincekis universal,Fis dense in the space of bounded continuous functions with respect to the supremum
norm. That is, for every h∈Candϵ>0, there∃h′∈Fsuch that supv|h(v)−h′(v)|<ϵ. Using the same
arguments as Lemma A.1, we can then immediately show that M(E,V;F) = 0iffE= 0almost surely.
Finally, using the reproducing property of the RKHS, we derive the equivalence to S-MMCEq:
M(E,V;F) = sup
h∈FE[E·h(V)]
= sup
h∈FE/bracketleftbig
E·/angbracketleftbig
h,k(V,·)/angbracketrightbig
H/bracketrightbig
= sup
h∈F/angbracketleftbig
h,E[E·ϕ(V)]/angbracketrightbig
H
=/angbracketleftbiggE[E·ϕ(V)]
∥E[E·ϕ(V)]∥H,E[E·ϕ(V)]/angbracketrightbigg
H
=/vextenddouble/vextenddouble/vextenddoubleE/bracketleftbig
E·ϕ(V)/bracketrightbig/vextenddouble/vextenddouble/vextenddouble
H
=S-MMCEq,(29)
whereϕis the feature map associated with kernel k.
20Published in Transactions on Machine Learning Research (12/2022)
A.3 Proof of Proposition 4.6
Lemma A.2. LetXbe a random variable. For any family of functions Fdefined over the range of X,
sup
h∈FE[h(X)]≤E[sup
h∈Fh(X)] (30)
Proof.For any fixed h′∈F, we haveh′(X)≤suph∈Fh(X), which implies E[h′(X)]≤E[suph∈Fh(X)].
Since this holds for all h′∈F,E[suph∈Fh(X)]is an upper bound of the set {E[h′(X)]:h′∈F}.
Lemma A.3. LetHbe an RKHS with kernel k. Then the following holds for all functions h∈H:
h(v)≤/radicalbig
k(v,v)∥h∥H (31)
Proof.Using the reproducing property of the RKHS and the Cauchy-Schwartz inequality, we have that
h(v) =⟨h,k(·,v)⟩H≤∥h∥H∥k(·,v)∥H
=/radicalbig
k(v,v)∥h∥H.(32)
We now proceed to prove Proposition 4.6.
Proof.We begin by writing S-MMCE in IPM form as in A.2:
S-MMCEq=M(|E[Y|V]−V|q,V;F)
= sup
h∈FE[|E[Y|V]−V|q·h(V)](33)
whereF:={h∈H:∥h∥H≤1}.
Applying Lemmas A.2 and A.3,
sup
h∈FE[|E[Y|V]−V|q·h(V)]≤E/bracketleftbigg
|E[Y|V]−V|q·sup
h∈Fh(V)/bracketrightbigg
≤E/bracketleftbigg
|E[Y|V]−V|q·sup
v/radicalbig
k(v,v)∥h∥H/bracketrightbigg
≤K1
2E[|E[Y|V]−V|q].(34)
Comparing definitions, we can write S-MMCE ≤K1
2qS-BCE.
A.4 Proof of Proposition 4.7
Proof.We begin by writing S-MMCE in IPM form as in A.2:
S-MMCEq=M(|E[Y|V]−V|q,V;F)
= sup
h∈FE[|E[Y|V]−V|q·h(V)](35)
whereF:={h∈H:∥h∥H≤1}.
Fix anyh∈Fandv∈[0,1]. For notational convenience, let Z:=Y|V=v, and define the function
φ(Z,v) :=|Z−v|q·h(v) (36)
Jensen’s inequality yields E[φ(Z,v)]≥φ(E[Z],v), asφis convex in Zfor every constant v. Applying the law
of iterated expectations then gives E[φ(Z,V)]≥E[φ(E[Z],V)]. Plugging back into Eq. (36), we have
E[|Y−V|q·h(V)]≥E[|E[Y|V]−V|q·h(V)]. (37)
21Published in Transactions on Machine Learning Research (12/2022)
As allh∈Fare part of the RKHS H, their evaluation functionals are bounded, giving
sup
h∈FE[|Y−V|q·h(V)]≥sup
h∈FE[|E[Y|V]−V|q·h(V)]. (38)
Rewriting Eq. (38) as a norm in the RKHS Hcompletes the proof.
A.5 Proof of Proposition 4.8
Proof.Our proof is similar to that of Bates et al. (2020). For ease of notation, let S:=˜g(X), where by
assumption Sis continuous and samples Si,i= 1,...,ηare i.i.d. For some parameter τ∈R, let
ˆCη(τ) :=1
ηη/summationdisplay
i=11{Si≥τ},and (39)
C(τ) :=E[1{S≥τ}]. (40)
Note that ˆCη(τ)is a random variable depending on S1,...,SnwhileC(τ)is a constant.
For everyτ∈Rand∀ϵ>0, Hoeffding’s inequality gives a pointwise bound on ˆCη(τ)’s deviation from C(τ):
P(ˆCη(τ)−C(τ)≥ϵ)≤e−2nϵ2. (41)
Recall the definition of ˆτfrom Eq. 16:
ˆτ:= sup/braceleftig
τ∈R:ˆCη(τ)≥ξ/bracerightig
. (42)
AsˆCηis a random variable, ˆτis also a random variable—as is C(ˆτ), the true expected coverage at random
threshold ˆτ. LetEbe the event that C(ˆτ)≤ξ−ϵ. We will now show that this event occurs with probability at
moste−2n2. Define the constant τ∗asτ∗:=sup{τ∈R:C(τ)≥ξ−ϵ}. Cis continuous, as Sis continuous
by assumption, therefore τ∗is well-defined, and satisfies C(τ∗) =ξ−ϵ. SupposeE. Then ˆτ≥τ∗, asC(τ)is
monotonically non-increasing. This implies ˆCη(τ∗)≥ξ, asˆCη(τ)is also monotonically non-increasing.
As the event C(ˆτ)≤ξ−ϵimplies the event ˆCη(τ∗)≥ξ,
P(C(ˆτ)≤ξ−ϵ)≤P(ˆCη(τ∗)≥ξ). (43)
However, by applying Hoeffding’s inequality at τ=τ∗we also have that
P(ˆCη(τ∗)≥ξ) =P(ˆCη(τ∗)≥C(τ∗) +ϵ)≤e−2nϵ2. (44)
Combining bounds in Eqs. (44) and (43) yields P(C(ˆτ)≤ξ−ϵ)≤e−2ϵ2.
Remark A.4. Alternatively, to guarantee coverage ≥ξwith high probability for any η(instead of providing
a probabilistic bound on its error), one can also use a corrected threshold via the conformal prediction, RCPS,
or CRC algorithms (see Vovk et al., 2005; Bates et al., 2020; Angelopoulos et al., 2022, respectively).
B Technical details
B.1 Empirical calibration error
We use equal-mass binning (Kumar et al., 2019; Roelofs et al., 2022) to estimate the expected calibration error.
Given a dataset of confidence predictions, Dtest={(f(x1),y1),..., (f(xn),yn)}, we sort and assign each
22Published in Transactions on Machine Learning Research (12/2022)
prediction pair (f(xi),yi)to one ofmequally sized bins B(i.e., binBkwill contain all examples with scores
falling in thek−1
m→k
mempirical quantiles). If yis binary, the expected binary calibration error is estimated by
BCE≈
|B|/summationdisplay
k=1|B|k
|Dtest|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
i∈Bkyi
|Bk|−/summationtext
i∈Bkf(xi)
|Bk|/vextendsingle/vextendsingle/vextendsingle/vextendsingleq
1
q
. (45)
Whenyis multi-class, we replace yiandf(xi)with 1{yi=argmaxy∈[K]f(xi)y}andmaxy∈[K]f(xi)y, respec-
tively to compute the TCE. We set the number of bins mtomin(15,⌊n
25⌋). For selective calibration error, we
first filterDtestto only include the examples that have g(Xi) = 1, and then apply the same binning procedure.
B.2 Training details
We use a Laplacian kernel with width 0.2in S-MMCE, similar to previous work (Kumar et al., 2018), i.e.,
k(ri,rj) = exp/parenleftbigg−|ri−rj|
0.2/parenrightbigg
(46)
All models are trained in PyTorch (Paszke et al., 2019), while high-level input features (§4.5) for gare
computed with sklearn (Pedregosa et al., 2011). We compute the S-MMCE with samples of size 1024 (of
examples with the same perturbation t∈Tapplied), with a combined batch size of m= 32. We train all
models for 5 epochs, with 50k samples (where one “sample” is a batch Dof1024perturbed examples) per
epoch (≈7.8k updates total at the combined batch size of 32). The loss hyper-parameters λ1andλ2were set
to|D|−0.5=1/32and1e-2×|D|−1≈1e-5, respectively. The κin our top-κcalibration error loss is set to 4.
B.3 NLST-MGH lung cancer data
The CT scans in the NLST data are low-dose CT scans from patients enrolled in lung cancer screening
programs across the 33 participating hospitals. The MGH data is aggregated across a diverse assortment
of all patients in their system who underwent anytype of chest CT between 2008-2018, and matched NLST
enrollment criteria (55-74 years old, current/former smokers with 30+ pack years). Note that since these
patients are not enrolled in a regular screening program, not all patients have complete 6-year follow-up records.
All patients without documented cancer diagnosis in the 2008-2018 period are assumed to be cancer-free.
C Additional results
Figure C.1 presents results per perturbation type on ImageNet-C. Like CIFAR-10-C (Figure 3), we see
consistent and significant selective calibration error reduction across perturbation types. This is in contrast to
the confidence-based selective classifier, which does not always reliably lead to improved selective calibration
error. Related to perturbations, a question that arises is what happens if perturbations are also included
during the training of f(X)? Figure C.2 presents results using an ftrained with AugMix perturbations
over CIFAR-10, on top of which we learn our gas before (also using the same AugMix perturbation family).
While the absolute calibration error of fis lower, in line with Hendrycks et al. (2020), using the selective
gfurther reduces the selective calibration error across all coverage levels. Next, Figure C.3 presents the
results of an ablation study on the S-MMCE loss formulation, applied to CIFAR-10-C. Specifically, we look
at the effect of not including any perturbations, including perturbations on an example level only (i.e., t(x)
rather than t◦Dtrain), and using all perturbations in a batch instead of only the κ-worst. Interestingly,
even training the S-MMCE loss only on in-domain, unperturbed data can lead to moderate improvements
in selective calibration error. Including perturbations, however, leads to significantly better results. Sharing
perturbations across batches or training on the κ-worst then offer additional, albeit small, improvements. We
also perform a similar ablation study on CIFAR-10-C over the features provided to our S-MMCE MLP (§4.5)
in Figure C.4. Finally, Figure C.5 demonstrates trade-offs between selective calibration and selective accuracy.
23Published in Transactions on Machine Learning Research (12/2022)
Figure C.1: ℓ2selective top-label calibration error AUC, reported for each of the 11 test-time perturbations in
ImageNet-C (and the average). Across perturbations, optimizing for S-MMCE consistently leads to significant
error reductions relative to a model without abstentions ("Full"), as well as the standard selective classifier
("Confidence"). In contrast, the confidence-based selector can sometimes lead to worse calibration error.
Figure C.2: Results on CIFAR-10-C averaged over perturbation types when including AugMix pertubrations
(T)during theoriginaltraining of f(X). As alsofirstreportedinHendryckset al.(2020), including such pertur-
bations during training indeed reduces calibration error out-of-the-box. Using even the same perturbation class
Tduring selective training, can yield a selective classifier with substantially lower relative calibration error.
Figure C.3: Ablation on CIFAR-10-C for different configurations of our S-MMCE optimization. Without T:
no perturbations. Per- xT: perturbations are mixed within a batch. Shared T, no DRO: update on all batches,
not just the κ-worst. SharedT, with DRO: update on the κworst examples per batch (main algorithm).
24Published in Transactions on Machine Learning Research (12/2022)
Figure C.4: Ablation on CIFAR-10-C holding out sets of input features to the gMLP. Without confidence: all
confidence features are removed (top-label confidence, entropy, the full distribution). Without class info: all
features that can be used to identify the predicted class are removed (one-hot class index, the full confidence
distribution). Without outlier scores: all derived outlier/novelty scores (One-Class SVM, Isolation Forest, etc)
are removed. We see that each of these feature sets has an impact on the aggregate classifier in somewhat
complementary ways: in particular, confidence helps performance most at high coverage levels, while outlier
scores help performance most at low coverage levels.
(a) CIFAR-10-C
 (b) ImageNet-C
 (c) Lung cancer (NLST-MGH)
Figure C.5: Selective accuracy results across CIFAR-10-C, ImageNet-C, and Lung cancer (higher AUC is
better). When more calibrated examples also tend to be more accurate (e.g., if higher confidence values
are also well-calibrated), then the selective accuracy of S-MMCE can be quite high (see CIFAR-10-C and
Lung cancer). Sometimes, however, there is a tradeoff between calibration and accuracy: on ImageNet-C,
selection based on S-MMCE scores results in significantly lowerselective accuracy relative to simple confidence
thresholding, but is significantly better calibrated (see Figure 2 in §6).
25