Under review as submission to TMLR
Learning optimal policies through contact in differentiable
simulation
Anonymous authors
Paper under double-blind review
Abstract
Model-Free Reinforcement Learning (MFRL) has garnered significant attention for its
effectiveness in continuous motor control tasks. However, its limitations become apparent
in high-dimensional problems, often leading to suboptimal policies even with extensive
training data. Conversely, First-Order Model-Based Reinforcement Learning (FO-MBRL)
methods harnessing differentiable simulation offer more accurate gradients but are plagued
by instability due to exploding gradients arising from the contact approximation model. We
propose Adaptive Horizon Actor Critic (AHAC), a massively parallel FO-MBRL approach
that truncates trajectory gradients upon encountering stiff contact, resulting in more stable
and accurate gradients. We experimentally show this on a variety of simulated locomotion
tasks, where our method achieves up to 64 % higher asymptotic episodic reward than state-of-
the-art MFRL algorithms and less hyper-parameter sensitivity than prior FO-MBRL methods.
Moreover, our method scales to high-dimensional motor control tasks while maintaining
better wall-clock-time efficiency. https://adaptive-horizon-actor-critic.github.io/
1 Introduction
Reinforcement Learning (RL) has achieved remarkable success in complex tasks, such as Atari games (Mnih
et al., 2013), Minecraft (Hafner et al., 2023) and Go (Silver et al., 2017). Combined with the Policy Gradients
Theorem (Sutton et al., 1999), we can derive approaches for solving continuous motor control tasks. Although
some of these Model-Free Reinforcement Learning (MFRL) approaches have achieved impressive results
(Hwangbo et al., 2017; Akkaya et al., 2019; Hwangbo et al., 2019), they suffer from subpar sample efficiency,
limiting their practical utility (Amos et al., 2021). Consequently, addressing this limitation has been a central
research focus for the past few years.
An alternative approach, Model-Based Reinforcement Learning (MBRL), seeks to model the environment’s
dynamics for improved efficiency. However, most MBRL methods still rely on experience data to learn
dynamics and often produce suboptimal policies compared to MFRL. To enhance sample efficiency, high-
performance physics simulators can be employed, offering abundant training data in ideal scenarios. When
combined with computationally efficient MFRL methods, these simulators enable quick training of robots
for tasks such as walking (Rudin et al., 2022). However, a question remains: even with extensive data, can
MFRL effectively tackle high-dimensional motor control problems?
Given the substantial effort put into producing accurate and efficient simulators, one should naturally ask,
why don’t we use them as models for MBRL? In turn, this makes it tempting to learn the policy using
first-order methods which are theoretically more efficient (Berahas et al., 2022). This has been explored in
the domain of model-based control literature, where we differentiate the model in order to plan trajectories
for applications such as autonomous driving (Kabzan et al., 2019) and agile quadrotor maunders (Kaufmann
et al., 2020). However, using first-order methods to learn feedback policies in standard RL settings has
received limited attention. Non-differentiable contact point discontinuities in available simulation models
have been a major hurdle, leading to the development of differentiable simulators (Hu et al., 2019a; Freeman
et al., 2021; Heiden et al., 2021; Xu et al., 2021).
1Under review as submission to TMLR
Figure 1: We find that FO-MBRL methods suffer from high dynamics gradients ∥∇f(s,a)∥≫0) which often
arise from stiff contact approximation. Our proposed method, AHAC, truncates model-based trajectories at
the point of and during stiff contact, thus avoiding both the gradient bias and learning instability exhibited
by previous methods using differentiable simulation.
Where model-based control literature hand-designs bespoke models for each problem, differentiable simulation
aims to create a physics engine that is fully differentiable. Thus, applying it to a different problem is as easy
as defining the structure of the problem (e.g. joints and links) and leaving the physics to be calculated by
the engine. These simulators have enabled a new family of FO-MBRL algorithms that can efficiently learn
complex control tasks. Short Horizon Actor Critic (SHAC) (Xu et al., 2022) is such an approach that utilises
the popular actor-critic paradigm (Konda & Tsitsiklis, 1999). The actor is trained in a first-order fashion,
while the critic is trained model-free. This allows SHAC to learn through the highly non-convex landscape by
using the critic as a smooth surrogate of the cumulative reward objective and avoiding exploding gradients
by employing short-horizon rollouts. While SHAC boasts incredible sample efficiency when compared against
MFRL, it is also brittle, exhibits higher learning instability, and requires extensive hyper-parameter tuning
(Suh et al., 2022).
In this study, we attempt to address those issues and shift our focus from sample efficiency to the asymptotic
performance of FO-MBRL methods in massively parallel differentiable simulations. We aim to answer the
following questions:
1.What causes learning instability in FO-MBRL approaches such as SHAC? Our analysis reveals that
first-order methods exhibit high empirical bias when estimating gradients through sample-limited
Monte-Carlo approximation, hindering efficiency and resulting in suboptimal policies. This bias is
primarily driven by the high magnitude dynamical gradients ( ∥∇f(s,a)∥≫ 0) arising from stiff
contact approximation.
2.Can FO-MBRL methods outperform MFRL in finding optimal policies? We introduce Adaptive
Horizon Actor Critic (AHAC), a first-order model-based algorithm that mitigates gradient issues
during stiff contact by adapting its trajectory rollout horizon (Figure 1). Experimentally, we show
that it is capable of achieving up to 64% more asymptotic reward in comparison to model-free
approaches across complex locomotion tasks.
3.Which methods are suitable for scaling to high-dimensional motor control tasks? We find that AHAC
exhibits lower variance during training, offering stability and gradient accuracy, allowing it to scale
effectively to high-dimensional motor control tasks with action dimension A=R152.
2 Preliminaries
In this paper, we study discrete-time, finite-horizon, fully-observable reinforcement learning problems where
the state of the system is defined as s∈Rn, actions are defined as a∈Rm, and the dynamics are governed
by the function f:Rn×Rm→Rn. Unlike traditional RL formulations, here we assume that the dynamics
(i.e., transition function) are deterministic. At each timestep t, we sample an action from a stochastic policy
2Under review as submission to TMLR
at∼πθ(·|st), which is parameterised by some parameters θ∈Rd, and in turn, we receive a reward from
r:Rn×Rm→R. We can define the H-step return as:
RH(s1,θ) =H/summationdisplay
h=1r(sh,ah)s.t.sh+1=f(sh,ah)ah∼πθ(·|sh)
As is typical in RL, the objective of the policy is to maximise the cumulative reward:
max
θJ(θ) := max
θEs1∼ρ
ah∼π(·|sh)[RH(s1,θ)] (1)
whereρis the initial state distribution. Without loss of generality, we make our work easier to follow by
making the following assumption:
Assumption 2.1. We assume that ρis a dirac-delta distribution.
Similar to prior work Duchi et al. (2012); Berahas et al. (2022); Suh et al. (2022), we are trying to exploit the
smoothing properties of stochastic optimisation on the landscape of our optimisation objective. Following
recent successful deep-learning approaches to MFRL (Schulman et al., 2017; Haarnoja et al., 2018), we
assume:
Assumption 2.2. We assume that our policy is stochastic one parameterised by θand expressed as πθ(·|s).
In order to solve our main optimisation problem in Equation 1, we consider using stochastic gradient estimates
ofJ(θ). These can be obtained via zero-order and first-order methods. To guarantee the existence of ∇J(θ),
we need to make certain assumptions:
Definition 2.3. A function g:Rd→Rdhaspolynomial growth if there exists constants a,bsuch that
∀z∈Rd,||g(z)||≤a(1 +||z||b).
Assumption 2.4. To ensure gradients are well defined, we assume that the policy πθ(·|s)is continuously
differentiable∀s∈Rn,∀θ∈Rd. Furthermore, the system dynamics fand reward rhave polynomial growth.
2.1 Zeroth-Order Batch Gradient (ZOBG) estimates
These weak assumptions are sufficient to make J(θ)differentiable in expectation by simply taking samples of
the function value in a zeroth-order fashion. This gives us estimates of ∇J(θ)via the stochasticity introduced
byπ, as first shown in (Williams, 1992), and commonly referred to as as the Policy Gradient Theorem (Sutton
et al., 1999).
Definition 2.5. Given a sample of the H-step return RH(s1) =/summationtextH
h=1r(sh,ah)following the policy π, we
can estimate zero-order policy gradients via:
∇[0]
θJ(θ) :=Eah∼πθ(·|sh)/bracketleftigg
RH(s1)H/summationdisplay
h=1∇θlogπθ(ah|sh)/bracketrightigg
(2)
Lemma 2.6. Under Assumptions 2.1 and 2.4, the ZOBG is an unbiased estimator of the stochastic objective
E/bracketleftbig¯∇[0]J(θ)/bracketrightbig
=∇J(θ)where ¯∇[0]J(θ)is the sample mean of NMonte Carlo estimates of Eq. 2.
These zero-order policy gradients are known to have high variance, and one way to reduce their variance is by
subtracting a baseline from the function estimates. Similar to (Suh et al., 2022), we do that by subtracting
the return given by the noise-free policy rollout:
∇[0]
θJ(θ) =Eah∼πθ(·|sh)/bracketleftigg/parenleftbigg
RH(s1)−R∗
H(s1)/parenrightbiggH/summationdisplay
h=1∇θlogπθ(ah|sh)/bracketrightigg
R∗
H(s1) :=H/summationdisplay
h=1r(sh,E[πθ(·|sh)])
2.2 First-Order Batch Gradient (FOBG) estimates
Given access to a differentiable simulator (with contact approximation), one can directly compute the analytic
gradients of∇θRH(s1)induced by the policy π:
∇[1]
θJ(θ) :=Eah∼πθ(·|sh)[∇θRH(s1)] (3)
3Under review as submission to TMLR
However, for these gradients to be well-defined, we need to make further assumptions:
Assumption 2.7. The system dynamics f(s,a)and the reward r(s,a)are continuously differentiable
∀s∈Rn,∀a∈Rm.
3 Learning through contact
1.0
 0.5
 0.0 0.5 1.0
1.0
0.5
0.00.51.0
H()
[H(a)]
Figure 2: Soft Heavside of Eq 4.
0 200 400 600 800 1000
Number of samples (N)0.00.51.01.52.02.5Gradient bias[1]J()
[0]J()
Figure 3: Empirical bias of ZOBG
and FOBG when approximating the
gradients of the Heaviside function.First-order gradients, as demonstrated in prior research, are asymp-
totically unbiased when N→∞(Schulman et al., 2015). However,
this ideal scenario is often impractical in real-world applications,
leading to observed empirical bias, as indicated by (Suh et al., 2022).
To illustrate this bias, we use the soft Heaviside function, a common
tool for studying discontinuous functions in physics simulations as
it is an approximation of the Coulomb friction model:
¯H(x) =

1x>ν/ 2
2x/ν|x|≤ν/2
−1x<−ν/2(4)
wherea∼πθ(·) =θ+N(0,σ2). As shown in Appendix A, Eπ/bracketleftbig¯H(a)/bracketrightbig
isasumoferrorfunctionswhosederivative ∇θEπ/bracketleftbig¯H(a)/bracketrightbig
̸= 0atθ= 0.
However, using FOBG, we obtain ∇θ¯H(a) = 0in samples where
|a|>ν/ 2, which occurs with probability at least ν/σ√
2π. Since in
practice we are limited in sample size, this translates to empirical
bias that is inversely proportional to sample size, as shown in Figure
3. Notably, when ν→0, we achieve a more accurate approximation
of the underlying discontinuous function, but we also increase the
likelihood of obtaining incorrect FOBG, thus amplifying bias in
stochastic scenarios. We used this particular example to showcase
empirical bias as our differentiable simulator used in Section 5 is
based on the Coulomb friction model.
By analysing the empirical bias from the perspective of bias and variance, we derive a practical upper bound:
Lemma 3.1. For an H-step stochastic optimisation problem under Assumptions 2.7, which also has Lipshitz-
smooth policies||∇πθ(a|s)||≤Bπand Lipshitz-smooth and bounded rewards r(s,a)≤||∇r(s,a)||≤Br
∀s∈Rn;a∈Rm;θ∈Rd, then zero-order estimates remain unbiased. However, first-order gradient exhibit
bias which is bounded by:
/vextenddouble/vextenddouble/vextenddoubleE/bracketleftig
∇[1]
θJ(θ)/bracketrightig
−E/bracketleftig
∇[0]
θJ(θ)/bracketrightig/vextenddouble/vextenddouble/vextenddouble≤H4B2
rB2
πEa∼π/bracketleftiggH/productdisplay
t=1∥∇f(st,at)∥2/bracketrightigg
(5)
The proof can be found in Appendix B
We can ensure that the rewards are designed to meet the condition r(s,a)≤∥∇r(s,a)∥≤Br. The
assumption over the policy ||∇πθ(·|s)||≤Bπis less straightforward if we are using high-capacity models such
as neural networks, but can be tackled via gradient normalisation techniques. However, giving any bounds
over the dynamics ||∇f(st,at)||is difficult, yet influential in Equation 5. The Lemma also suggests that
longer horizons results in exponential growth in bias.
To investigate the implications of Lemma 3.1, we constructed a simple scenario involving a ball rebounding
off a wall and aiming to reach a target location, as illustrated in Figure 5. In this setup, the initial position
s1= [x1,y1]and velocity of the ball are fixed. The objective is for the policy to learn the optimal initial
orientation θin order to reach a target position sTat the end, defined as RH(s1) =∥sH−sT∥−1
2. Similar to
before, we use an additive Gaussian policy a=θ+wwherew∼N(0,σ2). Alternatively, a∼πθ(·) =N(θ,σ2)
4Under review as submission to TMLR
0 10 20 30 40
Horizon (H)050100150200250First-order gradient bias
(a) FOBG empirical bias
0 10 20 30 40
Horizon (H)105
103
101
101103105Gradient varianceZOBG
FOBG (b) Gradient variance
0 50 100 150 200
Iteration0.050.100.150.200.25Episode rewardZOBG
FOBG
FOBG clipped (c) Optimising to reach target
Figure 4: Results from the toy ball problem . Fig (a) shows the empirical bias of FOBG measures by
comparing it to ZOBG. Fig (b) shows how the variance of the two gradient types evolves over time. Both
Figs (a) and (b) use different shades to show different stiffness configurations of the simulation with darker
shades designating stiffer (and more realistic) simulation. Fig (c) shows attempts at optimising the initial
angle of the ball to hit the target.
With this, zero-order gradients from Equation 2 can be expressed as:
∇[0]
θJ(θ) =Eah∼π(θ)/bracketleftbig/parenleftbig
RH(s1)−R∗
H(s1)/parenrightbig
∇logπθ(a)/bracketrightbig
≈1
Nσ2/parenleftbig
RH(s1)−R∗
H(s1)/parenrightbig
w
Figure 5: The toy problem where the ball
is shot against a wall to reach the blue box.We collect N= 1024samples of each gradient type for each
timestep with H= 40, starting from a randomly sampled
starting angle θ∼U(−π,π)for each environment. Figure 4a
shows how the empirical bias of FOBG grows as Hincreases,
validating the proposed lemma. The bias remains low until
the ball encounters contact, at which point it starts growing
exponentially. We also examined the variance of the gradients
in Figure 4b, observing that ZOBG follow Var/bracketleftbig
∇[0]J(θ)/bracketrightbig
≤
HB2
rB2
π
σ2first proposed by (Suh et al., 2022) - most importantly,
they scale linearly with H. However, FOBG variance behaves
similarly to the empirical bias bound described in Lemma 3.1.
exhibiting lower variance at the beginning of the rollout but growing exponentially. This is due mostly to
the stiff dynamics ||∇f||≫ 0, which can be clearly seen at timestep h= 6of Figure 4b where the ball first
makes contact with the ground. Towards the end of the rollout, FOBG’s variance can be up to five orders of
magnitude higher than ZOBG, which remains unaffected by stiff dynamics. Both the bias and variance issues
become even more pronounced as the contact stiffness increases, indicated by the darker shades in the figures.
This high empirical bias and the resulting high variance have significant consequences for optimisation and
policy learning. We find that the biased FOBG fail to find a solution, as shown in Figure 4c. In contrast, the
unbiased ZOBG have lower variance and slowly make progress towards a solution. This situation leads to a
critical question: Is it possible to leverage the efficiency of FOBG in the presence of high bias gradients?
Inspired by a common practice in deep learning, we attempt to normalise gradient norms of the dynamics.
Employing this approach in Figure 4c shows that FOBG are now able to converge to a solution at a much
faster rate than ZOBG.
˜∇shf(sh,ah) =gc(∇shf(sh,ah))∀h∈[0,H]
˜∇ahf(sh,ah) =gc(∇ahf(sh,ah))∀h∈[0,H]gc(x) :=

x
||x||2if||x||2>1
xotherwise
5Under review as submission to TMLR
4 Adaptive Horizon Actor Critic (AHAC)
4.1 Learning through contact in a single environment
s1 s2 s3
a1 a2 a3
r1 r2 r3
Cut gradients gradient flow
Figure 6: H=3 trajectory with
truncated gradients . The green
arrows indicate back-propagated
gradients for AHAC-1 .With a clearer understanding of stiff contact in differentiable simulators,
weaimtodevelopamodel-basedalgorithmemployingFOBGforeffective
learning in infinite-horizon robotics tasks. Although we can apply the
gradient clipping technique as above, in a multi-step problem, we can
avoid the stiff gradients altogether. Consider a scenario where we have
a 3-step trajectory ( H= 3), and contact occurs at timestep h= 3, as
shown in Figure 6. If we took the gradient normalisation approach, the
gradientsfor r(s3,a3)withaθ-parameterisedpolicywhere ah∼πθ(·|sh)
with respect to θ:
∇θr(s3,a3) =∇a3r(s3,a3)∇θπθ(a3|s3)
+∇s3r(s3,a3)˜∇a2f(s2,a2)∇θπθ(a2|s2)
+∇s3r(s3,a3)˜∇s2f(s2,a2)˜∇a1f(s1,a1)∇θπθ(a1|s1)
By definition of gc(x)we will be losing gradient information. An alternative would be to cut gradients at
h= 2. This would render ∇θr(s3,a3) = 0, preventing us from learning in a scenario such as our toy example
in Section 3. However, in a multi-step decision-making problem, r(s3,a3)still yields gradients:
∇θ/bracketleftbigg3/summationdisplay
h=1r(sh,ah)/bracketrightbigg
=∇a3r(s3,a3)∇θπθ(a3|s3)
+∇a2r(s2,a2)∇θπθ(a2|s2) +∇s2r(s2,a2)∇a1f(s1,a1)∇θπθ(a1|s1)
+∇a1r(s1,a1)∇θπθ(a1|s1)
Note how the gradients of dynamics in contact do not appear above. We call this technique contact truncation .
We present an FO-MBRL algorithm with an actor-critic architecture similar to SHAC (Xu et al., 2022). The
critic, denoted as Vψ(s), is model-free and trained using TD( λ) over anH-step horizon from timestep t:
Rh(st) :=t+h−1/summationdisplay
n=tγn−tr(sn,an) +γt+hVψ(st+h) ˆV(st) := (1−λ)/bracketleftbiggH−t−1/summationdisplay
h=1λh−1Rh(st)/bracketrightbigg
+λH−t−1RH(st)
The critic loss becomes LV(ψ), while the actor is trained using FOBG as in Equation 3, with the addition of
the critic value estimate:
LV(ψ) :=t+H/summationdisplay
h=t/vextenddouble/vextenddouble/vextenddoubleVψ(sh)−ˆV(sh)/vextenddouble/vextenddouble/vextenddouble2
2(6)J(θ) :=t+H−1/summationdisplay
h=tγh−tr(sh,ah) +γtVψ(st+T)(7)
Unlike fixed-horizon model-based rollouts in (Xu et al., 2022), our policy is rolled out until stiff contact is
encountered, which can be determined in simulation. This results in a dynamic FO-MBRL algorithm that
adjusts its horizon to avoid exploding gradients. However, not all contact results in high bias; therefore,
we want to truncate only on stiff contact ∥∇f(st,at)∥>C, whereCis the contact stiffness parameter we
set. We refer to this algorithm as Adaptive Horizon Actor Critic 1 (AHAC-1), which is designed for single
environments but not suitable for vectorised environments (see Appendix D).
Using this approach, we can investigate if truncating gradients on contact yields better policy than naively
cutting on fixed short-horizons. We compare SHAC and AHAC-1 on a simple contact-rich locomotion task,
Hopper, a single-legged agent that obtains a reward for forward velocity (Figure 1). Both algorithms share
the same hyperparameters, except for the ones related to horizons. SHAC uses a fixed H= 32, while AHAC-1
uses a maximum horizon of H= 64with a contact threshold of C= 500. From Figure 7, we observe
that AHAC-1 achieves a higher reward while exhibiting lower variance. Although difficult to analyse, we
believe that our approach avoids local minima by adapting its horizon to avoid stiff gradients. On the other
hand, SHAC gets pushed into local minima, which eventually results in policy collapse as seen in Figure
7. Unfortunately, AHAC-1 cannot be applied to parallel vectorised environments due to the challenge of
asynchronously truncating trajectories, leading to infinitely long compute graphs.
6Under review as submission to TMLR
0 100 k 200 k 300 k
Simulation steps01 k2 k3 k4 kEpisode rewardSHAC
AHAC
0 100 k 200 k 300 k
Simulation steps102030H
SHAC
AHAC
Figure 7: Comparison between SHAC and AHAC-1 ran on the Hopper task with only a single
environment . The left plot shows rewards achieved by the algorithms over five different random seeds,
with the mean and std. dev. plotted. The right plot is the moving window averaged mean horizon of
both approaches. Note that even though SHAC has fixed horizons, they can still vary if the environment is
terminated early due to early termination or episode end.
4.2 AHAC: A scalable approach for learning through contact
2022242628303234363840
H1618202224262830Gait period (timesteps)
02 k4 k6 k8 k
Episode reward
Figure 8: An ablation of short horizons
Hfor the SHAC algorithm applies to Ant.
Each run is trained until convergence for 5
random seeds.A straightforward solution to asynchronous truncation in
AHAC-1 might involve adopting the short-horizon approach of
SHAC and truncating the graph on stiff contact. Unfortunately,
this did not yield any performance improvements. Instead, we
investigate the impact of the horizon length on policy optimal-
ity. We find that contact-based tasks have an inherent optimal
solution, for instance, in a locomotion task, in the form of an
optimal gait pattern. We observe that an SHAC agent converges
to a particular solution, often suboptimal, depending on the
horizon parameter H. Importantly, we find that asymptotic
performance is maximised when the horizon Hmatches the
optimal (though unknown) gait frequency.
To empirically demonstrate this, we conducted an experiment
by parameterizing SHAC with different Hvalues for Ant lo-
comotion tasks, where a quadruped robot seeks to maximise
forward velocity rewards. As seen from the results in Figure
8, the gait period aligns with the horizon length until H= 28,
after which it attempts to fit two gait patterns within a single H-timestep rollout. Moreover, we noticed
that the asymptotic reward reaches its peak as the horizon-length Happroaches what we believe to be the
optimal gait period and displays the least variance across runs.
From these observations, we glean two insights: (1) each task has an optimal model-based horizon length H
that corresponds to the gait period, and (2) the associated optimal horizon results in the lowest variance
between runs, supported by Lemma 3.1. We leverage these insights to generalise the AHAC-1 algorithm into
a GPU-parallelisable approach, which we call AHAC. The critic training formulation remains the same as in
Equation 6, but we introduce a new constrained objective for the actor:
J(θ) :=t+H−1/summationdisplay
h=tγh−tr(sh,ah) +γtVψ(st+H)s.t.∥∇f(st,at)∥≤C∀t∈{0,..,H} (8)
In simple terms, this objective seeks to maximise the reward while ensuring that all contact stiffness remains
below a predefined threshold. Building on the inspiration from AHAC-1, we can progressively increase the
7Under review as submission to TMLR
horizon as long as the constraint is satisfied. Using the Lagrangian formulation, we derive the dual problem:
Lπ(θ,ϕ) =t+H−1/summationdisplay
h=tγh−tr(sh,ah) +γtVψ(st+H) +ϕT

∥∇f(st,at)∥
...
∥∇f(st+H,at+H)∥
−C
 (9)
Algorithm 1: Adaptive Horizon Actor-Critic
Given:γ: discount rate
Given:α: learning rate
Given:C: contact threshold
Initialise learnable parameters θ,ψ,H,ϕ=0
t←0
while episode not done do
/* rollout policy */
Initialise buffer D
forh= 0,1,..,Hdo
at+h∼πθ(·|st+h)
st+h+1=f(st+h,at+h)
D←D∪{(st+h,at+h,rt+h,Vψ(st+h+1))}
/* train actor with Eq. 9 */
θ←θ+α∇θLπ(θ,ϕ)
ϕ←ϕ+α∇ϕLπ(θ,ϕ)
H←H−α/summationtextH
t=0ϕt
/* train critic with Eq. 6 */
while not converged do
sample (s,ˆV(s))∼D
ψ←ψ−α∇ψLV(ψ)
t←t+HBy definition, ϕi= 0if the constraint is met and
ϕi>0otherwise. Thus, we can use ϕto adapt
the horizon, resulting in the full AHAC algorithm
shown in Algorithm 1. We train the critic until
convergence is defined as a sufficiently small change
in the last 5 critic training iterations,/summationtextn
i=n−5L(ψ)
where we take mini-batch samples from the buffer
(s,ˆV(s))∼D.
In practice, we find that truncating on ∇f(st,at)is
limiting since different tasks involve varying contact
forces, which often change throughout the learn-
ing process. Instead, we normalise the contact
forces with modified acceleration per state dimen-
sion ˆqt=max(qt,1)where the max is applied
element-wise, resulting in normalised contact forces
ˆ∇f(st,at) = diag(ˆqt)∇f(st,at). This allows us
to use a single Cparameter across different tasks.
Additionally, as contact approximation forces are
computed separately in differentiable simulators, we
don’t need to utilise the full Jacobian of the dynam-
ics. Instead, we can use the Jacobian derived only
from contact. The differences between SHAC and
AHAC are summarised in Appendix C.
5 Experiments
In this section, we aim to address the following key questions experimentally:
1.Can first-order model-based (FO-MBRL) policies outperform zeroth-order model-free (ZO-MBRL)
policies concerning asymptotoic episodic reward?
2. Does AHAC remain sample and wall-clock time efficient similar to prior FO-MBRL algorithms?
3. Does AHAC scale to high-dimensional environments?
4. Which components of AHAC contribute to its improved asymptotic reward compared to SHAC?
Previous FO-MBRL algorithms utilising differentiable simulators, such as SHAC (Xu et al., 2022), face
challenges related to instability arising from stiff contact, which empirically results in worse asymptotic
performance. The previous section and Figure 7 specifically suggest that the single-environment version of
AHAC manages to avoid local minima and continue to progressively obtain a higher performance policy.
We now investigate whether these benefits persist when scaling AHAC to N= 512parallel environments
and applying it to a more complex task: Ant, a quadruped with symmetrical legs, S=R37andA=R8.
The simulator used throughout this section is dflex, introduced by (Xu et al., 2022) and described in more
detail in Appendix E. We compare AHAC to SHAC, its predecessor; PPO, a state-of-the-art on-policy MFRL
algorithm (Schulman et al., 2017); SAC, an off-policy MFRL algorithm (Haarnoja et al., 2018); and SVG,
a FO-MBRL method that does not utilise a differentiable simulator but instead learns its model of the
dynamics1. A more explicit comparison of all of these approaches and more can be found in Section 6. We
1To make performance comparable, we attempted to vectorise SVG and found that it did not scale well with an increased
number of parallel environments. Therefore, the results presented in this paper are from the original single-environment version.
8Under review as submission to TMLR
0 10 M 20 M 30 M
Simulation steps0.00.51.01.5Episode reward
3 B
PPO SAC SVG SHAC AHAC0 10 20 30 40
Time (min)400
Figure 9: Episodic rewards of the Ant task against both simulation steps and wall clock time .
The episodic reward is normalised by the highest mean reward achieved by PPO (i.e. PPO-normalised). The
dashed lines represent the reward achieved by each respective algorithm at the end of their training runs.
tune and train all algorithms on the Ant task until convergence. Furthermore, we also train the MFRL
baselines for 3B timesteps to investigate their asymptotic performance given practically infinite data. To best
account for statistical errors given our limited runs, we employ the robust metrics suggested by (Agarwal
et al., 2021). All results presented in this section show the 50% IQM and the 95% CI across 10 runs.
Figure 9 provides insights into the asymptotic performance. It shows that AHAC achieves a 41% higher
reward than the closest model-free baseline, PPO, and outperforms SHAC with a smaller standard deviation
between runs. Remarkably, the MFRL algorithms PPO and SAC get worse episodic rewards over time
compared to AHAC, even when they are trained for 3B timesteps.
To answer questions (2) and (3) regarding efficiency and scalability, we conducted experiments across a variety
of locomotion tasks. We again compare AHAC against SHAC, PPO, SAC, and SVG. We tune our model-free
baselines sufficiently per-task to maximise performance. Due to long training times, SVG utilises similar
hyper-parameters as in the original work (Amos et al., 2021). SHAC is tuned per-task but uses H= 32across
all tasks (Xu et al., 2022). AHAC uses the same shared hyper-parameters as SHAC and only has a tuned
horizon learning rate of αψper task. The full hyper-parameter details can be found in Appendix F. The
following tasks share the same basic reward of maximising forward velocity with action penalties:
1.Hopper , a single-legged robot jumping only in one axis with S=R11andA=R3.
2.Anymal , a sophisticated quadruped with S=R49andA=R12modelled after (Hutter et al., 2016).
3.Humanoid , a classic contact-rich environment with S=R76andA=R21which requires extensive
exploration to find a good policy.
4.SNU Humanoid , a version of Humanoid lower body where instead of joint torque control, the
robot is controlled via A=R152muscles intended to challenge the scaling capabilities of algorithms.
Figure 10: Locomotion environments (left to right): Hopper, Ant, Anymal, Humanoid and SNU Humanoid.
9Under review as submission to TMLR
0 5 M 10 M
Simulation steps0.000.250.500.751.00Episode rewardHopper
0 5 M 10 M
Simulation steps0.00.51.01.5Anymal
0 2.5 M 5 M 7.5 M
Simulation steps0.00.51.0Humanoid
0 2.5 M 5 M 7.5 M
Simulation steps0.00.51.01.5SNU Humanoid
0.0 2.5 5.0 7.5
Time (min)0.000.250.500.751.00Episode reward
0 50 100 150
Time (min)0.00.51.01.5
PPO SAC SVG (10M timesteps) SHAC AHAC0 50 100 150
Time (min)0.00.51.0
0 50 100 150 200
Time (min)0.00.51.01.5
Figure 11: Reward curves for all tasks against both simulation steps and training time . The
reward axis are normalised by the highest mean reward achieved by PPO. We further apply exponentially
weighted smoothing with α= 0.98to increase legibility.
We compare the approaches against the number of simulation steps but also acknowledge that MFRL methods
are computationally simpler and thus also provide results against wall-clock time. From the results in
Figure 11, we observe that AHAC significantly outperforms the other methods while maintaining the sample
efficiency of SHAC. On the simpler Hopper task, all algorithms achieve similar performance when compared
against wall-clock time. However, with more complex tasks, we observe that AHAC not only learns at a
faster rate but is also capable of achieving higher asymptotic performance across all tasks. This gap only
becomes larger as we turn to more high-dimensional environments, showcasing the remarkable scalability of
our approach, where AHAC achieves a 64% higher asymptotic reward than PPO on the SNU Humanoid
task. Figure 12 shows aggregate summary statistics across all tasks, where AHAC outperforms all baselines
but exhibits a larger confidence interval. Regardless, even at the tail end of the worst runs, AHAC achieves
higher asymptotic rewards than our baselines, as seen in the score distributions. We also include tabular
results in Appendix G.
0.8 1.0 1.2 1.4
Episode rewardPPOSACSVGSHACAHAC
0.5 1.0 1.5 2.0
PPO Normalized Score ()
0.000.250.500.751.00Fraction of runs
with score >
Figure 12: Aggregate statistics comparing AHAC and our chosen baselines across all tasks.
The left figure shows 50% IQM of PPO-normalised reward with 95% CI. The right figure shows the score
distributions of the algorithms as recommended in (Agarwal et al., 2021), which allows us to gauge the
performance variability across tasks.
10Under review as submission to TMLR
Ablation study. To understand the factors contribut-
ing to the performance demonstrated by AHAC, we in-
vestigate its key modifications, as detailed in Appendix
C. Starting from our tuned baseline SHAC with H= 32,
we add only one change per ablation:
1. SHAC H=29: using the Hconverged by AHAC.
2. Adapt. Obj.: SHAC with Eq. 9 and fixed H= 32.
3.Adapt. Horizon: SHAC with Eq. 9 and adapting H.
4. Iterative critic: SHAC with iterative critic training.
5. Dual critic: SHAC with a dual critic.
1.0 1.2 1.4
Episode rewardSHAC H=321. SHAC H=292. Adapt. Obj.3. Adapt. Horizon4. Iterative Critic5. Dual CriticAHACActor CriticFigure 13: Ablations of AHAC on the Ant task.
All of the experiments use the same hyper-parameters tuned for SHAC, except for the horizon learning rate
αϕ, which was tuned for AHAC. It is worth noting that SHAC with adaptive horizon (3) is equivalent to
AHAC with single critic and no critic training until convergence. The outcomes presented in Figure 13
elucidate that the adaptive horizon objective notably enhances asymptotic reward. Rather surprisingly, SHAC
withH= 29achieves a higher reward than the baseline but fails to match the performance of the adaptive
horizon mechanism. Similar to Section 4.1, we hypothesise that even when SHAC is using the converged
optimalH= 29, it is still prone to getting stuck in local minima as the horizon might not be optimal during
training. Simultaneously, the dual critic strategy substantially mitigates variance between runs compared to
the target-critic employed in SHAC. Further ablation details and figures are provided in Appendix H.
6 Related work
In this section, we provide an overview of recent continuous control reinforcement learning (RL) methods,
all of which follow the actor-critic paradigm (Konda & Tsitsiklis, 1999). The critic estimates the value
of state-action pairs Q(s,a), and the actor learns optimal actions via maxaQ(s,a). While this section is
intended as a review of related work, we also attempt to classify methods by their method of policy (actor)
training, value estimator (critic) training, and their dynamics model f(s,a).
When no dynamics model is assumed, we are restricted to Model-Free Reinforcement Learning (MFRL)
methods. We can take Monte-Carlo samples of the Policy Gradients Theorem to find ∇θJ(θ)using Equation
2. This allows MFRL methods to learn a feedback policy that predicts the distribution of actions given the
state. On-policy methods, like Proximal Policy Optimisation (PPO) (Schulman et al., 2017), learn using only
the most recent samples following the policy. In contrast, off-policy approaches, such as Soft Actor-Critic
(SAC) (Haarnoja et al., 2018), can use all previously collected data at the expense of memory requirements.
Alternatively, Model-Based Reinforcement Learning (MBRL) methods aim to leverage a model for learning.
This model can be learned from experience data or assumed a priori. In a basic scenario, it serves as an
additional source of return estimates for the critic, which can still be trained in a model-free manner (Janner
et al., 2019). Alternatively, the model can be used to obtain simulated returns for the critic, which can
be first-order back-propagated through, known as Model-based Value Expansion (MVE) (Feinberg et al.,
2018). Actor training is more intricate in this context. It can be done using Policy Gradients augmented
by model-generated data (Janner et al., 2019) or as part of a gradient-free planning actor (Hafner et al.,
2019b). This family of approaches is termed Zeroth-Order MBRL (ZO-MBRL). Alternatively, the returns of
trajectories can be used to backpropagate through the model (Hafner et al., 2019a; Byravan et al., 2020), and
we refer to these methods as First-Order MBRL (FO-MBRL). Key recent work is summarised in Table 1.
Recent interest in differentiable simulation has given rise to several works that employ FOBG to optimise
objectives by back-propagating through the dynamics model (Hu et al., 2019b; Liang et al., 2019; Huang
et al., 2021; Du et al., 2021). Although not explicitly addressing RL, these works follow the idea of rolling out
a trajectory under a policy and iteratively optimising it until convergence. This approach can be reformulated
as a FO-MBRL algorithm, referred to as Back-Propagation-Through-Time (BPTT).
When employed for typical long episodic RL tasks, BPTT performs poorly due to a noisy optimisation
landscape and exploding gradients. (Xu et al., 2022) proposes Short Horizon Actor-Critic (SHAC) to address
11Under review as submission to TMLR
Algorithm Policy Learning Value Learning Dynamics Model
PPO (Schulman et al., 2017) Zeroth-order Model-free -
SAC (Haarnoja et al., 2018) Zeroth-order Model-free -
MBPO (Janner et al., 2019) Zeroth-order Model-free Ensemble NN
PlaNet (Hafner et al., 2019b) Gradient-free - Probabilistic NN
MVE (Feinberg et al., 2018) Zeroth-order Model-based Deterministic NN
STEVE (Buckman et al., 2018) Zeroth-order Model-based Probabilistic NN
Dreamer (Hafner et al., 2019a) First-order Model-based Probabilistic NN
IVG (Byravan et al., 2020) First-order Model-free Deterministic NN
SAC-SVG (Amos et al., 2021) First-order Model-free Deterministic NN
BPTT First-order - Differentiable sim.
SHAC (Xu et al., 2022) First-order Model-free Differentiable sim.
AHAC (this paper) First-order Model-free Differentiable sim.MFRL
ZO-MBRL
FO-MBRL
Table 1: Comparison between recent influential RL algorithms for continuous control. We classify these
approaches into the MFRL, ZO-MBRL and FO-MBRL categories predominantly by the way policy(actor) is
learned. Zeroth-order (policy gradient) methods are harnessing the gradient estimates following Equation
2 without the need of taking dynamics model gradients, while First-order methods differentiate the whole
trajectory following Equation 3. Model-based Value Learning refers to methods that fall under the Model-
based Value Expansion (MVE) approach (Feinberg et al., 2018).
these issues by (1) introducing a model-free critic that acts as a smooth surrogate of the reward-maximisation
objective and (2) by employing short rollouts to avoid high and unstable policy gradients. When run in a
massively parallel fashion, SHAC stands out as one of the few MBRL approaches that achieves comparable
asymptotic performance to MFRL methods while also demonstrating significantly better sample efficiency.
7 Conclusion
Our study aimed to compare the asymptotic performance of conventional Zeroth-order Model-Free RL
(MFRL) methods with First-Order Model-Based (FO-MBRL) methods in differentiable simulators. We
assessed the difference between both types of gradients and derived Lemma 3.1 showing that first-order
batch gradient (FOBG) empirical bias is upper-bounded by the stiffness of the dynamics. Unfortunately,
contact-rich tasks exhibit such properties, translating to FOBG with high bias and unstable learning.
Weexploredthisissueinatoyproblemandthenintroducedanalgorithmdesignedtomitigatetheaccumulation
of gradient bias stemming from stiff dynamics by truncating trajectories upon contact. When applied to high-
dimensional locomotion tasks, our proposed approach, Adaptive Horizon Actor-Critic (AHAC), achieved up
to a 64% increase in asymptotic episodic rewards compared to state-of-the-art MFRL methods. Surprisingly,
we found that even with near-infinite data, MFRL methods cannot solve tasks with similar asymptotic reward
to our proposed method. Additionally, AHAC retained the advantages commonly observed in FO-MBRL
approaches, including exceptional sample efficiency and scalability to higher-dimensional tasks. Notably, our
approach demonstrated the ability to learn complex locomotion policies for a quadruped robot in as little as
10 minutes on a single GPU, paving the way for RL scalability.
While AHAC outperforms MFRL methods in asymptotic rewards, it necessitates the development of differen-
tiable simulators, requiring substantial engineering effort. Thus, we cannot help but admire the simple yet
capable capable model-free algorithms such as PPO. Despite this, the performance of our proposed method
renders it promising for robotic applications. However, it is essential to acknowledge the sim2real gap, which
requires further exploration. Our vision for the next phase involves applying FO-MBRL approaches to real
robots in a closed-loop manner where simulations aid policy learning but continually adapt to match the real
environment. Furthermore, we believe that our proposed approach, AHAC, still has room for improvement,
in particular by truncating each parallel environment independently, instead of learning a uniform horizon.
12Under review as submission to TMLR
References
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep
reinforcement learning at the edge of the statistical precipice. Advances in neural information processing
systems, 34:29304–29320, 2021.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex
Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a robot hand.
arXiv preprint arXiv:1910.07113 , 2019.
Brandon Amos, Samuel Stanton, Denis Yarats, and Andrew Gordon Wilson. On the model-based stochastic
value gradient for continuous reinforcement learning. In Learning for Dynamics and Control , pp. 6–20.
PMLR, 2021.
Albert S Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. A theoretical and empirical
comparison of gradient approximations in derivative-free optimization. Foundations of Computational
Mathematics , 22(2):507–560, 2022.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-efficient
reinforcement learning with stochastic ensemble value expansion. Advances in neural information processing
systems, 31, 2018.
Arunkumar Byravan, Jost Tobias Springenberg, Abbas Abdolmaleki, Roland Hafner, Michael Neunert,
Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller. Imagined value gradients: Model-based
policy optimization with tranferable latent dynamics models. In Conference on Robot Learning , pp. 566–589.
PMLR, 2020.
Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew Spielberg, Daniela Rus, and Wojciech Matusik.
Diffpd: Differentiable projective dynamics. ACM Transactions on Graphics (TOG) , 41(2):1–21, 2021.
John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimization.
SIAM Journal on Optimization , 22(2):674–701, 2012.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine. Model-
based value expansionfor efficient model-free reinforcement learning. In Proceedings of the 35th International
Conference on Machine Learning (ICML 2018) , 2018.
C Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax–a
differentiable physics engine for large scale rigid body simulation. arXiv preprint arXiv:2106.13281 , 2021.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,
Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv
preprint arXiv:1812.05905 , 2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603 , 2019a.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson.
Learning latent dynamics for planning from pixels. In International conference on machine learning , pp.
2555–2565. PMLR, 2019b.
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through
world models. arXiv preprint arXiv:2301.04104 , 2023.
Eric Heiden, Miles Macklin, Yashraj S Narang, Dieter Fox, Animesh Garg, and Fabio Ramos. DiSECt: A
Differentiable Simulation Engine for Autonomous Robotic Cutting. Robotics: Science and Systems , 2021.
Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Frédo Durand.
Difftaichi: Differentiable programming for physical simulation. arXiv preprint arXiv:1910.00935 , 2019a.
13Under review as submission to TMLR
Yuanming Hu, Jiancheng Liu, Andrew Spielberg, Joshua B Tenenbaum, William T Freeman, Jiajun Wu,
Daniela Rus, and Wojciech Matusik. Chainqueen: A real-time differentiable physical simulator for soft
robotics. In 2019 International conference on robotics and automation (ICRA) , pp. 6265–6271. IEEE,
2019b.
Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B Tenenbaum, and Chuang Gan. Plas-
ticinelab: Asoft-bodymanipulationbenchmarkwithdifferentiablephysics. arXiv preprint arXiv:2104.03311 ,
2021.
Marco Hutter, Christian Gehring, Dominic Jud, Andreas Lauber, C Dario Bellicoso, Vassilios Tsounis, Jemin
Hwangbo, Karen Bodie, Peter Fankhauser, Michael Bloesch, et al. Anymal-a highly mobile and dynamic
quadrupedal robot. In 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS) ,
pp. 38–44. IEEE, 2016.
Jemin Hwangbo, Inkyu Sa, Roland Siegwart, and Marco Hutter. Control of a quadrotor with reinforcement
learning. IEEE Robotics and Automation Letters , 2(4):2096–2103, 2017.
Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and
Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics , 4(26):eaau5872,
2019.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy
optimization. In Advances in Neural Information Processing Systems , 2019.
Juraj Kabzan, Lukas Hewing, Alexander Liniger, and Melanie N Zeilinger. Learning-based model predictive
control for autonomous racing. IEEE Robotics and Automation Letters , 4(4):3363–3370, 2019.
Elia Kaufmann, Antonio Loquercio, René Ranftl, Matthias Müller, Vladlen Koltun, and Davide Scaramuzza.
Deep drone acrobatics. arXiv preprint arXiv:2006.05768 , 2020.
Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems ,
12, 1999.
Junbang Liang, Ming Lin, and Vladlen Koltun. Differentiable cloth simulation for inverse problems. Advances
in Neural Information Processing Systems , 32, 2019.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 ,
2015.
Miles Macklin. Warp: A high-performance python framework for gpu simulation and graphics. https:
//github.com/nvidia/warp , March 2022. NVIDIA GPU Technology Conference (GTC).
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 , 2013.
Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively
parallel deep reinforcement learning. In Conference on Robot Learning , pp. 91–100. PMLR, 2022.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic
computation graphs. Advances in neural information processing systems , 28, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human
knowledge. nature, 550(7676):354–359, 2017.
14Under review as submission to TMLR
Hyung Ju Suh, Max Simchowitz, Kaiqing Zhang, and Russ Tedrake. Do differentiable simulators give better
policy gradients? In International Conference on Machine Learning , pp. 20668–20696. PMLR, 2022.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. Advances in neural information processing systems ,
12, 1999.
Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends ®in
Machine Learning , 8(1-2):1–230, 2015.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Reinforcement learning , pp. 5–32, 1992.
Jie Xu, Tao Chen, Lara Zlokapa, Michael Foshey, Wojciech Matusik, Shinjiro Sueda, and Pulkit Agrawal. An
end-to-end differentiable framework for contact-aware robot design. arXiv preprint arXiv:2107.07501 , 2021.
Jie Xu, Viktor Makoviychuk, Yashraj Narang, Fabio Ramos, Wojciech Matusik, Animesh Garg, and Miles
Macklin. Acceleratedpolicylearningwithparalleldifferentiablesimulation. arXiv preprint arXiv:2204.07137 ,
2022.
15Under review as submission to TMLR
A Heavisde example
This appendix provides additional details on the Heaviside example used to obtain intuition regarding FOBG
bias in Section 3.
¯H(x) =

1x>ν/ 2
2x/ν|x|≤ν/2
−1x<−ν/2
Under stochastic input x∼πθ(·) =θ+wwherew∼N(0,σ), we can obtain the expected value:
Ew/bracketleftbig¯H(x)/bracketrightbig
=/integraldisplay∞
−∞¯H(x)πθ(x)dx
=−/integraldisplay−ν/2
−∞πθ(x)dx+/integraldisplayν/2
−ν/22x
νπθ(x)dx+/integraldisplay∞
ν/2πθ(x)dx
=−1
2erfc/parenleftbiggν+ 2θ
2√
2ν/parenrightbigg
+1
2erfc/parenleftbiggν−2θ
2√
2ν/parenrightbigg
+θ
νerf/parenleftbiggν−2θ
2√
2σ/parenrightbigg
+θ
νerf/parenleftbiggν+ 2θ
2√
2σ/parenrightbigg
+σ√
2
ν√π/parenleftig
exp−(ν+ 2θ)2
8σ2−exp−(ν−2θ)2
8σ2/parenrightig
From the expectation, we can obtain the gradient w.r.t. the parameter of interest:
∇θEw/bracketleftbig¯H(x)/bracketrightbig
=1√
2πσexp/parenleftig−(ν+ 2θ)2
8σ2/parenrightig
+1√
2πσexp/parenleftig−(ν−2θ)2
8σ2/parenrightig
+1
νerf/parenleftbiggν−2θ
2√
2σ/parenrightbigg
+1
νerf/parenleftbiggν+ 2θ
2√
2σ/parenrightbigg
−√
2θ√πνσexp/parenleftig−(ν−2θ)2
8σ2/parenrightig
+√
2θ√πνσexp/parenleftig−(ν+ 2θ)2
8σ2/parenrightig
−1√
2σνexp/parenleftbig
−(ν−2θ)1/4σ2/parenrightbig
(ν−2θ)1/4σ2−1−1√
2σνexp/parenleftbig
−(ν+ 2θ)1/4σ2/parenrightbig
(ν+ 2θ)1/4σ2−1
As seen from the equation above, the true gradient ∇θEw/bracketleftbig¯H(x)/bracketrightbig
̸= 0atθ= 0. However, using FOBG, we
obtain∇θ¯H(a) = 0in samples where |a|>ν/ 2, which occurs with probability at least ν/σ√
2π. Even though
both ZOBG and FOBG are theoretically unbiased as N→∞, both exhibit empirical bias as shown in Figure
14
1.0
 0.5
 0.0 0.5 1.0
0246810[1]J()
[0]J()
wH(x)
(a) Gradient estimates at N= 1000.
0 200 400 600 800 1000
Number of samples (N)0.00.51.01.52.02.5Gradient bias[1]J()
[0]J()
 (b) Change in gradient bias for different sample sizes N.
Figure 14: Gradient bias study for the Soft Heaviside function shown in Eq. 4. Both ZOBG and FOBG
exhibit bias at low samples sizes, however, FOBG are especially susceptible to the empirical bias phenomena.
16Under review as submission to TMLR
B Proof of Lemma 3.1
Assumption B.1. As well as being continuously differentiable (Assumption 2.7, the policy is 1-Lipshitz-
smooth:||∇θπθ(a|s)||≤Bπ≤1and the reward function is 1 Lipshitz-smooth and bounded rewards
r(s,a)≤||∇r(s,a)||≤Br≤1∀s∈Rn;a∈Rm;θ∈Rd.
Proof.First, we expand our definition of bias and define a random variable of a single Monte-Carlo sample
/vextenddouble/vextenddouble/vextenddoubleVar[∇][1]
θJ(θ)−Var[∇][0]
θJ(θ)/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NN/summationdisplay
i=1ˆ∇[1]
θJi(θ)−1
NN/summationdisplay
i=1ˆ∇[0]
θJi(θ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=1
N/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN/summationdisplay
i=1(ˆ∇[1]
θJi(θ)−ˆ∇[0]
θJi(θ))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble(10)
DefineXi=ˆ∇[1]
θJi(θ)−ˆ∇[0]
θJi(θ)and bound it:
Xi=H/summationdisplay
h=1/parenleftbigg
∇ahr(sh,ah)∇θπθ(ah|sh) +h−1/summationdisplay
h′=1∇shr(sh,ah)/parenleftbigh′/productdisplay
t=1∇stf(st,at)/parenrightbig
∇θπθ(ah′|sh′)/parenrightbigg
+H/summationdisplay
h=1r(sh,ah)∇logπθ(ah|sh)
=H/summationdisplay
h=1/parenleftbigg
∇ahr(sh,ah)∇θπθ(ah|sh)−r(sh,ah)∇θlogπθ(ah|sh)
+h−1/summationdisplay
h′=1∇shr(sh,ah)/parenleftbigh′/productdisplay
t=1∇stf(st,at)/parenrightbig
∇θπθ(ah′|sh′)/parenrightbigg
≤H/summationdisplay
h=1h−1/summationdisplay
h′=1∇shr(sh,ah)/parenleftbigh′/productdisplay
t=1∇stf(st,at)/parenrightbig
∇θπ(·,sh′) (Assumption B.1)
≤H/summationdisplay
h=1h−1/summationdisplay
h′=1BrBπh′/productdisplay
t=1∥∇f(st,at)∥
We can now sum up these random variables as Z=/summationtextN
i=1Xiand create an upper concentration bound. As
these RVs are difficult to bound, we can apply a Chebyshev Inequality (Tropp et al., 2015):
P(||Z−E[Z]||>ϵ)≤Var[Z]
ϵ2
17Under review as submission to TMLR
Since the gradient samples are assumed to be i.i.d., we can expand this variance using the definition of each
gradient type where all expectations are taken over the action distributions ahfor each step:
Var[Z] = Var/bracketleftiggN/summationdisplay
i=1Xi/bracketrightigg
= Var/bracketleftiggN/summationdisplay
i=1ˆ∇[1]
θJi(θ)−ˆ∇[0]
θJi(θ)/bracketrightigg
=N/summationdisplay
i=1Var/bracketleftig
ˆ∇[1]
θJi(θ)−ˆ∇[0]
θJi(θ)/bracketrightig
≤N/summationdisplay
i=1E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoubleˆ∇[1]
θJi(θ)−ˆ∇[0]
θJi(θ)/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
≤N/summationdisplay
i=1E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleH/summationdisplay
h=1h−1/summationdisplay
h′=1BrBπh′/productdisplay
t=1||∇f(st,at)||||2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble

≤NH4B2
rB2
πE/bracketleftiggH/productdisplay
t=1∥∇f(st,at)∥2/bracketrightigg
With this result we can return back to Equation 10 and obtain
/vextenddouble/vextenddouble/vextenddoubleVar[∇][1]
θJ(θ)−Var[∇][0]
θJ(θ)/vextenddouble/vextenddouble/vextenddouble≤1
NNH4B2
rB2
πE/bracketleftiggH/productdisplay
t=1∥∇f(st,at)∥2/bracketrightigg
=H4B2
rB2
πE/bracketleftiggH/productdisplay
t=1∥∇f(st,at)∥2/bracketrightigg
C Summary of modifications
To develop Adaptive Horizon Actor Critic (AHAC) algorithm, we used the Short Horizon Actor Critic (SHAC)
algorithm (Xu et al., 2022) as a starting point. This section details all modifications applied to the SHAC in
order to derive AHAC and achieve the reported results in this paper. We also note that some of these are not
exclusive to either approach approach.
1.Adaptive horizon objective - instead of optimising the for the short horizon rollout return, we
introduce the new constrained objective shown in Equation 8. To optimise that and adapt the horizon
H, we introduced the dual problem in Equation 9 and optimised it directly for policy parameters θ
and the Lagrangian coefficients ϕ.
J(θ) :=t+T−1/summationdisplay
h=tγh−tr(sh,ah) +γtVψ(st+T)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
SHAC objectives.t.∥∇f(st,at)∥≤C∀t∈{0,..,T}
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
AHAC objective
2.Dual critic - the original implementation of SHAC struggled with more complex tasks such as
Humanoid due the its highly non-convex value landscape. The authors of (Xu et al., 2022) solved
that by introducing a delayed target critic similar to prior work in deep RL (Lillicrap et al., 2015).
We found that approach brittle and requiring more hyper-parameter tuning. Instead, we replaced
it with a dual critic similar to (Haarnoja et al., 2018) which has been shown to stabilise on-policy
algorithms (Amos et al., 2021). For our work, we found that it reduced variance of asymptotic
rewards achieved by AHAC while removing a hyperparameter.
18Under review as submission to TMLR
3.Critic training until convergence - empirically we found that different problems present different
value landscapes. The more complex the landscape, the more training the critic required and the critic
often failed to fit the data with the limited number of critic iterations done in SHAC (16). Instead
of training the critic for a fixed number of iterations, we trained the (dual) critic of AHAC until
convergence defined by/summationtextn
i=n−5Li(ψ)−Li−1(ψ)<0.5whereLi(ψ)is the critic loss for mini-batch
iterationi. We allowed the critic to be trained for a maximum of 64 iterations. We found that this
resulted in asymptotic performance improvements on more complex tasks such as Humanoid and
SNU Humanoid while removing yet another hyper-parameter.
D AHAC-1 algorithm
Algorithm 2: Adaptive Horizon Actor-Critic (Single environment)
1Given:γ: discount rate
2Given:α: learning rate
3Given:H: maximum trajectory length
4Given:C: contact threshold
5Initialise learnable parameters θ,ψ
6t←0
7while episode not done do
/* rollout policy */
8Initialise buffer D
9Initialise return R←0
10while∥∇f∥≤Candh≤Hdo
11at∼πθ(·|st)
12st+1=f(st,at)
13D←D∪{(st+h,at+h,rt+h,Vψ(st+h+1))}
14t←t+ 1
/* train actor with Eq. 7 */
15θ←θ−α∇θJ(θ)
/* train critic with Eq. 6 */
16while not converged do
17 sample (s,ˆV(s))∼D
18ψ←ψ+α∇ψL(ψ)
E Simulation details
The experimental simulator, dflex (Xu et al., 2022), employed in Section 5, is a GPU-based differentiable
simulator utilizing the Featherstone formulation for forward dynamics. It employs a spring-damper contact
model with Coulomb friction.
The dynamics function fis modeled by solving the forward dynamics equations:
M¨q=JTF(q,˙q) +c(q,˙q) +τ(q,˙q,a)
where,q,˙q,¨qare joint coordinates, velocities, and accelerations, respectively. Frepresents external forces, c
includes Coriolis forces, and τdenotes joint-space actuation. Mass matrix Mand Jacobian Jare computed
concurrently using one thread per-environment. The composite rigid body algorithm (CRBA) is employed for
articulation dynamics, enabling caching of the matrix factorization for reuse in the backward pass through
parallel Cholesky decomposition.
After determining joint accelerations ¨q, a semi-implicit Euler integration step updates the system state
s= (q,˙q). Torque-based control is employed for simple environments, where the policy outputs τat each
19Under review as submission to TMLR
time-step. For further details, see (Xu et al., 2022). It is noted that dflex is no longer actively developed and
has been succeeded by warp (Macklin, 2022).
The rewards used across all experiments are designed to maximise the forward velocity vx:
Environment Reward
Hopper vx+Rheight +Rangle−0.1∥a∥2
2
Ant vx+Rheight + 0.1Rangle +Rheading−0.01∥a∥2
2
Anymal vx+Rheight + 0.1Rangle +Rheading−0.01∥a∥2
2
Humanoid vx+Rheight + 0.1Rangle +Rheading−0.002∥a∥2
2
Humanoid STU vx+Rheight + 0.1Rangle +Rheading−0.002∥a∥2
2
Table 2: Table of hyper-parameters for all algorithms bench-marked in Section 5.
We additionally use auxiliary rewards Rheightto incentivise the agent to, Rangleto keep the agents’ normal
vector point up, Rheadingto keep the agent’s heading pointing towards the direction of running and a norm
over the actions to incentivise energy-efficient policies. For most algorithms, none of these rewards apart
from the last one are crucial to succeed in the task. However, all of them aid learning policies faster.
Rheight =/braceleftigg
h−hterm ifh≥hterm
−200(h−hterm)2ifh<hterm
Rangle = 1−/parenleftbiggθ
θterm/parenrightbigg2
Rangle =∥qforward−qagent∥2
2is the difference between the heading of the agent qagentand the forward
vectorqagent.his the height of the CoM of the agent and θis the angle of its normal vector. htermand
θtermare parameters that we set for each environment depending on the robot morphology. Similar to other
high-performance RL applications in simulation, we find it crucial to terminate episode early if the agent
exceeds these termination parameters. However, it is worth noting that AHAC is still capable of solving all
tasks described in the paper without these termination conditions, albeit slower.
F Hyper-parameters
This section details all hyper-parameters used in the main experiments of Section 5. PPO and SAC, as our
MFRL baselines, have been tuned to perform well across all tasks, including task-specific hyper-parameters.
SVG has not been specifically tuned for all benchmarks due to time limitations but instead uses the hyper-
parameters presented in (Amos et al., 2021).2SHAC is tuned to perform well across all tasks using a fixed
H= 32as in the original work (Xu et al., 2022). AHAC shares all of its common hyper-parameters with
SHAC and only has its horizon learning rate αϕtuned per-task. The contact threshold Cand iterative critic
training criteria did not benefit from tuning. Note that the dual critic employed by AHAC, uses the same
hyper-parameters used by the SHAC critic. Therefore, we have left AHAC under-tuned in comparison to
SHAC in order to make to highlight the benefits of the adaptive horizon mechanism presented in this work.
Table 3 shows common hyper-parameters shared between all tasks. While table 4 shows hyper-parameters
specific to each problem. Where possible we attempted to use the hyper-parameters suggested by the original
works, however, we also attempted to share hyper-parameters between algorithms to ease comparison. If a
specific hyper-parameter is not mentioned, then it is the one used in the original work behind the specific
algorithm.
2Tuning SVG proved difficult as we were unable to vectorise the algorithm resulting in up to 2-week training times. This
made it difficult to tune for our benchmarks
20Under review as submission to TMLR
AHAC SHAC PPOSACSVG
Mini-epochs 16 5 4
Batch size 8 8 8321024
λ 0.95 0.95 0.95
γ 0.99 0.99 0.990.990.99
H - horizon 32 32 3
C - contact thresh. 500
Grad norm 1.0 1.0 1.0
ϵ 0.2
Actorlog(σ)bounds (-5,2)(-5,2)
α- temperature 0.20.1
λα 10−410−4
|D|- buffer size 106106
Seed steps 0 0 0 104104
Table 3: Table of hyper-parameters for all algorithms benchmarked in Section 5. These are shared across all
tasks.
Hopper Ant Anymal Humanoid SNU Humanoid
Actor layers (128, 64, 32) (128, 64, 32) (256, 128) (256, 128) (512, 256)
Actorαθ 2×10−32×10−32×10−32×10−32×10−3
Horizonαϕ 2×10−41×10−51×10−51×10−51×10−5
Critic layers (64, 64) (64, 64) (256, 128) (256, 128) (256, 256)
Criticαψ 4×10−32×10−32×10−35×10e−45×10−4
Criticτ0.2 0.2 0.2 0.995 0.995
Table 4: Task-specific hyper-parameters. All benchmarked algorithms share the same actor and critic network
hyper-parameters with ELU activation functions. AHAC and PPO do not have target critic networks and as
such do not have τas a hyper-parameter.
G Tabular experimental results
Below we present the asymptotic results of Section 5 in tabular form.
Hopper Ant Anymal Humanoid SNU Humanoid
PPO 1.00±0.11 1.00±0.12 1.00±0.03 1.00±0.05 1.00±0.09
SAC 0.87±0.16 0.95±0.08 0.98±0.06 1.04±0.04 0.88±0.11
SVG 0.84±0.08 0.83±0.13 0.84±0.19 1.06±0.16 0.75±0.23
SHAC 1.02±0.03 1.16±0.13 1.26±0.04 1.15±0.04 1.44±0.08
AHAC 1.10±0.00 1.41±0.08 1.46±0.06 1.35±0.07 1.64±0.07
Table 5: Tabular results of the asymptotic (end of training) rewards achieved by each algorithm across all
tasks. The results presented are 50 % IQM and standard deviation across 10 random seeds. All algorithms
have been trained until convergence. The rewards presented are PPO-normalised.
H Ablation study details
In Section 5, we provided an ablation study of the individual contributions of our proposed approach, AHAC,
as summarised in Appendix C. In this section, we provide further details on the conducted experiments. The
aim of the study is to understand what changes contribute to the asymptotic performance of AHAC. To best
21Under review as submission to TMLR
Ablation H Actor objective CriticIterative
critic training
SHAC H=32 32 Eq. 7 Single w/ target
SHAC H=29 29 Eq. 7 Single w/ target
Adapt. Objective 32 Eq. 9 Single w/ target
Adapt. Horizon adaptive Eq. 9 Single w/ target
Iterative critic 32 Eq. 7 Single w/ target ✓
Dual critic 32 Eq. 7 Dual
AHAC adaptive Eq. 9 Dual ✓Actor
ablations
Critic
ablations
Table 6: Differences between ablations studied, split into actor and critic ablations. All ablations only
introduce one component to the baseline, SHAC.
achieve that, we started from SHAC as the baseline using the tuned version detailed in Appendix F above.
Afterwards we add the individual components that contribute to AHAC using the hyper-parameter from the
section above. Note that only hyper-parameters particular to AHAC have been tuned to achieve the results
presented in this paper; all other hyper-parameters are the ones tuned to our baseline SHAC with H= 32.
In particular we have only tuned the adaptive horizon learning rate αψ, contact threshold Cand the criteria
for early stopping while doing iterative critic training. Table 6 shows the detailed differences between the
ablations presented in Section 5. The ablations include:
1. SHAC H=32 - our baseline with most hyper-parameters tuned to it.
2. SHAC H=29 - SHAC using the horizon Hwhich AHAC converges to asymptotically.
3.Adapt. Objective - SHAC using the adaptive horizon objective introduced in Eq. 9 but without
using it to adapt to horizon.
4.Adapt. Horizon - SHAC using the objective in Eq 9 and adapting the horizon. This is equivalent
AHAC without the dual critic with iterative training.
5.Iterative critic - SHAC with a single target critic utilising iterative critic training until convergence.
6. Dual critic - SHAC with a dual critic and no target.
We provide ablations results on these changes on the Ant task in Figure 13. Wee also provide the learning
curves for the same experiments in Figure 15 and tabular results in 7.
Ablation Asymptotic reward
SHAC H=32 1.16±0.14
SHAC H=29 1.23±0.17
Adaptive Objective 1.18±0.18
Adaptive Horizon 1.35±0.12
Iterative Critic 1.17±0.13
Dual Critic 1.20±0.07
AHAC 1.41±0.08
Table 7: Results of asymptotic performance of our ablation study showing 50% IQM and standard deviation.
22Under review as submission to TMLR
0 10 M 20 M 30 M 40 M
Simulation steps0.81.01.21.41.6Episode rewardSHAC H=32
SHAC H=29
0 10 M 20 M 30 M 40 M
Simulation steps0.81.01.21.41.6 SHAC H=32
Adapt. Obj.
0 10 M 20 M 30 M 40 M
Simulation steps0.81.01.21.41.6 SHAC H=32
Adapt. Horizon
0 10 M 20 M 30 M 40 M
Simulation steps0.81.01.21.41.6 SHAC H=32
Iterative Critic
0 10 M 20 M 30 M 40 M
Simulation steps0.81.01.21.41.6Episode rewardSHAC H=32
Dual Critic
0 10 M 20 M 30 M 40 M
Simulation steps0.81.01.21.41.6 SHAC H=32
AHAC
Figure 15: Standalone ablation results for the Ant task. These results are the same as in Figure 13 but
presented in a different format for improved legibility.
23