Published in Transactions on Machine Learning Research (10/2022)
A Note on “Assessing Generalization of SGD via
Disagreement”
Andreas Kirsch andreas.kirsch@cs.ox.ac.uk
Yarin Gal yarin.gal@cs.ox.ac.uk
OATML, Department of Computer Science
University of Oxford
Reviewed on OpenReview: https: // openreview. net/ forum? id= oRP8urZ8Fx
Abstract
Several recent works find empirically that the average test error of deep neural networks can
be estimated via the prediction disagreement of models, which does not require labels. In
particular, Jiang et al. (2022) show for the disagreement between two separately trained
networks that this ‘Generalization Disagreement Equality’ follows from the well-calibrated
nature of deep ensembles under the notion of a proposed ‘class-aggregated calibration.’ In
this reproduction, we show that the suggested theory might be impractical because a deep
ensemble’s calibration can deteriorate as prediction disagreement increases, which is precisely
when the coupling of test error and disagreement is of interest, while labels are needed to
estimate the calibration on new datasets1. Further, we simplify the theoretical statements
and proofs, showing them to be straightforward within a probabilistic context, unlike the
original hypothesis space view employed by Jiang et al. (2022).
1 Introduction
Machine learning models can cause harm when their predictions become unreliable, yet we trust them
blindly. This is not fiction but has happened in real-world applications of machine learning (Schneider, 2021).
Thus, there has been significant research interest in model robustness, uncertainty quantification, and bias
mitigation. In particular, finding ways to bound the test error of a trained deep neural network without
access to the labels would be of great importance: one could estimate the performance of models in the wild
where unlabeled data is ubiquitous, labeling is expensive, and the data often does not match the training
distribution. Crucially, it would provide a signal on when to trust the output of a model and when to defer
to human experts instead.
Several recent works (Chen et al., 2021; Granese et al., 2021; Garg et al., 2022; Jiang et al., 2022) look at the
question of how model predictions in a non-Bayesian setting can be used to estimate model accuracy. In
this paper, we focus on one work2(Jiang et al., 2022) specifically, and examine the theoretical and empirical
results from a Bayesian perspective.
In a Bayesian setting, epistemic uncertainty (Der Kiureghian & Ditlevsen, 2009) captures the uncertainty of
a model about the reliability of its predictions, that is epistemic uncertainty quantifies the uncertainty of
a model about its predictive distribution, while aleatoric uncertainty quantifies the ambiguity within the
predictive distribution and the label noise, c.f. Kendall & Gal (2017). Epistemic uncertainty thus tells us
whether we can trust a model’s predictions or not. Assuming a well-specified andwell-calibrated Bayesian
model, when its predictive distribution has low epistemic uncertainty for an input, it can be trusted. But
likewise, a Bayesian model’s calibration ought to deteriorate as epistemic uncertainty increases for a sample:
in that case, the predictions become less reliable and so does the model’s calibration.
1Code at https://github.com/BlackHC/2202.01851
2An ICLR 2022 Spotlight, which has spawned additional follow-up works, e.g. Baek et al. (2022).
1Published in Transactions on Machine Learning Research (10/2022)
In this context, calibration is an aleatoric metric for a model’s reliability (Gopal, 2021). Calibration captures
how well a model’s confidence for a given prediction matches the actual frequency of that prediction in the
limit of observations in distribution : when a model is 70%confident about assigning label A, does Aindeed
occur with 70%probability in these instances?
Jiang et al. (2022), while not Bayesian, make the very interesting empirical and theoretical discovery that
deep ensembles satisfy a ‘ Generalization Disagreement Equality ’ when they are well-calibrated according to a
proposed ‘ class-aggregated calibration ’ (or a ‘class-wise calibration ’) and empirically find that the respective
calibration error generally bounds the absolute difference between the test error and ‘ disagreement rate .’ Jiang
et al. (2022)’s theory builds upon Nakkiran & Bansal (2020)’s ‘ Agreement Property ’ and provides backing for
an empirical connection between the test error anddisagreement rate of two separately trained networks
on the same training data. Yet, while Nakkiran & Bansal (2020) limit the applicability of their Agreement
Property to in-distribution data, Jiang et al. (2022) carefully extend it: ‘ our theory is general and makes no
restrictions on the hypothesis class, the algorithm, the source of stochasticity, or the test distributions (which
may be different from the training distribution) ’ with qualified evidence: ‘ we present preliminary observations
showing that GDE is approximately satisfied even for certain distribution shifts within the PACS (Li et al.,
2017) dataset. ’
In this paper, we present a new perspective on the theoretical results using a standard probabilistic approach
for discriminative (Bayesian) models, whereas Jiang et al. (2022) use a hypothesis space of models that output
one-hot predictions. Indeed, their theory does not require one-hot predictions, separately trained models
(deep ensembles) or Bayesian models. Moreover, as remarked by the authors, their theoretical results also
apply to a single model that outputs softmax probabilities. We will see that our perspective greatly simplifies
the results and proofs.
This also means that the employed notion of disagreement rate does not capture epistemic uncertainty but
overall uncertainty , similar to the predictive entropy, which is a major difference to Bayesian approaches
which can evaluate epistemic uncertainty separately (Smith & Gal, 2018). Overall uncertainty is the sum
of aleatoric and epistemic uncertainty. For in-distribution data, epistemic uncertainty will generally be
low: overall uncertainty will mainly capture aleatoric uncertainty and align well with (aleatoric) calibration
measures. However, under distribution shift, epistemic uncertainty can be a confounding factor.
Importantly, we find that the connection between the proposed calibration metrics and the gap between test
error and disagreement rate exists because the introduced notion of class-aggregated calibration is so strong
that this connection follows almost at once.
Moreover, the suggested approach is circular3: calibration must be measured on the data distribution we
want to evaluate. Otherwise, we cannot bound the difference between the test error and the disagreement rate
and obtain a signal on how trustworthy our model is. This reintroduces the need for labels on the unlabeled
dataset, limiting practicality. Alternatively, one would have to assume that these calibration metrics do not
change for different datasets or under distribution shifts, which we show not to hold: deep ensembles are less
calibrated the more the ensemble members disagree (even on in-distribution data).
Lastly, we draw connections and show that the ‘class-aggregated calibration error’ and the ‘class-wise
calibration error’4are equivalent to the ‘adaptive calibration error’ and ‘static calibration error’ introduced
in Nixon et al. (2019) and its implementation.
Outline. We introduce the necessary background and notation in §2. In §3 we rephrase the theoretical
statements from Jiang et al. (2022) using a parameter distribution (instead of a version space) and auxiliary
random variables. This allows us to simplify the theoretical statements and proofs greatly in §4 and to
examine the connection to Nixon et al. (2019). Finally, in §5, we provide empirical evidence that deep
ensembles are less calibrated exactly when their ensemble members disagree.
3This was added as a caveat to the camera-ready version of Jiang et al. (2022) after reviewing a preprint of this paper.
4Which is not explicitly introduced in Jiang et al. (2022) but can be analogously constructed.
2Published in Transactions on Machine Learning Research (10/2022)
2 Background & Setting
We introduce relevant notation, the initial Bayesian formalism, the connection to deep ensembles, and the
probabilistic model. We restate the statements from Jiang et al. (2022) using this formalism in §3.
Notation. We use an implicit notation for expectations E[f(X)]when possible. For additional clarity, we
also use EX[f(X)]andEp(x)f(x), which fix the random variables and distribution, respectively, when needed.
We will use nested probabilistic expressions of the form E[p(ˆY=Y|X)]. Prima facie, this seems unambiguous,
but is p(ˆY=Y|X)a transformed random variable of only Xor also of Y(and ˆY): what are we taking the
expectation over? This is not always unambiguous, so we disambiguate between the probability for an event
defined by an expression P[. . .] =E[1{. . .}], where 1{. . .}is the indicator function5, and a probability given
specific outcomes for various random variables p(ˆy|x), c.f.:
P[ˆY=Y|X] =EˆY,Y[1{ˆY=Y}|X] =Ep(ˆy,y|X)1{ˆy=y}, (1)
which is a transformed random variable of X, while p(ˆY=Y|X)is simply a (transformed) random variable,
applying the probability density on the random variables YandX. Put differently, Yis bound within the
former but not the latter: P[. . .|X]is a transformed random variable of X, and any random variable that
appears within the . . .is bound within that expression.
Probabilistic Model. We assume classification with Kclasses. For inputs Xwith ground-truth labels Y,
we have a Bayesian model with parameters Ωthat makes predictions ˆY:
p(y,ˆy, ω|x) = p( y|x) p(ˆy|x, ω) p(ω). (2)
We focus on model evaluation. (Input) samples xcan come either from ‘in-distribution data’ which follows
the training set or from samples under covariate shift (distribution shift). The expected prediction over the
model parameters is the marginal predictive distribution :
p(ˆy|x) =EΩ[p(ˆy|x,Ω)]. (3)
Onp(ω).The main emphasis in Bayesian modelling can be Bayesian inference or Bayesian model averaging
(Wilson & Izmailov, 2020). Here we concentrate on the model averaging perspective, and for simplicity
take the model averaging to be with respect to somedistribution p(ω). Hence, we will use p(ω)as the
push-forward of models initialized with different initial seeds through SGD to minimize the negative log
likelihood with weight decay and a specific learning rate schedule (MLE or MAP) (Mukhoti et al., 2021):
Assumption 1. We assume that p(ω)is a distribution of possible models we obtain by training with a
specific training regime on the training data with different seeds. A single ωidentifies a single trained model.
Deep Ensembles. We cast deep ensembles (Hansen & Salamon, 1990; Lakshminarayanan et al., 2016),
which refer to training multiple models and averaging predictions, into the introduced Bayesian perspective
above by viewing them as an empirical finite sample estimate of the parameter distribution p(ω). Then,
ω1, . . . , ωN∼p(ω)drawn i.i.d. are the ensemble members .
Again, the implicit model parameter distribution p(w)is given by the models that are obtained through
training. Hence, we can view the predictions of a deep ensemble or the ensemble’s prediction disagreement
for specific x(or over the data) as empirical estimates of the predictions or the model disagreement using the
implicit model distribution, respectively.
Calibration. A model’s calibration for a given xmeasures how well the model’s top-1 (argmax) confidence
Conf Top1:= p( ˆY= arg max
kp(ˆY=k|X)|X) (4)
matches its top-1 accuracy
AccTop1:= p(Y= arg max
kp(ˆY=k|X)|X), (5)
5The indicator function is 1when the predicate ‘ . . .’ is true and 0otherwise.
3Published in Transactions on Machine Learning Research (10/2022)
where we define both as transformed random variables of X. The calibration error is usually defined as the
absolute difference between the two:
CE:=|AccTop1−Conf Top1|. (6)
In general, we are interested in the expected calibration error (ECE) over the data distribution (Guo et al.,
2017) where we bin samples by their top-1 confidence. Intuitively, the ECE will be low when we can trust the
model’s top-1 confidence on the given data distribution.
We usually use top-1 predictions in machine learning. However, if we were to draw ˆYaccording to p(ˆy|x)
instead, the (expected) accuracy would be:
Acc:=P[Y=ˆY|X] (7)
=/summationdisplay
kp(Y=k|X) p(ˆY=k|X) (8)
=EY[p(ˆY=Y|X)|X], (9)
as a random variable of X. Usually we are interested in the accuracy over the whole dataset:
P[ˆY=Y] =E[Acc] = EX[P[ˆY=Y|X]] =EX,Y[p(ˆY=Y|X)]. (10)
For example, for binary classification with two classes A and B, if class A appears with probability 0.7 and a
model predicts class A with probability 0.2 (and thus class B appears with probability 0.3, which a model
predicts as 0.8), its accuracy is 0.7×0.2 + 0 .3×0.8 = 0 .38, while the top-1 accuracy is 0.3. Likewise, the
predicted accuracy is 0.22+ 0.82= 0.68while the top-1 predicted accuracy is 0.8.
3 Rephrasing Jiang et al. (2022) in a Probabilistic Context
We present the same theoretical results as Jiang et al. (2022) but use a Bayesian formulation instead of a
hypothesis space and define the relevant quantities as (transformed) random variables. As such, our definitions
and theorems are equivalent and follow the paper but look different. We show these equivalences in §A in the
appendix and prove the theorems and statements themselves in the next section.
First, however, we note a distinctive property of Jiang et al. (2022). It is assumed that each p(ˆy|x, ω)
is always one-hot for any ω. In practice, this could be achieved by turning a neural network’s softmax
probabilities into a one-hot prediction for the arg max class. We call this the Top1-Output-Property (TOP).
Assumption 2. The Bayesian model p(ˆy, ω|x)satisfies TOP: p(ˆy|x, ω)is one-hot for all xandω.
Definition 3.1. Thetest error anddisagreement rate , as transformed random variables of Ω(and Ω′), are:
TestError :=P[ˆY̸=Y|Ω] (11)
/parenleftbig
= 1−P[ˆY=Y|Ω] (12)
= 1−EX,Y[p(ˆY=Y|X,Ω)], (13)
= 1−Ep(x,y)p(ˆY=y|x,Ω))/parenrightbig
, (14)
Dis:=P[ˆY̸=ˆY′|Ω,Ω′] (15)
/parenleftbig
= 1−P[ˆY=ˆY′|Ω,Ω′] (16)
= 1−EX,ˆY[p(ˆY′=ˆY|X,Ω′)|Ω,Ω′] (17)
= 1−Ep(x,ˆy|Ω)p(ˆY′= ˆy|x,Ω′)/parenrightbig
, (18)
where for the disagreement rate, we expand our probabilistic model to take a second model Ω′with prediction
ˆY′into account (and which uses the same parameter distribution), so:
p(y,ˆy, ω, ˆy′, ω′|x):= p(y|x) p(ˆy|x, ω) p(ω) p(ˆY= ˆy′|x,Ω =ω′) p(Ω = ω′).
4Published in Transactions on Machine Learning Research (10/2022)
Jiang et al. (2022) then introduce the property of interest:
Definition 3.2. A Bayesian model p(ˆy, ω|x)fulfills the Generalization Disagreement Equality (GDE) when:
EΩ[TestError(Ω)] = EΩ,Ω′[Dis(Ω ,Ω′)] (⇔E[TestError] = E[Dis]) . (19)
When this property holds, we seemingly do not require knowledge of the labels to estimate the (expected)
test error: computing the (expected) disagreement rate is sufficient.
Two different types of calibration are then introduced, class-wise andclass-aggregated calibration, and it is
shown that they imply the GDE:
Definition 3.3. The Bayesian model p(ˆy, ω|x)satisfiesclass-wise calibration when for any q∈[0,1]and
any class k∈[K]:
p(Y=k|p(ˆY=k|X) =q) =q. (20)
Similarly, the Bayesian model p(ˆy, ω|x)satisfiesclass-aggregated calibration when for any q∈[0,1]:
/summationdisplay
kp(Y=k,p(ˆY=k|X) =q) =q/summationdisplay
kp(p(ˆY=k|X) =q). (21)
Theorem 3.4. When the Bayesian model p(ˆy, ω|x)satisfies class-wise or class-aggregated calibration, it
also satisfies GDE.
Finally, Jiang et al. (2022) introduce the class-aggregated calibration error similar to the ECE and then use it
to bound the magnitude of any GDE gap:
Definition 3.5. Theclass-aggregated calibration error (CACE) is the integral of the absolute difference of
the two sides in eq. (21) over possible q∈[0,1]:
CACE :=/integraldisplay
q∈[0,1]/vextendsingle/vextendsingle/summationdisplay
kp(Y=k,p(ˆY=k|X) =q)−q/summationdisplay
kp(p(ˆY=k|X) =q)/vextendsingle/vextendsingledq. (22)
Theorem 3.6. For any Bayesian model p(ˆy, ω|x), we have:
|E[TestError]−E[Dis]|≤CACE .
In the following section, we simplify the definitions and prove the statements using elementary probability
theory, showing that notational complexity is the main source of complexity.
4 GDE is Class-Aggregated Calibration in Expectation
We show that proof for Theorem 3.6 is trivial if we use different but equivalent definitions of the class-wise
and class-aggregate calibration. First though, we establish a better understanding for these definitions by
examining the GDE property E[TestError ] =E[Dis]. For this, we expand the definitions of E[TestError ]and
E[Dis], and use random variables to our advantage.
We define a quantity which will be of intuitive use later on: the predicted accuracy
PredAcc :=EˆY[p(ˆY|X)|X] =/summationdisplay
kp(ˆY=k|X) p(ˆY=k|X), (23)
as a random variable of X. It measures the expected accuracy assuming the model’s predictions are correct,
that is the true labels follow p(ˆy|x). This also assumes that we draw ˆYaccordingly and do not always use
the top-1 prediction.
Revisiting GDE. On the one hand, we have:
E[TestError] = EΩ[P[ˆY̸=Y|Ω]] (24)
5Published in Transactions on Machine Learning Research (10/2022)
= 1−P[ˆY=Y] (25)
= 1−EX,ˆY[p(Y=ˆY|X)] (26)
= 1−E[Acc] (27)
and on the other hand, we have:
E[Dis] = EΩ,Ω′[P[ˆY̸=ˆY′|Ω,Ω′]] (28)
= 1−EΩ,Ω′[P[ˆY=ˆY′|Ω,Ω′]] (29)
= 1−P[ˆY=ˆY′] (30)
= 1−EX,ˆY[p(ˆY′=ˆY|X)] (31)
= 1−EX,ˆY[p(ˆY|X)] (32)
= 1−E[PredAcc] . (33)
The step from (30)to(31)is valid because ˆY⊥ ⊥ˆY′|X, and the step from (31)to(32)is valid because p(ˆy′|
x) = p(ˆ y|x). Thus, we can rewrite Theorem 3.4 as:
Lemma 4.1. The model p(ˆy|x)satisfies GDE, when
E[Acc] = E[p(Y=ˆY|X)] =E[p(ˆY|X)] =E[PredAcc] , (34)
i.e. in expectation, the accuracy of the model equals the predicted accuracy of the model, or equivalently,
the error of the model equals its predicted error.
Crucially, while Jiang et al. (2022) calls 1−EX,ˆY[p(ˆY|X)]the (expected) disagreement rate E[Dis], it actually
is just the predicted error of the (Bayesian) model as a whole.
Equally important, all dependencies on Ωhave vanished. Indeed, we will not use Ωanymore for the remainder
of this section. This reproduces the corresponding remark from Jiang et al. (2022)6:
Conclusion 1.The theoretical statements in Jiang et al. (2022) can be made about any discriminative
model with predictions p(y|x).
When is EX,ˆY[p(Y=ˆY|X)] =EX,ˆY[p(ˆY|X)]? Or in other words: when does p(Y=ˆy|x)equal p(ˆY=ˆy|
x)in expectation over p(x, y,ˆy)?
As a trivial sufficient condition, when the predictive distribution matches our data distribution— i.e. when
the model p(ˆy|x)is perfectly calibrated on average for all classes—and not only for the top-1 predicted class .
ECE = 0is not sufficient because the standard calibration error only ensures that the data distribution and
predictive distribution match for the top-1 predicted class (Nixon et al., 2019). But class-wise calibration
entails this equality.
Class-Wise and Class-Aggregated Calibration. To see this, we rewrite class-wise and class-aggregated
calibration slightly by employing the following tautology:
p(ˆY=k|p(ˆY=k|X) =q) =q, (35)
which is obviously true due its self-referential nature. We provide a formal proof in §D in the appendix. Then
we have the following equivalent definition:
Lemma 4.2. The model p(ˆy|x)satisfiesclass-wise calibration when for any q∈[0,1]and any class k∈[K]:
p(Y=k,p(ˆY=k|X) =q) = p( ˆY=k,p(ˆY=k|X) =q). (36)
6The remark did not exist in the first preprint version.
6Published in Transactions on Machine Learning Research (10/2022)
Similarly, the model p(ˆy|x)satisfies class-aggregated calibration when for any q∈[0,1]:
p(p(ˆY=Y|X) =q) = p(p( ˆY|X) =q), (37)
andclass-wise calibration implies class-aggregate calibration.
The straightforward proof is found in §D in the appendix.
Jiang et al. (2022) mention ‘level sets’ as intuition in their proof sketch. Here, we have been able to make
this even clearer: class-aggregated calibration means that level-sets for accuracy p(ˆY=Y|X)and predicted
accuracy p(ˆY|X)—as random variables of YandX, and ˆYandX, respectively—have equal measure, that
is probability, for all q.
GDE.Now, class-aggregated calibration immediately and trivially implies GDE. To see this, we use the
following property of expectations:
Lemma 4.3. For a random variable X, a function t(x), and the random variable T=t(X), it holds that
ET[T] =E[T] =EX[t(X)]. (38)
This basic property states that we can either compute an expectation over Tby integrating over p(T=t)
or by integrating t(x)over p(X=x). This is just a change of variable (push-forward of a measure).
We can use this property together with the class-aggregated calibration to see:
E[Acc]
=
EX,Y[p(ˆY=Y|X)]=
E[p(ˆY=Y|X)]=
Eq∼p(ˆY=Y|X)[q] =E[PredAcc]
=
EX,ˆY[p(ˆY|X)]=
E[p(ˆY|X)]=
Eq∼p(ˆY|X)[q], (39)
which is exactly Lemma 4.1, where we start with the equality following from class-aggregated calibration
and then apply Lemma 4.3 along each side. Thus, GDE is but an expectation over class-aggregated
calibration; we have:
Theorem 4.4. When a model p(ˆy|x)satisfies class-wise or class-aggregated calibration, it satisfies GDE.
Proof.We can formalize the proof to be even more explicit and introduce two auxiliary random variables:
S:= p( ˆY=Y|X), (40)
as a transformed random variable of YandX, and
T:= p( ˆY|X), (41)
as a transformed random variable of ˆYandX. Class-wise calibration implies class-aggregated calibration.
Class-aggregated calibration then is p(S=q) = p( T=q)(*). Writing out eq. (39), we have
E[p(ˆY=Y|X)] =EX,Y[S] =E[S] =ES[S] (42)
=/integraldisplay
p(S=q)q dq (43)
(∗)=/integraldisplay
p(T=q)q dq (44)
=ET[T] =E[T] =EX,ˆY[T] =E[p(ˆY|X)], (45)
which concludes the proof.
7Published in Transactions on Machine Learning Research (10/2022)
The reader is invited to compare this derivation to the corresponding longer proof in the appendix of Jiang et al.
(2022). The fully probabilistic perspective greatly simplifies the results, and the proofs are straightforward.
CACE. Showing that CACEbounds the gap between test error and disagreement is also straightforward:
Theorem 4.5. For any model p(ˆy|x), we have:
|E[TestError]−E[Dis]|≤CACE .
Proof.First, we note that
CACE =/integraldisplay
q∈[0,1]/vextendsingle/vextendsinglep(p(ˆY=Y|X) =q)−p(p(ˆY|X) =q)/vextendsingle/vextendsingledq. (46)
following the equivalences in the proof of Lemma 4.2. Then using the triangle inequality for integrals and
0≤q≤1, we obtain:
CACE (47)
=/integraldisplay
q∈[0,1]/vextendsingle/vextendsinglep(p(ˆY=Y|X) =q)−p(p(ˆY=ˆY|X) =q)/vextendsingle/vextendsingledq (48)
≥/integraldisplay
q∈[0,1]q/vextendsingle/vextendsinglep(p(ˆY=Y|X) =q)−p(p(ˆY=ˆY|X) =q)/vextendsingle/vextendsingledq (49)
≥/vextendsingle/vextendsingle/integraldisplay
q∈[0,1]qp(p(ˆY=Y|X) =q)dq−/integraldisplay
q∈[0,1]qp(p(ˆY|X) =q)dq/vextendsingle/vextendsingle. (50)
=/vextendsingle/vextendsingleE[S]−E[T]/vextendsingle/vextendsingle (51)
=/vextendsingle/vextendsingleE[TestError]−E[Dis]/vextendsingle/vextendsingle, (52)
where we have used the monotonicity of integration in (49) and the triangle inequality in (50).
The bound also serves as another—even simpler—proof for Theorem 4.4:
Conclusion 2.When the Bayesian model satifies class-wise or class-aggregated calibration, we have
CACE = 0 and thus E[TestError] = E[Dis], i.e. the model satisfies GDE.
Furthermore, note again that a Bayesian model was not necessary for the last two theorems. The model
parameters Ωwere not mentioned—except for the specific definitions of TestError andDiswhich depend on
Ωfollowing Jiang et al. (2022) but which we only use in expectation.
Moreover, we see that we can easily upper-bound CACEusing the triangle inequality by 2, narrowing the
statement in Jiang et al. (2022) that CACEcan lie anywhere in [0, K]:
Conclusion 3.CACE≤2.
Additionally, for completeness, we can also define the class-wise calibration error formally and show that it is
bounded by CACEusing the triangle inequality:
Definition 4.6. Theclass-wise calibration error (CWCE) is defined as:
CWCE :=/summationdisplay
k/integraldisplay
q∈[0,1]/vextendsingle/vextendsinglep(Y=k,p(ˆY=k|X) =q)−p(ˆY=k,p(ˆY=k|X) =q)/vextendsingle/vextendsingle. (53)
Lemma 4.7. CWCE≥CACE≥|E[Acc]−E[PredAcc]|.
Note that when we compute CACEempirically, we divide the dataset into several bins for different intervals
ofp(ˆY=k|X). Jiang et al. (2022) use 15 bins. If we were to use a single bin, we would compute
|E[Acc]−E[PredAcc]|directly.
In §B we show that CWCEhas previously been introduced as ‘adaptive calibration error’ in Nixon et al.
(2019) and CACEas ‘static calibration error’ (with noteworthy differences between Nixon et al. (2019)
and its implementation).
8Published in Transactions on Machine Learning Research (10/2022)
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)0.010.020.030.04Value
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
CIFAR-10 (In-Distribution)
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)0.010.020.030.04Value
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)0.010.020.030.04Value
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
CINIC-10 (Distribution Shift)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Predicted Error (Disagreement Rate)0.050.100.150.200.25Value
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
(a) Ensemble with TOP
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Predicted Error (Disagreement Rate)0.050.100.150.200.25Value
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error | (b) Ensemble without TOP (same underlying models)
Figure 1: Rejection Plot of Calibration Metrics for Increasing Disagreement In-Distribution (CIFAR-10)
and Under Distribution Shift (CINIC-10). Different calibration metrics ( ECE,CWCE,CACE) vary across
CIFAR-10 and CINIC-10 on an ensemble of 25 Wide-ResNet-28-10 model trained on CIFAR-10, depending
on the rejection threshold of the predicted error (disagreement rate). Thus, calibration cannot be assumed
constant for in-distribution data or under distribution shift. The test error increases almost linearly with the
predicted error (disagreement rate), leading to ‘GDE gap’ |Test Error−Predicted Error|becoming almost
flat, providing evidence for the empirical observations in Nakkiran & Bansal (2020); Jiang et al. (2022). The
mean predicted error (disagreement rate) is shown on the x-axis. (a)shows results for an ensemble using
TOP (following Jiang et al. (2022)), and (b)for a regular deep ensemble without TOP. The regular deep
ensemble is better calibrated but has higher test error overall and lower test error for samples with small
predicted error.
9Published in Transactions on Machine Learning Research (10/2022)
5 Deterioration of Calibration under Increasing Disagreement
Generally, we can only hope to trust model calibration for in-distribution data, while under distribution shift,
the calibration ought to deteriorate. In our empirical falsification using models trained on CIFAR-10 and
evaluated on the test sets of CIFAR-10 and CINIC-10, as a dataset with a distribution shift, we find in both
cases that calibration deteriorates under increasing disagreement. We further examine ImageNET and PACS
in §E.2. Most importantly though, calibration markedly worsens under distribution shift.
Specifically, we examine an ensemble of 25 WideResNet models (Zagoruyko & Komodakis, 2016) trained on
CIFAR-10 (Krizhevsky et al., 2009) and evaluated on CIFAR-10 and CINIC-10 test data. CINIC-10 (Darlow
et al., 2018) consists of CIFAR-10 and downscaled ImageNet samples for the same classes, and thus includes
a distribution shift. The training setup follows the one described in Mukhoti et al. (2021), see appendix §E.1.
Figure 1 shows rejection plots under increasing disagreement for in-distribution data (CIFAR-10) and under
distribution shift (CINIC-10). The rejection plots threshold the dataset on increasing levels of the predicted
error (disagreement rate)—which is a measure of epistemic uncertainty when there is no expected aleatoric
uncertainty in the dataset. We examine ECE, class-aggregated calibration error ( CACE), class-wise calibration
error ( CWCE), error E[TestError ], and ‘GDE gap’,|E[Acc]−E[PredAcc ]|, as the predicted error (disagreement
rate), E[Dis] = 1−E[PredAcc ], increases. We observe that all calibration metrics, ECE, CACE and CWCE,
deteriorate under increasing disagreement, both in distribution and under distribution shift, and also worsen
under distribution shift overall.
We also observe the same for ImageNet (Deng et al., 2009) and PACS (Li et al., 2017), which we show in
appendix §E.2.
This is consistent with the experimental results of Ovadia et al. (2019) which examines dataset shifts. However,
given that the calibration metrics change with the quantity of interest, we conclude that:
Conclusion 4.The bound from Theorem 3.6 might not have as much expressive power as hoped since the
calibration metrics themselves deteriorate as the model becomes more ‘uncertain’ about the data.
At the same time, the ‘GDE gap’, which is the actual gap between test error and predicted error, flattens,
and the test error develops an almost linear relationship with the predicted error (up to a bias). This shows
that there seem to be intriguing empirical properties of deep ensemble as observed previously (Nakkiran &
Bansal, 2020; Jiang et al., 2022). However, they are not explained by the proposed calibration metrics7.
As described previously, the results are not limited to Bayesian or version-space models but also apply to any
model p(ˆy|x), including a regular deep ensembles without TOP. In our experiment, we find that a regular
deep ensemble is better calibrated than the same ensemble made to satisfy TOP. We hypothesize that each
ensemble member’s own predictive distribution is better calibrated than its one-hot outputs, yielding a better
calibrated ensemble overall.
Given that all these calibration metrics require access to the labels, and we cannot assume the model to be
calibrated under distribution shift, we might just as well use the labels directly to asses the test error.
6 Discussion
Here, we discuss connections to Bayesian model disagreement and epistemic uncertainty, as well as connections
to information theory. We expand on these points in much greater detail in appendix §C.
Bayesian Model Disagreement. From a Bayesian perspective, as the epistemic uncertainty increases, we
expect the model to become less reliable in its predictions. The predicted error of the model is a measure of
the model’s overall uncertainty, which is the total of aleatoric and epistemic uncertainty and thus correlated
with epistemic uncertainty. Thus, we can hypothesize that as the predicted error increases, the model should
7The simplest explanation is that very few samples have high predicted error and thus the rejection plots flatten. This is not
true. For CINIC-10, the first bucket contains 50k samples, and each latter buckets adds additional ∼10k samples.
10Published in Transactions on Machine Learning Research (10/2022)
become less reliable, which will be reflected in increasing calibration metrics. This is exactly what we have
empirically validated in the previous section.
Connection to Information Theory. At first sight, Jiang et al. (2022) seems disconnected from information
theory. However, we can draw a connection by using ˆh(p):= 1−pas a linear approximation for Shannon’s
information content h(p) =−logp. Semantically, both this approximation and Shannon’s information content
quantify surprise (i.e., prediction error). Both are 0for certain events. For unlikely events, the former tends
to1while the latter tends to +∞.
This leads to common-sense definitions and statements from an information-theoretic point of view. We
can even formulate parallel statements using information theory and see that the statements relate to total
uncertainty and not epistemic uncertainty in a Bayesian sense.
Other Related Literature. Beyond Jiang et al. (2022), this note offers a perspective on Granese et al.
(2021) and Garg et al. (2022), which are proposing related approaches.
Granese et al. (2021) use the predicted error (disagreement rate), referred to as Dα, and the predicted top-1
error, Dβ, to estimate when the model will be wrong. As noted in §C, the predicted error can be seen as
an approximation of Shannon’s entropy. Thus, Dαis effectively using an approximation of the prediction
entropy for OOD detection and rejection classification. Similarly, Dβis the maximum class confidence. Both
are well-known baselines for OOD detection (Hendrycks & Gimpel, 2016). There is no ablation to see how
DαandDβdiffer from these baselines. We leave this to future work. The paper frames the question of
whether a model’s predictions will be correct as binary classification problem on top of the underlying model’s
output probabilities and investigate this from a theoretical point of view. They also examine using input
perturbations similar to Liang et al. (2017) and Lee et al. (2018).
Garg et al. (2022) threshold the predictive entropy or maximum class confidence to estimate the test error
under distribution shift. They estimate the threshold by calibrating it on in-distribution labelled data: the
threshold is chosen such that the percentage of rejected in-distribution validation data approximately equals
the test error on this in-distribution data. They call this approach Average Thresholded Confidence (ATC) .
They find that ATC using entropy performs better than ATC using the maximum confidence and other
approaches, including GDE. Their results show that ATC also degrades under increasing distribution shifts
similar to what we have seen for GDE in §5 as the choice of threshold is explicitly tied to the in-distribution8.
Garg et al. (2022) explicitly examine the theoretical limits when no further assumptions are made.
7 Conclusion
We have found that the theoretical statements in Jiang et al. (2022) can be expressed and proven more
concisely when using probabilistic notation for (Bayesian) models that output softmax probabilities.
Moreover, we empirically found the proposed calibration metrics to deteriorate under increasing disagreement
for in-distribution data, and as expected, we have found the same behavior under distribution shifts.
While Jiang et al. (2022) are careful to qualify their results for distribution shifts, above results should give us
pause: strong assumptions are still needed to conjecture about model generalization, and we need to beware
of circular arguments.
Acknowledgements
The authors would like to thank the anonymous TMLR reviewers for their kind, constructive, and helpful
feedback during the review process. Moreover, we would like to thank Jannik Kossen, Joost van Amersfoort,
and the members of OATML in general for their feedback at various stages of the project, and the authors
for Jiang et al. (2022) for constructively engaging with a preprint of this work. AK is supported by the UK
EPSRC CDT in Autonomous Intelligent Machines and Systems (grant reference EP/L015897/1).
8See also Figures 7, 8, and 9 in the appendix of Garg et al. (2022)
11Published in Transactions on Machine Learning Research (10/2022)
References
Christina Baek, Yiding Jiang, Aditi Raghunathan, and Zico Kolter. Agreement-on-the-line: Predicting the
performance of neural networks under distribution shift, 2022.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers, 2021.
Jiefeng Chen, Frederick Liu, Besim Avci, Xi Wu, Yingyu Liang, and Somesh Jha. Detecting errors and
estimating accuracy on unlabeled data with self-training ensembles, 2021.
Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet or cifar-10.
arXiv preprint arXiv:1810.03505 , 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee,
2009.
Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural safety , 31(2):
105–112, 2009.
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recognition at scale, 2020.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In
International Conference on Machine Learning , pp. 1183–1192. PMLR, 2017.
SaurabhGarg, SivaramanBalakrishnan, ZacharyC.Lipton, BehnamNeyshabur, andHanieSedghi. Leveraging
unlabeled data to predict out-of-distribution performance, 2022.
Achintya Gopal. Why calibration error is wrong given model uncertainty: Using posterior predictive checks
with deep learning, 2021.
Federica Granese, Marco Romanelli, Daniele Gorla, Catuscia Palamidessi, and Pablo Piantanida. Doctor: A
simple method for detecting misclassification errors, 2021.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
International Conference on Machine Learning , pp. 1321–1330. PMLR, 2017.
Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern analysis and
machine intelligence , 12(10):993–1001, 1990.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image
classification with convolutional neural networks, 2018.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in
neural networks, 2016.
Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classification
and preference learning. arXiv preprint arXiv:1112.5745 , 2011.
Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of SGD via
disagreement. In International Conference on Learning Representations , 2022. URL https://openreview.
net/forum?id=WvOGCEAQhxl .
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision?,
2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
12Published in Transactions on Machine Learning Research (10/2022)
Andreas Kirsch and Yarin Gal. A practical & unified notation for information-theoretic quantities in ml.
arXiv preprint arXiv:2106.12062 , 2021.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for
deep bayesian active learning. In Advances in Neural Information Processing Systems , pp. 7024–7035, 2019.
AndreasKirsch, ClareLyle, andYarinGal. Unpackinginformationbottlenecks: Unifying information-theoretic
objectives in deep learning. arXiv preprint arXiv:2003.12537 , 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty
estimation using deep ensembles. arXiv preprint arXiv:1612.01474 , 2016.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-
distribution samples and adversarial attacks, 2018.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE international conference on computer vision , pp. 5542–5550,
2017.
Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in
neural networks, 2017.
Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical
Statistics , pp. 986–1005, 1956.
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet
for the 2020s. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Jun
2022. doi: 10.1109/cvpr52688.2022.01167. URL http://dx.doi.org/10.1109/cvpr52688.2022.01167 .
Andrew McCallum and Kamal Nigam. Employing em and pool-based active learning for text classification.
InICML, 1998.
Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deterministic neural
networks with appropriate inductive biases capture epistemic and aleatoric uncertainty. arXiv preprint
arXiv:2102.11582 , 2021.
Preetum Nakkiran and Yamini Bansal. Distributional generalization: A new kind of generalization. arXiv
preprint arXiv:2009.08092 , 2020.
Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring
calibration in deep learning. In CVPR Workshops , 2019.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive
uncertainty under dataset shift. arXiv preprint arXiv:1906.02530 , 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems
32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf .
Shira Schneider. Algorithmic Bias: A New Age of Racism . PhD thesis, 2021.
13Published in Transactions on Machine Learning Research (10/2022)
Lewis Smith and Yarin Gal. Understanding measures of uncertainty for adversarial example detection. arXiv
preprint arXiv:1803.08533 , 2018.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.
Training data-efficient image transformers distillation through attention, 2020.
Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models , 2019.
Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of
generalization, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 , 2016.
14Published in Transactions on Machine Learning Research (10/2022)
A Equivalent Definitions
Jiang et al. (2022) defines a hypothesis space H. In the literature, this is also sometimes called a version
space. The hypothesis space induced by a stochastic training algorithm Ais namedHA.
We can identify each hypothesis h:X→ [K]with itself as parameter ωh=hand define p(ωh)as a uniform
distribution over all parameters/hypotheses in HA. This has the advantage of formalizing the distribution
from which hypothesis are drawn ( h∼HA), which is not made explicit in Jiang et al. (2022). h(x) =kthen
becomes arg maxˆyp(ˆy|x, ωh) =k. Moreover, as p(ˆy, ω|x)satisfies TOP, we have9
“1{h(x) =k}′′= p( ˆY=k|x, ωh). (54)
Thus, “ TestErr D(h)≜ED[1[h(X)̸=Y]]” is equivalent to:
“ TestErr D(h) =ED[1[h(X)̸=Y]]′′(55)
=EX,Y[p(ˆY̸=Y|X, ωh)] (56)
=EX[P[ˆY̸=Y|X, ωh]] (57)
=P[ˆY̸=Y|ωh] (58)
= TestError( ωh). (59)
Similarly, “ DisD(h, h′)≜ED[1[h(X)̸=h′(X)]]” is equivalent to:
“ DisD(h, h′)≜ED[1[h(X)̸=h′(X)]]′′(60)
=EˆY,ˆY′[P[ˆY̸=ˆY′|ωh, ωh′]] (61)
= Dis( ωh, ωh′). (62)
Further, “ ˜hk(x)≜EHA[1[h(x) =k]]” is equivalent to:
“˜hk(x)≜EHA[1[h(x) =k]]′′(63)
=EΩ[p(ˆY=k|x,Ω)] (64)
= p( ˆY=k|x). (65)
For the GDE, “ Eh,h′∼HA[DisD(h, h′)] =Eh∼HA[TestErr( h)]” is equivalent to:
“Eh,h′∼HA[DisD(h, h′)] =Eh∼HA[TestErr( h)]′′
⇔EΩ,Ω′[Dis(Ω ,Ω′)] =EΩ[TestError(Ω)] . (66)
For the class-wise calibration, “ p(Y=k|˜hk(X) =q) =q” is equivalent to:
“p(Y=k|˜hk(X) =q) =q′′(67)
⇔p(Y=k|p(ˆY=k|X) =q) =q. (68)
For the class-aggregated calibration, “/summationtextK−1
k=0p(Y=k,˜hk(X)=q)/summationtextK−1
k=0p(˜hk(X)=q)=q” (and note in Jiang et al. (2022), class indices
run from 0..K−1) is equivalent to:
“/summationtextK−1
k=0p(Y=k,˜hk(X) =q)
/summationtextK−1
k=0p(˜hk(X) =q)=q′′(69)
9We put definitions and expressions written using the notation and variables from Jiang et al. (2022) inside quotation marks
“” to avoid ambiguities.
15Published in Transactions on Machine Learning Research (10/2022)
⇔/summationtextK
k=1p(Y=k,p(ˆY=k|X) =q)
/summationtextK
k=1p(p(ˆY=k|X) =q)=q (70)
⇔K/summationdisplay
k=1p(Y=k,p(ˆY=k|X) =q)
=qK/summationdisplay
k=1p(p(ˆY=k|X) =q). (71)
Finally, for the class-aggregated calibration error, the definition is equivalent to:
“ CACE D(˜h)
≜/integraldisplay
q∈[0,1]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
kp/parenleftbig
Y=k,˜hk(X) =q/parenrightbig
/summationtext
kp/parenleftbig˜hk(X) =q/parenrightbig−q/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle·/summationdisplay
kp/parenleftbig˜hk(X) =q/parenrightbig
dq (72)
=/integraldisplay
q∈[0,1]/vextendsingle/vextendsingle/summationdisplay
kp/parenleftbig
Y=k,˜hk(X) =q/parenrightbig
−q/summationdisplay
kp/parenleftbig˜hk(X) =q/parenrightbig/vextendsingle/vextendsingledq′′(73)
=/integraldisplay
q∈[0,1]/vextendsingle/vextendsingle/summationdisplay
kp(Y=k,p(ˆY=k|X) =q)−q/summationdisplay
kp(p(ˆY=k|X) =q)/vextendsingle/vextendsingledq (74)
B Comparison of CACEandCWCEwith calibration metrics with ‘adaptive
calibration error’ and ‘static calibration error’
Nixon et al. (2019) examine shortcomings of the ECE metric and identify a lack of class conditionality,
adaptivity and the focus on the maximum probability (argmax class) as issues. They suggest an adaptive
calibration error which uses adaptive binning and averages of the calibration error separately for each class,
thus equivalent to the class-wise calibration error and class-wise calibration (up to adaptive vs. even binning).
In the paper, the static calibration error is defined as ACE with even instead of adaptive binning. However,
in the widely used implementation10, SCE is defined as equivalent to the class-aggregated calibration error.
C Expanded Discussion
Here, we discuss connections to Bayesian model disagreement and epistemic uncertainty, as well as connections
to information theory, the bias-variance trade-off, and prior literature.
C.1 Bayesian Model Disagreement
From a Bayesian perspective, as the epistemic uncertainty increases, we expect the model to become less
reliable in its predictions. The predicted error of the model is a measure of the overall uncertainty of the
model which is the total of aleatoric and epistemic uncertainty, and thus correlated with epistemic uncertainty.
Thus, we can hypothesize that as the predicted error increases, the model should become less reliable, which
will be reflected in increasing calibration metrics. This is what we have empirically validated in the previous
section. We can expand on the connection to the Bayesian perspective. In particular, we can connect the
statements of Jiang et al. (2022) to a well-known Bayesian measure of model disagreement.
Information Theory. We follow notation introduced in Kirsch & Gal (2021). In particular, for entropy, we
use an implicit or explicit notation, H[X]orH(p(X)), and use a notation for the cross-entropy H(p∥q)which
is follows the Kullback-Leibler divergence DKL(p∥q). We base these definitions on Shannon’s information
content h(p):
h(p):=−logp, (75)
10https://github.com/google-research/robustness_metrics/blob/baa47fbe38f80913590545fe7c199898f9aff349/
robustness_metrics/metrics/uncertainty.py#L1585 , added in April 2021
16Published in Transactions on Machine Learning Research (10/2022)
H(p(X)∥q(X)):=Ep(x)[h(q(x))], (76)
H[X]:= H(p( X)):= H(p( X)∥p(X)), (77)
DKL(p(X)∥q(X)):= H(p( X)∥q(X))−H(p(X)), (78)
where pandqare probabilities distributions. Conditional and joint entropies are defined as usual. For
completeness, for random variables XandY:
H[X|Y]:=Ep(x,y)[−log p( x|y)]. (79)
Finally, the mutual information I[X;Y]for random variables X,Yis defined as expected uncertainty
reduction—also sometimes called expected information gain (Lindley, 1956):
I[X;Y] = H[ X]−H[X|Y], (80)
as the entropy H[X]can be seen as quantifying the ’uncertainty’ about the random variable X, and H[X|Y]
as expected uncertainty about Xafter observing Y.
Bayesian Model Disagreement. The mutual information I[ˆY; Ω|x]between predictions ˆYand model
parameters Ωfor a given sample xis a well-known quantity that measures the model disagreement for that
sample. It can be computed without having to perform Bayesian inference:
I[ˆY; Ω|x] = H[ ˆY|x]−H[ˆY|x,Ω] (81)
=EˆY,X/bracketleftbig
−log p( ˆY|X)
−EΩ[−log p( ˆY|X,Ω)]/bracketrightbig(82)
To see that I[ˆY; Ω|x]measures model disagreement, observe that if each ωwas equally likely to obtain a
different one-hot class prediction ˆY, the second term would be 0, while the first term H[ˆY|x]would be
maximal =logKas the predictive distribution would be uniform. Equally, if the predictions for each ω
agreed, we would have p(ˆy|x) =p(ˆy|x, ω)for all ω, the first and second term would be equal, and the
mutual information would be 0(Kirsch et al., 2019).
Model disagreement is a proxy of epistemic uncertainty. Epistemic uncertainty measures model-dependent
uncertainty (i.e. reliability) and is also known as reducible uncertainty: if we were to update the model with a
label for xusing Bayesian inference, the updated model’s uncertainty for xwould reduce. For this reason,
I[ˆY; Ω|x]is often used in Bayesian active learning, where it is known as BALD (Bayesian Active Learning
by Disagreement) (Houlsby et al., 2011), or in Bayesian optimal experiment design, where it is known as
Expected Information Gain (Lindley, 1956). In non-Bayesian active learning, the very same term is known as
Query-by-Committee (McCallum & Nigam, 1998) and computed via a Kullback-Leilber divergence:
I[ˆY; Ω|x] = H[ ˆY|x]−H[ˆY|x,Ω]
= D KL(p(ˆY|x,Ω)∥p(ˆY|x)). (83)
Taking an expectation, we obtain the expected model disagreement over the data I[ˆY; Ω|X] =EX[I[ˆY; Ω|
X]].
In §E.2, we also report empirical results for rejection plots based on Bayesian model disagreement instead of
predicted error.
C.2 Connection to Information Theory
At first sight, Jiang et al. (2022) seems disconnected from information theory. However, we can recover
statements by using ˆh(p):= 1−pas a linear approximation for Shannon’s information content h(p):
ˆh(p) = 1−p≤−logp= h(p). (84)
ˆh(p)is just the first-order Taylor expansion of h(p) =−logparound 1. Semantically, both Shannon’s
information content and this approximation quantify surprise. Both are 0for certain events. For unlikely
events, the former tends to +∞while the latter tends to 1.
17Published in Transactions on Machine Learning Research (10/2022)
We can define an approximate entropy ˆH[X]using h′:
ˆH[X]:=E[ˆh(p(X))] = 1−E[p(x)] = 1−/summationdisplay
xp(x)2, (85)
and anapproximate mutual information ˆI[X;Y]:
ˆI[X;Y]:=ˆH[X]−ˆH[X|Y] =ˆH[X]−Ep(y)ˆH[X|y], (86)
following the semantic notion of mutual information as expected information gain in §2.
ˆI[ˆY; Ω|x]as Covariance Trace. This approximate mutual information has a surprisingly nice property,
which was detailed in Smith & Gal (2018) originally:
Proposition C.1. The approximate mutual information ˆI[ˆY; Ω|x]is equal the sum of the variances of ˆy|
x,Ωover all ˆy:
ˆI[ˆY; Ω|x] =K/summationdisplay
ˆy=1VarΩ[p(ˆy|x,Ω)]≥0. (87)
We present a proof in §D in the appendix. The sum of variances of the predictive probabilities (or trace of
the respective covariance matrix) is a common proxy for epistemic uncertainty (Gal et al., 2017), and here
the mutual information ˆI[ˆY; Ω|x]using ˆhis just that. This gives evidence that these definitions are sensible
and connects them to other prior Bayesian literature. Importantly, this also shows that ˆH[ˆY|x]≥ˆH[ˆY|x,Ω].
Connection to Jiang et al. (2022). As random variable of XandY,ˆH[ˆY=Y|X]is the test error:
ˆH[ˆY=Y|X] = 1−p(ˆY=Y|X) = TestError . (88)
Thus, the approximate cross-entropy
ˆH(p(Y|X)∥p(ˆY=Y|X)) =Ep(Y|X)[ˆh(p(ˆY=Y|X))] (89)
is the expected test error E[TestError] .
Similarly, when TOP is fulfilled, the mutual information ˆI[ˆY; Ω|X]is the expected disagreement rate E[Dis].
That is, when ˆY|X,Ωis one-hot, we have:
ˆH[ˆY|X,Ω] = 1−EXEˆY[p(ˆY|X,Ω)|X]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1= 0. (90)
and thus:
ˆI[ˆY; Ω|X] =ˆH[ˆY|X]−ˆH[ˆY|X,Ω] (91)
=ˆH[ˆY|X] (92)
= 1−EX,ˆY[p(ˆY|X)] (93)
=E[Dis]. (94)
Lemma C.2. When the model p(ˆy|x, ω)satisfies TOP, the GDE is equivalent to:
ˆH(p(Y|X)∥p(ˆY=Y|X)) =ˆI[ˆY; Ω|X]. (95)
This relates the approximate cross-entropy loss (test error) to the approximate Bayesian model disagreement.
Without TOP. If TOP does not hold, the actualexpected disagreement ˆI[ˆY; Ω|x]lower-bounds the
“expected disagreement rate” E[Dis], which then equals the expected predicted error 1−EX,ˆY[p(ˆY|X)]when
we have GDE. We have the following general equivalence to GDE:
18Published in Transactions on Machine Learning Research (10/2022)
Lemma C.3. For a model p(ˆy|x), the GDE is equivalent to:
ˆH(p(Y|X)∥p(ˆY=Y|X)) = ˆH[ˆY|X]≥ˆI[ˆY; Ω|X]. (96)
The other statements and proofs translate likewise, and intuitively seem sensible from an information-theoretic
perspective. We can go further and directly establish analogous properties using information theory in the
next subsection.
C.3 Information-Theoretic Version
Here, we derive an information-theoretic version of the GDE both under the assumption of TOP and without.
Importantly, we will not require a Bayesian model for any of the main statements as they hold for any model
p(ˆy|x). We show that we can artificially introduce a connection to disagreement using TOP.
Information-Theoretic GDE. We have already introduced the BALD equation eq. (81), which connects
expected disagreement and predictive uncertainty:
I[ˆY; Ω|x] = H[ ˆY|x]−H[ˆY|x,Ω]
The expected disagreement is measured by the mutual information I[ˆY; Ω|X], and the prediction error is
measured by the cross-entropy of the predictive distribution under the true data generating distribution
H(p(Y|X)∥p(ˆY=Y|X)). Indeed, the test error is bounded by it (Kirsch et al., 2020):
p(Y̸=ˆY)≤1−e−H(p(Y|X)∥p(ˆY=Y|X)). (97)
When our model fulfills TOP, we have H[ˆY|X,Ω] = 0, and thus I[ˆY; Ω|X] =H[ˆY|X]. The expected
disagreement then equals the predicted label uncertainty H[ˆY|X]. Generally, we can define an ‘entropic
GDE’:
Definition C.4. A model p(ˆy|x)satifies entropic GDE, when:
H(p(Y|X)∥p(ˆY=Y|X)) = H[ ˆY|X]. (98)
Lemma C.5. When a Bayesian model p(ˆy, ω|x)satisfies TOP, entropic GDE is equivalent to
H(p(Y|X)∥p(ˆY=Y|X)) = I[ ˆY; Ω|X]. (99)
The latter is close to GDE, especially when comparing to the previous section.
We can formulate an entropic class-aggregated calibration by connecting H[ˆy|x]with H[y|x], where we define
H[ˆy|x]:=h(p(ˆy|x)) =−log p(ˆy|x)(Kirsch & Gal, 2021). That is, instead of using probabilities, we use
Shannon’s information-content:
Definition C.6. The model p(ˆy|x)satisfiesentropic class-aggregated calibration when for any q≥0:
p(H[ ˆY=Y|X] =q) = p(H[ ˆY=ˆY|X] =q). (100)
Similarly, we can define the entropic class-aggregated calibration error (ECACE) :
ECACE :=/integraldisplay
q∈[0,∞)/vextendsingle/vextendsinglep(H[ ˆY=Y|X] =q)
−p(H[ ˆY=ˆY|X] =q)/vextendsingle/vextendsingledq.(101)
As−logpis strictly monotonic and thus invertible for non-negative p, entropic class-aggregated calibration
and class-aggregated calibration are equivalent. ECACE andCACEare not, though.
The expectation of the transformed random variable H[ˆY=Y|X](inYandX) is just the cross-entropy:
EX,YH[ˆY=Y|X] =Ep(x,y)H[ˆY=y|X] = H(p( Y|X)∥p(ˆY=Y|X)). (102)
Using this notation, and analogous to Theorem 3.6, we can show:
19Published in Transactions on Machine Learning Research (10/2022)
Theorem C.7. When H[ˆy|x] =−log p(ˆ y|x)is uppper-bounded by Lfor all ˆyandx, we have:
ECACE≥1
L/vextendsingle/vextendsingleH(p(Y|X)∥p(ˆY=Y|X))−H[ˆY|X]/vextendsingle/vextendsingle, (103)
and when the model satisfies TOP, equivalently:
=1
L/vextendsingle/vextendsingleH(p(Y|X)∥p(ˆY=Y|X))−I[ˆY; Ω|X]/vextendsingle/vextendsingle. (104)
There might be better conditions than the upper-bound above but this bound is in the spirit of Jiang et al.
(2022). Indeed, the proof of Theorem 3.6 is the same, except that we use q≤Linstead of q≤1. Finally, when
the model satisfies entropic class-aggregated calibration, ECACE = 0, cross-entropy (or negative expected log
likelihood) equals disagreement (respectively, predicted label uncertainty when TOP does not hold). Thus,
we have:
Theorem C.8. When the model p(ˆy|x)satisfies entropic class-aggregated calibration , it trivially also
satisfies entropic GDE:
H(p(Y|X)∥p(ˆY=Y|X)) = H[ ˆY|X]≥I[ˆY; Ω|X], (105)
and when TOP holds:
H(p(Y|X)∥p(ˆY=Y|X)) = I[ ˆY; Ω|X]. (106)
Without TOP. Again, if we do not expect one-hot predictions for our ensemble members, the analogy put
forward in Jiang et al. (2022) breaks down because the Bayesian disagreement I[ˆY; Ω|X]only lower bounds
the predicted label uncertainty H[ˆY|X]and can not be connected to ECACE the same way. But this also
breaks down in the regular version in Jiang et al. (2022).
D Additional Proofs
Lemma D.1. For a model p(ˆy|x), we have for all k∈[K]andq∈[0,1]:
p(ˆY=k|p(ˆY=k|X) =q) =q, (107)
when the left-hand side is well-defined.
Proof.This is equivalent to
p(ˆY=k,p(ˆY=k|X) =q) =qp(p(ˆY=k|X) =q), (108)
as the conditional probability is either defined or p(p(ˆY=k|X)=q) = 0. Assume the former. Let
p(p(ˆY=k|X)=q)>0. Introducing the auxiliary random variable Tk:=p(ˆY=k|X)as a transformed
random variable of X, we have
p(ˆY=k, Tk=q) =qp(Tk=q). (109)
We can write the probability as an expectation over an indicator function
p(ˆY=k, Tk=q) (110)
=EX,ˆY[1{ˆY=k, Tk(X) =q}] (111)
=EX,ˆY[1{ˆY=k}1{Tk(X) =q}] (112)
=EX[1{Tk(X) =q}EˆY[1{ˆY=k}|X]] (113)
=EX[1{Tk(X) =q}p(ˆY=k|X)]. (114)
20Published in Transactions on Machine Learning Research (10/2022)
Importantly, if 1{Tk(x) =q}= 1for an x, we have Tk(x) =p(ˆY=k|x) =q, and otherwise, we multiply
with 0. Thus, this is equivalent to
=EX[1{Tk(X) =q}q] (115)
=qEX[1{Tk(X) =q}] (116)
=qp(Tk(X) =q). (117)
Lemma 4.2. The model p(ˆy|x)satisfiesclass-wise calibration when for any q∈[0,1]and any class k∈[K]:
p(Y=k,p(ˆY=k|X) =q) = p( ˆY=k,p(ˆY=k|X) =q). (36)
Similarly, the model p(ˆy|x)satisfies class-aggregated calibration when for any q∈[0,1]:
p(p(ˆY=Y|X) =q) = p(p( ˆY|X) =q), (37)
andclass-wise calibration implies class-aggregate calibration.
Proof.Beginning from
p(Y=k|p(ˆY=k|X) =q) =q, (118)
we expand the conditional probability to
⇔p(Y=k,p(ˆY=k|X) =q) =qp(p(ˆY=k|X) =q), (119)
and substitute eq. (35) into the outer q, obtaining the first equivalence
⇔p(Y=k,p(ˆY=k|X) =q) = p( ˆY=k,p(ˆY=k|X) =q). (120)
For the second equivalence, we follow the same approach. Beginning from
/summationdisplay
kp(Y=k,p(ˆY=k|X) =q) =q/summationdisplay
kp(p(ˆY=k|X) =q), (121)
we pull the outer qinto the sum and expand using (35)
⇔/summationdisplay
kp(Y=k,p(ˆY=k|X) =q) =/summationdisplay
kqp(p(ˆY=k|X) =q) =/summationdisplay
kp(ˆY=k,p(ˆY=k|X) =q).(122)
In the inner expression, kis tied to Yon the left-hand side and ˆYon the right-hand side, so we have
⇔/summationdisplay
kp(Y=k,p(ˆY=Y|X) =q) =/summationdisplay
kp(ˆY=k,p(ˆY|X) =q). (123)
Summing over k, marginalizes out Y=kand ˆY=krespectively, yielding the second equivalence
⇔p(p(ˆY=Y|X) =q) = p(p( ˆY|X) =q). (124)
Finally, class-wise calibration implies class-aggregated calibration as summing over different kin(120), which
is equivalent to class-wise calibration, yields (122), which is equivalent to class-aggregated calibration.
Proposition C.1. The approximate mutual information ˆI[ˆY; Ω|x]is equal the sum of the variances of ˆy|
x,Ωover all ˆy:
ˆI[ˆY; Ω|x] =K/summationdisplay
ˆy=1VarΩ[p(ˆy|x,Ω)]≥0. (87)
21Published in Transactions on Machine Learning Research (10/2022)
Proof.We show that both sides are equal:
ˆI[ˆY; Ω|x] =ˆH[ˆY|x]−ˆH[ˆY|x,Ω] (125)
=EˆY[1−p(ˆY|x)]−EˆY,Ω[1−p(ˆY|x,Ω)] (126)
=EˆY,Ω[p(ˆY|x,Ω)]−EˆY[p(ˆY|x)] (127)
=EΩEp(ˆy,x,Ω)[p(ˆy|x,Ω)]−Ep(ˆy|x)p(ˆy|x) (128)
=EΩ
K/summationdisplay
ˆy=1p(ˆy|x,Ω)2
−K/summationdisplay
ˆy=1EΩ[p(ˆy|x,Ω)]2(129)
=K/summationdisplay
ˆy=1EΩ/bracketleftbig
p(ˆy|x,Ω)2/bracketrightbig
−EΩ[p(ˆy|x,Ω)]2(130)
=K/summationdisplay
ˆy=1VarΩ[p(ˆy|x,Ω)] (131)
≥0, (132)
where we have used that Ep(ˆy|x)p(ˆy|x) =/summationtextK
ˆy=1p(ˆy|x)2.
E Experimental Validation of Calibration Deterioration under Increasing
Disagreement
Here, we discuss additional details to allow for reproduction and present results on additional datasets. In
addition to the experiments on CIFAR-10 (Krizhevsky et al., 2009) and CINIC-10 (Darlow et al., 2018), we
report results for ImageNet (Deng et al., 2009) (in-distribution) using an ensemble of pretrained models and
PACS (Li et al., 2017) (distribution shift) where we fine-tune ImageNet models on PACS’ ‘photo’ domain,
which is close to ImageNet as source domain, and evaluate it on PACS’ ‘art painting’, ‘sketch’, and ‘cartoon’
domains. We use all three domains together for distribution shift evaluation to have more samples for the
rejection plots.
E.1 Experiment Setup
We use PyTorch (Paszke et al., 2019) for all experiments.
CIFAR-10 and CINIC-10. We follow the training setup from Mukhoti et al. (2021): we train 25
WideResNet-28-10 models (Zagoruyko & Komodakis, 2016) for 350 epochs on CIFAR-10. We use SGD with
a learning rate of 0.1and momentum of 0.9. We use a learning rate schedule with a decay of 10 at 150 and
250 epochs.
ImageNet and PACS. We use pretrained models with various architectures (specifically: ResNet-152-D
(He et al., 2018), BEiT-L/16 (Bao et al., 2021), ConvNext-L (Liu et al., 2022), DeiT3-L/16 (Touvron et al.,
2020), and ViT-B/16 (Dosovitskiy et al., 2020)) from the timm package (Wightman, 2019) as base models.
We freeze all weights except for the final linear layer, which we fine-tune on PACS’ ‘photo‘ domain using
Adam (Kingma & Ba, 2014) with learning rate 5×10−3and batch size 128 for 1000 steps. We then build an
ensemble using these different models.
E.2 Additional Results
In Figure 2, we see that for ImageNet and PACS, the calibration metrics behave like for CIFAR-10 and
CINIC-10, matching the described behavior in the main text. We use 5 models from each of the enumerated
architectures to build an ensemble of 25 models. Individual architectures also behave as expected as we
ablate in Figure 5.
22Published in Transactions on Machine Learning Research (10/2022)
0.2 0.4 0.6 0.8 1.0
Predicted Error (Disagreement Rate)0.000.050.100.150.20Value
0.2 0.4 0.6 0.8 1.0
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
ImageNet (In-Distribution)
0.2 0.4 0.6 0.8 1.0
Predicted Error (Disagreement Rate)0.000.050.100.150.20Value
0.2 0.4 0.6 0.8 1.0
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
0.2 0.4 0.6 0.8 1.0
Predicted Error (Disagreement Rate)0.000.050.100.150.20Value
0.2 0.4 0.6 0.8 1.0
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
PACS (Distribution Shift)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)0.000.050.100.150.200.250.300.35Value
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
(a) Ensemble with TOP
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)0.000.050.100.150.200.250.300.35Value
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error | (b) Ensemble without TOP (same underlying models)
Figure 2: Rejection Plot of Calibration Metrics for Increasing Disagreement In-Distribution (ImageNet) and
Under Distribution Shift (PACS ‘photo‘ domain →other domains). Different calibration metrics ( ECE,
CWCE,CACE) vary across ImageNet and PACS’ ‘art painting’, ‘cartoon’, and ‘sketch’ domains across an
ensemble of 5 models trained on ImageNet and 25 models fine-tuned on PACS’ ‘photo’ domain, depending
on the rejection threshold of the predicted error (disagreement rate). Again, calibration cannot be assumed
constant for in-distribution data or under distribution shift. The mean predicted error (disagreement rate) is
shown on the x-axis. (a)shows results for an ensemble using TOP (following Jiang et al. (2022)), and (b)
for a regular deep ensemble without TOP. Details in §E.2.
23Published in Transactions on Machine Learning Research (10/2022)
Additionally, inFigure3andFigure4, wealsoshowrejectionplotsusingtheExpectedInformationGain/BALD
for thresholding. We observe similar trajectories. Comparing these results with Figure 1 and Figure 2, we
see that both the predicted error and the Bayesian metric behave similarly. We hypothesize that this could
be because the datasets only contain few samples with high aleatoric uncertainty (e.g. noise), which would
otherwise act as confounder (Mukhoti et al., 2021). See also the discussion in §6.
24Published in Transactions on Machine Learning Research (10/2022)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Expected Information Gain/BALD (Bayesian Disagreement)0.000.010.020.030.04Value
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
CIFAR-10 (In-distribution)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Expected Information Gain/BALD (Bayesian Disagreement)0.000.010.020.030.04Value
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Expected Information Gain/BALD (Bayesian Disagreement)0.000.010.020.030.04Value
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
CINIC-10 (Distribution Shift)
0.0 0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)0.050.100.150.200.25Value
0.0 0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
(a) Ensemble with TOP
0.0 0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)0.050.100.150.200.25Value
0.0 0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error | (b) Ensemble without TOP (same underlying models)
Figure 3: Rejection Plot of Calibration Metrics for Increasing Bayesian Disagreement In-Distribution (CIFAR-
10) and Under Distribution Shift (CINIC-10). Different calibration metrics ( ECE,CWCE,CACE) vary
across CIFAR-10 and CINIC-10, depending on the rejection threshold of Bayesian disagreement (Expected
Information Gain/BALD). The trajectory matches the one for prediction disagreement. We hypothesize
this is because there are few noisy samples in the dataset which would act as a confounder for prediction
disagreement otherwise. Details in §E.2.
25Published in Transactions on Machine Learning Research (10/2022)
0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)0.0500.0750.1000.1250.1500.1750.2000.225Value
0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
ImageNet (In-distribution)
0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)0.0500.0750.1000.1250.1500.1750.2000.225Value
0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)0.0500.0750.1000.1250.1500.1750.2000.225Value
0.2 0.4 0.6 0.8 1.0
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
PACS (Distribution Shift)
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Expected Information Gain/BALD (Bayesian Disagreement)0.000.050.100.150.200.250.30Value
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
(a) Ensemble with TOP
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Expected Information Gain/BALD (Bayesian Disagreement)0.000.050.100.150.200.250.30Value
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Expected Information Gain/BALD (Bayesian Disagreement)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error | (b) Ensemble without TOP (same underlying models)
Figure 4: Rejection Plot of Calibration Metrics for Increasing Bayesian Disagreement In-Distribution (CIFAR-
10) and Under Distribution Shift (CINIC-10). Different calibration metrics ( ECE,CWCE,CACE) vary
across CIFAR-10 and CINIC-10, depending on the rejection threshold of Bayesian disagreement (Expected
Information Gain/BALD). The trajectory matches the one for prediction disagreement. We hypothesize
this is because there are few noisy samples in the dataset which would act as a confounder for prediction
disagreement otherwise. Details in §E.2.
26Published in Transactions on Machine Learning Research (10/2022)
0.2 0.4 0.6 0.8 1.0
Predicted Error (Disagreement Rate)0.000.050.100.150.20Value
0.2 0.4 0.6 0.8 1.0
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
PACS, BeiT-L/16
0.0 0.2 0.4 0.6 0.8
Predicted Error (Disagreement Rate)0.00.10.20.30.4Value
0.0 0.2 0.4 0.6 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
0.0 0.2 0.4 0.6 0.8
Predicted Error (Disagreement Rate)0.00.10.20.30.4Value
0.0 0.2 0.4 0.6 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
PACS, ResNet-152-D
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)0.00.20.40.60.81.0Value
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error |
(a) Ensemble with TOP
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)0.00.20.40.60.81.0Value
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Predicted Error (Disagreement Rate)
Measure
CWCE CACE T est Error ECE | T est Error - Predicted Error | (b) Ensemble without TOP (same underlying models)
Figure 5: Rejection Plot of Calibration Metrics for Increasing Disagreement Under Distribution Shift (PACS
‘photo‘ domain→other domains) for Specific Model Architectures. We use the same encoder weights and
evaluate on an ensemble of 5 models, which were last-layer fine-tuned on PACS. We show ResNet-152-D and
BeiT-L/16. Details in §E.2.
27