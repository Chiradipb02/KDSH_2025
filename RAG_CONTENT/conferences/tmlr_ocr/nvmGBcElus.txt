Published in Transactions on Machine Learning Research (10/2024)
Risk-Controlling Model Selection via Guided Bayesian Opti-
mization
Bracha Laufer-Goldshtein blaufer@tauex.tau.edu
Department of Electrical Engineering
Tel-Aviv University
Adam Fisch fisch@google.com
Google DeepMind
Regina Barzilay regina@csail.mit.edu
Computer Science and Artificial Intelligence Laboratory (CSAIL)
Massachusetts Institute of Technology
Tommi Jaakkola tommi@csail.mit.edu
Computer Science and Artificial Intelligence Laboratory (CSAIL)
Massachusetts Institute of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= nvmGBcElus
Abstract
Adjustable hyperparameters of machine learning models typically impact various key
trade-offs such as accuracy, fairness, robustness, or inference cost. Our goal in this paper
is to find a configuration that adheres to user-specified limits on certain risks while being
useful with respect to other conflicting metrics. We solve this by combining Bayesian
Optimization (BO) with rigorous risk-controlling procedures, where our core idea is to steer
BO towards an efficient testing strategy. Our BO method identifies a set of Pareto optimal
configurations residing in a designated region of interest. The resulting candidates are
statistically verified, and the best-performing configuration is selected with guaranteed risk
levels. We demonstrate the effectiveness of our approach on a range of tasks with multiple
desiderata, including low error rates, equitable predictions, handling spurious correlations,
managing rate and distortion in generative models, and reducing computational costs.1
1 Introduction
Deploying machine learning models in the real-world requires balancing different performance aspects such
as low error rate, equality in predictive decisions (Hardt et al., 2016; Pessach & Shmueli, 2022), robustness
to spurious correlations (Sagawa et al., 2019; Yang et al., 2023), and model efficiency (Laskaridis et al.,
2021; Menghani, 2023). In many cases, we can influence the model’s behavior favorably via hyperparameters
that determine the model configuration. However, selecting a configuration that accurately aligns with user-
specified requirements on test data can be particularly challenging. This complexity is amplified when the
process involves numerous possible configurations that demand significant resources to evaluate, such as
those necessitating the retraining of large neural networks for novel settings.
Bayesian Optimization (BO) is widely used for efficiently selecting configurations of functions that require
expensive evaluation, such as hyperparameters that govern the model architecture or influence the training
procedure (Shahriari et al., 2015; Wang et al., 2022; Bischl et al., 2023). The basic concept behind BO is
to substitute the costly function of interest with a cheap, easily optimized probabilistic surrogate model.
This surrogate is then used to select promising candidate configurations while balancing exploration and
1Our code is available at https://github.com/bracha-laufer/guidebo .
1Published in Transactions on Machine Learning Research (10/2024)
Figure 1: Demonstration of GuideBO for algorithmic fairness with gender as a sensitive attribute (left).
We would like to set the model configuration λ= [λ1,λ2]to minimize the difference in demographic parity,
while bounding the overall prediction error by α. Our method (right): (i) defines a region of interest in the
objective space, (ii) identifies Pareto optimal solutions in this region, (iii) statistically validates the chosen
solutions, and (iv) sets λto the best-performing verified configuration.
exploitation. Beyond single-function optimization, BO has been extended to handle multiple objectives.
In this context, the goal is to find a set of Pareto optimal configurations that represent the best possible
trade-offs for the given objectives (Karl et al., 2022). Additionally, BO can accommodate multiple inequality
constraints (Gardner et al., 2014). Nevertheless, none of these mechanisms provide formal guarantees
on model behavior at test time, and can suffer from unexpected fluctuations from the desired final
performance (Letham et al., 2019; Feurer et al., 2023).
The Learn then Test (LTT) framework (Angelopoulos et al., 2021) addresses model configuration selection
from a different perspective. It establishes a rigorous statistical testing approach for simultaneously con-
trolling multiple risk functions. The procedure is model-agnostic, distribution-free, and yields finite-sample
guarentees. While LTT provides exact theoretical verification, its practical application becomes challenging
when dealing with large configuration spaces. The increased computational costs and potential loss of sta-
tistical power hinder the identification of useful configurations. To mitigate these challenges, the recently
proposed Pareto Testing method (Laufer-Goldshtein et al., 2023) combines the strengths of multi-objective
optimization and statistical testing. The fundamental idea is to leverage multi-objective optimization to sig-
nificantly reduce the space of potential configurations. This approach aims to identify Pareto optimal config-
urationsthatarepromisingcandidatesfortesting. Whilethismethodenhancescomputationalandstatistical
efficiency, the identified subspace may still include configurations that are irrelevant. It may focus on con-
figurations that are either valid yet inefficient, or that highly improbable to meet the constraints. Therefore,
whenconsideringexpansiveconfigurationspaces, thisstrategycanagainbecomecostlyandstatisticallyloose.
In this work, we introduce GuideBO, a new synergistic approach to combine optimization and testing to
achieve efficient model selection under multiple risk constraints. Our approach centers around the concept
of the “region of interest” in the objective space, which aligns with the goal of achieving testing efficiency
while operating within a limited compute budget. To define the region of interest, we consider factors
such as data sample sizes, user-specified limits, and required certainty levels. Consequently, we propose an
adjusted BO procedure, recovering the part of the Pareto front that intersects with the defined region of
interest. The resulting focused optimization procedure recovers a dense set of configurations, representing
candidates that are both effective and likely to pass the test. In the final step, we apply statistical testing
to filter this chosen set and identify highly-performing configurations that exhibit verified control.
We demonstrate that GuideBO is a flexible approach applicable across diverse contexts for both predictive
andgenerativemodels. Iteffectivelytunesvarioustypesofhyperparametersthatimpactthemodel—whether
priortotrainingorpost-training. Specifically, weshowitsapplicabilityinthedomainsofalgorithmicfairness,
robustness to spurious correlations, rate and distortion in Variational Autoencoders (VAEs), accuracy-cost
2Published in Transactions on Machine Learning Research (10/2024)
trade-offs for pruning computations of large-scale Transformer models, and early-time classification in large
language models (LLMs). See Fig. 1 for an example and a high-level illustration of GuideBO.
Contribution. Our main ideas and results can be summarized as follows:
1. We introduce the region of interest in the objective space, which significantly reduces the search space for
candidate configurations, thereby leading to more efficient statistical testing with fewer computations.
2. We define a new BO procedure to identify configurations that are Pareto optimal and lie in the defined
region of interest. These configurations are subsequently validated through statistical testing.
3. Our approach facilitates risk-controlled model selection in complex and costly settings that necessitate
model retraining or involve extensive configuration spaces. We present a broad range of problems, where
our approach can be valuable for valid control and effective optimization of diverse performance aspects,
including classification fairness, predictive robustness, generation capabilities, model compression and
runtime reduction.
4. Through empirical experiments, we demonstrate that GuideBO selects highly efficient and verified
configurations under practical budget constraints, outperforming baselines.
2 Related work
Conformal prediction and risk control. Conformal prediction is a popular model-agnostic and
distribution-free uncertainty estimation framework that returns prediction sets or intervals containing the
true value with high probability (Vovk, 2002; Vovk et al., 2015; 2017; Lei et al., 2013; 2018; Gupta et al.,
2020; Barber et al., 2021). Coverage validity, provided by standard conformal prediction, has recently been
extended to controlling general statistical losses, allowing guarantees in expectation (Angelopoulos et al.,
2022) or with user-defined probability (Bates et al., 2021). Our contribution extends the foundational
work of Angelopoulos et al. (2021), which addresses the broader scenario of controlling multiple risk
functions. This is achieved by selecting an appropriate low-dimensional hyperparameter configuration using
multiple hypothesis testing (MHT). Additionally, we draw upon the recently introduced Pareto Testing
method (Laufer-Goldshtein et al., 2023) that further improves computational and statistical efficiency
by solving a multi-objective optimization (MOO) problem and focusing the testing procedure over the
approximated Pareto optimal set. In this paper, we point out that recovering the entire Pareto front is
redundant and costly and suggest instead to recover a focused part of the front that is aligned with the
purpose of efficient testing. This enables highly-expensive hyperparameter tuning that involves retraining
of large models with a limited compute budget.
Black-boxmodelselectionundermultipleobjectives. Thedemandforautomatingthecreationofma-
chinelearningpipelinesisrapidlyincreasing. Hyperparameteroptimizationaimstostreamlinethisprocessby
fine-tuning model configurations with minimal manual intervention. However, this is a challenging black-box
optimization problem that often involves trade-offs among factors such as predictive performance (Schmucker
et al., 2021), computational time (Wang et al., 2019; Lu et al., 2019), and fairness (Pfisterer et al., 2019;
Martinez et al., 2020). In this context, we focus on model selection with multiple objectives, incorporating
user-specified statistical constraints on certain losses.
Bayesian Optimization (BO). BO is a commonly used sequential model-based optimization technique
to efficiently find an optimal configuration for a given black-box objective function (Shahriari et al., 2015;
Frazier, 2018; Wang et al., 2022). It can be applied to constrained optimization problems (Gardner et al.,
2014) or multi-objective scenarios involving several conflicting objectives (Karl et al., 2022). However,
when used in model hyperparamaeter tuning, the objective functions can only be approximated through
validation data, resulting in no guarantees on test time performance. To account for that we resort to
statistical testing, and utilize the effectiveness of BO to efficiently explore the configuration space and
identify promising candidates for testing.
BO and Conformal Prediction. Several recent works proposed to integrate conformal prediction into
BO in order to improve the optimization process under model misspecification and in the presence of
3Published in Transactions on Machine Learning Research (10/2024)
observation noise (Stanton et al., 2023; Salinas et al., 2023). These works go in a different direction from our
approach, guaranteeing coverage over the approximation of the surrogate model, while ours provides validity
on configuration selection. Another recent work (Zhang et al., 2023) utilizes online conformal prediction for
maintaining a safety violation rate (limiting the fraction of unsafe configurations found during BO), which
differs from our provided guarantees and works under the assumption of a Gaussian observation noise.
Multi-Objective Optimization (MOO). Simultaneously optimizing multiple black-box objective
functions was traditionally performed with evolutionary algorithms, such as NSGA-II (Deb et al., 2002),
SMS-EMOA (Emmerich et al., 2005) and MOEA/D (Zhang & Li, 2007). Due to the need for numerous eval-
uations, evolutionary methods can be costly. Alternatively, BO methods are more sample efficient and can
be combined with evolutionary algorithms. Various methods were proposed exploiting different acquisition
functions (Knowles, 2006; Belakaria et al., 2019; Paria et al., 2020) and selection mechanisms, encouraging
diversity in the objective space (Belakaria et al., 2020) or in the design space (Konakovic Lukovic et al.,
2020). The central idea behind our approach is to design a Multi-Objective BO (MOBO) procedure that
recovers a small set of valid and efficient configurations. Subsequently, we calibrate this chosen set using
MHT (Angelopoulos et al., 2021).
Additional related work is given in Appendix A.
3 Problem formulation
Consider an input X∈Xand an associated label Y∈Ydrawn from a joint distribution pXY∈PXY2.
We learn a model fλ:X→Y, whereλ∈Λ⊆Rnis ann-dimensional hyperparameter that determines the
model configuration. The model weights are optimized over a training set Dtrainby minimizing a given loss
function, while the hyperparameter λdetermines different aspects of the training procedure or the final
setting of the model. For example, λcan weigh the different components of the training loss function, affect
the data on which the model is trained, or specify the final mode of operation in a post-processing procedure.
We wish to select a model configuration λaccording to different, often conflicting performance aspects, such
as low error rate, fairness across different subpopulations and low computational costs. In many practical
scenarios, we would like to constrain several of these aspects with pre-specified limits to guarantee a desirable
performance in test time. Specifically, we consider a set of objective functions of the form ℓ:PXY×Λ→R.
We assume that there are cconstrained objective functions ℓ1,....,ℓc, whereℓi(λ) =EpXY[Li(fλ(X),Y;λ)]
andLi:Y×Y× Λ→Ris a loss function. In addition, there is a free objective function ℓfreedefining a single
degree of freedom for minimization. The selection of λis carried out based on two disjoint data subsets: (i)
a validation setDval={Xi,Yi}k
i=1and (ii) a calibration set Dcal={Xi,Yi}k+m
i=k+1. We will use the validation
data to identify a set of candidate configurations, and the calibration data to calibrate the identified set. The
constraints are formulated by an (α,δ)-risk control criterion as was defined by Angelopoulos et al. (2021):
PDcal(ℓi(λ)≤αi)≥1−δ,∀i∈{1,...,c}, (1)
whereαiis the upper bound of the i-th objective function, and δis the desired confidence level, both selcted
by the user. Note that the probability in (1) is defined over the randomness of the calibration data Dcal,
namely ifδ= 0.1, then the selected configuration will satisfy the constraints at least 90%of the time across
different calibration datasets.
We provide here a brief example of our setup in the context of algorithmic fairness and derive additional
applications in §6. In many cases, we wish to increase the fairness of the model without significantly
sacrificing performance. For example, we would like to encourage similar true positive rates across different
subpopulations, while constraining the expected error. One approach to enhancing fairness involves adding
fairness-promoting terms to the standard cross-entropy loss function (Lohaus et al., 2020; Padh et al., 2021;
Chuang & Mroueh, 2020). In this case, λcontains the weights assigned to each loss term, which are then
combined to form the overall training loss. Adjusting these weights leads to different accuracy-fairness
trade-offs for the resulting model. Our goal is to select a configuration λthat optimizes fairness, while
guaranteeing that the overall error would not exceed a certain limit with high probability.
2We also address unsupervised learning problems, which do not involve labels. For generality, our problem formulation uses
bothXandY.
4Published in Transactions on Machine Learning Research (10/2024)
4 Background
In our method, two critical components play a central role: optimization of multiple objectives and
statistical testing for configuration selection. We hereby provide a short overview on these topics.
Multi-Objective Optimization (MOO). Consider an optimization problem over a vector-valued func-
tionℓ(λ) = (ℓ1(λ),...,ℓd(λ))consisting of dobjectives. When dealing with conflicting objectives, there is no
singleoptimalsolutionthatsimultaneouslyminimizesallobjectives. Instead, thereisasetofoptimalconfigu-
rationsrepresentingdifferenttrade-offsamongthegivenobjectives. Thisisthe Pareto optimal set , definedby:
Λp={λ∈Λ :{λ′∈Λ :λ′≺λ,λ′̸=λ}=∅}, (2)
whereλ′≺λdenotes that λ′dominatesλif for every i∈ {1,...d}, ℓi(λ′)≤ℓi(λ), and for some
i∈ {1,...d}, ℓi(λ′)< ℓi(λ). Accordingly, the Pareto optimal set consists of all points that are not
dominated by any point within Λ. Given an approximated Pareto front ˆP, a common quality measure is
the hypervolume indicator (Zitzler & Thiele, 1998) defined with respect to a reference point r∈Rd:
HV(ˆP;r) =/integraldisplay
Rd1H(ˆP;r)dz, (3)
whereH(ˆP;r) ={z∈Rd:∃p∈ˆP:p≺z≺r}and 1H(ˆP,r)is the Dirac delta function that equals 1
ifz∈H(ˆP;r)and 0 otherwise. An illustration is provided in Fig. B.1. The reference point defines the
boundaries for the hypervolume computation. It is usually set to the nadir point that is defined by the worst
objective values, so that all Pareto optimal solutions have positive hypervolume contributions (Ishibuchi
et al., 2018). For example, in model compression with error and cost as objectives, the reference point can
be set to (1.0,1.0), since the maximum error and the maximum normalized cost equal 1.0. The hypervolume
indicator measures both the individual contribution of each solution to the overall volume, and the global
diversity, reflecting how well the solutions are distributed. It can be used to evaluate the contribution of a
new point to the current Pareto front approximation, defined as the Hypervolume Improvement (HVI):
HVI (ℓ(λ),ˆP;r) =HV(ℓ(λ)∪ˆP;r)−HV(ˆP;r). (4)
whereℓ(λ)∪ˆPdenotes the extended set consisting of the current approximated Pareto front and a new
pointℓ(λ). The expression in (4) represents the increase in hypervolume achieved by adding a new point
ℓ(λ). The hypervolume indicator serves both as a performance measure for comparing different algorithms
and as a score for maximization in various MOO methods (Emmerich et al., 2005; 2006; Bader & Zitzler,
2011; Daulton et al., 2021).
BO.BO is an effective method for optimizing black-box objective functions that are costly to evaluate
(e.g. selecting the hyperparameters of large neural networks). These methods employ a surrogate model to
approximate the expensive objective function and iteratively select new configurations using an acquisition
function that balances exploration and exploitation. Formally, we assume a total budget of Niterations,
representing the maximum number of allowed function evaluations. We start with an initial set of random
configurationsC0={λ0,...,λN0}and their associated objective values L0={ℓ(λ1),...,ℓ (λN0)}, where
N0< N. We then perform N−N0iterations, each time adding one configuration to the current set
Cn,1≤n≤N−N0, where|Cn|denotes the set size. Commonly, a Gaussian Process (GP) (Williams
& Rasmussen, 2006) serves as a surrogate model, providing an estimate with uncertainty given by the
Gaussian posterior. We assume a zero-mean GP prior g(λ)∼N (0,k(λ,λ)), characterized by a kernel
functionκ: Λ×Λ→R. Letκ∈R|Cn|denote the vector of covariance values κi=κ(λ,λi)between
a new point λand a given point λj∈Cn, and Ka|Cn|×|Cn|matrix with elements Kij=κ(λi,λj),
i,j∈{1,...,|Cn|}. In addition, we define the label vector q∈R|Cn|consisting of the given objective values
qi=ℓ(λi),i∈{1,...,|Cn|}. Based on these definitions, the posterior distribution of the GP is given by
p(g|λ,Cn,Ln) =N(µ(λ),Σ(λ,λ)), withµ(λ) =κ(K+σ2I)−1qandΣ(λ,λ) =κ(λ,λ)−κT/parenleftbig
K+σ2I/parenrightbig−1κ.
Hereσ2is the observation noise variance, i.e. ℓ(λi)∼N(g(λi),σ2). The next configuration for evaluation,
is selected by optimizing an acquisition function defined on top of the surrogate model. Common acquisition
functions include: probability of improvement (PI) (Kushner, 1964), expected improvement (EI) (Močkus,
5Published in Transactions on Machine Learning Research (10/2024)
1975), and lower confidence bound (LCB) (Auer, 2002). For MOO, a GP is fitted to each objective. Then,
one approach is to perform scalarization (Knowles, 2006), which converts the problem back into a single-
objective optimization, allowing the use of standard acquisition functions. Alternatively, one can employ
modified acquisition functions designed for the multi-objective context, such as expected hypervolume
improvement (EHVI) (Emmerich et al., 2006) and predictive entropy search for multi-objective optimization
(PESMO) (Hernández-Lobato et al., 2016). After selecting a new configuration, it is evaluated and added
to the updated set Ct+1. This process is repeated until the maximum number of iterations is reached.
Learn then Test (LTT) & Pareto Testing. Angelopoulos et al. (2021) have introduced LTT, which is
a statistical framework for configuration selection based on multiple hypothesis testing (MHT). Given a set
of constraints of the form of Eq. (1), a null hypothesis is defined as Hλ:∃iwhereℓi(λ)> αii.e., that at
least one of the constraints is notsatisfied. For a given configuration, we can compute the p-value under the
null-hypothesis based on the calibration data. If the p-value is lower than the significance level δ, the null hy-
pothesis is rejected and the configuration is declared to be valid. When testing multiple model configurations
simultaneously, this becomes an MHT problem. In this case, it is necessary to apply a correction procedure
to control the family-wise error rate (FWER), i.e. to ensure that the probability of one or more wrong
rejections is bounded by δ. In large configuration spaces, this can be computationally demanding and result
in inefficient testing. In order to mitigate these challenges, Pareto Testing was proposed (Laufer-Goldshtein
et al., 2023), where the testing is focused on the most promising configurations identified using MOO.
Accordingly, only Pareto optimal configurations are considered and are ranked by their approximated
p-values from low to high risk. Then, Fixed Sequence Testing (FST) (Holm, 1979) is applied over the
ordered set, sequentially testing the configurations with a fixed threshold δuntil failing to reject for the first
time. Although Pareto Testing demonstrates enhanced testing efficiency, it recovers the entire Pareto front,
albeit focusing only on a small portion of it during testing. Consequently, the optimization budget is not
directly utilized in a way that enhances testing efficiency, putting an emphasis on irrelevant configurations
on one side and facing an excessive sparsity within the relevant area on the other, as illustrated in Fig. 2.
5 Method
Our approach involves two main steps: (i) performing BO to generate a small set of potential configurations,
and (ii) applying MHT over the candidate set to identify valid configurations. Considering the shortcomings
of Pareto Testing, we argue that the two disjoint stages of optimization followed by testing are suboptimal,
especially for resource-intensive MOO. As an alternative, we propose adjusting the optimization procedure
for better testing outcomes by focusing only on the most relevant parts in the objective space. To accomplish
this, we need to (i) specify a region of interest guided by our testing goal, and (ii) establish a BO procedure
capable of effectively identifying configurations within the defined region. In the following we describe these
steps in details.
5.1 Defining the Region of Interest
We aim to define a region of interest within the objective space Rc+1, where we seek to identify candidate
configurations that are likely to be valid and efficient during the process of MHT. We start with the case
of a single constraint ( c= 1). Recall that in the testing stage we define the null hypothesis Hλ:ℓ(λ)>α
for a candidate configuration λ, and compute a p-value for a given empirical loss over the calibration data
ˆℓcal(λ) =1
m/summationtextk+m
j=k+1ℓ(Xj,Yj;λ). A valid p-value pλhas to be super-uniform under the null hypothesis, i.e.
P(pλ≤u)≤u, for allu∈[0,1]. As demonstrated in (Angelopoulos et al., 2021), a valid p-value can be
computed based on concentration inequalities that quantify the proximity of the sample loss to the expected
population loss. If the loss is bounded by 1, we can apply Hoeffding’s inequality to derive the following
p-value (see Appendix B.1):
pHF
λ:=e−2m(α−ˆℓcal(λ))2
+. (5)
where (·)+= max(·,0). For a given significance level δ, the null hypothesis is rejected (the configuration is
declared to be risk-controlling), when pHF
λ<δ. By rearranging (5), we obtain that the maximum empirical
6Published in Transactions on Machine Learning Research (10/2024)
lossˆℓ(λ)that can pass the test with significance level δis given by (see Appendix B.1):
αmax=α−/radicalbigg
log (1/δ)
2m. (6)
As an example, consider the error rate as a loss function, and assume that we would like to bound the error
rate by 5%(α= 0.05), with a significance level of δ= 0.1. By (6), if the empirical loss of a calibration set
of sizem= 5000is up toαmax= 4%, then we have enough evidence to declare that this configuration is
safe and its error will not exceed 5%on new unseen data drawn from the same distribution.
In the BO procedure, we are interested in identifying configurations that are likely to be both valid and
efficient. On the one hand, in order to be valid the loss must not exceed αmax. On the other hand,
from efficiency considerations, we would like to minimize the free objective as much as possible. This
means that the constrained loss should be close to αmax(from bellow) due to the inverse relation between
the free objective and the constrained objective. An illustration demonstrating this idea is provided
in Fig. 2, where the irrelevant regions are: (i) the brown part on the right where the configurations are
not satisfying the constraint, and (ii) the green part on the left where the configurations are not effectively
minimizing ℓ2. Ideally, we would like to find configurations with expected loss equal to the limiting testing
thresholdαmax. However, during optimization we can only evaluate the loss over a finite-size validation
data with|Dval|=ksamples. To account for that, we construct an interval [ℓlow,ℓhigh]aroundαmax
based on the size of the validation data. In this region, we wish to include empirical loss values that are
likelyto correspond to an expected value of αmaxbased on the evidence provided by the validation data.
Letˆℓopt
1(λ) =1
k/summationtextk
j=1ℓ1(Xj,Yj;λ)denote the empirical loss computed over Dval. We define the region
R(α,k,m,δ,γ )containing ˆℓopt
1(λ)values that are likely to be obtained under ℓ1(λ) =αmaxwith at least
1−2γprobability. Specifically, there exists a region R(α,k,m,δ,γ ), withγ∈(0,0.5], such that:
P/parenleftig
ˆℓopt
1(λ)∈R(α,k,m,δ,γ )/vextendsingle/vextendsingleℓ1(λ) =αmax/parenrightig
≥1−2γ. (7)
For instance, by applying Hoeffding’s inequality, we can derive the following region of interest:
R(α,k,m,δ,γ ) =
αmax−/radicalbigg
log (1/γ)
2k/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
ℓlow,αmax+/radicalbigg
log (1/γ)
2k/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
ℓhigh
. (8)
Note that setting γis an empirical decision that is independent of both the MHT procedure and the chosen
significance level δ. For small γthe region expands, accommodating more optional configurations but with
a lower density. Conversely, a larger γproduces a smaller region, leading to denser sampling around the
limiting value. Also note that, whenever kincreases, the width of the region decreases, reflecting a growing
confidence that the observed losses are representative of actual expected loss. In practice, we use the tighter
Hoeffding-Bentkus inequality for the p-value computation in Eq. (5) and for defining the region of interest
by Eqs. (6) and (8) (see Appendix B.1).
In the case of multiple constraints, the null hypothesis is defined as Hλ:∃iwhereℓi(λ)>αi, i.e. that at
least one of the constraints is not satisfied. A valid p-value is given by pλ= maxi∈{1,...,c}pλ,i, wherepλ,iis
the p-value corresponding to the i-th constraint (see Appendix. B.2). Consequently, we define the region of
interest in the multi-constraint case as the intersection of the individual regions (as illustrated in Fig. B.2):
R(α,k,m,δ,γ ) =c/intersectiondisplay
i=1R(αi,k,m,δ,γ );α= (α1,...,αc) (9)
5.2 Local Hypervolume Improvement
Given our definition of the region of interest, we derive a BO procedure that recovers Pareto optimal points
in the intersection of R(α,k,m,δ,γ )andP. Our key idea is to use the HVI in Eq. (4) as an acquisition
function, while modifying it to capture only the region of interest. To this end, we properly define the
reference point r∈Rc+1to enclose the desired region.
7Published in Transactions on Machine Learning Research (10/2024)
Figure 2: Left: Illustration of the different parts of the Pareto front. The green region consists of
configurations that are low risk ( ℓ1≪α) but inefficient in terms of the free objective ℓ2. The brown region
consists of configurations that are efficient but high risk ( ℓ1≫α) and cannot pass the test. In the middle,
we define the region of interest containing configurations that are likely to be both valid and efficient. Right:
comparing GuideBO to full Pareto front recovery for optimization budget N= 10. In the full Pareto front
method there is no control on the distribution of the configurations over the front, while GuideBO focuses
on the region of interest. As a result, comparing the chosen valid configurations (marked by v), there exists
a noticeable advantage in favor of GuideBO in minimizing ℓ2.
Figure 3: GuideBO for two objectives. ℓ1is
controlled at αwhileℓ2is minimized. The
shaded area corresponds to our defined
region of interest. A reference point (in
red) is defined accordingly to enclose the
region of interest.Recall that the reference point defines the upper limit in each
direction. Therefore, for the constrained dimensions we set
ri=ℓhigh
i, i∈{1,...,c}using the upper bound in Eq. (8). As
fortheunconstraineddimension rc+1, wecanusethemaximum
possible value of ℓfree. However, this approach may unnecessar-
ily expand the defined region to include the green area shown
in Fig. 2. In this region, the configurations meet the constraint
but are sub-optimal with respect to the free objective. Instead,
we determine the final dimension based on the lower limiting
values. Specifically, we set rc+1to the point on the free axis
that corresponds to the intersection of the lower limits of the
constrained dimensions:
rc+1= ˆgfree(λfree) (10)
where
λfree= arg min
λ/vextenddouble/vextenddouble[ˆg1(λ),..., ˆgc(λ)]−[ℓlow
1,...,ℓlow
c]/vextenddouble/vextenddouble
2(11)
where we use the GP posterior mean as our objective estimator, i.e. ˆg=µ. As a result, we obtain the
following reference point:
r=/parenleftig
ℓhigh
1,...,ℓhigh
c,ˆgfree(λfree)/parenrightig
. (12)
We select the next configuration by maximizing the HVI (4) with respect to this reference point:
λn= arg max
λHVI (ˆg(λ),ˆP;r), (13)
which leads to recovering only the relevant section, rather than the entire front. We evaluate the objective
functions on the new selected configuration, and update our candidate set accordingly. The BO process
continues iterating until the maximum budget Nis reached. The resulting set of candidate configurations
is denoted asCBO. Our proposed BO method, GuideBO, is summarized in Algorithm 1 and illustrated in
Fig. 3 forc= 1.
8Published in Transactions on Machine Learning Research (10/2024)
Algorithm 1 GuideBO: Testing Guided Bayesian Optimization
Definitions: ℓ1,...,ℓcandℓfreeare the objective functions, g1,...,gcandgfreeare their associated sur-
rogate models. ℓlow
1,...,ℓlow
candℓhigh
1,...,ℓhigh
care the lower and upper bounds, respectively, for the first
cobjectives.C0={λ0,...,λN0}is an initial pool of configurations and L0={ℓ(λ1),...,ℓ(λN0)}are the
associated objectives. Nis our total budget. ParetoFront() filters Pareto optimal objective values.
1:function BO(ℓ,C0,L0,{ℓlow
1,...,ℓlow
c},{ℓhigh
1,...,ℓhigh
c},N)
2:Nmax←N−|C 0|
3: r←/parenleftig
ℓhigh
1,...,ℓhigh
c,maxλ∈C0ℓfree(λ)/parenrightig
▷Initialize reference point.
4:forn= 0,1,2,...,N max−1do
5: Fitˆgon (Cn,Ln) ▷Fit surrogate models.
6:rc+1←ˆgfree(λfree),λfree= arg min
λ/vextenddouble/vextenddouble[ˆg1(λ),..., ˆgc(λ)]−[ℓlow
1,...,ℓlow
c]/vextenddouble/vextenddouble
2▷Update ref. point.
7: ˆP←ParetoFront (Ln) ▷Filter Pareto front according to Eq. (2).
8:λn+1= arg maxλHVI (ˆg(λ),ˆP;r). ▷Optimize acquisition function.
9: Evaluateℓ(λn+1) ▷Evaluate new configuration.
10:Cn+1←Cn∪λn+1. ▷Add new configuration.
11:Ln+1←Ln∪ℓ(λn+1). ▷Add new objective values.
12:CBO←CNmax
13:returnCBO
Note that in MOBO it is common to use an HVI-based acquisition function that also takes into account
the predictive uncertainty as in EHVI (Emmerich et al., 2005) and SMS-EGO (Ponweiser et al., 2008).
However, our preliminary runs showed that these approaches do not work well in the examined scenarios
with small budget ( N∈[10,50]), as they often generated points outside the region of interest. Similarly,
for these scenarios the random scalarization approach, proposed in (Paria et al., 2020), was less effective for
generating well-distributed points inside the desired region.
5.3 Testing the Final Selection
We follow (Angelopoulos et al., 2021; Laufer-Goldshtein et al., 2023) for testing the selected candidate
set. Prior to testing we filter and order the configurations in the set CBO. Specifically, we retain only
Pareto optimal configurations from CBO, and then sort the remaining configurations by increasing p-values,
approximated by Dval. Next, we recompute the p-values based on Dcaland perform FST, where we start
testing from the first configuration and continue until the first time the p-value exceeds δ. As a result, we
obtain the validated set Cvalid, and choose a configuration minimizing the free objective:
λ∗= arg min
λ∈Cvalidℓfree(λ). (14)
The method is summarized in Algorithm C.1. As a consequence of (Angelopoulos et al., 2021; Laufer-
Goldshtein et al., 2023) we achieve a valid risk-controlling configuration, as we now formally state.
Theorem 5.1. LetDval={Xi,Yi}k
i=1andDcal={Xi,Yi}k+m
i=k+1be two disjoint datasets. Suppose the
p-valuepλ, derived fromDcal, is super-uniform under Hλfor allλ. Then the output λ∗of Algorithm C.1
satisfies Eq. (1).
The proof is provided in Appendix B.3. Note that in situations where we are unable to identify any
statistically valid configuration (i.e., Cvalid=∅), we setλ=null. To avoid this situation, the user
should select limits α1,...,αcthat are likely to be feasible. In practice, this can be achieved using
the initial pool of configurations C0, which is generated at the start of the BO procedure. This repre-
sentative set provides an indication of the possible achievable limits. Specifically, the user may select
αi∈[minλ∈C0ℓi(λ),maxλ∈C0ℓi(λ)],i∈{1,...,c}, and can further refine this choice during the BO iter-
ations as more function evaluations are accumulated.
9Published in Transactions on Machine Learning Research (10/2024)
6 Applications
We demonstrate the effectiveness of GuideBO across various tasks with diverse objectives. In each setting,
the definition of λvaries, affecting the model differently either during or after training.
Classification Fairness. In many classification tasks, it is important to take into account the behavior of
the predictor with respect to different subpopulations. Assuming a binary classification task and a binary
sensitive attribute a={−1,1}, we consider the Difference of Demographic Parity (DDP) as a fairness
score (Wu et al., 2019):
DDP (f) =E/bracketleftbig
1f(x)>0|a=−1/bracketrightbig
−E/bracketleftbig
1f(x)>0|a= 1/bracketrightbig
. (15)
We define the following loss, parameterized by λ= [λ1,λ2], which consists of two regularization terms that
prompt fairness:
R(f;λ) =BCE (f) +λ1·[DDP (f) +λ2·\MixUP (f), (16)
where BCE (f)is the standard binary cross-entropy loss, and [DDP (f)and\MixUP (f)reguralize the model’s
fairness. [DDP (f)is the hyperbolic tangent relaxation of (15) (Padh et al., 2021), and \MixUP (f)is a
mixup regularization interpolating samples between groups (Chuang & Mroueh, 2020) (see Appendix D
for further details). Changing the values of λleads to different models that trade-off accuracy for fairness.
In this setup, we have a 2-dimensional hyperparamter λand two objectives: (i) the error of the model
ℓerr(λ) =E/bracketleftbig
1fλ(X)̸=Y/bracketrightbig
, and (ii) the DDP defined in Eq. (15) ℓddp(λ) =DDP (fλ).
Classification Robustness. Predictors often rely on spurious correlations found in the data (such
as background features), which leads to significant performance variations among different subgroups.
Recently, Izmailov et al. (2022) demonstrated that models trained using expected risk minimization
surprisingly learn core features in addition to spurious ones. Accordingly, they proposed to enhance model
robustness by retraining the final layer on a balanced dataset. We modify their approach to generate
different configurations, balancing the trade-off between robustness to differences in subpopulations and
overall performance across the entire population.
Given a datasetD(either the training set or a part of the validation set), we define a parameterized dataset
Dλas follows. Suppose the data consists of samples (X,Y,G ), whereG∈Gis the group label and Gis the
set of all groups present in the data. We denote by λa|G|-dimensional hyperparameter combination that
lies in the|G|-1 probability simplex. The vector λconsists of the probabilities of each group appearing in
Dλ. To createDλ, we first sample the group membership label according to λ, and then uniformly sample
an example from the chosen group, allowing for repetition in sampling. Consequently, λis a|G|-dimensional
hyperparameter that controls the proportion of each group in Dλ. The dataset is considered evenly balanced
acrossgroupswhenallprobabilitiesareequalto 1/|G|. It isequivalentto theoriginaldatasetwhen λmatches
the prior probability over G. We define two objective functions: (i) the average error ℓerr(λ) =E/bracketleftbig
1fλ(X)̸=Y/bracketrightbig
,
and (ii) the worst error over all subgroups ℓworst-err (λ) = maxg∈GE/bracketleftbig
1fλ(X)̸=Y|G=g/bracketrightbig
.
Robust and Selective Classification. We also examine the case of selective classification and robustness.
The selective classifier can abstain from making a prediction when it is unsure (Geifman & El-Yaniv, 2017).
Specifically, the model abstains when its confidence is lower than a threshold τ, i.e.fλ(x)< τ, where
fλ(x)denotes the probability of the predicted class. This approach can potentially enhance prediction
performance by trading-off coverage, defined as the proportion of the population for which the classifier
provides a prediction. In this case, we have a |G|+ 1-dimensional hyperparameter λ′= (λ,τ)and an
additional objective function of the mis-coverage rate ℓmis-cover (λ′) =E/bracketleftbig
1fλ(x)<τ/bracketrightbig
.
VAE. Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) are generative
models that leverage a variational approach to learn the latent variables underlying the data, and can
generate new samples by sampling from the latent prior distribution. We focus on a β-VAE (Higgins et al.,
2016), which balances the reconstruction error (distortion) and the Kullback Leibler (KL) divergence (rate):
R(f;β) =Epd(x)/bracketleftbig
Eqϕ(z|x)[−logpθ(x|z)]/bracketrightbig
+β·Epd(x)[DKL(qϕ(z|x)||p(z))], (17)
where z∈RDis the latent embedding, fconsists of an encoder qϕ(z|x)and a decoder pθ(x|z), parameterized
byϕandθ, respectively, and p(z)is the latent prior distribution. Generally, models with low distortion
10Published in Transactions on Machine Learning Research (10/2024)
perform high-quality reconstruction but generate less realistic samples and vice versa. We define the
hyperparameter λ= (β,D)consisting of the KL penalty strength βand the latent dimension D. We specify
two objectives ℓrecon(f)andℓKLD(f)defined by the left and right terms in (17), respectively.
Transformer Pruning. We adopt the multi-dimensional transformer pruning scheme proposed in (Laufer-
Goldshtein et al., 2023), which involves three strategies for reducing computational complexity: (i) token
pruning, removingunimportanttokensfromtheinputsequence, (ii)layerearly-exiting, computingpartofthe
model’slayersforeasyexamples,and(iii)headpruning, removingaportionofattentionheadsfromthemodel
architecture. We obtain λ= (λ1,λ2,λ3)with the three thresholds controlling the pruning strength in each
dimension, and consider two objectives: (i) the accuracy difference between the full model and the pruned
modelℓdiff-acc (λ) =E/bracketleftbig
( 1f(X)=Y− 1fλ(X)=Y)+/bracketrightbig
and (ii) the respective cost ratio ℓcost(λ) =E/bracketleftig
C(fλ(X))
C(f(X))/bracketrightig
.
Early Time Classification. We adapt the early time classification scheme proposed by Ringel et al.
(2024) for predicting the label of a given input data stream as quickly as possible. Specifically3, we focus
on employing LLMs for the task of reading comprehension, where the goal is to analyze a long document
(given as context) and select an answer to a provided question. Let πt(X)denote a heuristic confidence
measure of the prediction made based on the input Xreceived until time t(e.g. the maximum predicted
probability). We define the stopping-time for input Xasτ(X) ={mint:πt(X)≥λtort=tmax}. In
this setting, the hyperparameter λ= (λ1,λ2,...,λtmax)consists of the thresholds for all possible stopping
timest= 1,...,tmax. We have two objectives: (i) the accuracy difference between the full-time prediction
and the early-time prediction ℓdiff-acc (λ) =E/bracketleftbig
( 1f(X)=Y− 1fλ(X)=Y)+/bracketrightbig
and (ii) the normalized halt time
ℓtime(λ) =E[τ(X)/tmax].
7 Experiments
We describe the experimental setup and present our main results. Further experimental details, as well as
additional results are provided in Appendixes D and E, respectively. In the experiments we demonstrate
the efficiency of the proposed method compared to baselines in two main ways: (i) For a fixed budget, we
show that GuideBO achieves the lowest values for the free objective function compared to baselines; (ii) For
a fixed level of the objective function, we show that GuideBO requires the smallest budget to achieve that
level. Additional ablation studies are conducted to highlight the robustness of GuideBO.
7.1 Baselines
We define several baselines. We emphasize that in the second testing stage, both GuideBO and the baselines
follow the same testing procedure that guarantees risk control. The baselines differ only in their optimization
mechanisms during the first stage; thus, they can all be considered variants of Pareto Testing (Laufer-
Goldshtein et al., 2023). We define two simple baselines and three multi-objective optimizers aimed at
recovering the full Pareto front:
•Uniform - defines a uniform grid of configurations in the hyperparameter space.
•Random - a uniform random sampling for n= 1, and Latin Hypercube Sampling (LHS) (McKay et al.,
2000) forn>1.
•HVI- uses the same acquisition function as in GuideBO, defined in Eq. (4). The key difference is that
the reference point is defined in the standard way by the maximum possible loss values, rather than our
focused reference point (12).
•EHVI(Emmerichetal.,2006)-similarto HVIbutincludesuncertaintyinthehypervloumecomputation.
Here too the reference point is defined by the maximum possible loss values.
•ParEGO (Knowles, 2006; Cristescu & Knowles, 2015) - uses random scalarization with Tchebycheff
function to convert the multi-objective function into a single-objective, then employs EI as the acquisition
function. We use the Smac3implementation (Lindauer et al., 2022).
3In addition, We provide results for the task of early time classification of structured time series data, as described in
Appendix D.
11Published in Transactions on Machine Learning Research (10/2024)
Table 2: Tasks Details
Task n (ℓ1,...,ℓfree) (bestℓ1,..., worstℓfree) (worstℓ1,..., bestℓfree)N N 0
Fairness 2 (Err., DDP) (0.154, 0.145) (0.225, 0.01) 10 5
Robustness 4 (Avg. Error, Worst Err.) (0.045, 0.62) (0.089, 0.11) 30 20
Selective & Robust. 5(Avg. Error, Mis-cover., Worst Err.) (0.045, 0.0, 0.62) (0.089, 0.0, 0.11) 30 20
VAE 2 (Recon. Err., KLD) (0.001, 88) (0.07, 0.001) 10 5
Pruning 3 (Acc. Difference, Rel. Cost) (0.0, 1.0) (0.8, 0.0) 50 30
Early-Time Class. 10 (Acc. Difference, Halt Time) (0.0, 1.0) (0.12, 0.1) 50 30
7.2 Datasets
Here we describe the datasets used for each task. Table 1 summarizes the number of samples for each
dataset according to the different splits (train/validation/calibration/test). We use the following datasets:
Table 1: Datasets Details
Dataset Train Validation Calibration Test
Adult 32,559 3,618 4 ,522 4,523
CelebA 162,770 19,867 9 ,981 9,981
MNIST 50,000 10,000 5 ,000 5,000
AG News 120,000 2,500 2 ,500 2,600
Quality - 1,537 1 ,536 1,536Fairness. We use the Adult(Dua et al., 2017) dataset,
which consists of samples of individuals with 14features
as an input. The goal is to predict whether their annual
income is above 50k$. Gender is considered as a sensitive
attribute.
Robustness + Robust and selective classifica-
tion.We use CelebA(Lin et al., 2019) and consider a bi-
nary prediction task of whether a person has a blond hair.
The spurious correlation is associated with the gender at-
tribute, resulting in |G|= 4groups: (blond, female), (blond, male), (non-blond, female), (non-blond, male).
VAE.We use the MNISTdataset (LeCun, 1998), which consists of grayscale images of handwritten digits.
Pruning. We use AG News (Zhang et al., 2015) dataset where the task is to predict the category (out of
four options) of news articles based on their content.
Early-Time Classification. We use the QuALITY dataset (Pang et al., 2022), which consists of triplets
with a question, multiple choice answers, and a long context, along with the corresponding correct choice.
The long context is partitioned into tmax= 10segments.
7.3 Evaluation
We emphasize again the purpose of each data split. The training data is used to learn the model parameters.
The validation data is used for selecting candidate hyperparameter configurations, using either GuideBO
(Algorithm 1) or the baseline procedures. The same data is also used for ordering the chosen configurations
before testing. The calibration data is used for the FST procedure over the ordered set of chosen configura-
tions. The final selected λ∗(14) determines the model setup. Lastly, the performance of the selected model
is assessed on the test dataset.
Since the same testing procedure is applied across all methods, the chosen configuration is guaranteed to
satisfy the specified constraints, as we verify empirically (see Fig. E.1). Therefore, our primary metrics for
evaluating the efficiency of each method are: (i) its ability to minimize the free objective function within a
given budget, and (ii) its capacity to reach certain levels of the free objective with the least budget.
We repeat the experiments with 5random seeds over the optimization procedure. For each seed, we further
generate 20random splits of calibration and test subsets. Accordingly, we obtain 5×20 = 100 random
trials, and report the mean and standard deviation across all trials. For each task, we choose the values
ofαaccording to the objective values obtained for the initially generated configurations. Table 2 lists the
range values for each objective. We select values that lie within the range defined by these extreme points,
ensuring they are not too close to either boundary. This is because values that are too small may not
be statistically achievable, while excessively large values can be trivially satisfied, with tighter control not
significantly improving the free objective. We set δ= 0.1andγ= 0.01.
7.4 Results
Minimization of the free objective function for a given budget. We examine the fol-
lowing scenarios: Fairness - error is controlled and DDP is minimized; Robustness - avg. er-
12Published in Transactions on Machine Learning Research (10/2024)
(a) Fairness
 (b) Robustness
 (c) Selective Robustness
(d) VAE
 (e) Pruning
 (f) Early Classification
Figure 4: The values of the free objective functions across tasks and different limits. The objectives are
evaluated overDtestfor the configuration that was chosen by each method. GuideBO consistently surpasses
the baselines in nearly all cases. In contrast, the performance of the baselines exhibits significant variability.
ror is controlled and worst error is minimized; Robustness and selective classification - error
and miscoverage are controlled while worst error is minimized; VAE- reconstruction error is con-
trolled and KLD is minimized; Pruning - error difference is controlled and relative cost is min-
imized;Early time classification - error difference is controlled and relative cost is minimized.
Figure 5: Rank count across settings and the average
rank. GuideBO is ranked first in almost all cases,
while the baselines have an inconsistent performance.Results are presented in Fig. 4 showing the values
of the free objective function, evaluated over test
data, across all tasks and αlevels. To summarize
the results, we rank the methods in each scenario,
and report the counts of all rankings, as well as the
average rank in Fig. 5. We observe that GuideBO
consistently outperforms all baselines in nearly all
cases. The multi-objective baselines outperform
the simple baselines that distribute configurations
across the entire space. Moreover, the baselines
exhibit inconsistent performance, delivering satis-
factory results for certain tasks or specific αvalues,
but falling short in others. This inconsistency can
be attributed to the arbitrary distribution of the
configurations for the baselines. As a result, we
sometimes randomly obtain configurations that are
close to the testing limit (thus efficient), while at
other times, the closest configuration is relatively
far (thus inefficient). In contrast, GuideBO achieves a dense sampling of the relevant part of the Pareto
front, leading to more precise and stable control across various conditions.
13Published in Transactions on Machine Learning Research (10/2024)
Additional Results. We first show that the constraints are satisfied in all cases by both GuideBO and
the baselines in Fig. E.1. In addition, we show the objective values obtained by the different methods as
a function of the budget (see Fig. E.2) and present the budget required to achieve specific objective levels
(see Fig. E.3). The results show that GuideBO consistently outperforms the baselines. Furthermore, we
explore the influence of varying the budget Nin comparison to a dense uniform grid (with 1000points) in
Fig. E.4. We show that N= 100is sufficient to match (and sometimes outperform) the performance of the
dense grid, highlighting the computational advantage of the proposed method. Furthermore, we conducted
five additional experiments for the early-time classification task using various structured time-series datasets
with large hyperparameter spaces ranging from 8 to 12 dimensions, demonstrating again that GuideBO is
preferable over the baselines (see Table D.1 and Fig. E.5). We also examine the influence of the parameter γ
in Fig. E.6, showing that the method is generally insensitive to γ. Moreover, Fig. E.7 shows that using the
proposed region is preferable over a single-sided upper bound at α, implying that it is important to exclude
inefficient configurations. Finally, we present examples of GuideBO’s outcomes in Fig. E.8, highlighting
its effectiveness in identifying relevant configurations within the defined region of interest, as opposed to
recovering the entire front.
Limitationsandfuturework. Whileourproposedmethodestablishesanefficientmechanismforselecting
risk-controlling model configurations and improves upon previous work, it has some limitations. Multi-
fidelity optimization is a popular method for hyperparameter optimization, where resources are allocated
efficiently (Li et al., 2018; 2020). In this approach, additional resources (e.g., more epochs) are allocated
to promising configurations that performed well with fewer resources, while configurations that showed
poorer performance are discarded. Our current method cannot be directly applied in this setting. Moreover,
the calibrated selection procedure requires splitting the data between the validation set used for selecting
the subset of promising configurations, and the calibration set used for their verification. Although this is
commonly done in other conformal prediction and risk control methods (Angelopoulos et al., 2021; Bai et al.,
2022; Ringel et al., 2024), in many practical settings there is only limited available data. Another issue is
that there might be a distribution shift between the validation/calibration data and test data, and thus the
selected configuration might not be risk-controlling with respect to shifted test data distributions (Gibbs &
Candès, 2024; Zollo et al., 2024). These challenges should be explored in future work.
8 Conclusion
We introduce a versatile framework designed for reliable model selection. This framework is capable of
meeting statistical risk limitations while simultaneously optimizing other conflicting metrics. We establish
a confined region within the objective space that is a promising target for statistical testing. Our proposed
method, referred to as GuideBO, is employed to pinpoint configurations that are Pareto optimal and lie in
the specified region. We statistically validate the set of candidate configurations using multiple hypothesis
testing to achieve verified control guarantees. The broad applicability and effectiveness of our approach are
demonstrated for tuning different types of hyperparameters across various tasks and objectives, including
high-accuracy, fairness, robustness, generation and reconstruction quality and cost and time considerations.
Acknowledgments
We thank the anonymous reviewers for helpful discussions and feedback. B.L.G. was supported in part by
Schmidt Sciences, LLC.
References
E Alpaydin and C Kaynak. Optical recognition of handwritten digits. uci machine learning repository, 1998.
Anastasios N Angelopoulos, Stephen Bates, Emmanuel J Candès, Michael I Jordan, and Lihua Lei. Learn
then test: Calibrating predictive algorithms to achieve risk control. arXiv preprint arXiv:2110.01052 ,
2021.
AnastasiosNAngelopoulos, StephenBates, AdamFisch, LihuaLei, andTalSchuster. Conformalriskcontrol.
arXiv preprint arXiv:2208.02814 , 2022.
14Published in Transactions on Machine Learning Research (10/2024)
D Anguita, A Ghio, L Oneto, X Parra, JL Reyes-Ortiz, et al. A public domain dataset for human activity
recognitionusingsmartphones. In 21th European Symposium on Artificial Neural Networks, Computational
Intelligence and Machine Learning (ESANN) , pp. 437–442. CIACO, 2013.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning
Research , 3(Nov):397–422, 2002.
Johannes Bader and Eckart Zitzler. Hype: An algorithm for fast hypervolume-based many-objective opti-
mization. Evolutionary computation , 19(1):45–76, 2011.
Yu Bai, Song Mei, Huan Wang, Yingbo Zhou, and Caiming Xiong. Efficient and differentiable conformal
prediction with general function classes. In International Conference on Learning Representations , 2022.
Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Predictive inference
with the jackknife+. The Annals of Statistics , 49(1):486–507, 2021.
Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael Jordan. Distribution-free,
risk-controlling prediction sets. Journal of the ACM (JACM) , 68(6):1–34, 2021.
Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-objective
bayesian optimization. In Advances in Neural Information Processing Systems , volume 32, 2019.
Syrine Belakaria, Aryan Deshwal, Nitthilan Kannappan Jayakodi, and Janardhan Rao Doppa. Uncertainty-
aware search framework for multi-objective bayesian optimization. Proceedings of the AAAI Conference
on Artificial Intelligence , 34(06):10044–10052, 2020.
Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas,
Theresa Ullmann, Marc Becker, Anne-Laure Boulesteix, et al. Hyperparameter optimization: Founda-
tions, algorithms, best practices, and open challenges. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery , 13(2):e1484, 2023.
Juergen Branke. Mcda and multiobjective evolutionary algorithms. Multiple criteria decision analysis: state
of the art surveys , pp. 977–1008, 2016.
Jürgen Branke and Kalyanmoy Deb. Integrating user preferences into evolutionary multi-objective optimiza-
tion. InKnowledge incorporation in evolutionary computation , pp. 461–477. Springer, 2005.
Jürgen Branke, Thomas Kaußler, and Harmut Schmeck. Guidance in evolutionary multi-objective optimiza-
tion.Advances in engineering software , 32(6):499–507, 2001.
Clément Chadebec, Louis Vincent, and Stephanie Allassonniere. Pythae: Unifying generative autoencoders
in python - a benchmarking use case. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and
A. Oh (eds.), Advances in Neural Information Processing Systems , volume 35, pp. 21575–21589. Curran
Associates, Inc., 2022.
Weiyu Chen and James Kwok. Multi-objective deep learning with adaptive reference vectors. Advances in
Neural Information Processing Systems , 35:32723–32735, 2022.
YanpingChen, EamonnKeogh, BingHu, NurjahanBegum, AnthonyBagnall, AbdullahMueen, andGustavo
Batista. The ucr time series classification archive. http://www.cs.ucr.edu/~eamonn/time_series_
data/, 2015.
Ching-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. In International Conference
on Learning Representations , 2020.
Cristina Cristescu and Joshua Knowles. Surrogate-based multiobjective optimization: Parego update and
test. InWorkshop on Computational Intelligence (UKCI) , volume 770, 2015.
Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Parallel bayesian optimization of multiple noisy
objectives with expected hypervolume improvement. Advances in Neural Information Processing Systems ,
34:2187–2200, 2021.
15Published in Transactions on Machine Learning Research (10/2024)
Kalyanmoy Deb and Jayavelmurugan Sundar. Reference point based multi-objective optimization using
evolutionary algorithms. In Proceedings of the 8th annual conference on Genetic and evolutionary compu-
tation, pp. 635–642, 2006.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective
genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation , 6(2):182–197, 2002.
Jean-AntoineDésidéri. Multiple-gradientdescentalgorithm(mgda)formultiobjectiveoptimization. Comptes
Rendus Mathematique , 350(5-6):313–318, 2012.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Dheeru Dua, Casey Graff, et al. Uci machine learning repository, 2017.
Michael Emmerich, Nicola Beume, and Boris Naujoks. An emo algorithm using the hypervolume measure as
selection criterion. In International Conference on Evolutionary Multi-Criterion Optimization , pp. 62–76.
Springer, 2005.
Michael TM Emmerich, Kyriakos C Giannakoglou, and Boris Naujoks. Single-and multiobjective evolu-
tionary optimization assisted by gaussian random field metamodels. IEEE Transactions on Evolutionary
Computation , 10(4):421–439, 2006.
Matthias Feurer, Katharina Eggensperger, Edward Bergman, Florian Pfisterer, Bernd Bischl, and Frank
Hutter. Mind the gap: Measuring generalization performance across multiple objectives. In International
Symposium on Intelligent Data Analysis , pp. 130–142. Springer, 2023.
Carlos M Fonseca and Peter J Fleming. Multiobjective optimization and multiple constraint handling with
evolutionary algorithms. i. a unified formulation. IEEE Transactions on Systems, Man, and Cybernetics-
Part A: Systems and Humans , 28(1):26–37, 1998.
Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811 , 2018.
Johannes Fürnkranz and Eyke Hüllermeier. Preference learning and ranking by pairwise comparison. In
Preference learning , pp. 65–82. Springer, 2010.
Jacob Gardner, Matt Kusner, Kilian Weinberger, John Cunningham, et al. Bayesian optimization with
inequality constraints. In International Conference on Machine Learning , pp. 937–945. PMLR, 2014.
Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. Advances in neural
information processing systems , 30, 2017.
Isaac Gibbs and Emmanuel J Candès. Conformal inference for online prediction with arbitrary distribution
shifts.Journal of Machine Learning Research , 25(162):1–36, 2024.
Chirag Gupta, Aleksandr Podkopaev, and Aaditya Ramdas. Distribution-free binary classification: predic-
tion sets, confidence intervals and calibration. In Advances in Neural Information Processing Systems
(NeurIPS) , 2020.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in
neural information processing systems , 29, 2016.
Daniel Hernández-Lobato, Jose Hernandez-Lobato, Amar Shah, and Ryan Adams. Predictive entropy search
for multi-objective bayesian optimization. In International conference on machine learning , pp. 1492–1501.
PMLR, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed,andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational
framework. In International conference on learning representations , 2016.
16Published in Transactions on Machine Learning Research (10/2024)
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. The collected works of
Wassily Hoeffding , pp. 409–426, 1994.
Sture Holm. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics , pp.
65–70, 1979.
D Ienco and R Gaetano. Tiselac: time series land cover classification challenge. https://www.
timeseriesclassification.com/description.php?Dataset=Tiselac , 2007.
Hisao Ishibuchi, Ryo Imada, Yu Setoguchi, and Yusuke Nojima. How to specify a reference point in hyper-
volume calculation for fair performance comparison. Evolutionary computation , 26(3):411–440, 2018.
Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew G Wilson. On feature learning in the presence
of spurious correlations. Advances in Neural Information Processing Systems , 35:38516–38532, 2022.
FlorianKarl,TobiasPielok,JuliaMoosbauer,FlorianPfisterer,StefanCoors,MartinBinder,LennartSchnei-
der, Janek Thomas, Jakob Richter, Michel Lang, et al. Multi-objective hyperparameter optimization–an
overview. arXiv preprint arXiv:2206.07438 , 2022.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,
2013.
Joshua Knowles. Parego: A hybrid algorithm with on-line landscape approximation for expensive multiob-
jective optimization problems. IEEE Transactions on Evolutionary Computation , 10(1):50–66, 2006.
Mina Konakovic Lukovic, Yunsheng Tian, and Wojciech Matusik. Diversity-guided multi-objective bayesian
optimizationwithbatchevaluations. Advances in Neural Information Processing Systems , 33:17708–17720,
2020.
HJ Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the presence
of noise. Journal of Basic Engineering , 86(1):97–106, 1964.
Stefanos Laskaridis, Alexandros Kouris, and Nicholas D Lane. Adaptive inference through early-exit net-
works: Design, challenges and directions. In Proceedings of the 5th International Workshop on Embedded
and Mobile Deep Learning , pp. 1–6, 2021.
Bracha Laufer-Goldshtein, Adam Fisch, Regina Barzilay, and Tommi Jaakkola. Efficiently controlling mul-
tiple risks with pareto testing. ICLR, 2023.
Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/ , 1998.
Jing Lei, James Robins, and Larry Wasserman. Distribution-free prediction sets. Journal of the American
Statistical Association , 108(501):278–287, 2013.
Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free
predictive inference for regression. Journal of the American Statistical Association , 113(523):1094–1111,
2018.
Benjamin Letham, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. Constrained bayesian optimization
with noisy experiments. Bayesian Analysis , 14(2), 2019.
Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-Tzur, Moritz Hardt, Ben-
jamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. Proceedings
of Machine Learning and Systems , 2:230–246, 2020.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A
novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research , 18
(185):1–52, 2018.
Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. Advances
in neural information processing systems , 32, 2019.
17Published in Transactions on Machine Learning Research (10/2024)
Xi Lin, Zhiyuan Yang, Qingfu Zhang, and Sam Kwong. Controllable pareto multi-task learning. arXiv
preprint arXiv:2010.06313 , 2020.
Marius Lindauer, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Difan Deng, Carolin Ben-
jamins, Tim Ruhkopf, René Sass, and Frank Hutter. Smac3: A versatile bayesian optimization package
for hyperparameter optimization. J. Mach. Learn. Res. , 23:54–1, 2022.
MichaelLohaus, MichaelPerrot, andUlrikeVonLuxburg. Toorelaxedtobefair. In International Conference
on Machine Learning , pp. 6360–6369. PMLR, 2020.
Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman, and Wolfgang
Banzhaf. Nsga-net: neural architecture search using multi-objective genetic algorithm. In Proceedings of
the genetic and evolutionary computation conference , pp. 419–427, 2019.
Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descent
with controlled ascent in pareto optimization. In International Conference on Machine Learning , pp.
6597–6607. PMLR, 2020.
Natalia Martinez, Martin Bertran, and Guillermo Sapiro. Minimax pareto fairness: A multi objective
perspective. In International conference on machine learning , pp. 6755–6764. PMLR, 2020.
Michael D McKay, Richard J Beckman, and William J Conover. A comparison of three methods for selecting
values of input variables in the analysis of output from a computer code. Technometrics , 42(1):55–61, 2000.
Gaurav Menghani. Efficient deep learning: A survey on making deep learning models smaller, faster, and
better.ACM Computing Surveys , 55(12):1–37, 2023.
Giovanni Misitano, Bekir Afsar, Giomara Lárraga, and Kaisa Miettinen. Towards explainable interactive
multiobjective optimization: R-ximo. Autonomous Agents and Multi-Agent Systems , 36(2):43, 2022.
Jonas Močkus. On bayesian methods for seeking the extremum. In Optimization Techniques IFIP Technical
Conference: Novosibirsk, July 1–7, 1974 , pp. 400–404. Springer, 1975.
Aviv Navon, Aviv Shamsian, Gal Chechik, and Ethan Fetaya. Learning the pareto front with hypernetworks.
arXiv preprint arXiv:2010.04104 , 2020.
Kirtan Padh, Diego Antognini, Emma Lejal-Glaude, Boi Faltings, and Claudiu Musat. Addressing fairness
in classification with a model-agnostic multi-objective algorithm. In Uncertainty in Artificial Intelligence ,
pp. 600–609. PMLR, 2021.
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh
Padmakumar, Johnny Ma, Jana Thompson, He He, et al. Quality: Question answering with long input
texts, yes! In 2022 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies , pp. 5336–5358, 2022.
Biswajit Paria, Kirthevasan Kandasamy, and Barnabás Póczos. A flexible framework for multi-objective
bayesian optimization using random scalarizations. In Uncertainty in Artificial Intelligence , pp. 766–776.
PMLR, 2020.
Dana Pessach and Erez Shmueli. A review on fairness in machine learning. ACM Computing Surveys
(CSUR), 55(3):1–44, 2022.
Florian Pfisterer, Stefan Coors, Janek Thomas, and Bernd Bischl. Multi-objective automatic machine learn-
ing with autoxgboostmc. arXiv preprint arXiv:1908.10796 , 2019.
Wolfgang Ponweiser, Tobias Wagner, Dirk Biermann, and Markus Vincze. Multiobjective optimization on a
limited budget of evaluations using model-assisted-metric selection. In International conference on parallel
problem solving from nature , pp. 784–794. Springer, 2008.
18Published in Transactions on Machine Learning Research (10/2024)
DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagationandapproximate
inference in deep generative models. In International conference on machine learning , pp. 1278–1286.
PMLR, 2014.
Liran Ringel, Regev Cohen, Daniel Freedman, Michael Elad, and Yaniv Romano. Early time classification
with accumulated accuracy gap control. arXiv preprint arXiv:2402.00857 , 2024.
Michael Ruchte and Josif Grabocka. Scalable pareto front approximation for deep multi-objective learning.
In2021 IEEE international conference on data mining (ICDM) , pp. 1306–1311. IEEE, 2021.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural
networksforgroupshifts: Ontheimportanceofregularizationforworst-casegeneralization. arXiv preprint
arXiv:1911.08731 , 2019.
David Salinas, Jacek Golebiowski, Aaron Klein, Matthias Seeger, and Cedric Archambeau. Optimizing
hyperparameters with conformal quantile regression. arXiv preprint arXiv:2305.03623 , 2023.
Robin Schmucker, Michele Donini, Muhammad Bilal Zafar, David Salinas, and Cédric Archambeau. Multi-
objective asynchronous successive halving. arXiv preprint arXiv:2106.12639 , 2021.
Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances in neural
information processing systems , 31, 2018.
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out
of the loop: A review of bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2015.
S Stanton, W Maddox, and AG Wilson. Bayesian optimization with conformal prediction sets. In Artificial
Intelligence and Statistics , 2023.
Chang Wei Tan, Geoffrey I Webb, and François Petitjean. Indexing and classifying gigabytes of time series
under time warping. In Proceedings of the 2017 SIAM international conference on data mining , pp.
282–290. SIAM, 2017.
Vladimir Vovk. On-line confidence machines are well-calibrated. In The 43rd Annual IEEE Symposium on
Foundations of Computer Science. , 2002.
Vladimir Vovk, Ivan Petej, and Valentina Fedorova. Large-scale probabilistic predictors with and without
guarantees of validity. In Advances in Neural Information Processing Systems (NeurIPS) , 2015.
Vladimir Vovk, Jieli Shen, Valery Manokhin, and Min-ge Xie. Nonparametric predictive distributions based
on conformal prediction. In Proceedings of the Sixth Workshop on Conformal and Probabilistic Prediction
and Applications , 2017.
Tobias Wagner and Heike Trautmann. Integration of preferences in hypervolume-based multiobjective evolu-
tionary algorithms by means of desirability functions. IEEE Transactions on Evolutionary Computation ,
14(5):688–701, 2010.
Bin Wang, Yanan Sun, Bing Xue, and Mengjie Zhang. Evolving deep neural networks by multi-objective
particle swarm optimization for image classification. In Proceedings of the genetic and evolutionary com-
putation conference , pp. 490–498, 2019.
Handing Wang, Markus Olhofer, and Yaochu Jin. A mini-review on preference modeling and articulation in
multi-objective optimization: current status and challenges. Complex & Intelligent Systems , 3:233–245,
2017.
Xilu Wang, Yaochu Jin, Sebastian Schmitt, and Markus Olhofer. Recent advances in bayesian optimization.
arXiv preprint arXiv:2206.03301 , 2022.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning . MIT press
Cambridge, MA, 2006.
19Published in Transactions on Machine Learning Research (10/2024)
Maciej Wołczyk, Bartosz Wójcik, Klaudia Bałazy, Igor T Podolak, Jacek Tabor, Marek Śmieja, and Tomasz
Trzcinski. Zero time waste: Recycling predictions in early exit neural networks. Advances in Neural
Information Processing Systems , 34:2516–2528, 2021.
Yongkai Wu, Lu Zhang, and Xintao Wu. On convexity and bounds of fairness-aware classification. In The
World Wide Web Conference , pp. 3356–3362, 2019.
Bin Xin, Lu Chen, Jie Chen, Hisao Ishibuchi, Kaoru Hirota, and Bo Liu. Interactive multiobjective opti-
mization: A review of the state-of-the-art. IEEE Access , 6:41256–41279, 2018.
Yuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is hard: A closer look at
subpopulation shift. arXiv preprint arXiv:2302.12254 , 2023.
Qingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decomposition. IEEE
Transactions on evolutionary computation , 11(6):712–731, 2007.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification.
Advances in neural information processing systems , 28, 2015.
Yunchuan Zhang, Sangwoo Park, and Osvaldo Simeone. Bayesian optimization with formal safety guarantees
via online conformal prediction. arXiv preprint arXiv:2306.17815 , 2023.
Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms—a comparative
case study. In International conference on parallel problem solving from nature , pp. 292–301. Springer,
1998.
Thomas P Zollo, Todd Morrill, Zhun Deng, Jake Snell, Toniann Pitassi, and Richard Zemel. Prompt risk
control: A rigorous framework for responsible deployment of large language models. In The Twelfth
International Conference on Learning Representations , 2024.
A Additional related work
Gradient-Based MOO. When dealing with differentiable objective functions, gradient-based MOO
algorithms can be utilized. The cornerstone of these methods is Multiple-Gradient Descent (MGD) (Sener
& Koltun, 2018; Désidéri, 2012), which ensures that all objectives are decreased simultaneously, leading to
convergence at a Pareto optimal point. Several extensions were proposed to enable convergence to a specific
point on the front defined by a preference vector (Lin et al., 2019; Mahapatra & Rajan, 2020), or learning
the entire Pareto front, using a preference-conditioned model (Navon et al., 2020; Lin et al., 2020; Chen &
Kwok, 2022; Ruchte & Grabocka, 2021). However, this line of research focuses on differentiable objectives,
optimizing the loss space used during training (e.g. cross entropy loss), which is typically different from the
ultimate non-differentiable metrics used for evaluation (e.g. error rates). Furthermore, it focuses on recov-
ering a single or multiple (possibly infinitely many) Pareto optimal points, without addressing the actual
selection of model configuration under specific constraints, which is the problem we tackle in this paper.
Incorporating user preferences in MOO. Another area of research has explored integrating decision
maker preferences into multi-objective optimization (Branke, 2016; Wang et al., 2017; Xin et al., 2018).
These methods are typically classified based on the stage in which they are applied in the optimization
process: before, during, or after optimization (Branke & Deb, 2005). Preferences can be specified in various
ways, such as using reference points representing an “ideal” solution (Deb & Sundar, 2006), imposing con-
straints (Fonseca & Fleming, 1998), setting maximal or minimal trade-offs (Branke et al., 2001), and using
desirability functions that non-linearly scale the objectives to [0,1](Wagner & Trautmann, 2010). Addition-
ally, preferences can be learned by asking the decision maker to rank solutions or select the most preferred
solution from a set (Fürnkranz & Hüllermeier, 2010). Another interactive preference learning approach
involves incorporating explanations that help the decision maker in understanding the trade-offs between
objectives (Misitano et al., 2022). In this paper, we focus on preferences provided by the user as hard limits
on certain objectives, which require validation through statistical testing. Our BO procedure aims to extract
a set of promising configurations that are designed to achieve tight control guarantees.
20Published in Transactions on Machine Learning Research (10/2024)
B Mathematical Details
B.1 Derivation of the Region of Interest
Suppose the loss is bounded above by 1, then Hoeffding’s inequality (Hoeffding, 1994) is given by:
P/parenleftig
ˆℓ(λ)−ℓ(λ)≤−t/parenrightig
≤e−2mt2, (18)
and
P/parenleftig
ˆℓ(λ)−ℓ(λ)≥t/parenrightig
≤e−2mt2. (19)
fort>0. Takingu=e−2mt2, we havet=/radicalig
log(1/u)
2m, hence:
P/parenleftigg
ˆℓ(λ)−ℓ(λ)≤−/radicalbigg
log (1/u)
2m/parenrightigg
≤u, (20)
and
P/parenleftigg
ˆℓ(λ)−ℓ(λ)≥/radicalbigg
log (1/u)
2m/parenrightigg
≤u. (21)
This implies an upper confidence bound
ℓ+
HF(λ) =ˆℓ(λ) +/radicalbigg
log (1/u)
2m, (22)
and a lower confidence bound
ℓ−
HF(λ) =ˆℓ(λ)−/radicalbigg
log (1/u)
2m. (23)
In addition, we can use Hoeffding’s inequality to derive a valid p-value under the null hypothesis Hλ:ℓ(λ)>
α. By (20), we get:
P/parenleftigg
ˆℓ(λ)−α≤−/radicalbigg
log (1/u)
2m/parenrightigg
≤P/parenleftigg
ˆℓ(λ)−ℓ(λ)≤−/radicalbigg
log (1/u)
2m/parenrightigg
≤u. (24)
Rearranging the inequality inside the probability for ˆℓ(λ)<α, we obtain:
ˆℓ(λ)−α≤−/radicalbigg
log (1/u)
2m
α−ˆℓ(λ)≥/radicalbigg
log (1/u)
2m
2m(α−ˆℓ(λ))2≥log (1/u)
−2m(α−ˆℓ(λ))2≤log (u)
e−2m(α−ˆℓ(λ))2≤u.(25)
Thus for ˆℓ(λ)<α, we obtain:
P/parenleftig
e−2m(α−ˆℓ(λ))2
+≤u/parenrightig
≤u, (26)
where we inserted (·)+= max(·,0)since we assume ˆℓ(λ)< α. Note that this statement also holds for
ˆℓ(λ)≥α, as we have P(1≤u)≤u, which is valid∀u∈[0,1]. This implies that pHF
λ:=e−2m(α−ˆℓ(λ))2
+is
21Published in Transactions on Machine Learning Research (10/2024)
super-uniform, hence is a valid p-value as required. Comparing pHF
λtoδ, yields the maximum empirical loss
ˆℓ(λ), evaluated over a calibration set of size m, which can pass the test with significance level δ:
αmax=α−/radicalbigg
log (1/δ)
2m. (27)
This can be equivalently obtained from the upper bound defined in Eq. (22).
The region of interest in Eqs. (7) and (8) is obtained based on Eqs. (20) and (21) (where u→γandm→k):
P/parenleftig
ˆℓopt
1(λ)∈R(α,k,m,δ,γ )/vextendsingle/vextendsingleℓ1(λ) =αmax/parenrightig
=
P/parenleftigg
ℓ1(λ)−/radicalbigg
log (1/γ)
2k<ˆℓopt
1(λ)<ℓ1(λ) +/radicalbigg
log (1/γ)
2k/parenrightigg
=
1−P/parenleftigg
ˆℓopt
1(λ)≤ℓ1(λ)−/radicalbigg
log (1/γ)
2k/parenrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Eqs. (20):≤γ−P/parenleftigg
ˆℓopt
1(λ)≥ℓ1(λ) +/radicalbigg
log (1/γ)
2k/parenrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Eqs. (21):≤γ≥1−2γ.(28)
Since the probability defined on the left-hand side in non-negative and bounded by 1, we have 0<γ≤0.5.
In practice, we use much smaller values of γ≪0.5to ensure a sufficiently wide region that encompasses all
potential configurations around αmax.
A tighter alternative to Hoeffding p-value was proposed in (Bates et al., 2021) based Hoeffding and Bentkus
inequalities. The Hoeffding-Bentkus p-value is given by:
pHB
λ= min/parenleftig
exp{−mh1(ˆℓ(λ)∧α,α)},eP/parenleftig
Binom (m,α)≤⌈mˆℓ(λ)⌉/parenrightig/parenrightig
, (29)
whereh1(a,b) =alog(a
b) + (1−a) log(1−a
1−b),a∧b= max(a,b)andedenotes the natural exponent. For
binary risk functions (e.g. error) we use the Binomial tail probability instead (without the efactor):
pBin
λ=P(Binom (m,α)≤⌈mˆℓ(λ)⌉). (30)
Note that for a given δwe can numerically extract from Eqs. (29) or (30) the upper and lower bounds
corresponding to a 1−2γconfidence interval, and use it to define the region of interest as in Eq. (8).
When the loss is unbounded, we can alternatively use a p-value defined by the central limit theorem, which
is asymptotically valid. Assuming that the loss has a finite mean and variance, we define:
pCLT
λ= 1−Φ/parenleftigg
α−ˆℓ(λ)
ˆσ(λ)/√m/parenrightigg
, (31)
where ˆσ(λ) =1
m−1/radicalig/summationtextm+k
j=k+1(ℓ(Xj,Yj;λ)−ˆℓ(λ))2denotes the empirical standard deviation, and Φis the
normal cumulative distribution function. We obtain that lim supm→∞P/parenleftbig
pCLT
λ≤u/parenrightbig
≤u.
B.2 A valid p-value for multiple constraints
We prove that taking the maximum p-value across constraints is a valid p-value for the combined hypothesis.
Lemma B.1. Letpλ,ibe a p-value for Hλ,i:ℓi(λ)>αi, for eachi∈{1,...,c}. Definepλ:= max 1≤i≤cpλ,i.
Then, for all λsuch thatHλ:∃iwhereℓi(λ)>αiholds, we have:
P(pλ≤u)≤u, (32)
whereu∈[0,1].
Proof.LetJ⊆{ 1,...,c}be the set of all true null hypotheses (unsatisfied constraints) at λ. We have:
P/parenleftig
pλ≤u/parenrightig
≤P/parenleftbigg
max
j∈Jpλ,j≤u/parenrightbigg
=P
/intersectiondisplay
j∈Jpλ,j≤u
≤max
j∈JP(pλ,j≤u). (33)
Since for each j∈J,P(pλ,j≤u)≤u, we have maxj∈JP(pλ,j≤u)≤u, implying that P(pλ≤u)≤u.
22Published in Transactions on Machine Learning Research (10/2024)
B.3 Proof of Proposition 5.1
The proof is based on (Angelopoulos et al., 2021; Laufer-Goldshtein et al., 2023), which we repeat here for
completeness.
Proof.Recall thatDvalandDcalare two disjoint, i.i.d. datasets. Therefore, Dcalis i.i.d. w.r.t the returned
configuration set optimized in Algorithm 1 over Dval.
We now prove that the testing procedure returns a set of valid configurations with FWER bounded by δ.
LetHλ′be the first true null hypothesis in the sequence. Given that pλ′is a super uniform p-value under
Hλ′, the probability of making a false discovery at λ′is bounded by δ. This means that the event that Hλ′
is rejected (false discovery) occurs with probability lower than δ. However, if Hλ′fails to be rejected (no
false discovery), then all other Hλthat follow in the sequence also fail to be rejected (regardless of if Hλis
true or not). Therefore, the probability of making any false discovery is bounded by δ, which satisfies the
FWER control requirement.
B.4 Hypervolume
An illustration of the hypervolume defined in Eq. (3) is given in Fig. B.1 for the 2-dimensional case. It can
be seen that the hypervolume is equivalent to the volume of the union of the boxes created by the Pareto
optimal points and the reference point.
Figure B.1: An illustration of the hypervolume in the 2-dimensional case. The reference point is marked in
red and three Pareto optimal points are marked in blue.
B.5 Region of Interest
An illustration of the region of interest defined in Eq. (9) is given in Fig. B.2 for the 3-dimensional case (two
constraints and a single free objective function). The volume is defined by the intersection of the regions
defined by each constrained dimension.
23Published in Transactions on Machine Learning Research (10/2024)
Figure B.2: An illustration of the region of interest in the 3-dimensional case.
C Algorithms
Our overall proposed method is summarized in Algorithm C.1.
Algorithm C.1 Configuration Selection
Definitions: fis a configurable model set by an hyperparameter λ.Dval={Xi,Yi}k
i=1andDcal={Xi,Yi}k+m
i=k+1are
two disjoint subsets of validation and calibration data, respectively. {ℓ1,...,ℓ c}are constrained objective functions,
andℓfreeisafreeobjective. {α1,...,α c}areuser-specifiedboundsfortheconstrainedobjectives. Λistheconfiguration
space.δis the tolerance. Nis the optimization budget, and N0is the size of the intial pool of configurations.
ParetoOptimalSet () returns Pareto optimal points.
1:function Select(Dval,Dcal,Λ,{α1,...,α c},δ,N)
2: Compute ℓlow
i,ℓhigh
ifori∈{1,...,c}based on (8) and (9) ▷Determine the region of interest.
3:C0,L0←Randomly sample an initial pool of configurations of size N0 ▷Generate an initial pool.
4:CBO←BO(Dval,ℓ,C0,Lo,{ℓlow
1,...,ℓlow
c},{ℓhigh
1,...,ℓhigh
j},N) ▷BO via Algorithm 1.
5:Cp←ParetoOptimalSet (CBO) ▷Filter Pareto optimal points according to Eq. (2).
6: Compute pval
λoverDvalfor allλ∈Cp▷Compute approximated p-values.
7:Co←Order configurations according to increasing pval
λ▷Order configurations.
8: Compute pcal
λoverDcalfor allλ∈Co▷Compute p-values.
9: Apply FST: Cvalid={λ(j):j <J}, J= min j{j:pcal
λ≥δ} ▷Apply FST.
10:λ∗= arg minλ∈Cvalidℓfree(λ) ▷select the best-performing configuration.
11: returnλ∗
D Implementation and dataset details
We provide here further details on the datasets, application specifications, model architectures, training
procedures, and examined scenarios.
Initialization. For GuideBO, HVI and EHVI we randomly sample an initial set of size N0, and perform
N−N0iterations of BO. We use a uniform grid for n= 1and LHS for n>1. The values of NandN0for
each task are provided in Table 2. For ParEGO we use the default initialization defined by the SMAC3
implementation.
Fairness. For computing [DDP (f), the indicator 1f(x)>0in Eq. (15) is relaxed using tanh(c·max(0,f(x)))
withc= 3(Padh et al., 2021). In addition, we define a linear interpolation in the input space: xt=
t·x1+ (1−t)·x−1, fort∈[0,1]wherex1andx−1represent samples with attributes a= 1anda=−1,
24Published in Transactions on Machine Learning Research (10/2024)
respectively. The mixup regularization is defined by:
\MixUP (f) =Et[|EX[⟨∇xf(xt),x−1−x1⟩]|], (34)
regularizing the expected inner product between the Jacobian on mixup samples and the difference x−1−
x1(Chuang & Mroueh, 2020). Our model is a 3-layer feed-forward neural network with hidden dimensions
[60,25]. We train all models using Adam optimizer with learning rate 1e−3for50epochs and batch size
256.
Robustness. We use a ResNet-50 model pretrained on ImageNet. Following (Izmailov et al., 2022), we
train the models for 50epochs with SGD with a constant learning rate of 1e−3, momentum decay of 0.9,
batch size 32and weight decay of 1e−4. We use random crops and horizontal flips as data augmentation.
We use half of the CelebA validation data to train the last layer, and the other half for BO.
VAE.We use the implementation provided by (Chadebec et al., 2022) of a ResNet-based encoder and
decoder, trained using AdamW optimizer with β1= 0.91,β2= 0.99, and weight decay 0.05. We set the
learning to 1e−4and the batch size to 64. The training process consisted of 10epochs. We use binary-cross
entropy reconstruction loss for training the model, and the mean squared error normalized by the total
number of pixels (728) as the reconstruction objective function for hyperparameter tuning.
Pruning. We use a BERT-base model (Devlin et al., 2018) with 12layers and 12heads per layer. We
follow the recipe in (Laufer-Goldshtein et al., 2023) and attach a prediction head and a token importance
predictor per layer. The core model is first finetuned on the task. We compute the attention head importance
scores based on 5K held-out samples out of the training data. We freeze the backbone model and train the
early-exit classifiers and the token importance predictors on the training data ( 115K samples).
Each prediction head is a 2-layer feed-forward neural network with 32dimensional hidden states, and ReLU
activation. The input is the hidden representation of the [CLS]token concatenated with the hidden repre-
sentation of all previous layers, following (Wołczyk et al., 2021).
Similarly, each token importance predictor is a 2-layer feed-forward neural network with 32dimensional
hidden states, and ReLU activation. The input is the hidden representation of each token in the current
layer and all previous layers (Wołczyk et al., 2021).
Early-Time Classification. We adapt the setup described in (Ringel et al., 2024), and use the processed
modeloutcomesthatappearintheirimplementation4. Thecontextofeachquestionisdividedintosentences,
which are grouped into tmax= 10sets. The input sequence until time tis provided as a prompt that includes
the context sentences up to timestep t, along with the question and its four options, labeled ‘A’, ‘B’, ‘C’,
and ‘D’. The prompt concludes with “ The answer is:\n\n ”. The prompt is processed by the Vicuna-13B
model.
We provide additional results for structured time series data, following (Ringel et al., 2024). The details
of all datasets are summarized in Table D.1. Each dataset is partitioned into four distinct parts: 80%for
training and the remaining 20%are equally divided to validation, calibration and test subsets. For each
dataset we define an exist point every fixed number of timesteps (hop-size) and obtain a hyperparameter
dimensionnranging from 8to12. A standard LSTM is used for feature extraction with one recurrent layer
with a hidden size of 32, except for WalkingSittingStanding where the model consists of 2recurrent layers,
each with a hidden size of 256. The output of the last recurrent layer is followed by two fully connected
classification heads, one for classifying the label and the other for estimating the classification confidence.
The loss consists of two terms: cross-entropy loss for the label, and binary cross entropy loss (weighted by
0.2) for whether the classifier is correct or not. The models are trained with Adam optimizer, with a learning
rate of 0.001, and a batch size of 64.
E Additional Results
In this section, we describe additional experiments and results.
4https://github.com/liranringel/etc
25Published in Transactions on Machine Learning Research (10/2024)
Table D.1: Summary of structured time-series datasets.
Dataset # Features # Classes # Samples # Timesteps Hop Size n
Tiselac 10 9 99687 23 2 12
ElectricDevices 1 7 16637 96 8 12
PenDigits 2 10 10992 8 8 8
Crop 1 24 24000 46 4 12
WalkingSittingStanding 3 6 10299 206 20 11
(a) Fairness
 (b) Robustness
 (c) Selective Robustness
(d) VAE
 (e) Pruning
 (f) Early Classification
Figure E.1: The values of the constrained objective functions across tasks and different limits (marked by
dashed red lines). The objectives are evaluated over Dtestfor the configuration that was chosen by each
method. All method satisfy the limits due to the testing procedure.
Satisfying Constraints and Tighter Control. We show the values of the constrained objective functions
in Fig. E.1, where the red dashed lines depict the limit. We see that the constraints are satisfied by all
methods as expected, since in any case the configurations are validated through the testing procedure.
Notably, GuideBO obtains tighter control compared to baselines in nearly all cases. This is consistent with
our earlier finding that GuideBO better minimizes the free objective function.
Varying Optimization Budget. In this experiment we fixed the constraint ( αlimit) for each task and
varied the optimization budget (number of iterations/function evaluations). We show the improvement in
the free objective function as a function of the optimization budget across tasks on Fig. E.2. It can be seen
that GuideBO delivers consistently strong performance in all scenarios, while the other baselines have mixed
results, excelling in some cases and underperforming in others. In addition, for each task we define three
levels of the free objective: between 90%of the maximum value and 110%of the minimum value obtained
by the different baselines, and order these levels from high to low value. We compare the budget that is
required for each baseline to reach a certain level in all tasks and for all defined levels. Results are shown in
26Published in Transactions on Machine Learning Research (10/2024)
(a) Fairness ( α= 0.17)
 (b) Robustness ( α= 0.055)
(c) Selective Robustness ( α1= 0.065,α2= 0.05)
 (d) VAE (α= 0.02)
(e) Pruning ( α= 0.025)
 (f) Early Classification ( α= 0.05)
Figure E.2: The value of the free objective as a function of the budget across different tasks. In each case
theαlimit is fixed. GuideBO consistently performs well across all cases, whereas the other baselines show
inconsistent performance, achieving better results in some instances and worse in others.
Fig. E.3. If a method cannot reach a certain level within the total maximum budget, no bar is displayed for
that method at that level. We observe that in almost all cases (15 out of 18 scenarios), GuideBo requires
the least budget compared to the baselines, and in many instances, it can reach levels that are unattainable
by the other baselines. These results indicate that GuideBo requires fewer iterations to achieve the same or
better results compared to the baselines.
Comparing to dense grid. We examine the effect of varying the optimization budget N. We show results
for the pruning task with N∈{20,50,100}. In addition, we compare to a dense grid with uniform sampling
27Published in Transactions on Machine Learning Research (10/2024)
(a) Fairness ( α= 0.17)
 (b) Robustness ( α= 0.055)
(c) Selective Robustness ( α1=
0.065,α2= 0.05)
(d) VAE (α= 0.02)
 (e) Pruning ( α= 0.025)
 (f) Early Classification ( α= 0.05)
Figure E.3: The budget that is required to achieve several different levels of the free objective function
across different tasks. In nearly all cases (15 out of 18 scenarios), we find that GuideBO requires the smallest
budget compared to the baselines. Moreover, it often achieves levels that the other baselines cannot reach.
Figure E.4: Results of the proposed method over AG News (pruning task) for different number of
evaluations, and with a grid of uniform thresholds. Accuracy reduction is controlled and cost is minimized.
of all 3 hyperparmeters with a total of N= 1000configurations. We see on Fig. E.4 that the relative cost
gradually improves with the increase in N. It reaches (and in some cases outperform) the dense grid baseline
withN= 100(that is 10%decrease in budget). This indicates that using our proposed method we can
significantly decrease the required budget without scarifying performance.
28Published in Transactions on Machine Learning Research (10/2024)
(a) Quality
 (b) Crop
 (c) PenDigits
(d) ElectricDevices
 (e) Tiselac
 (f) WalkingSittingStanding
Figure E.5: The values of the free objective functions across datasets and different limits for the task of
early time classification. GuideBO is ranked first in 20 out of 24 cases.
Additional experiments for early-time classification task with different datasets. We conducted
additionalexperimentsforthetaskofearlytimeclassificationwithdifferentdatasets. Following(Ringeletal.,
2024), we show results on five additional structured time series datasets that are publicly available via the
aeontoolkit5:Tiselac (Ienco&Gaetano,2007), ElectricDevices (Chenetal.,2015), PenDigits (Alpaydin
& Kaynak, 1998), Crop(Tan et al., 2017), and WalkingSittingStanding (Anguita et al., 2013). An LSTM
model serves as the base sequential classifier for all datasets. Results of all experiments on early time
classification (including Quality dataset) are presented on Fig. E.5. The average rank for each method
across datasets and αlevels is: GuideBO: 1.2, HVI: 2.8, Random: 3.2, EHVI: 3.4, ParEGO: 4.1, Uniform:
5.2. We conclude that, similar to our previous results, GuideBO outperforms the baselines in nearly all
cases, ranking first in 20 out of 24 scenarios.
Influence of γ.We examine the influence of γ, which determines the boundaries of the region of interest.
Figure E.6 shows the scores obtained for different values of γ. We observe that in most cases there is no
noticeable difference in the performance with respect to γ. However, it appears that moderate values, neither
too large nor too small, are preferable.
Ablation study - one-sided upper bound. We compare the proposed method to the case that the BO
search is constrained by a one-sided bound at the upper limit defined by α. This means that the reference
point is set to ri=αifori∈1,...,c, whilerc+1is set according to the maximum value of ℓfree, as in the
standard full Pareto front approach. Figure E.7 shows the values of the free objective across tasks. We
see that in most cases performing the search in the defined region of interest is preferable to a single-sided
bound. This shows the benefit of removing low risk, inefficient configurations from the search space (the
green section in Fig. 2).
Demonstration of BO Selection. We show the outcomes of the BO procedure across several tasks in
Fig. E.8. We compare the proposed method to HVI that recovers the entire Pareto front. The reference
point defined in (12) is marked by a green square, and the boundaries of the region of interest are depicted
5https://www.aeon-toolkit.org/en/stable/
29Published in Transactions on Machine Learning Research (10/2024)
(a) Fairness
 (b) Robustness
 (c) Selective Robustness
(d) VAE
 (e) Pruning
 (f) Early Classification
Figure E.6: Influence of γ. Showing the scores of the free objective for different values of γ, which controls
the width of the region of interest, defined in Eq. (8).
by dashed lines. The blue points correspond to the configurations in the initial pool C0, while the red
points correspond to the configurations selected by the BO procedure. We see that the specified region is
significantly smaller compared to the entire front. Moreover, we observe that by GuideBO we obtain a dense
set of configurations in the region of interest as desired. In contrast, for HVI we obtain samples all over
the front. In addition, the distribution of points is not always evenly spread along the front, so that certain
part of the front are denser than other. This explains why HVI (and similarly the other baselines) is inferior
compared to GuideBO since for certain αvalues the distribution of the selected configurations is sparse near
the limiting value.
30Published in Transactions on Machine Learning Research (10/2024)
(a) Fairness
 (b) Robustness
 (c) Selective Robustness
(d) VAE
 (e) Pruning
 (f) Early Classification
Figure E.7: Ablation study - comparing the proposed method with two-sided region to a one-sided upper
bound, defined by the limit α. The plots present the scores obtained for the free objective.
31Published in Transactions on Machine Learning Research (10/2024)
(a) Fairness (GuideBO, α= 0.17)
 (b) Fairness (Full)
(c) VAE (GuideBO, α= 0.03)
 (d) VAE (Full)
(e) Pruning (GuideBO, α= 0.075)
 (f) Pruning (Full)
(g) Early Class. (GuideBO, α= 0.05)
 (h) Early-Time Class. (Full)
Figure E.8: Demonstration of the selection outcomes of the BO procedure, comparing the proposed method
(left) to full recovery of the Pareto front by HVI (right): the green square is the defined reference point,
the blue points correspond to the initial set of configurations, and the red points correspond to selected
configurations. Dashed lines enclose the region of interest.
32