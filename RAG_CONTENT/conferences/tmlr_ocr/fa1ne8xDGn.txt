Published in Transactions on Machine Learning Research (07/2024)
Improved motif-scaffolding with SE(3) flow matching
Jason Yim jyim@csail.mit.edu
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Andrew Campbell campbell@stats.ox.ac.uk
Department of Statistics
University of Oxford
Emile Mathieu ebm32@cam.ac.uk
Department of Engineering
University of Cambridge
Andrew Y. K. Foong andrewfoong@microsoft.com
Microsoft Research AI4Science
Michael Gastegger mgastegger@microsoft.com
Microsoft Research AI4Science
José Jiménez-Luna jjimenezluna@microsoft.com
Microsoft Research AI4Science
Sarah Lewis sarahlewis@microsoft.com
Microsoft Research AI4Science
Victor Garcia Satorras victorgar@microsoft.com
Microsoft Research AI4Science
Bastiaan S. Veeling basveeling@microsoft.com
Microsoft Research AI4Science
Frank Noé franknoe@microsoft.com
Microsoft Research AI4Science
Regina Barzilay regina@csail.mit.edu
Computer Science and Articial Intelligence Laboratory
Massachusetts Institute of Technology
Tommi S. Jaakkola tommi@csail.mit.edu
Computer Science and Articial Intelligence Laboratory
Massachusetts Institute of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= fa1ne8xDGn
1Published in Transactions on Machine Learning Research (07/2024)
Abstract
Protein design often begins with the knowledge of a desired function from a motif which
motif-scaffolding aims to construct a functional protein around. Recently, generative models
have achieved breakthrough success in designing scaffolds for a range of motifs. However,
generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab
validation. In this work, we extend FrameFlow, an SE(3)flow matching model for protein
backbonegeneration, toperformmotif-scaffoldingwithtwocomplementaryapproaches. The
first ismotif amortization , in which FrameFlow is trained with the motif as input using a
data augmentation strategy. The second is motif guidance , which performs scaffolding using
an estimate of the conditional score from FrameFlow without additional training. On a
benchmark of 24 biologically meaningful motifs, we show our method achieves 2.5 times
more designable and unique motif-scaffolds compared to state-of-the-art. Code: https:
//github.com/microsoft/protein-frame-flow
1 Introduction
A common task in protein design is to create proteins with functional properties conferred through a pre-
specified arrangement of residues known as a motif. The problem is to design the remainder of the protein,
called the scaffold, that harbors the motif. Motif-scaffolding is widely used, with applications to vaccine
and enzyme design (Procko et al., 2014; Correia et al., 2014; Jiang et al., 2008; Siegel et al., 2010). For this
problem, diffusion models have greatly advanced capabilities in designing new scaffolds (Wu et al., 2023;
Trippe et al., 2022; Ingraham et al., 2023). While experimental wet-lab validation is the ultimate test for
evaluating a scaffold, in this work we focus on improving performance under computational validation of
scaffolds following prior works. In-silico success is defined as satisfying the designability1criteria which
has been found to correlate well with wet-lab success (Wang et al., 2021). The current state-of-the-art,
RFdiffusion (Watson et al., 2023), fine-tunes a pre-trained RosettaFold (Baek et al., 2023) neural network
with SE(3)diffusion (Yim et al., 2023b) and is able to successfully scaffold the majority of motifs in a
recent benchmark.2However, RFdiffusion suffers from low scaffold diversity which can hinder chances of a
successful design. Moreover, the large model size and pre-training used in RFdiffusion makes it slow to train
and difficult to deploy on smaller machines. In this work, we present a lightweight and easy-to-train model
with improved performance.
Our method adapts an existing SE(3)flow matching model, FrameFlow (Yim et al., 2023a), for motif-
scaffolding. We develop two approaches: (i) motif amortization , and (ii) motif guidance as illustrated in
Fig. 1. Motif amortization simply trains a conditional model with the motif as additional input when
generating the scaffold. We use data augmentation to amortize over all possible motifs in our training set
and aid in generalization to new motifs. Motif guidance relies on a Bayesian approach, using an unconditional
FrameFlow model to sample the scaffold residues, while the motif residues are guided at each step to their
final desired positions. An unconditional model in this context is one that generates the full protein backbone
without distinguishing between the motif and scaffold. Motif guidance was described in Wu et al. (2023) for
SE(3)diffusion. In this work, we develop the extension to SE(3)flow matching.
The two approaches differ in whether to use an conditional model or to re-purpose an unconditional model
for conditional generation. Motif guidance has the advantage that any unconditional model can be used to
readily perform motif scaffolding without the need for additional task-specific training. To provide a con-
trolled comparison, we train unconditional and conditional versions of FrameFlow on a dataset of monomers
from the Protein Data Bank (PDB) (Berman et al., 2000). Our results provide a clear comparison of the
modeling choices made when performing motif-scaffolding with FrameFlow. We find that FrameFlow with
1A metric based on using ProteinMPNN (Dauparas et al., 2022) and AlphaFold2 (Jumper et al., 2021) to determine the
quality of a protein backbone.
2First introduced in RFdiffusion as a benchmark of 24 single-chain motifs successfully solved across prior works published.
2Published in Transactions on Machine Learning Research (07/2024)
Figure 1: We present two strategies for motif-scaffolding. Top: motif amortization trains a flow model to
condition on the motif (blue) and generate the scaffold (red). During training, only the scaffold is corrupted
with noise. Bottom : motif guidance re-purposes a flow model that is trained to generate the full protein
for motif-scaffolding. During generation, the motif residues are guided to reconstruct the true motif at t= 1
while the flow model will adjust the scaffold trajectory to be consistent with the motif.
both motif amortization and guidance surpasses the performance of RFdiffusion, as measured by the number
of structurally uniquescaffolds3that pass the designability criterion.
This work is structured as follows. Sec. 2 provides background on SE(3)flow matching. We present our main
contribution extending FrameFlow for motif-scaffolding in Sec. 3. We develop motif amortization for flow
matching while motif guidance, originally developed for diffusion models, follows after drawing connections
between flow matching and diffusion models. Next we discuss related works Sec. 4 and present empirical
results Sec. 5. Our contributions are the following:
•We extend FrameFlow with two fundamentally different approaches for motif-scaffolding: motif amor-
tization and motif guidance. We are the first to extend conditional generation techniques with SE(3)
flow matching and apply them to motif-scaffolding. With all other settings kept constant, we perform a
empirical study of how each approach performs.
•On a benchmark of biologically meaningful motifs, we show our method can successfully scaffold 20 out of
24 motifs in the motif-scaffolding benchmark which is equivalent to previous state-of-the-art, while achiev-
ing 2.5 times more unique, designable scaffolds. Our results demonstrate the importance of measuring
diversity to detect mode collapse.
2 Background
Flow matching (FM) (Lipman et al., 2023; Albergo et al., 2023) is a simulation-free method for training
continuous normalizing flows (CNFs) (Chen et al., 2018). CNFs are deep generative models that generates
data by integrating an ordinary differential equation (ODE) over a learned vector field. Recently, flow
matching has been extended to Riemannian manifolds (Chen & Lipman, 2023), which we rely on to model
protein backbones via the local frame SE(3)representation. Sec. 2.1 gives an introduction to Riemannian
flow matching. Sec. 2.2 then describes how SE(3)flow matching is applied to protein backbones.
2.1 Flow matching on Riemannian manifolds
On a manifoldM, a CNFϕt(·) :M→M is defined via an ODE along a time-dependent vector field
v(z,t) :M×R→TzMwhereTzMis the tangent space of the manifold at z∈Mand time is t∈[0,1]:
d
dtϕt(z0) =v(ϕt(z0),t), ϕ0(z0) =z0. (1)
3The number of unique scaffolds is defined as the number of structural clusters. See Sec. 5.1
3Published in Transactions on Machine Learning Research (07/2024)
Starting with z0∼p0from an easy-to-sample prior distribution p0, simulating samples according to Eq. (1)
induces a new distribution referred as the push-forward pt= [ϕt]∗p0. One wishes to find a vector field v
such that the push-forward pt=1= [ϕt=1]∗p0(att= 1) matches the data distribution p1. Such a vector
fieldvis in general not available in closed-form, but can be learned by regressing conditional vector fields
u(zt,t|z1) =d
dtztwherezt=ϕt(z0|z1)interpolates between endpoints z0∼p0andz1∼p1. A natural choice
forztisthegeodesicpath: zt= expz0/parenleftbig
tlogz0(z1)/parenrightbig
, where expz0andlogz0aretheexponentialandlogarithmic
maps at the point z0. The conditional vector field takes the following form: u(zt,t|z1) = logzt(z1)/(1−t).
The key insight of conditional4flow matching (CFM) (Lipman et al., 2023) is that training a neural network
ˆvto regress the conditional vector field uis equivalent to learning the unconditional vector field v. This
corresponds to minimizing
L=EU(t;[0,1]),p1(z1),p0(z0)/bracketleftig
∥u(zt,t|z1)−ˆv(zt,t)∥2
g/bracketrightig
(2)
whereU(t; [0,1])is the uniform distribution for t∈[0,1]and∥·∥2
gis the norm induced by the Riemannian
metricg:TM×TM→ R. Samples can then be generated by integrating the ODE in Eq. (1) with Euler
steps using the learned vector field ˆvin place ofv.
2.2 Generative modeling on protein backbones
The atom positions of each residue in a protein backbone can be parameterized by an element T∈SE(3)of
the special Euclidean group SE(3)(Jumper et al., 2021; Yim et al., 2023b). We refer to T= (r,x)as a (local)
frame consisting of a rotation r∈SO(3)and translation vector x∈R3. The protein backbone is made of N
residues, meaning it can be parameterized by Nframes denoted as T= [T(1),...,T(N)]∈SE(3)N. We use
bold face to refer to vectors of all the residues, superscripts to refer to residue indices, and subscripts refer
to time. Details of the SE(3)Nbackbone parameterization can be found in App. B.1.
We use SE(3)flow matching to parameterize a generative model over the SE(3)Nrepresentation of protein
backbones. The application of Riemannian flow matching to SE(3)was previously developed in Yim et al.
(2023a); Bose et al. (2023). Endowing SE(3)with the product left-invariant metric, the SE(3)manifold
effectively behaves as the product manifold SE(3) = SO(3)×R3(App. D.3 of Yim et al. (2023b)). The
vector field over SE(3)can then be decomposed as v(n)
SE(3)(·,t) = (v(n)
R(·,t),v(n)
SO(3)(·,t)). Our goal is train a
neural network to parameterize the learned vector fields,
ˆv(n)
R(Tt,t) =ˆx(n)
1(Tt)−x(n)
t
1−t, ˆv(n)
SO(3)(Tt,t) =logr(n)
t(ˆr(n)
1(Tt))
1−t. (3)
The outputs of the neural network are denoised predictions ˆx(n)
1andˆr(n)
1which are used to calculate the
vector fields in Eq. (3). The loss becomes
LSE(3) =E/bracketleftig/vextenddouble/vextenddoubleuSE(3)(Tt,t|T1)−ˆ vSE(3)(Tt,t)/vextenddouble/vextenddouble2
SE(3)/bracketrightig
(4)
=E/bracketleftig
∥uR(xt,t|x1)−ˆ vR(Tt,t)∥2
R+/vextenddouble/vextenddoubleuSO(3) (rt,t|r1)−ˆ vSO(3) (Tt,t)/vextenddouble/vextenddouble2
SO(3)/bracketrightig
(5)
where the expectation is taken over U(t; [0,1]),p1(T1),p0(T0). We have used bold-face for collections of
elements, i.e. ˆ v(·) = [ˆv(1)(·),..., ˆv(N)(·)]. Our prior is chosen as p0(T0) =U(SO(3))N⊗N(0,I3)N, where
U(SO(3)) is the uniform distribution over SO(3)andN(0,I3)is the isotropic Gaussian where samples are
centered to the origin. Details of SE(3)flow matching such as architecture and hyperparameters closely
follow FrameFlow (Yim et al., 2023a), details of which are provided in App. B.2.
3 Motif-scaffolding with FrameFlow
We describe our two strategies for performing motif-scaffolding with the FrameFlow model: motif amor-
tization (Sec. 3.1) and motif guidance (Sec. 3.2). Recall the full protein backbone is given by T=
4Unfortunately the meaning of “conditional” is overloaded. The conditionals will be clear from the context.
4Published in Transactions on Machine Learning Research (07/2024)
Figure 2: Motif data augmentation. Each protein in the dataset does not come with pre-defined
motif-scaffold annotations. Instead, we construct plausible motifs at random to simulate sampling from the
distribution of motifs and scaffolds.
{T(1),T(2),...,T(N)}∈ SE(3)N. The residues can be separated into the motif TM={T(i1),...,T(ik)}
of lengthkwhere{i1,...,ik}⊂{ 1,...,N}are motif residue indices, and the scaffold TSis all the remaining
residues, such that T=TM∪TS. The task can then be framed as the problem of sampling from the
conditional distribution p(TS|TM).
3.1 Motif amortization
We train a variant of FrameFlow that additionally takes the motif as input when generating scaffolds (and
keeping the motif fixed). Formally, we model a motif-conditioned CNF via the following ODE,
d
dtϕt(TS
0|TM) =v(ϕt,t|TM), ϕ 0(TS
0|TM) =TS
0. (6)
Theflowϕttransformsapriordensityoverscaffoldsalongtime, inducingadensity pt(·|TM) = [ϕt]∗p0(·|TM).
We use the same prior as in Sec. 2.2: p0(TS
0|TM) =p0(TS
0). FrameFlow is trained to predict the
conditional vector field u(TS
t,t|TS
1,TM)where TS
tis defined by interpolating along the geodesic path,
TS
t= expTS
0/parenleftig
tlogTS
0(TS
1)/parenrightig
. The implication is that uis conditionally independent of the motif TMgiven
TS
1. This simplifies our formulation to u(TS
t,t|TS
1,TM) =u(TS
t,t|TS
1)that is defined in Sec. 2.2. However,
when we learn the vector field, the model needs to condition on TMsince the motif placement TMcontains
information on the true scaffold positions TS
1. The training loss becomes,
E/bracketleftig/vextenddouble/vextenddoubleuSE(3)(TS
t,t|TS
1)−ˆ vSE(3)(TS
t,t|TM)/vextenddouble/vextenddouble2
SE(3)/bracketrightig
(7)
where the expectation is taken over U(t; [0,1]),p(TM),p1(TS
1|TM),p0(TS
0). The above expectation requires
access to the motif and scaffold distributions, p(TM)andp1(TS
1|TM), during training. Future work can
look into incorporating known motif-scaffolds such as the CDR loops on antibodies (Dunbar et al., 2014).
While some labels exist for which residues correspond to the functional motif, the vast majority of protein
structures in the PDB do not have labels. We instead utilize unlabeled PDB structures to perform data
augmentation (see Sec. 3.1.1) that allows sampling a wide range of motifs and scaffolds.
To learn the motif-conditioned vector field ˆvt, we use the FrameFlow architecture with a 1D mask as ad-
ditional input with a 1 at the location of the motif and 0 elsewhere. To maintain SE(3)-equivariance, we
zero-center the motif and initial noise sample from p0(TS
0|TM). Zero-centering the motif also prevents the
model from using the motif offset from the origin to memorize scaffold locations which helps generalization.
3.1.1 Data augmentation.
The flow matching loss from Eq. (7) involves sampling from p(TM)andp1(TS
1|TM), which we do not have
access to, but can be approximated using unlabeled structures from the PDB. Our pseudo-labeled motifs
and scaffolds are generated as follows (also depicted in Fig. 2). First, a protein structure is sampled from the
PDB dataset. Second, a random number of residues are selected to be the starting locations of each motif.
Third, additional residues are appended onto each motif thereby extending their lengths. The length of each
motif is randomly sampled such that the total number of motif residues is between γminandγmaxpercent of
all the residues. We use γmin= 0.05andγmax= 0.5to ensure at least a few residues are used as the motif
5Published in Transactions on Machine Learning Research (07/2024)
but not more than half the protein. Finally, the remaining residues are treated as the scaffold and corrupted.
The motif and scaffold are treated as samples from p(TM)andp1(TS
1|TM)respectively. Importantly, each
protein will be re-used on subsequent epochs where new motifs and scaffolds will be sampled. Our pseudo
motif-scaffolds cover a wide range of scenarios that cover multiple motifs of different lengths.
The lack of functional annotations in the PDB requires training over all possible motif-scaffold annotations
to handle new scenarios our method may encounter in real world scenarios. In our experiments, we evaluate
how this data augmentation strategy transfers to real motif-scaffolding tasks. A similar strategy is used in
image infilling where image based diffusion models are trained to infill randomly masked crops of images to
approximate real image infilling scenarios (Saharia et al., 2022). Motif-scaffolding data augmentation was
mentioned in RFdiffusion but without algorithmic detail. Since RFdiffusion does not release training code,
we implemented our own data augmentation algorithm in Algorithm 1.
3.2 Motif guidance
We now present an alternative Bayesian approach to motif-scaffolding that does not involve learning a motif-
conditioned flow model. As such it does not require having access to motifs at training time, but only at
sampling time. This can be useful when an unconditional generative flow model is already available at hand
and additional training is too costly. The idea behind motif guidance, first described as a special case of
TDS (Wu et al., 2023) using diffusion models, is to use the desired motif TMto bias the model’s generative
trajectory such that the motif residues end up in their known positions. The scaffold residues follow a
trajectory that create a consistent whole protein backbone, thus achieving motif-scaffolding.
The key insight comes from connecting flow matching to diffusion models to which motif guidance can be
applied. The following ODE describes the relationship between the vector field ˆ vin flow models – learned
by minimizing CFM objective in Eq. (5) – and the Stein score ∇logpt(Tt),
dTt=ˆ v(Tt,t)dt=/bracketleftbigg
f(Tt,t)−1
2g(t)2∇logpt(Tt)/bracketrightbigg
dt. (8)
The gradient is taken with respect to the backbone at time twhich we omit for brevity, i.e. ∇=∇Tt.
Eq. (8) shows the ODE used to sample from flow models can be written as the probability flow ODE used
in diffusion models (Song et al., 2020) with fandgas the drift and diffusion coefficients. The derivation of
Eq. (8) requires standard linear algebra and calculus for our choice of vector field (see App. D).
Our goal is to sample from the conditional p(T|TM)from which we can extract p(TS|TM). The benefit of
Eq.(8)iswecanmanipulatethescoretermtoachievethisgoal. Wemodifytheabovetobeconditionedonthe
motif TMfollowed by an application of Bayes rule where ∇logpt(Tt|TM) =∇logpt(Tt)+∇logpt(TM|Tt).
dTt=/bracketleftig
f(Tt,t)−1
2g(t)2∇logpt(Tt|TM)/bracketrightig
dt (9)
=/bracketleftig
f(Tt,t)−1
2g(t)2/parenleftig
∇logpt(Tt) +∇logpt(TM|Tt)/parenrightig/bracketrightig
dt
=/bracketleftig
ˆ vSE(3)(Tt,t)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
unconditional pred.−1
2g(t)2∇logpt(TM|Tt)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
guidance term/bracketrightig
dt. (10)
We can interpret Eq. (9) as doing unconditional generation by following ˆvSE(3)(T,t)while∇logpt(TM|Tt)
guides the noised residues so as to be consistent with the true motif. Doob’s H-transform ensures Eq. (9)
will sample from p(T|TM)(Didi et al., 2023). The conditional score ∇logpt(TM|Tt)is unknown, yet it can
be approximated by marginalising out T1and using the neural network’s denoised output (Song et al., 2022;
Chung et al., 2022; Wu et al., 2023),
pt(TM|Tt) =/integraldisplay
p(TM|T1)p1|t(T1|Tt)dT1 (11)
≈/integraldisplay
p(TM|T1)δˆTM
1(Tt)(Tt)dT1=p(TM|ˆTM
1(Tt)). (12)
6Published in Transactions on Machine Learning Research (07/2024)
We now have the choice to define the likelihood in Eq. (12) to have higher probability the closer it is to the
desired motif:
p(TM|ˆTM
1(Tt))∝exp/parenleftbig
−∥xM−ˆxM
1(Tt)∥2
R/ω2
t/parenrightbig
exp/parenleftig
−∥rM−ˆrM
1(Tt)∥2
SO(3)/ω2
t/parenrightig
, (13)
which isinversely proportional tothe distance fromthe desired motif. Following SE(3)flow matching, Eq.(9)
becomes factorized into the translation and rotation components. Plugging p(TM|ˆTM
1(Tt))in Eq. (9), we
arrive at the following ODE we may sample p(T|TM)from
Translations: dxt=/bracketleftig
ˆ vR(Tt,t) +1
2g(t)2∇xt∥xM−ˆxM
1(Tt)∥2
R/ω2
t/bracketrightig
dt. (14)
Rotations: drt=/bracketleftig
ˆ vSO(3) (Tt,t) +1
2g(t)2∇rt∥rM−ˆrM
1(Tt)∥2
SO(3)/ω2
t/bracketrightig
dt. (15)
ωtis a hyperparameter that controls the magnitude of the guidance towards the desired motif which we set
toω2
t= (1−t)2/(t2+ (1−t)2)as done in Pokle et al. (2023); Song et al. (2021). While different choices of
g(t)are possible, Pokle et al. (2023) proposed to use g(t) = (1−t)/twith the motivation that this matches
the diffusion coefficient for the diffusion SDE that matches the marginals of the flow ODE. For completeness,
we provide the proof for g(t)in App. D. A similar calculation is non-trivial for SO(3), hence we use the same
g(t)as a reasonnable heuristic and observe good performance as done in (Wu et al., 2023).
4 Related work
Conditional diffusion and flows. The development of conditional generation methods for diffusion and
flow models is an active area of research. Two popular diffusion techniques that have been extended to flow
matching are classifier-free guidance (CFG) (Dao et al., 2023; Ho & Salimans, 2022; Zheng et al., 2023)
and reconstruction guidance (Pokle et al., 2023; Ho et al., 2022; Song et al., 2022; Chung et al., 2022).
Motif guidance is an application of reconstruction guidance for motif-scaffolding. Motif amortization is most
related to data-dependent couplings (Albergo et al., 2023), where a flow is learned with conditioning of
partial data.
Motif-scaffolding. Wang et al. (2021) first formulated motif-scaffolding using deep learning. SMCDiff
(Trippe et al., 2022) was the first proposed diffusion model for motif-scaffolding using Sequential Monte
Carlo (SMC). Twisted Diffusion Sampler (TDS) (Wu et al., 2023) later improved upon SMCDiff using
reconstruction guidance for each particle in SMC. Our motif guidance method follows from TDS (with one
particle) by deriving the equivalent guidance vector field from its conditional score counterpart. RFdiffusion
(Watson et al., 2023) fine-tunes a pre-trained neural network with motif-conditioned diffusion training. Our
FrameFlow-amortization approach in principle follows RFdiffusion’s diffusion training, but differs in (i) using
flow matching, (ii) not relying expensive pre-training, and (iii) uses a 3×smaller neural network5. Didi et al.
(2023) provides a survey of structure-based motif-scaffolding methods while proposing Doob’s h-transform
for motifs-scaffolding. EvoDiff (Alamdari et al., 2023) differs in using a sequence-based diffusion model
that performs motif-scaffolding with language model-style masked generation but performance falls short of
RFdiffusion and TDS.
5 Experiments
In this section, we report the results of training FrameFlow for motif-scaffolding. Sec. 5.1 describes training,
sampling, and metrics. Our main results on motif-scafolding are reported in Sec. 5.2 on the benchmark
introduced in RFdiffusion. Additional motif-scaffolding analysis is provided in App. G.
5.1 Set-up
Training. We train two FrameFlow models. FrameFlow-amortization is trained with motif amortization
as described in Sec. 3.1 with data augmentation using hyperparameters: γmin= 0.05so the motif is never
5FrameFlow uses 16.8 million parameters compared to RFdiffusion’s 59.8 million.
7Published in Transactions on Machine Learning Research (07/2024)
Figure3: Motif-scaffoldingresults. Topplot: RFdiffusionachievesthemostdesignablescaffoldsamongst
all methods in 9/24 test motifs compared to FrameFlow-amortization’s 7/24 and TDS’ 6/24; 2/24 are ties.
Bottom plot: However, we observe that RFdiffusion produces the highest number of unique designable scaf-
folds for only 2 out of the 24 test motifs. Therefore, previous approaches that only measure designability
(top plot) may be misleading since those generative models that may have the best designability can also be
repeatedly sampling similar scaffolds. This demonstrates the need to measure diversity alongside designabil-
ity and use the number of unique designable scaffolds as the metric of success.
degenerately small and γmax= 0.5to avoid motif being the majority of the backbone. FrameFlow-guidance,
to be used in motif guidance, is trained unconditionally on full backbones. Since unconditional generation
is not our focus, we leave the unconditional performance to App. F where we see the performance is slightly
worse than RFdiffusion – as we will see, the motif-scaffolding performance is better. Both models are trained
using the filtered PDB monomer dataset introduced in FrameDiff. We use the ADAM optimizer (Kingma
& Ba, 2014) with learning rate 0.0001. We train each model for 6 days on 2 A6000 NVIDIA GPUs with
dynamic batch sizes depending on the length of the proteins in each batch — a technique from FrameDiff.
Sampling. We use the Euler-Maruyama integrator with 500 timesteps for all sampling. Following the
motif-scaffolding benchmark proposed in RFdiffusion, we sample 100 scaffolds for each of the 24 monomer
motifs6. For each motif, the method must sample novel scaffolds with different lengths and different motif
locations along the sequence. The benchmark measures how well a method can generalize beyond the native
scaffolds for a set of biologically important motifs.
Hyperparameters. Our hyperparameters for neural network architecture, optimizer, and sampling steps
all follow the best settings found in FrameFlow (Yim et al., 2023a). We leave hyperparameter search as a
future work since it is not the focus of this work.
5.2 Motif-scaffolding results
Baselines. We consider RFdiffusion and the Twisted Diffusion Sampler (TDS) as baselines. RFdiffusion’s
performance is reported based on their published samples. TDS reported motif-scaffolding results with
arbitrary scaffold lengths that deviated the benchmark. Therefore, we re-ran TDS with their best settings
usingk= 8particles on the RFdiffusion benchmark. We refer to FrameFlow-amortization as our results
with motif amortization while FrameFlow-guidance uses motif guidance.
Metrics. Previously, motif-scaffolding was only evaluated through samples passing designability (Des.).
For a description of designability see App. E. Within the set of designable scaffolds, we also calculate the
6The benchmark has 25 motifs, but the motif 6VW1 involves multiple chains that FrameFlow cannot handle.
8Published in Transactions on Machine Learning Research (07/2024)
Figure 4: FrameFlow-amortization diversity. In blue is the motif while red is the scaffold. For each
motif (1QJG, 1YCR, 5TPN), we show FrameFlow-amortization can generate scaffolds of different lengths
and various secondary structure elements for the same motif. Each scaffold is in a unique cluster to showcase
the samples’structural diversity.
diversity (Div.) as the number of structurally unique clusters. This is crucial since designability can
be manipulated to have a 100% success rate by always sampling the same scaffold with trivial changes.
In real world scenarios, diversity is desired to gain the most informative feedback from expensive wet-lab
experiments (Yang et al., 2019). Thus diversity provides an additional data point to check for mode collapse
where the model is sampling the same scaffold repeatedly. Clusters are computed using MaxCluster (Herbert
& Sternberg, 2008) with TM-score threshold set to 0.5.
Benchmark. Fig. 3 shows how each method fares against each other in designability and diversity on each
motif of the motif-scaffolding benchmark. While it appears RFdiffusion gets lots of successful scaffolds, the
number of uniquescaffolds is far lower than both our FrameFlow approaches. TDS achieves lower designable
scaffolds on average, but demonstrates strong performance on a small subset of motifs. There are some motifs
that only RFdiffusion can solve (7MRX_85, 7MRX_128) while FrameFlow is able to solve cases RFdiffusion
cannot (1QJG, 4JHW, 5YUI).
Table 1: Motif-scaffolding aggregate metrics
Method Solved ( ↑) Div. (↑) Speed (↓)
FrameFlow-amort. 20 353 18s
FrameFlow-guid. 20 192 18s
RFdiffusion 20 141 50s
TDS 19 217 117s
Tab. 1 provides the number of motifs each method solves – which means at least one designable scaffold is
sampled – and the number of total designable clusters sampled across all motifs. Here we see each method
can solve 19-20 solves motifs, but FrameFlow-amortization can achieve nearly double the number of unique
scaffolds (clusters) as RFdiffusion. FrameFlow-amortization outperforms FrameFlow-guidance on diversity.
A potential reason for the improved diversity is the use of SE(3)flow matching in the unconditional model
whereas TDS uses SE(3)diffusion (Yim et al., 2023b). Bose et al. (2023) found SE(3) flow matching to
provide far better designability and diversity than its diffusion counterpart. Empirically, it is known flow
matching outperforms diffusion on Riemannian manifolds (Chen & Lipman, 2023).
Inthelastcolumnwegivethenumberofsecondstosamplealength100proteinonaA6000NvidiaGPUwith
each method. Both FrameFlow methods are significantly faster than RFdiffusion and TDS. TDS is notably
slower since its run time scales with its number of particles. We conclude that FrameFlow-amortization
matches RFdiffusion and TDS on the number of solved motifs while achieving much higher diversity and
faster inference.
9Published in Transactions on Machine Learning Research (07/2024)
Figure 5: Secondary structure analysis. 2D kernel density plots of secondary structure composition of
designable motif-scaffolds from FrameFlow-amortization and RFdiffusion. Here we see RFdiffusion tends to
mostly generate helical scaffolds while FrameFlow-amortization gets much more scaffolds with strands.
Diversityanalysis. Tovisualizethediversityofthescaffolds, Fig.4showsseveraloftheclustersformotifs
1QJG, 1YCR, and 5TPN where FrameFlow can generate significantly more clusters than RFdiffusion. Each
scaffold demonstrates a wide range of secondary structure elements across multiple lengths. To quantify
this in more depth, Fig. 5 plot the helical and strand compositions (computed with DSSP (Kabsch &
Sander, 1983)) of designable motif-scaffolds from FrameFlow-amortization compared to RFdiffusion. We see
FrameFlow-amotization achieves a better spread of secondary structure components than RFdiffusion. A
potential reason for RFdiffusion’s overall lower diversity is due to its lack of secondary structure diversity
– favoring to sample mostly helical structures. App. G provides additional analysis into the FrameFlow
motif-scaffolding results. We conclude FrameFlow-amortization achieves much more structural diversity
than RFdiffusion.
6 Discussion
In this work, we present two methods building on FrameFlow for tackling motif-scaffolding. These methods
can be used with any flow-based model. First, with motif-amortization we adapt the training of FrameFlow
to additionally be conditioned on the motif — in effect turning FrameFlow into a conditional generative
model. Second, with motif guidance, we use an unconditionally trained FrameFlow for the task of motif-
scaffolding though without any additional task-specific training. We empirically evaluated both approaches,
FrameFlow-amortization and FrameFlow-guidance, on the motif-scaffolding benchmark from RFdiffusion
where we find both methods achieve competitive results with state-of-the-art methods. Moreover, they are
able to sample more unique scaffolds and achieve higher diversity. It is important to note amortization
and guidance are complementary techniques. Amortization outperforms guidance but requires conditional
training while guidance can use unconditional flow models without further training. Guidance generally
performs worse due to approximation error in Eq. (12) from using an unconditional model in conditional
task. We stress the need to report both success rate and diversity to detect when a model suffers from mode
collapse. Lastly, we caveat that all our results and metrics are computational, which may not necessarily
transfer to wet-lab success.
Future directions. We have extended FrameFlow for motif-scaffolding; further extensions include binder,
enzyme, and symmetric design — all which RFdiffusion can currently achieve. For these capabilities, we
requireextendingFrameFlowtohandlemultimericproteins. Whilemotifguidancedoesnotoutperformmotif
amortization, it is possible extending TDS to flow matching could close that gap. Related to guidance, one
could explore conditioning mechanisms to control properties of the scaffold such as its secondary structure.
We make use of a heuristic for Riemannian reconstruction guidance that may be further improved. Despite
our progress, there still remains areas of improvement to achieve success in all 25 motifs in the benchmark.
References
Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and
Kevin K Yang. Protein generation with evolutionary diffusion: sequence is all you need. bioRxiv, pp.
2023–09, 2023.
10Published in Transactions on Machine Learning Research (07/2024)
MichaelSAlbergo, MarkGoldstein, NicholasMBoffi, RajeshRanganath, andEricVanden-Eijnden. Stochas-
tic interpolants with data-dependent couplings. arXiv preprint arXiv:2310.03725 , 2023.
Minkyung Baek, Ivan Anishchenko, Ian Humphreys, Qian Cong, David Baker, and Frank DiMaio. Efficient
and accurate prediction of protein structure using rosettafold2. bioRxiv, pp. 2023–05, 2023.
Nathaniel R Bennett, Brian Coventry, Inna Goreshnik, Buwei Huang, Aza Allen, Dionne Vafeados, Ying Po
Peng, Justas Dauparas, Minkyung Baek, Lance Stewart, et al. Improving de novo protein binder design
with deep learning. Nature Communications , 14(1):2625, 2023.
Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N
Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research , 28(1):235–242, 2000.
Avishek Joey Bose, Tara Akhound-Sadegh, Kilian Fatras, Guillaume Huguet, Jarrid Rector-Brooks, Cheng-
Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, and Alexander Tong. Se (3)-
stochastic flow matching for protein backbone generation. arXiv preprint arXiv:2310.02391 , 2023.
Ricky T. Q. Chen and Yaron Lipman. Riemannian Flow Matching on General Geometries, February 2023.
URL http://arxiv.org/abs/2302.03660 .
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. Advances in neural information processing systems , 31, 2018.
Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior
sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687 , 2022.
BrunoECorreia, JohnTBates, RebeccaJLoomis, GretchenBaneyx, ChrisCarrico, JosephGJardine, Peter
Rupert, Colin Correnti, Oleksandr Kalyuzhniy, Vinayak Vittal, Mary J Connell, Eric Stevens, Alexandria
Schroeter, Man Chen, Skye Macpherson, Andreia M Serra, Yumiko Adachi, Margaret A Holmes, Yuxing
Li, Rachel E Klevit, Barney S Graham, Richard T Wyatt, David Baker, Roland K Strong, James E Crowe,
Jr, Philip R Johnson, and William R Schief. Proof of principle for epitope-focused vaccine design. Nature,
507(7491):201–206, 2014.
Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. arXiv preprint
arXiv:2307.08698 , 2023.
J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet,
R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan, B. Koepnick,
H. Nguyen, A. Kang, B. Sankaran, A. K. Bera, N. P. King, and D. Baker. Robust deep learning-based
protein sequence design using ProteinMPNN. Science, 378(6615):49–56, 2022.
Kieran Didi, Francisco Vargas, Simon V Mathis, Vincent Dutordoir, Emile Mathieu, Urszula J Komorowska,
and Pietro Lio. A framework for conditional diffusion modelling with applications in motif scaffolding for
protein design. arXiv preprint arXiv:2312.09236 , 2023.
James Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges, Jiye Shi, and
Charlotte M Deane. Sabdab: the structural antibody database. Nucleic acids research , 42(D1):D1140–
D1146, 2014.
Alex Herbert and MJE Sternberg. MaxCluster: a tool for protein structure comparison and clustering. 2008.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 , 2022.
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet.
Video diffusion models, 2022.
John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frap-
pier, Dana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with
a programmable generative model. Nature, pp. 1–9, 2023.
11Published in Transactions on Machine Learning Research (07/2024)
Lin Jiang, Eric A Althoff, Fernando R Clemente, Lindsey Doyle, Daniela Rothlisberger, Alexandre
Zanghellini, Jasmine L Gallaher, Jamie L Betker, Fujie Tanaka, Carlos F Barbas III, Donald Hilvert,
Kendal N Houk, Barry L. Stoddard, and David Baker. De novo computational design of retro-aldol
enzymes. Science, 319(5868):1387–1391, 2008.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure
prediction with alphafold. Nature, 2021.
Wolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: pattern recognition
of hydrogen-bonded and geometrical features. Biopolymers: Original Research on Biomolecules , 22(12):
2577–2637, 1983.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Leon Klein, Andreas Krämer, and Frank Noé. Equivariant flow matching. arXiv preprint arXiv:2306.15030 ,
2023.
Jonas Köhler, Leon Klein, and Frank Noé. Equivariant flows: exact likelihood generative learning for
symmetric densities. In International conference on machine learning , pp. 5361–5370. PMLR, 2020.
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for
generative modeling. International Conference on Learning Representations , 2023.
Dmitry I Nikolayev and Tatjana I Savyolov. Normal distribution on the rotation group SO(3). Textures and
Microstructures , 29, 1970.
Ashwini Pokle, Matthew J Muckley, Ricky TQ Chen, and Brian Karrer. Training-free linear image inversion
via flows. arXiv preprint arXiv:2310.04432 , 2023.
Erik Procko, Geoffrey Y Berguig, Betty W Shen, Yifan Song, Shani Frayo, Anthony J Convertine, Daciana
Margineantu, Garrett Booth, Bruno E Correia, Yuanhua Cheng, William R Schief, David M Hockenbery,
Oliver W Press, Barry L Stoddard, Patrick S Stayton, and David Baker. A computationally designed
inhibitor of an Epstein-Barr viral BCL-2 protein induces apoptosis in infected cells. Cell, 157(7):1644–
1656, 2014.
Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and
Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference
proceedings , pp. 1–10, 2022.
Simo Särkkä and Arno Solin. Applied Stochastic Differential Equations . Cambridge University Press,
1 edition, April 2019. ISBN 978-1-108-18673-5 978-1-316-51008-7 978-1-316-64946-6. doi: 10.1017/
9781108186735.
Neta Shaul, Ricky TQ Chen, Maximilian Nickel, Matthew Le, and Yaron Lipman. On kinetic optimal
probability paths for generative models. In International Conference on Machine Learning , pp. 30883–
30907. PMLR, 2023.
Justin B Siegel, Alexandre Zanghellini, Helena M Lovick, Gert Kiss, Abigail R Lambert, Jennifer L StClair,
Jasmine L Gallaher, Donald Hilvert, Michael H Gelb, Barry L Stoddard, Kendall N Houk, Forrest E
Michael, and David Baker. Computational design of an enzyme catalyst for a stereoselective bimolecular
Diels-Alder reaction. Science, 329(5989):309–313, 2010.
Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for
inverse problems. In International Conference on Learning Representations , 2022.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456 , 2020.
12Published in Transactions on Machine Learning Research (07/2024)
Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with
score-based generative models. arXiv preprint arXiv:2111.08005 , 2021.
Brian L Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi
Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem.
arXiv preprint arXiv:2206.04119 , 2022.
Jue Wang, Sidney Lisanza, David Juergens, Doug Tischer, Ivan Anishchenko, Minkyung Baek, Joseph L
Watson, Jung Ho Chun, Lukas F Milles, Justas Dauparas, Marc Exposit, Wei Yang, Amijai Saragovi,
Sergey Ovchinnikov, and David A. Baker. Deep learning methods for designing proteins scaffolding func-
tional sites. bioRxiv, 2021.
Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach,
Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein
structure and function with rfdiffusion. Nature, pp. 1–3, 2023.
Luhuan Wu, Brian L Trippe, Christian A Naesseth, David M Blei, and John P Cunningham. Practical and
asymptotically exact conditional sampling in diffusion models. arXiv preprint arXiv:2306.17775 , 2023.
Kevin K Yang, Zachary Wu, and Frances H Arnold. Machine-learning-guided directed evolution for protein
engineering. Nature methods , 16(8):687–694, 2019.
Jason Yim, Andrew Campbell, Andrew YK Foong, Michael Gastegger, José Jiménez-Luna, Sarah Lewis,
Victor Garcia Satorras, Bastiaan S Veeling, Regina Barzilay, Tommi Jaakkola, et al. Fast protein backbone
generation with se (3) flow matching. arXiv preprint arXiv:2310.05297 , 2023a.
Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and
Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. arXiv preprint
arXiv:2302.02277 , 2023b.
Qinqing Zheng, Matt Le, Neta Shaul, Yaron Lipman, Aditya Grover, and Ricky TQ Chen. Guided flows for
generative modeling and decision making. arXiv preprint arXiv:2311.13443 , 2023.
13Published in Transactions on Machine Learning Research (07/2024)
Appendix
A Organisation of appendices
The appendix is organized as follows. App. B provides details and derivations for FrameFlow (Yim et al.,
2023a) that we introduce in Sec. 2.2. App. D provides derivation of motif guidance used in Sec. 3.2. Des-
ignability is an important metric in our experients, so we provide a description of it in App. E. Lastly, we
include additional results on unconditional generation App. F and motif-scaffolding App. G.
B FrameFlow details
B.1 Backbone SE(3) representation
A protein can be described by its sequence of residues, each of which takes on a discrete value from a vocabu-
lary of amino acids, as well as the 3D structure based on the positions of atoms within each residue. The 3D
structure in each residue can be separated into the backbone and side-chain atoms with the composition of
backbone atoms being constant across all residues while the side-chain atoms vary depending on the amino
acid assignment. For this reason, FrameFlow and previous SE(3)diffusion models (Watson et al., 2023; Yim
et al., 2023b) only model the backbone atoms with the amino acids assumed to be unknown. A second
model is typically used to design the amino acids after the backbone is generated. Each residue’s backbone
atoms follows a repeated arrangement with limited degrees of freedom due to the rigidity of the covalent
bonds. AlphaFold2 (AF2) (Jumper et al., 2021) proposed a SE(3)parameterization of the backbone atoms
that we show in Fig. 6. AF2 uses a mapping of four backbone atoms to a single translation and rotation
that reduces the degrees of freedom in the modeling. It is this SE(3)representation we use when modeling
protein backbones. We refer to Appendix I of Yim et al. (2023b) for algorithmic details of mapping between
elements of SE(3)and backbone atoms.
Figure 6: Backbone parameterization with SE(3). (a) Shows the full protein backbone atomic structure
without side-chains. (b) Zooms in the backbone atoms of a single residue. Note the repeated arrangements
of backbone atoms in each residue. (c) The transformation of turning each set of four backbone atoms into
an element of SE(3).
B.2 SE(3) flow matching implementation
This section provides implementation details for SE(3)flow matching and FrameFlow. As stated in Sec. 2.1,
SE(3)Ncan be characterized as the product manifold SE(3)N=RN×SO(3)N. It follows that flow matching
onSE(3)Nis equivalent to flow matching on R3NandSO(3)N. We will parameterize backbones with N
14Published in Transactions on Machine Learning Research (07/2024)
residues T= (x,r)∈SE(3)Nby translations x∈R3Nand rotations r∈SO(3)N. As a reminder, we use
bold face for vectors of all the residue: T= [T(1),...,T(N)],x= [x(1),...,x(N)],r= [r(1),...,r(N)].
Riemannian flow matching (Sec. 2.1) proceeds by defining the conditional flows,
x(n)
t= (1−t)x(n)
0+tx(n)
1, r(n)
t= expr(n)
0/parenleftig
tlogr(n)
0(r(n)
1)/parenrightig
, (16)
for each residue n∈{1,...,N}. As priors we use x(n)
0∼N(0,I3)andr(n)
0∼U(SO(3)).N(0,I3)is the
isotropic Gaussian in 3D with centering where each sample is centered to have zero mean – this is important
for equivariance later on. U(SO(3))is the uniform distribution over SO(3). The end points x(n)
1andr(n)
1are
samples from the data distribution p1.
Eq. (16) uses the geodesic path with linear interpolation; however, alternative conditional flows can be used
(Chen & Lipman, 2023). A special property of SO(3)is that expr0andlogr0can be computed in closed form
using the well known Rodrigues’ formula. The corresponding conditional vector fields are
u(n)
R(x(n)
t,t|x(n)
1) =x(n)
1−x(n)
t
1−t, u(n)
SO(3)(r(n)
t,t|r(n)
1) =logr(n)
t(r(n)
1)
1−t. (17)
We train neural networks to regress the conditional vector fields through the following parameterization,
ˆv(n)
R(Tt,t) =ˆx(n)
1(Tt)−x(n)
t
1−t,ˆv(n)
SO(3)(Tt,t) =logr(n)
t(ˆr(n)
1(Tt))
1−t, (18)
where the neural network outputs the denoised predictions ˆx(n)
1andˆr(n)
1while using the noised backbone
Ttas input. We now modify the loss from Eq. (5) with practical details from FrameFlow,
LSE(3) =EU(t;0,1),p1(T1),p0(T0)/bracketleftbig
LR(Tt,T1,t) + 2LSO(3) (Tt,T1,t) +1(t>0.5)Laux(Tt,T1,t)/bracketrightbig
(19)
LR(Tt,T1,t) =∥uR(xt|x1,t)−ˆ vR(Tt,t)∥2
R=∥x1−ˆx1∥2
R
(1−min(t,0.9))2(20)
LSO(3) (Tt,T1,t) =/vextenddouble/vextenddoubleuSO(3) (rt|r1,t)−ˆ vSO(3) (Tt,t)/vextenddouble/vextenddouble2
SO(3)=/vextenddouble/vextenddouble/vextenddoublelogr(n)
t(r(n)
1)−logr(n)
t(ˆr(n)
1(Tt))/vextenddouble/vextenddouble/vextenddouble2
SO(3)
(1−min(t,0.9))2.(21)
We up weight the SO(3)lossLSO(3)such that it is on a similar scale as the translation loss LR. Eq. (20)
is simplified to be a loss directly on the denoised predictions. Both Eq. (20) and Eq. (21) have modified
denominators (1−min(t,0.9))−2instead of (1−t)−1to avoid the loss blowing up near t≈1. In practice, we
sampletuniformly fromU[ϵ,1]for smallϵ. Lastly,Lauxis taken from section 4.2 in Yim et al. (2023b) where
they apply a RMSD loss over the full backbone atom positions and pairwise distances. We found using Laux
for allt>0.5to be helpful. The remainder of this section goes over additional details in FrameFlow.
Alternative SO(3) prior. Yim et al. (2023a) reported using the IGSO3(σ= 1.5)prior (Nikolayev &
Savyolov, 1970) for SO(3)instead ofU(SO(3)) lead to improved performance. The choice of σ= 1.5will
shift ther0samples away from πwhere near degenerate solutions can arise in the geodesic. We follow using
IGSO3(σ= 1.5)for training while using the U(SO(3))prior for sampling.
Pre-alignment. Following (Klein et al., 2023) and Shaul et al. (2023), we pre-align samples from the prior
and the data by using the Kabsch algorithm to align the noise with the data to remove any global rotation
that results in a increased kinetic energy of the ODE. Specifically, for translation noise x0∼N(0,I3)Nand
data x1∼p1where x0,x1∈R3×Nwe solver∗= arg minr∈SO(3)∥rx0−x1∥2
Rand use the alignednoiser∗x0
during training. Yim et al. (2023a) found this to aid in training efficiency which we adopt.
15Published in Transactions on Machine Learning Research (07/2024)
Symmetries. We perform all modelling within the zero center of mass (CoM) subspace of RN×3as in Yim
et al. (2023b). This entails simply subtracting the CoM from the prior sample x0and all datapoints x1. As
xtis a linear interpolation between the noise sample and data, xtwill have 0CoM also. This guarantees
that the distribution of sampled frames that the model generates is SE(3)-invariant. To see this, note that
the prior distribution is SE(3)-invariant and the learned vector field vSE(3)is equivariant because we use an
SE(3)-equivariant architecture. Hence by Köhler et al. (2020), the push-forward of the prior under the flow
is invariant.
Auxiliary losses. We use the same auxiliary losses in (Yim et al., 2023a).
SO(3) inference scheduler. The conditional flow in Eq. (16) uses a constant linear interpolation along
the geodesic path where the distance of the current point xto the endpoint x1is given by a pre-metric
dg:M×M→ Rinduced by the Riemannian metric gon the manifold. To see this, we first recall the
general form of the conditional vector field with x,x1∈Mis given as follows (Chen & Lipman, 2023),
ut(x|x1) =d logκ(t)
dtd(x,x1)∇d(x,x1)
∥∇d(x,x1)∥2(22)
=d logκ(t)
dt∇d(x,x1)2
2∥∇d(x,x1)∥2(23)
=d logκ(t)
dt−logx(x1)
∥∇d(x,x1)∥2(24)
=−d logκ(t)
dtlogx(x1), (25)
withκ(t)a monotonically decreasing differentiable function satisfying κ(0) = 1andκ(1) = 0, referred as the
interpolation rate7. Then plugging in a the linear schedule κ(t) = 1−t, we recover Eq. (17)
ut(x|x1) =−d logκ(t)
dtlogx(x1) =1
1−tlogx(x1). (26)
However, we found this interpolation rate to perform poorly for SO(3)for inference time. Instead, we utilize
an exponential scheduler κ(t) =e−ctfor some constant c. The intuition being that for high c, the rotations
accelerate towards the data faster than the translations which evolve according to the linear schedule. The
SO(3)conditional flow in Eq. (16) and vector field in Eq. (17) become the following with the exponential
schedule,
rt= expr0/parenleftbig/parenleftbig
1−e−ct/parenrightbig
logr0(r1)/parenrightbig
(27)
v(n)
r=clogr(n)
t/parenleftig
ˆr(n)
1/parenrightig
. (28)
We findc= 10or5to work well and use c= 10in our experiments. Interestingly, we found the best
performance when κ(t) = 1−twas used for SO(3)during training while κ(t) =e−ctis used during inference.
We found using κ(t) =e−ctduring training made training too easy with little learning happening.
The vector field in Eq. (28) matches the vector field in FoldFlow when inference annealing is performed (Bose
et al., 2023). However, their choice of scaling was attributed to normalizing the predicted vector field rather
than the schedule. Indeed they proposed to linearly scale up the learnt vector field via λ(t) = (1−t)cat
sampling time, i.e. to simulate the following ODE:
drt=λ(t)v(rt,t)dt.
However, as hinted at earlier, this is equivalent to using at sampling time a different vector field ˜v(rt,t)—
induced by an exponential schedule ˜κ(t) =e−ct—instead of the linearscheduleκ(t) = 1−t(that the neural
7κ(t)actsas aschedulerthat determinesthe rateat which d(·|x1)decreases, since we havethat ϕtdecreasesd(·,x1)according
tod(ϕt(x0|x1),x1) =κ(t)d(x0,x1)(Chen & Lipman, 2023).
16Published in Transactions on Machine Learning Research (07/2024)
network ˆrθ
1was trained with). Indeed we have
˜v(rt,t) =−∂tlog ˜κ(t) logrt(ˆr1) =−−∂tlog ˜κ(t)
−∂tlogκ(t)∂tlogκ(t) logrt(ˆr1) (29)
=−c(1−t)∂tlogκ(t) logrt(ˆr1) =c(1−t)v(rt,t) =λ(t)v(rt,t). (30)
C Data augmentation
Algorithm 1 Motif-scaffolding data augmentation
Require: Protein backbone T; Min and max motif percent γmin= 0.05,γmax= 0.5.
1:s∼Uniform{⌊N·γmin⌋,...,⌊N·γmax⌋} ▷Sample maximum motif size.
2:m∼Uniform{1,...,s} ▷Sample maximum number of motifs.
3:TM←∅
4:fori∈{1,...,m}do
5:j∼Uniform{1,...,N}\TM▷Sample location for each motif
6:ℓ∼Uniform{1,...,s−m+i−|TM|} ▷Sample length of each motif.
7: TM←TM∪{Tj,...,T min(j+ℓ,N)} ▷Append to existing motif.
8:end for
9:TS←{T1,...,TN}\TM▷Assign rest of residues as the scaffold
10:return TM,TS
D Motif guidance details
For the sake of completeness, we derive in this section the guidance term in Eq. (8) for the flow matching
setting. In particular, we want to derive the conditional vector field v(xt,t|y)in terms of the unconditional
vector field v(xt,t)and the correction term ∇logpt(y|xt). Beware, in the following we adopt the time
notation from diffusion models, i.e. t= 0for denoised data to t= 1for fully noised data. We therefore need
to swapt→1−tin the end results to revert to the flow matching notations.
Let’s consider the process associated with the following noising stochastic differential equation (SDE)
dxt=f(xt,t)dt+g(t)dBt (31)
which admits the following time-reversal denoising process
dxt=/bracketleftbig
f(xt,t)−g(t)2∇logpt(xt)/bracketrightbig
dt+g(t)dBt. (32)
Thanks to the Fokker-Planck equation, we know that the the following ordinary differential equation admits
the same marginal as the SDE Eq. (32):
dxt=/bracketleftbigg
f(xt,t)−1
2g(t)2∇logpt(xt)/bracketrightbigg
dt
=v(xt,t)dt. (33)
withv(xt,t)being the probability flow vector field.
Now, conditioning on some observation y, we have
dxt=v(xt,t|y)dt
=/bracketleftbigg
f(xt,t)−1
2g(t)2∇logpt(xt|y)/bracketrightbigg
dt
=/bracketleftbigg
f(xt,t)−1
2g(t)2(∇logpt(xt) +∇logpt(y|xt))/bracketrightbigg
dt
=/bracketleftbigg
v(xt,t)−1
2g(t)2∇logpt(y|xt)/bracketrightbigg
dt. (34)
17Published in Transactions on Machine Learning Research (07/2024)
Eq. (34) follows from the same reverse SDE theory of Eq. (32) except the initial state distribution is p(xt|y)).
The driftf(xt,t)and diffusion g(t)coefficients are unchanged while only the score reflect the new initial
distribution. More details can be found in App. I of Song et al. (2020). We only need to know g(t)to
adapt reconstruction guidance–which estimates ∇logpt(y|xt)–to the flow matching setting where we want
to correct the vector field. Given a particular choice of interpolation xtfrom flow matching, let’s derive the
associatedg(t).
Euclidean setting Assumex0is data and x1is noise, with x1∼N(0,I). In Euclidean flow matching, we
assume a linear interpolation xt= (1−t)x0+tx1. Conditioning on x0, we have the following conditional
marginal density pt|0=N/parenleftbig
(1−t)x0,t2I/parenrightbig
. Meanwhile, let’s derive the marginal density ˜pt|0induced by
Eq. (31). Assuming a linear drift f(xt,t) =µ(t)xt, we know that ˜pt|0is Gaussian. Let’s derive its mean
mt=E[xt]and covariance Σt= Cov[xt]. We have that (Särkkä & Solin, 2019)
d
dtmt=E[f(xt,t)] =µ(t)mt. (35)
thus
E[xt] = exp/parenleftbigg/integraldisplayt
0µ(s)ds/parenrightbigg
x0. (36)
Additionally,
d
dtΣt=E/bracketleftbig
f(xt,t)(mt−xt)⊤/bracketrightbig
+E/bracketleftbig
f(xt,t)⊤(mt−xt)/bracketrightbig
+g(t)2I (37)
= 2µ(t)Σt+g(t)2I, (38)
Matching ˜pt|0andpt|0, we get
exp/parenleftbigg/integraldisplayt
0µ(s)ds/parenrightbigg
x0= (1−t)x0 (39)
⇔/integraldisplayt
0µ(s)ds= ln(1−t) (40)
⇔µ(t) =−1
1−t(41)
and
2µ(t)t2+g(t)2= 2t (42)
⇔ − 21
1−tt2+g(t)2= 2t (43)
⇔g(t)2= 2t+ 21
1−tt2(44)
⇔g(t)2=2t
1−t. (45)
The equivalent SDE that gives the same marginals is Therefore, the following SDE gives the same conditional
marginal as flow matching:
dxt=−1
1−txtdt+/radicalbigg
2t
1−tdBt. (46)
SO(3)setting The conditional marginal density ˜pt|0induced by Eq. (31) with zero drift f(rt,t) = 0is
given by the IG SO(3)distribution (Yim et al., 2023b): ˜pt|0= IGSO(3) ( rt,r0,t). We are not aware of a
closed form formula for the variance of such a distribution.
On the flow matching side, we assume r0is data and r1is noise, with r1∼U(SO(3)), and a geodesic
interpolation rt= expr0(tlogr0(r1)). We posit that the induced conditional marginal pt|0isnotan IG SO(3)
distribution. As such, it appears non-trivial to derive the required equivalent diffusion coefficient g(t)for
SO(3). We therefore use as a heuristic the same g(t)as for Rd.
18Published in Transactions on Machine Learning Research (07/2024)
Figure 7: Schematic of computing motif-scaffolding designability. "Generative model" is a stand-in for the
method used to generate scaffolds conditioned on the motif. From there, we use ProteinMPNN (Dauparas
et al., 2022) to design the sequence then use AlphaFold2 (AF2) (Jumper et al., 2021) in predicting the
structure of the sequence. The RMSD is calculated on the scaffold (blue) and motif (red) separately with
alignments. A generated scaffold passes designability if the scaffold RMSD <2.0and motif RMSD <1.0.
E Designability
We provide details of the designability metric for motif-scaffolding and unconditional backbone generation
previously used in prior works (Watson et al., 2023; Wu et al., 2023). The quality of a backbone structure
is nuanced and difficult to find a single metric for. One approach that has been proven reliable in protein
design is using a highly accurate protein structure prediction network to recapitulate the structure after
the sequence is inferred. Prior works (Bennett et al., 2023; Wang et al., 2021) found the best method for
filtering backbones to use in wet-lab experiments was the combination of ProteinMPNN (Dauparas et al.,
2022) to generate the sequences and AlphaFold2 (AF2) (Jumper et al., 2021) to recapitulate the structure.
We choose to use the same procedure in determining the computational success of our backbone samples
which we describe next. As always, we caveat these results are computational and may not transfer to
wet-lab validation. While ESMFold (Jumper et al., 2021) may be used in place of AF2, we choose to follow
the setting of RFdiffusion as close as possible.
We refer to sampled backbones as backbones generated from our generative model. Following RFdiffusion,
we use ProteinMPNN at temperature 0.1 to generate 8 sequences for each backbone in motif-scaffolding and
unconditional backbone generation. In motif-scaffolding, the motif amino acids are kept fixed – Protein-
MPNN only generates amino acids for the scaffold. The predicted backbone of each sequence is obtained with
the fourth model in the five model ensemble used in AF2 with 0 recycling, no relaxation, and no multiple
sequence alignment (MSA) – as in the MSA is only populated with the query sequence. Fig. 7 provides
a schematic of how we compute designability for motif-scaffolding. A sampled backbone is successful or
referred to as designable based on the following criterion depending on the task:
•Unconditional backbone generation : successful if the Root Mean Squared Deviation (RMSD)
of all the backbone atoms is <2.0Åafter global alignment of the Carbon alpha positions.
•Motif-scaffolding : successful if the RMSD of motif atoms is <1Åafter alignment on the motif
Carbon alpha positions. Additionally, the RMSD of the scaffold atoms must be <2Åafter alignment
on the scaffold Carbon alpha positions.
19Published in Transactions on Machine Learning Research (07/2024)
F FrameFlow unconditional results
We present backbone generation results of the unconditional FrameFlow model used in FrameFlow-guidance.
We do not perform an in-depth analysis since this task is not the focus of our work. Characterizing the
backbone generation performance ensures we are using a reliable unconditional model for motif-scaffolding.
We evaluate the unconditionally trained FrameFlow model by sampling 100 samples from lengths 70, 100,
200, and 300 as done in RFdiffusion. The results are shown in Tab. 2. We find that FrameFlow achieves
slightly worse designability while achieving improved novelty. We conclude that FrameFlow is able to achieve
strongunconditionalbackbonegenerationresultsthatareonparwithacurrentstate-of-the-artunconditional
diffusion model RFdiffusion. We perform secondary structure analysis of the unconditional samples in Fig. 8.
Table 2: Unconditional generation metrics.
Method Des.( ↑) Div. (↑) Nov. (↓)
FrameFlow 0.86 155 0.61
RFdiffusion 0.89 159 0.65
We find FrameFlow has a tendency to sample more alpha helical structures than the data distribution but
still has roughly the same coverage of structures. Future work could investigate the cause of such helical
tendency and improve the secondary structure sample distribution.
Figure 8: Secondary structure analysis of unconditional samples. Using FrameFlow we sample 100 proteins
for each length 70, 100, 200, and 300. The 2D kernel density estimation plot of the secondary structure
composition is shown on left. On the right, we show the secondary structure composition of all length 70,
100, 200, and 300 proteins in the training set. We find FrameFlow has a tendency to sample more alpha
helical structures than the data distribution.
G FrameFlow motif-scaffolding analysis
In this section, we provide additional analysis into the motif-scaffolding results in Sec. 5.2. Our focus is
on analyzing the motif-scaffolding with FrameFlow: motif amortization and guidance. The first analysis is
the empirical cumulative distribution functions (ECDF) of the motif and scaffold RMSD shown in Fig. 9.
We find that the main advantage of amortization is in having a higher percent of samples passing the motif
RMSD threshold compared to the scaffold RMSD. Amortization has better scaffold RMSD but the gap is
smaller than motif RMSD. The ECDF curves are roughly the same for both methods.
Tab. 3 shows the average pairwise TM-score for all designable scaffolds per motif. We report this for each
method where we find our FrameFlow approaches get the lowest average pairwise TM-score in 19 out of 24
motifs. The average pairwise TM-score is meant to complement the cluster criterion for diversity in case
where clustering leads to pathological behaviors. Both metrics have their strengths and weaknesses but
together help provide more details on sample diversity.
20Published in Transactions on Machine Learning Research (07/2024)
Lastly, we visualize samples from FrameFlow-amortization on each motif in the benchmark. As noted in
Sec. 5.2, amortization is able to solve 21 out of 24 motifs in the benchmark. In Fig. 10, we visualize the
generated scaffolds that are closest to passing designability for the 3 motifs it is unable to solve. We find the
failure to be in the motif RMSD being over 1ÅRMSD. However, for 4JHW, 7MRX_60, and 7MRX_128 the
motif RMSDs are 1.2, 1.1, and 1.7 respectively. This shows amortization is very close to solve all motifs in
the benchmark. In Fig. 11, we show designable scaffolds for each of the 21 motifs that amortization solves.
We highlight the diverse range of motifs that can be solved as well as diverse scaffolds.
Figure 9: Empirical cumulative density plot of designability RMSD over the motif (left) and scaffold (right)
for FrameFlow-amortization (blue line) and FrameFlow-guidance (orange line).
Table 3: Under each method name are average TM-score over all designable scaffolds for each motif. Lower
is better which indicates more dissimilar pairwise scaffolds. N/A indicates no designable scaffolds were
sampled. We find our FrameFlow approaches have the lowest average TM-scores among all methods.
Motif FrameFlow-amortization FrameFlow-guidance TDS RFdiffusion
6E6R_med 0.3 0.31 0.36 0.39
2KL8 0.95 0.86 0.88 0.98
4ZYP 0.55 0.44 N/A N/A
5WN9 0.53 0.45 0.48 N/A
5TRV_short 0.48 0.42 0.46 0.66
7MRX_60 N/A N/A 0.29 0.59
6EXZ_short 0.48 0.49 0.47 0.35
1YCR 0.34 0.3 0.4 0.48
5IUS 0.6 N/A N/A 0.73
6E6R_short 0.37 0.35 0.39 0.41
3IXT 0.4 0.47 0.46 0.62
7MRX_85 N/A 0.36 N/A 0.56
1QJG 0.34 0.44 0.33 N/A
1BCF 0.76 0.69 0.5 0.83
5TRV_med 0.34 0.37 0.38 0.43
5YUI N/A N/A N/A N/A
5TPN 0.48 0.54 N/A 0.61
1PRW 0.75 0.66 N/A 0.76
6EXZ_med 0.37 0.38 0.36 0.49
5TRV_long 0.3 0.31 N/A 0.39
4JHW N/A N/A N/A N/A
7MRX_128 N/A N/A N/A 0.5
6E6R_long 0.28 0.28 0.46 0.35
6EXZ_long 0.3 0.3 0.38 0.38
21Published in Transactions on Machine Learning Research (07/2024)
Figure 10: Closest motif-scaffolds from FrameFlow-amortization on the three motifs it fails to solve. We
find the>1.0Åmotif RMSD is the reason for the method failing to pass designability.
Figure 11: Designable motif-scaffolds from FrameFlow-amortization on 20 out of 24 motifs in the motif-
scaffolding benchmark.
22