Under review as submission to TMLR
Transfer Learning in ℓ1Regularized Regression: Hyper-
parameter Selection Strategy based on Sharp Asymptotic
Analysis
Anonymous authors
Paper under double-blind review
Abstract
Transfer learning techniques aim to leverage information from multiple related datasets to
enhance prediction quality against a target dataset. Such methods have been adopted in the
context of high-dimensional sparse regression, and some Lasso-based algorithms have been
invented: Trans-Lasso and Pretraining Lasso are such examples. These algorithms require
the statistician to select hyperparameters that control the extent and type of information
transfer from related datasets. However, selection strategies for these hyperparameters,
as well as the impact of these choices on the algorithm’s performance, have been largely
unexplored. To address this, we conduct a thorough, precise study of the algorithm in a
high-dimensional setting via an asymptotic analysis using the replica method. Our approach
reveals a surprisingly simple behavior of the algorithm: Ignoring one of the two types of in-
formation transferred to the fine-tuning stage has little effect on generalization performance,
implying that efforts for hyperparameter selection can be significantly reduced. Our theoret-
ical findings are also empirically supported by applications on real-world and semi-artificial
datasets using the IMDb and MNIST datasets, respectively.
1 Introduction
The increasing availability of large-scale datasets has led to an expansion of methods such as pretraining and
transfer learning (Torrey & Shavlik, 2010; Weiss et al., 2016; Zhuang et al., 2021), which exploit similarities
between datasets obtained from different but related sources. By capturing general patterns and features,
which are assumed to be shared across data collected from multiple sources, one can improve the general-
ization performance of models trained for a specific task, even when the data for the task is too limited to
make accurate predictions. Transfer learning has demonstrated its efficacy across a wide range of applica-
tions, such as image recognition (Zhu et al., 2011), text categorization (Zhuang et al., 2011), bioinformatics
(Petegrosso et al., 2016), and wireless communications (Pan et al., 2011; Wang et al., 2021). It is reported
to be effective in high-dimensional statistics (Banerjee et al., 2020; Takada & Fujisawa, 2020; Bastani, 2021;
Li et al., 2021), with clinical analysis (Turki et al., 2017; Hajiramezanali et al., 2018; McGough et al., 2023;
Craig et al., 2024) being a major application area due to the scarcity of patient data and the need to comply
with privacy regulations. In such applications, one must handle high-dimensional data, where the number
of features greatly exceeds the number of samples. This has sparked interest in utilizing transfer learning to
sparse high-dimensional regression, which is the standard method for analyzing such data.
A classical method for statistical learning under high-dimensional settings is Lasso (Tibshirani, 1996), which
aims at simultaneously identifying the sparse set of features and their regression coefficients that are most
relevant for predicting the target variable. Its simplicity and interpretability of the results, as well as
its convex property as an optimization problem, has made Lasso a popular choice for sparse regression
tasks, leading to a flurry of research on its theoretical properties (Candes & Tao, 2006; Zhao & Yu, 2006;
Meinshausen & Bühlmann, 2006; Wainwright, 2009; Bellec et al., 2018).
ModificationstoLassohavebeenproposedtoincorporateandtakeadvantageofauxiliarysamples. Thedata-
shared Lasso (Gross & Tibshirani, 2016) and stratified Lasso (Ollier & Viallon, 2017) are both methods based
1Under review as submission to TMLR
on multi-target learning, where one solves multiple regression problems with related data in a single fitting
procedure. Two more recent approaches are Trans-Lasso (Bastani, 2021; Li et al., 2021) and Pretraining
Lasso (Craig et al., 2024), both of which require two-stage regression procedures. Here, the first stage is
focused on identifying common features shared across multiple related datasets, while the second stage is
dedicated to fine-tuning the model on a specific target dataset. To simplify the explanation, we introduce
a generalized algorithm that encompasses both methods, which is referred to as Generalized Trans-Lasso
hereafter. Given a set of datasets of size K,D={D(k)}K
k=1, each consisting of an observation vector of
dimensionMk,y(k)∈RMk, and its corresponding covariate matrix A(k)∈RMk×N, generalized Trans-Lasso
performs the following two steps:
1. Pretraining procedure: In the first stage, all datasets are used to identify a common feature vector
ˆx(1st)which captures shared patterns and relevant features across the datasets:
ˆx(1st)= arg min
x∈RNL(1st)(x;D),L(1st)(x;D) =1
2K/summationdisplay
k=1∥y(k)−A(k)x∥2
2+λ1∥x∥1.(1)
2. Fine-tuning: Here, the interest is only on a specific target dataset, which is designated by k= 1
without loss of generality. The second stage incorporates the learned feature vector’s configuration
and its support set into a modified sparse regression model. This is done by first offsetting the
observation vector y(1)by a common prediction vector deduced from the first stage, i.e. A(1)ˆx(1st).
Further, the penalty factor is also modified such that variables not selected in the first stage is
penalized more heavily than those selected. The above procedure is summarized by the following
optimization problem:
ˆx(2nd)= arg min
x∈RNL(2nd)(x;ˆx(1st),D(1)),
L(2nd)(x;ˆx(1st),D(1)) =1
2∥y(1)−κA(1)ˆx(1st)−A(1)x∥2
2+N/summationdisplay
i=1/parenleftig
λ2+ ∆λI[ˆx(1st)
i= 0]/parenrightig
|xi|,(2)
where Iis the indicator function. Here, κ,∆λ≥0are hyperparameters that determine the extent
of information transfer from the first stage.
It is crucial to note that the performance of this algorithm is highly dependent on the choice of hyperpa-
rameters, which control the strength of knowledge transfer. Suboptimal hyperparameter selection can lead
to insufficient knowledge transfer, or negative transfer (Pan et al., 2011), where the model’s performance
is degraded by the transfer of adversarial information. Thus, the choice of appropriate hyperparameters
should be addressed with equal importance as the algorithm itself. In fact, a distinctive difference between
Trans-Lasso and Pretraining Lasso is whether one should inherit support information from the first stage
or not. While this adds an additional layer of complexity to hyperparameter selection, it also provides an
opportunity to inherit significant information from the first stage, if the support of the feature vectors be-
tween the sources are similar. Empirical investigation of the qualitative effects of hyperparameters on the
algorithm’s performance can be computationally intensive, especially in high-dimensional settings.
To address the issue of hyperparameter selection without resorting to extensive empirical research, one can
turn to the field of statistical physics, which provides a set of tools to analyze the performance of high-
dimensional statistical models in a theoretical manner. While these theoretical studies are restricted to
inevitably simplified assumptions on the statistical properties of the dataset, they often provide valuable
qualitative insights generalizable to more realistic settings. The replica method (Mezard et al., 1986; Char-
bonneau et al., 2023), in particular, has been widely used to precisely analyze high-dimensional statistical
models including Lasso (Kabashima et al., 2009; Rangan et al., 2009; Obuchi & Kabashima, 2016; Takahashi
& Kabashima, 2018; Obuchi & Kabashima, 2019; Okajima et al., 2023), and has been shown to provide
accurate predictions in many cases.
In this work, we conduct a sharp asymptotic analysis of the generalized Trans–Lasso’s performance under a
high-dimensional setting using the replica method. Our study reveals a simple heuristic strategy to select
2Under review as submission to TMLR
hyperparameters. More specifically, it suffices to use either the support information or the actual value of
the feature vector ˆx(1st)obtained from the pretraining stage to achieve near-optimal performance after fine-
tuning. Our theoretical findings are complemented through empirical experiments on the real-world IMDb
dataset (Maas et al., 2011) and semi-artificial datasets derived from MNIST images (Deng, 2012).
1.1 Related Works
Trans-Lasso, first proposed by Bastani (2021) and further studied by Li et al. (2021), is a special case of the
two-stageregressionalgorithmgivenbyequations1and2. Inthisvariant, thesecondstageomitsinformation
transfer regarding the support of ˆx(1st)(i.e., ∆λ= 0) and setsκto unity. This method has demonstrated
efficacy in enhancing the generalization performance of Lasso regression, and has since been extended to
various regression models, such as generalized linear models (Tian & Feng, 2023; Li et al., 2024), Gaussian
graphical models (Li et al., 2023), and robust regression (Sun & Zhang, 2023). Moreover, the method also
embodies a framework to determine which datasets are adversarial in transfer learning, allowing one to avoid
negative transfer. While the problem of dataset selection is a practically important issue in transfer learning,
the present study will not consider this aspect, but rather focus on the algorithm’s performance given a fixed
set of datasets.
On the other hand, Pretraining Lasso controls both κand∆λusing a tunable interpolation parameter
s∈[0,1]as
κ= 1−s, ∆λ=1−s
sλ2. (3)
This formulation allows for a continuous transition between two extremes: Setting s= 1reduces the problem
to a standard single-dataset regression, while s= 0reduces to regression constrained on the support of ˆx(1st).
Note that there is no choice of swhich reduces Pretraining Lasso to Trans-Lasso.
Previous research has not yet established a clear understanding of the optimal hyperparameter selection
for these methods. Moreover, the introduction of additional hyperparameters, such as those proposed in
this study, has been largely unexplored in existing literature, possibly due to the computational demands
involved. A more thorough investigation into the performance of these methods is necessary to reevaluate
current, potentially suboptimal hyperparameter choices and to provide a more comprehensive understanding
of the algorithm’s behavior.
The analysis of our work is based on the replica method, which is a non-rigorous mathematical tool used
in a wide range of theoretical studies, such as those on spin-glass systems and high-dimensional statistical
models. The results from such analyses have been shown to be consistent with empirical results, and in
some cases later proved by mathematically rigorous methods, such as approximate message passing theory
(Bayati & Montanari, 2011; Javanmard & Montanari, 2013), adaptive interpolation (Barbier et al., 2019;
Barbier & Macris, 2019), Gordon comparison inequalities (Stojnic, 2013; Thrampoulidis et al., 2018; Miolane
& Montanari, 2021), and second-order Stein formulae (Bellec & Zhang, 2021; Bellec & Shen, 2022). The
application of the replica method to multi-stage procedures in machine learning has also been done in the
context of knowledge distillation (Saglietti & Zdeborová, 2022), self-training (Takahashi, 2024), and iterative
algorithms (Okajima & Takahashi, 2024). Our analysis can be seen as an extension of these works to this
generalized Trans-Lasso algorithm.
2 Problem Setup
As given in the Introduction, we consider a set of Kdatasets, each consisting of an observation vector
y(k)∈RMkand covariate matrices A(k)∈RMk×N. The observation vector y(k)is assumed to be generated
from a linear model with Gaussian noise, i.e.
y(k)=A(k)r(k)+e(k),e(k)∼N(0,(σ(k))2IMk). (4)
Here,e(k)∈RMkis a Gaussian-distributed noise term, while r(k)∈RNis the true feature vector of the
underlying linear model. We consider the common and individual support model (Craig et al., 2024), where
3Under review as submission to TMLR
{r(k)}K
k=1is assumed to have overlapping support. More concretely, let {I(k)}K
k=0be a set of disjoint indices,
whereI(k)⊂{1,2,...,N},|I(k)|=Nk,for allk= 0,···,K, and/summationtextK
k=0Nk≤N. Then,r(k)is given by
r(k)
I(0)=x(0)
⋆∈RN0,r(k)
I(k)=x(k)
⋆∈RNk,r(k)
\{I(k)∪I(0)}=0∈RN−Nk−N0, (5)
and
y(k)=A(k)
I(0)x(0)
⋆+A(k)
I(k)x(k)
⋆+e(k). (6)
Here,A(k)
Idenotes the submatrix of A(k)with columns indexed by I,r(k)
Idenotes the subvector of r(k)with
indices inI, and\Iis the shorthand of {1,2,...,N}\I. Therefore, each linear model is always affected by
the variables in the common support set I(0), with additional variables in the unique support set I(k)for
each dataset k= 1,···,K. We refer to x(0)
⋆as the common feature vector, while x(k)
⋆as the unique feature
vector for class k. The implicit aim of Trans-Lasso can be seen as estimating the common feature vector
x(0)in the first stage, treating the unique feature vectors in each class as noise. On the other hand, the
second stage is dedicated to estimating the unique feature vector x(1)
⋆given a noisy, biased estimator of the
common feature vector x(0)
⋆, i.e. ˆx(1st).
We evaluate the performance of the generalized Trans-Lasso using the generalization error in each stage,
which is defined as the expected mean square error of the model prediction made against a set of new
observations ˜D={(˜y(k),˜A(k))}K
k=1where ˜y(k)∈RMkand ˜A(k)∈RMK×N:
ϵ(1st)=1
NK/summationdisplay
k=1ED,˜D/bracketleftig
∥˜y(k)−˜A(k)ˆx(1st)∥2
2/bracketrightig
, (7)
for the first stage, while the generalization error for the second stage, with specific interest on class k= 1
without loss of generality, is given by
ϵ(2nd)=1
NED,˜D/bracketleftig
∥˜y(1)−˜A(1)(κˆx(1st)+ˆx(2nd))∥2
2/bracketrightig
. (8)
The objective of our analysis is to determine the behavior of the generalization error given the choice of
hyperparameters (λ1,λ2,κ,∆λ).
Note that the definition of ϵ(1st)in equation 7 is appropriate for the case where the new data is redrawn,
including the covariate matrix, from the common and independent support model with the same hyperpa-
rameters as the training data. In other scenarios, different definitions might be more suitable. For instance,
if we consider the case where only one datapoint is newly observed equally for each class k∈{1,2,...,K},
the generalization error should be defined to have equal weight over all the classes, which is different from
ours. In the analytical framework discussed below, that case can also be treated: The resultant formula
of the generalization error would become equation 21 with all α(k)fixed equal. This would give quantita-
tively different results from those based on equation 7, yielding another interesting situation. However, the
investigation of that situation requires another detailed computation which we leave as future work.
High-dimensional setup We focus on the asymptotic behavior in a high-dimensional limit, where the
length of the common and unique feature vectors, as well as the size of the datasets for each class, tend to
infinity at the same rate, i.e.
N0
N→π(0)=O(1),Nk
N→π(k)=O(1),Mk
N→α(k)=O(1), k = 1,···,K, asN→∞.(9)
Also, define the proportion of the number of variables not included in any of the support sets as π(neg.)=
1−/summationtextK
k=0π(k).The covariate matrices are all assumed to have i.i.d. entries with zero mean and variance
1/N, while the true feature vectors are also assumed to have i.i.d. standard Gaussian entries. While such
a setup is a drastic simplification of real-world scenarios, it should be noted that these conditions can be
further generalized to more complex settings. For instance, the analysis can be further extended to the case
where the random matrix A(k)inherits rotationally invariant structure (Vehkaperä et al., 2016; Takahashi
& Kabashima, 2018), or to the case where the observations are corrupted by heavy-tailed noise (Adomaityte
et al., 2023).
The notations used in this paper are summarized in table 1.
4Under review as submission to TMLR
Notation Description
I(0)⊂{1,2,...,N}Support of features common across all classes
I(k)⊂{1,2,...,N}Support of features unique to class k= 1,...,K
N Total number of features
N0 Size of common support, |I(0)|
Nk Size of independent support of class k= 1,...,K,|I(k)|
x(0)
⋆∈RN0 Common feature vector shared among all classes
x(k)
⋆∈RNk Unique feature vector exclusive to class k= 1,...,K
y(k)∈RMk Observation vector for dataset k= 1,...,K
e(k)∈RMk Gaussian noise term with i.i.d. elements of variance (σ(k))2
A(k)∈RMk×NCovariate matrix for dataset k= 1,...,K
D={D(k)}K
k=1 Set ofKdatasets.D(k)= (y(k),A(k))
˜D={˜D(k)}K
k=1 Set ofKtest datasets with same distribution and size as D
ˆx(1st)First stage feature vector (pretraining)
ˆx(2nd)Second stage feature vector (fine-tuning)
λ1 First stage regularization parameter
λ2 Second stage base regularization parameter
κ Transfer coefficient to second stage
∆λ Additional regularization for features non-selected during pretraining
π(0)Proportion of common features N0/N
π(k)Proportion of unique features Nk/N
π(neg.)Proportion of features absent from all linear models, 1−/summationtextK
k=0π(k)
α(k)Sample size ratio Mk/N
Table 1: Notations used in this paper.
3 Results of Sharp Asymptotic Analysis via the replica method
Here, we state the result of our replica analysis. The series of calculations to obtain our results is similar to
the well-established procedure used in the analyses for multi-staged procedures (Saglietti & Zdeborová, 2022;
Takahashi, 2024; Okajima & Takahashi, 2024). While a brief overview of the derivation is given in the next
subsection, we refer the reader to Appendix A for a detailed calculation. Although our theoretical analysis
lacks a standing proof from the non-rigorousness of the replica method, we conjecture that the results are
exact in the limit of large N. This is supported numerically via experiments on finite-size systems, which
will be shown to be consistent with the theoretical predictions.
In fact, since the first stage of the algorithm is a standard Lasso optimization problem under Gaussian
design, it is already known that at least up to the first stage of the algorithm, the replica method yields
the same formula derivable from Approximate Message Passing (Bayati & Montanari, 2011) or Convex
Gaussian Minmax Theorem (CGMT) (Thrampoulidis et al., 2018; Miolane & Montanari, 2021). Moreover,
CGMT can be further applied to multi-stage optimization problems with independent data in each stage
(Chandrasekheret al., 2023). The extension of thisproof tothe secondstage of ouralgorithm, whichcontains
data dependence among the two stages, is an unsolved problem that we leave for future work.
3.1 Overview of the calculation
Here, we briefly outline the derivation of the generalization errors ϵ(1st)andϵ(2nd)using the replica method.
Readers not interested in the derivation may skip this subsection and proceed to the next subsection for the
main results.
5Under review as submission to TMLR
Given the training dataset D={D(k)}K
k=1,withD(k)= (A(k),y(k)), k= 1,···,K, define the following joint
probability measure for x1∈RNandx2∈RNas
Pβ1,β2(x1,x2;D) =Z−1wβ1,β2(x1,x2;D),
wβ1,β2(x1,x2;D) = exp/bracketleftig
−β1L(1st)(x1;D)−β2L(2nd)(x2;x1,D(1))/bracketrightig
,(10)
whereZ=/integraltext
dx1dx2wβ1,β2(x1,x2;D)is the normalization constant. Under this measure, the generalization
errorϵ(1st)andϵ(2nd)can be expressed as
ϵ(1st)=ED,˜D/bracketleftigg
Z−1/integraldisplay
dx1dx2wβ1,β2(x1,x2;D)K/summationdisplay
k=1∥˜y(k)−˜A(k)x1∥2
2/bracketrightigg
= lim
β2→∞lim
β1→∞lim
t1→01
NED,˜D/bracketleftigg
∂
∂t1log/integraldisplay
dx1dx2wβ1,β2(x1,x2;D)et1/summationtextK
k=1∥˜y(k)−˜A(k)x1∥2
2/bracketrightigg
,(11)
ϵ(2nd)=ED,˜D/bracketleftigg
Z−1/integraldisplay
dx1dx2wβ1,β2(x1,x2;D)∥˜y(1)−˜A(1)(κx1+x2)∥2
2/bracketrightigg
= lim
β2→∞lim
β1→∞lim
t2→01
NED,˜D/bracketleftigg
∂
∂t2log/integraldisplay
dx1dx2wβ1,β2(x1,x2;D)et2∥˜y(1)−˜A(1)(κx1+x2)∥2
2/bracketrightigg
.(12)
The first lines of equations 11 and 12 stem from the fact that, in the successive limit of β1→∞followed
byβ2→∞, the joint probability measure Pβ1,β2(x1,x2;D)concentrates around (ˆx(1st),ˆx(2nd)), the solution
to the two-stage Trans-Lasso procedure. The second lines of equations 11 and 12 can be derived by simply
differentiating the logarithm, and exchanging the data average and the limit of t1→0ort2→0. These
expressions do not require the computation of the inverse normalization constant Z−1, which is difficult
to perform in general. Still, it requires one to evaluate the average of the logarithm of the integral of
wβ1,β2(x1,x2;D)ef(x1,x2;˜D)for appropriate choices of f. Techniques from statistical mechanics, however,
allow one to perform heuristic calculations to evaluate such expressions, by alternatively expressing the
logarithm of the integral as:
ED,˜Dlog/integraldisplay
dx1dx2wβ1,β2(x1,x2;D)ef(x1,x2;˜D)
= lim
n→+0∂
∂nlog/parenleftigg
ED,˜D/bracketleftig/integraldisplay
dx1dx2wβ1,β2(x1,x2;D)ef(x1,x2;˜D)/bracketrightign/parenrightigg
,(13)
where the second line can be confirmed by a Taylor expansion in n. Although the second line of the above
expression is not directly computable for arbitrary n∈R≥0, it turns out that under plausible assumptions,
this can be evaluated for n∈Nin the limit N→∞. The replica method Mezard et al. (1986); Charbonneau
et al. (2023) is based on the assumption that one can analytically continue this formula, defined only for
n∈N, ton∈R≥0. On this continuation, the limit n→+0can be taken formally.
In the process of calculating equation 13, a crucial observation is that from Gaussianity of the design matrices
A(k), theproductsbetweenthedesignmatricesandthefeaturevectors x1,x2areGaussianrandomvariables.
This not only simplifies the high-dimensional nature of the expectation over D, but it also reduces the
complexity of the analysis to consider only up to the second moments of these Gaussian random variables.
These moments are characterized by the inner product of the feature vectors, which happen to concentrate
to deterministic values in the limit N→∞. One is then left with an integral over this finite number of
concentrating scalar variables and other auxiliary variables introduced in this process, which can then be
evaluated using the standard saddle-point method, again in the limit N→∞. This finite set of variables,
as well as the saddle-point conditions imposed on them, are given in Definitions 1 and 2 in the following
section.
3.2 Equations of State and Generalization Error
The precise asymptotic behavior of the generalization error is given by the solution of a set of non-linear
equations, which we refer to as the equations of state. They are provided as follows.
6Under review as submission to TMLR
Definition 1 (Equations of state for the First Stage) .Define the set of finite scalar variables Θ1=
{{m(k)
1,ˆm(k)
1}K
k=0,q1,ˆq1,χ1,ˆχ1}as the solution to the following set of equations, which we refer to as the
equations of state for the first stage:
q1=K/summationdisplay
k=1π(k)E1/bracketleftig/parenleftbig
x(k)
1/parenrightbig2/bracketrightig
+π(neg.)E1/bracketleftig/parenleftbig
x(neg.)
1/parenrightbig2/bracketrightig
, ˆq1= ˆm(0)
1=α(tot)
1 +χ1,
m(k)
1=π(k)E1/bracketleftig
x(k)
1x(k)
⋆/bracketrightig
, ˆm(k)
1=α(k)
1 +χ1,
ˆχ1=K/summationdisplay
k=1α(k)q1−2(m(0)
1+m(k)
1) +ρ(k)
(1 +χ1)2,
χ1=K/summationdisplay
k=1π(k)E1/bracketleftigg
∂
∂z1x(k)
1/bracketrightigg
+π(neg.)E1/bracketleftigg
∂
∂z1x(neg.)
1/bracketrightigg
,(14)
where we have defined ρ(k)=π(k)+π(0)+ (σ(k))2, andα(tot)=/summationtextK
k=1α(k). Here, the random vari-
ables{x(k)
1}K
k=0,x(neg.)
1are the solution to the following random optimization problems dependent on
{{ˆm(k)
1}K
k=0,ˆq1,ˆχ1}:
x(k)
1= arg min
xE(k)
1(x), E(k)
1(x) =ˆq1
2x2−/parenleftig/radicalbig
ˆχ1z1+ ˆm(k)
1x(k)
⋆/parenrightig
x+λ1|x|(15)
x(neg.)
1 = arg min
xE(neg.)
1(x), E(neg.)
1(x) =ˆq1
2x2−/radicalbig
ˆχ1z1x+λ1|x|, (16)
where z1,{x(k)
⋆}K
k=0are i.i.d. standard Gaussian random variables. Finally, the average E1denotes the joint
expectation with respect to these random variables.
Definition 2 (Equations of state of the Second Stage) .LetΘ1be as defined in 1, i.e. the solution to the
equations of state 14. Define the set of finite variables Θ2={{m(k)
2,ˆm(k)
2}K
k=0,q2,ˆq2,qr,ˆqr,χ2,ˆχ2,χr,ˆχr}by
the solution to the following set of equations, which we refer to as the equations of state for the first stage:
q2=K/summationdisplay
k=1π(k)E2/bracketleftig/parenleftbig
x(k)
2/parenrightbig2/bracketrightig
+π(neg.)E2/bracketleftig/parenleftbig
x(neg.)
2/parenrightbig2/bracketrightig
, ˆq2=α(1)
1 +χ2,
qr=K/summationdisplay
k=1π(k)E2/bracketleftig
x(k)
2x(k)
1/bracketrightig
+π(neg.)E2/bracketleftig
x(neg.)
2 x(neg.)
1/bracketrightig
, ˆqr=−α(1)A
1 +χ2,
m(k)
2=π(k)E2/bracketleftig
x(k)
2x(k)
⋆/bracketrightig
, ˆm(k)
2=α(1)B
1 +χ2,
χ2=K/summationdisplay
k=1π(k)E2/bracketleftig∂
∂z2x(k)
2/bracketrightig
+π(neg.)E2/bracketleftig∂
∂z2x(neg.)
2/bracketrightig
,
ˆχ2=α(1)q2+A2q1+ 2Aqr+B2ρ(1)−2B(m(0)
2+m(1)
2)−2AB(m(0)
1+m(1)
1)
(1 +χ2)2,
χr=K/summationdisplay
k=1π(k)E2/bracketleftigg
∂
∂z1x(k)
2/bracketrightigg
+π(neg.)E2/bracketleftigg
∂
∂z1x(neg.)
2/bracketrightigg
,
ˆχr=−α(1)Bρ(1)−m(0)
2−m(1)
2−A(m(0)
1+m(1)
1)
(1 +χ1)(1 +χ2),(17)
7Under review as submission to TMLR
whereA=κ−χ1
1+χ1,B= 1−χr+κχ1
1+χ1. Here, the random variables {x(k)
2}K
k=0,x(neg.)
2are the solution to the fol-
lowing random optimization problems dependent on the scalar variables {{ˆm(k)
1,ˆm(k)
2}K
k=0,ˆq1,ˆχ1,ˆq2,ˆχ2,ˆqr}:
x(k)
2= arg min
xE(k)
2(x|x(k)
1), E(k)
2(x|x(k)
1) =ˆq2
2x2−/parenleftig/radicalbig
ˆχ2z2+ ˆm(k)
2x(k)
⋆+ ˆqrx(k)
1/parenrightig
x+r(x|x(k)
1),(18)
x(neg.)
2 = arg min
xE(neg.)
2(x|x(neg.)
1),E(neg.)
2(x|x(neg.)
1) =ˆq2
2x2−/parenleftig/radicalbig
ˆχ2z2+ ˆqrx(neg.)
1/parenrightig
x+r(x|x(neg.)
1),(19)
wherer(x|y) = (λ2+ ∆λI[y= 0])|x|. The random variables z1,{x(k)
⋆}K
k=0and{x(k)
1}K
k=0are as predefined in
Definition 1, and z2is a Gaussian random variable with conditional distribution
z2|z1∼N/parenleftigg
ˆχr√ˆχ1ˆχ2z1,1−ˆχ2
r
ˆχ1ˆχ2/parenrightigg
. (20)
Finally, the joint expectation with respect to these random variables is denoted as E2.
Given these equations of state, the generalization error is given from the following claims:
Claim 1 (Generalization error of the first stage) .Given Θ1, the expected generalization error ϵ(1st)converges
in the large Nlimit as
lim
N→∞ϵ(1st)=K/summationdisplay
k=1α(k)/bracketleftig
q1−2(m(0)
1+m(k)
1) +ρ(k)/bracketrightig
. (21)
Claim 2 (Generalization error of the second stage) .Given Θ1andΘ2, the expected generalization error
ϵ(2nd)converges in the large Nlimit as
lim
N→∞ϵ(2nd)=α(1)/bracketleftig
q2+κ2q1+ 2κqr+ρ(1)−2(m(0)
2+m(1)
2)−2κ(m(0)
1+m(0)
1)/bracketrightig
. (22)
Claims 1 and 2 indicate that for a given set of hyperparameters, the asymptotic generalization error can be
computed by solving a finite set of non-linear equations in Definitions 1 and 2. Therefore, one can obtain
the optimal choice of hyperparameters minimizing the generalization error via a numerical root-finding and
optimization procedure in finite dimension.
3.3 Interpretation of Θ1andΘ2
By choosing an appropriate function fin the expression equation 13, we can easily show that the following
claims hold.
Claim 3 (Expected inner product between the regressors) .Recall that ˆx(1st)
I(k)and ˆx(2nd)
I(k)are subvectors of
ˆx(1st)andˆx(2nd)with indices corresponding to the unique feature vector for class k,x(k)
⋆. Given Θ1,Θ2, the
expected inner product between ˆx(1st)
I(k),ˆx(2nd)
I(k), andx(k)
⋆are given by
lim
N→∞1
NED,˜D/bracketleftig
ˆx(1st)
I(k)·x(k)
⋆/bracketrightig
=m(k)
1, lim
N→∞1
NED,˜D/bracketleftig
ˆx(2nd)
I(k)·x(k)
⋆/bracketrightig
=m(k)
2. (23)
Furthermore, we also have that
lim
N→∞1
NED,˜D/bracketleftbig
∥ˆx(1st)∥2
2/bracketrightbig
=q1,lim
N→∞1
NED,˜D/bracketleftbig
∥ˆx(2nd)∥2
2/bracketrightbig
=q2,lim
N→∞1
NED,˜D/bracketleftbigˆx(1st)·ˆx(2nd)/bracketrightbig
=qr,(24)
that is,q1,q2denote the norm of the first and second stage regressors respectively, while qrdenotes the inner
product between the first and second stage regressors.
8Under review as submission to TMLR
Claim 3 not only clarifies the interpretation of the some of the parameters in Θ1and Θ2, but it also
provides an alternative derivation of the expressions in Claims 1 and 2. For instance, ϵ(1st)can be expressed
alternatively as
ϵ(1st)= lim
N→∞1
NK/summationdisplay
k=1ED,˜D/bracketleftigg/vextenddouble/vextenddouble/vextenddouble˜A(k)
I(0)/parenleftbig
x(0)
⋆−ˆx(1st)
I(0)/parenrightbig
+˜A(k)
I(k)/parenleftbig
x(0)
⋆−ˆx(1st)
I(k)/parenrightbig
−/summationdisplay
k′̸=0,k˜A(k)
I(k′)ˆx(1st)
I(k′)+e(k)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
2/bracketrightigg
= lim
N→∞K/summationdisplay
k=1Mk
NED/bracketleftigg
∥x(k)
⋆−ˆx(1st)
I(0)∥2
2+∥x(0)
⋆−ˆx(1st)
I(k)∥2
2+/summationdisplay
k′̸=0,k∥ˆx(1st)
I(k′)∥2
2+ (σ(k))2/bracketrightigg
, (25)
which results in the expression in Claim 1 by using equations 23 and 24. Similar calculations can be made
for the second stage generalization error ϵ(2nd), in Claim 2.
4 Comparison with synthetic finite-size data simulations
104
103
102
101
100
0.0240.0250.0260.0270.0280.0290.0300.031Generalization Error
((1),(2))=(0.2,0.8)
=0.0
=0.2
=0.5
=1.0
104
103
102
101
100
0.0240.0260.0280.0300.032
((1),(2))=(0.4,0.8)
=0.0
=0.2
=0.5
=1.0
104
103
102
101
100
0.0180.0200.0220.0240.0260.0280.030
((1),(2))=(0.6,0.8)
=0.0
=0.2
=0.5
=1.0
(0)=0.10,(1)=(2)=0.09
104
103
102
101
100
0.0120.0140.0160.0180.0200.0220.0240.0260.028Generalization Error
((1),(2))=(0.2,0.8)
=0.0
=0.2
=0.5
=1.0
104
103
102
101
100
0.0160.0170.0180.0190.0200.0210.022
((1),(2))=(0.4,0.8)
=0.0
=0.2
=0.5
=1.0
104
103
102
101
100
0.01550.01600.01650.01700.01750.01800.01850.01900.0195
((1),(2))=(0.6,0.8)
=0.0
=0.2
=0.5
=1.0
(0)=0.15,(1)=(2)=0.04
Figure 1: Comparison of generalization error obtained from Claim 2 (solid line) with finite size simulations
(markers). Error bars represent standard error obtained from 64realizations of data.
To verify our theoretical results, we conduct a series of synthetic numerical experiments to compare the
results obtained from Claim 2 and the empirical results for the generalization error obtained from datasets
of finite-size systems.
The synthetic data is specified as follows. We consider the case of K= 2classes, with α(1)= 0.2,0.4,or
0.6, andα(2)= 0.8. Two types of settings for the common and unique feature vectors are considered; one
withπ(0)= 0.10,π(1)=π(2)= 0.09, andπ(0)= 0.15,π(1)=π(2)= 0.04. These two settings are chosen
to represent the cases where the common feature vector is relatively small compared to the unique feature
vectors, and the case where the common feature vector is relatively large, but with the total number of
9Under review as submission to TMLR
non-zero features underlying each class being the same (0.19N). Finally, the noise level of each dataset
is set toσ(1)=σ(2)= 0.1. The numerical simulations are performed on 64random realizations of data
with sizeN= 16,000. The second stage generalization error is evaluated by further generating 256random
realizations of (˜A(1),˜y(1))for each dataset and calculating the average empirical generalization error over
these realizations. The hyperparameters λ1andλ2are chosen such that they minimize the generalization
error given in Claims 1 and 2 for each stage respectively. The finite size simulations are also performed using
the same values of λ1andλ2used in the theory. In figure 1, we plot the generalization error of the second
stage as a function of ∆λforκ= 0.0,0.2,0.5,and1.0. As evident from the plot, the generalization error is
in fairly good agreement with the theoretical predictions.
Case with scarce target data Let us look closely at the left column of figure 1 where the target sample
size is relatively scarce (α(1)= 0.2). For both settings of (π(0),π(1),π(2)), the generalization error is insen-
sitive to the choice of ∆λwhenκis tuned properly. Another important observation is that Trans-Lasso’s
hyperparameter choice (κ= 1,∆λ= 0)seems comparable with the optimal choice regarding the generaliza-
tion error. The same trend is also seen in the case where α(1)= 0.4and(π(0),π(1),π(2)) = (0.15,0.04,0.04)
(bottom center subfigure of figure 1).
Case with abundant target data Next, we turn our attention to the right column of figure 1 where
the target sample size is relatively abundant (α(1)= 0.6). In this case, the generalization error is sensitive
to the choice of ∆λbut is insensitive to the choice of κaround its optimal value (κ= 0.0), for both settings
of(π(0),π(1),π(2)). This indicates that under abundant target data, transferring the knowledge with respect
to the support of the pretrained feature vector ˆx(1st)is more crucial than the actual value of ˆx(1st)itself.
The same trend can be seen for the case when α(1)= 0.4and(π(0),π(1),π(2)) = (0.15,0.04,0.04)(top center
subfigure of figure 1).
5 Hyperparameter Selection Strategy
The experiments from the last section indicate not only the accuracy of our theoretical predictions but also
the practical implications for hyperparameter selection. The observations made from the two cases suggest
that there are roughly two behaviors for the optimal choice of hyperparameters: One where the choice of
κis more crucial, and the other where the choice of ∆λis more important. This observation suggests a
simple strategy for hyperparameter selection, where one fixes ∆λ= 0for the former case and κ= 0for the
latter case. To further investigate the validity of this strategy, we conduct a comparative analysis of the
generalization error among the following four strategies:
•κ= 0strategy Here, we fix κto zero, while letting ∆λto be a tunable parameter. This reflects on
theinsightobtainedinsubsection4fortheabundanttargetdatacase. Theregularizationparameters
λ1andλ2are both chosen such that they minimize the generalization error in the first and second
stages, respectively.
•∆λ= 0strategy Here, we fix ∆λto zero, while letting κto be a tunable parameter. This reflects
the insight obtained in subsection 4 for the scarce target data case. The regularization parameters
λ1andλ2are both chosen in the same way as the κ= 0strategy.
•Locally Optimal (LO) strategy Here, bothκand∆λare tunable parameters. The regularization
parameters λ1andλ2are both chosen in the same way as the κ= 0strategy.
•Globally Optimal (GO) strategy Here, all four parameters (λ1,λ2,κ,∆λ)are tuned such that
the second stage generalization error is minimized.
The last strategy requires retraining the first stage estimator ˆx(1st)for each choice of (λ1,λ2,κ,∆λ), which
is computationally demanding. Even the LO strategy can be problematic when the dataset size is large.
Basically, these two strategies are regarded as a benchmark for the other two strategies: κ= 0and∆λ= 0
strategies. Below,wecomparethesefourstrategiesintermsofthesecond-stagegeneralizationerrorcomputed
by our theoretical analysis. Specifically, the case (π(0),π(1),π(2)) = (0.10,0.09,0.09)is investigated. In figure
10Under review as submission to TMLR
0.00.20.40.60.8
1=0.2,2=0.8
0.00.51.01=0.3,2=0.8
0.00.51.01=0.4,2=0.8
103
102
101
103
102
101
104
103
102
101
0.00.51.01.52.01
0.00.51.01.52.0
0.00.51.01.52.0
102
101
100
101
Generalization Err.=0 Strategy
=0 Strategy
LO Strategy
GO Strategy
102
101
100
101
=0 Strategy
=0 Strategy
LO Strategy
GO Strategy
102
101
100
102
101
=0 Strategy
=0 Strategy
LO Strategy
GO Strategy
Figure2: Generalizationerrorandoptimalhyperparametersforeachstrategyasafunctionof σfor(α1,α2) =
(0.2,0.8),(0.3,0.8),(0.4,0.8). The region shaded in red indicates the range of σwhere the ∆λ= 0strategy
outperforms the κ= 0strategy, while the region shaded in green indicates the opposite.
2, we plot the second stage generalization error, as well as the optimal choice of hyperparameters for each
strategy as a function of σ, for (α1,α2) = (0.2,0.8),(0.3,0.8),and(0.4,0.8). Note that the shaded green
region indicates the range of σwhere theκ= 0strategy outperforms the ∆λ= 0strategy, while the shaded
red region indicates the opposite.
Let us first examine the behavior of optimal ∆λacross different strategies. With the exception of the case
(α1,α2) = (0.2,0.8), the optimal ∆λvalues for each strategy (excluding the ∆λ= 0strategy) are approx-
imately equal in the green region. Notably, for (α(1),α(2)) = (0.4,0.8), theκ= 0strategy demonstrates
generalization performance comparable to the LO strategy in this region, while the ∆λ= 0strategy exhibits
significantly lower performance. Above a certain noise threshold, the optimal value of ∆λfor both the LO
and GO strategies becomes strictly zero. At this point, the LO strategy and the ∆λ= 0strategy coincide,
indicating that the ∆λ= 0strategy can potentially be equivalent to the LO strategy.
Next, let us see the behavior of the optimal κin each strategy. In the green region, where the κ= 0
strategy outperforms the ∆λ= 0strategy, both the LO and GO strategies actually exhibit finite values of
κ. This means that the κ= 0strategy is suboptimal. However, it still exhibits a comparable performance
with the LO strategy and gives a significant improvement from the ∆λ= 0strategy. This underscores the
importance of solely transferring support information; transferring the first stage feature vector itself can be
less beneficial. This may be due to the crosstalk noise induced by different datasets: Using the regression
results on other datasets than the target can induce noises on the feature vector components specific to the
target. Consequently, it may be preferable to transfer more robust information, in this case the support
configuration, rather than the noisy vector itself obtained in the first stage.
11Under review as submission to TMLR
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.01.11.21.31.41.51.61.71.8
1.01.21.41.61.82.0
1.001.011.021.031.041.051.06=0.01, ((0),(1),(2))=(0.1,0.09,0.09)
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.001.011.021.031.041.05
1.001.051.101.151.201.251.301.35
1.001.011.021.031.04=0.1, ((0),(1),(2))=(0.1,0.09,0.09)
Figure 3: Ratios min{ϵ∆λ=0,ϵκ=0}/ϵLO,ϵPretrain/ϵLOandϵTrans/ϵLOunder setting (π(0),π(1),π(2)) =
(0.1,0.09,0.09)and noise level σ= 0.01(top figure) and 0.1(bottom figure) for various values of (α(1),α(2)).
Note that each heatmap has its own color scale bar; see Appendix B for a direct comparison with shared
color scale bars between the κ= 0or∆λ= 0strategy with Pretraining-Lasso and Trans-Lasso.
The above observations indicate that the performance closely comparable to the LO strategy can be achieved
across a wide range of noise levels by simply considering either the κ= 0or∆λ= 0strategy; the κ= 0
strategy is preferred in low noise scenarios while the ∆λ= 0strategy is better in high noise scenarios. For
practical purposes, these two strategies should be examined and the best hyperparameters should be chosen
based on comparison. This is the main message for practitioners in this paper.
Implications on the original Trans-Lasso The ∆λ= 0strategy provides a lower bound for the gen-
eralization error of the Trans-Lasso, as the latter constrains κto unity instead of treating it as a tunable
parameter. Our results demonstrate that the Trans-Lasso is suboptimal compared to strategies that in-
corporate support information in the second stage, particularly in the green region of figure 2. Notably,
for(α1,α2) = (0.4,0.8)andσ= 0.01, the ∆λ= 0strategy reduces the generalization error by over 30%
compared to the Trans-Lasso.
Ubiquityofthesimplifiedhyperparameterselection Tofurtherconfirmtheefficacyofthetwosimple
strategies, we investigate the difference in generalization error between the LO strategy and κ= 0or∆λ= 0
strategy. Morespecifically, let ϵLO,ϵκ=0andϵ∆λ=0bethegeneralizationerrorofthecorrespondingstrategies.
Figure 3 shows the minimum ratio min{ϵ∆λ=0,ϵκ=0}/ϵLOfor various values of (α1,α2). For comparison, the
same values are calculated for the Pretraining Lasso and Trans-Lasso, i.e. ϵPretrain/ϵLOandϵTrans/ϵLO, with
ϵPretrainandϵTransbeing the generalization error with the parameters chosen optimally (note that λ1andλ2
arechoseninthesamewayastheLOstrategy). Theresultsindicatethat, acrossabroadrangeofparameters,
at least one of the simplified strategies (either ∆λ= 0orκ= 0) achieves a generalization error within 10% of
that obtained by the more complex LO strategy. Notably, substantial relative differences are observed only
within a narrow region of the parameter space (α(1),α(2)), highlighting the effectiveness of the simplified
approach across a wide range of problem settings. We also report that for the case σ= 0.5, the ∆λ= 0
12Under review as submission to TMLR
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5
0.00.10.20.30.40.50.60.70.80.91.0/2
Drama
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5
0.00.10.20.30.40.50.60.70.80.91.0/2
Comedy
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5
0.00.10.20.30.40.50.60.70.80.91.0/2
Action
5.605.655.705.755.805.855.90
T est Error
5.805.855.905.956.006.05
T est Error
5.15.25.35.45.55.65.7
T est Error
Figure 4: Second stage test error for each IMDB genre plotted against varying values of hyperparameters κ
and∆λ/λ2. The plots demonstrate that the effect of ∆λon generalization performance is tiny across genres.
Method Drama Comedy Action
Gen. Trans-Lasso, ∆λ= 0 5.58±0.07 5.78±0.10 5.07±0.11
Gen. Trans-Lasso, κ= 0 5.67±0.08 5.90±0.10 5.48±0.13
Gen. Trans-Lasso, LO 5.58±0.07 5.78±0.10 5.19±0.12
Pretraining Lasso 5.63±0.07 5.78±0.10 5.19±0.12
Trans-Lasso 5.71±0.08 5.78±0.10 5.17±0.12
Table 2: Comparison of test errors for various Lasso-based methods and hyperparameter strategies applied
to the IMDb dataset. Error estimation on the test error is done by performing jackknife resampling on the
test dataset.
strategy always coincided with the LO strategy in the region (α(1),α(2))∈[0.05,0.45]×[0.1,0.9]. Results
consistent with the above observations were also seen for other settings of (π(0),π(1),π(2)); see Appendix B.
Theroleof λ1Forlownoisescenarios, theGOstrategytendstopreferlarger λ1inthefirststagecompared
to the other strategies. This observation may be beneficial when the noise level of each dataset is known a
priori: We may tune the λ1value to a larger value than the one obtained from the LO or its proxy strategies
(κ= 0or∆λ= 0strategies). This offers an opportunity to enhance our prediction beyond the LO strategy
while avoiding the complicated retraining process necessary for the GO strategy.
6 Real Data Application
6.1 Application to the IMDb dataset
To verify whether the above insights can be generalized to real-world scenarios, we here conduct experiments
on the IMDb dataset. The IMDb dataset comprises a pair of 25,000 user-generated movie reviews from the
Internet Movie Database, each being devoted to training and test datasets. The dataset is evenly split into
highly positive or negative reviews, with negative labels assigned to ratings given 4 or lower out of 10, and
positive labels to ratings of 7 or higher. Movies labeled with only one of the three most common genres
in the dataset, drama, comedy, and horror, are considered. By representing the reviews as binary feature
vectors using bag-of-words while ignoring words with less than 5 occurrences, we are left with 27743 features
in total, with 8286 drama, 5027 comedy, and 3073 horror movies in the training dataset, and 9937 drama,
4774 comedy, and 3398 horror movies in the test dataset. The datasets can be acquired from Supplementary
Materials of Gross & Tibshirani (2016).
For sake of completeness, we calculate the test error for (κ,∆λ/λ2)∈ { 0.0,0.1,0.2,..., 1.5} ×
{0.0,0.1,0.2,..., 1.0}to see if our insights are qualitatively consistent with experiments on real data. For
each pair of (κ,∆λ/λ2),λ1andλ2are determined by a 10-fold cross-validation procedure, where the train-
ing dataset is further split into a learning dataset and a validation dataset. Since λ1(and consequently
13Under review as submission to TMLR
ˆx(1st)) must be selected oblivious to the validation dataset, we perform Approximate Leave-One-Out Cross-
Validation (Obuchi & Kabashima, 2016; Rad & Maleki, 2020; Stephenson & Broderick, 2020) on the learning
dataset, which allows one to estimate the leave-one-out cross-validation error deterministically without ac-
tually performing the computationally expensive procedure. For the sake of comparison, we also compare
with the performance of Pretraining Lasso and Trans-Lasso, with λ1andλ2chosen using the same method
as above.
In Figure 4, we present the second stage test error for each genre across various values of (κ,∆λ/λ2). The
plot reveals that ∆λis essentially a redundant hyperparameter having almost no influence on generalization
performance. This observation strongly supports the efficacy of the ∆λ= 0strategy in the present dataset.
We compare the test errors obtained from three approaches within our generalized Trans-Lasso framework:
The ∆λ= 0strategy, the κ= 0strategy, and LO strategy. Additionally, we include test errors from
Pretraining Lasso and the original Trans-Lasso for comparison, whose results are summarized in Table 2.
Recall that Trans-Lasso fixes κto unity, while Pretraining Lasso employs (κ,∆λ/λ2) = (1−s,(1−s)/s)for
hyperparameter s∈[0,1]. In the experiments, we choose sfrom{0.0,0.1,0.2,..., 1.0}via cross-validation.
Our analysis demonstrates that the hyperparameter choices made by both the original Trans-Lasso and
Pretraining Lasso are suboptimal compared to the ∆λ= 0strategy of our generalized Trans-Lasso algorithm.
Note that the LO strategy can potentially select hyperparameters exhibiting higher test error compared to
the∆λ= 0strategy (as seen in the Action genre), as they are chosen via cross-validation and the true
generalization error is not directly minimized.
6.2 Application to Compressed Imaging task on the MNIST dataset
The performance of the Generalized Trans-Lasso algorithm is also evaluated on a compressed imaging task
(Donoho,2006;Takharetal.,2006;Lustigetal.,2007;Romberg,2008;Candès&Wakin,2008)ontheMNIST
dataset (Deng, 2012). The MNIST dataset consists of grayscale images of handwritten digits with 28×28
pixels, totaling784dimensions. Givennoisy, linearmeasurementsofthefirstsetof3handwrittenimages, “1”,
“7”, and “9” appearing in the MNIST dataset, the task is to refine the recovery performance of each image in
thewaveletbasisbytransferlearningtechniques. WeconsidertheGaussianmeasurementsetupforvectorized
imagesx“n”∈R784(n= 1,7,9), where measurements are given by y“n”=H“n”x“n”+e“n”∈RM“n”. The
matricesH“n”consist of i.i.d. standard Gaussian elements, while e“n”represents additive Gaussian noise
with i.i.d. centered elements calibrated to achieve uniform signal-to-noise ratio across measurements. The
measurement dimensions are set to M“1”= 200,M“7”= 400, andM“9”= 600. The image recovery task is to
reconstructx“n”under the assumption that its wavelet transform θ“n”=W−1x“n”is sparse, whereWis the
2-d wavelet transformation matrix. Therefore, all algorithms, generalized Trans-Lasso, Pretraining Lasso,
and original Trans-Lasso, are targeted to estimate the coefficients of each image in the wavelet basis, θ“n”,
given observations y“n”and covariate matrix A“n”=H“n”W−1. The test error is evaluated by generating
another random instance ˜y“n”with the same statistical profile and dimension as y“n”, and using this as the
test dataset.
We calculate the test error on a (κ,∆λ/λ2)grid, where κ={0.0,0.1,0.2,···,1.5}, and ∆λ/λ2is taken from
21 logarithmically equidistanced points between 10−2and10. Other procedures are equivalent to those of
the IMDb experiment.
In Figure 5, we present the second stage test error for different handwritten digits across various values of
(κ,∆λ)underdifferentSNRconditions. Consistentwithourfindingsfromsyntheticexperiments, thesupport
information plays a crucial role in improving generalization performance, particularly at high SNR values.
The test errors, with the hyperparameters chosen via cross-validation error minimization, are summarized
in Table 3. For high signal quality (SNR = 20), the κ= 0strategy demonstrates superior performance.
However, as the SNR decreases, the performance gap between different strategies diminishes, with differences
in test error becoming statistically insignificant. This empirical observation aligns with our theoretical
analysis (Figure 2), where the generalization error curves for the LO, ∆λ= 0, andκ= 0strategies collapse
into one curve at high noise levels.
14Under review as submission to TMLR
Figure 5: Second stage test error for each MNIST image for SNR = 20 (top), 5 (middle) ,and 2 (bottom),
plotted against varying values of hyperparameters κand∆λ/λ2. White markers are placed at the minimizer
of the test error for sake of visualization. The plots demonstrate that the effect of ∆λon generalization
performance is nontrivial.
15Under review as submission to TMLR
SNR = 20
Method n= 1 n= 7 n= 9
Gen. Trans-Lasso, LO 0.0721±0.0069 0.0361 ±0.0029 0.0139 ±0.0009
Gen. Trans-Lasso, κ= 0 0.0721±0.0069 0.0367±0.0029 0.0139±0.0009
Gen. Trans-Lasso, ∆λ= 0 0.0816±0.0076 0.0404 ±0.0030 0.0144 ±0.0009
Pretraining Lasso 0.0847±0.0078 0.0381 ±0.0029 0.0140 ±0.0009
Trans-Lasso 0.1333±0.0122 0.0499 ±0.0037 0.0171 ±0.0010
SNR = 5
Method n= 1 n= 7 n= 9
Gen. Trans-Lasso, LO 0.163±0.013 0.103 ±0.008 0.0453±0.0028
Gen. Trans-Lasso, κ= 0 0.163±0.013 0.103 ±0.008 0.0450 ±0.0028
Gen. Trans-Lasso, ∆λ= 0 0.169±0.014 0.106 ±0.008 0.0460 ±0.0028
Pretraining Lasso 0.165±0.014 0.103±0.008 0.0454±0.0028
Trans-Lasso 0.215±0.020 0.117 ±0.009 0.0504 ±0.0029
SNR = 2
Method n= 1 n= 7 n= 9
Gen. Trans-Lasso, LO 0.292±0.024 0.196±0.015 0.099 ±0.006
Gen. Trans-Lasso, κ= 0 0.290±0.025 0.196 ±0.015 0.101±0.006
Gen. Trans-Lasso, ∆λ= 0 0.303±0.026 0.203 ±0.015 0.101 ±0.006
Pretraining Lasso 0.292±0.025 0.196±0.015 0.099 ±0.006
Trans-Lasso 0.303±0.027 0.208 ±0.016 0.100 ±0.006
Table 3: Performance comparison of different methods across the three handwritten digits for SNR = 20, 5,
and 2. Error estimation on the test error is done by performing jackknife resampling on the test dataset.
7 Conclusion
In this work, we have conducted a sharp asymptotic analysis of the generalized Trans-Lasso algorithm,
precisely characterizing the effect of hyperparameters on its generalization performance. Our theoretical
calculations reveal that near-optimal generalization in this transfer learning algorithm can be achieved by
focusing on just one of two modes of knowledge transfer from the source dataset. The first mode transfers
only the support information of the features obtained from all source data, while the second transfers the
actualconfigurationofthefeaturevectorobtainedfromallsources. ExperimentsusingtheIMDbandMNIST
dataset confirm that this simple hyperparameter strategy is effective, potentially outperforming conventional
Lasso-type transfer algorithms.
References
Urte Adomaityte, Leonardo Defilippis, Bruno Loureiro, and Gabriele Sicuro. High-dimensional robust re-
gression under heavy-tailed data: Asymptotics and universality. arXiv preprint arXiv:2309.16476 , 2023.
Trambak Banerjee, Gourab Mukherjee, and Wenguang Sun. Adaptive sparse estimation with side informa-
tion.Journal of the American Statistical Association , 115(532):2053–2067, 2020.
Jean Barbier and Nicolas Macris. The adaptive interpolation method: a simple scheme to prove replica
formulas in bayesian inference. Probability theory and related fields , 174:1133–1185, 2019.
Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal errors and
phase transitions in high-dimensional generalized linear models. Proceedings of the National Academy of
Sciences, 116(12):5451–5460, 2019.
Hamsa Bastani. Predicting with proxies: Transfer learning in high dimension. Management Science , 67(5):
2964–2984, 2021.
16Under review as submission to TMLR
Mohsen Bayati and Andrea Montanari. The Lasso risk for gaussian matrices. IEEE Transactions on Infor-
mation Theory , 58(4):1997–2017, 2011.
Pierre C Bellec and Yiwei Shen. Derivatives and residual distribution of regularized m-estimators with
application to adaptive tuning. In Conference on Learning Theory , pp. 1912–1947. PMLR, 2022.
Pierre C. Bellec and Cun-Hui Zhang. Second-order Stein: SURE for SURE and other applications in high-
dimensional inference. The Annals of Statistics , 49(4):1864 – 1903, 2021.
Pierre C. Bellec, Guillaume Lecué, and Alexandre B. Tsybakov. Slope meets Lasso: Improved oracle bounds
and optimality. The Annals of Statistics , 46(6B):3603 – 3642, 2018.
Emmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal
encoding strategies? IEEE Transactions on Information Theory , 52(12):5406–5425, 2006.
EmmanuelJCandèsandMichaelBWakin. Anintroductiontocompressivesampling. IEEE signal processing
magazine , 25(2):21–30, 2008.
Kabir Aladin Chandrasekher, Ashwin Pananjady, and Christos Thrampoulidis. Sharp global convergence
guarantees for iterative nonconvex optimization with random data. The Annals of Statistics , 51(1):179 –
210, 2023.
Patrick Charbonneau, Enzo Marinari, Marc Mézard, Giorgio Parisi, Federico Ricci-Tersenghi, Gabriele Si-
curo, and Francesco Zamponi. Spin Glass Theory and Far Beyond . WORLD SCIENTIFIC, 2023.
Erin Craig, Mert Pilanci, Thomas Le Menestrel, Balasubramanian Narasimhan, Manuel Rivas, Roozbeh
Dehghannasiri, Julia Salzman, Jonathan Taylor, and Robert Tibshirani. Pretraining and the Lasso. arXiv
preprint arXiv:2401.1291 , 2024.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine , 29(6):141–142, 2012.
David L Donoho. Compressed sensing. IEEE Transactions on information theory , 52(4):1289–1306, 2006.
Cedric Gerbelot, Alia Abbara, and Florent Krzakala. Asymptotic errors for teacher-student convex gener-
alized linear models (or: How to prove Kabashima’s replica formula). IEEE Transactions on Information
Theory, 69(3):1824–1852, 2023.
Samuel M. Gross and Robert Tibshirani. Data shared Lasso: A novel tool to discover uplift. Computational
Statistics and Data Analysis , 101:226–235, 2016.
Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Alireza Karbalayghareh, Mingyuan Zhou, and Xiaoning
Qian. Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count
data. In Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018.
AdelJavanmardandAndreaMontanari. Stateevolutionforgeneralapproximatemessagepassingalgorithms,
with applications to spatial coupling. Information and Inference: A Journal of the IMA , 2(2):115–144,
2013.
Yoshiyuki Kabashima, Tadashi Wadayama, and Toshiyuki Tanaka. A typical reconstruction limit for com-
pressed sensing based on ℓp-norm minimization. Journal of Statistical Mechanics: Theory and Experiment ,
2009(09):L09003, 2009.
Sai Li, T. Tony Cai, and Hongzhe Li. Transfer Learning for High-Dimensional Linear Regression: Predic-
tion, Estimation and Minimax Optimality. Journal of the Royal Statistical Society Series B: Statistical
Methodology , 84(1):149–173, 11 2021.
Sai Li, T Tony Cai, and Hongzhe Li. Transfer learning in large-scale gaussian graphical models with false
discovery rate control. Journal of the American Statistical Association , 118(543):2171–2183, 2023.
17Under review as submission to TMLR
SaiLi, LinjunZhang, TTonyCai, andHongzheLi. Estimationandinferenceforhigh-dimensionalgeneralized
linear models with knowledge transfer. Journal of the American Statistical Association , 119(546):1274–
1285, 2024.
Michael Lustig, David Donoho, and John M Pauly. Sparse mri: The application of compressed sensing for
rapid mr imaging. Magnetic Resonance in Medicine: An Official Journal of the International Society for
Magnetic Resonance in Medicine , 58(6):1182–1195, 2007.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea
(eds.),Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies , pp. 142–150, Portland, Oregon, USA, June 2011. Association for Computational
Linguistics.
Sarah F. McGough, Svetlana Lyalina, Devin Incerti, Yunru Huang, Stefka Tyanova, Kieran Mace, Chris
Harbron, Ryan Copping, Balasubramanian Narasimhan, and Robert Tibshirani. Prognostic pan-cancer
and single-cancer models: A large-scale analysis using a real-world clinico-genomic database. medRxiv
preprint 2023.12.18.23300166 , 2023.
Nicolai Meinshausen and Peter Bühlmann. High-dimensional graphs and variable selection with the Lasso.
The Annals of Statistics , 34(3):1436 – 1462, 2006.
M Mezard, G Parisi, and M Virasoro. Spin Glass Theory and Beyond . WORLD SCIENTIFIC, 1986.
Léo Miolane and Andrea Montanari. The distribution of the Lasso: Uniform control over sparse balls and
adaptive parameter tuning. The Annals of Statistics , 49(4), 2021.
Tomoyuki Obuchi and Yoshiyuki Kabashima. Cross validation in LASSO and its acceleration. Journal of
Statistical Mechanics: Theory and Experiment , 2016(5):053304, 2016.
Tomoyuki Obuchi and Yoshiyuki Kabashima. Semi-analytic resampling in Lasso. Journal of Machine Learn-
ing Research , 20(70):1–33, 2019.
Koki Okajima and Takashi Takahashi. Asymptotic dynamics of alternating minimization for bilinear regres-
sion.arXiv preprint arXiv:2402.04751 , 2024.
Koki Okajima, Xiangming Meng, Takashi Takahashi, and Yoshiyuki Kabashima. Average case analysis of
Lasso under ultra sparse conditions. In Proceedings of The 26th International Conference on Artificial
Intelligence and Statistics , volume 206 of Proceedings of Machine Learning Research , pp. 11317–11330.
PMLR, 25–27 Apr 2023.
E. Ollier and V. Viallon. Regression modelling on stratified data with the Lasso. Biometrika , 104(1):83–96,
2017.
Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via transfer compo-
nent analysis. IEEE Transactions on Neural Networks , 22(2):199–210, 2011.
Raphael Petegrosso, Sunho Park, Tae Hyun Hwang, and Rui Kuang. Transfer learning across ontologies for
phenome–genome association prediction. Bioinformatics , 33(4):529–536, 11 2016.
Kamiar Rahnama Rad and Arian Maleki. A scalable estimate of the out-of-sample prediction error via
approximate leave-one-out cross-validation. Journal of the Royal Statistical Society Series B: Statistical
Methodology , 82(4):965–996, 2020.
SundeepRangan, VivekGoyal, andAlysonKFletcher. Asymptoticanalysisofmapestimationviathereplica
method and compressed sensing. In Advances in Neural Information Processing Systems , volume 22.
Curran Associates, Inc., 2009.
Justin Romberg. Imaging via compressive sampling. IEEE signal processing magazine , 25(2):14–20, 2008.
18Under review as submission to TMLR
Luca Saglietti and Lenka Zdeborová. Solvable model for inheriting the regularization through knowledge
distillation. In Mathematical and Scientific Machine Learning , pp. 809–846. PMLR, 2022.
William Stephenson and Tamara Broderick. Approximate cross-validation in high dimensions with guaran-
tees. InProceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics ,
volume 108 of Proceedings of Machine Learning Research , pp. 2424–2434. PMLR, 26–28 Aug 2020.
Mihailo Stojnic. A framework to characterize performance of Lasso algorithms. arXiv preprint
arXiv:1303.7291 , 2013.
Fei Sun and Qi Zhang. Robust transfer learning of high-dimensional generalized linear model. Physica A:
Statistical Mechanics and its Applications , 618:128674, 2023.
Masaaki Takada and Hironori Fujisawa. Transfer learning via ℓ1regularization. Advances in Neural Infor-
mation Processing Systems , 33:14266–14277, 2020.
Takashi Takahashi. The role of pseudo-labels in self-training linear classifiers on high-dimensional gaussian
mixture data. arXiv preprint arXiv:2205.07739 , 2024.
TakashiTakahashiandYoshiyukiKabashima. Astatisticalmechanicsapproachtode-biasinganduncertainty
estimation in Lasso for random measurements. Journal of Statistical Mechanics: Theory and Experiment ,
2018(7):073405, 2018.
Dharmpal Takhar, Jason N Laska, Michael B Wakin, Marco F Duarte, Dror Baron, Shriram Sarvotham,
Kevin F Kelly, and Richard G Baraniuk. A new compressive imaging camera architecture using optical-
domain compression. In Computational Imaging IV , volume 6065, pp. 43–52. SPIE, 2006.
ChristosThrampoulidis, EhsanAbbasi, andBabakHassibi. Preciseerroranalysisofregularized m-estimators
in high dimensions. IEEE Transactions on Information Theory , 64(8):5592–5628, 2018.
Ye Tian and Yang Feng. Transfer learning under high-dimensional generalized linear models. Journal of the
American Statistical Association , 118(544):2684–2697, 2023.
Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society.
Series B (Methodological) , 58(1):267–288, 1996.
Lisa Torrey and Jude Shavlik. Transfer Learning. In: Handbook of Research on Machine Learning Applica-
tions and Trends: Algorithms, Methods, and Techniques . IGI Global, 2010.
Turki Turki, Zhi Wei, and Jason T. L. Wang. Transfer learning approaches to improve drug sensitivity
prediction in multiple myeloma patients. IEEE Access , 5:7381–7393, 2017.
Mikko Vehkaperä, Yoshiyuki Kabashima, and Saikat Chatterjee. Analysis of regularized LS reconstruction
and random matrix ensembles in compressed sensing. IEEE Transactions on Information Theory , 62(4):
2100–2124, 2016.
Martin J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1-
constrained quadratic programming (Lasso). IEEE Transactions on Information Theory , 55(5):2183–2202,
2009.
Meiyu Wang, Yun Lin, Qiao Tian, and Guangzhen Si. Transfer learning promotes 6G wireless communica-
tions: Recent advances and future challenges. IEEE Transactions on Reliability , 70(2):790–807, 2021.
Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of Big data ,
3:1–40, 2016.
Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning Research , 7
(90):2541–2563, 2006.
19Under review as submission to TMLR
Yin Zhu, Yuqiang Chen, Zhongqi Lu, Sinno Jialin Pan, Gui-Rong Xue, Yong Yu, and Qiang Yang. Hetero-
geneous transfer learning for image classification. In Proceedings of the Twenty-Fifth AAAI Conference
on Artificial Intelligence , AAAI’11, pp. 1304–1309. AAAI Press, 2011.
Fuzhen Zhuang, Ping Luo, Hui Xiong, Qing He, Yuhong Xiong, and Zhongzhi Shi. Exploiting associations
between word clusters and document classes for cross-domain text categorization. Statistical Analysis and
Data Mining: The ASA Data Science Journal , 4(1):100–114, 2011.
Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing
He. A comprehensive survey on transfer learning. Proceedings of the IEEE , 109(1):43–76, 2021.
A Detailed derivation of Asymptotic analysis
Here, we provide a detailed derivation for the asymptotic formulae for the generalization errors ϵ(1st)and
ϵ(2nd). Recall that to calculate the generalization error, an appropriate choice of fin equation 13 is
f(x1,x2;˜D) =t1K/summationdisplay
k=1∥˜y(k)−˜A(k)x1∥2
2+t2∥˜y(1)−˜A(1)(κx1+x2)∥2
2. (26)
Therefore, the objective of our analysis is to calculate the data average of the n−th power of
Φ(D,˜D) :=/integraldisplay
dx1dx2e−β1L(1st)(x1;D)−β2L(2nd)(x2;x1,D(1))+t1/summationtextK
k=1∥˜y(k)−˜A(k)x1∥2
2+t2∥˜y(1)−˜A(1)(κx1+x2)∥2
2,(27)
in the successive limit limβ2→∞limβ1→∞. Forn∈N, one can express this as
ED,˜D/bracketleftig
Φn(D,˜D)/bracketrightig
=/integraldisplay/parenleftiggn/productdisplay
a=1dx1,adx2,a/parenrightigg
exp/bracketleftig
−n/summationdisplay
a=1/parenleftig
β1λ1∥x1,a∥1+β2r(x2,a|x1,a)/parenrightig/bracketrightig
L,
L=ED,˜DM/productdisplay
µ=1n/productdisplay
a=1exp/bracketleftigg
−β1
2K/summationdisplay
k=1/parenleftbig
y(k)
µ−A(k)
µ,:·x1,a/parenrightbig2−t1K/summationdisplay
k=1/parenleftbig
˜y(k)
µ−˜A(k)
µ,:·x1,a/parenrightbig2
−β2
2/parenleftbig
y(1)
µ−A(1)
µ,:·(κx1,a+x2,a)/parenrightbig2−t2/parenleftbig
˜y(1)
µ−˜A(1)
µ,:·(κx1,a+x2,a)/parenrightbig2/bracketrightigg
.(28)
Conditioned on{x1,a,x2,a}n
a=1, the distribution over {A(k)
µ,:·x1,a,˜A(k)
µ,:·x1,a,A(1)
µ,:·x2,a,˜A(1)
µ,:·x2,a,yµ,˜yµ}M
µ=1
is i.i.d. with respect to µ, whose profile is identical to that of the random variables
h(k)
1,a:=a(k)·x1,a, h 2,a:=a(1)·x2,a, ˜h(k)
1,a:=˜a(k)·x1,a, ˜h2,a:=˜a(1)·x2,a,(29)
H(k):=a(k)
I(k)·x(k)
⋆+a(k)
I(0)·x(0)
⋆+ξ(k), ˜H(k):=˜a(k)
I(k)·x(k)
⋆+˜a(k)
I(0)·x(0)
⋆+˜ξ(k),(30)
with{a(k),˜a(k)}K
k=1being a set of N-dimensional centered Gaussian vectors with i.i.d. elements of variance
1/N, andξ(k),˜ξ(k)being independent centered Gaussian random variables with variance (σ(k))2fork=
1,···,K. Note that the random variables with different index kare independent of each other. Following
this observation, Lcan be expressed alternatively as L:=/producttextK
k=1(EL(k))M(k)(E˜L(k))M(k),where
L(k)=n/productdisplay
a=1exp/bracketleftig
−β1
2(H(k)−h(k)
1,a)2−δ1kβ2
2(H(1)−κh(1)
1,a−h2,a)2/bracketrightig
, (31)
˜L(k)=n/productdisplay
a=1exp/bracketleftig
−t1(˜H(k)−˜h(k)
1,a)2−δ1kt2(˜H(1)−κ˜h(1)
1,a−˜h2,a)2/bracketrightig
. (32)
20Under review as submission to TMLR
Here,δij=I[i=j]denotestheKroneckerdelta, whichisunityif i=jandzerootherwise. FromtheGaussian
nature of{a(k),˜a(k)}K
k=1and{ξ(k),˜ξ(k)}K
k=1, the random variables {h(k)
1,a}n,K
a,k=1,{h2,a}n
a=1,and{H(k)}K
k=1are
all centered Gaussians with the following covariance structure:
E[h(k)
1,ah(l)
1,b] =δklQab
1, Qab
1:=1
Nx1,a·x1,b=1
N/parenleftiggK/summationdisplay
k′=0x(k′)
1,a·x(k′)
1,b+x(neg.)
1,a·x(neg.)
1,b/parenrightigg
,
E[h2,ah2,b] =Qab
2, Qab
2:=1
Nx2,a·x2,b=1
N/parenleftiggK/summationdisplay
k′=0x(k′)
2,a·x(k′)
2,b+x(neg.)
2,a·x(neg.)
2,b/parenrightigg
,
E[h(k)
1,ah2,a] =δ1kQab
r, Qab
r:=1
Nx1,a·x2,b=1
N/parenleftiggK/summationdisplay
k′=0x(k′)
1,a·x(k′)
2,b+x(neg.)
1,a·x(neg.)
2,b/parenrightigg
,
E[h(k)
1,aH(l)] =δkl/parenleftbig
m(k)
1+m(0)
1/parenrightbig
, m(k′)
1,a:=1
Nx(k′)
1,a·x(k′)
⋆ (k′= 0,1,···,K),
E[h2,aH(k)] =δk1/parenleftbig
m(k)
2+m(0)
2/parenrightbig
, m(k′)
2,a:=1
Nx(k′)
2,a·x(k′)
⋆ (k′= 0,1,···,K),
E[H(k)H(l)] =δkl/bracketleftigg
1
N∥x(k)
⋆∥2
2+1
N∥x(0)
⋆∥2
2+ (σ(k))2/bracketrightigg
=δklρ(k),(33)
for1≤k,l≤Kand 1≤a,b≤n. Here,x(k)
1,aandx(k)
2,aare defined as the subvectors of x1,aand
x2,aindexed byI(k)respectively, while x(neg.)
1,aandx(neg.)
2,aare both subvectors of x1,aandx2,awhose
indices are not included in/uniontextK
k=0I(k). The same covariance structure also follows for the random variables
{˜h(k)
1,a}n,K
a,k=1,{˜h2,a}n
a=1,and{˜H(k)}K
k=1. Define the average with respect to the random variables given in
equation 33, conditioned on Ω ={Qab
1,Qab
2,Qab
r,m(k)
1,a,m(k)
2,a}, asE|Ω. Inserting the trivial identities based on
the delta function corresponding to the definitions of Ωgiven in equation 33, we can rewrite equation 28 up
to a trivial multiplicative constant as
/integraldisplay
dΩ/integraldisplay/parenleftiggn/productdisplay
a=1K/productdisplay
k=0dx(k)
1,adx(k)
2,a/parenrightigg
exp/bracketleftigg
−n/summationdisplay
a=1/parenleftig
β1λ1∥x1,a∥1+β2r(x1,a|x2,a)/parenrightig
+K/summationdisplay
k=1M(k)log(E|ΩL(k))(E|Ω˜L(k))/bracketrightigg
×n/productdisplay
a≤b=1δ/parenleftigg
NQab
1−K/summationdisplay
k′=0x(k′)
1,a·x(k′)
1,b−x(neg.)
1,a·x(neg.)
1,b/parenrightigg
δ/parenleftigg
NQab
2−K/summationdisplay
k′=0x(k′)
2,a·x(k′)
2,b−x(neg.)
2,a·x(neg.)
2,b/parenrightigg
×n/productdisplay
a,b=1δ/parenleftigg
NQab
r−K/summationdisplay
k′=0x(k′)
1,a·x(k′)
2,b−x(neg.)
1,a·x(neg.)
2,b/parenrightigg
E⋆n/productdisplay
a=1K/productdisplay
k=0δ/parenleftbig
Nm(k)
1,a−x(k)
1,a·x(k)
⋆/parenrightbig
δ/parenleftbig
Nm(k)
2,a−x(k)
2,a·x(k)
⋆/parenrightbig
,
(34)
where E⋆is the average with respect to the set of ground truth vectors {x(k)
⋆}K
k=0, whose elements are i.i.d.
according to a standard normal distribution. Note that the delta functions can be expressed alternatively
21Under review as submission to TMLR
using its Fourier representation as
δ/parenleftigg
NQab
1−K/summationdisplay
k′=0x(k′)
1,a·x(k′)
1,b−x(neg.)
1,a·x(neg.)
1,b/parenrightigg
=/integraldisplay
CdˆQab
1eˆQab
1/parenleftbig
NQab
1−/summationtextK
k′=0x(k′)
1,a·x(k′)
1,b−x(neg.)
1,a·x(neg.)
1,b/parenrightbig
,
δ/parenleftigg
NQab
2−K/summationdisplay
k′=0x(k′)
2,a·x(k′)
2,b−x(neg.)
2,a·x(neg.)
2,b/parenrightigg
=/integraldisplay
CdˆQab
2eˆQab
2/parenleftbig
NQab
2−/summationtextK
k′=0x(k′)
2,a·x(k′)
2,b−x(neg.)
2,a·x(neg.)
2,b/parenrightbig
,
δ/parenleftigg
NQab
r−K/summationdisplay
k′=0x(k′)
1,a·x(k′)
2,b−x(neg.)
1,a·x(neg.)
2,b/parenrightigg
=/integraldisplay
CdˆQab
reˆQab
r/parenleftbig
NQab
r−/summationtextK
k′=0x(k′)
1,a·x(k′)
2,b−x(neg.)
1,a·x(neg.)
2,b/parenrightbig
,
δ/parenleftbig
Nm(k)
1,a−x(k)
1,a·x(k)
⋆/parenrightbig
=/integraldisplay
Cdˆm(k)
1,aeˆm(k)
1,a/parenleftbig
Nm(k)
1,a−x(k)
1,a·x(k)
⋆/parenrightbig
,
δ/parenleftbig
Nm(k)
2,a−x(k)
2,a·x(k)
⋆/parenrightbig
=/integraldisplay
Cdˆm(k)
2,aeˆm(k)
2,a/parenleftbig
Nm(k)
2,a−x(k)
2,a·x(k)
⋆/parenrightbig
,(35)
up to a trivial multiplicative constant.
To obtain an expression that is analytically continuable to n→0, we introduce the replica symmetric ansatz
(Charbonneau et al., 2023), which assumes that the integral over Ωis dominated by the contribution of the
subspace where Ωsatisfy the following constraints:
Qab
1=q1+ (1−δab)χ1
β1, Qab
2=q2+ (1−δab)χ2
β2, Qab
r=qr+ (1−δab)χr
β1,
m(k)
1,a=m(k)
1, m(k)
2,a=m(k)
2,(36)
for1≤a,b≤nand0≤k≤K. Although a general proof of this ansatz itself is still lacking, calculations
based on this assumption are known to be asymptotically exact in the limit N→∞for convex generalized
linear models (Gerbelot et al., 2023) and Bayes-optimal estimation (Barbier et al., 2019). Consequently, the
conjugate variables {ˆQab
1,ˆQab
2,ˆQab
r,ˆm(k)
1,a,ˆm(k)
2,a}are also assumed to be replica symmetric:
ˆQab
1=β1ˆq1−(1−δab)β2
1ˆχ1, ˆQab
2=β2ˆq2−(1−δab)β2
2ˆχ2, ˆQab
r=−β2ˆqr+ (1−δab)β1β2ˆχr,
ˆm(k)
1,a=−β1ˆm(k)
1, ˆm(k)
2,a=−β2ˆm(k)
2.(37)
Inserting equations 36 and 37 to equation 34, and rewriting E|ΩasE|Θgiven this simplified profile of Ωoffers
E⋆/integraldisplay
CdΘ1dΘ2/integraldisplay/parenleftiggn/productdisplay
a=1K/productdisplay
k=0dx(k)
1,adx(k)
2,a/parenrightigg
exp/bracketleftiggK/summationdisplay
k=1M(k)log(E|ΘL(k)) +K/summationdisplay
k=1M(k)log(E|Θ˜L(k))/bracketrightigg
×expnN/bracketleftigg
β1/parenleftigq1ˆq1−χ1ˆχ1
2−K/summationdisplay
k=0m(k)
1ˆm(k)
1/parenrightig
+β2/parenleftigq2ˆq2−χ2ˆχ2
2−qrˆqr−χrˆχr−K/summationdisplay
k=0m(k)
2ˆm(k)
2/parenrightig/bracketrightigg
×exp/bracketleftigg
−β1n/summationdisplay
a=1K/summationdisplay
k=0/parenleftigˆq1
2∥x(k)
1,a∥2
2−ˆm1x(k)
⋆·x(k)
1,a+λ1∥x(k)
1,a∥1/parenrightig
−β1n/summationdisplay
a=1/parenleftigˆq1
2∥x(neg.)
1,a∥2
2+λ1∥x(neg.)
1,a∥1/parenrightig
−β2n/summationdisplay
a=1K/summationdisplay
k=0/parenleftigˆq2
2∥x(k)
2,a∥2
2−( ˆm2x(k)
⋆+ ˆqrx(k)
1,a)·x(k)
2,a+r(x(k)
2,a|x(k)
1,a)/parenrightig
−β2n/summationdisplay
a=1/parenleftigˆq2
2∥x(neg.)
2,a∥2
2−ˆqrx(neg.)
1,a·x(neg.)
2,a+r(x(neg.)
2,a|x(neg.)
1,a)/parenrightig
+β2
1ˆχ1
2/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
a=1x1,a/vextenddouble/vextenddouble/vextenddouble2
2+β2
2ˆχ2
2/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
a=1x2,a/vextenddouble/vextenddouble/vextenddouble2
2+β1β2ˆχr/parenleftign/summationdisplay
a=1x1,a/parenrightig
·/parenleftign/summationdisplay
a=1x2,a/parenrightig/bracketrightigg
.(38)
22Under review as submission to TMLR
The last equation can be further simplified using the equality
exp/bracketleftigg
β2
1ˆχ1
2/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
a=1x1,a/vextenddouble/vextenddouble/vextenddouble2
2+β2
2ˆχ2
2/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
a=1x2,a/vextenddouble/vextenddouble/vextenddouble2
2+β1β2ˆχr/parenleftign/summationdisplay
a=1x1,a/parenrightig
·/parenleftign/summationdisplay
a=1x2,a/parenrightig/bracketrightigg
(39)
=Ez1,z2exp/bracketleftigg
β1/radicalbig
ˆχ1n/summationdisplay
a=1z1·x1,a+β2/radicalbig
ˆχ2n/summationdisplay
a=1z2·x2,a/bracketrightigg
(40)
wherez1,z2∈RNare random Gaussian vectors with elements i.i.d. according to z1,i∼z1,z2,i∼z2for
alli= 1,···,N. From this decomposition, the integrals over x1,aandx2,adecouple over both vector
coordinates i= 1,···,Nand replica indices a= 1,···,n:
/integraldisplay
dΘ1dΘ2/integraldisplay
e/summationtextK
k=1M(k)logE|ΘL(k)+/summationtextK
k=1M(k)logE|Θ˜L(k)K/productdisplay
k=0/braceleftig
E2/bracketleftig/integraldisplay
dx1dx2e−β1E(k)
1(x1)−β2E(k)
2(x2|x1)/bracketrightign/bracerightigN(k)
×/braceleftig
E2/bracketleftig/integraldisplay
dx1dx2e−β1E(neg.)
1(x1)−β2E(neg.)
2(x2|x1)/bracketrightign/bracerightigN−/summationtextK
k=0N(k)
.
(41)
In the successive limit of β1→∞andβ2→∞, the integrals over x1andx2can be evaluated using Laplace’s
method, yielding the asymptotic form:
/integraldisplay
dΘ1dΘ2K/productdisplay
k=1eM(k)logE|ΘL(k)+M(k)logE|Θ˜L(k)K/productdisplay
k=0/braceleftig
E2/bracketleftig
e−β1E(k)
1(x(k)
1)−β2E(k)
2(x(k)
2|x(k)
1)/bracketrightign/bracerightigN(k)
×/braceleftig
E2/bracketleftig
e−β1E(neg.)
1(x(neg.)
1)−β2E(neg.)
2(x(neg.)
2|x(neg.)
1)/bracketrightign/bracerightigN−/summationtextK
k=0N(k)
.(42)
where the definitions of E2and{x(k)
1,x(k)
2}K
k=0,x(neg.)
1,x(neg.)
2follow from Definitions 1 and 2.
Let us proceed with the calculation of E|ΘL(k). Define the centered Gaussian variables {H(k),h(k)
1,h(k)
2}and
{u1,a,u2,a}n
a=1as

H(k)
h(k)
1
h(k)
2
∼N
03,
ρ(k)m(k)
1m(k)
2
m(k)
1q1qr
m(k)
2qrq2

,/parenleftbiggu1,a
u2,a/parenrightbigg
∼N/parenleftbigg
02,/parenleftbiggχ1/β1χr/β1
χr/β1χ2/β2/parenrightbigg/parenrightbigg
,(43)
where{u1,a,u2,a}are independent for all a= 1,···,n. Then, we can see that the random variables admit
the decomposition h(k)
1,a=h(k)
1+u1,a,h2,a=h(k)
2+u2,a, offering the expression
E|ΘL(k)=Eh(k)
1,h(k)
2,H(k)n/productdisplay
a=1Eu1,a,u2,aexp/bracketleftigg
−β1
2(H(k)−h(k)
1−u1,a)2−β2δ1k
2/parenleftig
H(k)−κ(h(k)
1+u1,a)−h(k)
2−u2,a/parenrightig2/bracketrightigg
.
(44)
Taking into account that β1≫β2, the Gaussian measure over (u1,a,u2,a)can be written as
Cβ1,β2exp/bracketleftigg
−β1
2χ1(1 +cβ1)u2
1,a−β2
2χ2/parenleftig
u2,a−χr
χ1u1,a/parenrightig2/bracketrightigg
du1,adu2,a, (45)
whereCβ1,β2is a number subexponential in β1andβ2, andcβ1is a real number which converges to zero as
β1→∞. Using this expression to equation 44 yields
E|ΘL(k)=Eh(k)
1,h(k)
2,H(k)/braceleftigg/integraldisplay
du1du2exp/bracketleftigg
−β1
2χ1(1 +cβ1)u2
1−β1
2(H(k)−h(k)
1−u1)2
−β2
2χ2u2
2−β2δ1k
2/parenleftig
H(k)−κ(h(k)
1+u1)−h(k)
2−u2−χr
χ1u1/parenrightig2
+ logCβ1,β2/bracketrightigg/bracerightiggn
.(46)
23Under review as submission to TMLR
The same procedure can be applied to ˜L(k), this time accounting for t1,t2≪β2, offering
E|Θ˜L(k)=E˜h(k)
1,˜h(k)
2,˜H(k)expn/bracketleftigg
−t1/parenleftig
˜H(k)−˜h(k)
1/parenrightig2
−t2δ1k/parenleftig
˜H(k)−κ˜h(k)
1−˜h(k)
2/parenrightig2
+ logCβ1,β2/bracketrightigg
.(47)
Note that the expressions for ˜L(k)andL(k)are simple Gaussian integrals that can be computed explicitly.
Now that all the terms in equation 34 have been expressed in analytical form with respect to n, the limit
n→0can be taken formally. Evaluating equations 42, 46 and 47 up to first order of n, and neglecting
subleading terms with respect to β1andβ2, we finally obtain an expression of the form
ED,˜D/bracketleftig
Φn(D,˜D)/bracketrightig
=/integraldisplay
dΘ1dΘ2expN/bracketleftbig
nG(Θ1,Θ2) +O(n2)/bracketrightbig
, (48)
where
G(Θ1,Θ2) =β1G1(Θ1) +β2G2(Θ2|Θ1) +t1ϵ1+t2ϵ2, (49)
G1(Θ1) =q1ˆq1−χ1ˆχ1
2−K/summationdisplay
k=0m(k)
1ˆm(k)
1−K/summationdisplay
k=0π(k)E1/bracketleftig
E(k)
1/parenleftbig
x(k)
1/parenrightbig/bracketrightig
−π(neg.)E1/bracketleftig
E(neg.)
1/parenleftbig
x(neg.)
1/parenrightbig/bracketrightig
−K/summationdisplay
k=1α(k)
2q1−2(m(0)
1+m(k)
1) +ρ(k)
1 +χ1, (50)
G2(Θ2|Θ1) =q2ˆq2−χ2ˆχ2
2−qrˆqr−χrˆχr−K/summationdisplay
k=0m(k)
2ˆm(k)
2
−K/summationdisplay
k=0π(k)E2/bracketleftig
E(k)
2/parenleftbig
x(k)
2|x(k)
1/parenrightbig/bracketrightig
−π(neg.)E2/bracketleftig
E(neg.)
2/parenleftbig
x(neg.)
2|x(neg.)
1/parenrightbig/bracketrightig
−α(1)
2q2+A2q1+ 2Aqr+B2ρ(1)−2B(m(0)
2+m(1)
2)−2AB(m(0)
1+m(1)
1)
1 +χ2, (51)
and
ϵ1=K/summationdisplay
k=1π(k)/bracketleftig
q1−2(m(0)
1+m(k)
1) +ρ(k)/bracketrightig
, (52)
ϵ2=π(1)/bracketleftig
q2+κ2q1+ 2κqr+ρ(1)−2(m(0)
2+m(1)
2)−2κ(m(0)
1+m(1)
1)/bracketrightig
. (53)
For largeNand finiten, the integral over Θ1andΘ2can be evaluated using the saddle point method, where
the integral is dominated by the stationary point of the exponent. This finally yields
lim
n→0lim
N→∞1
N∂
∂nlogED,˜D/bracketleftig
Φn(D,˜D)/bracketrightig
= Extr
Θ1,Θ2G(Θ1,Θ2), (54)
where Extrdenotes the extremum operation of the function. The stationary conditions for Θ1andΘ2, in
the successive limit of β1→∞andβ2→∞are given by the equations of state in Definitions 1 and 2. Note
that the term t1ϵ1+t2ϵ2does not contribute to the stationary condition in the limit t1,t2→0, and thus
can be neglected until one takes the derivative with respect to t1andt2, as in equations 11 and 12.
B Additional numerical experiments
Here, we provide more numerical experiments highlighting the difference between the Generalized Trans-
Lasso, PretrainingLassoandTrans-Lasso. Infigures6and7, weshowtheratiosof min{ϵ∆λ=0,ϵκ=0},ϵPretrain
andϵTransagainstϵLOfor the cases (π(0),π(1),π(2)) = (0.15,0.04,0.04)and(0.05,0.14,0.14), respectively.
The former case represents the problem setting where the underlying common feature vector among the
24Under review as submission to TMLR
datasets is large, while the latter represents the problem setting where the underlying common feature vector
is small. In both cases, the minimum of the κ= 0or∆λ= 0strategy can obtain generalization errors close
to the one from the LO strategy for both low or moderate noise levels. This is not a property exhibited in
the Trans-Lasso or Pretraining Lasso, where both algorithms have relatively low generalization performance
whenσ= 0.01and(π(0),π(1),π(2)) = (0.15,0.04,0.04). Note that when (π(0),π(1),π(2)) = (0.05,0.14,0.14),
all three algorithms yield comparable generalization performance.
To directly compare the κ= 0or∆λ= 0strategy with the Pretraining Lasso and Trans-Lasso, in figures
8, 9 and 10, we also plot the same ratios with shared color scale bars. As evident from the plot, the κ= 0
or∆λ= 0strategy exhibits a clear improvement in generalization performance over Trans-Lasso. While the
difference between Pretraining Lasso and κ= 0or∆λ= 0strategy is comparable for σ= 0.1, we can see a
consistent advantage for lower noise levels ( σ= 0.01).
25Under review as submission to TMLR
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.01.52.02.53.03.54.0
123456
1.001.021.041.061.081.101.12=0.01, ((0),(1),(2))=(0.15,0.04,0.04)
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.001.011.021.031.041.05
1.01.21.41.61.82.02.22.42.6
1.0001.0051.0101.0151.0201.0251.0301.0351.040=0.1, ((0),(1),(2))=(0.15,0.04,0.04)
Figure 6: Ratios min{ϵ∆λ=0,ϵκ=0}/ϵLO,ϵPretrain/ϵLOandϵTrans/ϵLOunder setting (π(0),π(1),π(2)) =
(0.15,0.04,0.04)and noise level σ= 0.01(top figure) and 0.1(bottom figure) for various values of (α(1),α(2)).
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.001.021.041.061.081.101.12
1.001.021.041.061.081.101.12
1.0001.0051.0101.0151.0201.025=0.01, ((0),(1),(2))=(0.05,0.14,0.14)
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.0001.0051.0101.0151.0201.025
1.001.011.021.031.041.051.061.071.08
1.0001.0051.0101.0151.0201.025=0.1, ((0),(1),(2))=(0.05,0.14,0.14)
Figure 7: Ratios min{ϵ∆λ=0,ϵκ=0}/ϵLO,ϵPretrain/ϵLOandϵTrans/ϵLOunder setting (π(0),π(1),π(2)) =
(0.05,0.14,0.14)and noise level σ= 0.01(top figure) and 0.1(bottom figure) for various values of (α(1),α(2)).
26Under review as submission to TMLR
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.01.52.02.53.03.54.0
123456=0.01, ((0),(1),(2))=(0.15,0.04,0.04)
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.001.011.021.031.041.05
1.01.21.41.61.82.02.22.42.6=0.1, ((0),(1),(2))=(0.15,0.04,0.04)
Figure 8: Comparisons between ratios min{ϵ∆λ=0,ϵκ=0}/ϵLO,ϵPretrain/ϵLOandϵTrans/ϵLOunder setting
(π(0),π(1),π(2)) = (0.15,0.04,0.04).
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.01.11.21.31.41.51.61.71.8
1.01.21.41.61.82.0=0.01, ((0),(1),(2))=(0.1,0.09,0.09)
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.001.011.021.031.041.05
1.001.051.101.151.201.251.301.35=0.1, ((0),(1),(2))=(0.1,0.09,0.09)
Figure 9: Comparisons between ratios min{ϵ∆λ=0,ϵκ=0}/ϵLO,ϵPretrain/ϵLOandϵTrans/ϵLOunder setting
(π(0),π(1),π(2)) = (0.10,0.09,0.09).
27Under review as submission to TMLR
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.001.021.041.061.081.101.12
1.001.021.041.061.081.101.12=0.01, ((0),(1),(2))=(0.05,0.14,0.14)
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.451
Pretraining Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Trans-Lasso
0.1 0.3 0.5 0.7 0.9
2
0.050.150.250.350.45Generalized Trans-Lasso
1.0001.0051.0101.0151.0201.025
1.001.011.021.031.041.051.061.071.08=0.1, ((0),(1),(2))=(0.05,0.14,0.14)
Figure 10: Comparisons between ratios min{ϵ∆λ=0,ϵκ=0}/ϵLO,ϵPretrain/ϵLOandϵTrans/ϵLOunder setting
(π(0),π(1),π(2)) = (0.05,0.14,0.14).
28