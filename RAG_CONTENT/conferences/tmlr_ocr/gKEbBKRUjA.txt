Published in Transactions on Machine Learning Research (11/2023)
Understanding Curriculum Learning in Policy Optimization
for Online Combinatorial Optimization
Runlong Zhou vectorzh@cs.washington.edu
University of Washington
Zelin He zelinh2@uw.edu
University of Washington
Yuandong Tian yuandong@fb.com
Facebook AI Research
Yi Wu jxwuyi@gmail.com
Tsinghua University
Simon S. Du ssdu@cs.washington.edu
University of Washington
Reviewed on OpenReview: https: // openreview. net/ forum? id= gKEbBKRUjA
Abstract
Over the recent years, reinforcement learning (RL) starts to show promising results in tack-
ling combinatorial optimization (CO) problems, in particular when coupled with curriculum
learningtofacilitatetraining. Despiteemergingempiricalevidence, theoreticalstudyonwhy
RL helps is still at its early stage. This paper presents the first systematic study on policy
optimization methods for online CO problems. We show that online CO problems can be
naturally formulated as latent Markov Decision Processes (LMDPs), and prove convergence
bounds on natural policy gradient (NPG) for solving LMDPs. Furthermore, our theory
explains the benefit of curriculum learning: it can find a strong sampling policy and reduce
the distribution shift, a critical quantity that governs the convergence rate in our theorem.
For a canonical online CO problem, the Best Choice Problem (BCP), we formally prove that
distribution shift is reduced exponentially with curriculum learning even if the curriculum
is a randomly generated BCP on a smaller scale . Our theory also shows we can simplify the
curriculum learning scheme used in prior work from multi-step to single-step. Lastly, we
provide extensive experiments on the Best Choice Problem, Online Knapsack, and AdWords
to verify our findings.
1 Introduction
In recent years, machine learning techniques have shown promising results in solving combinatorial opti-
mization (CO) problems, including traveling salesman problem (TSP, Kool et al. (2019)), maximum cut
(Khalil et al., 2017) and satisfiability problem (Selsam et al., 2019). While in the worst case some CO prob-
lems are NP-hard, in practice, the probability that we need to solve the worst-case problem instance is low
(Cappart et al., 2021). Machine learning techniques are able to find generic models which have exceptional
performance on the majority of a class of CO problems.
A significant subclass of CO problems is called online CO problems, which has gained much attention
(Grötschel et al., 2001; Huang, 2019; Garg et al., 2008). Online CO problems entail a sequential decision-
making process, which perfectly matches the nature of reinforcement learning (RL).
1Published in Transactions on Machine Learning Research (11/2023)
This paper concerns using RL to tackle online CO problems. RL is often coupled with specialized techniques
including (a particular type of) Curriculum Learning (Kong et al., 2019), human feedback and correction
(Pérez-Dattari et al. (2018), Scholten et al. (2019)), and policy aggregation (boosting, Brukhim et al. (2021)).
Practitioners use these techniques to accelerate the training speed.
While these hybrid techniques enjoy empirical success, the theoretical understanding is still limited: it is
unclear when and why they improve the performance. In this paper, we particularly focus on RL with
Curriculum Learning (Bengio et al. (2009), also named “bootstrapping” in Kong et al. (2019)): train the
agent from an easy task and gradually increase the difficulty until the target task. Interestingly, these
techniques exploit the special structures of online CO problems.
Main contributions. In this paper, we initiate the formal study on using RL to tackle online CO problems,
with a particular emphasis on understanding the specialized techniques developed in this emerging subarea.
Our contributions are summarized below.
‚Formalization. For online CO problems, we want to learn a single policy that enjoys good perfor-
mance over a distribution of problem instances. This motivates us to use Latent Markov Decision Process
(LMDP) (Kwon et al., 2021a) instead of standard MDP formulation. We give concrete examples, the Best
Choice Problem (BCP,alsoknownas the Secretary Problem ),Online Knapsack , andAdWords (Online Match-
ing and Ad Allocation , ADW), to show how LMDP models online CO problems. With this formulation, we
can systematically analyze RL algorithms.
‚Provable efficiency of policy optimization. By leveraging recent theory on Natural Policy Gradient
for standard MDP Agarwal et al. (2021), we analyze the performance of NPG for LMDP. The performance
bound is characterized by the number of iterations, the excess risk of policy evaluation, the transfer error,
and the relative condition number κthat characterizes the distribution shift between the sampling policy
and the optimal policy. We also take into account the effect of entropy regularization. To our knowledge,
this is the first performance bound of policy optimization methods on LMDP.
‚UnderstandingandsimplifyingCurriculumLearning. UsingourperformanceguaranteeonNPGfor
LMDP, we study when and why Curriculum Learning is beneficial to RL for online CO problems. Our main
finding is that the main effect of Curriculum Learning is to give a stronger sampling policy . Under certain
circumstances, Curriculum Learning reduces the relative condition number κ, improving the convergence
rate. For BCP, we provably show that Curriculum Learning can exponentially reduce κcompared with
using the naïve sampling policy. Surprisingly, this means even a randomly constructed curriculum of BCP
accelerates the training exponentially . As a direct implication, we show that the multi-step Curriculum
Learning proposed in Kong et al. (2019) can be significantly simplified into a single-step scheme . Lastly,
to obtain a complete understanding, we study the failure mode of Curriculum Learning, in a way to help
practitioners to decide whether to use Curriculum Learning based on their prior knowledge. To verify our
theories, we conduct extensive experiments on three classical online CO problems [BCP, Online Knapsack
(decision version, OKD), and ADW (decision version)] and carefully track the dependency between the
performance of the policy and κ.
2 Related Works
Combinatorial Optimization problems. CO has been a long lasting field of people’s interest. There
are a rich literature regarding CO problems such as traveling salesman problem (Flood (1956); Bellmore &
Nemhauser (1968)), maximum cut (Karp (1972); Goemans & Williamson (1995)), and satisfiability problem
(Cook (1971); Trakhtenbrot (1984)).
RL for CO. There have been rich literature studying RL for CO problems, e.g., using Pointer Network
in REINFORCE and Actor-Critic for routing problems (Nazari et al., 2018), combining Graph Attention
Network with Monte Carlo Tree Search for TSP (Drori et al., 2020) and incorporating Structure-to-Vector
Network in Deep Q-networks for maximum independent set problems (Cappart et al., 2019). Bello et al.
(2017) proposed a framework to tackle CO problems using RL and neural networks. Kool et al. (2019)
combined REINFORCE and attention technique to learn routing problems. Vesselinova et al. (2020) and
Mazyavkina et al. (2021) are taxonomic surveys of RL approaches for graph problems. Bengio et al. (2020)
2Published in Transactions on Machine Learning Research (11/2023)
summarized learning methods, algorithmic structures, objective design and discussed generalization. In
particular scaling to larger problems was mentioned as a major challenge. Compared to supervised learning,
RL not only mimics existing heuristics, but also discover novel ones that humans have not thought of, for
example chip design (Mirhoseini et al., 2021) and compiler optimization (Zhou et al., 2020b). Theoretically,
thereisalineofworkonanalyzingdata-drivenapproachtocombinatorialproblems(Balcan,2020). However,
to our knowledge, the theoretical analysis for RL-based method is still missing.
Kong et al. (2019) focused on using RL to tackle onlineCO problems, which means that the agent must
make sequential and irrevocable decisions. They encoded the input in a length-independent manner. For
example, the i-th element of a n-length sequence is encoded by the fraction i{nand other features, so that
the agent can generalize to unseen n, paving the way for Curriculum Learning. Three online CO problems
were mentioned in their paper: ADW, Online Knapsack, and BCP. Currently, Online Matching (ADW)
and Online Knapsack have only approximation algorithms (Huang et al., 2019; Albers et al., 2021). There
are also other works about RL for online CO problems. Alomrani et al. (2021) uses deep-RL for Online
Matching. Oren et al. (2021) studies Parallel Machine Job Scheduling problem (PMSP) and Capacitated
Vehicle Routing problem (CVRP), which are both online CO problems, using offline-learning and Monte
Carlo Tree Search.
LMDP. We provide the exact definition of LMDP in Section 4.1. As studied by Steimle et al. (2021), in
the general cases, optimal policies for LMDPs are history-dependent . This is different from standard MDP
cases where there always exists an optimal history-independent policy. They showed that even finding the
optimal history-independent policy is NP-hard. Kwon et al. (2021a) investigated the sample complexity
and regret bounds of LMDP in the history-independent policy class. They presented an exponential lower-
bound for a general LMDP and derived algorithms with polynomial sample complexities for cases with
special assumptions. Kwon et al. (2021b) showed that in reward-mixing MDPs, where MDPs share the same
transition model, a polynomial sample complexity is achievable without any assumption to find an optimal
history-independent policy.
Convergence rate for policy gradient methods. There is line of work on the convergence rates of policy
gradient methods for standard MDPs (Bhandari & Russo (2021), Wang et al. (2020), Liu et al. (2020), Ding
et al. (2021), Zhang et al. (2021)). For softmax tabular parameterization , NPG can obtain an Op1{Tqrate
(Agarwal et al., 2021) where Tis the number of iterations; with entropy regularization, both PG and NPG
achieves linear convergence (Mei et al., 2020; Cen et al., 2021). For log-linear policies , sample-based NPG
makes anOp1{?
Tqconvergence rate, with assumptions on ϵstat,ϵbiasandκ(Agarwal et al., 2021) (see our
Definition 4); exact NPG with entropy regularization enjoys a linear convergence rate up to ϵbias(Cayci
et al., 2021). We extend the analysis to LMDP.
Curriculum Learning. There are a rich body of literature on Curriculum Learning (Zhou et al., 2021b;a;
2020a; Ao et al., 2021; Willems et al., 2020; Graves et al., 2017). As surveyed in Bengio et al. (2009),
Curriculum Learning has been applied to training deep neural networks and non-convex optimizations and
improves the convergence in several cases. Narvekar et al. (2020) rigorously modeled curriculum as a di-
rected acyclic graph and surveyed work on curriculum design. Kong et al. (2019) proposed a bootstrapping
(Curriculum Learning) approach: gradually increase the problem size after the model works sufficiently well
on the current problem size.
3 Motivating Online CO Problems
OnlineCOproblemsareanaturalclassofproblemsthatadmitconstructionsofsmall-scaleinstances, because
the hardness of them can be characterized by the input length, and instances of different scales are similar.
This property simplifies the construction of curricula and underscores curriculum learning. We also believe
online CO problems make the use of LMDP suitable, because under a proper distribution twmu,instances
in a large portion of the probability space have similar near optimal solutions .
In this section we introduce three motivating online CO problems. We are interested in these problems
because they have all been extensively studied. Furthermore, they were studied in Kong et al. (2019), the
3Published in Transactions on Machine Learning Research (11/2023)
paper that motivates our work. They also have real-world applications, e.g., auction design (Babaioff et al.,
2007) and advertisement targeting (Mehta et al., 2007).
3.1 The Best Choice Problem (BCP)1
In BCP, the goal is to maximize the probability of choosing the maximum among ndifferent numbers, where
nis known. They arrive sequentially and when the i-th number shows up, the decision-maker observes the
relative ranking Xiamong the first inumbers, which means being the Xith-best so far. A decision that
whether to accept or reject the i-th number must be made immediately when it comes, and such decisions
cannot be revoked . Once one number is accepted, the game ends immediately.
The ordering of the numbers is unknown. There are in total n!permutations, and an instance of BCP is
drawn from an unknown distribution over these permutations. In the classical BCP, each permutation is
sampled with equal probability. The optimal solution for the classical BCP is the well-known 1{e-threshold
strategy: reject all the first tn{eunumbers, then accept the first one which is the best so-far. In this paper,
we also study some different distributions.
3.2 Online Knapsack (decision version, OKD)
In Online Knapsack problems the decision-maker observes n(which is known) items arriving sequentially,
each with value viand sizesirevealed upon arrival. A decision to either accept or reject the i-th item must
be made immediately when it arrives, and such decisions cannot be revoked . At any time the accepted items
should have their total size no larger than a known budget B.
The goal of standard Online Knapsack is to maximize the total value of accepted items. In this paper, we
study its decision version, whose goal is to maximize the probability of total value reaching a known target
V.
We assume that all values and sizes are sampled independently from two fixed distributions, namely
v1,v2,...,vni.i.d.„Fvands1,s2,...,sni.i.d.„Fs. In Kong et al. (2019) the experiments were carried out
withFv“Fs“Unifr0,1s, and we also study other distributions.
Remark 1. A challenge in OKD is the sparse reward: the only signal is reward 1when the total value of
accepted items first exceeds V(see the detailed formulation in Appendix C.2), unlike in Online Knapsack the
reward ofviis given instantly after the i-th item is successfully accepted. This makes random exploration
hardly get reward signals, necessitating Curriculum Learning.
3.3 AdWords (decision version, ADW)
In ADW, there are nadvertisers each with budget 1andmad slots. Each ad slot jarrives sequentially
along with a vector pv1,j,v2,j,...,vn,jqwherevi,jis the value that advertiser iwants to pay for ad slot j.
Once an ad slot arrives, it must be irrevocably allocated to an advertiser or not allocated at all. If ad slot j
is allocated to advertiser iand the remaining budget of advertiser iis not less than vi,j, the total revenue
increases by vi,jwhile advertiser i’s budget decreases by vi,j.
We assume that for any advertiser i,vi,1,vi,2,...,vi,mi.i.d.„Fi. Kong et al. (2019) studied a very special case
called online b-matching where Fiis a Bernoulli distribution. We study different distributions.
The objective of the standard ADW is to maximize the total revenue. For a similar reason as in OKD, we set
a known target Vfor the decision version. The goal of ADW is to maximize the probability of total revenue
reachingV.
1We follow the statement in Kong et al. (2019) that BCP (secretary problem) is a CO problem. It is categorized as an
optimal stopping problem.
4Published in Transactions on Machine Learning Research (11/2023)
4 Problem Setup
In this section, we first introduce LMDP and why it naturally formulates online CO problems. Then we list
necessary components required by Natural Policy Gradient for LMDP (Algorithm 1).
Notations. For any positive integer n, we denoterns:“t1,2,...,nu. For any vector xPRn, we denote
xb:“xbx“xxJas the self-outer-product of x. Further for any yPRm, we denotepx˝yqpi,jq:“xpiqypjq.
4.1 Latent Markov Decision Process
Tackling an online CO problem entails handling a family of problem instances, and each instance can be
modeled as a Markov Decision Process. For online CO problems, we want to find one algorithm that works
for a family of problem instances and performs well on average over an (unknown) distribution over this
family. To this end, we adopt the concept of Latent MDP which naturally models online CO problems.
Latent MDP (Kwon et al., 2021a) is a collection of MDPs M“tM1,M2,...,MMu. All the MDPs share
state set S, action set Aand horizon H. Each MDP Mm“pS,A,H,νm,Pm,rmqhas its own initial state
distribution νmP∆pSq, transition Pm:SˆAÑ∆pSqand reward rm:SˆAÑr0,1s, where ∆pSqis the
probability simplex over S. Letw1,w2,...,wMbe the mixing weights of MDPs such that wmą0for anym
andřM
m“1wm“1. At the start of every episode, one MDP MmPMis randomly chosen with probability
wm.
Due to the time and space complexities of finding the optimal history-dependent policies, we stay in line
with Kong et al. (2019) and care only about finding the optimal history-independent policy. Let Π“tπ:
SÑ∆pAqudenote the class of all the history-independent policies.
Log-linear policy. Letϕ:SˆAÑRdbe a feature mapping function where ddenotes the dimension of
feature space. Assume that }ϕps,aq}2ďB. A log-linear policy is of the form:
πθpa|sq“exppθJϕps,aqqř
a1PAexppθJϕps,a1qq,whereθPRd.
Remark 2. Log-linear parameterization is a generalization of softmax tabular parameterization by setting
d“|S||A|andϕps,aq“One-hotps,aq. They are “scalable”: if ϕextracts important features from different
SˆAs with a fixed dimension d!|S||A|, then a single πθcan generalize.
Value function, Q-function and advantage function. The expected reward of executing πonMm
is defined via value functions. We incorporate entropy regularization for completeness because prior works
(especially empirical works) used it to facilitate training. Due to space limit, we defer all the entropy
regularized notations, algorithm and theorem to Appendix A. We define the value function:
Vπ
m,hpsq:“EMm,π«h´1ÿ
t“0rmpst,atqˇˇˇˇˇs0“sff
,
where the expectation is with respect to the randomness of trajectory induced by πinMm. DenoteVπ:“řM
m“1wmř
s0PSνmps0qVπ
m,Hps0q, then we need to find π‹“arg maxπPΠVπ. DenoteV‹:“Vπ‹.
The Q-function can be defined in a similar manner:
Qπ
m,hps,aq:“EMm,π«h´1ÿ
t“0rmpst,atqˇˇˇˇˇps0,a0q“ps,aqff
,
and the advantage function is defined as Aπ
m,hps,aq:“Qπ
m,hps,aq´Vπ
m,hpsq.
Modeling BCP. For BCP, each instance is a permutation of length n, and in each round an instance is
drawn from an unknown distribution over all permutations. In the i-th step for iPrns, the state encodes
thei-th number and its relative ranking so far. The transition is deterministic according to the problem
5Published in Transactions on Machine Learning Research (11/2023)
definition. A reward of 1is givenif and only if the maximum is accepted. We model the distribution as
follows: for the i-th number, it has probability Pito be the best so-far and is independent of other i1. Hence,
the weight of each instance is simply the product of the probabilities on each position. The classical BCP
satisfiesPi“1{i.
Modeling OKD. For OKD, each instance is a sequence of items with values and sizes drawn from unknown
distributions FvandFs. In thei-th step for iPrns, the state encodes the information of i-th item’s value
and size, the remaining budget, and the remaining target value to fulfill. The transition is also deterministic
according to the problem definition, and a reward of 1is givenif and only if the agent obtains the target
value for the first time. Fv“Fs“Unifr0,1sin Kong et al. (2019).
Modeling ADW. For ADW, each instance is a nˆmmatrixpvi,jqpi,jqPrnsˆrms, with each row isubject to
a distribution Fi. In thej-th step for jPrms, the state encodes the value vector pv1,j,v2,j,...,vn,jq, the
remaining budget vector pB1,B2,...,Bnq, and the remaining target revenue to fulfill. The transition is also
deterministic according to the problem definition, and a reward of 1is givenif and only if the agent obtains
the target revenue for the first time.
4.2 Algorithm components
In this subsection we will introduce some necessary notations used by our main algorithm.
Definition 1 (Visitation Distribution) .The state visitation distribution and state-action visitation distri-
bution at step hě0with respect to πinMmare defined as
dπ
m,hpsq:“PMm,πpsh“sq,
dπ
m,hps,aq:“PMm,πpsh“s,ah“aq.
We will encounter a grafted distribution rdπ
m,hps,aq“dπ
m,hpsq˝UnifApaqwhich in general is not the state-
action visitation distribution with respect to any policy. However, it can be attained by first acting under π
forhsteps to get states then sample actions from the uniform distribution Unif A. This distribution will be
useful when we apply a variant of NPG, where the sampling policy is fixed.
Denoted♣
m,h:“dπ♣
m,handd♣as short fortd♣
m,hu1ďmďM,0ďhďH´1, here ♣can be any symbol.
We also need the following definitions for NPG, which are different from the standard versions for discounted
MDP because weights twmumust be incorporated in the definitions to deal with LMDP. In the following
definitions, let vbe the collection of any distribution, which will be instantiated by d‹,dt, etc. in the
remaining sections.
Definition 2 (Compatible Function Approximation Loss) .Letgbe the parameter update weight, then NPG
is related to finding the minimizer for the following function:
Lpg;θ,vq:“
Mÿ
m“1wmHÿ
h“1Es,a„vm,H´h„´
Aπθ
m,hps,aq´gJ∇θlnπθpa|sq¯2ȷ
.
Definition 3 (Generic Fisher Information Matrix) .
Σθ
v:“Mÿ
m“1wmHÿ
h“1Es,a„vm,H´h“
p∇θlnπθpa|sqqb‰
.
Particularly, denote Fpθq“Σθ
dθas the Fisher information matrix induced by πθ.
5 Learning Procedure
In this section we introduce the algorithms: NPG supporting any customized sampler, and our proposed
Curriculum Learning framework.
6Published in Transactions on Machine Learning Research (11/2023)
Natural Policy Gradient. The learning procedure generates a series of parameters and policies. Starting
fromθ0, the algorithm updates the parameter by setting θt`1“θt`ηgt,whereηis a predefined constant
learning rate, and gtis the update weight. Denote πt:“πθt,Vt:“VπtandAt
m,h:“Aπt
m,hfor convenience.
We adopt NPG (Kakade, 2002) because it is efficient in training parameterized policies and admits clean
theoretical analysis. NPG satisfies gtParg mingLpg;θt,dθtq(see Appendix D.1 for explanation). When we
only have samples, we use the approximate version of NPG: gt«arg mingPGLpg;θt,dθtq, where G“ tx:
}x}2ďGufor some hyper-parameter G.
We also introduce a variant of NPG: instead of sampling from dθtusing the current policy πt, we sample
fromrdπsusing afixedsampling policy πs. The update rule is gt«arg mingPGLpg;θt,rdπsq. This version
makes a closed-form analysis for BCP possible.
The main algorithm is shown in Algorithm 1. It admits two types of training: ①Ifπs“None, it calls
Algorithm 4 (deferred to Appendix A) to sample s,a„dθt;②Ifπs‰None, it then calls Algorithm 4 to
samples,a„rdπs. Algorithm 4 also returns an unbiased estimation of Aπt
H´hps,aq.
In both cases, we denote dtas the sampling distribution and Σtas the induced Fisher Information Matrix
used in step t, i.e.dt:“dθt,Σt:“Fpθtqifπs“None;dt:“rdπs,Σt:“Σθt
rdπsotherwise. The update rule
can be written in a unified way as gt«arg mingPGLpg;θt,dtq.This is equivalent to solving a constrained
quadratic optimization and we can use existing solvers.
Remark 3. Algorithm 1 is different from Algorithm4 of Agarwal et al. (2021) in that we use a “batched”
update while they used successive Projected Gradient Descents (PGD). This is an important implementation
technique to speed up training in our experiments.
Curriculum Learning. We use Curriculum Learning to facilitate training. Algorithm 2 is our proposed
training framework, which first constructs an easy environment E1and trains a (near-)optimal policy πsof it.
The design of E1is problem-dependent. For the problems described in this paper (BCP, OKD, and ADW)
as well as any similar problems (online load balancing, online set cover, etc.), we can use n, the sequence
length of online decision-making, to represent the difficulty. For these problems, we construct E1to be the
environment with nsmaller than that of E. For other problems, we first find the hyperparameters controlling
the difficulty of the problem, e.g., the sequence length, the action space size, the number of interaction steps,
then reduce these hyperparameters to construct a smaller scale and simpler problem.
In the target environment E, we either use πsto sample data while training a new policy from scratch, or
simply continue training πs. To be specific and provide clarity for the results in Section 7, we name a few
training modes (without regularization) here, and the rest are in Table 1 in Appendix C.
curl, the standard Curriculum Learning, runs Algorithm 2 with samp“pi_t;fix_samp_curl stands for
the fixed sampler Curriculum Learning, running Algorithm 2 with samp“pi_s.directmeans directly
learning in Ewithout curriculum, i.e., running Algorithm 1 with πs“None; naive_samp also directly learns
inE, while using πs“naïve random policy to sample data in Algorithm 1.
6 Performance Analysis
Our analysis contains two important components, namely the sub-optimality gap guarantee of the NPG
we proposed, and the efficacy guarantee of Curriculum Learning on BCP. The first component can also be
extended to history-dependent policies with features being the tensor products of features from each time
step (exponentially large).
6.1 Natural Policy Gradient for Latent MDP
Letg‹
tParg mingPGLpg;θt,dtqdenote the true minimizer. We have the following definitions:
Definition 4. Define for 0ďtďT:
‚(Excess risk) ϵstat:“maxtErLpgt;θt,dtq´Lpg‹
t;θt,dtqs;
‚(Transfer error) ϵbias:“maxtErLpg‹
t;θt,d‹qs;
7Published in Transactions on Machine Learning Research (11/2023)
Algorithm 1 NPG(Full version: Algorithm 3)
1:Input: Environment E; learning rate η; episode number T; batch size N; initialization θ0; samplerπs; optimiza-
tion domain G.
2:fortÐ0,1,...,T´1do
3: For0ďnďN´1and0ďhďH´1, samplepapnq
h,spnq
hqand estimate pApnq
H´husing Algorithm 4.
4: Calculate:
pFtÐN´1ÿ
n“0H´1ÿ
h“0p∇θlnπθtpapnq
h|spnq
hqqb,
p∇tÐN´1ÿ
n“0H´1ÿ
h“0pApnq
H´h∇θlnπθtpapnq
h|spnq
hq.
5: Call any solver to get pgtÐarg mingPGgJpFtg´2gJp∇t.
6: Updateθt`1Ðθt`ηpgt.
7:end for
8:Return:θT.
Algorithm 2 Curriculum learning framework.
1:Input: Environment E; learning rate η; episode number T; batch size N; sampler type sampPtpi_s, pi_tu;
optimization domain G.
2:Construct an environment E1with a task easier than E. This environment should have optimal policy similar to
that ofE.
3:θsÐNPG(E1,η,T,N, 0d,None,G) (Algorithm 1).
4:ifsamp“pi_sthen
5:θTÐNPG(E,η,T,N, 0d,πs,G).
6:else
7:θTÐNPG(E,η,T,N,θ s,None,G).
8:end if
9:Return:θT.
‚(Relative condition number) κ:“maxtE„
supxPRdxJΣθt
d‹x
xJΣtxȷ
. Note that term inside the expectation is a
random quantity as θtis random.
The expectation is with respect to the randomness in the sequence of weights g0,g1,...,gT.
AllthequantitiesarecommonlyusedinliteraturementionedinSection2. ϵstatisduetothattheminimizer gt
from samples may not minimize the population loss L.ϵbiasquantifies the approximation error due to feature
maps.κcharacterizes the distribution mismatch between dtandd‹and isa key quantity in Curriculum
Learning and will be studied in more details in the following sections.
Our main result is based on a fitting error which depicts the closeness between π‹and any policy π.
Definition 5 (Fitting Error) .Suppose the update rule is θt`1“θt`ηgt, define
errt:“Mÿ
m“1wmHÿ
h“1Eps,aq„d‹
m,H´h”
At
m,hps,aq´gJ
t∇θlnπtpa|sqı
.
Theorem 6 shows the convergence rate of Algorithm 1, and its proof is deferred to Appendix A.3.
Theorem 6. With Definitions 4, 5 and 9, Algorithm 1 enjoys the following performance bound:
E„
min
0ďtďTV‹´Vtȷ
ďΦpπ0q
ηT`ηB2G2
2`1
TTÿ
t“0Ererrts
ďΦpπ0q
ηT`ηB2G2
2`a
Hϵbias`a
Hκϵ stat,
where Φpπ0qis the Lyapunov potential function which is only relevant to the initialization.
8Published in Transactions on Machine Learning Research (11/2023)
Remark 4. ①For the results of sample-based NPG with entropy regularization for LMDP, please see
Appendix A. ②Takingη“Θp1{?
Tqgives anOp1{?
Tqrate, matching the result in Agarwal et al. (2021).
③ϵstatcan be reduced using a larger batch size N(Lemma 20) that ϵstat“rOp1{?
Nq.④If somedt
(especially the initialization d0) is far away from d‹,κmay be extremely large (Section 6.2 as an example).
If we can find a policy whose κis small with a single curriculum , we do not need the multi-step curriculum
learning procedure used in Kong et al. (2019).
6.2 Curriculum learning for BCP
ForBCP,thereexistsathresholdpolicythatisoptimal(Beckmann,1990). Supposethethresholdis pPp0,1q,
then the policy is: accept the i-th number if and only if i{nąpandXi“1. For the classical BCP where
all then!instances have equal probability, the optimal threshold is 1{e.
To show that curriculum learning makes the training converge faster, Theorem 6 gives a direct hint: cur-
riculum learning produces a good sampler leading to much smaller κthan that of a naïve random sampler.
Here we focus on the cases where samp“pi_sbecause the sampler is fixed, while when samp“pi_tit is
impossible to analyze a dynamic procedure. We show Theorem 7 to characterize κin BCP. Its full statement
and proof is deferred to Appendix B.
Theorem 7. Assume that each number is independent of others and the i-th number has a probability Piof
being the maximum so far (Section 4.1). Assume the optimal policy is a p-threshold policy and the sampling
policy is aq-threshold policy. There exists a policy parameterization such that:
κcurl“Θ˜#śtnpu
j“tnqu`11
1´Pj, qďp,
1, q ąp,¸
,
κnaïve“Θ¨
˝2tnpumax$
&
%1,max
iětnpu`2i´1ź
j“tnpu`12p1´Pjq,
.
-˛
‚, (1)
whereκcurlandκnaïveareκof the sampling policy and the naïve random policy, respectively.
To understand how curriculum learning influences κ, we apply Theorem 7 to three concrete cases. They
show that, when the state distribution induced by the optimal policy in the small problem is similar to that
in the original large problem, then a single-step curriculum suffices (cf. ④of Remark 4).
The classical case: an exponential improvement. We study the classical BCP first, where all the n!
permutationsaresampledwithequalprobability. Theprobabilityseriesforthiscaseis Pi“1{i. Substituting
them into Equation (1) directly gives:
κcurl“#
tn{eu
tnqu, qď1
e,
1, qą1
e,κnaïve“2n´1tn{eu
n´1.
Except for the corner case where qă1{n, we have that κcurl“Opnqwhileκnaïve“Ωp2nq. Notice that any
distribution with Piď1{ileads to an exponential improvement.
A more general case. Now we try to loosen the condition where Piď1{i. Let us consider the case where
Piď1{2foriě2(by definition P1is always equal to 1). Equation (1) now becomes:
κcurlď"
2tnpu´tnqu, qďp,
1, qąp,κnaïveě2tnpu.
Clearly,κcurlďκnaïvealways holds. When qis close top, the difference is exponential in tnqu.
Failure mode of Curriculum Learning. Lastly we show further relaxing the assumption on Pileads to
failure cases. The extreme case is that all Pi“1, i.e., the maximum number always comes as the last one.
Supposeqă1´1{n, thendπqp1q“0. Henceκcurl“8, larger than κnaïve“2n´1. From Equation (1),
κnaïveď2n´1. Similar as Section3 of Beckmann (1990), the optimal threshold psatisfies:
nÿ
i“tnpu`2Pi
1´Piď1ănÿ
i“tnpu`1Pi
1´Pi.
9Published in Transactions on Machine Learning Research (11/2023)
Figure 1: One experiment of BCP. The x-axis is the number of trajectories, i.e., number of episodes ˆ
horizonˆbatch size. Dashed lines represent only final phase training and solid lines represent
Curriculum Learning. The shadowed area shows the 95%confidence interval for the expectation. The
explanation for different modes can be found in Section 5. The reference policy is the optimal threshold
policy.
Figure 2: One experiment of OKD. Legend description is the same as that of Figure 1. The reference policy
is the bang-per-buck algorithm for Online Knapsack (Section3.1 of Kong et al. (2019)).
So lettingPną1{2results inpP r1´1{n,1q. Further, if qă1´1{nandPją1´2´n
n´tnqu´1for any
tnqu`1ďjďn´1, then from Equation (1), κcurlą2nąκnaïve. This means that Curriculum Learning
can always be manipulated adversarially. Sometimes there is hardly any reasonable curriculum.
Remark 5. Here we only provide theoretical explanations for BCP when samp“pi_s, becauseκis highly
problem-dependent, and the analytical forms for κis tractable when the sampler is fixed. For samp“pi_t
and other CO problems such as OKD, however, we do not have analytical forms, so we resort to empirical
studies (Section 7).
7 Experiments
The experiments’ formulations are modified from Kong et al. (2019). Due to page limit, more formulation de-
tailsandresultsarepresentedinAppendixC,andcodecanbefoundat https://github.com/zhourunlong/
RL-for-Combinatorial-Optimization . In Curriculum Learning the entire training process splits into at
most two phases. We call the training on curriculum (small scale instances) “warm-up phase” and the train-
ing on large scale instances “final phase”. If the training is directly on large scale instances, we still call it
“final phase” for convenience. For each problem, we run multiple experiments using different distributions
of instances. Each experiment contains multiple training methods, e.g., direct training, curriculum learning,
etc. To highlight the effect of curriculum learning, we omit the results regarding regularization, and they
can be found in supplementary files. All the trainings in the same experiment have the same distributions
over LMDPs for final phase and warm-up phase (if any), respectively.
10Published in Transactions on Machine Learning Research (11/2023)
Figure 3: One experiment of ADW. Legend description is the same as that of Figure 1. The reference
policy is obtained by running a curlprocedure. The lnκand avgperrtqcurves are then plotted with the
above reference policy hard-coded into the environment.
The Best Choice Problem (BCP). We show one of the four experiments in Figure 1. Aside from
reward and lnκ, we plot the weighted average of err taccording to Theorem 6: avg perrtq “řt
i“0erri{T.
All the instance distributions are generated from parameterized series tPnuwith fixed random seeds, which
guarantees reproducibility and comparability. Aside from the fact that the curriculum is a smaller BCP ,
there isno other explicit relationship between the curriculum and the target environment , so the curriculum
can be viewed as random and independent . The experiments clearly demonstrate that curriculum learning
can boost the performance by a large margin and curriculum learning indeed dramatically reduces κ, even
the curriculum is randomly generated.
Online Knapsack (decision version, OKD). We show one of the three experiments in Figure 2. lnκ
and avg(err t) are with respect to the reference policy, a bang-per-buck algorithm, which is not the optimal
policy. Thus, they are only for reference. The curriculum generation is also parameterized, random and
independent of the target environment. The experiments again demonstrate the effectiveness of curriculum
learning and curriculum learning indeed dramatically reduces κ.
AdWords (decision version, ADW). We show one of the two experiments in Figure 3. The reference
policy is obtained by using curriculum learning and training until nearly convergence. The curriculum
generation is also parameterized, random and independent of the target environment. The experiments
again demonstrate the effectiveness of curriculum learning.
8 Conclusion
We showed online CO problems could be naturally formulated as LMDPs, and we analyzed the convergence
rate of NPG for LMDPs. Our theory shows the main benefit of curriculum learning is finding a stronger
sampling strategy, especially for classical BCP any curriculum exponentially improves the learning rate. Our
empirical results on BCP, OKD, and ADW also corroborated our findings. Our work is the first attempt to
systematically study techniques devoted to using RL to tackle online CO problems, which we believe is a
fruitful direction worth further investigations.
Acknowledgement
SSDacknowledgesthesupportofNSFIIS2110170, NSFDMS2134106, NSFCCF2212261, NSFIIS2143493,
NSF CCF 2019844, NSF IIS 2229881.
References
Alekh Agarwal, Sham M. Kakade, J. Lee, and Gaurav Mahajan. On the theory of policy gradient methods:
Optimality, approximation, and distribution shift. J. Mach. Learn. Res. , 22:98:1–98:76, 2021.
11Published in Transactions on Machine Learning Research (11/2023)
Susanne Albers, Arindam Khan, and Leon Ladewig. Improved online algorithms for knapsack and gap in
the random order model. Algorithmica , 83:1750 – 1785, 2021.
Mohammad Ali Alomrani, Reza Moravej, and Elias B Khalil. Deep policies for online bipartite matching:
A reinforcement learning approach. arXiv preprint arXiv:2109.10380 , 2021.
Shuang Ao, Tianyi Zhou, Guodong Long, Qinghua Lu, Liming Zhu, and Jing Jiang. CO-PILOT: COl-
laborative planning and reinforcement learning on sub-task curriculum. In A. Beygelzimer, Y. Dauphin,
P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021.
URL https://openreview.net/forum?id=uz_2t6VZby .
Moshe Babaioff, Nicole Immorlica, David Kempe, and Robert D. Kleinberg. A knapsack secretary problem
with applications. In APPROX-RANDOM , 2007.
Maria-Florina Balcan. Data-driven algorithm design. arXiv preprint arXiv:2011.07177 , 2020.
M.J. Beckmann. Dynamic programming and the secretary problem. Computers & Mathematics with Appli-
cations, 19(11):25–28, 1990. ISSN 0898-1221. doi: https://doi.org/10.1016/0898-1221(90)90145-A. URL
https://www.sciencedirect.com/science/article/pii/089812219090145A .
M. Bellmore and G. L. Nemhauser. The traveling salesman problem: A survey. Operations Research , 16(3):
538–558, 1968. ISSN 0030364X, 15265463. URL http://www.jstor.org/stable/168581 .
Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial opti-
mization with reinforcement learning, 2017.
YoshuaBengio, JérômeLouradour, RonanCollobert, andJasonWeston. Curriculumlearning. In Proceedings
of the 26th Annual International Conference on Machine Learning , ICML ’09, pp. 41–48, New York, NY,
USA, 2009. Association for Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553380.
URL https://doi.org/10.1145/1553374.1553380 .
Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a
methodological tour d’horizon. European Journal of Operational Research , 290, 08 2020. doi: 10.1016/j.
ejor.2020.07.063.
Jalaj Bhandari and Daniel Russo. On the linear convergence of policy gradient methods for finite mdps.
In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on
Artificial Intelligence and Statistics , volume 130 of Proceedings of Machine Learning Research , pp. 2386–
2394. PMLR, 13–15 Apr 2021. URL https://proceedings.mlr.press/v130/bhandari21a.html .
Nataly Brukhim, Elad Hazan, and Karan Singh. A boosting approach to reinforcement learning, 2021.
Quentin Cappart, Emmanuel Goutierre, David Bergman, and Louis-Martin Rousseau. Improving optimiza-
tion bounds using machine learning: Decision diagrams meet deep reinforcement learning. Proceedings
of the AAAI Conference on Artificial Intelligence , 33(01):1443–1451, Jul. 2019. doi: 10.1609/aaai.v33i01.
33011443. URL https://ojs.aaai.org/index.php/AAAI/article/view/3956 .
Quentin Cappart, Didier Chételat, Elias B. Khalil, Andrea Lodi, Christopher Morris, and Petar Veličković.
Combinatorial optimization and reasoning with graph neural networks. In Zhi-Hua Zhou (ed.), Proceedings
of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21 , pp. 4348–4355. Inter-
national Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/595.
URL https://doi.org/10.24963/ijcai.2021/595 . Survey Track.
Semih Cayci, Niao He, and R. Srikant. Linear convergence of entropy-regularized natural policy gradient
with linear function approximation, 2021.
ShicongCen, ChenCheng, YuxinChen, YutingWei, andYuejieChi. Fastglobalconvergenceofnaturalpolicy
gradient methods with entropy regularization. Operations Research , 12 2021. doi: 10.1287/opre.2021.2151.
12Published in Transactions on Machine Learning Research (11/2023)
Stephen A. Cook. The complexity of theorem-proving procedures. In Proceedings of the Third Annual ACM
Symposium on Theory of Computing , STOC ’71, pp. 151–158, New York, NY, USA, 1971. Association for
Computing Machinery. ISBN 9781450374644. doi: 10.1145/800157.805047. URL https://doi.org/10.
1145/800157.805047 .
Yuhao Ding, Junzi Zhang, and Javad Lavaei. On the global convergence of momentum-based policy gradient,
2021.
Iddo Drori, Anant Kharkar, William R. Sickinger, Brandon Kates, Qiang Ma, Suwen Ge, Eden Dolev,
Brenda L Dietrich, David P. Williamson, and Madeleine Udell. Learning to solve combinatorial optimiza-
tion problems on real-world graphs in linear time. 2020 19th IEEE International Conference on Machine
Learning and Applications (ICMLA) , pp. 19–24, 2020.
Merrill M. Flood. The traveling-salesman problem. Operations Research , 4(1):61–75, 1956. ISSN 0030364X,
15265463. URL http://www.jstor.org/stable/167517 .
Naveen Garg, Anupam Gupta, Stefano Leonardi, and Piotr Sankowski. Stochastic analyses for online com-
binatorial optimization problems. 2008.
Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and
satisfiability problems using semidefinite programming. J. ACM, 42(6):1115–1145, nov 1995. ISSN 0004-
5411. doi: 10.1145/227683.227684. URL https://doi.org/10.1145/227683.227684 .
Alex Graves, Marc G. Bellemare, Jacob Menick, Rémi Munos, and Koray Kavukcuoglu. Automated cur-
riculum learning for neural networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research ,
pp. 1311–1320. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/graves17a.html .
Martin Grötschel, Sven O Krumke, Jörg Rambau, Thomas Winter, and Uwe T Zimmermann. Combinatorial
online optimization in real time. Online optimization of large scale systems , pp. 679–704, 2001.
Zhiyi Huang. Online combinatorial optimization problems with non-linear objectives. In Nonlinear Combi-
natorial Optimization , pp. 179–205. Springer, 2019.
Zhiyi Huang, Peng Binghui, Zhihao Tang, Runzhou Tao, Xiaowei Wu, and Yuhao Zhang. Tight Competitive
Ratios of Classic Matching Algorithms in the Fully Online Model , pp. 2875–2886. 01 2019. ISBN 978-1-
61197-548-2. doi: 10.1137/1.9781611975482.178.
Sham M Kakade. A natural policy gradient. In T. Dietterich, S. Becker, and Z. Ghahramani (eds.), Advances
in Neural Information Processing Systems , volume 14. MIT Press, 2002. URL https://proceedings.
neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf .
Richard Karp. Reducibility among combinatorial problems. volume 40, pp. 85–103, 01 1972. ISBN 978-3-
540-68274-5. doi: 10.1007/978-3-540-68279-0_8.
Elias Boutros Khalil, Hanjun Dai, Yuyu Zhang, Bistra N. Dilkina, and Le Song. Learning combinatorial
optimization algorithms over graphs. In NIPS, 2017.
Weiwei Kong, Christopher Liaw, Aranyak Mehta, and D. Sivakumar. A new dog learns old tricks: Rl finds
classic optimization algorithms. In ICLR, 2019.
Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In ICLR, 2019.
Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. Rl for latent mdps: Re-
gret guarantees and a lower bound. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and
J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34, pp. 24523–
24534. Curran Associates, Inc., 2021a. URL https://proceedings.neurips.cc/paper/2021/file/
cd755a6c6b699f3262bcc2aa46ab507e-Paper.pdf .
13Published in Transactions on Machine Learning Research (11/2023)
Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. Reinforcement learning in
reward-mixing mdps. In NeurIPS , 2021b.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced)
policy gradient and natural policy gradient methods. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp.
7624–7636. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
56577889b3c1cd083b6d7b32d32f99d5-Paper.pdf .
Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for
combinatorial optimization: A survey. Computers & Operations Research , 134:105400, 05 2021. doi:
10.1016/j.cor.2021.105400.
Aranyak Mehta, Amin Saberi, Umesh Vazirani, and Vijay Vazirani. Adwords and generalized online match-
ing.Journal of the ACM (JACM) , 54(5):22–es, 2007.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates
of softmax policy gradient methods. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research ,
pp. 6820–6829. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/mei20b.html .
Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim M. Songhori, Shen Wang,
Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, Jiwoo Pak, Andy Tong, Kavya Srinivasa,
Will Hang, Emre Tuncer, Quoc V. Le, James Laudon, Richard Ho, Roger Carpenter, and Jeff Dean. A
graph placement methodology for fast chip design. Nature, 594 7862:207–212, 2021.
Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone. Cur-
riculum learning for reinforcement learning domains: A framework and survey. J. Mach. Learn. Res. , 21:
181:1–181:50, 2020.
M. Nazari, Afshin Oroojlooy, Lawrence V. Snyder, and Martin Takác. Reinforcement learning for solving
the vehicle routing problem. In NeurIPS , 2018.
Joel Oren, Chana Ross, Maksym Lefarov, Felix Richter, Ayal Taitler, Zohar Feldman, Christian Daniel,
and Dotan Di Castro. Solo: Search online, learn offline for combinatorial optimization problems. ArXiv,
abs/2104.01646, 2021.
Rodrigo Pérez-Dattari, Carlos Celemin, Javier Ruiz del Solar, and Jens Kober. Interactive learning with
corrective feedback for policies based on deep neural networks. In ISER, 2018.
Jan Scholten, Daan Wout, Carlos Celemin, and Jens Kober. Deep reinforcement learning with feedback-
based exploration. 2019 IEEE 58th Conference on Decision and Control (CDC) , Dec 2019. doi: 10.1109/
cdc40024.2019.9029503. URL http://dx.doi.org/10.1109/CDC40024.2019.9029503 .
Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. Learn-
ing a SAT solver from single-bit supervision. In International Conference on Learning Representations ,
2019. URL https://openreview.net/forum?id=HJMC_iA5tm .
Lauren N. Steimle, David L. Kaufman, and Brian T. Denton. Multi-model markov decision processes. IISE
Transactions , 53(10):1124–1139, 2021. doi: 10.1080/24725854.2021.1895454. URL https://doi.org/10.
1080/24725854.2021.1895454 .
B.A. Trakhtenbrot. A survey of russian approaches to perebor (brute-force searches) algorithms. Annals of
the History of Computing , 6(4):384–400, 1984. doi: 10.1109/MAHC.1984.10036.
NataliaVesselinova, RebeccaSteinert, DanielF.Perez-Ramirez, andMagnusBoman. Learningcombinatorial
optimization on graphs: A survey with applications to networking. IEEE Access , 8:120388–120416, 2020.
14Published in Transactions on Machine Learning Research (11/2023)
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global opti-
mality and rates of convergence. In International Conference on Learning Representations , 2020. URL
https://openreview.net/forum?id=BJgQfkSYDS .
Lucas Willems, Salem Lahlou, and Yoshua Bengio. Mastering rate based curriculum learning, 2020.
Junzi Zhang, Jongho Kim, Brendan O’Donoghue, and Stephen Boyd. Sample efficient reinforcement learning
with reinforce. In AAAI, 2021.
Tianyi Zhou, Shengjie Wang, and Jeffrey Bilmes. Curriculum learning by dynamic instance hardness. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Infor-
mation Processing Systems , volume 33, pp. 8602–8613. Curran Associates, Inc., 2020a. URL https:
//proceedings.neurips.cc/paper/2020/file/62000dee5a05a6a71de3a6127a68778a-Paper.pdf .
Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Curriculum learning by optimizing learning dynamics. In
Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Ar-
tificial Intelligence and Statistics , volume 130 of Proceedings of Machine Learning Research , pp. 433–441.
PMLR, 13–15 Apr 2021a. URL https://proceedings.mlr.press/v130/zhou21a.html .
Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Robust curriculum learning: from clean label detection
to noisy label self-correction. In International Conference on Learning Representations , 2021b. URL
https://openreview.net/forum?id=lmTWnm3coJJ .
Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma, Qiumin Xu, Hanxiao Liu,
Phitchaya Phothilimtha, Shen Wang, Anna Goldie, Azalia Mirhoseini, and James Laudon. Trans-
ferable graph optimizers for ml compilers. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 13844–
13855. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.cc/paper/2020/file/
9f29450d2eb58feb555078bdefe28aa5-Paper.pdf .
A Full Results of the Main Algorithm and Theorem for Entropy Regularization
A.1 Notations and Definitions
Entropy regularized value function, Q-function and advantage function. We incorporate entropy
regularization for completeness because prior works (especially empirical works) used it to facilitate training.
We define the value function in a unified way: Vπ,λ
m,hpsqis defined as the sum of future λ-regularized rewards
starting from sand executing πforhsteps inMm, i.e.,
Vπ,λ
m,hpsq:“EMm,π«h´1ÿ
t“0rπ,λ
mpst,atqˇˇˇˇˇs0“sff
,
whererπ,λ
mps,aq:“rmps,aq`λln1
πpa|sq, and the expectation is with respect to the randomness of trajectory
induced by πinMm. Clearly,Vπ
m,hpsq“Vπ,0
m,hpsq.
For any Mm,π,h, with Hpπp¨|sqq:“ř
aPAπpa|sqln1
πpa|sqPr0,ln|A|swe define
Hπ
m,hpsq:“EMm,π«h´1ÿ
t“0Hpπp¨|stqqˇˇˇˇˇs0“sff
.
In fact,Vπ,λ
m,hpsq“Vπ
m,hpsq`λHπ
m,hpsq.
DenoteVπ,λ:“řM
m“1wmř
s0PSνmps0qVπ,λ
m,Hps0qthenVπ“Vπ,0. The original goal is to find π‹“
arg maxπPΠVπ. Under regularization, we seek for π‹
λ“arg maxπPΠVπ,λinstead. Denote V‹,λ“Vπ‹
λ,λ.
15Published in Transactions on Machine Learning Research (11/2023)
SinceV‹ďVπ‹,λďV‹,λďVπ‹
λ`λHln|A|, the regularized optimal policy π‹
λcan be nearly optimal as
long as the regularization coefficient λis small enough. For notational ease, we abuse π‹withπ‹
λ.
The Q-function can be defined in a similar manner:
Qπ,λ
m,hps,aq:“EMm,π«h´1ÿ
t“0rπ,λ
mpst,atqˇˇˇˇˇps0,a0q“ps,aqff
,
and the advantage function is defined as Aπ,λ
m,hps,aq:“Qπ,λ
m,hps,aq´Vπ,λ
m,hpsq.
Denoteπt:“πθt,Vt,λ:“Vπt,λandAt,λ
m,h:“Aπt,λ
m,hfor convenience.
Definition 8 (Definition 2 with entropy regularization) .Letgbe the parameter update weight, then NPG
is related to finding the minimizer for the following function:
Lpg;θ,vq:“Mÿ
m“1wmHÿ
h“1Es,a„vm,H´h„´
Aπθ,λ
m,hps,aq´gJ∇θlnπθpa|sq¯2ȷ
.
Definition9 (LyapunovPotentialFunction(Caycietal.,2021)) .We define the potential function Φ : ΠÑR
as follows: for any πPΠ,
Φpπq“Mÿ
m“1wmH´1ÿ
h“0Eps,aq„d‹
m,h„
lnπ‹pa|sq
πpa|sqȷ
.
A.2 Algorithms
Algorithm 3 is the full version of Algorithm 1, with support of entropy regularization. Algorithm 4 is the
skipped sampling function.
A.3 Performance of Natural Policy Gradient for LMDP
We restate Theorem 6 with entropy regularization.
Theorem 6 (Full Statement of Theorem 6) .With Definitions 4, 5 and 9, Algorithm 3 enjoys the following
performance bound:
E„
min
0ďtďTV‹,λ´Vt,λȷ
ďλp1´ηλqT`1Φpπ0q
1´p1´ηλqT`1`ηB2G2
2`řT
t“0p1´ηλqT´tErerrts
řT
t1“0p1´ηλqT´t1
ďλp1´ηλqT`1Φpπ0q
1´p1´ηλqT`1`ηB2G2
2`a
Hϵbias`a
Hκϵ stat.
Proof.Here we make shorthands for the sub-optimality gap and potential function: ∆t:“V‹,λ´Vt,λand
Φt:“Φpπtq. From Lemma 16 we have
η∆tďp1´ηλqΦt´Φt`1`ηerrt`η2B2G2
2.
Taking expectation over the update weights, we have
Erη∆tsďp1´ηλqErΦts´ErΦt`1s`ηErerrts`η2B2G2
2.
Thus,
E«
ηTÿ
t“0p1´ηλqT´t∆tff
ďTÿ
t“0p1´ηλqT´t`1ErΦts´Tÿ
t“0p1´ηλqT´tErΦt`1s
16Published in Transactions on Machine Learning Research (11/2023)
Algorithm 3 NPG: Sample-based NPG (full version).
1:Input:Environment E; learning rate η; episode number T; batch size N; initialization θ0; samplerπs;
regularization coefficient λ; entropy clip bound U; optimization domain G.
2:fortÐ0,1,...,T´1do
3:Initialize pFtÐ0dˆd,p∇tÐ0d.
4:fornÐ0,1,...,N´1do
5:forhÐ0,1,...,H´1do
6:ifπsis not None then
7: sh,ah,pAH´hpsh,ahqÐ SamplepE,πs,True,πt,h,λ,Uq(see Algorithm 4).
//s,a„rdπs
m,h, estimateAt,λ
m,H´hps,aq.
8:else
9: sh,ah,pAH´hpsh,ahqÐ SamplepE,πt,False,πt,h,λ,Uq.
//s,a„dθt
m,h, estimateAt,λ
m,H´hps,aq.
10: end if
11:end for
12:Update:
pFtÐpFt`H´1ÿ
h“0∇θlnπθtpah|shqp∇θlnπθtpah|shqqJ,
p∇tÐp∇t`H´1ÿ
h“0pAH´hpsh,ahq∇θlnπθtpah|shq.
13:end for
14:Call any solver to get pgtÐarg mingPGgJpFtg´2gJp∇t.
15:Updateθt`1Ðθt`ηpgt.
16:end for
17:Return:θT.
`ηTÿ
t“0p1´ηλqT´tErerrts`η2B2G2
2Tÿ
t“0p1´ηλqT´t
“p1´ηλqT`1Φ0´ErΦT`1s`ηTÿ
t“0p1´ηλqT´tErerrts`η2B2G2
2Tÿ
t“0p1´ηλqT´t
ďp1´ηλqT`1Φ0`ηTÿ
t“0p1´ηλqT´tErerrts`η2B2G2
2Tÿ
t“0p1´ηλqT´t,
where the last step uses the fact that Φpπqě0. This is a weighted average, so by normalizing the coefficients,
E„
min
0ďtďT∆tȷ
ďλp1´ηλqT`1Φ0
1´p1´ηλqT`1`ηB2G2
2`řT
t“0p1´ηλqT´tErerrts
řT
t1“0p1´ηλqT´t1
ďλp1´ηλqT`1Φ0
1´p1´ηλqT`1`ηB2G2
2`a
Hϵbias`a
Hκϵ stat,
where the last step comes from Lemma 17 and Jensen’s inequality. This completes the proof.
Aside from Remark 4, we have extra remarks:
Remark 6. ①This is the first result for LMDP and sample-based NPG with entropy regularization. ②For
any fixedλą0we have a linear convergence, which matches the result of discounted infinite horizon MDP
(Theorem1 in Cayci et al. (2021)); the limit when λtends to 0isOp1{pηTq`ηq(which implies an Op1{?
Tq
rate), matching the result in Agarwal et al. (2021).
17Published in Transactions on Machine Learning Research (11/2023)
Algorithm 4 Sample: Sampler for s„dπsamp
m,hwherem„Multinomialpw1,...,wMq,a„UnifAifunif“
True anda„πsampp¨|sqotherwise, and estimate of At,λ
m,H´hps,aq.
1:Input: Environment E; sampler policy πsamp; whether to sample uniform actions after state unif;
current policy πt; time step h; regularization coefficient λ; entropy clip bound U.
2:E.reset().
3:foriÐ0,1,...,h´1do
4:siÐE.get_state().
5:Sample action ai„πsampp¨|siqandE.execute(ai).
6:end for
7:shÐE.get_state().
8:ifunif= Truethen
9:ah„UnifA.
10:else
11:ah„πsampp¨|shq.
12:end if
13:ps,aqÐpsh,ahq.
14:Get a random number p„Unifr0,1s.
15:ifpă1
2then
16:Overrideah„πtp¨|shq.
17:Set importance weight CÐ´2.
18:rhÐE.execute(ah).
19:Initialize cumulative reward RÐrh`λHpπtp¨|shqq.
20:else
21:CÐ2.
22:rhÐE.execute(ah).
23:RÐrh`λmintln1
πtpah|shq,Uu.
24:end if
25:foriÐh`1,h`2,...,H´1do
26:siÐE.get_state().
27:ai„πtp¨|siqandrhÐE.execute(ai).
28:RÐR`ri`λHpπtp¨|siqq.
29:end for
30:Return:s,a,pAt,λ
H´hps,aq“CR.
B Results of Curriculum Learning for the Best Choice Problem (BCP)
Theorem 7 (Formal statement of Theorem 7) .For BCP, set samp“pi_sin Algorithm 2. Assume that
each number is independent from others and the i-th number has probability Piof being the best so far (see
formulation in Section 4.1 and Appendix C.1). Assume the optimal policy is a p-threshold policy and the
sampling policy is a q-threshold policy. There exists a policy parameterization and quantities
kcurl“#śtnpu
j“tnqu`11
1´Pj, qďp,
1, q ąp,knaïve“2tnpumax$
&
%1,max
iětnpu`2i´1ź
j“tnpu`12p1´Pjq,
.
-,
such thatkcurlďκcurlď2kcurlandknaïveďκnaïveď2knaïve. Hereκcurlandκnaïvecorrespond to κinduced
by theq-threshold policy and the naïve random policy respectively.
Proof.We need to calculate three state-action visitation distributions: that induced by the optimal policy,
d‹; that induced by the sampler which is the optimal for the curriculum, rdcurl; and that induced by the naïve
random sampler, rdnaïve. This then boils down to calculating the state(-action) visitation distribution under
two types of policies: any threshold policy and the naïve random policy.
18Published in Transactions on Machine Learning Research (11/2023)
For any policy π, denotedπpi{nqas the probability for the agent acting under πto see states i{nwith
arbitraryxi. We do not need to take the terminal state ginto consideration, since it stays in a zero-reward
loop and contributes 0toLpg;θ,dq. We use the LMDP distribution parameterization tPnudescribed in
Section 7.
Denoteπpas thep-threshold policy, i.e. accept if and only if i{nąpandxi“1. Then
dπpˆi
n˙
“Ppreject all previous i´1states|πpq
“i´1ź
j“1ˆ
Pˆj
n,1˙
1„j
nďpȷ
`1´Pˆj
n,1˙˙
“i´1ź
j“tnpu`1ˆ
1´Pˆj
n,1˙˙
“i´1ź
j“tnpu`1p1´Pjq.
Denoteπnaïveas the naïve random policy, i.e., accept any number with probability 1{2regardless of the
state. Then
dπnaïveˆi
n˙
“Ppreject all previous i´1states|πnaïveq“1
2i´1.
For anyπ, we can see that the state visitation distribution satisfies dπpi{n,1q“Pidπpi{nqanddπpi{n,0q“
p1´Piqdπpi{nq.
To show the possible largest difference, we use a parameterization that for each state s,ϕpsq“One-hotpsq.
The policy is then satisfied into
πθpaccept|sq“exppθJϕpsqq
exppθJϕpsqq`1, πθpreject|sq“1
exppθJϕpsqq`1,
because there are only two actions. Denote πθpsq“πθpaccept|sq, we have
∇θlnπθpaccept|sq“p 1´πθpsqqϕpsq,∇θlnπθpreject|sq“´πθpsqϕpsq.
Now suppose the optimal threshold and the threshold learned through curriculum are pandq, then
Σθ
d‹“ÿ
sPSdπppsq`
πppsqp1´πθpsqq2`p1´πppsqqπθpsq2˘
ϕpsqϕpsqJ,
Σθ
rdcurl“ÿ
sPSdπqpsqˆ1
2p1´πθpsqq2`1
2πθpsq2˙
ϕpsqϕpsqJ,
Σθ
rdnaïve“ÿ
sPSdnaïvepsqˆ1
2p1´πθpsqq2`1
2πθpsq2˙
ϕpsqϕpsqJ.
Denoteκ♣pθq“supxPRdxJΣθ
d‹x
xJΣθ
rd♣x. From parameterization we know all ϕpsqare orthogonal. Abusing πqwith
πcurl, we have
κ♣pθq“max
sPSdπppsq`
π‹psqp1´πθpsqq2`p1´π‹psqqπθpsq2˘
d♣psq`1
2p1´πθpsqq2`1
2πθpsq2˘.
We can separately consider each sPSbecause of the orthogonal features. Observe that πppsqPt0,1u, so
forsPS, its corresponding term in κ♣pθqis maximized when πθpsq “1´πppsqand is equal to 2dπppsq
d♣psq.
19Published in Transactions on Machine Learning Research (11/2023)
By definition, κ♣“max 0ďtďTErκ♣pθtqs. Sinceθ0“0d, we haveκ♣ěκ♣p0dqwhereπθpsq“1{2and the
corresponding term isdπppsq
d♣psq. So
max
sPSdπppsq
d♣psqďκ♣ď2 max
sPSdπppsq
d♣psq.
We now have an order-accurate result k♣“maxsPSdπppsq
d♣psqforκ♣. Direct computation gives
kcurl“#śtnpu
j“tnqu`11
1´Pj, qďp,
1, q ąp,
knaïve“2tnpumax$
&
%1,max
iětnpu`2i´1ź
j“tnpu`12p1´Pjq,
.
-.
This completes the proof.
C Full Experiments
Here are all the experiments not shown in Section 7. All the experiments were run on a server with
CPU AMD Ryzen 9 3950X, GPU NVIDIA GeForce 2080 Super and 128G memory. For legend descrip-
tion please refer to the caption of Figure 1. For code please refer to https://github.com/zhourunlong/
RL-for-Combinatorial-Optimization .
Policy parameterization.
•For BCP and OKD, there are exactly two actions, so we can use ϕpsq“ϕps,acceptq´ϕps,rejectq
instead of ϕps,acceptqandϕps,rejectq. Now the policy is πθpaccept|sq “exppθJϕpsqq
exppθJϕpsqq`1and
πθpreject|sq“1
exppθJϕpsqq`1.
•For ADW, there are n`1actions (nfor assigning a slot to advertisers and 1for not assigning it).
So, we must follow the canonical form of log-linear policies.
Training schemes. We ran nine experiments in total, four for BCP, three for OKD, and two for ADW.
The difference between the experiments of the same problem lies in the distribution over instances (i.e.,
twmu). In the following subsections, we will introduce how we parameterized the distribution in detail. In
a single experiment, we ran eight setups, each representing a combination of sampler policies, initialization
policies of the final phase, and whether we used regularization. For visual clarity, we did not plot setups
with entropy regularization, but the readers can plot it using plot.py in the supplementary files. We make
a detailed list of the training schemes in Table 1.
C.1 The Best Choice Problem (BCP)
State and action spaces. States with Xią1are the same. To make the problem “scale-invariant”, we
usei{nto represent i. So the states are s“pi{n,xi“ 1rXi“1sq. There is an additional terminal state
g“p0,0q. For each state, the agent can either accept or reject.
Transition and reward. Any action in gleads back to g. Once the agent accepts the i-th number, the
state transits into g, and reward is 1ifiis the maximum in the instance. If the agent rejects, then the state
goes toppi`1q{n,xi`1qifiănandgifi“n. For all other cases, rewards are 0.
Feature mapping. Recall that all states are of the form pf,xqwherefP r0,1s, xP t0,1u. We set a
degreed0and the feature mapping is constructed as the collection of polynomial bases with degree less than
d0(d“2d0):
ϕpf,xq“p 1,f,...,fd0´1,x,fx,...,fd0´1xq.
20Published in Transactions on Machine Learning Research (11/2023)
Abbreviation Detailed setup Script
fix_samp_curl Fixedsamplercurriculumlearning. In
the warm-up phase, train a policy πsfrom
scratch (with zero initialization in parame-
ters) using a small environment E1. In the
finalphase, changetothetrueenvironment
E, useπsas the sampler policy to train a
policy from scratch.Run Alg.2 with
samp“pi_sand
λ“0.
fix_samp_curl_reg The same as fix_samp_curl , but add en-
tropyregularization to both phases.Run Alg.2 with
samp“pi_sand
λ‰0.
direct Directlearning. Only the final phase.
Train a policy from scratch directly in E.Run Alg.1 with
θ0“0d,πs“None
andλ“0.
direct_reg The same as direct, but add entropy
regularization.Run Alg.1 with
θ0“0d,πs“None
andλ‰0.
naive_samp Learning with the naïve samp ler. Only
the final phase. Use the naïve random pol-
icy as the sampler to train a policy from
scratch inE.Run Alg.1 with
θ0“0d,πs“naïve
random policy and
λ“0.
naive_samp_reg The same as naive_samp , but add entropy
regularization.Run Alg.1 with
θ0“0d,πs“naive
random policy and
λ‰0.
curl Curriculumlearning. In the warm-up
phase, train a policy πsfrom scratch in E1.
In the final phase, change to Eand con-
tinue on training πs.Run Alg.2 with
samp“pi_tand
λ“0.
curl_reg The same as curl, but add entropy
regularization.Run Alg.2 with
samp“pi_tand
λ‰0.
reference This is the reference policy. For BCP, it
is exactly the optimal policy since it can be
calculated. For OKD, it is a bang-per-buck
policy and is not the optimal policy (whose
exactformisnotclear). ForADW,itisthe
near optimal policy in our restricted policy
/ feature class (trained using curriculum
learning).N/A
Table 1: Detailed setups for each training scheme.
LMDPdistribution. Wemodelthedistributionasfollows: foreach i, wecanhave xi“1withprobability
Piand is independent from other i1. By definition, P1“1while other Pican be arbitrary. The classical BCP
satisfiesPi“1{i. We also experimented on three other distributions (so in total there are four experiments),
each with a series of numbers p2,p3,...,pni.i.d.„Unifr0,1sand setPi“1{i2pi`0.25.
21Published in Transactions on Machine Learning Research (11/2023)
For each experiment, we run eight setups, each with different combinations of sampler policies, initialization
policies of the final phase, and the value of regularization coefficient λ. For the warm-up phases we set
n“10and for final phases n“100.
Results. Figure 4 (with its full view Figure 5), Figure 6, Figure 7, along with Figure 1 (with seed
2018011309) show four experiments of BCP. They shared a learning rate of 0.2, batch size of 100per
step in horizon, final n“100and warm-up n“10(if applied curriculum learning).2
TheexperimentinFigure4wasdoneintheclassicalBCPenvironment, i.e., allpermutationshaveprobability
1{n!to be sampled. Experiments Figure 1, Figure 6 and Figure 7 were done with other distributions: the
only differences are the random seeds, which we fixed and used to generate Pis for reproducibility.
The experiment of classical BCP was run until the direct training of n“100converges, while all other
experiments were run to a maximum episode of 30000(hence sample number of THb“30000ˆ100ˆ100“
3ˆ108).
The optimal policy was derived from dynamic programming.
Figure 4: Classical BCP, truncated to 3ˆ108samples.
Figure 5: Classical BCP, full view.
2All the four trainings shown in the figures have their counterparts with regularization ( λ“0.01). Check the supplementary
files and use TensorBoard for visualization.
22Published in Transactions on Machine Learning Research (11/2023)
Figure 6: BCP, with seed 20000308.
Figure 7: BCP, with seed 19283746.
C.2 Online Knapsack (decision version, OKD)
State and action spaces. The states are represented as
s“˜
i
n,si,vi,ři´1
j“1xjsj
B,ři´1
j“1xjvj
V¸
,
wherexj“ 1ritemjwas successfully chosen sfor1ďjďi´1(in the instance). There is an additional
terminal state g“p0,0,0,0,0q. For each state (including gfor simplicity), the agent can either accept or
reject.
Transitionandreward. Thetransitionisimpliedbythedefinitionoftheproblem. Anyactioninterminal
stategleads back to g. The item is successfully chosen if and only if the agent accepts and the budget is
sufficient. A reward of 1is given only the first timeři
j“1xiviěV, and then the state goes to g. For all
other cases, reward is 0.
Feature mapping. Suppose the state is pf,s,v,r,qq. We set a degree d0and the feature mapping is
constructed as the collection of polynomial bases with degree less than d0(d“d5
0):ϕpf,s,v,r,qq “
pfifsisvivrirqiqqif,is,iv,ir,iqwherei♣Pt0,1,...,d 0´1u.
LMDP distribution. In Section 3.2 the values and sizes are sampled from FvandFs. IfFvor
Fsis not Unif r0,1s, we model the distribution as: first set a granularity granand takegrannumbers
p1,p2,...,pgrani.i.d.„Unifr0,1s.pirepresents the (unnormalized) probability that xPppi´1q{gran,i{granq.
To sample, we take i„Multinomialpp1,p2,...,pgranqand return x„pi´1`Unifr0,1sq{gran.
For each experiment, we ran four setups, each with different combinations of sampler policies and initializa-
tion policies of the final phase. For the warm-up phases n“10and for final phases we set n“100in all
23Published in Transactions on Machine Learning Research (11/2023)
experiments, while BandVvary. In one experiment it satisfies that B{nare close for warm-up and final,
andV{Bincreases from warm-up to final.
Results. Figure 8, Figure 9, along with Figure 2 (with Fv“Fs“Unifr0,1s) show three experiments of
OKD. They shared a learning rate of 0.1, batch size of 100per step in horizon, final n“100and warm-up
n“10(if applied curriculum learning).
Experiments in Figure 8 and Figure 9 were done with other value and size distributions: the only differences
are the random seeds, which we fixed and used to generate FvandFsfor reproducibility.
Allexperimentswereruntoamaximumepisodeof 50000(hencesamplenumberof THb“50000ˆ100ˆ100“
5ˆ108).
The reference policy is a bang-per-buck algorithm (Section3.1 of Kong et al. (2019)): given a threshold r,
accepti-th item if vi{siěr. We searched for the optimal rwith respect to Online Knapsack because we
found that in general the reward is unimodal to rand contains no “plain area”, so we can easily apply
ternary search (the reward of OKD contains “plain area”).
Figure 8: OKD, with seed 2018011309.
Figure 9: OKD, with seed 20000308.
C.3 AdWords (decision version, ADW)
State and action spaces. The states are represented as
s“ˆj
m,v1,j,v2,j,...,vn,j,B1,B2,...,Bn,Vj
V˙
,
whereVjis equal to the total revenue up until now. There is an additional terminal state g“02n`2. For
each state (including gfor simplicity), the agent has n`1actions, with 0representing not assigning the slot
and1,...,nrepresenting assigning to the corresponding advertiser.
24Published in Transactions on Machine Learning Research (11/2023)
Transitionandreward. Thetransitionisimpliedbythedefinitionoftheproblem. Anyactioninterminal
stategleads back to g. The slotjis successfully assigned to advertiser iif and only if the action is iand
Biěvi,j. The next state is then with BiÐBi´vi,jandVjÐVj`vi,j. A reward of 1is given only the
first timeVj`vi,jěV, and then the state goes to g. For all other cases, reward is 0.
Feature mapping. The feature design in ADW is a bit tricky, since the state dimension is super large.
We simplify the setting by assuming all the advertisers are symmetric, so we design a function ϕand for
action 1ďiďn,
ϕs,i“ϕˆj
m,vi,j,Bi,Vj
V˙
,
and
ϕs,0“ϕˆj
m,0,0,Vj
V˙
.
Actually, not assigning the slot is equal to assigning the slot to a virtual advertiser with value 0.
We set a degree d0andϕpf,v,B,qqis constructed as the collection of polynomial bases with degree less than
d0(d“d4
0):ϕpf,v,B,qq“pfifvivBiBqiqqif,iv,iB,iqwherei♣Pt0,1,...,d 0´1u.
LMDP distribution. In Section 3.3 the values vi,jare sampled from Fi. IfFiis not Unif r0,1s, we model
the distribution in the same manner as in OKD. For each experiment, we ran four setups, each with different
combinations of sampler policies and initialization policies of the final phase.
In the experiment depicted in Figure 3: For the warm-up phases we set pn,mq“p 3,6qandV“2.7. For
final phases we set pn,mq “ p 10,20qandV“9. The distributions are parameterized random ones with
gran“10.
In the experiment depicted in Figure 10: For the warm-up phases we set pn,mq“p 3,6qandV“2.64. For
final phases we set pn,mq “ p 8,32qandV“7.04. The distributions are specially designed distributions,
with probability pit has a 0.4value, and the rest 1´pmass is random on p0.6,1q. This distribution type
has a special near optimal policy class: either pick two 0.4, or pick anything in p0.8,1q.
Results. Figure 3 and Figure 10 are experiments of ADW. They shared a learning rate of 0.1and batch
size of 100per step in horizon.
The reference policy is obtained by first running a curriculum learning, then using the learned policy as the
reference policy. This is because after we simplify the feature representation, we need to compare with the
near optimal policy inside this restricted policy / feature class.
Figure 10: ADW, with seed 19260817 and special distributions.
25Published in Transactions on Machine Learning Research (11/2023)
D Technical Details and Lemmas
D.1 Natural Policy Gradient for LMDP
This section is a complement to Section 5. We give details about the correctness of Natural Policy Gradient
for LMDP.
Theorem 12 is the finite-horizon Policy Gradient Theorem for LMDP, which takes the mixing weight twmu
into consideration.
According to Agarwal et al. (2021), the unconstrained, full-information NPG update weight satisfies
Fpθtqgt“∇θVt,λ. Lemma 13 and Lemma 14 together show that: it is equivalent to finding a minimizer of
the fitting compatible function approximation loss (Definition 8).
Theorem 12 (Policy Gradient Theorem for LMDP) .For any policy πθparameterized by θ, and any 1ď
mďM,
∇θ´
Es0„νm”
Vπθ,λ
m,Hps0qı¯
“Hÿ
h“1Es,a„dθ
m,H´h”
Qπθ,λ
m,hps,aq∇θlnπθpa|sqı
.
As a result,
∇θVπθ,λ“Mÿ
m“1wmHÿ
h“1Es,a„dθ
m,H´h”
Qπθ,λ
m,hps,aq∇θlnπθpa|sqı
.
Proof.For any 1ďhďHandsPS, sinceVπθ,λ
m,hpsq“ř
aPAπθpa|sqQπθ,λ
m,hps,aq, we have
∇θVπθ,λ
m,hpsq“ÿ
aPA´
Qπθ,λ
m,hps,aq∇θπθpa|sq`πθpa|sq∇θQπθ,λ
m,hps,aq¯
.
Hence
Hÿ
h“1ÿ
sPSdθ
m,H´hpsq∇θVπθ,λ
m,hpsq“Hÿ
h“1ÿ
sPSdθ
m,H´hpsqÿ
aPA´
Qπθ,λ
m,hps,aq∇θπθpa|sq`πθpa|sq∇θQπθ,λ
m,hps,aq¯
“Hÿ
h“1ÿ
sPSdθ
m,H´hpsqÿ
aPAπθpa|sqQπθ,λ
m,hps,aq∇θlnπθpa|sq
`Hÿ
h“1ÿ
sPSdθ
m,H´hpsqÿ
aPAπθpa|sq∇θQπθ,λ
m,hps,aq
“Hÿ
h“1Es,a„dθ
m,H´h”
Qπθ,λ
m,hps,aq∇θlnπθpa|sqı
`Hÿ
h“1ÿ
sPSdθ
m,H´hpsqÿ
aPAπθpa|sq∇θQπθ,λ
m,hps,aq.
Next we focus on the second term. From the Bellman equation,
∇θQπθ,λ
m,hps,aq“∇θ˜
rθps,aq´λlnπθpa|sq`ÿ
s1PSPps1|s,aqVπθ,λ
m,h´1ps1q¸
“´λ∇θlnπθpa|sq`ÿ
s1PSPps1|s,aq∇θVπθ,λ
m,h´1ps1q.
Particularly, ∇θQπ,λ
i,1ps,aq“´λ∇θlnπθpa|sq. So
Hÿ
h“1ÿ
sPSdθ
m,H´hpsqÿ
aPAπθpa|sq∇θQπθ,λ
m,hps,aq
26Published in Transactions on Machine Learning Research (11/2023)
“Hÿ
h“1ÿ
sPSdθ
m,H´hpsqÿ
aPAπθpa|sq˜
´λ∇θlnπθpa|sq`ÿ
s1PSPps1|s,aq∇θVπθ,λ
m,h´1ps1q¸
“´λHÿ
h“1ÿ
sPSdθ
m,H´hpsqÿ
aPA∇θπθpa|sq
looooooomooooooon
“0`Hÿ
h“2ÿ
s1PS∇θVπθ,λ
m,h´1ps1qÿ
sPSdθ
m,H´hpsqÿ
aPAπθpa|sqPps1|s,aq
loooooooooooooooooooooomoooooooooooooooooooooon
“dθ
m,H´h`1ps1q
“Hÿ
h“2ÿ
s1PSdθ
m,H´h`1ps1q∇θVπθ,λ
m,h´1ps1q
“Hÿ
h“1ÿ
sPSdθ
m,H´hpsq∇θVπθ,λ
m,hpsq´ÿ
s0PSνmps0q∇θVπθ,λ
m,Hps0q,
where we used the definition of dandνm. So by rearranging the terms, we complete the proof.
Lemma 13. Suppose ΓPRnˆm,D“diagpd1,d2,...,dmq PRmˆmwherediě0andqPRm, then
x“pΓDΓJq:ΓDqis a solution to the equation ΓDΓJx“ΓDq.
Proof.DenoteD1{2“diagp?d1,?d2,...,?dmq,P“ΓD1{2,p“D1{2q, then the equation is reduced to
PPJx“Pp. Suppose the singular value decomposition of PisUΣVJwhereUPRnˆn,ΣPRnˆm,VP
RmˆmwhereUandVare unitary, and singular values are σ1,σ2,...,σk. SoPPJ“UpΣΣJqUJand
pPPJq:“UpΣΣJq:UJ. Notice that
ΣΣJ“diagpσ2
1,σ2
2,...,σ2
k,0,..., 0qPRnˆn,
we can then derive the pseudo-inverse of this particular diagonal matrix as
pΣΣJq:“diagpσ´2
1,σ´2
2,...,σ´2
k,0,..., 0q.
It is then easy to verify that pΣΣJqpΣΣJq:Σ“Σ. Finally,
PPJx“pPPJqrpPPJq:Pps
“UpΣΣJqUJUpΣΣJq:UJUΣVJp
“UpΣΣJqpΣΣJq:ΣVJp
“UΣVJp
“Pp.
This completes the proof.
Lemma 14 (NPG Update Rule) .The update rule θÐθ`ηFpθq:∇θVπθ,λwhere
Fpθq“Mÿ
m“1wmHÿ
h“1Es,a„dθ
m,H´h”
p∇θlnπθpa|sqqbı
is equivalent to θÐθ`ηg‹, whereg‹is a minimizer of the function
Lpgq“Mÿ
m“1wmHÿ
h“1Es,a„dθ
m,H´h„´
Aπθ,λ
m,hps,aq´gJ∇θlnπθpa|sq¯2ȷ
.
Proof.
∇gLpgq“´ 2Mÿ
m“1wmHÿ
h“1Es,a„dθ
m,H´h”´
Aπθ,λ
m,hps,aq´gJ∇θlnπθpa|sq¯
∇θlnπθpa|sqı
.
27Published in Transactions on Machine Learning Research (11/2023)
Supposeg‹is any minimizer of Lpgq, we have ∇gLpg‹q“0, hence
Mÿ
m“1wmHÿ
h“1Es,a„dθ
m,H´h“`
g‹J∇θlnπθpa|sq˘
∇θlnπθpa|sq‰
“Mÿ
m“1wmHÿ
h“1Es,a„dθ
m,H´h”
Aπθ,λ
m,hps,aq∇θlnπθpa|sqı
“Mÿ
m“1wmHÿ
h“1Es,a„dθ
m,H´h”
Qπθ,λ
m,hps,aq∇θlnπθpa|sqı
.
SincepuJvqv“pvvJqu, then
Fpθqg‹“∇θVπθ,λ.
Now we assign 1,2,...,MHSA as indices to all pm,h,s,aqPt1,...,Muˆt 1,...,HuˆSˆA, and set
γj“∇θlnπθpa|sq,
dj“wmdθ
m,H´hps,aq,
qj“Qπθ,λ
m,hps,aq,
wherejis the index assigned to pm,h,s,aq. ThenFpθq“ΦDΦJand∇θVθ“ΦDqwhere
Γ“rγ1,γ2,...,γMHSAsPRdˆMHSA,
D“diagpd1,d2,...,dMHSAqPRMHSAˆMHSA,
q“rq1,q2,...,qMHSAsJPRMHSA.
We now conclude the proof by utilizing Lemma 13.
D.2 Auxiliary lemmas used in the main results
Lemma 15 (Performance Difference Lemma) .For any two policies π1andπ2, and any 1ďmďM,
Es0„νm”
Vπ1,λ
m,Hps0q´Vπ2,λ
m,Hps0qı
“Hÿ
h“1Es,a„dπ1
m,H´h„
Aπ2,λ
m,hps,aq`λlnπ2pa|sq
π1pa|sqȷ
.
As a result,
Vπ1,λ´Vπ2,λ“Mÿ
m“1wmHÿ
h“1Es,a„dπ1
m,H´h„
Aπ2,λ
m,hps,aq`λlnπ2pa|sq
π1pa|sqȷ
.
Proof.First we fix s0. By definition of the value function, we have
Vπ1,λ
m,Hps0q´Vπ2,λ
m,Hps0q
“E«H´1ÿ
h“0rmpsh,ahq´λlnπ1pah|shqˇˇˇˇˇMm,π1,s0ff
´Vπ2,λ
m,Hps0q
“E«H´1ÿ
h“0rmpsh,ahq´λlnπ1pah|shq`Vπ2,λ
m,H`1´hpsh`1q´Vπ2,λ
m,H´hpshqˇˇˇˇˇMm,π1,s0ff
“E«H´1ÿ
h“0E”
rmpsh,ahq´λlnπ2pah|shq`Vπ2,λ
m,H`1´hpsh`1qˇˇˇMm,π2,sh,ahıˇˇˇˇˇMm,π1,s0ff
28Published in Transactions on Machine Learning Research (11/2023)
`E«H´1ÿ
h“0´Vπ2,λ
m,H´hpshq`λlnπ2pah|shq
π1pah|shqˇˇˇˇˇMm,π1,s0ff
,
where the last step uses law of iterated expectations. Since
E”
rmpsh,ahq´λlnπ2pah|shq`Vπ2,λ
m,H`1´hpsh`1qˇˇˇMm,π2,sh,ahı
“Qπ2,λ
m,H´hpsh,ahq,
we have
Vπ1,λ
m,Hps0q´Vπ2,λ
m,Hps0q“E«H´1ÿ
h“0Qπ2,λ
m,H´hpsh,ahq´Vπ2,λ
m,H´hpshq`λlnπ2pah|shq
π1pah|shqˇˇˇˇˇMm,π1,s0ff
“E«H´1ÿ
h“0Aπ2,λ
m,H´hpsh,ahq`λlnπ2pah|shq
π1pah|shqˇˇˇˇˇMm,π1,s0ff
.
By taking expectation over s0, we have
Es0„νm”
Vπ1,λ
m,Hps0q´Vπ2,λ
m,Hps0qı
“E«H´1ÿ
h“0Aπ2,λ
m,H´hpsh,ahq`λlnπ2pah|shq
π1pah|shqˇˇˇˇˇMm,π1ff
“H´1ÿ
h“0ÿ
ps,aqPSˆAdπ1
m,hps,aqˆ
Aπ2,λ
m,H´hps,aq`λlnπ2pa|sq
π1pa|sq˙
.
The proof is completed by reversing the order of h.
Lemma 16 (Lyapunov Drift) .Recall definitions in Definitions 5 and 9. We have that:
Φpπt`1q´Φpπtqď´ηλΦpπtq`ηerrt´η`
V‹,λ´Vt,λ˘
`η2B2}gt}2
2
2.
Proof.Denote Φt:“Φpπtq. This proof follows a similar manner as in that of Lemma6 in Cayci et al. (2021).
By smoothness (see Remark6.7 in Agarwal et al. (2021)),
lnπtpa|sq
πt`1pa|sqďpθt´θt`1qJ∇θlnπtpa|sq`B2
2}θt`1´θt}2
2
“´ηgJ
t∇θlnπtpa|sq`η2B2}gt}2
2
2.
By the definition of Φ,
Φt`1´Φt“Mÿ
m“1wmHÿ
h“1Eps,aq„d‹
m,H´h„
lnπtpa|sq
πt`1pa|sqȷ
ď´ηMÿ
m“1wmHÿ
h“1Eps,aq„d‹
m,H´h“
gJ
t∇θlnπtpa|sq‰
`η2B2}gt}2
2
2.
By the definition of err t, Lemma 15 and again the definition of Φ, we finally have
Φt`1´ΦtďηMÿ
m“1wmHÿ
h“1Eps,aq„d‹
m,H´h”
At,λ
m,hps,aq´gJ
t∇θlnπtpa|sqı
´ηMÿ
m“1wmHÿ
h“1Eps,aq„d‹
m,H´h„
At,λ
m,hps,aq`λlnπtpa|sq
π‹pa|sqȷ
´ηλMÿ
m“1wmHÿ
h“1Eps,aq„d‹
m,H´h„
lnπ‹pa|sq
πtpa|sqȷ
`η2B2}gt}2
2
2
29Published in Transactions on Machine Learning Research (11/2023)
“ηerrt´η`
V‹,λ´Vt,λ˘
´ηλΦt`η2B2}gt}2
2
2,
which completes the proof.
Lemma 17. Recall that g‹
tis the true minimizer of Lpg;θt,dtqin domain G.errtdefined in Definition 5
satisfies
errtďa
HLpg‹
t;θt,d‹q`a
HκpLpgt;θt,dtq´Lpg‹
t;θt,dtqq.
Proof.The proof is similar to that of Theorem6.1 in Agarwal et al. (2021). We make the following decom-
position of err t:
errt“Mÿ
m“1wmH´1ÿ
h“0Eps,aq„d‹
m,h”
At,λ
m,hps,aq´g‹J
t∇θlnπtpa|sqı
looooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooon
①
`Mÿ
m“1wmH´1ÿ
h“0Eps,aq„d‹
m,h“
pg‹
t´gtqJ∇θlnπtpa|sq‰
loooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooon
②.
SinceřM
m“1wmřH´1
h“0ř
ps,aqPSˆAd‹
m,hps,aq“H, normalize the coefficients and apply Jensen’s inequality,
then
①ďgffeMÿ
m“1wmH´1ÿ
h“0ÿ
ps,aqPSˆAd‹
m,hps,aq¨gffeMÿ
m“1wmH´1ÿ
h“0Eps,aq„d‹
m,h„´
At,λ
m,hps,aq´g‹J
t∇θlnπtpa|sq¯2ȷ
“a
HLpg‹
t;θt,d‹q.
Similarly,
②ďgffeHMÿ
m“1wmH´1ÿ
h“0Eps,aq„d‹
m,h”
ppg‹
t´gtqJ∇θlnπtpa|sqq2ı
“gffeHMÿ
m“1wmH´1ÿ
h“0Eps,aq„d‹
m,hrpg‹
t´gtqJ∇θlnπtpa|sqp∇θlnπtpa|sqqJpg‹
t´gtqs
(i)“b
H}g‹
t´gt}2
Σt
d‹
ďb
Hκ}g‹
t´gt}2
Σt,
where in (i), for vector v, denote}v}A“?
vJAvfor a symmetric positive semi-definite matrix A. Due to
thatg‹
tminimizesLpg;θt,dtqover the set G, the first-order optimality condition implies that
pg´g‹
tqJ∇gLpg‹
t;θt,dtqě0
for anyg. Therefore,
Lpg;θt,dtq´Lpg‹
t;θt,dtq
“Mÿ
m“1wmHÿ
h“1Es,a„dt
m,H´h„´
At,λ
m,hps,aq´g‹J
t∇lnπtpa|sq`pg‹
t´gqJ∇lnπtpa|sq¯2ȷ
´Lpg‹
t;θt,dtq
“Mÿ
m“1wmHÿ
h“1Es,a„dt
m,H´h”`
pg‹
t´gqJ∇θlnπtpa|sq˘2ı
30Published in Transactions on Machine Learning Research (11/2023)
`pg´g‹
tqJ˜
´2Mÿ
m“1wmHÿ
h“1Es,a„dt
m,H´h”´
At,λ
m,hps,aq´g‹J
t∇θlnπtpa|sq¯
∇θlnπtpa|sqı¸
“}g‹
t´g}2
Σt`pg´g‹
tqJ∇gLpg‹
t;θt,dtq
ě}g‹
t´g}2
Σt.
So finally we have
errtďa
HLpg‹
t;θt,d‹q`a
HκpLpgt;θt,dtq´Lpg‹
t;θt,dtqq.
This completes the proof.
D.3 Bounding ϵstat
Lemma 18 (Hoeffding’s Inequality) .SupposeX1,X2,...,Xnare i.i.d. random variables taking values in
ra,bs, with expectation µ. Let ¯Xdenote their average, then for any ϵě0,
P`ˇˇ¯X´µˇˇěϵ˘
ď2 expˆ
´2nϵ2
pb´aq2˙
.
Lemma 19. For any policy π, any state sPSand anyUěln|A|´1,
0ďÿ
aPAπpa|sqln1
πpa|sq´ÿ
aPAπpa|sqmin"
ln1
πpa|sq,U*
ď|A|
eU`1.
Proof.The first inequality is straightforward, so we focus on the second part. Set A1“taPA: ln1
πpa|sqą
Uu“taPA:πpa|sqă1
eUuandp“ř
aPA1πpa|sq, then
ÿ
aPAπpa|sqln1
πpa|sq´ÿ
aPAπpa|sqmin"
ln1
πpa|sq,U*
“ÿ
aPA1πpa|sqln1
πpa|sq´ÿ
aPA1πpa|sqU
“pÿ
aPA1πpa|sq
pln1
πpa|sq´pU
ďpln˜ÿ
aPA1πpa|sq
p1
πpa|sq¸
´pU
ďpln|A|
p´pU,
where the penultimate step comes from concavity of lnxand Jensen’s inequality. Let fppq“pln|A|
p´pU,
thenf1ppq “ln|A|´U´1´lnp. Recall that Uěln|A|´1, sofppqincreases when pP p0,|A|
eU`1qand
decreases when pPp|A|
eU`1,1q. Sincefp|A|
eU`1q“|A|
eU`1we complete the proof.
Lemma 20 (Loss Function Concentration) .If setπs“None andUěln|A|´1, then with probability
1´2pT`1qexp´
´2Nϵ2
C2¯
, the update weight sequence of Algorithm 3 satisfies: for any 0ďtďT,
Lppgt;θt,dθtq´Lpg‹
t;θt,dθtqď2ϵ`8λGB|A|
eU`1,
where
C“16HGBr1`λU`Hp1`λln|A|qs` 4HG2B2.
Ifπs‰None andλ“0, then with probability 1´2pT`1qexp´
´2Nϵ2
C2¯
, the update weight sequence of
Algorithm 3 satisfies: for any 0ďtďT,
Lppgt;θt,rdπsq´Lpg‹
t;θt,rdπsqď2ϵ,
31Published in Transactions on Machine Learning Research (11/2023)
where
C“16H2GB`4HG2B2.
Proof.We first prove the πs“None case. For time step t, Algorithm 3 samples HNtrajectories. Abusing
the notation, denote
pFt“1
NNÿ
n“1H´1ÿ
h“0∇θlnπθpan,h|sn,hqp∇θlnπθpan,h|sn,hqqJ,
p∇t“1
NNÿ
n“1H´1ÿ
h“0pAn,H´hpsn,h,an,hq∇θlnπθpan,h|sn,hq,
pLpgq“Mÿ
m“1wmHÿ
h“1Es,a„dθt
m,H´h”
At,λ
m,hps,aq2ı
loooooooooooooooooooooooomoooooooooooooooooooooooon
①`gJpFtg´2gJp∇t loooooooomoooooooon
②.
Notice that ①is a constant. From Algorithm 3, pgtis the minimizer of ②(hencepLpgq) inside the ball G. From
∇θlnπθpa|sq“ϕps,aq´Ea1„πθp¨|sqrϕps,a1qs,}ϕps,aq}2ďB,}g}2ďG, we know thatˇˇgJ∇θlnπθpa|sqˇˇď
2GB. So0ďgJpFtgď4HG2B2. From Algorithm 4, we know that any sampled pAsatisfies|pA|ď2r1`λU`
Hp1`λln|A|qs. So|gJp∇t|ď4HGBr1`λU`Hp1`λln|A|qs. We first have that
´8HGBr1`λU`Hp1`λln|A|qsď②ď8HGBr1`λU`Hp1`λln|A|qs` 4HG2B2.(2)
To apply any standard concentration inequality, we next need to calculate the expectation of ②. According
to Monte Carlo sampling and Lemma 19, for any 1ďmďM,1ďhďHandps,aqPSˆA, we have
At,λ
m,hps,aq´λ|A|
eU`1ďE”
pAt,λ
m,hps,aqı
ďAt,λ
m,hps,aq.
Denote ∇tas the exact policy gradient at time step t, then
ˇˇˇE”
gJp∇tı
´gJ∇tˇˇˇď}g}2›››E”
p∇tı
´∇t›››
2
ď}g}2¨H}∇θlnπθpa|sq}2›››E”
pAps,aqı
´Aps,aq›››
8
ď2λGB|A|
eU`1.
Since Monte Carlo sampling correctly estimates state-action visitation distribution, E”
pFtı
“Fpθtq. Notice
thatgJpFtgis linear in entries of pFt, we have E”
gJpFtgı
“gJFpθtqg. Now we are in the position to show
that
ˇˇˇE”
pLpgqı
´Lpgqˇˇˇď4λGB|A|
eU`1.
Hoeffding’s inequality (Lemma 18) gives
P´ˇˇˇpLpgq´E”
pLpgqıˇˇˇěϵ¯
ď2 expˆ
´2Nϵ2
C2˙
.
where from Equation (2),
C“16HGBr1`λU`Hp1`λln|A|qs` 4HG2B2.
32Published in Transactions on Machine Learning Research (11/2023)
After applying union bound for all t, with probability 1´2pT`1qexp´
´2Nϵ2
C2¯
the following holds for any
gPG:
ˇˇˇpLpg;θt,dθtq´Lpg;θt,dθtqˇˇˇďϵ`4λGB|A|
eU`1.
Hence
Lppgt;θt,dθtqďpLppgt;θt,dθtq`ϵ`4λGB|A|
eU`1
ďpLpg‹
t;θt,dθtq`ϵ`4λGB|A|
eU`1
ďLpg‹
t;θt,dθtq`2ϵ`8λGB|A|
eU`1.
Forπs‰None andλ“0, we notice that |pA| ď2Hand hence´8H2GBď②ď8H2GB`4HG2B2.
Moreover, E”
pAt,λ
m,hps,aqı
“At,λ
m,hps,aq. So by slightly modifying the proof we can get the result.
33