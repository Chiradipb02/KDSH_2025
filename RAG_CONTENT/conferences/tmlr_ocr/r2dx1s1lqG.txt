Published in Transactions on Machine Learning Research (05/2024)
Boosting Data-Driven Mirror Descent with Randomization,
Equivariance, and Acceleration
Hong Ye Tan hyt35@cam.ac.uk
Department of Applied Mathematics and Theoretical Physics
University of Cambridge, United Kingdom
Subhadip Mukherjee smukherjee@ece.iitkgp.ac.in
Department of Electronics and Electrical Communication Engineering
Indian Institute of Technology, Kharagpur, India
Junqi Tang j.tang.2@bham.ac.uk
School of Mathematics
University of Birmingham, United Kingdom
Carola-Bibiane Schönlieb cbs31@cam.ac.uk
Department of Applied Mathematics and Theoretical Physics
University of Cambridge, United Kingdom
Reviewed on OpenReview: https: // openreview. net/ forum? id= r2dx1s1lqG
Abstract
Learning-to-optimize (L2O) is an emerging research area in large-scale optimization with
applications in data science. Recently, researchers have proposed a novel L2O framework
called learned mirror descent (LMD), based on the classical mirror descent (MD) algorithm
with learnable mirror maps parameterized by input-convex neural networks. The LMD
approach has been shown to significantly accelerate convex solvers while inheriting the
convergence properties of the classical MD algorithm. This work proposes several practical
extensions of the LMD algorithm, addressing its instability, scalability, and feasibility for
high-dimensional problems. We first propose accelerated and stochastic variants of LMD,
leveraging classical momentum-based acceleration and stochastic optimization techniques
for improving the convergence rate and per-iteration computational complexity. Moreover,
for the particular application of training neural networks, we derive and propose a novel and
efficient parameterization for the mirror potential, exploiting the equivariant structure of
the training problems to significantly reduce the dimensionality of the underlying problem.
We provide theoretical convergence guarantees for our schemes under standard assumptions
and demonstrate their effectiveness in various computational imaging and machine learning
applications such as image inpainting, and the training of support vector machines and deep
neural networks.
1 Introduction
Large-scale optimization plays a key role in modern data science applications. In such applications, we
typically seek to infer a collection of parameters x∈Rnfrommi.i.d. data samples of a random variable
{zi}m
i=1, by solving optimization problems of the form:
x⋆∈arg min
x∈Rnf(x) :=1
mm/summationdisplay
i=1[fi(x;zi) +R(x)], (1)
1Published in Transactions on Machine Learning Research (05/2024)
where each fiis a fidelity function associated with the i-th data sample zi, whileR(x)is a regularizer
containing the prior knowledge of x. In the context of machine learning applications, xwould contain the
parameters of some models such as SVMs and neural networks, while zicontains a pair of of training data
samples and labels. For signal/image processing and inverse problems, ziis usually referred to as a pair of
measurement operators and measurement data, with the measurement operator appearing in the fidelity fi.
Many classical variational image reconstruction models can also take the form of (1), with a simple example
being total variation regularization (Rudin et al., 1992). In this case, we have m= 1, wheref1depends only
on the corrupted image, and the total variation regularizer R(x) =λ∥∇x∥1is theℓ1norm of the discrete
differences of x. In modern applications, the dimension of the optimization problems and the number of data
samples involved are often large, leading to heavy demands on the development of computationally efficient
optimization algorithms.
Throughout the past decade, we have witnessed tremendous successes by researchers in the field of optimiza-
tion in data science. In particular, efficient first-order algorithms utilizing momentum-based acceleration
(Nesterov, 1983; Beck & Teboulle, 2009) and stochastic approximation (Robbins & Monro, 1951; Zhang,
2004) have been proposed, with applications including sparse signal recovery and convex programming
(Bubeck, 2015; Ghadimi & Lan, 2016). Recently, stochastic gradient algorithms with theoretically optimal
convergence rates for solving generic classes of problems were developed (Lan, 2012; Lan & Zhou, 2015;
Defazio, 2016; Allen-Zhu, 2017; Chambolle et al., 2018; Tang et al., 2018; Zhou et al., 2018b; Driggs et al.,
2021), matching complexity lower bounds for convex (Woodworth & Srebro, 2016) or non-convex problems
(Arjevani et al., 2023).
Despite being theoretically optimal for generic optimization problem classes, stochastic gradient methods
result in poor performance when applied to problems with particular structures. Tang et al. (2020) demon-
strate that while optimal stochastic gradient methods perform very well in terms of convergence rates for
certain imaging inverse problems such as X-ray CT, they perform poorly for some other problems such as
image deblurring and compressed sensing. One important aspect is the fact that these methods are designed
for some wide generic class of problems, while in practice, each practitioner may be only interested in a very
narrow subclass of problems with a specific structure. The optimal algorithms for generic problems may still
perform suboptimally for the specific subclasses of interest.
1.1 Learning-to-Optimize
Learning-to-optimize (L2O), sometimes referred to as meta-learning in the machine learning literature, aims
to overcome this limitation when dealing with more specialized problem classes, such as natural image
reconstruction as a subset of all imaging problems. An underlying idea is to utilize the intrinsic structure of
the optimization problems to converge faster on tasks from the same distribution. This has been shown to be
effective for problems such as linear regression, training neural networks, and tomographic reconstruction,
beating standard analytic methods (Andrychowicz et al., 2016; Li & Malik, 2016; Li et al., 2019; Banert
et al., 2020). The aim is to generate optimizers that (1) optimize in-distribution functions at a faster rate,
and (2) possibly return a higher quality solution given the same computing budget (Chen et al., 2022).
The L2O framework can be formally described as follows (Chen et al., 2022). Given a learnable mapping
functiongwith parameters θ, as well as a set Fof training sample optimization problems minxf(x)for
f∈F, the optimizer is learned to generate updates of the form xk+1=xt−gθ(It). Here,Itrepresents
previous information, such as the iterates xt,xt−1,...or their gradients ∇f(xt),∇f(xt−1)etc. For example,
gradient descent with can be formulated as gθ(It) =η∇f(xt), with a learnable step-size parameter η. This
formulationofL2Oisflexibleinthesensethatallpreviousinformationisavailable, whichshouldtheoretically
allow for better informed steps. The parameters are usually learned by minimizing an objective of the form
min
θEf∈F/bracketleftiggT/summationdisplay
i=1wtf(xt)/bracketrightigg
, xt+1=xt−gθ(It). (2)
This can be interpreted as minimizing the objective value of the evolved iterates, with weights wt. Some
standard choices include wt≡1, orwT= 1,wT−1=...=w1= 0(Zhou et al., 2018a; Andrychowicz et al.,
2016).
2Published in Transactions on Machine Learning Research (05/2024)
1.2 Existing Theory in L2O
Learning-to-optimize frameworks broadly fall into two categories: fully learned “model-free” methods, and
“model-based” methods based on classical algorithms (Chen et al., 2022). Model-free methods directly pa-
rameterize the optimizer updates gθ(It)using neural networks, formulating fast optimization as an unrolled
loss objectives of the form Equation (2) (Andrychowicz et al., 2016; Li & Malik, 2016). Recent advances
for this paradigm include improving the generalization performance of both the optimizers and final opti-
mization objective when used to train neural networks (Almeida et al., 2021; Yang et al., 2023), more stable
computation (Metz et al., 2019), and learning to warm-start certain optimization methods (Sambharya et al.,
2023). However, major drawbacks include lack of convergence guarantees, as well as requiring many training
samples.
Model-based L2O methods instead inherit structure from classical optimization algorithms such as PDHG-
like proximal splitting algorithms, where certain optimizer parameters such as step-sizes or momentum
parameters are learned (Banert et al., 2020; Almeida et al., 2021; Banert et al., 2021; Gregor & LeCun,
2010). These methods are typically applied to convex problems rather than non-convex problems such as
neural network training, which also allows for theoretical convergence analysis of the learned schemes. One
advantage of building a L2O model from a classical optimization model is that convergence of the learned
model can usually be derived by slightly modifying existing convergence results. A more in-depth overview
of L2O can be found in Chen et al. (2022).
From a more statistical point of view, Chen & Hazan (2023) formulates learning-to-optimize as a feedback
control problem for smooth convex objective functions, showing an upper bound on the expected opti-
mality gap in a broader class of stabilizing controllers. In the case where the algorithm class has small
pseudo-dimension with respect to a set of problem instances, the empirical risk minimization problem ad-
mits statistical PAC bounds for the number of problem instances required (Gupta & Roughgarden, 2016).
Sambharya et al. (2023) demonstrates PAC-Bayes generalization bounds for certain classes of fixed-point
optimization methods. A related notion is that of (semi-)amortized optimization, where a model maps an
initialization to a solution with access to an objective f(Amos et al., 2023).
Combining L2O with mirror descent has mainly been explored in the online setting, with theoretical con-
tributions arising in terms of statistical transfer bounds and optimality gap bounds (Khodak et al., 2019;
Denevi et al., 2019). Gao et al. (2022) also consider a simple version of coordinate-based mirror descent,
providing a statistical PAC generalization bound. These are more suitable to common meta-learning bench-
marks such as few-shot training for classification where convexity is unavailable (Finn et al., 2017; Nichol
et al., 2018). These meta-learning methods that are focused on neural networks generally take a different
approach to the L2O methods discussed above – meta-learning accelerates by moving the initializations to
“better points”, in contrast to the L2O objective of learning an optimization trajectory (from any initializa-
tion). We consider a more general and less theoretically-backed framework where the meta-parameters are
baked into the optimizer, rather than common instances such as initialization which are difficult to interpret
in our context.
A provable learning-to-optimize framework called learned mirror descent (LMD) was recently proposed in
Tan et al. (2023b;a) based on the classical mirror descent (MD) algorithm by Nemirovski & Yudin (1983). By
using neural networks to implicitly learn and model the geometry of the underlying problem class via mirror
maps, LMD can achieve significantly faster convergence on certain convex problem classes, including model-
based image inpainting and training support vector machines. Moreover, the LMD framework proposes
approximate convergence results based on the accuracy of the mirror map inversion.
Building upon the vanilla LMD framework by Tan et al. (2023b), we make the following contributions:
1.Learned accelerated mirror descent and its theoretical analysis. Our first contribution is de-
veloping the learned accelerated mirror descent (LAMD) scheme, which was initially proposed in the
preliminary work of Tan et al. (2023a) without convergence analysis. In this work, we complete the theo-
retical analysis of the LAMD scheme under standard assumptions, demonstrating improved convergence
rates over vanilla LMD. In particular, we demonstrate in Theorem 1 that we obtain convergence to the
minimum, as opposed to a constant above the minimum as found in Tan et al. (2023b).
3Published in Transactions on Machine Learning Research (05/2024)
2.Learned stochastic mirror descent and its acceleration. We propose the learned stochastic mirror
descent (LSMD) and the accelerated version (LASMD) in the spirit of stochastic optimization, which is
crucial for the scalability of the LMD framework in large-scale problems prevalent in modern data science
applications. We also provide theoretical convergence analysis of these schemes and demonstrate their
effectiveness using numerical experiments including image processing, training of SVMs, and training
neural networks for classification.
3.Efficient mirror map parameterization by utilizing the equivariant structure. We propose
equivariant theory for L2O in the case where the target class of objectives satisfies some group sym-
metries. For the training of deep neural networks, we utilize the equivariant structure of the training
problems to significantly simplify the parameterization of the mirror map, further improving the effi-
ciency and practicality of the LMD framework, and allowing for applications in a non-convex setting. We
provide a derivation of this scheme, showing theoretically that learned optimizers trained under certain
group invariance and equivariance assumptions will also satisfy a group invariance property, removing
redundant parameters and reducing the problem dimensionality. We additionally utilize this to extend
the LMD framework to the task of training deep and convolutional neural networks, achieving competitive
performance with widely used and empirically powerful optimizers such as Adam.
1.3 Mirror Descent and Learned Mirror Descent
Mirror descent (MD) was originally proposed by Nemirovski & Yudin (1983) as a method of generalizing
gradient descent to general infinite-dimensional Banach spaces. The lack of Hilbert space structure and
isomorphisms between Xand their dualsX∗prevents the use of gradient descent, which usually identifies
the formal derivative Df∈X∗with the corresponding Riesz element ∇f∈X, satisfying⟨Df,x⟩X∗,X=
⟨∇f,x⟩X,X. MD proposes to address this by directly updating elements in the dual space X∗, and ‘tying’
the dual space updates in X∗to corresponding primal space updates in Xusing a “mirror map”, satisfying
various conditions.
GeneralizingtheclassicalEuclideanstructureusedforgradientdescenttomoregeneralBregmandivergences,
such as those induced by Lpnorms, can improve the dimensionality scaling of the Lipschitz constant. Having
a smaller Lipschitz constant with respect to another norm compared to the Euclidean norm directly impacts
the step size for which the methods converge, as well as the convergence rates. For example, for minimizing
a convex function fon then-simplex ∆n={x∈Rn|xi≥0,/summationtextn
i=1xi}, MD is able to achieve rates of
O(√logn∥f′∥∞/√
k), as opposed to (sub-)gradient descent which has rate O(∥f′∥2/√
k)(Beck & Teboulle,
2003; Ben-Tal et al., 2001). The ratio ∥f′∥2/∥f′∥∞can be as large as√n, leading to (n/logn)1/2asymptotic
speedup by using MD.
Gunasekar et al. (2021) demonstrate further that mirror descent corresponds to a Riemannian gradient flow
in the limiting case, and further generalize mirror descent to general Riemannian geometries. Moreover,
the mirror descent framework is amenable to modifications similar to gradient descent, with extensions such
as ergodicity (Duchi et al., 2012), composite optimization (Duchi et al., 2010), stochasticity (Lan et al.,
2012; Zhou et al., 2017) and acceleration (Krichene et al., 2015; Hovhannisyan et al., 2016). In summary,
the mirror descent algorithm allows us to utilize non-Euclidean geometry for optimization, which has been
shown to accelerate convergence in applications including online learning and tomography (Srebro et al.,
2011; Raskutti & Mukherjee, 2015; Allen-Zhu & Orecchia, 2014; Ben-Tal et al., 2001; Orabona et al., 2015;
Zimmert & Lattimore, 2019).
We present the MD algorithm as given in Beck & Teboulle (2003), as well as the data-driven version of which
this work is based, LMD (Tan et al., 2023b). The LMD framework serves as a base for the accelerated and
stochastic extensions that will be presented in this work. LMD aims to speed up convergence on a class of
“similar” functions, which are taken to be qualitatively similar, such as image denoising on natural images
or CT imaging (Banert et al., 2020). By using data to generate functions from the target function class, the
geometry of the class can be learned using these sample functions. Mirror descent then exploits this learned
geometry for faster convergence. Follow-up works additionally suggest that the learned geometry can be
transferred to other mirror-descent-type algorithms without the need for retraining. Moreover, the learned
4Published in Transactions on Machine Learning Research (05/2024)
mirror maps are robust to small perturbations in the forward operator such as from the identity to a blur
convolution, as well as change of data sets (Tan et al., 2023a).
We begin with some notation and definitions that will be used for mirror descent, as well as more standard
assumptions for the MD framework.
LetXbe the finite dimensional normed space (Rn,∥·∥). Denote byX∗= ((Rn)∗,∥·∥∗)the dual space,
where the dual norm is defined as ∥p∥∗= supx{⟨p,x⟩|∥x∥≤1}, and the bracket notation denotes evaluation
⟨p,x⟩=p(x)∈R. Let R:=R∪{+∞}denote the extended real line. Let f:X→Rbe a proper, convex,
and lower-semicontinuous function with well-defined subgradients. For a differentiable convex function h:
X → R, define the Bregman divergence asBh(x,y) =h(x)−h(y)−⟨h′(y),x−y⟩. Letg:X →X∗be a
function such that g(x)∈∂f(x)for allx∈X. We denote a (possibly noisy) stochastic approximation to g
by
G(x,ξ) =g(x) + ∆(x,ξ) +U(x),
where Eξ[∆(x,ξ)] = 0, andU(x) :X →X∗represents dual noise. One standard assumption that we will
use is that Ghas bounded second moments.
While we have defined Xto be a vector space, this is not necessary. Indeed, suppose instead that X
were a (proper) closed convex subset of Rn. Assuming that the inverse mirror function maps into X, i.e.,
∇ψ∗: (Rn)∗→X, such as when∇ψ(x)→∞asx→∂X, then the proposed algorithms will still hold. We
will restrict our exposition in this work to the simpler case where X=Rnto avoid these technicalities, but
the results can be extended.
Definition 1. We define a mirror potential to be anα-strongly convexC1functionψ:X→Rwithα>0.
We define a mirror map to be the gradient of a mirror potential ∇ψ:X→X∗.
We further denote the convex conjugate of ψbyψ∗:X∗→R. Note that under these assumptions, we have
that∇ψ∗= (∇ψ)∗ondomψ. Moreover, ψ∗isα−1-smooth with respect to the dual norm, or equivalently,
∇ψ∗isα−1-Lipschitz (Azé & Penot, 1995).
For a mirror potential ψand step-sizes (tk)∞
k=0, the mirror descent iterations of Beck & Teboulle (2003)
applied to an initialization x(0)∈Xare
x(k+1)=∇ψ∗(∇ψ(x(k))−tk∇f(x(k))). (3)
The roles of∇ψ:X→X∗and∇ψ∗:X∗→Xare tomirrorbetween the primal and dual spaces such that
the gradient step is taken in the dual space. A special case is when Ψ(x) =1
2∥x∥2
2, whereby∇ψand∇ψ∗
are the canonical isomorphisms between XandX∗, and the MD iterations (3) reduce to gradient descent.
In the case where fisL-Lipschitz and ψhas strong convexity parameter α > 0, mirror descent is able to
achieve optimality gap bounds of the following form (Beck & Teboulle, 2003):
min
1≤k≤sf(x(k))−f(x∗)≤Bψ(x∗,x(1)) + (2α)−1/summationtexts
k=1t2
k∥∇f(x(k))∥2
∗/summationtexts
k=1tk.
LMD arises when we replace ψandψ∗with neural networks, which we can then learn from data. We denote
learned variants of ψandψ∗byMθ:X → RandM∗
ϑ:X∗→Rrespectively. Note that MθandM∗
ϑ
are not necessarily convex conjugates of each other due to the lack of a closed-form convex conjugate for
general neural networks – we change the subscript to remind the reader of this subtlety. Equipped with
this new notation for learned mirror maps, the learned mirror descent algorithm arises by directly replacing
the mirror maps∇ψand∇ψ∗with learned variants MθandM∗
ϑrespectively, where ∇M∗
ϑis enforced to be
approximately (∇Mθ)−1during training (Tan et al., 2023b):
x(k+1)=∇M∗
ϑ(∇Mθ(x(k))−tk∇f(x(k))). (4)
Themismatchbetween ∇M∗
ϑand(∇Mθ)−1, alsocalledthe forward-backward inconsistency , necessarilyarises
as the convex conjugate of a neural network is generally intractable. Tan et al. (2023b) consider analyzing
LMD as an approximate mirror descent scheme, where the error bounds depend on the distance between
5Published in Transactions on Machine Learning Research (05/2024)
the computed (approximate) iterates (4) and the true mirror descent iterates (3). Under certain conditions
on the target function fand the mirror potential ψ=Mθ, they show convergence in function value up to a
constant over the minimum (Tan et al., 2023b, Thm. 3.1, 3.6). A sufficient condition for this convergence
is that the LMD iterations xiwith inexact mirror maps given by Equation (4) is close to the exact MD
iterations ˆx(k+1)= (∇Mθ)−1(∇Mθ(x(k))−tk∇f(x(k))), in the sense that ⟨∇ψ(ˆx(i))−∇ψ(x(i)),x−x(i)⟩and
⟨∇f(ˆxi),xi−ˆxi⟩are uniformly bounded for a given minimizer x.
This work proposes to address two current restrictions of LMD. The first restriction is the approximate
convergence and instability that arises due to the forward-backward inconsistency, such that the function
values are only minimized up to a constant above the minimum. This is demonstrated in Tan et al. (2023a,
Thm. 3.1), where LMD is also shown to diverge in later iterations as the forward-backward inconsistencies
accumulate. Section 2 presents an algorithm through which the constant can vanish if the forward-backward
inconsistency is uniformly bounded. The second drawback is the computational cost of training LMD, which
is prohibitive for expensive forward operators such as CT or neural network parameters. Sections 3 and 4
present stochastic and accelerated stochastic extensions of LMD that show convergence in expectation even
with possibly unbounded stochastic approximation errors. We demonstrate these extensions with function
classes considered in previous works in Section 5, utilizing pre-trained neural networks on tasks such as image
denoising, image inpainting and training support vector machines (Tan et al., 2023b;a).
Section 6 introduces a method of reducing the dimensionality of the mirror maps without affecting the ex-
pressivity, by exploiting the symmetries in the considered functions, with applications to training mirror
maps on neural networks. Moreover, the lower dimensionality allows us to consider parameterizations of
mirror maps with exact inverses, bypassing the forward-backward inconsistency problem. We utilize equiv-
ariance to extend a simple version of LMD to train deep neural networks in Section 7, where we demonstrate
competitive performance with Adam on non-convex problem classes.
2 Learned Accelerated MD
In this section, we first present a mirror descent analog to the Nesterov accelerated gradient descent scheme
(Nesterov, 1983). The resulting scheme, named accelerated mirror descent (AMD), can be considered as
a discretization of a coupled ODE. Krichene et al. (2015) show accelerated convergence rates of the ODE,
which translate to a discrete-time algorithm after a particular discretization. The analysis and convergence
rates of AMD are similar to the ODE formulation for Nesterov accelerated gradient descent given in Su et al.
(2014), where both papers show O(1/k2)convergence rate of function value.
By considering the discrete AMD iterations with inexact mirror maps, we then generalize this algorithm to
the approximate case, given in Algorithm 1, and provide the corresponding convergence rate bound. This
can then be applied in the case where the forward and backward mirror maps are given by neural networks,
leading to convergence results for learned accelerated mirror descent (LAMD). Our main contribution in this
section is Theorem 1, which shows convergence of the function value to the minimum, rather than to the
minimum plus a constant.
Algorithm 1 Accelerated mirror descent (AMD) with approximate mirror updates
Require: ˜x(0)= ˜z(0)=x(0), step-sizes tk, parameter r≥3
1:fork∈Ndo
2:x(k+1)=λk˜z(k)+ (1−λk)˜x(k)withλk=r
r+k
3: ˜z(k+1)=∇M∗
ϑ(∇Mθ(˜z(k))−ktk
r∇f(x(k+1)))
4: ˜x(k+1)= arg min˜x∈Rnγtk/angbracketleftbig
∇f(x(k+1)),˜x/angbracketrightbig
+R(˜x,x(k+1))
5:end for
Step 3 of Algorithm 1 is the mirror descent step of AMD, where ˜z(k)represents a (computable) approximate
mirror descent iteration. The additional step in Step 4 arises as a correction term to allow for convergence
analysis (Krichene et al., 2015). Here, Ris a regularization function where there exists 0< ℓR≤LRsuch
6Published in Transactions on Machine Learning Research (05/2024)
that for all x,x′∈X, we have
ℓR
2∥x−x′∥2≤R(x,x′)≤LR
2∥x−x′∥2.
For practical purposes, Rcan be taken to be the Euclidean distance R(x,x′) =1
2∥x−x′∥2, in which case
ℓR=LR= 1. TakingRof this form reduces Step 4 to the following gradient descent step:
˜x(k+1)=x(k+1)−γtk∇f(x(k+1)).
Similarly to Tan et al. (2023b), we additionally define a sequence ˆz(k)that comprises the true mirror descent
updates, applied to the intermediate iterates ˜z(k):
ˆz(k+1)= (∇Mθ)−1/parenleftbigg
∇Mθ(˜z(k))−ktk
r∇f(x(k+1))/parenrightbigg
. (5)
This additional variable will allow us to quantify the approximation error, with which we will show the
approximate convergence. For ease of notation, we will denote the forward mirror potential by ψ=Mθ.
With this notation, ψ∗is the convex conjugate, satisfying ∇ψ∗= (∇ψ)−1. The true mirror descent aux-
iliary sequence can be written as ˆz(k+1)=∇ψ∗/parenleftbig
∇ψ(˜z(k))−ktk
r∇f(x(k+1))/parenrightbig
. We begin with the following
lemma, which bounds the difference between consecutive energies to be the sum of a negative term and an
approximation error term.
Lemma 1. Consider the approximate AMD iterations from Algorithm 1, and let ˆz(k)be the exact MD
iterates given by Equation 5. Assume ψ∗isLψ∗-smooth with respect to a reference norm ∥·∥∗on the dual
space, i.e. Bψ∗(z,y)≤Lψ∗
2∥z−y∥2
∗, or equivalently that ∇ψ∗isLψ∗-Lipschitz. Assume further that there
exists 0< ℓR≤LRsuch that for all x,x′∈X,ℓR
2∥x−x′∥2≤R(x,x′)≤LR
2∥x−x′∥2. Define the energy
˜E(k)fork≥0as follows (where t−1= 0):
˜E(k):=k2tk−1
r/parenleftig
f(˜x(k))−f∗/parenrightig
+rBψ∗/parenleftig
∇ψ(˜z(k)),∇ψ(x∗)/parenrightig
. (6)
Assume the step-size conditions γ≥LRLψ∗, andtk≤lR
Lfγ. Then the difference between consecutive energies
satisfies the following:
˜E(k+1)−˜E(k)≤(2k+ 1−rk)tk
r/parenleftig
f(˜x(k+1))−f∗/parenrightig
−k2(tk−1−tk)
r/parenleftig
f(˜x(k))−f∗/parenrightig
+r/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
.
Proof.Deferred to Appendix A.1.
Lemma 1 gives us a way of telescoping the energy ˜E. To turn this into a bound on the objective values
f(˜x(k))−f∗, we need the initial energy. The following proposition gives a bound on the energy ˜E(1), which
when combined with telescoping with Lemma 1, will be used to derive bounds on the objective.
Proposition 1. Assume the conditions as in Lemma 1. Then
˜E(1)≤rBψ∗(∇ψ(x(0)),∇ψ(x∗)) +t0
r(f(x(0))−f∗) +r/angbracketleftig
∇ψ(˜z(1))−∇ψ(ˆz(1)),˜z(1)−x∗/angbracketrightig
(7)
Proof.The proof relies on bounding f(˜x(1))−f∗, which is done identically to that in Krichene et al. (2015).
Deferred to Appendix A.2.
Putting together Lemma 1 and Proposition 1, we get the following theorem that bounds the objective value
of the approximate AMD iterates.
7Published in Transactions on Machine Learning Research (05/2024)
Theorem 1. Assume the conditions as in Lemma 1. Assume also that the step-sizes (tk)∞
k=0are non-
increasing, and that the approximation error term, given by
/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
, (8)
is uniformly bounded by a constant M > 0. If our step-sizes are chosen as tk= Θ(k−c)forc∈[0,1), then
we getO(kc−1)convergence in objective value.
Proof.Summing the expression in Lemma 1, noting the first two terms are non-positive for k≥1sincetkis
non-increasing and r≥3, and noting that the final term is bounded by rM,
˜E(k+1)≤˜E(1)+krM.
By definition of ˜Eand non-negativity of Bψ∗, we also have
f(˜x(k+1))−f∗≤r
(k+ 1)2tk˜E(k+1)≤r
(k+ 1)2tk[˜E(1)+krM ] =O(kc−1). (9)
Remark 1. The condition that Equation (8)is bounded can be interpreted in terms of more natural func-
tional bound on the mirror potentials Mθ,M∗
ϑ. Indeed, the LHS is simply (∇Mθ◦∇M∗
ϑ−I)(∇Mθ(˜z(k))−
(ktk/r)∇f(x(k+1))). Therefore, if∥∇Mθ◦∇M∗
ϑ−I∥is uniformly bounded (over the iterates in the dual
space) and the iterates are bounded, by Cauchy-Schwarz, the approximation error term Equation (8)will also
be bounded.
This theorem shows that we are actually able to obtain convergence to the function objective minimum by
using acceleration, as opposed to the minimum plus a constant given in Tan et al. (2023b). There are two
main ideas at play here. One is that the acceleration makes the function value decrease fast enough, as
seen by the energy given in Equation 6 growing as O(k), while having a k2tkfactor in front of the objective
difference term. The second idea is that the λkfactor in Step 2 of Algorithm 1 decreases the importance of
the mirror descent step, which thus reduces the effect of the forward-backward inconsistency.
3 Learned Stochastic MD
Stochasticmirrordescent(SMD)ariseswhenweareabletosamplegradients G(xk,ξk)suchthattheexpecta-
tion of Gis equal to a subgradient of our convex objective f. For example, in large-scale imaging applications
such as CT, computing the forward operator may be expensive, and a stochastic gradient may consist of
computing a subset of the forward operator. Another example would be neural network training over large
data sets, where there is insufficient memory to keep track of the gradients of the network parameters over
all of the given data. Therefore, we wish to extend the analysis for approximate mirror descent to the case
where we only have access to stochastic gradients.
While it may be tempting to put the stochastic error into the approximation error terms of the previous
analyses, stochastic errors may be unbounded, violating the theorem assumptions. This section will extend
approximate MD to the case where the approximation error is now the sum of a bounded term and a zero-
mean term with finite second moments. In particular, Theorem 2 shows an expected optimality gap bound
depending on the stochastic noise based on bounded variance and bounded error assumptions.
We base the analysis of this section on the robust stochastic approximation approach (Nemirovski et al.,
2009). We require two additional assumptions in this setting as follows:
Assumption 1. We can draw i.i.d. samples ξ(0),ξ(1),...of a random vector ξ∈Ξ.
Assumption 2. An oracle Gexists for which, given an input (x,ξ)∈X× Ξ, returns a vector G(x,ξ)such
thatg(x):=Eξ[G(x,ξ)]is well defined and satisfies g(x)∈∂f(x).
8Published in Transactions on Machine Learning Research (05/2024)
Iffcan be written as an expectation f(x) =E[F(x,ξ)], where F(·,ξ)is convex with fhaving finite values in
a neighborhood of a point x, then we can interchange the subgradient with the expectation (Strassen, 1965),
∂f(x) =E[∂xF(x,ξ)].
Stochastic MD thus generalizes MD by replacing the subgradient ∇fat each step with this gradient oracle
G, which can be written as follows
x(k+1)=∇ψ∗/parenleftig
∇ψ(x(k))−tkG(x(k),ξ(k))/parenrightig
. (10)
Observe that under this formulation, we can additionally encode a noise component that arises as a result
of inexact mirror descent computation. Therefore, we may redefine G(x,ξ)as an inexact stochastic oracle
as in the introduction, having two components
G(x,ξ) =g(x) + ∆(x,ξ) +U(x),
where ∆(·,ξ)signifies stochastic noise satisfying E[∆(x)] =0, andU(x)is a deterministic error to model the
approximation error of computing MD steps. We will use the notation ∆k= ∆(x(k),ξ(k)), Uk=U(x(k))to
signify these values at each iteration.
Algorithm 2 Stochastic mirror descent (SMD) (Nemirovski et al., 2009)
Require:x(0)∈X, step-sizes tk>0, stochastic oracle G, random i.i.d. ξ(k)
1:fork∈Ndo
2:x(k+1)=∇ψ∗(∇ψ(x(k))−tkG(x(k),ξ(k)))
3:end for
We first reformulate the MD iterates into a “generalized proximal” form. In particular, a small modification
of the argument in Beck & Teboulle (2003) shows that the SMD iterations given in Equation (10) can be
written as follows:
x(k+1)= arg min
x′∈X/braceleftbigg/angbracketleftig
G(x(k),ξ(k)),x′/angbracketrightig
+1
tkBψ(x′,x(k))/bracerightbigg
. (11)
This can be written in terms of the prox-mapping Px: (Rn)∗→X, defined as follows:
Px(y) = arg min
x′∈X{⟨y,x′⟩+Bψ(x′,x)}, (12a)
x(k+1)=Px(k)/parenleftig
tkG(x(k),ξ(k))/parenrightig
. (12b)
The following result gives optimality gap bounds of approximate SMD in the deterministic-plus-stochastic
noise setting. In particular, the expected ergodic average is able to attain a loss that is a constant over the
minimum.
Theorem 2. Consider the approximate SMD iterations x(i)generated by Equation 10. Suppose that the
stochastic oracle satisfies E[∥G(x,ξ)∥2
∗]≤σ2for allx∈Xfor someσ≥0. Letx∗be some minimizer of f.
1. If⟨Ui,x(i)−x∗⟩is uniformly bounded by C, then the expected loss satisfies
E/bracketleftiggk/summationdisplay
i=0ti[f(x(i))−f∗]/bracketrightigg
≤Bψ(x∗,x(0)) +σ2
2αk/summationdisplay
i=0t2
i+Ck/summationdisplay
i=0ti. (13)
2. Iffisµ-strongly convex and ∥Ui∥2
∗is uniformly bounded by C′, the expected loss satisfies
E/bracketleftiggk/summationdisplay
i=0ti[f(x(i))−f∗]/bracketrightigg
≤Bψ(x∗,x(0)) +σ2
2αk/summationdisplay
i=0t2
i+C′
2µk/summationdisplay
i=0ti. (14)
9Published in Transactions on Machine Learning Research (05/2024)
In particular, the ergodic average defined by
˜xk
0=k/summationdisplay
i=0γix(i), γi=ti/summationtextk
i=0ti
satisfies respectively
E/bracketleftbig
f(˜xk
0)−f∗/bracketrightbig
≤/parenleftigg
Bψ(x∗,x(0)) +σ2
2αk/summationdisplay
i=0t2
i/parenrightigg/slashig/parenleftiggk/summationdisplay
i=0ti/parenrightigg
+C, (15)
E/bracketleftbig
f(˜xk
0)−f∗/bracketrightbig
≤/parenleftigg
Bψ(x∗,x(0)) +σ2
2αk/summationdisplay
i=0t2
i/parenrightigg/slashig/parenleftiggk/summationdisplay
i=0ti/parenrightigg
+C′
2µ. (16)
Proof.Deferred to Appendix B.
Remark 2. A stronger assumption to condition (1) in Theorem 2 is to assume uniformly bounded iterates,
as well as uniformly bounded deterministic errors Ui. In LMD, deterministic errors arise due to mismatches
in the learned mirror maps ∇M∗
ϑ≈(∇Mθ)−1, which can be controlled using a soft penalty.
We observe in Equations (15) and (16) that ℓ2–ℓ1summability (where/summationtextt2
i<+∞but/summationtextti= +∞) is a
sufficientconditiontoremovethecontributionofthefirsttermtotheergodicaverageerror. Thisisconsistent
with the empirical observation in Tan et al. (2023a) that extending the learned step-sizes using a reciprocal
ruletk=c/kgives the best convergence results.
4 Learned Accelerated Stochastic MD
In the previous sections, we considered accelerated and stochastic variants of mirror descent and presented
results pertaining to the convergence of these algorithms with noisy mirror maps. These two variants improve
different parts of MD, with acceleration improving the convergence rates, while stochasticity improves the
computational dependency on the gradient. Indeed, one of the drawbacks of SMD is the slow convergence
rate ofO(1/√
k)in expectation, where the constant depends on the Lipschitz constant of f. Acceleration as
a tool to counteract this decrease in convergence rate for SMD has recently been explored, with convergence
results such as convergence in high probability (Ene & Nguyen, 2022; Lan, 2020) and in expected function
value (Xu et al., 2018; Lan, 2012). These approaches decouple the gradient and stochastic noise, resulting in
O(L/k2+σ/√
k)convergence rate, where Cdepends on the Lipschitz constant of ∇f, andσ2is the variance
of the stochastic gradient.
We note that it is possible to extend Algorithm 1 (approximate AMD) to the stochastic case by directly
replacing the gradient ∇fwith a stochastic oracle G, albeit currently without convergence guarantees. In
this section, we consider another version of approximate accelerated SMD that comes with convergence
guarantees, using a slightly different proof technique.
For our analysis, we follow the accelerated SMD setup in Xu et al. (2018), in particular of Algorithm 2,
replicated as below. Suppose that fis differentiable and has Lf-Lipschitz gradient, and let G(x,ξ) =
∇xF(x,ξ)be a stochastic gradient satisfying
E/bracketleftig
G(x(k),ξ(k))/bracketrightig
=∇f(x(k)),E/bracketleftig
∥G∥2
∗|x(k)/bracketrightig
≤σ2.
For simplicity, we let ∆k=G(x(k),ξ(k))−∇f(x(k))denote the zero mean stochastic component.
An inexact convex conjugate can be modeled using a noise term in Step 3. With a noise term U, the step
would instead read
x(k+1)=τk
τk+ 1/bracketleftig
∇ψ∗(y(k)) +U(y(k))/bracketrightig
+1
τk+ 1x(k). (17)
10Published in Transactions on Machine Learning Research (05/2024)
Algorithm 3 Accelerated stochastic mirror descent (ASMD) (Xu et al., 2018, Alg. 2)
Require: Inputsx(0)∈X,y(0)=∇ψ(x(0))∈X∗, A0=s0= 1/2,
1:fork∈Ndo
2:Ak+1= (k+ 1)(k+ 2)/2, τk= (Ak+1−Ak)/Ak, sk= (k+ 1)3/2
3:x(k+1)=τk
τk+1∇ψ∗(y(k)) +1
τk+1x(k)
4:y(k+1)=y(k)−Ak+1−Ak
skG(x(k+1),ξ(k+1))
5:end for
The corresponding optimality conditions become
∇ψ∗(y(k)) =x(k+1)+Ak
Ak+1−Ak(x(k+1)−x(k))−U(k), (18)
y(k+1)−y(k)=−Ak+1−Ak
skG(x(k+1),ξ(k+1)). (19)
We can perform a similar convergence analysis using the energy function given in Xu et al. (2018). Let
E(k)=E[Ak(f(x(k))−f(x∗))+skBψ(x∗,∇ψ∗(y(k)))]. Werequireaboundonthediameteroftheoptimization
domain, as stated in the following assumption.
Assumption 3. There exists a constant Mψ>0such thatMψ= supx,x′∈XBψ(x,x′).
Theorem 3. SupposefhasLf-Lipschitz gradient, the mirror map is α-strongly convex, the approximation
errorU(k)is bounded, and that the stochastic oracle is otherwise unbiased with bounded second moments.
Assume Assumption 3 holds, and suppose further that there exists a constant Ksuch that for every iterate
x(k), we have
⟨∇f(x(k)),U(k)⟩≤K.
Then the convergence rate of approximate ASMD is,
E[f(x(k))−f(x∗)]≤K+E(0)/Ak+(k+ 1)3/2
Ak/bracketleftigg
Mψ+8L2
fMψ+ 2ασ2+ 4∥∇f(x∗)∥2
∗
α2/bracketrightigg
(20)
=K+O(k−2+k−1/2).
Proof.Deferred to Appendix C.
Theorem 3 thus extends the classical O(k−2+k−1/2)convergence rate for accelerated stochastic mirror
descent to the approximate case, up to a constant depending on the approximation error. We note that f
having bounded gradients is sufficient to satisfy the second assumption that ⟨∇f(x(k)),U(k)⟩is bounded, as
we have also assumed that U(k)is bounded.
5 Experiments
In this section, we demonstrate the performance of the proposed algorithms on image reconstruction and
various training tasks. As we will be using neural networks to model the mirror potentials, we refer to the
three proposed algorithms as learned AMD (LAMD), learned SMD (LSMD), and learned ASMD (LASMD)
for the algorithms proposed in Sections 2 to 4 respectively with the learned mirror potentials. We will
primarily consider GD, Nesterov accelerated GD, and the Adam optimizer as baselines for these tasks, with
SGDwherestochasticityisapplicable. Agridsearchwasusedtofindthebaselineoptimizerstep-sizes, chosen
to minimize the loss at iteration 100. Specific details for the choice of baseline step-sizes can be found in
Appendix E. In the image reconstruction experiments, proximal operators are easily available, which allows
us to compare with a learned primal-dual (LPD) method, given by Algorithm 4.4 in Banert et al. (2020).
This LPD scheme can be interpreted as a generalization of the PDHG and Douglas–Rachford algorithms.
11Published in Transactions on Machine Learning Research (05/2024)
For fairness, we use pre-trained mirror maps as given in Tan et al. (2023b;a), and replicate selected exper-
iments from these works to review the convergence behavior as compared to LMD. The mirror maps were
trained to minimize a loss function of the form
˜L(θ,ϑ) =N/summationdisplay
k=1/bracketleftig
f(˜x(i)
k) +sk∥(∇M∗
ϑ◦∇Mθ−I)(˜x(i)
k)∥/bracketrightig
, (21)
where ˜x(i)
kare the approximate MD iterations as in Equation (4), and the mirror maps are tied across
each iteration. The loss ˜Lis averaged over a family of training objectives f, such as TV-regularized image
reconstruction or SVM hinge losses, and the regularization parameter skis increased as training progresses.
The maximum unrolled iterations is set to N= 10, and the LMD step-sizes are also learned for these
iterations (Tan et al., 2023b).
All implementations were done in PyTorch, and training was done on Quadro RTX 6000 GPUs with 24GB
of memory (Paszke et al., 2019).
5.1 Numerical Considerations and Implementation
For numerical stability, we additionally store the dual variables ∇Mθ(x(k)), similarly to ASMD in Algo-
rithm 3. This avoids computing ∇Mθ◦∇M∗
ϑ, which would be the identity in the exact MD case where M∗
ϑ
is the convex conjugate of Mθ, and appears to be a source of instability. For example, the current LMD
scheme (with step-size t) computes
x(k+1)=∇M∗
ϑ/parenleftig
∇Mθ(x(k))−t∇f(x(k))/parenrightig
. (22)
We propose to replace this with updates in the dual
y(k+1)=y(k)−t∇f(∇M∗
ϑ(y(k))), y(k)=∇Mθ(x(k)). (23)
In the case where ∇M∗
ϑ= (∇Mθ)−1, both schemes correspond exactly to mirror descent. The key dif-
ference is that the inconsistency is now within the ∇fwhich would heuristically be close to 0 around
the minimum, instead of on the entire dual term. To put this in terms of forward-backward error, let
ˆx(k+1)=∇M−1
θ/parenleftbig
∇Mθx(k)−t∇f(x(k))/parenrightbig
be the exact mirror update on x(k)with mirror potential Mθ. The
forward-backward inconsistencies ∇Mθ(x(k+1))−∇Mθ(ˆx(k+1))for Equation (22) and Equation (23) would
then be given respectively as
∇Mθ(x(k+1))−∇Mθ(ˆx(k+1)) = (∇M∗
ϑ−(∇Mθ)−1)/parenleftig
∇Mθx(k)−t∇f(x(k))/parenrightig
for the primal update, (24)
∇Mθ(x(k+1))−∇Mθ(ˆx(k+1)) =t∇f(x(k))−t∇f(∇M∗
ϑ◦∇Mθx(k)) for the dual update. (25)
By pulling the difference between ∇M∗
ϑand(∇Mθ)−1into the dual term, we empirically observe a much
lower forward-backward error, which would correspond to smaller constants and tighter convergence bounds
for our presented theorems. We formalize the proposed learned AMD (LAMD) with R(x,x′) =1
2∥x−x′∥2,
learned SMD (LSMD), and learned ASMD (LASMD) algorithms with this dual update modification in the
following Algorithms 4 to 6.
Sincethe originalpre-trained models onlyhavestep-sizes available upto N= 10, weneed tochoose step-sizes
to continue optimizing. We consider running LAMD, LSMD and LASMD for 2000 iterations with various
choices of step-size. In particular, we consider three constant step-sizes tk= ˜c, as well as step-size regimes of
the form and tk=c/kandtk=c′/√
kfor LAMD and LSMD. These step-sizes were derived from step-sizes
given from the pre-trained models, given for the image denoising, image inpainting and SVM experiments
in Tan et al. (2023b;a). In particular, for the constant step-size extensions, we consider taking ˜cto be the
mean, minimum and final learned step-size, i.e. c=/summationtext10
i=1ti/10,c= min 1≤i≤10ti,c=t10respectively. For
12Published in Transactions on Machine Learning Research (05/2024)
the reciprocal step-size extensions tk=c/k, we compute cby taking the average c=/summationtext10
i=1iti. For root-
reciprocal extensions tk=c′/k, we similarly take c′=/summationtext10
i=1i√ti. These extensions act as best-fit constants
for the given learned step-sizes.
Recall that the conditions of convergence for AMD and SMD desire a non-increasing step-size condition.
Moreover, convergence of the ergodic average for SMD requires a ℓ2–ℓ1summability condition, which is
satisfied by the tk=c/kextension. We will demonstrate the behavior of these algorithms under both
constant, reciprocal and root-reciprocal step-size regimes.
We note that while ASMD does not have a step-size in its definition, we can artificially introduce a step-size
by adding a step-size in the dual update step in Step 4 of Algorithm 3, as
y(k+1)=y(k)−tkAk+1−Ak
skG(x(k+1),ξ(k+1)).
In the case where tk=tis constant, this can be interpreted as instead running ASMD without this step-size
modification on the scaled function tf. Therefore, we still get convergence guarantees, up to a constant
multiple factor.
Algorithm 4 Learned AMD (LAMD)
Require: Input ˜x(0)= ˜z(0)=x(0)∈X, parameter r≥3, step-sizes tk, number of iterations K
1:z(0)=∇Mθ(˜z(0))
2:fork= 0,...,Kdo
3:x(k+1)=λk∇M∗
ϑ(z(k)) + (1−λk)˜x(k)withλk=r
r+k
4:z(k+1)=z(k)−ktk
r∇f(x(k+1))
5: ˜x(k+1)=x(k+1)−γtk∇f(x(k+1))
6:end for
7:returnx(K+1)=λK∇M∗
ϑ(z(K)) + (1−λK)˜x(K)
Algorithm 5 Learned SMD (LSMD)
Require: Inputx(0)∈X, step-sizes tk, number of iterations K, stochastic gradient oracle G
1:y(0)=∇Mθ(x(0))
2:fork= 0,...,Kdo
3:y(k+1)=y(k)−tkG(∇M∗
ϑ(y(k)),ξ(k))
4:end for
5:returnx(K+1)=∇M∗
ϑ(y(K+1))
Algorithm 6 Learned ASMD (LASMD)
Require: Inputx(0)∈X, A0=s0= 1/2, step-sizes tk, number of iterations K
1:y(0)=∇Mθ(x(0))
2:fork= 0,...,Kdo
3:Ak+1= (k+ 1)(k+ 2)/2, τk= (Ak+1−Ak)/Ak, sk= (k+ 1)3/2
4:x(k+1)=τk
τk+1∇M∗
ϑ(y(k)) +1
τk+1x(k)
5:y(k+1)=y(k)−tkAk+1−Ak
skG(x(k+1),ξ(k+1))
6:end for
7:returnx(K+1)=τK
τK+1∇M∗
ϑ(y(K)) +1
τK+1x(K)
5.2 Image Denoising
We consider image denoising on an ellipse data set, with images of size 128×128, where we apply LAMD,
LSMD and LASMD with pre-trained mirror maps given in Tan et al. (2023a). The mirror maps ∇Mθ:
13Published in Transactions on Machine Learning Research (05/2024)
(a) Clean phantom
 (b) Noisy phantom
 (c) True TV reconstruction
Figure 1: Example of the ellipse phantom data, as well as the reconstruction gained by using gradient descent
on the TV function.
X →X∗,∇M∗
ϑ:X∗→Xare modelled using input-convex neural networks (ICNNs) (Amos et al., 2017),
and are trained to minimize the function values while also minimizing the forward-backward inconsistency ,
which is defined to be (∇M∗
ϑ◦∇Mθ−I)(x(k)). The training data used for the pre-trained mirror map are
noisy ellipse phantoms in X-ray CT, generated using the Deep Inversion Validation Library (DIVal) (Wang
& Zhou, 2006; Leuschner et al., 2019). The phantoms were generated in the same manner as in Tan et al.
(2023a). The target functions to optimize were given by TV model-based denoising,
F=/braceleftbig
f(x) =∥x−y∥2
2+λ∥∇x∥1/bracerightbig
,
whereyare noisy phantoms, and ∇xis the pixel-wise discrete gradient, with the regularization parameter
λ= 0.15.
Figure 1 presents some of the noisy ellipse data that we aim to reconstruct using a TV model. We observe
that the noise artifacts are non-Gaussian and have some ray-like artifacts. We model the stochasticity by
artificially adding Gaussian noise to the gradient G(x,ξ)−g(x) = ∆(x,ξ)∼N(0,σ2In×n), withσ= 0.05.
Figure 2 demonstrates the effect of extending the learned step-sizes for LMD, LAMD, LSMD and LASMD,
compared to the baseline optimization algorithms GD, Nesterov accelerated GD, and Adam, and LPD.
We observe that step-sizes following tk=c/kperform well for LAMD and LSMD, which is consistent
with observations made in Tan et al. (2023a), Theorem 2. For LASMD, we observe that a constant step-
size extension choice is optimal. We also observe the poor performance of Adam, for which various non-
convergence results are known for convex settings (Reddi et al., 2018; Zou et al., 2019). The plateauing
phenomenon with the baselines can not be avoided in this setting, with smaller step-sizes delaying the
“decrease phase” and lowering the eventual plateau. Moreover, LPD also plateaus, possibly due to non-
generalization of the learned method to higher iteration counts. We will use the aforementioned optimal
step-size extensions as found in Figure 2 to extend step-sizes for the following experiments.
5.3 Image Inpainting
We additionally consider the image inpainting setting in Tan et al. (2023b). The images to reconstruct are
STL-10 images, which are 96×96color images. The images are corrupted with a fixed 20% missing pixel
maskZ, then 5% Gaussian noise is added to all color channels to get the noisy image. The function class
we optimize over is the class of TV-regularized variational forms, with regularization parameter λ= 0.15as
chosen in Tan et al. (2023b), given by
F=/braceleftbig
f(x) =∥Z◦(x−y)∥2
2+λ∥∇x∥1/bracerightbig
,
whereyare images corrupted with missing pixels. We use mirror maps that are pre-trained with the training
regime given in Tan et al. (2023b), and measure their performance when applied with LAMD, LSMD and
LASMD as compared to MD.
We first note that this setting does not admit a stochastic interpretation, and thus we set the artificial noise
in LSMD and LASMD to zero. Notice that in this case, LSMD updates revert to mirror descent updates.
The main difference between the LSMD that we will present here with the LMD used in Tan et al. (2023b)
is that in this work’s computations, the dual iterates are saved, as described at the start of Section 5.
14Published in Transactions on Machine Learning Research (05/2024)
100101102103
Iteration101
100101102103104f(x(k))f*
Mean
Min
Final
c/k
c/prime/k
2c/k
c/2kReconstruction Loss
(a) LAMD
100101102103
Iteration101
100101102103104f(x(k))f*
Mean
Min
Final
c/k
c/prime/k
2c/k
c/2kReconstruction Loss (b) LSMD
100101102103
Iteration101
100101102103104f(x(k))f*
Mean
Min
Final
c/k
c/prime/k
2c/k
c/2kReconstruction Loss
(c) LASMD.
100101102103
Iteration101
100101102103104f(x(k))f*
GD
Nesterov
Adam
LPD
LAMD
LSMD
LASMDReconstruction Loss (d) Comparison with baselines.
Figure 2: Evolution of losses for the proposed LAMD, LSMD and LASMD methods with various step-size
extensions (a-c), as well as baselines, evaluated over 2000 iterations for TV model-based denoising of noisy
ellipse phantoms. We observe that reciprocal step sizes generally work well for LAMD and LSMD, with
larger step-sizes (given by the “mean” plot) resulting in non-convergence to minima. Constant step-sizes
work better for LASMD, which can be attributed to the already decaying step-sizes in LASMD. We observe
that the learned MD methods exhibit similar convergence behavior, which may be due to the relative strong
convexity of the underlying optimization problem.
15Published in Transactions on Machine Learning Research (05/2024)
100101102103
Iteration100101102103f(x(k))f*
GD
Nesterov
Adam
LPD
LAMD
LSMD
LASMD
LMDReconstruction Loss
Figure 3: Plot of the function optimality gap f(x(k))−f∗for 2000 iterations of various MD methods for
image inpainting. The step-size regime is reciprocal for LAMD, LSMD and LMD, and constant for LASMD.
We observe that each of LAMD, and LASMD both exhibit approximately O(k−2)convergnce. LMD and
LSMD exhibit approximately O(k−1)convergence, with instability around iteration 100 for LMD causing the
increase in function value. As we are not applying added stochastic noise in this setting, the only difference
between LSMD and LMD is that the dual variables are stored as described in Section 5, which allows the
LSMD iterates to stay bounded.
For comparison with baselines, we apply the proposed methods to optimize a function that arises from TV-
based variational model for a single noisy masked image from the STL-10 test data set. The step-sizes were
chosen by considering the best step-size extension out of those described at the start of Section 5, which
were reciprocal step-size extensions for LAMD, LSMD and LMD, and constant step-size for LASMD given
by the mean of the learned step-sizes. To compute the global minimum of the optimization problem, we run
gradient descent for 15000 iterations with a step-size of 5×10−4, followed by another 5000 iterations with
a step-size of 1×10−4.
Figure 3 demonstrates approximately that LAMD and LASMD are able to achieve O(k−2)convergence.
Moreover, we see the effect of storing the dual iterates instead of the primal iterates, as the LSMD con-
verges without the divergent behavior of LMD. Moreover, the convergence rate of LAMD is faster than is
suggested by Theorem 1 for step-size tk= Θ(k−1). This suggests that the approximation error term given
in Equation (8),/angbracketleftig
˜z(k+1)−x∗,∇ψ(˜z(k+1))−∇ψ(ˆz(k+1))/angbracketrightig
,
is decaying as well. This could be attributed to the iterate approaching the global minimum such that the
first term ˜z(k+1)−x∗is decaying. We additionally observe the speedup of LAMD compared to Nesterov
accelerated GD and Adam in the earlier iterates, coming from the learned mirror maps, as well as better
generalization performance to higher iterations as compared to LPD.
5.4 SVM Training
To demonstrate an application of stochasticity, we consider training a support vector machine (SVM) over a
dataset with many samples. Similarly to Tan et al. (2023b), the task is to classify the digits 4 and 9 from the
MNIST data set. A 5-layer neural network was used to compute feature vectors ϕi∈R50from the MNIST
images for classification. The SVM training problem in this case is to find weights w∈R50and a biasbsuch
16Published in Transactions on Machine Learning Research (05/2024)
100101102103
Iteration103
102
101
100101102103104f(x(k))f*
GD
Nesterov
Adam
LAMD
LSMD
LASMD
LMD
SGDHinge Loss
(a) SVM hinge loss
100101102103
Iteration102
101
ErrorGD
Nesterov
Adam
LAMD
LSMD
LASMD
LMD
SGDT est classification error (b) Test prediction error
Figure 4: Plots of the SVM hinge loss and test accuracy for the various optimization methods tested.
We observe that LAMD is able to outperform the baselines and SGD in terms of hinge loss, without the
plateauing behavior of Nesterov accelerated GD. We additionally observe that the end fluctuations of LSMD
are smaller due to the reciprocal step-size regime chosen. However, SGD is able to obtain solutions that
have lower classification error.
that for a given set of features and targets (ϕi,yi)i∈I∈R50×{± 1}, the prediction sign(w⊤ϕi+b)matches
yifor most samples. This can be reformulated into a variational problem as follows, where C > 0is some
positive constant,
min
w,b1
2w⊤w+C
|I|/summationdisplay
i∈Imax(0,1−yi(w⊤ϕi+b)). (26)
Using the pre-trained neural network, we train SVMs using the training fold of MNIST using the 4 and 9
classes, which consists of 11791 images. For testing the generalization accuracy of these methods, we use the
4 and 9 classes in the testing fold, which consists of 1991 images. For the stochastic methods, we consider
using a batch-size of 500 random samples from the testing fold, and the same for stochastic gradient descent.
To train the SVMs, full-batch gradient is used for LAMD, LMD, GD and Nesterov accelerated GD. Batched
gradients with batch-size 500 were used for LSMD, LASMD and SGD. We consider the same number of
optimization for both the full-batch methods and the stochastic variants. This is slightly different from
typical learned schemes where if the data is batched, then more training iterations have to be taken to keep
the number of “epochs” the same.
The optimization problem and initialization was taken to be the same as in Tan et al. (2023b), where 100
SVMs are initialized using (w,b)∼N (0,I50+1). Each SVM has its parameters optimized with respect to
the hinge loss (26) using the various optimization methods. As in the previous section, we consider running
each of the methods with all the step-size choices, and present the best choice for comparison. We note that
the LPD scheme is not applicable in this setting as the sum in (26) does not admit an easily computable
proximal term.
Figure 4 compares the losses on the entire training set, as well as the generalization error of the computed
SVMs on the test set. The step-size regimes chosen are reciprocal for LAMD, LSMD and LMD, and constant
for LASMD and SGD. LSMD is able to perform on par with full-batch GD despite only having access
to stochastic gradients, until eventually plateauing around the same loss as SGD. We observe significant
acceleration of LAMD compared to all the compared methods. LAMD also has almost identical final test
classification error to the baseline methods.
17Published in Transactions on Machine Learning Research (05/2024)
6 Equivariant MD for NNs
In the previous sections, we demonstrated some applications of LMD for convex optimization. However, the
mirror map models used still require hundreds of thousands of parameters. Compared to classical optimizers
such as SGD or Adam with less than 10 hyperparameters, this is a significant limitation in scaling LMD to
complex problems such as training neural networks. In this section, we propose to remedy this by reducing
the number of learnable parameters for LMD while retaining expressiveness. This is done by exploiting
symmetries of the problems at hand, sometimes known as equivariance in the literature.
Equivariance is a group-theoretic concept, where a group action commutes with a function application. In
practical terms, symmetries can be exploited to reduce problem complexity. For example, translations and
rotations of medical histopathology or electron microscopy images can also be considered as images from the
same distribution. Bekkers et al. (2018) exploit this to construct convolutional neural networks exploiting
this symmetry to achieve improved performance on various medical imaging segmentation tasks.
In the context of L2O, we aim to exploit symmetries to construct simpler or faster optimizers. Equivariance
for neural networks have recently been explored, from both an external viewpoint of optimization or training
losses,aswellastheinternalviewpointoflearnedfilters. Lenc&Vedaldi(2015)considerusingequivarianceto
interpret the filters learned in a convolutional neural network. Chen et al. (2021) consider using equivariance
in the unsupervised learning setting, using known equivariance properties to construct virtual operators and
drastically increase the reconstruction quality. A major field is the usage of equivariant neural networks
that enforce some type of group symmetry by using specific network architectures and activations, used in
inverse problems and graph learning problems (Celledoni et al., 2021; Cohen & Welling, 2016; Worrall et al.,
2017). By strictly enforcing symmetry, the resulting networks are more measurement-consistent and have
better reconstruction quality. We focus on the use of equivariance for dimensionality reduction, provably
demonstrating “weight-tying” when using LMD for neural networks.
To simplify the process of training LMD for neural networks, we consider exploiting the architecture of
a neural network. In particular, the weights of a neural network typically carry permutation symmetries,
whichareexploitedinworkssuchastheNeuralTangentKernelorNeuralNetworkGaussianProcesses(Jacot
et al., 2018; Lee et al., 2018). For example, a neural network with a dense layer and layer-wise activations
between two feature layers will be invariant to permutations of the weights and feature indices. Another
example is convolutional networks, where permuting the kernels will result in a permuted feature layer. In
this section, we aim to formalize equivariance in the context of neural networks, and eventually show that we
can MD methods that are equivariant stay equivariant along their evolution. This means that we can replace
parameterizations of MD with equivariant parameterizations, reducing the number of learnable parameters
while retaining the expressivity.
Let us first formalize the notion of symmetry that we aim for equivariance with. Let Gbe a group of
symmetries acting linearly on a real Hilbert parameter space (Z,⟨·,·⟩). In other words, Gacts onZvia the
binary operator·:G×Z→Z , and moreover, for any z,z′∈Z, λ∈R, we have
g·(z+z′) =g·z+g·z′, g·(λz) =λg·z.
Assume further that Ghas an adjoint in the inner product, i.e. ⟨g·z,z′⟩=⟨z,g−1·z′⟩. One example is
whenZ=Rnequipped with the standard inner product, and Gis a subset of the orthogonal group O(n)
(the group of n×northonormal matrices) with the natural action (matrix multiplication).
LetL:Z→ Rbe a loss function for our parameters. Suppose x(0)is initialized with some distribution
x(0)∼(Z,p)that is stable under G, i.e.,p(x(0)) =p(g·x(0))∀g∈G, and further that Lis stable under the
permutations L(g·x) =L(x). For example, for a feature vector x∈Rn, the loss function L(x) =∥x∥2and
initialization distribution N(0,In)are invariant under the orthogonal group G=O(n). The loss functions
L(x) =∥x∥1,∥x∥∞and any component-wise i.i.d. distribution are invariant under the permutation group
G= Sym(n).
Recall that the objective of LMD is to minimize the loss after some number of optimization iterations (Tan
et al., 2023b). To this end, let Ω :Z→Zbe an optimization iteration. For example, gradient descent on L
18Published in Transactions on Machine Learning Research (05/2024)
can be written as Ω(z) =z−η∇L(z). The objective of LMD can be written, for example, as
min
ΩE/bracketleftiggN/summationdisplay
i=1L(zi)/bracketrightigg
.
Suppose that we optimize Ωusing some sort of gradient descent (in, say, the Sobolev space H1(Z)ofL2
functions with L2derivative), and further that Ω0satisfies
Ω0(g·z) =g·[Ω0(z)],∀g∈G,z∈Z.
This reads that Ω0isG-invariant under conjugation, where the conjugation action of Gon functions F:
Z→Zis(g·F)(z) =g·(F(g−1·z)). We equivalently denote this as G-equivariance, as gcommutes with
Ω0. For a sanity check, consider the case where Ω0is gradient descent with on the objective function L. The
following proposition says that Ω0is indeedG-equivariant under these conditions.
Proposition 2. IfΩ0is gradient descent on the G-invariant objective function L, then it is G-equivariant,
i.e.Ω0(g·z) =g·[Ω0(z)]for allg∈G,z∈Z.
Proof.Relies on the Riesz representation theorem and adjoint property. Deferred to Appendix D.1.
Suppose first that N= 1, so that we want to minimize (initialized at a G-invariant Ω = Ω 0)
L(Ω):=Ez0∼p[L(z1)] =Ez0∼p[L(Ω(z0))]. (27)
We wish to show that after optimization, Ωcommutes with g(i.e. Ωstays invariant under the conjugation
action ofGon functionsZ →Z ). To do this, we observe that the discrete differences as computed by
autograd are invariant under the group action of g. Assume further Ω0∈L2(Z,Z,p)wherepis the
initialization probability measure. The corresponding inner product is
⟨Ω,Ξ⟩L2(Z,Z,p)=Ez∼p[⟨Ω(z),Ξ(z)⟩Z].
Gnaturally acts on L2(Z,Z,p)by the conjugation action
(g·Ω)(z) =g·[Ω(g−1·z)].
To check that this conjugation is an action, first note that the action by gis isotropic since G≤O(n):
/integraldisplay
∥(g·Ω)(z)∥2dp(z) =/integraldisplay
∥(g·Ω)(z)∥2dp(z)
=/integraldisplay
∥g·(Ω(g−1(z)))∥2dp(g·z)
=/integraldisplay
∥Ω(z)∥2dp(z).
Here, we used invariance of gunderp, and also that∥g·y∥2=⟨g·y,g·y⟩=⟨y,y⟩=∥y∥2from the adjoint.
The required composition and identity properties of the action are clear. Moreover, the action is linear, and
we have the adjoint property
⟨g·Ω,Ξ⟩L2(p)=/integraldisplay
⟨g·Ω(g−1·z),Ξ(z)⟩dp(z)
=/integraldisplay
⟨Ω(g−1·z),g−1·Ξ(z)⟩dp(z)
=/integraldisplay
⟨Ω(z),g−1Ξ(g·z)⟩dp(z)
=⟨Ω,g−1·Ξ⟩.
19Published in Transactions on Machine Learning Research (05/2024)
Assume that Ω0isG-equivariant. Then the next iterate satisfies the following
Ω1= Ω 0−ηdL
dΩ(Ω0). (28)
But we also have for any gandΩ,
L(Ω) = Ez0∼p[L(Ω(z0))]
=Ez0∼p[L(g·Ω(z0))] sinceL(z) =L(g·z)
=Ez0∼p[L(g·Ω(g−1·z0))]sincep(z) =p(g·z)
=Ez0∼p[L((g·Ω)(z0))]
=L(g·Ω).
Hence if Ω0=g·Ω0for allg, then
d
dΩL(Ω0) =d
d(g·Ω)L(Ω0) (29)
Since the variation of the Ωandg·Ωis the same at Ω0, we have that g·Ω1= Ω 1, i.e. Ω1isG-equivariant.
This argument can be repeated to get that ΩnisG-equivariant (under conjugation) for all n∈N. This is
summarized in the following proposition.
Proposition 3. Let(Z,⟨·,·⟩)be a Hilbert parameter space. Suppose the following hold:
1.Ga group acting linearly on Zwith an adjoint;
2. The loss function L:Z→Rand are stable under G, that is,L(g·z) =L(z)for anyg∈Gand
z∈Z;
3. The initialization distribution z(0)∼(Z,p)is stable under G, that is, the laws p(z(0))andp(g·z(0))
coincide for any g∈G.
LetΩ0:Z→Zbe an optimization iteration that is G-equivariant, such that Ω0(g·z) =g·Ω0(z)for all
z∈Z,g∈G. IfΩ0is evolved using a gradient step (with step-size η) to minimize
L(Ω) = Ez0∼p[L(z1)],
then the evolved optimizer Ω1= Ω 0−ηdL/dΩ(Ω 0)is alsoG-equivariant.
6.1 Application: Deep Neural Networks
For the sake of exposition with a concrete example, let us consider a two-hidden-layer dense neural network
of the form
N(x;Ai,bi) =A3σ2(A2σ1(A1x+b1) +b2) +b3,
wherex∈Rd0,σiare coordinate-wise or otherwise permutation-invariant activation functions such as
softmax, and Ai∈Rdi×di−1, bi∈Rdiare the weights and biases. In this case, the parameter space will be
Z=Rd, d=3/summationdisplay
i=1(di−1di+di).
Let us consider a least-squares regression problem with data (xj,yj), j= 1,...,k, which can be written as
minimizing the loss
L(z) =k/summationdisplay
j=1[N(xj;z)−yj]2.
Suppose that the weights are initialized entry-wise i.i.d. for each Ai,bi, such as in common initializa-
tions where Ai∼N (0,η2
iIdi−1di×di−1di)orAi∼Uniform (−d−1/2
i−1,d−1/2
i−1). Consider the symmetry group
20Published in Transactions on Machine Learning Research (05/2024)
G≃Sym(d1)×Sym(d2), consisting of permutations of the intermediate feature maps with appropriate
permutations on the weights and biases, i.e. of permutations ρ1,ρ2on[d1],[d2]respectively with the group
action as
gρ1,ρ2(z)[A1]p,q= (A1)p,ρ1(q),
gρ1,ρ2(z)[A2]p,q= (A2)ρ1(p),ρ2(q),
gρ1,ρ2(z)[A3]p,q= (A3)ρ2(p),q,
gρ1,ρ2(z)[b1]q= (b1)ρ1(q),
gρ1,ρ2(z)[b2]q= (b2)ρ2(q),
gρ1,ρ2(z)[b3]q= (b3)q.
We can check directly that this group action satisfies the requirements of Proposition 3:
•(Linearity ) Permutation of elements is linear.
•(Initialization Invariance ) Follows from i.i.d. property of entries of Ai,bi.
•(Loss Equivariance ) Follows from σiacting coordinatewise and invariance of sums under permuta-
tion.
Consider the simple approach of scaling each component of the gradient by a learned constant, i.e. MD with
a mirror map of the form ψ=x⊤Dxfor some learned positive definite diagonal D. Notice that starting
withD=Iyields gradient descent, which is equivariant under G. Using Proposition 3, we find that Dwill
continue to be G-equivariant. In other words, each of the components of Dmust be fixed under permutation
actions of the form (ρ1,ρ2)∈G.
We can explicitly demonstrate this experimentally on a small-scale neural network. In particular, we consider
training a classifier using a three-layer neural network with fully connected layers on the 2D moons data set.
We arbitrarily choose 50 dimensions for the middle feature layer. The neural network takes the form
N(x;Ai,bi) =σ2(A2σ1(A1x+b1) +b2),
whereAi∈Rdi×di−1, bi∈Rdi,d0= 2,d1= 50,d2= 1. In this case, we choose σ1to be a ReLU activation
andσ2to be a log-softmax. We train using the binary cross-entropy loss. Note that in the case of a
log-softmax, which takes the form
LogSoftmax(x) i= log/parenleftigg
exp(x i)/summationtext
jexp(x j)/parenrightigg
,
the activation is still equivariant under permutations in G, as the sum of the exponentials stays the same.
Therefore, loss equivariance holds and the above theory given by Proposition 3 applies.
Using the diagonal quadratic LMD introduced above, we thus expect from the equivariance theory that the
diagonal weights in Dwill be invariant under the effect of any permutations (ρ1)on[d1]. Using superscripts
to denote the weights corresponding to each learnable neural network component, we thus have that for any
indicesi∈[d1] = [2],j,j′∈[d2] = [50],k∈[d3] = [1]:
(DA1)i,j= (DA1)i,j′
(Db1)j= (Db1)j′
(DA2)j,k= (DA2)j′,k.
Therefore, from G-equivariance of LMD with diagonal weighting D, we have that the
weights are constant across the orbits of ρ1, and are thus fully determined by the weights
21Published in Transactions on Machine Learning Research (05/2024)
510152025A1
b1
A2Componentwise weighting
Figure 5: Componentwise gradient weightings learned for a two layer neural network on 2D moons data.
Each band consists of 50 datapoints, corresponding to elements of the weight matrix Dfor each neural
network parameter group. We observe strong grouping of the weightings within each parameter group. In
particular, we find that the weightings of A1are split into two groups, which correspond to the two input
dimensions.
(DA1)1,1,(DA1)2,1,(Db1)1,(DA2)1,1,(Db2)1. This reduces the number of variables that need to be
learned from dim(D) = 201 to only 5. The equivariance effect can be clearly seen in Figure 5, which
empirically demonstrates what happens if we train all the variables of Dwithout directly enforcing
equivariance. By initializing Dto be the identity, the initial mapping Ω0:Z→Zinduced by Dis gradient
descent, which we have shown before to be equivariant under permutations of the intermediate feature layer.
The theory presented thus suggests that the optimization path of Ω(or in this case, of D), will continue to
be equivariant under such permutations. Indeed, at the end of the optimization for D, we observe that the
weights do indeed satisfy approximate equivariance, as demonstrated by the tight grouping phenomenon
of the weights. We observe that the weights DA1become two groups, and Db1,DA2becoming one group
each, which supports the theory that if the initial map Ω0is equivariant, then so are subsequent maps after
training.
We note that this analysis can be used to partially explain the existing heuristic method of applying mirror
maps to neural networks, such as using p-normsψ(w) =∥w∥p
p(Azizan et al., 2021), squared p-norms
ψ(w) =∥w∥2
p(D’Orazio et al., 2021; Gunasekar et al., 2018), or element-wise maps induced by projections
such as hyperbolic tangent or softmax function (Ajanthan et al., 2021). For these maps, when considering
the weights as vectors, permuting components does not change the value of the mirror map (though general
orthogonal transformations will). This permutation symmetry over all parameters can be thought of as a
special case of the presented theory which suggests only equivariance within a layer.
7 Experiments: Neural Network Training using Equivariance
We consider the problem of training LMD on neural networks, based on the group equivariance demonstrated
in Section 6. This is a challenging non-convex optimization task, where the approximate theory for LMD
(and its extensions) does not apply, due to significant errors that may occur from small perturbations in
the parameters. This motivates us to consider simpler yet exact mirror maps to learn, which can be done
layer-wise by the analysis in Section 6. We demonstrate competitive performance with Adam, one of the
most widely used optimizers for neural network training (Kingma & Ba, 2015).
22Published in Transactions on Machine Learning Research (05/2024)
7.1 Deep Fully Connected Neural Network
We first consider training a classifier neural network on the MNIST image dataset. Neural networks are
usually trained using minibatched gradients, as computing gradients over the whole dataset is infeasible.
This is a prime use case for the stochastic variants of LMD. The neural network to train as well as the
LSMD parameterization are detailed as follows.
Algorithm 7 Learned AMD with minibatched gradients (LAMD*)
Require: Input ˜x(0)= ˜z(0)=x(0)∈X, parameter r≥3, step-sizes tk, number of iterations K
1:z(0)=∇Mθ(˜z(0))
2:fork= 0,...,Kdo
3:x(k+1)=λk∇M∗
ϑ(z(k)) + (1−λk)˜x(k)withλk=r
r+k
4:z(k+1)=z(k)−ktk
r∇fk(x(k+1))
5: ˜x(k+1)=x(k+1)−γtk∇fk(x(k+1))
6:end for
7:returnx(K+1)=λK∇M∗
ϑ(z(K)) + (1−λK)˜x(K)
Neural network architecture. The number of features in each layer of the neural network is 784–50–40–
30–20–10, with dense linear layers with bias between the layers and ReLU activations, for a total of 43350
parameters. The MNIST images are first flattened (giving 28×28 = 784 features), before being fed into the
network. The function objective corresponding to a neural network is the cross-entropy loss for classification
over the MNIST dataset. For LSMD, we associate each layer with two splines – one for the weight matrix
and one for the bias vector. This results in a total of 10 learned splines, a total of 400 learnable parameters
for LSMD.
LMD parameterization. Utilizing the reduced parameterization as suggested by Section 6, we can con-
sider only layer-wise mirror maps Ψ. This allows us to use more complicated mirror maps from R→R.
One parameterization is to use monotonic splines σas mirror maps, which are derivatives of convex mirror
potentials∇ψ=σ. We directly model the mirror map using a knot-based piecewise-linear spline param-
eterization as Goujon et al. (2022), where knots are defined at pre-determined locations, and the spline is
(uniquely) determined by the value of σat each knot. We note that since the monotonic spline σ:R→R
is continuous, the corresponding mirror potential given by the integral ψ(x) =/integraltextx
0σ(t) dtisC1. For these
experiments, we use 41knots equispaced on the interval [−1,1], with gap 0.05, which can be parameterized
using 40 parameters. More details on the construction of σcan be found in Appendix G.
The goal of LSMD is to train neural networks with architecture faster, given MNIST data. The function
class that we wish to optimize over is thus
F=/braceleftiggN/summationdisplay
k=1L(Nk)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleneural networksN0/bracerightigg
, (30)
whereLis the cross-entropy of a neural network (on an image mini-batch), and Nkis the neural network
after evolving the parameters through LSMD for kiterations. Note that the number of iterations that we
can unroll is significantly higher than in the image processing examples, due to the dimensionality reduction
from the equivariant theory. The minibatching process means that we are training using SMD instead of
the MD framework, similarly to how SGD is used instead of GD to train neural networks on large datasets.
We are allowed to use three epochs of MNIST to train, which we find to be sufficient for these experiments.
The MNIST batch-size is taken to be 500, resulting in unrolling N= 360steps. The neural networks N0
are initialized using the default PyTorch initialization, where a dense layer from Cin–Coutfeatures has the
entries of its weight matrix Aand bias matrix binitialized from Uniform (−C−1
in,C−1
in). We use 20 such neural
network initializations as a batch for each LMD training iteration, and average the cross-entropy over each
neural network as our loss function.
Figure 6 shows the performance of LSMD on this task, as well as minibatched LAMD* Algorithm 7 applied
with the spline-based mirror maps learned using the LSMD framework. LMD and LAMD* are compared
23Published in Transactions on Machine Learning Research (05/2024)
0 50 100 150 200 250 300 350
Iteration020406080100120140Cross-entropy LossSGD
Adam
SplineLAMD
SplineLMDTraining negative log-likelihood
(a) Cross-entropy loss
0 50 100 150 200 250 300 350
Iteration0.20.40.60.81.0AccuracySGD
Adam
SplineLAMD
SplineLMDTrain accuracy (b) Train accuracy
Figure 6: Spline-based LMD applied to training a 4-hidden-layer dense neural network for MNIST classifi-
cation. We observe a significant acceleration of LAMD as compared to LMD. There is a slight performance
gap between LAMD and Adam on this task.
against: SGD with learning rate 1×10−1, and Adam with learning rate 1×10−2, where other parameters are
kept as default. The values are averaged over 20 neural network initializations. We observe that Adam with
this learning rate is able to train this network architecture very rapidly. LAMD* is able to almost match
the performance of Adam at the end of the 3 training epochs, while both LAMD* and LMD significantly
outperform SGD.
7.2 Convolutional Neural Network
We additionally consider LMD to train a 2-layer convolutional neural network. We impose a diagonal
quadraticLMDpriorontheweights, similarlytoSection6.1. TheCNNistakentohaveasingleconvolutional
layer with 8 channels and kernel size 3, a max pooling layer with window size 2, followed by a dense layer from
288 features to 10 output features. The inputs of the CNN are MNIST images that have been downscaled
to have dimension 14×14. The target is to minimize the cross-entropy loss of the CNN after 2 epochs of
MNIST, with mini-batch size 1000, resulting in a total unrolling count of N= 120.
The equivariance here is due to the permutation invariance of each of the eight 3×3kernels. Therefore, we
can reduce the number of parameters of LMD by tying the diagonal weights across the channels, reducing the
number of parameters for this layer from 8×(9 + 1) = 72 to only 10, with 9 from the sliding window weights
and 1 from the bias term. While not strictly induced by the training problem1, we additionally impose
equivariance on the dense layer by tying the 288 features together, reducing the number of parameters from
288×10 + 10to1×10 + 10. In total, we learn 30 parameters.
Figure 7 shows the performance of LMD and LAMD* when averaged over training 20 randomly initialized
CNN problems, trained for 2 epochs of MNIST. We observe competitive performance with Adam for both
LAMD* and LMD. Interestingly, we observe that the testing accuracy of the training result of LAMD* is
marginally higher than that of Adam. This distinctly highlights the potential of learning the geometry for
gradient-based methods, as we achieve significant acceleration with only 30 parameters to the point of being
competitive with Adam, while retaining a principled approach to learning the model.
We note that the size of the CNN is limited due to the memory constraints of LMD. To train one iteration of
LMD, all the intermediate activations of the neural network need to be stored. In the case of a convolutional
layer, the intermediate layer is also image-like, which results in very high memory usage.
7.3 Few-Shot Learning
1Since the layer before the dense layer is a convolutional layer (followed by max-pooling), the features in this intermediate
layer still contain local information. Therefore, we do not have permutation invariance of the features.
24Published in Transactions on Machine Learning Research (05/2024)
0 20 40 60 80 100 120
Iteration010203040Cross-entropy LossSGD
Adam
LAMD
LMDTraining negative log-likelihood
(a) Cross-entropy loss
0 20 40 60 80 100 120
Iteration0.20.40.60.81.0AccuracySGD
Adam
LAMD
LMDT est accuracy (b) Test accuracy
Figure 7: Diagonal LMD and LAMD applied to training one-hidden-layer convolutional neural networks
with 2 epochs of MNIST, averaged over 20 runs. We observe that LAMD can achieve higher test accuracy
and lower training loss by the end of the 2 training epochs. Both LMD and LAMD are competitive with
Adam in this example.
TofurthercomparetheproposedLMDmethodsonnon-convexproblems, weconsidertheproblemoffew-shot
learning, where a classifier is trained on limited training data. We utilize the Omniglot dataset, consisting
of 20 instances each of 1623 characters (Lake et al., 2011). The characters are split into training and testing
sets of 1200 and 423 characters respectively as in Finn et al. (2017). We consider a 5-shot 10-way problem
to utilize the small CNN architecture as detailed in the previous section. In other words, a problem training
instance consists of choosing 10 characters from the respective training or testing set, training the network
to classify 5 instances of each character, and testing on the other 15 instances of the character.
As a baseline meta-learning approach, we consider the Reptile method (Nichol et al., 2018), which finds a
good neural network initialization such that a standard optimization algorithm such as SGD can minimize
the loss quickly. Further details on how the Reptile baseline is trained can be found in Appendix H.
The L2O approach of LMD instead considers a different approach of fast optimization by changing the
optimization trajectory, allowing for random neural network initializations. We consider training LMD
on the training character set in two settings: one with standard random initialization, and one from the
initialization learned using Reptile. These methods are then compared with SGD with learning rate 1×10−1
and Adam 1×10−2as before, with random initializations and Reptile initializations. We additionally
consider LAMD with the same learned parameters from both random initialization and from the Reptile
initialization. The methods are evaluated on 104randomly sampled 5-shot 10-way problems from characters
drawn from the test split, with gradients calculated in a full-batch manner.
Figure 8 contains a comparison of the equivariant LMD and LAMD methods with SGD and Adam, with ran-
dom intializations and from the Reptile initialization. We note that LMD outperforms its SGD counterpart
in both scenarios. Moreover, LAMD is able to achieve a significantly lower log-likelihood, outperforming
Adam with random network initialization as well as with Reptile initializations. Nonetheless, there is still
a gap between the LMD methods and Adam in terms of generalization performance, suggesting that the
trajectory than LMD takes trades off generalization performance for faster training. Interestingly, LAMD
with random initializations outperforms LAMD with the Reptile initialization, suggesting that random ini-
tializations plays a role in learning the geometry over a wider set of trajectories.
8 Conclusions
In this work, we present multiple extensions of the learned mirror descent method introduced in a recent work
on learning to optimize (Tan et al., 2023b;a). In particular, we introduce accelerated and stochastic versions,
as well as a new training paradigm that reduces the number of learnable parameters while maintaining
25Published in Transactions on Machine Learning Research (05/2024)
0 20 40 60 80 100
Iteration104
103
102
101
100Cross-entropy LossSGD
Adam
LMD
LAMD
Reptile-SGD
Reptile-Adam
Reptile-LMD
Reptile-LAMDTraining negative log-likelihood
(a) Cross-entropy loss
0 20 40 60 80 100
Iteration0.10.20.30.40.50.60.70.8AccuracySGD
Adam
LMD
LAMD
Reptile-SGD
Reptile-Adam
Reptile-LMD
Reptile-LAMDT est accuracy (b) Test accuracy
Figure 8: Training loss and test accuracy for 5-shot 10-way learning on the Omniglot dataset. We observe
that LMD with Reptile initialization is able to achieve similar test accuracy to the compared methods.
Moreover, LAMD is able to outperform Adam in terms of training loss, but seems to converge to a solution
that generalizes worse.
theexpressivity of the mirror maps by exploiting symmetries. Numerical experiments demonstrate that the
learned accelerated and stochastic MD methods are able to outperform their corresponding GD counterparts
aswellaspreviousLMDwiththesamemirrormaps, andweempiricallyimproveontheconstantoptimization
gap by utilizing the dual domain during mirror steps. These experiments additionally demonstrate that
mirror maps trained using LMD as in (Tan et al., 2023b;a) are able to successfully generalize to different
LMD-type optimization schemes. We empirically show that mirror maps respect group invariances under
training to support our equivariant theory, and exploit this to train LMD on the non-convex problems of
training deep and convolutional neural networks.
This work presented results as well as techniques for developing L2O schemes for large-scale applications.
Given the various modifications to LMD presented, the mirror maps were pretrained using a basic scheme
that penalizes the objective function as well as the forward-backward inconsistency for a fixed number of
unrolled iterates (Tan et al., 2023b). One consideration is the modification of the training scheme to instead
use the accelerated or stochastic mirror descent rather than full-batch mirror descent, which could help
in large-scale applications. Another possible direction would be to consider convergence properties of the
approximate MD methods for non-convex functions, similarly to the non-mirrored cases presented in Berahas
et al. (2017); Jin et al. (2017).
This work additionally presented an application of equivariance for dimensionality reduction of LMD, reduc-
ing the total number of parameters required to train. We demonstrated that the reduced parameterization
arises naturally from common training scenarios, and that the resulting LMD is able to perform compet-
itively with Adam. While we limit our equivariance to simple gradient-based methods, interesting future
directions could consider equivariance under the scope of other optimization algorithms utilizing techniques
such as scaling, momentum, or using proximal operators. Another direction to continue pushing LMD is to
address the current limitations caused by needing to store intermediate activations, allowing for this method
to be used for larger and more practical neural networks.
References
Thalaiyasingam Ajanthan, Kartik Gupta, Philip Torr, Richad Hartley, and Puneet Dokania. Mirror descent
view for neural network quantization. In International conference on artificial intelligence and statistics ,
pp. 2809–2817. PMLR, 2021.
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. The Journal of
Machine Learning Research , 18(1):8194–8244, 2017.
26Published in Transactions on Machine Learning Research (05/2024)
Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror
descent. arXiv preprint arXiv:1407.1537 , 2014.
Diogo Almeida, Clemens Winter, Jie Tang, and Wojciech Zaremba. A generalizable approach to learning
optimizers. arXiv preprint arXiv:2106.00958 , 2021.
Brandon Amos, Lei Xu, and J. Zico Kolter. Input convex neural networks. In International Conference on
Machine Learning , volume 70 of Proceedings of Machine Learning Research , pp. 146–155. PMLR, 2017.
Brandon Amos et al. Tutorial on amortized optimization. Foundations and Trends ®in Machine Learning ,
16(5):592–732, 2023.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan
Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances
in neural information processing systems , pp. 3981–3989, 2016.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower
bounds for non-convex stochastic optimization. Mathematical Programming , 199(1-2):165–214, 2023.
Dominique Azé and Jean-Paul Penot. Uniformly convex and uniformly smooth convex functions. In Annales
de la Faculté des sciences de Toulouse: Mathématiques , volume 4, pp. 705–730, 1995.
Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized nonlinear
models.IEEE Transactions on Neural Networks and Learning Systems , 33(12):7717–7727, 2021.
Sebastian Banert, Axel Ringh, Jonas Adler, Johan Karlsson, and Ozan Oktem. Data-driven nonsmooth
optimization. SIAM Journal on Optimization , 30(1):102–131, 2020.
Sebastian Banert, Jevgenija Rudzusika, Ozan Öktem, and Jonas Adler. Accelerated forward-backward
optimization using deep learning. arXiv preprint arXiv:2105.05210 , 2021.
A. Beck and M. Teboulle. Fast gradient-based algorithms for constrained total variation image denoising
and deblurring problems. IEEE Transactions on Image Processing , 18(11):2419–2434, 2009.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex
optimization. Operations Research Letters , 31(3):167–175, 2003.
Erik J Bekkers, Maxime W Lafarge, Mitko Veta, Koen AJ Eppenhof, Josien PW Pluim, and Remco Duits.
Roto-translation covariant convolutional networks for medical image analysis. In Medical Image Comput-
ing and Computer Assisted Intervention–MICCAI 2018: 21st International Conference, Granada, Spain,
September 16-20, 2018, Proceedings, Part I , pp. 440–448. Springer, 2018.
Aharon Ben-Tal, Tamar Margalit, and Arkadi Nemirovski. The ordered subsets mirror descent optimization
method with applications to tomography. SIAM Journal on Optimization , 12(1):79–108, 2001.
Albert S Berahas, Raghu Bollapragada, and Jorge Nocedal. An investigation of newton-sketch and subsam-
pled newton methods. arXiv preprint arXiv:1705.06211 , 2017.
Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends ®in Machine
Learning , 8(3-4):231–357, 2015.
Elena Celledoni, Matthias J Ehrhardt, Christian Etmann, Brynjulf Owren, Carola-Bibiane Schönlieb, and
Ferdia Sherry. Equivariant neural networks for inverse problems. Inverse Problems , 37(8):085006, 2021.
Antonin Chambolle, Matthias J Ehrhardt, Peter Richtarik, and Carola-Bibiane Schonlieb. Stochastic primal-
dual hybrid gradient algorithm with arbitrary sampling and imaging applications. SIAM Journal on
Optimization , 28(4):2783–2808, 2018.
Dongdong Chen, Julián Tachella, and Mike E Davies. Equivariant imaging: Learning beyond the range
space. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 4379–4388,
2021.
27Published in Transactions on Machine Learning Research (05/2024)
Tianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang, Howard Heaton, Jialin Liu, and Wotao
Yin. Learning to optimize: A primer and a benchmark. The Journal of Machine Learning Research , 23
(1):8562–8620, 2022.
Xinyi Chen and Elad Hazan. A nonstochastic control approach to optimization. arXiv preprint
arXiv:2301.07902 , 2023.
Edward Chlebus. An approximate formula for a partial sum of the divergent p-series. Applied Mathematics
Letters, 22(5):732–737, 2009.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on
machine learning , pp. 2990–2999. PMLR, 2016.
Aaron Defazio. A simple practical accelerated method for finite sums. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29 , pp.
676–684. Curran Associates, Inc., 2016.
Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online meta-
learning. Advances in Neural Information Processing Systems , 32, 2019.
Ryan D’Orazio, Nicolas Loizou, Issam Laradji, and Ioannis Mitliagkas. Stochastic mirror descent: Con-
vergence analysis and adaptive variants via the mirror stochastic polyak stepsize. arXiv preprint
arXiv:2110.15412 , 2021.
Derek Driggs, Junqi Tang, Jingwei Liang, Mike Davies, and Carola-Bibiane Schönlieb. A stochastic proximal
alternating minimization for nonsmooth and nonconvex optimization. SIAM Journal on Imaging Sciences ,
14(4):1932–1970, 2021.
John C Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari. Composite objective mirror descent.
InCOLT, volume 10, pp. 14–26. Citeseer, 2010.
John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent. SIAM
Journal on Optimization , 22(4):1549–1578, 2012.
Alina Ene and Huy L Nguyen. High probability convergence for accelerated stochastic mirror descent. arXiv
preprint arXiv:2210.00679 , 2022.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In International conference on machine learning , pp. 1126–1135. PMLR, 2017.
Boyan Gao, Henry Gouk, Hae Beom Lee, and Timothy M Hospedales. Meta mirror descent: Optimiser
learning for fast convergence. arXiv preprint arXiv:2203.02711 , 2022.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic
programming. Mathematical Programming , 156(1-2):59–99, 2016.
Alexis Goujon, Sebastian Neumayer, Pakshal Bohra, Stanislas Ducotterd, and Michael Unser. A neural-
network-based convex regularizer for image reconstruction. arXiv preprint arXiv:2211.12461 , 2022.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th
international conference on international conference on machine learning , pp. 399–406, 2010.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of
optimization geometry. In International Conference on Machine Learning , pp. 1832–1841. PMLR, 2018.
Suriya Gunasekar, Blake Woodworth, and Nathan Srebro. Mirrorless mirror descent: A natural derivation
of mirror descent. In International Conference on Artificial Intelligence and Statistics , pp. 2305–2313.
PMLR, 2021.
Rishi Gupta and Tim Roughgarden. A pac approach to application-specific algorithm selection. In Proceed-
ings of the 2016 ACM Conference on Innovations in Theoretical Computer Science , pp. 123–134, 2016.
28Published in Transactions on Machine Learning Research (05/2024)
VahanHovhannisyan,PanosParpas,andStefanosZafeiriou. MAGMA:Multilevelacceleratedgradientmirror
descent algorithm for large-scale convex composite minimization. SIAM Journal on Imaging Sciences , 9
(4):1829–1857, 2016.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. Advances in Neural Information Processing Systems , 31, 2018.
Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser. Deep convolutional neural
network for inverse problems in imaging. IEEE Transactions on Image Processing , 26(9):4509–4522, 2017.
Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-learning
methods. Advances in Neural Information Processing Systems , 32, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference
on Learning Representations , 2015.
Walid Krichene, Alexandre Bayen, and Peter L Bartlett. Accelerated mirror descent in continuous and
discrete time. Advances in Neural Information Processing Systems , 28, 2015.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple
visual concepts. In Proceedings of the annual meeting of the cognitive science society , volume 33, 2011.
Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming , 133
(1-2):365–397, 2012.
Guanghui Lan. First-order and stochastic optimization methods for machine learning , volume 1. Springer,
2020.
Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv preprint
arXiv:1507.02000 , 2015.
Guanghui Lan, Arkadi Nemirovski, and Alexander Shapiro. Validation analysis of mirror descent stochastic
approximation method. Mathematical programming , 134(2):425–458, 2012.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Deep neural networks as Gaussian processes. In International Conference on Learning Repre-
sentations , 2018.
Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance
and equivalence. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.
991–999, 2015.
Johannes Leuschner, Maximilian Schmidt, and David Erzmann. Deep inversion validation library. Software
available from https://github.com/jleuschn/dival , 2019.
Ke Li and Jitendra Malik. Learning to optimize. In International Conference on Learning Representations ,
2016.
Yanjun Li, Kai Zhang, Jun Wang, and Sanjiv Kumar. Learning adaptive random features. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 33, pp. 4229–4236, 2019.
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-Dickstein. Under-
standing and correcting pathologies in the training of learned optimizers. In International Conference on
Machine Learning , pp. 4556–4565. PMLR, 2019.
Arkadi Nemirovski and David Berkovich Yudin. Problem Complexity and Method Efficiency in Optimization
/ translated by E.R. Dawson. Wiley-Interscience series in discrete mathematics. Wiley, Chichester, 1983.
ISBN 0471103454.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approxima-
tion approach to stochastic programming. SIAM Journal on Optimization , 19(4):1574–1609, 2009.
29Published in Transactions on Machine Learning Research (05/2024)
Yurii Evgen’evich Nesterov. A method of solving a convex programming problem with convergence rate
O(1/k2). InDoklady Akademii Nauk , volume 269, pp. 543–547. Russian Academy of Sciences, 1983.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint
arXiv:1803.02999 , 2018.
Francesco Orabona, Koby Crammer, and Nicolo Cesa-Bianchi. A generalized online mirror descent with
applications to classification and regression. Machine Learning , 99:411–435, 2015.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 32 , pp. 8024–8035. Curran Associates, Inc., 2019.
Garvesh Raskutti and Sayan Mukherjee. The information geometry of mirror descent. IEEE Transactions
on Information Theory , 61(3):1451–1457, 2015.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International
Conference on Learning Representations , 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical
Statistics , 22(3):400–407, 1951.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms.
Physica D: nonlinear phenomena , 60(1-4):259–268, 1992.
Rajiv Sambharya, Georgina Hall, Brandon Amos, and Bartolomeo Stellato. Learning to warm-start fixed-
point optimization algorithms. arXiv preprint arXiv:2309.07835 , 2023.
Nati Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. Advances
in Neural Information Processing Systems , 24, 2011.
Volker Strassen. The existence of probability measures with given marginals. The Annals of Mathematical
Statistics , 36(2):423–439, 1965.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling Nesterov’s accelerated
gradient method: theory and insights. Advances in neural information processing systems , 27, 2014.
HongYeTan, SubhadipMukherjee, JunqiTang, AndreasHauptmann, andCarola-BibianeSchönlieb. Robust
data-driven accelerated mirror descent. In 2023 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pp. 1–5. IEEE, 2023a.
Hong Ye Tan, Subhadip Mukherjee, Junqi Tang, and Carola-Bibiane Schönlieb. Data-driven mirror descent
with input-convex neural networks. SIAM Journal on Mathematics of Data Science , 5(2):558–587, 2023b.
Junqi Tang, Mohammad Golbabaee, Francis Bach, and Mike E Davies. Rest-Katyusha: Exploiting the
solutions’ structure via scheduled restart schemes. In Advances in Neural Information Processing Systems ,
pp. 427–438. Curran Associates, Inc., 2018.
Junqi Tang, Karen Egiazarian, Mohammad Golbabaee, and Mike Davies. The practicality of stochastic
optimization in imaging inverse problems. IEEE Transactions on Computational Imaging , 6:1471–1485,
2020.
Yang Wang and Haomin Zhou. Total variation wavelet-based medical image denoising. International Journal
of Biomedical Imaging , 2006, 2006.
Blake E Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives. In
Advances in Neural Information Processing Systems , pp. 3639–3647, 2016.
30Published in Transactions on Machine Learning Research (05/2024)
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks:
Deep translation and rotation equivariance. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pp. 5028–5037, 2017.
Pan Xu, Tianhao Wang, and Quanquan Gu. Accelerated stochastic mirror descent: From continuous-time
dynamics to discrete-time algorithms. In International Conference on Artificial Intelligence and Statistics ,
pp. 1087–1096. PMLR, 2018.
Junjie Yang, Tianlong Chen, Mingkang Zhu, Fengxiang He, Dacheng Tao, Yingbin Liang, and Zhangyang
Wang. Learning to generalize provably in learning to optimize. In International Conference on Artificial
Intelligence and Statistics , pp. 9807–9825. PMLR, 2023.
Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In
International Conference on Machine learning , pp. 116. ACM, 2004.
Joey Tianyi Zhou, Kai Di, Jiawei Du, Xi Peng, Hao Yang, Sinno Jialin Pan, Ivor Tsang, Yong Liu, Zheng
Qin, and Rick Siow Mong Goh. Sc2net: Sparse lstms for sparse coding. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 32, 2018a.
Kaiwen Zhou, Fanhua Shang, and James Cheng. A simple stochastic variance reduced algorithm with fast
convergence rates. In International Conference on Machine Learning , pp. 5975–5984, 2018b.
ZhengyuanZhou, PanayotisMertikopoulos, NicholasBambos, StephenBoyd, andPeterWGlynn. Stochastic
mirrordescentinvariationallycoherentoptimizationproblems. Advances in Neural Information Processing
Systems, 30, 2017.
Julian Zimmert and Tor Lattimore. Connections between mirror descent, Thompson sampling and the
information ratio. Advances in Neural Information Processing Systems , 32, 2019.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences
of Adam and RMSprop. In Proceedings of the IEEE/CVF Conference on computer vision and pattern
recognition , pp. 11127–11135, 2019.
A Proofs in Section 2
A.1 Proof of Lemma 1
Lemma 2. Consider the approximate AMD iterations from Algorithm 1, and let ˆz(k)be the exact MD
iterates given by Equation 5. Assume ψ∗isLψ∗-smooth with respect to a reference norm ∥·∥∗on the dual
space, i.e. Bψ∗(z,y)≤Lψ∗
2∥z−y∥2
∗, or equivalently that ∇ψ∗isLψ∗-Lipschitz. Assume further that there
exists 0< ℓR≤LRsuch that for all x,x′∈X,ℓR
2∥x−x′∥2≤R(x,x′)≤LR
2∥x−x′∥2. Define the energy
˜E(k)fork≥0as follows (where t−1= 0):
˜E(k):=k2tk−1
r/parenleftig
f(˜x(k))−f∗/parenrightig
+rBψ∗/parenleftig
∇ψ(˜z(k)),∇ψ(x∗)/parenrightig
. (31)
Assume the step-size conditions γ≥LRLψ∗, andtk≤lR
Lfγ. Then the difference between consecutive energies
satisfies the following:
˜E(k+1)−˜E(k)≤(2k+ 1−rk)tk
r/parenleftig
f(˜x(k+1))−f∗/parenrightig
−k2(tk−1−tk)
r/parenleftig
f(˜x(k))−f∗/parenrightig
+r/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
.
Proof.The proof closely follows that of Lemma 2 in Krichene et al. (2015). We will use the following lemmas,
which will be stated without proof. The proofs of these results can be found in the supplementary material
of Krichene et al. (2015).
31Published in Transactions on Machine Learning Research (05/2024)
Lemma 3. Letfbe convex with Lf-Lipschitz gradient with respect to ∥·∥. Then for all x,x′,x+,
f(x+)≤f(x′) +⟨∇f(x),x+−x′⟩+Lf
2∥x+−x∥2.
Lemma 4. For any differentiable convex ψ∗andu,v,w∈domψ∗,
Bψ∗(u,v)−Bψ∗(w,v) =−Bψ∗(w,u) +⟨∇ψ∗(u)−∇ψ∗(v),u−w⟩.
Lemma 5. Ifψ∗hasLψ∗-Lipschitz gradient, then for all u,v∈(Rn)∗,
1
2Lψ∗∥∇ψ∗(u)−∇ψ∗(v)∥2≤Bψ∗(u,v)≤Lψ∗
2∥u−v∥2
∗.
We begin our analysis with these lemmas in place. Let x∗=z∗be some minimizer. We begin by bounding
the difference in Bregman divergence.
Bψ∗/parenleftig
∇ψ(˜z(k+1)),∇ψ(x∗)/parenrightig
−Bψ∗/parenleftig
∇ψ(˜z(k)),∇ψ(x∗)/parenrightig
(a)=−Bψ∗/parenleftig
∇ψ(˜z(k)),∇ψ(˜z(k+1))/parenrightig
+/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(˜z(k)),˜z(k+1)−x∗/angbracketrightig
(b)
≤−1
2Lψ∗∥˜z(k+1)−˜z(k)∥2+/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(˜z(k)),˜z(k+1)−x∗/angbracketrightig
(c)
≤−1
2Lψ∗∥˜z(k+1)−˜z(k)∥2+/angbracketleftbigg
−ktk
r∇f(x(k+1)),˜z(k+1)−x∗/angbracketrightbigg
+/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
.
(32)
In Step (a) above, we used ∇ψ∗◦∇ψ=Iandz∗=x∗together with Lemma 4. In Step (b), we use Lemma 5
to bound the first term. In Step (c), we decomposed the right-hand side of the inner product using
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)) +∇ψ(ˆz(k+1))−∇ψ(˜z(k)) =∇ψ(˜z(k+1))−∇ψ(ˆz(k+1))−ktk
r∇f(x(k+1)),
which follows directly from Equation 5. Recall Step 4 of Algorithm 1:
˜x(k+1)= arg min
˜x∈Rn/angbracketleftig
∇f(x(k+1)),˜x/angbracketrightig
+1
γtkR(˜x,x(k+1)),
wherelR
2∥x−y∥2≤R(x,y)≤LR
2∥x−y∥2. From the definition of ˜x(k+1), we have that for any x∈Rn,
R(x,x(k+1))≥R(˜x(k+1),x(k+1)) +γtk⟨∇f(x(k+1)),˜x(k+1)−x⟩. (33)
Recalling the step for x(k+1)in Step 2 of Algorithm 1, we can write
˜z(k+1)−˜z(k)=1
λk/parenleftig
λk˜z(k+1)+ (1−λk)˜x(k)−x(k+1)/parenrightig
=1
λk/parenleftig
d(k+1)−x(k+1)/parenrightig
,
where we define d(k+1):=λk˜z(k+1)+ (1−λk)˜x(k). We then compute
∥˜z(k+1)−˜z(k)∥2=1
λ2
k∥d(k+1)−x(k+1)∥2≥2
LRλ2
kR(d(k+1),x(k+1)),sinceR(x,x′)≤LR
2∥x−x′∥2.
=⇒∥˜z(k+1)−˜z(k)∥2≥2
LRλ2
k/parenleftig
R(˜x(k+1),x(k+1)) +γtk/angbracketleftig
∇f(x(k+1)),˜x(k+1)−d(k+1)/angbracketrightig/parenrightig
,
by settingx=d(k+1)in Equation 33. Now, using R(˜x(k+1),x(k+1))≥lR
2∥˜x(k+1)−x(k+1)∥2, we have:
∥˜z(k+1)−˜z(k)∥2≥2
LRλ2
k/parenleftbigglR
2∥˜x(k+1)−x(k+1)∥2+γtk/angbracketleftig
∇f(x(k+1)),˜x(k+1)−d(k+1)/angbracketrightig/parenrightbigg
.
32Published in Transactions on Machine Learning Research (05/2024)
By replacing d(k+1)=λk˜z(k+1)+ (1−λk)˜x(k)and multiplying both sides by λkkLR
2rγ, we get
λkkLR
2rγ∥˜z(k+1)−˜z(k)∥2≥klR
2rλkγ∥˜x(k+1)−x(k+1)∥2
+/angbracketleftbiggktk
r∇f(x(k+1)),1
λk˜x(k+1)−˜z(k+1)−1−λk
λk˜x(k)/angbracketrightbigg
.(34)
Subtracting Equation 34 from Equation 32:
Bψ∗(∇ψ(˜z(k+1)),∇ψ(x∗))−Bψ∗(∇ψ(˜z(k)),∇ψ(x∗))≤−αk∥˜z(k+1)−˜z(k)∥2−klR
2rλkγ∥˜x(k+1)−x(k+1)∥2
+/angbracketleftbigg−ktk
r∇f(x(k+1)),−x∗+1
λk˜x(k+1)−1−λk
λk˜x(k)/angbracketrightbigg
+/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
,
whereαk=1
2Lψ∗−kλkLR
2rγ. DefiningD(k+1)
1 =∥˜x(k+1)−x(k+1)∥2andD(k+1)
2 =∥˜z(k+1)−˜z(k)∥2, we rewrite
the last inequality as
Bψ∗(∇ψ(˜z(k+1)),∇ψ(x∗))−Bψ∗(∇ψ(˜z(k)),∇ψ(x∗))
≤−αkD(k+1)
2−klR
2rλkγD(k+1)
1 +ktk
r/angbracketleftig
−∇f(x(k+1)),˜x(k+1)−x∗/angbracketrightig
+1−λk
λkktk
r/angbracketleftig
−∇f(x(k+1)),˜x(k+1)−˜x(k)/angbracketrightig
+/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
.
Using Lemma 3 with x,x+,x′as follows, we bound the inner products:
/angbracketleftig
−∇f(x(k+1)),˜x(k+1)−˜x(k)/angbracketrightig
≤f(˜x(k))−f(˜x(k+1)) +Lf
2D(k+1)
1,(x=x(k+1),x+= ˜x(k+1), andx′= ˜x(k));
/angbracketleftig
−∇f(x(k+1)),˜x(k+1)−x∗/angbracketrightig
≤f∗−f(˜x(k+1)) +Lf
2D(k+1)
1,(x=x(k+1),x+= ˜x(k+1), andx′=x∗).
Combining the inequalities and using1−λk
λk=k
r, we obtain
Bψ∗(∇ψ(˜z(k+1)),∇ψ(x∗))−Bψ∗(∇ψ(˜z(k)),∇ψ(x∗))
≤−αkD(k+1)
2 +k2tk
r2/parenleftbigg
f(˜x(k))−f(˜x(k+1)) +Lf
2D(k+1)
1/parenrightbigg
+ktk
r/parenleftbigg
f∗−f(˜x(k+1)) +Lf
2D(k+1)
1/parenrightbigg
−klR
2rλkγD(k+1)
1 +/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
=k2tk
r2/parenleftig
f(˜x(k))−f(˜x(k+1))/parenrightig
+ktk
r/parenleftig
f∗−f(˜x(k+1))/parenrightig
−αkD(k+1)
2−βkD(k+1)
1
+/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
,
whereβk:=klR
2rλkγ−Lfk2tk
2r2−Lfktk
2r. Re-introducing the energy function for k≥1,
˜E(k):=k2tk−1
r(f(˜x(k))−f∗) +rBψ∗(∇ψ(z(k)),∇ψ(x∗)),
33Published in Transactions on Machine Learning Research (05/2024)
we can compute the difference ˜E(k+1)−˜E(k):
˜E(k+1)−˜E(k)=(k+ 1)2tk
r/parenleftig
f(˜x(k+1))−f∗/parenrightig
−k2tk−1
r/parenleftig
f(˜x(k))−f∗/parenrightig
+r(Bψ∗(∇ψ(˜z(k+1)),∇ψ(x∗))−Bψ∗(∇ψ(˜z(k)),∇ψ(x∗)))
=k2tk
r/parenleftig
f(˜x(k+1))−f∗/parenrightig
+(2k+ 1)tk
r/parenleftig
f(˜x(k+1))−f∗/parenrightig
−k2tk
r/parenleftig
f(˜x(k))−f∗/parenrightig
−k2tk−1−k2tk
r/parenleftig
f(˜x(k))−f∗/parenrightig
+k2tk
r/parenleftig
f(˜x(k))−f(˜x(k+1))/parenrightig
+ktk/parenleftig
f∗−f(˜x(k+1))/parenrightig
−rαkD(k+1)
2−rβkD(k+1)
1
+r/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
=(2k+ 1−rk)tk
r/parenleftig
f(˜x(k+1))−f∗/parenrightig
−k2(tk−1−tk)
r/parenleftig
f(˜x(k))−f∗/parenrightig
−rαkD(k+1)
2−rβkD(k+1)
1
+r/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
.
For the desired inequality to hold, it suffices that αk,βk≥0. Recalling their definitions (and λk), we want
1
2Lψ∗−kLR
2(r+k)γ≥0andk(r+k)lR
2r2γ−Lfk2tk
2r2−Lfktk
2r≥0,
which are equivalent to
γ≥kr
kr+r2LRLψ∗andtk≤lR
Lfγ.
It thus suffices to have
γ≥LRLψ∗, tk≤lR
Lfγ.
Under these conditions, we get the bound as required
˜E(k+1)−˜E(k)≤(2k+ 1−rk)tk
r/parenleftig
f(˜x(k+1))−f∗/parenrightig
−k2(tk−1−tk)
r/parenleftig
f(˜x(k))−f∗/parenrightig
+r/angbracketleftig
∇ψ(˜z(k+1))−∇ψ(ˆz(k+1)),˜z(k+1)−x∗/angbracketrightig
.
A.2 Proof of Proposition 1
Proposition 4. Assume the conditions as in Lemma 1. Then
˜E(1)≤rBψ∗(∇ψ(x(0)),∇ψ(x∗)) +t0
r(f(x(0))−f∗) +r/angbracketleftig
∇ψ(˜z(1))−∇ψ(ˆz(1)),˜z(1)−x∗/angbracketrightig
(35)
Proof.From Lemma 1 applied with k= 0, we have
˜E(1)≤˜E(0)+t0
r/parenleftig
f(˜x(1))−f∗/parenrightig
+r/angbracketleftig
∇ψ(˜z(1))−∇ψ(ˆz(1)),˜z(1)−x∗/angbracketrightig
=rBψ∗/parenleftig
∇ψ(z(0)),∇ψ(x∗)/parenrightig
+t0
r/parenleftig
f(˜x(1))−f∗/parenrightig
+r/angbracketleftig
∇ψ(˜z(1))−∇ψ(ˆz(1)),˜z(1)−x∗/angbracketrightig
.
By definition, ˜x(1)= arg min˜x∈Rnγt0⟨∇f(x(1)),˜x⟩+R(˜x,x(1)), thus (since R(x,x) = 0)
γt0/angbracketleftig
∇f(x(1)),˜x(1)/angbracketrightig
+R(˜x(1),x(1))≤γt0/angbracketleftig
∇f(x(1)),x(1)/angbracketrightig
. (36)
34Published in Transactions on Machine Learning Research (05/2024)
Therefore, we get
f(˜x(1))−f∗≤/angbracketleftig
∇f(x(1)),˜x(1)−x∗/angbracketrightig
+Lf
2∥˜x(1)−x(1)∥2by Lemma 3
≤/angbracketleftig
∇f(x(1)),˜x(1)−x∗/angbracketrightig
+Lf
lRR(˜x(1),x(1)) by assumption on R
≤/angbracketleftig
∇f(x(1)),˜x(1)−x∗/angbracketrightig
+1
γt0R(˜x(1),x(1))−Lf
lRR(˜x(1),x(1)) using2Lf
lR≤1
γt0
≤/angbracketleftig
∇f(x(1)),x(1)−x∗/angbracketrightig
−Lf
lRR(˜x(1),x(1)) using Equation 36
≤f(x(1))−f∗+Lf
2∥x(1)−x∗∥2−Lf
lRR(˜x(1),x(1)) using Lemma 3
≤f(x(1))−f∗by assumption on R.
Now recalling that x(1)=λ0˜z(0)+ (1−λ0)˜x(0)andλ0= 1, we havex(1)= ˜x(0)=x(0). Therefore, we have
f(˜x(1))−f∗≤f(x(0))−f∗. Further using ˜z(0)=x(0)shows the desired inequality.
B Proof of Section 3 (approximate SMD)
The following lemma characterizes the effect of a mirror step on the Bregman divergence. We use this lemma
to show an optimality gap bound on SMD with the inexact stochastic oracle G(x,ξ)similarly to Nemirovski
et al. (2009), where the bound depends on the variance of the stochastic component ∆(x,ξ)and the norm
of the inexactness U(x).
Lemma 6 (Nemirovski et al. 2009, Lem. 2.1) .For anyu,x∈Xandy∈(Rn)∗, we have (where ψis
α-strongly convex),
Bψ(u,Px(y))≤Bψ(u,x) +⟨y,u−x⟩+∥y∥2
∗
2α. (37)
Applying Equation 37 with x=x(k), y=tkG(x(k),ξ(k)), given any point u∈X, we have
tk⟨G(x(k),ξ(k)),x(k)−u⟩≤Bψ(u,x(k))−Bψ(u,x(k+1)) +t2
k
2α∥G(x(k),ξ(k))∥2
∗. (38)
Using the definition of G, we can expand
tk⟨g(x(k)),x(k)−u⟩≤Bψ(u,x(k))−Bψ(u,x(k+1)) +t2
k
2α∥G(x(k),ξ(k))∥2
∗
−tk⟨∆k,x(k)−u⟩−tk⟨Uk,x(k)−u⟩.(39)
Observe that since fis convex and g(x)∈∂f(x), we have⟨g(x(k)),x(k)−u⟩≥f(x(k))−f(u). Using this,
summing Equation 39 from 1 to k, and noting that Bψ≥0, we have
k/summationdisplay
i=0ti[f(x(i))−f(u)]≤Bψ(u,x(0)) +k/summationdisplay
i=0t2
i
2α∥G(x(i),ξ(i))∥2
∗−k/summationdisplay
i=0ti⟨∆i,x(i)−u⟩−k/summationdisplay
i=0ti⟨Ui,x(i)−u⟩.(40)
Observing that E[∆i|ξ0,...,ξi−1] = 0and thatx(i)lives in the σ-algebra defined by ξ0,...,ξi−1, we have that
for anyi,
E[⟨∆i,x(i)−u⟩] =E/bracketleftig
E/bracketleftig
⟨∆i,x(i)−u⟩|ξ0,...,ξi−1/bracketrightig/bracketrightig
= 0.
We can take expectations over ξto get
E/bracketleftiggk/summationdisplay
i=0ti[f(x(i))−f(u)]/bracketrightigg
≤Bψ(u,x(0)) +k/summationdisplay
i=0t2
i
2αE/bracketleftig
∥G(x(i),ξ(i))∥2
∗/bracketrightig
−k/summationdisplay
i=0tiE/bracketleftig
⟨Ui,x(i)−u⟩/bracketrightig
.
35Published in Transactions on Machine Learning Research (05/2024)
Assuming that the stochasticity is bounded
E[∥G(x,ξ)∥2
∗]≤σ2∀x∈X,
this gives an optimality gap bound, which can be extended to convergence of the ergodic average.
Iffwereµ-strongly convex, then we can remove the uniform boundedness assumption since this allows us
to control∥x(i)−u∥, using the fact that ⟨g(x(k)),x(k)−u⟩≥f(x(k))−f(u) +µ∥x(k)−u∥2/2. Equation 40
instead becomes
k/summationdisplay
i=0ti[f(x(i))−f(u)]≤Bψ(u,x(0)) +k/summationdisplay
i=0t2
i
2α∥G(x(i),ξ(i))∥2
∗
−k/summationdisplay
i=0ti⟨∆i,x(i)−u⟩−k/summationdisplay
i=0ti/bracketleftig
⟨Ui,x(i)−u⟩+µ
2∥x(i)−u∥2/bracketrightig
.(41)
Taking expectations as before and using Young’s inequality on the final term, we get
E/bracketleftiggk/summationdisplay
i=0ti[f(x(i))−f(u)]/bracketrightigg
≤Bψ(u,x(0)) +k/summationdisplay
i=0t2
i
2αE/bracketleftig
∥G(x(i),ξ(i))∥2
∗/bracketrightig
+k/summationdisplay
i=0ti
2µ∥Ui∥2
∗.
C Proof of Theorem 3 (approximate ASMD)
Theorem 4. SupposefhasLf-Lipschitz gradient, the mirror map is α-strongly convex, the approximation
errorU(k)is bounded, and that the stochastic oracle is otherwise unbiased with bounded second moments.
Assume the diameter of Xis finite, that is, there exists a constant Mψ>0such that
Mψ= sup
x,x′∈XBψ(x,x′).
Suppose further that there exists a constant Ksuch that for every iterate x(k), we have
⟨∇f(x(k)),U(k)⟩≤K.
Then the convergence rate of approximate ASMD is (where the right hand side is also given by Equation 44),
E[f(x(k))−f(x∗)]≤K+O(k−2+k−1/2).
Proof.Recall the definition of the energy,
E(k)=E[Ak(f(x(k))−f(x∗)) +skBψ(x∗,∇ψ∗(y(k)))]. (42)
Using the following identity (which can be shown by expanding the Bregman distances using their definitions
and using∇ψ∗= (∇ψ)−1)
Bψ(x∗,∇ψ∗(y(k+1)))−Bψ(x∗,∇ψ∗(y(k)))−Bψ(∇ψ∗(y(k)),∇ψ∗(y(k+1))) =⟨y(k)−y(k+1),x∗−∇ψ∗(y(k))⟩,
we compute the difference in energy
E(k+1)−E(k)=E[Ak+1(f(x(k+1))−f(x∗))−Ak(f(x(k))−f(x∗))] +E[(sk+1−sk)Bψ(x∗,∇ψ∗(y(k+1)))]
+skE[Bψ(x∗,∇ψ∗(y(k+1)))−Bψ(x∗,∇ψ∗(y(k)))]
=E[Ak+1(f(x(k+1))−f(x∗))−Ak(f(x(k))−f(x∗))] +E[(sk+1−sk)Bψ(x∗,∇ψ∗(y(k+1)))]
+skE[Bψ(∇ψ∗(y(k)),∇ψ∗(y(k+1))) +⟨y(k)−y(k+1),x∗−∇ψ∗(y(k))⟩]
=E[Ak+1(f(x(k+1))−f(x∗))−Ak(f(x(k))−f(x∗))] +E[(sk+1−sk)Bψ(x∗,∇ψ∗(y(k+1)))]
+E[skBψ(∇ψ∗(y(k)),∇ψ∗(y(k+1))) + (Ak+1−Ak)⟨G(x(k+1),ξ(k+1)),x∗−∇ψ∗(y(k))⟩]
36Published in Transactions on Machine Learning Research (05/2024)
where the second equality comes from the above identity, and the third equality from the definition of
y(k). We further compute using the optimality conditions in Equations (18) and (19) and expanding
G(x(k+1),ξ(k+1)) =∇f(x(k+1)) + ∆k+1:
E(k+1)−E(k)=E[Ak+1(f(x(k+1))−f(x∗))−Ak(f(x(k))−f(x∗))] +E[(sk+1−sk)Bψ(x∗,∇ψ∗(y(k+1)))]
+E[skBψ(∇ψ∗(y(k)),∇ψ∗(y(k+1)))]
+E[(Ak+1−Ak)⟨∇f(x(k+1)),x∗−x(k+1)⟩−Ak⟨∇f(x(k+1)),x(k+1)−x(k)⟩]
+E[(Ak+1−Ak)⟨∆k+1,x∗−∇ψ∗(y(k))⟩]
+E[(Ak+1−Ak)⟨G(x(k+1),ξ(k+1)),U(k)⟩]
≤E[Ak+1(f(x(k+1))−f(x∗))−Ak(f(x(k))−f(x∗))] +E[(sk+1−sk)Bψ(x∗,∇ψ∗(y(k+1)))]
+E[skBψ(∇ψ∗(y(k)),∇ψ∗(y(k+1)))]
+E[(Ak+1−Ak)(f(x∗)−f(x(k+1))) +Ak(f(x(k))−f(x(k+1)))⟩]
+E[(Ak+1−Ak)⟨∆k+1,x∗−∇ψ∗(y(k))⟩]
+E[(Ak+1−Ak)⟨G(x(k+1),ξ(k+1)),U(k)⟩]
=E[(sk+1−sk)Bψ(x∗,∇ψ∗(y(k+1)))] +E[skBψ(∇ψ∗(y(k)),∇ψ∗(y(k+1)))]
+E[(Ak+1−Ak)⟨∇f(xk+1),U(k)⟩]
where the second inequality follows from convexity of f, and the final equality from zero mean of ∆k+1and
independence with y(k),U(k).
Recalling that since ψisα-strongly convex, we have that ψ∗is1/α-strongly smooth. Using this, we have
the following bound on the Bregman divergence:
Bψ(∇ψ∗(y(k)),∇ψ∗(y(k+1))) =Bψ∗(y(k+1),y(k))≤1
2α∥y(k+1)−y(k)∥2
∗
≤8L2
fMψ+ 2ασ2+ 4∥∇f(x∗)∥2
∗
α(Ak+1−Ak)2
2αs2
k,
where the final iterate comes from the fact that for any x∈X,∥∇f(x)∥∗≤Lf√
2Mψ√α+∥∇f(x∗)∥∗, which is
derived from the smoothness of f, strong convexity of ψand bounded domain assumption. Furthermore, we
haveBψ(x∗,∇ψ∗(y(k+1))≤Mψ. Further assume that ⟨∇f(x(k+1)),U(k)⟩is bounded, say by K. Substituting
back we have
E(k+1)−E(k)≤Mψ(sk+1−sk) +4L2
fMψ+ασ2+ 2∥∇f(x∗)∥2
∗
α2(Ak+1−Ak)2
sk+|Ak+1−Ak|K.
Assuming that Akis monotonically increasing (which will be justified later), we can sum from 0tok−1
and get
E(k)≤E(0)+Mψsk+4L2
fMψ+ασ2+ 2∥∇f(x∗)∥2
∗
α2k−1/summationdisplay
j=0(Aj+1−Aj)2
sj
+AkK.
TakingAj=j(j+ 1)/2(which is monotonically increasing) and sj= (j+ 1)3/2, we get
E(k)≤E(0)+Mψ(k+ 1)3/2+4L2
fMψ+ασ2+ 2∥∇f(x∗)∥2
∗
α2k−1/summationdisplay
j=0/radicalbig
j+ 1 +AkK
≤E(0)+Mψ(k+ 1)3/2+8L2
fMψ+ 2ασ2+ 4∥∇f(x∗)∥2
∗
α2(k+ 1)3/2+AkK.
where we use the following lemma for divergent series for the final inequality.
37Published in Transactions on Machine Learning Research (05/2024)
Lemma 7 (Chlebus 2009) .Forp<0,
1 +k1−p−1
1−p≤k/summationdisplay
j=1j−p≤(k+ 1)1−p−1
1−p(43)
Dividing throughout by Ak, we get
E[f(x(k))−f(x∗)]≤E(0)/Ak+(k+ 1)3/2
Ak/bracketleftigg
Mψ+8L2
fMψ+ 2ασ2+ 4∥∇f(x∗)∥2
∗
α2/bracketrightigg
+K(44)
=K+O(k−2+k−1/2).
D Proofs in Section 6
D.1 Proof of Proposition 2
Proposition 2. IfΩ0is gradient descent on the G-invariant objective function L, then it is G-equivariant,
i.e.Ω0(g·z) =g·[Ω0(z)]for allg∈G,z∈Z.
Proof.Wefirstcheckthat ∇Lcommuteswith gactions. Forany y∈Z, thederivativeoperator DL:Z→Z∗
satisfies
DL(g·z)(y) = lim
t→0L(g·z+ty)−L(g·z)
t
= lim
t→0L(z−tg−1·y)−L(z)
t
=DL(z)(g−1y),
where the second inequality holds since L(g·z) =L(z), applied with g−1and linearity of gactions. Let ι
denote the canonical isomorphism ι:Z∗→Z. For anyy∈Z,
⟨ι−1∇L(g·z),y⟩=DL(g·z)(y)
=DL(z)(g−1·y)
=⟨ι−1∇L(z),g−1·y⟩
=⟨g·ι−1∇L(z),y⟩.
Hence,Gcommutes with ι−1∇L=DL. We now have that
Ω0(g·z) =g·z−η(∇L)(g·z)
=g·z−ηg·∇L(z)
=g·(z−ηg∇L(z))
=g·Ω0(z),
where the first equality comes from the fact that gtrivially commutes with ι(and thusι−1) using the induced
group action, and the second and third equality comes from the linearity of gactions. Since this is true for
ally∈Z, by the Riesz representation theorem, we have ∇L(g·z) =g·∇L(z).
E Step-size comparison for baseline methods
Here we detail the step-sizes used to compare. In general, we choose the best performing method at 100
iterations,with preference given to the one converging to a smaller value at higher iterations in case of
ambiguity. Note later iteration cutoffs usually mean insignificant performance at earlier iterations. The grid
used for the step-sizes are of the form {1,2,5}×10−k. The endpoints are
38Published in Transactions on Machine Learning Research (05/2024)
•GD [2×10−4,1×10−1]
•Adam [1×10−3,1×10−1]
•Nesterov [1×10−4,5×10−3]
Figure 9 shows the loss evolution for the proposed step-sizes up to 5000 iterations. We note that the grid is
sufficiently wide such that smaller or larger step-sizes are suboptimal, or lead to slow convergence in the low
iteration count setting. The specific choices are as below.
Image denoising. GD: 1×10−2. Adam: 2×10−3. Nesterov: 1×10−3.
Image inpainting. GD: 1×10−2. Adam: 1×10−2. Nesterov: 2×10−3.
SVM training. GD: 1×10−2. Adam: 2×10−2. Nesterov: 5×10−3.
100101102103
Iteration100101102f(x(k))f*
2e-4
5e-4
1e-3
2e-3
5e-3
1e-2
2e-2
5e-2
1e-1Reconstruction Loss
(a) GD (denoise)
100101102103
Iteration100101102103f(x(k))f*
2e-4
5e-4
1e-3
2e-3
5e-3
1e-2
2e-2
5e-2
1e-1Reconstruction Loss (b) GD (inpaint)
100101102103
Iteration102
101
100101102103104105f(x(k))f*
2e-4
5e-4
1e-3
2e-3
5e-3
1e-2
2e-2
5e-2
1e-1Reconstruction Loss (c) GD (SVM)
100101102103
Iteration100101102103f(x(k))f*
1e-3
2e-3
5e-3
1e-2
2e-2
5e-2
1e-1Reconstruction Loss
(d) Adam (denoise)
100101102103
Iteration100101102103f(x(k))f*
1e-3
2e-3
5e-3
1e-2
2e-2
5e-2
1e-1Reconstruction Loss (e) Adam (inpaint)
100101102103
Iteration102
101
100101102103104f(x(k))f*
1e-3
2e-3
5e-3
1e-2
2e-2
5e-2
1e-1Reconstruction Loss (f) Adam (SVM)
100101102103
Iteration101
100101102f(x(k))f*
1e-4
2e-4
5e-4
1e-3
2e-3
5e-3Reconstruction Loss
(g) Nesterov (denoise)
100101102103
Iteration100101102103f(x(k))f*
1e-4
2e-4
5e-4
1e-3
2e-3
5e-3Reconstruction Loss (h) Nesterov (inpaint)
100101102103
Iteration103
101
101103f(x(k))f*
1e-4
2e-4
5e-4
1e-3
2e-3
5e-3Reconstruction Loss (i) Nesterov (SVM)
Figure 9: Comparison of the baseline methods with various step-sizes for the ellipse denoising and image
inpainting tasks. Vertical gray line represents iteration 2000, which is the maximum iteration considered in
the main text figures.
F Comparison of loss against time
We consider the image inpainting experiment and plot against the wall-clock time instead of the iteration
count. In Figure 10, we consider various different step-sizes for the baselines GD, Nesterov accelerated GD,
and Adam, as chosen in Appendix E. Each solid line corresponds to a different choice of step-size. We observe
39Published in Transactions on Machine Learning Research (05/2024)
that the baseline methods plateau after a period of fast optimization, while the proposed LAMD method is
able to continue decreasing.
0 5 10 15 20 25
Time (s)104
102
100102104
GD
Nesterov
Adam
LAMD
LSMD
LASMD
LMDReconstruction Loss
Figure 10: Plot of reconstruction loss against wall-clock time for image inpainting experiment.
In Figure 11, we plot the training loss of CNN training on MNIST as in Section 7.2. Due to the compact
parameterization, we observe that the diagonal LMD methods introduce minor overheads compared to the
baseline SGD and Adam methods, which have been optimized in the PyTorch framework, retaining its
competitive nature when plotting against iteration count.
0.00 0.05 0.10 0.15 0.20 0.25
Time (s)010203040Cross-entropy LossSGD
Adam
LAMD
LMDTraining negative log-likelihood
Figure 11: Plot of training cross-entropy loss against wall-clock time when training CNN on MNIST. The
wall-clock time includes only the optimization phases and excludes overhead resulting from testing at each
iteration.
40Published in Transactions on Machine Learning Research (05/2024)
G Monotonic spline parameterization
In Section 7, we utilize an element-wise spline-based parameterization (shared across parameters of the same
layer) of the mirror map. Here we detail the implementation of the spline.
Recall that we define the mirror map (acting element-wise) to be
ψ:R→R, ψ(x) =/integraldisplayx
0σ(t)dt.
Here,σis a learnable piecewise-linear monotonic spline, with σ(0) = 0. Thusψ(x)is a piecewise quadratic
monotonic spline. σcan be parameterized in terms of knot locations t−K<...<t 0= 0<...<tK, as well as
the valuesci=σ(ti), with linear interpolation between the intervals (and extrapolation on (−∞,t−K)and
(tK,+∞)). The monotonic condition can be enforced during training by forcing c−K<c−K+1<...<cK.
We note that shifting σup or down by a constant does not change the mirror map dynamics, so without
loss of generality c0can be taken to be 0.
σ(x) =

ci+x−ti
ti+1−ti(ci+1−ci) , x∈[ti,ti+1];
c−K+1−t−K+1−x
t−K+1−tK(c−K+1−c−K), x<t−K;
cK−1+x−tK−1
tK−tK−1(cK−cK−1), x>tK.
In our implementation, we fix the knots as in Goujon et al. (2022) to be ti= 0.05i, i=−20,...,20, which
reduces the number of learnable parameters to only the values of σat the knots, which is 2K= 40. One
useful point of this characterization is that the inverse of σcan be easily computed by computing the values
at the knots σ(ti)and linearly interpolating.
H Reptile Initialization for Section 7.3
In Section 7.3, we compared LMD with baseline methods from a meta-learning objective. Informally, Reptile
can be treated as an algorithm that finds a solution that is close to each task’s manifold of optimal solutions.
Here we detail how the Reptile initialization is computed for our purpose. The algorithm is as follows:
Algorithm 8 Reptile (Nichol et al., 2018, Alg. 1)
1:Initialize parameters θ
2:fork= 1,...,Kdo
3:Sample task f∈F
4:Performnsteps of SGD/Adam on ffromθto obtain parameters ˜θ
5:Update initialization θ←θ+ϵ(˜θ−θ)
6:end for
7:returninitialization θ
We consider training on the 5-shot 10-way Omniglot dataset for K= 5×105meta-epochs. The inner
optimization loop in Step 4 is done using full-batch SGD with learning rate 1×10−2forn= 10iterations.
The meta-stepsize ϵis decreased linearly from ϵ= 0.1to0over the entire training process. This returns the
Reptile initialization θK.
41