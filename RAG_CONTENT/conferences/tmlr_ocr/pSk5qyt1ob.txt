Under review as submission to TMLR
On Training-Conditional Conformal Prediction and Binomial
Proportion Confidence Intervals
Anonymous authors
Paper under double-blind review
Abstract
Estimating the expectation of a Bernoulli random variable based on Nindependent trials is
a classical problem in statistics, typically addressed using Binomial Proportion Confidence
Intervals (BPCI). In the control systems community, many critical tasks—such as certifying
the statistical safety of dynamical systems—can be formulated as BPCI problems.
Conformal Prediction (CP), a distribution-free technique for uncertainty quantification, has
gained significant attention in recent years and has been applied to various control systems
problems, particularly to address uncertainties in learned dynamics or controllers. A variant
known as training-conditional CP was recently employed to tackle the problem of safety
certification.
In this note, we highlight that the use of training-conditional CP in this context does not
provide valid safety guarantees. We demonstrate why CP is unsuitable for BPCI problems
and argue that traditional BPCI methods are better suited for statistical safety certification.
1 Introduction
Uncertainty quantification is a critical aspect in fields where predictions influence safety and performance
guarantees, such as in control systems. Probabilistic guarantees, including those derived from the theory
of Probably Approximately Correct (PAC) learning, play an important role in providing bounds on the
accuracy of predictions under limited training data.
ConformalPrediction(CP)isoneoftheapproachesthathasgainedvisibilityduetoitsabilitytoprovidevalid
predictionsetswithoutrequiringstrongdistributionalassumptions. AdistinctivecharacteristicofCPisthat,
rather than providing a point prediction of the variable of interest, it provides set predictions with a valid
bound on the probability that the predicted set contains the true variable (Vovk et al., 2005). This technical
note focuses on a specific formulation of CP known as training-conditional CP (Vovk, 2012). However,
existing applications in areas such as safety verification for dynamical systems have shown limitations in the
interpretation of these guarantees. In particular, recent works have applied training-conditional CP to safety
verification problems in control systems (Chilakamarri et al., 2024; Lin & Bansal, 2024; Vincent et al., 2024).
While promising, these applications have misinterpreted the implications of CP’s set prediction framework,
especially in cases where the underlying data can be modeled as Bernoulli random variables. This paper
aims to rigorously analyze these limitations and provide an alternative framework for interpreting PAC-
based guarantees in such contexts. In section 2 we recall existing methods for estimating the expectation of
a Bernoulli random variable. In section 3 we introduce the formalism of training-conditional CP, followed
by a detailed analysis of its PAC guarantees. In section 4 we present a special case of interest, where the
nonconformity measure corresponds to an indicator function, leading to Bernoulli-distributed conformity
scores. We demonstrate that the PAC guarantees derived from this setting are unsuitable for estimating the
expectation of a Bernoulli random variable.
1Under review as submission to TMLR
2 Binomial Proportion Confidence Intervals
Consider a setting where there are N+ 1independent and identically distributed (i.i.d.) Bernoulli random
variables (r.v.) R1,R2,...,RN,RN+1with parameter b, i.e.Ri∼Bernband Pr Bernb(Ri= 1).=b. Given a
realization of the first Nr.v., the problem is to estimate an interval of values for the probability that the
N+ 1-th variable will be equal to 1, or in other words we want to estimate the parameter b. This is a very
well studied problem and it is known in the literature under the name of Binomial Proportion Confidence
Intervals (BPCI), see Dean & Pagano (2015) for a survey. We give below a quick overview of the setting.
Define the new r.v. Y.=/summationtextN
i=1Ri. It is well known that Yhas a binomial distribution Y∼BinN,bwithN
trials and probability of success b, defined by Pr BinN,b(Y=y).=/parenleftbigN
y/parenrightbig
by(1−b)N−yfory∈Z[0,N], where Z[0,N]
denotes the integers 0,1,...,N. Let ˇb:Z[0,N]→[0,1]andˆb:Z[0,N]→[0,1]be two random variables serving
as interval estimators. The coverage probability of the interval estimator [ˇb,ˆb]forY∼BinN,bis defined as
ρ(b,ˇb,ˆb).=PrBinN,b(ˇb(Y)≤b≤ˆb(Y)). (1)
In the expression above bis fixed and it’s the true parameter of the binomial distribution describing Y. Note
that ˇbandˆbare a transformation of the same random variable Y. This expression can also be rewritten
equivalently as
ρ(b,ˇb,ˆb) =/summationdisplay
y∈IPrBinN,b(Y=y),
whereI.={y∈Z[0,N]:ˇb(y)≤b≤ˆb(y)}. Forα∈(0,1)an interval estimator [ˇb,ˆb]is aconservatively valid
(sometimes also called ’exact’ or ’secure’) 1−αconfidence interval if the coverage probability ρ(b,ˇb,ˆb)is
greater or equal to 1−αfor all the values of b. An example of a conservatively valid interval estimator is
given by the Clopper-Pearson method (Clopper & Pearson, 1934), see also Dean & Pagano (2015) for more
estimators.
Before concluding this section, we rewrite equation 1 in an equivalent form that is more commonly found in
the literature on Probably Approximately Correct (PAC) bounds. First, note that Pr Bernb(RN+1= 1) =b.
Second, since Yisar.v. obtainedasatransformationofthei.i.d. Bernoullirandomvariables R1,...,RN, the
probability of any event M⊆Z[0,N], PrBinN,b(Y∈M)can be equivalently described by PrN
Bernb({(r1,...,rN) :/summationtext
i≤Nri∈M}), where PrN
Bernbis the product probability measure induced by the Ni.i.d. Bernoulli random
variables. Hence, we rewrite the definition of a conservatively valid 1−αconfidence interval by revisiting
equation 1:
PrBinN,b(ˇb(Y)≤b≤ˆb(Y)) =PrN
Bernb
ˇb
/summationdisplay
i≤NRi
≤PrBernb(RN+1= 1)≤ˆb
/summationdisplay
i≤NRi

≥1−α,(2)
for allb∈[0,1]. We will use this form of the coverage probability to draw a comparison with the guarantees
given by training-conditional CP.
3 Training-conditional Conformal Prediction
Conformal Prediction is a statistical tool that uses the available data sampled from identically and indepen-
dently from an underlying distribution to output predictions for which an error probability can be computed.
The original formulation of CP can be informally explained as follows. Suppose that we want to solve a
classification problem and we have method that given a feature xoutputs a label ˆy. Given a desired error
probability ϵ, conformal prediction uses the available data to generate a set of labels , typically containing ˆy,
containing the true label ycorresponding to the feature xwith a probability not smaller than 1−ϵ(Shafer &
Vovk, 2008). It is a method capable of augmenting a (usually unreliable) point prediction to a set prediction
with probabilistic guarantees of correctness, i.e. it construct a set predictor . The original formulation of CP
has been successfully applied to both classification and regression problems, see Angelopoulos et al. (2023);
Fontana et al. (2023) for a recent survey.
2Under review as submission to TMLR
In this section we introduce instead the basic concepts of training-conditional CP (Vovk, 2012). Training-
conditional CP is a variant of the original formulation of CP. While the quality of the guarantees differs
from the original, the core idea remains the same, that is, constructing set predictions with some form of
guarantees: training-conditionalCPproducesPAC-styleguarantees. Inthefollowing,wegiveaself-contained
overview of the theoretical details of training-conditional CP.
Let(Z,F,P)be a probability space where Z,FandPdenote a sample set, a σ-algebra, and a probability
measure respectively, and consider L+ 1i.i.d. random variables (r.v.) Z′
1,...,Z′
M,Z1,...,ZNandZN+1with
L=N+M. LetZ′
ifori= 1,...,Mbe thetraining set andZifori= 1,...,Nbe thecalibration set .
Note thatZN+1is not part of either set. We use the lower case of a r.v. to denote a realization1. An
Inductive Nonconformity M-measure (INM) is a measurable function A:ZM×Z→R. While no additional
requirements are needed for A, intuitively an effective INM will assign a high real number to any element
inZthat does not conform to a training set (in ZM). AnInductive Nonconformal Predictor (INP) is a set
predictor defined as
Γϵ(z1,...,zN,z′
1,...,z′
M).={z∈Z:pz>ϵ}, (3)
whereϵ∈[0,1]is thesignificance level , thep-values are defined as
pz.=|{i:Ri≥Rz}|+ 1
N+ 1, (4)
and
Ri.=A((z′
1,...,z′
M),zi)fori= 1,...N, Rz.=A((z′
1,...,z′
M),z), (5)
are thenonconformity scores .
In the following, when it is clear from the context we omit the arguments of the INP and write Γϵinstead of
Γϵ(z1,...,zN,z′
1,...,z′
M). Intuitively, zbelongs to the INP Γϵif there are strictly more than ⌊ϵ(N+ 1)−1⌋
elementsRiinthecalibrationsetwithahigher(worse)orequalnonconformityscorethan Rz. Itiseasytosee
thatϵ′<ϵ′′implies that Γϵ′′⊆Γϵ′. The INP is the set predictor mentioned in the discussion at the beginning
of this section: similarly to the original formulation of CP, given some prediction method depending on the
training set, the INP uses the available calibration set to produce a set prediction guaranteed to contain
the correct prediction. The elements included in the set prediction are all the z∈Zthat conform well
enough with the calibration set, according to the chosen INM. The following theorem specifies the PAC-style
guarantees for training-conditional CP.
Theorem 1 (Vovk (2012)) Chooseϵ,E∈[0,1]2, fix the training set Z′
1=z′
1,...,Z′
M=z′
M, letNbe the
size of the calibration set, and consider the event
SE.={(z1,...,zN)∈ZN:P(ZN+1∈Γϵ(z1,...,zN,z′
1,...,z′
M))≥1−E} (6)
in theσ-algebraFNof the product probability space (ZN,FN,PN), where Γϵis defined according to equations
3-5. It holds that
PN(SE)≥1−δ, (7)
whereδ.=BinN,E(J) =/summationtextJ
j=0/parenleftbigN
j/parenrightbig
Ej(1−E)N−jis the cumulative binomial distribution with Ntrials and
probability of success E, withJ.=⌊ϵ(N+ 1)−1⌋.
The quantities 1−δand1−Eare sometimes referred to as the confidence andcoverage probability (which
is not the coverage probability mentioned in section 2). Theorem 1 is to be understood in the following way.
Given two values ϵandE, for the given training set, the event SEis the subset of ZNcontaining all the
1Our considerations hold also for the case where Z=X×YwhereXandYrepresent a measurable feature space and label
space respectively and each z∈Zmay be written as z= (x,y)wherex∈Xis some feature and y∈Ya label. For clarity we
omit the exact structure of Z.
2A brief note on the notation. In the original formulation of CP ϵhas a double role: it is the significance level (appearing
as the index to the INP Γϵ) and it describes the coverage probability as 1−ϵ, see Shafer & Vovk (2008) for details. In the
training-conditional formulation the latter role is covered by E, that is 1−Eis the coverage probability and ϵremains the
significance level, see Vovk (2012).
3Under review as submission to TMLR
tuples (z1,...,zN)such that the INP Γϵcontains a realization of ZN+1with probability at least 1−E, or,
in other words, Γϵreturns a subset of Zof measure at least 1−E. Byequation 7the measure of this set
of tuplesSEis at least 1−δ, whereδdepends on ϵ,βandN. This form of guarantees where a double
layer of nested probabilities is present is called Probably Approximately Correct (PAC). Moreover, this is
a distribution-free result, that is, it holds for every Pas long as the samples used to construct the INP are
i.i.d. and P-distributed. In particular, in this work we focus on Bernoulli-distributed r.v.’s, and Theorem 1
holds for any value of the parameter bof a Bernoulli distribution. Observe that the confidence 1−δand
the quantity 1−αmentioned in section 2 play a similar role in that they described the outmost layer of
probability, compare for instance equations equation 7 and equation 2. Finally, we note that ϵandEare
chosena priori; in other words, they cannot be defined as random variables depending on a realization of
the calibration set, as is erroneously done in Lin & Bansal (2024).
4 A Special Case of Interest
In this section we draw a parallel between the BPCI and training-conditional CP and show the fundamental
difference between the two approaches.
Let the INM be an indicator function for the set Q⊂Z, that is
A((z′
1,...,z′
M),z).=/braceleftigg
1ifz∈Q,
0ifz∈Q.(8)
TypicallyQdepends on z′
1,...,z′
M; for instance, in the case of binary classification problems the training set
may be used to train a parametrized function to assign one of the two labels to all z∈Q, as in Support
Vector Machines. We omit this dependency here, since Theorem 1 assumes that the training set is given. A
pointzwith high nonconformity scores is to be interpreted as poorly conforming to the training set. For
this reason, in section 4.1 the set Qwill represent the unsafe region of a dynamical system.
For a fixed training set the nonconformity scores of the calibration set are i.i.d. Bernoulli distributed with
parameterb, i.e.Ri∼Bernb, whereb.=P(Q). Using a BPCI method it is directly possible to derive a
conservatively valid confidence interval for the parameter bdescribing the probability of drawing a sample in
Q, as shown in section 2. Is it possible to derive a conservatively valid confidence interval for the parameter
bfrom the calibration set using training-conditional CP? The answer is no, and we proceed with an example.
Example 1 - Part 1.
Suppose that the calibration set has size 2, i.e. N= 2. Up to reindexing, there are three distinct outcomes.
Case 1:With probability (1−b)2we havez1,z2/∈Q, resulting in nonconformity scores R1=R2= 0. We
construct the prediction set Γϵfollowing its definition equation 3. For all zinQwe have that Rz= 1,
resulting in|{i≤2 :Ri≥Rz}|= 0andpz=1
3, meaningzhas the highest (worst) nonconformity score.
Whereas for all zinQwe have that Rz= 0resulting in and pz= 1. The inclusion of zin the predicted
setΓϵdepends on the significance level ϵ. We distinguish two situations, either ϵ∈[1
3,1)orϵ∈[0,1
3). If
ϵ∈[1
3,1)then anyz∈Qis excluded from Γϵsince1
3≤ϵ. On the other hand, any z∈Qis included in Γϵ,
since 1>ϵ. To summarize if ϵ∈[1
3,1)then Γϵ=Q. Ifϵ∈[0,1
3)then Γϵ=Z, since any z∈Q∪Q=Zhas
a sufficiently high p-value.
Case 2:With probability 2b(1−b)we have (z1∈Q∧z2∈Q)or(z2∈Q∧z1∈Q)henceR1∪R2={0,1}. If
Rz= 1thenpz=2
3, whereas if Rz= 0thenpz= 1. We conclude that if ϵ∈[0,2
3)then Γϵ=Z, ifϵ∈[2
3,1)
then Γϵ=Q.
Case 3:With probability b2we havez1,z2∈QandR1=R2= 1. IfRz= 1thenpz= 1, but also if Rz= 0
thenpz= 1. Then for any significance level ϵ∈[0,1)it holds Γϵ=Z.
In summary, for any fixed value of ϵthe INP is entirely defined by the calibration set through equations 3-5;
as a result the set predictor Γϵcan be thought equivalently as a discrete random variable with support Q,
QandZ, see Figure 1.
Example 1 - Part 2.
4Under review as submission to TMLR
(z1,z2)
QZZ
Q×Q Q×QQ×Q Q×Q
ϵ QQZ
ΓϵMass
Figure 1: On the left, a representation of the product space Z2=Z×Z, partitioned accordingly to the
setsQandQ, and a hypothetical calibration set (z1,z2)as in Case 1. On the right, a summary of Case 1,
2 and 3. On the x-,y-,z-axes are represented the values of ϵ, the prediction (or support) of the INP, and
the probability mass function respectively. For any given ϵ, the INP Γϵcan be viewed as a discrete random
variable with support Q,QandZ. In the figure, for ϵ= 0.8andb= 0.3, the INP predicts Qwith probability
0,Zwith probability b2, andQwith probability 1−b2.
Now, fixE∈[0,1]and consider any ϵ∈[2
3,1). A straightforward application of Theorem 1 tells us that
P2(SE)≥E2. (9)
where
SE.={(z1,z2)∈Z2:P(Z3∈Γϵ(z1,z2,z′
1,...,z′
M))≥1−E}
We provide the correct interpretation of the above result, and we conclude that equation 9 does not provide
a confidence interval for the probability of drawing a new sample in Q, or conversely in Q. For the chosen
significance level ϵ∈[2
3,1),Γϵ=Zwith probability b2(from Case 3) and Γϵ=Qwith probability 1−b2
(from Case 1 and 2)3, see Figure 1. Theorem 1 is a distribution-free result and as such it holds for all values
ofb. We distinguish two situations, b≤Eandb>E:
1) Suppose that b≤E(and hence 1−b≥1−E). Ifz1,z2∈Q(Case 3) we have that P(ZN+1∈Γϵ) =
P(Z) = 1≥1−E, henceQ×Q⊆SE. If at least one of z1andz2belongs toQ(Case 1 and 2) we have
thatP(ZN+1∈Γϵ) =P(Q) = 1−b≥1−E, henceQ×Q⊆SE, which means that SE=Z2. Trivially,
P2(SE) =P2(Z2) = 1≥E2.
2) On the other hand, suppose that b > E(and hence 1−b <1−E). Ifz1,z2∈Q(Case 3), as before,
P(ZN+1∈Γϵ) =P(Z) = 1≥1−E, and once again Q×Q⊆SE. If at least one of z1andz2belongs toQ
(Case 1 and 2) then P(ZN+1∈Γϵ) =P(Q) = 1−b<1−E, hence such z1andz2do not belong to SEby
definition. Then P2(SE) =P2(Q×Q) =b2≥E2.
Theorem 1 holds for both cases b≤Eandb > E, since P2(Z2) = 1≥E2andP2(Q×Q) =b2≥E2
respectively. Assume that the true value of bis greater than value of Ethat we fixed (and smaller than 1)
and that the calibration set returns R1= 0andR2= 1. What can we say about b?
For the given calibration set and significance level the INP predicts Γϵ=Q, hence it is tempting to say that
P2(P(Q)≥1−E) =P2(1−b≥1−E)≥E2, or equivalently P2(b≤E)≥E2: recalling equation 2, we
3IfΓϵpredictsQit implies that the nonconformity score of ZN+1is predicted to be 0, whereas if it predicts Zthen all we
know is that the nonconformity score of ZN+1is predicted to be in {0,1}which is not useful.
5Under review as submission to TMLR
may conclude that [0,E]is aE2confidence interval for b. But this is clearly not true: since we assumed
thatb > Ethe interval [0,E]will never contain the parameter b(note that none of the arguments of P2()
depends on (z1,z2)in the preceding statement, unlike equation 2). We conclude from this example that this
is not a viable path to obtain a PAC bound for bcomparable to equation 2.
The example above leads us to the following remark and main message of this technical note.
Remark 1 Theorem 1 is a statement on the correctness of the setpredictor Γϵ. Adopting the frequentist
point of view, it is a statement on how often the set predictor Γϵconstructed from Nsamples attains the
desired coverage level 1−Efor a new realization of ZN+1. In other words, since we do not know the true
valueb, ifb > Ethe INP is correct (it attains the desired coverage level) only when Γϵ=Z(but this is
uninformative), and it is incorrect when Γϵ=Q. Essentially, the confidence level of E2is attained by
making trivial predictions sufficiently often. If instead b≤E, the INP is always correct. Since bis precisely
the quantity we are looking to estimate, and therefore unknown, this is not a valid reasoning to estimate it.
Theorem 1 does not provide information on the probability of seeing a specific score, or class, which is what
BPCI methods do. See the appendix for a graphical representation.
To further stress this point, consider the equivalent set predictor mapping the elements zpredicted by Γϵto
their respective nonconformity score
Γϵ(z1,...,zN,z′
1,...,z′
M).=/uniondisplay
z∈Γϵ(z1,...,zN,z′
1,...,z′
M)A((z′
1,...,z′
M),z),
whichamountsto Γϵ={0,1}when Γϵ=ZandΓϵ={0}when Γϵ=Q, andletRN+1.=A((z′
1,...,z′
M),ZN+1)
be the score of the N+ 1-th sample: then we can equivalently substitute the event RN+1∈ΓϵtoZN+1∈Γϵ
in equation 6. In a nutshell, both BPCI methods and training-conditional CP yield PAC guarantees. The
difference lies in the subject of the guarantees: while BPCI methods compute an interval containing the
true valuebdescribing the probability of the event that the N+ 1-th score equals 1, i.e.RN+1= 1(with
probability not less than 1−α), the training-conditional CP computes a lower bound for the probability of
the event that the N+ 1-th score is contained in the predicted set of scores, i.e. RN+1∈Γϵ(with probability
not less than 1−δ).
Remark 1 can be extended to any situation where the nonconformity score can take only a finite set of
values (as the binary case), therefore defining a classification problem. Essentially training-conditional CP
provides a methodology to construct a set predictor, guaranteed to attain the desired coverage level with
a minimum confidence. The predictor can output tight sets (corresponding to a low number of classes) for
‘good’ calibration data and loose sets (corresponding to a high number of classes) for ‘poor’ calibration data;
on average the probability that the calibration data returns a predictor attaining the coverage level of 1−E
is not smaller than 1−δ. Depending on the parameters’ choice ϵandE, we have shown that the 1−δ
confidence may be achieved simply by choosing the entire sample space (corresponding to all the classes)
sufficiently often, see also Figure 2. However, this does not provide useful information on the probability of
seeing one specific class, which is instead what equation 2, and in general BPCI methods.
4.1 A Note on Safety Verification for Dynamical Systems
Recent papers have used training-conditional conformal prediction, particularly Theorem 1, to provide PAC
guarantees on the safety of control systems with neural network-based controllers (Chilakamarri et al., 2024;
Lin & Bansal, 2024), and more broadly, on the safety of autonomous systems (Vincent et al., 2024). In this
section we show that these papers follow the reasoning outlined in section 4, and are therefore incorrect.
Below, we follow the notation used in Lin & Bansal (2024), but the same applies to the other works.
Consider a dynamical system defined by ˙x=f(x)wherex∈X⊆Rn, a fixed time horizon T∈R>0. Denote
byξx(τ)forτ∈[0,T]the state trajectory of the system at time τwhen initialized at x(for simplicity we
assume that the solution to the differential equation exists and is unique)4. LetXA⊂Xrepresent a set of
4In the original paper the trajectory ξdepends on a learned controller and depends on a training set Z′
1,...,Z′
M. For clarity
we omit this dependence here, since the training set is given and is fixed.
6Under review as submission to TMLR
undesirable states, and consider the cost function defined as
J(x).= min
τ∈[0,T]d(ξx(τ)),
whered:X→Ris such that d(x)≤γ⇐⇒x∈XAandd(x)> γ⇐⇒x∈X\XAwithγ∈R. The
functiondcan be interpreted as a measure of the distance between a point in the domain and the unsafe
setXA. An instructive example for the discussion is below is to choose γ= 0andd:X→{0,1}, with
d= 0⇐⇒x∈XAandd= 1⇐⇒x∈X\XA, but the same applies for any different choice. In
other words, Jassigns a positive real number to a point x∈Xif and only if the state trajectory from x
never intersects with XA. Let (X,F,P)be a probability space. To quantify the safety of this system in
probabilistic terms, it is of interest estimating the probability of drawing a state xsuch thatJ(x)>0. In
other words, we want to estimate P({x:J(x)>0}). Letx1,...,xNbeNvalues inX: in Lin & Bansal
(2024) the authors define the nonconformity score as Ri.=J(xi)fori= 1,...,N, and are therefore interested
in estimating P({x:Rx>0}). However, this is equivalent to defining a nonconformity measure as
A(x).=/braceleftigg
1ifx∈XA,
0ifx∈X\XA,(10)
and we have shown that this line of reasoning is not suitable for estimating the parameter bof a Bernoulli r.v.
Ri∼BernbgivenNi.i.d. realizations of it. The authors in Chilakamarri et al. (2024) rely on the framework
developed by Lin & Bansal (2024), and therefore incur in the same problem. Finally, in Vincent et al. (2024,
Theorem 1) the authors re-derive Theorem 1, originally derived in Vovk (2012). They explicitly make the
claim that training-conditional CP reduces to the Clopper-Pearson confidence interval if the underlying i.i.d.
random variables are Bernoulli distributed (see Sec. Proofs-D therein), yet we have disproved their claim.
5 Conclusion
In this note we examined existing methodologies to use training-conditional CP for statistical safety verifi-
cation, a problem that can be reduced to estimating the expectation of a Bernoulli random variable. While
training-conditional CP remains a powerful tool for uncertainty quantification we have shown that it is not
appropriate for BPCI problems. Specifically, we clarified the correct interpretation of confidence intervals
and PAC-style guarantees for training-conditional CP. We do not rule out the possibility that a different
formulation of CP could be applied to BPCI problems. This is left for future work.
References
Anastasios N Angelopoulos, Stephen Bates, et al. Conformal prediction: A gentle introduction. Foundations
and Trends ®in Machine Learning , 16(4):494–591, 2023.
Vamsi Krishna Chilakamarri, Zeyuan Feng, and Somil Bansal. Reachability analysis for black-box dynamical
systems. arXiv preprint arXiv:2410.07796 , 2024.
Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the case of the
binomial. Biometrika , 26(4):404–413, 1934.
Natalie Dean and Marcello Pagano. Evaluating confidence interval methods for binomial proportions in
clustered surveys. Journal of Survey Statistics and Methodology , 3(4):484–503, 2015.
Matteo Fontana, Gianluca Zeni, and Simone Vantini. Conformal prediction: a unified review of theory and
new challenges. Bernoulli , 29(1):1–23, 2023.
Albert Lin and Somil Bansal. Verification of neural reachable tubes via scenario optimization and conformal
prediction. In 6th Annual Learning for Dynamics & Control Conference , pp. 719–731. PMLR, 2024.
Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research ,
9(3), 2008.
7Under review as submission to TMLR
Joseph A Vincent, Aaron O Feldman, and Mac Schwager. Guarantees on robot system performance using
stochastic simulation rollouts. IEEE Transactions on Robotics , 2024.
Vladimir Vovk. Conditional validity of inductive conformal predictors. In Asian conference on machine
learning, pp. 475–490. PMLR, 2012.
Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world , vol-
ume 29. Springer, 2005.
A Appendix
Figure 2: On the left the curves resulting from b2,q>Eq, on the right the curves resulting from b1,q≤Eq,
forq= 0,...,98.
We validate empirically equation 9 as follows and represent the results graphically in Figure 2.
We define a list of values for EbyEq= 0.01 + 0.01∗qforq= 0,...,98. For every value of Eqwe consider
an underlying Bernoulli distribution with parameter b1,q=Eq−αEq<Eq(right figure) and an underlying
Bernoulli distribution with parameter b2,q=E+αEq%>Eq(left figure) with α= 0.005. For every value
ofq= 0,...,98we examine the two situations b1,q≤Eqandb2,q> Eq, as mentioned in Example 1 - Part
2. The significance level ϵis set to 2/3. We draw ncal= 5·104pairs of calibration points {z(i)
1,z(i)
2}ncal
i=1.
For every pair of calibration points z(i)
1,z(i)
2we construct the resulting INP as Γϵ
(i).= Γϵ(z(i)
1,z(i)
2,...), draw
ntest= 5·104test points{z(j)
N+1}ntest
i=jand compute the empirical frequency ˆgi=|{j=1,...n test:z(j)
N+1∈Γϵ
(i)}|
ntestas
an approximation for P(ZN+1∈Γϵ
(i)); finally we compute ˆh=|{i=j,...n cal:ˆgi≥1−E}|
ncalas an approximation to
P2(SE)shown in the plots as the solid red line. The solid black line represents the curve given by E2, which
remains always below the red line in both plots, as expected. The area shaded in blue represents the fraction
of the ˆgi’s for which the INP Γϵ
(i)is equal to Z, whereas the area shaded in red represents the fraction of
theˆgi’s for which the INP Γϵ
(i)is equal to Qandˆgiis greater or equal than 1−E. It is visible in the left
plot that the only reason why the solid red line (approximating P2(SEq) =b2
2,q) is aboveE2
qis that the INP
is allowed to predict the entire set Z. In contrast, on the right the solid red line approximates P2(SEq) = 1
since any pair of z(i)
1,z(i)
2results in a prediction Γϵ
(i)satisfying P(ZN+1∈Γϵ
(i))≥1−Eq; accordingly, for a
fixedq, the area shaded in red covers approximately 1−b2
1,qof the ’Probability’ axis and the area shaded in
blue approximately b2
1,q.
In summary, in both situations the theorem is confirmed empirically, since the red line is always above the
black line. In the first case, where b2,q>Eq, the minimum confidence level of E2
qis attained by predicting
sufficiently often the entire sample space Z, precisely with a frequency of b2
2,q, as this is the only set prediction
8Under review as submission to TMLR
attaining the required coverage probability of 1−Eq. Unfortunately, a prediction of the entire sample space
is uninformative. In the second case, where b1,q≤Eq, any predicted set between QandZattains the
required coverage probability of 1−Eq.
9