Published in Transactions on Machine Learning Research (12/2022)
Indiscriminate Data Poisoning Attacks on Neural Networks∗
Yiwei Lu yiwei.lu@uwaterloo.ca
University of Waterloo
Gautam Kamath†g@csail.mit.edu
University of Waterloo
Yaoliang Yu‡yaoliang.yu@uwaterloo.ca
University of Waterloo
Reviewed on OpenReview: https://openreview.net/forum?id=x4hmIsWu7e
Abstract
Data poisoning attacks, in which a malicious adversary aims to influence a model by inject-
ing “poisoned” data into the training process, have attracted significant recent attention. In
this work, we take a closer look at existing poisoning attacks and connect them with old and
new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss
function for the attacker and optimizing with algorithms that exploit second-order informa-
tion, we design poisoning attacks that are effective on neural networks. We present efficient
implementations by parameterizing the attacker and allowing simultaneous and coordinated
generation of tens of thousands of poisoned points, in contrast to most existing methods
that generate poisoned points one by one. We further perform extensive experiments that
empirically explore the effect of data poisoning attacks on deep neural networks. Our paper
sets a new benchmark on the possibility of performing indiscriminate data poisoning attacks
on modern neural networks.
1 Introduction
Adversarial attacks have repeatedly exposed critical vulnerabilities in modern machine learning (ML) mod-
els (Nelson et al., 2008; Szegedy et al., 2013; Kumar et al., 2020). As ML systems are deployed in increasingly
important settings, significant effort has been levied in understanding attacks and defenses towards robust
machine learning.
In this paper, we focus on data poisoning attacks . ML models require a large amount of data to achieve
good performance, and thus practitioners frequently gather data by scraping content from the web (Gao
et al., 2020; Wakefield, 2016). This gives rise to an attack vector, in which an adversary may manipulate
part of the training data by injecting poisoned samples. For example, an attacker can activelymanipulate
datasets by sending corrupted samples directly to a dataset aggregator such as a chatbot, a spam filter, or
user profile databases; the attacker can also passively manipulate datasets by placing poisoned data on the
web and waiting for collection. Moreover, in federated learning , adversaries can also inject malicious data
into a diffuse network (Shejwalkar et al., 2021; Lyu et al., 2020).
A spectrum of such data poisoning attacks exists in the literature, including targeted,indiscriminate and
backdoor attacks. We focus on indiscriminate attacks for image classification, where the attacker aims at
decreasing the overall test accuracy of a model by adding a small portion of poisoned points. Current
indiscriminate attacks are most effective against convex models (Biggio et al., 2011; Koh & Liang, 2017;
∗GK and YY are listed in alphabetical order.
†Supported by an NSERC Discovery Grant, an unrestricted gift from Google, and a University of Waterloo startup grant.
‡Supported by an NSERC Discovery Grant, Canada CIFAR AI chair program and WHJIL.
1Published in Transactions on Machine Learning Research (12/2022)
Koh et al., 2018; Shumailov et al., 2021), and several defenses have also been proposed (Steinhardt et al.,
2017; Diakonikolas et al., 2019). However, existing poisoning attacks are less adequate against more complex
non-convex models, especially deep neural networks, either due to their formulation being inherently tied to
convexity or computational limitations. For example, many prior attacks generate poisoned points sequen-
tially. Thus, when applied to deep models or large datasets, these attacks quickly become computationally
infeasible. To our knowledge, a systematic analysis of indiscriminate data poisoning attacks on deep neural
works is still largely missing—a gap we aim to fill in this work.
To address this difficult problem, we design more versatile data poisoning attacks by formulating the problem
as a non-zero-sum Stackelberg game, in which the attacker crafts some poisoned points with the aim of
decreasing the test accuracy, while the defender optimizes its model on the poisoned training set. We exploit
second-order information and apply the Total Gradient Descent Ascent (TGDA) algorithm to address the
attacker’s objective, even on non-convex models.
We also examine the effectiveness of alternative formulations, including the simpler zero-sum setting as well
aswhenthedefenderleadstheoptimization. Moreover, weaddresscomputationalchallengesbyproposingan
efficient architecture for poisoning attacks, where we parameterize the attacker as a separate network rather
than optimizing the poisoned points directly. By applying TGDA to update the attacker model directly, we
are able to generate tens of thousands of poisoned points simultaneously in one pass, potentially even in a
coordinated way.
In this work, we make the following contributions:
•We construct a new data poisoning attack based on TGDA that incorporates second-order optimiza-
tion. In comparison to prior data poisoning attacks, ours is significantly more effective and runs at
least an order of magnitude faster.
•We summarize and classify existing data poisoning attacks (specifically, indiscriminate attacks) in
both theoretical formulations and experimental settings.
•We propose an efficient attack architecture, which enables a more efficient clean-label attack.
•We conduct experiments to demonstrate the effectiveness of our attack on neural networks and its
advantages over previous methods.
Notation. Throughout this paper, we denote training data as Dtr, validation data as Dv, test data asDtest,
and poisoned data as Dp. We use Lto denote the leader in a Stackelberg game, ℓfor its loss function, x
for its action, and θfor its model parameters (if they exist). Similarly, we use Fto denote the follower, f
for its loss function, and wfor its model parameters. Finally, we use εas the poison budget, namely that
|Dp|=ε|Dtr|.
2 Background
In this section, we categorize existing data poisoning attacks according to the attacker’s powerandobjectives ,
and specify the type of attack we study in this paper.
2.1 Power of an attacker
Injecting poisoned samples. Normally, without breaching the defender’s database (i.e., changing the
existing training data Dtr), an attacker can only injectpoisoned data, actively or passively to the defender’s
database, suchthatitsobjectivecanbeachievedwhenthemodelisretrainedaftercollection. Suchasituation
may be realistic when the defender gathers data from several sources, some of which may be untrusted (e.g.,
when scraping data from the public Internet). The goal of the attacker can be presented as:
w∗=w∗(Dp)∈arg min
wL(Dtr∪Dp,w), (1)
where w∗is the attacker’s desired model parameter, which realizes the attacker’s objectives, L(·)is the loss
function. We focus on such attacks and further categorize them in the next subsection.
2Published in Transactions on Machine Learning Research (12/2022)
Perturbing training data. Some work makes the assumption that the attacker can directly change the
training dataDtr. This is perhaps less realistic, as it assumes the attacker has compromised the defender’s
database. We note that this threat model may be more applicable in an alternate setting, where the defender
wishes to prevent the data from being used downstream to train a machine learning model. This research
direction focuses on so called unlearnable examples (Huang et al., 2021; Yu et al., 2021; Fowl et al., 2021b;a),
and has faced some criticism that it provides “a false sense of security” (Radiya-Dixit et al., 2022). We
provide more details of this line of research in Appendix B.
In this paper, we focus on injecting poisoned samples as it is a more realistic attack.
2.2 Objective of an attacker
Data poisoning attacks can be further classified into three categories according to the adversary’s objective
(Cinà et al., 2022; Goldblum et al., 2022).
Targeted attack. The attacker adds poisoned data Dpresulting in a w∗such that a particular target
example from the test set is misclassified as the baseclass (Shafahi et al., 2018; Aghakhani et al., 2020;
Guo & Liu, 2020; Zhu et al., 2019). This topic is well studied in the literature, and we refer the reader to
Schwarzschild et al. (2021) for an excellent summary of existing methods.
Backdoor attack. This attack aims at misclassifying any test input with a particular trigger pattern (Gu
et al., 2017; Tran et al., 2018; Chen et al., 2018; Saha et al., 2020). Note that backdoor attacks require access
to both the training data as well as the input at inference time to plant the trigger.
Indiscriminateattack. Thisattackaimstoinduceaparametervector w∗thatbroadlydecreasesthemodel
utility. We consider image classification tasks where the attacker aims to reduce the overall classification
accuracy. Existing methods make different assumptions on the attacker’s knowledge:
•Perfect knowledge attack: the attacker has access to both training and test data ( DtrandDtest),
the target model, and the training procedure (e.g., the min-max attack of Koh et al. 2018).
•Training-only attack: the attacker has access to training data Dtr, the target model, and the training
procedure (e.g., Muñoz-González et al. 2017; Biggio et al. 2012).
•Training-data-only attack: the attacker only has access to the training data Dtr(e.g., the label flip
attack of Biggio et al. 2011).
In Appendix A we give a more detailed summary of the existing indiscriminate data poisoning attacks.
In this work, we focus on training-only attacks because perfect knowledge attacks are not always feasible
due to the proprietary nature of the test data, while existing training-data-only attacks are weak and often
fail for deep neural networks, as we show in Section 5.
3 Total Gradient Descent Ascent Attack
In this section, we formulate the indiscriminate attack and introduce our attack algorithm. We first briefly
introduce the Stackelberg game and then link it to data poisoning.
3.1 Preliminaries on Stackelberg Game
The Stackelberg competition is a strategic game in Economics in which two parties move sequentially (von
Stackelberg, 1934). Specifically, we consider two players, a leader Land a follower Fin a Stackelberg game,
where the follower Fchooses wto best respond to the action xof the leader L, through minimizing its loss
functionf:
∀x∈X⊆Rd,w∗(x)∈arg min
w∈Wf(x,w), (2)
and the leader Lchooses xto maximize its loss function ℓ:
x∗∈arg max
x∈Xℓ(x,w∗(x)), (3)
3Published in Transactions on Machine Learning Research (12/2022)
where (x∗,w∗(x∗))is known as a Stackelberg equilibrium. We note that an early work of Liu & Chawla
(2009) already applied the Stackelberg game formulation to learning a linear discriminant function, where
an adversary perturbs the whole training set first. In contrast, we consider the poisoning problem where the
adversary can only add a small amount of poisoned data to the unperturbed training set. Moreover, instead
of the genetic algorithm in Liu & Chawla (2009), we solve the resulting Stackelberg game using gradient
algorithms that are more appropriate for neural network models. The follow-up work of Liu & Chawla
(2010) further considered a constant-sum simplification, effectively crafting unlearnable examples (see more
discussion in Appendix B) for support vector machines and logistic regression. Finally, we mention the early
work of Dalvi et al. (2004), who essentially considered a game-theoretic formulation of adversarial training.
However, the formulation of Dalvi et al. (2004) relied on the notion of Nash equilibrium where both players
move simultaneously while in their implementation the attacker perturbs the whole training set w.r.t a fixed
surrogate model (naive Bayes).
Whenf=ℓwe recover the zero-sum setting where the problem can be written compactly as:
max
x∈Xmin
w∈Wℓ(x,w), (4)
see, e.g., Zhang et al. (2021) and the references therein.
For simplicity, we assume W=Rpand the functions fandℓare smooth, hence the follower problem is an
instance of unconstrained smooth minimization.
3.2 On Data Poisoning Attacks
TherearetwopossiblewaystoformulatedatapoisoningasaStackelberggame, accordingtotheactingorder.
Here we assume the attacker is the leader and acts first, and the defender is the follower. This assumption
can be easily reversed such that the defender acts first. Both of these settings are realistic depending on
the defender’s awareness of data poisoning attacks. We will show in Section 5 that the ordering of the two
parties affects the results significantly.
Non-zero-sum formulation. In this section, we only consider the attacker as the leader as the other
case is analogous. Here recall that the follower F(i.e., the defender) aims at minimizing its loss function
f=L(Dtr∪Dp,w)under data poisoning:
w∗=w∗(Dp)∈arg min
wL(Dtr∪Dp,w), (5)
while the leader L(i.e., the attacker) aims at maximizing a different loss function ℓ=L(Dv,w∗)on the
validation setDv:
Dp∗∈arg max
DpL(Dv,w∗), (6)
where the loss function L(·)can be any task-dependent target criterion, e.g., the cross-entropy loss. Thus
we have arrived at the following non-zero-sum Stackelberg formulation of data poisoning attacks (a.k.a., a
bilevel optimization problem, see e.g. Muñoz-González et al. 2017; Huang et al. 2020; Koh et al. 2018):
max
DpL(Dv,w∗),s.t.w∗∈arg min
wL(Dtr∪Dp,w). (7)
Note that we assume that the attacker can inject εNpoisoned points, where N=|Dtr|andεis the power
of the attacker, measured as a fraction of the training set size.
We identify that Equation (7) is closely related to the formulation of unlearnable examples (Liu & Chawla,
2010; Huang et al., 2021; Yu et al., 2021; Fowl et al., 2021b;a; Sandoval-Segura et al., 2022; Fu et al., 2021):
max
DpL(Dv,w∗),s.t.w∗∈arg min
wL(Dp,w). (8)
4Published in Transactions on Machine Learning Research (12/2022)
whereDp={(xi+σi,yi)}N
i=1,σiis the bounded sample-wise or class-wise perturbation ( ∥σi∥p≤εσ). The
main differences lie in the direct modification of the training set Dtr(often all of it). In comparison, adding
poisoned points to a clean training set would never results in 100 % modification in the augmented training
set. This seemingly minor difference can cause a significant difference in algorithm design and performance.
We direct interested readers to Appendix B for details.
Previous approaches. Next, we mention three previous approaches for solving Equation (7).
(1) A direct approach: While the inner minimization can be solved via gradient descent, the outer maxi-
mization problem is non-trivial as the dependence of L(Dv,w∗)onDpisindirectly through the parameter
wof the poisoned model. Thus, applying simple algorithms (e.g., Gradient Descent Ascent) directly would
result in zero gradients in practice . Nevertheless, we can rewrite the desired derivative using the chain rule:
∂L(Dv,w∗)
∂Dp=∂L(Dv,w∗)
∂w∗∂w∗
∂Dp. (9)
The difficulty lies in computing∂w∗
∂Dp, i.e., measuring how much the model parameter wchanges with respect
to the poisoned points Dp. Biggio et al. (2011) and Koh & Liang (2017) compute∂w∗
∂Dpexactly via KKT
conditions while Muñoz-González et al. (2017) approximate it using gradient ascent. We characterize that
the Back-gradient attack in Muñoz-González et al. (2017) can be understood as the k-step unrolled gradient
descent ascent (UGDA) algorithm in Metz et al. (2017):
xt+1=xt+ηt∇xℓ[k](xt,˜wt), (10)
wt+1=wt−ηt∇wf(xt,wt) (11)
whereℓ[k](x,w)is thek-time composition, i.e., we perform ksteps of gradient descent for the leader.
Furthermore, Huang et al. (2020) propose to use a meta-learning algorithm for solving a similar bilevel
optimization problem in targeted attack, and can be understood as running UGDA for Mmodels and taking
the average.
(2) Zero-sum reduction: Koh et al. (2018) also proposed a reduced problem of Equation (7), where the leader
and follower share the same loss function (i.e. f=ℓ):
max
Dpmin
wL(Dtr∪Dp,w). (12)
This relaxation enables attack algorithms to optimize the outer problem directly. However, this formulation
may be problematic as its training objective does not necessarily reflect its true influence on test data.
However, forunlearnableexamples, thezero-sumreductionisfeasible, andmightbetheonlyviableapproach.
See Appendix B for more details.
This problem is addressed by Koh et al. (2018) with an assumption that the attacker can acquire a target
model parameter, usually using a label flip attack which considers a much larger poisoning fraction ε. By
adding a constraint involving the target parameter wtar, the attacker can search for poisoned points that
maximize the loss ℓwhile keeping a low loss on wtar
∗. However, such target parameters are hard to obtain
since, as we will demonstrate, non-convex models appear to be robust to label flip attacks and there are no
guarantees that wtar
∗is the solution of Equation (7).
(3) Fixed follower (model): Geiping et al. (2020) propose a gradient matching algorithm for crafting targeted
poisoningattacks, whichcanalsobeeasilyadaptedto unlearnable examples (Fowletal.,2021a). Thismethod
fixesthefollowerandsupposesitacquirescleanparameter woncleandataDtr. Wedefineareversedfunction
f′, wheref′can be the reversed cross entropy loss for classification problems (Fowl et al., 2021a). As f′
discourages the network from classifying clean samples, one can mimic its gradient ∇wf′(w;Dtr)by adding
poisoned data such that:
∇wf′(w;Dtr)≈∇wf(w;Dtr∪Dp). (13)
To accomplish this goal, Geiping et al. (2020) define a similarity function Sfor gradient matching, leading
to the attack objective:
L=S(∇wf′(w;Dtr),∇wf(w;Dtr∪Dp)), (14)
5Published in Transactions on Machine Learning Research (12/2022)
where we minimize Lw.r.tDp. This method is not studied yet in the indiscriminate attack literature, but
would serve as an interesting future work.
TGDA attack. In this paper, we solve Equation (7) and avoid the calculation of∂w∗
∂Dpusing the Total
gradient descent ascent (TGDA) algorithm (Evtushenko, 1974; Fiez et al., 2020)1: TGDA takes a total
gradient ascent step for the leader and a gradient descent step for the follower:
xt+1=xt+ηtDxℓ(xt,wt), (15)
wt+1=wt−ηt∇wf(xt,wt) (16)
where Dx:=∇xℓ−∇wxf·∇−1
wwf·∇wℓis the total derivative of ℓwith respect to x, which implicitly
measures the change of wwith respect toDp. As optimizing ℓdoes not involve the attacker parameter θ, we
can rewrite Dx:=−∇wxf·∇−1
wwf·∇wℓ. Here, the product (∇−1
wwf·∇wℓ)can be efficiently computed using
conjugate gradient (CG) equipped with Hessian-vector products computed by autodiff. As CG is essentially
aHessian inverse-free approach (Martens, 2010), each step requires only linear time. Note that TGDA can
also be treated as letting k→∞in UGDA.
We thus apply the total gradient descent ascent algorithm and call this the TGDA attack . Avoiding
computing∂w∗
∂Dpenables us to parameterize Dpand generate points indirectly by treating Las a separate
model. Namely that Dp=Lθ(D′
tr), whereθis the model parameter and D′
tris part of the training set to be
poisoned. Therefore, we can rewrite Equation (15) as:
θt+1=θt+ηtDθℓ(θt,wt). (17)
Thus, we have arrived at a poisoning attack that generates Dpin a batch rather than individually, which
greatly improves the attack efficiency in Algorithm 1. Note that the TGA update does not depend on the
choice ofε. This is a significant advantage over previous methods as the running time does not increase as
the attacker is allowed a larger budget of introduced poisoned points, thus enabling data poisoning attacks
on larger training sets.
Algorithm 1: TGDA Attack
Input:Training setDtr={xi,yi}N
i=1, validation setDv, training steps T, attacker step size α, attacker
number of steps m, defender step size β, defender number of steps n, poisoning fraction ε,L
withθpreandℓ=L(Dv,w∗),Fwithwpreandf=L(Dtr∪Dp,w).
1Initialize poisoned data set D0
p←−{ (x′
1,y′
1),...,(x′
εN,y′
εN)}
2fort= 1,...,Tdo
3fori= 1,...,mdo
4θ←θ+αDθℓ(θ,wt) // TGA on L
5forj= 1,...,ndo
6 w←w−β∇wf(θ,w) // GD on F
7return model Lθand poisoned set Dp=Lθ(D0
p)
Necessity of Stackelberg game. Although Equation (7) is equivalent to the bilevel optimization problem
in Muñoz-González et al. (2017); Huang et al. (2020); Koh et al. (2018), our sequential Stackelberg formula-
tion is more suggestive of the data poisoning problem as it reveals the subtlety in the order of the attacker
and the defender.
4 Implementation
In this section, we (1) discuss the limitations of existing data poisoning attacks and how to address them,
(2) set an efficient attack architecture for the TGDA attack.
1There are other possible solvers for Equation (7), and we have listed them in Appendix C.
6Published in Transactions on Machine Learning Research (12/2022)
Table 1: Summary of existing poisoning attack algorithms, evaluations, and their respective code. While
some papers may include experiments on other datasets, we only cover vision datasets as our main focus is
image classification. The attacks: Random label flip and Adversarial label flip attacks (Biggio et al., 2011),
P-SVM: PoisonSVM attack (Biggio et al., 2011), Min-max attack (Steinhardt et al., 2017), KKT attack
(Koh et al., 2018), i-Min-max: improved Min-max attack (Koh et al., 2018), MT: Model Targeted attack
(Suya et al., 2021), BG: Back-gradient attack (Muñoz-González et al., 2017).
Attack Dataset Model |Dtr| |D test|ε Code Multiclass Batch
Random label flip toy SVM / / 0-40%  ✓ε|Dtr|
Adversarial label flip toy SVM / / 0-40% ×ε|Dtr|
P-SVM MNIST-17 SVM 100 500 0-9% × 1
Min-max MNIST-17/Dogfish SVM 60000 10000 0-30%  ✓ 1
KKT MNIST-17/Dogfish SVM, LR 13007/1800 2163/600 3% × 1
i-Min-max MNIST SVM 60000 10000 3%  ✓ 1
MT MNIST-17/Dogfish SVM, LR 13007/1800 2163/600 /  ✓ 1
BG MNIST SVM, NN 1000 8000 0-6%  ✓ 1
4.1 Current Limitations
We observe two limitations of existing data poisoning attacks.
Limitation 1: Inconsistent assumptions. We first summarize existing indiscriminate data poisoning
attacks in Table 1, where we identify that such attacks work under subtly different assumptions, on, for
example, the attacker’s knowledge, the attack formulation, and the training set size. These inconsistencies
result in somewhat unfair comparisons between methods.
Solution: We set an experimental protocol for generalizing existing attacks and benchmarking data poisoning
attacks for systematic analysis in the future. Here we fix three key variants:
(1) the attacker’s knowledge: as discussed in Section 2, we consider training-only attacks;
(2) the attack formulation: in Section 3, we introduce three possible formulations, namely non-zero-sum,
zero-sum, and zero-sum with target parameters. We will show in the experiment section that the latter two
are ineffective against neural networks.
Figure 1: Comparing the efficacy of
poisoning MNIST-17 with the Poi-
sonSVM and Back-gradient attacks.
The training set size is varied, while
the ratio of the number of poisoned
points to the training set size is fixed
at3%. These attacks become less ef-
fective as training set sizes increase.(3) the dataset size: existing works measure attack efficacy with
respect to the size of the poisoned dataset, where size is measured as
afractionεof the training dataset. However, some works subsample
and thus reduce the size of the training dataset. As we show in
Figure 1, attack efficacy is notinvariant to the size of the training
set: larger training sets appear to be harder to poison. Furthermore,
keepingεfixed, asmallertrainingsetreducesthenumberofpoisoned
data points and thus the time required for methods that generate
points sequentially, potentially concealing a prohibitive runtime for
poisoning the full training set. Thus we consider not only a fixed ε,
but also the complete training set for attacks.
Limitation2: Runningtime . AsdiscussedinSection3, manyex-
isting attacks approach the problem by optimizing individual points
directly, thus having to generate poisoned points one by one. Such
implementation takes enormous running time (see Section 5) and
does not scale to bigger models or datasets.
Solution: We design a new poisoning scheme that allows simultane-
ous and coordinated generation of Dpin batches requiring only one
pass. Thanks to the TGDA attack in Section 3, we can treat Las a
separate model (typically a neural network such as an autoencoder)
7Published in Transactions on Machine Learning Research (12/2022)
that takes part of the Dtras input and generates Dpcorrespondingly. Thus we fix the input and optimize
only the parameters of L.
4.2 A more efficient attack architecture
Once we have fixed the attack assumptions and poisoned data generation process, we are ready to specify
the complete three-stage attack architecture, which enables us to compare poisoning attacks fairly. One can
easily apply this unified framework for more advanced attacks in the future.
(1) Pretrain: The goals of the attacker Lare to: (a) Reduce the test accuracy (i.e., successfully attack).
(b) GenerateDpthat is close toDtr(i.e., thwart potential defenses).
The attacker achieves the first objective during the attack by optimizing ℓ. However, ℓdoes not enforce that
the distribution of the poisoned points will resemble those of the training set. To this end, we pretrain Lto
reconstructDtr, producing a parameter vector θpre. This process is identical to training an autoencoder.
For the defender, we assume that Fis fully trained to convergence. Thus we perform standard training on
Dtrto acquire Fwithwpre. Here we record the performance of FonDtest(denoted as acc 1for image
classification tasks) as the benchmark we are poisoning.
(2) Attack: We generate poisoned points using the TGDA attack. We assume that the attacker can inject
εNpoisoned points, where N=|Dtr|andεis the power of the attacker, measured as a fraction of the
training set size. We summarize the attack procedure in Figure 2.
Initialization: We take the pretrained model Lwith parameter θpreandFwith pretrained parameter wpre
as initialization of the two networks; the complete training set Dtr; a validation set Dvand part of the
training set as initialization of the poisoned points D0
p=Dtr[0 :εN].
Dtr
DvAttacker
θpre
θ∗pretrain
attack
wprew∗
DefenderDtrD′
trattack
pretrain
Figure 2: Our experimental protocol benchmarks data
poisoning attacks. (1) Pretrain: the attacker and the
defender are first trained on Dtrto yield a good au-
toencoder/classifier respectively. (2) During the at-
tack, the attacker generates the optimal θ∗(thusDp)
w.r.tDvand the the optimal w∗; the defender gener-
ates optimal w∗w.r.tD′
tr=Dtr∪Dpand the optimal
θ∗(which mimics testing).TGDA attack : In this paper, we run the TGDA
attack to generate poisoned data. But it can be
changed to any suitable attack for comparison.
Specifically, we follow Algorithm 1 and perform m
steps of TGA updates for the attacker, and nsteps
of GD updates for the defender in one pass. We
discuss the role of mandnin Section 5.
Note that previous works (e.g., Koh et al. 2018;
Muñoz-González et al. 2017) choose n= 1by de-
fault. However, we argue that this is not necessarily
appropriate. When a system is deployed, the model
isgenerallytraineduntilconvergenceratherthanfor
only a single step. Thus we recommend choosing a
much larger n(e.g.,n= 20in our experiments) to
better resemble the testing scenario.
Label Information: We specify that D0
p=
{xi,yi}εN
i=1. Prior works (e.g., Koh et al. 2018;
Muñoz-González et al. 2017) optimize xto produce
xp, andperformalabelflipon ytoproduceyp(more
detailsinAppendixA).Thisapproachneglectslabel
information during optimization.
In contrast, we fixyp=y, and concatenate xand
ytoD0
p={xi;yi}εN
i=1as input to L. Thus we gen-
erate poisoned points by considering the label infor-
mation. We emphasize that we do not optimize or
change the label during the attack, but merely use it to aid the construction of the poisoned xp. Thus, our
attack can be categorized as clean label.
8Published in Transactions on Machine Learning Research (12/2022)
(3) Testing : Finally, we discuss how we measure the effectiveness of an attack. In a realistic setting, the
testing procedure should be identical to the pretrain procedure, such that we can measure the effectiveness
ofDpfairly. The consistency between pretrain and testing is crucial as the model Fis likely to underfit with
fewer training steps.
Given the final θ, we produce the poisoned points Dp=Lθ(D0
p)and train Ffrom scratch onDtr∪Dp. Finally,
we acquire the performance of FonDtest(denoted as acc 2for image classification tasks). By comparing
the discrepancy between pretrain and testing acc 1−acc 2we can evaluate the efficacy of an indiscriminate
data poisoning attack.
5 Experiments
We evaluate our TGDA attack on various models for image classification tasks and show the efficacy of our
method for poisoning neural networks. In comparison to existing indiscriminate data poisoning attacks, we
show that our attack is superior in terms of both effectiveness and efficiency.
Specifically, our results confirm the following: (1) By applying the Stackelberg game formulation and in-
corporating second-order information, we can attack neural networks with improved efficiency and efficacy
using the TGDA attack. (2) The efficient attack architecture further enables the TGDA attack to generate
Dpin batches. (3) The poisoned points are visually similar to clean data, making the attack intuitively
resistant to defenses.
5.1 Experimental Settings
Hardwareandpackage: Experimentswererunonaclusterwith T4andP100GPUs. Theplatformweuse
is PyTorch (Paszke et al., 2019). Specifically, autodiff can be easily implemented using torch.autograd .
As for the total gradient calculation, we follow Zhang et al. (2021) and apply conjugate gradient for calcu-
lating Hessian-vector products.
Dataset: We consider image classification on MNIST (Deng, 2012) (60,000 training and 10,000 test images),
and CIFAR-10 (Krizhevsky, 2009) (50,000 training and 10,000 test images) datasets. We are not aware of
prior work that performs indiscriminate data poisoning on a dataset more complex than MNIST or CIFAR-
10, and, as we will see, even these settings give rise to significant challenges in designing efficient and effective
attacks. Indeed, some prior works consider only a simplified subset of MNIST (e.g., binary classification on
1’s and 7’s, or subsampling the training set to 1,000 points) or CIFAR-10 (e.g., binary classification on dogs
and fish). In contrast, we set a benchmark by using the full datasets for multiclass classification.
Training and validation set: During the attack, we need to split the clean training data into the training
setDtrand validation set Dv. Here we split the data to 70% training and 30% validation, respectively.
Thus, for the MNIST dataset, we have |Dtr|= 42000 and|Dv|= 18000. For the CIFAR-10 dataset, we have
|Dtr|= 35000 and|Dv|= 15000.
Attacker models and Defender models: (1) For the attacker model, for MNIST dataset: we use a three-
layer neural network, with three fully connected layers and leaky ReLU activations; for CIFAR-10 dataset,
we use an autoencoder with three convoluational layers and three conv transpose layers. The attacker takes
the concatenation of the image and the label as the input, and generates the poisoned points. (2) For the
defender, we examine three target models for MNIST: Logistic Regression, a neural network (NN) with three
layers and a convolutional neural network (CNN) with two convolutional layers, maxpooling and one fully
connected layer; and only the CNN model and ResNet-18 (He et al., 2016) for CIFAR-10 (as CIFAR-10
contains RBG images).
Hyperparameters: (1) Pretrain: we use a batch size of 1,000 for MNIST and 256 for CIFAR-10, and
optimize the network using our own implementation of gradient descent with torch.autograd . We choose
the learning rate as 0.1 and train for 100 epochs. (2) Attack: for the attacker, we choose α= 0.01,m= 1by
default; for the defender, we choose β= 0.1,n= 20by default. We set the batch size to be 1,000 for MNIST;
256 for CIFAR10 and train for 200 epochs, where the attacker is updated using total gradient ascent and
the defender is updated using gradient descent. We follow Zhang et al. (2021) and implement TGA using
9Published in Transactions on Machine Learning Research (12/2022)
Table 2: The attack accuracy/accuracy drop (%) and attack running time (hours) on the MNIST dataset.
We only record the attack running time since pretrain and testing time are fixed across different methods. As
the label flip attack does not involve optimization, its running time is always 0. We take three different runs
for TGDA to get the mean and the standard derivation. Our attack outperforms the Min-max, i-Min-max
and Back-gradient attacks in terms of both effectiveness and efficiency across neural networks.
ModelClean Label Flip Min-max i-Min-max BG TGDA(ours)
Acc Acc/Drop Time Acc/Drop Time Acc/Drop Time Acc/Drop Time Acc/Drop Time
LR 92.35 90.83 / 1.52 0 hrs 89.80/2.55 0.7 hrs 89.56/ 2.79 19 hrs 89.82 / 2.53 27 hrs 89.56 / 2.79±0.071.1 hrs
NN 98.04 97.99 / 0.05 0 hrs 98.07/-0.03 13 hrs 97.82/0.22 73 hrs 97.67 / 0.37 239 hrs 96.54 / 1.50±0.02 15 hrs
CNN 99.13 99.12 / 0.01 0 hrs 99.55/-0.42 63hrs 99.05/0.06 246 hrs 99.02 / 0.09 2153 hrs 98.02 / 1.11±0.01 75 hrs
conjugate gradient. We choose the poisoning fraction ε= 3%by default. Note that choosing a bigger εwill
not increase our running time, but we choose a small εto resemble the realistic setting in which the attacker
is limited in their access to the training data. (3) Testing: we choose the exact same setting as pretrain to
keep the defender’s training scheme consistent.
Baselines: There is a spectrum of data poisoning attacks in the literature. However, due to their attack
formulations, only a few attacks can be directly compared with our method. See Table 1 in Appendix A for
a complete summary. For instance, the Poison SVM (Biggio et al., 2011) and KKT (Koh et al., 2018) attacks
can only be applied to convex models for binary classification; the Min-max (Steinhardt et al., 2017) and
the Model targeted (Suya et al., 2021) attacks can be only applied to convex models. However, it is possible
to modify Min-max (Steinhardt et al., 2017) and i-Min-max (Koh et al., 2018) attacks to attack neural
networks. Moreover, we compare with two baseline methods that can originally attack neural networks: the
Back-gradient attack (Muñoz-González et al., 2017) and the Label flip attack (Biggio et al., 2011). It is also
possible to apply certain targeted attack methods (e.g., MetaPoison, Huang et al. 2020) in the context of
indiscriminate attacks. Thus we compare with MetaPoison on CIFAR-10 under our unified architecture. We
follow Huang et al. (2020) and choose K= 2unrolled inner steps, 60 outer steps, and an ensemble of 24
inner models.
5.2 Comparison with Benchmarks
MNIST. We compare our attack with the Min-max, i-Min-max, Back-gradient and the Label flip attacks
withε= 3%on MNIST in Table 2. Since the Min-max, i-Min-max, and Back-gradient attack relies on
generating poisoned points sequentially, we cannot adapt it into our unified architecture and run their code
directly for comparison. For the label flip attack, we flip the label according to the rule y←10−y, as there
are 10 classes in MNIST.
We observe that label flip attack, though very efficient, is not effective against neural networks. Min-max
attack, due to its zero-sum formulation, does not work on neural networks. i-Min-max attack is effective
against LR, but performs poorly on neural networks where the assumption of convexity fails. Although
Muñoz-González et al. (2017) show empirically that the Back-gradient attack is effective when attacking
subsets of MNIST (1,000 training samples, 5,000 testing samples), we show that the attack is much less
effective on the full dataset. We also observe that the complexity of the target model affects the attack
effectiveness significantly. Specifically, we find that neural networks are generally more robust against in-
discriminate data poisoning attacks, among which, the CNN architecture is even more robust. Overall,
our method outperforms the baseline methods across the three target models. Moreover, with our unified
architecture, we significantly reduce the running time of poisoning attacks.
CIFAR-10. We compare our attack with the Label flip attack and the MetaPoison attack with ε= 3%
on CIFAR-10 in Table 3. We omit comparison with the Back-gradient attack as it is too computationally
expensive to run on CIFAR-10. We observe that running the TGDA attack following Algorithm 1 directly
is computationally expensive on large models (e.g., ResNet, He et al. 2016). However, it is possible to run
TGDA on such models by slightly changing Algorithm 1: we split the dataset into 8 partitions and run
TGDA separately on different GPUs. This simple trick enables us to poison deeper models and we find it
works well in practice. We observe that the TGDA attack is very effective at poisoning the CNN and the
ResNet-18 architectures, Also, MetaPoison is a more efficient attack (meta-learning with two unrolled steps is
10Published in Transactions on Machine Learning Research (12/2022)
Table 3: The attack accuracy/accuracy drop (%) and attack running time (hours) on CIFAR-10. Note that
TGDA experiments are performed on 8 GPUs for parallel training. We take three different runs for TGDA
and MetaPoison to get the mean and the standard derivation.
ModelClean Label Flip MetaPoison TGDA(ours)
Acc Acc/Drop Time Acc/Drop Time Acc/Drop Time
CNN 69.44 68.99/0.45 0 hrs 68.14/1.13 ±0.12 35 hrs 65.15/4.29 ±0.09 42 hrs
ResNet-18 94.95 94.79/0.16 0 hrs 92.90/2.05 ±0.07108 hrs 89.41/5.54 ±0.03162 hrs
Table 4: Comparing the TGDA attack with different orders: attacker as the leader and defender as the leader
in terms of test accuracy/accuracy drop(%). Attacks are more effective when the attacker is the leader.
Target Model Clean Attacker as leader Defender as leader
LR 92.35 89.56 / 2.79 89.79 / 2.56
NN 98.04 96.54 / 1.50 96.98 / 1.06
CNN 99.13 98.02 / 1.11 98.66 / 0.47
much quicker than calculating total gradient), but since its original objective is to perform targeted attacks,
its application to indiscriminate attacks is not effective. Moreover, the difference between the efficacy of the
TGDA attack on MNIST and CIFAR-10 suggests that indiscriminate attacks may be dataset dependent,
with MNIST being harder to poison than CIFAR-10.
5.3 Ablation Studies
To better understand our TGDA attack, we perform ablation studies on the order in the Stackelberg game,
the attack formulation, roles in our unified attack framework, and the choice of hyperparameters. For com-
putational considerations, we run all ablation studies on the MNIST dataset unless specified. Furthermore,
we include empirically comparison with unlearnable examples in Appendix B.
Figure 3: We visualize the poi-
soned data generated by the
TGDA attack with/without pre-
training the leader Lon the
MNIST dataset.Who acts first. In Section 3, we assume that the attacker is the leader
and the defender is the follower, i.e., that the attacker acts first. Here,
we examine the outcome of reversing the order, where the defender acts
first. Table 4 shows the comparison. We observe that across all models,
reversing the order would result in a less effective attack. This result
shows that even without any defense strategy, the target model would be
more robust if the defender acts one step ahead of the attacker.
Attack formulation. In Section 3, we discuss a relaxed attack formula-
tion, where ℓ=fand the game is zero-sum. We perform experiments on
this setting and show results in Table 5. We observe that the non-zero-
sum formulation is significantly more effective, and in some cases, the
zero-sum setting actually increases the accuracy after poisoning. We also
find that using target parameters would not work for neural networks as they are robust to label flip attacks
even whenεis large. We ran a label flip attack with ε= 100%and observed only 0.1% and 0.07% accuracy
drop on NN and CNN architectures, respectively. This provides further evidence that neural networks are
robust to massive label noise, as previously observed by Rolnick et al. (2017).
Role of pretraining. In Section 4, we propose two desired properties of L, among which Lshould generate
Dpthat is visually similar to Dtr. Thus requires the pretraining of Lfor reconstructing images. We perform
experiments without pretraining Lto examine its role in effecting the attacker. Figure 3 confirms that
without pretraining, the attacker will generate images that are visually different from the Dtrdistribution,
thus fragile to possible defenses. Moreover, Table 6 indicates that without pretraining L, the attack will also
be ineffective. Thus we have demonstrated the necessity of the visual similarity between DpandDtr.
Differentε.We have set ε= 3%in previous experiments. However, unlike prior methods which generate
points one at a time, the running time of our attack does not scale with ε, and thus we can consider
significantly larger εand compare with other feasible methods. Figure 4 shows that attack efficacy increases
11Published in Transactions on Machine Learning Research (12/2022)
Table 5: Comparing the TGDA attack with different formulations: non-zero-sum and zero-sum in terms of
test accuracy/accuracy drop (%). Non-zero-sum is more effective at generating poisoning attacks.
Target Model Clean Non Zero-sum Zero-sum
LR 92.35 89.56 / 2.79 92.33 / 0.02
NN 98.04 96.54 / 1.50 98.07 / -0.03
CNN 99.13 98.02 / 1.11 99.55 / -0.42
Table 6: Comparing the TGDA attack with/without pretraining the attacker Lin terms of test accu-
racy/accuracy drop (%). Pretraining strongly improves attack efficacy.
Target Model Clean With Pretrain Without Pretrain
LR 92.35 89.56 / 2.79 92.09 / 0.26
NN 98.04 96.54 / 1.50 97.47 / 0.57
CNN 99.13 98.02 / 1.11 98.72 / 0.41
withε, but the accuracy drop is significantly less than εwhenεis very large. Moreover, TGDA outperforms
baseline methods across any choice of ε.
Number of steps mandn.We discuss the choice of mandn, the number of steps of LandF, respectively.
We perform three choices of mandnin Table 7. We observe that 20 steps of TGA and 1 step of GD results
in the most effective attack. This indicates that when m > n, the outer maximization problem is better
solved with more TGA updates. However, setting 4 ( m= 20,n= 1) takes 10 times more computation than
setting 3 (m= 1,n= 20), due to the fact that the TGA update is expensive. When m=n= 1, the attack
is less effective as the defender might not be fully trained to respond to the attack. When n= 0, the attack
is hardly effective at all as the target model is not retrained. We conclude that different choices of mandn
would result in a trade-off between effectiveness and efficiency.
Cold-Start. The methods we compare in this work all belong to the partial warm-start category for
bilevel optimization (Vicol et al., 2022). It is also possible to formulate the cold-start Stackelberg game for
data poisoning. Specifically, we follow Vicol et al. (2022) such that in Algorithm 1, the follower update is
modified to w←wpre−β∇wf(θ,w). We report the results in Table 8 on MNIST dataset. We observe that
in indiscriminate data poisoning, partial warm-start is a better approach than cold-start overall, and our
outerproblem(autoencoderfor generating poisonedpoints)doesnotappeartobehighlyover-parameterized.
5.4 Visualization of attacks
Finally, we visualize some poisoned points Dpgenerated by the TGDA attack in Figure 5.
The poisoning samples against NN and CNN are visually very similar with Dtr, as our attack is a clean label
attack (see Section 4). Moreover, we evaluate the magnitude of perturbation by calculating the maximum of
pixel-level difference. Both visual similarity and magnitude of perturbation provide heuristic evidence that
the TGDA attack may be robust against data sanitization algorithms. Note that Dpagainst LR is visually
distinguishable, and the reason behind this discrepancy between the convex model and the neural networks
Figure 4: Accuracy drop induced by our TGDA poisoning attack and baseline methods versus ε(left three
figures: MNIST; right two figures: CIFAR-10). Attack efficacy increases modestly with ε. Note that when
ε= 1, only 50% of the training set is filled with poisoned data.
12Published in Transactions on Machine Learning Research (12/2022)
Table 7: Comparing different numbers of steps of the attacker ( m) and defender ( n) in terms of test accu-
racy/accuracy drop (%). Many attacker steps and a single defender step produces the most effective attacks.
Model Clean m= 1, n= 0m= 1, n= 1m= 1, n= 20 m= 20, n= 1m=n= 20
LR 92.35 92.30 / 0.05 89.97 / 2.38 89.56 / 2.79 89.29 / 3.06 89.77 / 2.57
NN 98.04 98.02 / 0.02 97.03 / 1.01 96.54 / 1.50 96.33 / 1.71 96.85 / 1.19
Table 8: Comparing the TGDA attack with partial warm-start (our original setting) and cold-start in terms
of test accuracy/accuracy drop (%). Cold-start training is less effective overall.
Model Clean Partial Warm-start Cold-start
LR 92.35 89.56 / 2.79 89.84 / 2.41
NN 98.04 96.54 / 1.50 96.77 / 1.27
CNN 99.13 98.02 / 1.11 98.33 / 0.80
may be that the attacker Lis not expressive enough to generate extremely strong poisoning attacks against
neural networks.
5.5 Transferability of the TGDA attack
Even for training-only attacks, the assumption on the attacker’s knowledge can be too strong. Thus we study
the scenario when the attacker has limited knowledge regarding the defender’s model Fand training process,
where the attacker has to use a surrogate model to simulate the defender. We report the transferability
of the TGDA attack on different surrogate models in Table 9. We observe that poisoned points generated
against LR and NN have a much lower impact against other models, while applying CNNs as the surrogate
model is effective on all models.
5.6 Against Defenses:
To further evaluate the robustness of the TGDA attack against data sanitization algorithms:
(a) We perform the loss defense (Koh et al., 2018) by removing 3% of training points with the largest loss. We
compare with pGAN (Muñoz-González et al., 2019), which includes a constraint on the similarity between
the clean and poisoned samples, and is thus inherently robust against defenses. In Table 10, we observe that
although we do not add an explicit constraint on detectability in our loss function, our method still reaches
comparable robustness against such defenses with pGAN.
Figure 5: We visualize the poisoned data generated by the TGDA attack and report the magnitude of
perturbation (left: CIFAR-10; right: MNIST).
13Published in Transactions on Machine Learning Research (12/2022)
Table 9: Transferability expeirments on MNIST.
Surrogate LR NN CNN
Target LR NN CNN LR NN CNN LR NN CNN
Accuracy Drop(%) 2.79 0.12 0.27 0.13 1.50 0.62 3.22 1.47 1.11
Table 10: Comparison with pGAN on MNIST with loss defense.
Method TGDA (wo/w defense) pGAN(wo/w defense)
Target Model LR NN CNN LR NN CNN
Accuracy Drop (%) 2.79/2.56 1.50/1.49 1.11/1.104 2.52/2.49 1.09/1.07 0.74/0.73
Table 11: TGDA attack on MNIST with Influence/Sever/MaxUp defense.
ModelInfluence Sever MaxUp
wo defense w defense wo defense w defense wo defense w defense
LR 2.79 2.45 2.79 2.13 2.79 2.77
NN 1.50 1.48 1.50 1.32 1.50 1.50
CNN 1.11 1.10 1.11 0.98 1.11 1.11
(b) Other defenses remove suspicious points according to their effect on the learned parameters, e.g., through
influence functions (influence defense in Koh & Liang 2017) or gradients (Sever in Diakonikolas et al. 2016).
Specifically, influence defense removes 3% of training points with the highest influence, defined using their
gradients and Hessian-vector products (Koh & Liang, 2017); Sever removes 3% of training points with the
highest outlier scores, defined using the top singular value in the matrix of gradients. Here we examine
the robustness of TGDA against these two strong defenses. We observe in Table 11 that TGDA is robust
against Influence defense, but its effectiveness is significantly reduced by Sever. Thus, we conclude Sever is
potentially a good defense against the TGDA attack, and it might require special design (e.g., an explicit
constraint on the singular value) to break Sever sanitation.
(c) We examine the robustness of our TGDA attack against strong data augmentations, e.g., the MaxUp
defense2of Gong et al. (2020). In a nutshell, MaxUp generates a set of augmented data with random
perturbations and then aims at minimizing the worst case loss over the augmented data. This training
technique addresses overfitting and serves as a possible defense against adversarial examples. However, it
is not clear if MaxUp is a good defense against indiscriminate data poisoning attacks. Thus, we implement
MaxUp under our testing protocol, where we add random perturbations to the training and the poisoned
data, i.e.,{Dtr∪Dp}, and then minimize the worst case loss over the augmented set. We report the results
in Table 11, where we observe that even though MaxUp is a good defense against adversarial examples, it is
not readily an effective defense against indiscriminate data poisoning attacks. Part of the reason we believe
is that in our formulation the attacker anticipates the retraining done by the defender, in contrast to the
adversarial example setting.
6 Conclusions
While indiscriminate data poisoning attacks have been well studied under various formulations and settings
on convex models, non-convex models remain significantly underexplored. Our work serves as a first explo-
ration into poisoning neural networks under a unified architecture. While prior state-of-the-art attacks failed
at this task due to either the attack formulation or a computationally prohibitive algorithm, we propose a
novel Total Gradient Descent Ascent (TGDA) attack by exploiting second-order information, which enables
generating thousands of poisoned points in only one pass. We perform experiments on (convolutional) neu-
ral networks and empirically demonstrate the feasibility of poisoning them. Moreover, the TGDA attack
2We follow the implementation in https://github.com/Yunodo/maxup
14Published in Transactions on Machine Learning Research (12/2022)
produces poisoned samples that are visually indistinguishable from unpoisoned data (i.e., it is a clean-label
attack), which is desired in the presence of a curator who may attempt to sanitize the dataset.
Our work has some limitations. While our algorithm is significantly faster than prior methods, it remains
computationally expensive to poison deeper models such as ResNet, or larger datasets such as ImageNet.
Similarly, while our attacks are significantly more effective than prior methods, we would ideally like a poison
fraction ofεto induce an accuracy drop far larger than ε, as appears to be possible for simpler settings (Lai
et al., 2016; Diakonikolas et al., 2016; 2019). We believe our work will set an effective benchmark for future
work on poisoning neural networks.
Acknowledgement
We thank the action editor and reviewers for the constructive comments and additional references, which
have greatly improved our presentation and discussion.
References
Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni Vigna. Bulls-
eye polytope: A scalable clean-label poisoning attack with improved transferability. arXiv preprint
arXiv:2005.00191, 2020.
Samyadeep Basu, Philip Pope, and Soheil Feizi. Influence functions in deep learning are fragile. In Interna-
tional Conference on Learning Representations (ICLR) , 2021.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Support vector machines under adversarial label noise.
InProceedings of the Asian Conference on Machine Learning (ACML) , pp. 97–112, 2011.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In
Proceedings of the 29th International Conference on Machine Learning (ICML) , pp. 1467–1474, 2012.
Mengjie Chen, Chao Gao, and Zhao Ren. Robust covariance and scatter matrix estimation under Huber’s
contamination model. The Annals of Statistics , 46(5):1932–1960, 2018. URL https://doi.org/10.
1214/17-AOS1607 .
Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Sebastiano Vascon, Werner Zellinger, Bern-
hard A Moser, Alina Oprea, Battista Biggio, Marcello Pelillo, and Fabio Roli. Wild patterns reloaded:
A survey of machine learning security against training data poisoning. arXiv preprint arXiv:2205.01992 ,
2022.
Nilesh Dalvi, Pedro Domingos, Sumit Sanghai, and Deepak Verma. Adversarial classification. In Proceedings
of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining , pp. 99–108,
2004. URL https://dl.acm.org/doi/abs/10.1145/1014052.1014066 .
Li Deng. The MNIST database of handwritten digit images for machine learning research [best of the web].
IEEE Signal Processing Magazine , 29(6):141–142, 2012.
Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust
estimators in high dimensions without the computational intractability. In Proceedings of the 57th Annual
IEEE Symposium on Foundations of Computer Science , pp. 655–664, 2016.
Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart.
Sever: A robust meta-algorithm for stochastic optimization. In Proceedings of the 36th International
Conference on Machine Learning , pp. 1596–1606, 2019.
Ürün Dogan, Tobias Glasmachers, and Christian Igel. A unified view on multi-class support vector classifi-
cation.Journal of Machine Learning Research , 17(45):1–32, 2016.
Yu. G. Evtushenko. Iterative methods for solving minimax problems. USSR Computational Mathematics
and Mathematical Physics , 14(5):52–63, 1974.
15Published in Transactions on Machine Learning Research (12/2022)
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Implicit learning dynamics in Stackelberg games:
Equilibria characterization, convergence analysis, and empirical study. In Proceedings of the International
Conference on Machine Learning (ICML) , 2020.
Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geiping, Arpit Bansal, Wojtek Czaja, and Tom
Goldstein. Preventing unauthorized use of proprietary data: Poisoning for secure dataset release. arXiv
preprint arXiv:2103.02683, 2021a.
Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojtek Czaja, and Tom Goldstein. Adver-
sarial examples make strong poisons. arXiv preprint arXiv:2106.10807, 2021b.
Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, and Dacheng Tao. Robust unlearnable examples: Protecting
data privacy against adversarial learning. In International Conference on Learning Representations , 2021.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace
He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027, 2020.
Jonas Geiping, Liam Fowl, W Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and Tom
Goldstein. Witches’ brew: Industrial scale data poisoning via gradient matching. arXiv preprint
arXiv:2009.02276 , 2020.
Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander
Madry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor
attacks, and defenses. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022. URL
https://doi.org/10.1109/TPAMI.2022.3162397 .
Chengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu. Maxup: A simple way to improve generalization
of neural network training. arXiv preprint arXiv:2002.09024 , 2020.
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine
learning model supply chain. arXiv:1708.06733, 2017.
Junfeng Guo and Cong Liu. Practical poisoning attacks on neural networks. In European Conference on
Computer Vision , 2020.
Frank R Hampel. The influence curve and its role in robust estimation. Journal of the American Statistical
Association , 69(346):383–393, 1974.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the 2016 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ,
CVPR ’16, pp. 770–778, Washington, DC, USA, 2016. IEEE Computer Society.
Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, and Yisen Wang. Unlearnable examples:
Making personal data unexploitable. arXiv preprint arXiv:2101.04898, 2021.
W Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein. Metapoison: Practical
general-purpose clean-label data poisoning. In Advances in Neural Information Processing Systems , vol-
ume 33, pp. 12080–12091, 2020.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Proceedings
of the 34th International Conference on Machine Learning (ICML) , pp. 1885–1894, 2017.
Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization
defenses. arXiv:1811.00741, 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. tech. report, 2009. URL https:
//www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf .
16Published in Transactions on Machine Learning Research (12/2022)
RamShankarSivaKumar, MagnusNyström, JohnLambert, AndrewMarshall, MarioGoertzel, AndiComis-
soneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry perspectives. In IEEE
Security and Privacy Workshops (SPW) , pp. 69–75, 2020.
Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In
Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science , pp. 665–674,
2016.
Wei Liu and Sanjay Chawla. A game theoretical model for adversarial learning. In IEEE International
Conference on Data Mining Workshops , pp. 25–30, 2009. URL https://ieeexplore.ieee.org/
abstract/document/5360532 .
Wei Liu and Sanjay Chawla. Mining adversarial patterns via regularized loss minimization. Ma-
chine learning , 81(1):69–83, 2010. URL https://link.springer.com/article/10.1007/
s10994-010-5199-2 .
Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learning: A survey. arXiv preprint
arXiv:2003.02133, 2020.
James Martens. Deep learning via hessian-free optimization. In ICML, pp. 735–742, 2010.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. In
International Conference on Learning Representation (ICLR) , 2017.
Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C.
Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In
Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (AISec) , pp. 27–38, 2017.
Luis Muñoz-González, Bjarne Pfitzner, Matteo Russo, Javier Carnerero-Cano, and Emil C Lupu. Poisoning
attacks with generative adversarial nets. arXiv preprint arXiv:1906.07773, 2019.
Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini,
Charles Sutton, J Doug Tygar, and Kai Xia. Exploiting machine learning to subvert your spam filter.
LEET, 8:1–9, 2008.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems 32 , NeurIPS ’19, pp. 8026–8037. Curran Associates, Inc., 2019.
Evani Radiya-Dixit, Sanghyun Hong, Nicholas Carlini, and Florian Tramer. Data poisoning won’t save you
from facial recognition. In Proceedings of the 10th International Conference on Learning Representations ,
ICLR ’22, 2022.
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive label noise.
arXiv preprint arXiv:1705.10694, 2017.
Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In
Proceedings of the AAAI Conference on Artificial Intelligence , 2020.
Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom Goldstein, and David W Jacobs.
Autoregressive perturbations for data poisoning. arXiv preprint arXiv:2206.03693, 2022.
Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic
is data poisoning? A unified benchmark for backdoor and data poisoning attacks. In Proceedings of the
38th International Conference on Machine Learning (ICML) , 2021.
17Published in Transactions on Machine Learning Research (12/2022)
Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and
Tom Goldstein. Poison frogs! Targeted clean-label poisoning attacks on neural networks. In Advances in
Neural Information Processing Systems (NeurIPS) , pp. 6103–6113, 2018.
Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing board: A
critical evaluation of poisoning attacks on federated learning. arXiv:2108.10241, 2021.
Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A Erdogdu, and
Ross Anderson. Manipulating SGD with data ordering attacks. arXiv:2104.09667, 2021.
JacobSteinhardt, PangWeiKoh, andPercyLiang. Certifieddefensesfordatapoisoningattacks. In Advances
in Neural Information Processing Systems (NeurIPS) , pp. 3520–3532, 2017.
Fnu Suya, Saeed Mahloujifar, Anshuman Suri, David Evans, and Yuan Tian. Model-targeted poisoning at-
tacks with provable convergence. In Proceedings of the 38th International Conference on Machine Learning
(ICML), pp. 10000–10010, 2021.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In Advances in
Neural Information Processing Systems (NeurIPS) , 2018.
Paul Vicol, Jonathan P Lorraine, Fabian Pedregosa, David Duvenaud, and Roger B Grosse. On implicit
bias in overparameterized bilevel optimization. In International Conference on Machine Learning , pp.
22234–22259, 2022.
Heinrich von Stackelberg. Market structure and equilibrium . Springer, 1934.
Jane Wakefield. Microsoft chatbot is taught to swear on twitter. BBC News, 2016.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A follow-the-
ridge approach. In The 8th International Conference on Learning Representations (ICLR) , 2020.
Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Indiscriminate poisoning attacks are short-
cuts. arXiv preprint arXiv:2111.00898, 2021.
Guojun Zhang, Kaiwen Wu, Pascal Poupart, and Yaoliang Yu. Newton-type methods for minimax optimiza-
tion. InICML workshop on Beyond First-Order Methods in ML Systems , 2021.
Chen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein. Transferable
clean-label poisoning attacks on deep neural nets. In International Conference on Machine Learning , 2019.
18Published in Transactions on Machine Learning Research (12/2022)
A Indiscriminte data poisoning attacks
We first show that perfect knowledge attacks and training-only attacks can be executed by solving a non-
zero-sum bi-level optimization problem.
A.1 Non-zero-sum setting
For perfect knowledge and training-only attacks, recall that we aim at the following bi-level optimization
problem:
max
DpL(Dv,w∗),s.t.w∗∈arg min
w∈WL(Dtr∪Dp,w), (18)
where we constrain |Dp|=ε|Dtr|to limit the amount of poisoned data the attacker can inject. The attacker
can solve (18) in the training only attack setting. With a stronger assumption where Dtestis available, we
substituteDvwithDtestand arrive at the perfect knowledge attack setting.
Existing attacks generate poisoned points one by one by considering the problem:
max
xpL(Dv,w∗),s.t.w∗∈arg min
w∈WL(Dtr∪{xp,yp},w). (19)
While the inner minimization problem can be solved via gradient descent, the outer maximization problem
is non-trivial as the dependency of L(Dv,w∗)onxpis indirectly encoded through the parameter wof the
poisoned model. As a result, we rewrite the desired derivative using the chain rule:
∂D(Dv,w∗)
∂xp=∂D(Dv,w∗)
∂w∗∂w∗
∂xp, (20)
where the difficulty lies in computing∂w∗
∂xp, i.e., measuring how much the model parameter wchanges with
respect to the poisoning point xp. Various approaches compute∂w∗
∂xpby solving this problem exactly, using
either influence functions (Koh & Liang, 2017) (Influence attack) or KKT conditions (Biggio et al., 2011)
(PoisonSVM attack3). Another solution is to approximate the problem using gradient descent (Muñoz-
González et al., 2017). We discuss each of these approaches below.
Influence attack. The influence function (Hampel, 1974) tells us how the model parameters change as we
modify a training point by an infinitesimal amount. Borrowing the presentation from Koh & Liang (2017),
we compute the desired derivative as:
∂w∗
∂xp=−H−1
w∗∂2L({xp,yp},w∗)
∂w∗∂xp, (21)
whereHw∗is the Hessian of the training loss at w∗:
Hw∗:=λI+1
|Dtr∪Dp|/summationdisplay
(x,y)∈Dtr∪Dp∂2L((x,y),w∗)
∂(w∗)2(22)
Influence functions are well-defined for convex models like SVMs and are generally accurate for our settings.
However, they have been showed to be inaccurate for neural networks (Basu et al., 2021).
PoisonSVM attack. Biggio et al. (2012) replaces the inner problem with its stationary (KKT) conditions.
According to the KKT condition, we write the implicit function:
∂L(Dtr∪{xp,yp},w∗)
∂w∗= 0, (23)
3While this might naturally suggest the name “KKT attack,” this name is reserved for a different attack covered in Sec-
tion A.3.
19Published in Transactions on Machine Learning Research (12/2022)
which yields the linear system:
∂2L(Dtr∪{xp,yp},w∗)
∂w∗∂xp+∂2L(Dtr∪{xp,yp},w∗)
∂(w∗)2∂w∗
∂xp= 0, (24)
and thus we can solve the desired derivative as:
∂w∗
∂xp=−/parenleftbigg∂2L(Dtr∪{xp,yp},w∗)
∂(w∗)2/parenrightbigg−1∂2L(Dtr∪{xp,yp},w∗)
∂w∗∂xp. (25)
Note that despite their differences in approaching the derivative, both the influence attack and PoisonSVM
attack involve the inverse Hessian.
Back-gradient attack. Muñoz-González et al. (2017) avoid solving the outer maximization problem ex-
actly by replacing it with a set of iterations performed by an optimization method such as gradient descent.
This incomplete optimization of the inner problem allows the algorithm to run faster than the two above
methods, and poisoning neural networks.
A.2 Zero-sum Setting
Steinhardt et al. (2017) reduce Equation (18) to a zero-sum game by replacing L(Dv,w∗)withL(Dtr∪
Dp,w∗), and the original problem can be written as:
max
DpL(Dtr∪Dp,w∗),s.t.w∗∈arg min
w∈WL(Dtr∪Dp,w). (26)
which is identical to the saddle-point or zero-sum problem:
max
Dpmin
wL(Dtr∪Dp,w) (27)
For an SVM model, given that the loss function is convex, we can solve (27) by swapping the min and max
and expand the problem to:
min
w/summationdisplay
(x,y)∈DtrL({x,y},w) + max
{xp,yp}L({xp,yp},w), (28)
However, we emphasize that this relaxed gradient-based attack is problematic and could be ineffective since
the loss on the clean data Dtrcould still be low. In other words, the inner maximization does not address
the true objective where we want to change the model parameter to cause wrong predictions on clean data.
This can be addressed by keeping the loss on the poisoned data small, but this contradicts the problem
formulation. One solution to this is to use target parameters in Section A.3.
A.3 Zero-sum Setting with Target parameters
Gradient-based attacks solve a difficult optimization problem in which the poisoned data Dpaffects the
objective through the model parameter w∗. As a result, evaluating the gradient usually involves computing
a Hessian, a computationally expensive operation which can not be done in many realistic settings. Koh
et al. (2018) propose that if we have a target parameter wtar
∗which maximizes the loss on the test data
L(Dtest,w∗), then the problem simplifies to:
findDp,s.t.wtar
∗= arg min
w∈WL(Dtr∪Dp,w), (29)
KKT attack. Since the target parameter wtar
∗is pre-specified, the condition can be rewritten as:
wtar
∗= arg min
w∈WL(Dtr∪Dp,w) (30)
= arg min
w∈W/summationdisplay
{x,y}∈D trL({x,y},w) +/summationdisplay
{xp,yp}∈D pL({xp,yp},w), (31)
20Published in Transactions on Machine Learning Research (12/2022)
Again we can use the KKT optimality condition to solve the argmin problem for convex losses:
/summationdisplay
{x,y}∈D trL({x,y},wtar
∗) +/summationdisplay
{xp,yp}∈D pL({xp,yp},wtar
∗) = 0 (32)
Thus we can rewrite the problem as:
findDp,s.t./summationdisplay
{x,y}∈D trL({x,y},wtar
∗) +/summationdisplay
{xp,yp}∈D pL({xp,yp},wtar
∗) = 0. (33)
If this problem has a solution, we can find it by solving the equivalent norm-minimization problem:
min
Dp/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
{x,y}∈D trL({x,y},wtar
∗) +/summationdisplay
{xp,yp}∈D pL({xp,yp},wtar
∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2, (34)
where the problem can only be minimized if the KKT condition is satisfied. This attack is called the KKT
attack.
Of course, the success of this attack relies on the target parameter wtar
∗. Koh et al. (2018) propose to use
the label flip attack for such purpose where we use the trained parameter as the target. This attack achieves
comparable results to other attacks while being much faster since it can be solved efficiently using grid search
for binary classification. Note that for multi-class classification, this algorithm quickly become infeasible.
Improved min-max. Koh et al. (2018) applies the target parameters to address the issue for the relaxed
gradient-based attack, where we add the following constraint during training:
L({x,y},wtar
∗)≤τ, (35)
whereτis a fixed threshold. Thus the attacker can search for poisoned points that maximize loss under the
current parameter wwhile keeping low loss on the target parameter wtar
∗.
Model Targeted Poisoning. Suya et al. (2021) propose another algorithm for generating poisoned points
using target parameters. This attack considers a different attack strategy from the others, where the attacker
adopts an online learning procedure. In this case, the attacker does not have a poison fraction εto generate
a specific amount of poisoned data. Instead, the attacker aims at reaching a stopping criteria (can be either
a desired accuracy drop or desired distance to the target parameter). However, such attacking procedure
may cause the poison fraction εto be large and it is hard to measure the success of the attack. Thus, we
use the same setting as others for fair comparison.
A.4 Training-data-only attack
In the training-data-only attack setting, since the attacker does not have access to the training procedure,
the bi-level optimization methods are not applicable. The remaining strategies focus either on modifying
the labels only (i.e., label flip attacks).
Random label flip attack. Random label flipping is a very simple attack, which constructs a set of
poisoned points by randomly selecting training points and flipping their labels:
Dp={(xi,¯yi) : (xi,yi)∈Dtr}s.t.|Dp|=ε|Dtr|, (36)
where for each class j= 1,...,c, we set
¯yi=jwith probability pj. (37)
Note that the weights {pj}may depend on the true label yi. For instance, for binary classification (i.e.,
c= 2), we may set pc+1−yi= 1in which case ¯yisimply flips the true label yi.
21Published in Transactions on Machine Learning Research (12/2022)
Adversarial label flip attack. Biggio et al. (2011) consider an adversarial variant of the label flip attack,
where the choice of the poisoned points is not random. This attack requires access to the model and training
procedure, and thus is not a training-data-only attack. Biggio et al. (2011) design an attack focused on
SVMs. They choose to poison non-support vectors, as these are likely to become support vectors when an
SVM is trained on the dataset including these points with flipped labels.
Label flip for multi-class classification For binary classification, label flip is trivial. Koh et al. (2018)
provides a solution for multi-class classification problem. For marginal based models (for example, SVM),
we can write the multi-class hinge loss, where we have (Dogan et al., 2016):
L(w,(xi,yi)) = max{0,1 + max
j̸=yiwjxi−wyixi}, (38)
where the choice of jis obvious: we choose the index with the highest function score except the target class
yi. Naturally, we can use this index jas the optimal label flip. As for non-convex models, the choice of
optimal label flip is not clear. In this case, one can use a heuristic by choosing the class with the biggest
training loss.
B Unlearnable Examples
B.1 Stackelberg Game on Unlearnable Examples
We recall the general Stackelberg game in Section 3, where the follower Fchooses wto best respond to the
action xof the leader L, through minimizing its loss function f:
∀x∈X⊆Rd,w∗(x)∈arg min
w∈Wf(x,w), (39)
and the leader Lchooses xto maximize its loss function ℓ:
x∗∈arg max
x∈Xℓ(x,w∗(x)), (40)
where (x∗,w∗(x∗))is a Stackelberg equilibrium.
We then formulate unlearnable examples as a non-zero-sum Stackelberg formulation (Liu & Chawla, 2010;
Huang et al., 2021; Yu et al., 2021; Fowl et al., 2021b;a; Sandoval-Segura et al., 2022; Fu et al., 2021):
max
DpL(Dv,w∗),s.t.w∗∈arg min
wL(Dp,w). (41)
whereDp={(xi+σi,yi)}N
i=1,σiis the bounded sample-wise perturbation ( ∥σi∥p≤εσ), which can be
generalized to class-wise perturbation (Huang et al., 2021) . Similar to indiscriminate data poisoning attacks,
this primal formulation is difficult, as for the outer maximization problem, the dependence of L(Dv,w∗)on
Dporσisindirectly through the parameter wof the poisoned model.
However, in practice, we can perform a similar zero-sum reduction in Section 3 (Liu & Chawla, 2010):
max
Dpmin
wL(Dp,w). (42)
We recall that in indiscriminate data poisoning attacks, such formulation is problematic as the attacker may
simply perform well on poisoned points Dpbut poorly on clean points. However, we would not encounter
such a problem here as the perturbations are applied across the entire training set Dtr.
Now we are ready to categorize existing algorithms on unlearnable examples:
•Error-Minimizing Noise (EMN) (Huang et al., 2021): By slightly modifying Equation (42) to
min
wmin
DpL(Dp,w), (43)
Intuitively, Huang et al. (2021) construct the perturbation σto fool the model into learning a strong
correlation between σand the labels.
22Published in Transactions on Machine Learning Research (12/2022)
Table 12: Indiscriminate data poisoning attacks: the attack accuracy/accuracy drop (%) and attack running
time (hours) on CIFAR-10.
ModelClean Label Flip EMN TGDA(ours)
Acc Acc/Drop Time Acc/Drop Time Acc/Drop Time
CNN 69.44 68.99/0.45 0 hrs 69.00/0.44 2.2 hrs 65.15/ 4.29 42 hrs
ResNet-18 94.95 94.79/0.16 0 hrs 94.76/0.19 8.4 hrs 89.41/ 5.54 162 hrs
Table 13: Unlearnable examples: model accuracy (%) under different unlearnable percentages on CIFAR-10
with ResNet-18 model. Percentage of unlearnable examples is defined as|Dp|
|Dtr+Dp|.
Method 0% 20% 40% 60% 80% 100%
EMN 94.95 94.38 93.10 91.90 86.85 19.93
TGDA 94.95 93.22 92.80 91.85 85.77 16.65
•Robust Unlearnable Examples (Fu et al., 2021): Fu et al. (2021) further propose a min-min-max
formulation following Equation (43):
min
wmin
σumax
σaL(D′
p,w), (44)
where∥σu
i∥p≤εuis the defensive perturbation, which is forced to be imperceptible; ∥σa
i∥p≤εa
is the adversarial perturbation, which controls the robustness against adversarial training; D′
p=
{(xi+σu
i+σa
i,yi)}N
i=1. Fu et al. (2021) find this formulation generates robust unlearnable examples
against adversarial training.
•Adversarial poisoning (Error maximizing) (Fowl et al., 2021b): By freezing the follower entirely in
Equation (42), Fowl et al. (2021b) propose to solve the maximization problem:
max
DpL(Dp,w), (45)
such that it is similar to an adversarial example problem.
•Gradient Matching (Fowl et al., 2021a): Fowl et al. (2021a) solve the same maximization problem in
Equation (45) and apply the gradient matching algorithm in Geiping et al. (2020) (see more details
in Section 3).
B.2 Comparison with Indiscriminate Data Poisoning Attacks
Despitetheirdifferencesinproblemformulation,itispossibletocomparealgorithmsforunlearnableexamples
(we take EMN as an example here) with our TGDA attack. We identify two possible scenarios where we
may fairly compare TGDA and EMN empirically:
•Indiscriminate Data Poisoning Attacks: for EMN, we first craft perturbations using the original
algorithm. After the attack, we take Dp={(xi+δi),yi}εN
i=1, recallεis the attack budget (or poison
rate). Then, we follow our experimental protocol to perform the attack.
•Unlearnable Examples: for TGDA, we follow Equation (12) and perform the zero-sum reduction
of TGDA to perturb the entire training set. Note that we only consider sample-wise perturbation
across all experiments. Similar to our test protocol, we retrain the perturbed model and test the
performance of the attack on the test set.
We report the experimental results in Table 12 and Table 13, where we observe that:
•For indiscriminate data poisoning attacks: In Table 12, although EMN is efficient, its attack efficacy
is poor. Such poor performance is expected as the objective of EMN does not reflect the true
influence of an attack on clean test data.
•For unlearnable examples: In Table 13, we observe that TGDA (after simplification) can be directly
comparable with EMN, and the zero-sum simplification allows it to scale up to large models (i.e.,
ResNet)easily(trainingtimefor100%unlearnableexamplesis1.8hours). However,theperturbation
introduced by TGDA is not explicitly bounded.
23Published in Transactions on Machine Learning Research (12/2022)
C Other solvers than TGDA
We recall that in Section 3, we solve Equation (7) and approximate the calculation of∂w∗
∂Dpusing the total
gradient descent ascent (TGDA) algorithm (Evtushenko, 1974; Fiez et al., 2020):
xt+1=xt+ηtDxℓ(xt,wt), (46)
wt+1=wt−ηt∇wf(xt,wt) (47)
where Dx:=∇xℓ−∇wxf·∇−1
wwf·∇wℓis the total derivative of ℓwith respect to x.
Furthermore, it is possible to apply two other algorithms to solve Equation (7):
•Follow the ridge (FR) (Evtushenko, 1974; Wang et al., 2020):
xt+1=xt+ηtDxℓ(xt,˜wt), (48)
wt+1=wt−ηt∇wf(xt,wt) +ηt∇−1
wwf·∇xwf·Dxℓ(xt,wt), (49)
•Gradient descent Newton (GDN) (Evtushenko, 1974; Zhang et al., 2021):
xt+1=xt+ηtDxℓ(xt,˜wt), (50)
wt+1=wt−ηt∇−1
wwf·∇wf(xt,wt), (51)
Zhang et al. (2021) showed that both TGDA and FR are first-order approximations of GDN, despite
having similar computational complexity of all three.
In our preliminary experiments, TGDA appears to be most effective which is why we chose it as our main
algorithm.
24