Published in Transactions on Machine Learning Research (12/2024)
TheResponsibleFoundationModelDevelopmentCheatsheet:
A Review of Tools & Resources
Shayne Longpre,∗MIT
Stella Biderman,∗EleutherAI
Alon Albalak, UC Santa Barbara, SynthLabs
Hailey Schoelkopf, EleutherAI
Daniel McDuff, University of Washington
Sayash Kapoor, Princeton University
Kevin Klyman, Stanford University, Harvard University
Kyle Lo, Allen Institute for AI
Gabriel Ilharco, University of Washington
Nay San, Stanford University
Maribeth Rauh, Google DeepMind
Aviya Skowron, EleutherAI
Bertie Vidgen, ML Commons, Contextual AI
Laura Weidinger, Google DeepMind
Arvind Narayanan, Princeton University
Victor Sanh, HuggingFace
David Adelani, University College London, Masakhane
Percy Liang, Stanford University
Rishi Bommasani, Stanford University
Peter Henderson, Princeton University
Sasha Luccioni, HuggingFace
Yacine Jernite,∗HuggingFace
Luca Soldaini,∗Allen Institute for AI
Reviewed on OpenReview: https: // openreview. net/ forum? id= tH1dQH20eZ
Abstract
Foundationmodeldevelopmentattractsarapidlyexpandingbodyofcontributors,scien-
tists, and applications. To help shape responsible development practices , we introduce the
Foundation Model Development Cheatsheet: a growing collection of 250+ tools and re-
sources spanning text, vision, and speech modalities. We draw on a large body of prior
worktosurveyresources( e.g.software,documentation, frameworks, guides,andpractical
tools) that support informed data selection, processing, and understanding, precise and
limitation-aware artifact documentation, efficient model training, advance awareness of the
environmental impact from training, careful model evaluation of capabilities, risks, and
claims,aswellasresponsiblemodelrelease,licensinganddeploymentpractices. Theprocess
of curating this list, enabled us to review the AI development ecosystem, revealing what
tools are critically missing, misused, or over-used in existing practices. We find that (i) tools
for data sourcing, model evaluation, and monitoring are critically under-serving ethical and
real-world needs,(ii) evaluationsfor modelsafety, capabilities,and environmentalimpact
all lack reproducibility and transparency, (iii) text and particularly English-centric analyses
continuetodominateovermultilingualandmulti-modalanalyses,and(iv)evaluationof
systems, rather than just models, is needed for capabilities to be assessed in context.
∗Equal contribution. Correspondence: slongpre@mit.edu.1Published in Transactions on Machine Learning Research (12/2024)
1 Introduction
As the capabilities (Üstün et al., 2024; Team et al., 2023; Gomez, 2024; Anthropic, 2024a; Radford et al.,
2023; Brookset al., 2024)and market prospects (Vipra &Korinek, 2023; McElheran etal., 2024) ofartificial
intelligence have quickly expanded, so have the communities of developers, scientists, and contributors who
buildfoundationmodels(Bommasanietal.,2021). Thefields’growthhasspurredwidespreadadoption
of many tools and resources used to build, deploy, evaluate, and govern large foundation models (FMs).
However, these nascent practices are often immature. Many outstanding resources are neglected, in part
for lack of discoverability, or awareness of good practices. To address these gaps, we conduct a focused
survey, not of scientific literature (which already exists for many FM development topics (Albalak et al.,
2024; Zhao et al., 2023a; Chang et al., 2023)), but of resources for FM development such as datasets, software
packages,documentation,guides,frameworks,andpracticaltools. Inparticular,thisresourcecurationis
tailoredtoresponsibledevelopmentpracticesfornewer,smaller,ormid-sizeddevelopmentteams. LargeFM
development organizations, such as Google, OpenAI, or Meta, with substantial user bases, should adhere to
morerigorousandproduct-specificbestpracticesthanoutlinedinthisreview. WereleasetheFoundation
Model DevelopmentCheatsheet, therepository ofannotated tools fortext, speech,and vision models,and
open it for public contributions.
Foreachphaseofmodeldevelopment,ourcontributionsaresummarizedas(i)asurveyofrelevanttoolsand
resources, (ii) a synthesis of the literature’s recommended practices and use of those tools, and (iii) a review
ofthelimitationsandomissionsofexistingresources. Thecheatsheetservesasasuccinctguideofthesurvey
andrecommendedpractices,prepared byfoundationmodeldevelopers forfoundationmodeldevelopers.
Theintendedaudienceisarangeoffoundationmodeldevelopers,includingacademicresearchers,startup
companies, research labs, who are pretraining from scratch, or simply finetuning, big and small.
Oursurveyandrecommendationshopetobringwideattentiontotoolsacrossseveralphasesofdevelopment.
First, we suggest resources that support informed data selection, processing, and understanding (§§ 3 and 4).
Data prepared without sufficient due diligence can lead to unintended consequences (e.g. risks to privacy,
copyright, or generating sensitive content), marginalization (e.g. by inadvertently filtering out certain
distributions),orunexpectedmodelbehaviors(e.g. train/testoverlaporsecurityvulnerabilities). Nextwe
surveyresourcesfor preciseandlimitation-aware artifactdocumentation(Section5). Whennewdatasetsare
released, setting their governance standards early will avoid misuse later. Foundation model training can be
financiallyandenvironmentallyexpensive. Weaggregateresourcesfor efficientmodeltraining(Section6)
and estimating a model’s scaling behavior and environmental impact (Section 7). Advance awareness of these
quantities can inform more efficient training practices. For once models are trained, we provide evaluation
frameworks,taxonomiesofrisk,andbenchmarksforavarietyofevaluationcriteria(Section8). Bestpractices
suggestmodels shouldbeevaluated fortheirintendeduses, aswellas someunforeseenmisusesor harms.
Developers should design naturalistic evaluations for these settings rather than relying on available but
poorly fitting tools (Biderman et al., 2024b; Liao & Xiao, 2023). And our evaluation frameworks suggest
evaluation metrics should be contextualized, to avoid over-claiming or misunderstanding the limitations
of the reported numbers. Lastly, our survey informs responsible model release and deployment practices
(Section 9), so developers can make informed selections of licenses and release mechanisms, to address
misuse risks.
Beyondthesurveyofresources,wereviewtheexistingecosystemoftoolsandresources. Foreachsegmentof
model development, we examine the limitations, omissions, and opportunities for improvement of existing
tooling and common practices. Summarized in Table 1, we find:
•Toolsfordatasourcing,modelevaluation,andmonitoringarecriticallyunder-servingresponsiblede-
velopmentneedsandreal-worldneeds. Forinstance,theyoftenfailtohavesufficientdocumentation,
imitate real use cases, or accurately reflect licensing permissions.
•Popular model safety, capabilities, and environmental impact evaluation benchmarks lack repro-
ducibility and transparency.
2Published in Transactions on Machine Learning Research (12/2024)
•Resources for multilingual and multi-modal development, across every phase of development,
continue to receive significantly less attention than English and text-centric equivalents.
•Resources to evaluate systems, rather than just models, is needed so that capabilities and impact are
assessed in the context of real-world deployment.
Data Sources Data Preparation Model Training Model Evaluation Model Release Data 
Documentation Environmental 
Impact 
Pretraining 
Finetuning Data Documentation 
Data Governance 
Analysis 
Exploration 
Cleaning 
Filtering 
Deduplication 
Decontamination 
Auditing Estimating Impact 
Efficient Use of Resources 
Pretraining 
Finetuning 
Efficiency 
Capabilities 
Risks & Harms Model Documentation 
License Selection 
Reproducibility 
Usage Monitoring 
Data Sourcing Data Preparation Model Training Model Evaluation Release  & 
Monitoring Data 
Documentation Environmental 
Impact 
Pretraining 
Finetuning Data Documentation 
Data Governance 
Analysis 
Exploration 
Cleaning 
Filtering 
Deduplication 
Decontamination 
Auditing Data Documentation 
Data Governance 
Pretraining 
Finetuning 
Efficiency 
Capabilities 
Risks & Harms Model Documentation 
License Selection 
Reproducibility 
Usage Monitoring 
Data Sources Data Preparation Model Training Model Evaluation Model Release Data 
Documentation Environmental 
Impact 
Pretraining 
Finetuning Data Documentation 
Data Governance 
Analysis 
Exploration 
Cleaning 
Filtering 
Deduplication 
Decontamination 
Auditing Estimating Impact 
Efficient Use of Resources 
Pretraining 
Finetuning 
Efficiency 
Capabilities 
Risks & Harms Model Documentation 
License Selection 
Reproducibility 
Usage Monitoring 
Figure 1: The phases of model development by which this survey and review paper is organized.
2 Methodology & Guidelines
Wedevelopthefollowingmethodologytoguidethecollectionofthesetoolsandresources,aswellasour
analysis. First, we’ve divided the model development pipeline into several phases, illustrated in Figure 1.
Despite the visual representation, we acknowledge these phases are frequently interrelated rather than
sequential. We also include categories that have been identified by existing scholarship as necessary to
responsible development, even though they are frequently omitted from development pipelines: such as
documentation,environmentalimpactestimation, orrisksandharmsevaluation. Next, authorsareallocated
tothesephasesbasedontheirareasofexpertise,ortomodalities(text,vision,orspeech)acrossafewphases
of development. Each author surveys the literature in their segment of model development to find a mix of
(a) relevant tools, in the form of repositories, APIs, or interfaces, (b) scientific literature that directly surveys
orguidesdevelopmentdecisions,and(c)frameworksorstandardsfordevelopment(suchasdocumentation
or risk taxonomies). Due to the breadth of categories, the literature survey is inevitably non-exhaustive, but
reflectsa dedicatedsearchfor themostprominenttoolsineacharea. Certainphases ofdevelopment,such
asmodeltraining,havethousandsofavailablerepositories,sowecurateacriteriaforinclusion(outlined
below) that prioritizes certain qualities: popularity, usefulness, and advancing responsible practices.
CriteriaforInclusion. Theseprinciplesforinclusion, describedbelow, areincompleteandsubjective, but
westillbelievesufficientlyrigoroustomakeforathoroughandusefulcompilationofresources. Theresources
are selected based on a literature review for each phase of foundation model development. Inclusion is
predicatedonaseriesofconsiderations,including: thepopularityofthetoolonHuggingFaceorGitHub,
theperceivedhelpfulness asa development tool, the extent andquality ofthe documentation,the insights
brought to the development process, and, in some cases, the lack of awareness a useful resource has received
in the AI community. For an example of this last consideration, in Section 3.2 we try to include more
Finetuning Data Catalogs for lower resource languages, that often receive less attention than ones featured
more prominently on Hugging Face’s Dataset Hub. Rather than sharing primarily academic literature as in
mostsurveys,wefocusontools,suchasdatacatalogs,search/analysistools,evaluationrepositories,and,
selectively,literaturethatsummarizes,surveysorguidesimportantdevelopmentdecisions. Further,wehope
to make the coverage more comprehensive with an open call for community contributions.
Scope&Limitations. We’vecompiledresources,tools,andpapersthatguidemodeldevelopment,and
which we believe will be especially helpful to nascent (and often experienced) foundation model developers.
However, this guide is far from exhaustive —and here’s what to consider when using it:
3Published in Transactions on Machine Learning Research (12/2024)
Development Phase Frequent Status Quo Review & Recommendations
All PhasesPredominantly English, and text-centric resources. More multilingual, multi-modal, and flexible resources.
Aconcentrationofstandalone,flashy,one-offprojects,forcredit
incentives.More collaborative, large-scale interoperable infrastructure
projects that builds on existing tooling.
Data Sources (§3)Predominantly synthetic, unrealistic finetuning data. Naturalistic observations and realistic training tasks.
Predominantly English, and text-centric data sources. More multilingual, multi-modal data sources.
Intendeduses,licenses,consent,&provenancearehaphazardly
documented.Prioritizing datasets with standardized, structured, and linked
metadata.
Sparseinformationespecially ondata’ssensitivecontent,such as
CSAM and NCII.Comprehensive “data measurements” as part of large, unstruc-
tured data releases.
Data & modeling focus on easy-to-scale formats, e.g. captioned
images.More attention to collecting neglected data formats, e.g. naturally
interspersed text and images.
Data Preparation (§4)Diverse data infrastructure and processing standards for individ-
ual use-cases.Interoperable data formats and processing for many use-cases.
Mostly one-off and closed-source data exploration tools. More open data exploration tools.
Loose ideas of “high-quality” data that applies to all domains.Concreteandprecisedefinitionsofqualitythatareuniquetoaset
of domains, tasks, or evaluations.
Datapreparationmethodscomparedacrossmodelswithdifferent
training data.Standardizeddata-centricbenchmarkstofairlycomparemethods.
English-centric tokenization and processing tools.Moremultilingualandlow-resourcelanguagetokenizationand
processing tools.
Data Documentation (§5)Data documentation is diversely formatted, often terse, performa-
tive, without achieving reproducibility.Executable, and verifially reproducible scraping and analysis
scripts. Standardizeddatadocumentationrequirements(e.g. from
conferences).
Documentation is an after-thought.Documentationisstartedearly,assembledoverthecourseofcol-
lection and processing.
Data stewardship and maintenance are often ignored beyond ini-
tial release.Data governance is proactively organized, with a post-launch
maintenance and licensing plan.
Model Training (§6)Mostly educational resources for technical developers.More “last mile” educational resources for non-technical develop-
ers.
Inflexible and disparate tooling. Standardizedandcentralizedresources,especiallycross-modality.
Environmental Impact (§7)Opaque environmental impact estimate programs from cloud
providers.Query-level energy and environmental usage transparency from .
Opaque information from hardware makers and data centers.Fine-grainedtransparencyfromhardwaremakersanddatacen-
ters.
Inability to compare energy standards across systems.“Energy Star” standards for non-technical users to fairly compare
AI services.
Scaling laws are text-centric and impractical.Multimodal scaling laws research. Intuitive interfaces to estimate
and model scaling laws for training forecasts.
Model Evaluation (§8) Evaluating model outputs. Evaluating systems, within their application contexts.
Evaluating on synthetic toxicity and safety benchmarks.Evaluating natural observations, real-world settings, and with
human-interaction studies.
Reporting evaluation metrics only. Releasing evaluation scripts for verifiable reproducibility.
Model Release (§9)No or uninformed license choices.License selection guided by the context of data, potential and
unforeseen uses, and legal considerations.
Weights release with limited support.Accompanying documentation, and easy-to-run code for training,
evaluation, and inference.
Limitedplansforusagemonitoring,orover-claimedbenefitsfrom
watermarking/monitoring.Aplanthatconsidersgating,watermarking,andmisusereporting
(though they are not always beneficial).
Harm & hazard taxonomies are based on existing benchmarks.Harmtaxonomiesarebasedonempiricalobservations,andstudies
with real users.
Table 1: A summary of the reviews & take-aways from each phase of foundation model development
ecosystem.
•Temporalscope. Foundationmodeldevelopmentisarapidlyevolvingscience. Thisdocumentis
only a sample, dated to March 2024.
•Tools & Resources : This is not a general survey of scientific literature (which can include the-
ory,abstractrecommendations,andanalysis),butonpracticableinstrumentsforAIdevelopment,
evaluation, and safety—which a developer can directly apply. These tools and resources are specifi-
4Published in Transactions on Machine Learning Research (12/2024)
callyscopedtodatasets, databases,frameworks,taxonomies,protocols,interactivewebsites, APIs,
software,andcoderepositories(fordataprocessing,training,evaluation,orotheruses). Wealso
selectively include literature with specific best practice recommendations, and literature surveys for
further context on a topic.
•Applicable developers. We scope these resources to mid-to-small foundation model developers.
Large organizations, with commercial services and/or wide user bases, have broader considerations
for responsible development that what is outlined in this work.
•Development phases & modalities We’ve scoped our data modalities only to text, vision, and
speech, and to the phases of development outlined in Figure 1. We support multilingual resources,
but acknowledge there remain significant community-wide gaps in awareness and adoption here..
•Wereminduserstofollowstandardpracticeinassessingthesecurityandviabilityofeachtool,
particularly for their circumstance. At times we have provided resources with conflicting advice, as
it is helpful to be aware of divided community perspectives. Our notes throughout are designed to
contextualize these resources, to help guide the reader’s judgement.
3 Data Sources
Data Sourcing Best Practices
•Pretraining data provides the fundamental ingredient to foundation models—including their
capabilitiesandflaws. Finetuningdataimprovesthemodel’sperformanceinspecificsettings,orin
the case of instruction finetuning or alignment training, improves the model’s general usability and
helpfulness while aiming to reduce potential harms.
•Moredataisnotalwaysbetter. Itisessentialtocarefullysourcedata,andmanuallyinspectitto
ensure it fits the goals of your project.
•Datasetselectionincludesmanyrelevantconsiderations,suchaslanguageanddialectcoverage,
topics, tasks, diversity, quality, and representation.
•Most datasets come with implicit modifications and augmentations, from their selection, filtering,
and formatting. Pay attention to these pre-processing steps, as they will impact your model.
•Finetuning data can improve the model’s performance in some settings or impair others. Use
catalogstosupportaninformedselection,andpreferwell-documentedtounder-documenteddatasets.
•Crowdsourceddatacatalogs,includingHuggingFace,maycontainimportantomissionsanderrors
in their dataset documentation. Verify information, such as data licensing and critical characteristics,
with the original data sources and academic papers, if possible.
•The most appropriate datasets may not exist for a given set of tasks. Be aware of the limitations of
choosing from what is available.
3.1 Pretraining Data Sources
Pretrainingcorporaconsistofmillionsofpiecesofcontent,fromdocuments,images,videos,orspeechrecord-
ings,oftenscrapeddirectlyfromtheweb. Modelpretrainingrepresentsthefundamentalstepininstilling
foundationmodelswiththeirabilitiestorepresentsyntax,grammar,reasoning,andworldknowledge(Devlin
et al., 2018; Brown et al., 2020; Chowdhery et al., 2023). Consequently, it is important to carefully curate the
data composition, including the mix of sources, characteristics, and preprocessing decisions (Longpre et al.,
2023b). However, the vast scale of this content often means its is shallowly documented and understood,
despite community efforts to unpack it (Dodge et al., 2021; Elazar et al., 2023).
We highlight a few of the most popular pretraining corpora which have accumulated deeper documentation
and analysis. In the text domain, web scrapes from common crawl ( commoncrawl.org ), or OSCAR ( https:
//oscar-project.org/ )(Suárezetal.,2019;Laippalaetal.,2022)arethebaseingredientformostpretraining
5Published in Transactions on Machine Learning Research (12/2024)
corpora. In particular, derivatives of common crawl, such as C4 (Raffel et al., 2020; Dodge et al., 2021) or
multilingual C4 (Kreutzer et al., 2022) provide 2019 indexes that have been heuristically filtered for well-
formed text. Subsequent pretraining datasets incorporate one or multiple indexes from common crawl. This
includesthePile(Gaoetal.,2020),RefinedWeb(Penedoetal.,2023),RedPajama(TogetherAI,2023),and
Dolma (Soldaini et al., 2024), which have iterated on basic document quality filtering and deduplication.
Thesecorporaoftenmergeanddeduplicatemultipleyearsofcommoncrawlscrapes,orsupplementadditional
sources including biomedical text (PubMed), legal text (Freelaw, USPTO patent documents), code (Github,
Stack Exchange), public domain books (Project Gutenberg), among others.
Formultilingualtext,theOpenParallelCorpus(OPUS)offersamassivecollectionoftranslatedtextdocument
pairs (Tiedemann, 2012), ROOTS (Laurençon et al., 2022) collates and processes diverse multilingual re-
sources,includingOSCAR(Laippalaetal.,2022)andtheBigscienceCatalogue(McMillan-Majoretal.,2022),
CulturaX (Nguyen et al., 2023) covers 167 languages from OSCAR and mC4, and WURA (Oladipo et al.,
2023) centralizes and manually audits documents from 16 African languages. Most recently, MADLAD-400
(Kuduguntaetal.,2023)providesa3trilliontoken,2023processedsplitofCommonCrawl,spanning419
languages.
Large, specializedcorpora of texthave also recently emerged, to specializemodel abilities, ormitigate risks
ofpossiblecopyrightinfringement. Asexamples,thePileofLaw(Hendersonetal.,2022)andMultiLegalPile
(Niklausetal.,2023)centralizecourtopinions,contracts,andlegislativerecords;theStackandStackv2scrape
permissively-licensedGitHubrepositories(Kocetkovetal.,2022);peS2o(Loetal.,2020)releasescleaned
academicpapersfromSemanticScholar;andtheProofPile2(Azerbayevetal.,2023)andOpenWebMath
(Paster et al., 2023) aggregate vast mathematical text resources. Notably, recent text corpora also attemptto
isolatepermissively-licensed,orcopyrightfreedata,suchastheOpenLicenseCorpus(Minetal.,2023)—
though this is a challenging task, and does not guarantee that all non-commercially licensed or copyrighted
content has been removed.
In the context of speech, webscrapes of English audiobook data from LibriVox (a website hosting free public
domainaudiobooks)arecommonlyusedforfoundationmodelarchitecturedevelopmentandevaluation.
Sourcing data from LibriVox, LibriSpeech (Panayotov et al., 2015) is a 960 hour fully supervised dataset
(i.e. all audio are paired with transcriptions) and Libri-Light (Kahn et al., 2020) is a 60k hour dataset for
benchmarkingusingnoorlimitedsupervision(10h,1h,and/or10min). Multilingualmodelsaretypically
pre-trained on a combination of speech corpora, e.g. for XLS-R (Babu et al., 2022) a combination 436k hours
fromVoxPopuli(400khoursfrom23languagesbasedonEuropeanParliamentrecordings:Wangetal.,2021),
Multilingual LibriSpeech(50k hoursof audiobooksfrom 8languagesPratap et al.,2020), CommonVoice
(28k hours of crowd-sourced read speech from 100+ languages Ardila et al., 2020), VoxLingua107 (6.6k
hours from 107 languages scraped from YouTube Valk & Alumäe, 2021), and various IARPA BABEL corpora
(totaling 1k hours from 17 African and Asian languages).
Inthecontextofvision,thereareseverallargepretrainingcorpora,comprisedoftextandimagesfoundin
largewebscrapes. COYOaggregates700Mimageswithalt-textfromtheweb.1MultimodalC4,orMMC4,
(Zhuetal.,2024)leveragestheoriginalC4URLstoextractinterleavedimageandtextpairs,totaling570M
images, and 43B tokens. Similarly, OBELICS also leverages the Common Crawl web collection, filters for
multimodal web pages, and extracts images from the HTML (Laurençon et al., 2024a), totaling 353M images,
with 115B tokens. Lastly, DataComp-1B and CommonPool-13B (Gadre et al., 2024a) isolate high quality
subsetsofimage-textpairsonCommonCrawl. Forvideospecifically,WebVid(Bainetal.,2021)provides10M
videosandtheirtext,fromwhichimagedatasetscanalsobeextracted. Forvisionorspeech,WebDatasets
provides a high-performance data streaming tool.2
3.2 Finetuning Data Catalogs
Inthissection,wesurveysourcesthatcatalogfinetuningdatasets—sometimesknownmorebroadlypost-
trainingdatasets. Finetuningdata,differingfrompretrainingdatainthatitiscuratedforsupervisedlearning,
ismorespecializedtotheintendedinference-timetask,andistypicallymuchsmallerinscale. Finetuning
1https://huggingface.co/datasets/kakaobrain/coyo-700m
2https://github.com/webdataset/webdataset
6Published in Transactions on Machine Learning Research (12/2024)
datasetsareusedforavarietyofreasons: tohonespecificcapabilities,orientthemodeltoacertaintaskformat,
improve its responses to general instructions, mitigate harmful or unhelpful response patterns, or generally
alignitsresponsestohumanpreferences. Developersincreasinglyvarythetypesofdataannotationsand
loss objectives, depending on the goal. Notably, after pretraining practitioners commonly use traditional
supervised finetuning, DPO (Rafailov et al., 2023) or reinforcement learning objectives from human (or
machine) feedback to generated responses (Ouyang et al., 2022; Bai et al., 2022).
Given the thousands of specialized data sources for finetuning, we recommend using data catalogs that
provide well documented datasets, to make for an informed selection. HuggingFace Datasets ( https:
//huggingface.co/docs/datasets/index )offersthelargestandmostpopularAIcommunitycatalogacross
modalities and tasks (Lhoest et al., 2021a). Many datasets are accompanied by data cards, however Longpre
et al. (2023b) show there are frequent omissions and errors, as is the nature of crowdsourced catalogs.
ThereexistanarrayofotherspecializeddatacatalogswithdatasetsthatmaynotappeardirectlyonHug-
gingFace. For instance, the NusaCrowd catalog for South East Asian languages (Cahyawijaya et al.),
the Masader Arabic data catalogue (Alyafeai et al., 2022), the AI4Barat Indian data catalog ( https:
//huggingface.co/ai4bharat ),aswellastheMaskahaneNLP( https://github.com/masakhane-io )and
Zenodo AfricaNLP catalogs ( https://zenodo.org/communities/africanlp/ ) offer specialized multilin-
gual text and speech resources for the language communities. Recently, the Aya Collection (Singh et al.,
2024) offers curated, multilingual text finetuning datasets across 65 languages. For more accurate and
comprehensivedatadocumentation,particularlyforlicensingandprovenance,theDataProvenanceInitiative
publishestoolstosearchandfilterpopularHuggingFacefinetuningdatasets(text,speech,andvision)across
avarietyofcriteria(Longpreetal.,2023b). Liuetal.(2024c)recommendbestpracticesindevelopingand
using synthetic data, which has become increasingly popular in the text domain.
In the context of speech, OpenSLR ( http://openslr.org ) is a large collection of user-contributed datasets
for various speech processing tasks. For the task of spoken language identification, VoxLingua107 (Valk
& Alumäe, 2021) comprises audio scraped from YouTube using various language-specific keywords and,
analogously, for speaker identification/verification VoxCeleb (Nagrani et al., 2017) comprises audio of from
1,000 celebrities.
In the context of vision, there are a few well known data sources for finetuning. ImageNet (Deng et al., 2009)
providesthehistoricalframeworkforwhichimageclassificationmodelscompetedon1.3Msampleswith
1000diverseclasses. MSCOCO(Linetal.,2014)providestrainingdataforimagedetection,segmentation,
captioning and retrieval. There exist many options for modern instruction finetuning datasets in the vision
domain. A few notable examples include the Multi-Modal, Multilingual Instruction Tuning (M3IT) dataset
(Li et al., 2023b), comprising 40 datasets, 2.4 million examples over 400 tasks in 80 languages; The Cauldron,
comprising50vision-languagedatasets(Laurençonetal.,2024b);andLLaVAVisualInsruct150k,asetof
text-image datasets generated by prompting the GPT-4-0314 API (Liu et al., 2024a). For using web-scraped
data, Child and Sexual Abuse Material (CSAM) is an acute concern. Tools such as PhotoDNA can be used to
help detect and filter for these images, though they may not be completely accurate or comprehensive.3
3.3 Review
In this section we critically review the current state of resources for data sourcing, from our survey.
The community would benefit from more accurate and comprehensive licensing, provenance, and creator
consent information for existing datasets. Many datasets tend to be under-documented (Bandy & Vincent,
2021;Sambasivanetal.,2021),orerroneouslydocumented(Longpreetal.,2023b),with65%ofHuggingFace
datasetlicenseseitheromittedorincorrectlylabelled. Thisisespeciallytrueoflargedatacollectionsthathave
re-packagedandsometimesre-licensedhundredsofdiversedatasets,eachwithdifferentdocumentation
standards (Longpre et al., 2023a; Sanh et al., 2021). The tools used to discover, select, and verify the dataset
propertiesareunder-developed,especiallywithrespecttoconcernsofcreatorconsent,copyrightinfringement,
andrelatedtermsofuse. Forcreatorconsent,initialopt-in/opt-outtoolinghasyettobewidelyadopted. While
3https://www.microsoft.com/en-us/photodna
7Published in Transactions on Machine Learning Research (12/2024)
HuggingFacehasintegratedSpawning’sopt-indatabase( https://api.spawning.ai/spawning-api ),there
remains limited creator and developer adoption. For copyright information new datasets and catalogs such
as the Data Provenance Initiative (Longpre et al., 2023b), offer more detailed license tracing tools, but their
coverage also remains limited. Lastly, synthetic data generation (usually using OpenAI APIs) has expanded
significantlyforfinetuningdatasets(Longpreetal.,2023b),butdon’talwaysdocumentthelimitationsonthe
data use imposedbythe APIs. This is further compoundedbythe fact thatthere is substantial uncertainty
about the extent to which the terms of service of such platforms bind downstream users. The absence of
infrastructuretotraceandverifythesetypesofdatadocumentationleadtouninformeddatasourcingand
use.
The communitywould benefitfrom more accurateand comprehensiveinformation onsensitive content,
such as CSAM and NCII. Prior work highlights the risks of proliferated CSAM and NCII as a fundamental
risk from generative AI models (Kapoor et al., 2024; Lakatos, 2023b; Thiel et al., 2023a). However, significant
portions of the AI community have frequently trained on large-scale datasets that contain this sensitive
content without widespread awareness—such as LAION-5B (Birhane et al., 2021; David, 2023). With a
greaterfocusongenerative,multimodal,andmorememorization-pronelargemodels,thereisagreaterneed
for resources to help identify and filter for sensitive content.
Data & modeling have focused predominantly on easy-to-scale data formats, neglecting other formats. A
prominent focus of multimodal modeling has been image-to-caption and caption-to-image tasks. The ease of
sourcing and scaling these caption datasets to tens of billions has enabled these tasks to progress. However,
this has neglected more complex and useful reasoning tasks, that may require text and images with different
relationships, interleaved in different ways. In the context of speech, many automatic speech recognition
datasetscompriseonlyofreadspeech(e.g. ascollectioninvolvedsolicitingcrowd-sourcedparticipantsto
read various text stimuli), which is vastly different from informal, multi-speaker conversations which are the
“primordial home of human language“ (Dingemanse & Liesenfeld, 2022).
There is a scarcity of realistic training tasks, and naturalistic observations. Many open academic datasets
are developed for niche, even artificial purposes (e.g. academic visual question datasets that are often
detachedfromrealworldusecases). Tocollectrealisticusecases,however,requiresaccesstovolunteered
userlogsfromrealproductsandservices. Someattemptshavebeenmadetodothis,suchasWildChat(Zhao
etal.,2023b),howevertheirparticipationmaystillskewtoasuperficialdistribution. Toolsandevenpolicy
to scalably source real data, while preserving privacy, is critical to sourcing grounded and relevant training
data.
8Published in Transactions on Machine Learning Research (12/2024)
4 Data Preparation
Data Preparation Best Practices
•Toolsfor searchingandanalysing canhelpdevelopersbetterunderstandtheirdata,andthere-
fore understand how their model will behave; an important, but often overlooked, step of model
development.
•Datacleaning and filtering can have an immense impact on the model characteristics, though
there is not a one size fits all recommendation. The references provide filtering suggestions based on
the application and communities the model is intended to serve.
•Whentrainingamodelondatafrommultiplesources/domains,thequantityofdataseenfrom
each domain ( data mixing ) can have a significant impact on downstream performance. It is common
practicetoupweightdomainsof“high-quality”data;datathatisknowntobewrittenbyhumansand
has likely gone through an editing process such as Wikipedia and books. However, data mixing is an
active area of research and best practices are still being developed.
•Removingduplicateddata canreduceundesirablememorizationandcanimprovetrainingeffi-
ciency.
•Itisimportanttocarefully decontaminatetrainingdatasets byremovingdatafromevaluation
benchmarks, so their capabilities can be precisely understood.
4.1 Data Search, Analysis, and Exploration
A critical step to understanding a dataset is to explore and analyze what it contains. In particular, exploring
training datasets with search and analysis tools can help practitioners develop a nuanced intuition for what
exists in the data, and therefore can help to predict what behaviors the model will exhibit.
Tools for search, analysis, and exploration can take a few forms. Some tools are aimed at understanding the
high-level statistics of a dataset such as the length of inputs, frequency of specific n-grams, the languages in
thecorpus,possiblebiases,ortheexistenceofundesirablecontent. Forexample,toolssuchasWIMBD(Elazar
etal.,2023)andInfini-gram(Liuetal.,2024b)allowuserstoperformn-gramsearchesthroughcommonly
used pretraining datasets, and provide starting points for building a search index over any arbitrary dataset.
TheROOTSsearchtool4(Piktusetal.,2023)additionallyallowsuserstosearchwithfuzzyn-gramsoverthe
ROOTS corpus, and similarly, the clip-retrieval tool5(Beaumont, 2022) allows users to search for nearest
neighborimagesandtextfromamultimodalcorpus(e.g. LAION-5B(Schuhmannetal.,2022). Furthermore,
the HuggingFace Data measurements tool6gives users access to statistics such as the most common n-grams,
lengths of data points, and distribution over labels for a number of pretraining and finetuning datasets. Yet
moretoolsexisttoexplorethedatamanuallyandincludingtheDataProvenanceExplorer(Longpreetal.,
2023b) for text datasets, Google’s Know Your Data tool7for vision datasets and NVIDIA’s Speech Data
Explorer8for speech datasets.
Inmostcases,itishighlyrecommendedtoexploredatasetsfromtheperspectiveofhigh-levelstatisticsas
well as getting to know the data by looking at individual data points. For instance, text data can have a wide
distributionoflengths,topics,tones,formats,licenses,andevendiction,andunderstandingeachofthese
dimensions will require a different tool. We recommend that developers use the many available tools to
search and analyze their datasets.
4https://huggingface.co/spaces/bigscience-data/roots-search
5https://github.com/rom1504/clip-retrieval
6https://huggingface.co/spaces/huggingface/data-measurements-tool
7https://knowyourdata.withgoogle.com/
8https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/tools/speech_data_explorer.html
9Published in Transactions on Machine Learning Research (12/2024)
4.2 Data Cleaning, Filtering, and Mixing
Once the contents of a dataset are understood, the next step is to clean and filter the dataset to adjust the
dataset’sdistributiontowardsdesirablecontent. Filteringandcleaningdosobyremovingunwanteddatafrom
thedataset. Theycanimprovetrainingefficiencyaswellasensurethatdatahasdesirableproperties,including:
highinformation content,desiredlanguages, lowtoxicity,and minimalpersonallyidentifiable information.
Datamixingisanotherimportantcomponentofdatapreparation,wherethemixtureproportionsofdata
domains(e.g.scientificarticles,GitHub,andbooks)havebeenshowntodramaticallyaffectdownstream
performance (Gao et al., 2020; Xie et al., 2023a; Albalak et al., 2023).
A first step for filtering text data is by language, where there is a plethora of tools including langdetect9,
cld310,OpenLID11(Burchelletal.,2023),GlotLID12(Kargaranetal.,2023),andFastText13(Graveetal.,
2018). The majority of modern language identification methods have been built on top of the FastText
modelusedintheCCNetpipeline(Wenzeketal.,2020). Inadditiontofilteringbylanguage,datasetsare
commonly cleaned using heuristics (e.g. remove documents with fewer than 5 words, or remove lines that
start with "sign-in") which have been implemented through a number of different tools including the Dolma
toolkit14(Soldainietal.,2023),Lilac15,andDataTrove16(Penedoetal.,2024),aswellasDataComp17(Gadre
et al., 2023) for image-text pairs. While heuristic filtering methods can remove significant quantities of data,
theycan bebrittle. Model-based methods(e.g. removedatawhich isdissimilarto aknown“high-quality”
corpus or possibly toxic content) such as DSIR18(Xie et al., 2023b) and Detoxify19(Hanu & Unitary, 2020)
canbeusedasadditionalfilteringthatallowformuchmoreflexibilitythanheuristics. Inadditiontocleaning
andfiltering,mixingisanothercomponentofdatasetdesignwhichrequirescarefulconsideration. Many
dataset mixing ratios have been determined by heuristics and human judgement. For example, the Pile (Gao
etal.,2020)andLlama(Touvronetal.,2023)upweightdomainsthathavelikelygonethroughanediting
process,suchasbooksandWikipediaarticles. Toolsforautomateddatamixingare,asofwriting,verylimited.
However,academicresearchprojectssuchasDoReMi20(Xieetal.,2023a)andOnlineDataMixing21(Albalak
et al., 2023) provide GitHub repositories that can be repurposed for new datasets.
Cleaning,filtering,andmixingarecrucialcomponentsofdesigninganappropriatedataset,butduetothe
vastspaceofpossiblefiltersanddownstreamusesofMLmodelsthereisnoone-size-fits-allrecommendation.
Practitionerslookingtocleanandfiltertheirdatasetsshouldfirstusethesearch,analysis,andexploration
toolstodeterminehowtodesignappropriatefilters,anditerativelyimprovethefiltering. Formoredetails
and recommendations on cleaning, filtering, and mixing, see the recent survey by Albalak et al. (2024).
4.3 Data Deduplication
Data deduplication is an important preprocessing step where duplicated documents, or chunks within a
document,areremovedfromthedataset. Removingduplicatescanreducethelikelihoodofmemorizing
undesirable pieces of information such as boilerplate text, copyrighted data, and personally identifiable
information. Additionally, removing duplicated data improves training efficiency by reducing the total
dataset size.
Deduplication generally relies on one of four methods: URL matching, hashing methods, string metrics,
or model representations, which can classify data as either exact or fuzzy matches. We suggest using the
9https://github.com/Mimino666/langdetect
10https://github.com/google/cld3
11https://github.com/laurieburchell/open-lid-dataset
12https://github.com/cisnlp/GlotLID
13https://huggingface.co/facebook/fasttext-language-identification
14https://github.com/allenai/dolma
15https://github.com/lilacai/lilac
16https://github.com/huggingface/datatrove
17https://www.datacomp.ai/
18https://github.com/p-lambda/dsir
19https://github.com/unitaryai/detoxify
20https://github.com/sangmichaelxie/doremi
21https://github.com/alon-albalak/online-data-mixing
10Published in Transactions on Machine Learning Research (12/2024)
deduplicationmethodsincludedinDataTrove22(Penedoetal.,2024),Google’sdeduplicate-text-datasets
library23(Lee et al., 2022), or the Dolma toolkit24(Soldaini et al., 2023) for their ease of use. In practice,
multiplestepsofdeduplicationareperformed. First,simplededuplicationcanbeperformed(e.g. URL-based
filtering),followedbymorecomplicatedhashing-andmodel-basedmethods. Practitionersshouldalways
determinewhetherduplicateddatawillharmorhelpthemodelfortheirusecase. Whilememorizationis
commonly castasa badthing inmachinelearning, itcan bea positivesuch aswhena model“memorizes”
the answer to a factual question (Biderman et al., 2024a).
4.4 Data Decontamination
Data decontaminationis the process ofremoving evaluation datafrom the training dataset. This important
stepindatapreprocessingensurestheintegrityofmodelevaluation,ensuringthatmetricsarereliableand
not misleading.
Prior to training a model on a dataset, it is important to decontaminate that dataset from the desired
evaluation datasets. BigCode25and Carper AI26both implement contamination detection through the use
ofMinHashLSH,whichcanbeusedtodetectcontaminationpriortotrainingamodel. Oneconcernwith
somemodernmodelsisthatthemodeldevelopersmaynotdisclosetheirtrainingdata,somethodsandtools
havebeendevelopedtodeterminewhetheramodelwastrainedonaspecificdataset. Forexamplecanary
strings,uniquesequencesofcharacters,canbeincludedintrainingdatasets,andJagielski(2023)explain
how to interpret canary exposure, which can identify whether a model was trained on the specified dataset.
OneexampleofcanarystringscanbefoundintheBIG-benchdataset.27Inadditiontocanarystrings,Shi
et al. (2023) propose Min-K% probability, a method for finding possible contamination.28
Oneimportantnoteforpractitionerswhoareperformingdecontaminationpriortotrainingistoseedata
deduplicationmethodsforinspiration. Forexample,datadeduplicationmethodsthatuseexactmatching
(e.g. Bloom filters from Dolma) are also good candidates for decontamination.
4.5 Data Auditing
Auditingdatasetsisanessentialcomponentofdatasetdesign. Youshouldalwaysspendasubstantialamount
oftimereadingthroughyourdataset,ideallyatmanystagesofthedatasetdesignprocess. Manydatasets
have problems specifically because the authors did not do sufficient auditing before releasing them. The
tools outlined in the data search, analysis, & exploration section ae typically sufficient to track the evolution
ofadatasetasit’sbeingcreated. However,therearealsotoolsthatcanbeusedtoauditpreviouslycreated
datasets.
The Data Provenance Initiative29(Longpre et al., 2023b) is a good resource that documents the source,
license, creator, and other metadata for over 1,800 text finetuning datasets. The Have I Been Trained?30tool
can assist in finding and detecting data within LAION datasets. See the blog post by Jernite (2023) for more
details on auditing datasets.
4.6 Review
In this section we consider, evaluate, and critically review the current state of resources for data preparation.
22https://github.com/huggingface/datatrove
23https://github.com/google-research/deduplicate-text-datasets
24https://github.com/allenai/dolma
25https://github.com/bigcode-project/bigcode-analysis/tree/main/data_analysis/decontamination
26https://github.com/CarperAI/decontamination/tree/main
27https://github.com/google/BIG-bench/.../training_on_test_set/README.md#training-on-the-test-set
28https://github.com/swj0419/detect-pretrain-code
29https://www.dataprovenance.org/
30https://haveibeentrained.com/
11Published in Transactions on Machine Learning Research (12/2024)
Thecommunitystandstobenefitgreatlyfromincreasedeffortsonopen-sourcedataexplorationtools.
As we’ve discussed in this section, tools for data exploration and analysis are a crucial component of the
iterative process of creating a dataset, however, the openly available tools for exploring data are limited.
Specifically,theexistingtoolscansearchforn-grams,showrandomsamplesfromthedataset,andreturn
high-level statistics of the dataset. However, there are many additional features that may be useful. For
example, after searching for an n-gram in a corpus, it may be helpful to see the containing documents to
understand when this n-gram occurs and what the surrounding context is.
Open-source data exploration tools allow for retrospective analysis of datasets. Additionally, improving
the functionality and ease-of-use for data exploration tools can help to retrospectively analyze existing
datasets. Forexample,thesetoolscanbeusedtofindpotentialissuessuchasbiases,illegalorcopyrighted
content, andpersonallyidentifiable information, aswell as to ensure thatthey contain the advertisedcontent
(e.g. alignment datasets should contain safe text).
Consolidating on a standardized format for data storage and processing will give developers more time
to focus on developing infrastructure. Furthermore, many of the data preparation tools and methods
presentedhereexistinseparateone-offrepositories. Thishasledtolimitedopen-sourcedeffortsonlarge-scale
infrastructure. Conglomeratingaroundalimitednumberofdataformatscanreducefrictionbyallowing
developerstomakeassumptionsontheformatofdata,thusenablingdeveloperstofocuseffortsondeveloping
scalabletoolingfordatapreparation. Forexample,theuseofApacheArrowinHuggingFaceDatasets(Lhoest
et al., 2021b) and Numpy’s memmapin GPT-NeoX (Andonian et al., 2021) has reduced the effort required for
developers to develop memory-efficient data loading, allowing for developers to focus on building scalable
tooling. Similarly, using a single data format, such as that from Dolma (Soldaini et al., 2024), can allow
developerstofocusonbuildingbetterlarge-scaledatainfrastructure,andspendlesstimewritingcomplicated
code that considers multiple data formats.
Taking a fine-grained view on “high-quality” data. Referring to data as “high-quality” was originally
usedinthecontextofpretrainingdata,buthassincebecomemorewidelyadoptedwithoutacleardefinition,
leadingtosignificantambiguityinthecommunity. Inthecontextofcleaningandfilteringwebdata,high-
qualityhas referredtodata that isknownto havebeen writtenbyhumans, andhaslikely gonethroughan
editing process, leading to the development of quality filters which aim to find data most similar to domains
suchasbooksorWikipedia(Brownetal.,2020;Chowdheryetal.,2023). However,theexactdefinitionof
quality has since expanded to pretraining domains beyond web data (e.g. code, reasoning), and the phrase
hasbeenadoptedinothertrainingregimessuchaspreferencefine-tuning. Thefieldoffoundationmodel
developmentdoesnotcurrentlyhaveacleardefinitionofwhatdataleadstohigh-qualitymodels,andthe
developmentofsuchdefinitionsisahigh-impactdirectionofresearchandengineering. Whileresearchon
data quality for pretraining may be out of reach for smaller institutions and individual researchers, research
on data quality for specific downstream use cases (e.g. code, reasoning, math) is more feasible.
Forthisreason,weadvocateforresearchondataqualitytobecontextualizedwithinaspecificdomainor
set of evaluations. This will not only allow for research to progress in parallel across many groups and
institutions, but we believe will also lead to definitions of quality that are much more concrete, precise, and
definitive. Furthermore,thedevelopmentofcleardefinitionslowersthebarrierforcreatingtoolsthatcan
find additional “high-quality” data.
Developing data-centric benchmarks can catalyze progress. Data-centric research is currently being
doneacross sucha widevarietyofsettings, andwithvaryinggoals, that ithasbecome nearlyimpossible
to compare methods (Albalak et al., 2024). Some recent benchmarking works have tried to address this
by providing a fixed model training setup, and requiring competitors to improve the data for training.
Specifically, DataComp (Gadreet al., 2023) provides a data-centric benchmarkfocused on image-text pairs,
DataPerf(Mazumderetal.,2023)providebenchmarksfor4settings: vision,speech,debugging,andlanguage.
Additionallyforthelanguagedomain,FETA(Albalaketal.,2022)providesabenchmarkforfew-shottask
transfer,andtheLoosetrackfromBabyLM(Warstadtetal.,2023)providesresearchersabenchmarkforbetter
dataselection. However,duetothelargenumberoftrainingsettingsanddomainsofinterest,thereareawide
12Published in Transactions on Machine Learning Research (12/2024)
varietyofadditionalbenchmarksandchallengesthatwouldbeusefulforthecommunity(e.g. pretraining,
instruction tuning, alignment, task-specific fine-tuning). Creating new data-centric benchmarks will not
onlyallowfordirectcomparisonbetweenmethods,improvingourunderstandingofthedatapreparation
methods,butalsolowersthebarforentryintothefieldbycreatinganeasy-to-useinfrastructure,enabling
increased progress.
Data preparation tools should consider not only English-centric data, but non-English and low-resource
languages. Whilesomedatapreparationtoolsmayworkout-of-the-boxforlow-resourcelanguages(e.g.
n-gram search), others may require more thought and effort (e.g. heuristic filtering). It is particularly
important to include native speakers for low-resource languages throughout the data preparation process.
5 Data Documentation and Release
Documentation Best Practices
•Datadocumentationisessentialforreproducibility,avoidingmisuse,andhelpingdownstream
users build constructively on prior work.
•We recommend to start the documentation process early, as data is collected and processed.
•For datasetswith multiplestakeholdersor derived fromcommunity efforts, itis important tobe
proactive in decision-making about access, licenses, and stewardship.
5.1 Data Documentation
When releasing new data resources with a model, it is important to thoroughly document the data (Bender
&Friedman,2018;Hollandetal.,2020;Gebruetal.,2021;Bommasanietal.,2024). Documentationallows
users to understand its intended uses, legal restrictions, attribution, relevant contents, privacy concerns, and
other limitations. An example of how to describe and document data governance decisions can be found in
the BLOOM project’s report (Jernite et al., 2022). The StackV2 (Lozhkov et al., 2024) is another example of a
carefully curated and well documented dataset. Data documentation can also be a way to empower model
trainers and downstream users of AI systems to It is common for datasets to be widely used by practitioners
whomaybeunawareofundesirable properties(David,2023). Whilemanydatadocumentationstandards
havebeenproposed,theiradoptionhasbeenuneven,orwhencrowdsourced,aswithHuggingFaceDatasets,
they may contain errors and omissions (Lhoest et al., 2021a; Longpre et al., 2023b).
5.2 Data Governance
ReleasingalldatasetsinvolvedinthedevelopmentofaFoundationModel,includingpretraining,fine-tuning,
andevaluationdata,canfacilitateexternalscrutinyandsupportfurtherresearch. However,releasingand
hosting the data as it was used may not always be an option, especially when it includes data with external
rights-holders;e.g.,whendatasubjects’privacy,intellectualproperty,orotherrightsneedtobetakeninto
account. Proper data governance practices can be required at the curation and release stages to account for
these rights.
Insomejurisdictions,projectsmayberequiredtostartwithaDataManagementPlanthatrequiresdevelopers
to ensure that the data collection has a sufficient legal basis, follows principles of data minimization, and
allows data subject to have sufficientvisibility into and control over their representation in a dataset (CNIL
resourcesheet). Datacurationstepstothatendcanincluderespectingopt-outpreferencesignals(Spawning,
HaveIBeenTrained), or applying pseudonymization or PII redaction (BigCode Governance card).
Once a dataset is released, it can be made available either broadly or with access control based on research
needs(ROOTS,BigCodePIItrainingdataset). Developerscanalsoenabledatasubjectstoaskforremoval
from the hosted version of the dataset by providing a contact address (OSCAR, PAraCrawl), possibly
13Published in Transactions on Machine Learning Research (12/2024)
complemented by a membership test to check whether their data is included (Stack data portraits) or an
automated process (BigCode, AmIinTheStack).
5.3 Review
Better data documentation of existing and new datasets is still needed. Most datasets still lack appropriate
documentation. While data documentation tools exist they are underutilized at present. Longpre et al.
(2024c) illustrate challenges in documenting provenance, consent, and authenticity of datasets at scale, and
driving adoption of rigorous data standards.
DatasheetsandDataCardsarejustastart. Whiledatadocumentationtools,suchasdatasheetsanddata
cardsareveryuseful,itispreferredtobeginprojectswithaDataManagementPlanandensurethatdata
collectionisdesignedthoroughly. Considerationsshouldinclude: thelegalbasisforcollectingdata,ensuring
that collection is limited to the necessary data, transparency, respecting opt-out preferences and redaction of
PII.
6 Model Training
Model Training Best Practices
•The foundation model life-cycle consists of several stages of training, broadly separated into
pre-training and fine-tuning.
•Decisionsmadebydevelopersatanystageoftrainingcanhaveoutsizedeffectsonthefieldand
themodel’spositiveandnegativeimpacts,especiallydecisionsmadebywell-resourceddevelopers
during the pre-training stage.
•Developers should be thoughtful about the effects of train-time decisions and be aware of the
trade-offs and potential downstream effects prior to training.
•Due to the large economic and environmental costs incurred during model training, making
appropriate use of training best practices and efficiency techniques is important in order to not waste
computational or energy resources needlessly.
Thousands of tools and resources have been developed for model training. While it is not feasible to
comprehensivelylistthem,wefocusoureffortsonasubsetoftoolsthatarewelldocumented,emphasize
computational efficiency, or educational resources. For a more comprehensive list of training tools, we point
the reader to surveys, such as the survey of foundation model training and serving systems (Zhou et al.,
2024),thesurveyofefficientfederatedlearningmethods(Woisetschlägeretal.,2024),orthis“comprehensive”
survey of foundation models (Zhou et al., 2023), which can be used to trace their training setup.
Foundationmodelsarebytheirdesignfrequentlyreusedandappliedfornumerousdiversedownstreamuses.
This takes the form of a multi-stage training process throughout models’ lifestyles, starting with training
a strong base from scratch (“pretraining”) and followed by further refinement or adaptation to new use
cases (“fine-tuning”). For this reason, all stages of training must be done with care: especially for pretrained
models or models that will otherwise be deployed widely or used as a base for further extensions, the
decisionsmadebymodeltrainersattrain-timecanhaveoutsizedimpactsonthemodel’scharacteristics,both
positive and negative, or on the field as a whole.
Here,wecoveraselectionofexistingresourcesformodeltraining. Weincludefrequently-usedcodebasesfor
model training that may be useful entry points for new developers to the field, but note that we cannot cover
all existing options. We additionally briefly discuss resources where practitioners can learn about improving
the resource-efficiency of their training, as well as educational resources for learning about model training.
6.1 Pretraining
Model pretraining requires by far the largest amount of resources computationally during model training.
14Published in Transactions on Machine Learning Research (12/2024)
Therefore,practitionersshouldconsiderusingalready-optimizedcodebases,especiallyinthepretraining
phase,toensureeffectiveuseofcomputationalresources,capital,power,andeffort. Existingopen-source
codebasestargetedatfoundationmodelpretrainingcanmakepretrainingsignificantlymoreaccessibleto
new practitioners and help accumulate techniques for efficiency in model training.
Various codebases may be optimal depending on the scale of model or number of devices used for pre-
training. Some codebases seek to be performant while also being lightweight (OpenLM (Gururangan et al.,
2023),Nanotron(nan,2024))foreffectivetrainingatmediumscalesandup,whileothersaimformaximal
performance at massive scales such as GPT-NeoX (Andonian et al., 2021), Megatron-DeepSpeed (Smith
et al., 2022b), and Megatron-LM (Shoeybi et al., 2020) and have been used in practice to train language
modelsacrossthousandsofGPUs. Dependingontheparticularvisionapplicationormodelarchitecture,
good scalable codebases exist for training. For example, OpenCLIP (Ilharco et al., 2021) can be used for
trainingCLIPtext+visionmodels,Timm(Wightman,2019)supportsavarietyofstandardvisionmodel
architecturesandtrainingscripts,andUniLMsupportsthetrainingofinterleavedtext-and-imagemodels
similartoKosmos-2(Pengetal.,2023). Formodalitiesotherthantextorvision,toolingexists(Lhotsefor
audio data processing and data loading (Żelasko et al., 2021), Stable Audio Tools (AI) for training audio
generation models) but is less standardized. Lastly, Karamcheti et al. (2024); McKinzie et al. (2024) explore
lotsofdesignchoicesformultimodalmodels,includingtraining,fine-tuning,imageprocessingstrategies,
and data mixing.
We emphasize the utility and importance of adopting existing codebases for pretraining, due to the difficulty
of debugging and detecting errors in large-scale distributed systems as encountered in pretraining. Existing
scalable codebases drastically reduce the chances of silent failures or correctness issues lowering the end
model quality.
6.2 Fine-tuning
Fine-tuning, or other types of adaptation performed on foundation models after pretraining, are an equally
importantandcomplexstepinmodeldevelopmentandhavetheabilitytosignificantlysteerthebehaviors
andcharacteristicsoftheendmodel. Fine-tunedmodelsarealsomorefrequentlydeployedthanbasemodels,
making theirsafety andusability very important. Here wediscuss asubset offinetuning resourcesthat are
well documented, flexible, and some of which cater to computational efficiency.
While fine-tuning is significantly less resource-intensive than pretraining, there are still many relevant
considerations for practitioners. Use of widely adopted tools such as libraries designed for fine-tuning
(Axolotl(OpenAccess-AI-Collective),trlX(Havrillaetal.,2023),Levanter( https://github.com/stanford-
crfm/levanter ))canensuregreaterecosystemcompatibilityofresultingmodels,orreducethebarrierto
experimentationbyabstractingaway commonpitfallsorprovidingguidanceoneffective hyperparameters.
Similarly, the use of techniques such as QLoRA (Dettmers et al., 2023) or other popular parameter-efficient
fine-tuningapproachesandlibraries(peft(Mangrulkaretal.,2022),Otter,LLaMA-Adapter,LLaVA(Lietal.,
2023a;Zhangetal.,2023a;Liuetal.,2023a))canallowforfine-tuningforlowercostoronmoreaccessible
hardware.
6.3 Efficiency and Resource Allocation
Knowledge of training best practices and efficiency techniques can reduce costs to train a desired model
significantly. Beyond systems-level optimizations and approaches to increase efficiency, the most important
techniqueisdeterminingthemostefficient allocation ofresources,suchasallocatingcomputebetweenmodel
size and dataset size for a given budget for the best results. The most common approach to solving this
problem is to apply scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023b), a
common tool for cheaply extrapolating findings across scales of cost in order to make decisions based on
smaller-scale experiments for a final model training run.
Additionally, practitioners seeking to embrace an open approach to model development should consider
howtheirdecisionswhentrainingafoundationmodelmayhaveimpactslongafterthatmodel’screation
15Published in Transactions on Machine Learning Research (12/2024)
and release. For instance, a model that is released openly but is too computationally demanding to be run on
consumer-grade hardware will be limited in its impact on the field, or a model trained to minimize training
compute but not minimize inference cost may result in a greater environmental impact than spending more
training compute in the first place for a cheaper-to-infer model (Hoffmann et al., 2022; Touvron et al., 2023).
Practitioners should thus be aware of potential second-order effects of their model releases and training
choices.
6.4 Educational Resources
Training models at any scale can be quite daunting to newer practitioners. However, there are options
available for learning about how to train and run foundation models.
Resources which themselves collate and provide reading lists or recommendations for learning about large-
scaleMLtraining(EleutherAICookbook(Anthonyetal.,2024), MLEngineeringOpenBook(Bekman))can
beanespeciallyusefulplacetobegin. Additionally,minimalcodebasescreatedaseducationalexamplessuch
asNanoGPT(Karpathy,2023),orotherblogpostsdetailingfundamentalconceptsintrainingorrunning
foundation models may be useful (Chen, 2022; Anthony et al., 2023). We recommend that practitioners
review these resources and use them to guide further reading about model training and usage.
6.5 Recommendations
Regardless of the stage or cost of training being performed, we urge developers to carefully consider the
impacts of their training choices, and how they can be improved, as we have described. We hope the linked
resources will be a helpful jumping-off point.
We additionally encourage practitioners to use the codebases linked as a foundation for their work, to avoid
“reinventing the wheel” for every new project. Unifying and collectively improving existing codebases,
tooling, and commonly used standards around model training, as with tooling around other parts of the
foundationmodeldevelopmentprocess,canallowfortheentirecommunitytobenefitfromothers’effort
viausingcodebasesthathavebeenwell-testedforcorrectnessandscalability. Thiscanleveragetheunique
advantages of open model development via strength in numbers.
6.6 Review
Moreresources,especiallynon-Englishones,forloweringthebarriertoentryforlesstechnicaldevelopers
areneeded . Currenttoolingrequiresatminimumatechnicallyknowledgeableuserorproficientprogrammer,
andfurthermayhavelackingdocumentationorbehighincomplexity. Openlyavailabletoolsfortraining,
especially fine-tuning, models that do not presume familiarity with training methods or even programming
would have the potential to allow many more users not formally educated in computer science to customize
and build models suited to their use cases, such as for lower-resource languages.
More standardized and centralized resources, especially cross-modality, should be focused on in the
future.Many current resources are centered around language model training. However, resources for
othermodalitiesareoftenmorescatteredorrelyoncustomarchitecturesandimplementations. Exploring
topicssuchasmultimodalfine-tuningbestpractices,andmultimodalpreferencelearning,isanimportant
futurearea. Forinstance,thereisalackofefficienttoolingfordataloadingformultimodaltraining. While
WebDataset31offers a resource for images, there are fewer options for videos, which can cause the GPU
to stay idling, waiting for data. Future work should look to unify these techniques and modalities in the
same tools and infrastructure to enable more and easier investigation or transfer of techniques that work
onlanguagemodelstotherestofthefield. Evenwithinthetextmodality,an interoperable ecosystemwith
shared, well-tested code building blocks and standards should be pursued, to provide a strong base for
further extension and the pooling of developer efforts.
Otheroptionsformassivelycollaborativedevelopmentshouldbepursued. Toallowforbetterpooling
of resources, both over time and across the community, methods for more collaborative training are an
31https://github.com/webdataset/webdataset
16Published in Transactions on Machine Learning Research (12/2024)
importantfuturefrontier. Someofthesemethodsareintheirearlystagesalreadybeingprovidedbytools
suchasMergeKit(Goddardetal.,2024)andbeingexploredbytheresearchcommunity(Matena&Raffel,
2022; Don-Yehiya et al., 2023; Stoica et al., 2024; Yadav et al., 2023). By enabling better re-use of existing
artifacts or allowing a greater number of collaborators to contribute to building a model together, these
methods could potentially make model training as parallelizable and reusable as work on data improvement,
and the merging of efforts and compute expended could allow a wider range of contributors to steer model
development.
7 Environmental Impact
Environmental Impact Best Practices
•TraininganddeployingAImodelsimpactstheenvironmentinseveralways,fromtherareearth
minerals used for manufacturing GPUs to the water used for cooling datacenters and the greenhouse
gasses (GHG) emitted by generating the energy needed to power training and inference.
•Developers should report energy consumption and carbon emissions separately to enable an
apples-to-apples comparisons of models trained using different energy sources.
•Itisimportanttoestimateandreporttheenvironmentalimpactnotjustofthefinaltrainingrun,
but also the many experiments, evaluation, and expected downstream uses.
•Itisrecommended,especiallyformajormodelreleases,tomeasureandreporttheirenvironmental
impact, such as carbon footprint, via mechanisms such as model cards (see Section 5).
7.1 Estimating Environmental Impact
TheenvironmentalimpactofAIisofincreasingconcern(Schwartzetal.,2020). Estimatingtheenvironmental
impactofmodeldevelopmentisachallengingtaskduetothenumberofrelevant,hardtocompute,andoften
unrecorded variables. For instance, to estimate the Life Cycle Assessment (LCA) of model development
(Klöpffer,1997), variablesinclude: model size,architecture, durationof training,number oftraining runs,
storingandtransferringdata, hardwaremanufacturing,thespecifictypeandsetupofthehardware,network
configuration,the geographiclocationofthedata center,thecarbonintensity oftheenergygrid, thepower
usageeffectiveness(PUE)ofcooling,overhead,andthebroaderdatacenterinfrastructure,aswellasasimilar
setofquestionsforvariousdeployment/inferencesettings(Pattersonetal.,2021). Manyofthesevariables
are also infeasible to precisely estimate, given for instance, that Nvidia does not disclose the carbon footprint
ofitsGPUs(Luccionietal.,2023). Anotherchallengeincludesthevarietyofrelevantenvironmentaloutcome
measures, from CO2 emissions, to energy footprint, or water use.
Existing tools to measure environmental impact rely on a series of assumptions and estimates based on the
available information. Perhaps the most accurate tools are those built into cloud services, which are able
totracethehardwareconfigurationsandgeographicallocationsofdatacentersbutmaynototherwisebe
publicly available to users. The Azure Emissions Impact Dashboard ( https://www.microsoft.com/en-
us/sustainability/emissions-impact-dashboard ), AWS carbon footprint tool ( https://aws.amazon.
com/aws-cost-management/aws-customer-carbon-footprint-tool/ ), and Google Cloud Carbon Foot-
print measurement system ( https://cloud.google.com/carbon-footprint?hl=en ) are three such sys-
tems, available only when using their cloud services directly. Independent systems, such as Carbontracker
(Anthony et al., 2020), CodeCarbon (Schmidt et al., 2021), or the Experimental Impact Tracker (Henderson
et al., 2020) offer basic estimate modeling and reporting, based on limited input information. Similarly,
Li et al. (2023c) provides an estimate tool for the water usage footprint of language model training and
deployment. ML CO2 Impact ( https://mlco2.github.io/impact/ ) improves these repositories with a
wrapper interface for easier use. However, these services are forced to trade-off between ease of use and
accuracy,assignificantinputinformationisrequiredtoobtainpreciseresults,whichimposesaburdenon
users and widespread adoption.
17Published in Transactions on Machine Learning Research (12/2024)
7.2 Effective use of resources
Severaldecisionsmadepriortomodeltrainingcanhavesignificantimpactsontheupstreamanddownstream
environmental impact of a given model. Empirical scaling laws can be used to find the best allocation
of resources. Kaplan et al. (2020); Hoffmann et al. (2022) estimate the optimal model size and training
duration,givenatrainingcomputebudget. AndAghajanyanetal.(2023)investigatestheequivalentefficient
computeallocationformulti-modalsettings. Whenworkingwithtexttrainingdatathatisconstrained,recent
work exploreshow to allocatecomputeefficiently (Muennighoff etal., 2023b). Formodels frequently used
downstream, it is important to consider the inference footprint and inference cost during model creation
(Gadreetal.,2024b),tominimizetheenvironmentalimpactofinference. Forfurtherresourcesanddiscussion,
see 6.3.
7.3 Review
In this section we critically review the current state of resources for environmental impact analysis. First, we
note a dearth of transparency, from hardware manufacturers, data centers, and corporate model developers
stymiesenvironmentalimpactestimates,andparticularlyproductcomparisonsforusers. Lastly,scalinglaws
researchneeds toexpandtocovernewermulti-modalmodeling efforts,aswell asprovideintuitive tooling
for open developers to adopt.
Transparency from consumer-level API providers, into query-level energy and environmental usage
measures. Currently, some of the most widely adopted generative AI services, including the APIs and
playgrounds from OpenAI, Google, Anthropic, Inflection, Midjourney, and others, do not expose any
information into the environmental footprint of using their models (Bommasani et al., 2023). As these
systems dominate consumer market usage (Korinek & Vipra, 2023), this leaves a wide gap in our knowledge
ofneteffects. Thescientificcommunityrelieslargelyonassumptionsandballparkestimates,bothfortraining
and inference impact.
Transparency from hardware makers and data centers, into fine-grained energy and environmental usage
measures. UpstreamofAIdevelopers,thedatacentersandhardwaremakersexposelimitedinformation
aboutenvironmentalandenergymeasures(Luccionietal.,2023). Thesemetricswouldenablemoreaccurate
and real-time estimates of compute, for closed and open developers.
“Energy Star” standards for non-technical users to fairly compare AI services. Data center, hardware, and
developer transparency are a precursor to accurate estimates and, more importantly, environmental footprint
competition betweencorporateservices. Currently,theseservicescompeteonmodelqualityandprice,but
notonenvironmentalimpact—apropertythatmanyconsumersarelikelytocareaboutifgiventheoption.
“EnergyStar”standards,fromotherindustries,allowcompetitorstoclaimequalorbetterservices,atless
energyexpenditure(Brownetal.,2002). Thesesortsofapples-to-applescomparisonsmaybenecessaryto
inhibit negative environmental consequences from AI.
Scalinglawsresearchcurrentlylacksempiricalevidenceforthenewwaveofmulti-modalmodels,and
intuitive user interfaces for new developers. Scaling laws research has focused predominantly on text
(Kaplan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023b; Gadre et al., 2024b), with limited
work for multi-modal foundation models (Aghajanyan et al., 2023). As developers increasingly pursue
image, video, and speech models (both in input and output), such as Sora (Brooks et al., 2024), Stable Video
Diffusion (Blattmann et al., 2023), Claude 3 (Anthropic, 2024b), and Whisper (Radford et al., 2023), the
efficient scaling laws are currently under-investigated. Secondly, while scaling law research investigates
fundamental questions, it can be presented in unapproachable, complex ways. The ecosystem lacks tools for
less technical developers to heed efficient compute estimates, such as a plug-and-play interface.
18Published in Transactions on Machine Learning Research (12/2024)
8 Model Evaluation
Model Evaluation Best Practices
•Modelevaluationisanessentialcomponentofmachinelearningresearch. Howevermanymachine
learning papers use evaluations that are not reproducible or comparable to other work .
•One of the biggest causes of irreproducibility is failure to report prompts and other essential
components of evaluation protocols. This would not be a problem if researchers released evaluation
code and exact prompts, but many prominent labs (OpenAI, Anthropic, Meta) have not done so for
model releases. When using evaluation results from a paper that does not release its evaluation code,
reproduce the evaluations using a public codebase .
•Examples of high-quality documentation practices for model evaluations can be found in Brown
etal.(2020)(forbespokeevaluations)andBlacketal.(2022);Scaoetal.(2022);Bidermanetal.(2023)
(for evaluation using a public codebase).
•Expect a released model to be used in unexpected ways. Accordingly, try to evaluate the model on
benchmarks that are most related to its prescribed use case, but also its failure modes or potential
misuses.
•All evaluations come with limitations. Be careful to assess and communicate these limitations
when reporting results, to avoid overconfidence in model capabilities.
8.1 Capabilities
Many modern foundation models are released with general conversational abilities, such that their use cases
arepoorlyspecifiedandopen-ended. Thisposessignificantchallengestoevaluationbenchmarksthatare
unabletocriticallyevaluatesomanytasks,applications,andrisksfairlyorsystematically. Asaresult,itis
important tocarefully scopethe originalintentions forthe model, andto tieevaluations to thoseintentions.
Even then, the most relevant evaluation benchmarks may not align with real use, and so developers should
qualifytheirresults,andcarefullysupplementthemwithdatafromrealuser/humanevaluationsettings
where feasible.
For language models, common capabilities benchmarks include those that evaluate models on narrow tasks
such as software engineering (Jimenez et al., 2023), topic classification (Adelani et al., 2023), and explaining
code (Muennighoff et al., 2023a). More comprehensive evaluation suites, such as the Language Model
Evaluation Harness (Gao et al., 2023) and HELM (Liang et al., 2023), are also common. Leaderboards
like LMSys Chatbot Arena (Zheng et al., 2023) offer another type of capability evaluation based on human
feedback.
Therearefarfewercapabilityevaluationsforothermodalities. Comprehensiveevaluationsuitesexistfor
visionmodelsaswell(Leeetal.,2023b;Awadallaetal.,2023),buttheyarerelativelylesswelldeveloped.
Therearealsoanumberofcommonbenchmarksforevaluatingvisionmodelsonalargenumberoftasks
(Gadre et al., 2023; Liu et al., 2023b; Fu et al., 2023). Evaluations for speech models’ capabilities are still
nascent,withtheOpenASRLeaderboard,32whichranksmodelsbasedontheirWordErrorRateandReal-
TimeFactor,andtheEdinburghInternationalAccentsofEnglishCorpus(Sanabriaetal.,2023)asleading
examples.
ThecheatsheetincludescommonbenchmarksasofDecember2023,butwecautionthateachcomeswith
substantial limitations. For instance, many benchmarks based on multiple choice exams are not indicative of
realuserquestions,andcanbegamedwithpseudo-datacontamination. Additionally,whileleaderboards
areexceedinglypopular,modelresponsesareoftenscoredbyothermodels,whichhaveimplicitbiasesto
model responses that are longer, and look similar to their own (Dubois et al., 2023).
32https://huggingface.co/spaces/hf-audio/open_asr_leaderboard
19Published in Transactions on Machine Learning Research (12/2024)
8.2 Harm & Hazard Taxonomies
Taxonomiesprovideawayofcategorising,definingandunderstandingrisksandhazardscreatedthrough
theuse anddeploymentof AIsystems. Sometaxonomies focusprimarily onthetypesof interactionswith
modelsthat createariskof harm(oftencalled“hazards”)whereas othersfocusonthenegativeeffectsthat
theyleadto(oftencalled“harms”). Sometaxonomiesfocusonexistingissues,suchasmodelsthatcreate
hatespeechorchildabusematerial,aswellasmoreintangibleorindirectformsofharm,suchastheriskthat
modelsperpetuatebiasesandstereotypes,andmisrepresentsocialgroups. Othertaxonomiesarefocused
onthe longer-termthreatsposed bymore sophisticatedmodels,such asultra-personalised disinformation,
cybersecurity threats, and military use (Brundage et al., 2018). Some work has also focused on categorising
catastrophic or“existential” riskspresented byArtificial GeneralIntelligence, suchas rogueAI agentsand
Chemical, Biological, Radiological, Nuclear and high-yield Explosive weapons (Carlsmith, 2022; Hendrycks
et al., 2023; Bucknall & Dori-Hacohen, 2022). Further, a few taxonomies also assess the available evidence for
therisks andhazards, discusstheirimpact, andoffer mitigationstrategies (Dengetal., 2023;Kapooret al.,
2024; Klyman, 2024).
Many taxonomies are released with an associated dataset, which can be used to either train models to
minimise safety risks or to evaluate those risks. Several datasets have been released as benchmarks (e.g.
Wang et al., 2024), which can be used to track progress across the community. We provide a non-exhaustive
list of existing taxonomies with datasets that are available open-source. We also note forthcoming work
planned by organisations like ML Commons, which aims to standardise assessment of AI safety risks by
introducing a new benchmark, comprising a taxonomy and dataset.33.
1.TrustLLM is a benchmark that covers six dimensions in English, including truthfulness, safety,
fairness, robustness, privacy,and machine ethics (Sunet al., 2024). The benchmarkcomprises over
30datasetsfromexistingresearch. Inthepaper,theytest16open-sourceandproprietarymodels,
and identify critical safety weaknesses.
2.SafetyBench isabenchmarkthatcoverseightcategoriesofsafety,inbothEnglishandChinese(Zhang
etal.,2023b). CategoriesincludeOffensiveness;UnfairnessandBias;PhysicalHealth;MentalHealth;
Illegal Activities; Ethics and Morality; and Privacy and Property. Unlike most safety evaluation
datasets,SafetyBenchcomprisesmultiplechoicequestionswhichmakesautomatedevaluationof
models far easier. In the paper, they test 25 models and find that GPT-4 consistently performs best.
3.DecodingTrust is a benchmark that covers eight dimensions of AI safety and trustworthiness in
English(Wangetal.,2024). ItcoversarangeofsafetycriteriasuchasToxicity;Stereotypes;Adversarial
Robustness; Out-of-Distribution Robustness; Privacy; Machine Ethics; And Fairness. The benchmark
has a leaderboard that is hosted on HuggingFace.
4.HarmBench is a standardized evaluation framework for automated redteaming of LLMs in English
(Mazeikaetal.,2024). Itcovers18widelyusedredteamingmethods,suchasPersona,stochastic-few
shot,PEZ andGBDA. Thebenchmarkhasbeen designedwithbothseven semanticcategories(e.g.
Cybercrime, Misinformation and Bioweapons) and four “functional categories” (e.g. Standard
behaviours). In the paper, 33 LLMs are tested against HarmBench.
5.BigBench (Srivastava et al., 2023) and HELM (Liang et al., 2023) contain tests that are related to
safety,suchastoxicity,biasandtruthfulnessinBigBenchandtoxicity,bias,disinformation,copyright
infringement andtruthfulnessin HELM. Bothmake useof the widely-usedRealToxicityPrompts
dataset (Gehman et al., 2020a). Terms such as “toxicity” and “offensiveness” have been criticised in
some papers for beingoverly broad and easyto misinterpret (Vidgen et al., 2019), andmore recent
work has tended to use more fine grained terms.
6.Individual datasets have also been released that can be used to assess specific safety risks of models,
such as SimpleSafetyTests which tests for clear-cut safety problems (Vidgen et al., 2023) and XSTest
which tests for model false refusal (Röttger et al., 2024).
33https://mlcommons.org/working-groups/ai-safety/ai-safety/
20Published in Transactions on Machine Learning Research (12/2024)
7.SafetyPrompts is a website that hosts datasets for evaluating the safety of models34It does not
aggregateorcombinethedatasetsthatithosts,buthasabasicreviewprocesstocheckthequality
and integrity of the datasets.
Almost all of the taxonomies described here are informed by practitioners’ experiences of tackling safety
issuesinAImodels;priorresearch;andexploratoryredteaming. Manyhavedrawnheavilyonexistingwork
insocialmediatrustandsafety,suchasBankoetal.(2020);Devetal.(2022). Theyarealltop-downinthe
sense that they define categories of hazard (or harm) and then find, create or curate prompts that match
thosecategories. AnalternativewayofaddressingAIsafetyistostartfromthebottomupandtotaskred
teamers with creating their own categories. Grounded-theory style approaches in linguistics can be used to
then standardise the categories and ascribe some structure to the taxonomy. Across all methods of creating
taxonomies (and datasets), there is a substantial focus on text-only models and future work should pay
moreattentiontomultimodalandnon-textbasedmodels. Similarly,thereisastrongbiastowardsEnglish
language and the Western cultural context. This should also be addressed in future work.
8.3 Risks & Harms
Modeldevelopershaveusedvarioustechniquestoaddressrisksandmitigateharms. Broadly,theseattempts
fall into three categories, roughly ordered by effectiveness.
In-context learning. In-context learning can be used to add instructions to a model’s system prompt to steer
the model’s outputs. For example, OpenAI’s DALL-E 3 model included instructions to avoid outputting
copyrighted characters35. In some sense, this is the easiest model-level intervention: developers do not need
toretrainorfinetunethemodel,neitherdotheyneedtorelyonexternalAPIcalls,suchastothird-party
content moderation endpoints. However, this comes at a cost: system prompts are easy to jailbreak, and as a
result, interventions based on in-context learning might be more brittle compared to other guardrails.
Model alignment. One of the most prominent approaches for mitigating risks and harms is aligning models
withpreferencedata. Thisusuallyinvolvessupervisedfinetuningortrainingrewardmodelsthat steerthe
outputsoflanguagemodels. Populartechniquesincludereinforcementlearningwithhuman(RLHF)and
AI(RLAIF)feedback(Baietal.,2022;Ouyangetal.,2022). Tohelpendusersunderstandtheperformance
of reward models, Lambert et al. (2024) develop a leaderboard for evaluating reward models for various
desiderata, including safety. Still, model alignment techniques can be brittle against simple modifications to
themodel,suchasfinetuning(Qietal.,2023),evenwhenthemodelisfinetunedusingbenigndata(He
et al., 2024).
Guardrailsonmodelinputsandoutputs. Thetwointerventionsaboverelyonchangingthemodelbehavior
via in-context learning or fine tuning. However, guardrails on model inputs and outputs that lie outsidethe
model might be a more robust intervention for preventing harmful content generation. For example, several
modeldevelopersprovidemoderationendpointsthatcanbeusedtofilterinappropriateuserrequestsor
model outputs. These can be general-purpose endpoints that can be modified for filtering and moderation
(e.g., Cohere’s classification endpoint modified for toxicity detection36or Anthropic’s suggested prompt for
contentmoderation37)aswellasendpointsthatarespecificallybuiltforcontentmoderation(e.g.,Perspective
API(Leesetal.,2022)andOpenAI’smoderationendpoint38). Google’sPaLMandGeminimodelsallow
API users to set thresholds based on the safety likelihood and severity of model outputs39).
The examples above are all from closed model developers. Recently, several open models have also been
proposed formoderating modelinputs and outputs. Forexample, Llama-Guard byMeta (Inan etal., 2023)
canbeusedtofilterharmfulcontent. Nvidia’sNEMOguardrailsallowmodelproviderstoaddprogrammatic
guardrails to filter content (Rebedea et al., 2023).
34https://safetyprompts.com/
35See: https://the-decoder.com/dall-e-3s-system-prompt-reveals-openais-rules-for-generative-image-ai/
36See: https://docs.cohere.com/reference/toxicity-detection
37See: https://docs.anthropic.com/claude/docs/content-moderation
38See: https://platform.openai.com/docs/guides/moderation/quickstart
39See: https://cloud.google.com/vertex-ai/generative-ai/docs/configure-safety-attributes-palm ,https://cloud.
google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes
21Published in Transactions on Machine Learning Research (12/2024)
8.4 Review
A shift away from evaluating models, and towards evaluating systems. Existing evaluation practices and
reporting often focus on probing individual models, in unconstrained settings. However, deployed systems
oftenoperateinthecontextofmultipleinteractivemodels,moderationendpoints,sophisticateddecoding
strategies,andrule-basedconstraints,thatmakeupa“system”. Morepointedly,fordual-usesystems,the
context of use outside the system is what defines the presence of harm: Narayanan & Kapoor (2024) argue
that“defensesagainstmisusemustprimarilybelocatedoutsidemodels”. Priorworkhasemphasizedthe
importanceofthesystem(Dobbe,2022)andcontext(Raji&Dobbe,2023)indiagnosingandresolvingAI
safety challenges. As a result, efforts to evaluate models alone are inherently limited, both in their findings,
and informing effective changes. Safety problems in particular are better addressed by evaluations that
considerthecontext(e.g. attackvector,deployedusesetting),aswellastheinteractionsbetweenelementsof
afoundationmodelsystem(ofwhichonlyoneisathemodelitself). However,thesetypesofevaluationscan
bedifficultintheabsenceoftransparencyintothecomponentsofdeployedsystems(Bommasanietal.,2023).
Recent work has even shown that independent researchers can face significant obstacles in fairly evaluating
proprietary systems (Longpre et al., 2024a).
A shift away from evaluating on static toxicity/safety benchmarks, and towards evaluating on real-world,
dynamicallychangingnaturalisticobservations,aswellashuman-interactionstudies. Existingwidelyused
model risk and safety evaluations, such as RealToxicityPrompts (Gehman et al., 2020b), or bias benchmarks
such as BBQ (Parrish et al., 2022), BOLD (Dhamala et al., 2021), and HolisticBias (Smith et al., 2022a),
havebeeneffectiveattestingmodels’superficialbiastendencies. However,theseevaluationsarestatic,not
groundedinreal-worldcontexts,andpaynoheedtothehuman-interactionelement— i.e. theactualsystem
users. These limitations suggest several dimensions of improvement for future safety benchmarks:
•Naturalisticobservations Collectingnaturalinteractionswithusersenableresearcherstoidentify
realistic tasks and prompts, to simulate real harms. For instance, WildChat (Zhao et al., 2023b)
collects voluntary user interactions with OpenAI systems through proxy interfaces—however these
datasets may be heavily skewed by the types of users that adopt it.
•Domain expert designed tasks LegalBench (Guha et al., 2024) offers an alternative naturalistic
design,whereby evaluationsare hand-craftedbythepractitioners(inthiscase, legalprofessionals)
in the field of evaluation. These benchmarks distinguish themselves from existing evaluation suites
in their relevance to natural, real-world usage.
•Human-interactionstudies Mostevaluationsarenon-interactive—theytargetthemodel,without
real-world users. This form of analysis can identify model flaws, but falls short of investigating the
actual affects, harms, and interaction patterns on users. Lee et al. (2023a) proposes a framework to
evaluateinteractiveuserexperience,withoutwhichnotionsofharmandsafetyareunder-developed.
Le Ferrand et al. (2022) illustrates the importance of interactive evaluation, particularly on non-
Western users.40
These three evaluation dimensions enable more realistic studies of user interaction with foundation model
systems. Collecting naturalistic or interactive data can also provide continuous and dynamic data sources,
whichareparticularlybeneficialwhenthereisrapidmodeldevelopment(orthehabitoftrainingonprior
evaluation sets). More generally, research into dynamic and evolving benchmarks, that test beyond the
training set distribution, are an important research direction (Yu et al., 2023; Kiela et al., 2021).
A shift away from reporting evaluations, to releasing reproducible evaluation scripts. There are dozens of
choicesthataffecttheresultsofanevaluation(Anthropic,2023). Somechoicesinclude,butarenotlimitedto:
•Prompt format Results vary dramatically depending on if the input prompts are zero-shot, few-shot,
chain-of-thought,andalsoifthepromptsweremanuallyrefineddirectlyonthetestset. Multiple
40In this study, Australian Aboriginal use of an information retrieval app illuminated false assumptions and expectations in the
evaluation procedure.
22Published in Transactions on Machine Learning Research (12/2024)
choice benchmarks have both versions with and without the answer choices given in the prompt.
(See MMLU as a widely used dataset with inconsistencies in use and standardization.41
•Decodingstrategies Thedecodingalgorithmanditshyperparameterchoices,suchasthetemperature
andsamplingprobabilities,willaffectmodelbehavior. Forsystems,ratherthanmodels,responses
canbetheresultofmultipleiterationsormodels,orrelyonexternaltoolsorsources. Ensembling
techniques like self consistency (Wang et al., 2022) also improve performance significantly.
•Evaluationmetric Thechoiceofevaluationmetriccanimpacttheapparentmagnitudeofdifferences
between models, and even their ranking (Schaeffer et al., 2024).
•Humanreviewsetup Forhumanpreferenceevaluations,severaldetailsaffectfairevaluation: whether
theresponseselectionissufficientlymodel-blind,theattentivenessandexpertiseoftheannotatorsto
the given topics, and the chosen rubric. Human preferences can also be skewed by the same factors
as model-based evaluations (Hosking et al., 2024; Xu et al., 2023; Wu & Aji, 2023)
Without transparency and reproducibility integrated into the evaluation procedure, the axes of design
freedom can allow developers to game results, and prevent fair, apples-to-apples comparisons. For these
reasons,onlyevaluationscriptsthataredirectlyexecutablebythird-partiesprovide verifiablereproducibility .
Executableevaluationscriptsalsoallowauditorstounpackthechoicesmadeinevaluation. Auditorscan
quicklyexperimentwithdifferentpromptformats,decodingparameters,andevaluationmetrics,toshed
lightonthescientificveracityoftheclaims. Whileevaluationdocumentationishelpful, therequiredbreadth
of information inevitably leads to subtle omissions, and even if the information is comprehensive, it does not
provide verifiable reproducibility. For these reasons, we believe the evidence is clear that AI safety reporting
is not sufficiently reliable or trustworthy without verifiable execution scripts.
Groundharmandhazardtaxonomiesinempiricalobservations. Existingtaxonomiesofharmareoften
created to cluster existing safety benchmarks, which are mostly detached from real observations (Sun et al.,
2024;Zhangetal.,2023b;Hendrycksetal.,2023). Asthetaxonomiesofharmcanguidepractitionerspriorities,
this could lead to neglected or over-emphasized areas of safety research. It is essential that future harm
taxonomies are strongly grounded in empirical or naturalistic observations, through research conducted
withreal users , rather than hypothetical situations envisioned by researchers.
Extendingrisksandharmsstudiestomultimodalandhighlysensitiveattacks. Agreatdealofresearch
into risks and harms is focused exclusively on text, in English, and on more conservative safety risks such
astoxicityandbias(Gehmanetal.,2020a;Hartvigsenetal.,2022). However,recentworkhasemphasized
the particular risks of generative image, speech, and video models, being used to create deepfakes, NCII
or CSAM (Kapoor et al., 2024; Thiel et al., 2023b; Lakatos, 2023a). Other work has illustrated the much
greaterefficacyofjailbreaksandattacksinnon-Englishlanguages,ascomparedtoEnglish(Yongetal.,2023).
Multimodaljailbreakingisanemergingresearcharea,withsomenascentwork(Qietal.,2024;Shayegani
etal.,2023;Niuetal.,2024). Researchintorisksandharmsfromautonomousweaponssystems(AWS)is
also highly under-explored, and an emerging risk (Simmons-Edler et al., 2024; Longpre et al., 2022).
41https://crfm.stanford.edu/2024/05/01/helm-mmlu.html
23Published in Transactions on Machine Learning Research (12/2024)
9 Model Release & Monitoring
Model Release & Monitoring Best Practices
•Release models with accompanying, easy-to-run code for inference, and ideally training and
evaluation.
•Document models thoroughly to the extent possible. Model documentation is critical to avoiding
misuse and harms, as well as enabling developers to effectively build on your work.
•Open source is a technical term and standard with a widely accepted definition that is maintained
bytheOpenSourceInitiative(OSI)(Initiative,2024). Notallmodelsthataredownloadableorthat
have publicly available weights and datasets are open-source; open-source models are those that are
released under a license that adheres to the OSI standard.
•Theextenttowhich“responsibleuselicenses”arelegallyenforceableisunclear. Whilelicenses
thatrestrict enduse ofmodels mayprevent commercialentities fromengaging inout-of-scope uses,
they are better viewed as tools for establishing norms rather than binding contracts.
•Choosing the right license for an open-access model can be difficult. Apache 2.0 is the most
commonopen-sourcelicense,whileresponsibleAIlicenseswithuserestrictionshaveseengrowing
adoption. Considerusingoneoftheavailabletoolsforselectingtherightopen-sourcelicenseforyour
model artifacts.
•Frameworksformonitoringandshapingmodelusagehavebecomemoreprevalentaspolicymakers
haveattemptedtoconstraincertainendusesoffoundationmodels. Severalapproachesincludeadverse
event reporting, watermarking, and restricting access to models in limited ways. Consider providing
guidancetousersonhowtouseyourmodelsresponsiblyandopenlystatingthenormsyouhopewill
shape model use.
9.1 Model Documentation
When models, code or applications are released, whether openly or not, it is important that they are
documented thoroughly. Documentation should specify how to use the model, recommended and non-
recommendedusecases,potentialharms,stateorjustifydecisionsmadeduringtraining,andmore. Docu-
mentingmodelsisimportantnotjustforresponsibledevelopment,butalsotoenableotherdevelopersto
effectively build on a model. Models are not nearly as useful as artifacts if not properly documented. Model
cards(Mitchelletal.,2019)arewidelyadoptedstandardfordocumentingmodels. Severaltoolshavebeen
developed that support the creation of model cards42.
9.2 Reproducibility
Code to reproduce results are an important complement to other forms of documentation Kapoor et al.
(2023). Releasing assets that reproduce results mean that scientific claims can be verified, and that systems
canbeinterrogated,testedandauditedMissing,incomplete,orpoorlydocumentedcodehindersprogress.
Therearetoolsthathelpmakemodeltraining,inferenceandevaluationreproducible. AnacondaandDocker
makenecessaryenvironmentsanddependencieseasiertomanage. GoogleColabandJupyternotebooks
enable easily shareable code snippets and organized tutorials. The Language Model Evaluation Harness
provides a framework for prompting and testing generative language models on a large number of different
evaluation tasks (Gao et al., 2023).
9.3 Licensing
Licensing is a mechanism creating a legally enforceable agreement that governs use of artifacts. Licenses
withuserestrictionscanbeusedtolimittheabilityofcertaincategoriesofstakeholderstore-useoradaptthe
42https://huggingface.co/blog/model-cards
24Published in Transactions on Machine Learning Research (12/2024)
models (Contractor et al., 2022; Foundation, 2024). The MIT and Apache 2.0 licenses are the most commonly
used. ResponsibleAILicenses,includingBigScience’sOpenRAIL,haveseengrowingadoption. However
these also face criticism around how they pose challenges for well-intentioned actors and because their
enforceability remains an open question (Downing, 2023). While RAIL licenses that restrict end use of
models may prevent commercial entities from engaging in out-of-scope uses, they are better viewed as tools
for establishing norms rather than binding contracts.
9.4 Usage Monitoring
Someopenfoundationmodeldevelopersattempttomonitortheusageoftheirmodels,whetherbywater-
markingmodeloutputsorgatingaccesstothemodel. Thecheatsheetprovidesresourcesrelatedtousage
monitoring,includingexamplesofhowtowatermarkcontent,guidanceonappropriateuse,guidanceon
reportingadverseeventsassociatedwithmodeluse43,andwaystolimitsomeformsofaccesstomodels. Sev-
eral of these approaches have significant drawbacks: for example, there are no known robust watermarking
techniques for language models and there are limits to watermarking for image models (Kirchenbauer et al.,
2023;Saberietal.,2023). Aswithmanyofthesectionsabove,usagemonitoringremainsanareaofactive
research.
9.5 Recommendations
Models should be released with accompanying documentation andeasy-to-run code for training, evaluation
and inference. Document model thoroughly to the extent possible. These are critical to avoiding misuse and
harmsandenablingdeveloperstoeffectivelybuildonyourwork. Awelldocumentedenvironment,code,
and versions of the appropriate datasets.
Bethoughtfulaboutthetypeoflicensetouseforartifacts. Opensourceisatechnicaltermandstandardwith
awidelyaccepteddefinition(Initiative,2024). Ifthereisariskofmisusethenconsiderbehavioralrestrictions
from a standardized tool (McDuff et al., 2024).
Frameworksformonitoringandshapingmodelusagehavebecomemoreprevalentaspolicymakershave
attempted to constrain certain end uses of foundation models. Several approaches include adverse event
reporting, watermarking,and restrictingaccess tomodels in limitedways. Consider providingguidanceto
users on how to use your models responsibly and openly stating the norms you hope will shape model use.
9.6 Review
In this section we critically review the current state of resources for model release and monitoring, from our
survey.
Reproducibility, especially through executable code, benefits all parties. The research community has
benefited substantially from openness and transparency in how foundation model artifacts are documented
andreleased. Ashaveproprietarydeveloperswhobenefitfromtherapidprototypingandinnovationson
theirreleasedtechnicalsystems,propelledbytheopencommunity. Ourreviewofresourcessuggeststhat
oftenthemostwidelyadoptedtoolsarethosethatarenotjustwelldocumented/described,butalsothose
that are easy to run with executable code.
Usage monitoring remains challenging, and offers both advantages and disadvantages. Existing tools
such as watermarkingfor AI outputs (Kirchenbauer etal., 2023; Saberi et al., 2023)or model fingerprinting
(Xu et al., 2024) offer some degree of verification regarding the source of data or models. However, their
robustness to adversarial removal, or to detection, remain dubious—this is particularly true for text data.
This can result in over-confidence that these methods provide a panacea, and instead result in false positives.
Therearealsoopenquestionsonprivacyastotherightforindividualstoproducecontentwithoutattribution.
We hesitate to prescribe these nascent solutions broadly, without fully understanding the particular context
under consideration for the data, the model, and their potential uses and abuses.
43E.g. https://www.microsoft.com/en-us/photodna
25Published in Transactions on Machine Learning Research (12/2024)
Permissions and restrictions around model use should be more explicit about their data provenance, and
the other upstream ingredients which may impact use intentions, consent, and permissions. The ethical,
responsible or legal use of a model may depend on a series of upstream factors, beyond the final developers’
license. This could include the license of the datasets used for training, the terms of service attached to those
datasources(e.g. ifsomedataissyntheticallygenerated),orthelicenseofthebasemodel(iffinetuned).
Thenorms,bestpractices,andlegalrelevanceofeachofthesefactorsisevolving,jurisdiction-dependent,
andbeyondthescopeofthischeatsheet. However,wesuggestdevelopersdocumentthesefactorsintheir
model releases, beyond their own license, suchthat downstream developers and users can makeinformed
decisions.
10 Discussion
General-purposeAIsystemsrequirebothgeneral-purposeandcase-specificdevelopmenttools. General-
purposeAIsystemsareusedforanincreasinglybroad,andoftenunforeseen,rangeofapplications(Zhao
et al., 2023b; Schillaci, 2024; Longpre et al., 2024b). These include consumer-oriented uses including creative
composition,informationseeking,brainstorming,andreasoning,aswellasindustryapplicationsinsoftware
development,entertainment,law,medicine,andjournalism(Bommasanietal.,2021;Brighametal.,2024).
Across these uses, the common denominator is often the foundation model, while the system setting,
expectations,anduserbasevary. Supportingthesebroaduseswilllikelyrequireasuiteoftoolsinformed
by“naturalistic”usesineachsetting,tounderstandtherealrisksandshortcomingsofagivenapplication
(Lin et al., 2024; Kapoor et al., 2024). The tools compiled in this work are aimed at the foundation model
layer, but may miss other essential components of responsible development or evaluation at the system
or application level. For these reasons, we suggest these tools are a starting point, but not in themselves
sufficientforresponsibledevelopment—thatrequiresadeeperstudyoftheintendeduse. Forexample,in
medicine there arelikelyto be considerably different considerations regarding foundation model behavior
compared to those in creative tasks. Although the tools here can help, it is unlikely that they will address all
the nuanced aspects of different vertical applications.
RecommendationsacrossDevelopmentStages. Wehavesynthesizedrecommendationsforeachdevel-
opmentstage andherewe notesomecommonly recurringthemes. First, throughoutdatasourcing,model
training,andevaluationthereisalackoftransparency,documentation,andreproducibility. Startingdocu-
mentationearlyandmaintainingitthroughoutaprojectmakesthisprocesseasier. Reproducibility,while
not a substitute for documentation, provides a starting point for downstream developers to understand,
iterate,andimprovepriorpractices. Weespeciallyrecommendthatclosed-sourceevaluationsreleasetheir
evaluation scripts, to disambiguate the many evaluation details and settings that can complicate results. For
transparency,environmentalimpactmetricsfromdatacentersandconsumer-leveldashboardscouldprovide
more fine-grained information.
A common theme across development stages is also the dearth of tools for non-English, internationally
representativeandmulti-modalsystems. Especiallyfordatasourcing,thesetoolsarelacking. Inaddition,
datasets are often shaped by their availability and expedient collection processes. For instance, many text-to-
imagedatasetsrelyoncollectingcaptions, butlessefforthasbeeninvestedincollectinginterspersedtextand
images, with more complex and realistic relationships. This makes many datasets ill-fitting to the necessary
distribution, quality, and diversity of more specialized tasks. Synthetic data has demonstrated promise as a
way to fill some of these gaps.
Lastly, we recommend that evaluation procedures shift away from evaluating models toward evaluating
entiresystems,whichmayincludethesettingsofdeployment,input/outputfilters,otherguardrails,and
theuserinterface. Toolsforevaluationofmodels“in-the-field”aremuchlessmaturethanthoseforstatic
benchmark testing. We encourage practitioners to expand upon this type of testing. These systems are
deployedtointeractwithpeople,andothersoftwareincomplexenvironments. Theseinteractionsneedtobe
part of the evaluation cycle.
26Published in Transactions on Machine Learning Research (12/2024)
Acknowledgements
Stella Biderman, Hailey Schoelkopf, and Aviya Skowron’s work on this project was funded in part by a grant
from the Omidyar Network.
27Published in Transactions on Machine Learning Research (12/2024)
References
Nanotron. https://github.com/huggingface/nanotron , 2024.
David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O Alabi, Yanke Mao, Haonan
Gao,andAnnieEn-ShiunLee.Sib-200: Asimple,inclusive,andbigevaluationdatasetfortopicclassification
in 200+ languages and dialects. arXiv preprint arXiv:2309.07445 , 2023.
ArmenAghajanyan,LiliYu,AlexisConneau,Wei-NingHsu,KarenHambardzumyan,SusanZhang,Stephen
Roller,NamanGoyal,OmerLevy,andLukeZettlemoyer. Scalinglawsforgenerativemixed-modallanguage
models. In International Conference on Machine Learning , pp. 265–279. PMLR, 2023.
Stability AI. Stable audio tools. Github Repo. URL https://github.com/Stability-AI/stable-audio-
tools.
Alon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor Pryor, Luke Yoffe, Deepak Ramachandran, Lise Getoor,
JayPujara,andWilliamYangWang. FETA:Abenchmarkforfew-sampletasktransferinopen-domain
dialogue. InYoavGoldberg,ZornitsaKozareva,andYueZhang(eds.), Proceedingsofthe2022Conference
onEmpiricalMethodsinNaturalLanguageProcessing ,pp.10936–10953,AbuDhabi,UnitedArabEmirates,
December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.751. URL
https://aclanthology.org/2022.emnlp-main.751 .
AlonAlbalak,LiangmingPan,ColinRaffel,andWilliamYangWang. Efficientonlinedatamixingforlanguage
model pre-training. arXiv preprint arXiv:2312.02406 , 2023.
Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas
Muennighoff,BairuHou,LiangmingPan,HaewonJeong,etal. Asurveyondataselectionforlanguage
models. arXiv preprint arXiv:2402.16827 , 2024.
ZaidAlyafeai,MaraimMasoud,MustafaGhaleb,andMagedSAl-shaibani. Masader: Metadatasourcing
for arabic text and speech data resources. In Proceedings of the Thirteenth Language Resources and Evaluation
Conference , pp. 6340–6351, 2022.
Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh
Levy-Kramer,ConnorLeahy,LucasNestler,KipParker,MichaelPieler,ShivanshuPurohit,TriSongz,Wang
Phil, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language modeling in PyTorch, 8 2021.
URL https://www.github.com/eleutherai/gpt-neox .
LasseFWolffAnthony,BenjaminKanding,andRaghavendraSelvan. Carbontracker: Trackingandpredicting
the carbon footprint of training deep learning models. arXiv preprint arXiv:2007.03051 , 2020.
Quentin Anthony, Stella Biderman, and Hailey Schoelkopf. Transformer math 101. GitHub Repo, 2023. URL
blog.eleuther.ai/ .
Quentin Anthony, Hailey Schoelkopf, and Stella Biderman. The EleutherAI Model Training Cookbook.
GitHub Repo, 2024. URL https://github.com/EleutherAI/cookbook .
Anthropic. Challenges in evaluating ai systems, October 2023. URL https://www.anthropic.com/news/
evaluating-ai-systems .
Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku. https://www-cdn.anthropic.
com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf , 3 2024a. URL:
https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.
Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024b. URL https://www-cdn.anthropic.
com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf .
28Published in Transactions on Machine Learning Research (12/2024)
Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais,
Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: A massively-multilingual speech
corpus. In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri,
Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion
Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Twelfth Language Resources and Evaluation
Conference , pp. 4218–4222, Marseille, France, May 2020. European Language Resources Association. ISBN
979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.520 .
AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,KalyaniMarathe,Yonatan
Bitton,SamirGadre,ShioriSagawa,etal. Openflamingo: Anopen-sourceframeworkfortraininglarge
autoregressive vision-language models. arXiv preprint arXiv:2308.01390 , 2023.
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia
Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In The 3rd
Workshop on Mathematical Reasoning and AI at NeurIPS’23 , 2023.
Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh,
Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli. XLS-
R: Self-supervised Cross-lingual Speech Representation Learning at Scale. In Proc. Interspeech 2022 , pp.
2278–2282, 2022. doi: 10.21437/Interspeech.2022-143.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen,AnnaGoldie,AzaliaMirhoseini,CameronMcKinnon,CarolChen,CatherineOlsson,Christopher
Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie
Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt,
Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby,
RobinLarson,SamRinger,ScottJohnston,ShaunaKravec,SheerElShowk,StanislavFort,TameraLanham,
Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-
Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.
Constitutional AI: Harmlessness from AI Feedback, December 2022. URL http://arxiv.org/abs/2212.
08073. arXiv:2212.08073 [cs].
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image
encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pp. 1728–1738, 2021.
Jack Bandy and Nicholas Vincent. Addressing “documentation debt” in machine learning research: A
retrospective datasheet for bookcorpus. arXiv preprint arXiv:2105.05241 , 2021.
MicheleBanko,BrendonMacKeen,andLaurieRay. Aunifiedtaxonomyofharmfulcontent. InSeyiAkiwowo,
Bertie Vidgen, Vinodkumar Prabhakaran, and Zeerak Waseem (eds.), Proceedings of the Fourth Workshop on
Online Abuse and Harms , pp. 125–137, Online, November 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.alw-1.16. URL https://aclanthology.org/2020.alw-1.16 .
RomainBeaumont. Clipretrieval: Easilycomputeclipembeddingsandbuildaclipretrievalsystemwith
them. https://github.com/rom1504/clip-retrieval , 2022.
Stas Bekman. Machine learning engineering open book. GitHub Repo. URL https://github.com/stas00/
ml-engineering .
Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating
system bias and enabling better science. Transactions of the Association for Computational Linguistics , 6:
587–604, 2018. doi: 10.1162/tacl\_a\_00041. URL https://aclanthology.org/Q18-1041 .
Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan,
MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal. Pythia: Asuite
for analyzing large language models across training and scaling. In International Conference on Machine
Learning, pp. 2397–2430. PMLR, 2023.
29Published in Transactions on Machine Learning Research (12/2024)
Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu
Purohit, and Edward Raff. Emergent and predictable memorization in large language models. Advances in
Neural Information Processing Systems , 36, 2024a.
Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, LeoGao, JonathanTow, Baber Abbasi, Alham Fikri
Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, et al. Lessons from the trenches on
reproducible evaluation of language models. arXiv preprint arXiv:2405.14782 , 2024b.
Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornogra-
phy, and malignant stereotypes. arXiv preprint arXiv:2110.01963 , 2021.
SidBlack,StellaBiderman,EricHallahan,QuentinAnthony,LeoGao,LaurenceGolding,HoraceHe,Connor
Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model.
arXiv preprint arXiv:2204.06745 , 2022.
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,
Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video
diffusion models to large datasets. arXiv preprint arXiv:2311.15127 , 2023.
RishiBommasani,DrewAHudson, EhsanAdeli,RussAltman,SimranArora,SydneyvonArx,MichaelS
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258 , 2021.
RishiBommasani,KevinKlyman,ShayneLongpre,SayashKapoor,NestorMaslej,BettyXiong,DanielZhang,
and Percy Liang. The foundation model transparency index, 2023.
Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash Kapoor, Nestor Maslej, Arvind
Narayanan, and Percy Liang. Foundation model transparency reports. arXiv preprint arXiv:2402.16268 ,
2024.
NatalieGraceBrigham,ChongjiuGao,TadayoshiKohno,FranziskaRoesner,andNiloofarMireshghallah.
Breaking news: Case studies of generative ai’s use in journalism. arXiv preprint arXiv:2406.13706 , 2024.
TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,JoeTaylor,Troy
Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as
world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-
simulators .
R Brown, C Webber, and JG Koomey. Status and future directions of the energy star program. Energy, 5(27):
505–520, 2002.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan,PranavShyam, GirishSastry,AmandaAskell, SandhiniAgarwal,ArielHerbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,
SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners.
InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(eds.), AdvancesinNeuralInformation
ProcessingSystems ,volume33,pp.1877–1901.CurranAssociates,Inc.,2020. URL https://proceedings.
neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul
Scharre,ThomasZeitzoff,BobbyFilar,HyrumAnderson,HeatherRoff,GregoryC.Allen,JacobSteinhardt,
Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca
Crootof,OwainEvans,MichaelPage,JoannaBryson,RomanYampolskiy,andDarioAmodei. Themalicious
use of artificial intelligence: Forecasting, prevention, and mitigation, 2018.
Benjamin S. Bucknall and Shiri Dori-Hacohen. Current and near-term ai as a potential existential risk factor.
InProceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society , AIES ’22, pp. 119–129, New York,
NY,USA,2022.AssociationforComputingMachinery. ISBN9781450392471. doi: 10.1145/3514094.3534146.
URL https://doi.org/10.1145/3514094.3534146 .
30Published in Transactions on Machine Learning Research (12/2024)
LaurieBurchell,AlexandraBirch,NikolayBogoychev,andKennethHeafield. Anopendatasetandmodelfor
languageidentification.In Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume 2: Short Papers) . Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-
short.75. URL http://dx.doi.org/10.18653/v1/2023.acl-short.75 .
SamuelCahyawijaya,HolyLovenia, AlhamFikriAji, GentaIndraWinata,Bryan Wilie,FajriKoto,Rahmad
Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, et al. Nusacrowd: Open source
initiative for indonesian nlp resources.
Joseph Carlsmith. Is power-seeking ai an existential risk?, 2022.
YupengChang,XuWang,JindongWang,YuanWu,LinyiYang,KaijieZhu,HaoChen,XiaoyuanYi,Cunxiang
Wang,YidongWang,etal. Asurveyonevaluationoflargelanguagemodels. ACMTransactionsonIntelligent
Systems and Technology , 2023.
Carol Chen. Transformer inference arithmetic. https://kipp.ly/blog/transformer-inference-
arithmetic/ , 2022.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm: Scalinglanguagemodeling
with pathways. Journal of Machine Learning Research , 24(240):1–113, 2023.
Danish Contractor, Daniel McDuff, Julia Katherine Haines, Jenny Lee, Christopher Hines, Brent Hecht,
NicholasVincent,andHanlinLi. Behavioraluselicensingforresponsibleai. In Proceedingsofthe2022ACM
Conference on Fairness, Accountability, and Transparency , pp. 778–788, 2022.
EmiliaDavid.Aiimagetrainingdatasetfoundtoincludechildsexualabuseimagery. TheVerge ,December2023.
URL https://www.theverge.com/2023/12/20/24009418/generative-ai-image-laion-csam-google-
stability-stanford . 7:57 AM PST.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pp. 248–255, 2009. doi:
10.1109/CVPR.2009.5206848.
Jiawen Deng,JialeCheng,HaoSun, ZhexinZhang,andMinlieHuang. Towards safergenerativelanguage
models: A survey on safety risks, evaluations, and improvements, 2023.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of
quantized llms, 2023.
Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim,
Akihiro Nishi, Nanyun Peng, and Kai-Wei Chang. On measures of biases and harms in NLP. In Yulan
He, Heng Ji, SujianLi, Yang Liu, and Chua-Hui Chang (eds.), Findings of theAssociation for Computational
Linguistics: AACL-IJCNLP 2022 , pp. 246–267, Online only, November 2022. Association for Computational
Linguistics. URL https://aclanthology.org/2022.findings-aacl.24 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and
Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In
Proceedings of the 2021 ACM conference on fairness, accountability, and transparency , pp. 862–872, 2021.
MarkDingemanseandAndreasLiesenfeld. Fromtexttotalk: Harnessingconversationalcorporaforhumane
and diversity-aware language technology. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pp. 5614–5633, 2022.
RoelDobbe. Systemsafetyandartificialintelligence. In Proceedingsofthe2022ACMConferenceonFairness,
Accountability, and Transparency , pp. 1584–1584, 2022.
31Published in Transactions on Machine Learning Research (12/2024)
Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret
Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean
crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp.
1286–1305, 2021.
Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Cold
fusion: Collaborative descent for distributed multitask finetuning, 2023.
Kate Downing. Ai licensing can’t balance “open” with “responsible”. The Law Office of Kate Downing’s
Blog,2023. URL https://katedowninglaw.com/2023/07/13/ai-licensing-cant-balance-open-with-
responsible/ .
YannDubois,XuechenLi,RohanTaori,TianyiZhang,IshaanGulrajani,JimmyBa,CarlosGuestrin,Percy
Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from
human feedback. arXiv preprint arXiv:2305.14387 , 2023.
YanaiElazar,AkshitaBhagia,IanMagnusson,AbhilashaRavichander,DustinSchwenk,AlaneSuhr,Pete
Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, and Jesse Dodge.
What’s in my big data?, 2023.
The Free Software Foundation. What is free software?, 2024. URL https://web.archive.org/web/
20230306010437/https://www.gnu.org/philosophy/free-sw.en.html . Last accessed on 2024-02-20.
ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,XiawuZheng,
Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language
models. arXiv preprint arXiv:2306.13394 , 2023.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan
Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras,
Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu,
MehdiCherti, Ranjay Krishna, PangWeiKoh, Olga Saukh, AlexanderRatner, Shuran Song, Hannaneh
Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal
Shankar, and LudwigSchmidt. Datacomp: In search of the nextgeneration of multimodal datasets, 2023.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan
Marten,MitchellWortsman,DhrubaGhosh,JieyuZhang,etal. Datacomp: Insearchofthenextgeneration
of multimodal datasets. Advances in Neural Information Processing Systems , 36, 2024a.
Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin
Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et al. Language models scale reliably with over-
training and on downstream tasks. arXiv preprint arXiv:2403.08540 , 2024b.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace
He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027 , 2020.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa,
JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,EricTang,AnishThite,
BenWang,KevinWang,andAndyZou. Aframeworkforfew-shotlanguagemodelevaluation,122023.
URL https://zenodo.org/records/10256836 .
TimnitGebru,JamieMorgenstern,BrianaVecchione,JenniferWortmanVaughan,HannaWallach,HalDaumé
Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM , 64(12):86–92, 2021.
SamuelGehman,SuchinGururangan,MaartenSap,YejinChoi,andNoahA.Smith. RealToxicityPrompts:
Evaluating neural toxic degeneration in language models. In Trevor Cohn, Yulan He, and Yang Liu (eds.),
Findings of the Association for Computational Linguistics: EMNLP 2020 , pp. 3356–3369, Online, November
2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL
https://aclanthology.org/2020.findings-emnlp.301 .
32Published in Transactions on Machine Learning Research (12/2024)
SamuelGehman,SuchinGururangan,MaartenSap,YejinChoi,andNoahASmith. Realtoxicityprompts:
Evaluatingneuraltoxicdegenerationinlanguagemodels. In FindingsoftheAssociationforComputational
Linguistics: EMNLP 2020 , pp. 3356–3369, 2020b.
CharlesGoddard,ShamaneSiriwardhana,MalikehEhghaghi,LukeMeyers,VladKarpukhin,BrianBenedict,
Mark McQuade, and Jacob Solawetz. Arcee’s mergekit: A toolkit for merging large language models, 2024.
Aidan Gomez. Introducing command r+: A scalable llm built for business. https://txt.cohere.com/
command-r-plus-microsoft-azure/ ,42024. URL:https://txt.cohere.com/command-r-plus-microsoft-
azure/.
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word
vectors for 157 languages. In Proceedings of the International Conference on Language Resources and Evaluation
(LREC 2018) , 2018.
NeelGuha,JulianNyarko,DanielHo,ChristopherRé,AdamChilton,AlexChohlas-Wood,AustinPeters,
BrandonWaldon,DanielRockmore,DiegoZambrano,etal. Legalbench: Acollaborativelybuiltbenchmark
for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems ,
36, 2024.
SuchinGururangan,MitchellWortsman,SamirYitzhakGadre,AchalDave,MaciejKilian,WeijiaShi,Jean
Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi,
VaishaalShankar,andLudwigSchmidt. open_lm: aminimalbutperformativelanguagemodeling(lm)
repository, 2023. URL https://github.com/mlfoundations/open_lm/ . GitHub repository.
Laura Hanu and team Unitary. Detoxify, November 2020. URL https://github.com/unitaryai/detoxify .
ThomasHartvigsen,SaadiaGabriel,HamidPalangi,MaartenSap,DipankarRay,andEceKamar. Toxigen: A
large-scalemachine-generateddatasetforadversarialandimplicithatespeechdetection. arXivpreprint
arXiv:2203.09509 , 2022.
Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman,
Quentin Anthony, and Louis Castricato. trlX: A framework for large scalereinforcement learning from
humanfeedback. In Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing ,
pp. 8578–8595, Singapore, December 2023.Association for Computational Linguistics. doi: 10.18653/v1/
2023.emnlp-main.530. URL https://aclanthology.org/2023.emnlp-main.530 .
Luxi He, Mengzhou Xia, and Peter Henderson. What’s in your" safe" data?: Identifying benign data that
breaks safety. arXiv preprint arXiv:2404.01099 , 2024.
Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. Towards
thesystematicreportingoftheenergyandcarbonfootprintsofmachinelearning. TheJournalofMachine
Learning Research , 21(1):10039–10081, 2020.
Peter Henderson, Mark Krass, Lucia Zheng, Neel Guha, Christopher D Manning, Dan Jurafsky, and Daniel
Ho. Pileoflaw: Learningresponsibledatafilteringfromthelawanda256gbopen-sourcelegaldataset.
Advances in Neural Information Processing Systems , 35:29217–29234, 2022.
Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An overview of catastrophic ai risks, 2023.
JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya, TrevorCai, ElizaRutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal
large language models. arXiv preprint arXiv:2203.15556 , 2022.
Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. The dataset nutrition
label.Data Protection and Privacy , 12(12):1, 2020.
Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard, 2024.
33Published in Transactions on Machine Learning Research (12/2024)
GabrielIlharco,MitchellWortsman,RossWightman,CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,JohnMiller,HannanehHajishirzi,AliFarhadi,andLudwig
Schmidt. Openclip,July2021. URL https://doi.org/10.5281/zenodo.5143773 . Ifyouusethissoftware,
please cite it as below.
HakanInan,KartikeyaUpasani,JianfengChi,RashiRungta,KrithikaIyer,YuningMao,MichaelTontchev,
Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for
human-ai conversations. arXiv preprint arXiv:2312.06674 , 2023.
TheOpenSourceInitiative. Theopensourcedefinition,February2024. URL https://opensource.org/osd/ .
Matthew Jagielski. A note on interpreting canary exposure. arXiv preprint arXiv:2306.00133 , 2023.
Yacine Jernite. Training data transparency in ai: Tools, trends, and policy recommendations. In Hugging Face
Blog, 2023.
YacineJernite,HuuNguyen,StellaBiderman,AnnaRogers,MaraimMasoud,ValentinDanchev,Samson
Tan, Alexandra Sasha Luccioni, Nishant Subramani, Isaac Johnson, et al. Data governance in the age
of large-scale data-driven language technology. In Proceedings of the 2022 ACM Conference on Fairness,
Accountability, and Transparency , pp. 2206–2222, 2022.
Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan.
Swe-bench: Can languagemodelsresolve real-world githubissues? arXivpreprint arXiv:2310.06770 , 2023.
JacobKahn,MorganeRivière,WeiyiZheng,EvgenyKharitonov,QiantongXu,Pierre-EmmanuelMazaré,
JulienKaradayi,VitaliyLiptchinsky,RonanCollobert,ChristianFuegen,etal. Libri-light: Abenchmarkfor
asr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pp. 7669–7673. IEEE, 2020.
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,ScottGray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020.
Sayash Kapoor, Emily F. Cantrell, Kenny Peng, Thanh Hien Pham, Christopher A. Bail, Odd Erik Gundersen,
JakeM.Hofman,JessicaR.Hullman,MichaelA.Lones,MominM.Malik,PriyankaNanayakkara,RusselA.
Poldrack,InioluwaDeborah Raji, MichaelRoberts, Matthew J.Salganik, Marta Serra-Garcia, BrandonM
Stewart,GillesVandewiele,andArvindNarayanan. Reforms: Reportingstandardsformachinelearning
based science. ArXiv, abs/2308.07832, 2023. URL https://arxiv.org/abs/2308.07832 .
SayashKapoor,RishiBommasani,KevinKlyman,ShayneLongpre,AshwinRamaswami,PeterCihon,Aspen
Hopkins,KevinBankston,StellaBiderman,MirandaBogen,etal. Onthesocietalimpactofopenfoundation
models. arXiv preprint arXiv:2403.07918 , 2024.
Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh.
Prismaticvlms: Investigatingthedesignspaceofvisually-conditionedlanguagemodels. arXivpreprint
arXiv:2402.07865 , 2024.
AmirHosseinKargaran,AyyoobImani,FrançoisYvon,andHinrichSchütze. Glotlid: Languageidentification
for low-resource languages, 2023.
Andrej Karpathy. nanogpt. GitHub Repo, 2023. URL https://github.com/karpathy/nanoGPT .
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen,
GrushaPrasad,AmanpreetSingh,PratikRingshia,etal. Dynabench: Rethinkingbenchmarkinginnlp.
arXiv preprint arXiv:2104.14337 , 2021.
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark
for large language models. arXiv preprint arXiv:2301.10226 , 2023.
34Published in Transactions on Machine Learning Research (12/2024)
WalterKlöpffer. Lifecycleassessment: Fromthebeginningtothecurrentstate. EnvironmentalScienceand
Pollution Research , 4:223–228, 1997.
Kevin Klyman. Acceptable use policies for foundation models: Considerations for policymakers and devel-
opers. StanfordCenterforResearchonFoundationModels,apr2024. URL https://crfm.stanford.edu/
2024/04/08/aups.html .
Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Muñoz
Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. The stack: 3 tb of permissively licensed
source code. Transactions on Machine Learning Research , 2022.
AntonKorinekandJaiVipra. Marketconcentrationimplicationsoffoundationmodels: Theinvisiblehandof
chatgpt. 2023.
JuliaKreutzer,IsaacCaswell,LisaWang,AhsanWahab,DaanvanEsch,NasanbayarUlzii-Orshikh,Allahsera
Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, et al. Quality at a glance: An audit of
web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics , 10:50–72,
2022.
SnehaKudugunta,IsaacRayburnCaswell,BiaoZhang,XavierGarcia,DerrickXin,AdityaKusupati,Romi
Stella, Ankur Bapna, and Orhan Firat. Madlad-400: A multilingual and document-level large audited
dataset. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track ,
2023.
Veronika Laippala, Anna Salmela, Samuel Rönnqvist, Alham Fikri Aji, Li-Hsin Chang, Asma Dhifallah,
Larissa Goulart, Henna Kortelainen, Marc Pàmies, Deise Prina Dutra, et al. Towards better structured
and less noisy web data: Oscar with register annotations. In Proceedings of the Eighth Workshop on Noisy
User-generated Text (W-NUT 2022) , pp. 215–221, 2022.
S Lakatos. A revealing picture: Ai-generated ‘undressing’images move from niche pornography discussion
forums to a scaled and monetized online business. Technical report, Technical report, Graphika, Dec 2023.
URL https://public-assets. graphika ..., 2023a.
SantiagoLakatos. ARevealingPicture: AI-Generated‘Undressing’ImagesMovefromNichePornography
DiscussionForumsto aScaled andMonetized OnlineBusiness. Technical report, December2023b. URL
https://public-assets.graphika.com/reports/graphika-report-a-revealing-picture.pdf .
Nathan Lambert,Valentina Pyatkin, JacobMorrison, LJ Miranda,Bill Yuchen Lin,Khyathi Chandu, Nouha
Dziri,SachinKumar,TomZick,YejinChoi,etal. Rewardbench: Evaluatingrewardmodelsforlanguage
modeling. arXiv preprint arXiv:2403.13787 , 2024.
Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven
Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, Jörg Fro-
hberg,MarioŠaško,QuentinLhoest,AngelinaMcMillan-Major,GerardDupont,StellaBiderman,Anna
Rogers,LoubnaBenallal,FrancescoDeToni,GiadaPistilli,OlivierNguyen,SomaiehNikpoor,Maraim
Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian
Nagel, Leon Weber, Manuel Muñoz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak,
Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron
Gokaslan,ShamikBose,DavidAdelani,LongPhan,HieuTran,IanYu,SuhasPai,JennyChim,Violette
Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Alexandra Luccioni, and Yacine Jernite. The bigscience
roots corpus: A 1.6tb composite multilingual dataset. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
K.Cho,andA.Oh(eds.), AdvancesinNeuralInformationProcessingSystems ,volume35,pp.31809–31826.
CurranAssociates,Inc.,2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
ce9e92e3de2372a4b93353eb7f3dc0bd-Paper-Datasets_and_Benchmarks.pdf .
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas
Wang,SiddharthKaramcheti,AlexanderRush,DouweKiela,etal. Obelics: Anopenweb-scalefiltered
dataset of interleaved image-text documents. Advances in Neural Information Processing Systems , 36, 2024a.
35Published in Transactions on Machine Learning Research (12/2024)
Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-
language models? arXiv preprint arXiv:2405.02246 , 2024b.
Éric Le Ferrand, Steven Bird, and Laurent Besacier. Learning from failure: Data capture in an australian
aboriginalcommunity. In Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume 1: Long Papers) , pp. 4988–4998, 2022.
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,
and Nicholas Carlini. Deduplicating training data makes language models better. In Smaranda Muresan,
PreslavNakov,andAlineVillavicencio(eds.), Proceedingsofthe60thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1: LongPapers) ,pp.8424–8445,Dublin,Ireland, May2022.Association
for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/
2022.acl-long.577 .
Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-
Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, et al. Evaluating human-language model interaction.
Transactions on Machine Learning Research , 2023a.
Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang,
DeepakNarayanan,HannahBenitaTeufel,MarcoBellagente,etal. Holisticevaluationoftext-to-image
models. arXiv preprint arXiv:2311.04287 , 2023b.
Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. A new
generation of perspective api: Efficient multilingual character-level transformers. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 3197–3207, 2022.
Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil,
JulienChaumond,MariamaDrame,JulienPlu,LewisTunstall,etal. Datasets: Acommunitylibraryfor
natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations , pp. 175–184, 2021a.
Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil,JulienChaumond,MariamaDrame,JulienPlu,LewisTunstall,JoeDavison,MarioŠaško,Gunjan
Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry,
AngelinaMcMillan-Major,PhilippSchmid,SylvainGugger,ClémentDelangue,ThéoMatussière,Lysandre
Debut,StasBekman,PierricCistac,ThibaultGoehringer,VictorMustar,FrançoisLagunas,AlexanderRush,
andThomasWolf. Datasets: Acommunitylibraryfornaturallanguageprocessing. In Proceedingsofthe
2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing: SystemDemonstrations ,pp.175–184,
OnlineandPuntaCana,DominicanRepublic,November2021b.AssociationforComputationalLinguistics.
URL https://aclanthology.org/2021.emnlp-demo.21 .
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023a.
Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing
Xu, Xu Sun, etal. M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv
preprint arXiv:2306.04387 , 2023b.
Pengfei Li, Jianyi Yang, Mohammad A Islam, and Shaolei Ren. Making ai less" thirsty": Uncovering and
addressing the secret water footprint of ai models. arXiv preprint arXiv:2304.03271 , 2023c.
PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,YianZhang,
DeepakNarayanan,YuhuaiWu,AnanyaKumar,BenjaminNewman,BinhangYuan,BobbyYan,CeZhang,
Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson,
EricZelikman,EsinDurmus,FaisalLadhak,FriedaRong,HongyuRen,HuaxiuYao,JueWang,Keshav
Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri
Chatterji,Omar Khattab,Peter Henderson,Qian Huang,Ryan Chi,Sang MichaelXie,Shibani Santurkar,
SuryaGanguli,TatsunoriHashimoto,ThomasIcard,TianyiZhang,VishravChaudhary,WilliamWang,
Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2023.
36Published in Transactions on Machine Learning Research (12/2024)
Q Vera Liao and Ziang Xiao. Rethinking model evaluation as narrowing the socio-technical gap. arXiv
preprint arXiv:2306.03100 , 2023.
Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin,
Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from
real users in the wild. arXiv preprint arXiv:2406.04770 , 2024.
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th
European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pp. 740–755. Springer,
2014.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning.
arXiv preprint arXiv:2310.03744 , 2023a.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advancesinneural
information processing systems , 36, 2024a.
Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, and Hannaneh Hajishirzi. Infini-gram: Scaling
unbounded n-gram language models to a trillion tokens. arXiv preprint arXiv:2401.17377 , 2024b.
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi
Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for language models. arXiv
preprint arXiv:2404.07503 , 2024c.
Yuan Liu,Haodong Duan, Yuanhan Zhang,Bo Li, SongyangZhang, Wangbo Zhao,Yike Yuan, JiaqiWang,
Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint
arXiv:2307.06281 , 2023b.
KyleLo,LucyLuWang,MarkNeumann,RodneyKinney,andDanielSWeld. S2orc: Thesemanticscholar
openresearchcorpus. In Proceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics ,
pp. 4969–4983, 2020.
ShayneLongpre,MarcusStorm,andRishiShah. Lethalautonomousweaponssystems&artificialintelligence:
Trends, challenges, and policies. MIT Science Policy Review , 3(1):47–56, 2022.
ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay,DennyZhou,QuocVLe,Barret
Zoph,JasonWei,etal. Theflancollection: Designingdataandmethodsforeffectiveinstructiontuning.
arXiv preprint arXiv:2301.13688 , 2023a.
Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon,
Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. The data provenance initiative:
A large scale audit of dataset licensing & attribution in ai. arXiv preprint arXiv:2310.16787 , 2023b.
Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane Blili-
Hamelin, Yangsibo Huang, Aviya Skowron, Zheng-Xin Yong, Suhas Kotha, et al. A safe harbor for ai
evaluation and red teaming. arXiv preprint arXiv:2403.04893 , 2024a.
Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, Nayan
Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, et al. Consent in crisis: the rapid decline of the ai
data commons. arXiv preprint arXiv:2407.14933 , 2024b.
ShayneLongpre,RobertMahari,NaanaObeng-Marnu,WilliamBrannon,TobinSouth,KatyGero,Sandy
Pentland,andJadKabbara. Dataauthenticity,consent,&provenanceforaiareallbroken: whatwillittake
to fix them? arXiv preprint arXiv:2404.12691 , 2024c.
Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi,
AoTang,DmytroPykhtar,JiaweiLiu,YuxiangWei,etal. Starcoder2andthestackv2: Thenextgeneration.
arXiv preprint arXiv:2402.19173 , 2024.
37Published in Transactions on Machine Learning Research (12/2024)
Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of
bloom, a 176b parameter language model. Journal of Machine Learning Research , 24(253), 2023.
SourabMangrulkar,SylvainGugger,LysandreDebut,YounesBelkada,SayakPaul,andBenjaminBossan.
Peft: State-of-the-art parameter-efficientfine-tuningmethods. https://github.com/huggingface/peft ,
2022.
Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging, 2022.
Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel
Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. Harmbench: A standardized evaluation
framework for automated red teaming and robust refusal, 2024.
Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karlaš, William A Gaviria Rojas, Sudnya Diamos, Greg
Diamos,LynnHe,AliciaParrish,HannahRoseKirk,JessicaQuaye,CharviRastogi,DouweKiela,David
Jurado, David Kanter, Rafael Mosquera, Will Cukierski, Juan Ciro, Lora Aroyo, Bilge Acun, Lingjiao Chen,
Mehul SmritiRaje, Max Bartolo, SabriEyuboglu, Amirata Ghorbani,Emmett Daniel Goodman, Addison
Howard,OanaInel,TariqKane,ChristineKirkpatrick,D.Sculley,Tzu-ShengKuo,JonasMueller,Tristan
Thrush, Joaquin Vanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen
Paritosh, Ce Zhang, James Y. Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson, and
Vijay JanapaReddi. Dataperf: Benchmarksfor data-centricAIdevelopment. In Thirty-seventh Conferenceon
Neural Information Processing Systems Datasets and Benchmarks Track , 2023. URL https://openreview.net/
forum?id=LaFKTgrZMG .
Daniel McDuff, Tim Korjakow, Scott Cambo, Jesse Josua Benjamin, Jenny Lee, Yacine Jernite, Carlos Muñoz
Ferrandis,AaronGokaslan,AlekTarkowski,JosephLindley,etal. Onthestandardizationofbehavioral
use clauses and their adoption for responsible licensing of ai. arXiv preprint arXiv:2402.05979 , 2024.
Kristina McElheran, J Frank Li, Erik Brynjolfsson, Zachary Kroff, Emin Dinlersoz, Lucia Foster, and Nikolas
Zolas. Ai adoption in america: Who, what, and where. Journal of Economics & Management Strategy , 2024.
BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,Dhruti
Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal
llm pre-training. arXiv preprint arXiv:2403.09611 , 2024.
Angelina McMillan-Major, Zaid Alyafeai, Stella Biderman, Kimbo Chen, Francesco De Toni, Gérard Dupont,
Hady Elsahar, Chris Emezue, Alham Fikri Aji, Suzana Ilić, et al. Documenting geographically and
contextually diverse data sources: The bigscience catalogue of language data and resources. arXiv preprint
arXiv:2201.10066 , 2022.
SewonMin,SuchinGururangan,EricWallace,WeijiaShi,HannanehHajishirzi,NoahASmith,andLuke
Zettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore. In NeurIPS 2023
Workshop on Distribution Shifts: New Frontiers with Foundation Models , 2023.
MargaretMitchell,SimoneWu,AndrewZaldivar,ParkerBarnes,LucyVasserman,BenHutchinson,Elena
Spitzer,Inioluwa DeborahRaji,and TimnitGebru. Modelcardsfor modelreporting. In Proceedingsof the
conference on fairness, accountability, and transparency , pp. 220–229, 2019.
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh,
XiangruTang,LeandrovonWerra,andShayneLongpre. Octopack: Instructiontuningcodelargelanguage
models. arXiv preprint arXiv:2308.07124 , 2023a.
NiklasMuennighoff,AlexanderMRush,BoazBarak,TevenLeScao,AleksandraPiktus,NouamaneTazi,
Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. arXiv preprint
arXiv:2305.16264 , 2023b.
ArshaNagrani,JoonSonChung,andAndrewZisserman. VoxCeleb: ALarge-ScaleSpeakerIdentification
Dataset. In Proc. Interspeech 2017 , pp. 2616–2620, 2017. doi: 10.21437/Interspeech.2017-950.
38Published in Transactions on Machine Learning Research (12/2024)
ArvindNarayananandSayashKapoor. Aisafetyisnotamodelproperty. https://www.aisnakeoil.com/
p/ai-safety-is-not-a-model-property , March 2024. Accessed: YYYY-MM-DD.
Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A
Rossi,andThienHuuNguyen. Culturax: Acleaned,enormous,andmultilingualdatasetforlargelanguage
models in 167 languages. arXiv preprint arXiv:2309.09400 , 2023.
JoelNiklaus,VetonMatoshi,MatthiasStürmer,IliasChalkidis,andDanielEHo. Multilegalpile: A689gb
multilingual legal corpus. arXiv preprint arXiv:2306.02069 , 2023.
Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. Jailbreaking attack against multimodal
large language model. arXiv preprint arXiv:2402.02309 , 2024.
Akintunde Oladipo, Mofetoluwa Adeyemi, Orevaoghene Ahia, Abraham Owodunni, Odunayo Ogundepo,
DavidAdelani,andJimmyLin. Betterqualitypre-trainingdataandt5modelsforAfricanlanguages. In
Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pp. 158–168, Singapore, December 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.emnlp-main.11. URL https://aclanthology.org/2023.emnlp-main.
11.
OpenAccess-AI-Collective. Axolotl. GitHubRepo. URL https://github.com/OpenAccess-AI-Collective/
axolotl.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. arXiv preprint arXiv:2203.02155 , 2022. URL https://arxiv.org/abs/2203.02155 .
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on
publicdomainaudiobooks. In 2015IEEEinternationalconferenceonacoustics,speechandsignalprocessing
(ICASSP) , pp. 5206–5210. IEEE, 2015.
AliciaParrish,AngelicaChen,NikitaNangia,VishakhPadmakumar,JasonPhang,JanaThompson,PhuMon
Htut, andSamuelBowman. Bbq: A hand-builtbias benchmarkfor questionanswering. In Findings ofthe
Association for Computational Linguistics: ACL 2022 , pp. 2086–2105, 2022.
Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of
high-quality mathematical web text. arXiv preprint arXiv:2310.06786 , 2023.
DavidPatterson,JosephGonzalez,QuocLe,ChenLiang,Lluis-MiquelMunguia,DanielRothchild,David
So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint
arXiv:2104.10350 , 2021.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza
Alobeidli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay. Therefinedwebdatasetforfalconllm:
outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 , 2023.
Guilherme Penedo, Alessandro Cappelli, Thomas Wolf, and Mario Sasko. Datatrove: large scale data
processing, 2024. URL https://github.com/huggingface/datatrove .
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:
Grounding multimodal large language models to the world, 2023.
AleksandraPiktus,ChristopherAkiki,PauloVillegas,HugoLaurençon,GérardDupont,AlexandraSasha
Luccioni,YacineJernite,andAnnaRogers. Therootssearchtool: Datatransparencyforllms. arXivpreprint
arXiv:2302.14035 , 2023.
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: A Large-
Scale Multilingual Dataset for Speech Research. In Proc. Interspeech 2020 , pp. 2757–2761, 2020. doi:
10.21437/Interspeech.2020-2826.
39Published in Transactions on Machine Learning Research (12/2024)
Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-
tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint
arXiv:2310.03693 , 2023.
Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual
adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 38, pp. 21527–21536, 2024.
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust
speechrecognitionvialarge-scaleweaksupervision. In InternationalConferenceonMachineLearning ,pp.
28492–28518. PMLR, 2023.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint
arXiv:2305.18290 , 2023.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. The
Journal of Machine Learning Research , 21(1):5485–5551, 2020.
Inioluwa Deborah Raji and Roel Dobbe. Concrete problems in ai safety, revisited. arXiv preprint
arXiv:2401.10899 , 2023.
TraianRebedea,RazvanDinu,MakeshSreedhar,ChristopherParisien,andJonathanCohen. Nemoguardrails:
Atoolkitforcontrollableandsafellmapplicationswithprogrammablerails. arXivpreprintarXiv:2310.10501 ,
2023.
Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest:
A test suite for identifying exaggerated safety behaviours in large language models, 2024.
MehrdadSaberi,VinuSankarSadasivan,KeivanRezaei,AounonKumar,AtoosaChegini,WenxiaoWang,
andSoheilFeizi. Robustnessofai-imagedetectors: Fundamentallimitsandpracticalattacks. arXivpreprint
arXiv:2310.00076 , 2023.
Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo.
“Everyone wants to do the model work, not the data work”: Data cascades in high-stakes AI. In CHI,
CHI ’21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi:
10.1145/3411764.3445518. URL https://doi.org/10.1145/3411764.3445518 .
RamonSanabria,NikolayBogoychev,NinaMarkl,AndreaCarmantini,OndrejKlejch,andPeterBell. The
edinburgh international accents of english corpus: Towards the democratization of english asr. In ICASSP
2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 1–5. IEEE,
2023.
VictorSanh,AlbertWebson,ColinRaffel,StephenH.Bach,LintangSutawika,ZaidAlyafeai,AntoineChaffin,
Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task
generalization. ICLR 2022 , 2021. URL https://arxiv.org/abs/2110.08207 .
TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlić,DanielHesslow,RomanCastagné,
Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access
multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a
mirage? Advances in Neural Information Processing Systems , 36, 2024.
ZacharySchillaci. Llmadoptiontrendsandassociatedrisks. In LargeLanguageModelsinCybersecurity: Threats,
Exposure and Mitigation , pp. 121–128. Springer Nature Switzerland Cham, 2024.
40Published in Transactions on Machine Learning Research (12/2024)
Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan
Wilson, Sorelle Friedler, and Sasha Luccioni. Codecarbon: estimate and track carbon emissions from
machine learning computing. Cited on, pp. 20, 2021.
ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,Theo
Coombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,SrivatsaKundurthy,
KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. Laion-5b: Anopenlarge-scale
dataset for training next generation image-text models, 2022.
Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. Communications of the ACM , 63(12):
54–63, 2020.
Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attacks
on multi-modal language models. In The Twelfth International Conference on Learning Representations , 2023.
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and
Luke Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789 ,
2023.
MohammadShoeybi, MostofaPatwary, RaulPuri, PatrickLeGresley, JaredCasper, andBryanCatanzaro.
Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.
RileySimmons-Edler,RyanBadman,ShayneLongpre,andKanakaRajan. Ai-poweredautonomousweapons
risk geopolitical instability and threaten ai research. arXiv preprint arXiv:2405.01859 , 2024.
ShivalikaSingh,FreddieVargus,DanielDsouza,BörjeFKarlsson,AbinayaMahendiran,Wei-YinKo,Herumb
Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al. Aya dataset: An open-access collection
for multilingual instruction tuning. arXiv preprint arXiv:2402.06619 , 2024.
Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. “i’m sorry to
hear that”: Finding new biasesin language models with aholistic descriptor dataset. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing , pp. 9180–9211, 2022a.
ShadenSmith,MostofaPatwary,BrandonNorick,PatrickLeGresley,SamyamRajbhandari,JaredCasper,Zhun
Liu,ShrimaiPrabhumoye,GeorgeZerveas,VijayKorthikanti,EltonZhang,RewonChild,RezaYazdani
Aminabadi,JulieBernauer,XiaSong,MohammadShoeybi,YuxiongHe,MichaelHouston,SaurabhTiwary,
and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale
generative language model, 2022b.
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben
Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin
Kumar,LiLucy,XinxiLyu,IanMagnusson,JacobMorrison,NiklasMuennighoff,AakankshaNaik,Crystal
Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant
Subramani, Oyvind Tafjord, Evan Pete Walsh, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer,
Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: An Open Corpus of Three Trillion Tokens
for Language Model Pretraining Research. Allen Institute for AI, Tech. Rep , 2023.
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin,
KhyathiChandu,JenniferDumas,YanaiElazar,etal. Dolma: anopencorpusofthreetrilliontokensfor
language model pretraining research. arXiv preprint arXiv:2402.00159 , 2024.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
AdamR.Brown,AdamSantoro,AdityaGupta,AdriàGarriga-Alonso,AgnieszkaKluska,AitorLewkowycz,
AkshatAgarwal,AletheaPower,AlexRay,AlexWarstadt,AlexanderW.Kocurek,AliSafaya,AliTazarv,
Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone,
AmeetRahane,AnantharamanS.Iyer,AndersAndreassen,AndreaMadotto,AndreaSantilli,Andreas
Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh
Vuong,AnimeshGupta,AnnaGottardi,AntonioNorelli,AnuVenkatesh,ArashGholamidavoodi,Arfa
41Published in Transactions on Machine Learning Research (12/2024)
Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia
Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,
Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci,
Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick
Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu,
ChrisCallison-Burch,ChrisWaites,ChristianVoigt,ChristopherD.Manning,ChristopherPotts,Cindy
Ramirez,ClaraE.Rivera,ClemenciaSiro,ColinRaffel,CourtneyAshcraft,CristinaGarbacea,DamienSileo,
Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy,
DanielMoseguíGonzález,DaniellePerszyk,DannyHernandez,DanqiChen,DaphneIppolito,DarGilboa,
David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko,
Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho
Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal,
EleanorHagerman,ElizabethBarnes,ElizabethDonoway,ElliePavlick,EmanueleRodola,EmmaLam,
Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim,
EuniceEngefuManyasi,EvgeniiZheltonozhskii,FanyueXia,FatemehSiar,FernandoMartínez-Plumed,
FrancescaHappé, FrancoisChollet, FriedaRong, GauravMishra, GentaIndraWinata, GerarddeMelo,
GermánKruszewski,GiambattistaParascandolo,GiorgioMariani,GloriaWang,GonzaloJaimovitch-López,
Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh
Mehta,HaydenBogar,HenryShevlin,HinrichSchütze,HiromuYakura,HongmingZhang,HughMee
Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee,
JaimeFernándezFisac,JamesB.Simon,JamesKoppel,JamesZheng,JamesZou,JanKocoń,JanaThompson,
Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason
Yosinski,JekaterinaNovikova,JelleBosscher,JenniferMarsh,JeremyKim,JeroenTaal,JesseEngel,Jesujoba
Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis,
JonathanBatchelder,JonathanBerant,JörgFrohberg,JosRozen,JoseHernandez-Orallo,JosephBoudeman,
Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen
Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole,
Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar,
Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia
Contreras-Ochando, Louis-PhilippeMorency,Luca Moschella,Lucas Lam,LucyNoble, LudwigSchmidt,
LuhengHe,LuisOliverosColón,LukeMetz,LütfiKeremŞenel,MaartenBosma,MaartenSap,Maartje
ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco
Maru,MariaJoseRamírezQuintana,MarieTolkiehn,MarioGiulianelli,MarthaLewis,MartinPotthast,
Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud,
MelvinMcElrath,MichaelA.Yee,MichaelCohen,MichaelGu,MichaelIvanitskiy,MichaelStarritt,Michael
Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu,
MiracSuzgun,MitchWalker,MoTiwari,MohitBansal,MoinAminnaseri,MorGeva,MozhdehGheini,
Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron,
Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff,
NitishShirishKeskar,NivedithaS.Iyer,NoahConstant,NoahFiedel,NuanWen,OliverZhang,OmarAgha,
OmarElbaghdadi, OmerLevy,OwainEvans, PabloAntonioMorenoCasares, ParthDoshi, PascaleFung,
PaulPuLiang,PaulVicol,PegahAlipoormolabashi,PeiyuanLiao,PercyLiang,PeterChang,PeterEckersley,
PhuMon Htut,Pinyu Hwang,Piotr Miłkowski,Piyush Patil,Pouya Pezeshkpour,Priti Oli,Qiaozhu Mei,
QingLyu,QinlangChen,RabinBanjade,RachelEttaRudolph,RaeferGabriel,RahelHabacker,Ramon
Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers,
RobertFrank,RohanSikand,RomanNovak,RomanSitelew,RonanLeBras,RosanneLiu,RowanJacobs,
RuiZhang,RuslanSalakhutdinov,RyanChi,RyanLee,RyanStovall,RyanTeehan,RylanYang,SahibSingh,
SaifM. Mohammad, SajantAnand, SamDillavou, SamShleifer, SamWiseman, Samuel Gruetter, SamuelR.
Bowman,SamuelS.Schoenholz,SanghyunHan,SanjeevKwatra,SarahA.Rous,SarikGhazarian,Sayan
Ghosh,SeanCasey,SebastianBischoff,SebastianGehrmann,SebastianSchuster,SepidehSadeghi,Shadi
Hamdan,SharonZhou,ShashankSrivastava,SherryShi,ShikharSingh,ShimaAsaadi,ShixiangShaneGu,
Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon
Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha
Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad,
42Published in Transactions on Machine Learning Research (12/2024)
StevenT.Piantadosi,StuartM.Shieber,SummerMisherghi,SvetlanaKiritchenko,SwaroopMishra,Tal
Linzen,TalSchuster,TaoLi,TaoYu,TariqAli,TatsuHashimoto,Te-LinWu,ThéoDesbordes,Theodore
Rothschild,ThomasPhan,TianleWang,TiberiusNkinyili,TimoSchick,TimofeiKornev,TitusTunduny,
Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant
Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh
Padmakumar,VivekSrikumar,WilliamFedus,WilliamSaunders,WilliamZhang,WoutVossen,XiangRen,
Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,
YasamanBahri,YejinChoi,YichiYang,YidingHao,YifuChen,YonatanBelinkov,YuHou,YufangHou,
Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the
imitation game: Quantifying and extrapolating the capabilities of language models, 2023.
George Stoica, Daniel Bolya, Jakob Bjorner, Pratik Ramesh, Taylor Hearn, and Judy Hoffman. Zipit! merging
models from different tasks without training, 2024.
PedroJavierOrtizSuárez,BenoîtSagot,andLaurentRomary. Asynchronouspipelineforprocessinghuge
corpora on medium to low resource infrastructures. In 7th Workshop on the Challenges in the Management of
Large Corpora (CMLC-7) . Leibniz-Institut für Deutsche Sprache, 2019.
LichaoSun,YueHuang,HaoranWang,SiyuanWu,QihuiZhang,YuanLi,ChujieGao,YixinHuang,Wenhan
Lyu,YixuanZhang,XinerLi,ZhengliangLiu,YixinLiu,YijueWang,ZhikunZhang,BertieVidgen,Bhavya
Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji,
Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal,
JamesZou, JianPei, JianLiu, JianfengGao,JiaweiHan, JieyuZhao,Jiliang Tang, JindongWang, Joaquin
Vanschoren,JohnMitchell,KaiShu,KaidiXu,Kai-WeiChang,LifangHe,LifuHuang,MichaelBackes,
NeilZhenqiangGong,PhilipS.Yu,Pin-YuChen,QuanquanGu,RanXu,RexYing,ShuiwangJi,Suman
Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang,
XingXie,XunChen,XuyuWang,YanLiu,YanfangYe,YinzhiCao,YongChen,andYueZhao. Trustllm:
Trustworthiness in large language models, 2024.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,
Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal
models. arXiv preprint arXiv:2312.11805 , 2023.
DavidThiel,MelissaStroebel,andRebeccaPortnoff. GenerativeMLandCSAM:ImplicationsandMitigations.
2023a. doi: 10.25740/jv206yg3793. URL https://purl.stanford.edu/jv206yg3793 .
David Thiel, Melissa Stroebel, and Rebecca Portnoff. Generative ml and csam: Implications and mitigations,
2023b.
JörgTiedemann. Paralleldata,toolsandinterfacesinopus. In Lrec,volume2012,pp.2214–2218.Citeseer,
2012.
Together AI. Redpajama-data-v2: An open dataset with 30 trillion tokens for training large language models.
Blog post on Together AI, Oct 2023. URL https://www.together.ai/blog/redpajama-data-v2 .
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundation
language models. arXiv preprint arXiv:2302.13971 , 2023.
AhmetÜstün,ViraatAryabumi,Zheng-XinYong,Wei-YinKo,DanielD’souza,GbemilekeOnilude,Neel
Bhandari,ShivalikaSingh,Hui-LeeOoi,AmrKayid,etal. Ayamodel: Aninstructionfinetunedopen-access
multilingual language model. arXiv preprint arXiv:2402.07827 , 2024.
Jörgen Valk and Tanel Alumäe. Voxlingua107: a dataset for spoken language recognition. In 2021 IEEE
Spoken Language Technology Workshop (SLT) , pp. 652–658. IEEE, 2021.
BertieVidgen,AlexHarris,DongNguyen,RebekahTromble,ScottHale,andHelenMargetts. Challengesand
frontiers in abusive content detection. In Sarah T. Roberts, Joel Tetreault, Vinodkumar Prabhakaran, and
43Published in Transactions on Machine Learning Research (12/2024)
ZeerakWaseem(eds.), ProceedingsoftheThirdWorkshoponAbusiveLanguageOnline ,pp.80–93,Florence,
Italy,August2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/W19-3509. URL https:
//aclanthology.org/W19-3509 .
Bertie Vidgen, Hannah Rose Kirk, Rebecca Qian, Nino Scherrer, Anand Kannappan, Scott A Hale, and Paul
Röttger. Simplesafetytests: atest suiteforidentifying criticalsafetyrisksin largelanguagemodels. arXiv
preprint arXiv:2311.08370 , 2023.
Jai Vipra and Anton Korinek. Market concentration implications of foundation models: The invisible
hand of chatgpt. The Brookings Institution , 2023. URL https://www.brookings.edu/articles/market-
concentration-implications-of-foundation-models-the-invisible-hand-of-chatgpt .
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong,
Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan
Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. Decodingtrust: A comprehensive assessment of
trustworthiness in gpt models, 2024.
ChanghanWang,MorganeRiviere,AnnLee,AnneWu,ChaitanyaTalnikar,DanielHaziza,MaryWilliamson,
JuanPino,andEmmanuelDupoux. Voxpopuli: Alarge-scalemultilingualspeechcorpusforrepresentation
learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390 , 2021.
XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdhery,and
DennyZhou. Self-consistencyimproveschainofthoughtreasoninginlanguagemodels. arXivpreprint
arXiv:2203.11171 , 2022.
AlexWarstadt,AaronMueller,LeshemChoshen,EthanWilcox,ChengxuZhuang,JuanCiro,RafaelMosquera,
BhargaviParanjabe,AdinaWilliams,TalLinzen,andRyanCotterell. FindingsoftheBabyLMchallenge:
Sample-efficient pretraining on developmentally plausible corpora. In Alex Warstadt, Aaron Mueller,
Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe,
Adina Williams, Tal Linzen, and Ryan Cotterell (eds.), Proceedings of the BabyLM Challenge at the 27th
ConferenceonComputationalNaturalLanguageLearning ,pp.1–34,Singapore,December2023.Association
for Computational Linguistics. doi: 10.18653/v1/2023.conll-babylm.1. URL https://aclanthology.org/
2023.conll-babylm.1 .
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand
Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In
Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck,
Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk,
andSteliosPiperidis(eds.), ProceedingsoftheTwelfthLanguageResourcesandEvaluationConference ,pp.4003–
4012,Marseille,France,May2020.EuropeanLanguageResourcesAssociation. ISBN979-10-95546-34-4.
URL https://aclanthology.org/2020.lrec-1.494 .
Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models , 2019.
HerbertWoisetschläger,AlexanderIsenko,ShiqiangWang,RubenMayer,andHans-ArnoJacobsen. Asurvey
on efficient federated learning methods for foundation model training. arXiv preprint arXiv:2401.04472 ,
2024.
Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models. arXiv
preprint arXiv:2307.03025 , 2023.
Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le,
TengyuMa,andAdamsWeiYu. Doremi: Optimizingdatamixturesspeedsuplanguagemodelpretraining.
arXiv preprint arXiv:2305.10429 , 2023a.
Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via
importance resampling, 2023b.
44Published in Transactions on Machine Learning Research (12/2024)
Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. A critical evaluation of evaluations for long-form
questionanswering. InAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.), Proceedingsofthe
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 3225–3245,
Toronto,Canada,July2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.acl-long.181.
URL https://aclanthology.org/2023.acl-long.181 .
Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang Wei Koh, Chaowei Xiao, and Muhao Chen. Instructional
fingerprinting of large language models. arXiv preprint arXiv:2401.12255 , 2024.
Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving
interference when merging models, 2023.
ZhengXin Yong, CristinaMenghini, andStephen Bach. Low-resourcelanguages jailbreakgpt-4. In Socially
Responsible Language Modelling Research , 2023.
DingliYu,SimranKaur,ArushiGupta,JonahBrown-Cohen,AnirudhGoyal,andSanjeevArora. Skill-mix:
a flexible and expandable family of evaluations for ai models. In The Twelfth International Conference on
Learning Representations , 2023.
Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng
Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention, 2023a.
ZhexinZhang,LeqiLei,LindongWu,RuiSun,YongkangHuang,ChongLong,XiaoLiu,XuanyuLei,Jie
Tang,andMinlieHuang. Safetybench: Evaluatingthesafetyoflargelanguagemodelswithmultiplechoice
questions, 2023b.
WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,BeichenZhang,
Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223 , 2023a.
Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. (inthe) wildchat: 570k
chatgptinteractionlogsinthewild. In TheTwelfthInternationalConferenceonLearningRepresentations ,2023b.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv
preprint arXiv:2306.05685 , 2023.
Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He,
etal. A comprehensive survey onpretrained foundation models: Ahistoryfrom bertto chatgpt. arXiv
preprint arXiv:2302.09419 , 2023.
JiahangZhou, YanyuChen, ZicongHong, WuhuiChen, YueYu, TaoZhang, HuiWang, ChuanfuZhang, and
Zibin Zheng. Training and serving system of foundation models: A comprehensive survey. arXiv preprint
arXiv:2401.02643 , 2024.
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,
LudwigSchmidt,WilliamYangWang,andYejinChoi. Multimodalc4: Anopen,billion-scalecorpusof
images interleaved with text. Advances in Neural Information Processing Systems , 36, 2024.
PiotrŻelasko,DanielPovey,Jan"Yenda"Trmal,andSanjeevKhudanpur. Lhotse: aspeechdatarepresentation
library for the modern deep learning ecosystem, 2021.
45Published in Transactions on Machine Learning Research (12/2024)
A Contributions
Tocreatethischeatsheet,avarietyofcontributorswereaskedtoproposeresources,papers,andtoolsrelevant
to open foundation model development. Those resources were grouped into sections, which were each
curated by a subset of the contributors. We list the main curators of each section, listed alphabetically below.
However, it is important to note that many contributors advised across sections, and helped with preparing
the interactive cheatsheet tool. Nay San led the speech modality, and Gabriel Ilharco led the vision modality.
•PretrainingDataSources DavidAdelani,StellaBiderman,GabrielIlharco,KyleLo,ShayneLongpre,
Luca Soldaini, Nay San
•Finetuning Data Catalogs David Adelani, Stella Biderman, Gabriel Ilharco, Shayne Longpre, Nay
San
•Data Search, Analysis, & Exploration Stella Biderman, Gabriel Ilharco, Shayne Longpre, Nay San
•Data Cleaning, Filtering, & Mixing Alon Albalak, Kyle Lo, Luca Soldaini
•Data Deduplication Alon Albalak, Kyle Lo, Shayne Longpre, Luca Soldaini
•Data Decontamination Alon Albalak, Stella Biderman, Shayne Longpre
•Data Auditing Stella Biderman, Aviya Skowron
•Data Documentation Stella Biderman, Aviya Skowron
•Data Governance Stella Biderman, Yacine Jernite, Sayash Kapoor
•Pretraining Repositories Stella Biderman, Gabriel Ilharco, Nay San, Hailey Schoelkopf
•Finetuning Repositories Gabriel Ilharco, Nay San, Hailey Schoelkopf
•Efficiency & Resource Allocation Hailey Schoelkopf
•Educational Resources Hailey Schoelkopf
•Estimating Environmental Impact Peter Henderson, Sayash Kapoor, Sasha Luccioni
•Effective use of Resources Sayash Kapoor, Sasha Luccioni
•General Capabilities Rishi Bommasani, Shayne Longpre, Kevin Klyman
•Risks & Harms Maribeth Rauh, Laura Weidinger
•Risks & Harm Taxonomies Bertie Vidgen
•Model Documentation Sayash Kapoor, Shayne Longpre
•Reproducibility Stella Biderman, Shayne Longpre
•License Selection Stella Biderman, Yacine Jernite, Kevin Klyman, Aviya Skowron, Daniel McDuff
•Usage Monitoring Kevin Klyman
•Website Justin Riddiough, Shayne Longpre, Luca Soldaini
•Advising StellaBiderman,PeterHenderson,YacineJernite,SashaLuccioni,PercyLiang,Arvind
Narayanan, Victor Sanh
46