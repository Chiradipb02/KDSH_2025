Published in Transactions on Machine Learning Research (10/2023)
Complementary Sparsity: Accelerating Sparse CNNs with
High Accuracy on General-Purpose Computing Platforms
Kang Zhao∗zhaokang29@huawei.com
Huawei Noah Ark’s Lab
Yijun Tan∗tanyj1998@gmail.com
SKL of Processors, Institute of Computing Technology, CAS
Kai Han kai.han@huawei.com
Huawei Noah Ark’s Lab
Ting Hu huting35@huawei.com
Huawei Noah Ark’s Lab
Hanting Chen chenhanting@huawei.com
Huawei Noah Ark’s Lab
Tao Yuan yuantao38@huawei.com
Huawei Noah Ark’s Lab
Yunhe Wang†yunhe.wang@huawei.com
Huawei Noah Ark’s Lab
Jun Yao†yaojun97@huawei.com
Huawei Noah Ark’s Lab
Reviewed on OpenReview: https: // openreview. net/ forum? id= g1B4qgOw79
Abstract
Model sparsity is a promising approach to reducing parameters and FLOPs of convolutional
neural networks (CNNs). Compared to unstructured or coarse-grained structured sparsity,
ﬁne-grained structured sparsity, e.g., N:M sparse pattern, can achieve better balance be-
tween accuracy and eﬃciency on general computing platforms like CPUs and GPUs. In
particular, the 2:4 sparsity can accelerate CNN inference by 2 ×speed and with negligi-
ble accuracy drop. However, N:M sparsity needs to be supported by GPU within spe-
ciﬁc hardware circuits and hardly achieve signiﬁcant speedups on common GPUs. To ac-
celerate CNNs with general-purposed computing resources and simultaneously retain the
model accuracy as much as possible, this paper proposes complementary sparsity (CS).
CS denotes that only one weight can be retained for weights spaced at the same distance.
On the one hand, CS features high mask ﬂexibility, which is naturally favorable to high
model accuracy. Moreover, we propose a CS-speciﬁc sparse training method to improve
CS-based CNNs’ accuracy under high parameter sparsities ( >75%). On the other hand,
CS itself is memory-access balanced and robust to pattern hyperparameters, making it an
ideal candidate for speeding up CS-based convolution computation on CPUs and common
GPUs. We thus propose a CS convolution parallel computing algorithm that adapts to
common GPUs without sparse tensor cores. Experimental results show that compared to
other sparsity patterns, the proposed CS achieves the optimal trade-oﬀ in terms of accu-
∗Equal contribution.
†Corresponding author.
1Published in Transactions on Machine Learning Research (10/2023)
racy and latency for CPUs and common GPUs, respectively. Codes will be available at
https://gitee.com/mindspore/models/tree/master/research/cv/CS.
1 Introduction
Weight sparsiﬁcation is a crucial method to compress CNNs. The rationality behind weight sparsiﬁcation
is that there are redundant weights in regular CNNs which tend to generate overlapped features (Hoeﬂer
et al., 2021; Ayinde et al., 2019). Thus, removing a certain amount of weights in CNNs has little or manage-
able impact on CNN’s accuracy, while it can signiﬁcantly lower CNN’s number of ﬂoating-point operations
(FLOPs) during inference.
According to the extent of pruning freedom and acceleration aﬃnity, the existent weight sparsiﬁcation
technologies for CNNs can be divided into three categories: unstructured sparsity (US), coarse-grained
structured sparsity (CSS), and ﬁne-grained structured sparsity (FSS). US, depicted in Fig. 1(a), also called
random sparsity in some studies (Huang et al., 2022), permits pruning weights anywhere inside a weight
tensor (Zhu & Gupta, 2017; Gale et al., 2019; Mostafa & Wang, 2019; Evci et al., 2020; Kusupati et al., 2020;
Liu et al., 2021; Ma et al., 2021; Peste et al., 2021; Tai et al., 2022; Jaiswal et al., 2022; Park & No, 2022; Chen
et al., 2021; Li et al., 2022a). Due to US’s highest degree of pruning freedom, the most important weights
aﬀecting network quality can always be retained under any sparsity. Hence, unstructurally sparsed networks
can preserve a decent accuracy even if sparsity is very high ( ≥90%). However, the nonuniformity of weight
distribution makes it nearly impossible to accelerate US-based convolution on general-purpose computing
platforms. In contrast, for CSS, e.g., Fig. 1(b)-(d), the pruning granularity is block, channel, or ﬁlter -wise
(Wen et al., 2016; Li et al., 2016; Gray et al., 2017; Ji et al., 2018; Liu et al., 2017; Tang et al., 2020; Ding
et al., 2021; Hou et al., 2022; Liu et al., 2022; Chen et al., 2022; Zhang et al., 2022; He & Xiao, 2023). CSS’s
patterns are generally with high regularity, which can signiﬁcantly speedup the network inference. For some
of CSS patterns such as ﬁlter pruning and channel pruning, i.e., Fig. 1(c) and 1(d), the pruned CNNs can
directly operate on the original platforms without any new acceleration algorithm design. Nonetheless, the
relatively bigger pruning granularity inevitably entails that many important weights are removed together
with unimportant weights. Under the same accuracy, CNNs within CSS own a lower compression ratio
compared to that within US patterns.
In this study, we focus on FSS since it generally results in better tradeoﬀs between accuracy and eﬃciency.
We classify a sparsity pattern as FSS if its pruning granularity is vector-wise (Yao et al., 2019; Mishra et al.,
2021; Huang et al., 2022; Tan et al., 2022; Meng et al., 2020), e.g., Fig. 1(e)-(h). Compared with US and CSS,
FSS possesses both high prunability and decent acceleration aﬃnity. However, the existing FSS patterns
more or less have some shortcomings, as shown in Table 1. N:M sparsity, which has been the most popular
FSS pattern lately, mainly facilitates inference eﬃciency on speciﬁc hardware, e.g., Amphere architecture
within sparse tensor cores (Choquette & Gandhi, 2020). Some work tries to accelerate N:M-base convolution
on common GPUs without sparse tensor cores (Yao et al., 2019), but the practical speedup beneﬁts compared
to the dense convolution are probably limited, since the dense convolution has been adequately optimized
and perfectly supported by current common GPUs. Shﬂ_BW Huang et al. (2022) and OVW Tan et al.
(2022) sparse patterns achieve practical speedups on common GPUs, but CNNs using these two patterns
have not reached a satisﬁcatory accuracy so far.
In this paper, we propose a complementarily sparsed pattern to better balance the accuracy and inference
eﬃciency of sparse CNNs. The design principle behind complementary sparsity (CS) is to leverage
the non-conﬂict strengths of the prior sparse patterns as much as possible while addressing
their limitations . Firstly, like N:M sparsity, CS prunes weights inside a vector. Secondly, the positions of
the pruned weights and retained weights are complementary. For example, Fig. 1(e) shows a 50% CS. In
this subﬁgure, a vector’s shape is 8×1, as marked by the red frame. The ’complementary’ means that inside
the vector, if the 1st weight is pruned, then the 5th weight has to be retained. This logic is the same for
the 2nd and 6th weights, and so forth. Lastly, the size of a minimum vector to form CS is variable, which
property is called hyperparameter robustness. The hyperparameter robustness of CS makes the pattern very
adaptive to diﬀerent computing platforms, such as CPUs and GPUs.
2Published in Transactions on Machine Learning Research (10/2023)
Huawei Proprietary -Restricted Distribution 2
𝐶𝑖𝑛×𝐻×𝑊 𝐶𝑖𝑛
Unstructured Sparsity Block Sparsity
N:M Sparsity
（2:4）OVW Sparsity Complementary 
Sparsity (Ours)
Shfl_BW Sparsity𝐶𝑜𝑢𝑡 𝐶𝑜𝑢𝑡
×𝐻
×𝑊
𝐶𝑖𝑛×𝐻×𝑊
𝐶𝑜𝑢𝑡𝐶𝑜𝑢𝑡×𝐻×𝑊
𝐶𝑖𝑛 𝐶𝑖𝑛
×𝐻
×𝑊Filter Pruning
 Channel Pruning
𝐶𝑜𝑢𝑡
2
SELE.
1𝐶𝑖𝑛×𝐻×𝑊
𝐶𝑜𝑢𝑡（a） （b） （c） （d）
（e） （f） （g） （h）
Figure 1: Visualization of diﬀerent sparse patterns at the 50% sparsity. (a) Unstructured structured sparsity
that allows to discard weights of arbitratry positions. (b)-(d) Coarse-grained structured sparsity. (e)-(h)
Fine-grained structured sparsity. In particular, (e) is the proposed complementary sparsity and ’2 SELE. 1’
represents retaining one from two weights in all the complementary positions.
The major contributions of this paper are as follows:
•We propose a new sparse pattern—CS, which features both high mask ﬂexibility and high acceler-
ation aﬃnity. CS allows pruning CNNs with less accuracy reduction and accelerating sparse CNNs
on both CPUs and common GPUs.
•Used in the training phase, a CS-speciﬁc sparse training method is proposed to boost CS-based
CNNs’ accuracy under high sparsities. With the proposed method, CS-based CNNs perform on par
with or better than that with N:M sparsity in terms of accuracy. At the 50% sparsity on ImageNet,
CS-based ResNet50 achieves 76.37% accuracy with negligible accuracy loss. At the 93.75% sparsity,
CS-basedResNet50 achieves 71.07%accuracy witha 5.32% drop, which isbetterthan N:Msparsiﬁed
ResNet50 which drops 5.8%.
•Used in the inference phase, a parallel acceleration algorithm is proposed to speedup the CS-
based convolutions on common GPUs. With the acceleration algorithm, CNNs within CS achieves
2.59×~3.07×speedups at the 93.75% sparsity over the dense counterparts supported by cuDNN.
Through the algorithm-software co-optimization, the proposed CS reaches better tradeoﬀs between sparse
CNNs’ model quality and inference eﬃciency compared with other ﬁne-grained structured sparse patterns.
To be clear, the advantages of our CS over similar works are shown in Table 1.
2 Related Work
Unstructured sparsity (US) Neural networks within US have been researched for a long time. The
winning ticket hypothesis denotes that there always exists a sparse neural network inside a dense network
and the subnetwork can be trained to reach the comparable accuracy as the dense one (Frankle & Carbin,
2018). Since the hypothesis was proposed, a surge of studies have focused on developing good pruning
methods to form US. Gale et al. (2019); Zhu & Gupta (2017) improve the magnitude-based pruning methods
simply by gradually sparsifying and prolonging the training time, respectively. Rather than fully training
a dense network before pruning, Mostafa & Wang (2019); Evci et al. (2020); Ma et al. (2021) adopt the
3Published in Transactions on Machine Learning Research (10/2023)
Pattern Acceleration w/o ASICs Accu.
US Almost impossible High
Block sparsity Yes Low
Filter pruning Yes Low
Channel pruning Yes Low
N:MMishra et al. (2021) Hard High
Shﬂ_BWHuang et al. (2022) Yes Medium
OVWTan et al. (2022) Yes Medium
CS (Ours) Yes High
Table 1: Comparison among diﬀerent sparse patterns.
sparse training to directly generate the unstructured sparse neural networks. These sparse training methods
basically contain a common mechanism that periodically prunes and regrows some weights according to
some criterion. Furthermore, Peste et al. (2021) alternatively conducts sparse and dense training. In this
way, both dense and unstructured sparse neural networks are generated after training. Instead of pruning
weights by carefully designed criterion, Kusupati et al. (2020); Tai et al. (2022) learn the sparse masks by
diﬀerentiation. In particular, the proposed method in Tai et al. (2022) reports the state-of-the-art accuracy
of US-based CNNs on the ImageNet dataset. Generally, CNNs within US can not obtain signiﬁcant speedup
gains on CPUs and common GPUs due to the severe memory-access conﬂict.
Coarse-grained structured Sparsity (CSS) Normally, CSS can be enforced along either the input or
output axes of a weight tensor. Separately shrinking the output and input axis is called ﬁlter and channel
pruning, respectively. Pruning a square block in both input and output axes is called block sparsity (Gray
et al., 2017). To our knowledge, Wen et al. (2016) is the ﬁrst to propose ﬁlter pruning or channel pruning
for CNN compression. In their study, the group LASSO method is directly enforced into weights to induce
sparsities among ﬁlters or channels. Some studies like Liu et al. (2017); Ding et al. (2021) employ extra
indicators to evaluate the ﬁlters, e.g., scaling factors in batch normalization layers, or properly placed 1×1
convolutions that can be absorbed during inference. Since pruning ﬁlters also result in pruning the related
channels in the next layers, the proposed method in Li et al. (2016) jointly considers the impact of ﬁlter and
channel pruning on network accuracy. Rather than developing various hypotheses to measure the importance
of ﬁlters, Tang et al. (2020) assesses ﬁlters by observing the network responses to real data and adversarial
samples. Besides, some principles originally used for US have lately been introduced to realize CSS, e.g.,
Hou et al. (2022); Tai et al. (2022). Despite these eﬀorts, CSS-based CNNs’ accuracy is still relatively lower
and drops drastically especially when the required sparsity >70%.
Fine-grained structured sparsity (FSS) N:M sparsity is a well-known ﬁne-grained structured sparse
pattern where at most Nnon-zero weights are retained for every continuous Mweights (Yao et al., 2019;
Mishra et al., 2021; Lu et al., 2023; Zhang et al., 2023). However, N:M sparse pattern needs to be supported
by GPUs embedded with sparse tensor cores. On common GPUs, the pattern hardly outperforms dense
convolutions supported by cuDNN. To tackle the problem, Shﬂ_BW and OVW sparsity regard a M×1
vector as an entirety which is pruned or retained together (Huang et al., 2022; Tan et al., 2022). By this
design, the retained weights and the related features during convolution computation can be easily indexed.
Thus, Shﬂ_BWandOVWsparsitycanaccelerateconvolutionsoncommonGPUstoagreatextent. However,
the relatively large pruning unit of M×1still decreases the ﬂexibility (Hubara et al., 2021), which results in
reduced model accuracy. In contrast, our CS can help maintain similar or better model accuracy relative to
N:M sparsity, as well as achieve the practical speedups of sparse convolutions on common GPUs and CPUs.
GPU acceleration for convolutions So far, there are basically four sorts of parallelable algorithms to
implement convolutions: direct convolution, Winograd (Chikin & Kryzhanovskiy, 2022), FFT (Wang et al.,
2020), explicit general matrix multiplication (GEMM) (Jiang et al., 2022) and implicit GEMM (Zhou et al.,
2021b). Among these, GEMM-base convolution algorithms are more performant on GPUs since the modern
parallel computing platforms have highly optimized the GEMM operations (Chetlur et al., 2014; Jorda et al.,
2019; Li et al., 2022b). However, explicit GEMM-based convolutions need to ﬁrstly invoke img2col to change
4Published in Transactions on Machine Learning Research (10/2023)
tensors to matrices, which is memory access-intensive and time-consuming. By contrast, implicit GEMM-
based convolutions remove the memory access overheads, which is top-performed in most cases. Moreover,
Tan et al. (2022) employs the implicit GEMM to accelerate sparse convolutions, which implies the potential
of implicit GEMM to speedup other sparse patterns. In this work, the implicit GEMM is also utilized to
develop the parallel acceleration algorithm of CS-based convolutions on common GPUs.
3 Method
3.1 Complementary Sparsity Formulation
Forasparseweighttensor Wwiththeshapeof Cout×Cin×Fh×Fw, eachﬁlter Wihastheshape Cin×Fh×Fw
and is ﬂattened. We use Wi[j]to denote the jthweight in the ﬂattened Wi.Sis the sparsity of the weight
tensor. Two key parameters are introduced to conveniently describe CS: 1) K.Kdenotes the amount of
the complementary positions from which only one single weight should be selected. For instance, at the 50%
sparsity, there should be K= 2complementary positions for selecting a weight. for the 75% sparsity, there
areK= 4complementary positions from which a weight is selected. 2) M.Mrepresents the address oﬀset
with which weights in the complementary positions can mutually be located. Speciﬁcally,
K=1
1−S(1)
M=L
K, L∈{Cin/c, C in∗Fh∗Fw} (2)
In Equation 2, if L=Cin/c, the typical values of Minclude 2,4,8,16. Here conly means that Cinshould
be divisible by M. Then, a sparse weight tensor is regarded as complementarily sparsed as long as for any
Wi[j], i∈[1, Cout], j∈[1, M],
/bardblWi[j], Wi[j+ 1∗M], ...W i[j+ (K−1)∗M]/bardbl0< K (3)
Furthermore, a sparse weight tensor is regarded as strictly conforming to the pattern of CS if and only if
/bardblWi[j], Wi[j+ 1∗M], ...W i[j+ (K−1)∗M]/bardbl0= 1 (4)
To intuitively understand the deﬁnition of CS, Fig. 2 gives some examples strictly obeying the pattern of
CS across multiple sparsities. Accordingly, the parameter values, i.e., KandMof each example, are listed
in Table 2. Note that Kis only related to the speciﬁed sparsity Sand is independent of the speciﬁc weight
tensor shapes. Unless otherwise speciﬁed, the rest of the paper only discusses the strict CS.
Huawei Proprietary -Restricted Distribution 3
(a)
(b)
(c)
(d)
(e)0.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.4
0.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.40.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.4
0.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.4
0.8-0.10.21.51.2-1.3-0.40.20.72.00.9-0.51.00.32.1-1.4
Figure 2: Examples of CS at diﬀerent sparsities. The blue shading of (b)-(e) indicates the selected weights.
(a) a dense weight tensor. For convience, the tensor is 2D and with the shape of 1×16, i.e., Cout= 1and
Cin∗Fh∗Fw= 16(b) The 50% CS of the weight tensor. (c) The 75% CS of the weight tensor. (d) The
87.5% CS of the weight tensor. (e) The 93.75% CS of the weight tensor.
5Published in Transactions on Machine Learning Research (10/2023)
Table 2: Parameter values of the examples shown in Fig. 2.
Sparsity
(%)KMEncoding
bit numberEncoding
results
50 2 8 1 0,1,1,0,0,0,1,1
75 4 4 2 1,2,3,0
87.5 8 2 3 7,4
93.75 16 1 4 14
3.2 CS-speciﬁc Gradual Sparse Training Method
The aim of designing a CS-speciﬁc sparse training method is to attain universality. That is, the desired
training method can improve the accuracy of various CS-based CNNs among diﬀerent sparsities. With the
training method, users do not need to customize training recipes for diﬀerent CNN structures.
Conventionally, training a sparse neural network follows the process of dense training, single-shot pruning,
and ﬁnetuning. We argue the conventional training process is unfavorable to CS-based CNNs under high
sparsities ( >75%) in which case the derived sparse masks from the dense weight tensors tend to be subop-
timal. In contrast, we propose a two-phase training scheme: gradual sparse training followed by retraining.
As demonstrated in Algorithm 1, assuming that the allowed training iteration amount in each phase is I,
the ﬁrst phase starts training with a dense network and the sparsity of the network discretely steps up every
Piterations. The completion of the ﬁrst phase oﬀers the sparse masks conforming to the pattern of CS
and the weights as the initialization of the second phase. The second phase simply uses the same training
setups as the ﬁrst phase to retrain the retained weights in networks selected by the sparse masks. Obviously,
the novelty of our training method mainly lies in the ﬁrst phase. The ﬁrst phase comprises two features as
detailed below.
Variable number of steps for gradual sparsiﬁcation . By empirical observation, we ﬁnd the higher
the required sparsity is, the more steps for gradual sparsiﬁcation are needed to obtain the better model
quality. Hence, to obtain a CS-based CNN at the sparsity S, we set Ksteps for gradually sparsifying the
network. For instances, in case that the target sparsity is 75%, the change of sparsities during training
in the ﬁrst phase is 0%→25%→50%→75%, while for the target sparsity of 87.5%, the change is
0%→12.5%→25%→37.5%→50%→62.5%→75%→87.5%. To be formulated, for the ith training
iteration, the desired sparsity Siis:
Si=ceiling{i
I∗K}−1
K(5)
Equation 5 intuitively means that Siperforms the piecewise linear growth as training iterations. The
position of Equation 5 in the workﬂow of our training method is shown in Algorithm 1. Note that gradual
sparsiﬁcation only means the amounts of weights participating in the forward passes are gradually and
discretely reduced. During backward passes, the gradients of all the weights are computed and every weight
is updated no matter which sparsity step a network is at.
CS-speciﬁc weight reselection . Since all the weights are kept updating in the ﬁrst training phase, it is
beneﬁcial to reselect the weights, i.e., update the sparse masks once in a while. Supposing every Fiterations,
the sparse masks should be updated. The update process for CS is: 1) Reshape. A weight tensor with the
shape Cout×Cin×Fh×Fwis reshaped into G×K×L, where KandLis deﬁned in Equation 1 and 2,
respectively. Gcan be inferred given KandL. 2) Reselection. Along Kaxis to reselect kiweights with the
highest absolute value. kis related to the sparsity step that a network is at, i.e.,
ki= (1−Si)∗K (6)
The positions of the reslected weights in masks are set ones while the other positions are set zeros. 3)
Restoration, which means inversely reshape the weight tensor from G×K×Lback to Cout×Cin×Fh×Fw.
Fig. 3 shows an example of the CS-speciﬁc weight reselection procedure, which mainly visualizes the step
2). Notably, the gradual sparsiﬁcation is exactly achieved by our weight reselection procedure by properly
setting Fto make Pdivisible by F.
6Published in Transactions on Machine Learning Research (10/2023)
Algorithm 1 Workﬂow of Our Training Method
Initialization: Dense weights W
Input: Required sparsity S, training iterations I, data D
Output: Sparse weights WS
Key params: K, sparse mask M, mask updating freq. F
1:——————- Gradual Sparse Training Phase ——————-
2:fori= 0;i < I ;i+ +do
3:ifi%Fthen
4: Si←Equation 5( i, I, K)
5: M←Weights_Reselect( Si, W)
6:end if
7:Forward( W, M, D)
8:Backward( W, D)
9:Weights_Update( W) #Update all weights
10:end for
11:————————– Retraining Phase ——————————
12:fori= 0;i < ite ;i+ +do
13:Forward( W, M, D)
14:Backward( W, M, D)
15:Weights_Update( W, M) #Update selected weights
16:end for
17:——————————————————————————
18:WS←W*M
Huawei Proprietary -Restricted Distribution 4
-1.2 -0.2 1.2 2.0
0.4 -1.2 -0.3 1.9
1.0 -2.3 0.8 -2.3
-0.9 2.0 0.2 -0.7-2.1 -0.1 1.4 2.9
0.3 -0.2 -0.2 1.4
1.1 -1.3 0.9 -2.1
-0.7 1.0 0.1 -0.6
-1.3 -0.1 1.8 1.0
0.3 -0.3 -0.1 1.3
1.1 -2.9 0.4 -2.7
-0.6 1.0 0.1 -0.6-1.2 -0.1 1.2 2.0
0.3 -0.3 -0.1 1.3
1.0 -2.3 0.8 -2.3
-0.6 2.0 0.1 -0.6Selected weight
Pruned weight𝑆1=0
𝑘1=4𝑆2=25%
𝑘2=3
𝑆3=50%
𝑘3=2𝑆4=75%
𝑘4=1𝐾𝐿 𝐺
After 𝑃
iterations
After 𝑃
iterations
After 𝑃
iterations
Figure 3: An example of CS-speciﬁc weight reselection.
3.3 Acceleration of CS-based Convolution
The obtained sparse weight tensors after training are stored in the compressed format. That is, only the non-
zero weights and their indices are stored and used for inference. This procedure is formulated by, converting
WStoWsandIdx, where Wsis the non-zero weight tensor with the smaller shapes and Idxis the index
tensor. The index of a non-zero weight denotes the weight’s position number among Kcomplementary
positions, thus the value of an index is constantly less than K. Fig. 4 exempliﬁes the compression of CS at
the 50% sparsity. In this case, the Wshas half of the shape than WS. Although Idxhas the same shape as
Ws, each index in Idxcan be encoded with very low numbers of bits. As shown in Table 2, only at most 4
bits are needed to encode a non-zero weight’s index.
7Published in Transactions on Machine Learning Research (10/2023)
Huawei Proprietary -Restricted Distribution 5
-1.2 -0.2 1.2 2.0
0.4 -1.2 -0.3 1.9
1.0 -2.3 0.8 -2.3
-0.9 2.0 0.4 -0.7-1.2 -2.3 1.2 1.9
-0.9 2.0 0.4 -2.3
0101
1110Non-zero weights
(uniform)
Index-1.2 0 1.2 0
0 0 0 1.9
0-2.3 0-2.3
-0.9 2.0 0.4 0-1.2 -2.3 1.2 1.9
-0.9 2.0 0.4 -2.3
0101
1110Non-zero weights
(uniform)
Index
Figure 4: A diagram of CS compression at the 50% sparsity.
After acquiring WsandIdx, given an input activation X, the output featuremap Ycan be computed in
parallel. Contrary to the conventional method of feature reuse to speedup sparse convolutions Tan et al.
(2022) on common GPUs, we propose a weight reuse-based algorithm. In implicit GEMM-based dense
convolutions, each thread is generally in charge of a subblock in Y, e.g., of size 4∗4. Then,
Yi:i+4,j:j+4=L/summationdisplay
n=0Wi:i+4,n⊗Xn,j:j+4 (7)
In Equation 7, Lis only equal to Cin∗Fh∗Fwfor GPUs, and⊗represents the outer product operation of
two vectors. When CS-based convolution is conducted, Equation 7 can be modiﬁed into:
Yi:i+4,j:j+4=L∗(1−S)/summationdisplay
n=0Wi:i+4,n◦Xf(n,i),j:j+4, (8)
where
f(n, i) =n+M∗Idx[i:i+ 4, n] (9)
and◦in Equation 8 denotes the operation:
◦=
Wi∗Xn+M∗Idx[i,n],j:j+4
Wi+1∗Xn+M∗Idx[i+1,n],j:j+4
Wi+2∗Xn+M∗Idx[i+2,n],j:j+4
Wi+3∗Xn+M∗Idx[i+3,n],j:j+4
(10)
Equation 8 makes it possible to compute CS-based convolutions by chunk. For a non-zero weight, there are
Krelated sub-blocks in an input activation that may be indexed during convolution. On common GPUs,
by loading all the related Ksub-blocks to GPUs’ shared memory in advance, Equation 8 can be conducted
eﬃciently. Algorithm 2 shows this parallel acceleration process, where 50% CS-based convolution is taken
for example.
On CPUs, the direct method is employed to accelerate CS. Due to the instruction limit, CPUs can hardly
fetch multiple values far apart from each other. Accordingly, our CS allows diﬀerent values of Mwithout
reducing network accuracy, which is very friendly to CPU operation. During convolutions, CPUs conduct
sparse vector products along the Cinaxis. In this case, L=Cin/c.
4 Experiments
4.1 Datasets, Models and Settings
CIFAR100 Krizhevsky et al. (2009) and ImageNet-1k Deng et al. (2009) are two datasets used to test
the accuracy of CS-based CNNs. Speciﬁcally, on CIFAR100, we evaluate three classical CNNs including
VGG-19 Simonyan & Zisserman (2014), ResNet-18 and ResNet-50 He et al. (2016), and two parameter-
eﬃcient CNNs including MobileNetv2 and SqueezeNet Sandler et al. (2018); Iandola et al. (2016). Since
on CIFAR100, low sparsities ( ≤75%) may result in insigniﬁcant accuracy diﬀerences between CNNs with
8Published in Transactions on Machine Learning Research (10/2023)
Algorithm 2 Parallelization for 50% CS-based convolution
Input: Idx,W,X
Output: Y
Key params: M, L
__Shared__ ﬂoat4 local_w, local_idx
__Shared__ ﬂoat4 local_x[2], local_y
1:—————————— Parallelism ———————————-
2:for all N∗OC∗OH∗OW/ 16threadsdo
3:Subblock(Y)←SubConv ( thread[i], Idx,W,X)
4:end for
5:——————————— Details————————————
6:function SubConv (tid,Idx,W,X)
7:fori= 0;i < L ;i+ =BMdo
8: local_filter←Subblock( filter)
9: local_idx←Subblock( Idx)
10: local_x[0]←Subblock1( X)
11: local_x[1]←Subblock2( X)
12: Syncthreads();
13: local_y= Eq. 8( local_w,local_x,local_idx)
14:end for
15:return local_y
16:end function
our CS and with other sparse patterns, we test the three classical CNNs under high sparsities: 87.5% and
93.75%, while for MobileNetv2 and SqueezeNet, accuracy results under the 50% and 75% sparsities are
adequately distinguishable for comparison. At each sparsity, each CNN is sparsiﬁed by US, ﬁlter pruning,
N:M, OVW, and CS, respectively. All the sparse CNNs are ﬁrstly trained with the same training paradigm:
dense training, pruning, and ﬁnetuning. This paradigm has been widely used to form various sparse CNNs.
For simplicity, the ﬁnetuning phase uses the same setting as the dense training phase, which has been veriﬁed
as reasonable in Mishra et al. (2021). After that, CS-based CNNs are trained with the proposed CS-speciﬁc
gradual training method for comparison. All the trainings use the common and the same settings. The total
training epoch is 400. For the conventional training paradigm, the dense training phase uses 200 epochs and
the ﬁnetuning phase uses the rest. Similarly, for our CS-speciﬁc gradual training method, each training phase
equally uses 200 epochs as well. Each experiment is repeated three times and all the experimental results on
CIFAR100 are listed in the format of "mean ±standard deviation" to reduce the inﬂuence of random factors.
On ImageNet-1k, CS-based ResNet-50 at diﬀerent sparsities are trained for comparing with other related
works. We use the oﬃcially recommended hypermeter settings for our sparse training method zlm (2022).
Besides, the speedups of CS-based CNNs over the dense counterparts are measured on an Intel(R) Xeon(R)
Gold 6154 CPU and a Tesla V100 GPU without sparse tensor cores, respectively.
4.2 Results on CIFAR100
Table 3 shows the accuracy of diﬀerent networks within diﬀerent sparse patterns on CIFAR100. Firstly,
for the three classical CNNs, our CS achieves the best accuracy under high sparsities among all the sparse
patterns. Secondly, for MobileNetv2 and SqueezeNet, our CS also outperforms other ﬁne-grained structured
sparse pattern at the 75% sparsity. In particular, the accuracy of MobileNetv2 within CS at the 75%
sparsity is even higher than that within the unstructured sparsity, i.e., 62.63% >61.7%. Thirdly, the proposed
training method signiﬁcantly improves the accuracy of a series of CS-based CNNs, with the average accuracy
increasing from 0.48% to 2.83%. Notably, at the 50% sparsity, all types of sparse patterns lead to lossless
accuracy. In this case, we argue the accuracy diﬀerences among the patterns and training methods are
immaterial.
9Published in Transactions on Machine Learning Research (10/2023)
Spa.
(%)VGG19 Resnet18 Resnet50 Spa.
(%)Mobilenetv2 SqueezeNet
Origin 70.8 74.7 72.2 Origin 65.3 65.1
87.5Unstructured 71.3±0.1 73.3±0.1 72.3±0.3
50Unstructured 66.9±0.1 67.7±0.2
Filter pruning 47.8±0.6 59.0±0.2 48.4±1.0 Filter pruning 65.5±0.5 36.4±0.2
N:M(1:8) 70.6±0.1 72.9±0.1 72.1±0.1 N:M(2:4) 66.4±0.3 67.2±0.1
OVW 63.6±0.3 64.1±0.7 59.2±1.6 OVW 61.8±0.3 64.0±0.4
CS-C 70.7±0.2 73.0±0.1 72.5±0.3 CS-C 66.4±0.1 67.2±0.1
CS (Ours) 71.7±0.373.5±0.173.1±0.0 CS (Ours) 66.7±0.2 67.0±0.2
/triangle +1.4 +0.5 +0.6 /triangle +0.2 -0.2
93.75Unstructured 69.8±0.1 71.5±0.0 71.3±0.0
75Unstructured 61.7±0.1 64.5±0.2
Filter pruning 33.2±0.4 47.9±0.4 40.0±0.6 Filter pruning 53.4±1.1 15.9±0.2
N:M(1:16) 68.1±0.3 70.4±0.2 70.6±0.3 N:M(1:8) 58.9±0.4 63.4±0.3
OVW 59.6±0.2 60.6±0.3 41.0±13.8 OVW Failed 54.6 ±0.9
CS-C 68.5±0.2 70.5±0.1 70.5±0.2 CS-C 59.8±0.1 63.9±0.3
CS (Ours) 69.9±0.172.2±0.373.0±0.3 CS (Ours) 62.6±0.2 64.4±0.1
/triangle +1.4 +1.7 +2.5 /triangle +2.8 +0.5
Table 3: Accuracy of sparse CNNs on CIFAR100. ’Spa.’ means sparsity. ’CS-C’ represents CS-based
CNNs formed by the Conventional training paradigm, while ’CS (Ours)’ means that formed by the proposed
training method. ’ /triangle’ means the diﬀerence between ’CS-C’ and ’CS (Ours)’. For spatial brevity, all the data
are rounded to one signiﬁcant digit.
Apart from Table 3, the experimental results on CIFAR100 using the same network for all sparsity ratios are
shown in Figure 5. It is observed that the curves of our CS are generally above the curves of other structured
patterns and are overlapped with curves of unstructured sparsity. That observation further demonstrates
the superiority of our CS in terms of accuracy across all the sparsity ratios.
Huawei Proprietary -Restricted Distribution 20
W W W（a） （b） （c）
Figure 5: Accuracy vs. sparsity curves of diﬀerent CNNs whitin diﬀerent sparse patterns. (a) VGG19. (b)
ResNet18. (c) ResNet50.
Notably, on CIFAR100, our CS performs even better than unstructured sparsity due to the relatively small
dataset size. Generally, compared to modern DNNs’ capacity, CIFAR100 is easy to be overﬁtted. Thus,
within a certain sparsity range, weight sparsiﬁcation can largely regularize the model capacity and lead to
higher accuracy over dense counterparts (Hoeﬂer et al., 2021). Compared to unstructured sparsity, ﬁne-
grained structured sparsity such as CS and N:M constrains a model more strictly. This constraint generally
endows the model favorable regularization on small datasets like CIFAR100 and at low sparsity like 50%
and 75%. For example, as shown in Figure 7, at 50% sparsity for VGG19 on CIFAR100, N:M sparsity leads
to 73.00% accuracy, which is better than 72.82% of unstructured sparsity. At 75% sparsity, our CS-based
VGG19 reaches 72.51% accuracy, while unstructured sparsity-based VGG19 is 72.13%.
10Published in Transactions on Machine Learning Research (10/2023)
Under higher sparsity such as 93.75%, despite the rare possibility of overﬁtting, the accuracy gap between
ﬁne-grained structured patterns like ’ CS-C’ and ’unstructured’ was still narrow due to the small dataset
size. Hence, with our proposed training scheme, ’ CS(Ours) ’ can easily ﬁll the gap to further become on par
with or to even surpass ’unstructured’. By contrast, on ImageNet, our CS and other ﬁne-grained sparsity
patterns bring constantly lower accuracy than unstructured sparsity regardless of sparsity ratios and network
architectures.
4.3 Results on ImageNet
Table 4 shows the experimental results on ImageNet. Compared with the OVW and Shﬂ_BW patterns,
our CS with the proposed training scheme leads to better accuracy under high sparsities, e.g., 93.75%. For
other sparsities, our CS achieves comparable accuracy with the state-of-the-art N:M sparsity. However, the
diﬀerent settings of Min N:M sparsity signiﬁcantly aﬀect the network accuracy, e.g., Sun et al. (2021). On
the contrary, our CS is robust to the pattern hyperparameter setting which will be shown in the ablation
study.
Pattern SparsityError(%) Params
(M)Flops
(G) Ori. Pruned Gap
N:M 2:4 77.3 77.0 0.3 13.8 2.15
OVW 50% 76.12 75.76 0.36 13.8 2.15
CS(Ours) 50% 76.39 76.37 0.02 13.8 2.15
N:M 1:4 77.3 75.3 2 7.93 1.17
OVW 70% 76.12 73.35 2.77 9.14 1.37
CS(Ours) 75% 76.39 75.18 1.21 7.93 1.17
Shﬂ_BW 80% N/A 75.94 N/A 6.78 0.98
CS(Ours) 87.5% 76.39 72.44 3.95 5.02 0.69
N:M 1:16 77.3 71.5 5.8 3.52 0.44
Shﬂ_BW 90% N/A 73.09 N/A 4.43 0.59
CS(Ours) 93.75% 76.39 71.07 5.32 3.52 0.44
Table 4: ResNet50 accuracy comparison among diﬀerent ﬁne-grained structured sparse patterns on Ima-
geNet. The results of N:M, OVW, and Shﬂ_BW are from Zhou et al. (2021a), Tan et al. (2022) and Huang
et al. (2022), respectively. ’N/A’ means the related work does not report the result.
Besides, although this work mainly focuses on CNNs, we also give the preliminary results of CS and N:M
sparsity on Transformer structures as shown in Table 5. The DeiT-small is used and the training settings
of CS-based DeiT-small and N:M-based DeiT-small are identical. That is, our proposed training scheme
is not used this time for an absolutely fair comparison. Experimental results show once more that our CS
also incurs a comparable accuracy as N:M sparsity on Transformer architectures. Note that at 50% sparsity,
both N:M sparsity and CS -based DeiT-small surpass the dense one, so the diﬀerence between the two sparse
patterns at 50% sparsity is trivial.
With the experimental results of CNN and Transformer architectures, we would like to reaﬃrm that our
CS mainly achieves comparable accuracy to N:M sparsity under identical training settings. The role of our
proposed training scheme is only to improve a CS-based network’s accuracy under high sparsity ratios. For
instance, at 93.75%, ResNet50 within CS surpasses that within N:M sparsity by a decent margin, i.e., ~0.5%
on ImageNet as shown before in Table 4. In other words, a better acceleration aﬃnity on CPUs and common
GPUs under comparable accuracy, is truly our CS’s advantage over N:M sparsity.
Finally, due to the slow progress in the explainability of DNNs, the model accuracy of CS lacks clear
theoretical support. However, we found a metric called mask diversity that may provide some insights
(Hubara et al., 2021). The metric is deﬁned as the number of possible masks for a sparse pattern given a
sparsity ratio. For example, for a 8×8weight tensor and a 50% sparsity, our CS has 232= 4,294,967,296
possible masks, while other vector-wise sparsity such as OVW can only have/parenleftbig16
8/parenrightbig
= 12870 masks, not to
11Published in Transactions on Machine Learning Research (10/2023)
mention only/parenleftbig8
4/parenrightbig
= 70masks that channel pruning can provide. Altogether, CS’s high mask ﬂexibility
probably promotes the high accuracy of CS-based CNNs.
Sparsity(%) 50 75 87.5 93.75
N:M 77.61 73.69 67.56 60.08
CS(Ours) 77.4 73.66 67.63 60.01
Table 5: Comparison between CS and N:M sparsity using Top1 accuracy of DeiT-small on ImageNet. The
dense DeiT-small is 75.56%.
4.4 Ablation Study
Firstly, we investigate the eﬀect of diﬀerent mask updating frequencies in our training method, i.e., F
mentioned in Algorithm 1, on network accuracy. The results are shown in Table 6. In the table, F= 0
means updating the mask once for every iteration, F= 0.5means that updating frequency is 0.5 epoch, and
soon. Weﬁndthatthehighertherequiredsparsityis, thehigherthemaskupdatingfrequencyshouldbe. For
example, at the 50% sparsity, F= 8is the best, while at 93.75%, F= 0outperforms others. The Fsettings
in Table 6 is exactly used in training CS-based ResNet50 on ImageNet. Secondly, we investigate a pattern
hypermeter of CS: M. Speciﬁcally, under the same sparsity decided by K, CS-based CNNs with diﬀerent
settings of Mare trained and we conduct pairwise t-test on these CNNs’ accuracy. As shown in Table 7, all
thepvalues are larger than 0.05, which indicates that our CS is robust to pattern hyperparameters. The
robustness is quite beneﬁcial for acceleration as CPUs and GPUs can employ diﬀerent values of Mto meet
the respective constraints in memory access and instruction set.
F 50% 75% 87.5% 93.75%
0 74.7±0.08 74.14±0.63 73.44±0.1972.06±0.38
0.5 74.78±0.0974.55±0.06 73.57±0.2771.9±0.52
1 74.68±0.06 74.35±0.37 73.47±0.6 71.62±0.2
2 74.71±0.24 74.12±0.23 73.16±0.37 71.19±0.59
4 74.85±0.08 74.21±0.08 72.85±0.23 70.85±0.48
874.93±0.3774.28±0.21 72.54±0.61 70.47±0.17
10 74.66±0.19 74.5±0.19 73.06±0.17 70.14±0.09
Table 6: ResNet18 on CIFAR100: Impact of Facross sparsities.
Sparsity (M, K)VGG19 ResNet18 ResNet50 Sparsity (M, K)MobileNetv2 SqueezeNet
87.5%(2,8) 71.28±0.15 73.68±0.32 73.46±0.85
50%(2,2) 66.81±0.14 67.19±0.29
(4,8) 71.46±0.49 73.2±0.39 73.23±0.65 (4,2) 66.65±0.15 67.07±0.04
(8,8) 71.55±0.39 73.47±0.16 73.38±0.39 (8,2) N/A 67.27±0.09
t-test 0.77 0.29 0.82 t-test 0.18 0.57
93.75%(2,16) 69.62±0.09 71.89±0.19 72.23±1.4
75%(2,4) 62.72±0.14 64.26±0.07
(4,16) 69.76±0.19 72.22±0.39 72.56±0.11 (4,4) N/A 64.3±0.22
t-test 0.31 0.39 0.71 t-test N/A 0.7
Table 7: Accuracy of CS-based CNNs with diﬀerent settings of Mon CIFAR100
4.5 Results on Speedups
On CPU, Mis set 2,4,8,16 on demand. On GPUs, we set Mas large as possible, i.e., M=Cin∗Fh∗Fw/K.
We ﬁnd that the larger Mmakes indexing more easier on GPU. Fig. 6 shows the normalized speedups on CS.
Firstly, three typical convolutions in ResNet50 are used for test speedup. As shown in Fig. 6(a), with batch
size equal to 1, CPU achieves 4.27 ×, 5.46×and 7.7×speedups for the 3rd, 11th, 41th convolutions at the
12Published in Transactions on Machine Learning Research (10/2023)
Huawei Proprietary -Restricted Distribution 7
W W W（a） （b） （c）
Figure 6: Speedups for CS. (a) CPU speedups for three CS-based convolutions in ResNet50. (b) GPU
speedups for three CS-based convolutions in ResNet50. (c) GPU speedups for three CS-based CNNs.
Huawei Proprietary -Restricted Distribution 10
50
75
85
9560
70
807587.5
93.75
7590
Figure 7: ResNet50 on CIFAR100: Normalized accuracy-speedup curves of three ﬁne-grained structured
sparse patterns. Note N:M sparsity does not have acceleration gains on common GPUs.
93.75% complementary sparsity, respectively. Similarly, as shown in Fig. 6(b), with batch size equal to 64,
GPU respectively achieves 4.02 ×, 3.33×and 2.52×speedups at 93.75% over dense counterparts supported by
cuDNN. Secondly, we also estimate the speedups on network-level by averaging all the convolutions’ runtime.
GPU achieves 2.75 ×, 3.07×, and 2.59×speedups for VGG19, ResNet50 and SqueezeNet, respectively. These
speedup performances prove the eﬃciency of the proposed parallel acceleration algorithm. In addition, our
CS generally reaches better accuracy-speedup tradeoﬀs compared with the OVW and Shﬂ_BW pattern, as
shown in Fig. 7.
4.6 Results on Practical Inference Performance
Table 8 shows the speedup comparison of CS and N:M sparsity on A100 and V100 GPUs, respectively. Note
that the speedup data of N:M sparsity are directly cited from A100 materials that are publically available.
Although A100 GPUs have a far stronger computing power than previous ones, it is noteworthy that A100
GPUs are quite inﬂexible as it can only support the acceleration of 2:4 sparsity. Other sparsity ratios, such
as 75% (1:4), 87.5% (1:8), and 93.75% (1:16) can not incur any speedups on A100. In contrast, our CS
supports a wide range of sparsity ratios on common GPUs.
13Published in Transactions on Machine Learning Research (10/2023)
50% 75% 87.5% 93.75%
N:M
(A100)2 0 0 0
CS
(V100)1.39 1.86 2.48 3.07
Table 8: ResNet50 speedups comparison at diﬀerent sparsities on V100 and A100
In addition, CS-based ResNet50 inference energy consumptions at diﬀerent sparsities are shown in Table
9. As the sparsity ratio arises, the reduced FLOPs eﬃciently convert to the shorter inference time. At the
93.75% sparsity and 64 batch size, CS-based ResNet50 only consumes 3.28J energy for one inference, which
is energy-eﬃcient for cloud server scenarios.
Sparsity(%) Gﬂops Times(ms) Energy cost(J)
50 2.15 28.98 7.25
75 1.17 21.59 5.4
87.5 0.69 16.24 4.06
93.75 0.44 13.12 3.28
Table 9: Energy consumption of ResNet50 within CS pattern. batch_size= 64
5 Conclusion and Future Work
We propose a novel CS to accelerate sparse CNNs on CPUs and common GPUs and retain the network
accuracy as much as possible. To our knowledge, we are the ﬁrst to report the practical speedups on
both CPUs and common GPUs for a sparse pattern. Not only does the proposed CS feature high mask
ﬂexibility that contributes a lot to sparse CNNs’ accuracy, but also the network accuracy is robust to
pattern hyperparameters. The robustness enhances CS’s adaptability to diﬀerent computing platforms.
we would like to denote that the limitation of our proposed training scheme for CS is that the scheme is
hardly viable for massive pretraining data. It is because our training scheme needs to be embedded into
the pretraining stage and huge amounts of pretraining data make it actually impossible to replicate the
pretraining. Thus, it is diﬃcult for our scheme to be applied in scenarios such as LLMs. However, we argue
that the limitation does not disqualify our major contributions for CS and we will handle the limitation in
future work.
6 Acknowledgement
We gratefully acknowledge the support of MindSpore (Hua, 2020), CANN (Compute Architecture for Neural
Networks) and Ascend AI Processor used for this research.
References
Huawei, mindspore, 2020. MindSpore.https://www.mindspore.cn/.
Image classiﬁcation reference training scripts, 2022. https://github.com/pytorch/vision/tree/main/
references/classification .
Babajide O Ayinde, Tamer Inanc, and Jacek M Zurada. Regularizing deep neural networks by enhancing
diversity in feature extraction. IEEE transactions on neural networks and learning systems , 30(9):2650–
2661, 2019.
Tianlong Chen, Xuxi Chen, Xiaolong Ma, Yanzhi Wang, and Zhangyang Wang. Coarsening the granularity:
Towards structurally sparse lottery tickets. In International Conference on Machine Learning , pp. 3025–
3039. PMLR, 2022.
14Published in Transactions on Machine Learning Research (10/2023)
Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng
Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. Advances
in Neural Information Processing Systems , 34:19637–19651, 2021.
Sharan Chetlur, Cliﬀ Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and
Evan Shelhamer. cudnn: Eﬃcient primitives for deep learning. arXiv preprint arXiv:1410.0759 , 2014.
Vladimir Chikin and Vladimir Kryzhanovskiy. Channel balancing for accurate quantization of winograd
convolutions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 12507–12516, 2022.
Jack Choquette and Wish Gandhi. Nvidia a100 gpu: Performance & innovation for gpu computing. In 2020
IEEE Hot Chips 32 Symposium (HCS) , pp. 1–43. IEEE Computer Society, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee,
2009.
Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen Guo, and Guiguang Ding.
Resrep: Losslesscnnpruningviadecouplingrememberingandforgetting. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 4510–4520, 2021.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making
all tickets winners. In International Conference on Machine Learning , pp. 2943–2952. PMLR, 2020.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 , 2018.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint
arXiv:1902.09574 , 2019.
Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. arXiv preprint
arXiv:1711.09224 , 3:2, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Yang He and Lingao Xiao. Structured pruning for deep convolutional neural networks: A survey. arXiv
preprint arXiv:2303.00566 , 2023.
Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning:
Pruning and growth for eﬃcient inference and training in neural networks. The Journal of Machine
Learning Research , 22(1):10882–11005, 2021.
Zejiang Hou, Minghai Qin, Fei Sun, Xiaolong Ma, Kun Yuan, Yi Xu, Yen-Kuang Chen, Rong Jin, Yuan
Xie, and Sun-Yuan Kung. Chex: channel exploration for cnn model compression. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12287–12298, 2022.
Guyue Huang, Haoran Li, Minghai Qin, Fei Sun, Yufei Ding, and Yuan Xie. Shﬂ-bw: accelerating deep
neural network inference with tensor-core aware weight pruning. In Proceedings of the 59th ACM/IEEE
Design Automation Conference , pp. 1153–1158, 2022.
Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry. Accelerated sparse
neural training: A provable and eﬃcient method to ﬁnd n: m transposable masks. Advances in Neural
Information Processing Systems , 34:21099–21111, 2021.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint
arXiv:1602.07360 , 2016.
15Published in Transactions on Machine Learning Research (10/2023)
Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding, and Zhangyang Wang. Training your sparse
neural network better with any mask. In International Conference on Machine Learning , pp. 9833–9844.
PMLR, 2022.
Yu Ji, Ling Liang, Lei Deng, Youyang Zhang, Youhui Zhang, and Yuan Xie. Tetris: Tile-matching the
tremendous irregular sparsity. Advances in Neural Information Processing Systems , 31, 2018.
Jiazhi Jiang, Dan Huang, Jiangsu Du, Yutong Lu, and Xiangke Liao. Optimizing small channel 3d convolu-
tion on gpu with tensor core. Parallel Computing , 113:102954, 2022.
Marc Jorda, Pedro Valero-Lara, and Antonio J Pena. Performance evaluation of cudnn convolution algo-
rithms on nvidia volta gpus. IEEE Access , 7:70461–70473, 2019.
Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and
Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference
on Machine Learning , pp. 5544–5555. PMLR, 2020.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for eﬃcient
convnets. arXiv preprint arXiv:1608.08710 , 2016.
Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Eapruning: Evolutionary pruning for vision transformers and
cnns.arXiv preprint arXiv:2210.00181 , 2022a.
Shigang Li, Kazuki Osawa, and Torsten Hoeﬂer. Eﬃcient quantized sparse matrix operations on tensor
cores.arXiv preprint arXiv:2209.06979 , 2022b.
Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pech-
enizkiy, ZhangyangWang, andDecebalConstantinMocanu. Sparsetrainingviaboostingpruningplasticity
with neuroregeneration. Advances in Neural Information Processing Systems , 34:9908–9922, 2021.
Yufan Liu, Jiajiong Cao, Bing Li, Weiming Hu, and Stephen Maybank. Learning to explore distillability
and sparsability: a joint framework for model compression. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 2022.
ZhuangLiu, JianguoLi, ZhiqiangShen, GaoHuang, ShoumengYan, andChangshuiZhang. Learningeﬃcient
convolutional networks through network slimming. In Proceedings of the IEEE international conference
on computer vision , pp. 2736–2744, 2017.
Yucheng Lu, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov, Christopher De Sa, and Amir Yazdan-
bakhsh. Step: Learning n: M structured sparsity masks from scratch with precondition. arXiv preprint
arXiv:2302.01172 , 2023.
Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu, Yanzhi Wang, Yen-Kuang Chen, Rong
Jin, and Yuan Xie. Eﬀective model sparsiﬁcation by scheduled grow-and-prune methods. arXiv preprint
arXiv:2106.09857 , 2021.
Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu, and Xing Sun. Pruning ﬁlter
in ﬁlter. Advances in Neural Information Processing Systems , 33:17629–17640, 2020.
Asit Mishra, Jorge Albericio Latorre, Jeﬀ Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu,
and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378 ,
2021.
Hesham Mostafa and Xin Wang. Parameter eﬃcient training of deep convolutional neural networks by
dynamic sparse reparameterization. In International Conference on Machine Learning , pp. 4646–4655.
PMLR, 2019.
16Published in Transactions on Machine Learning Research (10/2023)
Jinhyuk Park and Albert No. Prune your model before distill it. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XI , pp. 120–136. Springer,
2022.
Alexandra Peste, Eugenia Ioﬁnova, Adrian Vladu, and Dan Alistarh. Ac/dc: Alternating com-
pressed/decompressed training of deep neural networks. Advances in Neural Information Processing Sys-
tems, 34:8557–8570, 2021.
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. C. Chen. Mobilenetv2: Inverted residuals and linear
bottlenecks. IEEE, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
Wei Sun, Aojun Zhou, Sander Stuijk, Rob Wijnhoven, Andrew O Nelson, Henk Corporaal, et al. Domi-
nosearch: Find layer-wise ﬁne-grained n: M sparse schemes from dense neural networks. Advances in
neural information processing systems , 34:20721–20732, 2021.
Kai Sheng Tai, Taipeng Tian, and Ser-Nam Lim. Spartan: Diﬀerentiable sparsity via regularized transporta-
tion.arXiv preprint arXiv:2205.14107 , 2022.
Yijun Tan, Kai Han, Kang Zhao, Xianzhi Yu, Zidong Du, Yunji Chen, Yunhe Wang, and Jun Yao. Acceler-
ating sparse convolution with column vector-wise sparsity. In Advances in Neural Information Processing
Systems, 2022.
Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang Xu. Scop: Scientiﬁc
controlforreliableneuralnetworkpruning. Advances in Neural Information Processing Systems , 33:10936–
10947, 2020.
Qinglin Wang, Dongsheng Li, Xiandong Huang, Siqi Shen, Songzhu Mei, and Jie Liu. Optimizing ﬀt-
based convolution on armv8 multi-core cpus. In Euro-Par 2020: Parallel Processing: 26th International
Conference on Parallel and Distributed Computing, Warsaw, Poland, August 24–28, 2020, Proceedings ,
pp. 248–262. Springer, 2020.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep
neural networks. Advances in neural information processing systems , 29, 2016.
Zhuliang Yao, Shijie Cao, Wencong Xiao, Chen Zhang, and Lanshun Nie. Balanced sparsity for eﬃcient
dnn inference on gpu. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 33, pp.
5676–5683, 2019.
Yuxin Zhang, Mingbao Lin, Chia-Wen Lin, Jie Chen, Yongjian Wu, Yonghong Tian, and Rongrong Ji.
Carrying out cnn channel pruning in a white box. IEEE Transactions on Neural Networks and Learning
Systems, 2022.
Yuxin Zhang, Yiting Luo, Mingbao Lin, Yunshan Zhong, Jingjing Xie, Fei Chao, and Rongrong Ji. Bi-
directional masks for eﬃcient n: M sparse training. arXiv preprint arXiv:2302.06058 , 2023.
Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hong-
sheng Li. Learning n: M ﬁne-grained structured sparse neural networks from scratch. arXiv preprint
arXiv:2102.04010 , 2021a.
YangjieZhou,MengtianYang,CongGuo,JingwenLeng,YunLiang,QuanChen,MinyiGuo,andYuhaoZhu.
Characterizing and demystifying the implicit convolution algorithm on commercial matrix-multiplication
accelerators. In 2021 IEEE International Symposium on Workload Characterization (IISWC) ,pp.214–225.
IEEE, 2021b.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the eﬃcacy of pruning for model
compression. arXiv preprint arXiv:1710.01878 , 2017.
17