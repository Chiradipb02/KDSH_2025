Published in Transactions on Machine Learning Research (09/2024)
A Semi-Bayesian Nonparametric Estimator of the Maximum
Mean Discrepancy Measure: Applications in Goodness-of-
Fit Testing and Generative Adversarial Networks
Forough Fazeli-Asl foroughf@hku.hk
Department of Statistics and Actuarial Science
University of Hong Kong
Michael Minyi Zhang mzhang18@hku.hk
Department of Statistics and Actuarial Science
University of Hong Kong
Lizhen Lin lizhen01@umd.edu
Department of Mathematics
The University of Maryland
College Park, MD, USA
Reviewed on OpenReview: https: // openreview. net/ forum? id= lUnlHS1FYT
Abstract
A classic inferential problem in statistics is the goodness-of-fit (GOF) test. Performing such
tests can be challenging when the hypothesized parametric model has an intractable like-
lihood and its distributional form is not available. Bayesian methods for GOF testing can
be appealing due to their ability to incorporate expert knowledge through prior distribu-
tions. However, standard Bayesian methods for this test often require strong distributional
assumptions on the data and their relevant parameters. To address this issue, we propose a
semi-Bayesian nonparametric (semi-BNP) procedure based on the maximum mean discrep-
ancy (MMD) measure that can be applied to the GOF test. We introduce a novel Bayesian
estimator for the MMD, which enables the development of a measure-based hypothesis test
for intractable models. Through extensive experiments, we demonstrate that our proposed
test outperforms frequentist MMD-based methods by achieving a lower false rejection and
acceptance rate of the null hypothesis. Furthermore, we showcase the versatility of our ap-
proachbyembeddingtheproposedestimatorwithinagenerativeadversarialnetwork(GAN)
framework. It facilitates a robust BNP learning approach as another significant application
of our method. With our BNP procedure, this new GAN approach can enhance sample
diversity and improve inferential accuracy compared to traditional techniques.
1 Introduction
Goodness-of-fit (GOF) tests are commonly used to evaluate an empirical data set against a hypothesized
parametricmodel. However,therearecaseswhenthelikelihoodoftheparametricmodelisintractableandthe
explicit form of the model distribution is unavailable, making it challenging to directly assess the model’s fit.
Onesuchexampleisthecaseofdeepgenerativemodels, whereindependentsamplescanbegenerated, butthe
required likelihood function needed for traditional GOF tests is intractable. In such situations, a potential
solution is to use the the maximum mean discrepancy (MMD) measure as an alternative approach for
conductingGOFtests(Grettonetal.,2012a;Keyetal.,2021)inadditiontosomeexistingkernelizedmethods
(Liu et al., 2016). The MMD is a metric on the space of probability distributions and is commonly used in
hypothesistestingtoquantifythedifferencebetweenthedistributionofthedataandthehypothesizedmodel.
1Published in Transactions on Machine Learning Research (09/2024)
It can be conveniently estimated using available samples generated from desired distributions. The MMD
estimator has proven to be effective in various applications, including analyzing large-scale datasets with
high-dimensional features and implementing generative models, especially generative adversarial networks
(GANs).
Bayesian nonparametric methods, while powerful, have received comparatively little attention, especially
regarding their application in estimating the MMD. One of the primary benefits of the Bayesian approach
is that expert knowledge can be incorporated into the prior distributions in a diagnostic setting. Moreover,
a BNP learning procedure can provide a certain level of regularization to the training process. This is
partially a result of placing uncertainty on the sampling distribution of the data, via a Dirichlet process
(DP). Therefore, the lack of such methods in MMD estimation proves to be a hindrance for the statistician
who wishes to be Bayesian without overly strong assumptions. This paper seeks to fill this crucial gap.
Inthispaper, weproposeaBNPestimatorthataccuratelyestimatestheMMDkernel-basedmeasurebetween
an intractable parametric model and an unknown distribution. To develop the procedure, we place the
DP prior solely on the unknown distribution. Therefore, we refer to this procedure as a semi-Bayesian
nonparametric (semi-BNP) estimator. Having established our MMD estimator, we demonstrate that we can
generalize the bootstrap procedure given in Dellaporta et al. (2022) beyond posterior parameter inference.
First, we apply our estimator in a variety of hypothesis testing problems. Next, we introduce a robust
Bayesiannonparametriclearning(BNPL)approachfortrainingGANsbasedonsimulatingfromtheposterior
distribution on the parameter space of the generator. Our approach utilizes the aforementioned estimator
as a robust discriminator between the generator’s distribution and a DP posterior on the empirical data
distribution. Specifically, our framework unifies concepts of the MMD measurement and the BNP inference
to leverage their respective benefits into a single discriminator. Furthermore, we will investigate the ability of
our discriminator to reduce mode collapse and increase the ability of the generator to fool the discriminator
more effectively than the frequentist counterpart for GAN training.
Thepaperisorganizedasfollows: InSection2,wereviewpreviousworksandmethodsrelatedtoourproposed
technique. We then introduce our novel semi-BNP estimator for the MMD measure between an unknown and
intractableparametricdistributioninSection3, andprovidetheoreticalpropertiesofourproposedestimator.
In Section 4, we utilize our semi-BNP estimator of the MMD measure to create a powerful GOF test based on
the relative belief (RB) ratio, which serves as the Bayesian evidence to judge the null hypothesis. Moreover,
Section 5 outlines the incorporation of the semi-BNP estimator as the discriminator in the GAN architecture.
This results in a robust BNPL procedure that accurately estimates the generator’s parameters for generating
realistic samples. The section also discusses the theoretical properties of the proposed discriminator, such
as robustness and consistency. We evaluate the novel semi-BNP procedures for hypothesis testing and GAN
training through numerical experiments in Section 6. Lastly, we conclude the paper in Section 7 and discuss
potential future directions. All proofs, algorithms, notations, and additional experiments are given in the
Appendix.
2 Previous Work
Our proposed method consists of two fundamental components: the MMD measure and the DP prior. First,
we will review these two concepts.
2.1 Maximum Mean Discrepancy Measure
For a given data space X, consider the random variables XandY, drawn from distributions F1andF2
respectively. Here, F1andF2belong to B(X), which represents the set of Borel probability distributions on
X. We consider the discrepancy d:B(X)×B(X)→[0,∞)through the integral pseudo-probability metric
(IPM) (Müller, 1997), defined as shown in (1). The class of functions Fis designed to be rich enough to
distinguish between F1andF2, and restrictive enough to provide accurate estimates based on a finite sample.
dIPM(F1,F2) = sup
h∈F|EF1(h(X))−EF2(h(Y)))|. (1)
2Published in Transactions on Machine Learning Research (09/2024)
The MMD is then defined by considering F={h∈Hk|||h||Hk≤1}, which represents a unit ball in a
reproducing kernel Hilbert space (RKHS) Hkwith associated kernel k:X×X→R. In this context, ||·||Hk
denotes the norm function in the RKHS. The function k(·,·)is positive definite, such that for any function
h∈Hkand any X∈X,h(X) =⟨h,k(X,·)⟩Hk, where⟨·,·⟩Hkrepresents the inner product in Hk. Consider
functionµF1(·) =EF1[k(X,·)]∈Hk, which is defined as the kernel mean embedding of the distribution
F1in Gretton et al. (2012a). Then, for given X,X′i.i.d.∼F1,Y,Y′i.i.d.∼F2, ifEF(/radicalbig
k(X,X))<∞for all
F∈B(X), the MMD is given by
MMD2(F1,F2) =||µF1−µF2||2
Hk=EF1[k(X,X′)]−2EF1,F2[k(X,Y)] +EF2[k(Y,Y′)].(2)
Note that MMD2(F1,F2) = 0if and only if F1=F2, when Hkis auniversal RKHS defined on a compact
metric space Xandk(·,·)iscontinuous (Gretton et al., 2012a, Theorem 5). In practice, distributions F1
andF2are not accessible, and then the biased, empirical estimator of (2) (V-statistic) is calculated using
empirical distributions F1,nandF2,mas
MMD2(F1,n,F2,m) =1
n2n/summationdisplay
i,j=1k(Xi,Xj)−2
mnn/summationdisplay
i=1m/summationdisplay
j=1k(Xi,Yj) +1
m2m/summationdisplay
i,j=1k(Yi,Yj), (3)
where X1,...,Xnis a sample from F1andY1,...,Ymis a sample generated from F2.
Recently, Key et al. (2021) proposed a GOF test using the MMD measure when the hypothesized model
belongs to a parametric family of intractable models. It was proposed to be employed in training generative
models such as toggle-switch models and GANs. There are also numerous generative models closely linked
to the implementation of MMD in GANs, which can be found in Briol et al. (2019), Niu et al. (2023), Oates
(2022), and Bharti et al. (2023). These models offer distinct MMD estimators that are specifically designed
to further improve the MMD’s capability in estimating the generator’s parameters.
2.2 Bayesian Methods: Approximate Bayesian Computation, the Dirichlet Process and Bayesian
Nonparametric Learning
Previous work in simulation-based inference has largely focused on applying discrepancy measures from a
frequentist nonparametric (FNP) perspective. A Bayesian perspective on simulation-based inference involves
asimilarmethodology,usingapproximateBayesiancomputation(ABC)toestimatethemodelparametersvia
simulation (Beaumont et al., 2002). In ABC, we first sample a proposal by sampling from a prior distribution
placed on the parameter space of the generative model (Step 1). Rather than inferring parameters directly
from the posterior distribution, we compare the summary statistics of the simulated data, given the current
state of the parameters, with those of the observed data using a discrepancy measure (Step 2). The simulated
parameter values corresponding to the accepted summary statistics are retained if the distance falls within
a predetermined threshold (Step 3).
Identifying informative summary statistics in ABC is a challenging task and it depends on the specific
application of the data being analyzed, such as the mean effective heterozygosity or the mean of variance
in repeat numbers in genetic populations (Blum & François, 2010; Csilléry et al., 2012). An inappropriate
choice may result in poor posterior inference from the data (Robert et al., 2011; Aeschbacher et al., 2012).
One solution proposed by Park et al. (2016) is to use the MMD metric between simulated and real data
distributions to avoid manually selecting the summary statistics. However, as the threshold approaches zero,
ABC tends to the standard Bayesian posterior, which is susceptible to model misspecification and lacks
robustness (Dellaporta et al., 2022)1. To address these two issues, generalized Bayesian inference (GBI)
proposes an alternative method by replacing the likelihood in the posterior distribution with the exponential
of a robust loss function.
Foradetailedexploration, Jewsonetal.(2018)offersvariousexamples, includingtheuseofexponentialofthe
HellingerdivergencewithintheGBI.WithintheGBIframework, therearetwoprominentproceduresthatuse
1A detailed discussion is given in (Dellaporta et al., 2022, Sections 2); however, we have provided some details in the
Appendix for easier access.
3Published in Transactions on Machine Learning Research (09/2024)
the MMD loss. Chérief-Abdellatif & Alquier (2020) propose a pseudo-likelihood based on the MMD metric
and approximate the posterior using variational inference. Pacchiardi & Dutta (2021) extend this method to
a more general Bayesian likelihood-free model using stochastic gradient Markov chain Monte Carlo (MCMC)
to perform posterior inference2. Stein variational gradient descent (Liu & Wang, 2016) is another variational
inference method that uses gradient-based updates derived from Stein’s method and kernel functions. It
leverages Bayesian principles to iteratively move particles to match the posterior distribution by minimizing
Kullback-Leibler divergence, particularly when exact inference is computationally intractable (Feng et al.,
2017).
However, Dellaporta et al. (2022) noted that the performance of GBI is very sensitive to the choice of a
learning rate and that there is no general heuristic for selecting this hyperparameter. Additionally, these
calculations often require MCMC sampling methods, which can impose a significant computational burden.
To address these issues, Dellaporta et al. (2022) developed an MMD posterior bootstrap procedure following
the BNPL strategy developed in Lyddon et al. (2018; 2019); Fong et al. (2019). In this BNPL strategy, a
BNP prior is defined on F, leading to a BNP posterior on F, denoted by Fpos. The key idea is that any
posterior on the generator’s parameter space Wcan be derived by mapping Fposthrough the push-forward
measure
ω∗(Fpos) := arg min
ω∈Wδ(Fpos,FGω),
which is visually depicted in Dellaporta et al. (2022, Figure 1). In particular, Dellaporta et al. (2022)
considered Fposas the DP posterior and δas the MMD measure.
The DP, introduced by Ferguson (1973), is a commonly used prior in Bayesian nonparametric methods.
It can be viewed as an infinite-dimensional generalization of the Dirichlet distribution constructed around
H(the base measure), a fixed probability measure, whose variation is controlled by a(the concentration
parameter), a positive real number. To formally define the DP, consider a space Xwith aσ-algebra Aof
subsets of X. For a base measure Gon(X,A)anda>0, a random probability measure F={F(A) :A∈A}
is called a DP on (X,A), denoted by Fpri:= (F∼DP(a,H)),if for every measurable partition A1,...,Ak
ofXwithk≥2,the joint distribution of the vector (F(A1),...,F (Ak))has the Dirichlet distribution with
parameters (aH(A1),...,aH (Ak)). It is assumed that H(Aj) = 0impliesF(Aj) = 0with probability one.
One of the most important properties of the DP is the conjugacy property–when the sample X1:n=
(X1,...,Xn)is drawn from F∼DP(a,H), the posterior distribution of Fgiven X1:n, denoted by Fpos, is
also a DP with concentration parameter a+nand base measure
H∗=a(a+n)−1H+n(a+n)−1Fn,
whereFndenotes the empirical cumulative distribution function (ECDF) of the sample X1:n. Note that,
H∗is a convex combination of the base measure HandFn. A guideline for choosing the hyperparameters
aandHfor the test of equality distributions will be covered in Section 4.
In previous work, there are several BNP GOF tests (Al-Labadi & Evans, 2018; Al-Labadi et al., 2021a;b), as
well as two-sample tests (Al-Labadi & Zarepour, 2017; Al-Labadi, 2021) and a multi-sample test (Al-Labadi
et al., 2022a), that are closely connected to the posterior-based distance estimation employed in the BNPL
procedure of Dellaporta et al. (2022). These methods are developed using different discrepancy measures to
compare the distance between DP posteriors, placed on unknown distributions, with the corresponding one
between DP priors. However, unlike our proposed method, none of them employ the MMD measure.
Sethuraman (1994) proposed an infinite series representation as an alternative definition for DP. The con-
struction of Sethuraman (1994) is known as the stick-breaking representation and is a popularly used method
in DP inference. Particularly, for a sequence of identically distributed (i.i.d.) random variables {βi}i≥1from
Beta(1,a), letw1=β1, andwi=βi/producttexti−1
j=1(1−βj), fori≥2. Then, the stick-breaking representation is
given byFSB=/summationtext∞
i=1wiδYi,where{Yi}i≥1is a sequence of i.i.d. random variables from H. However, Zare-
pour & Al-Labadi (2012) addressed some difficulties in using these representations. Meanwhile, Ishwaran &
2A comprehensive list of other GBI procedures for addressing this issue can be found in Dellaporta et al. (2022).
4Published in Transactions on Machine Learning Research (09/2024)
Zarepour (2002) proposed a finite representation to facilitate the simulation of the DP. Let
Fpri
N=N/summationdisplay
i=1Ji,NδYi,
where (J1,N,...,JN,N)∼Dirichlet (a/N,...,a/N ), andYii.i.d.∼H. Ishwaran & Zarepour (2002) showed
that{FN}∞
N=1converges in distribution to F, whereFNandFare random values in the space M1(R)of
probability measures on Rendowed with the topology of weak convergence. Thus, to generate {Ji,N}N
i=1
putJi,N= Γi,N//summationtextN
i=1Γi,N, where{Γi,N}N
i=1is a sequence of i.i.d. Gamma (a/N, 1)random variables
independent of{Yi}N
i=1. This form of approximation leads to some results in subsequent sections.
To determine the number of DP approximation terms, we apply a random stopping rule, inspired by the
method described in Zarepour & Al-Labadi (2012). This rule, given a specific ϵ∈(0,1), is defined as:
N= inf/braceleftigg
j:Γj,j/summationtextj
i=1Γi,j<ϵ/bracerightigg
. (4)
3 A Semi-BNP MMD Estimator
This section introduces our semi-BNP estimator for approximating the MMD measure. We consider a sce-
nario where F1represents a completely unknown distribution, while F2represents an intractable parametric
distribution with a complex generating process. For a given sample Y1,...,YmfromF2and by assuming
Fpri
1:= (F1∼DP(a,H))for a non-negative value aand a fixed probability measure H, we propose the
prior-based MMD estimator as
MMD2
BNP(Fpri
1,N,F2,m) =N/summationdisplay
ℓ,t=1Jℓ,NJt,Nk(Vℓ,Vt)−2
mN/summationdisplay
ℓ=1m/summationdisplay
t=1Jℓ,Nk(Vℓ,Yt) +1
m2m/summationdisplay
ℓ,t=1k(Yℓ,Yt),(5)
where (J1,N,...,JN,N)is sampled from Dirichlet (a/N,...,a/N ),V1,...,VNi.i.d.∼H, andNis the number
of terms in the DP approximation/summationtextN
ℓ=1Jℓ,NδVℓproposed by Ishwaran & Zarepour (2002). Since we only
impose the DP prior on the distribution of the real data, we refer to the approach as a semi-BNP procedure.
Theorem 1 For a non-negative real value aand fixed probability distribution H, letFpri
1:= (F1∼
DP(a,H)),HNbe the ECDF corresponding to H, andk(·,·)be any continuous kernel function with feature
space corresponding to a universal RKHS defined on a compact metric space X. Assume that|k(z,z′)|<K,
for any z,z′∈Rd. Then,
i.MMD2
BNP(Fpri
1,N,F2,m)a.s.−−→ MMD2(HN,F2,m), asa→∞,
ii. E (MMD2
BNP(Fpri
1,N,F2,m))→MMD2(H,F 2)asa→∞,N→∞, andm→∞,
iii. E (MMD2
BNP(Fpri
1,N,F2,m))<MMD2(H,F 2) + 3K, for anyN,m∈Nanda∈R+,
where “a.s.−−→” denotes the almost surely convergence, Ndenotes the natural numbers and R+denotes the
positive real numbers.
After observing samples X1,...,XnfromF1and considering V∗
1,...,V∗
Ni.i.d.∼H∗, and (J∗
1,N,...,J∗
N,N)∼
Dirichlet (a+n
N,...,a+n
N), we update the prior-based MMD estimator (5) to the posterior one as
MMD2
BNP(Fpos
1,N,F2,m) =N/summationdisplay
ℓ,t=1J∗
ℓ,NJ∗
t,Nk(V∗
ℓ,V∗
t)−2
mN/summationdisplay
ℓ=1m/summationdisplay
t=1J∗
ℓ,Nk(V∗
ℓ,Yt) +1
m2m/summationdisplay
ℓ,t=1k(Yℓ,Yt),(6)
where,H∗=a/(a+n)H+n/(a+n)F1,n,F1,ndenotes the empirical distribution of observed data, and
Fpos
1,Nrefers to the approximation of F1|X1:n∼DP(a+n,H∗). The following Theorem presents asymptotic
properties of MMD2
BNP(Fpos
1,N,F2,m).
5Published in Transactions on Machine Learning Research (09/2024)
Theorem 2 For a non-negative real value aand fixed probability distribution H, letFpri
1:= (F1∼
DP(a,H))andk(·,·)be any continuous kernel function with feature space corresponding to a universal
RKHS defined on a compact metric space X. Assume that|k(z,z′)|< K, for any z,z′∈Rd. Then, for a
given sample X1,...,Xnfrom distribution F1,
i.asa→∞(informative prior),
a.MMD2
BNP(Fpos
1,N,F2,m)a.s.−−→ MMD2(HN,F2,m),
b.E(MMD2
BNP(Fpos
1,N,F2,m))→MMD2(H,F 2),N→∞, andm→∞,
ii.asn→∞(consistency),
a.MMD2
BNP(Fpos
1,N,F2,m)a.s.−−→ MMD2(F1,N,F2,m),
b.E(MMD2
BNP(Fpos
1,N,F2,m))→MMD2(F1,F2), asN→∞,n→∞, andm→∞.
We conclude this section by presenting a corollary that plays a significant role in the two following sections.
Corollary 3 Under the assumption of Theorem 2,
i.asa→∞,N→∞,m→∞, then,
a.E(MMD2
BNP(Fpri
1,N,F2,m))→0, if and only if H=F2,
b.E(MMD2
BNP(Fpos
1,N,F2,m))→0, if and only if H=F2,
ii.for any choice of aandH,E(MMD2
BNP(Fpos
1,N,F2,m))→0, if and only if F1=F2, asN→∞, and
n→∞, andm→∞.
Besides the theoretical result presented in this section, we provided the density function of the posterior-
based estimator (6) compared to the baseline (3) in Figure 10 in the Appendix. This comparison, made as
sample sizes approach infinity under the null hypothesis, helps to examine and understand the asymptotic
distribution. The results indicated faster density convergence around zero for our proposed estimator.
4 Constructing a GOF Test with RB Ratio
In this section we introduce our novel semi-BNP test, utilizing the proposed estimator discussed in the
previous section, to evaluate the hypothesis H0:F1=F2. Let the RKHS be universal and the sample space
be compact, we put forward an equivalent formulation to test the hypothesis
H0: MMD2(F1,F2) = 0, (7)
using the RB3ratio, introduced by Evans (2015), as the Bayesian evidence.
By relating our problem to RB inference, with Ψ = MMD2(F1,F2)andψ0= 0, the RB ratio measures
the change in belief regarding the true value of ψ0, froma priori toa posteriori , given an observed sample
x1,...,xnfromF1. It can be expressed by
RBMMD2(F1,F2)(0|x1:n) =πMMD2(F1,F2)(0|x1:n)
πMMD2(F1,F2)(0), (8)
where,πMMD2(F1,F2)(·|x1:n)4andπMMD2(F1,F2)(·)denote the probability density functions (PDFs) of the
estimators given by (6) and (5), respectively.
3A detailed discussion on the RB ratio is provided in the Appendix.
4Note that the subscript (F1,F2)may be omitted whenever it is clear in the context.
6Published in Transactions on Machine Learning Research (09/2024)
The density in the denominator of (8) must support H0in order to reflect how well the data can support
the null hypothesis based on the comparison between the prior and the posterior, utilizing the fundamental
conceptsoftheRBratio. Here, supporting H0byπMMD2(·)meanstoplacemostofthepriormassonzero. To
enforce this term on πMMD2(·), it is enough to set H=F2inDP(a,H), which is deduced from the Theorem
1, part (iii), to make the denominator of (8) concentrate most of its mass around zero; otherwise, any other
choice forHcontradicts the RB ratio rule in GOFs. In this case, when H0is not true, for a fixed aandK
(the upper bound of the kernel k(·,·)), the range of MMD2
BNP(Fpri
1,N,F2,m)should, on average, vary within a
smaller range than its corresponding posterior version. Specifically, this range should be (0,3K)5, compared
to(0,MMD2(H∗,F2) + 3K)which can be similarly obtained for the posterior-based MMD estimator. This
indicates that H0should be rejected, as it is desirable. On the other hand, when H0is true, although
the prior- and posterior-based MMD estimators have approximately the same range of variation (0,3K),
Corollary 3(ii) implies that increasing the sample size leads the posterior to provide stronger evidence in
favor of the null hypothesis compared to the prior, resulting in the acceptance of H0. However, the above
discussion provides a general understanding rather than a tight inequality for comparison around zero. For
accuracy, we use a critical value di0/Mand the interval [0,di0/M)to approximate the RB, addressed in (10).
With regards to choosing the concentration parameter ain our proposed test, we note that acontrols the
variationof FpriaroundH, whichinturncontrolsthestrengthofbeliefinthetruthof H0. Itisrecommended
to choosea < n/ 2based on the definition of H∗inFpos(Al-Labadi & Zarepour, 2017). The idea behind
using such a value of ais to avoid the excessive effect of the prior Hon the test results by considering
the chance of sampling from the observed data to be at least twice the chance of generating samples from
H. Corollary 3(i) also clearly point to this issue in the informative prior case, as both expectations of
MMD2
BNP(Fpos
1,N,F2,m)andMMD2
BNP(Fpri
1,N,F2,m)tend to 0asa→∞,N→∞, andm→∞, iffH=F2.
Hence, both prior and posterior densities in (8) should be heavily massed and coincide with each other at
zero. It causes the value of (8) to become very close to 1, based on which no decision can be made about
H0.
For the proposed test, we will empirically choose ato be less than n/2and then compute (8). However,
some computational methods in the literature have been proposed to elicit athat one may be interested
in using (see Al-Labadi et al., 2022b; Al-Labadi, 2021). Generally, for a given a, Corollary 3(ii) implies
that MMD2
BNP(Fpos
1,N,F2,m)should be more dense than MMD2
BNP(Fpri
1,N,F2,m)at 0 if and only if H0is true.
Hence, the value of (8) presents evidence for or against H0, ifRBMMD2(0|x1:n)>1orRBMMD2(0|x1:n)<1,
respectively. Following Evans (2015), for t∈R+, the calibration of (8) is defined as:
StrMMD2(0|x1:n) = Π MMD2/parenleftbig
RBMMD2(t|x1:n)≤RBMMD2(0|x1:n)|x1:n/parenrightbig
, (9)
where, ΠMMD2(·|x1:n)istheposteriorprobabilitymeasurecorrespondingtothedensity πMMD2(·|x1:n). When
(7) is false, a small value of (9) provides strong evidence against ψ0, whereas a large value suggests weak
evidence against ψ0. Conversely, when (7) is true, a small value of (9) indicates weak evidence in favor of
ψ0, while a large value suggests strong evidence in favor of ψ0. Particular attention should be paid here to
the computation of (8) and (9). The densities used in (8) do not have explicit forms. Thus, we use their
corresponding ECDF based on ℓsample sizes to estimate (8) and (9), respectively, as
/hatwidestRBMMD2([0,ˆdi0/M)|x1:n) =ˆΠMMD2(ˆdi0/M|x1:n)
ˆΠMMD2(ˆdi0/M), (10)
/hatwidestStrMMD2([0,ˆdi0/M)|x1:n) =/summationdisplay
D/parenleftbigˆΠMMD2(ˆd(i+1)/M|x1:n)−ˆΠMMD2(ˆdi/M|x1:n)/parenrightbig
, (11)
where,D=/braceleftig
i0≤i≤M−1 :/hatwidestRBMMD2/parenleftbig
[ˆdi/M,ˆd(i+1)/M)|x1:n/parenrightbig
≤/hatwidestRBMMD2/parenleftbig
[0,ˆdi0/M)|x1:n/parenrightbig/bracerightig
,in whichM
is a positive number, ˆdi/Mis the estimate of di/M,the(i/M)-th prior quantile of (5),
/hatwidestRBMMD2([ˆdi/M,ˆd(i+1)/M)|x1:n) =ˆΠMMD2(ˆdi+1
M|x1:n)−ˆΠMMD2(ˆdi
M|x1:n)
ˆΠMMD2(ˆdi+1
M)−ˆΠMMD2(ˆdi
M)(12)
5This interval is the simplified form of (0,MMD2(H,F 2) + 3K), where MMD2(H,F 2)is zero based on setting H=F.
7Published in Transactions on Machine Learning Research (09/2024)
andi0in(10)ischosensothat i0/Misnottoosmall(typically i0/M= 0.05). Heredi/Misthethresholdvalue
that the prior-based estimator (the semi-BNP statistic under the null hypothesis) and the posterior-based
estimator is compared against. Further details are available in Algorithm 1 and Figure 6 in the Appendix.
For fixedM, asℓ→∞,then ˆdi/Mconverges almost surely to di/Mand (10) and (12) converge almost
surely toRBMMD2([0,di0/M)|x1:n)andRBMMD2([di/M,d(i+1)/M)|x1:n), respectively. These are obtained
from the left Riemann sum approximation and the relationship between CDFs and PDFs with considering
di/M+ϵ=d(i+1)/Mfor any small ϵ>0, as follows:
RBMMD2([di
M,di+1
M)|x1:n) =/integraltextdi+1
M
di
MπMMD2(t|x1:n)dt
/integraltextdi+1
M
di
MπMMD2(t)dt=ϵπMMD2(di
M|x1:n)
ϵπMMD2(di
M)≈RBMMD2(di/M|x1:n).
Consequently, (11) converges almost surly to StrMMD2([0,di0/M)|x1:n) =/summationtext
D′/integraltextd(i+1)/M
di/MπMMD2(t|x1:n)dt,
whereD′is defined similarly to Dby replacing/hatwidestRBand/hatwidedi/MwithRBanddi/MinD. The following result
from Al-Labadi & Evans (2018, Proposition 6) gives the consistency of the proposed test. If H0is true, then
RBMMD2([0,di0/M)|x1:n)a.s.−−→M/i 0(>1)andRBMMD2([di/M,d(i+1)/M)|x1:n)a.s.−−→ 0fori0≤i≤M−1,
asn→∞, which implies that StrMMD2([0,di0/M)|x1:n)converges to 1 almost surely. Otherwise, both
RBMMD2([0,di0/M)|x1:n)andStrMMD2([0,di0/M)|x1:n)converge to 0.
The proposed test is suggested to overcome several limitations present in its frequentist counterparts. In
a frequentist test, for a given permissible type I error rate denoted by α, the test rejects H0if the value
ofMMD2(F1,F2)is greater than some threshold cα. The corresponding p-value for this test can also be
computed by Pr(MMD2(F1,F2)≥cα|H0), which leads the test to reject H0if it is less than α. However, Li
et al. (2017) noted that if MMD2(F1,F2)is not significantly larger than cαfor some finite samples when H0
is not true, the null hypothesis H0is not rejected. Furthermore, there is a trade-off between the permissible
type I error rate αand the probability of failing to reject a false null hypothesis (type II error), denoted
byβ, asα+β≤1. Decreasing one error rate inevitably leads to an increase in the other, indicating that
we cannot arbitrarily drive to type I error rate to zero. Moreover, the p-values are uniformly distributed
between 0 and 1 under the null hypothesis. In fact, it does not allow evidence for the null, which is one of
their weaknesses compared to Bayesian criteria in hypothesis testing problems.
5 Embedding the Semi-BNP Estimator in GAN Learning
In this section, we propose a BNPL procedure that leverages a posterior-based MMD estimator to train
GANs. It is inspired by the idea presented in Dellaporta et al. (2022) to approximate the posterior on the
generator’s parameters.
5.1 Generative Adversarial Networks
TheGAN(Goodfellowetal.,2014)isamachinelearningtechniqueusedtogeneraterealistic-lookingartificial
samples. In this context, the discriminator Dcan be viewed as a black box that uses a discrepancy measure
δto differentiate between the real and fake data. Meanwhile, the generator Gωis trained by optimizing a
simpler objective function, given by
arg min
ω∈Wδ(F,FGω),
whereFGωrepresents the distribution of the generator. In fact, Dattempts to continuously train Gωby
computing distance δbetweenFandFGωuntil this distance is negligible, making their difference indistin-
guishable. This technique leads to omitting the neural network from D, whose optimization may lead to a
vanishing gradient. An effective measure of discrepancy for δis the MMD, which is a kernel-based measure
that offers several desirable properties such as consistency and robustness in generating samples (Gretton
et al., 2012a; Chérief-Abdellatif & Alquier, 2022).
8Published in Transactions on Machine Learning Research (09/2024)
Numerous frequentist GANs applying the MMD measure to estimate the generator’s parameters can be
found in the literature. (Dziugaite et al., 2015; Bińkowski et al., 2018; Li et al., 2015). These models
are devised by comparing the generated fake samples with real samples. In addition to the MMD, several
other discrepancy measures are commonly used for GANs, including the f-divergence measure (Nowozin
et al., 2016), the Wasserstein distance (Arjovsky et al., 2017), and the total variation distance (Lin et al.,
2018). Nevertheless, the MMD kernel-based measure is remarkably robust against outliers and has the
exceptional ability to capture complex dependencies in the data (Sejdinovic et al., 2013; Chérief-Abdellatif
& Alquier, 2022). This makes it highly effective in handling model misspecification and detecting subtle
differences between distributions. This property is particularly useful for modeling complicated datasets
such as images, which are a common application for GANs. Moreover, Al-Labadi et al. (2022a) used the
energy distance to expand their procedure, which is a member of the larger class of MMD kernel-based
measures (Sejdinovic et al., 2013). From here, it is obvious that choosing among a larger class can lead to
designing more sensitive discrepancy measures to detect differences.
Although a particular case of the test in Al-Labadi et al. (2022a) can be used to compare two distributions,
it cannot be easily used as a discriminator in the minimum distance estimation technique to train GANs.
In GANs, the objective is to update the parameter ωof the deterministic generative neural network Gω.
Therefore, treating FGωas an unknown distribution on which we place a BNP prior is nonsensical. Con-
sequently, a more suitable distance criterion is required to compare an intractable parametric distribution
with an unknown distribution.
5.2 Architecture
Various GAN architectures can be found in the literature to model complex high-dimensional distributions.
However, we consider the original architecture of the generator network in Goodfellow et al. (2014). Specif-
ically, we follow the neural network architecture in Goodfellow et al. by setting the generator, Gω, to
be a multi-layer neural network with parameters ω, rectified linear units activation function for each hid-
den layer, and a sigmoid function for the last layer (output layer). The generator receives a noise vector
U= (U1,...,Up)as its input nodes, where p<d, and each element of Uis independently drawn from the
same distribution FU.
Our discriminator follows the work of Dziugaite et al. (2015); Li et al. (2015) by optimizing the objective
function:
arg min
ω∈WMMD2
BNP(Fpos
N,FGω,m),
but we now estimate the MMD using our semi-BNP method. In fact, our BNPL procedure implicitly
approximates samples from the posterior distribution on the parameter ωby minimizing the posterior-based
MMD estimator. For any differentiable kernel function k(·,·), this optimization is performed by computing
the following gradient based on samples from F|X1:n∼DP(a+n,H∗), as
∂MMD2
BNP(Fpos
N,FGω,m)
∂ωi=N/summationdisplay
ℓ=1m/summationdisplay
t=1/braceleftigg
∂
∂Yt/bracketleftigg
−2
mm/summationdisplay
t=1J∗
ℓ,Nk(V∗
ℓ,Yt)
+1
Nm2m/summationdisplay
t,t′=1k(Yt,Yt′)/bracketrightigg
∂Yt
∂ω/bracerightigg
,
where, Yt=Gω(Ut),Ut= (Ut1,...,Utp), andUti’s are generated from a distribution FU, fort= 1,...,m,
andi= 1,...,p. Then, the backpropagation method is applied for calculating partial derivatives∂Yt
∂ωto
update the parameters of Gω.
However, Li et al. (2015, Equation 8) remarked that considering the square root of the MMD measure
given by (2) in the cost function of frequentist GANs is more efficient than using (2) to train network
Gω. They mentioned that since the gradient of/radicalig
MMD2(FN,FGω,m)with respect to ωis the product of
γ1=1
2√
MMD2(FN,FGω,m)andγ2=∂MMD2(FN,FGω,m)
∂ω, thenγ1forces the value of the gradient to be relatively
9Published in Transactions on Machine Learning Research (09/2024)
large, even if both MMD2(FN,FGω,m)andγ2are small. This can prevent the vanishing gradient, which
improves the learning of the parameters of Gωin the early layers of this network. We consider this point in
order to improve our semi-BNP objective function:
arg min
ω∈WMMD BNP(Fpos
N,FGω,m). (13)
Algorithm 2 in the Appendix provides steps for implementing the training.
Letω∗be the optimized parameter of Gωthat minimizes MMD BNP(Fpos
N,FGω,m). Since
MMD BNP(Fpos
N,FGω,m)can be viewed as a semi-BNP estimation of (2), it becomes imperative to assess
the accuracy of this estimation, specifically in terms of how effectively the proposed GAN can generate
realistic samples that faithfully represent the true data distribution (generalization error). Furthermore, it
is crucial to take into consideration the generator’s performance in dealing with outliers which includes a
small proportion of observations that deviate from the clean data distribution F0(robustness). The next
lemma addresses these two concerns.
Lemma 4 LetWbe the parameter space for Gωandω∗∈Wbe the value that optimizes the objective
function (13) and ω′be the true value that minimizes MMD(F,FGω). Assume that F∼DP(a,H)and let
k(·,·)be any continuous kernel function with feature space corresponding to a universal RKHS defined on a
compact metric space Xsuch that|k(z,z′)|<K, for any z,z′∈Rd. For a given sample X1,...,Xnfrom
distribution F:
i.Generalization error:
E(MMD(F,FGω∗))≤MMD(F,FGω′) +2K√n+4aK
a+n+ 2/radicaligg
(a+n+N)K
(a+n+ 1)N.
ii.Robustness: Suppose there exist outliers in the sample data, which arise from a noise distribution Q.
Consider the Hüber’s contamination model (Huber, 1992; Chérief-Abdellatif & Alquier, 2022), given by F=
(1−ϵ)F0+ϵQ, whereϵ∈(0,1
2)is the contamination rate, and the latent variables Z1,...,Zni.i.d.∼Bernoulli(ϵ)
are such that Xii.i.d.∼F0ifZi= 0; otherwise, Xii.i.d.∼Q. Then,
E(MMD(F0,FGω∗))≤min
ω∈WMMD(F0,FGω) + 4ϵ+2K√n+4aK
a+n+ 2/radicaligg
(a+n+N)K
(a+n+ 1)N.
Lemma 4(ii) demonstrates that despite encountering outlier data, FGω∗andF0are negligibly different for
a sufficiently large sample size. This feature results in the majority of the posterior on the parameter space
Wbeing distributed on value ω∗, which is a desirable outcome of the proposed method.
Although the preceding statements investigate properties of the estimated parameters by providing upper
bounds for the expectation of the MMD estimator, the next lemma presents stochastic bounds for the
estimation error in order to assess the posterior consistency.
Lemma 5 Building upon the general assumptions stated in Lemma 4, for a given sample X1,...,Xnfrom
distribution Fin the probability space (X,A,Pr)and anyϵ>0,
i.Pr/parenleftbig
|MMD(Fpos
N,FGω∗,m)−MMD(F,FGω′)|≥h(n,m,K,ϵ ) +|∆1|+|∆2|/parenrightbig
≤2 exp−ϵ2nm
2K(n+m),
ii.Pr (MMD(F,FGω∗)>ϵ)≤1
ϵ/parenleftigg
MMD(F,FGω′) +2K√n+4aK
a+n+ 2/radicaligg
(a+n+N)K
(a+n+ 1)N./parenrightigg
,
where,h(n,m,K,ϵ ) = 2√
K(√n+√m)/√nm+ϵ,∆1= MMD(Fpos
N,FGω∗)−MMD(Fn,FGω′,m), and ∆2=
MMD(F,FGω∗)−MMD(F,FGω′).
A direct consequence of Lemma 5(ii) is that for a fixed value of a,Pr(MMD(F,FGω∗)≥ϵ)→0, asn→∞
andN→∞, for anyϵ>0, when MMD(F,FGω′) = 0(well-specified case). This implies FGω∗converges in
probability to the data distribution Fas the sample size increases in well-specified cases.
10Published in Transactions on Machine Learning Research (09/2024)
The choice of ain the test proposed in Section 4 plays a crucial role in determining the degree of support for
the null hypothesis against the alternative. In the context of approximating the posterior on the parameter
space, the prior choice for Fand determining the strength of belief becomes challenging. We consider a
small value for aas a non-informative prior, following the suggestion by Dellaporta et al. (2022), thanks to
its broad ability to characterize uncertainty (Terenin & Draper, 2017). However, it’s important to note that
settinga= 0as done by Dellaporta et al. (2022) is not always well-defined mathematically, as the DP is
only defined for a>0. Therefore, we opt for a= 10−6.
The main distinction between our BNPL method and the one proposed by Dellaporta et al. (2022) lies in
the fact that we generalize their BNPL procedure beyond estimating parameters and explicitly consider
the terms of the DP posterior approximation and their corresponding weights. Dellaporta et al. used the
following DP approximation:
Fpos
n+N=n/summationdisplay
ℓ=1/tildewideJℓ,nδXℓ+N/summationdisplay
t=1Jt,NδVt,
where (/tildewideJ1:n,n,J1:N,N)∼Dirichlet (1,..., 1,a
N,...,a
N),(X1:n)i.i.d.∼F, and (V1:N)i.i.d.∼H. In contrast, we
employFpos
N=/summationtextN
i=1J∗
i,NδV∗
i, with (J∗
1:N,N)∼Dirichlet (a+n
N,...,a+n
N). Our approach offers an advantage
over the approximation used in Dellaporta et al. (2022) due to its reduced number of terms, significantly
reducing both computational and theoretical complexity, where our method scales O(N)and Dellaporta
et al. scales O(N+n). Additionally, a further difference is that Dellaporta’s bootstrap procedure needs to
query the loss function Btimes to simulate Bposterior parameters, whereas our procedure does not require
a bootstrap algorithm and we only need to simulate a single parameter. Although their bootstrap procedure
is embarrassingly parallelizable, the number of bootstrap samples generally should be a fairly large number
and the typical statistical practitioner does not have access to Bcores to truly parallelize the additional cost
of bootstrap sampling.
5.3 Kernel Settings
In our method, we choose to use the standard radial basis function (RBF) kernel as its feature space
corresponds to a universal RKHS. For a comprehensive understanding of RBF functions, refer to Section D
in the Appendix. Dziugaite et al. (2015); Li et al. (2015) and Li et al. (2017) used the Gaussian kernel in
training MMD-GANs because of its simplicity and good performance. Dziugaite et al. (2015) also evaluated
some other RBF kernels such as the Laplacian and rational quadratic kernels to compare the results of the
MMD-GANs with those obtained based on using Gaussian kernels. They found the best performance by
applying the Gaussian kernel in the MMD cost function.
Hence, we consider the Gaussian kernel function in our proposed procedure. To choose the bandwidth
parameterσ, we follow the idea of considering a set of fixed values of σ’s such as{σ1,...,σT}, then compute
the mixture of Gaussian kernels k(·,·) =/summationtextT
t=1kGσt(·,·), to consider in (6). For each σ(t),0≤kGσt(·,·)≤1;
hence, 0≤k(·,·)≤T, which satisfies the theoretical results presented in the paper. As it is mentioned in Li
et al. (2015), this choice reflects a good performance in training MMD-GANs.
6 Experimental Investigation
In this section, we empirically investigate our proposed methods through comprehensive numerical studies
in the following two subsections, which demonstrate the superior performance of our proposed semi-BNP
test as a standalone test as well as an embedded discriminator for the semi-BNP GAN6.
6.1 The Semi-BNP Test
To comprehensively study test performance evaluation, we consider some major representative examples in
two-sample comparison problems. For this, let y1,...,ynbe a sample generated from F2=N(0d,Id)and
6The relevant codes for the semi-BNP procedure are available at https://github.com/ForoughFazeliasl/SemiBNPMMD.git .
11Published in Transactions on Machine Learning Research (09/2024)
x1,...,xnbe an observed sample generated from each below distributions: F1=N(0d,Id)(No differences),
F1=N(0.5d,Id)(Mean shift), F1=LN(0d,Bd)(Skewness), F1=1
2N(−1d,Id) +1
2N(1d,Id)(Mixture),
F1=N(0d,2Id)(Variance shift), F1=t3(0d,Id)(Heavy tail), and F1=LG(0d,Id)(Kurtosis).
To implement the test, we set ℓ= 1000,M= 20, andϵ= 10−3to be used in Algorithm 1 in the Appendix.
WefirstconsideredthemixtureofsixGaussiankernelscorrespondingtothesuggestedbandwidthparameters
2,5,10,20,40,and80by Li et al. (2015). We found that although this choice can provide good results in
training GANs, it does not provide satisfactory results in hypothesis testing problems.
Instead of using a mixture of several Gaussian kernels, we propose choosing a specific value for the bandwidth
parameter that maximizes the area under the receiver operating characteristic curve (AUC) empirically.
In a binary classifier, which can also be thought of as a two-sample test assessing whether two samples
are distinguishable or not, the receiver operating characteristic (ROC) curve is a plot of true positive rates
(sensitivity) against the false positive rates (1-specificity) based on different choices of threshold to display
the performance of the test. The positive term refers to rejecting H0in (7), while, the negative term refers
to failing to reject H0. The false positive and false negative rates are equivalent to type I and type II errors,
respectively. Hence, a higher AUC indicates a better diagnostic ability of a binary test7.
The ROC curves and AUC values of the synthetic examples are provided in Figure 1 for the sample size
n= 50,d= 60,a= 25, and various values of the bandwidth parameter, including the median heuristic
σMH. The red diagonal line represents the random classifier. An ROC curve located higher than the
diagonal line indicates better test performance and vice versa. It is obvious from Figure 1 that the best
test performance ( AUC = 1 ) is first achieved for the bandwidth parameter 80. For thisσ, the AUC jumps
from zero to 1 as either H0is strongly rejected by too small values of RB (near to 0, the minimum RB)
in alternative experiments or H0is strongly accepted by large values of RB (near to 20, the maximum
RB) in null experiments. As noted in the Appendix, if σis too small or too large, the MMD approaches
zero, resulting in poor performance, as shown in Figure 1. In the context of this paper’s semi-BNP test,
both the prior- and posterior-based MMD estimators converge to zero under these conditions, causing their
corresponding density functions to coincide at zero, resulting in RB≈1and rendering the test unable to
evaluate H0.
Another test of interest is to assess the effect of different hyperparameter settings for aandHthrough
simulation studies to follow our proposed theoretical convergence results. To do this, we generate 100 60-
dimensionalsamplesofsizes n= 50frombothF1=t3(060,I60)andF2=N(060,I60)andrepresenttheresult
of the semi-BNP test by Figure 2 for two choices of the base measure H(H=F2andH=LG(060,I60))
and various values of a(a= 1,..., 1000). In this figure, the solid line represents the average of the RB and
the filled area around the line indicates a 95%confidence interval of the RB over the 100 samples. Figure
2-a clearly shows that by choosing H̸=F2, the test wrongly accepts the null hypothesis. It is because the
prior does not support the null hypothesis mentioned earlier when presenting the RB ratio in Section 4. On
the other hand, when H=F2, Figure 2-b shows good performance for the test at a=n/2. Failing to reject
H0for small values of ais due to the lack of sufficient support from the null hypothesis by the prior. We
remark that the value of adetermines the concentration of the prior FpriaroundH, thus it is obvious that
for small values of a, the test does not perform well. It should also be noted that for any choices of Hin
Figure 2, the ability of the test to evaluate the null hypothesis is reduced by letting ago to infinity, which
can be concluded by Corollary 3(i).
Now, to conduct a more comprehensive investigation, we present the average of RB and its relevant strength
over the 100 samples in Table 1 for n= 30,50. Furthermore, we present the results of the BNP-energy
test by Al-Labadi et al. (2022a) in Table 1, which demonstrate its weak performance in certain scenarios.
Additional results in the power comparison can be found in Section F.1 of the Appendix.
7Since we consider i0/M= 0.05to estimate the RB ratio, the values of RBcan vary between 0 and 20. Therefore, in
computing the AUC for the semi-BNP test, the threshold should vary from 0 to 20. More details for plotting the ROC and
computing the AUC are provided by Algorithm 3 in the Appendix.
12Published in Transactions on Machine Learning Research (09/2024)
Table 1: The average of RB, the average of its strength (Str ), and the relevant AUC out of 100 replications
based on using a= 25,ℓ= 1000,M= 20,ϵ= 10−3in (4), and bandwidth parameter σ= 80in RBF kernel
for two sample of data with n= 30,50.
Example dBNP FNP
MMD Energy MMD Energy
RB(Str) AUC RB(Str) AUC P.value AUC P.value AUC
30 50 30 50 30 50 30 50 30 50 30 50 30 50 30 50
No differences 1 2.08(0.62) 2.41(0.67) 1.78(0.59) 1.91(0.55) 0.50 0.45 0.50 0.49
5 4.06(0.77) 6.91(0.76) 3.46(0.65) 5.99(0.73) 0.48 0.50 0.54 0.52
10 6.21(0.78) 10.74(0.79) 5.92(0.67) 10.42(0.76) 0.50 0.51 0.54 0.47
20 9.62(0.80) 16.02(0.83) 8.24(0.73) 14.76(0.78) 0.46 0.50 0.51 0.50
40 13.07(0.88) 18.85(0.97) 11.56(0.75) 17.58(0.84) 0.51 0.49 0.53 0.46
60 14.09(0.87) 19.71(1) 13.38(0.81) 18.51(0.93) 0.52 0.46 0.50 0.48
80 15.2(0.89) 19.57(1) 14.16(0.87) 19.10(1) 0.46 0.47 0.53 0.56
100 15.83(0.91) 19.74(1) 14.84(0.92) 19.31(1) 0.48 0.46 0.49 0.55
Mean shift 1 0.76(0.24) 0.40(0.09) 0.82 0.960.67(0.21) 0.45(0.11) 0.87 0.900.15 0.05 0.86 0.910.19 0.12 0.79 0.86
5 0.21(0.03) 0.07(0) 0 .99 0.990.28(0.04) 0.09(0.01) 0.98 1 0.01 0.002 1 0 .980.02 0.004 0.97 0.97
10 0.09(0.01) 0.05(0) 1 1 0.17(0.05) 0.02(0) 0 .98 1 0.001 0.001 1 1 0.006 0.004 0.98 1
20 0.09(0.01) 0(0) 1 1 0.09(0.01) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
40 0.08(0) 0(0) 1 1 0.06(0.02) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
60 0.09(0.03) 0(0) 1 1 0.07(0.04) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
80 0.06(0.02) 0(0) 1 1 0.05(0.03) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
100 0.04(0.01) 0(0) 1 1 0.03(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
Skewness 1 0.01(0) 0(0) 0 .99 1 0.07(0) 0(0) 0 .991 0.009 0.001 0.98 1 0.007 0.004 0.94 1
5 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
10 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
20 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
40 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
60 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
80 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
100 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
Mixture 1 0.06(0) 0(0) 0 .90 0.970.19(0.03) 0.04(0) 0 .97 1 0.43 0.38 0.58 0.570.29 0.17 0.69 0.81
5 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.15 0.09 0.84 0.910.06 0.01 0.95 1
10 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.03 0.007 0.95 0.980.02 0.007 0.96 1
20 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.002 0.001 0.96 1 0.01 0.006 1 1
40 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.01 0.006 1 1
60 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.006 0.009 1 1
80 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.008 0.006 1 1
100 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.006 1 1
Variance shift 1 0.87(0.29) 0.85(0.19) 0.71 0.831.10(0.36) 1.08(0.33) 0.53 0.630.46 0.38 0.54 0.570.33 0.21 0.65 0.77
5 0.55(0.12) 0.56(0.15) 0.99 0.991.06(0.35) 0.99(0.32) 0.89 0.980.34 0.20 0.65 0.800.20 0.07 0.82 0.93
10 0.44(0.11) 0.27(0.05) 0.99 1 0.87(0.24) 0.80(0.25) 0.97 1 0.14 0.03 0.85 0.970.10 0.02 0.89 0.97
20 0.34(0.07) 0.08(0) 1 1 0.65(0.17) 0.60(0.13) 0.99 1 0.01 0.001 0.95 1 0.03 0.006 0.95 1
40 0.13(0.01) 0.02(0) 1 1 0.61(0.18) 0.58(0.14) 1 1 0.001 0.001 1 1 0.01 0.004 0.98 1
60 0.12(0.01) 0.01(0) 1 1 0.47(0.10) 0.45(0.11) 1 1 0.001 0.001 1 1 0.006 0.004 1 1
80 0.17(0.01) 0(0) 1 1 0.54(0.12) 0.47(0.11) 1 1 0.001 0.001 1 1 0.005 0.004 1 1
100 0.14(0.01) 0(0) 1 1 0.45(0.10) 0.41(0.08) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
Heavy tail 1 0.93(0.28) 0.66(0.20) 0.89 0.921.19(0.41) 1.10(0.38) 0.70 0.780.43 0.39 0.57 0.560.39 0.36 0.59 0.62
5 0.32(0.06) 0.37(0.08) 0.99 0.990.77(0.24) 0.78(0.23) 0.93 0.990.20 0.11 0.79 0.890.03 0.006 0.97 0.99
10 0.35(0.08) 0.13(0.02) 0.99 1 0.61(0.16) 0.68(0.19) 0.981 0.06 0.007 0.92 0.980.09 0.01 0.90 0.97
20 0.15(0.02) 0(0) 1 1 0.48(0.12) 0.46(0.12) 1 1 0.002 0.001 0.96 1 0.02 0.005 0.96 1
40 0.07(0.01) 0(0) 1 1 0.25(0.04) 0.18(0.04) 1 1 0.001 0.001 1 1 0.005 0.004 1 1
60 0.02(0) 0(0) 1 1 0.22(0.03) 0.14(0.01) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
80 0.01(0) 0(0) 1 1 0.13(0.01) 0.15(0.02) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
100 0.04(0) 0(0) 1 1 0.14(0.01) 0.09(0.01) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
Kurtosis 1 0.47(0.12) 0.19(0.04) 0.89 0.981.09(0.37) 0.88(0.28) 0.77 0.900.28 0.23 0.74 0.720.18 0.11 0.79 0.88
5 0.16(0.03) 0.06(0.01) 1 1 0.63(0.18) 0.41(0.09) 0.96 0.990.04 0.01 0.94 0.980.03 0.008 0.97 0.96
10 0.02(0) 0(0) 1 1 0.35(0.08) 0.32(0.06) 0.971 0.001 0.001 1 1 0.007 0.004 0.96 1
20 0(0) 0(0) 1 1 0.20(0.03) 0.18(0.02) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
40 0(0) 0(0) 1 1 0.06(0.01) 0.06(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
60 0(0) 0(0) 1 1 0.05(0) 0 .04(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
80 0(0) 0(0) 1 1 0.05(0) 0 .03(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
100 0(0) 0(0) 1 1 0.02(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
13Published in Transactions on Machine Learning Research (09/2024)
0.0 0.2 0.4 0.6 0.8 1.0
FPR (1-s ecificity)0.00.20.40.60.81.0TPR (sensitivity)AUC
σ
1= 0.045
AUC
σ
2= 0.0444
AUC
σ
3= 0.0604
AUC
σ
4= 0.3734
AUC
σ
5= 0.9996
AUC
σ
6= 1
AUC
σ
7= 1
AUC
σ
8= 0.9994
AUC
σ
9= 0.9778
AUC
σ
MH= 0.0924ROC Curve (Mean Shift)
σ
1= 2
σ
2= 5
σ
3= 10
σ
4= 20
σ
5= 40
σ
6= 80
σ
7= 102
σ
8= 104
σ
9= 106
σ
MH≈11
(a)y1,...,yn∼N(0.560,I60)
0.0 0.2 0.4 0.6 0.8 1.0
FPR (1-s ecificity)0.00.20.40.60.81.0TPR (sensitivity)AUC
σ
1= 0.0004
AUC
σ
2= 0
AUC
σ
3= 0.0016
AUC
σ
4= 0.236
AUC
σ
5= 0.98
AUC
σ
6= 1
AUC
σ
7= 1
AUC
σ
8= 0.899
AUC
σ
9= 0.0506
AUC
σ
MH= 0.0676ROC Curve (V ariance Shift)
σ
1= 2
σ
2= 5
σ
3= 10
σ
4= 20
σ
5= 40
σ
6= 80
σ
7= 102
σ
8= 104
σ
9= 106
σ
MH≈11 (b)y1,...,yn∼N(060,2I60)
0.0 0.2 0.4 0.6 0.8 1.0
FPR (1-specificity)0.00.20.40.60.81.0TPR (sensitivity)AUC
σ
1= 0.048
AUC
σ
2= 0.046
AUC
σ
3= 0.0544
AUC
σ
4= 0.4134
AUC
σ
5= 1
AUC
σ
6= 1
AUC
σ
7= 1
AUC
σ
8= 0.9722
AUC
σ
9= 0.3652
AUC
σ
MH= 0.1346ROC Cu ve (Heavy T ail)
σ
1= 2
σ
2= 5
σ
3= 10
σ
4= 20
σ
5= 40
σ
6= 80
σ
7= 102
σ
8= 104
σ
9= 106
σ
MH≈11 (c)y1,...,yn∼t3(060,I60)
0.0 0.2 0.4 0.6 0.8 1.0
FPR (1-specifici y)0.00.20.40.60.81.0TPR (sensi ivi y)AUC
σ
1= 0.0156
AUC
σ
2= 0.0152
AUC
σ
3= 0.0204
AUC
σ
4= 0.9924
AUC
σ
5= 1
AUC
σ
6= 1
AUC
σ
7= 1
AUC
σ
8= 1
AUC
σ
9= 0.9952
AUC
σ
MH= 0.1176ROC Curve (Mix ure)
σ
1= 2
σ
2= 5
σ
3= 10
σ
4= 20
σ
5= 40
σ
6= 80
σ
7= 102
σ
8= 104
σ
9= 106
σ
MH≈11
(d)y1,...,yn∼ 0.5N(−160,I60) +
0.5N(160,I60)
0.0 0.2 0.4 0.6 0.8 1.0
FPR (1- pecificity)0.00.20.40.60.81.0TPR ( en itivity)AUC
σ
1= 0.8768
AUC
σ
2= 1
AUC
σ
3= 1
AUC
σ
4= 1
AUC
σ
5= 1
AUC
σ
6= 1
AUC
σ
7= 1
AUC
σ
8= 1
AUC
σ
9= 0.992
AUC
σ
MH= 1ROC Curve (Skewne  )
σ
1= 2
σ
2= 5
σ
3= 10
σ
4= 20
σ
5= 40
σ
6= 80
σ
7= 102
σ
8= 104
σ
9= 106
σ
MH≈11(e)y1,...,yn∼LN(060,B60)
0.0 0.2 0.4 0.6 0.8 1.0
FPR (1- pecificity)0.00.20.40.60.81.0TPR ( en itivity)AUC
σ
1= 0.016
AUC
σ
2= 0.0152
AUC
σ
3= 0.0184
AUC
σ
4= 0. 7208
AUC
σ
5= 1
AUC
σ
6= 1
AUC
σ
7= 1
AUC
σ
8= 0.9692
AUC
σ
9= 0.3076
AUC
σ
MH= 0.1456ROC Curve (Kurto i )
σ
1= 2
σ
2= 5
σ
3= 10
σ
4= 20
σ
5= 40
σ
6= 80
σ
7= 102
σ
8= 104
σ
9= 106
σ
MH≈11 (f)y1,...,yn∼LG(060,I60)
Figure 1: The ROC curves and AUC values of the BNP-MMD test for x1,...,xn∼N(060,I60), using a
range of bandwidth parameters including σ= 2,5,10,20,40,80,102,104,106, as well as the median heuristic
σMH.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000044/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013/uni00000035/uni00000025
(a)H=LG(060,I60)
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000044/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000011/uni00000018/uni00000014/uni00000018/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000013/uni00000035/uni00000025
a = n / 2 (b)H=F2
Figure 2: The solid line represents the average of the RB and the pink area represents a 95%confidence
interval of the RB over the 100 samples with various choices of Handafor the heavy tail example. The
lower and upper bounds are the 2.5%and97.5%quantiles of the RB, respectively. The red dotted line
representsRB= 1.
To compare the BNP and FNP tests, the p-values of the frequentists counterparts corresponding to each
Bayesian test are presented in Table 1 using the Rpackages energy8andmaotai9. AUC values of all tests
arealsogiventofacilitatecomparisonbetweentests. Generally, theproposedtestreflectsbetterperformances
than its frequentist counterparts in lower dimensions. For instance, in the variance shift example, when
d= 5andn= 30, the average of the RBand its strength for the semi-BNP-MMD test are 0.55and0.12,
respectively, which shows strong evidence to reject the null. While the average of the p-value corresponding
to the MMD frequentist test is 0.34, which shows a failure to reject the null hypothesis. The AUC value of
the semi-BNP test is also 0.99which indicates a better ability than its frequentist counterpart with an AUC
8https://CRAN.R-project.org/package=energy
9https://CRAN.R-project.org/package=maotai
14Published in Transactions on Machine Learning Research (09/2024)
of0.65. To examine the large sample property, additional results for n= 500,1000are presented in Section
F.1 of the Appendix, revealing the relatively poor performance of the BNP-Energy test in comparison to
other tests.
6.2 The Semi-BNP GAN
According to the results reported in the previous subsection, the semi-BNP estimator suggests a test that
outperforms other competing tests in many scenarios. Therefore, we expect that embedding this estimator in
GANsasthediscriminatorwillaccuratelydistinguishrealandfakedata. Weusethedatabaseofhandwritten
digits with 10 modes, bone marrow biopsy histopathology, human faces, and brain MRI images to analyze
the model performance. Following the design choices of Li et al. (2015), we use the Gaussian neural network
for the generator with four hidden layers each having rectified linear units activation function and a sigmoid
function for the output layer. For fitting a deep neural network, there are numerous methods to choose
network parameters. Furthermore, we select the number of nodes in hidden layers and tuning parameters of
the network using Bayesian optimization (Snoek et al., 2012). We also set mini-batch sizes to be nmb= 1,000
and use a mixture of six Gaussian kernels corresponding to the bandwidth parameters 2,5,10,20,40,and80
to train networks discussed in this section.
6.2.1 MNIST Dataset (LeCun, 1998):
The MNIST dataset includes 60,000 handwritten digits of 10 numbers from 0 to 9 each having 784 ( 28×28)
dimensions. This dataset is split into 50000 training and 10000 testing images and is a good example to
demonstrate the performance of the method in dealing with the mode collapse problem. We use the training
set to train the network. A sample from the training MNIST dataset is shown in Figure 3-a. Following
rmb= 40,000iterations, we generate samples from the trained semi-BNP GAN using Algorithm 2 from the
Appendix, as depicted in Figure 3-b. The results of Li et al. (2015) are also presented by Figure 3-c as the
frequentist counterpart of our semi-BNP procedure10. Based on these preliminary results, we can see that
our generated images can, at least, replicate the results of Li et al. (2015) and in some cases produce sharper
images. This result can also be deduced from the presented values of certain score functions in Section F.2 of
the Appendix. On the other hand, unlike the semi-BNP test, our experimental results demonstrate that the
(a) Training data
 (b) Semi-BNP-MMD GAN
 (c) FNP-MMD GAN
Figure 3: Generated samples of sizes ( 10×10) from semi-BNP-MMD and MMD-FNP GAN for the MNIST
dataset using a mixture of Gaussian kernels in 40,000 iterations.
semi-BNP GAN, using a mixture of Gaussian kernels, outperforms the approach that considers only a single
Gaussian kernel. To investigate this matter further, we present several samples of the trained generator
using a Gaussian kernel with different values of σ, as well as the median heuristic σMH, in Figure 4. Note
10The code for the GAN proposed by Li et al. (2015) is available at https://www.dropbox.com/s/anf9z1zyqi7379n/
Generative-Moment-Matching-Networks-master.zip?file_subpath=%2FREADME.md
15Published in Transactions on Machine Learning Research (09/2024)
that the value of σMHis updated in each iteration, and therefore, no specific value is reported for it in this
figure. While increasing the value of σenhances the diversity of the generated images, it is evident that the
resolution of the images in Figure 4 does not reach the image quality achieved by the mixture kernel.
In contrast to using MMD kernel-based measures, it may also be interesting to consider the energy distance
in learning GANs from a BNP perspective. To address this concern, we embed the two-sample BNP-energy
test of Al-Labadi et al. (2022a) in training GANs as a discriminator and show the generated samples in
Figure 5-a. This image clearly shows the inefficiency of the two-sample BNP test of Al-Labadi et al. (2022a)
in training the generator. The main issue in this test procedure is treating FGωas unknown distribution
to place a DP prior on it which is contrary to update parameter ωin the parameterized generative neural
networkGω.
One may also be interested in considering the semi-BNP-energy procedure in learning GANs in which it may
be more sensible to compare the semi-BNP-MMD results. To do this, we use the energy distance instead of
the MMD in Algorithm 2 in the Appendix. The results are presented in Figure 5-b and show blurry and
unclear images with no variety, which reflect the inefficiency of using the energy distance compared to the
MMD kernel-based measure. More experiments are given in Section F.2 of the Appendix.
(a)σ= 2
 (b)σ= 5
 (c)σ= 10
 (d)σ= 20
(e)σ= 40
 (f)σ= 80
 (g)σMH
Figure 4: Generated samples from semi-BNP-MMD for the MNIST dataset using a single Gaussian kernel
with various values of bandwidth parameter σin 40,000 iterations.
(a)
 (b)
Figure 5: Generated samples from BNP-Energy GAN (a) and semi-BNP-Energy GAN (b) for the MNIST
dataset in 40,000 iterations.
7 Conclusion
Our semi-BNP approach effectively estimates the MMD measure between an unknown distribution and an
intractableparametricdistribution. ItoutperformsfrequentistcounterpartsandevensurpassesarecentBNP
competitor in certain scenarios (Al-Labadi et al., 2022a). This approach shows great potential in training
GANs, where the proposed estimator serves as a discriminator, inducing a posterior distribution on the
generator’s parameter space. Stick-breaking representation lacks normalization terms and exhibits stochastic
16Published in Transactions on Machine Learning Research (09/2024)
decrease, making it inefficient for simulations (Zarepour & Al-Labadi, 2012). Thus, exploring alternative
DP approximations for MMD estimation presents an intriguing avenue for future research. Future work will
focus on generating 3D medical images to further enhance results.
References
Simon Aeschbacher, Mark A. Beaumont, and Andreas Futschik. A novel approach for choosing summary
statistics in approximate Bayesian computation. Genetics , 192(3):1027–1047, 2012.
Luai Al-Labadi. The two-sample problem via relative belief ratio. Computational Statistics , 36(3):1791–1808,
2021.
Luai Al-Labadi and Michael Evans. Prior-based model checking. Canadian Journal of Statistics , 46(3):
380–398, 2018.
Luai Al-Labadi and Mahmoud Zarepour. Two-sample Kolmogorov-Smirnov test using a Bayesian nonpara-
metric approach. Mathematical Methods of Statistics , 26(3):212–225, 2017.
LuaiAl-Labadi, ForoughFazeliAsl, andZahraSaberi. ABayesiansemiparametricGaussiancopulaapproach
to a multivariate normality test. Journal of Statistical Computation and Simulation , 91(3):543–563, 2021a.
Luai Al-Labadi, Forough Fazeli Asl, and Zahra Saberi. A necessary Bayesian nonparametric test for assessing
multivariate normality. Mathematical Methods of Statistics , 30(3-4):64–81, 2021b.
Luai Al-Labadi, Forough Fazeli Asl, and Zahra Saberi. A Bayesian nonparametric multi-sample test in any
dimension. AStA Advances in Statistical Analysis , 106(2):217–242, 2022a.
Luai Al-Labadi, Forough Fazeli Asl, and Zahra Saberi. A test for independence via Bayesian nonparametric
estimation of mutual information. Canadian Journal of Statistics , 50(3):1047–1070, 2022b.
Luai Al-Labadi, Ayman Alzaatreh, and Michael Evans. How to measure evidence: Bayes factors or relative
belief ratios? arXiv preprint arXiv:2301.08994 , 2023.
Varghese Alex, Mohammed Safwan K.P., Sai Saketh Chennamsetty, and Ganapathy Krishnamurthi. Gener-
ative adversarial networks for brain lesion detection. In Medical Imaging 2017: Image Processing , volume
10133, pp. 113–121. SPIE, 2017.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International Conference on Machine Learning , pp. 214–223. PMLR, 2017.
Mark A. Beaumont. Approximate Bayesian computation. Annual Review of Statistics and its Application ,
6(1):379–403, 2019.
Mark A. Beaumont, Wenyang Zhang, and David J Balding. Approximate Bayesian computation in popula-
tion genetics. Genetics , 162(4):2025–2035, 2002.
Ayush Bharti, Masha Naslidnyk, Oscar Key, Samuel Kaski, and François-Xavier Briol. Optimally-
weighted estimators of the maximum mean discrepancy for likelihood-free inference. arXiv preprint
arXiv:2301.11674 , 2023.
Mikołaj Bińkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs.
InInternational Conference on Learning Representations , 2018.
Michael G.B. Blum and Olivier François. Non-linear regression models for approximate Bayesian computa-
tion.Statistics and Computing , 20:63–73, 2010.
Karsten M. Borgwardt and Zoubin Ghahramani. Bayesian two-sample tests. arXiv preprint
arXiv:0906.4032v1 , 2009.
Francois-Xavier Briol, Alessandro Barp, Andrew B Duncan, and Mark Girolami. Statistical inference for
generative models with maximum mean discrepancy. arXiv preprint arXiv:1906.05944 , 2019.
17Published in Transactions on Machine Learning Research (09/2024)
Badr-Eddine Chérief-Abdellatif and Pierre Alquier. MMD-Bayes: Robust Bayesian estimation via maximum
mean discrepancy. In Symposium on Advances in Approximate Bayesian Inference , pp. 1–21. PMLR, 2020.
Badr-Eddine Chérief-Abdellatif and Pierre Alquier. Finite sample properties of parametric MMD estimation:
Robustness to misspecification and dependence. Bernoulli , 28(1):181–213, 2022.
Katalin Csilléry, Olivier François, and Michael G.B. Blum. abc: an R package for approximate Bayesian
computation (ABC). Methods in Ecology and Evolution , 3(3):475–479, 2012.
Charita Dellaporta, Jeremias Knoblauch, Theodoros Damoulas, and François-Xavier Briol. Robust Bayesian
inference for simulator-based models via the MMD posterior bootstrap. In International Conference on
Artificial Intelligence and Statistics , pp. 943–970. PMLR, 2022.
GintareKarolinaDziugaite,DanielM.Roy, andZoubinGhahramani. Traininggenerativeneuralnetworksvia
maximum mean discrepancy optimization. In Proceedings of the Thirty-First Conference on Uncertainty
in Artificial Intelligence , pp. 258–267, 2015.
Michael Evans. Measuring statistical evidence using relative belief . CRC Press, Boca Raton, FL, 2015.
Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized Stein variational gradient
descent. arXiv preprint arXiv:1707.06626 , 2017.
Thomas S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics , 1(2):
209–230, 1973.
Edwin Fong, Simon P. Lyddon, and Chris C. Holmes. Scalable nonparametric sampling from multimodal
posteriors with the posterior bootstrap. In International Conference on Machine Learning , pp. 1952–1962.
PMLR, 2019.
Gonzalo García-Donato and Ming-Hui Chen. Calibrating Bayes factor under prior predictive distributions.
Statistica Sinica , 15(2):359–380, 2005.
Marc G. Genton. Classes of kernels for machine learning: A statistics perspective. Journal of Machine
Learning Research , 2(Dec):299–312, 2001.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information Processing
Systems, 27:2672–2680, 2014.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander J. Smola. A
kernel two-sample test. Journal of Machine Learning Research , 13(1):723–773, 2012a.
Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji
Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. Ad-
vances in Neural Information Processing Systems , 25, 2012b.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in Neural Infor-
mation Processing Systems , 30, 2017.
Chris C. Holmes, Franccois Caron, Jim E. Griffin, and David A. Stephens. Two-sample Bayesian nonpara-
metric hypothesis testing. Bayesian Analysis , 10:297–320, 2015.
Gary B. Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A
database for studying face recognition in unconstrained environments. In Workshop on Faces in ‘Real-
Life’ Images: Detection, Alignment, and Recognition , 2008.
Peter J. Huber. Robust estimation of a location parameter. In Breakthroughs in Statistics: Methodology and
Distribution , pp. 492–518. Springer, 1992.
18Published in Transactions on Machine Learning Research (09/2024)
Hemant Ishwaran and Mahmoud Zarepour. Exact and approximate sum representations for the Dirichlet
process. Canadian Journal of Statistics , 30(2):269–283, 2002.
Harold Jeffreys. Theory of Probability . Clarendon Press, Oxford, third edition, 1961.
Jack Jewson, Jim Q. Smith, and Chris C. Holmes. Principles of Bayesian inference using general divergence
criteria. Entropy, 20(6):442, 2018.
Wittawat Jitkrittum, Zoltán Szabó, Kacper P. Chwialkowski, and Arthur Gretton. Interpretable distribution
features with maximum testing power. Advances in Neural Information Processing Systems , 29, 2016.
Robert. E. Kass and Adrian E. Raftery. Bayes factors. Journal of the American Statistical Association , 90
(430):773–795, 1995.
Oscar Key, Tamara Fernandez, Arthur Gretton, and François-Xavier Briol. Composite goodness-of-fit tests
with kernels. arXiv preprint arXiv:2111.10275 , 2021.
Yann LeCun. The MNIST database of handwritten digits, 1998. URL https://yann.lecun.com/exdb/
mnist.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. MMD-GAN: Towards
deeper understanding of moment matching network. Advances in Neural Information Processing Systems ,
30, 2017.
Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In International
Conference on Machine Learning , pp. 1718–1727. PMLR, 2015.
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. PacGAN: The power of two samples in generative
adversarial networks. Advances in Neural Information Processing Systems , 31, 2018.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference algo-
rithm.Advances in Neural Information Processing Systems , 29, 2016.
Qiang Liu, Jason Lee, and Michael I. Jordan. A kernelized Stein discrepancy for goodness-of-fit tests. In
International Conference on Machine Learning , pp. 276–284. PMLR, 2016.
Simon P. Lyddon, Stephen G. Walker, and Chris C. Holmes. Nonparametric learning from Bayesian models
with randomized objective functions. Advances in Neural Information Processing Systems , 31, 2018.
Simon P. Lyddon, Chris C. Holmes, and Stephen G. Walker. General Bayesian updating and the loss-
likelihood bootstrap. Biometrika , 106(2):465–478, 2019.
Alfred Müller. Integral probability metrics and their generating classes of functions. Advances in Applied
Probability , 29(2):429–443, 1997.
Masoud Nickparvar. Brain tumor MRI dataset, 2021. URL https://www.kaggle.com/dsv/2645886 .
Ziang Niu, Johanna Meier, and François-Xavier Briol. Discrepancy-based inference for intractable generative
models using quasi-Monte Carlo. Electronic Journal of Statistics , 17(1):1411–1456, 2023.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using
variational divergence minimization. Advances in Neural Information Processing Systems , 29, 2016.
Chris Oates. Minimum kernel discrepancy estimators. arXiv preprint arXiv:2210.16357 , 2022.
Lorenzo Pacchiardi and Ritabrata Dutta. Generalized Bayesian likelihood-free inference using scoring rules
estimators. arXiv preprint arXiv:2104.03889 , 2021.
Mijung Park, Wittawat Jitkrittum, and Dino Sejdinovic. K2-ABC: Approximate Bayesian computation with
kernel embeddings. In Artificial Intelligence and Statistics , pp. 398–407. PMLR, 2016.
19Published in Transactions on Machine Learning Research (09/2024)
Christian P. Robert, Jean-Marie Cornuet, Jean-Michel Marin, and Natesh S. Pillai. Lack of confidence in
approximate Bayesian computation model choice. Proceedings of the National Academy of Sciences , 108
(37):15112–15117, 2011.
Bernhard Schölkopf, Alexander J. Smola, Francis Bach, et al. Learning with Kernels: Support vector Ma-
chines, Regularization, Optimization, and Beyond . MIT Press, 2002.
Antonin Schrab, Ilmun Kim, Mélisande Albert, Béatrice Laurent, Benjamin Guedj, and Arthur Gretton.
MMD aggregated two-sample test. arXiv preprint arXiv:2110.15073 , 2021.
Antonin Schrab, Ilmun Kim, Benjamin Guedj, and Arthur Gretton. Efficient aggregated kernel tests using
incomplete U-statistics. Advances in Neural Information Processing Systems , 35:18793–18807, 2022.
Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of distance-
based and RKHS-based statistics in hypothesis testing. Annals of Statistics , pp. 2263–2291, 2013.
Jayaram Sethuraman. A constructive definition of Dirichlet priors. Statistica Sinica , pp. 639–650, 1994.
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of machine learning
algorithms. Advances in Neural Information Processing Systems , 25, 2012.
Danica J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex J. Smola,
and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy.
arXiv preprint arXiv:1611.04488 , 2016.
Alexander Terenin and David Draper. A noninformative prior on a space of distribution functions. Entropy,
19(8):391, 2017.
Jakub M. Tomczak and Max Welling. Improving variational auto-encoders using householder flow. arXiv
preprint arXiv:1611.09630 , 2016.
Jelmer M. Wolterink, Anna M. Dinkla, Mark H.F. Savenije, Peter R. Seevinck, Cornelis A.T. van den Berg,
andIvanaIšgum. Deep MRtoCT synthesisusingunpaireddata. In International Workshop on Simulation
and Synthesis in Medical Imaging , pp. 14–23. Springer, 2017.
Xin Yi, Ekta Walia, and Paul Babyn. Generative adversarial network in medical imaging: A review. Medical
Image Analysis , 58:101552, 2019.
Mahmoud Zarepour and Luai Al-Labadi. On a rapid simulation of the Dirichlet process. Statistics &
Probability Letters , 82(5):916–924, 2012.
Kaifeng Zhang. On mode collapse in generative adversarial networks. In Artificial Neural Networks and
Machine Learning – ICANN 2021 , pp. 563–574, Cham, 2021. Springer International Publishing. ISBN
978-3-030-86340-1.
Feng Zhao, Chenhui Lei, Qingkun Zhao, Huiya Yang, Guoping Ling, Jiabin Liu, Haofei Zhou, and Hongtao
Wang. Predicting the property contour-map and optimum composition of Cu-Co-Si alloys via machine
learning. Materials Today Communications , 30:103138, 2022.
Appendix
A Technical Proofs
A.1 Theoretical Properties of the DP Approximation given by Ishwaran & Zarepour (2002)
Proposition 6 For a non-negative real value aand fixed probability distribution H, letFpri
1:=F1∼
DP(a,H)and(J1,N,...,JN,N)∼Dirichlet (a
N,...,a
N)be the weights in the approximation of Fpri, given
by Ishwaran & Zarepour (2002). Then, as a→∞,
20Published in Transactions on Machine Learning Research (09/2024)
i. Jℓ,Na.s.−−→1
N,for anyℓ∈{1,...,N},
ii. Jℓ,NJt,Na.s.−−→1
N2,for anyℓ,t∈{1,...,N},whereℓ̸=t.
Proof.Recall
Fpri
N=N/summationdisplay
i=1Ji,NδYi. (14)
SinceEFpri
1(Jℓ,N) =1
N, for anyℓ∈{1,...,N}andϵ>0, Chebyshev’s inequality implies
Pr{|Jℓ,N−1/N|≥ϵ}≤Var(Jℓ,N)
ϵ2,
where,VarFpri
1(Jℓ,N) =N−1
N2(a+1). Assuming a=κ2cforκ∈Nand a fixed positive number c, gives
Pr{|Jℓ,N−1/N|≥ϵ}≤1
κ2cϵ2.
The convergence of series/summationtext∞
κ=0κ−2implies/summationtext∞
κ=0Pr{|Jℓ,N−1/N|≥ϵ}<∞. By letting a→∞, the first
Borel Cantelli lemma concludes |Jℓ,N−1/N|a.s.−−→ 0and the result of (i) follows. To prove (ii), it is enough
to show Pr/braceleftbig
lima→∞(Jℓ,NJt,N)̸=1
N2/bracerightbig
= 0. To prove this for the probability space (Ω,F,Pr), let
A=/braceleftbigg
ω∈Ω : lim
a→∞(Jℓ,N(ω)Jt,N(ω))̸=1
N2/bracerightbigg
, B =/braceleftbigg
ω∈Ω : lim
a→∞(Jℓ,N(ω))̸=1
N/bracerightbigg
,
C=/braceleftbigg
ω∈Ω : lim
a→∞(Jt,N(ω))̸=1
N/bracerightbigg
,
where, Pr(B)andPr(C)are zero by (i). Since A⊆B∪C, then,
1−Pr/braceleftbigg
ω∈Ω : lim
a→∞(Jℓ,N(ω)Jt,N(ω)) =1
N2/bracerightbigg
= Pr(A)≤Pr(B) + Pr(C) = 0,
which concludes the result.
A.2 Proof of Theorem 1
Proof.For samples{Vℓ}N
ℓ=1and{Yℓ}m
ℓ=1, respectively, from HandF2, the triangle inequality implies
/vextendsingle/vextendsingle/vextendsingleMMD2
BNP(Fpri
1,N,F2,m)−MMD2(HN,F2,m)/vextendsingle/vextendsingle/vextendsingle≤K/braceleftiggN/summationdisplay
ℓ,t=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleJℓ,NJt,N−1
N2/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+2
mN/summationdisplay
ℓ=1m/summationdisplay
t=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleJℓ,N−1
N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracerightigg
.
By Proposition 6, which provides some theoretical properties of the DP approximation given in (14), the
right-hand side of the above inequality converges almost surely to 0 as a→∞for fixedN. This conver-
gence immediately concludes the proof of (i). To prove (ii), since (J1,N,...,JN,N)∼Dirichlet (a
N,...,a
N),
EFpri
1(Jℓ,N) =1
Nand
EFpri
1(Jℓ,NJt,N) =

a
(a+ 1)N2ifℓ̸=t,
a+N
(a+ 1)N2ifℓ=t.
21Published in Transactions on Machine Learning Research (09/2024)
Applying these properties in definition of MMD2
BNP(Fpri
1,N,F2,m)results in
EFpri
1(MMD2
BNP(Fpri
1,N,F2,m)|V1:N) =N/summationdisplay
ℓ=1N/summationdisplay
t̸=ℓak(Vℓ,Vt)
(a+ 1)N2+N/summationdisplay
ℓ=1N/summationdisplay
t=ℓ(a+N)k(Vℓ,Vt)
(a+ 1)N2
−2
NmN/summationdisplay
ℓ=1m/summationdisplay
t=1k(Vℓ,Yt) +1
m2m/summationdisplay
ℓ,t=1k(Yℓ,Yt). (15)
Now, it is sufficient to compute the following conditional expectation,
E(MMD2
BNP(Fpri
1,N,F2,m)) =EH,F2(EFpri
1(MMD2
BNP(Fpri
1,N,F2,m)|V1:N)). (16)
Since sets{Vi}N
i=1and{Yi}m
i=1include i.i.d. random variables, separately, replacing equation 15 in expecta-
tion (16) implies:
(16) =a(N−1)
(a+ 1)NEH[k(V1,V2)] +a+N
(a+ 1)NEH[k(V1,V1)]−2EH,F2[k(V1,Y1)]
+m−1
mEF2[k(Y1,Y2)] +1
mEF2[k(Y1,Y1)]. (17)
The proof of (ii) is concluded by letting a→∞,N→∞, andm→∞in the above equation. Lastly, since
1
m<1,m−1
m<1,a(N−1)
(a+1)N<1, anda+N
(a+1)N<2, then, for any N,m∈Nanda∈R+,
(17)<EH[k(V1,V2)]−2EH,F2[k(V1,Y1)] +EF2[k(Y1,Y2)] + 3K,
which concludes the proof of (iii).
A.3 Proof of Theorem 2
Proof.Applying triangular inequality implies
/vextendsingle/vextendsingle/vextendsingleMMD2
BNP(Fpos
1,N,F2,m)−MMD2(HN,F2,m)/vextendsingle/vextendsingle/vextendsingle≤N/summationdisplay
ℓ,t=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleJ∗
ℓ,NJ∗
t,Nk(V∗
ℓ,V∗
t)−1
N2k(Vℓ,Vt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+2
mN/summationdisplay
ℓ=1m/summationdisplay
t=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleJ∗
ℓ,Nk(V∗
ℓ,Yt)−1
Nk(Vℓ,Yt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle,(18)
where, samples{V∗
ℓ}N
ℓ=1and{Yℓ}m
ℓ=1are generated from H∗andF2, respectively. Similar to Proposition
6, it can be shown that J∗
ℓ,N→1/NandJ∗
ℓ,NJ∗
t,N→1/N2, asa→∞, using conjugacy property of DP. On
the other hand, since H∗→Hasa→∞, the chance of sampling from HandF1,ntends, respectively, to
one and zero, which implies V∗
i→Vi, whereVi∼H, fori= 1,2. Applying the continuous mapping theorem
impliesk(V∗
1,V∗
2)→k(Vl,Vt)andk(V∗
l,Yt)→k(Vl,Yt), which completes the proof of (i)(a). To prove
(i)(b), it follows from the proof of Theorem 1:
E(MMD2
BNP(Fpos
1,N,F2,m)) =h1(a,n,N )EH∗[k(V∗
1,V∗
2)] +h2(a,n,N )EH∗[k(V∗
1,V∗
1)]
−2EH∗,F2[k(V∗
1,Y1)] +m−1
mEF2[k(Y1,Y2)]
+1
mEF2[k(Y1,Y1)], (19)
whereh1(a,n,N ) =(a+n)(N−1)
(a+n+1)Nandh2(a,n,N ) =a+n+N
(a+n+1)N. Sincek(·,·)is bounded above by K,
the dominated convergence theorem implies EH∗[k(V∗
1,V∗
2)]→EH[k(V1,V2)]andEH∗,F2[k(V∗
1,Y1)]→
EH,F2[k(V1,Y1)]. Sinceh1(a,n,N )→1andh2(a,n,N )→0asa→∞,N→∞; and,m/(m−1)→1and
1/m→0, asm→∞, the results follow.
22Published in Transactions on Machine Learning Research (09/2024)
To prove (ii)(a) and (ii)(b), F1,n→F1, and thenH∗→F1asn→∞by the Glivenko-Cantelli theorem. It
indicates that the probability of sampling from HandF1,ntends, respectively, to zero and one. Therefore,
V∗
i→Xiasn→∞, whereXi∼F1, fori= 1,2. The proof of (ii)(a) is completed with the same strategy as
the proof of (i)(a) by letting n→∞in (18). The proof of (ii)(b) is also concluded with a similar argument
that in (i)(b), when n→∞in (19).
A.4 Proof of Corollary 3
Proof.The proofs are immediately followed by Theorem 1 and Theorem 2.
A.5 Proof of Lemma 4
Proof.The proof of Lemma 4(i) relies on the proof given in Dellaporta et al. (2022, Theorem 9) which is
expanded for infinite stick-breaking representation, while we consider the finite DP approximation given in
(14). By employing a similar technique as in the previously mentioned theorem, we have
E(MMD(F,FGω∗)) =EF(EFposMMD(F,FGω∗)|X1:n)
≤min
ω∈WMMD(F,FGω) + 2EF(MMD(Fn,F)) + 2EFpos(MMD(Fpos
N,H∗))
+ 2EF(EH∗(MMD(Fn,H∗)|X1:n)).
Building on the results of Dellaporta et al. (2022, Lemma 7), we can establish that
EFpos/parenleftbig
MMD2(Fpos
N,H∗)/parenrightbig
≤N/summationdisplay
ℓ=1EFpos[J∗2
ℓ,N]EH∗[k(V∗
ℓ,V∗
ℓ)]≤(a+n+N)K
(a+n+ 1)N,
where the right-hand side of the above inequality follows from the fact that k(·,·)≤KandEFpos[J∗2
ℓ,N] =
a+n+N
(a+n+1)N2. Now, the Jensen’s inequality implies
EFpos(MMD(Fpos
N,H∗))≤/radicaligg
(a+n+N)K
(a+n+ 1)N.
On the other hand, Chérief-Abdellatif & Alquier (2022, Lemma 7.1) and Dellaporta et al. (2022, Lemma 8),
respectively, imply that
EF(MMD(Fn,F))≤K√n,EF(EH∗(MMD(Fn,H∗)|X1:n))≤2aK
a+n,
which concludes the proof of (i). To establish (ii), we adopt the approach used in the proof of Dellaporta
et al. (2022, Corollary 5). Initially, we employ Chérief-Abdellatif & Alquier (2022, Lemma 3.3) to bound
MMD(F0,FGω∗)by2ϵ+ MMD(F,FGω∗), resulting in:
E(MMD(F0,FGω∗))≤2ϵ+E(MMD(F,FGω∗)).
Applying the result in (i) to the right-hand side of the above inequality implies:
E(MMD(F0,FGω∗))≤2ϵ+ min
ω∈WMMD(F,FGω) +2K√n+4aK
a+n+ 2/radicaligg
(a+n+N)K
(a+n+ 1)N.
Finally, we employ Chérief-Abdellatif & Alquier (2022, Lemma 3.3) once again, but this time to bound
MMD(F,FGω)by2ϵ+ MMD(F0,FGω)for any ω∈W, thereby completing the proof of (ii).
23Published in Transactions on Machine Learning Research (09/2024)
A.6 Proof of Lemma 5
Proof. LetLBNP(ω) = MMD( Fpos
N,FGω),Ln,m(ω) = MMD( Fn,FGω,m), and L(ω) = MMD( F,FGω).
Then, for ω∗∈W, Gretton et al. (2012a, Theorem 7) implies
Pr (|Ln,m(ω∗)−L(ω∗)|>h(n,m,K,ϵ ))<2 exp−ϵ2nm
2K(n+m). (20)
Hence, with a probability at least 1−2 exp−ϵ2nm
2K(n+m),
|Ln,m(ω∗)−L(ω∗)|≤h(n,m,K,ϵ ). (21)
On the other hand, the triangle inequality implies
|LBNP(ω∗)−L(ω′)|≤|Ln,m(ω∗)−L(ω∗)|+|LBNP(ω∗)−Ln,m(ω∗)|+|L(ω∗)−L(ω′)|.(22)
Finally, the proof of (i) is concluded by considering inequality 21 in equation 22. To prove (ii), Markov’s
inequality implies
Pr (MMD(F,FGω∗)≥ϵ)≤E(MMD(F,FGω∗))
ϵ.
The result follows by substituting the bounds from Lemma 4(i) into the right-hand side of the above in-
equality.
B Computational Algorithms
B.1 Implementing the Semi-BNP GOF Kernel-based Test
Algorithm 1 Pseudocode of semi-BNP two-sample MMD kernel test
1:Initializea,ℓ,M,i0, andϵin equation 4 to determine N. Consider an observed sample x1:nfromF1.
2:H←F2STEP 1: Computing the BNP MMD
3:forr←0toℓdo
4:Generate an approximate sample of F1∼DP(a,H)by using/summationtextN
i=1Ji,NδVi, where{Ji,N}N
i=1∼
Dirichlet (a
N,···,a
N), and{Vi}N
i=1∼H.
5:Generate an approximate sample of F1|x1:n∼DP(a+n,H∗)by using/summationtextN
i=1J∗
i,NδV∗
i, where
{J∗
i,N}N
i=1∼Dirichlet (a+n
N,···,a+n
N), and{V∗
i}N
i=1∼H∗.
6:Use samples generated in steps 4 and 5 to compute
MMD2
BNP(Fpri
1,N,F2,m)andMMD2
BNP(Fpos
1,N,F2,m), respectively.
7:end for
8:return{MMD2
BNPr(Fpri
1,N,F2,m)}ℓ
r=1and{MMD2
BNPr(Fpos
1,N,F2,m)}ℓ
r=1
STEP 2: Estimating RB and Str
9:/hatwideΠMMD2(·|x1:n)←ECDF ({MMD2
BNPr(Fpos
1,N,F2,m)}ℓ
r=1) ▷The ECDF of posterior-based MMD
10:/hatwideΠMMD2(·)←ECDF ({MMD2
BNPr(Fpri
1,N,F2,m)}ℓ
r=1) ▷The ECDF of prior-based MMD
11:/hatwidedi0/M←quantile ({MMD2
BNPr(Fpri
1,N,F2,m)}ℓ
r=1,i0/M) ▷The estimation of the i0/M-th quantile of
MMD2
BNP(Fpri
1,N,F2,m)
12:/hatwiderRBMMD2(0|x1:n)←/hatwideΠMMD 2(ˆdi0/M|x1:n)
/hatwideΠMMD 2(ˆdi0/M)
13:/hatwiderStr←0
14:fori←0toM−1do
15:/hatwidedi/M←quantile ({MMD2
BNPr(Fpri
1,N,F2,m)}ℓ
r=1,i/M )
16:/hatwided(i+1)/M←quantile ({MMD2
BNPr(Fpri
1,N,F2,m)}ℓ
r=1,(i+ 1)/M)
17:/hatwiderRBMMD2(/hatwidedi/M|x1:n)←/hatwideΠMMD 2(/hatwided(i+1)/M|x1:n)−/hatwideΠMMD 2(/hatwidedi/M|x1:n)
/hatwideΠMMD 2(/hatwided(i+1)/M)−/hatwideΠMMD 2(/hatwidedi/M)
24Published in Transactions on Machine Learning Research (09/2024)
18: if/hatwidestRBMMD2(/hatwidedi/M|x1:n)≤/hatwidestRBMMD2(0|x1:n)then
19:/hatwiderStr(0|x1:n)←/hatwiderStr+ [/hatwideΠMMD2(/hatwided(i+1)/M|x1:n)−/hatwideΠMMD2(/hatwidedi/M|x1:n)]
20: end if
21:end for
22:return/hatwiderRBMMD2,/hatwiderStr
iiiChoosing 𝐻≠𝐹2 leads to 𝜋MMD2∙|𝐱1:𝑛 not supporting ℋ0 The test fails to reject ℋ0 when it is a false hypothesis.
ii𝑅𝐵 is calculated as the ratio of the area under 𝜋MMD2∙|𝐱1:𝑛 to the area under 𝜋MMD2(∙) on the interval [0, 𝑑𝑖0/𝑀).𝐹𝑃𝑟𝑖is constructed around base measure  𝐻, whose variation is controlled by 𝑎 MMD2𝐹𝑃𝑟𝑖,𝐹2 is constructed around MMD2𝐻,𝐹2
Actual Condition𝑎<𝑛/2 𝑎≫𝑛
𝐻=𝐹2 𝐻≠𝐹2For any choice of 𝐻MMD2𝐻,𝐹2=0 MMD2𝐻,𝐹2≠0
i
i
ℋ0 is true
𝐹1=𝐹2ℋ0 is not  true
𝐹1≠𝐹2𝑅𝐵>1
Decision
Accept ℋ0 
𝑅𝐵<1
Decision
Reject ℋ0 𝑅𝐵>1
Decision
Accept ℋ0 
𝑅𝐵>1
Decision
Accept ℋ0 𝑅𝐵≈1
No decision can be made to reject or accept ℋ0!
iii
𝜋MMD2(∙) supports ℋ0⇔ 
𝐻=𝐹2 (See Theorem 1 
& Corollary 3)
Choosing an 𝑎 significantly larger than 𝑛 results in 𝜋MMD2∙|𝐱1:𝑛 and 𝜋MMD2(∙) coinciding
 iv The test is unable to evaluate ℋ0
ii
iv𝐱1:𝑛~𝐹1
Figure 6: General diagram of the BNP-GOF test. The prior density of MMD2versus the posterior density
after observing x1:nfrom distribution F1under all possible conditions for both true and false null hypothesis.
Here, we consider F1=F2=N(060,I60)as the true null hypothesis, while F1=t3(060,I60)andF2=
N(060,I60)for the false null hypothesis to plot densities.
B.2 Training the Semi-BNP GAN
Algorithm 2 Pseudocode of training a GAN using the semi-BNP approach
1:Seta= 10−6to employ a non-informative prior leading DP posterior DP(n,Fn).
2:Initializeϵin equation 4 to determine Nusing conjugacy property of DP.
3:rmn←Number of training iteration, nmb←Mini-batch size
4:ω0←An initial parameter for generator Gω,{xℓ}n
ℓ=1←real dataset
5:fori←0tormbdo
6:Generate a random sample {xmb
ℓ}nmb
ℓ=1from real dataset {xℓ}n
ℓ=1
7:Generate a sample of noise vector {uℓ}nmb
ℓ=1from uniform distribution U(−1,1)
8:Generate a sample from FGωi, distribution of Gωi, as{yℓ=Gωi(uℓ)}nmb
ℓ=1
9:Generate a sample of size NfromFpos=F|{xmb
ℓ}nmb
ℓ=1using/summationtextN
i=1J∗
i,Nδv∗
iby replacing F1byF, and
{xmb
ℓ}nmb
ℓ=1byxin step (4) of Algorithm 1.
10:Use generated samples in steps 9 and 10 to compute MMD2
BNP(Fpos
N,FGωi,N).
11:Compute the gradient:
∂MMD BNP(Fpos
N,FGωi,m)
∂ωi=1
2/radicalbig
MMD2
BNP(Fpos
N,FGω,m)∂MMD2
BNP(Fpos
N,FGω,m)
∂ω.
25Published in Transactions on Machine Learning Research (09/2024)
12:Use backpropagation for calculating partial derivatives∂Gωi(uℓ)
∂ωiin the previous step to update pa-
rameter ωi.
13:end for
14:return ω∗▷An optimized parameter for Gωthat minimizes the cost function.
B.3 Hypothesis Testing Evaluation
Algorithm 3 Pseudocode of plotting ROC and computing AUC in semi-BNP test
1:Initializea,N,ℓ, andM.
2:Set the number of repeating experiments: r←100
3:RB†|H0←ComputeRBforrsamples of sizes ngenerated under the null hypothesis.
4:RB|H1←ComputeRBforrsamples of sizes ngenerated under the alternative hypothesis.
5:T←A sequence of numbers between 0 to 20‡with length L.▷The discrimination threshold for the
semi-BNP test.
6:TP←A vector whose each component represents the number of components of the vector RB|H1which
is less than each component of T.
7:FN←A vector whose each component represents the number of components of the vector RB|H1which
is greater than each component of T.
8:FP←A vector whose each component represents the number of components of the vector RB|H0which
is less than each component of T.
9:TN←A vector whose each component represents the number of components of the vector RB|H0which
is greater than each component of T.
10:Compute the confusion matrix as:

TNR :=TN
TN+FPFNR :=FN
FN+TP
(1-Type I error) (Type II error)
FPR :=FP
FP+TNTPR :=TP
TP+FN
(Type I error) (1-Type II error)
.
11:ROC←Drawing a linear plot of TPRagainstFPR.
12:AUC←Computing the area under the ROC.
13:returnROC and AUC.
†It should be changed to the p-value in the FNP test.
‡It should be changed to 1 in the FNP test.
C Relative Belief Ratio: A Bayesian Measure of Evidence
The RB ratio (Evans, 2015) is a form of Bayesian evidence in hypothesis testing problems and has shown
excellent performance in many statistical hypothesis testing procedures (Al-Labadi et al., 2022a; 2021a;
2022b). The RB ratio is defined by the ratio of the posterior density to the prior density at a particular
parameter of interest in the population distribution whose correctness is under investigation. Precisely, for
a statistical model (X,F)withF={fθ:θ∈Θ}, letπbe a prior on the parameter space Θandπ(θ|x1:n)
be the posterior distribution of θafter observing the data x1:n= (x1,...,xn). Consider a parameter of
interest,ψ= Ψ(θ)such that Ψsatisfies regularity conditions so that the prior density πΨand the posterior
densityπΨ(·|x1:n)ofψexist with respect to some support measure on the range space for Ψ. WhenπΨand
πΨ(·|x1:n)are continuous at ψ, the RB ratio for a value ψis given by
RBΨ(ψ|x1:n) =πΨ(ψ|x1:n)/πΨ(ψ).
Otherwise for a sequence Nδ(ψ), the neighborhoods of ψthat converge nicely to ψasδ→0, the RB ratio is
defined byRBΨ(ψ|x1:n) = limδ→0ΠΨ(Nδ(ψ)|x1:n)/ΠΨ(Nδ(ψ)),where ΠΨandΠΨ(·|x1:n)are the marginal
prior and the marginal posterior probability measures, respectively.
Note thatRBΨ(ψ|x1:n)measures the change in the belief of ψbeing the true value a prioritoa posteriori .
Therefore, it is a measure of evidence. If RBΨ(ψ|x1:n)>1, then the probability of ψbeing the true value
from a priori to a posteriori is increased, consequently there is evidence based on the data that ψis the true
value. IfRBΨ(ψ|x1:n)<1, then the probability of ψbeing the true value from a priori to a posteriori is
decreased. Accordingly, there is evidence against based on the data that ψbeing the true value. For the
26Published in Transactions on Machine Learning Research (09/2024)
caseRBΨ(ψ|x1:n) = 1there is no evidence in either direction. For the null hypothesis H0: Ψ(θ) =ψ0, it is
obviousRBΨ(ψ0|x1:n)measures the evidence in favor of or against H0. In this scenario where evidence for
the null hypothesis is plausible, the frequentist notion of controlling the probability of falsely rejecting H0
(type I error) does not apply.
The possibility of calibrating RB ratios is a desirable feature that makes it attractive in hypothesis testing
problems. After computing the RB ratio, it is very critical to know whether the obtained value represents
strong or weak evidence for or against H0. A typical calibration of RBΨ(ψ0|x1:n)is given by the strength
of evidence
StrΨ(ψ0|x1:n) = Π Ψ[RBΨ(ψ|x1:n)≤RBΨ(ψ0|x1:n)|x1:n]. (23)
The value of equation 23 indicates that the posterior probability that the true value of ψhas a RB ratio no
greater than that of the hypothesized value ψ0.WhenRBΨ(ψ0|x1:n)<1, there is evidence against ψ0,then
a small value of (23) indicates strong evidence against ψ0because the posterior probability of the true value
having RB ratio bigger is large. On the other hand, a large value for (23) indicates weak evidence against
ψ0. Similarly, when RBΨ(ψ0|x1:n)>1, there is evidence in favor of ψ0,then a small value of (23) indicates
weak evidence in favor of ψ0, while a large value of (23) indicates strong evidence in favor of ψ0.
The RB can be considered as a strong alternative to the Bayes factor (BF) criteria. The BF is defined
as the ratio of the marginal likelihood of data under the null hypothesis to the alternative hypothesis in
Bayesian hypothesis testing problems. However, computing the BF often involves intractable calculations
of marginal likelihoods, which typically require computationally burdensome methods such as MCMC. The
tests proposed by Holmes et al. (2015) and Borgwardt & Ghahramani (2009) are two examples of BNP tests
that utilize marginal likelihood computation, and their practical usage in high-dimensional statistics is low
due to this computational issue.
On the other hand, the construction of tests using the BF relies on assigning a prior π0to the null hypothesis
H0, a priorπ1to the alternative hypothesis H1, and a discrete probability mass p0forH0. However,
practitioners often face challenges in eliciting these prior components within the overall prior π=p0π0+
(1−p0)π1. Another concern of using BFs is their calibration to indicate whether weak or strong evidence
is attained. For example, Jeffreys (1961) and Kass & Raftery (1995) proposed similar rules to calibrate BFs
but García-Donato & Chen (2005) pointed out that such rules are inappropriate to calibrate BFs as they
ignore the randomness of the data and, again, lead to improper inference11.
D Radial Basis Function Kernels Family
The construction of MMD-based procedures is proposed based on considering a kernel function with feature
space corresponding to a universal RKHS. The radial basis function (RBF) kernel is the most well-known
kernel family satisfying the above situation. For two vectors X,Y∈Rd, the RBF kernel is represented by
k(X,Y) =h(||X−Y||/σ),
where,his a function from the positive real numbers R+toR+,||·||represents the L2-norm, and σis the
bandwidth parameter that indicates the kernel size. There are many functions assigned to h, for example,
the Gaussian, exponential, rational quadratic kernels, and Matern, represented by
h1(x) = exp (−x2
2), h2(x) = exp (−x), h3(x) =/parenleftbigg
1 +x2
2α/parenrightbigg−α
, h4(x) = (1 +√
2νx)e−√
2νx,
respectively; where, αinh3is a positive-valued scale-mixture parameter, and the νinh4is a parameter
that controls the smoothness of the kernel results (Zhao et al., 2022; Genton, 2001).
One of the simplest kernel functions above is the Gaussian kernel, which is mostly used in machine learning
problems and only depends on bandwidth parameter σ. The Gaussian kernel tends to 0 and 1 when σ→0
andσ→∞, respectively. Both situations lead to MMD2being zero. Hence, the choice of the parameter
11A comprehensive study that explains why the RB ratio is a more appropriate measure of evidence than the BF can also be
found in Al-Labadi et al. (2023).
27Published in Transactions on Machine Learning Research (09/2024)
σhas a crucial effect on the performance of this kernel. Numerous methods are proposed to choose the
value ofσ, however, there is no definitive optimization method for this problem. The median heuristic is
one of the first methods used in choosing σempirically and will be denoted in our experimental results
byσMH. More precisely, for two samples {Xi}n
i=1and{Yi}m
i=1, theσMHis considered as the median
of{||Xi−Yj||2: 1≤i≤n,1≤j≤m}, which is mostly used in kernel-based tests (Schölkopf et al.,
2002). Selecting σbased on maximizing the power of two-sample problems is another strategy considered
by Jitkrittum et al. (2016). The selection of the MMD bandwidth on held-out data to maximize power
was first proposed by Gretton et al. (2012b) for linear-time estimates and by Sutherland et al. (2016)
for quadratic-time estimates. Recently, bandwidth selection without data splitting has been proposed for
quadratic (Schrab et al., 2021) and linear (Schrab et al., 2022) MMD estimates. Regarding the choice of σin
kernel-based GANs, a common idea is assigning several fixed values to σand then considering the mixture
of their corresponding Gaussian kernel. This strategy has received much attention and shown an acceptable
performance in training GANs12.
E Training Evaluation
E.1 Traditional Approaches
Evaluating the quality of samples generated by GANs is considered to assess the mode collapse problem
(Zhang, 2021).
E.1.1 Fréchet Inception Distance (FID)
The FID is a widely used metric to assess the similarity between the distribution of generated images and real
images(Heuseletal.,2017). Itisbasedontheconceptofcomparingthestatisticsoffeaturerepresentationsof
these images. Specifically, it computes the Fréchet distance between two multivariate Gaussian distributions
fitted to the feature representations of the inception network for real and generated images.
Let{ϕInc(Xi)}n
i=1be the feature representations for the real images {Xi}n
i=1in the inception network, if
µϕInc(X)andΣϕInc(X)are the sample mean vector and covariance matrix of {ϕInc(Xi)}n
i=1, andµϕInc(Y)and
ΣϕInc(Y)are the corresponding statistics for the generated images {Yi(ω∗)}n
i=1, then the FID is defined as:
FID =∥µϕInc(X)−µϕInc(Y)∥2+Tr(ΣϕInc(X)+ ΣϕInc(Y)−2(ΣϕInc(X)ΣϕInc(Y))1
2),
where “Tr” denotes matrix trace.
A lower FID indicates that the generated images are more similar to the real images.
E.1.2 Kernel Inception Distance (KID)
The KID is another metric used to evaluate the quality of generated images. Unlike FID, which assumes
the feature representations follow a Gaussian distribution, KID is based on the MMD between the real and
generated images’ feature representations (Bińkowski et al., 2018) as
KID = MMD2(FϕInc(X),n,FϕInc(Y),n),
whereFϕInc(X),ndenotes the empirical distribution corresponding to the sample {ϕInc(Xi)}n
i=1.
The KID uses the polynomial kernel function in calculating MMD, which is defined as:
k(ϕInc(Xi),ϕInc(Yi(ω∗))) = ((ϕInc(Xi))⊤ϕInc(Yi(ω∗)) + 1)ν,
whereνis the degree of the polynomial and “T” denotes matrix transpose.
Similarly to FID, a lower KID indicates higher similarity between the real and generated images.
12For further details, see Li et al. (2015) and Li et al. (2017).
28Published in Transactions on Machine Learning Research (09/2024)
E.2 An MMD Matching Score Function
To develop a stronger method for evaluating the differences between real and generated data manifolds, we
propose using the MMD dissimilarity measure as follows: For i= 1,...,rmb, let{Xij}nmb
j=1and{Yij(ω∗)}nmb
j=1
be two samples drawn, respectively, from the real dataset X1,...,Xnand the generated dataset Y1(ω∗),...,
Yn(ω∗)with the same sample size nmb<n. Then, we define the MMD-based matching score as
MMDS = max
i∈{1,...,rmb}MMD2(Fnmb(i),FGω∗,nmb(i)), (24)
where, MMD2(Fnmb(i),FGω∗,nmb(i))is the MMD approximation given by Equation (2, main paper) using
samples{Xij}nmb
j=1and{Yij(ω∗)}nmb
j=1(mini-batch samples). Our proposed matching score returns the max-
imum value of the MMD approximation between a subset of the real and a subset of the generated dataset
with the same size nmb(mini-batch sample size) over rmbresamplings (mini-batch iteration). According to
Equation (2, main paper), all components of mini-batch samples are compared together in the MMD mea-
sure, which provides a comprehensive assessment between subsets of the data in each iteration. Eventually,
it is obvious smaller values of MMDS indicate better quality and more diversity of the generated samples.
F Additional Experiments
F.1 The Semi-BNP Test
To further illustrate the difference in performance between the BNP and FNP tests, we conducted tests on
two alternative distributions: F1=N(0,σ2)forσ2∈[1,4]andF1= 0.5N(−1 +υ,1) + 0.5N(1−υ,1)for
υ∈[0,1]. The corresponding results are reported in Figure 7 and 8 for univariate cases with n= 50. Figure
7(a) specifically shows that the proposed test exhibits a higher growth rate of the AUC when σ2is increased
compared to the other tests. Additionally, Figure 7(b) indicates that our test starts to detect differences
earlier than other tests ( σ2≥1.67). Similar results can be found in Figure 8 for mixture distribution with
various means.
0.40.60.81.0
1 2 3 4
σ2AUC
Type of the test: BNP−Energy BNP−MMD FNP−Energy FNP−MMD
(a)
σ2=1.67σ2=1.67σ2=1.67
0.00.51.01.52.02.5
1 2 3 4
σ2RBType of the BNP test
Energy
MMD
Type I Error=0.05 Type I Error=0.05 Type I Error=0.05
Type I Error=0.01 Type I Error=0.01 Type I Error=0.010.00.20.40.6
1 2 3 4
σ2p−ValueType of the FNP test
Energy
MMD
0.00.10.20.30.40.5
−5.0 −2.5 0.0 2.5 5.0
xDensity (b)
Figure 7: (a) AUC values in testing alternative F1=N(0,σ2)forσ2∈(1,4)in variance shift example.
(b)-Top: Test critical values against different values of σ2. (b)-Bottom: The lighter density corresponds to
a larger value of σ2.
Figure 9 provides a more focused comparison between the semi-BNP test and its Bayesian competitor,
the BNP energy test. This figure illustrates the proportion of rejecting H0over the 100 samples for both
Bayesian tests mentioned, across different data dimensions. The first row of Figure 9 represents the type I
error, while the remaining rows represent the test power. The figure demonstrates the effectiveness of the
29Published in Transactions on Machine Learning Research (09/2024)
0.40.60.81.0
0.00 0.25 0.50 0.75 1.00
υAUC
Type of the test: BNP−Energy BNP−MMD FNP−Energy FNP−MMD
(a)
υ=0.61υ=0.61υ=0.61
υ=0.69υ=0.69υ=0.69
0.00.51.01.52.0
0.000.250.500.751.00
υRBType of the BNP test
Energy
MMD
Type I Error=0.05 Type I Error=0.05 Type I Error=0.05
Type I Error=0.01 Type I Error=0.01 Type I Error=0.010.00.20.40.6
0.000.250.500.751.00
υp−Value
Type of the FNP test
Energy
MMD
0.00.10.20.30.40.5
−5.0 −2.5 0.0 2.5 5.0
xDensity (b)
Figure 8: (a) AUC values in testing alternative F1= 0.5N(−1 +υ,1) + 0.5N(1−υ,1)forυ∈(0,1)in
mixture example. (b)-Top: Test critical values against different values of σ2. (b)-Bottom: The lighter
density corresponds to a smaller value of υ.
semi-BNP kernel-based test in detecting differences, especially in scenarios involving variance shift, heavy
tail, and kurtosis examples, where the BNP-energy test does not perform optimally in high sample sizes.
Moreover, to conduct a comprehensive analysis of the large sample property of all the tests in comparison,
we present Table 2 for sample sizes n= 500,1000. This table clearly demonstrates the weak performance of
the BNP-Energy test in particular scenarios that are currently being mentioned.
Additionally, to display the asymptotic behavior of the posterior-based estimator compared to the baseline
(3) (FNP-MMD), we provide the density plot of both under the null hypothesis in Figure 10 demonstrating
the superior performance of the semi-BNP estimator in terms of faster convergence to zero.
F.2 The Semi-BNP GAN
Now, we examine the performance of the proposed GAN through additional datasets, the details of which
are given below. The generated samples are shown in Figures 11. Generally, the generated images using
semi-BNP GAN show better resolution than the FNP GAN. The MMD scores presented in Table 3 are also
evidence to demonstrate this claim. To further assess the performance of MMD-based GANs, we report the
commonlyusedFréchetinceptiondistance(FID)andtheKernelinceptiondistance(KID)metrics(Bińkowski
et al., 2018). These metrics are well-suited for evaluating the performance of GANs. The corresponding
scores13are reported in Table 3. Similar to our MMD scores, the smaller values of FID and KID show better
performance of the GAN.
F.2.1 Bone Marrow Biopsy Dataset (Tomczak & Welling, 2016):
The bone marrow biopsy (BMB) dataset is a collection of histopathology of BMB images corresponding to
16 patients with some types of blood cancer and anemia: 10 patients for training, 3 for testing, and 3 for
validation. This dataset contains 10,800 images in the size of 28×28pixels, 6,800 of which are considered
for the training set. The rest of the images have been divided into two sets of equal size for testing and
validation. The whole dataset can be found at https://github.com/jmtomczak/vae_householder_flow/
tree/master/datasets/histopathologyGray . The results based on 6800 training images are presented in
Figure 11-(a-c).
13The codes to compute the KID and FID are available at https://github.com/mbinkowski/MMD-GAN/blob/master/gan/
compute_scores.py .
30Published in Transactions on Machine Learning Research (09/2024)
0.00.1No diferencesd = 1 d = 5 d = 10 d = 20 d = 40 d = 60 d = 80 d = 100
0.500.751.00Mean shift
0.80.91.0Sk ewness
0.500.751.00Mixture
0.00.51.0V ariance shift
0.00.51.0Heavy tail
0 1000
n0.00.51.0K urtosis
0 1000
n0 1000
n0 1000
n0 1000
n0 1000
n0 1000
n0 1000
n
Semi- BNP-MMD test
BNP-Energy test
Figure 9: The proportion of rejecting H0out of 100 replications against sample of sizes n= 10,..., 1000
based on using a= 25,ℓ= 1000,ϵ= 10−3in equation 4, M= 20for the semi-BNP-MMD (blue line) and
BNP-energy (red dotted) tests.
31Published in Transactions on Machine Learning Research (09/2024)
Table 2: The average of RB, the average of its strength (Str ), and the relevant AUC out of 100 replications
based on using a= 25,ℓ= 1000,ϵ= 10−3in equation 4, M= 20, and bandwidth parameter σ= 80in RBF
kernel for two sample of data with n= 500,1000.
Example dBNP FNP
MMD Energy MMD Energy
RB(Str) AUC RB(Str) AUC P.value AUC P.value AUC
500 1000 500 1000 500 1000 500 1000 500 1000 500 1000 500 1000 500 1000
No diferences 1 4.72(0.78) 6.53(0.80) 3.75(0.60) 4.30(0.60) 0.52 0.50 0.48 0.49
5 18.84(0.86) 19.65(0.93) 18.74(0.88) 19.58(0.76) 0.50 0.51 0.51 0.44
10 19.98(0.92) 20(1) 20(1) 20(1) 0.51 0.50 0.53 0.48
20 20(1) 20(1) 20(1) 20(1) 0.53 0.51 0.51 0.44
40 20(1) 20(1) 20(1) 20(1) 0.45 0.52 0.51 0.50
60 20(1) 20(1) 20(1) 20(1) 0.51 0.50 0.50 0.53
80 20(1) 20(1) 20(1) 20(1) 0.49 0.48 0.54 0.49
100 20(1) 20(1) 20(1) 20(1) 0.49 0.48 0.51 0.50
Mean shift 1 0(0) 0(0) 1 1 0(0) 0(0) 0 .98 0.980.001 0.001 1 1 0.004 0.004 1 1
5 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
10 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
20 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
40 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
60 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
80 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
100 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
Skewness 1 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
5 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
10 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
20 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
40 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
60 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
80 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
100 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
Mixture 1 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.06 0.01 0.93 0.990.004 0.004 1 1
5 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
10 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
20 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
40 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
60 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
80 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
100 0(0) 0(0) 1 1 0(0) 0(0) 1 1 0.001 0.001 1 1 0.004 0.004 1 1
Variance shift 1 0.01(0) 0(0) 1 1 1.73(0.59) 2.10(0.59) 0.93 0.810.07 0.01 0.93 0.990.006 0.004 0.99 1
5 0.42(0.07) 0.40(0.08) 0.99 1 4.42(0.72) 7.30(0.70) 0.73 0.640.001 0.001 1 1 0.004 0.004 1 1
10 0.39(0.06) 0.22(0.06) 1 1 8.69(0.66) 13.12(0.73) 0.55 0.400.001 0.001 1 1 0.004 0.004 1 1
20 0(0) 0(0) 1 1 13.43(0.78) 18.12(0.69) 0.35 0.070.001 0.001 1 1 0.004 0.004 1 1
40 0(0) 0(0) 1 1 18.01(0.68) 19.82(0.68) 0.11 0 0.001 0.001 1 1 0.004 0.004 1 1
60 0(0) 0(0) 1 1 19.19(0.55) 19.98(0.94) 0.02 0 0.001 0.001 1 1 0.004 0.004 1 1
80 0(0) 0(0) 1 1 19.64(0.47) 20(1) 0 0 0.001 0.001 1 1 0.004 0.004 1 1
100 0(0) 0(0) 1 1 19.82(0.64) 20(1) 0 0 0.001 0.001 1 1 0.004 0.004 1 1
Heavy tail 1 0.05(0) 0(0) 1 1 1.65(0.54) 1.70(0.54) 0.96 0.990.03 0.004 0.96 0.990.01 0.005 0.98 0.99
5 0.04(0) 0 .02(0) 1 1 2.89(0.71) 4.53(0.74) 0.91 0.760.001 0.001 1 1 0.004 0.004 1 1
10 0(0) 0(0) 1 1 4.49(0.78) 7.87(0.73) 0.78 0.640.001 0.001 1 1 0.004 0.004 1 1
20 0(0) 0(0) 1 1 5.66(0.76) 11.73(0.75) 0.77 0.420.001 0.001 1 1 0.004 0.004 1 1
40 0(0) 0(0) 1 1 9.40(0.79) 16.41(0.78) 0.54 0.200.001 0.001 1 1 0.004 0.004 1 1
60 0(0) 0(0) 1 1 11.02(0.74) 18.06(0.82) 0.52 0.160.001 0.001 1 1 0.004 0.004 1 1
80 0(0) 0(0) 1 1 12.53(0.77) 18.51(0.90) 0.41 0.090.001 0.001 1 1 0.004 0.004 1 1
100 0(0) 0(0) 1 1 13.17(0.75) 19.07(0.97) 0.30 0.060.001 0.001 1 1 0.004 0.004 1 1
Kurtosis 1 0(0) 0(0) 1 1 1.23(0.42) 1.55(0.52) 0.96 0.950.002 0.001 0.99 1 0.004 0.004 1 1
5 0(0) 0(0) 1 1 1.75(0.59) 3.54(0.70) 0.96 0.880.001 0.001 1 1 0.004 0.004 1 1
10 0(0) 0(0) 1 1 2.81(0.66) 6.41(0.76) 0.94 0.750.001 0.001 1 1 0.004 0.004 1 1
20 0(0) 0(0) 1 1 4.63(0.71) 9.90(0.78) 0.84 0.510.001 0.001 1 1 0.004 0.004 1 1
40 0(0) 0(0) 1 1 5.70(0.73) 13.43(0.77) 0.74 0.280.001 0.001 1 1 0.004 0.004 1 1
60 0(0) 0(0) 1 1 7.06(0.75) 16.38(0.81) 0.72 0.230.001 0.001 1 1 0.004 0.004 1 1
80 0(0) 0(0) 1 1 8.11(0.79) 17.50(0.83) 0.71 0.130.001 0.001 1 1 0.004 0.004 1 1
100 0(0) 0(0) 1 1 8.83(0.78) 18.52(0.89) 0.55 0.090.001 0.001 1 1 0.004 0.004 1 1
32Published in Transactions on Machine Learning Research (09/2024)
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
V alue050010001500200025003000DensityDensity  Plots of the Empirical Estimator of MMD2
 (V-Statistic)
Sample Sizes
n=5
n=10
n=20
n=40
n=60
n=80
n=100
n=140
n=160
n=180
n=200
n=240
n=260
n=280
n=300
n=400
n=500
n=1000
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
V alue02004006008001000DensityDensity  Plots of the Posterior-based Estimator of MMD2
 (BNP counterpart of V-Statistic)
Sample Sizes
n=5
n=10
n=20
n=40
n=60
n=80
n=100
n=140
n=160
n=180
n=200
n=240
n=260
n=280
n=300
n=400
n=500
n=1000
Figure 10: Density plots of the semi-BNP and FNP estimators of the MMD for various sample sizes.
F.2.2 Labeled Faces in the Wild Dataset (Huang et al., 2008):
The labeled faces in the wild dataset (LFD) include 13,000 facial image samples with 1,024 ( 32×32)
dimensions. The dataset is available at https://conradsanderson.id.au/lfwcrop/ .
F.2.3 Brain Tumor MRI Dataset (Nickparvar, 2021):
In the last experiment, we consider a more challenging medical dataset including brain MRI images available
athttps://www.kaggle.com/dsv/2645886 . This dataset has two groups including training and testing sets.
Both are classified into four classes: glioma, meningioma, no tumor, and pituitary. To train the networks, we
consider all 5,712 training images. The images vary in size and have extra margins. We use a pre-processing
33Published in Transactions on Machine Learning Research (09/2024)
code14to remove margins and then resize images to 50×50pixels. We also scale the pixel value of prepared
images to range 0-1 to make the range of distribution of feature values equal and prevent any errors in the
backpropagation computation.
(a) Training data
 (b) Semi-BNP-MMD GAN
 (c) MMD-FNP GAN
(d) Training data
 (e) Semi-BNP-MMD GAN
 (f) MMD-FNP GAN
(g) Training data
 (h) Semi-BNP-MMD GAN
 (i) MMD-FNP GAN
Figure 11: Generated samples of sizes ( 6×6) from semi-BNP-MMD and MMD-FNP GAN for the BMB and
LFW datasets using a mixture of Gaussian kernels in 40,000 iterations.
Table 3: The values of MMD, KID, and FID scores for four groups of datasets considering nmb= 1000and
rmb= 1000in equation 24.
ScoresDataset
MNIST BMB LFW MRI
Semi-BNP FNP Semi-BNP FNP Semi-BNP FNP Semi-BNP FNP
MMD 0.0384 0.0404 0.0285 0.0315 0.0281 0.0302 0.2059 0.2231
KID 0.0034 0.0046 0.0030 0.0036 0.0019 0.0026 0.0260 0.0264
FID 35.560 37.934 17.006 17.264 14.010 14.473 87.975 87.831
14https://github.com/masoudnick/Brain-Tumor-MRI-Classification/blob/main/Preprocessing.py
34Published in Transactions on Machine Learning Research (09/2024)
F.2.4 Learning Rate Comparison: Semi-BNP-MMD GAN versus FNP Counterpart
To assess whether the proposed discriminator used in the Semi-BNP-MMD GAN leads to faster or better
convergence of the generated samples compared to the baseline proposed by Li et al. (2015), we consider
the synthetic distribution1
2N(−1d,Id) +1
2N(1d,Id)as the true distribution and provide the corresponding
MMD values for both models over 20,000 iterations in the data generation process, as shown in Figure 12.
Our proposed GAN clearly displays a higher speed of convergence for the corresponding cost function to
zero, and thus better performance compared to the baseline.
0 2500 5000 7500 10000 12500 15000 17500 20000
Iteration0.050.100.150.200.25MMDBNP-Learning
FNP-Learning
Figure 12: Learning rate: Values of the cost function in the proposed GAN and its frequentist counterpart
(Li et al., 2015) over 20,000iterations.
G More Discussion on the Potential Research
GANsare increasingly usedinmedical imagingapplicationswhich areeffectivetools fortaskssuch asmedical
imaging reconstructions. The synthetic images generated have often been proven to be valuable especially
when the original image is noisy or expensive to obtain. GANs have also been used for generating images in
cross-modality synthesis problems, where we observe magnetic resonance imaging (MRI) for a given patient
but want to generate computed tomography (CT) images for that same patient (Wolterink et al., 2017).
This type of generative method for medical imaging can drastically reduce the time and cost of obtaining
data if the quality of the synthetic examples is sufficiently high. GANs have also been used in a diagnostic
capacity–for example, in detecting brain lesions in images (Alex et al., 2017).
Here, the GAN is trained by distinguishing between labeled data of brain images that contain and do not
contain lesions. Then, the discriminator of the GAN is used to detect brain lesions on new images. However,
GANs are far less commonly used for tasks like diagnosis. According to a survey on medical imaging research
in GANs, less than 10%of the top papers surveyed were dedicated towards making diagnoses, whereas the
vast majority of papers were dedicated towards generating realistic synthetic examples of medical images for
further analysis (Yi et al., 2019). We believe this is because where the cost of making errors in diagnosis is
immediately consequential to people, unlike other AI applications where GANs are largely used.
We plan to extend the current work by mapping the data to a lower dimensional space using a variational
auto-encoder (VAE), a dimensionality reduction model helps to reduce the noise in data and tries to optimize
the cost function between the real data and fake data in the code space. There is a significant potential to
combine the elements of Stein variational gradient descent (SVGD) and GANs. For instance, SVGD can be
used for the inference of latent variables to approximate the variational distribution (the distribution of the
latent variable given the observed dataset) in a VAE-GAN. Then, we will propose a 3D semi-BNP GAN in
the code space to improve the ability of the GAN to generate medical datasets. The VAE method should
35Published in Transactions on Machine Learning Research (09/2024)
further reduce the chance of mode collapse and the 3D semi-BNP GAN will reduce the blurriness of the
generated samples that may be caused by using the VAE. In future work, our model will be able to generate
3D images and, hence, increase the resolution of images, especially for MRI images. We hope that our future
work will make an impact in the field of medical imaging.
H Notations
Notation Definition
N(·,·)Normal distribution
LN(·,·)Lognormal distribution
t3(·,·)t-distribution with 3 degrees of freedom
LG(·,·)Logistic distribution
Bdd×dmatrix with 0.25on the main diagonal and 0.2off the diagonal
cdd-dimensional column vector of c’s
Idd×didentical matrix
In all distribution notations, the first component represents the mean vector and the second component
represents the covariance matrix.
I A Review on the Approximate Bayesian Computation
For a given data space Xand the set of Borel probability distributions B(X), consider the observations x1:n
drawn from FTrue∈B(X). In the standard Bayesian perspective, given a parametric model Bθ(X) ={Fθ:
θ∈Θ}⊂B(X), a priorπis placed on the parameter space θ. After observing data x1:n, the prior is updated
to obtain a posterior distribution given by:
π(θ|x1:n) =L(x1:n|θ)π(θ)/integraltext
ΘL(x1:n|θ)π(θ)dθ
whereL(x1:n|θ) =/producttextn
i=1fXi(xi|θ)is the likelihood function and fθis the density function corresponding to
the distribution Fθ.
For the standard Bayesian inference to be considered well-specified, there must exist a parameter θ0∈Θsuch
that the distribution Fθ0matches the true data distribution FTrue, i.e.,Fθ0=FTrue. When the parametric
modelFΘdoes not contain FTrue(i.e., there is no θ0∈Θsuch thatFθ0=FTrue), the model is said to be
misspecified. This can lead to problems in the standard Bayesian inference, as the posterior distribution will
be based on an incorrect model assumption.
ABC addresses some of these challenges, particularly in scenarios where the likelihood function is intractable
or difficult to compute. ABC circumvents the need for explicit likelihood evaluations by using simulations,
summary statistics, and a comparison mechanism between simulated and observed data. Below shows how
ABC works within this framework:
1.Simulation : Generate synthetic datasets by sampling parameters θfrom the prior distribution π
and simulating data y1:nfromFθ. HereFθcorresponds to the distribution of a generative model
with implicitly defined likelihood functions.
2.Summary Statistics : Reduce both the observed data x1:nand the simulated data y1:nto lower-
dimensional summary statistics S(x1:n)andS(y1:n). These summary statistics should capture the
essential features of the data relevant to the parameters of interest.
3.Comparison : Compute a distance metric between S(x1:n)andS(y1:n).
4.Acceptance Criterion : Accept the parameter values θif the distance between S(x1:n)andS(y1:n)
is less than a predefined tolerance level ϵ. This results in an approximate posterior distribution based
on the accepted parameters.
36Published in Transactions on Machine Learning Research (09/2024)
5.Posterior Approximation : The ABC posterior distribution is then given by:
πϵ(θ|S(x1:n))∝/integraldisplay
X···/integraldisplay
Xn/productdisplay
i=1π(θ)fS(Yi)(S(yi)|θ)I(δ(S(xi),S(yi))≤ϵ)dy1···dyn
where Iis an indicator function that equals 1 if the distance δis within the tolerance ϵ, and 0
otherwise (Beaumont, 2019).
As a result, the ABC posterior distribution converges to the standard posterior as ϵapproaches 0 (Beaumont,
2019; Dellaporta et al., 2022). This convergence follows from the fact that as ϵdecreases, the ABC posterior
places increasing weight on θvalues that generate simulated data close to the observed data, effectively
approximating the likelihood function. However, as already mentioned, the standard Bayesian posterior can
be sensitive to model misspecification. If the assumed model is not a good representation of the true data-
generating process, the Bayesian posterior (and thus the ABC posterior when ϵ→0) may give misleading
inferences. This lack of robustness to model misspecification can lead to poor performance in practice.
37