Published in Transactions on Machine Learning Research (06/2024)
A Note on the Convergence of Denoising Diffusion Proba-
bilistic Models
Sokhna Diarra Mbacke sokhna-diarra.mbacke.1@ulaval.ca
Université Laval
Omar Rivasplata o.rivasplata@ucl.ac.uk
University College London
Reviewed on OpenReview: https: // openreview. net/ forum? id= wLe1bG93yc
Abstract
Diffusion models are one of the most important families of deep generative models. In this
note, we derive a quantitative upper bound on the Wasserstein distance between the target
distribution and the distribution learned by a diffusion model. Unlike previous works on this
topic, our result does not make assumptions on the learned score function. Moreover, our
result holds for arbitrary data-generating distributions on bounded instance spaces, even
those without a density with respect to Lebesgue measure, and the upper bound does not
suffer from exponential dependencies on the ambient space dimension. Our main result
builds upon the recent work of Mbacke et al. (2023) and our proofs are elementary.
1 Introduction
Along with generative adversarial networks (Goodfellow et al., 2014) and variational autoencoders (VAEs)
(Kingma & Welling, 2014; Rezende et al., 2014), diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon,
2019; Ho et al., 2020) are one of the most prominent families of deep generative models. They have exhibited
impressive empirical performance in image (Dhariwal & Nichol, 2021; Ho et al., 2022) and audio (Chen et al.,
2021; Popov et al., 2021) generation, as well as other applications (Zhou et al., 2021; Sasaki et al., 2021; Li
et al., 2022; Trabucco et al., 2023).
There are two main approaches to diffusion models: denoising diffusion probabilistic models (DDPMs) (Sohl-
Dickstein et al., 2015; Ho et al., 2020) and score-based generative models (Song & Ermon, 2019) (SGMs).
The former kind, DDPMs, progressively transform samples from the target distribution into noise through
a forward process, and train a backward process that reverses the transformation and is used to generate
new samples. On the other hand, SGMs use score matching techniques (Hyvärinen & Dayan, 2005; Vincent,
2011) to learn an approximation of the score function of the data-generating distribution, then generate new
samples using Langevin dynamics. Since for real-world distributions the score function might not exist, Song
& Ermon (2019) propose adding different noise levels to the training samples to cover the whole instance
space, and train a neural network to simultaneously learn the score function for all noise levels.
Although DDPMs and SGMs might appear to be different approaches at first, Ho et al. (2020) showed that
DDPMs implicitly learn an approximation of the score function and the sampling process resembles Langevin
dynamics. Furthermore, Song et al. (2021b) derived a unifying view of both techniques using stochastic
differential equations (SDEs). The SGM of Song & Ermon (2019) can be seen as a discretization of the
Brownian motion process, and the DDPM of Ho et al. (2020) as a discretization of an Ornstein–Uhlenbeck
process. Hence, both DDPMs and SGMs are usually referred to as SGMs in the literature. This explains why
the previous works studying the theoretical properties of diffusion models utilize the score-based formulation,
which requires assumptions on the performance of the learned score function.
1Published in Transactions on Machine Learning Research (06/2024)
In this work, we take a different approach and apply techniques developed by Mbacke et al. (2023) for VAEs
to DDPMs, which can be seen as hierarchical VAEs with fixed encoders (Luo, 2022). This approach allows
us to derive quantitative Wasserstein-based upper bounds, with no assumptions on the data distribution,
no assumptions on the learned score function, and elementary proofs that do not require the SDE toolbox.
Moreover, our bounds do not suffer from any costly discretization step, such as the one in De Bortoli (2022),
since we consider the forward and backward processes as being discrete-time from the outset, instead of
seeing them as discretizations of continuous-time processes.
1.1 Related Works
There has been a growing body of work aiming to establish theoretical results on the convergence of SGMs
(Block et al., 2020; De Bortoli et al., 2021; Song et al., 2021a; Lee et al., 2022; De Bortoli, 2022; Kwon et al.,
2022; Lee et al., 2023; Chen et al., 2023; Li et al., 2023; Benton et al., 2023), but these works either rely
on strong assumptions on the data-generating distribution, derive non quantitative upper bounds, or suffer
from exponential dependencies on some of the parameters. We manage to avoid all three of these pitfalls.
The bounds of Lee et al. (2022) rely on very strong assumptions on the data-generating distribution, such as
log-Sobolev inequalities, which are not realistic for real-world data distributions. Furthermore, Song et al.
(2021a); Chen et al. (2023); Lee et al. (2023) establish upper bounds on the Kullback-Leibler (KL) divergence
or the total variation (TV) distance between the data-generating distribution and the distribution learned by
the diffusion model; however, as noted by Pidstrigach (2022) and Chen et al. (2023), unless one makes strong
assumptions on the support of the data-generating distribution, KL and TV reach their maximum values.
Suchassumptionsarguablydonotholdforreal-worlddata-generatingdistributionswhicharewidelybelieved
to satisfy the manifold hypothesis (Narayanan & Mitter, 2010; Fefferman et al., 2016; Pope et al., 2021).
The work of Pidstrigach (2022) establishes conditions under which the support of the input distribution is
equal to the support of the learned distribution, and generalizes the bound of Song et al. (2021a) to all
f-divergences. Assuming L2accurate score estimation, Chen et al. (2023) and Lee et al. (2023) establish
Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution, but their
Wasserstein-based bounds are not quantitative. De Bortoli (2022) derives quantitative Wasserstein distance
upper bounds under the manifold hypothesis, but their bounds suffer from exponential dependencies on some
of the problem parameters.
1.2 Our contributions
In this work, we avoid strong assumptions on the data-generating distribution, and establish a quantitative
Wasserstein distance upper bound without exponential dependencies on problem parameters including ambi-
entspacedimension. Moreover, acommonthreadintheworkscitedaboveisthattheirboundsdependonthe
error of the score estimator. According to Chen et al. (2023), “Providing precise guarantees for estimation of
the score function is difficult, as it requires an understanding of the non-convex training dynamics of neural
network optimization that is currently out of reach.” Hence, we derive upper bounds without assumptions
on the learned score function. Instead, our bound depends on a reconstruction loss computed on a finite
i.i.d. sample. Intuitively, we define a loss function ℓθ(xT,x0)(see equation 6), which measures the average
Euclidean distance between a sample x0from the data-generating distribution, and the reconstruction ˆx0
obtained by sampling noise xT∼q(xT|x0)and passing it through the backward process (parameterized by
θ). This approach is motivated by the work of Mbacke et al. (2023) on VAEs.
There are many advantages to this approach: no restrictive assumptions on the data-generating distribution,
no exponential dependencies on the dimension, and a quantitative upper bound based on the Wasserstein
distance. Moreover, our approach has the benefit of utilizing very simple and elementary proofs.
2 Preliminaries
Throughoutthepaper, weuselower-caseletterstodenotebothprobabilitymeasuresandtheirdensitiesw.r.t.
the Lebesgue measure, and we add variables in parentheses to improve readability (e.g. q(xt|xt−1)to indicate
a time-dependent conditional distribution). We consider an instance space Xwhich is a subset of RDwith
2Published in Transactions on Machine Learning Research (06/2024)
Figure 1: Denoising diffusion model
the Euclidean distance as underlying metric, and a target data-generating distribution µ∈M1
+(X). Notice
that we do not assume µhas a density w.r.t. the Lebesgue measure. Moreover, ∥·∥denotes the Euclidean
(L2) norm and we write Ep(x)as a shorthand for Ex∼p(x). Given probability measures p,q∈M1
+(X)and a
real number k>1, the Wasserstein distance of order kis defined as (Villani, 2009):
Wk(p,q) =/parenleftbigg
inf
π∈Γ(p,q)/integraldisplay
X∥x−y∥kdπ(x,y)/parenrightbigg1/k
,
where Γ(p,q)denotes the set of couplings of pandq, meaning the set of joint distributions on X×Xwith
respective marginals pandq. We refer to the product measure p⊗qas thetrivial coupling , and we refer to
the Wasserstein distance of order 1simply as the Wasserstein distance .
2.1 Denoising Diffusion Models
Instead of using the SDE arsenal, we present diffusion models using the DDPM formulation with discrete-
time processes. A diffusion model comprises two discrete-time stochastic processes: a forward process and
a backward process. Both processes are indexed by time 0≤t≤T, where the number of time-steps Tis a
pre-set choice. See Figure 1 for an illustration, and Luo (2022) for a detailed tutorial.
The forward process. The forward process transforms a datapoint x0∼µinto a noise distribution
q(xT|x0), via a sequence of conditional distributions q(xt|xt−1)for1≤t≤T. It is assumed that the forward
process is defined so that for large enough T, the distribution q(xT|x0)is close to a simple noise distribution
p(xT)which is referred to as the prior distribution . For instance, Ho et al. (2020) chose p(xT) =N(xT;0,I),
the standard multivariate normal distribution.
The backward process. The backward process is a Markov process with parametric transition kernels.
The goal of the backward process is to implement the reverse action of the forward process: transforming
noise samples into (approximate) samples from the distribution µ. Following Ho et al. (2020), we assume
the backward process to be defined by Gaussian distributions pθ(xt−1|xt)defined for 2≤t≤Tas
pθ(xt−1|xt) =N/parenleftbig
xt−1;gt
θ(xt),σ2
tI/parenrightbig
, (1)
and
pθ(x0|x1) =g1
θ(x1), (2)
where the variance parameters σ2
1,...,σ2
T∈R≥0are defined by a fixed schedule, the mean functions gt
θ:
RD→RDare learned using a neural network (with parameters θ) for 2≤t≤T, andg1
θ:RD→Xis a
separate function dependent on σ1. In practice, Ho et al. (2020) used the same network for the functions gt
θ
for2≤t≤T, and a separate discrete decoder for g1
θ.
Generatingnewsamplesfromatraineddiffusionmodelisdonebysampling xt−1∼pθ(xt−1|xt)for1≤t≤T,
starting from a noise vector xT∼p(xT)sampled from the prior p(xT).
We make the following assumption on the backward process.
Assumption 1. Weassumeforeach 1≤t≤Tthereexistsaconstant Kt
θ>0suchthatforevery x1,x2∈X,
/vextenddouble/vextenddoublegt
θ(x1)−gt
θ(x2)/vextenddouble/vextenddouble≤Kt
θ∥x1−x2∥.
In other words, gt
θisKt
θ-Lipschitz continuous. We discuss this assumption in Remark 3.2.
3Published in Transactions on Machine Learning Research (06/2024)
2.2 Additional Definitions
We define the distribution πθ(·|x0)as
πθ(·|x0) =q(xT|x0)pθ(xT−1|xT)pθ(xT−2|xT−1)...pθ(x1|x2)pθ(·|x1). (3)
Intuitively, for each x0∈X,πθ(·|x0)denotes the distribution on Xobtained by reconstructing samples from
q(xT|x0)through the backward process. Another way of seeing this distribution is that for any function
f:X→R, the following equation holds:1
E
πθ(ˆx0|x0)f(ˆx0) = E
q(xT|x0)E
pθ(xT−1|xT)... E
pθ(x1|x2)E
pθ(ˆx0|x1)f(ˆx0). (4)
Given a finite set S={x1
0,...,xn
0}iid∼µ, we define the regenerated distribution as the following mixture:
µn
θ=1
nn/summationdisplay
i=1πθ(·|xi
0). (5)
This definition is analogous to the empirical regenerated distribution defined by Mbacke et al. (2023) for
VAEs. The distribution on Xlearned by the diffusion model is denoted πθ(·)and defined as
πθ(·) =p(xT)pθ(xT−1|xT)pθ(xT−2|xT−1)...pθ(x1|x2)pθ(·|x1).
In other words, for any function f:X→R, the expectation of fw.r.t.πθ(·)is
E
πθ(ˆx0)f(ˆx0) =E
p(xT)E
pθ(xT−1|xT)... E
pθ(x1|x2)E
pθ(ˆx0|x1)f(ˆx0).
Hence, both πθ(·)andπθ(·|x0)are defined using the backward process, with the difference that πθ(·)starts
with the prior p(xT) =N(xT;0,I)whileπθ(·|x0)starts with the noise distribution q(xT|x0).
Finally, we define the loss function ℓθ:X×X→ Ras
ℓθ(xT,x0) = E
pθ(xT−1|xT)E
pθ(xT−2|xT−1)... E
pθ(x1|x2)E
pθ(ˆx0|x1)∥x0−ˆx0∥. (6)
Hence, given a noise vector xTand a sample x0, the lossℓθ(xT,x0)denotes the average Euclidean distance
between x0and any sample obtained by passing xTthrough the backward process.
2.3 Our Approach
The goal is to upper-bound the distance W1(µ,πθ(·)). Since the triangle inequality implies
W1(µ,πθ(·))≤W1(µ,µn
θ) +W1(µn
θ,πθ(·)), (7)
we can upper-bound the distance W1(µ,πθ(·))by upper-bounding the two expressions on the right-hand side
of equation 7 separately. The upper bound on W1(µ,µn
θ)is obtained using a straightforward adaptation of a
proof that was developed by Mbacke et al. (2023). First, W1(µ,µn
θ)is upper-bounded using the expectation
of the loss function ℓθ, then the resulting expression is upper-bounded using a PAC-Bayesian-style expression
dependent on the empirical risk and the prior-matching term.
The upper bound on the second term W1(µn
θ,πθ(·))uses the definition of µn
θ. Intuitively, the difference
betweenπθ(·|xi
0)andπθ(·)is determined by the corresponding initial distributions: q(xT|xi
0)forπθ(·|xi
0),
andp(xT)forπθ(·). Hence, if the two initial distributions are close, and if the steps of the backward process
are smooth (see Assumption 1), then πθ(·|xi
0)andπθ(·)are close to each other.
1More formally, we give a definition of πθ(·|x0)via expectations of test functions by requiring that equation 4 holds for every
functionfin some appropriate measure-determining function class.
4Published in Transactions on Machine Learning Research (06/2024)
3 Main Result
3.1 Theorem Statement
We are now ready to state our main result: a quantitative upper bound on the Wasserstein distance between
the data-generating distribution µand the learned distribution πθ(·).
Theorem 3.1. Assume the instance space Xhas finite diameter ∆ = supx,x′∈X∥x−x′∥<∞, and letλ>0
andδ∈(0,1)be real numbers. Using the definitions and assumptions of the previous section, the following
inequality holds with probability at least 1−δover the random draw of S={x1
0,...,xn
0}iid∼µ:
W1(µ,πθ(·))≤1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)ℓθ(xT,xi
0)/bracerightbigg
+1
λ/bracketleftiggn/summationdisplay
i=1KL(q(xT|xi
0)||p(xT)) + log1
δ/bracketrightigg
+λ∆2
8n+
/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)E
p(yT)∥xT−yT∥/bracerightbigg
+/parenleftiggT/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′∥ϵ−ϵ′∥.(8)
Where ϵ,ϵ′∼N (0,I)are standard Gaussian vectors.
Remark 3.1. Before presenting the proof, let us discuss Theorem 3.1.
•Because the right-hand side of equation 8 depends on a quantity computed using a finite i.i.d. sample
S, the bound holds with high probability w.r.t. the randomness of S. This is the price we pay for
having a quantitative upper bound with no exponential dependencies on problem parameters and
no assumptions on the data-generating distribution µ.
•The first term of the right-hand side of equation 8 is the average reconstruction loss computed over
the sample S={x1
0,...,xn
0}. Note that for each 1≤i≤n, the expectation of ℓθ(xT|xi
0)is only
computed w.r.t. the noise distribution q(xT|xi
0)defined by xi
0itself. Hence, this term measures how
well a noise vector xT∼q(xT|xi
0)recovers the original sample xi
0using the backward process, and
averages over the set S={x1
0,...,xn
0}.
•If the Lipschitz constants satisfy Kt
θ<1for all 1≤t≤T, then the larger Tis, the smaller the
upper bound gets. This is because the product of Kt
θ’s then converges to 0. In Remark 3.2 below,
we show that the assumption that Kt
θ<1for alltis a quite reasonable one.
•Thehyperparameter λcontrolsthetrade-offbetweentheprior-matching(KL)termandthediameter
term∆2
8n. IfKt
θ<1for all 1≤t≤TandT→∞, then the convergence of the bound largely depends
on the choice of λ. In that case, λ∝n1/2leads to a faster convergence, while λ∝nleads to a
slower convergence to a smaller quantity. This is because the bound of Mbacke et al. (2023) stems
from PAC-Bayesian theory, where this trade-off is common, see e.g. Alquier (2021).
•The last term of equation 8 does not depend on the sample size n. Hence, the upper bound given
by Theorem 3.1 does not converge to 0asn→∞. However, if the Lipschitz factors (Kt
θ)1≤t≤Tare
all less than 1, then this term can be very small, specially in low dimensional spaces.
3.2 Proof of the main theorem
The following result is an adaptation of a result by Mbacke et al. (2023).
Lemma 3.2. Letλ>0andδ∈(0,1)be real numbers. With probability at least 1−δover the randomness
of the sample S={x1
0,...,xn
0}iid∼µ, the following holds:
W1(µ,µn
θ)≤1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)ℓθ(xT,xi
0)/bracerightbigg
+1
λ/bracketleftiggn/summationdisplay
i=1KL(q(xT|xi
0)||p(xT)) + log1
δ/bracketrightigg
+λ∆2
8n.(9)
5Published in Transactions on Machine Learning Research (06/2024)
The proof of this result is a straightforward adaptation of Mbacke et al. (2023, Lemma D.1). We provide
the proof in the supplementary material (Section A.1) for completeness.
Now, let us focus our attention on the second term of the right-hand side of equation 7, namely W1(µn
θ,πθ(·)).
This part is trickier than for VAEs, for which the generative model’s distribution is simply a pushforward
measure. Here, we have a non-deterministic sampling process with Tsteps.
Assumption 1 leads to the following lemma on the backward process.
Lemma 3.3. For any given x1,y1∈Xwe have
E
pθ(x0|x1)E
pθ(y0|y1)∥x0−y0∥≤K1
θ∥x1−y1∥.
Moreover, if 2≤t≤T, then for any given xt,yt∈Xwe have
E
pθ(xt−1|xt)E
pθ(yt−1|yt)∥xt−1−yt−1∥≤Kt
θ∥xt−yt∥+σtE
ϵ,ϵ′∥ϵ−ϵ′∥,
where ϵ,ϵ′∼N (0,I), meaning Eϵ,ϵ′is a shorthand for Eϵ,ϵ′∼N(0,I).
Proof.For the first part, let x1,y1∈X. Since according to equation 2 we have pθ(x0|x1) =δg1
θ(x1)(x0)and
pθ(y0|y1) =δg1
θ(y1)(y0), then
E
pθ(x0|x1)E
pθ(y0|y1)∥x0−y0∥=/vextenddouble/vextenddoubleg1
θ(x1)−g1
θ(y1)/vextenddouble/vextenddouble≤K1
θ∥x1−y1∥.
For the second part, let 2≤t≤Tandxt,yt∈X. Sincepθ(xt−1|xt) =N/parenleftbig
xt−1;gt
θ(xt),σ2
tI/parenrightbig
by equation 1,
the reparameterization trick (Kingma & Welling, 2014) implies that sampling
xt−1∼pθ(xt−1|xt)
is equivalent to setting
xt−1=gt
θ(xt) +σtϵt,with ϵt∼N (0,I). (10)
Using equation 10, the triangle inequality, and Assumption 1, we obtain
E
pθ(xt−1|xt)E
pθ(yt−1|yt)∥xt−1−yt−1∥=E
ϵtE
ϵ′
t/vextenddouble/vextenddoublegt
θ(xt) +σtϵt−gt
θ(yt)−σtϵ′
t/vextenddouble/vextenddouble
≤E
ϵtE
ϵ′
t/vextenddouble/vextenddoublegt
θ(xt)−gt
θ(yt)/vextenddouble/vextenddouble+σtE
ϵtE
ϵ′
t∥ϵt−ϵ′
t∥
=/vextenddouble/vextenddoublegt
θ(xt)−gt
θ(yt)/vextenddouble/vextenddouble+σtE
ϵtE
ϵ′
t∥ϵt−ϵ′
t∥
≤Kt
θ∥xt−yt∥+σtE
ϵE
ϵ′∥ϵ−ϵ′∥,
where ϵ,ϵ′∼N (0,I).
Next, we can use the inequalities of Lemma 3.3 to prove the following result.
Lemma 3.4. LetT≥1. The following inequality holds:
E
pθ(xT−1|xT)E
pθ(yT−1|yT)E
pθ(xT−2|xT−1)E
pθ(yT−2|yT−1)... E
pθ(x0|x1)E
pθ(y0|y1)∥x0−y0∥≤
/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
∥xT−yT∥+/parenleftiggT/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥],
where ϵ,ϵ′∼N (0,I).
Proof Idea. Lemma 3.4 is proven by induction using Lemma 3.3 in the induction step. The details are in
the supplementary material (Section A.2).
6Published in Transactions on Machine Learning Research (06/2024)
Using the two previous lemmas, we obtain the following upper bound on W1(µn
θ,πθ(·)).
Lemma 3.5. The following inequality holds:
W1(µn
θ,πθ(·))≤/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)E
p(yT)∥xT−yT∥/bracerightbigg
+/parenleftiggT/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′∥ϵ−ϵ′∥,
where ϵ,ϵ′∼N (0,I).
Proof.Using the definition of W1, the trivial coupling, the definitions of µn
θandπθ(·), and Lemma 3.4, we
get
W1(µn
θ,πθ(·)) = inf
π∈Γ(µn
θ,πθ(·))/integraldisplay
d(x,y)dπ(x,y)
≤E
x∼µn
θE
y∼πθ(y)[∥x−y∥]
=1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)E
p(yT)E
pθ(xT−1|xT)E
pθ(yT−1|yT)... E
pθ(x0|x1)E
pθ(y0|y1)∥x0−y0∥/bracerightbigg
≤1
nn/summationdisplay
i=1

E
q(xT|xi
0)E
p(yT)
/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
∥xT−yT∥+
T/summationdisplay
t=2
t−1/productdisplay
j=1Kj
θ
σt
E
ϵ,ϵ′[∥ϵ−ϵ′∥]



=/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)E
p(yT)∥xT−yT∥/bracerightbigg
+/parenleftiggT/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′∥ϵ−ϵ′∥.
Combining Lemmas 3.2 and 3.5 with equation 7 yields Theorem 3.1 .
3.3 Special case using the forward process of Ho et al. (2020)
Theorem 3.1 establishes a general upper bound that holds for any forward process, as long as the backward
process satisfies Assumption 1. In this section, we specialize the statement of the theorem to the particular
case of the forward process defined by Ho et al. (2020).
LetX⊆RD. In Ho et al. (2020), the forward process is a Gauss-Markov process with transition densities
defined as
q(xt|xt−1) =N(xt;√αtxt−1,(1−αt)I),
whereα1,...,αTis a fixed noise schedule such that 0<αt<1for allt. This definition implies that at each
time step 1≤t≤T,
q(xt|x0) =N/parenleftbig
xt;√¯αtx0,(1−¯αt)I/parenrightbig
,with ¯αt=t/productdisplay
i=1αi.
The optimization objective to train the backward process ensures that for each time step tthe distribution
pθ(xt−1|xt)remains close to the ground-truth distribution q(xt−1|xt,x0)given by
q(xt−1|xt,x0) =N/parenleftbig
xt−1;µt
q(xt,x0),σ2
tI/parenrightbig
,
where
µt
q(xt,x0) =√αt(1−¯αt−1)
1−¯αtxt+√¯αt−1(1−αt)
1−¯αtx0. (11)
Now, we discuss Assumption 1 under these definitions.
7Published in Transactions on Machine Learning Research (06/2024)
Remark 3.2. We can get a glimpse at the range of Kt
θfor a trained DDPM by looking at the distribution
q(xt−1|xt,x0), sincepθ(xt−1|xt)is optimized to be as close as possible to q(xt−1|xt,x0).
For a given x0∼µ, let us take a look at the Lipschitz norm of x∝⇕⊣√∫⊔≀→µt
q(x,x0). Using equation 11, we have
µt
q(xt,x0)−µt
q(yt,x0) =√αt(1−¯αt−1)
1−¯αt(xt−yt).
Hence, x∝⇕⊣√∫⊔≀→µt
q(x,x0)isK′
t-Lipschitz continuous with
K′
t=√αt(1−¯αt−1)
1−¯αt.
Now, ifαt<1for all 1≤t≤T, then we have 1−¯αt>1−¯αt−1which implies K′
t<1for all 1≤t≤T.
Remark 3.2 shows that the Lipschitz norm of the mean function µt
q(·,x0)does not depend on x0. Indeed,
looking at the previous equation, we can see that for any initial x0, the Lipschitz norm K′
t=√αt(1−¯αt−1)
1−¯αt
only depends on the noise schedule, not x0itself. Since gt
θis optimized to match to µt
q(·,x0), for each x0in
the training set, and all the functions µt
q(·,x0)have the same Lipschitz norm K′
t, we believe it is reasonable
to assumegt
θis Lipschitz continuous as well. This is the intuition behind Assumption 1.
The prior-matching term. With the definitions of this section, the prior matching term
KL(q(xT|x0)||p(xT))has the following closed form:
KL(q(xT|x0)||p(xT)) =1
2/bracketleftig
−Dlog(1−¯αT)−D¯αT+ ¯αT∥x0∥2/bracketrightig
.
Upper-bounds on the average distance between Gaussian vectors. Ifϵ,ϵ′areD-dimensional
vectors sampled from N(0,I), then
E
ϵ,ϵ′∥ϵ−ϵ′∥≤√
2D.
Moreover, since q(xT|xi
0) =N/parenleftbig
xT; ¯αTxi
0,(1−¯αT)I/parenrightbig
and the prior p(yT) =N(yT;0,I),
E
q(xT|xi
0)E
p(yT)∥xT−yT∥≤/radicalig
¯αT/vextenddouble/vextenddoublexi
0/vextenddouble/vextenddouble2+ (2−¯αT)D.
Special case of the main theorem. With the definitions of this section, the inequality of Theorem 3.1
implies that with probability at least 1−δover the randomness of {x1
0,...,xn
0}iid∼µ:
W1(µ,πθ(·))≤1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)ℓθ(xT,xi
0)/bracerightbigg
+1
λ/bracketleftigg
1
2n/summationdisplay
i=1/bracketleftig
−Dlog(1−¯αT)−D¯αT+ ¯αT/vextenddouble/vextenddoublexi
0/vextenddouble/vextenddouble2/bracketrightig
+ log1
δ/bracketrightigg
+
λ∆2
8n+/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
1
nn/summationdisplay
i=1/radicalig
¯αT/vextenddouble/vextenddoublexi
0/vextenddouble/vextenddouble2+ (2−¯αT)D+√
2D/parenleftiggT/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
.
4 Conclusion
This note presents a novel upper bound on the Wasserstein distance between the data-generating distribution
and the distribution learned by a diffusion model. Unlike previous works in the field, our main result
simultaneously avoids strong assumptions on the data-generating distribution, assumptions on the learned
score function, and exponential dependencies, while still providing a quantitative upper bound. However,
our bound holds with high probability on the randomness of a finite i.i.d. sample, on which a loss function is
computed. Since the loss is a chain of expectations w.r.t. Gaussian distributions, it can either be estimated
with high precision or upper bounded using the properties of Gaussian distributions.
8Published in Transactions on Machine Learning Research (06/2024)
Acknowledgments
This research is supported by the Canada CIFAR AI Chair Program, and the NSERC Discovery grant
RGPIN-2020-07223. TheauthorssincerelythankPascalGermainforinterestingdiscussionsandsuggestions.
The first author thanks Mathieu Bazinet and Florence Clerc for proof-reading the manuscript.
References
Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. arXiv preprint arXiv:2110.11216 , 2021.
Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-Linear Convergence
Bounds for Diffusion Models via Stochastic Localization. arXiv preprint arXiv:2308.03686 , 2023.
Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative Modeling with Denoising Auto-Encoders
and Langevin Sampling. arXiv preprint arXiv:2002.00107 , 2020.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. WaveGrad:
Estimating Gradients for Waveform Generation. In International Conference on Learning Representations ,
2021.
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning
the score: theory for diffusion models with minimal data assumptions. In The Eleventh International
Conference on Learning Representations , 2023.
Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Transactions
on Machine Learning Research , 2022. ISSN 2835-8856.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger Bridge with
Applications to Score-Based Generative Modeling. In Advances in Neural Information Processing Systems ,
volume 34, pp. 17695–17709, 2021.
Prafulla Dhariwal and Alexander Nichol. Diffusion Models Beat GANs on Image Synthesis. In Advances in
Neural Information Processing Systems , volume 34, pp. 8780–8794, 2021.
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the Manifold Hypothesis. Journal of
the American Mathematical Society , 29(4):983–1049, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Networks. In Advances in Neural Information
Processing Systems , volume 27, 2014.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Advances in Neural
Information Processing Systems , volume 33, pp. 6840–6851, 2020.
Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.
Cascaded Diffusion Models for High Fidelity Image Generation. Journal of Machine Learning Research ,
23(1):2249–2281, 2022.
Aapo Hyvärinen and Peter Dayan. Estimation of Non-Normalized Statistical Models by Score Matching.
Journal of Machine Learning Research , 6(4), 2005.
Diederik P. Kingma and M. Welling. Auto-Encoding Variational Bayes. CoRR, abs/1312.6114, 2014.
Dohyun Kwon, Ying Fan, and Kangwook Lee. Score-based Generative Modeling Secretly Minimizes the
Wasserstein Distance. Advances in Neural Information Processing Systems , 35:20205–20217, 2022.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial
complexity. In Advances in Neural Information Processing Systems , volume 35, pp. 22870–22882, 2022.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data
distributions. In International Conference on Algorithmic Learning Theory , pp. 946–985. PMLR, 2023.
9Published in Transactions on Machine Learning Research (06/2024)
GenLi, YutingWei, YuxinChen, andYuejieChi. TowardsFasterNon-AsymptoticConvergenceforDiffusion-
Based Generative Models. arXiv preprint arXiv:2306.09251 , 2023.
Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen.
SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models. Neurocomputing , 479:47–59,
2022.
Calvin Luo. Understanding Diffusion Models: A Unified Perspective. arXiv preprint arXiv:2208.11970 , 2022.
Sokhna Diarra Mbacke, Florence Clerc, and Pascal Germain. Statistical Guarantees for Variational Autoen-
coders using PAC-Bayesian Theory. In Advances in Neural Information Processing Systems , 2023.
Hariharan Narayanan and Sanjoy Mitter. Sample Complexity of Testing the Manifold Hypothesis. In
Advances in Neural Information Processing Systems , volume 23, 2010.
Jakiw Pidstrigach. Score-Based Generative Models Detect Manifolds. In Advances in Neural Information
Processing Systems , volume 35, pp. 35852–35865, 2022.
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The Intrinsic Dimension
of Images and Its Impact on Learning. In International Conference on Learning Representations , 2021.
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS: A
Diffusion Probabilistic Model for Text-to-Speech. In International Conference on Machine Learning , pp.
8599–8608. PMLR, 2021.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approxi-
mate Inference in Deep Generative Models. In International Conference on Machine Learning , pp. 1278–
1286. PMLR, 2014.
Hiroshi Sasaki, Chris G Willcocks, and Toby P Breckon. UNIT-DDPM: UNpaired Image Translation with
Denoising Diffusion Probabilistic Models. arXiv preprint arXiv:2104.05358 , 2021.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning
usingNonequilibriumThermodynamics. In International Conference on Machine Learning , pp.2256–2265.
PMLR, 2015.
Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. In
Advances in Neural Information Processing Systems , volume 32, 2019.
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum Likelihood Training of Score-Based
Diffusion Models. In Advances in Neural Information Processing Systems , volume 34, 2021a.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference
on Learning Representations , 2021b.
Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective Data Augmentation
With Diffusion Models. arXiv preprint arXiv:2302.07944 , 2023.
Cédric Villani. Optimal Transport: Old and New , volume 338. Springer, 2009.
Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural Computation ,
23(7):1661–1674, 2011.
Linqi Zhou, Yilun Du, and Jiajun Wu. 3D Shape Generation and Completion through Point-Voxel Diffusion.
InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5826–5835, 2021.
10Published in Transactions on Machine Learning Research (06/2024)
A Omitted Proofs
A.1 Proof of Lemma 3.2
Recall Lemma 3.2 states that the following inequality holds with probability 1−δ:
W1(µ,µn
θ)≤1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)ℓθ(xT,xi
0)/bracerightbigg
+1
λ/bracketleftiggn/summationdisplay
i=1KL(q(xT|xi
0)||p(xT)) + log1
δ/bracketrightigg
+λ∆2
8n.
Proof of Lemma 3.2. Using the trivial coupling (product of marginals), the definition of µn
θ(equation 5),
and the definition of the loss function ℓθ, we get
W1(µ,µn
θ) = inf
π∈Γ(µ,µn
θ)/integraldisplay
X×X∥x−y∥dπ(x,y)
≤/integraldisplay
X/integraldisplay
X∥x−y∥dµ(x)dµn
θ(y)
=E
x∼µE
y∼µn
θ∥x−y∥
=E
x∼µ/bracketleftigg
1
nn/summationdisplay
i=1/parenleftbigg
E
q(xT|xi
0)E
pθ(xT−1|xT)... E
pθ(x1|x2)E
pθ(y|x1)∥x−y∥/parenrightbigg/bracketrightigg
=E
x∼µ/bracketleftigg
1
nn/summationdisplay
i=1E
q(xT|xi
0)ℓθ(xT,x)/bracketrightigg
.
Using Mbacke et al. (2023, Lemma B.1), the following inequality holds with probability 1−δ:
W1(µ,µn
θ)≤1
nn/summationdisplay
i=1/braceleftbigg
E
q(xT|xi
0)ℓθ(xT,xi
0)/bracerightbigg
+1
λ/bracketleftiggn/summationdisplay
i=1KL(q(xT|xi
0)||p(xT)) + log1
δ+
nlog E
x0∼µ⊗nE
xT∼p(xT)eλ(Ey0∼µ[ℓθ(xT,y0)]−1
n/summationtextn
i=1ℓθ(xT,x0))/bracketrightbigg
.(12)
Now, it remains to upper-bound the exponential moment of equation 9. If supx,x′∈X∥x−x′∥= ∆<∞,
andλ>0is a real number, then the definition of the loss function ℓθand Hoeffding’s lemma yield
nlogE
x0∼µE
xT∼p(xT)eλ(Ex′∼µ[ℓθ(xT,x′)]−1
n/summationtextn
i=1ℓθ(xT,x0))≤nlogexp/bracketleftbiggλ2∆2
8n2/bracketrightbigg
=λ2∆2
8n.
A.2 Proof of Lemma 3.4
Recall Lemma 3.4 states that the following inequality holds:
E
pθ(xT−1|xT)E
pθ(yT−1|yT)E
pθ(xT−2|xT−1)E
pθ(yT−2|yT−1)... E
pθ(x0|x1)E
pθ(y0|y1)∥x0−y0∥≤
/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
∥xT−yT∥+/parenleftiggT/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥],
Proof of Lemma 3.4. Let’s do a proof by induction on T.
11Published in Transactions on Machine Learning Research (06/2024)
•Base case. T= 1. The inequality becomes
E
pθ(x0|x1)E
pθ(y0|y1)∥x0−y0∥≤K1
θ∥x1−y1∥ (13)
which holds by the first part of Lemma 3.3.
•Induction step. AssumeT >1. The induction hypothesis is
E
pθ(xT−2|xT−1)E
pθ(yT−2|yT−1)... E
pθ(x0|x1)E
pθ(y0|y1)∥x0−y0∥≤
/parenleftiggT−1/productdisplay
t=1Kt
θ/parenrightigg
∥xT−1−yT−1∥+/parenleftiggT−1/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥].(14)
Using the induction hypothesis, the linearity of the expectation, Lemma 3.3 with t:=T,
E
pθ(xT−1|xT)E
pθ(yT−1|yT)... E
pθ(x0|x1)E
pθ(y0|y1)∥x0−y0∥
≤ E
pθ(xT−1|xT)E
pθ(yT−1|yT)/bracketleftigg/parenleftiggT−1/productdisplay
t=1Kt
θ/parenrightigg
∥xT−1−yT−1∥+/parenleftiggT−1/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥]/bracketrightigg
=/parenleftiggT−1/productdisplay
t=1Kt
θ/parenrightigg
E
pθ(xT−1|xT)E
pθ(yT−1|yT)[∥xT−1−yT−1∥] +/parenleftiggT−1/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥]
≤/parenleftiggT−1/productdisplay
t=1Kt
θ/parenrightigg/bracketleftbigg
Kt
θ∥xT−yT∥+σTE
ϵ,ϵ′∥ϵ−ϵ′∥/bracketrightbigg
+/parenleftiggT−1/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥]
=/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
∥xT−yT∥+/bracketleftigg/parenleftiggT−1/productdisplay
t=1Kt
θ/parenrightigg
σT+T−1/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/bracketrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥]
=/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
∥xT−yT∥+/bracketleftigg/parenleftiggT−1/productdisplay
i=1Ki
θ/parenrightigg
σT+T−1/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/bracketrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥]
=/parenleftiggT/productdisplay
t=1Kt
θ/parenrightigg
∥xT−yT∥+/parenleftiggT/summationdisplay
t=2/parenleftiggt−1/productdisplay
i=1Ki
θ/parenrightigg
σt/parenrightigg
E
ϵ,ϵ′[∥ϵ−ϵ′∥].
B Numerical Experiments
The goal of these experiments is to assess the numerical value of the bound of Theorem 3.1 on a synthetic
dataset. The data-generating distribution is chosen to be the uniform distribution on the square of side 2,
centered at the origin. Figure 2 shows samples from this target distribution.
The backward process uses a shared network with fully connected layers, and 128hidden units each. The
model is trained on 50,000samples from the original distribution, and the bound is computed with n= 5,000
independent samples. Samples from the trained model are shown in Figure 3.
We computed the bound for different values of λ. Given that the datapoints are confined to a square of side
2, using the primal form of the Wasserstein distance yields a straightforward upper bound of W1(µ,πθ(·))≤√
8≈2.828. We estimated the Lipschitz norms Kt
θusingK′
tfrom Remark 3.2, and the expected norms in
the last two terms of Theorem 3.1 are estimated using 106independent samples from each distribution.
λn/10n/5n/2nn/0.5n/0.1
Bound value 1.1241.2311.5181 2.0353.05611.061
12Published in Transactions on Machine Learning Research (06/2024)
Figure2: Thepointsrepresent 2000samplesfrom
the target data-generating distribution.
Figure3: Thepointsrepresent 2000samplesfrom
the trained diffusion model.
13