Under review as submission to TMLR
Targeted Advertising on Social Networks Using Online Vari-
ational Tensor Regression (revised ver.2.1)
Anonymous authors
Paper under double-blind review
Abstract
This paper is concerned with online targeted advertising on social networks. The main
technical task we address is to estimate the activation probability for user pairs, which
quantifies the influence one user may have on another towards purchasing decisions. This is
a challenging task because one marketing episode typically involves a multitude of market-
ing campaigns/strategies of different products for highly diverse customers. In this paper,
we propose what we believe is the first tensor-based contextual bandit framework for online
targeted advertising. The proposed framework is designed to accommodate any number of
feature vectors in the form of multi-mode tensor, thereby enabling to capture the hetero-
geneity that may exist over user preferences, products, and campaign strategies in a unified
manner. To handle inter-dependency of tensor modes, we introduce an online variational
algorithm with a mean-field approximation. We empirically confirm that the proposed
TensorUCB algorithm achieves a significant improvement in influence maximization tasks
over the benchmarks, which is attributable to its capability of capturing the user-product
heterogeneity.
1 Introduction
Online targeted advertising is one of the most interesting applications of machine learning in the Internet
age. In a typical scenario, a marketing agency chooses a set of “seed” users from the nodes (i.e. users) of
a social graph, and makes certain offers (e.g. coupons, giveaways, etc.), with the expectation that the seed
users will influence their followers and spread the awareness on the product(s) or service(s) being promoted.
An important question of interest is how to maximize the total purchases accrued over multiple marketing
campaign rounds under a fixed budget (i.e. the number of seed users per round). This task is commonly
referred to as (online) influence maximization (IM) in the machine learning community.
A key quantity of interest here is the activation probability {pi,j}, wherepi,jis the probability of user i
influencing user jinto buying the products being advertised. Since {pi,j}is unknown a priori, we are to
repeatedly update the estimate after each marketing round, starting from a rough initial estimate based, for
example, on demographic information. Many trials and errors are unavoidable especially in the beginning.
After a sufficient number of trials, however, we can expect to have systematically better estimates for
{pi,j}. These characteristics make the contextual bandits (CB) framework (Abe et al., 2003; Li et al., 2010;
Bouneffouf et al., 2020) a relevant and attractive solution approach. Here, the “bandit arms” correspond
to the seed users to be selected. The “context” corresponds to information specific to the products being
advertised and the users being targeted. The “reward” would be the number of purchases attained as a
result of the influence of the selected seed users.
There are two mutually interacting sub-tasks in IM as discussed in the literature: One is how to choose
the seed users when given {pi,j}; The other is how to estimate {pi,j}given a seed selection algorithm,
which is the focus of this paper. The approaches to the latter task can be further categorized into direct
andlatentmodeling approaches. The direct approaches mainly leverage graph connectivity combined with
simple features such as the number of purchases. Since transaction history is typically very sparse, the latent
modeling approach has been attracting increasing attention recently. In this category, two major approaches
1Under review as submission to TMLR
context 
tensor
social graphactiv . 
prob.
seed nodes
UCB
response
update probabilistic tensor 
regression model
Figure 1: Overview of TensorUCB (simplest case). The context tensor encodes the features of user pairs and
product/marketing strategy, and is used to estimate the activation probability matrix PG= [pz
i,j]. Based on
observed user responses, the estimation model is updated.
have been proposed to date. One is regression-based (Vaswani et al., 2017; Wen et al., 2017) and the other
isfactorization-based (Wu et al., 2019; Wang & Chen, 2017; Barbieri et al., 2013). Although encouraging
results have been reported in these works, there is one important limitation that restricts their usefulness
in practice: Absence of capability to incorporate product features. This is critical in practice since marketing
campaigns typically include many different products and strategies applied to a diverse population, and
different types of products are expected to follow different information diffusion dynamics.
To address this issue, we propose TensorUCB , a general tensor-based contextual bandit framework. Unlike
the prior works, we use a tensor to represent the context information ( “context tensor” ), which makes
it possible to handle anynumber of feature vectors in principle. Figure 1 illustrates our problem in the
simplest setting. The user context tensor Xis formed from three feature vectors in this simplest case:
user feature vectors of the i- andj-th users and a product feature vector z. By construction, the model
accommodates edge-level feedback that can depend on the product type. Then, the activation probability
matrix PG≜[pz
i,j]is estimated as a function of X. Here we used pz
i,jinstead ofpi,jto show the dependence
onz. We formalize this task as online probabilistic tensor regression. As shown later, it can effectively
capture the heterogeneity over product types using a low-rank tensor expansion technique. We integrate
probabilistic estimation from tensor regression with the upper confidence bound (UCB) policy in a way
analogous to the LinUCB algorithm (Li et al., 2010).
To the best of our knowledge, this is the first proposal of a contextual bandit framework extended to context
tensors and applied to the task of IM. Our empirical results show that the proposed methods outperform
baseline algorithms in the presence of product heterogeneity.
2 Related work
Prior works relevant to this paper can be categorized into three major areas: CB-based IM approaches,
tensor bandits, and tensor regression.
CB-based IM Following the pioneering works by Valko et al. (2014) and Chen et al. (2016) that framed
IM as an instance of the bandit problem, a few approaches have been proposed to incorporate contextual
information. Vaswani et al. (2017) proposed DILinUCB , a contextual version of IM bandits, which uses user
contextual features to learn pi,jwith linear regression. Wu et al. (2019) proposed IMFB, which exploits
matrix factorization instead of linear regression. Unlike our work, in which a single susceptibility tensor W
is shared by all the nodes, their approaches give latent parameters to each network node, and thus, tends
to require more exploration. Wen et al. (2017) proposed another regression-based approach IMLinUCB using
edge-specific features, which can be difficult to obtain in practice. There also exist prior works that attempt
to capture product features in addition to the user features. Sarıtaç et al. (2016) proposed COIN, which
assumes a predefined partition of product features and does not directly use the users’ context vectors. Our
2Under review as submission to TMLR
framework automatically learns multiple patterns in different products as well as different users through
multi-rank tensor regression. Chen et al. (2015) consider a topic distribution for the seed selection task, not
for learning{pz
i,j}.
Tensor bandits Apart from IM, bandits with structured arms are an emerging topic in the bandit research
community. The majority of the studies consider the bilinear setting, which can be solved through low-rank
matrix estimation or bilinear regression (Kveton et al., 2017; Zoghi et al., 2017; Katariya et al., 2017; Lu
et al., 2018; Hamidi et al., 2019; Jun et al., 2019b; Lu et al., 2021). However, it is not clear how they can be
extended to general settings having more than two contextual vectors which we are interested in. Azar et al.
(2013) is among the earliest works that used higher order tensors in bandit research. However, their task
is transfer learning among a multitude of bandits, which is different from ours. Recently, Hao et al. (2020)
proposed a tensor bandit framework based on the Tucker decomposition (Kolda & Bader, 2009). However,
their setting is not contextual and is not applicable to our task. Specifically, in our notation, their reward
model is defined solely for X=e1
j1◦···◦eD
jD, whereel
jlis thedl-dimensional unit basis vector whose jl-th
element is 1 and otherwise 0. As a result of this binary input, the coefficient tensor Wis directly observable
through the response u. In other words, the task is nota supervised learning problem anymore in contrast
to our setting. To the best of our knowledge, our work is the first proposal of variational tensor bandits in
the contextual setting.
Tensor regression We also believe this is the first work of contextual tensor bandits allowing an arbitrary
number of tensor modes. For generic tensor regression methods, limited work has been done on online
inference of probabilistic tensor regression. Most of the existing probabilistic tensor regression methods
(e.g. (Zhao et al., 2014; Imaizumi & Hayashi, 2016; Guhaniyogi et al., 2017; Ahmed et al., 2020)) require
either Monte Carlo sampling or evaluation of complicated interaction terms, making it difficult to directly
apply them to online marketing scenarios. In particular, they do not provide an analytic form of predictive
distribution, which is desirable to make the UCB framework applicable. We provide a tractable online
updatingequationbasedonavariationalmean-fieldapproximation. Tothebestofourknowledge, TensorUCB
is among the first works that explicitly derived an online version of probabilistic tensor regression.
3 Problem Setting
In the online influence maximization (IM) problem on social networks, there are three major design points,
as illustrated in Fig. 1:
•Estimation model for yz
i,j(userj’s response (purchase etc.) by user i’s influence for a product z).
•Scoring model for pz
i,j(the probability that user iactivates user jfor a product z).
•Seed selection model to choose the Kmost influential users, given {pz
i,j}and a social graph G.
Thispaperdealswiththefirstandthesecondtasksalone,followingtheexistingIMliterature(Wuetal.,2019;
Vaswani et al., 2017; Wen et al., 2017; Sarıtaç et al., 2016). We formalize the first task as online probabilistic
regression that takes a tensor as the contextual input (Sec. 4). The second task is handled by integrating
the derived probabilistic model with the idea of UCB (Sec. 5). The third task is not within the scope of this
paper. It takes care of the combinatorial nature of the problem and is known to be NP-hard (Kempe et al.,
2003). We assume to have a black-box subroutine (denoted by ORACLE) that produces a near-optimal solution
for a given{pi,j},K, and a social graph G. In our experiments we use an η-approximation algorithm (Golovin
& Krause, 2011) proposed by Tang et al. (2014).
Although the use of {pz
i,j}implies the independent cascade (IC) model (Kempe et al., 2003) as the underlying
diffusion process, we do not explicitly model the dynamics of information diffusion. Instead, we learn the
latentquantitypz
i,jas a proxy for diffusion dynamics among the users. This is in contrast to the direct
approaches (Bhagat et al., 2012; Li et al., 2013; Morone & Makse, 2015; Lei et al., 2015; Lu et al., 2015), as
mentioned in Introduction.
3Under review as submission to TMLR
Table 1: Main mathematical symbols.
symbol definition
yz
i,jBinary user response for the event i⇒jfor a product z.
¯uz
i,jExpected score (real-valued) for yz
i,j.
pz
i,jActivation probability of the event i⇒jfor a product z.
XContext tensor X=ϕ1◦ϕ2◦···◦ϕD(Eq. (4)) with ϕlinRdlforl= 1,...,D.
ϕ1Source user’s feature vector, which is xiin the event i⇒j.
ϕ2Target user’s feature vector, which is xjin the event i⇒j.
ϕ3Product feature vector (typically denoted by z).
wl,rCoefficient vector for ϕlof ther-th tensor rank.
¯wl,rPosterior mean of wl,r(Eq. (15)).
Σl,rPosterior covariance matrix of wl,r(Eq. (14)).
3.1 Data model
In addition to a social graph G= (V,E), whereVis the set of user nodes ( |V|is its size) andEis the set of
edges (|E|is its size), we consider two types of observable data. The firstis the contextual feature vectors.
There are two major types of feature vectors. One is the user feature vector {xi|i∈V}while the other is
the product feature vector denoted by z. Let us denote by i⇒jthe event that “user iactivates user j(into
buying a product z).” The contextual information of this event is represented by a tuple (xi,xj,z). There
can be other feature vectors representing campaign strategies, etc. In general, we assume that an activation
event is characterized by Dcontextual feature vectors ϕ1∈Rd1,...,ϕD∈RdD, whered1, etc., denote the
dimensionality. All the feature vectors are assumed to be real-valued column vectors.
In Fig. 1, we illustrated the case where ϕ1=xi,ϕ2=xj, andϕ3=zfor the user pair (i,j)and a product
having the feature vector z. As summarized in Table 1, we will always allocate ϕ1,ϕ2,ϕ3to the source user,
the target user, and the product feature vectors, respectively. Creating the feature vectors is not a trivial
task in general. See Section 7.1 for one reasonable method.
Thesecondobservable is the users’ response, denoted by yz
i,j∈{0,1}for the event i⇒jfor a product z.
yz
i,j= 1ifi⇒jhas occurred, and yz
i,j= 0otherwise. Although activation is not directly measurable in
general, a widely-used heuristic is a time-window-based method (Barbieri et al., 2013). Specifically, we set
yz
i,j= 1if userjbought the product after actively communicating with user iwithin a certain time window.
Active communications include “likes,” retweeting, and commenting, depending on the social networking
platform. The size of the time window is determined by domain experts and is assumed to be given.
3.2 Activation probability estimation problem
We consider the situation where a fixed number (denoted by K) of seed users are chosen in each campaign
round (“budgeted IM”). The seed nodes may have a different number of connected nodes, as illustrated in
Fig. 2. Thus, the dataset from the t-th marketing round takes the following form:
{(ϕt(k),1,...,ϕt(k),D,yt(k))|k= 1,...,nt}, (1)
wherent=/summationtext
i∈Stnout
i,Stis the set of seed users chosen in the t-th marketing round ( |St|=K) andnout
iis
the number of outgoing edges of the i-th node. In this expression, the identity of node pairs and the product
is implicitly encoded by k. In our solution strategy, the estimation model is updated as soon as a new sample
comes in. Hence, it is more useful to “flatten” (t,k)into a single “time” index τwhen considering all the
samples obtained up to the current time τ, denoted as
D1:τ≜{(ϕτ′,1,...,ϕτ′,D,yτ′)|τ′= 1,...,τ}. (2)
As a general rule, we use a subscript ( t(k)orτ) to denote an instance of a random variable.
Our main task is to estimate the activation probability matrix PG≜[pz
i,j]as a function of the contextual
feature vectors ϕ1,...,ϕD, whereϕ1=xi,ϕ2=xjandϕ2=zare assumed and we define pz
i,j= 0for
4Under review as submission to TMLR
Figure 2: Illustration of data structure in one marketing round and the prediction model, corresponding to
Eq. (3) (the noise term is omitted for simplicity). {5,13,i,92}is the set of seed nodes here. K= 4and
D= 3are assumed.
disconnected node pairs. As mentioned at the beginning of this section, the task is divided into two steps.
Thefirststep (estimation model) is to learn a regression function to predict yz
i,jfromϕ1,...,ϕD. We call
the output of the regression function the response score , and denote it by uz
i,j∈Rto distinguish it from the
binary response:
uz
i,j=HW(ϕ1,ϕ2,...,ϕD) + (noise ), (3)
where we have used the convention ϕ1=xi,ϕ2=xj, andϕ3=z, as mentioned before. HWis a parametric
model with Wbeing a random variable called the susceptibility tensor (defined in the next section). To
systematically treat the uncertainty in the user response, we wish to learn a probability distribution ofW
andui,jexplicitly, and eventually derive their updating rule to get a renewed estimate in every marketing
roundt. As described in the next section, the noise term is assumed to be Gaussian.
In thesecondstep (scoring model), once the distribution of uz
i,jis obtained, the activation probability
pz
i,j∈[0,1]is computed not only with the expectation ¯uz
i,jbut also with the variance through an appropriate
mapping function that reflects the UCB policy.
4 Online variational tensor regression
When activation i⇒joccurs, we naturally assume that the activation probability depends on the feature
vectors of the user pair andthe product. One straightforward approach in this situation is to create a con-
catenated vector and apply, e.g., the LinUCB algorithm (Li et al., 2010), in which HWis the linear regression
function. However, it is well-known that such an approach is quite limited in its empirical performance. For
concreteness, consider the D= 3case in Fig. 1 again. The main issue is that it amounts to treating xi,xj,z
separately hence failing to model their interactions :HWin this approach would be w⊤
1xi+w⊤
2xj+w⊤
3z,
and fitting the regression coefficients w1,w2,w3would result in giving the most weight on generally pop-
ular user and product types. This is not useful information in online advertising, as we are interested in
analyzing what kind of affinitythere might be in a specific combination of user pairs and products. The
proposed tensor-based formulation allows dealing with such interactions while keeping the computational
cost reasonable with a low-rank tensor approximation.
4.1 Tensor regression model
We instead assume to have the context tensor in the form
X=ϕ1◦ϕ2◦···◦ϕD, (4)
where◦denotes the direct product. For example, in the case shown in Fig. 1, the ( i1,i2,i3)-th element
ofXis given by the product of three scalars: [xi◦xj◦z]i1,i2,i3=xi,i1xj,i2zi3, where the square bracket
denotes the operator to specify an element of tensors. As mentioned before, ϕ1,ϕ2andϕ3correspond to
5Under review as submission to TMLR
the source user, target user, and product feature vectors, respectively. The other feature vectors ϕ4,...,ϕD
can represent marketing campaign strategies, etc. Note that Eq. (4) includes the regression model in bilinear
bandits (e.g. (Jun et al., 2019a)) and non-contextual tensor bandits (Hao et al., 2020) as special cases.
Specifically, bilinear models can handle only the D= 2case while our model can handle D≥3.Non-
contextual tensor regression cannot accommodate the feature vectors {ϕl∈Rdl|l= 1,...,D}.
For the regression function HWin Eq. (3), we employ a tensor regression model as
HW(ϕ1,...,ϕD) = (W,X), (W,X)≜/summationdisplay
i1,...,iDWi1,...,iDXi1,...,iD, (5)
where (·,·)denotes the tensor inner product, and we call the regression coefficient Wthesusceptibility
tensor. For tractable inference, we employ the CP (canonical polyadic) expansion (Cichocki et al., 2016;
Kolda & Bader, 2009) of order R, which simplifies Eq. (5) significantly:
W=R/summationdisplay
r=1w1,r◦w2,r◦···◦wD,r, ui,j=R/summationdisplay
r=1D/productdisplay
l=1ϕ⊤
lwl,r+ (noise ), (6)
where⊤denotes the transpose. In the second equation above, the r.h.s. now involves only the vector inner
products. There are two important observations to note here. First, this particularly simple form is due to
the specific approach of the CP expansion we employ. Other tensor factorization methods such as Tucker and
tensor-train do not yield simple expressions like Eq. (6), making the UCB analysis intractable. Second, with
R > 1, it has multiple regression coefficients for each tensor mode l. This flexibility provides the potential
to capture the characteristics of multiple product types, unlike vector-based linear regression.
Figure 2 illustrates one marketing round with K= 4andD= 3. Each seed user has a few connected users.
For example, the 5th user is a “friend” of the 65th and 81st users. For a user pair (i,j), the response score
ui,jis computed from ϕ1=xi,ϕ2=xj, andϕ3=zthrough Eq. (6).
4.2 Variational learning of susceptibility tensor
One critical requirement in CB-based IM is the ability to handle stochastic fluctuations of the user response.
Here we provide a fully probabilistic online tensor regression model.
As the first step, let us formalize a batch learning algorithm, assuming that all the samples up to the τ-th
“time” are available at hand under the flattened indexing as in Eq. (2). Define Xτ≜ϕτ,1◦···ϕτ,D. As
mentioned before, ϕτ,1andϕτ,2are used for the node feature vectors, serving as the proxy for the node
indexes. We employ Gaussian observation and prior models, which follow the standard CB approach except
tensor-based parameterization:
p(u|X,W,σ) =N(u|(W,X),σ2), p (W) =D/productdisplay
l=1R/productdisplay
r=1N(wl,r|0,Idl), (7)
wherep(·)symbolically represents a probability distribution and N(·|(W,X),σ2)denotes Gaussian with
mean (W,X)and variance σ2. Also,u∈Ris the user response score (at any time and user pair), and Idis
thed-dimensional identity matrix. Since σ2is assumed to be given and fixed, which is a common assumption
in the bandit literature, Wis the only model parameter to be learned.
DespitetheapparentsimplicityofEq.(6), inter-dependencyamongtheparametervectors {wl,r}makesexact
inference intractable. To address this issue, we introduce variational tensor bandits featuring variational
Bayes (VB) inference (Bishop, 2006). The key assumption of VB is to assume the posterior distribution in
a factorized form. In our case, the posterior of the succeptibility tensor Wis assumed to be:
Q(W) =Q({wl,r}) =D/productdisplay
l=1R/productdisplay
r=1ql,r(wl,r). (8)
6Under review as submission to TMLR
We determine the distribution {ql,r}by minimizing the Kullback-Leibler (KL) divergence:
Q= arg min
QKL[Q∥Q0], KL[Q∥Q0]≜/integraldisplayD/productdisplay
l=1R/productdisplay
r=1dwl,rQ(W) lnQ(W)
Q0(W), (9)
whereQ0(W)is the true posterior. We, of course, do not know the exact form of Q0(W), but we do know
that it is proportional to the product between the observation and prior models by Bayes’ theorem:
Q0(W)∝p(W)/productdisplay
τp(yτ|Xτ,W,σ). (10)
Equation (9) is a functional optimization problem. Fortunately, the Gaussian assumption allows us to find
an analytic form for the posterior. The result is simple: ql,r(wl,r) =N(wl,r|¯wl,r,Σl,r), where
¯wl,r=σ−2Σl,r/summationdisplay
τϕτlβl,r
τyl,r
τ, Σl,r= [σ−2/summationdisplay
τϕτ,lϕ⊤
τ,lγτ,l+Idl]−1. (11)
Here we have defined
βl,r
τ≜/productdisplay
l′̸=lϕ⊤
τ,l′¯wl′,r, yl,r
τ≜yτ−/summationdisplay
r′̸=r(ϕ⊤
τ,l¯wl,r′)βl,r′
τ, γτ,l≜/productdisplay
l′̸=lϕτ,l′⊤⟨wl′,r(wl′,r)⊤⟩\(l,r)ϕτ,l′,(12)
where⟨·⟩\(l,r)is the partial posterior expectation excluding ql,r. Derivation of Eqs. (11)-(12) is straightfor-
ward but needs some work. See Appendix A for the details.
4.3 Mean-field approximation and online updates
Equations (11)-(12) have mutual dependency among the {wl,r}and need to be performed iteratively until
convergence. This is numerically challenging to perform in their original form. In Eq. (11), γτ,l∈Rplays the
role of the sample weight over τ’s. Evaluating this weight is challenging due to the matrix inversion needed
forΣl′,r. For faster and more stable computation suitable for sequential updating scenarios, we propose
a mean-field approximation ⟨wl′,r(wl′,r)⊤⟩\(l,r)≈¯wl′,r(¯wl′,r)⊤, which gives γτ,l= (βl,r
τ)2. Intuitively, the
mean-field approximation amounts to the idea “think of the others as given (as their mean) and focus only
on yourself.” Using this, we have a simple formula for Σl,r:
Σl,r=/bracketleftigg
σ−2/summationdisplay
τ/parenleftbig
βl,r
τϕτ,l/parenrightbig/parenleftbig
βl,r
τϕτ,l/parenrightbig⊤+Idl/bracketrightigg−1
. (13)
Unlikethecrudeapproximationthatsetstheother {wl,r}toagivenconstant, wl,r’sarecomputediteratively
over alll,rin turn, and are expected to converge to a mutually consistent value. The variance is used
for comparing different edges in the UCB framework. The approximation is justifiable since the mutual
consistencymattersmoreinourtaskthanestimatingtheexactvalueofthevariance. InSec.7,wewillconfirm
that the variational tensor bandits significantly outperforms the baseline even under these approximations.
Now let us derive the online updating equations. Fortunately, this can be easily done because ¯wl,rin Eq. (11)
andΣl,rin Eq. (13) depend on the data only through the summation over τ. For any quantity defined as
Aτ+1≜/summationtextτ
s=1as, we straightforwardly have an update equation Aτ+1=Aτ+aτin general. Hence when a
new sample (Xτ,yτ)comes in: First, Σl,rcan be updated as
(Σl,r)−1←(Σl,r)−1+ (βl,r/σ)2ϕτ,lϕ⊤
τ,l, Σl,r←Σl,r−Σl,rϕτ,lϕ⊤
τ,lΣl,r
(σ/βl,r)2+ϕ⊤
τ,lΣl,rϕτ,l,(14)
where the second equation follows from the Woodbury matrix identity (Bishop, 2006). Second, for the
posterior mean ¯wl,r, with the updated Σl,r, we have
bl,r←bl,r+ϕτ,lβl,ryl,r
τ, ¯wl,r=σ−2Σl,rbl,r. (15)
Equations (14)-(15) are computed over all (l,r)until convergence. Note that when R=D= 1, these update
equations essentially derive the ones used in LinUCB(Li et al., 2010) as a special case.
7Under review as submission to TMLR
u
Figure 3: Illustration of Gaussian tail probability and its upper confidence bound.
5 Tensor UCB Algorithm
This section presents TensorUCB , based on the online probabilistic tensor regression framework in Sec. 4.
5.1 Predictive distribution
With the posterior distribution Q(W) =/producttext
l,rql,r, we can obtain the predictive distribution of the user
response score ufor anarbitrary context tensor X=ϕ1◦···◦ϕDas
p(u|X,D1:τ)=/integraldisplay
N(u|(W,X),σ2)Q(W) dW. (16)
DespiteQ(W)being a factorized Gaussian, this integration is intractable. We again use the mean-field
approximation when applying the Gaussian marginalization formula (see, e.g., Sec. 2.3.3 of (Bishop, 2006)).
The resulting predictive distribution is also a Gaussian distribution p(u|X,Dt) =N(y|¯u(X),¯s2(X))with
¯u(X) = ( ¯W,X) =R/summationdisplay
r=1D/productdisplay
l=1(¯wl,r)⊤ϕl,¯s2(X) =σ2+R/summationdisplay
r=1D/summationdisplay
l=1(βl,rϕl)⊤Σl,r(βl,rϕl). (17)
Notice that the predictive mean ¯u(X)is simply the inner product between the posterior mean ¯Wand
the input tensor X, which is a typical consequence of the mean-field approximation. We also see that
the predictive variance ¯s2depends on the context tensor X, and some users/products may have greater
uncertainty in the expected score ¯u. We leave the detail of the derivation of Eq. (17) to Appendix B.
5.2 Upper confidence bound
To transform ¯uinto the activation probability, we adopt the well-known UCB strategy. Thanks to the predic-
tivedistributionbeingGaussian, wecanstraightforwardlyprovidetheupperconfidenceboundcorresponding
to a tail probability.
We start with Markov’s inequality that holds for any non-negative random variable vand anyr>0:
P(v≥r)≤⟨v⟩
r, (18)
where P(·)is the probability that the argument holds true and ⟨·⟩is the expectation. Although the response
scoreucan be negative, we can use Markov’s inequality by setting v= eλu:
P(eλu≥eλh) =P(u≥h)≤⟨eλu⟩
eλh. (19)
Here we note that ⟨eλu⟩is the definition of the moment generating function, and a well-known analytic
expression is available for the Gaussian distribution (17):
⟨eλu⟩= exp/parenleftbigg
λ¯u(X) +1
2λ2¯s2(X)/parenrightbigg
. (20)
8Under review as submission to TMLR
Algorithm 1 TensorUCB for contextual influence maximization ( D= 3case)
Input:K,σ,Randc>0. Subroutine ORACLE.
Initialize{Σl,r,wl,r}and{pi,j}.
fort= 1,2,...,T do
Receive product context ϕ3=zof this round.
St←ORACLE ({pz
i,j},K,G)
Receive response from users connected to ∀i∈St.
fork= 1,...,ntdo
Retrieve user feature vectors ϕ1,ϕ2fromk.
Update{¯wl,r}and{Σl,r}with (Xt(k),yt(k)).
end for
for(i,j)∈Edo
Setϕ1=xi,ϕ2=xj, andX=ϕ1◦ϕ2◦ϕ3
Computepz
i,j= proj (¯u(X) +UCB(X))
end for
end for
The inequality (19) now reads
P(u≥h)≤exp/parenleftbigg
λ(¯u(X)−h) +1
2λ2¯s2(X)/parenrightbigg
. (21)
Here, recall that λisanypositive number. The idea of Chernoff bound is to exploit the arbitrariness of λ
to get the tightest bound. It is an elementary calculus problem to get the minimum of the r.h.s. of Eq. (21).
The minimum is achieved at λ= (h−¯u)/¯s2, yielding the Chernoff bound of Gaussian:
P(u≥h)≤exp/parenleftbigg
−(h−¯u(X))2
2¯s2(X)/parenrightbigg
. (22)
Now let us assume that the tail probability P(u≥h)on the l.h.s. equals to δ, and denote the corresponding
upper bound by hδ+ ¯u, as illustrated in Fig. 3. Solving the equation δ= exp/parenleftbig
−h2
δ/[2¯s2(X)]/parenrightbig
,we have
hδ=√
−2 lnδ×¯s(X). (23)
This is the upper confidence bound we wanted. Since σ2has been assumed to be a constant and
(βl,rϕl)⊤Σl,r(βl,rϕl)≥0in Eq. (17), it suffices to use
pz
i,j= proj (¯u(X) +UCB(X)), UCB(X)≜cR/summationdisplay
r=1D/summationdisplay
l=1/radicalig
(βl,rϕl)⊤Σl,r(βl,rϕl), (24)
where we remind the reader that ϕ1=xi,ϕ2=xj, andϕ3=zinX. Also, proj(·)is a (typically sigmoid)
function that maps a real value onto [0,1], andc >0is a hyperparameter. Again, D=R= 1reproduces
LinUCB (Li et al., 2010).
5.3 Algorithm summary
We summarize the proposed TensorUCB algorithm in Algorithm 1 in the simplest setting with D= 3.ORACLE
has been defined in Sec. 3. Compared with the existing budgeted IM works using linear contextual bandits,
TensorUCB has one extra parameter, R, the rank of CP expansion (6). Rcan be fixed to a sufficiently large
value within the computational resource constraints, typically between 10 and 100. As shown in Fig. 6 later,
the average regret tends to gradually improve as Rincreases up to a certain value. Except for the first
several values as in Fig. 6, changes are typically not drastic.
As for the other three “standard” parameters: The budget Kis determined by business requirements. σ2is
typically fixed to a value of O(1)such as 0.1. Note that σ2is also present in other linear contextual bandit
9Under review as submission to TMLR
frameworks but they often ignore it by assuming unit variance. cneeds to be chosen from multiple candidate
values (see Sec. 7), which is unavoidable in UCB-type algorithms. For initialization of the online algorithm,
Σl,ris typically set to Idlandwl,rcan be the vector of ones. {pz
i,j}can be non-negative random numbers
for connected edges and 0 otherwise.
Since our algorithm does not need explicit matrix inversion, the complexity per update can be evaluated
asO(RDd2), whered≜maxldl. Note that, if we vectorized Wto use standard vector-based inference
algorithms, the complexity would be at least O((/producttext
ldl)2), which can be prohibitive. The vectorized model
also hurts interpretability as it breaks natural groupings of the features.
6 Regret analysis
We leverage the predictive distribution (17) to evaluate the regret bound of TensorUCB . Letf(S,PG)be
the total number of target users activated by a selected seed set Sbased on the activation probabilities PG.
Suppose that ORACLE(see Sec. 3 for the definition) is an η-approximation algorithm (Golovin & Krause,
2011). In the CB-based IM literature, a scaled version of regret is typically used as the starting point (Wen
et al., 2017; Vaswani et al., 2017; Chen et al., 2016):
Rη
T=1
ηT/summationdisplay
t=1E[f(S∗,P∗
G)−f(St,P∗
G)], (25)
where P∗
G= [p∗z
i,j]is the ground truth of the activation probability matrix and S∗=ORACLE (P∗
G,K,G). One
campaign round is assumed to handle only one type of product. The expectation E(·)is taken over the
randomness of zand the other non-user contextual vectors (i.e., ϕlwithl≥3) as well as seed selection by
theORACLE. Let Pt,G= [pz
t,i,j]be the estimated activation probability matrix at the t-th round.
To derive a regret bound in our setting, we need to make a few assumptions. The first assumption (A1) is
called the bounded smoothness condition, which is commonly used in the bandit IM literature. We assume
that there exists a constant Bsuch that
f(St,Pt,G)−f(St,P∗
G)≤B/summationdisplay
i∈St/summationdisplay
j∼i|pz
t,i,j−p∗z
i,j|, (26)
for any P∗
G,St, where the second summation for jruns over the nodes connected to the selected seed node i.
The second assumption (A2) is another commonly used condition called the monotonicity condition. For a
seed user setSand a product z, the monotonicity states that, if pz
i,j≤p′z
i,jfor alli∈Sand their connected
nodesj, we havef(S,PG)≤f(S,P′
G). The third assumption (A3) is ∥βl,r
t(k)ϕt(k),l∥≤1,∀l,r,t,k, which can
be always satisfied by rescaling the feature vectors. Finally, the fourth assumption (A4) is about variability
due to the variational Bayes and mean-field approximations. Unlike vector-based linear regression, no exact
analyticsolutionisknownin probabilistic tensorregression, anditsrigoroustheoreticalanalysisinthecontext
ofcontextual bandits is still an open problem. In what follows, we ignore their variability, assuming that
UCB(X)in Eq. (24) is exact.
Now we state our main result on the regret bound:
Theorem 1. Under the assumptions (A1)-(A4) stated above and a condition
c≥DR/radicaligg
Kdln/parenleftbig
1 +TK
dσ2/parenrightbig
ln/parenleftbig
1 +1
σ2/parenrightbig+ max
l,r||wl,r||2, (27)
the upper regret bound of TensorUCB is given by
Rη
T≤O/parenleftigg
cB
η|V|DR/radicaligg
TKd ln/parenleftbig
1 +TK
dσ2/parenrightbig
ln/parenleftbig
1 +1
σ2/parenrightbig/parenrightigg
(28)
with probability at least 1−δ.
10Under review as submission to TMLR
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000017
/uni00000052/uni00000058/uni00000057/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000014/uni00000013/uni00000018/uni00000046/uni00000052/uni00000058/uni00000051/uni00000057
/uni00000027/uni0000004c/uni0000004a/uni0000004a
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000017
/uni00000052/uni00000058/uni00000057/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000046/uni00000052/uni00000058/uni00000051/uni00000057
/uni00000029/uni0000004f/uni0000004c/uni0000005b/uni00000056/uni00000057/uni00000048/uni00000055
Figure 4: Distribution of out-degrees.Table 2: Network statistics,
wherenout(m)denotes the m-th
percentile of the out-degree.
DiggFlixster
|V| 2 483 29 384
|E| 75 895 371 722
nout(50) 1 9
nout(90) 8 154
nout(99) 101 2064
The proof is given in Appendix C. Although the complexity of this bound can depend on the assumed
scenario, it is at least comparable to the ones reported in the literature: IMFB(Wu et al., 2019),
DILinUCB (Vaswanietal.,2017), and IMLinUCB (Wenetal.,2017)reportedtheregretboundsof O(d|V|5
2√
T),
O(d|V|2√
T), andO(d|V|3√
T), respectively, which hold under a similar condition to Eq. (27).
7 Experiments
This section reports on empirical evaluation of TensorUCB . Our goal is to illustrate how it captures users’
heterogeneity as the tensor rank Rincreases (Fig. 6), to show its advantage in the presence of product/user
heterogeneity (Fig. 5), and to examine its computational overhead (Fig. 7).
7.1 Datasets
We used two publicly available datasets with significantly different levels of heterogeneity in products and
users:Digg(Hogg & Lerman, 2012a;b) records users’ voting history to posted news stories and has |V|=
2 843,|E|= 75 895 , and 1 000stories.Flixster (Zafarani & Liu, 2009) records users’ movie rating history
and has|V|= 29 384 ,|E|= 371 722 , and 100 movies. Notice that the number of products is as large as
1 000. These are real-world datasets where naive product-wise modeling is unrealistic. Figure 4 compares
their distributions of out-degrees (i.e., the number of outgoing edges). In both datasets, the distribution is
highly skewed, as also seen in the out-degree percentiles summarized in Table 2. The out-degree distribution
of Digg is more power-law-like, where only a handful of users have a dominant number of followers.
In both, we removed isolated nodes and those with less than 50interactions in the log. Activation is defined
as voting for the same article (Digg), or as watching or liking the same movie within 7 days (Flixster). These
activation histories allow for learning of the activation probability without extra assumptions on the diffusion
process, as opposed to the setting of some of the prior works, e.g., (Vaswani et al., 2017; Wu et al., 2019),
where activations are synthetically simulated using a uniform distribution.
Constructing contextual feature vectors is not a trivial task. Our preliminary study showed that categorical
features such as ZIP code and product categories lead to too much variance that washes away the similarity
between users and between products. To avoid pathological issues due to such non-smoothness, we created
user and product features using linear embeddings, as proposed in (Vaswani et al., 2017).
Specifically, for generating user features {xi|i∈V}, we employed the Laplacian Eigenmap (Belkin &
Niyogi, 2002) computed from the social graph G, which is included in both Digg and Flixster datasets.
Following (Vaswani et al., 2017), we used the eigenspectrum to decide on the dimensionality, which gave d1=
d2= 10. For product (story or movie) features, we employed a probabilistic topic model (see, e.g. (Steyvers
& Griffiths, 2007)) with the number of topics being 10. In this model, an product is viewed as a document in
the “bag-of-votes” representation: Its i-th dimension represents whether the i-th user voted for the product.
The intuition is that two articles should be similar if they are liked by a similar group of people. As a result,
each product is represented by a 10-dimensional real-valued vector, which corresponds to a distribution over
11Under review as submission to TMLR
Table 3: Methods compared. IMFBlearns two node-wise coefficient vectors {(βi,θi)}.IMLinUCB learns only
one coefficient vector θ.DILinUCB learns one node-wise coefficient vector {βi}. ‘NA’ denotes ‘not available.’
score model ¯uz
i,jprobability model pz
i,jsource user target user product feature
COIN nq
i,j nq
i,j/Nq
i,j NA NAq= category(z)
IMFB β⊤
iθj proj(¯uz
i,j+ UCB) NA NA NA
IMLinUCB x⊤
i,jθ proj(¯uz
i,j+ UCB) xi,j=xi⊙xj NA
DILinUCB x⊤
jθi proj(¯uz
i,j+ UCB) NA xj NA
TensorUCB/summationtextR
r=1/producttextD
l=1ϕ⊤
lwl,rproj(¯uz
i,j+ UCB) ϕ1=xiϕ2=xjϕ3=z
the 10 latent topics identified. Once the product feature vectors are computed on a held-out dataset, they
are treated as a constant feature vector for each product.
7.2 Baselines
Baseline methods were carefully chosen to comprehensively cover the major existing latent-modeling IM
frameworks (see Introduction), as summarized in Table 3.
•COIN(Sarıtaç et al., 2016) learns the activation probability independently for each of the product
categories{q}. The probability is computed based on the number of successful activations (nq
i,j)and
the total number of seed-selections (Nq
i,j). A control function is used for exploration-exploitation
trade-off. To decide on q, we picked the most dominant topic (see Sec. 7.1), rather than the raw
product labels for a fair comparison.
•IMFB(Wu et al., 2019) is a factorization-based method, where two node-wise coefficient vectors are
learned from user response data without using user and product feature vectors.
•IMLinUCB (Wen et al., 2017) is an extension of the classical LinUCB to the edges as arms. This
method requires an edge-level feature vector. The authors used the element-wise product of the user
feature vectors as xi,j=xi⊙xjin their experiments, which we used, too.
•DILinUCB (Vaswani et al., 2017) is another LinUCB-like algorithm that learns the coefficient vector
in a node-wise fashion. The feature vector xjin the table is the same as that of TensorUCB .
In addition, we implemented Random, which selects the seeds for a given round randomly.
Table 3 summarizes the high-level characteristics of the baseline methods. Since the number of seed nodes is
almostalwaysmuchsmallerthan |V|,independentlylearningedge-wiseornode-wiseparametersischallenging
in general. Such an approach requires so many explorations to get a reasonable estimate of the activation
probability. COIN,IMFB, and DILinUCB are in this “overparameterized” category. On the other hand,
IMLinUCB imposes a single regression coefficient vector θon all the edges. While this “underparameterized”
strategy can be advantageous in capturing common characteristics between the edges, it cannot handle
the heterogeneity over different products and over different types of user-user interactions. TesnsorUCB is
designed to balance these over- and under-parameterized extremes: All the edges share the same coefficients
{wl,r}, but they still have the flexibility to have Rdifferent patterns. Importantly, TesnsorUCB is a nonlinear
model that can capture user-user and user-product interactions through the product over l. See the first
paragraph of Sec. 4 for a related discussion.
All the experiments used K= 10.ORACLEwas implemented based on (Tang et al., 2014) with η= 1−1
e−0.1.
ForTensorUCB , wefixedσ2= 0.1andthehyper-parameters Randcwereoptimizedusinganinitialvalidation
set of 50rounds. As the performance metric, we reported the average cumulative expected regret1
tRη
t
computed at each round. We reported on regret values averaged over five runs. For fair comparison, P∗
Gis
computed by fitting a topic-aware IC model using maximum likelihood on the interaction logs as proposed
in (Barbieri et al., 2013), whose parameterization is independent of any of the methods compared.
12Under review as submission to TMLR
Figure 5: Average cumulative regret for Digg (left) and Flixster (right). Best viewed in colors.
7.3 Comparison of cumulative regret
As mentioned in Sec. 1, our original motivation for the tensor-based formulation is to capture the hetero-
geneity over different products. In our experiment, a new product (story or movie) is randomly picked (from
1 000stories or 100movies)at each campaign round t. InTensorUCB ,cwas chosen from{10−3,10−2,10−1,1},
whileRwas chosen in the range of 1≤R≤50. We used the sigmoid function for proj(·)in Eq. (24).
Figure 5 compares TensorUCB against the baselines on Digg and Flixster. As is clearly shown, TensorUCB
significantly outperforms the baselines. The difference is striking in Flixster, which has only 100products
(movies). Interestingly, TensorUCB captures the majority of the underlying preference patterns with only
about 10rounds of explorations. This is in sharp contrast to the “slow starter” behavior of COINandIMFB,
which can be explained by their “overparameterized” nature, as pointed out in Sec. 7.2. On the other hand,
the Digg dataset includes as many as 1 000products, and the interaction patterns in it may not have been
fully explored in the relatively small number of rounds. This is consistent with the relatively small margin
in the left figure. In addition, Digg’s friendship network has a stronger power-law nature (Fig. 4). In such a
network, seed users tend to be chosen from a handful of hub nodes, making room for optimization relatively
smaller. Even in that case, however, TensorUCB captures underlying user preferences much more quickly
than any other method.
In Flixster, COINexhibits relatively similar behavior to TensorUCB .COINpartitions the feature space at the
beginning, and hence, its performance depends on the quality of the partition. In our case, partition was
done based on the topic model rather than the raw product label, which is a preferable choice for COIN.
In Flixster, the number of products is comparable to the number of training rounds, which was 50 in our
case. Thus the initial partitioning is likely to have captured a majority of patterns, while it is not the case
in Digg. When the number of product types is as many as that of Digg, product-wise modeling can be
unrealistic. In fact, many products (stories) ended up being in the same product category qin Digg. Unlike
the baseline methods, TensorUCB has a built-in mechanism to capture and generalize product heterogeneity
directly through product context vector.
In the figure, it is interesting to see that the behavior of IMFBis quite different between the two datasets:
It even underperforms Randomin Digg while it eventually captures some of the underlying user preference
patterns in Flixster. As shown in Table 3, IMFBneeds to learn two unknown coefficient vectors at each
node, starting from random initialization. Since it does not use contextual features, parameter estimation
can be challenging when significant heterogeneity exists over users and products, as in Digg. Note that
the previously reported empirical evaluation of IMFB(Wu et al., 2019) is based on simulated activation
probabilities generated from its own score model and does not apply to our setting. It is encouraging that
TensorUCB stably achieves better performance even under rather challenging set-ups as in Digg, indicating
its usefulness in practice.
13Under review as submission to TMLR
Figure 6: Average cumulative regret of
TensorUCB computed on Digg with differ-
entRs. Best viewed in colors.
/uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003||
/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000046/uni00000052/uni00000050/uni00000053/uni00000011/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000003/uni0000003e/uni00000056/uni00000040
/uni00000037/uni00000048/uni00000051/uni00000056/uni00000052/uni00000055/uni00000038/uni00000026/uni00000025
/uni00000026/uni00000032/uni0000002c/uni00000031
/uni0000002c/uni00000030/uni00000029/uni00000025
/uni0000002c/uni00000030/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025
/uni00000027/uni0000002c/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025Figure 7: Computational time per round aver-
aged over 200 rounds on Flixster. Best viewed in
colors.
7.4 Dependency on tensor rank
TensorUCB uses the specific parameterization of CP expansion (6), whose complexity is controlled by the
tensor rank R. Since, as discussed in Sec. 7.2, having R> 1potentially plays an important role in handling
heterogeneity in products and users, it is interesting to see how the result depends on R. Figure 6 shows how
the average regret behaves over the first several values of Ron Digg. To facilitate the analysis, we randomly
picked a single product (story) at the beginning and kept targeting the item throughout the rounds. In the
figure, we specifically showed the first five R’s, where the change in the average regret was most conspicuous.
In this regime, the average regret tends to improve as Rincreases. Depending on the level of heterogeneity of
the data, it eventually converges at a certain Rup to fluctuations due to randomness. The intuition behind
Eq. (6) was that R= 1amounts to assuming a single common pattern in the user preference while R > 1
captures multiple such patterns. This result empirically validates our modeling strategy.
7.5 Comparison of computational cost
Finally, Fig. 7 compares the computation time per round on Flixter, measured on a laptop PC (Intel i7 CPU
with 32 GB memory). Error bars are negligibly small and omitted for clearer plots. To see the dependency
on the graph size, we randomly sampled the nodes to create smaller graphs of |V|≈ 25 000,20 000,15 000.
As shown in the figure, the computation time scales roughly linearly, which is understandable because the
graph is quite sparse and the most nodes have a node degree that is much smaller than |V|.IMLinUCB is
fastest but significantly under-performed in terms of regrets in Fig. 5. TensorUCB is comparable to COINand
much faster than IMFBandDILinUCB . These results validate that TensorUCB significantly outperforms the
baselines without introducing much computational overhead.
8 Concluding remarks
We have proposed TensorUCB , a tensor-based contextual bandit framework, which can be viewed as a new
and natural extension of the classical UCB algorithm. The key feature is the capability of handling any
number of contextual feature vectors. This is a major step forward in the problem of influence maximization
for online advertising since it provides a practical way of simultaneously capturing the heterogeneity in users
and products. With TensorUCB , marketing agencies can efficiently acquire common underlying knowledge
from previous marketing campaigns that include different advertising strategies across different products.
We empirically confirmed a significant improvement in influence maximization tasks, attributable to its
capability of capturing and leveraging the user-product heterogeneity.
For future work, a more sophisticated analysis of the regret bound is desired. In particular, how to evaluate
the variability due to the variational approximations of probabilistic tensor regression is still an open prob-
14Under review as submission to TMLR
lem. From a practical perspective, how to define user and product feature vectors is a nontrivial problem.
Exploring the relationship with modern embedding techniques would be an interesting future direction.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.
InAdvances in Neural Information Processing Systems , pp. 2312–2320, 2011.
Naoki Abe, Alan W Biermann, and Philip M Long. Reinforcement learning with immediate rewards and
linear hypotheses. Algorithmica , 37(4):263–293, 2003.
Talal Ahmed, Haroon Raja, and Waheed U Bajwa. Tensor regression using low-rank and sparse tucker
decompositions. SIAM Journal on Mathematics of Data Science , 2(4):944–966, 2020.
Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Sequential transfer in multi-armed
banditwithfinitesetofmodels. In Proceedings of the 26th International Conference on Neural Information
Processing Systems , pp. 2220–2228, 2013.
Nicola Barbieri, Francesco Bonchi, and Giuseppe Manco. Topic-aware social influence propagation models.
Knowledge and Information Systems , 37(3):555–584, 2013.
M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In
Advances in Neural Information Processing Systems 14 , pp. 585–591, 2002.
Smriti Bhagat, Amit Goyal, and Laks VS Lakshmanan. Maximizing product adoption in social networks. In
Proceedings of the fifth ACM international conference on Web search and data mining , pp. 603–612, 2012.
Christopher M. Bishop. Pattern Recognition and Machine Learning . Springer-Verlag, 2006.
Djallel Bouneffouf, Irina Rish, and Charu Aggarwal. Survey on applications of multi-armed and contextual
bandits. In 2020 IEEE Congress on Evolutionary Computation (CEC) , pp. 1–8. IEEE, 2020.
Shuo Chen, Ju Fan, Guoliang Li, Jianhua Feng, Kian-lee Tan, and Jinhui Tang. Online topic-aware influence
maximization. Proceedings of the VLDB Endowment , 8(6):666–677, 2015.
Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial multi-armed bandit and its extension
to probabilistically triggered arms. Journal of Machine Learning Research , 17(1):1746–1778, 2016.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions.
InProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics , pp.
208–214, 2011.
Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan, Qibin Zhao, and Danilo P Mandic. Tensor
networksfordimensionalityreductionandlarge-scaleoptimization: Part1low-ranktensordecompositions.
Foundations and Trends in Machine Learning , 9(4-5):249–429, 2016.
Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit feedback.
In Rocco A. Servedio and Tong Zhang (eds.), 21st Annual Conference on Learning Theory - COLT 2008,
Helsinki, Finland, July 9-12, 2008 , pp. 355–366, 2008.
Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active learning
and stochastic optimization. Journal of Artificial Intelligence Research , 42:427–486, 2011.
R. Guhaniyogi, S. Qamar, and D. B. Dunson. Bayesian tensor regression. Journal of Machine Learning
Research , 18(79):1–31, 2017.
Nima Hamidi, Mohsen Bayati, and Kapil Gupta. Personalizing many decisions with high-dimensional co-
variates. Advances in Neural Information Processing Systems , 32:11473–11484, 2019.
Botao Hao, Jie Zhou, Zheng Wen, and Will Wei Sun. Low-rank tensor bandits. arXiv preprint
arXiv:2007.15788 , 2020.
15Under review as submission to TMLR
Tad Hogg and Kristina Lerman. Digg 2009 data set, 2012a. https://www.isi.edu/~lerman/downloads/
digg2009.html .
Tad Hogg and Kristina Lerman. Social dynamics of Digg. EPJ Data Science , 1(1):1–26, 2012b.
Masaaki Imaizumi and Kohei Hayashi. Doubly decomposing nonparametric tensor regression. In Proceedings
of the 33rd International Conference on Machine Learning , pp. 727–736, 2016.
Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and Robert Nowak. Bilinear bandits with low-rank
structure. In Proceedings of the 36th International Conference on Machine Learning , volume 97 of Pro-
ceedings of Machine Learning Research , pp. 3163–3172. PMLR, 2019a.
Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and Robert Nowak. Bilinear bandits with low-rank
structure. In Proceedings of the 36th International Conference on Machine Learning , pp. 3163–3172,
2019b.
Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire Vernade, and Zheng Wen. Stochastic rank-1
bandits. In Artificial Intelligence and Statistics , pp. 392–401. PMLR, 2017.
David Kempe, Jon Kleinberg, and Éva Tardos. Maximizing the spread of influence through a social network.
InProceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 137–146, 2003.
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM review , 51(3):
455–500, 2009.
Branislav Kveton, Csaba Szepesvári, Anup Rao, Zheng Wen, Yasin Abbasi-Yadkori, and S Muthukrishnan.
Stochastic low-rank bandits, 2017.
Siyu Lei, Silviu Maniu, Luyi Mo, Reynold Cheng, and Pierre Senellart. Online influence maximization.
InProceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 645–654, 2015.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th International Conference on World Wide Web ,
pp. 661–670, 2010.
Yanhua Li, Wei Chen, Yajun Wang, and Zhi-Li Zhang. Influence diffusion dynamics and influence maximiza-
tion in social networks with friend and foe relationships. In Proceedings of the sixth ACM international
conference on Web search and data mining , pp. 657–666. ACM, 2013.
WeiLu, WeiChen, andLaksVSLakshmanan. Fromcompetitiontocomplementarity: Comparativeinfluence
diffusion and maximization. Proceedings of the VLDB Endowment , 9(2):60–71, 2015.
Xiuyuan Lu, Zheng Wen, and Branislav Kveton. Efficient online recommendation via low-rank ensemble
sampling. In Proceedings of the 12th ACM Conference on Recommender Systems , pp. 460–464, 2018.
Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Low-rank generalized linear bandit problems. In
International Conference on Artificial Intelligence and Statistics , pp. 460–468. PMLR, 2021.
Flaviano Morone and Hernán A Makse. Influence maximization in complex networks through optimal
percolation. Nature, 524(7563):65–68, 2015.
Ömer Sarıtaç, Altuğ Karakurt, and Cem Tekin. Online contextual influence maximization in social networks.
In2016 54th Annual Allerton Conference on Communication, Control, and Computing) , pp. 1204–1211,
2016.
M. Steyvers and T. Griffiths. Probabilistic topic models. Handbook of latent semantic analysis , 427(7):
424–440, 2007.
16Under review as submission to TMLR
Y. Tang, X. Xiao, and Y. Shi. Influence maximization: Near-optimal time complexity meets practical
efficiency. In Proc. ACM SIGMOD Intl. Conf. Management of Data , pp. 75–86, 2014.
Michal Valko, Rémi Munos, Branislav Kveton, and Tomáš Kocák. Spectral bandits for smooth graph func-
tions. In International Conference on Machine Learning , pp. 46–54, 2014.
Sharan Vaswani, Branislav Kveton, Zheng Wen, Mohammad Ghavamzadeh, Laks VS Lakshmanan, and
Mark Schmidt. Model-independent online learning for influence maximization. In Proceedings of the 34th
International Conference on Machine Learning , pp. 3530–3539, 2017.
Qinshi Wang and Wei Chen. Improving regret bounds for combinatorial semi-bandits with probabilistically
triggeredarmsanditsapplications. In Advances in Neural Information Processing Systems , pp.1161–1171,
2017.
Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. Online influence maximization under
independent cascade model with semi-bandit feedback. In Advances in Neural Information Processing
Systems, pp. 3022–3032, 2017.
Qingyun Wu, Zhige Li, Huazheng Wang, Wei Chen, and Hongning Wang. Factorization bandits for online
influence maximization. In Proceedings of the 25th SIGKDD Conference on Knowledge Discovery and
Data Mining , pp. 636–646, 2019.
R. Zafarani and H. Liu. Social computing data repository at ASU, 2009. URL http://socialcomputing.
asu.edu.
Qibin Zhao, Guoxu Zhou, Liqing Zhang, and Andrzej Cichocki. Tensor-variate Gaussian processes regression
and its application to video surveillance. In Proc. IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pp. 1265–1269, 2014.
Masrour Zoghi, Tomas Tunys, Mohammad Ghavamzadeh, Branislav Kveton, Csaba Szepesvari, and Zheng
Wen. Online learning to rank in stochastic click models. In International Conference on Machine Learning ,
pp. 4199–4208. PMLR, 2017.
Appendix
A Derivation of posterior distribution
This section explains how to solve Eq. (9). The the key idea of VB is to look at individual components
{ql,r}ofQone by one, keeping all the others fixed. For instance, for a particular pair of (l,r) = (3,2), the
minimization problem of Eq. (8) reads
min
q3,2/braceleftigg/integraldisplay
dw3,2q3,2lnq3,2−/integraldisplayD/productdisplay
l=1R/productdisplay
r=1dwl,rql,rlnQ0(W)/bracerightigg
,
whereQ0has been defined in Eq. (10). The main mathematical tool to solve this functional optimization
problem is calculus of variations. A readable summary can be found in the appendix of Bishop (Bishop,
2006). Apart from deep mathematical details, its operational recipe is analogous to standard calculus. What
we do is analogous to differentiating xlnx−axto get lnx+ 1−a, and equating it to zero. For a general
(l,r), the solution is given by:
lnql,r= const.+⟨lnQ0(W)⟩\(l,r), (A.1)
⟨lnQ0(W)⟩\(l,r)≜/integraldisplay/productdisplay
l′̸=l/productdisplay
r′̸=rdwl′,r′ql′,r′lnQ0(W), (A.2)
where const.is a constant and ⟨·⟩\(l,r)denotes the expectation by Q({wl,r})over all the variables except for
the(l,r).
17Under review as submission to TMLR
A.1 Solution under the Gaussian model
Now let us derive an explicit form of the posterior. Using Eq. (10) for Q0together with Eq. (7) in Eq. (A.1),
we have
lnql,r= const.−1
2(wl,r)⊤wl,r−1
2σ2/summationdisplay
τ⟨{yτ−(W,Xτ)}2⟩\(l,r). (A.3)
The expression (6) allows writing the last term in terms of {wl,r}:
⟨{yτ−(W,Xτ)}2⟩\(l,r)= const.−2wl,r⊤ϕτ,lβl,r
τylr
τ
+wl,r⊤ϕτ,lϕ⊤
τ,lwl,r/productdisplay
l′̸=lϕ⊤
τl′⟨wl′rwl′r⊤⟩\(l,r)ϕτl′, (A.4)
whereβl,r
τandyl,r
τhave been defined in Eq. (12).
Equations (A.3) and (A.4) imply that lnql,ris quadratic in wl,rand thusql,ris Gaussian. For instance, for
(l,r) = (3,2), collecting all the terms that depend on w3,2, we have
lnq3,2= const.+ (w3,2)⊤1
σ2/summationdisplay
τ(β3,2
τϕτ,3)y3,r
τ−1
2(w3,2)⊤/braceleftigg
Id+1
σ2/summationdisplay
τ(β3,2
τϕτ,3)(β3,2
τϕτ,3)⊤/bracerightigg
w3,2.
By completing the square, we get the posterior covariance matrix Σ3,2and the posterior mean ¯w3,2as
Σ3,2=/braceleftigg
Id+1
σ2/summationdisplay
τ(β3,2
τϕτ,3)(β3,2
τϕτ,3)⊤/bracerightigg−1
, ¯w3,2=1
σ2Σ3,2/summationdisplay
τ(β3,2
τϕτ,3)y3,r
τ,(A.5)
which are the (batch-version of) solution given in the main text.
B Derivation of the predictive distribution
This section explains how to perform the integral of Eq. (16) under
Q(W) =D/productdisplay
l=1R/productdisplay
r=1N(wl,r|¯wl,r,Σl,r). (B.6)
We first integrate w.r.t. w1,r. By factoring out w1,rfrom the tensor inner product as (W,X) =/summationtext
r(ϕ1b1,r)⊤w1,r, we have
I1≜/integraldisplayR/productdisplay
r=1dw1,rN(w1,r|¯w1,r,Σ1,r)N(u|(W,X),σ2) =N(u|u1,σ2
1),
whereb1,r≜(ϕ⊤
2w2,r)···(ϕ⊤
DwD,r)and
u1=R/summationdisplay
r=1(ϕ⊤
1¯w1,r)b1,r, σ2
1=σ2+R/summationdisplay
r=1(b1,rϕ1)⊤Σ1,r(b1,rϕ1).
To perform the integral we used the well-known Gaussian marginalization formula. See, e.g., Eqs. (2.113)-
(2.115) in Sec. 2.3.3 of Bishop (2006).
Next, we move on to the l= 2terms, given I1. Unfortunately, due to the nonlinear dependency on w2,rin
σ2
1, the integration cannot be done analytically. To handle this, we introduce a mean-field approximation in
the same spirit of that of the main text:
σ2
1≈σ2+R/summationdisplay
r=1(β1,rϕ1)⊤Σ1,r(β1,rϕ1), (B.7)
18Under review as submission to TMLR
wherew2,r,...,wD,rhave been replaced with their posterior means ¯w2,r,..., ¯wD,r. The definition of βl,r
τ
is given by Eq. (12). We do this approximation for all σ2
1,...,σ2
Dwhile keeping u1,...,uDexact.
As a result, after performing the integration up to l=k, we have a Gaussian N(u|uk,σ2
k), where
uk=R/summationdisplay
r=1k/productdisplay
l=1(ϕ⊤
l¯wl,r)D/productdisplay
l′=k+1(ϕ⊤
l′wl′,r), σ2
k=σ2+k/summationdisplay
l=1R/summationdisplay
r=1(βl,rϕl)⊤Σl,r(βl,rϕl).(B.8)
By continuing this procedure, we obtain the predictive mean and the variance in Eq. (17).
C Derivation of the regret bound
This section derives the upper bound given in Theorem 1 on the scaled regret defined in Eq. (25).
C.1 Relationship with confidence bound
In Sec. 6, we have introduced the bounded smoothness condition (A1) and the monotonicity condition (A2).
LetPt,G= [pz
t,i,j]be an estimate of the activation probabilities at the t-th round. Based on the moniton-
isity condition and the property of the ORACLE’s seed selection strategy, Wu et al. (2019) argued that the
instantaneous regret (25) is upper-bounded as
1
ηE/bracketleftbig
f(S∗,P∗
G)−f(St,P∗
G)/bracketrightbig
≤1
ηE/bracketleftbig
f(St,Pt,G)−f(St,P∗
G)/bracketrightbig
(C.9)
with probability 1−δ, whereδis the tail probability chosen in the UCB approach. Since K≪|V|in general,
it is possible for the event that may occur with possibility δto play a significant role in the cumulative regret.
Fortunately, Lemma 2 of (Wen et al., 2017),which lower-bounds the UCB constant c, guarantees that its
contribution can be bounded by O(1), as argued by Wu et al. (2019). We will come back this point later to
provide a condition on cexplicitly.
Combining with the smoothness condition (26) and the expression of UCB (24), we have
Rη
T≤cB|V|
ηT/summationdisplay
t=1K/summationdisplay
k=1D/summationdisplay
l=1R/summationdisplay
r=1κt(k),l,r+O(1), (C.10)
κt(k),l,r≜/radicalig
(βl,r
t(k)ϕt(k),l)⊤Σl,r
t(k)(βl,r
t(k)ϕt(k),l), (C.11)
where we have used the “ t(k)” notation introduced in Sec. 3.2 in the main text. Here, Σl,r
t(k)is the covariance
matrix computed using the data up to the k-th seed node in the t-th round. βl,r
t(k)andϕt(k),lare defined
similarly. The summation over jin Eq. (26) produces a constant of the order of node degree, which is
bounded by|V|.
Now our goal is to find a reasonable upper bound of κt(k),l,r. This term has appeared in the definition of the
confidence bound UCB(X)in the main text. This is reminiscent of the regret analysis of vector contextual
bandits (Dani et al., 2008; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the analysis is reduced to
bounding the vector counterpart of κt(k),l,r.
C.2 Bounding confidence bound
Now that the cumulative scaled regret is associated with κt(k),l,r, let us prove the following Lemma, which
supports the regret bound reported in the main text:
19Under review as submission to TMLR
Lemma 1. Under the assumption ∥βl,r
t(k)ϕt(k),l∥≤1,∀l,r,t,k,
/summationdisplay
t,k,l,rκt(k),l,r≤R/radicaltp/radicalvertex/radicalvertex/radicalbtTKD/summationtext
ldlln/parenleftig
1 +TK
dlσ2/parenrightig
ln/parenleftbig
1 +1
σ2/parenrightbig, (C.12)
≤DR/radicaligg
TKd ln/parenleftbig
1 +TK
dσ2/parenrightbig
ln/parenleftbig
1 +1
σ2/parenrightbig, (C.13)
whered≜maxldl.
(Proof)Under thet(k)-notation, the updating equation for the covariance matrix looks like
(Σl,r
t(k+1))−1= (Σl,r
t(k))−1+/parenleftigg
βl,r
t(k)
σ/parenrightigg2
ϕt(k),lϕ⊤
t(k),l, (C.14)
which leads to an interesting expression of the determinant:
det|(Σl,r
T)−1|=T−1/productdisplay
t=1K/productdisplay
k=1/parenleftigg
1 +κ2
t(k),l,r
σ2/parenrightigg
. (C.15)
Thisfollowsfromrepeatedapplicationsofthematrixdeterminantlemma det|A+ab⊤|= det|A|(1+b⊤A−1a)
that holds for any vectors a,band invertible matrix Aas long as the products are well-defined. Equa-
tion (C.15) implies det|Σl,r
t(k)|≤1. By the assumption ∥βl,rϕt(k),l∥≤1, we haveκt(k),l,r≤1.
Interestingly, the cumulative regret (C.10) can be represented in terms of the determinant. Using an in-
equality
b2≤1
ln(1 +σ−2)ln(1 +b2
σ2), (C.16)
that holds for any b2≤1, we have
T/summationdisplay
t=1K/summationdisplay
k=1D/summationdisplay
l=1R/summationdisplay
r=1κt(k),l,r≤
TKDR/summationdisplay
t,k,r,lκ2
t(k),l,r
1
2
,
≤
TKDR/summationdisplay
t,k,r,lln(1 +κ2
t(k),l,r
σ2)
ln(1 +σ−2)
1
2
, (C.17)
=
TKDR/summationdisplay
r,lln det|(Σl,r
T)−1|
ln(1 +σ−2)
1
2
, (C.18)
where the last equality follows from Eq. (C.15).
The determinant is represented as the product of engenvalues. Since the geometrical mean is bounded by
the arithmetic mean, we have
det|(Σl,r
T)−1|1
dl≤1
dlTr[(Σl,r
T)−1], (C.19)
= 1 +1
dlσ2T−1/summationdisplay
t=1K/summationdisplay
k=1∥βl,r
t(k)ϕt(k),l∥2, (C.20)
≤1 +(T−1)K
dlσ2, (C.21)
20Under review as submission to TMLR
where the second equality is by Eq. (C.14) and the last inequality is by the assumption ∥βl,r
t(k)ϕt(k),l∥≤1.
Finally, we have
/summationdisplay
t,k,r,lκt(k),l,r≤/bracketleftigg
TKDR2
ln(1 +σ−2)D/summationdisplay
l=1dlln/parenleftbigg
1 +(T−1)K
dlσ2/parenrightbigg/bracketrightigg1
2
,
≤/bracketleftigg
TKDR2
ln(1 +σ−2)D/summationdisplay
l=1dlln/parenleftbigg
1 +TK
dlσ2/parenrightbigg/bracketrightigg1
2
. (C.22)
The final step for Eq. (C.13) is obvious.
As discussed above, to control the occurrence of tail events, the UCB constant c(introduced in Eq. (24)) has
to be lower-bounded. Using the expression (C.13) and following the same steps as in Lemma 3 in (Vaswani
et al., 2017), we can show that, if
c≥DR/radicaligg
Kdln/parenleftbig
1 +TK
dσ2/parenrightbig
ln/parenleftbig
1 +1
σ2/parenrightbig+ max
l,r||wl,r||2, (C.23)
the regret upper bound (28) holds with probability at least 1−δ.1
1We thank an anonymous reviewer for pointing out the need for explicitly stating the condition on c.
21