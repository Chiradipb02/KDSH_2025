Under review as submission to TMLR
Training Wasserstein GANs without Gradient Penalties via
Duality
Anonymous authors
Paper under double-blind review
Abstract
We propose a new method for training Wasserstein generative adversarial networks
(WGANs) without using gradient penalties. We develop a novel approach to accurately
estimate the Wasserstein distance between datasets, specifically tailored for the generative
setting. Instead of employing the commonly used gradient penalty term in the WGAN train-
ing procedure, we introduce two objective functions that utilize the c-transform based on
Kantorovich duality, which is a fundamental property in optimal transport theory. Through
our experiments, we observe that this algorithm effectively enforces the Lipschitz constraint
on the discriminator, paving the way for understanding the optimal transport problem via
a deep learning approach. As a result, our method provides an accurate estimation not only
for the optimal discriminator but also for the Wasserstein distance between the true and
generated distribution. Notably, our method eliminates the need for gradient penalties and
corresponding hyperparameter tuning. Moreover, we demonstrate its effectiveness by gener-
ating competitive synthetic images using various datasets such as MNIST, Fashion-MNIST,
CIFAR-10, and CelebA-HQ.
1 Introduction
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have achieved remarkable success in
generating synthetic images. GANs consist of two networks, the generator and the discriminator, which
competewitheachother. ThegoalofthisprocedureistofindaminimaxsolutionknownasNashequilibrium.
GANs have found numerous applications in machine learning, including semisupervised learning, text-to-
image generation, image-to-text generation, and speech synthesis (Ledig et al., 2017; Isola et al., 2017; Zhang
et al., 2017; Reed et al., 2016; Don, 2019). However, training GANs remains challenging due to the intrinsic
instability associated with mode collapse, and gradient vanishing (Gulrajani et al., 2017). Several works
have aimed to improve stability (Tolstikhin et al., 2017; Salimans et al., 2016; Nowozin et al., 2016; Radford
et al., 2015; Kodali et al., 2017; Liu et al., 2017) and have made substantial improvements beyond the
original GAN. Of particular interest in this context is the Wasserstein GAN (WGAN) framework proposed
in Arjovsky et al. (2017), where the training objective for the generator network is the Wasserstein distance
to the target distribution.
In terms of learning performance, experimental evidence has demonstrated the potential advantages of
WGANs over GANs, eliminating the possibility of gradient vanishing and mode collapse (Arjovsky et al.,
2017), but their mathematical properties have not yet been thoroughly understood. In WGANs, one aims
to solve a minimax problem involving the generator and the discriminator, where the discriminator is con-
strained to be a 1-Lipschitz function. The original WGAN introduced weight clipping to enforce the 1-
Lipschitz condition, but this approach can overly restrict the class of functions. To address this issue,
WGANs with gradient penalty (WGAN-GP) were proposed in Gulrajani et al. (2017). While successful in
generative settings, recent work (Milne & Nachman, 2022) shows that WGAN-GP is not trained in a way
that minimizes the Wasserstein distance. It is demonstrated that the optimal value of the objective function
used in WGAN-GP corresponds to the minimum cost of a congested transport model, diverging from its
original philosophy (Milne & Nachman, 2022). Furthermore, several attempts have been made to under-
1Under review as submission to TMLR
stand the training dynamics of WGANs by analyzing the sample complexity for estimating the Wasserstein
distance as found in Mallasto et al. (2019); Stanczuk et al. (2021).
Considering the nature of WGAN, its performance relies on the proper estimation of the Wasserstein distance
between two training datasets. Estimating the Wasserstein distance is of independent interest and has been
extensively studied in the literature, focusing on sample complexity and efficient computation methods (Bas-
setti et al., 2006; Dudley, 1969; Weed & Bach, 2019). However, there is relatively limited literature on deep
learning-based frameworks for estimating distance. In this paper, we investigate an efficient approach to
approximating the Wasserstein distance via neural networks.
We propose an intuitive and straightforward method to learn the Wasserstein distance within the generative
model framework. Our approach solely focuses on the formulation of the Wasserstein distance and does
not introduce any form of penalization, eliminating the need for parameter tuning. A key feature of our
proposed method is that we use sample batches to approximate the Wasserstein distance between two large,
high-dimensional datasets.
Moreover, we explore avenues to enhance the stability, accuracy, and computational efficiency of WGAN
training, aligning them more closely with the mathematical theory of optimal transport. Building upon
Mallasto et al. (2019), we investigate the c-transform methods, which have shown promise in accurately
estimating the true Wasserstein metric. However, these methods suffer from instability and perform sub-
optimally in the generative setting, as discussed in our theoretical investigation in Section 5 and Mallasto
et al. (2019).
1.1 Related work
1.1.1 Estimation of Wasserstein distance
Computing the Wasserstein distance efficiently is a long-standing problem that has attracted significant
attention in both mathematics and machine learning communities. The Wasserstein distance is highly
effective in quantifying the dissimilarity between two probability distributions. Cuturi (2013) introduces
the concept of the Sinkhorn distance, which utilizes entropic regularization to approximate the Wasserstein
distance. This approach has demonstrated remarkable computational efficiency compared to other methods
such as Pele & Werman (2009). In a subsequent study by Genevay et al. (2018), the Sinkhorn distance
is shown to be effective in the context of generative models. While these works address the approximate
optimal transport problem and the Wasserstein distance, our paper focuses on exact optimal transport and
its application to generative models.
Another closely related work is presented in Jacobs & Léger (2020), where the authors propose a novel
method called the back-and-forth method for computing the Wasserstein distance between distributions.
This method leverages the duality structure inherent in the optimal transport formulation. Not only is their
algorithm efficient in computing the Wasserstein distance, but it also facilitates the determination of optimal
transport maps. In our research, we exploit the structural properties of the back-and-forth method to devise
a deep learning algorithm for estimating the Wasserstein distance.
We also highlight the recent work by Xie et al. (2020), which introduces the Iterative Proximal Optimal
Transport (IPOT) algorithm. The algorithm efficiently approximates the Wasserstein distance without
introducing a gradient penalty term. The authors further demonstrate that IPOT can also be applied
to generative tasks. However, unlike our method, which directly enforces the 1-Lipschitz condition using
Kantorovich duality and the c-transform, IPOT focuses on optimizing the transport plan without explicitly
addressing the Lipschitz continuity of the discriminator.
1.1.2 Enforcement of 1-Lipshitz continuity
At the core of the implementation of Kantorovich-Rubinstein duality for estimating Wasserstein distance and
the framework of WGAN training is the enforcement of 1-Lipschitz continuity for an optimal discriminator
ϕ, which requires ensuring that the gradient norm of ϕis 1 almost everywhere, i.e., ∥Dϕ∥= 1where∥·∥
denotes the standard Euclidean norm throughout the paper. Traditionally, this involves explicitly computing
2Under review as submission to TMLR
the discriminator’s gradient at sample points and interpolated sample points. However, Wei et al. (2018)
argues that applying the gradient penalty only at sample points is insufficient. Additionally, Petzka et al.
(2017) emphasizes that globally satisfying ∥Dϕ∥= 1for an optimal discriminator can hinder optimization.
To address these challenges, they propose penalizing only gradient norms above 1 within a manifold around
the sample points, offering a stable approximation of the Wasserstein distance.
Another notable approach is introduced in Miyato et al. (2018), where the authors propose spectral normal-
ization to enforce Lipschitz continuity for functions parameterized by deep neural networks.
In contrast, our approach does not include any penalization term that enforces the condition ∥Dϕ∥= 1
on random samples obtained by interpolating real and synthetic data points. Therefore, our framework
eliminates the need for a weight parameter λthat controls the intensity of the penalization.
1.2 Our contribution
We propose a novel and stable training method for Wasserstein GANs (WGANs) that is inspired by the
back-and-forth method for the optimal transport problem on spatial grids, as introduced by Jacobs & Léger
(2020). In traditional WGANs, the objective function for training the discriminator and the generator is
defined using the Kantorovich duality formulation of the 1-Wasserstein distance between two probability
measuresµandνonΩ⊂Rd, which is given by
W1(µ,ν) = sup
ϕ∈Lip(Ω)(Eµ[ϕ]−Eν[ϕ]). (1.1)
Throughout the paper, Lip (Ω)denotes the set of 1-Lipschitz continuous functions on Ω, that is, functions
satisfying|ϕ(x)−ϕ(y)|≤∥x−y∥for anyx,y∈Ω. Instead, we consider the following two optimization
sup
ϕ∈C(Ω)(Eµ[ϕ] +Eν[ϕc])and sup
ϕ∈C(Ω)(Eµ[(−ϕ)c] +Eν[−ϕ]),
whereC(Ω)denotes the set of continuous functions in Ωandϕcis thec-transform of ϕthat will be dis-
cussed in Section 2.1. However, computing the c-transform can be computationally expensive for large
high-dimensional datasets, making it challenging to use the dual forms of Wasserstein distance. Our method
combines both formulations to develop a deep learning framework for estimating the 1-Wasserstein dis-
tance between large high-dimensional datasets, where the standard back-and-forth method is unavailable.
Additionally, our algorithm optimizes the traditional dual forms of Wasserstein distance.
One of the key advantages of our method is that it achieves stability in WGAN training without explicitly
penalizing the discriminator gradients, which is often required in approaches motivated by WGAN-GP.
This eliminates the need for hyperparameter tuning associated with gradient penalties whose mathematical
justification is unknown to the best of our knowledge.
Our contributions can be summarized as follows:
•(Section 3 & Algorithm 1) Based on the theoretical analysis, we propose a refined approach to esti-
mating the Wasserstein distance between large high-dimensional datasets using the c-transform. We
empirically verify the 1-Lipschitz condition without the need for a gradient penalty term, demon-
strating its effectiveness through various examples.
•(Section 4 & Algorithm 2) Leveraging the accurate estimation of the Wasserstein distance, we design
a stable WGAN training algorithm that enforces the correct 1-Lipschitz bound without the need for
a gradient penalty. This eliminates the requirement for hyperparameter tuning associated with the
penalty term.
•(Section 5) Our algorithm produces high-quality synthetic images and performs well with various
types of data, including mixtures of Gaussians, MNIST, Fashion-MNIST, CIFAR-10, and high-
dimensional datasets such as CelebA-HQ. The algorithm accurately computes the discriminator ϕ
during training and the 1-Wasserstein distance between the target and the distribution generated.
3Under review as submission to TMLR
2 Preliminaries
2.1 Optimal transport overview
For thed-dimensional Euclidean space Rd, letP(Ω)be the space of Borel probability measures on Ω∈Rd.
Thep-Wasserstein distance between two probability measures µandνinP(Ω)is defined as
Wp(µ,ν) :=/braceleftbigg
inf
γ∈Π(µ,ν)/integraldisplay
Ω×Ω∥x−y∥pdγ/bracerightbigg1
p
. (2.1)
Here, Π(µ,ν)isthesetofallBorelprobabilitymeasures πonΩ×Ωsuchthatπ(A×Ω) =µ(A)andπ(Ω×A) =
ν(A)for all measurable subsets A⊂Ω.
Under suitable assumptions on µandν, we also have
Wp(µ,ν) =/braceleftbigg
inf
T#µ=ν/integraldisplay
Ω∥x−T(x)∥pdµ(x)/bracerightbigg1
p
(2.2)
where a pushforward measure is defined as T#ρ(A) :=ρ(T−1(A))for a setA⊂Ωandρ∈P(Ω). It can be
interpreted as the minimum cost of transporting the distribution µtoνwith respect to the metric given by
∥x−y∥pforp≥1, which also can be used as the distance between two distributions. However, solving this
optimization problem is challenging in general.
Here, wegiveaformalderivationoftheKantorovich-Rubensteinduality. IntroducingtheLagrangemultiplier
ϕ: Ω→Rwhich enforces the condition T#µ=ν, and interchanging inf and sup, (2.2) can be reformulated
as
Wp
p(µ,ν) = inf
Tsup
ϕ/braceleftbigg/integraldisplay
Ω∥x−T(x)∥pdµ(x) +/integraldisplay
Ωϕdν−/integraldisplay
Ω(ϕ◦T)dµ/bracerightbigg
= sup
ϕinf
T/braceleftbigg/integraldisplay
Ω∥x−T(x)∥pdµ(x) +/integraldisplay
Ωϕdν−/integraldisplay
Ω(ϕ◦T)dµ/bracerightbigg
= sup
ϕ/braceleftbigg/integraldisplay
Ωϕdν+/integraldisplay
Ωinf
T{∥x−T(x)∥p−(ϕ◦T)}dµ(x)/bracerightbigg
.
Motivated by this, let us introduce the ctransform, ϕc(y) := infx∈Ω{∥x−y∥p−ϕ(x)}fory∈Ω(see
Proposition 1.11 in Santambrogio (2015)). Finally, the Kantorovich-Rubenstein duality yields that
Wp
p(µ,ν) = sup
ϕ∈C(Ω)/braceleftbigg/integraldisplay
Ωϕdµ+/integraldisplay
Ωϕcdν/bracerightbigg1
p
. (2.3)
Consequently, the duality property allows for a more efficient way of computing Wp(µ,ν)as one can avoid
explicitly constructing γ(x,y). Instead of optimizing over the infinite-dimensional space of couplings γ(x,y),
we optimize over a scalar function ϕ(x), which is known to be efficient in high dimensions.
Additionally, one can derive another representation for this formulation when p= 1, as in Remark 6.5 of
Villani (2008).
Proposition 1. Letµ,ν∈P(Ω). The 1-Wasserstein distance (1.1) between two probability measures µand
νis given by
W1(µ,ν) = sup
ϕ∈Lip(Ω)/braceleftbigg/integraldisplay
Ωϕd(µ−ν)/bracerightbigg
. (2.4)
Proof.See Villani (2008).
4Under review as submission to TMLR
The formulation (2.4) is useful not only for the development of an algorithm estimating the Wasserstein
distance between distributions but also for the generative model. In the framework of WGANs, ϕplays
the role of discriminators, and νis the generative distribution. The objective of WGAN is to find νfrom
a parametrized set of pushforward maps of a noise source that best mimics µ, which is given. In practice,
the integral is replaced by a sample average, and optimization is conducted iteratively using mini-batches of
samples.
2.2 The objective functions of Wasserstein GANs and its variants
For0< m≪dandp≥1, letµ∈P(Ω)denote the distribution of data, Ω⊂Rdandρ∈P(Rm)be a
source distribution such as Gaussian noise. The main goal of the Wasserstein GANs is to find a parametrized
generatorGθ:Rm→Rd, that minimizes Wp(µ,Gθ#ρ).
Here, the pushforward measure Gθ#ρis defined such that (Gθ#ρ)(B) =ρ(G−1
θ(B))for all measurable sets
B⊂Rd. Therefore, we solve the following minimax problem:
inf
θ/braceleftbigg
sup
η/braceleftbigg/integraldisplay
Ωϕη(dµ−dGθ#ρ) :ϕη∈Lip1/bracerightbigg/bracerightbigg
,
where the discriminator ϕηand the generator Gθare parametrized by neural networks. As briefly described
in the introduction, enforcing the Lipschitz condition is a challenging task in the implementation of WGANs,
particularly for large high-dimensional datasets. Various regularization tricks have been introduced in the
literature to resolve this issue. For instance, the loss function of WGAN-GP introduced in Gulrajani et al.
(2017) is given as follows:
inf
θsup
η/braceleftbigg/integraldisplay
Ωϕη(dµ−dGθ#ρ) +λ/integraldisplay
Ω(∥Dϕη∥−1)2dω/bracerightbigg
. (WGAN-GP)
A central aspect of this approach is to enforce the condition ∥Dϕη∥= 1throughout the entire domain,
although this is not a necessary and sufficient condition for (2.4) to attain equality. There also have been
several attempts to design a loss function trying to enforce 1-Lipschitzness such as WGAN-LP (Petzka
et al., 2017), WGAN-CT (Wei et al., 2018), and spectral normalizations (Miyato et al., 2018), but all use
WGAN-GP as the baseline by inserting the gradient penalty term in the loss function.
On the other hand, the c-transform-based method has been proposed in Mallasto et al. (2019). The objective
function is given as
inf
θsup
η/braceleftbigg/integraldisplay
Ωϕηdµ−/integraldisplay
Ωϕc
ηd(Gθ#ρ)/bracerightbigg
, (c-transform)
whereϕc
ηis thec-transform of ϕηdefined in (3.4) below. This does not have the gradient penalty term, but
as investigated in Mallasto et al. (2019) and Stanczuk et al. (2021), the method does not perform well in the
generative setting. More discussions can be found in Section 4.3.
3 Estimation of 1-Wasserstein distance
In this section, we present a theoretical framework for a new algorithm designed not only for the estimation
of Wasserstein distances but also for training WGANs without the need for gradient penalties. Our approach
involves a novel learning method that accurately estimates the 1-Wasserstein distance between large, high-
dimensional datasets. Subsequently, the question of whether WGANs create a distribution mimicking the
true distribution can be answered.
While various approaches have been previously discussed, we extend the concept of the back-and-forth
method to the realm of deep learning, enabling us to effectively estimate the 1-Wasserstein distance.
Building upon this proposed method, we introduce a new training technique for WGANs in the following
section. To accomplish this, we first introduce the notion of a universal admissible condition.
5Under review as submission to TMLR
Figure 1: Why does GP fail to enforce 1-Lipschitz regularity in the discrete setting? When the probability
measures are discrete, forcing ∥Dϕ∥= 1along random interpolation points between the real and fake data
may induce underestimate of the Lipschitz constant of ϕ,supx∼real,y∼fake|ϕ(x)−ϕ(y)
∥x−y∥|. The left shows that the
Lipschitz constant between real and fake data is less than 1, as depicted by the dashed red line. In high
dimensional space like the diagram on the right, WGAN-GP simply imposes the gradient norm constraint
on a random interpolation point denoted by blue, which does not help impose the 1-Lipschitz continuity of
the discriminator while our algorithm will directly compute the slope between these two points.
3.1 The universal admissible condition
To find a probability measure νthat best mimics µusing mini-batches, we first define the so-called universal
admissible set as the set of ϕ’s satisfying
ϕ(x)−ϕ(y)≤∥x−y∥for all (x,y)∈supp(µ)×supp(ν) (3.1)
for two probability measures µ,ν∈P(Ω).
The aforementioned condition offers two primary advantages: a reduced computational cost and an expanded
admissible set. The computational efficiency of utilizing (3.1) is more prominent when the support sets of
µandν, denoted as supp (µ)and supp (ν)respectively, are relatively small. This is particularly evident in
scenarios where the data points lie in a low-dimensional manifold.
Conversely, any function on Ωthat satisfies the 1-Lipschitz property also satisfies the relation (3.1). Hence,
our condition is less restrictive than enforcing strict 1-Lipschitzness on Ω. Consequently, (3.1) allows for the
representation of a broader class of functions.
To discuss the potential advantages of the utilization of the universal admissible condition, we point out the
limitations of imposing the gradient penalty. Fig. 1 demonstrates why training with the gradient penalty
fails to estimate the true Wasserstein distance between distributions accurately. The core of the WGAN-GP
training algorithm involves selecting points a pair of real and synthetic data points, followed by selecting a
random interpolation point, where the norm of the gradient is enforced to be 1. However, as depicted in
Fig. 1, this approach tends to underestimate the slope, particularly in high dimensional-spare datasets, even
if the norm of the gradient at discrete points is forced to be constant. An alternative approach, spectral
normalization (Miyato et al., 2018), which enforces the Lipschitz constant to be a constant everywhere,
possesses a similar limitation. Moreover, if the distribution of real data exhibits a specific shape, as shown
on the right of Fig. 1, such a training method that relies on interpolation of two points can fail to impose
1-Lipschitz constraint of the discriminator ϕ.
Using the duality formulation, we prove that the relaxation of the set of feasible functions still computes the
Wasserstein distance. While the proof is straightforward from the duality structure, the observation gives
us a clue for the construction of our new algorithm. Instead of the Lipschitz condition on Ω, it suffices to
enforce (3.1) while training WGANs. This observation plays a critical role in one of the main algorithms,
Algorithm 1.
6Under review as submission to TMLR
Theorem 1. For anyµ,ν∈P(Ω), we have
W1(µ,ν) = sup/braceleftbigg/integraldisplay
Ωϕd(µ−ν) :ϕ∈C(Ω)satisfies (3.1)/bracerightbigg
. (3.2)
Proof.Recall the duality formulation (2.4) of W1. As all 1-Lipschitz functions satisfy (3.1), we have
W1(µ,ν)≤sup/braceleftbigg/integraldisplay
Ωϕd(µ−ν) :ϕ∈C(Ω)satisfies (3.1)/bracerightbigg
.
On the other hand, consider a so-called transport plan,
γ: Ω×Ω→R,
which satisfies
γ(A×Ω) =µ(A)andγ(Ω×A) =ν(A) (3.3)
for all measurable subsets A⊂Ω. Since supp (γ)⊂supp(µ)×supp(ν), we have
/integraldisplay
Ωϕ(dµ−dν) =/integraldisplay
Ω×Ω(ϕ(x)−ϕ(y))dγ(x,y),
≤/integraldisplay
Ω×Ω∥x−y∥dγ
for allϕsatisfying (3.1) and any transport plan γsatisfying (3.3).
As a consequence of this inequality and (2.1), we have
sup/braceleftbigg/integraldisplay
Ωϕ(dµ−dν) :ϕ∈C(Ω)satisfies (3.1)/bracerightbigg
≤ inf
γ∈Π(µ,ν)/braceleftbigg/integraldisplay
Ω×Ω∥x−y∥dγ(x,y)/bracerightbigg
=W1(µ,ν).
Thus, the result (3.2) follows.
3.2 Our approach: comparison between pseudo-objective functions
Given two probability measures µ,ν∈P(Ω), we recall the dual forms, (2.3) and (2.4) to introduce following
pseudo-objective functions,
J1(ϕ;µ,ν) =/integraldisplay
Ωϕdµ+/integraldisplay
Ω(−ϕ)dν,
J2(ϕ;µ,ν) =/integraldisplay
Ωϕdµ+/integraldisplay
Ωϕc(·;µ)dν,
J3(ϕ;µ,ν) =/integraldisplay
Ω(−ϕ)c(·;ν)dµ+/integraldisplay
Ω(−ϕ)dν,
J4(ϕ;µ,ν) =/integraldisplay
Ω(−ϕ)c(·;ν)dµ+/integraldisplay
Ωϕc(·;µ)dν,
where the pseudoc-transform ofω∈P(Ω)is defined by
ϕc(y;ω) := inf
x∈supp(ω){∥x−y∥−ϕ(x)}fory∈Ω. (3.4)
An important feature of the formulation is that we shall take the infimum in the c-transform over the support
of the probability measure instead of the whole domain Ω, which is simpler to compute compared to the
originalc-transform.
7Under review as submission to TMLR
This concept has been implicitly used in the literature, e.g., in the works of Farnia & Ozdaglar (2020);
Mallasto et al. (2019). However, to our knowledge, the discrepancy between ϕcandϕc(·;ω)has not been
studied in detail.
To further explore the pseudo c-transform, let us provide some intrinsic properties of discrete c-transform
in comparison with the original one. It is clear that the original c-transform satisfies ϕc≤−ϕover Ω, but
this relation does not hold for ϕc(·;ω)in general. In turn, ϕcis not necessarily equal to −ϕeven ifϕis
a 1-Lipschitz function. However, the following inequalities hold for the Ji’s, which is a key ingredient for
developing our method.
Lemma 1. Given two proability measures, µ,ν∈P(Ω), assume that ϕsatisfies the admissibility property
(3.1). Then we have
J1(ϕ;µ,ν)≤J 2(ϕ;µ,ν)≤J 4(ϕ;µ,ν)and
J1(ϕ;µ,ν)≤J 3(ϕ;µ,ν)≤J 4(ϕ;µ,ν).
Proof.From (3.1) and supp (µ)⊂Ω, it holds that for all y∈Ω,
ϕc(y;µ) = inf
x∈supp(µ){∥x−y∥−ϕ(x)}
≥ inf
x∈supp(µ){−ϕ(y)}=−ϕ(y).
Therefore, we conclude J1(ϕ;µ,ν)≤J 2(ϕ;µ,ν). The other inequalities can be shown similarly.
Wenowdemonstratetheeffectivenessofleveragingthenotionofpseudo c-transforminabatch-basedlearning
approach.
Considerni.i.d. observations X1,X2,...,Xndistributed according to µ, and another ni.i.d. observations
Y1,Y2,...,Yndistributed according to ν. We denote the empirical measures based on samples Xi’s andYi’s
byµnandνnrespectively:
µn:=1
nn/summationdisplay
i=1δXiandνn:=1
nn/summationdisplay
i=1δYi. (3.5)
Since they are finite measures, the pseudo c-transformsJ1(ϕ;µn,νn),J2(ϕ;µn,νn),J3(ϕ;µ,ν)and
J4(ϕ;µn,νn)are well-defined and easy to compute as the optimization is performed over the finite sets,
supp(µn)and supp (νn). In addition to that, we have the comparison between them as a result of Lemma 1.
We now introduce criteria in terms of empirical measures for the condition (3.1) to be satisfied. In particular,
equivalence between the inequalities and (3.1) holds as follows.
Lemma 2. Letµ,ν∈P(Ω). Givenn∈N, if we have
J1(ϕ;µn,νn)≤J 2(ϕ;µn,νn)
for allµnandνn, thenϕsatisfies the admissibility property (3.1). Here, µnandνnare empirical measures
obtained from µandνas given in (3.5).
Proof.For anyx∈supp(µ)andy∈supp(ν), ifXi=xandYi=yfor alli= 1,2,...,n, then the empirical
measures are µn=δxandνn=δy.
FromJ1(ϕ;µn,νn)≤J 2(ϕ;µn,νn), we derive that
ϕ(x)−ϕ(y)≤ϕ(x) +ϕc(y;δx).
Asϕc(y;δx) =∥x−y∥−ϕ(x), we conclude that
ϕ(x)−ϕ(y)≤∥x−y∥.
Hence, (3.1) holds for any (x,y)∈supp(µ)×supp(ν).
Remark 1. The same result still holds if J1(ϕ;µn,νn)≤ J 2(ϕ;µn,νn)is replaced byJ1(ϕ;µn,νn)≤
J3(ϕ;µn,νn).
8Under review as submission to TMLR
3.3 Relation between J1and the Wasserstein distance
Our first goal is to design a learning algorithm that estimates the Wasserstein distance between two proba-
bility measures µandν. We begin by providing an equivalent representation beyond the Kantorovich duality
formula. To motivate, we observe first that
EXi∼µ,Yi∼ν
1≤i≤n/bracketleftigg
1
nn/summationdisplay
i=1(ϕ(Xi)−ϕ(Yi))/bracketrightigg
=/integraldisplay
Ωϕd(µ−ν). (3.6)
Recalling the equivalence between the 1-Lipschitz condition and the universal admissible condition for em-
pirical measures given in Lemma 1 and 2, we deduce the following equivalent maximization problem,
sup
ϕ∈B/braceleftbigg
EXi∼µ,Yi∼ν
1≤i≤n[J1(ϕ;µn,νn)]/bracerightbigg
, (3.7)
where
B:={ϕ∈C(Ω) : 0≤J 1(ϕ;µn,νn)≤J 2(ϕ;µn,νn)and
0≤J 1(ϕ;µn,νn)≤J 3(ϕ;µn,νn)for all empirical measures µnandνnas given in (3.5)}.
Altogether, we have the following alternative formulation for achieving the Wasserstein distance between
two probability measures µandν.
Theorem 2. Letn,d∈Nbe given and µ,ν∈P(Ω)forΩ⊂Rd. Then, the Wasserstein distance between µ
andνis equal to the optimal value of (3.7).
This yields thatJ1is indeed an unbiased estimator of the Wasserstein distance for a suitable choice of ϕ.
Therefore, if trained well to find ϕ, the functionalJ1would yield an accurate estimate for the Wasserstein
distance between two measures, µandν.
Proof.Forϕ∈C(Ω)and empirical measures µnandνndefined as (3.5), we have
J1(ϕ;µn,νn) =1
nn/summationdisplay
i=1(ϕ(Xi)−ϕ(Yi)).
By Theorem 1, the Wasserstein distance between µandνis given as
W1(µ,ν) = sup
ϕ∈Lip(Ω)EXi∼µ,Yi∼ν
1≤i≤n[J1(ϕ;µn,νn)]
= sup
ϕsatisfies (3.1)EXi∼µ,Yi∼ν
1≤i≤n[J1(ϕ;µn,νn)].
By Lemma 1 and 2, ϕsatisfies (3.1) if and only if ϕ∈B, and this completes the proof.
To evaluate (3.7), we estimate using random samples Xi∼µandYi∼ν. Given an optimal ϕ∈Bsolving
(3.7), weusethelawoflargenumberstotransformtheoriginaloptimizationproblemintoalearningproblem,
which is
1
nn/summationdisplay
i=1(ϕ(Xi)−ϕ(Yi))→EX∼µ[ϕ(X)]−EY∼ν[ϕ(Y)] =W1(µ,ν)w.p. 1 with respect to µ×ν,
asngrows to infinity.
The error can be quantified under suitable assumptions. For instance, as long as ϕis bounded, the Hoeffding
inequality (Bentkus, 2004) yields that
Pr/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1(ϕ(Xi)−ϕ(Yi))−W1(µ,ν))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥ϵ/parenrightigg
≤2 exp(−Cnϵ2)
9Under review as submission to TMLR
for some constant C > 0.
Lastly, in Algorithm 1, we use the following objective function to tackle this problem:
sup
ϕ∈C(Ω)EXi∼µ,Yi∼ν
1≤i≤n[J1(ϕ;µn,νn)1B−J1(ϕ;µn,νn)1Bc], (3.8)
where 1A(x) = 1ifx∈Aand1A(x) = 0ifx∈Acfor a given set A.
Theorem 3. Givenµ,ν∈P(Ω), the optimal value of (3.8) corresponds to W1(µ,ν).
Proof.From Theorem 1, (3.7) gives the Wasserstein distance between µandν, that is,
W1(µ,ν) = sup
ϕ∈B/braceleftbigg
EXi∼µ,Yi∼ν
1≤i≤n[J1(ϕ;µn,νn)]/bracerightbigg
. (3.9)
Letϕbe the maximizer of the equation above. Since
J1(ϕ;µn,νn)≤J 2(ϕ;µn,νn)andJ1(ϕ;µn,νn)≤J 3(ϕ;µn,νn),
we have that (3.8) is greater than or equal to (3.9).
The converse is true thanks to the condition J1(ϕ;µn,νn)≥0in (3.8) and we finish the proof.
4 Algorithm
4.1 Algorithm for estimating the Wasserstein distance
Inspired from (3.8), we propose an iterative scheme for estimating the Wasserstein distance between two
probability measures, which is given in Algorithm 1. Let us illustrate its procedure in detail. We begin by
parameterizing ϕbyη, such as deep neural networks with a parameter η. Given two sets of i.i.d samples
Xi∼µandYi∼νfori= 1,...,n, one computesJk(ϕ;µn,νn)fork∈{1,2,3,4}. IfJ1(ϕ;µn,νn)is greater
than eitherJ2(ϕ;µn,νn)orJ3(ϕ;µn,νn), one updates ηdo increaseJ1(ϕ;µn,νn). Otherwise, ηis updated
to decreaseJ1(ϕ;µn,νn).
Thisway, wecanbuild a1-Lipschitzcontinuous function ϕthatbetterapproximates theWasserstein distance
between probability measures using mini-batches. Note that Adam optimizer (Kingma & Ba, 2014) is used
for updating neural network parameters throughout the paper.
Algorithm 1 Our proposed algorithm of estimating Wasserstein distance
foriterof training iterations do
ifJ2(·;µn,νn)<J1(·;µn,νn)orJ3(·;µn,νn)<J1(·,µn,νn)then
η←Adam (J1(·,µn,νn),η)
end
η←Adam (−J1(·,µn,νn),η)
end
4.2 Main Algorithm for WGAN
We now propose a new framework for learning a target distribution ν∈Pby solving
inf
ν∈P(Ω)sup/braceleftbigg/integraldisplay
Ωϕd(µ−ν) :ϕsatisfies (3.1)/bracerightbigg
. (4.1)
Based on the observation in Lemmas 1 and 2, we propose the following training procedure as described in
Algorithm 2. For the discriminator ϕand the generator parametrized by ηandθrespectively, the algorithm
begins with minimizing J1(ϕ;µn,νn)if eitherJ2(ϕ;µn,νn)<J1(ϕ;µn,νn)orJ3(ϕ;µn,νn)<J1(ϕ;µn,νn)
10Under review as submission to TMLR
holds. Otherwise, ϕis updated to maximize J1. For the generator step, we follow the standard scheme. The
generator parameter θis updated to minimize J1(ϕ;µn,νn)whileϕfixed. Since the algorithm depends on
the comparison between Ji’s, we call it CoWGAN . A salient feature of this new algorithm is that it only
requires that ϕbe Lipschitz indirectly through the comparison process while keeping the generator-update
step the same as classical algorithms such as WGAN-GP.
Algorithm 2 Our proposed algorithm ( CoWGAN ) to find the minimum of (4.1)
foriterof training iterations do
fort= 1,2,...,ncriticdo
ifJ2(·;µn,νn)<J1(·;µn,νn)orJ3(·;µn,νn)<J1(·;µn,νn)then
η←Adam (J1(·;µn,νn),η)
end
η←Adam (−J1(·;µn,νn),η)
end
θ←Adam (J1(·;µn,νn),θ)
end
4.3 Why not optimizing J2or other objective functions?
In this section, we provide an illustrative example that empirically justifies our approach.
In practice, one does not have access to the true distribution but rather to mini-batches that are sampled
from the training dataset available. Although we expect J1to be close toJ2andJ3, we observe that the
objective functions J2andJ3may not be suitable for constructing the true distribution. As shown in Fig. 2,
blurry images are achieved if J2is used as an objective function.
Figure 2: In comparison to CoWGAN (left), only J2is used inc-transform (right)
In what follows, we discuss why we do not use J2orJ3as objective functions but rather use J1, and why
other methods based on J2may lead to poor generators.
The problem of minimizing (2.4) with respect to νcannot be solved by considering J2orJ3instead ofJ1:
inf
ν∈P(Ω)sup
ϕ∈Lip(Ω)EXi∼µ,Yi∼ν
1≤i≤n[J2(orJ3)]. (4.2)
We note from (3.5) that
J2=1
nn/summationdisplay
i=1/parenleftbigg
ϕ(Xi)−min
1≤j≤n{∥Xj−Yi∥−ϕ(Xj)}/parenrightbigg
, (4.3)
andE[J1]̸=E[Ji]fori= 2,3in general.
We will demonstrate that replacing J1withJ2might lead to a poor estimation of the target measure µ. To
see this, setting µ=µmfor somem> 1andn= 1in (4.2) with an objective function J2, (4.3) reduces to
inf
ν∈P(Ω)EX∼µm,Y∼ν∥X−Y∥. (4.4)
The question is whether an optimal νin (4.4) approximates the given probability measure µ. The answer is
noas illustrated in the following proposition and Fig. 2, which justifies the use of J1as an objective function
in our algorithm.
11Under review as submission to TMLR
CoWGAN (ours) c-transform WGAN-GP, λ= 1 WGAN-GP, λ= 10
0 1000 2000 3000 4000 5000
iteration0.2
0.00.20.40.60.81.01.21.41.61.8
MW
J1
W
0 1000 2000 3000 4000 5000
iteration0.2
0.00.20.40.60.81.01.21.41.61.8
MW
J1
W
0 1000 2000 3000 4000 5000
iteration0.2
0.00.20.40.60.81.01.21.41.61.8
MW
J1
W
0 1000 2000 3000 4000 5000
iteration0.2
0.00.20.40.60.81.01.21.41.61.8
MW
J1
W
0 1000 2000 3000 4000 5000
iteration1
0123MW
J1
W
0 1000 2000 3000 4000 5000
iteration1
0123MW
J1
W
0 1000 2000 3000 4000 5000
iteration1
0123MW
J1
W
0 1000 2000 3000 4000 5000
iteration1
0123MW
J1
W
Figure 3:J1, the true Wasserstein distance (W), and the minibatch Wasserstein distance (MW) with
mini-batch size 64 (top) and 8 (bottom) based on two 4-Gaussian synthetic datasets. Optimal Transport
Toolbox (Flamary et al., 2021) is used for comparison.
Proposition 2. IfΩ⊂R, the minibatch size n= 1andµ∈Pm(Ω)form > 1, thenν=δyis a global
minimizer of (4.4) for any median yofµwherePm(Ω)denote a set of empirical measures supported on
at mostmpoints in Ω. The median of a probability measure µis defined as a real number ksatisfying
µ((−∞,k])≥1
2andµ([k,∞))≥1
2. One can achieve multiple medians when supp (µ)contains intervals.
Proof.To see this let us define an empirical measure µm=1
m/summationtextm
i=1δXiwithX1<X 2<···<Xm.
We claim that if y∈[X[(m+1)/2],X[(m+2)/2]], thenν=δyis a global minimizer of (4.4). As d=n= 1, the
objective function in (4.4) can be represented as
EX∼µm,Y∼ν[∥X−Y∥] =:I.
To proceed, by the triangular inequality,
I=1
2EY∼ν/bracketleftiggm/summationdisplay
i=1∥Xi−Y∥+∥Xm+1−i−Y∥/bracketrightigg
≥1
2m/summationdisplay
i=1∥Xi−Xm+1−i∥.
On the other hand, if ν=δyfory∈[X[(m+1)/2],X[(m+2)/2]], then we have
EY∼ν/bracketleftiggm/summationdisplay
i=1∥Xi−Y∥/bracketrightigg
=[(m+1)/2]/summationdisplay
i=1(y−Xi) +m/summationdisplay
i=[(m+2)/2](Xi−y)
=[(m+1)/2]/summationdisplay
i=1(Xm+1−i−Xi).
As the minimum of (4.4) is attained at ν=δy, we finish the proof.
The result implies that νmight completely fail to mimic µusing mini-batches when µis a discrete measure.
This observation coincides with the blurry images obtained in previous works using the c-transform method
to train WGANs (Mallasto et al., 2019) and discussions presented in Stanczuk et al. (2021).
In general, the discrepancy between the true distance and the estimated distance using finite samples has
been studied by Arora et al. (2017; 2018); Bai et al. (2018). Furthermore, in higher dimensions, even if both
µn,νncome from the same distribution µ, one hasW1(µn,νn)≥C > 0for someC; see the works of Fatras
et al. (2020; 2021) for details.
12Under review as submission to TMLR
CoWGAN (ours) c-transform WGAN-GP, λ= 1 WGAN-GP, λ= 10
Figure 4: The discriminator ϕfor two mixtures of 4 Gaussians (samples shown as green and yellow dots) after
2000 iterations with different methods and mini-batch size 64 (top). The gradient norm of the discriminator
∥Dϕ∥for two mixtures of 4 Gaussians after 2000 iterations with different methods and mini-batch size 64
(bottom).
Figure 5: log(J1), and the logarithm of the true Wasserstein distance log(W)between 5,000 images of digit
1 and 5,000 images of digit 2 from the MNIST dataset
5 Experiments
Various empirical results are provided to validate our method. We first test our algorithm on various
trainingsetsrangingfromGaussianmixturedistributiontotheMNISTdatasettoestimatethe1-Wasserstein
distance. In the subsequent part, we validate the effectiveness of the proposed algorithm in the generative
setting. Particularly, we focus on the enforcement of the 1-Lipschitz condition, the landscape of loss during
the training, and the effect of hyperparameters.
5.1 Estimating the 1-Wasserstein distance
In this section, we provide empirical demonstrations of how well our method computes the 1-Wasserstein
distance using Algorithm 1. Synthetic data, as well as various benchmark datasets such as MNIST, Fashion-
MNIST, CIFAR10, and CelebA are used for the experiments. Furthermore, we also show the effectiveness
of our learning algorithm for estimating the Wasserstein distance even with a small batch size.
13Under review as submission to TMLR
5.1.1 Synthetic datasets in 2D
On synthetic datasets in 2D, we observe that the condition (3.1) can be effectively enforced when using the
comparison method.
In Fig. 3, we compute the Wasserstein distance between two different distributions based on four different
methods. Each distribution is a mixture of 4 different Gaussian distributions. Samples from them are
denoted by yellow and green points as shown in Fig. 4. It is observed that CoWGAN converges much faster
than the other algorithms tested, namely the c-transform method (Mallasto et al., 2019), and WGAN-GP
withλ= 1orλ= 10(Gulrajani et al., 2017) as shown in Fig. 3.
Thediscriminator ϕobtainedbyAlgorithm1capturesthefeaturethatdistinguishestwodistinctdistributions
with the same training time, which is the desired property for ϕ. It is demonstrated in Fig. 4. In particular,
we observe that in our algorithm ∥Dϕ∥= 1a.e. and the Lipschitzness of ϕis enforced well.
The optimal discriminator should have level sets orthogonal to the transport map. As shown in Fig. 4, this
property is observed in the result obtained by our method, unlike others.
5.1.2 The MNIST dataset
Computing the Wasserstein distance for high-dimensional data is challenging in general. We experimentally
verify that CoWGAN efficiently can estimate the Wasserstein distance between probability measures on a
high-dimensionality.
In Fig. 5, we sampled 5,000 images of digit 1 and 5,000 images of digit 2 from the MNIST dataset. Applying
Algorithm 1, we compute the Wasserstein distance between the two distributions, one representing 1 and
the other representing 2. We then compare the outcome of the algorithm with the true Wasserstein distance
between two sets of 5,000 images computed using the POT Python Library (Flamary et al., 2021). We note
that the algorithm well approximates the distance well while others induce non-negligible bias.
5.1.3 Training with small mini-batches
Surprisingly, Algorithm 1 can enforce the Lipschitz constraint very effectively, even with a very small mini-
batch size. We consider the same distributions as in the previous experiment, Fig. 4, but with the reduced
size of the mini-batches, 8. The result is presented in Fig. 6, and it shows that our algorithm still accurately
computes the optimal discriminator, yet the other methods have significant difficulties even after more
iterations. It is quite natural to see such a phenomenon as the gradient penalty method tries to enforce the
norm of the gradient of a randomly chosen point between real and synthetic data to 1. On the other hand,
CoWGAN just imposes a condition on the slope between the data, real and synthetic. As a consequence,
the norm of the slope is maintained to be a value close to 1.
5.2 Learning generative models
We now apply our algorithm to generative learning in a framework of WGAN. Several experimental results
usingtheMNIST,FashionMNIST,CIFAR-10, aswellasCelebA-HQdatasetsresizedto 64×64and128×128
are presented., Again we utilize the algorithm proposed in the previous section to generate synthetic images.
We find that the training procedure is more stable compared with WGAN-GP in terms of the Lipschitz
estimate while generating images of similar quality. In particular, the proposed machinery is robust to
hyperparameter tuning as the algorithm does not involve any parameters such as weight λorncritic, which
represents the number of discriminators updated per each generator update. The details on the used neural
network architecture are provided in Appendix A.
5.2.1 1-Lipschitzness
We present the quantitative differences between our algorithm and the classical WGAN-GP by examining the
Lipschitzconstantcomputedwithsamples. Tothisend, wefirstsample64pointsfromeachrealandsynthetic
14Under review as submission to TMLR
CoWGAN (ours) c-transform WGAN-GP, λ= 1 WGAN-GP, λ= 10
Figure 6: The discriminator ϕafter 5,000 iterations with mini-batches of size 8 (top). Shown is ∥Dϕ∥after
5,000 iterations with mini-batches of size 8 (bottom).
MNIST Fashion-MNIST CIFAR-10 CelebA-HQ
Figure 7: Lipschitz estimate within the real data denoted by Lip1(top), and synthetic data denoted by Lip2
(bottom); Lipschitz constant is close to 1 in our algorithm while WGAN-GP underestimates it.
CIFAR-10 CelebA-HQ
Figure 8: Empirical density of Lipschitz constant computed with 10,000 pairs of real/synthetic dataset.
dataset and compute the maximum slope of the discriminator within them, i.e., supx∼µ,y∼ν/vextendsingle/vextendsingle/vextendsingleϕ(x)−ϕ(y)
∥x−y∥/vextendsingle/vextendsingle/vextendsingleand
supx,y∼µ/vextendsingle/vextendsingle/vextendsingleϕ(x)−ϕ(y)
∥x−y∥/vextendsingle/vextendsingle/vextendsingle.
15Under review as submission to TMLR
MNIST Fashion-MNIST CIFAR-10 CelebA-HQ
Figure 9: The adjusted discriminator loss (the discriminator loss/the Lipschitz norm). Due to the smaller
Lipschitz estimate in Fig. 7, WGAN-GP underestimates the loss function. Our losses are smaller when the
Lipschitz norms of each discriminator are normalized to one.
The result is shown in Fig. 7. We observe that WGAN-GP and WGAN-GP with spectral normalization (SN)
constantly underestimate the Lipschitz constant. On the other hand, our algorithm maintains the Lipschitz
constant slightly above 1. This is mainly because the gradient penalty term unnecessarily forces the gradient
of points to be 1, even if the norm of their gradients does not have to be equal to one. Precisely, the optimal
ϕsatisfies∥Dϕ∥= 1only along the optimal coupling (see, e.g., Proposition 1 in Gulrajani et al. (2017)), but
the WGAN-GP algorithm imposes this condition for random points between real and synthetic data which
are chosen arbitrarily. However, our algorithm searches for a 1-Lipschitz continuous discriminator directly
by making use of the necessary condition, the duality.
In addition, Fig. 8 shows the empirical density of the Lipschitz constant, calculated using 10,000 pairs of data
from both real and synthetic datasets. These histograms provide a more detailed view of how the Lipschitz
constant behaves in practice. As illustrated, for more than 98.5% of the data pairs, the Lipschitz constant
remains less than or equal to 1, demonstrating that our method enforces the 1-Lipschitz condition well for
the majority of the data.
5.2.2 Loss and normalized loss - comparison after normalization
As shown in Fig. 7, WGAN-GP consistently underestimates the Lipschitz condition tested for real and
synthetic data, which does not agree with the intuition behind WGAN-GP; the optimal discriminator is
1-Lipschitz continuous, and the norm of gradients of the optimal discriminator is equal to 1 along arbitrary
optimal pairs.
We point out that the discriminator trained using the gradient penalty yields the possible upper bound for
the 1-Wasserstein distance between two distributions rather than the actual Wasserstein distance.
Remark 2. Letνgpandνcobe the distribution measures generated by WGAN-GP and CoWGAN, respec-
tively. As in Fig. 9, we empirically observe that the normalized loss function of CoWGAN is smaller than
that of WGAP-GP:
∥ϕco∥−1
LipJ1(ϕco;µ,νco)<∥ϕgp∥−1
LipJ1(ϕgp;µ,νgp),
where∥f∥Lipdenotes the Lipschitz constant of f. Given Theorem 3, our method computes the 1-Wasserstein
distance, at least in theory. Therefore, we conclude that our distribution is closer to the true distribution µ:
W1(µ,νco)<W 1(µ,νgp). (5.1)
Hence, the effectiveness of our method for estimating the 1-Wasserstein distance is corroborated by this
empirical evidence, as our normalized loss always lies below that of WGAN-GP.
5.2.3 Effect of ncritic
WGAN-GP and many subsequential improved algorithms usually choose ncriticto be five when d=g=
0.0001, which allows the algorithm to train the discriminator enough before updating the generator. In this
subsection, we heuristically analyze the effect of ncriticin our algorithm by choosing ncritic∈{1,5,10}. For
16Under review as submission to TMLR
Figure 10: With the same learning rate for both discriminator and generator, d=g= 0.0001, the effect of
ncriticthat indicates the number of discriminator updates per generator update is demonstrated using the
CIFAR-10 dataset.
the learning rate, we use d=g= 0.0001. As seen in Fig. 10, ncritic = 1shows the best performance in terms
of the stability of the loss curve and quality of synthetic pictures generated. Hence, we deduce that it is not
necessary to train the discriminator more than the generator under the CoWGAN framework.
5.2.4 Hyperparameter tunning
As noted, our algorithm does not contain the penalizing term, which is usually denoted by λ, unlike most
WGAN-GP and related models built on it, such as Wei et al. (2018).
Ontheotherhand,thechoiceoflearningrateisabitdelicate. Denotingthelearningrateforthediscriminator
and generator by dandgrespectively, it is common to set d=g= 0.0001. Meanwhile, Heusel et al. (2017)
evaluates the effectiveness of implementing a two-time scale update for training GANs. Motivated by this,
we setg>d, and the ratio between ganddwe use for the experiments is given in Table 1.
Dataset Discriminator (d) Generator (g)
CoWGAN MNIST,FMNIST d= 0.0001 5d
CIFAR-10 d= 0.0001 2d
CelebA-HQ d= 0.00005 2d
WGAN-GP same for all data d= 0.0001 d
Table 1: Diffrent learning rate depending on the training data.
5.2.5 Performance of generator
Dataset Method Inception Score (IS) ↑Frechet Inception Distance (FID) ↓
CIFAR10CoWGAN 6.03 ±0.13 26.34
WGAN-GP 6.14 ±0.16 27.66
WGAN-GP w/ SN 5.05 ±0.10 42.42
CelebA-HQCoWGAN 2.98 ±0.05 19.42
WGAN-GP 2.97 ±0.21 16.74
WGAN-GP w/ SN 2.87 ±0.13 26.32
Table 2: Comparison of CoWGAN and WGAN-GP on CIFAR and CelebA Datasets
Our algorithm performs well for generating synthetic images without computing the gradients explicitly, as
can be seen in Fig. 11 and Table 2. We test our algorithm with various datasets ranging from MNIST,
Fashion-MNIST, and CIFAR-10, which are resized to 32×32, and CelebA, which is resized to 64×64. An
additional experiment with the CelebA-HQ dataset resized into 128×128was conducted, and the result is
presented in Appendix B. Our algorithm and WGAN-GP generate comparable images. However, when using
the same neural network architecture (See Appendix A), WGAN-GP with spectral normalization exhibits
a slightly lower IS (Inception Score) and higher FID (Frechet Inception Distance). This lower performance
17Under review as submission to TMLR
WGAN-GP
CoWGAN(ours)
Figure 11: Top line is generated by WGAN-GP; the bottom is generated via CoWGAN. From left to right,
MNIST, Fashion-MNIST, CIFAR-10, and CelebA-HQ, that is resized to 64×64are used. Visually, the
generated images are of similar quality, but our algorithm runs six times faster in wall-clock time. The batch
size is chosen to be 64.
has been discussed in Miyato et al. (2018), where authors note that increasing the number of feature maps
can negatively impact the performance of WGAN-GP with spectral normalization.
6 Conclusion
In this work, we propose a novel algorithm that accurately estimates the Wasserstein distance between
two probability measures and can be used to train generative models. We point out that enforcing the
gradient norm condition tends to underestimate the Wasserstein distance. The proposed algorithm exploits
acombinationofobjectivefunctionsinspiredbytherecentlyproposedback-and-forthmethod. Itissupported
by sound theoretical properties obtained by Kantorovich duality and a few new inequalities. Importantly,
our method does not require an explicit computation of the gradients of the discriminator to implement the
Lipschitz constraint on the optimal discriminators in WGANs. A salient feature of our method is the absence
of gradient penalties, eliminating the need for corresponding hyperparameter tuning. The effectiveness of
this approach has been verified through numerous experiments. We also highlight that adding penalization
in the cost function, which is usually intended to force unknown functions to have desirable properties, needs
a careful examination when applied.
Broader Impact Statement
There are many potential societal consequences of our work, but we do not believe any of them need to be
specifically highlighted here.
References
Adversarial audio synthesis. International Conference on Learning Representations (ICLR) , 2019.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. Inter-
national Conference on Machine Learning (ICML) , pp. 214–223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in
generative adversarial nets (GANs). International Conference on Machine Learning (ICML) , pp. 224–
232, 2017.
18Under review as submission to TMLR
Sanjeev Arora, andrej Risteski, and Yi Zhang. Do GANs learn the distribution? Some theory and empirics.
International Conference on Learning Representations (ICLR) , 2018.
Yu Bai, Tengyu Ma, and andrej Risteski. Approximability of discriminators implies diversity in GANs.
International Conference on Learning Representations (ICLR) , 2018.
Federico Bassetti, Antonella Bodini, and Eugenio Regazzini. On minimum Kantorovich distance estimators.
Statistics & Probability Letters , 76(12):1298–1302, 2006.
Vidmantas Bentkus. On hoeffding’s inequalities. Annals of probability , pp. 1650–1673, 2004.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural
Information Processing Systems (NeurIPS) , 26, 2013.
Richard Mansfield Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical
Statistics , 40(1):40–50, 1969.
Farzan Farnia and Asuman Ozdaglar. Do GANs always have Nash equilibria? International Conference on
Machine Learning (ICML) , pp. 3029–3039, 2020.
Kilian Fatras, Younes Zine, Rémi Flamary, Rémi Gribonval, and Nicolas Courty. Learning with minibatch
Wasserstein: Asymptotic and gradient properties. International Conference on Artificial Intelligence and
Statistics (AISTATS) , 108:1–20, 2020.
Kilian Fatras, Younes Zine, Szymon Majewski, Rémi Flamary, Rémi Gribonval, and Nicolas Courty. Mini-
batch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792 , 2021.
Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanis-
las Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo Gautheron,
Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony
Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer.
POT: Python Optimal Transport. Journal of Machine Learning Research , 22(78):1–8, 2021. URL
http://jmlr.org/papers/v22/20-451.html .
Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with Sinkhorn divergences.
International Conference on Artificial Intelligence and Statistics (AISTATS) , pp. 1608–1617, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information Processing
Systems (NeurIPS) , pp. 2672–2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved
training of Wasserstein GANs. Advances in Neural Information Processing Systems (NeurIPS) , 30, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANS
trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in Neural Infor-
mation Processing Systems (NeurIPS) , 30, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. IEEE Conference on Compututer Vision and Pattern Recognition (CVPR) , pp.
1125–1134, 2017.
Matt Jacobs and Flavien Léger. A fast approach to optimal transport: The back-and-forth method. Nu-
merische Mathematik , 146(3):513–544, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of GANs. arXiv
preprint arXiv:1705.07215 , 2017.
19Under review as submission to TMLR
Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, An-
drew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-
resolution using a generative adversarial network. IEEE Conference on Compututer Vision and Pattern
Recognition (CVPR) , pp. 4681–4690, 2017.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of
generative adversarial learning. arXiv preprint arXiv:1705.08991 , 2017.
Anton Mallasto, Guido Montúfar, and Augusto Gerolin. How well do WGANs estimate the Wasserstein
metric?arXiv preprint arXiv:1910.03875 , 2019.
TristanMilneandAdrianINachman. WassersteinGANswithgradientpenaltycomputecongestedtransport.
Conference on Learning Theory (COLT) , pp. 103–129, 2022.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for gener-
ative adversarial networks. arXiv preprint arXiv:1802.05957 , 2018.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using
variational divergence minimization. Advances in Neural Information Processing Systems (NeurIPS) , 29,
2016.
Ofir Pele and Michael Werman. Fast and robust earth mover’s distances. IEEE International Conference
on Computer Vision (ICCV) , pp. 460–467, 2009.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of Wasserstein GANs. arXiv
preprint arXiv:1709.08894 , 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolu-
tional generative adversarial networks. arXiv e-prints, arXiv:1511.06434 , 2015.
ScottReed, ZeynepAkata, XinchenYan, LajanugenLogeswaran, BerntSchiele, andHonglakLee. Generative
adversarial text to image synthesis. International Conference on Machine Learing (ICML) , pp. 1060–1069,
2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. Advances in Neural Information Processing Systems (NeurIPS) , 29, 2016.
Filippo Santambrogio. Optimal Transport for Applied Mathematicians . Calculus of Variations, PDEs, and
Modeling. Birkhäuser, October 2015.
Jan Stanczuk, Christian Etmann, Lisa Maria Kreusser, and Carola-Bibiane Schönlieb. Wasserstein GANs
work because they fail (to approximate the Wasserstein distance). arXiv preprint arXiv:2103.01678 , 2021.
lya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Simon-Gabriel Carl-Johann, and Bernhard Schölkopf. Ada-
GAN: Boosting generative models. arXiv e-prints, arXiv:1701.02386 , 2017.
Cédric Villani. Optimal transport: Old and New , volume 338. Springer Science
and Business Media, 2008.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empirical
measures in Wasserstein distance. Bernoulli , 25(4A):2620–2648, 2019.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training of Wasser-
stein GANs: A consistency term and its dual effect. International Conference on Learning Representation
(ICLR), 2018.
Yujia Xie, Xiangfeng Wang, Ruijia Wang, and Hongyuan Zha. A fast proximal point method for computing
exact Wasserstein distance. In Uncertainty in Artificial Antelligence , pp. 433–453. PMLR, 2020.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N
Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks.
IEEE International Conference on Compututer Vision (ICCV) , pp. 5907–5915, 2017.
20Under review as submission to TMLR
Appendix
A Neural Network Architecture
We use standard convolution neural network (CNN) architecture for the experiments as in Gulrajani et al.
(2017); Wei et al. (2018). For MNIST and Fashion MNIST, the sigmoid function is utilized for the final
activation in the generator, while Tanh is used for color images, CIFAR-10, and CelebA-HQ.
Discriminator
Input: 1*32*32
4*4 conv. 256, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 512, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 1024, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 1, Pad = 0, Stride = 1
Reshape 1024*4*4 (D)
Sigmoid
Generator
Input: Noise z 100
4*4 deconv. 1024, Pad = 0, Stride = 1, BatchNorm2d, ReLU
4*4 deconv. 512, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 256, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 1, Pad = 1, Stride = 2, BatchNorm2d, ReLU
Table 3: Neural network Architectures for MNIST and Fashion-MNIST
Discriminator
Input: 1*32*32
4*4 conv. 256, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 512, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 1024, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 1, Pad = 0, Stride = 1
Reshape 1024*4*4 (D)
Tanh
Generator
Input: Noise z 100
4*4 deconv. 1024, Pad = 0, Stride = 1, BatchNorm2d, ReLU
4*4 deconv. 512, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 256, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 1, Pad = 1, Stride = 2, BatchNorm2d, ReLU
Table 4: Neural network Architectures for CIFAR-10
21Under review as submission to TMLR
Discriminator
Input: 3*128*128
4*4 conv. 256, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 512, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 1024, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 2048, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 4096, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 1, Pad = 0, Stride = 1
Reshape 4096*4*4 (D)
Tanh
Generator
Input: Noise z 100
4*4 deconv. 4096, Pad = 0, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 2048, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 1024, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 512, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 256, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 1, Pad = 1, Stride = 2, BatchNorm2d, ReLU
Table 6: Neural network Architectures for CelebA-HQ ( 128×128×3)
Discriminator
Input: 3*64*64
4*4 conv. 256, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 512, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 1024, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 2048, Pad = 1, Stride = 2, lReLU 0.2
4*4 conv. 1, Pad = 0, Stride = 1
Reshape 2048*4*4 (D)
Tanh
Generator
Input: Noise z 100
4*4 deconv. 2048, Pad = 0, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 1024, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 512, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 256, Pad = 1, Stride = 2, BatchNorm2d, ReLU
4*4 deconv. 1, Pad = 1, Stride = 2, BatchNorm2d, ReLU
Table 5: Neural network Architectures for CelebA-HQ ( 64×64×3)
B Additional experiments
We also test that CoWGAN performs well in a generative setting with high-dimensional data, center-cropped
128×128CelebA-HQ, which is shown in Fig. 12. The learning rate is set g= 3dwithd= 0.00001whered
denotes the learning rate of the discriminator, and the neural network architecture is described in Table 6.
22Under review as submission to TMLR
Figure 12: Real (top) and synthetic (bottom) data generated by CoWGAN algorithm using 128×128center-
cropped CelebA dataset.
23