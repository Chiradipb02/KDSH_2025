Published in Transactions on Machine Learning Research (05/2024)
Uncovering Sets of Maximum Dissimilarity
on Random Process Data
Miguel de Carvalho miguel.decarvalho@ed.ac.uk
School of Mathematics, University of Edinburgh, UK
CIDMA, Universidade de Aveiro, Portugal
Gabriel Martos gmartos@utdt.edu
Departamento de Matemática y Estadística, Universidad Torcuato Di Tella, Argentina
Reviewed on OpenReview: https: // openreview. net/ forum? id= ntWCJrlDD8
Abstract
The comparison of local characteristics of two random processes can shed light on periods
of time or space at which the processes differ the most. This paper proposes a method that
learns about regions with a certain volume, where the marginal attributes of two processes
are less similar. The proposed methods are devised in full generality for the setting where
the data of interest are themselves stochastic processes, and thus the proposed method can
be used for pointing out the regions of maximum dissimilarity with a certain volume, in
the contexts of point processes, functional data, and time series. The parameter functions
underlying both stochastic processes of interest are modeled via a basis representation,
and Bayesian inference is conducted via an integrated nested Laplace approximation. The
numerical studies validate the proposed methods, and we showcase their application with
case studies on criminology, finance, and medicine.
1 Introduction
Everydaymillionsofdatapatternsflowaroundtheworldatunprecedentedspeed,thusleadingtoanexplosion
on the demand for modeling stochastic process data—such as point processes, time series, and functional
data; each of these types of data plays a key role in machine learning, as can be seen, for instance, from the
recent papers of Berrendero et al. (2020), Faouzi & Janati (2020), and Xu et al. (2020). Hand in hand with
this shock on demand arrived a pressing need for the development of data-intensive methods, techniques,
and algorithms for learning and comparing random processes.
Problem . We deal with the following general problem on the comparison of stochastic processes:
For a pair of random processes, what is the region—with a given volume—where they sta-
tistically differ the most?
Since the target of interest consists of a set that fulfils an optimization criterion—i.e. a period of time or
region over which a marginal feature of two stochastic processes differ the most—some of the main concepts
in this paper can be framed as an optimization problem over a set function. Unlike standard functions that
assign a number to another number, set functions assign a value to each set. For example, a probability
measureF(A) =Pr(A), whereAis an event (say, A= [0,∞)), calculates the probability of that event;
another example is the counting measure F(A) = #A, which simply counts the number of elements in a
set (for instance, A={1,2,3}). As these examples illustrate, the sets of interest can be either discrete or
defined on a continuum. For the purposes of this paper, since the goal is to identify a region with a given
volume, we will focus on sets defined on a continuum.
1Published in Transactions on Machine Learning Research (05/2024)
The canonical problem in set function optimization is of the type,
max
A⊆AF(A)
s.t.A∈F,(1)
whereF:P(A)→Ris a set function, Ais a set,P(A)denotes the powerset of set A, andF⊆P(A)
is the collection of feasible sets defining the constraint. Optimization problems over set functions—such
as (1)—are commonplace in machine learning (e.g., Krause, 2010). For a recent review on the theory of
discrete set function optimization see Wu et al. (2019). Most state of the art developments have been made
ondiscreteorcombinatorial set function optimization , especially on the class of submodular functions (e.g.,
Goldengorin, 2009). As a byproduct, our paper provides one of the first steps towards continuous set function
optimization as here the aim will be to solve (1) when Fis a family of Borel subsets of a compact set T⊂Rd.
As it will be seen in Section 2, the objective set function of interest in our case will be a measure of proximity
between marginal features of the pair of stochastic processes of interest, whereas the collection of feasible
sets introduces the constraint on the ‘size’ of the feasible regions—i.e. periods of time or space—over which
the comparison is made.
Main contributions :
1. We pioneer the study, formulation, and analysis of the learning problem of tracking down sets of
maximum discrimination as described above—and formally defined in Sections 2–3.
2. Initsmoststandardversion, theproposedlearningproblemisshowntobeequivalenttoacontinuous
set function optimization on a monotone modular function, under a Lebesgue measure constraint.
Hence, as a byproduct, the paper contributes to the literature on set function optimization which
is mostly focused on a discrete and combinatorial framework, with a particular emphasis on mono-
tone submodular functions (e.g., Nemhauser et al., 1978; Calinescu et al., 2011; Goldengorin, 2009;
Buchbinder et al., 2017; Buchbinder & Feldman, 2018), under cardinality or matroid constraints.
Little is known on continuous set function optimization, and thus the tools, concepts, and strategies
devised herein can be of further interest elsewhere.
3. Our approach is fully general in the sense that it applies to most random process data (including
functional data, time series, and point processes). The functional parameters of the processes of
interest (say mean functions, volatility functions, or intensity functions) are modelled by composing
an inverse link function with a basis function representation, and Bayesian modeling is conducted
via latent Gaussian models and inference is conducted via INLA (Integrated Nested Laplace Ap-
proximations) (Rue et al., 2009; 2017).
4. An extension of the proposed method applies also to the context where the interest is on comparing
morethanonemarginalfeatureviaamulti-objectiveversionofthesetfunctionoptimizationproblem
of interest.
Related prior work . While the learning problem introduced above is new—and while there are novel
contributions that will arise from our solution to it—there are some recent approaches in the context of
functional data analysis (Ramsay & Silverman, 2002; 2006; Ferraty & Vieu, 2006; Horváth & Kokoszka,
2012) that are tangentially related to it, and that are briefly reviewed below.
Pini & Vantini (2016; 2017) propose an interval testing procedure for functional data that points out specific
differences between functional populations. Also in the context of functional data, Berrendero et al. (2016)
propose a discretization method consisting on learning to choose a finite collection of points in the domain of
a set of functions in order to improve the performance of a functional data classifier. Martos & de Carvalho
(2018) propose a Mann–Whitney type of statistic for functional data to learn about the regions at which
two processes differ the most on aspects related with symmetry. Finally, Dette & Kokot (2021) develop
hypothesis tests for the equivalence of functional parameters in a two sample functional data setup.
2Published in Transactions on Machine Learning Research (05/2024)
(a)
 (b)
 (c)
Figure1: SetsofmaximumdissimilarityforExamples1–3. Thesetsaredepictedusingblackdottedsegments
in the case of mean functions and volatility functions (a, c), and on the top of the box with a black dotted
line in the case of intensity functions (b). Raw data alongside the true and estimated functional parameters
θXandθYare depicted in red and blue respectively; the proposed inferences for the sets will be discussed
in Section 2.2.
Our approach differs from the ones mentioned above in a number of important ways. Perhaps the most
important ones are that: i) here the goal is not to test hypothesis but rather to learn about regions with a
given volume where two processes differ the most; ii) our approach applies to random processes in general
whereas the methodologies reviewed above have mainly been designed with the context of functional data
analysis in mind.
Outline . In Section 2 we introduce sets of maximum dissimilarity, present examples, and introduce the
inference methods, whereas in in Section 3 we comment on extensions. In Section 4 we showcase the proposed
methods on real data applications. Closing remarks are given in Section 5. Proofs and numerical experiments
with artificial data are presented in the Appendix. Table 1 in the Appendix lists the main symbols and
notation used throughout the article. We write X(t)instead ofXtwhen typographically convenient.
2 Learning About Sets of Maximum Dissimilarity
2.1 Sets of Maximum Dissimilarity
Instances
Prior to introducing sets of maximum dissimilarity in a formal fashion, we introduce some simple instances of
the concept based on different parameter functions θXandθY. The respective sets of maximum dissimilarity
and parameter functions for Examples 1–3 to be presented below are depicted in Fig. 1. The ‘size’ of these
sets (c >0) should be specified by the user based on the desired level of granularity for the analysis; this
enables tracking of maximum differences on a weekly, monthly, or annual basis, for example. This will
become clearer through the examples below and the data applications in Section 4.
Example 1 (Mean Functions) .LetXt=θX(t) +εX(t)andYt=θY(t) +εY(t)be two stochastic processes
defined on the unit interval, T= [0,1], with
/braceleftigg
θX(t) =E(Xt) =b(t) + 4 cos(10 t)−2(t−0.75)2,
θY(t) =E(Yt) =b(t) + 3 sin(12 t),(2)
whereb(t) = 1/2 exp{10(t−0.5)2}is a baseline curve, and εX(t)andεY(t)are zero mean Gaussian error
functions. Fig. 1(a) shows the connected set of size c= 0.2where both mean functions differ the most in
theL2sense.
3Published in Transactions on Machine Learning Research (05/2024)
Example 2 (Intensity Functions) .Consider the intensity functions associated to two point processes
/braceleftigg
θX(t) =λX(t) =γexp{−(t2
1+t2
2)/2},
θY(t) =λY(t) =δθX(t),(3)
defined over the region T= [−3,3]2withγ,δ > 0. Fig. 1(b) shows the true ball of maximum dissimilarity
with area 4πat which the two intensity functions differ the most in the L2sense in the case where γ= 100
andδ= 1/2.
Example 3 (Volatility Functions) .SupposeXtandYtare the log returns of two stock markets, with
E(Xt) =E(Yt) = 0, and that the goal is to search for the period of about a quarter (think of T= [0,1], so
thatc= 0.25), where the volatility between both markets differed the most. Let
/braceleftigg
θX(t) =σX(t) :={var(Xt)}1/2,
θY(t) =σY(t) :={var(Yt)}1/2.
Fig. 1(c) depicts a simulated example of two artificial stock prices evolving during a period of 1 year ( T=
[0,1]), along with the 3 month period over which the volatilities in both markets differed the most in the L2
sense.
To allow for visualizations, in the examples above we focused on low-dimensional instances but the theory
to be presented next holds in general for any compact ground set T⊂Rd.
Regions of maximum dissimilarity
LetθX≡θX(t)andθY≡θY(t)be functional parameters characterizing qdifferent marginal features of the
processesXandYfort∈T⊂Rd. Throughout, we will assume that the ground set Tis compact and that
θXandθYlive in the Banach space (Lp(T),∥·∥p), with∥f∥p= (/integraltext
T|f|pdµ)1/p, whereµis a measure over
the Borel sets on T; we will refer to∥f∥(A)
p= (/integraltext
A|f|pdµ)1/pas theLpsub–norm over A⊆T. We equip the
setAof compact and convex subsets of the ground set T, with the Hausdorff distance
dH(A,B) = max/braceleftbigg
max
t∈Ad(t,B),max
t∈Bd(t,A)/bracerightbigg
, A,B∈A.
whered(t,B)is the minimum Euclidean distance from the point t∈Ato the setB.
A compact and convex set A⊆Tis said to be a region of maximum dissimilarity (in theLpsense),with
volume bounded by c>0, if it maximizes F(A) =∥θX−θY∥(A)
pand if its volume, |A|, does not exceed c; the
formal definition is as follows.
Definition 1 (LpRegion of Maximum Dissimilarity) .SupposeθX,θY∈Lp(T)andT⊂Rdis compact. A
region of maximum dissimilarity (RMD) is defined as a set A∗
c⊆Tthat solves,
max
A⊆T∥θX−θY∥(A)
p
s.t.|A|≤c,
Ais compact and convex ,(4)
for a fixedc≥0. In addition, D∗
c(θX,θY) =∥θX−θY∥(A∗
c)
pis said to be the dissimilarity index.
The optimization problem in (4) is a continuous set function optimization problem similar to (1), with


F(A) =∥θX−θY∥(A)
p,
A=T,
Fc={A:|A|≤c,Ais compact and convex }.
Clearly,F(A)is an increasing set function, that is, if A⊆BthenF(A)≤F(B). Also,F(A)is modular (or
additive) in the sense that F(A) +F(B) =F(A∪B) +F(A∩B). While it is evident that A∗
cexists for a
few specific examples (say, A∗
|T|=Talmost everywhere, if Tis convex), the existence of A∗
cin general is not
straightforward and it is proved below in Theorem 1. RMDs and dissimilarity indices also have a number of
attributes which we summarize over Theorem 1.
4Published in Transactions on Machine Learning Research (05/2024)
Theorem 1. SupposeθX,θY∈Lp(T)andT⊂Rdis compact. The quantities A∗
candD∗
cobey the following
properties:
1.A∗
cexists, for every c≥0.
2. Suppose µ(A) =|A|. Then, the RMDs of (KθX,KθY)and(LθX,LθY)are respectively AK
c=A∗
c
andAL
c=A∗
α+βc, whereKθ(t) =α+βθ(t),Lθ(t) =θ(α+βt),α∈R, andβ̸= 0.
3.D∗
c(θX,θY)is a distance over Θc={θ|A:θ∈Lp(T),A∈Fc}, forc>0.
4.D∗
c(θX,θY)is non-decreasing as a function of c, for fixedθX,θY∈Lp(T).
Theorem 1 warrants some comments. The existence of RMDs (Claim 1) follows from the continuity of the
volume functional, |A|, and the compactness of the search domain, Fc; see Appendix A. An RMD needs
not however to be unique, as can be easily seen by considering the limiting case θX=θYfor which every
compact and convex subset of Twith measure cis an RMD. Claim 2 shows how RMDs are impacted by
a group of linear transformations acting either on the functional parameter or over time; the assumptions
µ(A) =|A|andβ̸= 0are only required for AL
c=A∗
α+βc, as indeed AK
c=A∗
cholds more generally. Claim 3
implies that dissimilarity indices have a metric interpretation. Finally, Claim 4 notes that D∗
c=D∗
c(θX,θY)
cannot decrease as cincreases.
For practical considerations entailed in the estimation of A∗
cfrom data (more details in Section 2.2), next
we introduce the Lpballs of maximum dissimilarity as a suitable parametric approximation to RMDs.
Balls of maximum dissimilarity
LetBpbe the family of all closed balls in Lp(T), forp≥1, that is,
Bp={B(t,r) :r>0,t∈T},
whereB(t,r) ={s:∥s−t∥p≤r}; to ease notation we drop the dependence of B(t,r)onp. A more
parsimonious option for modeling and computing is to put more structure on the shape of RMDs, and this
leads to the next definition.
Definition 2 (LpBall of Maximum Dissimilarity) .SupposeθX,θY∈Lp(T)andT⊂Rdis compact. An
Lpball of maximum dissimilarity (BMD) is defined as a set B∗
c⊆Tthat solves,
max
B⊆T∥θX−θY∥(B)
p
s.t.|B|≤c
B∈Bp,(5)
for a fixedc≥0.
As in Definition 5 we refer to D∗
c=D∗
c(θX,θY) =F(B∗
c)as the dissimilarity index for BMDs. Since the
volume of an Lp(T)ball of radius rinRdis
|B(t,r)|={2rΓ(1/p+ 1)}d
Γ(d/p+ 1),
for allt∈T, where Γ(z) =/integraltext∞xz−1e−xdxis the Gamma function, it follows that the volume constraint on
BMDs,|B|≤c, can be rewritten as a function of the radius, that is
r≤{cΓ(d/p+ 1)}1/d
2Γ(1/p+ 1)≡Rc. (6)
Equation (6) is computationally convenient and will be revisited in further detail in Section 2.2.
Given the similarities between the definitions of RMDs and BMDs it is not surprising that they have identical
properties (e.g., for c >0,D∗
c(θX,θY)is a also distance and D∗
cis non-decreasing). More generally, the
following theorem holds:
5Published in Transactions on Machine Learning Research (05/2024)
Theorem 2. SupposeθX,θY∈Lp(T)andT⊂Rdis compact. The quantities B∗
candD∗
cobey the following
properties:
1.B∗
cexists, for every c≥0.
2.D∗
cis continuous, and the “argmax” correspondence of center–radius, αc: [0,∞)↠T×[0,Rc],
defined asαc={(t,r)∈T×[0,Rc] :f(t,r) =D∗
c}, is upper hemicontinuous for every c≥0, where
f(t,r) =F{B(t,r)}.
Theorem 2 proves the existence of BMDs and notes that the smoothness of Rcis inherited by D∗
c.
Next, we switch gears and focus on learning about BMDs from data.
2.2 Learning About BMDs from Data
Computing and optimization
As anticipated earlier, BMDs are computationally convenient as a suitable parametric approximation to
RMDs. This becomes apparent from (5) as finding an optimal ball corresponds to a standard constrained
optimization problem, where the objective function depends solely on the center–radius vector with d+ 1
components. Inaddition, (6)highlightsanothercomputationaladvantageofmodelingRMDsviaBMDs: The
volume constraint can be directly expressed as a function of one of the arguments of the objective function,
specifically the radius. To put it differently, the key takeaway from (5) and (6) is that the set function
constrained optimization problem leading to BMDs can actually be formulated as a standard continuous
optimization problem over T×[0,Rc]⊂Rd+1. Indeed, it follows from (5) and (6) that computing a BMD
is equivalent to solving
max{f(t,r) : (t,r)∈T×[0,Rc]}, (7)
wheref(t,r) =F{B(t,r)}. When the center of the optimal BMD ( t∗
c) is ‘sufficiently far’ from the boundary
ofT, the optimal radius ( r∗
c) isRcin (6). That is, when d(t∗
c,∂T)> Rcthe optimal radius is r∗
c=Rc(as
f(t,r)is a non-decreasing function of r) in which case the optimization problem in (7) resumes to searching
fort∗
c∈Rd. This also implies that often in practice the marginal posterior of r∗
cis essentially degenerated.
Latent Gaussian model specification
To model BMDs in applications we consider a version of the latent Gaussian model specification in Rue
et al. (2009) adapted to our setup; to ease notation, below we only refer to X(t), and denote its functional
parameter by θ(t)≡θX(t), but all comments apply to Y(t)as well. A latent Gaussian model is essentially a
Bayesian generalized additive model that assigns Gaussian priors to parameters and a possibly non-Gaussian
prior to its hyperparameters. Specifically, suppose that Z(t) =h{X(t)}is in the exponential family, with
its mean function coinciding with the functional parameter, and that
θ(t) =g/parenleftbigg
β0+B/summationdisplay
i=1βiϕi(t)/parenrightbigg
. (8)
Here{ϕi≡ϕi(t)}B
i=1is a set of basis functions in Lp(T),β= (β0,...,βB)Tis a parameter, and gis an inverse
link function. Following Rue et al. we assign a multivariate Normal prior with a sparse precision matrix ( Q)
toβ, which induces a Gaussian process prior on g−1(θ(t))with a conditional independence property. Many
functional parameters can be modeled in this way including those from Examples 1–3 and all numerical
instances in Section 4 and Appendix C. The theoretical developments from Section 2.1 apply however more
generally beyond the modeling assumptions made over this section.
INLA-based inference for balls of maximum dissimilarity
We now discuss how to conduct inference for balls of maximum dissimilarity. It is well-known that the latent
Gaussian model described above can be fitted with an Integrated Nested Laplace Approximation (INLA)
6Published in Transactions on Machine Learning Research (05/2024)
(Rue et al., 2009); the method is effective even when the dimension of the precision matrix Qis large, and
is particularly tailored for the case where the number of hyperparameters, α, is moderate (say, 6–12). INLA
is a deterministic method for approximating the marginal posterior of each parameter that is based on the
Laplace approximation; loosely speaking, the Laplace approximation is an approximation for integrals of the
type/integraltext
e−nf(y)dyfor largen, that approximates the integrand ( e−nf(y)) with a Gaussian density centered at
its mode and sets the covariance matrix as the inverse of the curvature (around the mode); see, for instance,
Young & Smith (2005, Section 9.7). Below, we sketch some brief details on INLA. The first step of INLA
approximates the marginal posterior of αvia the Laplace approximation, that is,
p(α|data) =p(α,β|data)
p(β|α,data)≈p(α,β|data)
/tildewidep(β|α,data)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
β=β∗α, (9)
where/tildewidep(β|α,data)is the Gaussian approximation based on the mode of the full conditional of β, and where
βαis the mode of this approximated full conditional for a given α. Next, INLA approximates the marginal
posterior of each component of β. Letβ−ibe the elements of β, exceptβi. Similarly to (9) it follows that
p(βi|α,data)∝p(α,β|data)
p(β−i|α,βi,data), (10)
which can be approximated using a Laplace approximation for p(β−i|α,βi,data). Finally, the marginal
posterior density p(βi|data)is obtained by numerically integrating out α. Below, INLA is applied to the
data from processes XandYasDXandDY. Estimation and inference for BMDs can be conducted using
Algorithm 1 that combines the deterministic nature of INLA along with sampling.
Algorithm 1 INLA-based posterior inference for BMDs
Step 1: Fit the marginal posterior densities, p(βX,i|DX)andp(βY,i|DY), using the Integrated Nested
Laplace Approximation, for i= 0,...,BXandj= 0,...,BY.
Step 2: Samplemposterior draws from the full posterior of βX= (βX,0,...,βX,BX)andβY=
(βY,0,...,βY,BY), given by
β(1)
X,...,β(m)
Xiid∼p(βX|DX), β(1)
Y,...,β(m)
Yiid∼p(βY|DY),
to generate mposterior trajectories from the functional parameters using (8); that is, for k=
1,...,m, do
θ(k)
X(t) =g/parenleftbigg
β(k)
X,0+BX/summationdisplay
i=1β(k)
X,iϕi(t)/parenrightbigg
, θ(k)
Y(t) =g/parenleftbigg
β(k)
Y,0+BY/summationdisplay
j=1β(k)
Y,jϕj(t)/parenrightbigg
. (11)
Step 3: Use the posterior trajectories from Step 2 to obtain a sequence of posterior BMD draws for
{B(k)∗
c≡B(t(k)∗
c,r(k)∗
c)}m
k=1,
with (t(k)∗
c,r(k)∗
c)solving the optimization problem in (7), for k= 1,...,m.
Algorithm 1 warrants some comments. Step 1 is deterministic, it follows by numerically integrating out
the hyperparameters in (10), and to facilitate its implementation we recommend using the R-INLApackage
(Martins et al., 2013; Lindgren et al., 2015) from R(R Development Core Team, 2022), which is also equipped
with routines that facilitate the implementation of Step 2 (e.g., inla.posterior.sample ). Step 3 boils down
to solving the optimization problem in (7) using the pair of posterior trajectories in (11) ; thus we solve (7)
per each INLA iteration and the posterior quantifies the uncertainty surrounding the optimal centre and
radius. Except where mentioned otherwise, in all experiments reported below, we draw m= 1 000 times
from the posterior distribution of BMDs using Algorithm 1.
7Published in Transactions on Machine Learning Research (05/2024)
3 Multi-Objective RMDs
We now consider the context where the interest is on learning about regions over which a set of marginal
features of two processes most differ. Let
/braceleftigg
θX= (θX,1,...,θX,q) = (θX,1(t),...,θX,q(t)),
θY= (θY,1,...,θY,q) = (θY,1(t),...,θY,q(t)),
be functional parameters characterizing marginal features of these processes. In this section
Fi(A) =∥θX,i−θY,i∥(A), (12)
denotes the set objective functions of interest, for i= 1,...,q. Ideally, we would aim to simultaneously
maximize all targets (F1(A),...,Fq(A)). Yet, in practice targets may conflict each other, that is, if one is
increased some other may be decreased. The following Pareto optimal concept induces an order over the
collection of feasible sets Fc, and it defines optimality via a compromise across all objective set functions,
in the sense that improvements on one target cannot be made at the cost of deteriorating another target.
Definition 3 (Pareto Optimal Region of Multi-maximum Dissimilarity) .LetFi(A)be defined as in (12).
The setA∗
c∈Fcis a Pareto optimal region of multi-maximum dissimilarity if there exists no other set
A∈Fcsuch thatFi(A)≥Fi(A∗
c), for alli∈{1,...,q}, andFi(A)>Fj(A∗
c), for at least one j∈{1,...,q}.
In common with standard theory for multi-objective function, in our set function context the set of Pareto
optimal RMMDs can be analytically obtained only in very specific cases, and thus we need to resort to
scalarization(Pardalosetal.,2017, Ch.2)toreduceamulti-objectiveproblemintoasingle-objectiveproblem.
Here, we define and characterize the following linear scalarization method for our set function context
Fw(A) =q/summationdisplay
i=1wiFi(A), (13)
w= (w1,...,wq)∈(0,∞)q, and we will refer to
A∗
w,c= arg max
A∈FcFw(A), (14)
as the solution to the set function linear scalarization problem with weight w.
Theorem 3. Every solution to the linear scalarization problem is a Pareto optimal RMMD.
Section 4.2 illustrates how Theorem 3 can be applied in practice.
4 Empirical Section
In Appendix C we assess the performance of the proposed tools via a Monte Carlo study. Next, we showcase
real data applications.
4.1 Thefts in Buenos Aires
Buenos Aires is the most dense metropolis in Argentina and its crime rates are significantly higher in compar-
ison to the rest of the country. In this section we illustrate how the proposed method can reveal regions of the
city where nonviolent crimes—such as burglary, pickpocketing or nonviolent thefts—have changed the most,
comparing the years 2019 (pre COVID-19) and 2020 (when the COVID lockdown took place during several
months). The data are publicly available online in the city hall web page ( https://data.buenosaires.gob.ar ),
and consist of point process data on the latitude and longitude where thefts occurred during 2019 ( D2019)
and 2020 (D2020). Here, the functional parameters of interest are the intensity functions
θ2019(latitude,longitude ), θ 2020(latitude,longitude ),
8Published in Transactions on Machine Learning Research (05/2024)
and its BMD will represent region of the city, of a given size, where the most noteworthy changes in thefts
took place. The fitted BMDs were modeled according to (8) using a log-Gaussian Cox process with a similar
prior specification as in Appendix C. In Fig. 2(a) we depict the estimated BMD corresponding to an area
of8km2along with an heat map of the differences in the estimated posterior intensity functions between
consecutive years; the value of 8km2was chosen for illustration as it corresponds to about twice the size
of the largest neighborhood, which is Palermo. We also depict in Fig. 2(b) an heat map of the posterior
density corresponding to the center of the BMD which shows that these are substantially concentrated, thus
suggesting low uncertainty on the fitted BMD.
(a)
 (b)
(c)
 (d)
Figure 2: (a) Fitted BMD corresponding to c= 8km2along with a contour of pointwise differences between
fitted posterior intensity functions over consecutive years. (b) Posterior density estimation corresponding to
the centers of of the fitted BMD. (c) and (d) shows the fitted intensity functions for 2019 and 2020.
To clarify the applied meaning of such BMDs we provide some additional background on the social context
surrounding the empirical analysis. In Fig. 2(c–d) we depict the fitted intensity functions corresponding to
both years. As can be seen from Fig. 2, in 2019 and 2020 thefts were more likely to occur in APRV (Almagro,
Palermo, Recoleta, and Villa Crespo) which are some of the neighborhoods where several commercial and
touristic activities took place. Yet, important differences on the estimated intensity functions are perceived
betweenbothyears. Duringthefirsthalfofyear2020, localauthoritiestookstrongsocialdistancingmeasures
such as the limitation to the access the public transportation system, restrictions on business and commerce
9Published in Transactions on Machine Learning Research (05/2024)
during the day, limitations on gatherings and tourism activities, restriction to the capacity in bars and
restaurants, among others. The difference on the estimated intensity functions between consecutive years
evident from Fig. 2—and the implied reduction of thefts over 2020—is in line with the findings of Mohler
et al. (2020) that report similar evidence on the effect of COVID-19 lockdown and social distance policies
in nonviolent crime. The BMD in Fig. 2(a) suggests that APRV (Almagro, Palermo, Recoleta, and Villa
Crespo) are the neighborhoods where there was a most impactful effect of COVID-19 lockdown. To put it
differently, while nonviolent crime has decreased during lockdown over the entire city, what the fitted BMD
highlights is that such reduction was relatively much higher in the APRV neighborhoods.
4.2 Volatility in Stock Markets
Our second illustration will shed light on the multi-objective approach from Section 3. Data were gathered
from Yahoo Finance and consist of monthly values of the NYSE and NASDAQ composite indices from the
New York Stock Exchange ( {Xt}) and the NASDAQ Stock Exchange ( {Yt}), respectively. The data ranges
from January 1990 to May 2021, thus covering a variety of episodes of financial turbulence such as the
dotcom tech bubble that peaked around 2000, the subprime crisis that started around 2007, and the recent
COVID–19 global pandemic. In the multi-objective RMD analysis to be conducted here, we will consider
two functional parameters of interest: The mean values of the indices over time and the volatility of their
log returns, that is,
/braceleftigg
mX(t) =E(Xt),
mY(t) =E(Yt),/braceleftigg
σX(t) = [E{log(Xt/Xt−1)2}]1/2,
σY(t) = [E{log(Yt/Yt−1)2}]1/2.
These functional parameters were modeled according to (8), using an identity link function and B-spline
basis functions, choosing the number of basis functions using the DIC (Deviance Information Criterion;
Spiegelhalter et al. 2002; 2014), and using an uninformative prior.
We consider intervals of six months, one year, and two years (corresponding to c= 6,12, and 24) and we
aim to evaluate on what periods of such length these two stock markets differed the most—in terms of both
average returns as well as volatility. We thus seek for the interval of time B∗
c= [t∗
c−r∗
c,t∗
c+r∗
c]that as in
(14) maximizes the following scalarized set function optimization problem,
max{Fw{B(t,r)}: (t,r)∈T×[0,Rc]}, (15)
wherewis the scalarization parameter and
Fw{B(t,r)}=w/integraldisplayt+r
t−r|mX(u)−mY(u)|du+ (1−w)/integraldisplayt+r
t−r|σX(u)−σY(u)|du.
It follows from Theorem 3 that every solution to the linear scalarization problem (15) is a Pareto optimal
BMMD (ball of multi-maximum dissimilarity), and hence Pareto optimal BMMDs obtained by linear scalar-
ization have the nice feature of allowing one to put more emphasis on the mean values or on volatilities
according to how we set w. That is, by setting w= 0orw= 1, we only consider volatilities or mean
values respectively and the analysis corresponds to a standard BMD, while for w∈(0,1)absolute values in
differences between mean functions are more important than those in volatilities as wincreases. In terms
of computing we adapt Algorithm 1 to the multi-objective context. That is, inference about the BMMDs
is conducted by sampling m= 1 000times from the posteriors for means and volatilities—and rather than
solving (7) as in Algorithm 1—we now solve the scalarized set function optimization problem in (15).
In Fig. 3 we compare the results of the single BMD analysis [panels (a) to (d)] versus the multi-objective
analysis [panels (e) and (f)] considering a value of ccorresponding to a period of six months, one year, and
two years. As can be seen from Fig. 3(a–b), the BMD associated with the mean ( w= 1) concentrates around
the subprime crisis, indicating that July 2006 to July 2008 is the period over which mean levels of NYSE
and NASDAQ differed the most. In Fig. 3(c–d), we see that the BMD associated with volatility ( w= 0)
ranges from October 1999 to October 2001—which corresponds to the dotcom bubble burst. Finally, the
multi-objective approach with w= 2/3is depicted in Fig. 3 (e–f) and it suggests that volatility has a greater
10Published in Transactions on Machine Learning Research (05/2024)
BMD for mean (w= 1)
(a)
 (b)
BMD for volatility (w= 0)
(c)
 (d)
Multi-objective BMD for mean–volatility (w= 2/3)
(e)
 (f)
Figure 3: (a, c, e) Black solid segments corresponds to fitted multi-objective BMDs for six monts, one year,
and two years for w= 1,w= 0, andw= 2/3; the charts also show the realized (solid) and estimated
(dotted) volatilities corresponding to NYSE (red) and NASDAQ (blue). (b, d, f) Marginal posterior density
for the centre of the BMD.
control on the objective function in (15); that is, even when we set w= 2/3—that is, even when we set
more emphasis on the differences in means rather than the differences in volatility—we still get a similar
11Published in Transactions on Machine Learning Research (05/2024)
result as setting w= 0, as we end up recovering the period of the dotcom bubble burst as can be seen from
Fig. 3(e–f).
4.3 Electrocardiogram Data (ECG200)
For our final illustration we use the ECG200 dataset contributed by Olszewski (2001). The data is the
result of monitoring electrical activity recorded during one heartbeat and it consists of 200 ECG signals
sampled at 96 time instants, corresponding to 133 normal heartbeats ( DX) and 67 myocardial infarction
signals (DY); the data are publicly available from the UCR Time Series Classification and Clustering website
(http://www.cs.ucr.edu/ ∼eamonn/time_series_data_2018/ ). In this illustration, the functional parameters
of interest are the mean functions of ECG signals for both classes (normal heartbeat θX(t) = E(Xt), and
myocardial infarction θY(t) = E(Yt)) and one of the goals of the analysis is to track down periods, of a
given length, over a cardiac cycle where the differences between the two classes is most pronounced. To
model these functional parameters the Gaussian process prior specification in (8) was once more applied
using an identity link, B-spline basis functions, the DIC to select the number of basis functions, and a
Matérn covariance function with the PC prior of Fuglstad et al. (2019) setting P(σ > 1) = 0.001and
P(ℓ<0.05) = 0.001.
(a)
 (b)
Figure 4: ECG200 dataset: (a) Black solid segments correspond to fitted BMDs, /hatwideB∗
10⊂/hatwideB∗
20⊂/hatwideB∗
30; the chart
also shows the raw data (dots) and fitted mean functions (dashed) along with 95%credible bands, where
red and blue respectively correspond to normal heartbeats and myocardial infarction signals. (b) Marginal
posterior density for the centre of the set of maximum dissimilarity B∗
10.
In Fig. 4(a), we depict the fitted posterior estimates for a sequence of BMDs ( /hatwideB∗
10,/hatwideB∗
20, and/hatwideB∗
30) using
black solid segments, along with the raw data, and the fitted mean functions with 95% credible bands. The
obtained BMDs uncover periods of a given length where we observe largest differences between the estimated
mean ECG functions. Informally, we may think of such BMDs as corresponding to intervals of about 10–30
deciseconds, since the 96 time instants cover about a cardiac cycle for all subjects and thus are not expected
to last longer than 1 second. The fitted BMD centers are localized around the time instants 22–55. All in
all, the analysis suggests that while normal heartbeats and myocardial infarction signals have similar ‘peaks’
at the beginning of the sample period (i.e. they have similar Q waves, in ECG signal analysis terminology),
immediately in the period right after (i.e. over their ST segments) they greatly differ.
We close the analysis with two final remarks. First, in this illustration the fitted BMDs verify the chained
inclusions/hatwideB∗
10⊂/hatwideB∗
20⊂/hatwideB∗
03, but this property does not hold in general—nor for the fitted BMDs, nor
for the true ones; counterexamples can be constructed either numerically or analytically. Second, although
BMDs are unrelated to classification, since the ECG200 dataset is a popular benchmark for new classifiers it
12Published in Transactions on Machine Learning Research (05/2024)
may be sensible to ask whether the accuracy of some classifiers at discriminating outcome classes (diseased–
nondiseased) can be improved by focusing on BMDs rather than treating the entire time horizon equally; we
leave such open problem for future analysis.
5 Discussion
Sets of maximum dissimilarity and their variants are here proposed as a tool for acquiring knowledge on the
region with a given size where two stochastic processes differ the most. The proposed learning problem is
shown to be equivalent to a continuous set function optimization on a monotone modular function, under
a Lebesgue measure constraint. The existence of the proposed sets of maximum dissimilarity is nontrivial
but we prove their existence, and illustrate with artificial and real data that it only requires a moderate
computational investment to learn them from data. The proposed methods are developed in full generality
for the setting where the data of interest are themselves stochastic processes, and thus the proposed toolbox
can be used for unveiling the regions of maximum dissimilarity with a given volume, for a variety of random
process data. All modeling was framed within a latent Gaussian framework, with inference being conducted
usingtheIntegratedNestedLaplaceApproximation; clearly, othercomputationalapproachescouldhavebeen
employed as well, including, for example, variational inference (Blei et al., 2017). A multi-objective version
of the proposed framework is also devised to learn about multi-objective RMDs, where several functional
parameters are considered—each characterizing a specific feature of the processes being compared.
While the theoretical developments from Section 2.1 establish the existence of general compact and convex
sets of maximum dissimilarity, BMDs turn out be a much convenient simplification for a variety of reasons.
First, numerical optimization is much more challenging with general RMDs, whereas for BMDs it is relatively
simple as can be seen from (7). Second, the inference for general RMDs would entail averaging posterior
simulated RMDs—that is, averaging sets—whereas for BMDs it suffices to compute the posterior mean of
the(d+ 1)-dimensional centre-radius pair.
Some final comments on future research are in order. Firstly, while the paper focuses on scenarios where the
user setsc, this approach could be integrated into a decision theory framework; although it remains an open
problem for future analysis, Appendix E provides a hint on how this issue could be addressed. Secondly, this
paper pioneers and introduces sets of maximum dissimilarity, which have been designed to highlight potential
differences. Yet, in cases where {Xt}={Yt}, they may identify spurious regions. Assessing the statistical
significance of these regions through multiple testing procedures seems conceivable, but the effectiveness
of these approaches in this context requires further theoretical scrutiny and numerical analyses. Finally,
discrete analogues of our problem may also be of interest in their own right, and may present themselves as
a natural option for devising other practical algorithms for approximating continua RMD.
Acknowledgments
We are grateful to the Action Editor and three anonymous reviewers for their suggestions and construc-
tive feedback that have significantly improved the manuscript. We thank Vanda Inácio de Carvalho for
comments and feedback and Finn Lindgren for discussions on INLA. MdC was partially supported by the
Royal Society of Edinburgh and by FCT (Fundação para a Ciência e a Tecnologia, Portugal) under Grants
https://doi.org/10.54499/UIDB/04106/2020 and https://doi.org/10.54499/UIDP/04106/2020.
References
Milton Abramowitz and Irene A Stegun. Handbook of Mathematical Functions . Dover, New York, 1964.
CD Aliprantis and KC Border. Infinite Dimensional Analysis . Springer, New York, 2006.
Yoav Benyamini. Applications of the universal surjectivity of the Cantor set. The American Mathematical
Monthly, 105(9):832–839, 1998.
José R Berrendero, Antonio Cuevas, and José L Torrecilla. Variable selection in functional data classification:
A maxima-hunting proposal. Statistica Sinica , pp. 619–638, 2016.
13Published in Transactions on Machine Learning Research (05/2024)
José R Berrendero, Beatriz Bueno-Larraz, and Antonio Cuevas. On Mahalanobis distance in functional
settings. Journal of Machine Learning Research , 21(9):1–33, 2020.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association , 112(518):859–877, 2017.
Hans W. Borchers. pracma: Practical Numerical Math Functions , 2021. R package version 2.3.6.
Niv Buchbinder and Moran Feldman. Submodular functions maximization problems. In Handbook of Ap-
proximation Algorithms and Metaheuristics, 2nd ed , pp. 753–788. Chapman and Hall/CRC, Boca Raton,
FL, 2018.
Niv Buchbinder, Moran Feldman, and Roy Schwartz. Comparing apples and oranges: Query trade-off in
submodular maximization. Mathematics of Operations Research , 42(2):308–329, 2017.
Gruia Calinescu, Chandra Chekuri, Martin Pal, and Jan Vondrák. Maximizing a monotone submodular
function subject to a matroid constraint. SIAM Journal on Computing , 40(6):1740–1766, 2011.
Holger Dette and Kevin Kokot. Bio-equivalence tests in functional data by maximum deviation. Biometrika ,
108(4):895–913, 2021. ISSN 0006-3444. doi: 10.1093/biomet/asaa096.
Johann Faouzi and Hicham Janati. pyts: A python package for time series classification. Journal of Machine
Learning Research , 21(46):1–6, 2020.
FrédéricFerratyandPhilippeVieu. Nonparametric Functional Data Analysis: Theory and Practice . Springer,
New York, 2006.
Geir-Arne Fuglstad, Daniel Simpson, Finn Lindgren, and Håvard Rue. Constructing priors that penalize the
complexity of Gaussian random fields. Journal of the American Statistical Association , 114(525):445–452,
2019.
Boris Goldengorin. Maximization of submodular functions: Theory and enumeration algorithms. European
Journal of Operational Research , 198(1):102–112, 2009.
Lajos Horváth and Piotr Kokoszka. Inference for Functional Data with Applications . Springer, New York,
2012.
Andreas Krause. SFO: A toolbox for submodular function optimization. Journal of Machine Learning
Research , 11(38):1141–1144, 2010.
Finn Lindgren, Håvard Rue, et al. Bayesian spatial modelling with R-INLA. Journal of Statistical Software ,
63(19):1–25, 2015.
Thiago G Martins, Daniel Simpson, Finn Lindgren, and Håvard Rue. Bayesian computing with INLA: New
features. Computational Statistics & Data Analysis , 67:68–83, 2013.
G. Martos and M. de Carvalho. Discrimination surfaces with application to region-specific brain asymmetry
analysis. Statistics in Medicine , 11(37):1859–1873, 2018.
George Mohler, Andrea L Bertozzi, Jeremy Carter, Martin B Short, Daniel Sledge, George E Tita, Craig D
Uchida, and P Jeffrey Brantingham. Impact of social distancing during COVID-19 pandemic on crime in
Los Angeles and Indianapolis. Journal of Criminal Justice , 68:101692, 2020.
George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maxi-
mizing submodular set functions—I. Mathematical Programming , 14(1):265–294, 1978.
Robert T Olszewski. Generalized feature extraction for structural pattern recognition in time-series data.
Technical report, Carnegie-Mellon University, School of Computer Science, 2001.
Panos M Pardalos, Antanas Žilinskas, and Julius Žilinskas. Non-convex Multi-objective Optimization .
Springer, New York, 2017.
14Published in Transactions on Machine Learning Research (05/2024)
Alessia Pini and Simone Vantini. The interval testing procedure: A general framework for inference in
functional data analysis. Biometrics , 72(3):835–845, 2016.
Alessia Pini and Simone Vantini. Interval-wise testing for functional data. Journal of Nonparametric Statis-
tics, 29(2):407–424, 2017.
R Development Core Team. R: A Language and Environment for Statistical Computing . R Foundation for
Statistical Computing, Vienna, Austria, 2022.
James O. Ramsay and B. W. Silverman. Functional Data Analysis . Springer, New York, 2006.
James O Ramsay and Bernard W Silverman. Applied Functional Data Analysis: Methods and Case Studies .
Springer, New York, 2002.
Håvard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent Gaussian models
by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B
(Statistical Methodology) , 71(2):319–392, 2009.
Håvard Rue, Andrea Riebler, Sigrunn H Sørbye, Janine B Illian, Daniel P Simpson, and Finn K Lindgren.
Bayesian computing with INLA: A review. Annual Review of Statistics and Its Application , 4:395–421,
2017.
Rolf Schneider. Convex Bodies: The Brunn–Minkowski Theory . Cambridge University Press, Cambridge,
2014.
Rolf Schneider and Wolfgang Weil. Stochastic and Integral Geometry . Springer, New York, 2008.
Daniel Simpson, Janine Baerbel Illian, Finn Lindgren, Sigrunn H Sørbye, and Havard Rue. Going off grid:
Computationally efficient inference for log-Gaussian Cox processes. Biometrika , 103(1):49–70, 2016.
David J Spiegelhalter, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. Bayesian measures of
model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 64
(4):583–639, 2002.
David J Spiegelhalter, Nicola G Best, Bradley P Carlin, and Angelika Van der Linde. The deviance infor-
mation criterion: 12 years on. Journal of the Royal Statistical Society: Series B (Statistical Methodology) ,
76(3):485–493, 2014.
Terence Tao. Analysis I . Springer, New York, 2006.
Stefan Waldmann. Topology: An Introduction . Springer, New York, 2014.
Xiaofeng Wang, Yuryan Yue, and Julian J Faraway. Bayesian Regression Modeling with INLA . Chapman
and Hall/CRC, Boca Raton, FL, 2018.
Wei-Li Wu, Zhao Zhang, and Ding-Zhu Du. Set function optimization. Journal of the Operations Research
Society of China , 7(2):183–193, 2019.
Ganggang Xu, Ming Wang, Jiangze Bian, Hui Huang, Timothy R. Burch, Sandro C. Andrade, Jingfei Zhang,
and Yongtao Guan. Semi-parametric learning of structured temporal point processes. Journal of Machine
Learning Research , 21(192):1–39, 2020.
G. A Young and Richard L Smith. Essentials of Statistical Inference . Cambridge University Press, Cam-
bridge, UK, 2005.
15Published in Transactions on Machine Learning Research (05/2024)
A Auxiliary Lemmata
In this section we state some auxiliary results that will be used to prove the main results of this paper.
Lemma 1 (Blaschke selection theorem) .The set of all compact convex subsets of a fixed compact subset of
Rdis compact under the Hausdorff metric.
Lemma 2 (Berge’s maximum theorem) .Letφ :X↠Ybe a continuous corre-
spondence between topological spaces with nonempty compact values, and suppose that
g:Grφ→Ris continuous. Define the “value function” v:X→Rby
v(x) = max{g(x,y) :y∈φ(x)}
and the correspondence α:X↠Yof maximizers by
α(x) ={y∈φ(x) :g(x,y) =v(x)}.
Then:
1. The value function vis continuous.
2. The “argmax” correspondence αhas nonempty compact values.
3. IfYis Hausdorff, then the “argmax” correspondence αis upper hemicontinuous.
Lemma 3 (Product of correspondences theorem) .The product of correspondences obeys the following prop-
erties:
1. The product of a family of upper hemicontinuous correspondences with compact values is upper
hemicontinuous with compact values.
2. The product of a finite family of lower hemicontinuous correspondences is lower hemicontinuous.
Lemma 1 is a celebrated result on convex analysis; the version above can be found, for instance, in Benyamini
(1998). Lemmas 2 and 3 are well-known results on optimization of set-valued functions; the versions above
can be found in Aliprantis & Border (2006, Theorems 17.31 and 17.28).
B Proofs of Main Results
B.1 Proof of Theorem 1
Claim 1 . We start by showing that
Fc={A∈A:|A|≤c}
is compact, for every c≥0, whereAis the family of compact and convex subsets of the ground set T. Recall
that by the Blaschke selection theorem (Lemma 1), Ais compact under the Hausdorff metric. Further, since
the volume functional |·|is continuous (Schneider, 2014, Theorem 1.8.16), and given that Fcis the preimage
of the closed set [0,c], it follows that Fcis a closed subset of A, and hence it is compact.
Observe next that maximizing F(A) =∥θX−θY∥(A)
pis equivalent to maximizing Fp(A), forp>0, and as
we show next Fp(A)is upper semicontinuous under the Hausdorff metric, for every A∈Fc. LetAn→Ain
(Fc,dH), for a fixed c≥0. It can be easily shown that (e.g., Schneider & Weil, 2008, Theorem 12.3.6)
1A(t)≥lim sup
n1An(t), t∈T⊂Rd. (16)
Combining (16) with Fatou’s lemma yields that
Fp(A) =/integraldisplay
T1A(t)|θX(t)−θY(t)|pµ(dt)
≥/integraldisplay
Tlim sup
n1An(t)|θX(t)−θY(t)|pµ(dt)
≥lim sup
n/integraldisplay
T1An(t)|θX(t)−θY(t)|pµ(dt)
= lim sup
nFp(An),
16Published in Transactions on Machine Learning Research (05/2024)
thus showing that Fp(A)is upper semicontinuous under the Hausdorff metric, for every A∈Fc. The
final step of the proof is tantamount to a standard argument used for proving Weierstrass theorem. Let
uc= sup{Fp(A) :A∈Fc}∪{∞}. By definition, for every cthere exists a maximizing sequence An∈Fc
such thatFp(An)→uc. By compactness, we can assume that An→A∗
c. Upper semicontinuity of Fp(A)
implies that uc= lim supnFp(An)≤Fp(A∗
c),and on the other hand we have uc≥F(A∗
c)sinceucis the
supremum. This proves that uc=Fp(A∗
c)is the maximum of Fp(A), subject to A∈Fc, and hence A∗
c
exists that solves (4).
Claim 2 . First, note that
∥KθX−KθY∥(A)
p=∥α+βθX−(α+βθY)∥(A)
p=|β|×∥θX−θY∥(A)
p,
from where it follows that a set Athat maximizes ∥KθX−KθY∥(A)
palso maximizes
∥θX−θY∥(A)
p; that is,AK
c=A∗
cfor allc∈[0,∞). Second, it follows from the change of variables
formula that
∥LθX−LθY∥(AL)
p=/bracketleftbigg/integraldisplay
AL{θX(α+βt)−θY(α+βt)}pµ(dt)/bracketrightbigg1/p
=1
β1/p/bracketleftbigg/integraldisplay
A{θX(u)−θX(u)}pµ(du)/bracketrightbigg1/p
,
whereA={α+βt:t∈AL}. Thus,
arg max{∥LθX−LθY∥(AL)
p:AL∈Fc}= arg max{∥θX−θY∥(A)
p:A∈Fα+βc},
and hence, AL
c=A∗
α+βc.
Claim 3 . Note first that D∗
c≥0, for all 0<c<∞. Also, it holds that D∗
c= 0if and only if θX=θYinΘc;
indeed,D∗
c= 0implies that 0 =∥θX−θY∥(A∗
c)
p≥∥θX−θY∥(A)
p, for everyA∈Fc, which is only possible if
θX=θYinΘc, asc>0. Finally, the triangle inequality for ∥·∥(A)
pand∥F∥∞= maxA|F(A)|yields that
D∗
c(θX,θZ) = max{∥θX−θZ∥(A)
p:A∈Fc}
= max{∥(θX−θY) + (θY−θZ)∥(A)
p:A∈Fc}
≤max{∥θX−θY∥(A)
p+∥θY−θZ)∥(A)
p:A∈Fc}
≤max{∥θX−θY∥(A)
p:A∈Fc}+ max{∥θY−θZ)∥(A)
p:A∈Fc}
=D∗
c(θX,θY) +D∗
c(θY,θZ),
hence concluding the proof.
Claim 4 . First, we note that an increase in crepresents augmenting the search domain as
Fa⊂Fb,forb>a.
This combined with the fact that the set objective function F(A) =∥θX−θY∥(A)
pis non-decreasing ( A⊆B
impliesF(A)≤F(B)) yields that D∗
b≥D∗
a, forb>a.
B.2 Proof of Theorem 2
Claim 1. Tikhonov’s theorem implies that from the Cartesian product of two compact sets results a compact
set (Waldmann, 2014, Theorem 5.3.1). Hence, as a consequence of this, the search domain T×[0,Rc]is
compact for every c≥0. Next, it is a routine exercise to prove that f(t,r) =F{B(t,r)}, is continuous for all
(t,r)∈T×[0,Rc]as bothTand[0,Rc]are compact. The final result then follows from Weierstrass theorem.
Claim 2. LetθX,θY∈Lp(T)be fixed and set D∗
c=D∗
c(θX,θY).The proof uses Berge’s maximum theorem
(Lemma 2) which, as can be seen from Appendix A, claims that a value function is continuous provided
17Published in Transactions on Machine Learning Research (05/2024)
that both the objective function and the constraint correspondence are continuous. In our setup, the value
function is
D∗
c= max{g(c,t,r ) : (t,r)∈φ(c)}= max{f(t,r) : (t,r)∈φ(c)},
whereg(c,t,r ) =f(t,r) =F{B(t,r)}, and the constraint correspondence is φ: [0,∞)↠Rd+1, defined by
φ(c) =T×[0,Rc], c≥0. (17)
Thus, we just have to prove that φ(c)in (17) is continuous, given that the objective function g(c,t,r ) =
f(t,r) =F{B(t,r)}is trivially continuous for all (t,r)∈T×[0,Rc]. Continuity of φ(c)follows immediately
fromtheproductofcorrespondencestheorem(Lemma3), whichyieldsthat φ(c) =φ1(c)×φ2(c)iscontinuous
as bothφ1(c) =Tandφ2(c) = [0,Rc]are compacted-valued, and Rcis a continuous function for every c≥0.
Finally, upper semicontinuity of the argmax correspondence,
αc={(t,r)∈T×[0,Rc] :g(c,t,r ) =D∗
c}={(t,r)∈T×[0,Rc] :f(t,r) =D∗
c},
follows also from Berge’s maximum theorem as Rd+1is Hausdorff.
B.3 Proof of Theorem 3
Suppose by contradiction that A′
w,cis the solution to the set function linear scalarization problem (14), for
a fixedw∈(0,∞)q, but that A′
w,cwas not a Pareto optimal RMMD; then, there would exist a Pareto
improvement A∈Fc, withA̸=A′
w,c, so thatFi(A)≥Fi(A′
w,c), for alli, andFi(A)>Fi(A′
w,c), for at least
ani. But then,
q/summationdisplay
i=1wiFi(A)>q/summationdisplay
i=1wiFi(A′
w,c),
which is a contradiction as A′
w,csolves the set function linear scalarization problem.
C Monte Carlo Simulation Study
Data generating processes and simulation settings
We consider the following simulation scenarios based on Examples 1–2 from Section 2.1:
Scenario 1 (Mean Functions) BMDs between mean functions as in Example 1. Here, XtandYtare
Gaussian processes with mean functions given in (2) and where the same Matérn covariance function is
assumed for both processes. Specifically cov (Xs,Xt) =cov(Ys,Yt) =Cν(∥s−t∥2,σ,ℓ), where
Cν(d,σ,ℓ ) =σ221−ν
Γ(ν)/parenleftbigg√
2νd
ℓ/parenrightbiggν
Kν/parenleftbigg√
2νd
ℓ/parenrightbigg
,
for(s,t)∈[0,1]2, whereKνis the modified Bessel function (Abramowitz & Stegun, 1964, Section 9.6),
and where σ,ν, andℓare positive parameters, here set as σ=ν=ℓ= 1. The simulated data are then a
discretized version of nsimulated Gaussian processes evaluated over a grid on the unit interval,
DX={Xt,i:t∈{0/J,..., (J−1)/J}}n
i=1,
withn∈{10,50,100,200}andJ∈{10,20}; the same comments apply to DY.
Scenario 2 (Intensity Functions) BMDs between intensity functions as in Example 2. Here, points drawn
from non-homogeneous bivariate Poisson process with mean measures,
/braceleftigg
E{NX(A)}=/integraltext
Aγexp{−(t2
1+t2
2)/2}dt,
E{NY(A)}=δE{NX(A)},(18)
18Published in Transactions on Machine Learning Research (05/2024)
forA⊆T= [−3,3]2. While the sample sizes in this scenario are random quantities, given by NX=NX(T)
andNY=NY(T), the mean number of simulated points over Tis(E(NX),E(NY))≈(2πγ,2πγδ), a simple
yet accurate approximation that follows immediately from multiple Gaussian integrals. The simulated data
are given by the following collection of points
DX={(X1,1,X2,1),..., (X1,NX,X2,NX)},
withγ∈{25,50}andδ∈{2,4,8,16}, and whereDYis analogously defined.
Scenario 1
(a)
 (b)
Scenario 2
(c)
 (d)
Figure 5: One shot experiments for Scenarios 1 and 2 (a) Black solid segments corresponds to fitted BMDs
basedonthesimulatedGaussianprocessdata; thechartalsodepictsthefittedfunctionalparameters(dashed)
along with 95%credible bands, and the true parameters (solid). (b) Marginal posterior density for the center
t∗
cplotted against the true center (rug). (c) and (d) are identical to (a) and (b), respectively, but for the
simulated point process data from Scenario 2.
Prior specification and posterior inference
Inferences for the BMDs were carried out by sampling m= 1 000 and 500 times using Algorithm 1 for
Scenarios 1 and 2, respectively. As can be seen from Algorithm 1, inferences for BMDs are constructed from
the functional parameters, and thus we now comment on what versions of (8) have been used for fitting
the latter. For Scenario 1 the identity link function was used in (8) along with B-spline basis, and the
number of basis functions was selected using the DIC. The default uninformative priors of R-INLAhave been
used, which consist of diffuse priors for the β’s—i.e.β0∼N(0,∞)andβi∼N(0,1000)—and a long-tailed
prior for the variance of the error term—i.e. a log gamma distribution, where the gamma distribution has
meana/band variance a/b2, witha= 1andb= 10−5; see Wang et al. (2018, Section 5.2.1) for further
19Published in Transactions on Machine Learning Research (05/2024)
details. For Scenario 2 we follow Simpson et al. (2016) and specify a log-Gaussian Cox process using (8)
by setting a log link function, that links the intensity function with a Matérn random field using piecewise
linear basis functions over a mesh, and where the β’s are Gaussian-distributed. For the parameters of the
Matérn covariance function we use the PC prior approach of Fuglstad et al. (2019) setting P(σ>1) = 0.001
andP(ℓ<0.05) = 0.001.
One shot experiments
We first illustrate the methods on a single run experiment for some instances of Scenarios 1 and 2. In
Fig. 5(a) we show the fitted BMDs for Scenario 1, along with the corresponding mean functions, on a one
shot experiment with n= 10andJ= 10, forc= 0.1andc= 0.2. As can can be seen from Fig. 5(a),
the fitted BMDs accurately recover the true B∗
0.1= [0.165,0.265]andB∗
0.2= [0.115,0.315]. In Fig. 5(b)
we also display the marginal posterior density for the optimal center t∗
cwhich quantifies the uncertainty
surrounding the true. The marginal posterior for the radius is essentially degenerated, as predicted earlier
in the comments surrounding (7), and hence not shown.
In Fig. 5(c) we depict the fitted BMD for Scenario 2, on a one shot experiment with γ= 25andδ= 2, for
c= 2π, and display the fitted intensity functions. As it is evident from Fig. 5(c), the fitted BMD nicely
uncovers the true, and indeed it completely overlaps it to the point that the true (depicted in gray) is barely
visible.
Scenario 1 Scenario 2
Figure 6: Side-by-side boxplots of GHE in (19) for Monte Carlo simulation study.
Monte Carlo evidence
We redo the previous one shot analyses M= 1 000times, considering different samples sizes, and relying on
the GHE (Posterior Mean GlobalHausdorffError),
GHE =E/braceleftbigg/integraldisplay|T|
0DH(B∗
c,/hatwideB∗
c) dc/vextendsingle/vextendsingle/vextendsingle/vextendsingleDX,DY/bracerightbigg
(19)
to quantify how accurate on average are the estimated BMDs, /hatwideB∗
c, over 0≤c≤|T|. In Scenario 1 we use
DH(B∗
c,/hatwideB∗
c) = max{|(t∗
0,c−r∗
c)−(/hatwidet∗
0,c−/hatwider∗
c)|,|(t∗
0,c+r∗
c)−(/hatwidet∗
0,c+/hatwider∗
c)|}, while in Scenario 2 we use a numerical
approximation of DH(B∗
c,/hatwideB∗
c)implemented using Borchers (2021). Finally, the GHE for each simulated
dataset is computed as
GHEj=1
mm/summationdisplay
i=1/integraldisplay|T|
DH(B∗
c,/hatwideB∗[i,j]
c) dc,
20Published in Transactions on Machine Learning Research (05/2024)
where/hatwideB∗[i,j]
cis theith posterior sampled BMD, based on the jth simulated sample, for i= 1,...,mand
j= 1,...,M. As can be seen from Fig. 6, GHE tends to decrease as n,J,γ, andδincreases. Such behavior
confirms the expected frequentist behavior of the methods, as nandJdictates the amount of simulated data
for Scenario 1, and γandδdoes the same for Scenario 2. To put it differently, since larger values of γand
δimply larger sample sizes, the observed reduction in GHE as a function of the latter parameters suggests
a sensible asymptotic performance of the proposed Bayesian inferences.
D Selected Comments on the Choice of c
D.1 The Coincidence Set Criterion
Optimal data-driven selection of crequires a criterion or loss function and it remains an open problem for
future analysis. Motivated by a reviewer’s comment, this section offers hints on potential starting points.
ForT= [a,b]⊂R, a practical approach is to center the parameter functions and establish a criterion based
on the width of intervals defined by their coincidence point(s); we term this the coincidence set criterion
and will detail it next. While parts of the construction can be readily extended beyond the real line, fully
developing the criterion in Rdare outside the scope of this section.
Formally, define the demeaned parameter functions as
˜θX(t) =θX(t)−1
b−a/integraldisplayb
aθX(u) du, ˜θY(t) =θY(t)−1
b−a/integraldisplayb
aθY(u) du, (20)
and the coincidence set as
Π ={t:˜θX(t) =˜θY(t)}. (21)
In words, Πis the set of points where ˜θXand˜θYcoincide.
Proposition 1. AssumeθXandθYare continuous functions on T= [a,b]and differentiable on (a,b).
Then, the coincidence set is nonempty, that is, Π̸=∅.
Proof.The proof follows from the mean value theorem (e.g., Tao, 2006, Corollary 10.2.9). First, note that
Π ={t:˜θX(t)−˜θY(t) = 0}
=/braceleftbigg
t:θX(t)−θY(t) =1
b−a/integraldisplayb
aθX(u)−θY(u) du= 0/bracerightbigg
=/braceleftbigg
t:δ(t) =∆(b)−∆(a)
b−a/bracerightbigg
,
whereδ(t) =θX(t)−θY(t)and∆(b)−∆(a) =/integraltextb
aδ(u) du. The mean value theorem implies that there exists
anc∈(a,b)such that
δ(c) =∆(b)−∆(a)
b−a,
from where the final result follows.
To streamline the presentation of ideas, we first consider the case where ˜θX(t)and ˜θY(t)touch at only a
finite number of points, i.e., where Πhas Lebesgue measure 0. In that case, the criterion can be written as
csc= max{τi+1−τi}m
i=0, (22)
wherem<∞is the cardinality of Πanda=τ0<τ1<···<τm<τm+1=b, withτ1,...,τmdenoting the
ordered elements of Π.
21Published in Transactions on Machine Learning Research (05/2024)
The extension of (22) to the general case—that takes into account parameter functions could coincide over
a set Πwith positive measure and at an infinite number of points—is similar but slightly more technical.
First, it is convenient to partition the coincidence set as follows
Π = Π 0∪Π1,
where Π0={τi}M
i=1is a discrete set (with the same interpretation as above, but with M≤ ∞) and
Π1=/uniontextJ
j=1(αj,βj)is a collection of J≤∞open sets. The general definition of the coincidence criterion is
csc= max/braceleftig
τ1−a,sup{τi+1−τi}M−1
i=1,b−lim
i→Mτi/bracerightig
, (23)
where{τi}M
i=1are the ordered elements of Π0andM≤∞. Note that if ˜θX(t)and˜θY(t)touch at only a
finite number of points, then Π = Π 0and hence (23) becomes (22) with M=m.
D.2 Data Illustrations
Fig. 7 illustrates the BMDs obtained using this experimental criterion with two real data examples. The
findings are comparable to those presented in the main paper, although the regions are slightly larger.
Electrocardiogram data Stock market data
Figure 7: BMDs corresponding to the data driven cobtained via the coincidence set approach.
E List of Symbols
Table 1 lists symbols and notation used throughout the article.
22Published in Transactions on Machine Learning Research (05/2024)
Symbol Description
X,Ystochastic processes under comparison, that is, X={Xt}andY={Yt}
T ground, or index, set over which processes XandYare defined
θX,θYfunctional parameters for XandY
DX,DYdata on proceses XandY
∥·∥(A)
pLpsub-norm over A
F(·)set objective function ∥θX−θY∥(·)
p
f(t,r)set objective function evaluated at closed ball B(t,r), that is,F{B(t,r)}
Fi set objective functions in multi-objective context
A set of compact and convex subsets of the ground set T
|A| volume functional, i.e., Lebesgue measure of AinRd
Fc collection of feasible sets
A∗
c region of maximum or multi-maximum dissimilarity
D∗
c dissimilarity index for region of maximum dissimilarity
B∗
c ball of maximum or multi-maximum dissimilarity
D∗
c dissimilarity index for ball of maximum dissimilarity
t∗
c,r∗
ccenter and radius of ball of maximum dissimilarity
Bp set of all closed balls in Lp
dH(A,B)Hausdorf distance between sets AandB
d(x,B)minimum Euclidean distance between a point x∈Aand setB
∂A boundary of set A
Table 1: Symbols and notation used throughout the article.
23