Published in Transactions on Machine Learning Research (10/2023)
Federated Learning under Partially Class-Disjoint Data via
Manifold Reshaping
Ziqing Fan, Jiangchao YaoB, Ruipeng Zhang {zqfan_knight, sunarker, zhangrp}@sjtu.edu.cn
Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
Shanghai AI Laboratory
Lingjuan Lyu Lingjuan.Lv@sony.com
Sony AI
Ya Zhang, Yanfeng WangB{ya_zhang, wangyanfeng}@sjtu.edu.cn
Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
Shanghai AI Laboratory
Reviewed on OpenReview: https: // openreview. net/ forum? id= jLJTqJXAG7
Abstract
Statistical heterogeneity severely limits the performance of federated learning (FL), moti-
vating several explorations e.g.,FedProx, MOON and FedDyn, to alleviate this problem.
Despite effectiveness, their considered scenario generally requires samples from almost all
classes during the local training of each client, although some covariate shifts may exist
among clients. In fact, the natural case of partially class-disjoint data (PCDD), where each
client contributes a few classes (instead of all classes) of samples, is practical yet underex-
plored. Specifically, the unique collapse and invasion characteristics of PCDD can induce
the biased optimization direction in local training, which prevents the efficiency of federated
learning. To address this dilemma, we propose a manifold reshaping approach called FedMR
to calibrate the feature space of local training. Our FedMR adds two interplaying losses to
the vanilla federated learning: one is intra-class loss to decorrelate feature dimensions for
anti-collapse; and the other one is inter-class loss to guarantee the proper margin among cat-
egories in the feature expansion. We conduct extensive experiments on a range of datasets
to demonstrate that our FedMR achieves much higher accuracy and better communication
efficiency. Source code is available at: https://github.com/MediaBrain-SJTU/FedMR.
1 Introduction
Federated learning (McMahan et al. (2017); Li et al. (2020a); Yang et al. (2019)) has drawn considerable
attention due to the increasing requirements on data protection (Shokri & Shmatikov (2015); Zhu & Han
(2020); Hu et al. (2021); Li et al. (2021c); Lyu et al. (2020)) in real-world applications like medical image
analysis (Guo et al. (2021); Park et al. (2021); Yin et al. (2022); Dou et al. (2021); Jiang et al. (2022))
and autonomous driving (Liang et al. (2019); Pokhrel & Choi (2020)). Nevertheless, the resulting challenge
of data heterogeneity severely limits the application of machine learning algorithms (Zhao et al. (2018)) in
federated learning. This motivations a plenty of explorations to address the statistical heterogeneity issue
and improve the efficiency (Kairouz et al. (2021); Wang et al. (2020); Li et al. (2022)).
Existing approaches to address the statistical heterogeneity can be roughly summarized into two categories.
One line of research is to constrain the parameter update in local clients or in the central server. For
example, FedProx (Li et al. (2020b)), FedDyn (Acar et al. (2020)) and FedDC (Gao et al. (2022)) explore
how to reduce the variance or calibrate the optimization by adding the proximal regularization on parameters
in FedAvg (McMahan et al. (2017)). The other line of research focuses on constraining the representation
1Published in Transactions on Machine Learning Research (10/2023)
C1C2C3C4...
...CnClient 1
C1C3C4...
...CnClient 2
C1C2C3C4...
...CnClient N
...Server
C2
(a) Typical FL.
C1C2C3C4...
...CnClient 1
C1C2C3C4...
...CnClient 2
C1C2C3C4...
...CnClient N
...Server (b) FL under PCDD.
(c) Global / Collapsed / Reshaped (by FedMR) feature space.
Figure 1: Federated learning under partially class-disjoint data (PCDD).
from the model to implicitly affect the update. FedProc (Mu et al. (2021)) and FedProto (Tan et al. (2022))
introduceprototypelearningtohelplocaltraining, andMOON(Lietal.(2021b))utilizescontrastivelearning
to minimize the distance between representations learned by local model and global model, and maximize
the distance between representations learned by local model and previous local model. However, all these
methods validate their efficiency mostly under the support from all classes of samples in each client while
lacking a well justification on a natural scenario, namely partially class-disjoint data w.r.t. classes.
As illustrated in Figure 1(a), in typical federated learning, each client usually contains all classes of samples
but under different covariate shifts, and all clients work together to train a global model. However, in the
case of PCDD (Figure 1(b)), there are only a small subset of categories in each client and all clients together
provide information of all classes. Such a situation is very common in real-world applications. For example,
there are shared and distinct Thyroid diseases in different hospitals due to regional diversity (Gaitan et al.
(1991)). Hospitals from different regions can construct a federation to learn a comprehensive model for the
diagnostic of Thyroid diseases but suffer from the PCDD challenge. We conduct a toy study on a simulated
dataset (see details in the Appendix B), and visualize the feature space under centralized training (the left
panel of Figure 1(c)) and local training under PCDD (the middle panel of Figure 1(c)) by projecting on the
unit sphere. As shown in Figure 1(c), PCDD induces a dimensional collapse onto a narrow area due to the
lack of support from all classes, and causes a space invasion to the missing classes. Previous approaches such
as FedProc and MOON may implicitly constrain the space invasion by utilizing class prototypes or global
features generated from the global model, and methods like FedProx, FedDyn, and FedNova could also help
a bit from the view of optimization. However, these methods are not oriented towards the PCDD issue,
and they are inefficient to avoid the collapse and invasion characteristics of PCDD and achieve sub-optimal
performance from the both view of experimental performance shown in Table 1 and the feature variance of
all methods shown in Table 7.
To address this dilemma, we propose a manifold-reshaping approach called FedMR to properly prevent the
degeneration caused by the locally class missing. FedMR introduces two interplaying losses: one is intra-class
loss to decorrelate feature space for anti-collapse; and another one is the inter-class loss to guarantee the
proper margin among categories by means of global class prototypes. The right panel of Figure 1(c) provides
a rough visualization of FedMR. Theoretically, we analyze the benefit from the interaction of the intra-class
loss and the inter-class loss under PCDD, and empirically, we verify the effectiveness of FedMR compared
with the current state-of-the-art methods. Our contributions can be summarized as follows:
•We are among the first attempts to study dimensional collapse and space invasion challenges caused
by PCDD in Generic FL that degenerates embedding space and thus limits the model performance.
2Published in Transactions on Machine Learning Research (10/2023)
•WeintroduceaapproachtermedasFedMR,whichdecorrelatesthefeaturespacetoavoiddimensional
collapseandconstructsaproperinter-classmargintopreventspaceinvasion. Ourtheoreticalanalysis
confirms the rationality of the designed losses and their benefits to address the dilemma of PCDD.
•We conduct a range of experiments on multiple benchmark datasets under PCDD and a real-world
disease dataset to demonstrate the advantages of FedMR over the state-of-the-art methods. We also
develop several variants of FedMR to consider the communication cost and privacy concerns.
2 Related Works
2.1 Federated Learning
There are extensive works to address the statistical heterogeneity in federated learning, which induces the
bias of local training due to the covariate shifts among clients (Zhao et al. (2018); Li et al. (2022); Zhang et al.
(2023a)). A line of research handles this problem by adding constraints like normalization or regularization
on model weights in the local training or in the server aggregation. FedProx (Li et al. (2020b)) utilizes
a proximal term to limit the local updates so as to reduce the bias, and FedNova (Wang et al. (2020))
introduces the normalization on total gradients to eliminate the objective inconsistency. FedDyn (Acar et al.
(2020)) makes the global model and local models approximately aligned in the limit by proposing a dynamic
regularizer for each client at each round. FedDC (Gao et al. (2022)) reduces the inconsistent optimization
on the client-side by local drift decoupling and correction. Another line of research focuses on constraining
representations from local models and the global model. MOON (Li et al. (2021b)) corrects gradients in
local training by contrasting the representation from local model and that from global model. FedProc (Mu
et al. (2021)) utilizes prototypes as global information to help correct local representations. Our FedMR is
also conducted on representations of samples and classes but follows a totally different problem and spirit.
2.2 Representation Learning
The collapse problem is also an inevitable concern in the area of representation learning. In their research
lines, there are several attempts to prevent the potential collapse issues. For example, in self-supervised
learning, Barlow Twins, VICReg and Shuffled-DBN (Bardes et al. (2022); Zbontar et al. (2021); Hua et al.
(2021)) manipulate the rank of the (co-)variance matrix to prevent the potential collapse. In contrastive
learning, DirectCLR (Jing et al. (2021)) directly optimizes the representation space without an explicit
trainable classifier to promote a larger feature diversity. In incremental learning, CwD (Shi et al. (2022))
applies a similar technique to prevent dimensional collapse in the initial phase. In our PCDD case, it is more
challenging, since we should not only avoid collapse but also avoid the space invasion in the feature space.
2.3 Federated Prototype Learning
In many vision tasks (Snell et al. (2017); Yang et al. (2018); Deng et al. (2021)), prototypes are the mean
values of representations of a class and contain information like feature structures and relationships of
different classes. Since prototypes are population-level statistics of features instead of raw features, which
are relatively safe to share, prototype learning thus has been applied in federated learning. In Generic FL,
FedProc (Mu et al. (2021)) utilizes the class prototypes as global knowledge to help correct local training.
In Personalized FL, FedProto (Tan et al. (2022)) shares prototypes instead of local gradients to reduce
communication costs. We also draw spirits from prototype learning to handle PCDD. Besides, to make fair
comparison to methods without prototypes and better reduce extreme privacy concerns, we conduct a range
of auxiliary experiments in Section 4.4.
3 The Proposed Method
3.1 Preliminary
PCDD Definition. There are many nonnegligible real-world PCDD scenarios. ISIC2019 dataset (Codella
et al. (2018); Tschandl et al. (2018); Combalia et al. (2019)), a region-driven subsetof types of Thyroid
3Published in Transactions on Machine Learning Research (10/2023)
θW*
c 1c 2 W 1*
c 3
αβ
Figure 2: An illustration about the shift of the optimization direction under PCDD. Here, we assume our
client contains two classes c1andc2with one missing class c3.w∗is the optimal classifier direction for c2
(perpendicular to the plane α) when all classes exist, and w∗
1is the learned classifier direction when c3is
missing, which can be inferred by the decision plane βbetweenc1andc2. As can be seen, PCDD leads to
the angle shift θin the optimization.
diseases in the hospital systems, is utilized in our experiments. In landmark detection (Weyand et al.
(2020)) for thousands of categories with data locally preserved, most contributors only have a subsetof
categories of landmark photos where they live or traveled before, which is also a scenario for the federated
PCDD problem. To make it clear, we first define some notations of the partially class-disjoint data situation
in federated learning. Let Cdenote the collection of full classes and P denote the set of all local clients.
Considering the real-world constraints like privacy or environmental limitation, each local client may only
own the samples of partial classes. Thus, for the k-th clientPk, its corresponding local dataset Dkcan be
expressed as Dk={(xk,i,yk,i)|yk,i=c∈Ck},whereCk⊊C.The number of samples of the class c(c∈Ck)
inPkisNc
k. We denote a local model f(·;wk)on all clients as two parts: a backbone network f1(·;wk,1)
and a linear classifier f2(·;wk,2). The loss of the k-th client can be formulated as
ℓcls
k(Dk;wk) =1
NkNk/summationdisplay
i=1ℓ(yk,i,f2(zk,i;wk,2))/vextendsingle/vextendsingle
zk,i=f1(xk,i;wk,1),
wherezk,iis the feature representation of the input xk,iandℓ(·,·)is the loss measure. Under PCDD, we
can empricially find the dimensional collapse and the space invasion problems about representation.
FedAvg. The vanilla federated learning via FedAvg consists of four steps (McMahan et al. (2017)): 1) In
roundt, the server distributes the global model wtto clients that participate in the training; 2) Each local
client receives the model and continues to train the model, e.g.,thek-th client conducts the following,
wt
k←wt
k−η∇ℓk(bt
k;wt
k), (1)
whereηis the learning rate, and bt
kis a mini-batch of training data sampled from the local dataset Dk.
AfterEepochs, we acquire a new local model wt
k; 3) The updated models are then collected to the server
as{wt
1,wt
2,...,wt
K}; 4) The server performs the following aggregation to acquire a new global model wt+1,
wt+1←K/summationdisplay
k=1pkwt
k, (2)
wherepkis the proportion of sample number of the k-th client to the sample number of all the participants,
i.e.,pk=Nk//summationtextK
k′=1Nk′. When the maximal round Treaches, we will have the final optimized model wT.
3.2 Motivation
In Figure 2, we illustrate a low-dimensional example to characterize the directional shift of the localtraining
under PCDD on the client side. In the following, we use the parameter aggregation of a linear classification
4Published in Transactions on Machine Learning Research (10/2023)
to study the directional shift of globalmodel in the server, which further clarifies the adverse effect of PCDD.
Similar to Figure 2, let c1,c2andc3denote three classes of samples on a circular face respectively centered
at (1, 0), (0,√
3) and (0, -√
3) with radius r=1
2under a uniform distribution. Then, if there are samples of
all classes in each local client, we will get the optimal weight for all categories as follows:
w∗=
1 0
−√
3
21
2
−√
3
2−1
2
.
Note that, we omit the bias term in linear classification for simplicity. Conversely, among total three
participants, if each participant only has the samples of two classes, e.g., ( c1,c2), (c1,c3) and (c2,c3)
respectively, then their learned weights can be inferred as follows:
w∗
1, w∗
2, w∗
3=
1
2−√
3
2
−1
2√
3
2
0 0
,
1
2√
3
2
0 0
−1
2−√
3
2
,/bracketleftigg0 0
0 1
0−1/bracketrightigg
.
After the server aggregation, we have the estimated weight
ˆw∗=
1
30
−1
6√
3+2
6
−1
6−√
3+2
6
.
Then, we can find that except the difference on amplitude between w∗and ˆw∗, a more important issue is
the optimization direction for c2(orc3) shifts about 45◦by computing the angle between vector (−√
3
2,1
2)
and vector (−1
6,√
3+2
6)(or between vector (−√
3
2,−1
2)and vector (−1
6,−√
3+2
6)). Actually, the angle shift
can be enlarged in some real-world applications, when the hard negative classes are missing. However, if
we can have the statistical centroid of the locally missing class in the local client, namely c3in the case of
Figure 2, it is easy to find that the inferred optimal ˆw∗is same tow∗as the decision plane can be normally
characterized with the support of the single point c3. This inspires us to design the subsequent method1.
3.3 Manifold Reshaping
As the aforementioned analysis, PCDD in federated learning leads to the directional shift of optimization
both in the local models and in the global model. An empirical explanation is that the feature representation
of the specific class that should support classification is totally missing, inducing the feature representation
of other observed classes arbitrarily distributes as a greedy collapsed manifold, as shown in Figure 1(c). To
address this problem, we explore a manifold-reshaping method from both the intra-class perspective and the
inter-class perspective. In the following, we will present two interplaying losses and our framework.
3.3.1 Intra-Class Loss
The general way to prevent the representation from collapsing into a low-dimensional manifold, is to decor-
relate dimensions for different patterns and expand the intrinsic dimensionality of each category. Such a goal
can be implemented by manipulating the rank of the covariance matrix regarding representation. Specif-
ically, for each client, we can first compute the class-level normalization for the representation zc
k,i∈Rd
asˆzc
k,i=zc
k,i−µc
k
σc
k, whereµc
kandσc
kare the mean and standard deviation of features belonging to class c
and calculated as: µc
k=1
nk,c/summationtextnk,c
i=1zc
k,iandσc
k=/radicalig
1
nk,c/summationtextnk,c
i=1(zc
k,i−µc
k)2. Then, we compute an intra-class
covariance matrix based on the above normalization for each observed class in the k-th client:
Mc
k=1
Nc
k−1Nc
k/summationdisplay
i=1/parenleftig
ˆzc
k,i/parenleftbig
ˆzc
k,i/parenrightbig⊤/parenrightig
.
1Note that, we would like to point out that in the real-world case, we cannot acquire such statistical centroid in advance but
have to resort to the training process along with the special technique design.
5Published in Transactions on Machine Learning Research (10/2023)
Since each eigenvalue of Mc
k∈Rd×dcharacterizes the importance of a feature dimension within class, we
can make them distributed uniformly to prevent the dimensional collapse of each observed class. However,
considering the learnable pursuit of machine learning algorithms, we actually cannot directly optimize eigen-
values to reach this goal. Fortunately, it is possible to use an equivalent objective as an alternative, which
is clarified by the following lemma.
Lemma 1. Assuming a covariance matrix M∈Rd×dcomputed from the feature of each sample with the
standard normalization, and its eigenvalues {λ1,λ2,...,λd}, we will have the following equality that satisfied
d/summationdisplay
i=1(λi−1
dd/summationdisplay
j=1λj)2=||M||2
F−d.
The complete proof is summarized in the Appendix A.2. From Lemma 1, we can see that pursuing the
uniformity of the eigenvalues for the covariance matrix can transform into minimizing the Frobenius norm
of the covariance matrix. Therefore, our intra-class loss to prevent the undesired dimensional collapse for
observed classes is formulated as
ℓintra
k =1
|Ck|/summationdisplay
c∈Ck||Mc
k||2
F. (3)
3.3.2 Inter-Class Loss
Although the intra-class loss helps decorrelate the feature dimensions to prevent collapse, the resulting space
invasion for the missing classes can be concomitantly exacerbated. Thus, it is important to guarantee the
proper space of the missing classes in the expansion as encouraged by equation 3. To address this problem,
we maintain a series of global class prototypes and transmit them to local clients as support of the missing
classes in the feature space. Concretely, we first compute the class prototypes in the k-th client as the average
of feature representations (Snell et al. (2017); Yang et al. (2018); Deng et al. (2021)):


gc
k/vextendsingle/vextendsingle/vextendsingle/vextendsinglegc
k←1
Nc
kNc
k/summationdisplay
i=1zc
k,iforc∈Ck

.
Then, all client-level prototypes are submitted to the server along with local models in federated learning.
In the central server, the global prototypes for all classes are updated as
/braceleftigg
gt
c/vextendsingle/vextendsingle/vextendsingle/vextendsinglegt
c←K/summationdisplay
k=1pc
kgc
kforc∈C/bracerightigg
,
wheregt
cis the global prototype of the c-th class in round tandpc
k=Nc
k//summationtextK
k=1Nc
k. In the next round,
the central server distributes the global prototypes to all clients as the references to avoid the space inva-
sion. Formally, we construct the following margin loss by contrasting the distances from prototypes to the
representation of the sample.
ℓinter
k =1
|Ck|(|Ck|−1)/summationdisplay
ci∈Ck/summationdisplay
cj∈Ck\ciDci,cj, (4)
whereDci,cjis defined as:
Dci,cj=1
Nc
kNc
k/summationdisplay
n=1max{||zci
k,n−gt
ci||−||zci
k,n−gt
cj)||,0}.
In the following, we use a theorem to show how inter-class loss jointly with intra-class loss makes the
representation of the missing classes approach to the optimal.
6Published in Transactions on Machine Learning Research (10/2023)
Input Image: xClassifierLocal Model
Backbone
Featur e: zFinal LossLocal Client 1
Corr ect Featur e
SpaceIntra-class Loss  Inter -class Loss  
Decorr elate Featur e
Dimensions  Manifold Reshaping Loss
C2C3C4 Cn ......Local Class Pr ototypes
C1Global Class Pr ototypes
C1C2C3C4 Cn ......Classification LossClassification Loss
Aggr egationCentral Server
Client N
C1C2C3C4Cn......Client 1
C3C4Cn C1C2 ......
...
Global Model
C1C2C3C4 Cn......Global Class Pr ototypes...
Figure 3: The framework of FedMR. On the client side, except the vanilla training with the classification loss,
the manifold-reshaping parts, i.e.,the intra-class loss and the inter-class loss, respectively help conduct the
feature decorrelation to avoid the dimensional collapse, and leverage the global prototypes to construct the
proper margin among classes to prevent the space invasion. On the server side, except the model aggregation,
the global class prototypes are also the reference for missing classes participating in the local training.
Algorithm 1 FedMR
Input: a set ofKclients that participate in each round, the initial model weights w0, the maximal round
T, the learning rate η, the local training epochs E.
fort= 0,1,...,T−1do
randomly sample Kclients
updates global model weights and global class prototypes ( wt←/summationtextK
k=1pt
kwt−1
k∀c,gt
c←/summationtextK
k=1pc
kgc
k).
distribute wtandGc{gt
1,gt
2,...,gt
C}to theKclients.
do in parallel for ∀k∈Kclients
wt
k←wt.
forτ= 0,1,...,E−1do
sample a mini-batch from local dataset and perform updates( wt
k←wt
k−η∇Lk(bt
k,Gc;wt
k)).
end for
update local class prototypes ( ∀c, gc
k←/summationtextnc
k
i=11
nc
kzc
i) , and submit wt
kand {g1
k,g2
k,...,gC
k} to server.
end in parallel
end for
Theorem 1. Let the global optimal representation for class cbeg∗
c= [a∗
c,1,...,a∗
c,d], andzc,t
kbe the represen-
tation of sample xin the class cof thek-th client. Assuming that ∀i, both|a∗
c,i|andzc,t
k,iare upper bounded
byG, and all dimensions are disentangled, in round t, the i-th dimension of local representation zc,t
ksatisfies
|zc,t
k,i−a∗
c,i|≤2(1−ˆpc
kΓ)G+δΓ,
where ˆpc
kis the accumulation regarding the i-th dimension of the class- cprototype, Γ =1−(pc
k)t
1−pc
k,(pc
k)trefers
to thepc
kraised to the power of t, and δis the maximum margin of the inter-loss term.
Note that, Theorem 1 shows five critical points: 1) The proof of the theorem requires each dimension
of the representation to be irrelevant to each other, which is achieved by the intra-class loss. Although
disentanglement of dimensions might not be totally achieved in practical, empirically, we find that the
intra-class loss converges and maintains a relatively low value easily, meaning that the model achieves good
7Published in Transactions on Machine Learning Research (10/2023)
decorrelation. In the Appendix, we show the training curve of intra-class loss during the federated training.
2) In the theorem, δis a trade-off of training stability and theoretical results determined by the margin. The
larger the margin, the larger the δ, however the more stable the local training is. This is because the global
prototypes are not very accurate in the early stage of local training and directly minimizing the distance
of samples to their global class prototypes can bring side effect to the feature diversity. When the margin
is removed ( Dci,cj=1
Nc
k/summationtextNc
k
n=1||zci
k,n−gt
ci||),δwill be zero. 3) Without considering δ, astincreases, Γis
smaller and representation zc
kis closer to global optimal prototype g∗
c, showing the promise of our method.
4) When t is large enough, we can get an upper bound 21−pc
k−ˆpc
k
1−pc
kG, meaning more clients with the specific
dimensional information participating in the training, the tighter the upper bound is. When all other clients
can provide the support ( ˆpc
k= 1−pc
k), the error will be 0. 5) While ˆpc
kdenotes the proportions of a subset
of clients that can provide the support information for this dimension, the theoretical result depends on the
class distribution and overlap across the clients. The complete proof is summarized in the Appendix A.1.
3.3.3 The Total Framework
After introducing the intra-class loss and the inter-class loss, we give the total framework of FedMR. On the
client side, local models are trained on their partially class-disjoint datasets. Through manifold-reshaping
loss, dimensions of the representation are decorrelated and local class subspace is corrected to prevent the
space invasion, and gradually approach the global space partition. The total local objective including the
vanilla classification loss can be written as
Lk=ℓcls
k+/parenleftbig
µ1ℓintra
k +µ2ℓinter
k/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
manifold reshaping,(5)
whereµ1andµ2are the balancing hyperparameters and will be discussed in the experimental part. In
Figure 3, we illustrate the corresponding structure of FedMR and formulate the training procedure in Algo-
rithm 1. In terms of the privacy concerns about the prototype transmission and the communication cost,
we will give the comprehensive analysis on FedMR about these factors.
4 Experiment
4.1 Experimental Setup
Datasets. We adopt four popular benchmark datasets SVHN (Netzer et al. (2011)), FMNIST (Xiao et al.
(2017)), CIFAR10andCIFAR100(LeCunetal.(1998))infederatedlearningandareal-worldPCDDmedical
dataset ISIC2019 (Codella et al. (2018); Tschandl et al. (2018); Combalia et al. (2019)) to conduct experi-
ments. Regarding the data setup, although Dirichelet Distribution is popular to split data in FL, it usually
generates diverse imbalance data coupled with occasionally PCDD. In order to better study pure PCDD,
for the former four benchmarks, we split each dataset into ϱclients, each with ςcategories, abbreviated
asPϱCς. For example, P10C10 in CIFAR100 means that we split CIFAR100 into 10 clients, each with 10
classes. Please refer to the detailed explanations and strategies in the Appendix. ISIC2019 is a real-world
federated application under the PCDD situation and the data distribution among clients is shown in the
Appendix C.1.3. We follow the settings in Flamby benchmark (Terrail et al. (2022)).
Implementation. We compare FedMR with FedAvg (McMahan et al. (2017)) and multiple state-of-the-
arts including FedProx (Li et al. (2020b)), FedProc (Karimireddy et al. (2020)), FedNova (Li et al. (2021b)),
MOON (Wang et al. (2020)), FedDyn (Acar et al. (2020)) and FedDC (Gao et al. (2022)). To make a fair
and comprehensive comparison, we utilize the same model for all approaches and three model structures for
different datasets: ResNet18 (He et al. (2016)) (follow (Li et al. (2021b; 2022))) for SVHN, FMNIST and
CIFAR10, wide ResNet (Zagoruyko & Komodakis (2016)) for CIFAR100 and EfficientNet (Tan & Le (2019))
for ISIC2019. The optimizer is SGD with a learning rate 0.01, the weight decay 10−5and momentum 0.9.
The batch size is set to 128 and the local updates are set to 10 epochs for all approaches. The detailed
information of the model and training parameters are given in the Appendix.
8Published in Transactions on Machine Learning Research (10/2023)
Table 1: Performance of FedMR and a range of state-of-the-art approaches on four datasets under PCDD
partitions. Datasets are divided into ϱclients and each client has ςclasses (denoted as PϱCς). We compute
average accuracy of all partitions, highlight results of FedMR, underline results of best baseline, and show
the improvement of FedMR to FedAvg (subscript of FedMR) and to the best baseline ( ∆).
Datasets Split FedAvg FedProx FedProc FedNova MOON FedDyn FedDC FedMR ∆
FMNISTP5C2 67.29 69.60 66.28 66.87 66.82 71.0168.5075.51 8.22%↑+4.50
P10C2 67.33 67.76 69.0848.44 67.93 67.16 67.36 74.97 7.64%↑+5.89
P10C3 81.67 80.93 82.06 83.20 83.4283.00 83.24 83.55 1.88%↑+0.13
P10C5 88.53 89.22 89.2388.86 88.98 88.38 89.22 90.04 1.51%↑+0.81
IID 91.93 91.95 92.06 91.84 92.12 91.76 92.1592.19 0.26%↑+0.04
avg 79.35 79.89 79.74 75.84 79.87 80.2680.0983.45 4.10%↑+3.19
SVHNP5C2 81.85 81.8379.54 81.11 81.60 79.89 81.63 83.10 1.25%↑+1.27
P10C2 78.92 79.60 78.75 66.86 79.8376.24 78.96 82.47 3.55%↑+2.64
P10C3 87.70 87.40 88.1387.50 87.83 87.27 88.05 89.13 1.43%↑+1.00
P10C5 91.20 91.24 91.63 92.0991.16 90.17 91.64 92.18 0.98%↑+0.09
IID 92.74 92.89 93.5792.62 93.12 92.26 92.90 93.04 0.30%↑-0.53
avg 86.48 86.59 86.32 83.87 86.7185.17 86.64 87.98 1.50%↑+1.27
CIFAR10P5C2 67.68 68.18 69.27 67.57 66.86 69.6469.1874.19 6.51%↑+4.55
P10C2 67.27 71.0967.02 57.79 67.61 67.74 67.64 73.32 2.23%↑+2.23
P10C3 77.82 77.89 77.87 77.22 78.4277.99 77.94 82.75 4.93%↑+4.33
P10C5 88.22 88.34 88.19 88.20 88.00 88.3588.1489.06 0.84%↑+0.71
IID 91.88 92.14 92.62 92.37 92.56 92.29 92.8593.06 1.18%↑+0.21
avg 78.58 79.5378.99 76.63 78.69 79.20 79.15 82.48 3.90%↑+2.95
CIFAR100P10C10 54.31 54.79 54.69 54.45 54.98 55.9454.7357.27 2.96%↑+1.33
P10C20 64.81 65.37 64.98 65.7965.75 65.02 65.21 65.81 1.00%↑+0.02
P10C30 69.35 69.75 69.64 69.55 69.51 69.8469.3870.24 0.89%↑+0.40
P10C50 71.28 71.35 72.1371.25 71.54 71.25 72.11 72.17 0.89%↑+0.04
IID 72.28 72.55 73.0772.66 73.01 73.04 72.77 72.79 0.51%↑-0.28
avg 66.41 66.76 66.90 66.74 66.96 67.2266.8467.66 1.25%↑+0.44
Table 2: Global test accuracy of methods on CIFAR10 and CIFAR100 under larger scale of clients (P10, P50
and P100) and ISIC2019. PϱCςdenotes that the dataset is divided into ϱclients and each client has ςclasses
of samples. We highlight results of FedMR, underline results of best baseline, and show the improvement of
FedMR to FedAvg (bottom right corner of results) and to the best baseline ( ∆).
Datasets Split FedAvg FedProx FedProc FedNova MOON FedDyn FedDC FedMR ∆
CIFAR10P10C3 77.82 77.89 77.87 77.22 78.42 77.99 77.94 82.75 4.93%↑+4.33
P50C3 75.46 77.46 76.27 74.12 76.51 75.52 76.03 79.58 4.12%↑+2.12
P100C3 71.65 72.37 72.44 70.46 72.46 71.85 73.95 76.93 5.28%↑+2.98
CIFAR100P10C10 54.31 54.79 54.69 54.45 54.98 55.94 54.7357.27 2.96%↑+1.33
P50C10 49.84 51.17 51.94 50.22 52.19 50.53 51.17 53.36 3.52%↑+1.17
P100C10 47.90 48.26 49.01 48.07 48.94 49.24 48.7649.60 1.70%↑+0.36
ISIC2019 Real 73.14 75.41 75.26 73.62 75.46 75.07 75.25 76.55 3.41%↑+1.09
4.2 Performance under PCDD
In this part, we compare FedMR with FedAvg and other methods on FMNIST, SVHN, CIFAR10 and CI-
FAR100 datasets under partially class-disjoint situation. Note that, in FedProx, MOON, FedDyn, FedProc,
FedDC and our method, there are parameters that need to set. We use grid search to choose the best
parameters for each method. See more concrete settings in the Appendix C.2 and Section 4.1.
As shown in Table 1, with the decreasing class number in local clients, the performance of FedAvg and all
other methods greatly drops. However, comparing with all approaches, our method FedMR achieves far
better improvement to FedAvg, especially 7.64%improvement vs.1.75%of FedProc for FEMNIST (P10C2)
and6.51%improvement vs.1.96%of FedDyn for CIFAR10 (P5C2). Besides, FedMR also performs better
under less PCDD, and on average of all partitions listed in the table, our method outperforms the best
baseline by 3.19%on FMNIST and 2.95%on CIFAR10.
9Published in Transactions on Machine Learning Research (10/2023)
Table 3: The communication cost of approaches with prototypes ( i.e.,FedProc and FedMR) and without
prototypes ( i.e.,FedAvg, FedProx, FedNova, MOON, FedDyn and FedDC).
Method Type FMNIST SVHN CIFAR10 CIFAR100 ISIC2019
w/o Prototypes 11.182M 11.184M 11.184M 36.565M 4.875M
w/ Prototypes 11.187M 11.189M 11.189M 36.629M 4.880M
Additional cost 0.044%↑0.044%↑0.044%↑ 0.175%↑ 0.102%↑
Table 4: Number of communication rounds and the speedup of communication when reaching the best
accuracy of FedAvg in federated learning on CIFAR100 under three partition strategies.
Method P10C10 P50C10 P100C10
(CIFAR100) Commu. Speedup Commu. Speedup Commu. Speedup
FedAvg 400 1× 400 1× 400 1×
FedProx 352 1.14× 370 1.08× 384 1.04×
FedProc 357 1.12× 381 1.05× 389 1.03×
FedNova 290 1.38× 385 1.04× 382 1.05×
MOON 332 1.20× 373 1.07× 395 1.01×
FedDyn 178 2.25× 366 1.09× 384 1.04×
FedDC 275 1.45× 393 1.02× 387 1.03×
FedMR 149 2.68×293 1.37×368 1.09×
/uni00000019/uni0000005b/uni00000018/uni0000005b/uni00000017/uni0000005b/uni00000016/uni0000005b/uni00000015/uni0000005b/uni00000014/uni0000005b/uni00000013/uni0000005b/uni00000014/uni0000005b/uni00000015/uni0000005b/uni00000016/uni0000005b/uni00000017/uni0000005b/uni00000018/uni0000005b/uni00000019/uni0000005b/uni00000029/uni00000048/uni00000047/uni00000024/uni00000059/uni0000004a/uni00000029/uni00000048/uni00000047/uni00000033/uni00000055/uni00000052/uni0000005b/uni00000029/uni00000048/uni00000047/uni00000033/uni00000052/uni00000055/uni00000046/uni00000029/uni00000048/uni00000047/uni00000031/uni00000052/uni00000059/uni00000044/uni00000030/uni00000032/uni00000032/uni00000031/uni00000029/uni00000048/uni00000047/uni00000027/uni0000005c/uni00000051/uni00000029/uni00000048/uni00000047/uni00000027/uni00000026/uni00000029/uni00000048/uni00000047/uni00000030/uni00000035/uni0000002f/uni0000004c/uni00000057/uni00000048/uni00000018/uni00000013/uni0000002f/uni0000004c/uni00000057/uni00000048/uni00000014/uni00000013 /uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000011
/uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c
(a) Local burden.
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013
/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni0000000b/uni00000056/uni0000000c/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni00000019/uni0000001a/uni00000013/uni00000011/uni00000019/uni0000001c/uni00000013/uni00000011/uni0000001a/uni00000014/uni00000013/uni00000011/uni0000001a/uni00000016/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000057/uni00000048/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000008/uni0000000cFedMRFedMR
(Lite10)(Lite50)FedMR
FedNovaFedAvgMOONFedDC FedProxFedProcFedDyn (b) Computation times with global accu-
racy.
Figure 4: The average memory consuming, computation time of local training and performance on all
datasets of all baselines, FedMR and its light versions (Lite 10 and Lite 50) for accelerating.
4.3 Scalability and Robustness
In the previous section, we validate the FedMR under 5 or 10 local clients under partially class-disjoint data
situations. In order to make a comprehensive comparison, we increase the client numbers of CIFAR10 and
CIFAR100 and in each round, only 10 of clients participate in the federated procedures. Besides, we also add
one real federated application ISIC2019 with multiple statistical heterogeneity problems including PCDD.
The exact parameters and communication rounds of all methods can be found in the Appendix C.2.
In Table 2, we divide CIFAR10 and CIFAR100 into 10, 50 and 100 clients and keep the PCDD degree in
the same level. As can be seen, with the number of client increasing, the performance of all methods drops
greatly. No matter in the situations of fewer or more clients, our method achieves better performance and
outperforms best baseline by 2.98%in CIFAR-10 and 0.95%in CIFAR-100 on average. Besides, in Table 2,
we verify FedMR with other methods under a real federated applications: ISIC2019. As shown in the last
line of Table 2, our method achieves the best improvement of 3.41%relative to FedAvg and of 1.09%relative
to best baseline MOON, which means our method is robust in complicated situations more than PCDD2.
4.4 Further Analysis
Except performance under PCDD, we here discuss the communication cost between clients and server, local
burden of clients, and privacy, and conduct the ablation study.
2Note that, in Appendix, we provide two real-world datasets to further demonstrate the efficiency of FedMR compared with
baselines.
10Published in Transactions on Machine Learning Research (10/2023)
Table 5: Performance of FedMR on CIFAR10 and CIFAR100 when only 50%, 80% and 100% of clients are
allowed to submit their class prototypes under different partitions.
Datasets Split FedAvg 50% 80% 100%
CIFAR10P10C3 77.82 80.42 80.27 82.75
P50C3 75.46 79.14 79.43 79.58
P100C3 71.65 76.73 76.87 76.93
CIFAR100P10C10 54.31 56.41 56.73 57.27
P50C10 50.22 51.06 52.57 53.36
P100C10 47.90 48.80 48.88 49.60
Table6: TheablationstudyofFedMR.WeillustrateaverageaccuracyofFedMRonthefourdatasetswithout
the inter-class loss or the intra-class loss or both. Results of all partitions are shown in Appendix C.7.
Inter Intra FMNIST SVHN CIFAR10 CIFAR100
- - 79.35 86.48 78.58 66.41
✓-80.18 87.46 78.86 66.62
-✓81.03 87.15 80.89 67.11
✓ ✓83.45 87.98 82.48 67.66
Communication Concern. In terms of communication cost, our method needs to share the additional
prototypes. To show how much extra communication cost will be incurred, in Table 3, we show the number of
transmission parameters in each round to compare the communication cost of methods with prototypes (Fed-
Proc and FedMR) and without prototypes (FedAvg, FedProx, FedNova, MOON, FedDyn and FedDC). From
the results, the additional communication cost in single round is negligible. Except for sharing class pro-
totypes averaged from class representations, we also tried to use 1-hot vectors to save computation and
communication, but the performance is unsatisfactory and even worse than the FedAvg. This is because
such discriminative structure might not fit the optimal statistics as each class has different hardness to learn,
and it is not clear that arriving at such an optimization stationary from the given initialization can be a good
choice. In Table 4, we also provide the communication rounds on CIFAR100 under three different partitions,
where all methods require to reach the best accuracy of FedAvg within 400 rounds. From the table 4, we
can see that FedMR uses less communication rounds (best speedup) to reach the given accuracy, indicating
that FedMR is a communication-efficient approach.
Local Burden Concern. In real-world federated applications, local clients might be mobile phones or
other small devices. Thus, the burden of local training can be the bottleneck for clients. In Figure 4(a), we
compute the number of parameters that needs to be saved in local clients and the average local computation
time per round. As can be seen, FedDC, FedDyn and MOON require triple or even quadruple storing
memory than FedAvg, while FedProc and FedMR only need little space to additionally store prototypes.
In terms of local computation time, FedMR requires more time to carefully reshape the feature space. To
handle some computing-restricted clients, we provide light versions of FedMR, namely Lite 10 and Lite 50,
where local clients randomly select only 10 or 50 samples to compute inter-class loss. From Figure 4(a), the
training time of Lite 10 and Lite 50 decreases sharply, while their performance is still competitive and better
than other baselines, as shown in Figure 4(b). Please refer to Appendix C.6 for more details.
Privacy Concern. Although prototypes are population-level statistics of features instead of raw features,
which are relatively safe (Mu et al. (2021); Tan et al. (2022)), it might be still hard for some clients with
extreme privacy limitations. To deal with this case, one possible compromise is allowing partial local clients
not to submit prototypes. In Table 5, we verify this idea for FedMR on CIFAR10 and CIFAR100, where
at the beginning, we only randomly pre-select 50% and 80% clients to require prototypes. From Table 5,
even under the prototypes of 50% clients, FedMR still performs better than FedAvg, showing the elastic
potential of FedMR in the privacy-restricted scenarios. In the extreme case where all prototypes of clients
are disallowed to be submitted, we can remove the inter-class that depends on prototypes from FedMR and
use the vanilla federated learning with the intra-class loss. As shown in Table 6, it can achieve a promising
improvement than that without the intra-class loss. Note that, we cannot counteract the privacy concerns
of federated learning itself, and leave this in the future explorations.
11Published in Transactions on Machine Learning Research (10/2023)
Table 7: Variance of top 50 egienvalues of covariance matrices of all classes within a mini-batch on CIFAR10.
Method P5C2 P10C2 P10C3 P10C5 IID
FedAvg 1201 1274 1055 814 640
FedProx 931 977 874 727 582
FedProc 1044 1074 955 739 522
FedNova 1234 1277 977 755 572
MOON 749 866 774 744 579
FedDyn 854 906 844 759 599
FedDC 1077 1104 784 766 572
FedMR (intra) 478 437 372 407 538
FedMR (inter+intra) 570 538 566 579 635
Table 8: The average accuracy of all methods adopted in Table 1 with or without the aid of prototypes on
FMNIST. Since FedProc is the prototype version of FedAvg, here we don’t show them.
Method FedProx FedNova MOON FedDyn FedDC FedMR
w/o Prototypes 79.89 75.84 79.87 80.26 80.0981.03
w Prototypes 80.29 78.76 80.12 81.21 80.5783.45
Decorrelating Analysis Here, we empirically verify the effectiveness of FedGELA compared with all
methods on constraining feature spaces from the view of the variance of eigenvalues of the covariance matrix
M. In Table 7, we show the variance of top-50 eigenvalues (sort from the largest to the smallest) of the
covariance matrix M within a mini-batch (batchsize is 128) after training 100 rounds on CIFAR10, calculated
as:1
128/summationtext50
i=1(λi−1
50/summationtext50
j=1λj)2.As can be seen, when the PCDD problem eased, the variance of FedAvg
gradually drops, which means in more uniform data distribution, the variance should be relatively small.
The slight and sharp decreasing variance of prior federated methods and our FedMR indicate that they
indeed help but still suffer from the dimensional collapse problem caused by PCDD while FedMR successfully
decorrelates the dimensions. However the variance under the intra-class loss is too small and far away from
the values of FedAvg in the IID setting, meaning it may enlarge the risk of the space invasion. In order to
prevent space invasion, our inter-class loss provide a margin for the feature space expansion. In the table,
we could see that the variance of FedMR (under the intra-class loss and the inter-class loss) is a little larger
compared to FedMR (the intra-class loss) and approaches to the FedAvg under the IID setting.
Ablation Study. FedMR introduces two interplaying losses, the intra-class loss and the inter-class loss, to
vanilla FL. To verify the individual efficiency, we conduct an ablation experiment in Table 6. As can be seen,
the intra-class loss generally plays a more important role in the performance improvement of FedMR, but
their combination complements each other and thus performs best than any of the single loss, confirming our
intuition to prevent the collapse and space invasion under PCDD jointly. Besides, as FedMR and FedProc
need local clients additionally share class prototypes which might not fair for other baselines, in Table 8, we
properly configure all baselines with prototypes on FMNIST to show the superiority of FedMR.
5 Conclusion
In this work, we study the problem of partially class-disjoint data (PCDD) in federated learning, which is
practical and challenging due to the unique collapse and invasion problems, and propose a novel approach
called FedMR to address the dilemma of PCDD. Theoretically, we show how the proposed two interplaying
losses in FedMR to prevent the collapse and guarantee the proper margin among classes. Extensive ex-
periments show that FedMR achieves significant improvements on FedAvg under the PCDD situations and
outperforms a range of state-of-the-art methods.
Acknowledgements
The work is supported by the National Key R &D Program of China (No. 2022ZD0160702), STCSM (No.
22511106101, No. 22511105700, No. 21DZ1100100), 111 plan (No. BP0719010) and National Natural
Science Foundation of China (No. 62306178). Ziqing Fan and Ruipeng Zhang were partially supported by
Wu Wen Jun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong University.
12Published in Transactions on Machine Learning Research (10/2023)
References
Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh
Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning
Representations , 2020.
Adrien Bardes, Jean Ponce, and Yann Lecun. Vicreg: Variance-invariance-covariance regularization for
self-supervised learning. In ICLR 2022-10th International Conference on Learning Representations , 2022.
Hanna Borgli, Vajira Thambawita, Pia H Smedsrud, Steven Hicks, Debesh Jha, Sigrun L Eskeland,
Kristin Ranheim Randel, Konstantin Pogorelov, Mathias Lux, Duc Tien Dang Nguyen, et al. Hyper-
kvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy. Scientific
data, 7(1):283, 2020.
Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W Dusza,
Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion analysis toward
melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi),
hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th international symposium
on biomedical imaging (ISBI 2018) , pp. 168–172. IEEE, 2018.
Marc Combalia, Noel CF Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer Reiter,
Cristina Carrera, Alicia Barreiro, Allan C Halpern, Susana Puig, et al. Bcn20000: Dermoscopic lesions in
the wild. arXiv preprint arXiv:1908.02288 , 2019.
Jiankang Deng, Jia Guo, Jing Yang, Alexandros Lattas, and Stefanos Zafeiriou. Variational prototype
learning for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 11906–11915, 2021.
Qi Dou, Tiffany Y So, Meirui Jiang, Quande Liu, Varut Vardhanabhuti, Georgios Kaissis, Zeju Li, Weixin
Si, Heather HC Lee, Kevin Yu, et al. Federated deep learning for detecting covid-19 lung abnormalities
in ct: a privacy-preserving multinational validation study. NPJ digital medicine , 4(1):1–11, 2021.
Ziqing Fan, Yanfeng Wang, Jiangchao Yao, Lingjuan Lyu, Ya Zhang, and Qi Tian. Fedskip: Combatting
statistical heterogeneity with federated skip aggregation. In 2022 IEEE International Conference on Data
Mining (ICDM) , pp. 131–140. IEEE, 2022.
Eduardo Gaitan, Norman C Nelson, and Galen V Poole. Endemic goiter and endemic thyroid disorders.
World journal of surgery , 15(2):205–215, 1991.
Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu. Feddc: Federated learning
with non-iid data via local drift decoupling and correction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 10112–10121, 2022.
Pengfei Guo, Puyang Wang, Jinyuan Zhou, Shanshan Jiang, and Vishal M Patel. Multi-institutional col-
laborations for improving deep learning-based magnetic resonance image reconstruction using federated
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
2423–2432, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Rui Hu, Yanmin Gong, and Yuanxiong Guo. Federated learning with sparsification-amplified privacy and
adaptive optimization. In IJCAI, 2021.
Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao. On feature decorrelation
inself-supervisedlearning. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pp. 9598–9608, 2021.
13Published in Transactions on Machine Learning Research (10/2023)
Meirui Jiang, Zirui Wang, and Qi Dou. Harmofl: Harmonizing local and global drifts in federated learning on
heterogeneousmedicalimages. In Proceedings of the AAAI Conference on Artificial Intelligence , volume36,
pp. 1087–1095, 2022.
LiJing, PascalVincent, YannLeCun, andYuandongTian. Understandingdimensionalcollapseincontrastive
self-supervised learning. In International Conference on Learning Representations , 2021.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. Foundations and Trends ®in Machine Learning , 14(1–2):1–210, 2021.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International
Conference on Machine Learning , pp. 5132–5143. PMLR, 2020.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Ning Li, Tao Li, Chunyu Hu, Kai Wang, and Hong Kang. A benchmark of ocular disease intelligent recogni-
tion: One shot for multi-disease detection. In Benchmarking, Measuring, and Optimizing: Third Bench-
Council International Symposium, Bench 2020, Virtual Event, November 15–16, 2020, Revised Selected
Papers 3 , pp. 177–193. Springer, 2021a.
Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 10713–10722, 2021b.
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on
federated learning systems: vision, hype and reality for data privacy and protection. IEEE Transactions
on Knowledge and Data Engineering , 2021c.
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An
experimental study. In 2022 IEEE 38th International Conference on Data Engineering (ICDE) , pp. 965–
978. IEEE, 2022.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods,
and future directions. IEEE Signal Processing Magazine , 37(3):50–60, 2020a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. Proceedings of Machine Learning and Systems , 2:429–450, 2020b.
Xinle Liang, Yang Liu, Tianjian Chen, Ming Liu, and Qiang Yang. Federated transfer reinforcement learning
for autonomous driving. arXiv preprint arXiv:1910.06001 , 2019.
Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learning: A survey. arXiv preprint
arXiv:2003.02133 , 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics , pp. 1273–1282. PMLR, 2017.
Xutong Mu, Yulong Shen, Ke Cheng, Xueli Geng, Jiaxuan Fu, Tao Zhang, and Zhiwei Zhang. Fedproc:
Prototypical contrastive federated learning on non-iid data. arXiv preprint arXiv:2109.12273 , 2021.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised
Feature Learning , 2011.
Sangjoon Park, Gwanghyun Kim, Jeongsol Kim, Boah Kim, and Jong Chul Ye. Federated split task-agnostic
vision transformer for covid-19 cxr diagnosis. Advances in Neural Information Processing Systems , 34,
2021.
14Published in Transactions on Machine Learning Research (10/2023)
Shiva Raj Pokhrel and Jinho Choi. Federated learning with blockchain for autonomous vehicles: Analysis
and design challenges. IEEE Transactions on Communications , 68(8):4734–4746, 2020.
William Shakespeare et al. William Shakespeare: the complete works . Barnes & Noble Publishing, 1989.
Yujun Shi, Kuangqi Zhou, Jian Liang, Zihang Jiang, Jiashi Feng, Philip HS Torr, Song Bai, and Vincent YF
Tan. Mimicking the oracle: An initial phase decorrelation approach for class incremental learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 16722–16731,
2022.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM
SIGSAC conference on computer and communications security , pp. 1310–1321, 2015.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in
neural information processing systems , 30, 2017.
Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In
International conference on machine learning , pp. 6105–6114. PMLR, 2019.
Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto:
Federated prototype learning across heterogeneous clients. In AAAI Conference on Artificial Intelligence ,
volume 1, pp. 3, 2022.
Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg, Chaoyang He, Regis Loeb, Paul
Mangold, Tanguy Marchand, Othmane Marfoq, Erum Mushtaq, et al. Flamby: Datasets and benchmarks
for cross-silo federated learning in realistic healthcare settings. arXiv preprint arXiv:2210.04620 , 2022.
Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-
source dermatoscopic images of common pigmented skin lesions. Scientific data , 5(1):1–9, 2018.
JianyuWang,QinghuaLiu,HaoLiang,GauriJoshi,andHVincentPoor. Tacklingtheobjectiveinconsistency
problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481 , 2020.
Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale
benchmark for instance-level recognition and retrieval. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pp. 2575–2584, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu. Robust classification with convolutional
prototype learning. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 3474–3482, 2018.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and
applications. ACM Transactions on Intelligent Systems and Technology (TIST) , 10(2):1–19, 2019.
Jiangchao Yao, Shengyu Zhang, Yang Yao, Feng Wang, Jianxin Ma, Jianwei Zhang, Yunfei Chu, Luo Ji,
Kunyang Jia, Tao Shen, et al. Edge-cloud polarization and collaboration: A comprehensive survey for ai.
IEEE Transactions on Knowledge and Data Engineering , 35(7):6866–6886, 2022.
Rui Ye, Zhenyang Ni, Fangzhao Wu, Siheng Chen, and Yanfeng Wang. Personalized federated learning with
inferred collaboration graphs. 2023a.
Rui Ye, Mingkai Xu, Jianyu Wang, Chenxin Xu, Siheng Chen, and Yanfeng Wang. Feddisco: Federated
learning with discrepancy-aware collaboration. arXiv preprint arXiv:2305.19229 , 2023b.
Youtan Yin, Hongzheng Yang, Quande Liu, Meirui Jiang, Cheng Chen, Qi Dou, and Pheng-Ann Heng. Effi-
cient federated tumor segmentation via normalized tensor aggregation and client pruning. In International
MICCAI Brainlesion Workshop , pp. 433–443. Springer, 2022.
15Published in Transactions on Machine Learning Research (10/2023)
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference
2016. British Machine Vision Association, 2016.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning
via redundancy reduction. In International Conference on Machine Learning , pp. 12310–12320. PMLR,
2021.
Ruipeng Zhang, Ziqing Fan, Qinwei Xu, Jiangchao Yao, Ya Zhang, and Yanfeng Wang. Grace: A generalized
and personalized federated learning method for medical imaging. In International Conference on Medical
Image Computing and Computer-Assisted Intervention , pp. 14–24. Springer, 2023a.
Ruipeng Zhang, Qinwei Xu, Jiangchao Yao, Ya Zhang, Qi Tian, and Yanfeng Wang. Federated domain
generalization with generalization adjustment. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 3954–3963, 2023b.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning
with non-iid data. arXiv preprint arXiv:1806.00582 , 2018.
Ligeng Zhu and Song Han. Deep leakage from gradients. In Federated learning , pp. 17–31. Springer, 2020.
16