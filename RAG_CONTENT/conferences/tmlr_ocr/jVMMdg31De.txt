Published in Transactions on Machine Learning Research (03/2023)
A Cubic Regularization Approach for Finding Local Minimax
Points in Nonconvex Minimax Optimization
Ziyi Chen u1276972@utah.edu
Department of Electrical and Computer Engineering
University of Utah
Zhengyang Hu datou30@mail.ustc.edu.cn
School of Mathematical Sciences
University of Science and Technology of China
Qunwei Li qunwei.qw@antgroup.com
Ant Group
Zhe Wang wang.10982@osu.edu
JD company
Yi Zhou yi.zhou@utah.edu
Department of Electrical and Computer Engineering
University of Utah
Reviewed on OpenReview: https: // openreview. net/ forum? id= jVMMdg31De
Abstract
Gradient descent-ascent (GDA) is a widely used algorithm for minimax optimization. How-
ever, GDA has been proved to converge to stationary points for nonconvex minimax optimiza-
tion, which are suboptimal compared with local minimax points. In this work, we develop
cubic regularization (CR) type algorithms that globally converge to local minimax points
in nonconvex-strongly-concave minimax optimization. We first show that local minimax
points are equivalent to second-order stationary points of a certain envelope function. Then,
inspired by the classic cubic regularization algorithm, we propose an algorithm named Cubic-
LocalMinimax for finding local minimax points, and provide a comprehensive convergence
analysis by leveraging its intrinsic potential function. Specifically, we establish the global
convergence of Cubic-LocalMinimax to a local minimax point at a sublinear convergence rate
and characterize its iteration complexity. Also, we propose a GDA-based solver for solving
the cubic subproblem involved in Cubic-LocalMinimax up to certain pre-defined accuracy,
and analyze the overall gradient and Hessian-vector product computation complexities of
such an inexact Cubic-LocalMinimax algorithm. Moreover, we propose a stochastic variant
of Cubic-LocalMinimax for large-scale minimax optimization, and characterize its sample
complexity under stochastic sub-sampling. Experimental results demonstrate faster or com-
parable convergence speed of our stochastic Cubic-LocalMinimax than the state-of-the-art
algorithms such as GDA (Lin et al., 2020) and Minimax Cubic-Newton (Luo et al., 2022).
In particular, our stochastic Cubic-LocalMinimax was also faster as compared to several
other algorithms for minimax optimization on a particular adversarial loss for training a
convolutional neural network on MNIST.
1 Introduction
Minimax optimization (a.k.a. two-player sequential zero-sum games) is a popular modeling framework that
has broad applications in modern machine learning, including game theory (Ferreira et al., 2012), generative
1Published in Transactions on Machine Learning Research (03/2023)
adversarial networks (Goodfellow et al., 2014), adversarial training (Sinha et al., 2017), reinforcement learning
(Qiu et al., 2020; Ho and Ermon, 2016; Song et al., 2018), etc. A standard minimax optimization problem is
shown below, where fis a smooth bivariate function.
min
x∈Rmmax
y∈Rnf(x,y). (P)
In the existing literature, many optimization algorithms have been developed to solve different types of
minimax problems. Among them, a simple and popular algorithm is the gradient descent-ascent (GDA),
which alternates between a gradient descent update on xand a gradient ascent update on yin each iteration.
Specifically, the global convergence of GDA has been established for minimax problems under various types
of global geometries, such as convex-concave-type geometry ( fis convex in xand concave in y) (Nedić
and Ozdaglar, 2009; Du and Hu, 2019; Mokhtari et al., 2020; Zhang and Wang, 2021), bi-linear geometry
(Neumann, 1928; Robinson, 1951) and Polyak-Łojasiewicz geometry (Nouiehed et al., 2019; Yang et al., 2020),
yet these geometries are not satisfied by general nonconvex minimax problems in modern machine learning
applications. Recently, many studies proved the convergence of GDA in nonconvex minimax optimization for
both nonconvex-concave problems (Lin et al., 2020; Nouiehed et al., 2019; Xu et al., 2023) and nonconvex-
strongly-concave problems (Lin et al., 2020; Xu et al., 2023; Chen et al., 2021). In these studies, it has been
shown that GDA converges sublinearly to a stationary point where the gradient of an envelope-type function
Φ(x) := maxyf(x,y)vanishes.
Although GDA can find stationary points in nonconvex minimax optimization, the stationary points may
include candidate solutions that are far more sub-optimal than global minimax points, (e.g. saddle points of
the envelope function Φ) which are known to be the major challenge for training high-dimensional machine
learning models (Dauphin et al., 2014; Jin et al., 2017; Zhou and Liang, 2018). However, finding global
minimax points is in general NP-hard (Jin et al., 2020). Recently, Jin et al. (2020) proposed a notion of local
minimax point that is computationally tractable and is close to global minimax point (see Definition 2.1 for
the formal definition) .
In the existing literature, several studies have proposed Newton-type GDA algorithms for finding such local
minimax points. Specifically, Wang et al. (2020) proposed a Follow-the-Ridge (FR) algorithm, which is a
variant of GDA that applies a second-order correction term to the gradient ascent update. In particular, the
authors showed that any strictly stable fixed point of FR is a local minimax point, and vice versa. In another
work (Zhang et al., 2021), the authors proposed two Newton-type GDA algorithms that are proven to locally
converge to a local minimax point at a linear and super-linear convergence rate, respectively. However, these
second-order-type GDA algorithms only have asymptotic convergence guarantees that require initializing
sufficiently close to a local minimax point, and they do not have any global convergence guarantees. Therefore,
we are motivated to ask the following fundamental question.
•Q:Can we develop globally convergent algorithms that can efficiently find local minimax points in nonconvex
minimax optimization? What are their convergence rates and complexities?
In this work, we provide comprehensive answers to these questions. We develop deterministic and stochastic
cubic regularization type algorithms that globally converge to local minimax points in nonconvex minimax
optimization, and study their convergence rates, computation complexities and sample complexities.
1.1 Our Contributions
We consider the minimax optimization problem (P), where fis twice-differentiable with Lipschitz continuous
gradientandHessianandisnonconvex-strongly-concave. Inthissetting, wefirstshowthatlocalminimaxpoints
offare equivalent to second-order stationary points of the envelope function Φ(x) :=maxy∈Rnf(x,y). Then,
inspired by the classic cubic regularization algorithm, we propose an algorithm named Cubic-LocalMinimax
to find local minimax points. The algorithm uses gradient ascent to update y, which is then used to estimate
the gradient and Hessian involved in the cubic regularization update for x(see Algorithm 1 for more details).
Global convergence. We show that Cubic-LocalMinimax admits an intrinsic potential function Ht(see
Proposition 4.1) that monotonically decreases over the iterations. Based on this property, we prove that
every limit point of {xt}tgenerated by Cubic-LocalMinimax is a local minimax point. Moreover, to achieve
2Published in Transactions on Machine Learning Research (03/2023)
anϵ-accurate local minimax point, Cubic-LocalMinimax requires O/parenleftbig
L2κ1.5ϵ−3/parenrightbig
number of cubic updates and
/tildewideO/parenleftbig
L2κ2.5ϵ−3/parenrightbig
number of gradient ascent updates, where κ>1denotes the problem condition number.
GDA-Cubic solver. The updates of Cubic-LocalMinimax involve a cubic subproblem that has a very
special Hessian structure. To solve this subproblem, we reformulate it as a minimax optimization problem,
for which we develop a GDA-type solver (see Algorithm 2). We name such a variant of Cubic-LocalMinimax
as Inexact Cubic-LocalMinimax. By bounding the approximation error of the cubic solver carefully, we
establish a monotonically decreasing potential function and establish the same iteration complexity as that of
Cubic-LocalMinimax. Moreover, the total number of Hessian-vector product computations involved in the
cubic solver is of the order /tildewideO(L1κ2ϵ−4).
Sample complexity. We further develop a stochastic variant of Cubic-LocalMinimax named as Stochastic
Cubic-LocalMinimax, which applies stochastic sub-sampling to improve the sample complexity in large-scale
minimax optimization. In particular, we adopt time-varying batch sizes in a way such that the induced gradient
inexactness and Hessian inexactness are adapted to the optimization increment ∥xt−xt−1∥in the previous
iteration. Consequently, to achieve an ϵ-accurate local minimax point, stochastic Cubic-LocalMinimax
requires querying /tildewideO(κ3.5ϵ−7)number of gradient samples and /tildewideO(κ2.5ϵ−5)number of Hessian samples.
Reference Algorithm Setting f(·,y) Metric1#Gradients2
Nouiehed et al. (2019) Multi-step GDA Deterministic PL ϵ-NashO/parenleftbig
ϵ−2ln(ϵ−1)/parenrightbig
Nouiehed et al. (2019) Multi-step GDA Deterministic Concave ϵ-NashO/parenleftbig
ϵ−3.5ln(ϵ−1)/parenrightbig
Lin et al. (2020) GDA Deterministic Strongly-concave ϵ-stationary O(κ2ϵ−2)
Xu et al. (2023) AGP Deterministic Strongly-concave ϵ-stationary O(κ5ϵ−2)
Lin et al. (2020) GDA Deterministic Concave ϵ-stationary O(ϵ−6)
Xu et al. (2023) AGP Deterministic Concave ϵ-stationary O(ϵ−4)
Lin et al. (2020) GDA Stochastic Strongly-concave ϵ-stationary O(κ3ϵ−4)
Lin et al. (2020) GDA Stochastic Concave ϵ-stationary O(ϵ−8)
Xu et al. (2020b) SREDA Stochastic Strongly-concave ϵ-stationary O(κ3ϵ−3)
Qiu et al. (2020) VR-STS Stochastic Strongly-concave ϵ-stationary O(ϵ−3)
Table 1: Comparison of first-order algorithms for nonconvex minimax optimization.
1“Metric” denotes the point to which the algorithm will converge, with the following choices:
·ϵ-Nash:∥∇xf(x,y)∥≤ϵ,∥∇yf(x,y)∥≤ϵ.
·ϵ-stationary:∥∇Φ(x)∥≤ϵwhere Φ(x) := max y∈Rnf(x,y).
2“# Gradients” denotes the required number of gradient evaluations.
Reference Algorithm Setting #Gradients #Hv products1
Luo et al. (2022) Inexact MCN Deterministic /tildewideO/parenleftbig
κ2ϵ−1.5/parenrightbig
O(κ1.5ϵ−2)
Our Theorem 2 Algorithm 4 Deterministic /tildewideO/parenleftbig
κ2.5ϵ−1.5/parenrightbig/tildewideO(κ2ϵ−2)
Accelerated Algorithm 4 Deterministic /tildewideO/parenleftbig
κ2ϵ−1.5/parenrightbig/tildewideO(κ1.5ϵ−2)
Our Theorem 4 Algorithm 5 Stochastic /tildewideO/parenleftbig
κ3.5ϵ−3.5/parenrightbig/tildewideO/parenleftbig
κ2.5ϵ−2.5/parenrightbig
Table 2: Comparison of second-order algorithms for finding local minimax points in
nonconvex-strongly-concave optimization.
1The required number of Hessian-vector product evaluations.
1.2 Other Related Works
Deterministic first-order algorithms for minimax optimization : Under strongly-convex-strongly-
concave setting, Zhang and Wang (2021) obtained iteration complexity O/parenleftbig
κ1.5ln(ϵ−1)/parenrightbig
for optimistic gradient
descent-ascent (OGDA) algorithm, and Mokhtari et al. (2020) obtained iteration complexity O/parenleftbig
κln(ϵ−1)/parenrightbig
for
both OGDA and extra-gradient (EG) algorithm which applies two-step GDA in each iteration. Yang et al.
(2020) studied an alternating gradient descent-ascent (AGDA) algorithm in which the gradient ascent step uses
the current variable xt+1instead ofxtand achieved iteration complexity O/parenleftbig
κ3ln(ϵ−1)/parenrightbig
for two-sided Polyak
3Published in Transactions on Machine Learning Research (03/2023)
Łojasiewicz (PL) minimax optimization. These works require require strong geometric assumptions such
as strong convexity and two-sided PL, while our work focuses on general nonconvex minimax optimization.
Nonconvex minimax optimization is also widely studied in existing literature. For example, Xu et al. (2023)
studied an alternating gradient projection (AGP) algorithm which applies ℓ2regularizer to the local objective
function of GDA followed by projection onto the constraint sets and achieves iteration complexities O(ϵ−4)
andO(κ5ϵ−2)under nonconvex-concave and nonconvex-strongly-concave settings, respectively. Nouiehed et al.
(2019) studied multi-step GDA with multiple gradient ascent steps per iteration and iteration complexity
O/parenleftbig
ϵ−2ln(ϵ−1)/parenrightbig
under nonconvex-PL setting , and they also studied the momentum-accelerated version
which achieves iteration complexity O/parenleftbig
ϵ−3.5ln(ϵ−1)/parenrightbig
under nonconvex-concave setting. However, these works
under nonconvex setting only have convergence guarantee to stationary points, whereas all the variants of
our Cubic-LocalMinimax algorithm converge to local minimax points. More detailed comparison of the
convergence results of the above deterministic first-order algorithms are shown in Section 1.1.
Deterministic second-order algorithms for minimax optimization : Adolphs et al. (2019) analyzed
the stability of a second-order variant of the GDA algorithm. Huang et al. (2022b) also proposed a cubic
regularized Newton algorithm which solves convex-concave minimax optimization problem with O/parenleftbig
ln(ϵ−1)/parenrightbig
andO/parenleftbig
ϵ(1−θ)/(θ2)/parenrightbig
iterations respectively under Lipschitz-type and Hölder-type (with parameter θ) error
bound conditions, but the computation complexity for solving the cubic subproblem is lacking. In (Luo
et al., 2022), the authors proposed a Minimax Cubic-Newton (MCN) algorithm that is different from our
Cubic-LocalMinimax in various aspects, as we elaborate with more details in Sections 4 and 5. We list
three major differences as follows. First, we develop a GDA-based solver for solving the cubic subproblem
which does not require cubic objective evaluation, whereas they use a gradient-based solver with Chebyshev
polynomials which requires additional computation. Second, our algorithm requires much fewer gradient
ascent steps in practice since we adopt more adaptive approximation error bounds (see eqs. (3) & (4)). Third,
we develop a stochastic version of Cubic-LocalMinimax and analyze its sample complexity, which to our
knowledge has not been studied in the existing literature. More detailed comparison of the convergence
results between (Luo et al., 2022) and our work are shown in Section 1.1.
Stochastic algorithms for minimax optimization : Lin et al. (2020); Yang et al. (2020) analyzed
stochastic GDA and stochastic AGDA, respectively, which are direct extensions of GDA and AGDA to the
stochastic setting. Specifically, stochastic GDA algorithm (Lin et al., 2020) achieves sample complexities
O(κ3ϵ−4)andO(ϵ−8)under nonconvex-strongly concave and nonconvex-concave settings respectively. (Yang
et al., 2020) proposed stochastic AGDA algorithm and its accelerated version via SVRG variance reduction
technique, which achieve sample complexities O(κ5ϵ−1)andO(N+κ9)ln(ϵ−1)respectively for finite-sum
two-sided PL minimax optimization with Nsamples. First-order variance reduction reduction techniques have
also been applied to nonconvex strongly-concave stochastic minimax optimization. For example, SREDA (Xu
et al., 2020b) and STORM (Qiu et al., 2020) achieve sample complexities O(κ3ϵ−3)andO(ϵ−3)respectively.
More detailed comparison of the convergence results of the above stochastic first-order algorithms are shown in
Section 1.1. Gradient-free versions of these variance reduction techniques have also been applied to minimax
optimization, including SPIDER (Xu et al., 2020c) and STORM (Huang et al., 2022a). Compared with
our work which has convergence guarantee to local minimax points of nonconvex minimax optimization,
the above stochastic algorithms either require strong geometric assumptions such as strong convexity and
two-sided PL, or only converge to stationary points of nonconvex minimax optimization.
Cubic regularization (CR): The CR algorithm dates back to (Griewank, 1981), where global convergence is
established. In (Nesterov and Polyak, 2006), the author analyzed the convergence rate of CR to second-order
stationary points in nonconvex optimization. In (Nesterov, 2008), the authors established the sub-linear
convergence of CR for solving convex smooth problems, and they further proposed an accelerated version of CR
with improved sub-linear convergence. Yue et al. (2019) studied the asymptotic convergence properties of CR
under the error bound condition, and established the quadratic convergence of the iterates. Recently, Hallak
and Teboulle (2020) proposed a framework of two-directional method for finding second-order stationary
points in general smooth nonconvex optimization. The main idea is to search for a feasible direction toward
the solution and is not based on cubic regularization. Several other works proposed different methods to
solve the cubic subproblem of CR, e.g., (Agarwal et al., 2017; Carmon and Duchi, 2019; Cartis et al., 2011b).
Another line of work aimed at improving the computation efficiency of CR by solving the cubic subproblem
4Published in Transactions on Machine Learning Research (03/2023)
with inexact gradient and Hessian information. In particular, Ghadimi et al. (2017) proposed an inexact
CR for solving convex problem. Also, Cartis et al. (2011a) proposed an adaptive inexact CR for nonconvex
optimization, whereas Jiang et al. (2017) further studied the accelerated version for convex optimization.
Several studies explored subsampling schemes to implement inexact CR algorithms, e.g., (Kohler and Lucchi,
2017; Xu et al., 2020a; Zhou and Liang, 2018; Wang et al., 2018b).
2 Problem Formulation and Preliminaries
We consider the following standard minimax optimization problem (P), where fis a nonconvex-strongly-
concave bivariate function and is twice-differentiable. Throughout the paper, we define the envelope function
Φ(x) := maxy∈Rnf(x,y).
min
x∈Rmmax
y∈Rnf(x,y). (P)
Our goal is to develop algorithms that converge to a local minimax point of (P), which is defined as follows.
Definition 2.1 (Local minimax point Jin et al. (2020)) .A point (x∗,y∗)is a local minimax point of (P) if
y∗is a local maximum of function f(x∗,·), and there exists a constant δ0>0such thatx∗is a local minimum
of function gδ(x) := maxy:∥y−y∗∥≤δf(x,y)for anyδ∈(0,δ0].
Local minimax points are different from global minimax points, which require x∗andy∗to be the global
minimizer and global maximizer of the functions Φ(·)andf(x,·)simultaneously. In general minimax
optimization, it has been shown that global minimax points can be neither local minimax points nor even
stationary points (Jin et al., 2020). However, global minimax point necessarily implies local minimax point in
nonconvex-strongly-concave optimization. Moreover, under mild conditions, many machine learning problems
have been shown to possess local minimax points, e.g., generative adversarial networks (GANs) (Nagarajan
and Kolter, 2017; Zhang et al., 2021), distributional robust machine learning (Sinha et al., 2018), etc.
Local minimax point is also an extension (necessary condition) of local Nash equilibrium (Jin et al., 2020). A
point (x∗,y∗)is defined as local Nash equilibrium if f(x∗,y)≤f(x∗,y∗)≤f(x,y∗)for anyx,ythat satisfies
∥x−x∗∥≤δand∥y−y∗∥≤δ. In other words, local zero-duality gap minx:∥x−x∗∥≤δmaxy:∥y−y∗∥≤δf(x,y) =
maxy:∥y−y∗∥≤δminx:∥x−x∗∥≤δf(x,y)is achieved at (x∗,y∗). Such zero-duality gap condition is required for
simultaneous games where the min-player xand the max-player yact simultaneously (Jin et al., 2020). In fact,
Jin et al. (2020) pointed out that most machine learning applications are sequential games where min-player
and max-player act sequentially and the sequence (i.e., who plays first) is crucial and pre-specified. For
example, in adversarial training, classifier acts first and then the adversary generates an adversarial sample.
In GAN training, generator acts first followed by discriminator. Moreover, in sequential games, zero duality
gap does not necessarily hold, i.e., local Nash equilibrium may not exist, so local minimax solution is more
commonly used (Jin et al., 2020).
In (Jin et al., 2020), the following set of second-order conditions have been proved to be sufficient conditions
for local minimax points. Moreover, as we show later, our algorithm design is inspired by these conditions.
Definition 2.2 (Sufficient conditions for local minimax) .A point (x,y)is a local minimax point of (P) if
the following conditions hold.
1. Stationary:∇1f(x,y) =0,∇2f(x,y) =0;
2. Non-degeneracy: ∇22f(x,y)≺0, and/bracketleftbig
∇11f−∇ 12f(∇22f)−1∇21f/bracketrightbig
(x,y)≻0.
Throughout the paper, we adopt the following standard assumptions on the minimax optimization problem
(P). These conditions have been widely adopted in the related works (Lin et al., 2020; Jin et al., 2020; Zhang
et al., 2021).
Assumption 1. The minimax problem (P)satisfies:
1. Function f(·,·)isL1-smooth and function f(x,·)isµ-strongly concave for any fixed x;
2. The Hessian mappings ∇11f,∇12f,∇21f,∇22fareL2-Lipschitz continuous;
5Published in Transactions on Machine Learning Research (03/2023)
3. Function Φ(x) := maxy∈Rnf(x,y)is bounded below and has bounded sub-level sets.
To elaborate, item 1 considers the class of nonconvex-strongly-concave functions fthat has been widely
studied in the minimax optimization literature (Lin et al., 2020; Jin et al., 2020; Xu et al., 2023; Lu et al.,
2020), and it is also satisfied by many machine learning applications. Item 2 assumes that the block Hessian
matrices of fare Lipschitz continuous, which is a standard assumption for analyzing second-order optimization
algorithms (Nesterov and Polyak, 2006; Agarwal et al., 2017). Moreover, item 3 guarantees that the minimax
problem has at least one solution.
3 A Cubic Regularization Approach for Finding Local Minimax Points
In this section, we propose a cubic regularization type algorithm that leverages the cubic regularization
technique to find local minimax points of the nonconvex minimax problem (P). We first relate local minimax
points to certain second-order stationary points in Section 3.1, based on which we further develop the
algorithm in Section 3.2.
3.1 On Local Minimax and Second-Order Stationary Condition
Regarding the conditions of local minimax points listed in Definition 2.2, note that the stationary conditions in
item 1 are easy to achieve, e.g., by performing standard gradient updates. For the non-degeneracy conditions
listed in item 2, the first condition is guaranteed as f(x,·)is strongly concave. Therefore, the major challenge
is to achieve the other non-degeneracy condition/bracketleftbig
∇11f−∇ 12f(∇22f)−1∇21f/bracketrightbig
(x,y)≻0. Interestingly, in
nonconvex-strongly-concave minimax optimization, such a non-degeneracy condition has close connections to
a certain second-order stationary condition on the envelope function Φ(x), as formally stated in the following
proposition. Throughout, we denote κ=L1/µas the condition number.
Proposition 3.1. Let Assumption 1 hold. Then, the following statements hold.
1.The mapping y∗(x) := arg maxy∈Rnf(x,y)is unique and κ-Lipschitz continuous for every fixed x(Lin
et al., 2020);
2.Φ(x)isL1(1 +κ)-smooth and∇Φ(x) =∇1f(x,y∗(x))(Lin et al., 2020);
3.Define mapping G(x,y) =/bracketleftbig
∇11f−∇ 12f(∇22f)−1∇21f/bracketrightbig
(x,y). Then,Gis a Lipschitz continuous mapping
with Lipschitz constant LG=L2(1 +κ)2;
4.The Hessian of Φsatisfies∇2Φ(x) =G(x,y∗(x)), and it is Lipschitz continuous with Lipschitz constant
LΦ=LG(1 +κ) =L2(1 +κ)3.
The above proposition points out that the non-degeneracy condition G(x,y)≻0actually corresponds to
a second order stationary condition of the envelop function Φ(x). To explain more specifically, consider
a pair of points (x,y∗(x)), in whichy∗(x) := arg maxyf(x,y). Sincef(x,·)is strongly concave and y∗(x)
is the maximizer, we know that y∗(x)must satisfy the stationary condition ∇2f(x,y∗(x)) =0and the
non-degeneracy condition ∇22f(x,y∗(x))≺0. Therefore, in order to be a local minimax point, xmust satisfy
the stationary condition ∇1f(x,y∗(x)) =0and the non-degeneracy condition G(x,y∗(x))≻0, which, by
items 2 and 4 of Proposition 3.1 , are equivalent to the set of second-order stationary conditions stated in the
following fact.
Fact 1. (Evtushenko, 1974) Let Assumption 1 hold. Then, (x,y∗(x))is a local minimax point of (P) if x
satisfies the following set of second-order stationary conditions.
(Second-order stationary): ∇Φ(x) =0,∇2Φ(x)≻0.
To summarize, to find a local minimax point in nonconvex-strongly-concave minimax optimization, it suffices
to find a second-order stationary point of the smooth nonconvex envelope function Φ(x). Such a key
observation is the basis for developing our proposed algorithm in the next subsection. We also note that
the proof of Proposition 3.1 is not trivial. Specifically, we need to first develop bounds for the spectrum
6Published in Transactions on Machine Learning Research (03/2023)
norm of the block Hessian matrices in Lemma A.1 (see the first page of the appendix), which helps prove the
Lipschitz continuity of the Gmapping in item 1. Moreover, we leverage the optimality condition of f(x,·)
to derive an expression for the maximizer mapping y∗(x)(see eq. (46) in the appendix), which is used to
further prove item 2.
3.2 The Cubic-LocalMinimax Algorithm
In the existing literature, many second-order optimization algorithms have been developed for finding second-
order stationary points of nonconvex minimization problems (Nesterov and Polyak, 2006; Agarwal et al.,
2017; Yue et al., 2019; Zhou et al., 2018). Hence, one may want to apply these algorithms to minimize
the nonconvex function Φ(x)and find local minimax points of the minimax problem (P). However, these
algorithms are not directly applicable, as the function Φ(x)involves a special maximization structure and
hence its specific function form Φas well as the gradient ∇Φand Hessian∇2Φare implicit. Instead, our
algorithm design can only leverage information of the bi-variate function f.
Our algorithm design is inspired by the classic cubic regularization algorithm (Nesterov and Polyak, 2006).
Specifically, to find a second-order stationary point of the envelope function Φ(x), the conventional cubic
regularization algorithm would perform the following iterative update.
st+1∈arg min
s∇Φ(xt)⊤s+1
2s⊤∇2Φ(xt)s+1
6ηx∥s∥3,
xt+1=xt+st+1, (1)
whereηx>0is a proper learning rate. However, due to the special maximization structure of Φ, its gradient
and Hessian have complex formulas (see Proposition 3.1 ) that involve the mapping y∗(x), which cannot be
computed exactly in practice. Hence, we aim to develop an algorithm that efficiently computes approximations
of∇Φ(x),∇2Φ(x), and use them to perform the cubic regularization update.
To perform the cubic regularization update in eq. (1), we need to compute ∇Φ(xt) =∇1f(xt,y∗(xt))
and∇2Φ(xt) =G(xt,y∗(xt))(by Proposition 3.1), both of which depend on the maximizer y∗(xt)of the
functionf(xt,·). Sincef(xt,·)is strongly-concave, we can run Ntiterations of gradient ascent to obtain
an approximated maximizer /tildewideyNt≈y∗(xt), and then approximate ∇Φ(xt),∇2Φ(xt)using∇1f(xt,/tildewideyNt)and
G(xt,/tildewideyNt), respectively. Intuitively, these are good approximations due to two reasons: (i) /tildewideyNtconverges
toy∗(xt)at a fast linear convergence rate; and (ii) both ∇1fandGare shown to be Lipschitz continuous
in their second argument. We refer to this algorithm as Cubic Regularization for Local Minimax
(Cubic-LocalMinimax) , and summarize its update rule in Algorithm 1 below, which terminates whenever
the maximum of the previous two increments ∥st−1∥∨∥st∥is below a certain threshold ϵ′. Such an output
rule helps characterize the computation complexity of the algorithm. In Section 5, we provide a comprehensive
discussion on how to solve the cubic subproblem with the special Hessian matrix G(xt,yt+1)using first-order
GDA-type algorithms.
4 Convergence and Iteration Complexity of Cubic-LocalMinimax
Inthissection, westudytheglobalconvergencepropertiesandtheiterationcomplexityofCubic-LocalMinimax.
The key to our convergence analysis is characterizing an intrinsic potential function of Cubic-LocalMinimax
in nonconvex minimax optimization. We formally present this result in the following proposition.
Proposition 4.1 (Potential function) .Let Assumption 1 hold. For any α,β > 0, chooseϵ′≤αL1
βLG,
ηx≤(9LΦ+ 18α+ 28β)−1andηy=2
L1+µ. Define the potential function Ht:= Φ(xt) + (LΦ+ 2α+ 3β)∥st∥3.
Then, when Nt≥O/parenleftbig
κlnL1α∥st−1∥+L1(α+L2κ)∥st∥
LGβϵ′2/parenrightbig
, the output of Cubic-LocalMinimax satisfies the following
potential function decrease property for all t∈N.
Ht+1−Ht≤−(LΦ+α+β)/parenleftbig
∥st+1∥3+∥st∥3/parenrightbig
. (2)
Proposition 4.1 reveals that Cubic-LocalMinimax admits an intrinsic potential function Ht, which takes the
form of the envelope function Φ(x)plus the cubic increment term ∥st∥3. Moreover, the potential function Ht
7Published in Transactions on Machine Learning Research (03/2023)
Algorithm 1 Cubic-LocalMinimax
Input:Initializex0,y0, learning rates ηx,ηy, threshold ϵ′, numbers of iterations T,Nt
Define∥s0∥=ϵ′
fort= 0,1,2,...,T−1do
Initialize/tildewidey0=yt
fork= 0,1,2,...,Nt−1do
/tildewideyk+1=/tildewideyk+ηy∇2f(xt,/tildewideyk)
end
Setyt+1=/tildewideyNt. Solve the cubic problem for st+1:
argmin
s∇1f(xt,yt+1)⊤s+1
2s⊤G(xt,yt+1)s+1
6ηx∥s∥3
Updatext+1=xt+st+1
end
Output:xT′,yT′,T′= min{t:∥st−1∥∨∥st∥≤ϵ′}
is monotonically decreasing along the optimization path of Cubic-LocalMinimax, implying that the algorithm
continuously makes optimization progress.
The key for establishing such a potential function is that, by running a sufficient number of inner gradient
ascent iterations, we can obtain a sufficiently accurate approximated maximizer yt+1≈y∗(xt). Consequently,
the∇1f(xt,yt+1)andG(xt,yt+1)involved in the cubic subproblem are good approximations of ∇Φ(xt)and
∇2Φ(xt), respectively. In fact, the approximation errors are proven to satisfy the following bounds.
∥∇Φ(xt)−∇ 1f(xt,yt+1)∥≤β(∥st∥2+ϵ′2), (3)
∥∇2Φ(xt)−G(xt,yt+1)∥≤α(∥st∥+ϵ′). (4)
On one hand, the above bounds are tight enough to establish the decreasing potential function. On the
other hand, they are flexible and are adapted to the increment ∥st∥=∥xt−xt−1∥produced by the previous
cubic update. Therefore, when the increment is large (i.e., ∥st∥=O(1)≫O (ϵ′)), which usually occurs in
the early iterations, our algorithm requires only O(1)gradient ascent steps. As a comparison, the Minimax
Cubic-Newton (MCN) algorithm proposed in (Luo et al., 2022) adopts constant-level approximation errors, i.e.,
∥∇Φ(xt)−∇ 1f(xt,yt+1)∥≤O (ϵ′2)and∥∇2Φ(xt)−G(xt,yt+1)∥≤O (ϵ′)1, which requires much more gradient
ascent steps (O(ln(1/ϵ′))) in every iteration. In the converging phase where ||st||≤O (ϵ′), our algorithm
requiresO(ln(1/ϵ′))gradient ascent steps, which is of the same order as that of MCN. Combining the two
cases, our algorithm is more practical. Such an idea of adapting the inexactness to the previous increment
in eqs. (3) and (4) is further leveraged to develop a scalable stochastic variant of Cubic-LocalMinimax in
Section 6.
Based on Proposition 4.1, we obtain the following global convergence rate of Cubic-LocalMinimax to a
second-order stationary point of Φ. Throughout, we adopt the following standard measure of second-order
stationary introduced in (Nesterov and Polyak, 2006).
µ(x) =/radicalbig
∥∇Φ(x)∥∨−λmin/parenleftbig
∇2Φ(x)/parenrightbig
√33LΦ.
Intuitively, a smaller µ(x)means that the point xis closer to being second-order stationary.
Theorem 1 (Convergence and complexity of Cubic-LocalMinimax) .Let the conditions of Proposition 4.1
hold withα=β=LΦ. For any 0<ϵ≤L1√33LΦ
LG, chooseϵ′=ϵ√33LΦandT≥Φ(x0)−Φ∗+8LΦϵ′2
3LΦϵ′3. Then, the
output of Cubic-LocalMinimax satisfies
µ(xT′)≤ϵ. (5)
1Ourϵ′corresponds toO(√ϵ)in Luo et al. (2022).
8Published in Transactions on Machine Learning Research (03/2023)
Consequently, the total number of required cubic iterations satisfies T′≤O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
, and the total
number of required gradient ascent iterations satisfies/summationtextT′−1
t=0Nt≤/tildewideO/parenleftbig√L2κ2.5ϵ−3/parenrightbig
.
Remark 1: The convergence rate result in Theorem 1 is about the terminated iteration T′, which is specified
by a stopping criterion and is upper bounded by T. This is different from most of the existing last-iterate
convergence results where the last iterate Tis prespecified.
Remark 2: We note that the gradient ascent steps for updating /tildewideyk+1in Algorithm 1 can be accelerated by
using the standard Nesterov’s momentum. In this way, the total number of required gradient ascent iterations
will reduce to the order /tildewideO/parenleftbig√L2κ2ϵ−3/parenrightbig
, which matches that of the Minimax Cubic-Newton algorithm proposed
in (Luo et al., 2022).
The above theorem shows that the gradient norm ∥∇Φ(xt)∥vanishes at a sublinear rate O(T−2
3), and the
second-order stationary measure −λmin/parenleftbig
∇2Φ(x)/parenrightbig
converges at a sublinear rate O(T−1
3). Both results match
the convergence rates of the cubic regularization algorithm for nonconvex minimization (Nesterov and Polyak,
2006). As a comparison, the standard GDA does not guarantee the convergence of −λmin/parenleftbig
∇2Φ(x)/parenrightbig
, and
its convergence rate of ∥∇Φ(xt)∥is of the orderO(T−1
2)(Lin et al., 2020), which is orderwise slower than
that of Cubic-LocalMinimax. Therefore, by leveraging the curvature of the approximated Hessian matrix
G(xt,yt+1), Cubic-LocalMinimax is able to find second-order stationary points of Φat a fast rate.
We note that the proof of the global convergence results in Theorem 1 is critically based on the intrinsic
potential function Htthat we characterized in Proposition 4.1. Specifically, note that the cubic subproblem in
Cubic-LocalMinimax involves an approximated gradient ∇1f(xt,yt+1)and Hessian matrix G(xt,yt+1). Such
inexactness of the gradient and Hessian introduces non-negligible noise to the cubic regularization update of
Cubic-LocalMinimax. Consequently, Cubic-LocalMinimax cannot make monotonic progress on decreasing
the function value Φ, as opposed to the standard cubic regularization algorithm in nonconvex minimization
(which uses exact gradient and Hessian). Instead, we take a different approach and show that as long as the
gradient and Hessian approximations are sufficiently accurate, one can construct a monotonically decreasing
potential function Htthat leads to the desired global convergence guarantee.
5 How to Solve the Cubic Subproblem of Cubic-LocalMinimax?
The Cubic-LocalMinimax presented in Algorithm 1 involves a cubic subproblem that takes the following form.
st+1= arg min
sϕ(s) :=g⊤s+1
2s⊤As+1
6ηx∥s∥3, (6)
whereg=∇1f(xt,yt+1), A=H11−H12H−1
22H21withHkℓ=∇kℓf(xt,yt+1).
To solve the above cubic subproblem, one standard approach is to apply the existing gradient-based solvers
(Carmon and Duchi, 2019; Tripuraneni et al., 2018), which requires computing the Hessian-vector product
A·s. However, in Cubic-LocalMinimax, the Hessian matrix takes the complex form A=H11−H12H−1
22H21
that involves product of block Hessian matrices as well as matrix inverse. Hence, directly computing the
product of such a Hessian matrix with any vector can be highly inefficient. On the other hand, in (Luo et al.,
2022), the authors proposed two-timescale update rules for computing such Hessian-vector product, where
they approximate the matrix inverse H−1
22via Chebyshev polynomials2. To further simplify these update rules
and reduce computation, we next propose an efficient GDA-type algorithm to solve this cubic subproblem
with the special Hessian matrix A.
Our main idea is to reformulate the cubic subproblem in order to avoid the matrix inverse H−1
22involved
in the Hessian matrix A. Specifically, we observe that the above cubic subproblem can be rewritten as the
following minimax optimization problem.
min
smax
v/tildewideϕ(s,v) :=g⊤s+1
2s⊤H11s+s⊤H12v+1
2v⊤H22v+1
6ηx∥s∥3. (7)
2See eqs. (32)-(33) of (Luo et al., 2022).
9Published in Transactions on Machine Learning Research (03/2023)
To explain, note that the above bi-variate function /tildewideϕis strongly concave in vwith the unique maximizer given
byv∗(s) :=arg maxv/tildewideϕ(s,v) =−H−1
22H21s. Substituting this maximizer into the function /tildewideϕ/parenleftbig
s,·)yields the
original cubic subproblem, i.e., /tildewideϕ/parenleftbig
s,v∗(s)/parenrightbig
=ϕ(s). Moreover, since /tildewideϕ(s,v)is a nonconvex-strongly-concave
function (because H22⪯−µI), we are motivated to develop a GDA-type solver to solve it. Specifically, our
solver, named as GDA-Cubic Solver , is partially inspired by the existing gradient-based cubic solvers
(Tripuraneni et al., 2018) and is summarized in Algorithms 2 and 3. To elaborate, the solver performs updates
based on the following two cases.
•Large gradient∥g∥≥4L2
1κ2ηx: In this case, the first-order gradient gis far from being stationary, and
it is more preferable to constrain the solution of the cubic subproblem in eq. (6) to s=−γ
∥g∥gfor some
γ >0(Tripuraneni et al., 2018). In particular, the optimal choice γ∗, named as Cauchy radius, has been
shown in (Conn et al., 2000) to take the following form.
γ∗:= arg min
γ≥0ϕ/parenleftig
−γg
∥g∥/parenrightig
=/radicaligg/parenleftigηxg⊤Ag
∥g∥2/parenrightig2
+ 2ηx∥g∥−ηxg⊤Ag
∥g∥2. (8)
Here, to compute the quantityg⊤Ag
∥g∥2withA=H11−H12H−1
22H21, we propose to rewrite it as
g⊤Ag
∥g∥2=g⊤H11g
∥g∥2−(H21g)⊤w∗
∥g∥,wherew∗:=H−1
22H21g
∥g∥. (9)
Note that both H11gandH21gare Hessian-vector products that can be efficiently computed by the
popular machine learning platforms such as TensorFlow (Abadi, 2015) and PyTorch (Paszke, 2019). To
computew∗, note that it can be viewed as the unique maximizer of the µ-strongly concave problem
maxw1
2w⊤H22w−(H21g)⊤
∥g∥w. We can solve this problem by performing Kgradient descent steps (see
eq. (10)) and obtain an approximated minimizer wK≈w∗with high accuracy.
•Small gradient∥g∥<4L2
1κ2ηx: In this case, we propose to solve the equivalent cubic subproblem in eq. (7)
via a nested-loop GDA-type algorithm, as it is nonconvex-strongly-concave. Specifically, we first fix sand
maximize/tildewideϕ(s,·)via gradient ascent for multiple iterations to estimate the maximizer v∗(s)(see eq. (11)).
Then, we fix vand minimize /tildewideϕ(·,v)via one step of perturbed gradient descent (see eq. (12)).
We note that all the steps of GDA-Cubic Solver are based on computing Hessian-vector products, which can
be efficiently computed and does not require storing the Hessian matrix. Equipped with this GDA-Cubic
Solver, we propose the following Inexact Cubic-LocalMinimax algorithm summarized in Algorithm 4.
A similar algorithm named as Inexact Minimax Cubic-Newton (IMCN) with inexact cubic solver was first
proposed by (Luo et al., 2022). Our Algorithm 4 differs from IMCN in the following aspects.
•First, as mentioned in Section 4, we adopt the more relaxed adaptive gradient and Hessian approximations
in eqs. (3) & (4) for the gradient ascent steps, whereas they adopt constant approximation errors.
•Second , both our Inexact Cubic-LocalMinimax (Algorithm 4) and our GDA-based cubic solver (Algorithm
2) adopt simple termination rules that are purely based on tracking the norm of the increments ∥st−1∥,∥st∥,
which are directly accessible in each iteration. As a comparison, the termination rules of their Inexact
Minimax Cubic-Newton Algorithm and its cubic solver need to additionally track the approximate objective
function value of the cubic subproblem, which requires additional computation.
We obtain the following overall computation complexity result of Algorithm 4.
Theorem 2 (Computation complexity of Inexact Cubic-LocalMinimax) .Let Assumption 1 hold. For any 0<
ϵ≤min/parenleftig
53L1κ
228√LΦ,L2
1L−1/2
2κ1/2,L2κ2
L1/parenrightig
andδ∈(0,1), chooseϵ′=ϵ
106√LΦ,T= Θ/parenleftbig√LΦ[Φ(x0)−Φ∗+ϵ2]ϵ−3/parenrightbig
,
ηx= Θ/parenleftbig
L−1
Φ/parenrightbig
,ηy=2
L1+µandNt= Θ/parenleftig
κlnL1α∥/tildewidest−1∥+L1(α+L2κ)∥/tildewidest∥
LGϵ2/parenrightig
(see eq. (49)) in Algorithm 4. When
implementing Algorithm 2 at the t-th iteration, use hyperparameters in Lemma G.1 with δ′=δ/Tif
∥∇1f(xt,yt+1)∥≤4L2
1κ2ηx, and use those in Lemma G.2 otherwise. When implementing Algorithm 3, use
the hyperparameter choices in Lemma G.3. Then, with probability at least 1−δ, the output of Inexact
10Published in Transactions on Machine Learning Research (03/2023)
Algorithm 2 GDA-Cubic Solver
Input:Gradientg, HessiansH11,H12,H22, perturbation magnitude σ, learning rates ηx,ηv,ηs, numbers of
iterationsK,{N′
k}K−1
k=0
if∥g∥≥4L2
1κ2ηxthen
w0= 0
fork= 0,...,K−1do
wk+1=wk+ηv/parenleftig
H22wk−H21g
∥g∥/parenrightig
(10)
end
βK=g⊤
∥g∥H11g
∥g∥−(H21g)⊤wK
∥g∥
γK=/radicalbig
(ηxβK)2+ 2ηx∥g∥−ηxβK
s′
K=−γKg
else
Obtainξ∼Uniform ({x∈Rm:∥x∥= 1}).
fork= 0,...,K−1do
vk,0= 0
forℓ= 0,...,N′
k−1do
vk,ℓ+1=vk,ℓ+ηv(H⊤
12s′
k+H22vk,ℓ) (11)
end
vk=vk,N′
k.
s′
k+1=s′
k−ηs/parenleftig
g+σξ+H11s′
k+H12vk+∥s′
k∥
2ηxs′
k/parenrightig
(12)
end
end
Output:s′
K.
Algorithm 3 GDA-Cubic FinalSolver
Input:Gradientg, HessiansH11,H12,H22, learning rates ηx,ηv,ηs, numbers of iterations K,{N′
k}K−1
k=0
fork= 0,...,K−1do
vk,0= 0
forℓ= 0,...,N′
k−1do
Obtainvk,ℓ+1using eq. (11) with learning rate ηv.
end
vk=vk,N′
k.
gk=g+H11s′
k+H12vk+∥s′
k∥
2ηxs′
k (13)
s′
k+1=s′
k−ηsgk (14)
end
Output:s′
K′,K′= min{k:∥gk∥≤LΦϵ′2}
11Published in Transactions on Machine Learning Research (03/2023)
Algorithm 4 Inexact Cubic-LocalMinimax
Input:Initializex0,y0, learning rates ηx,ηy, threshold ϵ′, numbers of iterations T,Nt
Define∥/tildewides0∥=ϵ′
fort= 0,1,2,...,T−1do
Initialize/tildewidey0=yt
fork= 0,1,2,...,Nt−1do
/tildewideyk+1=/tildewideyk+ηy∇2f(xt,/tildewideyk)
end
Setyt+1=/tildewideyNt
Approximately solve the cubic problem argmins∇1f(xt,yt+1)⊤s+1
2s⊤G(xt,yt+1)s+1
6ηx∥s∥3for/tildewidest+1using
Algorithm 2 with g:=∇1f(xt,yt+1)andHkℓ:=∇kℓf(xt,yt+1)(k,ℓ∈{1,2})
Updatext+1=xt+/tildewidest+1
if∥/tildewidest−1∥∨∥/tildewidest∥≤ϵ′then
Obtain/tildewidesusing Algorithm 3 with g:=∇1f(xt,yt+1)andHkℓ:=∇kℓf(xt,yt+1)(k,ℓ∈{1,2})
T′:= min{t:∥/tildewidest−1∥∨∥/tildewidest∥≤ϵ′}←t
/tildewidexT′=xT′−1+/tildewides
Output:/tildewidexT′,yT′
end
end
Cubic-LocalMinimax satisfies
µ(/tildewidexT′)≤ϵ. (15)
Consequently, the total number of required cubic iterations satisfies T′≤O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
, the total number
of required gradient ascent iterations satisfies/summationtextT′−1
t=0Nt≤/tildewideO/parenleftbig√L2κ2.5ϵ−3/parenrightbig
, and the total number of required
Hessian-vector product computations (in Algorithms 2 & 3) is of the order /tildewideO(L1κ2ϵ−4).
Remark: We can apply the standard Nesterov’s momentum to further accelerate the convergence rate of
the gradient ascent steps of Algorithms 2, 3 and 4. The resulting total number of gradient ascent iterations
and total number of Hessian-vector products will then be improved to /tildewideO/parenleftbig√L2κ2ϵ−3/parenrightbig
and/tildewideO(L1κ1.5ϵ−4),
respectively, which match those of the Inexact Minimax Cubic-Newton algorithm in (Luo et al., 2022).
Compared with Theorem 1, it can be seen from the above Theorem 2 that the total number of cubic iterations
and that of gradient ascent iterations remain the same, demonstraing the effectiveness of our proposed
GDA-based cubic solver. Moreover, since our algorithm design and cubic solver design are different from
those of (Luo et al., 2022), our convergence proof of Theorem 2 is therefore substantially different from that
of (Luo et al., 2022) in the following aspects.
•First, our Algorithm 4 adopts the more relaxed adaptive approximation criteria (3) & (4) to save
computation in practice, and thus cannot guarantee monotonic decrease of Φ(xt). Instead, we established
Φ(xt+1)−Φ(xt)≤−(11LΦ+ 8α+ 11β)∥/tildewidest+1∥3+ (9LΦ+ 6α+ 9β)∥/tildewidest∥3(see eq. (98)), which implies that
our constructed potential function Ht:= Φ(xt) + (10LΦ+ 7α+ 10β)∥/tildewidest∥3is monotonically decreasing as
shown in Proposition G.4.
•Second, as our Inexact Cubic-LocalMinimax (Algorithm 4) uses the termination rule ∥/tildewidest−1∥∨∥/tildewidest∥≤ϵ′
that only relies on the norm of the increments, we need to prove ∥sT′∥,∥/tildewides∥≤O (ϵ′)in order to ensure the
second-order stationary condition µ(x)≤ϵ. In particular, we have proved eq. (76) in Lemma G.2, which
implies that∥/tildewidesT′∥≤ϵ′cannot hold underlarge gradient, so we conclude that ∥∇1f(xT′,yT′+1)∥≤4L2
1κ2ηx.
In this small gradient case, eq. (64) we proved in Lemma G.1 implies that the exact CR solution sT′
satisfies∥sT′∥≤3ϵ′, which combined with the final cubic solver (Algorithm 3) yields that the final CR
solution/tildewidesmust satisfy eq. (82) (i.e., ∥/tildewides∥≤7ϵ′) in Lemma G.3. As a comparison, the Inexact Minimax
12Published in Transactions on Machine Learning Research (03/2023)
Cubic-Newton Algorithm in (Luo et al., 2022) terminates based on tracking the objective function value of
the cubic subproblem, which requires additional Hessian-vector product computation in practice and does
not involve these technical developments.
6 Stochastic Cubic-LocalMinimax
In this section, we apply stochastic sub-sampling to further improve the performance and complexity of
Cubic-LocalMinimax in large-scale nonconvex minimax optimization with big data. Specifically, we consider
the following stochastic finite-sum minimax optimization problem (Q).
min
x∈Rmmax
y∈Rnf(x,y) :=1
NN/summationdisplay
i=1fi(x,y), (Q)
whereNdenotes the total number of training samples and ficorresponds to the loss function on the i-th
sample. We adopt the following assumptions.
Assumption 2. The stochastic minimax optimization problem (Q)satisfies:
1.For any sample i, functionfi(·,·)isL1-smooth, function fi(·,y)isL0-Lipschitz for any fixed y, and
functionfi(x,·)isµ-strongly concave for any fixed x;
2. For any sample i, the Hessian mappings ∇11fi,∇12fi,∇21fi,∇22fiareL2-Lipschitz continuous;
3. Function Φ(x) := maxy∈Rnf(x,y)is bounded below and has bounded sub-level sets.
Applying Cubic-LocalMinimax to solve the above problem (Q) would require querying full partial gradients
and full Hessian matrices that involve all the training samples. Instead, it is more efficient to approximate
these quantities via stochastic sub-sampling. Therefore, we propose a stochastic Cubic-LocalMinimax
algorithm, which replaces the exact quantities ∇1fandGinvolved in the cubic update by their corresponding
stochastic approximations.
Specifically, to approximate the partial gradient ∇1f, we sub-sample a mini-batch of samples B1with
replacement from the training set and construct the following sample average approximation.
/hatwide∇1f(x,y) =1
|B1|/summationdisplay
i∈B1∇1fi(x,y). (16)
On the other hand, to approximate the matrix G, we sub-sample mini-batches of samples B11,B12,B21,B22
with replacement and construct approximated Hessian matrices /hatwide∇11f,/hatwide∇12f,/hatwide∇21f,/hatwide∇22fin the same way as
above. Then, we construct the following approximation of G.
/hatwideG(x,y) =/bracketleftbig/hatwide∇11f−/hatwide∇12f(/hatwide∇22f)−1/hatwide∇21f/bracketrightbig
(x,y). (17)
We summarize the update rule of stochastic Cubic-LocalMinimax in Algorithm 5 below. In particular, we
run stochastic gradient ascent (SGA) in the inner iterations to obtain the approximated maximizer yt+1, and
its high-probability convergence rate has been established in the existing stochastic optimization literature
(Harvey et al., 2019) , as shown in Theorem 3 below.
Theorem 3. Let Assumption 2 hold. For all t,k, assume that∥∇2f(xt,/tildewideyk)∥≤L0and∥/hatwide∇2f(xt,/tildewideyk)−
∇2f(xt,/tildewideyk)∥≤1almost surely. The inner stochastic gradient ascent steps in Algorithm 5 converge at the
following rate with probability at least 1−δ.
∥yt+1−y∗(xt)∥≤O/parenleftig/radicaligg
L0ln(1/δ) +L2
0
µ2Nt/parenrightig
.
The following lemma characterizes the sample complexities of all the stochastic approximators for achieving a
certain approximation accuracy.
13Published in Transactions on Machine Learning Research (03/2023)
Algorithm 5 Stochastic Cubic-LocalMinimax
Input:Initializex0,y0, learning rates ηx, threshold ϵ′, numbers of iterations T,Nt
Define∥s0∥=ϵ′
fort= 0,1,2,...,T−1do
Initialize/tildewidey0=yt
fork= 0,1,2,...,Nt−1do
Query a sample ξto compute∇2fξ(xt,/tildewideyk)
/tildewideyk+1=/tildewideyk+2
µ(k+ 1)∇2fξ(xt,/tildewideyk)
end
Setyt+1=/summationtextNt−1
k=02k
Nt(Nt−1)/tildewideyk
Sample minibatch B1(t),B11(t),B12(t),B21(t),B22(t)to compute eq. (16) and eq. (17). Then, solve the
following cubic problem for st+1:
argmin
s/hatwide∇1f(xt,yt+1)⊤s+1
2s⊤/hatwideG(xt,yt+1)s+1
6ηx∥s∥3
Updatext+1=xt+st+1
end
Output:xT′,yT′withT′= min{t:∥st−1∥∨∥st∥≤ϵ′}
Lemma 6.1. Fix any 0<ϵ1≤2L0,0<ϵ2≤4L1and choose the following batch sizes
|B1|≥O/parenleftigL2
0
ϵ2
1lnm
δ/parenrightig
, (18)
|B11|,|B12|,|B21|,|B22|≥O/parenleftigL2
1
ϵ2
2lnm+n
δ/parenrightig
. (19)
Then, the stochastic approximators satisfy the following error bounds with probability at least 1−δ.
∥/hatwide∇1f(x,y)−∇ 1f(x,y)∥≤ϵ1, (20)
∥/hatwide∇2
kℓf(x,y)−∇2
kℓf(x,y)∥≤ϵ2,∀k,ℓ∈{1,2}, (21)
∥/hatwideG(x,y)−G(x,y)∥≤(κ+ 1)2ϵ2. (22)
Therefore, by choosing proper batch sizes, the inexactness of the stochastic gradient, Hessian and Hessian
estimators can be controlled within a desired range. From this perspective, stochastic Cubic-LocalMinimax
can be viewed as an inexact version of the Cubic-LocalMinimax algorithm.
To characterize the convergence and sample complexity of stochastic Cubic-LocalMinimax, we adopt an
adaptive inexactness criterion for the sub-sampling scheme. Specifically, we choose time-varying batch sizes
in a way such that the gradient and Hessian inexactness in iteration tare proportional to the previous
increment, i.e., ϵ1(t)∝∥st∥2,ϵ2(t)∝∥st∥. Such an adaptive inexact criterion has been justified in the cubic
regularization literature (Wang et al., 2018a; 2019) with the following advantages: 1) it is adapted to the
optimization increment and hence leads to reduced batch sizes when the increment is large in the early
iterations; 2) it makes the batch size scheduling scheme in Lemma 6.1 practical, as the batch sizes in iteration
tnow depend on the increment ∥st∥obtained in the previous iteration t−1. We also note that since the
sub-sampling scheme is adapted to the previous increment, the output rule of Algorithm 5 is designed to
control the value of both the current and the previous increments. This termination rule is critical to bound
the adapted gradient and Hessian inexactness in the analysis.
We obtain the following global convergence and sample complexity result of stochastic Cubic-LocalMinimax.
Theorem 4 (Convergence and sample complexity) .Let Assumption 2 and Theorem 3 hold. For any 0<ϵ≤
L1√33LΦ
LG, chooseϵ′=ϵ√33LΦ,ηx≤1
55LΦ,T≥Φ(x0)−Φ∗+8LΦϵ′2
3LΦϵ′3andNt≥O/parenleftbigL0ln(1/δ)+L2
0
κ−2(L2
Φ∥st∥4+ϵ4)∧L2
1(∥st∥2+ϵ2/LΦ)/parenrightbig
.
14Published in Transactions on Machine Learning Research (03/2023)
Moreover, in iteration t, choose the batch sizes according to eqs. (18)and(19)with the inexactness given by
ϵ1(t) =LΦ
2/parenleftig
∥st∥2+ϵ2
33LΦ/parenrightig
∧2L0, ϵ 2(t) =LΦ
2(κ+ 1)2/parenleftig
∥st∥+ϵ√33LΦ/parenrightig
∧4L1.
Then, the output of Stochastic Cubic-LocalMinimax satisfies
µ(xT′)≤ϵ. (23)
Consequently, the total number of cubic iterations satisfies T′≤O(√L2κ1.5ϵ−3), the total number of queried
gradient samples satisfies/summationtextT′
t′=0/parenleftbig
Nt+|B1(t)|/parenrightbig
≤O/parenleftig
L2
0κ3.5√L2
ϵ7 lnm
δ/parenrightig
, and the total number of queried Hessian
samples satisfies/summationtextT′−1
t=0/summationtext2
k=1/summationtext2
ℓ=1|Bk,ℓ(t)|≤O/parenleftig
L2
1κ2.5
√L2ϵ5lnm+n
δ/parenrightig
.
Therefore, under adaptive sub-sampling, the induced gradient and Hessian inexactness ϵ1(t),ϵ2(t)are properly
controlled so that the iteration complexity T′of stochastic Cubic-LocalMinimax remains in the same level
as that of Cubic-LocalMinimax. Moreover, as opposed to the sample complexity of Cubic-LocalMinimax
that scales linearly with regard to the data size N, the sample complexity of stochastic Cubic-LocalMinimax
is independent of N. For example, by comparing the more expensive Hessian sample complexity between
Cubic-LocalMinimax and stochastic Cubic-LocalMinimax, we conclude that stochastic sub-sampling helps
reduce the sample complexity so long as the data size is large, i.e., N≥/tildewideO/parenleftig
L2
1κ
L2ϵ2/parenrightig
.
7 Experiments
In this section, we test the numerical performance of Cubic-LocalMinimax and compare it with existing
algorithms in solving a synthetic minimax optimization problem and an adversarial deep learning problem.
All the experiments are implemented on Google Colab with Intel(R) Xeon(R) CPU (12 cores, 2.20GHz),
A100 GPU of cuda 11.2, and 83.48 GB memory. The code can be downloaded from https://github.com/
datou30/Cubic-Localminimax-experiment .
7.1 Synthetic Minimax Optimization Problem
We consider the following finite-sum minimax optimization problem with parameters x= [x1,x2,x3]∈R3
andy= [y1,y2]∈R2.
min
x∈R3max
y∈R2f(x,y) :=1
NN/summationdisplay
i=1fi(x,y),withfi(x,y) =w(x3)−y2
1
40+Aix1y1−5y2
2
2+Bix2y2,(24)
whereAi,Bi>0are independently drawn from a uniform distribution over the interval [0.5,1.5],N= 1000
is the total number of samples, and the function w(·)is a W-shaped nonconvex function whose exact form is
presented in Appendix H.1. In this setting, each function fiis nonconvex-strongly-concave.
We apply our stochastic Cubic-LocalMinimax algorithm to solve the above synthetic problem. We choose
the initialization point to be x= [0.1; 0.1; 1],y= [1; 1]and set the batch size |B1(t)|=|B11(t)|=|B12(t)|=
|B21(t)|=|B22(t)|to be 20,100,1000, respectively. We choose the number of gradient ascent steps Nt= 10
for each outer loop, the strong concavity constant µ= 1, and choose the learning rate ηx= 0.01. To solve the
cubic subproblem, we use the standard gradient descent with learning rate 0.01, as the gradient and Hessian
ofΦ(x)can be analytically computed for this synthetic problem.
Figure 1 shows the performance at each epoch (each update of xis considered as an epoch) (left figure) and
time complexity (right figure) of stochastic Cubic-LocalMinimax under different batch sizes. Here, the y-axis
denotes the function value of Φ(x) =maxyf(x,y), which we aim to minimize. It can be seen that stochastic
Cubic-LocalMinimax takes more epochs but much less time to converge under a smaller batch size. This
is because the synthetic minimax problem involves relatively small noise so that the stochastic gradients
computed over a small batch of samples are sufficiently accurate.
15Published in Transactions on Machine Learning Research (03/2023)
Figure 1: Performance at each epoch (left) and time complexity (right) of stochastic Cubic-LocalMinimax
under different batch sizes. The y-axis denotes the function value of Φ(x) =maxyf(x,y)that we aim to
minimize.
We further compare the performance of stochastic Cubic-LocalMinimax with that of the standard stochastic
GDA and second order algorithms including GDN/FR/TGDA/CN (Zhang et al., 2021) and Minimax Cubic-
Newton (MCN) algorithm (Luo et al., 2022). All the algorithms use batch size 100and the fixed initialization
pointx= [0; 0; 1],y= [1; 1]. For both stochastic Cubic-LocalMinimax and stochastic GDA , we choose the
gradient ascent steps Nt= 10andµ= 1. We do one step of cubic descent and one step of gradient descent in
each algorithm, respectively. The difference between the two algorithms is that the cubic solver in Algorithm
5 is replaced with one gradient descent step, and both cubic solver and gradient descent step have the same
learning rate 0.01. The implementation details of GDN/TGDA/FR/CN algorithms are a bit involved and
we refer them to Appendix H.3. For the MCN algorithm (Algorithm 2 of (Luo et al., 2022)), we apply 10
Nesterov’s accelerated gradient ascent steps on ywith learning rate η= 0.5and momentum coefficient θ= 0.5.
Figure 2 shows the comparison of the performance at each epoch and time complexity for both algorithms.
It can be seen that our stochastic Cubic-LocalMinimax has comparable performance to CN and MCN, but
converges faster than all the other algorithms under both number of epochs and time complexity measures,
demonstrating the effectiveness of both the cubic regularization approach and our proposed GDA-Cubic
Solver.
Figure 2: Comparison of all the algorithms for synthetic experiment.
16Published in Transactions on Machine Learning Research (03/2023)
7.2 Adversarial Deep Learning
We further compare the stochastic Cubic-LocalMinimax with the classical GDA and second order algorithms
including GDN/FR/TGDA/CN (Zhang et al., 2021) and Inexact Minimax Cubic-Newton (IMCN) algorithm
(Luo et al., 2022) in the application of adversarial deep learning, which aims to train an adversarially-robust
convolutional neural network model (see Section H.2 for the detail of the neural network structure) by solving
the following minimax optimization problem using the MNIST dataset with 50k training samples and 10k
test samples.
min
θmax
{ξi}n
i=11
nn/summationdisplay
i=1/bracketleftbig
ℓ(hθ(ξi,yi))−λ∥ξi−xi∥2/bracketrightbig
, (25)
wheren= 50kis the number of training samples, θis the parameter of the neural network hθ,(xi,yi)
corresponds to the i-th image and label respectively, ξirefers to the adversarial sample corresponding to xi,
and we choose cross-entropy loss function as ℓand penalty coefficient λ= 2.0.
When implementing stochastic Cubic-LocalMinimax (Algorithm 5), we choose batchsize |B1(t)|=|B11(t)|=
|B12(t)|=|B21(t)|=|B22(t)|= 512and implement Nt= 20gradient ascent steps with learning rate 0.1. For
the cubic solver, we implement Algorithm 2 with ηv= 1when∥g∥>10untilK= 30iterations is reached or
∥H22wk−H21g
∥g∥∥<10−3in the update rule (10); Otherwise, we choose σ= 0(i.e. no random perturbation),
K= 30,ηx= 0.01,ηs= 0.002,ηv= 0.1andN′
k= 5until either K= 30iterations is reached or the gradient
terms of both s′
kandvkare sufficiently small (i.e., max/parenleftbig
∥H⊤
12s′
k+H22vk,ℓ∥,/vextenddouble/vextenddoubleg+H11s′
k+H12vk+∥s′
k∥
2ηxs′
k/vextenddouble/vextenddouble/parenrightbig
<
10−3in the update rules (11) & (12)). When implementing the classical GDA algorithm, we replace the cubic
solver of stochastic Cubic-LocalMinimax with one stochastic gradient descent step to update θusing also
batchsize 512 and learning rate 0.01, while the rest hyperparameters are not changed. The implementation
details of GDN/TGDA/FR/CN algorithms are a bit involved and we refer them to Appendix H.3. For IMCN
algorithm (Algorithm 3 of (Luo et al., 2022)), we use ℓ= 1,µ= 0.4and the small gradient case of cubic-solver
with learning rate 0.002 and early termination rule ∥∇ϕ(s)∥<0.001whereϕis the cubic-regularization
objective function. The Chebyshev polynomial used to compute Hessian-vector product is truncated to
K′= 5terms.
Figure 3: Comparison of Φ(x)at each epoch for
adversarial deep learning. GDA, TGDA and FR
conincide.
Figure 4: Comparison of robust test accuracy at
each epoch for adversarial deep learning. GDA,
TGDA and FR conincide.
Figure 3 compares the objective function value Φ(x)at each epoch of both algorithms, which is estimated via
40 gradient ascent steps with learning rate 0.1 to obtain the approximate maximizer {ξi}n
i=1. It can be seen
that our stochastic Cubic-LocalMinimax algorithm and IMCN are comparable and both algorithms have
significantly faster convergence in terms of epoch than the other algorithms . Figure 4 further demonstrates
17Published in Transactions on Machine Learning Research (03/2023)
Figure 5: Comparison of Φ(x)per oracle call for
adversarial deep learning.
Figure 6: Comparison of robust test accuracy
per oracle call for adversarial deep learning .
Figure 7: Comparison of Φ(x)on time complex-
ity for adversarial deep learning.
Figure 8: Comparison of robust test accuracy on
time complexity for adversarial deep learning.
the advantage of stochastic Cubic-LocalMinimax algorithm and IMCN in robust test accuracy which is
estimated on test samples. It can be seen that the robust models trained by stochastic Cubic-LocalMinimax
and IMCN are also more robust in generalization.
Figure 5 and 6 compare the objective function value Φ(x)and robust test accuracy respectively per oracle
call (i.e., an evaluation of gradient or Hessian-vector product) of all the algorithms. Similarly, Figures 7 and 8
compare Φ(x)and robust test accuracy respectively on time complexity of all the algorithms. It can be seen
from these figures that our stochastic Cubic-LocalMinimax algorithm takes significantly fewer oracle calls
and less time than all the other algorithms to converge in both objective function and robust test accuracy.
8 Conclusion
We developed cubic regularization type algorithms that globally converge to local minimax points in nonconvex-
strongly-concave minimax optimization. These algorithms include the basic Cubic-LocalMinimax, Inexact
Cubic-LocalMinimax with our proposed GDA-based cubic solver and stochastic Cubic-LocalMinimax for large-
scale minimax optimization. By designing and leveraging an intrinsic potential function that monotonically
decreases over the iterations, we have obtained the computation or sample complexities required by each
algorithm to achieve an ϵ-approximate local-minimax point. Experimental results demonstrate faster
convergence of our stochastic Cubic-LocalMinimax than the standard stochastic GDA algorithm. A future
direction is to extend the proposed algorithms to bilevel optimization, which is a generalization of minimax
optimization.
18Published in Transactions on Machine Learning Research (03/2023)
References
Abadi, M. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Software available
from tensorflow.org.
Adolphs, L., Daneshmand, H., Lucchi, A., and Hofmann, T. (2019). Local saddle point optimization: A
curvature exploitation approach. In Proc. International Conference on Artificial Intelligence and Statistics
(AISTATS) , pages 486–495.
Agarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E., and Ma, T. (2017). Finding approximate local minima
faster than gradient descent. In Proc. Annual ACM SIGACT Symposium on Theory of Computing (STOC) ,
pages 1195–1199.
Carmon, Y. and Duchi, J. (2019). Gradient descent finds the cubic-regularized nonconvex newton step. SIAM
Journal on Optimization , 29(3):2146–2178.
Cartis, C., Gould, N. I. M., and Toint, P. (2011a). Adaptive cubic regularization methods for unconstrained
optimization. part ii: worst-case function- and derivative-evaluation complexity. Mathematical Programming ,
130(2):295–319.
Cartis, C., Gould, N. I. M., and Toint, P. L. (2011b). Adaptive cubic regularization methods for unconstrained
optimization. part i : motivation, convergence and numerical results. Mathematical Programming .
Chen, Z., Zhou, Y., Xu, T., and Liang, Y. (2021). Proximal gradient descent-ascent: Variable convergence
under kł geometry. In Proc. International Conference on Learning Representations (ICLR) .
Conn, A. R., Gould, N. I., and Toint, P. L. (2000). Trust region methods . SIAM.
Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014). Identifying and
attacking the saddle point problem in high-dimensional non-convex optimization. In Proc. Advances in
Neural Information Processing Systems (NeurIPS) , page 2933–2941.
Du, S. S. and Hu, W. (2019). Linear convergence of the primal-dual gradient method for convex-concave
saddle point problems without strong convexity. In Proc. International Conference on Artificial Intelligence
and Statistics (AISTATS) , pages 196–205.
Evtushenko, Y. G. (1974). Some local properties of minimax problems. USSR Computational Mathematics
and Mathematical Physics , 14(3):129–138.
Ferreira, M. A. M., Andrade, M., Matos, M. C. P., Filipe, J. A., and Coelho, M. P. (2012). Minimax theorem
and nash equilibrium. International Journal of Latest Trends in Finance & Economic Sciences .
Ghadimi, S., Liu, H., and Zhang, T. (2017). Second-order methods with cubic regularization under inexact
information. ArXiv: 1710.05782 .
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. (2014). Generative adversarial nets. In Proc. Advances in Neural Information Processing Systems
(NeurIPS) , pages 2672–2680.
Griewank, A. (1981). The modification of newton’s method for unconstrained optimization by bounding
cubic terms. Technical Report .
Hallak, N. and Teboulle, M. (2020). Finding second-order stationary points in constrained minimization: A
feasible direction approach. Journal of Optimization Theory and Applications , 186(2):480–503.
Harvey, N. J., Liaw, C., and Randhawa, S. (2019). Simple and optimal high-probability bounds for strongly-
convex stochastic gradient descent. ArXiv:1909.00843 .
Ho, J. and Ermon, S. (2016). Generative adversarial imitation learning. In Proc. Advances in Neural
Information Processing Systems (NeurIPS) , pages 4565–4573.
19Published in Transactions on Machine Learning Research (03/2023)
Huang, F., Gao, S., Pei, J., and Huang, H. (2022a). Accelerated zeroth-order and first-order momentum
methods from mini to minimax optimization. J. Mach. Learn. Res. , 23:36–1.
Huang, K., Zhang, J., and Zhang, S. (2022b). Cubic regularized newton method for the saddle point models:
A global and local convergence analysis. Journal of Scientific Computing , 91(2):1–31.
Jiang, B., Lin, T., and Zhang, S. (2017). A unified scheme to accelerate adaptive cubic regularization and
gradient methods for convex optimization. ArXiv:1710.04788 .
Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. (2017). How to escape saddle points efficiently.
InProc. International Conference on Machine Learning (ICML) , volume 70, pages 1724–1732.
Jin, C., Netrapalli, P., and Jordan, M. I. (2020). What is local optimality in nonconvex-nonconcave minimax
optimization? In Proc. International Conference on Machine Learning (ICML) .
Kohler, J. M. and Lucchi, A. (2017). Sub-sampled cubic regularization for non-convex optimization. In Proc.
International Conference on Machine Learning (ICML) , volume 70, pages 1895–1904.
Lin, T., Jin, C., and Jordan, M. I. (2020). On gradient descent ascent for nonconvex-concave minimax
problems. In Proc. International Conference on Machine Learning (ICML) .
Lu, S., Tsaknakis, I., Hong, M., and Chen, Y. (2020). Hybrid block successive approximation for one-sided
non-convex min-max problems: algorithms and applications. IEEE Transactions on Signal Processing .
Luo, L., Li, Y., and Chen, C. (2022). Finding second-order stationary points in nonconvex-strongly-concave
minimax optimization. In Proc. Advances in Neural Information Processing Systems (Neurips) .
Mokhtari, A., Ozdaglar, A., and Pattathil, S. (2020). A unified analysis of extra-gradient and optimistic
gradient methods for saddle point problems: Proximal point approach. In Proc. International Conference
on Artificial Intelligence and Statistics (AISTATS) , pages 1497–1507.
Nagarajan, V. and Kolter, J. Z. (2017). Gradient descent gan optimization is locally stable. In Proc. Advances
in Neural Information Processing Systems (Neurips) , page 5591–5600.
Nedić, A. and Ozdaglar, A. (2009). Subgradient methods for saddle-point problems. Journal of optimization
theory and applications , 142(1):205–228.
Nesterov, Y. (2008). Accelerating the cubic regularization of newton’s method on convex problems. Mathe-
matical Programming , 112(1):159–181.
Nesterov, Y. and Polyak, B. (2006). Cubic regularization of newton’s method and its global performance.
Mathematical Programming .
Neumann, J. v. (1928). Zur theorie der gesellschaftsspiele. Mathematische annalen , 100(1):295–320.
Nouiehed, M., Sanjabi, M., Huang, T., Lee, J. D., and Razaviyayn, M. (2019). Solving a class of non-convex
min-max games using iterative first order methods. In Proc. Advances in Neural Information Processing
Systems (NeurIPS) , pages 14934–14942.
Paszke, A. (2019). Pytorch: An imperative style, high-performance deep learning library. In Proc. Advances
in Neural Information Processing Systems (Neurips) , pages 8024–8035.
Qiu, S., Yang, Z., Wei, X., Ye, J., and Wang, Z. (2020). Single-timescale stochastic nonconvex-concave
optimization for smooth nonlinear td learning. ArXiv:2008.10103 .
Robinson, J. (1951). An iterative method of solving a game. Annals of mathematics , 54(2):296–301.
Sinha, A., Namkoong, H., andDuchi, J.(2018). Certifiabledistributionalrobustnesswithprincipledadversarial
training. In Proc. International Conference on Learning Representations (ICLR) .
Sinha, A., Namkoong, H., and Duchi, J. C. (2017). Certifying some distributional robustness with principled
adversarial training. In Proc. International Conference on Learning Representations (ICLR) .
20Published in Transactions on Machine Learning Research (03/2023)
Song, J., Ren, H., Sadigh, D., and Ermon, S. (2018). Multi-agent generative adversarial imitation learning.
InProc. Advances in Neural Information Processing Systems (NeurIPS) , pages 7461–7472.
Tripuraneni, N., Stern, M., Jin, C., Regier, J., and Jordan, M. I. (2018). Stochastic cubic regularization
for fast nonconvex optimization. In Proc. Advances in Neural Information Processing Systems (Neurips) ,
NIPS’18, page 2904–2913.
Wang, Y., Zhang, G., and Ba, J. (2020). On solving minimax optimization locally: A follow-the-ridge
approach. In Proc. International Conference on Learning Representations (ICLR) .
Wang, Z., Zhou, Y., Liang, Y., and Lan, G. (2018a). A note on inexact condition for cubic regularized
newton’s method. ArXiv:1808.07384 .
Wang, Z., Zhou, Y., Liang, Y., and Lan, G. (2018b). Sample Complexity of Stochastic Variance-Reduced
Cubic Regularization for Nonconvex Optimization. ArXiv:1802.07372v1 .
Wang, Z., Zhou, Y., Liang, Y., and Lan, G. (2019). Stochastic variance-reduced cubic regularization
for nonconvex optimization. In Proc. International Conference on Artificial Intelligence and Statistics
(AISTATS) , pages 2731–2740.
Xu, P., Roosta, F., and Mahoney, M. W. (2020a). Newton-type methods for non-convex optimization under
inexact hessian information. Mathematical Programming , 184(1):35–70.
Xu, T., Wang, Z., Liang, Y., and Poor, H. V. (2020b). Enhanced first and zeroth order variance reduced
algorithms for min-max optimization. ArXiv:2006.09361 .
Xu, T., Wang, Z., Liang, Y., and Poor, H. V. (2020c). Gradient free minimax optimization: Variance
reduction and faster convergence. ArXiv:2006.09361 .
Xu, Z., Zhang, H., Xu, Y., and Lan, G. (2023). A unified single-loop alternating gradient projection algorithm
for nonconvex–concave and convex–nonconcave minimax problems. Mathematical Programming , pages
1–72.
Yang, J., Kiyavash, N., and He, N. (2020). Global convergence and variance reduction for a class of
nonconvex-nonconcave minimax problems. In Proc. Advances in Neural Information Processing Systems
(NeurIPS) .
Yue, M.-C., Zhou, Z., and Man-Cho So, A. (2019). On the quadratic convergence of the cubic regularization
method under a local error bound condition. SIAM Journal on Optimization , 29(1):904–932.
Zhang, G. and Wang, Y. (2021). On the suboptimality of negative momentum for minimax optimization. In
Proc. International Conference on Artificial Intelligence and Statistics (AISTATS) , pages 2098–2106.
Zhang, G., Wu, K., Poupart, P., and Yu, Y. (2021). Newton-type methods for minimax optimization.
arXiv:2006.14592 .
Zhou, Y. and Liang, Y. (2018). Critical points of linear neural networks: Analytical forms and landscape
properties. In Proc. International Conference on Learning Representations (ICLR) .
Zhou, Y., Wang, Z., and Liang, Y. (2018). Convergence of cubic regularization for nonconvex optimization
under kl property. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , pages 3760–3769.
21Published in Transactions on Machine Learning Research (03/2023)
A Supporting Lemmas
We first prove the following auxiliary lemma that bounds the spectral norm of the Hessian matrices.
Lemma A.1. Let Assumption 1 hold. Then, for any x∈Rmandy∈Rn, the Hessian matrices of f(x,y)
andG(x,y) =/bracketleftbig
∇11f−∇ 12f(∇22f)−1∇21f/bracketrightbig
(x,y)satisfy the following bounds.
∥[∇22f(x,y)]−1∥≤µ−1, (26)
∥∇12f(x,y)∥=∥∇21f(x,y)∥≤L1, (27)
∥∇11f(x,y)∥≤L1, (28)
∥G(x,y)∥≤L1(1 +κ). (29)
The same bounds also hold for /hatwide∇11f,/hatwide∇12f,/hatwide∇21f,/hatwide∇22fand/hatwideGdefined in Section 6 under Assumption 2.
Proof.We first prove eq. (26). Consider any x∈Rmandy∈Rn. By Assumption 1 we know that f(x,·)is
µ-strongy concave, which implies that −∇22f(x,y)⪰µI. Thus, we further conclude that
∥[∇22f(x,y)]−1∥=λmax/parenleftbig
[−∇22f(x,y)]−1/parenrightbig
=/parenleftig
λmin/parenleftbig
−∇ 22f(x,y)/parenrightbig/parenrightig−1
≤µ−1.
Next, we prove eq. (27). Consider any x,u∈Rmandy∈Rn, we have
∥∇21f(x,y)u∥=/vextenddouble/vextenddouble/vextenddouble∂
∂t∇2f(x+tu,y)/vextendsingle/vextendsingle/vextendsingle
t=0/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddoublelim
t→01
t/bracketleftbig
∇2f(x+tu,y)−∇ 2f(x,y)/bracketrightbig/vextenddouble/vextenddouble/vextenddouble
= lim
t→01
|t|/vextenddouble/vextenddouble∇2f(x+tu,y)−∇ 2f(x,y)/vextenddouble/vextenddouble
≤lim
t→0L1
|t|/vextenddouble/vextenddoubletu/vextenddouble/vextenddouble=L1∥u∥, (30)
which implies that ∥∇21f(x,y)∥≤L1. Sincefis twice differentiable and has continuous second-order
derivative, we have ∇12f(x,y)⊤=∇21f(x,y), and hence eq. (27) follows. The proof of eq. (28) is similar.
Finally, eq. (29) can be proved as follows using eqs. (26)&(27).
∥G(x,y)∥≤∥∇ 11f(x,y)∥+∥∇12f(x,y)∥∥∇ 22f−1(x,y)∥∥∇ 21f(x,y)∥≤L1+L1µ−1L1=L1(1 +κ).
The proof is similar for the stochastic minimax optimization problem (Q) in Section 6 under Assumption 2.
The following lemma restates Lemma 3 of (Wang et al., 2018a), which states the necessary conditions of
exact CR solution .
Lemma A.2. The solution sk+1of the cubic regularization problem in Algorithm 1 satisfies the following
conditions,
∇1f(xt,yt+1) +G(xt,yt+1)st+1+1
2ηx∥st+1∥st+1=0, (31)
G(xt,yt+1) +1
2ηx∥st+1∥I⪰O, (32)
∇1f(xt,yt+1)⊤st+1+1
2s⊤
t+1G(xt,yt+1)st+1≤−1
4ηx∥st+1∥3. (33)
22Published in Transactions on Machine Learning Research (03/2023)
Lemma A.3. Suppose the gradient ∇1f(xt,yt+1)and Hessian G(xt,yt+1)involved in the cubic-regularization
step in Algorithm 1 satisfy the following bounds for all t≤T′−1withT′=min{t≥0 :∥st−1∥∨∥st∥≤ϵ′}:
∥∇Φ(xt)−∇ 1f(xt,yt+1)∥≤β(∥st∥2+ϵ′2), (34)
∥∇2Φ(xt)−G(xt,yt+1)∥≤α(∥st∥+ϵ′). (35)
Then, choosing ηx≤(9LΦ+ 18α+ 28β)−1, the sequence{xt}tgenerated by Algorithm 1 satisfies that for all
t≤T′−2,
Ht+1−Ht≤−(LΦ+α+β)(∥st+1∥3+∥st∥3), (36)
whereHt= Φ(xt) + (LΦ+ 2α+ 3β)∥xt−xt−1∥3. The same conclusion holds for Algorithm 5 by replacing
∇1f(xt,yt+1),G(xt,yt+1)with their stochastic estimators /hatwide∇1f(xt,yt+1),/hatwideG(xt,yt+1)respectively.
This is a key lemma which provides per-iteration decrease of the potential function given that we can access
sufficiently accurate gradient ∇1f(xt,yt+1)≈∇Φ(xt)and Hessian matrix G(xt,yt+1)≈∇2Φ(xt). In this
case, the CR objective formed by ∇1f(xt,yt+1)andG(xt,yt+1)is sufficiently close to the target CR objective
formed by∇Φ(xt)and∇2Φ(xt), so we can basically follow the standard CR convergence analysis with these
additional approximation errors.
Proof.By the Lipschitz continuity of the Hessian of Φ, we obtain that
Φ(xt+1)−Φ(xt)
≤∇Φ(xt)⊤(xt+1−xt) +1
2(xt+1−xt)⊤∇2Φ(xt)(xt+1−xt) +LΦ
6∥xt+1−xt∥3
=∇1f(xt,yt+1)⊤st+1+1
2s⊤
t+1G(xt,yt+1)st+1+LΦ
6∥st+1∥3
+/parenleftbig
∇Φ(xt)−∇ 1f(xt,yt+1)/parenrightbig⊤st+1+1
2s⊤
t+1/parenleftbig
∇2Φ(xt)−G(xt,yt+1)/parenrightbig
st+1
(i)
≤/parenleftigLΦ
6−1
4ηx/parenrightig
∥st+1∥3+β∥st+1∥(∥st∥2+ϵ′2) +α
2∥st+1∥2(∥st∥+ϵ′)
(ii)
≤/parenleftigLΦ
6−1
4ηx/parenrightig
∥st+1∥3+/parenleftigα
2+β/parenrightig
(2∥st+1∥3+∥st∥3+ϵ′3)
(iii)
≤/parenleftigLΦ
6−1
4ηx/parenrightig
∥st+1∥3+/parenleftigα
2+β/parenrightig
(3∥st+1∥3+ 2∥st∥3)
≤−/parenleftig1
4ηx−LΦ
6−3α
2−3β/parenrightig
∥st+1∥3+ (α+ 2β)∥st∥3,
(iv)
≤ − (2LΦ+ 3α+ 4β)∥st+1∥3+ (α+ 2β)∥st∥3
where (i) uses eqs. (33), (34)& (35), (ii) uses the inequality that ab2≤a3∨b3≤a3+b3,∀a,b≥0,
(iii) usesϵ′3≤∥st∥3∨∥st+1∥3≤∥st∥3+∥st+1∥3,∀0≤t≤T′−2based on the termination criterion of
T′, and (iv) uses ηx≤(9LΦ+ 18α+ 28β)−1. Eq. (36) follows from the above inequality by defining
Ht= Φ(xt) + (LΦ+ 2α+ 3β)∥xt−xt−1∥2.
Note that the cubic-regularization step in Algorithm 5 simply replaces ∇1f(xt,yt+1),G(xt,yt+1)in Algorithm
1 with their stochastic estimators /hatwide∇1f(xt,yt+1),/hatwideG(xt,yt+1)respectively. Hence, eq. (36) holds for Algorithm
5 after such replacement in eqs. (34) & (35).
Lemma A.4. Suppose all the conditions of Lemma A.3 hold. If T≥Φ(x0)−Φ∗+(LΦ+3α+4β)ϵ′2
(LΦ+α+β)ϵ′3in Algorithm
1, thenT′=min{t≥1 :∥st−1∥∨∥st∥≤ϵ′}≤Φ(x0)−Φ∗+(LΦ+3α+4β)ϵ′2
(LΦ+α+β)ϵ′3≤T. Consequently, the output of
Algorithm 1 has the following convergence rate
∥∇Φ(xT′)∥≤/parenleftig1
2ηx+LΦ+ 2α+ 2β/parenrightig
ϵ′2, (37)
23Published in Transactions on Machine Learning Research (03/2023)
∇2Φ(xT′)⪰−/parenleftig1
2ηx+LΦ+ 2α/parenrightig
ϵ′I. (38)
The same conclusion holds for Algorithm 5 by replacing ∇1f(xt,yt+1),G(xt,yt+1)in the conditions (34) &
(35) with their stochastic estimators /hatwide∇1f(xt,yt+1),/hatwideG(xt,yt+1)respectively.
This Lemma provides two nice properties of the last-iterate T′, i.e.,T′has a finite upper bound T, andxT′is
close to the second-order stationary condition ∇Φ(x) =0,∇2Φ(x)≻0given by Fact 1. These properties
can be proved respectively by two facts implied by the termination rule ∥st−1∥∨∥st∥≤ϵ′. First, before
termination (i.e., t≤T′−1), we have∥st−1∥∨∥st∥>ϵ′, which results in sufficient potential function decrease
Ht+1−Ht≤−(LΦ+α+β)ϵ′3given by eq. (36). The number T′of such sufficient decreases has to be
finitely bounded since Ht≥minxΦ(x)>−∞. Second, at the last iteration t=T′,∇1f(xt,yt+1)is very
close to 0andG(xt,yt+1)is very close to positive semi-definite based on eqs. (31) & (32). Also, the gradient
∇1f(xt,yt+1)≈∇Φ(xt)and Hessian G(xt,yt+1)≈∇2Φ(xt)haveϵ′-level small approximation error based
on eqs. (34) & (35) respectively. Hence, xtis approximately a second-order stationary point of Φ, i.e., eqs.
(37) & (38) hold.
Proof.SupposeT′≤Tdoes not hold, i.e., ∥st−1∥∨∥st∥>ϵ′,∀1≤t≤T, which implies that eq. (36) holds
for all 0≤t≤T−1based on Lemma A.3.
On one hand, telescoping eq. (36) over t= 0,1,...,T−1yield that
H0−HT≥(LΦ+α+β)T−1/summationdisplay
t=0(∥st+1∥3+∥st∥3)
≥(LΦ+α+β)T−1/summationdisplay
t=0(∥st∥∨∥st+1∥)3
>T(LΦ+α+β)ϵ′3(39)
On the other hand, recalling the definition of Htin Lemma A.3, we have
H0−HT= Φ(x0)−Φ(xT) + (LΦ+ 2α+ 3β)(∥s0∥2−∥sT∥2)
(i)
≤Φ(x0)−Φ∗+ (LΦ+ 3α+ 4β)ϵ′2, (40)
where (i) uses∥s0∥=ϵ′and Φ(xT)≥Φ∗=minx∈RmΦ(x). Note that eqs. (39) & (40) contradict.
Therefore, we must have 1≤T′≤Tfor anyT≥Φ(x0)−Φ∗+(LΦ+3α+4β)ϵ′2
(LΦ+α+β)ϵ′3, which implies that T′≤
Φ(x0)−Φ∗+(LΦ+3α+4β)ϵ′2
(LΦ+α+β)ϵ′3≤T.
Finally, we conclude that
∥∇Φ(xT′)∥
(i)=/vextenddouble/vextenddouble/vextenddouble∇Φ(xT′)−∇ 1f(xT′−1,yT′)−G(xT′−1,yT′)sT′−1
2ηx∥sT′∥sT′/vextenddouble/vextenddouble/vextenddouble
≤∥∇ Φ(xT′)−∇Φ(xT′−1)−∇2Φ(xT′−1)sT′∥+∥∇Φ(xT′−1)−∇ 1f(xT′−1,yT′)∥
+∥∇2Φ(xT′−1)sT′−G(xT′−1,yT′)sT′∥+1
2ηx∥sT′∥2
(ii)
≤/parenleftig
LΦ+1
2ηx/parenrightig
∥sT′∥2+β(∥sT′−1∥2+ϵ′2) +α(∥sT′−1∥+ϵ′)∥sT′∥
(iii)
≤/parenleftig1
2ηx+LΦ+ 2α+ 2β/parenrightig
ϵ′2, (41)
where (i) uses eq. (31), (ii) uses eqs. (34) & (35) and the item 4 of Proposition 3.1 that ∇2ΦisLΦ-Lipschitz,
and (iii) uses∥sT′−1∥∨∥sT′∥≤ϵ′. Also,
∇2Φ(xT′)(i)
⪰G(xT′−1,yT′)−∥G(xT′−1,yT′)−∇2Φ(xT′)∥I
24Published in Transactions on Machine Learning Research (03/2023)
(ii)
⪰−1
2ηx∥sT′∥I−∥G(xT′−1,yT′)−∇2Φ(xT′−1)∥I−∥∇2Φ(xT′)−∇2Φ(xT′−1)∥I
(iii)
⪰ −/parenleftig1
2ηx+LΦ/parenrightig
∥sT′∥I−α(∥sT′−1∥+ϵ′)I
(iv)
⪰ −/parenleftig1
2ηx+LΦ+ 2α/parenrightig
ϵ′I, (42)
where (i) uses Weyl’s inequality, (ii) uses eq. (32), (iii) uses eq. (35) and the item 4 of Proposition 3.1 that
∇2ΦisLΦ-Lipschitz, and (iv) uses ∥sT′−1∥∨∥sT′∥≤ϵ′.
For the stochastic minimax optimization problem (Q) in Section 6, we prove the following supporting lemma
on the error of the stochastic estimators.
Lemma A.5. Fix any 0<ϵ1≤2L0,0<ϵ2≤4L1and choose the following batch sizes
|B1|≥O/parenleftigL2
0
ϵ2
1lnm
δ/parenrightig
, (18)
|B11|,|B12|,|B21|,|B22|≥O/parenleftigL2
1
ϵ2
2lnm+n
δ/parenrightig
. (19)
Then, the stochastic approximators satisfy the following error bounds with probability at least 1−δ.
∥/hatwide∇1f(x,y)−∇ 1f(x,y)∥≤ϵ1, (20)
∥/hatwide∇2
kℓf(x,y)−∇2
kℓf(x,y)∥≤ϵ2,∀k,ℓ∈{1,2}, (21)
∥/hatwideG(x,y)−G(x,y)∥≤(κ+ 1)2ϵ2. (22)
Proof.Based on Lemmas 6 & 8 of (Kohler and Lucchi, 2017), we obtain that with probability at least 1−δ,
the following bounds hold. (We replaced δin (Kohler and Lucchi, 2017) with δ/5by applying union bound
to the following 5 events.)
∥/hatwide∇1f(x,y)−∇ 1f(x,y)∥≤4√
2L0/radicaligg
ln(10m/δ) + 1/4
|B1|≤ϵ1, (43)
∥/hatwide∇2
k,ℓf(x,y)−∇2
k,ℓf(x,y)∥≤4L1/radicaligg
ln/parenleftbig
10(m∨n)/δ/parenrightbig
|Bk,ℓ|≤ϵ2;k,ℓ∈{1,2}. (44)
Note that here we only consider the cases that |B1|,|Bk,ℓ|< Nfor allk,ℓ∈{1,2}. Otherwise,|B1|=N
yields∥/hatwide∇1f(x,y)−∇ 1f(x,y)∥= 0<ϵ1and|Bk,ℓ|=Nyields∥/hatwide∇2
k,ℓf(x,y)−∇2
k,ℓf(x,y)∥= 0<ϵ2. Hence,
in both cases, the above high probability bounds hold, which further implies eq. (22) following the argument
below.
∥/hatwideG(x,y)−G(x,y)∥
≤∥/hatwide∇11f(x,y)−∇ 11f(x,y)∥+∥(/hatwide∇12f(/hatwide∇22f)−1/hatwide∇21f)(x,y)−(∇12f(∇22f)−1∇21f)(x,y)∥
≤ϵ2+∥(/hatwide∇12f−∇ 12f)/parenleftbig
(/hatwide∇22f)−1/hatwide∇21f/parenrightbig
(x,y)∥
+∥∇12f/parenleftbig
(/hatwide∇22f)−1−(∇22f)−1/parenrightbig/hatwide∇21f(x,y)∥+∥∇12f(∇22f)−1(/hatwide∇21f−∇ 21f)(x,y)∥
(i)
≤ϵ2+ϵ2µ−1L1+L2
1∥∇22f(x,y)−1∥∥(∇22f−/hatwide∇22f)(x,y)∥∥/hatwide∇22f(x,y)−1∥+L1µ−1ϵ2
(ii)
≤ϵ2+ 2κϵ2+L2
1µ−2ϵ2≤(κ+ 1)2ϵ2. (45)
where (i) and (ii) use Lemma A.1.
Regarding the high-probability convergence rate of the inner stochastic gradient ascent (SGA) in Algorithm
5, the following result is a direct application of Theorem 3.1 in (Harvey et al., 2019).
25Published in Transactions on Machine Learning Research (03/2023)
B Proof of Proposition 3.1
Proposition B.1. Let Assumption 1 hold. Then, the following statements hold.
1.The mapping y∗(x) := arg maxy∈Rnf(x,y)is unique and κ-Lipschitz continuous for every fixed x(Lin
et al., 2020);
2.Φ(x)isL1(1 +κ)-smooth and∇Φ(x) =∇1f(x,y∗(x))(Lin et al., 2020);
3.Define mapping G(x,y) =/bracketleftbig
∇11f−∇ 12f(∇22f)−1∇21f/bracketrightbig
(x,y). Then,Gis a Lipschitz continuous mapping
with Lipschitz constant LG=L2(1 +κ)2;
4.The Hessian of Φsatisfies∇2Φ(x) =G(x,y∗(x)), and it is Lipschitz continuous with Lipschitz constant
LΦ=LG(1 +κ) =L2(1 +κ)3.
Proof.The items 1 & 2 have been proved in (Lin et al., 2020).
We first prove the item 3. Consider any x,x′∈Rmandy,y′∈Rn. For convenience we denote z= (x,y)and
z′= (x′,y′). Then, by Assumption 1 and using the bounds of Lemma A.1, we have that
∥G(x′,y′)−G(x,y)∥
≤∥∇ 11f(x′,y′)−∇ 11f(x,y)∥+∥∇12f(x′,y′)−∇ 12f(x,y)∥∥[∇22f(x′,y′)]−1∥∥∇ 21f(x′,y′)∥
+∥∇12f(x,y)∥∥[∇22f(x′,y′)]−1−[∇22f(x,y)]−1∥∥∇ 21f(x′,y′)∥
+∥∇12f(x,y)∥∥[∇22f(x,y)−1]∥∥∇ 21f(x′,y′)−∇ 21f(x,y)∥
≤L2∥z′−z∥+ (L2∥z′−z∥)µ−1L1
+L2
1∥[∇22f(x′,y′)]−1∥∥∇ 22f(x,y)−∇ 22f(x′,y′)∥∥[∇22f(x,y)]−1∥+L1µ−1(L2∥z′−z∥)
≤L2(1 + 2κ)∥z′−z∥+L2
1µ−1(L2∥z′−z∥)µ−1
≤L2(1 +κ)2∥z′−z∥.
Next, we prove the item 4. Consider any fixed x∈Rm, we know that f(x,·)achieves its maximum at y∗(x),
where the gradient vanishes, i.e., ∇2f(x,y∗(x)) =0. Thus, we further obtain that
0=∇x∇2f(x,y∗(x)) =∇21f(x,y∗(x)) +∇22f(x,y∗(x))∇y∗(x),
which implies that
∇y∗(x) =−[∇22f(x,y∗(x))]−1∇21f(x,y∗(x)). (46)
With the above equation, we take derivative of ∇Φ(x) =∇1f(x,y∗(x))and obtain that
∇2Φ(x) =∇11f(x,y∗(x)) +∇12f(x,y∗(x))∇y∗(x)
=∇11f(x,y∗(x))−∇ 12f(x,y∗(x))[∇22f(x,y∗(x))]−1∇21f(x,y∗(x)) (47)
=G(x,y∗(x)).
Moreover, we have that
∥∇2Φ(x′)−∇2Φ(x)∥=∥G(x′,y∗(x′))−G(x,y∗(x))∥
≤LG/bracketleftbig
∥x′−x∥+∥y∗(x′)−y∗(x)∥/bracketrightbig
≤LG(1 +κ)∥x′−x∥, (48)
where the last step uses the item 1 of Proposition 3.1. This proves the item 4.
26Published in Transactions on Machine Learning Research (03/2023)
C Proof of Proposition 4.1
Proposition C.1 (Potential function) .Let Assumption 1 hold. For any α,β > 0, chooseϵ′≤αL1
βLG,
ηx≤(9LΦ+ 18α+ 28β)−1andηy=2
L1+µ. Define the potential function Ht:= Φ(xt) + (LΦ+ 2α+ 3β)∥st∥3.
Then, when Nt≥O/parenleftbig
κlnL1α∥st−1∥+L1(α+L2κ)∥st∥
LGβϵ′2/parenrightbig
, the output of Cubic-LocalMinimax satisfies the following
potential function decrease property for all t∈N.
Ht+1−Ht≤−(LΦ+α+β)/parenleftbig
∥st+1∥3+∥st∥3/parenrightbig
. (2)
Note that Lemma A.3 already proves eq. (2) under the condition that the approximate gradient
∇1f(xt,yt+1)≈ ∇ Φ(xt)and Hessian G(xt,yt+1)≈ ∇2Φ(xt)are sufficiently accurate, as given by eqs.
(34) & (35) respectively. Hence, it remains to prove eqs. (34) & (35) by showing that the gradient ascent
steps on the µ-strongly convex function f(xt,y)yields sufficiently small error ∥yt+1−y∗(xt)∥as shown in eq.
(50) below.
Proof.The required number of inner gradient ascent steps is shown below
N0≥κln/parenleftbig
L1∥y0−y∗(x0)∥/(2βϵ′2)/parenrightbig
Nt≥κln/parenleftig2L1α∥st−1∥+L1(α+LGκ)∥st∥
LGβϵ′2/parenrightig
=O/parenleftig
κlnL1α∥st−1∥+L1(α+L2κ)∥st∥
LGβϵ′2/parenrightig
; 1≤t≤T′. (49)
To prove eq. (2), we first prove by induction that for any t≥0.
∥yt+1−y∗(xt)∥≤α(∥st∥+ϵ′)
LG∧β(∥st∥2+ϵ′2)
L1; 0≤t≤T′−1. (50)
Note thatyt+1is obtained by applying Ntgradient ascent steps starting from yt. Hence, by the convergence
rate of gradient ascent algorithm under strong concavity, we conclude that with learning rate ηy=2
L+µ,
∥yt+1−y∗(xt)∥≤(1−κ−1)Nt∥yt−y∗(xt)∥. (51)
Whent= 0, eq. (51) implies that
∥y1−y∗(x0)∥≤∥y0−y∗(x0)∥exp/parenleftbig
N0ln(1−κ−1)/parenrightbig
(i)
≤∥y0−y∗(x0)∥exp/parenleftig
−ln/parenleftigL1∥y0−y∗(x0)∥
2βϵ′2/parenrightig/parenrightig
=2βϵ′2
L1(ii)
≤α(∥s0∥+ϵ′)
LG∧β(∥s0∥2+ϵ′2)
L1(52)
where (i) uses eq. (49) and ln(1−x)≤−x<0forx=κ−1∈(0,1)and (ii) uses∥s0∥=ϵ′≤αL1
βLG. Hence, eq.
(50) holds when t= 0.
If eq. (50) holds for t=k−1∈[0,T′−2], then
∥yk+1−y∗(xk)∥
≤(1−κ−1)Nk∥yk−y∗(xk−1)∥+ (1−κ−1)Nk∥y∗(xk−1)−y∗(xk)∥
(i)
≤exp/parenleftbig
Nkln(1−κ−1)/parenrightbig/parenleftig/parenleftigα(∥sk−1∥+ϵ′)
LG∧β(∥sk−1∥2+ϵ′2)
L1/parenrightig
+κ∥sk∥/parenrightig
(ii)
≤exp/parenleftig
−ln/parenleftig2L1α∥sk−1∥+L1(α+LGκ)∥sk∥
LGβϵ′2/parenrightig/parenrightig/parenleftigα(∥sk−1∥+ϵ′)
LG+κ∥sk∥/parenrightig
(iii)
≤LGβϵ′2
2L1α∥sk−1∥+L1(α+LGκ)∥sk∥α(2∥sk−1∥+∥sk∥) +LGκ∥sk∥
LG
27Published in Transactions on Machine Learning Research (03/2023)
=βϵ′2
L1(iv)=αϵ′
LG∧βϵ′2
L1≤α(∥sk∥+ϵ′)
LG∧β(∥sk∥2+ϵ′2)
L1
where(i)useseq. (50)for t=k−1andthefactthat y∗isκ-Lipschitzmapping(see(Linetal.,2020;Chenetal.,
2021) for the proof), (ii) uses ln(1−κ−1)≤−κ−1and eq. (49), (iii) uses ϵ′≤∥sk−1∥∨∥sk∥≤∥sk−1∥+∥sk∥
fork≤T′−1, and (iv) uses the condition that ϵ′≤αL1
βLG. This proves eq. (50) holds for t=kand thus for
allt∈[0,T′−1], which further implies eqs. (34) & (35). Hence, by Lemma A.3, we prove that eq. (2) holds
for all 0≤t≤T′−1.
D Proof of Theorem 1
Theorem 1 (Convergence and complexity of Cubic-LocalMinimax) .Let the conditions of Proposition 4.1
hold withα=β=LΦ. For any 0<ϵ≤L1√33LΦ
LG, chooseϵ′=ϵ√33LΦandT≥Φ(x0)−Φ∗+8LΦϵ′2
3LΦϵ′3. Then, the
output of Cubic-LocalMinimax satisfies
µ(xT′)≤ϵ. (5)
Consequently, the total number of required cubic iterations satisfies T′≤O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
, and the total
number of required gradient ascent iterations satisfies/summationtextT′−1
t=0Nt≤/tildewideO/parenleftbig√L2κ2.5ϵ−3/parenrightbig
.
Eq. (5) is directly implied by the second-order stationary conditions (37) & (38). Hence, it remains to
compute the iteration numbers T′and/summationtextT′−1
t=0Ntby substituting the hyperparameters.
Proof.Substituting α=β=LΦ=L2(1 +κ)3andϵ′=ϵ√33LΦinto Lemma A.4 yields that when T≥
√33LΦ33(Φ(x0)−Φ∗)+8ϵ2
3ϵ3andηx= (55LΦ)−1, we haveT′≤√33LΦ33(Φ(x0)−Φ∗)+8ϵ2
3ϵ3 =O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
≤T,
and moreover,
∥∇Φ(xT′)∥≤/parenleftig1
2ηx+LΦ+ 2α+ 2β/parenrightig
ϵ′2≤ϵ2,
λmin/parenleftbig
∇2Φ(xT′)/parenrightbig
≥−/parenleftig1
2ηx+LΦ+ 2α/parenrightig
ϵ′≥−/radicalbig
33LΦϵ.
This proves eq. (5).
Note that the number of gradient ascent iterations Ntshould satisfy eq. (49). Substituting α=β=LΦ=
L2(1 +κ)3andϵ′=ϵ√33LΦinto eq. (49) yields that
N0≥κln/parenleftbig
L1∥y0−y∗(x0)∥/(2βϵ′2)/parenrightbig
=κln/parenleftbig
33L1∥y0−y∗(x0)∥/(2ϵ2)/parenrightbig
Nt≥κln/parenleftig2L1α∥st−1∥+L1(α+LGκ)∥st∥
LGβϵ′2/parenrightig
(i)=κln/parenleftig66L1(1 +κ)∥st−1∥+ 33L1(2 +κ)∥st∥
ϵ2/parenrightig
; 1≤t≤T′
where (i) uses LG=L2(1 +κ)2andLΦ=L2(1 +κ)3in Proposition 3.1.
When we select Ntsuch that the above holds with equality, then the total number of gradient ascent iterations
is upper bounded as follows
T′−1/summationdisplay
t=0Nt=κln/parenleftbig
33L1∥y0−y∗(x0)∥/(2ϵ2)/parenrightbig
+κT′−1/summationdisplay
t=0ln/parenleftig66L1(1 +κ)∥st−1∥+33L1(2 +κ)∥st∥
ϵ2/parenrightig
≤κln/parenleftbig
33L1∥y0−y∗(x0)∥/(2ϵ2)/parenrightbig
+κT′−1/summationdisplay
t=0ln/parenleftbig
∥st−1∥+∥st∥/parenrightbig
+κT′ln/parenleftbig
66L1ϵ−2(1 +κ)/parenrightbig
28Published in Transactions on Machine Learning Research (03/2023)
(i)
≤κln/parenleftbig
33L1∥y0−y∗(x0)∥/(2ϵ2)/parenrightbig
+κT′
31
T′T′−1/summationdisplay
t=0ln/parenleftbig
4∥st−1∥3+ 4∥st∥3/parenrightbig
+κT′ln/parenleftbig
66L1ϵ−2(1 +κ)/parenrightbig
(ii)
≤κln/parenleftbig
33L1∥y0−y∗(x0)∥/(2ϵ2)/parenrightbig
+κT′
3ln/parenleftig4
T′T′−1/summationdisplay
t=0/parenleftbig
∥st−1∥3+∥st∥3/parenrightbig/parenrightig
+κT′ln/parenleftbig
66L1ϵ−2(1 +κ)/parenrightbig
(iii)
≤κln/parenleftbig
33L1∥y0−y∗(x0)∥/(2ϵ2)/parenrightbig
+κT′
3ln/parenleftig4(H0−H∗)
3LΦT′/parenrightig
+κT′ln/parenleftbig
66L1ϵ−2(1 +κ)/parenrightbig
=/tildewideO(κT′)≤/tildewideO/parenleftbig/radicalbig
L2κ2.5ϵ−3/parenrightbig
where (i) uses (a+b)3≤4(a3+b3)for anya,b≥0, (ii) applies Jensen’s inequality to the concave function
ln(·), and (iii) telescopes eq. (2) over t= 0,1,...,T′−1.
E Proof of Theorem 3
Theorem 3. Let Assumption 2 hold. For all t,k, assume that∥∇2f(xt,/tildewideyk)∥≤L0and∥/hatwide∇2f(xt,/tildewideyk)−
∇2f(xt,/tildewideyk)∥≤1almost surely. The inner stochastic gradient ascent steps in Algorithm 5 converge at the
following rate with probability at least 1−δ.
∥yt+1−y∗(xt)∥≤O/parenleftig/radicaligg
L0ln(1/δ) +L2
0
µ2Nt/parenrightig
.
Proof.Based on Theorem 3.1 of (Harvey et al., 2019), the inner SGA steps in Algorithm 5 has the following
convergence rate with probability at least 1−δ.
f(xt,y∗(xt))−f(xt,yt+1)≤O/parenleftigL0ln(1/δ) +L2
0
µNt/parenrightig
, (53)
which byµ-strong concavity of f(xt,·)proves the following convergence rate.
∥yt+1−y∗(xt)∥≤/radicalig
2/parenleftbig
f(xt,y∗(xt))−f(xt,yt+1)/parenrightbig
/µ≤O/parenleftig/radicaligg
L0ln(1/δ) +L2
0
µ2Nt/parenrightig
.
F Proof of Theorem 4
Theorem 4 (Convergence and sample complexity) .Let Assumption 2 and Theorem 3 hold. For any 0<ϵ≤
L1√33LΦ
LG, chooseϵ′=ϵ√33LΦ,ηx≤1
55LΦ,T≥Φ(x0)−Φ∗+8LΦϵ′2
3LΦϵ′3andNt≥O/parenleftbigL0ln(1/δ)+L2
0
κ−2(L2
Φ∥st∥4+ϵ4)∧L2
1(∥st∥2+ϵ2/LΦ)/parenrightbig
.
Moreover, in iteration t, choose the batch sizes according to eqs. (18)and(19)with the inexactness given by
ϵ1(t) =LΦ
2/parenleftig
∥st∥2+ϵ2
33LΦ/parenrightig
∧2L0, ϵ 2(t) =LΦ
2(κ+ 1)2/parenleftig
∥st∥+ϵ√33LΦ/parenrightig
∧4L1.
Then, the output of Stochastic Cubic-LocalMinimax satisfies
µ(xT′)≤ϵ. (23)
Consequently, the total number of cubic iterations satisfies T′≤O(√L2κ1.5ϵ−3), the total number of queried
gradient samples satisfies/summationtextT′
t′=0/parenleftbig
Nt+|B1(t)|/parenrightbig
≤O/parenleftig
L2
0κ3.5√L2
ϵ7 lnm
δ/parenrightig
, and the total number of queried Hessian
samples satisfies/summationtextT′−1
t=0/summationtext2
k=1/summationtext2
ℓ=1|Bk,ℓ(t)|≤O/parenleftig
L2
1κ2.5
√L2ϵ5lnm+n
δ/parenrightig
.
29Published in Transactions on Machine Learning Research (03/2023)
Compared with deterministic optimization, the stochastic gradient error ∥/hatwide∇1f(xt,yt+1)−∇Φ(xt)∥results
from not only the gradient ascent induced error ∥∇1f(xt,yt+1)−∇Φ(xt)∥, but also stochastic approximation
error∥/hatwide∇1f(xt,yt)−∇ 1f(xt,yt)∥. The stochastic Hessian error ∥/hatwideG(xt,yt+1)−∇2Φ(xt)∥is similar. Hence,
the main idea is to show that after incorporating the stochastic approximation errors from Lemma 6.1, the
stochastic gradient and Hessian errors still satisfy the conditions (34) & (35), which by Lemma A.4 shows the
desired convergence result.
Proof.In Lemma 6.1, replace x,ywithxt,yt,B1withB1(t), andBk,ℓwithBk,ℓ(t)for anyk,ℓ∈{1,2}, and
substitute the following hyperparameters.
ϵ1(t) =LΦ
2/parenleftig
∥st∥2+ϵ2
33LΦ/parenrightig
∧2L0=O(LΦ∥st∥2+ϵ2),
ϵ2(t) =LΦ
2(κ+ 1)2/parenleftig
∥st∥+ϵ√33LΦ/parenrightig
∧4L1=O/parenleftbig
κ−2(LΦ∥st∥+/radicalbig
LΦϵ)/parenrightbig
.
Then, we obtain that using the following batchsizes


|B1(t)|≥O/parenleftigL2
0
(L2
Φ∥st∥4+ϵ4)lnm
δ/parenrightig
|Bk,ℓ(t)|≥O/parenleftigL2
1κ4
LΦ(LΦ∥st∥2+ϵ2)lnm+n
δ/parenrightig, (54)
the stochastic approximators satisfy the following error bounds with probability at least 1−δ.
∥/hatwide∇1f(xt,yt)−∇ 1f(xt,yt)∥≤ϵ1(t)≤LΦ
2/parenleftig
∥st∥2+ϵ2
33LΦ/parenrightig
, (55)
∥/hatwideG(xt,yt)−G(xt,yt)∥≤ϵ2(t)≤LΦ
2/parenleftig
∥st∥+ϵ√33LΦ/parenrightig
. (56)
Based on Theorem 3, using the following number of stochastic gradient ascent steps
Nt≥O/parenleftigL0ln(1/δ) +L2
0
µ2L2
Φ/parenleftbig
([∥st∥2+ϵ2/(33LΦ)]/L1)∧([∥st∥+ϵ/√33LΦ]/LG)/parenrightbig2/parenrightig
(i)=O/parenleftigL0ln(1/δ) +L2
0/parenleftbig
([LΦ∥st∥2+ϵ2]/κ)∧L1(∥st∥+ϵ/√LΦ)/parenrightbig2/parenrightig
=O/parenleftigL0ln(1/δ) +L2
0
κ−2(L2
Φ∥st∥4+ϵ4)∧L2
1(∥st∥2+ϵ2/LΦ)/parenrightig
, (57)
where (i) uses κ=L1/µandLΦ=LG(1 +κ)(Proposition 3.1). we have
∥yt+1−y∗(xt)∥≤O/parenleftig/radicaligg
L0ln(1/δ) +L2
0
µ2Nt/parenrightig
≤LΦ
2min/parenleftig1
L1/parenleftig
∥st∥2+ϵ2
33LΦ/parenrightig
,1
LG/parenleftig
∥st∥+ϵ√33LΦ/parenrightig/parenrightig
. (58)
Therefore,
∥∇Φ(xt)−/hatwide∇1f(xt,yt+1)∥
(i)
≤∥∇ 1f(xt,y∗(xt))−∇ 1f(xt,yt+1)∥+∥/hatwide∇1f(xt,yt+1)−∇ 1f(xt,yt+1)∥
(ii)
≤L1∥yt+1−y∗(xt)∥+LΦ
2/parenleftig
∥st∥2+ϵ2
33LΦ/parenrightig
(iii)
≤LΦ/parenleftig
∥st∥2+ϵ2
33LΦ/parenrightig
, (59)
30Published in Transactions on Machine Learning Research (03/2023)
where (i) uses∇Φ(x) =∇1f(x,y∗(x))in Proposition 3.1, (ii) uses eq. (55) and Assumption 1, and (iii) uses
eq. (58).
∥∇2Φ(xt)−/hatwideG(xt,yt+1)∥
(i)
≤∥G(xt,y∗(xt))−G(xt,yt+1)∥+∥/hatwideG(xt,yt+1)−G(xt,yt+1)∥
(ii)
≤LG∥yt+1−y∗(xt)∥+LΦ
2/parenleftig
∥st∥+ϵ√33LΦ/parenrightig
(iii)
≤LΦ/parenleftig
∥st∥+ϵ√33LΦ/parenrightig
, (60)
where (i) uses∇2Φ(x) =G(x,y∗(x))in Proposition 3.1, (ii) uses eq. (56) and the item 3 of Proposition 3.1, and
(iii) uses eq. (58). Eqs. (59) & (60) imply that the conditions (34) & (35) hold with α=β=LΦ=L2(1 +κ)3
andϵ′=ϵ√33LΦ. In Lemma A.4, by substituting these values of α,β,ϵ′andηx= (55LΦ)−1, we obtain that
whenT≥Φ(x0)−Φ∗+8LΦϵ′2
3LΦϵ′3, we haveT′≤√33LΦ33(Φ(x0)−Φ∗)+8ϵ2
3ϵ3 =O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
≤T, and moreover,
∥∇Φ(xT′)∥≤/parenleftig1
2ηx+LΦ+ 2α+ 2β/parenrightig
ϵ′2≤ϵ2,
λmin/parenleftbig
∇2Φ(xT′)/parenrightbig
≥−/parenleftig1
2ηx+LΦ+ 2α/parenrightig
ϵ′(i)
≥−/radicalbig
33LΦϵ,
This proves that eq. (23) holds with probability at least 1−δ.
Choosing both Ntin eq. (57) and the batchsizes in eq. (54) with equality, the number of gradient computations
has the following upper bound.
T′−1/summationdisplay
t=0/parenleftbig
Nt+|B1(t)|/parenrightbig
=T′−1/summationdisplay
t=0O/parenleftigL0ln(1/δ) +L2
0
κ−2(L2
Φ∥st∥4+ϵ4)∧L2
1(∥st∥2+ϵ2/LΦ)+L2
0
L2
Φ∥st∥4+ϵ4lnm
δ/parenrightig
≤T′−1/summationdisplay
t=0O/parenleftigL0ln(1/δ) +L2
0
ϵ4κ−2∧L2
1ϵ2/LΦ+L2
0
ϵ4lnm
δ/parenrightig
(i)
≤T′O/parenleftigL0κ2
ϵ4/parenleftbig
ln(1/δ) +L0/parenrightbig
+L2
0
ϵ4lnm
δ/parenrightig
(ii)
≤O/parenleftbig/radicalbig
L2κ1.5ϵ−3/parenrightbig
O/parenleftigL2
0κ2
ϵ4lnm
δ/parenrightig
≤O/parenleftigL2
0κ3.5√L2
ϵ7lnm
δ/parenrightig
, (61)
where (i) uses ϵ′=ϵ√33LΦ≤L1
LG=L1(1+κ)
LΦwhich implies that ϵ4κ−2≤ O (L2
1ϵ2/LΦ), (ii) uses T′≤
O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
we proved above. The number of Hessian computations has the following upper bound.
T′−1/summationdisplay
t=02/summationdisplay
k=12/summationdisplay
ℓ=1|Bk,ℓ(t)|
=T′−1/summationdisplay
t=0O/parenleftigL2
1κ4
LΦ(LΦ∥st∥2+ϵ2)lnm+n
δ/parenrightig
≤T′−1/summationdisplay
t=0O/parenleftigL2
1κ4
LΦϵ2lnm+n
δ/parenrightig
31Published in Transactions on Machine Learning Research (03/2023)
(i)
≤T′O/parenleftigL2
1κ
L2ϵ2lnm+n
δ/parenrightig
(ii)
≤O/parenleftigL2
1κ2.5
√L2ϵ5lnm+n
δ/parenrightig
(62)
where (i) uses LΦ=L2(1 +κ)3=O(L2κ3), (ii) usesT′≤√33LΦ33(Φ(x0)−Φ∗)+8ϵ2
3ϵ3 =O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
we
proved above.
G Solving Cubic-Regularization problem
In this section, we obtain the properties of the GDA-Cubic Solver (Algorithm 2) and GDA-Cubic FinalSolver
(Algorithm 3) in Lemmas G.1-G.3, and show in Proposition G.4 that Algorithm 4 using these cubic solvers
admits an intrinsic potential function Ht(see Proposition 4.1) that monotonically decreases over the iterations.
Finally, using these lemmas and Proposition G.4, we will prove Theorem 2 on the computation complexity of
Algorithm 4.
Lemma G.1. For any 0< δ′<1, when∥g∥ ≤ 4L2
1κ2ηx, implement Algorithm 2 with initialization
s′
0= 0and hyperparameters K= Θ/parenleftig
L1κηxϵ′−1/bracketleftig
ln/parenleftig
1 +√m(L2
1κ2ηx+LΦϵ′2)
LΦϵ′2δ′/parenrightig
+ln(L1κηxϵ′−1)/bracketrightig/parenrightig
],N′
k=N′=
Θ/parenleftig
ln[(L3
1κ3η2
x)/(LΦϵ′3)]
ln[(1−κ−1)−1]/parenrightig
,ηv=1
L1,ηs=1
22L1κ,σ=LΦϵ′3
108L1κηx. Then, the output s′
Ksatisfies the following
inequalities with probability at least 1−δ′wheres∗:= arg minsϕ(s).
ϕ(s′
K)−ϕ(s∗)≤ϵ′∥s∗∥2
240ηx+ 2LΦϵ′3(63)
∥s∗∥≤2∥s′
K∥+ϵ′
20+24ηxLΦϵ′3
∥s∗∥2(64)
Lemma G.1 provides convergence properties of GDA-Cubic Solver (Algorithm 2) in the small gradient case
∥g∥≤4L2
1κ2ηx. The key to prove the convergence rate (63) is to equivalently write the gradient descent step
(12) as gradient descent on the CR objective function ϕN′defined by eq. (67) below, which is ϵ′-close to the
target CR objective function ϕwith sufficiently large number N′of gradient ascent steps. Therefore, we
can leverage the existing convergence result of gradient descent on ϕN′from (Carmon and Duchi, 2019) and
extend the result to ϕ. The CR function ϕis also close to convex, so small ϕ(s′
K)−ϕ(s∗)meanss∗is not far
froms′
K, which implies eq. (64).
Proof.The gradient ascent step (11) can be rewritten as vk,ℓ+1= (I+ηvH22)vk,ℓ+ηvH⊤
12s′
k. By iterating it
overℓ= 0,1,...,N′and usingvk,0= 0, we obtain that
vk=vk,N′=ηvN′−1/summationdisplay
ℓ=0(I+ηvH22)ℓH⊤
12s′
k=−H−1
22/parenleftbig
I−(I+ηvH22)N′/parenrightbig
H⊤
12s′
k. (65)
By substituting the above equality, the gradient descent step (12) can be rewritten as
s′
k+1=s′
k−ηs/parenleftig
g+σξ+AN′s′
k+∥s′
k∥
2ηxs′
k/parenrightig
,whereAN′:=H11−H12H−1
22/parenleftbig
I−(I+ηvH22)N′/parenrightbig
H⊤
12.(66)
It can be easily verified that the above update rule (66) can be seen as gradient descent steps with random
perturbation σξon the following cubic-regularization problem
s∗
N′= arg min
sϕN′(s) :=g⊤s+1
2s⊤AN′s+1
6ηx∥s∥3(67)
Note that
∥AN′∥≤∥H11∥+∥H12∥∥H−1
22∥/vextenddouble/vextenddoubleI−(I+ηvH22)N′/vextenddouble/vextenddouble∥H12∥
32Published in Transactions on Machine Learning Research (03/2023)
(i)
≤∥H11∥+∥H12∥∥H−1
22∥∥H12∥
(ii)
≤L1+L1µ−1L1≤Amax:= 2L1κ (68)
where (i) uses−L1I⪯H22⪯−µI(Assumption 1) and ηv= 1/L1which imply that O⪯I+ηvH22⪯
(1−κ−1)Iand thus∥I−(I+ηvH22)N′∥≤ 1, and (ii) uses∥H11∥≤L1,∥H12∥≤L1,∥H−1
22∥≤µ−1
(Assumption 1). Similarly, we obtain that
∥A∥≤∥H11∥+∥H12∥∥H−1
22∥/vextenddouble/vextenddoubleH12≤Amax:= 2L1κ, (69)
and that
∥AN′−A∥=∥(I+ηvH22)N′∥(i)
≤(1−κ−1)N′∥A0−A∥(ii)
≤LΦϵ′3
(7L1κηx)2, (70)
where (i) uses−L1I⪯H22⪯−µI(Assumption 1) and ηv= 1/L1which imply that O⪯I+ηvH22⪯
(1−κ−1)I, and (ii) uses N′=ln[(98L3
1κ3η2
x)/(LΦϵ′3)]
ln[(1−κ−1)−1],A0=Oand∥A∥≤2L1κ.
Hence, the optimal solutions s∗:= arg min sϕ(s)satisfies
∥s∗∥(i)
≤∥A∥ηx+/radicalbig
(∥A∥ηx)2+ 2ηx∥g∥
(ii)
≤2∥A∥ηx+/radicalbig
2ηx∥g∥
(iii)
≤7L1κηx:=smax, (71)
wheretheproofof(i)isineq. (7a)of(CarmonandDuchi,2019), (ii)usestheinequalitythat√
a+b≤√a+√
b
for anya,b≥0, and (iii) uses∥A∥≤2L1κand∥g∥≤4L2
1κ2ηx. Similarly, s∗
N′:= arg minsϕN′(s)satisfies
∥s∗
N′∥≤smax. (72)
Then, based on Lemma 2.6 and Theorem 3.2 of (Carmon and Duchi, 2019), the gradient descent step (66)
of the cubic-regularization problem (67) yields that ∥s′
k∥≤∥s∗
N′∥≤smaxfor allkand thatϕN′(s′
k)≤
ϕN′(s∗
N′) +ϵ′∥s∗
N′∥2
240ηx+LΦϵ′3with probability at least 1−δ′, using the hyperparameter choices below which
can be easily verified to be satisfied by those used in this Lemma.
s′
0=0
ηs=1
4/parenleftbig
Amax+smax/(2ηx)/parenrightbig=1
22L1κ
σ=[ϵ′∥s∗
N′∥2/(240ηx) +LΦϵ′3]/(2ηx)
Amax+∥s∗
N′∥/ηxσ
12=(2LΦϵ′3)/(2ηx)
12(2L1κ+smax/ηx)=LΦϵ′3
108L1κηx(73)
K≥6 ln/parenleftig
1 +3√m
δ′σmin/parenrightig
+ 14 ln∥s∗
N′∥2(Amax+smax/ηx)
ϵ′∥s∗
N′∥2/(240ηx)+LΦϵ′3
(1/2)ηs10∥s∗
N′∥2
ϵ′∥s∗
N′∥2/(240ηx) +LΦϵ′3(74)
=211200L1κηx/bracketleftig
3 ln/parenleftig
1 +3√m(5L2
1κ2ηx+24LΦϵ′2)
10LΦϵ′2δ′/parenrightig
+ 7 ln2160L1κηx
ϵ′+240ηxLΦϵ′3∥s∗
N′∥−2/bracketrightig
ϵ′+ 240ηxLΦϵ′3∥s∗
N′∥−2,
where eq. (73) corresponds to σ=Amax+∥s∗
N′∥/ηx
Amax+smax/ηx2LΦϵ′3
ϵ′∥s∗
N′∥2/(240ηx)+LΦϵ′3∈/bracketleftbig
σmin,1/bracketrightbig
(σmin:=10LΦϵ′2
5L2
1κ2ηx+24LΦϵ′2,
since 0≤∥s∗
N′∥≤smax= 7L1κηx) defined in Theorem 3.2 of (Carmon and Duchi, 2019), which yields σmin
and(1/2)ηin eq. (74).
Then, eq. (63) can be proved as follows
ϕ(s′
K)−ϕ(s∗) =/parenleftbig
ϕ(s′
K)−ϕN′(s′
K)/parenrightbig
+/parenleftbig
ϕN′(s′
K)−ϕN′(s∗
N′)/parenrightbig
+/parenleftbig
ϕN′(s∗
N′)−ϕ(s∗)/parenrightbig
33Published in Transactions on Machine Learning Research (03/2023)
(i)
≤/parenleftbig
ϕ(s′
K)−ϕN′(s′
K)/parenrightbig
+ϵ′∥s∗
N′∥2
240ηx+LΦϵ′3+/parenleftbig
ϕN′(s∗)−ϕ(s∗)/parenrightbig
(ii)
≤1
2s′⊤
K(A−AN′)s′
K+ϵ′∥s∗
N′∥2
240ηx+LΦϵ′3+1
2s∗⊤(A−AN′)s∗,
(iii)
≤(7L1κηx)2·LΦϵ′3
(7L1κηx)2+ϵ′∥s∗
N′∥2
240ηx+LΦϵ′3
=ϵ′∥s∗
N′∥2
240ηx+ 2LΦϵ′3(75)
where (i) uses ϕN′(s′
k)≤ϕN′(s∗
N′) +ϵ′∥s∗
N′∥2
240ηx+LΦϵ′3andϕN′(s∗
N′)≤ϕN′(s∗), (ii) uses the definitions of ϕ(·)
andϕN′(·)in eqs. (6) & (67) respectively, (iii) uses eq. (70) and max(∥s∗∥,∥s′
k∥)≤smax:= 7L1κηx.
Eq. (64) can be proved as follows.
ϵ′∥s∗∥2
240ηx+ 2LΦϵ′3(i)
≥ϕ(s′
K)−ϕ(s∗)
(ii)=1
2(s′
K−s∗)⊤/parenleftig
A+∥s∗∥
2ηxI/parenrightig
(s′
K−s∗) +1
12ηx(∥s∗∥−∥s′
K∥)2(∥s∗∥+ 2∥s′
K∥)
(iii)
≥∥s∗∥
12ηx(∥s∗∥2+∥s′
K∥2−2∥s′
K∥∥s∗∥)
≥∥s∗∥2
12ηx(∥s∗∥−2∥s′
K∥),
where (i) uses eq. (63) proved above, (ii) uses eq. (6) of (Carmon and Duchi, 2019) and (iii) uses Proposition
2.1 of (Carmon and Duchi, 2019) which states that A+∥s∗∥
2ηxI⪰O.
Lemma G.2. When∥g∥>4L2
1κ2ηx, implement Algorithm 2 with hyperparameters K≥2 ln[100/(7ηx)]
ln[(1−κ−1)−1],
ηv= 1/L1and initialize s0= 0. Then, the output s′
Ksatisfiesϕ(s′
K)≤7
20/radicalig
ϵ3
L2with probability at least 1−δ.
Correspondingly, the approximate CR solution stin Algorithm 1 satisfies
∥s′
K∥≥L1κηx (76)
ϕ(s′
K)≤−1
4L3
1κ3η2
x (77)
Lemma G.2 provides the properties of the solution given by GDA-Cubic Solver (Algorithm 2) in the large
gradient case∥g∥>4L2
1κ2ηx. The main idea of proof is to show that s′
K=−γK
∥g∥gis close to−γ∗
∥g∥g, so
the properties about s′
Kcan be approximately obtained by studying −γ∗
∥g∥g.To show that γK≈γ∗, we only
need to show that wKobtained by gradient ascent step of µ-strongly concave function converges to w∗
exponentially fast as shown in eq. (80), and that there exists a Lipschitz continuous function ξsuch that
γK=ξ/parenleftig
g⊤
∥g∥H11g
∥g∥−(H21g)⊤wK
∥g∥/parenrightig
andγ∗=ξ/parenleftig
g⊤
∥g∥H11g
∥g∥−(H21g)⊤w∗
∥g∥/parenrightig
.
Proof.γ∗has the following lower bound.
γ∗=/radicaligg/parenleftigηxg⊤Ag
∥g∥2/parenrightig2
+ 2ηx∥g∥−ηxg⊤Ag
∥g∥2
(i)
≥ηx/parenleftigg/radicaligg/parenleftigg⊤Ag
∥g∥2/parenrightig2
+ 8L2
1κ2−g⊤Ag
∥g∥2/parenrightigg
(ii)
≥7
5L1κηx (78)
34Published in Transactions on Machine Learning Research (03/2023)
where (i) uses∥g∥≥4L2
1κ2ηx, and (ii) uses the monotonically decreasing property of the function/radicalbig
x2+ 8L2
1−
xand∥A∥≤∥H11∥+∥H12∥∥H−1
22∥∥H21∥≤L1(1 +κ)≤2L1κbased on Lemma A.1.
γ∗= arg minγ≥0ϕ(−γg/∥g∥)also satisfies the stationary condition below
0 =∂ϕ(−γg)
∂γ/vextendsingle/vextendsingle/vextendsingle
γ=γ∗=−∥g∥+γ∗
∥g∥2g⊤Ag+γ∗2
2ηx. (79)
Note that the gradient steps (10) aim at the L1-smooth,µ-strongly concave maximization problem w∗:=
arg maxw1
2w⊤H22w−(H21g)⊤
∥g∥w. Therefore, thesegradientstepswithlearningrate ηv= 1/L1andinitialization
w0= 0have the following convergence rate
∥wK−w∗∥≤(1−κ−1)K/2∥w0−w∗∥(i)
≤7
100κηx, (80)
where (i) uses w0= 0,∥w∗∥≤/vextenddouble/vextenddoubleH−1
22H21g
∥g∥/vextenddouble/vextenddouble≤∥H−1
22∥∥H21∥≤µ−1L1=κandK≥2 ln[40/(7ηx)]
ln[(1−κ−1)−1].
Denote the function ξ(u) =/radicalbig
u2+ 2ηx∥g∥−u(u∈R). Then we have
|γK−γ∗|=/vextendsingle/vextendsingle/vextendsingleξ/parenleftigg⊤
∥g∥H11g
∥g∥−(H21g)⊤wK
∥g∥/parenrightig
−ξ/parenleftigg⊤
∥g∥H11g
∥g∥−(H21g)⊤w∗
∥g∥/parenrightig/vextendsingle/vextendsingle/vextendsingle
(i)=/vextendsingle/vextendsingle/vextendsingleξ′/parenleftigg⊤
∥g∥H11g
∥g∥−(H21g)⊤[ωwK+ (1−ω)w∗]
∥g∥/parenrightig(H21g)⊤(wK−w∗)
∥g∥/vextendsingle/vextendsingle/vextendsingle
(ii)
≤7
50L1κηx(iii)
≤0.1γ∗
where (i) applies the Lagrange Mean Value Theorem to the function ξwithω∈[0,1], (ii) uses|ξ′(x)|=/vextendsingle/vextendsinglex√
x2+2ηx∥g∥−1/vextendsingle/vextendsingle≤2,∥H21∥≤L1and eq. (80), and (iii) uses eq. (78). The above inequality implies that
0.9γ∗≤γK≤1.1γ∗(81)
Therefore, eq. (76) can be proved as follows.
∥s′
K∥(i)=γK(ii)
≥0.9γ∗(iii)
≥7(0.9)
5L1κηx≥L1κηx
where (i) uses s′
K=−γK
∥g∥g, (ii) uses eq. (81), and (iii) uses eq. (78).
Eq. (77) can be proved as follows.
ϕ(s′
K) =−γK∥g∥+γ2
K
2∥g∥2g⊤Ag+γ3
K
6ηx
(i)
≤−9γ∗
10∥g∥+(1.1γ∗)2
2∥g∥2g⊤Ag+(1.1γ∗)3
6ηx
(ii)
≤−γ∗
4∥g∥−(γ∗)3
20ηx
(iii)
≤ −(1.4L1κηx)(4L2
1κ2ηx)
20−(1.4L1κηx)3
2500ηx
=−1
4L3
1κ3η2
x
where (i) uses eq. (81), (ii) uses eq. (79), and (iii) uses eq. (78) and ∥g∥≥4L2
1κ2ηx.
35Published in Transactions on Machine Learning Research (03/2023)
Lemma G.3. Implement Algorithm 3 with initialization s′
0= 0and hyperparameters K= Θ/parenleftig
L2
1κ2
L2
Φϵ′2/parenrightig
,
N′
k=N′=ln[2L1κϵ′−1max(24ηx,7/LΦ)]
ln[(1−κ−1)−1],ηv=1
L1,ηs=1
22L1κ. Then, if∥s∗∥≤3ϵ′and∥g∥≤4L2
1κ2ηx, the
algorithm will terminate with K′= min{k:∥gk∥≤LΦϵ′2}≤Kand the output s′
K′satisfies
∥s′
K′∥≤7ϵ′. (82)
∥∇ϕ(s′
K′)∥=/vextenddouble/vextenddouble/vextenddoubleg+As′
K′+∥s′
K′∥
2ηxs′
K′/vextenddouble/vextenddouble/vextenddouble≤2LΦϵ′2. (83)
Lemma G.3 provides convergence properties of GDA-Cubic FinalSolver (Algorithm 3) in the small gradient
case∥g∥≤4L2
1κ2ηx. Note that Algorithm 3 is noiseless version of Algorithm 2. Hence, the proof logic is
similar to Lemma G.1. To elaborate, Algorithm 3 is equivalent to apply gradient descent to the approximate
CR objective function ϕN′defined by eq. (67), which is ϵ′-close to the target CR objective function ϕ. Then
based on the nice geometry of the two close CR objective functions, their optimizers s∗
N′:=arg minsϕN′(s)
ands∗:=arg minsϕ(s)also have close norms such that ∥s∗∥≤3ϵ′implies∥s∗
N′∥≤7ϵ′, as shown by eqs. (87)
& (88). Then eq. (82) follows from ∥sK′∥≤∥s∗
N′∥which has been proved by Lemma 2.6 of (Carmon and
Duchi, 2019) for gradient descent steps on CR objective function ϕ.K′≤Kfollows from the nonconvex
convergence rate of min0≤k≤K−1∥gk∥2wheregK:=∇ϕN′(s′
K). Finally, eq. (83) follows from ∥gK′∥≤LΦϵ′2
and∇ϕ(s′
K)≈∇ϕN′(s′
K) =gK.
Proof.Sinces∗= arg minsϕ(s),0 =∇ϕ(s∗) =g+As∗+∥s∗∥
2ηxs∗, which implies that
∥g∥=/vextenddouble/vextenddouble/vextenddoubleAs∗+∥s∗∥
2ηxs∗/vextenddouble/vextenddouble/vextenddouble≤∥A∥∥s∗∥+∥s∗∥2
2ηx(i)
≤2L1κ(3ϵ′) +(3ϵ′)(7L1κηx)
2ηx≤13L1κϵ′(84)
where (i) uses∥A∥≤2L1κ,∥s∗∥≤3ϵ′and∥s∗∥≤smax:= 7L1κηx.
Following the proof of eq. (70), we can prove that when N′=ln[2L1κϵ′−1max(24ηx,7/LΦ)
ln[(1−κ−1)−1]
∥AN′−A∥=∥(I+ηvH22)N′∥≤(1−κ−1)N′∥A0−A∥≤2L1κ(1−κ−1)N′≤min/parenleftigϵ′
24ηx,LΦϵ′
7/parenrightig
,(85)
Substituting eqs. (65) & (66) into eq. (13), we obtain that
gk=g+AN′s′
k+∥s′
k∥
2ηxs′
k=∇ϕN′(s′
k). (86)
Hence, the update rule (14) can be seen as gradient descent step on solving the cubic-regularization problem
(67) without random perturbation. Therefore, based on Lemmas 2.3 and eq. (11) of (Carmon and Duchi,
2019),∥s′
k∥≤∥s∗
N′∥≤smaxand that when ηs=1
22L1κ≤1
4(∥A∥+smax/(2ηx)), we have
ηs
2K−1/summationdisplay
k=0∥gk∥2≤ϕN′(s′
0)−ϕN′(s∗)
(i)
≤∥g∥∥s∗∥+1
2∥AN′∥∥s∗∥2+∥s∗∥3
6ηx
(ii)
≤3ϵ′/parenleftbig
13L1κϵ′+L1κ(3ϵ′) + (3ϵ′)(7L1κηx)/(6ηx)/parenrightbig
≤115
2L1κϵ′2,
where (i) uses s′′
0= 0and the definition of function ϕN′in eq. (67), (ii) uses eq. (84), ∥s∗∥≤max(3ϵ′,7L1κηx)
and∥AN′∥≤2L1κ. Rearranging the above inequality, we obtain that
min
0≤k≤K−1∥gk∥2≤1
KK−1/summationdisplay
k=0∥gk∥2≤115L1κϵ′2
Kηs(i)
≤L2
Φϵ′4,
36Published in Transactions on Machine Learning Research (03/2023)
where (i) uses K=2530L2
1κ2
L2
Φϵ′2andηs=1
22L1κ. Hence,K′= min{k:∥gk∥≤LΦϵ′2}≤K−1.
Next, we will prove eq. (82). On one hand, using the same proof logic as that of eq. (64) (see the end of the
proof of Lemma G.1), we obtain that
ϕN′(s∗)−ϕN′(s∗
N′)(i)=1
2(s∗−s∗
N′)⊤/parenleftig
AN′+∥s∗
N′∥
2ηxI/parenrightig
(s∗−s∗
N′) +1
12ηx(∥s∗
N′∥−∥s∗∥)2(∥s∗
N′∥+ 2∥s∗∥)
(ii)
≥∥s∗
N′∥
12ηx(∥s∗
N′∥2+∥s∗∥2−2∥s∗∥∥s∗
N′∥)
(iii)
≥∥s∗
N′∥2
12ηx(∥s∗
N′∥−6ϵ′), (87)
where (i) uses eq. (6) of (Carmon and Duchi, 2019), (ii) uses Proposition 2.1 of (Carmon and Duchi, 2019)
which states that A+∥s∗
N′∥
2ηxI⪰Oand (iii) uses∥s∗∥≤3ϵ′. On the other hand,
ϕN′(s∗)−ϕN′(s∗
N′)
=/parenleftbig
ϕN′(s∗)−ϕ(s∗)/parenrightbig
+/parenleftbig
ϕ(s∗)−ϕN′(s∗
N′)/parenrightbig
(i)
≤/parenleftbig
ϕN′(s∗)−ϕ(s∗)/parenrightbig
+/parenleftbig
ϕ(s∗
N′)−ϕN′(s∗
N′)/parenrightbig
(ii)
≤1
2s∗⊤(AN′−A)s∗+1
2s∗⊤
N′(A−AN′)s∗
N′
(iii)
≤ϵ′
24ηx(9ϵ′2+∥s∗
N′∥2), (88)
where (i) uses ϕ(s∗) =minsϕ(s)≤ϕ(s∗
N′), (ii) uses the definitions of ϕ(·)andϕN′(·)in eqs. (6) & (67
respectively, and (iii) uses eq. (85) and ∥s∗∥≤3ϵ′. Combining eqs. (87) & (88) yields that
∥s∗
N′∥≤6ϵ′+ϵ′
2∥s∗
N′∥2(9ϵ′2+∥s∗
N′∥2) = 6.5ϵ′+9ϵ′3
2∥s∗
N′∥2.
Suppose∥s∗
N′∥>7ϵ′and substitute it into the right side of the above inequality. Then we obtain the
contradiction that ∥s∗
N′∥<6.8ϵ′. Therefore,∥sK′∥≤∥s∗
N′∥≤7ϵ′, i.e., eq. (82) is proved. Finally, eq. (83)
can be proved as follows
∥∇ϕ(sK′)∥≤∥∇ϕ(sK′)−gK′∥+∥gK′∥
(i)
≤∥(A−AN′)sK′∥+LΦϵ′2
(ii)
≤LΦϵ′
7(7ϵ′) +LΦϵ′2= 2LΦϵ′2, (89)
where (i) uses∥gK′∥≤LΦϵ′2and the definitions of ϕ(·)andϕN′(·)in eqs. (6) & (67) respectively, (ii) uses
eq. (85) and∥sK′∥≤7ϵ′.
Proposition G.4 (Potential decrease for Inexact Cubic-LocalMinimax) .Let Assumption 1 hold. For
anyα,β > 0,0< ϵ′≤αL1
βLGandδ∈(0,1), chooseηx= (168LΦ+ 120α+ 168β)−1,ηy=2
L1+µand
Nt≥Θ/parenleftig
κlnL1α∥/tildewidest−1∥+L1(α+L2κ)∥/tildewidest∥
LGβϵ′2/parenrightig
(see eq. (49)). When implementing Algorithm 2 at the t-th iteration,
use hyperparameters in Lemma G.1 with δ′=δ/Tif∥∇1f(xt,yt+1)∥≤4L2
1κ2ηx, and use those in Lemma
G.2 otherwise. Define the potential function Ht:= Φ(xt) + (10LΦ+ 7α+ 10β)∥/tildewidest∥3. Then, the output of
Cubic-LocalMinimax satisfies the following potential decrease property with probability at least 1−δ.
Ht+1−Ht≤−(LΦ+α+β)(∥/tildewidest+1∥3+∥/tildewidest∥3). (90)
Compared with Proposition 4.1 which also provides potential function decrease, Proposition G.4 only has
access to the inexact CR solution obtained from GDA-Cubic Solver (Algorithm 2). In small and large gradient
37Published in Transactions on Machine Learning Research (03/2023)
cases respectively, this inexact CR solution has properties given by Lemmas G.1 & G.2, and the Taylor
expansion of Φ(xt+1)−Φ(xt)can be upper bounded by eqs. (63) & (77), as shown in eqs. (96) & (98). Both
cases yield eq. (90).
Proof.Following the proof of Proposition 4.1, it can be seen that when ϵ′≤αL1
βLGandNt≥
Θ/parenleftig
κlnL1α∥/tildewidest−1∥+L1(α+L2κ)∥/tildewidest∥
LGβϵ′2/parenrightig
, the following bounds always hold, which are analogous to eqs. (34 &
(35 with exact solutions st−1andstreplaced by/tildewidest−1and/tildewidestrespectively.
∥∇Φ(xt)−∇ 1f(xt,yt+1)∥≤β(∥/tildewidest∥2+ϵ′2), (91)
∥∇2Φ(xt)−G(xt,yt+1)∥≤α(∥/tildewidest∥+ϵ′). (92)
Then we consider the following two cases.
(Case 1) When∥∇1f(xt,yt+1)∥≤4L2
1κ2ηx, the output s′
Ksatisfies eqs. (63) & (94) with probability at least
1−δ′. Also note that the input variables of Algorithm 2 are g:=∇1f(xt,yt+1)andA:=G(xt,yt+1) =
H11−H12H−1
22H21, the output s′
Kof Algorithm 2 is assigned to /tildewidest+1which is later used for the update
xt+1=xt+/tildewidest+1, and the optimal CR solution s∗in Algorithm 2 corresponds to st+1in Algorithm 1.
Therefore, eqs. (63) & (64) transform to the following inequalities at the t-th iteration of Algorithm 1, which
by applying union bound hold simultaneously for all 0≤t≤T−1with probability at least 1−Tδ′= 1−δ.
∇1f(xt,yt+1)⊤(/tildewidest+1−st+1)+1
2/tildewides⊤
t+1G(xt,yt+1)/tildewidest+1−1
2s⊤
t+1G(xt,yt+1)st+1+∥/tildewidest+1∥3−∥st+1∥3
6ηx
≤ϵ′∥st+1∥2
240ηx+ 2LΦϵ′3(93)
∥st∥≤2∥/tildewidest∥+ϵ′
20+24ηxLΦϵ′3
∥st∥2. (94)
Based on eq. (94), if ∥st∥>ϵ′, then∥st∥≤2∥/tildewidest∥+ϵ′sinceηx≤(168LΦ)−1. Otherwise,∥st∥≤ϵ′. Combining
the two cases yields the following inequality.
∥st∥≤2∥/tildewidest∥+ϵ′⇒∥st∥2≤8∥/tildewidest∥2+ 2ϵ′2. (95)
Therefore,
Φ(xt+1)−Φ(xt)
(i)
≤/tildewides⊤
t+1∇Φ(xt) +1
2/tildewides⊤
t+1∇2Φ(xt)/tildewidest+1+LΦ
6∥/tildewidest+1∥3
=/tildewides⊤
t+1/parenleftbig
∇Φ(xt)−∇ 1f(xt,yt+1)/parenrightbig
+/tildewides⊤
t+1∇1f(xt,yt+1)
+1
2/tildewides⊤
t+1/parenleftbig
∇2Φ(xt)−G(xt,yt+1)/parenrightbig
/tildewidest+1+1
2/tildewides⊤
t+1G(xt,yt+1)/tildewidest+1+LΦ
6∥/tildewidest+1∥3
(ii)
≤β∥/tildewidest+1∥(∥/tildewidest∥2+ϵ′2) +α
2∥/tildewidest+1∥2(∥/tildewidest∥+ϵ′) +s⊤
t+1∇1f(xt,yt+1) +1
2s⊤
t+1G(xt,yt+1)st+1
+∥st+1∥3
6ηx+/parenleftigLΦ
6−1
6ηx/parenrightig
∥/tildewidest+1∥3+ϵ′∥st+1∥2
240ηx+ 2LΦϵ′3
(iii)
≤/parenleftigα
2+β/parenrightig
(2∥/tildewidest+1∥3+∥/tildewidest∥3+ϵ′3)−∥st+1∥3
12ηx+/parenleftigLΦ
6−1
6ηx/parenrightig
∥/tildewidest+1∥3+4ϵ′∥/tildewidest+1∥2+ϵ′3
120ηx+ 2LΦϵ′3
(iv)
≤/parenleftigα
2+β/parenrightig
(3∥/tildewidest+1∥3+ 2∥/tildewidest∥3) +/parenleftigLΦ
6−1
6ηx/parenrightig
∥/tildewidest+1∥3
+4∥/tildewidest+1∥2(∥/tildewidest+1∥+∥/tildewidest∥) +∥/tildewidest+1∥3+∥/tildewidest∥3
120ηx+ 2LΦ(∥/tildewidest+1∥3+∥/tildewidest∥3)
(v)
≤/parenleftigα
2+β/parenrightig
(3∥/tildewidest+1∥3+ 2∥/tildewidest∥3) +/parenleftigLΦ
6−1
6ηx/parenrightig
∥/tildewidest+1∥3
38Published in Transactions on Machine Learning Research (03/2023)
+4∥/tildewidest+1∥3+ 4(∥/tildewidest∥3+∥/tildewidest+1∥3) +∥/tildewidest+1∥3+∥/tildewidest∥3
120ηx+ 2LΦ(∥/tildewidest+1∥3+∥/tildewidest∥3)
≤/parenleftig3α
2+ 3β+13LΦ
6−1
12ηx/parenrightig
∥/tildewidest+1∥3+/parenleftig
α+ 2β+ 2LΦ+1
24ηx/parenrightig
∥/tildewidest∥3
(vi)
≤ − (11LΦ+ 8α+ 11β)∥/tildewidest+1∥3+ (9LΦ+ 6α+ 9β)∥/tildewidest∥3(96)
where (i) uses xt+1=xt+st+1and the fact that ∇2Φ(x)isLΦ-Lipschitz continuous (see the item 4
of Proposition 3.1), (ii) uses eqs. (91), (92) & (93), (iii) uses eqs. (33) & (95) and the inequality that
ab2≤a3∨b3≤a3+b3,∀a,b≥0, (iv) usesϵ′n≤∥/tildewidest∥n∨∥/tildewidest+1∥n≤∥/tildewidest∥n+∥/tildewidest+1∥n,∀0≤t≤T′−2,n∈{1,3}
based on the termination criterion of T′in Algorithm 4, (v) uses ab2≤a3∨b3≤a3+b3,∀a,b≥0, and (vi)
usesηx= (168LΦ+ 120α+ 168β)−1.
(Case 2) When∥∇1f(xt,yt+1)∥>4L2
1κ2ηx, similar to case 1, eq. (77) transforms as follows in Algorithm 1.
∇1f(xt,yt+1)⊤/tildewidest+1+1
2/tildewides⊤
t+1G(xt,yt+1)/tildewidest+1+1
6ηx∥/tildewidest+1∥3≤−1
4L3
1κ3η2
x (97)
Then, we obtain that
Φ(xt+1)−Φ(xt)
≤s⊤
t+1∇Φ(xt) +1
2s⊤
t+1∇2Φ(xt)st+1+LΦ
6∥st+1∥3
=s⊤
t+1/parenleftbig
∇Φ(xt)−∇ 1f(xt,yt+1)/parenrightbig
+s⊤
t+1∇1f(xt,yt+1)
+1
2s⊤
t+1/parenleftbig
∇2Φ(xt)−G(xt,yt+1)/parenrightbig
st+1+1
2s⊤
t+1G(xt,yt+1)st+1+LΦ
6∥st+1∥3
(i)
≤β∥st+1∥(∥st∥2+ϵ′2) +α
2∥st+1∥2(∥st∥+ϵ′) +/parenleftigLΦ
6−1
6ηx/parenrightig
∥st+1∥3−1
4L3
1κ3η2
x
(ii)
≤−(11LΦ+ 8α+ 11β)∥/tildewidest+1∥3+ (9LΦ+ 6α+ 9β)∥/tildewidest∥3(98)
where (i) uses eqs. (91), (92) & (97), and (ii) follows the same proof logic as that of eq. (96).
Eq. (96) in case 1 and eq. (98) in case 2 are the same. By rearranging them and using Ht:= Φ(xt) + (10LΦ+
7α+ 10β)∥/tildewidest∥3, we can prove eq. (90).
Theorem 2 (Computation complexity of Inexact Cubic-LocalMinimax) .Let Assumption 1 hold. For any 0<
ϵ≤min/parenleftig
53L1κ
228√LΦ,L2
1L−1/2
2κ1/2,L2κ2
L1/parenrightig
andδ∈(0,1), chooseϵ′=ϵ
106√LΦ,T= Θ/parenleftbig√LΦ[Φ(x0)−Φ∗+ϵ2]ϵ−3/parenrightbig
,
ηx= Θ/parenleftbig
L−1
Φ/parenrightbig
,ηy=2
L1+µandNt= Θ/parenleftig
κlnL1α∥/tildewidest−1∥+L1(α+L2κ)∥/tildewidest∥
LGϵ2/parenrightig
(see eq. (49)) in Algorithm 4. When
implementing Algorithm 2 at the t-th iteration, use hyperparameters in Lemma G.1 with δ′=δ/Tif
∥∇1f(xt,yt+1)∥≤4L2
1κ2ηx, and use those in Lemma G.2 otherwise. When implementing Algorithm 3, use
the hyperparameter choices in Lemma G.3. Then, with probability at least 1−δ, the output of Inexact
Cubic-LocalMinimax satisfies
µ(/tildewidexT′)≤ϵ. (15)
Consequently, the total number of required cubic iterations satisfies T′≤O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
, the total number
of required gradient ascent iterations satisfies/summationtextT′−1
t=0Nt≤/tildewideO/parenleftbig√L2κ2.5ϵ−3/parenrightbig
, and the total number of required
Hessian-vector product computations (in Algorithms 2 & 3) is of the order /tildewideO(L1κ2ϵ−4).
The proof logic is very similar to that of Theorem 1, with the major difference that we leverage the properties
of inexact CR solution given by Lemmas G.1-G.3, and the potential decrease given by Proposition G.4 instead
of Proposition 4.1.
39Published in Transactions on Machine Learning Research (03/2023)
Proof.First, it can be easily verified that the hyperparameter choices of this Theorem fits those in Propo-
sition G.4 with α=β=LΦ. In particular, ϵ′=ϵ
106√LΦwith 0< ϵ≤106L1κ√LΦandδ∈(0,1)satisfies
0<ϵ′≤αL1
βLGrequired by Proposition G.4, since ηx= (456LΦ)−1,α=β=LΦandLG=LΦ/(1 +κ)≤LΦ/κ
(see Proposition 3.1).
SupposeT′≤Tdoes not hold, i.e., ∥st−1∥∨∥st∥>ϵ′=ϵ
106√LΦ,∀1≤t≤T. Then, on one hand, telescoping
eq. (90) over t= 0,1,...,T−1yield that
H0−HT≥(LΦ+α+β)T−1/summationdisplay
t=0(∥/tildewidest+1∥3+∥/tildewidest∥3)
≥3LΦT−1/summationdisplay
t=0(∥st∥∨∥st+1∥)3
≥3TLΦ/parenleftigϵ
106√LΦ/parenrightig3
(i)
≥Φ(x0)−Φ∗+ϵ2. (99)
where (i) uses T= 397006√LΦ[Φ(x0)−Φ∗+ϵ2]ϵ−3. On the other hand, recalling the definition of Htin
Proposition G.4, we have
H0−HT= Φ(x0)−Φ(xT) + 27LΦ(∥/tildewides0∥2−∥/tildewidesT∥2)(i)
≤Φ(x0)−Φ∗+ϵ2
137, (100)
where (i) uses∥s0∥=ϵ′=ϵ
106√LΦandΦ(xT)≥Φ∗=minx∈RmΦ(x). Note that eqs. (99) & (100) contradict.
Therefore, we must have 1≤T′≤T= 397006√LΦ[Φ(x0)−Φ∗+ϵ2]ϵ−3.
Since∥/tildewidesT′∥≤ϵ′, we have∥∇1f(xT′,yT′+1)∥≤4L2
1κ2ηx. Otherwise, eq. (76) directly implies the ccontradic-
tion that∥/tildewidesT′∥≥L1κηx=L1κ
456LΦ>ϵ′=ϵ
106√LΦ(sinceϵ<53L1κ
228√LΦ). Hence,∥∇1f(xT′,yT′+1)∥≤4L2
1κ2ηx,
which implies eq. (95) and thus implies ∥sT′∥≤2∥/tildewidesT′∥+ϵ′≤3ϵ′. Therefore, the conditions of Lemma G.3
are met, so eqs. (82) & (83) hold. Rewriting eqs. (82) & (83) with g←∇ 1f(xT′−1,yT′),A←G(xT′−1,yT′)
and/tildewides←s′
K′yields that
∥/tildewides∥≤7ϵ′(101)
/vextenddouble/vextenddouble/vextenddouble∇1f(xT′−1,yT′) +G(xT′−1,yT′)/tildewides+∥/tildewides∥
2ηx/tildewides/vextenddouble/vextenddouble/vextenddouble≤2LΦϵ′2(102)
Therefore,
∥∇Φ(/tildewidexT′)∥
(i)=/vextenddouble/vextenddouble/vextenddouble∇Φ(/tildewidexT′)−∇ 1f(xT′−1,yT′)−G(xT′−1,yT′)/tildewides−∥/tildewides∥
2ηx/tildewides/vextenddouble/vextenddouble/vextenddouble+ 2LΦϵ′2
≤∥∇ Φ(/tildewidexT′)−∇Φ(xT′−1)−∇2Φ(xT′−1)/tildewides∥+∥∇Φ(xT′−1)−∇1f(xT′−1,yT′)∥
+∥∇2Φ(xT′−1)/tildewides−G(xT′−1,yT′)/tildewides∥+∥/tildewides∥2
2ηx+ 2LΦϵ′2
(ii)
≤LΦ∥/tildewides∥2+β(∥/tildewidesT′−1∥2+ϵ′2) + 7αϵ′(∥/tildewidesT′−1∥+ϵ′) + 228LΦ(7ϵ′)2+ 2LΦϵ′2
(iii)
≤11191LΦϵ′2(iv)=ϵ2, (103)
where (i) uses eq. (102), (ii) uses eqs. (91), (92) & (101), /tildewidexT′=xT′−1+/tildewidesand the item 4 of Proposition 3.1
that∇2ΦisLΦ-Lipschitz, (iii) uses ηx= (168LΦ+ 120α+ 168β)−1= (456LΦ)−1,∥/tildewidesT′−1∥≤ϵ′,α=β=LΦ
and eq. (101), and (iv) uses ϵ′=ϵ
106√LΦ. Also,
∇2Φ(/tildewidexT′)(i)
⪰G(xT′−1,yT′)−∥G(xT′−1,yT′)−∇2Φ(xT′−1)∥I−∥∇2Φ(/tildewidexT′)−∇2Φ(xT′−1)∥I
40Published in Transactions on Machine Learning Research (03/2023)
(ii)
⪰−1
2ηx∥sT′∥I−α(∥/tildewidesT′−1∥+ϵ′)I−LΦ∥/tildewides∥I
(iii)
⪰ − 237LΦϵ′I⪰−3/radicalbig
LΦϵI, (104)
where (i) uses Weyl’s inequality, (ii) uses /tildewidexT′=xT′−1+/tildewides, eqs. (32) & (92) and the item 4 of Proposition 3.1
that∇2ΦisLΦ-Lipschitz, (iii) uses eq. (101), ηx= (456LΦ)−1,α=LΦ,∥/tildewidesT′−1∥≤ϵ′, and (iv) uses
ϵ′=ϵ
106√LΦ. Combining eqs. (103) & (104) proves eq. (15) where µ(x) =/radicalbig
∥∇Φ(x)∥∨−λmin[∇2Φ(x)]√33LΦ.
Finally, we compute the computation complexities. We have proved that the total number of cubic iterations
satisfiesT′≤T= 397006√LΦ[Φ(x0)−Φ∗+ϵ2]ϵ−3=O/parenleftbig√L2κ1.5ϵ−3/parenrightbig
(LΦ=L2(1 +κ)3=O(L2κ3)based
on Proposition 3.1). We can also prove that the total number of gradient ascent iterations have the same
bound/summationtextT′−1
t=0Nt≤/tildewideO/parenleftbig√L2κ2.5ϵ−3/parenrightbig
as that of Theorem 1, following the proof logic at the end of Appendix
D. Then we compute the total number of Hessian-vector product computations in cubic solvers (Algorithms 2
& 3). When implementing Algorithm 2 at the t-th iteration, if∥∇1f(xt,yt+1)∥≤4L2
1κ2ηx, then based on
Lemma G.1, the number of Hessian-vector product computations is proportional to
KN′=2 ln[(98L3
1κ3η2
x)/(LΦϵ′3)]
ln[(1−κ−1)−1]·/tildewideO(L1κηxϵ′−1)
=/tildewideO/bracketleftig
L1κ2L−1
Φ/parenleftigϵ√LΦ/parenrightig−1/bracketrightbig
=/tildewideO(L1L−1/2
Φκ2ϵ−1)(i)=/tildewideO(L1L−1/2
2κ1/2ϵ−1)
where (i) uses LΦ=L2(1 +κ)3=O(L2κ3). If∥∇1f(xt,yt+1)∥>4L2
1κ2ηx, based on Lemma G.2, the number
of Hessian-vector product computations is proportional to K=O(κ), whose order is not larger than the
above/tildewideO(L2
1L−1/2
2κ3/2ϵ−1)sinceϵ≤L2
1L−1/2
2κ1/2. Based on Lemma G.3, Algorithm 3 is implemented once
with the total number of Hessian-vector product computations proportional to
KN′=O/parenleftigL2
1κ2
L2
Φϵ′2/parenrightig
/tildewideO(κ) =/tildewideO/parenleftigL2
1κ3
L2
Φ(ϵ/√LΦ)2/parenrightig
=/tildewideO/parenleftigL2
1κ3
LΦ(ϵ)2/parenrightig
=/tildewideO(L2
1L−1
2ϵ−2).
As a result, the total number of Hessian-vector product computations in Algorithm 4 is
T/tildewideO(L1L−1/2
2κ1/2ϵ−1) +/tildewideO(L2
1L−1
2ϵ−2)
=O(/radicalbig
LΦϵ−3)/tildewideO(L1L−1/2
2κ1/2ϵ−1) +/tildewideO(L2
1L−1
2ϵ−2)
=/tildewideO(/radicalbig
L2κ3L1L−1/2
2κ1/2ϵ−4) +/tildewideO(L2
1L−1
2ϵ−2)
=/tildewideO(L1κ2ϵ−4),
where (i) uses ϵ≤L2κ2
L1to absorb the term /tildewideO(L2
1L−1
2ϵ−2).
H Experiment Details
In this section, we present the details of both synthetic minimax problem and the neural network simulation.
H.1 Details of Synthetic Minimax Problem
In this section we aim to solve the problem below:
min
x∈R3max
y∈R21
NN/summationdisplay
i=1/bracketleftbigg
w(x3)−y2
1
40+Aix1y1−5y2
2
2+Bix2y2/bracketrightbigg
(105)
whereAiandBiare independent random variables from uniform distribution range from 0.5 to 1.5 and w(x3)
has the exact form below :
41Published in Transactions on Machine Learning Research (03/2023)
w(x) =

√ϵ(x+ (L+ 1)√ϵ)2−1
3(x+ (L+ 1)√ϵ)3−1
3(3L+ 1)ϵ3/2, x≤−L√ϵ;
ϵx+ϵ3/2
3, −L√ϵ<x≤−√ϵ;
−√ϵx2−x3
3, −√ϵ<x≤0
−√ϵx2+x3
3, 0<x≤√ϵ
−ϵx+ϵ3/2
3,√ϵ<x≤L√ϵ;√ϵ(x−(L+ 1)√ϵ)2+1
3(x−(L+ 1)√ϵ)3−1
3(3L+ 1)ϵ3/2, L√ϵ≤x.(106)
and we set ϵ= 0.01andL= 5in our experiment. Through simple computing we can calculate that:
Φ(x) =w(x3) + 10/parenleftigx1
NN/summationdisplay
i=1Ai/parenrightig2
+1
10/parenleftigx2
NN/summationdisplay
i=1Bi/parenrightig2
(107)
Figure 9: Figure of the w-shaped function w(x).
H.2 Details of Neural Network Simulation
The network model we use is a convolutional neural network that consists of two convolution blocks followed
by two fully connected layers. Specifically, each convolution block contains a convolution layer, a max-pooling
layer with stride step 2, and a ReLU activation layer. The convolution layers in the two blocks have 1, 10
input channels and 10, 20 output channels, respectively, and both of them have kernel size 5, stride step 1
and no padding. The two fully connected layers have input dimensions 320, 50 and output dimensions 50, 10,
respectively.
H.3 Details of GDN/TGDA/FR/CN
We apply a slight variant of GDN (Zhang et al., 2021) to the equivalent minimax optimization problem
maxyminx−f(x,y)with 20 gradient ascent steps yt+1=yt+ 0.01∇2f(xt,yt)followed by Newton descent
stepxt+1=xt−(∇11f)−1(∇1f)(xt,yt+1)3.s= (∇11f)−1(∇1f)(xt,yt+1)for the update of xhas analytical
solution for synthetic simulation. For adversarial deep learning, we obtain sby solving the equivalent
optimization problem maxsF1(s) :=1
2s⊤∇11f(xt,yt+1)s+∇1f(xt,yt+1)⊤svia 30 gradient ascent steps with
learning rate 0.002 and early termination rule ∥∇F1(s)∥<0.001. Batchsizes 512 and 100 are used to compute
all the stochastic gradients and Hessians for synthetic simulation and adversarial deep learning respectively.
For TGDA, we apply 10, 20 gradient ascent steps on ywith learning rates 0.01,0.1for synthetic
simulation and adversarial deep learning respectively, followed by 1 Newton-type update step xt+1=
3Compared with the original GDN (Zhang et al., 2021), we switch the roles between xandy, sincef(x,·)is strongly concave
for which gradient ascent is sufficiently fast.
42Published in Transactions on Machine Learning Research (03/2023)
xt−0.01/parenleftbig
∇1f−∇ 12f(∇22f)−1∇2f/parenrightbig
(xt,yt+1)onx.s= (∇22f)−1(∇2f)(xt,yt+1)has analytical solution
for synthetic simulation. For adversarial deep learning, we obtain sby solving the equivalent optimization
problem maxsF2(s) :=1
2s⊤∇22f(xt,yt+1)s−∇ 2f(xt,yt+1)svia 30 gradient ascent steps with learning rate
0.1. Batchsizes 512 and 100 are used to compute all the stochastic gradients and Hessians for synthetic
simulation and adversarial deep learning respectively.
For FR, we apply 20 gradient ascent steps on ywith learning rates 0.01, 0.1 for synthetic simulation and
adversarial deep learning respectively, followed by 1 Newton-type update step xt+1=xt−0.01∇1f(xt,yt+1)−
α(∇11f)−1(∇12f)(∇2f)(xt,yt+1)onx.(∇11f)−1(∇12f)(∇2f)(xt,yt+1)has analytical solution for synthetic
simulation. For adversarial deep learning, we obtain sby solving the equivalent optimization problem
maxsF2(s) :=1
2s⊤∇11f(xt,yt+1)s−(∇12f)(∇2f)(xt,yt+1)svia 30 gradient ascent steps with early termi-
nation rule∥∇F(s)∥<0.001and learning rate 0.002. Batchsizes 512 and 100 are used to compute all the
stochastic gradients and Hessians for synthetic simulation and adversarial deep learning respectively.
CN algorithm follows the original update rule in (Zhang et al., 2021) that xt+1=xt−G(xt,yt)−1(∇1f)(xt,yt),
yt+1=yt−(∇22f)−1(∇2f)(xt+1,yt)whereG(x,y) =/bracketleftbig
∇11f−∇12f(∇22f)−1∇21f/bracketrightbig
(x,y). For synthetic exper-
iment,s=G(xt,yt)−1∇1f(xt,yt)has analytical solution and we compute s=/parenleftbig
G(xt,yt)−0.5I/parenrightbig−1∇1f(xt,yt)
instead to avoid singularity. For adversarial deep learning, to compute s=G(xt,yt)−1∇1f(xt,yt),
we apply GDA with 50 iterations to the minimax optimization problem minsmaxv(∇1f(xt+1,yt)⊤s+
1
2s⊤∇11f(xt+1,yt)s+s⊤∇12f(xt+1,yt)v+1
2v⊤∇22f(xt+1,yt)v, where each iteration has 1 gradient ascent
step onvwith learning rate 0.1 followed by 1 gradient descent step on swith learning rate 0.002. Batchsizes
512 and 100 are used to compute all the stochastic gradients and Hessians for synthetic simulation and
adversarial deep learning respectively.
43