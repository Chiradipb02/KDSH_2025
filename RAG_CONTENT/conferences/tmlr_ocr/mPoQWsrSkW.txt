Under review as submission to TMLR
ReMIX: Regret Minimization for Monotonic Value Function
Factorization in Multi-Agent Reinforcement Learning
Anonymous authors
Paper under double-blind review
Abstract
Value function factorization methods have become a dominant approach for cooperative
multi-agent reinforcement learning under a centralized training and decentralized execution
paradigm. By factorizing the optimal joint action-value function using a monotonic mixing
function of agents’ utilities, these algorithms ensure the consistency between joint and local
action selections for decentralized decision-making. Nevertheless, the use of monotonic mix-
ing functions also induces representational limitations. Finding the optimal projection of an
unrestricted mixing function onto monotonic function classes is still an open problem. To
this end, we propose ReMIX, formulating this optimal projection problem for value function
factorization as a regret minimization over the projection weights of diﬀerent state-action
values. Such an optimization problem can be relaxed and solved using the Lagrangian
multiplier method to obtain the close-form optimal projection weights. By minimizing the
resulting policy regret, we can narrow the gap between the optimal and the restricted mono-
tonic mixing functions, thus obtaining an improved monotonic value function factorization.
Our experimental results on Predator-Prey and StarCraft Multi-Agent Challenge environ-
ments demonstrate the eﬀectiveness of our method, indicating the better capabilities of
handling environments with non-monotonic value functions.
1 Introduction
Reinforcement learning has demonstrated great potential in solving challenging real-world problems, from
autonomous driving (Cao et al., 2012; Hu et al., 2019) to robotics and planning (Matignon et al., 2012;
Levine et al., 2016; Hüttenrauch et al., 2017). In many scenarios, these tasks involve multiple agents within
the same environment and thus require multi-agent reinforcement learning (MARL) (Vinyals et al., 2019;
Jaques et al., 2019; Baker et al., 2019; Wang et al., 2020b) to coordinate agents and learn desired behaviors
from their experiences. Due to practical communication constraints and the need to cope with vast joint
action space, MARL algorithms often leverage fully decentralized policies but learn them in a centralized
fashion with access to additional information during training. Value function factorization methods, e.g.,
QMIX (Rashid et al., 2018), QPLEX (Wang et al., 2020a), Qatten (Yang et al., 2020), FOP (Zhang et al.,
2021), and DOP (Wang et al., 2020c), have been a dominant approach for such centralized training and
decentralized execution (CTDE) MARL (Kraemer & Banerjee, 2016). By factorizing the optimal joint
action value function using a monotonic mixing function of per-agent utilities, these algorithms ensure the
consistencybetweenjointandlocalactionselectionsfordecentralizeddecision-making. Superiorperformance
has been reported in many MARL tasks, such as the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan
et al., 2019).
It is known that value function factorization can be viewed as an operator (Dugas et al., 2009), which
ﬁrst computes the optimal joint action value functions as targets and then projects them onto the space
representable by monotonic function classes. The projected monotonic mixing functions enable eﬃcient
maximization yet allow decentralized decision-making. However, it also poses representational limitations.
For instance, QMIX leverages a universal approximator for non-linear monotonic mixing functions. It pre-
vents QMIX from eﬃciently representing joint action value functions where agents’ orderings of their action
choices depend on each other (Mahajan et al., 2019). Later, the authors in the paper (Rashid et al., 2020)
1Under review as submission to TMLR
proposed an improved projection using Weighted QMIX (WQMIX). It assigns higher weights to the values
of optimal joint actions than the suboptimal ones, resulting in a better projection that more accurately
represents these optimal values. However, WQMIX relies purely on a heuristic design – such as Centrally-
Weighted (CW) and Optimistically-Weighted (OW) – where such weight term is a constant. Finding an
optimal projection onto the monotonic function class is still an open problem.
To this end, we propose ReMIX, formulating the optimal projection problem for value function factorization
as a regret minimization over the projection weights of diﬀerent state-action values. Speciﬁcally, we construct
an optimal policy following the optimal joint action-value function and a restricted policy using its projection
onto monotonic mixing functions. A policy regret is then deﬁned as the diﬀerence between the expected
discounted reward of the optimal policy and that of the restricted policy. By minimizing such policy regret
through an upper bound, we can narrow the gap between the optimal and restricted policies and thus force
the projected monotonic value function to approach the optimal one during learning, leading to an optimal
monotonic factorization with minimum regret. We note that while policy regret minimization has been
employed to formulate various optimizations in reinforcement learning, such as optimal prioritized experience
replay (Liu et al., 2021) and loss function design (Jin et al., 2018), to the best of our knowledge, this is the
ﬁrst proposal for optimizing value function factorization in MARL through policy regret minimization.
We show that the proposed regret minimization can be solved via the Lagrangian method (Bertsekas, 2014)
considering an upper bound. By examining a weighted Bellman equation involving monotonic mixing func-
tions and per-agent critics, we leverage the implicit function theorem (Krantz & Parks, 2002) and derive
Karush–Kuhn–Tucker (KKT) (Ghojogh et al., 2021) conditions to ﬁnd the optimal projection weights in
closed form. Our results highlight the key principles contributing to optimal monotonic value function fac-
torization. The optimal projection weights can be interpreted to consist of four components: Bellman error,
value underestimates, the gradient of the monotonic mixing function, and the on-policiness of available tran-
sitions. We note that the ﬁrst two terms relating to Bellman error and value underestimates are consistent
with the weighting heuristics proposed in WQMIX, thus providing a quantitative justiﬁcation and recovering
WQMIX as a special case. More importantly, our analysis reveals that an optimal value function factoriza-
tion should also depend on the gradient of the monotonic mixing function and the positive impact of more
current transitions.
Following the theoretical results, we provide a tractable approximation of the optimal projection weights
and propose a MARL algorithm of ReMIX with regret-minimizing monotonic value function factorization.
We validate the eﬀectiveness of ReMIX in Predator-Prey (Böhmer et al., 2020) and SMAC. Compared with
state-of-the-art factorization-based MARL algorithms (e.g., WQMIX, QPlex, FOP, DOP), ReMIX is shown
to better cope with environments with non-monotonic value functions, resulting in improved convergence
and superior empirical performance.
The main contributions of our work are as follows:
•We propose a novel method, ReMIX, formulating the optimal value function factorization as a policy
regret minimization and solving the weights of the optimal projection in closed form.
•The theoretical results and tractable weight approximations of ReMIX enable cooperative MARL
algorithms with improved value function factorization.
•Experiment results of ReMIX in Predator-Prey and SMAC environments demonstrate superior con-
vergence and empirical performance over state-of-the-art factorization-based methods. We further
perform ablation studies to demonstrate the contribution of each component in our design.
2 Background
2.1 Partially Observable Markov Decision Process
We describe a fully cooperative multi-agent sequential decision-making task as a decentralized partially
observable Markov decision process (Dec-POMDP) (Oliehoek & Amato, 2016) consisting of a tuple G=
/angbracketleftS,U,P,R,Z,O,n,γ /angbracketright, wheres∈Sdescribes the global state of the environment. At each time step, each
2Under review as submission to TMLR
agenta∈A≡{1,...,n}selects an action ua∈U, and all selected actions are combined to form a joint
action u∈U≡Ua. This process leads to a transition in the environment based on the state transition
functionP(s/prime|s,u) :S×U×S→[0,1]. All agents share the same reward function r(s,u) :S×U→Rwith
a discount factor γ∈[0,1).
In the partially observable environment, the agents’ individual observations z∈Zare generated by the
observation function O(s,u) :S×A→Z. Each agent has an action-observation history τa∈T≡(Z×U)∗.
Conditioning on the history, the policy becomes πa(ua|τa) :T×U→[0,1]. The joint policy πhas a joint
action-value function: Qπ(st,ut) =Est+1:∞,ut+1:∞[Rt|st,ut], wheretis the timestep and Rt=/summationtext∞
i=0γirt+iis
thediscountedreturn. Inthispaper, weadoptthecentralizedtraininganddecentralizedexecutionparadigm:
the learning algorithm has access to all local action-observation histories τand global state sduring training,
yet every agent can only access its individual history in execution. Although we compute individual policy
based on histories in practice, following the existing work (Su & Lu, 2022) where the state solely consists of
state and history, we will use π(u|s)andπa(ua|s)in analysis and proofs for simplicity.
2.2 Value Function Decomposition
In MARL, value function decomposition methods (Sunehag et al., 2017; Rashid et al., 2018; Son et al., 2019;
Wang et al., 2020c) learn a joint action value functions Qtot(s,u)as a function of combined individual action
value functions, conditioning individual local observation history, then these local action values are combined
with a learnable mixing neural network to produce joint action values, given by:
Qtot(s,u) = Mixer (s,Qa(τa,ua)) =fs(Qa(τa,ua)).
Under the principle of guaranteed consistency between global optimal joint actions and local optimal actions,
a global argmax performed on Qtotyields the same result as a set of individual arg maxoperations performed
on each local value, also known as Individual Global Maximum (IGM):
arg max
uQtot=/parenleftbigg
arg max
u1Q1,···,arg max
unQn/parenrightbigg
.
VDN (Sunehag et al., 2017) takes the joint action value function as a summation of local action value:
Qtot(s,u) =n/summationdisplay
i=1Qi(τi,ui),
while QMIX (Rashid et al., 2018) proposed a more general case of VDN by approximating a broader class
of monotonic functions to represent joint action value functions rather than summation of the local action
values, such that:
∂Qtot(s,u)
∂Qa(τa,ua)>0,∀a∈A≡1,...,n, (1)
which restricts the joint action value function to be a monotonic mixing of agents’ utilities, preventing it from
projecting non-monotonic joint action representation. Later proposed WQMIX (Rashid et al., 2020) solved
the limitation by introducing the weights into the projection to retrieve the optimal policy. The WQMIX
algorithms – OW and CW QMIXs – can place more importance on the better Qtotin minimizing the loss:/summationtextb
i=1w(s,u)(Qtot(s,u;θ)−¯yi)2, where ¯yi=r+γˆQ∗(s/prime,arg max u/primeQtot(s/prime,u/prime;θ−))is the ﬁxed target, ˆQ∗is
the unrestricted joint action-value function, bis the batch size and wis the weighting function. For example,
in OW, the weight wis given by:
w(s,u) =/braceleftBigg
1Qtot(s,u)<¯yi
αotherwise.(2)
When a transition is overestimated in the OW paradigm, it will be assigned with a constant weight α∈(0,1].
ComparedtoOW,CWhasasimilarmechanismbutassignsweightstoatransitionwhosejointaction uisnot
the best. We note that while insightful, these methods are based on heuristic designs of projection weights.
Finding optimal projection weights for monotonic value function factorization is still an open problem. In
this paper, we reformulate the problem as a policy regret minimization and solve the optimal projection
weights in closed form by relaxing the objective and the Lagrangian method.
3Under review as submission to TMLR
2.3 Policy Regret
The object of MARL is to ﬁnd a joint policy πthat can maximize the expected return: η(π) =
Eπ[/summationtext∞
i=0γirt+i]. Foraﬁxedpolicy, theMarkovdecisionprocessbecomesaMarkovrewardprocess, wherethe
discounted state distribution is deﬁned as dπ(s). Similarly, the discounted state-action distribution is deﬁned
asdπ(s,u) =dπ(s)π(u|s). Thus, we will have the expected return rewritten as η(π) =1
1−γEdπ(s,u)[r(s,u)].
We assume there exists an optimal joint policy π∗such that π∗= arg max πη(π). The regret of the
joint policy πis deﬁned as regret (π) =η(π∗)−η(π). The policy regret measures the expected loss when
following the current policy πinstead of optimal policy π∗. Sinceη(π∗)is a constant, minimizing the
regret is consistent with maximizing of expected return η(π). In this paper, we use regret as an alternative
optimization objective for ﬁnding the optimal projection in MARL, along with multiple constraints, e.g.,
the Bellman equation and the sum of projection weights. By minimizing the regret, the current policy πk
following a monotonic value factorization will approach the optimum π∗following an unrestricted value
function.
3 Related Work
3.1 Multi-Agent Reinforcement Learning
MARL algorithms have developed into neural-network-based methods that can cope with high-dimensional
state and action spaces. Early methods practice ﬁnding policies for a multi-agent system by directly learning
decentralized value functions or policies. For example, independent Q-learning (Tan, 1993) trains indepen-
dent action-value functions for each agent via Q-learning. (Tampuu et al., 2017) extends this technique
to DQN (Mnih et al., 2015). Recently, approaches for CTDE have come up as centralized learning of joint
actionsthatcanconvenientlysolvecoordinationproblemswithoutintroducingnon-stationary. COMA(Foer-
ster et al., 2018) uses a centralized critic to train decentralized actors to estimate a counterfactual advantage
function for every agent. Similar works (Gupta et al., 2017; Lowe et al., 2017) are also proposed based
on such analysis. Under the CTDE manner, value decomposition methods, such as QMIX (Rashid et al.,
2018), perform well in solving cooperative problems. Besides, other mechanisms can also solve competitive
problems or mixed problems. For instance, MADDPG (Lowe et al., 2017) utilizes the ensemble of policies
for each agent that leads to more robust multi-agent policies, showing strength in cooperative and compet-
itive scenarios, and the extension (Iqbal & Sha, 2019) of MADDPG has been proposed to realize further
optimization towards the original algorithm. This paper focuses on the cooperative setting in MARL and
aims to ﬁnd the optimal weighting scheme to retrieve the best projection onto the monotonic function class.
3.2 Value Decomposition Approaches
Value decomposition approaches (Guestrin et al., 2002; Castellini et al., 2019) are widely used in value-based
MARL. Such methods integrate each agent’s local action-value functions through a learnable mixing function
to generate global action values. For instance, VDN (Sunehag et al., 2017) and QMIX estimate the optimal
joint action-value function Q∗asQtotwith diﬀerent formations. VDN aims to learn a joint action-value
functionQtotof the sum of individual utilities for each agent. QMIX calculates Qtotby combining men-
tioned utilities via a continuous state-dependent monotonic function, generated by a feed-forward mixing
network with non-negative weights. QTRAN (Son et al., 2019) and QPLEX further extend the class of value
functions that can be represented. Besides value-based factorization algorithms, some works extend the
value decomposition method to policy-based actor-critic algorithms. In VDAC (Su et al., 2021), a factorized
actor-critic framework compatible with A2C can obtain a reasonable trade-oﬀ between training eﬃciency
and algorithm performance. Recently proposed FOP (Zhang et al., 2021) provides a new way to factorize
the optimal joint policy induced by maximum-entropy MARL into individual policies. DOP (Wang et al.,
2020c) addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and
continuous action spaces in the multi-agent actor-critic framework. In this paper, we recast the problem
of projecting an unrestricted value function onto monotonic function classes as a policy regret minimiza-
tion, whose solution allows us to ﬁnd the optimal projection weights to obtain an improved value function
factorization.
4Under review as submission to TMLR
4 Optimal Projection onto Monotonic Value Functions
4.1 Problem Formulation as Regret Minimization
LetQ∗be the unrestricted joint action value function and Qtot=fs(Q1(τ1,u1),...,Qn(τn,un))be
its estimation obtained through a monotonic mixing function fs(·)of per-agent utilities Qa(τi,ui)for
a= 1,...,n. For simplicity of notations, we use Qkto denoteQtotat stepk. AdoptingB∗Q∗
k−1as
the target with a Bellman operator B∗, we update Qkin tandem using a weighted Bellman equation:
Qk= arg min Q∈QEµ[wk(s,u)(Q−B∗Q∗
k−1)2(s,u)], wherewk(s,u)are non-negative projection weights for
diﬀerent transitions that need to be optimized. This projects the unrestricted value function onto a mono-
tonic function class Q∈Q.
To formulate the policy regret with respect to this projection, we consider a Boltzmann policy πkfollowing
the agent’s individual utilities Qa
kobtained from such monotonic value factorization, i.e., πk= [π1
k,...,πn
k]T
andπa
k=eQa
k(τa,ua)/[/summationtext
τa,u/primeaeQa
k(τa,u/prime
a)],aswellasasimilarpolicy π∗followingtheunrestrictedvaluefunction
Q∗that is deﬁned over joint actions in the Boltzmann manner. Our objective is to minimize the policy regret
η(π∗)−η(π)over non-negative projection weights under relevant constraints, i.e.,
min
wkη(π∗)−η(πk)
s.t.Qk= arg min
Q∈QEµ[wk(s,u)(Q−B∗Q∗
k−1)2(s,u)],
Eµ[wk(s,u)] = 1, wk(s,u)≥0,
Qk(s,u) =fs(Q1(τ1,u1),...,Qn(τn,un)),(3)
where π∗andπkare policies following the unrestricted and monotonic value functions, respectively. The
projection weights must sum up to 1, and µis the uniform distribution that we sample data from the replay
buﬀer. An additional table to summarize and explain the given notations is provided in Appendix A.
4.2 Solving Optimal Projection Weights
The solution to this optimization problem relies on the monotonic function fs(·)represented by a mixing
network, which takes the state and agent networks’ output Qa
kas inputs and generates an estimate of
joint value function Qtot. Solving the regret minimization problem through the Lagrangian method requires
analyzing the KKT conditions. Thus, we ﬁrst ﬁnd the ﬁrst-order derivative of the monotonic mixing network,
which will also be leveraged to ﬁnd an optimal solution. The mixing network is a universal approximator
consisting of a two-layer network of non-negative weight (Dugas et al., 2009). We compute its ﬁrst-order
derivative in the following lemma.
Lemma 1. Considering a two-layer mixing network of the weight matrix W1,W2, biasb1,b2and activation
functionh(·), the derivative of Qtotover one of the local utilities Qais:
f/prime
s,Qa=∂Qtot
∂Qa=h/prime
Qa(/vectorQTW1+b1)m/summationdisplay
j=1w1
ajw2
j,
where/vectorQ= [Q1,...,Qn]T.W1,W2are then×mand1×mmatrix correspondingly, with the respective
elementsw1
ijandw2
jin each matrix. nis the agent number, and mis the width of the mixing network.
Proof.See Appendix B.
Given that the monotonic mixing function is smooth and diﬀerentiable, we consider an upper bound of
the regret objective (obtained using a relaxation and Jensen’s inequality) and formulate its Lagrangian by
introducing Lagrangian multipliers with respect to the constraints. It allows us to solve the proposed regret-
minimization problem and obtain optimal projection weights in closed form (albeit with a normalization
factorZ∗).
5Under review as submission to TMLR
Theorem 1 (Optimal weighting scheme) .Under mild conditions, the optimal weight wk(s,u)to a relaxation
of the regret minimization problem in equation 3 with discrete action space is given by:
wk(s,u) =1
Z∗(Ek(s,u) +/epsilon1k(s,u)), (4)
where when Qk≤B∗Q∗
k−1, we have
Ek(s,u) =dπk(s,u)
µ(s,u)(B∗Q∗
k−1−Qk) exp(Q∗
k−1−Qk)
n/summationdisplay
j=11−πj
f/prime
s,Qj−1
,
and otherwise (i.e., when Qk>B∗Q∗
k−1), we have
Ek(s,u) = 0,
whereZ∗is the normalization factor, and /epsilon1k(s,u))is a negligible term when the probability of reversing back
to the visited state is small, or the number of steps agents take to revisit a previous state is large.
Proof.We give a sketch of the proof below and provide the complete proof in Appendix C. The derivation
of optimal weights consists of the following major steps: (i) Use a relaxation and Jensen’s inequality to
obtain a more tractable upper bound of the regret objective for minimization. (ii) Formulate the Lagrangian
for the new optimization problem and analyze its KKT conditions. (iii) Compute various terms in the
KKT condition and, in particular, analyze the gradient of Qkwith respect to weights pk(deﬁned through
the weighted Bellman equation) by leveraging the implicit function theorem (IFT). (iv) Derive the optimal
projection weights in closed form by setting the Lagrangian gradient to zero and applying KKT and its
slackness conditions.
Step 1: Relaxing the objective and adopting Jensen’s inequality. To begin with, we replace the original
optimization objective function, the policy regret, with a relaxed upper bound. This replacement can be
achieved through the following inequality:
η(π∗)−η(πk)≤Edπk(s)[(Q∗
k−1−Qk)(s,u∗)] +Edπk(s,u)[(Qk−Q∗
k−1)(s,u)]. (5)
The proof of this result is given in the appendix. The key idea is to rewrite the regret using the expectation
of the action-value functions with respect to discounted distribution dπk. After that, we adopt Jensen’s
inequality (McShane, 1937) to continue relaxing the intermediate objective function based on a convex
functiong(x) = exp(−x). Thus, a new optimization objective generated from equation 5 becomes:
min
wk−logEdπk(s)[exp(Qk−Q∗
k−1)(s,u∗)]−logEdπk(s,u)[exp(Q∗
k−1−Qk)(s,u)], (6)
where the constraints still hold for the new optimization objective.
Step 2: Computing the Lagrangian. In this step, we leverage the Lagrangian multiplier method to solve the
new optimization problem in equation 6. For simplicity, we use pkthat absorbs the data distribution µinto
wk. The constructed Lagrangian is:
L(pk;λ,ν) =−logEdπk(s)[exp(Qk−Q∗
k−1)(s,u∗)]−logEdπk(s,u)[exp(Q∗
k−1−Qk)(s,u)] +λ(/summationdisplay
s,upk−1)−νTpk,
wherepkis the weight wkmultiplied by the data distribution µ, andλ,νare the Lagrange multipliers.
Step 3: Computing the Gradients Required in the Lagrangian. According to the ﬁrst constraint in equation 3,
the gradient∂Qk
∂pkcan be computed via IFT given by:
∂Qk
∂pk=−[diag(pk)]−1[diag(Qk−B∗Q∗
k−1)].
6Under review as submission to TMLR
We also derive the gradient∂dπk(s,u)
∂pkfor solving the Lagrangian. The derivation details are given in the
appendix.
Step 4: Deriving the Optimal Weight. After having the equation for two gradients and an expression of the
Lagrangian, we can compute the optimal pkvia an application of the KKT conditions, which needs to set
the partial derivative of the Lagrangian equaling to zero, as∂L(pk;λ,ν)
∂pk= 0,where the optimal weight wkcan
be acquired from the pk.
Thetheoreticalresultsshedlightonthekeyfactorsdetermininganoptimalprojectionontomonotonicmixing
functions. Speciﬁcally, the optimal projection weights consist of four components relating to Bellman error,
value underestimation, the gradient of the monotonic mixing function, and the on-policiness of available
transitions. We will interpret these four components next and develop a deep MARL algorithm through
approximations of the optimal projection weights.
Bellman errorB∗Q∗
k−1−Qk:Qkis the estimation of the action-value function after the Bellman update.
This term measures the distance between the estimation and the Bellman target. A large diﬀerence in this
term means higher hindsight Bellman error. Due to the KKT slackness condition, our analysis indicates
that the optimal projection weight is zero when Qk>B∗Q∗
k−1is an overestimate of the target value, and
otherwise, a higher weight should be assigned when Qkis more underestimated.
Value underestimation exp(Q∗
k−1−Qk):IfQtotafter the Bellman update at current step kis smaller than
optimalQ∗
k−1, it results in an underestimate. In this case, we will assign a higher weight (always larger than
1) to this transition, which is proportional to the exponential of this underestimation gap. In contrast, when
overestimating (with a negative gap), the assigned weight becomes lower and always smaller than 1. This
is important because an underestimate of function approximation may lead to a sub-optimal Qkestimation
and thus non-optimal action selections.
Gradient of the mixing network/summationtextn
j=11−πj
f/prime
s,Qj−1:It turns out that the optimal projection weights also depend
on the inverse of the gradient of the monotonic mixing function fs(·), which is a new result. Intuitively,
the optimal projection weights would become higher when the monotonic mixing function is insensitive to
underlying per-agent utility values (i.e., having a small, positive gradient). We view this result as a form of
normalization with respect to diﬀerent shapes of monotonic mixing function fs(·). In practical algorithms,
we often use the two-layer mixing network with non-negative weights to approximate the monotonic function
fs(·)to produce Qk. The parameters of the mixing network are updated every step, and the gradient value
can be readily computed from these parameters. We have provided an instance regarding calculating the
gradient of a two-layer mixing network in Lemma 1. It is worth noting that similar gradients can also be
obtained for other value function factorization methods.
Measurement of on-policy transitionsdπk(s,u)
µ(s,u):The eﬃcient update of the joint action value function can
be achieved by focusing on transitions that are more possibly to be visited by the current policy, i.e., with
a higherdπk(s,u). Adding this term can speed up the search for the optimal Qkclose toQ∗
k−1.
4.3 Proposed Algorithm
Our analytical results in Theorem 1 identify four key factors determining the optimal projection weights.
Interestingly, the ﬁrst two terms, relating to Bellman error and value underestimation, recover the heuristic
designs in WQMIX. Speciﬁcally, when the Bellman error of a particular transition is high, which indicates
a wide gap between QkandQ∗
k−1, we may consider assigning a larger weight to this transition. Similarly,
value underestimation works as a correction term for incoming transitions: based on the diﬀerence of current
Qkand idealQ∗
k−1, it will compensate the underestimated Qkwith larger importance while penalizing
overestimated Qkwith a smaller weighting modiﬁer, consistent with OW scheme in equation 2.
Additionally, our analysis identiﬁes two new terms: the gradient of the monotonic mixing function and
measurement of on-policy transitions, which are crucial in obtaining an optimal projection onto monotonic
value function factorization. As discussed, we interpret the gradient term in optimal weights as a form
7Under review as submission to TMLR
Algorithm 1 ReMIX
1:Initialize step, the parameters of mixing network, agent networks, and hyper-network.
2:Set the learning rate αand replay buﬀer D
3:letθ−=θ
4:forstep= 1 :stepmaxdo
5:k= 0,s0=initial state
6:whilesk/negationslash=terminal and k<episode limit do
7:foreach agent ado
8:τa
k=τa
k−1∪(ok,uk−1)
9:ua
k=/braceleftBigg
arg maxua
kQ(τa
k,ua
k)with probability 1−/epsilon1
randint(1,|U|) with probability /epsilon1
10:end for
11:Obtain the reward rkand next state sk+1
12:Store the current trajectory into replay buﬀer D=D∪(sk,uk,rk,sk+1)
13:k=k+ 1,step=step+ 1
14:end while
15:Collectbsamples from the replay buﬀer Dfollowing uniform distribution µ.
16:foreach timestep kin each episode in batch bdo
17:EvaluateQk,Q∗and target values
18:Obtain the utilities Qafrom agents’ local networks, and compute the individual policy πa
k
19:Compute the weight:
wk∝

(B∗Q∗
k−1−Qk) exp(Q∗
k−1−Qk)/parenleftbigg/summationtextn
j=11−πj
f/prime
s,Qj−1/parenrightbigg
whenQk≤B∗Q∗
k−1
/epsilon1 whenQk>B∗Q∗
k−1
20:end for
21:Minimize the Bellman error for Qkweighted by wk, update the network parameter θ:
θ=θ−α(∇θ1
b/summationtextb
iwk(Qk−yi)2).
22:ifupdate-interval steps have passed then
23:θ−=θ
24:end if
25:end for
of normalization – by increasing the weights for transitions, where the monotonic mixing function is less
sensitive to the underlying per-agent utility, and decreasing the weights otherwise. The measurement of
on-policy transitions in the weighting expression emphasizes the useful information carried by more current,
on-policy transitions.
Following these theoretical results, we provide a tractable approximation of the optimal projection weights
and propose a MARL algorithm, ReMIX, with regret-minimizing projections onto monotonic value function
factorizations. The procedure of ReMIX can be found in Algorithm 1. We consider a new loss function with
respect to the optimal projection weights wkapplied to the Bellman equation of Qk(considering Qtotat
stepk), i.e.,
LReMIX =b/summationdisplay
i=1/bracketleftbig
wi(s,u)(Qk−yi)2(s,u)/bracketrightbig
, (7)
wherebis the batch size, and yi=B∗Q∗
k−1is a ﬁxed target using an unrestricted joint action-value function
that can be approximated using a separate network similar to WQMIX.
To compute the projection weights for Bellman error and value underestimation terms, we again leverage
the unrestricted joint action-value function Q∗to compute them quantitatively. We note that the Bellman
error term also works as the condition in Theorem 1 for deciding whether the weight should be zero. The
gradient of the monotonic mixing network can be directly computed using Lemma 1. Since the distribution
dπk(s,u)in the numerator of the measurement of on-policy transitions term is not readily available, we
8Under review as submission to TMLR
approximated this term via an additional replay buﬀer storing the recently generated transitions and sample
from it to obtain trajectories that are more possibly to be visited by current policy. To account for the
unknown normalization factor Z∗and improve the stability of the training process, we map the projection
weights to a given range, which is modeled as a hyperparameter of our algorithm. We provide numerical
results adjusting it in the experiment section.
5 Experiments
In this section, we present our experimental results on Predator-Prey and SMAC and demonstrate the
eﬀectiveness of ReMIX by comparing the results with several state-of-the-art MARL baselines. Besides,
we visualize the optimal weight pattern in heat maps to show the step-wise weight assignment for each
transition. Additionally, we conduct the ablation experiments by disabling each term in Theorem 1, and
deliver the sensitivity experiments regarding the normalization factor. More details about the environment
and hyper-parameter setting are provided in Appendix D. The code of this work is available on GitHub (see
supplementary ﬁles during the review period).
5.1 Predator-Prey
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000027/uni00000032/uni00000033
/uni00000029/uni00000032/uni00000033
/uni00000039/uni00000027/uni00000024/uni00000026
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b
(a) No punishment
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000014/uni00000013/uni00000013
/uni0000001a/uni00000018
/uni00000018/uni00000013
/uni00000015/uni00000018
/uni00000013/uni00000015/uni00000018/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000027/uni00000032/uni00000033
/uni00000029/uni00000032/uni00000033
/uni00000039/uni00000027/uni00000024/uni00000026
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b (b) Punishment =−0.5
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000014/uni00000013/uni00000013
/uni0000001a/uni00000018
/uni00000018/uni00000013
/uni00000015/uni00000018
/uni00000013/uni00000015/uni00000018/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000027/uni00000032/uni00000033
/uni00000029/uni00000032/uni00000033
/uni00000039/uni00000027/uni00000024/uni00000026
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b
(c) Punishment =−1.5
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000015/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000018/uni00000013
/uni00000013/uni00000018/uni00000013/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000027/uni00000032/uni00000033
/uni00000029/uni00000032/uni00000033
/uni00000039/uni00000027/uni00000024/uni00000026
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b (d) Punishment =−2
Figure 1: Average reward per episode on the Predator-Prey tasks for ReMIX and other baseline algorithms
of 4 settings.
To start with, we consider a complex partially-observable multi-agent cooperative environment, Predator-
Prey, that involves 8 agents in cooperation as predators to catch 8 prey on a 10 ×10 grid. In this task, a
successful capture with the positive reward of 1 must include two or more predator agents surrounding and
catching the same prey simultaneously, requiring a high level of cooperation. A failed coordination between
agents to capture the prey, which happens when only one predator catches the prey, will receive a negative
punishment reward. The greater punishment determines the degree of monotonicity. Algorithms that suﬀer
from relative overgeneralization issues or make poor trade-oﬀs in joint action-value function projection will
fail to solve this task.
We select multiple state-of-the-art MARL approaches as baseline algorithms for comparison, which include
value-basedfactorizationalgorithm(i.e., QMIX,WQMIX,andQPLEX),decomposedpolicygradientmethod
(i.e., VDAC), and decomposed actor-critic approaches (i.e., FOP and DOP). All mentioned baseline algo-
rithms have shown strength in handling MARL tasks in existing works.
9Under review as submission to TMLR
Figure1showstheperformanceofsevenalgorithmswithdiﬀerentpunishments, whereallresultsdemonstrate
the superiority of ReMIX over others. Besides, regarding eﬃciency, we can spot that ReMIX has the fastest
convergence speed in seeking the best policy. In Figure 1c and 1d, ReMIX signiﬁcantly outperforms other
state-of-the-art algorithms in a hard setting requiring a higher level of coordination among agents as learning
the best policy with improved joint action representation is required in this setting. Most algorithms, such as
QMIX,FOP,andDOP,enduplearningasub-optimalpolicywhereagentslearntoworktogetherwithlimited
coordination. Although ReMIX and WQMIX acquired good results eventually, compared to the latter,
ReMIX achieves better performance and converges to the optimal policy profoundly faster than WQMIX,
demonstrating that our optimal weighting approach can generate a better joint action-value projection.
5.2 SMAC
/uni00000013 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000057/uni00000048/uni00000056/uni00000057/uni00000003/uni0000005a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni00000008/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000039/uni00000027/uni00000024/uni00000026
/uni00000029/uni00000032/uni00000033
/uni00000027/uni00000032/uni00000033
(a) 1c3s5z (easy)
/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000057/uni00000048/uni00000056/uni00000057/uni00000003/uni0000005a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni00000008/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000039/uni00000027/uni00000024/uni00000026
/uni00000029/uni00000032/uni00000033
/uni00000027/uni00000032/uni00000033 (b) 3s_vs_5z (hard)
/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000057/uni00000048/uni00000056/uni00000057/uni00000003/uni0000005a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni00000008/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000039/uni00000027/uni00000024/uni00000026
/uni00000029/uni00000032/uni00000033
/uni00000027/uni00000032/uni00000033 (c) 5m_vs_6m (hard)
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000057/uni00000048/uni00000056/uni00000057/uni00000003/uni0000005a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni00000008/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000039/uni00000027/uni00000024/uni00000026
/uni00000029/uni00000032/uni00000033
/uni00000027/uni00000032/uni00000033
(d) 6h_vs_8z (super hard)
/uni00000013 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000057/uni00000048/uni00000056/uni00000057/uni00000003/uni0000005a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni00000008/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000039/uni00000027/uni00000024/uni00000026
/uni00000029/uni00000032/uni00000033
/uni00000027/uni00000032/uni00000033 (e) MMM2 (super hard)
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000057/uni00000048/uni00000056/uni00000057/uni00000003/uni0000005a/uni0000004c/uni00000051/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni00000008/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000034/uni00000033/uni0000002f/uni00000028/uni0000003b
/uni0000003a/uni00000034/uni00000030/uni0000002c/uni0000003b
/uni00000039/uni00000027/uni00000024/uni00000026
/uni00000029/uni00000032/uni00000033
/uni00000027/uni00000032/uni00000033 (f) corridor (super hard)
Figure 2: Results of 6 maps (from easy to super hard) on the SMAC benchmark.
Next, we evaluate ReMIX on the SMAC benchmark. We report the experiments on six maps consisting of
one easy map, two hard maps, and three super-hard maps. The selected state-of-the-art baseline algorithms
for this experiment are consistent with those in the Predator-Prey environment. The empirical results are
provided in Figure 2, demonstrating that ReMIX can eﬀectively generate optimal weight projection for joint
actions on SMAC for achieving a higher win rate, especially when the environment becomes substantially
complicated and harder, such as MMM2. We can see that several state-of-the-art policy-based factorization
algorithms are brittle when signiﬁcant exploration is undergone since joint action representations generated
by them are sub-optimal.
Speciﬁcally, ReMIX performs well on an easy map 1c3s5zin Figure 2a, albeit holding the comparable perfor-
mance among algorithms. On hard maps, such as 3s_vs_5z , the best policy found by our optimal weighting
approach signiﬁcantly outperforms the remaining baseline algorithms regarding winning rate. For super-
hard map 6h_vs_8z ,MMM2, andcorridor, ReMIX, along with QMIX, WQMIX, and QPLEX, can learn a
better policy than VDAC, DOP, and FOP. We achieve the highest winning rate by adopting our algorithm
on6h_vs_8z andMMM2. Compared to our method, QMIX and WQMIX suﬀer from this map as their
joint action representations are oblivious to some latent factors, such as the shape of the monotonic mixing
network, and therefore fail to generate an accurate joint action representation. On corridor, ReMIX man-
ages to learn the model with better performance than WQMIX, QPLEX, and other policy-based algorithms,
though standard QMIX has the fastest convergence rate among all baseline algorithms.
10Under review as submission to TMLR
5.3 Optimal Weight Pattern
Figure 3: Heatmap pattern of generated optimal weights (left) and WQMIX weights (right) used in the Predator-
Prey environment. The training episodes range from 0 to 1M.
In this part, we draw heat maps of the projecting weight probability distributions of ReMIX and WQMIX
as the training proceeds to better visualize and compare the weight evolution pattern of transitions sampled
as in a minibatch, shown in Figure 3. Adopted weights are generated from the Predator-Prey task with a
punishment of -2. We re-scale the absolute value of the transition number to logarithmic probability for scale
normalization. As shown in the ﬁgure, the probability value of a certain weight is represented by colors,
decreasing from 0 in light yellow to -10 in black. The vertical axis represents the training steps, and the
horizontal axis represents the normalized weight value, where ours ranges from 0.1 to 1 and WQMIX is either
0.1 or 1.
The heat map eﬀectively shows the general trend of the weight evolution pattern at diﬀerent steps. For
WQMIXontheFigure3right, withthetrainingofthealgorithm, thetransitionswiththesmallerweight(0.1)
will become more, and those with the larger weight (1) will become fewer. Evolution like this happens since
the transitions will approach optimal as the training goes on, while the algorithm will still take all transitions
as potential overestimations and assign smaller weights to them as adjustments. A similar evolution pattern
can be found in our weight pattern. On the left of Figure 3, during the training, the transitions with
higher weights become less, and most transitions will migrate to the bottom right with lower weights, which
empirically recovers the heuristic in WQMIX.
Moreover, as an optimal weight projection is used in ReMIX, we will assign diﬀerent weights to transitions
based on evaluating every one of them. We notice that some transitions are assigned with medium weight
during the training, given by the light yellow spots on the left of Figure 3. Such a phenomenon demonstrates
that the binary-weighted projections in WQMIX are not always accurate. Hence, ReMIX considers all
transitions by applying optimal weights to their projections, leading to better results, which also illustrates
the performance gap with other algorithms like WQMIX in previous experiments.
5.4 Sensitivity Experiment regarding Normalization
We run the experiment in the Predator-Prey environment with a punishment of -1.5 to report the sensitivity
with respect to the diﬀerent normalization of weight ranges. We keep the maximum normalized weight as 1
but test the eﬀects of using diﬀerent minimums, which are 0.1, 0.5, and 0.8.
As shown in Figure 4, the experiment results are sensitive to the range of the normalized weight. When
we map the weight to a minimum of 0.5, the agents in this task can only ﬁnd a sub-optimal solution. It
may be because there exist many overestimations in this task. The joint action representation generated
at the is not accurate. Higher minimum weight normalization damages the capability of ReMIX to adjust
the projection to retrieve a precise representation rapidly. Therefore, ReMIX performs well under 0.1 to 1
normalization of the weight in this scenario. Note in WQMIX weight is used as α=0.1 for Predator-Prey
andα=0.5 for SMAC according to their experiment settings.
11Under review as submission to TMLR
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000016/uni00000013/uni00000013
/uni00000015/uni00000013/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000013/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047
/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b/uni00000042/uni0000005a/uni00000020/uni00000013/uni00000011/uni00000014
/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b/uni00000042/uni0000005a/uni00000020/uni00000013/uni00000011/uni00000018
/uni00000035/uni00000048/uni00000030/uni0000002c/uni0000003b/uni00000042/uni0000005a/uni00000020/uni00000013/uni00000011/uni0000001b
Figure 4: Sensitivity of normalizing the minimum weight to 0.1, 0.5, and 0.8.
5.5 Ablation Experiment
/uni00000013 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013
/uni00000037/uni00000003/uni0000000b/uni00000014/uni00000013/uni0000004e/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000030/uni00000048/uni00000044/uni00000051/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000035/uni00000028/uni00000030/uni0000002c/uni0000003b
/uni00000051/uni00000052/uni00000003/uni00000050/uni0000004c/uni0000005b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000051/uni00000048/uni00000057/uni00000003/uni0000004a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000056
/uni00000051/uni00000052/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni00000058/uni00000051/uni00000047/uni00000048/uni00000055/uni00000003/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000051/uni00000052/uni00000003/uni00000045/uni00000048/uni0000004f/uni0000004f/uni00000050/uni00000044/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055
/uni00000051/uni00000052/uni00000003/uni00000052/uni00000051/uni00000010/uni00000053/uni00000052/uni0000004f/uni0000004c/uni00000046/uni0000005c/uni00000003/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056
Figure 5: Ablation by disabling one term each for ReMIX on MMM2 (super hard)
For ablations, we conduct experiments by disabling one single term (in Theorem 1) each at a time to
investigate their contribution to ﬁnding optimal projection weights, respectively. The ablation results are
given in Figure 5 showing the results on MMM2. The terms considered in these experiments are the Bellman
error, value underestimation, gradient of the mixing network, and measurement of on-policy transitions.
Compared to the original result, missing any terms will harm the performance. The tests without Bellman
error have the lowest ﬁnal winning rate, which is less than 10%, and the test result without on-policy
transitionsshowsarelativelyslowerconvergencespeedwithlessoptimalperformance. Furthermore, whenwe
disable the gradient of the mixing network term, the result is only around 60%, demonstrating that providing
a quantitative weight factorization for the value projection is the critical factor in value-factorization-based
MARL tasks. The designing of an optimal weighting scheme without considering the mixing network’s
inﬂuence will be less capable of achieving the ideal ﬁnal results.
6 Conclusion
In this paper, we formulate the optimal value function factorization as a policy regret minimization and solve
the optimal projection weights for the cooperative multi-agent reinforcement learning problems in closed
form. The theoretical results shed light on key factors for an optimal projection. Therefore, we propose
ReMIX as a tractable weight approximation approach to enable MARL algorithms with improved value
function factorization. Our experiment results in multiple MARL environments show the eﬀectiveness of
ReMIXbydemonstratingsuperiorconvergenceandempiricalperformanceoverstate-of-the-artfactorization-
based methods.
12Under review as submission to TMLR
References
BowenBaker, IngmarKanitscheider, TodorMarkov, YiWu, GlennPowell, BobMcGrew, andIgorMordatch.
Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528 , 2019.
Dimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods . Academic press, 2014.
Wendelin Böhmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International Con-
ference on Machine Learning , pp. 980–991. PMLR, 2020.
Yongcan Cao, Wenwu Yu, Wei Ren, and Guanrong Chen. An overview of recent progress in the study of
distributed multi-agent coordination. IEEE Transactions on Industrial informatics , 9(1):427–438, 2012.
Jacopo Castellini, Frans A Oliehoek, Rahul Savani, and Shimon Whiteson. The representational capacity of
action-value networks for multi-agent reinforcement learning. arXiv preprint arXiv:1902.07497 , 2019.
Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, and René Garcia. Incorporating functional
knowledge in neural networks. Journal of Machine Learning Research , 10(6), 2009.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Coun-
terfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artiﬁcial intelligence ,
volume 32, 2018.
Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and Mark Crowley. Kkt conditions, ﬁrst-order and second-
order optimization, and distributed optimization: Tutorial and survey. arXiv preprint arXiv:2110.01858 ,
2021.
Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In ICML,
volume 2, pp. 227–234. Citeseer, 2002.
Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep
reinforcement learning. In International conference on autonomous agents and multiagent systems , pp.
66–83. Springer, 2017.
Jian Hu, Siyang Jiang, Seth Austin Harding, Haibin Wu, and Shih-wei Liao. Riit: Rethinking the importance
of implementation tricks in multi-agent reinforcement learning. arXiv preprint arXiv:2102.03479 , 2021.
Yeping Hu, Alireza Nakhaei, Masayoshi Tomizuka, and Kikuo Fujimura. Interaction-aware decision mak-
ing with adaptive strategies under merging scenarios. In 2019 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) , pp. 151–158. IEEE, 2019.
Maximilian Hüttenrauch, Adrian Šošić, and Gerhard Neumann. Guided deep reinforcement learning for
swarm systems. arXiv preprint arXiv:1709.06011 , 2017.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In International
conference on machine learning , pp. 2961–2970. PMLR, 2019.
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z
Leibo, and Nando De Freitas. Social inﬂuence as intrinsic motivation for multi-agent deep reinforcement
learning. In International conference on machine learning , pp. 3040–3049. PMLR, 2019.
Peter Jin, Kurt Keutzer, and Sergey Levine. Regret minimization for partially observable deep reinforcement
learning. In International conference on machine learning , pp. 2342–2351. PMLR, 2018.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In In Proc.
19th International Conference on Machine Learning . Citeseer, 2002.
Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized
planning. Neurocomputing , 190:82–94, 2016.
13Under review as submission to TMLR
Steven George Krantz and Harold R Parks. The implicit function theorem: history, theory, and applications .
Springer Science & Business Media, 2002.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor
policies. The Journal of Machine Learning Research , 17(1):1334–1373, 2016.
Xu-Hui Liu, Zhenghai Xue, Jingcheng Pang, Shengyi Jiang, Feng Xu, and Yang Yu. Regret minimization
experiencereplayinoﬀ-policyreinforcementlearning. Advances in Neural Information Processing Systems ,
34:17604–17615, 2021.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. Advances in neural information processing
systems, 30, 2017.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational
exploration. arXiv preprint arXiv:1910.07483 , 2019.
Laëtitia Matignon, Laurent Jeanpierre, and Abdel-Illah Mouaddib. Coordinated multi-robot exploration
under communication constraints using decentralized markov decision processes. In Twenty-sixth AAAI
conference on artiﬁcial intelligence , 2012.
Edward James McShane. Jensen’s inequality. Bulletin of the American Mathematical Society , 43(8):521–527,
1937.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. nature, 518(7540):529–533, 2015.
Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs . Springer, 2016.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In
International Conference on Machine Learning , pp. 4295–4304. PMLR, 2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic
value function factorisation for deep multi-agent reinforcement learning, 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The star-
craft multi-agent challenge. arXiv preprint arXiv:1902.04043 , 2019.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning , pp. 5887–5896. PMLR, 2019.
Jianyu Su, Stephen Adams, and Peter Beling. Value-decomposition multi-agent actor-critics. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence , pp. 11352–11360, 2021.
Kefan Su and Zongqing Lu. Divergence-regularized multi-agent actor-critic. In International Conference on
Machine Learning , pp. 20580–20603. PMLR, 2022.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jader-
berg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for
cooperative multi-agent learning. arXiv preprint arXiv:1706.05296 , 2017.
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru,
and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. PloS one ,
12(4):e0172395, 2017.
14Under review as submission to TMLR
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the
tenth international conference on machine learning , pp. 330–337, 1993.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,
David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent
q-learning. arXiv preprint arXiv:2008.01062 , 2020a.
Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement learning
with emergent roles. arXiv preprint arXiv:2003.08039 , 2020b.
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Oﬀ-policy multi-agent
decomposed policy gradients. In International Conference on Learning Representations , 2020c.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten:
A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939 ,
2020.
Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing optimal joint
policy of maximum-entropy multi-agent reinforcement learning. In International Conference on Machine
Learning , pp. 12491–12500. PMLR, 2021.
15Under review as submission to TMLR
A Nomenclature
Table 1 summarizes the common notations in this paper.
Table 1: Deﬁnitions of notations.
Notation Deﬁnition
s State of the environment
a Agent
u Agents’ joint action
r Reward
γ Discount factor
π Individual policy
π Joint policy
π∗Expected optimal joint policy
η(π) Expected return under the joint policy π
dπ(s) Discounted state distribution
Q(·) Action value function
Qtot(·) Monotonic mixing of per-agent action value function
Q∗(·) Unrestricted joint action value function
V(·) Value function
A(·) Advantage function
fs(·) Monotonic function with input state s
B∗Bellman operator, where B∗Q(s,u)def=r(s,u) +γarg maxu/primeEs/primeQ(s/prime,u/prime)
w Projection weights of transitions
B Proof of Lemma 1
Considering a two-layer mixing network of the non-negative weight matrix W1,W2, biasb1,b2and activation
functionh(·). The input /vectorQis the vector of all the agents’ utilities. Assume there are nagents,/vectorQis:
/vectorQ= [Q1,...,Qn]T
We assume the mixing network has the width of m, based on the input/output dimension, W1should be a
n×mmatrix as:
W1=
w1
11... w1
1m
.........
w1
n1... w1
nm
,
andW2is am-dimension vector given by:
W2= [w2
1,...,w2
m]T.
Therefore,Qtotcalculated from the utility vector /vectorQbecomes:
fs(/vectorQ) =h(/vectorQTW1+b1)WT
2+b2. (8)
Considering one of the utilities Qa, as long as the derivative of activation h(·)exists (h(·)is smooth and
diﬀerentiable), based on equation 8, the result is:
f/prime
s,Qa=∂Qtot
∂Qa=h/prime
Qa(/vectorQTW1+b1)m/summationdisplay
j=1w1
ajw2
j. (9)
This concludes the proof.
16Under review as submission to TMLR
C Proof of Theorem 1
We have provided the outline of the proof containing four key steps. In this section, we present detailed proof
of the theorem. Following the existing work (Su & Lu, 2022), we will use state-based policies for simplicity.
Since we focus on the regret bound of the action value and deﬁne the Boltzmann policy in the regret via
action value, i.e., QtotandQ∗, we aim to ﬁnd the return gap between two action values characterized by
Boltzmann policy function under centralized training.
The original optimization problem needs solving is:
min
wkη(π∗)−η(πk)
s.t.Qk= arg min
Q∈QEµ[wk(s,u)(Q−B∗Q∗
k−1)2(s,u)],
Eµ[wk(s,u)] = 1, wk(s,u)≥0,
Qk(s,u) =fs(Q1(τ1,u1),...,Qn(τn,un)),
and this problem is equivalent to:
min
pkη(π∗)−η(πk)
s.t.Qk= arg min
Q∈QEpk[(Q−B∗Q∗
k−1)2(s,u)],
/summationdisplay
s,upk(s,u) = 1, pk(s,u)≥0,
Qk(s,u) =fs(Q1(τ1,u1),...,Qn(τn,un)),(10)
wherepk=wk(s,u)µ(s,u)is the solution to problem equation 10.
To solve the optimization problem in equation 10, we needed to provide some deﬁnitions, which are total
variation distance ,Wasserstein metric ,the diameter of a set , anduniversal approximator .
Deﬁnition 1 (Total variation distance) .The total variation distance of the distribution P and Q is deﬁned
asD(P,Q) =1
2/bardblP−Q/bardbl.
Deﬁnition2 (Wassersteinmetric) .For F,G two cumulative distribution functions over the reals, the Wasser-
stein metric is deﬁned as dp(F,G)def= infU,V/bardblU−V/bardblp, where the inﬁmum is taken over all pairs of random
variables (U,V) with cumulative distributions F and G, respectively.
Deﬁnition 3 (Diameter of a set) .The diameter of a set A is deﬁned as diam(A) = supx,y∈Am(x,y), where
m is the metric on A.
Deﬁnition 4 (Universal approximator) .A class of function ˆFfromRntoRis a universal approximator
for a class of functions FfromRntoRif for anyf∈F, any compact domain D⊂Rn, and any positive /epsilon1,
one can ﬁnd a ˆf∈ˆFwith supx∈D|f(x)−ˆf(x)|≤/epsilon1.
Furthermore, we introduce some mild assumptions as follows:
Assumption 1. The state space S, action space U, and observation space Zare compact metric spaces.
Assumption 2. The action-value and observation functions are continuous on S×UandZ, respectively.
Assumption 3. The transition function Tis continuous with respect to S×Uin the sense of Wasserstein
metric, which is lim(s,u)→(s0,u0)dp(T(·|s,u),T(·|s0,u0)).
Assumption 4. The joint policy πis the product of each agent’s individual policy πa(ua|sa).
Assumption 5. The monotonic mixing function fs(·)regarding per-agent action-value function Qafor
∀a∈Ais smooth and diﬀerentiable.
These assumptions are not strict and can be satisﬁed in most MARL environments.
17Under review as submission to TMLR
Letdπa(s)denote the discounted state distribution of agent a, anddπa
i(s)denote the distribution where the
state is visited by the agent for the i-th time. Thus, we have:
dπa(s) =∞/summationdisplay
i=1dπa
i(s), (11)
where each dπa
i(s)is given by:
dπa
i(s) = (1−γ)∞/summationdisplay
ti=0γtiPr(sti=s,stk=s,∀k= 1,...,i−1), (12)
where the Pr(sti=s,stk=s,∀k= 1,...,i−1)in this equation contains the probability of visiting state sfor
thei-th time at tiand a sequence of times tk, fork= 1,...,i, such that state sis visited at each tk. Thus,
stateswill be visited for itimes at time tiin total.
The following lemmas are proposed by Liu et al. (2021), where Lemma 2 support the derivation of the
Lemma 3, and the latter demonstrates that/vextendsingle/vextendsingle/vextendsingle∂dπa(s)
∂πa(s)/vextendsingle/vextendsingle/vextendsingleis a small quantity.
Lemma 2. Letfbe an Lebesgue integrable function. P and Q are two probability distributions, f≤C,
then:
|EP(x)f(x)−EQ(x)f(x)|≤C·D(P,Q). (13)
Lemma 3. Letρbe the probability of the agent astarting from (s,ua)and coming back to sat time step
tunder policy πa, i.e. Pr(s0=s,ua
0=ua,st=s,s1:t−1/negationslash=s;πa), and/epsilon1= sups,ua/summationtext∞
t=1γtρπa(s,ua,t). We
have: /vextendsingle/vextendsingle/vextendsingle/vextendsingle∂dπa(s)
∂πa(s)/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/epsilon1dπa
1(s), (14)
wheredπa
1(s) = (1−γ)/summationtext∞
t1=0γt1Pr(st1=s)and/epsilon1≤1.
Lemma 1 and 2 can be extended to suit the multi-agent scenario. Besides, we have the following lemma
holds in MARL:
Lemma 4. Given two policy πand¯π, where π=exp(Q(s,u))/summationtext
u/primeexp(Q(s,u/prime))is deﬁned by Boltzmann policy, we have:
Eu∼¯π[Q(s,u)]−Eu∼π[Q(s,u)]≤1. (15)
Proof.Suppose there are two joint actions uand¯u. LetQ(s,u) =p,Q(s,¯u) =qand letp≤q.
Eu∼¯π[Q(s,u)]−Eu∼π[Q(s,u)]≤q−pep+qeq
ep+eq
=q−p+qeq−p
1 +eq−p
=q−p−(q−p)eq−p
1 +eq−p.
Letf(z) =z−zez
1+ez, the maximum point z0satisﬁesf/prime(z) = 0, from which we further have 1 +ez0=z0ez0
wherez0∈(1,2). Therefore, we have
Eu∼¯π[Q(s,u)]−Eu∼π[Q(s,u)]≤f(q−p)≤z0−1≤1.
It is worth noting that the derived inequality can also be applied to the situation where we have joint action
more than two or we consider the situation regarding per-agent action.
18Under review as submission to TMLR
The following lemma is introduced by Kakade & Langford (2002). It was originally proposed for the ﬁnite
MDP, while it will also hold for the continuous scenario that is given by Assumption 1 and 2.
Lemma 5. For any policy πand˜π, we have
η(˜π)−η(π) =1
1−γEd˜π(s,u)[Aπ(s,u)], (16)
whereAπ(s,u)is the advantage function given by Aπ(s,u) =Qπ(s,u)−Vπ(s).
Lemma 6. Let/epsilon1πk= sups,u/summationtext∞
t=1γtρπ(s,u,t), the optimal solution pkto a relaxation of optimization
problem in equation 10 satisﬁes relationship as follows:
pk(s,u) =1
Z∗(Dk(s,u) +/epsilon1k(s,u)), (17)
where when Qk≤B∗Q∗
k−1, we haveDk(s,u) =dπk(s,u)(B∗Q∗
k−1−Qk) exp(Q∗
k−1−Qk)(/summationtextn
j=11−πj
f/prime
s,Qj−1),
and whenQk>B∗Q∗
k−1, we haveDk(s,u) = 0.Z∗is the normalization constant.
Proof.Suppose u∗∼π∗. Let π=π∗and˜π=πkin Lemma 5, we have
η(π∗)−η(πk)
=−1
1−γEdπk(s,u)[Aπ∗(s,u)]
=1
1−γEdπk(s,u)[V∗(s)−Q∗(s,u)]
=1
1−γEdπk(s,u)[V∗(s)−Qk(s,u∗) +Qk(s,u∗)−Qk(s,u) +Qk(s,u)−Q∗(s,u)]
(a)
≤1
1−γ/bracketleftbig
Edπk(s)(Q∗(s,u∗)−Qk(s,u∗)) +Edπk(s,u)(Qk(s,u)−Q∗(s,u)) + 1/bracketrightbig
,(18)
where (a) uses Lemma 4.
Since the original optimization is non-tractable, we consider this upper bound to obtain a closed-form
solution. Therefore, we replace the objective in equation 10 with the upper bound in equation 18 and solve
the relaxed optimization problem, given by
min
pkEdπk(s)[(Q∗
k−1−Qk)(s,u∗)] +Edπk(s,u)[(Qk−Q∗
k−1)(s,u)]
s.t.Qk= arg min
Q∈QEpk[(Q−B∗Q∗
k−1)2(s,u)],
/summationdisplay
s,upk(s,u) = 1, pk(s,u)≥0,
Qk(s,u) =fs(Q1(τ1,u1),...,Qn(τn,un)),(19)
The derived objective in equation 19 can be further relaxed with Jensen’s inequality, given by:
E[g(X)]≥g(E[X]), (20)
wheng(x)is a convex function on real space R.
According to equation 20, we select the convex function g(x) = exp(−x), and the objective can be further
relaxed as:
min
pk−logEdπk(s)[exp(Qk−Q∗
k−1)(s,u∗)]−logEdπk(s,u)[exp(Q∗
k−1−Qk)(s,u)]
s.t.Qk= arg min
Q∈QEpk[(Q−B∗Q∗
k−1)2(s,u)],
/summationdisplay
s,upk(s,u) = 1, pk(s,u)≥0,
Qk(s,u) =fs(Q1(τ1,u1),...,Qn(τn,un)),(21)
19Under review as submission to TMLR
In order to handle the optimization problem in equation 21, we follow the standard procedures of Lagrangian
multiplier method, which is:
L(pk;λ,ν) =−logEdπk(s)[exp(Qk−Q∗
k−1)(s,u∗)]−logEdπk(s,u)[exp(Q∗
k−1−Qk)(s,u)]+λ(/summationdisplay
s,upk−1)−νTpk,
(22)
After constructing the Lagrangian, we further compute some gradients that will be used in calculating the
optimal solution. We ﬁrst calculate the∂Qk
∂pkaccording to the implicit function theorem (IFT). Based on the
ﬁrst constraint in equation 21, we aim to ﬁnd the minimum Qkto satisfy the arg min(·), and therefore we
need to ensure the derivative of the term inside arg min(·)(we usef(pk,Qk)to denote this term) to be zero,
which is:
f/prime
Qk= 2/summationdisplay
s,upk(Qk−B∗Qk−1) = 0 (23)
We can notice that F(pk,Qk) :f/prime
Qk= 0is an implicit function regarding Qkandpk. Hence, we apply the
IFT on the F(pk,Qk)considering the Hessian matrices of pkandQkinf(pk,Qk)as follows:
∂Qk
∂pk=−F/prime
pk
F/prime
Qk=−[diag(pk)]−1/bracketleftbig
diag(Qk−B∗Q∗
k−1)/bracketrightbig
. (24)
Next, we derive the expression for∂dπk(s,u)
∂pkin the following equation:
∂dπk(s,u)
∂pk=∂dπk(s,u)
∂πk∂πk
∂Qa∂Qa
∂Qk∂Qk
∂pk
= diag(dπk(s) +/epsilon10(s))∂πk
∂Qa∂Qa
∂Qk∂Qk
∂pk
(b)= diag(dπk(s) +/epsilon10(s))diag( πk(1−πk))∂Qa
∂Qk∂Qk
∂pk
(c)=dπk(s,u)(1−πk)1
f/prime
s,Qk∂Qk
∂pk+/epsilon10(s)πk(1−πk)1
f/prime
s,Qk∂Qk
∂pk,(25)
where/epsilon10(s) =∂dπk(s,u)
∂πk(s)is a small quantity provided by Lemma 3. Besides, (b) is based on the the deﬁnition
of the Boltzmann policy and Assumption 4, and (c) is based on Assumption 5 the gradient of the monotonic
mixing function in Lemma 1.
Since we have all the preparations ready, we now compute the Lagrangian by applying the
Karush–Kuhn–Tucker (KKT) condition. We let the Lagrangian gradient to be zero, i.e.,
∂L(pk;λ,ν)
∂pk= 0 (26)
Besides, the partial derivative of the Lagrangian can be computed as:
∂L(pk;λ,ν)
∂pk=−∂logEdπk(s)[exp(Qk−Q∗
k−1)(s,u∗)]
∂pk−∂logEdπk(s,u)[exp(Q∗
k−1−Qk)(s,u)]
∂pk+λ−νs,u
=−1
Zexp(Q∗
k−1−Qk)/parenleftbigg∂dπk(s,u)
∂pk−dπk(s,u)∂Qk
∂pk/parenrightbigg
+λ−νs,u,
(27)
whereZ=Es/prime,u/prime∼dπk(s,u)exp(Q∗−Qk)(s/prime,u/prime).
20Under review as submission to TMLR
Based on equation 26 and equation 27, and substituting the expression of∂Qk
∂pkand∂dπk(s,a)
∂pkwith the derived
results in equation 24 and equation 25, we obtain:
pk(s,u) =1
Z(ν∗s,u−λ∗)
dπk(s,u)(Qk−B∗Q∗
k−1) exp(Q∗
k−1−Qk)
n/summationdisplay
j=11−πj
f/prime
s,Qj−1

+/epsilon10πk(Qk−B∗Q∗
k−1) exp(Q∗
k−1−Qk)n/summationdisplay
j=11−πj
f/prime
s,Qj
,(28)
According to Lemma 3, the value of /epsilon10is smaller than dπk(s)so the second term will not inﬂuence the
sign of the equation. Equation 28 will always be larger or equal to zero. By KKT condition, when the
Qk−B∗Q∗
k−1<0, we haveν∗
s,u= 0. When equation 28 equal to zero, we let ν∗
s,u= 0because the value of
ν∗
s,uwill not aﬀect pk. In the contrast, when the Qk−B∗Q∗
k−1>0, thepkshould equal to zero. Therefore,
by introducing a normalization factor Z∗, equation 28 can be simplify as follows:
pk(s,u) =1
Z∗(Dk(s,u) +/epsilon1k(s,u)), (29)
where when Qk≤B∗Q∗
k−1, we have
Dk(s,u) =dπk(s,u)(B∗Q∗
k−1−Qk) exp(Q∗
k−1−Qk)
n/summationdisplay
j=11−πj
f/prime
s,Qj−1

/epsilon1k=/epsilon10πk(Qk−B∗Q∗
k−1) exp(Q∗
k−1−Qk)n/summationdisplay
j=11−πj
f/prime
s,Qj(30)
and whenQk>B∗Q∗
k−1, we have
Dk(s,u) = 0
/epsilon1k= 0(31)
This concludes the proof.
D Environment Details
We use more recent baselines (i.e., FOP and DOP) that are known to outperform QTRAN (Son et al., 2019)
and QPLEX (Wang et al., 2020a) in the evaluation. In general, we tend to choose baselines that are more
closely related to our work and most recent. This motivated the choice of QMIX (baseline for value-based
factorization methods), WQMIX (close to our work that uses weighted projections so better joint actions
can be emphasized), VDAC (Su et al., 2021), FOP (Zhang et al., 2021), DOP (Wang et al., 2020c) (SOTA
actor-critic based methods). We acquired the results of QMIX, WQMIX based on their hyper-parameter
tuned versions from pymarl2(Hu et al., 2021) and implemented our algorithm based on it.
D.1 Predator-Prey
A partially observable environment on a grid-world predator-prey task is used to model relative overgeneral-
ization problem (Böhmer et al., 2020) where 8 agents have to catch 8 prey in a 10 ×10 grid. Each agent can
either move in one of the 4 compass directions, remain still, or try to catch any adjacent prey. Impossible
actions, i.e., moving into an occupied target position or catching when there is no adjacent prey, are treated
as unavailable. If two adjacent agents execute the catch action, a prey is caught and both the prey and
the catching agents are removed from the grid. An agent’s observation is a 5 ×5 sub-grid centered around
it, with one channel showing agents and another indicating prey. An episode ends if all agents have been
removed or after 200 steps. Capturing a prey is rewarded with r = 10, but unsuccessful attempts by single
agents are punished by a negative reward p. In this paper, we consider two sets of experiments with p= (0,
-0.5, -1.5, -2). The task is similar to the matrix game proposed by Son et al. (2019) but signiﬁcantly more
complex, both in terms of the optimal policy and in the number of agents.
21Under review as submission to TMLR
Table 2: Hyperparameter value settings.
Hyperparameter Value
Batch size 128
Replay buﬀer size 10000
Target network update interval Every 200 episodes
Learning rate 0.001
TD-lambda 0.6
D.2 SMAC
For the experiments on StarCraft II micromanagement, we follow the setup of SMAC (Samvelyan et al.,
2019) with open-source implementation including QMIX (Rashid et al., 2018), WQMIX (Rashid et al.,
2020), QPLEX (Wang et al., 2020a), FOP (Zhang et al., 2021), DOP (Wang et al., 2020c) and VDAC (Su
et al., 2021). We consider combat scenarios where the enemy units are controlled by the StarCraft II built-in
AI and the friendly units are controlled by the algorithm-trained agent. The possible options for built-in
AI diﬃculties are Very Easy, Easy, Medium, Hard, Very Hard, and Insane, ranging from 0 to 7. We carry
out the experiments with ally units controlled by a learning agent while built-in AI controls the enemy
units with diﬃculty = 7 (Insane). Depending on the speciﬁc scenarios(maps), the units of the enemy and
friendly can be symmetric or asymmetric. At each time step each agent chooses one action from discrete
action space, including noop, move[direction], attack[enemy_id], and stop. Dead units can only choose noop
action. Killing an enemy unit will result in a reward of 10 while winning by eliminating all enemy units
will result in a reward of 200. The global state information is only available in the centralized critic. Each
baseline algorithm is trained with 4 random seeds and evaluated every 10k training steps with 32 testing
episodes for main results, and with 3 random seeds for ablation results and additional results.
D.3 Implementation details and Hyperparameters
In this section, we introduce the implementation details and hyperparameters we used in the experiment.
We carried out the experiments on NVIDIA 2080Ti with ﬁxed hyperparameter settings. Recently Hu et al.
(2021) demonstrated that MARL algorithms are signiﬁcantly inﬂuenced by code-level optimization and other
tricks, e.g. using TD-lambda, Adam optimizer, and grid-searched hyperparameters (where many state-of-
the-art are already adopted), and proposed ﬁne-tuned QMIX and WQMIX, which is demonstrated with
signiﬁcant improvements from their original implementation. We implemented our algorithm based on its
open-sourced codebase and acquired the results of QMIX and WQMIX from it.
We use one set of hyperparameters for each environment, i.e., no tuned hyperparameters for individual maps.
We use epsilon greedy for action selection with annealing from /epsilon1= 0.995 decreasing to /epsilon1= 0.05 in 100000
training steps in a linear way. The performance for each algorithm is evaluated for 32 episodes every 1000
training steps. More hyperparameter values are given in Table 2.
22